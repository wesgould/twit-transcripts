
[00:00:00.000 --> 00:00:03.560]   It's time for Twit this week in Tech.
[00:00:03.560 --> 00:00:05.720]   I'm all alone in the studio.
[00:00:05.720 --> 00:00:07.320]   We sent everybody home.
[00:00:07.320 --> 00:00:08.520]   It's dark.
[00:00:08.520 --> 00:00:09.520]   It's New Year's Eve.
[00:00:09.520 --> 00:00:11.120]   And people are on vacation.
[00:00:11.120 --> 00:00:12.360]   So here's what we're going to do.
[00:00:12.360 --> 00:00:17.840]   A best-of-episode, some of the best moments from the years best episodes.
[00:00:17.840 --> 00:00:19.680]   And frankly, most of these were picked by you.
[00:00:19.680 --> 00:00:21.560]   So thank you for your contributions.
[00:00:21.560 --> 00:00:22.720]   I'm glad you're here.
[00:00:22.720 --> 00:00:24.560]   A great Twit ahead.
[00:00:24.560 --> 00:00:29.560]   Net-cast you love.
[00:00:29.560 --> 00:00:31.560]   From people you trust.
[00:00:31.560 --> 00:00:37.080]   This is Twit.
[00:00:37.080 --> 00:00:45.360]   Bandwidth for this week in Tech is provided by CashFly at C-A-C-H-E-F-L-Y.com.
[00:00:45.360 --> 00:00:51.560]   This is Twit, this week in Tech.
[00:00:51.560 --> 00:01:09.760]   We've hosted over 2.7 million interactive web events to connect with their audiences.
[00:01:09.760 --> 00:01:15.080]   For more, visit go-to-webinar.com/podcast.
[00:01:15.080 --> 00:01:16.560]   Welcome to this week in Tech.
[00:01:16.560 --> 00:01:19.480]   Leo Laport here, all alone in the studio.
[00:01:19.480 --> 00:01:20.480]   But don't worry.
[00:01:20.480 --> 00:01:21.480]   I'm not alone for long.
[00:01:21.480 --> 00:01:25.040]   Everybody makes an appearance in this show practically.
[00:01:25.040 --> 00:01:28.640]   We started the year, of course, with the inauguration of the new president.
[00:01:28.640 --> 00:01:35.120]   And Silicon Valley had kind of mixed reactions, especially to the immigration ban watch.
[00:01:35.120 --> 00:01:37.000]   I mean, I guess we have to talk about it.
[00:01:37.000 --> 00:01:39.840]   And, you know, I have mixed feelings about talking about politics.
[00:01:39.840 --> 00:01:43.800]   We try to keep to politics only where the inter-sex with technology.
[00:01:43.800 --> 00:01:47.280]   But in fact, there's a pretty big intersection.
[00:01:47.280 --> 00:01:52.280]   This is, of course, concluding of President Trump's first week in office, a lot of executive
[00:01:52.280 --> 00:01:53.280]   orders.
[00:01:53.280 --> 00:01:59.760]   And one yesterday that seemed to have really triggered a reaction from Silicon Valley.
[00:01:59.760 --> 00:02:03.320]   I shouldn't even say that from all of the tech community.
[00:02:03.320 --> 00:02:11.280]   And that's his ban on immigrants, visitors from seven nations that are predominantly
[00:02:11.280 --> 00:02:12.440]   Muslim.
[00:02:12.440 --> 00:02:17.200]   But more than that, even green card holders, legal residents of the United States, were
[00:02:17.200 --> 00:02:18.640]   blocked at the border.
[00:02:18.640 --> 00:02:21.040]   There was some question of whether that was the intent.
[00:02:21.040 --> 00:02:25.840]   Apparently the border control called the White House and said, "Now, do you want us to stop
[00:02:25.840 --> 00:02:27.080]   green card holders too?"
[00:02:27.080 --> 00:02:29.120]   And they said, "You bet."
[00:02:29.120 --> 00:02:31.080]   And a number of people were detained.
[00:02:31.080 --> 00:02:35.920]   The tech community's reaction to this was a varying strength.
[00:02:35.920 --> 00:02:43.280]   Zuckerberg, one of the first to respond with a somewhat tepid response on Facebook saying
[00:02:43.280 --> 00:02:48.200]   he didn't think it was a good idea, but not really calling it out.
[00:02:48.200 --> 00:02:51.840]   On the other hand, Sergey Brin, one of the founders of Google immediately went to San
[00:02:51.840 --> 00:02:55.920]   Francisco International Airport to join the protests there.
[00:02:55.920 --> 00:02:58.360]   And then there's Sergey.
[00:02:58.360 --> 00:03:01.400]   He of course is also an immigrant from the Soviet Union film.
[00:03:01.400 --> 00:03:02.400]   I think so, yeah.
[00:03:02.400 --> 00:03:03.400]   I think so.
[00:03:03.400 --> 00:03:04.400]   Yeah.
[00:03:04.400 --> 00:03:09.400]   So why is Silicon Valley upset about this?
[00:03:09.400 --> 00:03:16.000]   Yeah, to be honest with you, I'm not sure that the tech community is doing much good
[00:03:16.000 --> 00:03:17.000]   on this issue.
[00:03:17.000 --> 00:03:19.640]   They may actually be causing more problems.
[00:03:19.640 --> 00:03:20.640]   Yeah.
[00:03:20.640 --> 00:03:25.520]   Or even in the election in general, I think we are a little bit tone deaf about exactly
[00:03:25.520 --> 00:03:27.800]   who cares about what we think and who doesn't.
[00:03:27.800 --> 00:03:28.800]   Well, there's--
[00:03:28.800 --> 00:03:30.040]   We're becoming like Hollywood in this sense.
[00:03:30.040 --> 00:03:33.840]   It's a good point, but it's reasonable for Google, for instance, that has nearly 200
[00:03:33.840 --> 00:03:35.880]   employees that are blocked by this order.
[00:03:35.880 --> 00:03:36.880]   Of course.
[00:03:36.880 --> 00:03:40.120]   People were-- if they were overseas, would not be able to come back to Google.
[00:03:40.120 --> 00:03:41.120]   Sure.
[00:03:41.120 --> 00:03:47.640]   Many Google employees said since our colleagues can't travel, we are not going to travel anymore.
[00:03:47.640 --> 00:03:51.200]   Google issued a statement saying we're concerned about the impact of this order and any proposals
[00:03:51.200 --> 00:03:55.960]   that could impose restrictions on Googlers and their families, or that could create
[00:03:55.960 --> 00:03:57.720]   barriers to bringing great talent to the US.
[00:03:57.720 --> 00:04:00.760]   We continue to make our views on these issues, known to leaders in Washington and elsewhere.
[00:04:00.760 --> 00:04:03.480]   It does sound like they're not condemning the order.
[00:04:03.480 --> 00:04:06.800]   They're more just saying, hey, this impacts our business, so we're not happy about that.
[00:04:06.800 --> 00:04:11.120]   Yeah, and of course, every company has a responsibility to its employees and to make
[00:04:11.120 --> 00:04:15.440]   sure that everyone's OK and that business doesn't get interrupted.
[00:04:15.440 --> 00:04:20.840]   And the much larger question is, I think we do have some moral responsibilities to refugees,
[00:04:20.840 --> 00:04:23.160]   to immigrants.
[00:04:23.160 --> 00:04:27.440]   But I do think the discussion on both sides here has been pretty poor and not helpful.
[00:04:27.440 --> 00:04:29.480]   There's very few reasonable voices here.
[00:04:29.480 --> 00:04:33.680]   And the tech leaders, and Tim Cook, by the way, is in Washington right now, the CEO of Apple.
[00:04:33.680 --> 00:04:38.400]   He had dinner with Ivanka Trump and her husband, Jared Kushner.
[00:04:38.400 --> 00:04:42.520]   Trump's advisor on Thursday has been going around making the rounds.
[00:04:42.520 --> 00:04:45.200]   Cook issued a statement that I've heard from many of you who are deeply concerned about
[00:04:45.200 --> 00:04:49.960]   the executive order issued yesterday, restricting immigration from seven Muslim-majority countries.
[00:04:49.960 --> 00:04:51.640]   I share your concerns.
[00:04:51.640 --> 00:04:53.240]   It is not a policy we support.
[00:04:53.240 --> 00:04:59.080]   It was probably the strongest statement from all the CEOs.
[00:04:59.080 --> 00:05:02.720]   But Tim Cook went and sat down along with Elon Musk.
[00:05:02.720 --> 00:05:03.960]   In fact, Musk's now met twice.
[00:05:03.960 --> 00:05:06.160]   He met again this week with President Trump.
[00:05:06.160 --> 00:05:10.080]   Do you agree, having been in a position like this, do you agree with them that it's important
[00:05:10.080 --> 00:05:11.520]   to meet with the incoming president?
[00:05:11.520 --> 00:05:12.520]   Yeah, absolutely.
[00:05:12.520 --> 00:05:13.800]   I don't see why wouldn't you?
[00:05:13.800 --> 00:05:19.080]   Yeah, I don't see what's to be gained by shouting about everything.
[00:05:19.080 --> 00:05:22.000]   I think it's just going to make the world worse.
[00:05:22.000 --> 00:05:24.800]   There should be reasonable people on both sides.
[00:05:24.800 --> 00:05:25.960]   It's sometimes hard to do that.
[00:05:25.960 --> 00:05:29.720]   There's a lot of pressure to take a strong position.
[00:05:29.720 --> 00:05:32.480]   There's a lot of pressure to be very vocal.
[00:05:32.480 --> 00:05:36.040]   It's good that a lot of people are doing that, but I think we also need people who are actually
[00:05:36.040 --> 00:05:39.320]   trying to be reasonable and move the world forward.
[00:05:39.320 --> 00:05:44.920]   Right, but when you're championing yourself as a beacon of hope and light and basically
[00:05:44.920 --> 00:05:51.880]   constantly claiming that you're guiding the United States to this new frontier of great
[00:05:51.880 --> 00:05:53.480]  ness through technology.
[00:05:53.480 --> 00:05:58.080]   I think that you owe it to the people that you've promised, both American citizens, but
[00:05:58.080 --> 00:06:03.680]   also your employees, to take a stand against some of this stuff.
[00:06:03.680 --> 00:06:09.040]   I won't say that every single thing that Donald Trump is doing is wrong, but there are certainly
[00:06:09.040 --> 00:06:16.560]   some measures that can be described accurately as xenophobic and problematic when it comes
[00:06:16.560 --> 00:06:22.720]   to sourcing talent from other countries and basically creating this new and open world
[00:06:22.720 --> 00:06:26.360]   that all of these Silicon Valley companies are constantly talking about.
[00:06:26.360 --> 00:06:33.440]   I was actually really disappointed by the number of leaders that were willing to speak
[00:06:33.440 --> 00:06:38.440]   out and the amount of time that it took them to actually say something.
[00:06:38.440 --> 00:06:45.280]   There are people like Cheryl Sandberg who have spent years talking about progressive ideals
[00:06:45.280 --> 00:06:51.480]   and then when push comes to shove have actually been pretty quiet about these things.
[00:06:51.480 --> 00:06:57.720]   She did finally issue a statement about not about the immigration thing, but about the
[00:06:57.720 --> 00:07:04.280]   women's, no, I think Planned Parenthood, the women's march, she said.
[00:07:04.280 --> 00:07:07.120]   But you're right, she hasn't said a lot.
[00:07:07.120 --> 00:07:14.640]   I think as a sea level executive at a publicly held company, there is some responsibility
[00:07:14.640 --> 00:07:21.040]   to not be political, not to become a Meryl Streep, but to in fact to be more judicious.
[00:07:21.040 --> 00:07:22.040]   Is that right?
[00:07:22.040 --> 00:07:26.360]   So look, I have personal opinions on all this stuff and I'm actually totally happy talking
[00:07:26.360 --> 00:07:27.360]   about them.
[00:07:27.360 --> 00:07:28.360]   Yeah.
[00:07:28.360 --> 00:07:29.360]   But that's separate.
[00:07:29.360 --> 00:07:31.760]   Totally happy talking about them.
[00:07:31.760 --> 00:07:34.720]   What I'm also trying to be mindful of is what's actually going to be the most effective
[00:07:34.720 --> 00:07:38.120]   way for the world to move in the direction that I would like it to move.
[00:07:38.120 --> 00:07:43.680]   And very often that's not telling people from a position of assumed authority what they
[00:07:43.680 --> 00:07:45.440]   should think and how they should think.
[00:07:45.440 --> 00:07:47.000]   How would you go about it?
[00:07:47.000 --> 00:07:50.840]   Well, for example, it's not clear to me that Cheryl Sandberg, like being louder and faster
[00:07:50.840 --> 00:07:55.200]   about her position actually helps convince the people that need to be convinced to change
[00:07:55.200 --> 00:07:56.200]   the position.
[00:07:56.200 --> 00:07:58.840]   It maybe makes people feel better, maybe makes for better demonstrations.
[00:07:58.840 --> 00:08:02.920]   I'm not sure that it actually makes the world, it pushes the world further in the direction
[00:08:02.920 --> 00:08:04.440]   that Cheryl would want it to be put.
[00:08:04.440 --> 00:08:08.600]   There's also a significant risk, I think, of silica.
[00:08:08.600 --> 00:08:15.520]   At some point, people realizing that the problem isn't so much immigration.
[00:08:15.520 --> 00:08:22.960]   The problem is more created by Silicon Valley and automation and that in fact the tech community
[00:08:22.960 --> 00:08:25.120]   could quickly become the enemy here.
[00:08:25.120 --> 00:08:26.120]   Could they not?
[00:08:26.120 --> 00:08:28.360]   We are pretty tone deaf as an industry.
[00:08:28.360 --> 00:08:35.320]   I think we got used to being treated as heroes and the people that are making the world
[00:08:35.320 --> 00:08:38.040]   the past decade or so and I think that's on the verge of changing.
[00:08:38.040 --> 00:08:39.760]   I think we're kind of tone deaf about how we talk about it.
[00:08:39.760 --> 00:08:43.600]   I think like we would trip over that line a while ago.
[00:08:43.600 --> 00:08:47.240]   Hollywood's given up, Hollywood isn't here, Hollywood's Hollywood and I think most people
[00:08:47.240 --> 00:08:51.160]   in California feel the same way and we're close to following that.
[00:08:51.160 --> 00:08:53.080]   The tech industry could be going over that line.
[00:08:53.080 --> 00:08:54.080]   I would like us not to.
[00:08:54.080 --> 00:08:58.160]   I'd like us to remain effective and I think that a little bit of shouting can help hurts
[00:08:58.160 --> 00:08:59.160]   that.
[00:08:59.160 --> 00:09:03.080]   What I've learned over the past week is that Silicon Valley continues to exist in a bubble
[00:09:03.080 --> 00:09:08.240]   that is essentially unrelatable for a large majority of Americans.
[00:09:08.240 --> 00:09:15.720]   There's been a diversity problem in Silicon Valley for years that is basically underreported
[00:09:15.720 --> 00:09:21.120]   and under discussed.
[00:09:21.120 --> 00:09:26.120]   I don't want to put all of the blame on Cheryl Samberg but I think she in particular is an
[00:09:26.120 --> 00:09:32.600]   easy target because she wants to be or she positions herself to be considered a leader
[00:09:32.600 --> 00:09:39.400]   for the feminist movement and wrote the book, Lean In.
[00:09:39.400 --> 00:09:50.960]   I think when there is such divisive rhetoric that is just constantly coming from Washington,
[00:09:50.960 --> 00:09:56.880]   I think that you kind of have to choose one way or another.
[00:09:56.880 --> 00:10:00.880]   Either you're purely a capitalist and that's totally fine and you can refrain from saying
[00:10:00.880 --> 00:10:13.120]   anything but then you can't go on to expect to be treated as an authoritative leader on
[00:10:13.120 --> 00:10:14.320]   any progressive movement.
[00:10:14.320 --> 00:10:18.360]   I think Phil, you're saying they shouldn't be a leader in the progressive movement.
[00:10:18.360 --> 00:10:20.800]   They should stick to their knitting and do their job.
[00:10:20.800 --> 00:10:25.880]   I can understand why Sergey Brin went to that protest but I think as a private citizen,
[00:10:25.880 --> 00:10:29.640]   I don't think he said I'm bearing the banner of Google but as a private citizen, I feel
[00:10:29.640 --> 00:10:31.840]   strongly about this because I was a refugee.
[00:10:31.840 --> 00:10:34.720]   I think you've got three different issues here that are being conflated.
[00:10:34.720 --> 00:10:38.920]   One is the issue of what should people, how publicly should people state their views.
[00:10:38.920 --> 00:10:40.600]   The issue of that is publicly as they want.
[00:10:40.600 --> 00:10:44.200]   Everyone can make that decision and state the most publicly as they want.
[00:10:44.200 --> 00:10:49.920]   Second issue is what is the proper role of someone speaking for a company.
[00:10:49.920 --> 00:10:50.920]   That's a different matter entirely.
[00:10:50.920 --> 00:10:52.520]   It's a different matter entirely.
[00:10:52.520 --> 00:10:55.840]   Then there's a third issue which is what's most effective.
[00:10:55.840 --> 00:10:59.520]   It's that most effective issue where we are about to go over a cliff as an industry.
[00:10:59.520 --> 00:11:04.120]   We are about to take everything that we in general think is important and significantly
[00:11:04.120 --> 00:11:09.560]   hold those things back by being perceived as hectoring and lecturing and being tone deaf
[00:11:09.560 --> 00:11:10.560]   and being out of touch.
[00:11:10.560 --> 00:11:12.280]   We will make the situation worse if we keep doing it.
[00:11:12.280 --> 00:11:13.280]   Can I say something?
[00:11:13.280 --> 00:11:17.320]   I've been real quiet because I've tried not to do the yelling thing that you said.
[00:11:17.320 --> 00:11:25.000]   I feel like we are on the edge right now on the precipice of a patented O-Dr. Rand.
[00:11:25.000 --> 00:11:29.720]   As a person who doesn't live in a bubble, I think that you're generally wrong when you
[00:11:29.720 --> 00:11:34.360]   say that the tech community is tone deaf and that Hollywood is tone deaf.
[00:11:34.360 --> 00:11:39.240]   The country in a majority voted an opposite direction of what we currently have as a regime.
[00:11:39.240 --> 00:11:42.760]   The popular number of people.
[00:11:42.760 --> 00:11:46.600]   The way the people are speaking and when they speak out, it's not tone deaf in journalists
[00:11:46.600 --> 00:11:53.200]   you live in the middle of the country and you've seen your job creation go away.
[00:11:53.200 --> 00:11:57.520]   As opposed to Silicon Valley, where do they become an automated, we're not talking about
[00:11:57.520 --> 00:12:01.360]   what education people need to be having and what we should do for education for people
[00:12:01.360 --> 00:12:03.080]   to have these new future jobs.
[00:12:03.080 --> 00:12:06.280]   Because until we get the robots to build themselves, build the wall, pay the taxes for
[00:12:06.280 --> 00:12:09.000]   it, we've got to do some other things and fix these issues.
[00:12:09.000 --> 00:12:13.520]   So yes, it's important for a CEO of a major company who has immigrants working for them
[00:12:13.520 --> 00:12:16.240]   mostly as engineers or whatever they have because you never would see them on a higher
[00:12:16.240 --> 00:12:18.040]   echelon of work and jobs.
[00:12:18.040 --> 00:12:19.040]   So wait a minute.
[00:12:19.040 --> 00:12:23.040]   So I said in general and don't talk about it.
[00:12:23.040 --> 00:12:27.520]   Don't talk to me like that because he just made the statement of the fact that racially
[00:12:27.520 --> 00:12:30.560]   the waste of the Silicon Valley is, you know how it is in general.
[00:12:30.560 --> 00:12:32.120]   I said in general, I didn't say all.
[00:12:32.120 --> 00:12:33.200]   I didn't say every.
[00:12:33.200 --> 00:12:34.840]   I said generally.
[00:12:34.840 --> 00:12:35.840]   Okay.
[00:12:35.840 --> 00:12:36.840]   So again, let me finish.
[00:12:36.840 --> 00:12:43.360]   I'm just saying, I'm just saying they're not tone deaf and maybe if we did a little
[00:12:43.360 --> 00:12:46.920]   bit better educating the people in the middle of the country about what's about to happen
[00:12:46.920 --> 00:12:51.480]   to them, but we don't have to fall off a cliff or push ourselves off a cliff or repel off
[00:12:51.480 --> 00:12:52.480]   a cliff.
[00:12:52.480 --> 00:12:54.040]   What do you do with you on that together about it?
[00:12:54.040 --> 00:12:58.040]   There is a leadership role Silicon Valley could take, which is look, the change is coming.
[00:12:58.040 --> 00:12:59.200]   We understand this change.
[00:12:59.200 --> 00:13:00.720]   You might want to get ready for this.
[00:13:00.720 --> 00:13:01.720]   This is the educational.
[00:13:01.720 --> 00:13:06.880]   I mean, you push STEM, but this is these are things you've got to do as a country we've
[00:13:06.880 --> 00:13:12.720]   got to do to prepare ourselves and it's not it's not building a wall because that's not
[00:13:12.720 --> 00:13:13.880]   going to solve the problem.
[00:13:13.880 --> 00:13:15.240]   That's not where the problem is coming from.
[00:13:15.240 --> 00:13:16.240]   That's a mistake.
[00:13:16.240 --> 00:13:20.840]   I saw the best thing I saw for that was instead of building a wall, why don't we build a solar
[00:13:20.840 --> 00:13:23.840]   panel wall out there in the desert and get some free energy.
[00:13:23.840 --> 00:13:26.080]   I'm like, is the little things like that?
[00:13:26.080 --> 00:13:29.480]   I'm like, I like a 12 year old said it and you're like, that'd be amazing.
[00:13:29.480 --> 00:13:30.480]   All right.
[00:13:30.480 --> 00:13:33.360]   How many of you bought snap stock?
[00:13:33.360 --> 00:13:34.720]   We had Kevin Rose on yesterday.
[00:13:34.720 --> 00:13:36.760]   He said, I passed on snap.
[00:13:36.760 --> 00:13:37.760]   Do you pass on snap?
[00:13:37.760 --> 00:13:39.000]   It's not ethical for me.
[00:13:39.000 --> 00:13:40.000]   Yeah.
[00:13:40.000 --> 00:13:41.000]   Yeah.
[00:13:41.000 --> 00:13:42.000]   We can.
[00:13:42.000 --> 00:13:45.160]   I could, but I passed.
[00:13:45.160 --> 00:13:46.960]   So I think that why you don't believe in millennials?
[00:13:46.960 --> 00:13:48.720]   Sorry, I'm just kidding.
[00:13:48.720 --> 00:13:49.720]   Go ahead.
[00:13:49.720 --> 00:13:53.040]   No, see age.
[00:13:53.040 --> 00:13:56.360]   I actually think that it probably will be a little bit like Facebook where you had this
[00:13:56.360 --> 00:14:02.200]   big pop, people got excited and then they have a, you know, slow down earnings or growth
[00:14:02.200 --> 00:14:05.680]   doesn't shape up as quickly as possible and then it drops and so maybe I'll buy it when
[00:14:05.680 --> 00:14:06.680]   it drops.
[00:14:06.680 --> 00:14:09.640]   They're apparently they expect as many as a billion dollars.
[00:14:09.640 --> 00:14:10.640]   I'm sorry.
[00:14:10.640 --> 00:14:11.640]   Stop.
[00:14:11.640 --> 00:14:12.640]   Stop.
[00:14:12.640 --> 00:14:13.960]   I hate I had all played video.
[00:14:13.960 --> 00:14:17.640]   I thought I had it turned off apparently not.
[00:14:17.640 --> 00:14:21.120]   Yeah, there's as much as a billion dollars in shorting going on right now and snap.
[00:14:21.120 --> 00:14:24.920]   There's a lot of people betting as you will that that pop will go away.
[00:14:24.920 --> 00:14:27.440]   They were saying that they were going to, and you'll help me with this because you're
[00:14:27.440 --> 00:14:28.440]   the financial guy.
[00:14:28.440 --> 00:14:29.440]   Sure.
[00:14:29.440 --> 00:14:30.440]   Right.
[00:14:30.440 --> 00:14:34.760]   They were saying initially that the IPO would be 14 to $16.
[00:14:34.760 --> 00:14:39.960]   They ended up opting for $17 which means they took a lot of money off the table for the founders.
[00:14:39.960 --> 00:14:41.920]   Each took $300 million for themselves.
[00:14:41.920 --> 00:14:43.160]   You have quarter billion and 300 million.
[00:14:43.160 --> 00:14:45.360]   Yeah, I forget the final number, but right in there.
[00:14:45.360 --> 00:14:47.360]   Right in there.
[00:14:47.360 --> 00:14:53.800]   And in the first day they went from 17, actually they started trading at $24 a share which means
[00:14:53.800 --> 00:14:58.680]   there was a lot of institutional and insider trading upfront right before they went.
[00:14:58.680 --> 00:15:00.280]   Well, you set up price.
[00:15:00.280 --> 00:15:02.080]   It's a big dance on the first day of the IPO.
[00:15:02.080 --> 00:15:05.920]   You sell the shares, the underwriters, you fill the book beforehand at $17 a share and
[00:15:05.920 --> 00:15:08.160]   then the opening price is set by kind of a give and take.
[00:15:08.160 --> 00:15:09.160]   Dick puts the nasty.
[00:15:09.160 --> 00:15:11.160]   The underwriters, they got special clients.
[00:15:11.160 --> 00:15:14.240]   They got people they particularly like they get let them in on the deal, right?
[00:15:14.240 --> 00:15:15.240]   You and I are not going to get.
[00:15:15.240 --> 00:15:16.240]   We don't get that.
[00:15:16.240 --> 00:15:17.840]   That's a little bit of a complication I wouldn't think.
[00:15:17.840 --> 00:15:27.120]   If you did, if you were lucky enough to, 44% pop by the time the day was done, $24.48.
[00:15:27.120 --> 00:15:28.680]   And I don't know what it is right now.
[00:15:28.680 --> 00:15:30.360]   Share prices went as high as 26.
[00:15:30.360 --> 00:15:34.040]   Made a lot of people very rich, including one parochial high school.
[00:15:34.040 --> 00:15:35.240]   Yeah, I saw that.
[00:15:35.240 --> 00:15:37.160]   Which one was it in?
[00:15:37.160 --> 00:15:40.800]   Well, this the story is in Los Gatos.
[00:15:40.800 --> 00:15:43.160]   Yeah, it was a Bay Area high school.
[00:15:43.160 --> 00:15:45.160]   The story is that.
[00:15:45.160 --> 00:15:46.160]   Same Francis.
[00:15:46.160 --> 00:15:48.200]   The Eggers, you may know his name.
[00:15:48.200 --> 00:15:49.400]   He's at Lightspeed Ventures.
[00:15:49.400 --> 00:15:51.240]   They were one of the early snap investors.
[00:15:51.240 --> 00:15:52.240]   They were the first.
[00:15:52.240 --> 00:15:53.400]   Yeah, they were the first.
[00:15:53.400 --> 00:15:54.400]   They made a lot of money, right?
[00:15:54.400 --> 00:15:55.400]   They put it in half.
[00:15:55.400 --> 00:15:56.400]   They had a really good one.
[00:15:56.400 --> 00:15:58.680]   26,000 percent return.
[00:15:58.680 --> 00:16:02.080]   But Barry spread the wealth because he's a parent at that high school.
[00:16:02.080 --> 00:16:06.120]   And the high school, which is weird, had an investment fund, a financial growth fund.
[00:16:06.120 --> 00:16:08.480]   He's the chairman and he said, "Hey, I can get you.
[00:16:08.480 --> 00:16:09.480]   I can get you a taste of that."
[00:16:09.480 --> 00:16:10.480]   Well, Silicon Valley thing to do.
[00:16:10.480 --> 00:16:11.720]   I can get you a taste of that.
[00:16:11.720 --> 00:16:17.920]   So they bought $15,000 in shares, which are worth now.
[00:16:17.920 --> 00:16:20.680]   Well, they sold two thirds of them.
[00:16:20.680 --> 00:16:23.520]   They got $24 million.
[00:16:23.520 --> 00:16:24.520]   That's a new gymnasium.
[00:16:24.520 --> 00:16:26.440]   Did your high school have a fund?
[00:16:26.440 --> 00:16:29.080]   Because my public high school had like two pencils.
[00:16:29.080 --> 00:16:30.080]   I know.
[00:16:30.080 --> 00:16:31.080]   A lot of fun.
[00:16:31.080 --> 00:16:33.000]   I don't know how kind of cost will Christian school, so I'm nice.
[00:16:33.000 --> 00:16:36.760]   If they haven't downboxed, though, they have to invest in endowment.
[00:16:36.760 --> 00:16:39.760]   So a lot of private schools have endowments.
[00:16:39.760 --> 00:16:41.560]   People leave them money or whatever.
[00:16:41.560 --> 00:16:42.560]   It may not be a lot of money.
[00:16:42.560 --> 00:16:43.560]   It's $15,000.
[00:16:43.560 --> 00:16:44.560]   $15,000.
[00:16:44.560 --> 00:16:45.560]   Those not a huge amount.
[00:16:45.560 --> 00:16:48.480]   I would have loved to invest in $15,000.
[00:16:48.480 --> 00:16:50.040]   They didn't even sell all of it to get 20.
[00:16:50.040 --> 00:16:51.040]   You know what?
[00:16:51.040 --> 00:16:52.040]   Hindsight is always 20/20 on these things.
[00:16:52.040 --> 00:16:56.280]   If you put $15,000 into Uber or $15,000 into a lot of things, you'd be, you know.
[00:16:56.280 --> 00:16:57.280]   Sure.
[00:16:57.280 --> 00:16:59.440]   Alternatively, Clinkle or Rachio?
[00:16:59.440 --> 00:17:00.440]   Well, that's the problem.
[00:17:00.440 --> 00:17:01.440]   You don't know.
[00:17:01.440 --> 00:17:02.440]   Is it a Clinkle or is it a snap?
[00:17:02.440 --> 00:17:03.440]   You don't know.
[00:17:03.440 --> 00:17:08.520]   Simon Chiu, the president of St. Francis High School, said, "Barry, this is the president
[00:17:08.520 --> 00:17:09.520]   of the school.
[00:17:09.520 --> 00:17:13.560]   He now joins the illustrious list of individuals and families whose foresight and generosity
[00:17:13.560 --> 00:17:16.120]   have enriched St. Francis High School."
[00:17:16.120 --> 00:17:17.120]   So enriched.
[00:17:17.120 --> 00:17:19.720]   Did you come and rich my student loves?
[00:17:19.720 --> 00:17:21.200]   Come and rich me.
[00:17:21.200 --> 00:17:22.200]   Wow.
[00:17:22.200 --> 00:17:23.200]   "New York Times.
[00:17:23.200 --> 00:17:25.560]   Is American retail at a historic tipping point?"
[00:17:25.560 --> 00:17:26.560]   Oh, yeah.
[00:17:26.560 --> 00:17:28.840]   And that goes with the zombie shopping mall story, too.
[00:17:28.840 --> 00:17:29.840]   Yeah.
[00:17:29.840 --> 00:17:33.080]   What's wrong with America's retailers?
[00:17:33.080 --> 00:17:38.600]   More workers in general merchandise stores have been laid off since October, about 89,000
[00:17:38.600 --> 00:17:42.920]   Americans, more than all of the people employed in the United States' coal industry.
[00:17:42.920 --> 00:17:43.920]   Yeah.
[00:17:43.920 --> 00:17:45.880]   I mean, that's basically the coal industry is Arby's.
[00:17:45.880 --> 00:17:46.880]   Yeah.
[00:17:46.880 --> 00:17:47.880]   This is retail.
[00:17:47.880 --> 00:17:48.880]   Yeah.
[00:17:48.880 --> 00:17:53.160]   This is this particular sector of the economy, which is huge, going down the toilet.
[00:17:53.160 --> 00:17:59.480]   One analyst says, "The change is coming at you so fast it feels like it's accelerating.
[00:17:59.480 --> 00:18:03.520]   The transformation's hollowing out suburban shopping malls, bankrupting long-time brands
[00:18:03.520 --> 00:18:06.160]   and leading to staggering job losses."
[00:18:06.160 --> 00:18:07.600]   What's the zombie shopping mall?
[00:18:07.600 --> 00:18:11.320]   Well, they did a -- it's like photos in another story that's related to this.
[00:18:11.320 --> 00:18:13.640]   It's probably linked from it.
[00:18:13.640 --> 00:18:17.720]   That is, it's the same idea is that there are certain malls that have recast themselves
[00:18:17.720 --> 00:18:19.960]   and they've gone higher end and they've been successful.
[00:18:19.960 --> 00:18:20.960]   Yeah.
[00:18:20.960 --> 00:18:21.960]   There's a reason to go.
[00:18:21.960 --> 00:18:23.680]   But there are a lot of them that are just falling apart because you know their anchor
[00:18:23.680 --> 00:18:24.680]   stores are basically the anchor.
[00:18:24.680 --> 00:18:25.680]   Are you losing the anchor?
[00:18:25.680 --> 00:18:26.680]   Yeah.
[00:18:26.680 --> 00:18:27.680]   And those are not drawing people in.
[00:18:27.680 --> 00:18:31.760]   And it turns out that so many people are preferring to shop at big box stores or on
[00:18:31.760 --> 00:18:35.200]   the internet that the traditional shopping mall is now a lot less appealing.
[00:18:35.200 --> 00:18:36.200]   And so some of them can redefine themselves.
[00:18:36.200 --> 00:18:38.200]   These are closing like 100 stores.
[00:18:38.200 --> 00:18:39.200]   But a lot of them can't.
[00:18:39.200 --> 00:18:43.680]   And like Sears and Kmart, which Sears owns Kmart, that is a company that may not make
[00:18:43.680 --> 00:18:44.760]   it at all.
[00:18:44.760 --> 00:18:46.160]   They may just go out of business.
[00:18:46.160 --> 00:18:50.720]   And so what do you do with that space and how do you turn it into something else?
[00:18:50.720 --> 00:18:52.160]   Oh yeah, the story is good too.
[00:18:52.160 --> 00:18:53.520]   Is this zombie mall?
[00:18:53.520 --> 00:18:55.320]   This is Amazon.
[00:18:55.320 --> 00:18:57.760]   A lot of it, you know, and there are other trends too.
[00:18:57.760 --> 00:18:58.760]   Like you use a Walmart.
[00:18:58.760 --> 00:18:59.760]   And Amazon is hiring.
[00:18:59.760 --> 00:19:01.760]   Like didn't they just announce they're going to hire like tens of thousands of them?
[00:19:01.760 --> 00:19:02.760]   Yeah.
[00:19:02.760 --> 00:19:05.760]   And by the way, the worst jobs ever, you're working in these very --
[00:19:05.760 --> 00:19:06.760]   That is an issue.
[00:19:06.760 --> 00:19:07.760]   Yeah, difficult conditions.
[00:19:07.760 --> 00:19:10.400]   With robots that will eventually replace you there.
[00:19:10.400 --> 00:19:13.320]   And eventually you'll have pick and pull robots.
[00:19:13.320 --> 00:19:15.320]   But don't -- I don't know.
[00:19:15.320 --> 00:19:19.360]   I feel bad, but I'm happier buying my stuff on Amazon Prime than going to the mall.
[00:19:19.360 --> 00:19:20.360]   Me too.
[00:19:20.360 --> 00:19:22.040]   Do you buy clothes on Amazon?
[00:19:22.040 --> 00:19:23.040]   Yeah.
[00:19:23.040 --> 00:19:24.760]   Do you buy shoes on Amazon?
[00:19:24.760 --> 00:19:26.800]   Yeah, I just did.
[00:19:26.800 --> 00:19:30.160]   See, those are the reasons people would go somewhere to try something on to see how it
[00:19:30.160 --> 00:19:31.160]   looks in person.
[00:19:31.160 --> 00:19:32.840]   I still do that to a certain degree.
[00:19:32.840 --> 00:19:33.840]   Like I'll --
[00:19:33.840 --> 00:19:35.960]   I'll be shopping interface though for stuff like clothes.
[00:19:35.960 --> 00:19:40.440]   Yeah, so I'll get a pair of shoes at some point at a local store and I'll pay a lot
[00:19:40.440 --> 00:19:41.720]   for them.
[00:19:41.720 --> 00:19:45.680]   But when I want to refill, when I want that shoe again, if I can get it on Amazon, I'll
[00:19:45.680 --> 00:19:46.680]   just get it again.
[00:19:46.680 --> 00:19:48.480]   Because I know it fits and I know that I like it.
[00:19:48.480 --> 00:19:49.480]   Yeah.
[00:19:49.480 --> 00:19:52.120]   So it tends to be like the first purchase is at a store.
[00:19:52.120 --> 00:19:56.160]   But then if I want 10 more down the road, they don't get my sale because, you know,
[00:19:56.160 --> 00:19:58.200]   I paid them for their marked up first version.
[00:19:58.200 --> 00:20:01.680]   But then in the end, it's just so much more convenient to say, you know, it's six months
[00:20:01.680 --> 00:20:02.680]   later.
[00:20:02.680 --> 00:20:03.680]   I'm just going to have it set in a box.
[00:20:03.680 --> 00:20:07.440]   Yeah, I mean my sister's wedding in a couple months and I bought three pairs of shoes on
[00:20:07.440 --> 00:20:08.440]   Amazon last week.
[00:20:08.440 --> 00:20:09.440]   Okay.
[00:20:09.440 --> 00:20:10.440]   Partly because it's easy to return, right?
[00:20:10.440 --> 00:20:11.440]   Yeah.
[00:20:11.440 --> 00:20:13.720]   You buy as many as you want to look at.
[00:20:13.720 --> 00:20:14.720]   You look at them at home.
[00:20:14.720 --> 00:20:17.280]   You don't have to look at them in the store.
[00:20:17.280 --> 00:20:21.760]   They talk about a Burlington shopping mall which used to have 100 stores now has 20.
[00:20:21.760 --> 00:20:22.760]   Yeah.
[00:20:22.760 --> 00:20:24.880]   Just 20 tenants.
[00:20:24.880 --> 00:20:27.240]   Last Wednesday, a woman came to the mall looking for shoes but left frustrated because
[00:20:27.240 --> 00:20:29.160]   the payless store had just shuttered.
[00:20:29.160 --> 00:20:30.160]   Right.
[00:20:30.160 --> 00:20:35.160]   Two years ago, the mall's owner has announced a $230 million renovation but the plans have
[00:20:35.160 --> 00:20:36.160]   stalled.
[00:20:36.160 --> 00:20:39.720]   You know, I mean, I can't say I'm sad that malls are going out of business because,
[00:20:39.720 --> 00:20:44.680]   you know, but I'm sad about all the lost jobs and the lost sense of community.
[00:20:44.680 --> 00:20:47.080]   Pretty soon, nobody's going to even leave the house anymore.
[00:20:47.080 --> 00:20:48.080]   Right.
[00:20:48.080 --> 00:20:49.720]   Why would you need to?
[00:20:49.720 --> 00:20:51.440]   You've got a robot bringing you food.
[00:20:51.440 --> 00:20:52.440]   Right.
[00:20:52.440 --> 00:20:55.960]   Drones are bringing all your groceries to you.
[00:20:55.960 --> 00:21:01.440]   The, by the way, I on Twitter yesterday I asked about what you'd find at the zombie shopping
[00:21:01.440 --> 00:21:03.000]   mall.
[00:21:03.000 --> 00:21:06.640]   So forever dead at 21.
[00:21:06.640 --> 00:21:09.080]   Victim on a stick.
[00:21:09.080 --> 00:21:13.760]   Aber crumbling and Fitch was from my friend Phil, coffin and barrel.
[00:21:13.760 --> 00:21:15.560]   That's a good one.
[00:21:15.560 --> 00:21:17.360]   I love it.
[00:21:17.360 --> 00:21:22.440]   There are a lot of sax fifth circle of hell was gone.
[00:21:22.440 --> 00:21:28.120]   In a bun from Tampa Sasser, I like that one and of course, the Microsoft store.
[00:21:28.120 --> 00:21:29.120]   Anyway.
[00:21:29.120 --> 00:21:30.120]   Ow.
[00:21:30.120 --> 00:21:31.120]   Zing.
[00:21:31.120 --> 00:21:34.800]   The last one has to be a real one, right?
[00:21:34.800 --> 00:21:36.600]   That's the joke right there.
[00:21:36.600 --> 00:21:38.800]   It can be Nordstrom if you prefer.
[00:21:38.800 --> 00:21:39.800]   But yeah, zombie shop.
[00:21:39.800 --> 00:21:40.800]   It is scary, right?
[00:21:40.800 --> 00:21:47.200]   This is when we talk about like the future of work being impacted by automating things.
[00:21:47.200 --> 00:21:51.400]   Before we even get to that, we're going to have the future of these whole sectors like
[00:21:51.400 --> 00:21:54.840]   retail where just the convenience of online shopping.
[00:21:54.840 --> 00:21:56.400]   This is not the future.
[00:21:56.400 --> 00:22:00.440]   And where there is retail success, it is at Wal-Mart's and Costco's and other kind of
[00:22:00.440 --> 00:22:06.320]   like big box stores, targets that are not the mall where it's like trying to replicate
[00:22:06.320 --> 00:22:10.360]   the old downtown of a city where there are 50 different shops.
[00:22:10.360 --> 00:22:13.560]   Now that's also kind of peeling away.
[00:22:13.560 --> 00:22:15.280]   I love this story.
[00:22:15.280 --> 00:22:21.360]   I'm going to jump all over today because you guys are playful so we can have some fun.
[00:22:21.360 --> 00:22:23.400]   The Dark Overlord.
[00:22:23.400 --> 00:22:30.760]   The Dark Overlord has apparently hacked the production company that had oranges, the new
[00:22:30.760 --> 00:22:37.600]   black and a bunch of shows from ABC, Fox, National Geographic and IFC.
[00:22:37.600 --> 00:22:40.480]   And they tried to blackmail Netflix.
[00:22:40.480 --> 00:22:47.160]   They said we have the next season of oranges, the new black, which comes out in June.
[00:22:47.160 --> 00:22:49.240]   And if you don't pay us, we're going to put it on the torrents.
[00:22:49.240 --> 00:22:50.640]   Well, Netflix just laughed at them.
[00:22:50.640 --> 00:22:53.240]   So they put it on the torrents on yesterday.
[00:22:53.240 --> 00:22:57.880]   Netflix was like, we make house cards, we know how black mail works.
[00:22:57.880 --> 00:22:59.560]   You can't do this.
[00:22:59.560 --> 00:23:00.560]   Orange is a new black.
[00:23:00.560 --> 00:23:03.120]   According to Netflix is their most popular Netflix original.
[00:23:03.120 --> 00:23:04.120]   I didn't know that.
[00:23:04.120 --> 00:23:09.240]   They're very cagey about giving out numbers for each of their individual shows.
[00:23:09.240 --> 00:23:13.520]   So that is actually a real key piece of information for the industry.
[00:23:13.520 --> 00:23:19.280]   According to this, according to Variety, the studio is a ADR, which is something dialogue
[00:23:19.280 --> 00:23:22.360]   recording, additional dialogue recording, Larson Studios.
[00:23:22.360 --> 00:23:25.600]   They got hacked in the fall.
[00:23:25.600 --> 00:23:30.440]   But the hacker doesn't have the whole season, just the first 10 episodes.
[00:23:30.440 --> 00:23:32.920]   So Netflix is going, yeah, that's great.
[00:23:32.920 --> 00:23:34.520]   It's a great promo.
[00:23:34.520 --> 00:23:37.000]   You still have to subscribe to Netflix to find out what happens.
[00:23:37.000 --> 00:23:38.840]   Well, you're not going to get that season finale.
[00:23:38.840 --> 00:23:40.720]   You got to subscribe, find out.
[00:23:40.720 --> 00:23:44.560]   These hackers have really good tastes because I feel like Orange is the new black doesn't
[00:23:44.560 --> 00:23:47.800]   get as much talk as much as like house cards or something.
[00:23:47.800 --> 00:23:49.840]   Yeah, no, you're right.
[00:23:49.840 --> 00:23:51.800]   This is a good hype man.
[00:23:51.800 --> 00:23:52.800]   It's a great series.
[00:23:52.800 --> 00:23:54.320]   I bet you it's an inside job.
[00:23:54.320 --> 00:23:55.680]   What a great marketing campaign by an app.
[00:23:55.680 --> 00:23:59.320]   So now you're telling me that Doc Overlord is actually a Netflix employee trying to bait
[00:23:59.320 --> 00:24:00.320]   up.
[00:24:00.320 --> 00:24:02.960]   Well, first of all, first of all, it's obviously read Hastings guys.
[00:24:02.960 --> 00:24:04.720]   It's obviously read Hastings, the dark overlord.
[00:24:04.720 --> 00:24:07.760]   He calls himself that in executive meetings.
[00:24:07.760 --> 00:24:11.600]   What kind of hacker names himself the dark overlord?
[00:24:11.600 --> 00:24:13.600]   I mean, a very young one.
[00:24:13.600 --> 00:24:15.960]   This is a 14 year old.
[00:24:15.960 --> 00:24:17.600]   Definitely a kid.
[00:24:17.600 --> 00:24:21.200]   And by the way, a kid of somebody who works at that ADR house, right?
[00:24:21.200 --> 00:24:22.200]   Yeah.
[00:24:22.200 --> 00:24:23.200]   That's what we're all going to be.
[00:24:23.200 --> 00:24:24.200]   Something don't be.
[00:24:24.200 --> 00:24:25.200]   Something like that.
[00:24:25.200 --> 00:24:29.600]   Yeah, his Twitter handle has a three in it for the in the word hacker.
[00:24:29.600 --> 00:24:30.640]   So that's that's neat.
[00:24:30.640 --> 00:24:31.640]   He's late.
[00:24:31.640 --> 00:24:36.600]   So you know, he's really anyway, the deadline passed.
[00:24:36.600 --> 00:24:40.040]   He posted it and then he did a press release.
[00:24:40.040 --> 00:24:41.840]   He called it a press release.
[00:24:41.840 --> 00:24:42.840]   Oh, that's a dog.
[00:24:42.840 --> 00:24:43.840]   He called it a press.
[00:24:43.840 --> 00:24:44.840]   It's so cute.
[00:24:44.840 --> 00:24:45.840]   He's so adorable.
[00:24:45.840 --> 00:24:49.360]   He called let's let's go to the dark overlord hacker.
[00:24:49.360 --> 00:24:51.360]   No, this isn't it.
[00:24:51.360 --> 00:24:55.480]   I'm trying to find there's apparently many dark overlords on Twitter.
[00:24:55.480 --> 00:24:57.240]   There's so many.
[00:24:57.240 --> 00:25:03.440]   I'm surprised he needed he didn't was like the dark overlord 46.
[00:25:03.440 --> 00:25:05.960]   Oh, this is terrible.
[00:25:05.960 --> 00:25:09.720]   Anyway, I'm trying to find the dark overlord press release because it's the funniest thing
[00:25:09.720 --> 00:25:13.360]   I ever read.
[00:25:13.360 --> 00:25:15.320]   Let me see if I could find it somewhere.
[00:25:15.320 --> 00:25:16.960]   Can you find it, Carsten?
[00:25:16.960 --> 00:25:21.200]   Because he he's kind of is it the one where he says it didn't have to be this way.
[00:25:21.200 --> 00:25:22.840]   It didn't have to be this way.
[00:25:22.840 --> 00:25:23.840]   That was my favorite.
[00:25:23.840 --> 00:25:26.880]   He says he says it didn't have to be this way that flicks.
[00:25:26.880 --> 00:25:30.200]   Wait, can I can I just make you be dark overlord for his Ashley?
[00:25:30.200 --> 00:25:31.200]   It's dark overlord guys.
[00:25:31.200 --> 00:25:33.200]   I'll be our cover alert.
[00:25:33.200 --> 00:25:35.000]   You have to be this way Netflix.
[00:25:35.000 --> 00:25:38.680]   You're going to use a lot of money and all of this.
[00:25:38.680 --> 00:25:40.400]   What our modest offer was.
[00:25:40.400 --> 00:25:42.600]   We're quite ashamed to breathe this game.
[00:25:42.600 --> 00:25:43.600]   Eric you wait a minute.
[00:25:43.600 --> 00:25:47.040]   We're quite ashamed to breathe the same air as you.
[00:25:47.040 --> 00:25:48.840]   Show is changed.
[00:25:48.840 --> 00:25:51.840]   We figured a pragmatic business such as yourself.
[00:25:51.840 --> 00:25:56.280]   It's seen and understand the benefits of cooperating with a reasonable and merciful entity like
[00:25:56.280 --> 00:26:00.960]   ourselves merciful that we are the merciful dark overlord.
[00:26:00.960 --> 00:26:04.960]   And I really love the end where he says and to the others, there's still time to save
[00:26:04.960 --> 00:26:10.240]   yourselves are still on the table for now.
[00:26:10.240 --> 00:26:18.400]   I'll tell you what if you know that if I ever do something like this, I am going to be
[00:26:18.400 --> 00:26:21.680]   the dark overlord so that attribution just becomes a disaster.
[00:26:21.680 --> 00:26:25.000]   Like if you're going to do this, you do not want people tracking you down.
[00:26:25.000 --> 00:26:29.760]   You do not want this is really good way to obfuscate the people tracking you down because
[00:26:29.760 --> 00:26:34.200]   they're going to go like like hitting Netflix is going to be like bad news bears.
[00:26:34.200 --> 00:26:35.200]   They can come after you.
[00:26:35.200 --> 00:26:36.200]   Right.
[00:26:36.200 --> 00:26:37.200]   There's some smart people at Netflix.
[00:26:37.200 --> 00:26:38.200]   I'm your doctor.
[00:26:38.200 --> 00:26:46.200]   I'm going to get my braces off in six months and then you'll be sorry.
[00:26:46.200 --> 00:26:49.200]   A piece you want to rest to me before price.
[00:26:49.200 --> 00:26:50.200]   Okay.
[00:26:50.200 --> 00:26:52.640]   Didn't we see this person in social media?
[00:26:52.640 --> 00:26:54.800]   The person with the raven on the subway.
[00:26:54.800 --> 00:26:58.000]   Pretty sure that's the person with the live raven.
[00:26:58.000 --> 00:26:59.000]   Yeah.
[00:26:59.000 --> 00:27:03.600]   Oh, I need to get my hoodie and put my hoodie on like Mr. Robot.
[00:27:03.600 --> 00:27:05.100]   Yeah.
[00:27:05.100 --> 00:27:08.020]   And Mr. Robot didn't call him some of the dark overlord.
[00:27:08.020 --> 00:27:09.020]   Come on, kid.
[00:27:09.020 --> 00:27:10.020]   Come on.
[00:27:10.020 --> 00:27:12.020]   Get a good handle.
[00:27:12.020 --> 00:27:14.460]   Get a good handle dark overlord.
[00:27:14.460 --> 00:27:15.460]   I love that.
[00:27:15.460 --> 00:27:16.460]   That's so difficult.
[00:27:16.460 --> 00:27:20.140]   You know, I play right now like if I could give you some advice dark overlord you're
[00:27:20.140 --> 00:27:26.180]   watching do not attack HBO because if you take Game of Thrones, they will they will
[00:27:26.180 --> 00:27:28.780]   literally hire Liam Neeson to find you.
[00:27:28.780 --> 00:27:32.380]   Like it will be the plot of taking the next taken movie.
[00:27:32.380 --> 00:27:33.380]   Yeah.
[00:27:33.380 --> 00:27:34.380]   Very serious.
[00:27:34.380 --> 00:27:39.300]   Here's the I found the dark overlord who's apparently a Vincent van Gogh fan.
[00:27:39.300 --> 00:27:40.220]   Oh, big time.
[00:27:40.220 --> 00:27:41.700]   He says yeah.
[00:27:41.700 --> 00:27:45.060]   And he and I was no copyright of Vincent van Gogh.
[00:27:45.060 --> 00:27:47.420]   Oh, he's very he doesn't want to.
[00:27:47.420 --> 00:27:49.340]   Yeah, doesn't want to offend.
[00:27:49.340 --> 00:27:50.340]   Yeah.
[00:27:50.340 --> 00:27:51.340]   Oh, yeah.
[00:27:51.340 --> 00:27:52.340]   Oh, it's amazing.
[00:27:52.340 --> 00:27:53.340]   It would not.
[00:27:53.340 --> 00:28:00.140]   Actually, you do that voice very well.
[00:28:00.140 --> 00:28:02.700]   You will get to play the dark overlord when we make it.
[00:28:02.700 --> 00:28:03.700]   Oh my God.
[00:28:03.700 --> 00:28:07.780]   And the motion picture the pirate of Silicon Valley the dark overlord can be the villain
[00:28:07.780 --> 00:28:08.780]   in that.
[00:28:08.780 --> 00:28:09.780]   Awesome.
[00:28:09.780 --> 00:28:10.780]   Awesome.
[00:28:10.780 --> 00:28:13.140]   I love Ashley's sketches.
[00:28:13.140 --> 00:28:18.940]   Dark overlord that to me that may be one of the best of all and by the way, just to,
[00:28:18.940 --> 00:28:21.100]   you know, kind of circle around.
[00:28:21.100 --> 00:28:25.960]   No dark overlord did not take his or her revenge on Ashley.
[00:28:25.960 --> 00:28:28.420]   She she survived unscathed.
[00:28:28.420 --> 00:28:29.420]   Unescathed.
[00:28:29.420 --> 00:28:32.780]   Our show today brought to you by GoToWebinar.
[00:28:32.780 --> 00:28:36.020]   We're really pleased we were, you know, we always do these best stuffs, but often we
[00:28:36.020 --> 00:28:39.540]   don't have advertisers, but GoToWebinar came to us right at the last moment and said,
[00:28:39.540 --> 00:28:41.540]   you know, we'd like to buy the best stuff.
[00:28:41.540 --> 00:28:43.140]   So they're sponsoring all the best stuffs.
[00:28:43.140 --> 00:28:44.140]   Thank you.
[00:28:44.140 --> 00:28:45.140]   GoToWebinar.
[00:28:45.140 --> 00:28:46.140]   GoToWebinar.
[00:28:46.140 --> 00:28:47.140]   It's great.
[00:28:47.140 --> 00:28:48.140]   I've used it.
[00:28:48.140 --> 00:28:49.140]   I've made done a number of webinars with GoToWebinar.
[00:28:49.140 --> 00:28:50.140]   I'm not alone.
[00:28:50.140 --> 00:28:55.340]   They've hosted 2.7 million interactive web events this year.
[00:28:55.340 --> 00:28:56.940]   That's 60 million views.
[00:28:56.940 --> 00:28:57.940]   That's one of the things that's cool.
[00:28:57.940 --> 00:28:59.380]   What is that average then?
[00:28:59.380 --> 00:29:03.460]   More than 20 people per webinar, but it can be even bigger.
[00:29:03.460 --> 00:29:08.940]   GoToWebinar lets you present from one to six presenters as many people who can see it
[00:29:08.940 --> 00:29:10.980]   as you want, both live and on demand.
[00:29:10.980 --> 00:29:15.340]   And that's really, really you get the, you know, all the big audiences is you create
[00:29:15.340 --> 00:29:18.180]   a live webinar with live audience.
[00:29:18.180 --> 00:29:22.620]   You have polls, 20 polls up to 20 polls per webinar up to 20 questions per poll.
[00:29:22.620 --> 00:29:23.820]   You can set them up ahead of time.
[00:29:23.820 --> 00:29:25.220]   You can do it in the middle of the webinar.
[00:29:25.220 --> 00:29:30.140]   If you don't understand or you're worried that people are, you know, you just want to
[00:29:30.140 --> 00:29:33.500]   know more about your audience, whatever, you do that during the live webinar.
[00:29:33.500 --> 00:29:39.180]   But the live webinar is recorded and the polls are live and the numbers are live on
[00:29:39.180 --> 00:29:40.860]   the, on demand version.
[00:29:40.860 --> 00:29:41.940]   So that's really neat.
[00:29:41.940 --> 00:29:46.620]   People can still interact with it, still feel like it's kept the excitement of a live event,
[00:29:46.620 --> 00:29:48.220]   but you don't have to be there.
[00:29:48.220 --> 00:29:53.700]   You can do this webinar once and watch millions of people see it after the fact.
[00:29:53.700 --> 00:29:58.460]   It can be completely branded with your company's logo and custom image and all the webinar
[00:29:58.460 --> 00:29:59.460]   materials.
[00:29:59.460 --> 00:30:00.940]   Easy to start to you.
[00:30:00.940 --> 00:30:05.020]   They have automated mail templates that let you send out custom email invitations and
[00:30:05.020 --> 00:30:06.980]   confirmations and reminders.
[00:30:06.980 --> 00:30:08.660]   That's really important.
[00:30:08.660 --> 00:30:09.660]   Mobile friendly, that's right.
[00:30:09.660 --> 00:30:12.580]   You can do this by the way on your phone, iOS or Android.
[00:30:12.580 --> 00:30:16.940]   And you can only schedule a webinar, set it up, but you can edit a session and track
[00:30:16.940 --> 00:30:20.380]   performance, speaking of tracking, they have lots of metrics.
[00:30:20.380 --> 00:30:24.260]   So it's a great way if you're doing sales presentations to gather leads, if you're
[00:30:24.260 --> 00:30:28.220]   teaching to get a sense of how many students attended the whole thing.
[00:30:28.220 --> 00:30:30.580]   And actually that's very helpful in making your webinar better.
[00:30:30.580 --> 00:30:31.580]   Were they paying attention?
[00:30:31.580 --> 00:30:35.780]   Did they stick around so that the next time you even get more engagement?
[00:30:35.780 --> 00:30:37.620]   I'm a fan of GoToWebinar.
[00:30:37.620 --> 00:30:39.020]   I want you to try it.
[00:30:39.020 --> 00:30:44.380]   Go to webinar.com/podcast is the special address that way they know you saw it on
[00:30:44.380 --> 00:30:45.580]   our best of episodes.
[00:30:45.580 --> 00:30:49.540]   Go to webinar.com/podcast.
[00:30:49.540 --> 00:30:54.180]   Thank you GoToWebinar for making this New Year's Eve presentation even a little bit
[00:30:54.180 --> 00:30:56.220]   more special.
[00:30:56.220 --> 00:30:58.500]   Pop the champagne, folks.
[00:30:58.500 --> 00:31:01.780]   Mark Zuckerberg, running for president.
[00:31:01.780 --> 00:31:05.580]   Imagine yourself, a small town, Newton Falls, Ohio.
[00:31:05.580 --> 00:31:06.580]   Nothing ever happens exciting.
[00:31:06.580 --> 00:31:11.180]   You get a phone call from a company saying a billionaire philanthropist wants to have
[00:31:11.180 --> 00:31:12.380]   dinner with you next week.
[00:31:12.380 --> 00:31:13.540]   I can't tell you his name.
[00:31:13.540 --> 00:31:18.020]   In fact, I won't tell you his name until 15 minutes before he arrives, would you be
[00:31:18.020 --> 00:31:19.020]   willing to?
[00:31:19.020 --> 00:31:26.020]   Somebody said yes, and here's the picture of Mark Zuckerberg meeting real people.
[00:31:26.020 --> 00:31:32.940]   The grandmother to his left looks like, "Oh, God, what a nightmare."
[00:31:32.940 --> 00:31:36.140]   Anytime you see this picture, you assume she's looking at a smartphone, but there aren't
[00:31:36.140 --> 00:31:37.820]   any smartphones in evidence.
[00:31:37.820 --> 00:31:42.900]   They're eating on plastic plates because apparently Mark won't eat real people's food, so they
[00:31:42.900 --> 00:31:43.900]   had it catered.
[00:31:43.900 --> 00:31:45.700]   Is that why?
[00:31:45.700 --> 00:31:46.700]   Yeah.
[00:31:46.700 --> 00:31:48.740]   Did they really have a catered?
[00:31:48.740 --> 00:31:49.740]   Oh, my gosh.
[00:31:49.740 --> 00:31:58.740]   And the article, this is in Business Insider said that Mark's people were tapping away
[00:31:58.740 --> 00:32:00.820]   at their laptops in the other room.
[00:32:00.820 --> 00:32:02.380]   He brings it on to Raj.
[00:32:02.380 --> 00:32:03.940]   Of course, he'd be, yeah.
[00:32:03.940 --> 00:32:04.940]   So here's the thing.
[00:32:04.940 --> 00:32:08.100]   Okay, I really want to ask the panel here.
[00:32:08.100 --> 00:32:12.660]   I wrote a piece, I don't know, a few months ago, saying, "It sure as heck looks like Mark
[00:32:12.660 --> 00:32:14.740]   Zuckerberg's running for president."
[00:32:14.740 --> 00:32:18.420]   And there were some people that were absolutely, when you look at all these photos and there
[00:32:18.420 --> 00:32:23.700]   were like, "You're completely wrong," the old these photos, what is going on?
[00:32:23.700 --> 00:32:24.700]   Like, what is he doing?
[00:32:24.700 --> 00:32:28.220]   And don't tell me he's just going around the country meeting people because if he was doing
[00:32:28.220 --> 00:32:30.140]   that, why would he be the photographer?
[00:32:30.140 --> 00:32:36.540]   He specifically said to this family, "If when the press calls you and they will, please
[00:32:36.540 --> 00:32:38.740]   tell them I'm not running for president."
[00:32:38.740 --> 00:32:43.300]   So, yeah, I don't think he's old enough to run for president, is he?
[00:32:43.300 --> 00:32:45.380]   Yes, so he would be when he...
[00:32:45.380 --> 00:32:46.380]   In 2020, he will be.
[00:32:46.380 --> 00:32:50.820]   Turns 35 one month before the filing deadline for 2020.
[00:32:50.820 --> 00:32:56.300]   I mean, I think it's possible, I think it's highly improbable.
[00:32:56.300 --> 00:32:57.300]   Really?
[00:32:57.300 --> 00:32:58.300]   No, no.
[00:32:58.300 --> 00:32:59.540]   Why is he doing this then?
[00:32:59.540 --> 00:33:00.940]   Why is he doing this also?
[00:33:00.940 --> 00:33:03.420]   I think he's maybe eventually doing that.
[00:33:03.420 --> 00:33:08.900]   They've got a massive press problem on their hands right now with fake news and...
[00:33:08.900 --> 00:33:13.260]   Does this make people feel better about fake news to see Mark sitting at this table?
[00:33:13.260 --> 00:33:15.260]   With catered food.
[00:33:15.260 --> 00:33:16.260]   I mean, really?
[00:33:16.260 --> 00:33:21.020]   I think he's not running for in this cycle.
[00:33:21.020 --> 00:33:22.380]   He's not running in 2020.
[00:33:22.380 --> 00:33:24.780]   Unlike by theory, he's driving a drag.
[00:33:24.780 --> 00:33:25.780]   Oh, please.
[00:33:25.780 --> 00:33:31.580]   I have to tell you, I feel like I have a special point of view on this because I'm out there,
[00:33:31.580 --> 00:33:32.980]   in fact, right before I got here.
[00:33:32.980 --> 00:33:34.580]   I'm out there in Hall, Massachusetts.
[00:33:34.580 --> 00:33:35.580]   You're doing this.
[00:33:35.580 --> 00:33:37.220]   I'm doing this.
[00:33:37.220 --> 00:33:44.140]   And the thought of inviting myself over to someone's house is so deeply uncomfortable.
[00:33:44.140 --> 00:33:49.940]   And if you want to meet people, this is a completely wrong, really weird way to go
[00:33:49.940 --> 00:33:50.940]   about it.
[00:33:50.940 --> 00:33:56.020]   I have people that write me every week and I go, "Do you want to have coffee?"
[00:33:56.020 --> 00:33:59.220]   And that's sitting down in private and talking.
[00:33:59.220 --> 00:34:02.100]   It's not making it this weird media thing.
[00:34:02.100 --> 00:34:03.100]   It's just...
[00:34:03.100 --> 00:34:05.100]   It's clear he brings photographers, lights.
[00:34:05.100 --> 00:34:08.740]   Look at this picture talking to a fire department.
[00:34:08.740 --> 00:34:12.420]   This looks so canned, phony, and staged.
[00:34:12.420 --> 00:34:13.420]   He doesn't...
[00:34:13.420 --> 00:34:16.420]   It looks like he's been green-screened into this.
[00:34:16.420 --> 00:34:17.420]   I...
[00:34:17.420 --> 00:34:18.420]   Okay.
[00:34:18.420 --> 00:34:21.100]   The better question is, though, is it...
[00:34:21.100 --> 00:34:27.420]   So, Brianna, you're asking people if they want to have coffee with you.
[00:34:27.420 --> 00:34:29.300]   Wouldn't Zuck just tell us?
[00:34:29.300 --> 00:34:33.220]   We're going to have coffee with him through the miracles of AI and machine learning.
[00:34:33.220 --> 00:34:37.420]   And why bother going out into the real world when you have everybody's data, your fingertips?
[00:34:37.420 --> 00:34:38.700]   I think you're both right.
[00:34:38.700 --> 00:34:39.700]   I think...
[00:34:39.700 --> 00:34:40.700]   Wait, wait, wait.
[00:34:40.700 --> 00:34:41.700]   Hold on.
[00:34:41.700 --> 00:34:43.820]   I think there's evidence here to a politically...
[00:34:43.820 --> 00:34:45.300]   A political campaign.
[00:34:45.300 --> 00:34:53.300]   So they updated their S1 filing with the SEC to note that if Mark Zuckerberg decides to
[00:34:53.300 --> 00:34:58.060]   run for office and he could still remain CEO, why would you put that in there if that didn't
[00:34:58.060 --> 00:34:59.060]   mean anything?
[00:34:59.060 --> 00:35:05.220]   They've been hiring people from the Obama campaign on the foundation.
[00:35:05.220 --> 00:35:06.980]   There's just all these things that he's been doing.
[00:35:06.980 --> 00:35:10.580]   And these photos, it doesn't add up.
[00:35:10.580 --> 00:35:15.060]   You know, there's people that have been telling me, telling me, "Oh, well, it's just a PR push."
[00:35:15.060 --> 00:35:21.300]   But it's giving them more negative PR around this whole president thing if the goal is
[00:35:21.300 --> 00:35:22.300]   to try to do it.
[00:35:22.300 --> 00:35:23.300]   What the hell does he want to be president for, though?
[00:35:23.300 --> 00:35:24.300]   I mean, honestly.
[00:35:24.300 --> 00:35:25.300]   So, I have a theory.
[00:35:25.300 --> 00:35:28.980]   I don't know if it's president because I do think that it's too small of a job for him.
[00:35:28.980 --> 00:35:29.980]   I wonder if...
[00:35:29.980 --> 00:35:31.660]   He wants to be emperor of the world?
[00:35:31.660 --> 00:35:32.660]   He wants to be emperor.
[00:35:32.660 --> 00:35:34.740]   What's bigger than president of the United States?
[00:35:34.740 --> 00:35:38.740]   No, maybe he's going to try out for governor or even mayor.
[00:35:38.740 --> 00:35:39.740]   I don't know.
[00:35:39.740 --> 00:35:41.740]   He's kind of testing around.
[00:35:41.740 --> 00:35:42.740]   But there's something going on.
[00:35:42.740 --> 00:35:43.980]   There's clearly something going on.
[00:35:43.980 --> 00:35:45.700]   I think Amy, you're right.
[00:35:45.700 --> 00:35:46.700]   There is a question.
[00:35:46.700 --> 00:35:50.180]   In fact, many of our panelists have said this in the past.
[00:35:50.180 --> 00:35:52.020]   He's already got a very...
[00:35:52.020 --> 00:35:53.580]   Probably he's a king maker.
[00:35:53.580 --> 00:35:54.820]   He doesn't need to be the king.
[00:35:54.820 --> 00:35:56.900]   Always better to be the king maker.
[00:35:56.900 --> 00:35:59.780]   The ratio you have rather than...
[00:35:59.780 --> 00:36:03.420]   Because then you have more power, frankly, the king maker.
[00:36:03.420 --> 00:36:06.460]   However, so I don't think he's running in 2020.
[00:36:06.460 --> 00:36:08.340]   I think, though, that this is a...
[00:36:08.340 --> 00:36:12.140]   He's playing the long game that at some point he would...
[00:36:12.140 --> 00:36:13.140]   Look at this.
[00:36:13.140 --> 00:36:15.540]   You don't do that because of fake news.
[00:36:15.540 --> 00:36:20.620]   You don't feed a cow.
[00:36:20.620 --> 00:36:22.120]   I don't know.
[00:36:22.120 --> 00:36:27.460]   I got to say, because I'm...
[00:36:27.460 --> 00:36:32.100]   We advise many, many news organizations and also not this administration but the previous
[00:36:32.100 --> 00:36:33.100]   administration.
[00:36:33.100 --> 00:36:35.900]   He's got a pretty massive problem on his hands.
[00:36:35.900 --> 00:36:40.860]   If every news organization in the country, in our country, decided to cut off the tap,
[00:36:40.860 --> 00:36:42.820]   that would be a tremendous problem.
[00:36:42.820 --> 00:36:44.220]   They're doing it, right?
[00:36:44.220 --> 00:36:46.460]   Instant articles has lost the New York Times.
[00:36:46.460 --> 00:36:48.780]   They've lost the Washington Post.
[00:36:48.780 --> 00:36:53.580]   There may be part of this, maybe politics, but I also think that there's a piece of this
[00:36:53.580 --> 00:36:54.820]   that's a PR.
[00:36:54.820 --> 00:36:59.220]   Amy, how does this help fake news or instant articles?
[00:36:59.220 --> 00:37:01.660]   I don't understand how this helps anything.
[00:37:01.660 --> 00:37:04.060]   You know how to connect the dots?
[00:37:04.060 --> 00:37:05.060]   Yeah.
[00:37:05.060 --> 00:37:06.060]   I don't think that it...
[00:37:06.060 --> 00:37:07.060]   This is your job, Amy.
[00:37:07.060 --> 00:37:09.340]   This is what you do.
[00:37:09.340 --> 00:37:11.780]   You're predicting the future for us.
[00:37:11.780 --> 00:37:16.380]   Here's the thing.
[00:37:16.380 --> 00:37:22.540]   My hunch here is that we're also easily distracted no matter how smart we are and how prestigious
[00:37:22.540 --> 00:37:23.660]   our jobs might be.
[00:37:23.660 --> 00:37:26.300]   We get distracted over and over again.
[00:37:26.300 --> 00:37:27.820]   My first career was as a journalist.
[00:37:27.820 --> 00:37:31.020]   This is not about me just liking journalists or anything like that.
[00:37:31.020 --> 00:37:32.900]   I just think we get distracted.
[00:37:32.900 --> 00:37:41.860]   I think that campaign, whether it was for politics or for straight up, you know, graciating
[00:37:41.860 --> 00:37:48.900]   Zuck and Facebook back into everybody's good graces, I think was more about...
[00:37:48.900 --> 00:37:52.380]   Was a lot about likability.
[00:37:52.380 --> 00:37:54.300]   I think that that has a reverberating effect.
[00:37:54.300 --> 00:37:55.300]   I really do.
[00:37:55.300 --> 00:37:58.180]   I think you have to look at the big picture here too.
[00:37:58.180 --> 00:38:05.180]   I mean, look, he recently gave away a huge portion of his wealth for philanthropy worldwide.
[00:38:05.180 --> 00:38:08.180]   Well, I'm a sort of...
[00:38:08.180 --> 00:38:09.820]   I know it's quirky.
[00:38:09.820 --> 00:38:11.100]   I know it's quirky.
[00:38:11.100 --> 00:38:16.580]   But my point there is he's trying to raise an international presence here.
[00:38:16.580 --> 00:38:23.460]   When he has a positive view by the mass public, I think it enables him to do a lot of things.
[00:38:23.460 --> 00:38:27.020]   Like Facebook is trying to work with local governments.
[00:38:27.020 --> 00:38:29.180]   It's a huge part of what they end up doing.
[00:38:29.180 --> 00:38:34.540]   So I see this as an overall political move meant to open many doors for him.
[00:38:34.540 --> 00:38:37.100]   And maybe he runs for president, maybe he doesn't.
[00:38:37.100 --> 00:38:41.300]   But it certainly makes a lot of the things Facebook wants to do just easier.
[00:38:41.300 --> 00:38:45.820]   So I would mention to Mark's team that just remember Mike Dukakis in the tank with the
[00:38:45.820 --> 00:38:50.860]   helmet, that picture with basketball players does not show it again.
[00:38:50.860 --> 00:38:52.660]   Does not raise Sucks Thatcher.
[00:38:52.660 --> 00:38:53.660]   That's not...
[00:38:53.660 --> 00:38:55.900]   That is probably the one you want to throw out.
[00:38:55.900 --> 00:38:57.900]   Just saying.
[00:38:57.900 --> 00:38:58.900]   Also, I think...
[00:38:58.900 --> 00:38:59.900]   Here's the thing.
[00:38:59.900 --> 00:39:05.420]   Let's just pretend that the four of us run Mark's public relations team.
[00:39:05.420 --> 00:39:06.420]   Can you imagine...
[00:39:06.420 --> 00:39:10.140]   Oh, guys, there's been this huge problem with fake news.
[00:39:10.140 --> 00:39:11.140]   What do we do?
[00:39:11.140 --> 00:39:12.140]   And someone says, "I know.
[00:39:12.140 --> 00:39:15.140]   Let's go take a picture of Mark feeding a cow."
[00:39:15.140 --> 00:39:17.540]   Well, remember that Mark...
[00:39:17.540 --> 00:39:19.180]   And wait a minute, but wait, Nick.
[00:39:19.180 --> 00:39:22.020]   But how many different places did we all see that photo?
[00:39:22.020 --> 00:39:23.940]   I completely agree with you, but...
[00:39:23.940 --> 00:39:26.460]   Yeah, it worked in that respect.
[00:39:26.460 --> 00:39:27.460]   That's right.
[00:39:27.460 --> 00:39:28.460]   That's right.
[00:39:28.460 --> 00:39:29.460]   So he did...
[00:39:29.460 --> 00:39:32.060]   Mark does, as we know, these things.
[00:39:32.060 --> 00:39:36.100]   Like he was going to only cook and kill his own...
[00:39:36.100 --> 00:39:37.100]   He was only...
[00:39:37.100 --> 00:39:38.100]   He beat me.
[00:39:38.100 --> 00:39:39.100]   He was going to...
[00:39:39.100 --> 00:39:42.140]   He learned Chinese and actually did, which is kind of impressive.
[00:39:42.140 --> 00:39:47.060]   Having tried that for myself for four years in school, it's not easy.
[00:39:47.060 --> 00:39:50.540]   So one of his goals for this year was to go to all 50 states.
[00:39:50.540 --> 00:39:51.700]   Talk to people in all 50 states.
[00:39:51.700 --> 00:39:56.340]   So you could just say on the surface that he's just doing his weird kind of...
[00:39:56.340 --> 00:39:58.100]   I'm a billionaire, I could do anything I want.
[00:39:58.100 --> 00:40:01.700]   So instead of building a rocket ship, I'm going to meet everybody in every state.
[00:40:01.700 --> 00:40:02.700]   I don't know.
[00:40:02.700 --> 00:40:04.700]   I didn't see photos of him studying...
[00:40:04.700 --> 00:40:05.700]   Whatever.
[00:40:05.700 --> 00:40:06.700]   Not just getting verbs.
[00:40:06.700 --> 00:40:08.700]   He's like sitting at the house.
[00:40:08.700 --> 00:40:09.700]   Nihao.
[00:40:09.700 --> 00:40:10.700]   Nihao.
[00:40:10.700 --> 00:40:12.340]   Did you see photos of him killing his animals?
[00:40:12.340 --> 00:40:13.340]   No.
[00:40:13.340 --> 00:40:15.140]   So why are we seeing photos of this stuff?
[00:40:15.140 --> 00:40:16.140]   Let's pee.
[00:40:16.140 --> 00:40:17.140]   Okay.
[00:40:17.140 --> 00:40:18.140]   So we all agree it's PR.
[00:40:18.140 --> 00:40:19.140]   Oh, yes.
[00:40:19.140 --> 00:40:20.140]   Oh, look at median generals.
[00:40:20.140 --> 00:40:22.500]   No, that's important.
[00:40:22.500 --> 00:40:24.780]   You got to meet generals if you're going to...
[00:40:24.780 --> 00:40:27.780]   Is that an American general?
[00:40:27.780 --> 00:40:28.780]   What uniform is that?
[00:40:28.780 --> 00:40:30.980]   I've never seen that hat before.
[00:40:30.980 --> 00:40:31.980]   What is that?
[00:40:31.980 --> 00:40:32.980]   So here was the other thing.
[00:40:32.980 --> 00:40:34.580]   I do want to say one other thing.
[00:40:34.580 --> 00:40:42.460]   So when I written that Vanity Fair piece, BuzzFeed went along to Zuck, they got access
[00:40:42.460 --> 00:40:44.500]   to him and they said, "Are you running for president?"
[00:40:44.500 --> 00:40:45.940]   He said, "No."
[00:40:45.940 --> 00:40:47.100]   And then it was this big story.
[00:40:47.100 --> 00:40:48.900]   Like Zuck says he's not running for president.
[00:40:48.900 --> 00:40:52.060]   Of course he's not going to say he's running for president.
[00:40:52.060 --> 00:40:56.260]   Facebook stock would literally drop 20, 30 percent in five seconds.
[00:40:56.260 --> 00:41:02.300]   So I think that we should not take it as face value when he says no or don't tell people
[00:41:02.300 --> 00:41:03.940]   I'm running for president.
[00:41:03.940 --> 00:41:07.780]   There is something going on and we should be questioning it.
[00:41:07.780 --> 00:41:08.780]   So what do you want to start?
[00:41:08.780 --> 00:41:09.780]   You want to start with Google?
[00:41:09.780 --> 00:41:10.780]   Google I/O?
[00:41:10.780 --> 00:41:12.740]   Well, Google I/O was kind of the...
[00:41:12.740 --> 00:41:14.260]   It should have been the biggest news of the week.
[00:41:14.260 --> 00:41:15.580]   And it wasn't.
[00:41:15.580 --> 00:41:19.500]   And it really kind of...I think let a lot of people down.
[00:41:19.500 --> 00:41:21.340]   There's actually a lot of significant stuff.
[00:41:21.340 --> 00:41:25.780]   It just...they didn't really explain why the stuff mattered that good.
[00:41:25.780 --> 00:41:27.580]   Well, much other stuff wasn't available yet.
[00:41:27.580 --> 00:41:30.300]   Well, that's part of Google's...always Google's problem.
[00:41:30.300 --> 00:41:31.300]   Yeah.
[00:41:31.300 --> 00:41:35.900]   I think the Gizmodo's piece by Michael Nunez was the best that said that you just can't
[00:41:35.900 --> 00:41:38.260]   see that it's neater stuff, but you can't see it.
[00:41:38.260 --> 00:41:39.740]   You can't touch it.
[00:41:39.740 --> 00:41:43.580]   Well, and you know, in Google's defense, that's a good point.
[00:41:43.580 --> 00:41:49.960]   I mean, you know, if you're making big strides in artificial intelligence, well, that just
[00:41:49.960 --> 00:41:50.960]   kind of slips away.
[00:41:50.960 --> 00:41:54.900]   If that's one of the things Sundar Pachai said, he says, "We're moving from a mobile first
[00:41:54.900 --> 00:41:55.900]   world."
[00:41:55.900 --> 00:41:57.340]   By the way, I'm just catching up with mobile first.
[00:41:57.340 --> 00:41:58.340]   Yeah.
[00:41:58.340 --> 00:41:59.900]   A lot of companies are.
[00:41:59.900 --> 00:42:06.100]   We're moving from a mobile first world to a AI first world.
[00:42:06.100 --> 00:42:07.660]   And that's kind of interesting.
[00:42:07.660 --> 00:42:08.660]   Yeah.
[00:42:08.660 --> 00:42:10.940]   And well, that was kind of the sell last year for Google I/O.
[00:42:10.940 --> 00:42:12.140]   I thought they said the same thing.
[00:42:12.140 --> 00:42:13.140]   Yeah, you're right.
[00:42:13.140 --> 00:42:17.460]   You know, one of the things that sticks out to me is if you look at the complaints before
[00:42:17.460 --> 00:42:23.660]   Sundar took over as CEO about a year ago now, a lot of it was that you go to Google I/O
[00:42:23.660 --> 00:42:29.100]   and you see all these really fun G-Wiz kind of ideas that aren't necessarily fully realized
[00:42:29.100 --> 00:42:30.500]   that are costing a lot of money.
[00:42:30.500 --> 00:42:34.420]   Sundar came in with CEO and kind of focused the company, got rid of a lot of that sort
[00:42:34.420 --> 00:42:35.460]   of thing.
[00:42:35.460 --> 00:42:37.260]   And this is really what the company is focused on.
[00:42:37.260 --> 00:42:42.500]   And so this is iterative stuff versus last year in this AI first idea.
[00:42:42.500 --> 00:42:43.940]   And it's good progress.
[00:42:43.940 --> 00:42:49.820]   Like the cloud TPU setup is basically these computer chips that will help their machine
[00:42:49.820 --> 00:42:50.820]   learning system.
[00:42:50.820 --> 00:42:52.820]   Intense or processing units.
[00:42:52.820 --> 00:42:53.820]   Yeah.
[00:42:53.820 --> 00:42:54.820]   That's probably the biggest news of the event.
[00:42:54.820 --> 00:42:55.820]   This is exactly.
[00:42:55.820 --> 00:42:56.820]   It's a huge business implications.
[00:42:56.820 --> 00:42:57.820]   Fascinating.
[00:42:57.820 --> 00:43:02.060]   So these are inexpensive almost, they look like almost like Raspberry Pi.
[00:43:02.060 --> 00:43:06.420]   They got four processors on big heavy duty GPU stuff.
[00:43:06.420 --> 00:43:10.020]   This is no Raspberry Pi, 180 teraflops.
[00:43:10.020 --> 00:43:13.260]   And so this could give them an advantage in their cloud services versus Amazon.
[00:43:13.260 --> 00:43:15.340]   This could give them an advantage in building on their AI.
[00:43:15.340 --> 00:43:17.380]   They're also making available to third parties.
[00:43:17.380 --> 00:43:18.380]   Yeah.
[00:43:18.380 --> 00:43:19.540]   Part of the Google Cucut cloud.
[00:43:19.540 --> 00:43:21.540]   The one on the AI software.
[00:43:21.540 --> 00:43:24.100]   So they've already open sourced TensorFlow, the software.
[00:43:24.100 --> 00:43:29.300]   And these are hardware devices designed to run TensorFlow at speed.
[00:43:29.300 --> 00:43:34.340]   And if you're impressed with 180 teraflops, that's really not the point because the idea
[00:43:34.340 --> 00:43:39.900]   is they massively parallelize them.
[00:43:39.900 --> 00:43:49.500]   And these racks are multi-pedaflops per, this is 11.5 petaflops of machine learning in this
[00:43:49.500 --> 00:43:50.580]   rack.
[00:43:50.580 --> 00:43:56.860]   And so what I think the missing piece really was Sundar or somebody else from Google saying,
[00:43:56.860 --> 00:44:03.100]   "Hey, this is what the, here are the cool things we'll be able to build with this technology.
[00:44:03.100 --> 00:44:06.780]   Here's the stuff that you developers should come and help us build together.
[00:44:06.780 --> 00:44:09.860]   Regular people watching this, consumers and enthusiasts.
[00:44:09.860 --> 00:44:12.020]   Here's what the future is going to look like."
[00:44:12.020 --> 00:44:13.460]   And here's the vision, right?
[00:44:13.460 --> 00:44:19.620]   So Sundar's Google doesn't do as good of a job at illustrating that vision in these
[00:44:19.620 --> 00:44:27.180]   keynotes as, you know, let's face it, the tech journalists would rather have Sergey Brin
[00:44:27.180 --> 00:44:30.740]   parachuting in with Google goggles on, even though that was-
[00:44:30.740 --> 00:44:31.940]   You've got a better show.
[00:44:31.940 --> 00:44:34.020]   It's a better show, but equally meaningless.
[00:44:34.020 --> 00:44:35.020]   Exactly.
[00:44:35.020 --> 00:44:36.020]   Well, not even equally.
[00:44:36.020 --> 00:44:37.340]   Much more so.
[00:44:37.340 --> 00:44:38.340]   Much more meaningless.
[00:44:38.340 --> 00:44:39.540]   Yeah, genuinely meaningless.
[00:44:39.540 --> 00:44:40.540]   Yeah.
[00:44:40.540 --> 00:44:44.060]   So it's less exciting, but this is probably one of the most significant things Google's
[00:44:44.060 --> 00:44:45.060]   announced.
[00:44:45.060 --> 00:44:46.060]   And I think that's really right.
[00:44:46.060 --> 00:44:47.060]   It's enabling, right?
[00:44:47.060 --> 00:44:53.300]   You remember the name of that guy we talked about on the show for like an hour once was
[00:44:53.300 --> 00:44:55.420]   in Swiss or an Austrian AI genius?
[00:44:55.420 --> 00:44:56.420]   Yeah.
[00:44:56.420 --> 00:44:57.420]   Yeah.
[00:44:57.420 --> 00:44:58.420]   I can't remember his name either.
[00:44:58.420 --> 00:44:59.420]   Yeah.
[00:44:59.420 --> 00:45:00.420]   You're getting somebody.
[00:45:00.420 --> 00:45:03.900]   But he talked about how the theory is so far ahead of the practice because it was not
[00:45:03.900 --> 00:45:07.860]   the way you just said Nate is that the power to do what they want to do isn't there.
[00:45:07.860 --> 00:45:10.100]   So their theory is way ahead.
[00:45:10.100 --> 00:45:11.100]   And this-
[00:45:11.100 --> 00:45:12.100]   Oh, you're getting smitten.
[00:45:12.100 --> 00:45:13.100]   You're talking about you're getting smitten.
[00:45:13.100 --> 00:45:14.100]   You're getting smitten.
[00:45:14.100 --> 00:45:15.100]   Yes.
[00:45:15.100 --> 00:45:16.100]   Oh, yeah.
[00:45:16.100 --> 00:45:18.100]   Not the best website I've ever seen.
[00:45:18.100 --> 00:45:22.500]   See, I can't tell if this is like he hasn't updated his website since like 1996 or if it's
[00:45:22.500 --> 00:45:25.860]   like an ironic like I want it to look this way like in kind of like a hipster.
[00:45:25.860 --> 00:45:27.420]   Oh, click on what's new.
[00:45:27.420 --> 00:45:28.420]   Yeah.
[00:45:28.420 --> 00:45:29.420]   What's new?
[00:45:29.420 --> 00:45:30.420]   12 May 2017.
[00:45:30.420 --> 00:45:34.100]   I've also probably zoomed this in.
[00:45:34.100 --> 00:45:35.700]   That's what it's supposed to look like.
[00:45:35.700 --> 00:45:36.700]   That's much better.
[00:45:36.700 --> 00:45:41.100]   That'll zoom back in because I can't read it that size.
[00:45:41.100 --> 00:45:45.060]   But yeah, I mean, you know, like here's this opportunity.
[00:45:45.060 --> 00:45:46.220]   Sell us on the future.
[00:45:46.220 --> 00:45:51.780]   Like, tell us where this is all going and why building this together with you is so much
[00:45:51.780 --> 00:45:55.260]   better than putting your energies as a developer or your-
[00:45:55.260 --> 00:45:56.260]   Maybe not.
[00:45:56.260 --> 00:45:57.260]   Maybe not.
[00:45:57.260 --> 00:46:01.300]   Maybe the smartest thing for them just to shut up.
[00:46:01.300 --> 00:46:03.860]   But see, this is what Google doesn't do.
[00:46:03.860 --> 00:46:04.860]   Google doesn't shut up.
[00:46:04.860 --> 00:46:05.860]   Apple shuts up.
[00:46:05.860 --> 00:46:06.860]   Exactly.
[00:46:06.860 --> 00:46:09.100]   Google builds their products in front of you.
[00:46:09.100 --> 00:46:13.580]   Sometimes they fail, but they always are basically, you get to see how the sausage is
[00:46:13.580 --> 00:46:14.580]   made.
[00:46:14.580 --> 00:46:15.580]   Apple, they keep it secret.
[00:46:15.580 --> 00:46:19.060]   And then when they have a fully realized product, usually, unless it's like a beta
[00:46:19.060 --> 00:46:23.460]   like Syria or Apple Maps, then they'll bring it out to you.
[00:46:23.460 --> 00:46:26.820]   This is actually something that Mark and I have been talking about recently, just as
[00:46:26.820 --> 00:46:28.860]   friends but also on our Buzzkill podcast.
[00:46:28.860 --> 00:46:31.820]   But it's like, this is a difference between those companies.
[00:46:31.820 --> 00:46:32.820]   Yeah.
[00:46:32.820 --> 00:46:34.580]   Face, where's my plug buzzer?
[00:46:34.580 --> 00:46:40.020]   Yeah, Facebook also, with their F8 developers conference, they're further behind on the
[00:46:40.020 --> 00:46:43.380]   honest tech, but they showed some like, you know, camera apps and they got people excited.
[00:46:43.380 --> 00:46:46.980]   I kind of respect a company like Google where they're not going to do the flash.
[00:46:46.980 --> 00:46:50.500]   They're just going to do the- And remember, it's a developers conference and they're
[00:46:50.500 --> 00:46:51.500]   not going to try to impress.
[00:46:51.500 --> 00:46:52.500]   Yes.
[00:46:52.500 --> 00:46:56.060]   Well, it's a developers conference, but these keynotes have been so filled with consumer
[00:46:56.060 --> 00:46:59.540]   news that the expectation these days is that you're going to have something flashy to
[00:46:59.540 --> 00:47:00.540]   show us.
[00:47:00.540 --> 00:47:01.540]   I was a little disappointed.
[00:47:01.540 --> 00:47:02.540]   We didn't see anything about Chromebook.
[00:47:02.540 --> 00:47:04.500]   We didn't see anything about a lot of consumer products.
[00:47:04.500 --> 00:47:05.780]   They didn't mention the Pixel phones.
[00:47:05.780 --> 00:47:09.980]   They didn't mention- They did mention Google Home and the Assistant and the Assistant now
[00:47:09.980 --> 00:47:15.620]   is on the iPhone, which is really a stealth introduction to Assistant to get iPhone users
[00:47:15.620 --> 00:47:19.460]   to understand, you know, that Siri, you've got, that's crap.
[00:47:19.460 --> 00:47:21.740]   This is what a real Assistant can do.
[00:47:21.740 --> 00:47:25.860]   I think hoping to woo some people over to Android, but at least to get them to buy the
[00:47:25.860 --> 00:47:27.220]   Google Home.
[00:47:27.220 --> 00:47:29.620]   The other thing that- Maybe it's just me.
[00:47:29.620 --> 00:47:32.540]   I can't mention this, everybody's kind of ho-hum.
[00:47:32.540 --> 00:47:37.580]   I think the fact that you can make free calls to the- Any phone in the US and Canada on
[00:47:37.580 --> 00:47:40.140]   your Google Home is kind of impressive.
[00:47:40.140 --> 00:47:42.540]   I think I've got to mazes people our age.
[00:47:42.540 --> 00:47:43.540]   Maybe it's because we're old.
[00:47:43.540 --> 00:47:44.540]   Yeah.
[00:47:44.540 --> 00:47:45.540]   Yeah, as we're saying, we're there.
[00:47:45.540 --> 00:47:48.340]   You know, we're old enough for a little panelists.
[00:47:48.340 --> 00:47:50.780]   We're old enough that our parents would yell at us to get off the phone.
[00:47:50.780 --> 00:47:51.780]   Yeah.
[00:47:51.780 --> 00:47:53.780]   Because as long distance it's costing a lot of money.
[00:47:53.780 --> 00:47:54.780]   Right.
[00:47:54.780 --> 00:47:55.780]   See to me, that's not impressive at all.
[00:47:55.780 --> 00:47:56.780]   Because your cell phone does that.
[00:47:56.780 --> 00:47:57.780]   Yeah.
[00:47:57.780 --> 00:47:58.780]   You have unlimited minutes now.
[00:47:58.780 --> 00:48:02.100]   It's like, it goes- They don't want to talk on the phone.
[00:48:02.100 --> 00:48:03.340]   Nobody wants to talk on the phone.
[00:48:03.340 --> 00:48:04.340]   Yeah.
[00:48:04.340 --> 00:48:08.380]   If I have to go to my phone to set up my speaker, my speaker should know that my phone exists
[00:48:08.380 --> 00:48:11.380]   and it should be able to basically be a speaker phone.
[00:48:11.380 --> 00:48:12.380]   Like that doesn't seem crazy.
[00:48:12.380 --> 00:48:15.620]   I just- I just- You millennials don't- You can call all the other Alexa's.
[00:48:15.620 --> 00:48:17.460]   You can only call other echoes.
[00:48:17.460 --> 00:48:18.460]   Yeah.
[00:48:18.460 --> 00:48:23.300]   This- So in a couple of ways they scooped, I thought they sort of, you know, scooped Amazon's
[00:48:23.300 --> 00:48:26.860]   big announcement the last few weeks because that was one announcement is free echo to
[00:48:26.860 --> 00:48:27.860]   echo calling.
[00:48:27.860 --> 00:48:28.860]   Big deal.
[00:48:28.860 --> 00:48:29.860]   Yeah.
[00:48:29.860 --> 00:48:33.060]   And they also have, by the way, some privacy issues because there's no way to block calls.
[00:48:33.060 --> 00:48:37.040]   So the fact that anybody who figures out how to get a hold of you on your echo can bug
[00:48:37.040 --> 00:48:39.860]   the hell out of you in your house is a problem.
[00:48:39.860 --> 00:48:40.860]   Yeah, no things.
[00:48:40.860 --> 00:48:41.860]   No things.
[00:48:41.860 --> 00:48:42.860]   No things.
[00:48:42.860 --> 00:48:44.820]   But then remember Amazon has announced but not yet shipped this home.
[00:48:44.820 --> 00:48:45.820]   What do they call it?
[00:48:45.820 --> 00:48:46.820]   The show?
[00:48:46.820 --> 00:48:47.820]   Yeah.
[00:48:47.820 --> 00:48:48.820]   The show.
[00:48:48.820 --> 00:48:49.820]   The show has a screen on it.
[00:48:49.820 --> 00:48:51.740]   It's really just a tablet with an echo built into it.
[00:48:51.740 --> 00:48:56.060]   But Google says, well, we didn't do that because you got screens all over the house.
[00:48:56.060 --> 00:49:04.500]   So how about if you just can cast your calendar or your TV or any other cast-ready screen
[00:49:04.500 --> 00:49:09.100]   in your house and of course most people have cast-ready screens all over the place now?
[00:49:09.100 --> 00:49:11.220]   So I thought that was impressive.
[00:49:11.220 --> 00:49:15.180]   It does make me wonder though if, and this is something that Meghan Moroni brought up actually
[00:49:15.180 --> 00:49:22.180]   and we were talking about earlier, is whether or not this puts pressure on Amazon to come
[00:49:22.180 --> 00:49:24.060]   out with a phone again.
[00:49:24.060 --> 00:49:29.620]   Because even so Google has its apps on the iPhone and they of course have Android but
[00:49:29.620 --> 00:49:33.620]   basically if you're using an iOS device or an Android device they have a presence on
[00:49:33.620 --> 00:49:35.180]   your phone.
[00:49:35.180 --> 00:49:39.700]   And the reason why this might be a little bit of a jump ahead for them in this smart
[00:49:39.700 --> 00:49:43.820]   speaker phone calling thing is because they have access to your phone in ways that Amazon
[00:49:43.820 --> 00:49:44.820]   doesn't yet.
[00:49:44.820 --> 00:49:48.460]   Well, and that's the other thing Google has to kind of think about is the creepy line,
[00:49:48.460 --> 00:49:49.460]   right?
[00:49:49.460 --> 00:49:53.580]   Jeff, I mean every time they announce something there are going to be a group of people,
[00:49:53.580 --> 00:49:55.900]   privacy advocates say, "Oh hell no."
[00:49:55.900 --> 00:49:56.900]   Yeah.
[00:49:56.900 --> 00:49:58.900]   I'm not putting that in the house.
[00:49:58.900 --> 00:49:59.900]   Yeah.
[00:49:59.900 --> 00:50:00.900]   Right.
[00:50:00.900 --> 00:50:03.100]   The Google lens is kind of funny.
[00:50:03.100 --> 00:50:10.460]   They announced this is an AI capability that reminds you a lot of Google goggles and Amazon's
[00:50:10.460 --> 00:50:11.460]   fire phone.
[00:50:11.460 --> 00:50:12.460]   Seven years ago.
[00:50:12.460 --> 00:50:13.460]   Right.
[00:50:13.460 --> 00:50:20.500]   But I asked somebody at Google who was involved in integrating this into search and photos.
[00:50:20.500 --> 00:50:24.100]   He said, "Well, but this is AI, you know, goggles was kind of pre-populated.
[00:50:24.100 --> 00:50:25.780]   We had to teach it.
[00:50:25.780 --> 00:50:27.300]   This is, this is learning all the time."
[00:50:27.300 --> 00:50:28.460]   So for instance, I love this.
[00:50:28.460 --> 00:50:30.660]   They took, take a picture of a flower.
[00:50:30.660 --> 00:50:31.660]   This happens to me all the time.
[00:50:31.660 --> 00:50:33.260]   Why does that flower?
[00:50:33.260 --> 00:50:36.540]   And it will actually say, "Oh, that's a milk and wine, Lily."
[00:50:36.540 --> 00:50:38.980]   And it will even give you nearby florists.
[00:50:38.980 --> 00:50:40.740]   You can order it and buy it.
[00:50:40.740 --> 00:50:41.900]   That is cool.
[00:50:41.900 --> 00:50:44.700]   That's a business proposition for Google, but I think that's cool.
[00:50:44.700 --> 00:50:49.340]   If you have barcodes in the back of your router it will automatically configure, you
[00:50:49.340 --> 00:50:52.500]   know, connect to it by just taking a picture of it.
[00:50:52.500 --> 00:50:53.500]   It'll get them read it.
[00:50:53.500 --> 00:50:58.060]   And my favorite one was they showed, you know, you using your phone and I'm not sure
[00:50:58.060 --> 00:50:59.540]   what app this is.
[00:50:59.540 --> 00:51:02.220]   Maybe it's just the camera app because that has lens built into it.
[00:51:02.220 --> 00:51:06.060]   You turn on lens and then you point at different storefronts and they'll tell you what the
[00:51:06.060 --> 00:51:09.420]   star rating is, you know, how much it costs.
[00:51:09.420 --> 00:51:10.660]   You can make a reservation.
[00:51:10.660 --> 00:51:11.660]   How many are you going to make?
[00:51:11.660 --> 00:51:14.620]   Well, this is, this is AR beyond what, what Facebook can do because Google has all that
[00:51:14.620 --> 00:51:15.620]   data.
[00:51:15.620 --> 00:51:20.500]   And, you know, I know Apple at some point Apple is going to announce AR.
[00:51:20.500 --> 00:51:23.940]   This seems to me exactly what Apple might announce in the fall.
[00:51:23.940 --> 00:51:26.020]   And I think they've scooped them a little bit.
[00:51:26.020 --> 00:51:31.060]   Here's a, here's an example of, they took a picture of a marquee at one of my favorite
[00:51:31.060 --> 00:51:36.420]   clubs in San Francisco, Bimbo's 365 club of the Stone Foxes are playing May 17th.
[00:51:36.420 --> 00:51:41.180]   So the lens is smart enough to know that that's a Stone Foxes offers you Stone Foxes
[00:51:41.180 --> 00:51:42.180]   music.
[00:51:42.180 --> 00:51:48.220]   It offers to add that to your calendar, which is pretty impressive and shows you where you
[00:51:48.220 --> 00:51:51.780]   can buy tickets to that all from taking a picture of the marquee.
[00:51:51.780 --> 00:51:56.020]   Those things seem like the kinds of things that would be useful over and above what goggles
[00:51:56.020 --> 00:51:57.020]   could be.
[00:51:57.020 --> 00:52:00.180]   It's an entirely new user interface with the computer.
[00:52:00.180 --> 00:52:02.900]   They're typing, they have conversation and now they have sight.
[00:52:02.900 --> 00:52:04.780]   Can we get into some Amazon topics?
[00:52:04.780 --> 00:52:06.820]   I've been waiting to fight episode with Tom.
[00:52:06.820 --> 00:52:07.820]   Well, and it has come up.
[00:52:07.820 --> 00:52:08.820]   Yes.
[00:52:08.820 --> 00:52:11.820]   This is the one I wanted to bring up because while I'm in this news drought, I'm going to
[00:52:11.820 --> 00:52:14.380]   do this drought in South America.
[00:52:14.380 --> 00:52:23.820]   I read another story that Amazon bought whole foods for $13.7 billion.
[00:52:23.820 --> 00:52:26.940]   And again, I thought, what?
[00:52:26.940 --> 00:52:31.500]   And I couldn't wait to get back here and ask you all what you think of this.
[00:52:31.500 --> 00:52:33.140]   Amazon buys whole foods.
[00:52:33.140 --> 00:52:37.740]   Now there's some wags who said, I don't think this is how markets work, but who said, you
[00:52:37.740 --> 00:52:41.020]   know, Amazon stock went up by more than $13.7 billion.
[00:52:41.020 --> 00:52:42.020]   In fact, it was free.
[00:52:42.020 --> 00:52:43.020]   Yeah.
[00:52:43.020 --> 00:52:45.020]   It wasn't a stock deal though.
[00:52:45.020 --> 00:52:47.060]   So it's not really how it works.
[00:52:47.060 --> 00:52:52.940]   But yeah, but it certainly was a good move, at least from the point of view of the market.
[00:52:52.940 --> 00:52:55.100]   They thought, hey, this is a good acquisition.
[00:52:55.100 --> 00:52:56.100]   Is it?
[00:52:56.100 --> 00:52:59.220]   Why would Amazon buy a brick and mortar grocery store?
[00:52:59.220 --> 00:53:00.780]   Who wants to start?
[00:53:00.780 --> 00:53:02.300]   One had some struggles.
[00:53:02.300 --> 00:53:03.780]   Actually, I'll let you know.
[00:53:03.780 --> 00:53:04.780]   Yeah.
[00:53:04.780 --> 00:53:07.620]   But Tom Merrick, because apparently, Oh and JG Stone thinks they're going to have a fight.
[00:53:07.620 --> 00:53:10.740]   I want to go first so I could just take the opposite of whatever he says.
[00:53:10.740 --> 00:53:11.740]   That's all in technique.
[00:53:11.740 --> 00:53:13.540]   Oh man, little JG it too.
[00:53:13.540 --> 00:53:14.540]   That is it.
[00:53:14.540 --> 00:53:16.980]   See, this one I don't like Tom.
[00:53:16.980 --> 00:53:19.900]   I'm trying to get Tom out of here.
[00:53:19.900 --> 00:53:22.100]   Tom, no, but look, this was the greatest deal.
[00:53:22.100 --> 00:53:23.100]   Don't admit.
[00:53:23.100 --> 00:53:24.100]   Are you crazy?
[00:53:24.100 --> 00:53:25.340]   You know how awesome this is?
[00:53:25.340 --> 00:53:26.340]   Okay.
[00:53:26.340 --> 00:53:28.180]   Why is this a good deal?
[00:53:28.180 --> 00:53:34.220]   What's the first of all, whole foods, AKA whole paychecks has done the marketing research
[00:53:34.220 --> 00:53:36.660]   and they only are in rich environments.
[00:53:36.660 --> 00:53:40.940]   Most people are the people that can afford to pay Amazon to deliver and/or pick up and/or
[00:53:40.940 --> 00:53:44.540]   drop off their expensive groceries because regular people won't do it.
[00:53:44.540 --> 00:53:48.900]   So taking an Acme or a ShopRite or Lion or whoever grocery stores are going to help
[00:53:48.900 --> 00:53:49.900]   you.
[00:53:49.900 --> 00:53:52.220]   This is a national food grocery store.
[00:53:52.220 --> 00:53:54.460]   So get coast to coast, Key West to Key Largo.
[00:53:54.460 --> 00:53:57.340]   There's a bunch of benefits to Key West to Key Largo.
[00:53:57.340 --> 00:53:58.900]   That's both in the same state.
[00:53:58.900 --> 00:53:59.900]   It's a good thing.
[00:53:59.900 --> 00:54:00.900]   It sounds good though.
[00:54:00.900 --> 00:54:01.900]   It sounds good though.
[00:54:01.900 --> 00:54:08.260]   It's good though.
[00:54:08.260 --> 00:54:13.460]   It's great for them because they have many markets in certain areas, in affluent areas
[00:54:13.460 --> 00:54:17.780]   to test their whole pantry system and food that they're trying to sell.
[00:54:17.780 --> 00:54:20.100]   Key West to Key Largo is like 10 miles.
[00:54:20.100 --> 00:54:26.020]   Why are you asking 32 things in five seconds to use top up to you, Wes or Key Largo?
[00:54:26.020 --> 00:54:27.420]   Shut up and look, sounds good though.
[00:54:27.420 --> 00:54:28.420]   I like it.
[00:54:28.420 --> 00:54:29.420]   Go ahead.
[00:54:29.420 --> 00:54:31.020]   This is a smart dinosaur.
[00:54:31.020 --> 00:54:34.820]   Most companies, Blockbuster sat there and watched Red Box eat their lunch.
[00:54:34.820 --> 00:54:37.940]   They watched Netflix eat their lunch and they didn't do anything.
[00:54:37.940 --> 00:54:45.820]   Amazon is the end all be all of retail selling and this is a strong foothold coast to coast.
[00:54:45.820 --> 00:54:47.580]   San Francisco to Miami.
[00:54:47.580 --> 00:54:49.420]   That's one of them.
[00:54:49.420 --> 00:54:53.340]   Expand their grocery brand that they are obviously moving forward to.
[00:54:53.340 --> 00:54:57.020]   They're probably going to scale back on the high price items, but it's a great move for
[00:54:57.020 --> 00:55:00.980]   them and it gives them a foothold in something that they bought quote unquote on the cheap.
[00:55:00.980 --> 00:55:02.860]   They quote unquote got for free.
[00:55:02.860 --> 00:55:03.860]   It's great.
[00:55:03.860 --> 00:55:04.860]   It wasn't that cheap.
[00:55:04.860 --> 00:55:10.220]   I mean, by far the largest acquisition, the next biggest one is like zappos for two billion.
[00:55:10.220 --> 00:55:13.860]   It's expensive to me as cheap to you, Uncle Leo, it ain't nothing but a bang.
[00:55:13.860 --> 00:55:14.860]   That's true.
[00:55:14.860 --> 00:55:15.860]   You see what I'm saying?
[00:55:15.860 --> 00:55:16.860]   Jeff Bezos, what does he make?
[00:55:16.860 --> 00:55:19.380]   Somebody said he makes like $10 million a day.
[00:55:19.380 --> 00:55:21.060]   So I guess he could afford it.
[00:55:21.060 --> 00:55:22.300]   They have some cash on hand.
[00:55:22.300 --> 00:55:24.060]   So they could afford it.
[00:55:24.060 --> 00:55:25.060]   Why does that?
[00:55:25.060 --> 00:55:26.060]   I think this is a great deal.
[00:55:26.060 --> 00:55:29.580]   I don't disagree with anything that Owen is saying.
[00:55:29.580 --> 00:55:34.700]   And the thing that I was saying about Whole Foods was they were their profit margins have
[00:55:34.700 --> 00:55:35.700]   been degrading.
[00:55:35.700 --> 00:55:36.700]   >> They've been in trouble.
[00:55:36.700 --> 00:55:38.300]   >> A little bit in trouble, right?
[00:55:38.300 --> 00:55:41.260]   They're not >> Fending off a little bit of activist
[00:55:41.260 --> 00:55:43.420]   investor pressure.
[00:55:43.420 --> 00:55:51.260]   So this was a way for them to make an exit to somebody who can absorb that pressure and
[00:55:51.260 --> 00:55:54.980]   not worry about it because Amazon runs on thin margins all the time.
[00:55:54.980 --> 00:56:02.340]   And just made Amazon fresh, fresh again because they get a presence like Owen saying in all
[00:56:02.340 --> 00:56:08.180]   of these different markets and they get brick and mortar outlets that they can use as distribution
[00:56:08.180 --> 00:56:09.180]   for that.
[00:56:09.180 --> 00:56:13.620]   The problem is Instacart has a five year deal with Whole Foods.
[00:56:13.620 --> 00:56:18.220]   So they're gonna have to figure out what to do to either break that deal or coexist with
[00:56:18.220 --> 00:56:20.820]   Instacart delivering Whole Foods for five years.
[00:56:20.820 --> 00:56:24.940]   And Instacart's out there immediately saying Amazon is a threat to every other grocery
[00:56:24.940 --> 00:56:25.940]   store.
[00:56:25.940 --> 00:56:28.540]   You wanna sign up with us right now.
[00:56:28.540 --> 00:56:36.620]   >> Is this though, I mean, it seems like that assuming that this is a deal about Amazon
[00:56:36.620 --> 00:56:43.620]   delivering groceries to its customers seems like it's less than it really is.
[00:56:43.620 --> 00:56:46.340]   It feels like there's more to this than just that.
[00:56:46.340 --> 00:56:47.900]   >> More than just that.
[00:56:47.900 --> 00:56:53.180]   And I think they obviously want to jumpstart the same thing they've been doing with the
[00:56:53.180 --> 00:56:55.500]   brick and mortar book retail stores.
[00:56:55.500 --> 00:56:58.620]   And then there've been a lot of rumors that they were thinking about doing electronics
[00:56:58.620 --> 00:57:02.500]   outlets and they have that Amazon Go convenience store.
[00:57:02.500 --> 00:57:06.060]   So they maybe if they can get the kinks worked out, they could have bringing that kind of
[00:57:06.060 --> 00:57:11.220]   technology into the Whole Foods outlets to make them super high tech places to shop.
[00:57:11.220 --> 00:57:14.940]   So yeah, I don't think it's just about delivery, but that's the big early win for them.
[00:57:14.940 --> 00:57:16.780]   >> It's funny because it's been Thompson.
[00:57:16.780 --> 00:57:19.300]   It's been Thompson points out in the Stratecary.
[00:57:19.300 --> 00:57:24.940]   It was only two years ago that Whole Foods CEO John Mackie predicted that groceries would
[00:57:24.940 --> 00:57:28.460]   be Amazon's Waterloo.
[00:57:28.460 --> 00:57:30.940]   So if you can't beat him, buy him.
[00:57:30.940 --> 00:57:31.940]   >> Yeah.
[00:57:31.940 --> 00:57:33.140]   Here's how I think of this deal.
[00:57:33.140 --> 00:57:38.220]   And Larry Dignan on Zee-Dee-Net, my colleague, has written really smartly about it.
[00:57:38.220 --> 00:57:39.220]   He watches this stuff.
[00:57:39.220 --> 00:57:42.260]   And he's pretty skeptical too.
[00:57:42.260 --> 00:57:44.140]   He's pretty bullish on it as well.
[00:57:44.140 --> 00:57:45.740]   And obviously we've talked about it a lot.
[00:57:45.740 --> 00:57:47.380]   Here's kind of how I think of this.
[00:57:47.380 --> 00:57:53.300]   There's a real similarity that Whole Foods and Amazon have in that Whole Foods was struggling
[00:57:53.300 --> 00:57:59.100]   because all of these grocers were carrying the same products that Whole Foods carries.
[00:57:59.100 --> 00:58:04.740]   A lot of them, not all of them obviously, but they were carrying and they were starting
[00:58:04.740 --> 00:58:13.260]   their own brands to do all of these same kind of organics, clean eating kind of products.
[00:58:13.260 --> 00:58:18.940]   And that was slowly siphoning off more of Whole Foods business.
[00:58:18.940 --> 00:58:22.780]   Amazon had its biggest threat.
[00:58:22.780 --> 00:58:29.540]   I mean, let's be honest, Amazon is such a monster right now in the US last year in the
[00:58:29.540 --> 00:58:34.740]   holiday sales, which is when everybody makes all of their money in the fourth quarter.
[00:58:34.740 --> 00:58:39.380]   Amazon owned like 40% of the holiday gift market.
[00:58:39.380 --> 00:58:48.060]   The next closest competitor I believe was Best Buy in the low single digits, like 3-4%.
[00:58:48.060 --> 00:58:53.380]   So who wants to buy milk and eggs and bread online?
[00:58:53.380 --> 00:58:55.860]   I don't understand Leo.
[00:58:55.860 --> 00:58:57.020]   It's not that.
[00:58:57.020 --> 00:58:58.300]   It's what they're going to do.
[00:58:58.300 --> 00:59:04.340]   So their threat, the biggest threat for Amazon was all of the brick and mortar, some of the
[00:59:04.340 --> 00:59:08.500]   bigger click and mortar ones getting smarter about online.
[00:59:08.500 --> 00:59:09.820]   They are getting smarter.
[00:59:09.820 --> 00:59:12.380]   Their general transformation efforts are happening.
[00:59:12.380 --> 00:59:15.540]   Walmart is probably the biggest threat to Amazon, right?
[00:59:15.540 --> 00:59:17.580]   And vice versa, Walmart realized.
[00:59:17.580 --> 00:59:19.580]   Amazon's the biggest threat to Walmart.
[00:59:19.580 --> 00:59:20.580]   Yeah, yeah.
[00:59:20.580 --> 00:59:21.580]   The history fit right now.
[00:59:21.580 --> 00:59:24.020]   To Walmart's out here trying to do mafia style stuff.
[00:59:24.020 --> 00:59:25.420]   Oh, you shopping at Walmart?
[00:59:25.420 --> 00:59:27.220]   You don't want to come over here?
[00:59:27.220 --> 00:59:28.860]   They're doing weird stuff.
[00:59:28.860 --> 00:59:32.980]   Yeah, they're encouraging their partners to pull out of AWS and move to like Azure or
[00:59:32.980 --> 00:59:33.980]   something else.
[00:59:33.980 --> 00:59:34.980]   Yeah.
[00:59:34.980 --> 00:59:35.980]   Isn't that wild?
[00:59:35.980 --> 00:59:36.980]   Yeah.
[00:59:36.980 --> 00:59:37.980]   That's the Prano stuff.
[00:59:37.980 --> 00:59:44.580]   If you want to continue to do business with us, you probably want to reconsider your relationship
[00:59:44.580 --> 00:59:45.580]   with Amazon.
[00:59:45.580 --> 00:59:49.220]   I'm not saying that's going to happen, but you might not walk again.
[00:59:49.220 --> 00:59:52.700]   Well, when they saw this, right?
[00:59:52.700 --> 00:59:53.700]   Walmart flipped out.
[00:59:53.700 --> 00:59:54.700]   All right.
[00:59:54.700 --> 00:59:56.980]   We haven't talked about Travis Kalanek yet.
[00:59:56.980 --> 01:00:00.140]   I got my shot glass ready to pour a little out.
[01:00:00.140 --> 01:00:02.140]   I'm really going to try this.
[01:00:02.140 --> 01:00:03.140]   Uber.
[01:00:03.140 --> 01:00:07.140]   Yeah, it seemed for a while there every week there was a new Uber story.
[01:00:07.140 --> 01:00:09.820]   It's unbelievable story.
[01:00:09.820 --> 01:00:11.180]   Grown after grown.
[01:00:11.180 --> 01:00:12.180]   Grown after grown.
[01:00:12.180 --> 01:00:15.540]   Of course, the story this week, Travis Kalanek.
[01:00:15.540 --> 01:00:22.340]   Great story in the New York Times about how the board decided to fire after hours of
[01:00:22.340 --> 01:00:23.440]   drama.
[01:00:23.440 --> 01:00:26.420]   We are the board resistances.
[01:00:26.420 --> 01:00:31.340]   Mr. Kalanek's exit came under pressure after hours of drama involving Uber's investors,
[01:00:31.340 --> 01:00:35.100]   according to two people with colleges as a situation.
[01:00:35.100 --> 01:00:41.420]   They're phone calls and hotel calls and Bill Girly, who by the way, who's the venture
[01:00:41.420 --> 01:00:45.140]   partner at the benchmark capital, one of the big investors, has left Uber's board to
[01:00:45.140 --> 01:00:46.700]   put another partner on the board.
[01:00:46.700 --> 01:00:52.380]   Maybe he's somehow tarnished, but Kalanek was in Chicago.
[01:00:52.380 --> 01:00:58.580]   A letter hand delivered to him says, "Mr. Kalanek, time to say good night."
[01:00:58.580 --> 01:01:00.260]   Now, he's still going to be chairman of the board.
[01:01:00.260 --> 01:01:05.020]   He ain't going really anywhere, but he's not going to have day-to-day operational management.
[01:01:05.020 --> 01:01:10.700]   Interestingly, a thousand Uber employees signed a petition saying, "We must have him back.
[01:01:10.700 --> 01:01:14.020]   He's the only guy who can run Uber."
[01:01:14.020 --> 01:01:19.100]   The investors clearly felt that he was more of a liability than an asset to the company.
[01:01:19.100 --> 01:01:21.300]   Your thoughts?
[01:01:21.300 --> 01:01:23.820]   Tom, you start.
[01:01:23.820 --> 01:01:25.700]   Tom, go ahead.
[01:01:25.700 --> 01:01:28.700]   You got the shock, Lazan Hand.
[01:01:28.700 --> 01:01:36.300]   I think this is a very difficult situation for Uber to be, and they virtually have no
[01:01:36.300 --> 01:01:37.300]   sea level executives.
[01:01:37.300 --> 01:01:39.620]   They have a committee of 14 people running the company.
[01:01:39.620 --> 01:01:45.100]   Travis Kalanek was doing the things that he needed to do at this point.
[01:01:45.100 --> 01:01:49.660]   Now, you can rightly say too little too late as far as forgiving him personally, but he
[01:01:49.660 --> 01:01:53.860]   was finally saying, "Yes, we're going to get rid of people.
[01:01:53.860 --> 01:01:55.260]   We're going to change the company culture."
[01:01:55.260 --> 01:01:58.900]   I mean, Ariana Huffington was right there next to him cheering him on saying, "Here's
[01:01:58.900 --> 01:02:01.940]   all the things you have to do."
[01:02:01.940 --> 01:02:04.340]   And yet, that faction of the board said, "You know what?
[01:02:04.340 --> 01:02:08.140]   We'd just rather have you out of the way so we can pick an adult."
[01:02:08.140 --> 01:02:12.460]   And maybe not that they didn't think the world of Travis or his abilities, but it was too
[01:02:12.460 --> 01:02:14.260]   much of a liability.
[01:02:14.260 --> 01:02:19.540]   And I think what is odd to me is that they didn't have a candidate already lined up after
[01:02:19.540 --> 01:02:20.540]   all of this.
[01:02:20.540 --> 01:02:21.540]   That is interesting, right?
[01:02:21.540 --> 01:02:22.780]   We don't know who's taking his place.
[01:02:22.780 --> 01:02:25.980]   Yeah, and that will be the telling tale here, right?
[01:02:25.980 --> 01:02:29.260]   If they were to get a Sheryl Sandberg, I don't think they'll get Sheryl Sandberg to leave
[01:02:29.260 --> 01:02:33.740]   Facebook, but if they were to get somebody like that, that makes sense to me.
[01:02:33.740 --> 01:02:38.700]   If they get someone unknown or someone who doesn't have a proven track record, then that
[01:02:38.700 --> 01:02:40.540]   makes not a lot of sense at all.
[01:02:40.540 --> 01:02:42.140]   Yeah, and in fact, you're right.
[01:02:42.140 --> 01:02:46.340]   It would have made sense is to go out and talk to a Sheryl Sandberg or to...
[01:02:46.340 --> 01:02:49.580]   I'm sorry, let me heat auto play video.
[01:02:49.580 --> 01:02:50.580]   Thank you.
[01:02:50.580 --> 01:02:53.820]   Let me go out and talk to Susan Wojcicki.
[01:02:53.820 --> 01:02:56.820]   She's been floated as an agency of YouTube.
[01:02:56.820 --> 01:03:01.260]   Maybe a guy, maybe some like Alan Malali, another person who's been named as the former
[01:03:01.260 --> 01:03:07.860]   CEO Ford who is currently just kind of hanging out, not doing anything.
[01:03:07.860 --> 01:03:12.180]   And a smart guy, and if you think that self-driving vehicles are important to Uber's future, might
[01:03:12.180 --> 01:03:16.580]   be a good choice, wouldn't you go and talk to some of those people and say, "Look, we're
[01:03:16.580 --> 01:03:18.660]   going to boot Calinac."
[01:03:18.660 --> 01:03:19.660]   Yeah.
[01:03:19.660 --> 01:03:23.620]   Because the reason you get rid of Calinac as far as just brass text, if you're the board,
[01:03:23.620 --> 01:03:27.860]   I think you get rid of him obviously not to please the employees because you've got a
[01:03:27.860 --> 01:03:32.180]   bit of an employee revolt going on, but because you want to polish it up for the IPO.
[01:03:32.180 --> 01:03:34.420]   You don't want to have this in the way for the iPhone.
[01:03:34.420 --> 01:03:35.420]   And regulators.
[01:03:35.420 --> 01:03:36.420]   Yep.
[01:03:36.420 --> 01:03:38.780]   And you've got to consider government issues.
[01:03:38.780 --> 01:03:43.860]   I mean, Uber has been fighting the government since day one, local governments.
[01:03:43.860 --> 01:03:48.180]   They obviously considered him a toxic asset at this point.
[01:03:48.180 --> 01:03:51.220]   And so he had done a lot.
[01:03:51.220 --> 01:04:01.420]   His obviously super aggressive, he is the one that made Uber Uber for better and worse.
[01:04:01.420 --> 01:04:06.020]   And they had a lot of stuff they need to clean up and clean their house.
[01:04:06.020 --> 01:04:08.860]   And they just decided that he wasn't the one to clean it up.
[01:04:08.860 --> 01:04:11.140]   He's the one who created the mess.
[01:04:11.140 --> 01:04:16.140]   And him clean it up, you know, probably doesn't work is essentially what the board decided.
[01:04:16.140 --> 01:04:19.820]   When he owns the majority of voting shares, the only way you convince him of this is to
[01:04:19.820 --> 01:04:25.020]   say we need to bring in a caretaker CEO who's going to guide us through the IPO and make
[01:04:25.020 --> 01:04:26.900]   that really profitable for you Travis.
[01:04:26.900 --> 01:04:30.060]   And ideally, ideally a woman, right?
[01:04:30.060 --> 01:04:34.180]   Because if you bring in a woman, then you can maybe get rid of that shed that bro image.
[01:04:34.180 --> 01:04:35.180]   Yeah.
[01:04:35.180 --> 01:04:38.980]   And then maybe even say Travis will bring you back after the IPO a year or two later.
[01:04:38.980 --> 01:04:44.140]   Boy, it seemed like we did, we did Travis Kalinek and Uber stories all year long.
[01:04:44.140 --> 01:04:50.340]   This was, this was Uber's no good, very bad year, all year long.
[01:04:50.340 --> 01:04:52.860]   Alex Wilhelm, of course, is a regular.
[01:04:52.860 --> 01:04:53.860]   We love Alex.
[01:04:53.860 --> 01:04:57.300]   He's a great journalist and a lot of fun.
[01:04:57.300 --> 01:05:05.020]   But the weirdest thing is he's living or sometimes anyway, his girlfriend lives in my childhood
[01:05:05.020 --> 01:05:06.780]   home and Providence, Rhode Island.
[01:05:06.780 --> 01:05:11.820]   And from time to time, he's actually slept in my boyhood bedroom.
[01:05:11.820 --> 01:05:15.300]   And the man without a country, Alex Wilhelm, true.
[01:05:15.300 --> 01:05:17.300]   He's a bi-coastal.
[01:05:17.300 --> 01:05:18.300]   I now am bi-coastal.
[01:05:18.300 --> 01:05:19.300]   There's a rumor flow.
[01:05:19.300 --> 01:05:20.300]   Oh, you are?
[01:05:20.300 --> 01:05:21.300]   Yeah.
[01:05:21.300 --> 01:05:22.740]   I can say it, right?
[01:05:22.740 --> 01:05:23.740]   Yeah.
[01:05:23.740 --> 01:05:27.380]   I live in Leo's old house, ironically, in Providence part of the time now.
[01:05:27.380 --> 01:05:32.900]   I grew up, the house I grew up in, I moved out of when I was like 15.
[01:05:32.900 --> 01:05:34.380]   He's living in my house.
[01:05:34.380 --> 01:05:35.380]   Oh, wait.
[01:05:35.380 --> 01:05:36.380]   My girlfriend owns it.
[01:05:36.380 --> 01:05:37.380]   Mom and dad's bedroom.
[01:05:37.380 --> 01:05:38.380]   It's pretty funny.
[01:05:38.380 --> 01:05:40.020]   Talk about confusing.
[01:05:40.020 --> 01:05:41.500]   I just got back a week ago.
[01:05:41.500 --> 01:05:42.500]   I'm so confused.
[01:05:42.500 --> 01:05:43.500]   It's a great place.
[01:05:43.500 --> 01:05:45.020]   How did you guys figure out?
[01:05:45.020 --> 01:05:46.460]   How did you figure out it was his house?
[01:05:46.460 --> 01:05:47.940]   How did you make this connection?
[01:05:47.940 --> 01:05:52.780]   So his girlfriend was visiting and she mentioned that she grew up in Providence.
[01:05:52.780 --> 01:05:55.140]   And I said, oh, I grew up in Providence, Providence, Rhode Island.
[01:05:55.140 --> 01:05:57.020]   And she said, oh, where?
[01:05:57.020 --> 01:06:00.700]   And it didn't take, it took us about 30 seconds to figure out she owned my old house.
[01:06:00.700 --> 01:06:02.180]   It was, I lived on Street X.
[01:06:02.180 --> 01:06:03.180]   Oh, my gosh, me too.
[01:06:03.180 --> 01:06:04.180]   I grew up in X number.
[01:06:04.180 --> 01:06:05.380]   She said, what house on Street X?
[01:06:05.380 --> 01:06:06.380]   And I said, there's 21.
[01:06:06.380 --> 01:06:08.580]   And she said, that's my house.
[01:06:08.580 --> 01:06:09.580]   Yeah.
[01:06:09.580 --> 01:06:10.580]   Wow.
[01:06:10.580 --> 01:06:11.740]   I took pictures for you.
[01:06:11.740 --> 01:06:15.740]   That's by the way, that's when I knew we're in a simulation.
[01:06:15.740 --> 01:06:16.740]   That proved it.
[01:06:16.740 --> 01:06:18.140]   That's the glitch in the matrix.
[01:06:18.140 --> 01:06:20.260]   I now know it's a simulation.
[01:06:20.260 --> 01:06:21.260]   Is that happening to you guys?
[01:06:21.260 --> 01:06:22.860]   It happens to you every once in a while.
[01:06:22.860 --> 01:06:23.860]   You know, oh, simulation.
[01:06:23.860 --> 01:06:24.860]   Nothing on that scale.
[01:06:24.860 --> 01:06:25.860]   That's pretty amazing.
[01:06:25.860 --> 01:06:27.020]   It's like a glitch in the simulation.
[01:06:27.020 --> 01:06:30.500]   If this is a simulation, I want to reroll my character with better hair.
[01:06:30.500 --> 01:06:31.500]   Too late.
[01:06:31.500 --> 01:06:32.500]   Okay.
[01:06:32.500 --> 01:06:34.420]   Part of what you got to understand the rules of the simulation.
[01:06:34.420 --> 01:06:36.580]   The whole idea is that you have, this is it.
[01:06:36.580 --> 01:06:37.740]   You rolled the character.
[01:06:37.740 --> 01:06:39.740]   You got to go from birth to death.
[01:06:39.740 --> 01:06:40.740]   This is it.
[01:06:40.740 --> 01:06:41.740]   There's no fixing.
[01:06:41.740 --> 01:06:43.060]   Do I get to try it again?
[01:06:43.060 --> 01:06:45.500]   Yeah, next time you could roll a different character.
[01:06:45.500 --> 01:06:49.140]   But you've decided to do a chaotic mage.
[01:06:49.140 --> 01:06:50.140]   And that's it.
[01:06:50.140 --> 01:06:51.940]   You got to live with that.
[01:06:51.940 --> 01:06:53.820]   I don't even know that's offensive or a compliment.
[01:06:53.820 --> 01:06:55.980]   I just, I'll just take it.
[01:06:55.980 --> 01:06:57.460]   Let's do it.
[01:06:57.460 --> 01:07:00.580]   The simulation is saving Ram.
[01:07:00.580 --> 01:07:02.100]   It's right untoward.
[01:07:02.100 --> 01:07:05.340]   They didn't have enough Ram to have more houses.
[01:07:05.340 --> 01:07:07.420]   And they thought, well, what is the chance of a collision?
[01:07:07.420 --> 01:07:09.260]   Oh my God, we had a collision.
[01:07:09.260 --> 01:07:13.580]   There's somebody, there's some operator, some sissoming going, oh crap.
[01:07:13.580 --> 01:07:14.980]   I got to rewrite that code.
[01:07:14.980 --> 01:07:16.460]   Why did I give IBM this contract?
[01:07:16.460 --> 01:07:17.460]   Yeah.
[01:07:17.460 --> 01:07:19.180]   You shouldn't put more Ram in the damn thing.
[01:07:19.180 --> 01:07:20.180]   All right.
[01:07:20.180 --> 01:07:21.780]   Amy, we're going to put you on the spot.
[01:07:21.780 --> 01:07:26.260]   You have to represent all of woman kind.
[01:07:26.260 --> 01:07:29.220]   Although a number of men jumped in on this.
[01:07:29.220 --> 01:07:34.700]   So apparently there was a memo placed by a Googler.
[01:07:34.700 --> 01:07:39.780]   As yet unnamed Googler, although if his name is ever revealed, I think we'll find out
[01:07:39.780 --> 01:07:40.780]   about it.
[01:07:40.780 --> 01:07:47.340]   He posted it on apparently a Googler meme site as well as the internal equivalent of
[01:07:47.340 --> 01:07:49.500]   Google+.
[01:07:49.500 --> 01:07:55.860]   And the full content of the memo has been finally revealed by Gizmodo.
[01:07:55.860 --> 01:07:56.860]   They got it.
[01:07:56.860 --> 01:07:58.780]   Probably very hard.
[01:07:58.780 --> 01:08:03.860]   And I'm going to try to characterize this so that people can react to it.
[01:08:03.860 --> 01:08:05.780]   The reaction has been very, very strong.
[01:08:05.780 --> 01:08:08.140]   You know, Google has had a diversity problem.
[01:08:08.140 --> 01:08:10.220]   The FTC has been trying to investigate them.
[01:08:10.220 --> 01:08:13.220]   Google said, no, you can't have any documents.
[01:08:13.220 --> 01:08:19.900]   Google says we're satisfied that we're doing enough to hire more women and more minorities.
[01:08:19.900 --> 01:08:23.140]   It's apparent though, if you look at the numbers, they're not.
[01:08:23.140 --> 01:08:26.380]   Women engineers represent one in five Googlers.
[01:08:26.380 --> 01:08:28.860]   African Americans and Hispanics, even worse.
[01:08:28.860 --> 01:08:35.940]   I think it's one or two percent African American engineers at Google's Engineering Division
[01:08:35.940 --> 01:08:38.220]   in the United States.
[01:08:38.220 --> 01:08:45.420]   But this memo says, well, this is just another form of discrimination.
[01:08:45.420 --> 01:08:54.140]   There's all this diversity, all this attempt to, let me see if I can find the text of it.
[01:08:54.140 --> 01:08:55.140]   It's pretty long.
[01:08:55.140 --> 01:08:59.260]   Yeah, I don't want to read it.
[01:08:59.260 --> 01:09:00.260]   I can summarize.
[01:09:00.260 --> 01:09:02.380]   Yeah, can you, Amy?
[01:09:02.380 --> 01:09:03.380]   Go ahead.
[01:09:03.380 --> 01:09:07.060]   So there are different sections of it.
[01:09:07.060 --> 01:09:12.820]   But the sort of gist of it is women don't make good coders according to the person who
[01:09:12.820 --> 01:09:15.580]   wrote this because...
[01:09:15.580 --> 01:09:18.100]   They're genetically just not suited.
[01:09:18.100 --> 01:09:19.100]   Right.
[01:09:19.100 --> 01:09:21.140]   Genetically and socially not suited.
[01:09:21.140 --> 01:09:24.140]   Now they have other skills, he's saying.
[01:09:24.140 --> 01:09:26.740]   Women are extroverts, men are introverts.
[01:09:26.740 --> 01:09:27.740]   Right.
[01:09:27.740 --> 01:09:29.220]   So there's a whole litany of...
[01:09:29.220 --> 01:09:32.780]   Women like people, women like machines or things.
[01:09:32.780 --> 01:09:36.940]   This is like Larry Summers, then President of Harvard, saying, well, women aren't good
[01:09:36.940 --> 01:09:39.580]   mathematicians, it's just genetically the case.
[01:09:39.580 --> 01:09:41.700]   So that's why they love life.
[01:09:41.700 --> 01:09:45.180]   Yeah, here's what I would say.
[01:09:45.180 --> 01:09:49.500]   My viewpoint on this is possibly a little controversial, I don't think so.
[01:09:49.500 --> 01:09:54.380]   But I would say that we are not all the same.
[01:09:54.380 --> 01:09:57.340]   That women and men are not the same.
[01:09:57.340 --> 01:10:03.580]   But I would also say that within women, there's quite a bit of diversity, just like there's
[01:10:03.580 --> 01:10:05.860]   quite a bit of diversity within men.
[01:10:05.860 --> 01:10:11.380]   The three of you sitting at that table are not archetypes or carbon copies of each other.
[01:10:11.380 --> 01:10:13.820]   At least I don't think so.
[01:10:13.820 --> 01:10:20.540]   And by this guy's definition, I would read like a 16-year-old teenage boy if we agree
[01:10:20.540 --> 01:10:26.060]   with how he delineates the genders in the memo.
[01:10:26.060 --> 01:10:29.300]   So there's a couple of things going on.
[01:10:29.300 --> 01:10:36.940]   One, he's making broad generalizations about women and about men, which any sociologist
[01:10:36.940 --> 01:10:39.500]   would tell you, don't track.
[01:10:39.500 --> 01:10:47.020]   So his generalizations, anybody who's a coder has taken math, which means that he should
[01:10:47.020 --> 01:10:49.380]   have known better than to generalize to begin with.
[01:10:49.380 --> 01:10:55.860]   He says, the society proves that it doesn't.
[01:10:55.860 --> 01:10:57.220]   So that's point one.
[01:10:57.220 --> 01:10:59.060]   Is just wrong.
[01:10:59.060 --> 01:11:05.900]   Point two, this had to have been a pretty low-level engineer because the way that he's describing
[01:11:05.900 --> 01:11:11.620]   some of the work would tell me that he hasn't yet worked on any more advanced projects at
[01:11:11.620 --> 01:11:12.620]   Google.
[01:11:12.620 --> 01:11:16.460]   He's probably like an entry-level 23-year-old kid.
[01:11:16.460 --> 01:11:22.260]   Nothing wrong with 23-year-old kids, but what it does tell me is that his work has
[01:11:22.260 --> 01:11:23.260]   been binary.
[01:11:23.260 --> 01:11:25.660]   So he hasn't had to solve any real problems.
[01:11:25.660 --> 01:11:29.700]   And anybody who's ever had to think about the future or about the past or solve any
[01:11:29.700 --> 01:11:34.740]   real problems understands that biodiversity is a good thing.
[01:11:34.740 --> 01:11:37.420]   And that teams are requisite.
[01:11:37.420 --> 01:11:44.300]   So to me, this was probably written by somebody in reaction to either a bad performance review
[01:11:44.300 --> 01:11:50.580]   or a bad day at work or feeling left out of the club or maybe he got rejected by a fellow
[01:11:50.580 --> 01:11:53.740]   coder developer who happened to be a woman.
[01:11:53.740 --> 01:11:55.100]   Could have been any of those things.
[01:11:55.100 --> 01:12:04.180]   But there's so much inexperience throughout the entire memo to me that it sounds like
[01:12:04.180 --> 01:12:09.380]   a petulant 20-year-old who doesn't have a lot of experience and doesn't know what he's
[01:12:09.380 --> 01:12:10.380]   talking about anyway.
[01:12:10.380 --> 01:12:13.500]   So I wasn't super angry or upset by the memo.
[01:12:13.500 --> 01:12:22.900]   Now what's been interesting is the reaction to that memo, which to me almost feels a little
[01:12:22.900 --> 01:12:24.740]   inauthentic.
[01:12:24.740 --> 01:12:29.180]   It feels as though in the year 20, what was the year that movie PCU came out?
[01:12:29.180 --> 01:12:30.180]   Do you guys remember that movie?
[01:12:30.180 --> 01:12:31.180]   It was like early-
[01:12:31.180 --> 01:12:34.660]   It was quite some time ago, it was like '94-'95.
[01:12:34.660 --> 01:12:38.060]   I feel like we've circled all of history sites.
[01:12:38.060 --> 01:12:47.500]   Yeah, I feel like we've hit that cycle again where our responses are expected and I feel
[01:12:47.500 --> 01:12:50.980]   like some of the responses that I've seen to this memo have almost been formulaic.
[01:12:50.980 --> 01:12:53.620]   What about Google's response though?
[01:12:53.620 --> 01:12:56.500]   That's the one that really matters.
[01:12:56.500 --> 01:13:00.180]   And some people have been critical about the Google management's response to this, which
[01:13:00.180 --> 01:13:08.100]   was we want to make a safe place at Google for all beliefs, including these beliefs.
[01:13:08.100 --> 01:13:09.100]   Which is why-
[01:13:09.100 --> 01:13:10.100]   Was that official-
[01:13:10.100 --> 01:13:11.100]   Was that a press release that they-
[01:13:11.100 --> 01:13:13.580]   Yeah, that was their official response to it.
[01:13:13.580 --> 01:13:14.580]   Right, right.
[01:13:14.580 --> 01:13:16.300]   They just hired a diversity head.
[01:13:16.300 --> 01:13:20.300]   Yeah, she's literally been in the job two weeks and then this blips up.
[01:13:20.300 --> 01:13:24.820]   So it's interesting you say what you said, Amy, because before the show you mentioned
[01:13:24.820 --> 01:13:28.820]   you hadn't read Janatan Zungar's medium piece about this.
[01:13:28.820 --> 01:13:30.140]   No, no, no, I had.
[01:13:30.140 --> 01:13:31.140]   You had, okay.
[01:13:31.140 --> 01:13:32.140]   Yeah, yeah.
[01:13:32.140 --> 01:13:33.140]   So he was actually-
[01:13:33.140 --> 01:13:34.140]   He said the same thing.
[01:13:34.140 --> 01:13:35.140]   He said exactly the same thing.
[01:13:35.140 --> 01:13:40.260]   He's a very well-known Googler, very politically active.
[01:13:40.260 --> 01:13:42.100]   He has recently left Google.
[01:13:42.100 --> 01:13:44.660]   So he said, "Well, I could speak a little more freely about this than I would have if
[01:13:44.660 --> 01:13:46.340]   I were still a manager at Google."
[01:13:46.340 --> 01:13:47.820]   And he said exactly the same thing.
[01:13:47.820 --> 01:13:53.380]   He said, "When you first start coding, you are just dealing with a machine.
[01:13:53.380 --> 01:13:58.580]   But as you get more senior, you realize really the job is not coding.
[01:13:58.580 --> 01:14:04.940]   The job is solving problems and in particular interacting with other people to solve those
[01:14:04.940 --> 01:14:05.940]   problems.
[01:14:05.940 --> 01:14:08.500]   It isn't- I think you're right.
[01:14:08.500 --> 01:14:13.180]   I think a lot of young coders starting off think what coding is all about is learning
[01:14:13.180 --> 01:14:15.900]   how to instruct the machine.
[01:14:15.900 --> 01:14:21.660]   And he says essentially this is younger, engineering is all about cooperation, collaboration and
[01:14:21.660 --> 01:14:24.540]   empathy for both your colleagues and customers.
[01:14:24.540 --> 01:14:30.140]   Something that the memo writer said, "Oh, those are female traits.
[01:14:30.140 --> 01:14:33.020]   If someone told you engineering was a field where you could get away with not dealing
[01:14:33.020 --> 01:14:37.900]   with people or feelings, then I'm very sorry to tell you, you've been lied to.
[01:14:37.900 --> 01:14:42.660]   Solitary work is something that only happens at the most junior levels.
[01:14:42.660 --> 01:14:46.900]   And in fact, the traits which the manifesto described as female are the core traits which
[01:14:46.900 --> 01:14:49.060]   makes someone successful at engineering.
[01:14:49.060 --> 01:14:57.380]   But I think the bigger issue, Amy, is that the lack of diversity at Google and in Silicon
[01:14:57.380 --> 01:15:01.980]   Valley, does it hurt Google?
[01:15:01.980 --> 01:15:04.100]   Does it hurt the product?
[01:15:04.100 --> 01:15:07.540]   Well, I would say yes.
[01:15:07.540 --> 01:15:13.260]   And I would say one of the things that I'm researching most intently is artificial intelligence.
[01:15:13.260 --> 01:15:17.420]   So that's been a big part of my life for the past couple of years.
[01:15:17.420 --> 01:15:25.540]   And you can see the strange and weird ways that AI is starting to break or break down.
[01:15:25.540 --> 01:15:34.300]   And that's a result of having too few people with too narrow a worldview, trying to solve
[01:15:34.300 --> 01:15:35.900]   problems together.
[01:15:35.900 --> 01:15:42.900]   So everybody that I know that works in tech likes to talk incessantly about nature and
[01:15:42.900 --> 01:15:48.300]   yet they seem to not take some very basic cues from nature.
[01:15:48.300 --> 01:15:55.220]   Nature tells us that biodiversity is good for the ecosystem.
[01:15:55.220 --> 01:16:03.820]   And it's always good to introduce complimentary species and life forms to be together.
[01:16:03.820 --> 01:16:11.380]   What I find so interesting is that we see study after study and anecdotally and evidence
[01:16:11.380 --> 01:16:13.340]   all around the biodiversity is great.
[01:16:13.340 --> 01:16:20.460]   And yet we don't seem to take that advice when it comes to staffing the offices.
[01:16:20.460 --> 01:16:21.460]   It's a lot of culture.
[01:16:21.460 --> 01:16:23.620]   Where the stuff is being built.
[01:16:23.620 --> 01:16:28.340]   Now, it's difficult.
[01:16:28.340 --> 01:16:32.540]   And it's hard to have this conversation without using those broad generalizations.
[01:16:32.540 --> 01:16:33.540]   Right.
[01:16:33.540 --> 01:16:38.060]   Well, that's so I want you to school me because then I'm going to walk right into a minefield
[01:16:38.060 --> 01:16:39.820]   here.
[01:16:39.820 --> 01:16:40.820]   But that's OK.
[01:16:40.820 --> 01:16:44.380]   I want to be educated here.
[01:16:44.380 --> 01:16:49.700]   It is true that I mean, and of course, every individual is a unique individual and many
[01:16:49.700 --> 01:16:55.340]   guys have more female traits, many women have more male traits, whatever.
[01:16:55.340 --> 01:16:57.580]   My wife is much butcher than I am.
[01:16:57.580 --> 01:17:02.980]   But it seems to me there are gender, are there gender differences?
[01:17:02.980 --> 01:17:06.900]   Well, I think it depends on who you ask, right?
[01:17:06.900 --> 01:17:13.820]   I think Dinesh D'Souza, who's a right wing conservative writer, would tell you that stereotypes
[01:17:13.820 --> 01:17:17.140]   are born out of fact and that they exist for a reason.
[01:17:17.140 --> 01:17:21.140]   I think that any sociologist who's had any real training would tell you that it depends
[01:17:21.140 --> 01:17:22.660]   on the data set.
[01:17:22.660 --> 01:17:28.220]   And gender is not just influenced biologically, but also by the cultural norms and standards
[01:17:28.220 --> 01:17:30.460]   in each country and region and everything else.
[01:17:30.460 --> 01:17:32.500]   And yeah, you have kids, right?
[01:17:32.500 --> 01:17:33.900]   Yeah, I have a kid.
[01:17:33.900 --> 01:17:34.900]   Yeah.
[01:17:34.900 --> 01:17:36.580]   And I'm a boy and a girl.
[01:17:36.580 --> 01:17:42.500]   And I'm pretty convinced that we did not in any way try to push them in any direction,
[01:17:42.500 --> 01:17:52.140]   but the boy liked playing with guns and balls and the girl did not.
[01:17:52.140 --> 01:17:53.220]   And this was out of the womb.
[01:17:53.220 --> 01:17:54.780]   This was very early on.
[01:17:54.780 --> 01:17:58.180]   I don't think this was socialization.
[01:17:58.180 --> 01:18:02.420]   So there's a massive debate over nature versus nurture.
[01:18:02.420 --> 01:18:06.700]   And it's very hard to know what's nature and what's nurture.
[01:18:06.700 --> 01:18:10.100]   I guess, look, I think we do need diversity.
[01:18:10.100 --> 01:18:13.780]   Clearly we need diversity in Silicon Valley and everywhere.
[01:18:13.780 --> 01:18:17.940]   Everywhere is, every workplace is better if it's a diverse workplace.
[01:18:17.940 --> 01:18:21.300]   It's just better for everything, for everybody in the workplace.
[01:18:21.300 --> 01:18:22.340]   I know that.
[01:18:22.340 --> 01:18:24.660]   That's true.
[01:18:24.660 --> 01:18:28.740]   But part of the reason that's true is because people bring different flavors to it.
[01:18:28.740 --> 01:18:34.820]   I don't think it's unreasonable to observe that women bring different flavors than men.
[01:18:34.820 --> 01:18:39.140]   Or is that really a bad thing to look at?
[01:18:39.140 --> 01:18:42.100]   Here's what I would say.
[01:18:42.100 --> 01:18:44.100]   Humans are not wired for change.
[01:18:44.100 --> 01:18:49.380]   We are, our limbic systems react very poorly when the status quo is challenged.
[01:18:49.380 --> 01:18:51.700]   That puts us into fight or flight mode.
[01:18:51.700 --> 01:18:54.740]   And it causes us to make bad decisions.
[01:18:54.740 --> 01:18:58.660]   And more importantly, it causes us to pine for the way that things used to be.
[01:18:58.660 --> 01:19:02.460]   It's because of our limbic systems that a lot of people are not good at thinking about the
[01:19:02.460 --> 01:19:03.460]   future.
[01:19:03.460 --> 01:19:11.300]   However, if we look from a statistical standpoint, that change is often better.
[01:19:11.300 --> 01:19:15.540]   And that it helps us approach and understand the future in a better way.
[01:19:15.540 --> 01:19:20.580]   What's happening right now is that technology has reached a certain inflection point.
[01:19:20.580 --> 01:19:24.300]   We have enough people who are trained that can go into the field.
[01:19:24.300 --> 01:19:29.180]   We have enough compute power to be able to work on meaningful projects.
[01:19:29.180 --> 01:19:37.260]   And because of how things have always been, those roles are predominantly dominated by
[01:19:37.260 --> 01:19:43.020]   white males and males from a few other countries.
[01:19:43.020 --> 01:19:45.780]   So this is really about change.
[01:19:45.780 --> 01:19:47.460]   And it happens through every field.
[01:19:47.460 --> 01:19:52.300]   If you look throughout time, it's happening in every field and in every case.
[01:19:52.300 --> 01:19:58.340]   And once that change happens and the workforce has become more diversified, they tend to
[01:19:58.340 --> 01:19:59.340]   be more productive.
[01:19:59.340 --> 01:20:00.340]   They do better.
[01:20:00.340 --> 01:20:01.340]   All this data showing that.
[01:20:01.340 --> 01:20:07.900]   So I think when it comes down to it, we're just dealing with the latest round of this.
[01:20:07.900 --> 01:20:11.980]   And we feel it and sense it acutely because it's the field that we all pay attention.
[01:20:11.980 --> 01:20:15.580]   That us, the people who are on the show and who are listening to this show and watching
[01:20:15.580 --> 01:20:19.860]   it, it feels acute because we pay attention to it.
[01:20:19.860 --> 01:20:26.900]   But it's the same story that's been written over and over again in law and in academia.
[01:20:26.900 --> 01:20:30.380]   And in all of these other different fields and industries.
[01:20:30.380 --> 01:20:35.980]   One of the transitions that happened this year, particularly affected us, of course,
[01:20:35.980 --> 01:20:38.420]   is the loss of the great science fiction.
[01:20:38.420 --> 01:20:42.340]   Author and regular contributors who twit Jerry Pournell.
[01:20:42.340 --> 01:20:47.500]   And I just, I asked the team, it happened a few days before the episode, "Can you put
[01:20:47.500 --> 01:20:48.900]   something together?"
[01:20:48.900 --> 01:20:55.500]   And I'm just so grateful to, it was Victor Bognot, Carsten Bondi made some suggestions.
[01:20:55.500 --> 01:20:57.500]   I made some suggestions.
[01:20:57.500 --> 01:20:58.980]   And they just did a beautiful job.
[01:20:58.980 --> 01:21:05.340]   We thought, boy, we can't end the year without a revisit to our tribute to our great Jerry
[01:21:05.340 --> 01:21:09.580]   Pournell watch.
[01:21:09.580 --> 01:21:14.180]   Our guest today is somebody that I have, I've been a fan of literally for years ever
[01:21:14.180 --> 01:21:16.060]   since his column in Bite Magazine.
[01:21:16.060 --> 01:21:18.220]   And he certainly influenced my career.
[01:21:18.220 --> 01:21:19.220]   Let's welcome Jerry Pournell.
[01:21:19.220 --> 01:21:24.860]   A lot of people couldn't figure out why in the world was I better known than anybody
[01:21:24.860 --> 01:21:28.540]   else in the computer business, except maybe the war.
[01:21:28.540 --> 01:21:30.540]   I wrote the Field and Stream column.
[01:21:30.540 --> 01:21:31.980]   Me and Joe went hunting.
[01:21:31.980 --> 01:21:33.740]   Well, me and old Zeke went compute.
[01:21:33.740 --> 01:21:37.740]   And that was the great secret I never told anybody.
[01:21:37.740 --> 01:21:43.820]   Great many, if not most, scientists were heavily influenced by sci-fi and focused their research
[01:21:43.820 --> 01:21:49.420]   on areas that science fiction inspired them to study, which is why, in so many ways, modern
[01:21:49.420 --> 01:21:53.340]   scientific advances parallel what you guys are writing about.
[01:21:53.340 --> 01:21:59.580]   When you get around to listening to the molten gods, I pay attention to the pocket computer.
[01:21:59.580 --> 01:22:04.940]   I wrote that in 1972, and iPhone does most of what it says in there.
[01:22:04.940 --> 01:22:10.740]   But of course we had to set that a long way away because even in 1972, nobody thought
[01:22:10.740 --> 01:22:12.220]   that I would live to see...
[01:22:12.220 --> 01:22:13.700]   Isn't it amazing?
[01:22:13.700 --> 01:22:18.980]   Most of the people in this world accept fruits of technology in about the same way as a kitten
[01:22:18.980 --> 01:22:22.660]   except milk when you pour it into a bowl for it.
[01:22:22.660 --> 01:22:26.540]   There was a common phrase in the robot industry.
[01:22:26.540 --> 01:22:33.300]   You never understand how smart a moron is until you try to program a robot.
[01:22:33.300 --> 01:22:38.940]   It towards the end of the Soviet Union and Panoevich, which was doing the handwriting
[01:22:38.940 --> 01:22:41.740]   recognition software for Microsoft.
[01:22:41.740 --> 01:22:48.500]   He said, "My biggest problem is I don't have enough technical handwriting than I got my
[01:22:48.500 --> 01:22:50.500]   logbook out."
[01:22:50.500 --> 01:22:56.740]   That's the technical handwriting that was the standard, and that engine has been in
[01:22:56.740 --> 01:23:02.600]   every edition of a Microsoft handwriting recognition program since 1990.
[01:23:02.600 --> 01:23:03.900]   How is your handwriting, Jerry?
[01:23:03.900 --> 01:23:04.900]   Are you...
[01:23:04.900 --> 01:23:05.900]   It's god awful.
[01:23:05.900 --> 01:23:06.900]   Okay, great.
[01:23:06.900 --> 01:23:07.900]   It is really terrible.
[01:23:07.900 --> 01:23:14.300]   Most projects are this, go out to some place like Edwards or some awful place where nobody
[01:23:14.300 --> 01:23:15.300]   wants to be.
[01:23:15.300 --> 01:23:21.260]   Go out there and you tell them, "I want you to build the best whatever it is you can build
[01:23:21.260 --> 01:23:24.380]   with technology as of this afternoon."
[01:23:24.380 --> 01:23:26.100]   You build three copies of it.
[01:23:26.100 --> 01:23:29.340]   We test one to the edge of it and we're probably praying it.
[01:23:29.340 --> 01:23:34.220]   The second one we fly in because we learn from the first until we get all the information
[01:23:34.220 --> 01:23:35.220]   out of it.
[01:23:35.220 --> 01:23:37.820]   The third one ends up in the Smithsonian.
[01:23:37.820 --> 01:23:40.900]   Yeah, Jerry's the wordy one.
[01:23:40.900 --> 01:23:44.460]   I, Larry, has told a perfectly good story.
[01:23:44.460 --> 01:23:47.100]   It would be publishable now.
[01:23:47.100 --> 01:23:53.140]   But when I'm finished with it, it will be probably half again, maybe twice as long,
[01:23:53.140 --> 01:23:57.780]   and then he goes over it and finds some scene that what everybody will remember and they'll
[01:23:57.780 --> 01:24:00.140]   forget that I ever had any fun in it.
[01:24:00.140 --> 01:24:01.140]   What the hell?
[01:24:01.140 --> 01:24:04.580]   I've always been an operation research guy.
[01:24:04.580 --> 01:24:10.820]   That's a guy who knows less and less about more and more until he knows nothing at all
[01:24:10.820 --> 01:24:13.220]   about everything.
[01:24:13.220 --> 01:24:14.220]   One last question.
[01:24:14.220 --> 01:24:15.220]   You ever going to write your memoir?
[01:24:15.220 --> 01:24:16.420]   Maybe I'll go in a sense.
[01:24:16.420 --> 01:24:18.180]   I'm writing it now, aren't I?
[01:24:18.180 --> 01:24:19.980]   Maybe after a year or two we'd have the whole thing.
[01:24:19.980 --> 01:24:20.980]   No.
[01:24:20.980 --> 01:24:21.980]   Maybe take a little longer than that.
[01:24:21.980 --> 01:24:26.980]   How many people do you know whose personal computer has been on display in the Smithsonian?
[01:24:26.980 --> 01:24:28.980]   You're the only one I know of.
[01:24:28.980 --> 01:24:30.780]   Yeah, probably all of them.
[01:24:30.780 --> 01:24:33.180]   I know all of them.
[01:24:33.180 --> 01:24:42.660]   Great doctor Jerry Pournell was an honor to consider him a friend.
[01:24:42.660 --> 01:24:44.660]   We'll miss him.
[01:24:44.660 --> 01:24:46.180]   Thank you, Jerry.
[01:24:46.180 --> 01:24:51.860]   And Godspeed, Edgett22 made an amazing discovery.
[01:24:51.860 --> 01:24:57.100]   She looked at Kentucky Fried Chicken's official Twitter account @KFC and noticed it followed
[01:24:57.100 --> 01:24:59.980]   exactly 11 people.
[01:24:59.980 --> 01:25:03.300]   There's Spice Girls and six guys named Herb.
[01:25:03.300 --> 01:25:04.300]   Oh my God.
[01:25:04.300 --> 01:25:07.100]   11 herbs and spices.
[01:25:07.100 --> 01:25:10.940]   This probably is the most viral tweet of the last five years.
[01:25:10.940 --> 01:25:13.140]   The Spice Girls would pronounce it herbs.
[01:25:13.140 --> 01:25:14.140]   That's true.
[01:25:14.140 --> 01:25:15.140]   Herbs.
[01:25:15.140 --> 01:25:16.140]   That's true.
[01:25:16.140 --> 01:25:18.380]   I've got 11 herbs and spices.
[01:25:18.380 --> 01:25:20.380]   That is amazing.
[01:25:20.380 --> 01:25:24.500]   Is that the kudos to the social media director?
[01:25:24.500 --> 01:25:25.500]   The Louisville Company.
[01:25:25.500 --> 01:25:26.500]   Good job sir.
[01:25:26.500 --> 01:25:29.820]   Actually, I know they're social media directors so I'm going to ask about that.
[01:25:29.820 --> 01:25:30.820]   Because that's incredible.
[01:25:30.820 --> 01:25:32.060]   Find out if he's the guy.
[01:25:32.060 --> 01:25:33.060]   That would be a great story.
[01:25:33.060 --> 01:25:35.340]   He's the guy who thought of this.
[01:25:35.340 --> 01:25:36.340]   It's a lady.
[01:25:36.340 --> 01:25:37.540]   Okay, it's an Easter egg though, right?
[01:25:37.540 --> 01:25:39.340]   You do this and you don't tell anybody.
[01:25:39.340 --> 01:25:40.340]   Oh, amazing.
[01:25:40.340 --> 01:25:42.060]   He waits for somebody to notice it.
[01:25:42.060 --> 01:25:44.380]   But you said social media director, which is the right thing.
[01:25:44.380 --> 01:25:47.100]   People were like, oh my gosh, that social media intern is so great.
[01:25:47.100 --> 01:25:50.420]   And then people that work on social media were like, not the intern.
[01:25:50.420 --> 01:25:53.220]   That takes actual thought planning execution.
[01:25:53.220 --> 01:25:56.740]   Don't diminish the value of the work that just went viral on broad KFC.
[01:25:56.740 --> 01:25:59.100]   Not just attention on social media, but right here.
[01:25:59.100 --> 01:26:01.900]   I'm not saying KFC, which I don't say ever.
[01:26:01.900 --> 01:26:02.900]   But good for them.
[01:26:02.900 --> 01:26:06.860]   You'd have to and actually, Ed Jett's pretty smart because this is the people that KFC
[01:26:06.860 --> 01:26:07.860]   is following.
[01:26:07.860 --> 01:26:10.180]   You'd have to kind of say, Jerry Hornell, that's Jerry Halliwell.
[01:26:10.180 --> 01:26:11.420]   That was this vice girl.
[01:26:11.420 --> 01:26:12.420]   Wait a minute.
[01:26:12.420 --> 01:26:13.420]   Official Mel B.
[01:26:13.420 --> 01:26:14.420]   She was a Spice girl.
[01:26:14.420 --> 01:26:15.420]   Wait a minute.
[01:26:15.420 --> 01:26:18.660]   One, two, three, four, five, six, five Spice Girls.
[01:26:18.660 --> 01:26:19.660]   Where are the herbs?
[01:26:19.660 --> 01:26:23.980]   And then Herb Scribner, Herb Wesson, Herb Waters, Herb Dean, Random Herb.
[01:26:23.980 --> 01:26:26.980]   And Herb and Herb, and Herb Apper.
[01:26:26.980 --> 01:26:27.980]   All verified herbs.
[01:26:27.980 --> 01:26:29.700]   He's a great trumpet player actually.
[01:26:29.700 --> 01:26:30.860]   Oh, they are all big chicks.
[01:26:30.860 --> 01:26:31.860]   All verified herbs.
[01:26:31.860 --> 01:26:34.260]   Oh, they all, they did all verified accounts too.
[01:26:34.260 --> 01:26:35.980]   All verified herbs and spices.
[01:26:35.980 --> 01:26:38.740]   Well, you are legitimate, right?
[01:26:38.740 --> 01:26:40.940]   You can't just follow.
[01:26:40.940 --> 01:26:45.500]   Because milkshake duck, your herb can become, if you're, you've seen that thing.
[01:26:45.500 --> 01:26:47.780]   Oh, so milkshake duck is a meme.
[01:26:47.780 --> 01:26:49.540]   It says like, oh, we all love milkshake duck.
[01:26:49.540 --> 01:26:52.620]   And then five minutes goodbye, we're sorry to inform you that milkshake duck is racist.
[01:26:52.620 --> 01:26:56.020]   Whenever the internet finds something that's lovely and nice, it turns out to be bad.
[01:26:56.020 --> 01:26:59.780]   So if you found a herb that was non-verified, he may become milkshake duck and actually
[01:26:59.780 --> 01:27:02.940]   racist and KFC is to apologize, thus ruining the moment.
[01:27:02.940 --> 01:27:05.940]   So by following verified accounts, which apply a lower level of that happening.
[01:27:05.940 --> 01:27:06.940]   Yes.
[01:27:06.940 --> 01:27:09.220]   Anyway, social media is hard and that was hilarious.
[01:27:09.220 --> 01:27:10.700]   Media is hard.
[01:27:10.700 --> 01:27:12.980]   Kudos to anyone who has to manage social media.
[01:27:12.980 --> 01:27:14.180]   I can't even manage my own.
[01:27:14.180 --> 01:27:15.740]   You're pretty good on Twitter.
[01:27:15.740 --> 01:27:20.020]   So milkshake duck in the urban dictionary is defined as a person who rapidly becomes
[01:27:20.020 --> 01:27:25.020]   famous for something wholesome before they're revealed as a deeply flawed character with
[01:27:25.020 --> 01:27:27.220]   terrible opinions and or shady past.
[01:27:27.220 --> 01:27:28.220]   Welcome to human.
[01:27:28.220 --> 01:27:29.220]   Yeah.
[01:27:29.220 --> 01:27:30.220]   Do you remember Ken bone?
[01:27:30.220 --> 01:27:31.220]   Remember Ken bone?
[01:27:31.220 --> 01:27:34.300]   Ken bone and the sweater boy from the, from the base.
[01:27:34.300 --> 01:27:36.820]   All his adult video habits from Reddit were found out.
[01:27:36.820 --> 01:27:37.820]   Oh, poor Ken bone.
[01:27:37.820 --> 01:27:38.820]   Yep, exactly.
[01:27:38.820 --> 01:27:39.820]   Don't become famous.
[01:27:39.820 --> 01:27:40.820]   Don't do it.
[01:27:40.820 --> 01:27:41.820]   In cognitive mode.
[01:27:41.820 --> 01:27:42.820]   Yeah.
[01:27:42.820 --> 01:27:45.180]   Just don't leave comments on Reddit on porn under your own name.
[01:27:45.180 --> 01:27:46.420]   In cognitive mode.
[01:27:46.420 --> 01:27:52.100]   Well, the good news for Apple is we have a number of data points.
[01:27:52.100 --> 01:27:57.580]   Of course, Apple's own quarterly results came out the day before the iPhone 10 shipped and
[01:27:57.580 --> 01:28:03.420]   they say they had amazing sales of iPhones.
[01:28:03.420 --> 01:28:11.620]   The iPhone sold 46.6 million, 46.6 million units in the fourth quarter.
[01:28:11.620 --> 01:28:15.980]   Much of those probably iPhone eight.
[01:28:15.980 --> 01:28:23.100]   So iPhone eight was a success and then of course slice, which is an interesting story.
[01:28:23.100 --> 01:28:24.100]   I don't know.
[01:28:24.100 --> 01:28:26.380]   I'd love to hear whether you guys trust it or not.
[01:28:26.380 --> 01:28:34.140]   I use slice slices and app that monitors your Gmail to see of its customers and there
[01:28:34.140 --> 01:28:40.660]   are I guess millions of users monitors Gmail to see if your delivery is coming and when
[01:28:40.660 --> 01:28:41.660]   it's coming.
[01:28:41.660 --> 01:28:47.540]   Advanced information admittedly a sample but I think a fairly large sample size and they
[01:28:47.540 --> 01:28:54.100]   say according to their customers, their users, it's the largest product launch in the company's
[01:28:54.100 --> 01:28:55.580]   history.
[01:28:55.580 --> 01:28:56.580]   This is the graph.
[01:28:56.580 --> 01:29:01.020]   The iPhone eight and eight plus kind of one of the smallest product launches in the company
[01:29:01.020 --> 01:29:02.020]   history.
[01:29:02.020 --> 01:29:05.940]   But the iPhone 10 even bested the iPhone six, which was huge because it was the first big
[01:29:05.940 --> 01:29:06.940]   iPhone.
[01:29:06.940 --> 01:29:10.140]   This, you know, this is pretty significant.
[01:29:10.140 --> 01:29:15.500]   Do you Brian, are those useful numbers you think are these I feel like slices is useful?
[01:29:15.500 --> 01:29:18.340]   You know, I haven't been an Apple reporter in several years.
[01:29:18.340 --> 01:29:19.540]   So I really don't remember.
[01:29:19.540 --> 01:29:21.460]   But are they scanning receipts or something?
[01:29:21.460 --> 01:29:22.460]   Yeah.
[01:29:22.460 --> 01:29:24.460]   Is that how it works?
[01:29:24.460 --> 01:29:28.540]   I think Apple has disputed those metrics before like just from what I remember, they
[01:29:28.540 --> 01:29:29.540]   don't love them.
[01:29:29.540 --> 01:29:33.700]   And I'm not sure if it's because they're too accurate or if they're totally wildly off.
[01:29:33.700 --> 01:29:39.100]   But I do think that early orders are definitely a good metric for, you know, early adopter
[01:29:39.100 --> 01:29:45.500]   interests and just, you know, like an indicator of whether or not a product is very interesting
[01:29:45.500 --> 01:29:47.020]   to technology enthusiasts.
[01:29:47.020 --> 01:29:50.420]   You know, I don't know if it really tells us much about the long term over the course
[01:29:50.420 --> 01:29:54.860]   of a year, but it definitely is interesting to see the iPhone 10 and so much excitement
[01:29:54.860 --> 01:29:55.860]   over it.
[01:29:55.860 --> 01:29:56.860]   Yeah.
[01:29:56.860 --> 01:30:02.260]   Well, I think three out of the four of us have one in our hands.
[01:30:02.260 --> 01:30:06.180]   Ed bot is still using his Nokia 1050.
[01:30:06.180 --> 01:30:08.580]   But no, I'm teasing.
[01:30:08.580 --> 01:30:13.420]   But I wanted to hate this phone.
[01:30:13.420 --> 01:30:17.260]   For the price, for the attention it was getting, I just wanted to hate this phone.
[01:30:17.260 --> 01:30:20.620]   I wanted to find something wrong with it and I can't find anything wrong with it.
[01:30:20.620 --> 01:30:23.300]   So do you use Animoji?
[01:30:23.300 --> 01:30:24.300]   I have sent some.
[01:30:24.300 --> 01:30:26.300]   That is not going to be a killer.
[01:30:26.300 --> 01:30:27.300]   A feature.
[01:30:27.300 --> 01:30:29.540]   I've sent a few.
[01:30:29.540 --> 01:30:33.980]   And of course, our friend Harry McCracken claims to have invented something that is really
[01:30:33.980 --> 01:30:38.460]   going to end the world sooner, which is Animoji karaoke.
[01:30:38.460 --> 01:30:40.740]   I don't know if he was he the first one to do it.
[01:30:40.740 --> 01:30:43.340]   He claims to have invented it.
[01:30:43.340 --> 01:30:44.340]   Yeah.
[01:30:44.340 --> 01:30:50.140]   Yeah, I don't know either, but I'll play a little bit for you if in case you somehow
[01:30:50.140 --> 01:30:52.780]   have missed the excitement of animoji karaoke.
[01:30:52.780 --> 01:30:57.580]   It's actually kind of a hard thing to do because what you have to do is record using
[01:30:57.580 --> 01:31:03.660]   the screen recorder because you're limited to 10 seconds.
[01:31:03.660 --> 01:31:04.660]   No, this is an ad.
[01:31:04.660 --> 01:31:05.940]   Let's see if I can find it.
[01:31:05.940 --> 01:31:11.100]   I think that Renee has an actual how to on the Imar site of how to do this yourself.
[01:31:11.100 --> 01:31:15.940]   If you so wish to torch the rest of the world with an Animoji.
[01:31:15.940 --> 01:31:17.660]   And maybe Renee's found a better way to do it.
[01:31:17.660 --> 01:31:23.340]   But what I was told is that you have to use the screen recorder and then edit it out,
[01:31:23.340 --> 01:31:26.620]   crop out the UI stuff.
[01:31:26.620 --> 01:31:31.540]   And then you can, according to Buzzfeed, Harry McCracken invented it.
[01:31:31.540 --> 01:31:36.740]   And then you can, and then you can mix it into a, here's Harry's first or one of his
[01:31:36.740 --> 01:31:37.740]   first.
[01:31:37.740 --> 01:31:43.380]   So he's got a rabbit and a chicken singing.
[01:31:43.380 --> 01:31:46.180]   Oh, that's good though.
[01:31:46.180 --> 01:31:50.620]   I just singing blue sweets hooked on a feeling.
[01:31:50.620 --> 01:31:58.820]   Now I just want to remind ASCAP BMI that this is in use in service of news and commentary.
[01:31:58.820 --> 01:32:02.100]   Being used for any aesthetic value.
[01:32:02.100 --> 01:32:05.780]   I think it's pretty funny.
[01:32:05.780 --> 01:32:07.500]   So how does Renee's technique?
[01:32:07.500 --> 01:32:10.220]   Does he have to use the screen recorder and all that?
[01:32:10.220 --> 01:32:11.620]   Yeah, I think he's doing also.
[01:32:11.620 --> 01:32:12.620]   Yeah.
[01:32:12.620 --> 01:32:14.060]   Here's another one from Harry.
[01:32:14.060 --> 01:32:20.820]   This is the first one.
[01:32:20.820 --> 01:32:26.020]   It's Bugs Bunny and Elmer Fudd.
[01:32:26.020 --> 01:32:29.620]   It doesn't apparently exist in Elmer Fudd and emoji.
[01:32:29.620 --> 01:32:30.420]   It's kind of a limited.
[01:32:30.420 --> 01:32:34.420]   Actually, there's some concern about it.
[01:32:34.420 --> 01:32:38.820]   Believe it or not, I managed to find all the negatives.
[01:32:38.820 --> 01:32:46.540]   There's some concern about this because Apple, of course, Apple says your, the Face ID information
[01:32:46.540 --> 01:32:50.620]   is stored in the secure enclave is not available to anybody else.
[01:32:50.620 --> 01:32:51.620]   We don't see it.
[01:32:51.620 --> 01:32:52.620]   Only the phone sees it.
[01:32:52.620 --> 01:32:54.900]   And it does a very good job of recognizing you.
[01:32:54.900 --> 01:32:57.260]   I'm actually pretty impressed with Face ID.
[01:32:57.260 --> 01:33:02.180]   I wasn't, you know, it's as good as it could be, I think, in the circumstances.
[01:33:02.180 --> 01:33:06.700]   But there's some concern because there is an API for third party developers to use the
[01:33:06.700 --> 01:33:09.500]   cameras in the notch.
[01:33:09.500 --> 01:33:11.660]   I imagine Snapchat would be one of the first, right?
[01:33:11.660 --> 01:33:17.700]   It wouldn't that be a great way to get those Snapchat characters, use the notch and do
[01:33:17.700 --> 01:33:19.780]   a better job of it.
[01:33:19.780 --> 01:33:24.860]   And there's nothing to stop them from exfiltrating that information from the phone to the Snapchat.
[01:33:24.860 --> 01:33:29.620]   That servers, all the information delivered by the notch.
[01:33:29.620 --> 01:33:30.620]   So there's some.
[01:33:30.620 --> 01:33:32.180]   So you choose to say yes or no.
[01:33:32.180 --> 01:33:33.820]   You have to prove to an application.
[01:33:33.820 --> 01:33:37.780]   So you would approve it if you feel comfortable with them, maybe knowing how you feel when
[01:33:37.780 --> 01:33:39.340]   you look at their app.
[01:33:39.340 --> 01:33:40.340]   Yeah.
[01:33:40.340 --> 01:33:41.340]   I think that's fair.
[01:33:41.340 --> 01:33:46.580]   And the ACLU and the Center for Democracy and Technology say, well, maybe this is a problem.
[01:33:46.580 --> 01:33:47.580]   I don't know.
[01:33:47.580 --> 01:33:49.620]   You have to really stretch.
[01:33:49.620 --> 01:33:51.380]   You're right.
[01:33:51.380 --> 01:33:52.380]   You're right.
[01:33:52.380 --> 01:33:54.300]   You have to prove it.
[01:33:54.300 --> 01:33:58.060]   ACLU says a lot of things about Face ID.
[01:33:58.060 --> 01:34:00.060]   They don't like it.
[01:34:00.060 --> 01:34:01.660]   But something to be aware of.
[01:34:01.660 --> 01:34:08.940]   Senior policy analyst at the ACLU says, you know, a bad guy could, you know, what?
[01:34:08.940 --> 01:34:10.860]   Get your face.
[01:34:10.860 --> 01:34:12.180]   Big deal.
[01:34:12.180 --> 01:34:14.420]   All right.
[01:34:14.420 --> 01:34:17.500]   I'm trying to find negatives.
[01:34:17.500 --> 01:34:18.500]   I really am.
[01:34:18.500 --> 01:34:19.500]   It's not easy.
[01:34:19.500 --> 01:34:22.660]   It's interesting because Face ID misses me when I'm grumpy.
[01:34:22.660 --> 01:34:27.320]   Like if I have a grumpy face, it will miss me and if my hair is in my face, it'll miss
[01:34:27.320 --> 01:34:28.320]   you.
[01:34:28.320 --> 01:34:31.780]   You have a choice of having it require that you're paying attention, looking at the phone
[01:34:31.780 --> 01:34:33.020]   or not.
[01:34:33.020 --> 01:34:35.820]   I would imagine it's a little easier if you don't turn that on.
[01:34:35.820 --> 01:34:41.660]   I've left mine on so somebody can't sneak up behind you and I don't care about that.
[01:34:41.660 --> 01:34:45.100]   And grab your phone and then have to try to unlock it, not just steal the phone.
[01:34:45.100 --> 01:34:47.300]   And all you would have to do is go, I'm grumpy.
[01:34:47.300 --> 01:34:48.500]   All I have to do is be grumpy.
[01:34:48.500 --> 01:34:52.940]   That's it, which I probably would look shocked if someone, you know, smashed my, like that
[01:34:52.940 --> 01:34:54.580]   would probably be enough to stop.
[01:34:54.580 --> 01:34:57.300]   I have some anecdotal evidence that it does get better.
[01:34:57.300 --> 01:35:03.100]   Apple said it would get better because occasionally it doesn't recognize me.
[01:35:03.100 --> 01:35:06.020]   When I was, I did it first with glasses, but I didn't have glasses.
[01:35:06.020 --> 01:35:07.020]   It didn't.
[01:35:07.020 --> 01:35:08.020]   I entered the code.
[01:35:08.020 --> 01:35:10.220]   The, so then you have to use your pen.
[01:35:10.220 --> 01:35:12.860]   But from then on, it recognized me with glasses.
[01:35:12.860 --> 01:35:15.420]   So I have a feeling it's updating all the time.
[01:35:15.420 --> 01:35:16.420]   Yes.
[01:35:16.420 --> 01:35:18.660]   Apple said it would be and I think there is evidence is doing it.
[01:35:18.660 --> 01:35:19.660]   Same thing.
[01:35:19.660 --> 01:35:22.300]   The studio lighting confused it at first, not anymore.
[01:35:22.300 --> 01:35:23.300]   It gets better.
[01:35:23.300 --> 01:35:24.300]   It's pretty impressive.
[01:35:24.300 --> 01:35:25.300]   Yeah.
[01:35:25.300 --> 01:35:30.460]   It's to the point where a brother, a brother, you know, did the exact same thing.
[01:35:30.460 --> 01:35:35.380]   So they had similar faces and two brothers unlocked the phone by, you know, having the
[01:35:35.380 --> 01:35:39.540]   phone relearn the new face, which was actually the brother's face, which was similar enough
[01:35:39.540 --> 01:35:40.900]   for the phone to be mixed up.
[01:35:40.900 --> 01:35:45.740]   So there's probably, you know, some updates that will make it a little bit more attuned
[01:35:45.740 --> 01:35:48.260]   to that, but it is constantly trying to relearn.
[01:35:48.260 --> 01:35:52.300]   Like if I probably do the passcode with a grumpy face, it would probably notice grumpy
[01:35:52.300 --> 01:35:53.300]   Georgia.
[01:35:53.300 --> 01:35:54.300]   It doesn't.
[01:35:54.300 --> 01:35:55.300]   So it doesn't recognize you.
[01:35:55.300 --> 01:35:56.300]   It asks for the passcode.
[01:35:56.300 --> 01:35:59.420]   You enter it and I suspect it's then say, Oh, well, that was you, I guess.
[01:35:59.420 --> 01:36:02.820]   We'll add grumpy to your repertoire.
[01:36:02.820 --> 01:36:07.820]   We did try it with identical twins, Megan Maroney, one of my co hosts, tried her iPhone 10.
[01:36:07.820 --> 01:36:13.620]   She had one of her sons, Huck, unlock it with his face and then gave it to Milo Milo, was
[01:36:13.620 --> 01:36:14.620]   able to unlock it.
[01:36:14.620 --> 01:36:18.780]   This is what Apple said could happen with identical twins and they were right.
[01:36:18.780 --> 01:36:20.380]   So that's a drawback.
[01:36:20.380 --> 01:36:21.900]   You can only have one face in there.
[01:36:21.900 --> 01:36:26.100]   Unlike the fingerprint, I used to let my wife have her fingerprint on there so she can
[01:36:26.100 --> 01:36:27.860]   get to my phone when she needed to.
[01:36:27.860 --> 01:36:33.060]   Now it's, I can, you know, I have to give her the passcode.
[01:36:33.060 --> 01:36:41.460]   I fix it, ran to Australia as they are wants to do to get an iPhone 10 so they could destroy
[01:36:41.460 --> 01:36:42.820]   it.
[01:36:42.820 --> 01:36:47.060]   We had Kelsey from, I fixed it on the new screensavers yesterday.
[01:36:47.060 --> 01:36:49.140]   A couple of surprise, interest, not surprise.
[01:36:49.140 --> 01:36:50.420]   It's been interesting things.
[01:36:50.420 --> 01:36:55.180]   A, they gave it a repairability score of six, which is not bad.
[01:36:55.180 --> 01:36:56.540]   Battery almost than I thought.
[01:36:56.540 --> 01:36:58.460]   They found the batteries in two modules.
[01:36:58.460 --> 01:37:01.500]   It's an L-shaped battery with two different modules.
[01:37:01.500 --> 01:37:05.900]   And as a result, the logic board has an odd shape in his jam pack.
[01:37:05.900 --> 01:37:08.420]   They figured out it has three gigs of RAM.
[01:37:08.420 --> 01:37:11.660]   Apple never talks about that, but that seems more than adequate.
[01:37:11.660 --> 01:37:14.660]   Of course, it has the A11 Bionic as we expected.
[01:37:14.660 --> 01:37:20.100]   It actually has quite a few custom processors on it, which is kind of interesting.
[01:37:20.100 --> 01:37:22.220]   It's got a lot of smarts in here.
[01:37:22.220 --> 01:37:26.460]   There's the, you know, they give the chip numbers, but there's definitely a motion
[01:37:26.460 --> 01:37:30.460]   co-processor in there and some other things.
[01:37:30.460 --> 01:37:34.820]   It's a, for such a tiny logic board, there's a lot of stuff in there.
[01:37:34.820 --> 01:37:37.780]   All right, I don't know what else to say.
[01:37:37.780 --> 01:37:39.340]   That's the iPhone 10.
[01:37:39.340 --> 01:37:44.540]   Jeff Bezos, by the way, now officially for once and for all the richest man in the world,
[01:37:44.540 --> 01:37:45.540]   100.
[01:37:45.540 --> 01:37:46.820]   You got to show the picture, dude.
[01:37:46.820 --> 01:37:47.820]   Yeah.
[01:37:47.820 --> 01:37:48.820]   This has become a meme.
[01:37:48.820 --> 01:37:49.820]   The Bayzanator.
[01:37:49.820 --> 01:37:50.820]   Yeah.
[01:37:50.820 --> 01:37:55.860]   He's buff, but then of course it was an internet meme and there's all sorts of other images
[01:37:55.860 --> 01:38:00.940]   of him walking down looking, but is this at the Allen, this might have been the Allen
[01:38:00.940 --> 01:38:04.580]   co-meetings.
[01:38:04.580 --> 01:38:07.660]   They put the rock behind him, you know, also looking buff.
[01:38:07.660 --> 01:38:16.140]   Let me see if I can find that because Bezos did an interview this week with his brother
[01:38:16.140 --> 01:38:18.780]   and his brother and he, there you go.
[01:38:18.780 --> 01:38:22.780]   There's the picture with the rock and who's that other guy that's a Vin Diesel.
[01:38:22.780 --> 01:38:23.780]   Yeah.
[01:38:23.780 --> 01:38:24.780]   Come on, Leo.
[01:38:24.780 --> 01:38:27.100]   Bad in the, bad in the furious.
[01:38:27.100 --> 01:38:29.060]   Yeah.
[01:38:29.060 --> 01:38:35.900]   And so they were, they actually, his brother put that up on the screen and they were laughing.
[01:38:35.900 --> 01:38:36.900]   They were laughing.
[01:38:36.900 --> 01:38:40.460]   So if you look at his Jeff Bezos, his brother, he, he's been hanging out of the gym.
[01:38:40.460 --> 01:38:46.060]   I almost said I can find a picture of this.
[01:38:46.060 --> 01:38:50.580]   So he got his brother, as one would expect, got him to reveal a lot about himself.
[01:38:50.580 --> 01:38:55.580]   For instance, every summer from the time he was about six, the time he was about 17,
[01:38:55.580 --> 01:38:58.060]   Bezos would stop it.
[01:38:58.060 --> 01:38:59.500]   Autoplay video.
[01:38:59.500 --> 01:39:01.500]   Yeah.
[01:39:01.500 --> 01:39:05.420]   He would, he would go to his grandpa's.
[01:39:05.420 --> 01:39:06.780]   Pop.
[01:39:06.780 --> 01:39:08.780]   He called him where there was nothing.
[01:39:08.780 --> 01:39:12.140]   There was, there was, there was no connectivity.
[01:39:12.140 --> 01:39:17.180]   Bezos spent his summers working on his grandfather's ranch in South Texas.
[01:39:17.180 --> 01:39:20.340]   He says, I learned my resourcefulness from my grandfather.
[01:39:20.340 --> 01:39:25.940]   He tells him stories like the time his grandfather leapt.
[01:39:25.940 --> 01:39:28.060]   I don't know if I can tell this to this story.
[01:39:28.060 --> 01:39:30.660]   Justice leapt out of the, he's driving the pickup truck off the ranch.
[01:39:30.660 --> 01:39:31.780]   He leapt out of the pickup drive.
[01:39:31.780 --> 01:39:32.780]   I've done this.
[01:39:32.780 --> 01:39:35.340]   Who doesn't try to open the gate without stopping the truck.
[01:39:35.340 --> 01:39:38.980]   So he can open the gate, jump in the truck and keep going accidentally.
[01:39:38.980 --> 01:39:45.820]   The truck runs into his hand, his thumb and cuts off most of the top of his thumb.
[01:39:45.820 --> 01:39:51.620]   They rush to the hospital and pop Jeff's grandfather says, he's so mad.
[01:39:51.620 --> 01:39:54.060]   He says the thumb was hanging by a thread.
[01:39:54.060 --> 01:39:59.300]   He's so mad he takes the thumb, rips it off, throws it into the bushes, goes to the hospital
[01:39:59.300 --> 01:40:01.300]   and says, I don't want my thumb reattached.
[01:40:01.300 --> 01:40:08.100]   Just take some skin from my butt and put it over the hole and ever after Bezos is telling
[01:40:08.100 --> 01:40:12.940]   the story, he'd have butt hair growing off his thumb that he had to, when he shaved,
[01:40:12.940 --> 01:40:15.940]   he would shave his thumb.
[01:40:15.940 --> 01:40:18.300]   And now you know how he's the richest man in the world.
[01:40:18.300 --> 01:40:21.180]   I don't think the dots quite connect on that story.
[01:40:21.180 --> 01:40:26.660]   Well, the point Jeff was trying to make is I learned resourcefulness from my grandfather.
[01:40:26.660 --> 01:40:31.740]   He spent, he bought a non working combine.
[01:40:31.740 --> 01:40:32.740]   This is not Jeff.
[01:40:32.740 --> 01:40:34.900]   Jeff doesn't have the butt thumb, his grandpa is a butt thumb.
[01:40:34.900 --> 01:40:39.780]   He bought a non working combine and said, Jeff, your job, he's like Jeff is like 12, is to
[01:40:39.780 --> 01:40:41.580]   get that thing working.
[01:40:41.580 --> 01:40:43.900]   And they got, they ordered parts and Jeff worked on.
[01:40:43.900 --> 01:40:47.100]   They, some of the parts were so big, they couldn't lift them.
[01:40:47.100 --> 01:40:54.380]   So grandpa built a crane, a homemade crane so they could lift the parts into the combine.
[01:40:54.380 --> 01:41:02.100]   He said, he said, it's, this is where I learned how you invent your way out of the box.
[01:41:02.100 --> 01:41:05.020]   He says, choosing his wife, same thing.
[01:41:05.020 --> 01:41:11.020]   He, oh, she's, should I stop?
[01:41:11.020 --> 01:41:12.020]   Yeah.
[01:41:12.020 --> 01:41:18.100]   He, he says, I was looking, he was doing the Dayton thing like he had the app, he was doing
[01:41:18.100 --> 01:41:19.100]   the Dayton thing.
[01:41:19.100 --> 01:41:22.060]   And he said, I just want to find a woman who is resourceful.
[01:41:22.060 --> 01:41:23.060]   So that what was it?
[01:41:23.060 --> 01:41:26.900]   So that if I hurt myself, she can, I was, I got, I want to find this.
[01:41:26.900 --> 01:41:27.900]   It's just the funniest.
[01:41:27.900 --> 01:41:29.660]   If you get a chance, just read the interview.
[01:41:29.660 --> 01:41:32.340]   I guess that's, that's the bottom line.
[01:41:32.340 --> 01:41:35.300]   And I'm not making up the story about the thumb.
[01:41:35.300 --> 01:41:36.300]   Yeah.
[01:41:36.300 --> 01:41:37.300]   Yeah.
[01:41:37.300 --> 01:41:40.820]   The moral of that story is throw away the thing that you really need when you're mad
[01:41:40.820 --> 01:41:41.820]   at it.
[01:41:41.820 --> 01:41:43.820]   I cover it with butt hair.
[01:41:43.820 --> 01:41:48.700]   But as a kid in Montessori School, Bezos describes how teachers would have to physically
[01:41:48.700 --> 01:41:51.260]   move his chair from one task, you know, Montessori works.
[01:41:51.260 --> 01:41:54.340]   By the way, there's his brother who lifts even more than Jeff.
[01:41:54.340 --> 01:41:56.180]   Look at that.
[01:41:56.180 --> 01:41:57.780]   He was in Montessori School.
[01:41:57.780 --> 01:41:59.940]   You go from task station to task station.
[01:41:59.940 --> 01:42:05.700]   Jeff would get so focused that they had to actually pick up his chair and move him.
[01:42:05.700 --> 01:42:07.860]   He's not a fan of checking his phone at the dinner table.
[01:42:07.860 --> 01:42:09.380]   He's not big on multitasking.
[01:42:09.380 --> 01:42:12.260]   Even when it comes to his email, multitasking bothers me.
[01:42:12.260 --> 01:42:17.900]   If I'm reading my email, I just want to really be reading my email.
[01:42:17.900 --> 01:42:18.980]   Kids.
[01:42:18.980 --> 01:42:20.860]   That's how you become the richest man in the world.
[01:42:20.860 --> 01:42:23.580]   Obviously, multitasking is mostly a crock, right?
[01:42:23.580 --> 01:42:24.580]   It is.
[01:42:24.580 --> 01:42:25.580]   You can't really do it.
[01:42:25.580 --> 01:42:26.580]   Focusing is better.
[01:42:26.580 --> 01:42:28.380]   No, you can't really do it.
[01:42:28.380 --> 01:42:31.820]   It was Harry Thumb.
[01:42:31.820 --> 01:42:37.220]   You know, one of the things I take pride in on Twit and is the titles, right?
[01:42:37.220 --> 01:42:41.900]   Ever since we started Twit 600 some episodes ago, what are the 643?
[01:42:41.900 --> 01:42:46.420]   I've always tried to have interesting compelling titles, maybe not the best SEO, maybe not
[01:42:46.420 --> 01:42:50.900]   a title that makes you have to listen, but just something fun.
[01:42:50.900 --> 01:42:54.140]   I like to kind of make it almost an Easter egg.
[01:42:54.140 --> 01:42:58.220]   It's always something in the show that somebody says and it always has to do with something
[01:42:58.220 --> 01:42:59.580]   we're talking about.
[01:42:59.580 --> 01:43:03.300]   That particular story from Jeff Bezos gave us one of my favorite titles of the year, "Grandpa's
[01:43:03.300 --> 01:43:07.940]   Harry Thumb."
[01:43:07.940 --> 01:43:09.500]   I love that.
[01:43:09.500 --> 01:43:13.340]   I want to shout out to the chat room because in a way, at the end of our shows, it's always
[01:43:13.340 --> 01:43:18.340]   a competition.
[01:43:18.340 --> 01:43:21.700]   Very often, in fact, almost always, the title idea comes with the chat room.
[01:43:21.700 --> 01:43:23.460]   So thank you, chat room.
[01:43:23.460 --> 01:43:25.300]   Thank you everybody for watching.
[01:43:25.300 --> 01:43:29.020]   I'm so grateful that I get to do this show every Sunday.
[01:43:29.020 --> 01:43:30.420]   Some of my favorite journalists.
[01:43:30.420 --> 01:43:33.460]   We don't have a regular panel on this show because it's a chance for me to talk to all
[01:43:33.460 --> 01:43:34.460]   sorts of people.
[01:43:34.460 --> 01:43:37.380]   I hope you enjoy that.
[01:43:37.380 --> 01:43:38.380]   Let us know.
[01:43:38.380 --> 01:43:39.380]   Keep watching.
[01:43:39.380 --> 01:43:40.940]   It's the best way to do it.
[01:43:40.940 --> 01:43:44.420]   We will make, I promise you, some great shows for 2018.
[01:43:44.420 --> 01:43:46.700]   Have a happy New Year and safe New Year.
[01:43:46.700 --> 01:43:48.660]   We want you back next year.
[01:43:48.660 --> 01:43:54.060]   We'll be back here doing "Twit" January 7th, the whole new episode, lots of fresh ideas
[01:43:54.060 --> 01:43:55.620]   and of course, all the tech news.
[01:43:55.620 --> 01:43:58.140]   In fact, that'll be our CES edition.
[01:43:58.140 --> 01:43:59.140]   We'll see you then.
[01:43:59.140 --> 01:44:00.140]   I'm Leo LaPorte.
[01:44:00.140 --> 01:44:01.140]   Thanks for listening.
[01:44:01.140 --> 01:44:02.140]   Thanks for watching.
[01:44:02.140 --> 01:44:03.140]   Happy New Year.
[01:44:03.140 --> 01:44:04.140]   Another Twit.
[01:44:04.140 --> 01:44:05.140]   This is amazing.
[01:44:05.140 --> 01:44:12.140]   Do the Twit.
[01:44:12.140 --> 01:44:15.500]   Do it the twist. All right. Do it the twist.

