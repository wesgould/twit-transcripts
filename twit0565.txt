
[00:00:00.000 --> 00:00:04.280]   It's time for Twit this week at Tech Becky Warley and Harry McCracken are here.
[00:00:04.280 --> 00:00:08.140]   We're gonna sit down and talk about all the news, artificial intelligence.
[00:00:08.140 --> 00:00:13.680]   Are we living in a simulation? And I come off as Grumpy Cat in the new
[00:00:13.680 --> 00:00:16.400]   kitten wedding commercial. It's all coming up next.
[00:00:16.400 --> 00:00:17.400]   On Twit.
[00:00:17.400 --> 00:00:22.900]   Netcasts you love.
[00:00:22.900 --> 00:00:24.900]   From people you trust.
[00:00:27.900 --> 00:00:29.900]   This is Twit.
[00:00:29.900 --> 00:00:36.900]   Bandwidth for this week in Tech is provided by CashFly at C A C H E F L Y dot com.
[00:00:36.900 --> 00:00:44.900]   This is Twit this week in Tech.
[00:00:44.900 --> 00:00:51.400]   Episode 565 recorded Sunday June 5th 2016 Miss Honeybale.
[00:00:51.400 --> 00:00:56.400]   This week in Tech is brought to you by Carbonite.
[00:00:56.400 --> 00:01:03.400]   Keep your business safe this year. Protect files on your computer or server with automatic cloud backup from Carbonite.
[00:01:03.400 --> 00:01:06.400]   Try it free without a credit card at Carbonite dot com today.
[00:01:06.400 --> 00:01:10.400]   And use the offer code Twit to get two free bonus months if you decide to buy.
[00:01:10.400 --> 00:01:13.400]   And buy Wealthfront.
[00:01:13.400 --> 00:01:19.400]   Wealthfront is a low cost automated investment service and the most sophisticated way to invest your money.
[00:01:19.400 --> 00:01:24.400]   Whether you've got millions or you're just starting out visit wealthfront dot com slash Twit.
[00:01:24.400 --> 00:01:28.400]   And sign up to get your free personalized investment portfolio.
[00:01:28.400 --> 00:01:31.400]   That's wealthfront dot com slash Twit.
[00:01:31.400 --> 00:01:35.400]   And buy Stamps dot com.
[00:01:35.400 --> 00:01:38.400]   Start using your time more effectively with Stamps dot com.
[00:01:38.400 --> 00:01:43.400]   Use Stamps dot com to buy and print real U.S. postage the instant you need it right from your desk.
[00:01:43.400 --> 00:01:48.400]   To get my special offer go to Stamps dot com. Click the microphone and enter Twit.
[00:01:48.400 --> 00:01:52.400]   And buy IT Pro TV.
[00:01:52.400 --> 00:02:01.400]   IT Pro TV is an easy entertaining approach to online IT training for a free seven day trial and 30% off the life of your account.
[00:02:01.400 --> 00:02:06.400]   Go to itpro.tv/twit and use the code Twit30.
[00:02:06.400 --> 00:02:15.400]   It's time for Twit this weekend tech the show we covered the week's tech news.
[00:02:15.400 --> 00:02:20.400]   And this is going to I just I feel good. I'm already bathing in this show.
[00:02:20.400 --> 00:02:26.400]   I'm asking I'm soaking I'm basking in the glorious refolgence of our guests today.
[00:02:26.400 --> 00:02:33.400]   Becky Worley is here my dear friend from 20 years ago and tech TV ZD TV.
[00:02:33.400 --> 00:02:34.400]   What was that word?
[00:02:34.400 --> 00:02:35.400]   refolgence.
[00:02:35.400 --> 00:02:38.400]   That's a poly syllabic word I'm unfamiliar with.
[00:02:38.400 --> 00:02:40.400]   Only three refolgence.
[00:02:40.400 --> 00:02:43.400]   It's the glow the glow really doesn't bathing in the glow.
[00:02:43.400 --> 00:02:44.400]   Okay.
[00:02:44.400 --> 00:02:45.400]   That would be a better way to say it.
[00:02:45.400 --> 00:02:49.400]   Hey I already learned something. I'm just I am serious.
[00:02:49.400 --> 00:02:57.400]   I'm so excited about this show this week because there's an amazing amount of tech news that needs to be analyzed.
[00:02:57.400 --> 00:03:00.400]   And I really want to hear what you two big brains have to say.
[00:03:00.400 --> 00:03:03.400]   You came in bouncing with excitement.
[00:03:03.400 --> 00:03:04.400]   I am.
[00:03:04.400 --> 00:03:05.400]   It's awesome.
[00:03:05.400 --> 00:03:06.400]   It's a great news week.
[00:03:06.400 --> 00:03:06.400]   Awesome.
[00:03:06.400 --> 00:03:08.400]   Also here if you have a great news week.
[00:03:08.400 --> 00:03:10.400]   Harry McCracken is a great guy to have here.
[00:03:10.400 --> 00:03:14.400]   Fast company. The technologizer himself. Great to see you.
[00:03:14.400 --> 00:03:15.400]   Great to be here.
[00:03:15.400 --> 00:03:16.400]   Great to be here.
[00:03:16.400 --> 00:03:19.400]   What's nice is it's kind of.
[00:03:19.400 --> 00:03:21.400]   I don't know.
[00:03:21.400 --> 00:03:25.400]   I first of all friends talking but also smart friends who have great insight.
[00:03:25.400 --> 00:03:31.400]   And I thought we should probably start actually Becky thought we should probably start with artificial intelligence since we're so smart.
[00:03:31.400 --> 00:03:36.400]   Does it feel like we need the artificial lie do intelligence.
[00:03:36.400 --> 00:03:38.400]   No but the era has begun.
[00:03:38.400 --> 00:03:46.400]   I feel like there was a crescendo this week that we had sort of bots and Microsoft bringing this in and piece by piece.
[00:03:46.400 --> 00:03:49.400]   Facebook to their bots but then all of a sudden Google and IO.
[00:03:49.400 --> 00:03:50.400]   Right.
[00:03:50.400 --> 00:03:52.400]   And now it says we've been doing machine learning.
[00:03:52.400 --> 00:03:54.400]   They like to call it machine learning.
[00:03:54.400 --> 00:03:57.400]   And we're going to start.
[00:03:57.400 --> 00:03:59.400]   So I'll tell you.
[00:03:59.400 --> 00:04:02.400]   We can kick this off with the interview I did on Monday with Kevin Kelly.
[00:04:02.400 --> 00:04:05.400]   You know Kevin Kelly started wired the whole earth like planet.
[00:04:05.400 --> 00:04:09.400]   Like he's a great guy really a polymath brilliant writer.
[00:04:09.400 --> 00:04:13.400]   His new book is about the innovations in the next 20 years and how they're going to affect us.
[00:04:13.400 --> 00:04:16.400]   But he's talking about artificial intelligence and machine learning.
[00:04:16.400 --> 00:04:23.400]   He says when you get a new technology it kicks off innovation in an interesting way.
[00:04:23.400 --> 00:04:29.400]   He said when electricity became widespread in the US farmers would do things like hey there's my hand pump for water.
[00:04:29.400 --> 00:04:33.400]   What if I applied electricity to that and an invention comes out of that.
[00:04:33.400 --> 00:04:35.400]   We've seen that with the internet too.
[00:04:35.400 --> 00:04:39.400]   What if I take a magazine and let's just add a little internet to that.
[00:04:39.400 --> 00:04:40.400]   Now what do we have?
[00:04:40.400 --> 00:04:50.400]   He says we're in the era now where this is going to start happening with AI and you're seeing AI applied to all sorts of things and you're going to get all sorts of massive innovation around it.
[00:04:50.400 --> 00:04:52.400]   But he says there's all different.
[00:04:52.400 --> 00:04:56.400]   You know a lot of times we think of AI we think of a human like mind.
[00:04:56.400 --> 00:05:01.400]   And he actually had a list of all kinds of minds that would be appropriate.
[00:05:01.400 --> 00:05:04.400]   You don't think of Alexa as a human like mind.
[00:05:04.400 --> 00:05:05.400]   It's simple.
[00:05:05.400 --> 00:05:12.400]   But it's simple enough to do those basic things like shopping and what time is it instead of timer for cooking that are very useful.
[00:05:12.400 --> 00:05:15.400]   How do you define artificial intelligence Harry?
[00:05:15.400 --> 00:05:20.400]   Do you have a sense of it in your mind of what it is at its narrowest definition?
[00:05:20.400 --> 00:05:25.400]   I mean I think it's super amorphous because in the old days you're going to define it as thinking like a human.
[00:05:25.400 --> 00:05:29.400]   And a lot of the stuff that's interesting today is not all that human like.
[00:05:29.400 --> 00:05:33.400]   I think in terms of probably the technology that people use to build it.
[00:05:33.400 --> 00:05:38.400]   They are building technologies that work a little bit more like the human brain does than conventional software has.
[00:05:38.400 --> 00:05:39.400]   I'm going to get his book.
[00:05:39.400 --> 00:05:40.400]   Hold on.
[00:05:40.400 --> 00:05:43.400]   You're not necessarily trying to build something that can do all the things a human brain can do.
[00:05:43.400 --> 00:05:51.400]   In a lot of cases you're building technology to do one thing really quickly such as look at photographs and identify the objects in that photograph.
[00:05:51.400 --> 00:05:59.400]   So make decisions that we would have maybe thought of as subjective and only human that only humans were capable of in the past.
[00:05:59.400 --> 00:06:01.400]   Well there are fuzzier things.
[00:06:01.400 --> 00:06:06.400]   In the old days we thought computers is making decisions like a flowchart very binary.
[00:06:06.400 --> 00:06:11.400]   If then if you look at a photograph and you're looking to see whether there's a car in it that does not.
[00:06:11.400 --> 00:06:16.400]   Something you can flowchart that's a fuzzy decision because the car could be any car at any angle.
[00:06:16.400 --> 00:06:32.400]   And it also seems like now that we have so many devices that can handle human language processing that we can see AI in a more transparent way and it's become more realistic and possible in people's minds.
[00:06:32.400 --> 00:06:33.400]   It's everywhere.
[00:06:33.400 --> 00:06:36.400]   It's in a lot of places where you don't even particularly notice it's there but it is.
[00:06:36.400 --> 00:06:39.400]   But I really feel like it's the next big C change.
[00:06:39.400 --> 00:06:45.400]   In the 80s you have the PC and that wiped away most of the computer companies that existed ten years after that.
[00:06:45.400 --> 00:06:49.400]   You had the web that wiped away most of the PC companies.
[00:06:49.400 --> 00:06:53.400]   Then you had mobile that wiped away the phone companies.
[00:06:53.400 --> 00:07:03.400]   I feel before very long you will have companies that nail AI and survive and companies that don't figure out how to deal with AI and maybe go away or at least are nowhere near as relevant as they once were.
[00:07:03.400 --> 00:07:09.400]   If you were going to throw cold water on all of this so you could point out that the idea of AI is fairly old.
[00:07:09.400 --> 00:07:11.400]   Going back to Marvin Minsky.
[00:07:11.400 --> 00:07:14.400]   It was really a failed idea.
[00:07:14.400 --> 00:07:18.400]   Why didn't AI take off in the earliest days of its...
[00:07:18.400 --> 00:07:22.400]   I tweeted this idea that it's going to change everything.
[00:07:22.400 --> 00:07:24.400]   I've been saying that for decades.
[00:07:24.400 --> 00:07:29.400]   Michael Johnson from Pixar said we thought that in the 1980s and it didn't happen.
[00:07:29.400 --> 00:07:32.400]   And I thought it was maybe it is changing everything.
[00:07:32.400 --> 00:07:34.400]   It's just happening really slowly.
[00:07:34.400 --> 00:07:35.400]   It really is paying off now.
[00:07:35.400 --> 00:07:37.400]   And it may be a hockey stick.
[00:07:37.400 --> 00:07:40.400]   A lot of what we see is hockey stick.
[00:07:40.400 --> 00:07:42.400]   In other words it's exponential growth.
[00:07:42.400 --> 00:07:44.400]   Slow, slow, slow, slow fast.
[00:07:44.400 --> 00:07:47.400]   And that happened certainly with processors.
[00:07:47.400 --> 00:07:50.400]   They initially were very dumb, very slow, very limited.
[00:07:50.400 --> 00:07:54.400]   But when they took off they really took off and that's Moore's law.
[00:07:54.400 --> 00:07:57.400]   Yeah, I think it was Sundar Pachay.
[00:07:57.400 --> 00:08:05.400]   I've listened to so many people this week who are saying that there are three things that have come to a nice crossing point which is computational power.
[00:08:05.400 --> 00:08:08.400]   The amount of data that we now have.
[00:08:08.400 --> 00:08:09.400]   Yes.
[00:08:09.400 --> 00:08:12.400]   And then I think the third might have been natural language processing.
[00:08:12.400 --> 00:08:15.400]   But there was a third pillar that he had that listed.
[00:08:15.400 --> 00:08:20.400]   It's like it's a nexus and that we've gotten to that point where now everything can take off.
[00:08:20.400 --> 00:08:21.400]   Let me read to you.
[00:08:21.400 --> 00:08:23.400]   I went and ran and got Kevin's book.
[00:08:23.400 --> 00:08:24.400]   It's called The Inevitable.
[00:08:24.400 --> 00:08:25.400]   It comes out this week.
[00:08:25.400 --> 00:08:29.400]   He says it's silly to think of it as human minds.
[00:08:29.400 --> 00:08:34.400]   He says there could be a mind like a human mind but it's faster in answering.
[00:08:34.400 --> 00:08:36.400]   That's the easiest day I to imagine.
[00:08:36.400 --> 00:08:41.400]   But what about a very slow mind composed primarily of vast storage and memory?
[00:08:41.400 --> 00:08:46.400]   Or what about a global super mind composed of millions of dumb individual minds in concert?
[00:08:46.400 --> 00:08:49.400]   That's Kevin's a guy who came up with a hive mind.
[00:08:49.400 --> 00:08:51.400]   That sounds like his hive mind.
[00:08:51.400 --> 00:08:57.400]   How about a mind trained and dedicated to enhancing your personal mind but useless to anybody else?
[00:08:57.400 --> 00:08:58.400]   Right?
[00:08:58.400 --> 00:09:05.400]   Or a mind capable of successfully making a better mind once?
[00:09:05.400 --> 00:09:07.400]   That's the point is to be creative.
[00:09:07.400 --> 00:09:09.400]   We don't have to assume it's going to look like us.
[00:09:09.400 --> 00:09:11.400]   I think we often do with artificial intelligence.
[00:09:11.400 --> 00:09:16.400]   That's where it's a failure where you say, "Well, but can it appear to be human?"
[00:09:16.400 --> 00:09:21.400]   I think the Turing test, and I think that that's maybe a very limited way of thinking of AI.
[00:09:21.400 --> 00:09:23.400]   That's incredibly limited.
[00:09:23.400 --> 00:09:24.400]   Yeah.
[00:09:24.400 --> 00:09:28.400]   So if you have a broader definition of it, you can see it everywhere.
[00:09:28.400 --> 00:09:35.400]   Somebody's pointing out in the chatroom, Alexa had 135 skills in January, has a thousand this week.
[00:09:35.400 --> 00:09:37.400]   Let me just run through.
[00:09:37.400 --> 00:09:39.400]   This is just some of the things.
[00:09:39.400 --> 00:09:41.400]   So you guys jump in here as I go through.
[00:09:41.400 --> 00:09:46.400]   I'm going to jump in on this is some of the things that happened in AI this week that were sort of brought to the forefront.
[00:09:46.400 --> 00:09:51.400]   So Facebook says for the first time, "AI is doing more content monitoring than humans."
[00:09:51.400 --> 00:09:53.400]   That's a little bit of a tipping point.
[00:09:53.400 --> 00:09:55.400]   Yeah, but that may be Facebook's...
[00:09:55.400 --> 00:10:01.400]   Remember Facebook's the one who's denied they were human curators, and finally had to admit it.
[00:10:01.400 --> 00:10:04.400]   They may prefer that you think that it's all AI, right?
[00:10:04.400 --> 00:10:05.400]   Right.
[00:10:05.400 --> 00:10:08.400]   And that they're all perfectly in the middle of the blue-red spectrum.
[00:10:08.400 --> 00:10:10.400]   Okay.
[00:10:10.400 --> 00:10:19.400]   Jeff Bezos, he said at the Code Conference that artificial intelligence is in such early days that they made the baseball analogy,
[00:10:19.400 --> 00:10:23.400]   not only is it the first inning, he thinks it's the first guy at bat.
[00:10:23.400 --> 00:10:24.400]   Right.
[00:10:24.400 --> 00:10:26.400]   So he really sees this as early days.
[00:10:26.400 --> 00:10:29.400]   Sundar Pichai also said early days.
[00:10:29.400 --> 00:10:34.400]   Melinda Gates, also at the Code Conference, said all the books Bill is reading are about AI.
[00:10:34.400 --> 00:10:35.400]   Yeah.
[00:10:35.400 --> 00:10:38.400]   He said Bill Gates, he sees two problems.
[00:10:38.400 --> 00:10:41.400]   Loss of jobs.
[00:10:41.400 --> 00:10:44.400]   Who controls the AI, making sure people stay in charge.
[00:10:44.400 --> 00:10:45.400]   Right.
[00:10:45.400 --> 00:10:50.400]   Now, we'll get back to that because that's the Elon Musk story is incredible on that realm.
[00:10:50.400 --> 00:10:52.400]   Let's go into the details though.
[00:10:52.400 --> 00:10:58.400]   Sundar Pichai talking to Walt Mossberg and Cara Swisher said, "I thought this was interesting.
[00:10:58.400 --> 00:11:01.400]   I want to hear what you guys say."
[00:11:01.400 --> 00:11:05.400]   His sense of AI was so different than everyone else who was talking about it in the global
[00:11:05.400 --> 00:11:09.400]   and in the sort of speculative.
[00:11:09.400 --> 00:11:14.400]   It was really literal in like what are they doing here, the specifics where Google is,
[00:11:14.400 --> 00:11:18.560]   they made me realize that without bragging too much, I mean he did say AI is better than
[00:11:18.560 --> 00:11:23.000]   everyone else's, but they're really in the details.
[00:11:23.000 --> 00:11:26.000]   They're not in the huge overarching perspective.
[00:11:26.000 --> 00:11:28.600]   That's not tech is though, isn't it?
[00:11:28.600 --> 00:11:34.280]   The people who are making the tech are often very much in the weeds and maybe nobody's even
[00:11:34.280 --> 00:11:38.240]   looking at the larger implications of it.
[00:11:38.240 --> 00:11:41.400]   They're so busy solving problems, specific problems.
[00:11:41.400 --> 00:11:47.480]   Well, I think it's, yeah, the guys who are actually doing that, maybe that's why I feel
[00:11:47.480 --> 00:11:52.400]   like in some sense is this is just me when I really pay attention, I see big, bigger trends.
[00:11:52.400 --> 00:11:53.400]   Right.
[00:11:53.400 --> 00:11:54.400]   But.
[00:11:54.400 --> 00:11:56.400]   Well, that's kind of our job since we can't be in the weeds solving problems, at least
[00:11:56.400 --> 00:12:01.400]   we can be at 30,000 feet looking down and saying, hey, something to think about here.
[00:12:01.400 --> 00:12:04.400]   There are a lot of big brains doing that right now.
[00:12:04.400 --> 00:12:08.400]   Of course, that's what AI.org was kind of all about, right?
[00:12:08.400 --> 00:12:13.000]   That's Elon, who else got involved in it?
[00:12:13.000 --> 00:12:14.000]   Is this OpenAI?
[00:12:14.000 --> 00:12:15.000]   Oh, OpenAI, yeah.
[00:12:15.000 --> 00:12:16.000]   Yeah.
[00:12:16.000 --> 00:12:17.000]   Yeah.
[00:12:17.000 --> 00:12:18.000]   Okay.
[00:12:18.000 --> 00:12:19.000]   So let's-
[00:12:19.000 --> 00:12:22.040]   This was the idea was no one company should own this.
[00:12:22.040 --> 00:12:27.640]   It should be part of, you know, everybody should weigh in on this and it should be available,
[00:12:27.640 --> 00:12:31.000]   more importantly, maybe even from their point of view, it's OpenAI.com.
[00:12:31.000 --> 00:12:32.520]   It should be available to everybody.
[00:12:32.520 --> 00:12:37.400]   They're a nonprofit, but they are funding research and funding it big time.
[00:12:37.400 --> 00:12:41.560]   Well, let's get into that because that, I think, is what really, I mean, my mind felt
[00:12:41.560 --> 00:12:46.400]   like it was exploding listening to Elon talk for an hour and a half about artificial intelligence.
[00:12:46.400 --> 00:12:49.400]   Harry, what did you take away from that conversation?
[00:12:49.400 --> 00:12:52.600]   Well, I mean, a whole lot.
[00:12:52.600 --> 00:12:57.320]   And you mean, he talked about there being a company that scared him, but he wouldn't
[00:12:57.320 --> 00:12:58.320]   say what it was.
[00:12:58.320 --> 00:13:00.280]   It's got to be Facebook, right?
[00:13:00.280 --> 00:13:04.480]   Well, the one thing that does scare me about AI is the privacy implication.
[00:13:04.480 --> 00:13:05.480]   Right.
[00:13:05.480 --> 00:13:10.880]   So essentially, for good AI, you want a server on the other end to be able to do anything
[00:13:10.880 --> 00:13:12.680]   and everything with your data.
[00:13:12.680 --> 00:13:18.200]   And for privacy, you want end-to-end encryption, which essentially makes AI impossible.
[00:13:18.200 --> 00:13:21.640]   And the world is going to have to decide what it wants.
[00:13:21.640 --> 00:13:28.680]   And I really hope that it's not Facebook that decides on its own without any input from its
[00:13:28.680 --> 00:13:29.680]   users.
[00:13:29.680 --> 00:13:31.960]   I just get a lot of competition though.
[00:13:31.960 --> 00:13:35.600]   I mean, peso set recodes that there are a thousand people at Amazon working on it.
[00:13:35.600 --> 00:13:36.600]   Amazon has a thousand people.
[00:13:36.600 --> 00:13:39.120]   Amazon has nowhere near as much data about people.
[00:13:39.120 --> 00:13:40.120]   Who has the most?
[00:13:40.120 --> 00:13:41.120]   Facebook and Google.
[00:13:41.120 --> 00:13:42.120]   Yeah.
[00:13:42.120 --> 00:13:46.720]   And really, and Apple, except Apple, has been saying that they're opting out of this and
[00:13:46.720 --> 00:13:49.560]   they don't want to use your data and they want to encrypt everything.
[00:13:49.560 --> 00:13:55.680]   I feel like this, I don't want to say disingenuous of Apple, but clearly they're collecting
[00:13:55.680 --> 00:13:58.120]   and using your data.
[00:13:58.120 --> 00:14:01.600]   Maybe really all they're ultimately saying is we're just not going to give it to a third
[00:14:01.600 --> 00:14:02.800]   party.
[00:14:02.800 --> 00:14:04.240]   But clearly Apple is.
[00:14:04.240 --> 00:14:06.800]   A lot of the stuff that they do would work, right?
[00:14:06.800 --> 00:14:09.680]   Well, I'm fascinated that you think it's Facebook he was referring to.
[00:14:09.680 --> 00:14:10.680]   Who do you think it was?
[00:14:10.680 --> 00:14:12.000]   Well, can we listen to the clip?
[00:14:12.000 --> 00:14:13.000]   Yeah.
[00:14:13.000 --> 00:14:14.520]   Because the context is actually really important.
[00:14:14.520 --> 00:14:15.520]   Okay.
[00:14:15.520 --> 00:14:20.360]   Here's Elon Musk at the Recode conference.
[00:14:20.360 --> 00:14:26.320]   And the intent with OpenAI is to democratize AI power.
[00:14:26.320 --> 00:14:30.360]   There's a quote that I love from old Acton.
[00:14:30.360 --> 00:14:35.000]   He was the guy who came up with power corrupts and absolute power corrupts absolutely, which
[00:14:35.000 --> 00:14:40.840]   is that freedom consists of the distribution of power and despotism in its concentration.
[00:14:40.840 --> 00:14:45.320]   And so I think it's important if we have this incredible power of AI that it not be concentrated
[00:14:45.320 --> 00:14:49.840]   in the hands of a few and potentially lead to a church here at world that we don't want.
[00:14:49.840 --> 00:14:50.840]   And what world is that?
[00:14:50.840 --> 00:14:54.400]   What do you see for see that when you sit?
[00:14:54.400 --> 00:15:00.120]   It's difficult, I mean, it's called the singularity because it's difficult to predict what exactly
[00:15:00.120 --> 00:15:05.120]   what future that might be except I don't know a lot of people who love the idea of living
[00:15:05.120 --> 00:15:08.120]   under a despot.
[00:15:08.120 --> 00:15:12.960]   I don't think people generally choose to live in an democracy over a dictatorship.
[00:15:12.960 --> 00:15:15.720]   And the despot would be the computer?
[00:15:15.720 --> 00:15:18.640]   What are the people controlling the computer?
[00:15:18.640 --> 00:15:24.440]   And do you worry specifically about any of these companies I mentioned who've all seemed
[00:15:24.440 --> 00:15:31.040]   to now kind of be pivoting toward this as the battleground of the next 10 years?
[00:15:31.040 --> 00:15:33.840]   I wouldn't name a name, but there is only one.
[00:15:33.840 --> 00:15:35.040]   There's only one you want to know.
[00:15:35.040 --> 00:15:37.000]   I'd say there's two.
[00:15:37.000 --> 00:15:40.600]   And they're not at least I would make a car that will compete with you.
[00:15:40.600 --> 00:15:41.600]   I assume.
[00:15:41.600 --> 00:15:45.200]   There's only one.
[00:15:45.200 --> 00:15:46.520]   Maybe he's worried about Apple.
[00:15:46.520 --> 00:15:52.080]   That's where because that conversation had come from with Walt talking about Apple and
[00:15:52.080 --> 00:15:58.800]   Elon was very confrontational as I mean, never has anyone sounded so bored talking about
[00:15:58.800 --> 00:15:59.800]   such amazing things.
[00:15:59.800 --> 00:16:00.800]   He's thoughtful.
[00:16:00.800 --> 00:16:01.800]   He's not bored.
[00:16:01.800 --> 00:16:02.800]   He's thinking.
[00:16:02.800 --> 00:16:03.800]   Right.
[00:16:03.800 --> 00:16:07.240]   But it was like so mellow and monotone that here's why he's worried about Apple if it is
[00:16:07.240 --> 00:16:08.240]   Apple.
[00:16:08.240 --> 00:16:09.240]   Right.
[00:16:09.240 --> 00:16:11.920]   But so he was saying that Apple is a secret and Apple does what it's doing in secret.
[00:16:11.920 --> 00:16:16.040]   And at least with Google, maybe less so with Facebook, you know, they're kind of being
[00:16:16.040 --> 00:16:17.640]   open about what they're up to.
[00:16:17.640 --> 00:16:19.160]   Facebook's pretty open about it.
[00:16:19.160 --> 00:16:20.640]   I visited their AI labs.
[00:16:20.640 --> 00:16:23.800]   They did a big story on Facebook at the end of last year.
[00:16:23.800 --> 00:16:26.760]   And a large chunk of that was their AI stuff.
[00:16:26.760 --> 00:16:28.800]   They also published a lot of their research.
[00:16:28.800 --> 00:16:29.800]   I have this.
[00:16:29.800 --> 00:16:30.800]   I don't.
[00:16:30.800 --> 00:16:33.040]   If it is Apple, I disagree with Elon.
[00:16:33.040 --> 00:16:37.360]   I think you should worry more about Apple if you're a global fashion brand.
[00:16:37.360 --> 00:16:41.280]   If you're an AI developer.
[00:16:41.280 --> 00:16:45.560]   Apple seems to have changed their tuned cut quite a bit.
[00:16:45.560 --> 00:16:49.400]   You don't ever by the way, rarely do you hear the words Apple and AI together.
[00:16:49.400 --> 00:16:53.040]   And all you have to do is look at their one AI product Siri and say, well, that's not
[00:16:53.040 --> 00:16:54.040]   very scary.
[00:16:54.040 --> 00:16:59.800]   But you think they've got a lab somewhere that they can't opt out entirely.
[00:16:59.800 --> 00:17:05.080]   But I mean, Tim Cook has had this drumbeat of how we're not going to use your data.
[00:17:05.080 --> 00:17:07.840]   We don't want to store it on our servers.
[00:17:07.840 --> 00:17:10.080]   Other companies that do that you should be concerned about.
[00:17:10.080 --> 00:17:16.520]   So at some point, Apple's going to have to figure out a way to live the life it wants
[00:17:16.520 --> 00:17:19.160]   to live, but also address some of this stuff.
[00:17:19.160 --> 00:17:23.800]   Because I think if you want to set up scenarios where Apple is blown away, it's where the
[00:17:23.800 --> 00:17:25.720]   sea change happens.
[00:17:25.720 --> 00:17:29.080]   And they've opted out and they may have painted themselves into a corner.
[00:17:29.080 --> 00:17:30.080]   Right.
[00:17:30.080 --> 00:17:35.040]   I do find it odd that he talks about despotism given that you can opt out of Facebook.
[00:17:35.040 --> 00:17:38.000]   You don't have to use Google services.
[00:17:38.000 --> 00:17:41.040]   It's a little bit different than if the person running your country suddenly decides that
[00:17:41.040 --> 00:17:42.840]   he's going to tell you exactly what to do.
[00:17:42.840 --> 00:17:46.120]   I don't feel like you can opt out of Google if you're a real human.
[00:17:46.120 --> 00:17:48.200]   I mean, who's like living a day to day life?
[00:17:48.200 --> 00:17:51.040]   It would be tough, but it would not be impossible.
[00:17:51.040 --> 00:17:55.040]   And if they ever do anything terrible, millions of people could opt out of using Google.
[00:17:55.040 --> 00:17:56.040]   Fair enough.
[00:17:56.040 --> 00:17:57.040]   So, okay.
[00:17:57.040 --> 00:17:59.840]   So you made the case for potentially Facebook.
[00:17:59.840 --> 00:18:01.960]   Why probably it's not Apple.
[00:18:01.960 --> 00:18:05.680]   The other player would have to be Google.
[00:18:05.680 --> 00:18:09.920]   It would be his fear or concern.
[00:18:09.920 --> 00:18:13.800]   How would they play into that just because of the sheer amount of data they have on you
[00:18:13.800 --> 00:18:15.520]   in every facet of your life?
[00:18:15.520 --> 00:18:18.760]   Here's the thing that would make, if I were scared of Google, it would make me scared
[00:18:18.760 --> 00:18:22.320]   is that they have shown no reluctance to use these technologies, right?
[00:18:22.320 --> 00:18:26.400]   And Google controls a phone full of sensors, which Facebook doesn't do.
[00:18:26.400 --> 00:18:31.480]   So Google has way more opportunity than Facebook does in some respects to track where you
[00:18:31.480 --> 00:18:33.920]   are and what you're doing.
[00:18:33.920 --> 00:18:35.920]   Okay.
[00:18:35.920 --> 00:18:40.520]   We know we're going to talk about this later, but you were talking about on this week in
[00:18:40.520 --> 00:18:46.600]   Google about how when they bought Nest, they didn't want to put a microphone into it to
[00:18:46.600 --> 00:18:49.840]   make it the home assistant because they didn't want people to think, what are they doing with
[00:18:49.840 --> 00:18:50.840]   that?
[00:18:50.840 --> 00:18:51.840]   Right.
[00:18:51.840 --> 00:18:55.480]   Google's very aware of their public persona as being, everybody thinks Google's going
[00:18:55.480 --> 00:18:57.000]   to do Skynet, right?
[00:18:57.000 --> 00:19:03.040]   That Google's creating Skynet, the Terminator computer that set the machines after us.
[00:19:03.040 --> 00:19:06.560]   And so there's one candidate for Skynet, right, at this point, isn't it?
[00:19:06.560 --> 00:19:07.560]   It's Google.
[00:19:07.560 --> 00:19:09.840]   Although, I'm a little more nervous about Facebook.
[00:19:09.840 --> 00:19:16.080]   Did you read the article in Vanity Fair this week from a book about a former Facebook employee
[00:19:16.080 --> 00:19:20.560]   who is so supposedly blowing the lid off Facebook?
[00:19:20.560 --> 00:19:29.680]   And he talks about Mark Zuckerberg as being very nearly psychopathic in his drive to win
[00:19:29.680 --> 00:19:31.160]   at all costs.
[00:19:31.160 --> 00:19:35.080]   I didn't find it very in a damning article after reading it.
[00:19:35.080 --> 00:19:38.640]   I mean, the lead talks about Jim Jones and Jones Town.
[00:19:38.640 --> 00:19:39.640]   Yeah.
[00:19:39.640 --> 00:19:43.560]   And the actual story is just that he's incredibly ambitious and pushes people who work for him
[00:19:43.560 --> 00:19:48.920]   to be incredibly ambitious and to work, work, work, work, work, and Google maybe is at least
[00:19:48.920 --> 00:19:51.480]   a tiny bit more mellow than Facebook.
[00:19:51.480 --> 00:19:56.480]   So the lighthouse has turned off of Amazon in this rental movie.
[00:19:56.480 --> 00:19:57.480]   The eye of Mordor.
[00:19:57.480 --> 00:19:58.480]   Maybe Mordor.
[00:19:58.480 --> 00:20:03.480]   Maybe Mordor.
[00:20:03.480 --> 00:20:08.480]   I just meant the work culture and the whole idea that Bezos was this taskmaster that created
[00:20:08.480 --> 00:20:13.120]   a sort of a thunder dome work environment where it's killer be killed.
[00:20:13.120 --> 00:20:17.120]   And so it was interesting that that seems to be a story that resonates and they're looking
[00:20:17.120 --> 00:20:19.280]   for a new victim every six months.
[00:20:19.280 --> 00:20:22.680]   I've seen that story about a lot of companies.
[00:20:22.680 --> 00:20:25.440]   Is Facebook listening to you via your smartphone?
[00:20:25.440 --> 00:20:27.560]   You saw that story this week.
[00:20:27.560 --> 00:20:29.320]   Which I think was somewhat debunked.
[00:20:29.320 --> 00:20:34.720]   I mean, just reading that you could tell that it was just all theory.
[00:20:34.720 --> 00:20:35.720]   Yeah.
[00:20:35.720 --> 00:20:39.800]   Although there was an article a few months earlier in the BBC, actually it's kind of
[00:20:39.800 --> 00:20:45.160]   an interesting article in which he says, I was, let me see if I can find it.
[00:20:45.160 --> 00:20:47.640]   I just read it the other day.
[00:20:47.640 --> 00:20:52.240]   Even though it came out, I think in January or no March, is your smartphone listening to
[00:20:52.240 --> 00:20:53.240]   you?
[00:20:53.240 --> 00:20:54.240]   This is Zoey Klimann.
[00:20:54.240 --> 00:20:55.400]   So I shouldn't say he, her.
[00:20:55.400 --> 00:20:59.440]   I was doing some ironing when my mom came in to tell me my family friend had been killed
[00:20:59.440 --> 00:21:01.400]   in a road accident, Thailand.
[00:21:01.400 --> 00:21:03.560]   My phone, by the way, was on the worktop behind me.
[00:21:03.560 --> 00:21:06.840]   The next time I used a search engine on it, I popped the name of our friend in the words
[00:21:06.840 --> 00:21:08.920]   motorbike accident, Thailand.
[00:21:08.920 --> 00:21:13.680]   And the year in the suggested text below the search box, I was startled.
[00:21:13.680 --> 00:21:15.160]   Certain I'd not use my phone at the time.
[00:21:15.160 --> 00:21:16.520]   I'd had the conversation.
[00:21:16.520 --> 00:21:18.360]   Was the phone listening to me?
[00:21:18.360 --> 00:21:23.440]   We've heard a lot of anecdotal stories over and over again by people who think that their
[00:21:23.440 --> 00:21:29.560]   Echo or their phone is somehow monitoring and changing search results or shopping recommendations
[00:21:29.560 --> 00:21:31.840]   based on conversations they're having.
[00:21:31.840 --> 00:21:35.040]   There's certainly no way to prove that it's not doing that.
[00:21:35.040 --> 00:21:43.280]   In fact, in this article, she says she went to a pen testing company, Kenman Rowan, David
[00:21:43.280 --> 00:21:49.400]   Lodge from PenTest Partners, and said, can you write something, an app for an Android
[00:21:49.400 --> 00:21:55.720]   device that would listen in and collate what I said, and would it be detectable?
[00:21:55.720 --> 00:22:01.760]   So they actually did create a prototype app, demonstrated its functionality.
[00:22:01.760 --> 00:22:04.840]   He said, in order to use this app, of course, you have to have permission to use a microphone.
[00:22:04.840 --> 00:22:10.000]   Although many apps do, if you're going to talk to an app, you need to have that permission.
[00:22:10.000 --> 00:22:13.160]   They said we had to set up a listening server on the internet and everything that microphone
[00:22:13.160 --> 00:22:18.840]   heard on the phone, wherever it was, would come back and we could say, said it customized,
[00:22:18.840 --> 00:22:19.840]   adds.
[00:22:19.840 --> 00:22:21.560]   He said, the battery drain, this is the key.
[00:22:21.560 --> 00:22:22.560]   Would you notice it?
[00:22:22.560 --> 00:22:24.680]   The battery drain was minimal.
[00:22:24.680 --> 00:22:26.640]   That would be one way you'd notice it.
[00:22:26.640 --> 00:22:31.760]   And using Wi-Fi, you didn't see a real data plan spike.
[00:22:31.760 --> 00:22:36.360]   So it's, I mean, I would imagine, I would hope there's hackers and pen testers out there
[00:22:36.360 --> 00:22:41.760]   who are running wires from time to time on their devices to make sure it's not.
[00:22:41.760 --> 00:22:43.400]   And Google, of course, denies it.
[00:22:43.400 --> 00:22:50.640]   I'm going to sound so naive, but I just don't have enough conspiracy theory in my soul to
[00:22:50.640 --> 00:22:55.840]   believe that the companies would do this without having it explicitly in their terms
[00:22:55.840 --> 00:22:56.840]   of service.
[00:22:56.840 --> 00:23:02.720]   But what's to stop Amazon from getting a national security letter from the FBI saying,
[00:23:02.720 --> 00:23:07.440]   could you turn on Leo's echo just for the time being and we just like to listen in?
[00:23:07.440 --> 00:23:11.400]   And by the way, this is a national security letter, so you can't tell Leo or anybody else.
[00:23:11.400 --> 00:23:14.440]   If they had, I mean, I'm sure they have the means.
[00:23:14.440 --> 00:23:19.960]   Oh, I mean, what difference is that theoretically then search results?
[00:23:19.960 --> 00:23:21.680]   I mean, yes, of course.
[00:23:21.680 --> 00:23:31.200]   If you could get the paradigm there that it's akin to that, would it fall under, would it
[00:23:31.200 --> 00:23:36.040]   fall under different legal purview because it's a bug?
[00:23:36.040 --> 00:23:38.360]   Or is it, you know, you know, you can.
[00:23:38.360 --> 00:23:40.640]   No, I don't mean a bug like a bug in the software.
[00:23:40.640 --> 00:23:41.640]   I mean, it's a bug.
[00:23:41.640 --> 00:23:42.960]   They're bugging your house.
[00:23:42.960 --> 00:23:43.960]   Yeah.
[00:23:43.960 --> 00:23:45.000]   Well, of course, they'd have to get a court order.
[00:23:45.000 --> 00:23:46.160]   That's why it's a national security letter.
[00:23:46.160 --> 00:23:49.480]   They get the FISA court, which by the way, has never turned down a request in the last
[00:23:49.480 --> 00:23:52.120]   two years of thousands of requests.
[00:23:52.120 --> 00:23:53.760]   They've said yes to every one of them.
[00:23:53.760 --> 00:23:56.000]   So that's not so much of a protection.
[00:23:56.000 --> 00:24:01.560]   Even without any conspiratorial thoughts in my, in my being, I do see that as a possibility.
[00:24:01.560 --> 00:24:02.560]   Yeah.
[00:24:02.560 --> 00:24:03.560]   So we're going to take a break.
[00:24:03.560 --> 00:24:04.560]   Come back.
[00:24:04.560 --> 00:24:07.960]   Let's talk more about AI and as long as we're talking about AI, let's talk about any Elon.
[00:24:07.960 --> 00:24:16.640]   Let's talk about Elon and his, his suggestion that it's a, it's the chances are a billion
[00:24:16.640 --> 00:24:19.960]   to one that we are all living in a simulation.
[00:24:19.960 --> 00:24:24.120]   Not, not one in a billion, a billion to one.
[00:24:24.120 --> 00:24:25.880]   Oh, and don't forget neural lace.
[00:24:25.880 --> 00:24:26.880]   We got to talk about neural lace.
[00:24:26.880 --> 00:24:28.680]   I don't know what that is, but we'll talk about it.
[00:24:28.680 --> 00:24:29.680]   I showed him.
[00:24:29.680 --> 00:24:32.120]   It's not a fashion statement.
[00:24:32.120 --> 00:24:33.680]   No, neural lace.
[00:24:33.680 --> 00:24:34.680]   Hmm.
[00:24:34.680 --> 00:24:37.480]   No, don't go Google in that.
[00:24:37.480 --> 00:24:40.260]   Our show today brought to you by a carbonite online backup.
[00:24:40.260 --> 00:24:42.560]   You got to keep your business safe.
[00:24:42.560 --> 00:24:47.960]   And as you know, if you listen to any of our shows, ransomware is on the uptick hundreds
[00:24:47.960 --> 00:24:54.760]   of millions of dollars spent by businesses, not just businesses, hospitals, law, there
[00:24:54.760 --> 00:24:59.520]   was a Massachusetts police department that got bit by ransomware and paid the font, paid
[00:24:59.520 --> 00:25:01.560]   the fee to unencrypt their computers.
[00:25:01.560 --> 00:25:06.880]   If they just had carbonite for crying out loud, you got to wonder what kind of IT department
[00:25:06.880 --> 00:25:07.880]   this hospital had.
[00:25:07.880 --> 00:25:10.480]   Look, you can, you can have, you don't need an IT department.
[00:25:10.480 --> 00:25:13.840]   You can have the best backup solution that will protect you against ransomware right
[00:25:13.840 --> 00:25:17.720]   now for less than you'd imagine at carbonite.com.
[00:25:17.720 --> 00:25:18.800]   Why take the risk?
[00:25:18.800 --> 00:25:20.040]   Why worry about downtime?
[00:25:20.040 --> 00:25:25.640]   They've got carbonite for home, for office, for max, for PCs, for servers.
[00:25:25.640 --> 00:25:28.640]   More than 500 billion files have been backed up.
[00:25:28.640 --> 00:25:31.840]   More than a million and a half homes and businesses use carbonite.
[00:25:31.840 --> 00:25:34.400]   They have restored 50 billion files.
[00:25:34.400 --> 00:25:35.400]   That's 50 billion files.
[00:25:35.400 --> 00:25:38.480]   Plus forever, if it weren't for carbonite.
[00:25:38.480 --> 00:25:42.760]   And yes, it does protect you against ransomware because carbonite on Windows does versioning.
[00:25:42.760 --> 00:25:47.080]   That means if you get bit by ransomware and you don't catch it, it starts encrypting your
[00:25:47.080 --> 00:25:48.080]   files.
[00:25:48.080 --> 00:25:55.000]   Okay, first thing to do, stop the ransomware, then restore your unencrypted files from carbonite
[00:25:55.000 --> 00:25:56.000]   because they have versioning.
[00:25:56.000 --> 00:26:00.320]   You can even go back a couple of versions till that file is unencrypted.
[00:26:00.320 --> 00:26:02.320]   Try it today, free, no credit card needed.
[00:26:02.320 --> 00:26:06.120]   But do use our name, Twitter, and the offer code TWIT, and that way you'll get two free
[00:26:06.120 --> 00:26:08.120]   bonus months if you decide to buy.
[00:26:08.120 --> 00:26:09.280]   You got to back it up to get it back.
[00:26:09.280 --> 00:26:12.960]   Do it right with carbonite.
[00:26:12.960 --> 00:26:14.680]   Becky Worley is here from GMA.
[00:26:14.680 --> 00:26:16.200]   I didn't say what you're from.
[00:26:16.200 --> 00:26:17.200]   You're from GMA.
[00:26:17.200 --> 00:26:18.200]   I'm from your past.
[00:26:18.200 --> 00:26:20.120]   You're from my past and my future.
[00:26:20.120 --> 00:26:21.120]   Yes.
[00:26:21.120 --> 00:26:22.120]   No.
[00:26:22.120 --> 00:26:24.320]   Becky was a founding member of Tech News today.
[00:26:24.320 --> 00:26:26.840]   Thank you for setting that on its track.
[00:26:26.840 --> 00:26:27.840]   That's right.
[00:26:27.840 --> 00:26:28.840]   I'm back.
[00:26:28.840 --> 00:26:29.840]   She's a regular.
[00:26:29.840 --> 00:26:30.840]   Right.
[00:26:30.840 --> 00:26:31.840]   Every other week, right?
[00:26:31.840 --> 00:26:32.840]   I'm in the revenue.
[00:26:32.840 --> 00:26:34.520]   And how often are you on GMA these days?
[00:26:34.520 --> 00:26:35.520]   Once a week, twice a week.
[00:26:35.520 --> 00:26:38.080]   Are you still doing the consumer stuff?
[00:26:38.080 --> 00:26:39.080]   Consumer.
[00:26:39.080 --> 00:26:40.520]   It's some interesting things.
[00:26:40.520 --> 00:26:46.800]   This week talked about Amazon suing some of its reviewers, the people who buy fake reviews.
[00:26:46.800 --> 00:26:47.800]   We can talk about that.
[00:26:47.800 --> 00:26:48.800]   That was kind of interesting.
[00:26:48.800 --> 00:26:49.800]   Yeah, I do.
[00:26:49.800 --> 00:26:52.840]   I was thinking about it.
[00:26:52.840 --> 00:26:56.440]   If you guys, here's the analogy in terms of what you do for a living versus what I do
[00:26:56.440 --> 00:26:58.480]   for a living as a consumer reporter.
[00:26:58.480 --> 00:27:00.880]   Let's imagine that we're all working in the fashion industry.
[00:27:00.880 --> 00:27:04.120]   And you are setting up the windows at Barney's.
[00:27:04.120 --> 00:27:05.800]   Or you're creating a layout.
[00:27:05.800 --> 00:27:07.480]   By the way, my lifetime dream.
[00:27:07.480 --> 00:27:08.480]   Oh, okay.
[00:27:08.480 --> 00:27:11.760]   We'll make that happen in some reality for you.
[00:27:11.760 --> 00:27:13.480]   You're setting up the windows in Barney's.
[00:27:13.480 --> 00:27:16.040]   You're styling a shoot for Vogue.
[00:27:16.040 --> 00:27:18.280]   That's the way that your work goes around technology.
[00:27:18.280 --> 00:27:21.800]   Me, I'm folding the pajamas in the center of Costco.
[00:27:21.800 --> 00:27:22.800]   Okay?
[00:27:22.800 --> 00:27:24.440]   I'm putting the dungarees out there.
[00:27:24.440 --> 00:27:25.520]   But somebody's got to do it.
[00:27:25.520 --> 00:27:30.800]   I am with all my people in the everyday consumer.
[00:27:30.800 --> 00:27:32.200]   And technology is a huge part of that.
[00:27:32.200 --> 00:27:33.200]   But I love it.
[00:27:33.200 --> 00:27:34.200]   I love it.
[00:27:34.200 --> 00:27:36.000]   I think it's helpful to a lot of people.
[00:27:36.000 --> 00:27:38.400]   I'm going to completely derail this show.
[00:27:38.400 --> 00:27:39.400]   Oh, no.
[00:27:39.400 --> 00:27:41.160]   Since you mentioned folding clothes.
[00:27:41.160 --> 00:27:42.760]   Did you see the Kickstarter?
[00:27:42.760 --> 00:27:43.760]   No.
[00:27:43.760 --> 00:27:49.680]   There's a Kickstarter for a clothing, automatic clothing folding machine.
[00:27:49.680 --> 00:27:50.680]   No way.
[00:27:50.680 --> 00:27:52.520]   Oh my God.
[00:27:52.520 --> 00:27:53.520]   That would be...
[00:27:53.520 --> 00:27:54.520]   Is this it?
[00:27:54.520 --> 00:27:56.520]   It's called the folding mate.
[00:27:56.520 --> 00:27:58.520]   Is that right?
[00:27:58.520 --> 00:27:59.520]   Folding mate?
[00:27:59.520 --> 00:28:01.320]   It's the miracle fold.
[00:28:01.320 --> 00:28:04.520]   Apparently when I googled this, there's more than one.
[00:28:04.520 --> 00:28:05.520]   Wow.
[00:28:05.520 --> 00:28:07.520]   I'm saying there's more than one of anything.
[00:28:07.520 --> 00:28:09.320]   Anything good?
[00:28:09.320 --> 00:28:11.000]   Why is GAP not funding that?
[00:28:11.000 --> 00:28:12.000]   There's thread stacks.
[00:28:12.000 --> 00:28:14.000]   Is that what you're talking about?
[00:28:14.000 --> 00:28:15.000]   Wow.
[00:28:15.000 --> 00:28:16.000]   You know, stacks organized your closet.
[00:28:16.000 --> 00:28:17.000]   No, no, no, no, no.
[00:28:17.000 --> 00:28:18.000]   This is a machine.
[00:28:18.000 --> 00:28:19.000]   It's the funniest video ever.
[00:28:19.000 --> 00:28:20.000]   There's a stack.
[00:28:20.000 --> 00:28:22.360]   It's like full rosy the robot.
[00:28:22.360 --> 00:28:27.120]   Yeah, it sounds like that in principle until you see the video.
[00:28:27.120 --> 00:28:30.680]   And then you realize, is this it?
[00:28:30.680 --> 00:28:31.680]   I...
[00:28:31.680 --> 00:28:34.120]   Yes, yes, folding mate.
[00:28:34.120 --> 00:28:37.080]   They're taking pre-orders for next year, so it's not on Kickstarter.
[00:28:37.080 --> 00:28:38.080]   Wow.
[00:28:38.080 --> 00:28:41.640]   So you've been here, the kids, you're trying to fold and the kids are just throwing the
[00:28:41.640 --> 00:28:43.400]   clothes in the air.
[00:28:43.400 --> 00:28:45.760]   And it's one step forward, two steps backward.
[00:28:45.760 --> 00:28:47.880]   Well, enter the folding mate.
[00:28:47.880 --> 00:28:48.880]   Ooh.
[00:28:48.880 --> 00:28:54.320]   See, Mom just takes her clothes and clips them into the special folding mate device.
[00:28:54.320 --> 00:28:55.320]   Wow.
[00:28:55.320 --> 00:28:56.320]   Okay.
[00:28:56.320 --> 00:28:57.520]   And then watch what happens.
[00:28:57.520 --> 00:29:03.960]   The clothes are sucked in to the folding mate where the mechanical folding mate arms will
[00:29:03.960 --> 00:29:10.160]   not only automatically fold and smooth your fabric, but spray it with the odouriser.
[00:29:10.160 --> 00:29:11.160]   Whoa.
[00:29:11.160 --> 00:29:12.160]   And then...
[00:29:12.160 --> 00:29:14.080]   Oh, and even steam it for dew wrinkling.
[00:29:14.080 --> 00:29:15.080]   Oh, dew wrinkling.
[00:29:15.080 --> 00:29:16.880]   Yeah, very important.
[00:29:16.880 --> 00:29:21.120]   And then at the end of the day, the kids help too because it's so much fun.
[00:29:21.120 --> 00:29:22.120]   Oh gosh.
[00:29:22.120 --> 00:29:23.120]   And the...
[00:29:23.120 --> 00:29:26.040]   Yeah, and Dad's just sitting there going, "What?"
[00:29:26.040 --> 00:29:27.720]   And then the tray's full, but look.
[00:29:27.720 --> 00:29:32.080]   It spits out 12 perfectly folded t-shirts.
[00:29:32.080 --> 00:29:33.320]   I completely derailed this.
[00:29:33.320 --> 00:29:34.320]   And it's only $37,000.
[00:29:34.320 --> 00:29:37.320]   And it takes an hour.
[00:29:37.320 --> 00:29:38.320]   They don't really...
[00:29:38.320 --> 00:29:39.720]   700, 850.
[00:29:39.720 --> 00:29:42.320]   They know that's the target price, by the way.
[00:29:42.320 --> 00:29:43.840]   And they obviously haven't built it yet.
[00:29:43.840 --> 00:29:45.200]   That'll be on GMA next week.
[00:29:45.200 --> 00:29:46.200]   Thanks, guys.
[00:29:46.200 --> 00:29:47.200]   Appreciate you.
[00:29:47.200 --> 00:29:50.600]   So you really are, actually, in the back of Costco folding clothes.
[00:29:50.600 --> 00:29:52.360]   Does it do socks and underwear?
[00:29:52.360 --> 00:29:54.760]   Those don't need to be folded.
[00:29:54.760 --> 00:29:55.760]   Yeah, right.
[00:29:55.760 --> 00:30:00.800]   I'm such a guy because I'm the kind of person who only buys one color sock and like 20 or
[00:30:00.800 --> 00:30:02.320]   30 of them so that you just...
[00:30:02.320 --> 00:30:03.320]   It doesn't matter what you...
[00:30:03.320 --> 00:30:04.320]   That's really smart.
[00:30:04.320 --> 00:30:05.320]   Yeah.
[00:30:05.320 --> 00:30:06.520]   What do you need colors and socks for?
[00:30:06.520 --> 00:30:07.520]   Please.
[00:30:07.520 --> 00:30:08.520]   Come on.
[00:30:08.520 --> 00:30:09.520]   I know.
[00:30:09.520 --> 00:30:12.720]   Actually, oh, I shouldn't tell you this story.
[00:30:12.720 --> 00:30:13.720]   I'm kind of though.
[00:30:13.720 --> 00:30:15.080]   You know when I see that I'm gone.
[00:30:15.080 --> 00:30:16.760]   I just had a moral dilemma.
[00:30:16.760 --> 00:30:19.280]   Do I tell him not to tell the story or do I let him tell the story?
[00:30:19.280 --> 00:30:21.320]   If you were my producer, you would say.
[00:30:21.320 --> 00:30:23.440]   So I'm reading no...
[00:30:23.440 --> 00:30:28.560]   What was the book about the C-Seal Team Six and killing Osama bin Laden?
[00:30:28.560 --> 00:30:31.560]   It was called No Bad Day, No Good Day, something like that.
[00:30:31.560 --> 00:30:35.200]   Mark Bessinet, the guy who got in trouble because he's the guy who was on C-Seal Team
[00:30:35.200 --> 00:30:37.120]   Six that killed bin Laden.
[00:30:37.120 --> 00:30:38.920]   But there was a detail in the book.
[00:30:38.920 --> 00:30:42.520]   No, everybody's paying attention to how they arm themselves.
[00:30:42.520 --> 00:30:47.080]   Their strategies for getting in, how they found bin Laden, how they got him.
[00:30:47.080 --> 00:30:48.680]   I'm reading long and I hear...
[00:30:48.680 --> 00:30:55.080]   I read and we looked in his clothing drawer and all of his t-shirts were rolled up.
[00:30:55.080 --> 00:30:57.080]   And I thought, "Wait a minute.
[00:30:57.080 --> 00:31:00.320]   That's a great idea."
[00:31:00.320 --> 00:31:01.320]   So that's my takeaway.
[00:31:01.320 --> 00:31:02.880]   But the bin Laden technique.
[00:31:02.880 --> 00:31:04.760]   The bin Laden technique.
[00:31:04.760 --> 00:31:08.040]   So the t-shirts fold them in half, roll them up.
[00:31:08.040 --> 00:31:09.040]   Wow.
[00:31:09.040 --> 00:31:10.040]   Life hack.
[00:31:10.040 --> 00:31:11.040]   Works great.
[00:31:11.040 --> 00:31:14.920]   This is what I get.
[00:31:14.920 --> 00:31:16.840]   This is what I get from that whole book.
[00:31:16.840 --> 00:31:19.720]   Oh, I could learn how a C-Seal Team trains.
[00:31:19.720 --> 00:31:20.720]   No.
[00:31:20.720 --> 00:31:23.440]   Osama bin Laden folded his t-shirts.
[00:31:23.440 --> 00:31:24.920]   That's from Osama.
[00:31:24.920 --> 00:31:25.920]   Thank you, Osama.
[00:31:25.920 --> 00:31:27.440]   No, no thanks.
[00:31:27.440 --> 00:31:29.880]   I'm sure that he didn't invent it.
[00:31:29.880 --> 00:31:31.480]   Let's get back to artificial intelligence.
[00:31:31.480 --> 00:31:34.280]   Oh, and you said I was here, but you didn't say Harry's here.
[00:31:34.280 --> 00:31:35.280]   Tell these guys Harry's here.
[00:31:35.280 --> 00:31:36.280]   Oh, Harry's here too.
[00:31:36.280 --> 00:31:37.280]   Harry's here.
[00:31:37.280 --> 00:31:38.280]   The technologizer.
[00:31:38.280 --> 00:31:39.280]   You still use that?
[00:31:39.280 --> 00:31:46.640]   I am technologizer.com points to my flip board magazine where I put in my own stuff.
[00:31:46.640 --> 00:31:47.640]   I put in my own stuff.
[00:31:47.640 --> 00:31:48.640]   It's a good brand.
[00:31:48.640 --> 00:31:49.640]   I put in the web.
[00:31:49.640 --> 00:31:50.640]   Don't throw it out.
[00:31:50.640 --> 00:31:51.640]   Keep it.
[00:31:51.640 --> 00:31:55.360]   It's a curated experience rather than me creating a lot of new stuff.
[00:31:55.360 --> 00:31:56.360]   But you can read it fast enough.
[00:31:56.360 --> 00:31:59.600]   But all my fast company stuff is also on my flip board.
[00:31:59.600 --> 00:32:00.600]   People get mad at me.
[00:32:00.600 --> 00:32:01.600]   I call you the technologist.
[00:32:01.600 --> 00:32:02.600]   They say, "Oh, he's not that anymore."
[00:32:02.600 --> 00:32:03.600]   I'd say, "No, no, Harry's always."
[00:32:03.600 --> 00:32:04.600]   You will always be.
[00:32:04.600 --> 00:32:05.600]   You will always be.
[00:32:05.600 --> 00:32:06.600]   Epitaph.
[00:32:06.600 --> 00:32:07.600]   At least in your eyes.
[00:32:07.600 --> 00:32:10.280]   I have a very limited memory.
[00:32:10.280 --> 00:32:13.240]   I have to just put you in a box and keep you there.
[00:32:13.240 --> 00:32:16.720]   You're basically rolled up in my t-shirt drawer as it's a bonus.
[00:32:16.720 --> 00:32:21.560]   Forever, forever branded the technologizer.
[00:32:21.560 --> 00:32:25.000]   Elon Musk also speaking at the Recode conference.
[00:32:25.000 --> 00:32:26.640]   It's funny because I'm watching this video.
[00:32:26.640 --> 00:32:28.160]   I guess they had Q&A afterwards.
[00:32:28.160 --> 00:32:29.440]   Who shows up at the Q&A?
[00:32:29.440 --> 00:32:32.320]   But Joshua Topolsky, I've been wondering what he's been up to.
[00:32:32.320 --> 00:32:34.480]   A creator of Engadget, right?
[00:32:34.480 --> 00:32:37.140]   I had a editor in Chief-- I don't know if he created it, but he was editor in Chief
[00:32:37.140 --> 00:32:38.140]   of Engadget.
[00:32:38.140 --> 00:32:41.800]   Then he did the Verge, my next thing, right?
[00:32:41.800 --> 00:32:42.800]   Then he left, right?
[00:32:42.800 --> 00:32:43.800]   He left Fox.
[00:32:43.800 --> 00:32:45.960]   I don't know what he's doing these days.
[00:32:45.960 --> 00:32:47.440]   But apparently he's asking questions to the Recode.
[00:32:47.440 --> 00:32:48.440]   Went to Bloomberg for a while.
[00:32:48.440 --> 00:32:49.440]   Yeah, right.
[00:32:49.440 --> 00:32:50.440]   He went to Graham Bloomberg for a while.
[00:32:50.440 --> 00:32:51.440]   I think he's doing something new.
[00:32:51.440 --> 00:32:53.440]   He's starting something and he hasn't said too much about it?
[00:32:53.440 --> 00:32:54.440]   He hasn't said too much about it.
[00:32:54.440 --> 00:32:56.680]   Anyway, he had this question.
[00:32:56.680 --> 00:32:57.680]   I see.
[00:32:57.680 --> 00:32:58.680]   Of course.
[00:32:58.680 --> 00:33:02.920]   There's sort of a philosophic concept that a sufficiently advanced civilization will
[00:33:02.920 --> 00:33:04.920]   be able to create.
[00:33:04.920 --> 00:33:06.480]   So simulation.
[00:33:06.480 --> 00:33:07.480]   Yeah.
[00:33:07.480 --> 00:33:08.480]   Elon's really on.
[00:33:08.480 --> 00:33:09.480]   He knows.
[00:33:09.480 --> 00:33:11.040]   I've had so many simulation discussions.
[00:33:11.040 --> 00:33:12.040]   It's crazy.
[00:33:12.040 --> 00:33:13.040]   OK.
[00:33:13.040 --> 00:33:14.040]   What?
[00:33:14.040 --> 00:33:20.160]   In fact, it got to the point where basically every conversation was the AI/simulation conversation.
[00:33:20.160 --> 00:33:24.240]   And my brother and I finally agreed that we would ban such conversations if we're ever
[00:33:24.240 --> 00:33:25.240]   in a hot tub.
[00:33:25.240 --> 00:33:27.240]   And I was like, OK.
[00:33:27.240 --> 00:33:31.440]   There's something else besides a hot tub involved.
[00:33:31.440 --> 00:33:32.440]   I think he's right.
[00:33:32.440 --> 00:33:38.960]   Any sufficiently advanced civilization could create a simulation that's like our existence.
[00:33:38.960 --> 00:33:44.000]   And so the theory follows that maybe we're in the simulation.
[00:33:44.000 --> 00:33:45.000]   Have you thought about this?
[00:33:45.000 --> 00:33:46.000]   A lot.
[00:33:46.000 --> 00:33:47.000]   A lot.
[00:33:47.000 --> 00:33:48.000]   A lot.
[00:33:48.000 --> 00:33:50.000]   Are we even in hot tubs?
[00:33:50.000 --> 00:33:51.000]   I don't think so.
[00:33:51.000 --> 00:33:52.000]   A lot.
[00:33:52.000 --> 00:33:53.000]   This is our Tony Stark.
[00:33:53.000 --> 00:33:54.000]   That's exactly what I thought.
[00:33:54.000 --> 00:33:57.560]   Listen, listen to his explanation of why he believes it.
[00:33:57.560 --> 00:33:58.560]   Are we in?
[00:33:58.560 --> 00:33:59.560]   Are we in?
[00:33:59.560 --> 00:34:07.840]   I think he has, in my mind, the strongest argument for us being in a simulation, probably being
[00:34:07.840 --> 00:34:11.720]   in a simulation I think is the following.
[00:34:11.720 --> 00:34:17.520]   That 40 years ago we had pong, like two rectangles in a dot.
[00:34:17.520 --> 00:34:20.880]   That was what games were.
[00:34:20.880 --> 00:34:26.120]   Now 40 years later we have photo-realistic 3D simulations with millions of people playing
[00:34:26.120 --> 00:34:28.800]   simultaneously and it's getting better every year.
[00:34:28.800 --> 00:34:29.800]   And that was in 40 years.
[00:34:29.800 --> 00:34:34.680]   So virtual reality, augmented reality.
[00:34:34.680 --> 00:34:41.760]   If you assume any rate of improvement at all, then the games will become indistinguishable
[00:34:41.760 --> 00:34:45.480]   from reality, just indistinguishable.
[00:34:45.480 --> 00:34:51.520]   Even if that rate of advancement drops by a thousand from what it is right now, then
[00:34:51.520 --> 00:34:56.280]   you just say, okay, well, let's mention it's a 10,000 years in the future, which is nothing
[00:34:56.280 --> 00:34:58.760]   in the evolutionary scale.
[00:34:58.760 --> 00:35:06.320]   So given that we're clearly our trajectory to have games that are indistinguishable from
[00:35:06.320 --> 00:35:12.320]   reality and those games could be played on any set-top box or on a PC or whatever, and
[00:35:12.320 --> 00:35:23.680]   there would probably be billions of such computers or set-top boxes, it would seem to
[00:35:23.680 --> 00:35:29.480]   follow that the odds that we're in base reality is one in billions.
[00:35:29.480 --> 00:35:31.480]   Let's get this clear.
[00:35:31.480 --> 00:35:37.640]   The odds that we are in what they call base reality, like real reality, is one in a billion.
[00:35:37.640 --> 00:35:42.760]   Now that seems like pretty high odds that we are in a simulation, and he's not that crazy.
[00:35:42.760 --> 00:35:46.400]   Here's a Scientific American article from last month in which Neil deGrasse Tyson says,
[00:35:46.400 --> 00:35:48.800]   well, I think it's more like 50/50.
[00:35:48.800 --> 00:35:49.800]   What?
[00:35:49.800 --> 00:35:50.800]   What?
[00:35:50.800 --> 00:35:51.800]   Wow.
[00:35:51.800 --> 00:35:56.280]   I mean, I think it should be said that, well, games look incredibly better than they did
[00:35:56.280 --> 00:35:57.280]   40 years ago.
[00:35:57.280 --> 00:35:58.280]   They still kind of.
[00:35:58.280 --> 00:36:03.080]   The gameplay has not kept up, and a lot of games are not all that much more sophisticated
[00:36:03.080 --> 00:36:07.000]   than a really fancy version of power, space invaders or whatever.
[00:36:07.000 --> 00:36:10.680]   This is a panel that Scientific American had of physicists, philosophers, Neil deGrasse
[00:36:10.680 --> 00:36:12.040]   Tyson moderated it.
[00:36:12.040 --> 00:36:16.480]   And what the physicists said is, well, if you want to prove or disprove this hypothesis,
[00:36:16.480 --> 00:36:20.080]   what you do is you observe and you look for shortcuts that the programmers might have
[00:36:20.080 --> 00:36:21.080]   taken.
[00:36:21.080 --> 00:36:30.280]   Like, okay, you're laughing, but eventually you would discover something a little blurry,
[00:36:30.280 --> 00:36:31.280]   right?
[00:36:31.280 --> 00:36:33.320]   Now, the lack of it doesn't mean you're not in a simulation.
[00:36:33.320 --> 00:36:39.440]   It just means that the simulation is better than one would expect or better than maybe
[00:36:39.440 --> 00:36:41.000]   you think.
[00:36:41.000 --> 00:36:50.040]   He says, if there's an underlying simulation of the universe that has the problem of finite
[00:36:50.040 --> 00:36:55.320]   computational resources, just as we do, then the laws of physics have to be put in a finite
[00:36:55.320 --> 00:36:57.120]   set of points in a finite volumes.
[00:36:57.120 --> 00:37:00.120]   This is Zora Davoudi, who's a physicist at MIT.
[00:37:00.120 --> 00:37:03.440]   Then we go back and see what kind of signatures we could find that tell us we started from
[00:37:03.440 --> 00:37:04.720]   non-continuous space time.
[00:37:04.720 --> 00:37:09.800]   In other words, the kind of evidence that would convince me as a physicist is where
[00:37:09.800 --> 00:37:13.000]   it's just kind of not quite right.
[00:37:13.000 --> 00:37:17.560]   A glitch, I think they called it in the matrix.
[00:37:17.560 --> 00:37:19.520]   The problem is whether we are or not.
[00:37:19.520 --> 00:37:22.120]   What the hell are you going to do with it?
[00:37:22.120 --> 00:37:26.080]   I fact that fascinating that Elon brought it up.
[00:37:26.080 --> 00:37:27.920]   I guess really Joshua brought it.
[00:37:27.920 --> 00:37:28.920]   Right.
[00:37:28.920 --> 00:37:29.920]   And Elon though jumped on it.
[00:37:29.920 --> 00:37:31.400]   It was like, yeah.
[00:37:31.400 --> 00:37:32.800]   One in a billion.
[00:37:32.800 --> 00:37:34.480]   I mean, there's a guy really believes that.
[00:37:34.480 --> 00:37:35.480]   He says it's uncertainty.
[00:37:35.480 --> 00:37:36.840]   Basically, he says it's a certainty.
[00:37:36.840 --> 00:37:37.840]   Right.
[00:37:37.840 --> 00:37:39.720]   He's going through life like that is insanity.
[00:37:39.720 --> 00:37:43.800]   I mean, I walked away from this listening to this entire talk with my head just exploding,
[00:37:43.800 --> 00:37:47.400]   thinking, genius, insane, genius, insane.
[00:37:47.400 --> 00:37:51.360]   The one that he got to at the end of the talk was neural lace.
[00:37:51.360 --> 00:37:57.320]   So his point being, let's just say one in a billion were not in a game, but this is
[00:37:57.320 --> 00:37:58.320]   actually real life.
[00:37:58.320 --> 00:37:59.320]   Yeah.
[00:37:59.320 --> 00:38:00.320]   I mean, we'll wing it.
[00:38:00.320 --> 00:38:01.320]   There's always a possibility.
[00:38:01.320 --> 00:38:03.960]   Well, if that's the case, let's go for it.
[00:38:03.960 --> 00:38:04.960]   His thing.
[00:38:04.960 --> 00:38:06.560]   Well, okay, the robots are going to take over.
[00:38:06.560 --> 00:38:10.680]   I'm over generalizing, but he definitely has a fear of artificial intelligence as we
[00:38:10.680 --> 00:38:11.680]   discussed.
[00:38:11.680 --> 00:38:12.680]   Right.
[00:38:12.680 --> 00:38:17.080]   And he said that the way that humans will continue to evolve and exist is that we will
[00:38:17.080 --> 00:38:25.360]   create an artificial intelligence neural lace that intersects with our own brains to augment
[00:38:25.360 --> 00:38:27.640]   our brains.
[00:38:27.640 --> 00:38:33.280]   And then we will be able to compete and still maintain dominance over the artificial intelligence
[00:38:33.280 --> 00:38:35.280]   agents.
[00:38:35.280 --> 00:38:38.520]   And the way that he put it when speaking was, well, somebody's got to do it.
[00:38:38.520 --> 00:38:41.600]   I mean, if nobody else does it, I might do it.
[00:38:41.600 --> 00:38:45.800]   But somebody's kind of explaining Elon's willingness to bet big, big bets.
[00:38:45.800 --> 00:38:48.400]   It's like, well, if it's a simulation, you might as well.
[00:38:48.400 --> 00:38:49.400]   And what does it matter?
[00:38:49.400 --> 00:38:51.600]   You're playing with house money or just all software.
[00:38:51.600 --> 00:38:52.600]   Right.
[00:38:52.600 --> 00:38:56.920]   Well, the other fascinating insights he said when asked about space, he said, well, I
[00:38:56.920 --> 00:39:00.320]   like doing space because it's not just solving problems.
[00:39:00.320 --> 00:39:03.600]   It's actually something that interests me and keeps me walking in the door to work every
[00:39:03.600 --> 00:39:04.600]   day.
[00:39:04.600 --> 00:39:06.840]   Like Tesla is just solving problems.
[00:39:06.840 --> 00:39:08.960]   He needs, he needs a little extra.
[00:39:08.960 --> 00:39:09.960]   Oomph.
[00:39:09.960 --> 00:39:10.960]   Right.
[00:39:10.960 --> 00:39:11.960]   To keep him interested.
[00:39:11.960 --> 00:39:12.960]   How are we going to get to Mars?
[00:39:12.960 --> 00:39:16.840]   He says he's going to put the man on Mars in what a couple of decades.
[00:39:16.840 --> 00:39:17.840]   All right.
[00:39:17.840 --> 00:39:18.840]   Yeah.
[00:39:18.840 --> 00:39:21.200]   Well, it doesn't matter if it's a simulation.
[00:39:21.200 --> 00:39:23.160]   You just press, you know, put another quarter in.
[00:39:23.160 --> 00:39:27.560]   But then that's you go over and you listen to Jeff Bezos talk about space and you realize
[00:39:27.560 --> 00:39:29.520]   he makes it sound so much more sane.
[00:39:29.520 --> 00:39:36.800]   I mean, Elon made it sound sane in some aspects, but Jeff Bezos made it sound logical and pragmatic.
[00:39:36.800 --> 00:39:42.480]   I think the problem, here's the real problem.
[00:39:42.480 --> 00:39:46.840]   Once you have a certain amount of money, you can do anything you want.
[00:39:46.840 --> 00:39:47.840]   Yeah.
[00:39:47.840 --> 00:39:50.000]   And it's kind of boring.
[00:39:50.000 --> 00:39:54.320]   So you come up with crazy things to keep yourself interested because the things that
[00:39:54.320 --> 00:39:59.240]   challenge us, like, you know, where am I going to get a dinner tonight?
[00:39:59.240 --> 00:40:01.520]   And it's just not, you got people doing that.
[00:40:01.520 --> 00:40:05.840]   You don't need, you are already in a different universe from the rest of us.
[00:40:05.840 --> 00:40:06.840]   Don't you think?
[00:40:06.840 --> 00:40:09.000]   Do you know any really, really rich people?
[00:40:09.000 --> 00:40:10.000]   I do.
[00:40:10.000 --> 00:40:11.320]   You know, my mom's a realtor, right?
[00:40:11.320 --> 00:40:16.600]   So she, on Maui, so she deals with a lot of extraordinarily wealthy people.
[00:40:16.600 --> 00:40:17.600]   Yeah.
[00:40:17.600 --> 00:40:18.600]   Oprah has a house there.
[00:40:18.600 --> 00:40:19.600]   Oh, yeah.
[00:40:19.600 --> 00:40:20.600]   I mean, Peter Thiel, Steve.
[00:40:20.600 --> 00:40:24.200]   There's another example of a guy living in his own reality.
[00:40:24.200 --> 00:40:25.200]   Oh, yeah.
[00:40:25.200 --> 00:40:31.440]   Steve Finn, big VC down in the valley, the Senegals who own Costco, blah, blah, blah,
[00:40:31.440 --> 00:40:33.440]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:33.440 --> 00:40:34.440]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:34.440 --> 00:40:35.440]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:35.440 --> 00:40:36.440]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:36.440 --> 00:40:42.840]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:42.840 --> 00:40:47.680]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:47.680 --> 00:40:53.680]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:53.680 --> 00:40:57.280]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:40:57.280 --> 00:41:00.960]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:41:00.960 --> 00:41:03.120]   blah, blah, blah, blah, blah, blah, blah, blah, blah, blah, blah,
[00:41:03.120 --> 00:41:06.040]   - Is that Peter? - I've seen him talk on stage, I don't think I've ever exchanged
[00:41:06.040 --> 00:41:09.200]   what you're talking about. - Yeah. He supposedly was the inspiration for that
[00:41:09.200 --> 00:41:13.920]   weirdly autistic venture capitalist at the beginning of the first season of Silicon Valley.
[00:41:13.920 --> 00:41:20.720]   - I've heard that. - Is he like that? Is he like strange? And anyway, on the one hand,
[00:41:20.720 --> 00:41:24.560]   I'm thrilled that he was going after Gawker. I couldn't, I couldn't, it's, I'm really
[00:41:24.560 --> 00:41:27.800]   torn on that one, and we've talked about this before, but not with you guys. I'm really
[00:41:27.800 --> 00:41:32.560]   torn on that because on the one hand, and I was talked down last week on this, because
[00:41:32.560 --> 00:41:37.920]   I want him, I want somebody to take down Nick then. I just really hate Gawker. But you don't
[00:41:37.920 --> 00:41:44.640]   want people just because they have means to be able to undermine the press, undermine
[00:41:44.640 --> 00:41:51.320]   democracy, and use the courts against people just because you can afford to lose a lot of
[00:41:51.320 --> 00:41:56.440]   money, right? - Free speech, First Amendment.
[00:41:56.440 --> 00:42:00.680]   - Denton and the Gawker are protected by the First Amendment.
[00:42:00.680 --> 00:42:09.080]   The laws also create space for people to sue other people and for there to be consequences.
[00:42:09.080 --> 00:42:12.760]   - But this is the point. If you have a lot of money, you can afford to sue people on
[00:42:12.760 --> 00:42:18.200]   frivolous lawsuits. Bankrupt, normal people, even Hulk Hogan is a normal person when you
[00:42:18.200 --> 00:42:23.800]   come to Peter Thiel money, bankrupt normal people defending these lawsuits. I think he,
[00:42:23.800 --> 00:42:28.880]   I think it's safe to say that Nick Denton had to go out and borrow money to pursue this,
[00:42:28.880 --> 00:42:31.920]   to defend himself. - He said that. He said he had to take equity.
[00:42:31.920 --> 00:42:37.600]   - He had to take investors. Even if there's no merit in it, if you've got billions of
[00:42:37.600 --> 00:42:41.160]   dollars, you can spend $100 million going after somebody harassing them, even if there's no
[00:42:41.160 --> 00:42:46.040]   merit in it. That's not an appropriate use of the courts at all.
[00:42:46.040 --> 00:42:49.680]   - But it's within the purview of the law. - Well, that's true.
[00:42:49.680 --> 00:42:52.440]   - And the bottom line is- - I mean, the judge has the right to say this is frivolous.
[00:42:52.440 --> 00:42:55.040]   - It used to be illegal. - Mr. Thiel, yeah.
[00:42:55.040 --> 00:42:57.880]   - In the old days, you could not find somebody else's court case.
[00:42:57.880 --> 00:43:03.720]   - Well, and there are in some states slap rules that protect people against frivolous
[00:43:03.720 --> 00:43:07.960]   libel suits and things like that. - I mean, I think the basic issue is if
[00:43:07.960 --> 00:43:13.760]   Peter Thiel can do it to Gawker, someone can do it to Twitt, someone can do it to Faz company,
[00:43:13.760 --> 00:43:19.560]   most of us would have a hard time defending ourselves against somebody with $10 billion.
[00:43:19.560 --> 00:43:24.520]   - Yeah. Okay. So let's play the other side of the coin, which is, of course, you know,
[00:43:24.520 --> 00:43:28.560]   we're all journalists. We're going to protect the First Amendment and the right to free
[00:43:28.560 --> 00:43:33.440]   speech. So let's just say that's obvious. The law says you can sue people. That is the
[00:43:33.440 --> 00:43:34.840]   law. - Yeah.
[00:43:34.840 --> 00:43:41.160]   - It took so much for someone to finally say enough with Walker.
[00:43:41.160 --> 00:43:45.800]   - Yeah, he's had a lot of, yeah, I mean- - He's even admitted and he spoke on it this
[00:43:45.800 --> 00:43:50.200]   week and said we've had some terrible news judgment. Nick Denton said this about Walker.
[00:43:50.200 --> 00:43:53.580]   - Well, now he thinks so. - Right. He was actually very recalc- recalc- recalc-
[00:43:53.580 --> 00:43:59.920]   - Yeah. - And really not the tone that I expected him to come out with. And-
[00:43:59.920 --> 00:44:04.020]   - And just got religion. Maybe there's something to be said for Peter Thiel spanking him a little
[00:44:04.020 --> 00:44:08.320]   bit. - So, I mean, I really- I know in principle,
[00:44:08.320 --> 00:44:15.380]   I definitely stand with Gawker, but I also see the legal system and the financial system
[00:44:15.380 --> 00:44:22.440]   really of capitalism as a part of journalism playing into this because it took that much.
[00:44:22.440 --> 00:44:29.180]   - You just talk me back onto the ledge. - Go get him, Peter.
[00:44:29.180 --> 00:44:30.180]   - I do think- - Go get him.
[00:44:30.180 --> 00:44:32.420]   - If it was such a great idea, he should have done it in public.
[00:44:32.420 --> 00:44:35.980]   - Maybe he's doing it in public now. - Now he has for maybe the way to address
[00:44:35.980 --> 00:44:39.020]   this is you can pay for somebody else's court case. It'll also be public.
[00:44:39.020 --> 00:44:44.660]   - Yeah, maybe. Yeah. Where's- - Because he waged a secret war, not a public one.
[00:44:44.660 --> 00:44:50.220]   - Yeah. - I'm torn because I feel constricted as a journalist because-
[00:44:50.220 --> 00:44:54.880]   - Gawker's not good for journalism. - We have such stringent rules at ABC.
[00:44:54.880 --> 00:44:58.280]   - Right. - And we have such, you know, standards-
[00:44:58.280 --> 00:45:01.960]   - All my stuff has to go through standards, has to go through legal, and we really talk
[00:45:01.960 --> 00:45:07.200]   it out. And the news judgment that's applied is so center.
[00:45:07.200 --> 00:45:08.760]   - Right. - So-
[00:45:08.760 --> 00:45:14.640]   - Now is it that way because of a fear of lawsuit? Or because it's the right thing to
[00:45:14.640 --> 00:45:20.200]   do? So you see a lot of that lately is this cover your ass stuff. We don't-
[00:45:20.200 --> 00:45:24.380]   - Yeah. Yeah. You know what I'm saying? You know what I'm saying? And so that is the chilling
[00:45:24.380 --> 00:45:30.140]   effect. Doing things because it's the right thing to do is great. Doing things because
[00:45:30.140 --> 00:45:35.860]   you don't want exposure often is not good. And you know, our lawyers are always saying,
[00:45:35.860 --> 00:45:41.700]   "Oh, don't do- I get told this a lot. Shut up. Don't do that. Don't say that." Not that
[00:45:41.700 --> 00:45:45.740]   it's wrong or that you're wrong, but it's risky.
[00:45:45.740 --> 00:45:50.840]   - I think the point that I have learned in my 11 years working at a network is if you're
[00:45:50.840 --> 00:45:52.520]   going to take the risk, it better be-
[00:45:52.520 --> 00:45:54.000]   - You better back it up. - Yeah.
[00:45:54.000 --> 00:45:59.540]   - Yeah. - And you shouldn't be unnecessarily sued. If you have a story that might cause
[00:45:59.540 --> 00:46:02.780]   somebody to sue you, you should button down every detail.
[00:46:02.780 --> 00:46:05.660]   - Mm-hmm. - Make sure you're right.
[00:46:05.660 --> 00:46:08.460]   - That's good. - Don't poke somebody in the eye for pointless
[00:46:08.460 --> 00:46:09.460]   reasons. - Right.
[00:46:09.460 --> 00:46:12.620]   - Poke them in the eye because you have facts behind that. - Right.
[00:46:12.620 --> 00:46:19.620]   - I encourage defense, and I think it's really a disingenuous defense. It's really bad.
[00:46:19.620 --> 00:46:24.500]   It's a rationale, not a defense, as well. You know, we have to speak truth to power, but
[00:46:24.500 --> 00:46:28.380]   outing Peter Teal is not speaking truth to power in any respect.
[00:46:28.380 --> 00:46:32.740]   - And that's not even the only reason why he was ticked. Peter Teal said that he was
[00:46:32.740 --> 00:46:38.860]   mad because they had done Grievous Harm to his friends, including Sean Parker and some
[00:46:38.860 --> 00:46:39.860]   of the others-
[00:46:39.860 --> 00:46:44.220]   - People know somebody who has been slimed by Gawker. I mean, they really were slimy.
[00:46:44.220 --> 00:46:47.860]   - Right. But I mean, you have to put TMZ and page six and all these guys-
[00:46:47.860 --> 00:46:53.540]   - Okay. Here's where I say this is different. And Roberto Baldwin, who schooled me on this,
[00:46:53.540 --> 00:46:56.980]   he used to work for TMZ. In the early days, he worked for Harvey Levine and TMZ. He said,
[00:46:56.980 --> 00:47:06.020]   "You know what? This is a symbiotic relationship. Within that 30-mile range, anything goes because
[00:47:06.020 --> 00:47:11.820]   the celebrities want that coverage. They call TMZ and say, "You might find me outside of
[00:47:11.820 --> 00:47:12.820]   a Walmart."
[00:47:12.820 --> 00:47:15.740]   - Sometimes they want the coverage.
[00:47:15.740 --> 00:47:16.740]   - Maybe.
[00:47:16.740 --> 00:47:17.740]   - Johnny Depp does not want the coverage.
[00:47:17.740 --> 00:47:19.740]   - He did not want that coverage. Yeah.
[00:47:19.740 --> 00:47:20.740]   - Yeah.
[00:47:20.740 --> 00:47:24.740]   - So I think that's not as utilitarian-
[00:47:24.740 --> 00:47:25.740]   - But there is a-
[00:47:25.740 --> 00:47:26.740]   - I think it is.
[00:47:26.740 --> 00:47:30.900]   - There is between Hollywood and the gossip columnists. There is a symbiosis. And that's
[00:47:30.900 --> 00:47:34.980]   part of the industry. And frankly, if you're a movie star, you get into it. You know, you're
[00:47:34.980 --> 00:47:39.740]   not going into this. Peter Thiel is a venture capitalist. I don't think he kind of made
[00:47:39.740 --> 00:47:45.460]   that implicit agreement, right? That, "Well, my private life is now public." Why would
[00:47:45.460 --> 00:47:48.940]   that- why would he even consider that? Just because he's technically a public figure.
[00:47:48.940 --> 00:47:52.420]   Why would he even consider that fair game for any journalistic entity?
[00:47:52.420 --> 00:47:58.220]   - Yeah. It just doesn't feel good. Any of it. It feels so yucky that it's really hard
[00:47:58.220 --> 00:48:02.820]   to- you know, I think Jeff Bezos said this week that it's not the pretty speech that
[00:48:02.820 --> 00:48:07.380]   you need to ever defend. It's the ugly speech. And that's, you know, that's the bottom line
[00:48:07.380 --> 00:48:11.340]   is you get into these horrible situations where if you're defending the First Amendment,
[00:48:11.340 --> 00:48:14.060]   you always end up doing it in cases that make you feel yucky.
[00:48:14.060 --> 00:48:17.100]   - There's a lot of stuff that is yucky and should be absolutely legal.
[00:48:17.100 --> 00:48:20.100]   - Yeah, completely-
[00:48:20.100 --> 00:48:27.580]   - I want you to tweet that. And then just let the chips follow where they do. I don't
[00:48:27.580 --> 00:48:29.780]   know what- I'm trying to think of what that would be.
[00:48:29.780 --> 00:48:34.780]   - Well, a lot of journalism that makes people uncomfortable and causes trouble for people.
[00:48:34.780 --> 00:48:36.620]   - Well, that's appropriate though. If it's-
[00:48:36.620 --> 00:48:42.700]   - It's just bad judgment. I mean, you can't legislate bad judgment down to an inch of
[00:48:42.700 --> 00:48:46.620]   its life. And so, yeah.
[00:48:46.620 --> 00:48:51.300]   - We, you know, here's- this is not going to go away because we live in a new age for
[00:48:51.300 --> 00:48:56.580]   journalism where there is intense pressure on journalistic entities to survive and there
[00:48:56.580 --> 00:49:03.780]   are all threatened by the internet and kind of the end of the homepage, you know? And
[00:49:03.780 --> 00:49:10.020]   they're all kind of in desperate need to get clicks and traffic and links. And journalistic
[00:49:10.020 --> 00:49:15.140]   rules seem to be somewhat going out the window. I mean, you're both longtime journalists.
[00:49:15.140 --> 00:49:18.580]   Some of the things that we thought 20 years ago would never happen, happen all the time
[00:49:18.580 --> 00:49:22.100]   now, even from well-known journalistic entities.
[00:49:22.100 --> 00:49:26.860]   - Well, that's where the legal system and the financial sort of punishments here that may
[00:49:26.860 --> 00:49:32.740]   be handed down in this case. I'm not saying that Peter Thiel should have funded them secretly,
[00:49:32.740 --> 00:49:37.580]   but they may have a chilling effect on the needless publishing and stuff that doesn't
[00:49:37.580 --> 00:49:40.940]   need to be out there and doesn't help anyone. I mean, that's one of the things that Nick
[00:49:40.940 --> 00:49:48.220]   Denton said is that the amount of information that's now available has increased exponentially
[00:49:48.220 --> 00:49:54.700]   as much as the competition and all the other journalistic sources online news sources.
[00:49:54.700 --> 00:49:59.820]   So they have to be more discriminating about what they choose to publish.
[00:49:59.820 --> 00:50:07.260]   - Speaking of artificial intelligence, Google has now said that we are hiring novelists
[00:50:07.260 --> 00:50:15.860]   to give our artificial intelligence personalities a backstory. This is a fast company story.
[00:50:15.860 --> 00:50:18.700]   - I'm not really disappointing because one of the things I've lacked about Google now
[00:50:18.700 --> 00:50:24.220]   up until now is it's made no attempt to have a personality. It doesn't understand jokes.
[00:50:24.220 --> 00:50:32.460]   It just is really smart about the stuff it does as opposed to Siri and Cartana and Alexa,
[00:50:32.460 --> 00:50:35.220]   all of which are programmed to be funny.
[00:50:35.220 --> 00:50:36.540]   - You get annoying, don't you?
[00:50:36.540 --> 00:50:43.420]   - After a certain point, it was very entertaining and novelty when Siri did it a few years ago.
[00:50:43.420 --> 00:50:47.580]   I like the fact that Google is utilitarian and I'm a little worried about them getting
[00:50:47.580 --> 00:50:48.860]   a little bit too cute.
[00:50:48.860 --> 00:50:52.860]   - They're not after you. Just like the television networks really care about the millennial
[00:50:52.860 --> 00:50:59.820]   demo, they are trying to attract users for their entire life. And that's my kids.
[00:50:59.820 --> 00:51:03.060]   - And do your kids want the exchanges to be more human-like?
[00:51:03.060 --> 00:51:07.860]   - Oh, totally. Their expectation of human language is high. And that's why one of the
[00:51:07.860 --> 00:51:10.780]   women that they hired to do this is a woman named Emma Coates.
[00:51:10.780 --> 00:51:12.860]   - She's from Pixar. - She's from Pixar.
[00:51:12.860 --> 00:51:18.980]   And at the Mughfest, she and Ryan Germank, who was one of the other guys, they were joking
[00:51:18.980 --> 00:51:25.660]   that this is really about humor. And one of the things is they're preparing answers to
[00:51:25.660 --> 00:51:29.700]   questions that they get a lot, which is, "Do you fart?"
[00:51:29.700 --> 00:51:36.340]   This is a quote that they said, "Do you fart?" And they might lead the artificial intelligence
[00:51:36.340 --> 00:51:42.180]   to say, "Not recently." Or perhaps something more sassy, maybe Google could ping data about
[00:51:42.180 --> 00:51:51.940]   nearby air quality test results. So, I mean, I'm being flip about this, but there is a
[00:51:51.940 --> 00:51:58.380]   willingness to go to a different demo than your traditional tech reviewer, tech mature
[00:51:58.380 --> 00:52:04.380]   audience. And I think that they're realizing that there's an opportunity. We aren't going
[00:52:04.380 --> 00:52:06.580]   to ask those questions. Our kids will make the video.
[00:52:06.580 --> 00:52:08.900]   - I have to say they can do it better than Siri and Alexa.
[00:52:08.900 --> 00:52:14.060]   - Isn't it those very kind of singularly unsatisfying when Siri says, "I can't answer
[00:52:14.060 --> 00:52:16.540]   that question," or Alexis says, "I don't know what you're talking about."
[00:52:16.540 --> 00:52:20.420]   - Well, Google has a good or better chance than anybody of not having that happen.
[00:52:20.420 --> 00:52:28.460]   - They at least can answer the question. But what if you ask it a nonsensical question?
[00:52:28.460 --> 00:52:33.500]   What should its response be? Should it just be that question is nonsense? That's kind
[00:52:33.500 --> 00:52:34.500]   of insulting.
[00:52:34.500 --> 00:52:39.740]   - I personally would it be okay with it saying, "I don't understand, Dave." Or hearing.
[00:52:39.740 --> 00:52:43.740]   - Yeah, why are chickens blue? And it goes, "What is it going to say?"
[00:52:43.740 --> 00:52:47.500]   - But it is a good point. They want everybody to use this. And Google now is really great,
[00:52:47.500 --> 00:52:50.460]   but I have the feeling that even a pretty high percentage of people who have Android
[00:52:50.460 --> 00:52:56.100]   phones have not done nearly as much with Google now as they could, because it's not all that
[00:52:56.100 --> 00:52:57.500]   engaging. It's just useful.
[00:52:57.500 --> 00:53:01.380]   - Well, but this is the point, is that Senator Pichai said this week that they are trying
[00:53:01.380 --> 00:53:07.700]   to make an individual Google for every single user. And so that if they were true AI, it
[00:53:07.700 --> 00:53:10.060]   would learn whether you want that or not.
[00:53:10.060 --> 00:53:12.540]   - That would be good. There you go.
[00:53:12.540 --> 00:53:16.420]   - I think what Google, and I mentioned this before, but I want to say it again, Google's
[00:53:16.420 --> 00:53:21.700]   real opportunity here is to be your intermediary to the rest of the world. So where bots went
[00:53:21.700 --> 00:53:28.740]   wrong is that you had to have this relationship with a variety of fairly annoying things.
[00:53:28.740 --> 00:53:34.260]   And like whether it's 1-800-Flowers or Pancho, the weathercat, I don't want all these relationships.
[00:53:34.260 --> 00:53:40.020]   What I really want is one relationship, a personal assistant who then goes out and has
[00:53:40.020 --> 00:53:42.100]   the relationships with Pancho and these other things.
[00:53:42.100 --> 00:53:44.100]   - Microsoft is sort of drawing that distinction.
[00:53:44.100 --> 00:53:45.100]   - Is that what Cortana's going to do?
[00:53:45.100 --> 00:53:48.940]   - Microsoft says there are agents and an agent is something that works on your behalf
[00:53:48.940 --> 00:53:50.900]   and knows an awful lot about you.
[00:53:50.900 --> 00:53:54.460]   And your agent works with bots and a bot might be from 1-800-Flowers.
[00:53:54.460 --> 00:53:55.460]   - That's a better way to do it.
[00:53:55.460 --> 00:53:56.460]   - Or a hotel chain.
[00:53:56.460 --> 00:53:58.620]   - Yeah. - And they work on behalf of the hotel
[00:53:58.620 --> 00:54:00.100]   chain and know a lot about it.
[00:54:00.100 --> 00:54:04.020]   But the only access they have to your data is what your agent gives to them.
[00:54:04.020 --> 00:54:07.500]   - But Jeff Bezos said something that scared me at Recode this week. He said, "Well, we're
[00:54:07.500 --> 00:54:09.660]   all going to have a lot of these."
[00:54:09.660 --> 00:54:15.180]   And I imagine this man-o-piece filled with an echo and a Google home and a bunch of things.
[00:54:15.180 --> 00:54:19.780]   And then you have to remember which can do what and who to ask for and what the syntax
[00:54:19.780 --> 00:54:25.260]   is. And that seemed like the future Jeff Bezos envisioned. I hope that's not the case.
[00:54:25.260 --> 00:54:30.020]   This is just a whole new way of having the ecosystem discussion.
[00:54:30.020 --> 00:54:37.140]   So I think that we all live within multiple ecosystems and you'll choose which one does
[00:54:37.140 --> 00:54:43.900]   what for you. So if you really want to have something that, "Oh, I'm out of cornmeal,
[00:54:43.900 --> 00:54:49.020]   Alexa, or more cornmeal, I'm just making this up." Or if it's more about music or...
[00:54:49.020 --> 00:54:51.940]   - By the way, you just bought cornmeal for a several hundred people.
[00:54:51.940 --> 00:54:52.940]   - Oh, sorry.
[00:54:52.940 --> 00:54:53.940]   - You know what I'm listening to audience.
[00:54:53.940 --> 00:54:56.940]   I hope you can use that cornmeal, everybody.
[00:54:56.940 --> 00:54:57.940]   - Good stuff.
[00:54:57.940 --> 00:54:58.940]   Maybe I'll get it.
[00:54:58.940 --> 00:54:59.940]   - I'll be in a good recipe that involves cornmeal.
[00:54:59.940 --> 00:55:03.460]   - Alexa, give me a polenta recipe and then we'll be all set.
[00:55:03.460 --> 00:55:10.500]   - Alexa, cancel the cornmeal order, please.
[00:55:10.500 --> 00:55:14.820]   So I mean, I think that you will choose an ecosystem based on your most pressing needs,
[00:55:14.820 --> 00:55:17.180]   interests, and existing affiliations.
[00:55:17.180 --> 00:55:20.660]   - This is what Bezos said. I think they're going to be a bunch of artificially intelligent
[00:55:20.660 --> 00:55:25.500]   agents in the world. They're going to be specialties. You may not ask the same AI for everything.
[00:55:25.500 --> 00:55:29.700]   I bet the average household will use a number of these, but to me, that's a very exciting
[00:55:29.700 --> 00:55:34.580]   seed we've planted. I love working on stuff like that and the team is brilliant. I don't
[00:55:34.580 --> 00:55:35.580]   know.
[00:55:35.580 --> 00:55:38.500]   - It's complicated, though, because I want a relationship with Facebook. I want one with
[00:55:38.500 --> 00:55:39.500]   Google. I want one with Amazon.
[00:55:39.500 --> 00:55:40.500]   - Well, that's true.
[00:55:40.500 --> 00:55:41.500]   - They're all quite different.
[00:55:41.500 --> 00:55:42.500]   - One of them's going to win.
[00:55:42.500 --> 00:55:45.540]   - It's not like choosing between windows and the Mac where they're kind of doing the
[00:55:45.540 --> 00:55:49.900]   same thing in a largely similar way. These companies are all doing different stuff.
[00:55:49.900 --> 00:55:58.300]   See, in my imagination, I think that Google, let's say Google, is the winner, is the incumbent.
[00:55:58.300 --> 00:56:02.900]   My house will be a Google house. I will say Google, I'm out of toilet paper, order me
[00:56:02.900 --> 00:56:06.820]   some more, and it will tell Alexa, like, get Leo some toilet paper.
[00:56:06.820 --> 00:56:10.340]   - It'll talk to 50 different bots and I want to keep us the fastest.
[00:56:10.340 --> 00:56:15.660]   - I don't want 12 personal assistants, each with a different skill. No one wants that.
[00:56:15.660 --> 00:56:16.660]   Right?
[00:56:16.660 --> 00:56:21.940]   - I'm also going to start doubting Alexa who's making noise.
[00:56:21.940 --> 00:56:23.780]   - Alexa, by cornmeal stock.
[00:56:23.780 --> 00:56:25.660]   - Alexa's next to me now.
[00:56:25.660 --> 00:56:28.900]   - Oh, that's Echo. Who's this?
[00:56:28.900 --> 00:56:29.900]   - That's Echo.
[00:56:29.900 --> 00:56:30.900]   - Echo.
[00:56:30.900 --> 00:56:35.380]   - You say Echo. And by the way, that one is triggered by Echo, not by the A word.
[00:56:35.380 --> 00:56:40.220]   - Okay. I won't say her name or the repeating sound word.
[00:56:40.220 --> 00:56:43.860]   - It's like saying you want four different operating systems in your house. You want
[00:56:43.860 --> 00:56:45.820]   it. You're going to pick one, aren't you?
[00:56:45.820 --> 00:56:48.540]   And have it. That's what an operating system does. It intermediates.
[00:56:48.540 --> 00:56:50.740]   - No. Because right now, what I mean, my--
[00:56:50.740 --> 00:56:51.740]   - I guess we all have that.
[00:56:51.740 --> 00:56:53.060]   - Your house doesn't count because you're--
[00:56:53.060 --> 00:56:58.180]   - No, I'm not usual. But I guess everybody will have an iOS or an Android device and
[00:56:58.180 --> 00:57:01.140]   maybe a Mac and a Windows device. I guess that's not unusual.
[00:57:01.140 --> 00:57:02.820]   - Yeah. I mean, we have a Nest, which is--
[00:57:02.820 --> 00:57:04.180]   - Right. That's its own thing.
[00:57:04.180 --> 00:57:10.660]   - Yeah. Which is Google world. We have-- we buy all our stuff on Amazon. We have mostly
[00:57:10.660 --> 00:57:16.300]   Macs and iPhones. I mean, it's-- we are all living in a ecosystem that looks like a
[00:57:16.300 --> 00:57:17.300]   Venn diagram.
[00:57:17.300 --> 00:57:26.580]   - Yeah. That's not ideal. I mean, in her, you dealt with Scarlett, Johansson. And she
[00:57:26.580 --> 00:57:30.580]   did everything for you. Right? Isn't that what you want?
[00:57:30.580 --> 00:57:37.620]   - Well, you want-- you'd like to have a choice between her and competition.
[00:57:37.620 --> 00:57:38.620]   - You'll have a choice.
[00:57:38.620 --> 00:57:39.620]   - That's equally good.
[00:57:39.620 --> 00:57:40.620]   - You don't have it to be Microsoft.
[00:57:40.620 --> 00:57:41.620]   - You don't have a choice.
[00:57:41.620 --> 00:57:42.620]   - But you'll pick one.
[00:57:42.620 --> 00:57:43.620]   - But you'll pick one.
[00:57:43.620 --> 00:57:45.900]   - 1990s when it was a period where Microsoft seemed like the only available technology
[00:57:45.900 --> 00:57:46.900]   provider.
[00:57:46.900 --> 00:57:47.900]   - Right.
[00:57:47.900 --> 00:57:51.900]   - Well, we've seen this before in RIT, it's very, very small with instant messenger programs
[00:57:51.900 --> 00:57:56.540]   where every-- you know, you had ICQ and you had AIM and you had Microsoft Messenger and
[00:57:56.540 --> 00:58:01.540]   none of them talked to each other. And each of them wanted to own that space 100%. And
[00:58:01.540 --> 00:58:05.340]   really the way to win would have been to have one that can talk to everybody.
[00:58:05.340 --> 00:58:06.340]   - What is that?
[00:58:06.340 --> 00:58:08.740]   - But nobody wanted to do that because they wanted to own the market.
[00:58:08.740 --> 00:58:09.740]   - I think you're presuming this.
[00:58:09.740 --> 00:58:11.220]   - It killed AIM, by the way.
[00:58:11.220 --> 00:58:16.460]   - I think you're presuming this based on a hardware mindset. Because you're presuming
[00:58:16.460 --> 00:58:20.700]   that you're only going to buy one piece of hardware that's going to run your house.
[00:58:20.700 --> 00:58:23.460]   - I don't want a mantelpiece full of black tools.
[00:58:23.460 --> 00:58:26.740]   - This is an intermediary step. You're going to have all--
[00:58:26.740 --> 00:58:27.900]   - The house will listen.
[00:58:27.900 --> 00:58:31.180]   - You have all kinds of stuff now, your phone, blah, blah, blah. You could have all kinds
[00:58:31.180 --> 00:58:35.700]   of stuff that, you know, the air fresheners on your mantelpiece is an intermediary step.
[00:58:35.700 --> 00:58:36.700]   - Okay.
[00:58:36.700 --> 00:58:40.680]   - And the hardware is not the issue because you look at Amazon as an example. It is a
[00:58:40.680 --> 00:58:45.580]   crime that they are charging anything for this device, given that it is a conduit to
[00:58:45.580 --> 00:58:46.580]   sell more things.
[00:58:46.580 --> 00:58:47.580]   - And they charge a lot.
[00:58:47.580 --> 00:58:49.300]   - It's almost 200 bucks. It's really expensive.
[00:58:49.300 --> 00:58:52.860]   - They will give these things away because each one of them will find a business model
[00:58:52.860 --> 00:58:56.260]   that has nothing to do with the hardware. And that's why Apple's not in it right now
[00:58:56.260 --> 00:59:00.100]   is because they can't figure out the business model without the hardware, as my personal
[00:59:00.100 --> 00:59:02.620]   opinion. They still think of themselves as a hardware company.
[00:59:02.620 --> 00:59:07.100]   - So the news is that maybe they are working on something.
[00:59:07.100 --> 00:59:08.100]   - What?
[00:59:08.100 --> 00:59:11.900]   - That Apple is working on something at least roughly akin to an echo.
[00:59:11.900 --> 00:59:14.780]   - All right. So this is the WWDC announcement, right?
[00:59:14.780 --> 00:59:16.660]   - No, it's too soon.
[00:59:16.660 --> 00:59:21.900]   - Not for WWDC, but at some point. I mean, I think at WWDC there probably will be news
[00:59:21.900 --> 00:59:30.620]   about Siri that starts to tiptoe in this direction, such as opening up Siri to third-party services.
[00:59:30.620 --> 00:59:33.980]   - Would Apple's gonna come in and take such time? It doesn't need to come out with something
[00:59:33.980 --> 00:59:37.900]   like the echo immediately. It will do it when it feels like it has something which is more
[00:59:37.900 --> 00:59:39.540]   interesting than the echo.
[00:59:39.540 --> 00:59:45.020]   - I think Apple's gonna be at a disadvantage because they're so worried about privacy, right?
[00:59:45.020 --> 00:59:46.020]   - Hmm.
[00:59:46.020 --> 00:59:49.100]   - And they don't have huge AI chops yet.
[00:59:49.100 --> 00:59:57.260]   - Echo, order me an Elvis costume. Echo, order me an Elvis costume.
[00:59:57.260 --> 01:00:04.260]   - I found Leo's order history. I found Elvis now to look so low how this costume.
[01:00:04.260 --> 01:00:06.260]   White, extra large.
[01:00:06.260 --> 01:00:07.260]   - Wow.
[01:00:07.260 --> 01:00:17.260]   - Wow. So I could have an Elvis costume tomorrow.
[01:00:17.260 --> 01:00:21.260]   - Echo, are you sure he needs extra large?
[01:00:21.260 --> 01:00:27.260]   - Now, if echo is--
[01:00:27.260 --> 01:00:32.420]   - Now, is that a convenience or what? How many times have you been walking around the house
[01:00:32.420 --> 01:00:37.620]   thinking, "Gosh, you know what I really need as an Elvis costume?" And now you just ask
[01:00:37.620 --> 01:00:38.620]   the house and it delivers it.
[01:00:38.620 --> 01:00:41.260]   - Do you like clothes from Amazon? How does it not size?
[01:00:41.260 --> 01:00:46.260]   - Yeah, I could say more socks. Echo, order me some socks.
[01:00:46.260 --> 01:00:50.260]   - Jeff Bezos said that fashion is one of the categories he's most excited about in retail.
[01:00:50.260 --> 01:00:51.260]   - Yeah.
[01:00:51.260 --> 01:00:56.260]   - I can't find the answer to the question I have.
[01:00:56.260 --> 01:00:57.260]   - See, that's what you don't want to do.
[01:00:57.260 --> 01:00:58.260]   - Supposed to make a funny joke, I know.
[01:00:58.260 --> 01:01:02.260]   - Yeah. You should say socks. I thought you wanted sex.
[01:01:02.260 --> 01:01:09.260]   - No, that's not good. That's really not good. Sorry about that.
[01:01:09.260 --> 01:01:12.260]   - Sending porn to your phone now.
[01:01:12.260 --> 01:01:18.260]   - Tomorrow, you will receive some VCS, VHS tapes. Take a break.
[01:01:18.260 --> 01:01:20.260]   - Oh, Lord.
[01:01:20.260 --> 01:01:25.260]   - You've really kicked off the Alexa owners now. It was like a full fledged.
[01:01:25.260 --> 01:01:29.260]   - No, because I don't say the A word. I trained it to say the E word.
[01:01:29.260 --> 01:01:33.260]   Now, of course, there's people who listen to the show who say, "Oh, I can't use the A word,
[01:01:33.260 --> 01:01:35.260]   so I'm going to use the E word now. They're really mad."
[01:01:35.260 --> 01:01:36.260]   - I see.
[01:01:36.260 --> 01:01:39.260]   - But you can also use the other A word, Amazon.
[01:01:39.260 --> 01:01:40.260]   - Mm-hmm.
[01:01:40.260 --> 01:01:45.260]   - But that's another thing, by the way. Each of these should have its own unique trigger phrase.
[01:01:45.260 --> 01:01:51.260]   Why, in heaven's name, are we still like all using the same trigger phrase?
[01:01:51.260 --> 01:01:52.260]   That's terrible.
[01:01:52.260 --> 01:01:54.260]   - Tell me. Different Google for every reason.
[01:01:54.260 --> 01:01:59.260]   - By the way, have you seen our Google home? Have you tried it yet?
[01:01:59.260 --> 01:02:01.260]   It smells really good.
[01:02:01.260 --> 01:02:03.260]   - Oh, my.
[01:02:03.260 --> 01:02:06.260]   - Cinnamon and vanilla flavor. That's new. And probably not going to last.
[01:02:06.260 --> 01:02:08.260]   - I'm going to put that all away.
[01:02:08.260 --> 01:02:10.260]   - Our show today.
[01:02:10.260 --> 01:02:11.260]   - Thank you.
[01:02:11.260 --> 01:02:19.260]   - It is a Glade air freshener. But I think that that's pretty close in size and shape.
[01:02:19.260 --> 01:02:20.260]   - I like that.
[01:02:20.260 --> 01:02:24.260]   - In the chat room, the gem doctor says, "You know, an Elvis costume is such a waste of money.
[01:02:24.260 --> 01:02:28.260]   A Pantamom Ours costume, however. It would be far more useful."
[01:02:28.260 --> 01:02:31.260]   [laughter]
[01:02:31.260 --> 01:02:35.260]   That takes two. That's the problem. It takes two people to wear it.
[01:02:35.260 --> 01:02:38.260]   - You should not down for that.
[01:02:38.260 --> 01:02:40.260]   [laughter]
[01:02:40.260 --> 01:02:42.260]   - No, I'm not going to do it. I'm tempted.
[01:02:42.260 --> 01:02:46.260]   But I don't think there'll be a Pantamom Ours costume in my order history.
[01:02:46.260 --> 01:02:48.260]   It's surprising what is in my order history, though.
[01:02:48.260 --> 01:02:51.260]   You can try a few things. Just see what happens.
[01:02:51.260 --> 01:02:55.260]   That's another kind of a leak of information.
[01:02:55.260 --> 01:03:00.260]   Yes, I know. Wealthfront. I know. Our show today brought you my wealthfront.com.
[01:03:00.260 --> 01:03:06.260]   You invest, I hope, for the long term, for you and your financial health,
[01:03:06.260 --> 01:03:10.260]   your family's financial health. You got kids. You got kids going to college someday.
[01:03:10.260 --> 01:03:13.260]   It seems like that's a long way off. They're only eight years old.
[01:03:13.260 --> 01:03:17.260]   Ten years is nothing. Nothing's start putting money aside.
[01:03:17.260 --> 01:03:21.260]   But obviously, you're not going to put that in the savings account at the bank.
[01:03:21.260 --> 01:03:23.260]   You might as well put in their mattress if you could do that.
[01:03:23.260 --> 01:03:25.260]   It just goes a little way. Nothing.
[01:03:25.260 --> 01:03:28.260]   You need to invest it. But how do you invest?
[01:03:28.260 --> 01:03:33.260]   Do you hire somebody and pay them one, two, three percent of what you've got under management?
[01:03:33.260 --> 01:03:37.260]   By that time, you've got to make three percent more just to break even.
[01:03:37.260 --> 01:03:40.260]   Are you going to do it yourself? That's what I thought.
[01:03:40.260 --> 01:03:43.260]   I'll read all the books. I'll be smart and I'll learn.
[01:03:43.260 --> 01:03:47.260]   I think I did a good job ten years ago. I haven't checked in a while.
[01:03:47.260 --> 01:03:51.260]   That's the problem. You need to kind of monitor this stuff and rebalance and rejigger
[01:03:51.260 --> 01:03:54.260]   and kind of keep an eye on what's going on.
[01:03:54.260 --> 01:03:58.260]   Well, I got a way to do it that doesn't cost you three percent of what you've got under investment.
[01:03:58.260 --> 01:04:01.260]   It doesn't cost you any time. That does a better job than you could ever do
[01:04:01.260 --> 01:04:05.260]   because it's watching all the time. It's called Wealthfront.
[01:04:05.260 --> 01:04:09.260]   And because Wealthfront is computerized, it's software.
[01:04:09.260 --> 01:04:17.260]   Software works cheap. One quarter of one percent a year with no additional charges, no transaction fees or anything.
[01:04:17.260 --> 01:04:20.260]   Wealthfront is really, really interesting.
[01:04:20.260 --> 01:04:26.260]   This was created by some very, very smart people using something called modern portfolio theory.
[01:04:26.260 --> 01:04:32.260]   It adjusts your own personal risk tolerance and your time frame, but it does stay diversified.
[01:04:32.260 --> 01:04:40.260]   And tax efficient. They use tax loss harvesting to make sure that you get the highest return at the lowest tax cost to you.
[01:04:40.260 --> 01:04:46.260]   It's really cool. You can get this Wealthfront dashboard where you can see exactly what they're invested in.
[01:04:46.260 --> 01:04:52.260]   And for a day by day hour, by hour, minute by minute, it's not now, of course they're constantly rebalancing,
[01:04:52.260 --> 01:04:56.260]   but it's not day trading. This is for long-term investing.
[01:04:56.260 --> 01:05:00.260]   It's going to take a few minutes to sign up. I'm not saying do this right away,
[01:05:00.260 --> 01:05:06.260]   but I do want you to go right now to wealthfront.com/twit and at least get the portfolio analysis,
[01:05:06.260 --> 01:05:11.260]   answer just a few questions about time frame and risk, and they will actually create a portfolio for you.
[01:05:11.260 --> 01:05:20.260]   But they'll look at your existing portfolio, see how much you're spending in fees, how efficient it is, what the tax consequences are of your investments.
[01:05:20.260 --> 01:05:28.260]   And then every trade they make, once you do invest, you can see right there, along with all your other accounts, they put it all in one big dashboard, whether they're at Wealthfront or not.
[01:05:28.260 --> 01:05:32.260]   Wealthfront is now managing $3 billion in client assets because it works. It really works.
[01:05:32.260 --> 01:05:41.260]   I want you to find out more. Go to Wealthfront.com/twit. $500 is the minimum investment, so it's easy to test it out, but I love this.
[01:05:41.260 --> 01:05:46.260]   As a Twit listener, you're going to get $15,000 managed for free.
[01:05:46.260 --> 01:05:51.260]   You're first $15,000 entirely free of charge and not just for the first year, but forever.
[01:05:51.260 --> 01:06:05.260]   So that is a really great way to start your nest egg. If you haven't started saving for retirement, for college, for a house, for vacation, just a rainy day fund, everybody had one of those, Wealthfront.com/twit.
[01:06:05.260 --> 01:06:10.260]   We thank them so much for their support of this week in tech.
[01:06:10.260 --> 01:06:12.260]   What a week.
[01:06:12.260 --> 01:06:24.260]   AI is a good subject. Facebook is using AIs to look at offensive photos. That's actually good.
[01:06:24.260 --> 01:06:33.260]   I once saw an article about the poor people who have to look at every image. What a horrible, horrible job that must be.
[01:06:33.260 --> 01:06:43.260]   It's horrible.
[01:06:43.260 --> 01:07:06.260]   I think that's great.
[01:07:06.260 --> 01:07:15.260]   I regret ever having seen. Like me and Elvis. It would be nice that nobody ever had to see that at all.
[01:07:15.260 --> 01:07:32.260]   I did a story recently on Whisper, which is using AI to hide secrets, which people post to Whisper before they're using AI for something similar to what Facebook is doing, but for text messages.
[01:07:32.260 --> 01:07:38.260]   How is the big problem with secret and Whisper and all of these? People were using them to say really scurless things anonymously.
[01:07:38.260 --> 01:07:44.260]   Whisper doesn't want you to be able to do that. They're able to hide a lot of it without humans being involved. Not all of it, but not a lot.
[01:07:44.260 --> 01:07:45.260]   That would be great.
[01:07:45.260 --> 01:07:49.260]   They also say that computers are better at moderating stuff than human beings are.
[01:07:49.260 --> 01:07:56.260]   This I find hard to believe. Obviously they've gotten better than the days where it would measure the amount of flesh in a photo.
[01:07:56.260 --> 01:08:02.260]   That's what it did. More than 30% was naked flesh. We've got to figure there's some nudity here.
[01:08:02.260 --> 01:08:10.260]   How do you see a beheading, for instance, and know what that is? That's got to be pretty sophisticated.
[01:08:10.260 --> 01:08:17.260]   Although when I think about how Google Photos would a good job categorizing my photos, all of my beheading photos are in a single photo.
[01:08:17.260 --> 01:08:21.260]   That's a terrible joke.
[01:08:21.260 --> 01:08:26.260]   We've gone to be heading. That's a new rule.
[01:08:26.260 --> 01:08:31.260]   Was it periscope this week? Periscope has a new...
[01:08:31.260 --> 01:08:33.260]   I like this idea. A jury system.
[01:08:33.260 --> 01:08:40.260]   They're going to have crowdsourcing the comments that are deemed offensive.
[01:08:40.260 --> 01:08:46.260]   If someone who's broadcasting sees a comment that's offensive, they can flag it.
[01:08:46.260 --> 01:09:01.260]   The people who are in that broadcast viewing it, a select group will be chosen randomly to vote on whether they felt that that comment was inappropriate, offensive, or abusive.
[01:09:01.260 --> 01:09:08.260]   The way they described it is a little bit laborious because those seven or eight people vote.
[01:09:08.260 --> 01:09:17.260]   The results of that vote are shown to the entire group. They decide if the commenter should be banned from commenting in that broadcast.
[01:09:17.260 --> 01:09:23.260]   That's the other thing that I find to be a little bit of a problem here, is that the penalties are kind of limited.
[01:09:23.260 --> 01:09:34.260]   You'd like to see them maybe do that on a one-time basis, but if one user is banned repeatedly, maybe then they have a more serious repercussion.
[01:09:34.260 --> 01:09:37.260]   But the idea of crowdsourcing comments. Thoughts?
[01:09:37.260 --> 01:09:40.260]   I think this is interesting mostly because they choose a jury randomly.
[01:09:40.260 --> 01:09:46.260]   So that would, I would think, eliminate kind of mob, rule, and ganging up on people and stuff.
[01:09:46.260 --> 01:09:50.260]   But maybe not. I think it's a great social experiment. I'll be very curious what happens.
[01:09:50.260 --> 01:09:54.260]   And it's like the opposite of AI. It's using actual human brains.
[01:09:54.260 --> 01:09:59.260]   Well, we know ultimately aren't human brains better at this. Maybe not. I don't know. Facebook says not.
[01:09:59.260 --> 01:10:07.260]   I don't think so given. There are so many examples of humans either not moderating stuff they should have or moderating stuff they shouldn't have moderated.
[01:10:07.260 --> 01:10:16.260]   It is so fuzzy and it almost none of this is binary. And even Facebook as rules, which they don't entirely understand saying things like beheadings are okay.
[01:10:16.260 --> 01:10:19.260]   If you're not bringing them up there because you love beheadings.
[01:10:19.260 --> 01:10:22.260]   It's a news story because you hate beheadings and it's okay.
[01:10:22.260 --> 01:10:24.260]   How do you know? What's the sentiment?
[01:10:24.260 --> 01:10:37.260]   We can talk about AI all day long, but fair use, which is basically, I mean, whether you look at it in the legal context or in this context of posting something that's inappropriate in one context but appropriate in another.
[01:10:37.260 --> 01:10:43.260]   That is got to be one of the hardest things for humans and let alone.
[01:10:43.260 --> 01:10:47.260]   We think that Google Oracle trial, they had a judge for use. Wow.
[01:10:47.260 --> 01:10:50.260]   Yeah, on that. No thanks.
[01:10:50.260 --> 01:10:56.260]   Yeah, have you ever come reading comments on Facebook? I'm just picking Facebook randomly.
[01:10:56.260 --> 01:11:00.260]   Have you ever hit the abusive button to report views?
[01:11:00.260 --> 01:11:01.260]   Never.
[01:11:01.260 --> 01:11:04.260]   You don't see them. Maybe I don't see them.
[01:11:04.260 --> 01:11:07.260]   Now on Twitter, I would do it at least 50% of the time.
[01:11:07.260 --> 01:11:08.260]   Really?
[01:11:08.260 --> 01:11:09.260]   Oh, yeah.
[01:11:09.260 --> 01:11:10.260]   For a while, I would always report spammers.
[01:11:10.260 --> 01:11:11.260]   Yeah.
[01:11:11.260 --> 01:11:12.260]   Twitter could use some of these tools.
[01:11:12.260 --> 01:11:19.260]   Now, I think it's interesting that Periscope is doing this because I would wonder if this goes well on Periscope, Twitter owns Periscope, maybe they could apply this to the Twitter.
[01:11:19.260 --> 01:11:21.260]   They could apply this to tweets.
[01:11:21.260 --> 01:11:29.260]   I like the idea of the people who are in the content being polled to make the decision.
[01:11:29.260 --> 01:11:31.260]   Right. And you can do that with a live stream.
[01:11:31.260 --> 01:11:32.260]   You...
[01:11:32.260 --> 01:11:36.260]   Because they know the context. They're watching the video. They know contextually.
[01:11:36.260 --> 01:11:39.260]   Is that an offensive comment in this context?
[01:11:39.260 --> 01:11:42.260]   So would that be enough? I mean, I'm trying to think, draw the parallel to Twitter.
[01:11:42.260 --> 01:11:46.260]   Twitter, you can't really. We just have to be based on what you said.
[01:11:46.260 --> 01:11:47.260]   Yeah.
[01:11:47.260 --> 01:11:50.260]   But still, I think I like the idea of a randomly selected jury.
[01:11:50.260 --> 01:11:52.260]   You're always a wisdom of crowds, guys.
[01:11:52.260 --> 01:11:57.260]   Who would it be? Who would it be, though? Yeah, I like that. I like the wisdom of crowds. It's the individuals I hate.
[01:11:57.260 --> 01:12:05.260]   I mean, in theory, Twitter should be able to identify who the most upstanding members of their community are, at least in some respects in terms of...
[01:12:05.260 --> 01:12:08.260]   Would it be people following that person and the other person who...
[01:12:08.260 --> 01:12:13.260]   They know which people get followed a lot. They know which people get a lot of likes.
[01:12:13.260 --> 01:12:14.260]   Yes.
[01:12:14.260 --> 01:12:18.260]   What if it became a capture? Like, whenever you logged into Twitter, it said, "Okay."
[01:12:18.260 --> 01:12:21.260]   And just approved you're not a robot. Is this offensive?
[01:12:21.260 --> 01:12:22.260]   That's...
[01:12:22.260 --> 01:12:26.260]   Well, it's like... Well, didn't they say people were using porn to get...
[01:12:26.260 --> 01:12:27.260]   Yeah.
[01:12:27.260 --> 01:12:31.260]   To get captures done so you could use porn to get comments...
[01:12:31.260 --> 01:12:35.260]   Before you see Mary Jo and her friends...
[01:12:35.260 --> 01:12:38.260]   Is this offensive? That's the wrong people to ask.
[01:12:38.260 --> 01:12:39.260]   Right.
[01:12:39.260 --> 01:12:40.260]   Although, with Twitter...
[01:12:40.260 --> 01:12:41.260]   The barriers low.
[01:12:41.260 --> 01:12:46.260]   Twitter are more than almost anything. Context is everything. There's almost no tweet ever stands alone.
[01:12:46.260 --> 01:12:48.260]   You have to read the tweets before it and the ones that follow.
[01:12:48.260 --> 01:12:49.260]   Right.
[01:12:49.260 --> 01:12:53.260]   Sometimes you need to know whether the people tweeting each other know each other or not.
[01:12:53.260 --> 01:13:02.260]   I wonder how many people are on the team at Twitter. It's got to be fairly big that the abuse team that reviews these.
[01:13:02.260 --> 01:13:03.260]   Oh.
[01:13:03.260 --> 01:13:06.260]   This actually is a timeline...
[01:13:06.260 --> 01:13:10.260]   Who was it that suggested you send pizza to the Twitter?
[01:13:10.260 --> 01:13:15.260]   I read a Medium Post by a guy who said, "I probably shouldn't talk about this.
[01:13:15.260 --> 01:13:20.260]   This is a... I think it was today or yesterday, but I'm being trolled on Twitter."
[01:13:20.260 --> 01:13:26.260]   And these people have ordered pizza sent to my house.
[01:13:26.260 --> 01:13:30.260]   And let me see if I can find the articles. A very short article.
[01:13:30.260 --> 01:13:31.260]   Oh.
[01:13:31.260 --> 01:13:36.260]   And he says how it was...
[01:13:36.260 --> 01:13:48.260]   How he suggested at the end of it, he said, "I think everybody reading this should send pizzas to the Twitter team."
[01:13:48.260 --> 01:13:51.260]   Just to let them know what it's like to be trolled in this way.
[01:13:51.260 --> 01:13:52.260]   Yeah.
[01:13:52.260 --> 01:14:00.260]   His complaint was that they were slow to respond. I've not found that to be the case, but I can't find the article.
[01:14:00.260 --> 01:14:08.260]   Are we going to... When it comes to this issue, there seems to be reaching a crescendo pitch right now about abuse
[01:14:08.260 --> 01:14:12.260]   and the fact that all of these services are not handling it well.
[01:14:12.260 --> 01:14:20.260]   What's your feeling that we will either collectively get a thicker skin or technology will solve the problem?
[01:14:20.260 --> 01:14:24.260]   Oh, gosh. I don't think we're going to get a skin that thick. I don't think we'd ever get a skin that thick.
[01:14:24.260 --> 01:14:25.260]   Really?
[01:14:25.260 --> 01:14:28.260]   Some of this stuff is really terrible.
[01:14:28.260 --> 01:14:32.260]   Why do you think Snapchat is suddenly more popular than Twitter?
[01:14:32.260 --> 01:14:37.260]   In fact, this was the week that it surpassed Twitter in daily users.
[01:14:37.260 --> 01:14:42.260]   150 million people use Snapchat every day. That's more than Twitter, which is 140 million.
[01:14:42.260 --> 01:14:46.260]   I think it's because you don't get abuse on Snapchat, right?
[01:14:46.260 --> 01:14:48.260]   It's just your buddies and your friends sending you stuff.
[01:14:48.260 --> 01:14:52.260]   It's also the fact that nothing survives, nothing lives, it's temporary.
[01:14:52.260 --> 01:14:56.260]   My kids... Henry loves Snapchat. That's what he uses all the time.
[01:14:56.260 --> 01:14:58.260]   Your kids are too young.
[01:14:58.260 --> 01:15:03.260]   Well, it's meant to be ephemeral and therefore there's not as much meaning in what you do.
[01:15:03.260 --> 01:15:08.260]   Right. And your boss, your future boss, is going to check it out when you apply for a job in two years.
[01:15:08.260 --> 01:15:14.260]   I think that... I don't know. If I were Twitter, I'd be a little nervous about the whole thing.
[01:15:14.260 --> 01:15:20.260]   Well, it does seem like the fractured social ecosystem is cracking even more.
[01:15:20.260 --> 01:15:23.260]   It just feels like tribes.
[01:15:23.260 --> 01:15:29.260]   I think we went through an era where people thought, "Hey, it's the web. It should be open. It shouldn't be censored."
[01:15:29.260 --> 01:15:35.260]   I also think about the fact that lately a lot of websites have done away with comments.
[01:15:35.260 --> 01:15:38.260]   Because people in comments are so horrible.
[01:15:38.260 --> 01:15:42.260]   And a lot of them are trolls, they're just basically horrible people.
[01:15:42.260 --> 01:15:45.260]   In the old days, you would have said, "You need to let these people speak."
[01:15:45.260 --> 01:15:50.260]   And today, people are more like to say, "No, this stuff is horrible. It's not improving the experience.
[01:15:50.260 --> 01:15:55.260]   If you're not a troll, it's making it unbearable to go to these sites."
[01:15:55.260 --> 01:16:06.260]   And so I think we're sort of entering an era where it's not only acceptable, but preferable, not to let every person run wild on the Internet.
[01:16:06.260 --> 01:16:12.260]   I feel like we... Certainly, initially, I was a wide-eyed optimist about all of this.
[01:16:12.260 --> 01:16:16.260]   And I said, "Oh, democracy. Everybody has a voice."
[01:16:16.260 --> 01:16:26.260]   And when everybody's talking and you're shining light on everything, even if you turn over a rock and there's something horrible, the light of day will cleanse it.
[01:16:26.260 --> 01:16:31.260]   And the long run, the good stuff will out. I really believed that. Boy, was I wrong.
[01:16:31.260 --> 01:16:34.260]   It has not been that way at all, has it?
[01:16:34.260 --> 01:16:43.260]   And a tiny, tiny number of trolls can make life unbearable for the vast majority of people on the Internet who are thoroughly decent people.
[01:16:43.260 --> 01:16:49.260]   And I changed my Twitter nickname to be my name with three parentheses around it.
[01:16:49.260 --> 01:16:55.260]   Because that, as you know, that's the new dog whistle for anti-Semitism. Did you know about this?
[01:16:55.260 --> 01:16:57.260]   No, yes.
[01:16:57.260 --> 01:17:02.260]   I guess it originated at a well-known anti-Semite site.
[01:17:02.260 --> 01:17:07.260]   They wanted to highlight that this person is Jewish.
[01:17:07.260 --> 01:17:17.260]   And the way they did it was a dog whistle is something that normal people won't hear, but the people who you're aiming at will hear like a dog will hear a high-pitched whistle.
[01:17:17.260 --> 01:17:25.260]   By putting this "hugs," what used to be "hugs" around this name, it was a dog whistle to other anti-Semites that this is a Jewish person.
[01:17:25.260 --> 01:17:31.260]   And so I wanted, in solidarity, I thought, "Well, we should all just do this." And that would just go away.
[01:17:31.260 --> 01:17:39.260]   But isn't that reprehensible? So they published on Twitter and other places, names are Jewish people with these parentheses about it.
[01:17:39.260 --> 01:17:43.260]   They had a Chrome extension to help them manage their...
[01:17:43.260 --> 01:17:45.260]   They wrote a Chrome extension.
[01:17:45.260 --> 01:17:51.260]   Which Google just got rid of recently until recently was available.
[01:17:51.260 --> 01:17:59.260]   I'd say I live in more of the everyday where you don't see it as much.
[01:17:59.260 --> 01:18:08.260]   I just feel like so many people go through life kind of with the Amazon reviews philosophy, which is don't read the five-star reviews, don't read the one-star reviews.
[01:18:08.260 --> 01:18:14.260]   Just look for the middle and try and ignore the top and the bottom.
[01:18:14.260 --> 01:18:21.260]   But like you said, there are certain things you can't unsee. And when things strike a chord and they're critical, I think it...
[01:18:21.260 --> 01:18:28.260]   It damages the internet because you don't want to put yourself out there like that in that way.
[01:18:28.260 --> 01:18:32.260]   And all of these social platforms. But, oh, that's horrible.
[01:18:32.260 --> 01:18:43.260]   It is a little bit of a disappointment to me that what I thought would be this amazing democratizing force has just turned out to be a way of being a person.
[01:18:43.260 --> 01:18:46.260]   It turns out to be a way to spread hate and vile.
[01:18:46.260 --> 01:18:48.260]   And I think it's a small number.
[01:18:48.260 --> 01:18:56.260]   And what it turns out is that this small number has a megaphone just like the rest of us.
[01:18:56.260 --> 01:18:58.260]   And so it appears...
[01:18:58.260 --> 01:18:59.260]   They can find each other.
[01:18:59.260 --> 01:19:02.260]   Yeah, and they can find each other. And in fact, maybe even they're better at using the megaphone.
[01:19:02.260 --> 01:19:07.260]   And I have to point at the presidential election, which is coming up in this country.
[01:19:07.260 --> 01:19:21.260]   And the candidate that has benefited the most from Twitter is Donald Trump. Because it is an unmediated, unfiltered way for him to express himself and to find an audience for what he believes.
[01:19:21.260 --> 01:19:26.260]   It also rewards those who authentically speak.
[01:19:26.260 --> 01:19:30.260]   Well, isn't that interesting? Because of all the candidates, the only one who's using his own...
[01:19:30.260 --> 01:19:33.260]   If he's got somebody posting this for him, they're doing a really good job.
[01:19:33.260 --> 01:19:35.260]   He dictates a lot of them.
[01:19:35.260 --> 01:19:38.260]   "Hey, write a Twitter for me." And this is huge.
[01:19:38.260 --> 01:19:46.260]   You can tell he's dictating it because it's so off the cuff that if you actually wrote it down, you would say, "Well, I'm not going to post that."
[01:19:46.260 --> 01:19:50.260]   But because it's just like something... It's like him... It's like Tourette's.
[01:19:50.260 --> 01:19:54.260]   "Hey, Joe, write this." And it gets written.
[01:19:54.260 --> 01:19:57.260]   It's like... I mean, really.
[01:19:57.260 --> 01:20:07.260]   So the article, if you want to read about these things, comes from a Mike MIC magazine. It's about the three parentheses.
[01:20:07.260 --> 01:20:11.260]   I guess I thought it was fairly widely known.
[01:20:11.260 --> 01:20:17.260]   The secret symbol in the Anatsis used to target Jews online. People are probably wondering why I wrote my name with hugs around it now.
[01:20:17.260 --> 01:20:18.260]   That's why.
[01:20:18.260 --> 01:20:21.260]   I wanted a hug. I've known a brand.
[01:20:21.260 --> 01:20:37.260]   It came from an editor at the New York Times who wrote in a story about a week ago about how he has been harassed using this dog whistle.
[01:20:37.260 --> 01:20:40.260]   And he didn't know. Why are you putting my name in parenthesis?
[01:20:40.260 --> 01:20:41.260]   Wow.
[01:20:41.260 --> 01:20:47.260]   And somebody tweeted at him, "It's a dog whistle fool, bailing the cat for my fellow goyam."
[01:20:47.260 --> 01:20:48.260]   Eugh.
[01:20:48.260 --> 01:20:49.260]   Eugh.
[01:20:49.260 --> 01:20:55.260]   So it turns out we turn over the rock and all the snakes slither out in the public go, "Hey!"
[01:20:55.260 --> 01:20:57.260]   We've got a platform.
[01:20:57.260 --> 01:20:58.260]   We have a platform.
[01:20:58.260 --> 01:21:04.260]   I just think that we'll end up finding more tools to be critical users of the internet.
[01:21:04.260 --> 01:21:08.260]   Is it foolish of us to think that AI can help?
[01:21:08.260 --> 01:21:10.260]   AIRs are already helping. It can help, Mar.
[01:21:10.260 --> 01:21:11.260]   Can it?
[01:21:11.260 --> 01:21:13.260]   Good. I hope you're right.
[01:21:13.260 --> 01:21:19.260]   Slideside Bar, how many reviews on Amazon? What percentage do you think are fake?
[01:21:19.260 --> 01:21:22.260]   That's a really good question.
[01:21:22.260 --> 01:21:25.260]   And the secondary question is, do you think you can tell?
[01:21:25.260 --> 01:21:30.260]   Like, do you know what... Sometimes you know it's a fake review because it's a glowing review of a product nobody ever heard of.
[01:21:30.260 --> 01:21:32.260]   Old days, I think you could tell. Not anymore.
[01:21:32.260 --> 01:21:36.260]   The sophistication has gone up. Pick a number. What number do you think?
[01:21:36.260 --> 01:21:37.260]   Like...
[01:21:37.260 --> 01:21:38.260]   20%.
[01:21:38.260 --> 01:21:39.260]   I was gonna say 22.
[01:21:39.260 --> 01:21:40.260]   One in five.
[01:21:40.260 --> 01:21:44.260]   There are some numbers that say 13 to 15% of all the reviews online are fake.
[01:21:44.260 --> 01:21:46.260]   Those are the numbers that have been published.
[01:21:46.260 --> 01:21:47.260]   That's not so bad.
[01:21:47.260 --> 01:21:49.260]   So like your TripAdvisor...
[01:21:49.260 --> 01:21:50.260]   Yelp.
[01:21:50.260 --> 01:21:51.260]   Yelp.
[01:21:51.260 --> 01:21:52.260]   I mean, these are...
[01:21:52.260 --> 01:21:53.260]   But see, it's more...
[01:21:53.260 --> 01:21:54.260]   For instance...
[01:21:54.260 --> 01:21:56.260]   Yelp, it's higher, I'm sure.
[01:21:56.260 --> 01:22:03.260]   Because every business owner writes a Yelp review that's positive of his business and then goes and writes the bad reviews of all the competitors.
[01:22:03.260 --> 01:22:08.260]   I'm sure that happens a lot more on Yelp. I'd say it's more like 50% on Yelp.
[01:22:08.260 --> 01:22:12.260]   And Amazon, it feels like it would be a little bit better.
[01:22:12.260 --> 01:22:16.260]   Especially because you can kind of tell if somebody's used the product and if it's a real review.
[01:22:16.260 --> 01:22:19.260]   Well remember, you have to purchase the product to write the review.
[01:22:19.260 --> 01:22:22.260]   Right. If you're a verified purchaser, that's gonna be...
[01:22:22.260 --> 01:22:23.260]   Of that product.
[01:22:23.260 --> 01:22:24.260]   Right.
[01:22:24.260 --> 01:22:25.260]   Exactly.
[01:22:25.260 --> 01:22:26.260]   A more valid review.
[01:22:26.260 --> 01:22:28.260]   They told me this week, it's less than 1%.
[01:22:28.260 --> 01:22:29.260]   That low.
[01:22:29.260 --> 01:22:30.260]   Wow.
[01:22:30.260 --> 01:22:32.260]   Do you think they're lying?
[01:22:32.260 --> 01:22:35.260]   They are now suing people who...
[01:22:35.260 --> 01:22:37.260]   Sellers who buy fake reviews.
[01:22:37.260 --> 01:22:43.260]   Right. So back in April, they sued about 1,000 people who had written fake reviews.
[01:22:43.260 --> 01:22:45.260]   Now that was just going for the...
[01:22:45.260 --> 01:22:47.260]   But their businesses that do this, right?
[01:22:47.260 --> 01:22:49.260]   Right. So they were going for the small fish and I thought that's weird.
[01:22:49.260 --> 01:22:54.260]   Why would they be playing a game of whack-a-mole with these guys who were getting five, ten bucks to write a fake review?
[01:22:54.260 --> 01:23:02.260]   Well it turns out they were using the information from those lawsuits to find out who was buying the fake reviews.
[01:23:02.260 --> 01:23:04.260]   That's so now they're going after the sellers.
[01:23:04.260 --> 01:23:06.260]   They're going after the alleged...
[01:23:06.260 --> 01:23:07.260]   Right.
[01:23:07.260 --> 01:23:11.260]   ... purchasers of the fake reviews who are profiting from the fraud, allegedly.
[01:23:11.260 --> 01:23:12.260]   Right.
[01:23:12.260 --> 01:23:15.260]   And they've shut those sellers down.
[01:23:15.260 --> 01:23:18.260]   They have an arbitration agreement so they don't actually take them to court.
[01:23:18.260 --> 01:23:26.260]   They take them to arbitration but they have in their Yule they can shut them down and they're clearly sending a message to other sellers.
[01:23:26.260 --> 01:23:29.260]   And they're suing for damages in some cases.
[01:23:29.260 --> 01:23:30.260]   Yeah. I think so.
[01:23:30.260 --> 01:23:33.260]   1% seems low. I think it's higher than that, don't you?
[01:23:33.260 --> 01:23:37.260]   I was shocked. They did say that they have a really...
[01:23:37.260 --> 01:23:40.260]   And how do they know I guess would be the question.
[01:23:40.260 --> 01:23:42.260]   How would they know?
[01:23:42.260 --> 01:23:46.260]   They're using algorithms to figure out what are repeated phrases.
[01:23:46.260 --> 01:23:50.260]   Oh, you know syntactic analysis would probably work, wouldn't it?
[01:23:50.260 --> 01:23:55.260]   Because if you're writing fake reviews you're going to probably use those same phrases in multiple reviews.
[01:23:55.260 --> 01:23:58.260]   Right, especially if you're making that interesting.
[01:23:58.260 --> 01:23:59.260]   Interesting.
[01:23:59.260 --> 01:24:07.260]   And they think that a lot of this stuff is done out of the country so they're looking for not non-English speaking reviews and they're looking harder at those.
[01:24:07.260 --> 01:24:11.260]   Somebody in the chat room to take away from this is I can make $10.00 or I can take reviews.
[01:24:11.260 --> 01:24:13.260]   Where would I go to do that?
[01:24:13.260 --> 01:24:15.260]   Let me tweet that link to you guys.
[01:24:15.260 --> 01:24:17.260]   I'll hook you up.
[01:24:17.260 --> 01:24:20.260]   Work from home. I said that post on a telephone card.
[01:24:20.260 --> 01:24:25.260]   But honestly, while I agree Amazon and Yelp and everybody else should do the best they can to get rid of these.
[01:24:25.260 --> 01:24:26.260]   Can't we kind of tell?
[01:24:26.260 --> 01:24:27.260]   I mean I...
[01:24:27.260 --> 01:24:28.260]   I can.
[01:24:28.260 --> 01:24:31.260]   Yeah, when I buy something I always read the Amazon reviews before I buy it.
[01:24:31.260 --> 01:24:34.260]   And it's the negative information in reviews that's most awful.
[01:24:34.260 --> 01:24:35.260]   That's the key.
[01:24:35.260 --> 01:24:36.260]   It's not the plus thumbs up or thumbs up.
[01:24:36.260 --> 01:24:38.260]   Even if it's a great product.
[01:24:38.260 --> 01:24:41.260]   What you want to know is any potential pitfalls.
[01:24:41.260 --> 01:24:44.260]   No matter how many favorable reviews somebody stuffs in there.
[01:24:44.260 --> 01:24:47.260]   You can't prevent other people from exposing the pitfalls.
[01:24:47.260 --> 01:24:50.260]   Even for the most obscure product on Amazon.
[01:24:50.260 --> 01:24:53.260]   Like some iPhone case from a company you've never heard of.
[01:24:53.260 --> 01:24:57.260]   There do seem to be actual reviews from actual people who tell you what it's like.
[01:24:57.260 --> 01:25:05.260]   That's why I think the best product reviewers are curmudgeonly grumpy, detail oriented.
[01:25:05.260 --> 01:25:07.260]   You know, I am the worst.
[01:25:07.260 --> 01:25:08.260]   Devorak.
[01:25:08.260 --> 01:25:09.260]   I wasn't.
[01:25:09.260 --> 01:25:12.260]   But wait a minute, I'm going to tell you.
[01:25:12.260 --> 01:25:13.260]   That may not be true.
[01:25:13.260 --> 01:25:17.260]   Devorak once told me he could review a product just by looking at the box.
[01:25:17.260 --> 01:25:20.260]   Although I think in some cases, this was talking about software.
[01:25:20.260 --> 01:25:21.260]   But I think in some cases he was right.
[01:25:21.260 --> 01:25:24.260]   You can look at the box and go, "Oh, it's not going to be good.
[01:25:24.260 --> 01:25:25.260]   It's not going to be good."
[01:25:25.260 --> 01:25:28.260]   But I think you normally would want to install it just to try.
[01:25:28.260 --> 01:25:30.260]   But you're right.
[01:25:30.260 --> 01:25:31.260]   Cranky is good.
[01:25:31.260 --> 01:25:32.260]   Picky is good.
[01:25:32.260 --> 01:25:33.260]   Because they give you detail.
[01:25:33.260 --> 01:25:35.260]   And you know enough to go, "Well, he's being picky.
[01:25:35.260 --> 01:25:37.260]   I'm going to ignore that one."
[01:25:37.260 --> 01:25:41.260]   Amazon also pushes up the best negative reviews to the top, so it's easy to find them.
[01:25:41.260 --> 01:25:43.260]   They did say the ranking is really important.
[01:25:43.260 --> 01:25:49.260]   And one thing I hadn't thought about before is that they do a date based ranking because
[01:25:49.260 --> 01:25:54.260]   products change even though it's the same skew and the same,
[01:25:54.260 --> 01:25:59.260]   the product might have been listed on Amazon for the last six years,
[01:25:59.260 --> 01:26:01.260]   but the product has changed incrementally.
[01:26:01.260 --> 01:26:07.260]   So a review that might be really popular from way back when it may have been completely resolved by now.
[01:26:07.260 --> 01:26:08.260]   They do have to fix that.
[01:26:08.260 --> 01:26:12.260]   I sometimes will go to a site and it'll be a review of a related but different product on Amazon, right?
[01:26:12.260 --> 01:26:13.260]   That happens to me a lot.
[01:26:13.260 --> 01:26:18.260]   And some of the best Amazon reviews are from somebody who bought a camera and used it for nine months,
[01:26:18.260 --> 01:26:21.260]   which professional reviewers will almost never review.
[01:26:21.260 --> 01:26:22.260]   That's what you want.
[01:26:22.260 --> 01:26:23.260]   That's what I like.
[01:26:23.260 --> 01:26:25.260]   I got the review out of the way, the camera ships.
[01:26:25.260 --> 01:26:26.260]   I love reviews.
[01:26:26.260 --> 01:26:30.260]   There's all kinds of stuff they'll never discover that you will discover if you use it for a long time.
[01:26:30.260 --> 01:26:32.260]   All right, let's take a break.
[01:26:32.260 --> 01:26:33.260]   Come back with more.
[01:26:33.260 --> 01:26:36.260]   Harry McCracken is here from Fast Company.com from GMA.
[01:26:36.260 --> 01:26:37.260]   Becky Worling.
[01:26:37.260 --> 01:26:38.260]   Glad you're here today.
[01:26:38.260 --> 01:26:40.260]   We had a good week on Twitch.
[01:26:40.260 --> 01:26:41.260]   Hey, Becky.
[01:26:41.260 --> 01:26:43.260]   I caught you right in the middle of a drink.
[01:26:43.260 --> 01:26:44.260]   Water this time.
[01:26:44.260 --> 01:26:47.260]   By the way, weren't we going to have booze today?
[01:26:47.260 --> 01:26:48.260]   I'm a...
[01:26:48.260 --> 01:26:49.260]   I'm think...
[01:26:49.260 --> 01:26:50.260]   Are you okay?
[01:26:50.260 --> 01:26:51.260]   You didn't bring wine?
[01:26:51.260 --> 01:26:52.260]   I can get you some.
[01:26:52.260 --> 01:26:56.260]   You know, I was so excited about all this stuff.
[01:26:56.260 --> 01:26:59.260]   I'm getting the hot tub and talk about Sims.
[01:26:59.260 --> 01:27:01.260]   That doesn't work.
[01:27:01.260 --> 01:27:04.260]   Ah, Anthony Wiener.
[01:27:04.260 --> 01:27:06.260]   Wow.
[01:27:06.260 --> 01:27:11.260]   Anthony Nielsen, who for some reason I continually call Anthony Wiener.
[01:27:11.260 --> 01:27:13.260]   That's something you wish you could answer.
[01:27:13.260 --> 01:27:14.260]   Is that Freudian?
[01:27:14.260 --> 01:27:17.260]   That is like a combination of the author story and the Facebook image story.
[01:27:17.260 --> 01:27:20.260]   Some stories need to be challenged to power.
[01:27:20.260 --> 01:27:22.260]   But you don't need to see that image.
[01:27:22.260 --> 01:27:24.260]   Great documentary, by the way.
[01:27:24.260 --> 01:27:25.260]   We just saw...
[01:27:25.260 --> 01:27:26.260]   Anthony Wiener?
[01:27:26.260 --> 01:27:27.260]   The Wiener documentary.
[01:27:27.260 --> 01:27:28.260]   It is really...
[01:27:28.260 --> 01:27:30.260]   So what's the bottom line on that?
[01:27:30.260 --> 01:27:31.260]   Well, he...
[01:27:31.260 --> 01:27:38.260]   He and his wife allowed a camera crew to follow them when he was running for mayor.
[01:27:38.260 --> 01:27:41.260]   And they were incredibly open around the camera crew.
[01:27:41.260 --> 01:27:46.260]   And even after his campaign melted down, they continued to allow the camera crew to follow them.
[01:27:46.260 --> 01:27:49.260]   And it's the most real, like, little...
[01:27:49.260 --> 01:27:50.260]   Do you remember her saying that?
[01:27:50.260 --> 01:27:51.260]   I will absolutely watch that.
[01:27:51.260 --> 01:27:53.260]   Does she still work for Hillary Clinton?
[01:27:53.260 --> 01:27:54.260]   Yes.
[01:27:54.260 --> 01:27:55.260]   Interesting.
[01:27:55.260 --> 01:27:56.260]   She does.
[01:27:56.260 --> 01:27:57.260]   Oh, I got to see that.
[01:27:57.260 --> 01:27:58.260]   What's it called?
[01:27:58.260 --> 01:27:59.260]   The Wiener.
[01:27:59.260 --> 01:28:01.260]   What else have we called?
[01:28:01.260 --> 01:28:05.260]   If you thought you were going to get a hot dog documentary, no.
[01:28:05.260 --> 01:28:06.260]   Wiener house?
[01:28:06.260 --> 01:28:08.260]   Isn't that a chain?
[01:28:08.260 --> 01:28:09.260]   We are Wiener's.
[01:28:09.260 --> 01:28:13.260]   Bum, bum, bum, bum, bum.
[01:28:13.260 --> 01:28:17.260]   This episode of This Week in Tech brought to you by Stamps.com.
[01:28:17.260 --> 01:28:19.260]   I know a lot of you sell online.
[01:28:19.260 --> 01:28:21.260]   This is very popular.
[01:28:21.260 --> 01:28:23.260]   But what is your package when you send it?
[01:28:23.260 --> 01:28:24.260]   What does it look like?
[01:28:24.260 --> 01:28:27.260]   You're Etsy or your eBay, wherever you sell.
[01:28:27.260 --> 01:28:28.260]   Does it...
[01:28:28.260 --> 01:28:33.260]   Does it wrap in brown paper with twine and then a bunch of licked stamps on there and a handwritten address?
[01:28:33.260 --> 01:28:34.260]   That does not cast...
[01:28:34.260 --> 01:28:37.260]   I mean, first of all, your buyer may be reluctant to open it.
[01:28:37.260 --> 01:28:39.260]   I know I am.
[01:28:39.260 --> 01:28:41.260]   Like, what is this?
[01:28:41.260 --> 01:28:44.260]   Stamps.com makes it look professional.
[01:28:44.260 --> 01:28:47.260]   Stamps.com.
[01:28:47.260 --> 01:28:50.260]   You can buy and print real U.S. postage right from your computer and your printer.
[01:28:50.260 --> 01:28:52.260]   You do not need a postage meter.
[01:28:52.260 --> 01:28:54.260]   You just need a printer and a computer and it looks great.
[01:28:54.260 --> 01:28:56.260]   You can print a label for any kind of mailing.
[01:28:56.260 --> 01:28:59.260]   You print right on the envelope if you're sending out brochures or bills or whatever.
[01:28:59.260 --> 01:29:02.260]   With your company logo, it automatically puts your return address in.
[01:29:02.260 --> 01:29:05.260]   It'll take the address of the recipient from your address book.
[01:29:05.260 --> 01:29:07.260]   It reads almost all kinds of address books.
[01:29:07.260 --> 01:29:11.260]   If you sell on eBay or Amazon or Etsy or a bunch of other websites,
[01:29:11.260 --> 01:29:14.260]   you actually get the information from the website so it saves you typing.
[01:29:14.260 --> 01:29:19.260]   It'll automatically validate the address using the Postal Service database
[01:29:19.260 --> 01:29:22.260]   and make sure you've got exactly the right postage.
[01:29:22.260 --> 01:29:23.260]   It does that with its USB scale.
[01:29:23.260 --> 01:29:25.260]   I'll tell you what, I'm going to get you the scale.
[01:29:25.260 --> 01:29:28.260]   I'm going to get you Stamps.com for a month for free.
[01:29:28.260 --> 01:29:31.260]   I'm going to get you $55 in postage coupons.
[01:29:31.260 --> 01:29:34.260]   I got a really good way to try Stamps.com.
[01:29:34.260 --> 01:29:37.260]   Visit Stamps.com in the upper right-hand corner.
[01:29:37.260 --> 01:29:39.260]   There's a microphone.
[01:29:39.260 --> 01:29:40.260]   Do you see it?
[01:29:40.260 --> 01:29:41.260]   Go to the top.
[01:29:41.260 --> 01:29:42.260]   There it is.
[01:29:42.260 --> 01:29:44.260]   It says, "Hurt us on a podcast.
[01:29:44.260 --> 01:29:47.260]   Click the microphone and use a Twit" when it asks for the offer code.
[01:29:47.260 --> 01:29:48.260]   Just type in TWIT.
[01:29:48.260 --> 01:29:52.260]   That's how you get the really good trial offer.
[01:29:52.260 --> 01:29:54.260]   The postage scale is awesome.
[01:29:54.260 --> 01:29:56.260]   It'll tell you how much the item weighs.
[01:29:56.260 --> 01:29:59.260]   It'll give you mailing options for all classes of mail, even certified mail.
[01:29:59.260 --> 01:30:04.260]   It will fill out the forms for certified mail and for express mail
[01:30:04.260 --> 01:30:06.260]   so you don't have to customs forms if you have them.
[01:30:06.260 --> 01:30:07.260]   It fills them out.
[01:30:07.260 --> 01:30:09.260]   Does it all work for you?
[01:30:09.260 --> 01:30:11.260]   You never have to go to the post office.
[01:30:11.260 --> 01:30:13.260]   The mail carrier comes, picks it up.
[01:30:13.260 --> 01:30:15.260]   It is awesome.
[01:30:15.260 --> 01:30:16.260]   Stamps.com.
[01:30:16.260 --> 01:30:19.260]   Make everything you sell online.
[01:30:19.260 --> 01:30:22.260]   Make it all look super pro, super cool.
[01:30:22.260 --> 01:30:25.260]   If you've got a business that does mailing, we even use it.
[01:30:25.260 --> 01:30:28.260]   We don't do a lot of mailing, but we do enough that it's really worthwhile.
[01:30:28.260 --> 01:30:30.260]   Stamps.com.
[01:30:30.260 --> 01:30:34.260]   Get our special $110 bonus offer by going to Stamps.com.
[01:30:34.260 --> 01:30:38.260]   Clicking the microphone in the upper right-hand corner and using the promo code TWIT.
[01:30:38.260 --> 01:30:40.260]   Good week on TWIT.
[01:30:40.260 --> 01:30:42.260]   We've got a little mini movie.
[01:30:42.260 --> 01:30:46.260]   I think Victor put this together for us to show you some of the highlights.
[01:30:46.260 --> 01:30:48.260]   Previously on TWIT.
[01:30:48.260 --> 01:30:49.260]   I love your setup.
[01:30:49.260 --> 01:30:50.260]   Wow.
[01:30:50.260 --> 01:30:51.260]   Is this your man cave?
[01:30:51.260 --> 01:30:52.260]   This is a Mac cave.
[01:30:52.260 --> 01:30:53.260]   Yeah.
[01:30:53.260 --> 01:30:54.260]   Totally Mac.
[01:30:54.260 --> 01:30:56.260]   Have you heard the good word about Linux?
[01:30:56.260 --> 01:30:57.260]   Don't stop.
[01:30:57.260 --> 01:30:59.260]   (laughter)
[01:30:59.260 --> 01:31:00.260]   TWIT Live Specials.
[01:31:00.260 --> 01:31:02.260]   We're here at AWE.
[01:31:02.260 --> 01:31:03.260]   We're in the mental world.
[01:31:03.260 --> 01:31:04.260]   That's cool.
[01:31:04.260 --> 01:31:10.260]   We're here at AWE.
[01:31:10.260 --> 01:31:12.260]   We're here at AWE.
[01:31:12.260 --> 01:31:14.260]   We're here at AWE.
[01:31:14.260 --> 01:31:15.260]   We're here at AWE.
[01:31:15.260 --> 01:31:17.260]   We're here at AWE.
[01:31:17.260 --> 01:31:19.260]   We're here at AWE.
[01:31:19.260 --> 01:31:21.260]   We're here at AWE.
[01:31:21.260 --> 01:31:23.260]   We're here at AWE.
[01:31:23.260 --> 01:31:25.260]   We're here at AWE.
[01:31:25.260 --> 01:31:27.260]   We're here at AWE.
[01:31:27.260 --> 01:31:29.260]   We're here at AWE.
[01:31:29.260 --> 01:31:31.260]   We're here at AWE.
[01:31:31.260 --> 01:31:33.260]   We don't have to say you're here at AWE.
[01:31:33.260 --> 01:31:37.260]   We need to find something in the real world that you want to fix or you want to improve
[01:31:37.260 --> 01:31:40.260]   and then somehow making it better.
[01:31:40.260 --> 01:31:41.260]   All about Android.
[01:31:41.260 --> 01:31:48.260]   Bluebird got last week had a report about how Google is looking to revealing an Android update report card.
[01:31:48.260 --> 01:31:55.260]   They want to make a list and hand out gold star stickers like it's kindergarten and you're going to get the kids to try harder.
[01:31:55.260 --> 01:31:57.260]   You did such a great job on that article, Ron.
[01:31:57.260 --> 01:31:58.260]   TWIT.
[01:31:58.260 --> 01:32:00.260]   Some assembly required.
[01:32:00.260 --> 01:32:02.260]   Gold star for Ron.
[01:32:02.260 --> 01:32:04.260]   We got a week ahead.
[01:32:04.260 --> 01:32:05.260]   Who is it?
[01:32:05.260 --> 01:32:06.260]   Jason or Megan?
[01:32:06.260 --> 01:32:10.260]   Megan Moroni from Tech News Today taking a look at what's ahead.
[01:32:10.260 --> 01:32:11.260]   Thanks Leo.
[01:32:11.260 --> 01:32:15.260]   Coming up this week, it's the deadline for the second round of bids for Yahoo.
[01:32:15.260 --> 01:32:17.260]   Anyone want to make it off?
[01:32:17.260 --> 01:32:18.260]   Anyone?
[01:32:18.260 --> 01:32:19.260]   Twitter?
[01:32:19.260 --> 01:32:21.260]   Yes, many sources are reporting that Twitter has discussed.
[01:32:21.260 --> 01:32:22.260]   No.
[01:32:22.260 --> 01:32:27.260]   A merger with Yahoo, which sounds to me a little like one drowning person hanging on to another.
[01:32:27.260 --> 01:32:31.260]   The Fortune report says it's unclear whether the talk is for serious.
[01:32:31.260 --> 01:32:35.260]   Verizon still seems to be the front runner in the week after purchase Yahoo.
[01:32:35.260 --> 01:32:39.260]   In other news, Xiaomi will start selling its Mi Band 2 in China this week.
[01:32:39.260 --> 01:32:44.260]   That's a fitness tracker with a heart rate monitor and it has a 20 day battery life.
[01:32:44.260 --> 01:32:47.260]   You can get all of that for only $23.
[01:32:47.260 --> 01:32:50.260]   We chatted about the announcement on last Thursday's Tech News Today.
[01:32:50.260 --> 01:32:53.260]   So download that to watch or listen if you're interested in finding out more.
[01:32:53.260 --> 01:32:59.260]   Also this week at Lenovo World, we expect to see Google's first project Tango Phone.
[01:32:59.260 --> 01:33:03.260]   And we think that we will see new Moto and new Droid phones.
[01:33:03.260 --> 01:33:12.260]   You will hear all about this news and a whole lot more on Tech News Today hosted by Jason Howell and myself every week day at 4PM Pacific, 7PM Eastern.
[01:33:12.260 --> 01:33:13.260]   Back to you Leo.
[01:33:13.260 --> 01:33:14.260]   Thank you Megan Moroni.
[01:33:14.260 --> 01:33:15.260]   Make sure you watch TNT.
[01:33:15.260 --> 01:33:18.260]   That's a good show and this one here shows up from time to time.
[01:33:18.260 --> 01:33:24.260]   Oh those two together just they're so great and they are thoughtful and insightful and just such a nice people and it comes across.
[01:33:24.260 --> 01:33:25.260]   I love listening.
[01:33:25.260 --> 01:33:28.260]   Is that credible that Twitter might buy Yahoo? No.
[01:33:28.260 --> 01:33:33.260]   Sounds like there's an older rumor rather than something recent from what I don't think Twitter has the money.
[01:33:33.260 --> 01:33:35.260]   We're talking not eight billion.
[01:33:35.260 --> 01:33:36.260]   That was the original.
[01:33:36.260 --> 01:33:37.260]   But we're still talking three or four billion dollars.
[01:33:37.260 --> 01:33:39.260]   I don't think it doesn't make any sense.
[01:33:39.260 --> 01:33:40.260]   They're not going out.
[01:33:40.260 --> 01:33:41.260]   That's an old phone company.
[01:33:41.260 --> 01:33:43.260]   They're interested in Verizon, right?
[01:33:43.260 --> 01:33:45.260]   I think they seem like the most likely.
[01:33:45.260 --> 01:33:47.260]   They want those emails.
[01:33:47.260 --> 01:33:48.260]   Isn't that sad?
[01:33:48.260 --> 01:33:50.260]   That's all it is.
[01:33:50.260 --> 01:33:55.260]   It's just the information about users after all this time.
[01:33:55.260 --> 01:33:58.260]   Like seeing your brother-in-law go and drink in again.
[01:33:58.260 --> 01:34:00.260]   You don't drink there anymore.
[01:34:00.260 --> 01:34:01.260]   I don't drink there anymore.
[01:34:01.260 --> 01:34:02.260]   You don't work there anymore.
[01:34:02.260 --> 01:34:05.260]   My brother-in-law needs to go to rehab.
[01:34:05.260 --> 01:34:06.260]   It's so sad.
[01:34:06.260 --> 01:34:07.260]   I thought he pulled it together.
[01:34:07.260 --> 01:34:08.260]   Oh no.
[01:34:08.260 --> 01:34:09.260]   That kind of gutter.
[01:34:09.260 --> 01:34:11.260]   Now a lot of good people there.
[01:34:11.260 --> 01:34:12.260]   A lot of good people there.
[01:34:12.260 --> 01:34:16.260]   I still believe in Yahoo as a news platform and I think they do have potential there.
[01:34:16.260 --> 01:34:20.260]   They just have to figure out their business model and figure out what they're going to do with themselves.
[01:34:20.260 --> 01:34:22.260]   I don't blame them at all.
[01:34:22.260 --> 01:34:25.260]   I think that was a very difficult challenge.
[01:34:25.260 --> 01:34:27.260]   I think Marissa Meyer knew it.
[01:34:27.260 --> 01:34:29.260]   We said it when she went there.
[01:34:29.260 --> 01:34:33.260]   She must have looked at this and said, "Well, if I can do this, I can do anything."
[01:34:33.260 --> 01:34:34.260]   And she couldn't.
[01:34:34.260 --> 01:34:35.260]   Because it was just a challenge.
[01:34:35.260 --> 01:34:37.260]   I don't know what would you do with Yahoo.
[01:34:37.260 --> 01:34:38.260]   I don't know what you would do.
[01:34:38.260 --> 01:34:39.260]   How many hours you got?
[01:34:39.260 --> 01:34:40.260]   Yeah.
[01:34:40.260 --> 01:34:41.260]   Really?
[01:34:41.260 --> 01:34:42.260]   You got an idea?
[01:34:42.260 --> 01:34:43.260]   No.
[01:34:43.260 --> 01:34:44.260]   No.
[01:34:44.260 --> 01:34:45.260]   What do you do with Twitter?
[01:34:45.260 --> 01:34:46.260]   Same problem.
[01:34:46.260 --> 01:34:47.260]   What do you do?
[01:34:47.260 --> 01:34:48.260]   It's not that the...
[01:34:48.260 --> 01:34:50.260]   Now at least Twitter's making money.
[01:34:50.260 --> 01:34:52.260]   Actually Yahoo's still making money.
[01:34:52.260 --> 01:34:53.260]   Yeah.
[01:34:53.260 --> 01:34:55.260]   Their revenues come down significantly.
[01:34:55.260 --> 01:34:56.260]   You know what?
[01:34:56.260 --> 01:34:59.700]   Both of these companies could just continue doing what they're doing and make money and everybody
[01:34:59.700 --> 01:35:00.700]   be happy.
[01:35:00.700 --> 01:35:01.700]   It's a stock.
[01:35:01.700 --> 01:35:04.260]   The shareholders that are really forcing this.
[01:35:04.260 --> 01:35:06.780]   In a way, I think that's an unfortunate thing.
[01:35:06.780 --> 01:35:12.380]   The activist investors and people who want I want money for my shares.
[01:35:12.380 --> 01:35:16.220]   And I think that's forcing the company to do things that they don't want to do and if
[01:35:16.220 --> 01:35:19.420]   they're not growing quite fast enough to sell.
[01:35:19.420 --> 01:35:20.420]   And that's too...
[01:35:20.420 --> 01:35:22.180]   In a way, I think it's kind of unfortunate.
[01:35:22.180 --> 01:35:24.540]   Is Yahoo really doing that bad?
[01:35:24.540 --> 01:35:25.540]   They've been shrinking.
[01:35:25.540 --> 01:35:30.380]   I mean, their ad revenue has been going down, which is not a good thing in any context.
[01:35:30.380 --> 01:35:33.380]   Yeah, but they're not about to go bankrupt or anything.
[01:35:33.380 --> 01:35:34.380]   No.
[01:35:34.380 --> 01:35:38.700]   Are there in a market that's growing and they've been shrinking?
[01:35:38.700 --> 01:35:40.860]   Are they really in a market that's growing?
[01:35:40.860 --> 01:35:41.860]   I mean, aren't...
[01:35:41.860 --> 01:35:42.860]   That's, I guess, the question.
[01:35:42.860 --> 01:35:43.860]   Are you...
[01:35:43.860 --> 01:35:45.860]   It's growing except Facebook is getting most of the growth.
[01:35:45.860 --> 01:35:46.860]   Right.
[01:35:46.860 --> 01:35:47.860]   That's right.
[01:35:47.860 --> 01:35:48.860]   Right.
[01:35:48.860 --> 01:35:49.860]   Right.
[01:35:49.860 --> 01:35:50.860]   Right.
[01:35:50.860 --> 01:35:51.860]   We don't have to go through all of it, but to your point.
[01:35:51.860 --> 01:35:54.300]   So Mary Meeker's Internet Trends and Slide Debt came out this week.
[01:35:54.300 --> 01:36:00.980]   And one of the things I found interesting was she said that under monetized, over monetized
[01:36:00.980 --> 01:36:02.180]   is television advertising.
[01:36:02.180 --> 01:36:03.180]   Yeah.
[01:36:03.180 --> 01:36:05.740]   In other words, too much value for what it's worth.
[01:36:05.740 --> 01:36:06.740]   Right.
[01:36:06.740 --> 01:36:13.260]   Under monetized is still web advertising, specifically video and mobile.
[01:36:13.260 --> 01:36:16.180]   So the growth is on the side of Internet advertising.
[01:36:16.180 --> 01:36:20.500]   So then just bringing that up to answer, try and answer your question with some data from
[01:36:20.500 --> 01:36:24.500]   someone who knows a lot more than we do, because she uses it.
[01:36:24.500 --> 01:36:27.540]   Well, she's famous for this.
[01:36:27.540 --> 01:36:29.260]   Whether she's always writer, I don't know.
[01:36:29.260 --> 01:36:31.260]   I mean, she's pretty astute, I guess.
[01:36:31.260 --> 01:36:37.780]   I think the interesting, the things that caught my attention is that Internet growth is relatively
[01:36:37.780 --> 01:36:38.780]   flat.
[01:36:38.780 --> 01:36:43.460]   Three billion users, 42% of the world's population.
[01:36:43.460 --> 01:36:52.020]   Smartphone adoption growth is slowing, but Android is doing better than iOS.
[01:36:52.020 --> 01:36:57.380]   Video viewership, way up, exploding, Snapchat, Facebook Live.
[01:36:57.380 --> 01:36:58.620]   It's good for us.
[01:36:58.620 --> 01:37:00.180]   It should be good for me, right?
[01:37:00.180 --> 01:37:01.180]   Or Twitter.
[01:37:01.180 --> 01:37:02.180]   Yeah.
[01:37:02.180 --> 01:37:04.180]   I mean, I think people- Okay.
[01:37:04.180 --> 01:37:11.420]   I'm a little flea riding on the back of a whale, but a rising whale raises all fleas,
[01:37:11.420 --> 01:37:12.420]   right?
[01:37:12.420 --> 01:37:14.340]   I think you're a barnacle because you've held on for a long time.
[01:37:14.340 --> 01:37:15.340]   A barnacle, better than a flea.
[01:37:15.340 --> 01:37:16.340]   I'm a barnacle.
[01:37:16.340 --> 01:37:17.340]   Yeah.
[01:37:17.340 --> 01:37:18.860]   But the whale is going up.
[01:37:18.860 --> 01:37:19.860]   Yeah.
[01:37:19.860 --> 01:37:20.860]   Yeah.
[01:37:20.860 --> 01:37:22.580]   I'm going to get some new krill, some fresh krill.
[01:37:22.580 --> 01:37:24.020]   You got that going on.
[01:37:24.020 --> 01:37:25.020]   Yeah.
[01:37:25.020 --> 01:37:27.580]   And you're under monetized, as said.
[01:37:27.580 --> 01:37:29.580]   I am under monetized.
[01:37:29.580 --> 01:37:31.580]   I want more money.
[01:37:31.580 --> 01:37:39.140]   No, actually, we don't- Look, I'm not trying to become a big internet TV player by any
[01:37:39.140 --> 01:37:41.740]   means, but growth has been nice and steady.
[01:37:41.740 --> 01:37:43.020]   It always has been for 10 years.
[01:37:43.020 --> 01:37:44.660]   It's been very steady.
[01:37:44.660 --> 01:37:49.780]   And what I do see is a lot of advertisers fleeing traditional advertising where, you
[01:37:49.780 --> 01:37:52.380]   know, at first it was because we kick at metrics.
[01:37:52.380 --> 01:37:54.900]   Now it's because it's not working.
[01:37:54.900 --> 01:37:55.900]   Right.
[01:37:55.900 --> 01:38:00.100]   You know, I didn't like that we can't get metrics because that meant, oh, well, we want
[01:38:00.100 --> 01:38:01.300]   metrics from you.
[01:38:01.300 --> 01:38:03.940]   We were going to the internet because now we can measure our audience.
[01:38:03.940 --> 01:38:05.300]   We can find out more about our audience.
[01:38:05.300 --> 01:38:06.780]   We can measure clicks.
[01:38:06.780 --> 01:38:08.580]   I thought that was a bad reason to go.
[01:38:08.580 --> 01:38:13.420]   You should go because it works better because you are engaging your customers.
[01:38:13.420 --> 01:38:15.220]   You're having a conversation with them.
[01:38:15.220 --> 01:38:18.060]   You're dealing with them on a more equal basis.
[01:38:18.060 --> 01:38:19.580]   Your breath must smell.
[01:38:19.580 --> 01:38:21.340]   So buy some mouthwash.
[01:38:21.340 --> 01:38:23.980]   But here's the benefits of our product.
[01:38:23.980 --> 01:38:25.340]   And we have something to offer you.
[01:38:25.340 --> 01:38:27.020]   And if you be interested, why don't you try it?
[01:38:27.020 --> 01:38:28.380]   That to me is much more.
[01:38:28.380 --> 01:38:34.380]   I think that that media network should be really nervous about the Amazon model because
[01:38:34.380 --> 01:38:37.820]   it completely eradicates the need for advertising.
[01:38:37.820 --> 01:38:40.140]   What's the Amazon model?
[01:38:40.140 --> 01:38:47.500]   Basically that by providing media through Prime, they then create Prime member subscribers
[01:38:47.500 --> 01:38:51.380]   who have a higher buy rate than how does a brand do that?
[01:38:51.380 --> 01:38:53.620]   What are you going to have tied TV?
[01:38:53.620 --> 01:39:00.740]   We're going to have you're going to disintermediate with the loss of advertising and create relationships
[01:39:00.740 --> 01:39:04.780]   with businesses themselves that create media.
[01:39:04.780 --> 01:39:07.540]   I think that's also futile.
[01:39:07.540 --> 01:39:09.580]   This is the native content argument.
[01:39:09.580 --> 01:39:11.820]   I know everybody's trying to do that.
[01:39:11.820 --> 01:39:16.060]   First of all, I find it reprehensible because it's really, in my mind, native content is
[01:39:16.060 --> 01:39:19.700]   advertising the tricks you, is trying to trick you into thinking it's not advertising.
[01:39:19.700 --> 01:39:22.740]   But that's different than what this is.
[01:39:22.740 --> 01:39:26.820]   This is a business relationship that Amazon has with its viewers.
[01:39:26.820 --> 01:39:33.060]   It says we will give you something in return for your loyalty.
[01:39:33.060 --> 01:39:39.260]   Remember, this is where Jeff Bezos got all of these ideas from Senegal who's the head
[01:39:39.260 --> 01:39:41.140]   of Costco.
[01:39:41.140 --> 01:39:43.140]   Loyalty membership.
[01:39:43.140 --> 01:39:44.820]   Costco invented that, didn't they?
[01:39:44.820 --> 01:39:49.540]   Loyalty in membership creates long-term revenue with small margin.
[01:39:49.540 --> 01:39:52.940]   It's good for Amazon, but how does a brand take advantage of that?
[01:39:52.940 --> 01:39:55.140]   How does Procter and Gamble take advantage of that?
[01:39:55.140 --> 01:39:56.980]   They have to find an aggregator.
[01:39:56.980 --> 01:39:57.980]   That's bad.
[01:39:57.980 --> 01:39:59.820]   They have to find an aggregator like an Amazon.
[01:39:59.820 --> 01:40:03.100]   So you're not saying there should be Procter and Gamble TV?
[01:40:03.100 --> 01:40:04.100]   Because people did.
[01:40:04.100 --> 01:40:08.060]   People have proposed that, of course, that every brand should have REI, should have the
[01:40:08.060 --> 01:40:13.500]   camping channel and you're watching content that's aimed at REI customers, but features
[01:40:13.500 --> 01:40:16.100]   REI products, that's native advertising.
[01:40:16.100 --> 01:40:17.860]   I'm not as crazy about that.
[01:40:17.860 --> 01:40:20.180]   No, I think it's deceitful.
[01:40:20.180 --> 01:40:21.180]   Yeah.
[01:40:21.180 --> 01:40:22.340]   It is ultimately.
[01:40:22.340 --> 01:40:26.180]   You want to trick people into seeing brand messages without knowing it.
[01:40:26.180 --> 01:40:28.980]   Wow, those judges on American Idol sure like Coke.
[01:40:28.980 --> 01:40:32.100]   Boy, I want to drink some Coke, so I'll be just like them.
[01:40:32.100 --> 01:40:37.220]   Yeah, that's just, it's, we're sheeple, but not that much of a flock of sheeple.
[01:40:37.220 --> 01:40:38.220]   It might work.
[01:40:38.220 --> 01:40:39.940]   Amazon has a scale to do that.
[01:40:39.940 --> 01:40:41.860]   I'm curious whether anybody else can do it.
[01:40:41.860 --> 01:40:43.780]   I think that's what are the other models.
[01:40:43.780 --> 01:40:47.820]   It's not going to be the same, but I think one of the keys is ecosystem-based.
[01:40:47.820 --> 01:40:52.820]   So, what is each ecosystem going to do?
[01:40:52.820 --> 01:40:58.380]   So, for example, Jeff Bezos was talking this week about why the Amazon Prime player is
[01:40:58.380 --> 01:41:05.860]   not on the Apple TV because it doesn't bring people to the store.
[01:41:05.860 --> 01:41:11.020]   There's a conflict of ecosystem, therefore, it's not going to work because the model that
[01:41:11.020 --> 01:41:13.060]   Amazon's working on is too direct.
[01:41:13.060 --> 01:41:14.060]   Right.
[01:41:14.060 --> 01:41:17.580]   And he won't sell you on Apple TV because you can't watch Amazon content on it.
[01:41:17.580 --> 01:41:22.460]   By the way, I've had a little debate on Twitter over whose fault that is.
[01:41:22.460 --> 01:41:24.940]   And I guess there's some fault, there's some blame to both.
[01:41:24.940 --> 01:41:28.620]   Amazon in January said, "Well, we're making an app."
[01:41:28.620 --> 01:41:29.980]   But they never released it.
[01:41:29.980 --> 01:41:33.980]   And I guess the latest is, "Well, we didn't release it because we can't get good deal,
[01:41:33.980 --> 01:41:36.700]   good terms from Apple."
[01:41:36.700 --> 01:41:40.020]   It's a lot of curious, I think, given that they are on the iPad and the iPhone.
[01:41:40.020 --> 01:41:41.020]   Right.
[01:41:41.020 --> 01:41:43.900]   But Apple takes 30% of everything sold through the app.
[01:41:43.900 --> 01:41:47.180]   But they don't, as long as you don't have sign up within the app.
[01:41:47.180 --> 01:41:48.180]   Right.
[01:41:48.180 --> 01:41:53.620]   So what Amazon does on a lot of platforms is, I noticed this on, is it Roku?
[01:41:53.620 --> 01:41:59.500]   One of my TVs, I can watch anything I've purchased on Amazon Prime, but I can't buy
[01:41:59.500 --> 01:42:00.500]   anything there.
[01:42:00.500 --> 01:42:05.540]   I have to go to the Amazon website, buy it, and then I can stream it minutes later.
[01:42:05.540 --> 01:42:07.300]   But that's why it's not on Apple TV.
[01:42:07.300 --> 01:42:09.420]   But why don't they do that on Apple TV if that's the case?
[01:42:09.420 --> 01:42:10.420]   That's the best way.
[01:42:10.420 --> 01:42:16.340]   No, it doesn't make perfect sense, though, because you can't also access the store.
[01:42:16.340 --> 01:42:17.340]   So why will Amazon?
[01:42:17.340 --> 01:42:20.180]   So Amazon, I mean, Apple might be keeping it off.
[01:42:20.180 --> 01:42:22.380]   Who's keeping it off is my question.
[01:42:22.380 --> 01:42:27.620]   I guess what Amazon wants is they want the app on the Apple TV with the ability to buy
[01:42:27.620 --> 01:42:31.380]   stuff and not giving 30% to Apple.
[01:42:31.380 --> 01:42:32.380]   Ding, ding, ding.
[01:42:32.380 --> 01:42:34.860]   And Apple doesn't seem to be willing to negotiate on that, so.
[01:42:34.860 --> 01:42:35.860]   That would be crazy.
[01:42:35.860 --> 01:42:37.620]   You can decide which one is the bad guy.
[01:42:37.620 --> 01:42:39.620]   They're both being pigheaded, aren't they?
[01:42:39.620 --> 01:42:40.620]   Yeah.
[01:42:40.620 --> 01:42:45.900]   Here's how video ads can work per unruly, which I guess is a, this is a Mary-Maker slide.
[01:42:45.900 --> 01:42:46.900]   Okay.
[01:42:46.900 --> 01:42:52.340]   Online advertising still has a long way to go, but it can work if it's authentic.
[01:42:52.340 --> 01:42:57.980]   I think we check a few of these boxes, entertaining, evoked emotion, personal, relatable, useful,
[01:42:57.980 --> 01:42:58.980]   viewer control.
[01:42:58.980 --> 01:43:02.820]   The only one that doesn't work so well, work with the sound off and not in a rough to
[01:43:02.820 --> 01:43:03.820]   bad format.
[01:43:03.820 --> 01:43:06.420]   We miss those two, but that's how that's pretty good.
[01:43:06.420 --> 01:43:07.420]   You've still got visuals.
[01:43:07.420 --> 01:43:10.180]   Yeah, but you don't want to turn the sound off on a podcast.
[01:43:10.180 --> 01:43:12.220]   There's nothing wrong.
[01:43:12.220 --> 01:43:13.220]   Not much going on.
[01:43:13.220 --> 01:43:14.220]   Look at this one.
[01:43:14.220 --> 01:43:18.100]   It's the one that scares advertisers and content companies together.
[01:43:18.100 --> 01:43:19.980]   Global ad blocking users.
[01:43:19.980 --> 01:43:22.460]   It is, talk about a hockey stick.
[01:43:22.460 --> 01:43:27.260]   It is growing and in mobile, that's the blue line growing even faster because it went from
[01:43:27.260 --> 01:43:31.660]   zero to, you know, five, almost half a billion ad blocking users.
[01:43:31.660 --> 01:43:35.060]   And I don't know about you, but I noticed this a lot now on websites when I'm running
[01:43:35.060 --> 01:43:36.060]   an ad blocker.
[01:43:36.060 --> 01:43:37.580]   They'll say, I see you're running an ad blocker.
[01:43:37.580 --> 01:43:39.940]   They may not, they may let me in.
[01:43:39.940 --> 01:43:43.500]   Bloomberg makes me wait five seconds.
[01:43:43.500 --> 01:43:47.180]   Most of them encourage you to turn it off because this is how we make a living, which
[01:43:47.180 --> 01:43:48.820]   I think is the right thing to do.
[01:43:48.820 --> 01:43:52.580]   Some will not let you in, but most people have not taken that attitude yet.
[01:43:52.580 --> 01:43:53.580]   How do you solve this?
[01:43:53.580 --> 01:43:55.420]   What does Fast Company do?
[01:43:55.420 --> 01:43:58.740]   We worry, but we haven't done anything to prevent people from coming to our website.
[01:43:58.740 --> 01:43:59.740]   Right.
[01:43:59.740 --> 01:44:02.980]   Because that could have a negative reaction too, right?
[01:44:02.980 --> 01:44:06.340]   But I understand if I go to a fast company with an ad blocker, you make no money up.
[01:44:06.340 --> 01:44:09.980]   You know all the money, what you want to do is have advertising, which people actually
[01:44:09.980 --> 01:44:13.980]   find to be valuable on some level or at least not objectionable.
[01:44:13.980 --> 01:44:19.860]   And a lot of such a huge percentage of the advertising on the web is objectionable, particularly
[01:44:19.860 --> 01:44:22.860]   because it destroys the experience by sucking up so much bandwidth.
[01:44:22.860 --> 01:44:23.860]   Right.
[01:44:23.860 --> 01:44:24.860]   Well, that's the problem.
[01:44:24.860 --> 01:44:26.580]   Adtech just got way out of hand.
[01:44:26.580 --> 01:44:29.060]   You want a website that loads quickly.
[01:44:29.060 --> 01:44:32.860]   You don't want ads that are to in your face.
[01:44:32.860 --> 01:44:36.380]   Ideally, you want ads that people find to be a benefit somehow.
[01:44:36.380 --> 01:44:37.380]   Yeah.
[01:44:37.380 --> 01:44:38.380]   Yeah.
[01:44:38.380 --> 01:44:42.420]   Based on Mary Meeker's slides that you should translate the show into Chinese and have Chinese
[01:44:42.420 --> 01:44:43.420]   advertising.
[01:44:43.420 --> 01:44:44.420]   An India.
[01:44:44.420 --> 01:44:45.420]   Let's not forget India.
[01:44:45.420 --> 01:44:46.420]   Oh, man.
[01:44:46.420 --> 01:44:47.420]   Yeah.
[01:44:47.420 --> 01:44:48.420]   You know, I don't know if I'm allowed to talk about this or not.
[01:44:48.420 --> 01:44:54.940]   We have been talking with networks about being on linear.
[01:44:54.940 --> 01:44:57.100]   You know what linear is because you're a television person.
[01:44:57.100 --> 01:44:58.700]   I know I didn't know what it was either.
[01:44:58.700 --> 01:44:59.700]   I don't know.
[01:44:59.700 --> 01:45:02.260]   So we're on demand, right?
[01:45:02.260 --> 01:45:03.260]   Linear is like cable TV.
[01:45:03.260 --> 01:45:04.260]   Oh, I see.
[01:45:04.260 --> 01:45:05.460]   You watch it in order.
[01:45:05.460 --> 01:45:06.460]   It's real time.
[01:45:06.460 --> 01:45:07.860]   Well, it's like your live stream is linear.
[01:45:07.860 --> 01:45:11.860]   Live stream, if you're watching live, which very few people do, but if you're watching
[01:45:11.860 --> 01:45:15.780]   relatively, but if you're watching live, you're watching linear, but we've been talking
[01:45:15.780 --> 01:45:18.700]   with people about putting this on in linear and the internet.
[01:45:18.700 --> 01:45:22.580]   There's a lot of interest in India because they want to know about tech.
[01:45:22.580 --> 01:45:23.580]   They speak English.
[01:45:23.580 --> 01:45:28.540]   I don't think anybody's asked us to be in China, but who knows?
[01:45:28.540 --> 01:45:29.540]   Hey.
[01:45:29.540 --> 01:45:30.540]   Yeah.
[01:45:30.540 --> 01:45:35.420]   It is an interesting world we're living in and it is changing rapidly, I have to say.
[01:45:35.420 --> 01:45:38.020]   It's changing very rapidly for Tony Fidell at NEST.
[01:45:38.020 --> 01:45:40.020]   Oh, what do you think happened there?
[01:45:40.020 --> 01:45:45.580]   So Tony Fidell who created the iPod was one of the creators of the iPod of Apple and then
[01:45:45.580 --> 01:45:51.500]   about eight years ago founded NEST, that smart thermostat that so many of us have purchased
[01:45:51.500 --> 01:45:53.740]   and never used again.
[01:45:53.740 --> 01:45:54.740]   I do.
[01:45:54.740 --> 01:45:55.740]   I have two nests.
[01:45:55.740 --> 01:45:56.740]   I've never used them.
[01:45:56.740 --> 01:45:57.740]   I mean, we move.
[01:45:57.740 --> 01:45:59.860]   It was great the first time you program it first, but then I moved and I didn't bother putting
[01:45:59.860 --> 01:46:00.860]   it on because you know what?
[01:46:00.860 --> 01:46:05.380]   It's not so much greater than the existing dumb thermostat that turns on when I get
[01:46:05.380 --> 01:46:07.140]   home and turns off when I go to bed.
[01:46:07.140 --> 01:46:12.340]   It was a hundred times easier to program it in the first place, which was worth the cost.
[01:46:12.340 --> 01:46:15.620]   But sales plummeting because it's a lot of money.
[01:46:15.620 --> 01:46:16.620]   All right.
[01:46:16.620 --> 01:46:21.220]   There's competition from a lot of companies, including Honeywell, the incumbent and EECOB,
[01:46:21.220 --> 01:46:23.220]   which has some features that are superior.
[01:46:23.220 --> 01:46:24.220]   Did Fidell leave?
[01:46:24.220 --> 01:46:25.220]   Was he pushed out?
[01:46:25.220 --> 01:46:26.220]   I mean, he'd been there two years.
[01:46:26.220 --> 01:46:29.660]   Maybe he invested and he said, I never plan on staying.
[01:46:29.660 --> 01:46:30.660]   What happened?
[01:46:30.660 --> 01:46:32.060]   Come on, Harry, you know.
[01:46:32.060 --> 01:46:33.060]   We don't know.
[01:46:33.060 --> 01:46:34.940]   There are a bunch of things to look at.
[01:46:34.940 --> 01:46:40.580]   Supposedly, nests was not living up to the revenue expectations that Google had.
[01:46:40.580 --> 01:46:45.420]   A lot of the news about nests in the last few months has been stuff involving like unhappy
[01:46:45.420 --> 01:46:49.380]   nest employees who thought that Tony was pushing them too hard.
[01:46:49.380 --> 01:46:50.380]   It was obnoxious.
[01:46:50.380 --> 01:46:53.220]   You also feel like Alphabet was kind of shunning nests.
[01:46:53.220 --> 01:47:00.420]   Like they were, this happens in any workplace where somebody becomes the pariah.
[01:47:00.420 --> 01:47:04.660]   I think Alphabet still is this issue that they have all these different companies doing
[01:47:04.660 --> 01:47:08.300]   stuff that relates to hardware and or the smart home.
[01:47:08.300 --> 01:47:13.340]   The whole idea of Alphabet is they're going to have people like Tony Fidell run companies
[01:47:13.340 --> 01:47:16.820]   on their own with a lot of intervention.
[01:47:16.820 --> 01:47:18.340]   But they may fund competition.
[01:47:18.340 --> 01:47:19.340]   It's a great idea and principle.
[01:47:19.340 --> 01:47:20.820]   It's really hard to do in reality.
[01:47:20.820 --> 01:47:27.780]   Google hired Rick Osterlo from Motorola to run their hardware division.
[01:47:27.780 --> 01:47:28.780]   I think, yep.
[01:47:28.780 --> 01:47:32.820]   I have to think they're getting into competition with their own Alphabet buddy.
[01:47:32.820 --> 01:47:37.900]   I think Tony Fidell is a really significant figure and nest is still for all the years
[01:47:37.900 --> 01:47:39.780]   that the smart home has been around.
[01:47:39.780 --> 01:47:43.100]   If you ask somebody to name a smart home product at random, nest is the one.
[01:47:43.100 --> 01:47:48.460]   Or maybe drop cam, which nest bot.
[01:47:48.460 --> 01:47:51.540]   There's still no question as to whether people really want these smart devices in their home
[01:47:51.540 --> 01:47:52.540]   or not.
[01:47:52.540 --> 01:47:53.540]   It's kind of weird.
[01:47:53.540 --> 01:47:58.300]   I think depending on how you look at it, nest has been incredibly influential and important.
[01:47:58.300 --> 01:48:02.140]   Or maybe the whole category it's in is still something where there has not been anything
[01:48:02.140 --> 01:48:06.980]   that's been quite transformative on the level of the iPod.
[01:48:06.980 --> 01:48:10.980]   Whenever someone of this caliber leaves a company or moves, there's always a series
[01:48:10.980 --> 01:48:13.540]   of statements and letters that are made.
[01:48:13.540 --> 01:48:16.700]   This one in particular has made me really wish that there was something that was like
[01:48:16.700 --> 01:48:20.620]   a goodbye letter subtext generator.
[01:48:20.620 --> 01:48:21.620]   This would be--
[01:48:21.620 --> 01:48:24.460]   We need some syntactic analysis here.
[01:48:24.460 --> 01:48:25.900]   This is from Larry Page.
[01:48:25.900 --> 01:48:30.340]   Tender Tony's leadership nest is catapulted to the connected home into the mainstream.
[01:48:30.340 --> 01:48:31.340]   He was a journalist.
[01:48:31.340 --> 01:48:36.940]   Meaning subtext, given their competitors lots of stuff to work with and then catapult over
[01:48:36.940 --> 01:48:41.820]   them, secured leadership positions for each of its products grown in revenue in excess
[01:48:41.820 --> 01:48:43.820]   of 50% a year.
[01:48:43.820 --> 01:48:45.580]   That is pathetic in Google standards.
[01:48:45.580 --> 01:48:46.580]   Yeah.
[01:48:46.580 --> 01:48:49.780]   He's a true visionary subtext and an ass.
[01:48:49.780 --> 01:48:54.500]   And I look forward to continuing working with him in his new role as an advisor to alphabet.
[01:48:54.500 --> 01:48:55.500]   It's a subtext.
[01:48:55.500 --> 01:48:59.740]   Subtext, we've put him in the farthest corner of the campus and told him he doesn't have
[01:48:59.740 --> 01:49:00.980]   to come to work except on Wednesdays.
[01:49:00.980 --> 01:49:02.700]   That doesn't really mean a whole lot to me.
[01:49:02.700 --> 01:49:05.900]   When Andy Rubin left Android, he also stayed around for a while.
[01:49:05.900 --> 01:49:06.900]   Right.
[01:49:06.900 --> 01:49:11.060]   But eventually he laughed and you got to think that eventually Tony Fadel was a really smart
[01:49:11.060 --> 01:49:12.060]   and ambitious guy.
[01:49:12.060 --> 01:49:14.580]   We'll find something more exciting to do than advising.
[01:49:14.580 --> 01:49:17.940]   Maybe with Andy Rubin, they used to work together General Magic, right?
[01:49:17.940 --> 01:49:22.620]   I just think so many of these companies have matured to the point where you can't ask
[01:49:22.620 --> 01:49:29.900]   a guy who got his start basically driving a zodiac to jump on the Titanic and feel satisfied
[01:49:29.900 --> 01:49:31.860]   piloting the wheelhouse a couple days a week.
[01:49:31.860 --> 01:49:35.540]   Well, that was the point of alphabet is to give them that kind of feeling of being in
[01:49:35.540 --> 01:49:38.380]   a startup still even though they're under the alphabet umbrella.
[01:49:38.380 --> 01:49:41.500]   But I think that along with that comes other problems like having competition from other
[01:49:41.500 --> 01:49:45.820]   alphabet companies and losing the favor of Larry Page.
[01:49:45.820 --> 01:49:48.700]   That still is an issue, I would imagine.
[01:49:48.700 --> 01:49:55.020]   The FBI is building a tattoo tracking AI.
[01:49:55.020 --> 01:49:59.540]   You must have put this story in here to capture criminals.
[01:49:59.540 --> 01:50:00.540]   Why do you need an AI?
[01:50:00.540 --> 01:50:04.140]   Well, I was going to say I don't think this is so much AI as it is like when we were talking
[01:50:04.140 --> 01:50:05.140]   about facial recognition.
[01:50:05.140 --> 01:50:06.700]   I mean, yes, there's AI in it.
[01:50:06.700 --> 01:50:08.020]   There's all kinds of stuff.
[01:50:08.020 --> 01:50:13.380]   But this is, is there something about this story that just makes me laugh?
[01:50:13.380 --> 01:50:17.220]   Automated tattoo recognition tech.
[01:50:17.220 --> 01:50:21.300]   This is a study from the Electronic Frontier Foundation who says that they've been working
[01:50:21.300 --> 01:50:24.540]   on this with NIST, the National Institute of Standards and Technology.
[01:50:24.540 --> 01:50:28.540]   Since 2014, the idea, oh, this is more, this is deep.
[01:50:28.540 --> 01:50:31.660]   This is not merely, oh, you know, you've got a picture of a tiger on your chest.
[01:50:31.660 --> 01:50:33.620]   You must be the guy.
[01:50:33.620 --> 01:50:40.220]   It's to develop profiles of people based on what their tattoos say.
[01:50:40.220 --> 01:50:41.220]   Couldn't.
[01:50:41.220 --> 01:50:42.220]   What?
[01:50:42.220 --> 01:50:49.100]   I mean, wouldn't this be easier for using similar technology to facial, facial recognition?
[01:50:49.100 --> 01:50:53.940]   It's not just to recognize it's to say we're going to profile people.
[01:50:53.940 --> 01:50:56.420]   And this is why the EFF is a little worried about.
[01:50:56.420 --> 01:50:59.380]   They say it threatens free speech and privacy.
[01:50:59.380 --> 01:51:05.380]   If you have a particular tattoo, then you're at risk for a particular kind of crime.
[01:51:05.380 --> 01:51:06.380]   Oh.
[01:51:06.380 --> 01:51:07.380]   Oh.
[01:51:07.380 --> 01:51:09.100]   You're most lost to con your forehead.
[01:51:09.100 --> 01:51:12.100]   It is probably not a great sign.
[01:51:12.100 --> 01:51:16.820]   But you can't profile people just because they have a swastika on their forehead.
[01:51:16.820 --> 01:51:17.820]   Right?
[01:51:17.820 --> 01:51:24.940]   I mean, I was getting on a Virgin America flight last year and very nicely dressed flight
[01:51:24.940 --> 01:51:27.940]   attendant welcoming people on.
[01:51:27.940 --> 01:51:33.580]   She had her, you know, she stuck her arm out to take the boarding pass and she had kiss,
[01:51:33.580 --> 01:51:36.900]   the kiss logo tattooed on her wrist.
[01:51:36.900 --> 01:51:41.420]   And I just know that she had a young and vibrant, you know, her youth was exciting.
[01:51:41.420 --> 01:51:42.740]   Yes, very.
[01:51:42.740 --> 01:51:48.940]   And I'm sure nowadays she probably deeply regrets having a kiss tattoo on her wrist.
[01:51:48.940 --> 01:51:51.140]   You're the one she's 90.
[01:51:51.140 --> 01:51:53.660]   But I certainly wouldn't hold it against her.
[01:51:53.660 --> 01:51:54.660]   Right?
[01:51:54.660 --> 01:51:59.020]   She, you know, she just wanted to party all night and rock and roll all day or is it the
[01:51:59.020 --> 01:52:00.660]   other way around?
[01:52:00.660 --> 01:52:04.580]   Rock and roll all night party every day.
[01:52:04.580 --> 01:52:06.580]   Oh, yeah.
[01:52:06.580 --> 01:52:08.500]   See, it's tattooed in my brain.
[01:52:08.500 --> 01:52:10.820]   I don't have it on my wrist.
[01:52:10.820 --> 01:52:15.580]   I do think this is a little troubling if people were profiled based on the tattoos they have.
[01:52:15.580 --> 01:52:23.660]   But there are tattoos that are speaking of dog whistles and, you know, white power signifiers
[01:52:23.660 --> 01:52:28.860]   that may not, I mean, the Nazi swastika is very obvious, but there are less obvious ones.
[01:52:28.860 --> 01:52:34.860]   So every Christmas we get our Christmas tree from Delancey Street, which gives jobs to
[01:52:34.860 --> 01:52:35.860]   right.
[01:52:35.860 --> 01:52:38.220]   Excons who can't get jobs because they have a record.
[01:52:38.220 --> 01:52:44.260]   And it's a wonderful organization for rehabbing people getting it back into the main street.
[01:52:44.260 --> 01:52:49.500]   And so this year we had to go someplace else because of the timing they weren't open,
[01:52:49.500 --> 01:52:50.500]   blah, blah.
[01:52:50.500 --> 01:52:53.580]   And my daughter says to me, that wasn't as much fun.
[01:52:53.580 --> 01:52:57.100]   They didn't have the guys who have the teardrops right by their eyes.
[01:52:57.100 --> 01:53:00.780]   She had no idea, no idea.
[01:53:00.780 --> 01:53:05.300]   And then the year before my son was like reading the guy as he's putting the Christmas
[01:53:05.300 --> 01:53:09.260]   tree up on top of the minivan and across his forehead, it says live to hate.
[01:53:09.260 --> 01:53:12.100]   And my son's asking, live, live.
[01:53:12.100 --> 01:53:13.100]   I'm like, okay.
[01:53:13.100 --> 01:53:14.100]   That's moving on.
[01:53:14.100 --> 01:53:15.100]   Yeah.
[01:53:15.100 --> 01:53:16.100]   But it's just so funny.
[01:53:16.100 --> 01:53:19.900]   Like they don't, I mean, the teardrops, they didn't have any idea, but they identify
[01:53:19.900 --> 01:53:21.540]   that with Christmas trees.
[01:53:21.540 --> 01:53:23.060]   So there you have it.
[01:53:23.060 --> 01:53:28.860]   15,000 images of tattoos obtained from arrestees and inmates were handed over to third parties,
[01:53:28.860 --> 01:53:34.540]   including private companies with little restriction on how the images may be used or shared.
[01:53:34.540 --> 01:53:38.620]   Many of the images reviewed by the EFF contained personally identifying information, including
[01:53:38.620 --> 01:53:41.980]   people's names, faces and birth dates.
[01:53:41.980 --> 01:53:46.980]   But they also didn't follow protocol for ethical research involving humans.
[01:53:46.980 --> 01:53:52.860]   For instance, they only saw permission from supervisors after the first major set of experiments
[01:53:52.860 --> 01:53:55.180]   were completed.
[01:53:55.180 --> 01:53:58.860]   Same researchers have also not disclosed to their supervisors the tattooed data sets
[01:53:58.860 --> 01:54:02.500]   they're using to see the experiments came from prisoners and arrestees.
[01:54:02.500 --> 01:54:04.780]   So there's some real issues with this.
[01:54:04.780 --> 01:54:11.860]   And I, you know what, if you made a mistake in your youth and you got a tattoo that, you
[01:54:11.860 --> 01:54:15.900]   know, isn't something that reflects your personal beliefs today, I don't think that you should
[01:54:15.900 --> 01:54:19.220]   be targeted by law enforcement because of it.
[01:54:19.220 --> 01:54:24.420]   No, it just did make me think that, wow, if, if they're, I know this is not what the story
[01:54:24.420 --> 01:54:30.980]   is about, but what made me think was, wow, if, if there was a way to database tattoos,
[01:54:30.980 --> 01:54:36.140]   you'd think that are unknown criminals, you'd think that the government would be doing
[01:54:36.140 --> 01:54:37.380]   it because it's identifiable.
[01:54:37.380 --> 01:54:40.300]   Yeah, but this isn't like having a mole on your chin.
[01:54:40.300 --> 01:54:42.620]   It's, it's kind of predictive.
[01:54:42.620 --> 01:54:46.580]   Yes, that I completely agree with you that that's the competition.
[01:54:46.580 --> 01:54:47.580]   So they had a competition.
[01:54:47.580 --> 01:54:48.580]   Creepy whack-a-doo.
[01:54:48.580 --> 01:54:53.300]   They sent the tattoo database to 19 organizations, five research institutions, six universities,
[01:54:53.300 --> 01:54:58.820]   eight private companies, including Morpho Track, one of the largest marketers of biometric
[01:54:58.820 --> 01:55:03.740]   technology to law, Morpho Track to law enforcement agencies.
[01:55:03.740 --> 01:55:05.780]   And the idea was a challenge.
[01:55:05.780 --> 01:55:10.500]   They called it the tattoo recognition technology challenge.
[01:55:10.500 --> 01:55:14.180]   The experiments included identify whether an image contained a tattoo, whether algorithms
[01:55:14.180 --> 01:55:16.820]   could match different images of the same tattoo.
[01:55:16.820 --> 01:55:21.100]   But the most alarming research involved matching common visual elements between tattoos with
[01:55:21.100 --> 01:55:24.740]   operational goal of establishing connections between individuals.
[01:55:24.740 --> 01:55:27.060]   So a gang tattoo, perhaps.
[01:55:27.060 --> 01:55:29.460]   Like, well, you know, I don't know.
[01:55:29.460 --> 01:55:30.460]   I don't know.
[01:55:30.460 --> 01:55:31.460]   I don't know.
[01:55:31.460 --> 01:55:35.700]   Did you know that one in five adults in the US, one in five adults in the US has a tattoo?
[01:55:35.700 --> 01:55:39.220]   I did know that because I've done some stories on laser tattoo removal.
[01:55:39.220 --> 01:55:42.180]   How effective is that?
[01:55:42.180 --> 01:55:45.620]   It hasn't changed dramatically in the last few years.
[01:55:45.620 --> 01:55:51.420]   It's a place where I would see if you wanted to make some money, figure out how to do that
[01:55:51.420 --> 01:55:52.420]   better.
[01:55:52.420 --> 01:55:54.420]   Yeah, there's going to be a big market.
[01:55:54.420 --> 01:55:55.420]   Mm-hmm.
[01:55:55.420 --> 01:55:56.420]   Yeah.
[01:55:56.420 --> 01:55:59.660]   I assume it must be way higher for people under a certain age.
[01:55:59.660 --> 01:56:00.660]   Mm.
[01:56:00.660 --> 01:56:01.660]   Interesting.
[01:56:01.660 --> 01:56:02.660]   Is that trend dwindling?
[01:56:02.660 --> 01:56:03.660]   You got ink?
[01:56:03.660 --> 01:56:04.660]   You got ink?
[01:56:04.660 --> 01:56:05.660]   Got no ink.
[01:56:05.660 --> 01:56:06.660]   Oh, I do have ink.
[01:56:06.660 --> 01:56:07.660]   I do?
[01:56:07.660 --> 01:56:09.140]   I do have ink.
[01:56:09.140 --> 01:56:10.740]   I am one of the 20%.
[01:56:10.740 --> 01:56:15.820]   I'm a 20% or you want to see it?
[01:56:15.820 --> 01:56:16.820]   Wow.
[01:56:16.820 --> 01:56:19.900]   I've been in a hot tub with you and I have not seen that.
[01:56:19.900 --> 01:56:21.740]   That must be relatively recent.
[01:56:21.740 --> 01:56:22.740]   Yeah.
[01:56:22.740 --> 01:56:25.260]   It's the Twit logo.
[01:56:25.260 --> 01:56:30.660]   I got a tattooed on my butt.
[01:56:30.660 --> 01:56:31.660]   For charity.
[01:56:31.660 --> 01:56:33.940]   Yeah, you're about your wife told me about this story.
[01:56:33.940 --> 01:56:34.940]   It was New Year's Eve.
[01:56:34.940 --> 01:56:35.940]   Oh, damn.
[01:56:35.940 --> 01:56:36.940]   It was all public.
[01:56:36.940 --> 01:56:37.940]   Right.
[01:56:37.940 --> 01:56:38.940]   And what was I thinking?
[01:56:38.940 --> 01:56:40.780]   I also shaved my head that year.
[01:56:40.780 --> 01:56:41.780]   Great.
[01:56:41.780 --> 01:56:43.820]   And by the way, we've not had another New Year's Eve event since.
[01:56:43.820 --> 01:56:44.820]   Right.
[01:56:44.820 --> 01:56:45.820]   Connection.
[01:56:45.820 --> 01:56:46.820]   Mm, possibly.
[01:56:46.820 --> 01:56:47.820]   Ask the FBI.
[01:56:47.820 --> 01:56:48.820]   Mm-hmm.
[01:56:48.820 --> 01:56:50.820]   Lenovo is in the news again.
[01:56:50.820 --> 01:56:51.820]   Oh, you love this story.
[01:56:51.820 --> 01:56:52.820]   Oh, man.
[01:56:52.820 --> 01:56:54.340]   What is wrong with this company?
[01:56:54.340 --> 01:57:01.940]   So twice already, they've been tagged for putting basically malware on their system.
[01:57:01.940 --> 01:57:06.700]   In both cases, ostensibly to help users keep their system up to date.
[01:57:06.700 --> 01:57:12.540]   In one case, the malware is really about intruded, putting ads into their surfing,
[01:57:12.540 --> 01:57:13.700]   right, onto their browser.
[01:57:13.700 --> 01:57:18.180]   And it had the side effect of allowing man in the middle of text by the name Malefactor.
[01:57:18.180 --> 01:57:19.700]   So Lenovo apologized.
[01:57:19.700 --> 01:57:20.860]   We'll never do this again.
[01:57:20.860 --> 01:57:24.420]   They didn't do it on their business class computers, they did it on their consumer class
[01:57:24.420 --> 01:57:25.420]   computers.
[01:57:25.420 --> 01:57:29.740]   And I am sure, as usual, it was just a case of, we've got to make a little extra because
[01:57:29.740 --> 01:57:32.780]   we don't make enough on these consumer-grade computers.
[01:57:32.780 --> 01:57:41.260]   Now they have a updater app that unfortunately is vulnerable.
[01:57:41.260 --> 01:57:48.660]   Duo Security discovered that holes in the support app would allow eavesdropping attackers
[01:57:48.660 --> 01:57:51.540]   to tap into their unencrypted update channels.
[01:57:51.540 --> 01:57:57.500]   In other words, updating over HTTP, not HTTPS, and use that to compromise users.
[01:57:57.500 --> 01:58:01.820]   The Lenovo Accelerator application could lead to exploitation by an attacker with man in
[01:58:01.820 --> 01:58:03.220]   the middle capabilities.
[01:58:03.220 --> 01:58:08.300]   Lenovo says, "Oops, can you delete that?"
[01:58:08.300 --> 01:58:10.340]   Can you delete that from your Lenovo?
[01:58:10.340 --> 01:58:17.140]   Lenovo recommends customers uninstall the Lenovo Accelerator application.
[01:58:17.140 --> 01:58:18.620]   No credibility issues there.
[01:58:18.620 --> 01:58:20.660]   And by the way, they're not the only company.
[01:58:20.660 --> 01:58:28.220]   Five vendors, we're putting OEM software with equal laptops from Acer, ASUS, Dell, and
[01:58:28.220 --> 01:58:35.500]   HP, we're found to have a dozen vulnerabilities, all contain at least one flaw that would allow
[01:58:35.500 --> 01:58:40.540]   a bad guy to hijack the computer, most of which are easy to exploit.
[01:58:40.540 --> 01:58:41.900]   This is according to the register.
[01:58:41.900 --> 01:58:51.620]   Lenovo says, "46 notebook and 25 desktop lines are affected, including its top-end Y700 gaming...
[01:58:51.620 --> 01:58:53.940]   Oops, I keep spilling my water.
[01:58:53.940 --> 01:58:54.940]   I'm sorry.
[01:58:54.940 --> 01:59:01.020]   My gaming laptop, my ID center, an all-one desktops and the yoga flip netbooks.
[01:59:01.020 --> 01:59:04.620]   However, think-pad and think station once again have skated."
[01:59:04.620 --> 01:59:10.140]   "Basically, if you use a Windows PC, you want to use it with Windows in as close as possible
[01:59:10.140 --> 01:59:11.740]   to an adulterated state."
[01:59:11.740 --> 01:59:13.740]   Yeah, Paul Thorett's been saying this for some time.
[01:59:13.740 --> 01:59:19.460]   "Get the signature edition, Microsoft signature edition, which unfortunately a lot of OEMs
[01:59:19.460 --> 01:59:23.740]   have not offered signature PCs, but that is the best way to do it.
[01:59:23.740 --> 01:59:31.180]   Or, and I suppose I should do this with my HP that I just bought, wipe the machine and
[01:59:31.180 --> 01:59:33.140]   install the pure version of Lenovo.
[01:59:33.140 --> 01:59:36.460]   "Get it from Microsoft, not from HP.
[01:59:36.460 --> 01:59:39.540]   But I think, I feel like there's drivers and stuff I'll be missing by doing that."
[01:59:39.540 --> 01:59:42.140]   "I always think so too, but it never seems to be that much of an issue."
[01:59:42.140 --> 01:59:43.460]   "There for the case, is it?"
[01:59:43.460 --> 01:59:45.060]   "It's very similar with Android phones.
[01:59:45.060 --> 01:59:49.060]   You want an Android phone with this little stuff done to Android as possible?"
[01:59:49.060 --> 01:59:52.620]   "Yeah, fortunately you can buy a Nexus device from Google that is pure, and you can buy
[01:59:52.620 --> 01:59:54.540]   Microsoft devices that are pure.
[01:59:54.540 --> 01:59:58.620]   Although I've heard from some that maybe even though signature PCs aren't completely
[01:59:58.620 --> 02:00:04.580]   flip-free of bloatware, I'm not sure exactly what the status is of that.
[02:00:04.580 --> 02:00:06.820]   But my Surface book seems to be pure Windows.
[02:00:06.820 --> 02:00:08.900]   It doesn't seem to have anything else on it.
[02:00:08.900 --> 02:00:10.140]   Windows 10 is there.
[02:00:10.140 --> 02:00:11.220]   That may be.
[02:00:11.220 --> 02:00:18.980]   There are those who say that is not the best choice.
[02:00:18.980 --> 02:00:26.620]   Now there's a new report that says, this is from Zeedynet, Mary Jo Foley writing, "Microsoft
[02:00:26.620 --> 02:00:33.380]   makes blocking Windows 10 recommended update nearly impossible."
[02:00:33.380 --> 02:00:35.220]   When will they learn?
[02:00:35.220 --> 02:00:42.100]   Did you see the video of the, what was it, the Congolese Freedom Fighters?
[02:00:42.100 --> 02:00:47.540]   Random Patrol, go.
[02:00:47.540 --> 02:00:48.540]   What was it?
[02:00:48.540 --> 02:00:51.980]   They were fighting the rebels.
[02:00:51.980 --> 02:00:54.380]   Where was it?
[02:00:54.380 --> 02:00:56.380]   They were fighting poachers.
[02:00:56.380 --> 02:00:57.380]   Poachers.
[02:00:57.380 --> 02:00:58.380]   Oh, that was it.
[02:00:58.380 --> 02:00:59.660]   Poachers, not rebels.
[02:00:59.660 --> 02:01:02.140]   I confused poachers and rebels.
[02:01:02.140 --> 02:01:32.120]   "
[02:01:32.120 --> 02:01:39.120]   The
[02:01:39.120 --> 02:01:43.860]   DAN, they used satellite links to get their data.
[02:01:43.860 --> 02:01:47.020]   So the staff was a little more, little more than a little displeased when one of the donated
[02:01:47.020 --> 02:01:51.940]   laptops the teams used began upgrading to Windows 10 automatically, pulling in gigabytes
[02:01:51.940 --> 02:01:58.000]   of data over a radio link."
[02:01:58.000 --> 02:01:59.000]   This is just really ridiculous.
[02:01:59.000 --> 02:02:05.500]   I've been really enjoying the weather forecasts that's been interrupted by the, that's a
[02:02:05.500 --> 02:02:06.500]   fun one.
[02:02:06.500 --> 02:02:07.500]   Show that one.
[02:02:07.500 --> 02:02:08.500]   That is so good.
[02:02:08.500 --> 02:02:10.240]   You can tell the television news.
[02:02:10.240 --> 02:02:13.000]   She handled that quite well, I thought.
[02:02:13.000 --> 02:02:17.640]   The real truth is, in TV, you pray for moments.
[02:02:17.640 --> 02:02:19.320]   That's what you hope for.
[02:02:19.320 --> 02:02:20.320]   You really pray for moments.
[02:02:20.320 --> 02:02:24.240]   To take you out of your dull day-to-day routine.
[02:02:24.240 --> 02:02:25.240]   That's right.
[02:02:25.240 --> 02:02:28.800]   And if it's a Windows security update popping up on your weather wall, it's so much better
[02:02:28.800 --> 02:02:31.480]   than you falling down in front of your weather wall.
[02:02:31.480 --> 02:02:32.480]   That's really all you want.
[02:02:32.480 --> 02:02:33.760]   Yeah, don't fall down.
[02:02:33.760 --> 02:02:37.600]   There's the good YouTube highlights and the bad ones.
[02:02:37.600 --> 02:02:40.700]   So if it's Windows 10, sure.
[02:02:40.700 --> 02:02:42.540]   Sure.
[02:02:42.540 --> 02:02:50.740]   So Mary Jo Foley says that this process, every week there's more bad news from Microsoft.
[02:02:50.740 --> 02:02:56.440]   I have to say, I have several like friends who are civilians who have ended up with Windows
[02:02:56.440 --> 02:03:01.700]   10 on their computer and not been entirely sure how it got there.
[02:03:01.700 --> 02:03:05.980]   So here's a Microsoft's response.
[02:03:05.980 --> 02:03:07.500]   By the way, we always recommend it.
[02:03:07.500 --> 02:03:13.180]   And Mary Jo recommends and uses Steve Gibson's great utility, Never 10.
[02:03:13.180 --> 02:03:16.020]   There are other tools, but Never 10 really is the best.
[02:03:16.020 --> 02:03:23.420]   It uses Microsoft's own recommended procedure registry modifications to prevent this.
[02:03:23.420 --> 02:03:26.660]   Microsoft released an official statement on this.
[02:03:26.660 --> 02:03:28.300]   The register report is inaccurate.
[02:03:28.300 --> 02:03:32.940]   The Windows 10 upgrade is a choice.
[02:03:32.940 --> 02:03:36.380]   Need to help people take advantage of the most secure and most productive Windows.
[02:03:36.380 --> 02:03:41.880]   People receive multiple notifications to accept the upgrade and can reschedule or cancel
[02:03:41.880 --> 02:03:44.260]   the upgrade if they want.
[02:03:44.260 --> 02:03:51.580]   So the heck with you, that is kind of a tone deaf response, but the whole thing's been
[02:03:51.580 --> 02:03:53.180]   toned deaf all along.
[02:03:53.180 --> 02:03:54.780]   People are confused.
[02:03:54.780 --> 02:03:59.700]   It is just not the way, you know, you want people who love Windows 10, but you're making
[02:03:59.700 --> 02:04:00.700]   them hate it.
[02:04:00.700 --> 02:04:05.780]   99.5% of people aren't confused on Windows scale, one half of a percent.
[02:04:05.780 --> 02:04:06.780]   That's a lot of people.
[02:04:06.780 --> 02:04:07.780]   Millions.
[02:04:07.780 --> 02:04:10.580]   I like that you said civilians.
[02:04:10.580 --> 02:04:13.460]   People who are not like tech fans, people who don't read tech sites.
[02:04:13.460 --> 02:04:14.460]   Oh, muggles.
[02:04:14.460 --> 02:04:15.460]   Yeah.
[02:04:15.460 --> 02:04:16.460]   People who don't watch Twitter.
[02:04:16.460 --> 02:04:17.460]   Right.
[02:04:17.460 --> 02:04:18.460]   We start.
[02:04:18.460 --> 02:04:20.700]   There's been an evolution of that word as a tech journalist.
[02:04:20.700 --> 02:04:23.500]   It started as my mom, which I now have.
[02:04:23.500 --> 02:04:25.500]   Yes, we can't do that.
[02:04:25.500 --> 02:04:26.500]   Yes.
[02:04:26.500 --> 02:04:27.900]   Because most of our moms are very smart.
[02:04:27.900 --> 02:04:29.460]   Yes.
[02:04:29.460 --> 02:04:32.540]   And your mom used to call into the show every once in a while.
[02:04:32.540 --> 02:04:33.540]   She was great.
[02:04:33.540 --> 02:04:35.380]   My aunt, hey, called into the show.
[02:04:35.380 --> 02:04:36.540]   My mom is wired.
[02:04:36.540 --> 02:04:38.060]   She's got a Galaxy Note 5.
[02:04:38.060 --> 02:04:39.620]   She's got an iPhone 6s plus.
[02:04:39.620 --> 02:04:40.780]   She's got an iPad bro.
[02:04:40.780 --> 02:04:43.780]   Of course, I sent her them all and stuff, but she uses it.
[02:04:43.780 --> 02:04:47.340]   She's got a couple of Mac's PCs, a Windows 2000 PC.
[02:04:47.340 --> 02:04:48.340]   She's pretty wired.
[02:04:48.340 --> 02:04:49.340]   Got it.
[02:04:49.340 --> 02:04:51.340]   So we will not use my mom.
[02:04:51.340 --> 02:04:52.340]   Don't use my mom.
[02:04:52.340 --> 02:04:53.340]   That's not happening.
[02:04:53.340 --> 02:04:54.340]   Then it was average Joe's.
[02:04:54.340 --> 02:04:55.340]   Yeah, that's not it.
[02:04:55.340 --> 02:04:56.340]   Then it was Joe's six pack.
[02:04:56.340 --> 02:04:58.540]   Remember there was that we used that for a while.
[02:04:58.540 --> 02:04:59.540]   Joe's six pack.
[02:04:59.540 --> 02:05:01.820]   Then we had, I tend to use consumers a lot.
[02:05:01.820 --> 02:05:02.820]   Consumers?
[02:05:02.820 --> 02:05:03.820]   I say consumers.
[02:05:03.820 --> 02:05:04.820]   What do you say?
[02:05:04.820 --> 02:05:06.900]   But yeah, you're talking on GMA, which is talking mostly to normals.
[02:05:06.900 --> 02:05:07.900]   Yeah.
[02:05:07.900 --> 02:05:10.100]   You don't want to call them normals or muggles or civilians.
[02:05:10.100 --> 02:05:11.100]   I say us.
[02:05:11.100 --> 02:05:12.100]   There you go.
[02:05:12.100 --> 02:05:14.100]   Smart move.
[02:05:14.100 --> 02:05:18.180]   You know, we really don't understand this Windows 10 upgrade, do we?
[02:05:18.180 --> 02:05:19.340]   It's forcing it on us.
[02:05:19.340 --> 02:05:21.500]   What is Windows doing to us?
[02:05:21.500 --> 02:05:22.500]   Us.
[02:05:22.500 --> 02:05:23.500]   Yeah.
[02:05:23.500 --> 02:05:24.500]   The persecuted.
[02:05:24.500 --> 02:05:25.500]   Yeah, that's good.
[02:05:25.500 --> 02:05:26.500]   Us.
[02:05:26.500 --> 02:05:27.500]   Much.
[02:05:27.500 --> 02:05:28.500]   You know what?
[02:05:28.500 --> 02:05:31.820]   There's something you can do because you can't be in us and do what you do.
[02:05:31.820 --> 02:05:32.820]   Yeah.
[02:05:32.820 --> 02:05:34.580]   It's very difficult to write that line.
[02:05:34.580 --> 02:05:35.580]   Yeah.
[02:05:35.580 --> 02:05:38.860]   Well, yeah, I mean, I actually use Windows 10, so I don't.
[02:05:38.860 --> 02:05:41.700]   And the fact that Microsoft's giving it away seems good.
[02:05:41.700 --> 02:05:46.980]   But at the same time, I get people calling me all the time saying, "What happened?"
[02:05:46.980 --> 02:05:50.060]   I didn't want Windows 10 to suddenly have it.
[02:05:50.060 --> 02:05:51.420]   How did that happen?
[02:05:51.420 --> 02:05:54.620]   And there are good, legitimate reasons for not wanting an upgrade.
[02:05:54.620 --> 02:05:58.460]   Anything besides Siri that's going to come up with a new version of Windows 10?
[02:05:58.460 --> 02:06:00.460]   You come out of this WWDC Thunderbolt?
[02:06:00.460 --> 02:06:01.460]   We don't know.
[02:06:01.460 --> 02:06:02.460]   We don't know.
[02:06:02.460 --> 02:06:03.460]   It's a mystery.
[02:06:03.460 --> 02:06:04.700]   New version of iOS, new version of iOS.
[02:06:04.700 --> 02:06:05.700]   We know that for sure.
[02:06:05.700 --> 02:06:08.780]   You watch OS, probably, new Apple TV, OS, hopefully.
[02:06:08.780 --> 02:06:10.580]   How about hardware though?
[02:06:10.580 --> 02:06:12.940]   They'll only have hardware if they'd happens to be ready.
[02:06:12.940 --> 02:06:16.100]   They wouldn't have hardware because that's at the center of announcements, probably.
[02:06:16.100 --> 02:06:22.860]   There was a rumor which Renee Ritchie at IMOR has thrown out that there was going to
[02:06:22.860 --> 02:06:28.700]   be a new display with -- what was it built in GPS?
[02:06:28.700 --> 02:06:29.700]   >> It's GPU's.
[02:06:29.700 --> 02:06:30.700]   >> JOGPU's.
[02:06:30.700 --> 02:06:31.700]   Okay.
[02:06:31.700 --> 02:06:36.340]   Well, I want to be the first on record to say that Apple will not release a display with
[02:06:36.340 --> 02:06:39.980]   a GPS built in this year.
[02:06:39.980 --> 02:06:41.140]   >> Find my monitor.
[02:06:41.140 --> 02:06:42.980]   >> Oh, GPU.
[02:06:42.980 --> 02:06:44.620]   That makes a lot more sense.
[02:06:44.620 --> 02:06:46.100]   Actually, it doesn't.
[02:06:46.100 --> 02:06:48.860]   Why would you build a GPU into a display?
[02:06:48.860 --> 02:06:53.780]   Because then you can have a laptop that might be too wimpy to power a really great display
[02:06:53.780 --> 02:06:56.700]   and it will power it by having the GPU built into the display.
[02:06:56.700 --> 02:06:59.820]   >> You'd have to have like Thunderbolt 3 or something, so you have a lot of throughput,
[02:06:59.820 --> 02:07:00.820]   right?
[02:07:00.820 --> 02:07:04.300]   >> And this is based on stuff like Thunderbolt where you have great throughput.
[02:07:04.300 --> 02:07:05.300]   >> Right.
[02:07:05.300 --> 02:07:06.300]   Yeah.
[02:07:06.300 --> 02:07:07.300]   Okay.
[02:07:07.300 --> 02:07:09.740]   Monitor with GPS would be more interesting.
[02:07:09.740 --> 02:07:16.860]   There were rumors that there would be a new MacBook Pro as weird as a GPS, frankly.
[02:07:16.860 --> 02:07:21.780]   And OLED function keys that aren't function keys but are OLED screens.
[02:07:21.780 --> 02:07:22.780]   >> Why?
[02:07:22.780 --> 02:07:25.180]   >> It's going to be really programmable.
[02:07:25.180 --> 02:07:26.180]   >> Soft keys.
[02:07:26.180 --> 02:07:27.180]   Yeah.
[02:07:27.180 --> 02:07:28.180]   Do you think that'll happen?
[02:07:28.180 --> 02:07:29.180]   >> Sounds really cool.
[02:07:29.180 --> 02:07:30.180]   >> Sounds cool.
[02:07:30.180 --> 02:07:31.180]   I would buy it, but it doesn't sound like-
[02:07:31.180 --> 02:07:34.020]   >> At the moment, it sounds like something that might be real but probably it's not going
[02:07:34.020 --> 02:07:35.660]   to come out in the immediate future.
[02:07:35.660 --> 02:07:37.460]   >> Wasn't there a keyboard?
[02:07:37.460 --> 02:07:38.460]   Remember that?
[02:07:38.460 --> 02:07:41.620]   It seemed kind of fictional that had OLED keys.
[02:07:41.620 --> 02:07:42.620]   >> Whoa.
[02:07:42.620 --> 02:07:48.580]   Yeah, and it was like from a Russian guy and so that sounded art-lebative.
[02:07:48.580 --> 02:07:51.180]   But I think it was real now that I think about it.
[02:07:51.180 --> 02:07:52.180]   >> Huh.
[02:07:52.180 --> 02:07:54.660]   >> Did anybody ever get the optimist keyboard?
[02:07:54.660 --> 02:07:56.860]   >> It'd be cool for like editors and stuff.
[02:07:56.860 --> 02:07:57.860]   >> Exactly.
[02:07:57.860 --> 02:07:58.860]   >> You're different in the keyboard.
[02:07:58.860 --> 02:07:59.860]   >> They always have stickers on them.
[02:07:59.860 --> 02:08:00.860]   >> Yeah.
[02:08:00.860 --> 02:08:05.700]   >> So the idea is that instead of fixed keys that every key on the keyboard would be a
[02:08:05.700 --> 02:08:07.500]   little OLED screen.
[02:08:07.500 --> 02:08:08.500]   >> Awesome.
[02:08:08.500 --> 02:08:09.900]   >> I guess, is he selling these?
[02:08:09.900 --> 02:08:10.900]   >> Deaculously expensive.
[02:08:10.900 --> 02:08:11.900]   >> $1,500.
[02:08:11.900 --> 02:08:12.900]   >> Never mind.
[02:08:12.900 --> 02:08:13.900]   >> Exactly.
[02:08:13.900 --> 02:08:14.900]   >> Never mind.
[02:08:14.900 --> 02:08:15.900]   How about the popularis?
[02:08:15.900 --> 02:08:18.060]   >> Ooh, that one's down to 1,000.
[02:08:18.060 --> 02:08:20.260]   >> Yeah, the Maximus.
[02:08:20.260 --> 02:08:21.260]   That's going to be the most.
[02:08:21.260 --> 02:08:22.260]   It's sold out.
[02:08:22.260 --> 02:08:24.060]   >> That's a lot of keys.
[02:08:24.060 --> 02:08:25.060]   >> 2007.
[02:08:25.060 --> 02:08:26.060]   >> Jesus.
[02:08:26.060 --> 02:08:27.060]   >> They did sell.
[02:08:27.060 --> 02:08:35.340]   I think I do remember like these little pads so that you could, that would be like an editor
[02:08:35.340 --> 02:08:37.140]   would use that because that would be a controller.
[02:08:37.140 --> 02:08:38.540]   >> So you know our control room at GMA.
[02:08:38.540 --> 02:08:40.900]   You know how we used to have the control room and had all the monitors?
[02:08:40.900 --> 02:08:41.900]   >> Yeah.
[02:08:41.900 --> 02:08:42.900]   >> So I'll just one screen now.
[02:08:42.900 --> 02:08:43.900]   >> Oh neat.
[02:08:43.900 --> 02:08:44.900]   >> It's just a wall screen.
[02:08:44.900 --> 02:08:45.900]   >> Yeah, what do you want bezels for?
[02:08:45.900 --> 02:08:46.900]   You don't need bezels.
[02:08:46.900 --> 02:08:47.900]   >> No.
[02:08:47.900 --> 02:08:52.020]   And so they can maximize or minimize whatever they need the size to be based on whatever
[02:08:52.020 --> 02:08:53.420]   the live remotes are.
[02:08:53.420 --> 02:08:56.380]   >> I just bought this new Dell.
[02:08:56.380 --> 02:08:57.380]   We still haven't set it up.
[02:08:57.380 --> 02:08:58.380]   Maybe we're going to set it up next week.
[02:08:58.380 --> 02:09:01.300]   One of these days, 43 inch Dell monitor.
[02:09:01.300 --> 02:09:03.060]   It could be four displays.
[02:09:03.060 --> 02:09:05.060]   It's kind of like what you're doing in the control room.
[02:09:05.060 --> 02:09:06.420]   I don't know what I'm going to do with that.
[02:09:06.420 --> 02:09:08.140]   No, Don, you don't have to get it out.
[02:09:08.140 --> 02:09:09.740]   Every time I mention it, John pulls it out.
[02:09:09.740 --> 02:09:11.740]   He really wants us to set this thing up.
[02:09:11.740 --> 02:09:12.740]   >> The prop master.
[02:09:12.740 --> 02:09:13.740]   >> Yeah.
[02:09:13.740 --> 02:09:15.300]   He is the prop master, isn't he?
[02:09:15.300 --> 02:09:16.300]   Among other things.
[02:09:16.300 --> 02:09:17.900]   Let's take a little break.
[02:09:17.900 --> 02:09:20.220]   We're, oh, shoot.
[02:09:20.220 --> 02:09:21.220]   The game began.
[02:09:21.220 --> 02:09:22.220]   >> That's okay.
[02:09:22.220 --> 02:09:23.220]   Let's keep going.
[02:09:23.220 --> 02:09:24.220]   I got a DVR.
[02:09:24.220 --> 02:09:25.220]   I got a DVR.
[02:09:25.220 --> 02:09:26.220]   >> You got DVR.
[02:09:26.220 --> 02:09:27.220]   >> Oh, yeah.
[02:09:27.220 --> 02:09:28.220]   >> Those spoilers.
[02:09:28.220 --> 02:09:29.220]   >> Nice.
[02:09:29.220 --> 02:09:30.220]   Yeah.
[02:09:30.220 --> 02:09:31.220]   >> Becky's in the dub nation.
[02:09:31.220 --> 02:09:34.140]   >> That's not it.
[02:09:34.140 --> 02:09:35.500]   Are you in the dub nation?
[02:09:35.500 --> 02:09:36.500]   >> I'm in the dub.
[02:09:36.500 --> 02:09:37.500]   I live in Oakland.
[02:09:37.500 --> 02:09:38.500]   The church is in the show.
[02:09:38.500 --> 02:09:39.500]   >> Are you in the shark tank?
[02:09:39.500 --> 02:09:41.580]   I can't watch hockey except ones live.
[02:09:41.580 --> 02:09:43.380]   My eyes have gotten too bad.
[02:09:43.380 --> 02:09:44.380]   >> Yeah.
[02:09:44.380 --> 02:09:46.740]   It's hard to follow that, Puck.
[02:09:46.740 --> 02:09:48.540]   Our show today brought you by ITProTV.
[02:09:48.540 --> 02:09:49.740]   I know you love IT.
[02:09:49.740 --> 02:09:53.420]   You wouldn't be watching this show if you weren't in the technology.
[02:09:53.420 --> 02:09:57.820]   And being an IT as a job, I think for a lot of you, it would be the dream job, right?
[02:09:57.820 --> 02:09:59.620]   You get to work with technology all the time.
[02:09:59.620 --> 02:10:01.860]   But how do you get into the world of IT?
[02:10:01.860 --> 02:10:06.020]   Well, it turns out there are these certifications, these tests you can take.
[02:10:06.020 --> 02:10:09.020]   And getting a cert is often the key to getting a great job.
[02:10:09.020 --> 02:10:14.060]   But how do you learn ITProTV?
[02:10:14.060 --> 02:10:15.980]   They are incredible.
[02:10:15.980 --> 02:10:19.860]   They've been basically doing what we do here at TwiP, but for the IT professional and the
[02:10:19.860 --> 02:10:21.820]   aspiring IT professional.
[02:10:21.820 --> 02:10:22.820]   Now they have two studios.
[02:10:22.820 --> 02:10:28.780]   They do 50 hours of new content every week on their site in their library, thousands
[02:10:28.780 --> 02:10:32.420]   of hours of content in every aspect of IT.
[02:10:32.420 --> 02:10:37.220]   And now they are the first IT video provider to go do this new Amazon video direct thing.
[02:10:37.220 --> 02:10:43.500]   So you can actually buy ITProTV content on Amazon.
[02:10:43.500 --> 02:10:46.420]   But really the best way to do this is to subscribe, right?
[02:10:46.420 --> 02:10:50.580]   Because once you subscribe, one flat monthly or yearly rate, you get access to everything.
[02:10:50.580 --> 02:10:57.980]   Microsoft Server 2016, MCSC, Microsoft System Center Configuration Manager, CCNA, AWS Ethical
[02:10:57.980 --> 02:10:58.980]   Hacking.
[02:10:58.980 --> 02:11:01.820]   Yeah, there's an Ethical Hacking Cert, VMware.
[02:11:01.820 --> 02:11:05.820]   They just completed, I think it's pronounced SISA, C-I-S-A.
[02:11:05.820 --> 02:11:11.420]   It's a globally recognized cert for audit control and security of information systems.
[02:11:11.420 --> 02:11:15.660]   Brian O'Hara, who's written the book on it for Cybex, was their instructor.
[02:11:15.660 --> 02:11:19.420]   Coming up this month, CCNA Security with Cisco VCP6.
[02:11:19.420 --> 02:11:20.420]   That's VMware.
[02:11:20.420 --> 02:11:24.180]   Adam Gordon is back for the version nine of my favorite cert.
[02:11:24.180 --> 02:11:29.020]   I really want this cert, Certified Ethical Hacker.
[02:11:29.020 --> 02:11:31.940]   Three words that describe me.
[02:11:31.940 --> 02:11:33.180]   I've always thought that.
[02:11:33.180 --> 02:11:36.020]   In my dreams, Certified Ethical Hacker.
[02:11:36.020 --> 02:11:41.900]   But imagine what a great job you could get as a pen tester, as a security expert.
[02:11:41.900 --> 02:11:47.460]   I mean, this is a cert, I want it just for, I want to put it on my wall.
[02:11:47.460 --> 02:11:51.460]   They have a virtual machine lab, so you don't even have to have a Windows machine to take
[02:11:51.460 --> 02:11:53.180]   these, just an HTML5 browser.
[02:11:53.180 --> 02:11:57.260]   You can set up a server, set up clients, the Transcenter Practice Exam.
[02:11:57.260 --> 02:12:00.220]   So you take the test before you take the test.
[02:12:00.220 --> 02:12:02.620]   And it's all for one low monthly subscription price.
[02:12:02.620 --> 02:12:06.340]   And of course, once you get the cert, if you want to cancel, they make it easy.
[02:12:06.340 --> 02:12:08.380]   No hassle cancellation policy.
[02:12:08.380 --> 02:12:14.500]   If you're currently signed up for an enterprise account with an IT Pro TV competitor and the
[02:12:14.500 --> 02:12:17.740]   rates are going up, you know what I'm talking about, right?
[02:12:17.740 --> 02:12:23.580]   IT Pro TV will match your previous year's pricing on that account so that you don't
[02:12:23.580 --> 02:12:27.180]   have to do that increase and you get the benefit of IT Pro TV.
[02:12:27.180 --> 02:12:28.540]   I love it.
[02:12:28.540 --> 02:12:32.140]   Their clients include Harvard, MIT, UCSD, Stanford.
[02:12:32.140 --> 02:12:35.260]   The entire Harvard IT department is our customers.
[02:12:35.260 --> 02:12:41.860]   Go to itpro.tv/twit to upgrade your brain with the most popular certs.
[02:12:41.860 --> 02:12:45.340]   Premium subscriptions are only $57 a month, $570 a year.
[02:12:45.340 --> 02:12:46.340]   That's a really good deal.
[02:12:46.340 --> 02:12:50.860]   But when you use our offer code TWIT30, you'll get a free seven day trial.
[02:12:50.860 --> 02:12:55.900]   And if you decide to buy 30% off forever for the lifetime of your account that makes it
[02:12:55.900 --> 02:13:00.580]   less than $40 a month or by a year, it's just $399.
[02:13:00.580 --> 02:13:01.700]   And that is well worth it.
[02:13:01.700 --> 02:13:07.020]   IT Pro TV/twit, whether you're in IT and want to keep your skills up or you're interested
[02:13:07.020 --> 02:13:09.140]   in getting to IT, it is a great job.
[02:13:09.140 --> 02:13:10.140]   Trust me.
[02:13:10.140 --> 02:13:11.140]   And what a wonderful...
[02:13:11.140 --> 02:13:13.460]   I've got so many good courses.
[02:13:13.460 --> 02:13:17.660]   Forensics, our local police department, Piedleman Police Department came to me.
[02:13:17.660 --> 02:13:22.180]   The detective we'd worked with before he said, "I'm now in charge of our forensics division.
[02:13:22.180 --> 02:13:24.580]   Where can I learn how to use wire shark, how to do forensics?"
[02:13:24.580 --> 02:13:26.420]   I said, "I'm going to place for your IT Pro TV."
[02:13:26.420 --> 02:13:29.260]   They got all the courses he signed up.
[02:13:29.260 --> 02:13:32.500]   It's www.tpro.tv.
[02:13:32.500 --> 02:13:34.260]   Certified ethical hacker.
[02:13:34.260 --> 02:13:36.300]   That's the title of your dream.
[02:13:36.300 --> 02:13:37.300]   Yeah.
[02:13:37.300 --> 02:13:38.300]   I had...
[02:13:38.300 --> 02:13:41.260]   That's shocking to me because I thought that it was male exotic dancer.
[02:13:41.260 --> 02:13:42.620]   That's what I thought.
[02:13:42.620 --> 02:13:43.980]   That's where I thought you were going with that.
[02:13:43.980 --> 02:13:44.980]   Someday.
[02:13:44.980 --> 02:13:45.980]   M-M-E-D?
[02:13:45.980 --> 02:13:47.980]   You're going to be a M-E-D.
[02:13:47.980 --> 02:13:48.980]   I'm an M-E-D.
[02:13:48.980 --> 02:13:49.980]   Leo Lort, M-E-D.
[02:13:49.980 --> 02:13:50.980]   Leo Lort, M-E-D.
[02:13:50.980 --> 02:13:52.140]   I do have breakaway pants.
[02:13:52.140 --> 02:13:54.140]   That's the office costume.
[02:13:54.140 --> 02:13:55.140]   Actually.
[02:13:55.140 --> 02:13:56.140]   What?
[02:13:56.140 --> 02:13:57.140]   Yeah.
[02:13:57.140 --> 02:13:58.140]   Love it.
[02:13:58.140 --> 02:14:00.020]   You see an Uber lifter, "delive driver."
[02:14:00.020 --> 02:14:03.060]   You might see groceries inside while Mark's going to be testing delivery.
[02:14:03.060 --> 02:14:04.060]   That makes sense.
[02:14:04.060 --> 02:14:05.420]   You got a lot of drivers driving around.
[02:14:05.420 --> 02:14:06.780]   Uber drivers not doing anything.
[02:14:06.780 --> 02:14:13.900]   They have got to maximize capacity with their drivers to come driving or go driverless.
[02:14:13.900 --> 02:14:15.580]   That's the next step.
[02:14:15.580 --> 02:14:18.580]   First Uber is a company, a cab company that owns no cabs.
[02:14:18.580 --> 02:14:22.140]   Soon, there's going to be a cab company that owns no drivers, but then they'll have
[02:14:22.140 --> 02:14:23.980]   to buy the cabs.
[02:14:23.980 --> 02:14:25.980]   Yeah.
[02:14:25.980 --> 02:14:27.860]   How much does Saudi Arabia put in Uber?
[02:14:27.860 --> 02:14:28.860]   What?
[02:14:28.860 --> 02:14:30.420]   Some is huge amount of money.
[02:14:30.420 --> 02:14:32.540]   It was enormous.
[02:14:32.540 --> 02:14:33.540]   That is interesting.
[02:14:33.540 --> 02:14:34.700]   Is it the government?
[02:14:34.700 --> 02:14:40.420]   I guess when you talk to Saudi Arabia, it's still the family, whether it's the country,
[02:14:40.420 --> 02:14:41.420]   it's still the family, right?
[02:14:41.420 --> 02:14:42.420]   Fascinating.
[02:14:42.420 --> 02:14:46.540]   They're issuing a bond for the first time right now, raising cash.
[02:14:46.540 --> 02:14:47.540]   Saudi Arabia?
[02:14:47.540 --> 02:14:48.540]   Yeah.
[02:14:48.540 --> 02:14:52.300]   Also, just the finances there are so...
[02:14:52.300 --> 02:14:56.100]   It's still the royal family though, right?
[02:14:56.100 --> 02:14:59.180]   The south.
[02:14:59.180 --> 02:15:04.140]   So why was in Spain doing a documentary about green energy?
[02:15:04.140 --> 02:15:11.540]   They have one now in Nevada, but it's one of these solar arrays that uses mirrors to
[02:15:11.540 --> 02:15:15.620]   heat molten salt, and then the molten salt heats water, which makes steam, which makes
[02:15:15.620 --> 02:15:16.620]   electricity.
[02:15:16.620 --> 02:15:17.620]   Yeah.
[02:15:17.620 --> 02:15:20.900]   And their largest investor is Saudi Arabia.
[02:15:20.900 --> 02:15:22.740]   They got a lot of sun there.
[02:15:22.740 --> 02:15:23.740]   Because they have to diversify.
[02:15:23.740 --> 02:15:24.740]   Yeah.
[02:15:24.740 --> 02:15:25.740]   To diversify.
[02:15:25.740 --> 02:15:30.620]   If you're an oil power, what's the next big thing, right?
[02:15:30.620 --> 02:15:31.620]   Be big in sun.
[02:15:31.620 --> 02:15:32.980]   I think that's actually very small.
[02:15:32.980 --> 02:15:36.620]   So it makes sense that they're also looking at...
[02:15:36.620 --> 02:15:40.980]   They also want to create a car-based economy that's new.
[02:15:40.980 --> 02:15:45.460]   So if cars ownership is going to diminish, as many think that services like Uber and Lyft
[02:15:45.460 --> 02:15:50.460]   will lead to a reduction in car sales, despite the fact that cars are booming now because
[02:15:50.460 --> 02:15:52.380]   the gas is cheap and other reasons.
[02:15:52.380 --> 02:15:56.660]   But it makes sense that they would want to have a stake in that as well.
[02:15:56.660 --> 02:15:57.660]   Sad news.
[02:15:57.660 --> 02:15:58.660]   McJugger Nuggets is retiring.
[02:15:58.660 --> 02:16:01.060]   Oh, you're going to bring Minecraft to me again?
[02:16:01.060 --> 02:16:02.060]   No, no.
[02:16:02.060 --> 02:16:03.060]   That's another story.
[02:16:03.060 --> 02:16:04.060]   Minecraft.
[02:16:04.060 --> 02:16:05.060]   Yes.
[02:16:05.060 --> 02:16:06.060]   I know you.
[02:16:06.060 --> 02:16:07.060]   Do you still forbid your kids?
[02:16:07.060 --> 02:16:08.060]   Yep.
[02:16:08.060 --> 02:16:09.460]   No Minecraft for you.
[02:16:09.460 --> 02:16:11.100]   You know, it sold 100 million now.
[02:16:11.100 --> 02:16:12.740]   This is the new sales.
[02:16:12.740 --> 02:16:13.740]   What is it?
[02:16:13.740 --> 02:16:15.540]   53,000 copies every day.
[02:16:15.540 --> 02:16:17.780]   This is the beginning of the year.
[02:16:17.780 --> 02:16:23.420]   Every day 53,000 copies each month, more than 40 million people go into their Minecraft
[02:16:23.420 --> 02:16:24.420]   world.
[02:16:24.420 --> 02:16:25.780]   I have three Minecraft worlds on my server.
[02:16:25.780 --> 02:16:26.780]   Just running.
[02:16:26.780 --> 02:16:28.420]   I love it.
[02:16:28.420 --> 02:16:29.420]   I know.
[02:16:29.420 --> 02:16:30.820]   Minecraft is everywhere.
[02:16:30.820 --> 02:16:33.860]   What do you think my kids are going to be doing?
[02:16:33.860 --> 02:16:35.020]   My kids are eight.
[02:16:35.020 --> 02:16:40.460]   What's their best-case job when they're 25, 30?
[02:16:40.460 --> 02:16:45.580]   You know, there's actually a good living to be made building Minecraft worlds for corporations.
[02:16:45.580 --> 02:16:47.260]   I'm not kidding.
[02:16:47.260 --> 02:16:51.140]   One of our viewers does this for a living.
[02:16:51.140 --> 02:16:53.140]   Wow.
[02:16:53.140 --> 02:16:57.140]   The Sims have discarded gender rules for all clothing, customizations.
[02:16:57.140 --> 02:16:58.140]   Here's the good news.
[02:16:58.140 --> 02:16:59.780]   You can use any bathroom you want.
[02:16:59.780 --> 02:17:00.780]   Nice.
[02:17:00.780 --> 02:17:01.780]   Go Sims.
[02:17:01.780 --> 02:17:02.780]   Go Sims.
[02:17:02.780 --> 02:17:05.020]   You want to put heels on a male Sim?
[02:17:05.020 --> 02:17:12.580]   I didn't know you couldn't, but apparently they had rules, but they've rightly so unlocked
[02:17:12.580 --> 02:17:15.780]   the Sims 4 customization options.
[02:17:15.780 --> 02:17:16.780]   I love it.
[02:17:16.780 --> 02:17:22.380]   You see that L'Oreal is going to have makeup for Snapchat that they're going to virtually
[02:17:22.380 --> 02:17:24.140]   put on?
[02:17:24.140 --> 02:17:27.020]   That's the hottest thing is those Snapchat filters, right?
[02:17:27.020 --> 02:17:28.020]   Isn't that hot?
[02:17:28.020 --> 02:17:29.020]   They already have it.
[02:17:29.020 --> 02:17:31.020]   Oh, wow.
[02:17:31.020 --> 02:17:32.020]   I've done that.
[02:17:32.020 --> 02:17:33.020]   Yeah, no, didn't I have any to go?
[02:17:33.020 --> 02:17:34.020]   That's the future of advertising.
[02:17:34.020 --> 02:17:35.020]   Yeah.
[02:17:35.020 --> 02:17:36.020]   Yeah.
[02:17:36.020 --> 02:17:37.020]   You've seen my mom's...
[02:17:37.020 --> 02:17:38.020]   Wait a minute.
[02:17:38.020 --> 02:17:39.020]   Maybe you haven't.
[02:17:39.020 --> 02:17:40.020]   I haven't seen.
[02:17:40.020 --> 02:17:42.220]   My mom has an Instagram account.
[02:17:42.220 --> 02:17:45.740]   I told her she should do it on Snapchat.
[02:17:45.740 --> 02:17:48.220]   Where she plays a character using...
[02:17:48.220 --> 02:17:50.740]   It's the same idea.
[02:17:50.740 --> 02:17:52.140]   Oh, did she kill her account?
[02:17:52.140 --> 02:17:53.140]   Oh, I think she did.
[02:17:53.140 --> 02:17:55.220]   Is your mom on Snapchat?
[02:17:55.220 --> 02:17:57.340]   No, I couldn't figure out how to...
[02:17:57.340 --> 02:17:58.340]   I tried to.
[02:17:58.340 --> 02:18:04.980]   I couldn't figure out how to explain Snapchat to my mom was 83 and very smart, but she does
[02:18:04.980 --> 02:18:05.980]   have an Instagram account.
[02:18:05.980 --> 02:18:07.980]   I think it's gone, unfortunately.
[02:18:07.980 --> 02:18:09.820]   I'm sad to say.
[02:18:09.820 --> 02:18:15.700]   She had a character called Ms. Honeybell that was the makeup from like a Snapchat account.
[02:18:15.700 --> 02:18:19.580]   It's a Snapchat filter of big red cheeks and big eyes.
[02:18:19.580 --> 02:18:20.580]   Wow.
[02:18:20.580 --> 02:18:26.420]   And then I guess she started doing it and she started channeling some strange character,
[02:18:26.420 --> 02:18:27.420]   Ms. Honeybell.
[02:18:27.420 --> 02:18:28.420]   Ms. Honeybell.
[02:18:28.420 --> 02:18:29.420]   Wow.
[02:18:29.420 --> 02:18:31.820]   Is Ms. Honeybell Southern or she wrote Ellen Dee?
[02:18:31.820 --> 02:18:33.780]   Yes, she's from the South.
[02:18:33.780 --> 02:18:34.780]   Oh.
[02:18:34.780 --> 02:18:36.420]   I wonder if she has taken it down.
[02:18:36.420 --> 02:18:38.100]   Your mother's secret southern bell.
[02:18:38.100 --> 02:18:39.100]   She's very strange.
[02:18:39.100 --> 02:18:40.100]   Oh, that is so funny.
[02:18:40.100 --> 02:18:41.300]   Maybe she's still there.
[02:18:41.300 --> 02:18:42.300]   Oh, that is so funny.
[02:18:42.300 --> 02:18:43.300]   Yeah, I believe she is.
[02:18:43.300 --> 02:18:44.300]   You want to see...
[02:18:44.300 --> 02:18:45.300]   Oh, I want to see Ms. Honeybell.
[02:18:45.300 --> 02:18:47.540]   She's putting her art up there now, which is ridiculous.
[02:18:47.540 --> 02:18:48.540]   Oh, wow.
[02:18:48.540 --> 02:18:50.540]   You said I was joking.
[02:18:50.540 --> 02:18:52.540]   That is adorable.
[02:18:52.540 --> 02:18:54.540]   Oh, my God.
[02:18:54.540 --> 02:18:55.660]   Oh, you can't hear it.
[02:18:55.660 --> 02:18:57.540]   We haven't got the sound working.
[02:18:57.540 --> 02:18:59.180]   Oh, man.
[02:18:59.180 --> 02:19:02.060]   I want you to hear Ms. Honeybell.
[02:19:02.060 --> 02:19:03.260]   That's my mom.
[02:19:03.260 --> 02:19:05.060]   Yeah, that's...
[02:19:05.060 --> 02:19:07.140]   Laurie Al is going to be speaking to her.
[02:19:07.140 --> 02:19:08.140]   Wow.
[02:19:08.140 --> 02:19:09.140]   She's got this.
[02:19:09.140 --> 02:19:11.140]   She has got this.
[02:19:11.140 --> 02:19:12.140]   Ms. Honeybell.
[02:19:12.140 --> 02:19:14.620]   Well, you know, it's...
[02:19:14.620 --> 02:19:18.740]   I won't let my son play Minecraft and I won't let my daughter play Laurie Al makeup.
[02:19:18.740 --> 02:19:21.660]   So, you know, they have this to look forward to in their retirement.
[02:19:21.660 --> 02:19:22.660]   Yeah, yeah.
[02:19:22.660 --> 02:19:25.060]   Ms. Honeybell.
[02:19:25.060 --> 02:19:26.780]   I wish I had audio for you.
[02:19:26.780 --> 02:19:28.420]   I knew it.
[02:19:28.420 --> 02:19:29.620]   We worked so hard to get this.
[02:19:29.620 --> 02:19:30.620]   So, I'm...
[02:19:30.620 --> 02:19:32.780]   It's this honking giant laptop in front of me.
[02:19:32.780 --> 02:19:33.780]   Wow.
[02:19:33.780 --> 02:19:34.780]   Did it look...
[02:19:34.780 --> 02:19:35.780]   Show the single.
[02:19:35.780 --> 02:19:36.780]   Is this too big now?
[02:19:36.780 --> 02:19:37.780]   Is my laptop making my hand look big?
[02:19:37.780 --> 02:19:39.500]   Is my laptop making my hand look small?
[02:19:39.500 --> 02:19:40.500]   That's a first.
[02:19:40.500 --> 02:19:41.500]   It is giant.
[02:19:41.500 --> 02:19:43.940]   It's a giant brush wood size laptop.
[02:19:43.940 --> 02:19:44.940]   It's 17 inches.
[02:19:44.940 --> 02:19:47.020]   It's a Linux laptop.
[02:19:47.020 --> 02:19:51.980]   And I'm thinking now as I look at it, I might be a little too big.
[02:19:51.980 --> 02:19:52.980]   I know.
[02:19:52.980 --> 02:19:54.980]   It's very slimming.
[02:19:54.980 --> 02:19:55.980]   Slimming.
[02:19:55.980 --> 02:19:57.940]   Leo seems to have shrunk.
[02:19:57.940 --> 02:20:02.260]   It makes all of us look a lot piteier.
[02:20:02.260 --> 02:20:04.340]   Tell you what, let's wrap this sucker up.
[02:20:04.340 --> 02:20:07.020]   We got a ball game to go to.
[02:20:07.020 --> 02:20:08.020]   Whoop, whoop.
[02:20:08.020 --> 02:20:11.940]   Becky Dubnation Warley, so great to have you from Good Morning America.
[02:20:11.940 --> 02:20:13.620]   @bworley on Twitter.
[02:20:13.620 --> 02:20:17.100]   @bworley on Twitter.
[02:20:17.100 --> 02:20:18.100]   Anything else you want to plug?
[02:20:18.100 --> 02:20:19.100]   What's up to?
[02:20:19.100 --> 02:20:20.100]   What's going on?
[02:20:20.100 --> 02:20:21.100]   Oh, I don't know.
[02:20:21.100 --> 02:20:22.100]   Tweet me a good book you're reading.
[02:20:22.100 --> 02:20:23.460]   I just finished Boys in the Boat.
[02:20:23.460 --> 02:20:24.460]   Amazing.
[02:20:24.460 --> 02:20:25.460]   What's that about?
[02:20:25.460 --> 02:20:31.100]   It's about the 1936 crew team from the University of Washington that went to Berlin.
[02:20:31.100 --> 02:20:33.380]   And it's just a phenomenal summer read.
[02:20:33.380 --> 02:20:35.260]   I cannot say enough good things about it.
[02:20:35.260 --> 02:20:40.260]   I realized I loved what was the Zamperini book.
[02:20:40.260 --> 02:20:42.140]   Oh, yeah, I'm breakable.
[02:20:42.140 --> 02:20:43.140]   I'm broken.
[02:20:43.140 --> 02:20:44.140]   What a great book.
[02:20:44.140 --> 02:20:45.860]   It felt like that for me.
[02:20:45.860 --> 02:20:49.540]   I didn't love that movie because it was just too brutal, but the book was great.
[02:20:49.540 --> 02:20:50.540]   It was brutal.
[02:20:50.540 --> 02:20:53.260]   Yeah, so tweet me a good book like that if you love it.
[02:20:53.260 --> 02:20:54.260]   @bworley.
[02:20:54.260 --> 02:20:55.260]   Oh, that's nice.
[02:20:55.260 --> 02:20:56.260]   Yeah.
[02:20:56.260 --> 02:20:59.780]   I'm not going to be seeing the morning, but you know, you watched Good Morning America.
[02:20:59.780 --> 02:21:02.220]   Because you know, it's a way to connect with the girls.
[02:21:02.220 --> 02:21:04.220]   You'll be seeing the Foldit any day now.
[02:21:04.220 --> 02:21:05.220]   What what?
[02:21:05.220 --> 02:21:07.220]   The Foldit only going to be on.
[02:21:07.220 --> 02:21:08.700]   I want you to get one.
[02:21:08.700 --> 02:21:09.860]   I'm gonna bring him on.
[02:21:09.860 --> 02:21:10.860]   I'm gonna.
[02:21:10.860 --> 02:21:11.860]   Done.
[02:21:11.860 --> 02:21:12.860]   Harry McCracken.
[02:21:12.860 --> 02:21:13.860]   He's at the fastcompany.com.
[02:21:13.860 --> 02:21:14.860]   He's the technology.
[02:21:14.860 --> 02:21:15.860]   Yeah, we love him.
[02:21:15.860 --> 02:21:16.860]   I wanted you.
[02:21:16.860 --> 02:21:26.540]   You could tell I was going there, but he's also at Harry McCracken on Twitter and follow
[02:21:26.540 --> 02:21:27.540]   him on Instagram.
[02:21:27.540 --> 02:21:29.500]   I still love all the retro stuff.
[02:21:29.500 --> 02:21:30.500]   You post pictures.
[02:21:30.500 --> 02:21:31.500]   I love when you say that.
[02:21:31.500 --> 02:21:35.180]   Like I, my Instagram is the longest time I was following more people on Instagram.
[02:21:35.180 --> 02:21:38.540]   Then we're following me and whenever you plug me.
[02:21:38.540 --> 02:21:41.940]   But is this is becoming your calling?
[02:21:41.940 --> 02:21:43.420]   Like the retro thing?
[02:21:43.420 --> 02:21:46.140]   Because I went to the Gerald Schultz Museum before we came here and.
[02:21:46.140 --> 02:21:47.140]   Oh, is that fun?
[02:21:47.140 --> 02:21:48.140]   Yes.
[02:21:48.140 --> 02:21:49.140]   Don't you love that?
[02:21:49.140 --> 02:21:50.900]   Great exhibit on peanuts and Washington DC.
[02:21:50.900 --> 02:21:56.320]   Like with Ronald Reagan's letters to Charles Schultz and Snoopy wrote a letter to Hillary
[02:21:56.320 --> 02:22:02.900]   Clinton in 1968 when she became the president of the Wellesley School Government.
[02:22:02.900 --> 02:22:05.980]   So you're the, you're on Instagram as technologizer.
[02:22:05.980 --> 02:22:06.980]   Yes.
[02:22:06.980 --> 02:22:09.060]   So there's my stuff from the museum.
[02:22:09.060 --> 02:22:10.060]   We all voted.
[02:22:10.060 --> 02:22:12.300]   I voted for Linus for president.
[02:22:12.300 --> 02:22:13.780]   He ran.
[02:22:13.780 --> 02:22:14.780]   Too bad.
[02:22:14.780 --> 02:22:15.780]   He never won.
[02:22:15.780 --> 02:22:16.780]   He's running again this year.
[02:22:16.780 --> 02:22:17.780]   Yeah.
[02:22:17.780 --> 02:22:18.780]   I loved this.
[02:22:18.780 --> 02:22:20.780]   Farrah's glamour center.
[02:22:20.780 --> 02:22:23.940]   I think Lori Al could learn a little from this.
[02:22:23.940 --> 02:22:24.940]   Yeah.
[02:22:24.940 --> 02:22:25.940]   That was bad.
[02:22:25.940 --> 02:22:26.940]   You and Marie were at an antique store.
[02:22:26.940 --> 02:22:31.420]   I know because she posted a bunch of pictures from the antique store as well.
[02:22:31.420 --> 02:22:34.660]   That is, is that Ms. Honeybell?
[02:22:34.660 --> 02:22:38.140]   That's my mom who I just recently discovered was on Twitter.
[02:22:38.140 --> 02:22:39.140]   Oh, neat.
[02:22:39.140 --> 02:22:40.140]   You see?
[02:22:40.140 --> 02:22:41.140]   You see?
[02:22:41.140 --> 02:22:44.340]   It wasn't and I said something on Twitter and she commented it and I said what I thought
[02:22:44.340 --> 02:22:45.340]   you went on Twitter.
[02:22:45.340 --> 02:22:46.340]   I'm not.
[02:22:46.340 --> 02:22:47.820]   She replied to that saying only occasionally.
[02:22:47.820 --> 02:22:49.620]   My mom's on Twitter.
[02:22:49.620 --> 02:22:50.620]   All right.
[02:22:50.620 --> 02:22:51.900]   So she's still an egg though.
[02:22:51.900 --> 02:22:53.620]   I need to show her an uploader photo.
[02:22:53.620 --> 02:22:54.780]   Yeah.
[02:22:54.780 --> 02:22:56.620]   I don't know what and I what this was.
[02:22:56.620 --> 02:22:58.860]   I went to the new Apple store.
[02:22:58.860 --> 02:22:59.860]   We went to the on-site concert.
[02:22:59.860 --> 02:23:00.860]   Oh, that's Beyonce.
[02:23:00.860 --> 02:23:01.860]   You should have known.
[02:23:01.860 --> 02:23:03.020]   Yeah.
[02:23:03.020 --> 02:23:05.500]   You learned so much about your friends following him on Instagram.
[02:23:05.500 --> 02:23:08.220]   Harry McCracken is so much cooler than me.
[02:23:08.220 --> 02:23:09.220]   I really do.
[02:23:09.220 --> 02:23:11.220]   I'm glad I went.
[02:23:11.220 --> 02:23:14.740]   Actually, from a technological standpoint, it was really interesting.
[02:23:14.740 --> 02:23:17.860]   You know what changed concerts is these incredible screens.
[02:23:17.860 --> 02:23:21.700]   They have the projection that's like unbelievably tall because they don't have to build sets
[02:23:21.700 --> 02:23:22.700]   anymore.
[02:23:22.700 --> 02:23:24.500]   They can make it look like anything.
[02:23:24.500 --> 02:23:26.180]   It's really remarkable.
[02:23:26.180 --> 02:23:27.860]   Yeah, from a technology standpoint, it was fascinating.
[02:23:27.860 --> 02:23:28.860]   I agree.
[02:23:28.860 --> 02:23:32.740]   I know more about the screen technology they use at concerts these days because it's very
[02:23:32.740 --> 02:23:33.900]   good.
[02:23:33.900 --> 02:23:34.900]   It's vivid.
[02:23:34.900 --> 02:23:36.420]   And every seat in the house was pretty good?
[02:23:36.420 --> 02:23:37.420]   Yeah.
[02:23:37.420 --> 02:23:38.420]   Right.
[02:23:38.420 --> 02:23:42.140]   You can't have a bad seat because you're looking at this giant screen.
[02:23:42.140 --> 02:23:46.980]   And then of course, there's a lot of theater going on as well with the dancers and the
[02:23:46.980 --> 02:23:47.980]   performers.
[02:23:47.980 --> 02:23:48.980]   It really be staged.
[02:23:48.980 --> 02:23:49.980]   They sneak out into the audience.
[02:23:49.980 --> 02:23:50.980]   Yeah.
[02:23:50.980 --> 02:23:52.940]   These concerts have really become quite interesting, I think.
[02:23:52.940 --> 02:23:55.380]   The big tours.
[02:23:55.380 --> 02:23:56.540]   We thank you for being here.
[02:23:56.540 --> 02:24:02.500]   We do the show every Sunday afternoon, 3 p.m. Pacific, 6 p.m. Eastern Time, 2200 UTC.
[02:24:02.500 --> 02:24:05.900]   Love it if you can watch us live and be in our chatroom at IRC.tv.
[02:24:05.900 --> 02:24:09.580]   If you could be in the studio even better, just email tickets at twitter.tv.
[02:24:09.580 --> 02:24:11.180]   We'll put a chair out for you.
[02:24:11.180 --> 02:24:14.820]   You're going to want to do it soon because we're going to be moving out of the brick house
[02:24:14.820 --> 02:24:19.180]   in about three months, I think, at this point, at this stage.
[02:24:19.180 --> 02:24:22.340]   Construction has already begun in our new studio down the road apiece.
[02:24:22.340 --> 02:24:23.500]   We're very excited about it.
[02:24:23.500 --> 02:24:26.860]   If you watch at home, it won't look so very different.
[02:24:26.860 --> 02:24:29.060]   We're going to take our sets with us.
[02:24:29.060 --> 02:24:34.700]   But the experience in the studio will be somewhat different, I think.
[02:24:34.700 --> 02:24:38.020]   So come here now while you can.
[02:24:38.020 --> 02:24:44.660]   Oh, Michael O'Donnell, our staff photographer at this point.
[02:24:44.660 --> 02:24:47.780]   He, is that on his Twitter?
[02:24:47.780 --> 02:24:48.780]   Oh, man.
[02:24:48.780 --> 02:24:49.780]   Twitter.com/photo.
[02:24:49.780 --> 02:24:53.460]   Before the show, we were talking about the fact that my daughter, I decloated.
[02:24:53.460 --> 02:24:57.580]   She closed all the windows that she'd used to write up the wedding vows for her best friend
[02:24:57.580 --> 02:24:59.380]   and her best friend's kitten.
[02:24:59.380 --> 02:25:00.380]   And so.
[02:25:00.380 --> 02:25:02.540]   And so here they are, the...
[02:25:02.540 --> 02:25:03.540]   Kitten wedding.
[02:25:03.540 --> 02:25:05.540]   I look like Grumpy Cat.
[02:25:05.540 --> 02:25:08.020]   I don't know, you're marrying Grumpy Cat.
[02:25:08.020 --> 02:25:09.700]   I look hot.
[02:25:09.700 --> 02:25:10.700]   You look fantastic.
[02:25:10.700 --> 02:25:11.700]   Wow, glasses are like that.
[02:25:11.700 --> 02:25:12.700]   What the hell's going on with Grumpy Cat?
[02:25:12.700 --> 02:25:17.220]   I think you're mad that my ears are pointing in the wrong direction.
[02:25:17.220 --> 02:25:21.860]   That whole ear and nose and whiskers thing is working for me.
[02:25:21.860 --> 02:25:23.420]   Are you doing that on your phone, Michael?
[02:25:23.420 --> 02:25:24.420]   That was incredible.
[02:25:24.420 --> 02:25:26.660]   What app are you using?
[02:25:26.660 --> 02:25:28.140]   Perfect 365.
[02:25:28.140 --> 02:25:29.140]   Perfect 365.
[02:25:29.140 --> 02:25:30.140]   That's pretty cool.
[02:25:30.140 --> 02:25:31.140]   Dang.
[02:25:31.140 --> 02:25:34.340]   So if you want me to take pictures of us, you can do that too.
[02:25:34.340 --> 02:25:35.340]   Wow.
[02:25:35.340 --> 02:25:36.340]   Email tickets at Twit.tv.
[02:25:36.340 --> 02:25:39.260]   Now, of course, we make on-demand audio and video of all of our shows available at the
[02:25:39.260 --> 02:25:43.580]   website, Twit.tv or wherever you subscribe to podcasts.
[02:25:43.580 --> 02:25:48.060]   And of course, the Twit apps that are everywhere on every platform, thanks to our fabulous
[02:25:48.060 --> 02:25:51.660]   third-party developers here at the website.
[02:25:51.660 --> 02:25:52.660]   Thank you all for being here.
[02:25:52.660 --> 02:25:53.660]   We'll see you next time.
[02:25:53.660 --> 02:25:55.660]   Another Twit is in the can.
[02:25:55.660 --> 02:25:56.660]   Go Warriors!
[02:25:56.660 --> 02:25:57.660]   Go Shucks!
[02:25:57.660 --> 02:25:58.660]   This is the same.
[02:25:58.660 --> 02:25:59.660]   Let's go home!
[02:25:59.660 --> 02:26:00.660]   I'm on a Twit.
[02:26:00.660 --> 02:26:01.660]   Do on the Twit.
[02:26:01.660 --> 02:26:02.660]   All right.
[02:26:02.660 --> 02:26:03.660]   Do on the Twit, baby.
[02:26:03.660 --> 02:26:04.660]   Do on the Twit.
[02:26:04.660 --> 02:26:05.660]   All right.
[02:26:05.660 --> 02:26:06.660]   Do on the Twit.
[02:26:06.660 --> 02:26:07.660]   All right.
[02:26:07.660 --> 02:26:13.380]   Oh, I just think life is a ball.
[02:26:13.380 --> 02:26:20.980]   I've been going to so many parties lately that I haven't really done you justice, honey.
[02:26:20.980 --> 02:26:23.380]   I'm thinking of you all the time.

