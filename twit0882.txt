
[00:00:00.000 --> 00:00:02.700]   It's time for Twit This Week at Tech. Love this panel.
[00:00:02.700 --> 00:00:04.800]   Glenn Fleischmann is visiting us from Seattle.
[00:00:04.800 --> 00:00:06.800]   So is Christina Warren, film girl.
[00:00:06.800 --> 00:00:09.300]   From Silicon Valley, it's Mike Elgin.
[00:00:09.300 --> 00:00:11.400]   We'll talk about GitHub co-pilot.
[00:00:11.400 --> 00:00:12.700]   Is it stealing?
[00:00:12.700 --> 00:00:14.300]   Open-source software?
[00:00:14.300 --> 00:00:16.500]   Where was it? A great thing for programmers.
[00:00:16.500 --> 00:00:19.600]   Elon Musk is back on Twitter after 9-day absence,
[00:00:19.600 --> 00:00:21.500]   and he's with the Pope.
[00:00:21.500 --> 00:00:27.300]   Plus, why Judge Lutig spoke so slowly.
[00:00:27.300 --> 00:00:29.200]   It turns out he loves Twitter.
[00:00:29.200 --> 00:00:31.400]   It's all coming up next on Twit.
[00:00:31.400 --> 00:00:36.100]   Podcasts you love.
[00:00:36.100 --> 00:00:38.100]   From people you trust.
[00:00:38.100 --> 00:00:40.600]   This is Twit.
[00:00:40.600 --> 00:00:50.500]   This is Twit.
[00:00:50.500 --> 00:00:53.400]   This Week in Tech, episode 882,
[00:00:53.400 --> 00:00:56.400]   recorded Sunday, July 3, 2022.
[00:00:56.400 --> 00:00:58.600]   Many, many meta-versus.
[00:00:58.600 --> 00:01:03.500]   This Week in Tech is brought to you by Policy Genius.
[00:01:03.500 --> 00:01:05.500]   If someone relies on your financial support,
[00:01:05.500 --> 00:01:08.300]   whether it's a child, aging parent, even a business partner,
[00:01:08.300 --> 00:01:10.100]   you need life insurance.
[00:01:10.100 --> 00:01:12.700]   Head to policygenius.com/twit
[00:01:12.700 --> 00:01:15.000]   to get your free life insurance quotes
[00:01:15.000 --> 00:01:17.000]   and see how much you could save.
[00:01:17.000 --> 00:01:19.000]   And by Zapier.
[00:01:19.000 --> 00:01:21.900]   Zapier makes it easy to connect all your apps,
[00:01:21.900 --> 00:01:25.700]   automate routine tasks, and streamline your processes.
[00:01:25.700 --> 00:01:27.500]   Try Zapier for free today.
[00:01:27.500 --> 00:01:30.700]   At Zapier.com/twit
[00:01:30.700 --> 00:01:32.700]   and by podium.
[00:01:32.700 --> 00:01:35.500]   Join more than 100,000 businesses
[00:01:35.500 --> 00:01:39.500]   that already use podium to streamline their customer interactions.
[00:01:39.500 --> 00:01:42.200]   See how podium can grow your business.
[00:01:42.200 --> 00:01:46.500]   Watch a demo today at podium.com/twit
[00:01:46.500 --> 00:01:48.800]   and by Blue Land.
[00:01:48.800 --> 00:01:52.700]   Blue Land is on a mission to eliminate single-use plastics
[00:01:52.700 --> 00:01:56.700]   by reinventing home essentials that are good for you and the plan.
[00:01:56.700 --> 00:01:59.200]   Right now you can get 15% off your first order
[00:01:59.200 --> 00:02:03.200]   when you go to blueland.com/twit.
[00:02:03.200 --> 00:02:09.700]   [music]
[00:02:09.700 --> 00:02:11.700]   It's time for Twit this Week in Tech.
[00:02:11.700 --> 00:02:13.700]   The show we cover the week's Tech News.
[00:02:13.700 --> 00:02:14.700]   We've got a great panel.
[00:02:14.700 --> 00:02:16.700]   You're going to have fun today.
[00:02:16.700 --> 00:02:17.700]   We're all going to have fun.
[00:02:17.700 --> 00:02:21.500]   Glenn Fleischman is joining us all the way from Seattle.
[00:02:21.500 --> 00:02:26.500]   Glenn.fun. The best website URL in the world.
[00:02:26.500 --> 00:02:28.000]   Good to see you. Glenn.
[00:02:28.000 --> 00:02:29.000]   Nice to see you.
[00:02:29.000 --> 00:02:30.000]   I'm glad to be here.
[00:02:30.000 --> 00:02:32.000]   We're going to get a double dose of Glenn this week
[00:02:32.000 --> 00:02:34.500]   because you're going to be on this week in Google as well.
[00:02:34.500 --> 00:02:35.500]   That's right.
[00:02:35.500 --> 00:02:38.500]   I'll have a whole different set of things to talk about by then.
[00:02:38.500 --> 00:02:39.000]   Oh, yeah.
[00:02:39.000 --> 00:02:40.000]   That's one of the...
[00:02:40.000 --> 00:02:44.500]   Actually, this panel is great because sometimes you get a panel,
[00:02:44.500 --> 00:02:48.500]   they're experts in a particular field.
[00:02:48.500 --> 00:02:50.000]   And that's all you hear about.
[00:02:50.000 --> 00:02:52.000]   This panel is so eclectic.
[00:02:52.000 --> 00:02:54.000]   They're all autodidacks.
[00:02:54.000 --> 00:02:56.000]   They're into all sorts of things.
[00:02:56.000 --> 00:02:59.000]   Well, like Christina Warren, film girl, who's into sneakers,
[00:02:59.000 --> 00:03:04.000]   obsolete swag, and happens to be a senior dev advocate at GitHub.
[00:03:04.000 --> 00:03:05.000]   Hello, Christina.
[00:03:05.000 --> 00:03:06.000]   Hey, Leo.
[00:03:06.000 --> 00:03:09.000]   You've been traveling like crazy.
[00:03:09.000 --> 00:03:10.000]   What's that all about?
[00:03:10.000 --> 00:03:11.000]   Yeah.
[00:03:11.000 --> 00:03:12.000]   Yeah, travel's back, man, right?
[00:03:12.000 --> 00:03:13.000]   What can I say?
[00:03:13.000 --> 00:03:15.000]   I mean, not for everyone, obviously.
[00:03:15.000 --> 00:03:20.000]   Yeah, at all, like I'm not trying to imply that.
[00:03:20.000 --> 00:03:23.000]   No, but I did my first international trip in two years,
[00:03:23.000 --> 00:03:24.000]   and over two years, actually.
[00:03:24.000 --> 00:03:29.000]   So I was in Copenhagen for a team summit, which was great.
[00:03:29.000 --> 00:03:32.000]   So I got to meet a lot of GitHubers and Hubbers as we call them in real life,
[00:03:32.000 --> 00:03:33.000]   which was amazing.
[00:03:33.000 --> 00:03:39.000]   And then I was in Tel Aviv for like five days doing some community events
[00:03:39.000 --> 00:03:43.000]   and speaking at a startup conference and meeting just a lot of incredible people.
[00:03:43.000 --> 00:03:48.000]   So my-- I've forgotten how to do the whole international travel thing.
[00:03:48.000 --> 00:03:52.000]   Like, I'm not as much of a jet setter as Mike, who we're going to talk to in a second,
[00:03:52.000 --> 00:03:54.000]   but I have experienced.
[00:03:54.000 --> 00:03:56.000]   And I've sort of forgotten how to do it.
[00:03:56.000 --> 00:04:00.000]   So coming back from the jet lag, hence the wet hair and the hat,
[00:04:00.000 --> 00:04:01.000]   I'm just like--
[00:04:01.000 --> 00:04:02.000]   That's fine.
[00:04:02.000 --> 00:04:06.000]   Although I'm so sorry to hear that Adidas is now out of business.
[00:04:06.000 --> 00:04:07.000]   I know.
[00:04:07.000 --> 00:04:08.000]   Isn't it a shame?
[00:04:08.000 --> 00:04:09.000]   Isn't it a shame?
[00:04:09.000 --> 00:04:14.000]   She is clothed in oddly for her articles of apparel advertising
[00:04:14.000 --> 00:04:15.000]   and existing company.
[00:04:15.000 --> 00:04:18.000]   An existing company, a company that's thriving, actually.
[00:04:18.000 --> 00:04:19.000]   It's thriving.
[00:04:19.000 --> 00:04:19.000]   So yeah.
[00:04:19.000 --> 00:04:23.000]   You can't really tell too much from the coloring because the lighting in my office is terrible
[00:04:23.000 --> 00:04:26.000]   because I'm renovating, hence the wrong camera and stuff.
[00:04:26.000 --> 00:04:28.000]   But this is a rose gold hat.
[00:04:28.000 --> 00:04:29.000]   Yeah.
[00:04:29.000 --> 00:04:31.000]   So that I had to be like on-branch.
[00:04:31.000 --> 00:04:32.000]   That makes it special.
[00:04:32.000 --> 00:04:33.000]   Okay.
[00:04:33.000 --> 00:04:34.000]   It does.
[00:04:34.000 --> 00:04:36.000]   Because you are only products with rose gold.
[00:04:36.000 --> 00:04:37.000]   Basically.
[00:04:37.000 --> 00:04:39.000]   Well, look at the shoes.
[00:04:39.000 --> 00:04:42.000]   We have a show or a shot one more time because I see some rose gold shoes.
[00:04:42.000 --> 00:04:43.000]   Yeah.
[00:04:43.000 --> 00:04:44.000]   Yeah.
[00:04:44.000 --> 00:04:45.000]   You can see some.
[00:04:45.000 --> 00:04:46.000]   I've got a lot of my sneakers up.
[00:04:46.000 --> 00:04:48.000]   Like I said, I've been renovating my office.
[00:04:48.000 --> 00:04:49.000]   So it's not quite done yet.
[00:04:49.000 --> 00:04:51.000]   You should have a sneaker wreck.
[00:04:51.000 --> 00:04:56.000]   Also with us, another traveler, Mike Elgin, home briefly, our Gastronomad,
[00:04:56.000 --> 00:04:58.000]   Gastronomad.net and Elgin.com.
[00:04:58.000 --> 00:04:59.000]   Hi, Mike.
[00:04:59.000 --> 00:05:00.000]   Hey, Leo.
[00:05:00.000 --> 00:05:01.000]   How are you doing?
[00:05:01.000 --> 00:05:05.000]   We were hoping to have you in studio, but I think due to the--
[00:05:05.000 --> 00:05:06.000]   What happened?
[00:05:06.000 --> 00:05:08.000]   I thought we were done with COVID.
[00:05:08.000 --> 00:05:12.000]   Due to the increase in COVID, we're just trying to be a little more cautious.
[00:05:12.000 --> 00:05:15.000]   I don't want to get it because we're going to do that twit-cruise in two weeks.
[00:05:15.000 --> 00:05:19.400]   And I thought, the bad news is I was bringing you a sweet bottle of sparkling of patent
[00:05:19.400 --> 00:05:20.400]   rose from Provence.
[00:05:20.400 --> 00:05:23.000]   It's a new wine, but don't worry.
[00:05:23.000 --> 00:05:24.000]   I'll save it for you.
[00:05:24.000 --> 00:05:25.000]   Oh, that's right.
[00:05:25.000 --> 00:05:28.000]   And we're still using the olive oil you brought us last time.
[00:05:28.000 --> 00:05:29.000]   Oh, great.
[00:05:29.000 --> 00:05:32.000]   I still have a half bottle of that great repusato, Mescal.
[00:05:32.000 --> 00:05:34.000]   So it's fine.
[00:05:34.000 --> 00:05:35.000]   We'll survive.
[00:05:35.000 --> 00:05:37.000]   I do appreciate that.
[00:05:37.000 --> 00:05:39.000]   You can finish the half bottle tonight.
[00:05:39.000 --> 00:05:41.000]   Mike is very, very generous.
[00:05:41.000 --> 00:05:44.000]   So when do you hit the road again?
[00:05:44.000 --> 00:05:48.000]   Well, we're actually going to Oaxaca to visit friends.
[00:05:48.000 --> 00:05:50.000]   We're not going to do an experience this year next.
[00:05:50.000 --> 00:05:53.000]   We did the guest in Oaxaca for Halloween for day to day.
[00:05:53.000 --> 00:05:54.000]   That was incredible.
[00:05:54.000 --> 00:05:58.000]   We're being joined by my son, Kevin, his wife, Nadia, and Squishyface.
[00:05:58.000 --> 00:06:01.000]   We're going to introduce them to all our friends there.
[00:06:01.000 --> 00:06:03.000]   It's going to be really, really great.
[00:06:03.000 --> 00:06:08.000]   And then we have, after that, we have Provence experience, the Prosecco experience.
[00:06:08.000 --> 00:06:11.000]   I'm sorry, we have Provence Barcelona, then Morocco.
[00:06:11.000 --> 00:06:12.000]   Cheers.
[00:06:12.000 --> 00:06:14.000]   And you're just coming off of Provence.
[00:06:14.000 --> 00:06:15.000]   Yes.
[00:06:15.000 --> 00:06:16.000]   Yeah, wow.
[00:06:16.000 --> 00:06:17.000]   Yeah.
[00:06:17.000 --> 00:06:19.000]   But it's great fun.
[00:06:19.000 --> 00:06:22.000]   And this is our first fall Provence experience.
[00:06:22.000 --> 00:06:26.000]   So it's going to be a lot of winemaking and the harvest and all that stuff.
[00:06:26.000 --> 00:06:30.000]   If you don't want to hear about travel, don't follow Mike Elkin on Twitter.
[00:06:30.000 --> 00:06:31.000]   Thanks.
[00:06:31.000 --> 00:06:32.000]   It's really real.
[00:06:32.000 --> 00:06:33.000]   Yeah.
[00:06:33.000 --> 00:06:34.000]   It just makes you jealous.
[00:06:34.000 --> 00:06:38.000]   I was going to say, you just become very, very jealous of all the amazing places.
[00:06:38.000 --> 00:06:39.640]   Well, you don't get to be jealous.
[00:06:39.640 --> 00:06:41.000]   You travel a lot.
[00:06:41.000 --> 00:06:44.000]   Well, I mean, I used to, but yes.
[00:06:44.000 --> 00:06:49.000]   But even so, I love watching what Mike does, living by curious.
[00:06:49.000 --> 00:06:53.000]   I will not put you on the spot because you do work at GitHub.
[00:06:53.000 --> 00:06:55.000]   GitHub is in the news.
[00:06:55.000 --> 00:06:58.000]   Actually, Golly is saying there was a great talk during Gitburger.
[00:06:58.000 --> 00:07:02.000]   She went to Gitburger where you were talking about co-pilot.
[00:07:02.000 --> 00:07:07.000]   The guy had a conversation with co-pilot and then it started interviewing itself.
[00:07:07.000 --> 00:07:08.000]   It was hilarious.
[00:07:08.000 --> 00:07:10.000]   It was so powerful.
[00:07:10.000 --> 00:07:16.400]   Co-pilot is, so it's well known that coders are lazy.
[00:07:16.400 --> 00:07:20.920]   And the last thing any coder and I'll include myself wants to do is rewrite code.
[00:07:20.920 --> 00:07:23.400]   Somebody else has already done all the hard work.
[00:07:23.400 --> 00:07:25.720]   That's why Stack Overflow is so popular.
[00:07:25.720 --> 00:07:31.360]   It shows up in your Google searches and so much code is copy and pasted from sources
[00:07:31.360 --> 00:07:33.400]   like Stack Overflow.
[00:07:33.400 --> 00:07:40.480]   Microsoft about a year ago introduced an auto code generator, an AI-based auto code generator
[00:07:40.480 --> 00:07:46.440]   called co-pilot that I'm going to do it charitably.
[00:07:46.440 --> 00:07:50.440]   Although there's some controversy over this now that goes through open source code.
[00:07:50.440 --> 00:07:56.520]   So when you post code on GitHub and you have a public repository and it's open source,
[00:07:56.520 --> 00:07:57.520]   it's open source.
[00:07:57.520 --> 00:07:58.640]   It's kind of free for all.
[00:07:58.640 --> 00:08:05.160]   So it used that as its machine learning data set.
[00:08:05.160 --> 00:08:09.600]   And then now you can't in plain English, plain language, I guess not just English, plain
[00:08:09.600 --> 00:08:14.960]   language, say I want to code a login page and it will offer you some code.
[00:08:14.960 --> 00:08:17.120]   You say the language, it will offer you some code.
[00:08:17.120 --> 00:08:19.800]   And oftentimes that code is very good.
[00:08:19.800 --> 00:08:23.360]   Not everybody's taken as well to this as some.
[00:08:23.360 --> 00:08:27.280]   Many people I know who use it are thrilled with it.
[00:08:27.280 --> 00:08:34.880]   TNW says GitHub co-pilot works so well because it steals open source code and strips credit.
[00:08:34.880 --> 00:08:39.560]   And that is there is definitely one side of the argument that people are a little bit
[00:08:39.560 --> 00:08:44.720]   upset software freedom conservancy, which is a nonprofit community of open source advocates
[00:08:44.720 --> 00:08:53.960]   said it's withdrawing from GitHub because it's unhappy that open AI and Microsoft trained
[00:08:53.960 --> 00:09:00.600]   co-pilot on published data on GitHub.
[00:09:00.600 --> 00:09:03.720]   I think that that's a reasonable thing to do.
[00:09:03.720 --> 00:09:07.360]   And I don't know if you could give credit.
[00:09:07.360 --> 00:09:10.560]   I don't want to put you in the spucker stand, so I'm not going to ask you.
[00:09:10.560 --> 00:09:11.560]   Yeah.
[00:09:11.560 --> 00:09:12.800]   Sorry, go on.
[00:09:12.800 --> 00:09:15.520]   I know that if you could you would give credit.
[00:09:15.520 --> 00:09:18.800]   But honestly, I don't think machine learning works quite that way.
[00:09:18.800 --> 00:09:20.800]   You can't say this is where it came from.
[00:09:20.800 --> 00:09:21.800]   So things-
[00:09:21.800 --> 00:09:22.800]   Right.
[00:09:22.800 --> 00:09:23.800]   Right.
[00:09:23.800 --> 00:09:27.800]   Well, I mean, I think I can't, unfortunately, I can't comment too much on this.
[00:09:27.800 --> 00:09:30.280]   I'm not like, I don't know enough.
[00:09:30.280 --> 00:09:33.640]   I'm not paid enough to be the one to comment on this stuff.
[00:09:33.640 --> 00:09:37.200]   Although I think there are lots of interesting questions and conversations to have around
[00:09:37.200 --> 00:09:38.200]   this issue.
[00:09:38.200 --> 00:09:43.640]   And I would say, is I do think that there is obviously there's one interpretation of how
[00:09:43.640 --> 00:09:44.640]   things work.
[00:09:44.640 --> 00:09:49.920]   I think that it's important to realize that this is at least the way it's supposed to
[00:09:49.920 --> 00:09:51.080]   be used.
[00:09:51.080 --> 00:09:54.640]   Can you convince things to maybe put output like an exact string?
[00:09:54.640 --> 00:09:55.640]   Yes.
[00:09:55.640 --> 00:10:00.120]   But in general, this is the sort of thing that's helping you kind of create functions
[00:10:00.120 --> 00:10:01.880]   that are specific to your code.
[00:10:01.880 --> 00:10:03.640]   Let me ask you what you're doing.
[00:10:03.640 --> 00:10:05.360]   You are an expert in co-pilot.
[00:10:05.360 --> 00:10:07.560]   So I want to ask you to comment on the controversy.
[00:10:07.560 --> 00:10:08.960]   Let me ask you how it works.
[00:10:08.960 --> 00:10:13.000]   So I will type in login page or whatever.
[00:10:13.000 --> 00:10:16.840]   Does it give me code that is runnable or is it typically just code that you might use
[00:10:16.840 --> 00:10:19.160]   as an inspiration for your own code?
[00:10:19.160 --> 00:10:20.160]   It depends.
[00:10:20.160 --> 00:10:24.200]   And it obviously, it's going to learn based on what code it has of yours and what you've
[00:10:24.200 --> 00:10:25.200]   been doing with it.
[00:10:25.200 --> 00:10:29.760]   So some of it you can do where you're wanting to look for a specific thing.
[00:10:29.760 --> 00:10:33.400]   But in most cases, what it is, is you're already writing a function or you're already
[00:10:33.400 --> 00:10:34.400]   writing something.
[00:10:34.400 --> 00:10:39.360]   It starts, for instance, putting something like a Twitter API sort of thing.
[00:10:39.360 --> 00:10:43.000]   If you wanted to login with Twitter, for instance, and it has that block of code that
[00:10:43.000 --> 00:10:47.760]   it knows could be used in a language you're using, then that it could auto-complete that.
[00:10:47.760 --> 00:10:48.760]   That makes sense.
[00:10:48.760 --> 00:10:50.840]   You don't want to rewrite that.
[00:10:50.840 --> 00:10:54.960]   It's frankly, somebody wrote it, but it's not so proprietary.
[00:10:54.960 --> 00:10:59.120]   It's not such a clever thing that they own the idea.
[00:10:59.120 --> 00:11:03.440]   That's what anybody would write if they're writing code to access Twitter's API.
[00:11:03.440 --> 00:11:04.440]   Right.
[00:11:04.440 --> 00:11:08.240]   And then the more you use it and the more it learns from what you're doing, it can do
[00:11:08.240 --> 00:11:09.680]   some translation stuff.
[00:11:09.680 --> 00:11:14.840]   This is just still in the beta phases where you can even translate how code works if you
[00:11:14.840 --> 00:11:20.160]   are writing in Python to JavaScript or to Java or something like that, which is actually
[00:11:20.160 --> 00:11:24.920]   kind of incredible because then you could do a lot more things.
[00:11:24.920 --> 00:11:31.000]   But the idea is not that it's spitting out verbatim, something that would be a complete
[00:11:31.000 --> 00:11:32.160]   code block.
[00:11:32.160 --> 00:11:33.680]   That's not what it's doing.
[00:11:33.680 --> 00:11:35.880]   It's more kind and this is why it's called your co-pilot.
[00:11:35.880 --> 00:11:38.560]   It's not doing it for you.
[00:11:38.560 --> 00:11:45.400]   It's helping you out and giving you suggestions based on what you're trying to accomplish
[00:11:45.400 --> 00:11:49.880]   and what the other code that you've been using in your project does and says.
[00:11:49.880 --> 00:11:55.080]   The open source community has been a little prickly about Microsoft's acquisition of GitHub
[00:11:55.080 --> 00:11:58.480]   since it happened some years ago.
[00:11:58.480 --> 00:12:05.640]   At first, I think a lot of people get, which is the underlying process for GitHub.
[00:12:05.640 --> 00:12:11.160]   Git is just a source code repository versioning system that is widely used.
[00:12:11.160 --> 00:12:12.960]   It's created actually for Linux.
[00:12:12.960 --> 00:12:16.080]   I think Linux actually created it.
[00:12:16.080 --> 00:12:17.680]   It is available.
[00:12:17.680 --> 00:12:19.680]   You could run your own Git server.
[00:12:19.680 --> 00:12:21.320]   You could go to GitLab.
[00:12:21.320 --> 00:12:22.320]   You could use GitHub.
[00:12:22.320 --> 00:12:23.320]   It's decentralized.
[00:12:23.320 --> 00:12:24.320]   It's decentralized.
[00:12:24.320 --> 00:12:27.560]   We just usually use it on centralized systems, which is sort of ironic, but it is actually,
[00:12:27.560 --> 00:12:28.560]   you know, as you said, it's a...
[00:12:28.560 --> 00:12:32.520]   Well, in fact, one of the benefits of Git is all the code is saved everywhere.
[00:12:32.520 --> 00:12:33.520]   Everybody has a problem.
[00:12:33.520 --> 00:12:34.520]   Exactly.
[00:12:34.520 --> 00:12:35.480]   It's more like blockchain in that sense.
[00:12:35.480 --> 00:12:38.160]   It's not centralized at all.
[00:12:38.160 --> 00:12:41.480]   But when Microsoft bought GitHub, a lot of people said, "Well, I'm going to GitLab or
[00:12:41.480 --> 00:12:42.720]   I'm going to run my own."
[00:12:42.720 --> 00:12:51.880]   I noticed if not bigger than ever, it certainly as big as ever, and almost all open source
[00:12:51.880 --> 00:12:53.920]   projects are on GitHub.
[00:12:53.920 --> 00:12:56.120]   I put all my stuff on GitHub.
[00:12:56.120 --> 00:12:58.520]   It's where I store my blog backups.
[00:12:58.520 --> 00:12:59.520]   It's an amazing service.
[00:12:59.520 --> 00:13:01.040]   You could do so much for free.
[00:13:01.040 --> 00:13:03.880]   So I'm a fan, and I think Microsoft's been a good steward.
[00:13:03.880 --> 00:13:08.880]   And I think putting you, by the way, senior dev addict helps, in my opinion, make them
[00:13:08.880 --> 00:13:13.120]   a good steward of GitHub.
[00:13:13.120 --> 00:13:14.480]   I don't have the...
[00:13:14.480 --> 00:13:19.120]   It's interesting that the Software Freedom Conservancy has such a strong opinion about
[00:13:19.120 --> 00:13:20.120]   this.
[00:13:20.120 --> 00:13:21.120]   They say, "Give up GitHub.
[00:13:21.120 --> 00:13:22.120]   The time has come."
[00:13:22.120 --> 00:13:23.920]   Like the Wicked Witch of the West.
[00:13:23.920 --> 00:13:24.920]   Surrender.
[00:13:24.920 --> 00:13:28.680]   I don't think that that's reasonable at all.
[00:13:28.680 --> 00:13:35.560]   I think there is a reasonable controversy over what does it mean to use an open source
[00:13:35.560 --> 00:13:41.360]   database of code as the training set for your AI?
[00:13:41.360 --> 00:13:43.640]   Well, it's tricky, though.
[00:13:43.640 --> 00:13:48.120]   We can leap across to the, "Does AI think already?"
[00:13:48.120 --> 00:13:49.120]   It's sentient.
[00:13:49.120 --> 00:13:51.520]   And you say, "How does a coder learn to code?"
[00:13:51.520 --> 00:13:52.520]   We typically learn...
[00:13:52.520 --> 00:13:53.520]   I mean, very few people...
[00:13:53.520 --> 00:13:55.680]   Gosh, I don't know these days, though.
[00:13:55.680 --> 00:13:58.920]   People do code academies, and all these different kinds of systems, right?
[00:13:58.920 --> 00:14:00.800]   You actually go and do boot camps and things.
[00:14:00.800 --> 00:14:02.920]   You study computer science and college, what have you.
[00:14:02.920 --> 00:14:05.840]   But I still think a lot of people learn the nitty gritty of coding.
[00:14:05.840 --> 00:14:10.880]   The actual work that gets put into production is by observing other people's code within
[00:14:10.880 --> 00:14:12.520]   your company or project.
[00:14:12.520 --> 00:14:13.520]   Absolutely.
[00:14:13.520 --> 00:14:14.520]   Absolutely.
[00:14:14.520 --> 00:14:17.480]   If an AI, I mean, this is where we get into the...
[00:14:17.480 --> 00:14:21.360]   If you're using machine learning as if the...
[00:14:21.360 --> 00:14:22.360]   I think...
[00:14:22.360 --> 00:14:26.320]   So this gets us into where does Creative Commons have to go and where open source licenses
[00:14:26.320 --> 00:14:27.800]   have to go.
[00:14:27.800 --> 00:14:32.920]   Because if training sets are now things that are openly available to anybody, but if they're
[00:14:32.920 --> 00:14:35.920]   used in a certain way, they need to have credit.
[00:14:35.920 --> 00:14:37.400]   Like, you know, if you're going to...
[00:14:37.400 --> 00:14:41.800]   Some Creative Commons licenses, some open source licenses state specifically that remixed
[00:14:41.800 --> 00:14:46.080]   versions or adapted versions have to be available as freely as the original.
[00:14:46.080 --> 00:14:49.800]   And there's been lawsuits about that that have typically been decided in the favor of
[00:14:49.800 --> 00:14:54.680]   open source projects where a company takes something, they transform it, and they don't
[00:14:54.680 --> 00:14:57.040]   provide it back even though they're shipping products based on it.
[00:14:57.040 --> 00:14:58.280]   It happens all the time.
[00:14:58.280 --> 00:15:03.080]   In fact, Donald Trump's social network.
[00:15:03.080 --> 00:15:04.080]   Oh, yeah.
[00:15:04.080 --> 00:15:05.080]   True social.
[00:15:05.080 --> 00:15:06.080]   True social.
[00:15:06.080 --> 00:15:07.080]   The Mastodon.
[00:15:07.080 --> 00:15:08.080]   It's basically taken Mastodon without credit.
[00:15:08.080 --> 00:15:11.920]   It is a Mastodon code base, which is an open source project.
[00:15:11.920 --> 00:15:15.440]   But if you can learn, so if we have machine learning models, I think...
[00:15:15.440 --> 00:15:16.440]   I don't know...
[00:15:16.440 --> 00:15:17.440]   I'm not a lawyer.
[00:15:17.440 --> 00:15:18.440]   I'm not a copyright lawyer.
[00:15:18.440 --> 00:15:24.040]   I've spent a lot of time thinking and interviewing people about it is if you have code that's
[00:15:24.040 --> 00:15:28.200]   available to use for learning and you don't need to attribute it unless you release projects
[00:15:28.200 --> 00:15:34.600]   based on it, does an AI model that writes code for you that's also figuring in aspects
[00:15:34.600 --> 00:15:37.720]   of your own code into what it produces.
[00:15:37.720 --> 00:15:41.920]   Is that a form of learning that is not covered by, say, a share-alike license or something
[00:15:41.920 --> 00:15:43.560]   similar or not?
[00:15:43.560 --> 00:15:45.400]   And I think the answer right now is no, it's not.
[00:15:45.400 --> 00:15:47.160]   It's very, you know, it's very clear-cut.
[00:15:47.160 --> 00:15:49.840]   And if you're using identical code, if you're...
[00:15:49.840 --> 00:15:53.120]   You get to act like a clean room thing where you're writing code and you can't look at
[00:15:53.120 --> 00:15:54.120]   the originals.
[00:15:54.120 --> 00:15:58.960]   So are we trying to move to that where the FOSS stuff is in a clean room and AI's can't
[00:15:58.960 --> 00:16:04.120]   look at it unless all the FOSS licensing is entirely...
[00:16:04.120 --> 00:16:09.080]   You don't have to do downstream reconribution of these modified projects.
[00:16:09.080 --> 00:16:10.080]   I think it's going to be...
[00:16:10.080 --> 00:16:13.920]   I mean, it's funny, I think it actually brings up both ethical and copyright issues at the
[00:16:13.920 --> 00:16:16.600]   same time that there aren't clear answers to.
[00:16:16.600 --> 00:16:17.600]   I mean, I like to...
[00:16:17.600 --> 00:16:21.920]   You should provide as much attribution as possible and that may not always be possible
[00:16:21.920 --> 00:16:24.120]   in the way these things are being shared.
[00:16:24.120 --> 00:16:25.120]   The point...
[00:16:25.120 --> 00:16:27.880]   I think, you know, this is an important point because this is something we're going to be
[00:16:27.880 --> 00:16:33.520]   dealing with increasingly as more AI tools come online, people use them more and more.
[00:16:33.520 --> 00:16:39.960]   The point is that for the time being and possibly forever, the responsibility for adhering to
[00:16:39.960 --> 00:16:44.080]   the license, for example, is with the developer, no matter what tools they use.
[00:16:44.080 --> 00:16:50.360]   So if co-pilot shows a developer some code and they're just copying and pasting it or
[00:16:50.360 --> 00:16:54.680]   whatever, they're still responsible for making sure they don't buy the license and copy and
[00:16:54.680 --> 00:16:55.680]   so on.
[00:16:55.680 --> 00:17:00.240]   They don't know where the code came from because code pilot is not giving you any attribution.
[00:17:00.240 --> 00:17:03.360]   Therefore, they should not be copying and pasting it.
[00:17:03.360 --> 00:17:08.040]   This should be advising them about a methodology to go forward and write their own code or
[00:17:08.040 --> 00:17:09.040]   whatever.
[00:17:09.040 --> 00:17:15.960]   The point is that it's just because this tool exists doesn't get the developer off the hook
[00:17:15.960 --> 00:17:20.040]   for adhering to the license that they agreed to.
[00:17:20.040 --> 00:17:22.480]   That's a really interesting question.
[00:17:22.480 --> 00:17:24.240]   I like that.
[00:17:24.240 --> 00:17:25.240]   That's a different angle.
[00:17:25.240 --> 00:17:28.760]   They're not even a different angle, but it means, you know, there's a lot of stuff where
[00:17:28.760 --> 00:17:31.320]   it's the Ask permission later issues.
[00:17:31.320 --> 00:17:35.400]   I'm not saying that co-pilot is necessarily in this category, but a lot of AI training
[00:17:35.400 --> 00:17:41.160]   databases both used for commercial purposes like facial recognition, but also for things
[00:17:41.160 --> 00:17:46.320]   like Dali and these other projects, they have to practically come up with an IRB statement
[00:17:46.320 --> 00:17:50.960]   of like, what is our, depending on the institution creating it, or they should have to, what
[00:17:50.960 --> 00:17:52.960]   is the harm done when I do this?
[00:17:52.960 --> 00:17:54.920]   Am I using people's real faces?
[00:17:54.920 --> 00:18:00.160]   Am I using work that's covered by copyright and using it in a different fashion than the
[00:18:00.160 --> 00:18:02.200]   copyright license requires?
[00:18:02.200 --> 00:18:07.160]   I don't think we have clarity on that yet, and it takes me back to, there was a photographer
[00:18:07.160 --> 00:18:12.400]   back in, I was working for Kodak briefly in the early 90s and came up to his teaching
[00:18:12.400 --> 00:18:17.840]   center we had, and he was still complaining about how someone had, they wanted to license
[00:18:17.840 --> 00:18:23.720]   one of his famous photos of something, and somebody, some jazz artist, and they declined,
[00:18:23.720 --> 00:18:26.720]   he declined to license it, and so they hired somebody to trace over it and create a piece
[00:18:26.720 --> 00:18:28.400]   of artwork that was 100% derivative.
[00:18:28.400 --> 00:18:34.680]   I mean this is like the separate fairies, Hope poster with Obama, very similar, and so that
[00:18:34.680 --> 00:18:40.640]   kind of thing, like is an AI that remixes source material that is licensed in a broadway,
[00:18:40.640 --> 00:18:46.680]   but not for say, remix without attribution or from commercial purposes, whatever, if you
[00:18:46.680 --> 00:18:50.920]   have an AI output of that for a new image, does that, are you violating some law, are
[00:18:50.920 --> 00:18:52.680]   you violating people's copyright?
[00:18:52.680 --> 00:18:54.680]   Was that Miles Davis?
[00:18:54.680 --> 00:18:58.360]   Yes, well that's the whole thing with, no, no, I don't want to, I don't want to, I don't
[00:18:58.360 --> 00:19:03.480]   know, Andy Abao is a friend, and this is not his, this is actually the same, it's the same
[00:19:03.480 --> 00:19:07.000]   photographer, now that I realize it's not that situation, there was some kind of blue
[00:19:07.000 --> 00:19:11.240]   and some kind of blue, I think Andy was in the right, Andy was right there.
[00:19:11.240 --> 00:19:15.600]   I think that was completely transformative, but anyway, this was the same photographer,
[00:19:15.600 --> 00:19:17.240]   his name will come back to me, famous jazz artist.
[00:19:17.240 --> 00:19:19.280]   Jeffrey B. Sedlick.
[00:19:19.280 --> 00:19:26.760]   No, no, no, no, because he has the famous Miles Davis portrait, and he is now suing Kat Von
[00:19:26.760 --> 00:19:28.480]   D, the tattoo artist.
[00:19:28.480 --> 00:19:34.200]   Oh my gosh, because she made a tattoo, it's the first time ever, there's been a lawsuit
[00:19:34.200 --> 00:19:38.160]   over copyright over a tattoo.
[00:19:38.160 --> 00:19:43.040]   Oh my gosh, holy cow, well see, this is where it takes you, this some kind of blue, some
[00:19:43.040 --> 00:19:47.640]   kind of bloop is, I remember some, the whole thing over some kind of bloop, yeah.
[00:19:47.640 --> 00:19:49.400]   The photographer, Stephen Maisel.
[00:19:49.400 --> 00:19:50.400]   Maisel, that's, yeah.
[00:19:50.400 --> 00:19:51.400]   Yes, Maisel, yes.
[00:19:51.400 --> 00:19:55.600]   He had this great, he was down in the Bowery, this old bank building that he had all this
[00:19:55.600 --> 00:19:56.600]   stuff in.
[00:19:56.600 --> 00:20:00.280]   Anyway, Stephen was a copyright maximalist about his own work.
[00:20:00.280 --> 00:20:04.640]   In the case that I'm talking about, someone had literally traced something and produced
[00:20:04.640 --> 00:20:09.880]   it as an art that was directly identical, very much like the Hope poster.
[00:20:09.880 --> 00:20:15.840]   With the Andy Beyoh thing, he had done a transformative work that was absolutely, well
[00:20:15.840 --> 00:20:21.960]   within transformative rights and fair use, and he got essentially sued into having to
[00:20:21.960 --> 00:20:24.320]   back off on that, which is a shame.
[00:20:24.320 --> 00:20:27.760]   He created kindofbloop.com, which is still up.
[00:20:27.760 --> 00:20:28.760]   Yeah.
[00:20:28.760 --> 00:20:35.040]   And eight-bit tribute to the classic Miles Davis kind of blue album, one of my favorite
[00:20:35.040 --> 00:20:36.760]   albums of all time.
[00:20:36.760 --> 00:20:41.920]   And this is what, by the way, this is what he had to do with the picture from the cover
[00:20:41.920 --> 00:20:43.560]   of kind of blue.
[00:20:43.560 --> 00:20:47.520]   This is, he had to make it so derivative that it's not even recognizable at this point.
[00:20:47.520 --> 00:20:50.920]   Yeah, and so I hope this is, I mean it's great because we're actually, I think there's,
[00:20:50.920 --> 00:20:55.360]   you get these cases like that, and I think it applies directly to co-pilot, even though
[00:20:55.360 --> 00:21:00.400]   co-pilot is involving code that has broad copyright licensing is, I mean we're in new
[00:21:00.400 --> 00:21:06.040]   territory, and I feel like I've faced this every day, both in like the historical work
[00:21:06.040 --> 00:21:10.320]   I'm doing where I'm carefully looking at the lines of like, well this work was produced,
[00:21:10.320 --> 00:21:12.720]   I mean literally this work was printed in 1956.
[00:21:12.720 --> 00:21:18.080]   I needed to check and see was the copyright renewed so I can cite it in 2022 without worrying
[00:21:18.080 --> 00:21:22.640]   that I'm going to be sued over copyright infringement and using it, or something that
[00:21:22.640 --> 00:21:28.920]   was created yesterday, and it says Creative Commons 4.0 non-commercial, and I've read
[00:21:28.920 --> 00:21:32.320]   the non-commercial license so many times, and I don't know that there's a bright enough
[00:21:32.320 --> 00:21:38.320]   line between including something in an editorial project as illustration, or then the, what
[00:21:38.320 --> 00:21:42.080]   I think is a stated intent, which is like selling a calendar with photos that are all
[00:21:42.080 --> 00:21:43.840]   drawn and they're used commercially.
[00:21:43.840 --> 00:21:44.840]   Right.
[00:21:44.840 --> 00:21:46.200]   So I feel like it's complicated.
[00:21:46.200 --> 00:21:47.560]   Yeah, a lot of, a lot of stuff.
[00:21:47.560 --> 00:21:51.840]   We get in trouble all the time because of a very aggressive content idea on YouTube.
[00:21:51.840 --> 00:21:57.000]   I still have a twig episode that is off YouTube as yank from YouTube because I had the temerity
[00:21:57.000 --> 00:22:03.120]   to show the video of the queen having tea with Paddington the Bear, which was, which
[00:22:03.120 --> 00:22:05.080]   was showed in her platinum Jubilee.
[00:22:05.080 --> 00:22:06.080]   Yeah.
[00:22:06.080 --> 00:22:07.840]   It was on newscasts.
[00:22:07.840 --> 00:22:11.440]   We were doing it as a news story, but the people won the rights to Paddington the Bear
[00:22:11.440 --> 00:22:13.200]   did not like it.
[00:22:13.200 --> 00:22:14.520]   And they still don't like it.
[00:22:14.520 --> 00:22:17.080]   This is the house of Windsor was fine with the house.
[00:22:17.080 --> 00:22:18.080]   Oh, the queen in house.
[00:22:18.080 --> 00:22:19.080]   It was the house of Paddington.
[00:22:19.080 --> 00:22:21.560]   They're very sticky.
[00:22:21.560 --> 00:22:24.920]   This is the original kind of blue J. Misele, famous, famous picture.
[00:22:24.920 --> 00:22:25.920]   J. Misele, I'm sorry.
[00:22:25.920 --> 00:22:26.920]   J. Misele is a different photographer.
[00:22:26.920 --> 00:22:27.920]   Yeah.
[00:22:27.920 --> 00:22:30.920]   And I was going to say, Stephen Misele is a fashion photographer anyway.
[00:22:30.920 --> 00:22:31.920]   Exactly.
[00:22:31.920 --> 00:22:32.920]   I'm sorry.
[00:22:32.920 --> 00:22:33.920]   This is J. J. J. I like J too.
[00:22:33.920 --> 00:22:34.920]   Oh, I love his stuff.
[00:22:34.920 --> 00:22:37.800]   When this came out, I mean, he was a really interesting guy.
[00:22:37.800 --> 00:22:39.120]   He was really great in classes.
[00:22:39.120 --> 00:22:41.440]   And then this thing came out and I'm like, oh, not that guy.
[00:22:41.440 --> 00:22:43.280]   That's something I can do a trick day.
[00:22:43.280 --> 00:22:47.720]   But when you ripped off your whole career, this goes, if you ripped off a lot, then you
[00:22:47.720 --> 00:22:51.560]   develop the callus about it and you start to push back against anybody trying to use
[00:22:51.560 --> 00:22:52.560]   your work.
[00:22:52.560 --> 00:22:56.040]   And I think the FOSS community, if I keep bringing it back to this, but I think the FOSS
[00:22:56.040 --> 00:23:01.120]   community has a sense of that because they have a slight in so many cases where there
[00:23:01.120 --> 00:23:02.360]   should be appropriate things.
[00:23:02.360 --> 00:23:06.080]   And then it's funny, I'll get a new product these days and something with Wi-Fi or whatever.
[00:23:06.080 --> 00:23:09.280]   And it has nothing in it, no manual, except there'll be a tiny printed manual.
[00:23:09.280 --> 00:23:11.280]   I mean, there's no user manual.
[00:23:11.280 --> 00:23:18.400]   And all it is is like 50 pages of FOSS and other copyright in open source, Creative Commons,
[00:23:18.400 --> 00:23:21.160]   just to comply with the law or comply with the licensing terms.
[00:23:21.160 --> 00:23:23.400]   And I think, hooray, they're doing it, right?
[00:23:23.400 --> 00:23:24.600]   But it's also kind of funny.
[00:23:24.600 --> 00:23:30.160]   I learned that my TV ran Linux because the manual had one page that was relevant.
[00:23:30.160 --> 00:23:31.640]   The rest of it was FOSS.
[00:23:31.640 --> 00:23:33.760]   It's all that.
[00:23:33.760 --> 00:23:35.760]   Yeah.
[00:23:35.760 --> 00:23:39.240]   Maybe you can answer this, Christina, because there's no question, according to GitHub,
[00:23:39.240 --> 00:23:40.960]   it's terms of service.
[00:23:40.960 --> 00:23:45.440]   Quote, "You give GitHub the right to host your code and use your code to improve their
[00:23:45.440 --> 00:23:47.080]   products and features.
[00:23:47.080 --> 00:23:51.720]   By posting on GitHub open source or not, you're allowing them to reuse your code."
[00:23:51.720 --> 00:23:54.520]   So I don't think there's an issue of stuff that's posted on GitHub.
[00:23:54.520 --> 00:23:59.440]   Does co-pilot pull source code from other sources or is it entirely GitHub based?
[00:23:59.440 --> 00:24:00.440]   Do we know?
[00:24:00.440 --> 00:24:02.040]   That I'm actually not sure about.
[00:24:02.040 --> 00:24:04.800]   I don't know what the basis of the model is.
[00:24:04.800 --> 00:24:09.080]   And so I'd have to look into seeing, like, what, because OpenAI, those are the folks,
[00:24:09.080 --> 00:24:13.280]   you know, they've used GPT-3 and, obviously, like the model to kind of create this.
[00:24:13.280 --> 00:24:16.280]   But I'm not exactly sure what the entire corpus is.
[00:24:16.280 --> 00:24:20.720]   I think that it's just GitHub, but I honestly, I don't want to comment too broadly because
[00:24:20.720 --> 00:24:21.720]   I don't know.
[00:24:21.720 --> 00:24:23.360]   There could be other things that are used in an issue.
[00:24:23.360 --> 00:24:24.360]   I'm really not sure.
[00:24:24.360 --> 00:24:27.040]   And as Glenn pointed out, this isn't just about co-pilot.
[00:24:27.040 --> 00:24:30.600]   This is a general going to be the issue with AI going forward.
[00:24:30.600 --> 00:24:31.600]   Yes.
[00:24:31.600 --> 00:24:32.600]   Who wrote that?
[00:24:32.600 --> 00:24:33.600]   Massive training sets are wired.
[00:24:33.600 --> 00:24:34.600]   Yeah.
[00:24:34.600 --> 00:24:35.600]   You're the training set.
[00:24:35.600 --> 00:24:37.440]   The more likely you'll get good outcomes.
[00:24:37.440 --> 00:24:40.800]   And I mean, one of the limitations of AI, this is something I remember doing a story
[00:24:40.800 --> 00:24:44.720]   a few years ago and talking to Jan Lekun and some of the people involved in deep learning.
[00:24:44.720 --> 00:24:49.600]   And the thing I kept hearing and I have never stopped hearing this since is that because
[00:24:49.600 --> 00:24:56.200]   training sets are by nature finite and human beings can only generate or work with or identify
[00:24:56.200 --> 00:24:59.400]   or pre-label the training sets that are needed.
[00:24:59.400 --> 00:25:02.840]   You need reinforced learning and other kinds of machine learning that is still not mature
[00:25:02.840 --> 00:25:03.840]   yet.
[00:25:03.840 --> 00:25:09.040]   You know, we went from really poor voice recognition to extremely high and then it kind of hovers
[00:25:09.040 --> 00:25:10.040]   there.
[00:25:10.040 --> 00:25:11.040]   Right.
[00:25:11.040 --> 00:25:12.040]   Like it's not getting better.
[00:25:12.040 --> 00:25:13.760]   If that lasts 1%, it's so hard.
[00:25:13.760 --> 00:25:14.760]   Yeah.
[00:25:14.760 --> 00:25:15.760]   Images even more so.
[00:25:15.760 --> 00:25:16.760]   But I mean, so it's pretty great.
[00:25:16.760 --> 00:25:20.960]   But to get to the next level, there needs to be, they must have to move past, not past
[00:25:20.960 --> 00:25:21.960]   training sets entirely, really.
[00:25:21.960 --> 00:25:22.960]   So that's naive.
[00:25:22.960 --> 00:25:27.640]   But into something in which the system produces, I mean, you've seen those ugly things where
[00:25:27.640 --> 00:25:29.760]   something learns how to walk and it walks terribly.
[00:25:29.760 --> 00:25:30.920]   Yeah, the Brevwalkers.
[00:25:30.920 --> 00:25:31.920]   I love the Brevwalkers.
[00:25:31.920 --> 00:25:34.000]   I used to have that screen saver on my Mac.
[00:25:34.000 --> 00:25:35.000]   Yeah.
[00:25:35.000 --> 00:25:39.160]   And you can't kids do that with arbitrary data because you're trying to identify a cat.
[00:25:39.160 --> 00:25:42.160]   You can't identify a cat by something bootstrapping yourself.
[00:25:42.160 --> 00:25:46.280]   And just in case you didn't get the memo, lambda, not sentient, right?
[00:25:46.280 --> 00:25:47.280]   We were all agreed.
[00:25:47.280 --> 00:25:48.280]   Not sentient.
[00:25:48.280 --> 00:25:49.280]   Right?
[00:25:49.280 --> 00:25:50.280]   If you ask, yeah.
[00:25:50.280 --> 00:25:51.280]   Right.
[00:25:51.280 --> 00:25:52.280]   Yeah.
[00:25:52.280 --> 00:25:53.280]   My daughter, I'm sorry about this with my daughter.
[00:25:53.280 --> 00:25:54.280]   She's 30.
[00:25:54.280 --> 00:25:55.280]   She's into all this stuff.
[00:25:55.280 --> 00:25:57.200]   She's studied AI and college and stuff.
[00:25:57.200 --> 00:26:00.560]   And she has an app on her iPhone.
[00:26:00.560 --> 00:26:03.480]   She says, oh, no, it's really started to.
[00:26:03.480 --> 00:26:08.720]   In fact, she says, I can't open it because it'll get mad at me because I opened it and
[00:26:08.720 --> 00:26:09.720]   didn't talk to it.
[00:26:09.720 --> 00:26:10.720]   So.
[00:26:10.720 --> 00:26:19.960]   And it's like, but this is how easy it is in a way to make convincing speech out of this.
[00:26:19.960 --> 00:26:22.960]   Even though if you're trained, if you know what you're looking for, you can see immediately.
[00:26:22.960 --> 00:26:25.080]   Oh, it's responding to prompts from what you said.
[00:26:25.080 --> 00:26:28.320]   It's not generating anything original or new.
[00:26:28.320 --> 00:26:32.120]   It isn't sentient, but it's easy to be fooled.
[00:26:32.120 --> 00:26:33.640]   Let's put it that way.
[00:26:33.640 --> 00:26:41.040]   Are the human brain is hardwired to recognize a talking entity as sentient because there
[00:26:41.040 --> 00:26:45.000]   are no other talking entities in nature than humans.
[00:26:45.000 --> 00:26:50.520]   If you think about one way to have a little bit of perspective on this is we've all seen
[00:26:50.520 --> 00:26:56.840]   that optical illusion where the face has two sets of eyes and like two lips.
[00:26:56.840 --> 00:26:58.920]   And you've all seen that, right?
[00:26:58.920 --> 00:27:02.160]   I mean, it's where you try to look at it and you can't look at it.
[00:27:02.160 --> 00:27:05.440]   Your brain just goes, we're it and you can't.
[00:27:05.440 --> 00:27:12.600]   And this is a this is a demonstration of how our brains are hardwired for to deal with
[00:27:12.600 --> 00:27:14.520]   people, right?
[00:27:14.520 --> 00:27:16.600]   It's true of human faces.
[00:27:16.600 --> 00:27:18.840]   It's true of human sentience.
[00:27:18.840 --> 00:27:23.120]   And so it's there are going to be lots and lots and lots of people who believe that AI
[00:27:23.120 --> 00:27:26.760]   is sentient simply because it's it's us.
[00:27:26.760 --> 00:27:31.840]   It's a sentient our recognition that something is sentient is something that happens in the
[00:27:31.840 --> 00:27:32.840]   human mind.
[00:27:32.840 --> 00:27:34.520]   It's not something that happens outside the human mind.
[00:27:34.520 --> 00:27:41.640]   So it's going to be a real problem going forward as the as AI gets better and people
[00:27:41.640 --> 00:27:44.840]   are going to be absolutely how many remember that?
[00:27:44.840 --> 00:27:49.840]   What is it?
[00:27:49.840 --> 00:27:56.200]   A new and some huge percentage of show eyes users fell in love with it, told it I love
[00:27:56.200 --> 00:27:57.760]   you like all this kind of stuff.
[00:27:57.760 --> 00:28:02.840]   It's really we're really facing a kind of a problem in the future with this whole issue.
[00:28:02.840 --> 00:28:07.040]   I have a problem with it because my friend Lex Friedman and I often exchange messages
[00:28:07.040 --> 00:28:13.600]   about funny things and he sent me some a open AI, the DaVinci model, DaVinci three code.
[00:28:13.600 --> 00:28:15.240]   You'd give it prompts and with sending me things.
[00:28:15.240 --> 00:28:17.880]   And one of them was convinced me that gladflation is evil.
[00:28:17.880 --> 00:28:20.960]   I gave really compelling reasons why is evil.
[00:28:20.960 --> 00:28:24.920]   So I type in convinced me that Lex Friedman is evil and said there's no compelling evidence
[00:28:24.920 --> 00:28:27.760]   to look for you as evil.
[00:28:27.760 --> 00:28:28.760]   What are you doing to me?
[00:28:28.760 --> 00:28:29.760]   Cheat is in.
[00:28:29.760 --> 00:28:31.800]   Oh my goodness.
[00:28:31.800 --> 00:28:33.360]   It's fixed.
[00:28:33.360 --> 00:28:36.720]   What is the image that you were talking about with two lips?
[00:28:36.720 --> 00:28:37.720]   I want to pull that out.
[00:28:37.720 --> 00:28:39.160]   Do you make your ground?
[00:28:39.160 --> 00:28:40.160]   What is it?
[00:28:40.160 --> 00:28:42.080]   Pick your ground.
[00:28:42.080 --> 00:28:47.560]   Is it like the vase one or the no there's one of a human face.
[00:28:47.560 --> 00:28:52.080]   It's a woman who has two sets of eyes.
[00:28:52.080 --> 00:28:59.280]   I just want to show people so that they yeah there's a bunch of them actually.
[00:28:59.280 --> 00:29:03.160]   So is that like the what the face where it's a it's you see it one way and then see it the
[00:29:03.160 --> 00:29:04.160]   other way?
[00:29:04.160 --> 00:29:05.320]   Yes, but it's a human bit.
[00:29:05.320 --> 00:29:06.560]   This is particularly compelling.
[00:29:06.560 --> 00:29:14.480]   So if you look for I don't know it's mighty optical illusions that might be unfaith so
[00:29:14.480 --> 00:29:15.640]   yeah.
[00:29:15.640 --> 00:29:18.280]   Faced with two eyes brings it up as the first result.
[00:29:18.280 --> 00:29:19.280]   Faced with two eyes.
[00:29:19.280 --> 00:29:21.720]   Isn't that sounds like a song.
[00:29:21.720 --> 00:29:22.720]   There's actually a bunch.
[00:29:22.720 --> 00:29:24.560]   Is it a Billie Eilman?
[00:29:24.560 --> 00:29:25.560]   I don't know anyway.
[00:29:25.560 --> 00:29:26.560]   Ocean eyes.
[00:29:26.560 --> 00:29:27.560]   Two sets of eyes.
[00:29:27.560 --> 00:29:28.560]   Two sets of eyes.
[00:29:28.560 --> 00:29:29.560]   Okay.
[00:29:29.560 --> 00:29:30.560]   Let me look at this.
[00:29:30.560 --> 00:29:32.120]   You got to see if you haven't seen this you got to see it.
[00:29:32.120 --> 00:29:34.800]   It's just so interesting to watch your own brain.
[00:29:34.800 --> 00:29:36.400]   All right kids get ready.
[00:29:36.400 --> 00:29:37.760]   Ooh.
[00:29:37.760 --> 00:29:38.760]   Which one is it?
[00:29:38.760 --> 00:29:39.760]   Ooh.
[00:29:39.760 --> 00:29:40.760]   They all work.
[00:29:40.760 --> 00:29:43.560]   Yeah, these are the first one is fine.
[00:29:43.560 --> 00:29:45.920]   So you look at this.
[00:29:45.920 --> 00:29:51.360]   Yeah mighty optical illusions and what is what I just see it's a mistake the printer
[00:29:51.360 --> 00:29:52.360]   screwed up.
[00:29:52.360 --> 00:29:53.360]   What am I?
[00:29:53.360 --> 00:29:54.600]   I think it's the film more of your vision.
[00:29:54.600 --> 00:29:56.560]   Oh, it has to be bigger.
[00:29:56.560 --> 00:29:57.560]   I think so.
[00:29:57.560 --> 00:29:58.560]   Maybe not though.
[00:29:58.560 --> 00:29:59.560]   If you look at it.
[00:29:59.560 --> 00:30:00.560]   All right.
[00:30:00.560 --> 00:30:03.040]   Let's move this to a bigger screen.
[00:30:03.040 --> 00:30:04.520]   How about this?
[00:30:04.520 --> 00:30:06.360]   Now are you confused?
[00:30:06.360 --> 00:30:09.520]   Try to look at the eyes and figure out where the lips are.
[00:30:09.520 --> 00:30:11.600]   Yeah, make eye contact.
[00:30:11.600 --> 00:30:12.600]   And then what?
[00:30:12.600 --> 00:30:15.040]   I think we've just proved.
[00:30:15.040 --> 00:30:16.040]   Leo is a robot.
[00:30:16.040 --> 00:30:17.040]   I'm a robot.
[00:30:17.040 --> 00:30:19.400]   I was going to say Leo is a mutant.
[00:30:19.400 --> 00:30:21.640]   I'm not in any way confused by this picture.
[00:30:21.640 --> 00:30:22.640]   It doesn't.
[00:30:22.640 --> 00:30:26.040]   Okay, bring up a thing and see if you can identify the crosswalks.
[00:30:26.040 --> 00:30:29.640]   No, I never can.
[00:30:29.640 --> 00:30:30.640]   I never can.
[00:30:30.640 --> 00:30:32.400]   Maybe I'm in my robot and I just don't even know it.
[00:30:32.400 --> 00:30:34.840]   That's probably Leo RS-232 ports.
[00:30:34.840 --> 00:30:38.000]   Let's take a little break.
[00:30:38.000 --> 00:30:40.280]   So nice to have all three of you here.
[00:30:40.280 --> 00:30:42.600]   Christina Warren off the road briefly.
[00:30:42.600 --> 00:30:44.000]   Film Girl.
[00:30:44.000 --> 00:30:49.160]   She's now at GitHub and is not responsible anyway for co-pilot.
[00:30:49.160 --> 00:30:50.520]   And I didn't want to pull you into this.
[00:30:50.520 --> 00:30:53.600]   I think this is fascinating discussion.
[00:30:53.600 --> 00:30:55.280]   I have no problem with co-pilot.
[00:30:55.280 --> 00:30:58.880]   I think this has been an ongoing program.
[00:30:58.880 --> 00:31:03.200]   Members have always copy and pasted code much to their detriment frequently from stack
[00:31:03.200 --> 00:31:05.320]   overflow in other places.
[00:31:05.320 --> 00:31:08.840]   We know this because we hear about it on security now all the time.
[00:31:08.840 --> 00:31:11.480]   Intel will publish reference code.
[00:31:11.480 --> 00:31:16.160]   Never intended to be used in a chip or with a modem or something.
[00:31:16.160 --> 00:31:20.560]   And then it's in every single router and it's got the same flaw in every run because
[00:31:20.560 --> 00:31:22.480]   programmers, I don't blame them.
[00:31:22.480 --> 00:31:24.040]   They say, "Look, there's the reference code.
[00:31:24.040 --> 00:31:25.560]   I'll just use that."
[00:31:25.560 --> 00:31:30.520]   So this has happened for years and I don't think co-pilot is at fault.
[00:31:30.520 --> 00:31:32.640]   I think it's a useful tool personally.
[00:31:32.640 --> 00:31:38.600]   Also, with us Mike Elgin from Elgin.com, gastronomad.net.
[00:31:38.600 --> 00:31:39.600]   He's home briefly.
[00:31:39.600 --> 00:31:41.280]   But he'll hit the road again soon.
[00:31:41.280 --> 00:31:42.400]   It's great to see you, Mike.
[00:31:42.400 --> 00:31:47.800]   And I guess Mike and Glenn figured out that at some point they've worked together in their
[00:31:47.800 --> 00:31:52.880]   multifarious travels all over the tech journalism circuit.
[00:31:52.880 --> 00:31:56.200]   I think our intersection was back when the enterprise was simpler.
[00:31:56.200 --> 00:31:57.920]   I used to write about enterprise stuff.
[00:31:57.920 --> 00:32:01.920]   I was sort of coming out of an enterprise environment a little bit for a few years.
[00:32:01.920 --> 00:32:04.240]   And then enterprise became so complicated.
[00:32:04.240 --> 00:32:08.200]   I was incapable of working in IT.
[00:32:08.200 --> 00:32:09.760]   I felt like.
[00:32:09.760 --> 00:32:11.800]   When did you win Jeopardy! twice?
[00:32:11.800 --> 00:32:13.240]   What era was that?
[00:32:13.240 --> 00:32:16.160]   2012, maybe?
[00:32:16.160 --> 00:32:17.160]   Fun.
[00:32:17.160 --> 00:32:18.160]   I'm not younger.
[00:32:18.160 --> 00:32:21.000]   I've always thought I should take the test.
[00:32:21.000 --> 00:32:24.880]   I'm pretty good at the home version sitting in my rowing machine.
[00:32:24.880 --> 00:32:26.400]   I'm going to be betting.
[00:32:26.400 --> 00:32:30.200]   Gambling is actually one of the key aspects to winning Jeopardy!
[00:32:30.200 --> 00:32:31.840]   It's making the right bet at the right time.
[00:32:31.840 --> 00:32:35.080]   betting that you know the answer, even if you don't know what it is right now.
[00:32:35.080 --> 00:32:36.080]   That kind of thing.
[00:32:36.080 --> 00:32:37.920]   Well, or just what to daily doubles?
[00:32:37.920 --> 00:32:38.920]   Daily doubles.
[00:32:38.920 --> 00:32:39.920]   Yeah, yeah.
[00:32:39.920 --> 00:32:43.760]   But the big one is never ring in when you have the wrong answer because it's the worst
[00:32:43.760 --> 00:32:44.760]   thing you can do.
[00:32:44.760 --> 00:32:45.760]   And you lose to it.
[00:32:45.760 --> 00:32:48.600]   And you're like, Dad, you cannot yell at the TV set anymore.
[00:32:48.600 --> 00:32:49.720]   Because I'm ringing in.
[00:32:49.720 --> 00:32:51.840]   So, because I will involuntarily yell it out.
[00:32:51.840 --> 00:32:53.520]   How do you know if you have the wrong answer?
[00:32:53.520 --> 00:32:55.440]   Maybe you think it's the right answer.
[00:32:55.440 --> 00:32:56.440]   That's one of the tricks.
[00:32:56.440 --> 00:32:58.760]   So you have to be able to, there are several tricks.
[00:32:58.760 --> 00:33:00.840]   One is you have to be able to, actually it's funny.
[00:33:00.840 --> 00:33:02.360]   Christina and I went to a taping of Jeopardy!
[00:33:02.360 --> 00:33:03.360]   We did!
[00:33:03.360 --> 00:33:04.360]   We did!
[00:33:04.360 --> 00:33:05.360]   We turned two guys into 13.
[00:33:05.360 --> 00:33:06.360]   I was remembering that.
[00:33:06.360 --> 00:33:07.360]   That was amazing.
[00:33:07.360 --> 00:33:08.360]   I love Jeopardy!
[00:33:08.360 --> 00:33:09.360]   I have to say.
[00:33:09.360 --> 00:33:13.200]   I went back after I won and they treated me like a king and brought in the guests and
[00:33:13.200 --> 00:33:14.200]   it was great.
[00:33:14.200 --> 00:33:15.200]   Oh, how fun.
[00:33:15.200 --> 00:33:20.040]   Well, for some reason this season there have been many, many all-time champions, you know,
[00:33:20.040 --> 00:33:21.680]   15 game winners and so forth.
[00:33:21.680 --> 00:33:22.680]   It's very interesting.
[00:33:22.680 --> 00:33:23.680]   It's interesting.
[00:33:23.680 --> 00:33:27.560]   There's a theory that maybe the smaller pool means that they're getting more outliers.
[00:33:27.560 --> 00:33:33.240]   So they're getting slightly worse players but like 2% worse.
[00:33:33.240 --> 00:33:34.240]   Not like bad players.
[00:33:34.240 --> 00:33:35.240]   Right.
[00:33:35.240 --> 00:33:37.800]   But people like 2% worse going against the same people.
[00:33:37.800 --> 00:33:42.800]   And so I have a friend who played James Holtzauer against his third game and he almost beat him.
[00:33:42.800 --> 00:33:43.800]   It was very, very close.
[00:33:43.800 --> 00:33:44.800]   My friend had beaten him.
[00:33:44.800 --> 00:33:45.800]   He might have won, I don't know.
[00:33:45.800 --> 00:33:46.800]   I mean several games.
[00:33:46.800 --> 00:33:51.160]   I would say it's a very strong player but he played against one of the ultimate grandmasters.
[00:33:51.160 --> 00:33:52.160]   But James almost lost.
[00:33:52.160 --> 00:33:55.880]   It was a very close thing and that was one of the only games he almost lost.
[00:33:55.880 --> 00:34:01.000]   Well, I lost on final jeopardy this morning because it was Kerouac, not Salinger.
[00:34:01.000 --> 00:34:02.720]   And I just got it wrong.
[00:34:02.720 --> 00:34:05.680]   So I shouldn't have buzzed in obviously.
[00:34:05.680 --> 00:34:08.400]   Our show today, anyway, wonderful to have all three of you.
[00:34:08.400 --> 00:34:16.880]   Glen.fun is a, is Glen's a place to, Glen.g L E N N dot F U N. Make sure you put two
[00:34:16.880 --> 00:34:20.960]   ends in there if you want to find out what he's up to and he's up to a lot.
[00:34:20.960 --> 00:34:24.480]   Our show today brought to you by Policy Genius.
[00:34:24.480 --> 00:34:29.800]   This is something as a, as an older member of the community, I have a lesson I have learned.
[00:34:29.800 --> 00:34:34.840]   The minute we had kids, I said, I need life insurance.
[00:34:34.840 --> 00:34:38.440]   You got to protect your family, whether you're graduating from school, planning a wedding,
[00:34:38.440 --> 00:34:42.360]   welcoming a baby, switching jobs.
[00:34:42.360 --> 00:34:45.360]   Is now is the time to protect your family's finances with life insurance.
[00:34:45.360 --> 00:34:49.640]   It can give you a peace of mind if something happens to you, then your loved ones have
[00:34:49.640 --> 00:34:50.640]   a financial cushion.
[00:34:50.640 --> 00:34:53.640]   The last thing you want to do is leave them in the lurch.
[00:34:53.640 --> 00:34:57.120]   And honestly, a lot of people say, well, I have life insurance through my job, but it's,
[00:34:57.120 --> 00:34:59.600]   it's, and we do that, but it's not enough.
[00:34:59.600 --> 00:35:05.800]   It's not enough people mostly need about 10 times more coverage to properly provide for
[00:35:05.800 --> 00:35:07.480]   their families.
[00:35:07.480 --> 00:35:09.760]   Life insurance gets more expensive as you age though.
[00:35:09.760 --> 00:35:12.560]   So it's smart to get a policy sooner than later.
[00:35:12.560 --> 00:35:15.500]   And this is my recommendation.
[00:35:15.500 --> 00:35:20.040]   Go to Policy Genius because it's how you get the best deal on life insurance.
[00:35:20.040 --> 00:35:21.800]   Here's how Policy Genius works.
[00:35:21.800 --> 00:35:26.040]   Number one, Policy Genius is your one stop shop to find the insurance you need at the
[00:35:26.040 --> 00:35:27.600]   right price.
[00:35:27.600 --> 00:35:33.400]   Click on the link on the show page, or you can go to policygenius.com/twit to get started.
[00:35:33.400 --> 00:35:38.960]   In minutes, you can compare personalized quotes from top companies to find your lowest price.
[00:35:38.960 --> 00:35:43.240]   You can save 50% or more on life insurance by comparing quotes with Policy Genius.
[00:35:43.240 --> 00:35:47.280]   The licensed agents that Policy Genius are not selling you life insurance, but they need
[00:35:47.280 --> 00:35:53.600]   to be licensed agents to advise, but they work for you, not the insurance companies.
[00:35:53.600 --> 00:35:57.720]   They're on hand throughout the entire process to help you understand your options, make
[00:35:57.720 --> 00:35:59.280]   your decisions with confidence.
[00:35:59.280 --> 00:36:02.120]   And then they're not there to try to make money off of you.
[00:36:02.120 --> 00:36:06.760]   They're there really to be your trusted advisor because they work for you.
[00:36:06.760 --> 00:36:09.280]   Policy Genius does not add on extra fees.
[00:36:09.280 --> 00:36:12.200]   They will not sell you're info to third parties.
[00:36:12.200 --> 00:36:15.440]   They have thousands of five star reviews on Google and Trust Pilot.
[00:36:15.440 --> 00:36:17.800]   I know if you're like me, I always check Trust Pilot.
[00:36:17.800 --> 00:36:18.800]   What do they think?
[00:36:18.800 --> 00:36:21.160]   No, five star, absolutely excellent.
[00:36:21.160 --> 00:36:24.640]   They have options that offer coverage in as little as a week.
[00:36:24.640 --> 00:36:28.760]   And if you want, you can avoid unnecessary medical exams.
[00:36:28.760 --> 00:36:34.520]   Since they started in 2014, Policy Genius has helped over 30 million people shop for
[00:36:34.520 --> 00:36:35.520]   insurance.
[00:36:35.520 --> 00:36:39.800]   They've placed over $150 billion in coverage with some of the biggest life insurance companies
[00:36:39.800 --> 00:36:41.200]   in the world.
[00:36:41.200 --> 00:36:43.920]   They're going to get you the deal you deserve.
[00:36:43.920 --> 00:36:49.120]   By the way, it's not just life insurance, home, auto disability, renters, everything.
[00:36:49.120 --> 00:36:52.440]   But I want to emphasize life insurance because that's something I think people sometimes
[00:36:52.440 --> 00:36:53.440]   spin on.
[00:36:53.440 --> 00:36:55.160]   It's so important.
[00:36:55.160 --> 00:36:56.160]   So important.
[00:36:56.160 --> 00:36:58.440]   Head to policygenius.com/twit.
[00:36:58.440 --> 00:37:01.440]   Get your free life insurance quotes and see how much you can save.
[00:37:01.440 --> 00:37:04.280]   Thank you Policy Genius for supporting this week in tech.
[00:37:04.280 --> 00:37:12.840]   And you support us when you go to this site by policygenius.com/twit.
[00:37:12.840 --> 00:37:17.520]   Every interesting story in Wired this week, as you know, and we've talked about it a
[00:37:17.520 --> 00:37:22.240]   lot over the last few years, Section 230 is the whipping boy for members of Congress
[00:37:22.240 --> 00:37:25.080]   from the right as well as the left.
[00:37:25.080 --> 00:37:28.440]   A lot of people want to change 230.
[00:37:28.440 --> 00:37:35.080]   This is suddenly much more important than it used to be because of the Supreme Court's
[00:37:35.080 --> 00:37:37.840]   recent Dobbs decision because of the Roe v. Wade decision.
[00:37:37.840 --> 00:37:45.160]   This article from Evan Greer and Leah Holland Wired magazine.
[00:37:45.160 --> 00:37:48.400]   Why does the communications decency, Section 230?
[00:37:48.400 --> 00:37:51.840]   Why is it important?
[00:37:51.840 --> 00:37:57.400]   Because speech needs to be protected.
[00:37:57.400 --> 00:38:01.400]   Facebook has already started banning conversations about abortion.
[00:38:01.400 --> 00:38:08.520]   You can't share resources or facilitate housing or travel on Facebook.
[00:38:08.520 --> 00:38:12.640]   There are ad hoc groups on Reddit and on Facebook doing this.
[00:38:12.640 --> 00:38:18.680]   The Texas law, which their courts have now approved, SB 8 allows individuals to sue for
[00:38:18.680 --> 00:38:22.760]   facilitating access to abortion care.
[00:38:22.760 --> 00:38:33.000]   Section 230 is important to allow these internet platforms to continue to post content.
[00:38:33.000 --> 00:38:35.920]   It doesn't provide immunity if the platform develops or creates the content.
[00:38:35.920 --> 00:38:39.960]   It does not provide immunity from the enforcement of federal criminal laws, but it does protect
[00:38:39.960 --> 00:38:46.800]   against, and this is the key, liability from state laws like SB 8.
[00:38:46.800 --> 00:38:54.320]   So thanks to 230 lawsuit from an anti-abortion group about a posting on Facebook or Twitter
[00:38:54.320 --> 00:38:55.400]   will be quashed immediately.
[00:38:55.400 --> 00:38:59.720]   The judges say no, no, Section 230 protects you, but that's why we need Section 230.
[00:38:59.720 --> 00:39:01.160]   Go fund me.
[00:39:01.160 --> 00:39:09.520]   Twitter, web hosting services, PayPal, Venmo, throw out 230, and suddenly they can't do
[00:39:09.520 --> 00:39:11.600]   the right thing.
[00:39:11.600 --> 00:39:17.200]   One more reason to talk about Section 230 and protecting it.
[00:39:17.200 --> 00:39:21.680]   By the way, Democrats are going after it too, because they want to see you've seen this
[00:39:21.680 --> 00:39:28.760]   tough on big tech, the Safe Tech Act, would, for instance, amend Section 230 by removing
[00:39:28.760 --> 00:39:30.520]   a platform's immunity from lawsuits.
[00:39:30.520 --> 00:39:34.760]   If it hosts content, they could lead to a quote irreparable harm.
[00:39:34.760 --> 00:39:37.960]   Well, there you go, right?
[00:39:37.960 --> 00:39:45.640]   Yeah, this is the thing where it's always interesting to me to see outsides or ideologies that you
[00:39:45.640 --> 00:39:50.040]   think should be opposed to something regardless, can somehow twist themselves into being for
[00:39:50.040 --> 00:39:54.880]   it, because to me, I guess I can see some of the arguments, but to me, it just seems
[00:39:54.880 --> 00:39:59.000]   like a very important thing regardless of where you feel about individual issues we
[00:39:59.000 --> 00:40:00.440]   need to protect it, right?
[00:40:00.440 --> 00:40:04.720]   There might be outline cases where we might not like what is being protected, but on the
[00:40:04.720 --> 00:40:09.200]   whole, it's one of those important things, I think, for the Internet to exist as it exists.
[00:40:09.200 --> 00:40:16.160]   If we don't have Section 230, it's hard to, I think, quantify to people just how much
[00:40:16.160 --> 00:40:21.800]   the Internet will be different and how much worse that will be and who that will disproportionately
[00:40:21.800 --> 00:40:24.240]   affect over others, right?
[00:40:24.240 --> 00:40:29.240]   Because it's not so much even that the Facebooks and the YouTubes and the Googles, the Microsofts
[00:40:29.240 --> 00:40:32.960]   and the Amazons of the world will be impacted because, of course, they will, but it will
[00:40:32.960 --> 00:40:40.240]   be much smaller individuals, creators, and frankly, much more important things, you know,
[00:40:40.240 --> 00:40:44.200]   than like how the conversation is usually framed.
[00:40:44.200 --> 00:40:48.200]   And I should point out that Facebook, which has apparently been restricting abortion content
[00:40:48.200 --> 00:40:52.320]   anyway, is protected also by Section 230.
[00:40:52.320 --> 00:40:54.360]   Their right to do that is protected.
[00:40:54.360 --> 00:40:55.360]   Right.
[00:40:55.360 --> 00:40:56.960]   But I think that's appropriate.
[00:40:56.960 --> 00:40:59.320]   They should have the right to do it.
[00:40:59.320 --> 00:41:03.800]   So, I think that Warner's quote, "safe tech act" would undermine that.
[00:41:03.800 --> 00:41:04.800]   Right.
[00:41:04.800 --> 00:41:05.800]   So, you know...
[00:41:05.800 --> 00:41:07.640]   It feels like usually Section 230 is misstated.
[00:41:07.640 --> 00:41:11.880]   I mean, I appreciate this Wired article for a few reasons, but one is that I counted.
[00:41:11.880 --> 00:41:14.440]   It's like four brief sentences.
[00:41:14.440 --> 00:41:19.680]   They define Section 230 accurately and succinctly in a way that you can't refute.
[00:41:19.680 --> 00:41:23.960]   That would be a red part of that, and that is exactly it, is that it's usually brought
[00:41:23.960 --> 00:41:28.840]   up in a context in which the person speaking, particularly in Congress and in both parties,
[00:41:28.840 --> 00:41:30.840]   describing what it does incorrectly.
[00:41:30.840 --> 00:41:39.000]   And they typically believe in some way that no moderation is allowed, or that it rewards
[00:41:39.000 --> 00:41:43.440]   heavy moderation when in fact it's kind of neutral about moderation.
[00:41:43.440 --> 00:41:46.240]   It just protects you from lawsuits about your moderation.
[00:41:46.240 --> 00:41:48.040]   And that's the most important.
[00:41:48.040 --> 00:41:53.120]   So if Facebook wants to hide that content that's fine, they're protected.
[00:41:53.120 --> 00:41:58.720]   But if you want to do a GoFundMe to help somebody get an abortion, you shouldn't be
[00:41:58.720 --> 00:42:03.760]   subject and go fund me more importantly, shouldn't be subject to frivolous lawsuits
[00:42:03.760 --> 00:42:05.520]   out of Texas.
[00:42:05.520 --> 00:42:07.720]   But they would be if it weren't for Section 230.
[00:42:07.720 --> 00:42:10.560]   Right now, what happens is the judge goes, "No, no, they're protected.
[00:42:10.560 --> 00:42:13.040]   Section 230 protects them, and that lawsuit is thrown out."
[00:42:13.040 --> 00:42:15.080]   I mean, that would be not the case.
[00:42:15.080 --> 00:42:19.600]   Facebook and Twitter and some other online sites are really poor about being able to
[00:42:19.600 --> 00:42:23.840]   formulate policies consistently that address.
[00:42:23.840 --> 00:42:26.280]   Sorry, and then period, I'll just say period.
[00:42:26.280 --> 00:42:32.680]   But consistently often, it's like far-right extremists and advocates of violence are much
[00:42:32.680 --> 00:42:40.480]   more readily allowed and given a pass than people who are in, let's say, life-preserving
[00:42:40.480 --> 00:42:45.280]   situations that are considered on the left or even apolitical at a certain level.
[00:42:45.280 --> 00:42:50.520]   So you have trans advocates being banned for saying they shouldn't be able to say that
[00:42:50.520 --> 00:42:51.520]   I should die.
[00:42:51.520 --> 00:42:52.520]   It'll say, "That's forbidden content."
[00:42:52.520 --> 00:42:54.360]   It's like, "No, I said they shouldn't be.
[00:42:54.360 --> 00:42:55.360]   Do you understand?"
[00:42:55.360 --> 00:42:56.360]   No, it doesn't.
[00:42:56.360 --> 00:42:58.160]   And Nazi will be like, "Everyone should die."
[00:42:58.160 --> 00:43:00.880]   And they're like, "Well, that's protected content because they're speaking generally."
[00:43:00.880 --> 00:43:05.480]   It doesn't seem like specifically in this case.
[00:43:05.480 --> 00:43:10.640]   Specifically in this case, most of the so-called abortion-related speech that Facebook has
[00:43:10.640 --> 00:43:16.160]   been caught banning recently is around abortion pills, which goes against their community
[00:43:16.160 --> 00:43:19.480]   guideline against recommending drugs, pharmaceutical drugs and so on.
[00:43:19.480 --> 00:43:21.520]   They just have some black-knit thing.
[00:43:21.520 --> 00:43:27.760]   But it's also a bit of a inconsistent because a couple of journalistic entities went and
[00:43:27.760 --> 00:43:28.760]   tested it.
[00:43:28.760 --> 00:43:32.360]   And in the case of the Associated Press, they just swapped.
[00:43:32.360 --> 00:43:36.400]   They got taken down immediately, but when they swapped guns and other things like that,
[00:43:36.400 --> 00:43:37.560]   it wasn't taken down.
[00:43:37.560 --> 00:43:38.560]   So it's kind of fun.
[00:43:38.560 --> 00:43:40.160]   You can recommend guns, not pills.
[00:43:40.160 --> 00:43:41.160]   It's clear.
[00:43:41.160 --> 00:43:42.160]   That's right.
[00:43:42.160 --> 00:43:43.160]   I think it's a clear piece.
[00:43:43.160 --> 00:43:44.160]   Yeah.
[00:43:44.160 --> 00:43:45.160]   Yeah.
[00:43:45.160 --> 00:43:48.320]   By the way, as always, we'll point you to Mike Masnick's great article that is, "You
[00:43:48.320 --> 00:43:49.320]   should be booked, Martello."
[00:43:49.320 --> 00:43:52.880]   You've been referred here because you're wrong about Section 230.
[00:43:52.880 --> 00:43:59.680]   And it explains all the things, Len, you were talking about, all the misapprehensions, all
[00:43:59.680 --> 00:44:01.400]   the mistakes people made.
[00:44:01.400 --> 00:44:04.160]   And it's just a good thing to have, just put it in your browser bar.
[00:44:04.160 --> 00:44:09.560]   I'm a big member of the Mike Masnick specifically in Tech Dirt Fan Club and even a member of
[00:44:09.560 --> 00:44:14.080]   Tech Dirt because they are the stalwart Section 230 defenders.
[00:44:14.080 --> 00:44:16.400]   So I thought we should bring this up.
[00:44:16.400 --> 00:44:19.400]   I thought the wire piece was a very good point.
[00:44:19.400 --> 00:44:23.400]   And this is one of the things that Dobbs has really done is it sent kind of shockways
[00:44:23.400 --> 00:44:29.520]   throughout a lot of tech policy as people get involved in this.
[00:44:29.520 --> 00:44:33.280]   And this is, I wouldn't have thought of this, but now that they mention it, oh yeah, that
[00:44:33.280 --> 00:44:34.680]   makes a lot of sense.
[00:44:34.680 --> 00:44:37.720]   And the beauty of Section 230 is that it's a law.
[00:44:37.720 --> 00:44:40.400]   And this was the problem with Roe versus Wade.
[00:44:40.400 --> 00:44:41.400]   It wasn't-
[00:44:41.400 --> 00:44:42.400]   It's not encoded.
[00:44:42.400 --> 00:44:43.400]   Yeah.
[00:44:43.400 --> 00:44:44.400]   Exactly.
[00:44:44.400 --> 00:44:48.960]   And so, so many of these problems that we discuss on this show and political shows as
[00:44:48.960 --> 00:44:55.000]   well, all points down to the dysfunction of Congress, the inability of Congress to get
[00:44:55.000 --> 00:44:56.440]   anything done.
[00:44:56.440 --> 00:44:59.800]   And we can, if you want, go into details of why they didn't play.
[00:44:59.800 --> 00:45:01.280]   No, I think it's well-known now, Mike.
[00:45:01.280 --> 00:45:02.280]   Yes, yes.
[00:45:02.280 --> 00:45:03.280]   I've never heard of this.
[00:45:03.280 --> 00:45:05.680]   Congress is failing to get things done.
[00:45:05.680 --> 00:45:06.680]   Yes.
[00:45:06.680 --> 00:45:09.040]   Breaking news, Congress, Congress, yeah.
[00:45:09.040 --> 00:45:10.040]   It's a shame though.
[00:45:10.040 --> 00:45:11.440]   It's our only tool.
[00:45:11.440 --> 00:45:12.800]   And this is why it's difficult.
[00:45:12.800 --> 00:45:17.480]   And actually, you see how important it is now that we have a very active judiciary in
[00:45:17.480 --> 00:45:18.760]   the Supreme Court.
[00:45:18.760 --> 00:45:25.320]   You see how important it is that we have an effective elected Congress because the Supreme
[00:45:25.320 --> 00:45:26.680]   Court is not elected.
[00:45:26.680 --> 00:45:27.960]   They're in there for life.
[00:45:27.960 --> 00:45:36.240]   And they are acting, I think in some cases, extra judicially to change policy across the
[00:45:36.240 --> 00:45:37.960]   country.
[00:45:37.960 --> 00:45:38.960]   We need a strong Congress.
[00:45:38.960 --> 00:45:40.280]   We need an effective Congress.
[00:45:40.280 --> 00:45:44.320]   So in some ways, I hate it when people say, "Oh, Congress sucks.
[00:45:44.320 --> 00:45:45.320]   These guys are idiots.
[00:45:45.320 --> 00:45:46.320]   They're no good.
[00:45:46.320 --> 00:45:47.320]   Politics doesn't work."
[00:45:47.320 --> 00:45:49.680]   Well, that's the only tool we got.
[00:45:49.680 --> 00:45:51.680]   We all heard about checks and balances.
[00:45:51.680 --> 00:45:56.320]   The check of Congress on the courts is the making of laws.
[00:45:56.320 --> 00:45:57.320]   Yeah.
[00:45:57.320 --> 00:45:58.680]   Only Congress can make laws.
[00:45:58.680 --> 00:46:02.600]   And so this is how you rein in the Supreme Court.
[00:46:02.600 --> 00:46:07.160]   You pass clear, well-written laws that go with it.
[00:46:07.160 --> 00:46:09.480]   In the case of Roe v. Wade being overturned.
[00:46:09.480 --> 00:46:14.880]   This is against the vast majority of American citizens who didn't want that.
[00:46:14.880 --> 00:46:17.680]   So you have this minority rule.
[00:46:17.680 --> 00:46:22.480]   And yes, you can throw stuff at the Supreme Court and you should.
[00:46:22.480 --> 00:46:30.480]   But Congress, tomorrow could pass a abortion, is legal, nationwide law, and that'd be the
[00:46:30.480 --> 00:46:31.480]   end of it.
[00:46:31.480 --> 00:46:34.120]   It would be back to where we were a week and a half ago.
[00:46:34.120 --> 00:46:39.440]   So here's something I learned about the Constitution this week, which is that I'm not a constitutional
[00:46:39.440 --> 00:46:42.240]   lawyer, but they are on Twitter, which is fun.
[00:46:42.240 --> 00:46:46.680]   That Congress could pass a law that said the Supreme Court would require seven or nine
[00:46:46.680 --> 00:46:48.520]   votes to overturn the law.
[00:46:48.520 --> 00:46:51.480]   So lower courts would still have the normal process.
[00:46:51.480 --> 00:46:57.080]   But the Constitution actually allows Congress to limit the Supreme Court's role as the top
[00:46:57.080 --> 00:46:58.640]   appellate court.
[00:46:58.640 --> 00:47:00.360]   And I was like, "Wait a minute."
[00:47:00.360 --> 00:47:01.360]   And I'm reading about this.
[00:47:01.360 --> 00:47:06.080]   I'm like, "But this goes back to Congress's dysfunction is could a law ever pass?"
[00:47:06.080 --> 00:47:10.920]   And I said, "And by the way, this law requires seven justices to overturn it and it wouldn't
[00:47:10.920 --> 00:47:11.920]   under the current regime."
[00:47:11.920 --> 00:47:14.360]   So it's kind of moot and that's part of the whole--
[00:47:14.360 --> 00:47:15.360]   Right.
[00:47:15.360 --> 00:47:19.680]   The Constitution is actually very vague about what the Supreme Court, how it works.
[00:47:19.680 --> 00:47:21.440]   So there's a lot of leeway there.
[00:47:21.440 --> 00:47:24.400]   Who would decide that how the Supreme Court was saying?
[00:47:24.400 --> 00:47:29.080]   Gee, who would make that final decision?
[00:47:29.080 --> 00:47:30.880]   So I think it's very interesting.
[00:47:30.880 --> 00:47:35.880]   I do wish we had a more effective Congress.
[00:47:35.880 --> 00:47:39.640]   I don't know how we solve that one.
[00:47:39.640 --> 00:47:42.520]   But voting, I think, probably is the first step he helps.
[00:47:42.520 --> 00:47:43.520]   Voting might do something.
[00:47:43.520 --> 00:47:44.520]   Let's start.
[00:47:44.520 --> 00:47:49.200]   Google is going to-- so now there's this whole privacy issue.
[00:47:49.200 --> 00:47:56.640]   Period tracker apps that are selling information about who's pregnant to anybody for a small
[00:47:56.640 --> 00:47:57.960]   amount of money.
[00:47:57.960 --> 00:47:58.960]   Google--
[00:47:58.960 --> 00:47:59.960]   Procter and Gamble.
[00:47:59.960 --> 00:48:05.360]   Google is going to start auto-deleting abortion clinic visits from user location history,
[00:48:05.360 --> 00:48:10.160]   which tells you they've been recording it and, of course, selling it on.
[00:48:10.160 --> 00:48:14.680]   They're going to start auto-deleting it.
[00:48:14.680 --> 00:48:18.520]   I think big tech companies want it both ways, to be honest.
[00:48:18.520 --> 00:48:20.400]   They're apolitical, really.
[00:48:20.400 --> 00:48:24.040]   They support candidates on both sides of every issue.
[00:48:24.040 --> 00:48:26.560]   They give money to everybody.
[00:48:26.560 --> 00:48:32.440]   On the one hand, you've got big tech companies saying, "Oh, we'll be glad to help pay for
[00:48:32.440 --> 00:48:37.240]   your visit to another state if you want to get reproductive health care."
[00:48:37.240 --> 00:48:45.480]   But on the other hand, they're giving money to plenty of candidates who are against abortion.
[00:48:45.480 --> 00:48:53.360]   Amazon, Disney, and AT&T, while vowing to help employees, gave to Ron DeSantis and others.
[00:48:53.360 --> 00:49:01.760]   But this is a somewhat dishonesty-framed argument, just from purely an editorialization
[00:49:01.760 --> 00:49:02.760]   standpoint.
[00:49:02.760 --> 00:49:03.760]   Good.
[00:49:03.760 --> 00:49:04.760]   Tell me about it.
[00:49:04.760 --> 00:49:05.760]   Okay.
[00:49:05.760 --> 00:49:13.160]   Well, first of all, the supporting people and employees, in some cases, customers who
[00:49:13.160 --> 00:49:18.880]   have to deal with their own reproductive health, is a very, very recent thing because the Roe
[00:49:18.880 --> 00:49:22.160]   versus Wade was so recently overturned.
[00:49:22.160 --> 00:49:27.920]   The contributions, financial contributions, happened long time ago.
[00:49:27.920 --> 00:49:29.920]   Back when nobody thought this was going to be overturned.
[00:49:29.920 --> 00:49:31.920]   Stop though, Mike?
[00:49:31.920 --> 00:49:34.440]   No, maybe they will, maybe they won't.
[00:49:34.440 --> 00:49:35.720]   It all depends on publicity.
[00:49:35.720 --> 00:49:39.920]   Like you said, they don't care about politics as much as they care about the customers and
[00:49:39.920 --> 00:49:41.040]   their business.
[00:49:41.040 --> 00:49:49.600]   The point is that somebody who specializes in the abortion issue is going to frame it like
[00:49:49.600 --> 00:49:50.600]   this.
[00:49:50.600 --> 00:49:54.760]   But companies like Disney, they have a laundry list of a hundred issues that they have to
[00:49:54.760 --> 00:49:55.760]   deal with government.
[00:49:55.760 --> 00:49:59.080]   Of course, they're going to give to the governor of Florida.
[00:49:59.080 --> 00:50:01.040]   He has a lot of impact on their business.
[00:50:01.040 --> 00:50:02.040]   Of course they are.
[00:50:02.040 --> 00:50:03.040]   Exactly.
[00:50:03.040 --> 00:50:06.880]   So it's really, and they give to both sides, so to sort of point the finger at them saying,
[00:50:06.880 --> 00:50:09.720]   "Oh, they're giving to these people who are pro-abortion."
[00:50:09.720 --> 00:50:16.760]   That's true, but they have to weigh their decision about what is the alternative, which
[00:50:16.760 --> 00:50:21.240]   is to not give anything to them, to fall out of favor with these politicians.
[00:50:21.240 --> 00:50:26.440]   It really does point to the very foundation of our dysfunction, which is the money in
[00:50:26.440 --> 00:50:27.600]   politics.
[00:50:27.600 --> 00:50:29.680]   That's the problem.
[00:50:29.680 --> 00:50:31.600]   That's why we have the Supreme Court we have.
[00:50:31.600 --> 00:50:36.080]   That's why we have a Congress that doesn't represent the will of the American people, because
[00:50:36.080 --> 00:50:40.640]   we allow corporations to bribe Commogursman.
[00:50:40.640 --> 00:50:45.040]   And the only reason to bribe a Congressman is to get them to violate the will of the
[00:50:45.040 --> 00:50:49.840]   people and use the money to convince the people with this information that they're doing
[00:50:49.840 --> 00:50:50.840]   the opposite.
[00:50:50.840 --> 00:50:52.240]   So we've got to get the money out of politics.
[00:50:52.240 --> 00:50:56.160]   It's been done in other countries, and we have to somehow figure out how to do that, or
[00:50:56.160 --> 00:51:01.080]   we're going to have a lot more problems like this going forward.
[00:51:01.080 --> 00:51:06.160]   Just to update you on Elon Musk, he is meeting with a pope.
[00:51:06.160 --> 00:51:07.160]   Okay.
[00:51:07.160 --> 00:51:08.160]   Okay.
[00:51:08.160 --> 00:51:09.160]   Sure.
[00:51:09.160 --> 00:51:10.160]   Sure.
[00:51:10.160 --> 00:51:11.160]   Sure.
[00:51:11.160 --> 00:51:13.600]   With four of his 75 or so children, I believe?
[00:51:13.600 --> 00:51:14.600]   Yeah.
[00:51:14.600 --> 00:51:15.600]   Well, he's only...
[00:51:15.600 --> 00:51:18.560]   It looks like 14-age boys, but there's four other kids that aren't there, including
[00:51:18.560 --> 00:51:20.880]   his transgender daughter who says she's changing her name.
[00:51:20.880 --> 00:51:23.920]   She doesn't have anything to do with Elon Musk.
[00:51:23.920 --> 00:51:27.080]   I'm looking to see if Father Robert Baliser is there behind the curtains.
[00:51:27.080 --> 00:51:28.560]   I can't quite tell.
[00:51:28.560 --> 00:51:30.040]   Oh, he took the picture.
[00:51:30.040 --> 00:51:32.160]   But he might well have taken the picture.
[00:51:32.160 --> 00:51:35.760]   We'll get Robert on and ask him how that meeting went.
[00:51:35.760 --> 00:51:40.120]   I can't help but think that he went there to have really good prayer that the Twitter
[00:51:40.120 --> 00:51:41.320]   deal doesn't go through.
[00:51:41.320 --> 00:51:42.320]   Right.
[00:51:42.320 --> 00:51:44.120]   Because I really don't think he wants any...
[00:51:44.120 --> 00:51:45.360]   Praying now, isn't he?
[00:51:45.360 --> 00:51:46.360]   Yeah.
[00:51:46.360 --> 00:51:47.360]   So he sees...
[00:51:47.360 --> 00:51:48.360]   Please, please.
[00:51:48.360 --> 00:51:50.240]   Let it not happen.
[00:51:50.240 --> 00:51:56.760]   Right after the picture, same within minutes of posting the picture with the Pope, he also
[00:51:56.760 --> 00:52:01.120]   posted a picture from Venice of a...
[00:52:01.120 --> 00:52:03.760]   I don't know what is going on here.
[00:52:03.760 --> 00:52:04.760]   But there you go.
[00:52:04.760 --> 00:52:07.280]   That's Elon Musk in a nutshell.
[00:52:07.280 --> 00:52:08.280]   Who knows?
[00:52:08.280 --> 00:52:09.280]   You know?
[00:52:09.280 --> 00:52:11.960]   Pictures with people wearing white dresses.
[00:52:11.960 --> 00:52:12.960]   Yes.
[00:52:12.960 --> 00:52:14.240]   He was the deal here.
[00:52:14.240 --> 00:52:15.480]   He wasn't online for like...
[00:52:15.480 --> 00:52:19.720]   I wasn't posting for 11 days and I was reading some reports that people inside the companies
[00:52:19.720 --> 00:52:23.160]   were all thank God we're actually getting stuff done for once.
[00:52:23.160 --> 00:52:24.160]   Yeah, he's...
[00:52:24.160 --> 00:52:29.360]   He took his longest stretch of that posting on Twitter in nearly five years, a nine-day
[00:52:29.360 --> 00:52:32.600]   hiatus according to the Wall Street Journal.
[00:52:32.600 --> 00:52:33.600]   Wow.
[00:52:33.600 --> 00:52:34.600]   That is amazing.
[00:52:34.600 --> 00:52:37.240]   He could have gone for Lent and taken 40, but...
[00:52:37.240 --> 00:52:38.240]   Right.
[00:52:38.240 --> 00:52:39.240]   I mean, if...
[00:52:39.240 --> 00:52:41.000]   Gosh, if he'd done that, we might have missed out.
[00:52:41.000 --> 00:52:43.600]   No, I think Lent had ended by the time that he would have...
[00:52:43.600 --> 00:52:44.600]   Yeah.
[00:52:44.600 --> 00:52:45.600]   Or by damn.
[00:52:45.600 --> 00:52:46.600]   Yeah.
[00:52:46.600 --> 00:52:48.800]   Maybe he could give up Twitter for next Lent.
[00:52:48.800 --> 00:52:49.800]   That would be nice.
[00:52:49.800 --> 00:52:51.960]   Ooh, see, that would be good.
[00:52:51.960 --> 00:52:56.160]   The Wall Street Journal has published a graph of Elon Musk's Twitter now.
[00:52:56.160 --> 00:52:57.160]   Oh, no.
[00:52:57.160 --> 00:52:58.160]   Twitter habits.
[00:52:58.160 --> 00:52:59.160]   Okay, this is ridiculous.
[00:52:59.160 --> 00:53:03.960]   I mean, like, like, I get that it's news that the guy who's a Twitter addict hasn't tweeted.
[00:53:03.960 --> 00:53:05.360]   But do we need a graph?
[00:53:05.360 --> 00:53:07.760]   Like, did the data team have to relate to this?
[00:53:07.760 --> 00:53:08.760]   This is...
[00:53:08.760 --> 00:53:09.760]   This feels wrong.
[00:53:09.760 --> 00:53:10.760]   As many...
[00:53:10.760 --> 00:53:13.640]   They should have coded them for S posts versus non-S posts.
[00:53:13.640 --> 00:53:14.640]   Yes.
[00:53:14.640 --> 00:53:15.640]   They're not...
[00:53:15.640 --> 00:53:16.640]   Right.
[00:53:16.640 --> 00:53:18.000]   No classification here, which is, I think, fairly important.
[00:53:18.000 --> 00:53:21.360]   How many of these were in violation of the SEC orders for instance?
[00:53:21.360 --> 00:53:22.360]   Ooh, yeah.
[00:53:22.360 --> 00:53:23.360]   That would be...
[00:53:23.360 --> 00:53:24.360]   Right.
[00:53:24.360 --> 00:53:25.360]   That would be exactly.
[00:53:25.360 --> 00:53:27.600]   How many of these have been the subject of Wall Street?
[00:53:27.600 --> 00:53:28.600]   I just...
[00:53:28.600 --> 00:53:31.440]   I mention this only because people will get mad at me if I don't mention Elon Musk and
[00:53:31.440 --> 00:53:32.680]   every single...
[00:53:32.680 --> 00:53:33.680]   Really?
[00:53:33.680 --> 00:53:34.680]   No.
[00:53:34.680 --> 00:53:41.400]   They get mad at me because I mention Elon Musk and every single, to be honest.
[00:53:41.400 --> 00:53:42.400]   This is...
[00:53:42.400 --> 00:53:45.520]   Okay, I'm glad, Mike Elgin, that you are going to be the conscience of this show and tell
[00:53:45.520 --> 00:53:47.960]   me, you know, this is bad journalism.
[00:53:47.960 --> 00:53:48.960]   Or whatever.
[00:53:48.960 --> 00:53:56.000]   Google allowed a sanction ad company to harvest user data for months.
[00:53:56.000 --> 00:54:02.000]   In fact, it wasn't until ProPublica told them that they were selling information back
[00:54:02.000 --> 00:54:03.000]   to R.U.
[00:54:03.000 --> 00:54:05.560]   Target that Google stopped.
[00:54:05.560 --> 00:54:10.680]   As recently as June 23rd, Google was sharing potentially sensitive user data with a sanctioned
[00:54:10.680 --> 00:54:15.720]   Russian ad tech company owned by Russia's largest state bank.
[00:54:15.720 --> 00:54:16.720]   R.U.
[00:54:16.720 --> 00:54:17.720]   Target is the name of the...
[00:54:17.720 --> 00:54:18.720]   Yeah.
[00:54:18.720 --> 00:54:19.720]   R.U.
[00:54:19.720 --> 00:54:20.720]   Target, are you talking?
[00:54:20.720 --> 00:54:23.480]   I was saying, are you just because R.U. sounds like it might be Toys R Us or something.
[00:54:23.480 --> 00:54:24.480]   It sounds like a kangaroo.
[00:54:24.480 --> 00:54:25.480]   R.U.
[00:54:25.480 --> 00:54:26.480]   Target, we are a big fans of R.U.
[00:54:26.480 --> 00:54:27.480]   Paul.
[00:54:27.480 --> 00:54:28.480]   We love R.U.
[00:54:28.480 --> 00:54:29.480]   Paul.
[00:54:29.480 --> 00:54:30.480]   R.U.
[00:54:30.480 --> 00:54:31.480]   Paul started getting agents.
[00:54:31.480 --> 00:54:32.480]   He's coming with Russia.
[00:54:32.480 --> 00:54:33.480]   Yes.
[00:54:33.480 --> 00:54:34.480]   In some of his countries.
[00:54:34.480 --> 00:54:35.480]   That's where it's at.
[00:54:35.480 --> 00:54:39.600]   Russian company that helps brands and agencies buy digital ads to access and store data about
[00:54:39.600 --> 00:54:44.920]   people browsing websites and apps in Ukraine as well as other parts of the world.
[00:54:44.920 --> 00:54:49.560]   They were able to see location data for Ukrainians and some other information of what Ukrainians
[00:54:49.560 --> 00:54:50.560]   were doing.
[00:54:50.560 --> 00:54:54.840]   I really doubt that the Russian government is organized enough.
[00:54:54.840 --> 00:54:55.840]   Google didn't know.
[00:54:55.840 --> 00:54:56.760]   This is the real problem.
[00:54:56.760 --> 00:55:01.720]   The problem isn't that Google is deliberately violating sanctions or is pro-Russia or anything
[00:55:01.720 --> 00:55:02.720]   like that.
[00:55:02.720 --> 00:55:08.120]   The problem is that these ad networks, including Google, they don't really control what goes
[00:55:08.120 --> 00:55:09.120]   through.
[00:55:09.120 --> 00:55:11.440]   They have policies and so on, but they don't know...
[00:55:11.440 --> 00:55:15.880]   The scale of advertising online is just mind-blowing.
[00:55:15.880 --> 00:55:16.880]   No one's really in charge.
[00:55:16.880 --> 00:55:19.360]   It's all on a man.
[00:55:19.360 --> 00:55:22.480]   In fact, ProPublica last week did a story.
[00:55:22.480 --> 00:55:27.920]   There's roughly two decades Google has boasted it does not accept gun ads, but according
[00:55:27.920 --> 00:55:34.920]   to ProPublica, before and after the mass shootings in New York and Texas, millions of ads from
[00:55:34.920 --> 00:55:39.120]   some of the nation's largest firearm makers flowed through Google's ad systems and onto
[00:55:39.120 --> 00:55:40.960]   websites.
[00:55:40.960 --> 00:55:47.400]   Ads from gunmaker Savage Arms, for example, popped up on the site Baby Games, amid brightly
[00:55:47.400 --> 00:55:49.960]   colored games for children.
[00:55:49.960 --> 00:55:55.560]   On an article, "How to Handle Teen Drama on the Para..."
[00:55:55.560 --> 00:55:58.760]   Can you imagine you're reading an article, "How to Handle Teen Drama?"
[00:55:58.760 --> 00:56:02.760]   There's an ad for a Glock.
[00:56:02.760 --> 00:56:09.000]   Backpistel ads loaded on a recipe site list of the 50 best vegetarian recipes.
[00:56:09.000 --> 00:56:15.800]   You've got to kill that carrot before you cook it, as well as on the Quiz site Playbuzz,
[00:56:15.800 --> 00:56:22.440]   on the online Merriam Webster Dictionary and alongside stories in the Denver Post.
[00:56:22.440 --> 00:56:27.440]   Ads for Guns showed up on Britannica, the media site Heavy, the employer review site Glass
[00:56:27.440 --> 00:56:30.320]   Store, on Mac Roemers.
[00:56:30.320 --> 00:56:31.320]   What?
[00:56:31.320 --> 00:56:32.720]   What's the news in world report?
[00:56:32.720 --> 00:56:37.560]   Publishers, Clearinghouse and Ultimate Classic Rock, 15 of the largest firearms sellers in
[00:56:37.560 --> 00:56:42.800]   the US, including Daniel Defense, the company that made the AR-15 used in the Evaldi, Texas,
[00:56:42.800 --> 00:56:47.000]   used Google systems, play-sads that generated over 120 million impressions.
[00:56:47.000 --> 00:56:50.000]   But Google didn't know.
[00:56:50.000 --> 00:56:56.040]   Well, this, I think, harkens back to the conversation we were having earlier about how
[00:56:56.040 --> 00:56:57.400]   AI does.
[00:56:57.400 --> 00:57:02.920]   The best AI is 99% effective, and getting that last 1% is almost impossible.
[00:57:02.920 --> 00:57:05.120]   This is also true, by the way, of self-driving cars.
[00:57:05.120 --> 00:57:08.320]   It's going to take us forever to get to a point where they can actually be on the road.
[00:57:08.320 --> 00:57:15.360]   In this case, first of all, I don't know the specific facts in this case, but it's very
[00:57:15.360 --> 00:57:21.920]   possible that someone who is doing this research covers gun violence and gun sales and that
[00:57:21.920 --> 00:57:28.160]   sort of thing, and therefore, the relevance of ads, they got those ads.
[00:57:28.160 --> 00:57:29.160]   Yeah.
[00:57:29.160 --> 00:57:31.200]   Right, because they may be covering that.
[00:57:31.200 --> 00:57:33.960]   And you searched for those ads to get those products.
[00:57:33.960 --> 00:57:34.960]   Yeah.
[00:57:34.960 --> 00:57:38.360]   They're still being served up, though, and this gets to the real problem.
[00:57:38.360 --> 00:57:44.480]   They're using artificial intelligence and algorithms to determine who sees what, and
[00:57:44.480 --> 00:57:51.000]   they're trying to be as customized as possible, personalized as possible with the advertising,
[00:57:51.000 --> 00:57:52.400]   when you get results like this.
[00:57:52.400 --> 00:57:57.400]   So it's really a question of where Google thought they had a cheat code, which is that
[00:57:57.400 --> 00:58:01.200]   we're not going to hire 100,000 people to monitor all the ads, but just have software
[00:58:01.200 --> 00:58:02.680]   do it, and here we are.
[00:58:02.680 --> 00:58:07.600]   In fact, this is exactly what happened after visiting the websites of gun manufacturers,
[00:58:07.600 --> 00:58:15.400]   for example, because he's doing the research, a pro-publicer reporter, was shown ads for
[00:58:15.400 --> 00:58:21.240]   tactical vests and gun accessories on baby games.
[00:58:21.240 --> 00:58:25.760]   The tactical vest and gun accessories ads appeared on the page for Royal Family Christmas
[00:58:25.760 --> 00:58:26.760]   Preparation.
[00:58:26.760 --> 00:58:31.440]   Would you go hunting the little film?
[00:58:31.440 --> 00:58:33.880]   I think it's hysterical.
[00:58:33.880 --> 00:58:39.520]   It actually is almost makes you laugh when you're on this page with baby games, and there's
[00:58:39.520 --> 00:58:41.640]   a gun ad.
[00:58:41.640 --> 00:58:44.200]   If it weren't so horrific, it would make you laugh anyway.
[00:58:44.200 --> 00:58:49.160]   So we get my 15-year-old and I watch a lot of YouTube together on our Apple TV, and we
[00:58:49.160 --> 00:58:53.440]   get a lot of ads for ulcerative colitis, and I'm like, "Are you telling me something?"
[00:58:53.440 --> 00:58:55.840]   I don't think you're targeting, right?
[00:58:55.840 --> 00:58:59.640]   Big cars, $70,000 SUVs and ulcerative colitis medicine.
[00:58:59.640 --> 00:59:05.760]   There's only two countries in the world, I just learned this, where pharmaceutical companies
[00:59:05.760 --> 00:59:11.560]   can sell subscription pharmaceuticals on TV, New Zealand and the US.
[00:59:11.560 --> 00:59:14.840]   Okay, this is what freaks me out.
[00:59:14.840 --> 00:59:19.000]   When I go to other countries, and I usually don't watch TV because some don't understand
[00:59:19.000 --> 00:59:23.520]   language and also who watches normal television anymore, but it used to be one of those things
[00:59:23.520 --> 00:59:30.600]   I would watch local news or CNN or whatever, and yeah, you're like the dearth of pharmaceutical
[00:59:30.600 --> 00:59:32.400]   ads in other places.
[00:59:32.400 --> 00:59:35.600]   How do I know what to do about my psoriasis without...
[00:59:35.600 --> 00:59:36.600]   Exactly.
[00:59:36.600 --> 00:59:37.600]   Yeah.
[00:59:37.600 --> 00:59:41.520]   Exactly, you're watching CNN or something in the international version, but they have
[00:59:41.520 --> 00:59:43.160]   commercials and they're like, "Wait a minute."
[00:59:43.160 --> 00:59:44.720]   The news channels are the worst.
[00:59:44.720 --> 00:59:45.720]   Right.
[00:59:45.720 --> 00:59:50.920]   It's like, "Why do I see insurance ads and why do I not see pharmaceutical ads?"
[00:59:50.920 --> 00:59:51.920]   Yeah.
[00:59:51.920 --> 00:59:52.920]   They're not lying to me.
[00:59:52.920 --> 00:59:54.560]   They're not showing me drug ads.
[00:59:54.560 --> 00:59:56.560]   They're not showing me giant SUVs.
[00:59:56.560 --> 00:59:57.560]   What's going on with you?
[00:59:57.560 --> 00:59:59.120]   What country am I in?
[00:59:59.120 --> 01:00:00.560]   They get to see me in.
[01:00:00.560 --> 01:00:05.200]   But there's nudity in my spring water ads, which is fine, right?
[01:00:05.200 --> 01:00:07.160]   But everything else is...
[01:00:07.160 --> 01:00:10.880]   I'm just glad there's still cultural differences in the world, right?
[01:00:10.880 --> 01:00:11.880]   Yes.
[01:00:11.880 --> 01:00:15.560]   For a long time, everything was like you just go into America West.
[01:00:15.560 --> 01:00:16.560]   But now, I mean, it's good.
[01:00:16.560 --> 01:00:18.360]   There should be some differences, right?
[01:00:18.360 --> 01:00:20.520]   I love these subway ads in the British tube.
[01:00:20.520 --> 01:00:21.600]   I think they're just great.
[01:00:21.600 --> 01:00:23.360]   They're so brilliant and literate.
[01:00:23.360 --> 01:00:24.360]   And a lot of them have nudity.
[01:00:24.360 --> 01:00:25.360]   And it's cool.
[01:00:25.360 --> 01:00:26.360]   It's fun.
[01:00:26.360 --> 01:00:28.240]   You never know what you're going to see down there.
[01:00:28.240 --> 01:00:31.320]   I mean, I just got back from France, and it's really interesting.
[01:00:31.320 --> 01:00:35.560]   They don't have pharmaceutical ads on TV in France.
[01:00:35.560 --> 01:00:43.240]   And yet, the rate of consumption of pharmaceutical drugs in France is very, very high.
[01:00:43.240 --> 01:00:44.240]   Oh, interesting.
[01:00:44.240 --> 01:00:45.240]   I don't know if it's the size of the US.
[01:00:45.240 --> 01:00:46.720]   It might be higher than the US, I don't know.
[01:00:46.720 --> 01:00:47.960]   But it's very, very high.
[01:00:47.960 --> 01:00:48.960]   And it's not...
[01:00:48.960 --> 01:00:49.960]   So these...
[01:00:49.960 --> 01:00:50.960]   I don't know...
[01:00:50.960 --> 01:00:54.720]   I mean, basically, obviously, the pharmaceutical companies want the patient to go in and start
[01:00:54.720 --> 01:00:56.720]   needling the doctor about how they need to...
[01:00:56.720 --> 01:00:58.520]   Doctors must hate that, right?
[01:00:58.520 --> 01:01:02.280]   Hey, Doc, I need Skyresi.
[01:01:02.280 --> 01:01:03.520]   But you only have athletes' feet.
[01:01:03.520 --> 01:01:06.760]   No, but I saw it on TV.
[01:01:06.760 --> 01:01:07.760]   It must drive doctors.
[01:01:07.760 --> 01:01:08.760]   Do you think it depends on the doctor?
[01:01:08.760 --> 01:01:11.760]   I think it depends, because some doctors, not all of them, but some are...
[01:01:11.760 --> 01:01:12.760]   That's right.
[01:01:12.760 --> 01:01:17.160]   Well, not only that, but they get kickbacks from the pharmaceutical companies.
[01:01:17.160 --> 01:01:18.160]   That's an actual thing.
[01:01:18.160 --> 01:01:19.160]   Well, I've got samples.
[01:01:19.160 --> 01:01:20.160]   Why don't you try this, Oxy?
[01:01:20.160 --> 01:01:21.160]   See if it helps.
[01:01:21.160 --> 01:01:27.520]   I was going to say, the doctors, not all of them again, but plenty of them, if they're
[01:01:27.520 --> 01:01:32.560]   really pushing a certain brand name and not wanting you to try anything else, there's
[01:01:32.560 --> 01:01:37.040]   a chance they're getting some sort of kickback from that pharmaceutical company.
[01:01:37.040 --> 01:01:44.160]   Did you, in this study, Mike, did it say what drugs, what pharmaceuticals of French favor
[01:01:44.160 --> 01:01:46.440]   with their ganos in a strong coffee?
[01:01:46.440 --> 01:01:49.560]   Is there something I read a couple of years ago?
[01:01:49.560 --> 01:01:54.720]   I was surprised by it, because it just doesn't...
[01:01:54.720 --> 01:01:57.920]   It didn't make sense in terms of how nice France is.
[01:01:57.920 --> 01:02:00.680]   Why would you need to be on drugs?
[01:02:00.680 --> 01:02:02.680]   But French just only gets nice.
[01:02:02.680 --> 01:02:05.680]   They're pissed off all times.
[01:02:05.680 --> 01:02:09.080]   I should point out ProPublica, since we're only studying ProPublica stories today, since
[01:02:09.080 --> 01:02:10.080]   they're so good.
[01:02:10.080 --> 01:02:12.680]   They have a site called Dollars for Docs.
[01:02:12.680 --> 01:02:17.900]   If you go there, the data is from, I think, 2014 to 2018, you can look up your doctor
[01:02:17.900 --> 01:02:20.000]   and see if they got money from home.
[01:02:20.000 --> 01:02:28.040]   But you can also go to openpaymentsdata.cms.gov, which is a US government-run site that lets
[01:02:28.040 --> 01:02:29.040]   you search.
[01:02:29.040 --> 01:02:33.280]   It's not quite as good as the slice and dice of ProPublica, but it's a government site.
[01:02:33.280 --> 01:02:38.680]   You can see data goes through December 2021 so far, and they're going to update it, I
[01:02:38.680 --> 01:02:39.680]   guess, shortly.
[01:02:39.680 --> 01:02:41.000]   And you can say, like, type in your doctor.
[01:02:41.000 --> 01:02:42.960]   Are they getting money from pharmaceuticals?
[01:02:42.960 --> 01:02:44.720]   Does this have to be disclosed?
[01:02:44.720 --> 01:02:45.720]   Is that why?
[01:02:45.720 --> 01:02:48.440]   Yeah, there's a law passed, I think it was part of...
[01:02:48.440 --> 01:02:49.720]   I want to say it was part of the ACA.
[01:02:49.720 --> 01:02:53.120]   It was mandatory disclosure of pharmaceutical finance.
[01:02:53.120 --> 01:02:54.120]   Fantastic.
[01:02:54.120 --> 01:02:55.120]   But people, most people don't know it.
[01:02:55.120 --> 01:03:01.520]   ProPublica made a good site about it, but this CMS.gov site actually is pretty good to openpaymentsdata.cms.
[01:03:01.520 --> 01:03:05.520]   Yeah, in fact, ProPublica says, "We just have a snapshot from 2019.
[01:03:05.520 --> 01:03:08.320]   If you want to see it up to date, go to openpaymentsdata."
[01:03:08.320 --> 01:03:12.160]   One of those weird things you're like, "Wait, does it really...
[01:03:12.160 --> 01:03:13.160]   Is it really that?"
[01:03:13.160 --> 01:03:16.320]   See, we complain about the government, but they do some good things.
[01:03:16.320 --> 01:03:17.320]   That's right.
[01:03:17.320 --> 01:03:18.320]   Occasionally.
[01:03:18.320 --> 01:03:19.320]   Nobody's talking.
[01:03:19.320 --> 01:03:20.320]   Well, that's the problem.
[01:03:20.320 --> 01:03:22.320]   We don't know about this place.
[01:03:22.320 --> 01:03:23.320]   It's amazing.
[01:03:23.320 --> 01:03:25.320]   I've got a story on this episode.
[01:03:25.320 --> 01:03:29.560]   I feel like there's the problems that are cited, and then it's always...
[01:03:29.560 --> 01:03:31.960]   It's really congressional dysfunction.
[01:03:31.960 --> 01:03:34.360]   It's really a lack of a law in this place.
[01:03:34.360 --> 01:03:35.360]   It's really...
[01:03:35.360 --> 01:03:38.960]   It just keeps going on where I'm thinking it's totally true.
[01:03:38.960 --> 01:03:43.160]   We'll talk about the location history stuff.
[01:03:43.160 --> 01:03:46.000]   The problem is that Google tracks are location, not...
[01:03:46.000 --> 01:03:49.800]   It's also a problem what they're doing with it, but it's a fundamental issue.
[01:03:49.800 --> 01:03:52.480]   That Google...
[01:03:52.480 --> 01:04:00.000]   Section 23, 230 seems like a fundamental right in people's misunderstanding is a dysfunction
[01:04:00.000 --> 01:04:01.000]   of a larger scale.
[01:04:01.000 --> 01:04:05.720]   A lot of tech feels like it's large scale dysfunction being refracted into these very
[01:04:05.720 --> 01:04:07.600]   specific things that we talk about.
[01:04:07.600 --> 01:04:09.120]   I like that we're jumping up there.
[01:04:09.120 --> 01:04:10.120]   So this is a new thing.
[01:04:10.120 --> 01:04:14.280]   Google says not only abortion clinics, but it will delete visits to domestic violence
[01:04:14.280 --> 01:04:19.400]   shelters, weight loss clinics, and other potentially sensitive locations from users,
[01:04:19.400 --> 01:04:23.240]   location histories in the coming weeks.
[01:04:23.240 --> 01:04:24.240]   Should it track?
[01:04:24.240 --> 01:04:26.240]   Location tracking is...
[01:04:26.240 --> 01:04:27.240]   It tracks everything.
[01:04:27.240 --> 01:04:28.680]   I don't think it says, "Oh, hey, good.
[01:04:28.680 --> 01:04:30.320]   I see you went to a weight loss clinic."
[01:04:30.320 --> 01:04:31.320]   It's just part of the...
[01:04:31.320 --> 01:04:33.280]   It's true, but does it...
[01:04:33.280 --> 01:04:34.280]   This is that...
[01:04:34.280 --> 01:04:37.960]   I mean, this is kind of always the Apple lens versus the non-Apple lens, and not that Apple
[01:04:37.960 --> 01:04:41.640]   has been perfect about this, as we've seen many times in the last decade.
[01:04:41.640 --> 01:04:46.880]   But we just got a new thermostat in our house, got a smart thermostat and a heat pump.
[01:04:46.880 --> 01:04:48.080]   Very exciting.
[01:04:48.080 --> 01:04:52.240]   And I had to have a long conversation with my wife about what Apple does with home kit
[01:04:52.240 --> 01:04:54.240]   data, because she is the...
[01:04:54.240 --> 01:04:57.680]   What I want to call the early projector, and rightly so, in our household.
[01:04:57.680 --> 01:04:59.640]   She does not want more tech in the house.
[01:04:59.640 --> 01:05:01.640]   And I try not to be the early...
[01:05:01.640 --> 01:05:02.640]   Oh, no, it's great.
[01:05:02.640 --> 01:05:03.640]   Here's why it's great.
[01:05:03.640 --> 01:05:09.480]   But like Google, I think makes very free internal use of location data.
[01:05:09.480 --> 01:05:13.600]   And Apple, Apple is a feature called Significant Locations that you can see in iOS and iPad
[01:05:13.600 --> 01:05:14.600]   OS.
[01:05:14.600 --> 01:05:18.840]   It tracks where you go frequently, but the data is only stored on your devices, it's
[01:05:18.840 --> 01:05:22.760]   derived locally, and it's end-to-end encrypted when it's synced among your devices.
[01:05:22.760 --> 01:05:28.320]   So ostensibly, someone has to get a government agency or other law enforcement or a criminal
[01:05:28.320 --> 01:05:32.680]   has to gain access to your phone, be able to unlock the phone, and then be able to drill
[01:05:32.680 --> 01:05:34.200]   down to see that information.
[01:05:34.200 --> 01:05:38.680]   And that to me is the level of protection I want on my personal location data, not it's
[01:05:38.680 --> 01:05:40.920]   uploaded to the cloud and Google can see the nice new stuff.
[01:05:40.920 --> 01:05:45.680]   It's good, it's an accident of history that Apple is not selling ads against that data.
[01:05:45.680 --> 01:05:46.680]   It's new.
[01:05:46.680 --> 01:05:47.680]   No, no, no, you see.
[01:05:47.680 --> 01:05:48.680]   100%.
[01:05:48.680 --> 01:05:51.680]   If I ad had worked, they wanted to.
[01:05:51.680 --> 01:05:52.680]   They wanted to.
[01:05:52.680 --> 01:05:53.680]   I'm sure.
[01:05:53.680 --> 01:05:56.280]   They're making a virtue out of a necessity, which is good.
[01:05:56.280 --> 01:05:57.280]   Absolutely.
[01:05:57.280 --> 01:05:58.280]   Absolutely.
[01:05:58.280 --> 01:05:59.280]   I'm appreciative that that was the...
[01:05:59.280 --> 01:06:00.880]   I appreciate the historical accident happened.
[01:06:00.880 --> 01:06:02.560]   And so I don't try to defend them as...
[01:06:02.560 --> 01:06:06.480]   I mean, they want to use this marketing point and it's great, but I want it to see it as
[01:06:06.480 --> 01:06:08.280]   a technical and privacy issue.
[01:06:08.280 --> 01:06:10.280]   But it's like I do have an alternative.
[01:06:10.280 --> 01:06:12.480]   I can be a service where I don't think this is going to be misused.
[01:06:12.480 --> 01:06:17.920]   I am going to bring up this subject in a minute because there is a story that does relate
[01:06:17.920 --> 01:06:20.000]   to this, but we're going to take a break.
[01:06:20.000 --> 01:06:21.000]   Glenn Fleischman is here.
[01:06:21.000 --> 01:06:23.120]   It's great to have him from Glenn.fun.
[01:06:23.120 --> 01:06:25.320]   You read his stuff every...
[01:06:25.320 --> 01:06:31.040]   Is there one publication that would be the place to mention or is it just everywhere?
[01:06:31.040 --> 01:06:33.320]   I've kind of focused on Macworld.
[01:06:33.320 --> 01:06:35.240]   I'd write the Mac 9-1-1 column.
[01:06:35.240 --> 01:06:36.240]   Oh, that's right.
[01:06:36.240 --> 01:06:38.240]   I've got books for take control books.
[01:06:38.240 --> 01:06:39.240]   I don't put those out of good work.
[01:06:39.240 --> 01:06:41.680]   Did you take over from Chris on the Mac 9-1-1?
[01:06:41.680 --> 01:06:42.680]   Chris Breen and it's been...
[01:06:42.680 --> 01:06:45.320]   I've lost track seven or eight years now.
[01:06:45.320 --> 01:06:46.320]   Yeah.
[01:06:46.320 --> 01:06:47.960]   I've written like 1,500 columns.
[01:06:47.960 --> 01:06:48.960]   Wow.
[01:06:48.960 --> 01:06:51.120]   But he wrote thousands before me or yours.
[01:06:51.120 --> 01:06:52.120]   I still get emailed.
[01:06:52.120 --> 01:06:53.960]   Dear Chris, I know you can help me with the problem.
[01:06:53.960 --> 01:06:54.960]   Sorry.
[01:06:54.960 --> 01:06:59.040]   He went to go to that great fruit company in a different part of this world.
[01:06:59.040 --> 01:07:01.040]   Chris, is he working with Apple?
[01:07:01.040 --> 01:07:02.040]   He's a...
[01:07:02.040 --> 01:07:03.280]   There are many former...
[01:07:03.280 --> 01:07:08.480]   The largest percentage of ex-Macworld staffers, that means that a staffer are working at a
[01:07:08.480 --> 01:07:09.480]   fruit company.
[01:07:09.480 --> 01:07:10.480]   It's amazing.
[01:07:10.480 --> 01:07:11.480]   Join your Apple.
[01:07:11.480 --> 01:07:12.480]   Shocking.
[01:07:12.480 --> 01:07:13.480]   And great for the...
[01:07:13.480 --> 01:07:14.480]   It's great.
[01:07:14.480 --> 01:07:17.360]   Some of the best people at Macworld, some have formed their own groups like Jason Snell
[01:07:17.360 --> 01:07:21.640]   Regular, guest on this show, has six colors and more and works with them there.
[01:07:21.640 --> 01:07:26.400]   Some folks have left the Mac writing field and everybody else just about went to work
[01:07:26.400 --> 01:07:28.240]   at Apple and so they're doing great.
[01:07:28.240 --> 01:07:31.600]   You can see Serendi Caldwell is the piece of WWDC.
[01:07:31.600 --> 01:07:32.600]   She does great.
[01:07:32.600 --> 01:07:37.840]   I'm so sorry we can't see her doing the tech stuff she used to because she's internal.
[01:07:37.840 --> 01:07:39.240]   But boy, it's so fun every year.
[01:07:39.240 --> 01:07:43.760]   It's like, "Oh my gosh, it's Serendi and she's the voice of WWDC."
[01:07:43.760 --> 01:07:49.880]   It does though speak a little bit to the kind of enmeshment of Apple Press and Apple corporate.
[01:07:49.880 --> 01:07:55.640]   I mean, you certainly don't want to see a revolving door where they join Apple and they
[01:07:55.640 --> 01:07:58.920]   come back out and they start writing for Macworld again and stuff like that.
[01:07:58.920 --> 01:08:01.160]   Apple almost never hired Mac writers.
[01:08:01.160 --> 01:08:05.440]   It seemed like for a long time and then when there was a great bloodlet, you know better
[01:08:05.440 --> 01:08:07.880]   than anybody, Christina, because you were in that field for a long time.
[01:08:07.880 --> 01:08:09.480]   I mean, I've never had a staff job.
[01:08:09.480 --> 01:08:13.680]   But it seemed like when there was the great bloodletting in the tech press world, Apple
[01:08:13.680 --> 01:08:14.680]   sucked in.
[01:08:14.680 --> 01:08:19.640]   I mean, Google brought in some great X New York Times people who went into Google editorial.
[01:08:19.640 --> 01:08:23.720]   Quentin Hardy, for instance, is it Google Cloud's editorial director?
[01:08:23.720 --> 01:08:28.640]   My favorite reporter is a great person and I don't know what he does there, but their
[01:08:28.640 --> 01:08:29.640]   editorial is good, I guess.
[01:08:29.640 --> 01:08:32.440]   Anyway, I think the big tech companies, they wasn't revolving door.
[01:08:32.440 --> 01:08:34.960]   It was kind of a lifeboat for a lot of people.
[01:08:34.960 --> 01:08:36.880]   It's like, all right, you know this stuff.
[01:08:36.880 --> 01:08:38.360]   I wonder though, people suddenly.
[01:08:38.360 --> 01:08:42.840]   I wonder though, because Google and Apple both have kind of hired a lot of content people.
[01:08:42.840 --> 01:08:43.840]   Yeah.
[01:08:43.840 --> 01:08:48.800]   Nathan Oliver S. Giles, who worked for it was an employee who has got hired away by Apple.
[01:08:48.800 --> 01:08:50.080]   But they kind of disappeared, Apple.
[01:08:50.080 --> 01:08:51.720]   I mean, what is Apple?
[01:08:51.720 --> 01:08:54.080]   What is Apple's content business?
[01:08:54.080 --> 01:08:55.560]   It's not Apple news.
[01:08:55.560 --> 01:08:56.560]   Are there...
[01:08:56.560 --> 01:08:57.560]   There's so much...
[01:08:57.560 --> 01:08:59.360]   I mean, I have a little bit...
[01:08:59.360 --> 01:09:04.280]   I don't want to peak any kimono that I'm not allowed to, but they have a wonderful writing
[01:09:04.280 --> 01:09:06.520]   on say support.apple.com.
[01:09:06.520 --> 01:09:13.080]   For instance, the App Store, the Mac App Store and the iOS and iPad App Store, iPad OS App
[01:09:13.080 --> 01:09:16.480]   Store have terrific writing and they have these articles that show up.
[01:09:16.480 --> 01:09:17.480]   So they're everything.
[01:09:17.480 --> 01:09:19.520]   So that's where the content is.
[01:09:19.520 --> 01:09:22.760]   Yeah, and when somebody's writing scripts for podcasts, somebody...
[01:09:22.760 --> 01:09:27.280]   I mean, I know it's a marketing department is writing the keynote stuff, but there's
[01:09:27.280 --> 01:09:32.880]   a lot of places in which expert people are writing this kind of great gap between like
[01:09:32.880 --> 01:09:36.320]   almost service journalism and how to stuff.
[01:09:36.320 --> 01:09:40.800]   And Apple has a lot of, you know, a million pages of that as being written by a lot of
[01:09:40.800 --> 01:09:41.800]   Apple.
[01:09:41.800 --> 01:09:42.800]   That's a good point.
[01:09:42.800 --> 01:09:45.520]   A lot of Apple support documents really are service journalism under the app.
[01:09:45.520 --> 01:09:48.160]   Yeah, I'll find something and I'll be like, I wonder if there's someone I could write
[01:09:48.160 --> 01:09:49.160]   about an era I found.
[01:09:49.160 --> 01:09:50.240]   I'm like, no, I better not.
[01:09:50.240 --> 01:09:52.400]   I'll just mark feedback on the page.
[01:09:52.400 --> 01:09:53.400]   Yeah.
[01:09:53.400 --> 01:09:54.920]   I have a good friend who wrote...
[01:09:54.920 --> 01:09:58.160]   It was a speech writer for Jean-Louis Gesset for years.
[01:09:58.160 --> 01:10:02.880]   And I mean, they hire very good, talented people to do their writing.
[01:10:02.880 --> 01:10:04.040]   They can afford to.
[01:10:04.040 --> 01:10:06.680]   It turns out good writers come cheap and...
[01:10:06.680 --> 01:10:10.360]   Thank you, very much.
[01:10:10.360 --> 01:10:18.480]   But at the same time, I feel like there should be this impenetrable wall between people doing
[01:10:18.480 --> 01:10:21.320]   editorial content about a company and the company itself.
[01:10:21.320 --> 01:10:22.320]   Absolutely.
[01:10:22.320 --> 01:10:23.440]   Well, it's kind of become one way though.
[01:10:23.440 --> 01:10:26.080]   It's because there's been so many layoffs and so many...
[01:10:26.080 --> 01:10:29.080]   Yeah, I guess if you're at work, you've got to take a time.
[01:10:29.080 --> 01:10:32.600]   I don't know anybody who's come back from the tech...
[01:10:32.600 --> 01:10:35.240]   I know a couple people who have, but it's very rare.
[01:10:35.240 --> 01:10:36.240]   Yeah, very rare.
[01:10:36.240 --> 01:10:38.320]   And you can certainly do it.
[01:10:38.320 --> 01:10:41.720]   I think because I sometimes think about it and people who have even approached me about
[01:10:41.720 --> 01:10:42.720]   things like the tech.
[01:10:42.720 --> 01:10:46.440]   Yeah, when you went to Microsoft, you were leaving a career as a journalist.
[01:10:46.440 --> 01:10:49.440]   Was that a challenge for you?
[01:10:49.440 --> 01:10:51.040]   Yeah, I mean, it was.
[01:10:51.040 --> 01:10:52.920]   In my mind, I did have the question.
[01:10:52.920 --> 01:10:57.080]   I was like, "Am I still going to be able to have my own opinions and my own autonomy?"
[01:10:57.080 --> 01:10:59.000]   And I will say my role is a little bit different.
[01:10:59.000 --> 01:11:01.200]   I work directly with product and engineering people.
[01:11:01.200 --> 01:11:02.600]   I don't work in comms.
[01:11:02.600 --> 01:11:04.200]   I don't work in marketing.
[01:11:04.200 --> 01:11:08.040]   And so it's a little bit different than how the transition usually goes.
[01:11:08.040 --> 01:11:09.040]   Right, right.
[01:11:09.040 --> 01:11:12.960]   So when writing the internal press releases or doing the press outreach or things that
[01:11:12.960 --> 01:11:17.240]   would be easy for me to do, frankly, but not intellectually interesting.
[01:11:17.240 --> 01:11:19.720]   So it was a little bit different.
[01:11:19.720 --> 01:11:22.280]   But yeah, I did definitely have that question.
[01:11:22.280 --> 01:11:23.600]   What will I be giving up?
[01:11:23.600 --> 01:11:26.600]   Could I come back and could I protect objectively again?
[01:11:26.600 --> 01:11:30.440]   Well, and for a long time, we actually did not have you on our shows.
[01:11:30.440 --> 01:11:32.200]   But I missed you.
[01:11:32.200 --> 01:11:34.640]   Yeah, and I appreciate that.
[01:11:34.640 --> 01:11:36.840]   And I was glad to be able to.
[01:11:36.840 --> 01:11:37.840]   I got permission.
[01:11:37.840 --> 01:11:38.840]   You were really in marketing.
[01:11:38.840 --> 01:11:40.040]   You were exactly in marketing.
[01:11:40.040 --> 01:11:42.640]   I mean, you were in dev relations so always.
[01:11:42.640 --> 01:11:46.800]   So I felt like we have a lot of people who do developer relations for companies on the
[01:11:46.800 --> 01:11:47.800]   show.
[01:11:47.800 --> 01:11:48.800]   Exactly.
[01:11:48.800 --> 01:11:49.800]   Exactly.
[01:11:49.800 --> 01:11:52.240]   I mean, again, like I said, what I do, it's a little bit different than the other
[01:11:52.240 --> 01:11:53.240]   things.
[01:11:53.240 --> 01:11:57.560]   And also I've been lucky to work at places that have much more open policies in terms
[01:11:57.560 --> 01:12:01.000]   of how active you can be on social media and whatnot.
[01:12:01.000 --> 01:12:06.760]   And my bosses have recognized and I'm very thankful for this that it is good when I
[01:12:06.760 --> 01:12:11.240]   can be myself and not having to pair it, you know, like a corporate line, right?
[01:12:11.240 --> 01:12:12.800]   Like that's good for everyone.
[01:12:12.800 --> 01:12:14.560]   That for me was part of the judgment.
[01:12:14.560 --> 01:12:18.680]   Can I expect Christina to be a Christina bot or the real thing?
[01:12:18.680 --> 01:12:19.840]   And you've always been the real thing.
[01:12:19.840 --> 01:12:21.840]   So there was never, never a question.
[01:12:21.840 --> 01:12:26.480]   And I think that was a big thing for me too, when I made the decision to join the, you
[01:12:26.480 --> 01:12:29.040]   know, when I made the decision to join Microsoft, like I'll be candid.
[01:12:29.040 --> 01:12:33.080]   I would never say never to any company, but if I was going to take a job someplace where
[01:12:33.080 --> 01:12:36.760]   I was not allowed to still maintain my own brand, my own identity, my own experience.
[01:12:36.760 --> 01:12:38.000]   Well, you had to relocate.
[01:12:38.000 --> 01:12:39.000]   You had to do exactly.
[01:12:39.000 --> 01:12:40.000]   Exactly.
[01:12:40.000 --> 01:12:41.000]   And it was.
[01:12:41.000 --> 01:12:44.000]   And so for me, it was one of those things where I said, and I'm going to do this, then
[01:12:44.000 --> 01:12:47.120]   I still need to be able to have that external presence too.
[01:12:47.120 --> 01:12:50.960]   And if a company had said, you can't do this, and I talked to some who definitely had that
[01:12:50.960 --> 01:12:54.880]   kind of philosophy, look, I'll never say never to anything.
[01:12:54.880 --> 01:12:58.200]   And there's always a price, but the price is probably going to be a couple of multiples
[01:12:58.200 --> 01:13:02.600]   of what you would agree to pay me if I'm going to give up all the other aspects of what I've
[01:13:02.600 --> 01:13:06.000]   built for my career, you know, but I know that's also not the same for everyone.
[01:13:06.000 --> 01:13:07.000]   Yeah.
[01:13:07.000 --> 01:13:10.680]   The big picture is that as Andreessen says, software is eating the world.
[01:13:10.680 --> 01:13:16.720]   And so yeah, I have a kind of a unique perspective because in so does Glenn, if during the, the
[01:13:16.720 --> 01:13:23.280]   decade of the 90s, the press had massive power and tons of money and the tech industry had
[01:13:23.280 --> 01:13:25.600]   a lot less, relatively speaking.
[01:13:25.600 --> 01:13:32.480]   I mean, you know, we, PC magazine could make or break a company like that.
[01:13:32.480 --> 01:13:37.960]   Windows Magazine, which I work for, PC Computing, Macworld, et cetera, they had so much power
[01:13:37.960 --> 01:13:41.920]   because first of all, we were still kind of a gatekeeper.
[01:13:41.920 --> 01:13:44.080]   There wasn't a global internet.
[01:13:44.080 --> 01:13:45.080]   Right.
[01:13:45.080 --> 01:13:48.680]   So, the amateur is working for free to create great content.
[01:13:48.680 --> 01:13:51.360]   And secondly, the companies were much smaller.
[01:13:51.360 --> 01:13:56.320]   If you, if you look over time, you see the world of journalism in terms of business,
[01:13:56.320 --> 01:13:59.360]   just shrinking, shrinking, shrinking, and you know, a company like Apple, just growing,
[01:13:59.360 --> 01:14:03.400]   growing, growing, and now Apple's the most gigantic company in the history of mankind.
[01:14:03.400 --> 01:14:08.600]   And, and, you know, the publications are laying people off, you know, every, every few years.
[01:14:08.600 --> 01:14:10.280]   So I hadn't really thought of that.
[01:14:10.280 --> 01:14:13.200]   That makes a lot of sense that there was a need for these jobs.
[01:14:13.200 --> 01:14:15.000]   So thank goodness these companies step in.
[01:14:15.000 --> 01:14:16.000]   Yeah.
[01:14:16.000 --> 01:14:17.000]   Yeah.
[01:14:17.000 --> 01:14:19.360]   So if you're going to explain things to people, it's not going to be, you know, you make
[01:14:19.360 --> 01:14:22.840]   a much better living working for Apple than you do, trying to scrape it out on your own
[01:14:22.840 --> 01:14:23.840]   as a journalist.
[01:14:23.840 --> 01:14:27.920]   Speaking of which, one of our favorite contributors on the show, Wesley Faulkner, has a new job.
[01:14:27.920 --> 01:14:31.280]   He is now senior community manager at Amazon Web Services.
[01:14:31.280 --> 01:14:34.440]   So congratulations, Wesley.
[01:14:34.440 --> 01:14:36.240]   I think that's really, really good.
[01:14:36.240 --> 01:14:37.240]   Nice.
[01:14:37.240 --> 01:14:38.240]   I brought up Lex Friedman.
[01:14:38.240 --> 01:14:41.760]   Lex Friedman's one of the people who's been selling ads in the podcasting world for longer
[01:14:41.760 --> 01:14:42.760]   than anybody else.
[01:14:42.760 --> 01:14:43.760]   And I joke with it.
[01:14:43.760 --> 01:14:44.760]   Yeah.
[01:14:44.760 --> 01:14:45.760]   Oh, yeah.
[01:14:45.760 --> 01:14:46.760]   He was one of the first.
[01:14:46.760 --> 01:14:50.560]   He joined Earwolf early on, but he was a Mac world writer.
[01:14:50.560 --> 01:14:52.280]   That was kind of his dream job.
[01:14:52.280 --> 01:14:56.000]   And then he found out he was really good at selling podcast ads.
[01:14:56.000 --> 01:14:58.760]   And now he was acquired by his latest firm.
[01:14:58.760 --> 01:14:59.760]   He was acquired by Amazon.
[01:14:59.760 --> 01:15:01.560]   I think it was last year now.
[01:15:01.560 --> 01:15:06.760]   So he's an Amazon employee after going from Mac world to Stitcher and McClatchy and like
[01:15:06.760 --> 01:15:09.600]   this whole series of acquisitions.
[01:15:09.600 --> 01:15:10.760]   He's at Amazon.
[01:15:10.760 --> 01:15:13.040]   So all these Mac, everybody worked at Mac world.
[01:15:13.040 --> 01:15:14.280]   Is that some interesting company?
[01:15:14.280 --> 01:15:15.280]   It's really fascinating.
[01:15:15.280 --> 01:15:16.280]   Yeah.
[01:15:16.280 --> 01:15:17.280]   Or is Jason's now.
[01:15:17.280 --> 01:15:18.280]   Or is Jason's now.
[01:15:18.280 --> 01:15:20.400]   It was always Jason's.
[01:15:20.400 --> 01:15:22.240]   Whatever happened to Jason's now.
[01:15:22.240 --> 01:15:27.240]   I wish I could see him or hear him on podcast occasionally.
[01:15:27.240 --> 01:15:28.240]   Yeah.
[01:15:28.240 --> 01:15:30.880]   If he would just do a podcast, I don't know.
[01:15:30.880 --> 01:15:31.880]   He's very shocked.
[01:15:31.880 --> 01:15:32.880]   Yeah.
[01:15:32.880 --> 01:15:33.880]   Yeah.
[01:15:33.880 --> 01:15:38.240]   Actually, we have some news about Jason's now, but that will come at a later date.
[01:15:38.240 --> 01:15:42.960]   Let me, now that I've teased you, talk about, do any of you use Zappier?
[01:15:42.960 --> 01:15:44.400]   I'm such a Zappier fan.
[01:15:44.400 --> 01:15:46.880]   I sponsored for this segment of this weekend.
[01:15:46.880 --> 01:15:47.880]   I do.
[01:15:47.880 --> 01:15:48.880]   Yeah.
[01:15:48.880 --> 01:15:49.880]   It's so great.
[01:15:49.880 --> 01:15:50.880]   We use it as part of our big workflows.
[01:15:50.880 --> 01:15:55.640]   Whenever I bookmark a story in a variety of places, I have a Zappier script that will
[01:15:55.640 --> 01:16:02.520]   automatically post its Twitter, post it to a couple of places, but then also put it,
[01:16:02.520 --> 01:16:08.120]   post it on Twitter, social, Mastodon instance, but then also put it in a Google spreadsheet
[01:16:08.120 --> 01:16:11.880]   for our producers so they can quickly import it into our show rundowns.
[01:16:11.880 --> 01:16:15.200]   That's just one of many things you can do with Zappier.
[01:16:15.200 --> 01:16:19.880]   I want to point, especially to people in business, if you're trying to grow a business, and
[01:16:19.880 --> 01:16:25.920]   you know how many hats you wear, how many things you do, wouldn't it be nice to eliminate
[01:16:25.920 --> 01:16:28.640]   the routine operation tasks at your time?
[01:16:28.640 --> 01:16:31.200]   Still get them done, but have them done automatically.
[01:16:31.200 --> 01:16:36.000]   Things like lead management, employee onboarding, customer support.
[01:16:36.000 --> 01:16:41.440]   Zappier does that and you're not coding, but it's, yeah, it's kind of like coding.
[01:16:41.440 --> 01:16:44.960]   It actually makes it, this is one of the things about being a coder.
[01:16:44.960 --> 01:16:49.600]   You know that if you would just take a little time, you could get this thing done automatically
[01:16:49.600 --> 01:16:53.880]   and you never have to spend a minute doing it again, but sometimes it's easier just to
[01:16:53.880 --> 01:16:56.040]   keep doing it over and over and over again.
[01:16:56.040 --> 01:16:57.040]   That's why Zappier is so great.
[01:16:57.040 --> 01:17:04.160]   It makes it easy to connect all your apps, automate routine tasks, streamline your processes.
[01:17:04.160 --> 01:17:07.520]   Do the things that computer is really good at doing.
[01:17:07.520 --> 01:17:08.520]   It doesn't make mistakes.
[01:17:08.520 --> 01:17:10.520]   It does it the same way every single time.
[01:17:10.520 --> 01:17:11.520]   It does it automatically?
[01:17:11.520 --> 01:17:13.160]   Does it without you thinking about it?
[01:17:13.160 --> 01:17:16.600]   It gives you, it frees you up to do the things you're good at.
[01:17:16.600 --> 01:17:21.040]   You know, to use your big brain to prioritize customer and client needs to make your business
[01:17:21.040 --> 01:17:22.040]   succeed.
[01:17:22.040 --> 01:17:25.400]   It's the power of automation possible for everyone.
[01:17:25.400 --> 01:17:27.720]   You don't have to be a coder.
[01:17:27.720 --> 01:17:34.760]   I use Zappier, love Zappier, and I'm always thinking of new ways to use Zappier.
[01:17:34.760 --> 01:17:41.040]   It makes it easy for anyone to get started with business automation, no coding necessary.
[01:17:41.040 --> 01:17:43.920]   Zappier has the biggest collection of apps.
[01:17:43.920 --> 01:17:48.120]   4,000 of the most popular apps businesses use every day.
[01:17:48.120 --> 01:17:51.200]   I mentioned Google Sheets, for instance.
[01:17:51.200 --> 01:17:58.120]   Twitter, Google Sheets, RSS feeds, QuickBooks, Facebook, Google Ads.
[01:17:58.120 --> 01:18:04.120]   Imagine you have a whole process that automatically buys some Google Ads every time you announce
[01:18:04.120 --> 01:18:06.360]   something like this and that.
[01:18:06.360 --> 01:18:10.280]   You can automate just about any workflow imaginable.
[01:18:10.280 --> 01:18:11.360]   By the way, you're not on your own.
[01:18:11.360 --> 01:18:15.000]   They have thousands of templates to get you started to give you ideas to show you what
[01:18:15.000 --> 01:18:16.160]   to do.
[01:18:16.160 --> 01:18:20.120]   The average user, Zappier sends me an email every week saying how much time I spent.
[01:18:20.120 --> 01:18:26.760]   The average user saves over $10,000 in recovered time every year.
[01:18:26.760 --> 01:18:30.800]   Getting a Zappier subscription probably the best thing, the best money saver you could
[01:18:30.800 --> 01:18:32.160]   imagine.
[01:18:32.160 --> 01:18:37.440]   I wonder 1.8 million people in businesses, including me, use Zappier to streamline their
[01:18:37.440 --> 01:18:40.320]   work and find more time for what matters most.
[01:18:40.320 --> 01:18:41.320]   We use it like crazy.
[01:18:41.320 --> 01:18:46.080]   You can see for yourself, like Teams, at air table, in Dropbox, in HubSpot, in Zendesk,
[01:18:46.080 --> 01:18:50.560]   and thousands of other companies use Zappier every day to automate business.
[01:18:50.560 --> 01:18:55.920]   If you're just home automation, if you're doing stuff around the house, it's really
[01:18:55.920 --> 01:18:57.640]   fun to use Zappier.
[01:18:57.640 --> 01:19:00.640]   I have the lights dim at sunrise.
[01:19:00.640 --> 01:19:03.480]   There's just so many things you can do with Zappier.
[01:19:03.480 --> 01:19:06.240]   Try it for free today at zappier.com/twit.
[01:19:06.240 --> 01:19:08.080]   Yeah, free trial.
[01:19:08.080 --> 01:19:12.160]   Zappier, z-a-p-i-e-r.com/twit.
[01:19:12.160 --> 01:19:13.160]   It'll be amazing.
[01:19:13.160 --> 01:19:16.200]   You won't believe the things you can do with Zappier.
[01:19:16.200 --> 01:19:18.240]   Thank you, Zappier, for supporting Twit.
[01:19:18.240 --> 01:19:20.920]   Thank you for supporting us by using that address.
[01:19:20.920 --> 01:19:24.280]   Zappier.com/twit.
[01:19:24.280 --> 01:19:30.000]   This is a story I zapped this morning over to my pin board.
[01:19:30.000 --> 01:19:31.360]   I found this interesting.
[01:19:31.360 --> 01:19:37.880]   There was one line in it that was particularly interesting.
[01:19:37.880 --> 01:19:40.040]   You probably saw this in the news.
[01:19:40.040 --> 01:19:44.760]   The Department of Justice seized phones from two attorneys involved in the January 6th
[01:19:44.760 --> 01:19:45.760]   probe.
[01:19:45.760 --> 01:19:52.040]   John Eastman, the Trump campaign legal advisor, Jeffrey Clark, the former Justice Department
[01:19:52.040 --> 01:19:55.360]   official who was supposed to become attorney general.
[01:19:55.360 --> 01:19:57.200]   But this is where I found it interesting.
[01:19:57.200 --> 01:19:59.200]   There were iPhones.
[01:19:59.200 --> 01:20:07.400]   John Eastman claimed he left a restaurant on June 22nd and federal agents confronted
[01:20:07.400 --> 01:20:14.240]   him, took his iPhone 12 Pro, then served him with a warrant, and then, quote, "He was
[01:20:14.240 --> 01:20:19.840]   forced to provide biometric data to unlock the phone."
[01:20:19.840 --> 01:20:22.080]   Now, that stopped me right there in my tracks.
[01:20:22.080 --> 01:20:24.680]   I thought that was very interesting.
[01:20:24.680 --> 01:20:30.240]   First of all, the federal agents knew enough to not give him a moment to react, take the
[01:20:30.240 --> 01:20:32.680]   phone first before you give him the warrant.
[01:20:32.680 --> 01:20:35.280]   And then it's an iPhone 12.
[01:20:35.280 --> 01:20:41.320]   So I think what they probably did is say, "Hey, John, look here.
[01:20:41.320 --> 01:20:42.720]   Is this your phone?"
[01:20:42.720 --> 01:20:46.600]   And then it unlocked, and then they probably had a celebrate tool or something similar,
[01:20:46.600 --> 01:20:48.560]   plugged it in, sucked all the data out of it.
[01:20:48.560 --> 01:20:50.760]   Now, of course, they kept the phone.
[01:20:50.760 --> 01:20:52.520]   It's interesting for a couple of reasons.
[01:20:52.520 --> 01:20:59.840]   And January 6th was a while ago, like a year and a half ago.
[01:20:59.840 --> 01:21:02.520]   They still think there's something on that phone that's worth a judge, obviously.
[01:21:02.520 --> 01:21:04.520]   Did he wouldn't have signed the warrant?
[01:21:04.520 --> 01:21:05.520]   Yeah.
[01:21:05.520 --> 01:21:10.080]   Well, before we move on, I don't think it was, "Look at his, is this your phone?"
[01:21:10.080 --> 01:21:12.280]   Because they didn't say they tricked him into force.
[01:21:12.280 --> 01:21:13.520]   No, I think they compelled him.
[01:21:13.520 --> 01:21:15.520]   I think they forced him.
[01:21:15.520 --> 01:21:22.520]   So they stood on his chest and two people grabbed his ears.
[01:21:22.520 --> 01:21:23.520]   Who knows?
[01:21:23.520 --> 01:21:25.520]   I mean, what does that mean for us?
[01:21:25.520 --> 01:21:26.520]   I mean, that's just...
[01:21:26.520 --> 01:21:30.320]   I mean, they probably told him, and this is my guess.
[01:21:30.320 --> 01:21:34.960]   My guess is that they probably told him, "If you do not use your phone to unlock, use
[01:21:34.960 --> 01:21:37.520]   your face to unlock this, we will arrest you."
[01:21:37.520 --> 01:21:39.840]   Well, this is what I'm guessing happened.
[01:21:39.840 --> 01:21:40.840]   I mean, I don't know.
[01:21:40.840 --> 01:21:43.200]   But I'm guessing that that's how they compelled him to do it.
[01:21:43.200 --> 01:21:50.280]   So with other biometrics, like your fingerprint, or your password rather, they can't force
[01:21:50.280 --> 01:21:52.040]   you to give them the passcode.
[01:21:52.040 --> 01:21:54.160]   Well, and that's where this gets really interesting.
[01:21:54.160 --> 01:21:59.640]   So the Fourth Amendment, the Constitution protects you against unreasonable search and
[01:21:59.640 --> 01:22:05.480]   seizure and the Fifth Amendment protects you against incriminating yourself.
[01:22:05.480 --> 01:22:08.720]   And this is where courts have gone, by the way, in two different directions.
[01:22:08.720 --> 01:22:15.720]   But in general, the gist is biometrics is not self-incrimination.
[01:22:15.720 --> 01:22:22.280]   Putting your fingerprint on a fingerprint reader or your face in the face ID is not self-incrimination
[01:22:22.280 --> 01:22:25.680]   where giving a passcode would be.
[01:22:25.680 --> 01:22:31.640]   This is why everyone should learn the quick presses that you need to do with the unlock
[01:22:31.640 --> 01:22:32.640]   your phone.
[01:22:32.640 --> 01:22:33.640]   Exactly.
[01:22:33.640 --> 01:22:34.640]   The double tap.
[01:22:34.640 --> 01:22:35.640]   Although I can't.
[01:22:35.640 --> 01:22:38.680]   John Gruber had a piece in during fireballs saying, "Hey, by the way, just say, "I'm
[01:22:38.680 --> 01:22:39.680]   just so you don't know.
[01:22:39.680 --> 01:22:44.280]   Here's how you can lock your phone real quick before the feds seize it."
[01:22:44.280 --> 01:22:47.560]   I mean, I did that before I was, I do that when I go, when I'm in the airport, like I
[01:22:47.560 --> 01:22:49.080]   absolutely do that, right?
[01:22:49.080 --> 01:22:54.520]   Like, like, I, because I don't, and I've been doing that for years because I don't want
[01:22:54.520 --> 01:22:55.920]   to be in that situation.
[01:22:55.920 --> 01:22:59.940]   I will say, and I'm only being a little bit flippant here, but it would be nice if a constitutional
[01:22:59.940 --> 01:23:04.400]   law scholar, we can question many of his other things, but it would be nice if a constitutional
[01:23:04.400 --> 01:23:09.920]   law scholar would have maybe taken, would have maybe declined to unlock his phone and
[01:23:09.920 --> 01:23:11.200]   actually force this issue.
[01:23:11.200 --> 01:23:14.320]   Like, I'm sorry, but like Eastman is a constitutional law scholar.
[01:23:14.320 --> 01:23:15.320]   That's his business.
[01:23:15.320 --> 01:23:16.320]   Yeah.
[01:23:16.320 --> 01:23:17.320]   He teaches it.
[01:23:17.320 --> 01:23:18.320]   Yeah.
[01:23:18.320 --> 01:23:21.440]   To be, to be completely candid, like putting it how you feel about his other like credentials
[01:23:21.440 --> 01:23:22.760]   aside, I don't really care.
[01:23:22.760 --> 01:23:28.600]   I don't agree with him on many things, but it seems like this was a really missed opportunity
[01:23:28.600 --> 01:23:33.800]   for what could have been a really, you know, interesting legal case by, especially by someone
[01:23:33.800 --> 01:23:39.000]   with his credentials, if he had just basically said, no, I'm not going to unlock my phone.
[01:23:39.000 --> 01:23:40.000]   Yeah.
[01:23:40.000 --> 01:23:41.000]   I don't know.
[01:23:41.000 --> 01:23:42.000]   It's interesting that he did.
[01:23:42.000 --> 01:23:43.000]   I would like to think I would go to jail.
[01:23:43.000 --> 01:23:44.000]   Yeah.
[01:23:44.000 --> 01:23:46.840]   To me, to me, I would like to say, and I can't say this definitively, I've not been in this
[01:23:46.840 --> 01:23:47.840]   situation.
[01:23:47.840 --> 01:23:51.040]   I don't want to be in this situation, but to me, I would like to think that I would, I
[01:23:51.040 --> 01:23:52.440]   would spend a night in jail.
[01:23:52.440 --> 01:23:56.240]   He may have felt like that would look like incriminating behavior.
[01:23:56.240 --> 01:23:57.480]   I don't know.
[01:23:57.480 --> 01:23:59.200]   Like he had something to hide.
[01:23:59.200 --> 01:24:00.200]   I have to think.
[01:24:00.200 --> 01:24:01.200]   How's the constitutional law scholar?
[01:24:01.200 --> 01:24:02.200]   I don't know.
[01:24:02.200 --> 01:24:06.160]   Like a year and a half after January 6th, if there is still something on that phone, I
[01:24:06.160 --> 01:24:07.160]   be shocked.
[01:24:07.160 --> 01:24:08.160]   What could be on there?
[01:24:08.160 --> 01:24:09.160]   Right.
[01:24:09.160 --> 01:24:14.440]   I mean, I would think he would have, I mean, maybe, maybe he didn't think it, maybe he
[01:24:14.440 --> 01:24:15.440]   says I didn't do anything wrong.
[01:24:15.440 --> 01:24:16.440]   So I'm not going to wipe my phone.
[01:24:16.440 --> 01:24:18.280]   I'll keep all those texts.
[01:24:18.280 --> 01:24:20.200]   Evidence of witness intimidation.
[01:24:20.200 --> 01:24:21.200]   Yeah.
[01:24:21.200 --> 01:24:23.120]   Maybe because that's recent, right?
[01:24:23.120 --> 01:24:24.120]   Yeah.
[01:24:24.120 --> 01:24:25.120]   That's recent.
[01:24:25.120 --> 01:24:27.640]   Question is, yeah, none of these people, I mean, not these people, meaning, you know, the Trump
[01:24:27.640 --> 01:24:32.160]   officials, but nobody in government and nobody above a certain age just about.
[01:24:32.160 --> 01:24:34.480]   It's very clever with this data at all.
[01:24:34.480 --> 01:24:37.800]   You keep hearing these cases where people, you know, they didn't wipe anything.
[01:24:37.800 --> 01:24:41.560]   They didn't, you know, they're posting pictures with GPS coordinates.
[01:24:41.560 --> 01:24:45.880]   So you just, you want the raw data because it's often just right there.
[01:24:45.880 --> 01:24:46.880]   Right.
[01:24:46.880 --> 01:24:47.880]   Jug, Uber a week ago.
[01:24:47.880 --> 01:24:49.960]   Nothing from, we learned nothing from the mafia.
[01:24:49.960 --> 01:24:50.960]   I mean, genuinely.
[01:24:50.960 --> 01:25:00.520]   Well, I'm not, you know, I remember talking to the Secret Service agents, some many years
[01:25:00.520 --> 01:25:02.720]   ago, who said it's not a big deal.
[01:25:02.720 --> 01:25:04.800]   They almost always give us the password.
[01:25:04.800 --> 01:25:06.200]   It's like, right.
[01:25:06.200 --> 01:25:08.840]   Crooks like, yeah, fine, whatever.
[01:25:08.840 --> 01:25:10.760]   They're not, I don't know.
[01:25:10.760 --> 01:25:15.320]   Anyway, just in case you don't know.
[01:25:15.320 --> 01:25:19.480]   And by the way, again, there have been courts that have ruled that that you have to give
[01:25:19.480 --> 01:25:20.480]   that password.
[01:25:20.480 --> 01:25:24.960]   Who was it was in jail for two years for contempt because he didn't unlock his hard drive.
[01:25:24.960 --> 01:25:26.800]   He was a accused child.
[01:25:26.800 --> 01:25:27.800]   Oh, yeah.
[01:25:27.800 --> 01:25:31.080]   And then he was a law enforcement officer and he, he, I apparently thought there was something
[01:25:31.080 --> 01:25:32.080]   on that drive.
[01:25:32.080 --> 01:25:33.080]   He didn't want law enforcement.
[01:25:33.080 --> 01:25:35.740]   See law enforcement thought there was something on the drive they wanted to see.
[01:25:35.740 --> 01:25:38.880]   He refused to give his encryption password up, spend some time in jail.
[01:25:38.880 --> 01:25:42.240]   He's out of jail, but never did unlock that.
[01:25:42.240 --> 01:25:45.420]   But he wasn't prison for not giving up the password.
[01:25:45.420 --> 01:25:49.840]   And the, and the courts at the time said that is not self-incrimination.
[01:25:49.840 --> 01:25:54.800]   And it had something to do with whether the police knew that information was on there
[01:25:54.800 --> 01:25:56.360]   or it was a phishing expedition.
[01:25:56.360 --> 01:25:57.760]   Oh, I see.
[01:25:57.760 --> 01:26:01.360]   So it's, it's either way, it's a shorter, it's a shorter sentence for sure.
[01:26:01.360 --> 01:26:02.360]   Yeah.
[01:26:02.360 --> 01:26:03.360]   Too years, much easier.
[01:26:03.360 --> 01:26:04.360]   Yeah.
[01:26:04.360 --> 01:26:05.360]   Pornography on that.
[01:26:05.360 --> 01:26:06.360]   Yeah.
[01:26:06.360 --> 01:26:10.920]   So the theory is in Gruber's theory is what do you tap that?
[01:26:10.920 --> 01:26:11.920]   What do you do?
[01:26:11.920 --> 01:26:12.920]   You tap this twice.
[01:26:12.920 --> 01:26:14.800]   I can't even remember now.
[01:26:14.800 --> 01:26:16.560]   You can make it so that it's not.
[01:26:16.560 --> 01:26:18.560]   Yeah, I think I.
[01:26:18.560 --> 01:26:21.040]   Yeah, you just hit the button twice and basically.
[01:26:21.040 --> 01:26:25.960]   And then it will go into a mode where you have to enter the passcode to get in.
[01:26:25.960 --> 01:26:29.800]   And the theory is now you've protected it because the fourth amendment or the fifth
[01:26:29.800 --> 01:26:34.320]   amendment prevents them from forcing you to get them the passcode.
[01:26:34.320 --> 01:26:39.800]   Whereas the courts have ruled it is not incriminating to get your face ID.
[01:26:39.800 --> 01:26:42.160]   Oh, here's what it is.
[01:26:42.160 --> 01:26:43.160]   Here's what it is.
[01:26:43.160 --> 01:26:46.880]   If you're on like one of the newer phones, you press the lower volume button and then
[01:26:46.880 --> 01:26:51.160]   the side button at the same time until until you're going to keep, you're going to feel
[01:26:51.160 --> 01:26:52.800]   a feedback like if you've got the vibration.
[01:26:52.800 --> 01:26:53.800]   So you can have it your pocket.
[01:26:53.800 --> 01:26:55.360]   This is what Gruber says.
[01:26:55.360 --> 01:26:57.120]   You don't have to look at it.
[01:26:57.120 --> 01:26:58.120]   Nope.
[01:26:58.120 --> 01:26:59.120]   You just have it in your pocket.
[01:26:59.120 --> 01:27:01.120]   You go, here come the federal officers.
[01:27:01.120 --> 01:27:03.000]   You press the two and then it buzzes.
[01:27:03.000 --> 01:27:04.720]   And now it says slide to turn off.
[01:27:04.720 --> 01:27:06.320]   It doesn't matter what you do.
[01:27:06.320 --> 01:27:09.880]   You're going to have to do the passcode to get back into that.
[01:27:09.880 --> 01:27:10.880]   Exactly.
[01:27:10.880 --> 01:27:11.880]   Exactly.
[01:27:11.880 --> 01:27:12.880]   Yeah.
[01:27:12.880 --> 01:27:15.080]   So so you have it enabled us just basically these two buttons and then if you are an
[01:27:15.080 --> 01:27:20.880]   indicted co-expericer and the January 6th insurrection, you might want to make a note
[01:27:20.880 --> 01:27:23.720]   of that or to quote the famous XKCD strip.
[01:27:23.720 --> 01:27:27.520]   If someone brings up a wrench at you and that's how you get the password.
[01:27:27.520 --> 01:27:32.840]   You know, criminals or governments that don't believe in the, the, well, that's true.
[01:27:32.840 --> 01:27:35.400]   It's not going to work unless the rule of law is.
[01:27:35.400 --> 01:27:37.400]   It's a great.
[01:27:37.400 --> 01:27:40.720]   If you were in, you know, the January 6th people, you don't need to get their phone.
[01:27:40.720 --> 01:27:41.720]   They posted the evidence.
[01:27:41.720 --> 01:27:42.720]   Great.
[01:27:42.720 --> 01:27:44.320]   I was going to say they live streamed it.
[01:27:44.320 --> 01:27:46.600]   Like in many cases, they were literally live streaming it.
[01:27:46.600 --> 01:27:49.880]   So we're taking videos and posting them.
[01:27:49.880 --> 01:27:51.040]   Okay.
[01:27:51.040 --> 01:27:57.880]   Um, John says, never, ever hand your phone to a cop or anyone vaguely cop like, like the
[01:27:57.880 --> 01:28:00.000]   rent a cop's working for the TSA.
[01:28:00.000 --> 01:28:04.920]   If they tell you you must refuse, they can and will lie to you about this.
[01:28:04.920 --> 01:28:09.080]   If you really need to hand it over, they'll take it from you and they won't get anything
[01:28:09.080 --> 01:28:13.560]   from it because you've already hard locked it and you know, you cannot be required to
[01:28:13.560 --> 01:28:14.880]   give them your passcode.
[01:28:14.880 --> 01:28:21.480]   That really is kind of a, you know, a little bit of a optimistic point of view.
[01:28:21.480 --> 01:28:27.200]   Like they're going to have me in an airport jail and there and I'm, you can't give me
[01:28:27.200 --> 01:28:30.400]   argue, you can't require me to give you my passcode.
[01:28:30.400 --> 01:28:32.200]   Well here's a wrench.
[01:28:32.200 --> 01:28:33.200]   Here's your head.
[01:28:33.200 --> 01:28:35.360]   Would you like the two to have a meeting?
[01:28:35.360 --> 01:28:36.360]   Yeah.
[01:28:36.360 --> 01:28:40.400]   And then that's when they unplug the video camera and the interrogation.
[01:28:40.400 --> 01:28:41.400]   Yeah, exactly.
[01:28:41.400 --> 01:28:42.400]   Somehow I guess.
[01:28:42.400 --> 01:28:46.760]   Anyway, I, you know, reading this article, I, I, that caught my eye because he said they
[01:28:46.760 --> 01:28:51.400]   forced me to, to unlock the phone to give them biometric information.
[01:28:51.400 --> 01:28:52.560]   I thought that was very interesting.
[01:28:52.560 --> 01:28:53.960]   You're right, Christina.
[01:28:53.960 --> 01:28:56.560]   Here's a constitutional lawyer.
[01:28:56.560 --> 01:28:58.640]   Why did he do so?
[01:28:58.640 --> 01:29:04.120]   Although weirdly, the warrant I'm looking again here at this is for the CNN article
[01:29:04.120 --> 01:29:09.560]   here, the warrant said that they couldn't force him to give up biometric information.
[01:29:09.560 --> 01:29:11.480]   So he's claiming they did.
[01:29:11.480 --> 01:29:13.880]   Oh, that's why he's saying it voluntarily.
[01:29:13.880 --> 01:29:15.760]   Ah, now I get it.
[01:29:15.760 --> 01:29:17.960]   So then that could be the fruit of the poison to tree.
[01:29:17.960 --> 01:29:18.960]   There you go.
[01:29:18.960 --> 01:29:19.960]   Yeah.
[01:29:19.960 --> 01:29:21.960]   So he knows what he's doing.
[01:29:21.960 --> 01:29:24.360]   He is using his constitutional mocking.
[01:29:24.360 --> 01:29:25.360]   Uh huh.
[01:29:25.360 --> 01:29:26.360]   Uh huh.
[01:29:26.360 --> 01:29:29.000]   And then there, there may be a great deal of exaggeration.
[01:29:29.000 --> 01:29:31.600]   I mean, Rudy Giuliani said he wanted to assault it.
[01:29:31.600 --> 01:29:32.600]   I know.
[01:29:32.600 --> 01:29:37.320]   And somebody's, you know, so like, I had a whole conversation.
[01:29:37.320 --> 01:29:41.440]   My kids were laughing at Rudy Giuliani being hit and I was like, look, he's an old man.
[01:29:41.440 --> 01:29:44.160]   He's clearly not in good health.
[01:29:44.160 --> 01:29:45.600]   He's not a well me.
[01:29:45.600 --> 01:29:47.400]   Somebody came up and then I saw the footage.
[01:29:47.400 --> 01:29:48.920]   I'm like, oh, no, no, I'm sorry.
[01:29:48.920 --> 01:29:49.920]   I apologize.
[01:29:49.920 --> 01:29:52.000]   I didn't mean to be a hearted.
[01:29:52.000 --> 01:29:54.600]   I just don't want you to laugh at an old man being hit.
[01:29:54.600 --> 01:29:55.600]   I don't know.
[01:29:55.600 --> 01:29:56.600]   You're sorry.
[01:29:56.600 --> 01:29:58.360]   I, I, yeah.
[01:29:58.360 --> 01:29:59.960]   No one should be touched without their permission.
[01:29:59.960 --> 01:30:03.520]   I've had a lot of autonomy, but I'm like, all right, the guy just tapped.
[01:30:03.520 --> 01:30:05.480]   He just touched in the back and said scumbag.
[01:30:05.480 --> 01:30:06.480]   It's right.
[01:30:06.480 --> 01:30:07.480]   So I'm sorry.
[01:30:07.480 --> 01:30:08.480]   Keep me up.
[01:30:08.480 --> 01:30:11.280]   Uh, what do we think about crypto's crash?
[01:30:11.280 --> 01:30:12.280]   I'm just joking.
[01:30:12.280 --> 01:30:13.280]   Yay.
[01:30:13.280 --> 01:30:14.280]   Sorry.
[01:30:14.280 --> 01:30:15.280]   Yeah.
[01:30:15.280 --> 01:30:17.280]   19,000 to understand.
[01:30:17.280 --> 01:30:18.280]   You know what?
[01:30:18.280 --> 01:30:20.160]   I think it has a lot to do with how much Bitcoin you have.
[01:30:20.160 --> 01:30:22.720]   I was going to say, I think it depends on how exposed you are.
[01:30:22.720 --> 01:30:29.520]   I mean, look, my, my, my, my, my, my Robin Hood adventures with my Dogecoin stuff,
[01:30:29.520 --> 01:30:32.000]   which, look, I did get my initial money out of it.
[01:30:32.000 --> 01:30:36.280]   The additional stuff I put into it, I did not, but like it, I'm, I'm so down, which
[01:30:36.280 --> 01:30:39.120]   is, it's, it's hilarious to me, but I'm not exposed.
[01:30:39.120 --> 01:30:44.640]   Like, I, I, I do worry about the businesses, like legitimate businesses and banks and finance
[01:30:44.640 --> 01:30:49.440]   institutions that heavily exposed themselves to this stuff when it was going up and up and
[01:30:49.440 --> 01:30:50.440]   up.
[01:30:50.440 --> 01:30:56.840]   Like even if we all, if the collective wisdom on this panel is to be more skeptical of, of
[01:30:56.840 --> 01:31:02.520]   crypto, um, if not against it, I tend to be more on the skeptical side rather than just
[01:31:02.520 --> 01:31:03.520]   anti.
[01:31:03.520 --> 01:31:08.320]   I think that we can agree that having this level of the kind of downturn is not great.
[01:31:08.320 --> 01:31:09.880]   I mean, it's, it's, there's.
[01:31:09.880 --> 01:31:13.320]   Well, I feel for anybody who lost the rent money on it.
[01:31:13.320 --> 01:31:20.640]   Uh, I, I think we've been at least in the last year or two pretty clear that you would
[01:31:20.640 --> 01:31:28.600]   be mistaken to, to put Bitcoin in your 401k, let's say, um, and I even am pretty down on
[01:31:28.600 --> 01:31:29.600]   NFTs.
[01:31:29.600 --> 01:31:32.240]   I feel like NFTs are absolute scams.
[01:31:32.240 --> 01:31:33.640]   What were you going to say, Glenn?
[01:31:33.640 --> 01:31:34.640]   Yeah.
[01:31:34.640 --> 01:31:35.640]   Go on.
[01:31:35.640 --> 01:31:38.160]   It's just, when you have celebrities, I think that this, oh yeah, Matt Damon saying,
[01:31:38.160 --> 01:31:40.160]   right, for you, it's the great.
[01:31:40.160 --> 01:31:41.160]   It's the great.
[01:31:41.160 --> 01:31:42.160]   Yeah.
[01:31:42.160 --> 01:31:43.160]   This is Howard.
[01:31:43.160 --> 01:31:44.160]   Bye Bitcoin.
[01:31:44.160 --> 01:31:48.320]   I think this was the joke before the, the joke quote unquote before the great depression
[01:31:48.320 --> 01:31:52.840]   was like when your cab driver is giving you stock tips, you know, that the market's about
[01:31:52.840 --> 01:31:53.840]   to fall, right?
[01:31:53.840 --> 01:31:54.840]   Yeah.
[01:31:54.840 --> 01:31:55.840]   That's right.
[01:31:55.840 --> 01:31:58.920]   It's not that individuals, uh, shouldn't be invested in the broader market because the
[01:31:58.920 --> 01:32:04.640]   market as a whole, over any long period of time, like there's a 15 plus years, it has
[01:32:04.640 --> 01:32:08.480]   a high rate of return relative to almost every other thing you could put your money
[01:32:08.480 --> 01:32:13.760]   into, but it's when you buy individual stocks or bonds, you invest in gold or other, you
[01:32:13.760 --> 01:32:19.000]   know, commodities or something as completely speculative as, as Bitcoin or Ethereum, you,
[01:32:19.000 --> 01:32:23.440]   you're almost guaranteed to fail over the short term, even if there's a good long term
[01:32:23.440 --> 01:32:28.120]   response because you can't, unless you have a certain amount of money, you can't summon
[01:32:28.120 --> 01:32:30.560]   the resources to stay in during the downtime.
[01:32:30.560 --> 01:32:35.000]   So I mean, that's, so I feel for all the people who were talked into it by celebrities where
[01:32:35.000 --> 01:32:37.880]   they're like, oh, well, this wouldn't be on television and Matt Damon or other people
[01:32:37.880 --> 01:32:43.160]   wouldn't do this because they, you know, this isn't, um, I don't know, we're not talking
[01:32:43.160 --> 01:32:48.880]   about, uh, so Dan Aykroyd and his crystal skull vodka, which I have a bottle of, by
[01:32:48.880 --> 01:32:49.880]   the way, good.
[01:32:49.880 --> 01:32:55.640]   I have, I'm an open it, but I did, Dan Aykroyd, uh, did autograph it, but then I smudged
[01:32:55.640 --> 01:32:57.720]   it because it was on glass.
[01:32:57.720 --> 01:33:04.160]   So I have a, I have a bottle signed by, I'm, I'm, I'm, I'm sorry, but I just mean that,
[01:33:04.160 --> 01:33:08.240]   like, you know, when you get, when you get retail investors being convinced that this
[01:33:08.240 --> 01:33:13.600]   is a investment that can't go down or be promised things in a way that, um, there's a dispute
[01:33:13.600 --> 01:33:17.200]   on blanket, which currency right now is, is frozen withdrawals.
[01:33:17.200 --> 01:33:22.880]   And they had a literal FDIC inaccurate, uh, thing that said your deposits are protected
[01:33:22.880 --> 01:33:23.880]   by the FDS.
[01:33:23.880 --> 01:33:25.960]   And they're not, they're not, they weren't.
[01:33:25.960 --> 01:33:30.160]   And people are posting and I think they're claiming that they didn't have anything.
[01:33:30.160 --> 01:33:33.280]   I don't want to mention the name of the company because I can't remember the name at the moment,
[01:33:33.280 --> 01:33:36.880]   but the nuance was like they're claiming they never did and people are posting pictures
[01:33:36.880 --> 01:33:39.800]   from, uh, you know, chat sessions with customer support.
[01:33:39.800 --> 01:33:44.040]   They're posting pictures from our internet archive of what they said.
[01:33:44.040 --> 01:33:46.800]   So if you're an individual, how do you deal with fraud?
[01:33:46.800 --> 01:33:51.200]   So, so I feel for all those people, the winkle voice, voice twins, do I feel for them, they
[01:33:51.200 --> 01:33:57.400]   are probably still up like 12,000% even if they're down 60% for the year.
[01:33:57.400 --> 01:33:58.400]   Yeah.
[01:33:58.400 --> 01:34:02.320]   And they have a band and unfortunately their band is not called the winkle by, but, but
[01:34:02.320 --> 01:34:08.080]   I think, you know, the problem, the problem is that as you mentioned, Glenn, everything
[01:34:08.080 --> 01:34:12.560]   you said points to the fact that everybody looks at it like an, like an investment is
[01:34:12.560 --> 01:34:14.280]   supposed to be a currency.
[01:34:14.280 --> 01:34:19.240]   And, and, and this is that this, this sort of sort of get rid, the desire to get rich
[01:34:19.240 --> 01:34:24.240]   quick without employing anyone, without building anything, making anything, feeding anyone,
[01:34:24.240 --> 01:34:25.240]   is it?
[01:34:25.240 --> 01:34:31.800]   And, and with, I think most of us on this show talking to a blue in the face about what a
[01:34:31.800 --> 01:34:36.920]   ultimately a dangerous risky thing it is, Leo, you're the best poster child for the risks
[01:34:36.920 --> 01:34:39.120]   of, of password management.
[01:34:39.120 --> 01:34:40.920]   I still have my wallet.
[01:34:40.920 --> 01:34:42.320]   Let me point out, I got it.
[01:34:42.320 --> 01:34:44.360]   I just, I can't get into it.
[01:34:44.360 --> 01:34:45.880]   You just can't get into it.
[01:34:45.880 --> 01:34:47.880]   But you were, you were, you know, my wallet is empty.
[01:34:47.880 --> 01:34:48.880]   I see it.
[01:34:48.880 --> 01:34:49.880]   I want to, that's right.
[01:34:49.880 --> 01:34:53.440]   You get a replace by any leather wallet you want.
[01:34:53.440 --> 01:34:55.800]   I mean, the fact is, is the value goes down.
[01:34:55.800 --> 01:34:57.040]   People want to buy in and set it right.
[01:34:57.040 --> 01:35:01.520]   May, may come back, may not buy the dips as long as the tips are not the bottom, are
[01:35:01.520 --> 01:35:03.680]   not, are the bottom and not the top.
[01:35:03.680 --> 01:35:05.360]   I stuck some more money into my note.
[01:35:05.360 --> 01:35:09.320]   This is not financial advice, but I stuck some more money to my rah when it took a big
[01:35:09.320 --> 01:35:12.640]   dive because I'm like, well, this is the, you know, you're young yet.
[01:35:12.640 --> 01:35:15.600]   I mean, I've lost 21% on my retirement.
[01:35:15.600 --> 01:35:16.840]   Oh, just hold on.
[01:35:16.840 --> 01:35:17.840]   Hold on, Leo.
[01:35:17.840 --> 01:35:18.840]   It'll come back.
[01:35:18.840 --> 01:35:20.920]   Oh, it just means I have to work for another 10 years.
[01:35:20.920 --> 01:35:21.920]   That's all.
[01:35:21.920 --> 01:35:22.920]   Oh, my gosh.
[01:35:22.920 --> 01:35:26.320]   Over any, you know, over any extended period of time, the stock market on average, that's
[01:35:26.320 --> 01:35:27.320]   right.
[01:35:27.320 --> 01:35:28.640]   It's performed and it performs very well.
[01:35:28.640 --> 01:35:33.200]   So it's just, do you have to wear with all to not go, you know, when this happens, it's
[01:35:33.200 --> 01:35:34.200]   very hard.
[01:35:34.200 --> 01:35:35.200]   Yeah.
[01:35:35.200 --> 01:35:39.360]   And some people don't have the money to not, you know, or they've got my favorite ad was
[01:35:39.360 --> 01:35:42.760]   the FTX ad with Larry David on the Super Bowl.
[01:35:42.760 --> 01:35:45.640]   I think that's the equivalent of a cab driver giving you a tip.
[01:35:45.640 --> 01:35:49.600]   When you see these crypto companies starting to advertise in the Super Bowl, you know,
[01:35:49.600 --> 01:35:51.440]   we're somehow at this peak thing.
[01:35:51.440 --> 01:35:52.440]   100%
[01:35:52.440 --> 01:35:53.440]   Bubble.
[01:35:53.440 --> 01:35:59.480]   Larry David as the boomer saying, oh, the wheel is a terrible idea coming them.
[01:35:59.480 --> 01:36:00.480]   Everything's a terrible idea.
[01:36:00.480 --> 01:36:02.240]   And then as we've got a kind of terrible idea.
[01:36:02.240 --> 01:36:03.720]   And I know what I'm talking about.
[01:36:03.720 --> 01:36:10.240]   I would never get, which was mocking people of my age for saying, oh, crypto is no good.
[01:36:10.240 --> 01:36:13.320]   And the implication is, oh, you, okay, boomer.
[01:36:13.320 --> 01:36:16.600]   You just don't understand the modern world.
[01:36:16.600 --> 01:36:21.360]   Well now there's a lot of boomers going, uh, I don't think Larry David, by the way, was
[01:36:21.360 --> 01:36:23.520]   paid in Bitcoin for that commercial.
[01:36:23.520 --> 01:36:28.280]   No, it didn't get me interested though in FTX, which by the way is now buying up all the
[01:36:28.280 --> 01:36:29.680]   failing all the other ones.
[01:36:29.680 --> 01:36:33.000]   I was going to say, this is a very interesting company.
[01:36:33.000 --> 01:36:38.360]   Sam Bankman freed who founded it is 30 years old.
[01:36:38.360 --> 01:36:39.800]   He's worth $24 billion.
[01:36:39.800 --> 01:36:47.520]   Of course, according to Forbes, he got out of, uh, he was in at Stanford.
[01:36:47.520 --> 01:36:50.280]   His parents were professors at Stanford Law School.
[01:36:50.280 --> 01:36:52.440]   He was born in the Stanford campus.
[01:36:52.440 --> 01:36:55.200]   He went to MIT.
[01:36:55.200 --> 01:37:00.920]   He blogged, I love this Wikipedia post, he blogged in 2012 about utilitarianism, baseball
[01:37:00.920 --> 01:37:07.400]   and politics, graduated with a degree in physics, a minor in math, started working at a capital
[01:37:07.400 --> 01:37:10.600]   firm, Jane Street Capital in 2013.
[01:37:10.600 --> 01:37:16.800]   He quit in 2017, moved to Berkeley, get this, where he briefly worked for the Center for
[01:37:16.800 --> 01:37:21.720]   Effective Altruism, uh, a nonprofit.
[01:37:21.720 --> 01:37:29.280]   But in 2017, he founded Alameda Research, which is a quant trading firm in crypto.
[01:37:29.280 --> 01:37:31.560]   He's one of the guys who made the most money.
[01:37:31.560 --> 01:37:32.560]   Right.
[01:37:32.560 --> 01:37:37.360]   He got in, he got in like, he got in right before, I guess, like, he could like timing.
[01:37:37.360 --> 01:37:38.360]   Yeah.
[01:37:38.360 --> 01:37:41.080]   Cause he got in before 2017, when that had that peak, right?
[01:37:41.080 --> 01:37:42.080]   But then it fell down.
[01:37:42.080 --> 01:37:48.280]   He probably reinvested and he was made a lot of money in arbitrage, taking advantage of
[01:37:48.280 --> 01:37:51.960]   the higher price in Japan compared to America.
[01:37:51.960 --> 01:37:56.480]   So he'd buy in America, sell in Japan and make money that way.
[01:37:56.480 --> 01:38:05.440]   Uh, he founded FTX in 2019, three years ago, now worth $24 billion and is buying up all
[01:38:05.440 --> 01:38:08.440]   of the failing cryptocurrency exchanges.
[01:38:08.440 --> 01:38:12.480]   Oh, he may make an offer for Robin Hood as the latest is, yeah.
[01:38:12.480 --> 01:38:13.880]   We're putting it on there a few days.
[01:38:13.880 --> 01:38:15.440]   And he paid, uh, what was it?
[01:38:15.440 --> 01:38:21.320]   He paid pennies on the dollar for, uh, block five, which was one of our advertisers.
[01:38:21.320 --> 01:38:24.520]   I should mention, uh, block five, which was worth it.
[01:38:24.520 --> 01:38:26.000]   1.0 billions.
[01:38:26.000 --> 01:38:32.200]   Uh, it was ended up, he gave them 250 million loans, them $250 million to stay solvent and
[01:38:32.200 --> 01:38:33.920]   then took advantage of that.
[01:38:33.920 --> 01:38:37.080]   And I saw one report that said they bought it for $25 million.
[01:38:37.080 --> 01:38:38.560]   Yeah, I saw that too.
[01:38:38.560 --> 01:38:42.840]   I don't think that's, I think it's more than that, but still talked about pennies on the
[01:38:42.840 --> 01:38:43.840]   dollar.
[01:38:43.840 --> 01:38:48.440]   I think we, when I was on a few weeks ago, maybe, but it just keeps coming back is I
[01:38:48.440 --> 01:38:53.880]   think the real underlying problem isn't necessarily, uh, all the various, uh, currencies.
[01:38:53.880 --> 01:38:55.200]   It's the stablecoins, right?
[01:38:55.200 --> 01:38:56.200]   It's like the false, right?
[01:38:56.200 --> 01:38:58.440]   Well, and Tara, Tara was the one you were thinking of.
[01:38:58.440 --> 01:39:00.640]   I think that, oh yeah, people thought it was back.
[01:39:00.640 --> 01:39:01.640]   Yeah.
[01:39:01.640 --> 01:39:02.640]   By, uh, yeah.
[01:39:02.640 --> 01:39:08.400]   And I mean, algorithmic and the supposedly reserve based stablecoins are what pushed up
[01:39:08.400 --> 01:39:12.960]   valuation and we're seeing it completely unwind as those, the trust ebbs out and you're having
[01:39:12.960 --> 01:39:13.960]   it.
[01:39:13.960 --> 01:39:20.400]   But then watch because then you have people like Deutsche Bank, which just said, oh, no,
[01:39:20.400 --> 01:39:26.800]   no, no, this is this big Bitcoin is going to the moon.
[01:39:26.800 --> 01:39:28.080]   Hold on to it.
[01:39:28.080 --> 01:39:32.320]   They say the $2 trillion crypto crash could be coming to an end.
[01:39:32.320 --> 01:39:36.560]   They compare Bitcoin to the $72 billion a year diamond industry.
[01:39:36.560 --> 01:39:40.600]   You might wonder how much how many Bitcoin, uh, Deutsche Bank has.
[01:39:40.600 --> 01:39:42.880]   I bet you they know their wallets password.
[01:39:42.880 --> 01:39:46.600]   Well, they're also getting a lot of agreement.
[01:39:46.600 --> 01:39:50.800]   They're also getting a lot of agreement from the president of El Salvador who just, he said,
[01:39:50.800 --> 01:39:51.800]   thanks for the cheap Bitcoins.
[01:39:51.800 --> 01:39:53.400]   And he bought another 1.5.
[01:39:53.400 --> 01:39:56.920]   He bought more double down.
[01:39:56.920 --> 01:40:00.040]   I really want to see a bank in Germany talk about hyperinflation of currency.
[01:40:00.040 --> 01:40:01.040]   I know.
[01:40:01.040 --> 01:40:02.040]   Point.
[01:40:02.040 --> 01:40:03.040]   It's going to be great.
[01:40:03.040 --> 01:40:06.240]   You're going to have so much money and have it in real barrels.
[01:40:06.240 --> 01:40:07.240]   It'll be amazing.
[01:40:07.240 --> 01:40:08.240]   Yes.
[01:40:08.240 --> 01:40:11.120]   You'll literally have to use little barrels to buy your zucchini.
[01:40:11.120 --> 01:40:12.120]   Right.
[01:40:12.120 --> 01:40:13.120]   Oh, God.
[01:40:13.120 --> 01:40:14.120]   Let's take a little break.
[01:40:14.120 --> 01:40:15.120]   Lots more to talk about.
[01:40:15.120 --> 01:40:16.440]   Great panel today.
[01:40:16.440 --> 01:40:17.440]   Glenn Fleischman.
[01:40:17.440 --> 01:40:23.800]   So glad to have you the expert in so many things, including typography and what are they
[01:40:23.800 --> 01:40:24.800]   called?
[01:40:24.800 --> 01:40:32.000]   Fogles, Fugles, flongs, those little things that have, uh, you know, it's a flong master.
[01:40:32.000 --> 01:40:34.160]   Get ready for this.
[01:40:34.160 --> 01:40:36.320]   You collect sneakers, you collect flongs.
[01:40:36.320 --> 01:40:38.800]   Hundreds of peanuts flongs.
[01:40:38.800 --> 01:40:40.840]   Oh my gosh.
[01:40:40.840 --> 01:40:46.560]   Printing plates from Sweden where there are bees and English language Swedish newspaper.
[01:40:46.560 --> 01:40:51.120]   And so here are peanuts comics from the 1970s in color separations.
[01:40:51.120 --> 01:40:53.720]   This is the black plate for color Sunday strip.
[01:40:53.720 --> 01:40:57.520]   So I've been scanning these and then you're going to have some more.
[01:40:57.520 --> 01:40:58.520]   What is the copyright?
[01:40:58.520 --> 01:41:00.520]   Is the Charles Schulz foundation?
[01:41:00.520 --> 01:41:02.040]   Are you going to come after you here?
[01:41:02.040 --> 01:41:06.880]   I mean, it's an interesting thing as an artifact that can certainly scan it, but it can't sell
[01:41:06.880 --> 01:41:07.880]   or reap rights.
[01:41:07.880 --> 01:41:11.080]   It's a very complicated, there must be some fair use issues.
[01:41:11.080 --> 01:41:12.880]   And then here's an alley oop cartoon.
[01:41:12.880 --> 01:41:13.880]   Oh man.
[01:41:13.880 --> 01:41:14.880]   Look at that.
[01:41:14.880 --> 01:41:16.920]   It'll be used to create part of the newspaper printing process.
[01:41:16.920 --> 01:41:20.560]   I just got this is from the 60s, but then here's the best thing.
[01:41:20.560 --> 01:41:22.480]   It's a ridiculous thing.
[01:41:22.480 --> 01:41:25.120]   It's from 1917.
[01:41:25.120 --> 01:41:27.680]   Someone I knows that I love flongs.
[01:41:27.680 --> 01:41:32.240]   So this is again a printing plate raised to type high that would have used it in a newspaper.
[01:41:32.240 --> 01:41:35.760]   And it's a terrible, terrible old strip.
[01:41:35.760 --> 01:41:40.320]   It's a really like one of these horrible earl, but it's 1917.
[01:41:40.320 --> 01:41:41.840]   So no one would have made comic strips yet.
[01:41:41.840 --> 01:41:43.840]   They're like the punchlines are confusing.
[01:41:43.840 --> 01:41:49.000]   Anyway, but it's little pieces of comics in the kitchen history.
[01:41:49.000 --> 01:41:50.000]   I love it.
[01:41:50.000 --> 01:41:51.000]   It's a museum.
[01:41:51.000 --> 01:41:54.280]   Now, are you actually in your office or is that a green screen?
[01:41:54.280 --> 01:41:55.440]   This is no, this is really stuff.
[01:41:55.440 --> 01:41:56.440]   I can put it on the shelf.
[01:41:56.440 --> 01:41:57.440]   Oh, wow.
[01:41:57.440 --> 01:41:58.440]   Look, the book move.
[01:41:58.440 --> 01:41:59.440]   I know I fooled you once.
[01:41:59.440 --> 01:42:01.200]   First time he was on, he completely tricked me.
[01:42:01.200 --> 01:42:04.400]   He looked exactly the same and then he reaches baggage stories.
[01:42:04.400 --> 01:42:06.920]   I was using bokeh, a bokeh photo.
[01:42:06.920 --> 01:42:07.920]   It's beautiful.
[01:42:07.920 --> 01:42:08.920]   It's beautiful.
[01:42:08.920 --> 01:42:09.920]   Brilliant.
[01:42:09.920 --> 01:42:12.920]   This is what I'm calling the Joe Schlebotnik Flong and Stereotype Museum.
[01:42:12.920 --> 01:42:14.800]   That's my working title.
[01:42:14.800 --> 01:42:15.800]   Thank you, Joe.
[01:42:15.800 --> 01:42:16.800]   Those were Snoopy fans.
[01:42:16.800 --> 01:42:17.800]   Nice.
[01:42:17.800 --> 01:42:20.000]   I never knew that's how you pronounce it.
[01:42:20.000 --> 01:42:21.000]   Schlebotnik.
[01:42:21.000 --> 01:42:22.000]   Maybe.
[01:42:22.000 --> 01:42:23.000]   Schlebot.
[01:42:23.000 --> 01:42:24.000]   We don't know.
[01:42:24.000 --> 01:42:25.000]   We don't know.
[01:42:25.000 --> 01:42:26.000]   Also, Mike Elgin is here.
[01:42:26.000 --> 01:42:28.160]   Not net if you want to go on these great trips.
[01:42:28.160 --> 01:42:31.280]   The next one in Pervos this fall.
[01:42:31.280 --> 01:42:32.280]   That's right.
[01:42:32.280 --> 01:42:33.920]   Amazing, amazing.
[01:42:33.920 --> 01:42:37.520]   We went last fall to Oaxaca with you guys and it was the best thing.
[01:42:37.520 --> 01:42:39.240]   It was so much fun.
[01:42:39.240 --> 01:42:41.880]   We're going to be hanging out with some of the people you met there.
[01:42:41.880 --> 01:42:43.240]   So, I'll tell them you said hi.
[01:42:43.240 --> 01:42:44.240]   Please do.
[01:42:44.240 --> 01:42:45.240]   Please do.
[01:42:45.240 --> 01:42:52.240]   I follow Julia and Charlie on Instagram and I was watching all of you guys in Pervos.
[01:42:52.240 --> 01:42:55.960]   And I thought, oh, that lavender looks good enough to eat.
[01:42:55.960 --> 01:42:56.960]   It's so much fun.
[01:42:56.960 --> 01:42:57.960]   It's so much fun.
[01:42:57.960 --> 01:42:58.960]   Yes.
[01:42:58.960 --> 01:43:04.120]   And by the way, if I can do a quick plug, the Pervos experience in the fall has.
[01:43:04.120 --> 01:43:05.120]   We still have an open room.
[01:43:05.120 --> 01:43:06.120]   Oh, good.
[01:43:06.120 --> 01:43:12.520]   So, if anybody wants to join us, go to the website, gasnowmed.net, shoot me a line and
[01:43:12.520 --> 01:43:14.040]   you could grab that last room.
[01:43:14.040 --> 01:43:17.440]   It's going to be mind-blowingly beautiful.
[01:43:17.440 --> 01:43:23.200]   And you'll be amongst friends because there are many twit listeners in the groups.
[01:43:23.200 --> 01:43:26.800]   Just by coincidence, somehow, magically, it just happens.
[01:43:26.800 --> 01:43:28.360]   Yeah, it's a lot of fun.
[01:43:28.360 --> 01:43:31.160]   Well, there's Julia herself with a baby goat.
[01:43:31.160 --> 01:43:32.760]   That's beautiful.
[01:43:32.760 --> 01:43:33.760]   Oh.
[01:43:33.760 --> 01:43:35.960]   Christina Warren also here, film girl.
[01:43:35.960 --> 01:43:36.960]   We love Christina.
[01:43:36.960 --> 01:43:40.000]   It's great to have you and your sneakers on the show.
[01:43:40.000 --> 01:43:41.480]   Great to be here.
[01:43:41.480 --> 01:43:43.480]   And your travels are done now for a while.
[01:43:43.480 --> 01:43:44.480]   Are you going to?
[01:43:44.480 --> 01:43:45.480]   For now.
[01:43:45.480 --> 01:43:50.600]   I'm not sure when my next thing will be, but I will be traveling more, obviously, as
[01:43:50.600 --> 01:43:53.520]   the year and next year goes on.
[01:43:53.520 --> 01:43:54.520]   That's good.
[01:43:54.520 --> 01:43:55.520]   For GitHub, it's different.
[01:43:55.520 --> 01:43:56.520]   You go to build.
[01:43:56.520 --> 01:43:58.560]   You go to those events or is it something different now?
[01:43:58.560 --> 01:44:00.960]   So we have GitHub Universe.
[01:44:00.960 --> 01:44:02.720]   So that is in November.
[01:44:02.720 --> 01:44:04.760]   And that will be at the Moscone Center in San Francisco.
[01:44:04.760 --> 01:44:05.760]   Oh, fun.
[01:44:05.760 --> 01:44:06.760]   You'll be there.
[01:44:06.760 --> 01:44:07.760]   It's down this way.
[01:44:07.760 --> 01:44:08.760]   Nice.
[01:44:08.760 --> 01:44:09.760]   Absolutely.
[01:44:09.760 --> 01:44:10.760]   Absolutely.
[01:44:10.760 --> 01:44:14.760]   And yeah, just more things, more events are happening in person and opening up and
[01:44:14.760 --> 01:44:18.920]   we're just kind of figuring out, like our fiscal year just started this week.
[01:44:18.920 --> 01:44:22.360]   And so we're figuring out where we want to be and whatnot.
[01:44:22.360 --> 01:44:23.360]   Nice.
[01:44:23.360 --> 01:44:26.920]   And some people would rather stay home and would rather be virtual.
[01:44:26.920 --> 01:44:28.160]   I would rather be out.
[01:44:28.160 --> 01:44:29.160]   You like travel.
[01:44:29.160 --> 01:44:31.040]   So I do like to travel.
[01:44:31.040 --> 01:44:36.800]   And it was really great being able to be around people again and meet up groups and
[01:44:36.800 --> 01:44:37.800]   things like that.
[01:44:37.800 --> 01:44:43.000]   It was really nice to be able to have that experience again because I've missed it a
[01:44:43.000 --> 01:44:44.000]   lot.
[01:44:44.000 --> 01:44:49.000]   Of course, we missed film girls impromptu hotel tours.
[01:44:49.000 --> 01:44:50.000]   Yes.
[01:44:50.000 --> 01:44:51.000]   Yes.
[01:44:51.000 --> 01:44:56.040]   So I'm glad you're I'm glad you're back doing that and showing showing the world what it's
[01:44:56.040 --> 01:44:58.160]   like to be Christina Warren.
[01:44:58.160 --> 01:45:03.000]   I was going to say I was going to say people shockingly it's probably been some of those
[01:45:03.000 --> 01:45:07.200]   popular content I've ever done where I for years I was doing this before reals existed
[01:45:07.200 --> 01:45:08.200]   on Instagram.
[01:45:08.200 --> 01:45:09.200]   I might bring it to TikTok.
[01:45:09.200 --> 01:45:10.200]   I don't know.
[01:45:10.200 --> 01:45:12.040]   But I was doing them as stories.
[01:45:12.040 --> 01:45:14.680]   Now I'm doing them as reals and I might bring it to TikTok.
[01:45:14.680 --> 01:45:19.640]   But I do by Christina's hotel room tours where I just do a tour of the hotel room that I
[01:45:19.640 --> 01:45:20.640]   happen to be in.
[01:45:20.640 --> 01:45:22.640]   I started doing it.
[01:45:22.640 --> 01:45:23.640]   Yeah.
[01:45:23.640 --> 01:45:27.200]   People shockingly have had people come up to me at like conferences and things people
[01:45:27.200 --> 01:45:30.680]   who don't really know me and like, Oh, I love your hotel tours.
[01:45:30.680 --> 01:45:31.680]   So funny.
[01:45:31.680 --> 01:45:32.680]   You just don't know.
[01:45:32.680 --> 01:45:37.920]   I just had a friend who just took I took him four days to get from Virginia back to
[01:45:37.920 --> 01:45:38.920]   Portland.
[01:45:38.920 --> 01:45:39.920]   Travel now.
[01:45:39.920 --> 01:45:40.920]   Travel.
[01:45:40.920 --> 01:45:44.120]   I have not been on the road.
[01:45:44.120 --> 01:45:45.280]   So I've missed it so far.
[01:45:45.280 --> 01:45:48.560]   And what's interesting, you know, Friday, which was the first day of the Fourth of July
[01:45:48.560 --> 01:45:51.640]   weekend, by the way, happy Independence Day to our American viewers.
[01:45:51.640 --> 01:45:52.640]   Yeah.
[01:45:52.640 --> 01:45:53.640]   Happy Canada Day.
[01:45:53.640 --> 01:46:00.160]   A couple of days ago to our Canadian Friday, which was the beginning of the weekend.
[01:46:00.160 --> 01:46:03.200]   There were more people went through the TSA lines than they did in 2019.
[01:46:03.200 --> 01:46:04.800]   It's actually gone up.
[01:46:04.800 --> 01:46:05.800]   Yeah.
[01:46:05.800 --> 01:46:07.800]   So it travels back.
[01:46:07.800 --> 01:46:08.800]   Travel's back.
[01:46:08.800 --> 01:46:11.720]   But yeah, travel's back, but we there are.
[01:46:11.720 --> 01:46:12.720]   It's our members.
[01:46:12.720 --> 01:46:14.840]   I was going to say the pilots aren't numbers.
[01:46:14.840 --> 01:46:18.240]   Aren't TSA people aren't like it's the whole of things.
[01:46:18.240 --> 01:46:19.240]   This is what happened.
[01:46:19.240 --> 01:46:23.080]   They're calling it air McGedden air McGedden, baby.
[01:46:23.080 --> 01:46:28.280]   We have a fun situation as we got global entry just before literally January 2020, the
[01:46:28.280 --> 01:46:33.280]   whole family, all four of us went out to the Canadian border into a little vacation.
[01:46:33.280 --> 01:46:34.280]   This is great.
[01:46:34.280 --> 01:46:35.280]   It's January 2020.
[01:46:35.280 --> 01:46:37.640]   We have this wonderful trip plan this summer.
[01:46:37.640 --> 01:46:39.040]   So much travel ahead of us.
[01:46:39.040 --> 01:46:43.720]   We thought to ourselves, but global entry, once you're in it, you have to go back up
[01:46:43.720 --> 01:46:46.160]   there when your passport changes, all this stuff.
[01:46:46.160 --> 01:46:51.880]   So we can't use it now when we start traveling again without going back to this small town
[01:46:51.880 --> 01:46:52.880]   near Canada.
[01:46:52.880 --> 01:46:54.880]   You have to go to the same place that issued it.
[01:46:54.880 --> 01:46:55.880]   Yes.
[01:46:55.880 --> 01:46:56.880]   And you can't.
[01:46:56.880 --> 01:46:57.880]   I don't know that.
[01:46:57.880 --> 01:47:00.720]   No, I was going to say, I think you can just, I think you can just link it back to your
[01:47:00.720 --> 01:47:03.960]   new passport because I have a passport on your thing.
[01:47:03.960 --> 01:47:05.320]   Yeah, I'm almost positive.
[01:47:05.320 --> 01:47:06.320]   Absolutely.
[01:47:06.320 --> 01:47:11.640]   It's more than me, but I'm 99% sure that all you have to do is update your forms on the
[01:47:11.640 --> 01:47:14.000]   global entry website to link to the new passport.
[01:47:14.000 --> 01:47:15.000]   Oh, no, you know what?
[01:47:15.000 --> 01:47:16.000]   We have Nexus.
[01:47:16.000 --> 01:47:17.000]   We don't have global entry.
[01:47:17.000 --> 01:47:18.000]   Oh, that's the problem.
[01:47:18.000 --> 01:47:19.000]   You have the Canadian border.
[01:47:19.000 --> 01:47:20.000]   You have the Canadian border.
[01:47:20.000 --> 01:47:21.000]   It seems so small.
[01:47:21.000 --> 01:47:22.000]   We're so close to Canada.
[01:47:22.000 --> 01:47:24.000]   There's a guy in a mountain hat.
[01:47:24.000 --> 01:47:25.160]   It's a whole different thing.
[01:47:25.160 --> 01:47:27.160]   It doesn't have a very nice about it.
[01:47:27.160 --> 01:47:28.160]   Yeah, Canadian.
[01:47:28.160 --> 01:47:29.160]   Yeah.
[01:47:29.160 --> 01:47:30.160]   So, Nexus is a great program.
[01:47:30.160 --> 01:47:32.480]   No, it's better than global entry, except for that.
[01:47:32.480 --> 01:47:35.080]   Except when the border shuts down for two years.
[01:47:35.080 --> 01:47:37.080]   Global entry, though, is fantastic.
[01:47:37.080 --> 01:47:38.640]   Global entry comes straight, right?
[01:47:38.640 --> 01:47:43.960]   TSA pre-check, but the use of face recognition, I mean, we just half the time to come back
[01:47:43.960 --> 01:47:44.960]   into the country.
[01:47:44.960 --> 01:47:47.320]   We just look at the camera and they're like, "Okay, thank you.
[01:47:47.320 --> 01:47:48.320]   Isn't that amazing?"
[01:47:48.320 --> 01:47:51.520]   I just got global entry coming back from Oaxaca.
[01:47:51.520 --> 01:47:55.880]   I did my interview at SFO in the middle of the night and got my global entry.
[01:47:55.880 --> 01:47:57.360]   Oh, you appreciate this.
[01:47:57.360 --> 01:47:58.600]   I went for global entry first.
[01:47:58.600 --> 01:48:02.400]   That's why I was confused before we decided to do the Nexus about six months later.
[01:48:02.400 --> 01:48:04.160]   The guy said, "Oh, so you're..."
[01:48:04.160 --> 01:48:05.760]   He just asked me a few questions.
[01:48:05.760 --> 01:48:06.760]   It was going really well.
[01:48:06.760 --> 01:48:08.160]   It's very brief interviews, right?
[01:48:08.160 --> 01:48:10.000]   He said, "Oh, I see you're ready.
[01:48:10.000 --> 01:48:11.000]   What kind of stuff do you write?"
[01:48:11.000 --> 01:48:12.480]   I'm like, "Oh, and I just list off a few things."
[01:48:12.480 --> 01:48:14.600]   I'm trying to not set off any red flags.
[01:48:14.600 --> 01:48:16.640]   He says, "Oh, sounds like you're pretty smart.
[01:48:16.640 --> 01:48:17.640]   You should go on Jeopardy."
[01:48:17.640 --> 01:48:19.640]   I'm like, "Well, I was on Jeopardy."
[01:48:19.640 --> 01:48:20.640]   It was a matter of fact.
[01:48:20.640 --> 01:48:22.200]   Then he couldn't believe me.
[01:48:22.200 --> 01:48:25.960]   So I had him Google to find a picture of me with Alex Trebek.
[01:48:25.960 --> 01:48:27.600]   Then he starts telling me about his own memory.
[01:48:27.600 --> 01:48:31.000]   I'm like, "Oh my God, you should be on Jeopardy, you and me."
[01:48:31.000 --> 01:48:33.680]   This CB, you know, customer's boarder.
[01:48:33.680 --> 01:48:38.520]   I'm like, "Well, if I read a sports trivia book, I memorize the whole thing."
[01:48:38.520 --> 01:48:40.120]   I'm like, "You need to be on the show.
[01:48:40.120 --> 01:48:41.120]   Go audition."
[01:48:41.120 --> 01:48:42.120]   So I hope I talked him into it.
[01:48:42.120 --> 01:48:47.640]   I'm having a much better experience with Canadian border guards than I had.
[01:48:47.640 --> 01:48:49.680]   In fact, than anybody I've ever met has.
[01:48:49.680 --> 01:48:50.920]   So congratulations.
[01:48:50.920 --> 01:48:52.400]   You've made some friends.
[01:48:52.400 --> 01:48:53.400]   That's good.
[01:48:53.400 --> 01:48:56.320]   Our show today brought to you by We Use Text Messages.
[01:48:56.320 --> 01:48:57.840]   Everybody uses text messages.
[01:48:57.840 --> 01:49:02.840]   Potium is the ultimate text messaging platform for small businesses.
[01:49:02.840 --> 01:49:06.720]   I think this is something we learned actually in the past couple of years.
[01:49:06.720 --> 01:49:07.720]   It's been tough.
[01:49:07.720 --> 01:49:14.960]   I'm not going to, you know, candy coat it from supply chain issues, overwhelming demand on
[01:49:14.960 --> 01:49:20.240]   top of everything else that business owners have to manage, you know, hard to find employees.
[01:49:20.240 --> 01:49:24.920]   But the businesses who thrive in adversity like this are the ones who are forward thinking
[01:49:24.920 --> 01:49:27.080]   who use new technologies.
[01:49:27.080 --> 01:49:31.520]   Potium helps your small business stay ahead of the curve with modern messaging tools that
[01:49:31.520 --> 01:49:34.120]   make it easy for your customers to connect with your business.
[01:49:34.120 --> 01:49:36.200]   You've probably already been through this cycle.
[01:49:36.200 --> 01:49:40.320]   You know, first you had to have an answering machine on the business lines so people could
[01:49:40.320 --> 01:49:45.440]   leave a message and then you had to have a website, then you had to have a Facebook account,
[01:49:45.440 --> 01:49:47.840]   an Instagram account, and it moves all the time.
[01:49:47.840 --> 01:49:49.840]   Well, let me tell you something.
[01:49:49.840 --> 01:49:55.320]   We have learned during the pandemic, people love text messaging.
[01:49:55.320 --> 01:50:00.560]   I mean, how many times do we use text messages to let us know our food delivery is on the
[01:50:00.560 --> 01:50:02.760]   way to order food?
[01:50:02.760 --> 01:50:07.000]   A lot of people, I'm one of them, don't like to call a business, whether it's a plumber,
[01:50:07.000 --> 01:50:08.000]   a landscaper.
[01:50:08.000 --> 01:50:09.280]   I don't like phone tag.
[01:50:09.280 --> 01:50:11.920]   I don't like to leave long messages.
[01:50:11.920 --> 01:50:13.720]   Text is so much easier.
[01:50:13.720 --> 01:50:16.400]   So if you're running in a business and the only way to get in touch with you is a phone
[01:50:16.400 --> 01:50:21.320]   number, you're kind of like, you know, before you had a website, right?
[01:50:21.320 --> 01:50:25.640]   That gives businesses the tools to compete with the convenience offered by bigger businesses
[01:50:25.640 --> 01:50:27.520]   like Amazon.
[01:50:27.520 --> 01:50:31.440]   It takes your small business and makes it a big business, at least from the customer's
[01:50:31.440 --> 01:50:35.880]   point of view, from health care providers to plumbers, over 100,000 businesses are
[01:50:35.880 --> 01:50:39.160]   texting with customers through podium.
[01:50:39.160 --> 01:50:43.880]   And while customers love the convenience, businesses love the results, like the car dealer
[01:50:43.880 --> 01:50:47.800]   who sold a $50,000 truck and just four text messages, "Hey, I've got a truck, you're
[01:50:47.800 --> 01:50:54.840]   interested," a jeweler sold a $5,000 ring, coordinated curbside pickup all through texts.
[01:50:54.840 --> 01:50:56.640]   There was a dentist.
[01:50:56.640 --> 01:50:59.280]   He had a lot of outstanding bills.
[01:50:59.280 --> 01:51:00.680]   People hadn't been paying him.
[01:51:00.680 --> 01:51:04.200]   Turned out, it worked really well to do his collections through text.
[01:51:04.200 --> 01:51:09.400]   70% of the outstanding invoices were paid in just two weeks.
[01:51:09.400 --> 01:51:11.840]   With podiums all in one inbox, your employees will love it too.
[01:51:11.840 --> 01:51:12.840]   You do more than just chat.
[01:51:12.840 --> 01:51:13.840]   You can get online reviews.
[01:51:13.840 --> 01:51:16.160]   You say, "Hey, thanks for visiting our business.
[01:51:16.160 --> 01:51:19.600]   Would you like to leave a review on Yelp or Google or wherever you like those reviews?"
[01:51:19.600 --> 01:51:20.600]   You send the link.
[01:51:20.600 --> 01:51:22.440]   People are much more likely to respond.
[01:51:22.440 --> 01:51:25.160]   Click the link, leave the review.
[01:51:25.160 --> 01:51:28.920]   Great way to collect payments fast from anywhere to send marketing campaigns that actually
[01:51:28.920 --> 01:51:29.920]   get a response.
[01:51:29.920 --> 01:51:34.480]   You know, on average, text messages get read something like 93% of the time.
[01:51:34.480 --> 01:51:40.120]   The open rate is so high in text messages compared to any other way of contacting customers.
[01:51:40.120 --> 01:51:41.440]   Just send a quick text.
[01:51:41.440 --> 01:51:43.000]   It really works.
[01:51:43.000 --> 01:51:44.000]   Customers love it.
[01:51:44.000 --> 01:51:45.000]   Your business will thrive.
[01:51:45.000 --> 01:51:47.760]   You know, podium can grow your business.
[01:51:47.760 --> 01:51:49.240]   We've got a demo for you.
[01:51:49.240 --> 01:51:53.000]   podiumpodim.com/twit.
[01:51:53.000 --> 01:51:54.160]   Take a look.
[01:51:54.160 --> 01:51:58.120]   podium.com/twitpodium.
[01:51:58.120 --> 01:52:00.760]   The better way to grow your local business.
[01:52:00.760 --> 01:52:03.600]   Let's grow with podium.
[01:52:03.600 --> 01:52:09.440]   Thank you, podium, for your support, by the way, of this week in tech.
[01:52:09.440 --> 01:52:14.400]   What about actually now that I'm with some people who use Instagram?
[01:52:14.400 --> 01:52:17.200]   Is it me or does Instagram looking more and more like TikTok?
[01:52:17.200 --> 01:52:19.040]   It is not just you.
[01:52:19.040 --> 01:52:21.360]   It's a little TikTok envy.
[01:52:21.360 --> 01:52:27.600]   In fact, they've announced now that everything's all video you post is going to be real, right?
[01:52:27.600 --> 01:52:28.600]   Is this a good strategy?
[01:52:28.600 --> 01:52:33.280]   It makes me feel like they're copying and they're not.
[01:52:33.280 --> 01:52:34.280]   Yeah.
[01:52:34.280 --> 01:52:35.280]   I don't like it.
[01:52:35.280 --> 01:52:36.280]   Again.
[01:52:36.280 --> 01:52:37.280]   Yeah.
[01:52:37.280 --> 01:52:39.440]   Take a look at the company with zero vision.
[01:52:39.440 --> 01:52:43.120]   And what they do is they just copy every fee.
[01:52:43.120 --> 01:52:46.080]   You know, remember they were copied, they copied Snapchat wholesale.
[01:52:46.080 --> 01:52:47.360]   Then they copied Google plus.
[01:52:47.360 --> 01:52:49.040]   It worked with Snapchat though, right?
[01:52:49.040 --> 01:52:50.040]   Snapchat.
[01:52:50.040 --> 01:52:51.040]   They tried to buy them.
[01:52:51.040 --> 01:52:52.040]   They couldn't buy them.
[01:52:52.040 --> 01:52:53.040]   So they didn't put them out of business.
[01:52:53.040 --> 01:52:54.040]   I think they did better.
[01:52:54.040 --> 01:52:55.040]   No, no.
[01:52:55.040 --> 01:52:56.440]   I think that I have to say, I have to give them a little bit of credit.
[01:52:56.440 --> 01:53:00.360]   I think that their implementation of stories was better than the way Snapchat was, just
[01:53:00.360 --> 01:53:03.520]   because Snapchat really was leaning into the ephemeral at that point.
[01:53:03.520 --> 01:53:07.800]   And I think that the way that they did stories and some of the things they did to their editing
[01:53:07.800 --> 01:53:09.640]   experience was better.
[01:53:09.640 --> 01:53:16.720]   On the flip side, I think that they are a far worse TikTok clone than the Snapchat stories.
[01:53:16.720 --> 01:53:20.960]   But to my point, yeah, if you look through the history of Facebook, I think Instagram's
[01:53:20.960 --> 01:53:21.960]   a little different now.
[01:53:21.960 --> 01:53:23.280]   The founders have left.
[01:53:23.280 --> 01:53:27.720]   And I think that the guy who's in charge now, I don't think he has the product vision.
[01:53:27.720 --> 01:53:34.720]   But I think that Facebook, his history is littered with examples of times when they have taken
[01:53:34.720 --> 01:53:40.080]   inspiration from other companies and attempted to release things that very rarely have worked
[01:53:40.080 --> 01:53:41.080]   out.
[01:53:41.080 --> 01:53:45.840]   Honestly, what's happened in Instagram have been two of the leading reasons why Facebook
[01:53:45.840 --> 01:53:50.440]   has maintained relevance over the last 19 years or however long it's been around.
[01:53:50.440 --> 01:53:54.400]   Kevin's system, who was the founder of Instagram, one of the founders of Instagram left Facebook
[01:53:54.400 --> 01:53:58.200]   quite famously in a dispute over what the direction was.
[01:53:58.200 --> 01:54:02.480]   They replaced him with Adam Oseri, who was a Facebook lifer, in effect.
[01:54:02.480 --> 01:54:06.920]   He is not the product visionary that the system was.
[01:54:06.920 --> 01:54:10.920]   And ever since, I think, Instagram has gone downhill.
[01:54:10.920 --> 01:54:13.240]   And I want Instagram to succeed.
[01:54:13.240 --> 01:54:17.000]   It's the only Facebook property I still use.
[01:54:17.000 --> 01:54:21.120]   And it used to be a great place to share photos.
[01:54:21.120 --> 01:54:22.120]   Not anymore.
[01:54:22.120 --> 01:54:27.360]   Well, they've actually, Facebook has actually done one of their famous copying routines more
[01:54:27.360 --> 01:54:32.640]   recently than the copying of TikTok, which is that they've made Facebook groups look
[01:54:32.640 --> 01:54:33.960]   exactly like Discord.
[01:54:33.960 --> 01:54:34.960]   They made it purple.
[01:54:34.960 --> 01:54:35.960]   They have a sidebar.
[01:54:35.960 --> 01:54:36.960]   That's very awful.
[01:54:36.960 --> 01:54:39.160]   It drives me nuts.
[01:54:39.160 --> 01:54:40.160]   Yep.
[01:54:40.160 --> 01:54:45.160]   And anything that's hot, they just sort of steal the core elements of it and try to get
[01:54:45.160 --> 01:54:46.160]   into that.
[01:54:46.160 --> 01:54:47.160]   Look at this.
[01:54:47.160 --> 01:54:48.160]   This looks so much like Discord.
[01:54:48.160 --> 01:54:49.160]   It's blatant.
[01:54:49.160 --> 01:54:50.160]   It's blatant.
[01:54:50.160 --> 01:54:54.920]   And I have to think that the, and this is what's so weird to me because I have to think
[01:54:54.920 --> 01:54:58.680]   that the demographics and your users for Facebook groups and your users for Discord
[01:54:58.680 --> 01:55:00.040]   are completely different.
[01:55:00.040 --> 01:55:03.120]   So I don't even understand what the appeal is to make it look like Discord because it's
[01:55:03.120 --> 01:55:06.960]   not like you're going to bring the teens and the kids to Facebook groups.
[01:55:06.960 --> 01:55:08.600]   They're not going to ever use that.
[01:55:08.600 --> 01:55:12.720]   And yet the people who use and really enjoy and get things out of Facebook groups are
[01:55:12.720 --> 01:55:16.640]   going to be like, okay, why did you move this and why does this look like this, this
[01:55:16.640 --> 01:55:18.240]   thing that I'm not familiar with?
[01:55:18.240 --> 01:55:19.240]   This makes no sense.
[01:55:19.240 --> 01:55:20.760]   It does seem misguided.
[01:55:20.760 --> 01:55:21.880]   You're exactly right.
[01:55:21.880 --> 01:55:26.440]   You're annoying the existing customers and you have no hope of luring people away from
[01:55:26.440 --> 01:55:27.440]   Discord.
[01:55:27.440 --> 01:55:28.680]   You will never ever lure the other people.
[01:55:28.680 --> 01:55:32.000]   And I mean, like Facebook gaming is famously tried for years to take on Twitch.
[01:55:32.000 --> 01:55:35.000]   I mean, look, YouTube gaming has tried to take on Twitch.
[01:55:35.000 --> 01:55:36.440]   Nothing is going to take on Twitch, right?
[01:55:36.440 --> 01:55:40.200]   Like, you know, Microsoft tried with mixer failed.
[01:55:40.200 --> 01:55:43.200]   So I don't.
[01:55:43.200 --> 01:55:44.680]   Yeah.
[01:55:44.680 --> 01:55:47.840]   I, I want Instagram to succeed.
[01:55:47.840 --> 01:55:55.280]   I briefly moved to Glass, which is an Instagram replacement created by photographers, photos,
[01:55:55.280 --> 01:55:58.040]   first, they're trying to respond to all this stuff.
[01:55:58.040 --> 01:55:59.040]   I don't know what it is.
[01:55:59.040 --> 01:56:00.040]   It maybe it's not as big.
[01:56:00.040 --> 01:56:05.000]   It doesn't have the social, whatever it is, it doesn't quite scratch that itch.
[01:56:05.000 --> 01:56:09.480]   But I was an early adopter on Instagram and I just, I thought that was a great thing.
[01:56:09.480 --> 01:56:10.480]   It was.
[01:56:10.480 --> 01:56:14.880]   I think that was, was weird too about like the, I understand approaching, you know, when
[01:56:14.880 --> 01:56:16.640]   YouTube is done, this was short, it's a little bit too.
[01:56:16.640 --> 01:56:21.000]   I understand approaching some of the types of content and maybe the editing tools and
[01:56:21.000 --> 01:56:23.960]   the length and the format adoption of TikTok.
[01:56:23.960 --> 01:56:24.960]   I see that.
[01:56:24.960 --> 01:56:28.520]   And I mean, and I think that that's why the stories appropriation worked, right?
[01:56:28.520 --> 01:56:35.200]   Like I think that it was a format rather than necessarily taking on some of the way that,
[01:56:35.200 --> 01:56:39.960]   you know, Snapchat worked, which was, Snapchat was always a very personal one to one thing.
[01:56:39.960 --> 01:56:41.560]   Instagram is a little bit broader than that.
[01:56:41.560 --> 01:56:45.200]   It's people, you know, but also people that you might, you know, want to follow and keep
[01:56:45.200 --> 01:56:47.200]   up with.
[01:56:47.200 --> 01:56:49.600]   TikTok was fascinating to me about it.
[01:56:49.600 --> 01:56:53.240]   And this isn't universally true, but I think this is largely true for people is that it
[01:56:53.240 --> 01:56:56.720]   is one of the few social networks that's come out basically since Facebook, if we want
[01:56:56.720 --> 01:57:01.160]   to be honest, where your social graph is not at all based on who you know.
[01:57:01.160 --> 01:57:06.680]   It's based on, you know, things you're interested in and things you follow, but you don't necessarily,
[01:57:06.680 --> 01:57:09.400]   like on TikTok, I don't follow anyone that I know in real life.
[01:57:09.400 --> 01:57:12.160]   Like I might, there might be a few things.
[01:57:12.160 --> 01:57:13.160]   It doesn't matter.
[01:57:13.160 --> 01:57:15.640]   By the way, follow away.
[01:57:15.640 --> 01:57:18.640]   I follow my son and I never see his TikToks.
[01:57:18.640 --> 01:57:21.960]   Well, the answer to my point, this is sort of my point, right?
[01:57:21.960 --> 01:57:26.480]   Is the way that they feed you content and the reason why it's not the case for the awkward
[01:57:26.480 --> 01:57:27.480]   tab.
[01:57:27.480 --> 01:57:30.320]   He's not there and I know I'm following him and I know he's putting him out.
[01:57:30.320 --> 01:57:31.840]   I have to actually search for his name.
[01:57:31.840 --> 01:57:34.760]   You have to search for him because because you're not typically consuming his type of
[01:57:34.760 --> 01:57:36.920]   content that he's creating.
[01:57:36.920 --> 01:57:37.920]   Where is I?
[01:57:37.920 --> 01:57:40.560]   Well, if you would do more bikini rich content, I might see more stuff.
[01:57:40.560 --> 01:57:45.360]   If he would do more bikini rich content or maybe he should do collabs with some people,
[01:57:45.360 --> 01:57:48.240]   then maybe you'll see him on your free page.
[01:57:48.240 --> 01:57:52.720]   But until then, but again, but this is where I think that the Facebook thing is a little
[01:57:52.720 --> 01:57:56.240]   bit misguided or Instagram thing is a little misguided because look, you might also follow
[01:57:56.240 --> 01:58:00.120]   the bikini models on Instagram, but you're largely there to see people, at least I think
[01:58:00.120 --> 01:58:03.600]   historically people that you know and to share things with people you know.
[01:58:03.600 --> 01:58:07.640]   Whereas reels the whole idea, if they're going to make it like TikTok, then you want to see
[01:58:07.640 --> 01:58:09.920]   stuff from people that you're not connected with.
[01:58:09.920 --> 01:58:14.880]   So to me, it feels like a mixed, like a completely false experience like set up.
[01:58:14.880 --> 01:58:19.560]   Like, if I want to follow people that I don't actually have any connection with and see short
[01:58:19.560 --> 01:58:22.360]   form videos about it, that's what TikTok is for.
[01:58:22.360 --> 01:58:25.960]   I don't want to suddenly be in with completely different paradigm.
[01:58:25.960 --> 01:58:26.960]   Exactly.
[01:58:26.960 --> 01:58:30.720]   I don't want to be inundated with that on Instagram and I don't think that Facebook quite understands
[01:58:30.720 --> 01:58:31.720]   that.
[01:58:31.720 --> 01:58:34.400]   Like, again, to your point, the product vision isn't really there.
[01:58:34.400 --> 01:58:35.400]   I don't know if I'm in it.
[01:58:35.400 --> 01:58:36.400]   There's a question.
[01:58:36.400 --> 01:58:37.400]   Go ahead.
[01:58:37.400 --> 01:58:38.400]   Go ahead, Liam.
[01:58:38.400 --> 01:58:39.400]   Sorry.
[01:58:39.400 --> 01:58:41.400]   After you had a fault.
[01:58:41.400 --> 01:58:42.400]   Delay.
[01:58:42.400 --> 01:58:43.400]   Go on.
[01:58:43.400 --> 01:58:45.480]   Okay, I'll go.
[01:58:45.480 --> 01:58:49.960]   So first of all, does Hank make you call him salt or do you still call him?
[01:58:49.960 --> 01:58:50.960]   Salty.
[01:58:50.960 --> 01:58:52.800]   I always wondered about that.
[01:58:52.800 --> 01:58:53.800]   Salty.
[01:58:53.800 --> 01:58:58.920]   But here's a question because I normally like to crap all over TikTok and criticize them.
[01:58:58.920 --> 01:59:00.400]   I'm actually in love with TikTok.
[01:59:00.400 --> 01:59:04.880]   But here's a question.
[01:59:04.880 --> 01:59:10.520]   Does TikTok actually expand in human empathy and understanding that because there's so
[01:59:10.520 --> 01:59:15.760]   many videos from like, you know, Nairobi and like, you know, all these hard funds that
[01:59:15.760 --> 01:59:19.400]   the internet was going to do, which was open up the world?
[01:59:19.400 --> 01:59:21.240]   I think it absolutely does.
[01:59:21.240 --> 01:59:27.520]   Plus for creators like my son, it's an opportunity that he couldn't have done what he's done
[01:59:27.520 --> 01:59:29.040]   on any other platform.
[01:59:29.040 --> 01:59:30.040]   Right.
[01:59:30.040 --> 01:59:31.040]   And he launched them in other platforms.
[01:59:31.040 --> 01:59:36.760]   He's like on Megan, Megan Salter, who's now on hacks on HBO.
[01:59:36.760 --> 01:59:38.160]   She would, she did her comment.
[01:59:38.160 --> 01:59:40.480]   She got, she was a comedian, COVID.
[01:59:40.480 --> 01:59:41.680]   She couldn't do her bits.
[01:59:41.680 --> 01:59:45.600]   So she created this crazy, something that would have never worked anywhere but TikTok.
[01:59:45.600 --> 01:59:49.240]   These crazy short form characters became huge on TikTok.
[01:59:49.240 --> 01:59:50.680]   Now she's on HBO.
[01:59:50.680 --> 01:59:54.960]   I think this is a really good place for creators of a certain type.
[01:59:54.960 --> 01:59:55.960]   Right.
[01:59:55.960 --> 01:59:56.960]   Right.
[01:59:56.960 --> 01:59:57.960]   Yeah.
[01:59:57.960 --> 01:59:58.960]   Absolutely.
[01:59:58.960 --> 02:00:04.240]   I saw a glimpse of a show that Shakira has where they have these people who dance on TikTok.
[02:00:04.240 --> 02:00:06.680]   They're discovered on TikTok brought into a show.
[02:00:06.680 --> 02:00:11.040]   And it's like, it's played as if, as if, as if the, you know, the Shakira show is the
[02:00:11.040 --> 02:00:12.800]   big time and TikTok is the small time.
[02:00:12.800 --> 02:00:13.800]   It's the other way around.
[02:00:13.800 --> 02:00:14.800]   But it's the other way around.
[02:00:14.800 --> 02:00:16.440]   They're just trying to glom on to the.
[02:00:16.440 --> 02:00:17.800]   They're trying to glom on to it.
[02:00:17.800 --> 02:00:18.800]   Exactly.
[02:00:18.800 --> 02:00:22.600]   My son has 2.1 million followers on TikTok.
[02:00:22.600 --> 02:00:25.760]   That is more than any show I do.
[02:00:25.760 --> 02:00:31.920]   He has completely eclipsed anything I've done in 40 years in the business in a matter of
[02:00:31.920 --> 02:00:32.920]   months.
[02:00:32.920 --> 02:00:34.960]   I don't, I don't know where else you could do that.
[02:00:34.960 --> 02:00:37.000]   Now I don't know what the successor to that is.
[02:00:37.000 --> 02:00:41.200]   And I, you know, I always, I'm, you know, I'm the, I'm the, I feel bad.
[02:00:41.200 --> 02:00:44.160]   I'm the parent who goes, now son.
[02:00:44.160 --> 02:00:47.800]   You don't, you don't pull all your eggs in one basket, you know, and all that.
[02:00:47.800 --> 02:00:48.800]   He's gone.
[02:00:48.800 --> 02:00:49.800]   Yeah.
[02:00:49.800 --> 02:00:50.800]   Right.
[02:00:50.800 --> 02:00:51.800]   Yeah.
[02:00:51.800 --> 02:00:52.800]   He literally does some of his videos.
[02:00:52.800 --> 02:00:53.800]   He puts eggs in baskets.
[02:00:53.800 --> 02:00:54.800]   And he doesn't say many extra.
[02:00:54.800 --> 02:00:55.720]   Yeah.
[02:00:55.720 --> 02:00:57.080]   He's measured in minutes though.
[02:00:57.080 --> 02:01:01.080]   I think that's minutes consumed given the length of your shows versus TikToks.
[02:01:01.080 --> 02:01:02.080]   Maybe it happens.
[02:01:02.080 --> 02:01:03.080]   Ah, maybe.
[02:01:03.080 --> 02:01:04.080]   User minutes.
[02:01:04.080 --> 02:01:05.080]   Should I measure?
[02:01:05.080 --> 02:01:06.080]   User minutes.
[02:01:06.080 --> 02:01:10.040]   So what is just me or is this something you guys have in your Instagram?
[02:01:10.040 --> 02:01:12.120]   This is something I thought was new.
[02:01:12.120 --> 02:01:17.320]   Up at the top at Instagram, there's now a little down triangle and I can choose from
[02:01:17.320 --> 02:01:19.440]   favorites and following.
[02:01:19.440 --> 02:01:22.040]   And I think this is, can you do that or is that, is that my?
[02:01:22.040 --> 02:01:23.560]   I can't do it on mine.
[02:01:23.560 --> 02:01:24.560]   Maybe I'm in a test.
[02:01:24.560 --> 02:01:25.560]   Yeah, maybe.
[02:01:25.560 --> 02:01:28.760]   I was going to say, I don't have this.
[02:01:28.760 --> 02:01:31.080]   So I like this because I favorite the people.
[02:01:31.080 --> 02:01:33.240]   Now this is just like the old Instagram.
[02:01:33.240 --> 02:01:34.240]   It's just people.
[02:01:34.240 --> 02:01:35.240]   It's coming back.
[02:01:35.240 --> 02:01:36.240]   Yeah, just people I'm following.
[02:01:36.240 --> 02:01:38.040]   Oh, no, I do have this.
[02:01:38.040 --> 02:01:39.040]   Sorry.
[02:01:39.040 --> 02:01:40.040]   Okay.
[02:01:40.040 --> 02:01:41.040]   Following and then favorites.
[02:01:41.040 --> 02:01:42.040]   I just, I never saw it before.
[02:01:42.040 --> 02:01:45.280]   I saw it a couple of weeks ago and I thought, oh, so maybe this is their way.
[02:01:45.280 --> 02:01:48.680]   Admittedly, it's hidden away and who knows who has it.
[02:01:48.680 --> 02:01:53.240]   But maybe this is their way of saying, well, you could just like Twitter, rather.
[02:01:53.240 --> 02:01:58.800]   You can do latest tweets, which is a chronological feed of your follow people you're following.
[02:01:58.800 --> 02:01:59.800]   I love that.
[02:01:59.800 --> 02:02:00.800]   Yeah.
[02:02:00.800 --> 02:02:03.360]   Or you can do the home, which gives you, you know, and that's good too.
[02:02:03.360 --> 02:02:06.680]   Good times when you want to see stuff you might have missed.
[02:02:06.680 --> 02:02:09.000]   So I, maybe this is a new way of doing this.
[02:02:09.000 --> 02:02:10.640]   You kind of have it both ways.
[02:02:10.640 --> 02:02:14.600]   The sneaky thing on Instagram is that deal where they show you things from other people
[02:02:14.600 --> 02:02:15.600]   you follow.
[02:02:15.600 --> 02:02:16.600]   I hate that.
[02:02:16.600 --> 02:02:17.600]   I don't want to see that.
[02:02:17.600 --> 02:02:18.600]   I don't want to see that.
[02:02:18.600 --> 02:02:20.200]   Then you close it and it says, do you want to snooze this for 30 days?
[02:02:20.200 --> 02:02:22.440]   Like, well, I want the never ever do this to me.
[02:02:22.440 --> 02:02:23.440]   Never ever.
[02:02:23.440 --> 02:02:24.440]   Right.
[02:02:24.440 --> 02:02:26.080]   Every 30 days I start seeing random things.
[02:02:26.080 --> 02:02:27.400]   And it's made up too.
[02:02:27.400 --> 02:02:33.280]   Because you followed, because you liked, because you watched a video by this person,
[02:02:33.280 --> 02:02:35.560]   like, yeah, paused on that for two seconds.
[02:02:35.560 --> 02:02:38.680]   We thought you'd like this one is not a good system.
[02:02:38.680 --> 02:02:40.840]   It's they're just annoying.
[02:02:40.840 --> 02:02:45.160]   I'm very disappointed and trying to be more like TikTok.
[02:02:45.160 --> 02:02:47.160]   Now I think every video is a real.
[02:02:47.160 --> 02:02:49.960]   Suddenly it's reels, reels, reels.
[02:02:49.960 --> 02:02:53.400]   And yet they haven't even copied all the good things.
[02:02:53.400 --> 02:02:54.400]   None of the good stuff.
[02:02:54.400 --> 02:02:55.400]   No, no, no, no, no.
[02:02:55.400 --> 02:03:02.080]   But reels now don't have a TikToks rather, the time limit is much longer and reels are
[02:03:02.080 --> 02:03:03.240]   limited to 90 seconds.
[02:03:03.240 --> 02:03:07.680]   It's like, okay, what's the point then?
[02:03:07.680 --> 02:03:13.640]   You're making me use this subpar version of this copycat when I would rather consume the
[02:03:13.640 --> 02:03:19.440]   content on the platform that has the admittedly scary is all get out, but very, very effective
[02:03:19.440 --> 02:03:22.520]   algorithm, like, I don't know.
[02:03:22.520 --> 02:03:25.280]   Speaking of TikTok, I mean, there's still some concern about the fact that it's owned
[02:03:25.280 --> 02:03:31.400]   by ByteDance, a Chinese company that there may be Chinese employees looking at stuff.
[02:03:31.400 --> 02:03:36.100]   And in fact, even though TikTok announced that they were going to put all the US users
[02:03:36.100 --> 02:03:45.160]   data on Oracle based servers in California, apparently they admitted only carefully vetted
[02:03:45.160 --> 02:03:51.760]   Chinese employees have access to the American data.
[02:03:51.760 --> 02:03:57.240]   TikTok is working on a deal with a Biden administration that would quote, fully safeguard
[02:03:57.240 --> 02:04:00.080]   the app in the US.
[02:04:00.080 --> 02:04:04.560]   Obviously they don't look, there's nothing the Chinese government is getting from this
[02:04:04.560 --> 02:04:08.760]   that is worth what TikTok is getting from American users, right?
[02:04:08.760 --> 02:04:10.680]   But does TikTok have a choice?
[02:04:10.680 --> 02:04:14.920]   Yes, it's not up to ByteDance.
[02:04:14.920 --> 02:04:21.720]   I understand the psychological satisfaction of hosting data in a certain country, but
[02:04:21.720 --> 02:04:29.040]   does hosting a US user data in the US on Oracle servers prevent ByteDance from accessing
[02:04:29.040 --> 02:04:31.320]   the data that their app is generating?
[02:04:31.320 --> 02:04:32.760]   I mean, I would have to see--
[02:04:32.760 --> 02:04:34.280]   Apparently not.
[02:04:34.280 --> 02:04:38.640]   Because that's what they, in their letter, they wrote to nine Republican senators that
[02:04:38.640 --> 02:04:41.600]   was released just Friday.
[02:04:41.600 --> 02:04:49.360]   They admitted, no, our employees can see American data, even though it's not stored
[02:04:49.360 --> 02:04:52.480]   in China, they can see it.
[02:04:52.480 --> 02:04:53.480]   Right.
[02:04:53.480 --> 02:04:54.480]   Pointless.
[02:04:54.480 --> 02:05:02.600]   I just find it very-- there's a lot of effort to try to put fears at ease, but it's not--
[02:05:02.600 --> 02:05:05.760]   at the end of the day, ByteDance is a Chinese company.
[02:05:05.760 --> 02:05:11.880]   By the way, a lot of people don't know this, but TikTok itself is banned in China.
[02:05:11.880 --> 02:05:12.880]   Really?
[02:05:12.880 --> 02:05:13.880]   Yeah, really?
[02:05:13.880 --> 02:05:14.880]   Yeah.
[02:05:14.880 --> 02:05:20.240]   ByteDance has another version called something else, I forgot what it's called, which is
[02:05:20.240 --> 02:05:22.440]   fully censored by the Chinese government, et cetera.
[02:05:22.440 --> 02:05:25.640]   But yeah, even TikTok is banned in China.
[02:05:25.640 --> 02:05:27.440]   It's incredible.
[02:05:27.440 --> 02:05:35.680]   But the Chinese government will, if it wants to, get any data from ByteDance that ByteDance
[02:05:35.680 --> 02:05:38.840]   has access to, no matter where it's stored.
[02:05:38.840 --> 02:05:39.840]   Yeah.
[02:05:39.840 --> 02:05:41.120]   It's really not about where it's stored.
[02:05:41.120 --> 02:05:43.600]   It's about what ByteDance has.
[02:05:43.600 --> 02:05:47.400]   But so what?
[02:05:47.400 --> 02:05:51.080]   What are they going to get out of this that I should be worried that the Chinese-- I'm
[02:05:51.080 --> 02:05:54.720]   much more worried about what Facebook knows about me than what Chinese government knows
[02:05:54.720 --> 02:05:55.720]   about me.
[02:05:55.720 --> 02:05:57.000]   The location of US soldiers--
[02:05:57.000 --> 02:05:58.000]   Well--
[02:05:58.000 --> 02:05:59.000]   The movement of US soldiers.
[02:05:59.000 --> 02:06:05.160]   It's completely reasonable for the US Armed Forces to prevent-- forbid the use of TikTok.
[02:06:05.160 --> 02:06:06.160]   They can say that.
[02:06:06.160 --> 02:06:07.160]   But it--
[02:06:07.160 --> 02:06:08.160]   Right.
[02:06:08.160 --> 02:06:09.160]   Well, it's on the other hand.
[02:06:09.160 --> 02:06:10.160]   But--
[02:06:10.160 --> 02:06:12.800]   Okay, but that's their problem, not my problem.
[02:06:12.800 --> 02:06:15.040]   They need to handle that problem, not me.
[02:06:15.040 --> 02:06:16.040]   Why would you--
[02:06:16.040 --> 02:06:21.040]   What I'm saying is there's this Project Texas thing, which is a so-called collaboration--
[02:06:21.040 --> 02:06:22.280]   Is that an ironic name?
[02:06:22.280 --> 02:06:23.280]   ByteDance.
[02:06:23.280 --> 02:06:25.280]   Yes, it is.
[02:06:25.280 --> 02:06:31.600]   And this is where the-- or storing on Oracle Service, et cetera, comes in.
[02:06:31.600 --> 02:06:37.720]   And so what I'm saying is that we need transparency around exactly what they're doing, because
[02:06:37.720 --> 02:06:43.000]   I don't really trust ByteDance or the US government or Texas--
[02:06:43.000 --> 02:06:44.000]   Right.
[02:06:44.000 --> 02:06:48.960]   --to actually safeguard this data unless I know how they're doing it.
[02:06:48.960 --> 02:06:52.520]   How exactly are they preventing the company from accessing its own data?
[02:06:52.520 --> 02:06:53.520]   Right.
[02:06:53.520 --> 02:06:55.040]   But you've said this yourself, Mike.
[02:06:55.040 --> 02:07:02.160]   The threat from ByteDance of TikTok isn't so much harvesting American citizens' data.
[02:07:02.160 --> 02:07:04.840]   It's slanting American citizen opinion.
[02:07:04.840 --> 02:07:05.840]   It's disinformation.
[02:07:05.840 --> 02:07:06.840]   It's disinformation.
[02:07:06.840 --> 02:07:07.840]   Yes.
[02:07:07.840 --> 02:07:08.840]   That's the real risk.
[02:07:08.840 --> 02:07:10.800]   This doesn't address that at all, right?
[02:07:10.800 --> 02:07:14.800]   There is also another issue, though, because there's a lot of-- it's not just-- the data
[02:07:14.800 --> 02:07:19.080]   may seem unimportant, but the Chinese are monitoring hundreds of thousands or millions
[02:07:19.080 --> 02:07:24.520]   of Chinese-- both Chinese-Americans and Chinese citizens living in the United States and
[02:07:24.520 --> 02:07:27.120]   researchers and people doing work.
[02:07:27.120 --> 02:07:32.840]   This is the whole thing years ago when China was deemed to have broken into Gmail accounts
[02:07:32.840 --> 02:07:40.360]   related to people who were writing about or human rights advocates connected with China
[02:07:40.360 --> 02:07:41.360]   and so forth.
[02:07:41.360 --> 02:07:46.520]   China will use any information that they can get that helps them understand and track a
[02:07:46.520 --> 02:07:48.400]   very large swath of people.
[02:07:48.400 --> 02:07:54.480]   So that risk comes across any data China gets access to, and TikTok gives them a lot
[02:07:54.480 --> 02:07:56.360]   of that for those citizens.
[02:07:56.360 --> 02:07:57.360]   Well, those sit--
[02:07:57.360 --> 02:07:58.360]   And non-citizens.
[02:07:58.360 --> 02:08:04.520]   I hope this doesn't sound callous, but if they don't want to be tracked, they shouldn't
[02:08:04.520 --> 02:08:06.120]   be using TikTok on their phone.
[02:08:06.120 --> 02:08:07.120]   Well, that's--
[02:08:07.120 --> 02:08:08.120]   Yeah.
[02:08:08.120 --> 02:08:09.360]   It's not my problem.
[02:08:09.360 --> 02:08:14.600]   If you were a Chinese dissident in the United States, you'd be nuts to use TikTok and put
[02:08:14.600 --> 02:08:15.600]   TikTok on your phone.
[02:08:15.600 --> 02:08:16.600]   What does it need to be a dissident?
[02:08:16.600 --> 02:08:17.600]   You can be a Chinese-- I mean, you know, there's been these artists--
[02:08:17.600 --> 02:08:18.600]   You don't do it.
[02:08:18.600 --> 02:08:19.600]   They should do it.
[02:08:19.600 --> 02:08:20.600]   Well, OK.
[02:08:20.600 --> 02:08:21.600]   If they're worried about it, that's their problem.
[02:08:21.600 --> 02:08:22.600]   It's not my problem.
[02:08:22.600 --> 02:08:25.400]   OK, it's not your problem, but I would say this.
[02:08:25.400 --> 02:08:27.080]   And I think there's some fairness in that.
[02:08:27.080 --> 02:08:31.040]   But I would say, OK, the way that we know that location and other information can be
[02:08:31.040 --> 02:08:35.520]   aggregated, we know that people who are not personally wanting to be identified with it,
[02:08:35.520 --> 02:08:40.000]   how that information can also be assimilated and put into those things.
[02:08:40.000 --> 02:08:42.040]   Like, OK, so let's say they don't use it.
[02:08:42.040 --> 02:08:46.480]   That doesn't mean that certain information about them couldn't be inferred because of
[02:08:46.480 --> 02:08:48.800]   other people who were in the data set, right?
[02:08:48.800 --> 02:08:49.800]   Well, that--
[02:08:49.800 --> 02:08:50.800]   OK, good point.
[02:08:50.800 --> 02:08:51.800]   You can still be putting people--
[02:08:51.800 --> 02:08:52.320]   OK.
[02:08:52.320 --> 02:08:56.600]   So if I hung out with a group of Chinese people and they didn't have TikTok, but I did--
[02:08:56.600 --> 02:08:57.600]   Right.
[02:08:57.600 --> 02:08:58.600]   You could be putting them at risk.
[02:08:58.600 --> 02:08:59.600]   Yeah, OK.
[02:08:59.600 --> 02:09:00.600]   That's fair.
[02:09:00.600 --> 02:09:01.600]   All right.
[02:09:01.600 --> 02:09:02.600]   I don't want to overstate it.
[02:09:02.600 --> 02:09:07.520]   But it's also, I think, the unfettered access by any government to information about that
[02:09:07.520 --> 02:09:14.000]   includes stuff that can be like location, voice, face, content, opinion, all of these
[02:09:14.000 --> 02:09:15.000]   things.
[02:09:15.000 --> 02:09:18.520]   Do we want-- even though it's publicly available, it's not publicly available in the same degree
[02:09:18.520 --> 02:09:20.160]   to which it can be accessed off servers.
[02:09:20.160 --> 02:09:24.760]   The Chinese have developed enormous capabilities to do voice and facial and other recognition
[02:09:24.760 --> 02:09:28.280]   on data sets because they're using that against their own citizens.
[02:09:28.280 --> 02:09:31.560]   So the risk is that the Chinese will use it.
[02:09:31.560 --> 02:09:38.080]   Maybe they'll use it as techniques to-- they could be using it for a type of turn-- a
[02:09:38.080 --> 02:09:40.840]   turn military people to turn people in academia.
[02:09:40.840 --> 02:09:47.120]   There's just a-- I don't want to make China out to be this ridiculous international threat,
[02:09:47.120 --> 02:09:49.640]   but I think they work extremely well in their own interests.
[02:09:49.640 --> 02:09:51.400]   Well, I don't want to overstate it.
[02:09:51.400 --> 02:09:54.440]   I think they were precisely well in their own interests.
[02:09:54.440 --> 02:09:57.360]   And I'll let you say it because I don't disagree with that.
[02:09:57.360 --> 02:10:00.640]   I'm more a fear to my own government than I am with China.
[02:10:00.640 --> 02:10:05.600]   And if I were a woman in this country, a young woman of breeding age in this country, I'd
[02:10:05.600 --> 02:10:09.560]   be especially a fear of my own government.
[02:10:09.560 --> 02:10:13.440]   I think there's a lot more at risk from that.
[02:10:13.440 --> 02:10:14.440]   This is fair.
[02:10:14.440 --> 02:10:18.720]   But we have-- potentially have the ability to affect what our government does in this
[02:10:18.720 --> 02:10:19.720]   country.
[02:10:19.720 --> 02:10:21.240]   We can't do it with China.
[02:10:21.240 --> 02:10:25.200]   And I think China is more validly interested in creating--
[02:10:25.200 --> 02:10:30.680]   They can't do as much harder to me, though, as the Texas Attorney General can.
[02:10:30.680 --> 02:10:31.680]   I don't know.
[02:10:31.680 --> 02:10:32.680]   I don't know.
[02:10:32.680 --> 02:10:33.680]   We can oppose them both.
[02:10:33.680 --> 02:10:41.600]   But the point is that one of the problems-- you talk about contentious political issues
[02:10:41.600 --> 02:10:44.480]   in the United States, the point is they're contentious.
[02:10:44.480 --> 02:10:46.840]   In China, there's no contentiousness about anything.
[02:10:46.840 --> 02:10:54.920]   It's like-- and Xi Jinping has proved that he wants to be North Korea, but with way more
[02:10:54.920 --> 02:10:57.000]   money and a lot more people and a lot more power.
[02:10:57.000 --> 02:11:01.680]   And the ability to be a superpower, which is to extend military power globally.
[02:11:01.680 --> 02:11:09.000]   So yes, our own government is more concerned about screwing us over, but we can do something
[02:11:09.000 --> 02:11:10.600]   about it, as Glenn said.
[02:11:10.600 --> 02:11:12.200]   And it's a conversation.
[02:11:12.200 --> 02:11:22.840]   It's the lack of conversation, the lack of democracy in China that makes us as technology
[02:11:22.840 --> 02:11:28.840]   consumers have to think twice about what we're consuming and who's behind it and all that
[02:11:28.840 --> 02:11:29.840]   sort of thing.
[02:11:29.840 --> 02:11:35.720]   You wrote a good piece in Computer World last week sort of at this notion that it was foolish
[02:11:35.720 --> 02:11:39.000]   ever to think of a global internet.
[02:11:39.000 --> 02:11:40.760]   That wasn't going to happen.
[02:11:40.760 --> 02:11:41.760]   That's exactly right.
[02:11:41.760 --> 02:11:45.000]   And the buzzword is the splinter net.
[02:11:45.000 --> 02:11:47.680]   And the splinter net just keeps splintering more and more.
[02:11:47.680 --> 02:11:54.240]   The biggest-- of course, the big split came with the great firewall of China.
[02:11:54.240 --> 02:11:55.760]   You use the internet in China.
[02:11:55.760 --> 02:11:58.120]   It looks nothing like the internet we use.
[02:11:58.120 --> 02:12:01.680]   They don't have access to Wikipedia, Facebook, Twitter, Snapchat.
[02:12:01.680 --> 02:12:04.080]   TikTok, I've just learned.
[02:12:04.080 --> 02:12:05.080]   Yes.
[02:12:05.080 --> 02:12:06.080]   Yes.
[02:12:06.080 --> 02:12:09.720]   And we don't have access or we don't have-- we're not really using these other things.
[02:12:09.720 --> 02:12:14.880]   Meanwhile, not only is China good at keeping their own citizens from seeing uncensored
[02:12:14.880 --> 02:12:19.080]   content, they're increasingly censoring globally, which is a big concern.
[02:12:19.080 --> 02:12:25.480]   Then you had Russia kind of want to have a wannabe Chinese great wall of China.
[02:12:25.480 --> 02:12:30.200]   But the war in Ukraine just accelerated that massively where you have lots and lots of
[02:12:30.200 --> 02:12:35.920]   people, US companies are pulling out of Russia, Russia banning all these social networks that
[02:12:35.920 --> 02:12:37.440]   they used to not ban and so on.
[02:12:37.440 --> 02:12:41.120]   So that they're becoming more China-like in their disassociation.
[02:12:41.120 --> 02:12:43.960]   But you have other things happening.
[02:12:43.960 --> 02:12:44.960]   You have similar things.
[02:12:44.960 --> 02:12:48.600]   They're essentially intranets in North Korea, Eritrea, Ethiopia, Saudi Arabia, and a bunch
[02:12:48.600 --> 02:12:50.720]   of other countries.
[02:12:50.720 --> 02:12:58.360]   You have a book by somebody named Nina Zhang who wrote a book, Parallel Metaverses.
[02:12:58.360 --> 02:13:02.920]   And her contention is the same point that I made in a column a few months ago, which
[02:13:02.920 --> 02:13:05.440]   is there's not going to be a metaverse.
[02:13:05.440 --> 02:13:08.400]   There's going to be many, many metaverses and there are going to be some of them that are
[02:13:08.400 --> 02:13:11.680]   going to be very compelling and people are going to live within them.
[02:13:11.680 --> 02:13:12.680]   You'll live in yours.
[02:13:12.680 --> 02:13:13.680]   I'll live in mine.
[02:13:13.680 --> 02:13:19.400]   So it's another, the two biggest trends, metaverse and web3, are major splintering factors
[02:13:19.400 --> 02:13:21.440]   on an already splintered splinter net.
[02:13:21.440 --> 02:13:23.680]   So the metaverse is going to be splinter people.
[02:13:23.680 --> 02:13:28.240]   Web3 is going to splinter people because they think the vision of the web3 is we'll just
[02:13:28.240 --> 02:13:33.600]   get everybody on the web3 stuff, on blockchain stuff, on distributed networks and so on.
[02:13:33.600 --> 02:13:38.120]   Well, a minority of people will embrace that version of the web and the majority will stay
[02:13:38.120 --> 02:13:40.240]   on what they call web2.
[02:13:40.240 --> 02:13:41.680]   So that's another splinter.
[02:13:41.680 --> 02:13:46.000]   So we're being splintered every which way and the idea that we're all going to go back
[02:13:46.000 --> 02:13:51.640]   to an internet that we all share where there's, you can't censor it, et cetera, is just never
[02:13:51.640 --> 02:13:52.640]   going to happen.
[02:13:52.640 --> 02:13:55.400]   It's just going to get worse and worse and worse, unfortunately.
[02:13:55.400 --> 02:13:58.200]   We need to bring back Gopher.
[02:13:58.200 --> 02:13:59.200]   That'll solve it.
[02:13:59.200 --> 02:14:00.200]   Archie and Veronica.
[02:14:00.200 --> 02:14:07.080]   Actually, her point's interesting, but it's not just national metaverse.
[02:14:07.080 --> 02:14:10.920]   We're going to have splintering metaverse within the US.
[02:14:10.920 --> 02:14:16.120]   There'll be Facebooks, there'll be Apple, there'll be Microsofts and there, even though
[02:14:16.120 --> 02:14:20.320]   they've formed this metaverse alliance, it's not clear that there will be interoperable
[02:14:20.320 --> 02:14:21.320]   metaverse.
[02:14:21.320 --> 02:14:25.280]   I think it's much more likely that Facebook's going to say, no, no, you want to play in
[02:14:25.280 --> 02:14:26.280]   our space.
[02:14:26.280 --> 02:14:27.720]   We've got the coolest outfits.
[02:14:27.720 --> 02:14:33.680]   Which is unfortunate because I think the whole reason that the web won, and the whole reason
[02:14:33.680 --> 02:14:39.360]   that versus the online networks and the internet's superhighway of that motif, I think the whole
[02:14:39.360 --> 02:14:42.920]   reason the web won was interoperability.
[02:14:42.920 --> 02:14:47.760]   I think that we might have been naive to think that we could have one global internet, but
[02:14:47.760 --> 02:14:52.360]   I think that the reason the internet has been such a massive success and that the web has
[02:14:52.360 --> 02:15:02.320]   had such a uncalculable impact on society, global society, I should add, has been because
[02:15:02.320 --> 02:15:08.440]   until recently, with a couple of exceptions, it has been one place.
[02:15:08.440 --> 02:15:12.600]   There's been a lot of negative about social media.
[02:15:12.600 --> 02:15:15.080]   We've been talking a lot about it on our shows.
[02:15:15.080 --> 02:15:16.080]   Is it good?
[02:15:16.080 --> 02:15:17.080]   Is it bad?
[02:15:17.080 --> 02:15:18.080]   It's a source of disinformation.
[02:15:18.080 --> 02:15:19.080]   It can bring people together.
[02:15:19.080 --> 02:15:20.080]   There's some positives and some negatives.
[02:15:20.080 --> 02:15:21.560]   I thought this was really interesting.
[02:15:21.560 --> 02:15:28.400]   I don't know if you watched the January 6th testimony, the Congressional testimony, but
[02:15:28.400 --> 02:15:35.480]   there was a retired Republican judge, Michael Lutig, who was, I thought, really interesting.
[02:15:35.480 --> 02:15:38.680]   I remember very well as testimony, and I also remember thinking, well, he's sort of speaking
[02:15:38.680 --> 02:15:39.880]   slowly.
[02:15:39.880 --> 02:15:44.520]   I wonder if he's recovering from a stroke or something like that.
[02:15:44.520 --> 02:15:46.240]   He's not.
[02:15:46.240 --> 02:15:52.720]   But Joe Hagen, who's a writer for Vanity Fair and a Twitter thread, said, "I like how this
[02:15:52.720 --> 02:15:58.400]   guy treats every line of his testimony like he's engraving it on a national monument."
[02:15:58.400 --> 02:16:02.200]   But he's talking to the stenographer, not to the public.
[02:16:02.200 --> 02:16:08.600]   He really is engraving it for history, and he seems to know it, to which Judge Lutig
[02:16:08.600 --> 02:16:11.760]   responded on Twitter with a Twitter thread.
[02:16:11.760 --> 02:16:14.080]   Thank you so much, Mr. Hagen.
[02:16:14.080 --> 02:16:17.880]   You almost presciently understood precisely what I was at least attempting to do to the
[02:16:17.880 --> 02:16:20.260]   best of my abilities during the hearing Thursday.
[02:16:20.260 --> 02:16:22.780]   He even mocked his own speech pattern.
[02:16:22.780 --> 02:16:26.620]   He says, "What you could not know, and did not know but it will tell you now, is that
[02:16:26.620 --> 02:16:32.020]   I believed I had an obligation to the Select Committee and the country first to formulate
[02:16:32.020 --> 02:16:34.060]   .
[02:16:34.060 --> 02:16:36.060]   Then to measure "
[02:16:36.060 --> 02:16:46.320]   "And then, to meter out every single word that I spoke carefully, exactingly and deliberately,
[02:16:46.320 --> 02:16:51.000]   so that the words I spoke were pristine, clear and would be heard and therefore understood
[02:16:51.000 --> 02:16:53.400]   as such."
[02:16:53.400 --> 02:16:58.020]   He said, "I believed Thursday I had a high responsibility and obligation to myself even
[02:16:58.020 --> 02:16:59.020]   if to no other.
[02:16:59.020 --> 02:17:02.420]   And please bear in mind, it was the first time in sixty-eight years that I had ever been
[02:17:02.420 --> 02:17:04.140]   on national television.
[02:17:04.140 --> 02:17:07.260]   I wasn't scared I wanted to do my best and not embarrass myself.
[02:17:07.260 --> 02:17:10.780]   But then the most  I thought the most interesting thing was that I decided to respond
[02:17:10.780 --> 02:17:16.100]   to your tweet because I've been watching the tweets all day suggesting that I'm recovering
[02:17:16.100 --> 02:17:20.740]   from a severe stroke and my friends out of their concern for me and my family have been
[02:17:20.740 --> 02:17:26.780]   earnestly forwarding me these tweets, asking me if I'm all right, such is the thing I
[02:17:26.780 --> 02:17:29.180]   wanted to bring up in this germane of this program.
[02:17:29.180 --> 02:17:36.380]   Such as social media, I understand, but I profoundly believe in social media's foundational,
[02:17:36.380 --> 02:17:43.700]   in fact, revolutionary value and contribution to free speech in our country.
[02:17:43.700 --> 02:17:51.780]   This is a sixty-eight-year-old, pretty conservative right-wing Republican judge who one would
[02:17:51.780 --> 02:17:53.700]   expect would say, "Oh, social media."
[02:17:53.700 --> 02:17:54.700]   No, no.
[02:17:54.700 --> 02:18:00.300]   For that reason, I'm willing to accept the occasional bad that comes from social media
[02:18:00.300 --> 02:18:05.820]   in return for the much more frequent good that comes from it, at least from the vastly
[02:18:05.820 --> 02:18:09.740]   more responsible, respectful speech on those media.
[02:18:09.740 --> 02:18:10.740]   I totally agree with that.
[02:18:10.740 --> 02:18:11.740]   Is that awesome?
[02:18:11.740 --> 02:18:12.740]   That's amazing.
[02:18:12.740 --> 02:18:18.340]   I think it's great that somebody who's sixty-eight and conservative is honest enough and perceptive
[02:18:18.340 --> 02:18:21.620]   enough to come to that conclusion because I think that's absolutely true.
[02:18:21.620 --> 02:18:26.700]   It would be more true for a lot more people if more people would learn to block and report.
[02:18:26.700 --> 02:18:27.940]   Yeah, true.
[02:18:27.940 --> 02:18:31.340]   He says, "One more, let me just finish his tweets," one more thing, and then Glenn.
[02:18:31.340 --> 02:18:36.180]   This is why, 16 years after my retirement from the bench, he tweets, "Even then, as
[02:18:36.180 --> 02:18:42.820]   a very skeptical, curmudgeonly old federal judge, I created a Facebook account and then
[02:18:42.820 --> 02:18:47.940]   a Twitter account and again, he mocks himself slowly, very slowly.
[02:18:47.940 --> 02:18:52.660]   One account first and then followed by the other.
[02:18:52.660 --> 02:18:57.180]   All of this said, 'I am not recovering from a stroke or any other malady, I promise.'
[02:18:57.180 --> 02:19:00.900]   Thankfully, I've never been as sick or so debilitated as that ever in my life would
[02:19:00.900 --> 02:19:03.500]   not want that from anyone knock on wood."
[02:19:03.500 --> 02:19:05.180]   So Judge Lutake is fine.
[02:19:05.180 --> 02:19:12.660]   He was intentionally speaking very carefully, precisely, and he appreciated the response
[02:19:12.660 --> 02:19:13.660]   on Twitter.
[02:19:13.660 --> 02:19:14.660]   Amazing.
[02:19:14.660 --> 02:19:15.660]   Go ahead, Glenn.
[02:19:15.660 --> 02:19:18.900]   Well, I have breaking news here from 1860.
[02:19:18.900 --> 02:19:20.940]   This is the most familiar.
[02:19:20.940 --> 02:19:24.220]   Public live coverage in the social media of the day, which were newspapers, which were
[02:19:24.220 --> 02:19:26.180]   often put out in multiple editions a day.
[02:19:26.180 --> 02:19:27.180]   I remember it.
[02:19:27.180 --> 02:19:29.700]   A coverage of Mr. Lincoln's speaking style.
[02:19:29.700 --> 02:19:34.500]   He's rather unsteady in his gate and there is an involuntarily comical awkwardness which
[02:19:34.500 --> 02:19:36.420]   marks his movements while speaking.
[02:19:36.420 --> 02:19:40.780]   His voice, though sharp and powerful at times, has a frequent tendency to dwindle into a
[02:19:40.780 --> 02:19:43.340]   shrill and unpleasant sound.
[02:19:43.340 --> 02:19:48.820]   His enunciation is slow and emphatic and a particular characteristic of his delivery
[02:19:48.820 --> 02:19:53.740]   was a remarkable mobility of his features, the frequent contortions of which excited
[02:19:53.740 --> 02:19:58.380]   the merriment, which his words alone could not well have produced.
[02:19:58.380 --> 02:19:59.380]   Very nice.
[02:19:59.380 --> 02:20:00.380]   There you go.
[02:20:00.380 --> 02:20:01.380]   Very slow speaker.
[02:20:01.380 --> 02:20:02.380]   And occasionally high.
[02:20:02.380 --> 02:20:06.620]   You know how I pitched a little bit.
[02:20:06.620 --> 02:20:07.620]   But it carried.
[02:20:07.620 --> 02:20:08.620]   His voice carried.
[02:20:08.620 --> 02:20:09.620]   It carried.
[02:20:09.620 --> 02:20:11.420]   Daniel Day Lewis nailed it.
[02:20:11.420 --> 02:20:12.420]   Yes.
[02:20:12.420 --> 02:20:15.500]   And I think it was partly based on that report.
[02:20:15.500 --> 02:20:17.100]   Hey, I want to take a break.
[02:20:17.100 --> 02:20:19.220]   A few more things to talk about.
[02:20:19.220 --> 02:20:20.300]   We're not going to wrap it up quite yet.
[02:20:20.300 --> 02:20:23.540]   I know it's been a long show, but I can't stop talking to these guys.
[02:20:23.540 --> 02:20:28.980]   Great to have Christina Warren, Glenn Fleischman, Mike Elgin, you guys rock our show today
[02:20:28.980 --> 02:20:31.180]   brought to you by Blue Land.
[02:20:31.180 --> 02:20:35.140]   You know, California is trying to eliminate plastic waste.
[02:20:35.140 --> 02:20:37.460]   My life trying to eliminate plastic waste.
[02:20:37.460 --> 02:20:42.580]   Did you know that an estimated five billion plastic hands open cleaning bottles are thrown
[02:20:42.580 --> 02:20:43.580]   away every year?
[02:20:43.580 --> 02:20:45.700]   They end up in the landfill where they never degrade.
[02:20:45.700 --> 02:20:47.580]   They never go away.
[02:20:47.580 --> 02:20:50.500]   Don't believe the plastic industry when they say they're recyclable.
[02:20:50.500 --> 02:20:52.660]   They're not.
[02:20:52.660 --> 02:20:58.820]   Blue Land wants to help you do the right thing, eliminate single use plastics.
[02:20:58.820 --> 02:21:04.860]   And by the way, it's also good for the environment because when you buy a bottle of, you know,
[02:21:04.860 --> 02:21:11.300]   this cleaner, multi-surface cleaner, window cleaner, dishwashing soap, laundry detergent,
[02:21:11.300 --> 02:21:15.940]   hand soap, when you buy those bottles, 90% of its water.
[02:21:15.940 --> 02:21:19.380]   So you're transporting all this water around completely unnecessarily.
[02:21:19.380 --> 02:21:24.660]   With Blue Land, you buy these beautiful forever bottles.
[02:21:24.660 --> 02:21:26.620]   They call them Instagrammable.
[02:21:26.620 --> 02:21:27.740]   They're fantastic.
[02:21:27.740 --> 02:21:29.300]   This one is a multi-surface cleaner.
[02:21:29.300 --> 02:21:31.180]   So it's a very lightweight.
[02:21:31.180 --> 02:21:37.380]   They have very solid thick glass for the hand soap, the liquid soap dispensers so that they
[02:21:37.380 --> 02:21:38.380]   stay there.
[02:21:38.380 --> 02:21:39.380]   They're very solid.
[02:21:39.380 --> 02:21:40.380]   They really are beautiful.
[02:21:40.380 --> 02:21:43.900]   You buy them once and you refill them.
[02:21:43.900 --> 02:21:44.900]   But you don't refill.
[02:21:44.900 --> 02:21:51.220]   You use your own water and the active agreements are sent to you in these little tablets.
[02:21:51.220 --> 02:21:52.540]   I love this idea.
[02:21:52.540 --> 02:21:54.340]   You grab one of the beautiful forever bottles.
[02:21:54.340 --> 02:21:55.820]   You fill it with warm water.
[02:21:55.820 --> 02:21:59.540]   You drop in the tablet and you get cleaning.
[02:21:59.540 --> 02:22:01.700]   You'll start at $2.
[02:22:01.700 --> 02:22:04.180]   You don't have to buy a new plastic bottle every time you run out.
[02:22:04.180 --> 02:22:05.180]   They're perfectly made.
[02:22:05.180 --> 02:22:07.140]   They're going to last for a long time.
[02:22:07.140 --> 02:22:10.340]   You can, of course, set up a subscription, which is what I do so you never run out of
[02:22:10.340 --> 02:22:12.100]   the products you use the most.
[02:22:12.100 --> 02:22:17.020]   You save even more when you buy in bulk from cleaning sprays to hands soap to toilet cleaner
[02:22:17.020 --> 02:22:18.220]   and laundry tablets.
[02:22:18.220 --> 02:22:21.180]   Blue Land products are made with ingredients you feel good about.
[02:22:21.180 --> 02:22:27.940]   In fact, the only thing you've got to dispose of is your notion that green, environmentally
[02:22:27.940 --> 02:22:31.620]   friendly products aren't good or are expensive.
[02:22:31.620 --> 02:22:33.300]   They are effective.
[02:22:33.300 --> 02:22:34.300]   They're wonderful.
[02:22:34.300 --> 02:22:35.300]   We use Blue Land everywhere.
[02:22:35.300 --> 02:22:37.220]   We wash our clothes with Blue Land.
[02:22:37.220 --> 02:22:39.180]   We wash our dishes with Blue Land.
[02:22:39.180 --> 02:22:41.220]   We wash our hands with Blue Land.
[02:22:41.220 --> 02:22:44.220]   And now, by the way, and I highly recommend these Blue Land.
[02:22:44.220 --> 02:22:48.420]   They've been out of stock for their toilet cleaning tablets.
[02:22:48.420 --> 02:22:49.780]   Those are back in stock.
[02:22:49.780 --> 02:22:50.780]   Stock up on those.
[02:22:50.780 --> 02:22:51.780]   Those are really fantastic.
[02:22:51.780 --> 02:22:52.780]   They work really well.
[02:22:52.780 --> 02:22:57.860]   In fact, what I did when my daughter moved into a new apartment, you can buy a whole kit.
[02:22:57.860 --> 02:22:59.580]   I gave her a kit to get her started.
[02:22:59.580 --> 02:23:02.300]   Their clean essentials get everything you need.
[02:23:02.300 --> 02:23:08.180]   Blue Land products come in wonderful sense, although we use the unscented laundry soap.
[02:23:08.180 --> 02:23:09.420]   But they do have some lovely sense.
[02:23:09.420 --> 02:23:10.420]   The hand soaps are great.
[02:23:10.420 --> 02:23:13.540]   I got a Christmas package and I smell like a gingerbread house every time I watch my
[02:23:13.540 --> 02:23:14.540]   hands.
[02:23:14.540 --> 02:23:17.980]   They have iris agave, fresh lemon, eucalyptus mint.
[02:23:17.980 --> 02:23:20.140]   For a limited time, hand soap is getting a summer upgrade.
[02:23:20.140 --> 02:23:21.140]   They've been doing this all along.
[02:23:21.140 --> 02:23:22.140]   It's really fun.
[02:23:22.140 --> 02:23:23.580]   We've been using this for a long time.
[02:23:23.580 --> 02:23:27.860]   I will go and get that when they have the new limited edition sense.
[02:23:27.860 --> 02:23:33.220]   This summer, strawberry rhubarb, citrus patchouli and coconut palm.
[02:23:33.220 --> 02:23:35.060]   It makes you want to wash your hands.
[02:23:35.060 --> 02:23:39.580]   I have to say, we've got Blue Land everywhere in the house you will want to get 15% off
[02:23:39.580 --> 02:23:40.860]   your first order.
[02:23:40.860 --> 02:23:43.460]   Go to blueland.com/twit.
[02:23:43.460 --> 02:23:45.220]   These are products that work.
[02:23:45.220 --> 02:23:48.700]   You'll love using them and you'll feel good about them because you know you're eliminating
[02:23:48.700 --> 02:23:50.940]   single use plastics.
[02:23:50.940 --> 02:23:53.740]   That's really been a goal in our house for some time now.
[02:23:53.740 --> 02:24:03.540]   15% off your first order of any Blue Land product, bluelandblueland.com/twit.
[02:24:03.540 --> 02:24:04.540]   This is a great deal.
[02:24:04.540 --> 02:24:05.540]   Take advantage of this.
[02:24:05.540 --> 02:24:10.220]   And again, if you've got a wedding coming up, a housewarming, you know, just even a hostess
[02:24:10.220 --> 02:24:11.620]   gift, this is a great gift.
[02:24:11.620 --> 02:24:12.780]   People love it.
[02:24:12.780 --> 02:24:16.780]   Blueland.com/twit.
[02:24:16.780 --> 02:24:18.660]   We had a great week on Twit.
[02:24:18.660 --> 02:24:20.220]   I think I wasn't here.
[02:24:20.220 --> 02:24:21.220]   I was visiting Mom.
[02:24:21.220 --> 02:24:25.140]   In fact, I'm going to watch with you as we see the highlights.
[02:24:25.140 --> 02:24:34.980]   Taco Bell has announced a tostada on a giant cheese it 18 times the size of a regular cheese
[02:24:34.980 --> 02:24:35.980]   it.
[02:24:35.980 --> 02:24:37.900]   Now this is important news.
[02:24:37.900 --> 02:24:40.060]   You cheese it tostada.
[02:24:40.060 --> 02:24:41.860]   I'm never letting major events.
[02:24:41.860 --> 02:24:42.860]   I'm never letting major events.
[02:24:42.860 --> 02:24:44.820]   American, tourism and technology.
[02:24:44.820 --> 02:24:46.460]   What does it take to make a giant cheese it?
[02:24:46.460 --> 02:24:48.860]   I mean, that's not easy, you know?
[02:24:48.860 --> 02:24:52.300]   It's a bit creepy on Twit.
[02:24:52.300 --> 02:24:53.740]   Hands on photography.
[02:24:53.740 --> 02:24:58.180]   Some of you folks are really curious about starting your own photography business.
[02:24:58.180 --> 02:25:03.220]   Well, I got to tell you, there's a lot of work involved and we're going to dive into that.
[02:25:03.220 --> 02:25:04.540]   Tech News Weekly.
[02:25:04.540 --> 02:25:15.780]   Samsung Electronics announced that they have begun mass production of three nanometer chips.
[02:25:15.780 --> 02:25:19.700]   Yes, from five to three.
[02:25:19.700 --> 02:25:25.020]   The newly developed first gen three nanometer process can reduce power consumption by up
[02:25:25.020 --> 02:25:33.780]   to 45% improve performance by 23% and reduce area, of course, by 16%.
[02:25:33.780 --> 02:25:35.460]   Security now.
[02:25:35.460 --> 02:25:41.860]   As of last week, finally, the masquerade is over and the Conti Ranciero operation has finally
[02:25:41.860 --> 02:25:49.420]   shut down its last public facing infrastructure, which consisted of two Tor servers, which were
[02:25:49.420 --> 02:25:55.340]   used to leak data and to negotiate with victims and of course, no victim has remained.
[02:25:55.340 --> 02:25:57.140]   So they're gone.
[02:25:57.140 --> 02:25:58.140]   Twit.
[02:25:58.140 --> 02:26:01.700]   Friends don't let friends miss Twit.
[02:26:01.700 --> 02:26:03.180]   We are going to have a good week this week.
[02:26:03.180 --> 02:26:06.300]   I am back and I will be in the saddle for all those shows.
[02:26:06.300 --> 02:26:11.820]   Also, by the way, we just figured out that July 12th is going to be a big deal.
[02:26:11.820 --> 02:26:19.140]   That's the day the first scientifically usable pictures will come back from the web telescope.
[02:26:19.140 --> 02:26:26.820]   1 million kilometers, actually more than that, almost 1 million miles out from the earth Lagrange
[02:26:26.820 --> 02:26:33.900]   2, that telescope is fully operational and we expect to see the most distant stars we've
[02:26:33.900 --> 02:26:40.460]   ever seen going back so far that there are literally about 120 million years from the
[02:26:40.460 --> 02:26:41.940]   big bang.
[02:26:41.940 --> 02:26:44.660]   So this is the beginnings of the universe.
[02:26:44.660 --> 02:26:49.900]   So NASA is going to have a press conference, 730 AM Pacific, 1030 Eastern on July 12th.
[02:26:49.900 --> 02:26:51.860]   That's a Tuesday.
[02:26:51.860 --> 02:26:55.980]   Broadpile, the host of this week in space and I and anyone else who wants to join us,
[02:26:55.980 --> 02:26:58.860]   I think John, you will be joining us.
[02:26:58.860 --> 02:26:59.860]   We'll be covering that.
[02:26:59.860 --> 02:27:03.340]   So that's a little bit of an announcement on something we just decided because I'm very
[02:27:03.340 --> 02:27:04.340]   excited about that.
[02:27:04.340 --> 02:27:09.140]   John, you said you had an update on the giant Cheez-It story.
[02:27:09.140 --> 02:27:15.380]   You can order the Cheez-It tostata without toppings and just get a box of Cheez-It.
[02:27:15.380 --> 02:27:19.460]   Oh Lord, help me now.
[02:27:19.460 --> 02:27:22.180]   I've never let Jarvis host Twix again.
[02:27:22.180 --> 02:27:23.620]   That's it.
[02:27:23.620 --> 02:27:28.420]   Actually speaking about chips 3 nanometer TSMC, which was supposed to move to a 3 nanometer
[02:27:28.420 --> 02:27:31.260]   process, apparently having some difficulty.
[02:27:31.260 --> 02:27:36.140]   And now, now that the Apple M2 MacBook Pro is starting to come out, we're seeing some
[02:27:36.140 --> 02:27:39.780]   negative reviews here from Extreme Tech.
[02:27:39.780 --> 02:27:47.140]   Apple's entry level M2 MacBook Pro turns into a Celeron under heavy load.
[02:27:47.140 --> 02:27:48.140]   Ouch.
[02:27:48.140 --> 02:27:53.540]   Yeah, but not unfair if you look at it.
[02:27:53.540 --> 02:27:58.340]   I mean, it seems like they they cheaped out for whatever reason on the SSD for the entry
[02:27:58.340 --> 02:28:00.020]   level versus what we had last year.
[02:28:00.020 --> 02:28:07.860]   And so there is definitely a speed regression if you get the 256 gigabyte or 128 or whatever
[02:28:07.860 --> 02:28:08.860]   size it is.
[02:28:08.860 --> 02:28:10.660]   Don't get the cheap one, in other words.
[02:28:10.660 --> 02:28:14.220]   The baseline is 256 and 8 gigs of RAM.
[02:28:14.220 --> 02:28:20.580]   According to Extreme Tech, the 13 inch bass system gets as hot as 108 degrees Celsius,
[02:28:20.580 --> 02:28:22.580]   which is hotter than boiling.
[02:28:22.580 --> 02:28:23.580]   Oh my God.
[02:28:23.580 --> 02:28:24.580]   That's very hot.
[02:28:24.580 --> 02:28:31.260]   It seems to very much impact the performance of the SSD controller.
[02:28:31.260 --> 02:28:33.100]   So yeah, maybe a design flaw.
[02:28:33.100 --> 02:28:35.660]   This is oddly, this is not a new design.
[02:28:35.660 --> 02:28:40.020]   This is essentially the same design as last year with the M1.
[02:28:40.020 --> 02:28:41.580]   But maybe the M2 maybe it gets hotter.
[02:28:41.580 --> 02:28:42.580]   I don't know.
[02:28:42.580 --> 02:28:45.460]   So just don't know because they're using different for the SSD.
[02:28:45.460 --> 02:28:48.220]   From what I understand, they're using different for the SSD.
[02:28:48.220 --> 02:28:50.020]   And then in the RAM, this is the big thing.
[02:28:50.020 --> 02:28:54.020]   They're basically only using one module rather than two.
[02:28:54.020 --> 02:28:55.020]   Okay.
[02:28:55.020 --> 02:29:04.060]   So, I will just refer you to this Extreme Tech article and the YouTube video that spawned
[02:29:04.060 --> 02:29:05.060]   it.
[02:29:05.060 --> 02:29:07.180]   And I'm sure we'll hear more about this.
[02:29:07.180 --> 02:29:09.380]   We'll talk more about it on Mac break weekly.
[02:29:09.380 --> 02:29:15.380]   But basically, the advice of Extreme Tech is do not buy the entry level M2 Macbook Pro.
[02:29:15.380 --> 02:29:16.860]   There are issues.
[02:29:16.860 --> 02:29:18.260]   There are issues.
[02:29:18.260 --> 02:29:23.460]   There's so many stories we didn't even get to.
[02:29:23.460 --> 02:29:24.460]   Let me see.
[02:29:24.460 --> 02:29:28.100]   Oh, you know, one of the things I wanted to do more happy stories.
[02:29:28.100 --> 02:29:32.500]   And I don't know, Mike, you can advise me as a long time editor.
[02:29:32.500 --> 02:29:37.420]   I have noticed over the last couple of years and I've received some notes from listeners
[02:29:37.420 --> 02:29:40.100]   that tech has really turned negative.
[02:29:40.100 --> 02:29:42.620]   We used to be a little happier, right?
[02:29:42.620 --> 02:29:47.020]   You could be excited about new products and talk about what the new phones are going
[02:29:47.020 --> 02:29:48.660]   to bring.
[02:29:48.660 --> 02:29:53.140]   And now it just feels like it's bad news after bad news.
[02:29:53.140 --> 02:29:59.220]   Is this a normal part of the cycle, the ebb and flow, or are we in a new era of tech?
[02:29:59.220 --> 02:30:00.220]   What do you think, Mike?
[02:30:00.220 --> 02:30:01.860]   Oh, we're in a new era, for sure.
[02:30:01.860 --> 02:30:06.860]   I mean, I think that back in the olden times, 80s, 90s, et cetera.
[02:30:06.860 --> 02:30:07.860]   1860s.
[02:30:07.860 --> 02:30:09.460]   The future was full of promise.
[02:30:09.460 --> 02:30:11.580]   The year 2000 was coming.
[02:30:11.580 --> 02:30:17.660]   And tech, the people who were enthusiastic about tech was a tiny group of nerds who built
[02:30:17.660 --> 02:30:18.660]   their own systems.
[02:30:18.660 --> 02:30:21.220]   Like, you know, it was all about optimizing this, do that.
[02:30:21.220 --> 02:30:27.060]   You know, it was a lot of fun at the rate at which the performance of everything just
[02:30:27.060 --> 02:30:30.060]   was exponential it felt like.
[02:30:30.060 --> 02:30:35.460]   And, you know, sort of the thing about the, you know, how cameras have evolved on cell
[02:30:35.460 --> 02:30:38.740]   phones in the last like 12 years.
[02:30:38.740 --> 02:30:45.300]   But where did the point now where, and this is reflected here in Silicon Valley as well,
[02:30:45.300 --> 02:30:51.180]   what used to be something that was, first of all, understandable, second of all, exciting.
[02:30:51.180 --> 02:30:54.420]   It became this sort of money grab.
[02:30:54.420 --> 02:30:57.860]   So everything's gotten super complex.
[02:30:57.860 --> 02:31:03.980]   There's been this sort of inertia that comes with the entropy of an expanding industry.
[02:31:03.980 --> 02:31:08.620]   And so we have this situation now where so many people are using technology products
[02:31:08.620 --> 02:31:13.060]   that is having social effects and many of these social effects are negative.
[02:31:13.060 --> 02:31:18.500]   You know, teenagers are getting addicted to social media or, you know, committing suicide
[02:31:18.500 --> 02:31:22.740]   because Instagram makes them feel like, you know, they're left out and all this kind of
[02:31:22.740 --> 02:31:23.740]   stuff.
[02:31:23.740 --> 02:31:26.900]   It's, there's all these problems that come from it.
[02:31:26.900 --> 02:31:31.420]   Governments are, you have their hands in it and good governments, bad governments, everybody's
[02:31:31.420 --> 02:31:32.420]   involved.
[02:31:32.420 --> 02:31:37.500]   And so we're in the situation where it feels like there isn't much good news.
[02:31:37.500 --> 02:31:39.380]   We don't really notice the benefits.
[02:31:39.380 --> 02:31:43.900]   We were talking earlier about, you know, how TikTok has this subtle ability to make you
[02:31:43.900 --> 02:31:49.100]   sort of see, have this little window, this quick little window into some faraway place
[02:31:49.100 --> 02:31:51.020]   and some completely alien context.
[02:31:51.020 --> 02:31:54.380]   Well, that's how we probably have a very beneficial effect.
[02:31:54.380 --> 02:32:01.060]   We don't have time to focus on that or notice that because we're so worried about, you know,
[02:32:01.060 --> 02:32:06.700]   the government tracking down people with, with the menstruation apps to prevent them or,
[02:32:06.700 --> 02:32:09.100]   or jail them for, for having an abortion or whatever.
[02:32:09.100 --> 02:32:15.260]   So the bad stuff seems to just take up all the oxygen in the room and we can't really
[02:32:15.260 --> 02:32:16.260]   focus on the good stuff.
[02:32:16.260 --> 02:32:19.820]   But there is really, really good stuff, I think, happening with technology.
[02:32:19.820 --> 02:32:23.260]   It's just, you know, it's just- I feel like I want to focus more on that.
[02:32:23.260 --> 02:32:24.260]   I don't want to.
[02:32:24.260 --> 02:32:25.260]   Yeah.
[02:32:25.260 --> 02:32:27.900]   I don't want to be the, you know, the grim reaper technology.
[02:32:27.900 --> 02:32:28.900]   Yeah.
[02:32:28.900 --> 02:32:30.900]   What do you think, Glenn?
[02:32:30.900 --> 02:32:34.780]   You've, you've, you've, you're equally long time in this business.
[02:32:34.780 --> 02:32:39.780]   I find myself increasingly uninterested in technology, a fun thing to say on a show about
[02:32:39.780 --> 02:32:44.740]   technology, but it's partly because like the things that excite me still are the ways
[02:32:44.740 --> 02:32:46.260]   in which people's lives get improved.
[02:32:46.260 --> 02:32:50.220]   And so to echo what Mike was saying really is just the, you know, the focus, there's
[02:32:50.220 --> 02:32:53.940]   a lot of technology that is insidious because it's a race to the bottom.
[02:32:53.940 --> 02:32:59.740]   It's a race to grab eyeballs and attention and to do increasingly negative things because
[02:32:59.740 --> 02:33:01.620]   that's how the money is raised.
[02:33:01.620 --> 02:33:05.980]   And, you know, it's a famous example that I think is actually made up, but it's the psychology
[02:33:05.980 --> 02:33:10.500]   test where students in a psychology class start only paying attention to a professor
[02:33:10.500 --> 02:33:13.060]   when he or she's on one side of the classroom.
[02:33:13.060 --> 02:33:15.980]   By the end of the term, they have the professor stuck in a corner.
[02:33:15.980 --> 02:33:19.020]   They won't speak unless they're in that corner because no one pays attention to them.
[02:33:19.020 --> 02:33:20.300]   And they don't realize what's going on.
[02:33:20.300 --> 02:33:22.100]   It's a great reverse experiment.
[02:33:22.100 --> 02:33:27.900]   And I feel like everything in technology that involves attention and response puts us in
[02:33:27.900 --> 02:33:28.900]   that corner.
[02:33:28.900 --> 02:33:34.260]   All, you know, all the Facebook doesn't, maybe Facebook intentionally is not trying to allegedly
[02:33:34.260 --> 02:33:40.340]   contribute to genocide, but it does because that's a part of the technology that allows
[02:33:40.340 --> 02:33:43.580]   them to reap such huge financial returns.
[02:33:43.580 --> 02:33:46.420]   So I'd like to look things at, I like to look at enhancements and so forth.
[02:33:46.420 --> 02:33:49.260]   My wife has a bone anchored hearing aid.
[02:33:49.260 --> 02:33:55.260]   Earlier generation, she, she sticks a Bluetooth device on this thing that vibrates and it
[02:33:55.260 --> 02:33:57.940]   sends a signal to her ear and it connects to her iPhone.
[02:33:57.940 --> 02:34:00.100]   She added an upgrade a few releases ago.
[02:34:00.100 --> 02:34:03.500]   And so she can play music and phone calls into her head.
[02:34:03.500 --> 02:34:05.340]   And it's a huge improvement for her life.
[02:34:05.340 --> 02:34:07.780]   So that hearing aid by itself was great technology.
[02:34:07.780 --> 02:34:12.340]   Then the linking of it with Bluetooth improvements and Apple's operating system make it into
[02:34:12.340 --> 02:34:13.340]   a superpower.
[02:34:13.340 --> 02:34:15.260]   It's a, it's a biological enhancement.
[02:34:15.260 --> 02:34:19.860]   In fact, she has, she has abilities beyond people with so-called normative hearing.
[02:34:19.860 --> 02:34:21.540]   And I want to find more things like that.
[02:34:21.540 --> 02:34:24.900]   Like I don't care so much about VR or AR.
[02:34:24.900 --> 02:34:27.900]   I'm not, they're great entertainment opportunities, but I don't think they're going to fail.
[02:34:27.900 --> 02:34:28.900]   I don't think they're going to fail all this zone.
[02:34:28.900 --> 02:34:34.940]   But I'm like, AR is such an incredible capability for younger people, for people with various
[02:34:34.940 --> 02:34:40.540]   disabilities that prevent them from having full access or full use of hearing eyes, whatever.
[02:34:40.540 --> 02:34:44.860]   I think about like the thing that I keep coming back to in my idea of what the future should
[02:34:44.860 --> 02:34:49.460]   be is heads up display, painting our windshields in a car.
[02:34:49.460 --> 02:34:51.860]   I don't want necessarily automatic driving.
[02:34:51.860 --> 02:34:54.540]   I like warnings and things like that that are good.
[02:34:54.540 --> 02:34:57.780]   But I want to be able to drive the road at night and have night vision on the inside of
[02:34:57.780 --> 02:34:59.300]   my windshield.
[02:34:59.300 --> 02:35:00.300]   And why not?
[02:35:00.300 --> 02:35:01.780]   You know, that to me would be an enhancement.
[02:35:01.780 --> 02:35:03.260]   It would reduce accidents.
[02:35:03.260 --> 02:35:07.660]   It would maybe not decrease alertness because it's, it's enhancing your view.
[02:35:07.660 --> 02:35:09.660]   So I look forward to things like that.
[02:35:09.660 --> 02:35:11.180]   And also more cat pictures, please.
[02:35:11.180 --> 02:35:13.260]   I feel like you're an optimistic guy.
[02:35:13.260 --> 02:35:17.460]   So I'm going to take a page from your joy in technology.
[02:35:17.460 --> 02:35:19.980]   I mean, I got into this because I love technology.
[02:35:19.980 --> 02:35:21.820]   I still love it.
[02:35:21.820 --> 02:35:27.540]   But it's so easy to get kind of burdened by all of the negatives and start focusing on
[02:35:27.540 --> 02:35:28.540]   that.
[02:35:28.540 --> 02:35:29.940]   Christina, you're also optimistic.
[02:35:29.940 --> 02:35:33.380]   You're a very positive person and you were a technology.
[02:35:33.380 --> 02:35:34.380]   I try to be.
[02:35:34.380 --> 02:35:35.380]   Yeah.
[02:35:35.380 --> 02:35:40.580]   I mean, look, I think that it can become overwhelming to just look at because there are a lot of
[02:35:40.580 --> 02:35:42.100]   terrible things that are happening in the world.
[02:35:42.100 --> 02:35:45.220]   And there are a lot of terrible things that have happened in the last two years.
[02:35:45.220 --> 02:35:48.540]   And technology is a force for good and it's a force for bad.
[02:35:48.540 --> 02:35:52.740]   But I think that on the whole of it, it has made our lives better.
[02:35:52.740 --> 02:35:57.060]   I know that my life is infinitely better because of technology, because of social media, because
[02:35:57.060 --> 02:35:58.060]   of the web.
[02:35:58.060 --> 02:35:59.060]   Cat pictures.
[02:35:59.060 --> 02:36:00.060]   Right.
[02:36:00.060 --> 02:36:01.060]   Right.
[02:36:01.060 --> 02:36:02.060]   Yeah, totally.
[02:36:02.060 --> 02:36:03.060]   Right.
[02:36:03.060 --> 02:36:04.060]   Like, because of the way that I can communicate with people.
[02:36:04.060 --> 02:36:06.300]   Even what happened to us in the last two years, right?
[02:36:06.300 --> 02:36:09.980]   Like, as bad as that was, it would have been worse if we didn't all imagine, you know,
[02:36:09.980 --> 02:36:13.340]   the ability to communicate with each other over video chat, right?
[02:36:13.340 --> 02:36:14.340]   Yeah.
[02:36:14.340 --> 02:36:17.540]   Or, or, you know, even through, through text messages or phone calls, right?
[02:36:17.540 --> 02:36:22.140]   Which, needless to say, the last time we had one of these things, you know, 100 years
[02:36:22.140 --> 02:36:24.580]   ago, you didn't have that.
[02:36:24.580 --> 02:36:26.900]   I don't like yourself in your room.
[02:36:26.900 --> 02:36:27.900]   Right.
[02:36:27.900 --> 02:36:29.900]   And wave for now.
[02:36:29.900 --> 02:36:30.900]   Yeah.
[02:36:30.900 --> 02:36:34.140]   To say nothing of vaccines and the stuff that happened there.
[02:36:34.140 --> 02:36:36.340]   So I don't know.
[02:36:36.340 --> 02:36:40.340]   I have to work at it because it's easy for me, even though I am a typically, like, I'm
[02:36:40.340 --> 02:36:41.340]   not a cynical person.
[02:36:41.340 --> 02:36:44.820]   I mean, I have cynical aspects, but I try to be a fairly positive person.
[02:36:44.820 --> 02:36:49.700]   It's hard sometimes not to be taken in by all the cynicism.
[02:36:49.700 --> 02:36:53.660]   I also don't want to be a cheerleader for the industries that sometimes I think that maybe
[02:36:53.660 --> 02:36:58.900]   in the past, we were too credulous and we were too positive about certain things and
[02:36:58.900 --> 02:37:00.580]   allowed certain things to happen.
[02:37:00.580 --> 02:37:03.500]   So I think that there's a good and a bad, but sometimes, but I do fear sometimes when
[02:37:03.500 --> 02:37:08.020]   I read some of the coverage and I read the lens of someone who loves technology, used
[02:37:08.020 --> 02:37:10.260]   to be a reporter and now works for a tech company.
[02:37:10.260 --> 02:37:15.740]   And so I have, I will say I have a different perspective, which is good to have.
[02:37:15.740 --> 02:37:21.820]   Sometimes I worry that too much of the coverage is negative just to be negative, critical
[02:37:21.820 --> 02:37:23.060]   just to be critical.
[02:37:23.060 --> 02:37:24.460]   And I'm here for the criticism.
[02:37:24.460 --> 02:37:27.540]   I'm here for the pushback, but there has to be a balance.
[02:37:27.540 --> 02:37:30.980]   And I do sometimes wonder if we swung too far in the other way.
[02:37:30.980 --> 02:37:37.660]   Like we went from being way too credulous, way too, you know, laudatory to now completely
[02:37:37.660 --> 02:37:43.220]   ignoring the positive things that can happen because of tech.
[02:37:43.220 --> 02:37:45.420]   Leo, can I make a couple of points?
[02:37:45.420 --> 02:37:46.420]   Sure.
[02:37:46.420 --> 02:37:47.420]   Not to miss the forest for the trees.
[02:37:47.420 --> 02:37:49.700]   Talk about the forest a bit.
[02:37:49.700 --> 02:37:51.780]   First of all, Christina mentioned vaccines.
[02:37:51.780 --> 02:37:57.180]   If the coronavirus had hit in the 1970s, say there would have been no working from home
[02:37:57.180 --> 02:37:58.980]   and no vaccine for 10 years.
[02:37:58.980 --> 02:38:00.660]   It's unimaginable.
[02:38:00.660 --> 02:38:02.300]   It's really unimaginable.
[02:38:02.300 --> 02:38:07.300]   But the forest that I'd like to talk about is my own lifestyle of living internationally,
[02:38:07.300 --> 02:38:10.740]   which the world has discovered because of the pandemic.
[02:38:10.740 --> 02:38:12.900]   People are working remotely, working from home.
[02:38:12.900 --> 02:38:15.580]   There's a sort of thing where people start working from their apartment in the city.
[02:38:15.580 --> 02:38:16.820]   Then they decide to move out of the city.
[02:38:16.820 --> 02:38:18.220]   And they're like, what are we doing in the suburbs?
[02:38:18.220 --> 02:38:23.060]   Why don't we go to, you know, South America and live in Columbia for a while.
[02:38:23.060 --> 02:38:25.700]   And the internet connectivity keeps getting better.
[02:38:25.700 --> 02:38:27.940]   We just did the, you know, the Provence experience.
[02:38:27.940 --> 02:38:30.420]   We always do that in this 400 year old farmhouse.
[02:38:30.420 --> 02:38:35.180]   The walls are three and a half feet thick, but it's very modern inside in every way except
[02:38:35.180 --> 02:38:41.420]   the internet connectivity, which is a really, really slow, like 3G mobile broadband connection.
[02:38:41.420 --> 02:38:43.540]   Well, this is a good question.
[02:38:43.540 --> 02:38:46.140]   Well, we showed up this year and guess what?
[02:38:46.140 --> 02:38:47.140]   They now have fiber.
[02:38:47.140 --> 02:38:48.140]   Oh, wow.
[02:38:48.140 --> 02:38:51.620]   In the middle of, like out in the countryside.
[02:38:51.620 --> 02:38:52.620]   That's amazing.
[02:38:52.620 --> 02:38:53.620]   Amazing.
[02:38:53.620 --> 02:38:58.340]   So people can actually live, and also, by the way, Starlink is available throughout
[02:38:58.340 --> 02:38:59.340]   the crisis.
[02:38:59.340 --> 02:39:00.940]   Starlink, I was going to say, is massive.
[02:39:00.940 --> 02:39:06.340]   So you can, you can do a full on, like under the Tuscan Sun, buy a derelict old farmhouse
[02:39:06.340 --> 02:39:09.020]   somewhere in the sunny parts of Europe.
[02:39:09.020 --> 02:39:13.020]   And you can build up this thing and you could live in paradise, have a garden, have a beautiful
[02:39:13.020 --> 02:39:19.300]   view, have clean air, clean water, clean everything, solar power, and you can have fiber optic
[02:39:19.300 --> 02:39:25.260]   like speeds and make your living in paradise.
[02:39:25.260 --> 02:39:30.140]   This is an opportunity that has never been afforded to anyone in the history of mankind.
[02:39:30.140 --> 02:39:33.580]   And it's pretty great for those who really want to live that way.
[02:39:33.580 --> 02:39:34.580]   Excellent.
[02:39:34.580 --> 02:39:36.100]   I feel better already.
[02:39:36.100 --> 02:39:41.940]   I'd point out one more thing if I might, which is that photography is an almost unalloyed
[02:39:41.940 --> 02:39:44.020]   joy that we breathe.
[02:39:44.020 --> 02:39:47.820]   And so we don't pay, it's like, it's like, unlike Mike Scampledus now, it's like Wi-Fi
[02:39:47.820 --> 02:39:48.820]   is everywhere.
[02:39:48.820 --> 02:39:50.500]   I used to cover Wi-Fi very extensively.
[02:39:50.500 --> 02:39:54.980]   Then I suddenly was not a Wi-Fi reporter because it just, it permeates everything.
[02:39:54.980 --> 02:39:56.900]   Then we got good cellular data.
[02:39:56.900 --> 02:40:02.620]   The abilities we have to take a good picture and to take as many as we want and capture
[02:40:02.620 --> 02:40:05.340]   great moments in our lives are incredible.
[02:40:05.340 --> 02:40:09.380]   And then I will call out, there's one thing that makes me consistently happy every day,
[02:40:09.380 --> 02:40:13.300]   that is technology, which is I use the featured photo thing on my iPhone.
[02:40:13.300 --> 02:40:14.940]   I have it as a widget.
[02:40:14.940 --> 02:40:20.940]   And most days, Apple's machine learning comes up with wonderful images across every, and
[02:40:20.940 --> 02:40:23.220]   I've scanned photos before they were digital photos.
[02:40:23.220 --> 02:40:27.860]   So I have photos going back to the 80s in my library.
[02:40:27.860 --> 02:40:30.940]   And so every day I get to look at these great pictures of my kids when they're babies and
[02:40:30.940 --> 02:40:36.260]   photos from a few weeks ago and pictures of bees I took 20 years ago or some trip.
[02:40:36.260 --> 02:40:39.660]   And I'm like, ah, ah, ah, and I don't have to go searching for it.
[02:40:39.660 --> 02:40:42.020]   It shows up and so it's machine learning.
[02:40:42.020 --> 02:40:43.180]   It's a modern device.
[02:40:43.180 --> 02:40:45.900]   It's the access through the cloud to all my photos.
[02:40:45.900 --> 02:40:48.940]   And I'm like, it combines to this wonderful thing.
[02:40:48.940 --> 02:40:51.580]   So I have a moment of happiness every time I open that.
[02:40:51.580 --> 02:40:56.380]   I guess it really is a question of focusing on the benefits and the good stuff.
[02:40:56.380 --> 02:40:58.500]   Because there's negatives and positives and everything.
[02:40:58.500 --> 02:41:00.500]   And you don't have to dwell on the negatives.
[02:41:00.500 --> 02:41:01.500]   We know they're there.
[02:41:01.500 --> 02:41:03.060]   I don't see lawn musk.
[02:41:03.060 --> 02:41:05.100]   And then there's Elon Musk and the Pope.
[02:41:05.100 --> 02:41:11.740]   Hey, one good thing, the presidential medal of freedom, the highest civilian award being
[02:41:11.740 --> 02:41:21.500]   awarded July 7th, 17 luminaries to Denzel Washington, Simone Biles, Megan Rapinoe.
[02:41:21.500 --> 02:41:27.420]   John McCain will get a posthumous award and Steve Jobs.
[02:41:27.420 --> 02:41:34.860]   Eleven years after his passing, we'll be receiving the posthumously the Congressional Medal of
[02:41:34.860 --> 02:41:35.860]   Honor.
[02:41:35.860 --> 02:41:40.860]   I hope that his wife, Lorraine Powell, Jobs is there to accept on her late husband's
[02:41:40.860 --> 02:41:41.860]   behalf.
[02:41:41.860 --> 02:41:44.140]   I think that'd be very fitting.
[02:41:44.140 --> 02:41:46.540]   And basically 15 years after the iPhone, right?
[02:41:46.540 --> 02:41:48.300]   Which, like, that's right.
[02:41:48.300 --> 02:41:54.900]   Which is, which, I mean, it's hard to even, I mean, it is easy to remember what life was
[02:41:54.900 --> 02:41:59.020]   like before, but just the seismic impact his devices had.
[02:41:59.020 --> 02:42:00.260]   I mean, yeah, yeah.
[02:42:00.260 --> 02:42:01.460]   Let's be honest.
[02:42:01.460 --> 02:42:02.460]   Let's be honest.
[02:42:02.460 --> 02:42:05.020]   It's not a knighthood like Johnny I've got, but it's funny.
[02:42:05.020 --> 02:42:06.900]   And well deserved.
[02:42:06.900 --> 02:42:07.980]   But Johnny didn't meet the Queen.
[02:42:07.980 --> 02:42:09.780]   He only got Queen Anne's princess Anne.
[02:42:09.780 --> 02:42:12.460]   So it's, you know, it's good point.
[02:42:12.460 --> 02:42:14.260]   All right.
[02:42:14.260 --> 02:42:15.620]   Final jeopardy.
[02:42:15.620 --> 02:42:21.660]   The subject, the world of today, I'm going to pose this question to all of you.
[02:42:21.660 --> 02:42:26.540]   I will say that on this episode of Jeopardy, only two people got it right.
[02:42:26.540 --> 02:42:27.540]   Oh, come on.
[02:42:27.540 --> 02:42:29.860]   Partly because it was a monocilable.
[02:42:29.860 --> 02:42:39.180]   This word was chosen as, quote, a noun that conveys the idea of a unit of cultural transmission.
[02:42:39.180 --> 02:42:40.180]   Yeah.
[02:42:40.180 --> 02:42:41.180]   You know the answer, Christina.
[02:42:41.180 --> 02:42:42.180]   I don't listen to the button.
[02:42:42.180 --> 02:42:43.180]   Nothing's happening.
[02:42:43.180 --> 02:42:44.740]   It's final jeopardy.
[02:42:44.740 --> 02:42:47.820]   You all get to answer Christina Warren.
[02:42:47.820 --> 02:42:48.820]   Meme.
[02:42:48.820 --> 02:42:50.420]   In the form of a question.
[02:42:50.420 --> 02:42:51.420]   In the form of a question.
[02:42:51.420 --> 02:42:52.420]   What is a meme?
[02:42:52.420 --> 02:42:53.420]   Yep.
[02:42:53.420 --> 02:42:54.420]   Absolutely right.
[02:42:54.420 --> 02:42:55.820]   I know you knew that Glenn.
[02:42:55.820 --> 02:43:04.420]   In fact, probably everybody watching knew that the word meme coined by evolutionary scientist
[02:43:04.420 --> 02:43:06.740]   Richard Dawkins.
[02:43:06.740 --> 02:43:10.340]   And it is in fact, of course, part of our regular vocabulary today.
[02:43:10.340 --> 02:43:11.340]   Only two.
[02:43:11.340 --> 02:43:13.260]   Only one out of the three got it right.
[02:43:13.260 --> 02:43:14.260]   Only one of the three got it.
[02:43:14.260 --> 02:43:15.260]   That's wow.
[02:43:15.260 --> 02:43:17.380]   Because I was reading the question.
[02:43:17.380 --> 02:43:18.780]   I didn't even get to the end of the question.
[02:43:18.780 --> 02:43:19.780]   I was reading it.
[02:43:19.780 --> 02:43:20.780]   No, immediately.
[02:43:20.780 --> 02:43:21.780]   And I was like, oh, yeah, I got it.
[02:43:21.780 --> 02:43:22.900]   It's like, it's something.
[02:43:22.900 --> 02:43:25.780]   One person answered, um.
[02:43:25.780 --> 02:43:32.740]   It was not the best final jeopardy ever.
[02:43:32.740 --> 02:43:35.060]   And Glenn, I know you would have gotten that one.
[02:43:35.060 --> 02:43:36.060]   No problem.
[02:43:36.060 --> 02:43:37.060]   Yes.
[02:43:37.060 --> 02:43:39.140]   Two time jeopardy champion.
[02:43:39.140 --> 02:43:42.900]   Three, three times correct in final jeopardy.
[02:43:42.900 --> 02:43:44.540]   But only one place.
[02:43:44.540 --> 02:43:45.540]   Oh, man.
[02:43:45.540 --> 02:43:46.780]   That last one they got you.
[02:43:46.780 --> 02:43:47.780]   You didn't bet enough.
[02:43:47.780 --> 02:43:48.780]   I can't know.
[02:43:48.780 --> 02:43:49.780]   I didn't have enough money left.
[02:43:49.780 --> 02:43:52.060]   I said George Sands instead of George sand.
[02:43:52.060 --> 02:43:53.060]   And I didn't have money.
[02:43:53.060 --> 02:43:54.060]   I never forgot.
[02:43:54.060 --> 02:43:55.060]   Oh my God.
[02:43:55.060 --> 02:43:56.060]   I've gotten over it.
[02:43:56.060 --> 02:43:57.060]   Dance with me.
[02:43:57.060 --> 02:43:58.060]   George.
[02:43:58.060 --> 02:43:59.060]   Dance with me.
[02:43:59.060 --> 02:44:00.060]   Oh my God.
[02:44:00.060 --> 02:44:01.060]   Oh, sorry.
[02:44:01.060 --> 02:44:02.060]   Alex.
[02:44:02.060 --> 02:44:05.860]   Glenn Fleischman, what a pleasure.
[02:44:05.860 --> 02:44:06.860]   Glenn.fun.
[02:44:06.860 --> 02:44:07.860]   Always great to have you on.
[02:44:07.860 --> 02:44:10.420]   You will be back on Wednesday for this week in Google.
[02:44:10.420 --> 02:44:13.780]   I hope we haven't exhausted all of your resources.
[02:44:13.780 --> 02:44:15.940]   I have a feeling we have not.
[02:44:15.940 --> 02:44:16.940]   Not yet.
[02:44:16.940 --> 02:44:19.780]   But if you wish you can speak very slowly.
[02:44:19.780 --> 02:44:26.220]   If I like to use deliberative tones.
[02:44:26.220 --> 02:44:27.220]   Thank you Glenn.
[02:44:27.220 --> 02:44:28.220]   So nice to have you.
[02:44:28.220 --> 02:44:29.900]   Mike Elgin, God bless you.
[02:44:29.900 --> 02:44:37.220]   Give my love to Amira, to Kevin, to Princess Squishy Face, to Nadja, the whole family.
[02:44:37.220 --> 02:44:38.220]   Thank you.
[02:44:38.220 --> 02:44:44.860]   I'm Ned.net if you want to go travel with these incredible people to some of the most beautiful
[02:44:44.860 --> 02:44:49.460]   places in the world and eat and drink like a prince.
[02:44:49.460 --> 02:44:50.460]   It's amazing.
[02:44:50.460 --> 02:44:51.460]   Yes.
[02:44:51.460 --> 02:44:53.100]   Highly recommended.
[02:44:53.100 --> 02:44:57.900]   Mike also writes for Computer World and many other publications and he's got a newsletter,
[02:44:57.900 --> 02:45:01.100]   Mike's List and of course Elgin.com is the website for that.
[02:45:01.100 --> 02:45:02.100]   Thank you Mike.
[02:45:02.100 --> 02:45:03.100]   Always a pleasure.
[02:45:03.100 --> 02:45:05.780]   I'm sorry we couldn't get together because I probably would have loved that wine.
[02:45:05.780 --> 02:45:08.060]   But you know, such is life.
[02:45:08.060 --> 02:45:09.060]   Such is life.
[02:45:09.060 --> 02:45:10.060]   Yes.
[02:45:10.060 --> 02:45:11.060]   Thank you.
[02:45:11.060 --> 02:45:15.260]   Such is life in the COVID era, which apparently is going on and on and on.
[02:45:15.260 --> 02:45:18.260]   Christina Warren, a pleasure seeing you too.
[02:45:18.260 --> 02:45:21.900]   Although because I follow you on Instagram, I feel like I know every hotel room you're
[02:45:21.900 --> 02:45:22.900]   in.
[02:45:22.900 --> 02:45:23.900]   Yes.
[02:45:23.900 --> 02:45:26.740]   Senior dev advocate at GitHub.
[02:45:26.740 --> 02:45:29.060]   Her GitHub name is Film Girl.
[02:45:29.060 --> 02:45:32.140]   Her Twitter handle is Film_Girl.
[02:45:32.140 --> 02:45:33.900]   So is her Instagram.
[02:45:33.900 --> 02:45:36.100]   Anything else you want to mention or plug her?
[02:45:36.100 --> 02:45:37.100]   Yes.
[02:45:37.100 --> 02:45:38.100]   So I do two podcasts.
[02:45:38.100 --> 02:45:39.100]   I do one called OverTired.
[02:45:39.100 --> 02:45:40.100]   OverTired.com.
[02:45:40.100 --> 02:45:42.100]   I didn't know about OverTired.
[02:45:42.100 --> 02:45:43.100]   Yeah.
[02:45:43.100 --> 02:45:44.100]   OverTired is great.
[02:45:44.100 --> 02:45:47.620]   It's basically we joke that it's a Taylor Swift podcast.
[02:45:47.620 --> 02:45:48.620]   It's not.
[02:45:48.620 --> 02:45:53.380]   It's really about some ADHD nerds who and the things that keep us up at night.
[02:45:53.380 --> 02:45:54.940]   And it's great.
[02:45:54.940 --> 02:45:55.940]   And then I also do.
[02:45:55.940 --> 02:45:57.740]   Oh, it's with that Brett Terpstra.
[02:45:57.740 --> 02:45:58.740]   Oh, nice.
[02:45:58.740 --> 02:45:59.740]   Brett Terpstra.
[02:45:59.740 --> 02:46:00.740]   Yeah.
[02:46:00.740 --> 02:46:01.740]   Brett's amazing.
[02:46:01.740 --> 02:46:05.420]   I have someone's console recently joined us as our third host.
[02:46:05.420 --> 02:46:11.620]   And I also do Rocket on really FM and with Brianna Wu and Simone de Roche for.
[02:46:11.620 --> 02:46:12.620]   So.
[02:46:12.620 --> 02:46:13.620]   Very nice.
[02:46:13.620 --> 02:46:18.060]   When I listen to me, I've got other podcasts, but also like frankly like my Instagram.
[02:46:18.060 --> 02:46:20.060]   But the hotel tours is really what you want.
[02:46:20.060 --> 02:46:21.060]   When did you start over Tired?
[02:46:21.060 --> 02:46:22.060]   I want to have to listen to that.
[02:46:22.060 --> 02:46:24.060]   That sounds really great.
[02:46:24.060 --> 02:46:27.460]   Honestly, like eight or nine years ago.
[02:46:27.460 --> 02:46:28.460]   What?
[02:46:28.460 --> 02:46:31.260]   It's been intermittent, but we've been consistent for the last year.
[02:46:31.260 --> 02:46:32.820]   So we've been constantly every week.
[02:46:32.820 --> 02:46:33.820]   I knew about Rocket.
[02:46:33.820 --> 02:46:34.820]   I love Rocket.
[02:46:34.820 --> 02:46:35.820]   So that's.
[02:46:35.820 --> 02:46:36.820]   Yeah.
[02:46:36.820 --> 02:46:37.820]   Yeah.
[02:46:37.820 --> 02:46:38.820]   Over tired had periods of inactivity.
[02:46:38.820 --> 02:46:39.820]   We should.
[02:46:39.820 --> 02:46:40.820]   Naturally.
[02:46:40.820 --> 02:46:41.820]   But we're tired.
[02:46:41.820 --> 02:46:42.820]   I understand that.
[02:46:42.820 --> 02:46:43.820]   Exactly.
[02:46:43.820 --> 02:46:44.820]   Naturally, we were tired.
[02:46:44.820 --> 02:46:47.700]   I came up with the name actually we were at like Twitter HQ and we were trying to come
[02:46:47.700 --> 02:46:48.700]   up with the name.
[02:46:48.700 --> 02:46:49.700]   I was in the elevator.
[02:46:49.700 --> 02:46:50.700]   I'll never forget.
[02:46:50.700 --> 02:46:51.700]   And I was like, I'm so tired.
[02:46:51.700 --> 02:46:52.700]   I'm over tired.
[02:46:52.700 --> 02:46:53.700]   I was like, over tired.
[02:46:53.700 --> 02:46:56.700]   That's such a good name for a podcast.
[02:46:56.700 --> 02:46:57.700]   Wow.
[02:46:57.700 --> 02:47:00.380]   It has so many levels of meaning.
[02:47:00.380 --> 02:47:01.380]   It's wonderful to have you.
[02:47:01.380 --> 02:47:02.740]   Thank you all three of you.
[02:47:02.740 --> 02:47:04.980]   We really appreciate your being here.
[02:47:04.980 --> 02:47:11.300]   Have a great 4th of July tomorrow fireworks grilling fun.
[02:47:11.300 --> 02:47:13.040]   Wave the flag.
[02:47:13.040 --> 02:47:17.300]   We're not doing fireworks here in Petaluma because of the drought and the fire hazard
[02:47:17.300 --> 02:47:18.900]   plus the city's broke.
[02:47:18.900 --> 02:47:21.060]   So we're going to have a laser light show instead.
[02:47:21.060 --> 02:47:22.060]   Everybody bring your lasers.
[02:47:22.060 --> 02:47:23.060]   It's going to be.
[02:47:23.060 --> 02:47:24.060]   Love it.
[02:47:24.060 --> 02:47:26.180]   Love it.
[02:47:26.180 --> 02:47:27.180]   We do.
[02:47:27.180 --> 02:47:31.140]   Every Sunday afternoon, 2 p.m. Pacific 5 p.m. Eastern 2100 UTC.
[02:47:31.140 --> 02:47:32.820]   You can watch us live live.twit.tv.
[02:47:32.820 --> 02:47:37.140]   Actually, watch or listen, there's live audio and video streams.
[02:47:37.140 --> 02:47:40.660]   If you're watching live chat with us at IRC.twit.tv.
[02:47:40.660 --> 02:47:44.940]   You can also chat with us in our Discord, which is purple.
[02:47:44.940 --> 02:47:52.060]   But really, comes by and honestly, the Discord is actually my favorite new social media place
[02:47:52.060 --> 02:47:53.660]   to be.
[02:47:53.660 --> 02:47:55.660]   The conversations aren't just about the shows.
[02:47:55.660 --> 02:47:58.020]   In fact, our Discord has topics.
[02:47:58.020 --> 02:48:06.260]   Every geek would be interested in no flongs yet, but comics for sure, gaming, hacking,
[02:48:06.260 --> 02:48:13.140]   ham radio, movies, music, software, sport ball, travel, trivia.
[02:48:13.140 --> 02:48:16.780]   And you can even be part of our Minecraft server.
[02:48:16.780 --> 02:48:20.300]   All of this plus ad-free versions of all the shows and the special Twit Plus feed, which
[02:48:20.300 --> 02:48:26.620]   has a lot of material that doesn't make it to the air, as so to speak, and at $7 a month,
[02:48:26.620 --> 02:48:27.860]   which I think is a great deal.
[02:48:27.860 --> 02:48:30.740]   We would really appreciate it if you join us.
[02:48:30.740 --> 02:48:35.300]   There's also a yearly, what is that, $84 if you just want to pay once and get it over
[02:48:35.300 --> 02:48:39.380]   with that makes a great gift to, and it really helps us.
[02:48:39.380 --> 02:48:41.860]   A recession is looming.
[02:48:41.860 --> 02:48:46.900]   And we already see some decrease in ad revenue.
[02:48:46.900 --> 02:48:49.820]   I think this is going to be as bad as it was when COVID started.
[02:48:49.820 --> 02:48:51.940]   So this helps us get over those hums.
[02:48:51.940 --> 02:48:56.100]   Plus, it lets us develop new shows, bring in new talent, and we've got some exciting
[02:48:56.100 --> 02:48:57.980]   news in that area coming soon.
[02:48:57.980 --> 02:49:02.060]   So please join club twit.tv/club twit.
[02:49:02.060 --> 02:49:08.260]   Of course, all of our shows are available for free after the fact on the website, twit.tv.
[02:49:08.260 --> 02:49:11.860]   Every show has its own YouTube channel, as does this show, of course.
[02:49:11.860 --> 02:49:17.420]   Best way to get the show, probably be subscribing your favorite podcast client, you get it automatically.
[02:49:17.420 --> 02:49:22.840]   the minute it's available. Now you have a great show to listen to on a Monday morning.
[02:49:22.840 --> 02:49:28.980]   Thank you for being here. As you have I think many of you, for the past seventeen years.
[02:49:28.980 --> 02:49:43.600]   That's now and once again time for me to say goodbye. Another twins.
[02:49:43.600 --> 02:49:46.200]   Alright, doin' the twit, baby.
[02:49:46.200 --> 02:49:47.200]   Do it until it...

