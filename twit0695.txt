
[00:00:00.000 --> 00:00:05.520]   It's time for Twit this week in Tech, a very special episode to vendor Hardewar is here.
[00:00:05.520 --> 00:00:09.840]   He's got a new baby, Dwight Silverman's here, he's got a new tech burger,
[00:00:09.840 --> 00:00:16.000]   and Nate Langston visits from Bloomberg. We're going to talk about social networks of the future,
[00:00:16.000 --> 00:00:21.280]   and whether you should sign your newborn up for Instagram, the gross in Bopis,
[00:00:21.280 --> 00:00:27.200]   I think Dwight didn't really like that term, and Amazon's reinvent, all the things they announced.
[00:00:27.200 --> 00:00:29.280]   It's all coming up next on Twit.
[00:00:29.280 --> 00:00:33.920]   Netcast you love.
[00:00:33.920 --> 00:00:35.920]   From people you trust.
[00:00:35.920 --> 00:00:41.360]   This is Twit.
[00:00:41.360 --> 00:00:55.520]   This is Twit this week in Tech, episode 695 recorded Sunday December 2nd, 2018.
[00:00:55.520 --> 00:00:56.800]   Friends in Bikini's.
[00:00:56.800 --> 00:01:04.160]   This week in Tech is brought to you by Hopsy, the Nespresso or Currig, if you will, for beer.
[00:01:04.160 --> 00:01:09.520]   And just for the holidays, visit trihopsy.com/twit and use the promo code Twit to get the
[00:01:09.520 --> 00:01:15.440]   sub-home draft system, too many kegs of beer, two Hopsy glasses, and a free membership in the
[00:01:15.440 --> 00:01:19.520]   monthly beer club, all for only $99. Terms and conditions apply.
[00:01:20.480 --> 00:01:26.640]   And by WordPress. Reach more customers when you build your business website on WordPress.com.
[00:01:26.640 --> 00:01:33.120]   Plan start at just $4 a month. Get 15% off any new plan at WordPress.com/twit.
[00:01:33.120 --> 00:01:39.920]   And by Audible. For a limited time, you can get three months of Audible for just $6.95 a month.
[00:01:39.920 --> 00:01:44.880]   Go to audible.com/twit or text Twit to 500-500.
[00:01:45.600 --> 00:01:50.800]   And by Rocket Mortgage from Quick and Loans, introducing rate shield approval. If you're in the
[00:01:50.800 --> 00:01:55.680]   market to buy a home, rate shield approval locks up your rate for up to 90 days while you shop.
[00:01:55.680 --> 00:02:01.760]   It's a real game changer. Learn more and get started at rocketmortgage.com/twit2.
[00:02:01.760 --> 00:02:07.680]   It's time for Twit this week in Tech, the show where you cover the week's tech news
[00:02:07.680 --> 00:02:13.120]   with whatever tech journalists we can scrape together. No, with some of the best names and
[00:02:13.120 --> 00:02:17.920]   voices in the business. Like our longtime friend Dwight Silverman has been doing this show for
[00:02:17.920 --> 00:02:21.600]   I would say a decade, right, Dwight? Oh, probably more than that.
[00:02:21.600 --> 00:02:29.760]   When I last looked, I think I may be like the third or fourth longest running or something like that.
[00:02:29.760 --> 00:02:35.360]   I was kind of surprised how far up the chain. Dwight took a hiatus for reasons no one will
[00:02:35.360 --> 00:02:40.880]   understand he got out of tech journalism. Thank God they pulled him back in at the Houston Chronicle.
[00:02:40.880 --> 00:02:47.120]   He's responsible for tech burger. Tech burner. I tried to get out and they keep pulling it back.
[00:02:47.120 --> 00:02:53.600]   Also here from Engadget. Oh, another longtime friend of the show, Devindra Hardawar. I always
[00:02:53.600 --> 00:02:56.480]   love having you on. He's senior editor and Engadget. Hello, Devindra.
[00:02:56.480 --> 00:03:00.960]   Hey, happy to be here. As always. There's been a happy event since the last time we talked.
[00:03:00.960 --> 00:03:09.280]   A new Hardawar. A new Hardawar. Sophia Lea Hardawar has been born. She's very loud. She doesn't
[00:03:09.280 --> 00:03:13.600]   sleep and she doesn't let us sleep. But I love her anyway. Oh, wait a minute.
[00:03:13.600 --> 00:03:21.600]   Did you name her for Princess Lea? I mean, we wanted a vaguely like Hindu name. And that is,
[00:03:21.600 --> 00:03:27.680]   that name actually works across Hindi and Spanish, which my wife and I both really like. So there's
[00:03:27.680 --> 00:03:32.800]   that and it sounds like Lea, which is super cool. So I totally went there. Is Sophia a common Hindu name?
[00:03:32.800 --> 00:03:36.880]   No, not at all. Lea is the middle name. That's what we're going for.
[00:03:36.880 --> 00:03:42.960]   Got it. When you go, when you go to India, you call her Lea. Here you call her Sophie.
[00:03:42.960 --> 00:03:46.800]   When you go to America, you call her Sophia. It's perfect.
[00:03:46.800 --> 00:03:51.760]   Brand will be Sophia Lea. That's my plan. That's a good brand. Isn't it sad that we have to plan
[00:03:51.760 --> 00:03:56.880]   our children's brands? I'm like, should I register all her social accounts now? Should I hire that name?
[00:03:56.880 --> 00:04:02.600]   Yes. Absolutely. Yeah. In fact, I'm shocked you haven't. I got
[00:04:02.600 --> 00:04:07.800]   Abby Leport and Henry Leport.com, the right away. And I go, wow, but it is. I get 20 year res,
[00:04:07.800 --> 00:04:13.560]   you can get a 20 year registration just in case. Wow. And Abby's now using hers. I think Henry's
[00:04:13.560 --> 00:04:18.600]   still kind of waffling, but I figured eventually they'd get around to using it. Of course, by the
[00:04:18.600 --> 00:04:23.480]   time we get off the show, it will have been registered by somebody. Yeah. If you want to take a little
[00:04:23.480 --> 00:04:28.040]   time to get that name, go right ahead. I'll be real fast. It's a lot though. You got to go to
[00:04:28.040 --> 00:04:32.920]   Facebook, Twitter, Instagram, you got to go to dot com. You got to do everything. It's a lot.
[00:04:32.920 --> 00:04:37.320]   Yeah. I'm hoping by then like Facebook maybe just won't be a thing anymore. I don't know. By the
[00:04:37.320 --> 00:04:41.160]   time she's using it. Actually, that's a great question. But before we answer that, let's talk
[00:04:41.160 --> 00:04:46.200]   to Nate Langston. He's also here at European and consumer tech editor from Bloomberg. He joins us
[00:04:46.200 --> 00:04:52.120]   from his it looks like recording studio in London. That is what it is. It is a homemade
[00:04:52.120 --> 00:04:58.840]   recording studio with a variety of types of sound treatment to sound as good as possible. But I
[00:04:58.840 --> 00:05:03.000]   do a lot of podcasting as well in here. So it's pretty well suited for you. But you're a musician,
[00:05:03.000 --> 00:05:07.320]   obviously, I could see a sequencer behind you, a drum kit, a keyboard. What do you do?
[00:05:07.320 --> 00:05:13.880]   Yep. Well, I do play the drums. That's sort of my main instrument. But this is also kind of my
[00:05:13.880 --> 00:05:19.720]   home office for when I'm away from Bloomberg. So it's podcasting, drumming, piano, all that kind of
[00:05:19.720 --> 00:05:22.920]   thing. It sounds better than mine, man. You got to knock it off. That's too good.
[00:05:22.920 --> 00:05:29.880]   It sounds really good. It's great to have you. Nate, I know if you watch Tech News Weekly or TNT,
[00:05:29.880 --> 00:05:33.960]   you've seen Nate on our shows for many, many years. And I think I think we think he was on
[00:05:33.960 --> 00:05:41.000]   Twitch back in get ready for this the days of the cottage. Yeah, I believe so. I think that
[00:05:41.000 --> 00:05:47.720]   might have been back in the Tom Merritt Eric's. I believe I drove with him to the cottage for that
[00:05:47.720 --> 00:05:53.720]   show. So I'm not 100% sure. But that was definitely the last time that it's not the only time that
[00:05:53.720 --> 00:05:57.000]   you and I have actually met in person. That was going back some years.
[00:05:57.000 --> 00:05:59.000]   It's about time we got you back. I apologize.
[00:05:59.000 --> 00:06:01.880]   Ah, thank you for having me. It's great to be here.
[00:06:01.880 --> 00:06:08.200]   So you raised a really interesting question, actually, Divindra. Sophia's six months old today.
[00:06:08.200 --> 00:06:12.280]   She's not going to be six weeks old. Sorry. Yeah, she's young.
[00:06:12.280 --> 00:06:18.600]   She'll be 13 years before she can use any social network and 20 years before she'll care,
[00:06:18.600 --> 00:06:24.280]   21 years before she won't care. The question is, is it, does it even make sense to register
[00:06:24.280 --> 00:06:30.200]   an Instagram Facebook or Twitter domain for her? I mean, just to be safe, I pretty much am. But
[00:06:30.200 --> 00:06:34.680]   yeah, I don't know what those services will be. I don't know, you know, in 10 years, what sort of
[00:06:34.680 --> 00:06:39.480]   interfaces will kids really be into? It'll be like back to the future. The kids complaining,
[00:06:39.480 --> 00:06:43.480]   "Oh, this game's not even holographic. What are you doing playing on like a 2D screen?"
[00:06:43.480 --> 00:06:46.920]   You never know because my 15, 16 year old now, he just had a birthday,
[00:06:46.920 --> 00:06:52.360]   loves 8-bit games. Maybe because of Minecraft, but all his games look like he bought,
[00:06:52.360 --> 00:06:59.800]   there's a nostalgia store in town. He bought a Super NES and he's playing Mario on it.
[00:06:59.800 --> 00:07:00.280]   That's great.
[00:07:00.280 --> 00:07:04.360]   8-bit, you know, I mean, he wasn't even born when the NES was out. So it's not the sound.
[00:07:04.360 --> 00:07:08.120]   That's a classic era of gaming too. So I can see why that sticks around.
[00:07:08.120 --> 00:07:11.320]   I don't know what is going to be our nostalgia for Facebook in 10 years.
[00:07:11.320 --> 00:07:12.120]   I don't know.
[00:07:12.120 --> 00:07:13.160]   No one will be this subject.
[00:07:13.160 --> 00:07:16.680]   Yeah. Remember those days when you wanted to beat your head against the wall because you kept
[00:07:16.680 --> 00:07:20.120]   seeing stuff that made you crazy? Yeah, those were the days.
[00:07:20.120 --> 00:07:26.440]   The other way I think about it a little bit is if you think back sort of 13, 14 years ago and,
[00:07:26.440 --> 00:07:30.920]   you know, it's at MySpace era or Bebo or Friendster or something,
[00:07:30.920 --> 00:07:35.720]   and imagine your parents come to you and say, "Okay, well, happy birthday. Here's the passwords,
[00:07:35.720 --> 00:07:41.480]   your friends, the recount." I mean, happy birthday indeed.
[00:07:41.480 --> 00:07:43.720]   What's this, Pat Pops?
[00:07:43.720 --> 00:07:48.840]   Honestly, if my parents had bought me my domain, like my first name is a domain
[00:07:48.840 --> 00:07:52.200]   in the 90s or early aughts, I would have been super happy. I would have loved that.
[00:07:52.200 --> 00:07:53.080]   Yeah.
[00:07:53.080 --> 00:07:57.480]   But that's different because they would have been really smart and
[00:07:57.480 --> 00:08:02.440]   oppressive. And so that's the thing. You've got to skate to where the puck is going.
[00:08:02.440 --> 00:08:05.800]   You don't do what's hit. What's going to be the thing in 13 years?
[00:08:05.800 --> 00:08:11.800]   Maybe Snapchat. I mean, a 21-year-old today is not using Facebook. That's where it's where
[00:08:11.800 --> 00:08:12.360]   we really are.
[00:08:12.360 --> 00:08:15.080]   Not really. Yeah. What'll be around then is we...
[00:08:15.080 --> 00:08:17.080]   We don't know. It doesn't exist. I respect.
[00:08:17.080 --> 00:08:23.160]   And I have to say, it really feels like at least Twitter and Facebook, maybe not so much
[00:08:23.160 --> 00:08:28.280]   Instagram, but at least Twitter and Facebook are kind of on the tail end of their ascendancy.
[00:08:28.280 --> 00:08:30.120]   Is that maybe is that just me?
[00:08:30.680 --> 00:08:35.160]   I feel like they've gotten big enough and we're starting to see the pieces start to crumble.
[00:08:35.160 --> 00:08:39.880]   They've gotten so big that's been the goal for all tech companies for the past decade.
[00:08:39.880 --> 00:08:45.400]   And now that they're so big, oh, they're influencing elections. They're like people are using their
[00:08:45.400 --> 00:08:50.520]   platforms in site riots and they have no idea how to control any of it and doing their best to
[00:08:50.520 --> 00:08:55.560]   avoid responsibility. So it'll probably still be around, but I'm really wondering what tech
[00:08:55.560 --> 00:08:59.640]   companies are going to do to deal with this problem. Once you get so big, you can have some
[00:08:59.640 --> 00:09:04.840]   serious impacts. That's another issue. Yeah. And I imagine there'll be governmental regulation.
[00:09:04.840 --> 00:09:12.920]   There'll be societal pressure. I'm surprised we haven't dealt with regulation yet because
[00:09:12.920 --> 00:09:16.840]   I'm thinking back, I remember the 90s, the big thing was like Microsoft, any trust,
[00:09:16.840 --> 00:09:23.960]   web browsers in Windows. Oh my God. I know who cared by the time. By that time that Microsoft
[00:09:24.760 --> 00:09:30.840]   DOJ and then in the EU, those cases came to fruition. The solutions were ridiculous.
[00:09:30.840 --> 00:09:37.240]   I know Nate, you probably had to face the weird browser ballot system when you first
[00:09:37.240 --> 00:09:42.920]   installed Windows for a long time. It said, choose your browser. It really did. And you know what?
[00:09:42.920 --> 00:09:48.520]   I would take that a thousand times over what we have on the internet now in Europe. You go to
[00:09:48.520 --> 00:09:53.800]   any website and you get a big banner at the top saying, do you accept cookies and you get a giant
[00:09:53.800 --> 00:09:59.880]   overlay that says, we protect your privacy, please click here to accept. And then that's on top of
[00:09:59.880 --> 00:10:05.400]   all the regular kind of full page ads that you get. I mean, browsing the web has never been
[00:10:05.400 --> 00:10:12.600]   less enjoyable these days. I'm not kidding. I'm all for the amplification. Just, I mean,
[00:10:12.600 --> 00:10:17.320]   it is a horrendous way to surf the web now. Well, that's a solution to a real problem, right?
[00:10:17.320 --> 00:10:22.120]   At least it's your law. We still get those cookies banners too, but that's not even for
[00:10:22.120 --> 00:10:25.560]   our jurisdiction. That's right.
[00:10:25.560 --> 00:10:31.320]   Every site now has, we use cookies. And the clear effect of it is the opposite of what
[00:10:31.320 --> 00:10:36.360]   regulators intended, which is that people go, yeah, yeah, yeah, yeah. I just get rid of that thing.
[00:10:36.360 --> 00:10:41.960]   Well, it is. It doesn't make you more aware. It makes you less aware.
[00:10:41.960 --> 00:10:46.120]   Well, and Nate, you were, you were common about amplification. I think that's,
[00:10:46.120 --> 00:10:50.840]   that's exactly what's happened is people get more and more frustrated than annoyed with
[00:10:50.840 --> 00:10:56.360]   the way the web works for them. Either they're seeking out paid sites that have,
[00:10:56.360 --> 00:11:01.800]   you know, less of that, or they're going, they're moving to apps. I think it's your Facebook's primary
[00:11:01.800 --> 00:11:09.080]   presence is on apps is on its app. And it's a lot less annoying that way than it is on the desktop,
[00:11:09.080 --> 00:11:11.080]   I think. Yeah, definitely.
[00:11:11.080 --> 00:11:17.160]   There's some evidence of this because, of course, we had the biggest shopping day of the year
[00:11:17.960 --> 00:11:27.160]   last week on Monday, Cyber Monday, it was Amazon's biggest day ever beating Prime Day.
[00:11:27.160 --> 00:11:33.800]   But the evidence is that a lot of the sales that we saw during Black Friday, Cyber Monday,
[00:11:33.800 --> 00:11:37.960]   Giving Tuesday, which apparently is mostly about giving me something,
[00:11:37.960 --> 00:11:43.400]   were about, not charities, which you're supposed to do, but giving ourselves things.
[00:11:43.400 --> 00:11:47.960]   We're online, a huge number of these were on the phone. Thanksgiving day, a billion dollars
[00:11:47.960 --> 00:11:57.240]   worth of commerce on the phone. I can only imagine what the holidays look like now.
[00:11:57.240 --> 00:11:59.480]   I don't have to imagine. I know.
[00:11:59.480 --> 00:12:04.360]   You eat the meal. I'm surprised the try not to talk about politics. You eat the meal. You try not
[00:12:04.360 --> 00:12:10.120]   to talk about politics. As soon as the meal's done, you immediately scatter to look at your phone,
[00:12:10.120 --> 00:12:14.200]   the football games on your Soperific, your Koma toast practically from all that food,
[00:12:14.200 --> 00:12:18.360]   and you buy stuff. That's the holidays. Welcome. That's right.
[00:12:18.360 --> 00:12:24.840]   The interesting thing is to headline still. Go ahead. Go ahead, Devindra and then Nate.
[00:12:24.840 --> 00:12:29.800]   Yeah, sorry. Amazon always sends out this press release every year. It's the same thing.
[00:12:29.800 --> 00:12:33.400]   Like the biggest day ever on the days you'd expect. So I'm just surprised we're still
[00:12:33.400 --> 00:12:36.600]   seeing this. Of course, of course, this is where the conversation is. Of course,
[00:12:36.600 --> 00:12:39.080]   this is what's happening now. It doesn't feel like news to me.
[00:12:39.880 --> 00:12:43.960]   Adobe somehow got in this business. I don't know how with Adobe analytics. They're the ones who
[00:12:43.960 --> 00:12:53.400]   now do all the big stats. They say Cyber Monday sales hit $7.9 billion. So it wasn't just Amazon.
[00:12:53.400 --> 00:12:58.520]   It was the largest online shopping day of all time in the US. Still dwarfed by singles day in
[00:12:58.520 --> 00:13:05.960]   China, but still a significant holiday, 20% growth year over year. That's what I thought was interesting.
[00:13:07.080 --> 00:13:10.840]   Thanksgiving day and Black Friday, 28% growth year over year.
[00:13:10.840 --> 00:13:18.280]   So really kind of a surprising Saturday and Sunday. We're also big new record as the biggest
[00:13:18.280 --> 00:13:24.920]   online shopping weekend in the US $6.4 billion. So you could point to all sorts of economic
[00:13:24.920 --> 00:13:29.720]   indicators, but it sounds to me like people in the United States anyway, feel like happy days are
[00:13:29.720 --> 00:13:35.240]   here again and they're spending like crazy. What is singles day in China? What is that?
[00:13:35.240 --> 00:13:41.720]   1111 1111 because it's a single get it. And I think did Alibaba create it?
[00:13:41.720 --> 00:13:46.200]   They're certainly the benefit of it. They looked at what Amazon did and were like,
[00:13:46.200 --> 00:13:50.840]   hey, yeah, this is great. Let's do our own big corporate holiday.
[00:13:50.840 --> 00:13:59.800]   Singles day sales in 2018. Let me see business insider has the numbers.
[00:14:00.440 --> 00:14:12.120]   It's half of all Alibaba's sales happening one day. Yeah, it is $30 billion. Here's the graph
[00:14:12.120 --> 00:14:17.960]   just to give you a sense of it. $30 billion for singles day, Cyber Monday, 7.9, Black Friday,
[00:14:17.960 --> 00:14:22.680]   6.2 Thanksgiving. I'm still blown away. Thanksgiving day sales, $3.7 billion,
[00:14:22.680 --> 00:14:29.480]   $1 billion of that online. And that's really a big shift. That's like a 30% shift from last year.
[00:14:30.280 --> 00:14:36.760]   By the way, Adobe got into that because of their acquisition of Omniture, which a lot of corporate
[00:14:36.760 --> 00:14:42.680]   customers use. We used to use it at Hearst. And so that's where they get all that data is from
[00:14:42.680 --> 00:14:48.600]   Omniture. Now in the UK, Nate, I mean, you don't do Thanksgiving, but is it still a big day on
[00:14:48.600 --> 00:14:55.160]   Friday to shop? It's become a huge thing. Yeah, Black Friday, it's absolutely everywhere.
[00:14:55.160 --> 00:15:00.040]   There isn't a store you can go to pretty much within the whole of Britain that isn't trying
[00:15:00.040 --> 00:15:04.120]   to push something to do with Black Friday to you. But it's not even just on the day, frankly,
[00:15:04.120 --> 00:15:09.720]   it's more like a Black week. And you actually get stuff for several days before the Friday
[00:15:09.720 --> 00:15:14.920]   and several days after the Friday, everyone just jumps right in. I think one of the interesting
[00:15:14.920 --> 00:15:19.720]   things about the UK, though, that always stands out to me as being notably different to the US,
[00:15:19.720 --> 00:15:26.520]   is that about twice as many, rather than the percentage of people who shop online in Britain is around
[00:15:26.520 --> 00:15:32.120]   20% of all retail sales is done online in the UK. In the US, it's about 10%. According to some
[00:15:32.120 --> 00:15:38.040]   recent figures I've seen. So more people are physically going to stores in the US than in Britain.
[00:15:38.040 --> 00:15:44.120]   And so I think our average spend online on something like Black Friday actually skews slightly higher
[00:15:44.120 --> 00:15:48.040]   just because of the number of people doing it online now. Yeah, there's actually a new
[00:15:49.080 --> 00:15:56.920]   acronym, which I had never heard before. I think it's called BOPET, which is buy online, pick up,
[00:15:56.920 --> 00:16:03.400]   maybe it's BOP, I don't know what it is, buy online, pick up in store, BOPIS, buy online,
[00:16:03.400 --> 00:16:09.720]   pick up in store. And that's, I think Walmart chiefly has pushed that, but these brick and mortar
[00:16:09.720 --> 00:16:16.760]   stores have actually come up with a clever way of keeping their business going. BOPIS was increased
[00:16:16.760 --> 00:16:20.520]   this year. Maybe let's not use the app.
[00:16:20.520 --> 00:16:29.800]   But I like it. BOPIS was increased this year, 50% over last year. So it is a hot new thing.
[00:16:29.800 --> 00:16:34.280]   It's great because it's great for the people who don't decide and guess early enough,
[00:16:34.280 --> 00:16:40.840]   because the shipping window, that's the downside, right? For if you're like, yeah, if you just wait
[00:16:40.840 --> 00:16:45.880]   to the last minute, being able to buy online, just run to store and grab stuff is super great.
[00:16:45.880 --> 00:16:49.640]   I use that right most of the year or two, Best Buy has been doing it for a while.
[00:16:49.640 --> 00:16:53.960]   It's great. You never have to like go searching through an aisle. You kind of just go to the main
[00:16:53.960 --> 00:16:57.800]   thing and walk out with the DVD or whatever you need. Another thing that's changed, it might be
[00:16:57.800 --> 00:17:02.600]   helping drive this is Amazon, you used to be able to go to Amazon and avoid sales tax.
[00:17:02.600 --> 00:17:06.760]   And now that you have to pay sales tax, no matter what jurisdiction you're in,
[00:17:06.760 --> 00:17:12.200]   there's no advantage price advantage Amazon over buying it Walmart online and then picking it up.
[00:17:12.200 --> 00:17:16.920]   And there's the advantage of no shipping cost. So that might save you money and the speed
[00:17:16.920 --> 00:17:23.000]   just going down and pick it up. I mean, yeah, as you say, a lot of people wait the last minute,
[00:17:23.000 --> 00:17:27.960]   although it wouldn't be last minute in November. Well, a lot of the things that Walmart does,
[00:17:27.960 --> 00:17:32.360]   they don't actually have in the store, you order it and then you still wait two or three days and
[00:17:32.360 --> 00:17:37.800]   then you go get it. Oh, that's interesting. Yes. Walmart in Houston has been experimenting
[00:17:37.800 --> 00:17:43.480]   with these like towers, they're like self service kiosks. They're like 10 feet tall.
[00:17:43.480 --> 00:17:48.440]   And you walk up to it and you put your credit card in and you put a code in. And it's essentially
[00:17:48.440 --> 00:17:55.240]   the thing, the monolith ejects your product. And so you don't even have to deal with a human in it.
[00:17:55.240 --> 00:18:01.400]   This is the 2001 sequel. We did not expect the monolith is just spitting stuff out.
[00:18:01.400 --> 00:18:06.920]   That's right. That's right. Yeah, it's sort of just in time, not exactly just in time sort of.
[00:18:07.800 --> 00:18:13.320]   I saw. So there's one more aspect of this too. I think Walmart is doing, you can drive up to the
[00:18:13.320 --> 00:18:19.480]   store and they'll bring bags of groceries to your car. Yeah. Oh, well, the most 21st century
[00:18:19.480 --> 00:18:26.280]   thing is down in Phoenix, where you order it online, a waymo comes to your house,
[00:18:26.280 --> 00:18:31.160]   you get in the waymo, it drives you to the store, you pick up the groceries, the waymo drives you back.
[00:18:31.160 --> 00:18:36.040]   That is pretty awesome. We were actually talking about something on my podcast,
[00:18:36.040 --> 00:18:41.800]   Iliad today, that we've got this new rail system that's supposed to be launching in like several
[00:18:41.800 --> 00:18:47.720]   years time called HS2 connects basically the whole of the UK on a massive high speed track.
[00:18:47.720 --> 00:18:54.280]   And the two of the companies, there's Hitachi and Bombardier, who manufacture the trains,
[00:18:54.280 --> 00:18:58.840]   they have sort of teamed up to propose that Amazon deliveries and deliveries from other
[00:18:58.840 --> 00:19:04.120]   companies should be able to be made on the trains. So you could be sitting on a train
[00:19:05.320 --> 00:19:11.640]   and order something that you want in an hour's time. And when you pass through a station and it'll
[00:19:11.640 --> 00:19:15.240]   be there for you to pick up, or you could order your breakfast and they'll be sitting in your seat
[00:19:15.240 --> 00:19:21.080]   when you leave. And it sounds crazy, but you think about what's going on with drone technology
[00:19:21.080 --> 00:19:25.960]   and with Amazon's like two hour delivery thing on groceries. I'm not sure if that's the same in
[00:19:25.960 --> 00:19:30.920]   the US, but they have that they're here. I could totally believe it. I could totally believe it.
[00:19:30.920 --> 00:19:37.240]   They did an experiment, I think, in CES a couple of years ago, where they had drones delivering stuff
[00:19:37.240 --> 00:19:43.400]   to an autonomous vehicle. And that was supposed to be the future. Is it you want something in the car
[00:19:43.400 --> 00:19:49.400]   and the drone will just drop it through the sunroof? Not even kidding.
[00:19:49.400 --> 00:19:54.680]   Well, okay, so that's the future. David, you're in New York in Brooklyn.
[00:19:56.840 --> 00:20:02.840]   It's not so much of a thing here, but we're kind of in the suburbs. Do people pre-order their Starbucks
[00:20:02.840 --> 00:20:09.000]   drink and then just jump and skip the line and just pick it up? Not really. I mean, I'm sure maybe
[00:20:09.000 --> 00:20:12.440]   in Manhattan a lot of people are doing that. There aren't as many Starbucks out in Brooklyn.
[00:20:12.440 --> 00:20:17.800]   That's not a thing I've seen used too often. I've heard people complain about it because they're
[00:20:17.800 --> 00:20:23.560]   in line and then people come in the store and say, "There's my blonde juniper latte."
[00:20:24.280 --> 00:20:28.040]   And I'll pick it up. That sounds pretty good, actually. I had one of those. That is the weirdest
[00:20:28.040 --> 00:20:35.880]   flavor. This is the new Starbucks flavor juniper latte with sage sugar salt.
[00:20:35.880 --> 00:20:42.440]   It's just a gin latte. It's a gin latte. But it doesn't taste like gin. Great. I love that.
[00:20:42.440 --> 00:20:45.640]   It tastes like it should be bullion. It tastes like beef.
[00:20:45.640 --> 00:20:51.640]   Usually at the Starbucks, there's a separate place, or at least the ones I've been in,
[00:20:51.640 --> 00:20:55.400]   so that you can pick it up. Where you walk up, right, you go and you pick it up and you've already
[00:20:55.400 --> 00:21:01.160]   paid for it. So you don't have to go through that process. But I can sort it with anger people.
[00:21:01.160 --> 00:21:06.920]   Top sellers. You cover consumer electronics. I know Nate from Bloomberg and Devindra,
[00:21:06.920 --> 00:21:13.720]   your Mr. Engadget. Dwight, I don't know what Techburger does, but something like this.
[00:21:13.720 --> 00:21:20.920]   I care. I care. You care. Top sellers, the Nintendo Switch number one on Thanksgiving Day
[00:21:20.920 --> 00:21:23.640]   and I mean, people want the Switch. That's going to be a biggie.
[00:21:23.640 --> 00:21:31.960]   Number two is a category. So on Thanksgiving Day number five was something called Hatchie Babies.
[00:21:31.960 --> 00:21:37.880]   Number two on Cyber Monday was Little Live Pets. I had to look both of these up.
[00:21:37.880 --> 00:21:44.360]   Devindra, get ready because you're facing this in a few years. This is a hot new category of
[00:21:44.360 --> 00:21:52.920]   toy plus app. And the Hatchie Babies come in an egg. You get an egg and then it cracks and hatches
[00:21:52.920 --> 00:22:00.360]   and an ugly is sin stuffed animal comes out that you then have to take care of. It's like a
[00:22:00.360 --> 00:22:05.080]   little Tamagotchi and there's an app involved. Same thing with Little Live Pets.
[00:22:05.080 --> 00:22:09.240]   That sounds pretty cool actually. Yeah. Well, good because so the
[00:22:09.240 --> 00:22:14.440]   young is going to want these in about two years. Red Dead Redemption number three.
[00:22:14.440 --> 00:22:20.360]   Hot game. Great game. Great game. Although I again, maybe I'm too imaginative, but I just
[00:22:20.360 --> 00:22:25.960]   see people Thanksgiving Day going, I would like to shoot Uncle Joe. Maybe I'll just get this game.
[00:22:25.960 --> 00:22:33.400]   LG TVs number four. Pretty much all the OLEDs. I'm sure all the OLEDs. All the OLEDs.
[00:22:33.400 --> 00:22:38.360]   All the old. That's at the up. They are gorgeous. They're so good. I altered carbon on Netflix.
[00:22:38.360 --> 00:22:43.480]   It's a terrible show. I think I will agree. But why does that look good in HDR?
[00:22:43.480 --> 00:22:48.120]   It is the word. It's so disappointing because I love the novel.
[00:22:48.120 --> 00:22:53.000]   There was so much promise in this and it looks like it's kind of a sci-fi
[00:22:53.000 --> 00:22:58.520]   played runner future. And there's some really cool ideas in it. But something went wrong.
[00:22:58.520 --> 00:23:05.720]   I don't know what. But anyway, watch it just for looking at it. I was watching episode five,
[00:23:05.720 --> 00:23:09.320]   I think. And there's in there in a tube that's lit by daylight, but there's dark stuff.
[00:23:09.320 --> 00:23:16.600]   It's clear that most of the energy on the show has spent on making it look good in 4K HDR.
[00:23:16.600 --> 00:23:20.760]   Oh, yeah. Most definitely. I think most of the Netflix shows do really well with HDR.
[00:23:20.760 --> 00:23:25.720]   Daredevil is my favorite demo. Just the first episode, Daredevil. There's that scene where they
[00:23:25.720 --> 00:23:29.880]   walk into his apartment and the neon lights are outside kind of coming in. It's just amazing.
[00:23:29.880 --> 00:23:34.120]   They're literally scripting this stuff so that there's bright dark in one scene.
[00:23:35.000 --> 00:23:39.640]   And they feel very, very specific guidelines as well about how you have to do the kind of cameras
[00:23:39.640 --> 00:23:45.560]   that you have to use if you're doing a show for Netflix. They're very, very strict on the
[00:23:45.560 --> 00:23:51.880]   technical setup of a production for a Netflix original like some of these and things like Jessica
[00:23:51.880 --> 00:23:58.680]   Jones. Do they require Reds? Reds, one of the cameras that you can use. Yeah, there's a list
[00:23:58.680 --> 00:24:04.200]   you can find online somewhere that shows you the types of cameras that have to be used.
[00:24:04.200 --> 00:24:08.920]   And I'm not sure if it goes as far as lenses, but it's certainly cameras and formats.
[00:24:08.920 --> 00:24:15.400]   Yeah. Interesting. Yeah, they're very aware. I mean, even stranger things, there was a significant
[00:24:15.400 --> 00:24:20.600]   amount of time spent in a dark, like black room with a one guy sitting in the light,
[00:24:20.600 --> 00:24:26.200]   clearly showing off 4K HDR. I think that's kind of interesting.
[00:24:27.320 --> 00:24:34.200]   So also after LG TVs, drones, which kind of surprises me, the drones would be on this list of
[00:24:34.200 --> 00:24:39.400]   top sellers on Cyber Monday, DJI, Airhogs and Skyviper.
[00:24:39.400 --> 00:24:44.040]   I thought that surprising. What are people doing with these?
[00:24:44.040 --> 00:24:46.040]   Yeah. What are they doing? I don't know.
[00:24:46.040 --> 00:24:49.880]   Once you see your house from 100 feet, you kind of done.
[00:24:49.880 --> 00:24:55.480]   Drone selfies. It's drone selfies. I think in pictures of your friends and your party outside.
[00:24:56.040 --> 00:25:01.080]   That picture where there's close up and it looks like a normal scene and then they zoom
[00:25:01.080 --> 00:25:08.200]   up through the clouds. I've seen a very effective use for drones, actually, which is on dairy farms
[00:25:08.200 --> 00:25:16.760]   because cows, bear with me on this one, get cows naturally see humans as predators because
[00:25:16.760 --> 00:25:19.480]   front-facing eyes. Because we're going to eat them eventually.
[00:25:20.200 --> 00:25:28.920]   Yeah. So it's very difficult to actually measure the well-being and the happiness or comfort,
[00:25:28.920 --> 00:25:33.560]   if you like, of cows. So you can actually send drones or use cameras with facial recognition
[00:25:33.560 --> 00:25:39.240]   technology specifically for cows to actually get a more accurate reading because you don't
[00:25:39.240 --> 00:25:41.640]   have to have a human being in front of them, basically scaring them.
[00:25:41.640 --> 00:25:48.360]   And they're not scared by that thing going, "Mmm." No, because over the noise of the farm
[00:25:48.360 --> 00:25:53.400]   machinery, they're kind of used to it in a day like an industrialized dairy production.
[00:25:53.400 --> 00:25:57.720]   I don't think though that dairy farmers are driving this huge cyber money sales, but maybe,
[00:25:57.720 --> 00:26:07.240]   anything's possible. Farmers are in drones. Dell laptops, not any other Dell laptops,
[00:26:07.240 --> 00:26:13.720]   which means I presume windows. Did Dell do aggressive discounting for cyber money? Maybe that's why.
[00:26:13.720 --> 00:26:17.560]   I didn't see the specific deals, but they tend to have a good amount and their lineup is just
[00:26:17.560 --> 00:26:22.920]   really good. They just launched the G series line of cheap gaming laptops, and I know people
[00:26:22.920 --> 00:26:27.960]   really into that. There you go. There you go. Another one kind of like catchy
[00:26:27.960 --> 00:26:35.480]   animals and live pets, for real pets and Amazon Echo devices. That's no surprise because Amazon
[00:26:35.480 --> 00:26:40.360]   did deep discount all their Echo devices. Google tried too, but I think Amazon really kind of
[00:26:40.360 --> 00:26:48.920]   owns this. Revenue from smartphones. One day, Cyber Monday, $2.1 billion. That's almost 50%
[00:26:48.920 --> 00:26:55.080]   growth year over year. They should stop calling it Cyber Monday and call it cell phone Monday.
[00:26:55.080 --> 00:27:00.280]   But you know, there's an aspect to this that I'm really curious to get some stats on because it's
[00:27:00.280 --> 00:27:09.640]   not really been fully explained. In the gaming world, the vast majority of games now on mobile
[00:27:09.640 --> 00:27:14.840]   are free to play and they're monetized within our purchases. It's become extremely easy for
[00:27:14.840 --> 00:27:20.520]   developers and publishers to do these flash sales on huge amounts of discounted in-game
[00:27:20.520 --> 00:27:25.800]   currencies for Black Friday. It doesn't really cost them anything to do. I would love to see
[00:27:25.800 --> 00:27:31.320]   some really accurate statistics that show just how effective something like Black Friday can be,
[00:27:31.320 --> 00:27:36.520]   specifically for in-app payments, because I bet that is a huge market and it's only going to be
[00:27:36.520 --> 00:27:39.560]   getting bigger. How interesting. You don't normally think of in-app payments as something
[00:27:39.560 --> 00:27:46.440]   you'd go shop for on any particular day. It's Cyber Monday. I'm going to buy me some Fortnite
[00:27:46.440 --> 00:27:55.080]   skins. Yeah, but it's more the currency. Yeah, exactly. I was watching the Today Show by accident
[00:27:55.080 --> 00:28:02.520]   yesterday morning or Friday morning, I guess it was. In the most blatant plug ever,
[00:28:02.520 --> 00:28:07.880]   they're showing all the Fortnite physical goods you can buy, including piñata, llama,
[00:28:07.880 --> 00:28:16.760]   piñatas. This is really going way too far. Physical loot boxes. You don't know what's in them.
[00:28:16.760 --> 00:28:27.400]   That's crazy. Game economy has really entered the real world. Mobile on Monday, 51% of site visits.
[00:28:27.400 --> 00:28:31.320]   So they're going to have to... See, I thought for a long time, they shouldn't call it Cyber Monday
[00:28:31.320 --> 00:28:35.880]   because the story of Cyber Monday was it was originally created when everybody
[00:28:35.880 --> 00:28:40.600]   waited to shop until Monday, when they came into work for the high-speed internet.
[00:28:40.600 --> 00:28:44.520]   But that's obviously gone. I don't know if that was ever true.
[00:28:44.520 --> 00:28:49.480]   There's the story. Somebody just made up that concept. That's how they named it.
[00:28:49.480 --> 00:28:57.240]   But now it is really a 51% of sales. It's the first time ever, 51% of sales from mobile,
[00:28:57.240 --> 00:29:04.840]   smartphones and tablets, 34% of revenue, more than a third from smartphones and tablets.
[00:29:05.640 --> 00:29:12.840]   That is... Online is... We don't need any evidence to prove this, but it's clear. Online is a big deal.
[00:29:12.840 --> 00:29:20.200]   The best time to shop? 10 p.m. to 1 a.m. Not the best time to shop the most popular time.
[00:29:20.200 --> 00:29:28.360]   There's 1.7 billion in online sales between 10 p.m. and 1 a.m. Eastern on Cyber Monday.
[00:29:28.360 --> 00:29:34.440]   After the kids went to bed. Yeah. That's 300 million dollars more than an average day during the year.
[00:29:35.240 --> 00:29:41.160]   So what's interesting about those mobile numbers is that the actual purchases were 50%.
[00:29:41.160 --> 00:29:48.920]   But the amount of goods were much less than that, which means the average selling price
[00:29:48.920 --> 00:29:52.280]   on a mobile phone may be less than when you're buying it on a desktop.
[00:29:52.280 --> 00:29:56.040]   That kind of makes sense. You wouldn't buy a big ticket item on a phone. You might wait.
[00:29:56.040 --> 00:30:01.720]   You're just buying loot boxes. Loot boxes. Denver, the biggest shopping
[00:30:02.520 --> 00:30:08.920]   baskets, followed by San Francisco, New York, Portland, Seattle. Nationwide average, up 6%
[00:30:08.920 --> 00:30:16.440]   customers got more comfortable buying more online and, to your point, to buy bigger ticket items
[00:30:16.440 --> 00:30:22.360]   online. All right, that's enough. We've gone on. Weigh into the weeds on this. I just think
[00:30:22.360 --> 00:30:27.720]   it's interesting. I mean, it's such a strange phenomenon. Is it because of the deep discounts
[00:30:27.720 --> 00:30:31.560]   that merchant software, that's why people do this? Must be in the UK because there's no
[00:30:31.560 --> 00:30:37.560]   Thanksgiving to market. Yeah, it is. 100% people live a bargain. We drink a lot as well, so people
[00:30:37.560 --> 00:30:44.120]   get drunk and then up 10. They start shopping for I- That's the tip to you. I wish Adobe Analytics
[00:30:44.120 --> 00:30:50.040]   would have the drunk shopping stat. That's what I want to see. And 48% were inebriated.
[00:30:50.040 --> 00:30:55.880]   Yeah, I can imagine it, definitely. Well, speaking of which, it's time to take a break.
[00:30:55.880 --> 00:31:03.240]   It's a brand new sponsor, Hopsy. Look at this. This is going to make your holiday party so much
[00:31:03.240 --> 00:31:09.080]   better. Sometimes they call it the Nespresso or the Kureg for beer. We had this on Thanksgiving.
[00:31:09.080 --> 00:31:17.240]   This is my Hopsy tap. You have this in the house. You put this on the counter. And then you get,
[00:31:17.240 --> 00:31:23.160]   you get beer from Hopsy that you actually can subscribe. It brings you beer on tap in the
[00:31:23.160 --> 00:31:27.960]   comfort of your own home. And they're having a side beer week sale.
[00:31:27.960 --> 00:31:35.160]   So now's the time to get your Hopsy. I'll show you how easy this is to use. So for the holidays,
[00:31:35.160 --> 00:31:41.080]   only you can get the the sub home draft system that I just brought out plus too many kegs of
[00:31:41.080 --> 00:31:47.320]   delicious beer and two Hopsy glasses, just $99 plus a free membership in the monthly beer club.
[00:31:47.320 --> 00:31:50.920]   Here's how the beer comes. I'm going to give you guys choice even though you don't get,
[00:31:50.920 --> 00:31:57.880]   I only get to drink it. I have two Hopsy's. They get them from all over the country. They partner
[00:31:57.880 --> 00:32:03.080]   breweries. This is actually brewed at the strike brewery in San Jose, Schulis, Joe Brown Ale,
[00:32:03.080 --> 00:32:09.640]   or from the Coronado Brewing Company in San Diego, Guava Islander IPA. What should you like?
[00:32:09.640 --> 00:32:11.880]   Too bad I'm putting the Schulis Joe in.
[00:32:11.880 --> 00:32:16.040]   All right. I think you want a brown ale, would you name it?
[00:32:16.840 --> 00:32:22.280]   I always want a brown ale. I think so. So in the cap, this is so cool, is the little
[00:32:22.280 --> 00:32:29.400]   doohickey, the mechanism. This is kind of like Al Capone's vault here. I don't know, we don't
[00:32:29.400 --> 00:32:36.520]   have a shot of this, but I'm going to open this up and the hatch opens. And you slide the Hopsy in
[00:32:36.520 --> 00:32:42.760]   here and then you have to put the connector through here. So I'm going to slide the beer in here.
[00:32:42.760 --> 00:32:50.040]   This is fresh. It's so good. And I'm going to close the top and then I'm going to close this
[00:32:50.040 --> 00:32:55.400]   seal this up. That's how easy it is to put another mini keg in there and turn it on.
[00:32:55.400 --> 00:32:59.160]   Now it's going to take a little bit to chill. It's going to chill it up a little bit,
[00:32:59.160 --> 00:33:02.760]   although I can't, you keep these in the fridge so that they're going to be cold to start with.
[00:33:02.760 --> 00:33:07.640]   You start by selecting your holiday package, best sellers, which are your favorite beers
[00:33:08.200 --> 00:33:13.320]   from the best craft breweries. I mean, these are really good breweries. You also can choose,
[00:33:13.320 --> 00:33:16.760]   I know a lot of you like IPAs and Pale Ales. If you like those hops,
[00:33:16.760 --> 00:33:24.280]   hazy West Coast session and Pale Ales in there, or the easy drinkers, some are on tap from
[00:33:24.280 --> 00:33:28.920]   loggers to cultures to Pilsner's. That's probably where I would go. But you see, you can choose
[00:33:28.920 --> 00:33:34.360]   beers curated to your tastes. Afterwards, you'll get four mini kegs of your choice as part of the
[00:33:34.360 --> 00:33:38.840]   monthly subscription plan. Each mini keg stays fresh for 30 days. You want this fresh. The whole
[00:33:38.840 --> 00:33:45.080]   point of this is fresh outside of the sub home draft system. And then once you put it in the sub
[00:33:45.080 --> 00:33:50.200]   and tap it, you've got two weeks to drink it. We really had a great time in Thanksgiving. This was
[00:33:50.200 --> 00:33:59.720]   a big hit to tap this. We had a nice IPA for Thanksgiving. Now is a perfect time to gift yourself
[00:33:59.720 --> 00:34:07.320]   or friend Hopsy just for the holidays. A great deal. Here's where you go. T-R-Y-H-O-P-S-Y.
[00:34:07.320 --> 00:34:15.560]   Try Hopsy dot com slash twit. And if you use a promo code twit, you'll get the sub home draft
[00:34:15.560 --> 00:34:22.280]   system, too many kegs of beer. That's basically two six packs of beer, two Hopsy glasses and a
[00:34:22.280 --> 00:34:27.880]   free membership in the monthly beer club, all for a low price of $99. Terms and conditions do
[00:34:27.880 --> 00:34:35.240]   apply, of course. T-R-Y-H-O-P-S-Y dot com slash twit, promo code twit, the special $99 holiday
[00:34:35.240 --> 00:34:39.720]   package. And as soon as this light goes green, I'm going to, I could probably, I'll do it now.
[00:34:39.720 --> 00:34:47.720]   What the heck? Tap this out a little bit. Oh, I feel so left out. This is so good. I'm getting a nice
[00:34:47.720 --> 00:34:55.960]   brown ale. Try Hopsy dot com slash twit. This is my kind of advertising. I want more like this.
[00:34:55.960 --> 00:35:03.400]   Well, we probably buried the lead. The big story of the week, and we all agreed before we started,
[00:35:03.400 --> 00:35:17.000]   the Marriott hack, 500 million guests revealed there and a lot of information revealed about them.
[00:35:17.000 --> 00:35:25.400]   This Marriott acquired the Starwood group SPG in 2016, I think. Apparently, what they didn't know is
[00:35:25.400 --> 00:35:31.800]   they'd also acquired a system that was hacked in 2014. And not only did they not know it then,
[00:35:31.800 --> 00:35:37.560]   although you, one would think due diligence would have revealed it, they didn't know for
[00:35:37.560 --> 00:35:43.640]   another two years. So for four years, hackers have been roaming the Starwood system
[00:35:45.640 --> 00:35:50.440]   and have had apparently full reign. Marriott says we learned about it last week.
[00:35:50.440 --> 00:35:58.920]   And they're getting a lot of heat over this. So let me tell you what has been revealed.
[00:35:58.920 --> 00:36:02.600]   Almost everybody has a Starwood account. That's or a Marriott account. And that's probably why
[00:36:02.600 --> 00:36:11.080]   it's half a billion people for a three. This is what Marriott said. For 327 million of you,
[00:36:11.800 --> 00:36:17.480]   the information includes quote, some combination of name, address, phone number, email, passport
[00:36:17.480 --> 00:36:23.000]   number, account information, date of birth, gender, and arrival and departure information.
[00:36:23.000 --> 00:36:30.760]   For another 137 million of you, it also may include encrypted payment card information,
[00:36:30.760 --> 00:36:35.880]   but they couldn't rule out the possibility the encryption keys had also been stolen.
[00:36:37.240 --> 00:36:42.600]   So they can't say for sure whether that information has leaked out.
[00:36:42.600 --> 00:36:48.840]   I have to thank GDPR because it's, without a doubt, there's no way Marriott would have told
[00:36:48.840 --> 00:36:53.160]   anybody about this for quite some time. They had to. It's 72 hours.
[00:36:53.160 --> 00:37:01.160]   Right. Yeah, they have 72, 72 hours. Yeah. And companies that are found now to be in breach of
[00:37:01.160 --> 00:37:07.000]   GDPR, they face fines. I think it's up to 4% of their global annual revenue.
[00:37:07.800 --> 00:37:13.080]   So, I mean, you look at a size of a company like that or any of the big tech companies. I mean,
[00:37:13.080 --> 00:37:16.840]   4% of your annual revenue. That's not a small figure. That's not worth the risk.
[00:37:16.840 --> 00:37:24.680]   So one of the interesting things about this story is that it actually was only the Starwood
[00:37:24.680 --> 00:37:31.080]   hotels and people who had stayed there who were affected. It was not people who had accounts with,
[00:37:31.080 --> 00:37:35.400]   if you only had accounts with Marriott or courtyard by Marriott or that brand.
[00:37:35.400 --> 00:37:41.720]   But a lot of the press in headlines and sometimes in the stories never made that distinction.
[00:37:41.720 --> 00:37:46.120]   It was, if you, you know, this was a Marriott hack as opposed to a Starwood hack.
[00:37:46.120 --> 00:37:50.840]   Was the Starwood reservation six difference? Yeah. But let me tell you what hotels,
[00:37:50.840 --> 00:37:57.000]   Sheraton, Ritz Carlton, Autograph, collect, oh, no, I'm sorry. That's, no, yeah, that's right.
[00:37:57.000 --> 00:38:05.320]   On a graph collection, W hotels, Weston hotels. It's a lot of hotels. Yes. Yes. And although
[00:38:06.120 --> 00:38:12.360]   it actually is dwarfed by Marriott and the Marriott group proper. So that's good news.
[00:38:12.360 --> 00:38:15.320]   If there is any good news in this, that's good news.
[00:38:15.320 --> 00:38:21.320]   But here's one thing and I haven't seen anything on that. So today, I didn't know if I had a Starwood
[00:38:21.320 --> 00:38:27.320]   account and I went in and discovered that I did have one, but I don't think I've stayed there since
[00:38:27.320 --> 00:38:34.120]   before 2014. I mean, I hadn't since then. But what's interesting is that there's also a notification
[00:38:34.120 --> 00:38:42.680]   there that Marriott in August had merged its reservation database with Starwood.
[00:38:42.680 --> 00:38:47.480]   So beginning in August, they were putting together. So if you had a Marriott and a Starwood account,
[00:38:47.480 --> 00:38:53.000]   they actually were merging them, which means it's possible. But I haven't seen anything about this,
[00:38:53.000 --> 00:38:58.120]   that the hackers who were in there until they discovered them, then began to get access to the
[00:38:58.120 --> 00:39:03.000]   Marriott accounts. So it is really interesting. Yeah. It's possible. But I haven't seen anything on
[00:39:03.000 --> 00:39:09.560]   that at all. I mean, we could go on and on about the lax security.
[00:39:09.560 --> 00:39:14.200]   Did they not have some sort of intrusion detection system? Was there not some way to
[00:39:14.200 --> 00:39:22.040]   know these guys were roaming this database for four years? Well, apparently they were taking the data
[00:39:22.040 --> 00:39:29.000]   and encrypting it before they shipped it off. And if I remember reading the story correctly,
[00:39:29.880 --> 00:39:34.760]   the intrusion system, right, the hackers did, the intrusion system was designed to look for
[00:39:34.760 --> 00:39:41.960]   data that was moving offline that was not encrypted. So they used encryption more on site.
[00:39:41.960 --> 00:39:46.760]   It's hard for me not to castigate these guys, but please.
[00:39:46.760 --> 00:39:50.600]   But that's how it happened. That's why they didn't catch it.
[00:39:50.600 --> 00:39:55.320]   It's best practices and most companies will do this to change encryption keys regularly.
[00:39:56.360 --> 00:40:00.920]   Apparently they didn't do that. I mean, you can go on and on. This is just
[00:40:00.920 --> 00:40:09.160]   worst possible practices. I'm glad that hackers were using encryption, but apparently Marriott.
[00:40:09.160 --> 00:40:16.520]   It wasn't. But there's been an interesting move to companies that are using,
[00:40:16.520 --> 00:40:23.000]   or other companies are contracting third party security companies that use
[00:40:24.120 --> 00:40:34.040]   AI to try and learn the fundamental workings of a network in order to predict and to very,
[00:40:34.040 --> 00:40:39.160]   very quickly detect anomalies that perhaps could have got ahead of some of this. And it's been
[00:40:39.160 --> 00:40:44.200]   shown a lot of times now to be a lot faster and more effective than some of the previous
[00:40:44.200 --> 00:40:48.200]   ways that people have tried to stop this. But I just don't get the sense that it's ever going to
[00:40:48.200 --> 00:40:51.880]   that it's ever really going to stop. Someone's always going to be ahead of the game.
[00:40:51.880 --> 00:40:58.760]   And I think the comment, Leo, that you made about the timing of the acquisition and did nobody
[00:40:58.760 --> 00:41:04.280]   did nobody know this happened. Even that's happened before. We saw that with Yahoo with the Verizon
[00:41:04.280 --> 00:41:11.640]   acquisition. And it knocked a huge amount of value off Yahoo's sale price to Verizon when it
[00:41:11.640 --> 00:41:16.600]   came to light that they had this this hack that affected like three billion. Was it two, three
[00:41:16.600 --> 00:41:21.480]   billion? Three billion who account before you go say, wow, this must be the worst breach of all
[00:41:21.480 --> 00:41:26.520]   time a half billion people. It's not it's the third. It's the third worst. But number one and two are
[00:41:26.520 --> 00:41:31.800]   Yahoo three billion, half a billion for Yahoo two different breaches. Yahoo as you point out,
[00:41:31.800 --> 00:41:39.560]   Nate actively hid this from Verizon. It's pretty clear. They actually knew about it and didn't tell
[00:41:39.560 --> 00:41:45.880]   Verizon. And we saw similar with Uber as well with Uber had a massive data breach that if you
[00:41:45.880 --> 00:41:51.880]   remember, it was essentially tried to be covered up. This was prior to Dara Kazushahi taking over
[00:41:51.880 --> 00:41:58.200]   the company and a lot of that got publicized because of that. But yeah, I mean, this stuff just
[00:41:58.200 --> 00:42:04.120]   keeps on happening. No, you never learn. They never learn. Well, maybe they'll learn Brian Cribbs.
[00:42:04.120 --> 00:42:08.360]   If you're running an enterprise that has customer data, you ought to learn there's some pretty
[00:42:08.360 --> 00:42:14.360]   severe penalties from GDPR. And I expect Congress will at some point, if they ever
[00:42:14.360 --> 00:42:19.480]   decide to do anything, will act on this number of members of Congress, US senators have said,
[00:42:19.480 --> 00:42:23.800]   there needs to be jail time and severe fines. We're going to we're not going to see the end of
[00:42:23.800 --> 00:42:31.000]   this until there is. Maybe that's just a grandstanding. I don't know, Brian Cribbs on security talks about
[00:42:31.000 --> 00:42:37.720]   what companies should learn from this. And number one is that cybersecurity has to be part of the
[00:42:37.720 --> 00:42:41.560]   culture. It can't be an afterthought. It's not a necessary evil. It literally has to be
[00:42:42.680 --> 00:42:50.280]   job one, part of the culture. But he also has lessons for individuals. And for most of our
[00:42:50.280 --> 00:42:55.880]   listeners who aren't running major enterprises, my suggestion, if you are, you might check with
[00:42:55.880 --> 00:43:00.680]   some good security people. But we have an actually we have a sponsor that's an intrusion detection
[00:43:00.680 --> 00:43:07.400]   system that's really awesome. It's those things to canaries. And they would have they would have
[00:43:07.400 --> 00:43:15.640]   discovered this immediately because you disguise them as a NAS or a Windows share or a file, you
[00:43:15.640 --> 00:43:22.200]   know, payroll or something just something attractive to to bad guys. And the minute they touch it,
[00:43:22.200 --> 00:43:27.720]   either try to open it, try to enter a password, do anything, it immediately phones home.
[00:43:27.720 --> 00:43:34.440]   That's the I mean, obviously, Mary, I had nothing like that on these systems. For individuals,
[00:43:34.440 --> 00:43:41.240]   Brian says, you got to accept two unfortunate and harsh realities. Reality number one,
[00:43:41.240 --> 00:43:47.960]   bad guys already have access to personal data points. You may believe should be secret, but aren't,
[00:43:47.960 --> 00:43:53.960]   including credit card information, social security number, mothers made named date of birth address,
[00:43:53.960 --> 00:44:00.440]   previous address phone number. And yes, even your credit file, thanks to Equifax reality number two,
[00:44:00.440 --> 00:44:05.560]   Annie, this is really important. Any date a point you share with a company will,
[00:44:05.560 --> 00:44:11.640]   in all likelihood, eventually be hacked, lost, leaked, stolen or sold, usually through no fault
[00:44:11.640 --> 00:44:15.880]   of your own. And somebody called the radio show said, well, if I use two factor authentication,
[00:44:15.880 --> 00:44:23.480]   is that better? No, it isn't because you're secure, but but Marriott is not. And he says,
[00:44:23.480 --> 00:44:28.120]   if you're an American, it means, at least for the time being, you're recourse to do anything about it.
[00:44:29.160 --> 00:44:34.040]   When your information is hacked, lost, stolen, leaked or sold is nil.
[00:44:34.040 --> 00:44:37.320]   Nothing you can do. I love it.
[00:44:37.320 --> 00:44:43.160]   Marriott, wait, you know what they're doing. The usual, they're offering a year's worth of service
[00:44:43.160 --> 00:44:50.200]   of cyber monitoring from a company owned by security firm, CRL that advertises the ability to
[00:44:50.200 --> 00:44:55.560]   scour cyber crime underground markets for your data. He says, should you take them up on it? Well,
[00:44:56.120 --> 00:44:59.080]   it can't hurt as long as you're not expecting it to prevent anything.
[00:44:59.080 --> 00:45:05.080]   Sony did the same thing when the Sony PlayStation network, I mean, that's not a
[00:45:05.080 --> 00:45:09.960]   consolation prize. That should be good. Yeah. Thanks very much.
[00:45:09.960 --> 00:45:11.400]   Well, no, about do something.
[00:45:11.400 --> 00:45:14.440]   And then what if I find it? What, you know, what are people supposed to do?
[00:45:14.440 --> 00:45:22.840]   I see ads all the time from, you know, experiencing, hey, credit monitoring will monitor the deep
[00:45:22.840 --> 00:45:26.440]   dark web looking for your email address. Yeah. And then what?
[00:45:26.440 --> 00:45:32.280]   When you inevitably find it, all you have to do is go to have I've been pwned.com and you'll
[00:45:32.280 --> 00:45:38.840]   immediately see that your stuff is out there. And the web watcher thing that Starwood offered
[00:45:38.840 --> 00:45:44.040]   up that Marriott offered up is essentially is just a corporate version of pwned to own.
[00:45:44.040 --> 00:45:49.960]   I went and checked both of them and essentially the same things that were in pwned to own were
[00:45:49.960 --> 00:45:52.920]   also in the web. Have I been pwned? Yeah. Right. Yeah.
[00:45:52.920 --> 00:46:01.160]   Troy, Troy, Troy Hunt's great site, which has all of the, he doesn't have the Marriott stuff yet.
[00:46:01.160 --> 00:46:05.880]   I don't know if he will, because it requires, you know, I don't know how he gets this information.
[00:46:05.880 --> 00:46:10.680]   Let's see. Have I been pwned? Oh, no. My email address has been pwned on 20 breach sites,
[00:46:10.680 --> 00:46:19.000]   found on two paste bins. So what am I supposed to do about it? Nothing. Adobe.
[00:46:19.000 --> 00:46:23.640]   And that's what Apollo. Yeah. I mean, by the way, the people who are really going to be bit by this
[00:46:23.640 --> 00:46:29.080]   is people who used the same password on the starward login that they did for everywhere
[00:46:29.080 --> 00:46:32.760]   and else because all of those sites are now reached. Right.
[00:46:32.760 --> 00:46:36.120]   This should be a good chance for the password managers to really sell what they're offering
[00:46:36.120 --> 00:46:40.200]   here because, yeah, you want a really crazy complex password across everything.
[00:46:40.200 --> 00:46:45.560]   Yeah, we were just talking about GDPR before and like the implementation of it and kind of
[00:46:45.560 --> 00:46:50.200]   how that's ruining the web experience. But we still need something like that in America.
[00:46:50.200 --> 00:46:55.080]   We need some sort of like greater regulation when it comes to cybersecurity and things like that,
[00:46:55.080 --> 00:47:00.360]   unfortunately. Yeah. Brian says, if you use the same reuse passwords, you're screwed.
[00:47:00.360 --> 00:47:07.960]   So stop. He says, you should probably freeze your credit files with a major credit bureaus.
[00:47:07.960 --> 00:47:12.520]   You should regularly look at your credit file, but not from free credit report.com,
[00:47:12.520 --> 00:47:19.880]   which is not free, but annual credit report.com. It says, plant your flag at various online
[00:47:19.880 --> 00:47:24.120]   services before fraudsters do it for you, such as the Social Security Administration,
[00:47:24.120 --> 00:47:29.160]   the Postal Service IRS, your mobile provider. I'm not sure what planning your flag means.
[00:47:29.160 --> 00:47:34.120]   Let's look at this, click that link. In other words, get yourself, get yourself an account
[00:47:34.120 --> 00:47:39.400]   at each one of those. Oh, email address. Yeah. That's what you should be doing for Sophia right
[00:47:39.400 --> 00:47:45.080]   now to Vendra. Yes. Yes. Yes. Yes. Yes. Yes. Plant her flag. In the next slide. Plant her flag.
[00:47:45.080 --> 00:47:55.560]   And just don't trust anything that comes via email. I mean, I'm now I am now saying over and
[00:47:55.560 --> 00:47:59.800]   over again to the radio show where I talk to normal people. If you're putting it online,
[00:47:59.800 --> 00:48:04.040]   you're putting it in public. If you're doing it online, you do it in a public. Just remember,
[00:48:04.040 --> 00:48:09.320]   period and the. And the thing is, these massive data leaks, you know, they always get the headline
[00:48:09.320 --> 00:48:13.560]   and it's and it's important that they do and hopefully get reported on them in the right way.
[00:48:13.560 --> 00:48:21.400]   But I mean, I did a, I had a story for Bloomberg two or three weeks ago, actually, about just the
[00:48:21.400 --> 00:48:28.120]   number of large websites, large and small websites, actually, that are just routinely leaking low
[00:48:28.120 --> 00:48:34.840]   level customer data as a result of just not protecting how their websites are sending data to
[00:48:35.480 --> 00:48:40.840]   to Google, you know, within things like site maps or letting pages that shouldn't be indexed be
[00:48:40.840 --> 00:48:47.240]   indexed. And it's amazing. You can there, there is personal detail leaking out of like something
[00:48:47.240 --> 00:48:53.480]   like 20% of the biggest e-commerce websites in the US. You know, I found golf clubs. I found
[00:48:53.480 --> 00:48:59.480]   charities. I found, you know, restaurants, all kinds of stuff. And some of them, they're only leaking
[00:49:00.360 --> 00:49:06.680]   what looks like very innocent data, but it might be full names, address, donations made to certain
[00:49:06.680 --> 00:49:11.960]   charities, your state, you know, email address, all that kind of thing. But it's enough for somebody
[00:49:11.960 --> 00:49:18.920]   who has that data to be able to, you know, call up that person who may be vulnerable and put on a
[00:49:18.920 --> 00:49:23.960]   pretty good impression that they are legitimately from this institution that you paid money to.
[00:49:23.960 --> 00:49:29.480]   And it's so common, it is so common. And it's just, and it just never seems to go away. No matter
[00:49:29.480 --> 00:49:34.760]   how many of these breaches we see. I, every time I have a credit card stolen,
[00:49:34.760 --> 00:49:42.440]   one of the first things they do to test the credit card number is they donate a dollar to a charity.
[00:49:42.440 --> 00:49:49.320]   And, you know, some small, often it's a UK charity. I'm not sure why. But the reason is the
[00:49:49.320 --> 00:49:56.120]   charities have such lax security that they can almost always put the charge through and without
[00:49:56.120 --> 00:49:59.640]   raising any alarms. And that way they know they have a good credit card number.
[00:49:59.640 --> 00:50:05.160]   So yeah, I mean, and you know what charities don't have a lot of money, I can understand that.
[00:50:05.160 --> 00:50:10.440]   But a company like Marriott really should have better security. Mark Warren.
[00:50:10.440 --> 00:50:13.880]   What's that saying that everything has been hacked? We just don't know it yet?
[00:50:13.880 --> 00:50:15.240]   Yeah, or assume that.
[00:50:15.240 --> 00:50:23.720]   So I do recommend that you visit Brian's website, because I think it's a very good
[00:50:23.720 --> 00:50:27.240]   article on what it all means. And at the end, he quotes Senator Mark Warner,
[00:50:27.240 --> 00:50:31.480]   who says, "It seems like every other day we learn about a new mega breach affecting the
[00:50:31.480 --> 00:50:35.640]   personal data of millions of Americans, rather than accepting this trend as the new normal,
[00:50:35.640 --> 00:50:42.040]   this latest incident should strengthen Congress's resolve. We must pass laws that require data
[00:50:42.040 --> 00:50:47.320]   minimization, ensuring companies do not, that do not keep sensitive data they no longer need.
[00:50:47.320 --> 00:50:52.040]   And it's past time we enact data security laws and ensure companies account for security costs,
[00:50:52.040 --> 00:50:57.400]   rather than making their consumers shoulder the burden and harms resulting from these lapses.
[00:50:57.400 --> 00:51:00.520]   I hope he does this. That sounds like JDPR, frankly.
[00:51:00.520 --> 00:51:02.040]   Pretty much.
[00:51:02.040 --> 00:51:06.520]   I think that's where we're headed. I think Google, where's that Facebook knows that? Tim Cook at Apple
[00:51:06.520 --> 00:51:12.840]   said we needed. We were having similar discussions when like WannaCry hit and like was attacking
[00:51:12.840 --> 00:51:17.560]   everything years ago. Yeah, years ago, but it feels so soon. It's like really the only solutions.
[00:51:18.200 --> 00:51:22.680]   People, IT departments need to be better and they need to have incentives to be better about what
[00:51:22.680 --> 00:51:28.680]   they're doing. And really, the biggest stick we have to make companies do this is regulations.
[00:51:28.680 --> 00:51:33.720]   That's why a Marriott had to admit would happen. Otherwise, they would have tried to keep this
[00:51:33.720 --> 00:51:37.800]   quiet for a while. By the way, not because of US law, because of European law.
[00:51:37.800 --> 00:51:46.520]   So with a democratic controlled house, does that mean that we will be less apt or more apt to get
[00:51:46.520 --> 00:51:50.040]   a law like this passed? That's a great question.
[00:51:50.040 --> 00:51:58.280]   We got the best Congress money can buy. And I don't care what side of the aisle it is,
[00:51:58.280 --> 00:52:05.640]   Republican or Democrat. But these guys are bought and paid for by these big corporations.
[00:52:05.640 --> 00:52:12.840]   What's interesting about this new house that's coming in is a lot of them are very young.
[00:52:12.840 --> 00:52:18.760]   A lot of them may be more tech savvy and may be more interested in visiting this.
[00:52:18.760 --> 00:52:24.360]   I think a lot of, particularly on the Senate side, I think it befuddles many of them.
[00:52:24.360 --> 00:52:30.600]   Well, that's true. And I think that some members of Congress like Alexandria Ocasio-Cortez
[00:52:30.600 --> 00:52:38.040]   are not want and paid for. She's very progressive and ran a surprisingly strong campaign in New York.
[00:52:38.680 --> 00:52:45.800]   And I think she's going to be a firebrand. But the problem is you need all 51 votes in the Senate.
[00:52:45.800 --> 00:52:52.600]   You need all 200, what is it, 70 votes in the house. And there are so many members of Congress
[00:52:52.600 --> 00:52:59.080]   that are compromised by big money contributions from companies like Marriott that I just think
[00:52:59.080 --> 00:53:04.520]   it's very hard to expect Congress to pass a law that makes it harder to do business in this country.
[00:53:04.520 --> 00:53:12.760]   Yeah. Sad, but true. Sad, but true. I should give you a pause. Before we take a break,
[00:53:12.760 --> 00:53:17.080]   let's leave you with a positive note. The New York Times great article this week.
[00:53:17.080 --> 00:53:24.680]   We've all seen the pop-up alerts and windows that say this is from support.microsoft.com.
[00:53:24.680 --> 00:53:33.320]   Microsoft warning alert malicious pornographic spyware riskware detected. Please call us
[00:53:33.320 --> 00:53:38.440]   immediately. Do not ignore this critical alert. If you close this page, your computer access will
[00:53:38.440 --> 00:53:44.440]   be disabled to prevent further damage to your network. Your computer has alerted us, has been
[00:53:44.440 --> 00:53:51.960]   infected with with a pornographic spyware and riskware. Whatever a riskware is, the following
[00:53:51.960 --> 00:53:56.120]   information is being stolen. Financial data, Facebook logins, credit card details, email account
[00:53:56.120 --> 00:54:01.160]   logins, photos stored in this computer contact us immediately. So our expert engineers can walk
[00:54:01.160 --> 00:54:05.160]   you through the removal process over the phone to protect your identity. That'll be $300.
[00:54:05.160 --> 00:54:10.680]   These pop-ups and pop-ups just like them match phone calls we've been getting for years from,
[00:54:10.680 --> 00:54:16.600]   I got, I got one from Windows. I'm driving in my car. Your computer is infected. Well,
[00:54:16.600 --> 00:54:22.520]   Microsoft has worked with Indian police in New Delhi and they've traced a lot of these to
[00:54:22.520 --> 00:54:26.680]   boiler rooms in New Delhi, which is apparently, I thought Bangalore was India's capital for
[00:54:27.400 --> 00:54:32.920]   call centers, but I guess it's New Delhi. A lot of these guys doing these scams and a lot of the
[00:54:32.920 --> 00:54:39.160]   people the text you'll get when you call these numbers are actually by day real tech support call
[00:54:39.160 --> 00:54:46.680]   centers that moonlight at night to hack you. They got all the equipment right there. We're
[00:54:46.680 --> 00:54:51.880]   right there. We got the camera, which is go right ahead. On Tuesday and Wednesday, police from two
[00:54:51.880 --> 00:54:58.360]   Delhi suburbs rated 16 fake tech support centers arrested about three dozen people. Last month,
[00:54:58.360 --> 00:55:05.400]   24 people were arrested in similar rates and 10 call centers. These are scammers who have
[00:55:05.400 --> 00:55:11.400]   extracted a lot of money from mostly, I would say, elderly victims, American and Canadian
[00:55:11.400 --> 00:55:17.320]   with the pop-ups. The victims would then be offered a package of services ranging from $99
[00:55:17.320 --> 00:55:24.360]   to $1,000 to fix the problem. Microsoft did a survey and according to Microsoft,
[00:55:24.360 --> 00:55:30.840]   when these pop-ups show up or people call about 5% of people who see him respond,
[00:55:30.840 --> 00:55:39.560]   5% and once they respond, about 6% actually hand over money and allow these guys remote access to
[00:55:39.560 --> 00:55:51.400]   their computers. The advice from a researcher who study this, Najmeh Mira Mir Connie,
[00:55:51.400 --> 00:55:57.880]   who did a research paper from Stony Brook University, she estimated that a single pop-up
[00:55:57.880 --> 00:56:05.320]   campaign spread over 142 web domains, brought in $10 million in just two months. She said,
[00:56:05.320 --> 00:56:14.440]   the best thing, just don't pick up the phone. Don't stand away from the phone.
[00:56:14.440 --> 00:56:22.360]   By the way, you get ready because the next thing that happens in the next three months is IRS calls,
[00:56:22.360 --> 00:56:28.520]   also fake. I guess we should point out that neither the IRS nor Microsoft will call you or put pop-ups
[00:56:28.520 --> 00:56:35.640]   in your screen that they are scammers. The sad thing is that it often is elderly people,
[00:56:35.640 --> 00:56:39.320]   people who are less sophisticated technologically. I hear from people all the time,
[00:56:39.320 --> 00:56:44.280]   we got an email yesterday from a guy who's aunt just keeps answering these. It's not once,
[00:56:44.280 --> 00:56:49.320]   but just keeps doing it. He said, "What should I do?" I said, "Get a MacGutter Chromebook."
[00:56:49.320 --> 00:56:54.600]   It's great that smartphones are getting better about this too, right? Because with iOS 12,
[00:56:54.600 --> 00:56:59.320]   I'm getting all those alerts. This is a scam call. I know Android is doing really well too.
[00:56:59.320 --> 00:57:05.160]   I like the auto responses that we've been seeing. The tech is getting better to take care of some
[00:57:05.160 --> 00:57:10.680]   of this. On desktops, we're just open. Have you used the call screen thing on Pi where
[00:57:10.680 --> 00:57:17.880]   a call comes in and you can press a button, and then it will say, "This person is not taking calls
[00:57:17.880 --> 00:57:21.800]   until you identify yourself. You have to identify yourself and then you get a chance to screen."
[00:57:21.800 --> 00:57:27.080]   I haven't used it either. I got one of those calls in while Sophia was screaming in my arm.
[00:57:27.080 --> 00:57:30.520]   I was tempted to just answer the call and put the phone in front of the baby.
[00:57:30.520 --> 00:57:38.600]   That's very much like Google Voice. Google Voice. That's what I'm talking about.
[00:57:38.600 --> 00:57:43.960]   I think they got it from Google Voice. It's now an all Android devices.
[00:57:43.960 --> 00:57:50.840]   A lot of carriers also get intercepted now at the carrier level. Texts and phone calls.
[00:57:50.840 --> 00:57:56.760]   I am a T-mobile customer and I will see coming from them. It's definitely from them,
[00:57:56.760 --> 00:58:01.880]   scam call or spam call. The carriers are helping too.
[00:58:01.880 --> 00:58:07.320]   Does beg the question. If it's that easy to identify them, why do they even let them through?
[00:58:07.320 --> 00:58:12.120]   I guess false positives would be a problem. You don't want to block somebody's mom.
[00:58:13.000 --> 00:58:21.000]   I was AT&T previously. AT&T has an app where they do block them. They don't let them come through
[00:58:21.000 --> 00:58:26.840]   it all. You can go through the app periodically. It has a log. If there's something there that
[00:58:26.840 --> 00:58:32.120]   you should let through, you can actually whitelist it from there. I don't think T-mobile has the same
[00:58:32.120 --> 00:58:37.560]   thing, but there was very helpful on AT&T. There's not that much harm if you send it to email,
[00:58:37.560 --> 00:58:40.920]   or voice mail, because you'll at least get the message. I don't answer any.
[00:58:40.920 --> 00:58:45.800]   I honestly, it's so plagued by spam calls or robo calls, all that crap. I just don't even answer
[00:58:45.800 --> 00:58:49.720]   the phone unless I know that number. I don't answer the phone. I figure if it's important to
[00:58:49.720 --> 00:58:55.720]   leave a voice mail, and I can respond right away if I'll see the voice mail. If you call me,
[00:58:55.720 --> 00:59:01.800]   that's why I don't pick up mom. What's your number, mom? I never saw that one before.
[00:59:01.800 --> 00:59:07.560]   Microsoft says they get 11,000 complaints about these scams every month.
[00:59:08.760 --> 00:59:14.120]   Its internet monitors spot about 150,000 pop-up ads for the services every day.
[00:59:14.120 --> 00:59:19.640]   This is who everybody knows this goes on. There have been some Americans doing it too.
[00:59:19.640 --> 00:59:25.560]   Apparently, the authorities arrested some folks in Florida and Ohio for doing this,
[00:59:25.560 --> 00:59:29.000]   but the backbone, according to the New York Times, of the illicit industry is in India,
[00:59:29.000 --> 00:59:37.960]   because they got the call centers. We got trained personnel ready and willing to
[00:59:37.960 --> 00:59:44.440]   hack your system. I have to remember this plant your flag. That's a good term.
[00:59:44.440 --> 00:59:50.600]   Plant your flag. I sign up for every account you can now so somebody else won't.
[00:59:50.600 --> 00:59:57.240]   Well, if you're going to take a break, Devinda Harderwara is here. He's senior editor at Engadget,
[00:59:57.240 --> 01:00:02.680]   brand new Papa. Is Sophia in a soundproof booth right now?
[01:00:02.680 --> 01:00:06.120]   She is in the bedroom, which is a couple of rooms away with my wife.
[01:00:06.120 --> 01:00:11.160]   But, yeah, I wish we had a nice little soundproof booth. I feel bad for my neighbors,
[01:00:11.160 --> 01:00:13.080]   because in the middle of the night, she's just going.
[01:00:13.080 --> 01:00:19.320]   Oh, that's part of the deal. That is part of the deal. There's nothing you can do about crying
[01:00:19.320 --> 01:00:24.280]   babies. People sometimes get upset on the airplane when a baby is crying. I come over,
[01:00:24.280 --> 01:00:29.640]   tell the parents, "Okay, I've been there. You want me to hold the baby?" It's just, you know,
[01:00:29.640 --> 01:00:32.440]   give out your plugs now, so it should be less of a problem.
[01:00:34.680 --> 01:00:40.600]   That's the pass out of your plugs to your neighbors. I've actually seen people do that when they get
[01:00:40.600 --> 01:00:46.040]   on a plane with a baby. They'll have a little package to hand to the neighbors. You do that,
[01:00:46.040 --> 01:00:51.080]   Carsten? That's really cute with like snacks and earplugs and say, "Hey, I'm sorry. I know it.
[01:00:51.080 --> 01:00:55.720]   The baby might cry. Thank you for understanding." You do that, Carsten? I'm impressed.
[01:00:55.720 --> 01:00:58.520]   I did that. Your kid's 12 years old now, you still?
[01:00:58.520 --> 01:01:04.520]   No, I still do that. No. I did that. I had to go back east for my father-in-law's
[01:01:04.520 --> 01:01:09.960]   funeral when my son, my second son, was two months old. So I can have that.
[01:01:09.960 --> 01:01:15.240]   I think it's a nice, I like that idea. I think that's really good. Also from the tech burger,
[01:01:15.240 --> 01:01:20.680]   Houston Chronicles, tech page is the fabulous Dwight Silverman. He's back.
[01:01:20.680 --> 01:01:21.720]   Fabulous.
[01:01:21.720 --> 01:01:26.600]   Fabulous. Does don't make him sing. He can act. Just don't make him sing, apparently.
[01:01:26.600 --> 01:01:28.520]   Jim, just don't call me a fabulous either.
[01:01:28.520 --> 01:01:35.160]   Not a fabulous list. No, that would be wrong. We're very happy to have Nate with us. Nate Langson,
[01:01:35.160 --> 01:01:39.800]   he is tech editor at Bloomberg. He does consumer reporting for Bloomberg as well,
[01:01:39.800 --> 01:01:43.960]   consumer electronics reporting and is apparently a fabulous drummer.
[01:01:43.960 --> 01:01:46.840]   I wonder who told you that.
[01:01:46.840 --> 01:01:48.200]   What kind of drumming do you do?
[01:01:48.200 --> 01:01:56.600]   I do what's often referred to as technical metal or quite jazzy stuff, quite fast,
[01:01:57.960 --> 01:01:59.160]   quite technical.
[01:01:59.160 --> 01:01:59.720]   Technical metal.
[01:01:59.720 --> 01:02:02.120]   That's serious.
[01:02:02.120 --> 01:02:03.160]   Is it?
[01:02:03.160 --> 01:02:03.560]   Yeah.
[01:02:03.560 --> 01:02:04.760]   Because you have to be right on the beat.
[01:02:04.760 --> 01:02:06.600]   Super fast, super serious.
[01:02:06.600 --> 01:02:13.240]   Yeah. And stuff like any progressive rock metal fans out there will have heard of Dream Theater.
[01:02:13.240 --> 01:02:14.520]   Oh, yeah. I love Dream Theater.
[01:02:14.520 --> 01:02:17.320]   I play a lot of that sort of thing.
[01:02:17.320 --> 01:02:19.560]   Oh, that's great. Dream Theater is awesome.
[01:02:19.560 --> 01:02:21.560]   Yep, big fan.
[01:02:21.560 --> 01:02:23.160]   Big fan.
[01:02:23.160 --> 01:02:26.120]   Thank you for being here, Nate. I'm sorry, it's been so long. We will have you
[01:02:26.760 --> 01:02:29.720]   much more regularly from now on now that I know you're a drummer.
[01:02:29.720 --> 01:02:30.520]   Drummers are always.
[01:02:30.520 --> 01:02:32.840]   I look forward to it.
[01:02:32.840 --> 01:02:33.240]   Our show.
[01:02:33.240 --> 01:02:34.280]   You can give us a demo.
[01:02:34.280 --> 01:02:35.400]   Yeah.
[01:02:35.400 --> 01:02:38.200]   Going over to the kid. Hit the skins, man.
[01:02:38.200 --> 01:02:42.360]   Maybe a little later. It is quite late here in Britain.
[01:02:42.360 --> 01:02:43.720]   You know, I saw, oh, yeah, that's true.
[01:02:43.720 --> 01:02:46.280]   I saw we were watching a show
[01:02:46.280 --> 01:02:50.600]   that only had a drum soundtrack.
[01:02:50.600 --> 01:02:51.160]   What was it?
[01:02:51.160 --> 01:02:54.200]   It was, oh, it was get shorty.
[01:02:54.200 --> 01:02:54.600]   You're right.
[01:02:54.600 --> 01:02:55.240]   Get shorty.
[01:02:55.240 --> 01:02:55.560]   Yeah.
[01:02:55.560 --> 01:02:56.040]   Yep.
[01:02:56.040 --> 01:02:56.760]   Great show.
[01:02:56.760 --> 01:02:57.480]   Great show.
[01:02:57.480 --> 01:03:00.120]   It's a new, is it Netflix original?
[01:03:00.120 --> 01:03:01.160]   It's a, or...
[01:03:01.160 --> 01:03:01.720]   It's on...
[01:03:01.720 --> 01:03:03.480]   Epic.
[01:03:03.480 --> 01:03:04.040]   Epic.
[01:03:04.040 --> 01:03:04.680]   Yeah.
[01:03:04.680 --> 01:03:06.680]   Nobody watches that channel, but it's pretty fun.
[01:03:06.680 --> 01:03:08.600]   Yes, but it's moved over to Netflix.
[01:03:08.600 --> 01:03:09.400]   They went the first piece.
[01:03:09.400 --> 01:03:10.360]   That's how we saw it.
[01:03:10.360 --> 01:03:10.920]   Yeah.
[01:03:10.920 --> 01:03:11.560]   It's great.
[01:03:11.560 --> 01:03:12.120]   It's great.
[01:03:12.120 --> 01:03:12.680]   I love it.
[01:03:12.680 --> 01:03:14.520]   It's nothing like the novel or the movie.
[01:03:14.520 --> 01:03:16.920]   Kind of a similar premise.
[01:03:16.920 --> 01:03:23.000]   But kind of like, what was that movie that was just all the,
[01:03:23.000 --> 01:03:24.840]   was it Birdman?
[01:03:24.840 --> 01:03:26.360]   There was a, but it's just...
[01:03:26.360 --> 01:03:27.320]   Birdman had a lot of that.
[01:03:27.320 --> 01:03:27.560]   Yeah.
[01:03:27.560 --> 01:03:28.200]   It was just drumming.
[01:03:28.200 --> 01:03:28.760]   Yeah.
[01:03:28.760 --> 01:03:30.360]   But I thought it worked quite well.
[01:03:30.360 --> 01:03:32.040]   I didn't notice it for the first six episodes.
[01:03:32.040 --> 01:03:34.200]   Then I realized, oh, they're too cheap to get a score.
[01:03:34.200 --> 01:03:36.760]   But it's a good movie.
[01:03:36.760 --> 01:03:37.480]   I like it.
[01:03:37.480 --> 01:03:37.960]   Good show.
[01:03:37.960 --> 01:03:41.480]   Our show today brought to you by WordPress,
[01:03:41.480 --> 01:03:43.960]   the place to make your,
[01:03:43.960 --> 01:03:45.240]   now you want to plant your flag?
[01:03:45.240 --> 01:03:47.880]   The single most important way to plant your flag
[01:03:47.880 --> 01:03:48.760]   is to have a website.
[01:03:48.760 --> 01:03:51.320]   That's your home on the net,
[01:03:51.320 --> 01:03:54.600]   whether you're an individual or a business.
[01:03:54.600 --> 01:03:55.640]   Plant your flag.
[01:03:55.640 --> 01:03:56.360]   I'm going to use that.
[01:03:56.360 --> 01:03:58.360]   That's perfect with WordPress.
[01:03:58.360 --> 01:03:59.960]   WordPress.com makes it easy.
[01:03:59.960 --> 01:04:02.120]   They take care of the hosting, the security,
[01:04:02.120 --> 01:04:03.160]   the software updates.
[01:04:03.160 --> 01:04:06.360]   All you have to do is create some content.
[01:04:06.360 --> 01:04:08.360]   And whether you're a young person,
[01:04:08.360 --> 01:04:10.360]   you know, if you're a young person,
[01:04:10.360 --> 01:04:12.120]   I tell teenagers this all the time,
[01:04:12.120 --> 01:04:14.600]   you want to make sure that when people search for you,
[01:04:14.600 --> 01:04:17.320]   they don't find that stuff that your buddy,
[01:04:17.320 --> 01:04:19.320]   your so-called buddy's posting about you,
[01:04:19.320 --> 01:04:21.640]   they find your page with your best stuff on there.
[01:04:22.680 --> 01:04:23.560]   And yeah, you might say,
[01:04:23.560 --> 01:04:25.160]   "Well, I have a Facebook or a Twitter."
[01:04:25.160 --> 01:04:26.200]   No, that doesn't count.
[01:04:26.200 --> 01:04:27.480]   That's not your site.
[01:04:27.480 --> 01:04:28.920]   That's Mark Zuckerberg's site.
[01:04:28.920 --> 01:04:30.200]   That's Jack.
[01:04:30.200 --> 01:04:31.240]   What's his name, site?
[01:04:31.240 --> 01:04:34.760]   You need your own site at WordPress.com.
[01:04:34.760 --> 01:04:36.440]   And when plants start at just $4 a month,
[01:04:36.440 --> 01:04:37.640]   there's no reason not to.
[01:04:37.640 --> 01:04:41.480]   I've recently kind of pulled out of all the social media stuff.
[01:04:41.480 --> 01:04:43.560]   And I post my pictures.
[01:04:43.560 --> 01:04:44.840]   When I have an urge to Instagram,
[01:04:44.840 --> 01:04:46.520]   I posted at leooport.com.
[01:04:46.520 --> 01:04:47.320]   When I have urge to write,
[01:04:47.320 --> 01:04:48.520]   I posted at leooport.com.
[01:04:48.520 --> 01:04:50.600]   That is now my place on the net.
[01:04:50.600 --> 01:04:51.400]   And I own it.
[01:04:52.600 --> 01:04:55.160]   You can upload images, video, audio, and more.
[01:04:55.160 --> 01:04:56.280]   You don't have to use YouTube.
[01:04:56.280 --> 01:04:57.960]   You just put your video right there.
[01:04:57.960 --> 01:05:00.760]   You can just create a place to put a podcast.
[01:05:00.760 --> 01:05:03.720]   You can import and export content
[01:05:03.720 --> 01:05:05.720]   to and from your WordPress website.
[01:05:05.720 --> 01:05:06.600]   Because it's yours.
[01:05:06.600 --> 01:05:08.760]   It's your site, your home, your content.
[01:05:08.760 --> 01:05:11.720]   They have built-in SEO, search engine optimization.
[01:05:11.720 --> 01:05:13.400]   If you're a business,
[01:05:13.400 --> 01:05:15.560]   that helps you show up better on Google,
[01:05:15.560 --> 01:05:18.360]   social media, linking so your customers,
[01:05:18.360 --> 01:05:19.480]   your fans, your readers,
[01:05:19.480 --> 01:05:23.560]   your post links to your stuff on their Twitter
[01:05:23.560 --> 01:05:27.160]   and their Facebook help you promote great marketing tools too.
[01:05:27.160 --> 01:05:28.520]   And if you ever have a question,
[01:05:28.520 --> 01:05:30.760]   a 24/7 support team that's there when you need it
[01:05:30.760 --> 01:05:31.880]   and they're smart, they're nice.
[01:05:31.880 --> 01:05:35.080]   You need your own website.
[01:05:35.080 --> 01:05:37.400]   And if you struggled in the past,
[01:05:37.400 --> 01:05:40.120]   because it's been hard or complicated or expensive,
[01:05:40.120 --> 01:05:42.760]   you've, I found the place for you,
[01:05:42.760 --> 01:05:45.640]   wordpress.com/twit.
[01:05:45.640 --> 01:05:47.960]   32% of all websites run on WordPress.
[01:05:49.320 --> 01:05:52.760]   32% of all the websites in the world.
[01:05:52.760 --> 01:05:59.000]   Like a third of all the websites in the world run on WordPress,
[01:05:59.000 --> 01:06:01.000]   including some of the biggest brands,
[01:06:01.000 --> 01:06:02.040]   the biggest companies,
[01:06:02.040 --> 01:06:05.240]   and even a few little bloggers like me.
[01:06:05.240 --> 01:06:07.080]   WordPress.com/twit.
[01:06:07.080 --> 01:06:07.880]   If you go there right now,
[01:06:07.880 --> 01:06:10.040]   you'll get 15% off any new plan purchase.
[01:06:10.040 --> 01:06:13.720]   WordPress.com/twit.
[01:06:13.720 --> 01:06:15.560]   15% off your brand new website.
[01:06:15.560 --> 01:06:17.320]   Look no farther.
[01:06:17.320 --> 01:06:18.760]   I know you want to make a website.
[01:06:18.760 --> 01:06:20.680]   This is the easiest way to do it.
[01:06:20.680 --> 01:06:22.120]   And if you have a friend like Devindra,
[01:06:22.120 --> 01:06:23.000]   just had a baby,
[01:06:23.000 --> 01:06:27.160]   you should make SophiaLeahHeartowar.com
[01:06:27.160 --> 01:06:30.200]   and set up a site for him and give it to him as a gift.
[01:06:30.200 --> 01:06:33.000]   That's a good idea.
[01:06:33.000 --> 01:06:34.760]   I actually just set up a WordPress site too.
[01:06:34.760 --> 01:06:35.720]   It is fantastic.
[01:06:35.720 --> 01:06:36.440]   It's so good.
[01:06:36.440 --> 01:06:36.760]   Yeah.
[01:06:36.760 --> 01:06:38.360]   And I love the templates look good.
[01:06:38.360 --> 01:06:41.880]   And a little known fact, Leo,
[01:06:41.880 --> 01:06:43.400]   WordPress was born in Houston.
[01:06:43.400 --> 01:06:45.080]   Is that where Matt's from?
[01:06:45.080 --> 01:06:45.880]   Yep.
[01:06:45.880 --> 01:06:47.080]   He was originally from Houston.
[01:06:47.080 --> 01:06:49.880]   He attended the high school for the performing and visual arts.
[01:06:49.880 --> 01:06:52.280]   I believe he was a saxophonist.
[01:06:52.280 --> 01:06:53.240]   I had no idea.
[01:06:53.240 --> 01:06:55.000]   And he wrote it.
[01:06:55.000 --> 01:06:55.960]   You co-wrote it here.
[01:06:55.960 --> 01:06:59.080]   There was somebody I think in the UK he worked with.
[01:06:59.080 --> 01:07:01.880]   And a blogger here named it for him.
[01:07:01.880 --> 01:07:03.640]   He was trying to figure out a name for it.
[01:07:03.640 --> 01:07:05.880]   And Christine Tremblay came up with the day word.
[01:07:05.880 --> 01:07:06.520]   Oh, I'm kidding.
[01:07:06.520 --> 01:07:07.880]   Oh, I know Matt and I love.
[01:07:07.880 --> 01:07:12.200]   You know, I started using WordPress back then in the early 2000s.
[01:07:12.200 --> 01:07:15.320]   And I'd be using it ever since.
[01:07:15.320 --> 01:07:16.120]   That's really interesting.
[01:07:16.120 --> 01:07:17.560]   I didn't know he was from Houston.
[01:07:17.560 --> 01:07:20.040]   I figure Matt Mulliwig, he must be from Holland or somewhere.
[01:07:20.040 --> 01:07:23.960]   Yeah, his family still lives in Sugar Land here here.
[01:07:23.960 --> 01:07:25.000]   Sugar Land, Texas.
[01:07:25.000 --> 01:07:25.640]   Yeah.
[01:07:25.640 --> 01:07:25.960]   Nice.
[01:07:25.960 --> 01:07:29.000]   Let's see.
[01:07:29.000 --> 01:07:31.160]   We already did all the Cyber Monday shopping.
[01:07:31.160 --> 01:07:37.320]   Although we didn't do all the Amazon stories to Amazon.
[01:07:37.320 --> 01:07:40.600]   And there were quite a few this week at Amazon.
[01:07:40.600 --> 01:07:42.280]   Renew?
[01:07:42.280 --> 01:07:42.840]   What is it?
[01:07:42.840 --> 01:07:43.400]   Amazon.
[01:07:43.400 --> 01:07:45.720]   What does it reinvent?
[01:07:46.520 --> 01:07:46.840]   Rizz?
[01:07:46.840 --> 01:07:47.800]   Something.
[01:07:47.800 --> 01:07:48.840]   Amazon had an event.
[01:07:48.840 --> 01:07:49.640]   Yeah.
[01:07:49.640 --> 01:07:52.680]   And they announced a crapload of stuff.
[01:07:52.680 --> 01:07:57.400]   Including this car, which I immediately ordered.
[01:07:57.400 --> 01:07:58.920]   It won't be out till March.
[01:07:58.920 --> 01:08:02.520]   AWS Deep Racer, it's a fully autonomous,
[01:08:02.520 --> 01:08:04.920]   118th scale race car.
[01:08:04.920 --> 01:08:08.360]   It's to help developers learn machine learning.
[01:08:08.360 --> 01:08:12.920]   $249 on Amazon, although they said it'll be $400.
[01:08:12.920 --> 01:08:13.480]   So I don't know.
[01:08:15.000 --> 01:08:19.880]   You're going to get hands on literally with a machine learning technique called reinforcement
[01:08:19.880 --> 01:08:20.280]   learning.
[01:08:20.280 --> 01:08:25.800]   And then they're going to have like races.
[01:08:25.800 --> 01:08:28.760]   They'll be a deep racer league.
[01:08:28.760 --> 01:08:33.240]   I'm, I thought this was hysterical.
[01:08:33.240 --> 01:08:36.920]   Of all the things Amazon announced, the weirdest one.
[01:08:36.920 --> 01:08:39.720]   Anybody buy one of these?
[01:08:39.720 --> 01:08:41.640]   It's just going to be me racing against myself.
[01:08:43.560 --> 01:08:45.800]   Four megapixel camera, 1080p resolution.
[01:08:45.800 --> 01:08:51.720]   It's got 802.11 AC Wi-Fi, USB ports, two hour, two hour battery life.
[01:08:51.720 --> 01:08:54.200]   Now you want one, right?
[01:08:54.200 --> 01:08:56.040]   It runs Ubuntu.
[01:08:56.040 --> 01:08:59.880]   And now you sold me.
[01:08:59.880 --> 01:09:01.480]   Now you know I want it.
[01:09:01.480 --> 01:09:08.200]   Robot operating system in the Intel OpenVINO computer vision toolkit.
[01:09:08.200 --> 01:09:10.680]   This would be a great holiday gift.
[01:09:10.680 --> 01:09:12.120]   It'll come out March 6th.
[01:09:13.160 --> 01:09:16.760]   Here is a piece of paper that says, I bought you this.
[01:09:16.760 --> 01:09:17.640]   You'll get it in March.
[01:09:17.640 --> 01:09:23.720]   They also announced a managed blockchain feature.
[01:09:23.720 --> 01:09:24.440]   Yeah.
[01:09:24.440 --> 01:09:27.240]   So for years, Amazon said we'll never do blockchain.
[01:09:27.240 --> 01:09:29.720]   And then they did.
[01:09:29.720 --> 01:09:30.920]   Right.
[01:09:30.920 --> 01:09:31.720]   It's just a legend.
[01:09:31.720 --> 01:09:32.680]   Never say never.
[01:09:32.680 --> 01:09:33.880]   Never say never.
[01:09:33.880 --> 01:09:34.760]   It's just a ledger.
[01:09:34.760 --> 01:09:36.200]   So it gives those who want it.
[01:09:36.200 --> 01:09:39.240]   They have that as part of their cloud services.
[01:09:39.240 --> 01:09:43.400]   Yeah. I mean, a lot of what reinvent is about was AWS cloud services.
[01:09:43.400 --> 01:09:49.080]   Here's the list of all the product announcements at Amazon reinvent.
[01:09:49.080 --> 01:09:54.360]   13 new machine learning services, eight new storage services,
[01:09:54.360 --> 01:10:00.600]   three services to build and deploy faster, four new capabilities that make it easier to build
[01:10:00.600 --> 01:10:04.200]   IoT applications, AWS ground station.
[01:10:04.200 --> 01:10:05.720]   This might be the weirdest one.
[01:10:05.720 --> 01:10:07.320]   If you're a satellite operator.
[01:10:07.800 --> 01:10:09.800]   [laughter]
[01:10:09.800 --> 01:10:13.080]   Ground station as a service or GSAS.
[01:10:13.080 --> 01:10:17.240]   It's a new service that makes it easy and cost effective
[01:10:17.240 --> 01:10:22.200]   for customers to download data from satellites into AWS global infrastructure
[01:10:22.200 --> 01:10:24.520]   regions for analysis and things like that.
[01:10:24.520 --> 01:10:30.120]   I wonder if that's part of Blue Origin, part of Bezos's space.
[01:10:30.120 --> 01:10:31.320]   Yeah. First, I thought maybe.
[01:10:31.320 --> 01:10:35.160]   But then remember that's a separate, that's Bezos's personal investment,
[01:10:35.160 --> 01:10:36.360]   like the Washington Post.
[01:10:36.360 --> 01:10:37.080]   So I don't know.
[01:10:37.080 --> 01:10:40.120]   I mean, clearly he's interested in this kind of stuff.
[01:10:40.120 --> 01:10:43.560]   There's business intelligence.
[01:10:43.560 --> 01:10:47.720]   I mean, there was so much stuff.
[01:10:47.720 --> 01:10:50.280]   It was kind of hard to believe.
[01:10:50.280 --> 01:10:52.760]   So that's why the, I'm sure that's why they did the car.
[01:10:52.760 --> 01:10:59.000]   So the mainstream media and dumbies like me who don't really want to talk about lambda layers
[01:10:59.000 --> 01:11:02.840]   have something to show off.
[01:11:03.400 --> 01:11:06.040]   They may get some really interesting data from those cars too.
[01:11:06.040 --> 01:11:09.640]   So I wonder, you know, these are things that are out there collecting location data and all
[01:11:09.640 --> 01:11:14.920]   sorts of things and maybe mapping data won't be the same as like a full size test car, but who knows?
[01:11:14.920 --> 01:11:16.840]   Well, that's interesting.
[01:11:16.840 --> 01:11:20.120]   I'll have to look carefully at the license agreement.
[01:11:20.120 --> 01:11:27.640]   SageMaker Ground Truth Amazon Forecast, Amazon Comprehend Medical.
[01:11:27.640 --> 01:11:31.560]   Actually, John Doar, who's a big investor in Amazon, says he's pretty sure you're going to see
[01:11:31.560 --> 01:11:36.600]   Amazon health prime sometime this year, 2019.
[01:11:36.600 --> 01:11:39.720]   Amazon's clearly going into health, right?
[01:11:39.720 --> 01:11:43.240]   Because they already announced, yeah, health prime health.
[01:11:43.240 --> 01:11:45.720]   Is the health insurance company?
[01:11:45.720 --> 01:11:46.840]   Is that what he's talking about?
[01:11:46.840 --> 01:11:47.720]   Yeah, yeah.
[01:11:47.720 --> 01:11:49.480]   Maybe more than that.
[01:11:49.480 --> 01:11:52.680]   Maybe also a way to manage your health care.
[01:11:52.680 --> 01:11:57.560]   He says Amazon, Alphabet, and Apple all have a role in the future of health care.
[01:11:57.560 --> 01:12:00.040]   He was talking at the Forbes Health Care Summit.
[01:12:00.040 --> 01:12:04.440]   He says Amazon's among the best position companies to take what it's learned from its customers
[01:12:04.440 --> 01:12:06.280]   and use it to their benefit.
[01:12:06.280 --> 01:12:13.160]   It would be for medical and health products, I guess maybe you'd pay a flat fee and then have
[01:12:13.160 --> 01:12:15.640]   access to what about low-cost drugs, right?
[01:12:15.640 --> 01:12:19.800]   Didn't Amazon buy with a pill pack earlier this year?
[01:12:19.800 --> 01:12:24.040]   Yeah, one of our sponsors, which is a drug store, but it's an internet drug store.
[01:12:25.000 --> 01:12:30.120]   So when you go online at pill pack, you bring your prescription and then they send you,
[01:12:30.120 --> 01:12:30.840]   I actually did it.
[01:12:30.840 --> 01:12:31.320]   It's great.
[01:12:31.320 --> 01:12:36.840]   They send you these a box with all of your medications or vitamins or whatever it is
[01:12:36.840 --> 01:12:41.160]   in pouches per day, per time of day and day.
[01:12:41.160 --> 01:12:41.880]   It's so cool.
[01:12:41.880 --> 01:12:44.440]   So yeah, they own that.
[01:12:44.440 --> 01:12:51.640]   Remember they're doing a partnership with Berkshire Hathaway and I was at John Hancock to do
[01:12:52.280 --> 01:12:57.640]   health insurance for their employees, but that's exactly what you do before you roll out a
[01:12:57.640 --> 01:12:59.480]   larger consumer effort.
[01:12:59.480 --> 01:13:02.680]   Yeah, that was with JPMorgan Chase, I think.
[01:13:02.680 --> 01:13:03.640]   That's who it was.
[01:13:03.640 --> 01:13:05.640]   That's who it was, JPMorgan.
[01:13:05.640 --> 01:13:13.240]   Remember that both Google and Microsoft tried some stuff with health and both abandoned it.
[01:13:13.240 --> 01:13:18.760]   Not sure if it was because of privacy issues, but I would guess.
[01:13:19.560 --> 01:13:22.200]   Would you trust Amazon more with your health information?
[01:13:22.200 --> 01:13:24.040]   Trust it more than Facebook.
[01:13:24.040 --> 01:13:27.320]   Yeah, I ain't doing Facebook health.
[01:13:27.320 --> 01:13:28.520]   That's for sure.
[01:13:28.520 --> 01:13:30.200]   I'm waiting for an email from Facebook.
[01:13:30.200 --> 01:13:32.840]   They said it would be really easy to return those portals.
[01:13:32.840 --> 01:13:35.400]   They said you have 30 days to return them.
[01:13:35.400 --> 01:13:37.880]   And then I went to the page where it tells you how to return them.
[01:13:37.880 --> 01:13:39.480]   And it just says, just contact us.
[01:13:39.480 --> 01:13:42.360]   No link, no email, nothing.
[01:13:42.360 --> 01:13:43.160]   Just contact us.
[01:13:43.160 --> 01:13:45.720]   You figure out how and good luck.
[01:13:45.720 --> 01:13:51.640]   For a social network, they win the award for the least social ball company without question.
[01:13:51.640 --> 01:13:53.720]   How do you contact?
[01:13:53.720 --> 01:13:54.200]   I don't know.
[01:13:54.200 --> 01:13:55.160]   I sent them an email.
[01:13:55.160 --> 01:13:55.640]   We'll see.
[01:13:55.640 --> 01:14:08.040]   Amazon Translate Custom Terminology, Greengrass ML inference engines.
[01:14:08.040 --> 01:14:12.600]   I mean, I have nothing to say about any of this stuff because it's all just, you know,
[01:14:13.560 --> 01:14:15.720]   I saw one of them had the word quantum in it.
[01:14:15.720 --> 01:14:17.480]   Are they using any quantum computing?
[01:14:17.480 --> 01:14:21.160]   There was a quantum something, quantum ledger, I believe.
[01:14:21.160 --> 01:14:25.880]   Well, that's got to be a merely a marquee quantum database.
[01:14:25.880 --> 01:14:26.600]   It's just marketing.
[01:14:26.600 --> 01:14:28.200]   There is no quantum computing.
[01:14:28.200 --> 01:14:34.200]   And I just read an article from somebody fairly respectable in physics who says,
[01:14:34.200 --> 01:14:37.560]   there's not going to quantum computing is never going to happen.
[01:14:38.440 --> 01:14:43.320]   Which will be a great relief to all the people with the public private key pairs.
[01:14:43.320 --> 01:14:48.440]   QLDB quantum ledger database is a fully managed ledger database.
[01:14:48.440 --> 01:14:54.120]   It provides transparent, immutable, and cryptographically verifiable transaction log.
[01:14:54.120 --> 01:14:55.400]   Well, this sounds like blockchain.
[01:14:55.400 --> 01:14:57.000]   It does.
[01:14:57.000 --> 01:14:58.280]   Is this the blockchain thing?
[01:14:58.280 --> 01:15:00.520]   That's the blockchain thing.
[01:15:00.520 --> 01:15:04.280]   So it's got two buzzwords for the price of one.
[01:15:04.280 --> 01:15:07.320]   Quantum blockchain.
[01:15:07.320 --> 01:15:08.600]   Now how much would you pay?
[01:15:08.600 --> 01:15:11.240]   That sounds good.
[01:15:11.240 --> 01:15:12.040]   I'll take that.
[01:15:12.040 --> 01:15:15.880]   Anyway, I bought the car.
[01:15:15.880 --> 01:15:17.000]   I thought we'd have fun with it.
[01:15:17.000 --> 01:15:18.840]   And maybe I'll learn something about RL.
[01:15:18.840 --> 01:15:23.960]   Should we talk about Facebook?
[01:15:23.960 --> 01:15:27.960]   It's the gift that just keeps on giving.
[01:15:27.960 --> 01:15:28.520]   What is that?
[01:15:28.520 --> 01:15:28.600]   Yeah.
[01:15:28.600 --> 01:15:32.840]   What a soap opera going on at Facebook.
[01:15:32.840 --> 01:15:38.120]   Now the latest Facebook in Russia, UK, sees thousands of emails.
[01:15:38.120 --> 01:15:46.040]   This is a Bloomberg story by a guy named Nate Langson.
[01:15:46.040 --> 01:15:48.280]   What's the story?
[01:15:48.280 --> 01:15:51.080]   My colleague, Kitty Donaldson.
[01:15:51.080 --> 01:15:51.320]   Yeah.
[01:15:51.320 --> 01:15:53.080]   Okay.
[01:15:53.080 --> 01:15:59.960]   So Mark Zuckerberg, as I think everyone knows, has appeared in front of Congress.
[01:15:59.960 --> 01:16:02.360]   He's appeared in front of the European Parliament.
[01:16:02.360 --> 01:16:07.640]   And the consensus is that he got a fairly easy ride from both of those.
[01:16:07.640 --> 01:16:12.760]   There's another investigation going on into fake news in Britain.
[01:16:12.760 --> 01:16:18.520]   And that's being run by a cross party committee called DCMS,
[01:16:18.520 --> 01:16:20.680]   Departure for Culture, Media and Sport.
[01:16:20.680 --> 01:16:22.840]   And it's run by a guy called Damien Collins.
[01:16:22.840 --> 01:16:28.760]   And Damien Collins is a very, very tough guy when it comes to the activities of Facebook.
[01:16:29.880 --> 01:16:33.480]   Allegations of election meddling, all that kind of stuff.
[01:16:33.480 --> 01:16:40.440]   But Mark Zuckerberg has so far refused several times to appear in front of this committee.
[01:16:40.440 --> 01:16:41.640]   He sent other people.
[01:16:41.640 --> 01:16:44.680]   He sent Mike Schrepp for the CTO the other day.
[01:16:44.680 --> 01:16:50.760]   He sent Julian King, his name was, to speak, but Zuckerberg will not appear.
[01:16:50.760 --> 01:16:52.200]   It's a long flight.
[01:16:52.200 --> 01:16:56.920]   Yeah, it's a very long flight, but they offered to do it by video link and he wouldn't do that either.
[01:16:56.920 --> 01:16:57.880]   Nevermind.
[01:16:57.880 --> 01:17:02.760]   So, yeah, so he's just refusing point blank to do it.
[01:17:02.760 --> 01:17:07.400]   And they are just putting more and more and more and more pressure on him with every passing
[01:17:07.400 --> 01:17:08.040]   week, it seems.
[01:17:08.040 --> 01:17:12.040]   Do you think he's afraid that he won't get the white glove treatment he got from the US Congress?
[01:17:12.040 --> 01:17:13.640]   100%.
[01:17:13.640 --> 01:17:14.040]   Yeah.
[01:17:14.040 --> 01:17:16.760]   With without question, without question.
[01:17:16.760 --> 01:17:21.480]   I mean, if you look at some of the hearings, I mean, they are tough.
[01:17:21.480 --> 01:17:26.200]   I mean, they're practically yelling at them, really, really intense.
[01:17:26.200 --> 01:17:29.080]   Very entertaining as a reporter sitting there covering them as well, I have to say.
[01:17:29.080 --> 01:17:35.480]   But with this email thing, the end of the day, this committee is trying to get to the bottom
[01:17:35.480 --> 01:17:41.320]   of fake news and what happened around Brexit, ahead of the Brexit vote and obviously the 2016
[01:17:41.320 --> 01:17:45.640]   presidential elections, which is still obviously very relevant to any country.
[01:17:45.640 --> 01:17:54.680]   And so there was an order placed for a gentleman who actually has a lawsuit against Facebook,
[01:17:55.320 --> 01:17:57.320]   who is in London on a business trip.
[01:17:57.320 --> 01:18:01.880]   And he had a lot of emails and it was tipped off to somebody in this committee that he had a
[01:18:01.880 --> 01:18:02.920]   lot of emails.
[01:18:02.920 --> 01:18:09.080]   And so they sent a, essentially they sent a heavy, they send somebody around to his hotel room
[01:18:09.080 --> 01:18:14.040]   and demanded that he give them these thousands of emails.
[01:18:14.040 --> 01:18:14.520]   What?
[01:18:14.520 --> 01:18:22.600]   Because they believed, and it seems accurately so, that these emails contained a lot of internal
[01:18:22.600 --> 01:18:30.280]   discussion between Facebook engineers about the Russian interference, potential Russian
[01:18:30.280 --> 01:18:30.840]   interference.
[01:18:30.840 --> 01:18:35.320]   Now these emails were sealed by the court here in the United States.
[01:18:35.320 --> 01:18:42.920]   Yes. And I've got it on pretty good measure that they are going to release them next week,
[01:18:42.920 --> 01:18:50.200]   potentially unredacted in Britain, which would, I think, be contrary to what the,
[01:18:50.200 --> 01:18:53.240]   I think is a Californian court that sealed them as part of this lawsuit, right?
[01:18:53.240 --> 01:18:53.560]   Right.
[01:18:53.560 --> 01:18:55.160]   Sam and Tammy Superior Court.
[01:18:55.160 --> 01:19:00.920]   Yeah. I mean, I've seen about, I've seen maybe four or five of these emails,
[01:19:00.920 --> 01:19:07.480]   but I'm told that there are thousands of emails and it's just been, they're basically what?
[01:19:07.480 --> 01:19:08.200]   What do they say?
[01:19:08.200 --> 01:19:09.160]   What do the emails say?
[01:19:09.160 --> 01:19:17.080]   Well, the ones that I've seen that I refer to in this story were originally from, I think,
[01:19:17.080 --> 01:19:24.760]   2014, where a Facebook engineer had seen what looked like a huge number of API calls coming from
[01:19:24.760 --> 01:19:30.040]   Russian IP addresses. And the email chain that went through, that I saw,
[01:19:30.040 --> 01:19:35.480]   essentially tried to establish, you know, are these legitimate API calls? Are these
[01:19:35.480 --> 01:19:38.840]   genuinely coming from Russia? Are these things we need to be worried about?
[01:19:38.840 --> 01:19:45.320]   And over the course of the email chain, you essentially see that these emails concluded,
[01:19:45.320 --> 01:19:49.000]   I should say, that the API requests were legitimate.
[01:19:49.000 --> 01:19:54.760]   They were actually coming from Pinterest servers and were the result of a technical fault.
[01:19:54.760 --> 01:19:58.680]   And then the email chain moves on and moves on to other stuff.
[01:19:58.680 --> 01:20:02.120]   Of course, it wasn't real.
[01:20:02.120 --> 01:20:08.360]   Yeah, it wasn't real. But the fact of the matter is that there's still thousands of other emails
[01:20:08.360 --> 01:20:14.360]   that this gentleman has that now this parliamentary committee has seized and we're,
[01:20:14.360 --> 01:20:18.200]   you know, we're sort of sitting on the edge of our seats just waiting to get hold of them,
[01:20:18.200 --> 01:20:21.880]   which, as I say, I believe will be this coming week.
[01:20:21.880 --> 01:20:25.240]   Do you think there'll be a smoking gun in there? I mean, the smoking, and what would a smoking
[01:20:25.240 --> 01:20:28.840]   gun reveal that Facebook knew about Russian interference and did nothing?
[01:20:28.840 --> 01:20:35.480]   Yes. That's what they want to find. And it's too early to speculate because we haven't seen enough.
[01:20:35.480 --> 01:20:41.640]   We don't know enough about it. There's also, it's not for me to gossip about the way that these
[01:20:41.640 --> 01:20:46.760]   emails were discovered. But I would just encourage people to do a bit of googling and look at some
[01:20:46.760 --> 01:20:52.600]   of the background of how these emails were discovered in the first place. But certainly,
[01:20:52.600 --> 01:20:56.920]   that's what the committee that sees them is looking for. It's looking for a smoking gun that says,
[01:20:56.920 --> 01:21:01.640]   Facebook knew about Russian meddling and Facebook did not do anything. And obviously,
[01:21:01.640 --> 01:21:07.800]   Facebook vehemently denies all of this and has vehemently denied it up until as recently as this
[01:21:07.800 --> 01:21:12.040]   week. So it will continue to do so, I'm sure. But it's definitely a story that's going to keep on
[01:21:12.040 --> 01:21:18.120]   giving. So I don't know the story of 643, which is the company that had the emails. They had,
[01:21:18.120 --> 01:21:23.560]   they'd receive them in discovery, apparently they're suing Facebook for something.
[01:21:23.560 --> 01:21:29.720]   Yeah, they made a tool, a Facebook app that allowed you to find photos of your friends in
[01:21:29.720 --> 01:21:40.600]   Bikinis. It's really, it's highbrow stuff. So that's there by the way, that's, no, I'm sorry.
[01:21:40.600 --> 01:21:50.280]   So he's suing and then Facebook provided in legal discovery, some emails presumably relevant to
[01:21:50.280 --> 01:21:56.920]   the case. They were sealed by the court. But as you said, the founder of the company was in
[01:21:56.920 --> 01:22:03.480]   London on a business trip and accosted by a committee of UK lawmakers. This is a movie, man. Give us
[01:22:03.480 --> 01:22:08.440]   the emails. I won't give you the emails. Give us the email. They finally gave him the emails.
[01:22:08.440 --> 01:22:15.160]   Facebook's now saying, we want sanctions because he violated his court order, sealing the documents.
[01:22:15.160 --> 01:22:19.240]   That hearing, I think has just happened on Friday.
[01:22:22.200 --> 01:22:29.080]   Facebook, so do you think that these emails reveal in discovery would somehow have to do with
[01:22:29.080 --> 01:22:35.240]   Russian involvement? And why would they? Well, I mean, that's basically that all of this is to
[01:22:35.240 --> 01:22:42.760]   try and put pressure on getting Zuckerberg into the hearing. So whether the
[01:22:42.760 --> 01:22:48.360]   major main or not isn't really important. No, I mean, certainly I think that they're going
[01:22:48.360 --> 01:22:52.600]   through it to seeing if they can find anything, because if there's even a shred of detail in there
[01:22:52.600 --> 01:22:56.360]   that allows them to put additional pressure on Zuckerberg, then they're going to use it. I mean,
[01:22:56.360 --> 01:23:00.760]   to give you an example of how heavy handed they're being with us, while the parliamentary hearing
[01:23:00.760 --> 01:23:07.000]   was going on last week, this is the one where Facebook's privacy chief was being interviewed
[01:23:07.000 --> 01:23:10.440]   for several hours, these go on for hours, these hearings is ridiculous.
[01:23:10.440 --> 01:23:17.880]   The public relations team on behalf of the committee sent out a photograph, a professional
[01:23:17.880 --> 01:23:25.160]   photograph of a empty seat with Mark Zuckerberg's name on it, and the whole of the committee
[01:23:25.160 --> 01:23:32.440]   looking towards it, just to say he's not shown up again. And when I got that email, I was like,
[01:23:32.440 --> 01:23:36.680]   guys, seriously, this is such an obvious PR stunt. But I'm also I'm almost kind of glad they did,
[01:23:36.680 --> 01:23:41.320]   because it validates my theory that everything they're doing is to just put more and more and
[01:23:41.320 --> 01:23:46.040]   more public pressure and potentially embarrassment on Zuckerberg until he gets on a plane and comes
[01:23:46.040 --> 01:23:51.160]   here and answers some questions. And there are many people who agree with that. I mean,
[01:23:51.160 --> 01:23:57.000]   Christopher Wiley, the whistleblower behind it, I know him reasonably well these days,
[01:23:57.000 --> 01:24:02.760]   I speak to him quite often. And he's adamant that the main reason that Zuckerberg won't come to
[01:24:02.760 --> 01:24:07.800]   London is because he knows it's the only place he's going to get given a properly difficult time.
[01:24:07.800 --> 01:24:09.320]   And a hard time answering questions.
[01:24:09.320 --> 01:24:13.160]   Wiley was the whistleblower who had worked for Cambridge Analytica and told everybody,
[01:24:13.160 --> 01:24:19.880]   oh, this is what they really were doing. Yeah, exactly. He's the one that in March revealed all of this
[01:24:19.880 --> 01:24:26.360]   was happening. And there has been since then more and more evidence that Cambridge Analytica
[01:24:26.360 --> 01:24:30.360]   and Steve Bannon were involved in throwing the Brexit election.
[01:24:30.360 --> 01:24:38.280]   Yeah, and that's that's been again, like a massive reason for this committee trying, you know,
[01:24:38.280 --> 01:24:42.120]   they want to get to the bottom of this, they want to find out the truth, because if there is
[01:24:42.120 --> 01:24:51.160]   belief that Brexit was unduly influenced by any kind of rigging or persuasion, then they, you know,
[01:24:51.160 --> 01:24:55.640]   they all want to do. You can unlike the presidential election, you could have a doover, couldn't you?
[01:24:55.640 --> 01:25:01.960]   Yeah, I mean, talking about doing a doover, the Brexit, a second referendum in Britain right now
[01:25:01.960 --> 01:25:07.000]   is sort of it's on all the all the front pages. There's a lot of calls for that to happen.
[01:25:07.000 --> 01:25:11.560]   Interesting. It's a very, very controversial one, because, you know, for those of you who are
[01:25:11.560 --> 01:25:18.840]   interested, the people who are against this think that it's a punch in the face of democracy that,
[01:25:18.840 --> 01:25:22.840]   you know, you don't do another vote. It was like a result. So results. So we're going to do it again.
[01:25:22.840 --> 01:25:29.240]   Exactly. But the argument that I personally a little more on the side of is that so much has
[01:25:29.240 --> 01:25:34.280]   come out since then in terms of either whether it's how people were advertised to as part of
[01:25:34.280 --> 01:25:38.200]   campaigning. We have, you know, there are exceptionally strict rules that have been
[01:25:38.200 --> 01:25:43.480]   provably broken. But also the fact that we didn't even know what kind of a deal was on the table.
[01:25:43.480 --> 01:25:48.280]   There's a lot of people that think that you should at least have a vote for the deal that we now have
[01:25:48.280 --> 01:25:55.640]   or no deal. But that's not really a tech topic. So that shouldn't be a topic, which I think I feel
[01:25:55.640 --> 01:26:04.760]   like there's a certain amount of finger pointing both in the US and the UK, because a large number
[01:26:04.760 --> 01:26:11.320]   of people didn't like the results of the vote. And the finger pointing is at tech companies because
[01:26:11.320 --> 01:26:17.160]   they make great scapegoats. And I don't know anything about the UK situation, but here in the
[01:26:17.160 --> 01:26:22.520]   United States, it seems pretty clear to me that no matter what happened on Facebook and Twitter,
[01:26:22.520 --> 01:26:30.440]   the will of the people was reflected fairly accurately by this vote. That it wasn't a question of
[01:26:31.160 --> 01:26:35.800]   convincing people to vote against their best interests or do something they wouldn't have done
[01:26:35.800 --> 01:26:42.280]   otherwise. It certainly helped encourage people and gave people a chance to vent. But I don't know
[01:26:42.280 --> 01:26:47.160]   if you could say the vote was or could you? That was a pretty close election, though. Like,
[01:26:47.160 --> 01:26:51.880]   that's the thing. Yeah. It was so close, both. Yeah, in the US and the same. But it's always close.
[01:26:51.880 --> 01:26:59.000]   So this is the point is that it's close because it's close. There is a definite disagreement that
[01:26:59.000 --> 01:27:04.360]   there's two different populations in the United States, and they disagree. And they're fairly
[01:27:04.360 --> 01:27:11.000]   equal in size, and it's fairly close. And so it's easily, but basically it means it's a system that's
[01:27:11.000 --> 01:27:15.560]   close to tide. Yes, you're right. It could be thrown either way. But even if you say, well,
[01:27:15.560 --> 01:27:20.360]   it nudged the needle, the point is it's still close. I mean, it's not that different from
[01:27:20.360 --> 01:27:26.840]   half the people wanting this. I don't see any problem holding Facebook to task, certainly,
[01:27:26.840 --> 01:27:31.720]   like between what we're seeing with the what the Russian troll operation, and this and everything
[01:27:31.720 --> 01:27:37.480]   happening like clearly, a lot of people were using these platforms in bad faith and figured
[01:27:37.480 --> 01:27:41.000]   out ways to like socially engineer a lot of that. And I agree with you. I think these platforms
[01:27:41.000 --> 01:27:45.480]   deserve, yeah, they deserve to be criticized for not seeing. And I agree with you on that.
[01:27:45.480 --> 01:27:50.520]   I think that a lot of it, though, comes from people who want to do over. And that's, I don't
[01:27:50.520 --> 01:27:56.280]   agree with that. I mean, if you want to go after them for these these issues, fine, but
[01:27:56.280 --> 01:27:59.080]   not because it changed the result of the election, I guess.
[01:27:59.080 --> 01:28:03.720]   There are some very interesting things that have come out as a result of all this. There was
[01:28:03.720 --> 01:28:10.280]   another story that I wrote earlier this week based off something that Christopher Wiley had said
[01:28:10.280 --> 01:28:15.880]   that the preferences of people's fashion, and this was this was in the US, this was ahead of the
[01:28:15.880 --> 01:28:23.160]   presidential election, their fashion preferences, the brands that they they showed affinity towards.
[01:28:23.160 --> 01:28:29.560]   This is a great article, by the way. I love it. Thank you. We're a massive factor in determining
[01:28:29.560 --> 01:28:36.040]   whether somebody was more inclined towards the Republicans or the Democrats, specifically that,
[01:28:36.040 --> 01:28:41.480]   you know, if you wore Wrangler jeans, you're more likely to be persuadable to vote Republican.
[01:28:41.480 --> 01:28:48.040]   And if you were a shopper of Abercrombie and Fitch or Macy's, then you were more likely you could
[01:28:48.040 --> 01:28:52.760]   be persuaded in voting Democrat. And you know, it's amazing when you think about it, but the
[01:28:52.760 --> 01:28:57.720]   bottom line is that everybody has to make a choice about fashion. Like it nobody walks, well,
[01:28:57.720 --> 01:29:02.040]   some people walk out the house naked, but most people have the decency to put some clothes on,
[01:29:02.040 --> 01:29:05.080]   which means they have to go into a shop to buy some clothes, they make that choice,
[01:29:05.080 --> 01:29:08.600]   they interact with those brands, maybe online, maybe they're shopping for Black Friday deals.
[01:29:08.600 --> 01:29:14.520]   That data is there on Facebook and can be used when, you know, triangulated with other data points
[01:29:14.520 --> 01:29:18.680]   to pretty accurately reflect their political leaning.
[01:29:19.560 --> 01:29:23.000]   So Nate, just so you know, this is a Wrangler shirt, buddy.
[01:29:23.000 --> 01:29:28.280]   You're in Houston. It's a given that you voted red. I mean, that's just the way it is.
[01:29:28.280 --> 01:29:31.160]   It's a red Wrangler shirt.
[01:29:31.160 --> 01:29:38.120]   I believe in my article, I may have said Wrangler jeans, so I'm slightly side-stepping that.
[01:29:38.120 --> 01:29:41.000]   Oh, okay. Oh, so the shirt's okay. All right.
[01:29:41.000 --> 01:29:45.240]   But this, by the way, this goes back more than many, many decades.
[01:29:45.240 --> 01:29:51.240]   Consumer preferences have always been tied to, you know, I remember 30 years ago, somebody coming in
[01:29:51.240 --> 01:29:55.720]   from a company that did this kind of research well before the internet. And they said, you tell
[01:29:55.720 --> 01:29:59.640]   me your zip code, I could tell you what magazines you subscribe to, what kind of cars in your driveway.
[01:29:59.640 --> 01:30:04.760]   I know a lot about you because birds of a feather flock together. And this has always been
[01:30:04.760 --> 01:30:08.680]   marketing people have always known this. Yeah, this doesn't seem too surprising.
[01:30:08.680 --> 01:30:13.240]   Like after there was a story about what was it target, knowing your potentially pregnant,
[01:30:13.240 --> 01:30:16.520]   based on like the trends of what you're buying. And that seemed like Black magic.
[01:30:16.520 --> 01:30:21.400]   This is just like, yeah, Wrangler jeans. Yeah, you can tell who wants it.
[01:30:21.400 --> 01:30:24.040]   I bet if you like country music, you're more likely to vote Republican.
[01:30:24.040 --> 01:30:28.440]   Exactly. The issue though, as Nate brought up is, can you then say,
[01:30:28.440 --> 01:30:33.640]   ah, well, this person is influenceable. Can I nudge them in the direction I want them to go?
[01:30:33.640 --> 01:30:38.680]   And that's basically been the job of PR for many years. I mean, you can go back to
[01:30:38.680 --> 01:30:43.960]   the chap Edward Benays. A lot of people call him the father of PR. And he was being, you know,
[01:30:43.960 --> 01:30:49.640]   he had a campaign to promote cigarettes. I think maybe lucky strike. I'm not, I'm not sure. But
[01:30:49.640 --> 01:30:53.400]   it was something like that. They were green. And essentially the way he did it is he manipulated
[01:30:53.400 --> 01:30:57.640]   fashion trends so that people liked a particular color of green because the color of green was
[01:30:57.640 --> 01:31:01.000]   the same as the cigarettes. And therefore the cigarette cells went up.
[01:31:01.000 --> 01:31:06.680]   He was a Sigmund Freud's nephew. It was really quite a character. Yeah, he claimed uncle
[01:31:06.680 --> 01:31:11.240]   Siggy gave him a lot of ideas about the unconscious mind and how to influence it.
[01:31:11.240 --> 01:31:16.840]   He's the guy who did four out of five doctors. He was hired by the pork council
[01:31:16.840 --> 01:31:21.880]   because they wanted people to buy more bacon. So he went out and he's yes, I doubt he
[01:31:21.880 --> 01:31:26.920]   knew anybody. Yeah, I doubt he interviewed anybody. But he said, for I think he gave them a kind of
[01:31:26.920 --> 01:31:31.480]   a suspicious questionnaire. Do you think people should have a hearty breakfast? Oh, yes.
[01:31:31.480 --> 01:31:37.080]   Four out of five doctors recommend a hearty breakfast, including bacon and eggs. And in fact,
[01:31:37.080 --> 01:31:45.240]   completely changed American diet. We didn't eat bacon for breakfast until then. He convinced
[01:31:45.240 --> 01:31:52.760]   women it was okay to smoke because he had suffragette smoking lucky strikes in a, in a, in a parade
[01:31:52.760 --> 01:31:59.880]   in New York City. And what he can't even came up with a name for lucky strikes like freedom strikes
[01:31:59.880 --> 01:32:05.000]   that like, if you believe in the women's right to vote, you should smoke. And if you're a woman,
[01:32:05.000 --> 01:32:11.480]   it's okay. And completely introduced smoking for women in the United States. So it, yes, this is an
[01:32:11.480 --> 01:32:19.320]   old and long known principle, nothing new here. And I'm sure every politician, you know,
[01:32:19.320 --> 01:32:24.200]   and every political campaign has done, tried to do sometimes less successfully than others,
[01:32:24.200 --> 01:32:29.720]   but has tried to do the same thing. Admittedly, Facebook, Twitter, social media, give you weaponized
[01:32:29.720 --> 01:32:34.760]   are able to weaponize these things. Because for instance, instead, you can do dark ads instead of
[01:32:34.760 --> 01:32:39.000]   putting in added where everybody can see what you're trying to do, you could show these ads only
[01:32:39.000 --> 01:32:43.560]   susceptible people and no one knows what you're doing. Yeah. And this is one of the areas where
[01:32:43.560 --> 01:32:50.760]   sorry, Dwight, go ahead. I don't think, you know, Facebook has put several things in place in an
[01:32:50.760 --> 01:32:55.320]   attempt to kind of fix this. And there was some indication that they had some success in the
[01:32:55.320 --> 01:33:00.920]   midterms in terms of preventing it. But when you look at kind of what they are doing and the scope
[01:33:00.920 --> 01:33:07.000]   of it, I think they just feel like they're lost, like they don't, they have no idea how to really
[01:33:07.000 --> 01:33:12.360]   keep this from happening again. And I think, you know, I don't necessarily feel very good
[01:33:12.360 --> 01:33:16.280]   about the 2020 election in the United States. Well,
[01:33:16.280 --> 01:33:25.000]   2018 went all right for Democrats, right? Well, I, but I don't think necessarily that there
[01:33:25.000 --> 01:33:32.040]   was the indications from what we saw in the in the stories writing about it was that there wasn't
[01:33:32.040 --> 01:33:39.400]   kind of the attempts that were made weren't on the same scale as they were. And maybe that's why not.
[01:33:39.400 --> 01:33:45.640]   Right. Facebook. Well, and it could be that also this is a distributed election. The focus,
[01:33:45.640 --> 01:33:52.440]   they focused on one thing in the 2016 election. And that was the presidential election.
[01:33:53.000 --> 01:33:58.680]   And I think in 2018, it's a distributed election. You know, there are so many people running.
[01:33:58.680 --> 01:34:04.200]   And I just don't think that it necessarily was the kind of thing where it would be easy to target
[01:34:04.200 --> 01:34:09.640]   just one thing. And also American politicians will ruin the election on their own, right? So there was
[01:34:09.640 --> 01:34:18.120]   that. We don't need help. Okay. I mean, that's so I suppose as a tech
[01:34:20.360 --> 01:34:24.840]   supporter, I should, I should defend these tech companies and say, look, this stuff happens all
[01:34:24.840 --> 01:34:31.960]   the time. You can't blame the election on Facebook. Sure stuff happened, but it but it's just because
[01:34:31.960 --> 01:34:35.960]   stuff also happened in magazines and newspapers across the country. These are the media that people
[01:34:35.960 --> 01:34:42.360]   consume. This is where if you're going to try to exert influence, you will. And and that's just
[01:34:42.360 --> 01:34:47.880]   because Facebook's part of the American conversation. There weren't there. No worse than anybody else.
[01:34:48.440 --> 01:34:54.520]   I disagree. I disagree with that. I think if you look, you know, it's it's it's Marshall McLuhan.
[01:34:54.520 --> 01:35:00.520]   Social media is a hot media. It's very personal. It's very intimate. And it's very easy to target.
[01:35:00.520 --> 01:35:07.160]   You know, it's very difficult to target, you know, a major newspaper. You see everybody sees that.
[01:35:07.160 --> 01:35:12.360]   But as you said, you could weaponize it in social media. And it's it's very personal. It's on your
[01:35:12.360 --> 01:35:17.640]   phone. You know, it's on your computer. You see it at work. You see it all the time.
[01:35:17.640 --> 01:35:22.200]   Your friends share it. It's a much hotter medium. I think it's much more influential.
[01:35:22.200 --> 01:35:27.320]   Well, we talk we talk about like less tech savvy users not being able to determine like when a fake,
[01:35:27.320 --> 01:35:32.120]   you know, malware call or something is coming in. This stuff can be weaponized in many ways. We
[01:35:32.120 --> 01:35:36.920]   were hearing about what's happening in India with WhatsApp and the potential of like somebody
[01:35:36.920 --> 01:35:42.280]   forwarding a fake message to their groups of friends can lead to a mob of people killing somebody.
[01:35:42.280 --> 01:35:46.680]   It's been like that's insane. Yeah, that's pure messaging. And that's I would put a lot of blame
[01:35:46.680 --> 01:35:51.720]   on the technology platform for at least not doing something to help maybe prevent that sort of usage.
[01:35:51.720 --> 01:35:58.360]   Let me let me. So in the 60s, particularly President Kennedy, but and then later,
[01:35:58.360 --> 01:36:04.040]   Nixon, hi, I'm hearing myself back. I'm also hearing a tone. I'm not sure what that's
[01:36:04.040 --> 01:36:11.320]   coming from Nixon used ad men brought admin into the election for the very first time
[01:36:12.040 --> 01:36:19.880]   used television effectively. And you could probably make the case in 1960. I think some have that
[01:36:19.880 --> 01:36:24.200]   people who watched the Kennedy Nixon debates thought Kennedy won people who listened to them
[01:36:24.200 --> 01:36:28.840]   on the radio thought Nixon won television had a huge impact, but there weren't congressional
[01:36:28.840 --> 01:36:32.600]   here or maybe there were I don't think there were congressional hearings calling television
[01:36:32.600 --> 01:36:41.640]   to task because it gives politicians new ways to sway American voters. It's just the
[01:36:41.640 --> 01:36:47.800]   nature of technology. Well, I think if you love technology, you know, there's a problem. And just
[01:36:47.800 --> 01:36:52.360]   if you love your country, you want to fix that problem. I think that those of us who are boosters
[01:36:52.360 --> 01:36:58.040]   of technology know we've got to write this ship or it is going to come down on our heads with maybe
[01:36:58.040 --> 01:37:03.560]   some really onerous regulation that does more than just what it needs to do. Because let's face
[01:37:03.560 --> 01:37:09.400]   that our politicians don't understand it. They don't. I think they now be getting to understand
[01:37:09.400 --> 01:37:13.960]   the implications and the power of it. But I don't think they really understand how it works.
[01:37:13.960 --> 01:37:19.320]   And I think that, you know, I'm one of, as I said, one of the things I'm really happy about
[01:37:19.320 --> 01:37:25.480]   with this new crop of Democratic House members is that a lot of them are younger and a lot of them
[01:37:25.480 --> 01:37:29.320]   by nature are going to understand tech more than their predecessors. Well spoken.
[01:37:29.320 --> 01:37:33.480]   Yeah. And this is going to understand I'm playing devil's advocate, but I get it's fair to get that
[01:37:34.280 --> 01:37:38.920]   other point of view in. I mean, this is this underscore is part of the problem with something
[01:37:38.920 --> 01:37:43.640]   like GDPR that we were talking about earlier, which is that often things like that are written
[01:37:43.640 --> 01:37:49.000]   with good intentions. I think GDPR was written with very good intentions. But it actually misses
[01:37:49.000 --> 01:37:53.720]   a huge part of what makes some of these platforms so powerful. And there's a hypothetical example
[01:37:53.720 --> 01:37:59.480]   that I like to use to illustrate this, which is if you are an individual and you have a
[01:38:00.600 --> 01:38:05.320]   hereditary medical condition, let's say, and it's something that you post about on social media.
[01:38:05.320 --> 01:38:12.040]   Maybe you listed on Facebook for whatever reason you do that. But you you you you believe your data
[01:38:12.040 --> 01:38:17.960]   is to be completely private and secret. Let's say you then have a brother who is directly related
[01:38:17.960 --> 01:38:25.080]   to you, who doesn't list anything about that medical condition at all. It's theoretically possible
[01:38:25.080 --> 01:38:33.480]   that that brother could be targeted by whatever kind of messaging, because an advertiser or a platform
[01:38:33.480 --> 01:38:39.800]   knows that a direct blood relative has a hereditary health condition. Therefore, it's highly likely
[01:38:39.800 --> 01:38:44.760]   or certainly possible that you know, that relative may also have a condition or certainly may pay
[01:38:44.760 --> 01:38:50.120]   more attention to adverts about that condition. And that's not something that you know, that data
[01:38:50.120 --> 01:38:56.040]   about that that brother, that other person is not really covered in a lot of the data protection
[01:38:56.040 --> 01:39:00.200]   systems that we've seen discussed. And it's not something that's very easy to explain to lawmakers
[01:39:00.200 --> 01:39:04.040]   who are trying to put these rules together. And that's why I think, you know, as was said,
[01:39:04.040 --> 01:39:11.080]   the newer generation, younger generation hopefully will make technology kind of front of mind in
[01:39:11.080 --> 01:39:16.520]   all of their policymaking rather than an afterthought. So that's interesting. You think GDPR doesn't
[01:39:16.520 --> 01:39:23.560]   go far enough? I think it exists for very, you know, the intention behind it was very good. But I
[01:39:23.560 --> 01:39:28.680]   think it's probably going to be ineffectual in the long run in fundamentally changing a lot of the
[01:39:28.680 --> 01:39:34.120]   stuff that it wants to change. Yeah. And in a way, isn't it up to us? I mean, we, not Congress,
[01:39:34.120 --> 01:39:39.400]   not Facebook, but us, if we stopped using it, if we encourage people to stop using it, don't
[01:39:40.280 --> 01:39:46.360]   get that work as well. I think we're a little too big to fail, right? Like Facebook,
[01:39:46.360 --> 01:39:51.880]   billions of users, Google, billions of users, like you can't even boycott them because they're
[01:39:51.880 --> 01:39:58.520]   so big. But yeah, go ahead, think. It's like people, people don't, you know, if there's a car
[01:39:58.520 --> 01:40:03.480]   accident, people don't say, don't don't drive cars. They say make safer cars.
[01:40:04.600 --> 01:40:10.440]   Good point. All right. And that's I think fix it. Don't don't abandon it. Fix it. Yeah. If you
[01:40:10.440 --> 01:40:17.160]   love it, fix it. If you love it, I don't love it. That's why I left it. I think you pop in there
[01:40:17.160 --> 01:40:22.200]   every now and then you kind of jump in. You know why? Because to put the Facebook portal,
[01:40:22.200 --> 01:40:27.320]   I had to reactivate my account. I never, I don't delete these accounts because I plan in my flag,
[01:40:27.320 --> 01:40:31.480]   and I don't want anybody else to take it. So I can't delete the account. I don't want somebody to
[01:40:31.480 --> 01:40:35.400]   take over my Facebook account or Twitter account or Instagram account or Tumblr account or that
[01:40:35.400 --> 01:40:40.200]   etc, etc. So what I do is deactivate. But in order to buy the portal, I had to reactivate.
[01:40:40.200 --> 01:40:43.640]   And then by the way, when I got the portal, I found out I can deactivate. But even if you
[01:40:43.640 --> 01:40:51.320]   deactivate Facebook, messenger does not deactivate. Oh, so the portal still worked. But I still don't
[01:40:51.320 --> 01:40:55.240]   want the portal because I don't want anything to do with Facebook. So I think I'm deactivated now.
[01:40:55.240 --> 01:41:02.040]   Oh, I know I am because my birthday was Thursday. And the last thing in the world I wanted was 800
[01:41:02.040 --> 01:41:08.040]   Facebook wishes birthday wishes. They at least organize that better now. Yeah.
[01:41:08.040 --> 01:41:12.760]   Is it better? I don't know. I just didn't want to have to respond. I feel guilty. If I don't respond,
[01:41:12.760 --> 01:41:20.840]   I feel to put upon if I do is just is a no win. So I saw a notification on your birthday on
[01:41:20.840 --> 01:41:28.680]   LinkedIn. So you need to shut that crap. Oh, he pulling me in. Yeah.
[01:41:28.680 --> 01:41:36.280]   So you can tell if I've come or gone from Facebook. So I have I have a plug in in the browser
[01:41:36.280 --> 01:41:42.200]   Facebook called social fixer. And it gives me a notification when somebody unfollows.
[01:41:42.200 --> 01:41:50.760]   And what's interesting is I see a lot of people turn their Facebook off for like a few days and
[01:41:50.760 --> 01:41:54.920]   then come back. And they do that over and over. And I don't know if it's because they're trying to
[01:41:54.920 --> 01:42:02.200]   get away from a stalker or somebody who's abusing them. But I see that turn on and off fairly frequently
[01:42:02.200 --> 01:42:09.080]   for certain people. And the thing with with you and the portal that makes a lot of sense that
[01:42:09.080 --> 01:42:12.760]   that matches that behavior. So it looks like I'm unfollowing you when I leave.
[01:42:12.760 --> 01:42:18.920]   Yes, it says. Leo report is no longer on your friends list is what it says.
[01:42:18.920 --> 01:42:25.640]   Oh my God. I apologize to all the people who think I'm not. I follow them. Who use social
[01:42:25.640 --> 01:42:31.320]   fixer if you don't use this, you don't see that. Wow. That sounds like an anxiety nightmare.
[01:42:31.320 --> 01:42:39.160]   That that app. Yeah. Like, yeah. It hides ads and face. And it allows you to block keywords. So
[01:42:39.160 --> 01:42:45.080]   like if I don't want to see anything involving say the president, I can block on his name and I
[01:42:45.080 --> 01:42:50.920]   don't see any of those stories. So it's interesting because when I did rejoin, I went to look at my
[01:42:50.920 --> 01:43:00.040]   Facebook feed just to see. And it was so boring and horrible and and and and made me so sad that I
[01:43:00.040 --> 01:43:06.680]   just reaffirmed because you know, once you stop using it, you forget and you see it and you go,
[01:43:06.680 --> 01:43:13.000]   wow, why was I anything on that menu? Like, yeah, why was I addicted to this? This is this is not
[01:43:13.000 --> 01:43:18.360]   interesting. But I think like this conversation about Facebook and the elections and the responsibility.
[01:43:18.360 --> 01:43:22.120]   We've reached a new point for all these tech companies where I think we're looking more at the
[01:43:22.120 --> 01:43:27.400]   human impact of them. And this is a new phase of maybe how we're looking at tech because it's so
[01:43:27.400 --> 01:43:31.640]   deeply embedded in our lives. But we're seeing less than all friends, right? Like Amazon and
[01:43:31.640 --> 01:43:37.240]   labor practices and what they're shipping factories are like, even around Reddit redemption too,
[01:43:37.240 --> 01:43:41.000]   a game which a ton of people are super excited for. There's been a lot of great coverage around the
[01:43:41.000 --> 01:43:46.280]   labor practices at rockstar and kind of what they force workers to do to make something that big
[01:43:46.280 --> 01:43:52.520]   in that detail. So, you know, I love tech, but I want to hold all these companies to account and
[01:43:52.520 --> 01:43:57.000]   they're having more of more of an impact on our lives in ways that we can't even control.
[01:43:57.000 --> 01:44:00.760]   So we really have to like step up and make sure they're criticized properly.
[01:44:00.760 --> 01:44:03.800]   Well, you say that because you're a good person, but most people
[01:44:05.640 --> 01:44:11.800]   will say, oh, that's a bummer and still use the phone. You play the game.
[01:44:11.800 --> 01:44:16.520]   We can't stop them from using it. Like that's a thing. So the best we could do is like as
[01:44:16.520 --> 01:44:21.640]   maybe the educated users like kind of highlight these issues and bring it to tasks, you know,
[01:44:21.640 --> 01:44:24.040]   with governments with other in other ways as well.
[01:44:24.040 --> 01:44:26.600]   Interesting.
[01:44:26.600 --> 01:44:35.000]   Well, Dwight, if you ever get that extension that shows me in bikini pictures, I just want to warn
[01:44:35.000 --> 01:44:39.960]   you there are a few and they're not pretty. So,
[01:44:39.960 --> 01:44:41.960]   right. Be careful. I'm on it. Be careful.
[01:44:41.960 --> 01:44:48.520]   Our show today brought to you. I love this panel. We're going to have you all back. I've always,
[01:44:48.520 --> 01:44:51.800]   I mean, you're all great. I've always loved all three of you, but Nate, it's great to have
[01:44:51.800 --> 01:44:57.640]   you on again after all these years. Nate Langson is tech editor at European Tech Editor at Bloomberg
[01:44:57.640 --> 01:45:03.960]   and he also is consumer tech editor and is great to have him on. He's on Twitter at Nate LAN X
[01:45:03.960 --> 01:45:11.240]   ON. Devindra Hardewar, senior editor at Engadget at Devindra. If you want to congratulate him on
[01:45:11.240 --> 01:45:17.480]   his new member of his family and Dwight Silverman, you'll find him at the tech burger at the Houston
[01:45:17.480 --> 01:45:23.560]   Chronicle at D Silverman on the Twitter. Time for an audible ad. I don't get enough of these because
[01:45:23.560 --> 01:45:30.840]   I have so many audible recommendations. I could do audible ads all day. I love audible started using
[01:45:30.840 --> 01:45:37.720]   audible in the year 2000 almost 20 years ago because I had a long commute and I was going to kill somebody
[01:45:37.720 --> 01:45:42.200]   or myself at some point drive off the road by accident just because it was
[01:45:42.200 --> 01:45:48.040]   deathly boring two or three hours a day, maybe sometimes four. If traffic was really bad,
[01:45:48.040 --> 01:45:54.040]   doing nothing, listening to pop music, then I found audible. Now that time is well spent. In fact,
[01:45:54.040 --> 01:45:58.840]   I kind of miss my commute. I don't commute anymore, but I do listen to audible whenever I'm in the car,
[01:45:58.840 --> 01:46:04.600]   whenever I'm doing the dishes, walking the dog. I have the Amazon echoes set up so that they're
[01:46:04.600 --> 01:46:10.840]   all playing my latest book. I put them on Sonos now. You can listen on Sonos. It's great. In fact,
[01:46:10.840 --> 01:46:16.840]   I make the whole house play my audio book as I'm cleaning. It is so much fun. There's so many
[01:46:16.840 --> 01:46:21.960]   great books at audible. If you've heard me talk about it and you're thinking, I ought to get an
[01:46:21.960 --> 01:46:28.840]   audible book. We've got three months of audible, just 695 a month. If you go to audible.com/twit.
[01:46:28.840 --> 01:46:36.360]   Now that is a holiday deal or text to it to 500 500. Audible has the largest selection of
[01:46:36.360 --> 01:46:41.080]   audiobooks on the planet. Great stuff. And now the new audible originals. Have you guys done
[01:46:41.080 --> 01:46:48.120]   these audible originals? These are really awesome. Every month, there'll be a new set, six originals.
[01:46:48.120 --> 01:46:53.240]   I think it's six every month. We were talking about Broadway earlier that Carrie Mulligan,
[01:46:53.240 --> 01:46:57.960]   one woman show that's off Broadway Boys and Girls, very popular. You can get that in audible
[01:46:57.960 --> 01:47:03.400]   and listen to it. That's how I know about it. I love it. It was awesome. I'm listening to a
[01:47:03.400 --> 01:47:12.040]   Stephen Fry book right now part of my audible originals. He's taking Victorian personal journals
[01:47:12.600 --> 01:47:18.920]   and reading them. And man, those Victorians had quite a, it's called Victorian secrets.
[01:47:18.920 --> 01:47:24.520]   And it's fantastic. Stephen Fry is the perfect guy to narrate that. Also an audible original.
[01:47:24.520 --> 01:47:29.960]   Audible's recording so much material now that they're giving you great stuff. I just got this
[01:47:29.960 --> 01:47:35.800]   one too. It's called Have a Nice Day. It features anette-bending dick-cavit, Billy Crystal, Rachel
[01:47:35.800 --> 01:47:43.880]   Dratch, Christopher Jackson, Kevin Klein, a full cast. Is it a play? It's a live multi-cast script.
[01:47:43.880 --> 01:47:50.440]   It was a reading captured over two evenings in October at Audible's Monetal Lane Theatre in
[01:47:50.440 --> 01:47:55.960]   Greenwich Village. And you can listen to it. What fun? So add this to all the books that you're
[01:47:55.960 --> 01:48:03.480]   going to get. This is a great deal. I listen to Audible all the time, not only for escape,
[01:48:03.480 --> 01:48:08.040]   because I like a lot of science fiction, Audible and a lot of escapist novels and stuff like that.
[01:48:08.040 --> 01:48:14.920]   But also to learn. I just got the book that everybody in Silicon Valley is listening to
[01:48:14.920 --> 01:48:22.280]   from Yuval Noah Harari. His books, by the way, if you've not read any Harari, his books are all
[01:48:22.280 --> 01:48:29.800]   great. He's the one who wrote sapiens and Homo Deus. His newest is 21 Lessons for the 21st Century.
[01:48:29.800 --> 01:48:35.480]   People are talking about this in Silicon Valley like crazy. I mean, this is the stuff we're talking
[01:48:35.480 --> 01:48:42.280]   about. And Harari is a deep thinker. How do computers and robots change the meaning of being
[01:48:42.280 --> 01:48:48.120]   human? How do we deal with the epidemic of fake news? Are nations and religions still relevant?
[01:48:48.120 --> 01:48:56.120]   What should we teach our children? 21 Lessons for 21st Century, just one of many books on Audible.
[01:48:56.120 --> 01:49:02.120]   Learn, play, enjoy. Oh, there's a little Game of Thrones in there, some George RR Martin.
[01:49:02.120 --> 01:49:09.720]   These are the top 20 books right now. Sleep better. That's no surprise. That's a top 20 book.
[01:49:09.720 --> 01:49:18.680]   Go to audible.com/twit. Get three months of Audible just $6.95 a month or text "Twit" to 500-500
[01:49:18.680 --> 01:49:24.520]   to get a link. Exclusive audio fitness programs, audiobooks, Audible Originals. It's more than just
[01:49:24.520 --> 01:49:32.440]   books. It's the best kind of audio programming. And yes, I binge Audible. I admit it. In fact,
[01:49:32.440 --> 01:49:37.800]   sometimes Lisa and I, because we always have a book the two of us are listening to. So she has
[01:49:37.800 --> 01:49:42.520]   her subscription, I have mine. But there's always a book. And by the way, you can share your subscriptions,
[01:49:42.520 --> 01:49:49.000]   which we do. So if I go to my library, it includes books from her library and books from my library,
[01:49:49.000 --> 01:49:53.560]   which is awesome. But we like to have a book the two of us are listening to right now. It's
[01:49:53.560 --> 01:49:59.160]   Stephen King's the outsider. And often, instead, it will say, "Should we watch a show or,
[01:49:59.160 --> 01:50:03.960]   you know, let's just get in bed and listen to Audible?" And that is a really nice, cozy thing
[01:50:03.960 --> 01:50:09.160]   to do. We put a fire in the fireplace, pop some popcorn, and listen to a great book together.
[01:50:09.160 --> 01:50:18.680]   audible.com/twit. It's a good thing to do if you have a newborn as well. Put on the headphones
[01:50:18.680 --> 01:50:23.720]   while you're rocking Sophia in the middle of the night. I'm sorry to do all my podcasts now,
[01:50:23.720 --> 01:50:26.840]   but yeah, also it's modable. It's good for podcasts. Yeah.
[01:50:26.840 --> 01:50:36.200]   I like Audible. Let's see. Speaking of babysitters, Facebook and Twitter are cracking down on AI
[01:50:36.200 --> 01:50:44.360]   babysitter rating service. You didn't use this. Facebook and Twitter say the rating service
[01:50:46.120 --> 01:50:52.600]   called predict him analyzed babysitters online histories, including Facebook and Twitter
[01:50:52.600 --> 01:50:59.080]   offered ratings of whether they're at risk of drug abuse, bullying, or just having a bad attitude.
[01:50:59.080 --> 01:51:05.000]   This breaks the both social networks rules on user surveillance and data privacy,
[01:51:05.000 --> 01:51:12.760]   but I think is a brilliant use of data scraping. You know, that's better than a background check.
[01:51:12.760 --> 01:51:16.760]   This one has a bad attitude. We know because we saw her Facebook feed.
[01:51:16.760 --> 01:51:23.880]   So that reminds me of the story out of China, where China says it's going to start doing social
[01:51:23.880 --> 01:51:28.520]   ratings of all of its citizens. Yeah. And then that sounds like that's the same type of data
[01:51:28.520 --> 01:51:35.240]   scraping that they're using here only just for babysitters. Well, at least it's limited, right?
[01:51:35.240 --> 01:51:39.560]   That's right. That's right. Beijing is about to launch this social credit system.
[01:51:40.200 --> 01:51:45.960]   The capital of China has been in use in South China for some time. I think Shenzhen and others.
[01:51:45.960 --> 01:51:51.800]   And they had some stats. Let me see if I can find the story. They had some stats that just
[01:51:51.800 --> 01:51:55.400]   were mind-boggling about the number of people who had been kept from traveling,
[01:51:55.400 --> 01:52:01.080]   kept from buying stuff because they had a bad social credit score.
[01:52:01.080 --> 01:52:07.480]   Is this coming to America, you think? I wouldn't be surprised if some
[01:52:08.920 --> 01:52:12.040]   public relations companies don't do this with journalists.
[01:52:12.040 --> 01:52:18.920]   We have some kind of automated system that scrapes and looks at all of our social feeds.
[01:52:18.920 --> 01:52:22.760]   Well, you guys deserve to do flags with pride.
[01:52:22.760 --> 01:52:29.320]   Dvorak always swore that, and actually it's true that there was a black list of Microsoft and a
[01:52:29.320 --> 01:52:34.680]   black list. He says, I walked by an office of some of these at Microsoft and my name was on the list.
[01:52:34.680 --> 01:52:38.840]   And Apple's long been rumored to have some sort of punishment and reward system.
[01:52:38.840 --> 01:52:44.600]   Gizmodo for sure was on their list after the stolen iPhone. Although Gizmodo is going to those
[01:52:44.600 --> 01:52:52.040]   events now, I'm still not. I never stole an iPhone. I don't know why. What have I done?
[01:52:52.040 --> 01:52:58.760]   Glad to be on that list. According to the Beijing government, the city will have a black
[01:52:58.760 --> 01:53:06.120]   list that will be made available for market access, public services, travel, and employment.
[01:53:06.120 --> 01:53:11.400]   Residents with bad social credit scores will be limited in terms of movement and accessing
[01:53:11.400 --> 01:53:18.280]   services in the city. Those with good credit scores will have easy access to services in the city.
[01:53:18.280 --> 01:53:27.000]   Here's the Bloomberg story. This is going to go into place on 22 million citizens in Beijing by
[01:53:27.000 --> 01:53:31.160]   the end of 2020. They announced it. They posted the plan online.
[01:53:31.160 --> 01:53:40.120]   The problem is that there's probably a good group of people who say that's good.
[01:53:40.120 --> 01:53:42.600]   Everybody will behave better in public. That's a good thing.
[01:53:42.600 --> 01:53:48.760]   I imagine there are a few people in the United States who wouldn't mind that either.
[01:53:48.760 --> 01:53:50.840]   And it's certainly a protection unless it's a good thing.
[01:53:50.840 --> 01:53:54.360]   What's that black mirror episode?
[01:53:54.360 --> 01:53:58.200]   Yes. I forget the name of it. Right. Right. Right. It was in the third season.
[01:53:58.200 --> 01:54:04.680]   Your score would go up and down. So Ryan Gallagher added again in the intercept.
[01:54:04.680 --> 01:54:11.000]   He's got great sources inside of Google Project Dragonfly. That was the project to build
[01:54:11.000 --> 01:54:16.920]   a search engine for China, a Google branded search engine that would censor categories of
[01:54:16.920 --> 01:54:24.680]   information about human rights, democracy, peaceful protest. My guess is his sources,
[01:54:24.680 --> 01:54:31.000]   Janatan Zungar, who was at Google for many years, 14 years left, but has been very outspoken about
[01:54:31.000 --> 01:54:37.800]   this. He's quoted as saying he was president some of the early meetings about Dragonfly.
[01:54:37.800 --> 01:54:42.600]   He says he pointed out to executives managing the product. Chinese people could be at risk of
[01:54:42.600 --> 01:54:47.960]   interrogation or detention if they were found to have used Google to seek out information banned
[01:54:47.960 --> 01:54:54.120]   by the government. Scott Beaumont, head of operations in China and one of the key architects
[01:54:54.120 --> 01:55:00.120]   at Dragonfly, quote, "did not view Zungar's concern as significant enough to merit a change of course,"
[01:55:00.120 --> 01:55:05.960]   according to four people who worked on the project. "They shut out members of the company's security
[01:55:05.960 --> 01:55:12.360]   and privacy team from meetings about Dragonfly and tried to sideline a privacy review of the plan
[01:55:12.360 --> 01:55:18.440]   that sought to address potential human rights abuses." Zungar is the only one of the four who
[01:55:18.440 --> 01:55:26.040]   spoke on the record, but this is credible. It's a little disappointing.
[01:55:26.040 --> 01:55:33.720]   Even American companies can be complicit in this scheme, this social credit scheme.
[01:55:38.920 --> 01:55:44.440]   Apple was in the Supreme Court this week. We talked a little bit about this throughout the week.
[01:55:44.440 --> 01:55:50.840]   It is a kind of an interesting suit. The Supreme Court justices are going to only be ruling on
[01:55:50.840 --> 01:55:57.880]   standing. The lawsuit, a class action lawsuit, accuses Apple of breaking federal antitrust laws
[01:55:57.880 --> 01:56:04.920]   by monopolizing the market for iPhone software applications, which causes customers to overpay.
[01:56:04.920 --> 01:56:15.240]   Apple says, "No, no, no. We're just an agent. We don't actually sell direct to the customers,
[01:56:15.240 --> 01:56:23.240]   the companies that make the software do. So there's no standing to sue us. Go sue that company,
[01:56:23.240 --> 01:56:29.160]   nah, us. The Supreme Court will have to decide whether the lawsuit can go ahead. So they're not
[01:56:29.160 --> 01:56:34.040]   deciding on whether Apple is a monopoly, but merely on whether the lawsuit can go ahead.
[01:56:34.040 --> 01:56:43.960]   It's all based on a 1977 precedent called Illinois Brick. I don't know if I want to go on and on about
[01:56:43.960 --> 01:56:51.320]   this. I've become an expert in all of this over the last few days. Neil Gorsuch wondered whether
[01:56:51.320 --> 01:56:56.360]   Illinois Brick was still valid in a modern marketplace. Justice Roberts said he agreed with
[01:56:56.360 --> 01:57:01.320]   Apple's position. He expressed concern that for a single price increase, Apple could be held
[01:57:01.320 --> 01:57:08.040]   liable, but both consumers and app developers thought that wasn't right. I think Gorsuch nails
[01:57:08.040 --> 01:57:17.320]   it on the head. I think that this is a different beast than what Illinois Brick was. Illinois Brick
[01:57:17.320 --> 01:57:24.680]   was a suit that Illinois, the state of Illinois, brought against a consortium of brick manufacturers
[01:57:24.680 --> 01:57:30.440]   saying that they had colluded to keep prices high. But the Supreme Court said, "Well, you don't have
[01:57:30.440 --> 01:57:36.280]   standing state of Illinois because you're not buying the bricks." And through the case out,
[01:57:36.280 --> 01:57:45.240]   and that's been precedent for some time, ever since. Apple doesn't set the individual price for each
[01:57:45.240 --> 01:57:53.640]   app, Apple has tears, and the developer picks the tear. So to say that Apple keeps prices high is
[01:57:53.640 --> 01:57:58.440]   not necessarily true because the developer could charge 99 cents and the developer could charge
[01:57:58.440 --> 01:58:04.280]   $8.99 or whatever they want to charge. But it just has to be at that tear and step.
[01:58:04.280 --> 01:58:09.880]   And that to me, that's not an option. The class action suit asserted that prices were
[01:58:09.880 --> 01:58:12.840]   high because developers would pass along Apple's 30% cut.
[01:58:12.840 --> 01:58:20.920]   This seems strange rather than complaining that Apple is selling its own products on the
[01:58:20.920 --> 01:58:26.600]   marketplace, and their pricing determines maybe what the competitors would do.
[01:58:26.600 --> 01:58:30.040]   I feel like that's a little more valid. This one seems confusing.
[01:58:30.040 --> 01:58:37.880]   And Ben Thompson, as usual, a great article on Stratecary, why Apple is right.
[01:58:37.880 --> 01:58:47.480]   In this case, as you said too, Dwight. However, there is a good point to be made about Apple's
[01:58:47.480 --> 01:58:51.160]   App Store monopoly. It is in fact the only way you could put an app on iOS.
[01:58:51.160 --> 01:59:00.040]   Apple's contention is this keeps iOS secure. But Ben says Apple is an abusive monopoly.
[01:59:00.040 --> 01:59:05.400]   And I'll tell you where it really hurts is companies like Netflix and Amazon
[01:59:05.400 --> 01:59:14.680]   or even Audible who on Android, you could buy an Audible book through the Audible app on the
[01:59:14.680 --> 01:59:20.520]   Kindle. You can buy a Kindle book through the Kindle app. They don't do it on Apple's App Store,
[01:59:20.520 --> 01:59:24.680]   not because Apple forbids it, but because Apple would take 30% of the transaction even though they
[01:59:24.680 --> 01:59:29.960]   add no value. That's Amazon's choice. That's Amazon's choice to make the consumer experience worse.
[01:59:29.960 --> 01:59:36.200]   So they want to make 30% more. They want to make as much money as they can. But it's weird to me
[01:59:36.200 --> 01:59:40.360]   that Amazon is a company, their first reaction to all this. The same thing happened to Comixology.
[01:59:41.000 --> 01:59:44.680]   When that thing even had an app, there's not even an app anymore for that. But yeah,
[01:59:44.680 --> 01:59:49.160]   you couldn't buy those digital comics through through iOS. And now there's not even an app.
[01:59:49.160 --> 01:59:53.240]   It's all through the web. There's an argument that's made there.
[01:59:53.240 --> 01:59:57.880]   The difference is so Fortnite is a good example because Epic Games had to sell it through the
[01:59:57.880 --> 02:00:05.480]   App Store on iOS. And as a result, Apple made millions of dollars on in-app purchases because
[02:00:05.480 --> 02:00:12.040]   the game is free. But all of those in-app purchases went to Apple made literally,
[02:00:12.040 --> 02:00:16.600]   I can't remember what it was, $115 million in three months for Fortnite. That means Apple
[02:00:16.600 --> 02:00:23.880]   made more than $30 million. So when Epic came to Android, they elect it because they don't have
[02:00:23.880 --> 02:00:28.600]   to sell through the Play Store. They elect it to offer it as a direct download. But in order for
[02:00:28.600 --> 02:00:34.600]   consumers to get it, they have to turn off a setting and settings, download it, turn it back on for
[02:00:34.600 --> 02:00:41.480]   their protection. Fortnite makes more money that way. You could argue that's a worse experience for
[02:00:41.480 --> 02:00:46.440]   consumers. Yeah, that's more friction. But like Fortnite, by the time Fortnite hit Android,
[02:00:46.440 --> 02:00:50.760]   it was enough of a phenomenon that people will jump through fire to get that game on their phone.
[02:00:50.760 --> 02:00:55.640]   So it kind of works out there. But we were just talking about, yeah, I mentioned the Microsoft
[02:00:55.640 --> 02:01:00.840]   antitrust thing. What if Microsoft was locking off apps that were running on Windows in the 90s
[02:01:00.840 --> 02:01:04.920]   and saying, we're the sole provider, we're only weak in what's coming on there?
[02:01:04.920 --> 02:01:10.360]   I guess our definition of how we determine monopoly and how we look at these tech companies
[02:01:10.360 --> 02:01:15.800]   has changed a lot in two decades. Well, it's interesting because Microsoft does offer a version
[02:01:15.800 --> 02:01:22.600]   of Windows that you can only buy, get stuff, suffer from the Play Store, from the Microsoft
[02:01:22.600 --> 02:01:28.440]   Store. It's a mode of Windows 10s. It's a mode of Windows 10s. That's why they call it a mode now,
[02:01:28.440 --> 02:01:34.360]   not a version. It's indistinguishable. That's a distinction with that difference. But
[02:01:34.360 --> 02:01:38.760]   we're a difference with that distinction. But I do note that everybody who buys a Windows 10s
[02:01:38.760 --> 02:01:44.920]   computer immediately turns Windows 10s off so they can download Chrome. Otherwise, there's no Chrome.
[02:01:44.920 --> 02:01:50.360]   But I think a lot of that is also for education because it's a lot easier for schools to deploy
[02:01:50.360 --> 02:01:56.360]   those machines knowing that they are out of the box, perhaps either slightly more secure or
[02:01:56.360 --> 02:02:01.160]   it's just less distracting for kids. I think that might be part of that reason.
[02:02:01.160 --> 02:02:05.720]   That kicked off with the Surface laptop. I think that was like they were going to make that a
[02:02:05.720 --> 02:02:09.880]   big school push. And then they realized it's a terrible idea because yes, everybody was turning
[02:02:09.880 --> 02:02:16.040]   on going back to Windows 10 Pro. So now 10s is just a thing that's on on some devices and you
[02:02:16.040 --> 02:02:22.040]   could flip it to Pro easily if you need to. Even today, if Microsoft had made a Windows machine
[02:02:22.040 --> 02:02:27.160]   that only supported 10s, I think there'd be some squawking. I think that was called the Surface
[02:02:27.160 --> 02:02:31.720]   RT. Look how well that is. Oh, yeah. Yeah, right. Yeah, one of those. Somewhere.
[02:02:31.720 --> 02:02:41.320]   The Mac App Store allows you're able to download things on the Mac, both from the web as well as
[02:02:41.320 --> 02:02:48.600]   there. But you notice in the newer versions of Mac OS, when you download something from the web,
[02:02:48.600 --> 02:02:53.480]   it basically says, you downloaded something from the web. Do you really trust this developer?
[02:02:53.480 --> 02:02:58.360]   Yeah. And if the developer isn't on the approved list, you have to go into the security settings
[02:02:58.360 --> 02:03:03.000]   and basically say, yeah, even though these guys, you don't know anything about them,
[02:03:03.000 --> 02:03:10.440]   I'm going to go ahead and approve this. So they make you jump for it on the Mac, even though
[02:03:10.440 --> 02:03:16.200]   you can get it if you can get almost anything you want on the Mac App Store, but the prices
[02:03:16.200 --> 02:03:25.400]   there are kind of interesting too. Yeah. I feel like if you did this on desktop,
[02:03:25.400 --> 02:03:29.160]   computing people would be very upset. But something about mobile computing, it doesn't bother people.
[02:03:29.160 --> 02:03:34.920]   Well, I think the argument that it's secure is a good one. And I think you see what goes on with
[02:03:34.920 --> 02:03:41.720]   Android is kind of the proof of that. And you've got things on mobile that you don't typically
[02:03:41.720 --> 02:03:47.960]   have in the same way on desktop and chiefly with things like being able to send an SMS
[02:03:47.960 --> 02:03:54.600]   in order to pay for something. You have your card details on the device in some form. There's a
[02:03:54.600 --> 02:03:59.640]   lot of stuff that historically hasn't been as integrated on desktop as it has been on mobile.
[02:03:59.640 --> 02:04:06.360]   And I think a lot of it goes back to an earlier conversation we were having around the scammers
[02:04:06.360 --> 02:04:12.120]   and people calling up. That's something that just doesn't really happen on mobile devices.
[02:04:12.120 --> 02:04:17.720]   You don't get people calling up saying, "Hey, your phone's infected. You must call this number and
[02:04:17.720 --> 02:04:23.800]   give us your details." Give them time. Well, there's a sim part hijacking thing now. The sim hijacking
[02:04:23.800 --> 02:04:26.440]   is the other part of that. Oh, yeah, that's awful. That's awful.
[02:04:27.160 --> 02:04:33.960]   I think that's a good point. The people understand that this smartphone is uniquely vulnerable
[02:04:33.960 --> 02:04:39.800]   and contains a lot of really valuable information. So they're willing to put up with some lack of
[02:04:39.800 --> 02:04:43.720]   convenience for better security. Well, we also came out of the environment where you couldn't do
[02:04:43.720 --> 02:04:50.200]   anything on your phone. The old Nokia, the app's were terrible. Everything was terrible.
[02:04:50.200 --> 02:04:54.120]   So the fact we can do anything at all is amazing. We're grateful for anything.
[02:04:55.160 --> 02:04:58.840]   I did worry when my Apple first came out with Gatekeeper that technology you were talking about
[02:04:58.840 --> 02:05:02.760]   to vendor that they would lock the Mac down that at some point they would do exactly the same thing.
[02:05:02.760 --> 02:05:06.440]   And they may still, who knows, they are slowly working that way.
[02:05:06.440 --> 02:05:12.280]   Yeah, iPad Pro is kind of getting Mac-ish and now the new MacBook Air looks fundamentally
[02:05:12.280 --> 02:05:17.560]   useless. I don't know, is the computer? So are they trying to like, it seems like they want iOS
[02:05:17.560 --> 02:05:20.840]   to be more and more of their bigger platform moving forward?
[02:05:20.840 --> 02:05:26.040]   I bought the new iPad Pro the 11-inch and as much as I love it, it's just,
[02:05:26.040 --> 02:05:34.920]   I'm trying to think of a useful analogy here. But if you watch someone who's got a really,
[02:05:34.920 --> 02:05:42.120]   I don't know, some problem with their face and you watch it in a blurry image,
[02:05:42.120 --> 02:05:47.240]   it's not so noticeable and then you blow that up into 4K and it's like, if you're that person,
[02:05:47.240 --> 02:05:51.880]   you're like, "Oh, man, don't watch that. I look terrible and not put your... I sort of feel that
[02:05:51.880 --> 02:05:58.200]   that's iOS right now is that the iPad Pro is so capable, is so powerful. There's so much,
[02:05:58.200 --> 02:06:04.520]   like almost perfect and iOS is the single thing that's holding it back. That's in file access to
[02:06:04.520 --> 02:06:08.360]   USB devices with, through USB-C, but that's another story for another day.
[02:06:08.360 --> 02:06:13.240]   That's the universal review of the iPad Pro and I love it. I have a 12.9-inch,
[02:06:13.240 --> 02:06:16.200]   is that it's tomorrow's hardware running yesterday's software.
[02:06:16.200 --> 02:06:22.200]   Yeah, definitely. My view is definitely not unique, but I agree with every review I've seen that's
[02:06:22.200 --> 02:06:29.240]   highlighted that. Too bad, because that is the most amazing computer. I mean, it's ready for
[02:06:29.240 --> 02:06:33.480]   something. I don't know what, but Google's done kind of the same thing, the Pixelbook and now
[02:06:33.480 --> 02:06:39.480]   the Pixel Slate, the same exact problem where you have amazing hardware running buggy,
[02:06:40.440 --> 02:06:45.720]   inappropriate software. Google has been failing at making computers for a while, I'd say,
[02:06:45.720 --> 02:06:49.640]   like I think the Chromebook thing, like cheap Chromebooks, that's great. That's a great idea.
[02:06:49.640 --> 02:06:53.880]   The Pixel, like when they launched that, it was never a fan and yeah, the Pixelbook now seems like
[02:06:53.880 --> 02:06:59.240]   a nice Chromebook. I don't know why it's so expensive. I'm just sad to see Apple kind of
[02:06:59.240 --> 02:07:05.320]   following on a similar path. Meanwhile, Microsoft is releasing good mobile hardware
[02:07:05.320 --> 02:07:10.360]   and I'm totally baffled as to this reality. We did see that Microsoft
[02:07:10.360 --> 02:07:16.040]   is now worth more than Apple. Partly because Microsoft's gone up mostly because Apple's gone
[02:07:16.040 --> 02:07:21.080]   down a lot. They're now down below $850 million market worth. What's really cool though,
[02:07:21.080 --> 02:07:27.320]   is this look at the smiling such in a dollar, the new CEO of Microsoft,
[02:07:27.320 --> 02:07:34.760]   its market value has tripled under Nutella. He wasn't smiling. Yeah. He did all,
[02:07:36.440 --> 02:07:40.280]   there haven't been that many cases where a tech company reinvented itself
[02:07:40.280 --> 02:07:47.400]   and successfully navigated these very challenging waters, the innovators dilemma and all that.
[02:07:47.400 --> 02:07:51.480]   I don't even know if I can think of an example. There's far more examples like
[02:07:51.480 --> 02:07:58.120]   Apple in the 90s. Apple. Yeah, exactly. And IBM had a kind of a brief flourishing
[02:07:58.120 --> 02:08:04.920]   in the 2000s after it kind of it refound itself but then didn't keep up after that.
[02:08:04.920 --> 02:08:09.240]   So there've been a couple of them like that. But one of the interesting things about Microsoft
[02:08:09.240 --> 02:08:17.080]   is essentially it kind of went back to its roots as a business company. It pulled back a little
[02:08:17.080 --> 02:08:25.400]   except for things like Xbox. It pulled back from moving in such a consumer centric direction
[02:08:25.400 --> 02:08:31.000]   that it had and it really looked at what businesses wanted and began to provide that. Although I
[02:08:31.000 --> 02:08:37.800]   think the roadmap had previously been had previously been paved by Amazon with its
[02:08:37.800 --> 02:08:44.040]   cloud services. Giving up on phones almost immediately was probably the smartest thing
[02:08:44.040 --> 02:08:51.400]   you know, Nutella did. And honestly, I'm surprised Microsoft kept going with the Surface lineup
[02:08:51.400 --> 02:08:55.720]   because those first couple devices were not great. They were so terrible. I think one line
[02:08:55.720 --> 02:08:59.640]   from my Surface RT review was, you know, I wanted to throw this thing out of the window.
[02:09:00.840 --> 02:09:05.320]   But now I look at everything they're doing, at least their hardware design. Leo, you're using
[02:09:05.320 --> 02:09:09.720]   the Surface Studio. Like these are the things we kind of expected from Apple a decade ago.
[02:09:09.720 --> 02:09:13.720]   So just so weird. So we're so such a strange world right now.
[02:09:13.720 --> 02:09:20.520]   Good. If it were too predictable, it wouldn't be fun to miss job. All right. I left the biggest
[02:09:20.520 --> 02:09:25.560]   story of the year, maybe the decade for last. So we're going to take a break and then I will
[02:09:25.560 --> 02:09:31.720]   give you that in a moment in a few bits and pieces left over. But first a word from our sponsor
[02:09:31.720 --> 02:09:37.960]   Rocket Mortgage. If it's time to buy a new home, unless you are, I almost said Nelson
[02:09:37.960 --> 02:09:43.160]   Rockefeller, which would have dated me. Unless you are Bill Gates, well, even that dates me a
[02:09:43.160 --> 02:09:47.160]   little bit. Unless you're PewDiePie, you're getting alone, right? You're going to be,
[02:09:47.160 --> 02:09:51.880]   you're going to be going out. So that's the hip, the hip new one. You're going to go out,
[02:09:51.880 --> 02:09:55.880]   you're going to get alone. And if you're going to get alone, don't go to the big bank. Believe me,
[02:09:55.880 --> 02:09:59.880]   we did that five years ago. That was a nightmare. First of all, you've got to go to the big bank,
[02:09:59.880 --> 02:10:05.160]   got to dress up, fill out a long application, and then your homework, your troubles begin.
[02:10:05.160 --> 02:10:10.920]   You've got to find pay stubs from previous jobs, bank statements. You've got to find a fax machine
[02:10:10.920 --> 02:10:17.160]   and fax that to them. And it takes a long, long time. Let's get in the 21st century with
[02:10:17.160 --> 02:10:21.640]   quick and loads. The number one mortgage lender in the country, they've been a number one now
[02:10:21.640 --> 02:10:26.520]   in volume for a year. But that's because for nine years, they've been number one in customer
[02:10:26.520 --> 02:10:33.800]   satisfaction, according to JD Power. Nine years when you focus on customers, eventually you become
[02:10:33.800 --> 02:10:37.800]   the biggest, the best. That one of the ways they did this is what Rocket Mortgage, we've talked about
[02:10:37.800 --> 02:10:43.320]   this before, an entirely online mortgage approval process that takes minutes, not days and requires
[02:10:43.320 --> 02:10:47.960]   a trip to nowhere. You could actually do it in an open house on your smartphone. It's an entirely
[02:10:47.960 --> 02:10:53.400]   online process. Step one, you're answer a few simple questions, no big, long application. They
[02:10:53.400 --> 02:10:58.520]   check your credit and you get pre-qualified approval. They tell you what loans you qualify for.
[02:10:58.520 --> 02:11:02.280]   They've got great rates. You choose the rate, you choose the term, you choose the down payment,
[02:11:02.280 --> 02:11:08.360]   and you're in. That's the part one of the power buying process. But then the next part starts.
[02:11:08.360 --> 02:11:12.520]   Over the next 24 hours, in most cases without any intervention on your part, just kind of
[02:11:12.520 --> 02:11:17.640]   automatically, they'll verify income assets and credit and they'll give you a verified approval.
[02:11:17.640 --> 02:11:22.920]   Now, that is big. That means you have the strength of a cash buyer. You're good for the money. When
[02:11:22.920 --> 02:11:27.400]   you go to make an offer in a house, you've got a letter that says, "Yep, we approve them. They've
[02:11:27.400 --> 02:11:33.080]   got the loan. There's no contingency." That's huge, especially. I don't know what it's like in
[02:11:33.080 --> 02:11:37.800]   other parts of the country, but here in the California area, when you're putting an offer on a house,
[02:11:37.800 --> 02:11:42.120]   there are five other people bidden for that house too. And if there's somebody with cash,
[02:11:42.120 --> 02:11:46.760]   they're going to go way ahead of you unless you've got this letter that says, "There's no contingency.
[02:11:46.760 --> 02:11:51.320]   They got the loan." And once your verified step three kicks in, and this is fantastic,
[02:11:51.320 --> 02:11:56.200]   because interest rates have started to tick up and that adds some anxiety. If an interest rate goes up,
[02:11:56.200 --> 02:12:01.000]   while you're shopping, that house could cost tens of thousands of dollars more, unless
[02:12:01.000 --> 02:12:07.400]   unless you're using rocket mortgage, then you've got rate shield approval. All new,
[02:12:07.400 --> 02:12:11.640]   exclusive, they lock up your rate for up to 90 days while you shop. Once you get that
[02:12:11.640 --> 02:12:17.880]   verified approval, you qualify. If rates go up, your rate does not. It stays the same. If rates go
[02:12:17.880 --> 02:12:23.800]   down, your rate drops, so that's nice too. Either way you win. It's kind of like exactly what you'd
[02:12:23.800 --> 02:12:30.120]   expect from a customer-focused lender, America's number one mortgage lender, rocket mortgage and
[02:12:30.120 --> 02:12:34.200]   quick and loans. Get started today. Go to rocketmortgage.com/twit,
[02:12:34.200 --> 02:12:40.280]   to rocketmortgage.com/twit, and the number two, rate shield approval is only valid on certain 30-year
[02:12:40.280 --> 02:12:43.560]   purchase transactions. Additional conditions or exclusions may apply.
[02:12:43.560 --> 02:12:47.880]   Based on quick and loans data and comparison, the public data records equal housing lender
[02:12:47.880 --> 02:12:56.920]   licensed in all 50 states and MLS consumer access.org number 30-30. Just remember this, rocketmortgage.com/twit,
[02:12:56.920 --> 02:13:03.560]   to when we thank rocket mortgage. They've been a great sponsor all year long. Just really appreciate
[02:13:03.560 --> 02:13:11.720]   their support. MIT Media or I'm sorry, MIT Technology Review had the story. There's now maybe been some
[02:13:11.720 --> 02:13:17.960]   doubt cast on it. But I think if this is true, this is the story of the decade. The first
[02:13:17.960 --> 02:13:31.080]   example of CRISPR being used to edit in vitro and embryo, the gene they removed was CCR5. The
[02:13:31.080 --> 02:13:39.960]   idea was to give the baby resistance to HIV, smallpox and cholera. And according to the doctor who
[02:13:39.960 --> 02:13:50.200]   led this team, Hijiang Kui, two baby twin girls were born with this edited gene data. If this is
[02:13:50.200 --> 02:13:54.280]   true, this is a watershed moment, I think, in human history.
[02:13:57.560 --> 02:14:02.040]   Devendra, you agree? You just had a baby. Would you, if you could, would you have edited that gene out?
[02:14:02.040 --> 02:14:10.200]   It depends. Would I have done it today? Probably not. Because we don't know much about this
[02:14:10.200 --> 02:14:17.560]   technology, right? We're using CRISPR kind of like a sledgehammer in certain ways. We don't know how
[02:14:17.560 --> 02:14:25.320]   various bits of DNA interact. There's a lot we just don't know. In 20 or 30 years, when we kind of
[02:14:26.440 --> 02:14:31.720]   know exactly how everything's going, maybe. Your granddaughter, perhaps.
[02:14:31.720 --> 02:14:37.320]   I should point out illegal in every country of the world. China, no, you're not because it's a
[02:14:37.320 --> 02:14:45.800]   it's not a law that forbids it. It's a administrative recommendation. So it's unclear whether it has the
[02:14:45.800 --> 02:14:54.600]   effect of law. And there is also has has roiled the research community there. One of the people who
[02:14:54.600 --> 02:15:02.920]   was who was providing assistance, essentially consulting with the team was out of Rice University
[02:15:02.920 --> 02:15:06.600]   here in Houston. Oh, I know that. So yeah, yeah. So yeah, it's right.
[02:15:06.600 --> 02:15:13.000]   Into that as well. And so, you know, there was a lot of input into this process. And
[02:15:13.000 --> 02:15:18.520]   I think that there's going to be a lot of soul searching in research and the research community
[02:15:18.520 --> 02:15:27.640]   as a result of this. And when this if this was all presented at a big conference, genetics
[02:15:27.640 --> 02:15:34.200]   conference this week, and at the end of the conference, the organizers said, under no, no,
[02:15:34.200 --> 02:15:41.320]   no civilized nation thinks this is a good idea. If this did happen, it's probably illegal and
[02:15:41.320 --> 02:15:48.600]   it's certainly unethical. I don't know about that. But I, you know, the the the doctor, Dr.
[02:15:48.600 --> 02:15:55.480]   Her posted a video to YouTube at 2017 describing preliminary experience experiments on mice,
[02:15:55.480 --> 02:16:01.320]   monkeys, and more than 300 human embryos. The concern the concern you have to vendor and others
[02:16:01.320 --> 02:16:07.320]   have is that CRISPR isn't perfect in that it can introduce perhaps accidental mutations.
[02:16:07.960 --> 02:16:13.560]   Remember when we first cloned a sheep dolly, she died in a very quick, very like a couple of years,
[02:16:13.560 --> 02:16:21.160]   because it turns out the cloning didn't didn't reset the clock. And so she was old before when
[02:16:21.160 --> 02:16:28.760]   she was born. Ho says in our tests, few or no unwanted changes in the test embryos.
[02:16:28.760 --> 02:16:31.560]   So they went ahead and did it.
[02:16:34.680 --> 02:16:39.000]   He says, we got to be slow and cautious. Even a single case of failure could kill off the entire
[02:16:39.000 --> 02:16:45.080]   field. I don't think so. I think the gene editing is going to happen. It may be illegal. It may be
[02:16:45.080 --> 02:16:49.720]   black market, but there's people who will pay to have smarter children, faster children,
[02:16:49.720 --> 02:16:53.320]   healthier children, better looking children.
[02:16:53.320 --> 02:16:58.760]   We have seen fields like this kind of take a freeze. I think that happened to AI in the 80s.
[02:16:58.760 --> 02:17:05.640]   And people talk about that quite a bit too. So if something goes wrong, it could stall research.
[02:17:05.640 --> 02:17:10.840]   It could slow things down. And that's not necessarily something I want either.
[02:17:10.840 --> 02:17:15.720]   I just want us to be really careful about this because, yeah, we're going to build
[02:17:15.720 --> 02:17:19.320]   the world of designer babies. We're already going to have to deal with the fact that it's
[02:17:19.320 --> 02:17:23.160]   mainly going to be rich in affluent people who are going to be able to afford this and create a whole
[02:17:23.160 --> 02:17:29.880]   new generation of humans who have a clear advantage of everybody else. There are going to be so many
[02:17:29.880 --> 02:17:33.720]   other problems we're going to have to deal with first, aside from getting the DNA wrong.
[02:17:33.720 --> 02:17:40.360]   Still, I think it's historic if it's true, if you really did it.
[02:17:40.360 --> 02:17:44.040]   It's true. I don't see an evidence that it's not true.
[02:17:44.040 --> 02:17:51.640]   Wow. I wish it were something we could celebrate in a way because the unethical marker comes where,
[02:17:51.640 --> 02:17:56.200]   yeah, we don't know what's going to happen to these kids in five or 10 or 20 years.
[02:17:56.200 --> 02:18:03.480]   And is it ethical for her parents to decide that? Is it ethical for the doctor to kind of be
[02:18:03.480 --> 02:18:07.640]   involved in doing something like this? That's the big question.
[02:18:07.640 --> 02:18:14.920]   We should mention that on Thursday, the Chinese government's halted work by this team saying,
[02:18:14.920 --> 02:18:20.200]   we don't support this. His experiment, according to the Chinese government,
[02:18:20.200 --> 02:18:25.160]   crossed the line of morality and ethics adhered to by the academic community and was shocking
[02:18:25.160 --> 02:18:32.120]   and unacceptable. That's from Xu Nanping, the Vice Minister of Science and Technology in China.
[02:18:32.120 --> 02:18:39.240]   We have to be careful because we're dealing with so many things now that could,
[02:18:39.240 --> 02:18:47.640]   I don't know, add an extreme wipe out humanity. A crazy gene mutation could lead to who knows what.
[02:18:48.760 --> 02:18:53.480]   As we dabble in these world-ending things and civilization-ending things, we should probably
[02:18:53.480 --> 02:18:56.360]   take a breath and just make sure we know what we're doing.
[02:18:56.360 --> 02:18:58.840]   Oh, come on. Full speed ahead.
[02:18:58.840 --> 02:18:59.400]   Yeah.
[02:18:59.400 --> 02:19:00.520]   Oh, there's a great view.
[02:19:00.520 --> 02:19:02.680]   I've seen what could possibly go wrong.
[02:19:02.680 --> 02:19:04.600]   I've seen Godzilla. I know what could happen.
[02:19:04.600 --> 02:19:07.160]   Oh, that's what nuclear is.
[02:19:07.160 --> 02:19:12.760]   Well, I have one put on this, which is, we talk a lot, and Elon Musk talks a lot about
[02:19:12.760 --> 02:19:18.520]   going to Mars and the late Stephen Hawking talked about how the future of humanity would
[02:19:18.520 --> 02:19:24.760]   be in the hands of people who were able to leave Earth and colonize other planets and solar systems.
[02:19:24.760 --> 02:19:32.120]   My view is that if this can be perfected, maybe there's a way to breed,
[02:19:32.120 --> 02:19:37.720]   say, breed. It sounds horrible, doesn't it? But for children to be born in a way that
[02:19:37.720 --> 02:19:42.520]   genetically makes them more suitable for life outside of Earth.
[02:19:42.520 --> 02:19:46.520]   And that's the side of this that I find extremely exciting. We'll never get to see it in our lifetimes,
[02:19:46.520 --> 02:19:53.400]   I don't think. But that, I imagine, I could see happening in, you know, a hundred, hundred
[02:19:53.400 --> 02:19:54.520]   fifty years time.
[02:19:54.520 --> 02:19:56.600]   That's a dystopian novel, right?
[02:19:56.600 --> 02:20:01.160]   That's the Expanse. I think that's written quite well into the world at the Expanse,
[02:20:01.160 --> 02:20:03.400]   which is a fantastic book and TV series.
[02:20:03.400 --> 02:20:03.880]   Yeah.
[02:20:03.880 --> 02:20:04.520]   Right.
[02:20:04.520 --> 02:20:11.800]   Well, you said it, and before the show began, that you had a child with some nervousness
[02:20:11.800 --> 02:20:14.920]   about the world that Sophia was going to be born into.
[02:20:14.920 --> 02:20:16.920]   My kids are growing.
[02:20:16.920 --> 02:20:21.640]   My kids are growing, but I also feel like, and maybe that's how every generation feels about
[02:20:21.640 --> 02:20:29.160]   the future. But I do feel like we're striding full speed ahead into a very different world.
[02:20:29.160 --> 02:20:37.400]   I would say it's particularly terrifying just because you see the state of politics and the
[02:20:37.400 --> 02:20:41.560]   state of like social movements today, the rise of fascism and everything.
[02:20:42.200 --> 02:20:47.560]   But I've been listening to a great podcast called The End of the World podcast from
[02:20:47.560 --> 02:20:49.000]   The Stuff You Should Know, folks.
[02:20:49.000 --> 02:20:53.080]   And it is all about these existential threats that we're going through.
[02:20:53.080 --> 02:20:57.480]   And I think the key takeaway is we're living at a time where multiple things we're doing
[02:20:57.480 --> 02:21:00.680]   could be the end of humanity.
[02:21:00.680 --> 02:21:06.200]   You know, it could be, it could be gene editing, it could be AI, it could be who knows what?
[02:21:06.200 --> 02:21:10.520]   Climate change is also something we're dealing with and not stopping.
[02:21:10.520 --> 02:21:12.520]   So it's like all these threats.
[02:21:12.520 --> 02:21:13.160]   Nuclear war still out there.
[02:21:13.160 --> 02:21:14.280]   Nuclear war still a thing.
[02:21:14.280 --> 02:21:15.320]   Still out there.
[02:21:15.320 --> 02:21:20.760]   Yeah, we all grew up with the threat of nuclear war, of global annihilation under a
[02:21:20.760 --> 02:21:24.920]   Soviet Union that was armed to the teeth in the United States, it was armed to the teeth.
[02:21:24.920 --> 02:21:29.720]   And could at any moment, you know, end the world, we grew up under that.
[02:21:29.720 --> 02:21:33.080]   But now there's multiple possibilities.
[02:21:33.080 --> 02:21:34.440]   It's coming from every direction.
[02:21:34.440 --> 02:21:34.760]   Great.
[02:21:34.760 --> 02:21:37.880]   I'm actually less worried about the end of mankind.
[02:21:37.880 --> 02:21:40.360]   If that's what happens, probably it's a good thing.
[02:21:40.360 --> 02:21:45.480]   But I am more worried about creating a world that is just inhospitable inhumane,
[02:21:45.480 --> 02:21:49.080]   grim, gloomy, depressing, dystopian.
[02:21:49.080 --> 02:21:52.440]   And then our kids and our grandkids will grow up in that.
[02:21:52.440 --> 02:21:53.480]   That's really sad.
[02:21:53.480 --> 02:21:58.040]   That's a fate worse than annihilation.
[02:21:58.040 --> 02:22:02.280]   I can throw a little, I'm going to throw a little positive light on this.
[02:22:02.280 --> 02:22:03.480]   Oh, please do.
[02:22:03.480 --> 02:22:03.800]   Please.
[02:22:03.800 --> 02:22:06.040]   You got me so they suck.
[02:22:06.040 --> 02:22:06.360]   Yes.
[02:22:06.840 --> 02:22:13.800]   Okay, so the thing that I've noticed because of Brexit, and I'm sorry to bring up Brexit again,
[02:22:13.800 --> 02:22:22.200]   but I have a very good reason to do so, is that I have never in my life seen so many young people
[02:22:22.200 --> 02:22:25.000]   motivated by politics.
[02:22:25.000 --> 02:22:28.040]   So many people care so passionately.
[02:22:28.040 --> 02:22:34.760]   And, you know, if this all goes through and we do end up leaving Europe, we are going to have
[02:22:35.640 --> 02:22:40.920]   at least right now two generations of people living who almost certainly didn't want it
[02:22:40.920 --> 02:22:47.400]   on the whole, you know, it's known that the older generations favored to leave and younger
[02:22:47.400 --> 02:22:48.600]   favored to stay.
[02:22:48.600 --> 02:22:53.000]   And they are the people that are going to have the power and the influence and the motivation
[02:22:53.000 --> 02:22:58.200]   and the drive to do something about it and to make the future better.
[02:22:58.200 --> 02:23:03.800]   And I think that as long as there are people that fear that things will get worse and are
[02:23:03.800 --> 02:23:07.720]   motivated to try and prevent things, then we're going to have people that are going to be
[02:23:07.720 --> 02:23:09.400]   trying to make the world a better place.
[02:23:09.400 --> 02:23:12.680]   And that's the thought that keeps me very hopeful.
[02:23:12.680 --> 02:23:17.480]   I'd be far more worried if things like Brexit was taking place and nobody seemed to care
[02:23:17.480 --> 02:23:21.800]   and there were no protests because that would mean we've given up and then we really are all
[02:23:21.800 --> 02:23:22.920]   screwed.
[02:23:22.920 --> 02:23:24.920]   I'm feeling the same thing here in the US.
[02:23:24.920 --> 02:23:28.760]   So yeah, that's a little bit of sunshine in the clouds here.
[02:23:28.760 --> 02:23:31.480]   Yeah, it takes tough times to get people off their ass.
[02:23:31.480 --> 02:23:32.600]   Yeah.
[02:23:33.320 --> 02:23:34.040]   Now's the time.
[02:23:34.040 --> 02:23:38.440]   Well, and also, and also you have the pendulum effect.
[02:23:38.440 --> 02:23:45.400]   Ronald Reagan was kind of notoriously known socially as kind of a hard ass.
[02:23:45.400 --> 02:23:53.400]   But he was followed by his vice president, George H.W. Bush, who just died, who was a
[02:23:53.400 --> 02:23:55.720]   champion of compassionate conservatism.
[02:23:55.720 --> 02:23:58.680]   And then that we moved into the Clinton era.
[02:23:58.680 --> 02:24:06.040]   And so I think that what we're seeing now, hopefully there is a pendulum and we will swing back.
[02:24:06.040 --> 02:24:08.920]   At least that's kind of where I cast my hope.
[02:24:08.920 --> 02:24:12.200]   So let's end on a bright note then.
[02:24:12.200 --> 02:24:13.720]   That's a good note to end on.
[02:24:13.720 --> 02:24:16.840]   Thank you very much, Nate, for bringing this all up.
[02:24:16.840 --> 02:24:18.120]   We are so glad you were here.
[02:24:18.120 --> 02:24:18.760]   Come back again.
[02:24:18.760 --> 02:24:23.400]   Anytime Nate Langston is European and consumer tech editor at Bloomberg.
[02:24:23.400 --> 02:24:27.800]   You could follow him on Twitter and you're of course, read all his great work at Bloomberg.com.
[02:24:27.800 --> 02:24:28.920]   Thank you for being here, Nate.
[02:24:28.920 --> 02:24:30.600]   Thank you very much.
[02:24:30.600 --> 02:24:32.520]   I also do a podcast as well.
[02:24:32.520 --> 02:24:33.960]   What's your podcast, Nate?
[02:24:33.960 --> 02:24:41.720]   I have a podcast called Text Message, which is basically everything you just heard me
[02:24:41.720 --> 02:24:47.880]   save in the last two and a half hours, but every week rather than every now and again.
[02:24:47.880 --> 02:24:56.520]   So yeah, it was quite interesting because a lot of the technology podcasts in the world
[02:24:56.520 --> 02:24:59.640]   are obviously very global because our tech companies are so global.
[02:24:59.640 --> 02:25:05.240]   And I sort of thought, well, if you focus quite specifically on your country,
[02:25:05.240 --> 02:25:12.760]   then you create a little unique voice because no one else is exclusively talking about that.
[02:25:12.760 --> 02:25:18.520]   And what's been so interesting is that you had so many American listeners because they say,
[02:25:18.520 --> 02:25:21.960]   "Wow, it gives you so much more context on what's happening in your own country.
[02:25:21.960 --> 02:25:23.560]   You don't hear the same stories twice."
[02:25:23.560 --> 02:25:24.120]   Yeah, that's great.
[02:25:24.120 --> 02:25:27.800]   It's been really interesting to get the feedback.
[02:25:27.800 --> 02:25:35.480]   We focus on our country, but unfortunately, that's really not much of a focus, is it?
[02:25:35.480 --> 02:25:37.080]   Good job.
[02:25:37.080 --> 02:25:41.960]   It's Nate Langson and Text Message, T-E-C-H, Apostrophe S.
[02:25:41.960 --> 02:25:48.840]   You could find it on his website, Nate Langson, L-A-N-X-O-N.com/podcast.
[02:25:48.840 --> 02:25:50.120]   Thank you, Nate.
[02:25:50.120 --> 02:25:51.640]   Thank you.
[02:25:51.640 --> 02:25:52.360]   Thank you very much.
[02:25:52.360 --> 02:25:58.440]   Javindra Hardewar also has a podcast. He talks about movie/film.
[02:25:58.440 --> 02:26:01.960]   He's senior editor at Engadget in the /film podcast.
[02:26:01.960 --> 02:26:04.280]   Up to episode 494.
[02:26:04.280 --> 02:26:05.800]   Yeah, it's gone.
[02:26:05.800 --> 02:26:05.800]   Wow.
[02:26:05.800 --> 02:26:10.120]   We're going to hit 500 in January, so making some plans around that.
[02:26:10.120 --> 02:26:12.600]   That's huge. That's so great.
[02:26:12.600 --> 02:26:13.720]   Thank you.
[02:26:13.720 --> 02:26:15.000]   That's a good show.
[02:26:15.000 --> 02:26:16.520]   It is a good show.
[02:26:16.520 --> 02:26:20.200]   And you've tapped into something that people are very interested in.
[02:26:20.760 --> 02:26:25.880]   Yeah. We've been around, I guess, since long before podcasting became very popular.
[02:26:25.880 --> 02:26:29.560]   So it was kind of a couple of years after you started this whole network, Leo.
[02:26:29.560 --> 02:26:31.800]   So you were a big inspiration for us.
[02:26:31.800 --> 02:26:33.960]   So yeah, just wanted to say that.
[02:26:33.960 --> 02:26:39.640]   And I also just started another show just about tech and answering tech questions from people.
[02:26:39.640 --> 02:26:42.920]   So that's nomoretech@nomoretech.net.
[02:26:42.920 --> 02:26:43.560]   No with a K.
[02:26:43.560 --> 02:26:47.400]   Oh, I was going to say, I think that's the wrong title.
[02:26:47.400 --> 02:26:51.160]   It works both ways. It works both ways because I'm also tired of it.
[02:26:51.160 --> 02:26:52.120]   No more tech.
[02:26:52.120 --> 02:26:53.000]   Oh, no more.
[02:26:53.000 --> 02:26:55.000]   No more tech.
[02:26:55.000 --> 02:26:57.240]   Okay. Awesome.
[02:26:57.240 --> 02:26:59.000]   It's on iTunes.
[02:26:59.000 --> 02:27:01.160]   No more tech with the vendor hardware.
[02:27:01.160 --> 02:27:01.720]   Perfect.
[02:27:01.720 --> 02:27:04.680]   What's your podcast, Dwight?
[02:27:04.680 --> 02:27:06.840]   I'm a slacker.
[02:27:06.840 --> 02:27:08.040]   I'm clearly a slacker.
[02:27:08.040 --> 02:27:09.400]   You need a podcast.
[02:27:09.400 --> 02:27:10.920]   And I've got the voice for it.
[02:27:10.920 --> 02:27:11.560]   I should do it.
[02:27:11.560 --> 02:27:12.120]   You should.
[02:27:12.120 --> 02:27:12.680]   We'd listen.
[02:27:12.680 --> 02:27:15.560]   Not that I should be encouraging more podcasts.
[02:27:15.560 --> 02:27:18.440]   God knows, but what the hell?
[02:27:18.440 --> 02:27:19.000]   What?
[02:27:19.000 --> 02:27:19.480]   Let us.
[02:27:19.480 --> 02:27:19.560]   Let us.
[02:27:19.560 --> 02:27:19.960]   Let us.
[02:27:19.960 --> 02:27:21.080]   1000 flowers bloom.
[02:27:21.080 --> 02:27:22.440]   I might.
[02:27:22.440 --> 02:27:24.200]   I would think I would do something other than tech.
[02:27:24.200 --> 02:27:26.600]   I think the world needs a good pizza podcast.
[02:27:26.600 --> 02:27:27.160]   Ooh.
[02:27:27.160 --> 02:27:28.040]   Yes.
[02:27:28.040 --> 02:27:28.680]   Yes.
[02:27:28.680 --> 02:27:29.240]   There's no.
[02:27:29.240 --> 02:27:29.400]   Yep.
[02:27:29.400 --> 02:27:29.640]   There.
[02:27:29.640 --> 02:27:31.880]   And we definitely need a pizza podcast.
[02:27:31.880 --> 02:27:32.680]   We need pizza.
[02:27:32.680 --> 02:27:35.240]   Dwight Silverman.
[02:27:35.240 --> 02:27:37.640]   He's at the Houston Chronicles tech burger.
[02:27:37.640 --> 02:27:39.000]   Easy to find.
[02:27:39.000 --> 02:27:43.400]   Just go to the Houston Chronicle dot com slash tech burger.
[02:27:43.400 --> 02:27:44.600]   95% tech.
[02:27:44.600 --> 02:27:45.800]   5% burgers.
[02:27:45.800 --> 02:27:47.800]   100% yummy.
[02:27:47.800 --> 02:27:49.640]   Yummy.
[02:27:49.640 --> 02:27:51.160]   Thank you, Dwight.
[02:27:51.160 --> 02:27:51.800]   Great to see you.
[02:27:51.800 --> 02:27:52.360]   Thank you.
[02:27:52.360 --> 02:27:53.320]   Thank you.
[02:27:53.320 --> 02:27:56.360]   Thanks to all of you who listen to this show and watch this show
[02:27:56.360 --> 02:27:57.240]   each and every week.
[02:27:57.240 --> 02:27:58.280]   I really appreciate it.
[02:27:58.280 --> 02:28:00.520]   We've had a nice live audience.
[02:28:00.520 --> 02:28:02.360]   Mark and Carmen were visiting.
[02:28:02.360 --> 02:28:03.720]   It's great to have you too.
[02:28:03.720 --> 02:28:08.360]   If you want to be in our studio audience, just email tickets@twit.tv.
[02:28:08.360 --> 02:28:10.440]   We'll make sure you get a chair put out for you.
[02:28:10.440 --> 02:28:13.000]   Of course, if you have to be in the Petaluma area to watch live,
[02:28:13.000 --> 02:28:14.840]   there is no ticket charge though.
[02:28:14.840 --> 02:28:16.440]   It's a lot less than seeing Hamilton.
[02:28:16.440 --> 02:28:18.360]   It's also a lot less entertaining.
[02:28:18.360 --> 02:28:21.400]   At least you get what you pay for.
[02:28:21.400 --> 02:28:24.840]   You don't even have to come here to watch it live.
[02:28:24.840 --> 02:28:28.040]   You can watch the live stream at twit.tv/live.
[02:28:28.040 --> 02:28:32.840]   If you do that, join us in the chatroom at irce.twit.tv.
[02:28:32.840 --> 02:28:37.000]   You could download on-demand audio and video of this show and every show we do
[02:28:37.000 --> 02:28:39.080]   at our website, twit.tv.
[02:28:39.720 --> 02:28:43.400]   And probably the best way to do this is subscribe.
[02:28:43.400 --> 02:28:46.440]   You can subscribe in your favorite podcast application.
[02:28:46.440 --> 02:28:48.520]   In that way, you'll get every episode.
[02:28:48.520 --> 02:28:51.480]   The minute it comes out, that's what makes it a podcast.
[02:28:51.480 --> 02:28:53.320]   That's what it's just automatic.
[02:28:53.320 --> 02:28:54.920]   You just get it automatically.
[02:28:54.920 --> 02:29:00.120]   And I should always forget and I should remember that we are also on your local voice
[02:29:00.120 --> 02:29:01.000]   assistant device.
[02:29:01.000 --> 02:29:03.080]   All you have to do is say, "Hey, voice assistant.
[02:29:03.080 --> 02:29:04.520]   Listen to this week in tech."
[02:29:04.520 --> 02:29:07.320]   And you'll get the most recent version of the podcast.
[02:29:07.320 --> 02:29:10.760]   We're even on the Amazon Echo's flash briefing.
[02:29:10.760 --> 02:29:14.440]   The whole network has every one of us put a little thing on every day.
[02:29:14.440 --> 02:29:15.640]   It's a different one.
[02:29:15.640 --> 02:29:19.720]   So it's a great way to hear a little sampler of twit in your flash briefing on your Amazon Echo.
[02:29:19.720 --> 02:29:21.880]   Just look for it in the Echo app.
[02:29:21.880 --> 02:29:23.160]   Thanks for being here, everybody.
[02:29:23.160 --> 02:29:27.240]   I think it's time to go home because another twit is in the can.
[02:29:27.240 --> 02:29:27.720]   Bye-bye.
[02:29:27.720 --> 02:29:38.600]   [music]

