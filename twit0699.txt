
[00:00:00.000 --> 00:00:05.440]   It's time for Twit this weekend Tech. We've got a great round table of lots of people because this is our
[00:00:05.440 --> 00:00:12.800]   long-awaited, much anticipated, best of episodes. Some of the best moments from the year gone by
[00:00:12.800 --> 00:00:19.520]   Best Conversations, Best Insights, best fun with many of our great panelists. You're going to enjoy
[00:00:19.520 --> 00:00:21.680]   this week in Tech next.
[00:00:24.320 --> 00:00:31.120]   NetCasts you love. From people you trust.
[00:00:31.120 --> 00:00:33.040]   This is Twit.
[00:00:33.040 --> 00:00:48.880]   This is Twit this weekend Tech, episode 699. For Sunday, December 30th, 2018.
[00:00:49.440 --> 00:00:56.640]   Our year's best. This weekend Tech is brought to you by Eero. Never think about Wi-Fi again.
[00:00:56.640 --> 00:01:02.880]   When you can have brilliant, hyper-fast, super simple Wi-Fi with Eero. Now get total network
[00:01:02.880 --> 00:01:10.480]   protection with Eero Plus. Visit ero.com/twit and get $100 off the Eero base unit to beacons package
[00:01:10.480 --> 00:01:14.480]   plus one year of Eero Plus when you enter the code TWIT at check-out.
[00:01:15.360 --> 00:01:19.920]   And by LastPass. Make password management a priority in 2019.
[00:01:19.920 --> 00:01:25.120]   Secure every password protected entry point to your business and reduce the threat of breach
[00:01:25.120 --> 00:01:33.200]   at lastpass.com/twit. And by Betterment. The investing tool for those who refuse to settle for
[00:01:33.200 --> 00:01:39.280]   average investing. Sign up today at betterment.com/twit and get up to one year managed free.
[00:01:42.080 --> 00:01:48.640]   As another year gone by, I started the TWIT network. I say "we" because when we do TWIT,
[00:01:48.640 --> 00:01:53.680]   it's really a whole bunch of people involved. But in the earliest days, it was just me. Up in
[00:01:53.680 --> 00:02:00.080]   an attic somewhere with Kevin Rose and David Prager and Patrick Norton and Robert
[00:02:00.080 --> 00:02:04.880]   recording shows and having a good time. Robert, what's his last name?
[00:02:04.880 --> 00:02:08.560]   Heron, thank you. Robert's going to think I forgot him.
[00:02:09.520 --> 00:02:14.320]   Recording shows, having a good time talking about tech. Here we are. What is it now?
[00:02:14.320 --> 00:02:20.480]   Thirteen, 14 years in April. Later still doing it with some of the most interesting people.
[00:02:20.480 --> 00:02:25.840]   I love getting all these journalists together each Sunday afternoon and talking about tech.
[00:02:25.840 --> 00:02:30.800]   TWIT is absolutely our flagship show and we're really glad you watch it.
[00:02:30.800 --> 00:02:36.720]   We decided to take the week off because well the holidays are upon us. Christmas is coming
[00:02:36.720 --> 00:02:48.160]   New Year. New Year's Eve is Monday tomorrow. Instead of doing all new shows, dragging people in on
[00:02:48.160 --> 00:02:53.440]   the holidays, we thought it would be fun to go back and look at some of the great moments on TWIT
[00:02:53.440 --> 00:03:03.120]   from 2018. Starting, of course, with perhaps the biggest security story of 2018, the meltdown
[00:03:03.120 --> 00:03:07.600]   and specter flaws. Although, I think XKCD had a very nice take on it.
[00:03:07.600 --> 00:03:13.120]   Watch. Well, I guess we should talk about the big story, which is the worst security flaw in
[00:03:13.120 --> 00:03:18.160]   the history of all mankind. At least it would be if you watched mainstream media.
[00:03:18.160 --> 00:03:25.280]   Yes. I've been trying to and I've listened to Steve Gibson's explanation for this several times.
[00:03:25.280 --> 00:03:30.000]   It's over and over. I think I understand how well I don't understand this, but
[00:03:30.960 --> 00:03:35.600]   it's a very complicated thing. But the way that I would characterize this is that
[00:03:35.600 --> 00:03:45.040]   chips for the last... Since 1995. 13 years now. They guess what is going to happen to speed
[00:03:45.040 --> 00:03:51.280]   things up and we've now discovered that that information that they're guessing can be peaked at.
[00:03:51.280 --> 00:03:55.840]   Leaked. Leaked. Harvested. Whatever. Theoretically. It's all theoretical. So
[00:03:57.440 --> 00:04:00.960]   we'll start with the good news or the sunny perspective. That's why I'm here, Leo,
[00:04:00.960 --> 00:04:04.240]   is to provide the sunny, happy perspective. Mr. Optimus. Yes.
[00:04:04.240 --> 00:04:11.440]   Which is that, first of all, we don't know of any actual exploit that's taken place from this.
[00:04:11.440 --> 00:04:15.280]   And number two, it's going to be fixed and it's all going to be fine.
[00:04:15.280 --> 00:04:21.760]   That's the best possible scenario. Yeah, it's the best puzzle scenario, but it may actually not be
[00:04:21.760 --> 00:04:26.480]   true, but we'll delve into that a little bit. These are the two flaws, which you're
[00:04:26.480 --> 00:04:31.360]   interestingly discovered simultaneously meltdown and specter. Specter effects,
[00:04:31.360 --> 00:04:38.000]   intel chips, meltdown effects, not just intel chips, but AMD chips and some arm chips, including
[00:04:38.000 --> 00:04:44.240]   Qualcomm chips. And that's what is really the issue here. And one of the reasons I say it is
[00:04:44.240 --> 00:04:49.040]   probably the most serious security flaw we've seen is because it pretty much affects any modern
[00:04:49.040 --> 00:04:55.520]   computer, including an iPhone, including an iPad. And so that means it's widespread. What it
[00:04:55.520 --> 00:05:02.320]   isn't is easy. Right. And there's a, Bruce Schneider talks a little bit about the synchronicity of two
[00:05:02.320 --> 00:05:07.040]   incompletely independent teams discovering this. And apparently this is not unusual.
[00:05:07.040 --> 00:05:12.240]   He, well, Bruce is very literate, so he compares Leibniz and Newton discovery calculus at the
[00:05:12.240 --> 00:05:16.640]   same time. I don't think it's like that. But I think what it is, is researchers tend to look
[00:05:16.640 --> 00:05:23.920]   in the same alleyways. And this is a category of flaws, called timing flaws, that have been
[00:05:23.920 --> 00:05:30.240]   lately very fruitful. Rohammer is an example where people were able to kind of look into memory based
[00:05:30.240 --> 00:05:39.680]   on a kind of a weird race condition. So we will, we will, so in other words, it's interesting that
[00:05:39.680 --> 00:05:44.880]   these are discovered by independent teams. Schneider says, that's one reason we should worry about this,
[00:05:44.880 --> 00:05:51.440]   because if these two teams discovered it at roughly the same time independently,
[00:05:51.440 --> 00:05:56.160]   who else discovered it and who might have discovered it even a year ago or two years ago or 10 years
[00:05:56.160 --> 00:06:03.760]   ago, 10 years ago, including the NSA. So this is, and I think was it, Micah suggested this XKCD
[00:06:03.760 --> 00:06:12.080]   yeah, comic strip Randall is great at explaining this stuff. The melt, this is a XKCD number 1938.
[00:06:12.080 --> 00:06:20.480]   The meltdown inspector exploits use speculative execution. What's that? Well, you know, the
[00:06:20.480 --> 00:06:25.600]   trolley problem? Well, for a while now, geeks know the trolley problem because it's talked about a lot
[00:06:25.600 --> 00:06:31.760]   in self driving cars. Well, for a while now, CPUs have basically been sending trolleys down both
[00:06:31.760 --> 00:06:37.680]   paths, quantum style, while awaiting your choice at speculative execution. Intel started doing it in
[00:06:37.680 --> 00:06:43.120]   1995 because it really did speed up processors. They would guess what you will do next, start that
[00:06:43.120 --> 00:06:46.960]   execution. And when if they guess correctly, it would be massive improvements. If they guess
[00:06:46.960 --> 00:06:51.440]   wrong, yeah, you'd have to back up a little bit. But it turned out the massive improvement
[00:06:51.440 --> 00:06:56.640]   outweighed the slowdown. So every processor, modern processor uses speculative execution,
[00:06:56.640 --> 00:07:01.280]   except for maybe some of the dumbest arm processors, Raspberry Pi, for instance, says we don't do
[00:07:01.280 --> 00:07:07.040]   that. So we don't have to worry. So you know, the trolley problem? Well, for a while now, both CPUs
[00:07:07.040 --> 00:07:12.800]   have basically been sending trolleys down both paths, quantum style, while awaiting your choice.
[00:07:12.800 --> 00:07:18.320]   Then the unneeded phantom trolley disappears, they say, well, you didn't do that. But the
[00:07:18.320 --> 00:07:23.520]   phantom trolley isn't supposed to touch anyone. But it turns out you can still use it to do stuff.
[00:07:23.520 --> 00:07:29.200]   And it can drive through walls. What she's saying here, and what's happening with
[00:07:29.200 --> 00:07:34.320]   meltdown inspector is because another technique used by processors to speed up execution,
[00:07:34.320 --> 00:07:42.080]   very fast level one cash right next to the processor. In the speculative branch, it will load that
[00:07:42.080 --> 00:07:50.080]   cash with data for the next process. And it turns out the current process can peak at that data.
[00:07:50.080 --> 00:07:53.920]   This is why it isn't the most useful flaw ever, because that data, you don't really get to
[00:07:53.920 --> 00:08:00.320]   choose what's in that cash. But it might contain passwords, you know, logins, credit card numbers.
[00:08:00.320 --> 00:08:05.040]   If you could do it a lot, like for over a long period of time, it might be really valuable.
[00:08:05.040 --> 00:08:09.680]   That's where by the way, the biggest risk is to people running on processors,
[00:08:10.400 --> 00:08:16.080]   shared processors like in virtual machines or servers. So often when you have a web server,
[00:08:16.080 --> 00:08:21.360]   you're running on the same machine as a hundred other websites. If one of those websites were a
[00:08:21.360 --> 00:08:26.720]   bad actor, he could run software that would then peek into all the other websites, cash activities,
[00:08:26.720 --> 00:08:32.160]   and perhaps over a period of time, get stuff. So that's the biggest risk. And by the way,
[00:08:32.160 --> 00:08:38.560]   that's why Amazon Web Services, Google Services, Microsoft Azure, they've all been down lately.
[00:08:38.560 --> 00:08:43.920]   Even Epic Gaming went down. In fact, they said, if you're playing Fortnite on our servers,
[00:08:43.920 --> 00:08:46.720]   you may notice some slowness. We're trying to patch them as fast as we can.
[00:08:46.720 --> 00:08:53.920]   So this is a big problem on shared processors, because then you wouldn't have to ever have
[00:08:53.920 --> 00:08:58.720]   malware on your system. It could just be somebody else acting badly. But if you had malware or even
[00:08:58.720 --> 00:09:07.360]   an Apple discovered that this is a scary one, I think it was meltdown, can be achieved by a browser
[00:09:07.360 --> 00:09:11.840]   running a JavaScript program. So you could theoretically go to a bad website. And the
[00:09:11.840 --> 00:09:16.480]   JavaScript hidden on that website could actually try to read your memory contents. It would only
[00:09:16.480 --> 00:09:21.680]   get what was in the cache. It would be really hit or miss. It's a much better tool for targeted.
[00:09:21.680 --> 00:09:25.440]   That's why the NSA would love this. Because if you know, if I'm going after Mike Elgin,
[00:09:25.440 --> 00:09:29.440]   pardon pardon this long explanation, but it's complicated.
[00:09:29.440 --> 00:09:30.960]   I think it's important. Yeah.
[00:09:30.960 --> 00:09:38.480]   Even even the cartoon has to be explained. So that sounds bad. Honestly, I've been assuming
[00:09:38.480 --> 00:09:42.800]   we were doomed ever since I learned about row hammer. That was the other timing flaw.
[00:09:42.800 --> 00:09:49.600]   What's row hammer? If you toggle a row of memory cells on and off really fast,
[00:09:49.600 --> 00:09:55.280]   you can use electrical interference to flip nearby bits. And do we just suck at computers?
[00:09:55.840 --> 00:10:01.360]   Yep, especially shared ones. So you're saying the cloud is full of phantom trolleys armed with
[00:10:01.360 --> 00:10:09.520]   hammers. Yes, that's exactly right. Okay. I'll just install updates. Good idea.
[00:10:09.520 --> 00:10:17.200]   That's the bottom line. Yes. Update. Right. Because everybody, Microsoft, Apple, Qualcomm,
[00:10:17.200 --> 00:10:21.680]   even Intel and AMD say we're going to put out patches. Those would be the better patches,
[00:10:21.680 --> 00:10:26.400]   the patch that microcode level. But if you're running Windows, it's been patched. If you're
[00:10:26.400 --> 00:10:30.160]   running a recent version of Macintosh has been patched and those patches will continue to come
[00:10:30.160 --> 00:10:33.920]   out. For instance, the Safari patch, I don't think is out yet, but what kind of.
[00:10:33.920 --> 00:10:37.040]   And there'll probably be future patches. There'll be more elegance right now.
[00:10:37.040 --> 00:10:40.560]   One hopes putting band-aids on it and making sure that it doesn't.
[00:10:40.560 --> 00:10:45.200]   You know, it can't. That's one issue here, of course. And the register when it first reported
[00:10:45.200 --> 00:10:49.040]   this on Tuesday, it was said everybody's here on fire because they said you're going to see
[00:10:49.040 --> 00:10:55.040]   5 to 30% performance degradation. That's the at least initial patches turn off speculatively.
[00:10:55.040 --> 00:10:58.320]   Exactly. You just turn that off. That's what you get. Specular of execution, by the way,
[00:10:58.320 --> 00:11:03.360]   is the official name of Saudi Arabia's death penalty loss. That's an entirely different issue.
[00:11:03.360 --> 00:11:08.160]   That's if you just turn that off. And what they're going to do is over time, and Steve Gibson
[00:11:08.160 --> 00:11:13.680]   explained this beautifully, but they're going to get it back to the performance where there's
[00:11:13.680 --> 00:11:17.280]   no difference in performance. Eventually, I don't know, six months, whatever it is.
[00:11:17.280 --> 00:11:20.960]   Well, Apple, for instance, in its benchmark, said the patch on Safari,
[00:11:20.960 --> 00:11:24.880]   it's going to give you a 1% to 2% performance degradation. That's not noticeable.
[00:11:24.880 --> 00:11:27.440]   Anything below 5%, you wouldn't. Nobody will notice it.
[00:11:27.440 --> 00:11:36.000]   Yeah. So bottom line is you should patch, as soon as the patch is available.
[00:11:36.000 --> 00:11:41.760]   There are some complications, however. For instance, this is very annoying.
[00:11:43.440 --> 00:11:50.960]   If you are running a third party antivirus on Windows, it may be blocking the Microsoft patch
[00:11:50.960 --> 00:11:55.440]   because this is one of the reasons I tell people don't use third party antiviruses.
[00:11:55.440 --> 00:12:05.120]   Antiviruses use unsupported calls to make kernel memory calls. This would be blocked by the patch.
[00:12:05.120 --> 00:12:10.240]   And it would result in a blue screen of death. So a lot of people, I've heard from a number of
[00:12:10.240 --> 00:12:16.240]   people said, "Well, I installed Microsoft's out of bandwidth patch. It was late last week,
[00:12:16.240 --> 00:12:20.640]   and I got a blue screen of death immediately." That's because of your third party antivirus.
[00:12:20.640 --> 00:12:24.480]   And there's a list of third party antiviruses. This is from ZDNet.
[00:12:24.480 --> 00:12:30.320]   Although the list is being maintained by Kevin Beaumont, a security researcher,
[00:12:30.320 --> 00:12:33.840]   he's got a public spreadsheet. That would be more up to date. This goes back to January 5th.
[00:12:33.840 --> 00:12:39.520]   A lot of antiviruses are either have not been fixed or haven't done what Microsoft asked,
[00:12:39.520 --> 00:12:45.280]   which was to change a registry key to signal to Windows Update. Okay, it's okay to install now.
[00:12:45.280 --> 00:12:53.680]   So you stay may still not be getting it. It's kind of a mess, but I think, and correct me if I'm wrong,
[00:12:53.680 --> 00:12:58.240]   I don't think it's anything anybody should freak out about. It's very hard to use this.
[00:12:58.240 --> 00:13:02.400]   If you do want to freak out about it, here's one reason why you might want to do that.
[00:13:03.120 --> 00:13:09.840]   So up until this point, whether anybody who's exploited is anyone's guess, and like you've said,
[00:13:09.840 --> 00:13:14.960]   it could be a state actor or whatever, who knows what's going on, could be the NSA.
[00:13:14.960 --> 00:13:21.520]   But now that it's been announced, there's a scramble, no doubt, among hackers to figure out
[00:13:21.520 --> 00:13:27.840]   if and how and when they can exploit this. So it's a challenge. And so the industry is quickly
[00:13:27.840 --> 00:13:32.240]   patching it and trying to fix it and fix the leaks, but as you pointed out, some Windows users aren't
[00:13:32.240 --> 00:13:39.760]   getting it. There's going to be different rollouts. And so there's a new urgency in the hacker,
[00:13:39.760 --> 00:13:46.800]   malicious actor industrial complex to exploit this. Right, they know now it's there.
[00:13:46.800 --> 00:13:49.440]   Exactly. Timing floods are notoriously difficult, Amy.
[00:13:49.440 --> 00:13:52.880]   I would say to go ahead and freak out about it.
[00:13:52.880 --> 00:13:57.440]   Thanks for calming everything down here.
[00:13:57.440 --> 00:14:03.440]   You're going to freak out faction. But here's why. If it was the case that, and this is
[00:14:03.440 --> 00:14:11.360]   particularly complicated because it's a hardware issue. And on top of it, it's not a bug that
[00:14:11.360 --> 00:14:16.480]   appeared overnight because somebody was trying to sort of make an update and the process of making
[00:14:16.480 --> 00:14:21.600]   the update, they broke something. This was a hardware flaw that has been inherent in the system
[00:14:21.600 --> 00:14:31.600]   that people knew about. I could say the same of Sony, which is no stranger to problems with hackers
[00:14:31.600 --> 00:14:36.960]   and has a very, very long history of ignoring all of the weak signals.
[00:14:36.960 --> 00:14:43.760]   Sony would be turning something wrong. So their attack was a targeted attack that would be particularly
[00:14:43.760 --> 00:14:49.360]   vulnerable to this kind of vulnerability. Well, the point that I'm making is the part that people
[00:14:49.360 --> 00:14:54.640]   should be freaking out about is that our as technology becomes more complicated,
[00:14:54.640 --> 00:14:59.520]   it's going to start breaking in sort of strange ways, which means that companies need to be ever
[00:14:59.520 --> 00:15:06.560]   more vigilant about forecasting out in advance and pressure testing systems for things like timing
[00:15:06.560 --> 00:15:12.560]   flaws, which somebody somewhere in the organization I have to have thought raised, they had to have
[00:15:12.560 --> 00:15:18.240]   raised that issue at some point. It's not like this could not have come as a giant surprise.
[00:15:18.880 --> 00:15:25.920]   And I might even argue that it was intentional and the consequences wound up being worse than
[00:15:25.920 --> 00:15:30.800]   maybe the risk management people might have thought in advance. So the reason to sort of freak out
[00:15:30.800 --> 00:15:36.480]   is because we think that once we purchase technology, it's ours and we have agency and domain over it.
[00:15:36.480 --> 00:15:42.640]   This is yet another example that that's not the case. And I know that these glitches and these hacks
[00:15:42.640 --> 00:15:47.440]   are getting more and more complicated for the average person to understand. However, it's incumbent
[00:15:47.440 --> 00:15:53.200]   upon us to keep state focused and to ask for some serious answers.
[00:15:53.200 --> 00:15:57.920]   Until there are several, of course, class action lawsuits, that's not really
[00:15:57.920 --> 00:16:03.680]   probative, but there is some speculation that maybe this is a janky way to do it and
[00:16:03.680 --> 00:16:09.280]   Intel should be faulted for choosing this way to speed up their processors. This was like,
[00:16:09.280 --> 00:16:12.960]   I don't know, I'm not educated enough to say.
[00:16:14.400 --> 00:16:17.680]   And I'm not a hardware person.
[00:16:17.680 --> 00:16:26.000]   However, chip designer to really understand if it's just not like this was a brand new design
[00:16:26.000 --> 00:16:31.440]   and they were trying to do something totally radically different over the last design cycle.
[00:16:31.440 --> 00:16:33.600]   And this is going back to me.
[00:16:33.600 --> 00:16:38.960]   You're right, 95, this wasn't a problem. But remember, a little bit later, Intel's lunch
[00:16:38.960 --> 00:16:43.760]   was starting to get eaten by AMD. Intel went down a bad road with the Itenium.
[00:16:43.760 --> 00:16:46.640]   They suddenly realized you can't get faster than 4 GHz.
[00:16:46.640 --> 00:16:53.280]   Just becomes unreliable. They were up against, maybe not by 1995, but quickly thereafter,
[00:16:53.280 --> 00:16:57.520]   up against a wall of performance. And there was huge demand for more performance.
[00:16:57.520 --> 00:17:04.320]   And as I remember it, remember Intel happened to have this Skunkworx project in Israel that
[00:17:04.320 --> 00:17:09.520]   when Itenium failed, they went to that we're doing things like branch prediction and speculative
[00:17:09.520 --> 00:17:15.840]   execution to get more performance out of the x86 platform. They wanted to move away from it.
[00:17:15.840 --> 00:17:24.480]   And they chose to go with x86. Perhaps could fault them for making a poor design decision.
[00:17:24.480 --> 00:17:28.960]   But on the other hand, it's been 22 years before we found the problem.
[00:17:28.960 --> 00:17:35.040]   Right. And so again, I come back to, I hear from a lot of people, this has been around for 22
[00:17:35.040 --> 00:17:40.720]   years. It's impossible that it's just now somebody figured this out. I wouldn't worry about it.
[00:17:40.720 --> 00:17:45.280]   Right. If something horrible hasn't happened yet, what's the probability?
[00:17:45.280 --> 00:17:54.320]   And again, to me, this is yet another example of organizations making sacrifices for whatever gain
[00:17:54.320 --> 00:17:58.400]   that put consumers in harm's way.
[00:18:00.560 --> 00:18:09.520]   Yeah. But I mean, this is a really good example of something so technical that even we, as people
[00:18:09.520 --> 00:18:15.360]   who cover tech, have a hard time wrapping our head around it. Steve Gibson, we talked about this
[00:18:15.360 --> 00:18:19.760]   on Tuesday. And even on Tuesday, Steve Gibson was, you know, and nobody hears this other problem,
[00:18:19.760 --> 00:18:26.000]   might cause a slowdown. Nobody really understood the impact of this for a while. And it took a lot
[00:18:26.000 --> 00:18:33.120]   of minds thinking about it. How do you then explain this to regular people and CEOs and other people?
[00:18:33.120 --> 00:18:38.880]   I mean, how do you, this is a, this is the world we're living in. In fact, I remember the late
[00:18:38.880 --> 00:18:44.320]   Jerry Purnell writing, I think it was in Lucifer's hammer. And this was many years ago, he said,
[00:18:44.320 --> 00:18:48.160]   we were already in a world where most of the technology people use, they don't understand,
[00:18:48.160 --> 00:18:52.160]   they don't know how it's made. You could know how an internal combustion engine worked.
[00:18:52.160 --> 00:18:59.120]   Good luck explaining how a computer works. And if a catastrophic event or to happen like an
[00:18:59.120 --> 00:19:03.040]   asteroid hitting the earth as it does in the book, we wouldn't be able to recreate it. Nobody
[00:19:03.040 --> 00:19:09.440]   would know how it was made in the first place. We're using stuff that is effectively magical.
[00:19:09.440 --> 00:19:17.520]   And now we're asking the wizards who created the magic, hey guys, don't screw up. But we have no
[00:19:17.520 --> 00:19:22.880]   way to test that and just wait until the AI is creating the AI. Well, that's just a matter.
[00:19:22.880 --> 00:19:29.040]   How do we control? But that's already so but that's kind of my point, right? Because we're already
[00:19:29.040 --> 00:19:34.960]   so I will preface this by saying there's a lot of misplaced optimism and fear when it comes to
[00:19:34.960 --> 00:19:42.400]   writing the book. You're writing the book about that? Yes. So, but we can't submit ourselves
[00:19:42.400 --> 00:19:48.960]   every time we butt up against a rough problem. And sort of just say, well, this is like a black
[00:19:48.960 --> 00:19:52.960]   box. If I had a nickel for every time I've heard somebody describe a complicated technology as a
[00:19:52.960 --> 00:19:57.760]   black box that we just can't understand, we shouldn't be in a position where we're creating
[00:19:57.760 --> 00:20:05.600]   technology that we can't understand let alone can't explain, right? And there's a lot of,
[00:20:07.200 --> 00:20:12.560]   you know, there's a lot of that. So that either tells me that humans have just gotten lazy and
[00:20:12.560 --> 00:20:16.640]   we don't want to take the time to understand. I mean, obviously technology has gotten a lot
[00:20:16.640 --> 00:20:25.600]   more complicated, but you know, we should not be in a situation where we are entirely reliant on
[00:20:25.600 --> 00:20:33.680]   tools and services and code and devices that, you know, we can't in any meaningful way understand
[00:20:33.680 --> 00:20:38.480]   how it works, right? From a journalistic standpoint,
[00:20:38.480 --> 00:20:52.160]   from a journalistic standpoint, I think that this is another one of those challenges on multiple
[00:20:52.160 --> 00:20:55.760]   levels. One, because yes, you know, we're talking about trying to explain this to
[00:20:57.440 --> 00:21:06.560]   I go back to normal people who people who aren't steeped in technology like we are. And when we
[00:21:06.560 --> 00:21:12.240]   try to explain this a lot of times it's like people maybe necessarily who aren't interested in
[00:21:12.240 --> 00:21:17.040]   hearing about it and are just like, you know, they'll figure it out. So it does sort of go back to
[00:21:17.040 --> 00:21:23.440]   that apathy, I think about these magical boxes that we don't understand how they work. And so
[00:21:23.440 --> 00:21:29.120]   we're apathetic to the problems because maybe we've never experienced them before ourselves. And so
[00:21:29.120 --> 00:21:34.720]   there are two things that become an issue. One is just explaining it in general. But two is like
[00:21:34.720 --> 00:21:41.760]   keeping control of the of the message or making sure that the right message gets out there because
[00:21:41.760 --> 00:21:49.440]   inevitably how these things play out is it gets to the people that know more about this stuff and
[00:21:49.440 --> 00:21:54.080]   they write their articles, they write their explanations, they put up their blog posts,
[00:21:54.080 --> 00:22:00.320]   they make comics, what have you. And then it trickles down to more mainstream,
[00:22:00.320 --> 00:22:05.440]   it's like secondary, and then the main mainstream, it gets on local news outlets and things like that.
[00:22:05.440 --> 00:22:14.320]   And by the time it gets there, it's been bastardized so much and and sort of translated so much that
[00:22:14.320 --> 00:22:21.040]   oftentimes there's so much inaccuracy because of the vagueness of it. And so by the time it gets to
[00:22:21.040 --> 00:22:25.680]   a family member of mine who is not as steeped in technology as I am,
[00:22:25.680 --> 00:22:30.400]   they're hearing a whole different story and it's the worst possible thing. And you add on top of that
[00:22:30.400 --> 00:22:35.600]   the fact that all of these news outlets are scrambling for views, they're scrambling for clicks,
[00:22:35.600 --> 00:22:41.360]   they're scrambling for page views and things like that. And you have headlines that tell
[00:22:42.480 --> 00:22:47.760]   incorrect stories or don't tell the whole story. And a lot of times we know like we can tell from
[00:22:47.760 --> 00:22:54.960]   all of these different things that track our usage and our activity on pages that people oftentimes
[00:22:54.960 --> 00:23:00.320]   don't go through and read the article, they read headlines, they read Twitter sized bits of
[00:23:00.320 --> 00:23:06.400]   information. And so what we're doing is spreading misinformation and it's so hard to get caught up
[00:23:06.400 --> 00:23:12.400]   with that. And even if we do sort of come back and say, okay, you've heard this, but it's not
[00:23:12.400 --> 00:23:17.280]   exactly this. You try to explain what it is. Those articles get far less views than any of the
[00:23:17.280 --> 00:23:22.080]   articles that originally went out there and said, this is the end of the world. Everything's melting
[00:23:22.080 --> 00:23:28.240]   down. So it's it's it's problem. Like there's a problem. Your computer is a time bomb film of 11.
[00:23:28.240 --> 00:23:32.960]   But it is the world we live in where you want to be for we were talking about this before the
[00:23:32.960 --> 00:23:37.360]   show. You want to be first, you want to the only way to get the links is to be sensational
[00:23:37.360 --> 00:23:44.080]   and fast. And that's the worst situation. But I think both Amy and Mike are saying what I would
[00:23:44.080 --> 00:23:50.080]   completely agree with, what is ultimately, this is the job of journalism is to find is to
[00:23:50.080 --> 00:23:57.040]   do the work to understand the tough stuff and whether it's tech or government or whatever and
[00:23:57.040 --> 00:24:01.920]   explain it so that we can have an informed populace, right? That's the job. Well, they're
[00:24:01.920 --> 00:24:06.480]   journalists. They're reporters and then they're editorialists like myself. And so one of the
[00:24:06.480 --> 00:24:10.720]   things that I would say is it's probably less important. I mean, the public is never going to
[00:24:10.720 --> 00:24:15.760]   understand this stuff. The public can't locate Australia on a map. Yeah. Right. So the public is
[00:24:15.760 --> 00:24:21.120]   never going to understand this kind of security. And it's far more important that they have a
[00:24:21.120 --> 00:24:26.720]   sort of a behavioral operating system for coping with this stuff. And here's why. Every time we
[00:24:26.720 --> 00:24:31.760]   learn about these things, it's already been out there forever. I mean, this one's been out there
[00:24:31.760 --> 00:24:36.800]   for two decades. So it's like, so it doesn't matter what you know now. It matters what you've
[00:24:36.800 --> 00:24:44.160]   been doing for two decades. So we need to figure out how to instruct the public so that they can
[00:24:44.160 --> 00:24:50.560]   protect their data. Because we're always shocked and surprised by all the different creative ways
[00:24:50.560 --> 00:24:56.800]   that our information can be stolen from us. Every year, there's two or three major stories that
[00:24:56.800 --> 00:25:01.520]   it's like, I never even thought that was a way that people could harvest all my passwords.
[00:25:01.520 --> 00:25:10.480]   Third party cloud services are routinely hacked and all this stuff downloaded and so on. And so
[00:25:10.480 --> 00:25:16.960]   we there needs to be a much greater emphasis on providing the public with an all purpose set of
[00:25:16.960 --> 00:25:23.040]   behaviors and tools that will allow them to keep their data safe. No matter what is that possible?
[00:25:23.040 --> 00:25:28.800]   It's possible. It's possible to do the best we can with that. So right now,
[00:25:28.800 --> 00:25:32.240]   there's certainly update. Yeah, update back up in the cloud.
[00:25:32.240 --> 00:25:38.560]   Password all that Amy. Amy doesn't seem convinced. Amy, you read the article in the Atlantic in
[00:25:38.560 --> 00:25:44.240]   September, the coming software apocalypse. Yeah. Yeah. I feel like sometimes I feel like we're at
[00:25:44.240 --> 00:25:51.040]   the top of a house of cards and it's starting to teeter. Yeah. So a couple of things. We already
[00:25:51.040 --> 00:25:57.600]   have the tools to protect ourselves in a reasonable way against threats and hacks.
[00:25:57.600 --> 00:26:05.120]   I would walk out on the street right now and just ask random people when the last time was
[00:26:05.120 --> 00:26:12.240]   that they updated their OS, updated the router settings. I mean, like updated everything.
[00:26:12.240 --> 00:26:15.440]   People just don't do it for any number of different reasons.
[00:26:19.120 --> 00:26:24.480]   I don't know. I would also just, I mean, it's plausible to give people the tool set, but it's
[00:26:24.480 --> 00:26:32.720]   improbable that the everyday person is going to see the urgency in keeping themselves
[00:26:32.720 --> 00:26:38.320]   constantly safe. I mean, it's replicate. This is why companies don't, a lot of companies,
[00:26:38.320 --> 00:26:42.960]   don't participate in strategic foresight and they don't do the work of a futurist.
[00:26:42.960 --> 00:26:46.960]   Because until something happens, they don't see the value in it.
[00:26:46.960 --> 00:26:53.280]   They want to be reactive every, like Sony. Well, and that's right. So, you know, for Sony to have
[00:26:53.280 --> 00:26:58.320]   Sony to have completely changed what it was doing, the net, the many, many, many times that it got
[00:26:58.320 --> 00:27:05.360]   hacked would have been an investment for everyday people, for everyday people to do the type of
[00:27:05.360 --> 00:27:09.760]   safeguarding that we're talking about, which isn't just running patches and updates, but also
[00:27:09.760 --> 00:27:14.960]   paying attention to the news. The Secret Service told me this years ago, banks would rather pay
[00:27:14.960 --> 00:27:21.840]   the losses than fix the system. So, we're talking about an investment in time and money that companies,
[00:27:21.840 --> 00:27:26.560]   which have a financial stake in all of this, aren't willing to make. You know, I just don't see everyday
[00:27:26.560 --> 00:27:33.600]   people being willing to do that either. Now, I will say that this, you know, I'm a quantitative
[00:27:33.600 --> 00:27:39.520]   futurist and this has been my job for 15 years, but I had a first career. I was a journalist. I was
[00:27:39.520 --> 00:27:44.000]   correspondent and I worked at the Wall Street Journal and at Newsweek, that's why I lived in
[00:27:44.000 --> 00:27:48.640]   Japan and China and I went to the Columbia University Graduate School of Journalism. So,
[00:27:48.640 --> 00:27:56.880]   you know, the comments about journalists exploiting tech news for traffic gain,
[00:27:56.880 --> 00:28:04.800]   I think is some of that may be true and we certainly live in a digital economy where
[00:28:04.800 --> 00:28:11.040]   attention is currency, unfortunately. But just because you produce, like, just because, you know,
[00:28:11.040 --> 00:28:16.080]   Brandle Monroe creates a really great XKCD that explains this very complicated problem,
[00:28:16.080 --> 00:28:20.400]   doesn't mean that my dad, you know, is reading it and it doesn't mean that my dad will read it
[00:28:20.400 --> 00:28:26.160]   even if I send it to him, right? So, part of the problem is, again, I, we're quick to blame
[00:28:26.160 --> 00:28:31.440]   institutions and we're, you know, we're quick to blame journalists lately. We are reluctant to
[00:28:31.440 --> 00:28:37.280]   blame ourselves in being complicit in some of this. But I wouldn't expect your dad to learn what
[00:28:37.280 --> 00:28:41.760]   meltdown inspector do. I would expect him to run updates and he doesn't.
[00:28:41.760 --> 00:28:42.320]   Right.
[00:28:42.320 --> 00:28:48.480]   You know, I mean, and, and, you know, I have it scheduled. I don't run automatic updates because
[00:28:48.480 --> 00:28:52.880]   sometimes, you know, the update doesn't, the right one to run at that time. But I have, you know,
[00:28:52.880 --> 00:28:58.320]   I have a very methodical way of constantly updating all my stuff, you know, and I have secondary
[00:28:58.320 --> 00:29:02.240]   and tertiary checks running on different things. The average person is not going to do that, but,
[00:29:02.240 --> 00:29:06.960]   like, my dad can't wait for me to come home to visit him. You know,
[00:29:06.960 --> 00:29:13.200]   and, and, like, at that point, you all the tech support. I mean, you know, and, and then he,
[00:29:13.200 --> 00:29:18.080]   six hours a week on the radio talking to normal people trying to convince them to do this,
[00:29:18.080 --> 00:29:18.960]   but you're absolutely right.
[00:29:18.960 --> 00:29:23.920]   Until something happens to them, they don't see the, you know, they won't, but that's the problem.
[00:29:23.920 --> 00:29:28.240]   So, um, I love the idea of how, you know, having some kind of digital.
[00:29:29.920 --> 00:29:34.720]   Uh oh, we lost your audio. Is that us or is that Amy? We'll call you right back, Amy.
[00:29:34.720 --> 00:29:40.880]   Oh, wait. Oh, you said that was Elbow and I know that happens all the time.
[00:29:40.880 --> 00:29:41.280]   Yep.
[00:29:41.280 --> 00:29:43.600]   No, I was, um,
[00:29:43.600 --> 00:29:46.720]   you were saying about how we can't control our technology.
[00:29:46.720 --> 00:29:49.840]   No, um, going forward.
[00:29:49.840 --> 00:29:51.920]   Our elbows came.
[00:29:51.920 --> 00:29:54.080]   The irony is not lost, but I was fired up.
[00:29:56.480 --> 00:30:01.920]   Going forward, we all have to develop a different kind of digital street smarts that is, is,
[00:30:01.920 --> 00:30:07.200]   you know, unlike, I think in the past, we're just surrounded by more and more complicated
[00:30:07.200 --> 00:30:11.280]   technologies. And so this is the kind of thing we're not going to pick up on our own.
[00:30:11.280 --> 00:30:16.160]   So who, so who do you say is responsible? I mean, we'll, our, our, our, our, is, is like,
[00:30:16.160 --> 00:30:20.160]   the every man, is your dad, is it up to him? Because I hate to do that.
[00:30:20.640 --> 00:30:28.480]   Well, so I mean, yeah, I would say part because, you know, we're now all connected, right? So
[00:30:28.480 --> 00:30:33.840]   this isn't an individual's problem. It's not just him. Yeah. No, it's, it's all of us. And to some
[00:30:33.840 --> 00:30:39.920]   extent, you know, I love my father, but my dad, like, jacking around and not updating his iPad,
[00:30:39.920 --> 00:30:47.040]   for whatever reason, ultimately, in some way impacts me because when we have a network of devices
[00:30:47.040 --> 00:30:52.960]   that are vulnerable in some way, it makes it easier for people to find, you know, the big payoffs,
[00:30:52.960 --> 00:30:57.520]   the big zero days, right? Like the big zero days that everybody that everybody's looking for,
[00:30:57.520 --> 00:31:05.040]   you know, well, and what we're doing now, what we've, but last year has been about,
[00:31:05.040 --> 00:31:08.480]   and certainly this year will be bad, and we'll find out about it at CES is installing a bunch
[00:31:08.480 --> 00:31:14.400]   of internet connected dumb devices in our houses that are, in many cases, not updateable
[00:31:15.200 --> 00:31:20.080]   and are not properly configured. And just, you know, trusting to luck that they don't,
[00:31:20.080 --> 00:31:25.360]   and again, you make the excellent point, Amy, it's not about protecting us as an individual is
[00:31:25.360 --> 00:31:32.480]   protecting the ecosystem. And we haven't, we're talking about a chip, like a chipset. We haven't
[00:31:32.480 --> 00:31:37.840]   even like, we have devices now that listen to us. So all of these digital assistants,
[00:31:37.840 --> 00:31:41.200]   I'm just waiting for the first exploit to come down the pipeline, because you know, it is,
[00:31:41.200 --> 00:31:47.520]   right at some point, it's sort of herd immunity, and we're definitely failing as that said herd
[00:31:47.520 --> 00:31:54.960]   immunity. I don't see an easy answer. I mean, again, I would go back to the job of journalism
[00:31:54.960 --> 00:32:02.240]   is so important these days. It's a shame that it's being dragged through the mud. But this is an
[00:32:02.240 --> 00:32:08.240]   example of, I would, I would say, Amy, that if your dad doesn't need to understand it, but if
[00:32:08.240 --> 00:32:13.280]   your dad were writing for the Wall Street Journal, it would behoove him to talk to experts, find
[00:32:13.280 --> 00:32:19.440]   out as much as he can, and do the best job he can explaining it to people who need to notice
[00:32:19.440 --> 00:32:24.480]   information. I mean, the easiest way to do it, here's this, I think we just, I think the four of us
[00:32:24.480 --> 00:32:28.320]   actually just came up with the solution. And the solution is we need some kind of,
[00:32:28.320 --> 00:32:31.680]   no, no, no, no, no. Here's the solution, because
[00:32:33.920 --> 00:32:40.880]   I think we need like a CDC for, oh, I love it. There we go. You're right. We need, we need a,
[00:32:40.880 --> 00:32:44.560]   you know, the Centers of Disease Control, if you stop and think about sort of their mandate and
[00:32:44.560 --> 00:32:48.640]   what they do and where they are and how they're funded, a lot of what they do is try to predict
[00:32:48.640 --> 00:32:56.080]   and manage epidemics to manage the overall health of people living in our country. I think that
[00:32:56.080 --> 00:33:03.600]   we're probably due for some kind of CDC that does exactly what we were talking about moments ago,
[00:33:04.320 --> 00:33:11.840]   you know, which is like, they're kind of is, they're kind of is. I'm not saying it does a good job.
[00:33:11.840 --> 00:33:18.160]   No, I, but there's the US computer emergency readiness team, which is part of the Department
[00:33:18.160 --> 00:33:25.440]   of Homeland Security. That's kind of what it's supposed to be. Yeah, but I, because I've looked
[00:33:25.440 --> 00:33:31.760]   at their work before and I just, it's not, it's, it's not the same. So Mike made a very, very good
[00:33:31.760 --> 00:33:37.440]   point, which was that we need to position ourselves so that everyday people have tools and, and on
[00:33:37.440 --> 00:33:42.320]   that website, if you show it again, there, you can follow it and on the site, they do every now
[00:33:42.320 --> 00:33:47.440]   and then list, right. So the current activity and they've got that alerts and tips. Yeah.
[00:33:47.440 --> 00:33:52.320]   They do have like PDFs and things that you can download, but, you know, it doesn't,
[00:33:52.320 --> 00:34:00.880]   we need a CDC level kind of sort of institution that has the, the press savvy, you know, that can,
[00:34:00.880 --> 00:34:06.880]   that can help educate the public. That's not your good point. CDC has been a huge boon, I think,
[00:34:06.880 --> 00:34:14.160]   to public health. Sure. That's right. And, and I would include digital, our digital ecosystem
[00:34:14.160 --> 00:34:21.120]   and how we behave inside of it and how we are affected by it as part of our overall system of
[00:34:21.120 --> 00:34:25.760]   public health. Right? Yeah, it is. Yeah. I mean, you know, I was just, this is a little off topic,
[00:34:25.760 --> 00:34:30.000]   but I was just thinking about this the other day. There's so many apps these days where
[00:34:30.640 --> 00:34:34.640]   one of the first things that asks you is, hey, if you want to get in touch with all of your friends,
[00:34:34.640 --> 00:34:38.160]   go ahead and click this button and we'll make sure that if your friends are on this app,
[00:34:38.160 --> 00:34:43.200]   then you can use it too. And then you upload your entire database of contact information
[00:34:43.200 --> 00:34:50.880]   to their database, your entire library of contact information. And it's just, it's a small thing,
[00:34:50.880 --> 00:34:56.880]   but it's essentially sort of breaking the trust of everybody who, who trusted you with their
[00:34:56.880 --> 00:35:00.640]   information, who trusted you with their phone number, their email, their address, whatever ends up
[00:35:00.640 --> 00:35:04.240]   getting updated to those cloud servers. And we all, yeah, we all do it. It might be opposite
[00:35:04.240 --> 00:35:12.480]   if an STD alert. Yeah. Yeah. Yeah. Absolutely. I mean, and to me, the most outrageous and, and
[00:35:12.480 --> 00:35:19.360]   underappreciated example of that is Google Photos, which I love, absolutely love Google Photos, but
[00:35:19.360 --> 00:35:23.920]   Google Photos, I can take pictures of all kinds of people, strangers, anybody, and uploads of Google
[00:35:23.920 --> 00:35:28.960]   Photos and then it asks me to say who they are. So I'm basically saying that's the
[00:35:28.960 --> 00:35:32.400]   case. Yeah. I didn't ask you permission. They didn't ask you permission, but Facebook's even
[00:35:32.400 --> 00:35:36.880]   worse than. Oh, yeah. And part of the problem is Facebook, for example, leaks information in ways
[00:35:36.880 --> 00:35:41.840]   that the user doesn't know. Yeah. So there's a friend of friend problem on Facebook, which is if
[00:35:41.840 --> 00:35:46.400]   you take a Facebook quiz, which isn't really a quiz, I know you really want to know which game
[00:35:46.400 --> 00:35:51.680]   of thrones character you are, but really, really what's happening is you're giving a lot of information
[00:35:51.680 --> 00:35:57.520]   to a third party, like let's say Cambridge Analytica. But worse than that, as soon as you
[00:35:57.520 --> 00:36:02.320]   agree to that, they're getting access, not to your information, but your friends' information.
[00:36:02.320 --> 00:36:06.880]   So you're leaking, and I don't think people know that, and you're leaking your friends' information.
[00:36:06.880 --> 00:36:11.360]   You probably, if you knew that, might not even do it. But nobody knows. Right. So again,
[00:36:11.360 --> 00:36:16.640]   so now let's just take that entire story, Leo, that you just told, and instead of photos,
[00:36:16.640 --> 00:36:22.240]   and instead of face of a game of thrones and a game, let's say that because what you're describing
[00:36:22.240 --> 00:36:30.880]   a communicable disease, right? So, so, so, which everybody seems to want to get. It's worse. It's
[00:36:30.880 --> 00:36:39.280]   like, give me smallpox. If this was the CDC, and if we use the same vocabulary, because part of the
[00:36:39.280 --> 00:36:46.160]   problem is currently, like everyday people lack the analogies to understand the impact of these
[00:36:46.160 --> 00:36:51.440]   systems and tools. So you could describe what exactly what you just said and replace Facebook
[00:36:51.440 --> 00:36:57.440]   with smallpox. And people would immediately understand that, you know, this is something that they
[00:36:57.440 --> 00:37:01.200]   should be paying attention to. We don't have the shared vocabulary. And I'm not comparing,
[00:37:01.200 --> 00:37:04.000]   I'm not saying that Facebook is smallpox. I am.
[00:37:04.000 --> 00:37:14.720]   I am. Actually, if you look at what sort of does, if you look at what cert does,
[00:37:14.720 --> 00:37:20.800]   I'm looking at the cert alert for meltdown inspector, and it's actually quite good. And they
[00:37:20.800 --> 00:37:24.960]   put it out on January 3rd. They updated it January 4th. They updated it again on Friday.
[00:37:24.960 --> 00:37:29.840]   Apparently they went home for the weekend. That's too bad. But so they're underfunded. But it's
[00:37:29.840 --> 00:37:34.960]   pretty good with links to vendors and vendor mitigations, lots of additional information,
[00:37:34.960 --> 00:37:41.760]   including the original research. They refer to the United Kingdom, which has a nice national
[00:37:41.760 --> 00:37:48.480]   cyber security center. So they're working with other governmental similar governmental agencies.
[00:37:48.480 --> 00:37:52.560]   This is right. But what they what they lack is the one thing I think you're right that they need
[00:37:52.560 --> 00:37:59.760]   to do, which is a better public facing presence. Yes. And it's not written. You you know, if you
[00:37:59.760 --> 00:38:04.880]   have some of the language and you're a little bit more technically inclined, it's a great site to
[00:38:04.880 --> 00:38:10.080]   be on. But it's for sissing means an IT pros. Yeah, that's right. That's right. And to be fair,
[00:38:10.080 --> 00:38:16.080]   the CDC has plenty of very top level public health and medical vocabulary and language and
[00:38:16.080 --> 00:38:21.280]   everything else. But the everyday person and journalists can also go on to that website and
[00:38:21.280 --> 00:38:27.920]   still make sense of what's there. So that's, you know, we need and we have people from the CDC
[00:38:27.920 --> 00:38:32.480]   who go out and speak. We've had movies made about the CDC. We CDC is part of it. Yeah.
[00:38:32.480 --> 00:38:38.720]   Make it sexy. Yeah. That's right. Yeah. Right. I like that. So I think we just solved the problem.
[00:38:38.720 --> 00:38:42.960]   We have the backbone of it. We just need to really step it up and thank goodness President Trump is
[00:38:42.960 --> 00:38:49.920]   right up on the cyber. Yep. And it's gonna be huge. It's clearly gonna get get right to work on this.
[00:38:49.920 --> 00:38:54.720]   I know Don watches. We have a cypher. So if we can get Joe Scarborough to talk about it.
[00:38:55.280 --> 00:38:59.360]   Let's take a break. This is a great conversation and we've just begun. I think we've done with
[00:38:59.360 --> 00:39:03.840]   Meltdown Inspector, but I do encourage everybody. I mean, the bottom line, you're watching this.
[00:39:03.840 --> 00:39:08.400]   You understand it. But tell your friends and family, make sure you update those systems,
[00:39:08.400 --> 00:39:14.480]   make sure you do those patches. It isn't, you know, perhaps as easy as just doing the operating
[00:39:14.480 --> 00:39:20.080]   system patches, there will be micro code patches for the processors. And I have to say, I've read
[00:39:21.040 --> 00:39:28.640]   reports, good reports from security experts who say in some cases, new hardware will be required.
[00:39:28.640 --> 00:39:33.840]   That's a big problem because nobody, I just bought an iMac Pro. Do I have to buy another one in a
[00:39:33.840 --> 00:39:42.240]   year? Go it away. I'm gonna throw it away. It's got Meltdown Inspector. So this got the STG.
[00:39:42.240 --> 00:39:47.920]   It's got the it's got the meltdown. But good news. I'm Cersei Lannister. That's the yeah. So that's
[00:39:47.920 --> 00:39:56.160]   the good news. Amazon Go opens tomorrow. This is in the Amazon headquarters is the first. But the idea
[00:39:56.160 --> 00:40:02.400]   is there's no humans. You walk in, it recognizes you. I presume from your phone, you pick up what you
[00:40:02.400 --> 00:40:09.520]   want, you walk out cameras, register what you bought on the shelves and other places and charge
[00:40:09.520 --> 00:40:15.600]   you for it on, I guess, Android pay or what they call Google pay now. Yeah, this is so
[00:40:15.600 --> 00:40:20.480]   Amazon pay. You have to register your phone when you walk in. Okay, so you have to say here's my phone,
[00:40:20.480 --> 00:40:26.320]   you scan it. It's now your phone is matched to you. And then presumably the system with all the cameras
[00:40:26.320 --> 00:40:31.600]   in the in the store taking pictures of you watching where you go. There are sensors on the products.
[00:40:31.600 --> 00:40:36.240]   I mean, it's like a Vegas minibar, right? If you take the wine off the shelf, you bought it.
[00:40:36.240 --> 00:40:40.000]   Right. But what was interesting about it was that apparently in this demo store, there were still
[00:40:40.000 --> 00:40:44.000]   quite a few employees standing around to do things like ID checks on the alcohol section.
[00:40:44.000 --> 00:40:47.440]   Yeah, of course. Interesting. And you know, if you have any experience,
[00:40:47.440 --> 00:40:51.680]   and this is the really the basic version of this, it's not that crazy when you think about
[00:40:51.680 --> 00:40:56.400]   self check out at Safeway or what have you, but I don't know how often you've done that. I'm forced
[00:40:56.400 --> 00:41:02.880]   to do it all the time. It's a pain in the butt. There's always checkers having to come over and
[00:41:02.880 --> 00:41:07.680]   help people with their transactions for all kinds of reasons. And those particular those
[00:41:07.680 --> 00:41:12.800]   already existing systems don't let you take alcohol through them usually. So I think it's
[00:41:12.800 --> 00:41:18.480]   it's a proof of concept and and maybe a little bit getting people used to the idea that you
[00:41:18.480 --> 00:41:22.320]   don't have to interact with a human because kind of like the autonomous driving conversation,
[00:41:22.320 --> 00:41:27.680]   it's something that people will feel uncomfortable with for a long time until they do it and they
[00:41:27.680 --> 00:41:31.440]   see those charges pop up easily. And they'll just be like, wait, I can just come in and go,
[00:41:31.440 --> 00:41:35.200]   how are you going to know and what's going to burn? Why are you tracking me? And as I think
[00:41:35.200 --> 00:41:40.000]   they're just getting people used to this idea. I that's really you think people are going to
[00:41:40.000 --> 00:41:44.560]   have a hard time because I can't wait. I think that's exactly what I would love for a corner market.
[00:41:44.560 --> 00:41:48.720]   I just walk in, take what I want and leave. I think people are not going to like the tracking
[00:41:48.720 --> 00:41:54.880]   part of it. Oh, come on. I feel like they're stealing. If you're carrying that, that would be it.
[00:41:54.880 --> 00:41:59.360]   That would be it. Lindsay is that I would worry that it didn't actually check the sensor and I've
[00:41:59.360 --> 00:42:06.160]   now just stolen a whole bunch of that's even better. That's even better chasing you down the block.
[00:42:06.160 --> 00:42:10.000]   That's the actual thieves that are like, I stole this and they're like, no, actually,
[00:42:10.000 --> 00:42:16.080]   it's been charged to you. One of the New York Times reporter tried to steal, actually tried to
[00:42:16.080 --> 00:42:22.800]   shoplift and he got charged for it anyway. So don't, but I think that that's a bonus. I think
[00:42:22.800 --> 00:42:26.800]   there maybe it's a guy thing. I don't know. I think I may. I don't know. A lot of guys would say,
[00:42:26.800 --> 00:42:31.680]   hey, if they don't charge me, they don't charge me. I'm just going to take it. What I can't believe
[00:42:31.680 --> 00:42:36.320]   you would feel guilty about that. I think that people are just going to be like, like we were
[00:42:36.320 --> 00:42:41.600]   talking about earlier, people feel afraid of change. This is a big change. What if it doesn't
[00:42:41.600 --> 00:42:44.880]   work? It just takes time for people to get used to it. Although I will say that the idea of not
[00:42:44.880 --> 00:42:50.720]   having to show people what I'm buying is great. Right? It's all tracked, right? So it's not really,
[00:42:50.720 --> 00:42:54.560]   they know what you're buying. Look at it. You don't think they don't know what you're buying
[00:42:54.560 --> 00:43:00.080]   at Walmart? I mean, come on. Something really embarrassing. Someone can now. Okay, I won't
[00:43:00.080 --> 00:43:07.760]   say cash. They now know that you bought whatever cream for whatever. I think they know that anyway.
[00:43:07.760 --> 00:43:14.160]   Embarrassment is a very 20th century idea. It's gone. There's no more embarrassment. It's over.
[00:43:14.160 --> 00:43:18.080]   Just everybody knows everything. I'm mostly embarrassed about the cheap ros.
[00:43:18.080 --> 00:43:21.920]   Actually, you should do it. It's a embarrassing. Yeah, don't buy that there.
[00:43:21.920 --> 00:43:26.320]   So Nick Wingfield, who was writing the article, said he tried to trick the camera system by
[00:43:26.320 --> 00:43:32.960]   wrapping a shopping bag around a $4.35 four-pack of vanilla soda while he was still on the shelf,
[00:43:32.960 --> 00:43:38.640]   then tucked it under his arm and walked out of the store. He still got charged. Sorry, Nick.
[00:43:38.640 --> 00:43:44.960]   Thanks for shopping. You just bought four dollars and 35 cents with a dry soda that you didn't want.
[00:43:44.960 --> 00:43:50.480]   Was there an RF idea as part of this? I didn't see that in this piece.
[00:43:50.480 --> 00:43:52.000]   No, they invited us all for cameras.
[00:43:52.000 --> 00:43:57.520]   cameras, your location in the store and added to shots of you.
[00:43:57.520 --> 00:44:01.680]   I feel bad about people who work at 7-Eleven stores.
[00:44:01.680 --> 00:44:10.800]   Or any stores. Or bodegas. We are going to see fewer and fewer entry level jobs in general.
[00:44:10.800 --> 00:44:18.480]   Drivers, Uber drivers, convenience stores. What is more convenient than have? You could put
[00:44:18.480 --> 00:44:21.920]   little ones of these everywhere. You don't have to put wine in them. You just put little ones
[00:44:21.920 --> 00:44:27.040]   of them everywhere and you just walk in, grab something and go. I think that's the future.
[00:44:27.040 --> 00:44:32.000]   I'm going to predict this five years from that. I still have to walk in, though, Leo. That's the
[00:44:32.000 --> 00:44:36.240]   problem is that I don't want to have to walk in. Once I've walked in, I actually prefer to have a
[00:44:36.240 --> 00:44:41.120]   cashier than doing a self-checkout. I don't know. I like the process. I might be the only one.
[00:44:41.120 --> 00:44:43.760]   I don't like the self-checkouts, but that's mostly because they don't work very well. If I hadn't
[00:44:43.760 --> 00:44:47.520]   been wrong, they've delivered me all the stuff, then I'm all in.
[00:44:47.520 --> 00:44:53.040]   I mean, I do that too. I literally have an Amazon delivery every single day,
[00:44:53.040 --> 00:44:56.320]   several usually. I think it's going to be a hybrid. I think at some point, it may turn out to be
[00:44:56.320 --> 00:45:01.840]   sort of a luxury experience to have a check out. It's like in Oregon and New Jersey,
[00:45:01.840 --> 00:45:05.600]   where you pump your cash. You'd be like, well, not Oregon, not anymore. Not anymore.
[00:45:05.600 --> 00:45:08.320]   I don't know. Those poor people in Oregon are going to do. They have to figure out.
[00:45:08.320 --> 00:45:12.880]   It's so confusing. It's try-out panic. Wait a minute. I'm going to put a hose in my car.
[00:45:12.880 --> 00:45:16.400]   What are you talking about? That's why they need electric vehicles.
[00:45:16.400 --> 00:45:18.160]   They do. They need electric vehicles.
[00:45:18.160 --> 00:45:19.760]   And autonomous ones too.
[00:45:19.760 --> 00:45:23.760]   No, I'm very excited. I think this is a great idea. But again, I hate saying that because I feel
[00:45:23.760 --> 00:45:26.800]   bad about people who will be losing their jobs because maybe not.
[00:45:26.800 --> 00:45:30.400]   I don't know. I'm trying to be optimistic about this because I wanted to get the same thing.
[00:45:30.400 --> 00:45:34.240]   But somebody has a stock. Somebody has to polish everything. There may be somebody who's
[00:45:34.240 --> 00:45:38.400]   there sort of a concierge. Your job is a convenience store polisher.
[00:45:38.400 --> 00:45:44.960]   I don't know. But maybe this is over. Do they polish things at convenience stores?
[00:45:44.960 --> 00:45:47.520]   They do. They guess they have to.
[00:45:47.520 --> 00:45:49.040]   They've got to wipe those fingerprints out the door.
[00:45:49.040 --> 00:45:50.560]   They're like shiny.
[00:45:50.560 --> 00:45:51.840]   Modern.
[00:45:51.840 --> 00:45:54.080]   If you think about the number of people who actually work at a grocery store,
[00:45:54.080 --> 00:45:58.320]   department store, and who are actually, you know, rigging up customers, it's not 100%
[00:45:58.320 --> 00:46:01.760]   buying a mean. So there certainly will be a lot of other things to do at these stores, I would think.
[00:46:01.760 --> 00:46:06.240]   Yeah, I'm super excited about the potential as well. There are definitely times where,
[00:46:06.240 --> 00:46:09.120]   you know, just going to a gas station and needing a couple things. It would certainly be nice
[00:46:09.120 --> 00:46:12.560]   to just walk and grab it and walk out again rather than standing in line, waiting for everybody else
[00:46:12.560 --> 00:46:15.600]   to get the coffee order, right? That kind of thing certainly would be very nice.
[00:46:15.600 --> 00:46:20.560]   Should we be worried? You got young kids. I got young kids. Georgia has young kids.
[00:46:20.560 --> 00:46:23.120]   I don't know. Tim, do you have young kids?
[00:46:23.120 --> 00:46:25.120]   I don't want to leave you out.
[00:46:25.120 --> 00:46:25.760]   Old dogs.
[00:46:25.760 --> 00:46:26.400]   You have dogs.
[00:46:26.400 --> 00:46:32.240]   Should we be worried about when they, I mean, they're in the workforce in seven,
[00:46:32.240 --> 00:46:32.960]   10 years.
[00:46:33.920 --> 00:46:35.680]   Will there be jobs for these kids?
[00:46:35.680 --> 00:46:40.960]   I think there will be. What I'm hoping is that they then use this moment to find their own jobs,
[00:46:40.960 --> 00:46:44.720]   right? And so that my, hopefully that my kids, once they're in college, they're looking at
[00:46:44.720 --> 00:46:49.760]   opportunities and they're saying, oh, I am going to be an autonomous vehicle cleaner, right?
[00:46:49.760 --> 00:46:51.120]   Like I'm going to work for the new seminar.
[00:46:51.120 --> 00:46:52.080]   Let me polish it.
[00:46:52.080 --> 00:46:53.680]   I mean, that's probably a really gross job.
[00:46:53.680 --> 00:46:59.200]   But, but that there may be some entrepreneurial moments for them.
[00:47:00.320 --> 00:47:05.760]   And you know, there's still a lot to do. Like my yard is still not, there are no robots yet to
[00:47:05.760 --> 00:47:08.160]   trim my trees. Oh, that's just a moment. There's plenty.
[00:47:08.160 --> 00:47:09.200]   The most sure.
[00:47:09.200 --> 00:47:12.080]   There's self-timming trees. I think there's a crisp, crisp,
[00:47:12.080 --> 00:47:19.360]   self-timming trees. I don't know. You know, it's so funny because we, people who are in this field
[00:47:19.360 --> 00:47:23.840]   are kind of science fiction fans, many of us. And we, we, at the, on the one hand, we want to live
[00:47:23.840 --> 00:47:29.520]   in the sci-fi future. But the reality of it may not be quite so wonderful. And I don't know.
[00:47:29.520 --> 00:47:33.760]   I mean, I just don't know. I think it's inexorable. I think it's kind of hard to say, stop.
[00:47:33.760 --> 00:47:36.400]   We've got to keep the entry-level jobs. That's not going to happen.
[00:47:36.400 --> 00:47:39.520]   Well, I actually feel like I was thinking about this a lot the other day.
[00:47:39.520 --> 00:47:43.520]   At what point are we actually living in the sci-fi future? When we live there now,
[00:47:43.520 --> 00:47:46.560]   I think so. I think we feel like we're going to have a day.
[00:47:46.560 --> 00:47:50.320]   I just, you know, today I walk into my office, I say echo, turn on the lights.
[00:47:50.320 --> 00:47:50.800]   Yeah.
[00:47:50.800 --> 00:47:52.560]   You live there now.
[00:47:52.560 --> 00:47:54.720]   Yeah. We live in some ways we are living there now.
[00:47:54.720 --> 00:47:59.120]   We'll get back to our best of in just a second. I do want to thank our sponsors.
[00:47:59.680 --> 00:48:04.720]   All year long, Euro in particular. Euro has been a sponsor for a couple years. In fact,
[00:48:04.720 --> 00:48:10.160]   Euro joined us right at the very beginning when they first came out. That's when I started using
[00:48:10.160 --> 00:48:18.720]   Euro. They've moved on. Euro 2.0 is now out. And so I upgraded our home system to the new version
[00:48:18.720 --> 00:48:24.640]   of Euro. And I gave the old one to my mom who loves it. We're all Euroed up in the La Porte family.
[00:48:24.640 --> 00:48:31.280]   What is Euro? Well, it's the solution for bad Wi-Fi in a nutshell. The single router model
[00:48:31.280 --> 00:48:38.000]   these days, it doesn't work that well because we all have more devices on our Wi-Fi. We have
[00:48:38.000 --> 00:48:43.200]   neighbors with big hole. I feel like sometimes the neighbors must have big antennas aimed at us
[00:48:43.200 --> 00:48:49.680]   because they interfere with our Wi-Fi. It's physics like light waves, Wi-Fi waves,
[00:48:49.680 --> 00:48:53.920]   don't really travel very well around corners through walls, upstairs and downstairs.
[00:48:54.560 --> 00:48:59.040]   It's like asking a light bulb in your living room to light the whole house. So you need something
[00:48:59.040 --> 00:49:05.040]   a little bit smarter. You need a distributed system now and years gone by. We've had distributed
[00:49:05.040 --> 00:49:09.760]   systems, for instance, a twit for a decade. Most businesses have distributed systems.
[00:49:09.760 --> 00:49:15.040]   But that's been complicated, required in IT professional. It's not easy to do and it is
[00:49:15.040 --> 00:49:21.840]   absolutely not expensive. Fast forward to the present times we have a better choice. We have
[00:49:21.840 --> 00:49:28.640]   ERO, E-E-R-O brings that distributed Wi-Fi model. The enterprise grade Wi-Fi system to your home,
[00:49:28.640 --> 00:49:34.960]   set it up in a few minutes. You don't need an IT professional. It is not expensive and it really
[00:49:34.960 --> 00:49:39.920]   works. And I'm here to tell you, I use it and I know you put the ERO app in your iOS or Android
[00:49:39.920 --> 00:49:46.640]   device. It walks you through the setup. It's painless. You'll plug in the first unit, the base unit.
[00:49:46.640 --> 00:49:50.880]   And then you have a couple of beacons. Depends on the size of your house. The typical kit has
[00:49:50.880 --> 00:49:55.520]   one base and two beacons. Those beacons, no wires. They just plug into the wall. But you're going to
[00:49:55.520 --> 00:50:00.480]   use the app to help place them. You place them at strategic points. I like to keep them in halls.
[00:50:00.480 --> 00:50:04.960]   Because not only, I feel like the bandwidth is, it's like the highway for the bandwidth, but also
[00:50:04.960 --> 00:50:08.640]   the beacons are great because they have a little light on them. They make great nightlights. So
[00:50:08.640 --> 00:50:13.440]   they light up our halls. You can turn those off if you don't want them to do that. And then
[00:50:13.440 --> 00:50:19.360]   you've got Wi-Fi throughout your house and the ERO app lets you manage the network
[00:50:19.360 --> 00:50:24.800]   in the palm of your hand. You'll know how many devices are connected to any given access point.
[00:50:24.800 --> 00:50:29.600]   You'll know the internet speed you're getting. They test it nightly. You'll know, I mean, all sorts
[00:50:29.600 --> 00:50:35.280]   of stuff. Plus, they've got this new ERO Plus feature that I adore. Now, this is a subscription
[00:50:35.280 --> 00:50:41.040]   feature you don't have to get. You'll get all the benefits of ERO without it. But ERO Plus
[00:50:41.040 --> 00:50:48.000]   really is a nice add-on. It's simply design reliable security that defends you against threats
[00:50:48.000 --> 00:50:53.600]   in the outside world. Blocks malware from the whole network. Blocks spyware from the whole
[00:50:53.600 --> 00:50:59.520]   network phishing attacks. Unsuitable content. If you have kids in the house and we have a 16-year-old,
[00:50:59.520 --> 00:51:05.520]   I can just block the devices he uses from adult content, gambling, violence, that kind of thing
[00:51:05.520 --> 00:51:10.560]   on the internet. But we can all be blocked, all be prevented from getting malware. I really love
[00:51:10.560 --> 00:51:16.320]   that. You can also check the sites you visit. Well, actually, this is what it does automatically
[00:51:16.320 --> 00:51:22.320]   against their database of known threats. So it keeps you from even going to malicious sites.
[00:51:22.320 --> 00:51:28.880]   But it doesn't slow you down. That's the beauty part of it. It's actually a deal. Take a look.
[00:51:28.880 --> 00:51:33.600]   ERO Plus automatically tags sites that contain violent, illegal, or adult content. So you can
[00:51:33.600 --> 00:51:41.200]   choose what your kids can and cannot visit right in the ERO app. What else? Oh, you get a guest network
[00:51:41.200 --> 00:51:46.640]   that's completely separate from your network state of the art WPA2 encryption. I can't sell this
[00:51:46.640 --> 00:51:52.320]   enough. They don't mention it, but regular firmware updates. Very important nowadays. Don't buy a
[00:51:52.320 --> 00:51:58.080]   router that doesn't update automatically. The ERO does. They're constantly working to make sure
[00:51:58.080 --> 00:52:03.920]   it's the most secure, has the latest features and the latest security. And I have to admit,
[00:52:03.920 --> 00:52:09.600]   I when I first got the ERO, I had some questions about how to set it up and stuff. I called support.
[00:52:09.600 --> 00:52:15.120]   They were so great. You'll get a Wi-Fi expert fast within 30 seconds. And what was nice is they
[00:52:15.120 --> 00:52:20.560]   stayed on the phone with me till I said, "Yep, it's all lit up. They really walked me through it.
[00:52:20.560 --> 00:52:26.160]   Great support. What else can I tell you about them?" The guest network, WPA2 encryption, they've
[00:52:26.160 --> 00:52:31.200]   said already that they'll be able to upgrade to the new WPA3 and software as soon as that's ready.
[00:52:31.200 --> 00:52:37.680]   You know, as soon as that standard is defined, you can look at the ERO app at any time and see
[00:52:37.680 --> 00:52:43.280]   everything that's going on. I just love this thing. Look, I know the worst thing any geek can hear.
[00:52:43.280 --> 00:52:51.440]   Honey, the Wi-Fi's down. I used to get that all the time until we got an ERO. Fast, reliable,
[00:52:51.440 --> 00:52:57.040]   enterprise-grade networking for your home. You want this. Go right now. We're going to have a
[00:52:57.040 --> 00:53:02.960]   great deal for you. The $100 off the ERO base unit plus two beacons and a year of ERO plus.
[00:53:02.960 --> 00:53:07.360]   All you have to do is go to ERO, ERO.com/Twit and enter the code "Twit" to check out.
[00:53:07.360 --> 00:53:13.200]   They're giving us a very nice holiday gift. $100 off. You get the base unit, two beacons,
[00:53:13.200 --> 00:53:21.840]   and a year of ERO plus. All in one package and $100 off, E-E-R-O.com/Twit. Check out,
[00:53:21.840 --> 00:53:30.720]   you can enter "Twit". It's great. It's great. ERO.com/Twit. Enter the code "Twit" at checkout for 100 bucks off.
[00:53:30.720 --> 00:53:36.960]   Now back to "Twit". What the hell is Elon Musk making a flamethrower for the boring company?
[00:53:37.600 --> 00:53:42.320]   Because you can. What is that? It's brilliant marketing.
[00:53:42.320 --> 00:53:47.600]   It's useful. They're useless in a zombie apocalypse. Everyone's saying, "Well,
[00:53:47.600 --> 00:53:51.040]   maybe the boring company has gone down and opened and that latter gates are hell.
[00:53:51.040 --> 00:53:54.800]   Flamethrower is useless against zombies, particularly fast-worms. You need a good axe.
[00:53:54.800 --> 00:53:57.120]   That's all we're cricket back. Should I free order it?
[00:53:57.120 --> 00:54:03.040]   It's got to be illegal, right? No, no. Perfect illegal. It's legal in most states.
[00:54:03.040 --> 00:54:08.560]   It's not legal here. There's a guy in Minnesota who was making up laws quickly.
[00:54:08.560 --> 00:54:15.360]   Not legal here. There was a guy in Minnesota who broadcast himself on social media using one of
[00:54:15.360 --> 00:54:23.120]   these things to clear the snow off his front drive. Why is Elon saying, obviously, a flamethrower is
[00:54:23.120 --> 00:54:29.440]   a super terrible idea. Definitely don't buy one. Yet, they are apparently offering pre-orders.
[00:54:30.880 --> 00:54:35.280]   Unless you like fun is his subsequent tweet. He says, "These should be legal. The Bureau of
[00:54:35.280 --> 00:54:39.280]   Alcohol to Back on Firearms allows throwers with flames shorter than 10 feet."
[00:54:39.280 --> 00:54:44.240]   Not really a thrower. To me, more of an advanced cigarette lighter.
[00:54:44.240 --> 00:54:52.880]   Good light. Putting a lightsaber with two sabers on either side. It's just asking for trouble.
[00:54:52.880 --> 00:54:55.360]   I mean, you've got to promote your company one way or the other.
[00:54:55.360 --> 00:55:00.240]   Well, it worked. Did you see that there was some corresponding articles this week about the
[00:55:00.240 --> 00:55:04.480]   pointlessness of the boring company? This idea of running a tunnel from one side to the other,
[00:55:04.480 --> 00:55:08.960]   just big enough so that you can put a car in it and fundamentally restrict the incident for the
[00:55:08.960 --> 00:55:16.320]   elite. You can't move. Again, what's wrong with that? No, that's what Elon says. I don't like
[00:55:16.320 --> 00:55:21.680]   public transit. Why would you want to be in a train with a lot of other people? Because it's
[00:55:21.680 --> 00:55:26.400]   really efficient at moving large numbers of people from point A to point B. Which the boring company
[00:55:26.400 --> 00:55:32.000]   is not. If you can only have a boring bus, you can have a charge for one car. Because you dug a
[00:55:32.000 --> 00:55:35.840]   hole in there in a way. Because you dug a hole for that distance. At the end of the day, it's
[00:55:35.840 --> 00:55:41.680]   actually not going to solve the transport problem or the society as a whole. Because it's only going
[00:55:41.680 --> 00:55:46.320]   to move a few hundred people at a time. At most. Like at any given interval, you're only going to
[00:55:46.320 --> 00:55:51.360]   have a few hundred cars inside the tunnel. The vendor, you said something I thought fairly controversial.
[00:55:51.360 --> 00:55:56.000]   Oh no. Which was that the iPhone X was Apple's last great product. Does that...
[00:55:56.560 --> 00:56:00.400]   Since the only other product since then... It wasn't that long ago. It was like the HomePod.
[00:56:00.400 --> 00:56:07.040]   I'm thinking, is that a backdoor disk for the HomePod? I mean, the HomePod was delayed. And even
[00:56:07.040 --> 00:56:12.400]   now, it's not fully ready. Brian's review is really good. Don't rush to buy it, Sisbri.
[00:56:12.400 --> 00:56:18.240]   Well, it's a half-baked product right now. I don't know. I kind of feel like I was a little bit
[00:56:18.240 --> 00:56:23.120]   harder on it than other people. Do you guys feel that way? You weren't as hard on it as I was.
[00:56:23.120 --> 00:56:28.320]   Really? Okay. Because I felt like it just feels like it's unfinished.
[00:56:28.320 --> 00:56:34.720]   Siri does a lot more on the iPhone than it does on the HomePod. And so it's been training us to
[00:56:34.720 --> 00:56:39.840]   use Siri in a certain way. And yet, things like the calendar, which I use a lot with Siri,
[00:56:39.840 --> 00:56:43.840]   doesn't work on the HomePod. Things like hailing a car doesn't work on Siri
[00:56:43.840 --> 00:56:49.760]   on the HomePod, but it works on the iPhone and other things like that. So it just feels like,
[00:56:49.760 --> 00:56:53.200]   I don't know why they released it when it's not really done. I mean,
[00:56:53.200 --> 00:56:57.520]   indisputably, it's not done because that multi-room speaker feature is not out yet. So
[00:56:57.520 --> 00:57:03.920]   what I don't really get is if they're already late, why even come out when it's unfinished?
[00:57:03.920 --> 00:57:08.640]   She kind of makes it hard for me to write about because I'm like, well, the music part sounds
[00:57:08.640 --> 00:57:12.240]   great, but all this other stuff doesn't work. And that's awkward.
[00:57:12.240 --> 00:57:16.400]   Devindra, you agree? I mean, I've only listened to it a bit. Chris will ask,
[00:57:16.400 --> 00:57:21.600]   go down, gadget is going to be reviewing that for us. It sounds really good because it's a
[00:57:21.600 --> 00:57:27.200]   $350 super high-tech speaker. But yeah, I agree. The Siri stuff is it feels really
[00:57:27.200 --> 00:57:31.840]   unfinished, really weak in comparison to what we're getting from Alexa speakers.
[00:57:31.840 --> 00:57:37.600]   I also tested out the the Harman Carden Invoke, which is the Cortana speaker. And even that felt
[00:57:37.600 --> 00:57:41.760]   more fully featured than the HomePod. That thing's just 200 bucks.
[00:57:43.280 --> 00:57:48.160]   So did either of you get invited to the briefings that Apple had in Cooper?
[00:57:48.160 --> 00:57:52.640]   Yeah, I went to the HomePod briefing about two weeks ago. We went to the
[00:57:52.640 --> 00:57:55.120]   audio lab tour where they showed us how they tested.
[00:57:55.120 --> 00:57:59.120]   This was supposed to, you know, you're not getting the cool aids.
[00:57:59.120 --> 00:58:06.640]   So let me channel Tim Cook here and say, but what you guys don't understand, it's not a Siri device.
[00:58:07.280 --> 00:58:14.800]   It's a speaker. It's all about the sound. We're reinventing how stereos, how speakers
[00:58:14.800 --> 00:58:21.120]   work. The Siri capability is merely to play music. It's not a, you're thinking it's an echo.
[00:58:21.120 --> 00:58:25.840]   It's not an echo. It's a new kind of music listening device.
[00:58:25.840 --> 00:58:27.680]   But yeah, so it was Phil Schiller who said that.
[00:58:27.680 --> 00:58:30.080]   Oh, sorry. No, no, we're channeled the wrong guy.
[00:58:30.080 --> 00:58:34.960]   Basically, he was saying like, so he's giving us a history of how the HomePod was made, right?
[00:58:34.960 --> 00:58:39.520]   So it's been in development for six years on and off. Like they tabled it for a little bit in between.
[00:58:39.520 --> 00:58:44.720]   And so they started it before Apple music even launched, before they even bought
[00:58:44.720 --> 00:58:51.360]   Beats even. And before HomeKit came out, right? So the HomePod comes out after this. I mean,
[00:58:51.360 --> 00:58:54.480]   it's in development before that, but all this stuff comes out in the middle.
[00:58:54.480 --> 00:58:58.800]   And then suddenly they had to get this HomePod to work with these new services that Apple has,
[00:58:58.800 --> 00:59:04.080]   you know? So that's, you can kind of get the sense of why it was so chaotic to develop this thing.
[00:59:04.080 --> 00:59:07.920]   But then when you try, you're going to realize like, hmm, this thing doesn't really seem like
[00:59:07.920 --> 00:59:13.280]   it's that polished. It doesn't really seem like it's done. So I mean, how much can you justify it
[00:59:13.280 --> 00:59:17.200]   being that dumb? You know, it sounds amazing. It sounds great. But if you're competing--
[00:59:17.200 --> 00:59:18.640]   It's where I would disagree.
[00:59:18.640 --> 00:59:19.280]   OK.
[00:59:19.280 --> 00:59:20.800]   I don't think it sounds that amazing.
[00:59:20.800 --> 00:59:21.280]   OK.
[00:59:21.280 --> 00:59:24.880]   I think Apple fails on all fronts. That sounds good. It sounds fine.
[00:59:24.880 --> 00:59:25.360]   It's good.
[00:59:25.360 --> 00:59:26.400]   That's the first I've heard.
[00:59:26.400 --> 00:59:26.880]   OK.
[00:59:26.880 --> 00:59:29.200]   Yeah, but look it. I'll read some of the headlines.
[00:59:29.200 --> 00:59:33.360]   Apple HomePod review. This is Joanna Stern. Super sound, but not super smart.
[00:59:33.360 --> 00:59:38.240]   They agreed with you. The I'more folks. Retina for your ears.
[00:59:38.240 --> 00:59:40.880]   Yeah, I don't know about it.
[00:59:40.880 --> 00:59:48.080]   Oh, Neilai Patel in the Verge said, "I am a speaker fanatic," and he showed off his analog stereo.
[00:59:48.080 --> 00:59:53.360]   And I think this sounds amazing. But there's a problem. There's something called physics.
[00:59:53.360 --> 00:59:59.680]   And I know that, you know, the Australian Prime Minister thinks that physics is not an immutable law,
[00:59:59.680 --> 01:00:04.640]   but I think physics is an immutable law. And this, which is not a HomePod, it's just an amazing
[01:00:04.640 --> 01:00:12.640]   simulation. This is too small to really be a great speaker. So I think if you're going to say it
[01:00:12.640 --> 01:00:16.400]   sounds good, you really should add the clause. It sounds good for its size.
[01:00:16.400 --> 01:00:20.480]   Sure. Yeah, that's a good point. And it definitely does fill up the whole room, like,
[01:00:20.480 --> 01:00:21.680]   like a small size.
[01:00:21.680 --> 01:00:25.280]   That's one thing that's interesting because it has two ears all around it, so it doesn't need--
[01:00:25.280 --> 01:00:28.000]   You don't need to sit in front of it. Everywhere is equal.
[01:00:28.000 --> 01:00:31.440]   Yeah, and that's it. I'm not an audio file, so I'm not going to pretend that I know
[01:00:31.440 --> 01:00:36.320]   great details about these seven tweeters and all that stuff and have a great attention to detail
[01:00:36.320 --> 01:00:42.320]   when it comes to sound. But it sounds really good to me compared to a lot of speakers I've tried.
[01:00:42.320 --> 01:00:47.440]   But I don't think that really justifies the fact that Siri is quite inadequate compared to how
[01:00:47.440 --> 01:00:52.560]   it even is on the iPhone or how it's compared to Alexa even. Here's a thought experiment, guys.
[01:00:52.560 --> 01:00:57.680]   So imagine if Apple had waited another year and made all those features work that are missing on
[01:00:57.680 --> 01:01:03.760]   Siri and released it as it is with nice music, nice sound, and all those nice features working
[01:01:03.760 --> 01:01:08.480]   as a virtual assistant. Well, there's another problem. It only works with Apple Music. Here's my
[01:01:08.480 --> 01:01:15.360]   opinion. That's another. This is designed for somebody who is bathing in the warm glow of the
[01:01:15.360 --> 01:01:20.480]   Apple Pass. This is for somebody because you can't even set it up if you don't have an iPhone or
[01:01:20.480 --> 01:01:26.080]   an iPad. You can't even a Mac or a Mac. You cannot use it without an iPhone or an iPad.
[01:01:26.080 --> 01:01:32.640]   Wow. That's insane. So this is a product designed for somebody who's absolutely in the...
[01:01:32.640 --> 01:01:38.080]   I mean, I'm not going to say Apple sheep, but somebody who's really in the Apple ecosystem
[01:01:38.080 --> 01:01:42.240]   wants something that's already a subscriber to Apple Music, wants something that works with
[01:01:42.240 --> 01:01:49.520]   Apple Music. It sounds fine for its size. My issue is we did a comparison yesterday on the new
[01:01:49.520 --> 01:01:54.640]   screensavers. We compared the Envok, the Harmon Carden, which did not sound as good. The Sonos
[01:01:54.640 --> 01:02:02.480]   one, which is Sonos's small Play One speaker with Amazon Echo built in, which actually wasn't as
[01:02:02.480 --> 01:02:07.680]   good, but was close and it's half the price you could buy too and get a stereo pair. That might
[01:02:07.680 --> 01:02:11.520]   be fairly competitive. We compared it to... Or could it play three in an Echo Dot?
[01:02:11.520 --> 01:02:15.280]   Right. Like there are other ways to do that. Compared to a Play Three or a Play Five,
[01:02:15.280 --> 01:02:21.600]   I don't think the Apple's HomePod is as good. And I guess the point I would really make is if
[01:02:21.600 --> 01:02:26.640]   you want a good speaker, if you want good sound, this is how I think of it.
[01:02:26.640 --> 01:02:33.520]   I brought this up. Keith Richards, the guitarist for the Rolling Stone wrote most of their songs
[01:02:33.520 --> 01:02:39.200]   with Mick Jagger. He got his musical education listening to a tinny transistor radio in bed,
[01:02:39.200 --> 01:02:44.640]   listening to a pirate radio station out in the ocean. It sounded crappy, but that's how he heard
[01:02:44.640 --> 01:02:50.800]   howl and wolf and muddy waters and became a music fanatic. Paul McCartney, same thing. You can
[01:02:51.520 --> 01:02:57.280]   they're still getting some music, but there's all the music that you would hear in a live performance.
[01:02:57.280 --> 01:03:02.720]   And then with each smaller device, each less capable device, you get less and less of the music.
[01:03:02.720 --> 01:03:08.800]   If you're really into music, you should freaking get a stereo. Spend a little bit more money and
[01:03:08.800 --> 01:03:14.880]   get a stereo. Get two speakers and send it the apex of the triangle. And listen, and if I put
[01:03:14.880 --> 01:03:18.480]   the... And I did this, put the HomePod in the middle and listen to my stereo. It's not...
[01:03:18.480 --> 01:03:23.600]   It's like a different song. It's like different music. It's completely a different experience.
[01:03:23.600 --> 01:03:27.360]   You're getting a half a quarter of the full experience when you listen to the HomePod.
[01:03:27.360 --> 01:03:31.920]   So for Apple to say, "Well, no, no, it's not a Siri device. It's a new way of thinking about music."
[01:03:31.920 --> 01:03:37.440]   Is wrong. And you know, the deal breaker for me too, that I wrote about in the review,
[01:03:37.440 --> 01:03:43.280]   is it was picking the wrong music for me. Despite all the shortcomings of Siri or in addition to
[01:03:43.280 --> 01:03:47.600]   all the shortcomings, I was trying to go along with like, "Oh, this should be primarily about music.
[01:03:47.600 --> 01:03:52.400]   This should be great at playing music." And you know, in the Apple Music app, I selected all my
[01:03:52.400 --> 01:03:57.600]   favorite artists. They were like, "Smit, radio head, talking heads, blah, blah, blah." And instead of
[01:03:57.600 --> 01:04:02.720]   picking artists similar to mine, I would ask Siri to play some music and it would play Dashboard
[01:04:02.720 --> 01:04:08.000]   Confessional, which I'm sorry. I hate that band so much. I hate that. I can beat that. I hate it so
[01:04:08.000 --> 01:04:14.080]   much. I said, "Listen to classical music." It played old school hip hop. Classical for some,
[01:04:14.080 --> 01:04:21.760]   but not what I was going for. I like NWA as much as the next guy, but I was thinking something a
[01:04:21.760 --> 01:04:28.080]   little bit more mellow. I broke or something. Part of that is because Apple is Apple Music's
[01:04:28.080 --> 01:04:31.440]   problem, not the HomePod's problem, right? I thought Apple Music was supposed to be better.
[01:04:31.440 --> 01:04:36.720]   So I'm not personally an Apple Music user. I use Spotify, but I had tested it before,
[01:04:36.720 --> 01:04:42.560]   and these 4U playlists, more or less, had some pretty spot-on playlists generated for me back
[01:04:42.560 --> 01:04:47.440]   in the day when I was testing this. Spotify is really good too. So when I first turned on the HomePod,
[01:04:47.440 --> 01:04:51.520]   I said, "Play some music." I expected it to look at my history of what I listened to on Apple Music
[01:04:51.520 --> 01:04:56.000]   and also the artist that I picked, which is pretty obvious. It was playing all this random stuff.
[01:04:56.000 --> 01:05:01.520]   Dashboard Confessional being one. Leeray Francis. I never listened to Selena Gomez. I don't even
[01:05:01.520 --> 01:05:10.480]   know who these people are. Somehow it's 17-year-old or something. I think in general that is Apple
[01:05:10.480 --> 01:05:17.280]   Music. It's aimed at a generation. Even you, or a lot you're going to be listening to,
[01:05:17.280 --> 01:05:19.600]   what I like. That's the whole point of it. So that was really the deal.
[01:05:19.600 --> 01:05:23.040]   By the way, go look at Apple Music for classical music. Good luck finding it.
[01:05:23.040 --> 01:05:32.960]   Seriously. Here's the thing though. You make a good point about if you really want the audio
[01:05:32.960 --> 01:05:40.560]   experience, then get a stereo. But the point of all these devices, the Sonos, the Echo, the Invoke,
[01:05:40.560 --> 01:05:47.520]   which I have one of here, and this HomePod, the whole point of it is that you shouldn't have to
[01:05:47.520 --> 01:05:52.960]   whip out a phone or an iPad. You should be able to just drive the thing by talking.
[01:05:52.960 --> 01:06:01.520]   And so even if it isn't as intelligent as the others, even if it doesn't have the full Siri
[01:06:01.520 --> 01:06:06.960]   experience, you should at least be able to go through the basics and say, "Play this, play that,
[01:06:06.960 --> 01:06:12.000]   play the other thing, get my entire music collection from the Cloud Play this album or that album."
[01:06:12.000 --> 01:06:15.200]   And from what I understand, it doesn't do that. And if it doesn't do that,
[01:06:15.200 --> 01:06:21.520]   I think Brian's absolutely right. Why did they ship this thing? There's a case to be made,
[01:06:21.520 --> 01:06:27.520]   sometimes for shipping things early, when they have to be out in the world, and people have to
[01:06:27.520 --> 01:06:33.280]   be interacting with them so that the engineers can get feedback and they can fine tune the thing
[01:06:33.280 --> 01:06:37.920]   quickly. But that's not the case here. They just shipped this thing before it was ready.
[01:06:37.920 --> 01:06:42.720]   And given the amount of money that Apple has in the bank, I just don't understand why.
[01:06:42.720 --> 01:06:46.960]   Yeah. It almost feels like they have competitive pressure because Amazon made a killing during
[01:06:46.960 --> 01:06:52.240]   the holidays. They slashed the price of the Echo to 85 from 99. I think those are the numbers.
[01:06:53.760 --> 01:06:58.160]   And so they have more pressure than ever to put this thing out. But at the same time,
[01:06:58.160 --> 01:07:03.200]   Apple has been saying for years, "We don't rush to be first. We're fine with being later
[01:07:03.200 --> 01:07:08.960]   and doing a better job than everybody else." And in this case, I think they could only say that
[01:07:08.960 --> 01:07:14.160]   maybe Leo disagrees, but they did better with audio quality, but they really just dropped the
[01:07:14.160 --> 01:07:18.560]   ball and everything else. It's decent for what it is. There are times when you would want a home
[01:07:18.560 --> 01:07:22.640]   pod. For instance, in the kitchen, it's great because you don't stand... I mean, if you listen
[01:07:22.640 --> 01:07:27.120]   to music, you don't always get to sit in the sweet spot with two speakers left and right.
[01:07:27.120 --> 01:07:30.720]   You're in the kitchen. You're cooking. You're moving around. And it's great for that because of
[01:07:30.720 --> 01:07:35.120]   the tweeters all the way around. And it sounds good no matter where you are. But it's a
[01:07:35.120 --> 01:07:39.440]   single... It's a mono speaker. They don't have any stereo option. Well, you can add a second
[01:07:39.440 --> 01:07:43.200]   speaker that plays at the same time next to it. You can. It's not going to turn the best software
[01:07:43.200 --> 01:07:46.880]   update to that. It's huge. You're up there. And that's my point, right? It's not done. So,
[01:07:46.880 --> 01:07:51.680]   why didn't they wait six months or something to ship a perfectly polished product that would have
[01:07:51.680 --> 01:07:59.120]   been really nice? I blame Apple a little bit because I think the white earbuds that they ship with the
[01:07:59.120 --> 01:08:06.880]   iPhone and the iPod before it degraded the musical tastes of a whole generation. That people who grew
[01:08:06.880 --> 01:08:12.240]   up with crappling compressed music on crappy earbuds... You mean the earpods or the... No, no,
[01:08:12.240 --> 01:08:19.600]   no, just for the original... Music for a whole generation meant an iPod with white earbuds in.
[01:08:19.600 --> 01:08:25.200]   And that is not, frankly, very good quality. And I don't want Apple to start saying how great this
[01:08:25.200 --> 01:08:30.880]   sounds because I think it's doing a disservice to people who could hear a lot more of the music.
[01:08:30.880 --> 01:08:37.120]   You can't get a speaker that size for 350 bucks that's going to be a great speaker. You can't. It's
[01:08:37.120 --> 01:08:43.520]   great for its size. And I think there are even better speakers for that price, frankly. Yeah.
[01:08:43.520 --> 01:08:50.480]   The Sonos Play 3 is what? 300? Yeah. I think and that thing sounds a lot better. I think so.
[01:08:50.480 --> 01:08:53.680]   It has more bass for one thing. It has more bass. But you know,
[01:08:53.680 --> 01:08:58.160]   Apple's credit, I think what they did really well with the HomePod is that you can push that
[01:08:58.160 --> 01:09:02.720]   thing to max volume and it wants to store it. It does handle. It does handle. Nothing distorts.
[01:09:02.720 --> 01:09:06.800]   I think partially because it doesn't really go as max as some other things, right? We were
[01:09:06.800 --> 01:09:11.440]   comparing it to the Google Max. And that thing goes much louder. And then you start to hear like
[01:09:11.440 --> 01:09:16.720]   the bass starts to distort and some of the high notes. So you could see that Apple's trying to
[01:09:16.720 --> 01:09:21.440]   balance something here. But I think you're onto something Leo, like everybody who grew up with
[01:09:21.440 --> 01:09:28.800]   iPods or just in general, we got used to okay sounding music. And most people just don't have
[01:09:28.800 --> 01:09:34.160]   speakers in their homes. I think that's a bigger thing. Even with Surat, maybe because
[01:09:34.160 --> 01:09:38.160]   people my age might be around. Yeah, because you're watching movies on your big screen.
[01:09:38.160 --> 01:09:42.960]   I have all that stuff, but I know I'm way in like the minority here. But I think one of the people
[01:09:42.960 --> 01:09:48.560]   that you think have have home theater systems, right? Maybe not. I don't know.
[01:09:48.560 --> 01:09:53.040]   It's a good portion of people, but it's not I'm talking like mainstream consumers. So like just
[01:09:53.040 --> 01:09:57.680]   listening to music, you know, in your home with speakers, it's something that kind of died off.
[01:09:57.680 --> 01:10:01.920]   And this is good for that. People my generation bought the stupid bows sound acoustic way.
[01:10:01.920 --> 01:10:05.440]   Oh God. Those were terrible. The Paul's harvies.
[01:10:05.440 --> 01:10:10.080]   That would be good. Yeah. I mean, there's so I don't know. And by the way, my big
[01:10:10.080 --> 01:10:16.560]   my ears are shot from years of wearing headphones and listening to rock and roll way too loud.
[01:10:16.560 --> 01:10:21.360]   But I can tell the difference. And so I figured that somebody younger with better ears owes
[01:10:21.360 --> 01:10:26.320]   themselves something a little bit better. The Sonos play three is 250 bucks, 100 bucks less.
[01:10:26.320 --> 01:10:30.560]   Oh, wow. So for 50 bucks more, you can get a play five, which really sounds good.
[01:10:31.360 --> 01:10:36.000]   And even then though, I wish you would spend a little more and get a stereo.
[01:10:36.000 --> 01:10:41.520]   Well, let's let's let the ultimate dream. I would wish more consumers would do that. But in
[01:10:41.520 --> 01:10:46.000]   what I love about these devices is that Sonos and everything, like it made it easy for everybody
[01:10:46.000 --> 01:10:51.360]   to bring music out loud back into their homes. The big downside with HomePod for me is that it's
[01:10:51.360 --> 01:10:56.160]   so limited, right? It's limited to Apple music, but we're so many people are using Spotify now,
[01:10:56.160 --> 01:11:00.560]   or you have your library of music on other devices. You know, there's no aux jack, of course.
[01:11:01.280 --> 01:11:05.760]   There's none of the things you see even in simple Bluetooth speakers these days.
[01:11:05.760 --> 01:11:09.840]   I really like what Logitech's been doing, honestly. Like they just released their new lineup,
[01:11:09.840 --> 01:11:13.840]   which has Alexa built in. And they're also like Bluetooth portable speakers that you can take
[01:11:13.840 --> 01:11:17.440]   around everywhere. You could put them in the shower, you could take them to the beach. Like,
[01:11:17.440 --> 01:11:21.120]   we've there's so much innovation happening here. Like what HomePod offers just doesn't
[01:11:21.120 --> 01:11:26.800]   feel like it's enough today. Well, I mean, I think we're beating a dead horse. But
[01:11:27.760 --> 01:11:32.000]   I just I've it makes me feel bad. I think Ed, you're right. I mean, I think the future and by the way,
[01:11:32.000 --> 01:11:39.520]   this device, the HomePod is great for podcasting, as is the Amazon Echo and even the Invoke.
[01:11:39.520 --> 01:11:45.040]   Because you can buy voice in your house, say, let's, you know, let me listen to Twitter,
[01:11:45.040 --> 01:11:50.080]   let me listen to your podcast and, you know, slash film or and that's great for podcasting
[01:11:50.080 --> 01:11:56.240]   and puts podcasting in a way ahead of radio in a way that I think is going to be very good for us.
[01:11:56.240 --> 01:11:59.760]   So I'm not knocking these devices. And I agree with you, Ed, that that's the
[01:11:59.760 --> 01:12:05.200]   but Apple's not selling that. Apple's selling it as a great speaker. And I think that's I think
[01:12:05.200 --> 01:12:10.320]   it's doing us at this service. If you just said, here's a pretty good speaker that you can listen to
[01:12:10.320 --> 01:12:15.920]   music on in your kitchen. That's what it is. Sure. I'll give you that. And Ed, you're right. I mean,
[01:12:15.920 --> 01:12:23.440]   Siri at least does that much. It plays music. Apple music. The fact that doesn't do Spotify
[01:12:23.440 --> 01:12:27.680]   is a that's going to come to I feel like the whole pod is going to be one of those products. I might
[01:12:27.680 --> 01:12:33.040]   review again a year later. It feels like, Oh, by then they'll probably open it to Spotify,
[01:12:33.040 --> 01:12:37.840]   Yada Yada and and all those features that I mentioned are probably going to work by then.
[01:12:37.840 --> 01:12:41.920]   And it'll probably be a great product. It reminds me the Apple Watch, which in the first couple of
[01:12:41.920 --> 01:12:46.960]   generations was not compelling. Yeah, I really nailed it on version three. Yeah, I think I'm
[01:12:46.960 --> 01:12:50.960]   actually wearing this now every day because of. Well, this is actually this was actually the
[01:12:50.960 --> 01:12:56.480]   original problem with the original iPhone as well. You know, most people forget that the original
[01:12:56.480 --> 01:13:03.920]   iPhone, I mean, it didn't support apps. It, you know, it didn't, it didn't have an app store. It
[01:13:03.920 --> 01:13:10.240]   didn't didn't support exchange mail. It had a whole bunch of things that were wrong with it.
[01:13:10.240 --> 01:13:16.480]   And they fixed that over the first year. But that was also one of the that was also one of the
[01:13:16.480 --> 01:13:23.360]   things that had to go out into the world and be used by people to, you know, to improve in that
[01:13:23.360 --> 01:13:27.760]   fashion. And it isn't necessary with it. It still blew other smartphones away at the time, didn't
[01:13:27.760 --> 01:13:32.800]   it? Yeah. Like the keyboard is awesome. And Safari browser was amazing compared to the really
[01:13:32.800 --> 01:13:37.200]   pared down browsers on old cell phones. So even though there were all these things missing,
[01:13:37.200 --> 01:13:41.760]   like, right, it's it's hard to compare with the HomePod, I think, where like the iPhone at
[01:13:41.760 --> 01:13:46.000]   least was a working functional product. It's a good point. The HomePod is late to a market
[01:13:46.000 --> 01:13:50.080]   with a lot of competition. I mean, the iPhone was late to the smartphone market as well,
[01:13:50.080 --> 01:13:55.440]   but it just did a better job at a lot of things. And you look back at the iPod, like that was
[01:13:55.440 --> 01:14:00.320]   super limited to right then that only worked with Max for a long time, like ignoring the biggest
[01:14:00.320 --> 01:14:04.480]   PC market. Maybe a year or two, I don't remember, but yeah, I put it out on Windows. And that's
[01:14:04.480 --> 01:14:09.200]   when I became really big the iPod. For sure. You know, it's funny as we come almost full circle
[01:14:09.200 --> 01:14:14.160]   now because and Microsoft's been pushing this. They're probably at the build conference. They're
[01:14:14.160 --> 01:14:20.240]   going to talk a lot about progressive web apps, which are different, but in a way, they're the
[01:14:20.240 --> 01:14:25.600]   same as the web apps for the original iPhone. The idea that you would go to a website and you'd
[01:14:25.600 --> 01:14:30.640]   save it on your home screen. And now that's an app on your iPhone. That's what I'm excited about
[01:14:30.640 --> 01:14:34.560]   progressive web apps though. And I feel like that's a I wish Apple would get behind that. They're
[01:14:34.560 --> 01:14:39.200]   being a little slow to the party. I think so too. But in, you know, in both of these cases, in the
[01:14:39.200 --> 01:14:46.720]   case of progressive web apps and in the whole competition Spotify thing, what you're really
[01:14:46.720 --> 01:14:55.680]   seeing here is is Apple protecting its sort of core services. And in a way that's very, very
[01:14:55.680 --> 01:15:05.760]   Microsoft like they're refusing to disrupt themselves. You know, in fact, there was some
[01:15:06.320 --> 01:15:13.840]   forget who's who wrote this last week, but somebody did a study showing that Apple Music
[01:15:13.840 --> 01:15:19.600]   subscriptions are rising faster than Spotify subscriptions and Apple Music will probably catch
[01:15:19.600 --> 01:15:25.760]   up with Spotify by maybe by the end of this year in terms of the install base. And so, you know,
[01:15:25.760 --> 01:15:32.480]   a cynic could look at this and say, well, one reason that Apple's not putting Spotify on this
[01:15:32.480 --> 01:15:38.080]   and they're making it difficult for Spotify to succeed in the Apple ecosystem, hardware ecosystem
[01:15:38.080 --> 01:15:45.440]   is specifically so that Apple Music can surpass Spotify in terms of market share.
[01:15:45.440 --> 01:15:50.880]   A lot of money there. The proof of that is there's no Bluetooth in the home pod.
[01:15:50.880 --> 01:15:55.600]   Yeah, something that would allow you to open it up to an Android device, for instance.
[01:15:55.600 --> 01:16:00.320]   You could still AirPlay, right? So you can still AirPlay from your iPhone or iPad with Spotify
[01:16:00.320 --> 01:16:06.240]   or whatever. Yeah, it's proprietary, but you could still use that for Spotify. So, something.
[01:16:06.240 --> 01:16:09.760]   Well, I mean, let's look at the Apple Watch for comparison, right? So the Apple Watch,
[01:16:09.760 --> 01:16:15.120]   you have to have an iPhone to set it up, right? And it's still that way, even though it's more
[01:16:15.120 --> 01:16:21.360]   independent than it was before. But the Apple Watch so far seems pretty successful. So, and,
[01:16:21.360 --> 01:16:24.400]   you know, this is some inside information, but I've heard that they have considered
[01:16:24.960 --> 01:16:30.160]   sharing the Apple Watch of Android phones. But what's stopping them is that so many Android
[01:16:30.160 --> 01:16:36.080]   people are switching to the iPhone. So why would they give people incentive to stay of Android
[01:16:36.080 --> 01:16:40.160]   if so many people are switching to the iPhone? So that's how these decisions are made.
[01:16:40.160 --> 01:16:44.720]   But my point being that, you know, at counter-argument, the Apple Watch only works with the iPhone and
[01:16:44.720 --> 01:16:49.760]   yet people seem to love it and it's still pretty successful. If you're in the... You know what?
[01:16:49.760 --> 01:16:55.600]   I bet I would agree with this. The advantage of an ecosystem play like that and siloing it like
[01:16:55.600 --> 01:17:02.960]   that is you can make it a seamless, great experience. And Apple has done that. So if you live in the
[01:17:02.960 --> 01:17:07.360]   Apple world, you've got continuity, you know, you've got handoff, you've got all this great stuff,
[01:17:07.360 --> 01:17:13.680]   then your life is easy and smooth and it's better not to have to think about these other guys and
[01:17:13.680 --> 01:17:18.080]   what speakers better? You just buy the Apple speaker because it's an Apple speaker.
[01:17:18.080 --> 01:17:23.920]   And that sounds like laziness, right? Like what Ed was saying, that is totally Microsoft.
[01:17:23.920 --> 01:17:29.200]   People are happy. Like, yeah, they'll drink whatever you give them. But this doesn't feel like
[01:17:29.200 --> 01:17:34.560]   the same Apple that killed... You can't deny it's profitable. It certainly is. But right,
[01:17:34.560 --> 01:17:38.400]   Apple killed the iPod market with the iPhone. Right. The iPhone had to kill the iPod.
[01:17:38.400 --> 01:17:42.240]   Right. Like think of, think of like how... And Apple's willing to do that. Apple's always been
[01:17:42.240 --> 01:17:46.640]   willing to do that, which is... Are they? Anymore? Oh, you think they're not anymore.
[01:17:46.640 --> 01:17:50.400]   That's... We're talking about how safety a HomePod is. And I think that's exactly it.
[01:17:50.400 --> 01:17:57.120]   Good point. Yeah. Yeah. Well, when 70% of your revenue, as it comes from the iPhone,
[01:17:57.120 --> 01:18:01.840]   it does Apple. Right. It's pretty hard to imagine killing the iPhone.
[01:18:01.840 --> 01:18:07.200]   Well, not the iPhone, but something like killing its own services, right? Like Apple music,
[01:18:07.200 --> 01:18:11.600]   okay, it hasn't been super successful initially, but as I was pointing out, it's growing.
[01:18:11.600 --> 01:18:18.080]   Yeah. Are bigger for Apple, a bigger portion of the revenue than either the Mac or the iPad.
[01:18:18.080 --> 01:18:24.160]   Yep. Services are $10 billion a quarter for Apple. So you don't want to kill those either.
[01:18:24.160 --> 01:18:29.360]   In fact, you saw the Wall Street Journal said at the growth rate Apple music's going through,
[01:18:29.360 --> 01:18:33.760]   5% a month, it's going to eclipse Spotify soon. I wonder who leaked that statistic.
[01:18:33.760 --> 01:18:37.040]   Yeah. Journal's really good at repeating those. Convenient timing.
[01:18:40.000 --> 01:18:43.120]   With Spotify is about 2%. They don't include, of course, the free tier,
[01:18:43.120 --> 01:18:46.400]   which Apple doesn't have and Spotify has hundreds of hot-cut food.
[01:18:46.400 --> 01:18:49.680]   They have a three-month trial or something or is it one month? I forgot. Three months, yeah.
[01:18:49.680 --> 01:18:55.040]   Yeah. They also don't count. And they don't count the family subscriptions either, but...
[01:18:55.040 --> 01:19:00.080]   Right. So Ed, do you think it's... I mean, admitting that an ecosystem play is strong for Apple.
[01:19:00.080 --> 01:19:02.560]   You think in the long run, though, it's going to hurt them as it did Microsoft?
[01:19:03.360 --> 01:19:11.360]   Well, it has to because... Look, I think it's a justification to say the seamless experience
[01:19:11.360 --> 01:19:20.160]   is the goal here. That's a great rationalization, but the reality is there's just so much money
[01:19:20.160 --> 01:19:26.640]   coming into Apple's coffers. And when you sit down in a product meeting and somebody says,
[01:19:26.640 --> 01:19:32.720]   "Okay, I've got this great idea, but it's going to hurt this business that's bringing us $10
[01:19:32.720 --> 01:19:38.480]   billion a quarter." There's somebody there who's going to say, "Nope, we don't do that. We're not
[01:19:38.480 --> 01:19:43.120]   going to do that." And that ultimately, once the numbers get that big, once the
[01:19:43.120 --> 01:19:51.440]   fiefdoms, the individual fiefdoms within the giant spaceship there become powerful enough,
[01:19:51.440 --> 01:20:02.240]   then they start to block innovation and they start to build moats around each of their businesses
[01:20:02.240 --> 01:20:08.560]   and I've seen that play happen before. It's what happened to Microsoft in...
[01:20:08.560 --> 01:20:11.440]   it led to Windows Vista.
[01:20:11.440 --> 01:20:22.000]   And I have on my wall and I'm very happy to have it a signed copy of this fabulous
[01:20:22.000 --> 01:20:28.800]   cartoon from Bonkers World of the various org charts. And of course, Microsoft was all the fiefdoms
[01:20:29.360 --> 01:20:35.200]   shooting at all the other fiefdoms. And yeah, it hurt Microsoft badly. And actually,
[01:20:35.200 --> 01:20:41.200]   to Sachin Adele's credit, I think he's changed Microsoft and the attitude is very different now.
[01:20:41.200 --> 01:20:45.200]   It's dramatically. And it's made a difference in their operations.
[01:20:45.200 --> 01:20:50.640]   I feel like I bring this up every time I come on, but it does feel like Microsoft is doing a lot
[01:20:50.640 --> 01:20:56.240]   of the happily stuff, right? The innovation behind the Surface laptops, the Surface Book,
[01:20:56.240 --> 01:21:01.360]   the Surface Studio that you're using right now, Leo. These cool new devices that we're sitting here
[01:21:01.360 --> 01:21:05.920]   and watching Apple take away the ports people actually use on their MacBook Pros and give us
[01:21:05.920 --> 01:21:11.680]   keyboards that everybody hates. Everybody hates that keyboard. And I don't see we're seeing any
[01:21:11.680 --> 01:21:16.400]   indication that that's going to be fixed eventually. Apple's just going with this super flat design.
[01:21:16.400 --> 01:21:21.600]   A lot of things that we expect Apple to fix were just not seeing anymore and that's worrying me,
[01:21:21.600 --> 01:21:28.080]   honestly. I think to me, Tim Cook is feeling more and more like Steve Bomber. He's doing a good job
[01:21:28.080 --> 01:21:33.920]   of really pushing every product line and ramping everything up. But I don't know if we're going to
[01:21:33.920 --> 01:21:39.200]   see the innovation that we're kind of used to expecting from Apple. Amazon made some acquisitions
[01:21:39.200 --> 01:21:45.920]   this year. Some big ones. In fact, they kept buying our sponsors, which is fine. I'm glad they're
[01:21:45.920 --> 01:21:53.840]   doing well, including Jamie's Ring. But why? We don't know exactly how much initially we thought
[01:21:53.840 --> 01:22:02.800]   it was a billion dollars now. I'm seeing from recode numbers closer to 1.2 to 1.4 billion dollars.
[01:22:02.800 --> 01:22:08.160]   Now, Ring is a sponsor. We should mention we do ring ads all the time. I take some responsibility
[01:22:08.160 --> 01:22:14.640]   for Ring's success. I will get zero of this money. But I'm happy for Jamie and his team because that's
[01:22:14.640 --> 01:22:23.760]   a really nice. Here's the question. Why? Amazon key. Well, but they already have key. Remember,
[01:22:23.760 --> 01:22:28.320]   they tried to buy August. That fell through. In fact, August was pretty pissed off. They feel
[01:22:28.320 --> 01:22:32.800]   like Amazon found out how everything worked. They said, "Okay, well, never mind." And they built
[01:22:32.800 --> 01:22:36.720]   the shortly thereafter. They built the key system, which allows an Amazon delivery person
[01:22:36.720 --> 01:22:42.320]   that expands on that. Into your house. There's a camera. There's a lock. They can open the lock.
[01:22:42.320 --> 01:22:45.520]   The camera watches them while they put the package in the door and they leave.
[01:22:45.520 --> 01:22:50.000]   Is that available in Houston? Have you tried it? Yes, it is available. I have not tried it.
[01:22:50.000 --> 01:22:56.160]   I know somebody who has it and they love it. We have a real problem in Houston. There's a lot
[01:22:56.160 --> 01:23:03.280]   of other places with people stealing packages. Houston is the place. I think it is where the guy
[01:23:03.280 --> 01:23:08.560]   invented the box where you pick it up and a shotgun shell goes off in the box and scares the...
[01:23:08.560 --> 01:23:09.280]   What?
[01:23:09.280 --> 01:23:14.400]   Yes. All in Texas, my friends, Florida and some states.
[01:23:14.400 --> 01:23:22.400]   But that is a... It's such an issue here that the guy loves it. He doesn't mind at all.
[01:23:22.400 --> 01:23:26.800]   And I just could not do that. There's just no way I would allow that.
[01:23:26.800 --> 01:23:29.360]   And what we didn't do in my God would bite the guy, though.
[01:23:29.360 --> 01:23:29.680]   Yeah.
[01:23:29.680 --> 01:23:30.160]   I know it would be important.
[01:23:30.160 --> 01:23:34.560]   Well, my other problem is that it's the Amazon logistics, which I don't know what it's like in
[01:23:34.560 --> 01:23:39.600]   Houston. But in Amazon logistics, in Tacoma, is some person that you did not in a uniform
[01:23:39.600 --> 01:23:46.320]   in a normal car because they kind of do it like Uber, where they list deliveries that they have.
[01:23:46.320 --> 01:23:51.920]   And somebody says, "Okay, I'll do those." And so to me, that's like somebody off the street
[01:23:51.920 --> 01:23:57.040]   coming in my house. Now, I don't have the problem with packages getting stolen because I have a
[01:23:57.040 --> 01:24:04.240]   ring video doorbell. So maybe that's the... I don't know. So is that why they bought it?
[01:24:04.240 --> 01:24:05.840]   They want to replace key with ring?
[01:24:05.840 --> 01:24:08.160]   I don't think it's spam those services.
[01:24:08.160 --> 01:24:08.800]   Or have them on.
[01:24:08.800 --> 01:24:12.080]   I think it ties in with the whole ecosystem for both...
[01:24:12.080 --> 01:24:18.240]   I mean, when you look at Amazon, Google and Apple are all competing to own your home and
[01:24:18.240 --> 01:24:23.440]   have as many devices in your home as possible. And I think it ties in with the Echo. And I'm
[01:24:23.440 --> 01:24:27.040]   sure that they will continue to find ways to integrate. I suspect they're already integrated
[01:24:27.040 --> 01:24:35.280]   to some extent. But it's Amazon really having a really robust offering of home devices. And I
[01:24:35.280 --> 01:24:36.880]   think they're going to have more of them.
[01:24:36.880 --> 01:24:39.840]   Does Ring do anything other than a doorbell?
[01:24:39.840 --> 01:24:46.000]   They have door... Basically it's a doorbell for the front door and then they attach the doorbell
[01:24:46.000 --> 01:24:46.560]   to lights.
[01:24:46.560 --> 01:24:47.760]   It has security lights.
[01:24:47.760 --> 01:24:52.880]   Kind of the same thing. But they did remember this. They were going to launch last Christmas,
[01:24:52.880 --> 01:24:59.280]   a home security system and ADT sued them. And I think that lawsuits over. In fact,
[01:24:59.280 --> 01:25:06.080]   my understanding is shortly before Amazon bought them, Ring settled for a larger amount than they
[01:25:06.080 --> 01:25:11.520]   were agreed. They suddenly said, "Okay, here's 25 million." And the ADT suit went away. So they
[01:25:11.520 --> 01:25:14.240]   have now on their page, they are selling their home security system.
[01:25:14.240 --> 01:25:17.120]   Are they currently integrated to Alexa in any way?
[01:25:17.120 --> 01:25:18.000]   Yes, totally.
[01:25:18.000 --> 01:25:19.600]   Yeah. That's what I figured they would be in.
[01:25:19.600 --> 01:25:19.600]   Yeah.
[01:25:19.600 --> 01:25:22.640]   Yeah. But everything is... Everything is.
[01:25:22.640 --> 01:25:25.520]   Everything is. That doesn't give them an exclusive by any means.
[01:25:25.520 --> 01:25:26.320]   No, no.
[01:25:26.320 --> 01:25:27.920]   No, but it's not even a crazy theory.
[01:25:27.920 --> 01:25:33.920]   So Amazon is famous for a number of things. And one of them is having a kind of quasi or
[01:25:33.920 --> 01:25:40.400]   well-ian employee situation where they watch their employees in a microcosmic way.
[01:25:40.400 --> 01:25:40.880]   Oh, wow.
[01:25:40.880 --> 01:25:43.040]   Everyone's clocked in. They monitor their location.
[01:25:43.040 --> 01:25:49.360]   And they're about to launch inevitably into this world where Amazon is directly
[01:25:49.360 --> 01:25:53.680]   Amazon employees will be doing the lion's share of deliveries to the front door.
[01:25:53.680 --> 01:25:54.240]   Yes.
[01:25:54.240 --> 01:25:58.880]   And this is a way for them to monitor their own employees and their performance and their behavior.
[01:25:58.880 --> 01:26:01.040]   Interesting.
[01:26:01.040 --> 01:26:01.520]   Wow.
[01:26:01.520 --> 01:26:07.360]   But you have so many of those rings installed, not everybody's going to put those in there.
[01:26:07.360 --> 01:26:09.760]   And what about the privacy?
[01:26:09.760 --> 01:26:15.200]   The privacy. So if I have a ring in my house, do the Amazon have the right to look at the video?
[01:26:15.200 --> 01:26:15.600]   Okay.
[01:26:15.600 --> 01:26:16.560]   So that might ring.
[01:26:16.560 --> 01:26:22.160]   This is something that I've been kind of pursuing recently.
[01:26:22.160 --> 01:26:27.120]   I've asked Amazon several times, pointed questions about exactly what they do in terms of,
[01:26:27.120 --> 01:26:30.080]   do they listen when they're not listening to the keywords?
[01:26:30.080 --> 01:26:35.840]   And are they doing experiments where they have Alexa doing things that people don't necessarily
[01:26:35.840 --> 01:26:40.640]   expect? And whenever I ask those questions, they just stop interacting with me. They don't answer
[01:26:40.640 --> 01:26:45.520]   them. And that doesn't mean that they're doing those things. It doesn't mean anything nefarious,
[01:26:45.520 --> 01:26:48.800]   but it does mean that they have not answered those questions yet.
[01:26:48.800 --> 01:26:54.240]   So I've read their terms of service. There's nothing in the terms of service that I can find
[01:26:54.240 --> 01:26:59.920]   that prohibits them from monitoring video feeds, listening in through Amazon, doing any of these
[01:26:59.920 --> 01:27:04.160]   things. And I think it's an underexplored area. We have this black box that we're putting in every
[01:27:04.160 --> 01:27:12.160]   house. And I'm not a conspiracy theory minded person or suspicious of tech companies as a rule.
[01:27:12.160 --> 01:27:16.960]   But in this case, I'm a little concerned about Amazon because they're so close to the
[01:27:16.960 --> 01:27:21.120]   vest. They don't tell us exactly what they're doing. They have a history, for example, with the
[01:27:21.120 --> 01:27:26.720]   fire phone of when you take a picture of something, they're also transmitting the audio back to
[01:27:26.720 --> 01:27:32.400]   Amazon and doing God knows what with it. So there's a, I don't think they've earned our trust yet.
[01:27:32.400 --> 01:27:37.840]   They're not transparent enough yet. And I think that it behooves those of us in the media to keep
[01:27:37.840 --> 01:27:41.440]   pushing them to find out exactly what their intentions are, what they do, what they don't do.
[01:27:42.000 --> 01:27:47.040]   And it's especially urgent now that they have all these ring video doorbells.
[01:27:47.040 --> 01:27:52.560]   You know, it's funny because my, my echo will sometimes talk to me, even though I don't say
[01:27:52.560 --> 01:27:58.080]   the word. Oh, I have a little time. It's because you're watching TV, right? Or here's something else.
[01:27:58.080 --> 01:28:04.080]   It thinks you're saying the word. I apologize for saying that word on your show because I'm
[01:28:04.080 --> 01:28:09.440]   sure that millions of echoes just got activated when I said it. You know what? You know, it's only a
[01:28:09.440 --> 01:28:14.080]   problem if you're listening to the show on your echo, because then it stops the show
[01:28:14.080 --> 01:28:19.280]   and start and says, what do you want? And so that's a little annoying. But other than that,
[01:28:19.280 --> 01:28:23.680]   you know, these things happen. We, you know what? We actually bloop it out of the downloaded version.
[01:28:23.680 --> 01:28:28.800]   So it's only people watching live that get bit by that. But that's something we decided to start
[01:28:28.800 --> 01:28:33.760]   doing and bloop it out. And here's a weird thing that may surprise some of your listeners.
[01:28:33.760 --> 01:28:40.320]   But actually travel with both my what? Echo and also my ring video doorbell. I travel with both of
[01:28:40.320 --> 01:28:45.200]   these two of the most important things that I travel with. Yeah, everywhere we go, we plug in
[01:28:45.200 --> 01:28:50.240]   the the echo. We have music, we have information, weather reports. So you're not so worried about
[01:28:50.240 --> 01:28:58.640]   the privacy implications that you don't use it. No. Are you renting a place or are you in a
[01:28:58.640 --> 01:29:03.680]   hotel? Where are you that you can hook up a doorbell? I use mounting putty and I put it outside
[01:29:03.680 --> 01:29:08.560]   the door. I connected a Wi-Fi and everywhere I got it. It's got a one year battery in it.
[01:29:08.560 --> 01:29:11.680]   Uh, some of them. And so you don't have to worry about powering it. You could just
[01:29:11.680 --> 01:29:14.720]   find out where he's staying and go steal it. Yeah.
[01:29:14.720 --> 01:29:21.520]   I'm notting putty. Yeah. That's right. You have to pull hard. That's really interesting, Mike. I
[01:29:21.520 --> 01:29:25.360]   didn't, I didn't, I know you thought it was a good product for Airbnb.
[01:29:25.360 --> 01:29:32.720]   Rhettters everyone involved in renting Airbnb should be using this. It's just a fantastic. So the
[01:29:32.720 --> 01:29:37.360]   biggest thing, so we've, we've stayed in a lot of countries that are not necessarily safe and we,
[01:29:37.360 --> 01:29:41.440]   and when you're in an Airbnb, everyone in the neighborhood knows you're a foreigner with expensive
[01:29:41.440 --> 01:29:47.600]   equipment and all this stuff. And so it's kind of a little bit, you get kind of nervous and with a
[01:29:47.600 --> 01:29:52.240]   ring video doorbell, you come back to the house before you go in, you check to see if anyone has
[01:29:52.240 --> 01:29:56.880]   gone in. Right. And you can see nobody's gone in and you walk in with confidence. That's really nice.
[01:29:56.880 --> 01:30:01.600]   That's a great reason to do that. I don't have a ring, ring that I have a vivid system in. So if
[01:30:01.600 --> 01:30:06.240]   somebody comes to my door, I'll get a note of saying somebody that you're door. But by the time I get
[01:30:06.240 --> 01:30:10.720]   to look at them, they're usually gone because of the delay. Do you find that you actually can
[01:30:10.720 --> 01:30:14.160]   often see the person in real time and interact with them before they actually leave?
[01:30:14.160 --> 01:30:20.000]   Absolutely. Yeah. It's instantaneous almost. Maybe you might have worse latency.
[01:30:20.000 --> 01:30:24.320]   There's about 10 seconds, five to 10 seconds latency on, in worst case. But that's an
[01:30:24.320 --> 01:30:28.320]   only usually to catch the people wait at the door, but they ring the doorbell, they wait 10 seconds.
[01:30:28.320 --> 01:30:32.160]   Yeah, no mind my latency. I think it's about 30 seconds. So that's good to know.
[01:30:32.160 --> 01:30:35.520]   Yeah. They've done it right. And I mean, that's why Amazon bought them. But
[01:30:35.520 --> 01:30:45.520]   Amazon doesn't need to push the envelope. They're doing great. They don't have to take chances
[01:30:45.520 --> 01:30:50.000]   with people's privacy. They don't even have to make acquisitions of cameras if that makes people
[01:30:50.000 --> 01:30:55.920]   nervous. Now, I'm not a CEO and I never will be. So I don't maybe understand business.
[01:30:55.920 --> 01:31:01.680]   But why would Jeff Bezos take chances like that? I mean, if
[01:31:01.680 --> 01:31:05.920]   well, why why their whole business model is based on algorithmically
[01:31:05.920 --> 01:31:12.080]   determined sort of behavioral model. So they if you think about what they track and what they do
[01:31:12.080 --> 01:31:17.280]   on the Amazon website, and even when you go beyond the website, whether they go where you they figure
[01:31:17.280 --> 01:31:20.640]   out where you're going, what you're doing, how long the mouse hovers over this, is that mean you're
[01:31:20.640 --> 01:31:24.960]   interested? They're they're they're upset and look at their grocery store Amazon go.
[01:31:24.960 --> 01:31:29.360]   This is not about a convenient grocery store. This is about a physical brick and mortar grocery
[01:31:29.360 --> 01:31:35.440]   store that has the kind of behavioral data that their website has. And so they're obsessed with
[01:31:35.440 --> 01:31:42.560]   understanding what people do. And a camera is by far the most valuable sensor for figuring out what's
[01:31:42.560 --> 01:31:48.640]   happening in general with human behavior. They can see who's coming, who's going, what time, how long,
[01:31:49.440 --> 01:31:55.920]   all this stuff is happening. And so it's all they're like Google that's it's all about the data.
[01:31:55.920 --> 01:31:59.680]   It's about big data and how you can plug in artificial intelligence to get
[01:31:59.680 --> 01:32:04.400]   in to get information that enables you to be more competitive in everything you do.
[01:32:04.400 --> 01:32:07.840]   10 years from now, I'm going to take issue with your growth three bags if you walk in the house
[01:32:07.840 --> 01:32:11.200]   and figure out where you're buying your growth or even make sure that they're under big.
[01:32:11.200 --> 01:32:16.720]   Yeah, maybe. Wow, that's interesting. 10 years from now, we're going to look back and regret that we
[01:32:16.720 --> 01:32:23.440]   didn't work more proactive in fighting the growth of Amazon and Google and Facebook.
[01:32:23.440 --> 01:32:32.080]   Are we? Do you think so? I don't know. I don't think so. I think things change and
[01:32:32.080 --> 01:32:38.160]   happen so quickly that Google and Facebook and Amazon in 10 years may not be what they are now.
[01:32:38.160 --> 01:32:43.360]   And there may be somebody else we have to worry about fighting. Last night, I finally had it. I
[01:32:43.360 --> 01:32:50.960]   read the story. There's a way we've reported on this. We've been reporting on this for over a year.
[01:32:50.960 --> 01:32:58.800]   Facebook, I've told people for more than a year don't do Facebook quizzes because Facebook allows
[01:32:58.800 --> 01:33:03.200]   a quiz designer not only to get all the information about you because you took the quiz,
[01:33:03.200 --> 01:33:10.320]   but and this is incredibly infuriating. They also allow the quiz designer to get information about
[01:33:10.320 --> 01:33:18.640]   your friends. Facebook says, well, wait a minute. No, we only allow that to improve. Let me see if I
[01:33:18.640 --> 01:33:25.360]   can find the stupid quote. We're only allowing collection of friends data to improve user experience
[01:33:25.360 --> 01:33:33.280]   in the app. It can't be sold or used for advertising except that's exactly what Cambridge Analytica did.
[01:33:33.280 --> 01:33:39.360]   And now we know for a fact, even though we've known this for a while, we now have a whistleblower
[01:33:40.080 --> 01:33:47.520]   who has told the story of how Cambridge Analytica basically stole 50 million Facebook profiles,
[01:33:47.520 --> 01:33:52.160]   got all the information they need. Cambridge Analytica is owned by the Robert Mercer family
[01:33:52.160 --> 01:33:58.160]   and was run at the time by Steve Bannon. And of course, the information was intended to be used by
[01:33:58.160 --> 01:34:05.360]   Trump during the campaign. Although I don't blame Cambridge Analytica as much as I blame
[01:34:05.360 --> 01:34:09.680]   Facebook for even allowing this to happen. And you probably could say that other campaigns had
[01:34:09.680 --> 01:34:18.000]   done the same thing. The whistleblower said that they since early 2014, they built a system that
[01:34:18.000 --> 01:34:24.160]   would profile user US voters in order to target them with personalized political advertisements.
[01:34:24.160 --> 01:34:29.920]   We exploited Facebook to harvest millions of people's profiles and built models to exploit what we knew
[01:34:29.920 --> 01:34:36.720]   about them and target their inner demons. This was the basis the entire company was built on.
[01:34:38.480 --> 01:34:47.920]   You know, it's funny because Dan Patterson, who writes for me, was on this story in August of 2016.
[01:34:47.920 --> 01:34:55.520]   He was one of the first to write about this. And he was on it from when Ted Cruz's campaign
[01:34:55.520 --> 01:35:02.480]   were using Cambridge Analytica. And he was saying that there's just something fishy about this
[01:35:02.480 --> 01:35:11.280]   and it doesn't smell right. And it's amazing that we just didn't pay
[01:35:11.280 --> 01:35:17.360]   enough people did not pay attention to this. I was up in Orange a year ago, the observer
[01:35:17.360 --> 01:35:24.400]   which broke this story saw the documents. And this was confirmed by a Facebook statement that
[01:35:24.400 --> 01:35:30.800]   said that by late 2015, Facebook knew that this information had been harvested on unprecedented
[01:35:30.800 --> 01:35:36.240]   scale. It did not alert users. It took only limited steps to recover and secure the private
[01:35:36.240 --> 01:35:44.880]   information. And in fact, this really frustrates me when Facebook was told by the observer that
[01:35:44.880 --> 01:35:48.800]   we have this information, we're going to go live with a story tomorrow, then and only then they
[01:35:48.800 --> 01:35:56.560]   kicked Cambridge Analytica off the platform. Yep. Yep. They knew for years.
[01:35:57.840 --> 01:36:04.000]   And really did nothing. It was completely a cover up of everything that was happening to that.
[01:36:04.000 --> 01:36:09.520]   And the thing is, is that no company can police itself. We need to have legislation that's in
[01:36:09.520 --> 01:36:15.040]   place. I'm sorry. Yes, legislation, government, I hear you, but no one can police themselves.
[01:36:15.040 --> 01:36:21.280]   The problem is I sure I agree with you 100% Georgia, but Facebook's a black box to Congress
[01:36:21.280 --> 01:36:26.240]   and it's regulated. It can't be right. Like it can't be. How do you get that information?
[01:36:26.240 --> 01:36:30.640]   They don't want to give it. Well, if they want to operate in the United States,
[01:36:30.640 --> 01:36:34.640]   they're going to have to be able to let people know how something's run. What is the information
[01:36:34.640 --> 01:36:38.800]   going to be used for their Ulas have to be something that's readable to everyone that's going to be
[01:36:38.800 --> 01:36:44.960]   able to deal with it. And we have to reinstate educating people that work in the government
[01:36:44.960 --> 01:36:51.440]   about technology, which we've ended up just really. So why doesn't Facebook just relocate to the
[01:36:51.440 --> 01:36:56.000]   Philippines? They're already in tight with the Philippine president. We've already seen accusations
[01:36:56.000 --> 01:37:01.120]   leveled at Facebook saying that they are actively supporting the regime of Duerte over there who's
[01:37:01.120 --> 01:37:07.280]   quite, I don't want to get too carried away, but carrying out terrible things against the
[01:37:07.280 --> 01:37:11.360]   citizens. So Facebook could just up from Silicon Valley and relocate to the Philippines to continue
[01:37:11.360 --> 01:37:18.400]   on in the US government as effectively zero control, right? Yeah, it's true. The thing is,
[01:37:18.400 --> 01:37:25.200]   there are three companies right now that are disrupting markets that are turning
[01:37:25.200 --> 01:37:34.480]   society upside down in so many ways. And it's because they are five to 10 years ahead of
[01:37:34.480 --> 01:37:40.480]   everybody else on data. And they know more about us than we know about ourselves. And they're using
[01:37:40.480 --> 01:37:48.000]   that data to monetize the living crap out of everything that they have. And they are incented
[01:37:48.000 --> 01:37:55.840]   to keep that as private and proprietary as possible because it's a gold mine, right? It's an absolute
[01:37:55.840 --> 01:38:00.720]   gold mine. And obviously- They're very sensitive to keep it to themselves because if they give it
[01:38:00.720 --> 01:38:05.280]   away, they lose control. Somebody else can monetize. So the thing that- Yep. The thing here is that
[01:38:05.280 --> 01:38:09.520]   I bet I'm willing to bet a substantial sum of money that the executives inside of Facebook are
[01:38:09.520 --> 01:38:15.280]   mega pissed that someone else is making money from their data. That's the problem with gamer
[01:38:15.280 --> 01:38:19.520]   Janiline. They're not making money. They're not making money. You're supposed to buy that from us.
[01:38:19.520 --> 01:38:25.440]   What do you get? And the thing that frustrates me is I don't take Facebook quizzes because I know
[01:38:25.440 --> 01:38:29.280]   that that gives them your information. That's, you know, you see these quizzes, which game of
[01:38:29.280 --> 01:38:34.400]   thrones character are you? Are you smarter than a fifth grader? How's your spelling? And I see so
[01:38:34.400 --> 01:38:39.680]   many people on my timeline saying, "See how smart I am? I'm in the 99th percentile in spelling."
[01:38:39.680 --> 01:38:44.800]   I'm saying, "You're not smart. You're a moron. You just took that quiz." That was a dopey quiz.
[01:38:44.800 --> 01:38:48.800]   That was made up quiz. They just wanted your personal information. But you know what really
[01:38:48.800 --> 01:38:53.920]   frost me is, I don't take the quiz. It doesn't matter. My friends did. They got my information.
[01:38:53.920 --> 01:39:00.960]   And that's infuriating. It's true. It was like 270,000 people download of the app and 500 million
[01:39:00.960 --> 01:39:05.600]   people's information is out there. So that's a huge amount of people that did not do it. And
[01:39:05.600 --> 01:39:09.920]   you're still, because you're part of the ecosystem, in the end, the only thing that matters to Facebook
[01:39:09.920 --> 01:39:14.640]   is if you delete Facebook and ask them to delete all the data that they have on you because they're
[01:39:14.640 --> 01:39:20.480]   going to keep it anyways. And in the end, the amount of media literacy that we teach to our children
[01:39:20.480 --> 01:39:27.600]   and to ourselves. We are so easily sucked into these applications for internet points that it's
[01:39:27.600 --> 01:39:40.480]   pretty sad. The good news is, thanks to the EU and the GDPR, Facebook now has a way to actually
[01:39:40.480 --> 01:39:46.880]   permanently delete your information. In the past, when you got off of Facebook,
[01:39:46.880 --> 01:39:52.800]   they would deactivate your account for six months, hoping that you would at some point accidentally
[01:39:52.800 --> 01:39:57.520]   log in and they go, Oh, welcome back. By the way, when you leave Facebook, I should have done this
[01:39:57.520 --> 01:40:02.960]   on the air because it's hysterical. They put pictures of all your friends and they look sad.
[01:40:02.960 --> 01:40:09.680]   And they say, it literally said, Georgia Dow is so sorry, you're leaving Facebook. Do you want to
[01:40:09.680 --> 01:40:17.360]   send her a note telling her why? It's the most offensive emotional blackmail. Oh, yeah.
[01:40:17.360 --> 01:40:26.000]   G. So now you can do a complete delete of your Facebook data. And I really think this is because
[01:40:26.000 --> 01:40:30.720]   coming in May, they were going to get in trouble with the EU if they didn't offer this. So
[01:40:30.720 --> 01:40:35.360]   there's it's not just deactivating. It does still take two weeks. So last night,
[01:40:36.880 --> 01:40:41.360]   and I see here Facebook saying it'll take 90 days to delete all the things you've posted. Well,
[01:40:41.360 --> 01:40:45.600]   we'll see. Why would it take 90 days? Our computers will be faster. Because they're
[01:40:45.600 --> 01:40:47.280]   hoping you're going to come back. Right.
[01:40:47.280 --> 01:40:54.160]   Cash is in the primary system. But to do a complete deletion, you have to reach into all
[01:40:54.160 --> 01:40:59.840]   of the backups. Well, I hope they do that. I really, anyway, I have, I said, this is it.
[01:40:59.840 --> 01:41:03.920]   They're only way to really punish Facebook. Yes, they should be regulated. Yes. You know,
[01:41:03.920 --> 01:41:10.160]   Mark Warner's honest ads bill should go through and that should require that just as on radio and
[01:41:10.160 --> 01:41:15.680]   TV, political ads, you have to be identified. You have to say this, this ad was paid for by
[01:41:15.680 --> 01:41:20.640]   so and so that's not a requirement digital advertising. It should be that I hope happens.
[01:41:20.640 --> 01:41:26.480]   But it's still not going to you. Don't we know Mark Zuckerberg well enough by now?
[01:41:26.480 --> 01:41:29.920]   That's not going to stop. Are we ever going to see an adult in charge of these companies like
[01:41:29.920 --> 01:41:35.520]   an absolute grown up instead of these overblown teenagers that actually sort of.
[01:41:35.520 --> 01:41:41.600]   He's a grown up. He's maximizing revenue. This is a grown up who has become one of the richest
[01:41:41.600 --> 01:41:46.560]   men in the world. That's a grown up grown ups. Grown ups are no better. Grown ups are no better.
[01:41:46.560 --> 01:41:52.000]   In the end, any of these companies, all they want to do is make money. And I get it. But we have to
[01:41:52.000 --> 01:41:57.920]   make sure that I honestly, I don't even blame Facebook or the engineers. They're doing what,
[01:41:57.920 --> 01:42:02.400]   you know, this is capitalism. They're doing what they're they have a fiduciary responsibility,
[01:42:02.400 --> 01:42:07.120]   their shareholders to do maximize profit. They're supposed to do it legally. Okay, honestly,
[01:42:07.120 --> 01:42:12.080]   that's probably not right, but they're not completely legal, but they're trying their best to be
[01:42:12.080 --> 01:42:19.040]   to be sort of quasi legal, but they're maximizing profit. That's fine. So instead of saying,
[01:42:19.040 --> 01:42:24.640]   let's regulate them, I'm going to vote with my feet. And by the way, I don't I this is not the
[01:42:24.640 --> 01:42:30.160]   first time I've quit Facebook, but this is the last time I'm never going back. I'm going to remember
[01:42:30.160 --> 01:42:34.400]   Leo. No, I never remember when you go back to Facebook, I'm going to be like, I don't think
[01:42:34.400 --> 01:42:40.320]   I'm alone in this. I we learned Facebook even said that 2 million fewer users under 18
[01:42:40.320 --> 01:42:45.680]   in the past year. I think this is the beginning of an exodus. And I really believe this might be the
[01:42:45.680 --> 01:42:52.480]   the final straw for a lot of users. I don't know. I just think is the story is the data. The data
[01:42:52.480 --> 01:42:56.400]   is the story, right? It's not Cambridge Analytica's not making that much money.
[01:42:56.400 --> 01:43:02.400]   They were able to do this because of what's what Facebook has and the data that they have.
[01:43:02.400 --> 01:43:09.120]   And that data is a goldmine data is the new oil data is where the money is. And like I said,
[01:43:09.120 --> 01:43:14.880]   there's three companies that are so far ahead of everybody else. Microsoft not Microsoft.
[01:43:14.880 --> 01:43:20.000]   Google Amazon Amazon and Facebook and Facebook.
[01:43:20.000 --> 01:43:24.080]   And Alibaba. There's a bunch of others. Tencent Alibaba. You're forgetting the
[01:43:24.080 --> 01:43:28.240]   risk of the world. There's going to be a million. We chat. There's going to be a million.
[01:43:28.240 --> 01:43:35.840]   That are amazing. But in terms of global reach, they're still so focused on the Chinese market
[01:43:35.840 --> 01:43:42.560]   that they will eventually be global companies, but they aren't currently. But those three companies
[01:43:45.200 --> 01:43:49.360]   because they're in, they have very, very close relationships with their government,
[01:43:49.360 --> 01:43:56.080]   they're a bit protected for the moment. But those three companies that I mentioned,
[01:43:56.080 --> 01:44:03.360]   they have really been able to run roughshod over the entire globe with the exception of the EU
[01:44:03.360 --> 01:44:12.640]   and basically be able to do whatever they want with all this data and have no transparency
[01:44:12.640 --> 01:44:18.960]   about what they're doing. And that's the problem. Cambridge Analytica is just like a gnat that
[01:44:18.960 --> 01:44:27.360]   was able to use this platform to siphon a little bit off. The real problem, the real issue is the
[01:44:27.360 --> 01:44:33.200]   data and the fact that we have no regulation about it. And we're getting later and later in the game.
[01:44:33.200 --> 01:44:39.600]   And the EU and GDPR is making a difference because these companies as well as a lot of others,
[01:44:39.600 --> 01:44:43.520]   they essentially are making that their baseline. If they're going to have to do that there,
[01:44:43.520 --> 01:44:49.280]   then in Europe to be in that market, then it is having an effect on what they're doing in other
[01:44:49.280 --> 01:44:54.480]   markets as well because it's just less expensive for them to get back to fiduciary responsibility.
[01:44:54.480 --> 01:45:01.600]   But we still have this huge problem that we just haven't woken up to the fact that data is the new
[01:45:01.600 --> 01:45:07.920]   oil and we can't let it be a wild, wild west. And even if they're operating somewhere else,
[01:45:07.920 --> 01:45:12.480]   if you put legislation in place that if you're operating here as well, which they want to in
[01:45:12.480 --> 01:45:20.720]   the States is a huge area for them. So then they'll end up falling into line. And we need to, in the
[01:45:20.720 --> 01:45:25.840]   end, what we need though is media literacy. We need to teach our kids how to deal with social
[01:45:25.840 --> 01:45:32.000]   media, what's happening with it, and why. And I think that old data is worth nothing. And old
[01:45:32.000 --> 01:45:36.160]   is a really short period of time when we talk about the technological age. So if a lot of people,
[01:45:36.160 --> 01:45:40.880]   there's an exodus from Facebook, which, you know, in my, I don't use Facebook, so I think that would
[01:45:40.880 --> 01:45:45.200]   be great. But most of the young kids, they're still using other sets of social media. And our
[01:45:45.200 --> 01:45:51.920]   brains are primed for wanting to get those clicks on social media. That validation is going less
[01:45:51.920 --> 01:45:57.840]   and less with real people and more and more with internet clicks. And we're primed for that.
[01:45:57.840 --> 01:46:02.080]   I have to say that it's easy to leave Facebook these days because it's not that compelling,
[01:46:02.080 --> 01:46:08.560]   to be honest. And if you leave Facebook, you should also leave Facebook's WhatsApp and Instagram as well.
[01:46:08.560 --> 01:46:14.880]   But I haven't left Google and I haven't left Amazon and there'd be a lot harder to leave Google,
[01:46:14.880 --> 01:46:16.960]   right? Yeah. What am I going to do?
[01:46:16.960 --> 01:46:21.040]   I know is sad. Duck Duck go. I use it. It's sad. I'm just going to say it.
[01:46:21.040 --> 01:46:26.160]   It's hard. And I would have to give up Gmail. I would have to give up a lot of things. I actually
[01:46:26.160 --> 01:46:31.920]   did move my main mail account off Gmail. I used to use Gmail. I don't anymore. But it's
[01:46:31.920 --> 01:46:37.040]   it's a lot harder. It's easy to leave Facebook. So I'm putting a stake in the sand here. But I
[01:46:37.040 --> 01:46:42.320]   admit it's challenging. And I still buy a ton of stuff from Amazon. And Amazon arguably has more
[01:46:42.320 --> 01:46:47.200]   real data on me than Facebook did because they know everything I've purchased. They have microphones
[01:46:47.200 --> 01:46:52.320]   in my house. Yeah, but Amazon, I think this to be the elephant in the room here is the political
[01:46:52.320 --> 01:46:58.560]   angle. And we haven't touched on that. The simple fact is that the data that Facebook
[01:46:58.560 --> 01:47:05.760]   are let their partners, because these were signed up legally obligated partners who promised not
[01:47:05.760 --> 01:47:10.880]   to steal the data, but they stole the data anyway, highlights the fact that Facebook has very little
[01:47:10.880 --> 01:47:16.640]   control over its data, had very little and even now has once you order a Facebook employee,
[01:47:16.640 --> 01:47:20.560]   you can basically do anything you like. And the only reason we haven't seen more of this
[01:47:20.560 --> 01:47:24.880]   is just sheer blind luck. Facebook seems to need to be more blatant than any other
[01:47:25.520 --> 01:47:30.160]   of these other companies. It's stupid. It is the most stupid technology company. Google is not
[01:47:30.160 --> 01:47:35.680]   being caught up in politics. Amazon is not being caught up in politics. Facebook consistently makes
[01:47:35.680 --> 01:47:41.920]   horrendous, unforced errors in the face of every political situation. So every time something comes
[01:47:41.920 --> 01:47:45.760]   out about Russia, Facebook says, no, no, not us, not us. And then somebody comes up with the evidence
[01:47:45.760 --> 01:47:51.200]   and say, yeah, you got us. And then it goes through again. So frustrated. So so Greg, you're saying
[01:47:51.200 --> 01:47:55.280]   that them asking for all the information to be destroyed wasn't enough for them to do because
[01:47:55.280 --> 01:47:58.960]   they just asked. And then they were, they were, they were, they said that they did.
[01:47:58.960 --> 01:48:05.440]   There's nothing they can do. All they can do is leave us say, yeah, you have to agree and sign
[01:48:05.440 --> 01:48:09.760]   an agreement. And if they find out later on, then they can sue, blah, blah, blah, blah. But the
[01:48:09.760 --> 01:48:14.480]   horse has got the, you know, the gates open, the horse is gone. But the real issue here is that
[01:48:14.480 --> 01:48:21.040]   Cambridge Analytica was a UK company funded by alt-right run by Steve Bannon, which is in Trump's
[01:48:21.040 --> 01:48:26.080]   government. The whole alt-right got involved in the Brexit thing here in the UK. We've now got the
[01:48:26.080 --> 01:48:29.760]   Information Commissioner's Office in the UK announcing that they will open an investigation
[01:48:29.760 --> 01:48:36.400]   into this whole issue. So Facebook is now under fire. You're talking about the right wing of the
[01:48:36.400 --> 01:48:40.560]   American government is now looking at Facebook going, are you with us or are you against us?
[01:48:40.560 --> 01:48:44.960]   Then you've got cable, General Analytica getting onto the right wing over here and they're using
[01:48:44.960 --> 01:48:50.240]   it. And they're trying to say, this is scary. Yeah. I mean, if you're in a, if you want to create
[01:48:50.240 --> 01:48:54.640]   an authoritarian regime, Facebook's your best friend. And that's exactly what they've done in the
[01:48:54.640 --> 01:48:59.680]   Philippines under Duarte. Same thing in Myanmar. Same thing's happening in many other countries.
[01:48:59.680 --> 01:49:07.200]   Facebook has been co-opted by governments, strong men governments to control the population and to
[01:49:07.200 --> 01:49:12.800]   control the flow of information. And there's no reason for Facebook to stay in the US. It could
[01:49:12.800 --> 01:49:17.040]   just bunk out at any time except for the fact that the US is the market which generates the
[01:49:17.040 --> 01:49:21.200]   most revenue for them. But they could probably just scoot off if they needed to if they thought
[01:49:21.200 --> 01:49:25.760]   the pressure. And they're childish enough. I mean, your average Silicon Valley executive has the
[01:49:25.760 --> 01:49:31.520]   the mental brainpower of a 16 year old with a lot of hormones. They're so competitive.
[01:49:31.520 --> 01:49:37.120]   They're so inwardly looking. They're so focused on winning. They're not focused on the money as
[01:49:37.120 --> 01:49:41.440]   much as they are on the winning. They're socially incompetent to the level where I'm winning if
[01:49:41.440 --> 01:49:46.000]   I'm making more money. And so they're not actually following capitalism. They're not following any
[01:49:46.000 --> 01:49:51.120]   rules of society. They're just trying to win blindly, aggressively over and over and over,
[01:49:51.120 --> 01:49:56.400]   win, win, win. And the way that I keep score about winning is to make more money. And at some point,
[01:49:56.400 --> 01:50:01.600]   that's going to come off the rails because there's no more money to make. I suspect or something else
[01:50:01.600 --> 01:50:11.440]   happens. >> And so just to play devil's advocate here, I do think that to a degree Mark Zuckerberg
[01:50:11.440 --> 01:50:18.400]   is aware of the problem. And I think you've seen him the last couple years in pretty clear some
[01:50:18.400 --> 01:50:23.520]   statements knowing that this thing has gotten out of control. And he knows there's some serious
[01:50:23.520 --> 01:50:26.560]   problems. I mean, his number one, like his New Year's resolution, right? For this year was
[01:50:26.560 --> 01:50:30.800]   fixed Facebook. That's pretty bold statement. That's basically telling everybody, including
[01:50:30.800 --> 01:50:35.280]   your shareholders, that something's broken and he's be fixed. I mean,
[01:50:35.280 --> 01:50:39.680]   >> Someone shut the face right into it. He did true working out for himself. Somebody had to
[01:50:39.680 --> 01:50:45.600]   slam his face into the desk and go, look, bang, bang, bang. His head got slammed into the desk 10,
[01:50:45.600 --> 01:50:50.800]   12, 14 times before he went, yeah, there's a problem. >> No doubt. But he didn't admit it.
[01:50:50.800 --> 01:50:53.680]   He didn't just, he wasn't just blathering on about, you know.
[01:50:53.680 --> 01:50:57.360]   >> He says fix it. But I'd be interested to see what he's actually doing to fix it.
[01:50:57.360 --> 01:51:04.320]   The latest thing is Facebook told CNN, it was a rogue employee. We figured out it was a rogue
[01:51:04.320 --> 01:51:09.040]   employee that gave up all that information. So it wasn't our fault. My real problem is that
[01:51:09.040 --> 01:51:13.840]   if you're a normal person using Facebook, it's a very attractive thing. It's how you stay in touch
[01:51:13.840 --> 01:51:17.920]   with family and friends. You discover your high school sweetheart and what she's been up to.
[01:51:17.920 --> 01:51:23.600]   It's very attractive. Facebook acts as if you're giving them information, but they're keeping it
[01:51:23.600 --> 01:51:28.480]   private. You have control of it. You say, who's a friend who's an acquaintance, who is not to see
[01:51:28.480 --> 01:51:34.640]   your stuff. They really act as if you are controlling that information. They don't tell people that
[01:51:34.640 --> 01:51:39.040]   when you take a quiz, you're giving all your information to that company and you're giving up
[01:51:39.040 --> 01:51:43.200]   all your friends. I mean, you're a leader. >> They don't tell you that.
[01:51:43.200 --> 01:51:47.120]   They don't do it anymore because they realize that giving away the data
[01:51:47.120 --> 01:51:51.360]   meant other people will make it money or Facebook's platform. >> They don't do that third parties?
[01:51:51.360 --> 01:51:54.640]   >> No, I don't think so. >> They do still.
[01:51:54.640 --> 01:52:00.640]   >> You can check it on or off if you choose. But people usually don't even read that or know
[01:52:00.640 --> 01:52:06.960]   what they're doing. I would love to think that Zuckerberg actually wants to fix Facebook because
[01:52:06.960 --> 01:52:11.520]   who knows what that means. But the best way to tell if someone's behavior in the future is by
[01:52:11.520 --> 01:52:15.680]   their behavior in the past and they waited two years to be able to deal with this, they were
[01:52:15.680 --> 01:52:21.680]   not transparent about it. They only deleted them from their system when they found out and the
[01:52:21.680 --> 01:52:28.080]   information was still there anyways. So we know that in the end, their most important thing was
[01:52:28.080 --> 01:52:32.400]   Facebook and keep saving face in making the most amount of money. So no, I don't think that they
[01:52:32.400 --> 01:52:37.120]   really want to make a difference in the world or try to do the right thing. I think that they'll
[01:52:37.120 --> 01:52:42.560]   only do that if they have to or if it ends up hurting their bottom line by people deleting Facebook.
[01:52:42.560 --> 01:52:46.720]   And in the end, that's really like legislation I think is important. But I think that you're
[01:52:46.720 --> 01:52:50.640]   right, Leo, that deleting Facebook is the biggest message that you could send to them. And if there's
[01:52:50.640 --> 01:52:58.000]   a huge exodus, they'll start to listen. >> Legislation that Facebook wrote went into effect or went
[01:52:58.000 --> 01:53:00.960]   up for a vote and looks like we'll go into effect in the state of Maryland.
[01:53:00.960 --> 01:53:03.600]   >> Tell us about that. >> What was that?
[01:53:03.600 --> 01:53:10.480]   >> Right, so if you'll remember, every big media outlet was touting their exclusive sit down with
[01:53:10.480 --> 01:53:14.880]   Cheryl Sandberg and of course she was on a media tour. We're over and over again, she talked about
[01:53:14.880 --> 01:53:21.680]   industry leading thought, industry leading thoughts, industry leading ideas, industry leading policies,
[01:53:21.680 --> 01:53:28.800]   which was all lobbyist code for regulation. So while she's doing this and days before
[01:53:28.800 --> 01:53:33.840]   Zuckerberg shows up on the Hill, Facebook was meeting with lawmakers in different states,
[01:53:33.840 --> 01:53:39.680]   one of them being Maryland, where, and I can find the link to this, but Facebook wrote the
[01:53:39.680 --> 01:53:45.200]   legislation. >> See on the surface of this, this sounds like good legislation plans to regulate
[01:53:45.200 --> 01:53:51.760]   political ads on Facebook. >> Right, but they, so nothing is like,
[01:53:51.760 --> 01:53:58.560]   everything that happened was Zuckerberg giving testimony that was all for show,
[01:53:58.560 --> 01:54:03.200]   that was all for show. But everybody was making fun of it and they were very distracted, but on
[01:54:03.200 --> 01:54:09.760]   the back end, we were getting scraped on the back end. >> We got scraped on the back end folks.
[01:54:09.760 --> 01:54:15.920]   >> So Will Castleberry, Facebook's vice president for state policy, says,
[01:54:15.920 --> 01:54:20.480]   yeah, we helped draft the Maryland legislation and we look forward to implementing it.
[01:54:20.480 --> 01:54:25.680]   No, that doesn't, that's not necessarily bad, that means they want to get the heat off of them.
[01:54:25.680 --> 01:54:30.000]   Maybe they found a way that they think will help. >> We believe, but Leo, look, go back.
[01:54:30.000 --> 01:54:33.920]   So this was the key thing. >> We believe this bill will be a national model
[01:54:33.920 --> 01:54:38.400]   for the other 49 states to follow. Avoiding, by the way, federal regulation, right?
[01:54:38.400 --> 01:54:43.120]   >> Right, and if there's some regulation that gets started that takes all of the heat off of
[01:54:43.120 --> 01:54:50.560]   Zuckerberg, because technically nobody broke any laws. So this is all about avoiding, this is all
[01:54:50.560 --> 01:54:56.640]   about showing up in public, keeping the share price high and avoiding federal regulation.
[01:54:56.640 --> 01:55:01.760]   >> So part of the problem is that the laws that affect broadcasters about political ads,
[01:55:01.760 --> 01:55:05.040]   that they have to, for instance, reveal who bought the ad, don't apply to digital.
[01:55:05.040 --> 01:55:08.160]   >> Right. >> And this is the honest, this is the federal laws,
[01:55:08.160 --> 01:55:13.920]   the Honest Ads Act. Is Facebook supporting the Honest Ads Act? I think they are, or not.
[01:55:13.920 --> 01:55:20.960]   This Maryland law looks a lot like the Honest Ads Act. It says, this is an article from the
[01:55:20.960 --> 01:55:25.440]   Baltimore Senate, resembles legislation in Congress aimed at monitoring political advertising on
[01:55:25.440 --> 01:55:29.440]   social media. It would affect Google, by the way, as well as Facebook. New York, California,
[01:55:29.440 --> 01:55:34.240]   Connecticut also weighing their own measures. We want to keep the platforms accountable.
[01:55:34.240 --> 01:55:39.040]   I'm trying to find the part where it says what. >> Whether they are or not.
[01:55:39.040 --> 01:55:45.600]   >> What does? >> They will have to. Really what all of this is about at the end is that,
[01:55:45.600 --> 01:55:50.560]   and I know I've said this before on the show, but there's three companies that know data better
[01:55:50.560 --> 01:55:55.280]   than anybody else, and they're eating the world, they're disrupting industries, and they are
[01:55:56.000 --> 01:56:02.320]   undermining the privacy of people every single day, more and more, right? Facebook, Amazon, and
[01:56:02.320 --> 01:56:11.920]   Google. They're doing it largely with a black box of data that they control, and that data being
[01:56:11.920 --> 01:56:20.000]   the new oil, they're using that to become one of the three of the most powerful companies in the
[01:56:20.000 --> 01:56:29.840]   history of humanity. The transparency issue is a big part of the issue. Facebook, the least
[01:56:29.840 --> 01:56:35.200]   transparent, Amazon somewhere in the middle, Google, a little bit more transparent of what they do
[01:56:35.200 --> 01:56:43.440]   with the data and all that. All of these companies you can bet over the coming years are going to
[01:56:43.440 --> 01:56:49.920]   be under enormous pressure to tell what data they're actually collecting, what they do with it,
[01:56:49.920 --> 01:56:59.280]   what it's worth to them, and what they can do to actually empower people to better understand this
[01:56:59.280 --> 01:57:05.280]   and take responsibility for it. >> It is so interesting. I think what you said is such a great summary
[01:57:05.280 --> 01:57:11.680]   of what we're dealing with here, and the analogy of data as the new oil is a really compelling one,
[01:57:11.680 --> 01:57:16.720]   and what's fascinating about this is the first time probably in human history when the oil,
[01:57:16.720 --> 01:57:20.320]   the diamonds, or the gold, or whatever it is, is actually us. >> Us.
[01:57:20.320 --> 01:57:22.000]   >> Us. >> And it's- >> We're being mined.
[01:57:22.000 --> 01:57:28.800]   >> We're being mined. We're being mined for our money. It's basically an outgrowth of the advertising
[01:57:28.800 --> 01:57:35.120]   industry plus a lot more on top of that, and shouldn't we at least know when we're being mined,
[01:57:35.120 --> 01:57:41.200]   and when we're offering up bits of ourselves to somebody who's going to then turn around and
[01:57:41.200 --> 01:57:47.440]   sell to us. It's almost like we're giving something away and then being asked to buy it back for
[01:57:47.440 --> 01:57:54.640]   purchases that we make. >> I have to say though, on the face of it, the idea that Facebook would
[01:57:54.640 --> 01:58:01.200]   work with Maryland legislators to try to craft something that is both helpful and something
[01:58:01.200 --> 01:58:06.400]   that Facebook could live with is not necessarily a bad thing. That's how politics works, Amy, right?
[01:58:06.400 --> 01:58:11.440]   >> You're trying to reach consensus. >> And it's not unprecedented. That is what lobbying is.
[01:58:11.440 --> 01:58:21.040]   >> Yeah. >> I think that my job is to collect data and to model it and to figure out what all of
[01:58:21.040 --> 01:58:25.840]   it means and what the next order risk and opportunity is many, many years into the future.
[01:58:25.840 --> 01:58:30.960]   One of the early signals that we look for are contradictions. So to me, it was very-
[01:58:31.840 --> 01:58:35.600]   like these were some weak signals that I was paying attention to over the past couple of weeks. The
[01:58:35.600 --> 01:58:42.240]   fact that quietly this legislation was being written, it potentially becomes the model. It
[01:58:42.240 --> 01:58:48.320]   eludes some regulatory challenges right around the same time that it becomes apparent that our
[01:58:48.320 --> 01:58:54.080]   data is being continually mined, refined, and productized. But in ways that we may not have
[01:58:54.080 --> 01:59:00.240]   thought through or intended, all this sort of constellation of interesting facts and data over
[01:59:00.240 --> 01:59:05.760]   the past couple of weeks, I think points us in a direction that we should be concerned about.
[01:59:05.760 --> 01:59:11.200]   But it's hard to be concerned and to really think through things when the spectacle is so
[01:59:11.200 --> 01:59:15.920]   fascinating and fun. It was fun to watch all of this happening right now.
[01:59:15.920 --> 01:59:19.680]   >> Was it fun for everybody though? I think it was fun for us.
[01:59:19.680 --> 01:59:23.200]   >> It was fun for Mark. >> I think that most people, if you ask them,
[01:59:23.200 --> 01:59:28.320]   did you see what happened this week? They'd be like, "Oh, yeah, Mark Zuckerberg. He looks like
[01:59:28.320 --> 01:59:31.440]   data from Star Trek." That's it. >> That was it.
[01:59:31.440 --> 01:59:34.560]   >> That's what most people got out of that. Mark Zuckerberg is boring.
[01:59:34.560 --> 01:59:38.400]   >> You think Amy that Mark walked away saying, "Yes, victory."
[01:59:38.400 --> 01:59:45.680]   >> I mean, I would have if I was him. He escaped any serious questions. Clearly,
[01:59:45.680 --> 01:59:50.960]   did you hear some of the questions? I think it was Senator Orrin Hatch who was trying to
[01:59:50.960 --> 01:59:54.640]   suss out whether or not Facebook and Twitter were kind of the same thing.
[01:59:54.640 --> 01:59:56.400]   >> I know. >> I think one of the questions--
[01:59:56.400 --> 01:59:59.040]   >> It's depressing. >> I might have him confused with Lindsey Graham,
[01:59:59.040 --> 02:00:03.360]   but somebody asked the question, "Is Twitter the same as Facebook?" It's like,
[02:00:03.360 --> 02:00:11.920]   it wasn't even rhetorical. He escaped unscathed. If he had gone in front of people who
[02:00:11.920 --> 02:00:18.560]   wouldn't have had to ask-- People were asking questions about whether or not,
[02:00:18.560 --> 02:00:23.440]   even if you log out of Facebook, you're still being tracked. It's a clear indication that they
[02:00:23.440 --> 02:00:30.240]   don't understand cookies. If he had actually faced people that asked serious questions,
[02:00:30.240 --> 02:00:34.320]   or if anybody who was up there that didn't understand the tech could have and would have
[02:00:34.320 --> 02:00:40.960]   pressed on the other civil liberties piece of this, I think it would have been different.
[02:00:40.960 --> 02:00:45.360]   >> I think the platform was designed not to even allow that chat that somebody wanted to.
[02:00:45.360 --> 02:00:51.920]   I observed this before it began. I said, "This is really just going to be a chance for a member
[02:00:51.920 --> 02:00:55.440]   of Congress to stand up, say something that they can use in an ad later." Because
[02:00:55.440 --> 02:00:59.920]   in the Senate, they got five minutes. In the House, they got four minutes. There was no time for
[02:00:59.920 --> 02:01:08.080]   follow-up. Mark very cleverly used up the clock by doing longer answers, which many members of
[02:01:08.080 --> 02:01:12.480]   Congress were at pains to interrupt trying to get their second question in. >> Yeah, even drink
[02:01:12.480 --> 02:01:16.720]   his water slowly. >> Yeah, whoever designed it, designed it with Facebook in mind, frankly,
[02:01:16.720 --> 02:01:24.560]   wasn't. I don't know that the... I don't want to be Alex Jones here and a conspiracy theorist.
[02:01:24.560 --> 02:01:28.080]   But we... >> I don't think it's conspiracy just to say that...
[02:01:28.080 --> 02:01:33.360]   >> They're in each other's pockets. >> Well, they played the game.
[02:01:33.360 --> 02:01:38.720]   >> They literally relied on Facebook to get elected. I mean, that was a good... How much money did
[02:01:38.720 --> 02:01:42.800]   each one of those... There's a great story somewhere. Somebody... >> They all got money from Mark.
[02:01:43.600 --> 02:01:48.000]   And not just the tangent, not the obvious money, but every one of them relied on Facebook to get
[02:01:48.000 --> 02:01:54.240]   elected. Every one of them. >> That's a larger issue that probably isn't for us. But there's the
[02:01:54.240 --> 02:01:58.720]   larger political issue. We talked about this on Wednesday. There's an economic theory, I think
[02:01:58.720 --> 02:02:04.480]   it's called the Peltzman Theory, that there are three constituents here. There's Monopoly's big
[02:02:04.480 --> 02:02:11.600]   business. There's politicians and there's users. And they each have kind of differing goals.
[02:02:11.600 --> 02:02:16.640]   The politicians want to get reelected. The users want to pay less. Or, you know, in the case of
[02:02:16.640 --> 02:02:22.480]   Facebook where it's free, maybe, I don't know, get spied on less. And the big businesses want as a
[02:02:22.480 --> 02:02:27.360]   minimal regulation as they can get to continue on with what they're doing. And what happens is you
[02:02:27.360 --> 02:02:32.000]   get this kind of nice little feedback loop and everybody gets kind of what they want. Nobody is
[02:02:32.000 --> 02:02:39.200]   100% happy. But at the same time, we don't get any kind of... The voters really don't get any kind
[02:02:39.200 --> 02:02:43.920]   of privacy protection or regulation of these companies because that's not what politicians
[02:02:43.920 --> 02:02:48.560]   are looking for. In fact, Facebook's very helpful to them. They're not trying to shut down Facebook.
[02:02:48.560 --> 02:02:51.920]   >> No. And if there was somebody... I wish I could remember who wrote this piece, but there was a
[02:02:51.920 --> 02:02:57.920]   piece of documenting which politicians went up to Zuckerberg afterward and basically fond over him.
[02:02:57.920 --> 02:03:00.720]   >> Yeah. >> We're so proud of what you started.
[02:03:00.720 --> 02:03:01.600]   >> Well, of course. >> He's a kid in your dorm room.
[02:03:01.600 --> 02:03:07.600]   >> He's a kid maker. >> This is the symbol of American success. I don't think a lot of interest
[02:03:07.600 --> 02:03:14.080]   in actually angering or unsettling him outside of that grandstanding for sure. But I think it would
[02:03:14.080 --> 02:03:18.960]   be interesting to talk for a second about whether or not people would have the appetite to pay for
[02:03:18.960 --> 02:03:21.840]   a service like Facebook. We were just talking about that. >> They brought that up.
[02:03:21.840 --> 02:03:24.640]   >> Journalism, they brought it up. People have been bringing it up for the whole week.
[02:03:24.640 --> 02:03:29.120]   "Hey, what if we paid for it? Do you need to change your business model Facebook?"
[02:03:29.120 --> 02:03:32.560]   And I think this is, again, I keep coming back to this, but it's sort of a personal
[02:03:32.560 --> 02:03:39.040]   responsibility thing. I don't think that there's any actual appetite for paying for social media.
[02:03:39.040 --> 02:03:45.840]   As soon as any service starts to bring up a fee, people freak out and walk away, which is weird
[02:03:45.840 --> 02:03:50.960]   because we'll pay what, like $12, $13, $14 to go to the movies? >> Yeah.
[02:03:50.960 --> 02:03:55.600]   >> Yeah. >> It's kind of confounding. >> As much time as people spend on it,
[02:03:55.600 --> 02:04:00.160]   I mean, I agree completely. I never thought people would pay for the information either,
[02:04:00.160 --> 02:04:04.560]   that it would survive more than a year, but it eventually- >> Well, there's a key difference,
[02:04:04.560 --> 02:04:08.880]   though. There've been plenty of studies. The information was always a paid product.
[02:04:08.880 --> 02:04:13.520]   The challenge is there's a cognitive bias that happens. >> Once it's free.
[02:04:13.520 --> 02:04:20.640]   >> When something was free and now you're being compelled for whatever reason to pay,
[02:04:20.640 --> 02:04:25.040]   that feels like something is being taken away from us. And historically speaking,
[02:04:25.040 --> 02:04:29.920]   regardless of what industry or field it is, we won't do it. >> It doesn't work sometimes,
[02:04:29.920 --> 02:04:36.000]   like think about with cell phones, we completely destroyed the value of cell phones for about five
[02:04:36.000 --> 02:04:41.840]   or six years there. And then eventually we got to the point where now we're paying whatever,
[02:04:41.840 --> 02:04:47.120]   40 bucks a month or 30 bucks a month or having to buy one separately.
[02:04:47.120 --> 02:04:51.040]   >> But it was a different type of phone. That the free, you're talking about the free model?
[02:04:51.040 --> 02:04:54.720]   >> Right, right. Well, you know, when you used to be, you get a free upgrade,
[02:04:54.720 --> 02:05:00.960]   you pay $200 for your phone, which was massively subsidized. Now you pay everybody's pay- >> But
[02:05:00.960 --> 02:05:06.320]   in Japan, you would get the phones for free. You would get a free phone when I lived there.
[02:05:06.320 --> 02:05:12.640]   The early smartphones were free. You just signed up for the service. But once the truly smartphones
[02:05:12.640 --> 02:05:20.640]   hit, so the first smartphone that I had was in i-mode from entity Dogumall, which was a
[02:05:20.640 --> 02:05:24.400]   totally different kind of phone. Everything else was free, but you had to pay for that.
[02:05:24.400 --> 02:05:33.760]   We've shifted over to the fancy models that people pay for, because there's never been an
[02:05:33.760 --> 02:05:37.840]   alternative, there's never been a way not to pay for a brand new iPhone. >> There's also this
[02:05:37.840 --> 02:05:42.800]   sort of half-paid model. I live in Berkeley. There's a regional news site called Berkeley
[02:05:42.800 --> 02:05:48.560]   side, which I'm a huge fan of. They've done a fantastic job creating a serious local
[02:05:48.560 --> 02:05:53.840]   publication that's internet only. And it's first in the nation, they just raised a million
[02:05:53.840 --> 02:05:58.960]   dollars from its readers in a direct public offering. >> Wow, interesting. >> Small, local news.
[02:05:58.960 --> 02:06:01.680]   >> Isn't that interesting? >> High quality products. >> That's Berkeley, though. >> Totally free.
[02:06:01.680 --> 02:06:05.120]   >> That's Berkeley. >> Still. >> But no, it's Berkeley. >> There's something to be said, though.
[02:06:05.120 --> 02:06:09.600]   Like, you do a very good job, and then you say, look, it's kind of the NPR model, too,
[02:06:09.600 --> 02:06:15.040]   right? Like, please pay for this thing. We're going to give it to you for free, but we're going to use
[02:06:15.040 --> 02:06:20.000]   guilt. >> Guilt works. I mean, guilt absolutely works. >> It's not democratic, though. It's
[02:06:20.000 --> 02:06:25.920]   fundamentally undemocratic, and that's part of the problem I have is that news especially,
[02:06:25.920 --> 02:06:29.840]   I'm kind of the opinion software, all software should be free, that the paid software model is
[02:06:29.840 --> 02:06:34.560]   a broken model. And certainly, social networks should all be free, and news should be free.
[02:06:34.560 --> 02:06:38.560]   You've got to find other ways to monetize it, because it's undemocratic to say that only people
[02:06:38.560 --> 02:06:43.600]   who can afford news should be able to get the good news, and the rest of you are going to get
[02:06:43.600 --> 02:06:47.040]   fake news, and crappy news, and link-baity news. That's not right.
[02:06:47.040 --> 02:06:50.640]   >> And yet, when Freedom of the Press started, everybody had to pay to get that cheat.
[02:06:50.640 --> 02:06:52.400]   >> Yeah, the broadsheets. >> The broadsheets.
[02:06:52.400 --> 02:06:53.040]   >> Yeah. >> Right?
[02:06:53.040 --> 02:06:55.200]   >> But some of the reasons we've never charged for it.
[02:06:55.200 --> 02:07:02.000]   >> One person bought it, and then other people said it, talked about it, whatever, shared it around.
[02:07:02.000 --> 02:07:09.120]   I think that it's kind of HBO model. >> Like, we know people steal it, but some of you are going
[02:07:09.120 --> 02:07:13.360]   to pay for it, and it all works out in the end. >> The LA Times accidentally ran a new
[02:07:13.360 --> 02:07:18.640]   business model experiment without even realizing it. A couple weeks ago,
[02:07:18.640 --> 02:07:23.120]   their page loads times were really really slow, and people couldn't fear it was going on.
[02:07:23.120 --> 02:07:29.760]   A friend of mine was poking around, and somebody was using their site to mine for bitcoins.
[02:07:29.760 --> 02:07:31.200]   >> Oh my God.
[02:07:31.200 --> 02:07:33.680]   >> So they had been hacked in a weird way.
[02:07:33.680 --> 02:07:37.600]   >> Oh my God. >> But also maybe a genius way to raise some cash.
[02:07:37.600 --> 02:07:38.480]   >> Yeah. >> Right?
[02:07:38.480 --> 02:07:43.600]   Just not for the LA Times, unfortunately. >> And to close the loop on the Facebook thing,
[02:07:43.600 --> 02:07:52.560]   I do think Facebook is primarily over, and under 30, there's hardly any people that are
[02:07:52.560 --> 02:07:57.840]   actively engaged on Facebook. So what's next is the important question.
[02:07:57.840 --> 02:08:04.560]   And for the hearings themselves, I think that you have to take what happened,
[02:08:04.560 --> 02:08:08.720]   not just in the event itself, which was somewhat sad and disappointing, and
[02:08:08.720 --> 02:08:15.520]   not what it could have been or should have been. But think of it as a first step or maybe second
[02:08:15.520 --> 02:08:20.080]   step, right? Because they did call after the election a few of these tech heads before
[02:08:20.080 --> 02:08:30.160]   the Congress to testify. Again, it's about this scrutiny coming on to these folks about the way
[02:08:30.160 --> 02:08:34.720]   that they use data. And while this wasn't ideal, it wasn't what it could have been or should have
[02:08:34.720 --> 02:08:35.360]   been. >> To start.
[02:08:35.360 --> 02:08:40.160]   >> It is a start. And you're going to increasingly in the coming years, see more pressure on them
[02:08:40.160 --> 02:08:45.120]   to be transparent. And the Maryland legislation, I think, is another step in that direction.
[02:08:45.120 --> 02:08:50.720]   They're going to be more transparent about what data they take, how they use it, how much it's
[02:08:50.720 --> 02:08:56.880]   worth, and then what they can do to give people control of their own data.
[02:08:56.880 --> 02:09:01.360]   And somebody in the chat room brought this up and maybe not exactly this way, but also there's
[02:09:01.360 --> 02:09:07.440]   the possibility that a transparency here, I'm going to sell your data to marketers.
[02:09:07.440 --> 02:09:11.680]   You will see ads related to the things that you do on this platform. If you would like to not
[02:09:11.680 --> 02:09:15.360]   receive these ads, it will cost you $10 a month to use the platform.
[02:09:15.360 --> 02:09:18.160]   >> Yeah, I bet most people will say, fine. I don't want to-
[02:09:18.160 --> 02:09:18.640]   >> Pay it.
[02:09:18.640 --> 02:09:24.480]   >> Yeah, am I wrong? I have a problem with Facebook using my information to sell ads.
[02:09:24.480 --> 02:09:30.880]   That's not where the issue is. The issue is more using my mood to take advantage of me to sell ads
[02:09:30.880 --> 02:09:38.160]   or selling my information to a foreign government or using my information in kind of sleazier ways
[02:09:38.160 --> 02:09:42.800]   than just straight up. You talked about Jeeps, you should see Jeep ads. What would be wrong with
[02:09:42.800 --> 02:09:47.040]   that? You're actually interested in Jeeps. >> They're taking more data than what they're telling
[02:09:47.040 --> 02:09:51.440]   us. The problem- >> They're taking more data and they're using it in ways that are kind of not so
[02:09:51.440 --> 02:09:55.920]   nice. >> We're talking about what's already happened.
[02:09:55.920 --> 02:10:06.000]   Facebook saying, if at the end of all of this, the end result is, we'll give you the opportunity
[02:10:06.000 --> 02:10:13.440]   to opt out so that we won't use your data to surface ads. That sort of shields us from
[02:10:13.440 --> 02:10:17.760]   everything that's coming. The next social networks are going to be part of mixed reality, not
[02:10:18.560 --> 02:10:22.560]   text and photo and video-based social networks that we have right now.
[02:10:22.560 --> 02:10:26.320]   >> Now's the time that we get to work on social networks that don't rely on surveillance
[02:10:26.320 --> 02:10:29.840]   capitalism to survive. There must be some other way to create the next thing
[02:10:29.840 --> 02:10:36.720]   that doesn't require this kind of schiz-y invasion of privacy, or is that not good?
[02:10:36.720 --> 02:11:04.080]   >> Twitter. >> Twitter. >> Twitter. >> Twitter. >> Twitter.
[02:11:04.080 --> 02:11:05.040]   What was it, not been?
[02:11:05.040 --> 02:11:07.480]   But we just have to report that when it comes
[02:11:07.480 --> 02:11:11.400]   to actual smartphone sales, Apple and that product line
[02:11:11.400 --> 02:11:14.640]   are taking over a third of the entire revenue
[02:11:14.640 --> 02:11:15.880]   straight out the back.
[02:11:15.880 --> 02:11:18.080]   So all those mobile phone manufacturers out there,
[02:11:18.080 --> 02:11:20.040]   the billions of smartphones out there,
[02:11:20.040 --> 02:11:21.080]   Apple is sucking it up.
[02:11:21.080 --> 02:11:24.000]   Now we have a mixed panel here.
[02:11:24.000 --> 02:11:26.360]   Some Apple users, some Android users,
[02:11:26.360 --> 02:11:28.860]   Mike, you've actually bought your wood framed iPad here,
[02:11:28.860 --> 02:11:31.880]   you haven't gone for the iPhone X or X,
[02:11:31.880 --> 02:11:33.200]   depending on how you describe it.
[02:11:33.200 --> 02:11:34.480]   10 or X, which one would--
[02:11:34.480 --> 02:11:35.320]   10, yeah.
[02:11:35.320 --> 02:11:38.600]   --the 10, yep, because I think the next one's going to be an 11.
[02:11:38.600 --> 02:11:40.800]   They pull the Microsoft, they skipped a generation,
[02:11:40.800 --> 02:11:43.280]   there has been no iPhone 9.
[02:11:43.280 --> 02:11:44.840]   But yeah, iPhone 10, for sure.
[02:11:44.840 --> 02:11:47.320]   iPhone 789, no, sorry.
[02:11:47.320 --> 02:11:48.600]   One of the things I was surprised about,
[02:11:48.600 --> 02:11:51.080]   obviously, Apple always dominates profits.
[02:11:51.080 --> 02:11:53.240]   If you think about what does Apple make,
[02:11:53.240 --> 02:11:56.400]   the one thing they make better than anyone is profits.
[02:11:56.400 --> 02:11:59.320]   But iPhone X actually had much significantly higher
[02:11:59.320 --> 02:12:01.920]   profits than any of the other iPhones.
[02:12:01.920 --> 02:12:03.440]   The numbers are ridiculously priced, too.
[02:12:03.440 --> 02:12:06.160]   Yeah, well, I guess, I mean, we're probably
[02:12:06.160 --> 02:12:08.280]   going to talk about the Samsung, the cost of the Samsung
[02:12:08.280 --> 02:12:09.000]   screen, and so on.
[02:12:09.000 --> 02:12:11.360]   So it's expensive for them to make.
[02:12:11.360 --> 02:12:15.360]   And it's amazing they got as many people to buy it as they did.
[02:12:15.360 --> 02:12:20.800]   35% of the industry's profits were the iPhone X, 19%.
[02:12:20.800 --> 02:12:29.800]   The iPhone 8, 15% iPhone 8+, 6% iPhone 7, 5% iPhone 7.
[02:12:29.800 --> 02:12:33.400]   The first non-Apple device on the list is the Galaxy Note 8
[02:12:33.400 --> 02:12:37.040]   at 3.9%, basically 4% of the industry profits.
[02:12:37.040 --> 02:12:39.800]   So Apple, once again, is winning it all.
[02:12:39.800 --> 02:12:43.200]   And this doesn't even tell you what they're making on app sales.
[02:12:43.200 --> 02:12:45.160]   They get a third of all the app sales.
[02:12:45.160 --> 02:12:49.760]   So getting on the streaming services like Apple Music and so on,
[02:12:49.760 --> 02:12:50.880]   it goes on and on and on.
[02:12:50.880 --> 02:12:54.840]   They make money on everything, every aspect of the--
[02:12:54.840 --> 02:12:56.640]   just about every aspect of the phones.
[02:12:56.640 --> 02:12:58.800]   But this is just for selling the hardware.
[02:12:58.800 --> 02:13:02.360]   And it's really-- it's the business story of, I think,
[02:13:02.360 --> 02:13:04.600]   the decade or the millennium or something like that.
[02:13:04.600 --> 02:13:06.040]   They just are printing money.
[02:13:06.040 --> 02:13:07.720]   And it's kind of ridiculous.
[02:13:07.720 --> 02:13:10.720]   They're on track to be the first trillion dollar valuation
[02:13:10.720 --> 02:13:11.640]   company.
[02:13:11.640 --> 02:13:13.400]   I think Amazon might get there first.
[02:13:13.400 --> 02:13:14.440]   I'm not sure.
[02:13:14.440 --> 02:13:15.440]   Could be.
[02:13:15.440 --> 02:13:15.960]   Could be.
[02:13:15.960 --> 02:13:16.360]   They may have--
[02:13:16.360 --> 02:13:17.800]   There's a fight for it at the moment.
[02:13:17.800 --> 02:13:20.400]   Not if President Trump has anything to do with it.
[02:13:20.400 --> 02:13:21.240]   Yes.
[02:13:21.240 --> 02:13:22.800]   We may come on to that later on.
[02:13:22.800 --> 02:13:24.560]   I mean, Greg and Dwight, what's your view on this?
[02:13:24.560 --> 02:13:26.480]   I mean, are you Apple fanboys?
[02:13:26.480 --> 02:13:29.440]   Are you Android users?
[02:13:29.440 --> 02:13:32.360]   I am an Apple user.
[02:13:32.360 --> 02:13:37.000]   And I've used pretty much all their products.
[02:13:37.000 --> 02:13:40.080]   I'm-- it's not a surprise.
[02:13:40.080 --> 02:13:42.800]   They are aiming at the high end of the market,
[02:13:42.800 --> 02:13:45.240]   where the profits naturally are better.
[02:13:45.240 --> 02:13:48.040]   They made Apple an aspirational brand.
[02:13:48.040 --> 02:13:52.080]   So you want to have it, maybe even if you can't afford it.
[02:13:52.080 --> 02:13:55.680]   And they have a variety of products
[02:13:55.680 --> 02:13:59.280]   in a variety of price ranges.
[02:13:59.280 --> 02:14:01.480]   So I think they're being very smart about it.
[02:14:01.480 --> 02:14:04.320]   The question is, is how much can they sustain?
[02:14:04.320 --> 02:14:08.080]   They have earnings coming up on May 1.
[02:14:08.080 --> 02:14:11.360]   And there has been several analysts who have said,
[02:14:11.360 --> 02:14:15.320]   well, they're just not doing as well as expected
[02:14:15.320 --> 02:14:17.280]   on the iPhone 10.
[02:14:17.280 --> 02:14:20.160]   And so we'll actually-- we'll get a better look at what
[02:14:20.160 --> 02:14:23.800]   is actually going to happen when those earnings drop.
[02:14:23.800 --> 02:14:26.320]   So I'm an Apple user or an Apple consumer.
[02:14:26.320 --> 02:14:27.760]   I'm not a fanboy.
[02:14:27.760 --> 02:14:31.160]   I was a fanboy, but increasingly I'm less inspired
[02:14:31.160 --> 02:14:32.080]   by Apple's going forward.
[02:14:32.080 --> 02:14:34.560]   I think the price is getting out of whack
[02:14:34.560 --> 02:14:36.880]   with the value being delivered.
[02:14:36.880 --> 02:14:42.520]   So as it says here, 21% of the total revenue goes to Apple,
[02:14:42.520 --> 02:14:45.480]   but they make 35% of the total industry profits.
[02:14:45.480 --> 02:14:46.880]   I've heard different numbers.
[02:14:46.880 --> 02:14:49.880]   I heard they sort of take 30% of the total revenue,
[02:14:49.880 --> 02:14:52.400]   but 70% of the total profits or whatever.
[02:14:52.400 --> 02:14:55.040]   But either way, Apple's charging is working
[02:14:55.040 --> 02:14:58.520]   in the premium end of the market and charging premium pricing
[02:14:58.520 --> 02:15:01.160]   that it has a vast profit margin.
[02:15:01.160 --> 02:15:03.480]   But then not only that, but part of the reason they're
[02:15:03.480 --> 02:15:05.400]   making the profit margin is a couple of things.
[02:15:05.400 --> 02:15:07.000]   One is security.
[02:15:07.000 --> 02:15:09.840]   Unlike a Microsoft or an Android, your device
[02:15:09.840 --> 02:15:12.840]   is more or less secure, although there's signs
[02:15:12.840 --> 02:15:14.720]   that there's cracks in that coming forward.
[02:15:14.720 --> 02:15:18.560]   It's not buggy compared to anybody else's smartphones
[02:15:18.560 --> 02:15:20.800]   or anybody else's products.
[02:15:20.800 --> 02:15:23.680]   And so they do actually have a reason for being regarded
[02:15:23.680 --> 02:15:26.240]   as a premium supplier.
[02:15:26.240 --> 02:15:28.400]   But I think the other thing that we are not covering here
[02:15:28.400 --> 02:15:30.000]   that's not covered in this report, of course,
[02:15:30.000 --> 02:15:33.560]   is that Apple is very much increasing its output,
[02:15:33.560 --> 02:15:37.200]   Avenue revenue per customer, or average revenue per user,
[02:15:37.200 --> 02:15:38.480]   to be literal.
[02:15:38.480 --> 02:15:40.760]   By having things like the headphones,
[02:15:40.760 --> 02:15:43.920]   the Apple earphones they've got with the X1 chip inside,
[02:15:43.920 --> 02:15:46.280]   Apple TVs, the Mac has increasingly
[02:15:46.280 --> 02:15:49.080]   in accessory to the iPhone, the App Store,
[02:15:49.080 --> 02:15:51.280]   the music, the HomePod.
[02:15:51.280 --> 02:15:54.080]   All those accessories mean that you're not just buying
[02:15:54.080 --> 02:15:55.120]   the phone anymore.
[02:15:55.120 --> 02:15:59.000]   The phone is the linchpin of an entire ecosystem around,
[02:15:59.000 --> 02:16:01.920]   which you're going to buy one or more of those fries
[02:16:01.920 --> 02:16:03.520]   with the iPhone burger.
[02:16:03.520 --> 02:16:06.320]   And I think you don't underestimate
[02:16:06.320 --> 02:16:08.200]   that part of the strategy.
[02:16:08.200 --> 02:16:10.280]   Greg, just to follow up on that and clarify,
[02:16:10.280 --> 02:16:14.600]   you said that your understanding was that the Apple
[02:16:14.600 --> 02:16:17.960]   was north of 70% total profits.
[02:16:17.960 --> 02:16:20.760]   That used to be the case, and now they're up to 87%.
[02:16:20.760 --> 02:16:23.760]   The 35% is the iPhone 10 alone.
[02:16:23.760 --> 02:16:27.600]   So the iPhone 10 by itself as one of the many profitable
[02:16:27.600 --> 02:16:32.320]   Apple devices makes more profit than the top 600 Android
[02:16:32.320 --> 02:16:33.600]   makers combined.
[02:16:33.600 --> 02:16:34.920]   It's utterly bonkers.
[02:16:34.920 --> 02:16:37.760]   I mean, it's almost like an Android is a race to the bottom,
[02:16:37.760 --> 02:16:39.480]   whereas Apple has just said, yeah,
[02:16:39.480 --> 02:16:40.920]   let's make this a premium thing.
[02:16:40.920 --> 02:16:44.280]   But then it isn't really reflected in their other product
[02:16:44.280 --> 02:16:44.880]   lines, and that--
[02:16:44.880 --> 02:16:46.480]   I mean, Greg, you said this is the linchpin,
[02:16:46.480 --> 02:16:49.800]   but I know a lot of MacBook users who are really, really
[02:16:49.800 --> 02:16:52.800]   annoyed at the moment that their laptops are not getting
[02:16:52.800 --> 02:16:53.120]   loved.
[02:16:53.120 --> 02:16:55.200]   We're using outdated hardware.
[02:16:55.200 --> 02:16:56.960]   They've just said that the next MacBook isn't going
[02:16:56.960 --> 02:17:00.040]   to be coming through the line until 2019.
[02:17:00.040 --> 02:17:03.720]   I mean, as Apple given up on laptops--
[02:17:03.720 --> 02:17:06.320]   I think there's a few things happening there.
[02:17:06.320 --> 02:17:10.160]   One is Intel's product line has been disrupted.
[02:17:10.160 --> 02:17:12.000]   Mr. Coupler Generations in the CPUs.
[02:17:12.000 --> 02:17:15.080]   We always expected to see Intel release new CPUs.
[02:17:15.080 --> 02:17:16.160]   They did the TikTok.
[02:17:16.160 --> 02:17:17.760]   And we expected them to happen every year.
[02:17:17.760 --> 02:17:21.120]   And then Intel shifted to a three-year strategy.
[02:17:21.120 --> 02:17:23.240]   And I just have the sense-- and this
[02:17:23.240 --> 02:17:25.960]   is just pure speculation on my part--
[02:17:25.960 --> 02:17:31.280]   Apple was doing something, and they missed a generation or two
[02:17:31.280 --> 02:17:34.320]   because whatever it was they were working on didn't work.
[02:17:34.320 --> 02:17:38.400]   So I know that, for example, Apple Intel completely
[02:17:38.400 --> 02:17:41.800]   failed to deliver on mobile processors and mobile CPUs.
[02:17:41.800 --> 02:17:43.640]   And yeah, that's right.
[02:17:43.640 --> 02:17:47.120]   And so if you were betting you had a whole production line ready
[02:17:47.120 --> 02:17:50.920]   to go, and then Intel suddenly pulls the pin on that CPU
[02:17:50.920 --> 02:17:53.800]   that they were going to push out there, and you're Apple.
[02:17:53.800 --> 02:17:56.160]   Because Apple puts all of its weight
[02:17:56.160 --> 02:17:57.760]   behind a single product.
[02:17:57.760 --> 02:18:01.240]   There's not-- like in a Samsung, they have 9 to 10 product
[02:18:01.240 --> 02:18:02.000]   families.
[02:18:02.000 --> 02:18:05.160]   Apple has two smartphones in the market today, basically
[02:18:05.160 --> 02:18:06.680]   the X and the 8.
[02:18:06.680 --> 02:18:08.440]   They don't have this massive diversity,
[02:18:08.440 --> 02:18:11.080]   which allows them to fail in one product line and fail over.
[02:18:11.080 --> 02:18:13.680]   They go all the way in.
[02:18:13.680 --> 02:18:17.160]   So if Intel pulled the product line too late,
[02:18:17.160 --> 02:18:20.760]   then Apple might not have had a backup strategy in place.
[02:18:20.760 --> 02:18:23.800]   And that's why we haven't seen the iteration.
[02:18:23.800 --> 02:18:26.520]   So I'm not entirely sure that putting the blame at Apple's
[02:18:26.520 --> 02:18:29.400]   feet is the thing to do.
[02:18:29.400 --> 02:18:30.960]   Of course, it is because it's up to them
[02:18:30.960 --> 02:18:33.400]   to meet the market and meet users' expectations
[02:18:33.400 --> 02:18:34.800]   and to manage user expectations.
[02:18:34.800 --> 02:18:37.440]   I think that's the thing that Apple's failed to do here.
[02:18:37.440 --> 02:18:39.640]   We have an expectation as Apple customers
[02:18:39.640 --> 02:18:43.200]   to see a refresh cycle on the Mac roughly every two years,
[02:18:43.200 --> 02:18:44.440]   a major trend.
[02:18:44.440 --> 02:18:45.840]   And we haven't seen that.
[02:18:45.840 --> 02:18:47.880]   And they haven't come out and talked about it
[02:18:47.880 --> 02:18:49.320]   or been public about it.
[02:18:49.320 --> 02:18:51.680]   And we really feel a bit abandoned.
[02:18:51.680 --> 02:18:55.120]   Like I'm sitting here with a 2012 MacBook waiting
[02:18:55.120 --> 02:18:56.760]   patiently for the next one, because I'm not
[02:18:56.760 --> 02:18:58.600]   leaving my ports behind.
[02:18:58.600 --> 02:19:03.040]   This was the company Google bought briefly
[02:19:03.040 --> 02:19:06.480]   until they realized that Boston Dynamics was doing robots
[02:19:06.480 --> 02:19:12.800]   for the military, scary robots like this running robot.
[02:19:12.800 --> 02:19:15.720]   I think this is the same robot atlas
[02:19:15.720 --> 02:19:20.200]   that they showed earlier, one of the Boston Dynamics employees
[02:19:20.200 --> 02:19:23.680]   kicking, knocking over with a stick.
[02:19:23.680 --> 02:19:26.280]   And the robot was so nice and just got up
[02:19:26.280 --> 02:19:27.880]   and kept doing things.
[02:19:27.880 --> 02:19:30.400]   But I have to think it's kind of making a list, right?
[02:19:30.400 --> 02:19:32.280]   Look at it, it can jump.
[02:19:32.280 --> 02:19:34.120]   I don't know, like a baby when it does that,
[02:19:34.120 --> 02:19:36.080]   like a baby story of what it thinks about it.
[02:19:36.080 --> 02:19:39.840]   Would you like one of these Boston Dynamics for your very own--
[02:19:39.840 --> 02:19:42.240]   What I'd like to know is how much range does it--
[02:19:42.240 --> 02:19:44.480]   how much of a battery does that thing have?
[02:19:44.480 --> 02:19:45.480]   90 minutes?
[02:19:45.480 --> 02:19:46.480]   90 minutes?
[02:19:46.480 --> 02:19:48.120]   Not that specific one, but the one
[02:19:48.120 --> 02:19:49.560]   they're about to sell, 90 minutes.
[02:19:49.560 --> 02:19:51.760]   In 2019, they announced this at TechCrunch
[02:19:51.760 --> 02:19:53.200]   Disrupt this week.
[02:19:53.200 --> 02:19:57.160]   Boston Dynamics is going to start selling their dog.
[02:19:57.160 --> 02:19:57.680]   Wow.
[02:19:57.680 --> 02:19:59.160]   That's the 90-minute one.
[02:19:59.160 --> 02:20:01.120]   Spot Mini.
[02:20:01.120 --> 02:20:03.360]   This is a video that they posted on YouTube of it,
[02:20:03.360 --> 02:20:06.440]   autonomously navigating their office and labs.
[02:20:06.440 --> 02:20:10.680]   Now, they had to run it through once to get an image.
[02:20:10.680 --> 02:20:13.400]   So the robot--
[02:20:13.400 --> 02:20:15.720]   Do you really want this in your house?
[02:20:15.720 --> 02:20:16.720]   They're speeding it up here.
[02:20:16.720 --> 02:20:19.320]   You've seen the other video where it opens the door.
[02:20:19.320 --> 02:20:19.800]   Yeah.
[02:20:19.800 --> 02:20:20.680]   And then let's say the other--
[02:20:20.680 --> 02:20:24.640]   You can get one with an articulated arm.
[02:20:24.640 --> 02:20:28.080]   Be perfect for pinching people.
[02:20:28.080 --> 02:20:30.600]   The robot-- they say it's the quietest robot they've ever made.
[02:20:30.600 --> 02:20:31.120]   It's--
[02:20:31.120 --> 02:20:32.360]   I mean, I don't know how quiet it is,
[02:20:32.360 --> 02:20:36.720]   but it doesn't sound like a steam engine coming at you anyway.
[02:20:36.720 --> 02:20:38.160]   Just give me a canine.
[02:20:38.160 --> 02:20:39.040]   This is autonomous.
[02:20:39.040 --> 02:20:40.800]   This is like an autonomous vehicle.
[02:20:40.800 --> 02:20:42.520]   This is like the DARPA Grand Challenge.
[02:20:42.520 --> 02:20:45.600]   I'd rather pick up the dog poop instead of having one of these.
[02:20:45.600 --> 02:20:47.680]   Well, Kevin Rose suggested they cover it with fur.
[02:20:47.680 --> 02:20:48.920]   I think that's probably--
[02:20:48.920 --> 02:20:49.480]   I don't know.
[02:20:49.480 --> 02:20:51.120]   I don't know if it'll look like this when they sell it.
[02:20:51.120 --> 02:20:52.120]   I don't know how expensive it'll be.
[02:20:52.120 --> 02:20:54.160]   You can cover the little Sony dog with fur in that--
[02:20:54.160 --> 02:20:54.640]   Yeah, I know.
[02:20:54.640 --> 02:20:55.680]   But it looks dog-like.
[02:20:55.680 --> 02:20:57.160]   This thing looks--
[02:20:57.160 --> 02:20:58.160]   I don't know.
[02:20:58.160 --> 02:21:00.160]   This thing looks like a crawler.
[02:21:00.160 --> 02:21:00.680]   Yeah.
[02:21:00.680 --> 02:21:01.680]   Like a--
[02:21:01.680 --> 02:21:02.680]   Well, wait a-- wait a-- wait a-- yeah.
[02:21:02.680 --> 02:21:05.400]   You can't get away from it by running upstairs.
[02:21:05.400 --> 02:21:07.240]   It's just going to follow you.
[02:21:07.240 --> 02:21:09.320]   Have you seen the Black Mirror episode,
[02:21:09.320 --> 02:21:11.480]   a hard metal or heavy metal?
[02:21:11.480 --> 02:21:12.480]   Is it the new--
[02:21:12.480 --> 02:21:13.320]   It's on the latest series.
[02:21:13.320 --> 02:21:14.720]   Yeah, no, I'm not caught up yet.
[02:21:14.720 --> 02:21:15.240]   I can't wait.
[02:21:15.240 --> 02:21:16.160]   I love that show.
[02:21:16.160 --> 02:21:17.600]   At least it does stare slowly.
[02:21:17.600 --> 02:21:19.160]   So if you run up the stairs, you might be OK.
[02:21:19.160 --> 02:21:20.440]   You could run back down.
[02:21:20.440 --> 02:21:21.000]   Yeah.
[02:21:21.000 --> 02:21:21.920]   And then kick it.
[02:21:21.920 --> 02:21:23.760]   By the way, it doesn't go forward downstairs.
[02:21:23.760 --> 02:21:27.240]   It goes backwards, just like me.
[02:21:27.240 --> 02:21:29.280]   So how much is this thing going to come?
[02:21:29.280 --> 02:21:31.720]   I don't say, but they do say they're going to sell it to end users
[02:21:31.720 --> 02:21:33.560]   in 2019.
[02:21:33.560 --> 02:21:34.560]   I mean--
[02:21:34.560 --> 02:21:36.080]   Mini Spot Mini.
[02:21:36.080 --> 02:21:38.440]   This was expensive.
[02:21:38.440 --> 02:21:39.960]   It does look really expensive.
[02:21:39.960 --> 02:21:40.480]   $10,000?
[02:21:40.480 --> 02:21:42.600]   Sounds like you know about this if you said 90 minutes.
[02:21:42.600 --> 02:21:42.920]   Have you--
[02:21:42.920 --> 02:21:43.640]   Have you talked with the--
[02:21:43.640 --> 02:21:45.640]   I was just brushing up before we went on there.
[02:21:45.640 --> 02:21:45.840]   Yeah.
[02:21:45.840 --> 02:21:48.360]   The Google-- the Google folks sold this company.
[02:21:48.360 --> 02:21:49.960]   They realized it's kind of creepy for Google to own.
[02:21:49.960 --> 02:21:50.960]   Or by software now.
[02:21:50.960 --> 02:21:51.480]   Yeah.
[02:21:51.480 --> 02:21:53.040]   That's what's really interesting.
[02:21:53.040 --> 02:21:58.360]   Sun-san, Masayoshi Sun, who has pledged a $100 billion
[02:21:58.360 --> 02:22:00.360]   investment in Silicon Valley Tech.
[02:22:00.360 --> 02:22:02.240]   It's already spent $30 billion of it.
[02:22:02.240 --> 02:22:05.040]   He bought Sprint.
[02:22:05.040 --> 02:22:09.040]   It is now the owner of these scary robots.
[02:22:09.040 --> 02:22:10.800]   They also bought a big chunk of Uber.
[02:22:10.800 --> 02:22:11.240]   Did he?
[02:22:11.240 --> 02:22:11.800]   Yeah.
[02:22:11.800 --> 02:22:13.520]   He's got money to spend.
[02:22:13.520 --> 02:22:15.640]   He bought about 15% of Uber.
[02:22:15.640 --> 02:22:16.240]   Is that interesting?
[02:22:16.240 --> 02:22:18.040]   I don't really understand where all that money came from.
[02:22:18.040 --> 02:22:20.920]   SoftBank was originally a software distributor, I think.
[02:22:20.920 --> 02:22:21.080]   Right?
[02:22:21.080 --> 02:22:22.080]   That's where the name came from.
[02:22:22.080 --> 02:22:22.560]   Yep.
[02:22:22.560 --> 02:22:23.480]   Owner of Ziffdavis.
[02:22:23.480 --> 02:22:24.480]   They bought Ziffdavis.
[02:22:24.480 --> 02:22:25.560]   I remember, yeah.
[02:22:25.560 --> 02:22:27.920]   I was still working there when we had
[02:22:27.920 --> 02:22:28.920]   a call on Sun-san.
[02:22:28.920 --> 02:22:33.480]   By the way, that sound--
[02:22:33.480 --> 02:22:36.640]   I'm not making baseball caps for everybody in the audience.
[02:22:36.640 --> 02:22:39.800]   That's the spot mini still running around.
[02:22:39.800 --> 02:22:41.400]   That sounded like an embroidery machine.
[02:22:41.400 --> 02:22:46.600]   I mean, it's cute.
[02:22:46.600 --> 02:22:47.120]   I don't know.
[02:22:47.120 --> 02:22:48.560]   It's growing on me after a day.
[02:22:48.560 --> 02:22:49.640]   Really?
[02:22:49.640 --> 02:22:50.560]   It doesn't have any personal--
[02:22:50.560 --> 02:22:52.200]   I've always wanted to robot dog.
[02:22:52.200 --> 02:22:53.600]   I'm just--
[02:22:53.600 --> 02:22:57.840]   I mean, really, the technology is kind of impressive, right?
[02:22:57.840 --> 02:23:00.000]   And those legs-- you see how they went sideways, too?
[02:23:00.000 --> 02:23:02.440]   They don't just go forwards and backwards.
[02:23:02.440 --> 02:23:03.960]   I am spot.
[02:23:03.960 --> 02:23:05.680]   What if it had John Legend's voice?
[02:23:05.680 --> 02:23:06.760]   Now, how much would you pay?
[02:23:06.760 --> 02:23:08.000]   I don't-- oh, god.
[02:23:08.000 --> 02:23:09.000]   John Legend.
[02:23:09.000 --> 02:23:10.960]   [LAUGHTER]
[02:23:10.960 --> 02:23:14.080]   We'll get back to our best-of episode in just a little bit.
[02:23:14.080 --> 02:23:16.160]   I'll tell you one of the things the best-ofs for me--
[02:23:16.160 --> 02:23:18.840]   and it's not just been this year for the last 10 years--
[02:23:18.840 --> 02:23:21.160]   last pass, my password manager.
[02:23:21.160 --> 02:23:22.960]   You use LastPass, right?
[02:23:22.960 --> 02:23:27.480]   If you use LastPass, listen to this commercial anyway.
[02:23:27.480 --> 02:23:30.560]   So you could tell your friends why they should get LastPass.
[02:23:30.560 --> 02:23:32.000]   Actually, you might want to tell the boss,
[02:23:32.000 --> 02:23:33.320]   because we use LastPass at work.
[02:23:33.320 --> 02:23:35.640]   Let me tell you.
[02:23:35.640 --> 02:23:37.560]   It's one thing to protect yourself.
[02:23:37.560 --> 02:23:41.000]   But if you're a business, you have to remember,
[02:23:41.000 --> 02:23:42.880]   you can have all the best IT in the world.
[02:23:42.880 --> 02:23:43.600]   And we do here.
[02:23:43.600 --> 02:23:44.360]   Russell's great.
[02:23:44.360 --> 02:23:46.480]   He protects us against all sorts of threats.
[02:23:46.480 --> 02:23:49.320]   But your employees could be the weak link.
[02:23:49.320 --> 02:23:50.800]   They might be using bad passwords.
[02:23:50.800 --> 02:23:53.800]   Remember, you have to give them access to your bank account,
[02:23:53.800 --> 02:23:57.080]   your website, your bookkeeping, your servers,
[02:23:57.080 --> 02:23:59.520]   all the stuff that you use on the internet.
[02:23:59.520 --> 02:24:01.000]   They have access to.
[02:24:01.000 --> 02:24:03.320]   And they may not have the best password hygiene,
[02:24:03.320 --> 02:24:04.400]   if you know what I mean.
[02:24:04.400 --> 02:24:05.400]   Look around your office.
[02:24:05.400 --> 02:24:11.360]   See if there's a Post-it note stuck on a monitor like that
[02:24:11.360 --> 02:24:13.040]   with a corporate password on it.
[02:24:13.040 --> 02:24:14.520]   I bet there is.
[02:24:14.520 --> 02:24:16.160]   This is why you need LastPass.
[02:24:16.160 --> 02:24:18.120]   No more Post-it notes.
[02:24:18.120 --> 02:24:19.080]   Throw them away.
[02:24:19.080 --> 02:24:20.520]   In fact, your employees will love it.
[02:24:20.520 --> 02:24:26.240]   LastPass generates long, strong passwords.
[02:24:26.240 --> 02:24:27.720]   You can't remember.
[02:24:27.720 --> 02:24:29.880]   But you don't have to remember, because LastPass keeps
[02:24:29.880 --> 02:24:31.400]   track of it in the password vault.
[02:24:31.400 --> 02:24:33.440]   LastPass is strong encrypted.
[02:24:33.440 --> 02:24:35.920]   In fact, we ran it through Steve Gibson,
[02:24:35.920 --> 02:24:38.440]   because I wanted to make sure, before I was recommending this
[02:24:38.440 --> 02:24:40.240]   to people, I've been using it forever.
[02:24:40.240 --> 02:24:44.000]   Steve interviewed the guy who wrote it, Joe Seagrest.
[02:24:44.000 --> 02:24:45.000]   He looked at the source code.
[02:24:45.000 --> 02:24:46.560]   He said, yeah, they're doing the right things.
[02:24:46.560 --> 02:24:49.720]   Things like PBDKF so that you can't even
[02:24:49.720 --> 02:24:51.520]   brute force this thing.
[02:24:51.520 --> 02:24:54.040]   And it's only decrypted on device.
[02:24:54.040 --> 02:24:57.120]   Even the guys at LastPass can't get to your password vault.
[02:24:57.120 --> 02:24:59.400]   Only you can get to your password vault.
[02:24:59.400 --> 02:25:00.880]   Of course, they support two-factor.
[02:25:00.880 --> 02:25:02.600]   We require employees to use it.
[02:25:02.600 --> 02:25:04.840]   That's another thing you can get with LastPass Enterprise.
[02:25:04.840 --> 02:25:09.480]   There's over 100 sane, strong, secure policies
[02:25:09.480 --> 02:25:13.080]   you can implement regarding master password length,
[02:25:13.080 --> 02:25:14.600]   two-factor authentication.
[02:25:14.600 --> 02:25:17.080]   You get and have password resets.
[02:25:17.080 --> 02:25:18.760]   You really control the whole thing.
[02:25:18.760 --> 02:25:21.440]   If an employee leaves, access is revoked,
[02:25:21.440 --> 02:25:22.960]   passwords can be chained automatically.
[02:25:22.960 --> 02:25:24.800]   You can even give people passwords without giving them
[02:25:24.800 --> 02:25:26.080]   access to the password.
[02:25:26.080 --> 02:25:28.760]   They can log in to QuickBooks, but they can't actually
[02:25:28.760 --> 02:25:29.600]   see the password.
[02:25:29.600 --> 02:25:32.360]   Things like that make a huge difference.
[02:25:32.360 --> 02:25:34.560]   LastPass-- I trust it so much--
[02:25:34.560 --> 02:25:35.640]   we put everything in there.
[02:25:35.640 --> 02:25:40.000]   Not just passwords, database keys, SSH keys, PGP keys,
[02:25:40.000 --> 02:25:42.800]   software licenses, all our business information.
[02:25:42.800 --> 02:25:45.560]   I even put my passport and driver's license.
[02:25:45.560 --> 02:25:47.160]   Everything's in LastPass.
[02:25:47.160 --> 02:25:49.760]   It's what you need, a secure store,
[02:25:49.760 --> 02:25:52.200]   for the most valuable stuff in your business
[02:25:52.200 --> 02:25:53.400]   or in your home.
[02:25:53.400 --> 02:25:57.600]   LastPass-- they have a great multi-factor authenticator
[02:25:57.600 --> 02:25:58.520]   program.
[02:25:58.520 --> 02:26:01.360]   I love it because instead of making you memorize six digits
[02:26:01.360 --> 02:26:03.160]   and entering them in, it just you press an approve
[02:26:03.160 --> 02:26:05.200]   or deny button on your phone.
[02:26:05.200 --> 02:26:06.360]   It automatically logs you in.
[02:26:06.360 --> 02:26:08.480]   And you can, by the way, as an Enterprise user,
[02:26:08.480 --> 02:26:10.080]   you can require that.
[02:26:10.080 --> 02:26:13.840]   Two-factor is a huge improvement in security in general.
[02:26:13.840 --> 02:26:15.960]   And LastPass makes it easy.
[02:26:15.960 --> 02:26:18.520]   If you're a Microsoft Active Directory company,
[02:26:18.520 --> 02:26:22.680]   you can use your AD credentials for a single sign-on that's
[02:26:22.680 --> 02:26:23.520]   so great.
[02:26:23.520 --> 02:26:24.400]   Here's the deal.
[02:26:24.400 --> 02:26:26.160]   Choose the LastPass that's right for you.
[02:26:26.160 --> 02:26:28.160]   For your business, there's LastPass teams
[02:26:28.160 --> 02:26:30.520]   for businesses of 50 or fewer.
[02:26:30.520 --> 02:26:33.280]   And LastPass Enterprise-- that's what we use--
[02:26:33.280 --> 02:26:35.720]   for individuals, there's LastPass premium.
[02:26:35.720 --> 02:26:38.320]   And actually, at home, we use LastPass families
[02:26:38.320 --> 02:26:41.240]   because that way I could share passwords with my wife.
[02:26:41.240 --> 02:26:44.640]   She's my kind of emergency access contact point.
[02:26:44.640 --> 02:26:46.240]   So if something happens to me, she
[02:26:46.240 --> 02:26:48.960]   can get access to my passwords and settle my affairs, things
[02:26:48.960 --> 02:26:49.280]   like that.
[02:26:49.280 --> 02:26:52.000]   That's actually really important.
[02:26:52.000 --> 02:26:54.120]   I can give you 1,000 reasons to get LastPass.
[02:26:54.120 --> 02:26:55.080]   Just do it.
[02:26:55.080 --> 02:26:57.640]   Go to lastpass.com/twit.
[02:26:57.640 --> 02:27:03.720]   Join the 16 million users and the 43,000 businesses
[02:27:03.720 --> 02:27:06.720]   from leading tech brands like MailChimp to Fortune 500
[02:27:06.720 --> 02:27:09.640]   companies and Twit, who trust LastPass.
[02:27:09.640 --> 02:27:12.000]   It's the number one most preferred password manager.
[02:27:12.000 --> 02:27:16.240]   LastPass.com/twit.
[02:27:16.240 --> 02:27:18.840]   Make security a priority in 2019.
[02:27:18.840 --> 02:27:24.440]   Back to this weekend tech and the roundtable.
[02:27:24.440 --> 02:27:29.560]   A great article, which made me feel a lot better about GDPR.
[02:27:29.560 --> 02:27:31.560]   I have such mixed feelings about GDPR.
[02:27:31.560 --> 02:27:35.400]   On the one hand, this general data protection regulation
[02:27:35.400 --> 02:27:39.000]   that the EU put into law two years ago,
[02:27:39.000 --> 02:27:42.600]   but now is actually going to go into effect on Friday,
[02:27:42.600 --> 02:27:46.120]   does things that everybody would agree are good.
[02:27:46.120 --> 02:27:47.960]   For instance, requires companies that
[02:27:47.960 --> 02:27:52.160]   have a data breach to disclose within 72 hours.
[02:27:52.160 --> 02:27:53.080]   I mean, really fast.
[02:27:53.080 --> 02:27:56.000]   And also, put some real teeth into those laws.
[02:27:56.000 --> 02:27:58.160]   These regulations have existed, but there
[02:27:58.160 --> 02:28:00.680]   was no enforcement in the past.
[02:28:00.680 --> 02:28:01.920]   Now there will be.
[02:28:01.920 --> 02:28:05.520]   In fact, this is a really good example of what happens
[02:28:05.520 --> 02:28:08.720]   if you make a law or you make a regulation.
[02:28:08.720 --> 02:28:12.800]   What was the name of the privacy regulation, Patrick?
[02:28:12.800 --> 02:28:16.400]   It's been in effect for a year or two.
[02:28:16.400 --> 02:28:19.120]   I'm not sure you remember the name of it.
[02:28:19.120 --> 02:28:21.560]   I mean, there are things like the right to be forgotten.
[02:28:21.560 --> 02:28:22.320]   No, not that.
[02:28:22.320 --> 02:28:25.200]   But there's a general privacy regulation
[02:28:25.200 --> 02:28:29.640]   that effectively was GDPR, but had no teeth.
[02:28:29.640 --> 02:28:31.920]   And it's been in place for a while.
[02:28:31.920 --> 02:28:33.440]   Look it up.
[02:28:33.440 --> 02:28:35.640]   In any event, the EU decided, well, we've
[02:28:35.640 --> 02:28:37.760]   got to have some-- if this is going to work,
[02:28:37.760 --> 02:28:40.560]   we've got to have a significant penalty.
[02:28:40.560 --> 02:28:42.440]   And the penalty is fairly significant,
[02:28:42.440 --> 02:28:49.880]   as much as 20 million euros or 4% of your global revenue,
[02:28:49.880 --> 02:28:52.760]   whichever is higher, that's enough even to scare a big company
[02:28:52.760 --> 02:28:53.840]   like Google or Facebook.
[02:28:53.840 --> 02:28:55.320]   Clearly, it's intended for them.
[02:28:55.320 --> 02:29:03.160]   But some things I think I'm in favor of,
[02:29:03.160 --> 02:29:05.360]   then there are other things that a lot of people
[02:29:05.360 --> 02:29:09.840]   are very worried about that compliance will be difficult.
[02:29:09.840 --> 02:29:10.920]   I read a good article.
[02:29:10.920 --> 02:29:14.800]   This is Jacques Mat, I think you pronounce it.
[02:29:14.800 --> 02:29:20.240]   He wrote a piece on his blog called GDPR hysteria, which
[02:29:20.240 --> 02:29:22.200]   calmed me down a lot.
[02:29:22.200 --> 02:29:23.920]   He said, you know--
[02:29:23.920 --> 02:29:25.240]   It's in here, Leo.
[02:29:25.240 --> 02:29:27.440]   It's the European data privacy directive.
[02:29:27.440 --> 02:29:28.520]   That was the previous one.
[02:29:28.520 --> 02:29:31.440]   That's it, which was kind of a first attempt,
[02:29:31.440 --> 02:29:33.320]   but didn't do anything because it had no teeth.
[02:29:33.320 --> 02:29:40.280]   So he's pointing out, look, this isn't going to happen overnight.
[02:29:40.280 --> 02:29:42.240]   You're going to get-- if you're out of compliance,
[02:29:42.240 --> 02:29:44.720]   you're going to get a warning.
[02:29:44.720 --> 02:29:46.280]   That's the maximum fine.
[02:29:46.280 --> 02:29:50.120]   There may not be a fine at all if you fix it.
[02:29:50.120 --> 02:29:53.640]   I am in favor of privacy.
[02:29:53.640 --> 02:29:56.240]   But I think also here in the States, a lot of people say,
[02:29:56.240 --> 02:30:00.440]   how dare the EU tell us how to run our business?
[02:30:00.440 --> 02:30:02.080]   Well, they're not.
[02:30:02.080 --> 02:30:04.720]   They're telling you that that's what you have to do
[02:30:04.720 --> 02:30:06.880]   if you have customers in the EU.
[02:30:06.880 --> 02:30:12.360]   They could perfectly make two different sets of rules and settings,
[02:30:12.360 --> 02:30:15.800]   one for the EU customers and one for the US customers.
[02:30:15.800 --> 02:30:18.840]   Now, of course, it would get interesting reactions
[02:30:18.840 --> 02:30:22.520]   from the US customers and the press, I'm guessing,
[02:30:22.520 --> 02:30:26.040]   because the EU customers would have settings and rights
[02:30:26.040 --> 02:30:28.600]   to delete their data, portability of their data,
[02:30:28.600 --> 02:30:32.160]   downloading what you have used on their network,
[02:30:32.160 --> 02:30:34.840]   and magically, the US customers wouldn't.
[02:30:34.840 --> 02:30:35.520]   So--
[02:30:35.520 --> 02:30:37.800]   Actually, for a while, we thought Facebook might do that.
[02:30:37.800 --> 02:30:40.160]   They might implement it just for EU customers.
[02:30:40.160 --> 02:30:43.840]   They said then later, no, we're going to implement it for everybody.
[02:30:43.840 --> 02:30:44.680]   In fact, it's great.
[02:30:44.680 --> 02:30:46.200]   You can download your Facebook data now.
[02:30:46.200 --> 02:30:48.520]   You can delete your Facebook data now if you want.
[02:30:48.520 --> 02:30:49.160]   That's a good thing.
[02:30:49.160 --> 02:30:50.760]   Yeah.
[02:30:50.760 --> 02:30:53.160]   It's the power of government.
[02:30:53.160 --> 02:30:57.200]   Yes, government is sometimes overbearing and cumbersome,
[02:30:57.200 --> 02:31:00.200]   but sometimes you need it because it's the only entity that
[02:31:00.200 --> 02:31:05.520]   can have some kind of influence on other more very potent
[02:31:05.520 --> 02:31:07.640]   entities, like some huge companies.
[02:31:07.640 --> 02:31:10.240]   And I do think that--
[02:31:10.240 --> 02:31:12.160]   I mean, Europeans have always been more
[02:31:12.160 --> 02:31:14.160]   concerned with all of these issues
[02:31:14.160 --> 02:31:16.800]   than the Americans for a couple of reasons.
[02:31:16.800 --> 02:31:21.320]   Historically, we have more genuine concern about all of this
[02:31:21.320 --> 02:31:22.960]   and the way our data is used.
[02:31:22.960 --> 02:31:26.080]   And of course, those big companies are usually
[02:31:26.080 --> 02:31:27.800]   not in the EU.
[02:31:27.800 --> 02:31:32.160]   So we have an easier time looking at them and saying, oh, these--
[02:31:32.160 --> 02:31:32.520]   Right.
[02:31:32.520 --> 02:31:33.320]   That's one of the--
[02:31:33.320 --> 02:31:35.320]   That's one of the under--
[02:31:35.320 --> 02:31:38.080]   the subtext of all of this is, oh, here comes the EU
[02:31:38.080 --> 02:31:41.400]   after successful American companies again.
[02:31:41.400 --> 02:31:43.080]   Yeah, but no one is saying that what
[02:31:43.080 --> 02:31:47.080]   is being required of those companies is completely unreasonable.
[02:31:47.080 --> 02:31:49.720]   Oh, not at all, especially for the big ones, right?
[02:31:49.720 --> 02:31:53.680]   It's things that really they should have implemented themselves
[02:31:53.680 --> 02:31:56.320]   a long time ago.
[02:31:56.320 --> 02:31:58.040]   I went off on a tiny, tiny rant on Twitter
[02:31:58.040 --> 02:32:02.440]   because after the 25th email I received from one of those
[02:32:02.440 --> 02:32:07.400]   companies saying, that's company X. We value your privacy.
[02:32:07.400 --> 02:32:09.800]   And this is why we're doing this.
[02:32:09.800 --> 02:32:10.800]   Yeah.
[02:32:10.800 --> 02:32:12.840]   If you valued our privacy, you wouldn't
[02:32:12.840 --> 02:32:16.240]   have waited until the law obliged you to do these things,
[02:32:16.240 --> 02:32:18.560]   to put them into place.
[02:32:18.560 --> 02:32:21.640]   I would have preferred that some of them would say, hey,
[02:32:21.640 --> 02:32:26.400]   so the GDPR is coming into law and it requires this and this
[02:32:26.400 --> 02:32:28.880]   or so we're making these modifications.
[02:32:28.880 --> 02:32:34.000]   You didn't have to put that hypocritical PR spin to it
[02:32:34.000 --> 02:32:36.520]   saying, well, of course they did have to do it
[02:32:36.520 --> 02:32:39.480]   because that's how they work, but it's a little bit frustrating.
[02:32:39.480 --> 02:32:41.040]   But it's--
[02:32:41.040 --> 02:32:42.200]   I understand the concerns.
[02:32:42.200 --> 02:32:46.800]   And again, I think it's good to ask and discuss them.
[02:32:46.800 --> 02:32:54.680]   But overall, I have a hard time justifying a full criticism
[02:32:54.680 --> 02:32:59.040]   of this GDPR ensemble of requirements.
[02:32:59.040 --> 02:33:03.760]   It seems like things that should have been put into place
[02:33:03.760 --> 02:33:06.040]   maybe earlier.
[02:33:06.040 --> 02:33:08.160]   And I have to admit that myself, I
[02:33:08.160 --> 02:33:10.840]   might have been a little bit skeptical about all of it
[02:33:10.840 --> 02:33:12.120]   until a couple of years ago.
[02:33:12.120 --> 02:33:15.320]   But I think now it's clear to everyone in Europe
[02:33:15.320 --> 02:33:19.240]   that it's needed and I'm guessing in the US, most people agree.
[02:33:19.240 --> 02:33:20.280]   I agree that it's needed.
[02:33:20.280 --> 02:33:21.920]   And I think that you make--
[02:33:21.920 --> 02:33:23.760]   I basically agree with everything you said.
[02:33:23.760 --> 02:33:26.880]   I do have some questions still about how this is actually
[02:33:26.880 --> 02:33:28.000]   going to be enforced.
[02:33:28.000 --> 02:33:30.040]   I think if you're a larger company, a Facebook or Google,
[02:33:30.040 --> 02:33:35.160]   a Microsoft and Apple, a Samsung, an Amazon,
[02:33:35.160 --> 02:33:37.280]   you obviously are going to be in compliance
[02:33:37.280 --> 02:33:39.000]   because you're going to be watched very carefully.
[02:33:39.000 --> 02:33:41.400]   But if you are a Chinese-based company
[02:33:41.400 --> 02:33:46.240]   or if you are Eastern European-based country who's not
[02:33:46.240 --> 02:33:51.760]   part of the EU, or if you are just smaller and don't care,
[02:33:51.760 --> 02:33:54.080]   I am curious about how these things are actually
[02:33:54.080 --> 02:33:55.320]   going to be enforced.
[02:33:55.320 --> 02:33:58.200]   I think actually that's why Leo mentioned this piece
[02:33:58.200 --> 02:34:00.640]   specifically because I read this earlier in the week also,
[02:34:00.640 --> 02:34:00.920]   Leo.
[02:34:00.920 --> 02:34:03.920]   And the point that he's making is that he's specifically
[02:34:03.920 --> 02:34:06.000]   saying, are you just a person that has a blog?
[02:34:06.000 --> 02:34:08.160]   Are you going to be on the hook for $20 million
[02:34:08.160 --> 02:34:09.360]   or whatever it finds?
[02:34:09.360 --> 02:34:11.040]   And he's saying, no, that's not the way
[02:34:11.040 --> 02:34:12.280]   that the EU works.
[02:34:12.280 --> 02:34:17.040]   Things like the DPD, the European Data Protection Directive,
[02:34:17.040 --> 02:34:20.040]   he said, that's been in effect for two decades.
[02:34:20.040 --> 02:34:21.320]   Two decades?
[02:34:21.320 --> 02:34:23.000]   Have you never even heard of it?
[02:34:23.000 --> 02:34:26.200]   Because the point is not that they wanted to suddenly
[02:34:26.200 --> 02:34:27.760]   find everyone.
[02:34:27.760 --> 02:34:30.840]   They wanted to have the laws in the place.
[02:34:30.840 --> 02:34:31.160]   Right.
[02:34:31.160 --> 02:34:32.400]   And I understand that what I'm saying
[02:34:32.400 --> 02:34:34.560]   is I'm not necessarily talking about your website
[02:34:34.560 --> 02:34:37.040]   or a small company who--
[02:34:37.040 --> 02:34:40.880]   I'm talking about potentially big conglomerates, big industries,
[02:34:40.880 --> 02:34:43.280]   big players who are based in countries that just aren't
[02:34:43.280 --> 02:34:44.080]   going to care.
[02:34:44.080 --> 02:34:45.840]   How are they going to enforce this
[02:34:45.840 --> 02:34:49.720]   against someone like Tencent, for instance, or someone else?
[02:34:49.720 --> 02:34:51.560]   Where someone's really going to come back?
[02:34:51.560 --> 02:34:53.680]   I mean, that's becomes the interesting question.
[02:34:53.680 --> 02:34:57.000]   Are you going to start blocking and accessing their traffic?
[02:34:57.000 --> 02:34:59.400]   What-- how much teeth is this really going to have?
[02:34:59.400 --> 02:35:01.280]   And I guess we will have to see on that
[02:35:01.280 --> 02:35:03.840]   because what we've seen before with Right to be Forgotten
[02:35:03.840 --> 02:35:07.040]   and other sanctions that have gone against Google
[02:35:07.040 --> 02:35:09.480]   and other companies, those companies
[02:35:09.480 --> 02:35:11.200]   tend to fight back even when it is the law.
[02:35:11.200 --> 02:35:13.120]   So I don't know.
[02:35:13.120 --> 02:35:16.880]   Yes, but the Right to be Forgotten has been implemented.
[02:35:16.880 --> 02:35:19.160]   Initially, I thought it was a horrible idea,
[02:35:19.160 --> 02:35:21.480]   but it has been implemented by Google.
[02:35:21.480 --> 02:35:23.840]   And it seems to work relatively well.
[02:35:23.840 --> 02:35:27.720]   Of course, there are always issues in every--
[02:35:27.720 --> 02:35:30.160]   Oh, I still think that one's a horrible law
[02:35:30.160 --> 02:35:35.000]   because instead of saying pull down the original content,
[02:35:35.000 --> 02:35:37.440]   they're putting the burden on the search engine
[02:35:37.440 --> 02:35:39.400]   to pull down the search results.
[02:35:39.400 --> 02:35:41.280]   They're also putting Google into--
[02:35:41.280 --> 02:35:42.720]   in the position of judge and jury,
[02:35:42.720 --> 02:35:44.840]   Google has to go through each of these requests
[02:35:44.840 --> 02:35:47.040]   and judge whether it has merit or not.
[02:35:47.040 --> 02:35:48.760]   That's not something Google should be doing.
[02:35:48.760 --> 02:35:54.360]   Yes, but Leo, this is the practical approach to problems.
[02:35:54.360 --> 02:35:56.520]   Sometimes the solution is not perfect,
[02:35:56.520 --> 02:35:57.960]   but it's better than doing nothing.
[02:35:57.960 --> 02:36:03.000]   This is what we're asking of YouTube, of Facebook, of Twitter.
[02:36:03.000 --> 02:36:06.280]   We're telling them, take away the objectionable content
[02:36:06.280 --> 02:36:07.360]   within 24 hours.
[02:36:07.360 --> 02:36:08.120]   This is horrible.
[02:36:08.120 --> 02:36:09.440]   It shouldn't be on the internet.
[02:36:09.440 --> 02:36:13.760]   And my initial concern was we're asking private enterprises
[02:36:13.760 --> 02:36:17.760]   to be the judge of what is acceptable or not acceptable.
[02:36:17.760 --> 02:36:18.360]   I agree.
[02:36:18.360 --> 02:36:19.720]   This is a valid concern.
[02:36:19.720 --> 02:36:23.200]   However, on the other side of it--
[02:36:23.200 --> 02:36:26.640]   and I'm sure some people would have more to say about it
[02:36:26.640 --> 02:36:30.800]   than me, but you see some horrible things on Twitter.
[02:36:30.800 --> 02:36:33.440]   And sometimes you are very frustrated
[02:36:33.440 --> 02:36:36.320]   that it's still there and that the accounts aren't blocked.
[02:36:36.320 --> 02:36:38.920]   So is the right thing to do to go to Google and say,
[02:36:38.920 --> 02:36:42.240]   hi, that Twitter search result or to go to Twitter
[02:36:42.240 --> 02:36:43.960]   and say, pull the tweet down?
[02:36:43.960 --> 02:36:44.760]   And that's the problem.
[02:36:44.760 --> 02:36:45.960]   I have the right to be forgotten.
[02:36:45.960 --> 02:36:49.040]   It makes Google responsible for this, not the site that
[02:36:49.040 --> 02:36:50.960]   hosts the bad content.
[02:36:50.960 --> 02:36:53.800]   I think in the case of Twitter, obviously, you
[02:36:53.800 --> 02:36:54.560]   would go to Twitter.
[02:36:54.560 --> 02:36:54.920]   Yes.
[02:36:54.920 --> 02:36:56.440]   But there are many other--
[02:36:56.440 --> 02:36:59.520]   The right to be forgotten is about Google.
[02:36:59.520 --> 02:37:00.840]   Yes, absolutely.
[02:37:00.840 --> 02:37:04.840]   And it's not about the link to Twitter.
[02:37:04.840 --> 02:37:07.040]   If it's a link to Twitter, you're going to go see Twitter.
[02:37:07.040 --> 02:37:10.160]   It's a general--
[02:37:10.160 --> 02:37:14.080]   because in practice, Google is our doorway to the internet,
[02:37:14.080 --> 02:37:15.040]   especially in Europe.
[02:37:15.040 --> 02:37:16.480]   Remember, they have 80%--
[02:37:16.480 --> 02:37:18.120]   Yes, dominant.
[02:37:18.120 --> 02:37:19.400]   Yeah, market share in Europe.
[02:37:19.400 --> 02:37:20.960]   And in Germany's in the chat room,
[02:37:20.960 --> 02:37:23.040]   and he says, you still have to delete the content
[02:37:23.040 --> 02:37:24.560]   from the original sites by yourself.
[02:37:24.560 --> 02:37:25.960]   You go to those sites.
[02:37:25.960 --> 02:37:28.400]   But the reason Google is in the equation
[02:37:28.400 --> 02:37:29.880]   is because they cache all this stuff,
[02:37:29.880 --> 02:37:32.840]   so you have to get them to kill the cache as well, I guess.
[02:37:32.840 --> 02:37:33.360]   I don't know.
[02:37:33.360 --> 02:37:35.360]   Well, no, no, no, not just the cache.
[02:37:35.360 --> 02:37:36.560]   They also have to kill the link.
[02:37:36.560 --> 02:37:37.800]   They have to hide the link.
[02:37:37.800 --> 02:37:38.280]   Right.
[02:37:38.280 --> 02:37:41.840]   Yeah, but if the content's gone, what does it link matter?
[02:37:41.840 --> 02:37:42.560]   Of course, yes.
[02:37:42.560 --> 02:37:44.760]   But usually the issue is that the content
[02:37:44.760 --> 02:37:49.520]   is more difficult to get to the internet archive, which I think
[02:37:49.520 --> 02:37:52.520]   is a really valuable, important thing, the way back
[02:37:52.520 --> 02:37:55.800]   machine, is saving the internet.
[02:37:55.800 --> 02:37:56.880]   Nobody's saving the internet.
[02:37:56.880 --> 02:37:58.600]   It's disappearing.
[02:37:58.600 --> 02:37:59.360]   Absolutely.
[02:37:59.360 --> 02:38:00.440]   Leo, I completely agree.
[02:38:00.440 --> 02:38:03.480]   But I will answer to you what I basically
[02:38:03.480 --> 02:38:05.920]   would like to formulate to Christina about the concerns
[02:38:05.920 --> 02:38:11.000]   that he was voicing, which is you're always
[02:38:11.000 --> 02:38:15.800]   going to have issues in any system, in any rule, in any law.
[02:38:15.800 --> 02:38:20.000]   You can point out instances where it doesn't work
[02:38:20.000 --> 02:38:21.880]   or where it breaks something.
[02:38:21.880 --> 02:38:27.560]   But having a few exceptions, where it doesn't work out well,
[02:38:27.560 --> 02:38:30.080]   doesn't mean that you shouldn't do something about the problem.
[02:38:30.080 --> 02:38:31.120]   I agree.
[02:38:31.120 --> 02:38:32.200]   Without a doubt, I agree.
[02:38:32.200 --> 02:38:33.680]   And to be clear, although I actually
[02:38:33.680 --> 02:38:35.720]   have a lot of issues, especially from a first
[02:38:35.720 --> 02:38:37.640]   and then a perspective with the right to be forgotten,
[02:38:37.640 --> 02:38:40.280]   I have serious, serious issues about that.
[02:38:40.280 --> 02:38:41.800]   I don't have a problem with GDPR.
[02:38:41.800 --> 02:38:44.440]   And I think that these are things that I'm with.
[02:38:44.440 --> 02:38:45.600]   They're very different.
[02:38:45.600 --> 02:38:46.120]   Yeah.
[02:38:46.120 --> 02:38:46.640]   Absolutely.
[02:38:46.640 --> 02:38:48.000]   And I think these are things that, frankly,
[02:38:48.000 --> 02:38:50.520]   most of these big companies should have been doing already.
[02:38:50.520 --> 02:38:53.480]   And should have that act that they were actively avoiding.
[02:38:53.480 --> 02:38:55.440]   And I think that the fact that we've had two years
[02:38:55.440 --> 02:38:57.600]   and there was all this pushback is proof.
[02:38:57.600 --> 02:39:02.200]   Facebook being publicly forced through the terrible press
[02:39:02.200 --> 02:39:04.800]   of it is getting to not have two option screens.
[02:39:04.800 --> 02:39:07.320]   But one is kind of proof of that.
[02:39:07.320 --> 02:39:08.280]   I agree with you.
[02:39:08.280 --> 02:39:10.680]   I'm just saying not to say not to do it.
[02:39:10.680 --> 02:39:12.040]   Obviously, you still do it.
[02:39:12.040 --> 02:39:16.720]   But I do have questions that, other than some of the biggest
[02:39:16.720 --> 02:39:19.240]   American run conglomerates, that you might still
[02:39:19.240 --> 02:39:22.520]   have some very big services that will just not respond to this.
[02:39:22.520 --> 02:39:24.400]   And I think that that's just something that people should
[02:39:24.400 --> 02:39:25.760]   reconcile.
[02:39:25.760 --> 02:39:26.440]   Sure.
[02:39:26.440 --> 02:39:28.880]   So the thing is, if one of those big companies--
[02:39:28.880 --> 02:39:31.720]   let's say a Chinese company for the sake of argument--
[02:39:31.720 --> 02:39:34.840]   if they do business in Europe, if they are big enough
[02:39:34.840 --> 02:39:37.000]   or serious enough, at this point, they still
[02:39:37.000 --> 02:39:40.600]   do have to have some kind of presence in the country.
[02:39:40.600 --> 02:39:42.640]   Usually, you need to have a company that
[02:39:42.640 --> 02:39:46.840]   is created in Europe to do business with European customers.
[02:39:46.840 --> 02:39:50.200]   You might have employees.
[02:39:50.200 --> 02:39:51.800]   You have someone who's responsible.
[02:39:51.800 --> 02:39:54.720]   So if they're big enough, the likeliness
[02:39:54.720 --> 02:39:57.080]   is that if there's a big problem, there
[02:39:57.080 --> 02:40:00.360]   is someone you can point to go after a company you can find
[02:40:00.360 --> 02:40:01.120]   stuff like that.
[02:40:01.120 --> 02:40:02.680]   Yes, there might be a few instances
[02:40:02.680 --> 02:40:04.120]   where this doesn't happen.
[02:40:04.120 --> 02:40:05.480]   But it's--
[02:40:05.480 --> 02:40:07.080]   Actually, that's an interesting question.
[02:40:07.080 --> 02:40:09.040]   Christian raises an interesting question, though.
[02:40:09.040 --> 02:40:12.560]   What if you say, no, I'm not going to pay the fine?
[02:40:12.560 --> 02:40:13.640]   What do they do?
[02:40:13.640 --> 02:40:16.160]   Are they going to block you on the internet?
[02:40:16.160 --> 02:40:18.680]   No, if you have a French--
[02:40:18.680 --> 02:40:20.000]   this is the reason why--
[02:40:20.000 --> 02:40:20.680]   What are they going to do though?
[02:40:20.680 --> 02:40:21.600]   Close your office?
[02:40:21.600 --> 02:40:22.880]   What are they going to do?
[02:40:22.880 --> 02:40:23.380]   Yes.
[02:40:23.380 --> 02:40:24.200]   Yeah, absolutely.
[02:40:24.200 --> 02:40:25.400]   If you don't pay the fine, then you
[02:40:25.400 --> 02:40:27.200]   can-- you're liable to criminal--
[02:40:27.200 --> 02:40:27.920]   I'm guessing.
[02:40:27.920 --> 02:40:29.120]   I'm not a lawyer.
[02:40:29.120 --> 02:40:31.120]   So let's say it's not going to happen.
[02:40:31.120 --> 02:40:34.320]   But let's say they said went after me and said, you're not--
[02:40:34.320 --> 02:40:37.200]   Leo, you're not erasing IP addresses of people
[02:40:37.200 --> 02:40:38.760]   who download your content.
[02:40:38.760 --> 02:40:40.280]   Well, again, then I said--
[02:40:40.280 --> 02:40:41.480]   Well, screw you.
[02:40:41.480 --> 02:40:43.280]   What are they going to do?
[02:40:43.280 --> 02:40:44.720]   You don't have a company in France.
[02:40:44.720 --> 02:40:45.360]   Google does.
[02:40:45.360 --> 02:40:46.040]   Facebook does.
[02:40:46.040 --> 02:40:49.520]   But everybody who has viewers or listeners or readers
[02:40:49.520 --> 02:40:54.920]   or customers in France are still liable to the EU
[02:40:54.920 --> 02:40:56.840]   for those customers, right?
[02:40:56.840 --> 02:40:58.560]   Yes.
[02:40:58.560 --> 02:40:59.040]   Yes.
[02:40:59.040 --> 02:41:01.000]   And in theory, they could go after you.
[02:41:01.000 --> 02:41:02.360]   In theory, yes, he'll.
[02:41:02.360 --> 02:41:02.720]   Yeah, I know.
[02:41:02.720 --> 02:41:03.240]   I'm too small.
[02:41:03.240 --> 02:41:04.800]   They're not going to, obviously.
[02:41:04.800 --> 02:41:05.480]   Yeah, exactly.
[02:41:05.480 --> 02:41:07.040]   But yeah, I mean, I guess--
[02:41:07.040 --> 02:41:09.840]   I mean, Google's going to pay or at least they've appealed it.
[02:41:09.840 --> 02:41:13.400]   But Google's being fined what it was at $1.7 billion or something.
[02:41:13.400 --> 02:41:15.760]   They'll eventually pay that.
[02:41:15.760 --> 02:41:17.520]   I mean, or they will get a knock down.
[02:41:17.520 --> 02:41:18.020]   Right.
[02:41:18.020 --> 02:41:19.020]   Because this is my question.
[02:41:19.020 --> 02:41:19.640]   This is what I'm saying.
[02:41:19.640 --> 02:41:21.020]   Speaking of Google specifically, they've
[02:41:21.020 --> 02:41:22.660]   been fined multiple times by the EU.
[02:41:22.660 --> 02:41:26.660]   To my knowledge, they paid much smaller fines than what's
[02:41:26.660 --> 02:41:28.260]   been issued to the new--
[02:41:28.260 --> 02:41:30.680]   $2.7 billion is the current one.
[02:41:30.680 --> 02:41:31.580]   Yeah.
[02:41:31.580 --> 02:41:33.460]   But there was a Google shopping thing
[02:41:33.460 --> 02:41:34.000]   that I think this--
[02:41:34.000 --> 02:41:35.160]   That's the shopping one.
[02:41:35.160 --> 02:41:36.020]   That's the shopping one.
[02:41:36.020 --> 02:41:38.140]   OK, there was another one too that they put off.
[02:41:38.140 --> 02:41:39.940]   I mean, they've been putting this off for years.
[02:41:39.940 --> 02:41:43.140]   So part of me does kind of wonder, when are they actually
[02:41:43.140 --> 02:41:45.300]   going-- I mean, I'm not opposed to this law at all.
[02:41:45.300 --> 02:41:47.100]   As I said, I agree with it.
[02:41:47.100 --> 02:41:49.780]   Part of me, though, does wonder how much of this is bluster
[02:41:49.780 --> 02:41:50.980]   and how much of this is just going
[02:41:50.980 --> 02:41:53.020]   to carry on in the courts ad nauseam.
[02:41:53.020 --> 02:41:57.100]   Listen, how many emails have you gotten of the company
[02:41:57.100 --> 02:41:58.860]   that you're taking it seriously?
[02:41:58.860 --> 02:42:01.780]   As they should, because it's the right thing to do.
[02:42:01.780 --> 02:42:02.860]   Well, I agree with--
[02:42:02.860 --> 02:42:06.020]   When I first joined Microsoft, I joined Microsoft a year ago,
[02:42:06.020 --> 02:42:09.860]   they were already deep in the GDPR rollout plan.
[02:42:09.860 --> 02:42:11.540]   It was already deeply underway.
[02:42:11.540 --> 02:42:12.900]   And I'm sure that that's the same way
[02:42:12.900 --> 02:42:14.580]   that it's been at every other major company.
[02:42:14.580 --> 02:42:16.780]   This is not something that two months ago people were like,
[02:42:16.780 --> 02:42:17.780]   oh, we've got to get on this.
[02:42:17.780 --> 02:42:19.260]   This has been probably in the works
[02:42:19.260 --> 02:42:21.140]   for two years at most of these places.
[02:42:21.140 --> 02:42:21.780]   I agree.
[02:42:21.780 --> 02:42:24.620]   I'm just saying, if people find that there
[02:42:24.620 --> 02:42:29.220]   isn't enough being done or Google were to do something
[02:42:29.220 --> 02:42:31.620]   that the people claimed violated this,
[02:42:31.620 --> 02:42:34.620]   I have no doubt in my mind that there would be an appeal
[02:42:34.620 --> 02:42:36.540]   for burning us sort of fine.
[02:42:36.540 --> 02:42:39.660]   And actually, I mean, I think this is a good thing.
[02:42:39.660 --> 02:42:42.700]   I'm just pointing out, I think that sometimes we say,
[02:42:42.700 --> 02:42:43.980]   oh, it's fixed now.
[02:42:43.980 --> 02:42:46.780]   And that's not necessarily going to be the case,
[02:42:46.780 --> 02:42:51.300]   because companies will look for ways to still capture
[02:42:51.300 --> 02:42:52.100]   what they can.
[02:42:52.100 --> 02:42:54.540]   I do have a question, because I'm not clear on this.
[02:42:54.540 --> 02:43:00.620]   If you ask users to opt into your agreement that says,
[02:43:00.620 --> 02:43:03.060]   I will willingly give you this information,
[02:43:03.060 --> 02:43:05.620]   can you bypass some of those restrictions?
[02:43:05.620 --> 02:43:08.740]   If you are able to get them to agree to a different user
[02:43:08.740 --> 02:43:10.500]   agreement that says, it's fine, you
[02:43:10.500 --> 02:43:13.060]   can capture my information.
[02:43:13.060 --> 02:43:14.260]   Well, I think that's the point.
[02:43:14.260 --> 02:43:17.180]   I think you have to explicitly have them agree
[02:43:17.180 --> 02:43:20.940]   to giving you this information instead of implicitly doing that.
[02:43:20.940 --> 02:43:23.580]   And if they give the right to delete it,
[02:43:23.580 --> 02:43:26.660]   they can come to you and say, I'll delete my information.
[02:43:26.660 --> 02:43:28.740]   And that is the other point I wanted to make.
[02:43:28.740 --> 02:43:31.340]   We can argue about whether or not they would comply or not.
[02:43:31.340 --> 02:43:32.780]   The fact is they are complying.
[02:43:32.780 --> 02:43:35.500]   You now have the ability to delete all of your information
[02:43:35.500 --> 02:43:36.820]   from all of these networks.
[02:43:36.820 --> 02:43:39.260]   Of course, most people are choosing not to do that
[02:43:39.260 --> 02:43:41.620]   and are probably going to keep choosing not to do that.
[02:43:41.620 --> 02:43:45.780]   But if you want to talk about how much teeth that law has
[02:43:45.780 --> 02:43:50.100]   and how much weight, you know, ultimately it's all about money.
[02:43:50.100 --> 02:43:53.660]   All these companies are making a lot of money in EU, in the EU,
[02:43:53.660 --> 02:43:57.020]   and they're willing to invest a little bit of money
[02:43:57.020 --> 02:43:59.340]   in modifying their systems, first of all,
[02:43:59.340 --> 02:44:01.660]   because it's ultimately the right thing to do.
[02:44:01.660 --> 02:44:03.740]   I think a lot of them are saying now, well, maybe we
[02:44:03.740 --> 02:44:06.900]   should be a little bit regulated if it's the right way.
[02:44:06.900 --> 02:44:09.140]   And second of all, it's a small investment
[02:44:09.140 --> 02:44:12.260]   compared to the money they're making in the territory.
[02:44:12.260 --> 02:44:16.180]   And yes, they could close down their offices in EU
[02:44:16.180 --> 02:44:19.980]   and have everyone go through amazon.com or whatever,
[02:44:19.980 --> 02:44:21.260]   you know, US side it is.
[02:44:21.260 --> 02:44:24.300]   But in practice, that's just not the way it works.
[02:44:24.300 --> 02:44:27.220]   If you want to do business, even in the age of the internet,
[02:44:27.220 --> 02:44:29.060]   you need to have a presence in the country,
[02:44:29.060 --> 02:44:31.420]   or at least in the EU.
[02:44:31.420 --> 02:44:34.060]   So it's currently working.
[02:44:34.060 --> 02:44:37.860]   Now, of course, if they end up having a fine of 20 million
[02:44:37.860 --> 02:44:43.060]   euros or however much it might be 4%, they will fight it.
[02:44:43.060 --> 02:44:44.220]   It will take years.
[02:44:44.220 --> 02:44:45.660]   But that's not even the point.
[02:44:45.660 --> 02:44:50.100]   The point is the deterring effect is working
[02:44:50.100 --> 02:44:51.820]   because they're already doing it.
[02:44:51.820 --> 02:44:52.820]   Send me a note this week.
[02:44:52.820 --> 02:44:54.180]   I think we all read it.
[02:44:54.180 --> 02:44:59.900]   A couple of posts on Medium about detoxing.
[02:44:59.900 --> 02:45:03.220]   Yes, from data, from our data apocalypse
[02:45:03.220 --> 02:45:05.460]   that always seems just upon us.
[02:45:05.460 --> 02:45:09.220]   But I did a personal process that I went through courtesy
[02:45:09.220 --> 02:45:11.300]   of these artists and activists out of Germany
[02:45:11.300 --> 02:45:14.220]   called Tactical Tech that lets you kind of review
[02:45:14.220 --> 02:45:15.940]   your own exposure to data.
[02:45:15.940 --> 02:45:17.980]   And so I did a personal journey and took notes
[02:45:17.980 --> 02:45:19.180]   and chronicled it.
[02:45:19.180 --> 02:45:22.220]   And out of that experience, which I've been processing
[02:45:22.220 --> 02:45:26.020]   for over a year, honestly, I wrote a humbly titled
[02:45:26.020 --> 02:45:30.460]   New Tech Manifesto on ways I think that we could collectively
[02:45:30.460 --> 02:45:32.780]   not just be afraid of what the present is,
[02:45:32.780 --> 02:45:34.140]   but like write a better future.
[02:45:34.140 --> 02:45:38.620]   My hope is that we don't feel disarmed by all of this
[02:45:38.620 --> 02:45:42.020]   or too depressed by it, but that we actually feel inspired.
[02:45:42.020 --> 02:45:43.540]   We're still early days of the internet
[02:45:43.540 --> 02:45:46.860]   in this whole network world, and it doesn't have to be
[02:45:46.860 --> 02:45:50.580]   an advertising hellscape where we are all in someone's dossier
[02:45:50.580 --> 02:45:51.300]   of us.
[02:45:51.300 --> 02:45:53.940]   So encourage folks to check that out at veritudate.com.
[02:45:53.940 --> 02:45:54.900]   I think the timing is good.
[02:45:54.900 --> 02:45:56.740]   I feel like we're making progress in that regard.
[02:45:56.740 --> 02:45:57.140]   Aren't we?
[02:45:57.140 --> 02:46:00.580]   Thanks to the PPR and the Facebook debacle.
[02:46:00.580 --> 02:46:01.900]   We were watching the Facebook.
[02:46:01.900 --> 02:46:05.300]   So on the NBA playoffs, there were three, not one, not two,
[02:46:05.300 --> 02:46:07.980]   but three apology commercials.
[02:46:07.980 --> 02:46:08.460]   Yes.
[02:46:08.460 --> 02:46:11.780]   Facebook, which has been running for several months.
[02:46:11.780 --> 02:46:13.900]   And I loved Facebook's because it said,
[02:46:13.900 --> 02:46:16.300]   you know, all the things you love about Facebook,
[02:46:16.300 --> 02:46:20.220]   and then something happened as if they had nothing to do with it.
[02:46:20.220 --> 02:46:22.180]   Yeah, it was like passive event that occurred.
[02:46:22.180 --> 02:46:23.260]   They were the victims here.
[02:46:23.260 --> 02:46:24.260]   Then something happened.
[02:46:24.260 --> 02:46:25.780]   Fake news happened.
[02:46:25.780 --> 02:46:26.860]   Poor Zuckerberg, really.
[02:46:26.860 --> 02:46:27.820]   We should all just privacy--
[02:46:27.820 --> 02:46:29.060]   Privacy, sympathy for him.
[02:46:29.060 --> 02:46:30.900]   People lost their data.
[02:46:30.900 --> 02:46:32.300]   It just happened.
[02:46:32.300 --> 02:46:33.580]   It was pretty funny.
[02:46:33.580 --> 02:46:34.540]   And then there's Wells Fargo.
[02:46:34.540 --> 02:46:36.100]   They took a little more, I think a little more.
[02:46:36.100 --> 02:46:38.780]   The Wells Fargo was the most annoying ad to listen to.
[02:46:38.780 --> 02:46:40.100]   But it was the most honest, I think,
[02:46:40.100 --> 02:46:41.500]   in admitting their own flaws.
[02:46:41.500 --> 02:46:43.700]   They said, we lost your trust, accurate.
[02:46:43.700 --> 02:46:47.460]   I wish Facebook had said we lost it.
[02:46:47.460 --> 02:46:49.860]   That's the only kind of apology that works.
[02:46:49.860 --> 02:46:51.620]   One of the things-- one of my favorite--
[02:46:51.620 --> 02:46:54.420]   honestly, my favorite line from my little short series
[02:46:54.420 --> 02:46:57.420]   was Facebook has been not just on an apology tour,
[02:46:57.420 --> 02:47:00.260]   but a corrective blogging exercise,
[02:47:00.260 --> 02:47:03.780]   where if you go to newsroom.fb.com,
[02:47:03.780 --> 02:47:06.300]   which I encourage everyone to do,
[02:47:06.300 --> 02:47:07.980]   because we probably have it.
[02:47:07.980 --> 02:47:09.860]   They are launching all these new tools.
[02:47:09.860 --> 02:47:12.580]   And they're saying, we're going to audit the app developers.
[02:47:12.580 --> 02:47:14.020]   And we're going to get them.
[02:47:14.020 --> 02:47:16.460]   We're going to get them for abusing your data.
[02:47:16.460 --> 02:47:19.900]   And it's like, dude, it was like you opened the door.
[02:47:19.900 --> 02:47:22.340]   You left the keys in the lock.
[02:47:22.340 --> 02:47:26.100]   You had signs up that said, grab all this data while you can.
[02:47:26.100 --> 02:47:27.420]   Everything must go.
[02:47:27.420 --> 02:47:31.660]   So Facebook auditing and investigating app developers
[02:47:31.660 --> 02:47:35.020]   is like Walter White investigating Jesse,
[02:47:35.020 --> 02:47:37.780]   all that meth Jesse made in Walter's lab
[02:47:37.780 --> 02:47:39.580]   using his scientific knowledge.
[02:47:39.580 --> 02:47:42.460]   And until they not only apologize,
[02:47:42.460 --> 02:47:44.940]   but really come clean and acknowledge
[02:47:44.940 --> 02:47:48.340]   that it was part of their corporate growth strategy
[02:47:48.340 --> 02:47:51.420]   to let all these folks in and sell us all out,
[02:47:51.420 --> 02:47:53.340]   it's going to be very hard to take them seriously
[02:47:53.340 --> 02:47:54.460]   when they ask for our trust back.
[02:47:54.460 --> 02:47:55.860]   I can't even trust you to be honest
[02:47:55.860 --> 02:47:58.340]   about the problem you helped create.
[02:47:58.340 --> 02:48:00.020]   But that would require a little bit of humility
[02:48:00.020 --> 02:48:01.020]   on the side of Facebook.
[02:48:01.020 --> 02:48:03.020]   And I don't think that corporation has humility
[02:48:03.020 --> 02:48:04.220]   built into its DNA.
[02:48:04.220 --> 02:48:07.420]   I think it goes right back to the business model.
[02:48:07.420 --> 02:48:11.220]   I mean, Zuckerberg has been playing fast and loose
[02:48:11.220 --> 02:48:14.980]   with his users data since he was in short pants in Harvard.
[02:48:14.980 --> 02:48:16.980]   And he keeps apologizing.
[02:48:16.980 --> 02:48:18.220]   And it's always the same thing.
[02:48:18.220 --> 02:48:22.220]   It's just he's dependent on his business
[02:48:22.220 --> 02:48:26.780]   is dependent on the users sticking around watching ads.
[02:48:26.780 --> 02:48:32.780]   And if you're going to prioritize user engagement,
[02:48:32.780 --> 02:48:37.940]   then you're going to have things like the 2016 election.
[02:48:37.940 --> 02:48:42.940]   Did you see the thing that they did on the Daily Show,
[02:48:42.940 --> 02:48:46.980]   the Facebook skit?
[02:48:46.980 --> 02:48:47.980]   No.
[02:48:47.980 --> 02:48:48.980]   What a Facebook bar.
[02:48:48.980 --> 02:48:51.660]   You have that video?
[02:48:51.660 --> 02:48:52.380]   I could play it.
[02:48:52.380 --> 02:48:54.380]   What if Facebook?
[02:48:54.380 --> 02:48:56.020]   This is recently or?
[02:48:56.020 --> 02:48:57.020]   Yeah.
[02:48:57.020 --> 02:48:58.020]   Very good.
[02:48:58.020 --> 02:48:59.780]   I used to work for the Daily Show, right?
[02:48:59.780 --> 02:49:00.300]   I did.
[02:49:00.300 --> 02:49:00.820]   Yeah.
[02:49:00.820 --> 02:49:04.180]   Was that during the John Stewart era or the Trevor Noah era?
[02:49:04.180 --> 02:49:06.700]   It was the first of the Trevor Noah era.
[02:49:06.700 --> 02:49:08.180]   Oh, you did a weirded job, man.
[02:49:08.180 --> 02:49:09.980]   Here's Trevor and...
[02:49:09.980 --> 02:49:11.620]   Go to T20 in the...
[02:49:11.620 --> 02:49:12.620]   Yeah, we'll go.
[02:49:12.620 --> 02:49:14.540]   This is a longer piece about Facebook.
[02:49:14.540 --> 02:49:16.260]   We'll go into the video.
[02:49:16.260 --> 02:49:18.780]   If Facebook was a real physical place.
[02:49:18.780 --> 02:49:29.540]   Hey, want to see you pick up my kid?
[02:49:29.540 --> 02:49:30.540]   Nope.
[02:49:30.540 --> 02:49:31.540]   I'm heading out.
[02:49:31.540 --> 02:49:33.540]   You want to stay for another drink?
[02:49:33.540 --> 02:49:35.540]   Sorry, but this place kind of blows.
[02:49:35.540 --> 02:49:37.540]   I procrastinated long enough.
[02:49:37.540 --> 02:49:38.540]   Okay.
[02:49:38.540 --> 02:49:41.940]   Just get a Hillary Clinton button on our purse.
[02:49:41.940 --> 02:49:44.300]   Hey, check it.
[02:49:44.300 --> 02:49:48.740]   Before you go, this guy says Bernie would have won.
[02:49:48.740 --> 02:49:49.740]   Oh, boy.
[02:49:49.740 --> 02:49:50.740]   Seriously?
[02:49:50.740 --> 02:49:52.740]   It's fight words.
[02:49:52.740 --> 02:49:53.740]   Right.
[02:49:53.740 --> 02:49:55.740]   Here's what I hate about Bernie Brooks.
[02:49:55.740 --> 02:49:56.740]   You want another drink?
[02:49:56.740 --> 02:49:57.740]   Yeah.
[02:49:57.740 --> 02:49:58.740]   Bernie's a socialist.
[02:49:58.740 --> 02:50:01.740]   The idea that a Rust Belt voter would ever...
[02:50:01.740 --> 02:50:04.740]   What do you know about Rust Belt voters?
[02:50:04.740 --> 02:50:06.740]   We're the real American.
[02:50:06.740 --> 02:50:09.740]   Bartender, I'm not Patrick, please.
[02:50:09.740 --> 02:50:11.740]   We're gonna be here tomorrow.
[02:50:11.740 --> 02:50:12.740]   Oh.
[02:50:12.740 --> 02:50:14.740]   Bartender has a little face.
[02:50:14.740 --> 02:50:16.580]   The climate change is real.
[02:50:16.580 --> 02:50:19.580]   Did you even see Leonardo DiCaprio's documentary series?
[02:50:19.580 --> 02:50:21.580]   Want to see you pick up my kid?
[02:50:21.580 --> 02:50:23.580]   I'm tired of how it is.
[02:50:23.580 --> 02:50:24.580]   Nope.
[02:50:24.580 --> 02:50:25.580]   They should stick to acting.
[02:50:25.580 --> 02:50:28.580]   They should stick to not sexually harassing.
[02:50:28.580 --> 02:50:33.580]   #MeToo, #TimesUp, #BoycottHollywood.
[02:50:33.580 --> 02:50:35.580]   Hey, take a joke, snowflake, LOL.
[02:50:35.580 --> 02:50:40.580]   Hey, I know it's problematic for me to say this as a straight white cis male, but check
[02:50:40.580 --> 02:50:44.580]   your privilege and show this beautiful goddess some respect.
[02:50:44.580 --> 02:50:45.580]   Oh, boy.
[02:50:45.580 --> 02:50:50.580]   Bartender, you can do something about the misogyny in here.
[02:50:50.580 --> 02:50:51.580]   Absolutely.
[02:50:51.580 --> 02:50:55.580]   We're doing everything we can to block people like that from coming in here.
[02:50:55.580 --> 02:50:59.580]   And if we can't do that, we don't deserve to serve you.
[02:50:59.580 --> 02:51:03.580]   Speaking of, we deserve you.
[02:51:03.580 --> 02:51:05.580]   Hell yeah, you can.
[02:51:05.580 --> 02:51:06.580]   I have another drink.
[02:51:06.580 --> 02:51:08.580]   They don't even have a word for poverty.
[02:51:08.580 --> 02:51:11.580]   F2Pock, I like La Yatti.
[02:51:11.580 --> 02:51:13.580]   You seem so trustworthy.
[02:51:13.580 --> 02:51:15.580]   This is fairly accurate.
[02:51:15.580 --> 02:51:18.580]   Why would this not happen in real life?
[02:51:18.580 --> 02:51:19.580]   I love that accent.
[02:51:19.580 --> 02:51:20.580]   Because they wouldn't talk to you.
[02:51:20.580 --> 02:51:21.580]   You can't pretend like that.
[02:51:21.580 --> 02:51:22.580]   Yeah, yeah, yeah, yeah.
[02:51:22.580 --> 02:51:23.580]   No.
[02:51:23.580 --> 02:51:25.580]   Blue lives matter.
[02:51:25.580 --> 02:51:28.580]   If all lives matter, you both wrong.
[02:51:28.580 --> 02:51:31.580]   I think you mean you are both wrong.
[02:51:31.580 --> 02:51:33.580]   Hey, how's your thing?
[02:51:33.580 --> 02:51:36.580]   Like, hey, awww!
[02:51:36.580 --> 02:51:37.580]   Awwww.
[02:51:37.580 --> 02:51:40.580]   That's what you call calls up and slugs them.
[02:51:40.580 --> 02:51:43.580]   And he's just collecting money, more beer, more drinks.
[02:51:43.580 --> 02:51:46.580]   The far is jammed.
[02:51:46.580 --> 02:51:47.580]   You didn't worse.
[02:51:47.580 --> 02:51:49.580]   And everybody's fighting.
[02:51:49.580 --> 02:51:51.580]   Guys with Russian hats.
[02:51:51.580 --> 02:51:52.580]   I can't wait.
[02:51:52.580 --> 02:51:55.580]   You said I'd like to pick up my kit, but I thought them now.
[02:51:55.580 --> 02:51:56.580]   Stop it!
[02:51:56.580 --> 02:51:58.580]   Look at what this place is doing to you.
[02:51:58.580 --> 02:52:00.580]   It's toxic.
[02:52:00.580 --> 02:52:01.580]   Yeah, you're right.
[02:52:01.580 --> 02:52:03.580]   Should we leave?
[02:52:08.580 --> 02:52:14.580]   So I apologize for people listening, but I think you can imagine what the video is.
[02:52:14.580 --> 02:52:16.580]   They're pretty accurate audio-efficient.
[02:52:16.580 --> 02:52:17.580]   Yeah.
[02:52:17.580 --> 02:52:18.580]   Yeah.
[02:52:18.580 --> 02:52:24.580]   So the thing about real life is there's a natural damper on that kind of stuff, right?
[02:52:24.580 --> 02:52:26.580]   You don't want to confront somebody directly.
[02:52:26.580 --> 02:52:31.580]   So there are people occasionally you'll be in a bar that you'll meet somebody like that,
[02:52:31.580 --> 02:52:33.580]   but they're shut down fairly quickly, right?
[02:52:33.580 --> 02:52:34.580]   Right.
[02:52:34.580 --> 02:52:36.580]   And the bartender tries to keep things calm.
[02:52:36.580 --> 02:52:38.580]   Instead of lighting a fire.
[02:52:38.580 --> 02:52:39.580]   Right.
[02:52:39.580 --> 02:52:42.580]   I feel like Twitter's even worse.
[02:52:42.580 --> 02:52:47.580]   I just want to say, Facebook's getting a lot of heat, but really, if you want to go into
[02:52:47.580 --> 02:52:51.580]   a bar fight, just say anything on Twitter.
[02:52:51.580 --> 02:52:55.580]   Yeah, but the argument about Facebook's data usage is pretty separate from the toxicity
[02:52:55.580 --> 02:52:56.580]   of the platform itself.
[02:52:56.580 --> 02:52:59.580]   They're similar points, but they are distinct, I think.
[02:52:59.580 --> 02:53:04.580]   And the idea that Facebook knows all these things about us and has been making money the
[02:53:04.580 --> 02:53:06.580]   time off of that deep knowledge.
[02:53:06.580 --> 02:53:13.220]   That is, to Alex, is more specific to Facebook than Twitter, which is a uncontrolled space,
[02:53:13.220 --> 02:53:17.220]   but not as deeply ingrained in our psyche as Facebook.
[02:53:17.220 --> 02:53:18.220]   Yeah.
[02:53:18.220 --> 02:53:21.580]   Brianna, you probably didn't have the luxury to go to E3 this year, so you followed it
[02:53:21.580 --> 02:53:22.580]   in the moment.
[02:53:22.580 --> 02:53:23.580]   I sure wish.
[02:53:23.580 --> 02:53:24.580]   Wow.
[02:53:24.580 --> 02:53:25.580]   Yeah.
[02:53:25.580 --> 02:53:26.580]   That would have been great.
[02:53:26.580 --> 02:53:27.580]   Yeah.
[02:53:27.580 --> 02:53:28.580]   But Jeff, you were there.
[02:53:28.580 --> 02:53:29.580]   I was.
[02:53:29.580 --> 02:53:30.580]   Yes, indeed.
[02:53:30.580 --> 02:53:31.580]   Great fun.
[02:53:31.580 --> 02:53:32.580]   Wait, wait, wait.
[02:53:32.580 --> 02:53:33.580]   You agree with Brianna?
[02:53:33.580 --> 02:53:34.580]   I would say best yet.
[02:53:34.580 --> 02:53:37.340]   I think it was a solid year.
[02:53:37.340 --> 02:53:42.740]   It's an interesting, we're in a sort of weird transition state for E3 because many of the
[02:53:42.740 --> 02:53:48.780]   big publishers are pulling out or having satellite shows that are concurrent with E3,
[02:53:48.780 --> 02:53:50.860]   but not actually part of E3.
[02:53:50.860 --> 02:54:01.180]   And so the big, crazy floor chaos that used to be what defined E3 has subsided a bit, and
[02:54:01.180 --> 02:54:06.340]   the floor itself is still cacophonous and intense and has a lot of companies, but fewer
[02:54:06.340 --> 02:54:07.340]   than it used to.
[02:54:07.340 --> 02:54:12.380]   I mean, EA does its own event now in Hollywood instead of downtown Los Angeles.
[02:54:12.380 --> 02:54:16.700]   Microsoft this year pulled out and had an event that was adjacent at their-
[02:54:16.700 --> 02:54:17.700]   Isn't that funny?
[02:54:17.700 --> 02:54:21.740]   I thought they were at E3 because they had the press conference just like always.
[02:54:21.740 --> 02:54:22.740]   Yeah.
[02:54:22.740 --> 02:54:26.780]   Well, they, you know, the Microsoft sponsors the theater at LA Live now.
[02:54:26.780 --> 02:54:30.980]   So it is adjacent to the convention center, but it wasn't actually part of E3.
[02:54:30.980 --> 02:54:35.900]   And their floor presence on the show floor of E3 was nonexistent because they have their
[02:54:35.900 --> 02:54:37.860]   own location now.
[02:54:37.860 --> 02:54:38.860]   Yeah.
[02:54:38.860 --> 02:54:39.860]   So it's interesting.
[02:54:39.860 --> 02:54:41.300]   It's in a really interesting state.
[02:54:41.300 --> 02:54:47.980]   And of course, Nintendo for years has done what they call Nintendo direct, which is not
[02:54:47.980 --> 02:54:49.340]   an actual press conference.
[02:54:49.340 --> 02:54:53.260]   There are no human beings in an auditorium listening to someone on the stage.
[02:54:53.260 --> 02:54:59.700]   They have a prerecorded, edited, composed presentation that they just livestream whenever
[02:54:59.700 --> 02:55:00.700]   they feel like it.
[02:55:00.700 --> 02:55:04.540]   And they do that at E3, but they do it multiple times during the year.
[02:55:04.540 --> 02:55:10.980]   And I think you're going to see many companies moving away from E3 and moving to that model
[02:55:10.980 --> 02:55:14.060]   because they have control over the message.
[02:55:14.060 --> 02:55:17.060]   They don't have to deal with pesky human beings.
[02:55:17.060 --> 02:55:20.340]   And they're able to cut out the middlemen.
[02:55:20.340 --> 02:55:24.780]   And why, as you keep saying on this show many times, I've heard disintermediate, that's
[02:55:24.780 --> 02:55:25.780]   the-
[02:55:25.780 --> 02:55:29.540]   There's no reason to have press between you and your message anymore.
[02:55:29.540 --> 02:55:35.540]   That's what's killed or not killed, but certainly caused problems for trade shows in general.
[02:55:35.540 --> 02:55:38.020]   I mean, Microsoft pulled out at CES.
[02:55:38.020 --> 02:55:39.740]   Comdex has disappeared.
[02:55:39.740 --> 02:55:42.620]   Well, yeah, I mean, Apple showed the way.
[02:55:42.620 --> 02:55:44.020]   Apple doesn't go to these events.
[02:55:44.020 --> 02:55:46.500]   You just have your own events and you get more attention.
[02:55:46.500 --> 02:55:48.420]   You're not competing with anybody else.
[02:55:48.420 --> 02:55:51.180]   Somehow though, it seems like Microsoft's Sony Nintendo still feel like they have to
[02:55:51.180 --> 02:55:53.700]   be somewhere around E3.
[02:55:53.700 --> 02:55:55.500]   That's still the week, right?
[02:55:55.500 --> 02:55:56.500]   It's still the week.
[02:55:56.500 --> 02:55:58.900]   It's the week that all the news is coming out, right?
[02:55:58.900 --> 02:56:05.580]   And for a gamers perspective, you don't really care if Nintendo is announcing Smash as part
[02:56:05.580 --> 02:56:08.380]   of E3 or with the Nintendo Direct.
[02:56:08.380 --> 02:56:10.620]   I just care that a new Smash is coming out.
[02:56:10.620 --> 02:56:15.940]   So I really hear what you're saying, Jeff, but they've struggled with this for years.
[02:56:15.940 --> 02:56:20.100]   Leo, you were saying this was the first E3 that was open to the press.
[02:56:20.100 --> 02:56:25.020]   If I'm remembering this correctly, they tried that a few years ago.
[02:56:25.020 --> 02:56:29.580]   Like letting people in, then they shut it off to journalists again and let people back
[02:56:29.580 --> 02:56:32.740]   in if I'm remembering that correctly.
[02:56:32.740 --> 02:56:37.300]   I just think ultimately it matters like what games were announced and what games are coming
[02:56:37.300 --> 02:56:40.700]   out because they're all kind of competing for that mind share.
[02:56:40.700 --> 02:56:46.300]   Like is Fallout 77 going to be the big game that people care about is Anthem?
[02:56:46.300 --> 02:56:50.660]   And I think what we saw in this year were some fresh new IPs really getting a lot of
[02:56:50.660 --> 02:56:51.740]   attention from people.
[02:56:51.740 --> 02:56:52.740]   Yeah.
[02:56:52.740 --> 02:56:57.900]   I think according to a game spot, E3 opens the public for the first time ever.
[02:56:57.900 --> 02:56:59.460]   15,000 tickets.
[02:56:59.460 --> 02:57:01.300]   Oh, wait, wait, that was last year.
[02:57:01.300 --> 02:57:02.420]   OK, there you go.
[02:57:02.420 --> 02:57:03.980]   So it is the second year in a row.
[02:57:03.980 --> 02:57:04.980]   OK.
[02:57:04.980 --> 02:57:05.980]   Second year.
[02:57:05.980 --> 02:57:09.220]   And this year, the first time they had different hours for the public and the press, which
[02:57:09.220 --> 02:57:10.220]   was kind of nice.
[02:57:10.220 --> 02:57:14.340]   We got a couple of hours at the beginning to go in and just have press only.
[02:57:14.340 --> 02:57:15.780]   But I agree with you, Brianna.
[02:57:15.780 --> 02:57:16.780]   I think you're right.
[02:57:16.780 --> 02:57:21.500]   I think it ultimately comes down to what games were shown, what things were talked about
[02:57:21.500 --> 02:57:24.780]   because it is the biggest week in the gaming calendar.
[02:57:24.780 --> 02:57:26.900]   And that's really the most important thing.
[02:57:26.900 --> 02:57:29.580]   It is interesting how the show is changing.
[02:57:29.580 --> 02:57:33.020]   And I think kind of receding in a certain sense.
[02:57:33.020 --> 02:57:39.100]   But from a gaming standpoint, from just what was announced, it really was a strong year
[02:57:39.100 --> 02:57:40.300]   for some exciting stuff.
[02:57:40.300 --> 02:57:44.100]   We did get a lot of sequels, as we always do.
[02:57:44.100 --> 02:57:48.940]   But some exciting new stuff, some interesting trends.
[02:57:48.940 --> 02:57:50.900]   Go ahead.
[02:57:50.900 --> 02:57:57.020]   And just the I2 I'm a gamer, although I suck at games, it's a huge part.
[02:57:57.020 --> 02:57:58.900]   It is explicitly my hobby.
[02:57:58.900 --> 02:58:01.420]   Like I'm not working if I'm playing games.
[02:58:01.420 --> 02:58:06.140]   And it's been fascinating to watch games both pioneer some of the spaces that we've
[02:58:06.140 --> 02:58:08.540]   talked about, specifically social hooks.
[02:58:08.540 --> 02:58:12.780]   I mean, in social mechanics, they figured out long before social media, cyber security
[02:58:12.780 --> 02:58:17.220]   and cyber defense game, the game industry had to pioneer these things.
[02:58:17.220 --> 02:58:20.580]   And they're now kind of following the SAS model.
[02:58:20.580 --> 02:58:25.220]   And it's interesting to see the games industry take a page out of the services book and really
[02:58:25.220 --> 02:58:31.820]   transition almost every major publisher has transitioned from releasing standalone products
[02:58:31.820 --> 02:58:38.820]   and single player products over the last decade to these always on service models.
[02:58:38.820 --> 02:58:44.020]   And whether you, gamers, I know are very vocal with their opinions, especially on Reddit.
[02:58:44.020 --> 02:58:49.060]   But whether you love this or hate this, it really has represented this kind of massive
[02:58:49.060 --> 02:58:53.700]   sea change that's happened in a multi-billion dollar industry that the mainstream media
[02:58:53.700 --> 02:58:57.860]   has kind of turned a blind eye to.
[02:58:57.860 --> 02:59:02.060]   One of the things you'll see from time to time on Twitter is you'll see a little Reddit
[02:59:02.060 --> 02:59:03.340]   emerge into Twitter.
[02:59:03.340 --> 02:59:07.780]   It turns out Carson Bondi, our producer and I are both massive Reddit fans.
[02:59:07.780 --> 02:59:10.420]   I think Carson found this one.
[02:59:10.420 --> 02:59:12.220]   Now, is this a spoiler?
[02:59:12.220 --> 02:59:13.900]   Should we do a spoiler alert?
[02:59:13.900 --> 02:59:18.900]   If you've still not seen, I can't believe anybody's not seen.
[02:59:18.900 --> 02:59:19.900]   What is it called?
[02:59:19.900 --> 02:59:20.900]   Infinity War?
[02:59:20.900 --> 02:59:21.900]   Infinity War?
[02:59:21.900 --> 02:59:26.180]   If you've not seen Infinity War, shame on you.
[02:59:26.180 --> 02:59:31.620]   Just buy it for Christmas, watch it, then watch this great take on Infinity War from
[02:59:31.620 --> 02:59:32.620]   Reddit.
[02:59:32.620 --> 02:59:38.100]   If you were a fan of Infinity War, you might be in the subreddit.
[02:59:38.100 --> 02:59:41.060]   The Reddit dot com.
[02:59:41.060 --> 02:59:48.060]   Google signed up for it.
[02:59:48.060 --> 02:59:48.060]   Flash R slash Thanos did nothing wrong.
[02:59:48.060 --> 02:59:53.860]   Josh Brolin, aka Thanos, has a message for you.
[02:59:53.860 --> 02:59:59.060]   Here we go, Reddit users.
[02:59:59.060 --> 03:00:00.700]   Tomorrow.
[03:00:00.700 --> 03:00:05.260]   By the way, notice Josh Brolin, half of his clothes went away when he snapped his fingers.
[03:00:05.260 --> 03:00:08.580]   Let's face it, if you've got a chest like that, I'd be walking around so close to the
[03:00:08.580 --> 03:00:10.580]   side as well.
[03:00:10.580 --> 03:00:16.100]   Tomorrow, July 9th, at 9, actually, we don't know what time.
[03:00:16.100 --> 03:00:23.660]   Sometime during the day, exactly one half of all of the subscribers to the Thanos has
[03:00:23.660 --> 03:00:25.460]   a message for you.
[03:00:25.460 --> 03:00:29.300]   I'm sorry, to the Thanos did nothing wrong.
[03:00:29.300 --> 03:00:31.100]   Subreddit will be banned.
[03:00:31.100 --> 03:00:32.420]   So everybody needs to sign up.
[03:00:32.420 --> 03:00:38.420]   I noticed that the number has gone up a lot since this article was it seen at?
[03:00:38.420 --> 03:00:40.860]   Yeah, 557,000 subscribers now.
[03:00:40.860 --> 03:00:41.860]   It's more than doubled.
[03:00:41.860 --> 03:00:45.900]   And we're going to do more to get that increased.
[03:00:45.900 --> 03:00:46.900]   It's going up fast.
[03:00:46.900 --> 03:00:48.900]   It's probably doubled again since it was on the show.
[03:00:48.900 --> 03:00:52.780]   Yeah, because everybody wants 48 hours to be banned by Thanos.
[03:00:52.780 --> 03:00:57.900]   So we'll get back to you next time and tell you how that went.
[03:00:57.900 --> 03:00:59.180]   I joined immediately.
[03:00:59.180 --> 03:01:02.940]   You have to not only subscribe, Karsten, who somehow knows all this, says you also have
[03:01:02.940 --> 03:01:04.780]   to say something.
[03:01:04.780 --> 03:01:09.980]   And Karsten told me that what I should say is I don't feel so good, right?
[03:01:09.980 --> 03:01:11.660]   Is that the canonical Thanos?
[03:01:11.660 --> 03:01:15.900]   That is a good thing to say in any situation.
[03:01:15.900 --> 03:01:18.060]   You guys don't watch Sportball.
[03:01:18.060 --> 03:01:20.420]   I don't get the comic book movies.
[03:01:20.420 --> 03:01:25.420]   No, I mean, the whole Marvel things were sort of, I mean, okay, they were fun for a
[03:01:25.420 --> 03:01:27.220]   while and occasionally they're quite good.
[03:01:27.220 --> 03:01:29.340]   But mainly it's kind of brain candy.
[03:01:29.340 --> 03:01:30.340]   You go in.
[03:01:30.340 --> 03:01:31.340]   I have had...
[03:01:31.340 --> 03:01:32.340]   Ant-Man was really good.
[03:01:32.340 --> 03:01:33.340]   Yeah, I hear that's funny.
[03:01:33.340 --> 03:01:38.940]   I did feel a very weird pathology during the big battle scenes.
[03:01:38.940 --> 03:01:39.940]   Now they're so overwhelming.
[03:01:39.940 --> 03:01:44.340]   It started with Black Panther, it happened in Infinity War, during those massive chaotic
[03:01:44.340 --> 03:01:45.340]   battle scenes.
[03:01:45.340 --> 03:01:46.340]   I fell asleep.
[03:01:46.340 --> 03:01:47.340]   Yeah.
[03:01:47.340 --> 03:01:48.340]   I don't love that stuff either.
[03:01:48.340 --> 03:01:50.620]   I'm just not like, oh, I'm bored.
[03:01:50.620 --> 03:01:54.380]   I literally just fall asleep and then wake up when they're over.
[03:01:54.380 --> 03:01:55.700]   And I said, who won?
[03:01:55.700 --> 03:02:00.060]   I podcast on The Uncomparable with Jason Snell and Matt Crew.
[03:02:00.060 --> 03:02:03.820]   And we're doing a Marvel summer where we re-watch and analyze movies.
[03:02:03.820 --> 03:02:09.260]   And I'm on next week for Avengers One, Avengers Two and Captain America Civil War.
[03:02:09.260 --> 03:02:10.260]   And it's been really interesting.
[03:02:10.260 --> 03:02:12.300]   Can we do a DC summer next year?
[03:02:12.300 --> 03:02:15.100]   If they make decent movies, sure.
[03:02:15.100 --> 03:02:16.100]   And then...
[03:02:16.100 --> 03:02:17.100]   That won't be gross.
[03:02:17.100 --> 03:02:18.100]   But no.
[03:02:18.100 --> 03:02:19.100]   It's awesome.
[03:02:19.100 --> 03:02:20.100]   All right, that's it.
[03:02:20.100 --> 03:02:25.900]   Did you see the Twitter thread sort of deconstructing the votes website?
[03:02:25.900 --> 03:02:27.900]   They're the blockchain voting system?
[03:02:27.900 --> 03:02:28.900]   No.
[03:02:28.900 --> 03:02:30.900]   Let me see if I can find it.
[03:02:30.900 --> 03:02:32.660]   Who's Kevin Beaumont?
[03:02:32.660 --> 03:02:33.660]   Okay.
[03:02:33.660 --> 03:02:36.500]   So, and he looks at the website.
[03:02:36.500 --> 03:02:42.420]   He's like out of date, SSH, Apache, PHP, pop three, plus.
[03:02:42.420 --> 03:02:44.420]   And then he just keeps going through.
[03:02:44.420 --> 03:02:46.020]   Please, really, plus.
[03:02:46.020 --> 03:02:47.020]   Yeah.
[03:02:47.020 --> 03:02:50.420]   At some point, votes comes in and is like, no, you don't understand.
[03:02:50.420 --> 03:02:54.700]   And then he's like, oh, don't understand where you left your source code on GitHub with hard-coded
[03:02:54.700 --> 03:02:59.580]   usernames and passwords.
[03:02:59.580 --> 03:03:01.940]   And it's a pretty bad cell phone.
[03:03:01.940 --> 03:03:03.620]   Lots of good, actually, information.
[03:03:03.620 --> 03:03:10.540]   Here's Rachel Tobak at the Defcon Hacking Conference learning how to gain physical,
[03:03:10.540 --> 03:03:17.500]   admin access to a voting machine requires no tools, takes under two minutes.
[03:03:17.500 --> 03:03:18.500]   What?
[03:03:18.500 --> 03:03:20.540]   It's used in 18 states this system.
[03:03:20.540 --> 03:03:21.540]   What?
[03:03:21.540 --> 03:03:22.540]   You just press a button and it open.
[03:03:22.540 --> 03:03:23.540]   Yeah, wait a minute.
[03:03:23.540 --> 03:03:24.540]   I'm just going to show you this.
[03:03:24.540 --> 03:03:25.540]   You can turn on my audio.
[03:03:25.540 --> 03:03:26.540]   Yeah.
[03:03:26.540 --> 03:03:27.540]   This is Rachel.
[03:03:27.540 --> 03:03:31.140]   The voting machine is used in 18 different states and it's extremely easy to get admin
[03:03:31.140 --> 03:03:32.140]   access on this machine.
[03:03:32.140 --> 03:03:34.140]   So let me show you how quick it is.
[03:03:34.140 --> 03:03:35.940]   It's about a little under two minutes.
[03:03:35.940 --> 03:03:38.420]   So, by the way, this is the voting machine in the books.
[03:03:38.420 --> 03:03:41.740]   All of the questions that actor would be to open up this machine by pressing this button
[03:03:41.740 --> 03:03:42.740]   right here.
[03:03:42.740 --> 03:03:50.980]   When it's off, removing the card reader, removing this, you don't need any tools to do this.
[03:03:50.980 --> 03:03:51.980]   Unplugging this.
[03:03:51.980 --> 03:03:53.500]   Again, you don't need any tools to do this.
[03:03:53.500 --> 03:03:54.500]   Oh, wow.
[03:03:54.500 --> 03:03:56.460]   It goes the hard guy.
[03:03:56.460 --> 03:04:00.980]   Turning it on, all you have to do is pick this lock here with a ballpoint pen.
[03:04:00.980 --> 03:04:01.980]   What?
[03:04:01.980 --> 03:04:04.620]   Press the red button.
[03:04:04.620 --> 03:04:09.900]   And we're going to let it boot up here.
[03:04:09.900 --> 03:04:13.380]   And then I'll show you the admin access.
[03:04:13.380 --> 03:04:15.020]   This is a premier voting system.
[03:04:15.020 --> 03:04:16.860]   It's used in 18 states, kids.
[03:04:16.860 --> 03:04:17.860]   18 states.
[03:04:17.860 --> 03:04:19.420]   This is not okay.
[03:04:19.420 --> 03:04:21.020]   Yeah, Rachel is great.
[03:04:21.020 --> 03:04:24.260]   She actually interviewed her about social engineering.
[03:04:24.260 --> 03:04:26.820]   She's a very, very talented actor.
[03:04:26.820 --> 03:04:30.220]   So it's loading with what they call a six-year-old lead.
[03:04:30.220 --> 03:04:31.220]   It looks like Windows C.
[03:04:31.220 --> 03:04:32.220]   Do you know right now?
[03:04:32.220 --> 03:04:33.220]   You see that?
[03:04:33.220 --> 03:04:37.380]   The problem is a lot of this stuff came up following the 2000 election.
[03:04:37.380 --> 03:04:40.580]   This is copyright 2003 to 2008.
[03:04:40.580 --> 03:04:41.580]   Yeah.
[03:04:41.580 --> 03:04:42.740]   This software running up.
[03:04:42.740 --> 03:04:47.820]   There was a big push to update voting systems to get rid of hanging chads and all that.
[03:04:47.820 --> 03:04:50.820]   And we ended up with this press.
[03:04:50.820 --> 03:04:55.820]   This error messages is just like cancel and okay.
[03:04:55.820 --> 03:05:01.820]   And now I have full-time assets under two minutes.
[03:05:01.820 --> 03:05:04.820]   All she had to do is really pull the hard drive and boot up to the front.
[03:05:04.820 --> 03:05:07.660]   Yeah, that's what she did.
[03:05:07.660 --> 03:05:09.260]   Does this make any...
[03:05:09.260 --> 03:05:15.500]   Does anybody want to now start a voting booth company that would like have cameras in it?
[03:05:15.500 --> 03:05:17.980]   No code that's open source.
[03:05:17.980 --> 03:05:21.020]   Why should voting be proprietary?
[03:05:21.020 --> 03:05:22.620]   Voting systems.
[03:05:22.620 --> 03:05:24.740]   Why should it be proprietary?
[03:05:24.740 --> 03:05:25.740]   Why is it not open source?
[03:05:25.740 --> 03:05:30.420]   By the way, one of the few strengths we have in all this is that every county uses a different
[03:05:30.420 --> 03:05:31.420]   system.
[03:05:31.420 --> 03:05:32.940]   So it's a little harder to hack it.
[03:05:32.940 --> 03:05:34.380]   At least you don't have a monoculture.
[03:05:34.380 --> 03:05:35.780]   Yeah, there's not a monoculture.
[03:05:35.780 --> 03:05:36.780]   But you're right.
[03:05:36.780 --> 03:05:39.020]   This should be an open source for one thing.
[03:05:39.020 --> 03:05:46.540]   We've got to look at companies like DeBold that don't really seem to care.
[03:05:46.540 --> 03:05:47.540]   It's depressing.
[03:05:47.540 --> 03:05:49.340]   Anyway, it's going on right now at Def Con.
[03:05:49.340 --> 03:05:52.060]   I'm sure there'll be more like that to Rachel too.
[03:05:52.060 --> 03:05:54.980]   Look, what else can be hacked?
[03:05:54.980 --> 03:05:55.980]   Everything can be hacked.
[03:05:55.980 --> 03:05:56.980]   I mean, it's true.
[03:05:56.980 --> 03:05:57.980]   And everything.
[03:05:57.980 --> 03:05:59.940]   We'll be back with our best oven just a second.
[03:05:59.940 --> 03:06:02.740]   The stock market's crazy, isn't it?
[03:06:02.740 --> 03:06:04.300]   It's going up and down.
[03:06:04.300 --> 03:06:07.620]   Betterment is the smart way to manage your money.
[03:06:07.620 --> 03:06:11.260]   It's the largest independent online financial advisor.
[03:06:11.260 --> 03:06:16.780]   They're designed to help you build wealth, but also plan for retirement, save for college,
[03:06:16.780 --> 03:06:18.340]   achieve all your financial goals.
[03:06:18.340 --> 03:06:23.300]   Look, you don't want to put it in a savings account that's earning less than the inflation
[03:06:23.300 --> 03:06:24.300]   rate.
[03:06:24.300 --> 03:06:25.300]   You need to invest.
[03:06:25.300 --> 03:06:26.300]   Betterments the better way.
[03:06:26.300 --> 03:06:30.420]   Their technology is designed to help you make more from your investments with tax efficient
[03:06:30.420 --> 03:06:31.420]   investing strategies.
[03:06:31.420 --> 03:06:33.620]   It'll give you an edge.
[03:06:33.620 --> 03:06:34.620]   And betterment.
[03:06:34.620 --> 03:06:36.780]   By the way, this is a good time to be in betterment.
[03:06:36.780 --> 03:06:42.020]   When the market goes down, betterment will help you save more of your money by taking
[03:06:42.020 --> 03:06:45.060]   advantage of things like tax loss harvesting.
[03:06:45.060 --> 03:06:48.740]   Betterment provides unlimited expert advice designed to help you make smart financial
[03:06:48.740 --> 03:06:49.740]   decisions.
[03:06:49.740 --> 03:06:53.260]   You're still in control, but betterment can help you.
[03:06:53.260 --> 03:06:55.460]   And this, I can't underscore this enough.
[03:06:55.460 --> 03:06:57.540]   They're what they call fiduciary.
[03:06:57.540 --> 03:07:01.940]   That means everything they recommend is based on your best interest.
[03:07:01.940 --> 03:07:04.260]   They are non-incentivized to recommend certain funds.
[03:07:04.260 --> 03:07:06.620]   They don't have their own investment products to sell.
[03:07:06.620 --> 03:07:08.060]   They are working for you.
[03:07:08.060 --> 03:07:10.580]   And you can't say that about most financial advisors.
[03:07:10.580 --> 03:07:11.900]   This is what you want.
[03:07:11.900 --> 03:07:17.500]   Somebody who's going to give you real, good, clear advice that's for your benefit.
[03:07:17.500 --> 03:07:20.820]   Betterment gives you constant access to information and tools that let you track progress towards
[03:07:20.820 --> 03:07:25.180]   your goals so you can always feel like a smart, savvy investor.
[03:07:25.180 --> 03:07:28.980]   Now, look, nobody's going to say investment doesn't involve risk.
[03:07:28.980 --> 03:07:30.380]   But do it right.
[03:07:30.380 --> 03:07:31.460]   Do it the right way.
[03:07:31.460 --> 03:07:33.260]   Do it the better way with betterment.
[03:07:33.260 --> 03:07:35.420]   You can get up to one year managed free.
[03:07:35.420 --> 03:07:39.020]   By the way, I should emphasize this.
[03:07:39.020 --> 03:07:41.300]   Betterment's fee structure is flat fee.
[03:07:41.300 --> 03:07:44.700]   So no matter how many trades, they don't charge you for trade.
[03:07:44.700 --> 03:07:50.620]   It's a very small, very affordable flat fee based on the amount of money you have invested.
[03:07:50.620 --> 03:07:53.980]   And you can look it up on the site because it varies depending on what you're doing.
[03:07:53.980 --> 03:07:58.020]   But how about getting no fees for one year, managed free?
[03:07:58.020 --> 03:08:00.940]   For more information, go to betterment.com/twit.
[03:08:00.940 --> 03:08:03.100]   Betterment.com/twit.
[03:08:03.100 --> 03:08:04.100]   Betterment.
[03:08:04.100 --> 03:08:05.860]   Outsmart average.
[03:08:05.860 --> 03:08:09.300]   And now back to the show.
[03:08:09.300 --> 03:08:14.100]   In the sport of being healthy and in the sport of Elon Musk's latest views on how to
[03:08:14.100 --> 03:08:15.580]   do these sort of things.
[03:08:15.580 --> 03:08:17.860]   Oh, who's up for a split?
[03:08:17.860 --> 03:08:18.860]   Right now.
[03:08:18.860 --> 03:08:20.980]   No, no, no, I'm kidding.
[03:08:20.980 --> 03:08:22.500]   This is a family show.
[03:08:22.500 --> 03:08:26.620]   There is nothing in there other than pure legal tobacco, which will only kill you if
[03:08:26.620 --> 03:08:29.300]   you use it as per the manufacturers instructions.
[03:08:29.300 --> 03:08:31.380]   But what was Elon thinking?
[03:08:31.380 --> 03:08:33.540]   I mean, seriously.
[03:08:33.540 --> 03:08:36.860]   So are we familiar with the Joe Rogan show as a group in general?
[03:08:36.860 --> 03:08:37.860]   Yes.
[03:08:37.860 --> 03:08:38.860]   We've seen this before.
[03:08:38.860 --> 03:08:39.860]   Yes.
[03:08:39.860 --> 03:08:40.860]   Okay.
[03:08:40.860 --> 03:08:44.060]   So for people who don't know the Joe Rogan show, it's one dude who is in the MMA scene, Joe
[03:08:44.060 --> 03:08:46.420]   Rogan, who's also a quasi-comedian, who doesn't know.
[03:08:46.420 --> 03:08:48.220]   MMA is mixed martial arts.
[03:08:48.220 --> 03:08:52.780]   I knew him from News Radio back in the 1990s when he was basically just the beau honk that
[03:08:52.780 --> 03:08:55.220]   was brought on to be the muscular engineer type.
[03:08:55.220 --> 03:08:58.740]   And it sort of amazes me that he swerved into this niche.
[03:08:58.740 --> 03:08:59.740]   Well, it's-
[03:08:59.740 --> 03:09:01.140]   I mean, he's mastered it.
[03:09:01.140 --> 03:09:02.140]   So kudos to him.
[03:09:02.140 --> 03:09:03.940]   Yeah, but his show has great reach.
[03:09:03.940 --> 03:09:06.940]   Like James Headflip from Metallica went on after the latest album.
[03:09:06.940 --> 03:09:10.060]   Every celebrity of knowns dropped by and it's a long form interview.
[03:09:10.060 --> 03:09:15.260]   Now what would happen if you took that format and put a kind of really under stress tech
[03:09:15.260 --> 03:09:18.740]   executive with a history for drug use and crazy sayings?
[03:09:18.740 --> 03:09:21.060]   Well, what you can imagine happened.
[03:09:21.060 --> 03:09:22.060]   So-
[03:09:22.060 --> 03:09:27.100]   Yeah, but I mean, 9% off the share price of the company initially it bounced back slightly.
[03:09:27.100 --> 03:09:31.140]   The US military is now looking into this because as a federal contractor, he can't actually,
[03:09:31.140 --> 03:09:34.260]   he's not supposed to have anything to do with drugs.
[03:09:34.260 --> 03:09:37.220]   I mean Dwight, at the end of the day, was this mental breakdown?
[03:09:37.220 --> 03:09:39.380]   Was this just publicity seeking?
[03:09:39.380 --> 03:09:44.340]   Well, if you watch the whole video, you know, I don't think Elon necessarily was expecting
[03:09:44.340 --> 03:09:45.340]   it.
[03:09:45.340 --> 03:09:54.300]   But haven't there been tweets between his girlfriend and another musician about him getting stoned
[03:09:54.300 --> 03:09:55.300]   all the time?
[03:09:55.300 --> 03:09:57.940]   I seem to remember a story about that a couple of weeks.
[03:09:57.940 --> 03:10:00.980]   He was joking about taking you off any of the property of private.
[03:10:00.980 --> 03:10:02.700]   Yeah, that's right, exactly.
[03:10:02.700 --> 03:10:08.620]   And so I think he's got, I think he's not, I think he's 420 friendly anyway.
[03:10:08.620 --> 03:10:15.460]   And he's not the guy, you know, actually working, you know, turning wrenches on the rocket.
[03:10:15.460 --> 03:10:18.420]   So I'm not sure exactly how that extends up.
[03:10:18.420 --> 03:10:24.580]   One of the things that's really fascinated me about the legalization of pot around the
[03:10:24.580 --> 03:10:29.980]   country is you have places where it's now legal, but you have businesses who will fire
[03:10:29.980 --> 03:10:31.740]   you if you do it.
[03:10:31.740 --> 03:10:37.580]   And this kind of dovetails into that a little bit because, you know, what are Teslas and
[03:10:37.580 --> 03:10:41.100]   SpaceX's rules about doing marijuana?
[03:10:41.100 --> 03:10:42.980]   Are you even on your own time?
[03:10:42.980 --> 03:10:43.980]   It's certainly in public.
[03:10:43.980 --> 03:10:47.540]   Yeah, there was a report of a woman who was fired from Tesla because she tested positive
[03:10:47.540 --> 03:10:49.820]   for marijuana cannabis.
[03:10:49.820 --> 03:10:53.140]   And she was mad because Elon was out there smoking weed.
[03:10:53.140 --> 03:10:56.460]   Now, I don't have a problem with what you do in your own time for sure.
[03:10:56.460 --> 03:11:01.620]   But I do think that it was a failure of crisis communications to in the middle of the struggles
[03:11:01.620 --> 03:11:05.660]   that Tesla has had in the last 12, 18, 24 months, do go through this level of distraction
[03:11:05.660 --> 03:11:07.020]   and stupidity into the mix.
[03:11:07.020 --> 03:11:08.020]   Yeah.
[03:11:08.020 --> 03:11:14.500]   Well, I guess the question is the question I have is how does Musk understand his relationship
[03:11:14.500 --> 03:11:18.060]   with his shareholders and his board of directors?
[03:11:18.060 --> 03:11:22.140]   Because it's one thing when it's your company and there are not people who have put their
[03:11:22.140 --> 03:11:25.780]   money into it and are relying on you to make the kind of decisions that will increase market
[03:11:25.780 --> 03:11:27.060]   valuation.
[03:11:27.060 --> 03:11:30.700]   But when you become a public company, you are essentially making a contract with shareholders
[03:11:30.700 --> 03:11:35.740]   that says, when I act as a representative of this company in the public space, everything
[03:11:35.740 --> 03:11:39.380]   I do, I do in the interest of increasing value.
[03:11:39.380 --> 03:11:41.380]   And I don't know.
[03:11:41.380 --> 03:11:43.180]   It's totally price value, doesn't it?
[03:11:43.180 --> 03:11:46.940]   Well, he hasn't demonstrated an awareness of that.
[03:11:46.940 --> 03:11:51.620]   He hasn't demonstrated an understanding that once your company is publicly traded, your
[03:11:51.620 --> 03:11:55.780]   relationship to both your intellectual property and your corporation is much different than
[03:11:55.780 --> 03:11:57.660]   when you own the whole thing the whole way down.
[03:11:57.660 --> 03:12:02.020]   Also, he spends so much time complaining about short sellers going after his stock and then
[03:12:02.020 --> 03:12:06.340]   to give them such blatant ammunition seems quite frankly a little bit bizarre.
[03:12:06.340 --> 03:12:10.580]   It's self-defeating even, but this is the story of Elon in the last couple of years.
[03:12:10.580 --> 03:12:13.780]   I mean, he's done with through his firms, a lot of really great stuff.
[03:12:13.780 --> 03:12:16.380]   I still watch SpaceX launches come a huge nerd.
[03:12:16.380 --> 03:12:17.380]   No, no, no, me too.
[03:12:17.380 --> 03:12:18.940]   He's revolutionized in the area.
[03:12:18.940 --> 03:12:20.500]   The live video is fantastic.
[03:12:20.500 --> 03:12:22.780]   And the Model 3 launch has finally revved up.
[03:12:22.780 --> 03:12:24.500]   They're claiming to hit profitability this year.
[03:12:24.500 --> 03:12:27.940]   If he just said nothing, a lot of stuff would have gone away.
[03:12:27.940 --> 03:12:34.900]   Instead, Joe Rogan, a large blunt and a thousand million memes that were born from that moment.
[03:12:34.900 --> 03:12:38.620]   I mean, just unnecessary and he's better friends to tell him not to do this stuff.
[03:12:38.620 --> 03:12:41.540]   Well, I also wonder if there's the fallacy of intelligence where the idea is if you're
[03:12:41.540 --> 03:12:44.020]   really smart in one area, you're smart in all areas.
[03:12:44.020 --> 03:12:45.660]   The Germans do have a word for this.
[03:12:45.660 --> 03:12:49.020]   It means someone who is highly skilled in one area or complete fool in others.
[03:12:49.020 --> 03:12:50.260]   I'm going to Google what that is.
[03:12:50.260 --> 03:12:51.260]   Yeah, I just...
[03:12:51.260 --> 03:12:52.260]   Please do.
[03:12:52.260 --> 03:12:58.020]   But because I wouldn't contest his innovation or his genius in a lot of ways.
[03:12:58.020 --> 03:13:01.140]   And in that interview, one of the things I really liked was how he nerd it out over
[03:13:01.140 --> 03:13:02.620]   alternative energy sources.
[03:13:02.620 --> 03:13:03.620]   Oh, yeah.
[03:13:03.620 --> 03:13:05.220]   And the idea of breaking free of the fossil fuel economy.
[03:13:05.220 --> 03:13:09.260]   I mean, good for him for getting those ideas out to an audience that is not typically hearing
[03:13:09.260 --> 03:13:13.180]   those ideas, like good for him for that and good for him for being smart about this stuff.
[03:13:13.180 --> 03:13:17.260]   But is he smart about how to be a CEO of a public facing publicly traded company?
[03:13:17.260 --> 03:13:21.500]   Well, in that same interview, he also reiterated the idea that maybe we're all just living
[03:13:21.500 --> 03:13:23.500]   in a simulation.
[03:13:23.500 --> 03:13:25.020]   That came up again.
[03:13:25.020 --> 03:13:26.780]   And that was after the talk, I believe.
[03:13:26.780 --> 03:13:30.020]   There's a philosophical term for that too, where...
[03:13:30.020 --> 03:13:31.020]   And I forget what that is.
[03:13:31.020 --> 03:13:36.380]   I read about it recently, where there's somebody like did an entire philosophical syllogism
[03:13:36.380 --> 03:13:38.140]   who's like, "This is the only logical conclusion."
[03:13:38.140 --> 03:13:39.140]   I was like, "Really?"
[03:13:39.140 --> 03:13:42.060]   Well, that's what Elon believes.
[03:13:42.060 --> 03:13:44.860]   And that can't be good for a shareholder's either.
[03:13:44.860 --> 03:13:46.140]   He doesn't want shareholder hope.
[03:13:46.140 --> 03:13:47.140]   I mean, that's clear.
[03:13:47.140 --> 03:13:49.140]   He said he wanted to get rid of him and he had to...
[03:13:49.140 --> 03:13:50.140]   He really like messed with him and he had to eat.
[03:13:50.140 --> 03:13:52.140]   He just had like a bunch of Mr. Smiths come in and sit down.
[03:13:52.140 --> 03:13:57.500]   A bunch of Hugo Weavings look like sitting in the audience all just completely unmused
[03:13:57.500 --> 03:13:59.780]   and simulation this Elon.
[03:13:59.780 --> 03:14:03.500]   Hey, all that cheating.
[03:14:03.500 --> 03:14:05.580]   Simulation comes for you in this case, I guess.
[03:14:05.580 --> 03:14:07.260]   Exactly, exactly.
[03:14:07.260 --> 03:14:09.020]   But he just needs to have to be reigned in.
[03:14:09.020 --> 03:14:10.300]   I'm sorry, like flat out.
[03:14:10.300 --> 03:14:11.300]   I know he's rich.
[03:14:11.300 --> 03:14:13.500]   I know he's interesting, but you've got to be a better manager in that.
[03:14:13.500 --> 03:14:15.340]   And you're screwing up your employees.
[03:14:15.340 --> 03:14:17.260]   That was not just yours.
[03:14:17.260 --> 03:14:18.260]   It's not just about you.
[03:14:18.260 --> 03:14:20.860]   In the same way that politics feels all Trump right now.
[03:14:20.860 --> 03:14:21.860]   All car feels Elon.
[03:14:21.860 --> 03:14:26.140]   But there's thousands of people that work there that have 401(k)s and have, you know,
[03:14:26.140 --> 03:14:27.140]   I'm sure R.C.
[03:14:27.140 --> 03:14:29.140]   Who are invested in this and they don't like it.
[03:14:29.140 --> 03:14:30.140]   The Thomas and the Christopher College.
[03:14:30.140 --> 03:14:33.660]   This is the conundrum of being a CEO is if you're doing it right, a lot of your time
[03:14:33.660 --> 03:14:38.060]   is actually spent looking at really big picture stuff and understanding the effects that you
[03:14:38.060 --> 03:14:43.140]   have on dozens of people, dozens of constituencies, each with their own priorities.
[03:14:43.140 --> 03:14:47.820]   And as a public facing CEO, he doesn't demonstrate an understanding or knowledge of that.
[03:14:47.820 --> 03:14:50.180]   He has the EQ of a six year old.
[03:14:50.180 --> 03:14:51.900]   I was not here for this.
[03:14:51.900 --> 03:14:56.220]   So I did not see this, but I'm so glad we have resurrected it.
[03:14:56.220 --> 03:14:57.740]   This was when I was on vacation.
[03:14:57.740 --> 03:15:02.500]   Oh, doctor decided he had something to say about the new iPhones and he did it in an interesting
[03:15:02.500 --> 03:15:03.500]   way.
[03:15:03.500 --> 03:15:09.780]   The size aspect is, I think, a good thing because here's my iPhone 8 and then here's
[03:15:09.780 --> 03:15:15.980]   the guys here 3D printed the new iPhone here.
[03:15:15.980 --> 03:15:20.340]   And you can see it's pretty much the same form factor, at least of my.
[03:15:20.340 --> 03:15:21.780]   But which one are you referring to?
[03:15:21.780 --> 03:15:22.940]   Is that the R?
[03:15:22.940 --> 03:15:24.940]   This is the S10S.
[03:15:24.940 --> 03:15:28.260]   You see how hard I'm working?
[03:15:28.260 --> 03:15:32.420]   My there's smoke coming out of my ears as I'm trying to remember this.
[03:15:32.420 --> 03:15:34.060]   These are the worst names.
[03:15:34.060 --> 03:15:39.140]   These are worst names since two months ago or a month ago when the note nine came out
[03:15:39.140 --> 03:15:41.540]   with lavender purple and ocean blue.
[03:15:41.540 --> 03:15:43.140]   I'm like, hey, they the same thing.
[03:15:43.140 --> 03:15:45.380]   If you just said lavender, what not noted?
[03:15:45.380 --> 03:15:46.380]   That's purple.
[03:15:46.380 --> 03:15:48.220]   If you said ocean with, I know that blue.
[03:15:48.220 --> 03:15:49.980]   How you just redundancy with the name?
[03:15:49.980 --> 03:15:52.540]   And then Apple comes along and says, hold my beer.
[03:15:52.540 --> 03:15:54.220]   And then they come out with all this stuff.
[03:15:54.220 --> 03:15:57.500]   And so you're saying that all the phones are the best phones now.
[03:15:57.500 --> 03:15:59.060]   Oh, no, they're not.
[03:15:59.060 --> 03:16:00.060]   No, they're not.
[03:16:00.060 --> 03:16:01.060]   That are.
[03:16:01.060 --> 03:16:02.060]   Okay.
[03:16:02.060 --> 03:16:04.500]   Can we are we we need to feature part yet?
[03:16:04.500 --> 03:16:05.500]   So I can just do it.
[03:16:05.500 --> 03:16:06.500]   I want to hear this.
[03:16:06.500 --> 03:16:07.500]   He was going.
[03:16:07.500 --> 03:16:09.820]   Let me tell you something.
[03:16:09.820 --> 03:16:11.660]   Oh, Dules Simpards.
[03:16:11.660 --> 03:16:12.660]   Welcome to 2012.
[03:16:12.660 --> 03:16:14.700]   Oh, we got Dules Simpards.
[03:16:14.700 --> 03:16:16.660]   You know, you can do that, right?
[03:16:16.660 --> 03:16:18.060]   You got to eat some card.
[03:16:18.060 --> 03:16:19.820]   Oh, every other people have had that cool.
[03:16:19.820 --> 03:16:20.820]   Okay.
[03:16:20.820 --> 03:16:21.820]   Next thing.
[03:16:21.820 --> 03:16:22.820]   Oh, stereo recording.
[03:16:22.820 --> 03:16:23.820]   Oh, what they had to do on the phone.
[03:16:23.820 --> 03:16:24.820]   Oh my God.
[03:16:24.820 --> 03:16:28.660]   But ours is magical stereo recording because we have waves in the video.
[03:16:28.660 --> 03:16:29.660]   It's amazing.
[03:16:29.660 --> 03:16:30.740]   Oh, wait.
[03:16:30.740 --> 03:16:31.900]   What's another feature that we have?
[03:16:31.900 --> 03:16:37.580]   Oh, we have HDR picture technology nano bike gigabyte thermal timer learning.
[03:16:37.580 --> 03:16:41.420]   Oh, pixel had that in their camera last year with.
[03:16:41.420 --> 03:16:42.420]   Oh, okay.
[03:16:42.420 --> 03:16:44.540]   So what do we have to do again?
[03:16:44.540 --> 03:16:45.540]   What do we do?
[03:16:45.540 --> 03:16:46.540]   What do we innovate?
[03:16:46.540 --> 03:16:47.540]   I don't know.
[03:16:47.540 --> 03:16:48.540]   I don't.
[03:16:48.540 --> 03:16:51.180]   So, oh, then you go talk about the battery life.
[03:16:51.180 --> 03:16:56.900]   What cell phone have you ever seen in your life that doesn't just tell you 13 hours with
[03:16:56.900 --> 03:17:02.140]   the asterisk 14 hours or ashes, Apple came along and said 30 minutes more than all phone
[03:17:02.140 --> 03:17:05.820]   you used to buy, 30 minutes more than that little phone you used to have.
[03:17:05.820 --> 03:17:09.580]   And maybe 15 minutes more than this new phone you going to get.
[03:17:09.580 --> 03:17:13.420]   If you get the big one, just tell me how long the battery lasts because you won't tell
[03:17:13.420 --> 03:17:15.100]   me what's in there for the battery.
[03:17:15.100 --> 03:17:20.460]   And then the one meter versus the two meter, you couldn't have just made that are the same
[03:17:20.460 --> 03:17:21.460]   depth.
[03:17:21.460 --> 03:17:22.460]   Really?
[03:17:22.460 --> 03:17:24.740]   I still got you just the 2.4 cents.
[03:17:24.740 --> 03:17:28.380]   See what it calls you because you tell me I still got the same chip in it and I'm not
[03:17:28.380 --> 03:17:29.620]   supposed to be jealous.
[03:17:29.620 --> 03:17:32.020]   Then you come with this whack-a-doodle screen.
[03:17:32.020 --> 03:17:33.740]   You charge $749.
[03:17:33.740 --> 03:17:35.140]   This phone should be $5.99.
[03:17:35.140 --> 03:17:37.540]   They want this phone for everybody and every kid.
[03:17:37.540 --> 03:17:41.620]   There's so many, so much better phones out there for that price range that you could
[03:17:41.620 --> 03:17:43.380]   get in the world.
[03:17:43.380 --> 03:17:45.540]   And they say, oh, $749 for everybody.
[03:17:45.540 --> 03:17:46.540]   For who?
[03:17:46.540 --> 03:17:49.740]   Entitled children are going to bug their kids so they can do this emoji thing.
[03:17:49.740 --> 03:17:50.740]   That's all there for.
[03:17:50.740 --> 03:17:52.300]   You're just trying to get money.
[03:17:52.300 --> 03:17:53.300]   Apple A innovated nothing.
[03:17:53.300 --> 03:17:54.300]   Oh, they do.
[03:17:54.300 --> 03:17:55.300]   I'm sorry, I'm yelling.
[03:17:55.300 --> 03:17:56.300]   I'm so upset.
[03:17:56.300 --> 03:18:00.940]   All they do is raise the price of their phones and they make more money on the margins.
[03:18:00.940 --> 03:18:07.060]   So when I spend $1,100 on a max and I only get 64 gigs and I look at a note and it starts
[03:18:07.060 --> 03:18:11.660]   at 128 gigs because they live in America and the world as we live in and they know that
[03:18:11.660 --> 03:18:18.460]   nobody wants the 64 gigs, then they offer you a phone at 512 for $1,500.
[03:18:18.460 --> 03:18:20.980]   And then you could put another 512 on top of that.
[03:18:20.980 --> 03:18:23.580]   I could get a tear about a space with a note.
[03:18:23.580 --> 03:18:27.700]   You know how many videos of cats and dogs in my child, I can do with a tear about on
[03:18:27.700 --> 03:18:28.700]   my phone.
[03:18:28.700 --> 03:18:31.500]   I can record my life for a month straight with a tear about.
[03:18:31.500 --> 03:18:36.900]   But Apple is just squeaking on by with a 64 then to a 256.
[03:18:36.900 --> 03:18:39.220]   I hate so much the things that they do.
[03:18:39.220 --> 03:18:44.380]   Now ask me if I'm buying it and which one I'm buying.
[03:18:44.380 --> 03:18:54.500]   I'm sorry 17 anonymous sources in government, you know, three letter agencies, Amazon and
[03:18:54.500 --> 03:19:03.860]   Apple said that Chinese spies had placed a tiny rice sized microchip on Super Micro
[03:19:03.860 --> 03:19:06.260]   Motherboards in 2015.
[03:19:06.260 --> 03:19:10.220]   These Super Micro Motherboards were used in many, many places.
[03:19:10.220 --> 03:19:16.660]   By Apple, the Department of Defense, we even had one because this all came to light.
[03:19:16.660 --> 03:19:19.340]   According to now, I'm going to say this a couple of times in the story because I want
[03:19:19.340 --> 03:19:24.900]   to emphasize according to Bloomberg Business Week, we even had one because it was first
[03:19:24.900 --> 03:19:28.060]   discovered by Amazon when they were doing their due diligence.
[03:19:28.060 --> 03:19:30.860]   They were about to acquire a company called Elemental Technologies.
[03:19:30.860 --> 03:19:35.980]   Elemental makes a streaming box, a box we've used for some years, a very expensive box
[03:19:35.980 --> 03:19:42.260]   that takes our video, compresses it and then feeds it out to Ustream, YouTube, Twitch,
[03:19:42.260 --> 03:19:46.020]   Mixer, whoever it is that's re streaming our stuff.
[03:19:46.020 --> 03:19:51.140]   These Elemental Motherboards, Amazon during a routine security audit before the acquisition
[03:19:51.140 --> 03:20:01.620]   found this little chip, according to Bloomberg Business Week, the chip was posing as a support,
[03:20:01.620 --> 03:20:03.460]   a minor support chip on the motherboard.
[03:20:03.460 --> 03:20:08.180]   Nobody would pay any attention to it, but when the mother was first booted in a server,
[03:20:08.180 --> 03:20:13.860]   would modify the operating system, phone home, the Chinese military and help the Chinese
[03:20:13.860 --> 03:20:18.820]   military hack these server servers in the Pentagon.
[03:20:18.820 --> 03:20:21.940]   Elementals widely used, as I said, we use it.
[03:20:21.940 --> 03:20:27.300]   I presume if this is true, our motherboards were hacked, but then the story gets weirder.
[03:20:27.300 --> 03:20:33.580]   So 17 sources, anonymous sources, Bloomberg Business Week, highly credible source.
[03:20:33.580 --> 03:20:41.660]   The next day, Amazon, Apple, even the freaking Chinese government, unequivocally deny it.
[03:20:41.660 --> 03:20:43.500]   They say it never happened.
[03:20:43.500 --> 03:20:49.860]   Apple said the story was wrong and misinformed.
[03:20:49.860 --> 03:20:55.540]   The Department of Homeland Security yesterday, well, actually their denial was a little less
[03:20:55.540 --> 03:20:57.180]   unequivocal.
[03:20:57.180 --> 03:21:02.940]   They said, we have no reason to doubt.
[03:21:02.940 --> 03:21:07.900]   We have no reason to doubt that Apple and Amazon are telling the truth.
[03:21:07.900 --> 03:21:10.100]   That's not exactly like no one ever happened.
[03:21:10.100 --> 03:21:14.500]   That's like, well, we have no reason in this particular circumstance.
[03:21:14.500 --> 03:21:17.340]   We currently have no reason to doubt.
[03:21:17.340 --> 03:21:21.380]   Nevertheless, the denials from Amazon and Apple were really strong.
[03:21:21.380 --> 03:21:23.380]   No, it didn't happen.
[03:21:23.380 --> 03:21:24.780]   Bloomberg's completely wrong.
[03:21:24.780 --> 03:21:26.740]   How does this happen?
[03:21:26.740 --> 03:21:27.740]   Is this a true?
[03:21:27.740 --> 03:21:30.940]   Actually, I'm going to poll you.
[03:21:30.940 --> 03:21:32.540]   We'll start with you, Rich.
[03:21:32.540 --> 03:21:34.180]   Who's right?
[03:21:34.180 --> 03:21:40.380]   Oh, man, I feel like this is the biggest fear of the tech world.
[03:21:40.380 --> 03:21:42.740]   Like all this stuff is made over in China.
[03:21:42.740 --> 03:21:48.380]   This is the perfect way of infiltrating the systems with a tiny chip that nobody recognizes
[03:21:48.380 --> 03:21:52.380]   or even cares to notice.
[03:21:52.380 --> 03:21:53.780]   They can take control of these things.
[03:21:53.780 --> 03:21:58.140]   They can modify the system, start up, whatever they want to do.
[03:21:58.140 --> 03:22:03.420]   It's tough for me to believe that Business Week has this many people saying this happened
[03:22:03.420 --> 03:22:04.700]   and it hasn't happened.
[03:22:04.700 --> 03:22:08.820]   I understand why these big companies, including Amazon and Apple, are strongly denying this
[03:22:08.820 --> 03:22:14.420]   because it's pretty bad for them if this happened to them.
[03:22:14.420 --> 03:22:16.020]   I believe it.
[03:22:16.020 --> 03:22:18.780]   I think that just reading the story, it's fascinating.
[03:22:18.780 --> 03:22:20.740]   I couldn't put this thing down.
[03:22:20.740 --> 03:22:21.740]   Oh, yeah.
[03:22:21.740 --> 03:22:25.740]   It's so well written and so like I just wanted to know more and more and more.
[03:22:25.740 --> 03:22:32.580]   But I think what was missing in this story to me is the consequence, concrete, what they
[03:22:32.580 --> 03:22:34.100]   did with these little chips.
[03:22:34.100 --> 03:22:38.500]   Maybe I missed that, but I didn't see, did they take down something?
[03:22:38.500 --> 03:22:43.740]   Or is this all just laying in wait for something that they want to do in the future?
[03:22:43.740 --> 03:22:46.100]   That was the part that I was confused upon.
[03:22:46.100 --> 03:22:48.780]   It's a great question.
[03:22:48.780 --> 03:22:51.540]   Who do you believe?
[03:22:51.540 --> 03:22:58.700]   Actually, my gut is actually siding with Apple just because of their history regarding security
[03:22:58.700 --> 03:23:00.020]   and privacy.
[03:23:00.020 --> 03:23:01.700]   And why would they lie?
[03:23:01.700 --> 03:23:08.180]   I mean, that would be a big deal for Apple's PR to issue and they issue two statements.
[03:23:08.180 --> 03:23:10.220]   They come forth pretty quickly about it.
[03:23:10.220 --> 03:23:15.340]   And then I just think back to other scenarios in the past with Apple as far as like when
[03:23:15.340 --> 03:23:19.660]   we had that shooting at that school in California and they wanted to search the phones and Apple
[03:23:19.660 --> 03:23:24.620]   was trying to do the right thing from a privacy and security standpoint.
[03:23:24.620 --> 03:23:28.020]   So I just, my gut just says, I'm going to go with Apple.
[03:23:28.020 --> 03:23:31.220]   Now, had it just been Amazon saying, no, that's not true.
[03:23:31.220 --> 03:23:34.580]   I'd have had the hairy eye.
[03:23:34.580 --> 03:23:36.020]   I'm dressed.
[03:23:36.020 --> 03:23:40.220]   Apple said specifically quote on this, I mean, this is as strong as you can get.
[03:23:40.220 --> 03:23:42.820]   We can be very clear.
[03:23:42.820 --> 03:23:47.620]   Apple has never found malicious chips, hardware manipulations or vulnerabilities purposely
[03:23:47.620 --> 03:23:51.540]   planted in any server.
[03:23:51.540 --> 03:23:54.140]   Daniel Rabino, what do you think you've been covering this for a long time?
[03:23:54.140 --> 03:23:55.140]   What do you think?
[03:23:55.140 --> 03:24:00.980]   Yeah, I'm leaning towards this is just going to be some truth to this now, but we may not
[03:24:00.980 --> 03:24:04.580]   know what the exact details actually don't know the exact details.
[03:24:04.580 --> 03:24:08.500]   And there could be some crossed wires here and information.
[03:24:08.500 --> 03:24:13.340]   Maybe some hardware was shipped to Apple, but it was caught in time and never deployed
[03:24:13.340 --> 03:24:17.500]   it in which case, both things could be true.
[03:24:17.500 --> 03:24:24.100]   But yeah, there's been a lot of reports around Huawei and other companies to write about
[03:24:24.100 --> 03:24:26.100]   this possibility, this threat.
[03:24:26.100 --> 03:24:27.860]   And it works both ways.
[03:24:27.860 --> 03:24:30.780]   There's concerns to the US companies doing the same.
[03:24:30.780 --> 03:24:32.740]   I don't know.
[03:24:32.740 --> 03:24:35.380]   My hunch says that there's probably some truth to this.
[03:24:35.380 --> 03:24:42.220]   It's just a question of to what extent the damage was done, but none of this is surprising
[03:24:42.220 --> 03:24:46.100]   to anyone who's been covering this industry for the last couple of years as this big concern
[03:24:46.100 --> 03:24:51.700]   or the strength of Chinese tech and its influence on US economy.
[03:24:51.700 --> 03:24:59.860]   Everybody knows that the way you wage war is changing from a pure military exercise to
[03:24:59.860 --> 03:25:05.020]   work technological or information in the case of what Russia has been doing or accused
[03:25:05.020 --> 03:25:06.980]   of doing a lot recently.
[03:25:06.980 --> 03:25:13.420]   So just because the companies deny this stuff, if it's an issue of national security, they
[03:25:13.420 --> 03:25:19.700]   could be quite fucking told to don't talk about this because you don't want to expose
[03:25:19.700 --> 03:25:20.980]   those vulnerabilities.
[03:25:20.980 --> 03:25:24.180]   Maybe there's active investigations going on.
[03:25:24.180 --> 03:25:28.580]   We don't know all the details here, but I find it hard to believe with 17 sources that
[03:25:28.580 --> 03:25:30.820]   none of this is true.
[03:25:30.820 --> 03:25:33.540]   I think it's more about degrees at this point.
[03:25:33.540 --> 03:25:37.980]   Zach Wittekuri who writes about security at TechCrunch and has worked at most recently
[03:25:37.980 --> 03:25:40.140]   at CBS, he's been covering security forever.
[03:25:40.140 --> 03:25:46.780]   He says, "Welcome to the murky world of national security reporting."
[03:25:46.780 --> 03:25:52.660]   He says, "Even with this story, my gut is mixed."
[03:25:52.660 --> 03:25:58.220]   There is one potential scenario that maybe means both of these, that the companies aren't
[03:25:58.220 --> 03:26:00.020]   lying, but the story is true.
[03:26:00.020 --> 03:26:05.220]   I have to say, to add myself to this poll, there's no way Bloomberg Business Week runs
[03:26:05.220 --> 03:26:12.820]   this story and they stand behind it, by the way, unless they absolutely have unequivocal
[03:26:12.820 --> 03:26:14.340]   evidence of this.
[03:26:14.340 --> 03:26:19.820]   17 sources, all anonymous, yeah, but you'd have to say Bloomberg Business Week is making
[03:26:19.820 --> 03:26:22.740]   this up.
[03:26:22.740 --> 03:26:26.260]   That's too many people to not be true.
[03:26:26.260 --> 03:26:32.340]   So here's maybe a very narrow, maybe the Apple PR team that responded and this was John
[03:26:32.340 --> 03:26:34.660]   Gruber's theory didn't know about it, right?
[03:26:34.660 --> 03:26:39.180]   If something like this happens, you're not going to send out a memo at everybody in Apple
[03:26:39.180 --> 03:26:42.100]   saying, "Hey, by the way, look what we found."
[03:26:42.100 --> 03:26:44.860]   You're going to keep it very quiet.
[03:26:44.860 --> 03:26:49.180]   So maybe a handful of people at Apple know and the ones who don't know are the ones making
[03:26:49.180 --> 03:26:51.460]   the unequivocal denial.
[03:26:51.460 --> 03:26:55.780]   There's also a possibility there's a US government gag order, although Apple again says, "We
[03:26:55.780 --> 03:27:00.540]   are in no way under a gag order."
[03:27:00.540 --> 03:27:02.100]   But again.
[03:27:02.100 --> 03:27:06.780]   Maybe the people who are under the gag order are a handful of people, not the people writing
[03:27:06.780 --> 03:27:08.460]   the press release.
[03:27:08.460 --> 03:27:11.460]   That's the only way I could see both sides being accurate.
[03:27:11.460 --> 03:27:15.380]   I don't think Apple PR is lying in such an outright.
[03:27:15.380 --> 03:27:18.940]   They would say nothing or they would say little or they would say, "There's lots of
[03:27:18.940 --> 03:27:22.860]   ways they could slice this without saying, 'No, it did not happen.'"
[03:27:22.860 --> 03:27:30.580]   Yeah, it's odd that they came out so strong in their denial of this immediately and emailed
[03:27:30.580 --> 03:27:32.500]   the media, including myself.
[03:27:32.500 --> 03:27:38.140]   And that rarely happens when they, usually the way Apple works is they kind of talk to
[03:27:38.140 --> 03:27:41.700]   the certain media that covers the beat that they want to get the story out with.
[03:27:41.700 --> 03:27:47.020]   This time they just sent out a blanket email through their legal guy denying it and linking
[03:27:47.020 --> 03:27:50.180]   to this, which I found to be very heavy-handed.
[03:27:50.180 --> 03:27:53.380]   So it's like one of these things where you want to believe Apple, but at the same time
[03:27:53.380 --> 03:27:55.220]   it seems like it's overly.
[03:27:55.220 --> 03:28:01.100]   I understand they have a lot to lose in this, but it just seems like they really want to
[03:28:01.100 --> 03:28:03.100]   make sure that like, "No, we are not involved in this.
[03:28:03.100 --> 03:28:04.380]   This is not true at all."
[03:28:04.380 --> 03:28:09.020]   I wonder if their reporter is going to be invited to the next Apple event.
[03:28:09.020 --> 03:28:11.620]   That's Apple's stick, baby.
[03:28:11.620 --> 03:28:12.620]   You're probably not.
[03:28:12.620 --> 03:28:18.020]   We won't be seeing you at the next Apple event.
[03:28:18.020 --> 03:28:23.700]   Somebody who hasn't been invited to an Apple event in years, I could say that stick isn't
[03:28:23.700 --> 03:28:24.700]   that weighty.
[03:28:24.700 --> 03:28:29.340]   I was going to say another angle to this possibly is more of a conspiratorial angle is that
[03:28:29.340 --> 03:28:31.580]   this is really a story.
[03:28:31.580 --> 03:28:36.940]   The US government has a vested interest in undermining Chinese tech, or at least eroding
[03:28:36.940 --> 03:28:39.740]   the trust in it for such circumstances.
[03:28:39.740 --> 03:28:45.500]   We all see about the Qualcomm Broadcom attempt to buy out and how that was blocked.
[03:28:45.500 --> 03:28:51.220]   Do you think Bloomberg got played by the US intelligence committee?
[03:28:51.220 --> 03:28:53.900]   In a way, no, 17 seems like a lot.
[03:28:53.900 --> 03:28:59.300]   On the other hand, this is what intelligence is supposed to be doing.
[03:28:59.300 --> 03:29:01.860]   I don't think we can discount it.
[03:29:01.860 --> 03:29:06.660]   I still think there's actually a threat from China in this sense because why it went in
[03:29:06.660 --> 03:29:07.660]   a thing.
[03:29:07.660 --> 03:29:09.900]   That's how counterintelligence works.
[03:29:09.900 --> 03:29:12.220]   We do the same thing.
[03:29:12.220 --> 03:29:18.540]   There may be some truth to this, but it may have been played up by some people in the
[03:29:18.540 --> 03:29:21.500]   government too.
[03:29:21.500 --> 03:29:27.380]   Even this current administration is very much against giving China too much leverage when
[03:29:27.380 --> 03:29:29.900]   it comes to manufacturing.
[03:29:29.900 --> 03:29:31.260]   I don't know.
[03:29:31.260 --> 03:29:36.220]   It's a hard story to follow and believe, especially when you're talking about espionage counter
[03:29:36.220 --> 03:29:38.700]   intelligence and national security.
[03:29:38.700 --> 03:29:43.260]   Bloomberg breaks the company's denials are countered by six current and former senior
[03:29:43.260 --> 03:29:49.900]   national security officials who in conversations that began during the Obama administration
[03:29:49.900 --> 03:29:53.760]   and continued under the Trump administration, detailed the discovery of the chips in the
[03:29:53.760 --> 03:29:55.100]   government's investigation.
[03:29:55.100 --> 03:30:00.380]   This has been going on for several years.
[03:30:00.380 --> 03:30:05.340]   It sounds like Bloomberg reporters have been working on this story for several years.
[03:30:05.340 --> 03:30:10.420]   One of those officials and two people inside Amazon provided extensive information on
[03:30:10.420 --> 03:30:13.940]   how the attack played out at elemental in Amazon.
[03:30:13.940 --> 03:30:14.940]   That's another thing.
[03:30:14.940 --> 03:30:21.100]   You would not only have to, it's one thing to say, "Well, you can easily get 17 CIA people
[03:30:21.100 --> 03:30:22.580]   to say, "Yeah, this happened."
[03:30:22.580 --> 03:30:25.140]   I mean, that's their job.
[03:30:25.140 --> 03:30:28.700]   But then you're getting people inside Amazon saying confirming it.
[03:30:28.700 --> 03:30:31.060]   The details match.
[03:30:31.060 --> 03:30:34.700]   The officials and one of the insiders also described Amazon's cooperation with the government
[03:30:34.700 --> 03:30:35.700]   investigation.
[03:30:35.700 --> 03:30:40.300]   In addition to the three Apple insiders, four of the six US officials confirmed that Apple
[03:30:40.300 --> 03:30:42.020]   was a victim.
[03:30:42.020 --> 03:30:47.180]   17 people confirmed the manipulation of super micros hardware.
[03:30:47.180 --> 03:30:51.900]   The sources were granted anonymity because of the sensitive and in some cases classified
[03:30:51.900 --> 03:30:53.180]   nature of the information.
[03:30:53.180 --> 03:30:55.100]   Daniel, that's an interesting theory though.
[03:30:55.100 --> 03:31:00.380]   I kind of, maybe this is, you know, it's clear something's going on.
[03:31:00.380 --> 03:31:03.820]   Yeah, I think that's the truth.
[03:31:03.820 --> 03:31:06.820]   But we don't know what it is.
[03:31:06.820 --> 03:31:12.060]   One warfare, it's the new battle of the 21st century, super dangerous, super hard to counter.
[03:31:12.060 --> 03:31:15.500]   It's who do you believe, who do you trust, and when you undermine all institutions around
[03:31:15.500 --> 03:31:19.380]   you, people can't even trust new sources anymore.
[03:31:19.380 --> 03:31:21.700]   So it's a brave new world.
[03:31:21.700 --> 03:31:24.300]   It's not a, I'm kind of nervous about it.
[03:31:24.300 --> 03:31:28.940]   During my did political science, undermining classic institutions is the first way you
[03:31:28.940 --> 03:31:30.420]   destabilize society.
[03:31:30.420 --> 03:31:31.420]   So that's exactly right.
[03:31:31.420 --> 03:31:32.900]   Is a real threat to it.
[03:31:32.900 --> 03:31:34.140]   That's exactly right.
[03:31:34.140 --> 03:31:39.420]   Well, and from the consumer standpoint, I mean, obviously this was kind of big server things,
[03:31:39.420 --> 03:31:43.940]   but for the average consumer, I think it makes you think twice too.
[03:31:43.940 --> 03:31:48.100]   With everything that you have in your house, I mean, we're all putting these smart speakers
[03:31:48.100 --> 03:31:50.420]   in our homes.
[03:31:50.420 --> 03:31:51.620]   Where are those manufactured?
[03:31:51.620 --> 03:31:56.380]   I mean, this is becoming a reality for the average person to think about the stuff that
[03:31:56.380 --> 03:32:00.060]   we're carrying with us and the stuff that we're putting in our homes, the routers, the
[03:32:00.060 --> 03:32:01.060]   systems.
[03:32:01.060 --> 03:32:05.740]   And this becomes like very clear that, oh my gosh, like this is something that, of course,
[03:32:05.740 --> 03:32:08.180]   we've thought of, but not into this detail.
[03:32:08.180 --> 03:32:11.780]   And you're just like, this is exactly how it's done in a big way.
[03:32:11.780 --> 03:32:12.780]   Right.
[03:32:12.780 --> 03:32:15.580]   IoT is a huge security vulnerability right now.
[03:32:15.580 --> 03:32:19.540]   Microsoft's trying to counter with Azure Sphere, which has been in the headlines recently,
[03:32:19.540 --> 03:32:24.140]   is their first attempt at making these sort of systems more secure for everybody, anyone
[03:32:24.140 --> 03:32:25.460]   who's manufacturing devices.
[03:32:25.460 --> 03:32:29.500]   But right now, it's Wild West, people are just putting microphones and internet into all
[03:32:29.500 --> 03:32:34.140]   these devices, we're putting them in our homes and there's no regulation or checks.
[03:32:34.140 --> 03:32:35.140]   It's going to come back to buy this.
[03:32:35.140 --> 03:32:36.140]   I guarantee it will.
[03:32:36.140 --> 03:32:37.340]   It's just a matter of time.
[03:32:37.340 --> 03:32:40.900]   Well, really, if you believe I said, go ahead, Anne.
[03:32:40.900 --> 03:32:46.460]   As much as I said, I trust what Apple's response was, at the same time, I still have
[03:32:46.460 --> 03:32:51.660]   that little fear in the back of my head of the button that the Chinese could just push
[03:32:51.660 --> 03:32:57.140]   at any time because of everything that I'm looking at right now was not made right here
[03:32:57.140 --> 03:33:02.220]   in the US and they could easily just push a button to make things just go nuts on me
[03:33:02.220 --> 03:33:08.100]   and maybe cause some even bigger problems if they want to because we don't know.
[03:33:08.100 --> 03:33:13.100]   I can crack this machine open and I don't know what all of these transistors and capacitors
[03:33:13.100 --> 03:33:14.100]   and all of that.
[03:33:14.100 --> 03:33:15.580]   I don't know what any of that stuff is doing.
[03:33:15.580 --> 03:33:17.740]   I just know what it's my motherboard.
[03:33:17.740 --> 03:33:19.700]   Well, I mean, you make an excellent point.
[03:33:19.700 --> 03:33:24.860]   Very few people understand the complexity of our modern world and fewer still understand
[03:33:24.860 --> 03:33:28.580]   the security issues and could detect these problems.
[03:33:28.580 --> 03:33:34.980]   If you have a determinant adversary with governmental, with nation state level resources behind
[03:33:34.980 --> 03:33:42.500]   them, even if it's just North Korea, it seems to me that it would be very, very difficult
[03:33:42.500 --> 03:33:46.540]   if you had somebody wanted to do this to prevent it.
[03:33:46.540 --> 03:33:50.620]   Everything is made in China, almost everything.
[03:33:50.620 --> 03:33:53.220]   The supply chain is coming out of China.
[03:33:53.220 --> 03:33:59.620]   If you're buying a phone made in Korea or Taiwan or Thailand or India or Brazil, many
[03:33:59.620 --> 03:34:02.500]   of the parts are made in China.
[03:34:02.500 --> 03:34:07.940]   So I doubt there's one consumer electronic device in your house that it hasn't something
[03:34:07.940 --> 03:34:11.100]   that was manufactured in China.
[03:34:11.100 --> 03:34:14.140]   I'm sitting here looking at junk on my desk.
[03:34:14.140 --> 03:34:16.980]   This is a power distribution board.
[03:34:16.980 --> 03:34:19.340]   Pretty sure this was not made in the USA.
[03:34:19.340 --> 03:34:23.060]   Does it say super micro on it?
[03:34:23.060 --> 03:34:25.020]   Just look for that.
[03:34:25.020 --> 03:34:27.340]   It's like the equivalent of the Intel inside sticker.
[03:34:27.340 --> 03:34:30.420]   Can you imagine if super micro had one of those and now everyone just sees those?
[03:34:30.420 --> 03:34:34.900]   I'm going to go from the flip side though on the fact that China knows that all this
[03:34:34.900 --> 03:34:36.540]   stuff is big business.
[03:34:36.540 --> 03:34:41.820]   And I think, I mean, clearly we love to believe in these conspiracy theories and we love to
[03:34:41.820 --> 03:34:45.820]   imagine that it's like us versus them versus Russia.
[03:34:45.820 --> 03:34:48.940]   I do think there, of course, are elements of that in our world.
[03:34:48.940 --> 03:34:52.900]   But at the same time, I want to believe that they know that the business that they're
[03:34:52.900 --> 03:34:57.660]   going to make off these components are way better than the business they're going to
[03:34:57.660 --> 03:35:00.900]   make off of whatever marketplace they're dealing with.
[03:35:00.900 --> 03:35:01.900]   Absolutely.
[03:35:01.900 --> 03:35:02.900]   The shady stuff.
[03:35:02.900 --> 03:35:08.180]   What's going on in China and it's true in other places too is there's more than one
[03:35:08.180 --> 03:35:10.500]   constituency.
[03:35:10.500 --> 03:35:14.420]   So there are absolutely, I would say most of the Chinese government wants to maintain
[03:35:14.420 --> 03:35:19.300]   good relations because after all, we're their biggest market.
[03:35:19.300 --> 03:35:24.820]   On the other hand, there are and this heck, it's presumed came from the Chinese military.
[03:35:24.820 --> 03:35:29.300]   There are, I'm sure hard liners in the Chinese military who say, "Yeah, let's just in case,
[03:35:29.300 --> 03:35:32.140]   let's be ready for cyber warfare."
[03:35:32.140 --> 03:35:35.220]   I don't think it's monolithic in other words.
[03:35:35.220 --> 03:35:37.060]   And I completely agree with you, Daniel.
[03:35:37.060 --> 03:35:41.340]   This completely could be a conspiracy from the United States government who wants us to
[03:35:41.340 --> 03:35:44.140]   say exactly that.
[03:35:44.140 --> 03:35:45.140]   It's crazy.
[03:35:45.140 --> 03:35:51.940]   At this point, the only way America can hack the rest of the world is by putting something
[03:35:51.940 --> 03:35:56.660]   in a frappuccino because that's the only thing we're exporting is Starbucks.
[03:35:56.660 --> 03:35:58.860]   Wait, I would have got Facebook though.
[03:35:58.860 --> 03:36:00.580]   Facebook I hear is not so secure.
[03:36:00.580 --> 03:36:03.540]   Yes, that's a good point.
[03:36:03.540 --> 03:36:04.620]   What a world.
[03:36:04.620 --> 03:36:05.780]   What a world we live in.
[03:36:05.780 --> 03:36:10.660]   But it was almost inevitable when the tools we use got so complex.
[03:36:10.660 --> 03:36:18.060]   Harry Pournell, I've mentioned this to Jerry before he passed in his book Lucifer's Hammer.
[03:36:18.060 --> 03:36:22.100]   The story is that a giant asteroid hits the United States, the great book.
[03:36:22.100 --> 03:36:23.100]   Yeah, isn't it good?
[03:36:23.100 --> 03:36:29.260]   And there's just a few survivors in the world who have to cobble together existence again.
[03:36:29.260 --> 03:36:33.220]   But he points out that we live in a world where we use stuff every day.
[03:36:33.220 --> 03:36:39.340]   We have no idea how it works, nor could we recreate it if we had to.
[03:36:39.340 --> 03:36:40.980]   And that's your car.
[03:36:40.980 --> 03:36:44.860]   That's your freaking microwave oven, let alone your computers and your phones.
[03:36:44.860 --> 03:36:45.860]   I'm guilty.
[03:36:45.860 --> 03:36:49.020]   We're all guilty.
[03:36:49.020 --> 03:36:53.100]   And I don't think there's any one person who could do this who could recreate this.
[03:36:53.100 --> 03:37:01.540]   So that's kind of the consequence of that is there's a huge amount of trust in using
[03:37:01.540 --> 03:37:03.380]   all of this stuff.
[03:37:03.380 --> 03:37:07.980]   And if you had an actor who decided that he didn't care about being trusted or wanted
[03:37:07.980 --> 03:37:15.340]   to subvert this, it's a little, is it chilling or is it industrial espionage?
[03:37:15.340 --> 03:37:19.820]   I mean, a lot of these servers are used by the Department of Defense.
[03:37:19.820 --> 03:37:25.020]   These elemental servers alone are used all over the place.
[03:37:25.020 --> 03:37:33.500]   The CIA, the NSA, they're used in drone placement.
[03:37:33.500 --> 03:37:35.420]   They're used controllers.
[03:37:35.420 --> 03:37:38.220]   They're used all over the place.
[03:37:38.220 --> 03:37:40.340]   But okay, but here's the question.
[03:37:40.340 --> 03:37:43.580]   Has anyone cracked one of these open and found this little chip?
[03:37:43.580 --> 03:37:46.740]   I mean, wouldn't that be the first thing that one of these companies would do is kind
[03:37:46.740 --> 03:37:47.740]   of look in there?
[03:37:47.740 --> 03:37:52.540]   I mean, I know it's only been a couple of days since the story came out, but wouldn't
[03:37:52.540 --> 03:37:54.540]   that be part of this reporting?
[03:37:54.540 --> 03:37:58.700]   I mean, I think that's what the original report actually did say was that the, when
[03:37:58.700 --> 03:38:04.820]   Amazon was looking to buy elemental, they sent their boards into for review to a third-party
[03:38:04.820 --> 03:38:10.780]   company who found a little chip on there that was not part of the original diagram designed
[03:38:10.780 --> 03:38:11.980]   for that chipset.
[03:38:11.980 --> 03:38:13.620]   And that's what the red flag was.
[03:38:13.620 --> 03:38:17.540]   And then I think trying to figure out what it does or its potential was a matter of sort
[03:38:17.540 --> 03:38:19.380]   of reverse engineering it.
[03:38:19.380 --> 03:38:24.340]   Well, I can bet that everybody who has a super micro motherboard and has the capability.
[03:38:24.340 --> 03:38:25.980]   The problem is who has the capability?
[03:38:25.980 --> 03:38:27.620]   These are, there's a lot of little chips on them.
[03:38:27.620 --> 03:38:29.980]   Have you ever looked at a little motherboard?
[03:38:29.980 --> 03:38:30.980]   And they're labeled.
[03:38:30.980 --> 03:38:34.820]   I mean, I don't know how you would have to, you would have to take the thing apart, test
[03:38:34.820 --> 03:38:35.900]   each one individually.
[03:38:35.900 --> 03:38:37.460]   All right.
[03:38:37.460 --> 03:38:38.460]   Non-trivial.
[03:38:38.460 --> 03:38:40.180]   And who has those resources?
[03:38:40.180 --> 03:38:42.460]   And elemental cost $30,000.
[03:38:42.460 --> 03:38:44.700]   I am not taking my elemental.
[03:38:44.700 --> 03:38:47.420]   And by, I actually proposed this to our engineering team.
[03:38:47.420 --> 03:38:50.500]   I said we have, because we have a new elemental from AWS.
[03:38:50.500 --> 03:38:53.140]   So that we just got a few months ago.
[03:38:53.140 --> 03:38:57.020]   So that will not obviously, but we had the old elemental from this timeframe.
[03:38:57.020 --> 03:38:59.060]   We were using it back then.
[03:38:59.060 --> 03:39:01.540]   But unfortunately we had to send it back.
[03:39:01.540 --> 03:39:03.100]   Where's the, I fix it tear down?
[03:39:03.100 --> 03:39:05.580]   Yeah, I want to tear down.
[03:39:05.580 --> 03:39:07.460]   I fix it maybe could do that.
[03:39:07.460 --> 03:39:08.460]   Yeah.
[03:39:08.460 --> 03:39:15.460]   They specialize in identifying unlabeled or poorly labeled or incorrectly labeled chips.
[03:39:15.460 --> 03:39:20.060]   I think we all, we can all agree that this is, there's potential for this to happen,
[03:39:20.060 --> 03:39:24.300]   whether it actually did happen or not.
[03:39:24.300 --> 03:39:26.820]   And I think Bloomberg didn't get it wrong.
[03:39:26.820 --> 03:39:33.860]   I just don't understand what happened, what the response is and why the response is that.
[03:39:33.860 --> 03:39:37.940]   And I don't think it's, I don't think Apple is so venous to say, well, that will hurt
[03:39:37.940 --> 03:39:38.940]   our business.
[03:39:38.940 --> 03:39:40.260]   We better lie about it.
[03:39:40.260 --> 03:39:41.860]   I agree with you in that case.
[03:39:41.860 --> 03:39:43.380]   There's no way that's the case.
[03:39:43.380 --> 03:39:48.260]   I think it's much more that whoever issued this release didn't know if anything.
[03:39:48.260 --> 03:39:50.780]   You know, and nobody's, nobody's talking.
[03:39:50.780 --> 03:39:52.580]   Maybe even Tim Cook doesn't know.
[03:39:52.580 --> 03:39:53.580]   I don't know.
[03:39:53.580 --> 03:40:00.260]   It's like Trey Reklev made his name on Google+, because it was such a good place for photographers.
[03:40:00.260 --> 03:40:05.020]   People like Mike Elgin really sang its praises.
[03:40:05.020 --> 03:40:06.220]   I loved it.
[03:40:06.220 --> 03:40:11.860]   But I mean, I'm looking at my Google+ pages filled with spam of a sexual nature in many
[03:40:11.860 --> 03:40:12.860]   cases.
[03:40:12.860 --> 03:40:17.300]   And I'm a member of a, remember they pushed communities?
[03:40:17.300 --> 03:40:19.500]   So I joined a lot of geek communities.
[03:40:19.500 --> 03:40:22.300]   And now I'm getting porn spam.
[03:40:22.300 --> 03:40:25.940]   It's not a good look for Google+.
[03:40:25.940 --> 03:40:30.780]   The one metric from the product strobe, the one was that I think that the average time
[03:40:30.780 --> 03:40:31.780]   spent on the age of...
[03:40:31.780 --> 03:40:32.780]   Google+ fee.
[03:40:32.780 --> 03:40:33.780]   Don't show it on TV.
[03:40:33.780 --> 03:40:36.260]   Well, you see, Marcin, to join the geek porn community.
[03:40:36.260 --> 03:40:38.180]   It's not a geek porn community.
[03:40:38.180 --> 03:40:39.180]   It's geeks.
[03:40:39.180 --> 03:40:41.820]   I'm going to, I guess I should unjoin it.
[03:40:41.820 --> 03:40:43.300]   Nobody's, that's part of the problem.
[03:40:43.300 --> 03:40:44.300]   Right, Ron?
[03:40:44.300 --> 03:40:47.820]   If you don't moderate your community, the bad guys just totally...
[03:40:47.820 --> 03:40:48.820]   And they get to it.
[03:40:48.820 --> 03:40:49.820]   They get there.
[03:40:49.820 --> 03:40:53.460]   Leave geeks, the Google+ geek community apparently is not...
[03:40:53.460 --> 03:40:55.900]   Oh man, what's this one?
[03:40:55.900 --> 03:40:56.900]   Photography.
[03:40:56.900 --> 03:40:59.180]   Oh, now I have to leave photography too.
[03:40:59.180 --> 03:41:00.780]   They're all full of it.
[03:41:00.780 --> 03:41:05.060]   The data point that was the average time spent on page by Google+ users was like six seconds.
[03:41:05.060 --> 03:41:06.060]   They said...
[03:41:06.060 --> 03:41:08.700]   This is in the project's show announcement.
[03:41:08.700 --> 03:41:12.780]   We found that 90% of Google+ users left the page after five seconds.
[03:41:12.780 --> 03:41:13.780]   Yeah.
[03:41:13.780 --> 03:41:14.780]   Because they went...
[03:41:14.780 --> 03:41:16.700]   Whoa, why my eyes?
[03:41:16.700 --> 03:41:19.860]   Before they went to the page and posted that link and then went to the next one.
[03:41:19.860 --> 03:41:20.860]   Right.
[03:41:20.860 --> 03:41:21.860]   Because they're just spamming it.
[03:41:21.860 --> 03:41:24.820]   It's too bad because honestly, it's a beautiful...
[03:41:24.820 --> 03:41:25.820]   Am I wrong?
[03:41:25.820 --> 03:41:27.100]   I think it's a beautiful social network.
[03:41:27.100 --> 03:41:32.180]   It has moderation tools better than Twitter, better than Facebook.
[03:41:32.180 --> 03:41:34.340]   I thought Google...
[03:41:34.340 --> 03:41:35.740]   Google+ should have won.
[03:41:35.740 --> 03:41:38.100]   Why did it not win?
[03:41:38.100 --> 03:41:40.820]   Google fundamentally doesn't understand social.
[03:41:40.820 --> 03:41:42.820]   As a company, as a culture, they fundamentally don't understand the whole...
[03:41:42.820 --> 03:41:43.900]   What if you just give people a great tool?
[03:41:43.900 --> 03:41:46.660]   Why don't they use it?
[03:41:46.660 --> 03:41:50.820]   They don't think anybody building the products or steering it understands social.
[03:41:50.820 --> 03:41:51.820]   I just don't.
[03:41:51.820 --> 03:41:54.340]   I think that you can have all the best things.
[03:41:54.340 --> 03:41:57.420]   If you're not coming at it from that perspective...
[03:41:57.420 --> 03:42:00.540]   There was an interesting medium post or it wasn't a medium, sorry, but somebody's blog
[03:42:00.540 --> 03:42:01.540]   who...
[03:42:01.540 --> 03:42:05.380]   Someone who's on the design team for the first eight months who talked about his experiences.
[03:42:05.380 --> 03:42:10.540]   They're obviously colored in some ways that maybe aren't fair to the final product.
[03:42:10.540 --> 03:42:15.460]   It was always from the very beginning, it was a defensive move, always.
[03:42:15.460 --> 03:42:17.180]   That comes through.
[03:42:17.180 --> 03:42:22.700]   I think that the fact that it was designed because Google realized that they'd miss social.
[03:42:22.700 --> 03:42:27.820]   Then the fact that it was pushed down everyone's throats so much to the point where there
[03:42:27.820 --> 03:42:29.980]   wasn't even a reason for people to want to use it.
[03:42:29.980 --> 03:42:31.140]   You were almost turned off.
[03:42:31.140 --> 03:42:34.180]   There was a point where it was linked to all of your Google stuff.
[03:42:34.180 --> 03:42:38.300]   I'm verified on YouTube because of it, so I'm thankful for that.
[03:42:38.300 --> 03:42:42.980]   I had a ridiculous number of Google+ followers.
[03:42:42.980 --> 03:42:46.220]   I would joke about it because it was hilarious.
[03:42:46.220 --> 03:42:47.220]   It still is.
[03:42:47.220 --> 03:42:49.780]   I didn't really ever see that sort of engagement.
[03:42:49.780 --> 03:42:53.660]   It was just this weird thing foisted upon people.
[03:42:53.660 --> 03:42:57.860]   I don't think it ever had direction from people who saw it as anything other than we've got
[03:42:57.860 --> 03:43:03.620]   to try to beat Facebook because that's an important part of the future going forward.
[03:43:03.620 --> 03:43:05.580]   It is what it is.
[03:43:05.580 --> 03:43:07.140]   It's a shame, but it is what it is.
[03:43:07.140 --> 03:43:09.660]   It is a shame.
[03:43:09.660 --> 03:43:14.500]   By the way, if you want to learn Google+, the Brooklyn Public Library on Tuesday is having
[03:43:14.500 --> 03:43:16.580]   a class.
[03:43:16.580 --> 03:43:18.900]   It's never too late to learn Google+.
[03:43:18.900 --> 03:43:19.900]   Oh, God.
[03:43:19.900 --> 03:43:21.300]   It will be in 10 months.
[03:43:21.300 --> 03:43:22.300]   That's so good.
[03:43:22.300 --> 03:43:23.300]   That's my old library.
[03:43:23.300 --> 03:43:24.780]   That's my old library.
[03:43:24.780 --> 03:43:25.780]   I'm so...
[03:43:25.780 --> 03:43:26.780]   Oh, God.
[03:43:26.780 --> 03:43:27.780]   I want to go to that too.
[03:43:27.780 --> 03:43:28.780]   Me too.
[03:43:28.780 --> 03:43:29.780]   You literally...
[03:43:29.780 --> 03:43:30.780]   I'll go.
[03:43:30.780 --> 03:43:31.780]   You guys want...
[03:43:31.780 --> 03:43:33.780]   I'm just five minutes from Brooklyn.
[03:43:33.780 --> 03:43:34.780]   Check it out.
[03:43:34.780 --> 03:43:40.740]   That was literally like a five-minute walk from my apartment for many years.
[03:43:40.740 --> 03:43:42.740]   I'm very sad that I can't go to that.
[03:43:42.740 --> 03:43:43.740]   That's so good.
[03:43:43.740 --> 03:43:45.380]   That's so good.
[03:43:45.380 --> 03:43:49.740]   Everybody in Brooklyn who loves Google+, should go to this event.
[03:43:49.740 --> 03:43:51.220]   It's a very Brooklyn thing too.
[03:43:51.220 --> 03:43:54.300]   It's like, if you go to this ironically.
[03:43:54.300 --> 03:43:55.300]   What day and time is it?
[03:43:55.300 --> 03:43:58.380]   It's a Tuesday at 11 a.m. at the Pacific Library.
[03:43:58.380 --> 03:44:00.900]   I was going to say Tuesday is when we do all about Android.
[03:44:00.900 --> 03:44:03.300]   No, but you can do it before all about Android.
[03:44:03.300 --> 03:44:04.300]   Yeah.
[03:44:04.300 --> 03:44:07.940]   25/4th Street at Pacific Avenue, Brooklyn.
[03:44:07.940 --> 03:44:12.260]   Everybody show up just to show the flag.
[03:44:12.260 --> 03:44:15.100]   It's sad to me because it really has great tools.
[03:44:15.100 --> 03:44:17.100]   It could have been...
[03:44:17.100 --> 03:44:19.180]   I see Mike still posting.
[03:44:19.180 --> 03:44:21.900]   That's where I got that link by the way from Mike Elgin.
[03:44:21.900 --> 03:44:27.100]   Trey Rakcliffe is actually putting together a documentary kind of an online thing about
[03:44:27.100 --> 03:44:28.740]   saying goodbye where people are going to...
[03:44:28.740 --> 03:44:31.540]   He wanted me to record something and I'll do it.
[03:44:31.540 --> 03:44:37.620]   They're thoughts and memories of Google+, it's sad.
[03:44:37.620 --> 03:44:42.980]   Although I know this Carson will not show my Google+, he's not going to show my Google,
[03:44:42.980 --> 03:44:45.660]   he saw what happened, right?
[03:44:45.660 --> 03:44:47.300]   Poor Mark Millian's going, "Why?
[03:44:47.300 --> 03:44:50.980]   I don't want to see that."
[03:44:50.980 --> 03:44:52.380]   The spammers just got a hold of it.
[03:44:52.380 --> 03:44:53.860]   How do you survive that Ron?
[03:44:53.860 --> 03:44:56.580]   You have to go in every day, don't you?
[03:44:56.580 --> 03:44:58.220]   His name is Jason Howell and he's amazing.
[03:44:58.220 --> 03:45:01.300]   He's a blessing for all of us.
[03:45:01.300 --> 03:45:04.700]   We flag, but also the community does.
[03:45:04.700 --> 03:45:08.660]   We flag and mark things as spam and stuff like that, but stuff gets through.
[03:45:08.660 --> 03:45:10.740]   But if you flag it, that's the thing.
[03:45:10.740 --> 03:45:11.980]   It has moderation tools.
[03:45:11.980 --> 03:45:14.940]   You can immediately get rid of it.
[03:45:14.940 --> 03:45:21.220]   But it's a testament to how few groups are being moderated that these spammers are flourishing
[03:45:21.220 --> 03:45:23.060]   because it works.
[03:45:23.060 --> 03:45:25.060]   The president...
[03:45:25.060 --> 03:45:30.660]   You just laugh when I say that, don't you?
[03:45:30.660 --> 03:45:33.500]   The president, according to the New York Times, has three phones.
[03:45:33.500 --> 03:45:34.580]   All three of them iPhones.
[03:45:34.580 --> 03:45:38.260]   Two of them are carefully modified to be secure by the Secret Service.
[03:45:38.260 --> 03:45:40.620]   One, for instance, only has Wi-Fi.
[03:45:40.620 --> 03:45:45.900]   The third is a stock iPhone, which according to the Times, the president keeps because
[03:45:45.900 --> 03:45:47.980]   it has his contact list on it.
[03:45:47.980 --> 03:45:52.940]   That's the one he uses to make phone calls.
[03:45:52.940 --> 03:45:57.660]   According to the New York Times, it's the one the Chinese and Russians are hoping he'll
[03:45:57.660 --> 03:46:00.580]   use because they can listen in.
[03:46:00.580 --> 03:46:06.380]   So all of us have experienced doing tech support for older people.
[03:46:06.380 --> 03:46:08.180]   Yeah, that's what it's like.
[03:46:08.180 --> 03:46:10.940]   But it's my contacts are on this one.
[03:46:10.940 --> 03:46:12.940]   Yeah, and the CIA.
[03:46:12.940 --> 03:46:20.180]   The best part of this story to me, there's so much of it that's just horrifying and stupid
[03:46:20.180 --> 03:46:21.380]   and wrong.
[03:46:21.380 --> 03:46:29.340]   But the best part of it, I think, was the Chinese trade minister who said, "Yes, you should
[03:46:29.340 --> 03:46:31.340]   use a Huawei phone."
[03:46:31.340 --> 03:46:33.700]   It would be secure.
[03:46:33.700 --> 03:46:37.220]   That was an amazing troll on that one.
[03:46:37.220 --> 03:46:39.860]   I got to get big props to that one.
[03:46:39.860 --> 03:46:40.860]   That was...
[03:46:40.860 --> 03:46:44.140]   When it said that, I just imagined him putting it on speaker and holding it to his ear the
[03:46:44.140 --> 03:46:45.780]   way you see people in coffee shops.
[03:46:45.780 --> 03:46:46.780]   Yeah, yeah.
[03:46:46.780 --> 03:46:47.780]   Talk on the phone.
[03:46:47.780 --> 03:46:48.780]   Really loud.
[03:46:48.780 --> 03:46:49.780]   They're breaking to hear it.
[03:46:49.780 --> 03:46:50.780]   Yeah.
[03:46:50.780 --> 03:46:51.780]   They don't know there's an earpiece.
[03:46:51.780 --> 03:46:53.660]   It's like on FaceTime or something and it's like up to their ear.
[03:46:53.660 --> 03:46:54.660]   We do.
[03:46:54.660 --> 03:46:55.660]   Yeah, that's some serious shade.
[03:46:55.660 --> 03:46:59.300]   The Huawei thing was some big time shooting.
[03:46:59.300 --> 03:47:01.860]   And ZTE is going, "What about some love for us?"
[03:47:01.860 --> 03:47:02.860]   Really nothing?
[03:47:02.860 --> 03:47:08.180]   Actually, I talked with a PR person from Huawei just this week because I really liked the
[03:47:08.180 --> 03:47:12.780]   P20, but I said to her, "The first thing it does is say can we send everything you're
[03:47:12.780 --> 03:47:14.580]   doing back to the office?"
[03:47:14.580 --> 03:47:16.740]   She said, "Here's the good news.
[03:47:16.740 --> 03:47:19.820]   That's all sent to Germany, not to China."
[03:47:19.820 --> 03:47:20.820]   Why?
[03:47:20.820 --> 03:47:22.620]   So, there, I feel better.
[03:47:22.620 --> 03:47:23.620]   That actually is good.
[03:47:23.620 --> 03:47:28.580]   Actually, it's much if you're going to send to one country in the world, send it to Germany.
[03:47:28.580 --> 03:47:29.580]   Exactly.
[03:47:29.580 --> 03:47:35.860]   They started by talking about Macs, which first was very, as a Mac fan, was very gratifying.
[03:47:35.860 --> 03:47:39.460]   The Hallelujah, a little Mac love.
[03:47:39.460 --> 03:47:45.220]   They announced a new MacBook Air with frankly an anemic processor.
[03:47:45.220 --> 03:47:47.220]   Yeah, very poor spec.
[03:47:47.220 --> 03:47:49.820]   Very poorly spec and very inexpensive, $11.99.
[03:47:49.820 --> 03:47:51.580]   We were hoping it'll be Sub-1000s, not.
[03:47:51.580 --> 03:47:53.140]   They do have a Sub-1000 MacBook Air.
[03:47:53.140 --> 03:47:55.340]   That's the old one.
[03:47:55.340 --> 03:47:59.020]   Then they announced a Mac Mini, which with lots of ports we bought both.
[03:47:59.020 --> 03:48:02.740]   We're going to use that Mac Mini.
[03:48:02.740 --> 03:48:03.740]   Also more expensive.
[03:48:03.740 --> 03:48:08.100]   Well, a hundred bucks more than the base model last time, which was, "Let's face it four
[03:48:08.100 --> 03:48:09.500]   years ago, last time."
[03:48:09.500 --> 03:48:11.980]   I think the dollar was worthless.
[03:48:11.980 --> 03:48:21.260]   But nevertheless, then they go to the iPads and suddenly everything shiny, bright, new,
[03:48:21.260 --> 03:48:23.380]   better.
[03:48:23.380 --> 03:48:26.420]   I realized if you had started with the iPads and went to the Macs, it would have been
[03:48:26.420 --> 03:48:29.220]   like, "Oh, man, how depressing is this?"
[03:48:29.220 --> 03:48:34.380]   It's the essential message that pushing, which for me was, "You will buy an iPad, whether
[03:48:34.380 --> 03:48:35.380]   you like it or not.
[03:48:35.380 --> 03:48:37.660]   We're going to price and unspeccal systems to do it."
[03:48:37.660 --> 03:48:41.460]   It felt very much to me like the end of the line on the Apple II.
[03:48:41.460 --> 03:48:44.620]   You remember that probably like where they had a conference.
[03:48:44.620 --> 03:48:46.140]   Apple had a conference in San Francisco.
[03:48:46.140 --> 03:48:47.140]   Apple II Forever.
[03:48:47.140 --> 03:48:48.700]   They had a song.
[03:48:48.700 --> 03:48:50.860]   "Apple II Forever."
[03:48:50.860 --> 03:48:53.820]   And they killed it like six months later.
[03:48:53.820 --> 03:49:00.260]   It was very clear to me that this is basically the end of the line for Mac OS.
[03:49:00.260 --> 03:49:04.740]   They emphasized the, we're giving them what they want.
[03:49:04.740 --> 03:49:07.580]   People for some reason like our laptops, so we're...
[03:49:07.580 --> 03:49:08.580]   We don't get it.
[03:49:08.580 --> 03:49:09.580]   We don't know why.
[03:49:09.580 --> 03:49:13.140]   For some reason, they like Mac Mini, so here's another Mac Mini.
[03:49:13.140 --> 03:49:18.940]   But now let's talk about the future of computing, the iPad.
[03:49:18.940 --> 03:49:19.940]   It was really clear.
[03:49:19.940 --> 03:49:23.740]   The A12X Spionic, which is faster than the MacBook Pro.
[03:49:23.740 --> 03:49:27.020]   I predict that all the benchmarking is done.
[03:49:27.020 --> 03:49:30.260]   The fastest computer that Apple makes is going to be the new iPad.
[03:49:30.260 --> 03:49:32.180]   Yeah, I can quote that.
[03:49:32.180 --> 03:49:38.540]   The graphics too, like a guy I know that hasn't had an iPad since the second iPad, it was the
[03:49:38.540 --> 03:49:41.340]   graphics performance that he's like, "All right, I'm getting one."
[03:49:41.340 --> 03:49:42.340]   Yeah.
[03:49:42.340 --> 03:49:50.020]   The 8-core CPU, a 7-core GPU, they even have a machine learning core, dedicated machine
[03:49:50.020 --> 03:49:53.660]   learning core.
[03:49:53.660 --> 03:49:57.980]   It seems to me that all of the real innovation that's happening at Apple is happening in those
[03:49:57.980 --> 03:49:58.980]   chipsets.
[03:49:58.980 --> 03:50:03.340]   When I saw that the new Apple Pencil magnetically sticks to the iPad, "Ah, they're copying the
[03:50:03.340 --> 03:50:05.500]   Pixelbook," and then they're like, "Oh, and then it charges."
[03:50:05.500 --> 03:50:06.500]   They're like, "Really?"
[03:50:06.500 --> 03:50:07.500]   Yeah, that's a huge...
[03:50:07.500 --> 03:50:08.500]   Yeah, that's a huge...
[03:50:08.500 --> 03:50:11.940]   Pixelbook's Pencil or Pen or whatever you call it, has a quadruple-A battery.
[03:50:11.940 --> 03:50:12.940]   Did you know this existed?
[03:50:12.940 --> 03:50:15.300]   It didn't until I got one of them, yeah.
[03:50:15.300 --> 03:50:16.300]   Yeah, it's tiny.
[03:50:16.300 --> 03:50:17.300]   It's tiny.
[03:50:17.300 --> 03:50:18.300]   Yes, baby's something.
[03:50:18.300 --> 03:50:19.300]   Maybe something.
[03:50:19.300 --> 03:50:20.300]   Think of something.
[03:50:20.300 --> 03:50:21.300]   Oh, yeah.
[03:50:21.300 --> 03:50:22.300]   Come on.
[03:50:22.300 --> 03:50:23.300]   It's very small.
[03:50:23.300 --> 03:50:24.540]   I have standards.
[03:50:24.540 --> 03:50:27.540]   Not many, admittedly, but...
[03:50:27.540 --> 03:50:32.060]   So it wasn't just me, because I love macOS and I want to keep using macOS.
[03:50:32.060 --> 03:50:36.900]   Apple also had AutoCAD on stage and they said very clearly this has the desktop AutoCAD
[03:50:36.900 --> 03:50:42.620]   engine in it at Adobe's conference a couple of weeks ago.
[03:50:42.620 --> 03:50:48.780]   They announced Adobe Photoshop, basically all the features of desktop on the iPad.
[03:50:48.780 --> 03:50:50.300]   This comes back to what Mike was saying earlier.
[03:50:50.300 --> 03:50:54.300]   They want iOS because it's a nice little wall garden that they control absolutely and
[03:50:54.300 --> 03:50:59.540]   completely and can monetize that, whereas macOS is messy, other people playing it.
[03:50:59.540 --> 03:51:00.540]   And for all the tools...
[03:51:00.540 --> 03:51:03.060]   Well, furthermore, they're also holding the Intel, right?
[03:51:03.060 --> 03:51:04.060]   Yeah.
[03:51:04.060 --> 03:51:08.580]   And it's that Apple's Macintosh line has been held back as an Intel is stymied.
[03:51:08.580 --> 03:51:10.380]   It can't seem to do a 10-animeter chip.
[03:51:10.380 --> 03:51:11.860]   Apple is very clear.
[03:51:11.860 --> 03:51:15.540]   We've got the first 7-animeter chip in production, the A12X.
[03:51:15.540 --> 03:51:16.540]   Yeah.
[03:51:16.540 --> 03:51:17.540]   Yeah.
[03:51:17.540 --> 03:51:21.020]   I feel like Apple would love to get out from under Intel, even for the Macs.
[03:51:21.020 --> 03:51:23.580]   Well, Intel's got massive problems at the moment.
[03:51:23.580 --> 03:51:26.220]   They work on 10-animeter as completely stalled.
[03:51:26.220 --> 03:51:29.460]   Cobalt, which we were trying out as a method of getting around.
[03:51:29.460 --> 03:51:30.460]   It hasn't worked.
[03:51:30.460 --> 03:51:34.700]   At the same time, other chip companies are eating their breakfast and they've got major
[03:51:34.700 --> 03:51:39.340]   supply problems on the server side because they've got 99% of the market and people are
[03:51:39.340 --> 03:51:42.340]   still buying those chips and they can't produce them fast enough.
[03:51:42.340 --> 03:51:47.620]   So I mean, and plus they've got lost their CEO in rather unusual circumstances.
[03:51:47.620 --> 03:51:50.780]   So yeah, Intel just seems to be flailing at the moment.
[03:51:50.780 --> 03:51:52.780]   I don't blame Apple for trying to dub them.
[03:51:52.780 --> 03:51:53.780]   Yeah.
[03:51:53.780 --> 03:51:58.580]   The other thing is that they're grappling with a new generation of people who, like you
[03:51:58.580 --> 03:52:00.700]   were saying earlier, only want to use their phones.
[03:52:00.700 --> 03:52:02.060]   They grew up with touch devices.
[03:52:02.060 --> 03:52:03.380]   They only remember touch devices.
[03:52:03.380 --> 03:52:06.860]   There are lots of kids these days who never used a mouse before.
[03:52:06.860 --> 03:52:11.020]   They think it's kind of a weird contraption from the Victorian era or something like that.
[03:52:11.020 --> 03:52:13.140]   And let's face it, this is how change really happens.
[03:52:13.140 --> 03:52:17.780]   As old people die and young people are born and they, you know, they, and so they basically
[03:52:17.780 --> 03:52:22.660]   they're thinking, okay, in 20 years or 10 years, how are we going to get today's iPhone,
[03:52:22.660 --> 03:52:28.180]   you know, 20-year-old iPhone users to buy a really expensive second computer for their
[03:52:28.180 --> 03:52:29.180]   phone.
[03:52:29.180 --> 03:52:31.180]   And it's got to be something that's going to be iPhone.
[03:52:31.180 --> 03:52:33.180]   Like otherwise they're not going to buy it.
[03:52:33.180 --> 03:52:36.140]   Are people going to do that or they're just going to have the phone?
[03:52:36.140 --> 03:52:41.900]   I think more and more we're seeing things like Microsoft's continuity, Intel's, I can't
[03:52:41.900 --> 03:52:46.380]   remember what they call it, Samsung's little device where the phone becomes a desktop.
[03:52:46.380 --> 03:52:47.380]   Right.
[03:52:47.380 --> 03:52:48.380]   Yeah.
[03:52:48.380 --> 03:52:49.380]   Which I think is a wonderful model.
[03:52:49.380 --> 03:52:50.380]   Yeah, which is a huge photo.
[03:52:50.380 --> 03:52:55.780]   HP tried to do with Windows Mobile but unfortunately died on its backside because mobile was terrible.
[03:52:55.780 --> 03:52:59.740]   Augmented reality is going to be a big thing that people use while they're sitting at
[03:52:59.740 --> 03:53:00.740]   a desk, I think.
[03:53:00.740 --> 03:53:05.260]   So the screen will be virtual, it'll be imaginary, be projected on your eyeballs instead of sitting
[03:53:05.260 --> 03:53:06.260]   in front of it.
[03:53:06.260 --> 03:53:07.260]   That's part of it.
[03:53:07.260 --> 03:53:11.700]   I do think, I do think there's a future for giant iPad-like devices and for the Surface
[03:53:11.700 --> 03:53:13.340]   Studio will be an iPad-side.
[03:53:13.340 --> 03:53:14.340]   Exactly.
[03:53:14.340 --> 03:53:19.460]   And I think also there'll be things like Office Windows will be TVs like that one.
[03:53:19.460 --> 03:53:24.340]   Giant screens that can be, you know, so I think there are going to be screens everywhere
[03:53:24.340 --> 03:53:28.780]   on your face, on the wall, on the desktop surface will be a screen.
[03:53:28.780 --> 03:53:32.180]   But you know, this whole touch, it's all going to be iOS-like.
[03:53:32.180 --> 03:53:35.620]   It's all going to be touch gestures and that sort of thing.
[03:53:35.620 --> 03:53:37.620]   So, but Apple has a couple of issues.
[03:53:37.620 --> 03:53:38.620]   Go ahead, Brian.
[03:53:38.620 --> 03:53:43.100]   Yeah, I was going to say that's fine and great but there could be a fairly long interim
[03:53:43.100 --> 03:53:46.220]   here before that reality comes.
[03:53:46.220 --> 03:53:51.540]   So the question is, and this has been debated, I'm not the first one to bring this up, but
[03:53:51.540 --> 03:53:53.780]   at what point do they risk?
[03:53:53.780 --> 03:54:00.020]   Apple's always been the company of creatives and high-end professionals.
[03:54:00.020 --> 03:54:05.700]   It's the thing that kept Apple alive as a company in the late 90s.
[03:54:05.700 --> 03:54:10.940]   What's the tipping point where those people have to go away and go to someone else?
[03:54:10.940 --> 03:54:14.580]   And also, what is the tipping point where Apple is fine with that?
[03:54:14.580 --> 03:54:17.380]   Well, Brian, wasn't at the point of the event, the point of the day.
[03:54:17.380 --> 03:54:18.380]   That's what I'm saying.
[03:54:18.380 --> 03:54:19.380]   Let's see what's in the making.
[03:54:19.380 --> 03:54:22.580]   How creativity is a permanent academy of music in the opera house?
[03:54:22.580 --> 03:54:25.980]   It was you create with an iPad.
[03:54:25.980 --> 03:54:28.780]   Wow, what a year this has been.
[03:54:28.780 --> 03:54:33.700]   I am so thrilled that you decided to watch our holiday rap special.
[03:54:33.700 --> 03:54:37.700]   I know many of you have seen all these segments, but it's fun to get them all together.
[03:54:37.700 --> 03:54:42.020]   Thanks to Carson Bonny, our producer and our crack editors who, you know, stitched this
[03:54:42.020 --> 03:54:45.660]   all together and I'm really glad they did because it gave them and me a chance to take
[03:54:45.660 --> 03:54:46.820]   this week off.
[03:54:46.820 --> 03:54:51.620]   I hope you're enjoying your holiday week celebrating it with family and friends.
[03:54:51.620 --> 03:54:55.820]   I also hope you have a wonderful new year, 2019.
[03:54:55.820 --> 03:55:01.380]   We will be back for the first twit of 2019, which is what January 6th.
[03:55:01.380 --> 03:55:02.380]   Is that right?
[03:55:02.380 --> 03:55:03.380]   January 5th?
[03:55:03.380 --> 03:55:04.380]   7th?
[03:55:04.380 --> 03:55:09.780]   Yeah, 8th, January 7th, 6th.
[03:55:09.780 --> 03:55:10.980]   One of those numbers is right.
[03:55:10.980 --> 03:55:12.620]   January 6th will be back.
[03:55:12.620 --> 03:55:17.900]   As always, Sunday afternoon at 3 p.m. Pacific, 6 p.m. Eastern, 2300 UTC for this week in
[03:55:17.900 --> 03:55:20.060]   TechArt Tech News Roundtable.
[03:55:20.060 --> 03:55:23.860]   And one of the things we're going to keep doing, I hope you like it, all of other shows have
[03:55:23.860 --> 03:55:25.380]   a regular cast.
[03:55:25.380 --> 03:55:28.940]   But for some reason, I think historic reasons.
[03:55:28.940 --> 03:55:32.220]   Twit is a rotating cast of people that I think we've all come to love.
[03:55:32.220 --> 03:55:37.020]   I like that chance to get new voices, a variety of voices on talking about the weeks.
[03:55:37.020 --> 03:55:40.300]   Tech News, we're going to keep doing that in 2019.
[03:55:40.300 --> 03:55:42.900]   I hope I pray you will keep listening.
[03:55:42.900 --> 03:55:44.180]   In fact, do me a favor.
[03:55:44.180 --> 03:55:48.180]   Subscribe at twit.tv or in your favorite podcast application.
[03:55:48.180 --> 03:55:51.180]   And that way you won't miss another episode of Twit.
[03:55:51.180 --> 03:55:53.300]   I've got to raise my glass.
[03:55:53.300 --> 03:55:58.460]   Yes, I've been drinking eggnog all the show long and I'm getting a little tipsy.
[03:55:58.460 --> 03:56:01.500]   I've got to raise my glass to all of you.
[03:56:01.500 --> 03:56:06.940]   Thank you so much for making Twit a part of your lives, for giving all of us a chance
[03:56:06.940 --> 03:56:08.820]   to make these shows.
[03:56:08.820 --> 03:56:12.540]   We love doing what we do, but it wouldn't make any sense doing it unless you watched.
[03:56:12.540 --> 03:56:14.940]   So I really appreciate it.
[03:56:14.940 --> 03:56:21.900]   Here's a cheers to you and yours and to a happy and peaceful 2019.
[03:56:21.900 --> 03:56:25.940]   And now, without further ado, a word from Santa.
[03:56:25.940 --> 03:56:28.900]   Good night, everybody.
[03:56:28.900 --> 03:56:30.300]   Another twit.
[03:56:30.300 --> 03:56:32.700]   It's literally in the can.
[03:56:32.700 --> 03:56:42.700]   I'm not a twit.

