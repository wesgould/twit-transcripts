
[00:00:00.000 --> 00:00:02.880]   It's time for Twit this week in Tech.
[00:00:02.880 --> 00:00:07.000]   With a big week at the Supreme Court, we're really glad Mike Masnick from Tech Dirt is
[00:00:07.000 --> 00:00:08.000]   here.
[00:00:08.000 --> 00:00:10.360]   Owen Thomas joins us from the San Francisco Examiner.
[00:00:10.360 --> 00:00:13.120]   And from Mac Creek Weekly, Alex Lindsay is in the house.
[00:00:13.120 --> 00:00:16.840]   We'll talk about Section 230, the oral arguments.
[00:00:16.840 --> 00:00:23.240]   Why the NSA probably should be governed a little bit more by the Supreme Court.
[00:00:23.240 --> 00:00:26.320]   A potential Apple Watch ban.
[00:00:26.320 --> 00:00:34.680]   We got the hacker who hacked all those Twitter accounts and then an iPhone that sold for $63,000
[00:00:34.680 --> 00:00:39.600]   all that and more coming up next on Twit.
[00:00:39.600 --> 00:00:45.200]   Podcasts you love from people you trust.
[00:00:45.200 --> 00:00:52.640]   This is Twit.
[00:00:52.640 --> 00:01:01.720]   This week in Tech, episode 916 recorded Sunday, February 26th, 2023.
[00:01:01.720 --> 00:01:04.560]   Fetch happens.
[00:01:04.560 --> 00:01:07.200]   This week in Tech is brought to you by Noom.
[00:01:07.200 --> 00:01:12.000]   Stay focused on what's important to you with Noom weight's psychology-based approach.
[00:01:12.000 --> 00:01:16.760]   And check out Noom's first ever book, The Noom Mindset, a deep dive into the psychology
[00:01:16.760 --> 00:01:18.160]   of behavior change.
[00:01:18.160 --> 00:01:20.080]   Available to buy now wherever books are sold.
[00:01:20.080 --> 00:01:25.360]   And of course sign up for your trial at Noom.com/Twit.
[00:01:25.360 --> 00:01:31.440]   And by Worldwide Technology with an innovative culture, thousands of IT engineers, application
[00:01:31.440 --> 00:01:36.040]   developers, unmatched labs and integration centers for testing and deploying technology
[00:01:36.040 --> 00:01:37.360]   at scale.
[00:01:37.360 --> 00:01:42.080]   WWT helps customers bridge the gap between strategy and execution.
[00:01:42.080 --> 00:01:47.920]   To learn more about WWT, visit www.wt.com/Twit.
[00:01:47.920 --> 00:01:50.480]   And by www.stamps.com.
[00:01:50.480 --> 00:01:53.120]   Set your business up for success when you start today.
[00:01:53.120 --> 00:01:57.360]   Sign up with the promo code TWIT for a special offer that includes a four-week trial, free
[00:01:57.360 --> 00:01:59.680]   postage and a free digital scale.
[00:01:59.680 --> 00:02:04.120]   Just go to www.stamps.com, click the microphone at the top of the page and enter the code
[00:02:04.120 --> 00:02:05.640]   TWIT.
[00:02:05.640 --> 00:02:08.400]   And by ACI Learning.
[00:02:08.400 --> 00:02:11.520]   If you love ITPro, you'll love ACI Learning.
[00:02:11.520 --> 00:02:17.320]   ACI Learning offers fully customizable training for your team in formats for all kinds of
[00:02:17.320 --> 00:02:21.000]   learners across audit, cybersecurity and IT.
[00:02:21.000 --> 00:02:26.400]   From entry-level training to putting people on the moon, ACI Learning has you covered.
[00:02:26.400 --> 00:02:32.680]   Visit go.acilurning.com/Twit to learn more.
[00:02:32.680 --> 00:02:39.600]   It's time for TWIT this week at Tech, the show we cover the weeks.
[00:02:39.600 --> 00:02:42.040]   Tech news, I have assembled.
[00:02:42.040 --> 00:02:44.040]   I have assembled, actually, Jason.
[00:02:44.040 --> 00:02:45.800]   Jason Howell has assembled.
[00:02:45.800 --> 00:02:47.760]   The best panel of journalists.
[00:02:47.760 --> 00:02:48.760]   Money could buy.
[00:02:48.760 --> 00:02:51.520]   Well, since we don't pay them, it's...
[00:02:51.520 --> 00:02:53.840]   Anyway, Mike Masnick is here.
[00:02:53.840 --> 00:02:54.840]   Techter.com.
[00:02:54.840 --> 00:02:56.200]   Now that's pretty good.
[00:02:56.200 --> 00:02:57.200]   Hey, Mike.
[00:02:57.200 --> 00:02:58.200]   Hello.
[00:02:58.200 --> 00:02:59.200]   Good to see you.
[00:02:59.200 --> 00:03:02.880]   We had your colleague, Kathy, on on Twig to talk about the Supreme Court.
[00:03:02.880 --> 00:03:04.120]   But you had some great takes, too.
[00:03:04.120 --> 00:03:10.720]   I want to get your thoughts about Section 230, Gonzalez versus Google.
[00:03:10.720 --> 00:03:16.120]   We also have a wonderful friend, Owen Thomas, columnist for the Examiner now.
[00:03:16.120 --> 00:03:18.800]   He's the Herb Kane of Tech in San Francisco.
[00:03:18.800 --> 00:03:19.800]   Oh, stop.
[00:03:19.800 --> 00:03:21.280]   Go on you.
[00:03:21.280 --> 00:03:27.840]   Also, his personal blog, I hate to publish it to promote it because you haven't published
[00:03:27.840 --> 00:03:29.160]   anything there in seven years.
[00:03:29.160 --> 00:03:30.840]   But someday he will.
[00:03:30.840 --> 00:03:32.080]   ditherrody.com.
[00:03:32.080 --> 00:03:33.720]   I keep threatening to.
[00:03:33.720 --> 00:03:34.720]   I keep threatening.
[00:03:34.720 --> 00:03:35.720]   You're busy now.
[00:03:35.720 --> 00:03:38.080]   You're writing for a real place.
[00:03:38.080 --> 00:03:39.080]   So I understand.
[00:03:39.080 --> 00:03:40.960]   Would Herb Kane have a daily blog?
[00:03:40.960 --> 00:03:41.960]   Probably, actually.
[00:03:41.960 --> 00:03:43.480]   Actually, I wonder what Herb would do.
[00:03:43.480 --> 00:03:45.160]   That's a really interesting question.
[00:03:45.160 --> 00:03:46.160]   Yeah.
[00:03:46.160 --> 00:03:50.360]   He was the legendary San Francisco Chronicle columnist.
[00:03:50.360 --> 00:03:52.280]   He was the guy.
[00:03:52.280 --> 00:03:55.680]   This is back when really newspapers are just about display ads.
[00:03:55.680 --> 00:03:59.240]   They were the thing in between the display ads and he was the guy his column was right
[00:03:59.240 --> 00:04:02.280]   next to the Macy's ad every single day.
[00:04:02.280 --> 00:04:05.480]   And the whole point was to get you look at the Macy's ad, I think.
[00:04:05.480 --> 00:04:07.680]   Also with his Alex Lindsey.
[00:04:07.680 --> 00:04:09.560]   Regular, a legbreak weekly.
[00:04:09.560 --> 00:04:12.120]   We like to bring in people from our other shows from time to time.
[00:04:12.120 --> 00:04:16.440]   090.media is his business, his day job.
[00:04:16.440 --> 00:04:19.320]   After hours, it's office hours dot global.
[00:04:19.320 --> 00:04:20.320]   Hi, Alex.
[00:04:20.320 --> 00:04:21.320]   Hi.
[00:04:21.320 --> 00:04:22.320]   It's going to be here.
[00:04:22.320 --> 00:04:23.520]   It's nice to see you on a Sunday.
[00:04:23.520 --> 00:04:25.080]   I know, I know.
[00:04:25.080 --> 00:04:27.560]   It's usually hard, but it's great to be here.
[00:04:27.560 --> 00:04:31.840]   Let me start with the bad news from the Supreme Court.
[00:04:31.840 --> 00:04:39.640]   They decided not to take the case, Wikipedia, the parent company, Wikipedia had sued the
[00:04:39.640 --> 00:04:43.040]   NSA over their upstream surveillance.
[00:04:43.040 --> 00:04:47.520]   As you probably know, I mean, this was a lawsuit from 2015.
[00:04:47.520 --> 00:04:51.840]   Thanks to Edward Snowden and other revelations, we've realized that the NSA wasn't getting
[00:04:51.840 --> 00:04:54.440]   into your Google and Facebook and Twitter account.
[00:04:54.440 --> 00:04:58.920]   They were just doing it upstream as it as it came out of those servers into the real
[00:04:58.920 --> 00:05:00.280]   world.
[00:05:00.280 --> 00:05:05.480]   And they were doing it basically for everyone.
[00:05:05.480 --> 00:05:07.080]   It was the suspicionless.
[00:05:07.080 --> 00:05:12.480]   That's the word they use collection and searching of internet traffic on data transmission lines
[00:05:12.480 --> 00:05:14.280]   flowing into and out of the United States.
[00:05:14.280 --> 00:05:18.640]   They said, well, it's only mostly it was just foreign targets, except for anybody the
[00:05:18.640 --> 00:05:20.440]   foreign target was talking to in the US.
[00:05:20.440 --> 00:05:22.600]   You were there too.
[00:05:22.600 --> 00:05:28.260]   But the justices said, no, it's protected because release of some information would
[00:05:28.260 --> 00:05:30.000]   damage US national security.
[00:05:30.000 --> 00:05:32.480]   That seems sensible actually, Mike.
[00:05:32.480 --> 00:05:36.280]   I'm going to challenge that.
[00:05:36.280 --> 00:05:41.680]   So it's actually there are a bunch of issues with the program and the setup and it goes
[00:05:41.680 --> 00:05:44.080]   beyond kind of what you just described a little bit.
[00:05:44.080 --> 00:05:51.640]   So they were tapping all communications basically as they left or came into the United States,
[00:05:51.640 --> 00:05:57.720]   which you said, and they were doing that directly through the backbone providers, AT&T, Verizon,
[00:05:57.720 --> 00:05:58.960]   whoever else.
[00:05:58.960 --> 00:06:03.600]   The issue was not just foreign nationals communicating with Americans.
[00:06:03.600 --> 00:06:08.920]   It was that they were basically scanning through everything and it was too from and then it
[00:06:08.920 --> 00:06:13.720]   also included the much more scary about communication.
[00:06:13.720 --> 00:06:20.320]   So if you mentioned someone in an email that was on a target list or if you mentioned a
[00:06:20.320 --> 00:06:27.160]   phone number or an email address or possibly some other keyword that was on a list, then
[00:06:27.160 --> 00:06:28.520]   it would get collected.
[00:06:28.520 --> 00:06:30.120]   And again, there's no warrant here.
[00:06:30.120 --> 00:06:34.480]   So you could be as an American having your communications collected.
[00:06:34.480 --> 00:06:39.160]   And then once it was collected, it would go into a larger database that then the FBI
[00:06:39.160 --> 00:06:40.600]   could search through without a warrant.
[00:06:40.600 --> 00:06:45.480]   And so there was all sorts of information that is not the type of thing that would normally
[00:06:45.480 --> 00:06:50.200]   be collected and certainly would raise Fourth Amendment issues in general.
[00:06:50.200 --> 00:06:54.960]   And yet because of the sort of funky way in which it was done and set up and that it
[00:06:54.960 --> 00:07:01.920]   was collected outside the US through the backbone and then sort of backed into this database
[00:07:01.920 --> 00:07:07.600]   that the FBI could search, it sort of had this one weird trick to get around the Fourth
[00:07:07.600 --> 00:07:08.680]   Amendment.
[00:07:08.680 --> 00:07:12.160]   And that's what the lawsuit was really trying to challenge.
[00:07:12.160 --> 00:07:16.800]   And the court basically said for national security reasons, we're not even going to look
[00:07:16.800 --> 00:07:17.800]   into that.
[00:07:17.800 --> 00:07:22.560]   And that seems pretty questionable because if you can just get around the entire Fourth
[00:07:22.560 --> 00:07:26.960]   Amendment by saying national security interests and maybe some other sort of fancy footwork
[00:07:26.960 --> 00:07:31.080]   to say, well, this is happening outside the US, not inside the US, we're all good.
[00:07:31.080 --> 00:07:41.280]   The fact that it is collecting up tons and tons of American information without a warrant
[00:07:41.280 --> 00:07:48.000]   and then allowing it to be searched by the FBI does seem to me like a pretty big deal.
[00:07:48.000 --> 00:07:54.080]   They said this is permitted by the FISA Act, our good friend the FISA Act.
[00:07:54.080 --> 00:07:58.080]   Yeah, so that I mean, that's the other thing is that read the FISA Act and tell me where
[00:07:58.080 --> 00:07:59.080]   that's permitted.
[00:07:59.080 --> 00:08:06.760]   Yeah, because it's an interesting interpretation of the FISA Act that the NSA has said allows
[00:08:06.760 --> 00:08:07.760]   them to do that.
[00:08:07.760 --> 00:08:09.200]   But that's never really been reviewed by a court.
[00:08:09.200 --> 00:08:15.760]   And that was the point of this entire case by Wikipedia represented by the ACLU was the
[00:08:15.760 --> 00:08:17.920]   idea that, you know, let's see that.
[00:08:17.920 --> 00:08:18.920]   Let's see the language.
[00:08:18.920 --> 00:08:25.160]   How do you justify from the language of the FISA Amendments Act that this this would be allowed?
[00:08:25.160 --> 00:08:28.120]   And the courts basically said, we're not even going to review that because the government
[00:08:28.120 --> 00:08:29.680]   is saying national security secrets.
[00:08:29.680 --> 00:08:33.960]   It was dismissed by the Fourth Circuit a couple of years ago, appealed to the Supreme Court
[00:08:33.960 --> 00:08:36.920]   with who declined to give cert.
[00:08:36.920 --> 00:08:45.040]   The ACLU lawyers argued that they should listen to the case saying, quote, although this mass
[00:08:45.040 --> 00:08:49.920]   surveillance of Americans private communications raises grave constitutional questions.
[00:08:49.920 --> 00:08:54.640]   This lawfulness has yet to be considered by any ordinary court, civil or criminal in
[00:08:54.640 --> 00:08:56.840]   more than 20 years of operation.
[00:08:56.840 --> 00:09:01.280]   Yeah, it feels like that's exactly what the Supreme Court should be ruling on.
[00:09:01.280 --> 00:09:02.280]   Is this constitutional?
[00:09:02.280 --> 00:09:03.280]   Yeah.
[00:09:03.280 --> 00:09:08.120]   Is there no mechanism for them to do that kind of secretly?
[00:09:08.120 --> 00:09:10.720]   I mean, as the guys Congress does.
[00:09:10.720 --> 00:09:13.920]   No, I think that's the problem is the Supreme Court doesn't really have a mechanism to do
[00:09:13.920 --> 00:09:16.920]   that, to do it, to manage top secret.
[00:09:16.920 --> 00:09:19.160]   You're not set up for that.
[00:09:19.160 --> 00:09:20.160]   Processes.
[00:09:20.160 --> 00:09:23.560]   And so that's all a matter of public record, I believe.
[00:09:23.560 --> 00:09:26.440]   And so that becomes part of the pushback.
[00:09:26.440 --> 00:09:33.160]   And in other shows, I'm very pollinated about the fact that phones and everything else should
[00:09:33.160 --> 00:09:34.160]   be protected.
[00:09:34.160 --> 00:09:37.960]   Our phone, we shouldn't be just letting the, if we haven't locked off, we don't want
[00:09:37.960 --> 00:09:38.960]   to hand it off.
[00:09:38.960 --> 00:09:40.360]   It's something that's very personal to us.
[00:09:40.360 --> 00:09:47.680]   I will say that I work all over the world and things are pretty safe here.
[00:09:47.680 --> 00:09:52.520]   We just want to remember that as we start to unravel these, we start to unravel these.
[00:09:52.520 --> 00:09:56.520]   A lot of the reasons they're safe is because of a lot of these information gathering.
[00:09:56.520 --> 00:10:00.880]   When you understand, tactically, how they work, they become, they're very, very, very
[00:10:00.880 --> 00:10:05.560]   powerful in a way that we don't have, I work in countries where everybody has a fence around
[00:10:05.560 --> 00:10:08.560]   their house and broken glass and barbed wire across the top.
[00:10:08.560 --> 00:10:09.560]   And that's the other side.
[00:10:09.560 --> 00:10:13.560]   I don't want to be a fear monger, but there's a lot of people gunning for us.
[00:10:13.560 --> 00:10:17.560]   We just want to make sure that we're clear that asymmetrical threats are a real thing.
[00:10:17.560 --> 00:10:22.120]   And most of them get buried pretty quickly because of these information gathering tools.
[00:10:22.120 --> 00:10:23.120]   And so-
[00:10:23.120 --> 00:10:27.040]   Does it come down, Alex, to whether you trust the NSA with that data, you know, they built,
[00:10:27.040 --> 00:10:28.440]   they call it BumbleHive.
[00:10:28.440 --> 00:10:31.960]   They built a data center famously in Utah.
[00:10:31.960 --> 00:10:33.640]   They can hold exabytes of data.
[00:10:33.640 --> 00:10:37.080]   And the thinking is that they're just storing all traffic.
[00:10:37.080 --> 00:10:39.680]   I don't think- In the BumbleHive.
[00:10:39.680 --> 00:10:44.280]   Yeah, I don't, I mean, I think that there's obviously some major issues that could occur.
[00:10:44.280 --> 00:10:46.400]   I mean, and I think that trust is a big piece of it.
[00:10:46.400 --> 00:10:50.640]   And I think that, you know, I don't know if they've always been trustworthy in that area.
[00:10:50.640 --> 00:10:56.000]   You got to love the fact that the BumbleHive has a web page.
[00:10:56.000 --> 00:11:01.800]   Here it is, domestic surveillance directorate, defending our nation, securing the citizens.
[00:11:01.800 --> 00:11:06.160]   And right in front of BumbleHive, there's a big sign, you know, kind of sign you'd see
[00:11:06.160 --> 00:11:07.680]   in front of a church.
[00:11:07.680 --> 00:11:09.680]   Welcome to the Utah data center.
[00:11:09.680 --> 00:11:12.920]   And then they have those little temporary letters they put up where you're slogan of
[00:11:12.920 --> 00:11:14.400]   the week.
[00:11:14.400 --> 00:11:18.720]   It says, if you have nothing to hide, you have nothing to fear.
[00:11:18.720 --> 00:11:19.720]   Holy mackerel.
[00:11:19.720 --> 00:11:22.760]   It's not the good model, I don't think I put that there.
[00:11:22.760 --> 00:11:23.760]   Yeah, yeah.
[00:11:23.760 --> 00:11:27.200]   So the- Oh, if that's the exact motto the NSA would like to pursue.
[00:11:27.200 --> 00:11:32.400]   But that tells you something- I feel like Eric Schmidt said something to that effect.
[00:11:32.400 --> 00:11:36.720]   And, you know, the irony there is that Eric Schmidt with his pants.
[00:11:36.720 --> 00:11:37.720]   His pants.
[00:11:37.720 --> 00:11:38.720]   His pants.
[00:11:38.720 --> 00:11:39.720]   And the extramarigal relationships.
[00:11:39.720 --> 00:11:40.720]   And his-
[00:11:40.720 --> 00:11:42.640]   We closed the door on the bathroom.
[00:11:42.640 --> 00:11:46.560]   I got nothing to hide, but it's not a your business.
[00:11:46.560 --> 00:11:51.720]   Yeah, and I do think that we have to be very careful about this, but we also have to be
[00:11:51.720 --> 00:11:54.960]   very careful of, you know, they've, you know, I think that probably the Supreme Court looked
[00:11:54.960 --> 00:11:56.520]   at it and said, well, it is outside the country.
[00:11:56.520 --> 00:11:59.640]   And the NSA, of course, was built a long time ago.
[00:11:59.640 --> 00:12:02.920]   And it is specifically designed to do what it's doing.
[00:12:02.920 --> 00:12:06.600]   Like it is not like we built something that kinder decided to move into this.
[00:12:06.600 --> 00:12:09.760]   You know, Echalin was built to manage to manage traffic.
[00:12:09.760 --> 00:12:10.760]   For a long time.
[00:12:10.760 --> 00:12:13.080]   But for a long time they denied the existence of Echalin.
[00:12:13.080 --> 00:12:14.080]   We only know about that.
[00:12:14.080 --> 00:12:14.520]   But Fice- But Fice-
[00:12:14.520 --> 00:12:16.600]   G-C-A-Q kind of admitted it.
[00:12:16.600 --> 00:12:17.600]   Whoops.
[00:12:17.600 --> 00:12:19.560]   Yeah, so Fice- the Fice- thing is new.
[00:12:19.560 --> 00:12:20.560]   It's 20 years old.
[00:12:20.560 --> 00:12:22.280]   Echalin is a lot older than that.
[00:12:22.280 --> 00:12:25.480]   You know, and so- This was- This was that keyword grabbing thing Mike was talking about
[00:12:25.480 --> 00:12:27.080]   where it just looks at all traffic.
[00:12:27.080 --> 00:12:30.800]   And say the word bomb four times and beetle juice appears.
[00:12:30.800 --> 00:12:35.560]   Well, it's more of- So when- The big thing about gathering all that data into the past
[00:12:35.560 --> 00:12:38.120]   is what it allows you to do is go forward and backward.
[00:12:38.120 --> 00:12:42.600]   So basically what happens is someone does something bad and I can just go backwards
[00:12:42.600 --> 00:12:46.000]   in time and look at who they were talking to in the past.
[00:12:46.000 --> 00:12:50.000]   So like let's say someone- Someone does a terrorist act.
[00:12:50.000 --> 00:12:53.360]   I can go back to before they were a terrorist and see who they were talking to.
[00:12:53.360 --> 00:12:54.360]   Right.
[00:12:54.360 --> 00:12:55.360]   Oh, I see the value of it.
[00:12:55.360 --> 00:12:56.360]   I don't understand the value.
[00:12:56.360 --> 00:13:04.440]   But the thing is, is that that is devastating for a terrorist organization, for us to be
[00:13:04.440 --> 00:13:09.400]   able to find you before you even- Before someone told you to start having trade craft, before
[00:13:09.400 --> 00:13:13.520]   you were trained in trade craft, you are still- We know who you were around and then
[00:13:13.520 --> 00:13:17.280]   you go forward and what happens is you go backward, you find those people and you go
[00:13:17.280 --> 00:13:20.160]   forward and you find everyone that they're connected to.
[00:13:20.160 --> 00:13:26.640]   And so it makes every terrorist act very, very difficult, very hard to avoid what happens
[00:13:26.640 --> 00:13:27.640]   next.
[00:13:27.640 --> 00:13:30.400]   And so I think that they- Yes, it is.
[00:13:30.400 --> 00:13:31.720]   It is a fissionable material.
[00:13:31.720 --> 00:13:35.040]   It is something that is pretty dangerous in that area.
[00:13:35.040 --> 00:13:40.080]   And it is also why people like me, when I travel all over the world, if I see someone
[00:13:40.080 --> 00:13:44.840]   that is remotely, like I have to admit, if they are remotely interesting, like they say
[00:13:44.840 --> 00:13:49.160]   things that are a little like on the edge of that, I don't talk to them anymore.
[00:13:49.160 --> 00:13:57.360]   I don't want to be- How about this also from the NSA's very interesting website.
[00:13:57.360 --> 00:14:02.120]   Our ultimate target, 256-bit AES.
[00:14:02.120 --> 00:14:03.760]   They have a supercomputer.
[00:14:03.760 --> 00:14:05.440]   This is, by the way, this is old data.
[00:14:05.440 --> 00:14:09.200]   They built a supercomputer in like 2004, a cray.
[00:14:09.200 --> 00:14:15.040]   I'm sure they have much better technology, but they want to crack AES.
[00:14:15.040 --> 00:14:19.280]   So that is the defense, right?
[00:14:19.280 --> 00:14:20.960]   Hey, we need to do this.
[00:14:20.960 --> 00:14:22.440]   You got to trust the NSA.
[00:14:22.440 --> 00:14:25.440]   We got to protect you.
[00:14:25.440 --> 00:14:31.520]   Yeah, but there are things like the Constitution and the Fourth Amendment and the laws that
[00:14:31.520 --> 00:14:36.400]   Congress has written that authorize what the NSA can and cannot do within the limits of
[00:14:36.400 --> 00:14:37.400]   the Fourth Amendment.
[00:14:37.400 --> 00:14:41.200]   And the issue here was not so much whether or not they're protecting us, but whether
[00:14:41.200 --> 00:14:44.480]   or not this is legal and whether or not this is constitutional.
[00:14:44.480 --> 00:14:46.480]   And that's something that the courts are supposed to decide.
[00:14:46.480 --> 00:14:47.480]   I think it's reasonable.
[00:14:47.480 --> 00:14:48.640]   That's what the Supreme Court does, right?
[00:14:48.640 --> 00:14:51.840]   Here's more fun quotes from Oak Ridge.
[00:14:51.840 --> 00:14:56.080]   Our classified NSA Oak Ridge facility made a stunning breakthrough that's leading us on
[00:14:56.080 --> 00:14:59.920]   a path toward building the first exaflop machine by 2018.
[00:14:59.920 --> 00:15:01.800]   Again, this is a little dated.
[00:15:01.800 --> 00:15:07.680]   Since the capability to break the AES, 256-bit encryption key within an actionable time period
[00:15:07.680 --> 00:15:14.080]   may still be decades away, our Utah facility is sized to store all encrypted and thereby
[00:15:14.080 --> 00:15:18.480]   suspicious data for safekeeping.
[00:15:18.480 --> 00:15:22.640]   So if you're sending any, if you're using signal, it's being stored there.
[00:15:22.640 --> 00:15:24.720]   So all encrypted data is suspicious.
[00:15:24.720 --> 00:15:26.280]   That's an interesting formula.
[00:15:26.280 --> 00:15:30.720]   It's the same thing as if you got nothing, you know, you're going to have nothing to
[00:15:30.720 --> 00:15:31.720]   hide.
[00:15:31.720 --> 00:15:32.720]   You've got nothing to fear.
[00:15:32.720 --> 00:15:36.520]   I would flip that around on the NSA and the intelligence community broadly.
[00:15:36.520 --> 00:15:38.480]   If you have nothing to hide, you have nothing to fear.
[00:15:38.480 --> 00:15:41.760]   If there is, you know, why are they hiding?
[00:15:41.760 --> 00:15:48.200]   You know, Alex, where I would push back on what you're saying is like, we have to take
[00:15:48.200 --> 00:15:53.440]   their word for all these threats that they claim their systems are stopping.
[00:15:53.440 --> 00:15:57.720]   We don't have evidence because it is secret.
[00:15:57.720 --> 00:16:03.880]   Now obviously, you know, you can't like bust everything wide open and share your sources
[00:16:03.880 --> 00:16:08.240]   and methods because then, you know, then you wouldn't be able to stop anything anywhere
[00:16:08.240 --> 00:16:09.480]   anytime.
[00:16:09.480 --> 00:16:16.160]   But I think that, you know, some level of accountability and, you know, judicial review, legislative
[00:16:16.160 --> 00:16:23.560]   review, accountability through Congress and the courts is, you know, is needed here simply
[00:16:23.560 --> 00:16:26.600]   for public trust.
[00:16:26.600 --> 00:16:31.440]   And I think that what you're looking at here is really a essentially the intelligence version
[00:16:31.440 --> 00:16:32.760]   of a double-butch sandwich.
[00:16:32.760 --> 00:16:36.840]   You know, like basically they have done what corporations do with taxes, where they said,
[00:16:36.840 --> 00:16:40.680]   well, it's not, you know, it's outside the country, which is the way that the NSA has
[00:16:40.680 --> 00:16:43.040]   worked for a long time.
[00:16:43.040 --> 00:16:47.320]   You know, Congress can make it, can write new laws if they want to in this area.
[00:16:47.320 --> 00:16:50.880]   I think that it would be very difficult for the Supreme Court to dig into this in the way
[00:16:50.880 --> 00:16:51.880]   they'd like to.
[00:16:51.880 --> 00:16:56.480]   You know, we have, there's the privacy and civil liberties oversight board, the PCLOB
[00:16:56.480 --> 00:16:59.720]   that was tasked with actually looking at this particular program.
[00:16:59.720 --> 00:17:00.720]   And they did.
[00:17:00.720 --> 00:17:02.480]   And they found problems with the program.
[00:17:02.480 --> 00:17:04.360]   And so again, then what?
[00:17:04.360 --> 00:17:07.600]   You know, once you have this board that's found problems with the program, then you would hope
[00:17:07.600 --> 00:17:09.680]   that the courts could review it as well.
[00:17:09.680 --> 00:17:11.160]   And you would hope that Congress would review it.
[00:17:11.160 --> 00:17:19.000]   Now this particular program, it's the, you know, 702 program or whatever is set to sunset
[00:17:19.000 --> 00:17:20.400]   at the end of this year.
[00:17:20.400 --> 00:17:24.240]   And so that's going to be the next big debate because it has a six year time period.
[00:17:24.240 --> 00:17:29.440]   And every time that it is about to sunset last time in 2017, towards the end of the year,
[00:17:29.440 --> 00:17:33.720]   you're going to see all sorts of scare stories in the media about how if we let this program
[00:17:33.720 --> 00:17:36.840]   go away, people are going to die, there's going to be terrorists attacks and all sorts
[00:17:36.840 --> 00:17:37.840]   of stuff.
[00:17:37.840 --> 00:17:43.800]   And yet there has, you know, for whatever you're saying about how this system protects
[00:17:43.800 --> 00:17:49.520]   us, there has yet to be any evidence given by the government, by the NSA, of how this
[00:17:49.520 --> 00:17:53.200]   particular program has actually stopped any kind of terrorist activity.
[00:17:53.200 --> 00:17:56.360]   They can, you know, you can make all these claims all you want, but they've yet to present
[00:17:56.360 --> 00:17:57.800]   any evidence of that.
[00:17:57.800 --> 00:18:00.920]   And so I'm sure at the end of this year, we're going to see the same thing where all these
[00:18:00.920 --> 00:18:04.360]   stories and all these people saying, Oh, if we don't renew this program, people are
[00:18:04.360 --> 00:18:06.440]   going to die.
[00:18:06.440 --> 00:18:12.160]   And you know, without the evidence to support that, it gets right back to what Owen was saying,
[00:18:12.160 --> 00:18:14.040]   you know, what are they hiding?
[00:18:14.040 --> 00:18:16.720]   If you actually have evidence of that, show it, show it to us.
[00:18:16.720 --> 00:18:20.560]   Show us how this program is actually protected us before we get Congress to renew it.
[00:18:20.560 --> 00:18:25.440]   And so, you know, it would be nice if Congress, and from my perspective, I think Congress
[00:18:25.440 --> 00:18:30.000]   shouldn't renew it, or if they do renew it, how they will, of course they will, of course
[00:18:30.000 --> 00:18:34.600]   they will, but it should be greatly restricted from what it is now.
[00:18:34.600 --> 00:18:40.000]   And again, like the law, if you read the law right now, it is not clear that it enables
[00:18:40.000 --> 00:18:41.000]   this.
[00:18:41.000 --> 00:18:45.880]   This is an interpretation by the NSA of how, you know, how to interpret this law, like
[00:18:45.880 --> 00:18:47.000]   make it explicit.
[00:18:47.000 --> 00:18:51.320]   If you want the NSA to do this, put that in the law so that people know, because this
[00:18:51.320 --> 00:18:55.440]   is the whole thing, like nobody knew about this program, despite the law being public
[00:18:55.440 --> 00:18:59.680]   until Edwards, Nord, Snowden released all these documents that revealed how they were
[00:18:59.680 --> 00:19:00.880]   interpreting the law.
[00:19:00.880 --> 00:19:02.400]   And that alone is a problem.
[00:19:02.400 --> 00:19:06.800]   You know, if you want to say that this is a good program and an important program, fine,
[00:19:06.800 --> 00:19:08.200]   but be public about it.
[00:19:08.200 --> 00:19:12.440]   Isn't the theory, though, that there's the congressional oversight and whatever it is,
[00:19:12.440 --> 00:19:17.520]   the gang of eight who are cleared to see this, it's their job as our representatives to look
[00:19:17.520 --> 00:19:22.480]   at this, and it protects both the security of the NSA and our privacy.
[00:19:22.480 --> 00:19:23.720]   Isn't that the theory?
[00:19:23.720 --> 00:19:30.160]   Yeah, but part of the reality is also that the intelligence community has lied to the
[00:19:30.160 --> 00:19:31.160]   overseers, right?
[00:19:31.160 --> 00:19:35.240]   This is what Senator Wyden has revealed a bunch of times.
[00:19:35.240 --> 00:19:37.120]   Well, he'll ask questions.
[00:19:37.120 --> 00:19:40.720]   And as soon as you see Senator Wyden asking questions of the intelligence community,
[00:19:40.720 --> 00:19:44.320]   you know they're lying about something because that's how it works.
[00:19:44.320 --> 00:19:50.360]   You know, he'll basically try, he's trying to trap them in the lie without, you know,
[00:19:50.360 --> 00:19:54.720]   violating security clearance that he has.
[00:19:54.720 --> 00:19:59.320]   And so, you know, over and over again, he asked them, you know, please explain how this
[00:19:59.320 --> 00:20:03.360]   is happening or what is happening here or what are the, you know, to what extent are
[00:20:03.360 --> 00:20:04.360]   you doing this?
[00:20:04.360 --> 00:20:07.360]   And what comes out is that they're lying.
[00:20:07.360 --> 00:20:13.160]   They are, maybe, you know, they wouldn't say lying, but they are using words in ways
[00:20:13.160 --> 00:20:17.360]   that nobody else defines those words in order to justify what it is that they're doing.
[00:20:17.360 --> 00:20:19.600]   And again, that gets to the problem.
[00:20:19.600 --> 00:20:22.600]   You know, if you want to do this, be public about it.
[00:20:22.600 --> 00:20:24.240]   Explain exactly what you're doing.
[00:20:24.240 --> 00:20:29.280]   Explain how it's justified or how it's, you know, authorized by the law.
[00:20:29.280 --> 00:20:30.680]   And then let's have a debate about it.
[00:20:30.680 --> 00:20:31.960]   Then let's have a conversation about it.
[00:20:31.960 --> 00:20:32.960]   That's not what we have.
[00:20:32.960 --> 00:20:38.640]   Maybe right to the members of the gang of eight, Mike Turner, Jim Himes, Mark Warner,
[00:20:38.640 --> 00:20:44.200]   Marco Rubio, Kevin McCarthy, Jeffries, Chuck Schumer, and Mitch McConnell and say, ask
[00:20:44.200 --> 00:20:46.280]   some tough questions.
[00:20:46.280 --> 00:20:51.880]   We, the people, don't want this blank check to continue.
[00:20:51.880 --> 00:20:53.880]   When is the renewal up this year?
[00:20:53.880 --> 00:20:55.120]   The end of this year.
[00:20:55.120 --> 00:20:56.120]   Okay.
[00:20:56.120 --> 00:20:59.120]   I forget if it's like, you know, December 31st, whatever, but it's towards the end of this
[00:20:59.120 --> 00:21:00.120]   year.
[00:21:00.120 --> 00:21:06.000]   Actually, interestingly, the UK is looking at something called the online safety bill,
[00:21:06.000 --> 00:21:10.560]   which would basically, they don't admit it, but would ban end-to-end encryption.
[00:21:10.560 --> 00:21:12.920]   It would at least have to provide a backdoor.
[00:21:12.920 --> 00:21:19.480]   And the folks at Signal said this week that they, they'd walk away from the UK if that
[00:21:19.480 --> 00:21:22.960]   bill undermined end-to-end encryption.
[00:21:22.960 --> 00:21:28.360]   Governments, especially the five eyes all over the world are trying to break down this door
[00:21:28.360 --> 00:21:29.360]   to encryption.
[00:21:29.360 --> 00:21:34.000]   I, by the way, I owe an apology to the NSA as wonderful as this site is.
[00:21:34.000 --> 00:21:35.200]   It is not their site.
[00:21:35.200 --> 00:21:38.040]   It's nsa.gov one.info.
[00:21:38.040 --> 00:21:40.640]   I should have known better looking at the URL.
[00:21:40.640 --> 00:21:42.520]   It is a parody site.
[00:21:42.520 --> 00:21:46.520]   But in a way, the parody strikes home.
[00:21:46.520 --> 00:21:47.720]   So I just, I apologize.
[00:21:47.720 --> 00:21:50.920]   That is not the official NSA slogan.
[00:21:50.920 --> 00:21:53.760]   And I think that, or is it the front of the-
[00:21:53.760 --> 00:21:56.240]   I think you touched that.
[00:21:56.240 --> 00:21:58.160]   I thought it did seem strange.
[00:21:58.160 --> 00:21:59.160]   It seems a lot.
[00:21:59.160 --> 00:22:00.160]   I should have-
[00:22:00.160 --> 00:22:01.720]   I should have believed you on the nose.
[00:22:01.720 --> 00:22:02.720]   Used my head.
[00:22:02.720 --> 00:22:03.720]   Yeah.
[00:22:03.720 --> 00:22:04.720]   Yeah.
[00:22:04.720 --> 00:22:07.400]   The, I think that, you know, a lot of times people talk about security with encryption.
[00:22:07.400 --> 00:22:10.920]   When it comes to encryption, they talk about the fact that right now they have an HD signal
[00:22:10.920 --> 00:22:16.480]   that is all like it's, it's 1920 by 1080 of all the information of people around them.
[00:22:16.480 --> 00:22:18.280]   And there's one pixel missing.
[00:22:18.280 --> 00:22:19.760]   They don't like those dark pixels.
[00:22:19.760 --> 00:22:22.360]   They're like, I don't know what's going on in that dark pixel.
[00:22:22.360 --> 00:22:23.680]   And I want, I want to have that opened up.
[00:22:23.680 --> 00:22:28.720]   And I, and I do think that we do have to be careful of, I, as far as gathering it, as
[00:22:28.720 --> 00:22:31.760]   I said, I think that if the NSA is doing it overseas, I think that, I think it's going
[00:22:31.760 --> 00:22:32.760]   to be very hard to stop.
[00:22:32.760 --> 00:22:35.160]   I don't know whether I agree with it or not, but I think it's going to be very hard to ever
[00:22:35.160 --> 00:22:37.200]   get anyone to rule on that.
[00:22:37.200 --> 00:22:39.920]   Because usually they bring up the cases that we can't see.
[00:22:39.920 --> 00:22:43.400]   Anyway, so, but when it comes to starting to break down our encryption, I think that
[00:22:43.400 --> 00:22:44.640]   that is a big deal.
[00:22:44.640 --> 00:22:48.000]   Like, like the, you know, what the, what the UK is trying to do if we want to get rid
[00:22:48.000 --> 00:22:49.000]   of the encryption.
[00:22:49.000 --> 00:22:53.080]   One thing to collect it where by the time they get into it, it probably will, you know,
[00:22:53.080 --> 00:22:55.880]   will be not doing that anymore, whatever it was.
[00:22:55.880 --> 00:23:01.920]   The, but I think that taking apart the last little bit of us being able to, to keep our
[00:23:01.920 --> 00:23:04.760]   stuff encrypted if we choose to, I think is a big deal.
[00:23:04.760 --> 00:23:09.960]   The WhatsApp folks say they would refuse to lower security for any government.
[00:23:09.960 --> 00:23:14.480]   But unfortunately, I mean, as we've seen before, if a government requires it, you really don't
[00:23:14.480 --> 00:23:15.480]   have the choice.
[00:23:15.480 --> 00:23:17.840]   You either leave the country or comply, right?
[00:23:17.840 --> 00:23:19.080]   You can't just leave the country.
[00:23:19.080 --> 00:23:22.520]   Yeah, you should leave, but I don't know.
[00:23:22.520 --> 00:23:24.080]   That's pretty draconian to it.
[00:23:24.080 --> 00:23:32.040]   Of course, the online protect safety bill is, as usual, raise the paper tiger of protecting
[00:23:32.040 --> 00:23:34.640]   the children.
[00:23:34.640 --> 00:23:39.800]   But I think the people who want this are really much more about, let's see what's going on
[00:23:39.800 --> 00:23:40.800]   everywhere.
[00:23:40.800 --> 00:23:44.240]   Yeah, there are a lot of problems with the online safety bill.
[00:23:44.240 --> 00:23:45.680]   It touches on a whole bunch of different things.
[00:23:45.680 --> 00:23:47.680]   And of course, it sort of keeps changing.
[00:23:47.680 --> 00:23:50.680]   So it's a little, it's hard to know what they're going to do.
[00:23:50.680 --> 00:23:51.680]   Yeah.
[00:23:51.680 --> 00:23:54.760]   And so, you know, and so because of that, you'll have people say that it doesn't actually
[00:23:54.760 --> 00:23:55.760]   attack encryption.
[00:23:55.760 --> 00:23:58.200]   It sort of does so in an underhanded manner.
[00:23:58.200 --> 00:24:04.160]   And that's the way most of the online safety bill actually works is this kind of vague
[00:24:04.160 --> 00:24:10.080]   language that, which is effectively like, just don't do anything wrong and we won't
[00:24:10.080 --> 00:24:11.240]   bother you.
[00:24:11.240 --> 00:24:18.000]   And if you do something that we consider wrong, then the government will come and bring the
[00:24:18.000 --> 00:24:21.040]   hammer down.
[00:24:21.040 --> 00:24:27.680]   And the challenge is always that the intelligence agencies are highly technical, highly focused
[00:24:27.680 --> 00:24:29.200]   and know exactly what they want.
[00:24:29.200 --> 00:24:31.560]   And they're talking to legislators around the world.
[00:24:31.560 --> 00:24:33.720]   They have no idea what they're talking about.
[00:24:33.720 --> 00:24:34.720]   You know, and they're in leisure.
[00:24:34.720 --> 00:24:37.480]   More encountering in terms, it's hard to say, no, if you're saying, look, we just want
[00:24:37.480 --> 00:24:39.160]   to catch pedophiles.
[00:24:39.160 --> 00:24:44.320]   It's hard as an elected official to say, well, no, I'm not in favor of that.
[00:24:44.320 --> 00:24:46.160]   So that's why they use that, right?
[00:24:46.160 --> 00:24:47.880]   Consider the children.
[00:24:47.880 --> 00:24:51.200]   This is what the home office, the British home office says is important.
[00:24:51.200 --> 00:24:55.360]   The technology companies make every effort to ensure their platforms do not become a
[00:24:55.360 --> 00:24:58.120]   deep reading ground for pedophiles.
[00:24:58.120 --> 00:25:00.720]   The online, get this, talk about mealy mouth.
[00:25:00.720 --> 00:25:03.720]   The online safety bill doesn't, I should do this in a British accent.
[00:25:03.720 --> 00:25:08.360]   The online safety bill does not represent a ban on end-to-end encryption, governor,
[00:25:08.360 --> 00:25:14.080]   but makes it clear that technological changes should not be implemented in a way that diminishes
[00:25:14.080 --> 00:25:18.040]   public safety, especially the safety of children online.
[00:25:18.040 --> 00:25:21.400]   And of course now then they draw the line, well, encryption does that diminishes the
[00:25:21.400 --> 00:25:23.120]   safety of children.
[00:25:23.120 --> 00:25:26.120]   But they say it's not a choice between privacy or child safety.
[00:25:26.120 --> 00:25:30.240]   We can and we must have both.
[00:25:30.240 --> 00:25:32.200]   How I don't know.
[00:25:32.200 --> 00:25:34.200]   Without without the encryption, it's hard.
[00:25:34.200 --> 00:25:35.480]   Again, we get back to that.
[00:25:35.480 --> 00:25:38.680]   I believe that as long as we have encryption and people who know what they're, you know,
[00:25:38.680 --> 00:25:43.280]   if you're doing something, you know, again, most of the communication that I do when it's
[00:25:43.280 --> 00:25:49.400]   not in messages and signal, knowing that it will be, you know, probably okay.
[00:25:49.400 --> 00:25:52.960]   And I'm not, I'm talking about like, I think it's for, you're not alone.
[00:25:52.960 --> 00:25:59.560]   presentation Samuel Bank McEnfrey. Love signal. A lot of people love signal apparently members
[00:25:59.560 --> 00:26:05.780]   of the Congress love signal Matt Gaetz love signal. A lot of people love signal. Yeah.
[00:26:05.780 --> 00:26:10.740]   The problem is in, and you know the, the quote of they've got a high dev screen and
[00:26:10.740 --> 00:26:15.340]   this is true technology has given law enforcement. This incredible view into all of our lives
[00:26:15.340 --> 00:26:18.180]   and they just don't like these little dark parts. I mentioned this to you before it comes
[00:26:18.180 --> 00:26:23.140]   from Phil Zimmerman, the guy who created PGP. And from the very beginning, people said to
[00:26:23.140 --> 00:26:26.580]   Phil, well, you're going to allow people to encrypt stuff. Bad guys are going to use
[00:26:26.580 --> 00:26:35.180]   it. And his answer always was collateral damage. If you, you know, we got to have privacy.
[00:26:35.180 --> 00:26:39.620]   The politicians, that's a hard one because, you know, the next big terrorist, you know,
[00:26:39.620 --> 00:26:42.980]   like the secure, I know they don't want to get told as agencies say, here's the deal.
[00:26:42.980 --> 00:26:46.340]   I know. Here's a head that's going to sit on your desk. If anything, like, you know,
[00:26:46.340 --> 00:26:50.060]   like, like literally they tell them this all the time that of course something goes bad.
[00:26:50.060 --> 00:26:52.940]   If a thousand people die because of this, we're going to blame you. It's your fault.
[00:26:52.940 --> 00:26:56.700]   Like, you know, like we're going to get up and we're going to blame you. How do we balance
[00:26:56.700 --> 00:27:00.300]   that? Mike, do you have a suggestion? That's really hard for a politician. I understand
[00:27:00.300 --> 00:27:05.300]   we want safety. Yeah. But I think, I mean, I think you guys made the point already, which
[00:27:05.300 --> 00:27:10.420]   is that it's not a question of balancing, right? Encryption is part of protecting people
[00:27:10.420 --> 00:27:15.580]   and making them safe. The idea that law enforcement is having all sorts of difficulty because
[00:27:15.580 --> 00:27:20.420]   of encryption still is not proven. They just keep saying that. And yet, as you've said,
[00:27:20.420 --> 00:27:24.940]   they have more access to more information than ever before. The ability to go forward
[00:27:24.940 --> 00:27:29.100]   and back in time, as we were discussing earlier, the ability to get all sorts of access to
[00:27:29.100 --> 00:27:32.220]   information that was impossible before, the ability to solve all sorts of crimes that
[00:27:32.220 --> 00:27:37.220]   were impossible before is there now they have more power now than ever before and more visibility
[00:27:37.220 --> 00:27:41.780]   into all the stuff than ever before. If there are a few small areas, even in the cases where
[00:27:41.780 --> 00:27:46.100]   people are using encryption, the reality is that almost everybody uses encryption trips
[00:27:46.100 --> 00:27:51.740]   up somehow, right? There's all sorts of things. If you're really doing bad, there are ways
[00:27:51.740 --> 00:27:57.460]   for law enforcement to get at the information they need to take you down. Almost always,
[00:27:57.460 --> 00:28:00.260]   that's the case. The idea that we need to break encryption for everybody else, where
[00:28:00.260 --> 00:28:06.380]   it is keeping them safe and where it is protecting them from malicious actors and hackers and
[00:28:06.380 --> 00:28:11.300]   foreign governments that are trying to do bad things, the value that you get from the
[00:28:11.300 --> 00:28:17.660]   encryption is so much higher than whatever little, as yet unproven problems there are
[00:28:17.660 --> 00:28:24.060]   for law enforcement. I don't think that the conversation should be about balance because
[00:28:24.060 --> 00:28:29.380]   we already have what we need. We have the tools to protect ourselves and law enforcement
[00:28:29.380 --> 00:28:32.140]   has more tools than ever before to stop bad actors.
[00:28:32.140 --> 00:28:39.620]   It's interesting that Apple, who has been selling privacy for some time and now is sort
[00:28:39.620 --> 00:28:44.740]   of starting to live up to its promise. They've offered this advanced data protection, which
[00:28:44.740 --> 00:28:50.820]   encrypts everything on iCloud. As I understand it, fully encrypts it. There's no backdoor.
[00:28:50.820 --> 00:28:57.580]   They decided to back down on their CSAM scanning, again, something that Nick Mick and National
[00:28:57.580 --> 00:29:04.540]   Center for Missing and Splitted Children and advocates of that ilk wanted and Apple was
[00:29:04.540 --> 00:29:11.540]   going to do until they heard the hue and cry. Is that a bellwether? Is the fact that Apple
[00:29:11.540 --> 00:29:14.860]   is going to support good encryption, a bellwether?
[00:29:14.860 --> 00:29:19.740]   Apple is slowly tightening privacy news. If they do too much of it too quickly, people
[00:29:19.740 --> 00:29:24.020]   will not let them do it. They just, every update is just a little tighter than it was
[00:29:24.020 --> 00:29:28.580]   before. Every update, ATT and all the other things they're doing, all of those things
[00:29:28.580 --> 00:29:36.340]   are just tightening screws very slowly to keep on trying to perfect us not being able,
[00:29:36.340 --> 00:29:39.540]   not being tracked in any way shape or form, which is really upsetting a lot.
[00:29:39.540 --> 00:29:44.300]   But they're going to come up against governments saying, well, you can't do that.
[00:29:44.300 --> 00:29:46.300]   Eventually, they do it all at one time.
[00:29:46.300 --> 00:29:50.420]   Immediately, the thing is, if you boil that frog very slowly, you can slowly tie it in
[00:29:50.420 --> 00:29:54.260]   and you build it so that so many of those things, so many of the things that Apple's
[00:29:54.260 --> 00:29:59.020]   doing is they're knocking all the pegs off behind them so that it's very hard to unwrap
[00:29:59.020 --> 00:30:00.020]   back out.
[00:30:00.020 --> 00:30:03.780]   If someone said, well, we have to get access, Apple's a couple of versions away from going
[00:30:03.780 --> 00:30:04.780]   well.
[00:30:04.780 --> 00:30:05.780]   Can't help you.
[00:30:05.780 --> 00:30:12.220]   There's no way to build that back into the older versions. The thing is, is that Apple
[00:30:12.220 --> 00:30:13.220]   would like that.
[00:30:13.220 --> 00:30:18.740]   I know that Apple would like to be able to say, sorry, send us all the subpoenas you want.
[00:30:18.740 --> 00:30:21.060]   We don't have that information.
[00:30:21.060 --> 00:30:26.820]   But at some point, China, Russia, maybe the US, England, they're going to say, but we
[00:30:26.820 --> 00:30:28.660]   need that information.
[00:30:28.660 --> 00:30:32.620]   Russia doesn't really matter anymore, but the Apple.
[00:30:32.620 --> 00:30:34.580]   China's another matter, though.
[00:30:34.580 --> 00:30:39.540]   China's a big part, but most companies are diversifying from China out of the fear that
[00:30:39.540 --> 00:30:41.940]   they're going to create trouble in Taiwan.
[00:30:41.940 --> 00:30:47.380]   They can't do it right now, but everybody's diversifying right now from China because
[00:30:47.380 --> 00:30:50.020]   they see it as the next problem, child.
[00:30:50.020 --> 00:30:54.620]   Mike, do you think that maybe the tech industry is starting to come down on the right side
[00:30:54.620 --> 00:30:56.060]   in this or?
[00:30:56.060 --> 00:31:03.140]   Well, I mean, it's still a challenge. I think everybody's trying to explore, and they are
[00:31:03.140 --> 00:31:05.660]   afraid of government cracking down.
[00:31:05.660 --> 00:31:11.780]   I think Apple, when they tried to do the CCM scanning, that was definitely a step in
[00:31:11.780 --> 00:31:16.500]   the wrong direction. Luckily, there was a very loud outcry, and they realized to go in the
[00:31:16.500 --> 00:31:22.540]   other direction and now even take a much bigger step in the other direction, which was encrypting
[00:31:22.540 --> 00:31:24.620]   the cloud backup data.
[00:31:24.620 --> 00:31:28.900]   And so I think Apple has definitely made some moves in the right direction. I think Google's
[00:31:28.900 --> 00:31:32.660]   made some moves in the right direction on encryption as well.
[00:31:32.660 --> 00:31:36.100]   But it is an ongoing challenge.
[00:31:36.100 --> 00:31:42.020]   And as Alex said, if they go too fast, then that leads to more things like what Australia
[00:31:42.020 --> 00:31:46.460]   has done effectively trying to outlaw encryption, what the UK is trying to do to outlaw encryption.
[00:31:46.460 --> 00:31:51.340]   And there are some bills that are popping up here and there in the US and a little bit
[00:31:51.340 --> 00:31:54.740]   in the EU too, where they're approaching this kind of thing, where they're trying to get
[00:31:54.740 --> 00:32:01.980]   them to effectively outlaw or backdoor encryption, which is really the same thing.
[00:32:01.980 --> 00:32:07.980]   And I think the companies, for the most part, do seem to want to do the right thing. But
[00:32:07.980 --> 00:32:12.700]   it is definitely walking the line and balancing how hard the different governments are going
[00:32:12.700 --> 00:32:13.700]   to come down on that.
[00:32:13.700 --> 00:32:18.980]   The lesson though from that Apple CSAM thing, they listened when enough of us stood up and
[00:32:18.980 --> 00:32:25.620]   said, no, no, no, this is a bad idea when the EFF and others petitioned when everybody
[00:32:25.620 --> 00:32:29.740]   stood up said this is a bad idea, they did. And they finally have backed down entirely
[00:32:29.740 --> 00:32:31.460]   and they're not going to do it.
[00:32:31.460 --> 00:32:32.460]   So yeah.
[00:32:32.460 --> 00:32:36.140]   And I think to the CSAM, I think what Apple was trying to do is we're going to keep turning
[00:32:36.140 --> 00:32:41.380]   up that they saw the CSAM, what they were doing there was that we're going to keep on
[00:32:41.380 --> 00:32:47.700]   tightening this up and people are going to use CSAM, they're going to use that issue.
[00:32:47.700 --> 00:32:51.100]   They're going to use this issue. So they thought that they could build a solution that would
[00:32:51.100 --> 00:32:55.780]   protect them. So as they keep on turning it, you can't use that against them. And so I
[00:32:55.780 --> 00:33:00.940]   think that that was their mentality when they did it. It just didn't ring very well.
[00:33:00.940 --> 00:33:10.180]   And I guess my message is it works. So if people like Mike keep rattling the cage and
[00:33:10.180 --> 00:33:16.740]   say, no, no, no, you know, we, this is, there is no safe way to backdoor encryption. Encryption
[00:33:16.740 --> 00:33:22.340]   protects all of us. That's good. We got to all stand up and be counted on that.
[00:33:22.340 --> 00:33:26.300]   I want to agree with one example is so, you know, we had this thing with a TSA, you know,
[00:33:26.300 --> 00:33:29.860]   TSA, you have those little TSA locks, which are worthless now. Why are they worthless?
[00:33:29.860 --> 00:33:34.620]   Why are the TSA workload worthless? So the TSA locks, the keys in the TSA lock are one
[00:33:34.620 --> 00:33:37.220]   dimensional, you know, they're just outlines. They don't have any extra code.
[00:33:37.220 --> 00:33:38.220]   And there's only one.
[00:33:38.220 --> 00:33:42.300]   No, no, no, there's like eight or nine. Oh, they're more than one. Okay. But the thing
[00:33:42.300 --> 00:33:46.060]   is, is sort of to have a TSA locks. So what we did is we said, okay, we trust TSA, they're
[00:33:46.060 --> 00:33:49.740]   going to, we're going to put a lock in so they're only TSA can get into our bags. Right. The
[00:33:49.740 --> 00:33:54.940]   problem was, is some doofus got interviewed by the Washington Post or whatever. And they
[00:33:54.940 --> 00:33:57.780]   let him take a, they said, can we take a picture of the keys? And he's like, sure, they took
[00:33:57.780 --> 00:34:01.980]   a picture of the keys. Now it's completely like those locks became, because you can't
[00:34:01.980 --> 00:34:05.260]   go back and change them. I mean, they've sold millions and millions and millions of these
[00:34:05.260 --> 00:34:09.780]   locks. And so at this point, because of that one article and because someone just, you
[00:34:09.780 --> 00:34:15.860]   know, we left the backdoor open to, to the TSA. And what they did with it was let someone
[00:34:15.860 --> 00:34:18.660]   take a picture of it, which now invalidated all those locks you might as well just use
[00:34:18.660 --> 00:34:22.220]   zip ties at this point. Like, which is what we do. We use zip ties because, Oh, interesting.
[00:34:22.220 --> 00:34:26.300]   Yeah. And they're going to cut the zip tie, but you don't care about that. Well, they'll
[00:34:26.300 --> 00:34:29.700]   put the zip tie back. The secret to zip ties is that with a lock, you don't know whether
[00:34:29.700 --> 00:34:33.820]   they got into your case. If you put a, if you put an orange zip tie, TSA only owns clear
[00:34:33.820 --> 00:34:39.500]   zip ties. So you can look at all your bags. I'll, I'll like, I do production. So we are
[00:34:39.500 --> 00:34:44.900]   supposed to put a little piece of paper in this. But we make, but it helps us to know
[00:34:44.900 --> 00:34:49.980]   I literally, because what happens to look at one way, well, the number one way we break
[00:34:49.980 --> 00:34:54.900]   equipment is TSA opens our bag and then packs it badly. Right. And so what we do is we,
[00:34:54.900 --> 00:34:59.100]   we used to look at all the coming off, well, 20 bags coming off and we'll see the two that
[00:34:59.100 --> 00:35:02.940]   they went through, pull them open, look at them, see if they broke anything. And then that
[00:35:02.940 --> 00:35:05.700]   just allows us to air correctly. We now have don't have a monitor.
[00:35:05.700 --> 00:35:13.460]   Another tip, don't put ground coffee in your bags, because apparently, drug smugglers use
[00:35:13.460 --> 00:35:18.380]   that to hide the smell of the drugs. And Lisa likes to pack her coffee on our trips. And
[00:35:18.380 --> 00:35:23.540]   the last time we went somewhere, she said, can I, can you carry with my coffee in your
[00:35:23.540 --> 00:35:27.060]   bag? It took me a while to get through TSA that day.
[00:35:30.180 --> 00:35:33.580]   And I'm not sure I'd want to drink the coffee after they dug through it. But anyway, just
[00:35:33.580 --> 00:35:37.900]   a tip, just a little tip. Hey, we have a great panel here. This was just one story, the
[00:35:37.900 --> 00:35:42.900]   NSA story. But we, there is another big Supreme Court story. We're going to get to that. Mike
[00:35:42.900 --> 00:35:48.060]   Masnick is here from Techdert. It's always great to have you. That's mics on the left on
[00:35:48.060 --> 00:35:53.300]   the right. We've got Alex Lindsay from office hours.global and oh nine oh media. Somebody
[00:35:53.300 --> 00:36:00.380]   said finally some balance on this show. Alex, you, you I guess, passed for what is balanced
[00:36:00.380 --> 00:36:03.900]   on the show. I guess I guess I don't know what that means. Cause no one's ever claimed
[00:36:03.900 --> 00:36:09.260]   that I was balanced. So the imbalance. Michael, Alex Lindsay. And of course, my good friend,
[00:36:09.260 --> 00:36:13.900]   Owen Thomas, who is now a columnist at the San Francisco Examiner. I'm going to keep
[00:36:13.900 --> 00:36:19.460]   saying the Herb Kane of Silicon Valley until somebody stops me. You're it. You're the
[00:36:19.460 --> 00:36:25.100]   guy. I'll keep taking it. You're the guy. Hey, I want to talk, take a break for one of our
[00:36:25.100 --> 00:36:28.340]   sponsors and talk about something I've been using now for more than a year. Lisa and I
[00:36:28.340 --> 00:36:34.020]   both have. It's called noom. I probably like you saw a lot of ads a couple of years ago
[00:36:34.020 --> 00:36:39.620]   for noom, this new weight management program. And I saw the ads and I thought, that's
[00:36:39.620 --> 00:36:43.820]   interesting. That's interesting. Finally, one day I got tired of the yo-yo dieting I'd
[00:36:43.820 --> 00:36:48.380]   been doing it. I said, let me try it. And Lisa, my, my wife to her credit said, I'll do
[00:36:48.380 --> 00:36:53.660]   it with you. She's slender. She had a few pounds, I guess to lose. I didn't seem that
[00:36:53.660 --> 00:37:00.460]   way to me, but she felt that way. So we both do it. Now Lisa is the poster child for noom.
[00:37:00.460 --> 00:37:06.340]   She lost 10 pounds and has kept it off for more than a year. I lost about 16 pounds. It's
[00:37:06.340 --> 00:37:11.020]   been up and down a little bit, but every time I lose some more, I just do more of those
[00:37:11.020 --> 00:37:17.660]   lessons and noom. It really, really works. When you decide to lose weight, it's more than
[00:37:17.660 --> 00:37:22.980]   just the number on the scale. There are other reasons, more health, right? Or because you
[00:37:22.980 --> 00:37:28.540]   want to look better. And there's lots of reasons that's hard to do. You know, with noom, you
[00:37:28.540 --> 00:37:35.020]   learn about what it is that's causing you to eat. Do you eat because you're bored? Do
[00:37:35.020 --> 00:37:39.740]   you eat because you can't pass up food when it's free? I can't pass up food, period. If
[00:37:39.740 --> 00:37:44.380]   it's if there's a cupcake in front of me, I will resist. But eventually I'm going to
[00:37:44.380 --> 00:37:50.220]   eat that. So that's what noom really helps you with. It helps you break the cycle, understand
[00:37:50.220 --> 00:37:55.100]   what you're doing and break those habits, change them for good. Noom weight uses psychology
[00:37:55.100 --> 00:37:59.740]   to help you understand what's going on in your brain. So you can learn to make healthier
[00:37:59.740 --> 00:38:05.340]   choices every day. Losing weight starts with your brain, not with your mouth, not with
[00:38:05.340 --> 00:38:09.740]   your, not with your fork. It starts with your brain. The program helps you understand the
[00:38:09.740 --> 00:38:15.580]   science behind your eating choices, why you're craving. I am a stuffer. I will go home unconsciously.
[00:38:15.580 --> 00:38:21.260]   I will eat a thousand calories before I even notice it. And as soon as I learned that,
[00:38:21.260 --> 00:38:26.300]   I was able to control it, which is amazing. I'm very much aware of it when I was completely
[00:38:26.300 --> 00:38:32.860]   unconscious before. Noom weight really works. More than 4.6 million people now have lost weight.
[00:38:32.860 --> 00:38:36.780]   On average, 15 pounds and 16 weeks, that's exactly what I did, by the way.
[00:38:38.380 --> 00:38:45.020]   We have friends who've lost much more. Brann O'woo lost 100 pounds on Noom and has kept it off and
[00:38:45.020 --> 00:38:50.460]   looks great. We have one of our chatters, one of our regulars in both Discord and IRC,
[00:38:50.460 --> 00:38:55.740]   went on that Alaska cruise with us. And I said, I knew he was going to be out and I said,
[00:38:55.740 --> 00:39:00.140]   I texted him. I said, where are you? I don't see. He said, he texted me back. I'm standing right
[00:39:00.140 --> 00:39:04.700]   next to you. I didn't recognize him. Well, he shaved his beard too. So it wasn't completely the
[00:39:04.700 --> 00:39:11.180]   weight, but he'd lost 60 pounds in Noom. And yes, he was a new man. He really was. Noom's flexible
[00:39:11.180 --> 00:39:17.340]   program focuses on progress, not perfection. You don't have to give up carbs or create some other
[00:39:17.340 --> 00:39:21.980]   weird restrictive diet. I don't know about you, but when I have something restricted,
[00:39:21.980 --> 00:39:28.620]   that's the one I want, right? Can't eat anymore pasta. I need pasta. And eventually it breaks you
[00:39:28.620 --> 00:39:34.300]   down. Noom doesn't do that. But I got to say, everyone's journey is different. You get personalized
[00:39:34.300 --> 00:39:37.820]   lessons to your goals. That's why when you sign up for Noom, you're going to, they're going to ask
[00:39:37.820 --> 00:39:42.460]   you a lot of questions. I know it takes some time, but that's really to kind of prepare those
[00:39:42.460 --> 00:39:47.340]   lessons to fit exactly what your needs. They use things like cognitive behavioral therapy,
[00:39:47.340 --> 00:39:55.740]   which really, really works. If you have cravings or food FOMO, I have big food FOMO. Noom weight can
[00:39:55.740 --> 00:39:59.100]   help you lose weight while still enjoying your favorite foods. And you could choose the level of
[00:39:59.100 --> 00:40:03.020]   support. You could do five minute daily check-ins. You could have personal coaching. You have groups.
[00:40:03.740 --> 00:40:07.100]   And of course, you have those lessons you can spend as much or a little time on those as you want.
[00:40:07.100 --> 00:40:12.700]   I think the lessons are fantastic. Noom has published one of the 30 peer reviewed scientific articles to
[00:40:12.700 --> 00:40:17.180]   inform users, practitioners, scientists in the public about their methods and effectiveness,
[00:40:17.180 --> 00:40:23.180]   peer reviewed scientific articles. There is a real basis for Noom. It's not a gimmick. It's not a
[00:40:23.180 --> 00:40:29.740]   fad diet. It's not a diet at all. It's an education program that helps you stay focused on what's
[00:40:29.740 --> 00:40:34.780]   important to you. Noom weight psychology based approach. It worked for me. It worked for Lisa.
[00:40:34.780 --> 00:40:38.620]   It worked for Brianna. It worked for so many people I know. Sign up for your trial today at
[00:40:38.620 --> 00:40:48.380]   noom.com/twit. Honest. It really, really works. N-O-O-M.com/twit. And of course, sign up for your trial.
[00:40:48.380 --> 00:40:52.780]   You can see if it works for you. There's also a book. They just published their first ever book,
[00:40:52.780 --> 00:40:59.020]   The Noom Mindset. The Noom Mindset, a deep dive into the psychology of behavior change.
[00:40:59.020 --> 00:41:03.020]   If nothing else, get the book and read it because I think it'll give you a better idea of what Noom's
[00:41:03.020 --> 00:41:10.220]   all about. Available to buy now wherever books are sold, noomn-o-m.com/twit. We thank you so much
[00:41:10.220 --> 00:41:18.700]   for their support of this week in tech. So that wasn't the only thing the Supreme Court did this
[00:41:18.700 --> 00:41:24.060]   week. There were two other cases that some say, I don't know, is it hyperbole, Mike Masnick, to say
[00:41:24.700 --> 00:41:28.700]   the fate of the internet rests in the hands of these nine justices?
[00:41:28.700 --> 00:41:35.660]   Well, potentially it may depend on what they have to say in ruling about these cases.
[00:41:35.660 --> 00:41:39.580]   Justice Kagan, when she, I listened to the oral arguments, which is great fun.
[00:41:39.580 --> 00:41:46.460]   Kathy Gallis, your contributor, was there in the room. She's admitted to the Supreme Court.
[00:41:46.460 --> 00:41:52.300]   Justice Kagan at one point said, "These are not the nine greatest experts on the internet."
[00:41:53.740 --> 00:41:58.700]   And the room burs down laughter, which I don't think is very common in the chambers.
[00:41:58.700 --> 00:42:05.660]   She was, and another justices felt the same way, admitted that this is a big decision. So
[00:42:05.660 --> 00:42:11.500]   there were two of them, but this was Gonzalez versus Google, the family of a young woman who was
[00:42:11.500 --> 00:42:19.100]   killed and tragically killed in Paris in an ISIS terrorist attack. For some reason, the family
[00:42:19.100 --> 00:42:25.260]   decided to blame YouTube, even though there was no direct causal connection.
[00:42:25.260 --> 00:42:32.060]   And in particular, the YouTube algorithms recommending ISIS videos, radicalizing people.
[00:42:32.060 --> 00:42:39.660]   And it really threatens section 230 of the Communications Decency Act. This 26 words that
[00:42:39.660 --> 00:42:46.220]   some say saved the internet, made the internet, that say that you're not responsible, whether it's a
[00:42:46.220 --> 00:42:53.260]   website or a chat room or our discord or our Mastodon instance, we're not responsible for the
[00:42:53.260 --> 00:42:58.380]   content people or YouTube post there. That is not something we're publishing. So we, it gives us
[00:42:58.380 --> 00:43:03.500]   two rights. One, the right to leave it up, two, the right to take it down. We can't be sued for either.
[00:43:03.500 --> 00:43:11.020]   Right. Is that a fair, you have the best page on tech. Somebody has directed you to this page on
[00:43:11.020 --> 00:43:15.660]   section 230 because you got it wrong. And it's commonly wrong. In fact, I saw a Mastodon
[00:43:15.660 --> 00:43:19.580]   toot from you saying, would somebody please refer the New York Times to me before they write about
[00:43:19.580 --> 00:43:24.780]   section 230 because they just get it wrong? Why? What is it that people get wrong? Why don't they
[00:43:24.780 --> 00:43:31.580]   understand this? There are so many things that they get that people get wrong about 230. It's hard
[00:43:31.580 --> 00:43:37.420]   to summarize them in a short form. That's one of the reasons I wrote that article that has all
[00:43:37.420 --> 00:43:43.820]   these different things. And what's funny is people get it wrong based on what they want the law to
[00:43:43.820 --> 00:43:49.740]   be. Often the exact opposite of what it is. Some people think that if you moderate too much,
[00:43:49.740 --> 00:43:55.180]   that you lose your 230 protections, for example, which the law is actually explicitly the opposite
[00:43:55.180 --> 00:44:01.420]   of that. So you can moderate? Exactly. And it was put in place because of a case that said the
[00:44:01.420 --> 00:44:08.140]   opposite, that there was, prodigy was this company that had moderated and lost and what became
[00:44:08.140 --> 00:44:13.100]   liable for content on their forums because they had moderated. And 230 was designed to protect that
[00:44:13.100 --> 00:44:19.900]   and fix that. Ron Wyden, by the way, second name time we've mentioned him, is the guy who wrote 230.
[00:44:19.900 --> 00:44:24.220]   And he kind of felt like it was needed because of the CDA. Like he said, we've got to carve some
[00:44:24.220 --> 00:44:30.060]   safe harbor out for the internet so that people can publish. So they can have forums,
[00:44:30.060 --> 00:44:36.700]   they can have communities. Yeah. And it's, you know, it is a really, it's a very simple law in
[00:44:36.700 --> 00:44:41.820]   concept, which is, you know, this idea, the real idea behind it is that you want to put the liability
[00:44:41.820 --> 00:44:48.540]   on the party who actually did the whatever is violating the law, rather than the tools that they
[00:44:48.540 --> 00:44:54.380]   use. And, you know, if you think of it that way, then it becomes a pretty straightforward.
[00:44:54.380 --> 00:45:03.180]   It makes sense. Yes. Right. And you wrote it. But for whatever reason, people have a lot of
[00:45:03.180 --> 00:45:08.220]   trouble with it. And they just really believe that, you know, especially in this case, you know,
[00:45:08.220 --> 00:45:14.140]   the real focus of the Gonzalez case was whether or not the algorithm, the recommendation algorithm
[00:45:14.140 --> 00:45:19.340]   is protected by 230 or not protected by 230. That was one. And I have to say I came around a
[00:45:19.340 --> 00:45:25.980]   little bit. We had a bit of a debate. Jeff Jarvis spanked me on this week in Google because I
[00:45:25.980 --> 00:45:29.740]   said, well, couldn't you? I mean, look, no one loves these algorithms. These algorithms are
[00:45:29.740 --> 00:45:35.580]   are there to make more money to make more sticky content to promote content that keeps people
[00:45:35.580 --> 00:45:40.860]   watching. And not I'm not saying it's intentional, but inadvertently, as a result,
[00:45:40.860 --> 00:45:44.860]   they tend to push more extreme content, more and more extreme content, because that keeps you
[00:45:44.860 --> 00:45:51.580]   engaged. So shouldn't they be liable for that? So there are a couple of couple of things on that.
[00:45:51.580 --> 00:45:57.340]   One is, you know, whether or not they actually do push more extreme content is there's there's
[00:45:57.340 --> 00:46:00.940]   less and less evidence of that. There were a couple reports from from seven or eight years ago
[00:46:00.940 --> 00:46:04.940]   that suggested that there have been multiple studies in the last like three to four years that
[00:46:04.940 --> 00:46:09.420]   have actually suggested that's not true. That what the companies have discovered to some extent is
[00:46:09.420 --> 00:46:12.940]   that if you're just pushing people further and further down in an extremist rabbit hole,
[00:46:12.940 --> 00:46:16.540]   that's actually not good for business. It's your advertisers get kind of mad about that.
[00:46:16.540 --> 00:46:23.740]   That's true. Yeah, look at your Twitter. Yeah, users get angry and start to go elsewhere.
[00:46:23.740 --> 00:46:28.220]   You know, there are all sorts of reasons why companies actually don't, you know, there is this
[00:46:28.220 --> 00:46:33.420]   belief certainly that algorithms are only negative and bad. You know, and the reality is that for
[00:46:33.420 --> 00:46:38.380]   the most part that, you know, many algorithms are actually somewhat helpful. You know, I think
[00:46:38.380 --> 00:46:42.140]   I think the internet would be a lot worse without many of these. Well, there was an amicus brief
[00:46:42.140 --> 00:46:47.900]   filed by Reddit moderators. Yeah. Who said without algorithms, we couldn't do our job. Not, you know,
[00:46:47.900 --> 00:46:52.460]   and this was the problem. And you know, what's interesting, the justices actually seem to get it.
[00:46:52.460 --> 00:46:59.180]   Even I was surprised Brett Kavanaugh, basically said, you know, this isn't for us to decide. This
[00:46:59.180 --> 00:47:07.180]   is for Congress to decide. Justice Clarence Thomas said, algorithms, you got to have algorithms.
[00:47:07.180 --> 00:47:12.700]   Without algorithms, there's no internet. He likes those weird analogies. I think he talked about a
[00:47:12.700 --> 00:47:17.980]   pizza joint or something. But in any event, he got he got it. I was kind of impressed.
[00:47:17.980 --> 00:47:23.900]   Yeah, I mean, it's it's just an interesting question that that point that, you know,
[00:47:23.900 --> 00:47:28.540]   without algorithms, you can't have the internet. Obviously, we had the internet before algorithms
[00:47:28.540 --> 00:47:33.580]   when moderation was done manually. It does not scale. It just doesn't scale. We had Yahoo, right?
[00:47:33.580 --> 00:47:37.260]   Yahoo was done by humans. But it was quickly superseded by excite,
[00:47:37.260 --> 00:47:40.300]   Altevista, and eventually Google who used algorithms.
[00:47:40.300 --> 00:47:47.100]   And it is an interesting question. Is an algorithm, is it speech? Is it conduct?
[00:47:47.100 --> 00:47:51.580]   Is it a product feature? And there is a case it has that I don't believe has made it up to the
[00:47:51.580 --> 00:47:58.220]   Supreme Court. It's being considered by a court in Georgia about Snapchat getting sued over speed
[00:47:58.220 --> 00:48:03.020]   filter that showed how fast you were going when you posted a Snapchat.
[00:48:03.020 --> 00:48:07.580]   I use that on the Shinkansen in Japan to show how fast I was going. But unfortunately,
[00:48:07.580 --> 00:48:13.020]   people also use it while they're driving. Right. And the point made by the plaintiffs,
[00:48:13.020 --> 00:48:17.580]   I believe, was that this encouraged people to use their phone while driving.
[00:48:17.580 --> 00:48:26.140]   Right. And, you know, is it snap ended up pulling the feature, making it kind of a moot point
[00:48:26.140 --> 00:48:34.300]   going forward. But that principle of, you know, is the way that a site that happens to host speech
[00:48:34.300 --> 00:48:41.660]   by its users, the way it operates, is that itself speech by the publisher of the site?
[00:48:41.660 --> 00:48:47.820]   Is it tied into the speech of its users? Or is it, you know, or is it something besides speech?
[00:48:47.820 --> 00:48:53.660]   I think that is a fair and interesting legal question. But I'm sure my good Alex have thoughts
[00:48:53.660 --> 00:48:59.500]   there. Yeah. I mean, it's a tricky thing. But it gets back to this simple question of who is the
[00:48:59.500 --> 00:49:05.100]   one who is actually violating the law? And what is the law that's being violated and how is it
[00:49:05.100 --> 00:49:11.820]   being violated? So, you know, the snap case in terms of the speed filter, I actually think
[00:49:11.820 --> 00:49:15.500]   there've been a couple of different cases with that same sort of fact pattern. And I think the one
[00:49:15.500 --> 00:49:21.500]   that has gotten the most attention, which is snap the lemon, I think that was decided incorrectly,
[00:49:21.500 --> 00:49:28.860]   where they said that that snap doesn't get 230 protections. That case has then gone back and is
[00:49:28.860 --> 00:49:33.340]   currently in discovery about whether or not snap is actually liable. This was one of the mistakes
[00:49:33.340 --> 00:49:37.500]   that the New York Times recently made in their reporting. They said that the court said that snap
[00:49:37.500 --> 00:49:44.140]   was liable, which has not actually been determined in a court yet. You know, so if you look at that
[00:49:44.140 --> 00:49:49.420]   case as an example, you know, what is it that snap is actually doing? Right? So they have created
[00:49:49.420 --> 00:49:55.500]   this filter that tells you what your speed is. Is that encouraging speeding? You know, that's
[00:49:55.500 --> 00:50:01.020]   kind of an open question you could say. But even if it was, is that by itself violating the law?
[00:50:01.020 --> 00:50:06.860]   Is encouraging someone to speed by itself violating the law? It's not. So, you know,
[00:50:06.860 --> 00:50:14.860]   the idea that there's some sort of legal liability here beyond that doesn't really make sense. The
[00:50:14.860 --> 00:50:18.940]   person who is violating the law is the person who is actually speeding. And so, you know, putting
[00:50:18.940 --> 00:50:23.980]   the blame on the person who is actually speeding is the right thing to do. And all that 230 does,
[00:50:23.980 --> 00:50:29.500]   and in these cases, you know, in this case, they got around 230, but, you know, is make sure that
[00:50:29.500 --> 00:50:33.260]   you're talking about putting the legal liability on the party who's actually taking the action,
[00:50:33.260 --> 00:50:40.300]   which violates the law. Right. But my product liability, you know, does not, product liability
[00:50:40.300 --> 00:50:45.900]   law does not forbid you to manufacture a product that could be dangerous. It's all a question of
[00:50:45.900 --> 00:50:49.260]   when you discover that the project is dangerous, who's at fault?
[00:50:49.260 --> 00:50:54.940]   Here's a similar example. Who's responsible if a Tesla running full self-driving
[00:50:54.940 --> 00:51:01.500]   causes a fatality? Is it the driver who used the full self-driving? Under your argument,
[00:51:01.500 --> 00:51:07.500]   Mike, it would be. But I think Tesla advertising full self-driving, when it clearly doesn't work,
[00:51:08.140 --> 00:51:13.740]   might give them some liability too. Right. Right. What if you take advice from chat GPT?
[00:51:13.740 --> 00:51:19.340]   Yeah. Oh, there's your problem right there. The court in the lemon versus snap said, well,
[00:51:19.340 --> 00:51:24.700]   Snapchat made a product that was unsafe in effect.
[00:51:24.700 --> 00:51:30.620]   And it doesn't seem unreasonable to me. I mean,
[00:51:30.620 --> 00:51:35.820]   you're free to make products that are unsafe. It's just you will be liable for.
[00:51:35.820 --> 00:51:40.700]   And clearly the kids who, you know, caused the fatal crash were the
[00:51:40.700 --> 00:51:47.100]   you know, the main culprits. But it's snapchat does have a responsibility.
[00:51:47.100 --> 00:51:50.140]   Don't they not to make stuff that encourages bad beef?
[00:51:50.140 --> 00:51:54.620]   I don't know. But I think that it's one thing for you to build a tool that actually generates
[00:51:54.620 --> 00:51:58.940]   behavior, which is what all tools do. I mean, you know, we oftentimes say when we're talking
[00:51:58.940 --> 00:52:02.860]   about measurement is that you don't measure what matters, what you measure matters.
[00:52:03.500 --> 00:52:07.420]   It becomes something that's important. And if you look at, if you talk to YouTubers, for instance,
[00:52:07.420 --> 00:52:11.900]   they are, you know, they are obsessed with the algorithm. Like they are obsessed with,
[00:52:11.900 --> 00:52:16.620]   what does the algorithm do with when people watch this a certain amount of time? How does it handle
[00:52:16.620 --> 00:52:21.980]   the thumbnails? How does it do all these other things? So they, they behavior is definitely
[00:52:21.980 --> 00:52:26.940]   driven by those algorithms. And so the thing is, is that that's when you're generating those
[00:52:26.940 --> 00:52:31.260]   things, if it's generating bad behavior, that's one thing. But then the other side of this is like
[00:52:31.260 --> 00:52:36.460]   when people put things on TikTok that are dangerous, you know, do you have a, I think part of this is,
[00:52:36.460 --> 00:52:39.980]   and this is where 230, I think comes into a little bit more of a situation is when you're
[00:52:39.980 --> 00:52:44.860]   creating something as a company, you're still probably liable. When you are allowing content
[00:52:44.860 --> 00:52:49.340]   to go on to your site, that is driving people towards a behavior, you know, some kind of herd
[00:52:49.340 --> 00:52:56.300]   behavior that is damaging. Are you liable for that? Now, I think that I think the idea of tampering
[00:52:56.300 --> 00:53:00.940]   with 230 is terrifying. Like, like there's, I don't think that there's any version of this,
[00:53:00.940 --> 00:53:06.860]   any way that you can unwrap 230 in a way, I feel so strongly about it, that when someone
[00:53:06.860 --> 00:53:10.060]   on Twitter or somewhere else posts anything that's positive about like,
[00:53:10.060 --> 00:53:25.140]   undisman
[00:53:25.140 --> 00:53:31.140]   it is the, you know, there's this, you know, I, a lot of times we think of risk as the chances
[00:53:31.140 --> 00:53:36.500]   of something going wrong multiplied by the consequences. Like that is the math of risk, right? And the
[00:53:36.500 --> 00:53:41.780]   chances of something going wrong are medium, medium, the consequences of catastrophic, you
[00:53:41.780 --> 00:53:44.580]   know, like, like, you know, like, and you're just talking about fiddling with something that we
[00:53:44.580 --> 00:53:48.660]   built an entire and whether it was perfect or not, it doesn't matter anymore.
[00:53:48.660 --> 00:53:52.500]   Actually, that was interesting. One of the judges, I think it was Kavanaugh, correct me if I'm wrong,
[00:53:52.500 --> 00:54:00.820]   like, did in effect say that, that the consequences to the economy of changing 230 could be dire.
[00:54:00.820 --> 00:54:06.340]   Yeah, that was Kavanaugh, Kavanaugh, honestly, and I wrote this, you know, I'm kind of hoping
[00:54:06.340 --> 00:54:11.220]   that he writes whatever the decision is out of this. And I would not consider myself generally
[00:54:11.220 --> 00:54:18.180]   speaking a Brett Kavanaugh fan. But I was, you know, he's ruled on a couple cases that sort of
[00:54:18.180 --> 00:54:23.300]   touched on internet and free speech issues and really actually does seem to get that and
[00:54:23.300 --> 00:54:30.260]   to get the deeper nuances here. And that became clear in the oral arguments in both of the cases
[00:54:30.260 --> 00:54:36.420]   last week that he really seems to recognize that that getting this wrong will have massive,
[00:54:36.420 --> 00:54:41.940]   massive consequences for both speech and for sort of economic development and innovation.
[00:54:41.940 --> 00:54:46.740]   And so he's the one who seems most spooked by the possibility of getting this wrong.
[00:54:47.620 --> 00:54:50.500]   And I think that's that is the right attitude to have.
[00:54:50.500 --> 00:54:57.300]   Yeah. But is it is it is it possible to narrow section 230 without
[00:54:57.300 --> 00:55:05.380]   overturning it? Yeah, like, not that I've seen, like, of course, I am open to the idea that that
[00:55:05.380 --> 00:55:11.060]   is possible if somebody could show me a way to do that. And to date, nobody has shown me a way
[00:55:11.060 --> 00:55:17.140]   that doesn't really obliterate the entire law. Every sort of narrow change to 230 that I've seen
[00:55:17.140 --> 00:55:23.300]   in reality obliterated law. And and and Kathy Gellis, my colleague, wrote a piece a couple
[00:55:23.300 --> 00:55:29.060]   of years ago, I think, that effectively said that, which is that, you know, any reform to 230
[00:55:29.060 --> 00:55:35.140]   really is a repeal to 230. And this gets somewhat into the weeds, but it's sort of how 230 actually
[00:55:35.140 --> 00:55:39.220]   works and what the mechanism is, because I think people sort of get confused by that.
[00:55:39.220 --> 00:55:46.340]   The reality is that section 230 is the sort of, you know, a procedural benefit that gets rid of
[00:55:46.340 --> 00:55:53.060]   frivolous cases very early on. And if you change that in any way, all you're really doing is asking
[00:55:53.060 --> 00:55:59.700]   people to have to go through long and expensive court processes in order to prove that they were
[00:55:59.700 --> 00:56:04.980]   right in the first place. And and once you've sort of opened up the legal process and the expensive
[00:56:04.980 --> 00:56:12.820]   reminds me of fair use. There are there are fantastic similarities to sort of the fair use
[00:56:12.820 --> 00:56:18.100]   thing. Because fair use means you go to court. Right. I mean, that's the famous Larry Lessig line
[00:56:18.100 --> 00:56:22.100]   is fair use just means the right to hire a lawyer. Yeah. Right. Because that's that's all it is.
[00:56:22.100 --> 00:56:25.620]   And you're going to have to fight over it. And because of that, what happens? We really don't
[00:56:25.620 --> 00:56:30.180]   have fair use. It's a chilling effect. Yeah. I am very careful about what we put on this podcast
[00:56:30.180 --> 00:56:35.300]   because YouTube will take it down. Even though it's fair, I know it's fair use. And if I asked
[00:56:35.300 --> 00:56:40.100]   them, they'd say so. Certainly a lawyer would doesn't matter. It's a chilling effect because I
[00:56:40.100 --> 00:56:45.700]   don't go to go to court to defend. Right. So almost any reform and I would say every
[00:56:45.700 --> 00:56:51.700]   reform that I've seen of 230 really does that. It basically just means you're going to have to go
[00:56:51.700 --> 00:56:55.940]   through a very long and very expensive court process, which means many companies won't. They'll
[00:56:55.940 --> 00:57:01.140]   just back down. Oh, and it sort of gives gives people a hecklers veto that would stop whatever it
[00:57:01.140 --> 00:57:04.580]   is those companies are doing. That's a really good scooter. That's an excellent point.
[00:57:05.620 --> 00:57:10.180]   Because just even slicing a sliver off means suddenly, well, now the court has to decide.
[00:57:10.180 --> 00:57:17.460]   So you really by by by not keeping it a integral whole, the whole is what protects you against
[00:57:17.460 --> 00:57:24.020]   frivolous lawsuits. Any sliver, does that make sense? Oh, and any sliver, then now suddenly we
[00:57:24.020 --> 00:57:30.900]   have to adjudicate. Well, I think, you know, I think that Mike does an excellent point.
[00:57:31.620 --> 00:57:34.580]   Mike does an excellent job of untangling. Like, so good. I know.
[00:57:34.580 --> 00:57:37.620]   It's actually in section 230, you know, a verse to say the First Amendment.
[00:57:37.620 --> 00:57:44.180]   And, you know, they definitely are intertwined in that, you know, like your First Amendment right
[00:57:44.180 --> 00:57:51.940]   to put something on your website or not is kind of, you know, is fundamental to understanding how
[00:57:51.940 --> 00:58:00.900]   section 230 plays out. But, you know, I do wonder if what we're after is the underlying
[00:58:00.900 --> 00:58:08.340]   speech or conduct and putting the, you know, putting the right responsibility, the right liability
[00:58:08.340 --> 00:58:15.140]   where it belongs. We do have to look at these cases where it's, you know, is this really speech?
[00:58:15.140 --> 00:58:21.620]   Is this really, you know, and I would say, and this is more about the Snap versus Lemon case,
[00:58:22.340 --> 00:58:28.820]   you don't want to carte blanche to let companies create anything they want. And, you know, you do
[00:58:28.820 --> 00:58:33.620]   want some product liability. This, I mean, you don't want to say section 230 extends so far
[00:58:33.620 --> 00:58:37.300]   that a company could do anything it wants. And if somebody does something stupid, it's their fault.
[00:58:37.300 --> 00:58:41.220]   I think that I think that the thing is, is that the company's doing something is one thing.
[00:58:41.220 --> 00:58:44.660]   Their users doing something. So I think that's the important piece is when their user puts
[00:58:44.660 --> 00:58:48.980]   something on their site, then that protects the company from that when the company does something.
[00:58:48.980 --> 00:58:53.780]   And I've worked with a lot of these companies. And so they pay a lot of attention to liability
[00:58:53.780 --> 00:58:57.540]   because generally if you, this is a very different thing. Well, and that was the debate, though,
[00:58:57.540 --> 00:59:02.180]   talking about claiming that was the debate. And court was, did Google's algorithm was at the
[00:59:02.180 --> 00:59:08.820]   company doing something, right? Yeah. And I think that, you know, it's, it's, the company creating a
[00:59:08.820 --> 00:59:13.380]   product that affects many, many things and may inadvertently affect something that they're continuing
[00:59:13.380 --> 00:59:18.100]   to work on. I think is one thing, them creating a contest that obviously drives people towards an
[00:59:18.100 --> 00:59:22.740]   unhealthy behavior is a different thing. And I think that the, I do think that an algorithm
[00:59:22.740 --> 00:59:27.300]   that is generally working for most people and happen to do something which they haven't even
[00:59:27.300 --> 00:59:33.780]   proven yet, which is probably, you know, something that is very hard to the plan. And again, I think
[00:59:33.780 --> 00:59:38.580]   that I think the 230 is imperfect. I think that it probably could have been written better with
[00:59:38.580 --> 00:59:42.820]   hindsight. They were writing it. We have to remember that 230 was written when they had no idea what
[00:59:42.820 --> 00:59:48.180]   was going to happen. And like it was. And so now we've built it is, but now it is a cornerstone,
[00:59:48.180 --> 00:59:54.980]   you know, or, you know, of a giant building which forms the world's economy. And you're talking about
[00:59:54.980 --> 00:59:59.780]   like, well, that thing was a little off. It should have been, why don't we just take the centerpiece,
[00:59:59.780 --> 01:00:04.740]   this part out of it. And, and oh, if it, again, the consequences is, man, we don't get it right.
[01:00:04.740 --> 01:00:07.300]   It'll have the, like the entire corner is over.
[01:00:09.220 --> 01:00:13.860]   You know, it's like saying, it's like, like, yeah, I think we, I think we're going to change and have
[01:00:13.860 --> 01:00:19.380]   the, the gas be something slightly different. And all engines and all, you know, all gas stations
[01:00:19.380 --> 01:00:24.340]   have to change if we do this, or someone can pass a law that forces everybody to change everything
[01:00:24.340 --> 01:00:27.380]   across the United States. Now you're talking formula one. That's a different matter.
[01:00:27.380 --> 01:00:31.140]   No, but I mean, but we're going to change the formula of gas. And we're going to just say that
[01:00:31.140 --> 01:00:35.140]   that's, you know, we'll just tweak it a little bit and see how it works. And, you know, it's a big
[01:00:35.140 --> 01:00:38.740]   economy that, that you're turning, you're starting about turning it over. So I think that I think it's
[01:00:38.740 --> 01:00:43.220]   so dangerous. Like it is, it's just, it's, it's again, what we call fissionable material. Like,
[01:00:43.220 --> 01:00:47.380]   you can really cause so much damage. There was a quote I saw yesterday that
[01:00:47.380 --> 01:00:54.260]   Section 230 is the load bearing wall of the internet. And, and I think there's some, some truth
[01:00:54.260 --> 01:00:59.140]   to that. And just to, to make up an important point here, you know, when you start to talk about
[01:00:59.140 --> 01:01:03.300]   the difference between speech and conduct, which is where a lot of these discussions are going and
[01:01:03.300 --> 01:01:08.100]   sort of where, where I think Owen was going with his comments and, and where like the
[01:01:08.100 --> 01:01:13.940]   Julia Angwin, New York Times opinion piece last week was saying, Oh, separate out, you know, speech
[01:01:13.940 --> 01:01:19.620]   and conduct. You know, the problem is that every single plaintiff and every single plaintiff's lawyer
[01:01:19.620 --> 01:01:23.540]   know exactly what they'll do in that case. And that is they will declare everything to be
[01:01:23.540 --> 01:01:26.980]   conduct rather than speech. And then you're at that point that I was talking about where you have
[01:01:26.980 --> 01:01:32.260]   to litigate it. And once you have to litigate it, you've lost all of the value of Section 230.
[01:01:32.260 --> 01:01:36.420]   Even if the defendants in all of these cases are going to win, just the fact that you've now
[01:01:36.420 --> 01:01:41.380]   set it up that you have to litigate the question of 230 and then go to the discovery, perhaps go
[01:01:41.380 --> 01:01:46.420]   to trial, you know, all this stuff that is extremely expensive that will force most companies to
[01:01:46.420 --> 01:01:51.220]   either change their practices or settle cases too early, even if they would have won in the long run
[01:01:51.220 --> 01:01:56.980]   effectively wipes out the, the tremendous value and benefits of Section 230.
[01:01:57.540 --> 01:02:03.780]   Angwin's a piece, which she probably did not write the headline for, but it definitely had a link
[01:02:03.780 --> 01:02:10.660]   bait headline. It's time to tear up big techs, get out of jail free card. That's the kind of
[01:02:10.660 --> 01:02:20.020]   rhetoric that really does spark flames. I wrote a response to this. And, and at first, I actually
[01:02:20.020 --> 01:02:23.940]   had a whole paragraph where I said, you know, she probably didn't write the headline. And I sort
[01:02:23.940 --> 01:02:27.140]   of, you know, ripped apart the headline. But then I was, as I was going through the article,
[01:02:27.140 --> 01:02:32.500]   she basically did say almost exactly that in the article. And so I pulled that out. She might not
[01:02:32.500 --> 01:02:36.900]   have written the headline, but she got pretty close to it in the article. I think, you know, and,
[01:02:36.900 --> 01:02:41.220]   and I have tremendous respect for Julia Angwin. I think she's a wonderful investigative reporter.
[01:02:41.220 --> 01:02:45.780]   I think a lot of her reporting has been really, really important over the last decade in terms of
[01:02:45.780 --> 01:02:50.660]   exposing, you know, questionable activities by all of the big companies. And I think that,
[01:02:50.660 --> 01:02:54.980]   that's, that's a really valuable thing. But I do not think this was her best work. And in fact, I
[01:02:54.980 --> 01:03:01.620]   think it had multiple factual errors. And then it really just did not understand what Section 230
[01:03:01.620 --> 01:03:09.140]   does. And, and her focus on, on how to, what she claimed, fix the law, or take away the get out
[01:03:09.140 --> 01:03:14.020]   of jail free card, which it is not a get out of jail free card. This is a very important point.
[01:03:14.020 --> 01:03:19.700]   It is a put the liability on the actual party that did the wrong thing card. And, and, you know,
[01:03:19.700 --> 01:03:26.020]   get the, the innocent tool provider away from having to, to deal with an expensive litigation,
[01:03:26.020 --> 01:03:32.260]   you know, vexatious litigation card. You know, once you begin to understand those things,
[01:03:32.260 --> 01:03:36.980]   you know, her piece was, was very, very confused and, and misguided.
[01:03:36.980 --> 01:03:40.740]   There's a certain similarity between the first discussion we had about
[01:03:40.740 --> 01:03:46.180]   encryption, both enabling good things and bad things and, and her arguments against Section
[01:03:47.060 --> 01:03:53.700]   230, she, she asserts that Section 230 lets big tech kind of get away with stuff or, or defer
[01:03:53.700 --> 01:03:58.420]   responsibility for things they create. And I agree with you, Mike, it's a little hard to justify.
[01:03:58.420 --> 01:04:05.220]   But it is kind of that argument that there are benefits from 230 and there perhaps our adverse
[01:04:05.220 --> 01:04:12.580]   effects to 230. This is the way of the world. Nothing is 100% good or 100% bad. But we do have
[01:04:12.580 --> 01:04:18.020]   to decide what's important to us. And I think I would say encryption is important to us,
[01:04:18.020 --> 01:04:22.820]   the security and privacy. And 230, I know is important to me. I wouldn't have a chat room.
[01:04:22.820 --> 01:04:27.060]   I wouldn't have a discord. I wouldn't have a forums. I wouldn't have a mastodon.
[01:04:27.060 --> 01:04:32.260]   If I were liable for everything, anything somebody puts there or anything I take down from there.
[01:04:32.260 --> 01:04:37.460]   So in, in the, in the balance, I think that's more important to preserve that discourse on the
[01:04:37.460 --> 01:04:43.540]   internet than it is that Facebook maybe can get away with, you know, redlining for some periods.
[01:04:43.540 --> 01:04:50.020]   You know, all I'd say is that, you know, approximately 46,000 people die in cars every day, every,
[01:04:50.020 --> 01:04:54.900]   every, every, you know, in the United States, they are very useful. Yeah. They, they, no,
[01:04:54.900 --> 01:05:00.820]   just regular cars, people live in their lives, 46,000 people die. Bad things happen because cars
[01:05:00.820 --> 01:05:04.500]   exist, you know, and, and it doesn't, you know, you're not going to get to a point where we get
[01:05:04.500 --> 01:05:09.860]   that to zero. We do though, Alex, regulate, significantly regulate vehicles for that reason.
[01:05:09.860 --> 01:05:14.340]   Primary, and I will admit that the primary regulation of vehicles is lawsuits,
[01:05:14.340 --> 01:05:20.180]   you know, like liability is the primary way that vehicles are, are, are regulated. But at the same
[01:05:20.180 --> 01:05:23.940]   time, what I would say is that there's nothing in a nut there. You're not going to ever be perfect.
[01:05:23.940 --> 01:05:27.620]   You know, you're not going to, you know, you're not going to get no laws going to. Yeah. Yeah.
[01:05:27.620 --> 01:05:32.260]   And what we try to do is not have laws cause more damage than they cause good. And it's unclear.
[01:05:32.260 --> 01:05:39.700]   And it's just, you know, you're, we have something that is working fairly well right now,
[01:05:39.700 --> 01:05:45.860]   very imperfect. But the, the consequences of changing this is so radical that it would be,
[01:05:45.860 --> 01:05:50.100]   I think it'd be reckless. And I think that it sounded like the Supreme Court was leaning that
[01:05:50.100 --> 01:05:54.340]   Yeah. We never know. Mike, you made that point in your article that you can't tell from oral
[01:05:54.340 --> 01:05:59.300]   arguments what they're going to do. But I was quite relieved. And I know I'm not alone,
[01:05:59.860 --> 01:06:04.020]   that they sounded like at least they acknowledged their, their limits, the difficulty of this and
[01:06:04.020 --> 01:06:10.180]   the potential hazards of it. They seem to, they seem pretty clear. It certainly could have been
[01:06:10.180 --> 01:06:14.180]   much worse, but, but still like the devil's very much in the details. They could do one little
[01:06:14.180 --> 01:06:18.820]   thing that would be disastrous. Right. It doesn't have to. Yeah. Yeah. And that's the real fear at
[01:06:18.820 --> 01:06:23.380]   this point, which is that, you know, they could, they, like many people see is like, Oh, well,
[01:06:23.380 --> 01:06:28.660]   a little tweak won't matter that much. And they don't quite understand, you know, how that would
[01:06:28.660 --> 01:06:33.220]   play out. Nobody understands exactly how that play will play out. Those of us who spend a lot of time
[01:06:33.220 --> 01:06:37.300]   looking at this can sort of game out where we think a lot of these things are going to go and
[01:06:37.300 --> 01:06:43.060]   how much damage they would do. But, you know, so the real fear at this point, they do recognize it.
[01:06:43.060 --> 01:06:47.620]   It was definitely like there was a sort of sigh of relief in listening to the oral arguments and
[01:06:47.620 --> 01:06:53.380]   seeing what they were saying that they weren't leaning towards like a full scale, like 230 doesn't
[01:06:53.380 --> 01:06:58.900]   apply at all. And in fact, like everybody sort of expected that that Justice Thomas and Justice
[01:06:58.900 --> 01:07:03.220]   Alito in particular were probably the main reasons why this case was being heard because it was kind
[01:07:03.220 --> 01:07:08.580]   of a surprise. People didn't expect it. It wasn't a strong case to take this case. Terrell case.
[01:07:08.580 --> 01:07:14.660]   Yeah. Yeah. And so, and yet they were also really skeptical of the plaintiff's argument. And so
[01:07:14.660 --> 01:07:19.940]   that was kind of eye opening. The thinking was that politically they wanted some excuse to overturn
[01:07:19.940 --> 01:07:26.980]   230. And so, they were casting around for any 230 case they could, and they gave cert to this case,
[01:07:26.980 --> 01:07:32.020]   even though it was fairly weak case. I mean, the plaintiffs don't even assert that the
[01:07:32.020 --> 01:07:36.580]   people who killed their daughter. I'm sorry about that. Yeah. We're watching YouTube videos.
[01:07:36.580 --> 01:07:40.900]   There's like, it's like, yeah, I think that the easiest way out with the Supreme Court is just
[01:07:40.900 --> 01:07:46.980]   declared there's no standing. Well, Kathy said that Kathy said even they could even at this point
[01:07:46.980 --> 01:07:50.340]   say, oh, we made a mistake. We shouldn't have taken this case. Forget it.
[01:07:50.340 --> 01:07:55.300]   They can. It's called dig, right? It's a,
[01:07:55.300 --> 01:07:59.540]   improvidently dismissed, very improvidently granted. I love that. I love that.
[01:07:59.540 --> 01:08:04.980]   We could dig the case, which is basically, we never should have taken this. Yeah. And I mean,
[01:08:04.980 --> 01:08:10.340]   which would honestly probably be the best result, but also like kind of frustrating when you look at
[01:08:10.340 --> 01:08:16.740]   how much time everybody spent, you know, there were whatever 90 amicus briefs or whatever,
[01:08:17.140 --> 01:08:21.460]   filed in this case and everything that made your law firm. Yes. Representing me. Right.
[01:08:21.460 --> 01:08:27.300]   Right. Would it open the door for yet another 230 case to come through if they dig it?
[01:08:27.300 --> 01:08:34.900]   Yes. Yes. And that's the problem. We want to resolve this. We want to affirm affirmation of 230.
[01:08:34.900 --> 01:08:40.740]   It would be, but the chances are that happening in this case is actually pretty slim as well.
[01:08:40.740 --> 01:08:45.060]   And, you know, what is going to happen almost certainly next term. And some people will say,
[01:08:45.060 --> 01:08:49.140]   these are not actually 230 cases, though I would argue they really are 230 cases,
[01:08:49.140 --> 01:08:55.780]   is that the Florida social media content moderation law and the Texas content moderation law,
[01:08:55.780 --> 01:08:59.860]   like those were both supposed to be heard by the Supreme Court this term and they sort of
[01:08:59.860 --> 01:09:04.980]   punted on it and asked the US government to weigh in, which was weird because it's unclear what the
[01:09:04.980 --> 01:09:08.260]   US government is going to say that is going to influence the Supreme Court's decision or whether
[01:09:08.260 --> 01:09:12.580]   not to hear these cases. But really it seemed like that was a way to say like, let's not deal with
[01:09:12.580 --> 01:09:18.900]   this until next term. And that's going to be an even bigger and more important series of cases.
[01:09:18.900 --> 01:09:23.300]   Again, with the caveat, like that could change depending on what the Supreme Court decides in
[01:09:23.300 --> 01:09:26.820]   the Gonzales case when it comes out in most likely June.
[01:09:26.820 --> 01:09:29.860]   All right. This just makes you all and glad that you don't write about tech that much anymore.
[01:09:29.860 --> 01:09:38.660]   No, it actually makes me think that it's a great illustration of when this law was first
[01:09:38.660 --> 01:09:44.020]   written and passed in the 90s, we were really thinking about a model of like internet service
[01:09:44.020 --> 01:09:47.460]   providers and like users posting on home pages or use.
[01:09:47.460 --> 01:09:49.220]   This was the copy survey. Right.
[01:09:49.220 --> 01:09:55.700]   Yeah. And it really did not embrace the idea of like massive scale algorithmic recommendation.
[01:09:55.700 --> 01:10:03.300]   And even as this is being heard and potentially decided on like that set of facts, that generation
[01:10:03.300 --> 01:10:11.300]   of technology, we are moving forward to a generate of AI. And I'm really curious, like, who is the
[01:10:11.300 --> 01:10:19.700]   author of speech generated by a chat bot? Is it, you know, is it open AI for chat GPT?
[01:10:19.700 --> 01:10:25.300]   Is it the user who is feeding in prompts and therefore operating a program and usually,
[01:10:25.300 --> 01:10:29.700]   you know, the operator of a program that produces output is thought of as the author of that,
[01:10:30.260 --> 01:10:36.420]   that output. News organizations are already talking about potentially suing open AI for,
[01:10:36.420 --> 01:10:42.260]   you know, chat GPT incorporating their work. I mean, there's going to be a whole bunch of
[01:10:42.260 --> 01:10:49.540]   issues that are coming up now that are not going to be addressed by whatever is decided in these
[01:10:49.540 --> 01:10:55.460]   cases. Yeah. Yeah. And that actually came up a little bit in the case. And Neil Gorsuch brought up
[01:10:56.660 --> 01:11:01.060]   the AI and the generative AI aspect to it and wondering whether or not the Gonzalez versus
[01:11:01.060 --> 01:11:05.860]   Google case actually, you know, how that impacted those discussions, because certainly when that
[01:11:05.860 --> 01:11:11.300]   case was brought, we didn't have chat GPT and all of these things out there. And so now it is a
[01:11:11.300 --> 01:11:17.220]   really big question. And there was an article last week also on law fair by Matt Paralt claiming
[01:11:17.220 --> 01:11:24.820]   that section 230 doesn't protect chat GPT output. And I'm not sure that that article is correct.
[01:11:24.820 --> 01:11:29.780]   Yeah. And a lot of people read that and sort of were promoting it and saying, oh, okay, well,
[01:11:29.780 --> 01:11:37.620]   this is settled. And I don't think I necessarily agree with his analysis. I think it is more of an
[01:11:37.620 --> 01:11:43.940]   open question that, you know, I think basically all of the points that Owen brought up are unclear.
[01:11:43.940 --> 01:11:48.260]   And, you know, is it based on the prompts? And if it's based on the prompts, is that user generated
[01:11:48.260 --> 01:11:55.780]   content? I'm not sure we know that yet. And I think I think it's a lot more fuzzy than than
[01:11:55.780 --> 01:12:00.260]   we know right now. And I think it is going to be a big battle that the courts are going to have to
[01:12:00.260 --> 01:12:06.020]   deal with in the same way that they're dealing with the question of whether or not output from
[01:12:06.020 --> 01:12:11.220]   these things are copyrightable or whether or not the inputs to these to the large language learning
[01:12:11.220 --> 01:12:17.140]   models or whatever are similarly copyrighted both. There are a lot of really big legal questions raised
[01:12:17.140 --> 01:12:22.500]   by the general of AI space, both for copyright and for 230 and for a variety of other things.
[01:12:22.500 --> 01:12:28.340]   And there will be no shortage of stuff for you to be talking about Leo over the next few years.
[01:12:28.340 --> 01:12:34.900]   Matt's contention is that in effect, large language models and chat GPT are information
[01:12:34.900 --> 01:12:40.660]   content providers so protected. Wow, that's a bit of a leap. Yeah.
[01:12:41.860 --> 01:12:46.820]   I'm not sure his I mean, it's it's an interesting article. It's thought provoking.
[01:12:46.820 --> 01:12:52.340]   And a lot of people were convinced by it. I am not sure that the courts will see it the same way.
[01:12:52.340 --> 01:12:55.460]   Yeah. Similarly, the copyright office says you can't
[01:12:55.460 --> 01:13:01.860]   we can't copyright a mid journey drawing nor can mid journey copyright a mid journey drawing it.
[01:13:01.860 --> 01:13:08.260]   Go ahead, Noah. You couldn't theory you couldn't theory copyright. The prompt. The prompt is unique.
[01:13:08.260 --> 01:13:14.980]   The prompt is creative enough to register for copyright. It's it's it'll be very interesting.
[01:13:14.980 --> 01:13:18.980]   Sorry, I didn't mean to cut off. We live in interest. No, no, no, no. I think there is
[01:13:18.980 --> 01:13:26.100]   absolutely going to be a product liability lawsuit over over generative AI. So I'm answer that one
[01:13:26.100 --> 01:13:31.860]   of these chatbots gives is going to be acted on by someone and then they are going to sue the.
[01:13:31.860 --> 01:13:35.620]   Well, I'll give you a case company that operates the bot. I'll give you a case you could file right
[01:13:35.620 --> 01:13:43.700]   now. Mal script kitties who've used chat GPD to write malware and and use it. And if I were one of
[01:13:43.700 --> 01:13:49.060]   those people attacked by malware, I'd sue the script kitties and I'd sue open AI because they
[01:13:49.060 --> 01:13:57.140]   wrote it. Yeah, I'm not sure the court would would I mean against the actual person who who used
[01:13:57.140 --> 01:14:03.540]   the malware used it. Sure. Sure. But it wrote it. But but the the the AI generator, I don't think
[01:14:03.540 --> 01:14:07.780]   you would win that case. I think you would lose. Well, I'm not going to file it. So there.
[01:14:07.780 --> 01:14:13.620]   Let's take a break. I can't afford it. So that's why I'm going to do another ad. And maybe I can't
[01:14:13.620 --> 01:14:18.500]   after this ad. How about that? Okay. We have a what a great panel. Alex Lindsey, so nice to have
[01:14:18.500 --> 01:14:23.460]   you in a different venue. So to speak, it's fun to be here. It's fun to talk about something other
[01:14:23.460 --> 01:14:29.220]   than just Max. Yes. I love Max, but it's fun to have a wider conversation. Exactly. Yeah, I feel
[01:14:29.220 --> 01:14:35.700]   the same way. Oh, nine Oh, dot media to hire him. Office hours, Doc Global to get to know him.
[01:14:35.700 --> 01:14:40.820]   How about that? Yeah, it's right. Oh, and Thomas, the Herb Kane of Silicon Valley.
[01:14:40.820 --> 01:14:45.380]   Calling us at the examiner. I love your column. Second, we're going to talk about
[01:14:45.380 --> 01:14:51.620]   one of them. Hayes Valley is the center of AI in San Francisco. Okay, we're going to have to
[01:14:51.620 --> 01:14:56.740]   dig down on the on that. Oh, yes. Let's let's dig in. Great to have you, Owen. And of course,
[01:14:56.740 --> 01:15:01.620]   Mike Masnick, who it's always a privilege. We can get you on Mac. I'm like, I know you're a busy guy,
[01:15:01.620 --> 01:15:07.220]   but boy, what a what a great week to have you and Kathy Gellison on two different shows.
[01:15:07.220 --> 01:15:12.100]   Because I feel like I'm much better informed on all this stuff than I would have been otherwise.
[01:15:12.100 --> 01:15:17.140]   I mean, you you're really good. Really. Always nice to hear. Yeah. Our show they brought to you
[01:15:17.140 --> 01:15:24.660]   by worldwide technology. So last trip, Lisa and I took before COVID was March 2020.
[01:15:25.620 --> 01:15:31.060]   Before shutdown began, we went out to Missouri to look at the advanced technology center that
[01:15:31.060 --> 01:15:36.980]   WWT has. Were you that you were there too? Weren't you Alex? Did you come up with us? I think you did.
[01:15:36.980 --> 01:15:41.780]   Yeah, it was great. Well, not only that, I I went there. We spoke. It was a.
[01:15:41.780 --> 01:15:46.980]   We did a great. Amazing location. Yeah. Great panel. And really, I was, I didn't know that
[01:15:46.980 --> 01:15:52.260]   WWT was there. Meaning, and I've I've actually had the opportunity to build projects with them.
[01:15:52.260 --> 01:15:56.660]   Oh, nice. You know, so actually do projects after that. We got together and built some apps.
[01:15:56.660 --> 01:16:02.260]   Oh, that's wonderful. Yeah. So and and they are just amazing to work with. Like, I just have to
[01:16:02.260 --> 01:16:05.940]   say that I was thoroughly and but now every time I think about something, I'm like, well, then we'll
[01:16:05.940 --> 01:16:11.540]   have WWT do this. Yeah. It's just it just they they provide. I mean, they're super organized,
[01:16:11.540 --> 01:16:17.060]   super high level brings me to the ground of like, I got a big idea. And then they just figure it
[01:16:17.060 --> 01:16:22.500]   all out. They somewhat hide their light beneath a bushel. For instance, that app development,
[01:16:22.500 --> 01:16:27.220]   we never talk about that. But yeah, they do app development. That's pretty amazing.
[01:16:27.220 --> 01:16:34.180]   Yeah, we went we went to the out there. Mary Jo Foley was there too. We saw the advanced
[01:16:34.180 --> 01:16:38.900]   technology center. This was a it's funny. This is the history of WWT. Oh, I should explain what
[01:16:38.900 --> 01:16:45.380]   they do probably before we go much further. They they partner with big companies for enterprise
[01:16:45.380 --> 01:16:48.900]   technology. So if you need storage, if you're going to the cloud, if you want hybrid cloud,
[01:16:48.900 --> 01:16:55.380]   you know, if you want security, if you need apps done, they do so many things. As I said,
[01:16:55.380 --> 01:16:59.460]   I think they kind of hide their light under a bushel. They do so many things. But at the heart
[01:16:59.460 --> 01:17:04.340]   of WWT is this advanced technology center. It's kind of amazing. One little building when they
[01:17:04.340 --> 01:17:10.340]   started 10 years ago, it's spread and spread now. It's a multiple buildings rack after rack.
[01:17:10.980 --> 01:17:17.220]   Basically all half a billion dollars worth of equipment, all of the leading OEMs, all of the
[01:17:17.220 --> 01:17:21.620]   little emerging disruptors. And why do they do that? Well, initially did it because their
[01:17:21.620 --> 01:17:24.900]   engineers needed it so they could get to know the technologies, they could spin up proofs of
[01:17:24.900 --> 01:17:29.780]   concepts so they could, you know, they could build projects for their clients. But then they
[01:17:29.780 --> 01:17:33.380]   had this brilliant insight. This is a couple of years ago. It's actually after we visited them,
[01:17:33.380 --> 01:17:38.980]   they decided to open it up to everybody. They virtualized it. And now anybody who's a member of
[01:17:38.980 --> 01:17:45.140]   the ATC platform, and by the way, that's free, can access the labs remotely from anywhere in
[01:17:45.140 --> 01:17:50.820]   the world anytime of the day or night. The Advanced Technology Center has hundreds of on-demand
[01:17:50.820 --> 01:17:56.740]   and schedulable labs featuring solutions that include technologies representing the newest
[01:17:56.740 --> 01:18:03.620]   advances in every area of enterprise technology, cloud, security, networking, primary and secondary
[01:18:03.620 --> 01:18:10.100]   storage data analytics, AI, of course, DevOps, and so much more.
[01:18:10.100 --> 01:18:16.580]   WWTs engineers and partners use the ATC to spin up proofs of concept and pilots,
[01:18:16.580 --> 01:18:21.860]   which helps customers confidently select the best solutions, cuts evaluation time for months to
[01:18:21.860 --> 01:18:27.540]   weeks. That's one of the things that makes WWT so special, so nimble. The thing I should really say
[01:18:27.540 --> 01:18:33.940]   about WWT is their business people too. So yeah, they're technologists, but they understand that
[01:18:33.940 --> 01:18:39.860]   all this technology has to go hand in hand with your business strategy. It is not existing in a
[01:18:39.860 --> 01:18:46.740]   vacuum. And so they're very good about understanding what your goals are and making it work. And you
[01:18:46.740 --> 01:18:52.180]   can do the same with the ATC, test out products and solutions before you go to market. It's more
[01:18:52.180 --> 01:18:58.740]   than just the labs you can do. There's technical articles, white papers, expert insights, demonstration
[01:18:58.740 --> 01:19:05.060]   videos. And of course, all the community stuff they do like that panel that Alex, Mary, Joe, and I
[01:19:05.060 --> 01:19:12.980]   did all those years ago, we got to go out there again. WWT makes us available to everybody, the ATC
[01:19:12.980 --> 01:19:21.220]   platform. It can be yours. All you have to do is go to www.wwt.com/twit. It's free. Check out
[01:19:21.220 --> 01:19:26.180]   WWT's events and communities too for all sorts of ways you can learn about technology trends.
[01:19:26.180 --> 01:19:29.860]   Hear about the latest research and insights from their experts. This is this company's a partner.
[01:19:29.860 --> 01:19:34.340]   That's that's their goal. They want to be your partner. They want to help you understand this
[01:19:34.340 --> 01:19:39.780]   how it fits in with your strategy. Whatever your business needs, WWT can deliver scalable,
[01:19:39.780 --> 01:19:45.780]   tried and tested tailored solutions. Worldwide technology brings strategy and execution together
[01:19:45.780 --> 01:19:51.220]   to make that new world happen for you. Learn more about WWT, the Advanced Technology Center, gain
[01:19:51.220 --> 01:19:56.900]   access to all the free resources. It's really simple. Just go to www.wwt.com/twit. Create a free
[01:19:56.900 --> 01:20:06.100]   account on the ATC platform. It's time to go out there. I miss the fried ravioli. www.wwt.com/twit.
[01:20:06.100 --> 01:20:11.220]   We thank WWT for all the support. All this time, they've been a great partner with us.
[01:20:12.100 --> 01:20:18.980]   Worldwide technology. Let's see. I think we did our Supreme Court. There were actually two cases.
[01:20:18.980 --> 01:20:27.060]   Did the second case not impact Section 230, Mike? That was the impression I got on the Wednesday
[01:20:27.060 --> 01:20:33.860]   case. The second case, the fact pattern behind it was actually almost identical to the first case.
[01:20:33.860 --> 01:20:42.020]   It was about Twitter, right? Twitter and another terrorist attack that killed someone.
[01:20:42.020 --> 01:20:47.380]   There have been maybe half a dozen of these kinds of cases. It's just that these two were the ones
[01:20:47.380 --> 01:20:54.180]   that went to Supreme Court. It does feel like Scalia. Not Scalia. He's been replaced.
[01:20:54.180 --> 01:21:02.420]   The Clarence Thomas and Allioto. I'm getting old in my old age. Allioto's former mayor of San
[01:21:02.420 --> 01:21:09.940]   Francisco, Elito, had cherry picked a couple of cases. But maybe didn't do the best cherry picking.
[01:21:11.220 --> 01:21:19.460]   And so the Tamna versus Twitter case technically did not cover Section 230 because of the way that
[01:21:19.460 --> 01:21:25.380]   the Ninth Circuit ruled on it. That case was more direct on the question of whether or not
[01:21:25.380 --> 01:21:33.300]   it was, I forget what the terrorist act is, the ATA helping terrorists.
[01:21:33.300 --> 01:21:39.060]   And so that was more direct on that. And so not directly on point related to
[01:21:39.060 --> 01:21:42.580]   Twitter. They were comparing Twitter to a bank that gives terrorists money.
[01:21:42.580 --> 01:21:50.580]   The Twitter by putting ISIS tweets up was in effect aiding and abetting terrorism.
[01:21:50.580 --> 01:21:57.940]   And I think that's a long way to go. Yeah. It's an argument that is unlikely to succeed. But in some
[01:21:57.940 --> 01:22:03.780]   ways was kind of interesting because it sort of presented the world in which you don't have 230
[01:22:03.780 --> 01:22:08.260]   and which you have to go through that argument. And you have to, it's kind of what I was talking
[01:22:08.260 --> 01:22:13.060]   about before, where if you slice away 230, then suddenly you have to go through these more detailed,
[01:22:13.060 --> 01:22:17.700]   much more expensive arguments in court about whether or not, you know, who is liable for what,
[01:22:17.700 --> 01:22:22.980]   which part and why. And so that's really what we saw was like these crazy analogies that were
[01:22:22.980 --> 01:22:26.820]   happening and everybody debating back and forth. Is it like a bank? Is it like something else?
[01:22:26.820 --> 01:22:32.980]   Which is what you would have to see in all of these other cases that, you know, really silly cases
[01:22:32.980 --> 01:22:39.780]   that thankfully got dismissed because of section 230. So the relationship to 230 with the Tamna case
[01:22:39.780 --> 01:22:44.900]   is effectively like, here's what the unfortunate situation the world would look like if we got
[01:22:44.900 --> 01:22:49.620]   rid of 230 is we'd be arguing about all these things in every single case where something went
[01:22:49.620 --> 01:22:55.220]   wrong and someone tried to blame the internet for it. Yeah. Well, we'll see. It's going to be what
[01:22:55.220 --> 01:23:02.900]   June, maybe before we get these decisions. Almost certainly June. In theory, it could come anytime
[01:23:02.900 --> 01:23:08.820]   between now and the decision. Right. They could. But chances are with like big
[01:23:08.820 --> 01:23:15.860]   cases like this, where there's certainly going to be some debate and discussion in the chambers,
[01:23:15.860 --> 01:23:22.260]   it almost certainly will come out sometime in June. Okay. Well, we'll wait with baited breath and
[01:23:22.260 --> 01:23:27.620]   it could go, it could regardless of what they said at the oral arguments, it could go badly for
[01:23:27.620 --> 01:23:35.540]   the internet. Yes. Then we'll see. So enjoy, enjoy your mastodon discord discourse,
[01:23:35.540 --> 01:23:40.820]   IRC. Well, you got it, kids. Well, that's the irony of this. Everybody talks about big tech,
[01:23:40.820 --> 01:23:45.300]   but it, but big tech could support these lawsuits. It's people like us, you and me,
[01:23:45.300 --> 01:23:51.780]   my, that would just that's why, you know, the amicus brief that we submitted that that Kathy
[01:23:52.580 --> 01:23:58.340]   wrote for us. It wasn't just for me. We also had Chris Riley on it and Chris runs a
[01:23:58.340 --> 01:24:02.900]   mastodon instance tech policy dot social for people in the sort of tech policy world. And we
[01:24:02.900 --> 01:24:08.500]   thought it was really important for the Supreme Court to be reminded that section 230 also protects
[01:24:08.500 --> 01:24:13.300]   whoever is running a mastodon instance. Right. And so that was, you know, they
[01:24:13.300 --> 01:24:17.460]   somewhat want to publish punish big tech, right? That's what the right right wants to do is punish
[01:24:17.460 --> 01:24:25.140]   big tech. And the left, frankly, there's, I forget which bill it was, but one bill attempted to
[01:24:25.140 --> 01:24:31.460]   impose like moderation requirements based on the size of a company measured by market cap. And
[01:24:31.460 --> 01:24:37.380]   of course, 50 million, yeah, you know, in the course of, I think was Twitter and
[01:24:37.380 --> 01:24:45.060]   that I think 500 billion, I think there are market cap actually fell, right, whatever the
[01:24:45.060 --> 01:24:49.540]   threshold was. So it's like, you know, they were trying to go after big tech and ended up being
[01:24:49.540 --> 01:24:53.780]   medium tech instead. And it's like, it's a good example. We know here in California, we have this
[01:24:53.780 --> 01:25:02.180]   thing AB five, which they aimed at Uber and Lyft, missed them and just caused mass chaos.
[01:25:02.180 --> 01:25:07.780]   Yeah. Like my resistance to things like 230 are definitely driven from AB five because it
[01:25:07.780 --> 01:25:12.900]   impacts my business a lot. Right. A lot of our freelancers, look, we wouldn't have an
[01:25:12.900 --> 01:25:18.100]   adequate his job because he was living in California. He couldn't work at tech Republic
[01:25:18.100 --> 01:25:23.460]   because he couldn't be a freelancer because AB, whatever. And AB five was such a disaster.
[01:25:23.460 --> 01:25:29.860]   It is literally shaking my, my, my belief in government regulation. It was just so bad. Like,
[01:25:29.860 --> 01:25:34.420]   it was, and when I think about what could happen with 230, I think of AB five. And it's so bad
[01:25:34.420 --> 01:25:37.780]   that they won't repeal it because they don't know and wants to talk about it anymore. Like,
[01:25:37.780 --> 01:25:41.460]   no one wants to, like, no one even wants to have it come up in the press because they,
[01:25:41.460 --> 01:25:45.860]   they screwed it up so badly. And, and so, but I think that what, what happened was they missed
[01:25:45.860 --> 01:25:50.100]   the target. Just like we're talking here, they completely missed the targets and then just, but
[01:25:50.100 --> 01:25:56.580]   just hailed every, you know, like, just buckshot across the entire every industry. It's just, it's
[01:25:56.580 --> 01:26:01.140]   complete disaster. Then they started putting in all these, all these exemptions for
[01:26:01.140 --> 01:26:07.620]   industries that had powerful lobbyists in Sacramento. And, you know, that's kind of a
[01:26:07.620 --> 01:26:12.100]   direction you can see if they start, we followed very carefully in the company that I work in.
[01:26:12.100 --> 01:26:16.820]   We, we follow AB five to the letter. So do we have to, we have to, and you followed the letter,
[01:26:16.820 --> 01:26:19.220]   but I was talking to someone, like, I was talking to someone in Hollywood and they're like, oh,
[01:26:19.220 --> 01:26:21.620]   no one, no one pays attention to that. Everyone just ignores it. And I was like, why did they
[01:26:21.620 --> 01:26:26.100]   ignore it? And I'm like, because it's a really bad idea. And we, and we make videos. Like, like,
[01:26:26.100 --> 01:26:30.260]   the thing is, is that like, you know, you, you, you, everyone, we're gonna take a,
[01:26:30.260 --> 01:26:32.340]   they go after us. We will bury them. You know, like,
[01:26:32.980 --> 01:26:37.940]   oh, good. Maybe I can ignore it then. That's Ares. You know, and I'd, I wouldn't, but you know what
[01:26:37.940 --> 01:26:42.500]   I'm saying? Like we're, you're kind of a freelancer these days. Do you have to pay attention to this?
[01:26:42.500 --> 01:26:49.220]   Is this hurt your business? It has, it has not come up there. There was a, there was an issue
[01:26:49.220 --> 01:26:56.420]   with freelancers that I think got, got resolved with, with either like an amendment to the, the law,
[01:26:56.420 --> 01:27:02.020]   or like a different interpretation. There was a crazy situation where like, you know, if you were
[01:27:02.020 --> 01:27:07.700]   a videographer, it didn't apply. But if you were a photographer, it did based on the number of
[01:27:07.700 --> 01:27:14.020]   works. And then like the question about like, is a photo shoot, you know, one work or is like
[01:27:14.020 --> 01:27:18.980]   each individual photo. And if you bring your computer, like there was some, we've had some,
[01:27:18.980 --> 01:27:22.980]   some folks that are like, if they bring their laptop, they're, they're no longer fitting under
[01:27:22.980 --> 01:27:26.980]   AB5. Like as long as they bring their laptop and their laptop, they do work on it. And it's just,
[01:27:26.980 --> 01:27:31.060]   it's just, and this just shows you like government run or run awry. So,
[01:27:31.060 --> 01:27:41.060]   Twitter, speaking thereof, just came in a couple hours ago. Yesterday we heard that 50 people were
[01:27:41.060 --> 01:27:48.340]   fired now, according to Zoe Schiffer on platformer confirmed by Alex Heath at the Verge. We learn
[01:27:48.340 --> 01:27:54.100]   who some of those people are including the woman in charge of Twitter blue, Esther Crawford,
[01:27:54.980 --> 01:28:00.500]   and most of the Twitter blue team fired yesterday. Things are going well.
[01:28:00.500 --> 01:28:10.740]   Like then by the numbers, you know, this thing is never going to be a meaningful contributor
[01:28:10.740 --> 01:28:15.860]   to Twitter. So it was something like 100,000 people have signed up for this $8 a month plan.
[01:28:15.860 --> 01:28:22.820]   Right. And the amount of like angst and, you know, agonizing that, you know, that's gone on over
[01:28:22.820 --> 01:28:28.980]   Twitter blue and the verification component of it and all that. And you know, it's, it's just,
[01:28:28.980 --> 01:28:32.180]   it's not a meaningful contributor to the business. It's the bottom line.
[01:28:32.180 --> 01:28:38.420]   Well, and there's some speculation the Verge is speculating that maybe this is the beginning of
[01:28:38.420 --> 01:28:45.860]   what some have expected of Elon just putting in his whole team, you know, a loyal to Elon team
[01:28:45.860 --> 01:28:47.620]   running everything.
[01:28:47.620 --> 01:28:52.740]   Well, the thing is, Esther Crawford, who was apparently laid off, was the one who famously
[01:28:52.740 --> 01:28:58.820]   slept on the floor of Twitter and had posted excitedly about it on Twitter about how, you know,
[01:28:58.820 --> 01:29:02.500]   if you really love your job, you sleep on the floor or whatever. And now people are pointing
[01:29:02.500 --> 01:29:09.380]   out, well, you know, how did that work out? I loved you, Elon. I loved you. And you broke up with me.
[01:29:09.380 --> 01:29:17.380]   She was the one with the hashtag sleep where you work, which honestly, just take it for me.
[01:29:17.860 --> 01:29:23.220]   Not a good idea. Yeah. I have this other theory. And this is based on just a
[01:29:23.220 --> 01:29:29.780]   long observation of Musk, which is that Esther Crawford was getting too much attention.
[01:29:29.780 --> 01:29:36.740]   She was prominently profiled. Yeah. And you don't like that. Musk wants to take credit for
[01:29:36.740 --> 01:29:41.300]   everything that happens at his companies. Right. Not only does he want to be the number one
[01:29:41.300 --> 01:29:50.660]   person on Twitter, like his most one of his most recent tweets, why can't witchcraft defeat inflation
[01:29:50.660 --> 01:29:56.260]   or or something about something, something Einstein or
[01:29:56.260 --> 01:30:04.980]   Sunday, the first day of the rest of your life or rewatching notice, by the way, how my for use
[01:30:04.980 --> 01:30:11.620]   section is completely filled with Elon rewatching step brothers. So good. I'm so glad to hear that
[01:30:11.620 --> 01:30:18.980]   Elon, but a lot of engagement on that 30.4 million people viewing it, 163,000 people liking it.
[01:30:18.980 --> 01:30:24.260]   Not as many as the first day of the rest of your life. That was 165,000.
[01:30:24.260 --> 01:30:27.700]   I mean, if you were if you were a Twitter engineer who wanted to have, you know,
[01:30:27.700 --> 01:30:31.540]   any kind of career at the company, wouldn't you write a script that just like inflates
[01:30:31.540 --> 01:30:35.380]   Elon Musk's tweet? Oh, that's that's what they did. Apparently, that's what's happened.
[01:30:35.380 --> 01:30:41.620]   That's what they did. I think that's that's been more or less confirmed is that, you know,
[01:30:41.620 --> 01:30:46.020]   I mean, because he he freaked out after the Super Bowl, right, where Joe Biden's tweet got more
[01:30:46.020 --> 01:30:50.340]   attention than his did. And he cleared, you know, so that that's clearly wrong. That's clearly a
[01:30:50.340 --> 01:30:56.340]   problem because how could that be? And therefore, they wrote this this script that effectively gave
[01:30:56.340 --> 01:31:02.500]   a boost to Elon's tweets. And you know, they sounds like they had to tweak it. But then he denied all
[01:31:02.500 --> 01:31:07.380]   of that. The whole thing is happening again. That's all I guess my point, which is it did stop for
[01:31:07.380 --> 01:31:13.780]   a while. And now it's now he's back, baby. So yeah, your mistake is logging into Twitter.
[01:31:13.780 --> 01:31:18.020]   No, I don't. And I just thought this is for work purposes.
[01:31:22.420 --> 01:31:25.860]   Actually, I'd like to go to Twitter just to see what the trending topics are.
[01:31:25.860 --> 01:31:31.540]   Sure. And not because it's so valuable, because usually it's just nonsense.
[01:31:31.540 --> 01:31:36.500]   But it's interesting, I guess. Yeah, I have to admit, like, I think the problem is, is that I
[01:31:36.500 --> 01:31:40.580]   kind of got off of everything else. Like, I stopped really going to Facebook and I don't really,
[01:31:40.580 --> 01:31:44.020]   I should go to LinkedIn more than I do. And and I don't do any of the other ones. So
[01:31:44.020 --> 01:31:48.340]   Twitter is like my last bastion of where I'm at. So it gets into a little like, well,
[01:31:48.340 --> 01:31:52.740]   you know, like I'm here, you know, I don't have any, you know, I don't have other places.
[01:31:52.740 --> 01:31:57.860]   Yeah, I tweet. But my tweets are mostly answer people's tweets. And I have funny things to say.
[01:31:57.860 --> 01:32:01.780]   I don't really have any strong need for it. But it is useful as an engine for when I need
[01:32:01.780 --> 01:32:06.580]   one point people towards something. And so, so the, and it's working fine. I mean, I have to admit,
[01:32:06.580 --> 01:32:12.420]   I am so aggressive with blocking and muting that my, my Twitter hasn't changed at all.
[01:32:12.420 --> 01:32:17.940]   Like it is like completely the same. But I mute like 200 words. I block, I don't know,
[01:32:18.260 --> 01:32:20.420]   someone just posts something I go, I don't want to hear from them anymore.
[01:32:20.420 --> 01:32:24.740]   Yeah. Like, you know, like, and I just block things all the time. And I, and I don't.
[01:32:24.740 --> 01:32:30.500]   And so what's happened is, is that I follow audio engineers and programmers and some press
[01:32:30.500 --> 01:32:34.980]   people and my friends. And I have an awesome Twitter feed. Like, when people say, Oh, my
[01:32:34.980 --> 01:32:38.660]   Twitter feed is I'm like, you're not pruning it. Like, if your Twitter feed is horrible,
[01:32:38.660 --> 01:32:41.460]   like, I just, I just have this prune, which looks like a chainsaw.
[01:32:41.460 --> 01:32:46.100]   But you don't ever go to the for you tab, I betcha, because the for you tab is not who you're
[01:32:46.100 --> 01:32:50.900]   following. But it, but it's greatly affected by it. And it, and it, and my for you tab is fine,
[01:32:50.900 --> 01:32:55.220]   you know, like, again, it doesn't show me I have 200 muted words and doesn't show me a lot of things.
[01:32:55.220 --> 01:32:59.060]   So that's the word. So the thing is, is that the mute, the mute works inside of that. And it's like,
[01:32:59.060 --> 01:33:02.820]   it's politics and other things like that. I just, I'm like, I don't need to, I don't,
[01:33:02.820 --> 01:33:07.140]   I don't look at Twitter for news. I look at it purely for enjoyment. And so if I'm not enjoying
[01:33:07.140 --> 01:33:10.820]   my feed, I look at who's making me not enjoy that feed. And I just look at the words that they're
[01:33:10.820 --> 01:33:14.660]   using to go mute that one, mute that one, mute that one. And I won't hear from them anymore.
[01:33:14.660 --> 01:33:20.260]   And so the thing is, is that it's just, it's really, for me, it's really easy. And again, I,
[01:33:20.260 --> 01:33:25.540]   you know, I look at it like I don't have another platform right now that I'm going to go to that
[01:33:25.540 --> 01:33:30.260]   has any, that has any flow. And so, so I haven't left mostly because there's nothing there's nowhere
[01:33:30.260 --> 01:33:35.140]   else to go. But, but I, but I would say that, and again, just because I don't use the platform,
[01:33:35.140 --> 01:33:39.860]   but I've pruned this one so effectively over a decade. And as soon as I learned that mute words
[01:33:39.860 --> 01:33:45.220]   was a thing, I started getting aggressive. And I, it's like my favorite toy is to figure out how
[01:33:45.220 --> 01:33:50.020]   I'm going to mute. And I guess you're not Twitter blue. You're not giving him money, right?
[01:33:50.020 --> 01:33:54.820]   I am. You are. Oh, you're better blue. Yeah, because otherwise it's, yeah, again, it's like,
[01:33:54.820 --> 01:33:59.380]   and I have to admit, I work in a lot of countries. I work in a lot of states. I look at everything
[01:33:59.380 --> 01:34:03.700]   from a very pragmatic, like when in, when I'm in this country, I just do the thing. And for me,
[01:34:03.700 --> 01:34:07.860]   Twitter is just a country that I'm in. Sure. You know, like, like, you know, like, I don't,
[01:34:07.860 --> 01:34:12.100]   again, I don't have another platform that I really use other than discord. I mean, I'm on discord and,
[01:34:12.100 --> 01:34:16.820]   you know, and I'm in things in my, you know, my own stuff, but I don't have another one to go to.
[01:34:16.820 --> 01:34:22.100]   And it is important for what I do. So it is. And I, and I get, and I will admit, I get at least
[01:34:22.100 --> 01:34:28.740]   eight, two, I get at least two lattes worth of enjoyment out of it. So so I, I do, I am, I am Twitter
[01:34:28.740 --> 01:34:33.220]   blue. And I don't know if it makes any difference or not. But again, it was just kind of like, well,
[01:34:33.220 --> 01:34:37.620]   you know, sure, it makes me ill, would make me ill to give you a lot of money. But I guess if I
[01:34:37.620 --> 01:34:42.980]   visit Twitter, I'm just costing him money. So that's not so bad. I don't, I don't tweet. I will
[01:34:42.980 --> 01:34:47.540]   not be tweeting my $8. Yeah. Yeah. I just don't want to support that kind of thing.
[01:34:47.540 --> 01:34:52.820]   I'm not, I'm kind of the, go ahead. I will say the other things I just want to say, I have a very
[01:34:52.820 --> 01:34:59.940]   low opinion about many people that I make money to. I know that's true. Like, the thing is, if we
[01:34:59.940 --> 01:35:05.300]   start talking about, well, I can't buy this because I made it like, whatever that Hogwarts
[01:35:05.300 --> 01:35:11.300]   you know, but, but I don't usually admit it in public. Like we used to, I mean, we used to think
[01:35:11.300 --> 01:35:14.420]   that, you know, lots of people were great. And we find out more about them and they're like,
[01:35:14.420 --> 01:35:19.300]   no, they're all people are horrible. And so, so the thing is, is that so I don't make my decisions
[01:35:19.300 --> 01:35:23.220]   about whether I'm going to use like that based on that, because it's just point, if you start
[01:35:23.220 --> 01:35:27.620]   going, if you start pulling on that tip, you know, on the, on that thread of I'm not going to use,
[01:35:27.620 --> 01:35:31.380]   do a business because I'm not reading any more Dilberts. I'm sorry. That's it right there.
[01:35:31.380 --> 01:35:36.420]   No more Dilberts. Yeah. I'll never read Dilberts again. I don't know if I've actually read
[01:35:36.420 --> 01:35:40.180]   Dilberts. So it's really easy for me to promise. It's funny when you read it now in light of,
[01:35:40.180 --> 01:35:46.100]   of all of this filter makes a little, you kind of now understand why it's the way it is.
[01:35:46.100 --> 01:35:51.220]   Sorry, we keep interrupting you. Please. Oh, and I'm sorry. Oh, no, I just wanted to say I'm the
[01:35:51.220 --> 01:35:56.340]   opposite of Alex and how I use Twitter. I actually use it as a news reader. I find it very helpful
[01:35:56.340 --> 01:36:02.180]   for following, for example, the war in Ukraine, where, you know, sure, I could go to Google News or,
[01:36:02.180 --> 01:36:09.060]   you know, or CNN or any number of sources and get like news articles. But I really like following
[01:36:09.060 --> 01:36:16.180]   the key of independence, war correspondent, Ilya Pannemorenkur. Yeah. And you know, like, and
[01:36:16.180 --> 01:36:21.860]   that's, he's still posting on Twitter. And you know, his, his updates are very, you know,
[01:36:21.860 --> 01:36:28.340]   very interesting, tactical, informative. And there is still a lot of stuff on Twitter that is,
[01:36:28.340 --> 01:36:36.180]   is not elsewhere. So I think, you know, I think we, we, we dismiss the value that's there at our,
[01:36:36.180 --> 01:36:40.900]   well, but that should also make you angry or at Elon, right? I mean, he's ruining this good thing.
[01:36:40.900 --> 01:36:45.460]   Well, it was a good thing. He's also defending Scott Adams on it. So,
[01:36:45.460 --> 01:36:49.860]   of course he is. There you go. Of course he is. And yeah, it's a really good thing. And I don't
[01:36:49.860 --> 01:36:53.460]   know if it could be, you know, one of the things you look at, these things get built up and I don't,
[01:36:53.460 --> 01:36:58.900]   I don't know what you would replace it with, you know, as far as, you know, what it does. And so I'm,
[01:36:58.900 --> 01:37:05.300]   I'm not like, and I like Mastodon a lot. Yeah. Yeah. I mean, I found that Mastodon has almost
[01:37:05.300 --> 01:37:11.300]   entirely replaced Twitter for me. And, you know, the other thing is I definitely for, you know,
[01:37:11.300 --> 01:37:18.740]   probably about a decade was, was relying on Twitter as my main source of news. And that has
[01:37:18.740 --> 01:37:24.260]   gone away. And now it is a combination of Mastodon and I've gone back to using RSS readers. And
[01:37:24.260 --> 01:37:29.060]   God bless them. They're still around it. They're still around. And I, and as I've sort of gotten
[01:37:29.060 --> 01:37:34.740]   back into the habit of using RSS readers, which I used prior to sort of relying on Twitter,
[01:37:34.740 --> 01:37:39.060]   I've sort of realized like how much I missed it and how much better are the experiences.
[01:37:39.060 --> 01:37:44.900]   What RSS feeder do you use? So now because I've gone back, I've sort of like re-experienting
[01:37:44.900 --> 01:37:51.060]   with ones. But the one I'm using right now is just mainly as is fresh RSS, which is a very,
[01:37:51.060 --> 01:37:57.540]   very simple lightweight RSS reader. And I found it to be really, really valuable. There are a couple
[01:37:57.540 --> 01:38:00.980]   other ones that people have recommended that this week I'm going to start playing around with to
[01:38:00.980 --> 01:38:06.580]   see. But I found fresh RSS as just if you just want something very lightweight and straightforward,
[01:38:06.580 --> 01:38:14.100]   I've been really happy with it. Those of us who do the news need something that we can do
[01:38:14.100 --> 01:38:18.500]   a beat check for one of a better word, where we can daily go through and see what all the
[01:38:18.500 --> 01:38:23.780]   stories are. And that's how I prepare for all the shows. I select stories and all that.
[01:38:23.780 --> 01:38:27.460]   It's interesting because there was an RSS winner after Google Reader went
[01:38:27.460 --> 01:38:34.500]   away where people just said, "Oh, it's over. RSS is dead." But it's not because there's all these
[01:38:34.500 --> 01:38:42.100]   little things like fresh RSS that are fresh RSS has been around a while, but they're kind of,
[01:38:42.820 --> 01:38:49.940]   they're growing in the in the in the abandoned lot that is the RSS, the universe, all of these
[01:38:49.940 --> 01:38:54.420]   weeds are growing. There's some I use something called Sumi News, which is not self-hosted. It's
[01:38:54.420 --> 01:39:02.020]   it's a page where you could follow stuff. And this really scratches my itch, but I also have net
[01:39:02.020 --> 01:39:07.860]   news wire and, you know, reader and all all these others. I'll have to try fresh RSS. You host it
[01:39:07.860 --> 01:39:13.860]   yourself? No, I'm using there are a few hosted versions. Okay. So basically in order to test it
[01:39:13.860 --> 01:39:19.060]   out, and I've been using for now, I think about three weeks, I found one of the hosted versions,
[01:39:19.060 --> 01:39:25.220]   and I've just been using that. I think it's hosted in France. Oh, there's one I see a lot of people,
[01:39:25.220 --> 01:39:30.820]   you know, a lot of geeks using called Tiny Tiny RSS, which is it's a Docker container. It's similar.
[01:39:30.820 --> 01:39:36.340]   It's similar. It's you host it yourself. Yep. I know. And actually, I want to create RSS as
[01:39:36.340 --> 01:39:39.860]   it's why I've been looking at Feedly, which is cost money. Feedly is great. So yeah,
[01:39:39.860 --> 01:39:43.460]   but it's it's kind of industrial version of I'm going to look at what I want to do is be able
[01:39:43.460 --> 01:39:47.620]   to look at a bunch of feeds coming in and then select the select the articles that I think are
[01:39:47.620 --> 01:39:50.900]   interesting and then generate a new feed from that. Yeah. So that if people want to follow that,
[01:39:50.900 --> 01:39:55.460]   and I'm going to experiment with the kind of do that that you would like about Mastodon as any
[01:39:55.460 --> 01:40:01.140]   account can be an RSS feed. So if you say I'm going to follow Mike Masnick, let me let me look him up
[01:40:01.140 --> 01:40:06.900]   real quickly. What's your what's your handle? It's just MS Nick M. There you go. So I'm going to
[01:40:06.900 --> 01:40:14.980]   look up Mike. I see him there and now I can add RSS to the end of his feed. And I will have an RSS
[01:40:14.980 --> 01:40:19.860]   feed of actually, which is pretty good idea. I should do an RSS feed of Mike Mike, though.
[01:40:19.860 --> 01:40:25.540]   You got a you got a you got to put the rail dot me and your linkings there. I know we got to know
[01:40:25.540 --> 01:40:30.580]   it's you man. I know there are a whole bunch of things that I need to do that I just haven't
[01:40:30.580 --> 01:40:36.340]   quite gotten to because we're sort of debating we'll probably set up our own yeah.
[01:40:36.340 --> 01:40:41.300]   Mastodon instance, but like I've been sort of waiting because there are sort of these like
[01:40:41.300 --> 01:40:45.220]   various forks of Mastodon that are looking pretty interesting that I know we're working on.
[01:40:45.220 --> 01:40:50.900]   Yes. And it's like do I want to set this up until that that some of these projects are released.
[01:40:50.900 --> 01:40:54.340]   And so I've been holding off and that includes like, you know,
[01:40:54.340 --> 01:40:58.820]   a role model. What does it take to build a to put together a Mastodon server?
[01:40:58.820 --> 01:41:01.860]   Shouldn't say mass first of all, let's not say Mastodon. Let's say Fediverse.
[01:41:01.860 --> 01:41:07.540]   And and let's say activity pub because really all you have to do is hook into activity pub and
[01:41:07.540 --> 01:41:12.260]   you can be part of this Fediverse. So there are lots of clients and pixel fed is an Instagram
[01:41:12.260 --> 01:41:18.580]   that is based on activity pub. Mastodon is just one instance of some, you know,
[01:41:18.580 --> 01:41:24.180]   I can writing some software that happens to hook into the Fediverse, but it's not the only way by
[01:41:24.180 --> 01:41:29.940]   any means. Plaroma is another one. What do you look at that? There's there's one called Calki
[01:41:29.940 --> 01:41:39.700]   C-A-L-C-K-E-Y. And they have been upgrading it over the last couple months. And they're
[01:41:39.700 --> 01:41:46.260]   so they had just released an upgraded version a couple weeks ago. And they're working on what
[01:41:46.260 --> 01:41:50.900]   is effectively sort of a pro level version of it as well with a bunch of other features.
[01:41:50.900 --> 01:41:55.940]   And I'm sort of holding out for something along those lines. It's a fork of Miski.
[01:41:55.940 --> 01:42:08.100]   Yes. So Miski is another activity pub compatible server, which has has some nice features and
[01:42:08.100 --> 01:42:15.300]   and a very sort of clean UI. And it's adding in a bunch of other, I think really useful features
[01:42:15.300 --> 01:42:20.340]   that Mastodon is missing. But I'm sort of holding out. And now I need to look at this.
[01:42:20.340 --> 01:42:26.820]   Yeah, I will look at this. We run Mastodon only because it's three or three years ago in 2019.
[01:42:26.820 --> 01:42:31.220]   I had always had something to do with StatusNet,
[01:42:31.220 --> 01:42:40.660]   Identica, new social. This is all kind of all the same thing. And we used to have the Twitter
[01:42:40.660 --> 01:42:46.660]   army, Identica and stuff. So I was interested in Mastodon, set up a Mastodon server to its social,
[01:42:46.660 --> 01:42:52.340]   using somebody called Masto.host, which is hosted Mastodon. Very good. It was, I think,
[01:42:52.340 --> 01:42:57.380]   was five euros a month. It was so little that I forgot I was even paying for it. I thought,
[01:42:57.380 --> 01:43:01.220]   this is free. And everyone's like, I don't know, I've been paying for it all the time. It's through
[01:43:01.220 --> 01:43:06.980]   PayPal or something. And then all of a sudden something happened around October of last year.
[01:43:08.180 --> 01:43:16.820]   Oh, we went from like 12 users to, I think, 7,000 active users. It suddenly cost me $350 a month.
[01:43:16.820 --> 01:43:22.020]   But it's well worth it. I'm going to look at some of these other ones. This is really interesting.
[01:43:22.020 --> 01:43:27.780]   The problem is I'm lazy. And so I like a hosted solution, which Masto host scratches that itch.
[01:43:27.780 --> 01:43:33.780]   But this looks really cool. Yeah, there are some really nice things that they're working on too,
[01:43:33.780 --> 01:43:38.820]   where they've sort of recognized some of the limitations of Mastodon as it is today. And
[01:43:38.820 --> 01:43:43.140]   my guess is that some of the hosting companies, I don't know about Masto host in particular,
[01:43:43.140 --> 01:43:47.300]   but some of the other, there are a few different companies that will host Mastodon instances
[01:43:47.300 --> 01:43:51.620]   are going to start offering other cow key and some of the other ones as well.
[01:43:51.620 --> 01:43:58.740]   You go, you go, you got to do cow key man or whiskey or something. Of course,
[01:43:59.700 --> 01:44:05.220]   that's the beauty of this is you can subscribe to an account on any of these and they can go into
[01:44:05.220 --> 01:44:10.180]   your Mastodon or whatever it is that you use. It's pretty interesting. In fact, it's so interesting
[01:44:10.180 --> 01:44:17.220]   now venture capital companies are starting to invest in Mastodon stuff. There's a new, a lot of
[01:44:17.220 --> 01:44:24.500]   attention to pay to a new Mac app called, oh, I forgot, Mammus, I think it's called. That's
[01:44:24.500 --> 01:44:30.260]   venture funded. And that's always when I get nervous is when I start seeing the money,
[01:44:30.260 --> 01:44:35.700]   the money coming in because you think, well, but the good news about the Fennivers is no one can
[01:44:35.700 --> 01:44:42.660]   own it. So go ahead, let them invest. That's fine. Let them invest. Although to his credit,
[01:44:42.660 --> 01:44:47.140]   Eugene Rothko, who's, Rochko, who's created Mastodon has turned down a lot of
[01:44:47.140 --> 01:44:52.740]   investment. She says, no, no, we don't need that. We'll just have our Patreon and keep going.
[01:44:53.700 --> 01:44:58.500]   You talk, can I go ahead and blow your mind and tell you what, what I was using as a
[01:44:58.500 --> 01:45:04.820]   an RSS reader two decades ago. Let's talk about it. Yes. Live journal. Live journal, let you add
[01:45:04.820 --> 01:45:10.740]   RSS feeds and you could just read the, you know, whatever article you wanted alongside your friends
[01:45:10.740 --> 01:45:18.340]   posts. And it was such a great feature. And that in a way, that was also kind of more like micro
[01:45:18.340 --> 01:45:23.140]   blogging because you could write short little posts. It was more like a traditional blog,
[01:45:23.140 --> 01:45:31.300]   sometimes, right? Or link role kind of a thing, right? So you could write a Twitter style post on that.
[01:45:31.300 --> 01:45:37.380]   And I mean, live journal, actually, the technology behind live journal helped inspire Facebook. I mean,
[01:45:37.380 --> 01:45:42.740]   it was a very foundational company, actually. Unfortunately sold off to a Russian company
[01:45:42.740 --> 01:45:50.020]   called Supe. And I forget, I think it's been passed around now. But yeah, a shadow of its
[01:45:50.020 --> 01:45:55.300]   former self, but a lot of great ideas there that that I think actually anticipated the
[01:45:55.300 --> 01:45:59.620]   Fediverse because you could take live journal. It was open source code. You could basically
[01:45:59.620 --> 01:46:03.300]   clone live journal set up another site. They had no problem with that.
[01:46:03.300 --> 01:46:11.140]   Right. Well, remember browsers used to do RSS feeds. You could, you could open an RSS feed in a
[01:46:11.140 --> 01:46:19.060]   browser and read it. So times have, times have changed. Chatroom. Oh, I would do AI real quickly.
[01:46:19.060 --> 01:46:22.740]   And then we'll do some other interesting stories. But we do want to talk about AI.
[01:46:22.740 --> 01:46:29.140]   This was something they covered on Tech News weekly on Thursday. There's a science fiction
[01:46:29.140 --> 01:46:36.100]   magazine called Clark's World, not named after R3C Clark. I thought it was, but in fact, Neil Clark,
[01:46:36.100 --> 01:46:43.060]   its founder and editor, has written a blog post saying, I got to turn down story submissions
[01:46:43.860 --> 01:46:51.460]   because you guys knock it off. All of a sudden they are submitting AI written science fiction
[01:46:51.460 --> 01:46:58.100]   stories. Because you know, I guess he pays his tweet. Submissions are currently closed. It shouldn't
[01:46:58.100 --> 01:47:06.020]   be hard to guess why. Just look at the graph. This is, this is the number of people we've had to ban
[01:47:06.820 --> 01:47:14.340]   by month. In prior months, it was plagiarism. Now it's machine generated submissions.
[01:47:14.340 --> 01:47:20.180]   And it's gone through the roof. I'm curious how he knows his chat GPT. Maybe it's just not,
[01:47:20.180 --> 01:47:26.420]   it's so crapily written. But then isn't a lot of sci fi, Pope sci fi, not so well,
[01:47:26.420 --> 01:47:34.260]   written. I mean, I'm sympathetic to, to, to the issue of having to wade through all that. But
[01:47:34.260 --> 01:47:40.660]   there is a part of it. It's just like, if chat GPT wrote a really good sci fi, is that so bad?
[01:47:40.660 --> 01:47:48.100]   Yeah. The irony of a sci fi play blocking sci fi articles, like, you know, the articles are
[01:47:48.100 --> 01:47:53.060]   written by the machine. And the sci fi thing doesn't take the articles that are written by the thing
[01:47:53.060 --> 01:47:59.300]   that, the thing, sci fi, yeah. Yeah, it seems like it seems, the problem is just the level of
[01:47:59.300 --> 01:48:03.860]   production that it's capable of, you know, so that if you're paying the problem, if a person's
[01:48:03.860 --> 01:48:08.820]   typing, there's a certain amount of effort that is required for that. If you, if you could generate
[01:48:08.820 --> 01:48:13.860]   a 20 page book by just continuing to push a button, you end up with, you know, that's what these
[01:48:13.860 --> 01:48:18.100]   people are doing, right? They're just, they're just generating whole books. And, and maybe pay
[01:48:18.100 --> 01:48:23.380]   for, but if you had this kind of opened, like, I'm just going to pay as we put this all in, it really,
[01:48:23.380 --> 01:48:27.460]   you know, ruins it for everyone. When, when you, because what'll happen is you'll say, well,
[01:48:27.460 --> 01:48:30.740]   we'll just start paying a penny instead of 12 cents a word or whatever for everything,
[01:48:30.740 --> 01:48:34.740]   because it's not worth as much, or we won't pay anything. And now all the people that were writing
[01:48:34.740 --> 01:48:40.580]   things and doing things get, this is the whole issue with chat GPT. Yeah. And Amazon has Kindle
[01:48:40.580 --> 01:48:44.500]   books where chat GPT is listed as an author or co author. So people are just
[01:48:44.500 --> 01:48:50.820]   spamming, they'll sort. Yeah. When I know people who have put books into Amazon that are not,
[01:48:50.820 --> 01:48:57.540]   they, they did all the images in mid journey, all of the things, all the text in chat GPT and how
[01:48:57.540 --> 01:49:01.060]   would you know, unless there's too many fingers, right? Like, you know, like, like, so without
[01:49:01.060 --> 01:49:05.540]   without the show hands, that's the first rule. Never, if you never have fingers, you can general
[01:49:05.540 --> 01:49:10.420]   or glasses, you can generally do so they're not, they're not declaring it. They're not running
[01:49:10.420 --> 01:49:15.140]   press releases. They're just putting it on Amazon. Well, as Ben G Edwards points out in our
[01:49:15.140 --> 01:49:20.900]   his technical, there are multiple YouTube videos. Here's a few that he found how to make money with
[01:49:20.900 --> 01:49:28.340]   chat GPT writing ebooks, $800 a day, how to use chat GPT to make children's story books in five
[01:49:28.340 --> 01:49:34.740]   minutes and sell on KDP, how to make passive income with chat GPT and mid journey, $23,000 a
[01:49:34.740 --> 01:49:39.540]   month. This got to be a gold rush going on. This is, I don't know if you really do that, but.
[01:49:39.540 --> 01:49:46.020]   And it's, yeah, and it's not just this one sci-fi journal, by the way, that a lot of literary
[01:49:46.020 --> 01:49:50.900]   journals are also complaining about the wave of of submissions. And so you have like these
[01:49:50.900 --> 01:49:56.020]   these automated processes against, you know, sometimes jet GPT does a better, I mean, it does
[01:49:56.020 --> 01:50:00.020]   a better job. It does a better job than I do. Like one of the reasons I have chat GPT, I'm,
[01:50:00.020 --> 01:50:04.820]   you know, I subscribe to that as well as Twitter, as well as mid journey, as well as
[01:50:04.820 --> 01:50:08.820]   mid journey, right? You spend a lot of money on these things. Yeah, I don't know. It's like
[01:50:08.820 --> 01:50:15.380]   48 dollars a month. So anyway, so the, but the club would only cost you seven dollars a month.
[01:50:15.380 --> 01:50:18.020]   The, the,
[01:50:18.020 --> 01:50:22.100]   do us and the protector, Patreon. I'm in club Twitter, but I think I get,
[01:50:22.100 --> 01:50:26.740]   so I think it's for free. We give it to you. I get it for free. So, so the, so the, I pay
[01:50:26.740 --> 01:50:31.940]   for it otherwise. So the, but the thing is, is that with chat GPT, what I really find interesting
[01:50:31.940 --> 01:50:37.700]   is I love asking it to write a description of something I already know. And I'll go write this,
[01:50:37.700 --> 01:50:42.660]   and it will write this super concise, really well written thing. And there's three or four
[01:50:42.660 --> 01:50:45.780]   things that are wrong. And I go in and I go fix those things, but it's actually a better description
[01:50:45.780 --> 01:50:51.060]   than what I would have done. The English structure is actually that chat GPT does the, the, the,
[01:50:51.060 --> 01:50:56.900]   shrunken white pass, so to speak, is much better. Oh, yeah. Chat GPT than most people who write.
[01:50:56.900 --> 01:51:02.260]   Oh, yeah. And so, you know, so that like, it does what I would call a strunk, a shrunken white pass,
[01:51:02.260 --> 01:51:07.220]   you know, active verbs. Well, so that's what it makes it sound authoritative, is that it has,
[01:51:07.220 --> 01:51:12.180]   it's using proper English as opposed to research English. Research English is lots of passive
[01:51:12.180 --> 01:51:17.060]   verbs, lots of run on sentences, lots of other things. And so, so it actually reads better than,
[01:51:17.060 --> 01:51:22.900]   than a lot of those things. And again, it's wrong almost every time somewhere, like it's never,
[01:51:22.900 --> 01:51:26.260]   it's never like, oh, that's the right answer. It's like optimized for fact,
[01:51:26.260 --> 01:51:31.540]   factualness, right? No, it's, and so, and again, but it doesn't make it not useful. I know people who
[01:51:31.540 --> 01:51:35.700]   are programming that use it all day. Like they go write this, you know, write me an app that does
[01:51:35.700 --> 01:51:41.140]   this get a scope pallets based on chat GPT and is quite effective. And, and the, and the chat
[01:51:41.140 --> 01:51:45.620]   you even cheat GPT went down. I know there was like a division that, at some big company that just
[01:51:45.620 --> 01:51:50.660]   stopped because they were, because they've gotten so, you know, it's such a little bit, well, it's
[01:51:50.660 --> 01:51:55.300]   just that it's they're 10 times more effective. No, coders have always pasted code. This is,
[01:51:55.300 --> 01:51:58.660]   yeah, they've only, and this is just way better than pasting code because you don't have to figure
[01:51:58.660 --> 01:52:03.540]   out how this cut and paste fits in with this cut and paste. You just go, and it's not, and
[01:52:03.540 --> 01:52:07.380]   it's able to write the interesting thing is it's able to write native code. So you're not writing
[01:52:07.380 --> 01:52:12.900]   monkey code, you know, right? Native react native and react and, you know, and all those monkey code
[01:52:12.900 --> 01:52:18.180]   things that bear is why we're everything so unstable. And so, so anyway, so the, so, but chat
[01:52:18.180 --> 01:52:22.740]   GPT can just write what you were going to write, but natively. And then you have to go, you need
[01:52:22.740 --> 01:52:25.940]   an expert though to fix it. That's the big thing is that, you know, and with stories,
[01:52:25.940 --> 01:52:30.820]   that's where CNET kind of got a little trouble because even though they had an editor checking
[01:52:30.820 --> 01:52:39.940]   all the articles they were writing for personal finance with NAI, still mistakes slipped through,
[01:52:39.940 --> 01:52:46.900]   and people were a little unhappy, frankly. Yeah, I don't know how carefully CNET was.
[01:52:46.900 --> 01:52:52.820]   Yeah, we were into all these. Because, because, I mean, you know, there was one about, you know,
[01:52:52.820 --> 01:52:56.900]   the annual percentage yield on a savings account that just
[01:52:58.420 --> 01:53:05.140]   didn't pass a sanity for that. Yeah, yeah. We had Connie Goyelmo on when that story broke,
[01:53:05.140 --> 01:53:09.940]   editor in chief had seen it. And, you know, she said, well, this is, these are stories nobody
[01:53:09.940 --> 01:53:14.500]   wants to write. These are the dumb stories that, you know, you'd have an intern write, you have
[01:53:14.500 --> 01:53:18.900]   an editor check them. And it makes sense for a machine to write it because it's kind of wrote
[01:53:18.900 --> 01:53:25.620]   anyway. I don't have a problem with them using a machine to insert stock prices in articles.
[01:53:25.620 --> 01:53:29.780]   That's fine. That's been going on for years. You probably could write most finance stories and
[01:53:29.780 --> 01:53:36.660]   sports stories kind of in an automated fashion. But I remember a journalist
[01:53:36.660 --> 01:53:41.380]   tweeting, a journalist friend of mine tweeting some years ago about a story she was getting ready
[01:53:41.380 --> 01:53:46.420]   to write and how she needed a drink to write it. And it was like exactly. Yeah, imagine how your
[01:53:46.420 --> 01:53:53.220]   readers feel when they read that story. Like, you know, it's like, if we think so little of these
[01:53:53.220 --> 01:53:59.380]   stories, why are we actually? I'll tell you why, because it's good link bait for a search engines.
[01:53:59.380 --> 01:54:04.340]   That's why it gets search results. And then we get affiliate links. I know I was.
[01:54:04.340 --> 01:54:09.540]   Which are which are not going to exist if, you know, if the chatbot, you know, AI model
[01:54:09.540 --> 01:54:13.860]   takes off like you're not going to click through to anything anymore. So we won't have to.
[01:54:13.860 --> 01:54:18.340]   They're almost, they're almost digging their own grave. It's interesting, isn't it? Yeah.
[01:54:20.340 --> 01:54:24.580]   You know what? We should start having our copy written by chat GPT. I want to.
[01:54:24.580 --> 01:54:29.620]   We have humans write our ads, but I think there's no reason for that.
[01:54:29.620 --> 01:54:32.820]   I think that you're going to end up, you know, like we were talking about some of the places
[01:54:32.820 --> 01:54:37.140]   a lot of this is going to go. And you're going to see, like, for instance, I think with when you
[01:54:37.140 --> 01:54:41.540]   speech to text and a lot of those other things, I think you're going to end up with, like, for
[01:54:41.540 --> 01:54:47.060]   instance, all the city council meetings or whatever, we'll just get converted to text.
[01:54:47.060 --> 01:54:52.180]   Right. We'll then be summarized by chat GPT and then put back into text to speech.
[01:54:52.180 --> 01:54:55.780]   And so you can just listen to the summary of like, you know, any one of these any time you want.
[01:54:55.780 --> 01:54:57.940]   Well, and who needs a local one? That's not even technology to be hard.
[01:54:57.940 --> 01:55:02.340]   A local reporter. Nobody nobody likes doing that on the newspaper, do the city council meetings.
[01:55:02.340 --> 01:55:08.020]   So I just have chat GPT do it. Well, again, I think that you need what we have to do,
[01:55:08.020 --> 01:55:11.860]   especially as Americans is always stay ahead of it. You have to like thinking that you're going to
[01:55:11.860 --> 01:55:17.300]   be able to take a college, you know, get a get a degree and it's all going to work out is probably
[01:55:17.300 --> 01:55:20.820]   that's fading away pretty quickly. That went away 20 years ago, dude.
[01:55:20.820 --> 01:55:26.260]   So I've never had one. So I don't know. So I don't either. Everybody I went to school with in the
[01:55:26.260 --> 01:55:31.780]   70s was pre met or pre law. I hope they're enjoying their long careers 40 years in.
[01:55:31.780 --> 01:55:38.020]   I don't have a high school. I majored in Chinese. So I don't know, you know, the, but the
[01:55:39.140 --> 01:55:45.940]   I can't say high school. It's complicated. It's complicated. I made a strong calculation on
[01:55:45.940 --> 01:55:52.020]   how many credits I needed and then took study halls 11 study halls in my last term. Like, I was
[01:55:52.020 --> 01:55:54.900]   like, I'm not going to do any more work than I have to. I'm just going to sit there. What's funny
[01:55:54.900 --> 01:55:58.980]   is I didn't just sit there. I tried to set all the physics records. The reason I did it was that
[01:55:58.980 --> 01:56:02.100]   I had plenty of time. I was like, I'm going to set all of them. I didn't set all of them. I said,
[01:56:02.100 --> 01:56:08.500]   one. Anyway, so. Did you get a job after this? Or what did you do? Were you thinking about a job?
[01:56:09.460 --> 01:56:13.140]   Not really. I got out of high school and I started working electronics company building mainframes
[01:56:13.140 --> 01:56:17.460]   because I had a skill that when you're 18 and then you can solder, you become really interesting.
[01:56:17.460 --> 01:56:23.700]   When you learn to solder when you're 10, you're you're you're you're able to wield a
[01:56:23.700 --> 01:56:29.540]   and then I started building mainframes and then I then went back to college. I had I was able to
[01:56:29.540 --> 01:56:36.340]   go to college because I went to community college. So I had college transcripts. And so I just
[01:56:36.340 --> 01:56:40.420]   transferred from the community college to Penn State. You played you game. That's all you game.
[01:56:40.420 --> 01:56:45.140]   I kind of feel bad. I sent my son to a fancy private school in high school.
[01:56:45.140 --> 01:56:49.300]   Fancy college. See you where he took a degree in broadcast journalism. And
[01:56:49.300 --> 01:56:55.620]   now he's making kubano sandwiches with Guy Fieri. So I don't know. My my my wife has a
[01:56:55.620 --> 01:56:59.620]   master's from Harvard. And so she evens that out. And the only way we get like we were able to get
[01:56:59.620 --> 01:57:03.620]   an apartment in San Francisco is because of her like they look like we have it. Yeah, this is the
[01:57:03.620 --> 01:57:10.420]   same way we bought a house. At least I had good credit. She's buying the house. Not me. Not me.
[01:57:10.420 --> 01:57:15.620]   Let me take a little break here with a wonderful panel. I hate to keep you on the hook for a couple
[01:57:15.620 --> 01:57:24.340]   more ads. This one is not written by AI. It's stamps.com and I can talk blue streak about stamps.com.
[01:57:24.340 --> 01:57:31.220]   We've been doing it since 2012. I've been doing ads for stamps.com and we've been using stamps.com.
[01:57:31.860 --> 01:57:36.340]   It is the way if you've got a small business, if you're doing mailing of any kind, especially
[01:57:36.340 --> 01:57:41.220]   if you've got an eBay or an Etsy business or anywhere where you're doing mail and you need
[01:57:41.220 --> 01:57:46.980]   stamps.com for all your mailing and shipping. It lets you print your own postage and shipping
[01:57:46.980 --> 01:57:51.140]   labels. You don't need fancy equipment. You don't need a postage meter. You just need to
[01:57:51.140 --> 01:57:56.100]   your computer and an internet connection and you're and a printer and you're done.
[01:57:56.980 --> 01:58:02.100]   We've been using stamps.com since 2012. I'm telling you about stamps.com since 2012.
[01:58:02.100 --> 01:58:07.540]   And if you still haven't used them yet, why are you waiting? Do you know? Did you know the
[01:58:07.540 --> 01:58:11.380]   postage rates just increased again? I don't have to worry about it at all. I just have stamps.com
[01:58:11.380 --> 01:58:16.900]   put me some stamps. They have the best discounts in the industry too. They save you with rates.
[01:58:16.900 --> 01:58:24.020]   You literally can't find anywhere else. Up to 84% off United States Postal Service.
[01:58:24.020 --> 01:58:31.940]   Oh, UPS. The guys in the brown shorts, you get discounts there too. I mean, really, it's a one-stop
[01:58:31.940 --> 01:58:37.860]   shop for all your mailing needs. And stamps.com will automatically tell you the cheapest and fastest
[01:58:37.860 --> 01:58:43.380]   shipping options. So you could choose the best way to ship that document or book or whatever.
[01:58:43.380 --> 01:58:50.580]   For 25 years, stamps.com has been indispensable for over a million businesses. You get the Postal
[01:58:50.580 --> 01:58:56.100]   Service and UPS, everything you need to run your business right from your desk anytime
[01:58:56.100 --> 01:59:01.140]   day or night. No lines, no traffic, no waiting, no parking. It's the best way to sell online because
[01:59:01.140 --> 01:59:05.220]   it connects with all of the major marketplaces. You don't even have to type anything in.
[01:59:05.220 --> 01:59:11.780]   Stamps.com, whether it's an envelope, a package, printing a label, it'll take your, it knows your
[01:59:11.780 --> 01:59:16.820]   address. It'll put your company logo on there. It'll take the address of the recipient from the
[01:59:16.820 --> 01:59:22.740]   website. It'll even fill out customs forms or a certified mail, whatever kind of forms you need
[01:59:22.740 --> 01:59:28.740]   to fill out automatically, print it out, then you press a button and the carrier comes to you.
[01:59:28.740 --> 01:59:32.020]   UPS or the Postal Service, they come to you, they pick up the package you're done.
[01:59:32.020 --> 01:59:37.620]   It's the best. The silliest thing in the world, and I still get this I order from Etsy frequently.
[01:59:37.620 --> 01:59:42.820]   And we'll get these packages wrapped with a bunch of twine and tape and 15 stamps
[01:59:43.620 --> 01:59:50.500]   manually licked and placed. And it just does not give off the impression of professionalism.
[01:59:50.500 --> 01:59:57.460]   A beautiful package from stamps.com with your logo on it and the bar code for the postage,
[01:59:57.460 --> 02:00:03.300]   exactly the right postage. How many times I get all the time I get from Etsy, especially I'll get
[02:00:03.300 --> 02:00:11.060]   postage to packages. It's like, dude, get stamps.com. They'll even send you a free scale, a USB scale.
[02:00:11.060 --> 02:00:19.060]   So you always have the right postage. I think stamps.com is the single best way to level up
[02:00:19.060 --> 02:00:23.300]   your business. Do it right now. Set up your business for success. Get started with stamps.com
[02:00:23.300 --> 02:00:27.940]   today. Promo code, Twit. You just click the microphone in the upper right hand corner,
[02:00:27.940 --> 02:00:33.860]   put in TWIT. You'll get a four week trial. You'll get that digital scale for free. You'll get free
[02:00:33.860 --> 02:00:40.820]   postage. No long term commitments, no contracts. Stamps.com. Click the microphone at the top of the
[02:00:40.820 --> 02:00:44.420]   page. Don't forget to use the code TWIT though. Because that's how they know you heard it here.
[02:00:44.420 --> 02:00:50.500]   Stamps.com. We love them. Thank you, stamps for all your support. We appreciate it.
[02:00:50.500 --> 02:01:00.180]   Hayes Valley, huh? I'm going to move to Hayes Valley, the Herb Kane of the Internet, Owen Thomas,
[02:01:00.180 --> 02:01:10.500]   has a new article in the San Francisco Examiner. This apparently, the Hayes Valley in San Francisco
[02:01:10.500 --> 02:01:15.140]   is now the place. I know downtown, you tell me, because I haven't been downtown in a while,
[02:01:15.140 --> 02:01:17.140]   downtown I'm told is a little bit of a ghost town.
[02:01:17.140 --> 02:01:24.660]   I think that's overstated. South of market, definitely. Especially central South of market.
[02:01:24.660 --> 02:01:32.100]   Which used to be the up and coming area, right? Oh, yeah. That's where you wanted to be.
[02:01:32.100 --> 02:01:37.940]   If you had a startup, you wanted to be walking distance. Exactly.
[02:01:39.140 --> 02:01:42.580]   That's where Twitter was more on the swings on South part.
[02:01:42.580 --> 02:01:51.300]   Exactly. Yeah. It's where actually a lot of VCs set up offices as they migrated north
[02:01:51.300 --> 02:02:02.100]   from Sandhill Road. But you don't want to go to Soma anymore if you're trying to run into a VC.
[02:02:02.100 --> 02:02:09.060]   It definitely seems to be Hayes Valley where the action has centered. It's a fun neighborhood
[02:02:09.220 --> 02:02:16.580]   it's relatively new in the city. There used to be a freeway there and they tore it down.
[02:02:16.580 --> 02:02:22.420]   Oh, yeah. Oh, yeah. That's right. Yeah. So you had a lot of urban renewal there.
[02:02:22.420 --> 02:02:29.620]   So you've got newer businesses and a nice vibe. It's relatively convenient, pretty central.
[02:02:29.620 --> 02:02:32.580]   So there's lots of reasons why it makes sense.
[02:02:32.580 --> 02:02:38.900]   This was the most terrifying area in town because the Embarcadero freeway
[02:02:38.900 --> 02:02:43.140]   loomed above you and made shade everywhere no matter what the data was like.
[02:02:43.140 --> 02:02:48.100]   It was like a cavern. It was the worst. Plus there are trucks and cars going over your head.
[02:02:48.100 --> 02:02:53.140]   Nothing good happened there. Then the earthquake hit in 1989.
[02:02:53.140 --> 02:02:58.100]   The freeway is damaged and they had the good sense to tear it down and it opened up all this area.
[02:02:58.100 --> 02:03:03.220]   Yeah. In this case, it was the central freeway, but it was the same. Same reason why.
[02:03:03.220 --> 02:03:06.420]   Yep. Yeah. Same reason why the freeways came down.
[02:03:08.420 --> 02:03:15.060]   Why his valley? It seems to be something where there just comes kind of is some consensus that
[02:03:15.060 --> 02:03:21.620]   like people want to get together in person. Why not pick this neighborhood and people are
[02:03:21.620 --> 02:03:27.300]   migrating to selling there for some a little bit, I think, right? It's now the place to be.
[02:03:27.300 --> 02:03:31.220]   I remember I used to take that fell street exit because I lived up in the hate
[02:03:31.220 --> 02:03:38.020]   aspect. When they tore it down, I was like, "Well, how do I get home? I can't." That was my off ramp.
[02:03:38.020 --> 02:03:42.500]   You start off, I spotted my target eye. He was wearing a light gray,
[02:03:42.500 --> 02:03:49.620]   north face puffer jacket wrapped around a cross fit taut torso with a conspicuously large
[02:03:49.620 --> 02:03:54.900]   wristwatch peeking out, engaged in rap conversation with a younger man across a small table
[02:03:55.540 --> 02:04:03.460]   at La Boulangerie on Hayes. Here it was, a venture capitalist in search of the next big thing.
[02:04:03.460 --> 02:04:09.620]   It seems to be the place. One thing is that you can actually sit down there like blue bottle,
[02:04:09.620 --> 02:04:14.420]   which is very famous in Hayes Valley. Got it started as a kiosk there. It's still just a kiosk.
[02:04:14.420 --> 02:04:19.220]   There's nowhere to really sit there. Same thing with ritual. You can get coffee there,
[02:04:19.220 --> 02:04:25.460]   but then you have to take it outside and pictures is green. There's just lines of little tables
[02:04:25.460 --> 02:04:29.860]   perfect for having a one-on-one conversation between a founder and a
[02:04:29.860 --> 02:04:36.180]   I remember Kevin Rose saying, "I think he was a blue bottle guy. There was a rivalry between
[02:04:36.180 --> 02:04:44.580]   ritual and blue bottle." Actually, I remember the blue bottle at fifth-in-mission when I worked at
[02:04:44.580 --> 02:04:52.180]   the Chronicle, which is across the street there. That got a lot of start of founder conversations.
[02:04:52.900 --> 02:04:58.020]   That one was great because it's right behind that old building. It's this little alleyway
[02:04:58.020 --> 02:05:02.180]   if you didn't know it was there, you wouldn't know it was there kind of thing.
[02:05:02.180 --> 02:05:07.060]   Touch away. Absolutely. When I worked in the financial district, actually, there was a blue
[02:05:07.060 --> 02:05:15.780]   bottle that went from closed during the pandemic many places to open. We could always just waltz
[02:05:15.780 --> 02:05:22.580]   in there and get a coffee in 30 seconds to packed with meetings. I think, Sam, for
[02:05:22.580 --> 02:05:27.140]   Francisco generally is back to an extent people really don't credit.
[02:05:27.140 --> 02:05:35.940]   The interesting thing is it's not the same old scene that there was, say, the creamery,
[02:05:35.940 --> 02:05:43.460]   which is closed down in Soma. It's a new scene that people are building around a new technology.
[02:05:43.460 --> 02:05:46.580]   You call it Cerebral Valley because there's a lot of AI going on.
[02:05:46.580 --> 02:05:50.900]   I can't take credit for that. I talked to Amber Yang, who's a venture capitalist who kind of
[02:05:50.900 --> 02:05:56.500]   popularized that term. She said she got it from a friend of hers, Bo Han Lu, who just made it
[02:05:56.500 --> 02:06:05.220]   in an offhand remark. People have gravitated to it. I joke that a lot of times these things are
[02:06:05.220 --> 02:06:11.540]   trying to make fetch happen. In this case, fetch seems to be happening unlike mean girls.
[02:06:11.540 --> 02:06:17.140]   What's fetch? Oh, have you watched Mean Girls? There's a scene where there's this one...
[02:06:17.700 --> 02:06:20.100]   Oh, come on, Leo. You've got to watch Mean Girls.
[02:06:20.100 --> 02:06:26.580]   There's a musical, actually. You should come down to San Francisco too. Mean Girls are musical.
[02:06:26.580 --> 02:06:36.660]   So it's a famous movie written by Tina Fey about the culture of high school girls.
[02:06:36.660 --> 02:06:40.980]   And there's this one girl who just keeps saying things are so fetch.
[02:06:40.980 --> 02:06:43.300]   Oh my god. Sure for fetching?
[02:06:44.180 --> 02:06:49.380]   Yeah. Well, not really clear. And finally, this other girl, the kind of the leader of the Mean
[02:06:49.380 --> 02:06:54.660]   Girls pack, snaps at her and says, Gretchen, stop trying to make fetch happen. It's not going to happen.
[02:06:54.660 --> 02:06:58.820]   I always wonder, there's somebody who's trying to make it happen.
[02:06:58.820 --> 02:06:59.220]   Which is now a meme.
[02:06:59.220 --> 02:07:00.020]   Yeah.
[02:07:00.020 --> 02:07:06.020]   If you do a meme search for stop trying to make fetch happen, you'll find it.
[02:07:06.020 --> 02:07:11.380]   Yeah. So I think it's... My instinctive reaction was, okay, come on,
[02:07:11.380 --> 02:07:18.260]   Hayes Valley, stop trying to make AI happen. But there's going to be a 200-person summit held
[02:07:18.260 --> 02:07:25.220]   at the end of March. There's a data plus AI summit actually happening in Moscone Center.
[02:07:25.220 --> 02:07:30.980]   Later this summer, there seems to be this summer of AI movement happening.
[02:07:30.980 --> 02:07:36.260]   And part of it is just like San Francisco is a really friendly place. If you are a
[02:07:36.260 --> 02:07:42.580]   nerdy geek who likes to talk about generative prompt engineering for hours at a time,
[02:07:42.580 --> 02:07:48.100]   or obscure mathematical models, if you're a really good time at a party,
[02:07:48.100 --> 02:07:49.620]   San Francisco is the place for you.
[02:07:49.620 --> 02:07:54.580]   This is good. I'm looking at all these... I went to knowyourmeme.com. All the fetch happened.
[02:07:54.580 --> 02:07:59.860]   Memes. Google, stop trying to make plus happen. It's not going to happen.
[02:07:59.860 --> 02:08:02.980]   Stop trying to make internet explorer happen. It's not going to happen. It didn't happen.
[02:08:02.980 --> 02:08:08.020]   In fact, they killed it. Stop trying to make being my default browser. It's not going to happen.
[02:08:08.020 --> 02:08:13.220]   Wow, this is good. This is good. I like it.
[02:08:13.220 --> 02:08:18.180]   And that's Regina George. Regina George is the ultimate mean girl.
[02:08:18.180 --> 02:08:21.380]   Stop trying to make fetch happen. It's not going to happen. A little doggy.
[02:08:21.380 --> 02:08:24.580]   That's my dog. My dog does not fetch.
[02:08:24.580 --> 02:08:27.540]   No, no fetching. Not going to happen. That's cute.
[02:08:30.580 --> 02:08:36.740]   Let's talk about Spotify. I have to say a little Schadenfreude on your article.
[02:08:36.740 --> 02:08:40.180]   You can say that. I was like, "Mike, I'm going to go to the lab."
[02:08:40.180 --> 02:08:48.820]   Schadenfreude on this one. Spotify's podcast colonization flops. What happened, Mike?
[02:08:48.820 --> 02:08:56.580]   Well, I mean, Spotify really tried to take over the podcast market and lock it up.
[02:08:56.580 --> 02:09:02.020]   Not necessarily behind its paywalt because they were offering podcasts for free.
[02:09:02.020 --> 02:09:05.380]   But Rogan was exclusive behind the paywall.
[02:09:05.380 --> 02:09:11.060]   Some didn't have to be because as long as it was exclusive, they could sell ads. In fact,
[02:09:11.060 --> 02:09:16.660]   Rogan's ads, I'm told, were a million dollars so they could make enough money on Rogan just
[02:09:16.660 --> 02:09:21.300]   on the free stuff. Yeah, I think all the podcast stuff is available for free.
[02:09:22.500 --> 02:09:28.980]   But in my mind, they weren't podcasts anymore. The whole idea of podcasts is that they were
[02:09:28.980 --> 02:09:33.860]   open. They were MP3 and RSS. You could listen to them on whatever player you wanted.
[02:09:33.860 --> 02:09:40.820]   What Spotify was doing was putting them exclusive. There were a whole bunch that they paid off.
[02:09:40.820 --> 02:09:45.060]   Big name, podcast is obviously Joe Rogan was the biggest, but a bunch of others as well.
[02:09:45.060 --> 02:09:49.060]   They bought a bunch of podcast studios, including Gimlet famously.
[02:09:49.060 --> 02:09:55.620]   And then also, what was it? The ringer. They bought the ringer. They spent a lot of money on that one.
[02:09:55.620 --> 02:10:03.780]   I was so jealous. Yeah, $250 million on it. But the idea was then that you could only listen to
[02:10:03.780 --> 02:10:09.140]   those on Spotify. And they also did that with the Obamas. The Obamas had to deal with Spotify,
[02:10:09.140 --> 02:10:13.300]   where you could only listen to the different podcasts that they were doing through Spotify.
[02:10:13.300 --> 02:10:18.420]   And I kept arguing, like, stop calling those podcasts. It's a proprietary audio format. It is not a
[02:10:18.420 --> 02:10:24.100]   podcast anymore. And the economics of why they were trying to do it was pretty clear, because
[02:10:24.100 --> 02:10:29.460]   they have to pay for music. And music every time it's played, they have to pay more for it, whereas
[02:10:29.460 --> 02:10:34.980]   a podcast they could pay up front and then not have to pay for every play. And so the economics
[02:10:34.980 --> 02:10:41.460]   were there. But I was very concerned that that was sort of killing off the great open nature of
[02:10:41.460 --> 02:10:46.740]   the podcast ecosystem built on open protocols that anyone could mix and match and find the player
[02:10:46.740 --> 02:10:51.540]   that you wanted and do all of these things. And so I was particularly concerned about Spotify,
[02:10:51.540 --> 02:10:54.980]   because they were big and powerful and had lots of money. There are a couple other players who were
[02:10:54.980 --> 02:11:00.100]   sort of trying to come into the space and do the same sort of thing. I heart, my former employer,
[02:11:00.100 --> 02:11:05.620]   Amazon with Audible. Yes. I'll tell you what, I had to stop using,
[02:11:05.620 --> 02:11:14.100]   what was it? I think it was Amazon music, because half the time instead of music, it was podcasts.
[02:11:14.820 --> 02:11:20.980]   It's like, no, no, I'm listening to music, Amazon music, but they kept saying, but you got to listen
[02:11:20.980 --> 02:11:26.740]   this ad free podcast, ad free podcast. Yeah. And by the way, can I say there's collateral damage?
[02:11:26.740 --> 02:11:31.940]   Because not only did Spotify buy those podcast companies and good for them, you know,
[02:11:31.940 --> 02:11:38.900]   sure, call her daddy got $60 million for her podcast. Good for her. I'm glad.
[02:11:38.900 --> 02:11:43.620]   They wouldn't give us any money, but that's okay. That's fine. I'm not jealous about that. But what
[02:11:43.620 --> 02:11:50.260]   was bad was they bought the two companies that we use for analytics that our advertisers told us
[02:11:50.260 --> 02:11:56.500]   we have to use chartable and pod sites. They bought both of them. And they turned off chartable.
[02:11:56.500 --> 02:12:02.020]   So we couldn't use chartable anymore. We could still use pod sites, but I think it's just a
[02:12:02.020 --> 02:12:05.700]   matter of time before they turn that off. And there's only one reason why they would do this.
[02:12:05.700 --> 02:12:10.900]   And this is the only reason the pod sites is still, I think, open, is to kill the ad market for
[02:12:10.900 --> 02:12:16.340]   everybody else. Because advertisers want these analytics, they can go to Spotify. Spotify knows
[02:12:16.340 --> 02:12:20.500]   because you're using the app exactly when you listen to how much you listen, which ads you heard,
[02:12:20.500 --> 02:12:25.220]   how many people they know your name, address phone numbers, credit card, they know everything.
[02:12:25.220 --> 02:12:31.540]   And an advertiser, they eat that stuff up. So the only way we compete was with these little,
[02:12:31.540 --> 02:12:37.060]   and we did it very in a very private focused way with pod sites. We still do,
[02:12:37.060 --> 02:12:46.580]   where the sponsor will put a pixel on their site, and we will send pod sites are download
[02:12:46.580 --> 02:12:51.700]   IP addresses. They hold them. They don't give them the advertiser. The advertiser sends the
[02:12:51.700 --> 02:12:57.380]   IP addresses that their pixel tracking pixel shows. Pod sites matches them and says to the
[02:12:57.380 --> 02:13:04.020]   advertiser, 43% of the people who heard your ad ended up on your website. Those metrics, at least,
[02:13:04.020 --> 02:13:08.500]   it's the minimum that frankly these days we can offer an advertiser and hope to get an ad by.
[02:13:08.500 --> 02:13:13.460]   But now that Spotify owns it, I don't know how much longer that's going to go. We were using
[02:13:13.460 --> 02:13:18.740]   chartable and they pulled the plug abruptly, by the way. We've caused us some deals.
[02:13:18.740 --> 02:13:24.180]   So I think they're really gunning for open podcasting.
[02:13:24.180 --> 02:13:28.420]   Yeah, well, they were, but it sounds like they're sort of recognizing.
[02:13:30.180 --> 02:13:36.100]   Whoops, it didn't work out so good. Daniel X said in hindsight, I got a little carried away and
[02:13:36.100 --> 02:13:43.460]   over invested half a billion, I think, right? More than that, more than that. And then,
[02:13:43.460 --> 02:13:50.660]   and I believe that a bunch of sort of top executives that were running the podcasting part of Spotify
[02:13:50.660 --> 02:13:56.740]   have all been rushing to the doors, sort of seeing where it's going. So it's not dead.
[02:13:56.740 --> 02:14:02.660]   The Obama's moved too audible, right? The Obama's moved. That was pretty early on where they moved,
[02:14:02.660 --> 02:14:09.300]   and there were some issues there. And it feels like whatever is left of GIMLIT is basically disintegrated.
[02:14:09.300 --> 02:14:16.500]   And a couple of the people that they had that were sort of leading the charge for podcasts are
[02:14:16.500 --> 02:14:21.300]   basically gone. And then in the earnings call, Daniel was basically saying like,
[02:14:21.300 --> 02:14:27.140]   yeah, maybe we made some mistakes, maybe we'll sort of pull back on the podcasting space.
[02:14:27.140 --> 02:14:30.820]   And so as you said, though, they didn't have much choice because they were getting squeezed by
[02:14:30.820 --> 02:14:37.780]   the record labels. That business was, at any moment, the labels can pull the plug on that.
[02:14:37.780 --> 02:14:43.220]   Yeah. And so again, like all of the incentives for everybody, right? The people who sold out,
[02:14:43.220 --> 02:14:48.660]   certainly it made sense, but the end result was sort of bad for the public.
[02:14:48.660 --> 02:14:56.900]   The people who didn't sell out like us. Not so good. And you know, I directly attribute that,
[02:14:56.900 --> 02:15:02.420]   there are other causes as well. But the podcast advertising has got fallen off a cliff. NPR
[02:15:02.420 --> 02:15:07.380]   said they've lost $300 million in advertising. And I think there was staff.
[02:15:07.380 --> 02:15:13.460]   They were saying that the podcasting space itself, that advertising, actually, the total amount of
[02:15:13.460 --> 02:15:17.860]   advertising and podcasts went down. And fewer podcasts launched in 2020,
[02:15:17.860 --> 02:15:19.540]   80% fewer 21. Yeah.
[02:15:19.540 --> 02:15:23.780]   And one of the things I think that I was talking to someone about this that
[02:15:23.780 --> 02:15:29.140]   deals with advertising. And they said one of the behaviors that they were kind of tracking and not
[02:15:29.140 --> 02:15:33.220]   sure if it was really making a difference or not was the fact that obviously in COVID,
[02:15:33.220 --> 02:15:38.260]   a lot less people have were driving. So of course, that took a huge hit because it just didn't have
[02:15:38.260 --> 02:15:44.820]   the same of the time that they used for it wasn't there anymore. And that the fact that a lot of them
[02:15:44.820 --> 02:15:49.060]   haven't, the thing that they're looking at right now is a lot of them haven't gone back to the
[02:15:49.060 --> 02:15:53.620]   office. So they're still not driving. Yeah, but I can tell you to be a moral relation. Our own
[02:15:53.620 --> 02:15:58.980]   information is that the numbers have come back dramatically since COVID. So they're right. And so,
[02:15:58.980 --> 02:16:02.820]   but overall, that's the calculation they were looking at. They came back, but did they come back
[02:16:02.820 --> 02:16:05.540]   to where they were pre COVID as the market did. Yeah, ours did. Yeah. Yeah. Yeah.
[02:16:06.820 --> 02:16:12.420]   But and I don't think that that's unusual because what's happened also in the last few years,
[02:16:12.420 --> 02:16:17.220]   according to Edison is the number of Americans, and I guess is true globally, but they have
[02:16:17.220 --> 02:16:23.540]   Americans numbers who listen to podcasts is creased dramatically. So the awareness of podcasting,
[02:16:23.540 --> 02:16:30.180]   and that's maybe something Spotify did do as a favor. And I also wonder how much all the audio,
[02:16:30.180 --> 02:16:34.900]   books on audio, and now there's other content. Yeah, there's so much. So I listen to, I mean,
[02:16:34.900 --> 02:16:39.780]   I'm listening to content all the time. Like I don't read, really read content. I listen to content.
[02:16:39.780 --> 02:16:45.060]   And and I, but I noticed that that cuts into everything else. I'm listening to foreign affairs,
[02:16:45.060 --> 02:16:50.500]   the economist or whatever the news over air, audio or whatever. I'm listening to all of those
[02:16:50.500 --> 02:16:55.780]   things. And then I do then listen to some podcasts and then I read books and or listen to books. And
[02:16:55.780 --> 02:16:59.700]   so I think people are the behaviors there. I think the problem really is, is that it's just
[02:16:59.700 --> 02:17:05.860]   getting spread out to almost everything. I do have another theory, which is that, you know,
[02:17:05.860 --> 02:17:11.540]   as connected TV grows in usage, and that's, you know, that's everything from Disney Plus to Netflix
[02:17:11.540 --> 02:17:19.780]   with ads, to the, you know, the new fast channels, Pluto, to be all of those. There's just a lot more
[02:17:19.780 --> 02:17:27.540]   options for advertisers, and it's video, it's more attractive arguably than audio only ads. And,
[02:17:27.540 --> 02:17:33.060]   you know, I feel like podcasts are just getting squeezed. We're definitely squeezed. I don't know
[02:17:33.060 --> 02:17:39.620]   why, but yeah, I mean, YouTubers are getting a lot of those views too, right? I think, you know,
[02:17:39.620 --> 02:17:45.140]   there's always the flavor of the month. And we were only the flavor of the month for about three
[02:17:45.140 --> 02:17:51.700]   seconds. So if I recall alphabet's latest earnings, YouTube, YouTube's growth was kind of, yeah,
[02:17:51.700 --> 02:17:57.300]   it was it was flat or dead. Yeah, it was flat. And and that's because there's a lot more
[02:17:57.300 --> 02:18:03.780]   competition for, you know, basically targetable, addressable, online video ad inventory.
[02:18:03.780 --> 02:18:08.580]   Is streaming suffering though? I mean, I mean, it seems like a lot of media is suffering, maybe
[02:18:08.580 --> 02:18:14.980]   because of a glut of content. Well, I think the issue with streaming is more that companies were
[02:18:14.980 --> 02:18:18.820]   like massively over investing. And that's kind of like what Spotify did with podcasts.
[02:18:18.820 --> 02:18:25.140]   And they were over investing because they thought Wall Street wanted growth over,
[02:18:25.860 --> 02:18:30.580]   you know, over every over profitability, really, you know, subscriber count over
[02:18:30.580 --> 02:18:35.620]   anything else like Wall Street wanted to know that you were spending a ton on content and that
[02:18:35.620 --> 02:18:39.700]   your subscribers were growing. And they didn't really care about anything else. And then suddenly,
[02:18:39.700 --> 02:18:46.340]   you know, Wall Street started caring about profitability again. So it was it was a land grab,
[02:18:46.340 --> 02:18:51.300]   right? It was you try and get in everybody with the idea that that later on down the road, you'll
[02:18:51.300 --> 02:18:56.180]   you'll be able to own the market someday. And I think I think the other thing is that you did have
[02:18:56.180 --> 02:19:00.340]   to get in because there was a certain level of saturation. I know that I still haven't subscribed
[02:19:00.340 --> 02:19:03.620]   to Paramount because I got to a point where I was like, I can't subscribe to any more of these.
[02:19:03.620 --> 02:19:11.380]   Like I just I'm on, I have Netflix and Apple and Amazon and HBO and I kind of in Disney.
[02:19:11.380 --> 02:19:15.700]   And I was like, okay, I've got so much content right now along with YouTube and YouTube TV
[02:19:15.700 --> 02:19:20.660]   that I'm done. And so anything that came after that, that's why I think they had to rush in,
[02:19:20.660 --> 02:19:24.580]   is because anything that came after that, there's a whole lot of people that, you know, don't want
[02:19:24.580 --> 02:19:30.580]   to buy anymore. Well, YouTube's getting into podcasting, right? They've been slowly building
[02:19:30.580 --> 02:19:36.980]   their podcast tools and now have a create button that lets you actually use YouTube to create a
[02:19:36.980 --> 02:19:43.620]   podcast post it. I don't know if they have to make video, which is, you know, what's weird about
[02:19:43.620 --> 02:19:50.580]   that is like so many podcasts, you know, present company included like posts on YouTube and
[02:19:50.580 --> 02:19:55.140]   distribute as an audio podcast. Already do. I mean, a bunch of the podcasts that I listen to,
[02:19:55.140 --> 02:19:59.620]   I know, are also YouTube videos. So I wasn't entirely clear on what YouTube is doing here.
[02:19:59.620 --> 02:20:02.580]   Just easier to do it. I think that's all.
[02:20:02.580 --> 02:20:04.180]   It's just recognizing the behavior.
[02:20:04.180 --> 02:20:09.380]   So in our case, we create it outside of YouTube and then we put it on YouTube.
[02:20:09.380 --> 02:20:14.020]   They want you to create it. You want it to be exclusive, ultimately. But the question,
[02:20:14.020 --> 02:20:19.460]   and again, like this was not clear to me from that story, is it going to actually be a podcast,
[02:20:19.460 --> 02:20:23.300]   or is it going to be proprietary to YouTube, big end? It's not a real podcast.
[02:20:23.300 --> 02:20:27.220]   So it's like Spotify. It's another. If you create, you're not loaded.
[02:20:27.220 --> 02:20:33.300]   If you create a podcast through this YouTube thing, can people listen to it outside of the
[02:20:33.300 --> 02:20:37.700]   YouTube ecosystem or are they forced to? Well, it's probably not an RSS feed, right?
[02:20:37.700 --> 02:20:40.420]   They're not forcing you to be, you could download it and put it out.
[02:20:40.420 --> 02:20:44.660]   But I don't think that you wouldn't be able to, it's not going to be an RSS feed, but what
[02:20:44.660 --> 02:20:46.420]   it would do. That's the question. That's the question.
[02:20:46.420 --> 02:20:46.980]   That's the question. That's the question. That's the question.
[02:20:46.980 --> 02:20:48.340]   Yeah. Yeah. Yeah.
[02:20:48.340 --> 02:20:51.540]   But look at the push. If you go to youtube.com/podcast,
[02:20:51.540 --> 02:20:57.700]   I mean, this is a big push for YouTube. You know what? They want to own all content.
[02:20:57.700 --> 02:21:03.700]   Right. That's, I mean, you know, obviously, regular people who are watching content,
[02:21:03.700 --> 02:21:08.420]   that is that they are listening to every week or every month or whatever is compelling for
[02:21:08.420 --> 02:21:11.220]   anybody that sells media. And so I think that that definitely makes sense. I think the problem
[02:21:11.220 --> 02:21:13.860]   is, is when you make all this stuff easy and they, you know, YouTube's been doing this for a
[02:21:13.860 --> 02:21:18.180]   long time. Like you said, go live and now you can go live. The problem really is, is that,
[02:21:18.180 --> 02:21:21.300]   is that most people don't know how to do that very well. And I think one of the problems with
[02:21:21.300 --> 02:21:25.620]   podcast and this isn't a problem here because Twitter pays a lot of attention to quality and
[02:21:25.620 --> 02:21:31.220]   everything else, but a lot of podcasts to me are unlistenable on a variety of levels, both in the
[02:21:31.220 --> 02:21:34.980]   content, the organization of that content and the quality of the audio.
[02:21:34.980 --> 02:21:37.940]   And so most people don't do most of the content.
[02:21:37.940 --> 02:21:38.420]   Well, that awesome.
[02:21:38.420 --> 02:21:39.540]   It's easier to hit the button.
[02:21:39.540 --> 02:21:40.340]   That hurts us.
[02:21:40.340 --> 02:21:40.900]   Make it make it make it.
[02:21:40.900 --> 02:21:45.380]   That hurts us too, because people make judgments about podcasts based on what they've heard.
[02:21:45.380 --> 02:21:53.140]   You know, all the celebrity podcasts that for a while were going to transform podcasting.
[02:21:53.140 --> 02:21:59.060]   The celebrities got bored or got a movie deal and those went away. So it's all hurt us because
[02:21:59.060 --> 02:22:04.500]   here we've been for 19 or whatever it is years since 2015, 18 years doing this.
[02:22:04.500 --> 02:22:11.700]   And we haven't changed at all, by the way. Maybe that's our mistake. We haven't changed at all,
[02:22:11.700 --> 02:22:16.340]   but people have made judgments about. And I think our effectiveness for advertisers hasn't
[02:22:16.340 --> 02:22:21.540]   changed and our audience size has gone up. But that doesn't really doesn't really help.
[02:22:21.540 --> 02:22:24.100]   Well, just just just wait until chat GPT starts.
[02:22:24.100 --> 02:22:24.980]   Oh, great.
[02:22:26.020 --> 02:22:33.380]   Oh, great. Actually, one of our listeners in our IRC has submitted a chat GPT
[02:22:33.380 --> 02:22:40.980]   written ad for stamps.com. That's quite good. Actually, sound of a busy office with printers
[02:22:40.980 --> 02:22:45.460]   worrying and phones ringing in the background. Are you tired of waiting in long lines at the
[02:22:45.460 --> 02:22:50.180]   post office just to buy stamps? This is exactly it. It's it's
[02:22:50.180 --> 02:22:55.460]   I feel bad for our cut. We have a we have a person who writes copy.
[02:22:55.460 --> 02:23:03.620]   Yeah. All they have to do is make up a podcast, do some voicing and profit.
[02:23:03.620 --> 02:23:06.660]   There is there is generative AI voicing. So you can.
[02:23:06.660 --> 02:23:08.180]   Oh, I know. I'm in trouble.
[02:23:08.180 --> 02:23:15.140]   Well, I did not have a chat GPT right this next ad. We're going to quickly mention our sponsor and
[02:23:15.140 --> 02:23:19.780]   then wrap things up. We have a few more big stories, but I do want to talk about ACI learning.
[02:23:19.780 --> 02:23:24.340]   You've probably throughout the show seen the banner for ACI learning and saying,
[02:23:24.340 --> 02:23:28.180]   well, what is this ACI learning? Well, thank you ACI learning. First of all, for buying
[02:23:28.180 --> 02:23:33.220]   studio sponsorship because that makes a big difference to us. But you probably already know
[02:23:33.220 --> 02:23:41.620]   them as I T pro I T pro has merged with ACI learning to create a new and I think fantastic
[02:23:41.620 --> 02:23:48.900]   set of tools for you to get into it. That's what it's all about. Either get that first job
[02:23:48.900 --> 02:23:54.900]   or get a better job in it with it pro and ACI learning ACI learning is expanded their production
[02:23:54.900 --> 02:24:00.900]   capabilities, bringing you the content you want. And now thanks to ACI learning the style you want
[02:24:00.900 --> 02:24:08.740]   on demand live streaming or in person with the ACI learning hubs. So if for instance, you want to
[02:24:08.740 --> 02:24:14.420]   learn a lot of content through on demand videos, you can do that and then periodically maybe go
[02:24:14.420 --> 02:24:20.020]   for a few days to an ACI hub and get some hands on and experience with an instructor, you can do
[02:24:20.020 --> 02:24:25.620]   that. And that's what's great, the flexibility of the and the power of ACI learning and I T pro
[02:24:25.620 --> 02:24:31.780]   together. I T pro has been such a success. They joined us very early on they had just started.
[02:24:31.780 --> 02:24:39.060]   Tim and Don were fans. There's Don Pazette scrolling by on the screen. He's still putting all these
[02:24:39.060 --> 02:24:44.180]   great videos together. And over the 10 years they've been with us, they've built a 227,000
[02:24:44.180 --> 02:24:52.420]   strong community of I T pro learners, 6,800 hours of content, always fresh because they're always
[02:24:52.420 --> 02:24:57.860]   adding new content. They also get all sorts of training. Look at their podcasts. By the way,
[02:24:57.860 --> 02:25:03.460]   really good Don does tech NATO. And now because there's audit pro as well, the skeptical auditor
[02:25:03.460 --> 02:25:09.060]   podcast another great way. If you want to listen to a podcast to get some sense of the business,
[02:25:09.060 --> 02:25:14.420]   the great thing about it is it's growing. There are more jobs now open than ever before,
[02:25:14.420 --> 02:25:20.420]   unfilled in many cases, especially in cyber security. So it really is a great time to get into it.
[02:25:20.420 --> 02:25:26.580]   You can get team training for your team too from it pro. If you've got an it team, a security team,
[02:25:26.580 --> 02:25:31.860]   an audit team to keep them up to date on the latest information, get their up skilled,
[02:25:31.860 --> 02:25:37.140]   even get new certs and new skills, which is they'll love and you'll love the benefits for you. You
[02:25:37.140 --> 02:25:44.900]   can get team training for CompTIA, Microsoft IT Cisco Linux Apple security cloud and on and on.
[02:25:44.900 --> 02:25:50.900]   We talk a lot about the CompTIA certs. The entry level cert for IT is the A plus cert. CompTIA
[02:25:50.900 --> 02:25:55.620]   courses from IT pro and ACI learning make it easy to level up your employees who are interested
[02:25:55.620 --> 02:26:04.420]   in cyber security as well. The cybersecurity certs include CISSP, AWS, ISACA, CCNA, Technical Support
[02:26:04.420 --> 02:26:09.220]   Specialist, Computer User Support Specialist, Information Security Analyst, and a whole lot
[02:26:09.220 --> 02:26:14.900]   more. They have live chat rooms. If you're watching live, just like we do, they also have full transcripts
[02:26:14.900 --> 02:26:19.780]   of everything. So you can read along. A lot of people find it easier to read than listen.
[02:26:19.780 --> 02:26:24.020]   Sometimes people like to do both. That's the beauty of this with ACI learning. Now you have
[02:26:24.020 --> 02:26:30.740]   basically any kind of way you like to learn. They've got that for you. Certifications are so
[02:26:30.740 --> 02:26:35.940]   important, not just because they certify that you have that knowledge, that skill set,
[02:26:35.940 --> 02:26:43.060]   but it also shows your potential employer or your boss or your customers that you're committed to
[02:26:43.060 --> 02:26:48.420]   keeping up to date. Your organization is committed to keeping those skills up to date so your stuff
[02:26:48.420 --> 02:26:54.500]   is secure and safe, right? Everybody wants that nowadays. ACI learning can help you
[02:26:54.500 --> 02:27:00.180]   with fully customizable training for your team. Their dashboards, great tracks, your team's metrics
[02:27:00.180 --> 02:27:05.620]   like logins, viewing time, tracks completed, monthly usage reports. You get visual reports you can
[02:27:05.620 --> 02:27:10.500]   show to the boss or the stakeholders so they know they're getting an ROI for their investment.
[02:27:10.500 --> 02:27:16.980]   Your team's love it too. It's engaging, entertaining, and very informative. So they think it's a real
[02:27:16.980 --> 02:27:21.140]   benefit to them. It's a wonderful benefit to provide and you get the benefit of their
[02:27:21.140 --> 02:27:27.940]   skills and their knowledge, which is fantastic. Respected companies and government agencies around
[02:27:27.940 --> 02:27:33.460]   the globe turn to IT Pro and ACI learning year after year to help them maintain their competitive edge.
[02:27:33.460 --> 02:27:41.140]   Supporting organizations across audit, IT, cybersecurity readiness, ACI learning keeps you and your
[02:27:41.140 --> 02:27:46.180]   team at the top of your game. So for entry level training or for putting people on the moon,
[02:27:46.180 --> 02:27:52.660]   ACI learning has it covered. Maintain your company's competitive edge with ACI learning. Here's the
[02:27:52.660 --> 02:28:00.580]   website. Go.acilearning.com/twit. If you want to start today with a standard or premium individual
[02:28:00.580 --> 02:28:13.220]   IT Pro membership, use the offer code TWIT30. You'll get 30% off. Visit go.acilearning.com/twit. Go.acilearning.com/twit. A great company,
[02:28:13.220 --> 02:28:19.860]   great partnership. We're really happy to be with ACI and IT Pro. We have been over the years and I
[02:28:19.860 --> 02:28:25.460]   think you're going to find some real value there. So check it out. Go.acilearning.com/twit. Don't forget
[02:28:25.460 --> 02:28:36.100]   that/twit. That's really important. Go.acilearning.com/twit. Victor prepared a lovely short video for us
[02:28:36.100 --> 02:28:42.260]   about all the fun and games we had this week on Twitch. Watch. I put Windows on my MacBook.
[02:28:42.260 --> 02:28:51.540]   Ooh, how exciting. It's now official. You can use Windows on ARM in parallels at least right now,
[02:28:51.540 --> 02:28:55.860]   but it runs pretty... I was surprised how snappy it was. I was able to play solitaire just, you know,
[02:28:55.860 --> 02:29:01.540]   like that. It's great. No problem. Let's not lose our mind with free-sells. I have solitaire.
[02:29:01.540 --> 02:29:08.740]   Previously on Twitch, this week in Google, attorney Kathy Gellis was actually in the courtroom
[02:29:08.740 --> 02:29:15.860]   yesterday during oral arguments, the Supreme Court case, Google versus Gensales, the future of the
[02:29:15.860 --> 02:29:23.860]   internet hangs in the balance. We were expecting a very hostile audience that was holding onto a
[02:29:23.860 --> 02:29:30.820]   lot of the myths about 230. We ended up getting a bench that was surprisingly informed and seemed
[02:29:30.820 --> 02:29:37.540]   to get it and seem to understand what was at stake. Tech News Weekly. Mark Gurman from Bloomberg talks
[02:29:37.540 --> 02:29:42.900]   all about Apple's progress, the progress that they're making with glucose tracking with the Apple
[02:29:42.900 --> 02:29:49.380]   Watch. It uses a technology called optical spectroscopy as well as a new chick technology
[02:29:49.380 --> 02:29:55.460]   called Silicon Photonics. It's using lights and lasers that shoot light into what is known as
[02:29:55.460 --> 02:30:02.020]   interstitial fluid, right? And it's able to then reflect back to sensors beneath the watch.
[02:30:02.020 --> 02:30:07.780]   And what's reflecting is the concentration of glucose within that fluid. All about Android.
[02:30:07.780 --> 02:30:14.100]   Our good friends at Huawei actually are working on a new wearable and
[02:30:14.100 --> 02:30:23.220]   Huawei working on a wearable. They're called watch buds. And it's a smart watch with earbuds
[02:30:23.220 --> 02:30:28.260]   that dock inside the watch itself. If you missed to it this week, you missed a lot.
[02:30:29.780 --> 02:30:34.980]   Yeah. Some really interesting. This has been a good week for Tech News. A lot of interesting
[02:30:34.980 --> 02:30:40.260]   stuff happening. And we covered it all on Twitter. A few more stories before we wrap things up
[02:30:40.260 --> 02:30:46.420]   with our wonderful panel. Mike Masnick is here from tech dirt.com. He's on Massed on Social.
[02:30:46.420 --> 02:30:54.740]   And Masnick. Yay. M-A-S-N-I-C-K. You could follow him with his RSS feed. What do you do?
[02:30:54.740 --> 02:31:00.260]   Slash RSS. Something like that. After the end. I forget. I forget. You could Google it. You could
[02:31:00.260 --> 02:31:07.060]   or Bing it. Bing it. Ask chat GPT. It'll know. Also with us Owen Thomas. Great to see the Herb
[02:31:07.060 --> 02:31:12.900]   Kane of Silicon Valley. Columnist at the San Francisco Examiner. So nice to see you from your
[02:31:12.900 --> 02:31:20.260]   house instead of the boring old protocol offices. I liked it. It was a nice perch downtown. But
[02:31:20.260 --> 02:31:25.300]   I'm back in North Beach. Yeah. Are you North Beach? I love North Beach. You used to live there.
[02:31:25.300 --> 02:31:32.660]   Do you ever go to Mario's Bohemian cigar store for a focaccia sandwich and a latte?
[02:31:32.660 --> 02:31:40.260]   That's on the other side of Washington Square. It's a good-go-cross-lodgen square. Come on.
[02:31:40.260 --> 02:31:47.060]   I'm kind of a Freddy sandwiches. Oh, okay. But you're on the side of the good focaccia over there.
[02:31:47.060 --> 02:31:51.300]   That's the best focaccia right next to the church. There's a- Oh, La Guria, Baker.
[02:31:51.300 --> 02:31:57.300]   La Guria. Oh my god. But you have to like they start baking it like five in the morning and
[02:31:57.300 --> 02:32:00.820]   they're sold out by eleven. There's a long line out the door and if you don't get there before
[02:32:00.820 --> 02:32:05.380]   noon forget it. And then they wrap it in brown paper with twine. I bet they still do.
[02:32:05.380 --> 02:32:11.940]   It's- Oh yeah. Yeah. It's the best. The guy just like trolls it and like it's suddenly wrapped.
[02:32:11.940 --> 02:32:17.300]   Yeah. It's amazing. It's so cool. Oh, jealous. I love- I used to- I love living in North Beach.
[02:32:17.300 --> 02:32:23.620]   The Italian- Old Italian district of San Francisco. And from beautiful downtown Marin,
[02:32:23.620 --> 02:32:29.620]   it's Alex Lindsay, office hours.global. I wear downtown Marin. I guess there's Marin City.
[02:32:29.620 --> 02:32:34.900]   There is no downtown Marin. I make a joke. It's like South Detroit. There's no such thing.
[02:32:34.900 --> 02:32:39.220]   Exactly. It's just- it's a concept. Not a geographic location.
[02:32:40.980 --> 02:32:45.140]   This was an old story. You may remember a few years ago.
[02:32:45.140 --> 02:32:51.220]   Remember that Twitter hack where the kid got on and asked for Bitcoin said if you send me
[02:32:51.220 --> 02:32:56.420]   Bitcoin, I'll double it. I'll send it back doubled. He hacked Joe Biden's account and Barack Obama's
[02:32:56.420 --> 02:33:02.820]   account and lots of famous people. Probably didn't make a lot of money. Kim Kardashian,
[02:33:02.820 --> 02:33:09.300]   Bill Gates, Warren Buffett, Benjamin Netanyahu, Jeff Bezos, Michael Bloomberg and Kanye West.
[02:33:09.300 --> 02:33:15.620]   And you know it was fake because Kanye isn't talking about Bitcoin. Anyway, he got caught.
[02:33:15.620 --> 02:33:24.180]   Just a kid. He's 20 years old at the time. He is now being held in Spain, but a Spanish court has-
[02:33:24.180 --> 02:33:27.940]   he's British. I don't know what he's doing in Spain, but a Spanish court has said,
[02:33:27.940 --> 02:33:37.860]   "Yep. He can be remanded to the US extradited for trial there." So they got him. I like to do
[02:33:37.860 --> 02:33:42.180]   stories where they get these guys because- The long arm of the cyber.
[02:33:42.180 --> 02:33:47.060]   Slow, but eventually. Yeah. I don't. It was a dumb hack.
[02:33:47.060 --> 02:33:52.340]   Daddy made any money on it, but anyway. I don't care where they make money or not.
[02:33:52.340 --> 02:33:56.820]   I just think that you get a half the- If you're out there causing mischief, it should be
[02:33:56.820 --> 02:34:02.900]   adjudicated. So encryption did not protect him. Whatever it was.
[02:34:03.620 --> 02:34:09.060]   Exactly. You know what happens? And I've talked to many people in the law enforcement, the Secret
[02:34:09.060 --> 02:34:13.940]   Service. A lot of these guys boast. They go in a chat room and say, "Hey, I did that." Because if
[02:34:13.940 --> 02:34:21.140]   you do that and you don't boast, does anybody know who you are? No, you can't tell- Why do it if
[02:34:21.140 --> 02:34:26.100]   you can't boast. Why do it if you can't tell your friends? That's the whole point. So they always
[02:34:26.100 --> 02:34:29.620]   incriminate themselves in the law. Maybe not the ransomware gangs.
[02:34:30.980 --> 02:34:39.140]   Apple has reputedly secured every single three nanometer chip that TSMC can make
[02:34:39.140 --> 02:34:47.300]   for the iPhone 15 and perhaps the M3 chip and upcoming Mac books later this year or early next
[02:34:47.300 --> 02:34:53.140]   year. Now, that might sound like good news. It's certainly bad news for everybody else.
[02:34:53.140 --> 02:34:58.900]   But I should mention that even though the yields are good on these three nanometer, the N3 process,
[02:35:00.260 --> 02:35:08.100]   they're only making about 45,000 wafers a month. So you do the math. That is not enough for all
[02:35:08.100 --> 02:35:13.220]   the iPhones they'll sell. It's not enough for all the iPhones they'd sell in a month. What is that?
[02:35:13.220 --> 02:35:21.380]   45,000 times 12. It's five to six million. It may be enough to sell for the Macs or the
[02:35:21.380 --> 02:35:27.460]   Macs or the iPads or something like that. And again, that's usually where it starts.
[02:35:28.020 --> 02:35:32.820]   But it doesn't sound like they may wait another year for the iPhones themselves to do this,
[02:35:32.820 --> 02:35:36.820]   whereas they can get it started with the larger devices.
[02:35:36.820 --> 02:35:41.540]   We'll talk about this on the Mac break, I'm sure, on Tuesday.
[02:35:41.540 --> 02:35:46.580]   I mean, yeah, it's good news for Apple. They can they can sew up that production.
[02:35:46.580 --> 02:35:49.700]   Bad news for everybody else.
[02:35:53.780 --> 02:36:02.100]   This was a little bit annoying. There is currently a lawsuit by the filmmakers who made such
[02:36:02.100 --> 02:36:10.180]   wonderful movies as the Hitman's wife's bodyguard. London has fallen and hell boy.
[02:36:10.180 --> 02:36:17.620]   These film companies were suing internet provider R.C.N. because they're not doing enough to stop
[02:36:17.620 --> 02:36:28.180]   subscribers from pirating on their network. Now, now R.C.N., which denied most of the allegations,
[02:36:28.180 --> 02:36:38.340]   try to get the case dismissed. Now, they're going after Redditors, people who anonymously
[02:36:38.340 --> 02:36:46.100]   posted on Reddit talking about piracy, not committing it, just talking about it.
[02:36:46.100 --> 02:36:51.620]   Some Reddit users specifically mentioned R.C.N. others referred to ISPs in more general terms,
[02:36:51.620 --> 02:36:58.500]   which could be could be R.C.N. We don't know. The filmmakers want to know who these people are.
[02:36:58.500 --> 02:37:10.180]   So they have obtained a subpoena compelling Reddit to tell the movie makers who been 125125
[02:37:10.980 --> 02:37:18.180]   squatting, Croat, Gryftog 21, a romantic botanist, Chikara fan, copy pack,
[02:37:18.180 --> 02:37:24.820]   Dot Samantha, I like Pi 96 and Matt 3324R.
[02:37:24.820 --> 02:37:32.500]   Wow. I can't believe I can't believe the court went along with this. Reddit, of course, is going
[02:37:32.500 --> 02:37:40.340]   to fight it, but I think Reddit did turn over one. They've been Ben, Ben 125125.
[02:37:40.980 --> 02:37:50.580]   Got handed over. I mean, it's all of these cases are a little bit sketchy for a variety of reasons,
[02:37:50.580 --> 02:37:58.260]   but what they're looking for, I believe, from what details are revealed is that they claim
[02:37:58.260 --> 02:38:04.340]   that these people are saying on Reddit that R.C.N. was not killing their account, so therefore,
[02:38:04.340 --> 02:38:10.500]   they see it as evidence in their case. But it feels like a stretch and you're dragging in these
[02:38:10.500 --> 02:38:16.100]   random people. And as soon as you receive a subpoena, that's serious and they got to lawyer up.
[02:38:16.100 --> 02:38:19.540]   That's really very aggressive.
[02:38:19.540 --> 02:38:26.900]   Si. But you know what? The Hitman's wife's bodyguards, friends and family got to get that
[02:38:26.900 --> 02:38:34.980]   money. So okay. Okay. I don't know what's going to happen with this earlier this week,
[02:38:34.980 --> 02:38:45.220]   actually was early last week, the White House decided not to block the ITC's ban on the Apple Watch.
[02:38:45.220 --> 02:38:52.500]   Yeah. A live core, which makes and a great product, by the way, the Cartier device,
[02:38:52.500 --> 02:39:01.460]   which allows you to do a kind of a little AKG with your thumbs, they claim that Apple stole that
[02:39:01.460 --> 02:39:07.860]   and put it in the Apple Watch. They're suing the ITC ruled and even though, okay, now this is
[02:39:07.860 --> 02:39:11.780]   what's really weird. The patent and trademark office said, yeah, that patent sucks and they
[02:39:11.780 --> 02:39:16.980]   threw it out. Nevertheless, the International Trade Commission ruled that Apple infringed
[02:39:16.980 --> 02:39:25.380]   on this non-existent patent and they're potentially going to block imports of Apple watches.
[02:39:25.380 --> 02:39:33.060]   Yeah. White House declined to get involved. This one is kind of crazy because of the whole
[02:39:33.060 --> 02:39:38.260]   patent and trademark office. And there was some really interesting timing on all of this,
[02:39:38.260 --> 02:39:43.860]   which is that you have these dual processes and this could get into the weeds. So I'm not going
[02:39:43.860 --> 02:39:50.740]   to go too deep on it where the ITC process, which is the International Trade Commission and then the
[02:39:50.740 --> 02:39:54.820]   PTED, the Patent and Trademark Appeals Board process, are two totally separate processes,
[02:39:54.820 --> 02:40:00.820]   which actually allow, well, and the end of regular court process, it allows patent holders to go
[02:40:00.820 --> 02:40:06.900]   through multiple routes. And the ITC was supposed to rule first on these things, but then delayed
[02:40:06.900 --> 02:40:13.780]   its ruling until after the Patent and Trademark Appeals Board came out and said,
[02:40:13.860 --> 02:40:17.540]   all of these patents are invalid. We never should have granted them in the first place.
[02:40:17.540 --> 02:40:23.860]   The patents are terrible, like get rid of them. And the ITC, which, you know, technically is its
[02:40:23.860 --> 02:40:29.060]   own independent organization and doesn't have to listen to that, then came out two weeks later
[02:40:29.060 --> 02:40:36.820]   and said, well, Apple violates these patents and therefore, and the only remedy that the ITC
[02:40:36.820 --> 02:40:41.620]   has, they can't do fines, but they can block import. So you can't bring it into the country.
[02:40:41.620 --> 02:40:48.260]   Yeah, you can't bring it into the country. So Apple ran to the White House, which had protected them
[02:40:48.260 --> 02:40:52.900]   a decade ago. Obama protected them. There was an iPad block, right?
[02:40:52.900 --> 02:41:00.100]   Right. With Samsung, it was a very similar sort of situation. And Obama rejected the ITC's
[02:41:00.100 --> 02:41:04.900]   recommendation and allowed the import to continue. And so Apple, I think, was very much hoping.
[02:41:04.900 --> 02:41:09.380]   And I think for good reason, they had a pretty strong argument in that the PTEDB had said,
[02:41:09.380 --> 02:41:14.820]   these patents are not valid. So the whole thing is a joke. And yet, for whatever reason,
[02:41:14.820 --> 02:41:18.100]   the Biden administration decided not to go against the ITC.
[02:41:18.100 --> 02:41:24.740]   Chants in hell that the Apple Watch would be blocked. Sure. It absolutely could be.
[02:41:24.740 --> 02:41:30.420]   The more likely thing is that Apple is going to cough up a huge sum of money to a lifecore and
[02:41:30.420 --> 02:41:33.540]   or Apple will buy a lifecore and bury them. Right.
[02:41:33.540 --> 02:41:36.740]   Right. That's kind of the one thing.
[02:41:36.740 --> 02:41:41.140]   They have to be careful here because actually, although Apple brought in
[02:41:41.140 --> 02:41:49.780]   a whole bunch of, I think, 500 testimonials saying, "Apple Watch saved my life." A lifecore
[02:41:49.780 --> 02:41:53.300]   saved some lives too. A lifecore was a big breakthrough in AFib.
[02:41:53.300 --> 02:41:58.500]   So, I mean, it had a breakthrough. But the thing is, is that number one, the patents didn't hold
[02:41:58.500 --> 02:41:59.860]   up. It didn't hold up.
[02:41:59.860 --> 02:42:04.180]   Is the problem really is, is there's a big difference between having to buy something different to
[02:42:04.180 --> 02:42:08.260]   have to measure what your situation is and having it on your watch?
[02:42:08.260 --> 02:42:13.380]   And I think the public value of having an on your watch is pretty high.
[02:42:13.380 --> 02:42:19.300]   So, the idea of, I think that Apple's making is, number one, the patent isn't, I mean,
[02:42:19.300 --> 02:42:28.020]   this case is nuts. The idea that the patent isn't valid. And the idea of blocking it when it
[02:42:28.020 --> 02:42:34.340]   actually is making a measurable difference in life-saving at a level that's at least 100,
[02:42:34.340 --> 02:42:40.500]   if not 1,000 times what a lifecore can do is just a cookie. I mean, this is the government
[02:42:40.500 --> 02:42:48.660]   runner-ry. This is bureaucrats doing bureaucrat things like not thinking about anything else.
[02:42:48.660 --> 02:42:50.660]   It's nutty.
[02:42:50.660 --> 02:42:51.140]   Can you...
[02:42:51.140 --> 02:42:57.780]   Well, I put it on a lifecore too. They are clearly pushing this angle as hard as they
[02:42:57.780 --> 02:43:01.460]   can under presumably because they don't have a business model with that. With Apple going down
[02:43:01.460 --> 02:43:04.420]   the path that's going, a lifecore is dead. That's the problem.
[02:43:04.420 --> 02:43:09.940]   Is that like, if Apple puts all the health stuff into your watch, and even if it's not,
[02:43:09.940 --> 02:43:12.260]   you're their toast, they're toast. That's the problem.
[02:43:12.260 --> 02:43:12.500]   Right.
[02:43:12.500 --> 02:43:21.860]   Well, maybe what you should do is buy a sealed iPhone and hold it for 20 some years or whatever
[02:43:22.420 --> 02:43:29.620]   because you could make some money. That's what Karen Green did. She bought the original iPhone
[02:43:29.620 --> 02:43:43.620]   in 2007 and never opened it. The sealed iPhone in a box just sold for $63,356 to some
[02:43:43.620 --> 02:43:48.020]   cracked collector. I guess...
[02:43:48.900 --> 02:43:54.900]   That's amazing foresight because the original iPhone, the first generation iPhone,
[02:43:54.900 --> 02:44:00.740]   was not a big seller. It was seen as a novelty, like a...
[02:44:00.740 --> 02:44:08.980]   It was a tack on to Apple's very large successful iPod business. It even used the 30-pin iPod
[02:44:08.980 --> 02:44:11.220]   connector. Basically, an iPod...
[02:44:11.220 --> 02:44:14.100]   We forget that. It didn't have cut and paste in the apps.
[02:44:15.460 --> 02:44:19.380]   And yet it was a revolution. Alex, you and I were doing Mac break week. I remember Scott
[02:44:19.380 --> 02:44:23.300]   Bourn made a... wanted to make a fist. Yeah, we all went down.
[02:44:23.300 --> 02:44:26.020]   Scott and I, Scott and I, especially, we were definitely on the...
[02:44:26.020 --> 02:44:31.300]   There was a picture of him in the San Francisco Chronicle as the first guy to come up with the
[02:44:31.300 --> 02:44:36.180]   Apple Store. He was holding up his iPhone like this with that original iPhone. Scott, if you'd only
[02:44:36.180 --> 02:44:37.460]   put it away...
[02:44:37.460 --> 02:44:38.980]   Exactly.
[02:44:38.980 --> 02:44:39.780]   ...in front of the bottom.
[02:44:39.780 --> 02:44:42.740]   Has anyone found out why this iPhone was never opened?
[02:44:45.300 --> 02:44:46.740]   No. Unknown.
[02:44:46.740 --> 02:44:48.980]   Maybe Karen got two?
[02:44:48.980 --> 02:44:52.180]   Maybe it was a corporate buy? No, no, we don't know.
[02:44:52.180 --> 02:44:56.020]   Because that's the part that I'm wondering about.
[02:44:56.020 --> 02:45:00.740]   Because there's not a cheap item for most people.
[02:45:00.740 --> 02:45:00.740]   For most people.
[02:45:00.740 --> 02:45:01.700]   For 500 bucks.
[02:45:01.700 --> 02:45:02.180]   600 bucks.
[02:45:02.180 --> 02:45:03.780]   And I've certainly...
[02:45:03.780 --> 02:45:04.420]   There are certainly...
[02:45:04.420 --> 02:45:07.460]   Definitely, I have purchased gadgets and left them in the packaging.
[02:45:07.460 --> 02:45:10.180]   If I'm gonna leave all on the packaging for now on.
[02:45:10.180 --> 02:45:13.060]   But I'm surprised.
[02:45:13.060 --> 02:45:13.380]   Yeah.
[02:45:13.380 --> 02:45:13.940]   We're saying.
[02:45:15.140 --> 02:45:16.100]   And there was finally...
[02:45:16.100 --> 02:45:17.300]   There was a great picture.
[02:45:17.300 --> 02:45:20.420]   There's a site for Steve Jobs' birthday.
[02:45:20.420 --> 02:45:21.380]   Would have been this week.
[02:45:21.380 --> 02:45:25.460]   And there's the Steve Jobs archives.
[02:45:25.460 --> 02:45:28.580]   Which for a long time didn't really have much in there.
[02:45:28.580 --> 02:45:29.300]   It's being...
[02:45:29.300 --> 02:45:30.420]   I think it's being run by...
[02:45:30.420 --> 02:45:34.020]   I think Lorraine put up money and some other fans.
[02:45:34.020 --> 02:45:34.900]   Maybe Tim Cook.
[02:45:34.900 --> 02:45:38.180]   And they also gave a lot of their...
[02:45:38.180 --> 02:45:40.420]   A lot of their...
[02:45:40.420 --> 02:45:44.660]   You know, stuff that they had to the archives.
[02:45:44.660 --> 02:45:46.900]   Steve Jobs Archive.com.
[02:45:46.900 --> 02:45:48.420]   And now they're starting to release stuff.
[02:45:48.420 --> 02:45:49.140]   Which is nice.
[02:45:49.140 --> 02:45:55.220]   The latest thing for his birthday was a picture of Steve.
[02:45:55.220 --> 02:45:56.580]   Let me see if I can find it.
[02:45:56.580 --> 02:45:58.740]   Walking...
[02:45:58.740 --> 02:46:01.540]   He was walking in New Orleans.
[02:46:01.540 --> 02:46:03.940]   And noticed in an Apple store.
[02:46:03.940 --> 02:46:04.260]   Or...
[02:46:04.260 --> 02:46:05.940]   No, it was a Best Buy or somewhere.
[02:46:05.940 --> 02:46:07.300]   They didn't have Apple stores, right?
[02:46:07.860 --> 02:46:12.820]   Notice somebody playing with a Macintosh.
[02:46:12.820 --> 02:46:15.700]   I don't know where this is on the site.
[02:46:15.700 --> 02:46:17.620]   I can't find it.
[02:46:17.620 --> 02:46:20.980]   Maybe they just released it to the press.
[02:46:20.980 --> 02:46:24.820]   Let me see if I can find this picture.
[02:46:24.820 --> 02:46:26.660]   But he was...
[02:46:26.660 --> 02:46:29.300]   You know, at the time,
[02:46:29.300 --> 02:46:32.500]   the Mac had just come out there.
[02:46:32.500 --> 02:46:33.140]   It is.
[02:46:33.140 --> 02:46:37.460]   He was probably not sure whether it was going to be a huge success.
[02:46:37.460 --> 02:46:38.980]   The Lisa had kind of been a flop.
[02:46:38.980 --> 02:46:41.940]   His frankly job was on the line.
[02:46:41.940 --> 02:46:45.140]   Believe it or not, in fact, he did lose his job shortly after this.
[02:46:45.140 --> 02:46:48.260]   But there he is staring through a window.
[02:46:48.260 --> 02:46:51.220]   A young woman playing with a Macintosh
[02:46:51.220 --> 02:46:53.940]   for the first time in 1984.
[02:46:53.940 --> 02:46:55.940]   Steve Jobs 28 years old at the time.
[02:46:55.940 --> 02:47:02.340]   Would have been his 68th birthday on February 24th.
[02:47:02.340 --> 02:47:05.140]   Happy birthday, Steve.
[02:47:07.380 --> 02:47:10.260]   And that is the end of that.
[02:47:10.260 --> 02:47:14.740]   Mike, thank you so much for spending a couple of three hours with us.
[02:47:14.740 --> 02:47:16.020]   We really appreciate it.
[02:47:16.020 --> 02:47:20.020]   It is the best place to go to read about.
[02:47:20.020 --> 02:47:22.740]   I don't know how you don't have a heart attack, to be honest.
[02:47:22.740 --> 02:47:25.540]   Because everything...
[02:47:25.540 --> 02:47:28.340]   It makes me mad just to read Tech Dirt.
[02:47:28.340 --> 02:47:29.300]   You have to write this.
[02:47:29.300 --> 02:47:32.980]   I mean, how do you do it?
[02:47:32.980 --> 02:47:35.060]   How do you stay calm?
[02:47:36.980 --> 02:47:39.220]   I do have a blood pressure monitor off to the side.
[02:47:39.220 --> 02:47:39.940]   Yeah.
[02:47:39.940 --> 02:47:45.620]   I mean, there's so much just stupidity out there.
[02:47:45.620 --> 02:47:51.380]   And this is the place to read articles about stupidity.
[02:47:51.380 --> 02:47:57.620]   Yeah, but we have a sort of weird optimism beneath all of this.
[02:47:57.620 --> 02:47:58.420]   I know, I know.
[02:47:58.420 --> 02:48:02.180]   The reason we're calling it out is because we think people can be better
[02:48:02.180 --> 02:48:05.540]   and we think that innovation and the internet can be better.
[02:48:05.540 --> 02:48:11.140]   And therefore, we are angry at the things that hold that back,
[02:48:11.140 --> 02:48:13.860]   but we are optimistic about where we get to in the end.
[02:48:13.860 --> 02:48:16.420]   I like that.
[02:48:16.420 --> 02:48:20.580]   You also have a podcast, which everybody should subscribe to.
[02:48:20.580 --> 02:48:23.060]   You can get all of this at TechDirt.com.
[02:48:23.060 --> 02:48:26.100]   And I do really want to encourage people to support Tech Dirt.
[02:48:26.100 --> 02:48:28.980]   We talk about our own club all the time.
[02:48:31.060 --> 02:48:35.780]   If you only had seven bucks to spend it on Tech with Tech Dirt,
[02:48:35.780 --> 02:48:37.540]   Lisa's going to kill me.
[02:48:37.540 --> 02:48:40.500]   But honestly, you guys are doing God's work.
[02:48:40.500 --> 02:48:43.140]   Now, if you have 15 bucks, spend it on both of us.
[02:48:43.140 --> 02:48:49.620]   But supporting Tech Dirt really is worthwhile because you are literally
[02:48:49.620 --> 02:48:52.820]   doing the most important work on the internet right now.
[02:48:52.820 --> 02:48:54.100]   I think it's super important.
[02:48:54.100 --> 02:48:56.580]   And I'm so glad we always are thrilled when you can join us.
[02:48:56.580 --> 02:48:57.220]   Thank you, Mike.
[02:48:57.220 --> 02:48:57.780]   I appreciate it.
[02:48:57.780 --> 02:48:59.140]   Thanks for having me.
[02:48:59.140 --> 02:49:00.260]   It's always fun.
[02:49:00.260 --> 02:49:05.140]   Best thing I can do for you, Owen Thomas, is keep calling you the Herb Kane of Silicon Valley.
[02:49:05.140 --> 02:49:07.300]   Owen, as you remember.
[02:49:07.300 --> 02:49:09.460]   The second most important writer on the internet.
[02:49:09.460 --> 02:49:11.540]   Second most important writer on the internet.
[02:49:11.540 --> 02:49:14.900]   As you may remember, it was a protocol protocol fell apart.
[02:49:14.900 --> 02:49:16.260]   Now a columnist for the Examiner.
[02:49:16.260 --> 02:49:16.660]   And you know what?
[02:49:16.660 --> 02:49:17.700]   I love your stuff.
[02:49:17.700 --> 02:49:20.660]   I am going to start reading this.
[02:49:20.660 --> 02:49:22.340]   I'm going to add you to my RSS.
[02:49:22.340 --> 02:49:22.980]   How about that?
[02:49:22.980 --> 02:49:25.300]   Great.
[02:49:25.300 --> 02:49:30.100]   Great kind of columns about the real world in San Francisco and
[02:49:30.100 --> 02:49:30.820]   stuff.
[02:49:30.820 --> 02:49:34.820]   I haven't read the Examiner in a long time since the kind of the
[02:49:34.820 --> 02:49:38.820]   the newspaper wars knocked it out.
[02:49:38.820 --> 02:49:40.820]   But I'm glad to see it's doing it.
[02:49:40.820 --> 02:49:42.260]   It's doing it.
[02:49:42.260 --> 02:49:47.700]   I remember in the 90s when I first got to town, I was more of an Examiner kid.
[02:49:47.700 --> 02:49:48.500]   Me too.
[02:49:48.500 --> 02:49:49.460]   It was the afternoon.
[02:49:49.460 --> 02:49:50.180]   A lot of people.
[02:49:50.180 --> 02:49:50.820]   Yeah.
[02:49:50.820 --> 02:49:53.700]   It was also the city paper.
[02:49:53.700 --> 02:49:54.100]   Right.
[02:49:54.100 --> 02:49:57.860]   They still capitalize the sea and city, which I love.
[02:49:59.940 --> 02:50:02.100]   And there are only San Francisco.
[02:50:02.100 --> 02:50:03.700]   There are no other cities.
[02:50:03.700 --> 02:50:04.420]   And Owens got.
[02:50:04.420 --> 02:50:06.900]   When you talk about the city, this is it.
[02:50:06.900 --> 02:50:06.900]   Yeah.
[02:50:06.900 --> 02:50:09.300]   And Owens got the stories about San Francisco.
[02:50:09.300 --> 02:50:16.660]   And I think well worth reading at sfxaminar.com.
[02:50:16.660 --> 02:50:20.340]   I'm telling you, this is really we need a herb cane in San Francisco.
[02:50:20.340 --> 02:50:21.860]   You could be our new herb cane.
[02:50:21.860 --> 02:50:25.700]   He was the great columnist who dominated San Francisco newspapers
[02:50:25.700 --> 02:50:28.980]   for decades at the at the Chronicle than the Examiner.
[02:50:28.980 --> 02:50:30.020]   Then back at the Chronicle.
[02:50:30.020 --> 02:50:33.460]   I think this is going to be an opportunity for you.
[02:50:33.460 --> 02:50:34.500]   Oh, there's other Owens.
[02:50:34.500 --> 02:50:36.020]   Don't look at that other Owen.
[02:50:36.020 --> 02:50:37.460]   Look at Owen Thomas.
[02:50:37.460 --> 02:50:39.860]   I stood a search for Owen and Thomas.
[02:50:39.860 --> 02:50:40.500]   Thank you, Owen.
[02:50:40.500 --> 02:50:42.180]   Really appreciate it.
[02:50:42.180 --> 02:50:46.820]   His website will someday be resurrected, much like that Ficus behind him.
[02:50:46.820 --> 02:50:48.740]   ditherrody.com.
[02:50:48.740 --> 02:50:51.300]   That's the place to go at Owen Thomas on the Twitter.
[02:50:51.300 --> 02:50:54.820]   Mr. Alex Lindsey, you'll be back on Tuesday.
[02:50:54.820 --> 02:50:57.540]   We will talk about so much stuff.
[02:50:57.540 --> 02:50:59.380]   Lots of news in the Apple world.
[02:50:59.380 --> 02:51:03.300]   090.media if you want to hire him for your next streaming event.
[02:51:03.300 --> 02:51:05.780]   But everybody should go to office hours.global
[02:51:05.780 --> 02:51:10.740]   because it's a great place to hang for free and learn about all sorts of stuff.
[02:51:10.740 --> 02:51:15.940]   Mindfulness today, which is kind of cool.
[02:51:15.940 --> 02:51:18.260]   Content creation strategies and processes.
[02:51:18.260 --> 02:51:19.060]   I think yesterday, yeah.
[02:51:19.060 --> 02:51:20.260]   That was yesterday.
[02:51:20.260 --> 02:51:21.220]   For educators, yeah.
[02:51:21.220 --> 02:51:22.020]   That's okay.
[02:51:23.380 --> 02:51:29.140]   Sony FR7, MIDI programming, other world computing's Larry O'Connor.
[02:51:29.140 --> 02:51:29.860]   Oh, yeah, there it is.
[02:51:29.860 --> 02:51:30.580]   Saturday's show.
[02:51:30.580 --> 02:51:31.060]   Yeah, yeah.
[02:51:31.060 --> 02:51:33.380]   Lots of stuff on here.
[02:51:33.380 --> 02:51:34.500]   And it's open to all.
[02:51:34.500 --> 02:51:38.020]   You can look at their stuff on YouTube, but you can also join the Zoom conversation
[02:51:38.020 --> 02:51:39.540]   and ask questions and so forth.
[02:51:39.540 --> 02:51:40.660]   Just go to office hours.
[02:51:40.660 --> 02:51:42.180]   Doc Global for more information.
[02:51:42.180 --> 02:51:44.100]   Thank you, Alex.
[02:51:44.100 --> 02:51:45.140]   Thank you.
[02:51:45.140 --> 02:51:49.700]   We do Twitter every Sunday afternoon, right after I asked the tech guys.
[02:51:49.700 --> 02:51:51.940]   I think it's a really good Sunday lineup now.
[02:51:51.940 --> 02:51:54.820]   Mike and I start in the morning with ask the tech guys.
[02:51:54.820 --> 02:51:59.700]   Then about two o'clock Pacific 5 p.m. Eastern, 2,200 UTC.
[02:51:59.700 --> 02:52:03.940]   We gather together with the best minds and technology to talk about the week's tech news.
[02:52:03.940 --> 02:52:11.220]   On this week in tech, you can watch us do it live from two to five Pacific time.
[02:52:11.220 --> 02:52:15.460]   That would be five to eight p.m. Eastern time 2,200 UTC.
[02:52:15.460 --> 02:52:18.100]   The livestreams at live.tv.
[02:52:18.100 --> 02:52:20.420]   Actually, that's an aggregate of all the different live streams.
[02:52:20.420 --> 02:52:23.060]   So you can pick your favorite audio or video livestream.
[02:52:23.060 --> 02:52:27.540]   If you're watching live chat with us live at irc.twit.tv.
[02:52:27.540 --> 02:52:28.820]   That's open to all.
[02:52:28.820 --> 02:52:33.620]   You can also chat in our Discord if you remember of Club Twitter again, seven bucks a month.
[02:52:33.620 --> 02:52:34.980]   It's a great deal.
[02:52:34.980 --> 02:52:37.460]   You get ad-free versions of all the shows.
[02:52:37.460 --> 02:52:40.740]   You get to participate in the Discord and all the special events
[02:52:40.740 --> 02:52:41.700]   that Ant puts together.
[02:52:41.700 --> 02:52:42.740]   Oh, you got some new events.
[02:52:42.740 --> 02:52:44.500]   Samable Samad coming up.
[02:52:44.500 --> 02:52:45.700]   Stacey's book club.
[02:52:45.700 --> 02:52:47.860]   Insight to it with Victor.
[02:52:47.860 --> 02:52:50.260]   Alex Wilhelm will be doing an ask me anything.
[02:52:50.580 --> 02:52:54.180]   And Sean Powers from Floss Weekly for a fireside chat.
[02:52:54.180 --> 02:52:58.980]   All coming up and Pruitt as our community manager puts together some fun stuff for members
[02:52:58.980 --> 02:53:00.500]   and members only.
[02:53:00.500 --> 02:53:04.100]   There's also members only shows like Hands on Macintosh with Micah.
[02:53:04.100 --> 02:53:05.540]   Paul Therat's Hands on Windows.
[02:53:05.540 --> 02:53:07.540]   The Untetal Linux Show with Jonathan Bennett.
[02:53:07.540 --> 02:53:10.020]   Stacey's book club that gives Fizz with Dick Teabartoto.
[02:53:10.020 --> 02:53:11.780]   There's lots of reasons to be in the club.
[02:53:11.780 --> 02:53:16.740]   But the number one reason to join the club, it really keeps us on the air,
[02:53:16.740 --> 02:53:22.900]   keeps the lights on, keeps the team employed, and increasingly as podcast advertising dwindles,
[02:53:22.900 --> 02:53:24.740]   it's going to be the club that keeps us going.
[02:53:24.740 --> 02:53:29.380]   So if you can afford it, if you've already donated a detector,
[02:53:29.380 --> 02:53:31.700]   you've already bought a subscription to the examiner,
[02:53:31.700 --> 02:53:34.100]   then the seven bucks left.
[02:53:34.100 --> 02:53:35.460]   Join club to it.
[02:53:35.460 --> 02:53:38.740]   Go to twit.tv/club to it.
[02:53:38.740 --> 02:53:41.620]   Thank you very much in advance for your support.
[02:53:41.620 --> 02:53:43.540]   That just about does it.
[02:53:43.540 --> 02:53:48.020]   You can get a show also after the fact that the website twit.tv.
[02:53:48.020 --> 02:53:50.100]   You can also subscribe on your favorite podcast player.
[02:53:50.100 --> 02:53:50.900]   There's a YouTube channel.
[02:53:50.900 --> 02:53:52.180]   Lots of ways to watch Twit.
[02:53:52.180 --> 02:53:55.220]   But please do watch it every week because we love having you here.
[02:53:55.220 --> 02:53:56.820]   I thank you for joining me.
[02:53:56.820 --> 02:54:01.380]   And as I have said, for the last 15 years, I'll say it one more time.
[02:54:01.380 --> 02:54:03.860]   Another twit is in the can.
[02:54:03.860 --> 02:54:05.620]   This is amazing.
[02:54:05.620 --> 02:54:15.620]   [MUSIC]

