;FFMETADATA1
title=Baby's First Chinese Internet
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=714
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100
Failed to align segment (""): no characters in this segment found in model dictionary, resorting to original...
Start time: 0.30
End time: 26.14
Speaker: SPEAKER_01
Transcript:  It's time for Twit this week in tech.  Thanks to Jason Stel for filling in for me last week.  I'm back.  And man, we have a great show for you.  Amy Webb is here.  Christina Warren.  We're going to talk about the arrest of Julian Assange.  Can you defend him?  Maybe you can.  We'll also talk about Amazon.  They're moving into your house.  And Baby's First Chinese Internet.  How Amy keeps her child safe online.  It's all coming up next on Twit.

Start time: 28.75
End time: 29.98
Speaker: SPEAKER_00
Transcript:  Netcasts you love.

Start time: 30.83
End time: 31.90
Speaker: SPEAKER_02
Transcript:  From people you trust.

Start time: 35.92
End time: 37.34
Speaker: SPEAKER_00
Transcript:  This is Twit.

Start time: 45.28
End time: 154.60
Speaker: SPEAKER_01
Transcript:  This is Twit.  This Week in Tech, Episode 714, recorded Sunday, April 14, 2019.  Baby's First Chinese Internet.  This Week in Tech is brought to you by ZipRecruiter.  Hiring is challenging, but there's one place you can go where hiring is simple and smart.  That place is ZipRecruiter, where growing businesses connect to qualified candidates.  Try it free at ziprecruiter.com slash twit.  And by Molecule.  Molecule is reimagining the future of clean air, starting with the air purifier.  For $75 off your first order, visit molecule.com and enter the promo code TWIT1.  And by ExpressVPN.  Protect your online privacy with one click.  It's that easy.  For three extra months free with a one-year package, go to expressvpn.com slash twit.  And by WordPress.  Turn your dreams into reality and launch your website at WordPress.com.  Get 15% off any new plan at WordPress.com slash twit.  It's time for Twit This Week in Tech, the show where we cover the week's tech news.  Aloha from Hawaii East.  I'm Leo Laporte.  Thanks to Jason Snell for filling in for me last week.  Back from vacation, as you might have noticed, and we thought we'd have some fun this week  with two of my favorite people.  Amy Webb is here, our futurist from amywebb.io.  She is the founder of the Future Today Institute, professor of strategic foresight at NYU Stern  School and the author of two wonderful books.  The Signals Are Talking, where she explains how you can do her job.  And then the Big Nine, where she shows how you can't do her job.  Only Amy Webb can do her job.  This is the new book and really, really good.  Hi, Amy.  Welcome.

Start time: 155.25
End time: 156.24
Speaker: SPEAKER_06
Transcript:  Hey, Leo.  Welcome back.

Start time: 156.48
End time: 174.20
Speaker: SPEAKER_01
Transcript:  It's good to have you here.  Also joining us, she's been all over the world.  Christina Warren, senior cloud developer advocate at Microsoft.  She's been on the Ignite tour to Milan and Dubai and all over the place.  Hello, Christina.

Start time: 174.96
End time: 176.12
Speaker: SPEAKER_05
Transcript:  Hey, Leo.  Welcome back.

Start time: 176.30
End time: 177.73
Speaker: SPEAKER_01
Transcript:  Thank you.  Welcome back to you too.

Start time: 178.92
End time: 179.39
Speaker: SPEAKER_05
Transcript:  Thank you.

Start time: 180.06
End time: 180.78
Speaker: SPEAKER_01
Transcript:  You're not done though.

Start time: 182.22
End time: 197.36
Speaker: SPEAKER_05
Transcript:  No, I will be in Stockholm next week and then I will have a few weeks off.  But build is in between that time.  And then I will be in Mumbai at the end of May and then knock on wood.  I will not be traveling internationally for a little bit.

Start time: 197.52
End time: 201.80
Speaker: SPEAKER_01
Transcript:  So what do you do on these Ignite tours?  They're for developers, right?

Start time: 202.38
End time: 267.82
Speaker: SPEAKER_05
Transcript:  Yeah, they're for developers and for IT professionals and operations people.  And so I actually give two talks at each stop is what I've been doing.  So I help out with the opener, which is kind of like what we're doing in lieu of a keynote where we kind of show off some of the different things that you can do with them.  You know, Visual Studio code with live share and and some of our different CI CD interfaces.  And then I give a talk on introduction to Azure and cloud computing.  So it's kind of like a high level 101, you know, course for people who maybe aren't familiar with cloud computing and especially kind of the Azure ecosystem.  It goes into things like, you know, the differences between containers and VMs and talks a little bit about what serverless computing is and things like that.  And then I do a talk on introduction to Azure networking, which is talking basically about how various networking, virtual networking, a load balancer and other types of services work on Azure and how people can, you know, use that to connect their on-prem stuff to what might be in the cloud or and all that.

Start time: 269.00
End time: 381.44
Speaker: SPEAKER_01
Transcript:  Well, it's great to have you both on.  I followed the tech news the whole time I was in Kauai.  I did.  I know because I knew that I was coming back today and I thought I don't want to not know what happened.  I wasn't I admit a little shocked to turn on the TV a couple of days ago and see Julian Assange carted away from the Ecuadorian Embassy in London looking.  I'm afraid a little like little Haggard.  Well, I think he was going for the Jesus thing.  I don't know what it was.  He almost was like blessing people as as they took him out.  There he is.  So but there are two takes on this.  The US government says, oh no, we're not we're not going after him for what he published.  We're going after him because he hacked and encouraged hacking.  He helped Chelsea Manning break in to a classified system.  They have emails saying my team is working on it.  That's the smoking gun apparently.  On the other hand, it's pretty obvious that the minute they arrested Chelsea Manning, they were gunning for Julian Assange WikiLeaks published a stunning video, which I vividly remember called collateral murder.  Showing US drones used on civilians actually was military helicopters on civilians and journalists.  And this is where WikiLeaks really became famous.  More recently, of course, in the election in 2016, Julian went from a hero of the left to enemy of the left when he we don't know he published emails from the Democratic National Committee.  We don't know if he was acting on Russia's behalf or not.  But in this in any case, that's not what he's been arrested for.  He's been arrested for hacking.

Start time: 382.72
End time: 428.86
Speaker: SPEAKER_06
Transcript:  Well, the Obama administration, I think he was public enemy number one for the Obama administration.  Yeah, it was.  But there was a I think a very salient reason that the Obama administration did not pursue charges against him.  And that was because we don't have a clear definition of what constitutes speech in an era of big tech.  And so if they were to pursue charges and if somebody were to claim that he was a publisher doing an act of journalism, then the First Amendment protects him.  Then well, I mean, it may or may not have protected him, but it opened up an arena for discussion that I think we not me, but like the people who were involved weren't yet ready to have.

Start time: 428.92
End time: 490.06
Speaker: SPEAKER_01
Transcript:  Look, Nixon didn't like didn't like Daniel Ellsberg because he leaked the Pentagon Papers.  But without the Pentagon Papers, we wouldn't have known the true state of the Vietnam War.  That was protected.  The New York Times and The Washington Post published it.  I hope you saw the movie about it.  The Post.  It was very dramatic.  They tried to break into a to Hassan Ellsberg psychiatrist's office.  They wanted him, but they never they never got him because I think this is different though.  Well, that's my question.  Is this different?  It's a good article by James Ball in the Atlantic.  You don't have to like Julian Assange to defend him.  The ask the effort to extradite and prosecute the Wikileaks founder threatens the free medium.  Which is in contrast to the motherboard article we showed earlier, which says it's it's it's not about free press.  It's about hacking.  But it's a look.  It's probably it's trouble troubling.  It's problematic.  Assange is a problematic person.  But that's the whole point of free speech.  You have to defend.

Start time: 491.14
End time: 524.22
Speaker: SPEAKER_06
Transcript:  But it raises a really good point, which is we don't have there are a lot of fundamental questions that unfortunately we don't we are not yet discussing in this country because our country in order to have those conversations requires a lawsuit first right to have the conversations where we get to action.  So we have a lot of questions around what is speech what constitutes speech if it's been automated are we you know,  Are the algorithms to blame is the data to blame?  You know from that to who owns your face who owns your biometrics.

Start time: 525.24
End time: 528.10
Speaker: SPEAKER_01
Transcript:  And you think the proper place is for the courts to decide.

Start time: 528.42
End time: 544.56
Speaker: SPEAKER_06
Transcript:  No, I don't think so.  But that unfortunately is the way that democracy, you know, that's how it works.  Democracy.  It does in this country.  Now in other countries, they've just jumped ahead to regulation.  But this is a this is a problem.  That's that's not going to go away.

Start time: 545.34
End time: 609.99
Speaker: SPEAKER_01
Transcript:  I should point out that the reason Assange was in the Ecuadorian Embassy was not because of US charges, but Swedish charges against him for rape.  Those charges were dropped.  But now he's arrested for on basically the London Metropolitan Police arrested him on the behalf of the United States.  They're going to extradite him because he was indicted in the US for this for this hacking at the end of his piece ball says.  Assange might be an a-hole scratch that Assange is an a-hole, but we're going to have to stand up for him.  Anyway, I know I'm I have very mixed feelings about this.  I think he did a lot of good with collateral justice or collateral murder rather.  I think he was probably a Russian cutout in the 2016 election.  So there's that.  On the other hand, I think there are I don't know.  I know the minute he published collateral murder.  He would be there would be trumped up charges against him of some kind.  I thought so.

Start time: 610.22
End time: 660.38
Speaker: SPEAKER_06
Transcript:  I've had I've had two jobs two careers.  I've been a futurist for 15 years and the years before that I was a foreign correspondent.  So I worked at the Wall Street Journal and at Newsweek.  And here's what I would have to say, you know, there are plenty of cases like the Panama Papers where there's been widespread collaboration and journalists working for the public interests and and much like the Pentagon Papers releasing information, you know, in a way to shine the light on on corruption.  I think we need some kind of, you know, Assange makes it sound WikiLeaks make it sound like this is all open and through that openness and freedom comes justice.  But it's not entirely open.  There's still an arbiter.  And you know what I mean?  Yeah, it's it's and it is and this is an untrained arbiter who's who's co-opted by other governments in his own ways.

Start time: 665.63
End time: 699.38
Speaker: SPEAKER_01
Transcript:  I mean, Chelsea Manning, her sentence was what did Obama do?  He didn't he didn't he commuted it, right?  He didn't know he didn't pardon her.  He commuted her sentence.  Right.  He said you've served enough time.  We're going to let you out.  Clearly, people were very angry at WikiLeaks for releasing that.  But I think I think it's very analogous to the Panama Papers or the Panama Pentagon Papers.  These were government leaks, but they were of information that was in the public interest.

Start time: 700.07
End time: 728.56
Speaker: SPEAKER_06
Transcript:  And there's also timing.  So while there were certainly revelations about lives that were put in danger, lives that were lost once that information was dumped.  However, the there were instances of new people whose lives were put in, say, you know, in danger.  Again, this is these are the kinds of things that a journalist or a team of journalists working together would.

Start time: 730.97
End time: 757.70
Speaker: SPEAKER_01
Transcript:  No, Snowden did it responsibly, right?  Snowden took the material and gave it to a number of reputable publications and said you need to vet this and you need to put it out in an appropriate way because I'm not capable of doing that.  Assange did the opposite.  In fact, I think what you're thinking about that was really troublesome troubling was the American diplomatic cables that were leaked that had the names of American agents and put them in grave peril.

Start time: 758.74
End time: 762.62
Speaker: SPEAKER_06
Transcript:  That's right. And just because the documents are leaked doesn't mean that they are correct.

Start time: 762.92
End time: 769.32
Speaker: SPEAKER_01
Transcript:  They just dumped them out. They didn't they didn't vet them.  They didn't give them to a journalistic entity.  They claimed we're a journalistic entity, but they just dumped them.

Start time: 770.12
End time: 820.00
Speaker: SPEAKER_05
Transcript:  Right. Which to me, I guess, is why I've always viewed Snowden and Assange differently.  And because of exactly what you said, I've always viewed them in different ways.  And and in my my my thoughts on Chelsea Manning, although I think that the way that she was treated while she was incarcerated and a lot of those things have been terrible,  you know, because she was, you know, a soldier when she leaked that information.  You know, there is some certain things that for me made that a little bit more, I guess, complicated, too.  But I think that that gets to the key point in this right is that, you know, what he what he's now being kind of accused of are things that are, you know, I guess, fundamentally comes to the question of whether or not what he does is journalism.  And although I see is he making that argument?

Start time: 821.00
End time: 825.20
Speaker: SPEAKER_06
Transcript:  Is he saying I'm a journalist?  I'm doing journalism.  I don't think he has said that.

Start time: 825.20
End time: 890.96
Speaker: SPEAKER_05
Transcript:  He's not saying that right now.  He has said that in the past.  WikiLeaks has absolutely said that in the past.  But WikiLeaks has changed so much in the last decade that it's it's hard to know where they stand.  Right.  And and I think it's interesting that a lot of the First Amendment Amendment, you know, advocates and experts have very clearly tried to narrow this and say this is not about the media.  This is not going under journalistic things.  And again, this is about hacking, which whether you're a journalist or not is is is you know, you're not immune to those sorts of things.  If you're breaking into other people's databases and whatnot.  So but but it's still this question does come up.  Right.  Like is what was the role in this and how is this different?  I'm with you, Leo.  It's complicated.  It's troubling to me.  It is troubling in a lot of ways.  It's interesting to think about what would have happened if he would have just not been such a jerk.  Such a dick.  As the embassy.  But that's OK.  Like honestly, like because it got to the point that it was it was it was Ecuador finally had enough and they're like, we're not dealing with you anymore.

Start time: 892.78
End time: 929.01
Speaker: SPEAKER_06
Transcript:  But this is a this is a clear example of.  Short term gain without thinking about the downstream implications.  And I think that in all of my.  From what I've read, you know, it seems as though some of the people involved had their hearts in the right places.  But again, like you've got to think in a very rational, measured way.  What are all of the implications of making this information public at this particular time in this particular way?  Well, I think we can agree that he should have been done.

Start time: 930.24
End time: 935.36
Speaker: SPEAKER_01
Transcript:  At least should have been done in Snowden fashion.  Something else possibly right.

Start time: 936.70
End time: 955.35
Speaker: SPEAKER_06
Transcript:  Right. There are but he's not he's not a US citizen.  Right.  So this is the other thing.  Like I know a lot about the First Amendment because I had to study it.  You know, people a lot of Americans don't quite understand the First Amendment and people outside the United States aren't bound by it.  Right.

Start time: 955.72
End time: 960.00
Speaker: SPEAKER_01
Transcript:  So again, I think this is where he's being prosecuted by the US, not by people outside the United States.

Start time: 960.16
End time: 966.52
Speaker: SPEAKER_06
Transcript:  Yeah, but it's not First Amendment is not a piece of this.  This is under a well, just standard cyber crimes law.

Start time: 966.70
End time: 1031.52
Speaker: SPEAKER_01
Transcript:  To reiterate your point, this is from the Ball article.  The Obama administration had an institutional policy that concluded prosecuting Assange for the publication of classified documents was too controversial and legally risky  because a successful prosecution could serve as the foundation for future misuse of that authority to clamp down on more traditional media outlets.  The risk to the First Amendment was significant.  And then he points out perhaps the Trump administration has discovered additional evidence that changes that analysis.  And that's what we don't know yet.  But you have but I mean half the optics of it are if you're going to prosecute him for the email that says well our team is working on it.  Well, we're going to help you crack this Chelsea.  If you're going to prosecute him on that, it's clear that what you're really you're prosecuting him for the for the release of the information.  You just you just don't want to call it that.  It's like putting Al Capone in jail for tax evasion.  That's you know, it wasn't the real reason he was in Alcatraz, but it was a good it was an effective surrogate.

Start time: 1032.28
End time: 1044.24
Speaker: SPEAKER_06
Transcript:  Well, and also let's not forget that you never want to know lawyers or governments want to bring somebody to trial or bring charges if they're not very,  very certain that they're going to win.  Right.  So.

Start time: 1046.54
End time: 1056.28
Speaker: SPEAKER_01
Transcript:  And you know, Assange complicated it by by his personality disorder.  I guess I could call it a personality.  Something went on that caused him to.

Start time: 1057.12
End time: 1062.78
Speaker: SPEAKER_06
Transcript:  I think if you're confined for seven years, that could be the reason.  You know, yeah, I'm confined in my own house for a weekend and I go to visit.

Start time: 1062.88
End time: 1068.46
Speaker: SPEAKER_01
Transcript:  One of the reasons the Ecuadorian embassy kicked him out is he wasn't cleaning up after his cat.  That's one of the reasons they gave.

Start time: 1069.00
End time: 1072.62
Speaker: SPEAKER_04
Transcript:  Yeah, well, there's other cleaning issues playing music loud at night.

Start time: 1073.30
End time: 1076.64
Speaker: SPEAKER_01
Transcript:  He was stinky and he was cleaning up after his cat.

Start time: 1077.58
End time: 1095.76
Speaker: SPEAKER_05
Transcript:  There was a great there was a great article a few months ago about somebody who I guess like he was their houseguest and he wouldn't leave.  Who described what it was like.  And this is years before, you know, he was he went into the Ecuadorian embassy.  Apparently, he's like the houseguest from hell, no matter where he is and no matter what his circumstances.

Start time: 1096.26
End time: 1099.50
Speaker: SPEAKER_01
Transcript:  So, you know, but you shouldn't put somebody in jail for that.

Start time: 1100.56
End time: 1109.40
Speaker: SPEAKER_04
Transcript:  No, I agree with you.  But at the same time, I'm with you.  But I also sort of sympathize in some ways with the Ecuadorian.  Oh, I don't blame the Ecuadorians.

Start time: 1109.40
End time: 1110.11
Speaker: SPEAKER_01
Transcript:  In fact, I'm impressed.

Start time: 1110.72
End time: 1113.12
Speaker: SPEAKER_04
Transcript:  You're like we didn't sign up for this for nine years.

Start time: 1113.20
End time: 1114.61
Speaker: SPEAKER_01
Transcript:  They put up with that. It's pretty good.

Start time: 1115.40
End time: 1136.82
Speaker: SPEAKER_06
Transcript:  Like the sad thing is.  The sad thing is that this is a good opportunity for us to talk about how to move forward and what needs to change going forward and how to, you know,  And that's not happening.  We're, you know, like nobody's taking this as an opportunity to recalibrate how we deal with, you know, our laws and technology and crime and all that stuff.

Start time: 1137.00
End time: 1144.94
Speaker: SPEAKER_01
Transcript:  Your point being that things have changed thanks to technology and we need to update our thinking about what that means, what the First Amendment means in that light.

Start time: 1145.80
End time: 1176.98
Speaker: SPEAKER_06
Transcript:  Yeah. And I feel like this we cycle every couple of years.  And there's a Wired reporter named Andy Greenberg, who wrote this really great book called this machine steal secrets.  Like every couple of years, there's this conversation that that sort of pipes up and we all get fascinated about it.  And then we forget about it and we move on.  And then we're surprised when it all happens again or we're stuck in a similar situation where we don't know what to do.  And now, you know, I just I feel like we keep missing these opportunities to make some changes.

Start time: 1177.26
End time: 1191.30
Speaker: SPEAKER_01
Transcript:  You know, and let's not forget, speaking of forgetting one more person.  I don't want to forget who's reality winner who is in jail five years for leaking the hacking report that told us that Russia was hacking the elections.

Start time: 1192.33
End time: 1194.82
Speaker: SPEAKER_06
Transcript:  And that one's a stranger case to me.

Start time: 1195.14
End time: 1219.16
Speaker: SPEAKER_05
Transcript:  Yeah. Yeah.  That that that one I feel bad for her in some ways.  A, you know, she she tried to pass this off the right way.  The the intercept really messed up there.  You know, like honestly, you look at that and you're like, if you're going to be accepting, if you're going to be in my opinion, if you're saying send us secure things and we're going to protect you, then you need to take that at the most importance.  And and that they really dropped the ball there in that one.

Start time: 1219.32
End time: 1226.44
Speaker: SPEAKER_01
Transcript:  She's under the longest sentence ever imposed in federal court for an unauthorized release of government information to the media.

Start time: 1227.54
End time: 1230.16
Speaker: SPEAKER_00
Transcript:  So, you know, this is more than a slap in the wrist.

Start time: 1230.46
End time: 1263.56
Speaker: SPEAKER_06
Transcript:  So this is interesting, right?  So right after WikiLeaks, a handful of news organizations got together, including The Guardian, and created a secure leaking area of their website.  Right. So, right.  So and that had been in use and the standard.  I mean, she reality winner, as far as I understand, followed the appropriate steps to try to port internally and then leak in a way that she would have been covered under whistleblower.  I thought this is, by the way, Snowden did what he did.

Start time: 1263.66
End time: 1270.96
Speaker: SPEAKER_01
Transcript:  He he felt that he would go to jail if he tried that route.  Yeah, they proved Snowden right in effect.

Start time: 1271.96
End time: 1302.92
Speaker: SPEAKER_05
Transcript:  Yeah, no, but I mean, and I don't know where the breakdown was of the intercept with that process.  I know that when because Gizmodo Media Group had secure drop and I know that even get access like I did do like an all day training thing to even be able to use it.  And there were very specific things and our lawyers were involved.  And there was a whole you know, but there was a whole process.  So I don't know where the breakdown was with with when they published things or whatnot, like what was visible and what wasn't.  But it was something that clearly, you know, was not done correctly.

Start time: 1303.08
End time: 1321.70
Speaker: SPEAKER_01
Transcript:  They what she she snuck printouts out.  And I think that they published those unchanged and they were able to because of the secret dots on printouts.  They were able to trace it back to her because they they published photos of the printouts rather than the content itself by itself, something like that.

Start time: 1322.68
End time: 1332.38
Speaker: SPEAKER_06
Transcript:  So I'm super curious the training that you had to look at content that had been leaked.  You know, what was the training like?  What do they what what did you learn?

Start time: 1333.68
End time: 1343.52
Speaker: SPEAKER_05
Transcript:  Well, I mean, it was it was about how to use the system, how to access things.  And then there were things about yeah, I mean, in terms of like when when viewing, you know, data, how to handle it, you know,

Start time: 1344.54
End time: 1356.84
Speaker: SPEAKER_06
Transcript:  this was was there like a prescriptive like aside from the technical, like how to get the content?  Was there like a codified in this like a decision tree in this case?  Tell our legal department in this case, you can.

Start time: 1358.80
End time: 1381.56
Speaker: SPEAKER_05
Transcript:  Yeah, I mean, that was definitely part of it.  And and it wasn't like maybe codified in that way, but it was definitely one of those things.  And it was definitely I mean, for us, it was like when in doubt, you know, talk to the lawyers who were were fortunately downstairs, you know,  to get to get clearance and definitely, you know, talk to the lawyers before uploading any sort of documents you get from secure drop to document cloud or whatever.

Start time: 1381.90
End time: 1392.80
Speaker: SPEAKER_01
Transcript:  Although Gizmodo is probably more more concerned about Apple than the U.S. government, right?  I mean, it was not necessarily really.  You did you expect leaks from the NSA?

Start time: 1394.10
End time: 1413.36
Speaker: SPEAKER_05
Transcript:  I mean, at this point, some of the best reporters who you have there are Adele Cameron and right.  You know, I mean, so not necessarily.  I mean, you never know when you're going to be getting to it wasn't just it wasn't just Gizmodo.  You know, it was also the investigations team and it was, you know, lots of other parts of other sites.  But no, I mean, you would be surprised.

Start time: 1413.62
End time: 1431.14
Speaker: SPEAKER_01
Transcript:  I'm showing you know what I'm showing my my age and my bias because I still think of gadget blogs versus real journalism.  And that's completely right.  I mean, I see a headline in BuzzFeed.  I think is this six ways to cook a steak or is this going to be, you know, an expose?  And and often I'm wrong.

Start time: 1431.30
End time: 1472.38
Speaker: SPEAKER_05
Transcript:  So, yeah, I mean, and the truth of the matter is, is that because we were, you know, a media entity that was known to publish things,  people would leak things regardless.  You know, people knew that they could come to us because that would get published, which is not always the case.  So there were things that we would get that others wouldn't or that we might be hungry for because we would publish those things.  And that's still the case with that organization now across all of its different properties is they'll publish things that not everyone else will.  So you would be surprised the sort of stuff that would come in and especially around, you know, any of the election stuff, especially around cyber crime stuff.  Yeah, you know, there's a lot of that stuff.

Start time: 1472.40
End time: 1493.58
Speaker: SPEAKER_01
Transcript:  Yeah. And if there's a big overlap these days between the cyber crime and political crime.  So, absolutely.  All right. Well, let's move on.  Lots more to talk about.  Google's in a little bit of trouble.  I'm a little worried about Apple and Amazon's moving to Bellevue.  Are you going to you're going to I don't know if they just built the spheres.

Start time: 1494.14
End time: 1502.75
Speaker: SPEAKER_06
Transcript:  I know they just built them.  What are they are gorgeous.  I mean, you like this.  I love it. 73 degrees.  All your account.

Start time: 1503.10
End time: 1505.50
Speaker: SPEAKER_01
Transcript:  Yeah, like Hawaii in Seattle.

Start time: 1505.52
End time: 1513.08
Speaker: SPEAKER_06
Transcript:  It's beautiful.  There's like 40,000 species of plant life.  It's perfect humidity.  It smells good.  It's like I want to move in there.

Start time: 1513.50
End time: 1527.66
Speaker: SPEAKER_05
Transcript:  You could work in there.  I know I live.  I live just about, you know, a quarter of a mile away from them, which is why I don't mind the moving to Bellevue because maybe Seattle Westside home prices will go down.

Start time: 1527.80
End time: 1784.70
Speaker: SPEAKER_01
Transcript:  Maybe maybe I want to write a dystopian science fiction novel in which Seattle has moved out of the spheres and they've been taking over by gangs.  And I don't know.  There's something there.  There's something there.  The future.  Let's take a break.  Our show today brought to you by Zip Recruiter.  Hello, Zip Recruiter.  Good to see you.  We love Zip.  We're big fans of Zip Recruiter.  Hiring is such an important part of building a company.  The company.  What a company.  What is a company?  A company is made of people with an aligned goal working together.  Getting the right people can take your company to the moon and stars.  Getting the wrong people can take you down.  And that's why hiring is the most important thing you do in your company.  It's also sometimes the most challenging.  And what's really hard if you're a small company like ours or you just have a few employees is you're doing all this when you're down a person.  You're already in the penalty box and now you got to win the game.  That's why you need Zip Recruiter on your team.  Zip Recruiter is the smart way to hire.  First of all, we've already mentioned this.  When you go to ZipRecruiter.com slash twit and post your job, you're sending it to over 100 of the world's top job boards.  So you're spreading your net wide.  Your message goes out.  You're more likely to reach that perfect person.  You just don't know.  You know, they're out there.  You just don't know where they are.  So you're more likely to get to that person.  But there's more than that.  And this is what's so cool.  They don't use the word AI.  But I almost want to use this word AI because they use matching technology.  See, they already have millions of resumes because lots of people come to Zip Recruiter.  It's a job site.  So they scan those resumes to find people with the right experience for your job.  And then they say, you might want to apply to this job.  And I've seen this work because when our bookkeeper six months ago gave us two weeks notice, Lisa went,  oh no, I have to do the books.  I don't want to do it.  I don't get a high.  So weird.  This is ZipRecruiter.  I said ZipRecruiter.  She posted it before lunch.  We had three.  They came in, like kept coming in.  Oh, this is good.  Oh, this person's great.  We had three before lunch great candidates.  That's that artificial intelligence.  That's that matching technology.  As the applications come in, ZipRecruiter analyzes each one, spotlights the top candidates.  So you never miss a great match.  And by the way, they don't flood your inbox.  They don't call you.  It all goes into the ZipRecruiter interface.  All the resumes are formatted.  So they look the same as easy.  You can even have screening questions, essay questions, true false, multiple choice.  So you can eliminate people who just don't fit.  ZipRecruiter is so effective that four out of five employers who post on ZipRecruiter  get a quality candidate through the site within the first day.  We got one within the first hour.  It blew my mind.  Right now, this week in Tech Listeners can try ZipRecruiter free at our exclusive web  address, ziprecruiter.com slash twit.  ZipRecruiter.com slash twit.  ZipRecruiter is the smartest way to hire.  ZipRecruiter.com slash twit.  We thank them so much for a service that we have used and loved.  So I got a little nervous when I read this in the Apple developer documentation.  Microsoft's moving this way too.  I'm actually curious, Christina, what you think of this.  I think every operating system looks at mobile operating systems, especially iOS, and says,  iOS is more secure because Apple requires developers to have a signature and they vet  the apps.  Microsoft released Windows S. Same idea.  You got to get it through the store.  I know that Apple has Gatekeeper on Mac OS.  And now the latest thing, which scared some people, and maybe it depends how you read  Apple says before 10.14.5, the next version of Mac OS, if you want to distribute software  on Mac OS, your software has to be notarized in order to run.  In a future version of Mac OS, notarization will be required by default for all software.

Start time: 1786.00
End time: 1801.48
Speaker: SPEAKER_06
Transcript:  That really confuses me.  I heard about this and I read through it because I've got a couple of scripts that I wrote  that are going to be broken.  But I don't understand.  So code signing makes sense for security.

Start time: 1801.90
End time: 1810.63
Speaker: SPEAKER_01
Transcript:  But right now, if you download an app on Mac OS and it's not signed, it's more like Android  where you could check a box.

Start time: 1811.96
End time: 1814.47
Speaker: SPEAKER_05
Transcript:  You have to right click and say, I agree to install this anyway.

Start time: 1815.44
End time: 1825.32
Speaker: SPEAKER_06
Transcript:  So it strikes me that this is a way to lock out anybody who has not gotten the additional  layer of...  Which is not free, by the way.

Start time: 1825.40
End time: 1829.06
Speaker: SPEAKER_01
Transcript:  You have to have a developer ID.  You have to...  Like, what is this?

Start time: 1831.68
End time: 1836.04
Speaker: SPEAKER_06
Transcript:  I mean, why make the change, I guess, is the thing that I don't quite understand.

Start time: 1836.56
End time: 1864.27
Speaker: SPEAKER_01
Transcript:  There's also issues if you have your own scripts.  If you're doing development at home and you write a program for yourself, can you run it?  So I've seen a lot of commentary on this and the question is, what does Apple mean?  Apple has yet to explain that when they say notarization will be required by default for  all software, does that mean that the end user...  It must mean that the end user...  That's the default, but the end user can override it.  Must mean.  Otherwise, you couldn't write your own programs.

Start time: 1865.52
End time: 1913.28
Speaker: SPEAKER_05
Transcript:  Yeah, I believe that's what...  I mean, that's how I'm reading it, is that it's going to be by default the same way that  the developer keeper is by default.  It's going to say that apps have to be notarized.  And I assume that the reason they're doing this is that what has happened a number of  times is that there will be software that is signed by a developer certificate and then  is...  Something happens to it.  It's bypassed in some way and so they want to have that one extra level of protection  to ensure that that's happening.  If you're writing your own scripts and whatnot and if you do have a developer ID, there is  a way that you can kind of build this into your workflow and have the notarization process  part of it.  And I pay for a developer account in part just to do stuff like that and also because  I like to get the early access builds to things.

Start time: 1913.42
End time: 1916.18
Speaker: SPEAKER_01
Transcript:  It's 99 bucks.  It's not prohibitively expensive.

Start time: 1916.44
End time: 1949.79
Speaker: SPEAKER_05
Transcript:  But it's also not one of those things that every single person who creates a few custom  Apple script things should have to do if they don't want to get alerted all the time.  There's kind of a ridiculous process there.  It would be nice if they had a way to get notarized without having to be part of the  developer program.  If it was something that would be free, that was separate.  It's like, okay, if you want to distribute in an app store, you have to pay.  But if you're not distributed in the app store, we'll still give you a developer account.  We can notarize your stuff.

Start time: 1950.64
End time: 1956.38
Speaker: SPEAKER_01
Transcript:  You can self notarize, but you have to have a developer ID and a certificate.

Start time: 1956.46
End time: 1960.73
Speaker: SPEAKER_06
Transcript:  I don't think that's just like a name, like a new name for an existing work stream.

Start time: 1962.40
End time: 1988.38
Speaker: SPEAKER_01
Transcript:  Essentially my understanding, the point of this would be that we know who made every  app running on your system.  Do they not currently know that?  No, of course not.  I can give you an app.  I can send you an app.  Oh, I see what you mean.  So now you have to be verified by Apple.  This is you.  This is your real address.  And as you point out, Christina, this has not worked perfectly.  There were Turkish certificates that were used by malware.  They were legitimate.

Start time: 1988.46
End time: 2001.40
Speaker: SPEAKER_05
Transcript:  Yeah, there have been a lot of those.  And there's also been the sort of thing that's happened is that people oftentimes for pirated  stuff, because we were seeing this on iOS as well, where people would create, they would  use the enterprise certificates and use that as a way to distribute.

Start time: 2001.44
End time: 2003.89
Speaker: SPEAKER_01
Transcript:  Including Facebook and Google.  Exactly.

Start time: 2004.54
End time: 2064.57
Speaker: SPEAKER_05
Transcript:  Precisely, right?  What they were doing was actually something that a lot of companies have used for years  to kind of get away with some of the app store restrictions, because for the enterprise things,  you can bypass a lot of that stuff.  So like, oh, we want to access pirated content.  We can do that using this app that has been done by a developer certificate.  And I guess because they have so many of those, those aren't vetted in the same way.  And like you said, the Turkish malware instance, that's been the sort of situation.  I think that they're trying to stop that as a vector of spreading malware and spreading  other things, because people do use their developer certificates to distribute stuff.  And there might not be that same level of scrutiny that you think that there might be.  But you're not wrong in the fact that it's going to be an edge case, but it's going to be the edge  cases where this is one of those instances, at least in my opinion, where the people who are  most exposed by this are some of the power users and the people who are really dedicated to being  on Mac OS.

Start time: 2065.50
End time: 2110.10
Speaker: SPEAKER_01
Transcript:  Well, and people who use Microsoft's Visual Studio on Mac, because it says you have to use Xcode 10  or later.  The only reason any nor- if Microsoft said this, everybody going, yeah, no, that's, you know,  there's always going to be a way.  But people, maybe it's just me, but I'm paranoid about Apple.  The day they started the App Store and Gatekeeper, I thought it's only a matter of time before they  make sure that everything goes to the App Store.  This is their new model.  We collect 30% of all revenue for the privilege of letting you run something on Mac OS.  There is an advantage that, you know, a case you can make to consumers, well, it makes you safer.  I guess if I- it would be really problematic to do it on a desktop operating system though.

Start time: 2110.50
End time: 2123.40
Speaker: SPEAKER_06
Transcript:  Yeah.  So, like, I have a couple of, like, there's a couple of different ways of distributing prototype  software, and I'm just- now I'm trying to figure out how does that even work if you're trying to track-

Start time: 2123.42
End time: 2126.98
Speaker: SPEAKER_01
Transcript:  Well, on iOS, they have a way of doing it.  They have a test flight application.

Start time: 2127.54
End time: 2128.79
Speaker: SPEAKER_04
Transcript:  Yeah, exactly. There's test flight.

Start time: 2129.40
End time: 2131.32
Speaker: SPEAKER_01
Transcript:  And that's when you use that enterprise certificate.

Start time: 2132.13
End time: 2141.22
Speaker: SPEAKER_06
Transcript:  Right, exactly.  What part then is notarized?  Do you have to, like, go through this entire process in the initial build?  Yes. I can't just send you something to run on iOS.

Start time: 2142.08
End time: 2175.40
Speaker: SPEAKER_05
Transcript:  Right. But I think that what it'll happen is that it's to be part of that build workflow.  So once you build it in Xcode, it's going to be the same way it would be signed now,  the way that you would have to still sign some sort of, you know, prototype thing to go through, you know,  hockey or test flight or whatever.  The notarization process is going to be part of that workflow.  And I don't know definitively.  I'll have to check for Visual Studio for Mac.  But I assume that because there is, like, a CLI and there are different ways that you can incorporate  Xcode stuff into other IDEs like Visual Studio for Mac, that they will just, you know, use that.

Start time: 2175.40
End time: 2179.08
Speaker: SPEAKER_01
Transcript:  That would make sense, right? You just use that part of Xcode to sign.

Start time: 2179.44
End time: 2197.16
Speaker: SPEAKER_05
Transcript:  Exactly. And so I assume it's going to be a pain when you get set up.  But I think that the idea will be once you have it done and once it's in your kind of key chain of things  that you're doing with your tooling, theoretically, it shouldn't be that.  I hope so. There shouldn't be any difference.

Start time: 2197.44
End time: 2201.06
Speaker: SPEAKER_01
Transcript:  I'm sure I'm overstating this. I just you could see why people went, whoa.

Start time: 2201.82
End time: 2240.36
Speaker: SPEAKER_05
Transcript:  Well, without a doubt, they haven't done a great job of explaining what this is.  And it's also, you know, when they say by default that that implies, again, like with Gatekeeper,  that there's still going to be a way to install other things.  And I'm with you. I think that if your main operating system, like even, you know, S mode is a mode, you know,  if you need to install things outside of the store or things that can't be done from the store,  you can turn that off. I think that, you know, I don't know how many people I know I wouldn't be willing to accept an operating system.  It's one of the reasons I don't use Chrome OS.  It only lets you run certain types of applications.  That's just, you know, where this comes from.  I won't put up with it on a laptop.

Start time: 2241.48
End time: 2279.57
Speaker: SPEAKER_01
Transcript:  I just get I'm nervous because I feel like this so-called post PC era, we're heading hurdling down this track  where everything will be like iOS or Chrome OS.  And certainly consumers in many respects should be using Chrome OS and iOS.  I tell people all the time that's a safer alternative if your needs are simple.  But as a driver of a truck, in effect, I want to be able to do my own thing and I want others to be able to.  I want open source software to exist.  There's just a lot of reasons why some operating systems need to be open.  I mean, I guess Linux will always be open, right?

Start time: 2280.40
End time: 2301.96
Speaker: SPEAKER_06
Transcript:  Well, there's also interoperability, right?  So I guess we kind of been talking about that, but this this creates another fissure and more balkanization,  which is not good because this isn't just about a singular OS.  It ties into cloud services, right?  So when cloud services have AI now as a service on the cloud and, you know, there's a longer tail here.

Start time: 2302.40
End time: 2308.92
Speaker: SPEAKER_01
Transcript:  Is it something that we need to do to protect users?  The security environment has become so hostile.

Start time: 2309.48
End time: 2327.38
Speaker: SPEAKER_06
Transcript:  Yeah, I mean, how many?  I mean, nobody takes nobody up to if they weren't forced to.  People wouldn't wouldn't go through on firmware updates.  They don't change their passwords.  They're using legacy code.  They're downloading what they think are games to their devices.  So it's your brother in law's fault.

Start time: 2327.81
End time: 2332.24
Speaker: SPEAKER_01
Transcript:  It's not our fault.  It's your brother in law's fault.  Tell him to knock it off.  I should do that.

Start time: 2332.52
End time: 2337.32
Speaker: SPEAKER_05
Transcript:  Yeah, I mean, knock it off.  My dad has been the victim of ransomware twice.

Start time: 2339.41
End time: 2349.40
Speaker: SPEAKER_01
Transcript:  Although so has Norse Kyro, the second biggest aluminum producer in the world.  Merck, one of the biggest pharmaceutical companies in the world.  It's not like it's restricted to family members.

Start time: 2349.67
End time: 2385.56
Speaker: SPEAKER_06
Transcript:  Think about this.  There's another way to think about this, which is if it's the case that people are very concerned about data and privacy  and it's an election year and Apple's already fairly far ahead with differential privacy and other kinds of measures, right?  Then it kind of to me looks like a way to generate strategic advantage and leverage over everybody else.  If their products are beloved and they've locked them down such that like nobody else can get inside and they're the literally keepers of all that data.  Yeah.  Under the guise of privacy.  And that's the funny thing.  Yeah.

Start time: 2386.40
End time: 2390.34
Speaker: SPEAKER_01
Transcript:  Regulations that are being protected.  We'll protect you.  You're safe in our walled garden.

Start time: 2390.95
End time: 2399.40
Speaker: SPEAKER_06
Transcript:  That's right.  That's exactly right.  That would be probably music to the ears of a lot of legislators who are proposing all kinds of crazy privacy measures right now.

Start time: 2399.94
End time: 2469.38
Speaker: SPEAKER_01
Transcript:  Well, speaking of privacy, New York Times with not one but two articles about Google technology that's being used by law enforcement.  See, Google knows where you are at all times.  And apparently Google is happy to share that information with law enforcement.  The thing that we I'd heard about this being proposed in the past that I think was an upper New York state.  A police department asked Google, well, we'd like to know everybody who is in the vicinity of these convenience store robberies at these times.  And Google fought it.  And I thought at the time that was kind of refused by the courts as a phishing expedition, except that now in the New York Times, this happens all the time where Google will be asked to provide information.  In this case, they started with a murder in a Phoenix suburb.  A search warrant required Google to provide information on all the devices, all the devices near the killing.  Potentially capturing the whereabouts of anyone in the area.

Start time: 2470.39
End time: 2494.40
Speaker: SPEAKER_06
Transcript:  You know, this isn't just Google.  Amazon has also the external cameras and Google's external house cameras.  Yeah.  But there are a lot of communities where police are asking the neighbors to let them connect.  There's a community in Dallas where the I'm sorry, Houston, where the police have asked neighbors to let them take over their cameras and look at footage whenever.

Start time: 2494.44
End time: 2557.40
Speaker: SPEAKER_01
Transcript:  And I mean, I've seen enough police procedurals on TV to know that the first thing you do is to go to the convenience store across the street and you ask to see the videos because they have a camera pointing at the crime scene.  That doesn't bother me.  As much as.  Because everybody who was in that vicinity is drawn up in this law enforcement net.  And in fact, the case that the New York Times talks about, a suspect was arrested because due to this data, because his iPhone, his phone said he was on the scene.  And it turned out after he spent a week in jail, it wasn't him.  It was his mother's ex-boyfriend who borrowed his car.  And so this guy, because of this, and this is to me the classic fishing expedition where police, I know police would love to say, well, we'd like to know everybody who was in the in the area.  I thought that was illegal, but maybe I'm right.  Isn't that unconstitutional?

Start time: 2557.76
End time: 2569.36
Speaker: SPEAKER_05
Transcript:  No, I'm with you. I'm with you. I mean, I don't know. I'm not a fan of this either.  The first thing I thought about when I saw this, I was like, OK, well, now I want to purge all of my Google history, you know, on my phone and whatnot.

Start time: 2569.74
End time: 2580.40
Speaker: SPEAKER_01
Transcript:  Or, I mean, that was by the way, on Hacker News, that was the immediate response of everybody on Hacker News was, well, here's how you stop that by turning off all your Google location information.

Start time: 2580.86
End time: 2584.32
Speaker: SPEAKER_05
Transcript:  But of course, what's frustrating about that is that it's it's helpful to have.

Start time: 2584.40
End time: 2585.38
Speaker: SPEAKER_01
Transcript:  And then your maps don't work.

Start time: 2586.42
End time: 2609.40
Speaker: SPEAKER_05
Transcript:  Exactly. That's what I'm saying.  And it's not like I even like I'm not anticipating being in a situation where this will matter or while I will be implicated in something.  But it's not about that. I don't like the idea, as you say, of like it's one thing if the camera is that data is captured from the convenience store or whatever.  I know I'm in a public place. I know my image might be there.  And if they can identify me and ask me questions, that's fine.  Maybe that's what's different.

Start time: 2609.44
End time: 2617.38
Speaker: SPEAKER_01
Transcript:  Because you're in a public area.  But but that GPS information and I know some courts have ruled this is a more personal thing.

Start time: 2618.32
End time: 2646.10
Speaker: SPEAKER_05
Transcript:  Right. And also we've seen before where, you know, it can be far beyond just that vicinity.  You know, they might be asking, oh, we just want this radius.  But if they go a little bit further, you know, I mean, it just I'm it makes me uneasy.  I'm not a fan. And I don't like that.  It seems like this has become commonplace.  And to your point that Google is because I remember the first time this came up, they're like, oh, no, we're going to fight this night as I naively assumed.  Oh, well, they're going to continue to fight this.  And now it's like, eh, now we'll just hand it over.  It's like, really, really?

Start time: 2646.40
End time: 2658.40
Speaker: SPEAKER_06
Transcript:  So again, they're like, there are fixes here so that the easy O.S. fix is just like build it into the O.S. on your devices and put an off like toggle on off button in a visible, easy to understand.

Start time: 2658.40
End time: 2664.38
Speaker: SPEAKER_01
Transcript:  Yeah. But remember, like Google getting in trouble because it turns out when you did turn it off, it didn't actually turn it off.

Start time: 2664.46
End time: 2696.06
Speaker: SPEAKER_06
Transcript:  Well, some of the other apps wouldn't work.  I don't know. Again, like we but this is not the first time we've had this conversation.  We had this conversation after the San Bernardino shootings.  What was it now? Three years ago.  You know, I so we're going to keep cycling back to this.  And what I would say is, you know, the New York Times is a very large publication, but most of America doesn't read it.  So like we're having this conversation.  You know, most people aren't reading that story and have absolutely no idea how their data are being collected.

Start time: 2696.68
End time: 2761.95
Speaker: SPEAKER_01
Transcript:  Google actually has a brand name for it.  They call it Sensor Vault.  And according to the New York Times, Google's Sensor Vault is a boon for law enforcement.  This is how it works.  There's a database.  It's connected to a Google service called Location History.  Everybody should go to Google.com slash dashboard and look at your location use history.  You'll see how granular it is.  By the way, according to the Times, location history is not on by default.  But as soon as you turn on your phone and you start to use a Google service like Maps, it'll say, can I turn on location history or photos?  Can I turn on location? You know, and most people say yes.  Google says we use this to target ads and to measure how effective they are.  So, by the way, that means they're watching you go into stores and then they watch the credit card transaction and they know the credit card was yours.  And they even offer this information to people who buy Google ads.  It's not just a click.  We can also tell you if the person ended up buying your product using their credit card because we're watching where they are.

Start time: 2764.70
End time: 2768.82
Speaker: SPEAKER_06
Transcript:  So and that's actually old technology that launched a couple of years ago where they.

Start time: 2769.48
End time: 2831.34
Speaker: SPEAKER_01
Transcript:  But the thing that's to me a little scary is and I know I don't blame law enforcement.  It's a tool. And as long as Google is willing to give this to you, why not?  Sure. These are called so-called geo fence requests.  So they say instead of and see, this is what I thought was unconstitutional.  Instead of saying, where was Leo on the night of Friday, January, July 17th, they say, well, who was in that area on the night of July 17th?  Let's see them all and we'll talk to each one of them or we'll investigate some other way.  Each one of them, Google labels the devices with anonymous ID numbers.  Detectives look at locations and movement patterns, according to the New York Times, to see if any appear relevant to the crime.  Once they narrow the field to a few devices, Google then presto pulls off the the the the the hider and reveals the information like names and email addresses.  So anyway, it's something people should be aware of, I guess.  You're right. Very few people will be except people who read the New York Times or listen to this show.

Start time: 2831.54
End time: 2872.38
Speaker: SPEAKER_06
Transcript:  But I I'm surprised this is so that again, that begs the question.  This is where we keep where we get stuck.  You know, we buy the technology and so we feel like we own it.  And there's a feeling like we own it, I think makes people believe that they have some agency in what's being done.  And in fact, we don't own any of the technology.  We own the ability to use it for some amount of time.  But we don't own it outright. Like when you buy a water bottle, you get to determine all the things that happen with that water bottle on your own.  This is this not. And I think we've done a piss poor job of making sure that everyday people understand this.

Start time: 2872.84
End time: 2946.48
Speaker: SPEAKER_01
Transcript:  And, you know, as you're to your point that, you know, this is something that probably governments should look into and regulations about it.  Illinois attempted this April 10th.  The Illinois State Senate passed something called the Keep Internet Devices Safe Act.  And in fact, it did pass.  But after fierce lobbying from an industry association backed by Amazon and Google,  they defanged it a little bit by taking away any punishment.  But the idea was that no private entity could turn on or enable a device's microphone unless you agreed.  And the bill requires any recordings or other personal information captured by devices protect against unauthorized access,  acquisition, destruction, use, modification and disclosure of the data.  Initially, the bill would have made this an unlawful practice under the Consumer Fraud and Deceptive Business Practices Act,  which could result in fines of up to $50,000 per case.  But lobbying by the Internet Association defanged it by saying, well, we don't have to have a punishment.

Start time: 2948.28
End time: 2976.38
Speaker: SPEAKER_06
Transcript:  Ultimately, Illinois has actually been pretty far ahead in fighting big tech.  Really? This is interesting.  But here's the thing. The laws and the regulation only matters if they're if they are enforced.  And also if they deter people from using things that the regulators think are bad for them.  And the problem is that people keep buying and using the stuff, knowing that that there may be negative outcomes.

Start time: 2978.34
End time: 2999.34
Speaker: SPEAKER_01
Transcript:  But I'm going to give you my reaction to this, which is kind of the opposite, which is, oh, great.  Now all these privacy nervous Nellies are going to make it illegal for my Amazon Echo to work.  And I want my Echo.  That's the thing, right? That's always the convenience store.  I want it to listen to me.

Start time: 3000.35
End time: 3031.07
Speaker: SPEAKER_06
Transcript:  The convenience store.  So so if somebody was robbed in a convenience store in the 80s, the police are still going to show up and they're going to ask a bunch of people who were there.  And then they're going to round up the usual suspects and probably a very, very racist way.  Right. I would argue that that's not the only difference between the 1980s and the 20 a late 2010s is that it requires less humans and more of that process is automated.  But it strikes me that it's sort of like the same process still.  No, I don't know.

Start time: 3032.46
End time: 3039.16
Speaker: SPEAKER_01
Transcript:  You're right. I mean, now we have racist face recognition.  So that's that's speeds up the process.

Start time: 3039.52
End time: 3044.40
Speaker: SPEAKER_06
Transcript:  It speeds up. Right.  So it's like speeds up what we were kind of already doing.  I'm not saying that's good or bad.

Start time: 3044.58
End time: 3086.83
Speaker: SPEAKER_01
Transcript:  Oh, it's just look, it's just another tool for law enforcement.  And you said it.  There's got to be checks and balances.  Law enforcement. I understand.  And we want them to fight crime and arrest bad guys.  And we want them to do it in a constitutionally manner that protects us against overreaching state authorities.  So we want both. And we need both.  I think I'm not against law enforcement using the technology.  I just think it needs to be done conforming with the Constitution and particularly the rules against unlawful search and seizure and, you know, testifying against yourself.  The Fourth and Fifth Amendments.

Start time: 3088.04
End time: 3102.04
Speaker: SPEAKER_06
Transcript:  Who had that story recently about the people who do QA on I think it was Amazon devices and like they what they've been listening to as part of that QA process.  Oh, yeah. That just came out this week, too.

Start time: 3102.46
End time: 3134.36
Speaker: SPEAKER_01
Transcript:  Yeah. Yeah.  Amazon said, well, and but you know what?  This is not a surprise to anybody who understands how AI works.  Right.  This is called training.  Yeah.  So Amazon says humans are going to transcribe to improve the customer experience.  Some of your recordings.  This comes from a Bloomberg story.  So we'll give Bloomberg the credit.  Amazon workers are listening to what you tell Echo.  A global team.  Not everybody.  No, no.  It's not everybody.

Start time: 3134.42
End time: 3151.92
Speaker: SPEAKER_06
Transcript:  They're doing it in small matches.  They can't.  Right.  They can't.  And I think some of them were in Romania and some of them.  But, you know, it sounded like they had questions.  Like some people thought they heard overheard domestic violence.  You know, other ones heard really.  So they were PTSD.

Start time: 3152.42
End time: 3209.40
Speaker: SPEAKER_01
Transcript:  According to Bloomberg, employs thousands of people around the world to help improve Amazon's digital assistant.  The team listens to voice recordings captured in Echo owners homes and offices.  The recordings are transcribed, annotated and fed back into the software to this is to make it better.  Right. To eliminate gaps in Echo's understanding of human speech.  And actually, Amazon's asked us to do that, too.  Right. If you can actually go back and look at all the things you said, you can hear what you said and you could tell Echo if you got it right.  The voice review process and Bloomberg had seven people, anonymous sources.  It's not a surprise.  Mix of contractors, full time Amazon employees work from Boston to Costa Rica to India to Romania.  They can't speak publicly about the program because they signed non-disclosure agreements.  They work nine hours a day.  What a nightmare job.  Parsing as many as a thousand audio clips per shift.  According to two workers.

Start time: 3209.46
End time: 3216.36
Speaker: SPEAKER_06
Transcript:  It sounds just, I mean, it sounds about as bad as working as a Facebook stacker in the Amazon warehouse.

Start time: 3216.88
End time: 3222.87
Speaker: SPEAKER_01
Transcript:  Yeah. Look, it's clear that technology has created a lot of inhuman jobs, but at least there's jobs that humans can do.

Start time: 3226.46
End time: 3233.40
Speaker: SPEAKER_06
Transcript:  That's like one of the saddest things I've ever heard.  That really got me.  At least we got a job, man.  You know?  It's so depressing, but you're right.

Start time: 3233.83
End time: 3328.58
Speaker: SPEAKER_01
Transcript:  Well, it's true.  Until we train them well enough and then we'll be out of work.  It's like Uber drivers, basically.  You're just a stand in for the artificial intelligence until we get that good enough.  Yeah.  The work is mostly mundane.  One worker in Boston said he mined accumulated voice data.  You'll like this film girl.  For specific utterances such as Taylor Swift, apparently they were listening to your Echo and annotated them to indicate the searcher meant the musical artist.  As opposed to, I don't know, Taylor Swift who wrote Gulliver's Travels?  What? I don't know.  Right.  Occasionally.  Now, on the other hand, well, I will say, occasionally the listeners picked up things Echo owners likely would rather stay private.  A woman singing badly off key in the shower.  Well, who doesn't do that?  Or a child screaming for help.  Ooh, that would be hard.  The teams use internal chat rooms to share files when they need help parsing a muddled word.  Or come across an amusing recording.  So Amazon's response, we take the security and privacy of our customers personal information seriously.  We only annotated an extremely solid sample of Echo voice recordings.  I'm sorry.  I said the A word in order to improve the customer experience.  For example, this information helps us train our speech recognition and natural language understanding systems to make sure that we're able to communicate with our customers.  This was exactly the same thing that we were learning about.  Who was that that was doing that?  Nest?

Start time: 3330.87
End time: 3331.40
Speaker: SPEAKER_00
Transcript:  I can't remember.

Start time: 3331.56
End time: 3333.54
Speaker: SPEAKER_06
Transcript:  With the microphone?  Yeah.

Start time: 3334.66
End time: 3372.38
Speaker: SPEAKER_01
Transcript:  This is just what this is.  This is how I think people.  Technology is always assumed to be.  It's do it first and apologize later.  Well, but also it's also assumed there's no human intervention.  It's in the shiny box.  But there's but somebody's got to train this stuff.  We use multi-factor authentication to restrict access service encryption and audits of our control environment to protect it.  And by the way, this article does not imply that there were leaks of any of this information.  It's just a revelation that it happens.  But if you'd asked me before this, I would have and I thought about it.  I said, well, yeah, I'm sure they're doing that.

Start time: 3373.54
End time: 3379.32
Speaker: SPEAKER_06
Transcript:  And so what happens if somebody decides to do a audio file dump on the WikiLeaks using all of this or whatever?

Start time: 3380.19
End time: 3382.14
Speaker: SPEAKER_01
Transcript:  I mean, you can have me singing off key.

Start time: 3383.62
End time: 3389.40
Speaker: SPEAKER_06
Transcript:  But like and, you know, supposedly it's anonymized and there's but you can still associate the account number according to the story.

Start time: 3391.02
End time: 3397.84
Speaker: SPEAKER_01
Transcript:  This account number with this validates all the people say I will never have an Amazon Echo or Google Home or any of these things in my house.

Start time: 3400.08
End time: 3400.40
Speaker: SPEAKER_06
Transcript:  Yes, they will.

Start time: 3400.42
End time: 3421.40
Speaker: SPEAKER_05
Transcript:  Yeah, I was going to say, I mean, like there are some people who won't.  But once you've had the ability to say, hey, you know, keyword play whatever or read me the weather or, you know, what's on my calendar?  It's kind of awesome.  Like, you know, I mean, like as creepy as all of this stuff has the potential to be, there's this other side of it that is really compelling.

Start time: 3421.66
End time: 3436.40
Speaker: SPEAKER_06
Transcript:  And but it's also yeah, you know, yeah, but it's also like really hard to buy a device that doesn't have a camera or a speaker in it.  Like your television have a camera, the remote controls you get from your cable services have a microphone built in and some of them you just talk to them.

Start time: 3436.42
End time: 3441.34
Speaker: SPEAKER_01
Transcript:  Why do you think that is? Is that because everybody's demanding the ability to ask to watch Game of Thrones?

Start time: 3442.47
End time: 3475.40
Speaker: SPEAKER_05
Transcript:  Because they know because they want the information to sell to other people.  The companies want to be able to do that.  Well, I mean, the thing is, is that with the TV capture stuff, they don't even need to have the camera and the TV thing.  Like it'll literally capture what pixels are on the screen and then send them out to say, oh, this is what you were watching.  I mean, that was what happened with the Vizio and the Samsung cases.  It's not even about a camera.  It's literally taking a sampling of the pixels that are on the screen.  And if you I've tried to do this, I tried to do this recently to try to find like a non smart television set because I was just looking for like a dumb.

Start time: 3476.00
End time: 3478.26
Speaker: SPEAKER_00
Transcript:  I want to monitor. I'm with you. I just want to monitor.

Start time: 3478.69
End time: 3480.38
Speaker: SPEAKER_01
Transcript:  I'll add the intelligence.  You can't do it. No, you can't.

Start time: 3480.64
End time: 3498.40
Speaker: SPEAKER_05
Transcript:  And you can't do it.  Like it's not possible unless you're getting something that's years old out of warranty and all kinds of other things.  They literally don't exist.  Even at the very high end where you used to be able to go.  And if you spent like X thousands of dollars, you know, it wouldn't have it.  Even those. It's like, no, it's going to have the stuff in it.

Start time: 3498.44
End time: 3522.40
Speaker: SPEAKER_01
Transcript:  You know, my how my my rule of thumb for knowing if this is problematic is if my reaction is, well, I have nothing to hide.  I'm not doing anything wrong.  So if Google knows where I am, big deal.  I'm not saying anything bad.  So if Amazon's listening to me, big deal.  And I know as soon as I say that to myself, which I have, we got a problem.  There's a big problem.  Nobody should ever have to say, well, I've got nothing to hide.

Start time: 3523.39
End time: 3523.89
Speaker: SPEAKER_04
Transcript:  No, never.

Start time: 3526.19
End time: 3589.81
Speaker: SPEAKER_06
Transcript:  Did I ever tell you about the origin of the word privacy in Japanese?  It sounds like a non sequitur, but it's it's.  So I lived in Japan the first time in the in the 90s.  And this was right on the cusp of the Internet and being an e-commerce and being able to share Internet data and buy stuff.  And I remember in Japan that I couldn't just order stuff online yet because it was a very private country and people thought it was crazy that you would just type in a credit card number.  Wow.  Like that was insane.  This bastion of all the super high tech was a place where Internet e-commerce hadn't hadn't quite landed yet.  And when it finally did, they had to invent a word to talk about privacy because privacy was eroding and they had never had to discuss it before because it was just assumed that that you had it.  And the Japanese word for privacy is Puraibashi.  They borrowed it.  Wow.

Start time: 3590.90
End time: 3597.28
Speaker: SPEAKER_01
Transcript:  Isn't that funny that it's not because they didn't know about privacy, it's because it was just you didn't need a word for it.  It just is.

Start time: 3598.08
End time: 3626.40
Speaker: SPEAKER_06
Transcript:  Right. So to what you were just saying, which is I don't have anything to hide.  I don't like the fact that I have to have that conversation.  We didn't used to have to have that conversation.  And now here we are where either we just assume that we hope that we don't have anything to hide.  And yet we have no control over it anyway.  So this is just the way we're appeasing ourselves, I guess.  Or we try to rail against it or we just close our eyes and and forget that it's that that everybody else is looking in.

Start time: 3626.58
End time: 3662.38
Speaker: SPEAKER_01
Transcript:  I mean, this is a good a good panel for these this topic because I think all well, I'll leave myself out.  Both of you embrace the future.  I will include myself in that.  We're technologists.  We love we love what technology has done and can do for the future.  But at the same time, especially you, Amy, as a futurist, we have to grapple with some very difficult challenges that technology brings us.  Yeah.  And I think you're both very well aware of both sides of that double edged sword.

Start time: 3664.75
End time: 3668.70
Speaker: SPEAKER_06
Transcript:  So, Christina, you have devices listening in your your place of residence, right?  Yeah, I do.

Start time: 3670.56
End time: 3680.32
Speaker: SPEAKER_05
Transcript:  And I have conflicted things about it, right?  Like, I mean, you know, we have been plugged in.  But then, yeah, you know, but I think about that.  I'm like, is this good enough?  Is this something that I really want?

Start time: 3680.72
End time: 3685.36
Speaker: SPEAKER_01
Transcript:  And then you're more likely because you've lived in public for a long time.  This is true.

Start time: 3685.81
End time: 3694.36
Speaker: SPEAKER_06
Transcript:  This is very true.  Yeah.  If you if you ever feel like you need to have a like a conversation, do you have you ever like unplugged your device to have like a private conversation?

Start time: 3695.65
End time: 3696.18
Speaker: SPEAKER_05
Transcript:  I haven't.

Start time: 3696.90
End time: 3699.38
Speaker: SPEAKER_01
Transcript:  Lisa won't let me have a device with a camera in the bedroom.

Start time: 3699.40
End time: 3703.40
Speaker: SPEAKER_05
Transcript:  Yeah, I don't do that.  I don't have it.  I don't have it in the bedroom.

Start time: 3703.46
End time: 3746.85
Speaker: SPEAKER_01
Transcript:  We have so many listening devices in the bedroom that if I say the A word, several of them will.  I'll hear blip blip blip blip blip.  By the way, it's a problem in France because the words for with her are avexa.  Apparently, that's a problem.  Apparently, that's a problem.  And in Mexico, it's a problem because the word for make it show it is also it.  Apparently, that's Amazon's reason for having this facility in Romania is the darn French and Mexicans keep saying the word by accident.  I know mine wakes up all the time for no apparent reason.  So does my Google.  Yeah.  Yeah. For no Cortana never wakes up, though, oddly.

Start time: 3748.40
End time: 3750.30
Speaker: SPEAKER_06
Transcript:  What do you have Cortana on?  They don't have a speaker out.

Start time: 3750.40
End time: 3753.40
Speaker: SPEAKER_01
Transcript:  Carmen Carden makes a Cortana speaker and I have it.

Start time: 3753.46
End time: 3756.67
Speaker: SPEAKER_05
Transcript:  Yeah, I don't even I don't even have that.  I have a stack in my office.

Start time: 3759.46
End time: 3767.96
Speaker: SPEAKER_01
Transcript:  I have a Google Home Max, which is the biggest on top of that.  The Echo Show.  And then on top of that, a little rack.  It's not even a rack.  They're just standing on top of each other.

Start time: 3768.56
End time: 3771.50
Speaker: SPEAKER_06
Transcript:  It's some kind of like crazy boombox.  I want to see glued all together.  Yeah.

Start time: 3772.94
End time: 3775.86
Speaker: SPEAKER_01
Transcript:  If I had a social network, I would post a picture of it, but I don't.

Start time: 3777.34
End time: 3789.40
Speaker: SPEAKER_05
Transcript:  Yeah, no, I have a I have I have a few Echo devices.  I have a bunch of Sonos things that now have the you know, the Echo things built in.  I do have a HomePod, but I mean, who cares about HomePod or Siri?

Start time: 3790.45
End time: 3792.38
Speaker: SPEAKER_01
Transcript:  Siri is listening because she does wake up sometimes.

Start time: 3792.40
End time: 3795.40
Speaker: SPEAKER_05
Transcript:  Right. Oh, I know. She does wake up sometimes.  I sometimes I'll hear her babbling.

Start time: 3795.80
End time: 3802.13
Speaker: SPEAKER_01
Transcript:  She's in her kitchen and I'll just I'll go in the kitchen.  She's just babbling like she's lost her mind.  Like she's talking.

Start time: 3802.50
End time: 3824.38
Speaker: SPEAKER_05
Transcript:  I will say I will say I do love Siri and my Apple TV, but that's about the only Siri that I like.  And then I got a Google Home Mini from Spotify because they had like some oh, if you're in a family account, we'll send you a free one.  But I'll be honest, I haven't hooked that one up.  I don't know. I should. I should put it in.  I should put it in the in in one of the places.

Start time: 3824.62
End time: 3829.02
Speaker: SPEAKER_01
Transcript:  I don't know. I feel like I'm podcasting all the time, no matter what somebody's your point.

Start time: 3829.92
End time: 3844.38
Speaker: SPEAKER_05
Transcript:  But I think to your point, the whole having to have sort of lived in public thing, that might be part of the difference.  There's still lines that I will draw.  Like you said, I don't have cameras in my bedroom or any of those places.  But well, that was my wife. I would have done it.

Start time: 3845.28
End time: 3848.86
Speaker: SPEAKER_01
Transcript:  I don't care. I do have I do have an Echo Spot in my closet.

Start time: 3850.41
End time: 3853.06
Speaker: SPEAKER_06
Transcript:  And what about smart mirrors? Do you do either of you have any of that?

Start time: 3853.84
End time: 3856.40
Speaker: SPEAKER_05
Transcript:  There's a whole I don't know. I'm kind of into it.

Start time: 3857.26
End time: 3865.14
Speaker: SPEAKER_06
Transcript:  Home automation is one of the big trends that we're tracking this year.  And so there's a suite of Peloton like home smart home.

Start time: 3865.64
End time: 3876.63
Speaker: SPEAKER_01
Transcript:  Oh, yeah. Yeah, I know the guy.  I know Ryan Vance, who was a longtime tech TV guy and did the Tony Gonzalez Fit program is working with them.  I can't wait to get one. Are they out yet?

Start time: 3877.50
End time: 3925.40
Speaker: SPEAKER_06
Transcript:  The one is out. There's another one.  The the really cool looking one that I have a feeling I would rip right off of.  I don't know how it does because it has stuff attached to it. Right. Right. Right.  But this is kind of the point. Right.  I think we that we are trading cool and convenience for a willing and willful ignorance about what happens on the back end.  And I think we're all doing it. I mean, if if if the three of us are doing it and we're hyper connected and we understand,  think of the implications for people who are moving into Amazon homes.  Amazon and Lennar have Lennar is America's largest home builder.  They've partnered and are building smart homes that don't just have a few Alexa device a word.  Sorry, devices that have imagine your I'm sorry.

Start time: 3925.58
End time: 3927.32
Speaker: SPEAKER_01
Transcript:  I started playing. This is the total.

Start time: 3927.56
End time: 3963.40
Speaker: SPEAKER_05
Transcript:  You know, the smart thing. I'm actually a friend of mine successfully led her apartment complex.  And they're a big, you know, kind of a chain of building, you know, I guess management to not install the smart locks.  She's a security professional and she was very, very adamant. Right.  She's right. Because and look, to me, that is one of those places where I would completely not be OK with that, where I'm being told you have to have the smart lock installed.  You don't have any alternative. This is what we're going to do.  We can enter your house at any time, especially when the then the security behind those has been so easily hacked and whatnot.

Start time: 3963.50
End time: 3972.36
Speaker: SPEAKER_01
Transcript:  Yeah, but come on. I mean, aren't locks just a suggestion anyway?  It's just a social norm. It's not really.

Start time: 3973.67
End time: 3976.40
Speaker: SPEAKER_06
Transcript:  And there's windows. If somebody wants to break in there, somebody wants to get into your house.

Start time: 3976.44
End time: 3980.38
Speaker: SPEAKER_01
Transcript:  Somebody is a mind breaking the social norm. They can get into your house.

Start time: 3982.71
End time: 3992.40
Speaker: SPEAKER_06
Transcript:  This is where it's useful to like look at the bigger constellation of of things.  So Amazon has also invested in prefab home companies, you know, in.

Start time: 3992.70
End time: 3996.08
Speaker: SPEAKER_01
Transcript:  Oh, yeah, they have a whole system. The whole house is smart.

Start time: 3996.92
End time: 4026.30
Speaker: SPEAKER_06
Transcript:  That's right. So that's what I'm saying. Like we're quibbling right now over smart whether or not we have a camera in our bedroom.  And I guess what I'm trying to say is unless we change how we think about this, it's inevitable that we will all have camera that will all constantly be under persistent surveillance,  which in some ways I think are great. But we don't have all of the back end legal infrastructure and privacy infrastructure, all that other stuff in place.  Yet the technology as usual is pushing far ahead of our capacity to think about what it all means.

Start time: 4027.30
End time: 4140.40
Speaker: SPEAKER_05
Transcript:  Right. I mean, but the one thing I would say that kind of gives me hope a little bit with this, because of course, you're completely right.  But there does seem to be kind of a breaking point where people are pushed too far.  And I mean, Facebook is going through that right now where for many, many years, many of us never expected that the public would ever turn against Facebook,  despite all the different things that they were doing and all the different breaches that were happening.  And then with the combination of Cambridge Analytica and the involvement with Myanmar and the ridiculous amount of other atrocities, less people are using them.  They have actually seen fewer people signing into standard Facebook.  Now, they still have their myriad of other services that they can get all kinds of information from.  But there is this palpable kind of backlash against that.  And so, you know, it's about finding that that balance.  But I think to your point, yeah, when even those of us who know better are still willing to take the bad parts,  even knowing what can potentially be bad about being in this always unsurveilled society, like what chances anyone else have.  I mean, I even think about myself. So I travel a lot.  And before the show, you know, you both were talking, you both travel a lot as well.  You know, with biometrics and whatnot, with, you know, the amounts of times I go through airports, you know, I use I use clear.  I'm in these other things like my photo, my fingerprints, other information about me is maintained and in unknowing, an uncountable number of databases.  You know, my passport information, my photos, all that stuff as I travel through airports and it's now being held by foreign governments.  And that's not even something that I have the opportunity to opt out of.  Right. Like that's just if you want to fly through someplace, that's what you have to do.  Aren't you just tempted to give up at some point? I mean, have you given up or not?  I mean, I have and I haven't. It's like I'll have certain things that I will.  But yeah, there's a certain amount of malaise, I think, and fatigue with all of it.

Start time: 4141.64
End time: 4146.40
Speaker: SPEAKER_01
Transcript:  Fighting is so hard. And ultimately, you have to move to a cabin in the woods.

Start time: 4146.66
End time: 4228.30
Speaker: SPEAKER_06
Transcript:  Or you or you try to create systemic change, which is what you do.  And I like that. I am because I I to me, giving up is tantamount to giving up on our futures.  The big nine. I'm not willing to do that. Amy's book is all about that.  Well, because we can we can I don't I don't want to like if you like stop for a moment and just think about what's the worst possible feeling you could have.  For me, it's regret like that. That is the worst.  And I don't want to be 30 years from now looking like feeling a horrific sense of regret that we could have taken a different path.  And we could have chosen we could have chosen to slow down for five seconds and figure out how can everybody still make plenty of money?  You know, but but but how do how does everybody how can we make this so that everybody wins and doesn't everybody wins a little more and loses a little less?  I don't want to be filled with regret that I didn't play my part.  I didn't do something when I could have like that's the worst that regret.  That's like the worst feeling because where it comes from is a place of there was something that I could have done differently or I didn't have to make that mistake.  You know, and I could have I could have had a better outcome.  I don't want to I don't want to be in that place.  So that's why I refuse to give up.

Start time: 4228.60
End time: 4237.40
Speaker: SPEAKER_01
Transcript:  I've even given up on regret.  I just know I'm going to regret this.  It's just life regrets.  I've had a few.

Start time: 4237.40
End time: 4238.30
Speaker: SPEAKER_06
Transcript:  That's why we're talking about.

Start time: 4241.64
End time: 4284.40
Speaker: SPEAKER_01
Transcript:  You know, I think there's a good chance that we'll end up in the world of idiocracy where I think we're assuming that.  All this stuff is going to work and then we're going to make a perfect surveillance system and it's going to know everything about it.  There's also the option.  Here's the smart home that Lennar is building with Amazon.  This is the pantry.  It's lined with something that no longer exists.  Dash buttons.  They thought, oh, this will be great in the pantry.  We'll put dash buttons next to everything.  And then you press the button when you run out, except Amazon stopped making them.  And I feel like we're creating a janky future that's just going to break down.  That's again, like, look at the bigger picture.

Start time: 4284.78
End time: 4293.83
Speaker: SPEAKER_06
Transcript:  There's sensor technology being built inside of materials that are in our pantries.  So you don't need a dash button if you as long as you.  It was just early technology that we.

Start time: 4294.58
End time: 4301.67
Speaker: SPEAKER_01
Transcript:  Yeah, it's like a Las Vegas hotel where you take it off the shelf.  Mini bar.  You're paying for the money.  You're paying for it.  Yeah.

Start time: 4302.42
End time: 4319.40
Speaker: SPEAKER_05
Transcript:  Well, I mean, look, I had a bunch of dash buttons and I loved the idea of them in theory.  And I did actually hack them so that they could be used for other things, which was really fun.  But that the problem was right.  Like, OK, so you get them for toilet paper.  That's a perfect thing to get them for.  Except you don't think about the fact that you need toilet paper until you're on your last.

Start time: 4320.32
End time: 4324.40
Speaker: SPEAKER_01
Transcript:  Just stay there for a day or two and it will be delivered.

Start time: 4325.11
End time: 4357.70
Speaker: SPEAKER_05
Transcript:  Well, and that's the issue. Right.  At that point, you're like, OK, I'm going to actually have to run down the street and buy toilet paper rather than waiting for prime to come.  Now, if they had integrated the dash buttons with prime now, like the same day to live, that's right.  Right. Right. Because I use prime now all the time and way too much.  And in that point, I'm like, oh, I can get this in an hour if I pay another five dollars.  Done like that would be useful.  But it wasn't. So, you know, I did enjoy my Haribo one, though, because I had one for the gummy bears.

Start time: 4359.61
End time: 4360.57
Speaker: SPEAKER_01
Transcript:  Oh, that's dangerous.

Start time: 4361.40
End time: 4380.28
Speaker: SPEAKER_06
Transcript:  Can I get a totally unrelated tangent?  That's funny. But related to gummy bears.  Always. OK. So where I grew up, I grew up on the I grew up in northwest Indiana, just outside of Chicago.  And there's this gummy bear factory called Albany's and they make both regular gummy bears and sugar free gummy bears.

Start time: 4380.40
End time: 4382.40
Speaker: SPEAKER_01
Transcript:  Do not do the sugar free gummy bears.

Start time: 4382.42
End time: 4384.38
Speaker: SPEAKER_04
Transcript:  Oh, my sugar free ones are so bad. Oh, no, no, no.

Start time: 4384.44
End time: 4393.26
Speaker: SPEAKER_06
Transcript:  If you want to have a really good laugh, go on to Amazon and look at the reviews for the sugar free gummy bears.  It's like, oh, my God, it'll be like the best 10 minutes you've spent ever.

Start time: 4393.98
End time: 4405.76
Speaker: SPEAKER_01
Transcript:  Let's for those who are wondering what could possibly go wrong.  It's totally true. The fake sweetener they use has a slightly laxative effect.  You can imagine the rest. Yeah.

Start time: 4406.92
End time: 4414.38
Speaker: SPEAKER_06
Transcript:  Well, if you only eat one, you're OK. You eat like three to four of those.  It's explosive. You know, it's very explosive.

Start time: 4414.72
End time: 4418.40
Speaker: SPEAKER_05
Transcript:  We had somebody who actually sell them in five pound bags.

Start time: 4418.52
End time: 4421.32
Speaker: SPEAKER_01
Transcript:  It's the best Amazon review ever. Oh, yeah.

Start time: 4421.66
End time: 4440.40
Speaker: SPEAKER_05
Transcript:  No, that's what I'm saying, because I buy the five pound regular bags all the time and you have to make sure you don't get the sugar free ones because yes, there was actually we had somebody.  I think it was a Gizmodo who like knowingly did a review of them and they oh, it can't be that bad.  And then it was worse than anyone could have ever imagined.

Start time: 4440.50
End time: 4453.10
Speaker: SPEAKER_01
Transcript:  So the top positive review on Haribo sugar free classic gummy bear one pound bag is the horror at 30000 feet.  The top critical review is one of the worst days of my life.

Start time: 4453.40
End time: 4462.40
Speaker: SPEAKER_06
Transcript:  That's the Haribo. You need the Albanese.  Albanese is worse. No, no, no. Yeah, that's that's where the hilarious ones.  Oh, you seem fairly, you seem fairly hilarious.

Start time: 4462.84
End time: 4467.18
Speaker: SPEAKER_01
Transcript:  OK, Albanese. Yeah. So this is the this is the hometown favorite.

Start time: 4467.84
End time: 4470.40
Speaker: SPEAKER_06
Transcript:  It is the results are noxious and disgusting.

Start time: 4470.56
End time: 4474.30
Speaker: SPEAKER_01
Transcript:  Use it your own risk and be prepared for a fate worse than death.

Start time: 4475.02
End time: 4482.40
Speaker: SPEAKER_06
Transcript:  So all those five hundred sixty one critical reviews are hilarious detailed accounts about partly because they sell them in five.

Start time: 4482.54
End time: 4483.35
Speaker: SPEAKER_01
Transcript:  Yeah, yeah.

Start time: 4484.82
End time: 4488.61
Speaker: SPEAKER_06
Transcript:  There's a really funny one that's like, why would you sell this in a five pound bag?

Start time: 4491.10
End time: 4526.52
Speaker: SPEAKER_01
Transcript:  The night of 1000 waterfalls.  That's a good one.  Like distant thunder.  Just don't. It's a gift for someone you hate.  These are good for losing 10 pounds.  All right. So we won't buy these.  I didn't realize that there was an alternative to Haribo.  Well, the funny thing is, even though they're sugar free, they have exactly the same caloric count as non sugar free.  So what are they using for the sweetener?

Start time: 4528.61
End time: 4530.24
Speaker: SPEAKER_05
Transcript:  Oh, who even knows that it's something bad.

Start time: 4531.69
End time: 4532.93
Speaker: SPEAKER_01
Transcript:  Something don't you don't want to eat.

Start time: 4536.31
End time: 4539.38
Speaker: SPEAKER_06
Transcript:  Regular ones are good.  They're flavorful. They're flavorful.

Start time: 4539.88
End time: 4548.49
Speaker: SPEAKER_01
Transcript:  They're delicious.  It's in the heart of everything we do.  It's not just on our bag.  It's at the heart of everything we do.

Start time: 4550.09
End time: 4553.79
Speaker: SPEAKER_06
Transcript:  They're like a family run company.  I feel so bad for them.  That's been there forever.  Yeah.

Start time: 4554.48
End time: 4560.82
Speaker: SPEAKER_01
Transcript:  Yeah. They're using erythritol or some sort of sugar alcohol, I'm sure.  Oh, malatol.  That is actually literally baby laxative.

Start time: 4561.83
End time: 4563.28
Speaker: SPEAKER_06
Transcript:  Really?  Yeah.

Start time: 4563.46
End time: 4568.35
Speaker: SPEAKER_01
Transcript:  Oh my God.  That's good.  It's the number one ingredient.

Start time: 4570.22
End time: 4571.19
Speaker: SPEAKER_06
Transcript:  That's so messed up.

Start time: 4573.95
End time: 4590.05
Speaker: SPEAKER_01
Transcript:  Yum.  All right.  Let's take a little tiny break.  Amy, I'm here.  I'm looking for your big book.  I want to get this.  Now you sent it to me, but people can get it themselves right from the Future Today Institute.  TechTrend Report.

Start time: 4590.42
End time: 4602.96
Speaker: SPEAKER_06
Transcript:  This is available for free online.  We give away all of our research.  We always print up some for our clients.  And so I've got like we have a handful.  If you want to buy a hard copy, you can go to our website, but you can download it for free.

Start time: 4603.40
End time: 4605.57
Speaker: SPEAKER_01
Transcript:  So yeah, you can make your own printout if you want.

Start time: 4606.79
End time: 4608.94
Speaker: SPEAKER_06
Transcript:  Yeah.  I mean, it's a ream of paper, but yeah.

Start time: 4610.21
End time: 4625.34
Speaker: SPEAKER_01
Transcript:  Future Today Institute.  But Amy, I'm even more excited about this than the five pound bag of gummy bears you said you're going to send me.  She said she's sending me one.  I can't wait.  And also, Film Girl, it suddenly got dark in...

Start time: 4625.71
End time: 4641.16
Speaker: SPEAKER_05
Transcript:  It suddenly got dark.  So during the break, I'm going to turn on the lights in the building go off at a certain period of time.  So during the break, I'm going to switch them back on again.  Yeah, they're trying to like, you know, save the environment or something, save money.  But I will go reset that in just a second.  That's good.

Start time: 4641.50
End time: 4649.40
Speaker: SPEAKER_01
Transcript:  I'm taking a break.  Here's something you would love, by the way, in your team space.  How many people are in that team space?  It varies.

Start time: 4649.82
End time: 4654.30
Speaker: SPEAKER_05
Transcript:  I mean, I think we've got like six desks, but...  Okay, you need a molecule air freshener for sure.

Start time: 4654.58
End time: 4872.40
Speaker: SPEAKER_01
Transcript:  Yeah.  We have this in our house.  It's funny because Lisa, we were in Hawaii.  I don't know why.  No pollen, no runny nose, no headache.  The minute she gets home, pollen in the air.  Our cats come in.  They're yellow.  They're black cats, but they're coated with yellow pollen.  You can see around the food bowl, all this yellow pollen.  It's that time of year.  And we've had a big rainy season.  So it's an allergy nightmare.  Thank God we have the molecule.  This saved our lives.  We've had the molecule now for a couple of years.  And it's fun because if somebody turns it off or we go away,  Lisa knows immediately.  But when that's running, it is a miracle.  Molecule is not...  It's not a HEPA filter.  It's reimagining the future of clear air.  It uses something they call Pico technology.  The HEPA filter has been around since World War II.  It's a filter, literally, and it can trap big particles,  but not the little ones that can cause some of the worst allergies  and other problems, health problems.  The Pico technology, short for photo electrochemical oxidation,  goes well beyond what a HEPA filter will do.  It not only captures allergens, okay, HEPA filter might do that,  but it eliminates them.  Mold, bacteria, viruses, viruses, particles so small,  they go right through a HEPA filter, trapped and eliminated.  Even airborne chemicals, volatile organic compounds,  like formaldehyde from your carpet or fumes from paint.  Pollutants 1,000 times smaller than those a HEPA filter can catch.  Molecule's technology has been personally effective  and verified by science and used by real people like us,  including allergy and asthma sufferers around the country.  We have a...  My son has a friend who has asthma.  He comes with his breather, his inhaler.  He never needs to use it when he's in our house.  We like it so much we got one for Michael's bedroom.  We now have one in the studio, too.  It's amazing.  The technology was funded by the EPA.  It has been tested by third parties in university laboratories  like the University of South Florida's Center for Biological Defense,  Minnesota University's Particle Calibration Laboratory.  Really beautiful.  It's a solid, sleek aluminum.  It's kind of like the Apple of Air purifiers.  Very easy to change the filters.  It has two filters, a pre-filter and then that Pico filter,  it's almost like a catalytic converter.  It captures the particles and then UV light burns the particles up.  It's really kind of remarkable.  Plus you can tie it to your Wi-Fi network  or a Bluetooth network, control it via Bluetooth.  It has controls on the top, but if you want to do it with your phone.  And the nice thing about tying it to your Wi-Fi,  it'll automatically order new filters when you need them, auto refills.  Molecule's amazing.  We're going to get you $75 off your first order.  And if you've heard these ads before and went and saw  that they were sold out, they now have them in stock.  They do sell out fast though, so go to Molecule with a K,  M-O-L-E-K-U-L-E dot com and use the promo code TWIT1.  That's a new promo code, TWIT1, at checkout for $75 off your first order.  Don't just capture allergens, destroy them with Molecule.  You'll thank me later.  Molecule dot com, promo code TWIT and the number one, M-O-L-E-K-U-L-E.  And we thank them for their support.  This is the gym on the wall that you were talking about, Amy.  It's called the tunnel.  So this is a smart mirror.  I don't know if this is what I want, but I thought a smart mirror would just say,  hey, you look good today.

Start time: 4872.40
End time: 4888.02
Speaker: SPEAKER_06
Transcript:  There is another one.  There are a couple out there where this looks like it's just showing.  The ones that I've seen, and I can't remember the brand.  This has stuff on them.  It looks at what you're doing and then automatically changes the tension  and tells you to move your posture.

Start time: 4888.40
End time: 4899.75
Speaker: SPEAKER_01
Transcript:  Xbox used to do that.  I like that.  With the Kinect.  It would know your heart rate.  It would know if you're working hard enough.  It would adjust it based on that.  That really killed me when they killed that product.  I thought that was a great product.

Start time: 4900.40
End time: 4902.30
Speaker: SPEAKER_05
Transcript:  It was great.  It had good stuff for sure.

Start time: 4902.40
End time: 4903.40
Speaker: SPEAKER_01
Transcript:  It just shows you sometimes.

Start time: 4903.50
End time: 4914.81
Speaker: SPEAKER_05
Transcript:  But people freaked out.  But people freaked out.  Is that why?  When the Kinect 1 came out, people freaked out about the Kinect stuff  and the always on stuff.  And a lot of the things, people were not into it.  So yeah.

Start time: 4915.62
End time: 4920.70
Speaker: SPEAKER_01
Transcript:  So Apple bought the company.  It was an Israeli company.  And it's in the front of your iPhone X and XS.

Start time: 4921.54
End time: 4929.60
Speaker: SPEAKER_05
Transcript:  I know.  And it's funny because Windows Hello has a lot of that same stuff with it too.  And it's great.  I was amazed by how.  Kinect.

Start time: 4931.73
End time: 4935.77
Speaker: SPEAKER_06
Transcript:  I thought Kinect was a, was it just only ever a subsidiary?  No, no, no.

Start time: 4936.50
End time: 4944.10
Speaker: SPEAKER_01
Transcript:  They didn't buy Kinect.  They bought the company that did the foundational technology for Kinect.  Apple stole them out.

Start time: 4944.50
End time: 4955.20
Speaker: SPEAKER_05
Transcript:  The company that made the camera, the three dimensional kind of IR camera sort of tech  for the very first Kinect.  The second Kinect, the Kinect for the Xbox One was made, I think, in-house.

Start time: 4955.72
End time: 4968.02
Speaker: SPEAKER_01
Transcript:  PrimeSense was the sensor company.  PrimeSense, yes.  And Apple.  Apple bought them, which is kind of weird that Microsoft, I guess Microsoft knew.

Start time: 4969.66
End time: 4981.40
Speaker: SPEAKER_05
Transcript:  Well, I mean, although I mean, like I said, I think it was done in-house because the second  Kinect was made by a different, it was made with different stuff.  And Windows Hello is largely the same idea.

Start time: 4981.48
End time: 4985.22
Speaker: SPEAKER_01
Transcript:  And yeah, once you get the idea, it's not probably too hard.

Start time: 4986.11
End time: 5008.36
Speaker: SPEAKER_05
Transcript:  Yeah.  Windows Hello always impresses me, but it can be like at a weird angle.  And it'll still, as long as I'm not out in bright sunlight, it'll recognize me basically  anywhere.  I wish I had Face ID on my laptop.  I mean, Touch ID is fine, but there is something about just having to like casually glance  up and automatically be logged in that's very nice.

Start time: 5008.40
End time: 5029.77
Speaker: SPEAKER_01
Transcript:  So I got a new Windows laptop.  I got that new Syncpad.  And I have the choice between all the different because it has a Hello camera.  It has a Hello fingerprint reader.  Fingerprint reader.  I could also do the, there's like eight ways to log in and I'm using the fingerprint.  But you know what I think, is Hello as secure, the face recognition as secure as a fingerprint?  Is it good?

Start time: 5030.40
End time: 5035.22
Speaker: SPEAKER_05
Transcript:  Yeah, it's the exact same.  I mean, it's the exact same encryption.  So whether you're using the fingerprint sensor or the camera, it's the same.

Start time: 5036.08
End time: 5041.60
Speaker: SPEAKER_01
Transcript:  Somebody can't, my twin can't unlock it, right?  Actually, I don't have a twin, so that probably is true.

Start time: 5042.86
End time: 5044.40
Speaker: SPEAKER_05
Transcript:  Yeah, I mean, I think your twin probably could unlock it.

Start time: 5044.46
End time: 5048.40
Speaker: SPEAKER_01
Transcript:  That was a problem with Apple's iPhone X, right?  The family members were able to unlock it.

Start time: 5048.46
End time: 5066.34
Speaker: SPEAKER_05
Transcript:  Well, I mean, I think it's a problem with anything.  If you have an identical twin, like they're probably going to be able to use their facial  thing to do it, probably, honestly.  But that's if you have like an identical twin, it's not enough to be you look alike.  Like even fraternals, it might not work.  But if you have an identical twin, I mean, you also have the same DNA.

Start time: 5066.66
End time: 5091.40
Speaker: SPEAKER_01
Transcript:  So, you know, when the iPhone X came out, there were a mother and son and he could unlock  In fact, wait a minute, Megan Moroney's son could unlock her iPhone X.  Come to think of it.  And they're identical twins, so they could unlock each other and they could unlock her.  So it's I think, as I remember, it does things like spacing between the eyes.  It does the geometry of the face.  It's doing all these weird things.

Start time: 5092.02
End time: 5102.40
Speaker: SPEAKER_05
Transcript:  And then it's changing things every time you're using it, updating.  So, you know, the fingerprint sensor is obviously really fast, too, because one of my laptops,  my Huawei doesn't have a face ID or doesn't have the Windows Hello camera.

Start time: 5102.40
End time: 5104.40
Speaker: SPEAKER_01
Transcript:  Oh, but it has a better fingerprint reader, though, I think.

Start time: 5104.44
End time: 5124.40
Speaker: SPEAKER_05
Transcript:  Yeah, but it has a but it does have the Windows Hello fingerprint reader.  So that's instantly like it's super fast.  But, you know, on my service book, obviously that has Windows Hello.  And it's interesting how even if your hair changes or makeup or, you know, weight or whatever,  like it continues to just be able to pick it up.

Start time: 5126.55
End time: 5142.30
Speaker: SPEAKER_01
Transcript:  It does. I know why.  See, the problem, this is a desktop, the Surface Studio.  And every time I want to log in, I either have to lean over it like Lurch or raise it up so that it can see me,  because it has to be at the right angle.  So there is a disadvantage to having it on the desktop.

Start time: 5142.50
End time: 5154.16
Speaker: SPEAKER_05
Transcript:  I was going to say I have the same issue with my with my iPad Pro, because if I'm like laying in bed with it  and I've got it at a weird thing that I have to like sit up exactly, I've got the same sort of situation.  Yeah. Yeah.

Start time: 5155.63
End time: 5203.13
Speaker: SPEAKER_06
Transcript:  So, you know, that Wal-Mart has a smart shopping cart.  Did I talk to you about this last time I was on?  So that's interesting.  How smart is it?  So real smart.  So it's not in production yet, but they've they've built a concept for a shopping cart that collects basic biometrics.  Once you get to the store, so you would lay your hands on the shopping cart and it takes hands on the shopping cart.  It takes a baseline reading of your heartbeat, your perspiration, the tension that you're holding the cart with.  And obviously there are also cameras all around the store.  And I think the idea is as you move throughout the store, it looks for fluctuations.  So if you get to aisle seven and you're trying to find your Captain Crunch and you're ready to, you know, you can't find it.  You're ready to blow a gasket.

Start time: 5203.40
End time: 5207.36
Speaker: SPEAKER_01
Transcript:  My heart always pounds when I get to the Captain Crunch aisle.  There is a.

Start time: 5207.54
End time: 5209.34
Speaker: SPEAKER_04
Transcript:  No, that's really that is really smart.

Start time: 5209.40
End time: 5211.40
Speaker: SPEAKER_06
Transcript:  You know, who comes over to help you find it.  But obviously there's data being.

Start time: 5211.62
End time: 5221.40
Speaker: SPEAKER_01
Transcript:  Now, are they pitching this to say, oh, and it could save your life if you have a heart attack at our store or no, you know, I don't I don't know what their pitch.

Start time: 5221.42
End time: 5243.38
Speaker: SPEAKER_06
Transcript:  I don't think so at the moment.  But here's the I mean, you guys are giving away your faces and your fingerprints and we don't we don't you know, what happens if one of these companies gets bought or sold?  Right.  I mean, because we're sort of black, we're kind of like lackadaisical about our all of our biometrics as well as our DNA.

Start time: 5243.56
End time: 5269.91
Speaker: SPEAKER_05
Transcript:  No, that's that's why I haven't done any of those DNA kits, frankly, even though I would really like the information from them.  And I think would be really good.  I haven't done it.  That's kind of where I draw the line.  My fingerprint stuff, I don't really have a say in the matter.  In some of the cases, you know, especially if you're if you're like I said, if you're traveling a lot or whatever.  But for some of the in my face, I certainly don't.  But my DNA is the one thing.  I mean, our faces are like they're in right.

Start time: 5271.46
End time: 5272.53
Speaker: SPEAKER_06
Transcript:  But like some of this other.

Start time: 5273.40
End time: 5314.38
Speaker: SPEAKER_01
Transcript:  I mean, did you did you read today's New York Times, the privacy project Sarah Jong writing about?  At first, I thought, you know, the title is AI is changing insurance.  Some technologies are better left in the laboratory.  At first, I thought, oh, oh, I'd like to know what is insured.  This is more a potential threat.  Mm hmm.  But this is something if I have to work, you know, whenever we talk about privacy, this is always the example he would use.  Well, you know, you go buy donuts a lot.  If the GPS says you're always at the Dunkin store, you're going to have a hard time getting insurance.  But maybe this is what insurance companies are planning to do.  Oh, completely. It is.

Start time: 5315.84
End time: 5321.11
Speaker: SPEAKER_05
Transcript:  I mean, that's that's always been I mean, they'll give you the subsidized Apple Watch or Fitbit or Fitbit knockoff, which is.  Yeah, they have. That's real.

Start time: 5321.70
End time: 5324.87
Speaker: SPEAKER_01
Transcript:  They offer if you use a Fitbit, you get a reduction in life insurance.  Right. Right. Right.

Start time: 5325.64
End time: 5339.30
Speaker: SPEAKER_05
Transcript:  Because my parents, for instance, like, I guess, whatever their thing is, like my dad had like a knockoff Fitbit and he it broke and he didn't like it.  So for his birthday, which was like a week and a half ago, I bought him whatever the latest, greatest Fitbit was.  I was like, here you go.  John Hancock did this last year, I remember.

Start time: 5339.40
End time: 5340.73
Speaker: SPEAKER_01
Transcript:  Yeah. And and but, you know, and that's a good thing.

Start time: 5342.42
End time: 5395.14
Speaker: SPEAKER_05
Transcript:  You can get you can get reduced rates and whatnot.  But then you do wonder, OK, at least for now, how it's worked is that they'll just give you kind of, you know, like a, you know, some some, you know, like a lower premium or whatever, regardless of what your health is.  But then you wonder, OK, are they going to start trying to gather the information from that?  And at least right now, they're not.  But that's got to be the next level, right, which is they're saying, oh, you know, if you're if we're seeing this sort of activity, then we can anticipate that you'll have you can have lower premiums.  But the inverse is true, too.  If we see, you know, this type of activity, then we will charge you more that I'm not super jazzed about.  And I say this as somebody who I, you know, all accounts would would be on the lower premiums, at least for things like, you know, like heart disease and cholesterol and blood pressure and whatnot.  I'm waving at you from the other end of that scale.

Start time: 5395.40
End time: 5397.61
Speaker: SPEAKER_01
Transcript:  Hello.  Well, here's what I'm.

Start time: 5398.46
End time: 5401.34
Speaker: SPEAKER_04
Transcript:  So, you know, I have a picture this picture this.

Start time: 5401.40
End time: 5422.54
Speaker: SPEAKER_06
Transcript:  You're you're wearing devices.  You're in.  You're effectively plugging into Apple and or Apple and Google and Amazon or some combination.  And those companies increasingly are mining and refining health data because they're increasingly offering their own health things or they're starting to build their own Samsung as a health app.  Talk.

Start time: 5423.48
End time: 5427.08
Speaker: SPEAKER_01
Transcript:  I know it's monitoring everything because I can.  Here's the concern that I have.

Start time: 5427.40
End time: 5521.40
Speaker: SPEAKER_06
Transcript:  The concern that I have is that all this technology that we've been talking about today, 10 years from now or perhaps, you know, maybe five years from now makes a decision that we're going to be able to do that.  Five years from now makes a decision that I haven't burned enough calories and it's somebody's decided to optimize and nudge me into better health.  And therefore, I can't open my garage door and drive to work or walk.  But you would do that because you get a 30 percent discount in your insurance.  But again, wait a minute, wait a minute.  This is kind of a serious point is because the problem is that decision making technologies are by design and flexible.  And there's always context.  So, sure, there are days that I'm feeling like lazy and I don't want to move and I don't want to exercise and I just want to eat whatever and be left alone.  You know, there are other days when there may be very real reasons that I cannot like right now.  I've got two broken ankles.  Right.  And did you jump out of an airplane without a parachute?  It's a long story.  Oh, dear.  I fell off of a stage.  Oh, God.  But then like David Grohl, I picked up my guitar and I kept rockin'.  I'm OK.  Kind of.  And then I have to go in for surgery in a couple of weeks.  But my point is like my foot hurts, you know, and I technically can walk, but it hurts really badly to walk right now.  So you shouldn't.  The thing is that our smart devices, again, like you have to start thinking this through.  It's not just about insurers cost you charging less money or more money.

Start time: 5521.74
End time: 5532.40
Speaker: SPEAKER_01
Transcript:  We'll make it possible for you to call and ask for a waiver today.  But think of your number forty three.  Your call is important to us.

Start time: 5533.60
End time: 5592.12
Speaker: SPEAKER_06
Transcript:  My concern is that there's relatively like a few number of people who are making these decisions about what our best lives and our most optimized lives are who don't have as wide a cultural perspective.  As I might like, who can't possibly think through every possible, you know, variable.  And we don't have overwrite capacity.  So and terms of service are changing all the time.  And if you're not if you don't have like an eagle eye paying attention to all of this, you miss it.  And my concern is for my concern is always for everyday people, everyday people who are caught in the middle of all of this stuff, whose lives wind up unintentionally worse off.  Because there wasn't transparency or nobody thought that, you know, explaining things was all that important or that they were just going to force them to live their best life, their best, healthiest life.  And therefore we were going to use their data in this way.  You know, there's there's always external circumstances that we got to take into account.

Start time: 5593.27
End time: 5642.40
Speaker: SPEAKER_05
Transcript:  I 1000 percent agree. I mean, and you're right.  And we don't do a good enough job with the way that we train these models.  And there isn't really the push to train them better.  We're like you said, we look at those nuances and people think about what the potential consequences are.  And and even if, you know, my personal circumstances right now or wouldn't be negatively impacted doesn't mean that they couldn't be down the line or that I even want it to be part of consideration at all.  You know, because a lot of times the data doesn't even show what people want to read from it.  You know, I mean, I think you're exactly right.  These things can have long term negative impacts on our society that we don't take into account and that doesn't have enough scrutiny because people are saying, oh, look at how great this is and look at how much this will be optimized and how much more productive will be.

Start time: 5642.62
End time: 5647.30
Speaker: SPEAKER_01
Transcript:  But well, and more importantly, I think it's clear that insurance companies need to be more profitable.

Start time: 5648.28
End time: 5661.42
Speaker: SPEAKER_06
Transcript:  Somebody actually just in the IRC wrote KV just wrote it's a privacy tax on the poor.  And I think that's a really completely.  That's completely accurate.  Absolutely. Yeah.  But I think it's a privacy tax on the poor and the and those who either explain that.

Start time: 5662.62
End time: 5664.65
Speaker: SPEAKER_01
Transcript:  Pardon.  Explain how it's a privacy tax.

Start time: 5665.40
End time: 5719.28
Speaker: SPEAKER_06
Transcript:  Yeah.  So I don't think it is at the moment, but I think it will be increasingly if you've got all of these devices.  And so so the way that I like to think about this is, you know, Apple charges a premium, but Apple's devices are the privacy is fairly short up and people understand what data probably is moving around and where that's not the case with all of these other devices.  And if you're somebody who wants to be connected, but can't afford to buy into Apple's ecosystem, you know, then your data are more likely to be extracted and mined and refined and given away.  So but it's not like entirely transparent that that's what's happening.  Right.  You know, in the health device.  So if you're if you're somebody who can't afford to pay or you need to pay less for your insurance for whatever reason, think, you know, and you're willing to give up your data.  I mean, it is.  It's kind of like a privacy tax.

Start time: 5720.17
End time: 5727.40
Speaker: SPEAKER_01
Transcript:  In other words, you have worse insurance.  It'll pay more for insurance if you can't afford to have the fancy monitoring stuff.

Start time: 5727.40
End time: 5736.28
Speaker: SPEAKER_06
Transcript:  Or you're trying to just pay less for whatever reason.  Exactly.  You don't understand what you're giving up in the process.  Right.  Right.

Start time: 5736.40
End time: 5771.36
Speaker: SPEAKER_05
Transcript:  Well, and it's not just the poor.  I would also say the elderly, because like my parents are both of the age where I guess they're on they're on Medicare or whatever.  And, you know, that's what's offering.  I guess my dad gets his free Fitbit thingy or whatever.  You know, and obviously, once you're at, you know, once you're at retirement age and whatnot, you know, and you're you're living off of your retirement off of your retirement age.  Off of your savings off of whatever stipends you're getting.  That's going to go into consideration, too.  So I think it's the poor and the elderly.  And we're all going to be older at one point, regardless of what we do.  So, yeah, well, there is an optimistic framing here, though.

Start time: 5771.40
End time: 5771.87
Speaker: SPEAKER_06
Transcript:  How about this?

Start time: 5772.40
End time: 5824.20
Speaker: SPEAKER_01
Transcript:  A new policy proposal by the Trump administration calls for the surveillance of disabled people's social media profiles to determine the necessity of their disability benefits.  The proposal, which aims to cut down on the number of fraudulent disability claims would monitor the profiles of disabled people and flag content that shows them doing physical activities.  That's horrific.  Or how about this?  Here's a patent application from State Farm for they call it the aggregation and correlation of data for life management purposes.  It's a plan for aggregating home data, vehicle data and personal health data to help you have a better life.  Of course, State Farm's interest is not in having you have a better life.  No, it's not.  I was hit by the car.

Start time: 5824.46
End time: 5875.32
Speaker: SPEAKER_05
Transcript:  Look at this diagram.  I was hit by a car a year and a half ago.  I'm dealing with insurance stuff right now because I was some idiot drove his car into me when I was crossing the street in a pedestrian crosswalk.  This is separate from the bus incident?  This was the bus incident.  So I was hit by the car thrown under the bus.  And so I'm dealing with insurance right now with his insurance company and it's a whole thing.  And my God, like if you mean and already it's dealing with they're trying to kind of be like, oh, it wasn't that bad.  You went to work the next day.  And I was like, well, yeah, because I'm dumb and because I didn't, you know, like, yeah, there you go.  They got you.  I'm like, I'm like, I still couldn't write.  I couldn't use I'm right handed.  I couldn't type.  I couldn't use my wrist for, you know, like sufficiently.  Like I've got all kinds of evidence and everything.

Start time: 5875.42
End time: 5914.40
Speaker: SPEAKER_01
Transcript:  There was a Jack Lemmon movie many years ago called The Fortune Cookie.  Jack Lemmon's an insurance examiner following a guy around on crutches to find out if he really is hurt.  Here's the by the way, this is a brilliant, a brilliant patent application.  Here's here's you, Christina.  Here's your car.  Here's your house.  It all goes into the cloud.  The brain.  Where a computing device, which includes a processor, memory and user interfaces, processes it.  Puts it back down in the data processor so that we can.  I don't know, manage your life.  Yeah, judge so that we can judge.  I need a picture of the judging.

Start time: 5914.40
End time: 5987.26
Speaker: SPEAKER_06
Transcript:  So I'm going to say this once again.  Everybody loves to point the finger at China and China's social credit system.  And, you know, wow, that's crazy.  Thank God we don't live there.  And, you know, ours is just not that coordinated.  Right.  But you're crazy to think that the thing that really gets me is that it's happening in slow motion right in front of our eyes.  And we marvel at it and we fetishize the future and we don't take any action.  And again, like, let's do this is where things get sticky because we have to be able to mine and refine and automate some of these processes in order to do things like  use machines to help us figure out cures to cancer, like to do all of these amazingly difficult, wicked problem solving that we that we want to accomplish.  That's possible. That's on the horizon.  We've got to figure out a way to do this without our consumerism getting in the way and our, you know, strong desire to make a buck every, you know,  Well, thank you.  This is complicated because these companies are public and they need to give a fiduciary responsibility to their shareholders.  We got to sort this stuff out.

Start time: 5988.06
End time: 5993.36
Speaker: SPEAKER_01
Transcript:  I'm just grateful that Google has decided to create an A.I. ethics board to manage this.

Start time: 5994.44
End time: 5996.38
Speaker: SPEAKER_06
Transcript:  It's going so well.  Oh, wait a minute.

Start time: 5996.94
End time: 6008.11
Speaker: SPEAKER_01
Transcript:  So well, they canceled it one week later.  That was never mind.  It was only going to meet four times a year anyway.  So doomed from the start.

Start time: 6009.40
End time: 6009.85
Speaker: SPEAKER_04
Transcript:  Completely.

Start time: 6010.40
End time: 6016.48
Speaker: SPEAKER_06
Transcript:  That was the strangest assemblage of people that I've I mean, like that was a weird.  That was weird from from the beginning.  Yeah.

Start time: 6018.82
End time: 6037.38
Speaker: SPEAKER_01
Transcript:  Let's take let's take a break and we'll be back with more first before we do anything we should.  I wasn't here last week and I and I've asked we put cameras everywhere in the building and I've asked our team to put together a surveillance video of what I missed this week on Twitter.  Let's watch together on Twitter.

Start time: 6037.42
End time: 6043.57
Speaker: SPEAKER_00
Transcript:  This was part of my pirate costume that I wore for Halloween.  I think a couple of years ago.  Right. Your Johnny Depp costume.

Start time: 6044.42
End time: 6047.49
Speaker: SPEAKER_03
Transcript:  That's what everybody said.  I was like, no, I'm just a standard pirate because Johnny Depp.

Start time: 6048.60
End time: 6049.85
Speaker: SPEAKER_01
Transcript:  Yeah.  No, not him.

Start time: 6050.52
End time: 6056.50
Speaker: SPEAKER_03
Transcript:  Yeah.  OK, sorry.  All about Android.  Look at this gaming laptop of a phone.  Oh, that is a gamer phone.

Start time: 6060.56
End time: 6062.97
Speaker: SPEAKER_01
Transcript:  Why are there connectors on the sides of this case?

Start time: 6064.58
End time: 6068.86
Speaker: SPEAKER_03
Transcript:  Interesting.  Because intellectual property be damned.  Yeah, clearly.

Start time: 6070.56
End time: 6070.95
Speaker: UNKNOWN
Transcript:  Cool.

Start time: 6071.40
End time: 6094.14
Speaker: SPEAKER_03
Transcript:  The Black Shark 2 is the device.  That's really interesting.  Tech News Weekly.  According to a new Gallup poll, 61 percent of people polled believe that they use their phone less than the people around them.  That's 61 percent of people believe that they use their phone less.  In reality, in my case, it's true.  Am I part of the problem?  I wouldn't consider you part of the problem, but I think you're lying to yourself.

Start time: 6094.40
End time: 6096.58
Speaker: SPEAKER_06
Transcript:  You know what you do do?  What?

Start time: 6097.52
End time: 6110.93
Speaker: SPEAKER_03
Transcript:  We share an office and you talk to your Google Assistant a lot and I think you're talking to me.  And there needs to be a word like you're just like, remind me to get the laundry.  I was like, I'm not going to remind you to get the laundry.  Twitter.  Technology isn't always pretty, but we are.

Start time: 6112.42
End time: 6116.40
Speaker: SPEAKER_00
Transcript:  Oh, Jason Howell.  How is that phone not getting her?

Start time: 6116.40
End time: 6119.40
Speaker: SPEAKER_06
Transcript:  How is that phone not getting hardly not getting sued by Nintendo?

Start time: 6119.72
End time: 6123.40
Speaker: SPEAKER_01
Transcript:  Oh, yeah, because it's exactly the same as the switch.  Yeah, it looks pretty cool.

Start time: 6123.60
End time: 6129.03
Speaker: SPEAKER_03
Transcript:  Nintendo is getting sued by Game Shark over.  Really?  Yes. For real.  For real. Yeah.

Start time: 6129.46
End time: 6135.92
Speaker: SPEAKER_01
Transcript:  That's the shark. So Game Shark did it first.  Yes. Then Nintendo did it.  Then Game Shark people suing Nintendo.

Start time: 6136.95
End time: 6137.89
Speaker: SPEAKER_06
Transcript:  That's nuts. I didn't know that.

Start time: 6138.90
End time: 6294.36
Speaker: SPEAKER_01
Transcript:  See, it's it's it's something called.  There's a there's a phrase for it.  Reverse confusion.  When a company comes along, I don't know, let's say you had a podcast network called Twitter and a company comes along and names itself similarly and then becomes really big and successful, then people just assume that you little tweet copied the big successful company.  When in fact, it was Game Shark that had the idea all along.  Our show today.  You like my like my beads.  Our show today brought to you.  I got this in a luau.  And I got this when we checked in.  Our show today brought to you by Express VPN.  You better believe when I'm on the road.  If I am on an app and Wi Fi access point, you know, we get to the hotel and they say, oh, hey, good news.  Just go ahead and use the Wi Fi.  There's no password.  There's no anything.  Just go ahead and use it.  That's crazy.  That means I'm on the same network as everybody else at that hotel and anybody who's sitting anywhere nearby because everybody can use it.  That's when you fire up the VPN and you better believe I fired up Express VPN.  Express VPN is a virtual private network that will protect you on public Wi Fi.  If you leave that Internet connection unencrypted, you might as well be writing your password and your credit card numbers on a billboard for the rest of the world to see.  Express VPN protects you by encrypting all the traffic from right through that hotel's Wi Fi all the way from your computer to the Express VPN servers.  And by the way, servers all over the world, 160 different locations.  So you could choose a server near you or you could choose a server that's in the country you want to emerge in.  If you're in a foreign land and you want to use your Netflix, no problem.  Just use the Express VPN server in the US and you're there.  You're there. You're secured. You're anonymized. Your data is encrypted.  And very important when you're looking for a VPN, Express VPN does no logging.  They are not tracking you.  So if you're worried that you're being tracked by your ISP or your carrier, if you're worried that you're being snooped upon, if you're worried that you're being attacked, get Express VPN.  They have simple apps that run on any iPhone, Android phone, Mac or PC.  One button click and you're protected and safe.  And it's less than seven dollars a month for the number one VPN service, according to TechRadar and a 30 day money back guarantee.  So protect your online activity today.  Find out how you can get three extra months free with a one year package at ExpressVPN.com slash twit.

Start time: 6295.76
End time: 6303.00
Speaker: SPEAKER_00
Transcript:  Express, E-X-P-R-E-S-S, ExpressVPN.com slash twit.  Don't ask me how to spell VPN.

Start time: 6303.84
End time: 6383.56
Speaker: SPEAKER_01
Transcript:  Express VPN.com slash twit. Three extra months with a one year subscription.  We thank them for their support. We thank you for supporting them and thereby supporting us.  YouTube struggling a little bit because there's a lot of video on there that advertisers don't want to buy.  And they're getting a lot of heat for a lot of weird things.  So according to Bloomberg, they're now trying a new metric Bloomberg calls responsibility.  Two new internal metrics at YouTube introduced over the last two years to figure out how well videos are performing, not how many subscribers, not how many views.  One tracks the total time people spend on YouTube, including comments they post and read, not just clips.  The other is a measurement called quality watch time.  The goal is anyway to spot content that achieves something more constructive than just keeping you watching.  Is this a sensible plan from YouTube's point of view?  One of the problems people have with YouTube is the recommendation engine that slowly pushes you towards more and more extreme content in order to increase your watch time.  This is Google's attempt to get around that.

Start time: 6386.03
End time: 6389.75
Speaker: SPEAKER_00
Transcript:  Thoughts?  Christina?

Start time: 6390.90
End time: 6392.05
Speaker: SPEAKER_05
Transcript:  It's not a bad option to go to, I guess.

Start time: 6394.40
End time: 6397.02
Speaker: SPEAKER_01
Transcript:  Did you ever consider being a YouTube star, Christina?

Start time: 6397.86
End time: 6399.20
Speaker: SPEAKER_05
Transcript:  If I were 10 years younger, totally.

Start time: 6399.99
End time: 6401.04
Speaker: SPEAKER_00
Transcript:  What does that have to do with it?

Start time: 6401.86
End time: 6402.45
Speaker: SPEAKER_05
Transcript:  I think I'm too old.

Start time: 6403.40
End time: 6406.98
Speaker: SPEAKER_01
Transcript:  How old do you have to be? You're a kid.  What do you have to be, 12?

Start time: 6409.88
End time: 6418.40
Speaker: SPEAKER_05
Transcript:  I think you definitely need to be in your early 20s.  Yeah, I think you need to be in your early 20s.  If I were 10 years younger, I think that would have been the direction I would have gone in.

Start time: 6418.40
End time: 6420.96
Speaker: SPEAKER_01
Transcript:  That explains why I can just never get my YouTube numbers up.

Start time: 6421.42
End time: 6434.96
Speaker: SPEAKER_06
Transcript:  We have a family friend that's grooming.  It's not my family. It's my husband's family knows people who are grooming their 11 or 12 year old to become an influencer.

Start time: 6436.84
End time: 6442.15
Speaker: SPEAKER_01
Transcript:  It's like grooming your kid to be horrifically disappointed in life.  For real.

Start time: 6444.82
End time: 6473.94
Speaker: SPEAKER_06
Transcript:  There's something analogous to what YouTube is currently doing and what the broadcast industry did, I think, just after TV started broadcasting.  Again, I think this has to do with potential impending regulation.  It's proving that there's some type of beneficial component to the content that's being aired.  When kids programming was first on, the industry went through that decades and decades ago.

Start time: 6474.44
End time: 6490.72
Speaker: SPEAKER_01
Transcript:  It strikes me as the same thing, though, that every tech company, Facebook's doing this too now, where they throw out ideas that they have no intention of...  They're not going to change anything, but they're so terrified that Congress is going to come down on them.  It's basically hand waving, if you ask me.

Start time: 6492.21
End time: 6503.38
Speaker: SPEAKER_06
Transcript:  Yeah, I think so.  Listen, every time...  What's the workstream process?  I'd love to know exactly how this is going to work.

Start time: 6503.46
End time: 6509.34
Speaker: SPEAKER_01
Transcript:  That's very vague. What is quality time? What does that even mean?  How do you judge that?

Start time: 6509.82
End time: 6510.51
Speaker: SPEAKER_06
Transcript:  Who's defining it?

Start time: 6513.12
End time: 6541.16
Speaker: SPEAKER_01
Transcript:  The problem really is the same problem Facebook faces, which is if you...  It's algorithmically easy to do. You optimize for engagement. That's something a computer program can easily do.  Optimize for engagement.  Facebook experiences this in the news feed, and YouTube is experiencing it under the recommendation engine.  It just descends to the lowest...  I guess TV went through this too, didn't it? Lowest possible quality stuff.

Start time: 6541.80
End time: 6562.51
Speaker: SPEAKER_06
Transcript:  Right. When I think of...  When I see... First of all, my daughter doesn't...  How old is she?  She's eight. As far as she's concerned...  She's prime YouTube material.  Right. Except that we built her her own network, so she thinks she has free and unfettered access to the internet.  We built her a little Chinese internet inside of our house.  That's amazing.

Start time: 6563.56
End time: 6565.40
Speaker: SPEAKER_01
Transcript:  Baby's first Chinese internet. I like it.

Start time: 6565.42
End time: 6572.36
Speaker: SPEAKER_06
Transcript:  Yeah. No, she thinks she's ballin'.  She's got all access and surprise.

Start time: 6573.11
End time: 6580.68
Speaker: SPEAKER_01
Transcript:  How do you do this?  Clearly, things like parents thought, oh, YouTube kids, that'll be safe.  Then they found out it's creepy as hell.

Start time: 6581.42
End time: 6643.28
Speaker: SPEAKER_06
Transcript:  Now, it's not. A lot of the content...  My first window into what this world was like were unboxing videos.  I could not, for the life of me, figure out why.  That's what she wanted to watch a couple years ago.  I was just like, I don't understand this.  I think it was in the New Yorkers.  There was a doctor who had written an explanation of why these unboxing videos were so attractive to little kids.  At very early ages, their sensory systems and emotional maturity are just starting to tap into the element of surprise and delight.  Right.  But without the negative repercussions.  It's like stimulating the part of the brain that's just starting to develop.  That stupid...  What's the name of that stupid doll thing?  They come in these big spheres.  The Kinder eggs?  No, they're not Kinder eggs.

Start time: 6643.40
End time: 6646.40
Speaker: SPEAKER_01
Transcript:  There's all these YouTube unwrapping videos of these stupid things.

Start time: 6647.31
End time: 6662.34
Speaker: SPEAKER_06
Transcript:  These toys were reverse engineered.  They were a toy version of an unboxing video.  Yes.  There's like 100 different things to...  Hatchimals?  No, it's not a Hatchimal. What the hell are these stupid things?

Start time: 6662.40
End time: 6664.08
Speaker: SPEAKER_01
Transcript:  That's a horrible name, whatever that is.

Start time: 6664.88
End time: 6665.41
Speaker: SPEAKER_06
Transcript:  At any rate...

Start time: 6666.40
End time: 6667.75
Speaker: SPEAKER_01
Transcript:  They're Hatchimal collectible.

Start time: 6669.42
End time: 6672.84
Speaker: SPEAKER_06
Transcript:  Believe it or not, Hatchimals are something else that are all equally...

Start time: 6677.00
End time: 6681.40
Speaker: SPEAKER_01
Transcript:  So, the Great Firewall of Amy, how does this protect her against Hatchimals?

Start time: 6682.08
End time: 6700.51
Speaker: SPEAKER_06
Transcript:  So, originally we were like YouTube kids.  I guess I was drinking the Kool-Aid along with everybody else.  How horrible would this be?  It's safe.  Some of it is like this kind of programming, which is stimulating in a way.  There's also plenty of fine...

Start time: 6703.40
End time: 6707.14
Speaker: SPEAKER_01
Transcript:  Sorry, I don't know why my volume is so high.  I'm watching a little Hatchimals.

Start time: 6708.17
End time: 6712.32
Speaker: SPEAKER_06
Transcript:  That's actually not it. This is something worse.  Even worse?  Yeah, yeah.

Start time: 6713.01
End time: 6717.22
Speaker: SPEAKER_01
Transcript:  It's the Hatchimals unboxing.  LOL Surprise dolls.  What is it? What are they called?

Start time: 6717.98
End time: 6729.40
Speaker: SPEAKER_06
Transcript:  LOL Surprise dolls.  They are expensive. They are horrible.  But that is what was reverse engineered.  They're brilliant. That's what got reverse engineered.  Oh, I'm looking at these.

Start time: 6729.74
End time: 6734.36
Speaker: SPEAKER_01
Transcript:  Both Christy and I are quickly Googling LOL Surprise dolls.  I gotta have...

Start time: 6735.69
End time: 6748.34
Speaker: SPEAKER_06
Transcript:  So, I let her watch YouTube at the beginning and then I saw what it was.  And not all of it's bad.  Some of it's bad.  Some of it, I think, must have been what me watching Bill Nye the Science Guy was probably like to my parents.

Start time: 6748.48
End time: 6750.36
Speaker: SPEAKER_01
Transcript:  Well, that's the problem. How do you judge, right?

Start time: 6750.40
End time: 6776.40
Speaker: SPEAKER_06
Transcript:  That's the point. Bill Nye the Science Guy was...  Obviously, that was educational. That was amazing.  But for somebody who was older, it was a lot of jarring quick cuts and loud music and big graphics,  which I'm sure a lot of people railed against and older people and thought, well, this is horrible.  Much like today, we are railing against some of what we're now seeing on YouTube,  was the point that I was trying to get to 20 minutes ago.  Anyhow, we don't use LOL in the...

Start time: 6777.54
End time: 6807.40
Speaker: SPEAKER_05
Transcript:  Or actually, if not Bill Nye, I mean, because that's actually educational.  This would be more like Jim and the various other cartoons like Shia Joe.  The things that they created, like the animated show Jim, was literally created to sell a toy brand, right?  Like they created these Saturday morning cartoons with the express purpose of selling toys.  So, they did the same type of thing where they're like, okay, we're going to create this TV show  that its whole purpose is to launch a toy line.

Start time: 6807.50
End time: 6815.36
Speaker: SPEAKER_01
Transcript:  I don't know if LOL... I mean, LOL Surprise Dolls is kind of cool, although it's clearly...  It even says on the doll, unbox me.

Start time: 6816.26
End time: 6827.77
Speaker: SPEAKER_06
Transcript:  So, the whole point of it is, right, so if you watch any more of it, it is an unboxing video that is just made out of plastic.  So, the problem that I have with these is that once... It is an activity.  So, once... So...

Start time: 6829.14
End time: 6832.26
Speaker: SPEAKER_01
Transcript:  And it doesn't encourage kids to create their own YouTube channel? I mean, is that...

Start time: 6834.71
End time: 6852.12
Speaker: SPEAKER_06
Transcript:  No, my daughter doesn't want to do that. But once you've unwrapped...  There is no toy. So, the thing that you do is unbox it. You unwrap it in all these different places.  And then you're not going to... They're little tiny... You're not going to play with them.  So, all we're doing is generating a lot of waste that winds up getting thrown out.

Start time: 6853.34
End time: 6857.36
Speaker: SPEAKER_01
Transcript:  It's a ball full of plastic crap that you're going to throw out later.

Start time: 6858.53
End time: 6873.20
Speaker: SPEAKER_06
Transcript:  That's right. And they're selling an activity, right?  And these, by the way, are really expensive.  But this is somebody who reverse engineered a commercially viable internet meme, which are unboxing videos and has done quite well.

Start time: 6873.40
End time: 6875.38
Speaker: SPEAKER_00
Transcript:  There's the gauntlets or the boots or something. Yeah.

Start time: 6875.94
End time: 6887.34
Speaker: SPEAKER_06
Transcript:  These are like... These little balls that you're seeing get unwrapped. Those things are like 50 bucks.  50 bucks?  Some of them... The bigger... The larger that they are, the more expensive. These things are expensive.

Start time: 6887.60
End time: 6904.36
Speaker: SPEAKER_01
Transcript:  Yeah. Okay. But you get a doll, see? And then it dresses up.  For $50?  Like, that's...  Okay. So, this is not clearly Bill Nye, the science guy.  No, but my point was like...

Start time: 6904.64
End time: 6909.28
Speaker: SPEAKER_06
Transcript:  But we can't judge. We're a different generation.  I think so. And I think that...

Start time: 6909.40
End time: 6918.28
Speaker: SPEAKER_01
Transcript:  And the problem is your daughter in 15 years is not going to be able to talk to her peers about LOL support.  Surprise me, dolls.

Start time: 6918.52
End time: 6928.40
Speaker: SPEAKER_06
Transcript:  There is plenty of, I think, content on YouTube created by younger kids that is probably educational.  Nowhere near what Bill Nye was... Like, they're not even comparable.

Start time: 6928.54
End time: 6931.20
Speaker: SPEAKER_01
Transcript:  There is good science video on YouTube that kids watch.

Start time: 6931.40
End time: 6944.40
Speaker: SPEAKER_06
Transcript:  It's not a fair comparison. Right. So, the question is, isn't there value in watching a cartoon that is about...  Like, the reboot of She-Ra is amazing. Now, that's not on YouTube. That was on Netflix. But it's like...

Start time: 6944.50
End time: 6959.38
Speaker: SPEAKER_01
Transcript:  So, I still want to know about the great firewall of Amy. So, do you have a whitelist of things she can look at?  Or how does this work?  Do you have a Plex server where you stock it with family-approved videos?

Start time: 6960.22
End time: 6966.72
Speaker: SPEAKER_06
Transcript:  So, yes. So, we have... How much do I invite the outside world into...

Start time: 6967.56
End time: 6969.38
Speaker: SPEAKER_01
Transcript:  Well, just the general gist so somebody can duplicate this.

Start time: 6969.52
End time: 6987.72
Speaker: SPEAKER_06
Transcript:  Yeah. So, the gist is we have multiple networks that serve different purposes that are all behind different types of firewalls themselves.  You are such a geek.  Her network... The joke in our family is that my husband is hardware and I'm software.  Except when... It's a whole other thing.

Start time: 6988.66
End time: 6992.69
Speaker: SPEAKER_01
Transcript:  So, you have VLANs in your house?  Yes.  And each...

Start time: 6993.40
End time: 6999.28
Speaker: SPEAKER_06
Transcript:  We bought a 100-year-old house and the first thing we did was run conduit everywhere so that we could run our own tables.

Start time: 6999.50
End time: 7002.40
Speaker: SPEAKER_01
Transcript:  Yep. Do you have a closet? Like a wiring?

Start time: 7003.12
End time: 7007.38
Speaker: SPEAKER_06
Transcript:  We have a beautiful rack in the basement. It's gorgeous.  Amazing. It's very stunky.

Start time: 7007.48
End time: 7010.96
Speaker: SPEAKER_01
Transcript:  The great firewall of Amy has a beautiful rack in the basement.

Start time: 7011.72
End time: 7013.76
Speaker: SPEAKER_05
Transcript:  No, I'm pretty intrigued by this because...

Start time: 7014.40
End time: 7016.88
Speaker: SPEAKER_01
Transcript:  I'm making... The title is getting longer and longer and longer.

Start time: 7017.46
End time: 7022.95
Speaker: SPEAKER_05
Transcript:  No, like... I'm not a parent and I'm not going to be one. But this seems like this is the right way if you're going to do it to do it.  No, this is brilliant.

Start time: 7024.82
End time: 7042.26
Speaker: SPEAKER_01
Transcript:  But there is a risk. I have to point out there is a risk.  Because the world view is created by mom and dad and it is a limited world view.  Now, I understand she's young enough that's probably okay and it's normal.  But I also know kids who grew up and the only thing they could watch was Bible verse cartoons.

Start time: 7045.22
End time: 7066.61
Speaker: SPEAKER_06
Transcript:  No, no, no. It's not like that. It's more... We want to give her...  So yes, we've basically turned the internet off except for white labeled places where she can go.  I think that's great.  There is no YouTube kids.  At what age will you stop this though?  I don't know. As she matures. She's an only child. We're old parents. She's a fairly mature eight-year-old, but we'll see.

Start time: 7069.40
End time: 7073.40
Speaker: SPEAKER_01
Transcript:  This reminds me a lot of what Stacey Higginbotham does with her daughter, by the way.

Start time: 7073.80
End time: 7121.28
Speaker: SPEAKER_06
Transcript:  Yeah. And so Stacey and I know each other. I mean, I think that there are parents who are sort of...  I don't believe in no screen time. I believe in smart screen time.  Cultivated screen time.  That's right. She has access to all of the robots and all of the device.  We've got a house full of stuff that she can access, but she's only accessing it on her network.  Are you teaching her coding?  No, they do some of that at school, but I'll tell you what they do at school that's amazing.  They have digital literacy classes where once a week they have to learn, you know, Bobby put a photo without asking Jane.  Oh, that's really good.  That is really good.  It's part of their values class is digital values, which is amazing, but not every school does that.

Start time: 7122.24
End time: 7167.24
Speaker: SPEAKER_05
Transcript:  No, but see what I like about this, because see, my old thing, most of the parents that I've talked to who try to do similar things,  they don't go to the extreme you have, but this is smart because I always think about like, what if it were me at that age?  And if it were me at that age, I would figure out the mom and dad were censoring the Internet and I would find out how to like override the system.  But this is pretty...  She knows.  Right.  Right. No, I know.  We're not fighting it from her. She totally knows.  I'm not saying you are. What I'm saying though is that I would know and then I would be like, OK, well, then what do I need to do to take control of the router?  Exactly.  The whole thing, which at that point, I would like to think that if I were a parent, and again, I'm not one and I'm not going to be one.  But if I were, I would like to think that if my child did do that successfully, I'd be like, OK, you know what?  Congratulations. You deserve this.

Start time: 7167.40
End time: 7169.96
Speaker: SPEAKER_01
Transcript:  Yeah. You have to earn your way out of the tech prison.

Start time: 7170.48
End time: 7175.40
Speaker: SPEAKER_05
Transcript:  Right. But this is a level above and beyond like what the typical...

Start time: 7175.48
End time: 7181.24
Speaker: SPEAKER_01
Transcript:  Most people can't do this. What they do is they go out and buy the Disney Circle thing and they let Disney decide.

Start time: 7182.08
End time: 7263.52
Speaker: SPEAKER_06
Transcript:  This is the point that I've been trying to make all night.  Disney is eating it over. Yeah.  Right. So the problem is that my husband's best friend is a white hat.  You know, we run in very technical circles and so we're all on Keybase sharing ideas.  Parenting, you know, like we're in a different kind of situation than most people.  And so, yeah, we all share. We've got a Plex server that everybody shares and there is a like kid approved.  If you're seven years old, this is the stuff you get to watch.  The problem is that I'm in a rarefied group of people and I have all this additional knowledge.  And my point is like, I feel like everybody else should have access to at least the knowledge so that they can then make decisions or they can afford.  We can make it so that people can afford to buy in.  Again, this goes back to the comment somebody made about privacy and taxes and wealth.  You know, if you don't know this stuff or if you can't afford to make alternate choices or build out and provision like six different networks in your home, right,  because you can barely afford the one network that you've got access to and the standard modem and router situation,  then you're kind of left. You're left open.  And that's that's that that impacts the future generation of people who grow up.

Start time: 7264.42
End time: 7301.28
Speaker: SPEAKER_01
Transcript:  You know, this is interesting because this is the opposite of the the privacy tax for poor people.  This is this there is a technologically savvy group of adults who are creating this safe space for a very small.  You have to write a book, Amy, and I want you to call it the great firewall of thinking this could be productized like honestly, this could be something.  The problem is productized is the problem because Disney Circle is going to have a corporate bias.  I mean, I bet you anything, the new Disney streaming service will be part of the Disney circles.

Start time: 7302.66
End time: 7312.36
Speaker: SPEAKER_05
Transcript:  I call it like Synology or someone, you know what I mean?  There's there's there's there's a way to do this if you were like a server company or like Amplify or somebody.

Start time: 7312.58
End time: 7320.40
Speaker: SPEAKER_01
Transcript:  It seems to me this is the this is the growing gulf between the haves and have nots, except it's the technically literate and the technically right.

Start time: 7320.84
End time: 7355.40
Speaker: SPEAKER_06
Transcript:  And this is the problem that I have with all the people in the valley who are now like the new autism, the new like I'm on the spectrum is my kids get no screens and they don't use any of the products that I work on.  And the problem that I have with that is that they're either being dumb or they're just trying to like that's a stupid.  That doesn't make any sense.  You know, like I'm with you can't live in a world that doesn't have technology.  You're obviously tech savvy people.  Why would you just sort of go in this weird alternate?  It doesn't it's totally disingenuous or it's it's like intentionally being kind of dumb.

Start time: 7356.34
End time: 7370.40
Speaker: SPEAKER_05
Transcript:  You know, well, I think a lot of it is the same because the same people who used to like when.  You know, I was growing up and when you're growing up with parents, it would be like, oh, well, my child doesn't watch TV.  We don't have a TV in our house, which, you know, it was like, OK, good for you, I guess.

Start time: 7370.44
End time: 7376.40
Speaker: SPEAKER_00
Transcript:  But I it seems I can't even imagine Christina Warren not being able to watch TV.

Start time: 7377.36
End time: 7400.32
Speaker: SPEAKER_05
Transcript:  No, in fact, my parents know it was so funny is that my parents like limited what I could watch to a certain age, you know, and then to half an hour a night.  And then I would get in trouble and I would negotiate with them and I would say, OK, so you put me on restriction.  Fine. Melrose Place is on and I can't miss Melrose Place.  So I'll take two days if I can watch this.

Start time: 7400.40
End time: 7405.08
Speaker: SPEAKER_01
Transcript:  I would gladly trade you for a Melrose Place tonight.

Start time: 7405.74
End time: 7481.32
Speaker: SPEAKER_06
Transcript:  Those are skills that you developed in the process.  Right. Negotiation.  I mean, I'll tell you, I when I left, so the first time I was basically gone, I missed the the Clinton hearings.  I missed like the mid 90s through the mid 2000s.  I missed all that pop culture.  And it was like an alien when I came back.  And there's 10 years of my, you know, there's a big chunk of my life where I can't follow along a conversation once people like I'm not part of that collective culture in my own country.  Is that a disadvantage?  I mean, it isn't. Yeah, I mean, it is in some ways.  I miss some I miss some cues.  I mean, obviously I can follow along.  But to me, this is why you don't you can rage against the machine without cutting yourself off of the lifeblood that is the culture of the place.  Right. I like it.  So, again, it's like this.  But we don't somehow we've lost our ability to be flexible in how we think about stuff.  It's either all or nothing.  It's like and then there's plenty of parents that we go out to dinner and they plop their kids down with a and I'm sorry if people listening to this are a family like this.  But almost everybody is I should just they plop their kids down with a screen.  And that's how they do their dinner.

Start time: 7481.64
End time: 7484.38
Speaker: SPEAKER_01
Transcript:  I never judge parents because it's such a hard job.  And yeah, no.

Start time: 7484.56
End time: 7494.00
Speaker: SPEAKER_06
Transcript:  And you shouldn't.  And that's an excellent way to get trolled.  So I'm not judging.  I'm just stating.  And there are parents who do completely the opposite, which is nobody gets any screen time ever.

Start time: 7494.92
End time: 7727.20
Speaker: SPEAKER_01
Transcript:  I just give my kid a five pound bag of sugar free gummy bears.  And I said, Kid, enjoy.  Watch as much TV as you can.  And I'll see you later.  Our show today brought to you by WordPress.  I'll tell you one thing.  And I think this is true of your daughter.  Not yet.  But there at some point you the other thing you have to decide with kids is is their presence on the Internet because you can't keep a kid off the internet.  Because you can't keep a kid off or anybody off the Internet because that vacuum will be filled and just be filled by other people's posts about your child, videos about your child, name calling and all sorts of stuff.  So I always tell teenagers and I think it's right about seventh or eighth grade.  Get a website.  Start putting your best stuff on it.  This is going to be you when people Google your name.  So this is something you need to do.  And it's true for individuals.  It's true for businesses.  You need to have a website.  And that's why I always tell people about WordPress.com because it's the easiest free way to create your website right now that will become who you are.  In fact, my suggestion is, you know, if you're having a kid get their domain name now, register it.  I did for my kids.  Got a 20 year domain name registered.  I figured by then they'd know if they wanted it.  Get their email accounts.  Start this process right now.  The minute Matt Mullenweg announced it, I started using it for my blog and I've used it ever since.  I've been on WordPress.com for 12 years because let them do the hard work.  Let them do the hosting, the tech support.  You know, you probably heard about some WordPress security flaws that somebody was just releasing into the wild this week without any warning.  Hey, don't worry.  WordPress.com takes care of that stuff.  So you don't have to constantly follow all the security bullies and make sure you're safe.  You are safe.  You're on WordPress.com.  WordPress.com was started by Matt many years ago so that anyone could publish their ideas, have their voice on the web, put a portfolio up, open a store, start a blog, let people know about your business.  And it's a site that's free to start but can grow with you all the way up to an e-commerce site and some of the biggest publications in the world.  They're using WordPress.com.  No two week trials, no hidden fees.  And most importantly, you own your content forever.  Sure, it's fine for your kid to have a Facebook page or an Instagram site or a Snapchat, whatever, but it's so important for you as an individual and for every business to have one place that's yours that you own forever.  Upload anything you want.  Text, pictures, video, audio.  Download it again and get it out.  It's never trapped.  And a great customer support team.  They're not just page turners in a notebook.  They're actually WordPress lovers and users and experts.  And they're there 24 hours a day to help you, even weekends.  The WordPress platform is powerful and flexible.  It can grow as big as you need it to be, but it can start as small as you want.  Millions of people use WordPress.com every day to turn their dreams into reality.  In fact, the number that blows me away every time I read it, 33% of the entire internet is powered by WordPress.  One third of all the internet runs on the WordPress software.  The best way to do it, WordPress.com.  Go to WordPress.com slash twit.  You'll get 15% off any new plan purchase right now.  WordPress.com slash twit.  15% off any new plan purchase.  WordPress.com slash twit.  Please go there so they know you heard it here.  That helps us and we thank you WordPress for me personally.  12 years of LeoLaporte.com.  I would imagine, given how sophisticated you are with your kid, Amy,  a website would be something you would do at some point too, right?  Do you agree with what I said about you've got to put yourself out there so that you control your reputation?

Start time: 7728.20
End time: 7754.01
Speaker: SPEAKER_06
Transcript:  So I guess I'll just, we're all getting to know each other tonight.  Why not, right?  So we created a digital trust fund for her when she was born, which means that I registered her.  I've got her domain.  I've got her social media names locked up and accounts and I keep adding to it, but we don't post anything.  So she's got it when she's ready.

Start time: 7755.70
End time: 7788.82
Speaker: SPEAKER_01
Transcript:  And then the thing I wish I had thought of, of course, there was no Gmail at the time.  My kids are 24 and 27, but I love this idea.  Somebody told me about this years ago.  Maybe it was one of the geek dads.  Get a Gmail account when your baby's born.  Start sending stuff to it and then give her the keys to the account when she's 18 or 21, whenever you think is appropriate.  I think that's a, I love that idea.  That's the best baby book ever, right?  Yeah.  Technology could be, if you think about it, it could be used well.  I wish you'd write a book or something.

Start time: 7790.05
End time: 7806.00
Speaker: SPEAKER_06
Transcript:  On parenting? Are you crazy?  Years ago, no, no, like this is, there's like, I would rather talk all day long about any other hot button issue you have.  I know, it's the hardest thing in the world.  Parenting is the easiest way to invite lots of trouble into your life.

Start time: 7807.98
End time: 7819.40
Speaker: SPEAKER_01
Transcript:  You know another way? Talk about politics.  But I'm going to have to do it.  The House has voted to save net neutrality.  And the White House is of course going to veto it.

Start time: 7819.50
End time: 7821.16
Speaker: SPEAKER_06
Transcript:  I feel like I have whiplash.

Start time: 7821.68
End time: 7839.60
Speaker: SPEAKER_01
Transcript:  Yeah, this goes back and forth, except it really doesn't go back and forth.  It's really, they've, they eliminated it and now they're just fighting and fighting and fighting to get it back.  Meanwhile, states are going ahead and passing their own resolutions.  So the issue, and of course, you know, that's problematic too, because the internet is a global phenomenon.

Start time: 7841.00
End time: 7858.40
Speaker: SPEAKER_05
Transcript:  No, you're not wrong. I'm just saying, I'm glad I live in the state of Washington.  You have the best law of all, right?  Right. At least until this is all figured out, I don't have to worry about, you know, my ISP.  Being given information, regardless of what the federal statutes are.

Start time: 7858.72
End time: 7923.22
Speaker: SPEAKER_01
Transcript:  So you're going to hear from the White House when they veto it, because they will save the Internet Act.  You're going to hear stats that are essentially bogus.  Since the new rule was adopted in 2018, consumers have benefited from a greater than 35 percent increase in average fixed broadband download speeds.  And we went from 13th to sixth in the world.  By the way, even that's nothing to celebrate. In 2018, fiber was also made available in more new homes than any previous year.  Capital investment by the nation's top six Internet service providers increased $2.3 billion.  Some of this, of course, has been ongoing for years.  Anyway, there's a good Verge article that debunks a lot of these claims and it'd probably be worth reading it.  And let's not give up the fight for net neutrality, because we know that that's something has to happen.  Twenty three state attorneys general are filing lawsuits, have filed lawsuits and state by state, of course, including Washington state states are enacting their own net neutrality.

Start time: 7924.62
End time: 7955.20
Speaker: SPEAKER_06
Transcript:  So that's awesome. Yes. That means we're going to wind up with one of the one of the tech trends we talked about and modeled a couple of years ago was splinter nuts.  Splinter nuts. Yeah, it's not a I made up. But I mean, it's just again, like this is so stupid.  Like with it. Yep. The last thing you want now is to have a state by like to is to like have the states figure out how to handle this.  Absolutely. Like what does that do for business? Internet data crosses state lines. What happens to. Yeah, exactly.

Start time: 7955.44
End time: 7962.53
Speaker: SPEAKER_05
Transcript:  I mean, no, I mean, it's happening on international levels, too, right? Like with with some of the different proposals. Look at this one.  I lived in Philly for a while. Is this going to be a thing where you like?

Start time: 7964.40
End time: 7975.38
Speaker: SPEAKER_06
Transcript:  I'm on the Philly Internet. Yeah, basically.  Over to like like Texas to get slightly better. Like you're going to change your IP so that you're like 10 miles away.

Start time: 7975.60
End time: 7984.08
Speaker: SPEAKER_01
Transcript:  Now we have a good use for VPNs. No, seriously.  Servers one for each state. You choose the Internet. You not wind up happening.

Start time: 7985.76
End time: 7999.30
Speaker: SPEAKER_05
Transcript:  No, that's totally what's going to wind up happening.  Unless we can have an administration change and then roll back, you know, this stuff and the FCC can do what it should be doing, which is to, you know, like protect people and not just hand everything over to the ISPs.

Start time: 7999.88
End time: 8034.47
Speaker: SPEAKER_01
Transcript:  Although as a classic example of what could possibly go wrong, the French Internet Referral Unit has reported to archive dot org.  550 of their archived URLs as terrorist contents falsely.  I might add take down notices from the French IRU for a bunch, including, by the way, the Gutenberg Smithsonian, the Grateful Dead.  Actually, the Grateful Dead might be considered terrorists to certain government agencies.  I don't know. Crazy.

Start time: 8037.34
End time: 8045.40
Speaker: SPEAKER_06
Transcript:  What is possibly what was in that C-SPAN report? Like what was who was holding up? What post? What floor chart that was deemed?

Start time: 8047.38
End time: 8116.36
Speaker: SPEAKER_01
Transcript:  The problem is with the use laws, you don't you you get less than 24 hours to respond. The archive dot org is is is panicking.  That's totally not the way that they intended.  No, there's a one hour. I'm sorry. Did I get it? I exaggerated.  There's a one hour requirement. You have to take those URLs down immediately. You have one hour to do so.  Or what? Or somebody comes and somebody comes and hits you with a baguette. I don't know. I don't know.  I'll take that baguette. I'll take that baguette.  The French Internet Referral Unit falsely identified hundreds of URLs on archive dot org as terrorist propaganda.  The one hour requirement means, by the way, there was no one there when they came in. It was on the weekend.  It's a it's a mistake.  Blocking procedures may be implemented against us, says archive dot org, if we don't remove the content.  So they'll be blocked in France. Splinter Net.

Start time: 8118.67
End time: 8181.24
Speaker: SPEAKER_06
Transcript:  That's the in the way we do not want our we do not want a the balkanization of our of our tech and our digital infrastructure.  This gives extraordinary power to authoritarian regimes who would love to see us fighting these stupid dumb.  I mean, just these this is just again, this is like this is why regulation often doesn't work.  I mean, I understand why people why we pursue it. But in practice, it doesn't work.  And you've been arguing for it all along. No, I've not been arguing for regulation.  I'm very much I think the blunt instrument of regulation is the is the not the smart way forward.  The smart way forward is to come up with something we don't have yet, which I know is much more difficult.  But we've got to figure out a way to collaborate.  We're going to, you know, we have three like epicenters of power, at least in this country, Wall Street, D.C.  and our West Coast capital, which is the Valley slash Bellevue, you know, and we've got to figure out a way to get them to collaborate.

Start time: 8182.01
End time: 8183.12
Speaker: SPEAKER_01
Transcript:  Oh, my God, we're screwed.

Start time: 8184.10
End time: 8237.97
Speaker: SPEAKER_06
Transcript:  Well, I'm just saying, like otherwise, it's not going to happen.  No, but regulation is bad.  And this is another one of these things where like this kind of regulation, because it doesn't make sense.  Look, you just raise to us. It's crazy. It's crazy.  And France doing its own thing and all of these different companies now and countries coming up with their own policies and value statements around all different types of technology, including A.I.  There's an evangelical group of Christians who have now come up with a Christian stance on what A.I. should be.  Oh, God.  So I what the VAT would like that this is my point.  Like we're all going about this.  We're all we're all doing our own thing.  And what happens when everybody's individual interests wind up colliding, which they're going to, you know, then you wind up with a takedown notice for floor charts shown on C-SPAN.  Makes no sense.

Start time: 8239.98
End time: 8262.10
Speaker: SPEAKER_01
Transcript:  I really there's so many great stories, but we've gone way too long.  So we're going to have to wrap it up.  I really did want to talk about the Secret Service agent when they arrested that Chinese woman who had four passport notes, two, three, four phones, malware, malware, and a USB stick about the Secret Service agent who plugged it into a machine to see what was on it.  But we'll just save that for another day.

Start time: 8263.08
End time: 8264.63
Speaker: SPEAKER_06
Transcript:  Was that a male or a female agent?

Start time: 8266.75
End time: 8267.36
Speaker: SPEAKER_01
Transcript:  Does it matter?

Start time: 8267.40
End time: 8272.47
Speaker: SPEAKER_06
Transcript:  Yes, it does.  Because I bet you that person has a lot of sex without condoms.

Start time: 8275.56
End time: 8297.40
Speaker: SPEAKER_01
Transcript:  Samuel Ivanovich, who was the first person to interview Zhang at Mar-a-Lago testified that.  Oh, no, he didn't.  So he didn't do it.  Another agent put Zhang's thumb drive into his computer and immediately began to install files.  A very out of the ordinary event he had never seen happen before.

Start time: 8297.40
End time: 8298.40
Speaker: SPEAKER_05
Transcript:  What are you doing?

Start time: 8299.67
End time: 8312.38
Speaker: SPEAKER_01
Transcript:  Oh, my God.  This is from the Miami Herald.  Oh, my God.  Well, maybe we don't know.  Maybe it was a very special personal computer that wasn't connected to the Internet or something.  I mean, we can hope.

Start time: 8312.42
End time: 8324.40
Speaker: SPEAKER_05
Transcript:  But like, I meet some I arrest somebody for suspicion on something.  I'm totally just going to take their thumb drive and plug it into my computer.  Like, yeah, of course.  That's the very first thing I'm going to do.

Start time: 8324.40
End time: 8345.40
Speaker: SPEAKER_01
Transcript:  I'm like, the start immediately began to install files.  A quote very out of the ordinary event he had never seen happen before during this kind of analysis analysis.  What is the analysis?  Let's open it and see what happens.  The agent had to immediately stop the analysis to halt any further corruption of his computer.  The analysis is still ongoing, but inconclusive.

Start time: 8346.13
End time: 8350.24
Speaker: SPEAKER_06
Transcript:  I don't think they know what analysis.  I think it's your brother in law.  Your dad.  Your dad.

Start time: 8351.16
End time: 8353.28
Speaker: SPEAKER_05
Transcript:  All right.  My dad's totally my dad.

Start time: 8354.41
End time: 8363.23
Speaker: SPEAKER_01
Transcript:  Well, let's see what's on it.  Let's see what happens.  You know, you can turn off auto run.  I'm just saying that EXE.

Start time: 8364.40
End time: 8367.04
Speaker: SPEAKER_06
Transcript:  What does that mean?  Excellent.  All right.

Start time: 8370.40
End time: 8398.20
Speaker: SPEAKER_01
Transcript:  Enough. Enough.  I am so happy to have you two on here.  We we decide normally we have four person panel.  We said no, no.  Christina Warren, Amy Webb.  That's all we need.  That's a show.  And I absolutely prove true.  Christina is the greatest.  We've known her since the good old days of Mashable Senior Developer Advocate Cloud Developer Advocate at Microsoft.  Now, catch her at the Ignite Tour coming soon to a town near you.

Start time: 8398.85
End time: 8399.13
Speaker: UNKNOWN
Transcript:  Yes.

Start time: 8399.82
End time: 8401.91
Speaker: SPEAKER_01
Transcript:  And on Channel 9, you do stuff there too, right?  I sure do.

Start time: 8403.42
End time: 8407.26
Speaker: SPEAKER_05
Transcript:  YouTube.com slash Microsoft Developer.  Nice.  So you are on YouTube.

Start time: 8407.40
End time: 8409.18
Speaker: SPEAKER_06
Transcript:  You are a YouTube star.  She's a YouTube star.

Start time: 8410.20
End time: 8415.30
Speaker: SPEAKER_05
Transcript:  I'm a YouTube star for, you know, geeks, nerds, the best, the best audience.

Start time: 8415.40
End time: 8425.40
Speaker: SPEAKER_01
Transcript:  Without a doubt.  You are an influencer.  She's an influencer.  Of course she is.  And always glad to have you.  Thank you for keeping the lights on late at night in Redmond.

Start time: 8426.43
End time: 8427.18
Speaker: UNKNOWN
Transcript:  Thank you for having me.

Start time: 8427.86
End time: 8430.40
Speaker: SPEAKER_01
Transcript:  Are you rushing off to watch Game of Thrones?  We have how many hours?

Start time: 8430.58
End time: 8432.89
Speaker: SPEAKER_06
Transcript:  Yes.  Happy Game of Thrones to you.  We have an hour.

Start time: 8433.40
End time: 8437.14
Speaker: SPEAKER_01
Transcript:  Now, they put it out on, if you have HBO Go, you can watch it early, right?

Start time: 8437.74
End time: 8440.32
Speaker: SPEAKER_04
Transcript:  That's what I'm, which is what I'm doing exactly.  The East Coast.

Start time: 8440.50
End time: 8442.52
Speaker: SPEAKER_01
Transcript:  So in one hour, and you could watch it right now, Amy.  Yeah.

Start time: 8443.76
End time: 8447.65
Speaker: SPEAKER_06
Transcript:  In fact, if I don't leave pretty soon, I'm going to get yelled at.  Are you having a party?

Start time: 8449.31
End time: 8504.09
Speaker: SPEAKER_01
Transcript:  A party of two.  Is your, you don't let your eight-year-old watch Game of Thrones.  It's not on the, it's blocked by the great firewall of Amy, you know, provision.  No, not provision.  No, I don't blame you.  All right.  All right.  Amy Webb, the author of The Big Nine.  You got to read this great book and always welcome on our, on our air.  You can find out more at the Future Today Institute or just go to amywebb.io.  And I'm glad to know you're on Keybase.  I did not know that.  I will, I will figure out your handle and follow you.  Good luck.  I am.  Yeah.  I searched for Amy Webb.  I didn't find it.  You could follow me.  I'm Leo Laporte.  Keybase.  Everybody who's a geek should know about keybase.io.  It's free and it's an amazing service.  I use their encrypted Git for all my private stuff.  It's fantastic and great.  Yeah, that's awesome.  It keeps your PGPs, keys and all that stuff.  Thank you, Amy.  Thank you, Christina.  What a great show to come back to.  This is actually better than Kawhi.  No, it's not.

Start time: 8505.85
End time: 8509.36
Speaker: SPEAKER_05
Transcript:  Thank you.  Thank you for saying that.  It's better than the luau we saw last night.

Start time: 8509.40
End time: 8584.26
Speaker: SPEAKER_01
Transcript:  I'll say that.  How about that?  Thank you everybody.  We do Twitter every Sunday afternoon, around about 2 15, right after the radio show.  That's 2 15 Pacific 5 15 Eastern Time.  That's about, oh, I don't know, 21 20 UTC, something like that.  Come by and watch twit.tv slash live.  There's a live audio and video stream so you can watch or listen.  If you're doing that, chat room is a great place to hang out.  They're watching and listening live too.  It's irc.twit.tv.  Everybody, even Amy's daughter will feel safe there.  It should be part.  It should be inside the great firewall of Amy.  If you want to watch us in studio, you're more than welcome to do so.  We had a great live audience from all over the world from Australia and, oh, I don't know, let's see, Connecticut, Pennsylvania, Washington, D.C. and Mountain View.  Please come and join us.  Just email tickets at twit.tv.  We'll put a chair out for you.  You can also get on demand versions of everything we do at our website, twit.tv or best idea yet, subscribe in your favorite podcast application.  That way you won't miss an episode.  You'll have in time for your Monday commute.  Enjoy Game of Thrones, everybody.  We'll see you next time.  Thank you.  Another twit is in the can.

