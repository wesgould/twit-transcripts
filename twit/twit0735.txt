;FFMETADATA1
title=I've Got a Chatroom in My Thigh
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=735
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.500]   Coming up, we got a great Twit panel for you.
[00:00:02.500 --> 00:00:05.200]   Security, experts, Seth Rosenblatt is here,
[00:00:05.200 --> 00:00:08.100]   along with Jason Heiner from CBS Interactive.
[00:00:08.100 --> 00:00:10.700]   My good buddy, Harry McCracken from Fast Company.
[00:00:10.700 --> 00:00:13.900]   We will talk about the Apple Google spat,
[00:00:13.900 --> 00:00:16.200]   a little shade being thrown.
[00:00:16.200 --> 00:00:20.400]   Facebook leaks 400 million users' phone numbers.
[00:00:20.400 --> 00:00:23.300]   Carson says, "That's no different than the phone book."
[00:00:23.300 --> 00:00:26.500]   And the Samsung Fold, are you ready to spend two grand?
[00:00:26.500 --> 00:00:28.300]   Maybe it won't be as expensive as you thought.
[00:00:28.300 --> 00:00:29.700]   It's all coming up next.
[00:00:29.700 --> 00:00:30.700]   On Twit.
[00:00:30.700 --> 00:00:35.000]   Netcast, you love.
[00:00:35.000 --> 00:00:37.000]   From people you trust.
[00:00:37.000 --> 00:00:42.200]   This is Twit.
[00:00:42.200 --> 00:00:52.200]   This is Twit, this week in tech,
[00:00:52.200 --> 00:00:57.900]   Episode 735, recorded Sunday, September 8th, 2019.
[00:00:57.900 --> 00:00:59.900]   I'm going to chat with my Thigh.
[00:00:59.900 --> 00:01:02.800]   This week, Attack is brought to you by Mint Mobile.
[00:01:02.800 --> 00:01:05.700]   They provide the same premium network coverage you're used to,
[00:01:05.700 --> 00:01:09.300]   but at a fraction of the cost, because everything is online.
[00:01:09.300 --> 00:01:12.500]   Mint Mobile makes it easy to cut your wireless bill down
[00:01:12.500 --> 00:01:15.900]   at just $15 a month with their three-month introductory plan.
[00:01:15.900 --> 00:01:21.600]   Plus, get the plan shipped to your door for free at Mint Mobile.com/Twit.
[00:01:21.600 --> 00:01:26.600]   And by Casper, a sleep brand that continues to revolutionize its line of products,
[00:01:26.600 --> 00:01:29.300]   introducing the new Casper Hybrid mattress,
[00:01:29.300 --> 00:01:33.000]   featuring their award-winning foam layers now combined with springs,
[00:01:33.000 --> 00:01:36.100]   get $100 off towards select mattresses
[00:01:36.100 --> 00:01:38.700]   by visiting Casper.com/Twit1
[00:01:38.700 --> 00:01:41.600]   and using the promo code "Twit1" at checkout.
[00:01:41.600 --> 00:01:44.100]   And by Wasabi Hot Cloud Storage.
[00:01:44.100 --> 00:01:46.500]   Thinking about moving your data storage to the cloud,
[00:01:46.500 --> 00:01:49.200]   Wasabi is enterprise-class cloud storage
[00:01:49.200 --> 00:01:53.000]   at 1/5 the price of Amazon S3 and up to six times faster,
[00:01:53.000 --> 00:01:56.200]   with no hidden fees for egress or API requests.
[00:01:56.200 --> 00:02:00.500]   Calculate your savings and try Wasabi with free unlimited storage for a month
[00:02:00.500 --> 00:02:03.600]   at Wasabi.com/Twit.
[00:02:03.600 --> 00:02:05.400]   And by ZipRecruiter.
[00:02:05.400 --> 00:02:08.400]   Hiring is challenging, but there's one place you can go
[00:02:08.400 --> 00:02:10.400]   where hiring is simple and smart.
[00:02:10.400 --> 00:02:12.300]   That place is ZipRecruiter.
[00:02:12.300 --> 00:02:15.400]   Where growing businesses connect to qualified candidates,
[00:02:15.400 --> 00:02:19.200]   try it free at zipprecruiter.com/Twit.
[00:02:19.200 --> 00:02:23.900]   It's time for "Twit" this week in Tech,
[00:02:23.900 --> 00:02:26.200]   the show where we cover the week's Tech News.
[00:02:26.200 --> 00:02:29.200]   We've assembled a great panel for you.
[00:02:29.200 --> 00:02:30.500]   It was going to all be in studio,
[00:02:30.500 --> 00:02:33.700]   but Jason Heiner got caught in airline delays.
[00:02:33.700 --> 00:02:36.000]   Sorry to hear that, Jason, but he's still with us.
[00:02:36.000 --> 00:02:38.000]   Fias, kai-pai, Jason.
[00:02:38.000 --> 00:02:39.500]   Hey, glad to be here.
[00:02:39.500 --> 00:02:43.300]   Officially now, editorial director at CNET.
[00:02:43.300 --> 00:02:44.100]   That's right.
[00:02:44.100 --> 00:02:46.500]   We've promoted you upstairs.
[00:02:46.500 --> 00:02:48.700]   Yes, sorta.
[00:02:48.700 --> 00:02:50.800]   He's at the 64th floor of the Marriott.
[00:02:50.800 --> 00:02:51.800]   That's why.
[00:02:51.800 --> 00:02:52.800]   [laughter]
[00:02:52.800 --> 00:02:55.200]   Great to have you.
[00:02:55.200 --> 00:02:56.400]   Always a pleasure.
[00:02:56.400 --> 00:02:58.800]   Also joining us in studio, Harry McCracken.
[00:02:58.800 --> 00:03:01.100]   Great to have the technologizer on site.
[00:03:01.100 --> 00:03:01.800]   Hey, Leo.
[00:03:01.800 --> 00:03:04.000]   Tech Editor at Fast Company.
[00:03:04.000 --> 00:03:07.300]   And look who the cat drug in.
[00:03:07.300 --> 00:03:09.200]   Seth Rosenblatt is also here.
[00:03:09.200 --> 00:03:11.400]   Seth's at the Parallax.com now,
[00:03:11.400 --> 00:03:13.500]   formerly at CNET, the Dash Parallax.
[00:03:13.500 --> 00:03:14.500]   Great to meet you.
[00:03:14.500 --> 00:03:15.700]   Seth, thanks for coming.
[00:03:15.700 --> 00:03:16.200]   Thank you.
[00:03:16.200 --> 00:03:19.900]   He gave me his 2FA FTW sticker.
[00:03:19.900 --> 00:03:21.700]   I couldn't agree with that more.
[00:03:21.700 --> 00:03:22.200]   Right.
[00:03:22.200 --> 00:03:25.600]   And as a result, my Firefox said,
[00:03:25.600 --> 00:03:28.000]   "I need your Yuba key before I'm going to go on."
[00:03:28.000 --> 00:03:30.000]   It's nice that they're doing that.
[00:03:30.000 --> 00:03:31.500]   Yeah, it took him a while.
[00:03:31.500 --> 00:03:33.500]   It took him for its word.
[00:03:33.500 --> 00:03:38.000]   Actually, let's talk security since we have Seth here.
[00:03:38.000 --> 00:03:40.800]   Alex Stamos on Twitter this week.
[00:03:40.800 --> 00:03:42.700]   Apple posted a response to,
[00:03:42.700 --> 00:03:47.300]   I guess we should go back in time and talk about Google's zero-day blog,
[00:03:47.300 --> 00:03:51.600]   publishing the story of an exploit, an iOS exploit,
[00:03:51.600 --> 00:03:53.800]   that they uncovered in February,
[00:03:53.800 --> 00:03:57.400]   responsibly notified Apple about Apple fixed within a week.
[00:03:57.400 --> 00:03:58.900]   Back in February,
[00:03:58.900 --> 00:04:02.100]   Google was a little coy in their original post.
[00:04:02.100 --> 00:04:04.700]   They said they wouldn't say who the exploit took advantage of it.
[00:04:04.700 --> 00:04:07.600]   But they called it a mass exploit
[00:04:07.600 --> 00:04:10.500]   because it took advantage of a variety,
[00:04:10.500 --> 00:04:13.200]   it was really a complex attack
[00:04:13.200 --> 00:04:19.100]   that took advantage of a variety of zero days in iOS 10, 11, 12,
[00:04:19.100 --> 00:04:26.200]   four years worth of iOS's and was on websites.
[00:04:26.200 --> 00:04:31.700]   So everybody who visited the website potentially with an iPhone was potentially hacked.
[00:04:31.700 --> 00:04:38.200]   This project zero blog talked about how anybody in a sensitive subgroup
[00:04:38.200 --> 00:04:41.600]   that might be under attack from a government would be,
[00:04:41.600 --> 00:04:44.800]   you know, this would be a serious cause for concern.
[00:04:44.800 --> 00:04:47.400]   Google was not saying who it was,
[00:04:47.400 --> 00:04:50.300]   but shortly thereafter, TechCrunch came out with a story.
[00:04:50.300 --> 00:04:54.300]   I suspect they got briefed by, on background by Google,
[00:04:54.300 --> 00:04:57.100]   that it was the Uighurs Muslim majority in China.
[00:04:57.100 --> 00:04:59.200]   It was the Chinese government that had perpetrated the track,
[00:04:59.200 --> 00:05:02.200]   which kind of makes sense because it's such a complicated attack.
[00:05:02.200 --> 00:05:03.900]   It's usually nation states.
[00:05:03.900 --> 00:05:05.900]   And usually nation states use these zero days,
[00:05:05.900 --> 00:05:08.700]   which are very millions of dollars often to get.
[00:05:08.700 --> 00:05:12.900]   They use them in targeted attacks on dissidents and individuals.
[00:05:12.900 --> 00:05:16.000]   But in a way, this was a targeted attack.
[00:05:16.000 --> 00:05:21.400]   It was on eight different sites that were, you know, Uighur News,
[00:05:21.400 --> 00:05:24.400]   you know, sites that Uighurs would be likely to visit.
[00:05:24.400 --> 00:05:27.300]   Although anybody who visited them from anywhere in the world
[00:05:27.300 --> 00:05:29.500]   would have been open to this attack.
[00:05:29.500 --> 00:05:37.500]   Apple wrote a response kind of slamming Google saying it wasn't an en masse attack.
[00:05:37.500 --> 00:05:41.300]   You know, so now that you know the background,
[00:05:41.300 --> 00:05:44.400]   let me read you the response, the Twitter response from Alex Stamos
[00:05:44.400 --> 00:05:48.400]   is former security officer at Yahoo, then at Facebook.
[00:05:48.400 --> 00:05:51.500]   He now teaches security at Stanford.
[00:05:51.500 --> 00:05:53.800]   He says he's a recovered CISO.
[00:05:53.800 --> 00:05:57.600]   Apple's response to the worst known iOS attack in history
[00:05:57.600 --> 00:06:02.400]   should be graded somewhere between disappointing and disgusting.
[00:06:02.400 --> 00:06:06.100]   First off, disputing Google's correct use of indiscriminate
[00:06:06.100 --> 00:06:08.300]   when describing a watering hole attack.
[00:06:08.300 --> 00:06:12.100]   That is an attack on websites that attacks anybody who visits the website.
[00:06:12.100 --> 00:06:12.900]   Smacks up.
[00:06:12.900 --> 00:06:16.000]   Well, it's okay. It didn't hit white people.
[00:06:16.000 --> 00:06:17.400]   That might be going a little far.
[00:06:17.400 --> 00:06:22.200]   The use of multiple exploits against an oppressed minority in authoritarian state
[00:06:22.200 --> 00:06:25.100]   makes the likely outcomes terrible.
[00:06:25.100 --> 00:06:27.500]   It's possible this data contributed to real people being
[00:06:27.500 --> 00:06:29.700]   reeducated or even executed.
[00:06:29.700 --> 00:06:32.400]   Even if we accept Apple's framing that exploiting Uighurs
[00:06:32.400 --> 00:06:35.300]   isn't as big a deal as Google makes it out to be.
[00:06:35.300 --> 00:06:38.800]   They have no idea whether these exploits were used by the People's Republic of China
[00:06:38.800 --> 00:06:40.600]   and more targeted situations.
[00:06:40.600 --> 00:06:45.200]   Dismissing such a possibility out of hand is extremely risky.
[00:06:45.200 --> 00:06:51.300]   Then accuses Apple of not mentioning China because of the leverage China has
[00:06:51.300 --> 00:06:53.600]   over the world's most valuable public company.
[00:06:53.600 --> 00:06:57.800]   To be fair, Google's post also didn't mention China.
[00:06:57.800 --> 00:07:02.300]   But finally, the pivot to Apple's arrogant marketing is not only tone deaf,
[00:07:02.300 --> 00:07:04.300]   but really rings hollow to the security community
[00:07:04.300 --> 00:07:07.400]   when Google did all the heavy lifting here.
[00:07:07.400 --> 00:07:11.900]   I'm guessing we won't hear Tim talk about how they're going to do better on stage next week.
[00:07:11.900 --> 00:07:15.400]   Since you cover, security will start with you, Seth.
[00:07:15.400 --> 00:07:16.400]   Sure.
[00:07:16.400 --> 00:07:17.900]   That was a lot of facts.
[00:07:17.900 --> 00:07:18.900]   Yes.
[00:07:18.900 --> 00:07:20.400]   There's a lot in there.
[00:07:20.400 --> 00:07:22.900]   Now I want to know what you think about it.
[00:07:22.900 --> 00:07:24.900]   First of all, was Alex Wright?
[00:07:24.900 --> 00:07:26.900]   Do I think Alex is right in this case?
[00:07:26.900 --> 00:07:27.900]   Absolutely.
[00:07:27.900 --> 00:07:28.900]   Absolutely.
[00:07:28.900 --> 00:07:36.900]   I think, you know, security often winds up being a sort of the brakes on a car.
[00:07:36.900 --> 00:07:46.400]   For tech, security plays this role where it can allow tech to proceed even faster in some cases,
[00:07:46.400 --> 00:07:48.900]   but you still have to understand how to use the brakes.
[00:07:48.900 --> 00:07:55.900]   And I think it's long overdue that companies like Apple and Google, of course,
[00:07:55.900 --> 00:08:00.900]   you know, have their sort of, you know, come to the guru moment
[00:08:00.900 --> 00:08:03.900]   and start taking this stuff more seriously.
[00:08:03.900 --> 00:08:08.900]   And one of the challenges is that it requires the security teams from these different companies
[00:08:08.900 --> 00:08:11.900]   to interact more and better.
[00:08:11.900 --> 00:08:18.900]   And so in a situation like this where Google has an in-house sort of skunkworks team
[00:08:18.900 --> 00:08:28.900]   that has been for years finding zero days and giving the organizations that own those zero days
[00:08:28.900 --> 00:08:32.900]   relatively short amount of time to fix them, 90 days in most cases, I think.
[00:08:32.900 --> 00:08:33.900]   That's the normal time though.
[00:08:33.900 --> 00:08:35.900]   That's what they call responsible disclosure.
[00:08:35.900 --> 00:08:40.900]   It's a little bit faster and it sort of depends on really what it is that's being disclosed.
[00:08:40.900 --> 00:08:42.900]   Sometimes it's not easy to fix in that case.
[00:08:42.900 --> 00:08:44.900]   In this case, seven days, Apple fixed it.
[00:08:44.900 --> 00:08:46.900]   Apple did fix it, which is great.
[00:08:46.900 --> 00:08:56.900]   But having more transparency about the process and having more awareness among the people who are potentially being targeted.
[00:08:56.900 --> 00:09:01.900]   And I think one of the points that Alex makes that's really key here is that this is thought
[00:09:01.900 --> 00:09:09.900]   to have been used against the Uighurs but for one thing being shuttled off into concentration camps
[00:09:09.900 --> 00:09:10.900]   and Chinese.
[00:09:10.900 --> 00:09:11.900]   It's a nasty situation.
[00:09:11.900 --> 00:09:12.900]   It's very bad.
[00:09:12.900 --> 00:09:14.900]   Because of their religion basically.
[00:09:14.900 --> 00:09:15.900]   Sure.
[00:09:15.900 --> 00:09:21.900]   And we don't know where else China has been using this exploit and we don't know if anyone else has been using them either.
[00:09:21.900 --> 00:09:26.900]   Did Apple downplay the seriousness of this exploit?
[00:09:26.900 --> 00:09:29.900]   From their statement, it looks like they did.
[00:09:29.900 --> 00:09:38.900]   One of the things they were upset about is that Google implied that they said Google's post issued six months after Iowa's patches were released.
[00:09:38.900 --> 00:09:45.900]   It creates the false impression of mass exploitation to monitor the private activities of entire populations in real time.
[00:09:45.900 --> 00:09:46.900]   This was never the case.
[00:09:46.900 --> 00:09:52.900]   I think Apple was at great pains to tell the world you're not being attacked.
[00:09:52.900 --> 00:09:59.900]   Which I understand but at the same time downplaying the real genuine danger to the people who were being attacked.
[00:09:59.900 --> 00:10:00.900]   Harry?
[00:10:00.900 --> 00:10:01.900]   Yeah, I mean, well, Apple has a huge amount.
[00:10:01.900 --> 00:10:03.900]   Put the microphone in front.
[00:10:03.900 --> 00:10:04.900]   Yes, good idea.
[00:10:04.900 --> 00:10:05.900]   Get you on like.
[00:10:05.900 --> 00:10:13.900]   Apple has a huge amount riding on the notion that the iPhone takes privacy more seriously than any other.
[00:10:13.900 --> 00:10:16.900]   Yeah, they're the ones who are saying we're the most secure planet.
[00:10:16.900 --> 00:10:30.900]   They literally run ads about this stuff and so they're not surprisingly really sensitive about stuff that might lead average consumers to believe that they aren't taking this seriously or sleep at the wheel.
[00:10:30.900 --> 00:10:36.900]   It feels like you can read that into the comments they put out.
[00:10:36.900 --> 00:10:41.900]   Apple accused Google of stoking fear among all iPhone users that their devices have been compromised.
[00:10:41.900 --> 00:10:42.900]   This was never the case.
[00:10:42.900 --> 00:10:54.900]   And it's possible for something to be really serious even if it is likely that average consumers in the US were never targeted.
[00:10:54.900 --> 00:11:05.900]   And I think also the fact that it's Google who discovered this and told the world about it probably plays a part that this was discovered by Apple's principal competitor.
[00:11:05.900 --> 00:11:12.900]   And there's absolutely no reason to think that the people at Google who discovered this have any vested interest in it.
[00:11:12.900 --> 00:11:13.900]   No, I think.
[00:11:13.900 --> 00:11:17.900]   Look bad, but I imagine that played into Apple's reaction to it as well.
[00:11:17.900 --> 00:11:20.900]   Yeah, well, in fact, on MacPray quickly we talked about it.
[00:11:20.900 --> 00:11:23.900]   And Renee Richie said this is an attack by Google.
[00:11:23.900 --> 00:11:27.900]   They conspicuously admit other Google vulnerabilities.
[00:11:27.900 --> 00:11:29.900]   But this wasn't about that.
[00:11:29.900 --> 00:11:32.900]   This was a standard kind of blog post about an exploit.
[00:11:32.900 --> 00:11:36.900]   And then another thing is the fact that this involves China.
[00:11:36.900 --> 00:11:40.900]   This incredibly important market for Apple.
[00:11:40.900 --> 00:11:46.900]   But this all boils down to a human rights issue.
[00:11:46.900 --> 00:12:01.900]   And I would I would like to see Apple acknowledge that in this response and having acknowledged that they could point out that this was pretty much targeted and does not mean that the iPhone is broken.
[00:12:01.900 --> 00:12:05.900]   Is iPhone as a platform security wise?
[00:12:05.900 --> 00:12:13.900]   That's I mean, that's a pretty far bridge to take that iPhone security is broken.
[00:12:13.900 --> 00:12:29.900]   But for consumers to think that whether you're consumers in in, you know, first world countries or whether your consumer is using, you know, a third hand iPhones to think that because you're on an iPhone you're secure is also nonsense.
[00:12:29.900 --> 00:12:36.900]   And the marketing of security has been, you know, at Apple has been, this is because we're privacy forward.
[00:12:36.900 --> 00:12:38.900]   We're also security forward.
[00:12:38.900 --> 00:12:41.900]   And that hasn't always been the case.
[00:12:41.900 --> 00:12:44.900]   And it's a it's misleading to consume.
[00:12:44.900 --> 00:12:55.900]   It would be more accurate for Apple and everybody else to say, if you're a target, especially if you're a target of a nation state, there's no you're pretty vulnerable no matter what device you're using.
[00:12:55.900 --> 00:12:58.900]   You shouldn't say, oh, I'm just going to use an iPhone and I have to worry, right, Jason?
[00:12:58.900 --> 00:13:03.900]   Yeah, certainly the this is also similar argument, like with Mac, right?
[00:13:03.900 --> 00:13:04.900]   There are a lot of people that went that.
[00:13:04.900 --> 00:13:05.900]   Yeah, I use the Mac.
[00:13:05.900 --> 00:13:06.900]   I don't have to worry about viruses.
[00:13:06.900 --> 00:13:08.900]   I don't have to worry about malware.
[00:13:08.900 --> 00:13:09.900]   Right.
[00:13:09.900 --> 00:13:12.900]   If you are targeted, you know, you are vulnerable.
[00:13:12.900 --> 00:13:15.900]   All of these platforms have zero days.
[00:13:15.900 --> 00:13:17.900]   They all have vulnerabilities.
[00:13:17.900 --> 00:13:27.900]   And now the level of attacks that can, you know, access devices and target people is, you know,
[00:13:27.900 --> 00:13:36.900]   is are so sophisticated and so powerful that it's a very vulnerable place to be for all of us.
[00:13:36.900 --> 00:13:45.900]   And journalists, I want to point out to increasingly being targeted for these kinds of things.
[00:13:45.900 --> 00:13:53.900]   And so it's that does make it a scary time and a time where you have to be more careful than ever and more thoughtful than ever.
[00:13:53.900 --> 00:14:04.900]   What about Alex's, is Stamos's implication, Jason, that Apple downplayed it because they have to be careful with China, which is a very important market for them.
[00:14:04.900 --> 00:14:06.900]   Do you think that's the case?
[00:14:06.900 --> 00:14:07.900]   Yeah, I don't know.
[00:14:07.900 --> 00:14:16.900]   It's so hard to say that there's a lot of he said, I guess he said he said he said he said they said they said there's no women involved at all.
[00:14:16.900 --> 00:14:20.900]   Yeah, yeah, a lot of they said they said in this thing.
[00:14:20.900 --> 00:14:26.900]   You know, it's hard to find people's motivations. And there's not a whole lot of bread comes that leads to one.
[00:14:26.900 --> 00:14:33.900]   Yeah, that's fair. I wouldn't want to impune Project Zero's motivations, but I wouldn't want to impune Apple's either.
[00:14:33.900 --> 00:14:34.900]   Agreed.
[00:14:34.900 --> 00:14:45.900]   I think Apple's real point with this release was to reassure the general public that you're not being attacked, but it was a little tone deaf because a minority was being attacked and life was at risk.
[00:14:45.900 --> 00:14:47.900]   And I think Stamos does have a good point now.
[00:14:47.900 --> 00:14:52.900]   It doesn't seem like Apple has been disingenuous or that they don't have something something.
[00:14:52.900 --> 00:14:53.900]   They weren't underplaying it.
[00:14:53.900 --> 00:15:03.900]   But it does kind of, it is a good example of how security and marketing are all mashed together and difficult to separate.
[00:15:03.900 --> 00:15:11.900]   I think that was actually Stamos's original point, which is don't let your mark on guys undermine the good work your security guys are doing.
[00:15:11.900 --> 00:15:12.900]   Right.
[00:15:12.900 --> 00:15:18.900]   Yeah.
[00:15:18.900 --> 00:15:23.900]   It, Melanie Ensign tweeted in response to something that Stamos wrote.
[00:15:23.900 --> 00:15:27.900]   Stamos said, I've worked for companies that took too long to publicly address their responsibilities.
[00:15:27.900 --> 00:15:30.900]   This is not the path you want to take Facebook.
[00:15:30.900 --> 00:15:34.900]   That's why he left Facebook, but also Yahoo.
[00:15:34.900 --> 00:15:36.900]   And he left Yahoo.
[00:15:36.900 --> 00:15:37.900]   That's right.
[00:15:37.900 --> 00:15:38.900]   And he left Yahoo.
[00:15:38.900 --> 00:15:40.900]   He's worked for two of them for reasons.
[00:15:40.900 --> 00:15:46.900]   He says, Apple does some incredible security work, but this kind of legal comms driven response can undermine that work.
[00:15:46.900 --> 00:15:47.900]   Right.
[00:15:47.900 --> 00:15:50.900]   And he's saying this to Apple employees, demand better.
[00:15:50.900 --> 00:15:56.900]   And I think that's a really important point that for one thing, there's not going to be a lot of external forces.
[00:15:56.900 --> 00:16:00.900]   I think that are going to be able to change how companies operate in this instance.
[00:16:00.900 --> 00:16:07.900]   It has to be coming from the inside and it has to be coming from the security teams saying to the rest of the company, hey, stop screwing us.
[00:16:07.900 --> 00:16:23.900]   And then Melanie, who worked at Facebook with Alex, and I think she's now at Uber, is a great security comms person, said, you know, the problem in Facebook's case was that the legal comms response was actually a policy political response.
[00:16:23.900 --> 00:16:24.900]   Right.
[00:16:24.900 --> 00:16:25.900]   And I think that's what's happening here.
[00:16:25.900 --> 00:16:26.900]   That's what's happening to Apple as well.
[00:16:26.900 --> 00:16:34.900]   It's policy political and it's concern about legal issues, not about the security issues that are really at stake here.
[00:16:34.900 --> 00:16:39.900]   What's the takeaway for everybody who uses iPhones?
[00:16:39.900 --> 00:16:41.900]   It's throw them out.
[00:16:41.900 --> 00:16:44.900]   That's exactly what Apple is worried that you might sell.
[00:16:44.900 --> 00:16:46.900]   No, it's no different.
[00:16:46.900 --> 00:16:49.900]   Your security is no less than it was before this happened, right?
[00:16:49.900 --> 00:16:50.900]   Yeah.
[00:16:50.900 --> 00:16:52.900]   Be more careful than ever, right?
[00:16:52.900 --> 00:16:54.900]   Two factor authentication.
[00:16:54.900 --> 00:16:57.900]   Be real mindful about apps that you install.
[00:16:57.900 --> 00:17:01.900]   Look at the permissions that those apps are taking.
[00:17:01.900 --> 00:17:03.900]   Some of them are pretty egregious.
[00:17:03.900 --> 00:17:09.900]   And the scary thing is some of them are apps that are supposed to help you with security.
[00:17:09.900 --> 00:17:14.900]   We just published an article this past week about five reasons why you should never use a free VPN.
[00:17:14.900 --> 00:17:28.900]   A lot of the free VPN apps out there that people are downloading, right, to help protect their security are they can end up getting you infected with malware and spyware and tracking software and all kinds of things.
[00:17:28.900 --> 00:17:36.900]   So unfortunately, it's a tough time to be a consumer and to have to deal with all of the security and privacy issues out there.
[00:17:36.900 --> 00:17:39.900]   And this is going to continue to multiply over the next few years.
[00:17:39.900 --> 00:17:53.900]   So all of us have an important job to do and helping whatever ways we can to inform the public to help them find easy ways and on ramps to protect themselves in ways that aren't going to be too onerous.
[00:17:53.900 --> 00:17:59.900]   Because we know if it's the thing with security, if it's too onerous, people just opt out and they won't do it and then they end up vulnerable.
[00:17:59.900 --> 00:18:03.900]   I'm going to ask Seth, you cover security, honestly.
[00:18:03.900 --> 00:18:04.900]   Yeah, a little.
[00:18:04.900 --> 00:18:11.900]   Is it even if you really wanted to be secure and private, you probably wouldn't use a smartphone at all.
[00:18:11.900 --> 00:18:12.900]   Right?
[00:18:12.900 --> 00:18:13.900]   I mean, it's a...
[00:18:13.900 --> 00:18:14.900]   You wouldn't use any of this stuff.
[00:18:14.900 --> 00:18:17.900]   No, you wouldn't be on the internet.
[00:18:17.900 --> 00:18:27.900]   So, Steve Kipson always says, "If you want a private conversation, go out in a field. In fact, both of you should be stripped naked and whisper into one another's ear."
[00:18:27.900 --> 00:18:28.900]   That sounds horrible.
[00:18:28.900 --> 00:18:29.900]   Doesn't sound like fun.
[00:18:29.900 --> 00:18:30.900]   You had Bernie in the past week?
[00:18:30.900 --> 00:18:31.900]   Yeah, right.
[00:18:31.900 --> 00:18:32.900]   Sounds like Bernie missed.
[00:18:32.900 --> 00:18:33.900]   But it's true.
[00:18:33.900 --> 00:18:37.900]   I mean, the only real private communication is one that is so...
[00:18:37.900 --> 00:18:45.900]   Okay, so this is actually the other... the flip side of this issue in talking about security is how do we minimize fun?
[00:18:45.900 --> 00:18:46.900]   How do we talk about these issues?
[00:18:46.900 --> 00:18:47.900]   Ah, I like that.
[00:18:47.900 --> 00:18:56.900]   In ways that are serious and valuable, but do not spread unnecessary fear and uncertainty and doubt.
[00:18:56.900 --> 00:18:58.900]   I like to think of it as doom, but that's just me.
[00:18:58.900 --> 00:18:59.900]   Doom's good, too.
[00:18:59.900 --> 00:19:11.900]   So in educating the public about what's happening with security and privacy and what they need to know, and not spreading fun is a very fine line to tread.
[00:19:11.900 --> 00:19:15.900]   Well, and reasonable parties can disagree about what is fun and what's not sure.
[00:19:15.900 --> 00:19:17.900]   Absolutely, right?
[00:19:17.900 --> 00:19:24.900]   But one of the things that we talk a lot about in security is your personal threat model.
[00:19:24.900 --> 00:19:27.900]   And we talked a little bit about this earlier, how journalists have to be...
[00:19:27.900 --> 00:19:36.900]   They have to really start thinking about their threat models, not only in terms of some nation state hacking them, but perhaps government surveillance.
[00:19:36.900 --> 00:19:40.900]   Perhaps there are companies that don't like their reporting.
[00:19:40.900 --> 00:19:45.900]   And so it's really important for journalists to think about this, no matter what level of journalists you are.
[00:19:45.900 --> 00:19:50.900]   And then beyond that, what's your personal threat model?
[00:19:50.900 --> 00:19:53.900]   Who do you think is actually going to be attacking you?
[00:19:53.900 --> 00:19:55.900]   Where are you using your credit cards?
[00:19:55.900 --> 00:19:59.900]   Are you using tap to pay instead of swipe all these different things?
[00:19:59.900 --> 00:20:05.900]   And Apple's pretty good about giving us technologies that allow us easily to do things like that.
[00:20:05.900 --> 00:20:10.900]   Apple Pay and the Apple credit card is a more private kind of credit card, right?
[00:20:10.900 --> 00:20:11.900]   Absolutely.
[00:20:11.900 --> 00:20:13.900]   We're even using your fingerprint to log in.
[00:20:13.900 --> 00:20:21.900]   I have issues with Face ID, but Touch ID was revolutionary, not in the sense that it was technologically revolutionary.
[00:20:21.900 --> 00:20:24.900]   We've had fingerprint logins for devices for decades.
[00:20:24.900 --> 00:20:34.900]   But to be able to log into your phone and suddenly have everybody using iPhones, because so many people upgrade to new iPhones so rapidly to be able to do that,
[00:20:34.900 --> 00:20:36.900]   dramatically increase the security of the device.
[00:20:36.900 --> 00:20:38.900]   And that's the kind of thing that people need to understand.
[00:20:38.900 --> 00:20:39.900]   Built-in encryption.
[00:20:39.900 --> 00:20:40.900]   Built-in encryption.
[00:20:40.900 --> 00:20:44.900]   And Google and Apple have both done that now with their phones.
[00:20:44.900 --> 00:20:46.900]   It comes encrypted out of the box.
[00:20:46.900 --> 00:20:48.900]   Google's efforts to do HTTPS.
[00:20:48.900 --> 00:20:55.900]   There's a lot going on, but at the same time, if you're a minority in a country, you're targeted.
[00:20:55.900 --> 00:20:56.900]   You're out of luck.
[00:20:56.900 --> 00:20:57.900]   If a nation-state wants you.
[00:20:57.900 --> 00:21:03.900]   You're not out of luck, but there are things that you can do to make sure that you're being more secure when you do have to use your device.
[00:21:03.900 --> 00:21:04.900]   Yeah.
[00:21:04.900 --> 00:21:08.900]   There was a thread on Reddit on the Privacy Tools thread.
[00:21:08.900 --> 00:21:11.900]   What does Edward Snowden do?
[00:21:11.900 --> 00:21:15.900]   And it's generally agreed after some back-and-forth conversations.
[00:21:15.900 --> 00:21:20.900]   Edward Snowden has recommended signal and cubes OS and so forth.
[00:21:20.900 --> 00:21:25.900]   But it was generally agreed at this point, both the US and Russia know exactly where Edward Snowden is.
[00:21:25.900 --> 00:21:28.900]   They're probably monitoring all his communications.
[00:21:28.900 --> 00:21:33.900]   If you're really the target of this kind of surveillance, it's going to be pretty hard to avoid.
[00:21:33.900 --> 00:21:35.900]   So this is actually a really interesting thing.
[00:21:35.900 --> 00:21:41.900]   A couple of years ago, I did a story on traveling internationally and how to be secure when you travel internationally.
[00:21:41.900 --> 00:21:52.900]   And one person I spoke with for the story said that if you are, for example, an activist in Syria, you know that you're going to be surveyed.
[00:21:52.900 --> 00:21:54.900]   There's no question that governments are watching you.
[00:21:54.900 --> 00:21:57.900]   The question is which governments are watching you?
[00:21:57.900 --> 00:22:11.900]   And you're probably better off having the US spying on you than you are having Lebanon, which amazingly because of the decrease in cost of tools is now spying on people, you know, than having Lebanon spying you.
[00:22:11.900 --> 00:22:19.900]   And so the source said, if you know you're being watched, it's a good idea to use Skype and you do-
[00:22:19.900 --> 00:22:24.900]   Be out in the open.
[00:22:24.900 --> 00:22:29.900]   Be out in the open about your verbal conversation, but they're probably not observing the video.
[00:22:29.900 --> 00:22:34.900]   And so you can use the video, you can use prearranged signals to communicate in ways that are nonverbal.
[00:22:34.900 --> 00:22:35.900]   That's what you need though.
[00:22:35.900 --> 00:22:38.900]   You need a further encryption technology.
[00:22:38.900 --> 00:22:44.900]   But sometimes knowing that you're being watched is safer in the sense that you know how to deal with it.
[00:22:44.900 --> 00:22:52.900]   What a world we live in. You've said an interesting thing, Seth, which is that the costs have gone down to be a surveillance state.
[00:22:52.900 --> 00:22:57.900]   So you can pretty much assume any interested party has the tools.
[00:22:57.900 --> 00:22:59.900]   Or at least has some tools.
[00:22:59.900 --> 00:23:00.900]   Yikes.
[00:23:00.900 --> 00:23:05.900]   Seth's point two is really good in that there's a lot of misinformation about security.
[00:23:05.900 --> 00:23:13.900]   The fact that we are vulnerable, marketing, which I talked about earlier, sort of there are marketing companies that market their products.
[00:23:13.900 --> 00:23:18.900]   That try to take advantage of the fear and say scary things.
[00:23:18.900 --> 00:23:24.900]   And so it's also our job to debunk some of those and make sure that people understand.
[00:23:24.900 --> 00:23:25.900]   Yeah.
[00:23:25.900 --> 00:23:30.900]   It's possible either to be not scared enough or too scared about a lot of the stuff or to be too scared about the wrong things.
[00:23:30.900 --> 00:23:35.900]   I mean, I think about all my friends who are not terribly technically savvy,
[00:23:35.900 --> 00:23:40.900]   who whenever anything goes wrong with any other devices, the first thing that comes to mind is they have a virus.
[00:23:40.900 --> 00:23:48.900]   Which I have when I went in fact 75% of the time it's user error or random normal technology going bad.
[00:23:48.900 --> 00:23:49.900]   Kind of stuff.
[00:23:49.900 --> 00:23:50.900]   Yeah.
[00:23:50.900 --> 00:23:55.900]   Or earlier before we got on the air you were talking about how you use a VPN all the time.
[00:23:55.900 --> 00:23:57.900]   Yeah, that surprised me. This is something new you do.
[00:23:57.900 --> 00:23:59.900]   It's really relatively recent.
[00:23:59.900 --> 00:24:00.900]   Yeah.
[00:24:00.900 --> 00:24:01.900]   And that's not always the best thing to do.
[00:24:01.900 --> 00:24:02.900]   I learned that.
[00:24:02.900 --> 00:24:03.900]   You can't get in our chat room.
[00:24:03.900 --> 00:24:05.900]   I can't get in a lot of my financials.
[00:24:05.900 --> 00:24:06.900]   You wouldn't be able to watch Netflix.
[00:24:06.900 --> 00:24:07.900]   I can't get into my financial sites.
[00:24:07.900 --> 00:24:09.900]   Most things would reject you.
[00:24:09.900 --> 00:24:11.900]   But as VPNs are used, they're double edged sword.
[00:24:11.900 --> 00:24:14.900]   They're used by bad guys also to obscure their identity.
[00:24:14.900 --> 00:24:18.900]   Yeah. So maybe I need to, I mean, why did you start using a VPN everywhere?
[00:24:18.900 --> 00:24:21.900]   It's not a general paranoia probably.
[00:24:21.900 --> 00:24:22.900]   Well, it's like me.
[00:24:22.900 --> 00:24:23.900]   I stopped using Chrome.
[00:24:23.900 --> 00:24:30.900]   I started using Firefox because Firefox, especially the new Firefox 69 has really good anti fingerprint tracking,
[00:24:30.900 --> 00:24:32.900]   anti tracking technologies.
[00:24:32.900 --> 00:24:38.900]   And Google has basically indicated its desire to kind of preserve cookie tools.
[00:24:38.900 --> 00:24:41.900]   And it's not a lot of cookie tracking and other kinds of tracking.
[00:24:41.900 --> 00:24:42.900]   They need it.
[00:24:42.900 --> 00:24:43.900]   It's their business model.
[00:24:43.900 --> 00:24:48.900]   And while I love Chrome, you know, I'm starting to think maybe I shouldn't be using it.
[00:24:48.900 --> 00:24:50.900]   Firefox is a very capable, good browser.
[00:24:50.900 --> 00:24:53.900]   We need, by the way, we also don't want to mono culture.
[00:24:53.900 --> 00:24:57.900]   Everything seems to be chromium based, including Microsoft's new edge browser.
[00:24:57.900 --> 00:25:00.900]   It's a good idea to support Firefox so that there's a choice.
[00:25:00.900 --> 00:25:02.900]   If I'm on a Wi-Fi network, I don't know too much about.
[00:25:02.900 --> 00:25:03.900]   It seems sensible to you.
[00:25:03.900 --> 00:25:04.900]   It seems VPN.
[00:25:04.900 --> 00:25:05.900]   Yeah, that makes sense.
[00:25:05.900 --> 00:25:06.900]   Yeah.
[00:25:06.900 --> 00:25:14.900]   So this idea of using only one browser, I think, is a gross misconception of what's happening
[00:25:14.900 --> 00:25:16.900]   in the browser space.
[00:25:16.900 --> 00:25:18.900]   I mean, as an individual, I should have multiple browsers.
[00:25:18.900 --> 00:25:19.900]   Yeah.
[00:25:19.900 --> 00:25:20.900]   Why?
[00:25:20.900 --> 00:25:21.900]   Yeah, I do.
[00:25:21.900 --> 00:25:26.900]   Because you want to have your financials only happening in one browser.
[00:25:26.900 --> 00:25:32.900]   But you don't necessarily want to go to Amazon from the same browser that you're doing your financials.
[00:25:32.900 --> 00:25:34.900]   Because of cross-site scripting?
[00:25:34.900 --> 00:25:35.900]   Or cross-site scripting?
[00:25:35.900 --> 00:25:37.900]   Any, just the idea of--
[00:25:37.900 --> 00:25:39.900]   Because the third--
[00:25:39.900 --> 00:25:41.900]   Okay, the third blights, but use the top-up.
[00:25:41.900 --> 00:25:44.900]   The third-party tracking cookies are live in a browser.
[00:25:44.900 --> 00:25:46.900]   They don't live on your system.
[00:25:46.900 --> 00:25:51.900]   So if you use the same browser that enables better tracking, so use different browsers.
[00:25:51.900 --> 00:25:52.900]   Sure.
[00:25:52.900 --> 00:25:53.900]   That's interesting.
[00:25:53.900 --> 00:25:54.900]   I never thought of that.
[00:25:54.900 --> 00:25:58.900]   Also it allows you to use, which is my favorite thing, as many tabs as you can handle.
[00:25:58.900 --> 00:25:59.900]   [laughter]
[00:25:59.900 --> 00:26:01.900]   Oh, you're one of those.
[00:26:01.900 --> 00:26:02.900]   It's probably interesting.
[00:26:02.900 --> 00:26:03.900]   Interesting.
[00:26:03.900 --> 00:26:04.900]   The geek world device--
[00:26:04.900 --> 00:26:05.900]   Have to.
[00:26:05.900 --> 00:26:07.900]   Are you a multi-tab kind of guy?
[00:26:07.900 --> 00:26:08.900]   Oh, yeah.
[00:26:08.900 --> 00:26:09.900]   Jason?
[00:26:09.900 --> 00:26:10.900]   Yeah.
[00:26:10.900 --> 00:26:11.900]   And multi-browser too.
[00:26:11.900 --> 00:26:12.900]   Yeah.
[00:26:12.900 --> 00:26:14.900]   I think the world divides into people who close their tabs.
[00:26:14.900 --> 00:26:15.900]   Look, I got one tab open.
[00:26:15.900 --> 00:26:16.900]   Who closed their tabs.
[00:26:16.900 --> 00:26:17.900]   Wow.
[00:26:17.900 --> 00:26:19.900]   As hygienic to be hygienic.
[00:26:19.900 --> 00:26:22.900]   And then those who just keep them all open.
[00:26:22.900 --> 00:26:24.900]   It's whatever now when you might need a tab.
[00:26:24.900 --> 00:26:26.340]   Or an email, right?
[00:26:26.340 --> 00:26:27.340]   We keep all of our emails.
[00:26:27.340 --> 00:26:28.900]   Oh, I just ignore my email.
[00:26:28.900 --> 00:26:29.900]   [laughter]
[00:26:29.900 --> 00:26:31.900]   I felt so bad, Ant and I were talking last night about an old friend.
[00:26:31.900 --> 00:26:33.900]   I said, "I wonder if I've heard from her recently.
[00:26:33.900 --> 00:26:35.900]   I got an email from her in 2017.
[00:26:35.900 --> 00:26:36.900]   I didn't respond to it.
[00:26:36.900 --> 00:26:37.900]   I felt so bad."
[00:26:37.900 --> 00:26:42.900]   And now I'm debating, "Can I respond now?"
[00:26:42.900 --> 00:26:43.900]   Yes.
[00:26:43.900 --> 00:26:44.900]   Yeah.
[00:26:44.900 --> 00:26:45.900]   I just say, "I'm really sorry.
[00:26:45.900 --> 00:26:47.900]   I just slipped through the cracks."
[00:26:47.900 --> 00:26:50.900]   Or, "Do I pretend it didn't get here?"
[00:26:50.900 --> 00:26:52.900]   Well, I have like 25-year-old email.
[00:26:52.900 --> 00:26:53.900]   I still have.
[00:26:53.900 --> 00:26:56.900]   It'll be fun to randomly respond to somebody from 1984.
[00:26:56.900 --> 00:26:57.900]   Thanks for writing.
[00:26:57.900 --> 00:27:00.900]   I've been thinking since 1994, and I've come up with an answer.
[00:27:00.900 --> 00:27:01.900]   That would be fun.
[00:27:01.900 --> 00:27:04.900]   I've got people who are probably not on the same email address as stuff.
[00:27:04.900 --> 00:27:05.900]   [laughter]
[00:27:05.900 --> 00:27:07.900]   You could put it one night, you're in 2000-8 or something.
[00:27:07.900 --> 00:27:10.900]   Anybody sent me email that I'm responding to?
[00:27:10.900 --> 00:27:13.900]   I'm sure I just missed it.
[00:27:13.900 --> 00:27:18.900]   Sometimes I actually confess, sometimes I'll read an email and go, "That's too much work to answer right now.
[00:27:18.900 --> 00:27:21.900]   I'll answer it later," which means I never, ever did.
[00:27:21.900 --> 00:27:22.900]   Sure.
[00:27:22.900 --> 00:27:23.900]   You do that, right?
[00:27:23.900 --> 00:27:24.900]   Everybody does that.
[00:27:24.900 --> 00:27:27.900]   Well, you know, Gmail introduced, I think it was earlier this year, this snooze button.
[00:27:27.900 --> 00:27:28.900]   Yeah.
[00:27:28.900 --> 00:27:29.900]   That's a good idea.
[00:27:29.900 --> 00:27:30.900]   God, I love that.
[00:27:30.900 --> 00:27:31.900]   You snooze it.
[00:27:31.900 --> 00:27:35.900]   See, if I snooze it, then it's just next time I log in, I've got a bunch of email hitting me.
[00:27:35.900 --> 00:27:37.900]   I don't even want to use email.
[00:27:37.900 --> 00:27:38.900]   Yeah.
[00:27:38.900 --> 00:27:42.900]   If you care about your security, stop using Chrome and stop using Facebook.
[00:27:42.900 --> 00:27:43.900]   That's probably the best thing to do.
[00:27:43.900 --> 00:27:45.900]   If you're really paranoid, the best thing to do.
[00:27:45.900 --> 00:27:49.900]   I celebrated my one-year anniversary off of Facebook last month.
[00:27:49.900 --> 00:27:52.900]   I can't say I missed it, but again, I'm in an unusual situation.
[00:27:52.900 --> 00:27:54.900]   I know there are lots of people.
[00:27:54.900 --> 00:27:58.900]   That's how they keep in touch with family and friends, and it's not an option for them.
[00:27:58.900 --> 00:28:00.900]   I still have an account.
[00:28:00.900 --> 00:28:07.580]   I don't install their apps on any of my devices because they have been so egregious with them.
[00:28:07.580 --> 00:28:12.900]   I feel like their policy looks like they just are like, "We'll take the fines.
[00:28:12.900 --> 00:28:15.900]   We'll just, you know, if we compromise, people's security, we'll just keep the fines."
[00:28:15.900 --> 00:28:16.900]   Oh, look at this.
[00:28:16.900 --> 00:28:17.900]   This is the new one.
[00:28:17.900 --> 00:28:20.300]   Thank you, Carson, for pulling this up from MIT Technology Reviews.
[00:28:20.300 --> 00:28:24.100]   Remember when Facebook asked for your phone number for two-factor authentication, and
[00:28:24.100 --> 00:28:27.660]   then it came out that they were using your phone number to market to you?
[00:28:27.660 --> 00:28:37.180]   Now it's come out that they have put 419 million phone numbers in a database in public without
[00:28:37.180 --> 00:28:38.900]   password access.
[00:28:38.900 --> 00:28:40.620]   Just anybody could see it.
[00:28:40.620 --> 00:28:45.860]   I would like to note, though, that AT&T did this as well.
[00:28:45.860 --> 00:28:46.860]   Okay.
[00:28:46.860 --> 00:28:50.020]   Just to be fair, they had non-password...
[00:28:50.020 --> 00:28:54.860]   They stored user phone numbers in paper files.
[00:28:54.860 --> 00:29:00.580]   They delivered these files to everyone in the United States.
[00:29:00.580 --> 00:29:04.580]   Well, you mean the phone book?
[00:29:04.580 --> 00:29:06.500]   Yeah.
[00:29:06.500 --> 00:29:08.660]   He's got a link to whitepages.com.
[00:29:08.660 --> 00:29:11.020]   Okay, smarty pants.
[00:29:11.020 --> 00:29:14.220]   You actually had to pay them to not list a phone number.
[00:29:14.220 --> 00:29:15.220]   You still do.
[00:29:15.220 --> 00:29:16.220]   You have an enlisted phone number.
[00:29:16.220 --> 00:29:17.220]   You have to pay for it.
[00:29:17.220 --> 00:29:18.220]   It's true.
[00:29:18.220 --> 00:29:19.220]   It's just crazy.
[00:29:19.220 --> 00:29:21.220]   Security researcher discovered that she's Carson.
[00:29:21.220 --> 00:29:23.300]   That was a long way to go for that one.
[00:29:23.300 --> 00:29:28.340]   Am I wrong that it was not that Facebook had this stuff public, but that somebody else
[00:29:28.340 --> 00:29:31.140]   who somehow had the phone numbers?
[00:29:31.140 --> 00:29:33.300]   They may have been scraped.
[00:29:33.300 --> 00:29:41.500]   It was discovered by a security researcher, San Yamjain, passes Discovery on to TechCrunch.
[00:29:41.500 --> 00:29:46.700]   The server hosting the database was not password protected.
[00:29:46.700 --> 00:29:48.580]   Anyone could find it and access it.
[00:29:48.580 --> 00:29:57.620]   It included Facebook IDs, names, genders, countries, and phone numbers for 133 million
[00:29:57.620 --> 00:30:03.940]   US-based users, 18 million UK users, 50 million Vietnamese users.
[00:30:03.940 --> 00:30:05.660]   200 million people total.
[00:30:05.660 --> 00:30:08.780]   No way to know if your data was among the leaked records.
[00:30:08.780 --> 00:30:10.820]   Is this FUD?
[00:30:10.820 --> 00:30:11.820]   Is this FUD?
[00:30:11.820 --> 00:30:12.820]   Is this FUD?
[00:30:12.820 --> 00:30:13.820]   No.
[00:30:13.820 --> 00:30:14.820]   Yes.
[00:30:14.820 --> 00:30:16.820]   Who cares about phone numbers?
[00:30:16.820 --> 00:30:19.300]   Well, phone spam is a serious issue right now.
[00:30:19.300 --> 00:30:22.100]   My phone only rings for spam.
[00:30:22.100 --> 00:30:23.100]   It's terrible.
[00:30:23.100 --> 00:30:24.100]   Well, also, and it's also--
[00:30:24.100 --> 00:30:25.100]   Also, I'm trying to--
[00:30:25.100 --> 00:30:29.260]   Remember, if it's linked to more than just a phone number, it's not just a random list
[00:30:29.260 --> 00:30:34.860]   of phone numbers, if it's linked to additional information, that's not good, Carson.
[00:30:34.860 --> 00:30:40.180]   It's not the individual piece that's bad, but the problem is-- and this is the big problem
[00:30:40.180 --> 00:30:45.740]   in general-- is the aggregation of multiple data sources into one giant dossier.
[00:30:45.740 --> 00:30:46.740]   It's just--
[00:30:46.740 --> 00:30:47.740]   Somebody hit--
[00:30:47.740 --> 00:30:49.260]   Yeah, sorry, go ahead, Leo.
[00:30:49.260 --> 00:30:53.900]   It's just another piece of information about you that shouldn't be in public unless you
[00:30:53.900 --> 00:30:54.900]   want it to be.
[00:30:54.900 --> 00:30:57.300]   And the fact that you-- here's the real offensive part.
[00:30:57.300 --> 00:31:02.460]   The fact that people gave these numbers to Facebook, assuming that they would be protected
[00:31:02.460 --> 00:31:08.060]   and not be-- and were used for two-factor or other legitimate reasons and not be used
[00:31:08.060 --> 00:31:14.460]   for marketing purposes, and that Facebook cavalierly just put this database online with
[00:31:14.460 --> 00:31:16.740]   no security is offensive.
[00:31:16.740 --> 00:31:19.820]   Whether it really is a bad thing-- I mean, my phone numbers out in the public, I don't
[00:31:19.820 --> 00:31:20.820]   care about that.
[00:31:20.820 --> 00:31:22.060]   I just don't answer the phone anymore.
[00:31:22.060 --> 00:31:26.860]   Well, I mean, that's a big part of the issue, I think, is that Facebook has proven to be
[00:31:26.860 --> 00:31:27.860]   extremely cavalier--
[00:31:27.860 --> 00:31:28.860]   Just cavalierly?
[00:31:28.860 --> 00:31:29.860]   Yes, exactly.
[00:31:29.860 --> 00:31:36.740]   --exactly what they use with their user data, despite their assurances that only they exploit
[00:31:36.740 --> 00:31:37.740]   it.
[00:31:37.740 --> 00:31:41.140]   Which is not-- was only slightly assuring.
[00:31:41.140 --> 00:31:42.140]   Yeah, right.
[00:31:42.140 --> 00:31:43.140]   Yeah.
[00:31:43.140 --> 00:31:44.540]   Only we will exploit you.
[00:31:44.540 --> 00:31:46.140]   Oh, that's OK then.
[00:31:46.140 --> 00:31:48.020]   But that's what people are expecting.
[00:31:48.020 --> 00:31:49.020]   Yes.
[00:31:49.020 --> 00:31:50.020]   And--
[00:31:50.020 --> 00:31:51.020]   Yes.
[00:31:51.020 --> 00:31:52.020]   Yeah.
[00:31:52.020 --> 00:31:55.660]   I mean, honestly, most people who signed up for Facebook, even today, assume that Facebook
[00:31:55.660 --> 00:31:59.940]   will only reveal that information to your friends.
[00:31:59.940 --> 00:32:00.940]   You know?
[00:32:00.940 --> 00:32:04.660]   Yeah, what worries me is that if somebody knows your mobile number, then they can--
[00:32:04.660 --> 00:32:05.660]   Spoof it.
[00:32:05.660 --> 00:32:06.660]   --move your two-factor--
[00:32:06.660 --> 00:32:08.620]   There you go, Carsten.
[00:32:08.620 --> 00:32:11.380]   And we know simjacking is a massive problem.
[00:32:11.380 --> 00:32:18.260]   Just ask Jack, who got simjacked by the Chuckling Squad.
[00:32:18.260 --> 00:32:24.460]   Jack Dorsey, of course, the CEO of Twitter, whose account was hacked because he had signed
[00:32:24.460 --> 00:32:30.260]   up for a company that Twitter bought that allowed you to text, message, a tweet without
[00:32:30.260 --> 00:32:31.260]   any authentication.
[00:32:31.260 --> 00:32:36.900]   And if Twitter said, oh, that came from your phone number, must be you.
[00:32:36.900 --> 00:32:40.340]   So they've chapped that off at least temporarily.
[00:32:40.340 --> 00:32:42.220]   All right.
[00:32:42.220 --> 00:32:43.220]   Apple has an event.
[00:32:43.220 --> 00:32:44.220]   There's a happy Apple story.
[00:32:44.220 --> 00:32:46.980]   We're going to get to the happy Apple story.
[00:32:46.980 --> 00:32:49.660]   Happy Apple story in just a little bit.
[00:32:49.660 --> 00:32:56.380]   By the way, Facebook spokesperson Jan Nancaro said the data had been scraped before Facebook
[00:32:56.380 --> 00:32:58.980]   cut off access to user phone numbers.
[00:32:58.980 --> 00:33:00.940]   So it's OK.
[00:33:00.940 --> 00:33:01.940]   It's OK.
[00:33:01.940 --> 00:33:04.060]   This was old data.
[00:33:04.060 --> 00:33:05.740]   This was back when we allowed that.
[00:33:05.740 --> 00:33:08.740]   It dates from when they were being pretty cavalier, so they--
[00:33:08.740 --> 00:33:09.740]   Yeah.
[00:33:09.740 --> 00:33:10.740]   --and they were cavalier.
[00:33:10.740 --> 00:33:11.740]   The camera general, Dachara.
[00:33:11.740 --> 00:33:12.740]   The data set is old.
[00:33:12.740 --> 00:33:17.020]   It appears to have information obtained before we made changes last year.
[00:33:17.020 --> 00:33:18.820]   See, Nat, did you see?
[00:33:18.820 --> 00:33:22.260]   See, Nat got the phone number for Chris Hughes.
[00:33:22.260 --> 00:33:23.260]   Oh, perfect.
[00:33:23.260 --> 00:33:25.260]   This is the Facebook co-founder who now is--
[00:33:25.260 --> 00:33:26.260]   Who is the anti-Facebook?
[00:33:26.260 --> 00:33:27.260]   --an anti-Facebook.
[00:33:27.260 --> 00:33:28.260]   Yeah.
[00:33:28.260 --> 00:33:31.500]   And they texted him, and it turned out it's somebody else now.
[00:33:31.500 --> 00:33:32.500]   Yeah.
[00:33:32.500 --> 00:33:36.460]   But it's somebody else who says, I keep getting called by people who are trying to reach Chris
[00:33:36.460 --> 00:33:39.580]   Hughes at this phone number.
[00:33:39.580 --> 00:33:40.580]   Oh, Lord.
[00:33:40.580 --> 00:33:42.660]   Oh, Lord.
[00:33:42.660 --> 00:33:44.380]   I'm going to have fun with this panel.
[00:33:44.380 --> 00:33:45.380]   This is great.
[00:33:45.380 --> 00:33:46.380]   Thank you, Seth, for joining us.
[00:33:46.380 --> 00:33:49.180]   This is first time on, but he feels like he's been here forever.
[00:33:49.180 --> 00:33:52.500]   Seth Rosenblatt is EIC at theparallax.com.
[00:33:52.500 --> 00:33:56.780]   Somebody who has been here forever are friend Harry McCracken for the Technologizer.
[00:33:56.780 --> 00:33:57.780]   And Jason Heiner, too.
[00:33:57.780 --> 00:34:01.420]   I guess you're a longtime Twitter editorial director at CNET.
[00:34:01.420 --> 00:34:03.580]   I have to treat you better now.
[00:34:03.580 --> 00:34:06.940]   He's always treating me great.
[00:34:06.940 --> 00:34:07.940]   He's a big shot.
[00:34:07.940 --> 00:34:08.940]   He's always treating me great.
[00:34:08.940 --> 00:34:11.260]   Show today brought to you by Mint Mobile.
[00:34:11.260 --> 00:34:15.220]   I have become a huge fan of Mint Mobile.
[00:34:15.220 --> 00:34:20.820]   My Mint Mobile phone, I use a OnePlus 7 Pro on the Mint Mobile.
[00:34:20.820 --> 00:34:21.820]   I just love it.
[00:34:21.820 --> 00:34:27.340]   For $300 a year, I get 12 gigabytes of data, a month unlimited text and phone numbers.
[00:34:27.340 --> 00:34:30.540]   It's literally the least expensive service I have.
[00:34:30.540 --> 00:34:32.940]   Because Mint Mobile runs on T-Mobile.
[00:34:32.940 --> 00:34:35.420]   They're what we call an MVNO.
[00:34:35.420 --> 00:34:37.540]   They use T-Mobile's towers.
[00:34:37.540 --> 00:34:42.260]   I get the same great service I get on my other phones for a whole lot less.
[00:34:42.260 --> 00:34:48.700]   If you're still using one of the big wireless providers in 2019, really you are overpaying.
[00:34:48.700 --> 00:34:52.380]   And part of it is because you have to pay for those expensive retail stores and flated
[00:34:52.380 --> 00:34:54.620]   prices, hidden fees.
[00:34:54.620 --> 00:34:56.140]   They know you'll pay.
[00:34:56.140 --> 00:34:58.060]   Most people don't even know they have a choice.
[00:34:58.060 --> 00:35:03.140]   Enter Mint Mobile, the same premium network coverage you're used to.
[00:35:03.140 --> 00:35:07.220]   But at a fraction of the cost because they don't have stores, it's all online.
[00:35:07.220 --> 00:35:08.460]   No retail locations.
[00:35:08.460 --> 00:35:12.620]   That overhead is just gone and they pass those savings on to you.
[00:35:12.620 --> 00:35:16.500]   Your wireless bill could be as little as $15 a month with their three month introductory
[00:35:16.500 --> 00:35:17.500]   plan.
[00:35:17.500 --> 00:35:20.900]   Every plan comes with unlimited nationwide text and talk.
[00:35:20.900 --> 00:35:24.060]   And you don't pay for data you'll never use.
[00:35:24.060 --> 00:35:27.580]   What you do is you look at your phone bill and say, "How much data do I use?"
[00:35:27.580 --> 00:35:31.540]   Three, eight, 12 gigabytes of LTE data a month.
[00:35:31.540 --> 00:35:34.460]   Then you pick the plan that's right for you.
[00:35:34.460 --> 00:35:38.060]   I picked the 12 gig plan because I know I'll never use more than 12 gigs.
[00:35:38.060 --> 00:35:39.220]   That's actually more than I use.
[00:35:39.220 --> 00:35:42.140]   But boy, the price was so good.
[00:35:42.140 --> 00:35:43.140]   I thought this is great.
[00:35:43.140 --> 00:35:44.900]   I paid for a year.
[00:35:44.900 --> 00:35:50.740]   That got the price way, way down, $300 a year for 12 gigabytes.
[00:35:50.740 --> 00:35:54.780]   But they have plans as little as $15 a month for three gigabytes.
[00:35:54.780 --> 00:35:56.500]   You figure out how much data you need.
[00:35:56.500 --> 00:35:58.380]   You bring your own phone if you want.
[00:35:58.380 --> 00:35:59.740]   You can even port your number.
[00:35:59.740 --> 00:36:01.580]   So you keep the same phone number.
[00:36:01.580 --> 00:36:04.500]   All your contacts, people will still not reach you.
[00:36:04.500 --> 00:36:07.300]   All your ditching is their old wireless bill.
[00:36:07.300 --> 00:36:14.100]   Start saving with Mint Mobile at mintmobile.com/twit to get your new wireless plan just $15 a
[00:36:14.100 --> 00:36:17.940]   month with their three month introductory plan and get it all shipped to your door for
[00:36:17.940 --> 00:36:18.940]   free.
[00:36:18.940 --> 00:36:22.060]   Go to mintmobile.com/twit.
[00:36:22.060 --> 00:36:23.300]   I love the Fox.
[00:36:23.300 --> 00:36:27.220]   I've been very happy with the Fox so much so that I just really want to move all my
[00:36:27.220 --> 00:36:29.300]   phones over to Mint Mobile.
[00:36:29.300 --> 00:36:31.300]   But unfortunately, I have way too many.
[00:36:31.300 --> 00:36:37.660]   If you stole my mobile bills, way too many accounts, way too many phones.
[00:36:37.660 --> 00:36:38.820]   Mintmobile.com/twit.
[00:36:38.820 --> 00:36:41.100]   You're going to love it.
[00:36:41.100 --> 00:36:42.100]   Mint mobile.
[00:36:42.100 --> 00:36:43.100]   Highly recommend it.
[00:36:43.100 --> 00:36:45.940]   Mintmobile.com/twit.
[00:36:45.940 --> 00:36:51.340]   So this is an expensive time of year for me.
[00:36:51.340 --> 00:36:52.740]   The new iPhone will be out.
[00:36:52.740 --> 00:36:54.940]   The new Apple Watch will be out.
[00:36:54.940 --> 00:36:57.620]   The new Pixel 4 will be out.
[00:36:57.620 --> 00:36:59.860]   There's a bunch of phones at IFA.
[00:36:59.860 --> 00:37:02.780]   Let's start with Tuesday.
[00:37:02.780 --> 00:37:08.020]   "Apples by innovation only event."
[00:37:08.020 --> 00:37:10.500]   There's the invitation which I did not get.
[00:37:10.500 --> 00:37:11.900]   Harry, you surely got it.
[00:37:11.900 --> 00:37:12.900]   It will be there.
[00:37:12.900 --> 00:37:13.900]   You'll be going.
[00:37:13.900 --> 00:37:14.900]   Jason, will you be going?
[00:37:14.900 --> 00:37:16.540]   Is that why you're in town?
[00:37:16.540 --> 00:37:21.500]   It's why I'm in town, but I'm actually running the control room from HQ.
[00:37:21.500 --> 00:37:24.380]   Basically I've got people on the team that are there.
[00:37:24.380 --> 00:37:30.940]   I'm basically figuring out what happens, what surprises they are, make sure we've got
[00:37:30.940 --> 00:37:31.940]   everybody covered.
[00:37:31.940 --> 00:37:37.020]   We've already got our game plan, but there's always a few surprises.
[00:37:37.020 --> 00:37:38.020]   Do you
[00:37:38.020 --> 00:37:39.020]   What do you guys
[00:37:39.020 --> 00:37:41.500]   So your coverage is on CNET.
[00:37:41.500 --> 00:37:42.500]   Do you stream?
[00:37:42.500 --> 00:37:44.540]   Do you do anything else with that?
[00:37:44.540 --> 00:37:45.540]   We do.
[00:37:45.540 --> 00:37:46.540]   We have a live show.
[00:37:46.540 --> 00:37:50.300]   So like you guys, we do a live show where we talk it up.
[00:37:50.300 --> 00:37:55.500]   We also will publish a bunch of stuff on YouTube so you can find a lot of our videos
[00:37:55.500 --> 00:37:56.500]   on YouTube.
[00:37:56.500 --> 00:38:02.700]   First takes, first looks from the hands-on area.
[00:38:02.700 --> 00:38:06.260]   We'll have post event.
[00:38:06.260 --> 00:38:13.220]   We'll have deep coverage of all the stuff that's announced as well as reviews and first
[00:38:13.220 --> 00:38:16.220]   takes, first perspectives of all the good stuff.
[00:38:16.220 --> 00:38:17.460]   We'll be doing that too.
[00:38:17.460 --> 00:38:19.660]   I'll be in here at 10am.
[00:38:19.660 --> 00:38:22.900]   I'll be joined by Micah Sargent.
[00:38:22.900 --> 00:38:27.740]   Apple for the first time is going to stream this on YouTube, which might explain why
[00:38:27.740 --> 00:38:34.700]   Apple keeps taking us down on YouTube when we do our streaming coverage of their events.
[00:38:34.700 --> 00:38:37.420]   Maybe they thought, "You know, we don't want anybody to think that this is the only
[00:38:37.420 --> 00:38:38.420]   way you can watch it.
[00:38:38.420 --> 00:38:40.340]   I don't know anyway."
[00:38:40.340 --> 00:38:44.460]   Apple will stream on YouTube, "We will not," because we don't want to get taken down.
[00:38:44.460 --> 00:38:48.660]   When they take us down, it's 12 weeks or so before we get back up, so that's not worth
[00:38:48.660 --> 00:38:50.140]   it for us.
[00:38:50.140 --> 00:38:52.140]   We will put it in other places.
[00:38:52.140 --> 00:38:56.580]   I'll leave it to you to figure out where I don't want the Apple lawyers to know.
[00:38:56.580 --> 00:38:58.540]   You can watch the library, you can watch after the fact.
[00:38:58.540 --> 00:39:00.220]   It's always fun.
[00:39:00.220 --> 00:39:06.180]   It's really almost, at least it started in 2007 as a religious holiday.
[00:39:06.180 --> 00:39:13.220]   The Apple acolytes gathering to hear the great, the one, the only, the Steve dispenses
[00:39:13.220 --> 00:39:14.220]   wisdom.
[00:39:14.220 --> 00:39:17.700]   In the early days of the iPhone, it was a big deal.
[00:39:17.700 --> 00:39:19.580]   Every year there was something new and amazing.
[00:39:19.580 --> 00:39:24.340]   There was gasp-worthy one more things and bands and so forth.
[00:39:24.340 --> 00:39:26.940]   It's hard for Apple, though, to keep doing that year after year.
[00:39:26.940 --> 00:39:29.020]   Here we are in what is it?
[00:39:29.020 --> 00:39:30.940]   Year 12 of the iPhone.
[00:39:30.940 --> 00:39:34.660]   There are way more leaks now and you're far more likely to know the basic features.
[00:39:34.660 --> 00:39:39.580]   I do think, I mean, it is still fun to hear them tell the story because the leaks are
[00:39:39.580 --> 00:39:42.100]   just about speeds and feeds.
[00:39:42.100 --> 00:39:44.220]   Well, the leaks are facts.
[00:39:44.220 --> 00:39:45.220]   You don't get their spin.
[00:39:45.220 --> 00:39:46.220]   Right, right.
[00:39:46.220 --> 00:39:47.220]   Let's be fair.
[00:39:47.220 --> 00:39:52.980]   It is spin, but it's fun to say and often, ultimately, you do not buy a new phone for
[00:39:52.980 --> 00:39:54.300]   speeds and feeds.
[00:39:54.300 --> 00:39:57.900]   You buy it because of the new things that will let you do.
[00:39:57.900 --> 00:39:59.740]   Are you an iPad user?
[00:39:59.740 --> 00:40:04.340]   At the moment, I'm an iPad user and a Pixel 3 user.
[00:40:04.340 --> 00:40:05.340]   Yeah.
[00:40:05.340 --> 00:40:08.700]   And on Tuesday, I'll decide whether my next phone is going to be an iPhone or a Pixel
[00:40:08.700 --> 00:40:09.700]   4.
[00:40:09.700 --> 00:40:11.980]   It's challenging because we don't know much about the Pixel 4 yet and that won't be
[00:40:11.980 --> 00:40:12.980]   till next month.
[00:40:12.980 --> 00:40:15.860]   You know, a little bit more than we might have because when Google was faced with the
[00:40:15.860 --> 00:40:19.620]   Pixel 4 lakes, they decided to say yes.
[00:40:19.620 --> 00:40:21.300]   They took the bull by the horns.
[00:40:21.300 --> 00:40:22.300]   Yeah.
[00:40:22.300 --> 00:40:23.300]   Yeah.
[00:40:23.300 --> 00:40:24.300]   I like to jump back and forth.
[00:40:24.300 --> 00:40:26.740]   I mean, they're both platforms are cool.
[00:40:26.740 --> 00:40:29.020]   And if you write about this stuff, it's useful to know both of them.
[00:40:29.020 --> 00:40:30.580]   I usually carry both.
[00:40:30.580 --> 00:40:36.100]   Although I lately instead of the Pixel 3 XL, I've been carrying one of the Samsung, either
[00:40:36.100 --> 00:40:38.740]   the 10 plus or the Note 10.
[00:40:38.740 --> 00:40:41.180]   And I really, I actually like what Samsung's doing.
[00:40:41.180 --> 00:40:44.020]   Their hardware is really, really good.
[00:40:44.020 --> 00:40:46.620]   I stopped using them at six.
[00:40:46.620 --> 00:40:47.620]   How come?
[00:40:47.620 --> 00:40:49.220]   Oh, because it was crap.
[00:40:49.220 --> 00:40:50.220]   It was crap.
[00:40:50.220 --> 00:40:51.220]   You're right.
[00:40:51.220 --> 00:40:52.220]   You're the software.
[00:40:52.220 --> 00:40:53.220]   They're, they're, they're.
[00:40:53.220 --> 00:40:54.220]   Everything.
[00:40:54.220 --> 00:40:57.460]   It was just, it felt plasticy, but it also kept crashing on me.
[00:40:57.460 --> 00:41:01.860]   And it was only, I think, a year and a half old and I was, I was done.
[00:41:01.860 --> 00:41:02.860]   I was so done.
[00:41:02.860 --> 00:41:03.860]   They don't.
[00:41:03.860 --> 00:41:04.860]   I'll let you play.
[00:41:04.860 --> 00:41:05.860]   I'll let you play with mine for a little.
[00:41:05.860 --> 00:41:06.860]   You can see how you feel.
[00:41:06.860 --> 00:41:08.700]   You're gonna unlock your phone for me.
[00:41:08.700 --> 00:41:10.300]   I'm gonna unlock my phone.
[00:41:10.300 --> 00:41:14.700]   Because I'll use the highly secure on screen finger.
[00:41:14.700 --> 00:41:16.140]   Oh, it's already unlocked.
[00:41:16.140 --> 00:41:17.140]   Great.
[00:41:17.140 --> 00:41:18.140]   Wonderful.
[00:41:18.140 --> 00:41:21.380]   I think their phones are gotten a lot better and I think they don't crap up the Android
[00:41:21.380 --> 00:41:22.380]   UI.
[00:41:22.380 --> 00:41:23.380]   Hardware is great.
[00:41:23.380 --> 00:41:24.380]   They still have some soft brushes.
[00:41:24.380 --> 00:41:25.380]   Yeah.
[00:41:25.380 --> 00:41:26.380]   Well, so does Google.
[00:41:26.380 --> 00:41:27.380]   Let's be fair.
[00:41:27.380 --> 00:41:29.740]   The Pixel 3 had some awful soft brushes.
[00:41:29.740 --> 00:41:32.860]   Some people, I've had a great experience on the Pixel 3, but I know some people.
[00:41:32.860 --> 00:41:34.460]   It was aggressively closing apps.
[00:41:34.460 --> 00:41:36.500]   You couldn't run apps in the background in the first six months.
[00:41:36.500 --> 00:41:40.940]   I heard somebody say something which seems to me to be true, which is, Pixel phones are
[00:41:40.940 --> 00:41:47.460]   a little bit like a Windows PC 10 years ago, where if you use them for six months, they
[00:41:47.460 --> 00:41:48.460]   become progressively.
[00:41:48.460 --> 00:41:49.460]   Right.
[00:41:49.460 --> 00:41:50.460]   We call it bit rot.
[00:41:50.460 --> 00:41:51.460]   Bit rot.
[00:41:51.460 --> 00:41:53.260]   They become progressively less reliable and a little wonkier.
[00:41:53.260 --> 00:41:56.580]   Google Assistant doesn't always work for me right now and I'm not entirely sure why.
[00:41:56.580 --> 00:41:58.580]   Yeah, that's not good.
[00:41:58.580 --> 00:42:00.580]   That is their flagship product.
[00:42:00.580 --> 00:42:03.180]   The assistant doesn't work.
[00:42:03.180 --> 00:42:05.380]   It's pretty nice.
[00:42:05.380 --> 00:42:06.380]   Yeah.
[00:42:06.380 --> 00:42:08.980]   Do you use handwriting recognition?
[00:42:08.980 --> 00:42:11.180]   Do you use the rounded edges on that?
[00:42:11.180 --> 00:42:12.620]   Use them for what?
[00:42:12.620 --> 00:42:14.740]   Yeah, it's got edge lighting.
[00:42:14.740 --> 00:42:15.740]   Yeah.
[00:42:15.740 --> 00:42:20.260]   I've got little edge menus so I can swipe through the edge menu.
[00:42:20.260 --> 00:42:21.260]   Yeah.
[00:42:21.260 --> 00:42:23.500]   I know this is not using Samsung's launcher.
[00:42:23.500 --> 00:42:25.260]   I'm using an action launcher.
[00:42:25.260 --> 00:42:27.140]   But I think it's a very nice phone.
[00:42:27.140 --> 00:42:28.140]   I love the cameras.
[00:42:28.140 --> 00:42:29.260]   In fact, I'm really pleased.
[00:42:29.260 --> 00:42:34.780]   I think Samsung was the first to maybe not even where the first actually, maybe Huawei
[00:42:34.780 --> 00:42:38.140]   or OnePlus was the first to use multiple lenses.
[00:42:38.140 --> 00:42:42.620]   But Apple apparently will now have at least two, maybe three lenses on their screen.
[00:42:42.620 --> 00:42:43.620]   They've got two now.
[00:42:43.620 --> 00:42:44.620]   Right.
[00:42:44.620 --> 00:42:45.620]   So it'll be three.
[00:42:45.620 --> 00:42:46.620]   It's a likely three.
[00:42:46.620 --> 00:42:47.620]   That's what Samsung does.
[00:42:47.620 --> 00:42:48.620]   Now, is Apple going to do the same thing?
[00:42:48.620 --> 00:42:49.860]   The Samsung does with the three lenses.
[00:42:49.860 --> 00:42:51.660]   You've got a telephoto.
[00:42:51.660 --> 00:42:55.980]   You've got a normal and you've got a wide angle or is Apple going to do what other companies
[00:42:55.980 --> 00:43:01.100]   do which is combine data from three lenses to make a super photo?
[00:43:01.100 --> 00:43:02.100]   Do we know?
[00:43:02.100 --> 00:43:04.100]   I mean, it could be.
[00:43:04.100 --> 00:43:10.500]   So there's reports that the latter is true in that you will take that photo and you
[00:43:10.500 --> 00:43:14.460]   will have that sort of super photo as you called it.
[00:43:14.460 --> 00:43:19.020]   And so afterwards, say you cut somebody's arm off or something, right?
[00:43:19.020 --> 00:43:20.100]   Which is easy to do.
[00:43:20.100 --> 00:43:25.900]   And when you're using your phone and taking it real quick, you'll be able to zoom out
[00:43:25.900 --> 00:43:28.820]   on that photo afterwards, right?
[00:43:28.820 --> 00:43:30.220]   So that's one of the reports.
[00:43:30.220 --> 00:43:32.420]   I like the ways that they're going to do that.
[00:43:32.420 --> 00:43:36.060]   That would be pretty neat, you know, to say like, oh, I shot this just a little bit wrong,
[00:43:36.060 --> 00:43:39.220]   which I gave some more headroom or whatever.
[00:43:39.220 --> 00:43:42.740]   And so that would be that would be pretty, pretty cool, pretty spectacular.
[00:43:42.740 --> 00:43:48.580]   But yeah, the idea is that it's likely three camera lens with the sort of, we think it
[00:43:48.580 --> 00:43:52.540]   was a standard camera, which is a wide, basic wide angle and ultra wide angle.
[00:43:52.540 --> 00:43:53.660]   So a zoom out.
[00:43:53.660 --> 00:43:56.500]   And then of course, the telephoto, which is a zoom in.
[00:43:56.500 --> 00:44:02.580]   I love the ultra wide angle on the latest Samsungs because the ability to get.
[00:44:02.580 --> 00:44:05.300]   You don't have to have all arms to take selfies anymore.
[00:44:05.300 --> 00:44:06.780]   It's really fantastic.
[00:44:06.780 --> 00:44:12.140]   So I have to wonder if they're developing these ultra wide angles so that you can shoot holding
[00:44:12.140 --> 00:44:15.380]   the phone vertically like a goober and.
[00:44:15.380 --> 00:44:19.540]   But the goober that is everyone today.
[00:44:19.540 --> 00:44:25.500]   They're all wrong and get and still get a horizontally oriented photo.
[00:44:25.500 --> 00:44:28.500]   Oh, maybe you can.
[00:44:28.500 --> 00:44:32.740]   Apple does an interesting thing, a very opinionated thing.
[00:44:32.740 --> 00:44:38.140]   If you edit your vertical video in iMovie, they make it wide and they just put bars on
[00:44:38.140 --> 00:44:39.140]   the side.
[00:44:39.140 --> 00:44:40.140]   They go, no, we're not.
[00:44:40.140 --> 00:44:41.780]   No, we don't do vertical video.
[00:44:41.780 --> 00:44:42.780]   Good.
[00:44:42.780 --> 00:44:45.060]   Apple says, no, Seth says, don't do it.
[00:44:45.060 --> 00:44:46.060]   We don't do it.
[00:44:46.060 --> 00:44:47.060]   It's standard.
[00:44:47.060 --> 00:44:48.060]   Apple.
[00:44:48.060 --> 00:44:49.300]   So we used to watch this on a phone.
[00:44:49.300 --> 00:44:50.300]   My sense.
[00:44:50.300 --> 00:44:51.300]   Yeah, yeah, yeah.
[00:44:51.300 --> 00:44:52.300]   Right.
[00:44:52.300 --> 00:44:53.620]   But, but can I ask?
[00:44:53.620 --> 00:44:55.620]   Isn't that where you watch everything?
[00:44:55.620 --> 00:44:57.500]   Well, it's fair.
[00:44:57.500 --> 00:45:01.260]   You don't go home and watch your phone video on your TV.
[00:45:01.260 --> 00:45:02.260]   You watch it on your phone.
[00:45:02.260 --> 00:45:03.580]   You share it on your phone.
[00:45:03.580 --> 00:45:04.940]   Your friends watch it on the phone.
[00:45:04.940 --> 00:45:06.660]   I throw stuff to the TV.
[00:45:06.660 --> 00:45:07.660]   Same.
[00:45:07.660 --> 00:45:10.980]   I'll text it to somebody or yeah.
[00:45:10.980 --> 00:45:12.500]   They all are weird.
[00:45:12.500 --> 00:45:15.780]   90% of this viewed on another phone, whether you're a certain.
[00:45:15.780 --> 00:45:17.580]   Vertical video is the future.
[00:45:17.580 --> 00:45:21.820]   All the kids are using vertical, even TikTok's vertical.
[00:45:21.820 --> 00:45:22.820]   God.
[00:45:22.820 --> 00:45:26.020]   In fact, I think if TikTok, if you turn it sideways, it says, no, no, no, you're doing
[00:45:26.020 --> 00:45:27.020]   it wrong.
[00:45:27.020 --> 00:45:28.420]   The kids have been doing vertical for so long.
[00:45:28.420 --> 00:45:29.420]   They're not kids anymore.
[00:45:29.420 --> 00:45:30.420]   That's right.
[00:45:30.420 --> 00:45:31.420]   Adults.
[00:45:31.420 --> 00:45:32.420]   So, yeah.
[00:45:32.420 --> 00:45:33.420]   Tell me about this.
[00:45:33.420 --> 00:45:37.540]   My sense is that Google does still have the edging, what we call computational photography.
[00:45:37.540 --> 00:45:42.740]   They were able to do with a single lens on the Pixel 3, what it took others to do with
[00:45:42.740 --> 00:45:46.780]   multiple lenses and still get better images with night sight and things like that is Google
[00:45:46.780 --> 00:45:47.780]   still.
[00:45:47.780 --> 00:45:48.780]   Or the light.
[00:45:48.780 --> 00:45:49.780]   Yeah.
[00:45:49.780 --> 00:45:50.780]   I think it's a little light.
[00:45:50.780 --> 00:45:52.380]   I don't know about the other stuff.
[00:45:52.380 --> 00:45:55.980]   I think I'll be honest, I think it's sometimes a little overblown.
[00:45:55.980 --> 00:46:03.140]   But for low light, 100%, Google with their software and then Huawei with their hardware,
[00:46:03.140 --> 00:46:07.740]   they're so far head and shoulders about everybody else with low light photography.
[00:46:07.740 --> 00:46:11.500]   The note tends a little better and is closer, but still doesn't matter.
[00:46:11.500 --> 00:46:12.500]   It's not as good.
[00:46:12.500 --> 00:46:15.580]   Well, they have night sight now where August update, but it's still not as good.
[00:46:15.580 --> 00:46:16.580]   It's good.
[00:46:16.580 --> 00:46:19.980]   It's good enough for most people, but it's not the best.
[00:46:19.980 --> 00:46:24.020]   Google, Pixel and Huawei are just so good.
[00:46:24.020 --> 00:46:26.380]   And it is remarkable to give them credit.
[00:46:26.380 --> 00:46:30.380]   It's remarkable what they've done with one lens and the fact that they're going to go
[00:46:30.380 --> 00:46:32.380]   with multiple lenses.
[00:46:32.380 --> 00:46:37.580]   You know, they obviously have really smart people that really care about this and are
[00:46:37.580 --> 00:46:38.740]   doing good stuff.
[00:46:38.740 --> 00:46:45.420]   I think with that thing going to multiple lenses, they've got a lot of potential to really set
[00:46:45.420 --> 00:46:46.420]   a new bar for that.
[00:46:46.420 --> 00:46:50.820]   Kind of why I'm waiting for the Pixel 4 before I decide which is going to be.
[00:46:50.820 --> 00:46:54.180]   I mean, I'm going to have to buy them all anyway, but I feel like the Pixel 4 might
[00:46:54.180 --> 00:47:00.020]   be because of its computational chops and now multiple lenses, the best phone for photography.
[00:47:00.020 --> 00:47:04.220]   Apple went through such a long period where they were the undisputed camera leader.
[00:47:04.220 --> 00:47:05.220]   That's right.
[00:47:05.220 --> 00:47:07.460]   And then the last year or so, that's not been true.
[00:47:07.460 --> 00:47:12.500]   And I would think that that gives Apple a strong incentive to catch up to the other guys
[00:47:12.500 --> 00:47:13.500]   and get a little ahead.
[00:47:13.500 --> 00:47:18.620]   It is true that Google has incredible resources for anything computational.
[00:47:18.620 --> 00:47:21.420]   We may have gotten beyond the point where it matters.
[00:47:21.420 --> 00:47:24.220]   In other words, I feel like we're now better.
[00:47:24.220 --> 00:47:27.260]   All of the camera phones are good enough for normal users.
[00:47:27.260 --> 00:47:28.260]   We're bad.
[00:47:28.260 --> 00:47:30.060]   We're worried about stuff that most people don't even worry about anymore.
[00:47:30.060 --> 00:47:33.500]   But the possible exception of low light where that's really important.
[00:47:33.500 --> 00:47:37.700]   And I don't use night sight as much as I thought I might.
[00:47:37.700 --> 00:47:45.660]   It doesn't work well in situations where your subject has a pulse.
[00:47:45.660 --> 00:47:52.860]   If there's any movement by the subject in the camera, it still needs the thing to be
[00:47:52.860 --> 00:47:53.860]   stationary.
[00:47:53.860 --> 00:47:59.540]   So if you're doing a very low light sunset or landscape scene or whatever, I find it
[00:47:59.540 --> 00:48:02.140]   to be fine.
[00:48:02.140 --> 00:48:05.980]   But when you're trying, you see all these people, I was at the massive attack show last
[00:48:05.980 --> 00:48:08.620]   night and everyone's holding up their cameras.
[00:48:08.620 --> 00:48:09.620]   I know.
[00:48:09.620 --> 00:48:13.180]   And it's the bane of live music these days.
[00:48:13.180 --> 00:48:16.700]   So shitty concert photos have replaced.
[00:48:16.700 --> 00:48:19.020]   Shitty concert t-shirts.
[00:48:19.020 --> 00:48:21.020]   Oh, t-shirts.
[00:48:21.020 --> 00:48:22.020]   They're cheaper.
[00:48:22.020 --> 00:48:25.300]   But everyone posts their crappy photos to Instagram.
[00:48:25.300 --> 00:48:26.540]   It's awful.
[00:48:26.540 --> 00:48:29.740]   I was at a show, what was I at the other day?
[00:48:29.740 --> 00:48:33.980]   And the guy stood up and said, just as a reminder, we're using professional videographers to
[00:48:33.980 --> 00:48:35.220]   make a recording of this.
[00:48:35.220 --> 00:48:36.220]   You don't need to.
[00:48:36.220 --> 00:48:37.220]   Was this a wedding?
[00:48:37.220 --> 00:48:38.220]   I don't remember.
[00:48:38.220 --> 00:48:40.860]   But it was like a very good point.
[00:48:40.860 --> 00:48:43.900]   We actually have professionals recording this.
[00:48:43.900 --> 00:48:47.620]   I think it was probably a show on HBO I was watching, most likely.
[00:48:47.620 --> 00:48:51.620]   My worst thing is, and I mentioned this before, so forgive me for repeating, but it still
[00:48:51.620 --> 00:48:56.140]   bugs the hell out of me because I'm an old man, is when you see people down at the front
[00:48:56.140 --> 00:48:58.940]   of the massive attack show, they're in the front row.
[00:48:58.940 --> 00:49:00.220]   They should be enjoying the music.
[00:49:00.220 --> 00:49:06.540]   Instead, they're turning around and shooting a selfie with the band behind them.
[00:49:06.540 --> 00:49:11.060]   And it's clear there's really only one intent to show your friends what a great life you
[00:49:11.060 --> 00:49:12.060]   have.
[00:49:12.060 --> 00:49:14.140]   You don't think that's clever?
[00:49:14.140 --> 00:49:15.140]   No.
[00:49:15.140 --> 00:49:18.380]   It is not clever.
[00:49:18.380 --> 00:49:21.860]   It just shows me that you have a personality disorder.
[00:49:21.860 --> 00:49:24.700]   Well, that's the whole front row.
[00:49:24.700 --> 00:49:26.500]   It's the whole front row, isn't it?
[00:49:26.500 --> 00:49:27.500]   Yeah.
[00:49:27.500 --> 00:49:29.780]   I was at a train concert the other day.
[00:49:29.780 --> 00:49:32.660]   And the guy from Train basically embraces it.
[00:49:32.660 --> 00:49:34.180]   He says, "Let me do it."
[00:49:34.180 --> 00:49:37.900]   And he takes the phone from people and shoots it himself.
[00:49:37.900 --> 00:49:39.660]   Just let me do it.
[00:49:39.660 --> 00:49:40.660]   What else are you doing?
[00:49:40.660 --> 00:49:44.780]   People are throwing their phones on stage for him to take a picture.
[00:49:44.780 --> 00:49:45.940]   Jeez.
[00:49:45.940 --> 00:49:47.100]   That seems like a bad idea.
[00:49:47.100 --> 00:49:49.540]   What if you get the wrong phone back?
[00:49:49.540 --> 00:49:53.940]   Yeah, because he just takes the picture and throws it back out.
[00:49:53.940 --> 00:49:54.940]   Yeah.
[00:49:54.940 --> 00:49:58.020]   It seems like a dumb thing to do.
[00:49:58.020 --> 00:50:00.580]   People are supposedly going to announce a new watch as well.
[00:50:00.580 --> 00:50:05.260]   Maybe this time in ceramic and titanium, as well as the sporty aluminum.
[00:50:05.260 --> 00:50:07.060]   And I guess they have a stainless steel.
[00:50:07.060 --> 00:50:11.460]   They may apparently have sleep features.
[00:50:11.460 --> 00:50:12.460]   And sleep.
[00:50:12.460 --> 00:50:14.420]   Which personally might get me back on the Apple Watch.
[00:50:14.420 --> 00:50:15.420]   Really?
[00:50:15.420 --> 00:50:17.060]   You want to wear a watch to bed?
[00:50:17.060 --> 00:50:18.460]   Like a porn star?
[00:50:18.460 --> 00:50:20.660]   What's with you?
[00:50:20.660 --> 00:50:23.780]   Well, am I wrong?
[00:50:23.780 --> 00:50:25.180]   I don't want to get too much thought.
[00:50:25.180 --> 00:50:26.180]   Okay.
[00:50:26.180 --> 00:50:27.180]   I just don't give that to my thought.
[00:50:27.180 --> 00:50:30.060]   I didn't even say it.
[00:50:30.060 --> 00:50:34.860]   But yes, I do like the idea of being able to see what kind of hours I've been sleeping.
[00:50:34.860 --> 00:50:39.100]   Because the Apple Watch for so long, really only had enough juice to get you through the
[00:50:39.100 --> 00:50:41.100]   day was a non-starter.
[00:50:41.100 --> 00:50:44.220]   And one where, A, I don't want to wear a watch to bed.
[00:50:44.220 --> 00:50:45.220]   And B, you're exactly right.
[00:50:45.220 --> 00:50:47.980]   When are you going to charge it if you're wearing it day and night?
[00:50:47.980 --> 00:50:48.980]   It's an issue for sure.
[00:50:48.980 --> 00:50:49.980]   It's charging.
[00:50:49.980 --> 00:50:50.980]   Just take them up with one of the more.
[00:50:50.980 --> 00:50:54.860]   So you think they need fast charging fast enough to charge while you shower.
[00:50:54.860 --> 00:50:55.860]   Exactly.
[00:50:55.860 --> 00:50:56.860]   Like a porn star.
[00:50:56.860 --> 00:51:02.300]   There's other watches with bad screens, but long battery life just to get the sleep.
[00:51:02.300 --> 00:51:07.300]   I'm just going to add like a porn star to random sentences now throughout the show and
[00:51:07.300 --> 00:51:08.300]   see what happens.
[00:51:08.300 --> 00:51:09.300]   Please do.
[00:51:09.300 --> 00:51:10.300]   Please do.
[00:51:10.300 --> 00:51:11.300]   No, please don't.
[00:51:11.300 --> 00:51:12.700]   I'm supposed to say please do not.
[00:51:12.700 --> 00:51:13.700]   Please do not.
[00:51:13.700 --> 00:51:22.460]   I've been using the sleep watch on the iPhone.
[00:51:22.460 --> 00:51:25.100]   Maybe for like four months.
[00:51:25.100 --> 00:51:29.540]   So this is a program that's on the phone, but you have to sleep with your phone then.
[00:51:29.540 --> 00:51:30.540]   No, no.
[00:51:30.540 --> 00:51:31.540]   Sorry.
[00:51:31.540 --> 00:51:35.980]   It's an app you download for the phone, but it's a watch app.
[00:51:35.980 --> 00:51:36.980]   Oh, I see.
[00:51:36.980 --> 00:51:37.980]   You can't hold a watch app.
[00:51:37.980 --> 00:51:40.140]   So it gives your Apple watch sleep capabilities?
[00:51:40.140 --> 00:51:41.140]   It does.
[00:51:41.140 --> 00:51:48.820]   And you know, the interesting thing, like Harry, I was wanting to track this because I was
[00:51:48.820 --> 00:51:54.220]   knowing that I was not getting enough sleep and I wanted to really put some data behind
[00:51:54.220 --> 00:51:55.540]   it.
[00:51:55.540 --> 00:51:59.460]   And I was really worried about the charging thing.
[00:51:59.460 --> 00:52:02.380]   It's actually I've never run out of battery on my watch.
[00:52:02.380 --> 00:52:10.420]   I charge it like for if I'm going to about to go to bed and I've only got 50% power
[00:52:10.420 --> 00:52:12.900]   because the before has great battery life.
[00:52:12.900 --> 00:52:15.420]   Let's go get it the end of the day and it's 50% power.
[00:52:15.420 --> 00:52:20.140]   I'll put it on the charger for 20 minutes and while I'm reading before I go to bed,
[00:52:20.140 --> 00:52:26.620]   put it on, it's like it's 78%, 85% it's plenty to sleep through the night and get through
[00:52:26.620 --> 00:52:27.820]   the full next day.
[00:52:27.820 --> 00:52:28.820]   It's also big.
[00:52:28.820 --> 00:52:33.220]   It's the additional question of what do you do with this data?
[00:52:33.220 --> 00:52:36.020]   Because all it does is make you feel tired.
[00:52:36.020 --> 00:52:40.540]   So so years ago, when when the first health trackers came out, I remember reading the
[00:52:40.540 --> 00:52:46.660]   story about this guy who had collected months and months and months of of of his pulse
[00:52:46.660 --> 00:52:53.180]   data and stuff and took it to a doctor and had printed out these reams of data and said
[00:52:53.180 --> 00:52:55.300]   to the doctor, what do you think of this?
[00:52:55.300 --> 00:52:59.340]   And he looks at him and he takes his pulse and he says, you're fine.
[00:52:59.340 --> 00:53:05.420]   Like like like we may have we may have we may know down to the to the nanosecond how many
[00:53:05.420 --> 00:53:07.820]   you know how much sleep we're getting per night.
[00:53:07.820 --> 00:53:08.820]   But what do you do?
[00:53:08.820 --> 00:53:09.820]   That's how it is.
[00:53:09.820 --> 00:53:11.700]   Look, I have a sleep.
[00:53:11.700 --> 00:53:13.060]   So I don't wear a watch to bed.
[00:53:13.060 --> 00:53:14.420]   I have like a porn star.
[00:53:14.420 --> 00:53:17.820]   I have paddles out of my bed like a normal person.
[00:53:17.820 --> 00:53:22.100]   They're in between my mattress and my box spring and they manner and measure it.
[00:53:22.100 --> 00:53:26.500]   This is from Sleep Tracker and I now know I try that too.
[00:53:26.500 --> 00:53:27.500]   I try that too.
[00:53:27.500 --> 00:53:29.620]   I watch it so much more accurate though, Leo.
[00:53:29.620 --> 00:53:30.620]   I try that.
[00:53:30.620 --> 00:53:31.620]   Ooh, cares if it's accurate.
[00:53:31.620 --> 00:53:34.260]   I got 57 out of 100 of my sleep scores.
[00:53:34.260 --> 00:53:35.260]   Is that good?
[00:53:35.260 --> 00:53:36.260]   Is that bad?
[00:53:36.260 --> 00:53:37.260]   I don't know.
[00:53:37.260 --> 00:53:38.260]   Sounds borderline.
[00:53:38.260 --> 00:53:39.260]   Should I be tired?
[00:53:39.260 --> 00:53:40.260]   I failed sleep.
[00:53:40.260 --> 00:53:41.260]   I got a C minus.
[00:53:41.260 --> 00:53:42.260]   I got a D and sleep.
[00:53:42.260 --> 00:53:43.260]   A D and sleep.
[00:53:43.260 --> 00:53:47.820]   I mean to me it's the same reason that tracking stops like a porn star.
[00:53:47.820 --> 00:53:51.900]   The fact I know I'm tracking them makes me more likely to walk more.
[00:53:51.900 --> 00:53:55.380]   If I'm tracking my sleep, I'm more likely to sleep for this.
[00:53:55.380 --> 00:53:59.540]   Well, to sleep in a more normal schedule, it's not that I don't get enough sleep.
[00:53:59.540 --> 00:54:05.620]   It's that I'm all over the board in terms of whether I go to bed early or late.
[00:54:05.620 --> 00:54:11.060]   The general has decided that a 57 is not a D, is an F.
[00:54:11.060 --> 00:54:13.060]   So he's been an F.
[00:54:13.060 --> 00:54:14.260]   So thank you, man.
[00:54:14.260 --> 00:54:16.060]   I was waiting on the curve.
[00:54:16.060 --> 00:54:18.300]   You got an F in sleep like a porn star.
[00:54:18.300 --> 00:54:20.460]   An F in sleep like a porn star.
[00:54:20.460 --> 00:54:23.620]   That's the first sentence that actually makes sense.
[00:54:23.620 --> 00:54:29.220]   You know, the thing is when I tracked it, because I did try some of those other ways
[00:54:29.220 --> 00:54:34.460]   to do it, especially the under mattress thing, it was wildly inaccurate.
[00:54:34.460 --> 00:54:36.460]   So I can't even use that.
[00:54:36.460 --> 00:54:37.460]   All right.
[00:54:37.460 --> 00:54:38.460]   There's a one you're using now, accurate.
[00:54:38.460 --> 00:54:41.620]   How do you know if it's accurate?
[00:54:41.620 --> 00:54:45.780]   Because well, I start to track like when I actually fall asleep and when I get up and
[00:54:45.780 --> 00:54:48.820]   then I sort of see like is it actually doing it?
[00:54:48.820 --> 00:54:50.660]   It's been accurate.
[00:54:50.660 --> 00:54:57.260]   I've only found in since June, like two times where I thought that it wasn't quite accurate.
[00:54:57.260 --> 00:54:58.500]   And it makes more sense, right?
[00:54:58.500 --> 00:55:03.500]   It's on your body, it's sensing your heart rate, it's sensing your movement.
[00:55:03.500 --> 00:55:05.340]   It's not a surprise that it's more accurate.
[00:55:05.340 --> 00:55:11.340]   I do think what, what it's been useful for me is it's actually told me that when I actually
[00:55:11.340 --> 00:55:17.260]   get sleep, when I do it, my sleep actually is very efficient.
[00:55:17.260 --> 00:55:21.260]   So a lot of people find, I was worried that maybe that wasn't the case.
[00:55:21.260 --> 00:55:23.460]   So I stopped worrying this much to kind of set points.
[00:55:23.460 --> 00:55:25.100]   It was like, sleep is fine.
[00:55:25.100 --> 00:55:27.660]   So don't stress out about it.
[00:55:27.660 --> 00:55:34.540]   But to Harry's point, it has maybe more aware of the fact of actually making sure I'm getting
[00:55:34.540 --> 00:55:39.980]   enough sleep the same way tracking your steps does, the same way tracking your meals.
[00:55:39.980 --> 00:55:44.300]   When you observe the behavior, you change the behavior by the very nature of it, which
[00:55:44.300 --> 00:55:46.340]   social scientists have said for a long time.
[00:55:46.340 --> 00:55:49.740]   So it's more in that vein.
[00:55:49.740 --> 00:55:55.180]   And I think what Apple's going to do, I doubt that Apple's going to do anything as involved
[00:55:55.180 --> 00:55:57.180]   as what SleepWatch is.
[00:55:57.180 --> 00:56:00.060]   SleepWatch gives you some amazing data.
[00:56:00.060 --> 00:56:04.260]   I think Apple's going to give the basic set of data.
[00:56:04.260 --> 00:56:07.540]   It's going to do probably a few smarter things that'll make all these other apps, because
[00:56:07.540 --> 00:56:10.300]   there's other apps that do this too for the watch.
[00:56:10.300 --> 00:56:14.620]   It's going to do things like the reports are, it's going to remind you if you're at like
[00:56:14.620 --> 00:56:19.820]   less than 50% before you go to bed and you're usually tracking your sleep, it'll say, hey,
[00:56:19.820 --> 00:56:25.140]   put your watch on the charger for 20 minutes so that you get enough of a charge.
[00:56:25.140 --> 00:56:27.740]   To get to the night and start your night tomorrow.
[00:56:27.740 --> 00:56:31.140]   The series four does seem to be the thing that makes it possible because the battery
[00:56:31.140 --> 00:56:32.900]   life is so much better.
[00:56:32.900 --> 00:56:38.180]   So a new Apple Watch Tuesday with maybe a little better battery life, maybe a newer,
[00:56:38.180 --> 00:56:43.940]   faster processor, sleep tracking, titanium and ceramic.
[00:56:43.940 --> 00:56:47.900]   Might be a price drop because there's rumors that it may not be a series five.
[00:56:47.900 --> 00:56:52.380]   It may just be a series four with new finishes and sleep tracking, which is really just an
[00:56:52.380 --> 00:56:53.780]   app, right?
[00:56:53.780 --> 00:56:57.660]   And then of course with WatchOS, remember, is getting its own apps.
[00:56:57.660 --> 00:57:00.420]   We learned about that at WWDC.
[00:57:00.420 --> 00:57:05.460]   So Watch does have a few software upgrades, but the hardware, the reports are the latest
[00:57:05.460 --> 00:57:09.940]   reports, you know, are that it may, they may call it series five.
[00:57:09.940 --> 00:57:12.300]   You never know, like that's a marketing decision.
[00:57:12.300 --> 00:57:16.660]   They may also just call it series four with new finishes and maybe a little bit more of
[00:57:16.660 --> 00:57:17.660]   price drop.
[00:57:17.660 --> 00:57:22.660]   I don't know about that because they're already just cleaning up with the smartwatch
[00:57:22.660 --> 00:57:23.660]   market.
[00:57:23.660 --> 00:57:24.660]   They're just owning it.
[00:57:24.660 --> 00:57:28.700]   So I don't know if maybe they have some data that they could go, you know, this much further
[00:57:28.700 --> 00:57:30.300]   and sell them to these many more people.
[00:57:30.300 --> 00:57:34.140]   If the price was, you know, just a bracket lower or something.
[00:57:34.140 --> 00:57:37.260]   Well, you also don't want to, you don't want to glide too much.
[00:57:37.260 --> 00:57:42.580]   You want to, even if you're doing well, you want to kind of keep the engine rolling.
[00:57:42.580 --> 00:57:45.180]   They also, there's also a rumor people who have looked at the.
[00:57:45.180 --> 00:57:46.180]   Wait, wait, wait.
[00:57:46.180 --> 00:57:49.820]   I, while we're talking about sleep, Carsten Bondi, do you want me to do a sleep?
[00:57:49.820 --> 00:57:51.260]   While we're talking about sleep.
[00:57:51.260 --> 00:57:54.260]   I'm going to talk about my mattress and why I don't care.
[00:57:54.260 --> 00:57:58.340]   I don't care what my Apple Watch says.
[00:57:58.340 --> 00:58:01.500]   I don't care if I got a failing grade in sleep.
[00:58:01.500 --> 00:58:07.740]   I've got a Casper, which means every minute I spend in bed, I feel good.
[00:58:07.740 --> 00:58:14.460]   Casper is an amazing sleep brand that just has revolutionized the mattress industry by
[00:58:14.460 --> 00:58:19.140]   eliminating the most costly part of the mattress industry.
[00:58:19.140 --> 00:58:22.180]   It's not the expense of making the mattress.
[00:58:22.180 --> 00:58:24.500]   It's not the high quality materials they use.
[00:58:24.500 --> 00:58:28.780]   It's that silly store that you go to so you can lie in a mattress for a minute or two
[00:58:28.780 --> 00:58:30.780]   and decide which mattress you want.
[00:58:30.780 --> 00:58:33.300]   The store doubles the cost of the mattress.
[00:58:33.300 --> 00:58:36.300]   So Casper said, what if we didn't do that?
[00:58:36.300 --> 00:58:41.180]   What if we let people take their mattress delivery right to their house and sleep on
[00:58:41.180 --> 00:58:42.180]   it for 100 nights?
[00:58:42.180 --> 00:58:46.940]   And if at any time in that first 100 nights, they say, this is not for me.
[00:58:46.940 --> 00:58:50.620]   They can send it back at every penny back, no cost.
[00:58:50.620 --> 00:58:55.340]   Casper is improving all the time they've done so much research on mattresses.
[00:58:55.340 --> 00:58:57.260]   They now have a new hybrid mattress.
[00:58:57.260 --> 00:58:58.260]   I just got it.
[00:58:58.260 --> 00:59:00.340]   It features their award-winning foam layers.
[00:59:00.340 --> 00:59:04.180]   They're famous for, but also springs.
[00:59:04.180 --> 00:59:06.860]   But it still comes in that surprisingly compact box.
[00:59:06.860 --> 00:59:07.860]   Why would you want springs?
[00:59:07.860 --> 00:59:09.940]   Well, a couple of reasons.
[00:59:09.940 --> 00:59:16.540]   It offers luxurious comfort, but also resilient support, elevated lift for active support,
[00:59:16.540 --> 00:59:21.260]   durability for all body types, increased airflow within the spring network.
[00:59:21.260 --> 00:59:24.300]   It's always important to sleep cool and those springs help with that.
[00:59:24.300 --> 00:59:29.340]   And this actually is the reason I love my new Casper, the hybrid mattress because it's
[00:59:29.340 --> 00:59:32.860]   got enhanced edge support.
[00:59:32.860 --> 00:59:36.140]   As you're getting in and out of bed, it's easier because it supports you a little bit
[00:59:36.140 --> 00:59:40.940]   better and I just love that.
[00:59:40.940 --> 00:59:44.020]   I think the Casper mattresses are the best in the business.
[00:59:44.020 --> 00:59:46.700]   They're going to want to try it and it's easy to do.
[00:59:46.700 --> 00:59:50.460]   Casper offers free delivery and painless returns in the US and Canada.
[00:59:50.460 --> 00:59:54.100]   A 100 night risk-free sleep on a trial.
[00:59:54.100 --> 00:59:55.100]   And they still come.
[00:59:55.100 --> 00:59:56.100]   There's the box.
[00:59:56.100 --> 00:59:57.100]   This is the new one with the springs.
[00:59:57.100 --> 00:59:58.860]   I'm going to open up this box.
[00:59:58.860 --> 01:00:02.580]   One of the best things about the Casper mattresses, you don't have to air it out.
[01:00:02.580 --> 01:00:05.380]   It is fresh as a daisy right out of the box.
[01:00:05.380 --> 01:00:06.380]   I can't say that.
[01:00:06.380 --> 01:00:07.540]   I bought some in the past.
[01:00:07.540 --> 01:00:11.620]   I bought such very expensive mattresses that you didn't want to sleep on for a week because
[01:00:11.620 --> 01:00:13.860]   they had to air out, not the Casper.
[01:00:13.860 --> 01:00:14.860]   And watch this.
[01:00:14.860 --> 01:00:20.460]   As soon as you take it out of its vacuum sealed bag, it goes, "Whew!"
[01:00:20.460 --> 01:00:23.220]   And expands to size.
[01:00:23.220 --> 01:00:25.940]   And suddenly you've got the most comfortable night sleep watch.
[01:00:25.940 --> 01:00:26.940]   Watch, watch.
[01:00:26.940 --> 01:00:30.140]   Oh, that is a comfy mattress.
[01:00:30.140 --> 01:00:32.420]   I love my Casper.
[01:00:32.420 --> 01:00:34.540]   And by the way, so do our pets.
[01:00:34.540 --> 01:00:35.780]   Get a Casper mattress today.
[01:00:35.780 --> 01:00:42.380]   You can save $100 towards select mattresses by going to Casper, C-A-S-P-E-R.com/Twit1
[01:00:42.380 --> 01:00:45.540]   and use the promo code TWIT1 at checkout.
[01:00:45.540 --> 01:00:46.980]   $100 off.
[01:00:46.980 --> 01:00:51.220]   This is one more reason you need a Casper.
[01:00:51.220 --> 01:00:54.220]   Casper, C-A-S-P-E-R.com/Twit1.
[01:00:54.220 --> 01:00:56.740]   The promo code is TWIT1 at checkout.
[01:00:56.740 --> 01:00:58.180]   Terms and conditions apply.
[01:00:58.180 --> 01:00:59.180]   We love Casper.
[01:00:59.180 --> 01:01:00.180]   You will too.
[01:01:00.180 --> 01:01:01.180]   Casper.com/Twit1.
[01:01:01.180 --> 01:01:05.620]   Thank you, Casper, for supporting our shows for so very long.
[01:01:05.620 --> 01:01:10.300]   We've been with them since practically the beginning and the beginning of Casper, not
[01:01:10.300 --> 01:01:11.300]   the beginning of Twit.
[01:01:11.300 --> 01:01:14.140]   We're older than dirt.
[01:01:14.140 --> 01:01:17.420]   But since the beginning of Casper and we're glad to have you aboard and you support TWIT,
[01:01:17.420 --> 01:01:20.740]   by the way, by using that special address, Casper.com/Twit1.
[01:01:20.740 --> 01:01:23.300]   Promo code TWIT1.
[01:01:23.300 --> 01:01:26.740]   Thank you, Carson, for helping me get a good night's sleep.
[01:01:26.740 --> 01:01:27.740]   Look, cardless.
[01:01:27.740 --> 01:01:28.740]   You're very, very good.
[01:01:28.740 --> 01:01:32.420]   Of what my watch says.
[01:01:32.420 --> 01:01:37.100]   There is at least some information in the Apple source code for our iOS 13 that they
[01:01:37.100 --> 01:01:44.380]   are going to do a Bluetooth tracker, a competitor to Tile and Tracker.
[01:01:44.380 --> 01:01:48.180]   I don't know what to say about that, except why not?
[01:01:48.180 --> 01:01:49.180]   This is smart of Apple.
[01:01:49.180 --> 01:01:52.860]   They're just adding lots of incremental little features that make you use Apple services
[01:01:52.860 --> 01:01:53.860]   more.
[01:01:53.860 --> 01:01:56.100]   This will be part of their Find My app.
[01:01:56.100 --> 01:01:59.020]   I'll hook you on that stuff.
[01:01:59.020 --> 01:02:04.660]   Yeah, there were reports that was going to happen in the spring around WWDC.
[01:02:04.660 --> 01:02:12.980]   For WWDC, they released the new Find My app, which combines Find My Friends with Find
[01:02:12.980 --> 01:02:17.660]   My phone, my French, my garage, Find My Pornstar, whatever you're looking for.
[01:02:17.660 --> 01:02:18.660]   It's in Find My--
[01:02:18.660 --> 01:02:20.660]   But Tracker on it, you can find it now.
[01:02:20.660 --> 01:02:21.660]   Yeah, whatever it's on.
[01:02:21.660 --> 01:02:22.660]   There you go.
[01:02:22.660 --> 01:02:23.660]   There you go.
[01:02:23.660 --> 01:02:25.780]   You can find your Android phone.
[01:02:25.780 --> 01:02:26.780]   You just put it in the case.
[01:02:26.780 --> 01:02:30.860]   You find your Android phone from your Mac or something.
[01:02:30.860 --> 01:02:33.700]   That would be pretty funny, wouldn't it?
[01:02:33.700 --> 01:02:34.700]   It's not allowed.
[01:02:34.700 --> 01:02:38.980]   All right, I think that's enough Apple for today.
[01:02:38.980 --> 01:02:39.980]   I'm--
[01:02:39.980 --> 01:02:44.820]   We didn't mention the one juiciest rumor of the whole thing.
[01:02:44.820 --> 01:02:46.820]   Oh, what is that?
[01:02:46.820 --> 01:02:47.820]   Free--
[01:02:47.820 --> 01:02:48.820]   Free--
[01:02:48.820 --> 01:02:49.820]   Free--
[01:02:49.820 --> 01:02:50.820]   Free--
[01:02:50.820 --> 01:02:51.820]   Free--
[01:02:51.820 --> 01:02:52.820]   Free YouTube album with every iPhone purchase?
[01:02:52.820 --> 01:02:53.820]   Yeah, if only.
[01:02:53.820 --> 01:03:00.940]   But the-- you know, Stephen, Trautland Smith and a few others have found some source code
[01:03:00.940 --> 01:03:12.260]   in the AR code for iOS 13 that shows stereoscopic vision involved.
[01:03:12.260 --> 01:03:15.660]   So what would you use stereoscopic vision for?
[01:03:15.660 --> 01:03:16.660]   Goggles.
[01:03:16.660 --> 01:03:17.660]   A pair of glass--
[01:03:17.660 --> 01:03:18.660]   Special goggles.
[01:03:18.660 --> 01:03:19.660]   --and, right?
[01:03:19.660 --> 01:03:20.660]   So very unlikely--
[01:03:20.660 --> 01:03:23.420]   Yeah, I was going to say, I'll make you a bet.
[01:03:23.420 --> 01:03:24.900]   I'll make you a bet.
[01:03:24.900 --> 01:03:27.300]   I'll bet you $100.
[01:03:27.300 --> 01:03:32.980]   Well, it doesn't-- not only does not announce that this year, never announces it, but okay.
[01:03:32.980 --> 01:03:33.980]   Okay.
[01:03:33.980 --> 01:03:34.980]   And, I mean, if it--
[01:03:34.980 --> 01:03:36.500]   If it works, so--
[01:03:36.500 --> 01:03:38.580]   There's no market.
[01:03:38.580 --> 01:03:40.340]   This isn't going to happen.
[01:03:40.340 --> 01:03:43.420]   Yeah, I don't know about that.
[01:03:43.420 --> 01:03:48.860]   But if they were going to, they put a lot into AR, right?
[01:03:48.860 --> 01:03:49.860]   So--
[01:03:49.860 --> 01:03:54.740]   Yeah, and remember Tim Cook, every quarterly analyst call would say AR, he stopped about
[01:03:54.740 --> 01:03:55.740]   six months ago.
[01:03:55.740 --> 01:03:56.740]   He hasn't mentioned this.
[01:03:56.740 --> 01:03:57.740]   Interesting.
[01:03:57.740 --> 01:03:58.740]   Yeah, I wonder why.
[01:03:58.740 --> 01:04:02.300]   Either he's not mentioning it because he's going to do it or--
[01:04:02.300 --> 01:04:03.300]   There's--
[01:04:03.300 --> 01:04:04.300]   Right.
[01:04:04.300 --> 01:04:05.300]   That's why he stopped mentioning it.
[01:04:05.300 --> 01:04:06.300]   Or Leo's right.
[01:04:06.300 --> 01:04:08.460]   And there's no market for it.
[01:04:08.460 --> 01:04:09.980]   It can be.
[01:04:09.980 --> 01:04:12.900]   But there's-- if they were going to do it, I would say it would be a preview.
[01:04:12.900 --> 01:04:13.900]   It wouldn't be--
[01:04:13.900 --> 01:04:14.900]   Yeah.
[01:04:14.900 --> 01:04:15.900]   --more announcing it.
[01:04:15.900 --> 01:04:16.900]   And here it is.
[01:04:16.900 --> 01:04:17.900]   It's used to follow lens.
[01:04:17.900 --> 01:04:19.700]   Is this something that just everybody's got to have?
[01:04:19.700 --> 01:04:20.700]   No.
[01:04:20.700 --> 01:04:21.700]   No.
[01:04:21.700 --> 01:04:22.700]   Definitely not.
[01:04:22.700 --> 01:04:23.700]   I'm not a fan.
[01:04:23.700 --> 01:04:26.700]   I'm just saying it's one of the reports that's the juiciest of rumors.
[01:04:26.700 --> 01:04:27.700]   It's out there.
[01:04:27.700 --> 01:04:31.780]   There is also a report, though, that new Apple TV hardware is coming.
[01:04:31.780 --> 01:04:36.820]   That's much more likely because of all the stuff with Apple Arcade and Apple TV Plus.
[01:04:36.820 --> 01:04:41.620]   Actually, I will tell you, the Apple Arcade is the one thing I think actually could be
[01:04:41.620 --> 01:04:43.220]   a sleeper hit.
[01:04:43.220 --> 01:04:47.420]   A hundred games, the rumor is five bucks a month.
[01:04:47.420 --> 01:04:48.660]   These are not freemium.
[01:04:48.660 --> 01:04:50.500]   These are not in app purchases.
[01:04:50.500 --> 01:04:52.700]   You just get to play the game.
[01:04:52.700 --> 01:04:54.820]   And you get new games every month.
[01:04:54.820 --> 01:04:55.820]   Games go on.
[01:04:55.820 --> 01:04:56.820]   They go off.
[01:04:56.820 --> 01:05:00.460]   I think that is a very big market for Apple.
[01:05:00.460 --> 01:05:03.140]   I think of everything Apple might announce on Tuesday.
[01:05:03.140 --> 01:05:04.140]   That will be the one.
[01:05:04.140 --> 01:05:06.660]   And if that ships, at least for me, I would buy it.
[01:05:06.660 --> 01:05:09.180]   Wouldn't you pay five bucks a month and just know that you're going to have a bunch of
[01:05:09.180 --> 01:05:10.700]   interesting games?
[01:05:10.700 --> 01:05:11.700]   Maybe you play them.
[01:05:11.700 --> 01:05:12.700]   Maybe you won't, but you got them.
[01:05:12.700 --> 01:05:18.980]   I'd be much given up on mobile games because I'm sick of feeling like I'm playing a slot
[01:05:18.980 --> 01:05:19.980]   machine essentially.
[01:05:19.980 --> 01:05:20.980]   Yeah.
[01:05:20.980 --> 01:05:22.740]   So that's the whole reason this is going to be good.
[01:05:22.740 --> 01:05:24.460]   You'll give the money up front.
[01:05:24.460 --> 01:05:25.460]   Front.
[01:05:25.460 --> 01:05:27.460]   This is going to be a huge thing.
[01:05:27.460 --> 01:05:28.460]   I would happily.
[01:05:28.460 --> 01:05:29.460]   I would happily.
[01:05:29.460 --> 01:05:30.460]   It's going to be great.
[01:05:30.460 --> 01:05:31.460]   Yeah.
[01:05:31.460 --> 01:05:34.460]   Because indie gamers who don't want to go through this whole process, they just want
[01:05:34.460 --> 01:05:35.460]   to make a nice game.
[01:05:35.460 --> 01:05:40.060]   Apple apparently has been paying out money in advance to these companies.
[01:05:40.060 --> 01:05:43.220]   You know, that's how you're going to get this seated is paying companies to develop
[01:05:43.220 --> 01:05:44.420]   games for it.
[01:05:44.420 --> 01:05:47.460]   I think Apple are capable of getting the surprise hit of it.
[01:05:47.460 --> 01:05:52.020]   And I think Apple TV plus is going down.
[01:05:52.020 --> 01:05:54.980]   Have you watched anybody watch any of the trailers?
[01:05:54.980 --> 01:05:55.980]   They're awful.
[01:05:55.980 --> 01:05:57.980]   Sexy L. Emily Dickinson.
[01:05:57.980 --> 01:06:00.140]   What's the deal with Dickinson?
[01:06:00.140 --> 01:06:02.820]   There's nothing sexy about Emily Dickinson.
[01:06:02.820 --> 01:06:04.460]   Oh, you're so wrong.
[01:06:04.460 --> 01:06:06.460]   You're so wrong.
[01:06:06.460 --> 01:06:07.460]   You're so wrong.
[01:06:07.460 --> 01:06:08.460]   Dang out of this one.
[01:06:08.460 --> 01:06:11.700]   I'll give you a crazy theory on Apple TV plus.
[01:06:11.700 --> 01:06:12.700]   Okay.
[01:06:12.700 --> 01:06:15.540]   If you want it.
[01:06:15.540 --> 01:06:20.180]   It will be, there's two reasons why I think it's going to be free for longer than a month
[01:06:20.180 --> 01:06:21.180]   to start.
[01:06:21.180 --> 01:06:22.180]   Okay.
[01:06:22.180 --> 01:06:23.180]   Well, that would be key.
[01:06:23.180 --> 01:06:26.380]   You got to get the buzz.
[01:06:26.380 --> 01:06:27.780]   There's no back catalog, right?
[01:06:27.780 --> 01:06:29.700]   They've got no back catalog to start with.
[01:06:29.700 --> 01:06:33.620]   So what are you who's going to pay $10 a month for three shows?
[01:06:33.620 --> 01:06:34.980]   Not very many people.
[01:06:34.980 --> 01:06:37.980]   So you have that.
[01:06:37.980 --> 01:06:44.580]   You also have the fact that remember the most well known thing about that March event
[01:06:44.580 --> 01:06:49.540]   was what it was Oprah's line, a billion pockets, y'all.
[01:06:49.540 --> 01:06:57.700]   So clearly, Apple sold Oprah on the idea that this was going to be access to a billion
[01:06:57.700 --> 01:06:58.700]   pockets.
[01:06:58.700 --> 01:07:00.100]   How do you get to a billion pockets?
[01:07:00.100 --> 01:07:01.500]   Not by charging 10 bucks.
[01:07:01.500 --> 01:07:05.300]   We get to a billion pockets by making it free to everybody that has the most, like the
[01:07:05.300 --> 01:07:08.180]   current version of iOS.
[01:07:08.180 --> 01:07:09.660]   That's how you get to a billion pockets.
[01:07:09.660 --> 01:07:15.780]   You may have hit it because I don't see people are getting subscription fatigue already.
[01:07:15.780 --> 01:07:17.380]   Just put it until fall.
[01:07:17.380 --> 01:07:18.380]   Right.
[01:07:18.380 --> 01:07:19.860]   You're a Disney.
[01:07:19.860 --> 01:07:25.900]   I was thinking maybe, I mean, this is totally speculative, but would Apple's had a very
[01:07:25.900 --> 01:07:28.700]   positive relationship with Disney for a long time?
[01:07:28.700 --> 01:07:30.180]   Maybe it comes with Disney.
[01:07:30.180 --> 01:07:32.780]   Wonder if it comes with Disney.
[01:07:32.780 --> 01:07:34.140]   It can be.
[01:07:34.140 --> 01:07:35.820]   I think Disney walls really flop.
[01:07:35.820 --> 01:07:40.500]   I guess there's enough people who are fans of Marvel and Star Wars and Marvel and Star
[01:07:40.500 --> 01:07:41.500]   Wars with Cara and that.
[01:07:41.500 --> 01:07:42.500]   How much is Disney going to be?
[01:07:42.500 --> 01:07:43.500]   Seven bucks.
[01:07:43.500 --> 01:07:44.900]   That's not too bad, I guess.
[01:07:44.900 --> 01:07:46.460]   Plus all the kids movies.
[01:07:46.460 --> 01:07:48.460]   You just drop the place in front of the child.
[01:07:48.460 --> 01:07:50.340]   Five hour movies.
[01:07:50.340 --> 01:07:56.060]   All the old Disney TV shows.
[01:07:56.060 --> 01:07:57.540]   Mickey Mouse Club.
[01:07:57.540 --> 01:07:59.940]   Mickey Mouse Club will come back.
[01:07:59.940 --> 01:08:01.940]   So many parents?
[01:08:01.940 --> 01:08:05.260]   So many parents I know are just taking a parents.
[01:08:05.260 --> 01:08:06.260]   Right.
[01:08:06.260 --> 01:08:07.260]   Like parents who want this.
[01:08:07.260 --> 01:08:08.260]   All right.
[01:08:08.260 --> 01:08:09.260]   Okay.
[01:08:09.260 --> 01:08:10.260]   So before we move on from.
[01:08:10.260 --> 01:08:11.260]   I've.
[01:08:11.260 --> 01:08:12.260]   Oh, hold on.
[01:08:12.260 --> 01:08:13.260]   We.
[01:08:13.260 --> 01:08:16.580]   We're going to hang up on you Jason Heiner and call you back.
[01:08:16.580 --> 01:08:17.900]   You're going to hit the TV.
[01:08:17.900 --> 01:08:18.900]   Jason.
[01:08:18.900 --> 01:08:19.900]   He's freezing now.
[01:08:19.900 --> 01:08:20.900]   We'll call you as the hotel.
[01:08:20.900 --> 01:08:21.900]   Wifi killing him.
[01:08:21.900 --> 01:08:22.900]   Go ahead Seth.
[01:08:22.900 --> 01:08:23.900]   Yeah.
[01:08:23.900 --> 01:08:28.820]   So before we move on from iOS 13, there's one thing that flagged my attention.
[01:08:28.820 --> 01:08:35.260]   They're going to stop allowing VoIP apps to function in the background on the iPhone,
[01:08:35.260 --> 01:08:37.460]   which means that if you're using WhatsApp or for using Facebook.
[01:08:37.460 --> 01:08:40.140]   Oh, that puts them right out of business because you don't get calls, right?
[01:08:40.140 --> 01:08:41.140]   Well, you do.
[01:08:41.140 --> 01:08:43.220]   But you have to always be in the app.
[01:08:43.220 --> 01:08:44.220]   You can't.
[01:08:44.220 --> 01:08:46.380]   You see you do something else.
[01:08:46.380 --> 01:08:47.380]   Yeah.
[01:08:47.380 --> 01:08:50.340]   Well, it ends the call, but will you get with a phone ring with somebody?
[01:08:50.340 --> 01:08:54.940]   If I call you on WhatsApp and it's not running, we'll you'll get a notification.
[01:08:54.940 --> 01:08:55.940]   You'll get a ring.
[01:08:55.940 --> 01:08:56.940]   Yeah.
[01:08:56.940 --> 01:08:58.700]   So that doesn't kill that, but it just keeps you from from switching.
[01:08:58.700 --> 01:08:59.700]   Yeah.
[01:08:59.700 --> 01:09:00.700]   Well, that's all right.
[01:09:00.700 --> 01:09:01.900]   I don't think it's the end of the world.
[01:09:01.900 --> 01:09:04.020]   I think that's pretty serious.
[01:09:04.020 --> 01:09:07.300]   People with Verizon dealt with that for years where you couldn't do a phone call and data
[01:09:07.300 --> 01:09:08.300]   at the same time.
[01:09:08.300 --> 01:09:09.300]   Yeah, stupid.
[01:09:09.300 --> 01:09:10.300]   It was bad.
[01:09:10.300 --> 01:09:13.300]   I do calls on my iPad and then do other stuff.
[01:09:13.300 --> 01:09:16.220]   I mean, what people with IPans still are for app.
[01:09:16.220 --> 01:09:22.300]   Or if you're on one of these conference call apps and Zoom or blue jeans or whatever, and
[01:09:22.300 --> 01:09:25.460]   then you have to switch to go check your email or check the check web browser.
[01:09:25.460 --> 01:09:26.460]   Okay.
[01:09:26.460 --> 01:09:29.260]   I think it's going to be a hard thing for for those services.
[01:09:29.260 --> 01:09:32.660]   Is it an anti competitive move or is just Apple trying to save battery life?
[01:09:32.660 --> 01:09:34.180]   They say it's security.
[01:09:34.180 --> 01:09:35.180]   Security.
[01:09:35.180 --> 01:09:36.180]   I don't know.
[01:09:36.180 --> 01:09:37.620]   I had one more Apple phone.
[01:09:37.620 --> 01:09:38.620]   Yes.
[01:09:38.620 --> 01:09:43.100]   I figured about the 16 inch Mac book, which at least I am to announce that Tuesday.
[01:09:43.100 --> 01:09:44.100]   Are they?
[01:09:44.100 --> 01:09:45.500]   Well, it was on some list of possibilities.
[01:09:45.500 --> 01:09:49.020]   I saw although it seems more likely they'll have a more Mac separate Mac.
[01:09:49.020 --> 01:09:50.020]   I had Mac.
[01:09:50.020 --> 01:09:51.020]   Yeah.
[01:09:51.020 --> 01:09:52.020]   Yeah.
[01:09:52.020 --> 01:09:56.100]   But it sounds like it's the first new Mac book in a while that's an all new Mac book
[01:09:56.100 --> 01:09:59.940]   given that the last major new one was the Mac book air and the whole idea behind that
[01:09:59.940 --> 01:10:01.620]   was that it was an old design.
[01:10:01.620 --> 01:10:02.620]   Right.
[01:10:02.620 --> 01:10:08.100]   And there are so many reasons why Apple does need a new laptop design these days.
[01:10:08.100 --> 01:10:10.220]   Well, starting with the keyboard, but.
[01:10:10.220 --> 01:10:11.220]   And that's the rumor.
[01:10:11.220 --> 01:10:14.460]   It's most exciting about this is they might have banned and finally the butterfly keyboard
[01:10:14.460 --> 01:10:16.300]   and do a decent keyboard.
[01:10:16.300 --> 01:10:20.180]   But this will also be a pro, which means it will be a very expensive.
[01:10:20.180 --> 01:10:23.340]   I'm 16 inches, which for a lot of people is too big, but it.
[01:10:23.340 --> 01:10:26.060]   Well, in a 15 inch body.
[01:10:26.060 --> 01:10:27.740]   It's going to be it's going to be be bezel less.
[01:10:27.740 --> 01:10:32.100]   It'll be the same size as their which is which by the way, every other manufacturer's
[01:10:32.100 --> 01:10:34.700]   been doing for at least a couple of years.
[01:10:34.700 --> 01:10:36.300]   So are they going to get rid of the hairy?
[01:10:36.300 --> 01:10:39.980]   Did you jump from your iPad to Harry?
[01:10:39.980 --> 01:10:43.580]   This is not necessarily a machine I buy for myself because even 15 inches is a bit big
[01:10:43.580 --> 01:10:49.740]   for me, but I just I like the idea of Apple doing a genuinely new laptop platform.
[01:10:49.740 --> 01:10:53.820]   Because I want to keep Mac as I shall live with all due respect to your iPad.
[01:10:53.820 --> 01:10:57.980]   Maybe I have a 13 inch MacBook Pro, which eventually I will need to replace.
[01:10:57.980 --> 01:11:02.860]   I might look at something bigger because I use it mainly at home and therefore a larger
[01:11:02.860 --> 01:11:04.980]   screen would be fine.
[01:11:04.980 --> 01:11:06.140]   And don't forget.
[01:11:06.140 --> 01:11:11.260]   One time before the end of the year that Mac Pro that massively expensive Mac Pro has to
[01:11:11.260 --> 01:11:17.300]   come out, maybe that's a Mac announcement in October instead of usually they want to
[01:11:17.300 --> 01:11:18.620]   keep the iPhone announcements.
[01:11:18.620 --> 01:11:20.020]   That's such an important announcement for them.
[01:11:20.020 --> 01:11:21.380]   They want to keep that separate from two.
[01:11:21.380 --> 01:11:28.700]   And if they're doing a phone and a watch and a TV box and multiple operating system updates
[01:11:28.700 --> 01:11:30.580]   they have a full site.
[01:11:30.580 --> 01:11:31.580]   Yeah.
[01:11:31.580 --> 01:11:32.580]   Yeah.
[01:11:32.580 --> 01:11:35.220]   I'd be surprised if they if they talk Mac tomorrow.
[01:11:35.220 --> 01:11:36.220]   I would be too.
[01:11:36.220 --> 01:11:37.220]   But you never know.
[01:11:37.220 --> 01:11:38.220]   Not tomorrow.
[01:11:38.220 --> 01:11:39.220]   Yeah.
[01:11:39.220 --> 01:11:40.220]   They definitely.
[01:11:40.220 --> 01:11:41.220]   Tomorrow.
[01:11:41.220 --> 01:11:42.220]   The people listening to this.
[01:11:42.220 --> 01:11:44.500]   Maybe they will tomorrow just to get it out of the way.
[01:11:44.500 --> 01:11:45.500]   They've done that actually.
[01:11:45.500 --> 01:11:46.500]   They did that last time.
[01:11:46.500 --> 01:11:50.260]   They slipstreamed PR play press announcements of other hardware updates first.
[01:11:50.260 --> 01:11:54.420]   Apple music came out in a web version a few days ago, which actually ties into something
[01:11:54.420 --> 01:12:01.380]   I care about, which is that the on the iPad, the web browser for Safari is so much better
[01:12:01.380 --> 01:12:07.260]   in this version and Safari is finally capable of running sophisticated apps.
[01:12:07.260 --> 01:12:08.700]   Go on iPad.
[01:12:08.700 --> 01:12:10.700]   Yes, on the iPad.
[01:12:10.700 --> 01:12:12.360]   Yeah.
[01:12:12.360 --> 01:12:18.460]   How many people are buying a Samsung Galaxy fold?
[01:12:18.460 --> 01:12:19.460]   Crickets.
[01:12:19.460 --> 01:12:20.460]   Crickets.
[01:12:20.460 --> 01:12:21.460]   Crickets.
[01:12:21.460 --> 01:12:26.060]   It comes out on the on the on the by list you would have gotten two hundred fifty bucks
[01:12:26.060 --> 01:12:27.060]   free.
[01:12:27.060 --> 01:12:28.700]   I think they can't didn't they cancel the pre-orders.
[01:12:28.700 --> 01:12:33.040]   They sold the pre-orders, but they're giving everybody on the pre-order list two hundred
[01:12:33.040 --> 01:12:34.040]   fifty bucks.
[01:12:34.040 --> 01:12:35.040]   I'm on the pre-order list.
[01:12:35.040 --> 01:12:36.040]   You got two hundred fifty bucks?
[01:12:36.040 --> 01:12:38.700]   Yeah, but it says two thousand dollar phone.
[01:12:38.700 --> 01:12:42.100]   Now it's only a one thousand seven hundred fifty dollar phone.
[01:12:42.100 --> 01:12:43.900]   It launches Friday and South Korea.
[01:12:43.900 --> 01:12:45.460]   We don't know when it's going to launch in the US.
[01:12:45.460 --> 01:12:51.820]   It launches in France, Germany, Singapore, the UK.
[01:12:51.820 --> 01:12:57.020]   No carriers specified in the US sometime.
[01:12:57.020 --> 01:13:03.540]   The first in South Korea, September 6th, there will be a 5G option.
[01:13:03.540 --> 01:13:05.740]   They say they fixed the fold problems.
[01:13:05.740 --> 01:13:12.420]   They will apparently have sort of concierge repair service, specialized customer care
[01:13:12.420 --> 01:13:20.100]   service to reassure you, including one on one access to Samsung experts and a 24/7 support
[01:13:20.100 --> 01:13:21.100]   hub.
[01:13:21.100 --> 01:13:24.580]   I still have to think it's going to be a hard sell.
[01:13:24.580 --> 01:13:30.820]   This has got to be an enormous amount of investment, not just the R&D, but then making
[01:13:30.820 --> 01:13:32.900]   it happen and then making it happen with people.
[01:13:32.900 --> 01:13:34.460]   I'm starting all over again.
[01:13:34.460 --> 01:13:37.340]   So why are they doing this Seth?
[01:13:37.340 --> 01:13:40.900]   I don't know.
[01:13:40.900 --> 01:13:41.900]   Because Huawei.
[01:13:41.900 --> 01:13:46.300]   No, they want to be in front of Huawei, right?
[01:13:46.300 --> 01:13:51.660]   They see this as one of the next big potential things for phones.
[01:13:51.660 --> 01:13:58.860]   Huawei, they've been in a race with Huawei to see who could release theirs first, essentially.
[01:13:58.860 --> 01:14:06.260]   I think they will release this because they've been working on it for a long time.
[01:14:06.260 --> 01:14:07.260]   All the sources of the...
[01:14:07.260 --> 01:14:09.260]   It would be capitulation, wouldn't it, if say that?
[01:14:09.260 --> 01:14:11.380]   Displays are a major business for Samsung.
[01:14:11.380 --> 01:14:16.380]   So even beyond the fact that they make devices, they don't want some other company to become
[01:14:16.380 --> 01:14:19.540]   the leader and folding displays, if they matter long-term.
[01:14:19.540 --> 01:14:23.780]   I think in one way or another, folding will matter over the next few years.
[01:14:23.780 --> 01:14:24.780]   Yeah.
[01:14:24.780 --> 01:14:26.420]   Sure, but at the phone level?
[01:14:26.420 --> 01:14:30.420]   So, yeah, I mean, I'm one of the relatively few people who spent some time with the Galaxy
[01:14:30.420 --> 01:14:31.420]   Fold.
[01:14:31.420 --> 01:14:37.340]   Now, it was not sold on that device at that price point with currently existing apps,
[01:14:37.340 --> 01:14:44.260]   but I was sold on the idea that this general idea and this general technology can be valuable.
[01:14:44.260 --> 01:14:46.660]   Because it's almost a tablet or...
[01:14:46.660 --> 01:14:48.300]   It's a tablet that fits in your pocket.
[01:14:48.300 --> 01:14:49.300]   Yeah.
[01:14:49.300 --> 01:14:50.300]   Yeah.
[01:14:50.300 --> 01:14:53.540]   Well, it says the guys in an iPad.
[01:14:53.540 --> 01:14:55.380]   We had a very similar approach.
[01:14:55.380 --> 01:15:03.140]   Jessica Dolkort has looked at it for seeing that pretty in-depth multiple times now.
[01:15:03.140 --> 01:15:05.340]   And we like the possibilities.
[01:15:05.340 --> 01:15:11.340]   We're not going to recommend this device to many people to get, but it is a tantalizing
[01:15:11.340 --> 01:15:16.380]   look at the future and it certainly has holds some interesting possibilities.
[01:15:16.380 --> 01:15:19.620]   Not folding that screen.
[01:15:19.620 --> 01:15:22.820]   You know, I really have some questions in terms of long-term durability to be able to
[01:15:22.820 --> 01:15:26.740]   do that, but it is an interesting idea.
[01:15:26.740 --> 01:15:30.260]   I think Samsung's version is a little bit nicer than Huawei's.
[01:15:30.260 --> 01:15:34.220]   The one that could be the Dark Horse is actually the Motorola Razer.
[01:15:34.220 --> 01:15:36.980]   Interestingly enough, people love that form factor.
[01:15:36.980 --> 01:15:39.180]   It's a flip phone still?
[01:15:39.180 --> 01:15:43.900]   It's a flip phone and both sides are a display.
[01:15:43.900 --> 01:15:44.900]   Right?
[01:15:44.900 --> 01:15:49.780]   I think that that one could be the Dark Horse would be in the most useful of this first
[01:15:49.780 --> 01:15:52.660]   round of these devices.
[01:15:52.660 --> 01:15:58.380]   But Samsung, as Harry said, really wants to be in this for the display business, if nothing
[01:15:58.380 --> 01:15:59.940]   else, right?
[01:15:59.940 --> 01:16:05.580]   That is what they do and it's a big part of the future from their perspective.
[01:16:05.580 --> 01:16:10.580]   So I have the same issue with the Razer as I do with the Samsung.
[01:16:10.580 --> 01:16:14.020]   Is there going to be a crease where that thing folds?
[01:16:14.020 --> 01:16:15.620]   Did you see a crease here?
[01:16:15.620 --> 01:16:19.820]   There's a little crease, if you look at it, it's there, but it did not bother me when
[01:16:19.820 --> 01:16:20.820]   I was after you.
[01:16:20.820 --> 01:16:23.620]   So it doesn't...
[01:16:23.620 --> 01:16:24.620]   It just feels like that's...
[01:16:24.620 --> 01:16:26.900]   You don't see it so much head on.
[01:16:26.900 --> 01:16:29.300]   And we don't know how well it will wear.
[01:16:29.300 --> 01:16:31.300]   We still don't know how well it will wear.
[01:16:31.300 --> 01:16:32.300]   That's the key, right?
[01:16:32.300 --> 01:16:33.300]   How well...
[01:16:33.300 --> 01:16:35.460]   When it unfolded every day multiple times.
[01:16:35.460 --> 01:16:38.700]   Before Samsung ran into the problems, they were talking about how they had tested it
[01:16:38.700 --> 01:16:42.900]   and validated that you could open and close it hundreds of thousands of times.
[01:16:42.900 --> 01:16:46.660]   And they gave it to Engadget and they broke it.
[01:16:46.660 --> 01:16:48.060]   Like in one day.
[01:16:48.060 --> 01:16:52.780]   So I guess that testing wasn't so good after all.
[01:16:52.780 --> 01:16:57.460]   It doesn't give really lend much credibility to their testing methodology that they didn't
[01:16:57.460 --> 01:17:00.020]   know that half the reviewers would break it.
[01:17:00.020 --> 01:17:02.260]   But they used it wrong.
[01:17:02.260 --> 01:17:04.060]   No, yeah, they put...
[01:17:04.060 --> 01:17:07.700]   But not everybody peeled off the sticker, the protective shield.
[01:17:07.700 --> 01:17:08.700]   It's true.
[01:17:08.700 --> 01:17:10.900]   They were in a race to get it out, right?
[01:17:10.900 --> 01:17:14.340]   This has been a race this year to see who could get it out first.
[01:17:14.340 --> 01:17:18.660]   That trumped testing and that trumped practicality.
[01:17:18.660 --> 01:17:20.300]   I have to say who could get it out first.
[01:17:20.300 --> 01:17:25.100]   This Razor V4, because it's so compact when it's folded, it just opens up to a normal phone
[01:17:25.100 --> 01:17:26.340]   size.
[01:17:26.340 --> 01:17:28.940]   But when you fold it up, it's this little pocket square.
[01:17:28.940 --> 01:17:31.140]   I think that's an interesting form factor.
[01:17:31.140 --> 01:17:33.540]   Sounds like Samsung's video one that folds twice.
[01:17:33.540 --> 01:17:36.340]   Yeah, yeah, like the pocket handkerchief.
[01:17:36.340 --> 01:17:39.020]   The Samsung pocket handkerchief.
[01:17:39.020 --> 01:17:41.100]   I think it's like Captain Kirk too, right?
[01:17:41.100 --> 01:17:43.300]   Like you put that thing open.
[01:17:43.300 --> 01:17:47.500]   That's why they called it the Star-Tac when it first came out.
[01:17:47.500 --> 01:17:48.500]   Yep.
[01:17:48.500 --> 01:17:54.860]   So when is this 2019, the Motorola Razor V4?
[01:17:54.860 --> 01:17:56.500]   When is it coming out?
[01:17:56.500 --> 01:17:57.740]   It will be...
[01:17:57.740 --> 01:18:00.220]   This fall, I think they've said...
[01:18:00.220 --> 01:18:02.300]   Ooh, it's going to be expensive.
[01:18:02.300 --> 01:18:03.300]   $1,500.
[01:18:03.300 --> 01:18:05.300]   Is that right?
[01:18:05.300 --> 01:18:06.300]   Wow.
[01:18:06.300 --> 01:18:07.300]   Yep.
[01:18:07.300 --> 01:18:08.300]   Wow.
[01:18:08.300 --> 01:18:09.300]   That sounds right.
[01:18:09.300 --> 01:18:10.300]   That's a little pricey.
[01:18:10.300 --> 01:18:11.300]   I'm not sure I'll pay that.
[01:18:11.300 --> 01:18:12.500]   I wouldn't pay that for that.
[01:18:12.500 --> 01:18:13.500]   This is a...
[01:18:13.500 --> 01:18:15.900]   Leo, you will buy it, Leo.
[01:18:15.900 --> 01:18:16.900]   Let's be honest.
[01:18:16.900 --> 01:18:18.300]   You will buy it.
[01:18:18.300 --> 01:18:23.500]   Do it for the future cyber punks.
[01:18:23.500 --> 01:18:25.300]   If it were banana colored, I might.
[01:18:25.300 --> 01:18:27.180]   Is that your thing?
[01:18:27.180 --> 01:18:28.180]   Yeah, banana phones.
[01:18:28.180 --> 01:18:29.180]   I love banana.
[01:18:29.180 --> 01:18:30.180]   Banana phone?
[01:18:30.180 --> 01:18:31.180]   Yeah.
[01:18:31.180 --> 01:18:32.180]   The Nokia banana...
[01:18:32.180 --> 01:18:33.180]   You have the Nokia banana phone?
[01:18:33.180 --> 01:18:34.180]   No, I didn't.
[01:18:34.180 --> 01:18:35.180]   I almost did, though.
[01:18:35.180 --> 01:18:36.180]   I came close.
[01:18:36.180 --> 01:18:37.180]   Who do we know that bought the banana phone?
[01:18:37.180 --> 01:18:38.180]   Somebody.
[01:18:38.180 --> 01:18:39.180]   Um...
[01:18:39.180 --> 01:18:43.540]   So, I don't...
[01:18:43.540 --> 01:18:47.140]   This was widely mocked, but I thought since I have such smart people on the panel, I'd
[01:18:47.140 --> 01:18:49.540]   ask you guys.
[01:18:49.540 --> 01:18:55.900]   Seth, Harry, Jason, Allen Institute, which is the Institute for Artificial Intelligence,
[01:18:55.900 --> 01:19:04.420]   Paul Allen, I presume, since it's in Seattle, unveiled a new AI that passed an eighth-grade
[01:19:04.420 --> 01:19:05.420]   science test.
[01:19:05.420 --> 01:19:09.660]   I actually did better on an eighth-grade scientist than I did on my sleep test.
[01:19:09.660 --> 01:19:10.660]   In fact...
[01:19:10.660 --> 01:19:11.900]   That's a low bar.
[01:19:11.900 --> 01:19:12.900]   I know.
[01:19:12.900 --> 01:19:13.900]   I know.
[01:19:13.900 --> 01:19:20.100]   Maybe let's give you some sample questions, see if you can do as well as eristo.
[01:19:20.100 --> 01:19:24.460]   Which cell structure do nutrients pass through to enter a cell?
[01:19:24.460 --> 01:19:28.060]   The membrane chloroplacidoplasm or nucleus?
[01:19:28.060 --> 01:19:30.060]   I'm going to say cell membrane.
[01:19:30.060 --> 01:19:31.060]   Correct.
[01:19:31.060 --> 01:19:32.580]   Eristo got that as well.
[01:19:32.580 --> 01:19:38.220]   Which equipment will best separate a mixture of iron filings and black pepper?
[01:19:38.220 --> 01:19:40.180]   That's going to come in handy.
[01:19:40.180 --> 01:19:43.900]   Magnet, filter paper, triple being balance or voltmeter.
[01:19:43.900 --> 01:19:44.900]   Um...
[01:19:44.900 --> 01:19:45.900]   Magnet...
[01:19:45.900 --> 01:19:46.900]   Magnet...
[01:19:46.900 --> 01:19:47.900]   With food as being mechanically...
[01:19:47.900 --> 01:19:48.900]   This is...
[01:19:48.900 --> 01:19:49.900]   You know what?
[01:19:49.900 --> 01:19:52.260]   Okay, an eighth-gradeer I could see this is a good test, but this is impressive for an
[01:19:52.260 --> 01:19:53.660]   AI to know this.
[01:19:53.660 --> 01:19:54.660]   Right?
[01:19:54.660 --> 01:20:00.580]   When food is being mechanically digested, whatever that is, it's being changed into another substance
[01:20:00.580 --> 01:20:04.900]   is being made smaller in size, being converted into energy, is being excreted from the body.
[01:20:04.900 --> 01:20:06.900]   I have no idea.
[01:20:06.900 --> 01:20:09.420]   Inverting it to be made smaller in size.
[01:20:09.420 --> 01:20:10.420]   Smaller?
[01:20:10.420 --> 01:20:11.420]   Really?
[01:20:11.420 --> 01:20:13.460]   Or about changing to another substance?
[01:20:13.460 --> 01:20:15.100]   No, it's being made smaller in size.
[01:20:15.100 --> 01:20:16.420]   Alright, you're right.
[01:20:16.420 --> 01:20:18.260]   That's why he went to Hoffa.
[01:20:18.260 --> 01:20:20.420]   This system...
[01:20:20.420 --> 01:20:25.580]   This question is the only one eristo missed, it gets converted into energy.
[01:20:25.580 --> 01:20:31.540]   According to New York State, which should know, it's made smaller in size.
[01:20:31.540 --> 01:20:35.460]   In New York State, speaking of which, there is a greater chance of precipitation falling
[01:20:35.460 --> 01:20:42.940]   as snow in January than in March because January in the Northern Hemisphere is tilted
[01:20:42.940 --> 01:20:46.980]   toward the sun where temperatures are warmer, toward the sun where temperatures are colder,
[01:20:46.980 --> 01:20:55.420]   away from the sun and temperatures are warmer away from the sun and temperatures are colder.
[01:20:55.420 --> 01:20:58.380]   One would think it's colder, right?
[01:20:58.380 --> 01:21:03.180]   So we got to eliminate warmer away from the sun and temperatures are colder.
[01:21:03.180 --> 01:21:06.660]   Yes, eristo got that right and so did you, you're so smart.
[01:21:06.660 --> 01:21:09.980]   The movement of the liquid in the thermometer shows changes in the temperature and increase
[01:21:09.980 --> 01:21:14.300]   in temperature indicates the molecule and the liquid moved slower and closer together.
[01:21:14.300 --> 01:21:18.100]   This is an increase in temperature, move faster and spread farther apart, contract it in
[01:21:18.100 --> 01:21:20.740]   size when he expanded in size when he...
[01:21:20.740 --> 01:21:23.900]   Well move faster and spread farther apart, right?
[01:21:23.900 --> 01:21:28.580]   Well I almost passed, eristo got that right.
[01:21:28.580 --> 01:21:35.900]   So is this impressive or is this just more Jim Krakery like IBM's Deep Blue?
[01:21:35.900 --> 01:21:36.980]   Are you impressed?
[01:21:36.980 --> 01:21:42.940]   It seems to me those are hard questions for an AI because they imply, un-understanding
[01:21:42.940 --> 01:21:46.060]   of a physical universe the AI doesn't even exist in.
[01:21:46.060 --> 01:21:49.180]   Well it's an accomplishment and synthesis of information.
[01:21:49.180 --> 01:21:51.580]   It doesn't mean it doesn't mean true understanding any of that.
[01:21:51.580 --> 01:21:56.540]   But it is good at understanding the question and synthesizing enough information to get
[01:21:56.540 --> 01:22:00.340]   it right which is valuable long term.
[01:22:00.340 --> 01:22:05.060]   It seems like there's a huge business opportunity in computers synthesizing vast amounts of
[01:22:05.060 --> 01:22:08.580]   information and grinding them down.
[01:22:08.580 --> 01:22:14.100]   Engineer in our chat room says it isn't in fact a better understanding of the physical
[01:22:14.100 --> 01:22:17.700]   world but it's better language models driven by neural networks.
[01:22:17.700 --> 01:22:23.340]   Eristo is actually based on a neural network technology called BERT that was created by
[01:22:23.340 --> 01:22:29.140]   Google and it was instructed to read thousands of articles and books through which it learned
[01:22:29.140 --> 01:22:30.140]   about the patterns.
[01:22:30.140 --> 01:22:31.660]   That makes sense.
[01:22:31.660 --> 01:22:34.740]   I mean it's really the training, right?
[01:22:34.740 --> 01:22:40.700]   The important thing here is that they've figured out how to build AI better, not so
[01:22:40.700 --> 01:22:42.540]   much its answers.
[01:22:42.540 --> 01:22:46.500]   The answers are important because they indicate how the machine learning has been trained
[01:22:46.500 --> 01:22:52.620]   but without being able to apply that elsewhere then it's not really going to have much of
[01:22:52.620 --> 01:22:56.300]   an impact but if they've figured out how to take the way that they've trained this particular
[01:22:56.300 --> 01:23:05.980]   AI to apply to other machine learning instances then they'll have something that's "smarter"
[01:23:05.980 --> 01:23:13.060]   but shows that we've grown in how we're building AI's because we don't want them to identify
[01:23:13.060 --> 01:23:19.140]   to do all the racist stuff that Google AI was doing with photos and all these other bias
[01:23:19.140 --> 01:23:20.980]   mistakes.
[01:23:20.980 --> 01:23:25.940]   These were multiple choice questions which means that it was trained to do multiple choices.
[01:23:25.940 --> 01:23:29.820]   Which is a lot easier than if you asked them the same questions and did not give them four
[01:23:29.820 --> 01:23:32.740]   things to choose from.
[01:23:32.740 --> 01:23:38.340]   It's very good what AI is really good at and far better than humans is pattern recognition.
[01:23:38.340 --> 01:23:45.900]   So things like this it can do far better and so it's really good at enhancing humans.
[01:23:45.900 --> 01:23:51.900]   Not so good at replacing for many things where you have gray areas where you need to understand
[01:23:51.900 --> 01:23:57.220]   context where you need to understand and make judgment call on ambiguity.
[01:23:57.220 --> 01:24:02.540]   And so one of the dirty little secrets about big data is a lot of companies that are working
[01:24:02.540 --> 01:24:10.100]   on it the most are hiring armies of humans to do all that sorting and collecting and deciding
[01:24:10.100 --> 01:24:13.860]   on the ambiguities, deciding on the gray areas, making those decisions because the computers
[01:24:13.860 --> 01:24:19.620]   aren't very good at it and they sort and organize the data and then you have the AI crunching
[01:24:19.620 --> 01:24:22.660]   right and it's super fast at those things.
[01:24:22.660 --> 01:24:32.420]   And so yes there are some of these AI's going to replace certain types of work that involve
[01:24:32.420 --> 01:24:35.740]   where you have some of that basic crunching because it's faster.
[01:24:35.740 --> 01:24:42.140]   It's also going to create more different kinds of jobs and that's a lot of what this discussion
[01:24:42.140 --> 01:24:47.660]   almost always seems to boil down to is like when is the AI going to take my job.
[01:24:47.660 --> 01:24:50.420]   And so I think that's the thing to understand about it.
[01:24:50.420 --> 01:24:57.940]   You know AI still is not good and in our lifetime is not likely to be very good at understanding
[01:24:57.940 --> 01:25:04.660]   ambiguity, understanding context and that's where interestingly enough people who can mix
[01:25:04.660 --> 01:25:13.140]   liberal arts where which is all about those things with some types of data science and
[01:25:13.140 --> 01:25:16.300]   that type of work have become extremely valuable.
[01:25:16.300 --> 01:25:21.020]   Jason you've seen that on the internet people, human people, we think they're human but they
[01:25:21.020 --> 01:25:25.460]   are terrible at understanding context and ambiguity.
[01:25:25.460 --> 01:25:29.340]   Maybe they're Russian troll bots.
[01:25:29.340 --> 01:25:32.940]   No they don't even understand sarcasm.
[01:25:32.940 --> 01:25:37.020]   You have to put a little smiley face so they'll know you're joking.
[01:25:37.020 --> 01:25:38.020]   And these are your friends.
[01:25:38.020 --> 01:25:39.380]   And these are people with brains.
[01:25:39.380 --> 01:25:46.180]   Yeah I think when you read stuff like this you don't get over optimistic about the future
[01:25:46.180 --> 01:25:49.780]   of AI and by the way at the end of the article the New York Times clearly put this in to
[01:25:49.780 --> 01:25:51.340]   make you all feel better.
[01:25:51.340 --> 01:25:57.700]   They said the same researchers tried to teach artificial intelligence to pass the graduate
[01:25:57.700 --> 01:26:02.940]   record exam which is the exam to get in graduate school and they just couldn't do it.
[01:26:02.940 --> 01:26:09.020]   So no AI's in graduate school but maybe ninth grade maybe.
[01:26:09.020 --> 01:26:13.260]   Our show today actually before we do the ad let's do the promo.
[01:26:13.260 --> 01:26:17.700]   We have a little video we made because you know I know a lot of you listen to this show
[01:26:17.700 --> 01:26:22.580]   but maybe not know about all the other shows we do here on the Twit Network with new shows
[01:26:22.580 --> 01:26:27.300]   coming along with Mike as Sargent he's developing some shows and you're going to be doing a
[01:26:27.300 --> 01:26:31.540]   well I won't say but I'm excited about the show Ants Peru is working on.
[01:26:31.540 --> 01:26:35.500]   Here's what you missed this week we had some fun on Twit.
[01:26:35.500 --> 01:26:37.260]   Previously on Twit.
[01:26:37.260 --> 01:26:41.100]   It uses a wait a minute this isn't a battery.
[01:26:41.100 --> 01:26:44.580]   Wait one minute it looks like an e-prom or something.
[01:26:44.580 --> 01:26:46.100]   So it's a right ROM.
[01:26:46.100 --> 01:26:47.100]   Oh a literal ROM.
[01:26:47.100 --> 01:26:51.100]   Do you think it was CMOS yet or was it what was before CMOS.
[01:26:51.100 --> 01:26:54.100]   Rec Con says CMOS was preceded by Nomos.
[01:26:54.100 --> 01:26:59.300]   So no no no no security now.
[01:26:59.300 --> 01:27:04.620]   Google has expanded the scope of their bug bounty program as we know Google has deep
[01:27:04.620 --> 01:27:11.260]   pockets and it's nice to see them using some of that to further strengthen the play store.
[01:27:11.260 --> 01:27:12.260]   Mac break weekly.
[01:27:12.260 --> 01:27:14.260]   I have not been invited to the iPhone.
[01:27:14.260 --> 01:27:17.260]   But Renee Rich he got his invite right from the sector.
[01:27:17.260 --> 01:27:18.260]   Did you get yours Renee?
[01:27:18.260 --> 01:27:19.260]   I did.
[01:27:19.260 --> 01:27:22.940]   I just sit around outside and wait for cool people to come talk to me.
[01:27:22.940 --> 01:27:23.940]   That's depressing.
[01:27:23.940 --> 01:27:25.900]   I said with that puppy dog face.
[01:27:25.900 --> 01:27:28.340]   That's so depressing.
[01:27:28.340 --> 01:27:29.500]   Tech news weekly.
[01:27:29.500 --> 01:27:33.820]   In this time of heightened awareness around autonomous technologies it makes more sense
[01:27:33.820 --> 01:27:38.300]   than ever that we would begin to hear about technologies designed for war and battle that
[01:27:38.300 --> 01:27:40.460]   are driven by autonomous systems.
[01:27:40.460 --> 01:27:46.060]   One of the big shifts here is the defensive use of automation to offensive use.
[01:27:46.060 --> 01:27:50.860]   If we can go beyond radar if we can do things like using video feeds to understand potential
[01:27:50.860 --> 01:27:55.900]   targets now we can look at offensive applications that are much more extensive.
[01:27:55.900 --> 01:27:56.900]   Twit.
[01:27:56.900 --> 01:27:58.980]   Line 10 print I love to it.
[01:27:58.980 --> 01:28:01.900]   Line 20 go to line 10.
[01:28:01.900 --> 01:28:02.900]   Wow.
[01:28:02.900 --> 01:28:06.420]   Jim Cutler did a little basic programming right in the announce there.
[01:28:06.420 --> 01:28:07.420]   That's impressive.
[01:28:07.420 --> 01:28:08.420]   Impressive as heck.
[01:28:08.420 --> 01:28:09.420]   You're a stadium model 100.
[01:28:09.420 --> 01:28:11.660]   Like be still my beating heart.
[01:28:11.660 --> 01:28:13.180]   It's in the other room.
[01:28:13.180 --> 01:28:14.180]   It's yours.
[01:28:14.180 --> 01:28:15.180]   I have two of them.
[01:28:15.180 --> 01:28:16.180]   Oh, you already have two.
[01:28:16.180 --> 01:28:18.820]   Yeah, that was the original model 200 which you gave me.
[01:28:18.820 --> 01:28:20.100]   I gave you a model 200.
[01:28:20.100 --> 01:28:21.300]   You moved or one of your me.
[01:28:21.300 --> 01:28:22.300]   Oh wow.
[01:28:22.300 --> 01:28:23.940]   You're me yesterday to do over to me.
[01:28:23.940 --> 01:28:24.940]   This is depressing.
[01:28:24.940 --> 01:28:28.420]   We're actually getting back stuff we gave away the last time we moved.
[01:28:28.420 --> 01:28:30.940]   This is not going to end well.
[01:28:30.940 --> 01:28:32.660]   Please don't send us your old computers.
[01:28:32.660 --> 01:28:33.660]   Please.
[01:28:33.660 --> 01:28:34.660]   I beg of you.
[01:28:34.660 --> 01:28:35.660]   I've got this box of cables.
[01:28:35.660 --> 01:28:36.660]   No, no, no.
[01:28:36.660 --> 01:28:38.380]   I don't want your cable box.
[01:28:38.380 --> 01:28:39.660]   Hey, by the way, Steve Gibson.
[01:28:39.660 --> 01:28:41.580]   You saw a little bit of Steve Gibson security now.
[01:28:41.580 --> 01:28:43.940]   We are going to Boston.
[01:28:43.940 --> 01:28:46.700]   We're going to do an event for LastPass.
[01:28:46.700 --> 01:28:51.500]   And Steve and I will be joined by Bill Chezwick, Chez, who is the guy who invented the firewall
[01:28:51.500 --> 01:28:52.500]   at Bell Labs.
[01:28:52.500 --> 01:28:54.340]   He's a really great security researcher.
[01:28:54.340 --> 01:28:57.100]   And Jerry Bookell, who is the CISO of LogMe.
[01:28:57.100 --> 01:29:03.380]   And we're going to do an event in Boston on October 3rd at 3.45 in the afternoon at the
[01:29:03.380 --> 01:29:04.380]   Intercontinental Hotel.
[01:29:04.380 --> 01:29:07.140]   If you'd like to go, it is free.
[01:29:07.140 --> 01:29:12.260]   And in fact, everybody who attends will get a $100 token to donate to the charity of your
[01:29:12.260 --> 01:29:14.580]   choice, which is really, really cool.
[01:29:14.580 --> 01:29:21.100]   Our topic, authentication, passwords, and why they're so bad and what's coming next.
[01:29:21.100 --> 01:29:23.580]   Cyber security and identity trends unlocked.
[01:29:23.580 --> 01:29:27.900]   If you want to sign up, there is some limited space, but I wanted to announce it on this
[01:29:27.900 --> 01:29:30.420]   show so that you have a chance to go.
[01:29:30.420 --> 01:29:33.700]   You've got to be in Boston on October 3rd, so don't sign up if you're not going to be
[01:29:33.700 --> 01:29:34.700]   in Boston.
[01:29:34.700 --> 01:29:40.420]   Twit.to, that's our URL short, and twit.to/unlocked.
[01:29:40.420 --> 01:29:45.420]   If you're interested, we will have, we still have some seats left at that event, Thursday,
[01:29:45.420 --> 01:29:49.300]   October 3rd at the Intercontinental Hotel in Boston.
[01:29:49.300 --> 01:29:53.940]   Steve will be talking no doubt about Squirrel, his authentication platform.
[01:29:53.940 --> 01:29:58.340]   Chez just wrote a great piece on why passwords suck.
[01:29:58.340 --> 01:30:00.220]   And I think it's going to be really interesting.
[01:30:00.220 --> 01:30:02.620]   What's next for authentication?
[01:30:02.620 --> 01:30:08.420]   Our show today brought to you by the cloud, the hot cloud, the wasabi hot cloud storage.
[01:30:08.420 --> 01:30:11.020]   Yeah, we were talking about this last night.
[01:30:11.020 --> 01:30:15.380]   We were talking about my two buddies, Jeff Flowers and David Friend.
[01:30:15.380 --> 01:30:21.020]   They founded Carbonite, Jeff's the CTO, David's the CEO.
[01:30:21.020 --> 01:30:24.660]   And Jeff had patented, actually, together they patented a really interesting way of
[01:30:24.660 --> 01:30:32.220]   writing data disks sequentially instead of block by block, which makes it so much faster,
[01:30:32.220 --> 01:30:33.740]   so much more efficient.
[01:30:33.740 --> 01:30:36.780]   That's what was kind of the foundational technology of Carbonite.
[01:30:36.780 --> 01:30:42.940]   And they've now created a new cloud storage company called wasabi that is doing amazing
[01:30:42.940 --> 01:30:45.060]   things.
[01:30:45.060 --> 01:30:48.940]   But the problem is everybody knows Amazon and Google and Microsoft, nobody's ever heard
[01:30:48.940 --> 01:30:52.100]   of wasabi, so I really want to get the word out about this.
[01:30:52.100 --> 01:30:59.500]   Wasabi is a fifth the cost of Amazon's S3, one fifth the cost, and is up to six times
[01:30:59.500 --> 01:31:04.620]   faster, and that's thanks to this revolutionary patented technology they're using.
[01:31:04.620 --> 01:31:09.220]   It's also as secure as you can get, eleven nines of data durability.
[01:31:09.220 --> 01:31:14.620]   They have data integrity checking, so you never lose a bit.
[01:31:14.620 --> 01:31:18.100]   They also have a feature I think is really important that you can set some or all of
[01:31:18.100 --> 01:31:19.340]   your data as immutable.
[01:31:19.340 --> 01:31:24.220]   That means it can't be modified by ransomware or fumble-fingered employees.
[01:31:24.220 --> 01:31:27.020]   So you can say this data is sacred, it's sacrosanct.
[01:31:27.020 --> 01:31:30.180]   It can only be changed if I say it can be changed.
[01:31:30.180 --> 01:31:31.580]   This is really important.
[01:31:31.580 --> 01:31:36.100]   Being able to efficiently and affordably store data in the cloud is important for so many
[01:31:36.100 --> 01:31:37.260]   businesses.
[01:31:37.260 --> 01:31:39.140]   Everybody's moving to the cloud.
[01:31:39.140 --> 01:31:44.780]   And of course nobody ever gets fired by using one of the big three.
[01:31:44.780 --> 01:31:49.780]   But honestly, take the boss this information because I think when you say boss you could
[01:31:49.780 --> 01:31:54.740]   save 80%, it'll be six times faster.
[01:31:54.740 --> 01:31:59.060]   There's no hidden fees for egress or API usage.
[01:31:59.060 --> 01:32:02.140]   That by itself is going to save you a ton of money.
[01:32:02.140 --> 01:32:06.380]   And it supports the Amazon S3 API, so you already know how to use it.
[01:32:06.380 --> 01:32:10.100]   And yes, it's more secure than your on-premises storage.
[01:32:10.100 --> 01:32:11.780]   HIPAA compliant, FINRA compliant.
[01:32:11.780 --> 01:32:13.460]   Look at the companies that are starting to use it.
[01:32:13.460 --> 01:32:15.440]   I think this is really exciting.
[01:32:15.440 --> 01:32:17.060]   You calculate the savings for yourself.
[01:32:17.060 --> 01:32:20.700]   And if you want to bang on it, you can get unlimited storage for a month.
[01:32:20.700 --> 01:32:24.980]   If you go to wasabi.com and click the free trial link, but use the offer code TWIT that
[01:32:24.980 --> 01:32:30.300]   way, you will get an unlimited free trial for 30 days.
[01:32:30.300 --> 01:32:31.780]   And so you can really put data on there.
[01:32:31.780 --> 01:32:32.860]   You can try to change it.
[01:32:32.860 --> 01:32:34.060]   You can see how fast it is.
[01:32:34.060 --> 01:32:36.500]   You can really see what it can do.
[01:32:36.500 --> 01:32:37.500]   Join the movement.
[01:32:37.500 --> 01:32:38.500]   Migrate your data to the cloud.
[01:32:38.500 --> 01:32:42.060]   Do it with confidence and be one of the smart ones.
[01:32:42.060 --> 01:32:44.300]   This is a disruptor in this industry.
[01:32:44.300 --> 01:32:48.500]   And from guys who really know what it's all about, wasabi.com.
[01:32:48.500 --> 01:32:49.500]   Offer code TWIT.
[01:32:49.500 --> 01:32:51.580]   Thank you wasabi.
[01:32:51.580 --> 01:32:53.900]   David and Jeff have been great supporters of TWIT all along.
[01:32:53.900 --> 01:32:55.660]   I love to support him back.
[01:32:55.660 --> 01:33:00.940]   We talked, I think, a couple of weeks ago about a conundrum facing our network.
[01:33:00.940 --> 01:33:05.380]   We had booked an interview with Joey Icho, who was the director of it.
[01:33:05.380 --> 01:33:08.180]   It was, by the way, the director of the MIT Media Lab.
[01:33:08.180 --> 01:33:09.180]   He has a new book out.
[01:33:09.180 --> 01:33:11.260]   Denise Howe was going to interview him.
[01:33:11.260 --> 01:33:18.220]   And then it started to come out that Joey had solicited and taken contributions from
[01:33:18.220 --> 01:33:25.260]   a former convicted child molester, Jeffrey Epstein.
[01:33:25.260 --> 01:33:28.700]   And we were now really conflicted about this interview.
[01:33:28.700 --> 01:33:33.380]   Fortunately, I told Denise that if he comes on, you're going to have to ask him the hard
[01:33:33.380 --> 01:33:34.380]   questions.
[01:33:34.380 --> 01:33:36.300]   We don't normally do that at TWIT.
[01:33:36.300 --> 01:33:37.940]   We don't want to get in confrontation interviews.
[01:33:37.940 --> 01:33:41.180]   But if we're going to interview Joey Icho, who, by the way, I've known for years and
[01:33:41.180 --> 01:33:44.780]   have great respect for, you're going to have to ask them the tough questions.
[01:33:44.780 --> 01:33:48.380]   Why did you take money from Jeffrey Epstein?
[01:33:48.380 --> 01:33:53.260]   Well, a couple of days ago, Ronan Farrow, who's been doing such amazing work for the New
[01:33:53.260 --> 01:34:00.300]   Yorker, published an article based on documents they got from MIT that showed, A, that MIT
[01:34:00.300 --> 01:34:03.860]   had said, "We cannot take any more donations from this guy.
[01:34:03.860 --> 01:34:05.740]   He's a child molester."
[01:34:05.740 --> 01:34:09.260]   And that, B, Joey Icho took steps and it's in writing.
[01:34:09.260 --> 01:34:14.700]   It's in emails to hide the fact that the money was coming from Jeffrey Epstein.
[01:34:14.700 --> 01:34:19.020]   He said, "Any Jeffrey Epstein donation has to be designated anonymous.
[01:34:19.020 --> 01:34:20.220]   They didn't use his name.
[01:34:20.220 --> 01:34:22.060]   They only used initials in meetings.
[01:34:22.060 --> 01:34:24.220]   It was a smoking gun."
[01:34:24.220 --> 01:34:29.260]   And within a few hours of the release of that article, Joey Icho resigned as director
[01:34:29.260 --> 01:34:31.140]   of MIT Media Lab.
[01:34:31.140 --> 01:34:35.340]   A lot of people had, at least two people had left before that.
[01:34:35.340 --> 01:34:37.940]   But that is quite a shocker.
[01:34:37.940 --> 01:34:41.940]   And I think one of the questions we would have asked Icho is, "Why?
[01:34:41.940 --> 01:34:47.620]   Is money so tight that you have to take money from a source like Epstein?"
[01:34:47.620 --> 01:34:52.100]   Apparently, according to the Farrow article in the New Yorker, Epstein went to Bill Gates
[01:34:52.100 --> 01:34:53.620]   and said, "Give money to Media Lab."
[01:34:53.620 --> 01:34:59.060]   He was an intermediary and Bill Gates gave money to Media Lab as well.
[01:34:59.060 --> 01:35:04.340]   Which was somewhat mystifying because it's not like Bill Gates didn't know these guys
[01:35:04.340 --> 01:35:05.340]   in the first place.
[01:35:05.340 --> 01:35:06.340]   He could have.
[01:35:06.340 --> 01:35:09.780]   It's not entirely clear that Jeffrey Epstein needed to be a middle man in this case.
[01:35:09.780 --> 01:35:14.900]   I think this is the subtext of this, that Epstein was using all of this influence and
[01:35:14.900 --> 01:35:19.300]   all of these relationships to scrub his record.
[01:35:19.300 --> 01:35:28.820]   And so that makes complicit in Epstein's attempt to rejuvenate his reputation, an attempt that
[01:35:28.820 --> 01:35:33.020]   failed.
[01:35:33.020 --> 01:35:35.020]   I guess there's nothing more to say about it.
[01:35:35.020 --> 01:35:36.020]   Jason Moore.
[01:35:36.020 --> 01:35:37.700]   It's just a bad...
[01:35:37.700 --> 01:35:44.740]   And again, I think Joe has done great work at Media Lab.
[01:35:44.740 --> 01:35:47.260]   And I had a lot of respect for him.
[01:35:47.260 --> 01:35:50.380]   I'm just disappointed and shocked, frankly.
[01:35:50.380 --> 01:35:54.300]   Is Joe Edo still on the board or whatever his...
[01:35:54.300 --> 01:35:55.300]   At the New York Times?
[01:35:55.300 --> 01:35:57.660]   No, he basically resigned from everything.
[01:35:57.660 --> 01:36:00.300]   He was including digital garage.
[01:36:00.300 --> 01:36:01.300]   That was his startup.
[01:36:01.300 --> 01:36:02.300]   He also took money for that startup.
[01:36:02.300 --> 01:36:07.700]   He took a project and all about what he resigned from the Times and the Knight Foundation and
[01:36:07.700 --> 01:36:08.700]   several others.
[01:36:08.700 --> 01:36:11.700]   He had a gig at Harvard, which he stepped down from.
[01:36:11.700 --> 01:36:13.220]   He left the MacArthur Foundation.
[01:36:13.220 --> 01:36:14.380]   He left the James...
[01:36:14.380 --> 01:36:15.980]   John S. and James L. Knight Foundation.
[01:36:15.980 --> 01:36:17.900]   He left the New York Times board.
[01:36:17.900 --> 01:36:19.820]   He left a visioning professorship at Harvard.
[01:36:19.820 --> 01:36:22.660]   I mean, this is all of it.
[01:36:22.660 --> 01:36:29.140]   Before all this came out, he said that he was his responsibility to lead the healing.
[01:36:29.140 --> 01:36:30.140]   That's a bad sign.
[01:36:30.140 --> 01:36:33.940]   Anybody who's been involved with something says they're going to fix it, it almost never
[01:36:33.940 --> 01:36:34.940]   works out.
[01:36:34.940 --> 01:36:40.940]   In the Fero article, there's a very dramatic set piece where he's speaking to an all-hands
[01:36:40.940 --> 01:36:43.140]   at the Media Lab.
[01:36:43.140 --> 01:36:45.220]   And people are crying.
[01:36:45.220 --> 01:36:47.460]   People are devastated.
[01:36:47.460 --> 01:36:54.180]   And I think also a little bit disturbed by his lack of kind of forth-rightness in all
[01:36:54.180 --> 01:36:55.180]   of this.
[01:36:55.180 --> 01:36:57.980]   Nicholas Negropontai, who I've also had a lot of respect for, does not come out of the
[01:36:57.980 --> 01:36:58.980]   looking good.
[01:36:58.980 --> 01:37:00.140]   He basically said that...
[01:37:00.140 --> 01:37:01.140]   He was an apologist.
[01:37:01.140 --> 01:37:02.140]   Yeah.
[01:37:02.140 --> 01:37:05.980]   And then Eto, apparently, went to him and said, "You're not making things any easier."
[01:37:05.980 --> 01:37:06.980]   Wow.
[01:37:06.980 --> 01:37:13.500]   So, there you go.
[01:37:13.500 --> 01:37:16.740]   I mean, this story is important in our tech community.
[01:37:16.740 --> 01:37:22.060]   It's a small community and everybody knows Edo as well as Medellipani.
[01:37:22.060 --> 01:37:25.660]   And I hope that the Media Lab, which does such great work, does not suffer from this,
[01:37:25.660 --> 01:37:26.660]   but I think this is awesome.
[01:37:26.660 --> 01:37:27.660]   Yeah.
[01:37:27.660 --> 01:37:28.660]   Exactly.
[01:37:28.660 --> 01:37:31.580]   The Media Lab does amazing things.
[01:37:31.580 --> 01:37:37.460]   Also, MIT Technology Review also does amazing work, really respects the people over there.
[01:37:37.460 --> 01:37:38.460]   We just quoted them.
[01:37:38.460 --> 01:37:39.460]   I mean, that's good stuff.
[01:37:39.460 --> 01:37:42.340]   They had some good coverage of this.
[01:37:42.340 --> 01:37:43.820]   I think they're going to skate.
[01:37:43.820 --> 01:37:48.220]   Now, the interesting thing is there's a Netflix documentary called Inside Bill's Brain that
[01:37:48.220 --> 01:37:51.780]   comes out in a couple of weeks, September 20th.
[01:37:51.780 --> 01:37:52.780]   And Fast Company...
[01:37:52.780 --> 01:37:54.300]   I call it David.
[01:37:54.300 --> 01:37:55.300]   David Litsky.
[01:37:55.300 --> 01:37:57.860]   You know, asks an interesting question.
[01:37:57.860 --> 01:37:59.220]   Is Netflix going to pull this?
[01:37:59.220 --> 01:38:02.060]   Does this besmirch Bill Gates as well?
[01:38:02.060 --> 01:38:06.540]   It seems like at the moment it is somewhat unclear what was going on with Gates and this
[01:38:06.540 --> 01:38:07.540]   stuff.
[01:38:07.540 --> 01:38:13.220]   I mean, his people kind of strenuously deny that Epstein was involved in funneling Gates
[01:38:13.220 --> 01:38:18.980]   money to the Media Lab, which is somewhat hard to reconcile on the face of it with the
[01:38:18.980 --> 01:38:20.660]   stuff in the Farrow article.
[01:38:20.660 --> 01:38:21.660]   Yeah.
[01:38:21.660 --> 01:38:31.860]   I worry about that strenuous denial, too, because it was so emphatic that if there are
[01:38:31.860 --> 01:38:36.780]   any loose connections, it makes them look really bad.
[01:38:36.780 --> 01:38:41.940]   David reached out to Netflix, no response.
[01:38:41.940 --> 01:38:43.620]   I was interested in this documentary.
[01:38:43.620 --> 01:38:47.700]   I didn't have high hopes for it because it was kind of under Bill Gates' supervision,
[01:38:47.700 --> 01:38:52.380]   which means it wasn't going to be exactly a hard hitting piece of our Bill Gates, but
[01:38:52.380 --> 01:38:55.220]   still fascinating fellow.
[01:38:55.220 --> 01:38:56.780]   He does amazing stuff now.
[01:38:56.780 --> 01:38:59.220]   He was not very nice to work with.
[01:38:59.220 --> 01:39:05.100]   Any all of us were around in that era and kind of have...
[01:39:05.100 --> 01:39:09.500]   Every time I talk to a tech journalist who covered Gates in that era, including you guys
[01:39:09.500 --> 01:39:15.260]   Paul Thorett, Mary Jo Foley, say, "Man, this guy was not a nice guy.
[01:39:15.260 --> 01:39:21.220]   I remember going to the office in '87 workshop and seeing him rip into somebody who asked
[01:39:21.220 --> 01:39:22.220]   a question."
[01:39:22.220 --> 01:39:23.220]   Wow.
[01:39:23.220 --> 01:39:28.900]   It was at the tail end of Notch a Spill and not all that long thereafter, he became
[01:39:28.900 --> 01:39:31.820]   sweater wearing affable Bill.
[01:39:31.820 --> 01:39:32.820]   What happened?
[01:39:32.820 --> 01:39:33.820]   He'd marry Melinda?
[01:39:33.820 --> 01:39:34.820]   Is that what changed?
[01:39:34.820 --> 01:39:35.820]   He'd gotten no idiot kids.
[01:39:35.820 --> 01:39:36.820]   I mean, that might help change anymore.
[01:39:36.820 --> 01:39:37.820]   I'm tired.
[01:39:37.820 --> 01:39:38.820]   He retired.
[01:39:38.820 --> 01:39:40.460]   He wasn't running the business anymore.
[01:39:40.460 --> 01:39:46.700]   He turned his attention to problems that matter to the world more than selling Office '97.
[01:39:46.700 --> 01:39:47.700]   Yeah.
[01:39:47.700 --> 01:39:48.700]   Yeah.
[01:39:48.700 --> 01:39:50.380]   I mean, talk about rehabilitating your image.
[01:39:50.380 --> 01:39:55.900]   He definitely had an image to rehabilitate and did.
[01:39:55.900 --> 01:39:59.100]   But yeah, I mean, they were in it.
[01:39:59.100 --> 01:40:04.860]   The culture of the company starts with the leader and in the 80s and 90s, they had a
[01:40:04.860 --> 01:40:13.180]   really, really aggressive, amoral culture there that was toxic.
[01:40:13.180 --> 01:40:19.380]   And yeah, it was known in the industry and having known people that worked there.
[01:40:19.380 --> 01:40:22.140]   It was a brutal place to be sometimes.
[01:40:22.140 --> 01:40:26.180]   So I was, yeah, NFL is back.
[01:40:26.180 --> 01:40:27.180]   We're excited.
[01:40:27.180 --> 01:40:30.460]   Thursday night football, there'll be football today and tonight.
[01:40:30.460 --> 01:40:33.340]   I should check and see how our Niners did.
[01:40:33.340 --> 01:40:34.340]   No.
[01:40:34.340 --> 01:40:37.980]   It says, don't check.
[01:40:37.980 --> 01:40:41.540]   No, I'm surprised.
[01:40:41.540 --> 01:40:51.380]   But I also saw a lot of ads on the Thursday night game for Verizon rolling out 5G into
[01:40:51.380 --> 01:40:54.500]   13 NFL stadiums.
[01:40:54.500 --> 01:41:02.020]   Then I read this Venture Beat article by Jeremy Horowitz, Verizon's bizarre rollout 5G rollout
[01:41:02.020 --> 01:41:06.660]   now covers some seats in 13 NFL stadiums.
[01:41:06.660 --> 01:41:12.180]   They don't actually identify, they identify all but one of the stadiums.
[01:41:12.180 --> 01:41:17.020]   5G will be concentrated parts of the seating areas but could be available in other locations
[01:41:17.020 --> 01:41:18.820]   in and around the stadium as well.
[01:41:18.820 --> 01:41:24.380]   This is not the impression one would have gotten from the ad.
[01:41:24.380 --> 01:41:29.020]   If you have a 5G phone, I mean, my thought originally was, oh, great, I'll go watch a
[01:41:29.020 --> 01:41:37.060]   football game, but you have to be in the right seat at certain stadiums and maybe it won't
[01:41:37.060 --> 01:41:40.860]   be on the whole time and this is just more 5G fun.
[01:41:40.860 --> 01:41:45.620]   This is a metaphor for the state of 5G for sure.
[01:41:45.620 --> 01:41:51.540]   We've done a lot of testing all over the place, you know, from Australia to South Korea to
[01:41:51.540 --> 01:41:57.260]   the UK to all across the US and it's great when it works.
[01:41:57.260 --> 01:41:58.660]   But it's so inconsistent.
[01:41:58.660 --> 01:42:05.340]   I feel like just the rollout of this is just too soon and it's like over promising under
[01:42:05.340 --> 01:42:07.740]   delivering in a big way.
[01:42:07.740 --> 01:42:14.260]   It's so bad in the US, is it at least good in South Korea or other places?
[01:42:14.260 --> 01:42:15.580]   No.
[01:42:15.580 --> 01:42:16.580]   It's not much better.
[01:42:16.580 --> 01:42:20.340]   The bottom line is no, it's not much better.
[01:42:20.340 --> 01:42:22.780]   Is it because you have to add so many towers?
[01:42:22.780 --> 01:42:25.300]   Is that part of the problem or is it?
[01:42:25.300 --> 01:42:29.900]   I mean, I'm starting to think maybe this is the why max of our generation.
[01:42:29.900 --> 01:42:31.060]   Is this about technology?
[01:42:31.060 --> 01:42:32.540]   No, it's not that.
[01:42:32.540 --> 01:42:33.540]   It's not that.
[01:42:33.540 --> 01:42:34.980]   It's just that it's not built out.
[01:42:34.980 --> 01:42:41.020]   You know, to get the what we think of as 5G, we really think of as millimeter wave and
[01:42:41.020 --> 01:42:47.340]   that to your point, Leo, you have to have a ton of access points everywhere all over
[01:42:47.340 --> 01:42:49.340]   the place, right?
[01:42:49.340 --> 01:43:00.740]   And then the sort of the 5G, the light is the version that is about half as fast but acts
[01:43:00.740 --> 01:43:07.100]   in a tower scenario much similar to like 4G today.
[01:43:07.100 --> 01:43:08.580]   And it's still super fast, right?
[01:43:08.580 --> 01:43:17.660]   We're just still talking 400, you know, 400 megabits if you're the only one using it.
[01:43:17.660 --> 01:43:20.020]   Right, of course.
[01:43:20.020 --> 01:43:25.140]   But with all these technologies, they're really fast until everybody starts using it.
[01:43:25.140 --> 01:43:28.380]   Yes, Sub-6 is the one that that one, right?
[01:43:28.380 --> 01:43:37.180]   So Sub-6 is a big upgrade from 4G LTE networks, although some of those networks are already
[01:43:37.180 --> 01:43:39.340]   pushing up there, right?
[01:43:39.340 --> 01:43:41.020]   It's just going to take that more consistent.
[01:43:41.020 --> 01:43:48.060]   Millimeter wave is a much more extensive build out and it's only going to make sense where
[01:43:48.060 --> 01:43:52.300]   there's a lot of density, population density, right?
[01:43:52.300 --> 01:43:58.100]   So it just takes time and it's going to take a lot more time than sort of these early ones
[01:43:58.100 --> 01:44:02.460]   are like, here's one corner where you can go and try out the millimeter wave just to
[01:44:02.460 --> 01:44:05.020]   see what it's going to be like.
[01:44:05.020 --> 01:44:08.820]   But as soon as you move, you know, across the street, you'll lose your connection.
[01:44:08.820 --> 01:44:09.820]   So whatever you do.
[01:44:09.820 --> 01:44:12.620]   So don't buy a 5G phone at this point.
[01:44:12.620 --> 01:44:15.820]   They should not be marketing this yet.
[01:44:15.820 --> 01:44:17.300]   It's too soon.
[01:44:17.300 --> 01:44:18.300]   Yeah.
[01:44:18.300 --> 01:44:19.300]   Okay.
[01:44:19.300 --> 01:44:20.300]   It should be beta software.
[01:44:20.300 --> 01:44:26.540]   It should be marketed like beta software, you know, and buyer beware.
[01:44:26.540 --> 01:44:27.540]   Early or out?
[01:44:27.540 --> 01:44:28.540]   It might be...
[01:44:28.540 --> 01:44:29.540]   I think that's true.
[01:44:29.540 --> 01:44:36.140]   Am I misremembering or is 5G really mostly for like enterprise use for within organizations?
[01:44:36.140 --> 01:44:39.460]   A lot of the cool stuff is enterprise.
[01:44:39.460 --> 01:44:42.580]   It should be really cool for cars eventually.
[01:44:42.580 --> 01:44:47.700]   In terms of making your phone better, it's always nice to have a faster phone.
[01:44:47.700 --> 01:44:49.180]   That's probably will not be a true phone.
[01:44:49.180 --> 01:44:50.580]   I feel like LTE is pretty fast.
[01:44:50.580 --> 01:44:52.500]   It's fast enough for anything I want to do.
[01:44:52.500 --> 01:44:55.220]   But yeah, there are all kinds of business things where this is going to be valuable
[01:44:55.220 --> 01:44:56.540]   to get real-time data.
[01:44:56.540 --> 01:44:58.020]   It's low latency is the other thing.
[01:44:58.020 --> 01:44:59.780]   It's not really speed, right?
[01:44:59.780 --> 01:45:00.860]   It's low latency.
[01:45:00.860 --> 01:45:08.700]   If your equipment on your factory floor can be streaming data, low latency, and tell you
[01:45:08.700 --> 01:45:12.340]   when it's about to break down before it has, that kind of stuff is super valuable.
[01:45:12.340 --> 01:45:13.340]   Right.
[01:45:13.340 --> 01:45:15.700]   So more nodes connecting to the network.
[01:45:15.700 --> 01:45:17.660]   So your points that that is...
[01:45:17.660 --> 01:45:21.780]   There is a lot of really good enterprise applications where you're going to have a lot
[01:45:21.780 --> 01:45:28.540]   more things connect to the network, you know, from stoplights to water meters to power meters,
[01:45:28.540 --> 01:45:30.780]   you know, all kinds of stuff.
[01:45:30.780 --> 01:45:33.140]   So it's going to have a big use for that.
[01:45:33.140 --> 01:45:37.460]   I think in consumer where you're going to see the most interest in it is like mobile
[01:45:37.460 --> 01:45:38.460]   gaming.
[01:45:38.460 --> 01:45:42.580]   So the timing on something like Apple Arcade and Google Stadia is going to be pretty good
[01:45:42.580 --> 01:45:45.100]   because those things are by team.
[01:45:45.100 --> 01:45:50.140]   Clearly that's what they're being built out for, right?
[01:45:50.140 --> 01:45:51.620]   Is these improved networks?
[01:45:51.620 --> 01:45:54.940]   Although I guess, you know, Stadia at home would be pretty good.
[01:45:54.940 --> 01:45:55.940]   Yep.
[01:45:55.940 --> 01:45:56.940]   Well, some...
[01:45:56.940 --> 01:46:00.780]   And Apple TV... or sorry, Apple Arcade, you can download the games.
[01:46:00.780 --> 01:46:02.980]   So you just need a good connection at home.
[01:46:02.980 --> 01:46:03.980]   Yeah, you can play offline.
[01:46:03.980 --> 01:46:04.980]   You can play offline.
[01:46:04.980 --> 01:46:05.980]   I love that about Arcade.
[01:46:05.980 --> 01:46:07.740]   I didn't mention that.
[01:46:07.740 --> 01:46:09.140]   That's smart.
[01:46:09.140 --> 01:46:11.900]   This story...
[01:46:11.900 --> 01:46:17.740]   I'm a fan of the idea of gun control, but this is going, I think, a little too far.
[01:46:17.740 --> 01:46:23.500]   According to Forbes, Thomas Brewster at Forbes, the feds have demanded that Apple and Google
[01:46:23.500 --> 01:46:31.140]   hand over the names, phone numbers, and other data of people who have downloaded an app,
[01:46:31.140 --> 01:46:38.380]   an app that's designed to tune up a gun scope.
[01:46:38.380 --> 01:46:41.180]   It's the ATM Obsidian 4 application.
[01:46:41.180 --> 01:46:46.380]   You have to have a scope manufactured by the company.
[01:46:46.380 --> 01:46:51.500]   And then you can use it to get streaming video from the scope, but you can also use it to
[01:46:51.500 --> 01:46:54.420]   calibrate the scope.
[01:46:54.420 --> 01:46:59.100]   According to the Play Store, 10,000 users, at least 10,000 users have downloaded the
[01:46:59.100 --> 01:47:00.540]   app.
[01:47:00.540 --> 01:47:04.380]   On September 5th, the DOJ requested information.
[01:47:04.380 --> 01:47:10.580]   They actually have it's called Obsidian 4 from American Technologies Network.
[01:47:10.580 --> 01:47:13.420]   They asked for information about everybody who downloaded it.
[01:47:13.420 --> 01:47:17.060]   10,000 people on Google Play, Apple doesn't provide download numbers, so it's not clear
[01:47:17.060 --> 01:47:19.020]   how many iPhone people.
[01:47:19.020 --> 01:47:20.700]   The court has yet to approve the demand.
[01:47:20.700 --> 01:47:21.700]   I hope they don't.
[01:47:21.700 --> 01:47:27.020]   This is to me the scary, terrifying fishing expedition.
[01:47:27.020 --> 01:47:32.540]   If government starts saying, "Well, I want to know everybody who downloaded that app,"
[01:47:32.540 --> 01:47:34.580]   that's a real problem.
[01:47:34.580 --> 01:47:35.580]   That's a witch hunt.
[01:47:35.580 --> 01:47:41.060]   And yeah, there's just not a really good legal basis in the United States for that.
[01:47:41.060 --> 01:47:46.940]   So we'll watch to see what the court says in response to this application.
[01:47:46.940 --> 01:47:53.260]   It's a little disturbing that the DOJ even asked for it, but let's hope the court holds
[01:47:53.260 --> 01:47:54.260]   strong.
[01:47:54.260 --> 01:48:02.420]   And then, of course, the question if the court say, "Okay, what will Apple and Google do?"
[01:48:02.420 --> 01:48:05.660]   If the court signs off and the order Apple and Google will be told to hand over not just
[01:48:05.660 --> 01:48:10.860]   the names of anyone who's downloaded the app since 2017, but their telephone numbers
[01:48:10.860 --> 01:48:16.580]   and their IP addresses, they also want to know what users are operating the app.
[01:48:16.580 --> 01:48:21.020]   And now perhaps this gun site was used in some crime.
[01:48:21.020 --> 01:48:26.660]   We don't even know that, but certainly not all of the tens of thousands of people downloaded
[01:48:26.660 --> 01:48:28.140]   the app are criminals.
[01:48:28.140 --> 01:48:29.140]   Exactly.
[01:48:29.140 --> 01:48:39.060]   If that's the case, then get a warrant specifically involving that crime or involving those things.
[01:48:39.060 --> 01:48:43.900]   This has happened before with other governments.
[01:48:43.900 --> 01:48:48.820]   An unnamed government asked Apple for data for 58 million users of a single app.
[01:48:48.820 --> 01:48:50.260]   They were trying to trace a terrorist.
[01:48:50.260 --> 01:48:53.860]   So Apple said, "No," and was not compelled to do so.
[01:48:53.860 --> 01:48:54.860]   But this is a little different.
[01:48:54.860 --> 01:49:00.100]   It's a US court, so the US Department of Justice would be hard for Apple to say, "No,
[01:49:00.100 --> 01:49:01.100]   I think."
[01:49:01.100 --> 01:49:04.940]   There is another flaw in the Super Micro.
[01:49:04.940 --> 01:49:11.700]   We never did hear whether Bloomberg nailed it with the Super Micro flaw last year.
[01:49:11.700 --> 01:49:16.020]   Everybody denied it, and Bloomberg never really proved it.
[01:49:16.020 --> 01:49:19.620]   So I'm going to have to assume that that story was an error.
[01:49:19.620 --> 01:49:20.620]   But this is a new one.
[01:49:20.620 --> 01:49:24.420]   This is from Wired, a newly discovered vulnerability in Super Micro hardware, brings the threat
[01:49:24.420 --> 01:49:27.140]   of malicious USBs to corporate servers.
[01:49:27.140 --> 01:49:33.700]   Seth, it sounds like you can, in effect, mount a USB drive remotely over the internet using
[01:49:33.700 --> 01:49:34.700]   this flaw.
[01:49:34.700 --> 01:49:35.700]   Yes.
[01:49:35.700 --> 01:49:36.700]   That's bizarre.
[01:49:36.700 --> 01:49:38.540]   That's what it sounds like.
[01:49:38.540 --> 01:49:40.380]   Plug in a virtual thumb drive.
[01:49:40.380 --> 01:49:47.860]   I do want to throw in, though, that this is very, very different from Bloomberg's Super
[01:49:47.860 --> 01:49:48.860]   Micro story.
[01:49:48.860 --> 01:49:50.620]   Yes, that was a little bit of hardware.
[01:49:50.620 --> 01:49:52.700]   I don't want people conflating those two.
[01:49:52.700 --> 01:49:53.980]   No, no, no, no, no, no.
[01:49:53.980 --> 01:50:01.540]   Because even if Bloomberg's big picture for that story was wrong, there's a lot of parts
[01:50:01.540 --> 01:50:04.260]   of it that may have been correct.
[01:50:04.260 --> 01:50:06.260]   And anyway.
[01:50:06.260 --> 01:50:09.900]   It sounds like you have some information on this Bloomberg story.
[01:50:09.900 --> 01:50:10.900]   You want to talk about it?
[01:50:10.900 --> 01:50:11.900]   You want to talk about it?
[01:50:11.900 --> 01:50:13.540]   Nothing that I can say at this point.
[01:50:13.540 --> 01:50:19.500]   I'll say that it's been very hard for, I think, everybody who's looked at last year's
[01:50:19.500 --> 01:50:26.460]   Super Micro story to verify what Bloomberg wrote as they wrote it.
[01:50:26.460 --> 01:50:35.740]   Nevertheless, the outline of the exploit is completely credible and a supply chain sure
[01:50:35.740 --> 01:50:36.740]   in writing.
[01:50:36.740 --> 01:50:37.940]   In theory?
[01:50:37.940 --> 01:50:42.420]   The other things that people brought up last year was there are so many flaws in Super
[01:50:42.420 --> 01:50:43.420]   Micro's firmware.
[01:50:43.420 --> 01:50:49.420]   Here's another one that who needs to put a rice-sized grain of chip on the motherboard.
[01:50:49.420 --> 01:50:51.940]   We got the firmware we could mess with.
[01:50:51.940 --> 01:50:54.340]   And here's another positive.
[01:50:54.340 --> 01:51:00.980]   And that's sort of the point of this story is that it could happen.
[01:51:00.980 --> 01:51:02.940]   It's very enterprise-focused.
[01:51:02.940 --> 01:51:05.020]   This is not something that the average consumer needs to know about.
[01:51:05.020 --> 01:51:08.020]   I have a Super Micro motherboard at home, but I don't have to worry about that.
[01:51:08.020 --> 01:51:09.460]   Do you have a connected to anything?
[01:51:09.460 --> 01:51:11.260]   Yeah, it's my Minecraft server.
[01:51:11.260 --> 01:51:12.260]   Oh.
[01:51:12.260 --> 01:51:14.420]   Can you do after we're about somebody hacking my mind?
[01:51:14.420 --> 01:51:15.620]   Is it next to your Bitcoin?
[01:51:15.620 --> 01:51:16.620]   Oh, yeah.
[01:51:16.620 --> 01:51:17.620]   It's in the network.
[01:51:17.620 --> 01:51:19.900]   Yeah, it's everything's there.
[01:51:19.900 --> 01:51:20.900]   So I worry?
[01:51:20.900 --> 01:51:22.380]   I don't know.
[01:51:22.380 --> 01:51:24.620]   That's the thing you should be worried about.
[01:51:24.620 --> 01:51:26.860]   Oh, man.
[01:51:26.860 --> 01:51:29.060]   It's a public-facing Minecraft server.
[01:51:29.060 --> 01:51:31.380]   It's running on a Super Micro motherboard.
[01:51:31.380 --> 01:51:32.380]   Okay.
[01:51:32.380 --> 01:51:37.100]   Well, I thought I needed a Xeon and a lot of RAM because I want to really have a beefy
[01:51:37.100 --> 01:51:39.420]   Minecraft server.
[01:51:39.420 --> 01:51:40.420]   The kids use it.
[01:51:40.420 --> 01:51:41.420]   Sure.
[01:51:41.420 --> 01:51:42.420]   Yeah, it's all for the kids.
[01:51:42.420 --> 01:51:43.420]   I do it for the kids.
[01:51:43.420 --> 01:51:44.420]   Is this Ethereum?
[01:51:44.420 --> 01:51:45.420]   It's all for the Joe.
[01:51:45.420 --> 01:51:46.420]   It's all for the Joe.
[01:51:46.420 --> 01:51:48.180]   Now, for a while, I was mining something.
[01:51:48.180 --> 01:51:49.460]   I forgot what, though.
[01:51:49.460 --> 01:51:50.540]   But no, you can't mine.
[01:51:50.540 --> 01:51:54.900]   You can't reasonably mine any crypto these days in your home.
[01:51:54.900 --> 01:51:56.460]   Your power bill wouldn't cost so much.
[01:51:56.460 --> 01:51:59.580]   You need a nuclear power plant.
[01:51:59.580 --> 01:52:00.580]   Yeah.
[01:52:00.580 --> 01:52:07.700]   Huawei says, oh, this Huawei story is a gift that just keeps on giving.
[01:52:07.700 --> 01:52:14.140]   That the US, by the way, there was no evidence offered, has been pressuring its employees.
[01:52:14.140 --> 01:52:17.540]   The government has pressured the companies employees to turn against it, turn against
[01:52:17.540 --> 01:52:24.860]   Huawei and has used cyber attacks to infiltrate the firm's computer systems in recent months.
[01:52:24.860 --> 01:52:29.820]   I have to say I'm a little sympathetic to Huawei because we've never seen any real evidence.
[01:52:29.820 --> 01:52:36.500]   It felt like Huawei was just a scapegoat for a trade war.
[01:52:36.500 --> 01:52:41.380]   But on the other hand, maybe, and we've also heard bad things about Huawei from others.
[01:52:41.380 --> 01:52:49.180]   I think it's just that Huawei has a few executives that have close ties to the Chinese military.
[01:52:49.180 --> 01:52:50.180]   That's the issue.
[01:52:50.180 --> 01:52:51.180]   That's the issue.
[01:52:51.180 --> 01:52:55.340]   That's what they've consistently said for a long time.
[01:52:55.340 --> 01:52:57.820]   It's a presumed problem.
[01:52:57.820 --> 01:53:04.460]   Huawei hasn't necessarily denied that, but they would deny that the Chinese government
[01:53:04.460 --> 01:53:08.500]   is running the company.
[01:53:08.500 --> 01:53:13.260]   This is an overreaction to what's there.
[01:53:13.260 --> 01:53:18.780]   That said, we know that there is cyber warfare going on between China and the US.
[01:53:18.780 --> 01:53:20.860]   It has been for a long time.
[01:53:20.860 --> 01:53:26.940]   If FBI agents are posing as Huawei employees, would you be surprised?
[01:53:26.940 --> 01:53:27.940]   I wouldn't be.
[01:53:27.940 --> 01:53:28.940]   All right, let's take a break.
[01:53:28.940 --> 01:53:35.500]   We'll wrap this thing up.
[01:53:35.500 --> 01:53:38.660]   I'm just looking at some headlines just to see.
[01:53:38.660 --> 01:53:42.260]   Biohackers chase Johnny Nemonic with peg-leg implanted hard drive.
[01:53:42.260 --> 01:53:43.500]   Well, let Seth explain that.
[01:53:43.500 --> 01:53:45.580]   That was Seth's idea.
[01:53:45.580 --> 01:53:47.900]   I don't even know what those words mean.
[01:53:47.900 --> 01:53:50.340]   I know what each individual word means, but I don't know what the guy...
[01:53:50.340 --> 01:53:51.660]   That was a pretty crazy story.
[01:53:51.660 --> 01:53:53.860]   All right, we'll talk about that in just a second.
[01:53:53.860 --> 01:53:56.740]   But first, I have a very happy story.
[01:53:56.740 --> 01:53:59.300]   We hired the new guy.
[01:53:59.300 --> 01:54:01.100]   Just the other day, we hired a production assistant.
[01:54:01.100 --> 01:54:02.100]   He's working out, right?
[01:54:02.100 --> 01:54:03.100]   He's good.
[01:54:03.100 --> 01:54:04.100]   I like the guy.
[01:54:04.100 --> 01:54:05.100]   I like the kid, right?
[01:54:05.100 --> 01:54:06.100]   You guys all went out drinking with him, right?
[01:54:06.100 --> 01:54:07.100]   He gets your vote of confidence.
[01:54:07.100 --> 01:54:09.140]   You know how we found him?
[01:54:09.140 --> 01:54:10.660]   Zip recruiter.
[01:54:10.660 --> 01:54:11.660]   Love the zip recruiter.
[01:54:11.660 --> 01:54:17.140]   Man, if you are doing hiring for a company, you know that that is a tough job.
[01:54:17.140 --> 01:54:19.500]   But it's also...I mean, I'll give you a lot of credit.
[01:54:19.500 --> 01:54:20.500]   It's an important job.
[01:54:20.500 --> 01:54:23.220]   Any company, our company is made up of employees.
[01:54:23.220 --> 01:54:25.620]   Good employees can help that company soar.
[01:54:25.620 --> 01:54:28.220]   A bad employee can just drag you right down.
[01:54:28.220 --> 01:54:29.540]   It's important.
[01:54:29.540 --> 01:54:30.540]   This is not a...
[01:54:30.540 --> 01:54:34.980]   But it's a tough job because your phone lights up, your inbox is jammed, and you don't know
[01:54:34.980 --> 01:54:38.060]   how to figure the right persons out there somewhere.
[01:54:38.060 --> 01:54:39.060]   But how do you reach them?
[01:54:39.060 --> 01:54:40.340]   There's so many job boards.
[01:54:40.340 --> 01:54:45.220]   I got to tell you, we have just keep going back to Zip recruiter and it is the best way
[01:54:45.220 --> 01:54:46.220]   to hire.
[01:54:46.220 --> 01:54:57.780]   And I think, if you're getting out of the way, you're getting out to the widest possible
[01:54:57.780 --> 01:54:59.340]   audience.
[01:54:59.340 --> 01:55:00.980]   But that's not all.
[01:55:00.980 --> 01:55:02.180]   That's not all.
[01:55:02.180 --> 01:55:05.780]   As applications come in, you can have screening questions.
[01:55:05.780 --> 01:55:07.980]   Yes, no true false.
[01:55:07.980 --> 01:55:10.140]   Even essay questions and AI couldn't answer.
[01:55:10.140 --> 01:55:13.980]   And you can screen out people who aren't right for the job, narrow it down.
[01:55:13.980 --> 01:55:15.780]   They don't come to your phone or your inbox.
[01:55:15.780 --> 01:55:18.020]   They all go to the Zip recruiter interface.
[01:55:18.020 --> 01:55:20.260]   And then there's this new thing that I just love.
[01:55:20.260 --> 01:55:25.940]   Zip recruiter has powerful matching technology that scans the resumes they already have
[01:55:25.940 --> 01:55:32.180]   on file to find people with the right experience and then invite them to apply to your job.
[01:55:32.180 --> 01:55:36.340]   They also analyze each one and they spotlight the top candidates right to the top so you
[01:55:36.340 --> 01:55:37.740]   don't miss a great match.
[01:55:37.740 --> 01:55:39.500]   This works so well.
[01:55:39.500 --> 01:55:43.980]   Zip recruiter says with four out of five employers who post on Zip recruiter, they're going to
[01:55:43.980 --> 01:55:47.900]   get a quality candidate within the first day.
[01:55:47.900 --> 01:55:51.100]   In our experience, it's often within an hour.
[01:55:51.100 --> 01:55:52.540]   It's kind of an amazing thing.
[01:55:52.540 --> 01:55:54.140]   We'll post it breakfast before lunch.
[01:55:54.140 --> 01:55:58.140]   We've got three great candidates thanks to Zip recruiter.
[01:55:58.140 --> 01:55:59.340]   I just love Zip recruiter.
[01:55:59.340 --> 01:56:00.340]   You will too.
[01:56:00.340 --> 01:56:01.700]   Right now you could try it free.
[01:56:01.700 --> 01:56:06.260]   Get a sense of how it'll work for you at zip recruiter.com/twit.
[01:56:06.260 --> 01:56:08.460]   This is literally the smartest way to hire.
[01:56:08.460 --> 01:56:12.300]   It's going to save you a lot of time and get you a great employee.
[01:56:12.300 --> 01:56:14.580]   Zip recruiter.com/twit.
[01:56:14.580 --> 01:56:17.460]   Zip recruiter.com/twit.
[01:56:17.460 --> 01:56:20.860]   The smartest way to hire.
[01:56:20.860 --> 01:56:23.340]   Thank you, Zip recruiter, for supporting Twit.
[01:56:23.340 --> 01:56:24.340]   All right.
[01:56:24.340 --> 01:56:25.340]   Bio hackers.
[01:56:25.340 --> 01:56:26.340]   I know what that is.
[01:56:26.340 --> 01:56:30.620]   It's like those guys who put stuff in their arm.
[01:56:30.620 --> 01:56:32.460]   Chase Johnny Nemonic.
[01:56:32.460 --> 01:56:33.460]   I know what that is.
[01:56:33.460 --> 01:56:37.340]   That's the William Gibson novel and we'll be at the same name, short story and we'll
[01:56:37.340 --> 01:56:39.740]   be the same name.
[01:56:39.740 --> 01:56:42.580]   This is in quotes, peg leg.
[01:56:42.580 --> 01:56:45.500]   Don't know what that is, but I know what a peg leg is, but I don't know what peg leg
[01:56:45.500 --> 01:56:48.140]   is, implanted hard drive.
[01:56:48.140 --> 01:56:54.300]   Here's a picture of them doing it.
[01:56:54.300 --> 01:56:57.100]   This guy got a hard drive implanted in his leg.
[01:56:57.100 --> 01:56:58.100]   Why?
[01:56:58.100 --> 01:56:59.100]   Yep.
[01:56:59.100 --> 01:57:06.700]   First of all, is it a spinning driver or a solid state?
[01:57:06.700 --> 01:57:07.700]   It's a solid state.
[01:57:07.700 --> 01:57:08.700]   Okay, good.
[01:57:08.700 --> 01:57:10.580]   So in the story, I detail what the device is.
[01:57:10.580 --> 01:57:14.620]   But he upgraded it to 100 gig drive in a year.
[01:57:14.620 --> 01:57:21.340]   The fascinating thing about it is that it's a customized version of a Raspberry Pi.
[01:57:21.340 --> 01:57:27.580]   So the size of the drive is limited only by your budget for a micro SD card.
[01:57:27.580 --> 01:57:29.500]   Is there an actual Raspberry Pi with this?
[01:57:29.500 --> 01:57:30.500]   Because that's big.
[01:57:30.500 --> 01:57:31.500]   I don't want that at a point.
[01:57:31.500 --> 01:57:34.500]   Well, it's about the size of a mini candy bar, right?
[01:57:34.500 --> 01:57:37.500]   Like the one you hand out at a Halloween?
[01:57:37.500 --> 01:57:38.500]   It's the zero.
[01:57:38.500 --> 01:57:39.500]   Oh, it's the Pi zero.
[01:57:39.500 --> 01:57:40.700]   Yeah, it's the Pi zero.
[01:57:40.700 --> 01:57:42.100]   So it's smaller.
[01:57:42.100 --> 01:57:45.700]   They coated in a biosafe acrylic resin.
[01:57:45.700 --> 01:57:46.900]   He is smiling.
[01:57:46.900 --> 01:57:47.900]   Yeah.
[01:57:47.900 --> 01:57:54.820]   And then without getting too graphic, made a subcutaneous pocket between the skin and
[01:57:54.820 --> 01:57:57.380]   the muscle shove the thing in and then sort it up.
[01:57:57.380 --> 01:57:58.380]   Isn't this illegal?
[01:57:58.380 --> 01:58:00.620]   Like that's practicing medicine without a license.
[01:58:00.620 --> 01:58:02.580]   Not if you're doing it to yourself.
[01:58:02.580 --> 01:58:03.580]   You could do it to yourself.
[01:58:03.580 --> 01:58:06.940]   So the guy in the picture.
[01:58:06.940 --> 01:58:11.620]   The face mask on is the surgeon who did it to himself.
[01:58:11.620 --> 01:58:18.140]   And then Michael Laufer, who is on the table with his leg wrapped, also had one done.
[01:58:18.140 --> 01:58:20.140]   So they call this implant a peg leg?
[01:58:20.140 --> 01:58:24.940]   Yes, because it's the part of the pirate that's left over.
[01:58:24.940 --> 01:58:26.580]   And what can you do with it?
[01:58:26.580 --> 01:58:28.340]   You can put data on it.
[01:58:28.340 --> 01:58:29.540]   It's not connected to the internet.
[01:58:29.540 --> 01:58:32.540]   It's a mesh network connected device.
[01:58:32.540 --> 01:58:35.820]   So you can put whatever data you want, stream data from it.
[01:58:35.820 --> 01:58:39.340]   As long as the devices are connected directly to it.
[01:58:39.340 --> 01:58:42.180]   So Keanu Reeves had this, but he wasn't in brain.
[01:58:42.180 --> 01:58:43.180]   It was in the brain.
[01:58:43.180 --> 01:58:44.180]   It was in the brain.
[01:58:44.180 --> 01:58:45.180]   Yeah.
[01:58:45.180 --> 01:58:46.660]   So is it the same idea?
[01:58:46.660 --> 01:58:51.180]   It's the same idea, except that this is a substantial piece of hardware.
[01:58:51.180 --> 01:58:57.940]   And the Johnny Nemonic story was using human wetware, essentially.
[01:58:57.940 --> 01:59:05.260]   But the ideas that were in Johnny Nemonic 40 years ago, almost 40 years ago, are now
[01:59:05.260 --> 01:59:07.740]   starting to leak out into the real world.
[01:59:07.740 --> 01:59:10.220]   And what we've seen is two things.
[01:59:10.220 --> 01:59:15.660]   One, we've seen when there's large innovations like this.
[01:59:15.660 --> 01:59:20.100]   The very first peg leg was done earlier this year and was giant.
[01:59:20.100 --> 01:59:25.700]   It was about the size of the guys, of the recipient's bicep.
[01:59:25.700 --> 01:59:26.700]   And it's huge.
[01:59:26.700 --> 01:59:27.700]   And I've seen photos.
[01:59:27.700 --> 01:59:31.060]   And it was quite gory.
[01:59:31.060 --> 01:59:32.260]   These things get smaller.
[01:59:32.260 --> 01:59:36.700]   So a few months later, they've already made one that is less than half the size.
[01:59:36.700 --> 01:59:40.500]   I presume they're working on version three.
[01:59:40.500 --> 01:59:44.660]   And the other thing we've seen is that if these guys are doing something like this, you
[01:59:44.660 --> 01:59:51.460]   can rest assured that tech companies that are investing in biotechnology are looking
[01:59:51.460 --> 01:59:52.940]   at how to do similar things.
[01:59:52.940 --> 01:59:54.460]   And we've seen it with contact lenses.
[01:59:54.460 --> 01:59:58.860]   We've seen it with all kinds of things.
[01:59:58.860 --> 02:00:00.660]   Were you at this event?
[02:00:00.660 --> 02:00:03.540]   I hesitate to call it an event.
[02:00:03.540 --> 02:00:09.780]   But in the middle of Black Hat, we got into a car and drove from Vegas to Hachapi.
[02:00:09.780 --> 02:00:12.340]   So you saw this happen?
[02:00:12.340 --> 02:00:16.340]   Did he have anesthesia?
[02:00:16.340 --> 02:00:17.740]   Or did he grimace a lot?
[02:00:17.740 --> 02:00:21.060]   There was...
[02:00:21.060 --> 02:00:22.060]   It's got to hurt.
[02:00:22.060 --> 02:00:24.860]   There was anesthesia of some kind.
[02:00:24.860 --> 02:00:28.860]   But again, do you have to do it to yourself like somebody can't help you?
[02:00:28.860 --> 02:00:29.860]   Well, no.
[02:00:29.860 --> 02:00:35.460]   So Laufer, who's the bearded guy on the table, he had it done to him.
[02:00:35.460 --> 02:00:36.460]   He had it done to him.
[02:00:36.460 --> 02:00:41.100]   But the surgeon did it to himself first.
[02:00:41.100 --> 02:00:45.940]   And the surgeon is a guy named Jeff Tibbitt.
[02:00:45.940 --> 02:00:54.580]   He runs an event called Grindfest, which is not about schlocky movies, but the sort of
[02:00:54.580 --> 02:01:01.380]   nom de gar of this community are grinders, they're people who do self-insertions or other
[02:01:01.380 --> 02:01:03.740]   insertions of biotechnology.
[02:01:03.740 --> 02:01:07.900]   A lot of it's electromagnets and fingertips.
[02:01:07.900 --> 02:01:10.860]   This is probably the most substantial implant ever done.
[02:01:10.860 --> 02:01:11.860]   I know, kidding.
[02:01:11.860 --> 02:01:17.220]   If here's a picture of the mini Hershey's candy bar electronics.
[02:01:17.220 --> 02:01:19.140]   How much storage?
[02:01:19.140 --> 02:01:21.020]   Whatever is on the micro SD card.
[02:01:21.020 --> 02:01:23.940]   So these days it could be two terabytes now, right?
[02:01:23.940 --> 02:01:29.100]   I feel like you're going to look back at this and say, gosh, I wish I could upgrade it.
[02:01:29.100 --> 02:01:32.380]   Well, so there's a couple of problems there, right?
[02:01:32.380 --> 02:01:33.700]   There's issues of upgrading.
[02:01:33.700 --> 02:01:38.860]   There's also what happens if you need an MRI or a CAT scan.
[02:01:38.860 --> 02:01:39.860]   That could be damaged.
[02:01:39.860 --> 02:01:42.740]   But you said he was able to get through airport security.
[02:01:42.740 --> 02:01:50.940]   The first recipient was able to get through security in LAX and an international airport.
[02:01:50.940 --> 02:01:56.420]   Really just went through the magnetic scanner, not through, sorry, went through the standard
[02:01:56.420 --> 02:02:00.180]   scanner, not the body, millimeter wave scanner.
[02:02:00.180 --> 02:02:05.660]   We had Josh Zainerd, didn't we, on the new screensaver some time ago?
[02:02:05.660 --> 02:02:12.220]   He's the biohacker who injected himself with CRISPR and he was selling a kit.
[02:02:12.220 --> 02:02:13.900]   He's actually now being investigated.
[02:02:13.900 --> 02:02:14.900]   Yeah.
[02:02:14.900 --> 02:02:20.340]   But you know, one of the interesting things about the biohacking movement is that it encompasses
[02:02:20.340 --> 02:02:27.060]   a very wide range of technology that people are not necessarily being given access to.
[02:02:27.060 --> 02:02:29.980]   They know that the technology exists to help them.
[02:02:29.980 --> 02:02:37.420]   And one of the most apparent ways that this is coming across is literally building artificial
[02:02:37.420 --> 02:02:40.860]   pancreases out of insulin pumps.
[02:02:40.860 --> 02:02:41.860]   See, that's great.
[02:02:41.860 --> 02:02:43.100]   Well, they think this is great.
[02:02:43.100 --> 02:02:44.900]   I mean, it's an interesting idea.
[02:02:44.900 --> 02:02:45.900]   Hey, it's their body.
[02:02:45.900 --> 02:02:46.900]   I can't.
[02:02:46.900 --> 02:02:47.900]   I mean, I can't knock it.
[02:02:47.900 --> 02:02:52.020]   Even the chat room was asking whether it's battery powered or rechargeable or so.
[02:02:52.020 --> 02:02:56.260]   So the photo that Leo threw up shows the device.
[02:02:56.260 --> 02:03:00.820]   And threw up is the app phrase.
[02:03:00.820 --> 02:03:05.360]   Shows the device on a wireless battery.
[02:03:05.360 --> 02:03:08.940]   And so when it's called a peg leg because it's in the leg, right?
[02:03:08.940 --> 02:03:09.940]   Right.
[02:03:09.940 --> 02:03:10.940]   Right.
[02:03:10.940 --> 02:03:11.940]   Right.
[02:03:11.940 --> 02:03:12.940]   Keep charging.
[02:03:12.940 --> 02:03:13.940]   Right.
[02:03:13.940 --> 02:03:14.940]   Keep the metal.
[02:03:14.940 --> 02:03:15.980]   And when you have the battery in your pocket, it turns on.
[02:03:15.980 --> 02:03:18.140]   You don't want someone to see the device.
[02:03:18.140 --> 02:03:19.140]   You pull the battery.
[02:03:19.140 --> 02:03:21.140]   So that's why you put it where it is.
[02:03:21.140 --> 02:03:24.820]   But then you have to wear cargo pants, which is a crime in its own right.
[02:03:24.820 --> 02:03:25.820]   So all right.
[02:03:25.820 --> 02:03:26.820]   It's you.
[02:03:26.820 --> 02:03:29.140]   Is he actually operating himself in that picture?
[02:03:29.140 --> 02:03:31.660]   Is the surgeon like actually a surgeon?
[02:03:31.660 --> 02:03:34.180]   The surgeon is a nurse, registered nurse.
[02:03:34.180 --> 02:03:35.660]   Nurse is an RN.
[02:03:35.660 --> 02:03:38.620]   And has this clean room in his garage.
[02:03:38.620 --> 02:03:40.180]   That's one does.
[02:03:40.180 --> 02:03:41.180]   As one does.
[02:03:41.180 --> 02:03:42.180]   Yeah.
[02:03:42.180 --> 02:03:46.700]   But it was super interesting to see this kind of stuff happen in real time.
[02:03:46.700 --> 02:03:47.700]   Yeah.
[02:03:47.700 --> 02:03:48.940]   And I admire these guys.
[02:03:48.940 --> 02:03:49.940]   They're on the fringe.
[02:03:49.940 --> 02:03:54.300]   Sometimes I think though maybe they're a little over influenced by science fiction like this.
[02:03:54.300 --> 02:03:57.940]   This is maybe more cool to do.
[02:03:57.940 --> 02:04:01.500]   So you remember the EpiPencil story from I think it was two or three years ago?
[02:04:01.500 --> 02:04:02.500]   Which was a great story.
[02:04:02.500 --> 02:04:05.300]   That was also Michael Laufer and his work for that.
[02:04:05.300 --> 02:04:06.300]   I admire him for that.
[02:04:06.300 --> 02:04:10.260]   Because the company that made the EpiPen was charging more than a thousand dollars.
[02:04:10.260 --> 02:04:11.260]   Oh, it was insane.
[02:04:11.260 --> 02:04:12.260]   It was a crazy amount.
[02:04:12.260 --> 02:04:14.820]   So they showed how you could easily make one of your own.
[02:04:14.820 --> 02:04:20.060]   And so a lot of what he does with his fourth Eve's vinegar collective is demonstrate the
[02:04:20.060 --> 02:04:23.660]   possibilities of technology, put the instructions out there.
[02:04:23.660 --> 02:04:24.660]   Good for him.
[02:04:24.660 --> 02:04:31.500]   And then theoretically people will take advantage of it.
[02:04:31.500 --> 02:04:34.620]   I think were they to become more popular?
[02:04:34.620 --> 02:04:37.620]   There would be I think some ethical concerns that would have to be discussed.
[02:04:37.620 --> 02:04:40.740]   That's where Zayner got in trouble with selling this to the CRISPR home.
[02:04:40.740 --> 02:04:43.620]   Do it yourself DNA kit to the public.
[02:04:43.620 --> 02:04:48.500]   They're not proposing that the public start embedding hard drives into their thighs.
[02:04:48.500 --> 02:04:50.100]   They're putting it out there.
[02:04:50.100 --> 02:04:56.180]   If you have the means, if you have an operating theater in your garage.
[02:04:56.180 --> 02:04:58.100]   Wow.
[02:04:58.100 --> 02:05:07.220]   But the device itself runs software that has not only storage, but messaging system and
[02:05:07.220 --> 02:05:08.340]   chat rooms.
[02:05:08.340 --> 02:05:11.420]   So you can if you got a chat room in my thigh.
[02:05:11.420 --> 02:05:14.740]   So what's interesting is that you could have one of these on an airplane.
[02:05:14.740 --> 02:05:16.220]   I need a chat room in my head.
[02:05:16.220 --> 02:05:19.060]   I have one of these on an airplane and everyone's device could connect to it.
[02:05:19.060 --> 02:05:23.060]   I've been trying to figure out a way to because I rely on the chat room for all my best lines.
[02:05:23.060 --> 02:05:25.100]   I've been trying to figure out a way to take the chat room with me.
[02:05:25.100 --> 02:05:26.100]   Now you know.
[02:05:26.100 --> 02:05:29.620]   Now I'm going to embed you guys into my thigh.
[02:05:29.620 --> 02:05:31.780]   And they would love that.
[02:05:31.780 --> 02:05:32.780]   They would love that.
[02:05:32.780 --> 02:05:34.260]   Oh, they would.
[02:05:34.260 --> 02:05:35.260]   They would.
[02:05:35.260 --> 02:05:36.260]   Hey, Seth.
[02:05:36.260 --> 02:05:37.260]   Thank you so much.
[02:05:37.260 --> 02:05:38.260]   That's a great story.
[02:05:38.260 --> 02:05:39.860]   For being here for the whole show.
[02:05:39.860 --> 02:05:40.860]   Seth Rosenblatt.
[02:05:40.860 --> 02:05:43.700]   He is the editor in chief at the parallax.com.
[02:05:43.700 --> 02:05:46.500]   T-A-G dash parallax.
[02:05:46.500 --> 02:05:52.420]   The guy who has the parallax.com wanted $25,000 for the URL.
[02:05:52.420 --> 02:05:54.740]   I said no thank you.
[02:05:54.740 --> 02:05:57.060]   So the dash, hence the dash.
[02:05:57.060 --> 02:06:00.220]   It's a $25,000 savings, folks.
[02:06:00.220 --> 02:06:06.540]   If there's a very generous benefactor out there who is not connected to Jeremy Epstein,
[02:06:06.540 --> 02:06:08.060]   please contact me.
[02:06:08.060 --> 02:06:09.740]   Somebody said I should put it in my hip.
[02:06:09.740 --> 02:06:11.940]   Then they could say it'd be hip to be in Twitch.
[02:06:11.940 --> 02:06:12.940]   Oh boy.
[02:06:12.940 --> 02:06:13.940]   Wow.
[02:06:13.940 --> 02:06:14.940]   Mm-hmm.
[02:06:14.940 --> 02:06:15.940]   The parallax.
[02:06:15.940 --> 02:06:16.940]   Great stuff.
[02:06:16.940 --> 02:06:17.940]   Really fascinating story.
[02:06:17.940 --> 02:06:18.940]   Thank you so much for being.
[02:06:18.940 --> 02:06:19.940]   It's great to meet you, Seth.
[02:06:19.940 --> 02:06:20.940]   Yes, it's a great story.
[02:06:20.940 --> 02:06:23.700]   Thanks also to, of course, the technologizer.
[02:06:23.700 --> 02:06:27.420]   Harry McCracken, he's tech editor at Fast Company and always a pleasure.
[02:06:27.420 --> 02:06:28.420]   Thank you, Leon.
[02:06:28.420 --> 02:06:29.580]   And you brought me, which was nice.
[02:06:29.580 --> 02:06:32.580]   She only lasted half the show before she ran out screaming.
[02:06:32.580 --> 02:06:33.580]   But it was nice.
[02:06:33.580 --> 02:06:35.860]   It was nice to see her briefly.
[02:06:35.860 --> 02:06:37.580]   Now I'm sure she had work to do.
[02:06:37.580 --> 02:06:38.580]   It was three quarters of the show.
[02:06:38.580 --> 02:06:39.580]   Three quarters.
[02:06:39.580 --> 02:06:40.980]   Of course she ran out screaming.
[02:06:40.980 --> 02:06:43.820]   Thank you so much, Jason Heiner.
[02:06:43.820 --> 02:06:44.820]   I'm sorry you didn't make it up.
[02:06:44.820 --> 02:06:46.100]   It'd have been great to see you.
[02:06:46.100 --> 02:06:48.300]   But enjoy at the Apple event.
[02:06:48.300 --> 02:06:49.300]   Have a great time.
[02:06:49.300 --> 02:06:50.300]   There's Marie.
[02:06:50.300 --> 02:06:51.300]   She came back.
[02:06:51.300 --> 02:06:52.540]   And I hope all is well.
[02:06:52.540 --> 02:06:55.300]   Anything you want to plug, Jason?
[02:06:55.300 --> 02:06:59.180]   I just want to give a shout out to my friend, Aunt Pruitt, who joined the Twitch team.
[02:06:59.180 --> 02:07:01.540]   So excited to see that, you know.
[02:07:01.540 --> 02:07:02.540]   I love and love.
[02:07:02.540 --> 02:07:04.020]   And love and so much.
[02:07:04.020 --> 02:07:07.540]   I had the great pleasure of watching a college show.
[02:07:07.540 --> 02:07:11.460]   I watched football game with Aunt last night.
[02:07:11.460 --> 02:07:14.980]   And that was a unique experience, to say the least.
[02:07:14.980 --> 02:07:16.180]   No, it was a lot of fun.
[02:07:16.180 --> 02:07:20.620]   We had a nice glass of whiskey and talked about you, actually, of all things.
[02:07:20.620 --> 02:07:21.620]   Thank you, Jason.
[02:07:21.620 --> 02:07:23.620]   Anything you want to plug, Harry, that you're up to?
[02:07:23.620 --> 02:07:28.060]   I have something up today, which is I talked to Brad Smith, who's the president of Microsoft.
[02:07:28.060 --> 02:07:36.060]   He has a book out about AI and social media and immigration and all these issues that intersect
[02:07:36.060 --> 02:07:37.060]   with technology.
[02:07:37.060 --> 02:07:38.860]   And it's actually a surprisingly good read.
[02:07:38.860 --> 02:07:42.260]   You would kind of expect that something by the president of a large tech company would
[02:07:42.260 --> 02:07:43.260]   be boilerplate.
[02:07:43.260 --> 02:07:50.260]   But he has an interesting take on things and is pretty open about discussing the challenges
[02:07:50.260 --> 02:07:54.740]   Microsoft has faced and how they figured out how to deal with them in a reasonably successful
[02:07:54.740 --> 02:07:57.540]   fashion compared to a lot of other large tech companies.
[02:07:57.540 --> 02:07:58.780]   Brad's been around a long time.
[02:07:58.780 --> 02:08:01.340]   He's been there since 1993, I believe.
[02:08:01.340 --> 02:08:02.340]   Yeah.
[02:08:02.340 --> 02:08:04.060]   And a big part of Microsoft.
[02:08:04.060 --> 02:08:11.220]   Yeah, he's not a household name, but if you are a government official or a competitor
[02:08:11.220 --> 02:08:15.500]   or a whole bunch of other players out there, he's kind of who you go to.
[02:08:15.500 --> 02:08:17.820]   He's super influential.
[02:08:17.820 --> 02:08:21.500]   The book is called Tools and Weapons, the Promised and Paral of the Digital Age.
[02:08:21.500 --> 02:08:26.100]   And you can read Harry's interview with Brad Smith at Fast Company.com.
[02:08:26.100 --> 02:08:28.940]   It's no love letter, letter, the book.
[02:08:28.940 --> 02:08:30.860]   Wow, very interesting.
[02:08:30.860 --> 02:08:32.500]   Seth, anything you want to plug?
[02:08:32.500 --> 02:08:35.500]   Do you do a parallax podcast?
[02:08:35.500 --> 02:08:38.740]   Working on a couple of different podcast things at the moment.
[02:08:38.740 --> 02:08:41.900]   I'll let you know when those actually take off.
[02:08:41.900 --> 02:08:42.900]   We'll have you back.
[02:08:42.900 --> 02:08:43.900]   You can plug them.
[02:08:43.900 --> 02:08:44.900]   Yeah.
[02:08:44.900 --> 02:08:45.900]   How about that?
[02:08:45.900 --> 02:08:46.900]   That'd be great.
[02:08:46.900 --> 02:08:47.900]   All right.
[02:08:47.900 --> 02:08:49.660]   This week on triangulation, I'm really jealous.
[02:08:49.660 --> 02:08:54.380]   Mike Asargent got to talk to David Weinberger, who is really kind of legendary.
[02:08:54.380 --> 02:08:57.380]   One of the authors, the very famous authors of the Clutrain Manifesto.
[02:08:57.380 --> 02:09:02.060]   He's written a new book called Everyday Chaos Technology Complexity and how we're
[02:09:02.060 --> 02:09:06.780]   thriving in the new world of possibility.
[02:09:06.780 --> 02:09:09.700]   And Ant, who saw the show, said it was really good.
[02:09:09.700 --> 02:09:14.420]   So it's on my list of much, must watch episodes of triangulation.
[02:09:14.420 --> 02:09:20.340]   You might want to catch it at twit.tv/tri.
[02:09:20.340 --> 02:09:23.580]   We do Twit every Sunday afternoon, right after the radio show.
[02:09:23.580 --> 02:09:28.340]   That's usually around 230 Pacific, 530 Eastern, 2130 UTC.
[02:09:28.340 --> 02:09:30.940]   If you want to be in studio, just email tickets at twit.tv.
[02:09:30.940 --> 02:09:31.940]   Tony came and visited us.
[02:09:31.940 --> 02:09:33.740]   Nice to have you, Tony.
[02:09:33.740 --> 02:09:35.340]   Some guy named Ants here.
[02:09:35.340 --> 02:09:38.580]   He's kind of bigger than an Ant.
[02:09:38.580 --> 02:09:39.580]   Miss named.
[02:09:39.580 --> 02:09:41.860]   I always love having you in studio.
[02:09:41.860 --> 02:09:46.060]   Just email tickets at twit.tv so we're going to make sure we get a seat for you.
[02:09:46.060 --> 02:09:50.700]   You can also watch the live stream from wherever you are, anywhere in the world, twit.tv/live.
[02:09:50.700 --> 02:09:54.380]   Listen to, we've got audio and video streams at twit.tv/live.
[02:09:54.380 --> 02:09:56.660]   If you're doing that, the chat room's the place to be.
[02:09:56.660 --> 02:09:58.940]   It's right here in my thigh.
[02:09:58.940 --> 02:10:03.100]   I got it on a hard drive in my thigh.
[02:10:03.100 --> 02:10:05.740]   But you don't have to plug into my thigh.
[02:10:05.740 --> 02:10:12.780]   You just go to IRC.tv and join us because it's a lot of fun.
[02:10:12.780 --> 02:10:13.780]   It's family friendly.
[02:10:13.780 --> 02:10:15.060]   It's a great place to hang out.
[02:10:15.060 --> 02:10:16.620]   Some nice people in there.
[02:10:16.620 --> 02:10:24.300]   On-demand versions of everything we do available at the website, twit.tv or ask your Amazon
[02:10:24.300 --> 02:10:27.860]   Echo or your Google Assistant say, "Hey, Echo.
[02:10:27.860 --> 02:10:30.460]   Listen to this week in tech or play this week in tech."
[02:10:30.460 --> 02:10:32.860]   You can get the most recent episode.
[02:10:32.860 --> 02:10:34.780]   In many cases, it'll also play the live stream.
[02:10:34.780 --> 02:10:38.980]   You can say, "Echo, play Twit Live," and it will play the live stream so you can listen
[02:10:38.980 --> 02:10:40.420]   in wherever you are.
[02:10:40.420 --> 02:10:43.660]   You can also download copies of the episode by subscribing.
[02:10:43.660 --> 02:10:44.860]   That's probably the best way to do it.
[02:10:44.860 --> 02:10:48.780]   Your Apple Podcasts, your Google Podcasts, subscribe on YouTube.
[02:10:48.780 --> 02:10:53.540]   Wherever you get your podcast, pocket casts, if you subscribe, we'll be sure to get it
[02:10:53.540 --> 02:10:57.220]   right into your device the minute it's ready so you have it for your Monday morning commute.
[02:10:57.780 --> 02:10:59.020]   Thanks everybody for being here.
[02:10:59.020 --> 02:10:59.900]   We'll see you next time.
[02:10:59.900 --> 02:11:02.300]   Another Twit is in the can.
[02:11:02.300 --> 02:11:02.700]   Bye bye.
[02:11:02.700 --> 02:11:09.700]   This is amazing.
[02:11:09.700 --> 02:11:12.700]   Do the Twit.
[02:11:12.700 --> 02:11:14.700]   Do the Twit.
[02:11:14.700 --> 02:11:15.700]   All right.
[02:11:15.700 --> 02:11:16.700]   Do the Twit, baby.
[02:11:16.700 --> 02:11:17.700]   Do the Twit.
[02:11:17.700 --> 02:11:18.700]   All right.
[02:11:18.700 --> 02:11:19.700]   Do the Twit.
[02:11:19.700 --> 02:11:24.060]   Now, if they could have a suppository chatroom, I would do that.
[02:11:24.060 --> 02:11:25.060]   Would you?
[02:11:25.060 --> 02:11:27.220]   We did that.
[02:11:27.220 --> 02:11:28.220]   We did that.
[02:11:28.220 --> 02:11:34.380]   First episode of Attack of the Show on G4, we had a chatroom suppository.
[02:11:34.380 --> 02:11:35.380]   You're kidding?
[02:11:35.380 --> 02:11:36.380]   Yep.
[02:11:36.380 --> 02:11:37.380]   Like, real or was it a joke?
[02:11:37.380 --> 02:11:44.860]   It was a real server that we inserted into someone's rectum.
[02:11:44.860 --> 02:11:46.020]   What?
[02:11:46.020 --> 02:11:47.460]   It's been done.
[02:11:47.460 --> 02:11:48.460]   How?
[02:11:48.460 --> 02:11:49.460]   Look at--
[02:11:49.460 --> 02:11:50.460]   I believe you.
[02:11:50.460 --> 02:11:51.460]   --watch the first episode of Attack of the Show.
[02:11:51.460 --> 02:11:53.500]   That was in the very first Attack of the Show.
[02:11:53.500 --> 02:11:54.500]   Yep.
[02:11:54.500 --> 02:11:55.500]   Wow.
[02:11:55.500 --> 02:11:56.500]   Wow.
[02:11:56.500 --> 02:11:58.180]   Charles thought that would go over well, huh?
[02:11:58.180 --> 02:12:00.060]   Look how far we've come.
[02:12:00.060 --> 02:12:01.060]   Yeah.
[02:12:01.060 --> 02:12:02.060]   Look how far we've come.
[02:12:02.060 --> 02:12:03.060]   Now we put it in your thigh.
[02:12:03.060 --> 02:12:04.060]   Yes.
[02:12:04.060 --> 02:12:05.060]   It's moving.
[02:12:05.060 --> 02:12:06.060]   [LAUGHS]

