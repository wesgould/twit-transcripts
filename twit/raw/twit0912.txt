;FFMETADATA1
title=Let Me Consult My AI Lawyer
artist=Leo Laporte, Harry McCracken, Tim Stevens, Christina Warren
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-01-30
track=912
language=English
genre=Podcast
comment=AI everywhere, Hive hacked, Microsoft earnings, To Leslie, @ElonJet
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:07.200]   It's time for Twit this week in tech. We have a great show. Tim Stevens is here. Harry McCracken,
[00:00:07.200 --> 00:00:13.440]   Christina Warren, three of my favorite people. And of course, AI is the topic. It's an amazing
[00:00:13.440 --> 00:00:18.160]   world we live in and some of the new things that are happening with AI and some of the old things
[00:00:18.160 --> 00:00:27.040]   that maybe aren't so good. We'll talk about the Microsoft quarterly results. Not so hot. Intel,
[00:00:27.040 --> 00:00:34.400]   the worst quarter in a long time and about the Oscar campaign that took Twitter by storm
[00:00:34.400 --> 00:00:37.040]   and works. It's all coming up next on Twit.
[00:00:37.040 --> 00:00:45.600]   Podcasts you love from people you trust. This is Twit.
[00:00:51.280 --> 00:00:59.360]   This is Twit. This week in tech, episode 912 recorded Sunday, January 29th, 2023.
[00:00:59.360 --> 00:01:05.680]   Let me consult my AI lawyer. This week in tech is brought to you by
[00:01:05.680 --> 00:01:12.400]   Worldwide Technology with an innovative cultured thousands of IT engineers, application developers,
[00:01:12.400 --> 00:01:17.920]   unmatched labs and integration centers for testing and deploying technology at scale.
[00:01:17.920 --> 00:01:22.960]   WWT helps customers bridge the gap between strategy and execution.
[00:01:22.960 --> 00:01:28.000]   To learn more about WWT, visit www.wt.com/twit.
[00:01:28.000 --> 00:01:35.600]   And by ACI learning. Tech is one industry where opportunities outpace growth, especially in
[00:01:35.600 --> 00:01:41.520]   cybersecurity. One third of information security jobs require a cybersecurity certification
[00:01:41.520 --> 00:01:50.720]   to maintain your competitive edge across audit, IT and cybersecurity readiness. Visit go.acilearning.com/twit.
[00:01:50.720 --> 00:01:57.360]   And by Bitwarden, get the password manager that offers a robust and cost-effective solution
[00:01:57.360 --> 00:02:02.160]   that can drastically increase your chances of staying safe online.
[00:02:02.160 --> 00:02:06.880]   Get started with a free trial of a Teams or Enterprise plan or get started for free across
[00:02:06.880 --> 00:02:12.000]   all devices as an individual user at bitwarden.com/twit.
[00:02:12.000 --> 00:02:18.320]   Thanks for listening to this show. As an ad supported network, we are always looking for new
[00:02:18.320 --> 00:02:24.160]   partners with products and services that will benefit our qualified audience. Are you ready to
[00:02:24.160 --> 00:02:29.520]   grow your business? Reach out to advertise at twit.tv and launch your campaign now.
[00:02:35.520 --> 00:02:39.600]   It's time for twit this week at Tech. The show we cover the weeks. Tech news.
[00:02:39.600 --> 00:02:44.720]   I'm just going to put a little black-arm band if you don't mind on the San Francisco 49ers.
[00:02:44.720 --> 00:02:52.960]   Gold throwback jacket. That's life. Tim Stevens is here. Hello. Oh, that was Harry. Hello, Tim.
[00:02:52.960 --> 00:02:56.240]   Good to see you. Hey, Leo. Good to see you as well. Thank you for having me.
[00:02:56.240 --> 00:03:01.200]   Tim, of course, has been on for many years. He is now a freelancer at Jalopnik at TechCrunch
[00:03:01.200 --> 00:03:07.200]   at Motor Trend and The Verge. He has his very own sub stack. Tim Stevens, that's sub stack.
[00:03:07.200 --> 00:03:14.880]   Come. Great article on your visit to the Dakar rally in Saudi Arabia. Wow.
[00:03:14.880 --> 00:03:20.400]   Yeah, that was quite a trip. A really interesting social experience on a lot of levels. It'll
[00:03:20.400 --> 00:03:24.800]   nice to do a lot of things. Amazing events and doing a lot of great travel lately. So, I've been
[00:03:24.800 --> 00:03:28.800]   very lucky. Nice. Well, it's great to have you back. Not much ice race. Thanks, Leo.
[00:03:30.000 --> 00:03:33.680]   Sadly not. Are you aware that Tim can't really be heard at least by the...
[00:03:33.680 --> 00:03:39.360]   You can't hear him? Not just barely. All right. Hold on. We're not ready to begin yet.
[00:03:39.360 --> 00:03:45.920]   Oh, you have a little... He's not... He doesn't have headphones on.
[00:03:45.920 --> 00:03:50.320]   So, we can't do a bleed. I understand why we have the bleed. That's why there's a bleed.
[00:03:50.320 --> 00:03:53.840]   Mary, do you mind wearing headphones? I'm happy with whatever works best.
[00:03:53.840 --> 00:03:57.600]   We could provide you with headphones. Sure. I apologize. No problem.
[00:03:58.400 --> 00:04:01.360]   Do you have some... Burke, you can get him out of my office if you don't.
[00:04:01.360 --> 00:04:10.880]   Give him some nice ones. The good stuff. Give him good stuff. Give him the good ones.
[00:04:10.880 --> 00:04:18.080]   I think there's an unopened box in my cupboard on the left there. Give him some sterile headphones.
[00:04:18.080 --> 00:04:24.080]   I'm sorry, Harry. I... That's okay. It wasn't paying attention. Yeah, usually we use a bleed,
[00:04:24.080 --> 00:04:27.760]   but I think... I suddenly realized you might not be aware that I couldn't hear him. Yeah,
[00:04:27.760 --> 00:04:34.960]   that would be kind of a disadvantage to the overall program. It'd be kind of a bad thing.
[00:04:34.960 --> 00:04:45.520]   Good. Nobody should hire D'Amico Ryan's as a head coach. That would be a terrible idea.
[00:04:45.520 --> 00:04:53.200]   It's Lisa's birthday and I really wanted her to have a nice birthday.
[00:04:56.240 --> 00:05:03.520]   Happy birthday, Lisa. It's also our anniversary because I foolishly thought if we got married on
[00:05:03.520 --> 00:05:09.680]   her birthday, I would only have to give her one gift. You didn't think that. That wasn't why you did that.
[00:05:09.680 --> 00:05:15.920]   I just thought it'd be easier to remember. One fewer date to remember or something. I don't know.
[00:05:15.920 --> 00:05:22.160]   Yeah. It was fun. Unfortunately, the place we got married,
[00:05:22.960 --> 00:05:26.240]   Calistoga Ranch has burned to the ground and the wildfires is gone.
[00:05:26.240 --> 00:05:30.960]   Which makes me sad. She says, "Ah, it's life." Lisa, this...
[00:05:30.960 --> 00:05:36.560]   No, because we used to go there on our anniversary and stuff. It was really nice.
[00:05:36.560 --> 00:05:42.560]   But I've just learned that we had a caller from the new Kona Village, which is opening this
[00:05:42.560 --> 00:05:49.120]   summer in Kona, Hawaii. I've just learned that that's reopening and that's somewhere I've always...
[00:05:49.120 --> 00:05:54.720]   I was on my bucket list somewhere to stay. That's where Steve Jobs was staying when the iPhone 4
[00:05:54.720 --> 00:06:00.960]   and Tenegate happened. And if you didn't want to come back, but they made him come back.
[00:06:00.960 --> 00:06:02.800]   That's how good it is, I guess.
[00:06:02.800 --> 00:06:10.240]   At the time, there were no TVs, no phones, no internet. It was like you were in a sting in a
[00:06:10.240 --> 00:06:14.240]   traditional Hawaiian hale. Sounds alright.
[00:06:17.840 --> 00:06:22.720]   How's that? You can hear it and you get the volume there so you can control that. Don't
[00:06:22.720 --> 00:06:29.600]   deafen yourself. Can you hear me, Harry? Harry, can you hear me? Harry, can you hear me?
[00:06:29.600 --> 00:06:34.240]   I can hear you because you're sitting next to me. Oh. It's not a good test.
[00:06:34.240 --> 00:06:42.160]   One tooth, can you, can you hear me? Are you receiving me? Should I tell you what I have for
[00:06:42.160 --> 00:06:46.960]   breakfast? Okay, good. Alright, awesome. Alright, I think we're good. Yay.
[00:06:47.200 --> 00:06:48.720]   [sniff]
[00:06:48.720 --> 00:06:54.000]   Alright, here we go. Yes, someday we'll have four in the studio again.
[00:06:54.000 --> 00:07:01.120]   That has happened. Christina was here, but I don't think we've had it. When was the last time
[00:07:01.120 --> 00:07:07.680]   we had it all in person shows? It's been a while. Alright, starting over. You can hear. I can hear it.
[00:07:07.680 --> 00:07:14.480]   Alright, thank you. It's time for Twit this week at Tech the Show. We cover the week's tech news
[00:07:14.960 --> 00:07:21.040]   with a panel of fabulous people. I'll start over on my right with Mr. Tim Stevens.
[00:07:21.040 --> 00:07:26.640]   We haven't seen in a while freelance writer now. You see Tim's stuff all over the place,
[00:07:26.640 --> 00:07:31.600]   Jalopnik and Tech Crunching, Motor Trend and The Verge. He also has his very own
[00:07:31.600 --> 00:07:39.040]   sub-stack called Around the Next Bend. I made it myself. All by your lonesome.
[00:07:39.040 --> 00:07:42.880]   That's awesome. Thanks for having me, Leo. It's great to be here. Oh, it's great to see you. I missed
[00:07:42.880 --> 00:07:51.120]   you. Lots of stuff to talk about. You just came back from Dakar and the Dakar Road Rally.
[00:07:51.120 --> 00:07:57.360]   I loved your pictures, but it was an interesting mixed, I guess, mixed bag of experiences.
[00:07:57.360 --> 00:08:01.280]   Yeah, thanks. It was a great trip to Saudi Arabia. I learned a lot of things, both good and bad.
[00:08:01.280 --> 00:08:12.720]   Also with us in studio, because COVID is over. No. Harry McCracken, global tech editor at Fast Company.
[00:08:12.720 --> 00:08:17.200]   Fingers crossed. Hello, Harry. Good to see you. Nice to actually see you in person.
[00:08:17.200 --> 00:08:21.520]   Yay. You brought your wonderful wife, Marie, with you. Great to see you all.
[00:08:21.520 --> 00:08:27.760]   She has custody of Lily. The Twitter pet. She will be taking Lily home with us, I'm sure.
[00:08:27.760 --> 00:08:33.840]   Lily is about the best dog you ever saw in your life, Birx Dog. But she lives here. Well,
[00:08:33.840 --> 00:08:37.920]   I shouldn't say that because I think it's in our lease. She's not allowed to spend any time here,
[00:08:37.920 --> 00:08:43.680]   but I didn't say that. I don't think the landlord watches Twitter.
[00:08:43.680 --> 00:08:50.880]   Also great to see Christina Warren from GitHub last time you were in studio.
[00:08:50.880 --> 00:08:53.920]   Senior dev advocate over there at GitHub. Good to see you.
[00:08:53.920 --> 00:09:01.440]   Glad to be here. You made the move this week. When Ivory came out, Tapbox was one of the
[00:09:01.440 --> 00:09:09.120]   third party apps that Mr. Musk clobbered at first without warning, then with a lie saying,
[00:09:09.120 --> 00:09:15.680]   "You've been violating the rules for 15 years?" You just noticed. Finally, they said,
[00:09:15.680 --> 00:09:21.440]   "Oh, they retroactively changed the rules. No third parties." But that wasn't enough to push
[00:09:21.440 --> 00:09:26.400]   you to master on Ivory was the thing that did it. Tapbox was a very, very nice, or tweetbot,
[00:09:26.400 --> 00:09:33.280]   rather. It was a very nice Twitter client from Tapbox. Ivory is basically tweetbot for Mastodon.
[00:09:33.280 --> 00:09:41.440]   Yeah. Honestly, it was a combination of things. It was that. That was, I think,
[00:09:41.440 --> 00:09:47.280]   really the final straw also has a lot of people have commented on, "My posts don't show up
[00:09:47.280 --> 00:09:51.520]   on people's feeds, and I don't see replies, and I don't see other people's posts. The whole
[00:09:51.520 --> 00:09:57.920]   experience was becoming degraded. Not only did I have Ivory, which was great, but there's IceCubes,
[00:09:57.920 --> 00:10:04.080]   which is a great open-source client. IceCubes is fantastic. There's elk.zone, which is a great
[00:10:04.080 --> 00:10:09.280]   web interface. I've actually have a GitHub list that I've been making of different cool
[00:10:09.280 --> 00:10:12.480]   maps on my channel. This is the beauty of open-source and an open standard.
[00:10:12.480 --> 00:10:15.040]   It is. Anybody can develop, and they can't cut you off.
[00:10:16.240 --> 00:10:23.680]   Right. I've already got it. There will be some people who follow me on Mastodon who didn't
[00:10:23.680 --> 00:10:29.760]   follow me on Twitter, but I've got 10% of the followers that I had on Twitter now on Mastodon,
[00:10:29.760 --> 00:10:35.680]   which is not bad for four or five days in. A lot of people are reporting increased engagement,
[00:10:35.680 --> 00:10:39.760]   even though there are fewer followers on Mastodon. I've certainly noticed that.
[00:10:39.760 --> 00:10:41.920]   Absolutely. The quality is high. I have two.
[00:10:44.320 --> 00:10:50.000]   No, that will change over time. I think that is more people joining. You will see less of that
[00:10:50.000 --> 00:10:54.240]   high signal to noise, but right now I totally agree. I'm definitely seeing higher engagement,
[00:10:54.240 --> 00:11:00.880]   higher quality. It's pretty clear that Elon has decided to heavily algorithmatize the feed on
[00:11:00.880 --> 00:11:09.280]   Twitter. He's even said, "You pay eight bucks and more people will see you." I don't know why he
[00:11:09.280 --> 00:11:16.240]   thinks eight bucks from a few hundred million at best. Users is going to make enough money to pay
[00:11:16.240 --> 00:11:22.400]   for Twitter and the loss of ad revenue, but he's doing whatever he can. That's the problem,
[00:11:22.400 --> 00:11:28.800]   though, is that it then tells people, "Oh, nobody's engaging with me. I don't want to be here."
[00:11:28.800 --> 00:11:35.920]   So you're driving off your creators. Actually, Corey, Dr. Rowe, wrote a good story about this this week.
[00:11:37.760 --> 00:11:41.520]   It's an impolite title. I'm going to say TikTok's
[00:11:41.520 --> 00:11:47.600]   "enshertification" using the good places euphemism for that word.
[00:11:47.600 --> 00:11:55.920]   I thought was quite insightful. As usual, Corey made something that's been around and obvious to
[00:11:55.920 --> 00:12:01.920]   all of us. Crystal clear, put in words that a light bulb goes off. He says, "Here's how platforms
[00:12:01.920 --> 00:12:07.680]   die. First, they're good to their users. Then they abuse their users to make things better for
[00:12:07.680 --> 00:12:14.000]   their business customers. Finally, they abuse those business customers to claw back all the value
[00:12:14.000 --> 00:12:20.080]   for themselves. Then they die." He gives an example, Amazon, which was customer first,
[00:12:20.080 --> 00:12:26.400]   customer first, and then as the customer base got locked in with a variety of techniques like
[00:12:26.400 --> 00:12:31.440]   Amazon Prime and DRM and so forth. Then they said, "All right, now business is businesses."
[00:12:31.440 --> 00:12:36.000]   Then the market plate of 50% of Amazon sales are in the marketplace, third-party sellers,
[00:12:36.000 --> 00:12:43.840]   but they got locked in even though they lose 45% of revenue to Amazon and fees. Now, Amazon
[00:12:43.840 --> 00:12:51.200]   says, "Screw you. You're locked in." They start monetizing. He says the company's $31 billion.
[00:12:52.160 --> 00:12:57.280]   He puts it in quotes, "Advertizing program is really a peyola scheme that pits sellers against
[00:12:57.280 --> 00:13:01.920]   each other, forcing them to bid on the chance to be at the top of your search." What ultimately
[00:13:01.920 --> 00:13:07.120]   happens is you've insuredified your platform to the point where no one wants to use it anymore.
[00:13:07.120 --> 00:13:12.160]   This is very clearly where Elon is. Twitter at first was all about the users. They couldn't
[00:13:12.160 --> 00:13:16.000]   figure out how to monetize it. Then they got brands to go there. In fact, that's one of the
[00:13:16.000 --> 00:13:21.360]   reasons all of us were there. That's the best. You have to be there to promote your brand to build
[00:13:21.360 --> 00:13:26.880]   your audience. Then once they got them locked in, now they can say, "Hey, if you want to reach that
[00:13:26.880 --> 00:13:33.440]   audience which we own, it'll be $8, please." But you do that at the risk of driving people like
[00:13:33.440 --> 00:13:40.560]   Christina away. He's talking in this article particularly about TikTok doing this. But it
[00:13:40.560 --> 00:13:45.440]   happens to every one of these companies. His position, which I really agree with,
[00:13:46.400 --> 00:13:52.720]   is this is the way it is. You just move. You go to the next thing. You leave May Space
[00:13:52.720 --> 00:13:58.160]   for Facebook. You leave Facebook for somewhere else. But what we need regulation for is to make
[00:13:58.160 --> 00:14:04.880]   sure it's as friction-free as possible to move, to avoid the lock-in. You need interoperability.
[00:14:04.880 --> 00:14:13.040]   You need to make it easy to move somewhere else. Then you can let the market rule.
[00:14:14.080 --> 00:14:18.000]   He says, as I said at the start of this essay, this is towards the end, "In shirtification
[00:14:18.000 --> 00:14:27.280]   exerts a nearly irresistible gravity on platform capitalism." The staff, the executives, the
[00:14:27.280 --> 00:14:32.720]   shareholders, eventually they all say, "No, you got to inshertify. We need the money."
[00:14:32.720 --> 00:14:41.360]   But even the most locked-in user eventually reaches a breaking point and walks away or gets pushed.
[00:14:42.240 --> 00:14:46.480]   Individual product managers, executives,
[00:14:46.480 --> 00:14:51.120]   activists, shareholders all give preference to quick returns at the cost of sustainability.
[00:14:51.120 --> 00:14:56.480]   And they're in a race to see who can... I love Corey. Eat their seed corn first.
[00:14:56.480 --> 00:15:01.280]   Inshertification has only lasted for as long as it has because the internet has
[00:15:01.280 --> 00:15:06.000]   devolved into five giant websites, each filled with screenshots of the other four.
[00:15:06.000 --> 00:15:11.200]   Corey's getting a little cranky in his old age. I don't know.
[00:15:11.200 --> 00:15:17.600]   Inshertification kills. Google just laid off 12,000. And the company's in a full-blown panic
[00:15:17.600 --> 00:15:24.480]   over the rise of AI chatbots. What are your thoughts, Tim?
[00:15:24.480 --> 00:15:30.960]   I definitely... The pattern is very clear when we certainly seen it before. What's missing,
[00:15:30.960 --> 00:15:35.200]   though, I think, is the actual death of these platforms. I think Twitter is certainly struggling,
[00:15:35.200 --> 00:15:39.680]   and I think a lot of us are thinking that this day is outnumbered, but it's still incredibly
[00:15:39.680 --> 00:15:44.720]   huge, incredibly popular. And as Musk goes to say, engagement numbers are up because everyone's
[00:15:44.720 --> 00:15:48.960]   kind of watching the dumpster fire is smolder. So I think it's a little too early to say that
[00:15:48.960 --> 00:15:54.000]   Amazon has died, that Google has died, that Twitter has died. And so I think that's the piece of the
[00:15:54.000 --> 00:15:59.520]   pattern that's missing in this case for better or for worse. We certainly, there are, of course,
[00:15:59.520 --> 00:16:03.520]   plenty of companies. And this is, by the way, not just tech companies, any company with any
[00:16:03.520 --> 00:16:09.040]   consumers. There are plenty of companies in that graveyard. We're in the process of watching these
[00:16:09.040 --> 00:16:13.360]   companies move in that direction. But you're right. I mean, it's hard to imagine Google going away.
[00:16:13.360 --> 00:16:21.120]   Facebook, maybe it's not so hard to imagine. Although Facebook's latest data on engagement
[00:16:21.120 --> 00:16:27.120]   since they started pushing videos from people I don't even follow into my feed. Apparently,
[00:16:27.120 --> 00:16:31.600]   that's actually working at least right now in terms of engagement. The AI they're using
[00:16:31.600 --> 00:16:36.880]   to put videos in front of you actually does seem to determine stuff that people will watch. And so
[00:16:36.880 --> 00:16:44.800]   the numbers are a little encouraging lately, especially given how a little good news.
[00:16:44.800 --> 00:16:47.840]   Facebook has had any front in the last couple of years. Yeah.
[00:16:47.840 --> 00:16:57.760]   Christina, does this process end with the end of Twitter or does Twitter just kind of drag on?
[00:16:59.520 --> 00:17:03.440]   Well, it can be both, right? And we've seen both because we've definitely seen
[00:17:03.440 --> 00:17:09.280]   social networks just go under and just disappear. And that has happened. Google Plus is a great
[00:17:09.280 --> 00:17:14.160]   example of that where obviously, Google put a lot of money and a lot of effort into that. And it
[00:17:14.160 --> 00:17:18.320]   failed. And then they just shut it down and got rid of all the archives even, which I actually
[00:17:18.320 --> 00:17:24.320]   thought that was not a great move to not even keep the public archives available. But that was
[00:17:24.320 --> 00:17:30.400]   like a high-profile failure. There have been other ones. But then you also have instances where they
[00:17:30.400 --> 00:17:36.320]   continue to kind of stick around until they're sold and deleted and whatnot. MySpace being a
[00:17:36.320 --> 00:17:42.960]   great example of that where that has now had God only knows how many owners and people trying to
[00:17:42.960 --> 00:17:50.560]   use that very worthless at this point email list of users. But MySpace was bigger than Facebook
[00:17:50.560 --> 00:17:57.440]   up until about 2009, I want to say. And then you started seeing a really big migration of people
[00:17:57.440 --> 00:18:03.920]   from MySpace to Facebook to the point that MySpace just kind of became a dead zone except for a very
[00:18:03.920 --> 00:18:11.520]   specific niche of people. And that wasn't really unlike live journal and geosities and tumbler and
[00:18:11.520 --> 00:18:16.480]   some other things. It wasn't really because of any policy changes that MySpace made. It was just
[00:18:16.480 --> 00:18:23.600]   because the masses were all on Facebook. And Twitter is interesting because as Tim says,
[00:18:23.600 --> 00:18:30.320]   it's still this giant place. I think that what will potentially be pushing people off of it is
[00:18:30.320 --> 00:18:37.200]   less the alternatives and more when the overall experience becomes degraded, whether because
[00:18:37.200 --> 00:18:43.120]   more toxicity is there or just because you can't, you're having errors in your feet, you're not
[00:18:43.120 --> 00:18:47.680]   able to post things the right way, you can't refresh as quickly, you don't see all of your replies.
[00:18:47.680 --> 00:18:51.920]   That's the sort of thing that makes people go, "Okay, why am I investing time in this?"
[00:18:51.920 --> 00:18:59.680]   And arguably, you could say that the demise of Twitter started probably 2016. Ironically,
[00:18:59.680 --> 00:19:05.040]   when its engagement was higher, when you started to see a lot of the previous high-profile users of
[00:19:05.040 --> 00:19:12.240]   Twitter leave the platform for Instagram and then later TikTok. But you stop seeing the celebrities
[00:19:12.240 --> 00:19:17.280]   on Twitter. And I don't know. It's just one of those, what was the
[00:19:17.280 --> 00:19:21.280]   Neolosheism like it happens slowly and then all at once?
[00:19:21.280 --> 00:19:24.080]   It's like the collapse of the room.
[00:19:24.080 --> 00:19:30.480]   Gibbons said it first. F. Scott Fitzgerald said it about somebody going bankrupt, but I think it
[00:19:30.480 --> 00:19:36.080]   was Gibbons who said the Roman Empire collapsed slowly at first and then suddenly, and then it's
[00:19:36.080 --> 00:19:41.760]   been applied to a lot of things. You're right, although Instagram seems to be quite suddenly
[00:19:41.760 --> 00:19:44.480]   collapsing in on itself. Am I wrong?
[00:19:44.480 --> 00:19:49.760]   Totally. Well, again, you're not. And I think Instagram was one of those interesting ones where
[00:19:49.760 --> 00:19:55.840]   if they just stuck to their guns, when they copied Snapchat, that was brilliant because they
[00:19:55.840 --> 00:20:00.720]   did stories better than Snapchat did. They had a bigger audience and they added some features that
[00:20:00.720 --> 00:20:05.440]   made it better. So that was a perfect example of copying the right way. With TikTok, I think
[00:20:05.440 --> 00:20:10.160]   they just had fundamentally misunderstood their audience and it misunderstood that it's a completely
[00:20:10.160 --> 00:20:15.200]   different expectation. And if they wanted to create a TikTok compete, they should have created
[00:20:15.200 --> 00:20:18.720]   an app called Instagram Reels that I bet would have been very popular.
[00:20:18.720 --> 00:20:25.680]   But by loading it down with stuff that people you don't follow, people you're not even necessarily
[00:20:25.680 --> 00:20:31.280]   interested in, an algorithm that is not as good as TikToks and then you don't even see your friends'
[00:20:31.280 --> 00:20:36.000]   photos, the whole reason why people are there to begin with. Yeah, I spend a lot less time on
[00:20:36.000 --> 00:20:40.400]   Instagram because I'm like, what's the point? I used to come here for a specific reason.
[00:20:40.400 --> 00:20:45.120]   Now this isn't there. And even worse, it's a watered down version of this other thing that
[00:20:45.120 --> 00:20:51.040]   already exists. But we are creatures of habit and you're right, Tim, these things don't die,
[00:20:51.040 --> 00:20:56.480]   but they don't exactly thrive. There will be something called Twitter 10 years from now.
[00:20:56.480 --> 00:21:00.000]   It's just not entirely clear whether anyone will go get it all in my space. I believe they're
[00:21:00.000 --> 00:21:05.840]   still a friendster. By the way, I'm sorry, Sun also rises. It was happening. You win.
[00:21:05.840 --> 00:21:09.280]   I thought it was a special joke. You win in Jeopardy. I recently realized it wasn't.
[00:21:09.280 --> 00:21:12.640]   It was a maybe like a Fitzgerald quote. But maybe he's still not from Gibbon.
[00:21:12.640 --> 00:21:17.360]   I think Gibbon said it first, but I might be wrong on that as well. How did you go bankrupt
[00:21:17.360 --> 00:21:24.720]   two ways, gradually, then suddenly? Did you collapse two ways, gradually, then suddenly?
[00:21:26.800 --> 00:21:31.840]   This actually leads into this. There are tributaries off of this into a lot of the stories that we're
[00:21:31.840 --> 00:21:37.520]   talking about these days. And I don't want to do another Elon Musk-filled
[00:21:37.520 --> 00:21:42.160]   Twitter. So we're not everybody's gone. Oh, thank God. I thought he was going to start talking about
[00:21:42.160 --> 00:21:47.920]   Elon. But really, it's about companies in general going through this business cycle.
[00:21:48.880 --> 00:21:56.160]   Seeing that, your former employer, Tim Stevens, has been accused of some interesting shenanigans.
[00:21:56.160 --> 00:22:01.520]   We had Connie Gogelmo on two weeks ago, right when this was breaking. Remember, the 75 stories
[00:22:01.520 --> 00:22:06.880]   an AI had written in their personal finance section. She said, "Well, these are stories no
[00:22:06.880 --> 00:22:14.560]   reporter wants to write the basic boring stories. We had the AI write a first draft,
[00:22:14.560 --> 00:22:20.800]   and then an editor look at it, correct it, finish it, and then put it out. But now it's coming out that
[00:22:20.800 --> 00:22:25.280]   in fact there were far more errors that were not corrected, that a lot of the content wasn't very
[00:22:25.280 --> 00:22:30.560]   good, and that perhaps seen that has been using it more than just those 75 articles. She said,
[00:22:30.560 --> 00:22:36.880]   "Yeah, and we've used for years as many publications do, programs to put in stock prices."
[00:22:36.880 --> 00:22:42.800]   That's not using AI to write a story. There's a very different thing there. I don't blame
[00:22:42.800 --> 00:22:49.840]   for that. The Verge, though, has been really hammering on CNET. I don't know. Maybe they have a vested
[00:22:49.840 --> 00:22:57.360]   interest in knocking down a competitor. I don't know. But they're accusing CNET of doing something
[00:22:57.360 --> 00:23:04.640]   a little bit more nefarious. Remember, CNET was sold to an equity capital company called Red
[00:23:04.640 --> 00:23:08.880]   Ventures. Tim, you have some probably direct experience with this.
[00:23:08.880 --> 00:23:16.640]   A bit. Yes, I do. What always happens with these acquisitions is that the equity capital companies
[00:23:16.640 --> 00:23:21.840]   raise a lot of debt to acquire these companies. They're saddled with big debt.
[00:23:21.840 --> 00:23:27.520]   When you look at it across the corporate landscape these days, heavily encumbered companies owning
[00:23:27.520 --> 00:23:32.640]   these companies. A lot of debt. There's pressure on them from both their shareholders and their
[00:23:34.080 --> 00:23:41.920]   lenders to monetize. These companies very often either sell off pieces of the company that they
[00:23:41.920 --> 00:23:49.280]   bought or attempt to monetize it as Elon is doing with Twitter. The Verge is accusing CNET and Red
[00:23:49.280 --> 00:23:57.760]   Ventures. By the way, Red Ventures also owns a number of sites like The Points Guy, Bank Rate,
[00:23:57.760 --> 00:24:03.440]   and CreditCards.com, which are sites that make their money through credit card affiliate fees.
[00:24:04.240 --> 00:24:12.640]   The Verge is accusing them in effect of turning CNET into that kind of site with auto-generated
[00:24:12.640 --> 00:24:20.800]   link bait articles designed to rank highly in searches that they can then monetize with ads
[00:24:20.800 --> 00:24:25.920]   or affiliate fees. Bank Rate and Credit Cards have also published AI-written articles about
[00:24:25.920 --> 00:24:32.400]   credit cards with ads for credit cards nestled within. It turns out the same guy responsible for
[00:24:32.400 --> 00:24:38.080]   this at Bank Rate and Credit Cards is responsible for at its CNET, Lance Davis, vice president of
[00:24:38.080 --> 00:24:44.640]   content at Red Ventures. I think there's an interesting accusation here that Red Ventures is
[00:24:44.640 --> 00:24:51.680]   basically taking this venerable, highly respected name in technology journalism and turning it into
[00:24:51.680 --> 00:25:00.880]   an SEO farm. Tim, I'll give you the chance to mention it either to recuse yourself or to give us
[00:25:00.880 --> 00:25:07.040]   your thoughts. Obviously, I need to be a little bit careful with what I say here both because
[00:25:07.040 --> 00:25:10.320]   this is my former employer we're talking about. I have a lot of friends and a lot of people who
[00:25:10.320 --> 00:25:14.320]   I respect. I'm really, really, really interested in seeing that. I should say that. So many people,
[00:25:14.320 --> 00:25:20.240]   including Connie, that I love and respect and honor. I don't blame CNET for this one. I think
[00:25:20.240 --> 00:25:27.040]   this comes from Red Ventures. My take on this is a little bit complicated. I do think that
[00:25:28.240 --> 00:25:33.760]   clearly the Verge has an interest in making CNET look bad. They're competitors. That's fine.
[00:25:33.760 --> 00:25:37.760]   I don't think that anything that the Verge has reported this far from what I've seen has been
[00:25:37.760 --> 00:25:42.960]   inaccurate. I want to say one thing for sure. I wasn't aware of any of the stuff that was going
[00:25:42.960 --> 00:25:48.080]   on when I was there. I left CNET around August of last year. There was rumors and talk and that
[00:25:48.080 --> 00:25:51.280]   kind of thing, but I wasn't aware of anything going on. So I have no one's that are knowledge about
[00:25:51.280 --> 00:25:57.040]   how any of this came to pass. But I will say that CNET was using tools like Word Smith and others.
[00:25:57.840 --> 00:26:02.080]   And those are tools that a lot of outlets use. A lot of publications use those. And basically,
[00:26:02.080 --> 00:26:05.600]   what they do is they help you optimize the content that you're writing to make sure that they include
[00:26:05.600 --> 00:26:12.640]   the right keywords to make sure that they perform well in an algorithm-based environment. And that
[00:26:12.640 --> 00:26:16.560]   is really what consumers are operating within right now. Anyone who goes on the internet and
[00:26:16.560 --> 00:26:21.600]   searches for a thing is asking an algorithm what thing should I read. And so it's only natural for
[00:26:21.600 --> 00:26:26.800]   publications to want to make sure that their content performance as well as possible. The
[00:26:26.800 --> 00:26:31.440]   thing is when you use a tool like that, it can begin to feel like you are basically reverse
[00:26:31.440 --> 00:26:35.920]   engineering Google, your reverse engineering, a search engine. And that's really what this
[00:26:35.920 --> 00:26:40.880]   game comes down to. It's AI talking to AI, isn't it? Right. And that's what we were talking about.
[00:26:40.880 --> 00:26:46.080]   For Leo, at some point, what's the best tool to optimize content for an algorithm? It would be
[00:26:46.080 --> 00:26:51.280]   another algorithm effectively. And so I think by extension, it's a natural thing that CNET would
[00:26:51.280 --> 00:26:55.200]   do this. I don't think anybody would be surprised at CNET being one of- Well, especially when there's
[00:26:55.200 --> 00:27:01.280]   financial pressure to turn around a big acquisition, right? Maybe so. I think that the timing is a
[00:27:01.280 --> 00:27:04.320]   little bit irrelevant here. I mean, CNET has definitely been on the cutting edge of a lot of
[00:27:04.320 --> 00:27:10.480]   different publication types over the years, whether it be integrated, affiliate links, things like that.
[00:27:10.480 --> 00:27:13.600]   They've definitely been at the bleeding edge. So there's no surprise that they would be at the
[00:27:13.600 --> 00:27:17.920]   bleeding edge of adopting AI technology. My concern really is that there wasn't enough
[00:27:17.920 --> 00:27:22.800]   transparency involved here. I think that's what my problem is. If CNET had come out and said,
[00:27:22.800 --> 00:27:26.080]   "Hey, we're experimenting with AI. This is kind of fun and new. We don't really know what this is
[00:27:26.080 --> 00:27:29.200]   going to be, but here's what we're trying. This is where we're trying it on. This is some content
[00:27:29.200 --> 00:27:34.160]   that was written by AI. What do you think?" I think that this would have been, I'm sure that
[00:27:34.160 --> 00:27:38.880]   they would have gotten some blowback for sure. But from what I could see from my perspective and
[00:27:38.880 --> 00:27:44.160]   reading through coverage on the Virgin and elsewhere, it just seemed like it was, they were hoping
[00:27:44.160 --> 00:27:48.240]   that nobody would notice. And I feel like that's really the wrong way to go about doing this. If
[00:27:48.240 --> 00:27:52.320]   you're going to be embracing this kind of technology or investing in it, especially when you're talking
[00:27:52.320 --> 00:27:55.920]   about giving people recommendations about where they should put their money in a mortgage,
[00:27:55.920 --> 00:28:01.440]   I think it's important to be incredibly transparent. And Connie's piece was very transparent,
[00:28:01.440 --> 00:28:05.840]   but that came out long after the story had kind of blown up long after the
[00:28:05.840 --> 00:28:12.560]   Virgin's piece. And I think it's unfortunate that CNET wasn't more upfront with what was going on.
[00:28:12.560 --> 00:28:15.760]   Beyond the scenes, there were certainly, we saw the old disclaimers on Google and things like that.
[00:28:15.760 --> 00:28:20.320]   But that was, in my opinion, it was not enough. And that's where I'm disappointed in this whole thing.
[00:28:20.320 --> 00:28:25.920]   Yeah. And I don't blame Connie at all, or even Lindsey Turrentine, also, who has been a regular
[00:28:25.920 --> 00:28:30.640]   on this show for many years, I think I have huge respect for both of them. If anything,
[00:28:30.640 --> 00:28:35.280]   I feel like they might have been sandbagged by this. And they didn't know the full extent of what
[00:28:35.280 --> 00:28:44.640]   was going on and ended up being kind of hung out to dry, so to speak. The Verge quotes a former
[00:28:44.640 --> 00:28:50.480]   CNET employee saying Red Ventures was using automated technology for content long before the AI
[00:28:50.480 --> 00:28:55.280]   buy line began cropping up in November. They mentioned this Word Smith tool, which you talked
[00:28:55.280 --> 00:29:01.600]   about, Tim Nicknamed, "Morkgo Tron." Or, "Morkgo," I don't know how you pronounce that, "Morkgo
[00:29:01.600 --> 00:29:06.720]   Tron internally," because if it's used in mortgage stories, they said it had been used for at least
[00:29:06.720 --> 00:29:12.480]   a year and a half. But the siloed natures of the teams across CNET and Red Ventures makes it
[00:29:12.480 --> 00:29:17.520]   difficult for journalists at the site to understand the chain of command who's using what tools and
[00:29:17.520 --> 00:29:28.000]   when. So no blame on our friends at CNET. I'm very happy, frankly, to blame Red Ventures
[00:29:28.000 --> 00:29:32.800]   in any equity capital company, because I feel like these guys are, to some degree, the bane
[00:29:32.800 --> 00:29:40.800]   of our existence. But it's not just VC firms that are pushing companies to use this kind of content.
[00:29:40.800 --> 00:29:44.720]   A lot of editorial properties use SEO optimization tools. If you want to perform,
[00:29:44.720 --> 00:29:48.240]   if you want to be in the first page on a Google search, you have to be using these tools. So I
[00:29:48.240 --> 00:29:53.040]   know a lot of automotive properties are using them. This is not proprietary software. This is
[00:29:53.040 --> 00:29:56.480]   stuff that you can go out and license that anybody can use. I'll tell you what keywords that you
[00:29:56.480 --> 00:30:00.880]   need to inject into your content. And again, it does make you feel like your reverse engineering
[00:30:00.880 --> 00:30:03.920]   is your writing. But this is not proprietary stuff.
[00:30:03.920 --> 00:30:09.440]   Yeah. Well, in a way, that's scarier. If it's even more widespread use that we don't know about.
[00:30:09.440 --> 00:30:13.040]   I mean, I think they're, but for the grace of God goes everybody in the media business.
[00:30:13.040 --> 00:30:18.320]   Not us. We haven't figured out a way to do that yet with podcasts.
[00:30:18.320 --> 00:30:23.520]   Over the course, not immediately, but over the course of time, I think you will see AI play a role
[00:30:23.520 --> 00:30:31.920]   a lot more, particularly as some of the issues CNET ran into are less of an issue. And also,
[00:30:31.920 --> 00:30:35.840]   I mean, they've seen that probably made a lot of mistakes. So the rest of us don't have to and
[00:30:35.840 --> 00:30:42.560]   can learn from them in terms of disclosure. But I feel like, well, we're not doing any of this
[00:30:42.560 --> 00:30:48.080]   and have no plans to do this. In fact, it might not really work well for us anyhow. I would not say
[00:30:48.080 --> 00:30:54.400]   that FES company will never use AI in any form because I think things are going to happen quite
[00:30:54.400 --> 00:30:59.920]   quickly. And there might be ways to use it, which are actually completely above board and reasonable
[00:30:59.920 --> 00:31:08.640]   and resolved in better content rather than just cheaper content. Somebody said, I'm trying to find
[00:31:08.640 --> 00:31:18.160]   the article that chat GPT is the absolute definition of BS. Yes. Because and by the way,
[00:31:18.160 --> 00:31:23.040]   open AI, the creators of chat, GPT say this. They say, we never said you had to be accurate.
[00:31:23.040 --> 00:31:27.680]   That's not in the training at all. It has no idea what it's saying and whether it's correct or not.
[00:31:27.680 --> 00:31:32.240]   Sometimes it happens to be accurate, but that's not what the technology is. It's an accident if
[00:31:32.240 --> 00:31:38.000]   it happens to be accurate almost, right? I did a piece. I have a new newsletter, which I should
[00:31:38.000 --> 00:31:41.840]   plug at the end of the show. Oh, yes. Plug it now. It's called plugged in. And if you go to our
[00:31:41.840 --> 00:31:47.280]   FES company homepage, there should be a newsletter is one that will let you subscribe. And I, because
[00:31:47.280 --> 00:31:51.360]   I'm interested in the history of cartoons, I asked chat GPT what the first TV cartoon was.
[00:31:51.360 --> 00:31:56.960]   And every time I asked, it would give a different answer. Many of them very convincing and none of
[00:31:56.960 --> 00:32:02.960]   them correct. Basically, there are so many things where chat GPT has no idea what it's saying. And
[00:32:02.960 --> 00:32:07.680]   unless you already know what the answer is, you might well be fooled because it is able to
[00:32:07.680 --> 00:32:12.160]   lie in such a convincing fashion. But it's important to understand that that's not its mandate to
[00:32:12.160 --> 00:32:17.760]   tell the truth or to be accurate. It's not a fact generator. It's a BS generator. It's really good
[00:32:17.760 --> 00:32:23.600]   at stringing words together. You call it a glib bot, which I think is a very very good from now on.
[00:32:23.600 --> 00:32:34.000]   I'm calling it a glib bot. So in a way, then it makes you wonder, should we be, you know,
[00:32:34.000 --> 00:32:39.600]   your company GitHub and you can you can disclaim this again. I know you have nothing to do with
[00:32:39.600 --> 00:32:44.960]   Strutina, but is getting a little heat right now from the open source community over its AI
[00:32:44.960 --> 00:32:51.040]   code generator co-pilot, which is kind of impressive. Co-pilot also uses, we should mention,
[00:32:51.040 --> 00:32:54.640]   the same open AI technology as chat GPT. It is using GPT.
[00:32:54.640 --> 00:32:59.760]   Yeah, I was going to say, I can't comment on any of the loss that's already about stuff.
[00:32:59.760 --> 00:33:08.560]   But co-pilot does use the GPT 3, 3, 5, you know, a large language model that chat GPT is based on.
[00:33:08.560 --> 00:33:12.800]   It uses something called codecs, which is specifically focused on source code rather than,
[00:33:12.800 --> 00:33:19.440]   you know, the corpus that chat GPT uses, which is much more broad. But if you use chat GPT to say,
[00:33:19.440 --> 00:33:24.720]   right, you know, program that does this, this and this, most of its data set is probably coming from
[00:33:24.720 --> 00:33:30.320]   Yeah, because chat GPT can write code. In fact, one of the stories we had on security now is that
[00:33:30.320 --> 00:33:36.800]   script kitties are having chat GPT right effective malware.
[00:33:36.800 --> 00:33:49.200]   Malware that works. We know somebody who use chat GPT to write a PowerShell script for Steve
[00:33:49.200 --> 00:33:54.560]   on security now that looked through your last pass vault and told you some of its attributes
[00:33:54.560 --> 00:34:00.720]   and it worked. And it was a lot easier to develop it because and I guess co co pilot be even better.
[00:34:00.720 --> 00:34:06.720]   Now, clearly with co pilot, unlike chat GPT, there must be some rules in there to say,
[00:34:06.720 --> 00:34:11.360]   oh, and by the way, make sure this isn't made up that it actually works, right?
[00:34:11.360 --> 00:34:17.280]   For the most part, I mean, there are suggestions that you can get that will not run. So it is,
[00:34:17.280 --> 00:34:21.600]   that's why we call it co pilot. It's not do it for you. It's your co pilot. It's, you know,
[00:34:21.600 --> 00:34:27.360]   auto fill and suggestions, you know, plus one, right? So and the more that you use it, the more
[00:34:27.360 --> 00:34:32.320]   that it gets to know your code. It does get to know kind of your style and your intent and it
[00:34:32.320 --> 00:34:36.560]   can give you better and better suggestions for what you're doing. But no, you can absolutely the
[00:34:36.560 --> 00:34:42.960]   same way, you know, you could get a wrong suggestion. You could get, you know, a wrong, I guess,
[00:34:42.960 --> 00:34:47.360]   paragraph from chat GPT, you could get some incorrect code suggestions. For the most part,
[00:34:47.360 --> 00:34:52.880]   though, I think that the training model there is a little bit better because it is, you know,
[00:34:52.880 --> 00:34:58.480]   focused more on one thing rather than, you know, however great the corpus is for everything that
[00:34:58.480 --> 00:35:04.880]   chat GPT is doing. And as I said, it is learning based on your own style and the stuff that is
[00:35:04.880 --> 00:35:11.280]   in your project folder. But no, I mean, this is why I always tell people, look, co pilot is amazing
[00:35:11.280 --> 00:35:16.000]   and it has saved me so much time, especially with boilerplate stuff. But if you're trying to use it
[00:35:16.000 --> 00:35:20.400]   to just, you think you can just automate it to write a program for you, you might get lucky if
[00:35:20.400 --> 00:35:26.400]   it's something really simple, like a PowerShell script or something like that. But you, you really
[00:35:26.400 --> 00:35:32.320]   need to have a better idea of what you're doing so that you can actually see what coded suggestion
[00:35:32.320 --> 00:35:36.400]   suggesting and then make edits if that needs to be the case. But even if you still need to make
[00:35:36.400 --> 00:35:42.080]   edits, I think there's still value there because it can save you, you know, a lot of time of having
[00:35:42.080 --> 00:35:49.120]   to know manually Google and, you know, command C, command V from from stack overflow or wherever
[00:35:49.120 --> 00:35:54.480]   you are. Yeah, people, well, and that's every programmer knows this, but maybe a lot of civilians
[00:35:54.480 --> 00:35:59.200]   don't that almost all code is to some degree or another copy to paste from somebody else.
[00:35:59.200 --> 00:36:05.360]   That's kind of how it works. So co pilot is a natural way to do this. Co-pilot is quite
[00:36:05.360 --> 00:36:10.320]   impressive. It's quite amazing. Here's the story from earlier this month by Checkpoint
[00:36:10.320 --> 00:36:18.400]   Research, a malware research company. They call it O-PON AI cyber criminals starting to use chat
[00:36:18.400 --> 00:36:25.280]   GPT and Checkpoint's research. Previous blog, we described how chat GPT successfully conducted
[00:36:25.280 --> 00:36:31.840]   a full infection flow from creating a convincing spearfishing email to running a reverse shell
[00:36:32.400 --> 00:36:42.560]   capable accepting commands in English. That's pretty scary. This is a case of something called
[00:36:42.560 --> 00:36:51.360]   info stealer, which was created late last year by chat GPT. A cyber criminal showing how he used
[00:36:51.360 --> 00:37:00.320]   chat BPT to write the code looks like JavaScript. A hard code, it had a right code to basically
[00:37:00.320 --> 00:37:09.680]   steal files from a FTP server. It's kind of amazing what they're doing. One of the things that
[00:37:09.680 --> 00:37:14.320]   really becomes obvious is this is a conversation a year ago we might not have had. This has happened
[00:37:14.320 --> 00:37:20.400]   all of a sudden out of nowhere. And you can measure we're not that far away from these AAs being
[00:37:20.400 --> 00:37:25.840]   able to emulate. I mean, they can already do very compelling voice work. So how far are they from
[00:37:25.840 --> 00:37:31.280]   being to emulate your voice, your mom's voice, and make up a call and say, hey, you know,
[00:37:31.280 --> 00:37:36.240]   your mom, I forgot my password. Can you can you tell me? Oh, yeah, I think that's already I'm
[00:37:36.240 --> 00:37:43.680]   sure that's already that should be doable right now. Yeah. Yeah. There is a generative AI music
[00:37:43.680 --> 00:37:48.320]   already. It's not I don't think it's quite there yet. This is a paper from Google Research.
[00:37:48.320 --> 00:37:54.800]   They call it music LM. It's based on large language model like lambda. Generating music from a
[00:37:54.800 --> 00:38:03.360]   text prompt. Yep. Here is the main soundtrack of this is the prompt the main soundtrack of an arcade
[00:38:03.360 --> 00:38:10.720]   game. It is fast paced and upbeat. We didn't check my audio. Do you do you? I think we'll try it.
[00:38:10.720 --> 00:38:15.680]   Turn my audio on. I want to play this song. It's fast paced and upbeat with a catchy electric
[00:38:15.680 --> 00:38:20.560]   guitar riff. The music is repetitive and easy to remember with unexpected sounds like cymbal crashes
[00:38:20.560 --> 00:38:26.800]   or drum rolls. Does this sound like an arcade game to you? Maybe the
[00:38:26.800 --> 00:38:32.080]   front screen, the interact mode, or maybe sonic is running down the
[00:38:32.080 --> 00:38:41.760]   that's completely AI generated. Although I currently it's generated by an AI that
[00:38:41.760 --> 00:38:47.600]   there's a fair amount of plagiarizing, which is why oh yeah, it's really seemed to this. Yeah,
[00:38:47.600 --> 00:38:52.960]   it's totally play. Here's a slow tempo bass and drums led reggae song.
[00:38:52.960 --> 00:39:02.880]   Yeah, man. Everybody get together. We're going down to the beach. No, no. Ants is noted that
[00:39:02.880 --> 00:39:10.320]   seems like it has the potential to blow away the stock music industry pretty quickly.
[00:39:10.320 --> 00:39:13.760]   But yeah, it's a lot better than the crap stock music we're using.
[00:39:16.400 --> 00:39:22.640]   You'll be able to generate something unique to your own 280,000 hours of real music as the training
[00:39:22.640 --> 00:39:30.080]   model to generate coherent songs for descriptions of significant complexity as the creators put it.
[00:39:30.080 --> 00:39:36.480]   You want to feel like you're lost in space, Ant, and is becoming our taste tester.
[00:39:36.480 --> 00:39:38.800]   Let's see if Ant agrees this is.
[00:39:38.800 --> 00:39:44.640]   Sounds like an AI did it, doesn't it?
[00:39:46.240 --> 00:39:47.360]   Sounds like robot music.
[00:39:47.360 --> 00:39:54.960]   Now here's the question. Can we get taken down from YouTube for playing that?
[00:39:54.960 --> 00:39:58.160]   You may get sued by a bot.
[00:39:58.160 --> 00:40:02.480]   See that's good. No, that's well, that's going to be an interesting thing. I think actually,
[00:40:02.480 --> 00:40:03.920]   who owns it?
[00:40:03.920 --> 00:40:07.760]   You're able to generate these unique things. Right. Well, that that's an interesting question,
[00:40:07.760 --> 00:40:12.800]   but also I think it becomes a very interesting question, which is, you know, I think that this
[00:40:12.800 --> 00:40:19.840]   YouTube relies on someone else being able to say, I have the copyright of this and usually have
[00:40:19.840 --> 00:40:25.920]   like a file registers in place that their, you know, a content ID can go and find the same thing.
[00:40:25.920 --> 00:40:31.680]   But if it's a uniquely original file, then content is not going to find it. So that's...
[00:40:31.680 --> 00:40:35.120]   What a world we live in. That's cool. What a world.
[00:40:35.120 --> 00:40:41.120]   But do you think there might be some cool stuff that might happen if actual human musicians
[00:40:41.120 --> 00:40:46.560]   work with some of these tools to brainstorm and riff on ideas? And it seems like that could be kind
[00:40:46.560 --> 00:40:51.600]   of cool. Yeah, I'm sure it's happening right now. No, exactly. I mean, honestly, I think that
[00:40:51.600 --> 00:40:56.160]   the way that... And I know that a lot of creators are really freaked out by generative art and
[00:40:56.160 --> 00:41:02.240]   generative music and all this stuff. And I understand the fear, but for me, what excites me about this
[00:41:02.240 --> 00:41:08.960]   is that the best AI art that I've seen has been from actual artists. Like those are the people who
[00:41:08.960 --> 00:41:12.560]   have been using the best prompts or have been taken some of the prompts and have taken some of the
[00:41:12.560 --> 00:41:17.200]   results and then made really great things. And I think with music, it's the exact same way, right?
[00:41:17.200 --> 00:41:22.320]   Like you might be able to get something that sounds slightly better than stock music, but it's
[00:41:22.320 --> 00:41:28.480]   still not going to be great, right? It's going to take a real artist to then take that and edit it
[00:41:28.480 --> 00:41:33.760]   and interpolate it and do what real artists have always done and turn it into something else. And
[00:41:33.760 --> 00:41:39.920]   so what I've been trying to tell people, because this isn't going away. Whatever your feelings
[00:41:39.920 --> 00:41:44.640]   on this stuff is, it's not going away and it's only going to become bigger. We can have conversations
[00:41:44.640 --> 00:41:49.120]   about ethics and we should. We can have conversations about safety rails and we should, but this is
[00:41:49.120 --> 00:41:53.920]   not going away. And so what I've been... The conversation I've been having with people for the last year or so
[00:41:53.920 --> 00:42:01.760]   is like, embrace this as a tool to your arsenal to make new, unique and better things rather than
[00:42:01.760 --> 00:42:06.240]   looking at this as some sort of existential threat, because you're not going to outpace this. This
[00:42:06.240 --> 00:42:11.280]   is not going to be something that you can get away from, but it might be something that if you
[00:42:11.280 --> 00:42:17.680]   are able to use, you could actually enhance the stuff that you do naturally. And that goes for
[00:42:17.680 --> 00:42:22.960]   writers as well. Last week, Brianna Wu, who was on the show, her husband writes science fiction among
[00:42:22.960 --> 00:42:28.320]   many other things, said that Frank was stuck with a story that he was, I think he was writing for
[00:42:28.320 --> 00:42:34.240]   analog, but he was stuck with a story and he gave a very extensive prompt to chat GPT, which wrote
[00:42:34.240 --> 00:42:39.760]   kind of a mediocre story, but came up with a lot of things that became a starting point for him
[00:42:39.760 --> 00:42:44.800]   and unstuck him. And that seems like that's a very good use of something like chat GPT.
[00:42:44.800 --> 00:42:51.680]   I've heard so many descriptions. I love your name for it. What is it?
[00:42:51.680 --> 00:43:00.720]   Glib PT. I like that. I like remember I wrote that. Yeah, that's good. I've also heard it say
[00:43:00.720 --> 00:43:07.360]   the ultimate man's splainer, because it's confidently wrong. Right? And it's so confident. It's a little
[00:43:07.360 --> 00:43:11.760]   patronizing. It's like, Oh, no, let me explain to you how the world works. Although, if you tell it
[00:43:11.760 --> 00:43:17.600]   that it's wrong, then it gets really humble and apologizes a great length and says it'll never
[00:43:17.600 --> 00:43:21.840]   do it again. Does it correct itself? If you correct it, does it stay correct? In fact, if it says
[00:43:21.840 --> 00:43:25.120]   something that's correct and you tell it that it's wrong, it will apologize for that too.
[00:43:25.120 --> 00:43:34.400]   Stephen Wolfram wrote a very good piece about how confidently wrong chat GPT is on things that
[00:43:34.400 --> 00:43:39.520]   Wolfram Alpha, his his own kind of AI, is it an AI? I don't know what you call Wolfram Alpha,
[00:43:39.520 --> 00:43:46.720]   search engine for knowledge or something. But he said, if we should partner because we're good at
[00:43:46.720 --> 00:43:52.800]   getting the math right, which chat GPT is terrible. And then if we worked it, if we work together,
[00:43:52.800 --> 00:43:58.240]   we maybe get something out of it. He pointed out some really hysterical examples. This is his
[00:43:58.240 --> 00:44:04.800]   article from his blog at Stephen Wolfram.com. Some hysterical examples of just chat TPT getting
[00:44:04.800 --> 00:44:10.800]   it terribly wrong. How far is Chicago from Tokyo to which chat GPT confidently says,
[00:44:10.800 --> 00:44:17.200]   the distance from Chicago, Illinois to Tokyo, Japan is approximately 7,600 miles, that would be 12,200
[00:44:17.200 --> 00:44:23.040]   kilometers. It's a very long distance. Blah, blah, blah, blah. Turns out it's not even close. It's 6,313
[00:44:23.040 --> 00:44:31.920]   miles. So you correct it. So you tell it and it says, thank you for correcting me. You're correct.
[00:44:31.920 --> 00:44:41.040]   So of course, the distance is 6,313 miles. How far is Chicago to Tokyo? And then it gets it right,
[00:44:41.040 --> 00:44:48.240]   at least in that continued conversation. I think that's interesting. But kids don't do your math
[00:44:48.240 --> 00:44:54.320]   homework with chat GPT stick to Wolfram Alpha, because it doesn't even know three to the power of 73,
[00:44:54.320 --> 00:45:03.600]   which is pretty pathetic. By the way, not even close. It said 14 billion. I can't say how big the
[00:45:03.600 --> 00:45:09.760]   number is. It's a lot larger. There was that story about chat GPT passing an MBA exam. But the
[00:45:09.760 --> 00:45:14.320]   article which said it also pointed out that it wasn't capable of doing high school math, which
[00:45:14.320 --> 00:45:19.920]   right? That's not interesting because I was-- So many MBAs can't do that. I didn't realize you
[00:45:19.920 --> 00:45:26.160]   could become an MBA without having high school math. But I think it just passed a law school
[00:45:26.160 --> 00:45:31.920]   exam too, didn't it? This is now the new thing is for professors to give their exams to chat GPT.
[00:45:31.920 --> 00:45:38.960]   There was a paper that a couple of the-- it was from the University of Chicago when someone else
[00:45:38.960 --> 00:45:45.200]   did with a GPT passing the bar. And they gave it part of the multiple choice parts of the bar exam.
[00:45:45.200 --> 00:45:51.600]   And it did better than random selection. And it came close to humans in a couple of categories.
[00:45:51.600 --> 00:45:57.360]   It got a C plus. But it's not like a-- Right. It didn't quite pass. But it is impressive,
[00:45:57.360 --> 00:46:01.040]   because the interesting thing though was that it did significantly better than random selection.
[00:46:01.040 --> 00:46:06.080]   Like it wasn't one of those things where you're just randomly, okay, how would you have done if
[00:46:06.080 --> 00:46:13.440]   you were just randomly selected in the answer? So it had some better accuracy. And in some categories,
[00:46:13.440 --> 00:46:18.240]   it was close to humans. But obviously, this is only for the multiple choice parts. And it did
[00:46:18.240 --> 00:46:24.080]   better in certain areas than others. But I mean, to me, all this really says is, okay, then you--
[00:46:24.080 --> 00:46:29.920]   if you're big concern, whether it's high school students or graduate students,
[00:46:29.920 --> 00:46:35.040]   and professional taking tests, if you're big concern is the AI cheating at the test,
[00:46:35.040 --> 00:46:39.120]   well, then you need to start changing how you're testing. You're obviously not testing the right
[00:46:39.120 --> 00:46:45.440]   things. That to me is the big takeaway. We shouldn't be freaked out that these AI's are able to pass
[00:46:45.440 --> 00:46:49.760]   the test. It's more like, okay, well, what's the goal of this? And are we testing the right way?
[00:46:49.760 --> 00:46:53.440]   And I think in most cases, the answer would be no, we're not testing the right way.
[00:46:53.440 --> 00:47:00.080]   Yeah, maybe that's the flaw of the tests. Although, as you point out, chat GPT doesn't do math very
[00:47:00.080 --> 00:47:06.320]   well. It's good in constitutional law. How long do we think it'll be until chat GPT is our
[00:47:06.320 --> 00:47:10.960]   public defender and that you need to pay extra if you want a human to defend you in a lawsuit?
[00:47:10.960 --> 00:47:18.240]   I don't think that's going to happen. The guy who was doing the robot lawyer, I think,
[00:47:18.240 --> 00:47:22.640]   has decided to run away with his tail between his legs because--
[00:47:22.640 --> 00:47:28.400]   so do not pay, which is good. Actually, a really cool service would help you get out of traffic
[00:47:28.400 --> 00:47:35.360]   tickets, created an AI-powered robot lawyer that was going to go into court. I don't know,
[00:47:35.360 --> 00:47:39.920]   first of all, I think any judge that would throw it out immediately was going to go into a court
[00:47:39.920 --> 00:47:47.920]   to help fight a traffic ticket. State bar prosecutors threatened the Joshua Bratter as the CEO of
[00:47:47.920 --> 00:47:55.440]   Do Not Pay with Jail Time. And so Joshua says, "We're postponing our court case and we're going to
[00:47:55.440 --> 00:48:02.320]   stick to consumer rights." Well, okay. Totally. Well, this is the whole thing, right? It's like,
[00:48:02.320 --> 00:48:08.720]   couldn't it maybe? But do you think that there-- of any profession, can you think of any class of
[00:48:08.720 --> 00:48:18.000]   profession who would be less likely to allow this into-- no. Even if you could potentially
[00:48:18.000 --> 00:48:22.800]   automate things and do things better than your typical public defender, do you really think that
[00:48:22.800 --> 00:48:30.640]   bar association and the various lobbying groups for lawyers, do you really think that they would
[00:48:30.640 --> 00:48:34.960]   allow this? And their course, absolutely not. They're going to protect their own interests
[00:48:34.960 --> 00:48:38.480]   above and beyond more than any other industry. They're going to be the ones who are like,
[00:48:38.480 --> 00:48:42.640]   "Nope, not happening." Yeah, that's a good point. If you're going to pick an industry to
[00:48:42.640 --> 00:48:48.880]   disintermediate, do podcasters. Don't do lawyers. We're putch over. It's absolutely.
[00:48:48.880 --> 00:48:51.200]   It'll be a lot easier to go after us.
[00:48:51.200 --> 00:48:59.520]   Your company, Microsoft, just to acknowledge that they're putting in-- they already put a
[00:48:59.520 --> 00:49:05.680]   billion dollars in. They were one of the founders of OpenAI. Now they have an even better deal with
[00:49:05.680 --> 00:49:11.360]   OpenAI. The rumor was $10-- additional $10 billion, I think that was confirmed by such an Adela
[00:49:11.360 --> 00:49:18.320]   over a period of time, obviously. And the chat GPT or that kind of technology will be used in
[00:49:18.320 --> 00:49:25.200]   Microsoft Office. But I think a number of people are saying the real thing to watch is Bing.
[00:49:27.760 --> 00:49:31.360]   Thoughts about that? I know. And you work at--
[00:49:31.360 --> 00:49:36.960]   I work at GitHub. So it's just owned by Microsoft. Pinn is my own. Look, I think this is exciting.
[00:49:36.960 --> 00:49:43.920]   I think that there's also been reporting that Google's been having a crisis about how successful
[00:49:43.920 --> 00:49:51.440]   chat GPT has been. And I don't blame them because Google has amassed-- and this is not in any way
[00:49:51.440 --> 00:49:57.280]   to try to integrate any other company. But they have probably amassed the largest quantity of AI
[00:49:57.280 --> 00:50:02.880]   talent from academia and from industry of anyone. And the fact that it was
[00:50:02.880 --> 00:50:06.720]   chat GPT, which was interesting to me about that, is that it wasn't really-- that's
[00:50:06.720 --> 00:50:10.960]   emotionally different from any of the other GPT-3 things that have been available. It was just the
[00:50:10.960 --> 00:50:17.520]   interface that I think made it so accessible has become this very mainstream thing where I've
[00:50:17.520 --> 00:50:22.960]   been thinking and have been talking about OpenAI stuff for several years. But now this is a
[00:50:22.960 --> 00:50:28.800]   mainstream thing because the interface was so ripened. And yeah, I definitely think that search is a
[00:50:28.800 --> 00:50:35.040]   great area where it could be helpful. People have created extensions to add chat GPT things
[00:50:35.040 --> 00:50:41.680]   alongside Google results. And it's better. And I think that Google results-- Google is the primary
[00:50:41.680 --> 00:50:47.760]   search engine that I use. And the results have gotten worse over time. And I don't think that it's
[00:50:47.760 --> 00:50:55.200]   because of the SEO stuff. I think it's because Google has optimized for different sorts of results.
[00:50:55.200 --> 00:51:00.880]   And they've wanted to highlight other things. And so I often end up piping Reddit into my search
[00:51:00.880 --> 00:51:04.960]   because I find that I get much better results from Reddit than I do.
[00:51:04.960 --> 00:51:09.040]   >> Because those are-- because what you're really doing is asking for information from real
[00:51:09.040 --> 00:51:14.720]   experts about a topic, right? >> Right. Or I know I just want to actually get the conversation,
[00:51:14.720 --> 00:51:19.600]   like the info where it's actually going to be. But searching Reddit.com is a mess. So searching
[00:51:19.600 --> 00:51:24.640]   Google for querying and adding Reddit to it is a good alternative. But people have created,
[00:51:24.640 --> 00:51:30.320]   you know, like side-by-side extensions to add, you know, chat GPT stuff to Google things. And I
[00:51:30.320 --> 00:51:34.960]   think that, yeah, this is an opportunity for Bing. I think it's an opportunity for a lot of consumer
[00:51:34.960 --> 00:51:42.160]   products. Obviously, one of the big wins here is for Azure for, you know, other businesses who
[00:51:42.160 --> 00:51:47.280]   want to take advantage of those models and build it into their products, having kind of AIs of service.
[00:51:47.280 --> 00:51:52.800]   I think, you know, look, this is going to be hot. Everybody's going to be-- this is going to be
[00:51:52.800 --> 00:51:59.600]   becoming an arms race, right? Even more than it already has been. But for one of a reason, you know,
[00:51:59.600 --> 00:52:06.000]   open AIs has been the first to really commercialize this in a way that the mainstream understands.
[00:52:06.000 --> 00:52:12.640]   And it's exciting. I mean, personally, as a technologist, to me, all the other kind of peers,
[00:52:12.640 --> 00:52:17.440]   we might have around it. Like, I look at this as a moment of this is exciting. Like, to me,
[00:52:17.440 --> 00:52:21.840]   this is much more than an expect thing versus the metaverse. Like, this is much more exciting to me
[00:52:21.840 --> 00:52:27.200]   and seems much more casual as to what the next big place of computing is going to be. Forget about
[00:52:27.200 --> 00:52:32.240]   the metaverse stuff. The AI stuff is, I think, really what's exciting.
[00:52:32.240 --> 00:52:37.920]   OpenAI has less to lose than Google or Microsoft. Well, that's why OpenAI was created, really, right?
[00:52:37.920 --> 00:52:43.920]   I mean, these are enormous companies with enormous customer basis and reputations and
[00:52:43.920 --> 00:52:50.720]   paying customers. OpenAI, not having any of that stuff. Why not throw it out into the public and
[00:52:50.720 --> 00:52:57.360]   see what happens? Although, Jan Lecun, who does is the genius AI researcher at Facebook,
[00:52:59.600 --> 00:53:05.360]   said that chat GPT isn't particularly innovative. We've been doing that for years.
[00:53:05.360 --> 00:53:08.720]   I think if you're an AI scientist, you know, Dr. Transformers.
[00:53:08.720 --> 00:53:09.680]   Yeah. Yeah.
[00:53:09.680 --> 00:53:12.720]   Which were in Google, basically, in the Transformer technology.
[00:53:12.720 --> 00:53:14.400]   Oh, this is what Lambda did.
[00:53:14.400 --> 00:53:17.120]   Yes, they did. Yeah. And meta's done some cool stuff with them too.
[00:53:17.120 --> 00:53:22.000]   But it also feels a little bit like sour grapes, right? It's like, oh, no, we did that.
[00:53:22.000 --> 00:53:25.600]   I was just going to say, sure, you have, but you didn't bother sizing.
[00:53:25.600 --> 00:53:26.640]   You didn't tell anybody.
[00:53:26.640 --> 00:53:30.560]   No, like, right. You didn't, you didn't productize it. Like, I don't think that anybody would make
[00:53:30.560 --> 00:53:34.880]   the argument. I don't think Sam Altman, or do you want from OpenAI, would be like, oh,
[00:53:34.880 --> 00:53:38.720]   this is the most innovative thing and no one else has done this. I think what they would say is,
[00:53:38.720 --> 00:53:43.680]   this is the first time that the public has actually been able to interact with it in a way that had
[00:53:43.680 --> 00:53:46.880]   a really good user interface. Chatbot was a brilliant user interface.
[00:53:46.880 --> 00:53:52.240]   That's what Lecun said. He said chat GPT is, quote, well put together. He said that compared to
[00:53:52.240 --> 00:53:57.680]   other companies in the field, OpenAI is not particularly advanced. Google meta.
[00:53:57.680 --> 00:54:01.760]   And he said half a dozen other startups have equivalent technologies.
[00:54:01.760 --> 00:54:07.840]   But that's the difference. They were doing this in public and letting the public use it.
[00:54:07.840 --> 00:54:13.040]   Although it makes me wonder, is there something better under the hood somewhere else?
[00:54:13.040 --> 00:54:16.640]   Well, GPT-4 apparently isn't an enormous advanced over 3.5.
[00:54:16.640 --> 00:54:21.440]   Well, Sam Altman, CEO of OpenAI, is it? Don't get your hopes up. It's not, it's not, it's not
[00:54:21.440 --> 00:54:24.880]   AGI, right? It's not the general, artificial general intelligence.
[00:54:24.880 --> 00:54:28.400]   Right. Meta actually did put out a AI chatbot a few months ago.
[00:54:28.400 --> 00:54:31.760]   And they immediately got flack for it.
[00:54:31.760 --> 00:54:33.200]   Did they get racist instantly?
[00:54:33.200 --> 00:54:37.120]   Being racist and anti-Semitic and so forth. So they, they were,
[00:54:37.120 --> 00:54:41.440]   they tried to be bold, but they weren't quite as bold as chat GPT. So they didn't get as much
[00:54:41.440 --> 00:54:46.080]   credit and they got a lot more flack about it. I think partially because Meta's the kind of
[00:54:46.080 --> 00:54:50.480]   company that's going to get flack no matter what it does, which is not true of OpenAI at least yet.
[00:54:50.480 --> 00:54:54.880]   I've been using a search engine that was founded about five years ago by
[00:54:54.880 --> 00:55:00.160]   former Google search executives called NIVA. Are you familiar with this?
[00:55:00.160 --> 00:55:01.280]   I read a big story on it.
[00:55:01.280 --> 00:55:02.880]   Oh, it's like that's how I learned about it.
[00:55:02.880 --> 00:55:07.200]   And the CEO is a former top guy at YouTube.
[00:55:07.200 --> 00:55:08.640]   Yeah.
[00:55:08.640 --> 00:55:13.040]   And he got a little bit depressed about the monetization of search.
[00:55:13.040 --> 00:55:14.800]   The inshertification of Google.
[00:55:14.800 --> 00:55:15.360]   Have you worked at Google?
[00:55:15.360 --> 00:55:19.280]   So he went off to do a search engine with a paid model.
[00:55:19.280 --> 00:55:21.680]   And yeah, that's the premise. We don't run ads. We don't have.
[00:55:21.680 --> 00:55:24.720]   In fact, even when Google started Larry Page famously wrote,
[00:55:24.720 --> 00:55:30.960]   a search engine can't have advertising or it will then become beholden to the advertisers.
[00:55:30.960 --> 00:55:37.120]   They only held that off for a few years before getting involved in advertising.
[00:55:37.120 --> 00:55:42.800]   So I pay five bucks a month for NIVA. I get a lot of, they actually give you a free one
[00:55:42.800 --> 00:55:47.440]   password account and other stuff. But I think it's really good. And also,
[00:55:48.080 --> 00:55:55.440]   because they're in this arms race, they added a AI generator at the beginning of search results.
[00:55:55.440 --> 00:56:01.200]   So I search for chat, GPT and Bingjis now. And this is the result I got from the AI.
[00:56:01.200 --> 00:56:04.880]   I think AI is very good at synopsizing and summarizing other content.
[00:56:04.880 --> 00:56:09.200]   So they even do footnotes to say where this information comes from.
[00:56:09.200 --> 00:56:11.680]   CNET, the Guardian Observer and the Verge.
[00:56:11.680 --> 00:56:14.880]   Microsoft is reportedly integrating ADI technology,
[00:56:14.880 --> 00:56:20.160]   such as chat GPT into its Bing search engine, which could potentially revolutionize searches.
[00:56:20.160 --> 00:56:23.280]   We know it. This technology is capable of generating a wide variety of
[00:56:23.280 --> 00:56:26.080]   text and human-like ways in response to written prompts.
[00:56:26.080 --> 00:56:29.040]   Microsoft hopes to launch this feature before the end of March
[00:56:29.040 --> 00:56:32.480]   in a bid to make Bing more competitive with Google.
[00:56:32.480 --> 00:56:37.520]   I think Google should be scared, not just by Bing, but by NIVA. I think this is pretty cool.
[00:56:37.520 --> 00:56:42.960]   I've been using NIVA full time instead of Google everywhere,
[00:56:42.960 --> 00:56:46.880]   including on my iPhone for about a month now.
[00:56:46.880 --> 00:56:51.840]   The only negative, the only hit on it is it's amazing how quickly Google comes back with the
[00:56:51.840 --> 00:56:58.080]   result. NIVA, there's a palpable second or two. But other than that, the results are excellent.
[00:56:58.080 --> 00:57:04.400]   I love this AI thing. And there's no ads. It doesn't favor Google content over anybody else's
[00:57:04.400 --> 00:57:12.160]   content. I think there's a dash of Bing and NIVA's technology along with some of its own technology.
[00:57:12.160 --> 00:57:15.120]   Oh, isn't interesting. I believe they've licensed some data.
[00:57:15.120 --> 00:57:18.080]   Okay. Do they have their own crawler, right? Yes.
[00:57:18.080 --> 00:57:23.040]   I think they kind of mashed together some of their own stuff and some stuff they've licensed.
[00:57:23.040 --> 00:57:27.920]   And I don't know what this thing is, but it's pretty cool. There's a little slider here at the top.
[00:57:27.920 --> 00:57:34.080]   Currently showing top news from all sources. Currently showing top news from all sources.
[00:57:34.080 --> 00:57:38.320]   I don't know. There's something going on there that I can move around that slider.
[00:57:39.840 --> 00:57:44.080]   I think it's very innovative. I have no relationship with them. In fact,
[00:57:44.080 --> 00:57:47.760]   I'm meant to ask you about this because I did read your article about it. You talk to them.
[00:57:47.760 --> 00:57:52.400]   You think they're pretty compelling? They're smart folks. They've added a lot of stuff since
[00:57:52.400 --> 00:57:56.240]   they launched. It's brave to say we're going to go against Google. Yeah. I mean, they're a tiny
[00:57:56.240 --> 00:58:02.000]   company. But maybe now we've all been used to getting our search free for the last 20 years.
[00:58:02.000 --> 00:58:08.240]   But I think if there is a time where we're at an inflection point where the idea of going up
[00:58:08.240 --> 00:58:12.560]   against Google no longer sounds quite so insane, it's now. Although, of course,
[00:58:12.560 --> 00:58:16.960]   Microsoft is probably in the best place to take advantage of this inflection point,
[00:58:16.960 --> 00:58:22.080]   given that it's already a large company with a large search engine. Although I am curious how
[00:58:22.080 --> 00:58:28.560]   they could shortly roll out chat GPT as part of being just because of this issue with accuracy.
[00:58:28.560 --> 00:58:35.120]   Yeah. I think the way Niva does it with the footnotes is the only way you could do it, right?
[00:58:36.720 --> 00:58:40.080]   And this is the difference between that knowledge graph in Google, which is
[00:58:40.080 --> 00:58:43.680]   almost entirely from Wikipedia almost always and is never sourced.
[00:58:43.680 --> 00:58:49.440]   At least Niva says where this stuff came from. I've found it actually quite useful. I'll ask it
[00:58:49.440 --> 00:58:54.880]   kind of technical questions like coding questions like what described Dijkstra's algorithm.
[00:58:54.880 --> 00:59:03.040]   And it does a really good job. This is exactly what chat GPT should be good at. And nevertheless,
[00:59:04.880 --> 00:59:09.760]   to beat Google at its own game is not maybe not. I don't know. Maybe now's it. Now's the time to do it.
[00:59:09.760 --> 00:59:18.160]   This is this is your your article from last June. Soon before last. Oh, yeah, 2021.
[00:59:18.160 --> 00:59:19.680]   That was right when they were first launching. Yeah.
[00:59:19.680 --> 00:59:27.040]   I hope they do well. I'm very it's very it's an interesting bet. And I love not having the ads in
[00:59:27.040 --> 00:59:33.840]   there. And I just hope they continue to be kind of agnostic, you know, not picking sides. I don't
[00:59:33.840 --> 00:59:38.880]   want them to be being the licensee or a duck duck go licensee. They also, by the way, when you install
[00:59:38.880 --> 00:59:45.840]   it, they install a tracker, which an anti tracker tool plug in in your browser, which shows you
[00:59:45.840 --> 00:59:55.360]   what trackers are on fast company. There you go. Not bad. There are far worse. Let me tell you,
[00:59:55.360 --> 01:00:00.480]   there's somewhere there's 30 or 40 trackers on a single page. It's kind of kind of amazing.
[01:00:00.480 --> 01:00:05.840]   We were trying to make our pages meaner and leaner just because that makes them run faster and
[01:00:05.840 --> 01:00:11.440]   got a load fast results and happier users. Yeah. All right. Want to take a little break. There is
[01:00:11.440 --> 01:00:15.040]   a lot more to talk about. We've got a great panel. Couldn't have a better panel for this
[01:00:15.040 --> 01:00:22.160]   conversation. Tim Stevens is with us now freelance and doing great. He's driving his way home on
[01:00:22.160 --> 01:00:30.000]   sub stack at Tim Stevens dot sub stack dot com. He is also unmasted on the masted on dot social,
[01:00:30.000 --> 01:00:36.160]   but still says a little bit, a little tiny bit of Twitter in there too. Thank you, Tim,
[01:00:36.160 --> 01:00:40.160]   for being here. We appreciate it. Harry McCracken. Thank you. The technology,
[01:00:40.160 --> 01:00:45.760]   global tech editor at Fast Company. We started putting people's mastathons up on the screen.
[01:00:45.760 --> 01:00:50.400]   There we go. I think that's great. You can't add anything more because it's like two seems to
[01:00:50.400 --> 01:00:55.280]   be the maximum. Yeah. Well, we got your Twitter and your mastathons. There you go. Are you on
[01:00:55.280 --> 01:00:59.920]   post as well? I have an account, but I haven't really been using that. See, to me, going to post
[01:00:59.920 --> 01:01:03.760]   is like not learning the lesson of Twitter. It's like, Oh, good. Let them mark and Driessen
[01:01:03.760 --> 01:01:08.800]   run everything. Right? No, I don't. I think it's better to be. I love the idea that we know
[01:01:08.800 --> 01:01:13.920]   somewhere that is not owned by somebody. Right? I really like that. And if you if you suddenly
[01:01:13.920 --> 01:01:18.880]   are you're on Twitch, social, or mastodon, you hate the way I'm running it, you go somewhere else.
[01:01:18.880 --> 01:01:25.520]   You know, that's easy. Also on the new mastodon user and more than welcome,
[01:01:25.520 --> 01:01:30.560]   Christina Warren, film girl. And are you using film girl? You are at Mastodon. That's social.
[01:01:30.560 --> 01:01:36.320]   I am. Yeah. Yeah. I'm at film industry. I might wind up switching to another instance at some point
[01:01:36.320 --> 01:01:41.520]   because the mastodon. You're the big one. So big, big, big, big. Yeah, there can have had
[01:01:41.520 --> 01:01:51.040]   the accounts since 2018. I don't know. I I had it just to have it, but it's pretty easy to move
[01:01:51.040 --> 01:01:57.040]   your followers. It's hard to move your toots. You can do it, but most of the time, I think the
[01:01:57.040 --> 01:02:02.960]   stuff that you have tooted or tweeted the old stuff, eh, you know, that's water under the bridge.
[01:02:02.960 --> 01:02:06.400]   Start fresh, but you at least can bring your followers with you. That's very easy to do that
[01:02:06.400 --> 01:02:15.280]   on the Mastodon. I I like our Twitch social because it's you have to be a Twitch listener to be in there.
[01:02:15.280 --> 01:02:21.120]   So it is a community. You're on Harry's on SFBA, which is for San Francisco Bay Area people.
[01:02:21.120 --> 01:02:28.080]   The local timeline really gets a point of view. If you choose wisely, when you're on somewhere
[01:02:28.080 --> 01:02:32.000]   like Mastodon, I thought social is just like a mini Twitter, basically, it's everybody who
[01:02:32.640 --> 01:02:38.000]   didn't didn't look farther than the biggest instance. And it's also pretty big now. It's well over
[01:02:38.000 --> 01:02:42.880]   a hundred thousand people. So that could be good. Yeah, or the people who signed up in 2018,
[01:02:42.880 --> 01:02:47.520]   and they didn't have a lot of these other things. There was no Twitch social. I actually also
[01:02:47.520 --> 01:02:56.560]   was also on a smaller one, a XOXO.Zone for my favorite my very favorite conference. Yeah,
[01:02:56.560 --> 01:03:02.320]   so yeah. And I did migrate, but I never used it. So I did go ahead and migrate the followers that
[01:03:02.320 --> 01:03:07.920]   I had a mask there over to. And there was some some overlap, I'm sure, but I did migrate those
[01:03:07.920 --> 01:03:15.520]   followers over to the the main account that I'm on. But and that was actually seamless. I was
[01:03:15.520 --> 01:03:20.240]   worried about what that process was going to be like, but it wasn't difficult. So that's good news.
[01:03:20.240 --> 01:03:26.480]   I've done the same thing. I was unmasked on that social way back when when it was the only mess
[01:03:26.480 --> 01:03:31.840]   not instance. And when I started my own, I migrated over to Twitter social. I also have something
[01:03:31.840 --> 01:03:38.240]   on pixel fed, which is a Fediverse, not Mastodon, but kind of Instagram clone. And I really like
[01:03:38.240 --> 01:03:44.480]   it on pixel fed social. I really like it because it's Instagram like it used to be with just a
[01:03:44.480 --> 01:03:53.520]   bunch of photos. Golly, whoever thought of that, what no reels, no dancing chipmunks? What what
[01:03:53.520 --> 01:04:00.640]   kind of what kind of places that? So and one of the nice things about ivory and these other clients
[01:04:00.640 --> 01:04:09.040]   is you can you can actually have multiple accounts in your clients. So you can have your photos on
[01:04:09.040 --> 01:04:18.000]   pixel fed and your toots somewhere else. Our show today brought to you by our good friends
[01:04:18.000 --> 01:04:22.960]   at worldwide technology. Worldwide technology is at the forefront of innovation. We love these
[01:04:22.960 --> 01:04:27.120]   guys. We had it was actually the last trip, Lisa and I took before COVID went out there in March
[01:04:27.120 --> 01:04:33.600]   of 2020 to visit their advanced technology center. Wow, is that cool? You guys probably remember in
[01:04:33.600 --> 01:04:39.840]   the old days, if Davis had that big testing lab in Foster City. And that's why PC magazine could
[01:04:39.840 --> 01:04:44.880]   test 100 printers because they had the capability of doing that. You remember that? Well, that's
[01:04:44.880 --> 01:04:50.960]   what the ATC is all about for enterprise technology. That's that's worldwide technologies business
[01:04:50.960 --> 01:04:58.800]   is enterprise technology. They created the advanced technology center to try to research all of this
[01:04:58.800 --> 01:05:05.040]   great technology out there for enterprise. It now has all the technologies from all the leading OEMs
[01:05:05.040 --> 01:05:10.800]   including some of the big new disruptors, more than half a billion dollars in equipment. It started
[01:05:10.800 --> 01:05:15.840]   one rack and one building it spread to four or five buildings now many, many racks. But here's the
[01:05:15.840 --> 01:05:20.160]   great part and I really honor WWT for this. They don't keep it to themselves. Sure, their engineers
[01:05:20.160 --> 01:05:24.880]   use it to spin up proofs of concept and learn about new technologies and so forth to help their
[01:05:24.880 --> 01:05:31.200]   clients. But they also make it available to you. The advanced technology center, you don't have to
[01:05:31.200 --> 01:05:34.560]   go to St. Louis. You can use it anywhere in the world. They offer hundreds of on-demand and
[01:05:34.560 --> 01:05:40.800]   schedulable labs featuring solutions that include technologies representing all the latest,
[01:05:40.800 --> 01:05:46.720]   the newest advances in cloud and security, networking, primary and secondary storage,
[01:05:47.520 --> 01:05:54.560]   data analytics and AI, DevOps and on and on and on. It is not just for those great
[01:05:54.560 --> 01:05:59.760]   WWT engineers and partners. It's for anybody. It's free to anybody who wants to use the ATC platform
[01:05:59.760 --> 01:06:05.840]   which means your evaluation time can go from months to weeks. Your knowledge level can go
[01:06:05.840 --> 01:06:10.240]   through the roof. You could test out products and solutions before you go to market. But it's more
[01:06:10.240 --> 01:06:15.920]   than just the labs. You can access technical articles, expert insights, demonstration videos,
[01:06:15.920 --> 01:06:20.480]   white papers, all the tools you need to stay up on the latest enterprise technology.
[01:06:20.480 --> 01:06:24.240]   They also have a great community. In fact, when you go to the ATC platform,
[01:06:24.240 --> 01:06:28.880]   check out WWT's events and communities. Learn about technology trends here about the latest
[01:06:28.880 --> 01:06:34.480]   research and insights from experts. Not only is the ATC at physical labs based in St. Louis,
[01:06:34.480 --> 01:06:38.880]   and if you get a chance to see it do, it was the most amazing thing, but it's completely virtual
[01:06:38.880 --> 01:06:45.200]   so you can use it. If you're on the ATC platform anytime, anywhere in the world, 365 days a year.
[01:06:45.840 --> 01:06:52.000]   Whatever your business needs, this is the point. WWT is the best partner for anybody using enterprise
[01:06:52.000 --> 01:06:57.600]   technology. Worldwide technology can deliver scalable, tried, and tested tailored solutions.
[01:06:57.600 --> 01:07:05.680]   Because WWT understands it's in business, technology is not for technology's sake. It's
[01:07:05.680 --> 01:07:11.520]   there to support your business strategy. WWT brings strategy and execution together to make
[01:07:11.520 --> 01:07:16.240]   this exciting new world happen. Learn more about WWT, the Advanced Technology Center,
[01:07:16.240 --> 01:07:23.280]   to get access to all these free resources. Very easy. Go to www.com/twit.
[01:07:23.280 --> 01:07:31.200]   www.wt.com/twit. Create a free account on the ATC platform and learn and explore and grow
[01:07:31.200 --> 01:07:37.440]   and use these technologies the way they're intended. WWT.com/twit. These guys are the good guys. These
[01:07:37.440 --> 01:07:42.800]   are the guys you need as a partner. www.wt.com/twit.
[01:07:42.800 --> 01:07:52.720]   Let's see. Oh, I do want to do a quick plug for our, I think it's the last chance to take the
[01:07:52.720 --> 01:07:59.440]   survey. Yeah, we have only two days left. Twit.tv/survey23. We survey our audience once a year. We don't
[01:07:59.440 --> 01:08:05.200]   want to spy on you. We can't put trackers in a podcast, it's RSS. But we'd like to know more
[01:08:05.200 --> 01:08:10.960]   about you. Our advertisers would like to know who those ads are going to. We can't compete with
[01:08:10.960 --> 01:08:14.800]   people like Spotify, we'll spy on your every move and know who you are and all that stuff.
[01:08:14.800 --> 01:08:19.680]   It's the survey. It's our only tool, but it helps us a lot. So it should only take a few minutes.
[01:08:19.680 --> 01:08:22.400]   It's completely optional, of course, to answer any questions you want.
[01:08:22.400 --> 01:08:29.840]   www.twit.tv/survey23. I want to get people from every show participating, though, so we know
[01:08:30.480 --> 01:08:37.280]   about what we're doing and whether it fits your needs. Twit.tv/survey23. Last chance,
[01:08:37.280 --> 01:08:47.120]   don't put it off. And we thank you in advance. Some really interesting news from the Department
[01:08:47.120 --> 01:08:55.040]   of Justice. There was a ransomware gang called Hive. Ransomware has become a plague, obviously.
[01:08:55.040 --> 01:09:00.400]   It's really a problem, although I saw that the revenues, and they know this because they can look
[01:09:00.400 --> 01:09:07.280]   at Bitcoin transfers, were significantly down in 2022. And the thinking is because people aren't
[01:09:07.280 --> 01:09:12.560]   paying. It's not that Ransomware is not hitting you. It's just people who said, "Sprove that,
[01:09:12.560 --> 01:09:17.360]   we're not giving you any money." Maybe they've got better strategies for mitigating a ransomware
[01:09:17.360 --> 01:09:24.080]   attack. But also, the DOJ is going after them. This was a press conference from Deputy Attorney
[01:09:24.080 --> 01:09:32.960]   General Lisa Omonico. It turns out, I think this is fascinating, that the US had infiltrated,
[01:09:32.960 --> 01:09:40.480]   the FBI had infiltrated the Hive Ransomware group last July. And as a result,
[01:09:40.480 --> 01:09:49.520]   maybe this is why Ransomware is going down, too, under the covers, that's not quite right.
[01:09:50.320 --> 01:09:56.240]   Officers were able to warn victims of impending attacks in secret. They're saying, "Hey, watch out,
[01:09:56.240 --> 01:10:02.080]   they're going after you." They also got decryption keys. And they were able to hand out more than
[01:10:02.080 --> 01:10:09.360]   300 decryption keys to people who had been hit by the Hive Ransomware, saving them more than $130
[01:10:09.360 --> 01:10:17.760]   million. The US estimates Hive and its affiliates, it's one of those ransomware as a service
[01:10:18.880 --> 01:10:23.520]   companies. I don't want to use word company, but that's kind of what it is. Collected over
[01:10:23.520 --> 01:10:28.000]   $100 million from more than 1,500 victims, they went after, and this was their mistake,
[01:10:28.000 --> 01:10:33.520]   hospitals, school districts, critical infrastructure. In more than 80 countries around the world,
[01:10:33.520 --> 01:10:38.960]   one hospital was left unable to accept new patients because of Hive. They worked with the UK's
[01:10:38.960 --> 01:10:45.120]   National Crime Agency and other law enforcement agencies around the world to help victims.
[01:10:46.320 --> 01:10:53.520]   In the UK, 50 organizations were given decryption keys. And on Thursday, the FBI shut it down.
[01:10:53.520 --> 01:10:58.160]   They took Hive's website and communications networks down with the help of police forces
[01:10:58.160 --> 01:11:05.120]   in Germany and the Netherlands. That is a successful attack on the attackers.
[01:11:07.840 --> 01:11:18.560]   I don't know if they arrested anybody. I don't see that. And that's the problem because as
[01:11:18.560 --> 01:11:24.080]   the head of intelligence at Mandiant, John Hultquist said, until you arrest them, they're not going
[01:11:24.080 --> 01:11:31.360]   to be gone. It's like cockroaches. They just move somewhere else. It'll slow them down. If
[01:11:31.360 --> 01:11:36.240]   you went to the Hive Crew's website, you would see this notice from the FBI.
[01:11:37.200 --> 01:11:42.960]   This hidden site has been seized with lots of badges.
[01:11:42.960 --> 01:11:50.960]   This is not how it was not the biggest of the ransomware gangs. There are bigger ones, although
[01:11:50.960 --> 01:11:57.360]   Rievel, which was perhaps the biggest in 2020 and 2021, did get arrested around the world.
[01:11:57.360 --> 01:12:05.840]   So this is good. Dark Side was taken down in June of 2021. This is good. This is what it takes.
[01:12:06.720 --> 01:12:11.680]   Let's see. What else? Intel. Do you want to talk about Intel?
[01:12:11.680 --> 01:12:20.320]   Not a good quarter for Intel. The worst beating in over a decade.
[01:12:20.320 --> 01:12:26.560]   And/or rights from Apple and Sider. They're maybe a little happier than they ought to be about this.
[01:12:26.560 --> 01:12:34.480]   32% drop in revenue since year over year since the holiday quarter of last year, 2021, actually.
[01:12:35.280 --> 01:12:41.760]   Fourth quarter results coming out. Revenue 14 billion down 30% year over year.
[01:12:41.760 --> 01:12:51.440]   Entire year revenue down 20% year over year. This goes along with drops of 30, 40% in PC sales as
[01:12:51.440 --> 01:12:55.920]   well. So it's just been a bad year for PCs. Does that mean anything, Harry?
[01:12:55.920 --> 01:13:02.640]   Well, I think Intel has known and acknowledged for a while now that it's in this rebuilding process
[01:13:02.640 --> 01:13:08.720]   after falling way behind other chip companies and that it was not going to result in fantastic
[01:13:08.720 --> 01:13:15.200]   numbers immediately because they have to get back to where their process is competitive again
[01:13:15.200 --> 01:13:20.400]   with other technologies. And I believe they've said that maybe by year after next,
[01:13:20.400 --> 01:13:27.920]   they think they'll be in a place where the technology is great again, which is maybe as long as they
[01:13:27.920 --> 01:13:34.400]   give Pat Gelsinger their CEO time to get there. Maybe that's when we can really judge them.
[01:13:34.400 --> 01:13:40.400]   And if the numbers are still this bad, then it's a really bad sign. But I think that at least
[01:13:40.400 --> 01:13:46.880]   as of when Gelsinger started and I wrote a feature about him last year, the board had given them
[01:13:46.880 --> 01:13:50.400]   quite a bit of runway understanding that there was going to be difficult and there would be
[01:13:50.400 --> 01:13:54.000]   more bad news before there was any good news. Although they may not have anticipated the
[01:13:54.000 --> 01:14:02.000]   degree to which the PC business would be so crummy. And I think people and companies may just be
[01:14:02.000 --> 01:14:06.880]   postponing PC purchases because everybody's so cautious about the economy this year.
[01:14:06.880 --> 01:14:12.320]   Yeah, everything's down. It's not just PCs. Plus, we bought a lot of PCs during COVID.
[01:14:12.320 --> 01:14:16.800]   Right. People have relatively new nice computers now in a way they didn't before the pandemic.
[01:14:19.200 --> 01:14:26.880]   Right. And if you look at the increase in ships between 2020 and now, not to say that some of the
[01:14:26.880 --> 01:14:30.640]   gains haven't been impressive, but if you're not an enthusiast, you're not actually going to really
[01:14:30.640 --> 01:14:37.120]   notice, I think, for a lot of people. And it's increasingly looking more and more like what was
[01:14:37.120 --> 01:14:44.720]   happening. All that excess buying in 2020 and even a little bit into 2021 was a combination of both
[01:14:45.600 --> 01:14:52.960]   the supply chain, maybe even making more of a frenzy because people couldn't get things,
[01:14:52.960 --> 01:14:57.280]   people having to work from home. It's an anomaly. And I think to then, as a lot of businesses did,
[01:14:57.280 --> 01:15:02.880]   try to base it up as like, well, this is the new baseline was clearly a mistake because that's,
[01:15:02.880 --> 01:15:10.560]   you know, has not continued. And I think that with the, I guess, being able to kind of look back,
[01:15:10.560 --> 01:15:16.080]   we can say, no, why would we have expected those trends to continue, you know, year over year,
[01:15:16.080 --> 01:15:21.200]   because that's just not not consumer buying patterns in the last decade or so, you know,
[01:15:21.200 --> 01:15:26.800]   we haven't seen that. So yeah. We have been saying for a while the end of desktop computing.
[01:15:26.800 --> 01:15:32.240]   But I think it's, what do you think, Tim, is the end of desktop computing exaggerated?
[01:15:32.240 --> 01:15:38.720]   I definitely think it is. I mean, I think we've got a long time to go before that. And certainly,
[01:15:38.720 --> 01:15:42.880]   people's usage patterns show that they're shifting away from desktop computing. If you look at overall
[01:15:42.880 --> 01:15:47.600]   utilization, you know, what devices they're consuming content on and even creating content
[01:15:47.600 --> 01:15:52.720]   on. But if you look at overall time, I think that number is going up and desktop usage,
[01:15:52.720 --> 01:15:56.560]   probably staying pretty much static for the past few years. So yeah, I think we still have a long
[01:15:56.560 --> 01:16:00.960]   way to go there. But if you also look at the number of layoffs, we've seen lately, I mean,
[01:16:00.960 --> 01:16:05.600]   that's a lot fewer corporate laptops that are being needed and certainly with nobody hiring,
[01:16:05.600 --> 01:16:09.120]   that means that there are fewer laptops being needed there too. And if you do get hired now,
[01:16:09.120 --> 01:16:11.040]   I think there's probably a pretty good chance you're getting a hand me down.
[01:16:11.040 --> 01:16:16.560]   You're trying to get Joe's laptop. We fired him last night. Yeah. Sorry, Joe.
[01:16:16.560 --> 01:16:26.160]   At 12,000 layoffs at Google, I mean, it's just been tough. We had on Wednesday on Twig,
[01:16:26.160 --> 01:16:30.560]   we had just completely a representative because I think what, you know, we talked about these
[01:16:30.560 --> 01:16:38.720]   layoffs and I think tech industry since the beginning of the year 200,000 jobs lost. We talk about that
[01:16:38.720 --> 01:16:44.080]   and just kind of abstract numbers. I wanted to bring a face to it. So we had Richard Hayon. He
[01:16:44.080 --> 01:16:49.600]   was a Google engineer, had been an engineer for 17 years at Google and was one of the people just
[01:16:49.600 --> 01:16:55.360]   summarily dismissed, kind of abruptly lost his job without any warning. His boss didn't even know
[01:16:55.360 --> 01:16:58.960]   ahead of time. And I wanted just to kind of bring home the face of it because these are,
[01:16:58.960 --> 01:17:07.520]   that's 200,000 people with families, with bills, with mortgages, with rent, and they don't know
[01:17:07.520 --> 01:17:14.480]   what tomorrow is going to bring. That's a huge hit. And I don't want to diminish it in any way by
[01:17:14.480 --> 01:17:20.560]   just talking about numbers. Yeah. It really is a shame how that has to happen these days.
[01:17:20.560 --> 01:17:25.680]   Like the corporatification of layoffs is really tragic and nauseating, honestly,
[01:17:25.680 --> 01:17:32.880]   having recently been through that myself. How depersonalized it has been mandated that you
[01:17:32.880 --> 01:17:40.000]   cannot have any empathy. You cannot talk to anybody about the situation. You are very restricted in
[01:17:40.000 --> 01:17:44.960]   what you can say when you can say it. And you know, as someone who was trying to be an empathic
[01:17:44.960 --> 01:17:50.000]   leader, someone who, you know, treated his employees like his friends, you have to go through that is
[01:17:50.000 --> 01:17:56.480]   really difficult on the side of the equation. So yeah, I don't know where this pattern came from or
[01:17:56.480 --> 01:18:03.200]   why it is almost legislated into corporate law these days. But it is really disgusting that that
[01:18:03.200 --> 01:18:09.600]   is where we've gotten to a point now where your ability to be an empathic leader has to end at
[01:18:09.600 --> 01:18:15.520]   the time when it's most important for you to be an empathic leader. Yes. I worry that Elon Musk
[01:18:15.520 --> 01:18:21.280]   sent the bar so low for being empathetic or just decent to the people who worked for you that
[01:18:21.280 --> 01:18:26.960]   if these large companies beat Elon, they figure that it's okay. But I mean, there were stories
[01:18:26.960 --> 01:18:32.000]   about Google employees who came into work and waved their badge to get in and either
[01:18:32.000 --> 01:18:36.000]   turned green and they were able to go in or it turned red and they knew they had been laid off.
[01:18:36.000 --> 01:18:39.920]   And that's how they got the news. And I don't understand what the excuse is for that.
[01:18:40.880 --> 01:18:47.760]   Yeah, no, I mean, there's there's there's no good way to do layoffs is the reality.
[01:18:47.760 --> 01:18:57.040]   But there are there are ways that you can do it worse, right? And I agree like for all the excuses
[01:18:57.040 --> 01:19:01.920]   that and this has been a thing I think I've noticed because I first was seeing this in media where
[01:19:01.920 --> 01:19:06.960]   people would find out sometimes that they were laid off by losing access and slack and then
[01:19:06.960 --> 01:19:13.040]   people would disappear and it would be like, you know, like the snap and and you're like what happened,
[01:19:13.040 --> 01:19:18.000]   you know, it brought back PTSD one day when people lost access for slack for a completely unrelated
[01:19:18.000 --> 01:19:23.200]   reason and everybody freaked out there like, what does this mean? It just fired. Yeah, right.
[01:19:23.200 --> 01:19:29.200]   And you know, you do this for the automation reasons though we don't want people to have access
[01:19:29.200 --> 01:19:33.680]   to things. It's like, okay, especially for people who you're paying a certain amount of money and
[01:19:33.680 --> 01:19:38.400]   who you have worked for you for a certain amount of time. It's like, have some freaking humanity,
[01:19:38.400 --> 01:19:45.120]   you know, there's a way to do it. There's a way to take access away. It doesn't mean that someone
[01:19:45.120 --> 01:19:50.240]   is entering the office at 7 a.m. hasn't checked their email. Their person or email doesn't know
[01:19:50.240 --> 01:19:58.320]   what's going on. Waves a badge and finds out that way like that that's just it's awful. And
[01:20:00.000 --> 01:20:04.960]   there are better ways to do it. There's no good way to do it, period. But there are ways to do it
[01:20:04.960 --> 01:20:11.360]   that are worse than others. Somebody in the chat room just told me that Chris Tibona, who was one of
[01:20:11.360 --> 01:20:19.520]   the founders of Floss Weekly, great friend is also an ex-googler. He was a director of open source
[01:20:19.520 --> 01:20:25.840]   at Google. I did not realize he had lost his job as well. So I didn't either. That's a massive
[01:20:25.840 --> 01:20:31.280]   loss for them. And that's a massive concern for open source because of all the work and money
[01:20:31.280 --> 01:20:36.160]   and resources that Google has given open source projects over the years, sponsoring
[01:20:36.160 --> 01:20:41.120]   conferences and other things. That's been a discussion that has come up in the last couple
[01:20:41.120 --> 01:20:46.080]   of weeks with these big layoffs is what does that mean for the open source ecosystem. And I don't
[01:20:46.080 --> 01:20:50.880]   think that it's a wrong one because budgets are tight everywhere. And these are things that,
[01:20:50.880 --> 01:20:55.360]   you know, some people in the open source movement don't like to acknowledge. But a lot of the money
[01:20:56.320 --> 01:21:00.400]   and a lot of the funding really does come from these corporations, whether you're comfortable
[01:21:00.400 --> 01:21:07.280]   with that or not. And if those checks go away, like what does that mean? Because the sustainability
[01:21:07.280 --> 01:21:13.440]   and open source has been a really big topic for the last number of years. And corporate
[01:21:13.440 --> 01:21:18.880]   good will is or corporations paying their own way for a service and support is one thing.
[01:21:18.880 --> 01:21:23.760]   But the good will aspect, which has been increasingly a thing that we've seen happen,
[01:21:24.320 --> 01:21:32.000]   like I can see that potentially at some places, you know, going away. And that's really discouraging.
[01:21:32.000 --> 01:21:38.320]   And I think it have really negative consequences because people haven't always wanted to maybe
[01:21:38.320 --> 01:21:44.000]   acknowledge how much of a role those checks and that funding can really play for a lot of small
[01:21:44.000 --> 01:21:50.480]   projects. Microsoft also laying off about 10,000 workers. And this is the thing, you know, you go,
[01:21:50.480 --> 01:21:54.080]   I'm sure you do this to Christina, you go and you look and you just check and see.
[01:21:54.080 --> 01:21:59.520]   Oh gosh. And I'm sure there's corporate, you know, slacks and stuff that you can go to and
[01:21:59.520 --> 01:22:05.840]   see who's there apparently a Twitter there with these salute icons as people dropped off the face
[01:22:05.840 --> 01:22:09.120]   of the earth. Microsoft's quarter. Go ahead.
[01:22:09.120 --> 01:22:15.120]   No, it's going to say I think that would mean it hard for Microsoft. And I'm sure for Google too,
[01:22:15.120 --> 01:22:19.680]   is everybody, you know, a lot of people working from home. And so yeah, there were a lot of,
[01:22:19.680 --> 01:22:23.680]   you know, kind of shadow groups of people. Well, you know, yeah, you don't know,
[01:22:23.680 --> 01:22:26.480]   but people check in with one another. I mean, that's what I was doing. I was checking
[01:22:26.480 --> 01:22:31.600]   out with my friends at Microsoft. I'm in a few group chats and that's what I was doing. And then,
[01:22:31.600 --> 01:22:36.960]   you know, checking Twitter and seeing, you know, and some people were laid off and whatnot. And
[01:22:36.960 --> 01:22:42.480]   when you're talking about numbers this big, it's not about performance. It really is, you know,
[01:22:42.480 --> 01:22:46.800]   decisions made usually about entire divisions. You slash divisions.
[01:22:46.800 --> 01:22:51.280]   There is. You know, one of the stories, and again, I hate to, I'm not going to put you on the spot.
[01:22:51.280 --> 01:22:55.920]   You don't represent Microsoft by any stretch of imagination. But one of the things that we did
[01:22:55.920 --> 01:23:04.480]   learn is that Microsoft's VR AR, HoloLens division suffered massive cuts. And that sounds to me
[01:23:04.480 --> 01:23:12.880]   like more of a strategic decision to not pursue those areas. And instead of making hardware to
[01:23:12.880 --> 01:23:17.840]   make their software available to companies like HTC and Meta that are going to make the hardware
[01:23:17.840 --> 01:23:23.520]   and Apple one imagines that are going to make the hardware and then, you know, make the productivity
[01:23:23.520 --> 01:23:29.840]   software for that hardware, which actually probably is a better bet than putting all your chips in on
[01:23:29.840 --> 01:23:35.760]   on the legless, sexless people wandering around in a low poly count.
[01:23:36.320 --> 01:23:44.400]   Microsoft also to take Corey's pros. Yeah, Microsoft also gave up on all space VR, which was a
[01:23:44.400 --> 01:23:52.240]   startup data query a few years ago. So it was a platform. So I mean, I would be cautious about
[01:23:52.240 --> 01:23:59.200]   assuming that Microsoft doesn't have any AR VR slash metaverse platform ambitions forever.
[01:23:59.200 --> 01:24:04.720]   But if they do, maybe this seems like a little bit of a reset and it seems perfectly sensible at
[01:24:04.720 --> 01:24:11.280]   this point to redeploy some of that metal band, wealth and resources into AI, which so clearly
[01:24:11.280 --> 01:24:15.760]   is going to have so much impact starting at this very moment, as opposed to the metaverse,
[01:24:15.760 --> 01:24:21.840]   which is still a maybe at some point and maybe not to the degree we expected kind of thing.
[01:24:21.840 --> 01:24:26.480]   Is it a risky though to chase the flavor of the month? Because I mean, that's why they chased VR.
[01:24:26.480 --> 01:24:32.640]   True. Although, I mean, I don't know, for all the reasons to be cautious about AI, I think even
[01:24:32.640 --> 01:24:37.840]   if it's only 5% as impactful as people expect that it's going to be incredibly important.
[01:24:37.840 --> 01:24:44.960]   There were a lot of reasons to think VR was not going anywhere from day one. I mean, that 11%
[01:24:44.960 --> 01:24:50.080]   of the people who use it were nauseated is a pretty good indicator that there's going to be
[01:24:50.080 --> 01:24:54.800]   this may not be the mass appeal product. You hope it will be. There's some if you want to have
[01:24:54.800 --> 01:24:58.480]   magical glasses of look like these, but have great battery life and fantastic. Well, that's
[01:24:58.480 --> 01:25:02.960]   Apple's plan, right? Spectacles. Well, they gave up on that too. There's just there's some fundamental
[01:25:02.960 --> 01:25:07.600]   piece of technology. We have no idea how to build so far. Right. We know how to do AI.
[01:25:07.600 --> 01:25:12.480]   I mean, it's a battery life. Yeah, exactly. Chemistry moves at a glacial pace compared to
[01:25:12.480 --> 01:25:16.960]   digital stuff. That was one of the stories from the week. Mark Gurman saying Apple is going to
[01:25:16.960 --> 01:25:24.000]   push off its, you know, spectacle based AR vision for at least a couple of years to 2025,
[01:25:24.560 --> 01:25:30.640]   if not later, because they can't get it working. Even their headset, which they're still rumors
[01:25:30.640 --> 01:25:38.960]   are strongly going to offer for $3,000 this year has a battery in your pocket because it's too heavy
[01:25:38.960 --> 01:25:44.560]   to wear on your head. Right. So yeah, I think there are some fundamental technical issues with.
[01:25:44.560 --> 01:25:50.480]   The problem is we all read the same science fiction stories by William Gibson and Neil Stevens.
[01:25:50.480 --> 01:25:56.960]   We all want this. We all want to jack into the metaverse. We all want this. But it's not sci-fi.
[01:25:56.960 --> 01:26:01.520]   I mean, I mean, the battery, no, the battery thing is one of the biggest ones. I mean, I've been
[01:26:01.520 --> 01:26:06.880]   a proponent of going nuclear for a decade, at least. You want a little nuclear power plant
[01:26:06.880 --> 01:26:12.400]   near your head? I mean, honestly, I would trust it more than lithium ion. If you look at the
[01:26:12.400 --> 01:26:17.440]   safety record, I honestly would. Do we have that technology? I know we have pocket nuclear
[01:26:17.440 --> 01:26:21.120]   reactors for power, but pocket means the size of this room.
[01:26:21.120 --> 01:26:27.600]   I mean, I don't know if we do or not. My point is more like I wish that we'd have been investing
[01:26:27.600 --> 01:26:32.880]   more over the last decades in looking at that as a power source than some of these other things,
[01:26:32.880 --> 01:26:38.720]   because I do think that in my mind, that's the only way you can get the long lasting battery life
[01:26:38.720 --> 01:26:43.760]   and the the micro-ization that you'll need for these things. But I just don't think it's going
[01:26:43.760 --> 01:26:48.320]   to be possible with with with the empolymers. I just don't. I'm not sure that
[01:26:48.320 --> 01:26:53.280]   physics and chemistry that needs. I understand. I'm just not sure people are
[01:26:53.280 --> 01:27:00.320]   anxious to wear a nuclear power plant hat. You're not wrong, but but again, I mean,
[01:27:00.320 --> 01:27:05.280]   maybe you need to rebranding. I'm just saying like the new clear hat.
[01:27:05.280 --> 01:27:09.600]   Okay, that's a good. I'm just saying like it's a branding thing, but like,
[01:27:10.800 --> 01:27:16.640]   I think that the technology, like that's obviously the one of the only solutions that I can think
[01:27:16.640 --> 01:27:21.120]   of that we already have, because solar is certainly not going to be fast enough or powerful enough
[01:27:21.120 --> 01:27:26.080]   to do that sort of thing. Our listeners in Australia are probably aware of the fact that
[01:27:26.080 --> 01:27:36.800]   a tiny CZM 137 capsule went missing on its way to purse this past week. Just one.
[01:27:38.080 --> 01:27:42.880]   Let me see if I find a picture of it because it's it's so small. They're warning the public not to
[01:27:42.880 --> 01:27:48.560]   touch it, but it's so small. I don't even know how you would find it. It's about the size of one of
[01:27:48.560 --> 01:27:57.200]   those little lithium ion batteries that you put in your in your pocket. Here it is. Here's the size
[01:27:57.200 --> 01:28:05.360]   next to a Australian something 10 10 pence piece. I don't know what that is. 6 millimeters by 8
[01:28:05.360 --> 01:28:11.680]   millimeters. If you see it, don't touch it. Don't pick it up. Yeah, it's like,
[01:28:11.680 --> 01:28:17.280]   you guys are too young to remember, when I was a kid, there are constant ads not to touch blasting
[01:28:17.280 --> 01:28:23.840]   caps. Do you remember that? Do you remember that, John? Blasting caps. That was like, maybe that
[01:28:23.840 --> 01:28:29.680]   was a major problem. I know kids, if you see this, don't touch it. Well, kids, if you see a 6 millimeter
[01:28:29.680 --> 01:28:37.520]   by 8 millimeter shiny silver capsule, don't touch it. It could kill you. It could kill you.
[01:28:37.520 --> 01:28:41.840]   So do you want to wear that on your head? I don't know.
[01:28:41.840 --> 01:28:46.240]   Christina's point though. We haven't seen any real progress in alternate sources for power in a
[01:28:46.240 --> 01:28:53.360]   while. I remember to see us about a decade ago, there was like a hall full of portable devices for
[01:28:53.360 --> 01:28:57.520]   hydrogen power, basically. So you could have a fuel cell in your laptop or on your phone, even.
[01:28:57.520 --> 01:29:01.040]   And there were so many different vendors, it seemed like it was just a couple of years away.
[01:29:01.040 --> 01:29:05.200]   I'm guessing they started popping, exploding in people's pants, and that was probably the end of
[01:29:05.200 --> 01:29:10.720]   that. But we haven't really seen anything since then. So certainly, solid state batteries are just
[01:29:10.720 --> 01:29:14.480]   around the corner. I think we'll see those soon. Those will provide a pretty big step forward in
[01:29:14.480 --> 01:29:20.000]   terms of charging speed, discharge speed, and will help to reduce the overall volume of a given
[01:29:20.000 --> 01:29:25.600]   capacity of battery. But really, like I said, the ochemistry happens slowly, and there really isn't
[01:29:25.600 --> 01:29:30.320]   any kind of shot in the darkness coming soon for portable devices for cars, you know, super
[01:29:30.320 --> 01:29:34.560]   capacitors and things like that. I think we'll have some big gains in a decade or so. But there's
[01:29:34.560 --> 01:29:40.400]   nothing like that coming for smaller stuff. Yeah. Microsoft's recorder was not great. Revenue is
[01:29:40.400 --> 01:29:49.440]   up 2%, profit down 12%. This primarily, I think, due to this PC drop off. Both below Wall Street
[01:29:49.440 --> 01:29:57.040]   expectations. Amy Hood, Microsoft's chief financial officer, said new business slowed in December,
[01:29:57.040 --> 01:30:02.480]   but it expects and it expects growth to continue to slow in the current quarter, which ends March 31st.
[01:30:02.480 --> 01:30:08.640]   On the other hand, I think Microsoft is very well positioned. This open AI investment
[01:30:08.640 --> 01:30:15.840]   is looking very smart right now. Clearly, if AI is taking off businesses like Azure are going to
[01:30:15.840 --> 01:30:22.400]   do very well, nobody wants to invest in the storage and the TPU capacity that's required for learning
[01:30:22.400 --> 01:30:30.800]   big sets of data. So they do it often in the cloud. Microsoft Google Amazon all benefiting for that.
[01:30:30.800 --> 01:30:38.720]   So I think I would be bullish about Microsoft, Christine, I think you're in a, and certainly
[01:30:38.720 --> 01:30:47.840]   about GitHub. GitHub passed 100 million developers this week. Yes. Yes. That was very, very exciting
[01:30:47.840 --> 01:30:55.440]   news. 100 million developers and a couple of years ahead of schedule. So the goal would be in 2025.
[01:30:55.440 --> 01:31:03.840]   We were able to hit it early 2023. So very, very exciting about that. And when you look at the
[01:31:03.840 --> 01:31:10.000]   trajectory of how many developers have joined the platform, even in like going back to 2016,
[01:31:10.000 --> 01:31:16.560]   like it's really ramped up. And I think it's because one of the great things is that the
[01:31:16.560 --> 01:31:22.720]   definition of developer has changed, I think, in a really important way. And so people who
[01:31:22.720 --> 01:31:29.760]   are working on working around code or making contributions that might not be code focused
[01:31:29.760 --> 01:31:37.440]   can still use platforms like GitHub. The rise of the lower code movement around people who are
[01:31:37.440 --> 01:31:42.880]   building business applications and doing other types of things where you see a lot of data scientists
[01:31:42.880 --> 01:31:49.040]   and other people doing really innovative stuff. But again, in their mind, 10 years ago, they
[01:31:49.040 --> 01:31:53.200]   might have said, oh, I'm not a developer. Now you can be like, no, you are. The stuff that you're
[01:31:53.200 --> 01:32:00.160]   doing might not be code in a traditional sense. But it definitely is impacting things or in some
[01:32:00.160 --> 01:32:06.720]   cases is absolutely what's the weirdest thing people are using GitHub for? Maybe that's a
[01:32:06.720 --> 01:32:10.800]   loaded question. I don't know that. No, it's interesting because well, I mean, people so
[01:32:10.800 --> 01:32:16.080]   not all code. I mean, I know novelists and writers use it, right? Yeah, I was going to say,
[01:32:16.080 --> 01:32:19.360]   I was going to say, you know, the grid, this was, we have this product called a GitHub projects,
[01:32:19.360 --> 01:32:24.880]   and which is like a project management stuff. And you will see people who will just use it to
[01:32:24.880 --> 01:32:30.000]   just manage their life, like to just have it as like a very organized kind of to-do list thing.
[01:32:30.000 --> 01:32:35.040]   And that's really cool to see. But as you see, a novelist, people who use it for writing,
[01:32:35.040 --> 01:32:40.000]   I think that is definitely a really cool way. Then we also see, you know, it used in really
[01:32:40.000 --> 01:32:46.400]   interesting ways, you know, like by, you know, people in NASA and in other organizations.
[01:32:46.400 --> 01:32:51.520]   It's interesting to see a lot of the data science stuff is really interesting because you can see
[01:32:51.520 --> 01:32:57.360]   people putting their Jupiter notebooks and their other outputs there. That I think is actually
[01:32:57.360 --> 01:33:04.240]   really great. I think seeing notebooks has been such a great feature to happen, I think, in code
[01:33:04.240 --> 01:33:09.040]   for a lot of reasons. And as we've gotten better support for that stuff within GitHub,
[01:33:09.040 --> 01:33:14.320]   I think that's been a really cool thing to see the data sets and that stuff that people have used.
[01:33:14.320 --> 01:33:18.400]   That's really awesome to me because those are things that wouldn't fit with a lot of traditional
[01:33:18.400 --> 01:33:24.960]   code things, but is a really great way where we had an incident, I think it was last year where we
[01:33:24.960 --> 01:33:33.840]   got rid of one of our original short URL shorteners. And we did it because the code behind it was
[01:33:33.840 --> 01:33:40.720]   really antiquated and it hadn't been up-kept. But we had to wind up migrating a lot of the
[01:33:40.720 --> 01:33:44.960]   URLs over and kind of keep them working because it turned out that there were a number of academic
[01:33:44.960 --> 01:33:49.840]   papers where people had used the URL shortener, which would just go to a GitHub repo
[01:33:49.840 --> 01:33:53.920]   in their academic papers. And that's always really interesting to see how many people
[01:33:53.920 --> 01:33:58.400]   will put the full data sets and other information, academic papers on GitHub repos. That's always
[01:33:58.400 --> 01:34:05.760]   really cool to see. Somebody has got an open AI chat GPT prompt for a link-bait article. Better
[01:34:05.760 --> 01:34:12.080]   GitHub with this one weird trick. I think we should write that right now. Somebody will come up with
[01:34:12.080 --> 01:34:16.640]   that. Totally write that. Somebody I hugely admire one of the most famous programmers in the
[01:34:16.640 --> 01:34:25.120]   world, Peter Norvig. He's a scientist at AI scientist at Google. He uses Jupyter Notebooks on GitHub.
[01:34:25.120 --> 01:34:30.560]   I follow him because I do the advent of code coding problems and he does these every year.
[01:34:30.560 --> 01:34:34.800]   And of course, here's one of the best programmers in the world. This is what a Jupyter Notebook
[01:34:34.800 --> 01:34:41.200]   looks like on GitHub. He's got cartoons. He's got code that runs. He's got results. I mean,
[01:34:41.200 --> 01:34:47.360]   it's amazing. He's even got visualizations in here because Jupyter Notebooks, which is just one
[01:34:47.360 --> 01:34:52.560]   of many kinds of notebooks, but Jupyter is probably the most popular, allow you to run code and write
[01:34:52.560 --> 01:34:57.920]   texts. So you can you could do true literate programming. I think this is fantastic. I am
[01:34:57.920 --> 01:35:03.440]   so impressed. I think to me, this is a great use of GitHub. I mean, this is actual Python code that
[01:35:03.440 --> 01:35:11.920]   runs. Exactly. It's such a great teaching tool. Honestly, it really is, I think, one of the best
[01:35:11.920 --> 01:35:17.440]   ways to teach stuff. And so using that with advent of code, and that's beautiful. That's really cool.
[01:35:17.440 --> 01:35:24.640]   Isn't that great? I love that. It's marvelous to look at his code because it's a clear and it is
[01:35:24.640 --> 01:35:31.920]   a little terse, but it's very clear and precise and inspired. I mean, this is a guy who speaks
[01:35:32.720 --> 01:35:38.560]   code and it's so fun to look at this. I always wait until after I've tried to solve the problem
[01:35:38.560 --> 01:35:44.320]   before I read his posts. He also has got somebody doing cartoons. And all this, this is a GitHub
[01:35:44.320 --> 01:35:53.200]   page. This is a repository, which is pretty darn cool, if you ask me. Anyway, Microsoft Tough
[01:35:53.200 --> 01:36:01.280]   Quarter, but I think the market rewarded missing its targets with a 4% bump in the stock price,
[01:36:01.280 --> 01:36:07.600]   because I think of the future of AI and everybody knew that this PC slowdown was going to hit
[01:36:07.600 --> 01:36:11.680]   Microsoft just as much and if not more, because of course they make the operating system for most
[01:36:11.680 --> 01:36:18.320]   of these computers. So I think you're at a good company. If I were you, Christine, I would keep
[01:36:18.320 --> 01:36:24.800]   that job. Just my advice. Definitely. That's definitely the plan, right? Like, I don't have,
[01:36:24.800 --> 01:36:30.480]   everybody, everybody is, there's uncertainty everywhere, but that is definitely the plan.
[01:36:30.480 --> 01:36:35.760]   I definitely feel very lucky to get GitHub. And yeah. Well, we love you and you could
[01:36:35.760 --> 01:36:39.120]   always come here if you need to, but I don't think I could pay you anything like Microsoft is you.
[01:36:39.120 --> 01:36:47.200]   So I appreciate it. Just so you know, bring your shoes. Come on over. Yeah. I will bring my
[01:36:47.200 --> 01:36:52.400]   shoes. I'll come to Peloma. Okay. What's your new kick kicks? What's your, what's your hot new kick?
[01:36:52.400 --> 01:36:59.200]   Anything exciting? Okay. So I don't, yes, actually, I got, I did not, I don't have them in this room
[01:36:59.200 --> 01:37:03.920]   with me there in the other room, but I went to Vegas last week with my mom. I took her to
[01:37:03.920 --> 01:37:10.480]   see a Dell last weekend. It was amazing. Oh, how fun. Oh, how fun. Yeah. My mom has never been to
[01:37:10.480 --> 01:37:15.520]   Las Vegas and I haven't been for a non-work-related reason in a really long time. It's a very different
[01:37:15.520 --> 01:37:19.840]   experience, isn't it, when you're not going to the convention center every day, all day?
[01:37:19.840 --> 01:37:24.960]   Honestly, it was, it was like a completely different thing for me. We had such a great time,
[01:37:24.960 --> 01:37:29.680]   but we were staying at the, at the, at the plot. So it was just part of the Venetian and they have a,
[01:37:29.680 --> 01:37:35.440]   a big mall. And then there's like the, the win in the Encore next door. And anyway, I went into
[01:37:35.440 --> 01:37:41.600]   Ferragamo and I bought a pair of Ferragamo sneakers. I will put them in the chats. They are great,
[01:37:41.600 --> 01:37:47.760]   but that is my, that is my new. Can I, I shouldn't, this is Gosh of me. How much were they? Can I ask?
[01:37:47.760 --> 01:37:54.080]   Um, 800? Yeah. Well, that's not bad. Everything at Ferragamo starts at 800. So you really,
[01:37:54.080 --> 01:37:57.840]   really got a deal. I think. Yeah, exactly. I, I got the low and here's the thing. There was a
[01:37:57.840 --> 01:38:00.880]   pair of, I like it. They were a little bit more expensive. They were still within my budget,
[01:38:00.880 --> 01:38:08.000]   what I would have spent, but I'm a five and a half, which the, the, the sales person, um, had never
[01:38:08.000 --> 01:38:14.080]   seen someone with feet as small as mine. Tiny little, I have tiny little feet too. Yeah. I don't
[01:38:14.080 --> 01:38:19.840]   know what that, I feel like I'm just going to fall over in a, in a stiff wind. So, uh, yeah. All right.
[01:38:20.960 --> 01:38:24.640]   We're going to take a break. You go get those Ferragamos if you want, because we're going to talk about
[01:38:24.640 --> 01:38:31.760]   our sponsor. Thank you, ACI for sponsoring our studios for the year. Uh, ACI learning,
[01:38:31.760 --> 01:38:37.360]   you say, well, I don't know them, but you do know the name IT pro for the last decade. Our
[01:38:37.360 --> 01:38:43.360]   partners at IT pro have brought you engaging, entertaining IT content to level up your career,
[01:38:43.360 --> 01:38:50.480]   your organization. Uh, in fact, I think, uh, a great many of our listeners are IT pro members.
[01:38:50.480 --> 01:38:57.920]   There are 227,000 people in the IT pro community. That is a great learning community. Many of them
[01:38:57.920 --> 01:39:04.720]   to it listeners. Well, IT pro has partnered now with ACI learning, which really expands
[01:39:04.720 --> 01:39:11.600]   its reach. This is really good news for all of us. Uh, expanded production capabilities.
[01:39:12.400 --> 01:39:19.280]   ACI learning is as expertise and not just IT and of course, IT pro is the best, but they also have
[01:39:19.280 --> 01:39:25.600]   audit pro. So audit readiness is a big part of IT these days that can help you there. They also
[01:39:25.600 --> 01:39:31.440]   have a cybersecurity division, which is the best they have even learning hubs where you can go
[01:39:31.440 --> 01:39:35.840]   and learn in person, which for some people, at least part of the time is a valuable adjunct to
[01:39:35.840 --> 01:39:41.840]   the online learning that IT pro is famous for. One of the most widely recognized beginner certificates,
[01:39:41.840 --> 01:39:47.760]   the cop Tia A plus cert. I know many of you in IT, that's where you started, right? That desktop
[01:39:47.760 --> 01:39:54.400]   support cert cop Tia courses with IT pro from ACI learning make it easy to go from being kind of
[01:39:54.400 --> 01:40:00.800]   a daydreamer about getting that career and IT to actually having a career in IT. Earning those
[01:40:00.800 --> 01:40:07.440]   certs is really the required, the most important thing to do to get into an entry level IT position.
[01:40:07.440 --> 01:40:11.360]   You don't have job experience, right? You don't, you can't say, well, I did this and this,
[01:40:11.360 --> 01:40:15.040]   but if you've got that cert, they know you've got the skills, the qualification,
[01:40:15.040 --> 01:40:20.640]   and it gets you started to move on in your field. And that's what IT pro from ACI learning is so good
[01:40:20.640 --> 01:40:27.920]   at tech is one industry where the opportunities now are outpacing growth, especially in cybersecurity.
[01:40:27.920 --> 01:40:34.480]   There are more than a million open, unfilled jobs in cybersecurity right now. A recent LinkedIn
[01:40:34.480 --> 01:40:41.680]   study predicts IT jobs will be the most in demand roles in 2023. Don't waste time. Get going. This
[01:40:41.680 --> 01:40:48.000]   is a career that will reward you. It's, it's fun. You already interested in technology. About a
[01:40:48.000 --> 01:40:56.240]   one third of the information security jobs require a cybersecurity certification. About 23% of IT jobs
[01:40:56.240 --> 01:41:02.720]   require that, but, but a third require of cybersecurity jobs require a cert. Organizations are obviously
[01:41:02.720 --> 01:41:07.840]   very hungry for cybersecurity talent, but they want to know that you've got what it takes. The average
[01:41:07.840 --> 01:41:16.240]   salary for cybersecurity specialists, $116,000 a year, ACI learning information security analyst
[01:41:16.240 --> 01:41:22.800]   and cybersecurity specialist programs can help you get that money, get certified, get that job.
[01:41:22.800 --> 01:41:29.120]   Last year, the global cybersecurity workforce gap grew bigger, not smaller. It increased by 26.2%
[01:41:29.760 --> 01:41:36.240]   over 2021. There's a job out there waiting for you. ACI learning offers multiple cybersecurity
[01:41:36.240 --> 01:41:41.840]   training programs that can prepare you to enter or advance within this exciting industry. The most
[01:41:41.840 --> 01:41:49.600]   popular cybersecurity certs, not in any particular order, CISSP. I love this is the one I want to do.
[01:41:49.600 --> 01:41:56.480]   EC Council's certified ethical hacker. I just thought that'd be great to have CEH after my name.
[01:41:56.480 --> 01:42:02.320]   Certified network defender. There's a cybersecurity audit school. That's a specialty that is going
[01:42:02.320 --> 01:42:11.520]   to be in huge demand as people need to prove compliance to customers, to higher ups, to regulators.
[01:42:11.520 --> 01:42:16.880]   Very important job. Learn cybersecurity frameworks. They've got great classes in that too.
[01:42:16.880 --> 01:42:22.960]   When, where, and how you learn makes a big difference. ACI learning makes it easy. They offer fully
[01:42:22.960 --> 01:42:28.720]   customizable training. No matter what kind of learning you learn or you are, you know, some people really
[01:42:28.720 --> 01:42:34.000]   want to be in the classroom in person. Some people are really more comfortable remote. Some people
[01:42:34.000 --> 01:42:40.640]   like it live remote. Some people want it on demand. ACI learning has it all. Explore what ACI learning
[01:42:40.640 --> 01:42:46.000]   offers with IT Pro, audit Pro, which includes enterprise solutions, webinars, and their great
[01:42:46.000 --> 01:42:51.760]   podcast, the skeptical auditor podcast. They've got practice labs so you can get hands on
[01:42:51.760 --> 01:42:56.880]   just from your own home in a browser. They've got learning hubs where you can actually go in
[01:42:56.880 --> 01:43:01.760]   and get in person instruction and they've got their partnership program too. This is really an
[01:43:01.760 --> 01:43:09.840]   exciting move for IT Pro. ACI learning. A great partner. I'm very excited. Tech is the one industry
[01:43:09.840 --> 01:43:15.040]   where opportunities outpace growth, especially in cybersecurity. One-third of information security
[01:43:15.040 --> 01:43:21.200]   jobs require that cert. Get the cert, get the job to maintain your competitive edge across IT,
[01:43:21.200 --> 01:43:29.520]   cybersecurity readiness. Visit the website go.acilarning.com/twit. You got that and please use that so they
[01:43:29.520 --> 01:43:38.800]   know you saw it here. That's important to us. Go.acilarning.com/twit. We also have a great offer code.
[01:43:39.360 --> 01:43:45.600]   Thank you ACI learning. Use the offer code TWIT30 for 30% off a standard or premium individual
[01:43:45.600 --> 01:43:54.400]   IT Pro membership. 30% off TWIT30 at go.acilarning.com/twit. IT Pro has been such a great partner for
[01:43:54.400 --> 01:44:00.080]   us since they started back in 2013 and we're thrilled to welcome ACI learning and IT Pro into the family.
[01:44:00.080 --> 01:44:03.840]   Thank you for supporting the studio and supporting what we do. We really appreciate it.
[01:44:04.480 --> 01:44:10.720]   All right. Let's see those kicks, Christina. Christina's new kicks. These are...
[01:44:10.720 --> 01:44:16.880]   Yeah. What's that logo? Is that deferagama? What is that?
[01:44:16.880 --> 01:44:24.240]   I guess so. I'm not even sure. I did like how it looks. And then I really liked the back,
[01:44:24.240 --> 01:44:31.040]   which is like this black and white polka dot thing. And again, I'm not defending.
[01:44:31.040 --> 01:44:34.960]   Are you ever going to wear them or are you just going to put them on the shelf and sell them to
[01:44:34.960 --> 01:44:41.440]   somebody else someday? Oh, no, no, no, no. I wear my shoes. I don't buy them for the resell value.
[01:44:41.440 --> 01:44:46.080]   A, my foot is so small that... No, it's going to buy a five and a half.
[01:44:46.080 --> 01:44:52.240]   No, exactly, right? Like that's that's that's that's it. There's a very small number of people
[01:44:52.240 --> 01:44:57.680]   who we will wear my shoe size. No, I buy them to wear. I have them on the back wall. I know,
[01:44:57.680 --> 01:45:02.400]   I see them and I can see the souls are used. You are not one of those people. Yeah, exactly.
[01:45:02.400 --> 01:45:05.040]   Yeah, yeah. Just puts a shoe on a shelf. No, I mean,
[01:45:05.040 --> 01:45:10.880]   let's it suffer in silence. I have some friends who do that. That's not me. For me, I'm like, no,
[01:45:10.880 --> 01:45:16.160]   shoes are to be worn, fashion is to be worn. Like don't don't hoard it in that way. Because if I
[01:45:16.160 --> 01:45:21.440]   spent money, even if it was $50 on a pair of shoes and then never wore it, like that's,
[01:45:21.440 --> 01:45:30.800]   I don't know, that's a waste. Yeah, I agree. You buy it to enjoy it. Yeah. I I'm wearing a 49ers
[01:45:30.800 --> 01:45:38.640]   jersey, which is now for sale cheap if anybody. No, I'm just kidding. But I bought Lisa's birthday
[01:45:38.640 --> 01:45:46.320]   today in our anniversary. So she got a lovely birthday present. But I bought her a, you know,
[01:45:46.320 --> 01:45:52.320]   our young star quarterback, the rookie, Mr. irrelevant, Brock Purdy. I brought her a
[01:45:52.320 --> 01:45:56.800]   Brock Purdy jersey to where during the big, there's a big game today for those of you.
[01:45:56.800 --> 01:45:59.520]   I learned about that. Yes, you didn't know at first now.
[01:45:59.520 --> 01:46:07.040]   Sportball, what is that? So I bought her a Brock Purdy jersey. But see, I now, Marie,
[01:46:07.040 --> 01:46:11.360]   you tell me if I'm right or wrong on this, I thought, what size should I get? And then I said,
[01:46:11.360 --> 01:46:18.560]   I'm getting the small, right? Because if it's too small, that's fine. If I got large,
[01:46:18.560 --> 01:46:27.440]   no, that would have been bad. So a little husband tip, start with the smallest size,
[01:46:27.440 --> 01:46:32.720]   whether it's a shoe or a shirt, sorry, with the smallest size, you can always return it and get
[01:46:32.720 --> 01:46:39.040]   the next one up, which I'm going to have to do because she's not that small. She is, she's tiny.
[01:46:39.040 --> 01:46:45.200]   I thought it would fit. But I guess women's small is pretty small. It's probably the equivalent of
[01:46:45.200 --> 01:46:52.080]   a five and a half shoe. So I did something really gloomy last night. I watched a movie called Too
[01:46:52.080 --> 01:46:58.400]   Leslie. Anybody see that yet? I haven't yet. It's on a list, though, for sure. No, I haven't.
[01:46:58.400 --> 01:47:02.880]   No, spoilers. Yeah, but everybody started talking about it and then I got all the nominations.
[01:47:02.880 --> 01:47:09.760]   That was a little fun. So this is proof that Twitter, for all its problems, still is very
[01:47:09.760 --> 01:47:16.160]   powerful. Normally, this time, or actually last month in December, you see, especially in Los
[01:47:16.160 --> 01:47:22.560]   Angeles, which is, you know, it's a company town, billboards, ads, and every magazine, TV ads for
[01:47:22.560 --> 01:47:29.040]   your consideration, movies that they want the members of the Academy to vote for, to nominate for
[01:47:29.040 --> 01:47:32.720]   Best to Add Picture, Best Actor, because it makes a big difference in box office, right?
[01:47:32.720 --> 01:47:41.440]   So there was a tiny little movie. It only made $27,000 at the box office called Too Leslie. The
[01:47:41.440 --> 01:47:48.000]   movie company could not possibly afford even one billboard on Sunset Strip for your consideration.
[01:47:48.000 --> 01:47:58.560]   But somehow they got every mainstream A-list actor in the world to tweet something just like this.
[01:47:58.560 --> 01:48:04.080]   This is Edward Norton. I don't post a lot about film or actor performances. Maybe I should more
[01:48:04.080 --> 01:48:09.760]   often. But for those interested in really great acting, I'll share that Andrea Rizboro's portrayal
[01:48:09.760 --> 01:48:16.000]   in Too Leslie just knocked me sideways. It's about the most fully committed, emotionally deep. And
[01:48:16.000 --> 01:48:19.440]   then there's a dot, dot, dot. I don't know. Maybe there's more. Oh, here it is. Physically
[01:48:19.440 --> 01:48:24.160]   harrowing performances I've seen in a while just raw and utterly devoid of performative BS. It's
[01:48:24.160 --> 01:48:28.400]   tough, but really elegant and compassionate film by Michael Morris, where the emotion is really
[01:48:28.400 --> 01:48:33.200]   learned. I happened to catch it. And wow, I was really three tweets staggered by the depth she
[01:48:33.200 --> 01:48:38.720]   reached. Very rare. Check it out. But turns out it wasn't just Edward Norton. It was pretty much
[01:48:38.720 --> 01:48:49.040]   everybody in Hollywood tweeted this. This was a mass Twitter campaign to get this actress who's
[01:48:49.040 --> 01:48:56.160]   frankly not well known. Oscar nomination, plate plan, Chet, Steven Spielberg, Oprah,
[01:48:56.160 --> 01:49:03.360]   happy birthday Oprah, Meryl Streep, Daniel Day Lewis, Martin Scorsese. But Brian Rowe pointed
[01:49:03.360 --> 01:49:08.880]   this out on Twitter. All use this exact phrase, the greatest screen performance in the history of
[01:49:08.880 --> 01:49:15.280]   the cinematic medium. Hmm. Maria working PR. Do you think that was a coincidence?
[01:49:15.280 --> 01:49:22.000]   And was that the greatest performance in the history of the cinematic? It was a
[01:49:22.000 --> 01:49:29.680]   a okay. There is no lie there. Oh, you watched it to it. It was a really good performance. It was
[01:49:29.680 --> 01:49:38.080]   amazing. She got a nomination for best actors beating out some people who everybody thought were
[01:49:38.080 --> 01:49:46.320]   shoeing, including Viola Davis for Wakanda. And I'm sorry, the queen, right? What was the name of it?
[01:49:46.320 --> 01:49:54.080]   Woman King, Queen King, woman King. Apparently great. I did not see it. And then there was Till
[01:49:54.080 --> 01:50:00.640]   and the actors in Till who everybody thought both both actresses snubbed by the Golden Globes
[01:50:00.640 --> 01:50:08.160]   and now snubbed by the Academy. But this very little known actress with a film that made $27,000
[01:50:08.160 --> 01:50:12.880]   got all of this attention and got a nomination.
[01:50:12.880 --> 01:50:21.040]   That's the power of Twitter, right? You didn't need a billboard on Twitter and
[01:50:21.040 --> 01:50:26.080]   go ahead. Well, it was Twitter and then was also didn't like an Edward and some other
[01:50:26.080 --> 01:50:30.320]   celebrities didn't have like screenings for cutting over. So Jennifer Aniston says,
[01:50:30.320 --> 01:50:35.840]   come over to my house and we can watch this fine movie to Leslie. Who's going to turn that down,
[01:50:35.840 --> 01:50:42.160]   right? Distributor momentum pictures did not have any money to mount a campaign.
[01:50:42.160 --> 01:50:49.040]   Riceboro was not nominated in the Golden Globes or the SAG Awards. It's basically a word of mouth
[01:50:49.040 --> 01:50:56.160]   campaign kicked off kicked off two days before Oscar voting began to it was a late entry.
[01:50:57.280 --> 01:51:03.520]   Very late campaign. That's incredible. Like even though obviously it was like, you know, getting the
[01:51:03.520 --> 01:51:08.640]   voters to see it, but the Twitter thing, you're exactly right. Like I am and I followed this
[01:51:08.640 --> 01:51:13.520]   stuff for a long time. They didn't have money for a campaign. So this is a really interesting,
[01:51:13.520 --> 01:51:19.440]   I think, example of the right connected people stepping up and using social platforms to,
[01:51:19.440 --> 01:51:24.880]   you know, highlight something that otherwise would not have been getting the sort of attention,
[01:51:24.880 --> 01:51:30.160]   whether or not, you know, she's going to win or not, it remains to be seen. But that's pretty,
[01:51:30.160 --> 01:51:34.320]   that's pretty fantastic. Here's a, here's a tweet by Crazy Cons. He tweets,
[01:51:34.320 --> 01:51:41.200]   something weird is happening. Here's Mia Farrow. Here's Meredith Vieira. Here's Joe Mantegna. And
[01:51:41.200 --> 01:51:45.760]   by the way, all of them say a small film with a giant heart, a small film with a giant heart,
[01:51:45.760 --> 01:51:50.960]   a small film with a giant heart to lay hill, a small film with a giant heart. I do want to
[01:51:50.960 --> 01:51:55.360]   cring gradually Mark Marin, who is billed as an executive producer, probably because they couldn't
[01:51:55.360 --> 01:52:03.840]   pay him for it. But he's a well-known podcaster, does the WTF podcast famous comedian. I feel like
[01:52:03.840 --> 01:52:09.120]   he's one of our own. He has a very large role. And he's quite good. Didn't you think Mark Marin
[01:52:09.120 --> 01:52:12.880]   was good at it? And do you know, even know who he is? He was the guy with the beard who did the,
[01:52:12.880 --> 01:52:20.400]   and then the other guy who was in it is Bubbles from the wire. And I'm watching this guy,
[01:52:20.400 --> 01:52:25.440]   I'm saying, I know this character. Who is this actor? Remember Bubbles in the wire? He was the
[01:52:25.440 --> 01:52:30.800]   kind of strung out junkie and former that was actually, you couldn't take your eyes off him when
[01:52:30.800 --> 01:52:36.720]   he was on the screen. He's in it as well. What was your give it a out of five stars? How many?
[01:52:36.720 --> 01:52:44.880]   Three and a half. I give it more than that, but it's very grim.
[01:52:46.880 --> 01:52:50.000]   It's dark. And then it has a, well, I don't want to spoil it for you.
[01:52:50.000 --> 01:52:55.520]   But you guarantee, I guarantee you that this suddenly is going to make millions of dollars,
[01:52:55.520 --> 01:53:00.240]   right? In streaming, you can stream it on all the major streamers. And also open up questions.
[01:53:00.240 --> 01:53:06.080]   Sorry, go ahead, Christina. No, I was just going to say, I'm definitely going to be streaming it.
[01:53:06.080 --> 01:53:08.160]   I meant to watch it this weekend, and I didn't have a chance.
[01:53:08.160 --> 01:53:11.520]   I made a point of watching last night, so I'd be ready for today.
[01:53:14.240 --> 01:53:19.280]   It's good. I'm glad I watched it. I mean, it's no Wakanda forever, but you know, it's okay.
[01:53:19.280 --> 01:53:24.160]   And it's raising questions about the ethics of these campaigns. And there are rules about
[01:53:24.160 --> 01:53:29.920]   what you can and can't do with these campaigns, but they may not have anticipated the Twitter era.
[01:53:29.920 --> 01:53:36.160]   And this campaign, which if I was based primarily on Twitter, didn't cost anything, but was apparently
[01:53:36.160 --> 01:53:43.200]   extremely effective. And it shows you the power of, well, coming over to Cape Plant Chet's house is
[01:53:43.200 --> 01:53:50.400]   one. That's the big thing, right? Yeah. But honestly, that's the big thing.
[01:53:50.400 --> 01:53:56.240]   The last one I can remember that I guess was similar to this was the campaign for Frozen River
[01:53:56.240 --> 01:54:01.680]   with Melissa Leo. And that was nominated for Best Picture. And that was a very small film.
[01:54:01.680 --> 01:54:07.520]   And Melissa Leo was nominated for Best Actress. She won the following year for Best Supporting
[01:54:07.520 --> 01:54:15.120]   Actress for the fighter. And I don't think she would have won had she not been in Frozen River
[01:54:15.120 --> 01:54:19.600]   the year before, even though the fighter had a very large campaign behind it. I think that Melissa
[01:54:19.600 --> 01:54:25.920]   Leo won because of the Frozen River campaign the year earlier. But no, but it's interesting
[01:54:25.920 --> 01:54:30.720]   to see in your right, Harry, like there are ethical things, but at the same time, yeah,
[01:54:30.720 --> 01:54:34.240]   they're all used in the same language because some PR person send it to you.
[01:54:34.240 --> 01:54:39.760]   There was somehow or a clay plant show wrote to everybody saying, here's a suggested tweet.
[01:54:39.760 --> 01:54:46.800]   And then everybody just coffee pasted and just the same as you do the Instagram influencers,
[01:54:46.800 --> 01:54:51.280]   like the Kardashians oftentimes we just coffee the entire prompt, including the stuff they weren't
[01:54:51.280 --> 01:54:58.640]   supposed to copy and they post it on their accounts. But I mean, I don't think this breaks any of
[01:54:58.640 --> 01:55:04.000]   the rules. I mean, I think that you having a very famous and influential person decides to have
[01:55:04.000 --> 01:55:08.960]   other people voters over at their home to watch something. I don't think that breaks any rules.
[01:55:08.960 --> 01:55:15.760]   Maybe it should, but I don't think it does, you know, or maybe just like everybody knows Andrea
[01:55:15.760 --> 01:55:21.600]   Rizebrow and thinks she's really wonderful. Now I'm learning she's English, which does impress me
[01:55:21.600 --> 01:55:26.480]   more because she didn't play an English character in there. She plays a Southern character.
[01:55:28.720 --> 01:55:35.280]   She that's hard. Yeah, that's hard. She hasn't done a lot. She was in some movies I've heard of,
[01:55:35.280 --> 01:55:43.600]   but never saw like nocturnal animals and the death of Stalin. And I don't know. It's a it's an
[01:55:43.600 --> 01:55:48.000]   interesting thing. What were you going to say, Harry? I think I said it. Oh, you said it already.
[01:55:48.000 --> 01:55:56.240]   Okay. By the way, it's the character who played Bubbles in The Wire is Andre Royo. I want to give
[01:55:56.240 --> 01:56:04.480]   him credit. I have not seen him ever since, but he was he plays Royal in the movie. It's worth
[01:56:04.480 --> 01:56:12.640]   seeing that for Mark Merrin and Andrea Royo. You know, it's nothing else. And yes, Andrea
[01:56:12.640 --> 01:56:19.200]   Rizebrow is good. I was as I'm watching it. I don't know. I don't want to spoil it. I'm thinking,
[01:56:19.200 --> 01:56:23.360]   don't do that. Don't do it. I know they're going to do it, but I don't want them to do it. And they
[01:56:23.360 --> 01:56:29.680]   did it. That's all I'm going to say. I don't know. That's not that's not a spoiler.
[01:56:29.680 --> 01:56:33.680]   Hey, by the way, there are so couple of cool things we didn't mention with GitHub. I just want
[01:56:33.680 --> 01:56:40.080]   to mention there's now a a co-pilot paintbrush, right? That you paint your code. And now you can
[01:56:40.080 --> 01:56:47.600]   say, Hey, GitHub. Yeah. That's wild. So you can, which is fantastic. So you can say,
[01:56:47.600 --> 01:56:52.240]   if you've got carpal tunnel or something, you could just say, Hey, GitHub, write this log in code for
[01:56:52.240 --> 01:56:58.480]   me. I'm too tired. Oh, it wants me to log in. Okay. That's pretty cool. Hey, GitHub.
[01:56:58.480 --> 01:57:08.400]   Yeah, it's very cool. import pandas import graph plotting library. Hey, GitHub, insert new line,
[01:57:08.400 --> 01:57:14.560]   get Titanic CSV data from the web and assign it to the variable Titanic data.
[01:57:14.560 --> 01:57:15.440]   Holy cow.
[01:57:15.440 --> 01:57:21.920]   From Titanic data, where ages null, fill null values of column fair with average column values.
[01:57:22.800 --> 01:57:27.360]   Drop duplicates from the frame Titanic data. Hey, GitHub, new line.
[01:57:27.360 --> 01:57:35.040]   Plot line graph of age versus fair column. Change to scatterplot. Show plot.
[01:57:35.040 --> 01:57:40.800]   Hey, GitHub, exit code mode. Hey, GitHub, run program.
[01:57:40.800 --> 01:57:47.440]   Oh my God. That's pretty impressive that demo right there. It's writing Python code. No typing
[01:57:47.440 --> 01:57:54.080]   involved. That's a very, this is a very common kind of data query for data scientists.
[01:57:54.080 --> 01:57:59.360]   And you don't have to type all those brackets and tabs and semi-colons.
[01:57:59.360 --> 01:58:05.040]   Exactly. And what's the impressive thing with that is that there are obviously there's been a
[01:58:05.040 --> 01:58:10.560]   lot of Texas speech technology for years that's very good, but it has not worked well with code
[01:58:10.560 --> 01:58:17.200]   because that's so specialized in what it's designed for. And exactly. And like when I was hit by
[01:58:17.200 --> 01:58:25.520]   the car five years ago and I broke my wrist, my primary hand, like typing was before,
[01:58:25.520 --> 01:58:32.080]   before I was in a cast, when I was kind of traction, was impossible. And it made coding
[01:58:32.080 --> 01:58:37.920]   basically impossible. And I was using a lot of, you know, text to speech stuff or voice to text
[01:58:37.920 --> 01:58:45.120]   stuff rather. And code was, that was the biggest challenge. And so when I looked at
[01:58:45.120 --> 01:58:49.920]   hey, GitHub, I was like, okay, not only is it so cool that you can just speak what you want it to do
[01:58:49.920 --> 01:58:54.640]   and it can write it the right way in natural language. But the fact is that you can say things
[01:58:54.640 --> 01:59:01.360]   like new line, or you can say, you know, in hand, as in other things, and it's not getting confused
[01:59:01.360 --> 01:59:06.000]   because it's been trained for this specialized thing, as you said, which is really awesome.
[01:59:06.000 --> 01:59:19.040]   Amazing. Kind of incredible. Interesting story about A D S B. So I had never heard of ADSB not
[01:59:19.040 --> 01:59:28.640]   being a pilot. But if you heard about the Elon jet tracker, that's what was using ADSB, which
[01:59:28.640 --> 01:59:37.040]   is a database is actually technically ADSB exchanged. And it was kind of like IMDB or
[01:59:37.040 --> 01:59:43.920]   Wikipedia. It was created by users. And the reason it worked is because
[01:59:43.920 --> 01:59:51.280]   jet airplanes have all airplanes, I guess, have transponders transponding their
[01:59:51.280 --> 01:59:56.240]   tail number and their location as they fly around. That's how they know where everybody is. And
[01:59:56.240 --> 02:00:01.120]   air traffic control uses it. And I imagine other planes use it. Well, it turns out, if you're an
[02:00:01.120 --> 02:00:06.160]   enthusiast, you can also have a little receiver on the ground and monitor all the traffic going
[02:00:06.160 --> 02:00:12.080]   ahead. And then if somebody were to write a way to aggregate that data onto a map, and you had
[02:00:12.080 --> 02:00:18.000]   enough people with those little receivers all over the world, you'd have a pretty good tracking
[02:00:19.040 --> 02:00:27.280]   map of all the flights. Well, that's what ADSB exchange was. But, and I say was because
[02:00:27.280 --> 02:00:36.400]   it was owned by one person. A lot of people contributed, but Dan Strouffer founded the site and was the
[02:00:36.400 --> 02:00:45.680]   sole owner of the site. And he sold it to jetnet, which was, by the way, owned by get ready private
[02:00:45.680 --> 02:00:54.640]   equity company. And at this point, there is a little rebellion going on, including by the guy who does
[02:00:54.640 --> 02:01:01.920]   Elon jet, who said, I'm not going to use this data anymore, and I'm not going to contribute it
[02:01:01.920 --> 02:01:09.840]   to it anymore. It's understandable. I mean, the server costs, the hosting costs were expensive.
[02:01:10.800 --> 02:01:16.960]   ADSB exchange couldn't really monetize very well. It's free to use. They used advertising,
[02:01:16.960 --> 02:01:25.520]   and then they had a kind of higher paid tier. But it's still an expensive thing to run. And so,
[02:01:25.520 --> 02:01:32.480]   at some point, Strouffer decided that he was going to sell it. Jack Sweeney runs the Elon
[02:01:32.480 --> 02:01:41.680]   jet Twitter account said today is a sad day. If you feed ADSB exchange, we encourage you to stop
[02:01:41.680 --> 02:01:48.720]   feeding. ADSB exchange was founded on the principles of hobbyists community, not for profit,
[02:01:48.720 --> 02:01:58.240]   private equity firms. So, it'll be interesting to see within a few hours after the sale became
[02:01:58.240 --> 02:02:05.360]   public, the 11,000 feeders, 11,000 people running these receivers dropped significantly to 9,500
[02:02:05.360 --> 02:02:09.920]   people in a span of a few hours. I don't know where it stands right now. I'm not an expert on this,
[02:02:09.920 --> 02:02:20.160]   but I'd be very curious to see what happens. One user said flight aware, flight radar,
[02:02:20.160 --> 02:02:31.600]   win, Elon wins. All the guys who are out to get us win. So, remember the saga of Elon jet and
[02:02:31.600 --> 02:02:36.960]   Elon chasing it off Twitter, and he went to Mastodon, and then Elon blocked every Mastodon
[02:02:36.960 --> 02:02:45.920]   mentioned on Twitter. There was a final line in that story. It's kind of sad. Wednesday,
[02:02:45.920 --> 02:02:52.480]   they announced that they had been acquired, kind of like IMDB or CDDB or all these other
[02:02:52.480 --> 02:02:58.160]   fortunately, nobody's acquired Wikipedia. I hope not. And no one can acquire Mastodon. Again,
[02:02:58.160 --> 02:03:04.240]   this is the argument for these distributed places. Jetnet is owned by Silversmith Capital
[02:03:04.240 --> 02:03:09.840]   Partners. They were acquired last year. The acquisition is the second of what the company
[02:03:09.840 --> 02:03:15.600]   anticipates will be several future acquisitions as Jetnet expands, is data driven product offerings,
[02:03:16.000 --> 02:03:20.080]   for the aviation industry. So, you got a problem there. If you've got volunteers
[02:03:20.080 --> 02:03:27.200]   freely uploading this data, and suddenly you make a killing, selling it, and this private equity
[02:03:27.200 --> 02:03:36.400]   comes along, you need the volunteers, don't you? Anything to say about that, or should we move on?
[02:03:39.680 --> 02:03:45.280]   Sort of sympathetic, I guess, to the volunteers. At the same time, the jet's tracking stuff,
[02:03:45.280 --> 02:03:50.960]   I know it's legal. I'm not arguing the legality at all, because obviously you have to be able to,
[02:03:50.960 --> 02:03:55.120]   the FAA has to be able to know what planes are in the air. I'm not questioning any of that.
[02:03:55.120 --> 02:03:57.600]   But I do think the jet tracking stuff is gross. I do.
[02:03:57.600 --> 02:04:01.360]   Well, honestly, I understand Elon's point. I mean,
[02:04:03.280 --> 02:04:09.760]   but it's not exactly a sessionation coordinates. And Sweeney could have done some things.
[02:04:09.760 --> 02:04:13.840]   No, I like delaying the tweet by an hour or two.
[02:04:13.840 --> 02:04:21.200]   Right. And I'm not saying that it was a assassination thing. I think that was hyperbole.
[02:04:21.200 --> 02:04:26.080]   People and fandoms like teenage girls have been doing this for years for their favorite pop stars,
[02:04:26.080 --> 02:04:30.560]   and it was gross then, and they would put stuff on Twitter and on Tumblr and whatnot.
[02:04:30.560 --> 02:04:37.200]   It's gross now. I do feel for the aviation enthusiast community who feels like this
[02:04:37.200 --> 02:04:41.200]   thing they've been contributing to is now been sold to private equity who will be making money
[02:04:41.200 --> 02:04:44.640]   off of it. But at the same time, the data's either open or it's not. You know what I mean?
[02:04:44.640 --> 02:04:50.560]   Like, you can create your own thing. But I mean, this is public data for a reason.
[02:04:50.560 --> 02:04:59.680]   I remember watching Gaga's movie, and I just watched Taylor Swift's "Miz Americana" movie.
[02:04:59.680 --> 02:05:03.280]   And the thing that I really sticks in my mind is these poor people go out of their
[02:05:03.280 --> 02:05:11.200]   doors of their apartment. And at any time of the day or night, there are hundreds of fans
[02:05:11.200 --> 02:05:16.400]   standing there waiting for them. They have to have big security guards just to get them to the car
[02:05:16.400 --> 02:05:26.080]   and barriers. And it's and I'm starting to read much to my chagrin Prince Harry's spare.
[02:05:26.960 --> 02:05:30.320]   And it's somewhat the similar situation. It killed Princess Diana.
[02:05:30.320 --> 02:05:36.880]   Right. Well, and the way that a lot of the paparazzi finds where the celebrities are going to be
[02:05:36.880 --> 02:05:40.720]   is that they track their jets because a lot of them have if they own their own jet, then it's
[02:05:40.720 --> 02:05:47.200]   registered. If they are simply renting one, then it's harder. But like Taylor Swift owns her own
[02:05:47.200 --> 02:05:53.360]   planes. And now she's doing the thing, I think, where she hides the registration, which you can do
[02:05:53.360 --> 02:05:59.040]   a certain way. People still, her fans are insane. And I say this as a big Taylor Swift fan, but
[02:05:59.040 --> 02:06:03.440]   not one who appreciates or encourages any of this because I think this stuff is just
[02:06:03.440 --> 02:06:09.200]   gross and disgusting. The K-pop fans are the same way where they will literally track
[02:06:09.200 --> 02:06:15.840]   exactly where people are at all times to try to know and put it up on the internet and not
[02:06:15.840 --> 02:06:20.640]   realizing that they get mad about the paparazzi stalking their favorite stars. It's like,
[02:06:20.640 --> 02:06:24.960]   how do you think they're figuring out exactly where they're landing and then showing up at private
[02:06:24.960 --> 02:06:31.280]   airports or, god forbid, they're having to fly commercial, showing up literally a baggage
[02:06:31.280 --> 02:06:37.440]   claim outside LAX. That's because people are doing things like this and they're tracking their
[02:06:37.440 --> 02:06:44.080]   up-removement. And there's something gross about that. And again, I think that's, I'm not trying
[02:06:44.080 --> 02:06:48.640]   to say that everybody, in fact, most of the people part of this community are not involved in that
[02:06:48.640 --> 02:06:54.080]   at all. But I do think that when we have those discussions, and again, I don't think that
[02:06:54.080 --> 02:07:01.440]   calling it assassination wasn't anyway correct, but there is this very gross aspect of for high
[02:07:01.440 --> 02:07:07.520]   profile people having no privacy because you have really obsessive people out there who are
[02:07:07.520 --> 02:07:13.200]   tracking their remove and then in turn passing that on to people who are then going to take photos
[02:07:13.200 --> 02:07:19.760]   to sell for lots of money. Yeah. I know that
[02:07:19.760 --> 02:07:29.120]   Taylor, let me see if I can, I don't want to jeopardize her safety. I feel bad for anybody in
[02:07:29.120 --> 02:07:37.040]   this situation. She uses face recognition at her concerts to find the most. Go ahead.
[02:07:37.040 --> 02:07:42.320]   Yeah, she did. I think it was the last concert. Yeah. I think they said.
[02:07:42.320 --> 02:07:46.720]   Because they know who these most dangerous staff. I don't remember showing my face.
[02:07:46.720 --> 02:07:51.120]   Yeah. Oh, you don't? Well, but they don't need to, you don't need to walk up to a camera and
[02:07:51.120 --> 02:07:58.400]   smile. They see you coming in. Yeah, I guess so. I guess, I guess, yeah, I guess I was just,
[02:07:58.400 --> 02:08:02.000]   in my mind, I was, because I saw I remember being at the concert and seeing, you know,
[02:08:02.000 --> 02:08:06.640]   the signs up, but I don't remember, obviously, there wasn't anything when you entered where you
[02:08:06.640 --> 02:08:11.680]   had to like scan your face. No, no, they just look at the crowd. They just watch you coming in.
[02:08:12.240 --> 02:08:15.920]   And they have, apparently, they have face recognition data for people who are
[02:08:15.920 --> 02:08:21.840]   considered threats. And you know, I'm more power to her. I don't blame her for doing that. I don't
[02:08:21.840 --> 02:08:27.680]   blame her people for doing that because her life is at risk. It's a shame she has to. But it does
[02:08:27.680 --> 02:08:33.520]   raise some interesting questions. So there are big signs saying what you're being, your face is
[02:08:33.520 --> 02:08:38.960]   being captured. Yeah, something like that. I took a photo of it. I'll have to find it. I don't have
[02:08:38.960 --> 02:08:44.720]   enough top of my hand, but I did take a photo of it when I saw it at the Seattle reputation
[02:08:44.720 --> 02:08:51.920]   tour concert. I'm sure it was at the one that I saw in New Jersey as well. This was in 2018, so
[02:08:51.920 --> 02:08:56.560]   which was the last time she toured. But yeah, there was something like that that said that, you know,
[02:08:56.560 --> 02:09:03.280]   that their, you know, your photo may be used, you know, by being at this concert, like you've
[02:09:03.280 --> 02:09:07.680]   consented, you know, to your photo being used, you know, in a database for whatever the purpose
[02:09:07.680 --> 02:09:14.320]   might be, which, you know, fair enough, if it's something that if you want to attend this concert,
[02:09:14.320 --> 02:09:18.560]   you have to make that trade off. There are plenty of people I'm sure who would be like, well, I will
[02:09:18.560 --> 02:09:25.440]   never go to a concert that does that. But I obviously, so many people have my face, my faces, and so
[02:09:25.440 --> 02:09:32.720]   many databases. I wanted to see the concert. So apparently they put rehearsal clips up on a kiosk.
[02:09:34.080 --> 02:09:37.520]   And then people would go over and look. That's what it was. They would go over and look at the
[02:09:37.520 --> 02:09:43.440]   clips. That was sneaky. And there was a camera inside the display taking their picture. Right.
[02:09:43.440 --> 02:09:48.640]   That's what it was. And yeah, it was this casting. And then there was a sign on the kiosk that told
[02:09:48.640 --> 02:09:53.440]   you what it was doing. Oh my God. The images, this is from Rolling Stone, which broke the story,
[02:09:53.440 --> 02:09:58.480]   this back in 2018. The images were being transferred to a Nashville command post,
[02:09:58.480 --> 02:10:02.640]   where they were cross referenced for the database of hundreds of the pop stars and hundreds
[02:10:03.200 --> 02:10:11.200]   of the pop stars known stalkers. Everybody who went by would stop and stare at it and the software
[02:10:11.200 --> 02:10:20.640]   would start working. And presumably, if you were one of those people, some big burly guy with a
[02:10:20.640 --> 02:10:26.880]   walkie talkie would walk over and say, excuse me, sir. Now, this is relevant to today because
[02:10:26.880 --> 02:10:31.680]   it's been happening. And we've talked about this before at Madison Square Garden, the Dolan's who
[02:10:31.680 --> 02:10:36.880]   own MSG and Madison Square Garden owns a bunch of other stuff. Radio City Music Hall.
[02:10:36.880 --> 02:10:42.240]   Well, it's happened first at Radio City Music Hall, a mother with her Girl Scout troop
[02:10:42.240 --> 02:10:49.600]   went to see the rockets for the holiday show and was informed as she enters. No, sorry lady,
[02:10:49.600 --> 02:10:55.760]   you can't come in. Had to wait outside out front while her girls watched the rockets,
[02:10:56.640 --> 02:11:03.440]   found out it was because she works for a law firm that has a lawsuit with MSG. And apparently,
[02:11:03.440 --> 02:11:09.040]   the Dolan's have been doing this MSG's been doing this to any lawyer that has anything going on with
[02:11:09.040 --> 02:11:15.680]   MSG. They have face recognition and they will lock you out. Or just if you work for a law firm
[02:11:15.680 --> 02:11:20.960]   that also has other lawyers. Oh, yeah, the mom said, I don't know anything about this. This is not
[02:11:20.960 --> 02:11:28.160]   my I don't I'm not suing him. Sorry, lady. And of course, James Dolan, who came fairly fiery
[02:11:28.160 --> 02:11:33.760]   interview about this couple of days ago, says that's all right. It's a private institution to
[02:11:33.760 --> 02:11:40.960]   which the liquor licensing authority in New York says, well, not exactly. Because when you have a
[02:11:40.960 --> 02:11:48.480]   liquor license, there are caveats, covenants, things you agree, including being open to the public,
[02:11:48.480 --> 02:11:54.800]   you can't have a private liquor license. So there is some question. In fact, New York State
[02:11:54.800 --> 02:12:01.680]   Attorney General, Leticia James, is paying attention investigating New York state legislatures
[02:12:01.680 --> 02:12:06.240]   have introduced a bill that would ban face recognition and sporting events.
[02:12:06.240 --> 02:12:11.600]   And now the liquor authority, the New York State liquor authority, SLA,
[02:12:12.880 --> 02:12:20.240]   is saying your liquor license is in jeopardy. Dolan gave an interview Thursday, a fiery, I'm told,
[02:12:20.240 --> 02:12:28.320]   I didn't watch an interview with Fox five channel five in New York, in which he defended his family's
[02:12:28.320 --> 02:12:34.080]   right to block anybody we don't like from coming in. And of course, Master Square Garden is the home
[02:12:34.080 --> 02:12:39.760]   of the Rangers hockey team and a couple of lawyers. Now, we mentioned earlier, you don't want to get
[02:12:39.760 --> 02:12:46.800]   on the bad side of lawyers. I'd say, well, sue your ass. And I think there probably will be some
[02:12:46.800 --> 02:12:52.320]   lawsuits. Dolan says, well, all right, liquor authority, you watch, I'm gonna, I'm gonna pick a
[02:12:52.320 --> 02:12:57.120]   day and we're not gonna serve any any beer at a Rangers game and then see how you feel.
[02:12:57.120 --> 02:13:01.440]   And I think he said he was going to be of the phone number of the liquor authorities.
[02:13:01.440 --> 02:13:06.720]   You call him. Yeah. He docked him. He actually gave out the number on the TV.
[02:13:08.000 --> 02:13:13.280]   It seems incredibly petty and a great way to get bad publicity without really accomplishing much
[02:13:13.280 --> 02:13:19.520]   of anything. So I understand why Taylor might do this at her concerts. In fact, I'd sad that she
[02:13:19.520 --> 02:13:27.120]   has to. But I understand why. I don't think James Dolan really has to block lawyers from coming into
[02:13:27.120 --> 02:13:32.960]   Rangers games. No, no, I mean, I think it's one thing to be like, okay, we have, I mean, she's had
[02:13:32.960 --> 02:13:38.160]   people like show up in her house. Yeah. When she's not there and like take showers and it's awful.
[02:13:38.160 --> 02:13:43.360]   And she has very serious mentally disturbed people after her totally get that. It's been another
[02:13:43.360 --> 02:13:48.720]   thing to be like, Oh, you work at a law firm that's involved in litigation with my company. So you're
[02:13:48.720 --> 02:13:55.440]   banned from entering the premises. I mean, a, that's really concerning that you have like the facial
[02:13:55.440 --> 02:14:00.640]   data of everybody who works at law firm. Like that's, that's concerning right there. And then B,
[02:14:00.640 --> 02:14:05.280]   it's like really, really so they can't even come see the rockets. Like, what does that have to do
[02:14:05.280 --> 02:14:11.760]   with anything? Here is, I don't have to zoom in on this, but here's a little thumbnail from YouTube
[02:14:11.760 --> 02:14:18.000]   of Dolan holding up the name of the SLA's chief executive and his phone number
[02:14:18.000 --> 02:14:27.600]   and his email and his picture saying, I'm going to put this wherever we sell alcohol, I'm going to
[02:14:27.600 --> 02:14:33.200]   put this up in the, in the stadium. I can't imagine all that many people siding with him.
[02:14:33.200 --> 02:14:42.320]   Wow. No, no, no. New York State Senator who represents the part of Manhattan that Madison Square
[02:14:42.320 --> 02:14:47.200]   Garden is in described Dolan's interview, according to the Washington Post, is a public meltdown,
[02:14:47.200 --> 02:14:54.000]   called him the poster child of privilege, who receives, and this is an important point, a $43
[02:14:54.000 --> 02:15:00.400]   million a year tax break from New Yorkers, as is often the case with these big sports venues.
[02:15:00.400 --> 02:15:07.280]   Sometimes face recognition gone wrong. Sometimes most of the time, again, Taylor Swift seems to
[02:15:07.280 --> 02:15:13.920]   me the only actual legitimate use of this, because you got to protect Taitay. I'm sorry,
[02:15:13.920 --> 02:15:17.600]   that's just, you know, not okay. She's all right. She's doing okay though, right?
[02:15:20.000 --> 02:15:24.880]   Yeah. I think so. Yeah. It's a good movie. I liked it. I enjoyed it. That's the thing now.
[02:15:24.880 --> 02:15:30.560]   Everybody has to do this. Selena Gomez, Gaga, I think, I don't know if she started it, didn't she?
[02:15:30.560 --> 02:15:35.120]   And then Gaga? Yeah, she really did. It was truth or dare. Yeah. Truth or dare. With Warren
[02:15:35.120 --> 02:15:40.000]   Beatty hanging around the dressing room. Same. What are you doing? You want to go out after the
[02:15:40.000 --> 02:15:46.480]   show? You want to have a drink? Want to hang out? All right, one more break. Then we are going to
[02:15:47.280 --> 02:15:51.680]   wrap this puppy up. But it's such an important advertiser. I want to tell everybody,
[02:15:51.680 --> 02:15:58.800]   you got to get Bitwarden. Bitwarden is my choice for password manager. I know a lot of you
[02:15:58.800 --> 02:16:07.280]   followed our advice. I'm sorry. And went with the other guys. That hasn't ended up so well. We
[02:16:07.280 --> 02:16:15.440]   didn't know. Honest. If you're looking for a better password manager, can I say, in my experience,
[02:16:15.440 --> 02:16:21.280]   open source is always the way to go with anything like this. Because you know exactly what's going
[02:16:21.280 --> 02:16:26.000]   on. If at any point you don't like it, you can fork it. In fact, Bitwarden, a lot of people run
[02:16:26.000 --> 02:16:30.880]   their own server with the Bitwarden vault. So it's not on Bitwarden's vault. You can do that with
[02:16:30.880 --> 02:16:38.000]   your individual account. That's awesome. And Bitwarden has its own server software. But there's a
[02:16:38.000 --> 02:16:43.200]   beautiful rust fork of it called vault. Warren, you can run if you don't want to run that. That's
[02:16:43.200 --> 02:16:51.600]   the beauty of open source. Bitwarden is the only open source cross platform password manager.
[02:16:51.600 --> 02:16:56.720]   You can use at home at work or on the go. It's trusted by millions. Steve Gibson.
[02:16:56.720 --> 02:17:02.560]   I think he knows Bitwarden's a sponsor, but I know Steve is a pretty independent thinker. He
[02:17:02.560 --> 02:17:07.680]   was the guy who turned us on last pass in the beginning. He moved off last pass to Bitwarden as
[02:17:07.680 --> 02:17:12.640]   well. I've been on Bitwarden for several years. We had been using LastPass Enterprise. We are
[02:17:12.640 --> 02:17:18.320]   moving now to Bitwarden Enterprise. Russell started that process this week. I'm really excited about
[02:17:18.320 --> 02:17:23.760]   this. And I love it because Bitwarden lets you be an individual. I have my individual account,
[02:17:23.760 --> 02:17:29.200]   but you can also have an enterprise account. Now, let me explain. Of course, you want to know this.
[02:17:29.200 --> 02:17:34.400]   All your data in Bitwarden's vault is end to end encrypted. They don't have access to it.
[02:17:34.400 --> 02:17:40.560]   Not just the passwords, but unlike some other companies, all the metadata, the sites you visit,
[02:17:40.560 --> 02:17:46.720]   when you visited them, all that stuff is encrypted, just like your passwords. That's really important.
[02:17:46.720 --> 02:17:51.120]   And of course, Bitwarden doesn't track your data in the mobile app. All it does is crash
[02:17:51.120 --> 02:17:55.600]   reporting. If you don't like that, this is where open source is beautiful. Get the after
[02:17:55.600 --> 02:18:01.280]   installation. You won't even have that. Bitwarden's open source. It invites anyone to review library
[02:18:01.280 --> 02:18:07.280]   implementations at any time on GitHub. You can review their privacy policies at bitwarden.com/privacy.
[02:18:07.280 --> 02:18:10.880]   You can protect your personal data and privacy. You can add security to your passwords.
[02:18:10.880 --> 02:18:15.840]   Use Bitwarden to generate strong, randomly generated passwords for every account.
[02:18:15.840 --> 02:18:20.320]   If you go to the Bitwarden site, you'll see they have a password strength meter. You can try out
[02:18:20.320 --> 02:18:26.000]   your passwords there safely. See how strong it is. They also have, and I love this feature,
[02:18:26.000 --> 02:18:31.760]   a username generator. So, you know, when you create an account, you use your email and a password.
[02:18:31.760 --> 02:18:37.840]   Well, what if the email you used was completely unique and never used before and will never use
[02:18:37.840 --> 02:18:43.520]   again? That's what the username generator does. It generates unique usernames for every account,
[02:18:43.520 --> 02:18:48.880]   stores them. And you, of course, you still want to get those recovery emails. So, what they do is
[02:18:48.880 --> 02:18:55.520]   they work with five, the big five integrated email alias services. Our other sponsor, Fastmail's
[02:18:55.520 --> 02:19:02.400]   one of them, SimpleLogin, a non-addie, Firefox relay. They just added DuckDuckGo. So, you still
[02:19:02.400 --> 02:19:07.040]   get the email, but you use an obfuscated address. So, the company doesn't have your address.
[02:19:07.040 --> 02:19:14.160]   This is a great way to increase the security and to make sure that every single login is unique
[02:19:14.160 --> 02:19:19.120]   and is never used again. Keep your main email address out of the databases too, right?
[02:19:20.560 --> 02:19:25.680]   And that's an, I do that too. I think it's a great reason to use it. It integrates beautifully with
[02:19:25.680 --> 02:19:30.800]   Bitwarden in those services and for your business. We're very happy. We're moving to Bitwarden for
[02:19:30.800 --> 02:19:35.840]   business. It's fully customizable, adapts to your business needs. There's a team organization's plan
[02:19:35.840 --> 02:19:40.960]   that's $3 per month per seat. There's an enterprise organization plan that's the one we're going to
[02:19:40.960 --> 02:19:49.120]   $5 a month per seat. It's great. You can share data privately with coworkers across departments of
[02:19:49.120 --> 02:19:54.240]   the truck company. People share passwords. We know that. They write it on a piece of paper and they
[02:19:54.240 --> 02:20:02.640]   say, "Here, Marie, here's the password to the Wi-Fi." No, don't do that. Use Bitwarden. You can
[02:20:02.640 --> 02:20:07.840]   securely share those passwords. And if you've got a Bitwarden individual account as I do,
[02:20:07.840 --> 02:20:12.800]   it's very easy to integrate your individual account with the organizational account without
[02:20:12.800 --> 02:20:16.720]   crossing that barrier so your passwords are still separate, but you only have one login and
[02:20:16.720 --> 02:20:22.080]   there's all your passwords. There's also, of course, the basic free account, unlimited free forever,
[02:20:22.080 --> 02:20:28.080]   unlimited passwords. I think the $10 a year for premium is worth it just to support Bitwarden.
[02:20:28.080 --> 02:20:32.240]   I'm a big fan. I've been doing that for a couple of years. Family organization option. If you've
[02:20:32.240 --> 02:20:39.680]   got a family, you want to get them all on Bitwarden. Up to six users, total cost $3.33 a month, $3.33
[02:20:39.680 --> 02:20:45.920]   cents a month. I think that's worth it as well. And of course, it makes it so easy to import from
[02:20:45.920 --> 02:20:52.080]   any other password manager. Export out of it. Important to Bitwarden. I hear from everybody,
[02:20:52.080 --> 02:20:58.160]   "Well, that was easy." That was easy. The only hard part is changing all those passwords
[02:20:58.160 --> 02:21:04.640]   that that other company let out into the open. Bitwarden, trusted by millions of individuals,
[02:21:04.640 --> 02:21:08.880]   teams and organizations worldwide. It's the only open source cross platform password manager.
[02:21:08.880 --> 02:21:14.560]   You can use that home on the go or at work. They've got a command line version for Linux.
[02:21:14.560 --> 02:21:19.600]   Open source is a beautiful thing. Get started with a free trial of a Teams or Enterprise plan
[02:21:19.600 --> 02:21:27.760]   or get started for free across all devices as an individual user. Bitwarden.com/twit.
[02:21:27.760 --> 02:21:31.680]   I think the world is converging. I think the world has said, "You know what? This is the way to go."
[02:21:31.680 --> 02:21:42.560]   Open source, baby. Bitwarden.com/twit. Highly recommended. We thank you so much for supporting
[02:21:42.560 --> 02:21:51.200]   our show. When Bitwarden called, I said, "Yes. Yes. Yes. I will do your ads. I will happily do your ads."
[02:21:51.200 --> 02:21:55.520]   Hey, we had a lot of fun this week on Twit. And you know what we did? Because we were so worried
[02:21:55.520 --> 02:21:59.760]   some of you might have missed some of the exciting moments. We've made this mini movie
[02:21:59.760 --> 02:22:05.760]   for your consideration. Watch. I hereby verify that I, Leo Laport, like
[02:22:05.760 --> 02:22:11.760]   Descripts to create an overdubbed version of my voice. Why do you want this, Anthony?
[02:22:11.760 --> 02:22:19.520]   No, I do. What the hell? What the hell? Well, hey, hey, it's A.I. Leo Laport, the A.I. Tech Guy.
[02:22:19.520 --> 02:22:26.080]   This week on Twit. Mac Break Weekly. Jason has his reviews of the new Apple hardware. We'll talk
[02:22:26.080 --> 02:22:33.360]   about that. It's our first Apple Silicon, you know, boring speed bump release, right? Where
[02:22:33.360 --> 02:22:38.720]   not that they're bad. They're remarkable computers. They're just not particularly new. They're just
[02:22:38.720 --> 02:22:46.400]   what you expect from last year or from two years ago, I guess, except faster.
[02:22:46.400 --> 02:22:52.080]   This week in Google, Richard Hay is here. He is the face of all of those layoffs,
[02:22:52.080 --> 02:22:57.520]   Google cutting 12,000 jobs. Richard, I had breakfast with a friend at Google's by Happen.
[02:22:57.520 --> 02:23:03.520]   And he said that his boss has hundreds of employees and didn't know. Yeah, my boss,
[02:23:03.520 --> 02:23:08.080]   I mean, and I just had a meeting with him on Tuesday and there was no inkling of anything like
[02:23:08.080 --> 02:23:12.640]   this on the horizon, right? So it was the decision was made on a whole nother level.
[02:23:12.640 --> 02:23:20.160]   Tech News Weekly, detailer Swift Saga continues Live Nation Ticketmaster and a whole lot of angry
[02:23:20.160 --> 02:23:26.000]   Swifties. When Senator Blumenthal says that Ticketmaster needs to look in the mirror and says,
[02:23:26.000 --> 02:23:32.400]   it's me, I'm the problem. It's me. You don't expect those kinds of jokes to happen at a Senate hearing.
[02:23:32.400 --> 02:23:39.440]   There are die-hard Swifties that are watching this hearing. And these aren't people that are
[02:23:39.440 --> 02:23:48.640]   typically like, yeah, let me tune in to a Senate dot gov slash whatever. It's me. Hi, I'm the real Leo.
[02:23:49.280 --> 02:23:57.760]   It's me. Okay, it still has a way to go, I think. But that was that was pretty good. Thank you to
[02:23:57.760 --> 02:24:04.960]   Anthony Nielsen who snuck in here to just read this if you don't mind. And boy, that's scary.
[02:24:04.960 --> 02:24:10.240]   That's terrifying. So Christina, it's completely coincidental, but two, not one, but two Taylor
[02:24:10.240 --> 02:24:16.640]   Swift stories in one episode. Just for you, which is very exciting. Just just for me, I should point
[02:24:16.640 --> 02:24:21.280]   out that if anybody, I'm going to put a link in it both in the Discord and the IRC. I found this
[02:24:21.280 --> 02:24:28.560]   this week thanks to Mastodon, thanks to Jeff Atwood from Coding Horror, who there's a mashup of,
[02:24:28.560 --> 02:24:34.560]   where's my mind from the Pixies and anti-hero from Taylor Swift, and then it's edited to include
[02:24:34.560 --> 02:24:41.840]   both Fight Club and the anti-hero music video. It's A, the baseline fits perfectly. It's one of the
[02:24:41.840 --> 02:24:48.240]   best like mashups that's heard in a long time and B, the video editing is superb. So if you're a fan
[02:24:48.240 --> 02:24:52.800]   of Fight Club and Taylor Swift, which I know is a Venn diagram, which might just be me,
[02:24:52.800 --> 02:24:59.440]   is everything you've ever wanted in your life. It's fantastic. Small group of people. But it's so
[02:24:59.440 --> 02:25:08.320]   good. There was actually, we didn't do it as a story, but Alex Lindsey sent me a link to a Billy
[02:25:08.320 --> 02:25:17.520]   Eilish song that somebody used AI to replace Billy Eilish's voice with Arianda Grande's voice.
[02:25:17.520 --> 02:25:24.240]   Have you seen that? Huh, I probably can't play it. I probably shouldn't play it. Let me see if I
[02:25:24.240 --> 02:25:34.960]   can find the link. It's on YouTube. It's the song is happier than ever. Should I play it?
[02:25:34.960 --> 02:25:39.760]   Well, I get now do I get taken down if I play a Billy Eilish song with Ariana Grande singing it?
[02:25:39.760 --> 02:25:44.080]   I don't think so. Who would sue?
[02:25:44.080 --> 02:25:48.880]   Hey, let me ask my lawyer over here.
[02:25:48.880 --> 02:25:58.800]   It's actually it's interesting because it gives you some idea of what can what could be done. Let me
[02:25:58.800 --> 02:26:04.000]   play it. What could possibly go wrong?
[02:26:04.000 --> 02:26:09.120]   Don't relate to you. No, because I know what's really this.
[02:26:09.120 --> 02:26:19.120]   Is that weird? Because I don't know the Eilish song. Oh my god. No, I do. This is amazing.
[02:26:19.120 --> 02:26:25.760]   It really sounds like Ariana Grande. It really does. Ariana never sung those
[02:26:25.760 --> 02:26:33.600]   saying those words. And I guess they just took a Billy Eilish audio and applied Ariana Grande's
[02:26:33.600 --> 02:26:40.960]   prosody to it or something like that. And go ahead, YouTube sue me. I did ask the lawyer.
[02:26:40.960 --> 02:26:46.240]   And it says it is possible to get sued for playing an AI revision of a song on YouTube. If the
[02:26:46.240 --> 02:26:50.560]   revision infringes on so much copyright, copyright laws are very bi-country, but in general creating
[02:26:50.560 --> 02:26:54.800]   an AI revision of a song that incorporates substantial parts of the original song without
[02:26:54.800 --> 02:27:00.800]   permission could be considered copyright infringement. So go after this tears hero guy, not me. Okay.
[02:27:00.800 --> 02:27:07.680]   He's a guy. He's a guy who did this. That's kind of wild. I think we're going to see AI is
[02:27:07.680 --> 02:27:11.200]   this is you know, I'm happy because I was tired of saying things like
[02:27:11.200 --> 02:27:18.400]   Elon Musk ruins Twitter again. And I'm I'm looking forward to talking more about what AI can do.
[02:27:18.400 --> 02:27:24.400]   What AI can ruin from now on? Yeah, let AI ruin it. Nobody will defend AI.
[02:27:24.640 --> 02:27:31.440]   I'm guessing Tim Stevens. I appreciate all you do. And I'm so glad that you have landed
[02:27:31.440 --> 02:27:38.720]   successfully at substack. Tim Stevens dot substack.com. Now we got to get you to write more for it.
[02:27:38.720 --> 02:27:44.800]   Right? How long you've been doing it? I watched a couple of weeks after I left CNET. So I've been
[02:27:44.800 --> 02:27:49.120]   trying to do about opposed to week give or take. But that's really just kind of a place for me to
[02:27:49.120 --> 02:27:53.760]   air my thoughts that kind of thing. You can definitely check me out on the Jopnik Road and Track Motor
[02:27:53.760 --> 02:27:57.920]   Trend Tech Crunch a bunch of other places. Well, I've been really fortunate to have a lot of
[02:27:57.920 --> 02:28:01.840]   great assignments. And there's a lot more good stuff coming up to. Good. I'm really pleased.
[02:28:01.840 --> 02:28:05.120]   It's always great to see. I'm sorry you didn't get to do any ice fishing this year.
[02:28:05.120 --> 02:28:09.200]   Yeah, that's okay. All right. Did you do any ice fishing? That's the question.
[02:28:09.200 --> 02:28:16.000]   Not that either. No. Okay. Thanks Tim. I appreciate it. Christina Warren, always a pleasure to see
[02:28:16.000 --> 02:28:22.240]   you. Thank you so much for bringing your shoes, your tiny feet, and your brilliance to this show.
[02:28:22.240 --> 02:28:28.800]   Thank you so much for having me. I'm sorry if we're having audio or video problems, but this has
[02:28:28.800 --> 02:28:33.040]   been great. It's been great being on with Harry and Tim and always loved talking about stuff.
[02:28:33.040 --> 02:28:37.600]   Always loved being on Twitch. Yeah. She's a senior developer advocate, the senior developer
[02:28:37.600 --> 02:28:47.520]   advocate at GitHub. Mastodon.social@film_girl, our newest Mastodoner. And Tim Stevens is on Mastodon
[02:28:47.520 --> 02:28:54.160]   social as well. Tim Stevens, Mastodon social. Harry McCracken, you're also on Mastodon,
[02:28:54.160 --> 02:28:58.800]   but you're on the San Francisco Bay Area Mastodon. That's FBA.social. That's awesome.
[02:28:58.800 --> 02:29:05.040]   Slash Harry McCracken. Technologizer, global tech editor at Fast Company. Can I plug my newsletter
[02:29:05.040 --> 02:29:11.200]   again? Yes. I have a new newsletter called Pluged In. You can either go to fastcompany.com and
[02:29:11.200 --> 02:29:16.000]   click on the hamburger menu and look for newsletter or just Google Fast Company newsletter and
[02:29:16.000 --> 02:29:18.720]   you'll see how to sign up. It comes out every Wednesday morning.
[02:29:18.720 --> 02:29:25.040]   I really enjoy it. I've always enjoyed your writing because the thing that's great about you,
[02:29:25.040 --> 02:29:32.800]   you have a unique and I think well-informed take on what's going on in tech. You've been doing
[02:29:32.800 --> 02:29:39.760]   this a long time and you have a voice. Who else would write? Big tech's layoff binge stinks
[02:29:41.680 --> 02:29:47.040]   as a headline. If Max get touch screens, Apple's Age of Intransigence really is over.
[02:29:47.040 --> 02:29:51.600]   How many writers would you know would use the word intransigence in a sense?
[02:29:51.600 --> 02:29:55.280]   I think that's over and I'm sure if I asked Grammarly, they would have told me, no, people
[02:29:55.280 --> 02:30:02.320]   don't know this word. I like it. If chat GPT doesn't get a better grasp of facts, nothing else
[02:30:02.320 --> 02:30:10.640]   matters. I agree. And nine tech products you found essential in 2022. All of that and more
[02:30:10.640 --> 02:30:17.600]   at the new plugged in newsletter at Fast Company, fastcompany.com. Just look for plugged in in the
[02:30:17.600 --> 02:30:23.680]   hammer, hammering menu. And thanks for bringing Marie. It's great to see you, Marie. I appreciate it.
[02:30:23.680 --> 02:30:30.240]   We thank all of you for joining us. We do this show every week, 2PM Pacific, 5PM Eastern,
[02:30:30.240 --> 02:30:35.760]   2200 UTC on a Sunday afternoon. It's the best way to spend your Sunday with us.
[02:30:35.760 --> 02:30:40.720]   If you want to watch it live at live.twit.tv, if you're doing that, join us in the chatroom,
[02:30:40.720 --> 02:30:46.480]   IRC.twit.tv. All you need is a browser. But if you have an IRC client, if you're an old school
[02:30:46.480 --> 02:30:52.080]   kind of person, you can also use that. We have a Discord. Thanks to our fabulous club,
[02:30:52.080 --> 02:30:57.600]   Twit members. Club Twit is seven bucks a month and gives us a little bit of a financial boost,
[02:30:57.600 --> 02:31:02.880]   which these days would kind of need, but it also gives you ad-free versions of all of our shows.
[02:31:03.600 --> 02:31:08.640]   Access to the Discord, where you can find all sorts of fight things. Oh, this is the fight club
[02:31:08.640 --> 02:31:12.560]   thing. I might play this after the show so we don't get taken down.
[02:31:12.560 --> 02:31:17.680]   Christina. Absolutely, but it is very good. Put that in there. See, she's in our Discord.
[02:31:17.680 --> 02:31:24.720]   You also get shows that we don't normally put in the regular feeds like Michael Sargent's
[02:31:24.720 --> 02:31:30.560]   Hands on Macintosh. Paul Therat does Hands on Windows. Coming up in a couple of weeks,
[02:31:30.560 --> 02:31:35.440]   Win-Two Dows, Fireside Chat. She's of course the host of All About Android.
[02:31:35.440 --> 02:31:40.720]   February 10th, Daniel Suarez joins us. His new book is coming out in just a couple of days.
[02:31:40.720 --> 02:31:45.840]   And we will be talking about critical mass with Daniel. And if you're in the club,
[02:31:45.840 --> 02:31:50.880]   you'll get to ask him questions directly. So that's great. Samibles, Samadar Carr Guy will be
[02:31:50.880 --> 02:31:55.680]   talking March 2nd. Stacey's book club. We've decided on a book, Sea of Tranquility.
[02:31:56.240 --> 02:32:01.440]   Oh, look, Victor's going to do an inside Twitch chat. One of our favorite editors,
[02:32:01.440 --> 02:32:05.120]   Victor Bognow will be doing that. So we and Pruitt, our community managers,
[02:32:05.120 --> 02:32:09.840]   put together a lot of events. It's kind of like, I don't know, it's like the 92nd Street
[02:32:09.840 --> 02:32:15.120]   Y for the internet. You know, come on by, join the club. Seven bucks a month. Look at all you get.
[02:32:15.120 --> 02:32:21.840]   Twit.tv/club. Twit. Thank you so much for your support. Thank you all for being here. We'll see
[02:32:21.840 --> 02:32:29.120]   you next time. Another Twitch is in the can. Bye-bye.
[02:32:29.120 --> 02:32:36.080]   Do the Twitter. All right. Do the Twitter, baby. Do the Twitter. All right. Do the Twitter.

