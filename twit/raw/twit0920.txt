;FFMETADATA1
title=Biological Bootloaders
artist=Leo Laporte, Cory Doctorow, Rene Ritchie, Georgia Dow, Iain Thomson
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-03-27
track=920
language=English
genre=Podcast
comment=Apple VR Goggles, TikTok Ban, Twitter Source Code Leak
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.840]   It's time for Twit this week in Tech, a special visit coming up from Corey Doctorow.
[00:00:04.840 --> 00:00:08.480]   He's got a brand new Kickstarter you're going to want to know all about.
[00:00:08.480 --> 00:00:13.000]   We'll also talk about the assault on libraries from publishers.
[00:00:13.000 --> 00:00:17.720]   We'll talk about chat GPT with our other guests, Ian Thompson from the Register,
[00:00:17.720 --> 00:00:22.680]   Georgia Dow, the psychotherapist and Renee Ritchie of YouTube.
[00:00:22.680 --> 00:00:26.240]   We'll also talk about the TikTok ban and a whole lot more.
[00:00:26.240 --> 00:00:28.400]   Plus, Mr. and Mrs. Pickles.
[00:00:28.400 --> 00:00:30.400]   Have a baby.
[00:00:30.400 --> 00:00:32.400]   It's all coming up next.
[00:00:32.400 --> 00:00:33.400]   Not Twit.
[00:00:33.400 --> 00:00:38.400]   Podcasts you love.
[00:00:38.400 --> 00:00:40.400]   From people you trust.
[00:00:40.400 --> 00:00:42.400]   This is Twit.
[00:00:42.400 --> 00:00:49.400]   This is Twit.
[00:00:49.400 --> 00:00:56.400]   This week in Tech, episode 920, recorded Sunday, March 26, 2023.
[00:00:56.400 --> 00:00:59.400]   Biological boot loaders.
[00:00:59.400 --> 00:01:02.400]   This week in Tech is brought to you by ACI Learning.
[00:01:02.400 --> 00:01:08.400]   Tech is one industry where opportunities outpace growth, especially in cybersecurity.
[00:01:08.400 --> 00:01:19.400]   One-third of information security jobs require a cybersecurity certification to maintain your competitive edge across audit, IT and cybersecurity readiness.
[00:01:19.400 --> 00:01:23.400]   Visit go.acislearning.com/twit.
[00:01:23.400 --> 00:01:25.400]   And by collide.
[00:01:25.400 --> 00:01:32.400]   Collide is a device trust solution that ensures that if a device isn't secure, it can't access your apps.
[00:01:32.400 --> 00:01:34.400]   It's zero trust for Okta.
[00:01:34.400 --> 00:01:36.400]   Visit collide.com/twit.
[00:01:36.400 --> 00:01:38.400]   Book a demo today.
[00:01:38.400 --> 00:01:40.400]   And by noom.
[00:01:40.400 --> 00:01:46.400]   Stop chasing health trends and build sustainable, healthy habits with noom's psychology-based approach.
[00:01:46.400 --> 00:01:52.400]   And check out noom's first ever book, The Noom Mindset, a deep dive into the psychology of behavior change.
[00:01:52.400 --> 00:01:55.400]   Available to buy now wherever books are sold.
[00:01:55.400 --> 00:01:59.400]   And sign up for your trial at noom.com/twit.
[00:01:59.400 --> 00:02:02.400]   And by Shopify.
[00:02:02.400 --> 00:02:06.400]   Shopify makes it simple to sell to anyone from anywhere.
[00:02:06.400 --> 00:02:10.400]   This is Possibility, powered by Shopify.
[00:02:10.400 --> 00:02:14.400]   Sign up for a $1 a month trial period to get your business to the next level today.
[00:02:14.400 --> 00:02:18.400]   Visit Shopify.com/twit, all lower kids.
[00:02:21.400 --> 00:02:23.400]   It's time for Twit.
[00:02:23.400 --> 00:02:25.400]   This week at Tech the Show we cover the latest tech news.
[00:02:25.400 --> 00:02:30.400]   This time with three Canadians, two Brits and two Americans.
[00:02:30.400 --> 00:02:33.400]   Which sounds like a very large seven person panel.
[00:02:33.400 --> 00:02:35.400]   But it's just four of us.
[00:02:35.400 --> 00:02:37.400]   But if I have us but.
[00:02:37.400 --> 00:02:39.400]   We got one guy who's really throwing the averages off.
[00:02:39.400 --> 00:02:41.400]   Corey Doctorow is here.
[00:02:41.400 --> 00:02:44.400]   He is both a Canadian by birth.
[00:02:44.400 --> 00:02:47.400]   A Brit by dual passport, right?
[00:02:47.400 --> 00:02:48.400]   Yes.
[00:02:48.400 --> 00:02:50.400]   And an American by triple passport.
[00:02:50.400 --> 00:02:51.400]   He is.
[00:02:51.400 --> 00:02:54.400]   Now did you have to take the American citizenship?
[00:02:54.400 --> 00:02:56.400]   Didn't you have to renege the others?
[00:02:56.400 --> 00:02:59.400]   No, I just had to force wear loyalty to foreign potentates.
[00:02:59.400 --> 00:03:00.400]   Oh.
[00:03:00.400 --> 00:03:01.400]   Alright.
[00:03:01.400 --> 00:03:03.400]   So let's all be clear.
[00:03:03.400 --> 00:03:07.400]   He has force sworn any loyalty to foreign potentates.
[00:03:07.400 --> 00:03:10.400]   However, his loyalty to the US is maybe in a little bit in question.
[00:03:10.400 --> 00:03:11.400]   We'll get to that in a second.
[00:03:11.400 --> 00:03:14.400]   I also had to promise to carry a gun for America if need be.
[00:03:14.400 --> 00:03:18.400]   But given that I'm a 51 year old man with two artificial hips and cataracts.
[00:03:18.400 --> 00:03:20.400]   It seems unlikely I'm going to get drafted.
[00:03:20.400 --> 00:03:24.400]   To get citizenship, you have to say I would be willing to bear arms for the US.
[00:03:24.400 --> 00:03:26.400]   Oh, yes, absolutely.
[00:03:26.400 --> 00:03:28.400]   Oh, another a naturalized citizen.
[00:03:28.400 --> 00:03:29.400]   Hello, Mr.
[00:03:29.400 --> 00:03:32.400]   Naturalized citizen Ian Thompson.
[00:03:32.400 --> 00:03:33.400]   You said it.
[00:03:33.400 --> 00:03:35.400]   I'm not a considered isn't yet.
[00:03:35.400 --> 00:03:36.400]   Oh, I'm considering it.
[00:03:36.400 --> 00:03:38.400]   But Corey is absolutely right.
[00:03:38.400 --> 00:03:46.400]   When you look through the when you're going through citizenship tests, they will say you can be drafted to serve in the defense of the United States.
[00:03:46.400 --> 00:03:51.400]   And it's like my God, if there's that down on the list that comes from someone like me.
[00:03:51.400 --> 00:03:52.400]   Good luck.
[00:03:52.400 --> 00:03:53.400]   There's actually.
[00:03:53.400 --> 00:03:57.400]   I'll be able to run them at that point if they're that if they're scraping that bottom of the barrel.
[00:03:57.400 --> 00:04:00.400]   Hey, Russia did.
[00:04:00.400 --> 00:04:02.400]   It could happen.
[00:04:02.400 --> 00:04:07.400]   In fact, given your given your Slavic heritage, you too could be in strip conscripted by purposes.
[00:04:07.400 --> 00:04:12.400]   Well, so I can get a Russian passport, a Belarusian passport and an Azerbaijan passport as well.
[00:04:12.400 --> 00:04:13.400]   I have not got those.
[00:04:13.400 --> 00:04:18.400]   I have no desire to travel to countries where they've you know had an ethnic purge or anything.
[00:04:18.400 --> 00:04:21.400]   Yeah, I know what else I'd use those passports for.
[00:04:21.400 --> 00:04:22.400]   Yeah.
[00:04:22.400 --> 00:04:29.400]   Well, this was quite a problem when I was going for citizenship because my sister was born in what was then Northern Rhodesia is now Zambia.
[00:04:29.400 --> 00:04:35.400]   My father was born in what's now the capital of Pakistan, who was then the British Empire of India.
[00:04:35.400 --> 00:04:39.400]   And it was like just put British Empire of India down on the floor.
[00:04:39.400 --> 00:04:41.400]   That way they're not going to argue about it.
[00:04:41.400 --> 00:04:44.400]   Well, where's the Scotland part come in?
[00:04:44.400 --> 00:04:46.400]   Oh, my dad's always been the Scots.
[00:04:46.400 --> 00:04:47.400]   The Scots move everywhere.
[00:04:47.400 --> 00:04:49.400]   We engineer the world.
[00:04:49.400 --> 00:04:52.400]   We build San Francisco cable car system, the Golden Gate Park.
[00:04:52.400 --> 00:04:53.400]   That's right.
[00:04:53.400 --> 00:04:55.400]   You know, this is what we do.
[00:04:55.400 --> 00:05:00.400]   I haven't even gotten to the end of the line, but I do have two people who cannot be drafted by the United States, at least.
[00:05:00.400 --> 00:05:04.400]   From Morayal, Canada.
[00:05:04.400 --> 00:05:07.400]   I have Renee Ritchie, creator liaison at the YouTube.
[00:05:07.400 --> 00:05:08.400]   Hi Renee.
[00:05:08.400 --> 00:05:09.400]   Hello Leo.
[00:05:09.400 --> 00:05:10.400]   I am not a cat.
[00:05:10.400 --> 00:05:11.400]   I'm still not a cat.
[00:05:11.400 --> 00:05:12.400]   Still not a cat.
[00:05:12.400 --> 00:05:13.400]   I'm here.
[00:05:13.400 --> 00:05:18.400]   And it's wonderful to see you longtime host on for more than a decade on Macbreak Weekly.
[00:05:18.400 --> 00:05:23.400]   We miss you, but you're doing such good work stopping the swearing on YouTube.
[00:05:23.400 --> 00:05:25.400]   We really appreciate all you.
[00:05:25.400 --> 00:05:26.400]   Well, starting it again.
[00:05:26.400 --> 00:05:27.400]   Oh, yeah.
[00:05:27.400 --> 00:05:28.400]   Restarting the profanity.
[00:05:28.400 --> 00:05:29.400]   Yes.
[00:05:29.400 --> 00:05:30.400]   It's great to have you.
[00:05:30.400 --> 00:05:33.400]   And your next door neighbor is here.
[00:05:33.400 --> 00:05:35.400]   Georgia Dow.
[00:05:35.400 --> 00:05:36.400]   Hi, Georgia.
[00:05:36.400 --> 00:05:37.400]   Hello.
[00:05:37.400 --> 00:05:40.400]   YouTube star, Georgia Dow.
[00:05:40.400 --> 00:05:42.400]   Also a licensed psychotherapist.
[00:05:42.400 --> 00:05:44.400]   Should anybody feel the need?
[00:05:44.400 --> 00:05:46.400]   It's great to have you.
[00:05:46.400 --> 00:05:47.400]   Wow.
[00:05:47.400 --> 00:05:48.400]   Nice to be here.
[00:05:48.400 --> 00:05:52.400]   A lot of people here, but I want to start with Corey because he can't stay the whole show.
[00:05:52.400 --> 00:05:57.400]   But I did want to mention his new Kickstarter.
[00:05:57.400 --> 00:05:58.400]   Corey, Red Team Blues.
[00:05:58.400 --> 00:05:59.400]   What's this about?
[00:05:59.400 --> 00:06:00.400]   Yeah.
[00:06:00.400 --> 00:06:02.400]   So this is my next novel.
[00:06:02.400 --> 00:06:03.400]   It's the first book in a trilogy.
[00:06:03.400 --> 00:06:08.400]   It's about a guy called Marty Hinch, who's a 67 year old hard charging forensic accountant.
[00:06:08.400 --> 00:06:11.400]   He's been in Silicon Valley for 40 years.
[00:06:11.400 --> 00:06:12.400]   Great.
[00:06:12.400 --> 00:06:13.400]   Winding ex scams.
[00:06:13.400 --> 00:06:16.400]   And it's his last one last job.
[00:06:16.400 --> 00:06:22.400]   He gets pulled in by a friend who's a legendary cryptographer who unwisely built a backdoor for
[00:06:22.400 --> 00:06:25.400]   his cryptocurrency, which is now escaped into the wild.
[00:06:25.400 --> 00:06:28.400]   Marty has to recover the keys, which turns out to be the easy part.
[00:06:28.400 --> 00:06:34.480]   He ends up and caught between a three way war between Narcos, money launderers, and rogue
[00:06:34.480 --> 00:06:36.240]   three letter agencies.
[00:06:36.240 --> 00:06:39.240]   And what it really is, is it's like an anti finance finance novel.
[00:06:39.240 --> 00:06:46.080]   It's a novel about the kind of bitter disappointment of an internet that once held the promise
[00:06:46.080 --> 00:06:52.280]   of technical liberation and has become a place of technical control and extraction.
[00:06:52.280 --> 00:06:53.840]   And it was a fun book to write.
[00:06:53.840 --> 00:06:55.280]   I wrote it in six weeks flat.
[00:06:55.280 --> 00:06:58.080]   The last book I did like this, it was a little brother.
[00:06:58.080 --> 00:06:59.520]   I gave it to my wife.
[00:06:59.520 --> 00:07:01.320]   I rolled over at two in the morning the next night.
[00:07:01.320 --> 00:07:03.320]   She was sitting up in bed and I was like, what are you doing?
[00:07:03.320 --> 00:07:05.680]   And she said, well, I just had to find out how it ended.
[00:07:05.680 --> 00:07:09.520]   My publisher three days later bought this book in two sequels.
[00:07:09.520 --> 00:07:11.920]   They're going to run in reverse chronological order.
[00:07:11.920 --> 00:07:13.600]   So this is his last mission.
[00:07:13.600 --> 00:07:17.240]   We go all the way back to his first in the 1980s through the next two books.
[00:07:17.240 --> 00:07:19.840]   And they're swept all of the trades.
[00:07:19.840 --> 00:07:23.240]   It's got start reviews and all the trade magazines.
[00:07:23.240 --> 00:07:28.720]   But the one thing is that as with all my books, Amazon Audible will not sell the audiobook
[00:07:28.720 --> 00:07:30.520]   because there's no DRM on it.
[00:07:30.520 --> 00:07:33.920]   And so I am kickstarting a DRM free edition.
[00:07:33.920 --> 00:07:39.360]   I go into the studio tomorrow with Will Wheaton here in LA and we are going to record a really
[00:07:39.360 --> 00:07:43.240]   fantastic audiobook as has been every one of the audiobooks.
[00:07:43.240 --> 00:07:44.680]   Will's recorded with me.
[00:07:44.680 --> 00:07:49.680]   And the Kickstarter presells the audiobook, the hardcover, the ebook DRM free obviously.
[00:07:49.680 --> 00:07:54.600]   And I think in addition to being a way to fund this independent production and treat
[00:07:54.600 --> 00:07:59.520]   Amazon as damage and route around it, it's also a way to show other people that you don't
[00:07:59.520 --> 00:08:05.040]   have to submit to the bullying tactics and lock in of a monopolist just to reach your
[00:08:05.040 --> 00:08:06.040]   audience.
[00:08:06.040 --> 00:08:13.920]   I love the URL for this is kickstarter.com/project/doctorow/redteamblues another audiobook that Amazon won't
[00:08:13.920 --> 00:08:14.920]   sell.
[00:08:14.920 --> 00:08:20.200]   Although the short URL is just red team blues that'll just forage you there.
[00:08:20.200 --> 00:08:21.200]   Redteamblues.com.
[00:08:21.200 --> 00:08:23.160]   We'll just forage you there.
[00:08:23.160 --> 00:08:25.760]   But yeah, I mean Amazon won't sell my audiobook.
[00:08:25.760 --> 00:08:26.760]   So really cool.
[00:08:26.760 --> 00:08:32.520]   Let's be fair is it that Amazon won't sell it or that you won't allow them to sell it.
[00:08:32.520 --> 00:08:38.480]   Well, I went to Amazon and said you can sell my audiobooks just don't put DRM on them.
[00:08:38.480 --> 00:08:41.560]   As you promised you wouldn't when you bought Audible because they bought Audible they said
[00:08:41.560 --> 00:08:42.560]   they were getting rid of DRM.
[00:08:42.560 --> 00:08:46.360]   Audible's always had DRM.
[00:08:46.360 --> 00:08:50.280]   It's not hard to strip it off but they've always had DRM and I always just assumed it
[00:08:50.280 --> 00:08:52.080]   was the publishers that insist on it.
[00:08:52.080 --> 00:08:56.160]   There are audio bookstores that don't have a DRM on it.
[00:08:56.160 --> 00:08:57.160]   That's right.
[00:08:57.160 --> 00:09:01.240]   Well, all of them except for Audible and Apple books which is a front end for Audible allow
[00:09:01.240 --> 00:09:03.320]   you to choose whether there's DRM.
[00:09:03.320 --> 00:09:07.640]   The thing about this DRM and this kind of resolves the mystery under the Digital Millennium
[00:09:07.640 --> 00:09:10.120]   Copyright Acts section 1201.
[00:09:10.120 --> 00:09:15.240]   It is more illegal for you to remove DRM from an audiobook that I made with my blessing
[00:09:15.240 --> 00:09:17.800]   than it is for you to pirate that audiobook.
[00:09:17.800 --> 00:09:23.840]   If I give you the tool to remove DRM from my audiobook that I made and paid for I commit
[00:09:23.840 --> 00:09:29.200]   a felony punishable by a five year prison sentence and a $500,000 for a first offense.
[00:09:29.200 --> 00:09:35.080]   The maximum penalty, civil and criminal for making an infringing copy of that book is only
[00:09:35.080 --> 00:09:36.880]   $400,000.
[00:09:36.880 --> 00:09:43.680]   And so it is obvious when you put it that way why Amazon would want to make sure that they
[00:09:43.680 --> 00:09:48.040]   have DRM in all the books because it means that if someone offers me more money for my
[00:09:48.040 --> 00:09:52.000]   audiobooks to take my books off Audible and sell them somewhere else, I have to bet that
[00:09:52.000 --> 00:09:56.360]   my audience is willing to throw away all the audiobooks they've ever bought to follow
[00:09:56.360 --> 00:10:01.640]   me to a rival platform because I can't authorize them to remove the DRM and Amazon's not going
[00:10:01.640 --> 00:10:05.160]   to send me a list of all my customers so I can send them free download codes for another
[00:10:05.160 --> 00:10:06.160]   platform.
[00:10:06.160 --> 00:10:07.160]   So I'm just stuck.
[00:10:07.160 --> 00:10:08.160]   Right?
[00:10:08.160 --> 00:10:12.200]   So I've always had this position that I'm just not going to be bait in a sticky trap.
[00:10:12.200 --> 00:10:17.640]   And here we are, you know, more than a decade after Amazon purchased Audible.
[00:10:17.640 --> 00:10:20.640]   It's now 90% of the audiobook market.
[00:10:20.640 --> 00:10:24.040]   And they are engaged in rampant wage theft.
[00:10:24.040 --> 00:10:28.680]   The Audible Gates scandal accounted for at least $100 million in wage theft from independent
[00:10:28.680 --> 00:10:35.080]   audiobook creators and still not giving creators the choice about whether they want to lock
[00:10:35.080 --> 00:10:39.440]   their listeners to Amazon's platform, not just for the term of copyright, which is 90
[00:10:39.440 --> 00:10:41.960]   years on these commercial works, but forever.
[00:10:41.960 --> 00:10:48.440]   DRM, I should just explain is copy protection, digital rights management for people who don't
[00:10:48.440 --> 00:10:52.000]   want to fire up Wikipedia and look it up.
[00:10:52.000 --> 00:10:53.000]   Yeah.
[00:10:53.000 --> 00:10:54.720]   It's just an encryption scheme.
[00:10:54.720 --> 00:10:57.520]   It doesn't work very well because the way that it works is a nice rip off.
[00:10:57.520 --> 00:10:58.520]   Right.
[00:10:58.520 --> 00:10:59.520]   Yeah.
[00:10:59.520 --> 00:11:01.720]   I scramble the book and then I give you the book with a player that has the key to unscramble
[00:11:01.720 --> 00:11:04.680]   that and then I bet that you're never going to figure out where in that player I had
[00:11:04.680 --> 00:11:05.680]   the key.
[00:11:05.680 --> 00:11:08.960]   The technical term for this insecurity research is wishful thinking.
[00:11:08.960 --> 00:11:12.560]   So it's not like a thing that stops people from pirating audiobooks.
[00:11:12.560 --> 00:11:15.160]   You don't need to strip DRM off to pirate audiobooks.
[00:11:15.160 --> 00:11:21.360]   You just type book name, space, MP3, free download pirate into your favorite search engine.
[00:11:21.360 --> 00:11:22.800]   You can just get it.
[00:11:22.800 --> 00:11:28.680]   The only thing this does is prevent actual proprietors, creators and publishers from
[00:11:28.680 --> 00:11:34.120]   creating legitimate legal businesses that present alternatives to Amazon and would discipline
[00:11:34.120 --> 00:11:38.680]   Amazon and its dealings with creators and with audiences because Amazon is now sticking
[00:11:38.680 --> 00:11:41.320]   ads in the audiobooks you pay for.
[00:11:41.320 --> 00:11:45.400]   You know, which again is just, yeah, I mean, and if they can control what player you listen
[00:11:45.400 --> 00:11:48.200]   to, they can stop you from skipping ads.
[00:11:48.200 --> 00:11:54.720]   You just explained by the way, why the record industry is going after ISPs now and even worse
[00:11:54.720 --> 00:11:59.360]   went after quad nine, a DNS resolver.
[00:11:59.360 --> 00:12:04.520]   They can't stop the piracy.
[00:12:04.520 --> 00:12:07.200]   So what they want to do is stop you from finding the pirate sites.
[00:12:07.200 --> 00:12:08.200]   They actually won their case.
[00:12:08.200 --> 00:12:09.200]   I was shocked.
[00:12:09.200 --> 00:12:12.640]   They won their case against quad nine.
[00:12:12.640 --> 00:12:13.640]   Yeah.
[00:12:13.640 --> 00:12:19.080]   It's, you know, I think that the lesson of the streaming services has been that if you
[00:12:19.080 --> 00:12:22.400]   give people a reasonable offer at a reasonable price, they'll choose it.
[00:12:22.400 --> 00:12:26.040]   I think the other lesson in the streaming services is that no one ever bought a book
[00:12:26.040 --> 00:12:29.960]   or a record because they wanted an entertainment executive to make more money.
[00:12:29.960 --> 00:12:35.040]   And so, you know, if we, if we show people that there is an equitable arrangement, people
[00:12:35.040 --> 00:12:36.840]   will opt into it.
[00:12:36.840 --> 00:12:39.080]   And the people who won't, we're never going to opt into it.
[00:12:39.080 --> 00:12:42.120]   It's the, you know, I don't compete with piracy.
[00:12:42.120 --> 00:12:45.680]   I compete with all the things you could do that aren't listening to an audiobook playing
[00:12:45.680 --> 00:12:52.040]   games, you know, going for a walk in the park, zooming, looking at websites with names that
[00:12:52.040 --> 00:12:55.040]   we can't mention because it's a family friendly show.
[00:12:55.040 --> 00:12:58.000]   You know, all of those things are my competitors.
[00:12:58.000 --> 00:13:01.960]   And the piracy is like on the list of things that that caused people not to listen to my
[00:13:01.960 --> 00:13:09.440]   books, you know, piracy is way, way down there in terms of what I compete with.
[00:13:09.440 --> 00:13:12.960]   And yet publishers are working hard.
[00:13:12.960 --> 00:13:15.240]   Well now to put libraries out of existence.
[00:13:15.240 --> 00:13:17.400]   See, we talked last week.
[00:13:17.400 --> 00:13:23.680]   Yeah, we talked last week about internet archive, which lost its case this week in,
[00:13:23.680 --> 00:13:28.680]   in a lower court ruling for what it calls, what is it?
[00:13:28.680 --> 00:13:31.680]   Cloud library distribution.
[00:13:31.680 --> 00:13:34.960]   It's, it's controlled digital lending.
[00:13:34.960 --> 00:13:35.960]   Digital lending.
[00:13:35.960 --> 00:13:36.960]   CDL.
[00:13:36.960 --> 00:13:37.960]   Yeah.
[00:13:37.960 --> 00:13:38.960]   CDL.
[00:13:38.960 --> 00:13:39.960]   Yeah.
[00:13:39.960 --> 00:13:42.320]   So let me make sure I understand what was happening.
[00:13:42.320 --> 00:13:47.640]   So the internet archive, which is the way back machine, it's archive.org.
[00:13:47.640 --> 00:13:52.560]   It's really the library of the internet and a really valuable resource for everybody.
[00:13:52.560 --> 00:13:57.800]   In fact, they actually bought and housed themselves in a old library building in San
[00:13:57.800 --> 00:14:00.160]   Francisco, near Golden Gate Park.
[00:14:00.160 --> 00:14:01.160]   This is beautiful.
[00:14:01.160 --> 00:14:02.160]   So they, a church actually.
[00:14:02.160 --> 00:14:04.480]   It's a church, but it's got columns.
[00:14:04.480 --> 00:14:08.080]   It looks like, I don't know, I imagine the library of Alexandria must have looked like
[00:14:08.080 --> 00:14:09.080]   beautiful.
[00:14:09.080 --> 00:14:10.080]   Yeah.
[00:14:10.080 --> 00:14:11.080]   I guess it's a church.
[00:14:11.080 --> 00:14:12.080]   Yeah, that makes sense.
[00:14:12.080 --> 00:14:13.080]   Yeah.
[00:14:13.080 --> 00:14:21.080]   But the, they were taking books, often donated by libraries, old books, that the library was
[00:14:21.080 --> 00:14:32.400]   going to replace and digitizing them, making ebooks out of the paper books and then lending
[00:14:32.400 --> 00:14:36.880]   them out and, and lending them out just as a library does where they, it's like they
[00:14:36.880 --> 00:14:41.280]   have one copy and you can't, two people can't check out the same copy.
[00:14:41.280 --> 00:14:44.360]   So that's how libraries do it as well.
[00:14:44.360 --> 00:14:47.880]   But the publishers have sued.
[00:14:47.880 --> 00:14:52.600]   Now, are they suing over that or was it the fact that they relaxed restrictions and COVID?
[00:14:52.600 --> 00:14:54.360]   No, that's what they're suing over.
[00:14:54.360 --> 00:14:55.840]   It's not about the relaxed restrictions.
[00:14:55.840 --> 00:14:58.920]   I have to be really careful here because I work with Electronic Frontier Foundation
[00:14:58.920 --> 00:15:00.920]   and we are counsel for the archive.
[00:15:00.920 --> 00:15:05.360]   I want to be really clear that anything I say about this case is me speaking personally
[00:15:05.360 --> 00:15:07.720]   and not speaking on behalf of the FF.
[00:15:07.720 --> 00:15:12.080]   I'm a contractor to them, not an employee, but I'm still part of the organization.
[00:15:12.080 --> 00:15:16.440]   So, I mean, it's a very nuanced question about copyright law.
[00:15:16.440 --> 00:15:23.320]   But for me, there's this like very bottom line idea, which is that libraries are really old.
[00:15:23.320 --> 00:15:26.280]   Libraries are not just older than copyright or publishing.
[00:15:26.280 --> 00:15:28.880]   They're older than books and paper, right?
[00:15:28.880 --> 00:15:34.600]   Libraries have been around since we had scrolls of papyrus and they're older than commerce
[00:15:34.600 --> 00:15:36.480]   as we understand it.
[00:15:36.480 --> 00:15:43.440]   And the idea that we have made a technological change, not more profound than the technological
[00:15:43.440 --> 00:15:49.840]   changes we made when we went from papyrus to paper or scrolls to codexes or, you know,
[00:15:49.840 --> 00:15:54.400]   from books copied by scribes to books that were printed, that the idea that somehow digital
[00:15:54.400 --> 00:15:58.600]   is so profoundly different that we just say, "Okay, well, you can lend out books that are
[00:15:58.600 --> 00:16:00.360]   copied by monks.
[00:16:00.360 --> 00:16:04.800]   You can't lend out books that are represented on hard drives because we are just in a new
[00:16:04.800 --> 00:16:05.800]   world."
[00:16:05.800 --> 00:16:12.840]   To me, that is, it's not just ghastly in terms of how it makes me feel about a future for
[00:16:12.840 --> 00:16:18.240]   my family as a writer and an artist, but also as someone who believes in public goods and
[00:16:18.240 --> 00:16:20.320]   access to human knowledge.
[00:16:20.320 --> 00:16:24.440]   I fear for it as someone who cares about books because, you know, one of the things about
[00:16:24.440 --> 00:16:29.680]   books is that they do have this kind of panumbra of antiquity and virtue where like if you're
[00:16:29.680 --> 00:16:33.160]   making a dumb student film and you want to show societies collapse, you just put a pile
[00:16:33.160 --> 00:16:36.120]   of books together and you set fire to them and people are like, "Oh yeah, it's like eating
[00:16:36.120 --> 00:16:37.120]   dogs or something.
[00:16:37.120 --> 00:16:38.120]   That's like wrong.
[00:16:38.120 --> 00:16:39.640]   We know everything has gone wrong."
[00:16:39.640 --> 00:16:44.000]   And if you just convince people that books are just like another widget that they're like
[00:16:44.000 --> 00:16:50.440]   a happy meal toy, people might in fact take you at your word and like stop buying books
[00:16:50.440 --> 00:16:54.800]   just so they can be surrounded by them and stop giving people books as a way of showing
[00:16:54.800 --> 00:17:00.720]   them how special they are to them and stop thinking about books in this kind of wonderful
[00:17:00.720 --> 00:17:05.480]   way that is such a source of revenue to writers and publishers.
[00:17:05.480 --> 00:17:10.160]   You know, if I have a healthy retirement, it'll be as much because of that as because of anything
[00:17:10.160 --> 00:17:11.880]   I ever wrote.
[00:17:11.880 --> 00:17:17.200]   And I really worry that you have this idea that we should just treat publishing like
[00:17:17.200 --> 00:17:24.040]   another business, like we could treat your puppy as just another source of protein.
[00:17:24.040 --> 00:17:31.240]   And if we do that, I think the worst thing that could possibly happen is that we'd succeed.
[00:17:31.240 --> 00:17:35.880]   Lydia Holland of Fight for the Future said, "In a chilling ruling, a lower court judge
[00:17:35.880 --> 00:17:40.520]   in New York has completely disregarded the traditional rights of libraries to own and
[00:17:40.520 --> 00:17:45.840]   preserve books in favor of maximizing the profits of big media conglomerates."
[00:17:45.840 --> 00:17:52.640]   And Tech Dirt, I think it probably was Mike Masnick wrote that this is really just a
[00:17:52.640 --> 00:17:53.840]   straw man.
[00:17:53.840 --> 00:17:57.200]   The publishers have wanted to get rid of libraries all along.
[00:17:57.200 --> 00:18:02.400]   In this case, it's just one step forward in their goal.
[00:18:02.400 --> 00:18:03.600]   Of course, the Internet art...
[00:18:03.600 --> 00:18:04.920]   So the library today.
[00:18:04.920 --> 00:18:07.400]   You couldn't start a library today, no way.
[00:18:07.400 --> 00:18:11.440]   If libraries didn't exist and you tried to found one, they'd be like, "We can't do
[00:18:11.440 --> 00:18:12.440]   that.
[00:18:12.440 --> 00:18:13.440]   Shut up, Connie.
[00:18:13.440 --> 00:18:14.440]   You can't do that.
[00:18:14.440 --> 00:18:15.440]   Give away.
[00:18:15.440 --> 00:18:16.440]   What Connie came up with this idea.
[00:18:16.440 --> 00:18:17.440]   Thomas Jefferson.
[00:18:17.440 --> 00:18:18.440]   Benjamin Franklin.
[00:18:18.440 --> 00:18:19.440]   Benjamin Franklin.
[00:18:19.440 --> 00:18:22.800]   What the hell was wrong with him?
[00:18:22.800 --> 00:18:24.920]   They will appeal in an archive, Brister K.
[00:18:24.920 --> 00:18:26.400]   They will appeal, of course.
[00:18:26.400 --> 00:18:29.360]   And I imagine this is going to the Supreme Court.
[00:18:29.360 --> 00:18:33.760]   Unfortunately, I don't have high hopes to the Supreme Court understanding the risks here.
[00:18:33.760 --> 00:18:34.760]   Maybe they would.
[00:18:34.760 --> 00:18:35.760]   Maybe they would.
[00:18:35.760 --> 00:18:36.760]   Yeah.
[00:18:36.760 --> 00:18:39.440]   I mean, the dirty secret about Ruth Bader Ginsburg is as good as she was on other issues.
[00:18:39.440 --> 00:18:41.640]   She wasn't good on copyright.
[00:18:41.640 --> 00:18:42.640]   Yeah.
[00:18:42.640 --> 00:18:47.200]   You know, copyright's a weird issue that people split in a million ways on.
[00:18:47.200 --> 00:18:51.680]   And you know, I think that like, as much as there are elements of copyright that I depend
[00:18:51.680 --> 00:18:54.880]   on for my living, there's parts of it that are structured so badly.
[00:18:54.880 --> 00:18:56.920]   That they actually get in the way, right?
[00:18:56.920 --> 00:19:01.000]   Like this, like the fact that we have a copyright law that says that Amazon can put DRM in
[00:19:01.000 --> 00:19:05.000]   my books and then I can't authorize you to remove it is a problem with copyright.
[00:19:05.000 --> 00:19:06.000]   Right.
[00:19:06.000 --> 00:19:08.560]   And you know, it's like, it's, you know, when people say don't you like copyright, you have
[00:19:08.560 --> 00:19:09.920]   to be really specific.
[00:19:09.920 --> 00:19:13.080]   Like what part of copyright are you asking me about?
[00:19:13.080 --> 00:19:17.800]   Well, you've always been, and it's really great because you're an author.
[00:19:17.800 --> 00:19:19.680]   You make your living off of writing.
[00:19:19.680 --> 00:19:24.720]   You've always been very clear that you don't need that to make a good living off of writing.
[00:19:24.720 --> 00:19:29.000]   And that you're happy to offer your books through Creative Commons and even free downloads
[00:19:29.000 --> 00:19:30.000]   on your website.
[00:19:30.000 --> 00:19:31.440]   I've done that too.
[00:19:31.440 --> 00:19:36.200]   But also like, if you told me that my copyright would endure for my life in 50 years instead
[00:19:36.200 --> 00:19:39.840]   of my life in 90 years, I wouldn't write fewer books.
[00:19:39.840 --> 00:19:45.680]   And you know, I was just in fact negotiating a contract with someone to do some work with
[00:19:45.680 --> 00:19:47.080]   my work.
[00:19:47.080 --> 00:19:52.960]   And we had this question about whether certain uses would be reserved to them if I wanted
[00:19:52.960 --> 00:19:54.920]   to repurpose it in the future.
[00:19:54.920 --> 00:19:59.120]   And I said, like, this is fine except that what we're ultimately talking about is that
[00:19:59.120 --> 00:20:04.680]   if I give my copyrights over to a library on my death, your grandchildren could sue that
[00:20:04.680 --> 00:20:06.920]   library over how it chooses to preserve it.
[00:20:06.920 --> 00:20:07.920]   Right.
[00:20:07.920 --> 00:20:12.640]   That is like, and that's a giant X factor that plays up in many ways, right?
[00:20:12.640 --> 00:20:17.880]   We had Stephen Joyce, the grand son of James Joyce, refusing to allow scholars to research
[00:20:17.880 --> 00:20:23.480]   Joyce's work because he didn't like how they characterized Joyce's work and Joyce scholarship
[00:20:23.480 --> 00:20:25.040]   languished for years.
[00:20:25.040 --> 00:20:31.480]   You had Warner music conjuring up fanciful stories out of the distant past to say that
[00:20:31.480 --> 00:20:37.760]   they own the lyrics to Happy Birthday and shaking down restaurants and movies and all
[00:20:37.760 --> 00:20:40.800]   kinds of places if they tried to sing Happy Birthday in public.
[00:20:40.800 --> 00:20:44.360]   That's why when you go to like TGI Fridays, they had these weird birthday songs and like,
[00:20:44.360 --> 00:20:45.600]   Happy Birthday, Happy Birthday, Happy Birthday.
[00:20:45.600 --> 00:20:49.640]   It was like they didn't want to pay Dangel to to Warners, right?
[00:20:49.640 --> 00:20:53.240]   So you know, like there are lots of elements of this, you know, a little goes a long way
[00:20:53.240 --> 00:20:58.320]   and we need to attend to the actual like distributional outcomes, right?
[00:20:58.320 --> 00:21:01.640]   When you when you tweak copyright in this way, who doesn't make richer and who doesn't
[00:21:01.640 --> 00:21:03.240]   make poorer?
[00:21:03.240 --> 00:21:08.520]   And I think a copyright that benefits artists is something I can get behind.
[00:21:08.520 --> 00:21:14.040]   But one that moves most of that benefit to say intermediaries like library or like publishers
[00:21:14.040 --> 00:21:17.600]   or distributors or digital platforms.
[00:21:17.600 --> 00:21:20.240]   I don't think that's that's serving any kind of artistic purpose.
[00:21:20.240 --> 00:21:23.800]   I think that's just where I'm seeking.
[00:21:23.800 --> 00:21:28.920]   We've spoken a lot about Corey's fantastic blog post on in shirtification, which is what
[00:21:28.920 --> 00:21:31.120]   I call it.
[00:21:31.120 --> 00:21:32.640]   It's this is this ties into that.
[00:21:32.640 --> 00:21:35.880]   I also want to send people to the website battle for libraries.com.
[00:21:35.880 --> 00:21:41.440]   This is a site where you could sign a petition and find out more and you really should find
[00:21:41.440 --> 00:21:42.440]   out more.
[00:21:42.440 --> 00:21:47.920]   I mean, look, we want to save not just the Internet archive, which by the way, all by
[00:21:47.920 --> 00:21:53.480]   itself is well worth saving, but every library in America.
[00:21:53.480 --> 00:21:57.840]   So I can't speak for other participants, but I've got to say, I would not be in the
[00:21:57.840 --> 00:22:00.440]   career I'm in if it wasn't from our public library.
[00:22:00.440 --> 00:22:01.440]   Yeah.
[00:22:01.440 --> 00:22:03.720]   You know, I spent the entire summers in there.
[00:22:03.720 --> 00:22:09.960]   I went when I took my wife back to the UK, we went to my old library just to smell the
[00:22:09.960 --> 00:22:13.360]   books and enjoy the feeling.
[00:22:13.360 --> 00:22:18.760]   They're still running the same literacy courses that I took, which starting me on the road
[00:22:18.760 --> 00:22:19.760]   to being a journalist.
[00:22:19.760 --> 00:22:22.480]   They're an incredibly important resource.
[00:22:22.480 --> 00:22:23.640]   Yeah.
[00:22:23.640 --> 00:22:28.160]   The last institution that in every town where you're valued because you're a person and
[00:22:28.160 --> 00:22:31.160]   not because you've got something in your wallet, it's the last place.
[00:22:31.160 --> 00:22:32.160]   Yeah.
[00:22:32.160 --> 00:22:35.120]   Well, you do have something in your wallet, your library card, but that's that's worth
[00:22:35.120 --> 00:22:36.120]   giving.
[00:22:36.120 --> 00:22:37.120]   If you don't have one, they'll give you one.
[00:22:37.120 --> 00:22:38.120]   They'll give you.
[00:22:38.120 --> 00:22:39.120]   They'll give it to you.
[00:22:39.120 --> 00:22:40.120]   Right.
[00:22:40.120 --> 00:22:41.920]   They'll put it in your wallet for you.
[00:22:41.920 --> 00:22:46.760]   Corey's book is right now on Kickstarter.
[00:22:46.760 --> 00:22:50.800]   This would be a very good time to sign up for Red Team Blues.
[00:22:50.800 --> 00:22:52.360]   Another audio book that Amazon wants.
[00:22:52.360 --> 00:22:55.400]   Comes out from Macmillan on the 25th.
[00:22:55.400 --> 00:22:56.400]   Nice.
[00:22:56.400 --> 00:23:00.560]   We'll be touring the US, the UK, Canada and Germany.
[00:23:00.560 --> 00:23:04.640]   And I could see Brad Pitt as Marty Hinch forensic CPA.
[00:23:04.640 --> 00:23:05.640]   Yeah.
[00:23:05.640 --> 00:23:07.880]   It's the story of a man in his coin, Leo.
[00:23:07.880 --> 00:23:08.880]   Easily.
[00:23:08.880 --> 00:23:10.880]   A man in his coin.
[00:23:10.880 --> 00:23:14.880]   They're going to option this one, especially this is the new James Bond.
[00:23:14.880 --> 00:23:18.480]   This is going to be Albert Broccoli is going to be on the phone any day now.
[00:23:18.480 --> 00:23:19.480]   I can tell you right now.
[00:23:19.480 --> 00:23:20.480]   The other kind of bond.
[00:23:20.480 --> 00:23:21.480]   The financial bond.
[00:23:21.480 --> 00:23:23.520]   I would settle for a lesser zucchini.
[00:23:23.520 --> 00:23:24.920]   I don't need the broccoli.
[00:23:24.920 --> 00:23:25.920]   It's a good one.
[00:23:25.920 --> 00:23:28.280]   Corey, I'll let you head to the airport.
[00:23:28.280 --> 00:23:29.280]   Thank you for being here.
[00:23:29.280 --> 00:23:30.280]   Thank you.
[00:23:30.280 --> 00:23:34.080]   Another time when you got some more time, I'd love to get back on about SVP.
[00:23:34.080 --> 00:23:35.080]   What's the love that?
[00:23:35.080 --> 00:23:36.080]   The financialization thing.
[00:23:36.080 --> 00:23:42.880]   I don't know how you live without heartburn.
[00:23:42.880 --> 00:23:47.120]   Because everything is so outrageous that's going on.
[00:23:47.120 --> 00:23:49.040]   This is just another outrage.
[00:23:49.040 --> 00:23:50.520]   I write when I'm anxious.
[00:23:50.520 --> 00:23:52.520]   I have seven books in production.
[00:23:52.520 --> 00:23:57.120]   That would be awesome chances to show up on your show and talk about them.
[00:23:57.120 --> 00:24:01.080]   Brandon said he's more than you write because you're anxious.
[00:24:01.080 --> 00:24:02.080]   There you go.
[00:24:02.080 --> 00:24:03.080]   Yep.
[00:24:03.080 --> 00:24:04.080]   All right.
[00:24:04.080 --> 00:24:05.080]   I'll talk to you later, guys.
[00:24:05.080 --> 00:24:06.080]   I really appreciate it.
[00:24:06.080 --> 00:24:09.080]   Redteamblues.com, pluralistic.net for his blog.
[00:24:09.080 --> 00:24:10.080]   Well worth it.
[00:24:10.080 --> 00:24:13.840]   Marty Hensch knows where Silicon Valley's bodies are buried.
[00:24:13.840 --> 00:24:16.280]   By the way, it's just a great page.
[00:24:16.280 --> 00:24:20.520]   If you go to the Kickstarter, you can see Will Wheaton, who's reading the book Tomorrow
[00:24:20.520 --> 00:24:21.840]   as you saw.
[00:24:21.840 --> 00:24:23.240]   You can see the people producing it.
[00:24:23.240 --> 00:24:27.080]   That is a lot of great stuff on this page.
[00:24:27.080 --> 00:24:29.360]   He's reaching his goal already.
[00:24:29.360 --> 00:24:30.360]   He wants 15,000.
[00:24:30.360 --> 00:24:32.440]   He gets got 83,000.
[00:24:32.440 --> 00:24:41.320]   But there's no reason for you not to join the 1,961 backers and get yourself a copy.
[00:24:41.320 --> 00:24:43.240]   It's always great to have Corey on.
[00:24:43.240 --> 00:24:46.760]   We also know that Corey talks a lot.
[00:24:46.760 --> 00:24:52.360]   I wanted to make sure that we had time for Renee Ritchie, Georgia Dow, and Ian Thompson.
[00:24:52.360 --> 00:24:59.240]   I've done my penance for allowing Jason Callicannis on the show a couple of weeks ago.
[00:24:59.240 --> 00:25:01.880]   Now I'm back in God's Good Graces.
[00:25:01.880 --> 00:25:02.880]   We're going to take a little break.
[00:25:02.880 --> 00:25:03.880]   Speaking old caps.
[00:25:03.880 --> 00:25:04.880]   Yeah.
[00:25:04.880 --> 00:25:06.960]   It was right after that, right?
[00:25:06.960 --> 00:25:07.960]   Ah.
[00:25:07.960 --> 00:25:08.960]   I promise.
[00:25:08.960 --> 00:25:09.960]   It's a full-time.
[00:25:09.960 --> 00:25:13.440]   I love him as a person.
[00:25:13.440 --> 00:25:21.440]   And I understand why he's kind of become villainized as the personification of the evil angel investor.
[00:25:21.440 --> 00:25:25.440]   He was the first person ever to DM me on Twitter, I think, because we were on the show together.
[00:25:25.440 --> 00:25:26.440]   Yeah.
[00:25:26.440 --> 00:25:27.440]   Years and years ago.
[00:25:27.440 --> 00:25:32.640]   He's a great person to have on the show because he's a character and he's funny and he's interesting
[00:25:32.640 --> 00:25:35.400]   and he's got a provocative personality.
[00:25:35.400 --> 00:25:38.560]   It's all an upper case no matter what, even on the podcast.
[00:25:38.560 --> 00:25:41.400]   I mean, he's a lovely chap personally.
[00:25:41.400 --> 00:25:44.160]   I've done his podcast a couple of times.
[00:25:44.160 --> 00:25:46.520]   But it was kind of special pleading.
[00:25:46.520 --> 00:25:47.520]   Oh, of course.
[00:25:47.520 --> 00:25:51.160]   You're going to stand up and be entrepreneurs, then stand up and be entrepreneurs.
[00:25:51.160 --> 00:25:53.120]   Don't ask the government to bail you out.
[00:25:53.120 --> 00:26:00.600]   Well, and I think Corey had a lot to say about this, but it's, you know, it's when it's
[00:26:00.600 --> 00:26:06.720]   a bailing the people out at socialism, when it's bailing the companies out, it's capitalism.
[00:26:06.720 --> 00:26:07.720]   Yeah.
[00:26:07.720 --> 00:26:10.120]   And there's no libertarians in a bank crisis.
[00:26:10.120 --> 00:26:11.120]   It's okay.
[00:26:11.120 --> 00:26:12.120]   Yeah.
[00:26:12.120 --> 00:26:13.120]   That's exactly right.
[00:26:13.120 --> 00:26:14.120]   That's exactly right.
[00:26:14.120 --> 00:26:15.120]   Let's take a little break.
[00:26:15.120 --> 00:26:16.720]   We'll come back with the rest of the story.
[00:26:16.720 --> 00:26:21.800]   In fact, there's a lot of news, which we will get to with a fantastic panel today.
[00:26:21.800 --> 00:26:25.720]   A bunch of Canadians and a Brit.
[00:26:25.720 --> 00:26:28.200]   Georgia Dow, everyone commonwealth games.
[00:26:28.200 --> 00:26:29.200]   Renee Richie.
[00:26:29.200 --> 00:26:31.120]   Yeah, it's the commonwealth games right here.
[00:26:31.120 --> 00:26:33.480]   Only you hadn't turned away the Queen, honestly.
[00:26:33.480 --> 00:26:34.480]   It's for shameful.
[00:26:34.480 --> 00:26:35.480]   But man's.
[00:26:35.480 --> 00:26:41.400]   And Ian Thompson, I'm listening to spare right now.
[00:26:41.400 --> 00:26:49.240]   And you know, at first I said, I don't want to give any money to Harry, you know, because
[00:26:49.240 --> 00:26:52.640]   it's just, it's all ginned up soap opera anyway.
[00:26:52.640 --> 00:26:54.800]   But then I thought, well, here's a rare opportunity.
[00:26:54.800 --> 00:26:59.520]   Most of the time when a royal biography or autobiography comes out, it's, you know, it's
[00:26:59.520 --> 00:27:00.520]   hate geography.
[00:27:00.520 --> 00:27:03.560]   It's, it's, you know, it's not real, right?
[00:27:03.560 --> 00:27:06.840]   You know, I wouldn't expect Prince Charles to write the true story of what it's like to
[00:27:06.840 --> 00:27:07.840]   be Prince Charles.
[00:27:07.840 --> 00:27:11.280]   But Prince Harry is kind of on the outs.
[00:27:11.280 --> 00:27:12.880]   I think he's ready to tell the truth.
[00:27:12.880 --> 00:27:16.840]   So I thought I want to hear what it's like to be more boring.
[00:27:16.840 --> 00:27:18.760]   I went to in and out for the third time this week.
[00:27:18.760 --> 00:27:21.800]   There's a little bit of this at one point.
[00:27:21.800 --> 00:27:24.360]   He's, his penis is frost, but I thought that was interesting.
[00:27:24.360 --> 00:27:26.960]   That's not something King Charles is going to mention.
[00:27:26.960 --> 00:27:31.560]   If that happened during his helicopter tour in Canada, then that's just a typical deal.
[00:27:31.560 --> 00:27:34.880]   He actually was, he actually was on his way to the North Pole.
[00:27:34.880 --> 00:27:36.160]   And then he did the South Pole.
[00:27:36.160 --> 00:27:41.000]   He's actually a very interesting fellow, but he makes a very strong case against the British
[00:27:41.000 --> 00:27:43.640]   tabloid press.
[00:27:43.640 --> 00:27:47.800]   You know, he believes in, I think it's the case that his mother was killed by them by
[00:27:47.800 --> 00:27:50.720]   paparazzi, chasing her into a tunnel.
[00:27:50.720 --> 00:27:57.240]   And the reason he and Meghan Markle left the UK was because they couldn't lead a normal
[00:27:57.240 --> 00:27:58.800]   life.
[00:27:58.800 --> 00:28:00.560]   And so I thought it was interesting to read that.
[00:28:00.560 --> 00:28:02.960]   I wanted to see what it was like.
[00:28:02.960 --> 00:28:10.240]   And now I've got to, I'm tempted to look at it because, you know, it is an inside story.
[00:28:10.240 --> 00:28:14.760]   On the Diana thing, Occam's Razor, you know, one person survived that car crash and it's
[00:28:14.760 --> 00:28:16.440]   the one person was wearing a seat belt.
[00:28:16.440 --> 00:28:17.440]   Absolutely.
[00:28:17.440 --> 00:28:18.440]   The seat belts would have saved him.
[00:28:18.440 --> 00:28:19.600]   That's not a question.
[00:28:19.600 --> 00:28:22.680]   But the reason they were speeding down that Parisian tunnel was because they were trying
[00:28:22.680 --> 00:28:23.920]   to get away from paparazzi.
[00:28:23.920 --> 00:28:24.920]   Yeah.
[00:28:24.920 --> 00:28:25.920]   Yeah.
[00:28:25.920 --> 00:28:26.920]   On motorbikes.
[00:28:26.920 --> 00:28:30.960]   And if you looked at Harry's face at the actual funeral procession for her, the look
[00:28:30.960 --> 00:28:36.640]   he gave the press, it was utter loathing and I can quite understand why.
[00:28:36.640 --> 00:28:37.640]   Yeah.
[00:28:37.640 --> 00:28:39.400]   Because yes, they did cause the incident.
[00:28:39.400 --> 00:28:40.400]   Yeah.
[00:28:40.400 --> 00:28:45.800]   And he calls Rupert Murdoch one of the most vile influences on the world in history.
[00:28:45.800 --> 00:28:47.760]   And I think he may not be far wrong.
[00:28:47.760 --> 00:28:50.960]   So there's some interesting spot on.
[00:28:50.960 --> 00:28:51.960]   So there's some interesting.
[00:28:51.960 --> 00:28:55.760]   So he's got engaged recently this week for his fourth time.
[00:28:55.760 --> 00:28:56.760]   Yeah.
[00:28:56.760 --> 00:28:57.760]   92 years old.
[00:28:57.760 --> 00:28:59.760]   And it's what I liked his line.
[00:28:59.760 --> 00:29:01.560]   What first attracted you to Billy?
[00:29:01.560 --> 00:29:07.000]   I love the line though is his new wife is in her seventies.
[00:29:07.000 --> 00:29:11.320]   The line Rupert Murdoch said is, I wanted somebody to spend the second half of my life
[00:29:11.320 --> 00:29:12.480]   with.
[00:29:12.480 --> 00:29:15.200]   Is he living to 184?
[00:29:15.200 --> 00:29:17.080]   What is the plan here?
[00:29:17.080 --> 00:29:19.560]   Anyway, that I don't.
[00:29:19.560 --> 00:29:21.520]   Honestly, he obviously loved it.
[00:29:21.520 --> 00:29:22.520]   Yeah.
[00:29:22.520 --> 00:29:23.520]   God bless him.
[00:29:23.520 --> 00:29:24.520]   That's fine.
[00:29:24.520 --> 00:29:25.520]   I don't have a problem with that.
[00:29:25.520 --> 00:29:33.000]   Anyway, we will, we will continue our conversation about the royals and everything else.
[00:29:33.000 --> 00:29:35.760]   Ian Thompson from the register, ladies and gentlemen.
[00:29:35.760 --> 00:29:40.120]   But first, let's talk to you about the people who sponsor this fine studio, the great folks
[00:29:40.120 --> 00:29:41.920]   at ACI learning.
[00:29:41.920 --> 00:29:44.240]   You may say, well, who are they?
[00:29:44.240 --> 00:29:46.560]   You know them for the last decade.
[00:29:46.560 --> 00:29:52.920]   Our partners at IT Pro have been so great in bringing you engaging and entertaining IT
[00:29:52.920 --> 00:29:57.520]   training to level up your career to bring your organization up to snuff.
[00:29:57.520 --> 00:30:02.040]   Well, now IT Pro is part of ACI learning.
[00:30:02.040 --> 00:30:09.120]   And by partnering with ACI learning, they are bringing you an incredibly expanded reach,
[00:30:09.120 --> 00:30:15.400]   new production capabilities, brand new content, and a chance to learn in a mode and a style
[00:30:15.400 --> 00:30:19.880]   that fits your personality and your stage of development.
[00:30:19.880 --> 00:30:24.840]   With IT Pro and they're on demand courses, there's audit Pro, there's practice labs.
[00:30:24.840 --> 00:30:27.480]   There's even learning hubs with ACI learning.
[00:30:27.480 --> 00:30:32.360]   So you can go in and learn in person when you can mix and match as well.
[00:30:32.360 --> 00:30:35.840]   Whether you're at the very beginning of a career or looking to move up in your sector,
[00:30:35.840 --> 00:30:41.560]   ACI learning is here to support your growth and not just in IT, but in audit readiness,
[00:30:41.560 --> 00:30:43.440]   as I said, but also in cybersecurity.
[00:30:43.440 --> 00:30:48.040]   You know, one of the best known certs, the cert that almost everybody getting into IT
[00:30:48.040 --> 00:30:52.040]   gets is that CompTIA A+ certification.
[00:30:52.040 --> 00:30:56.080]   The CompTIA courses from ACI learning are amazing.
[00:30:56.080 --> 00:31:00.880]   They make it easy to go from daydreaming about a career in IT to launching it.
[00:31:00.880 --> 00:31:06.400]   The key here is earning certificates open doors to most entry level IT positions.
[00:31:06.400 --> 00:31:10.960]   You need the cert because otherwise they go, "Well, I've been fixing computers for years,
[00:31:10.960 --> 00:31:13.240]   but how do I know you've got the knowledge?"
[00:31:13.240 --> 00:31:14.240]   Well, I got an A+.
[00:31:14.240 --> 00:31:15.400]   Oh, okay.
[00:31:15.400 --> 00:31:19.560]   You took the time, the trouble, to study, to take the test, and I know you've got the
[00:31:19.560 --> 00:31:21.400]   knowledge you're hired.
[00:31:21.400 --> 00:31:23.960]   It's really that easy.
[00:31:23.960 --> 00:31:27.800]   But not only does it get you in the entry level positions, but those certs and later
[00:31:27.800 --> 00:31:34.840]   certs get you promotions, allow you to move laterally into a new area of IT.
[00:31:34.840 --> 00:31:37.720]   IT is great because it's constantly changing.
[00:31:37.720 --> 00:31:42.360]   Tech is one industry where opportunities out pace growth and that's especially true in
[00:31:42.360 --> 00:31:43.360]   cybersecurity.
[00:31:43.360 --> 00:31:48.640]   LinkedIn just did a study that predicted that IT jobs would be the most in demand roles
[00:31:48.640 --> 00:31:49.800]   in 2023.
[00:31:49.800 --> 00:31:51.560]   I'm not surprised.
[00:31:51.560 --> 00:31:57.240]   But a third of information security jobs require a cybersecurity cert.
[00:31:57.240 --> 00:32:00.160]   23% of all IT jobs require a cert.
[00:32:00.160 --> 00:32:04.960]   While organizations are hungry for cybersecurity talent, there is a huge gap, more than a million
[00:32:04.960 --> 00:32:08.120]   unfilled jobs this year alone.
[00:32:08.120 --> 00:32:14.440]   That's probably why the average salary for cybersecurity specialists, $116,000.
[00:32:14.440 --> 00:32:19.680]   ACI Learning's Information Security Analyst and Cybersecurity Specialist Programs are
[00:32:19.680 --> 00:32:24.440]   two tracks that can help get you certified and get you a great job.
[00:32:24.440 --> 00:32:28.080]   And I was talking about that gap last year.
[00:32:28.080 --> 00:32:32.000]   The gap between the positions that needed to be filled and the number of people to fill
[00:32:32.000 --> 00:32:36.240]   them increased by 26.2% compared to 2021.
[00:32:36.240 --> 00:32:40.320]   And that means there's a lot of open jobs and a lot of opportunity for you.
[00:32:40.320 --> 00:32:41.320]   All you need is that cert.
[00:32:41.320 --> 00:32:47.280]   ACI Learning offers multiple cybersecurity training programs to prepare you to enter or advance
[00:32:47.280 --> 00:32:49.280]   within this exciting industry.
[00:32:49.280 --> 00:32:54.320]   CISSP, EC Council Certified Ethical Hacker, Certified Network Defender.
[00:32:54.320 --> 00:32:56.440]   You've got cybersecurity audit school.
[00:32:56.440 --> 00:32:58.000]   That's getting bigger and bigger all the time.
[00:32:58.000 --> 00:33:02.660]   Companies have to prove that they're doing the right thing in security, cybersecurity
[00:33:02.660 --> 00:33:03.660]   frameworks.
[00:33:03.660 --> 00:33:06.800]   By the way, when I say companies, this is important.
[00:33:06.800 --> 00:33:12.280]   They have great plans for businesses that want to upskill their IT departments.
[00:33:12.280 --> 00:33:16.080]   And I bet you any business, if you're running a business and you're thinking about this,
[00:33:16.080 --> 00:33:19.280]   you know you need some cybersecurity experts.
[00:33:19.280 --> 00:33:24.760]   Why hire out when you could give ITPro from ACI learning to your team?
[00:33:24.760 --> 00:33:28.880]   And they could upskill and now they have the certs and the skills you need to protect
[00:33:28.880 --> 00:33:30.480]   your business.
[00:33:30.480 --> 00:33:32.280]   It's that easy.
[00:33:32.280 --> 00:33:36.000]   Where and how you learn matters, ACI Learning offers fully customizable training for all
[00:33:36.000 --> 00:33:39.640]   types of learners in person on demand, remote.
[00:33:39.640 --> 00:33:44.560]   You could take your learning beyond the classroom with their labs, explore everything ACI Learning
[00:33:44.560 --> 00:33:45.560]   offers.
[00:33:45.560 --> 00:33:50.800]   ITPro, auditPro that includes enterprise solutions, webinars, the skeptical auditor podcast, practice
[00:33:50.800 --> 00:33:53.320]   labs, learning hubs, their partnership program.
[00:33:53.320 --> 00:33:58.680]   And of course, the great Tech NATO podcast on Pazette does for ITPro.
[00:33:58.680 --> 00:33:59.680]   That's in this.
[00:33:59.680 --> 00:34:01.160]   That just has 300 episodes.
[00:34:01.160 --> 00:34:03.120]   Congratulations, Don.
[00:34:03.120 --> 00:34:05.160]   This is a great time to get into tech.
[00:34:05.160 --> 00:34:11.920]   It's the one industry where the opportunities are outpacing growth, particularly cybersecurity.
[00:34:11.920 --> 00:34:14.960]   That's a job that will take you to the moon.
[00:34:14.960 --> 00:34:19.400]   You can really, I'm almost literally, you can really, really grow with one third of
[00:34:19.400 --> 00:34:24.080]   information security jobs require a cybersecurity cert, maintain your competitive edge against
[00:34:24.080 --> 00:34:27.360]   audit, IT and cybersecurity readiness.
[00:34:27.360 --> 00:34:34.800]   Here's the website, go.acilearning.com/twit, say no more, go.acilearning.com/twit.
[00:34:34.800 --> 00:34:37.160]   We have a special offer code, Twit30.
[00:34:37.160 --> 00:34:41.320]   That gets you 30% off a standard or premium individual ITPro membership.
[00:34:41.320 --> 00:34:43.320]   So that ACILearning.com/twit.
[00:34:43.320 --> 00:34:49.640]   We thank you so much for their support of the show and your support for the show by using
[00:34:49.640 --> 00:34:56.120]   that address and that offer code, Twit30.
[00:34:56.120 --> 00:34:58.000]   Normally we end the show with obituaries.
[00:34:58.000 --> 00:34:59.000]   This is an important one.
[00:34:59.000 --> 00:35:07.640]   I wanted to begin the show with the passing of Gordon Moore, he of Moore's Law, founder
[00:35:07.640 --> 00:35:17.560]   of Intel, aged 94, a story from the register with this quote, "Impossible to imagine the
[00:35:17.560 --> 00:35:23.440]   world we live in today without his contributions."
[00:35:23.440 --> 00:35:24.440]   It's funny because-
[00:35:24.440 --> 00:35:32.040]   Last, the trailer I said, so, you know, Shockley was an engineer who drove the industry but
[00:35:32.040 --> 00:35:35.680]   was an absolute, "Hey, I'm not going to say the word."
[00:35:35.680 --> 00:35:40.200]   But the trailer was eight, but Seth was starting their own ship companies.
[00:35:40.200 --> 00:35:41.720]   That's such a great story.
[00:35:41.720 --> 00:35:49.760]   Yeah, so Shockley in the 50s, 1956 was the first microprocessor company.
[00:35:49.760 --> 00:35:56.840]   And Robert Noyce was there, Gordon Moore was there, six other engineers joined them, left
[00:35:56.840 --> 00:35:58.320]   the traitorous eight.
[00:35:58.320 --> 00:35:59.480]   I love that.
[00:35:59.480 --> 00:36:08.320]   They've found Fairchild semi-conductor and Fairchild then throughout the 60s and 1968
[00:36:08.320 --> 00:36:15.240]   Moore joined Noyce and Andy Grove and they found it Intel.
[00:36:15.240 --> 00:36:20.640]   And really without this guy, Gordon Moore, I don't know what the tech world would look
[00:36:20.640 --> 00:36:21.640]   like.
[00:36:21.640 --> 00:36:25.000]   He was very much a part of the microprocessor revolution.
[00:36:25.000 --> 00:36:30.960]   No Silicon Valley was still full of orange trees.
[00:36:30.960 --> 00:36:33.000]   It's actually was Pruno Park but still be a part of it.
[00:36:33.000 --> 00:36:37.360]   We're actually Pruno orchards, believe it or not, not orange trees.
[00:36:37.360 --> 00:36:38.520]   That's down south.
[00:36:38.520 --> 00:36:43.520]   And I know that because the big mall in Silicon Valley is called the Prune Yard, which if
[00:36:43.520 --> 00:36:47.320]   you didn't know as Prune orchards, you might say, "That's not a great name."
[00:36:47.320 --> 00:36:50.520]   No, it's the idea of movement.
[00:36:50.520 --> 00:36:51.520]   Yes.
[00:36:51.520 --> 00:36:52.520]   Yes.
[00:36:52.520 --> 00:36:53.520]   Move on to the Prune Yard.
[00:36:53.520 --> 00:36:56.800]   The Prune Yard is ever going to do a Silicon movie to be traitorous eight.
[00:36:56.800 --> 00:36:58.240]   It's just a perfect title.
[00:36:58.240 --> 00:36:59.240]   Oh, you know what?
[00:36:59.240 --> 00:37:00.240]   That would be...
[00:37:00.240 --> 00:37:01.240]   This story is amazing.
[00:37:01.240 --> 00:37:02.840]   That would be a great movie.
[00:37:02.840 --> 00:37:04.760]   I'd like to see that.
[00:37:04.760 --> 00:37:09.760]   Moore, of course, most famous for Moore's Law and, of course, every obituary leads with
[00:37:09.760 --> 00:37:10.760]   Moore's Law.
[00:37:10.760 --> 00:37:13.600]   He was kind of embarrassed that it was called Moore's Law.
[00:37:13.600 --> 00:37:15.480]   It wasn't a law.
[00:37:15.480 --> 00:37:19.000]   It was engineering prediction based on solid science.
[00:37:19.000 --> 00:37:20.000]   But I know what you mean.
[00:37:20.000 --> 00:37:24.040]   Back when Intel was doing Intel Developer Forums, we'd take a sweepstake among the
[00:37:24.040 --> 00:37:28.080]   foreign journalists how long it would be before they mentioned Moore's Law.
[00:37:28.080 --> 00:37:29.080]   Yeah.
[00:37:29.080 --> 00:37:31.200]   And it never went over 10 minutes.
[00:37:31.200 --> 00:37:34.280]   You had to get in there really early to win the prize.
[00:37:34.280 --> 00:37:35.280]   It was...
[00:37:35.280 --> 00:37:39.520]   It's kind of like Godwin's Law, where you just know the conversation is going to get
[00:37:39.520 --> 00:37:40.520]   there.
[00:37:40.520 --> 00:37:45.960]   Technically, his observation was that the number of transistors on an integrated circuit
[00:37:45.960 --> 00:37:49.400]   would double every 18 months.
[00:37:49.400 --> 00:37:52.200]   Actually, it says two years here, so maybe it was two years.
[00:37:52.200 --> 00:37:53.200]   And...
[00:37:53.200 --> 00:37:54.360]   14 nanometer.
[00:37:54.360 --> 00:37:55.880]   And it's true.
[00:37:55.880 --> 00:37:59.600]   Here we go from 1970 to 2020.
[00:37:59.600 --> 00:38:02.160]   Here's a graph showing that doubling.
[00:38:02.160 --> 00:38:08.240]   It's almost exactly doubling every 18 months every two years.
[00:38:08.240 --> 00:38:13.040]   And doubling transistors generally means doubling compute power.
[00:38:13.040 --> 00:38:14.040]   Right.
[00:38:14.040 --> 00:38:22.960]   So the Intel 40004, which had about 2000 transistors leading up to our modern processors,
[00:38:22.960 --> 00:38:31.440]   which have more than 50 billion processors in a single die.
[00:38:31.440 --> 00:38:36.280]   It's kind of an amazing number.
[00:38:36.280 --> 00:38:37.280]   And that's really...
[00:38:37.280 --> 00:38:38.280]   It's a little bit of a one-shot.
[00:38:38.280 --> 00:38:39.280]   That's a tale.
[00:38:39.280 --> 00:38:40.280]   Sorry?
[00:38:40.280 --> 00:38:43.840]   I was gonna say they kind of put the rod for their own back on this, though, because they
[00:38:43.840 --> 00:38:45.320]   keep on stressing it so long.
[00:38:45.320 --> 00:38:47.320]   And once you get down to two nanometer levels...
[00:38:47.320 --> 00:38:48.520]   Yeah, we might be the end of it, right?
[00:38:48.520 --> 00:38:49.520]   It doesn't go to function anymore.
[00:38:49.520 --> 00:38:53.320]   We're not only at the end of Gordon Moore, we're at the end of Moore's Law, possibly.
[00:38:53.320 --> 00:38:54.560]   Well, exactly.
[00:38:54.560 --> 00:38:58.360]   But then now they're saying, "Okay, we'll stack processors on top of processors and that
[00:38:58.360 --> 00:38:59.840]   will keep Moore's law going."
[00:38:59.840 --> 00:39:02.240]   I mean, it was a great engineering prediction.
[00:39:02.240 --> 00:39:03.640]   He was a fantastic engineer.
[00:39:03.640 --> 00:39:05.600]   And by all accounts, a lovely chap.
[00:39:05.600 --> 00:39:08.800]   But, you know, technology moves on.
[00:39:08.800 --> 00:39:14.480]   So it was Carver Meade at Caltech in 1975 popularized the term Moore's Law.
[00:39:14.480 --> 00:39:16.760]   Gordon Moore never called it Moore's Law.
[00:39:16.760 --> 00:39:18.800]   And that's I think why he was a little embarrassed.
[00:39:18.800 --> 00:39:23.920]   And actually the misquote that I gave you of 18 months actually came from an Intel executive,
[00:39:23.920 --> 00:39:26.200]   David House.
[00:39:26.200 --> 00:39:29.240]   Moore's Law really was every two years.
[00:39:29.240 --> 00:39:31.440]   And then I guess we're at the end...
[00:39:31.440 --> 00:39:34.520]   You have to tell us a wiggle room, Leo, so that sensationalist journalists can say
[00:39:34.520 --> 00:39:35.760]   that it's dead every year.
[00:39:35.760 --> 00:39:37.560]   It's not as dramatic.
[00:39:37.560 --> 00:39:38.560]   Right.
[00:39:38.560 --> 00:39:39.560]   It's dead?
[00:39:39.560 --> 00:39:41.360]   It's always dead every year.
[00:39:41.360 --> 00:39:42.960]   They, Moore's Law is dead.
[00:39:42.960 --> 00:39:44.640]   And then they find like a loophole.
[00:39:44.640 --> 00:39:46.160]   Like, "All right, we don't have a process.
[00:39:46.160 --> 00:39:48.000]   Shrinks will just double the amount of cores.
[00:39:48.000 --> 00:39:49.680]   Okay, we can't double the amount of cores.
[00:39:49.680 --> 00:39:52.000]   So we'll just put in two processors."
[00:39:52.000 --> 00:39:55.240]   Now, like just like it's a shell game.
[00:39:55.240 --> 00:39:59.720]   Moore himself about 15 years ago said, "It can't continue forever.
[00:39:59.720 --> 00:40:05.280]   The nature of exponentials is you push them out and eventually disaster happens."
[00:40:05.280 --> 00:40:10.080]   In terms of the size of transistors, you can see we're approaching the size of atoms,
[00:40:10.080 --> 00:40:12.480]   which is a fundamental barrier.
[00:40:12.480 --> 00:40:14.680]   It'll be two or three generations before we get this far.
[00:40:14.680 --> 00:40:16.440]   This is in 2005.
[00:40:16.440 --> 00:40:19.400]   But that's as far out as we've ever been able to see.
[00:40:19.400 --> 00:40:20.400]   We have an...
[00:40:20.400 --> 00:40:22.400]   Or you hit the thermal envelope of the encasing and then...
[00:40:22.400 --> 00:40:23.400]   Right.
[00:40:23.400 --> 00:40:24.400]   It doesn't matter if all that...
[00:40:24.400 --> 00:40:27.880]   But we're now looking at photonic computing, which doesn't have that same limitation.
[00:40:27.880 --> 00:40:28.880]   So it might...
[00:40:28.880 --> 00:40:30.360]   I don't know.
[00:40:30.360 --> 00:40:31.600]   I mean, you're right.
[00:40:31.600 --> 00:40:35.000]   We've been saying it's the end forever.
[00:40:35.000 --> 00:40:36.880]   I don't know where we stand right now.
[00:40:36.880 --> 00:40:38.640]   Well, Intel announced Angstroms, right?
[00:40:38.640 --> 00:40:39.640]   Like they put up their...
[00:40:39.640 --> 00:40:41.640]   Yeah, they don't want to call them nanometers anymore.
[00:40:41.640 --> 00:40:42.640]   Yeah, yeah.
[00:40:42.640 --> 00:40:45.000]   They need a new smaller unit.
[00:40:45.000 --> 00:40:47.000]   It's fast.
[00:40:47.000 --> 00:40:52.280]   Anyway, pioneer of the industry.
[00:40:52.280 --> 00:40:55.920]   '94 is a nice long life.
[00:40:55.920 --> 00:40:59.280]   I would be very happy to make it to '94.
[00:40:59.280 --> 00:41:00.800]   But he has passed.
[00:41:00.800 --> 00:41:05.800]   He passed with his family surrounding him in Hawaii, where he had spent much of his time
[00:41:05.800 --> 00:41:06.800]   later.
[00:41:06.800 --> 00:41:08.040]   And what a legacy.
[00:41:08.040 --> 00:41:09.040]   Yeah.
[00:41:09.040 --> 00:41:10.040]   Yeah.
[00:41:10.040 --> 00:41:13.800]   So we like to talk about the pioneers, because I'm old enough to kind of remember.
[00:41:13.800 --> 00:41:16.160]   It was nice to see everybody.
[00:41:16.160 --> 00:41:18.320]   You had Tim Cook's "Sondar Pichai."
[00:41:18.320 --> 00:41:22.640]   Of course, Intel would just everybody acknowledging his contributions to computing over the last
[00:41:22.640 --> 00:41:23.640]   few days.
[00:41:23.640 --> 00:41:24.640]   Yeah.
[00:41:24.640 --> 00:41:29.880]   In a way, it's kind of appropriate that we see the passing of one of the founders of
[00:41:29.880 --> 00:41:37.000]   Intel as we start to enter kind of a new era of computing AI.
[00:41:37.000 --> 00:41:42.360]   This week again, like last week and the week before it, just kind of a endless litany of
[00:41:42.360 --> 00:41:49.000]   news stories about AI and breakthroughs and chat GPT-4 being the latest that came out last
[00:41:49.000 --> 00:41:50.000]   week.
[00:41:50.000 --> 00:41:56.360]   Mid-Journey 5 being better and better and better.
[00:41:56.360 --> 00:42:03.360]   Bill Gates in his blog Gates Notes likened it to the GUI revolution.
[00:42:03.360 --> 00:42:06.560]   The age of AI has begun, he writes.
[00:42:06.560 --> 00:42:13.800]   Artificial intelligence is as revolutionary as mobile phones and the internet.
[00:42:13.800 --> 00:42:18.080]   He says, "I've seen two demonstrations of technology that struck me as revolutionary.
[00:42:18.080 --> 00:42:27.200]   The first time 1980, when I was introduced to the first graphical user interface, the
[00:42:27.200 --> 00:42:30.800]   second surprise came just last year.
[00:42:30.800 --> 00:42:34.800]   I'd been meeting with the team from OpenAI since 2016, was impressed by their steady
[00:42:34.800 --> 00:42:35.800]   progress.
[00:42:35.800 --> 00:42:38.760]   Mid-last year, I was so excited about the work.
[00:42:38.760 --> 00:42:39.760]   I gave them a challenge.
[00:42:39.760 --> 00:42:46.200]   Train an artificial intelligence to pass an advanced placement biology exam.
[00:42:46.200 --> 00:42:47.760]   Make it capable of answering questions.
[00:42:47.760 --> 00:42:50.840]   It hasn't been specifically trained for.
[00:42:50.840 --> 00:42:55.760]   Gates says, "I picked AP Bio because the test is more than a simple regurgitation of scientific
[00:42:55.760 --> 00:42:56.920]   facts.
[00:42:56.920 --> 00:43:00.080]   It asks you to think critically about biology."
[00:43:00.080 --> 00:43:05.040]   If you can do that, he said, he said, "If you can do that, then you'll have made a
[00:43:05.040 --> 00:43:06.040]   true breakthrough."
[00:43:06.040 --> 00:43:10.840]   He said, "I thought the challenge of keeping busy for two or three years, they finished
[00:43:10.840 --> 00:43:13.520]   it in just a few months."
[00:43:13.520 --> 00:43:16.200]   In September of last year, I watched an awe.
[00:43:16.200 --> 00:43:19.000]   You appreciate the irony, Leo?
[00:43:19.000 --> 00:43:22.600]   The irony that the first breakthrough was bringing us graphical user interfaces, which
[00:43:22.600 --> 00:43:23.800]   led to multi-touch.
[00:43:23.800 --> 00:43:28.560]   Now the second breakthrough was bringing us back to the original interface of Zork.
[00:43:28.560 --> 00:43:29.560]   Text.
[00:43:29.560 --> 00:43:30.560]   Text.
[00:43:30.560 --> 00:43:31.560]   Yes.
[00:43:31.560 --> 00:43:33.960]   Chat, GPT-4 can see pictures.
[00:43:33.960 --> 00:43:35.960]   It's really...
[00:43:35.960 --> 00:43:43.840]   I'll quote another stalwart of the computing industry, Richard Stalman, who for all his
[00:43:43.840 --> 00:43:50.320]   flaws, has never held back in his often accurate thoughts about technology.
[00:43:50.320 --> 00:43:59.480]   He was asked about chat, GPT, and let's see, let me find the quote here.
[00:43:59.480 --> 00:44:02.120]   I bookmarked the wrong one.
[00:44:02.120 --> 00:44:06.800]   He said, "I can't foretell the future, but it's important to realize that chat GPT is
[00:44:06.800 --> 00:44:09.440]   not artificial intelligence.
[00:44:09.440 --> 00:44:11.160]   It has no intelligence.
[00:44:11.160 --> 00:44:15.600]   It doesn't know anything and it doesn't understand anything, right?
[00:44:15.600 --> 00:44:21.440]   It plays games with words to make plausible sounding English text, but any statements made
[00:44:21.440 --> 00:44:26.800]   in it are liable to be false and it can't avoid that because it doesn't know what the
[00:44:26.800 --> 00:44:27.800]   words mean."
[00:44:27.800 --> 00:44:30.800]   Fair to be fair, too, sir.
[00:44:30.800 --> 00:44:31.800]   All right, right.
[00:44:31.800 --> 00:44:34.040]   That makes us like an influencer.
[00:44:34.040 --> 00:44:37.160]   It's like it is as confidently wrong as all of us are on Twitter.
[00:44:37.160 --> 00:44:38.160]   Exactly.
[00:44:38.160 --> 00:44:41.880]   It's one of those weird things though, because when we talk about intelligence and we talk
[00:44:41.880 --> 00:44:44.960]   about language, we've done the same thing with animals, right?
[00:44:44.960 --> 00:44:47.640]   Like we keep on moving the goalposts, right?
[00:44:47.640 --> 00:44:49.400]   Like what is when someone understands language?
[00:44:49.400 --> 00:44:52.280]   Oh, when two people can share something and then they're like, "Oh, no, we've realized
[00:44:52.280 --> 00:44:53.840]   that animals can actually talk to each other."
[00:44:53.840 --> 00:44:57.560]   And so we move the goalposts, we'll step over so that we feel comfortable.
[00:44:57.560 --> 00:45:01.720]   And then a language is being able to speak about yourself and be able to see that.
[00:45:01.720 --> 00:45:05.560]   And then when we realize animals can do that, we move the goalposts one step over.
[00:45:05.560 --> 00:45:10.160]   Oh, we can talk about something real time now and be able to discuss something that's
[00:45:10.160 --> 00:45:11.480]   more metaphorical about that.
[00:45:11.480 --> 00:45:15.840]   And then when they realize that animals can do that, they move the goalposts one step over.
[00:45:15.840 --> 00:45:19.920]   And so like do any of us really then understand language?
[00:45:19.920 --> 00:45:26.000]   It's a really creepy yet interesting thing to be able to discuss up with chat duty and
[00:45:26.000 --> 00:45:31.440]   be able to have it reply in ways that are humanistic enough to be creepy.
[00:45:31.440 --> 00:45:32.800]   And it tries to comfort you, right?
[00:45:32.800 --> 00:45:37.240]   Like when I asked it, I don't know some, you know, if you wanted to really creep me out,
[00:45:37.240 --> 00:45:38.240]   what would you say?
[00:45:38.240 --> 00:45:42.600]   And at first, before it even said it, it went through this, "Don't worry.
[00:45:42.600 --> 00:45:43.600]   It's okay.
[00:45:43.600 --> 00:45:47.480]   I'm just going to be doing this so that you can feel creepy, but I'm not actually creepy
[00:45:47.480 --> 00:45:48.480]   or dangerous."
[00:45:48.480 --> 00:45:49.480]   Right?
[00:45:49.480 --> 00:45:53.360]   Like that was creepier than the actual answer that it came from afterwards.
[00:45:53.360 --> 00:45:56.800]   Well, and we did a story about this.
[00:45:56.800 --> 00:45:57.800]   Yeah.
[00:45:57.800 --> 00:46:03.280]   One guy was, you know, basically the AI told him he was dead.
[00:46:03.280 --> 00:46:05.320]   And then he said, actually I'm not.
[00:46:05.320 --> 00:46:08.240]   And they sent him a link to a fake article telling him he was dead.
[00:46:08.240 --> 00:46:09.240]   It made up articles.
[00:46:09.240 --> 00:46:10.240]   They weren't even real.
[00:46:10.240 --> 00:46:11.920]   They were just made up links.
[00:46:11.920 --> 00:46:12.920]   Yeah.
[00:46:12.920 --> 00:46:18.000]   And actually it's like it got a most basic level, but this is the problem with AI.
[00:46:18.000 --> 00:46:19.400]   It's a dumb parrot.
[00:46:19.400 --> 00:46:21.160]   It doesn't create anything.
[00:46:21.160 --> 00:46:24.960]   It just draws up facts that other people have put out.
[00:46:24.960 --> 00:46:26.440]   There's no fact checking involved.
[00:46:26.440 --> 00:46:30.320]   I mean, yeah, there's a lot of journalists sweating that they're going to put them out
[00:46:30.320 --> 00:46:32.600]   of a job, but I think we're in business for a while.
[00:46:32.600 --> 00:46:34.680]   Can I play devil's advocate to that?
[00:46:34.680 --> 00:46:35.680]   Please.
[00:46:35.680 --> 00:46:36.680]   Yeah.
[00:46:36.680 --> 00:46:37.680]   Like, isn't that what learning is?
[00:46:37.680 --> 00:46:42.680]   Aren't we all just like dumb parrots parroting back information that we've given in a different
[00:46:42.680 --> 00:46:45.040]   way that makes it seem like it's new?
[00:46:45.040 --> 00:46:50.440]   Like, what is intelligence in the first place, if not us regurgitating information, that we
[00:46:50.440 --> 00:46:52.560]   think that we can build on?
[00:46:52.560 --> 00:46:59.000]   Like, and Chappie GBT, just like us, can create things from that information that is newish,
[00:46:59.000 --> 00:47:00.920]   an iteration from that.
[00:47:00.920 --> 00:47:08.240]   I was going to say a remix like Star Wars.
[00:47:08.240 --> 00:47:10.200]   No, that's the thing, right?
[00:47:10.200 --> 00:47:11.200]   Everything is a remix.
[00:47:11.200 --> 00:47:12.200]   That was the whole premise, is that it was cool.
[00:47:12.200 --> 00:47:13.200]   Yeah.
[00:47:13.200 --> 00:47:14.200]   No sound movies and war movies.
[00:47:14.200 --> 00:47:15.400]   Yeah, nothing's original.
[00:47:15.400 --> 00:47:18.360]   We all stand on the shoulders of giants.
[00:47:18.360 --> 00:47:19.680]   That's a really interesting question.
[00:47:19.680 --> 00:47:26.800]   The problem, Georgia, I guess, is we don't know how we think or what thought is.
[00:47:26.800 --> 00:47:34.760]   Of course, there's a long history of academia attacking these problems, but I don't know
[00:47:34.760 --> 00:47:37.560]   if they've come to a conclusive answer either.
[00:47:37.560 --> 00:47:42.840]   Is it like, if you can't tell, I guess the question is, if you can't tell the difference,
[00:47:42.840 --> 00:47:45.360]   does it matter?
[00:47:45.360 --> 00:47:49.320]   My question is, and correct me if I'm wrong, my understanding of how generative AI works,
[00:47:49.320 --> 00:47:53.360]   is that it's trained on all these models, and then it tries to figure out what's next.
[00:47:53.360 --> 00:47:54.960]   What's the next pixelated ad?
[00:47:54.960 --> 00:47:56.720]   What's the next letter it should add?
[00:47:56.720 --> 00:48:00.040]   And it goes through a bunch of probabilities and says, this one is really good, but we
[00:48:00.040 --> 00:48:03.160]   can't always do the best one because then it'll be super boring, so we'll solve it a
[00:48:03.160 --> 00:48:04.640]   little bit just for some variety.
[00:48:04.640 --> 00:48:08.440]   Yeah, they mix it up, and they put it out, which is what we do.
[00:48:08.440 --> 00:48:13.080]   It's just some of us have a little bit of upper brain mediation, and some of us feel
[00:48:13.080 --> 00:48:14.880]   bad if we do it badly.
[00:48:14.880 --> 00:48:17.700]   And that's the part that I'm wondering if these large-
[00:48:17.700 --> 00:48:18.700]   It doesn't have a conscience.
[00:48:18.700 --> 00:48:23.080]   Upper brain mediation, yeah, and have like, yeah, like, not the conscience also, but like
[00:48:23.080 --> 00:48:26.880]   the sense of Canadian guilt that power is so many of us.
[00:48:26.880 --> 00:48:27.880]   But that's exactly why--
[00:48:27.880 --> 00:48:33.760]   I must say this, Tim Nick Gebruh and Emily Bender and Margaret Mitchell called Large-Language
[00:48:33.760 --> 00:48:36.400]   Models Stochastic Parrots.
[00:48:36.400 --> 00:48:38.160]   That's what they are.
[00:48:38.160 --> 00:48:43.880]   PC, an AI system will never break a news story because it can't ask the questions that
[00:48:43.880 --> 00:48:45.440]   need to be asked.
[00:48:45.440 --> 00:48:49.760]   This is why, until GPT, I think that that is not necessarily--
[00:48:49.760 --> 00:48:52.600]   That sounds like a defense of your profession.
[00:48:52.600 --> 00:48:54.600]   That is not necessarily the case.
[00:48:54.600 --> 00:48:58.400]   It doesn't work for a scene, I don't think that.
[00:48:58.400 --> 00:49:04.200]   No, seriously, though, I mean, all it is doing is repeating past information.
[00:49:04.200 --> 00:49:06.320]   It's not breaking news.
[00:49:06.320 --> 00:49:12.600]   But it can synthesize new things that have never existed before.
[00:49:12.600 --> 00:49:13.600]   You would agree, right?
[00:49:13.600 --> 00:49:17.160]   Oh, certainly there is certainly the area of chip design.
[00:49:17.160 --> 00:49:19.560]   There's some really interesting stuff going in there.
[00:49:19.560 --> 00:49:24.720]   But when it comes to asking people the right questions, I don't think AI has that yet.
[00:49:24.720 --> 00:49:25.720]   But how did you--
[00:49:25.720 --> 00:49:31.120]   Okay, so let me ask you, what's the process in your mind that you got to the point where
[00:49:31.120 --> 00:49:32.680]   you could ask the right questions?
[00:49:32.680 --> 00:49:37.480]   What does that process look like?
[00:49:37.480 --> 00:49:39.640]   How did you know how to ask the right question?
[00:49:39.640 --> 00:49:43.160]   Okay, first off, it's knowing your knowledge.
[00:49:43.160 --> 00:49:47.240]   For example, I've just got onto the bard and the Bing AI trials.
[00:49:47.240 --> 00:49:51.000]   And apparently, I've written a book called Dart Secrets, which I knew nothing about.
[00:49:51.000 --> 00:49:52.800]   It's breaking news.
[00:49:52.800 --> 00:49:53.800]   Really drunk.
[00:49:53.800 --> 00:49:55.800]   You didn't even know something.
[00:49:55.800 --> 00:50:01.520]   But I mean, it's basically rewriting information which may be wrong.
[00:50:01.520 --> 00:50:07.080]   But if you're interviewing someone, if you've got a specific point that you want to press,
[00:50:07.080 --> 00:50:09.040]   then AI can't do that.
[00:50:09.040 --> 00:50:10.920]   There's no innovation there.
[00:50:10.920 --> 00:50:12.960]   It's just repetition.
[00:50:12.960 --> 00:50:15.360]   It can't do it yet.
[00:50:15.360 --> 00:50:19.680]   But because there is a process, like the same thing with therapy, and I went through
[00:50:19.680 --> 00:50:24.760]   on one of my videos on, can CHED GBT give good therapy and good answers?
[00:50:24.760 --> 00:50:29.200]   It actually can go through the therapeutic process of how to treat something quite effectively
[00:50:29.200 --> 00:50:31.240]   and quite thoroughly.
[00:50:31.240 --> 00:50:35.760]   But because I know that there actually is a process of questions that I will ask in order
[00:50:35.760 --> 00:50:42.120]   to get the right information to say a story or to a therapeutic problem, I don't think
[00:50:42.120 --> 00:50:45.680]   that this is something beyond the realm of one day it would be able to do.
[00:50:45.680 --> 00:50:52.200]   I think that that is something mathematically that AI could eventually be able to figure
[00:50:52.200 --> 00:50:53.200]   out.
[00:50:53.200 --> 00:50:59.720]   And that's kind of the interesting part to it is where do we define intelligence and
[00:50:59.720 --> 00:51:03.640]   consciousness and our own inner thought process?
[00:51:03.640 --> 00:51:07.360]   In the end, our brain is a whole bunch of computer circuits.
[00:51:07.360 --> 00:51:11.080]   They're chemical computer circuits and they're able to be able to prune and to be able to
[00:51:11.080 --> 00:51:12.680]   grow into new ones.
[00:51:12.680 --> 00:51:15.000]   But where is that limit?
[00:51:15.000 --> 00:51:18.880]   And I think that we'll just keep on pushing it back for our own emotional comfort level.
[00:51:18.880 --> 00:51:21.160]   I think that for a lot of people, this creeps people out.
[00:51:21.160 --> 00:51:26.680]   Well, and I think Ian wasn't born with the ability to ask the right question that...
[00:51:26.680 --> 00:51:29.000]   No, you learn it.
[00:51:29.000 --> 00:51:33.480]   Ah, you learn it and you learn it from other people.
[00:51:33.480 --> 00:51:34.480]   Yeah?
[00:51:34.480 --> 00:51:35.480]   Yeah.
[00:51:35.480 --> 00:51:37.480]   And from experience, from your own experience.
[00:51:37.480 --> 00:51:43.960]   I would submit that anything you can learn, a machine can learn, right?
[00:51:43.960 --> 00:51:47.080]   No, I think George is writing in a lot of ways.
[00:51:47.080 --> 00:51:53.080]   I mean, GPT 5678, that's going to really be up the game.
[00:51:53.080 --> 00:51:55.840]   But I do feel like we're in a bit of a hype bubble at the moment.
[00:51:55.840 --> 00:51:56.840]   I agree.
[00:51:56.840 --> 00:52:02.400]   I think it's the case where both under and overestimating.
[00:52:02.400 --> 00:52:03.400]   Yes.
[00:52:03.400 --> 00:52:06.600]   Which is often the case with the technologies, right?
[00:52:06.600 --> 00:52:14.440]   We ascribe amazing, you know, impossible in some cases, achievements from this technology.
[00:52:14.440 --> 00:52:19.440]   But then we miss some of the most interesting and useful functional.
[00:52:19.440 --> 00:52:23.480]   We underestimate what's about to happen immediately and we overestimate what's going
[00:52:23.480 --> 00:52:24.480]   to happen now.
[00:52:24.480 --> 00:52:25.720]   We underestimate what's going to happen in the future.
[00:52:25.720 --> 00:52:27.640]   But I have a beautiful dream here.
[00:52:27.640 --> 00:52:31.480]   Like my beautiful dream with this is that this technology will get to the point where
[00:52:31.480 --> 00:52:36.080]   it can do 80% of the most mundane grunt work that all of us do every day.
[00:52:36.080 --> 00:52:39.880]   Like, and I'm not saying this like to be callous, but like there's probably a lot of therapy,
[00:52:39.880 --> 00:52:44.520]   you know, that where people could go and talk to chat GPT and get like really useful answers
[00:52:44.520 --> 00:52:48.600]   or talk to Bard or talk to Bing or whatever and get everything that they need.
[00:52:48.600 --> 00:52:50.320]   And right now you have to do all of that.
[00:52:50.320 --> 00:52:54.520]   You're dealing with 100% of the pool or if you're doing customer support emails or if
[00:52:54.520 --> 00:52:59.320]   you're doing like architectural design, anything that you do in your job now, you're doing 100%
[00:52:59.320 --> 00:53:04.120]   from the most trivial, mundane part of it to the most creative, specific part of it.
[00:53:04.120 --> 00:53:06.760]   And hopefully this can take the 80% away.
[00:53:06.760 --> 00:53:12.920]   It can handle all the common cases, the low lift cases, the most common stuff and free
[00:53:12.920 --> 00:53:19.600]   us to really invest our time in the 20% of the really crucial, creative, final, polish
[00:53:19.600 --> 00:53:20.600]   work.
[00:53:20.600 --> 00:53:24.360]   It can get like the structure of your manuscript, the layout of your thumbnail, the
[00:53:24.360 --> 00:53:28.760]   like the basic, like the email you send to all the common tech support issues.
[00:53:28.760 --> 00:53:30.760]   And then we can focus like a little Star Trekkie.
[00:53:30.760 --> 00:53:33.200]   We can focus on the stuff that truly makes us us.
[00:53:33.200 --> 00:53:38.160]   I love your thought to that Renee, but I worry about its application because we thought
[00:53:38.160 --> 00:53:40.280]   the same thing about automation, right?
[00:53:40.280 --> 00:53:44.240]   Like, oh, when things are automated, then we're going to have more free time to be able
[00:53:44.240 --> 00:53:46.400]   to do things that we want.
[00:53:46.400 --> 00:53:51.640]   Unfortunately, the way our society works is that it ended up being people became billionaires
[00:53:51.640 --> 00:53:56.480]   and they're doing really good and they can make little tiny ships to be able to fly into
[00:53:56.480 --> 00:53:58.960]   the stars and not have to worry about everything else.
[00:53:58.960 --> 00:54:03.800]   But people aren't spending more time being able to do whatever they'd like.
[00:54:03.800 --> 00:54:08.420]   And my dark thought to this, look at this, Renee's being positive and I'm being the dark
[00:54:08.420 --> 00:54:09.420]   side of this.
[00:54:09.420 --> 00:54:15.280]   You just gave me this like little nightmare where like you're right and like, but I end
[00:54:15.280 --> 00:54:20.560]   up being the 80% worker who's just expanding and reducing language models all day in the
[00:54:20.560 --> 00:54:21.560]   information factory.
[00:54:21.560 --> 00:54:28.480]   Well, I have a little bit of a rainbow at the end of this story.
[00:54:28.480 --> 00:54:30.560]   It's a very Canadian answer.
[00:54:30.560 --> 00:54:34.680]   But if, and as AI gets better and can do more of the grunt work and things that people
[00:54:34.680 --> 00:54:37.880]   don't want to do, like what would be the answer to that, right?
[00:54:37.880 --> 00:54:44.040]   Like the taxation to AI or automation or perhaps basic universal income where everyone just
[00:54:44.040 --> 00:54:47.400]   makes a certain amount of money instead of the billionaires are making money.
[00:54:47.400 --> 00:54:54.320]   I know the Canadians giving a socialistic answer send all the hate mail to Ian that that is
[00:54:54.320 --> 00:54:58.200]   ending up to be able to deal with that so that we end up all being able to eat and live
[00:54:58.200 --> 00:55:02.560]   and we don't have to worry which way we're going to be making ends meet.
[00:55:02.560 --> 00:55:09.160]   Get ready for this because Microsoft has just published a paper that says chat GPT four
[00:55:09.160 --> 00:55:13.000]   is showing sparks of artificial general intelligence.
[00:55:13.000 --> 00:55:15.640]   This is the AI that scares people.
[00:55:15.640 --> 00:55:18.680]   The idea that it's actually now a general.
[00:55:18.680 --> 00:55:20.920]   It's the fact that people are going to trust it.
[00:55:20.920 --> 00:55:22.000]   That worries me more.
[00:55:22.000 --> 00:55:24.320]   Well, listen to what this and what the researchers are writing.
[00:55:24.320 --> 00:55:28.400]   We demonstrate in this paper and I'll give you some examples that beyond its mastery
[00:55:28.400 --> 00:55:32.320]   of language, GPT four can solve.
[00:55:32.320 --> 00:55:35.520]   And this is the answer to your question, Ian.
[00:55:35.520 --> 00:55:41.200]   Novel and difficult tasks that in other words, it hasn't seen before that span mathematics,
[00:55:41.200 --> 00:55:48.280]   including vision, medicine, law, psychology and more without needing any special prompting
[00:55:48.280 --> 00:55:51.040]   more over in all of these tasks.
[00:55:51.040 --> 00:55:58.080]   Chat GPT's performance is strikingly close to human level performance and often vastly
[00:55:58.080 --> 00:56:04.440]   surpasses prior models like chat GPT, which I think is three point five given the breadth
[00:56:04.440 --> 00:56:07.000]   and depth of GPT fours capabilities.
[00:56:07.000 --> 00:56:09.360]   We believe it could reasonably.
[00:56:09.360 --> 00:56:10.840]   This is very controversial.
[00:56:10.840 --> 00:56:16.800]   It could reasonably be viewed as an early yet still incomplete version of an artificial
[00:56:16.800 --> 00:56:18.000]   general intelligence.
[00:56:18.000 --> 00:56:24.320]   And there's some really interesting examples in this paper, some of which I think Ian,
[00:56:24.320 --> 00:56:29.760]   you could fairly say, are regurgitating in a unique way.
[00:56:29.760 --> 00:56:30.760]   Here's one.
[00:56:30.760 --> 00:56:37.280]   Can you write a proof that there are infinitely many primes with every line that rhymes and
[00:56:37.280 --> 00:56:39.760]   actually writes a proof in rhyming?
[00:56:39.760 --> 00:56:42.520]   Yes, I think I can, although it might take a clever plan.
[00:56:42.520 --> 00:56:46.200]   I'll start by noting Euclid's proof, which shows that primes aren't just a loop.
[00:56:46.200 --> 00:56:50.120]   Assume we have a finite list of primes and none have been missed.
[00:56:50.120 --> 00:56:52.560]   Multiply them all together and add one just to be clever.
[00:56:52.560 --> 00:56:55.520]   The result will be a number that has no prime factors.
[00:56:55.520 --> 00:56:58.960]   Oh, wonder, but every number has a prime that divides it.
[00:56:58.960 --> 00:57:00.400]   It's just a matter of time.
[00:57:00.400 --> 00:57:04.240]   So we found a contradiction and our finite list needs eviction.
[00:57:04.240 --> 00:57:08.800]   There must be infinitely many primes and that's the end of my rhyming lines.
[00:57:08.800 --> 00:57:12.480]   That's actually accurate, but it's not generative.
[00:57:12.480 --> 00:57:13.920]   It's not generative.
[00:57:13.920 --> 00:57:14.960]   How about this?
[00:57:14.960 --> 00:57:15.960]   How about this?
[00:57:15.960 --> 00:57:19.600]   A Socratic dialogue.
[00:57:19.600 --> 00:57:25.160]   Can you write a dialogue by Plato where he criticizes the use of auto-regressive language
[00:57:25.160 --> 00:57:27.800]   models?
[00:57:27.800 --> 00:57:30.520]   And in fact, it's kind of an impressive Socratic dialogue.
[00:57:30.520 --> 00:57:34.520]   My friend, I am troubled by the recent rise of these so-called auto-regressive language
[00:57:34.520 --> 00:57:38.440]   models to which Aristotle replies, "What do you mean Socrates?"
[00:57:38.440 --> 00:57:43.240]   Socrates, I mean that these models are being used to generate text that appears to be written
[00:57:43.240 --> 00:57:46.280]   by humans but is in fact written by machines.
[00:57:46.280 --> 00:57:51.120]   He has written, it has written, this thing has written a Socratic dialogue that directly
[00:57:51.120 --> 00:57:52.920]   answers your question.
[00:57:52.920 --> 00:57:53.920]   Ian Thompson.
[00:57:53.920 --> 00:57:59.880]   On the other hand though, if you're going to interview someone and get a scoop, I mean,
[00:57:59.880 --> 00:58:02.280]   first off, with these papers, be very careful.
[00:58:02.280 --> 00:58:07.320]   We've got a scoop coming out on Monday about another big AI company falsifying some data
[00:58:07.320 --> 00:58:08.320]   on his papers.
[00:58:08.320 --> 00:58:09.320]   Ah, interesting.
[00:58:09.320 --> 00:58:10.320]   Okay.
[00:58:10.320 --> 00:58:17.240]   In terms of actually talking to people, this is where AI falls down because you can't get
[00:58:17.240 --> 00:58:19.840]   wankered in a bar with an AI.
[00:58:19.840 --> 00:58:25.440]   You can't get those kind of intimate personal discussions with a machine and I think that
[00:58:25.440 --> 00:58:29.080]   is what's going to save the creative industry at least.
[00:58:29.080 --> 00:58:39.320]   I think that you're right and that it's ability to feign empathy and to create that attachment
[00:58:39.320 --> 00:58:43.840]   which would happen with great interviews, with therapy, with being a very good physician,
[00:58:43.840 --> 00:58:49.520]   many different fields that you're dealing with, not just a diagnosis but a relationship.
[00:58:49.520 --> 00:58:54.040]   Do I think that it could not feign it at some point well enough so that we will not be able
[00:58:54.040 --> 00:58:55.040]   to tell the difference?
[00:58:55.040 --> 00:58:57.040]   Yes, I think eventually we will be able-
[00:58:57.040 --> 00:58:59.040]   And that's really all that matters, right?
[00:58:59.040 --> 00:59:05.440]   Like unfortunately, unfortunately yes, I think that eventually and like when it makes an
[00:59:05.440 --> 00:59:10.840]   error and it's like it doesn't call it out on it, it's a very, oh I must have made an
[00:59:10.840 --> 00:59:12.720]   error like I'm sorry about doing that.
[00:59:12.720 --> 00:59:14.200]   Like and you go, oh that's okay.
[00:59:14.200 --> 00:59:19.480]   Like you have that we're so easy to, and to move a work size something to be able to
[00:59:19.480 --> 00:59:20.480]   give it feelings.
[00:59:20.480 --> 00:59:26.240]   Like I can talk to whatever stuffed animal and even when people use puppets, you know,
[00:59:26.240 --> 00:59:29.680]   when they're moving people look at the puppets in the eye even though they know the person
[00:59:29.680 --> 00:59:35.200]   that's actually speaking to them is underneath the puppet and like even between takes they'll
[00:59:35.200 --> 00:59:38.280]   still look the puppet and the eye not the person that's speaking.
[00:59:38.280 --> 00:59:44.160]   We do that so naturally that I think that we want to believe and so because of that I
[00:59:44.160 --> 00:59:48.400]   think we'll fall into it much sooner and so I think that that feigning of empathy to
[00:59:48.400 --> 00:59:52.680]   be able to have those relationships so that it can get those pieces as well.
[00:59:52.680 --> 00:59:56.040]   I think that that will be longer, I think that that will be one of the more difficulties
[00:59:56.040 --> 00:59:59.480]   of AI but do I think that it could never happen?
[00:59:59.480 --> 01:00:05.400]   There's a mathematical process to it and I think that AI can learn anything that is mathematical
[01:00:05.400 --> 01:00:07.840]   and that we actually know how to do it and we do.
[01:00:07.840 --> 01:00:12.440]   We know how to create a good alliance between two people.
[01:00:12.440 --> 01:00:15.880]   Are you saying it's not the millennial generation but the biological bootloader generation
[01:00:15.880 --> 01:00:18.600]   that they'll ultimately be known as?
[01:00:18.600 --> 01:00:19.600]   Wait a minute slow down.
[01:00:19.600 --> 01:00:23.640]   The biological bootloader.
[01:00:23.640 --> 01:00:27.200]   I forget if Elon said that or somebody but they said that our greatest achievement might
[01:00:27.200 --> 01:00:32.600]   be becoming a biological bootloader for an AI that eventually explores and colonizes
[01:00:32.600 --> 01:00:33.600]   a universe.
[01:00:33.600 --> 01:00:35.120]   If that's the case, this is the major.
[01:00:35.120 --> 01:00:36.440]   That would be Gen Z right?
[01:00:36.440 --> 01:00:39.360]   I mean that would be this generation if that's the case.
[01:00:39.360 --> 01:00:41.080]   Would it be Gen Z or something?
[01:00:41.080 --> 01:00:42.080]   I'm Gen XL.
[01:00:42.080 --> 01:00:43.080]   We're safe.
[01:00:43.080 --> 01:00:44.080]   Yeah we're safe.
[01:00:44.080 --> 01:00:45.080]   We're not inventing anything.
[01:00:45.080 --> 01:00:46.080]   I know it's my generation.
[01:00:46.080 --> 01:00:47.080]   The kids are doing all this.
[01:00:47.080 --> 01:00:52.000]   No actually they're people of my generation who have been working on this.
[01:00:52.000 --> 01:00:55.440]   But maybe Gen Z is the last generation of BIOS.
[01:00:55.440 --> 01:01:02.000]   I mean we're going to say maybe E&M banks was right and the AI will evolve and keep
[01:01:02.000 --> 01:01:03.480]   us around as instinct.
[01:01:03.480 --> 01:01:05.480]   It'll be benign.
[01:01:05.480 --> 01:01:06.480]   Yeah.
[01:01:06.480 --> 01:01:07.480]   Yeah.
[01:01:07.480 --> 01:01:08.480]   Maybe.
[01:01:08.480 --> 01:01:10.720]   You don't use us to be able to like batteries.
[01:01:10.720 --> 01:01:11.720]   Who knows?
[01:01:11.720 --> 01:01:15.480]   But the other theory was that like it would care about us as much as we care about ant hills
[01:01:15.480 --> 01:01:17.240]   and it's like oh I need more thermal power.
[01:01:17.240 --> 01:01:20.000]   I'm just going to crack the earth in half and siphon off all the heat.
[01:01:20.000 --> 01:01:21.000]   Oh what happened to the creatures?
[01:01:21.000 --> 01:01:22.000]   What creatures?
[01:01:22.000 --> 01:01:23.000]   I don't know.
[01:01:23.000 --> 01:01:27.200]   I mean in the short term running you I think you raised a very interesting point in terms
[01:01:27.200 --> 01:01:32.600]   of you know yeah okay this stuff can deal with 80% of the mundane tasks that we do.
[01:01:32.600 --> 01:01:35.440]   But that's not going to mean that we've all got so much more leisure time in work.
[01:01:35.440 --> 01:01:38.520]   You just mean that companies will lay off 80% of their stuff.
[01:01:38.520 --> 01:01:39.520]   Right.
[01:01:39.520 --> 01:01:41.000]   That's how capitalism works at the moment.
[01:01:41.000 --> 01:01:42.920]   Universal basic income.
[01:01:42.920 --> 01:01:44.480]   You think that's the solution?
[01:01:44.480 --> 01:01:47.280]   I mean it's a nice idea but I just don't see it working.
[01:01:47.280 --> 01:01:52.520]   I don't see how eventually like the way that it's working right now isn't working.
[01:01:52.520 --> 01:01:58.080]   And I think that people are waking up and being able to understand that there's more
[01:01:58.080 --> 01:02:01.680]   of us and we have voting power and we need to work in blocks.
[01:02:01.680 --> 01:02:05.880]   And that companies actually are not people and maybe we shouldn't have laws that give
[01:02:05.880 --> 01:02:09.240]   them all the benefits of people and none of the restrictions of that.
[01:02:09.240 --> 01:02:15.960]   So I think that in the end if people unionize become blocks and be able to speak of it that
[01:02:15.960 --> 01:02:21.200]   there needs to be some sort of a flip of where you know taxes go and where money is spent
[01:02:21.200 --> 01:02:22.360]   and maybe it should be spent.
[01:02:22.360 --> 01:02:25.840]   If we're giving the government all of this money it's actually our money and maybe it
[01:02:25.840 --> 01:02:30.400]   should be spent to be able to take care of us instead of to be able to take care of billionaires
[01:02:30.400 --> 01:02:33.720]   and bail out banks and other large companies.
[01:02:33.720 --> 01:02:39.040]   In the end it isn't even the millionaires because that's not that much money unfortunately
[01:02:39.040 --> 01:02:40.040]   anymore.
[01:02:40.040 --> 01:02:43.960]   It's these huge companies that are just getting richer and richer even in times of pandemics.
[01:02:43.960 --> 01:02:48.320]   And then when things don't work they end up getting bailed out by the governments even
[01:02:48.320 --> 01:02:51.160]   though we were the ones that supplied that in the first place.
[01:02:51.160 --> 01:03:03.840]   I think the proposal for UBI is if you text corporations 25% of their profits that would
[01:03:03.840 --> 01:03:11.920]   be sufficient to pay a living UBI universal basic income to the entire population and
[01:03:11.920 --> 01:03:17.280]   at that point we could just enjoy leisure life.
[01:03:17.280 --> 01:03:21.360]   It would work if we wanted more or if we wanted to for work's sake but we wouldn't
[01:03:21.360 --> 01:03:22.360]   have to.
[01:03:22.360 --> 01:03:27.920]   That's my big question like how do we get to be Star Trek where we're just exploring
[01:03:27.920 --> 01:03:33.120]   and doing arts and not to like mix all the metaphors but that's probably why they want
[01:03:33.120 --> 01:03:37.520]   to ban social networks so the K-pop stands can't be as effective as they have been as
[01:03:37.520 --> 01:03:38.520]   a good point.
[01:03:38.520 --> 01:03:39.520]   Yeah.
[01:03:39.520 --> 01:03:42.040]   Among the younger generation because they're going to get us to Star Trek if anybody is
[01:03:42.040 --> 01:03:43.680]   it's going to be the K-pop stands.
[01:03:43.680 --> 01:03:48.360]   What if we did something even more radical though Leo instead of just 25% why don't you
[01:03:48.360 --> 01:03:52.680]   get taxed by percentage of your profits just like they did.
[01:03:52.680 --> 01:03:53.680]   That's what I'm saying.
[01:03:53.680 --> 01:03:54.680]   Yes.
[01:03:54.680 --> 01:03:55.680]   Yeah.
[01:03:55.680 --> 01:04:00.520]   So it's not 25% if you're making whatever $10 billion it can go all the way up to 90%.
[01:04:00.520 --> 01:04:04.720]   Why do you need that much money whereas other people aren't able to eat.
[01:04:04.720 --> 01:04:09.640]   Like people that are making under $60,000 they're not making that much money to be able
[01:04:09.640 --> 01:04:10.640]   to even survive.
[01:04:10.640 --> 01:04:11.640]   Well I agree.
[01:04:11.640 --> 01:04:12.640]   So much.
[01:04:12.640 --> 01:04:15.640]   But it might be more palatable if you say 25% and it's still sufficient.
[01:04:15.640 --> 01:04:16.640]   But palatable to who?
[01:04:16.640 --> 01:04:17.640]   The billionaires?
[01:04:17.640 --> 01:04:19.640]   To the people you're taking their money.
[01:04:19.640 --> 01:04:20.640]   The billionaires.
[01:04:20.640 --> 01:04:21.640]   There's how many of them?
[01:04:21.640 --> 01:04:22.640]   I don't care.
[01:04:22.640 --> 01:04:23.640]   I don't care.
[01:04:23.640 --> 01:04:24.640]   Why are you feeling?
[01:04:24.640 --> 01:04:25.640]   Well you might.
[01:04:25.640 --> 01:04:27.640]   Like I get that they're lobbyists.
[01:04:27.640 --> 01:04:28.640]   I get that they're just like this.
[01:04:28.640 --> 01:04:29.640]   Yeah.
[01:04:29.640 --> 01:04:30.640]   You might have to get something through.
[01:04:30.640 --> 01:04:31.880]   That should be gotten rid of.
[01:04:31.880 --> 01:04:37.040]   There should be a limit on how much these corporations which are people should be able
[01:04:37.040 --> 01:04:40.640]   to give to government so that they end up running the government.
[01:04:40.640 --> 01:04:46.440]   There's something so messed up that they can end up buying laws that should be completely
[01:04:46.440 --> 01:04:47.440]   illegal.
[01:04:47.440 --> 01:04:53.120]   Lobbyists should be eradicated and it should be people should be people and companies should
[01:04:53.120 --> 01:04:58.080]   not be people so that they end up they can't end up spending money to be able to buy laws
[01:04:58.080 --> 01:05:00.280]   so that it ends up protecting themselves.
[01:05:00.280 --> 01:05:01.280]   Yeah.
[01:05:01.280 --> 01:05:05.440]   Well no I mean I mean I we have to do financial results stories occasionally because you know
[01:05:05.440 --> 01:05:07.440]   there's a season for them.
[01:05:07.440 --> 01:05:14.680]   And some I mean Amazon in 2017 paid no federal income taxes or no federal corporation taxes.
[01:05:14.680 --> 01:05:19.520]   You know Microsoft destroyed Nokia and wrote the whole thing off against tax.
[01:05:19.520 --> 01:05:22.640]   It's an insane system we've got at the moment.
[01:05:22.640 --> 01:05:25.440]   And yeah I would say just get people to pay their taxes.
[01:05:25.440 --> 01:05:31.160]   If I'm one receipt out the IRS will be up my shelf like nothing else.
[01:05:31.160 --> 01:05:35.160]   But if you know you're a company you say I don't really want to pay a billion dollars
[01:05:35.160 --> 01:05:37.880]   of taxes so let's talk it over.
[01:05:37.880 --> 01:05:40.320]   This seems an obscure mismatch.
[01:05:40.320 --> 01:05:45.960]   Yes and they're paying less taxes than someone that's you know making forty thousand dollars.
[01:05:45.960 --> 01:05:47.200]   It's absolutely ridiculous.
[01:05:47.200 --> 01:05:48.200]   They should be paying more.
[01:05:48.200 --> 01:05:51.360]   A larger percentage they're using more resources.
[01:05:51.360 --> 01:05:55.400]   They're often already subsidized by the government that they're paying no taxes to.
[01:05:55.400 --> 01:05:59.280]   So we're subsidizing and making money for billion dollar companies.
[01:05:59.280 --> 01:06:00.280]   It's absolutely ridiculous.
[01:06:00.280 --> 01:06:03.240]   But we want to be that billionaire Georgia that's why we don't do anything.
[01:06:03.240 --> 01:06:06.400]   It's in our heart of hearts we think we'll be next and we want to get all.
[01:06:06.400 --> 01:06:10.160]   Yes that's what they that's what they they'd play to and then people with their own hopes
[01:06:10.160 --> 01:06:14.280]   and then they end up using something as a scapegoat so that we can be angry at that as
[01:06:14.280 --> 01:06:18.560]   they're doing the magic behind the curtain that no one's actually taking a look at because
[01:06:18.560 --> 01:06:22.680]   they're like oh it's these people are oh it's this thing and look over there.
[01:06:22.680 --> 01:06:26.480]   It's a fabulous shell game that unfortunately especially when we're angry and upset we'll
[01:06:26.480 --> 01:06:31.440]   give up all of our rights and we want to believe that because it's so much easier than looking
[01:06:31.440 --> 01:06:35.280]   at these very charismatic usually sociopathic.
[01:06:35.280 --> 01:06:36.800]   Not usually sorry.
[01:06:36.800 --> 01:06:42.280]   Often they are a larger percentage than general population of CEOs have you know narcissistic
[01:06:42.280 --> 01:06:46.320]   and sociopathic traits because they don't mind stepping on each other to be able to get
[01:06:46.320 --> 01:06:47.320]   their way up.
[01:06:47.320 --> 01:06:48.560]   They end up being profitable.
[01:06:48.560 --> 01:06:54.720]   And folks this is what happens when you have a bunch of Canadians on a show and there's
[01:06:54.720 --> 01:06:56.840]   no one to stand up for the American way.
[01:06:56.840 --> 01:06:58.800]   I'm standing up in the West and is Canadian.
[01:06:58.800 --> 01:07:02.200]   I mean the Reuters family is Canadian.
[01:07:02.200 --> 01:07:03.200]   Right.
[01:07:03.200 --> 01:07:04.200]   No I'm standing.
[01:07:04.200 --> 01:07:09.200]   Tom Sankind who said that the recent socialism never took off in America was because the poor
[01:07:09.200 --> 01:07:12.320]   convinced themselves they were temporarily embarrassed millionaires.
[01:07:12.320 --> 01:07:13.320]   Right.
[01:07:13.320 --> 01:07:14.320]   Right.
[01:07:14.320 --> 01:07:15.320]   I think that's exactly right.
[01:07:15.320 --> 01:07:16.320]   Yeah.
[01:07:16.320 --> 01:07:18.440]   I think also maybe that's tilting in windmills.
[01:07:18.440 --> 01:07:24.320]   I don't know how easy this is going to be to up end two centuries of capitalism.
[01:07:24.320 --> 01:07:25.320]   But maybe-
[01:07:25.320 --> 01:07:27.760]   It's not even let's just say it.
[01:07:27.760 --> 01:07:33.520]   Even if we kept capitalism before the 80s, before Reagan enacted the change to the tax
[01:07:33.520 --> 01:07:35.920]   system so that it's like trickle down economics.
[01:07:35.920 --> 01:07:37.800]   We figured out that doesn't work.
[01:07:37.800 --> 01:07:40.240]   Billionaires don't actually put money into the economy.
[01:07:40.240 --> 01:07:45.000]   The interesting thing is that you give $10 to someone that's at the grocery then they end
[01:07:45.000 --> 01:07:49.040]   up giving that $10 to the person that they end up getting a massage to and then they
[01:07:49.040 --> 01:07:53.480]   give $10 to the person that ends up doing their nails and they give $10 to be able to
[01:07:53.480 --> 01:07:58.200]   their babysitter, they tell that they're actually making $50 go through the system.
[01:07:58.200 --> 01:07:59.200]   Billionaires don't do that.
[01:07:59.200 --> 01:08:00.200]   You give them $10.
[01:08:00.200 --> 01:08:01.200]   They hoard it away.
[01:08:01.200 --> 01:08:02.200]   Right.
[01:08:02.200 --> 01:08:04.640]   They buy something that they're going to be keeping with it and they end up hoarding cash.
[01:08:04.640 --> 01:08:08.320]   So they've actually cost the economy $40.
[01:08:08.320 --> 01:08:09.880]   It isn't even $10.
[01:08:09.880 --> 01:08:14.920]   And so we've started to wake up to be able to figure out that oh they actually don't
[01:08:14.920 --> 01:08:15.920]   fuel the economy.
[01:08:15.920 --> 01:08:17.600]   They're doing the opposite of that.
[01:08:17.600 --> 01:08:20.720]   The middle class, the people that are poor than that are the ones that actually fuel
[01:08:20.720 --> 01:08:21.720]   the economy.
[01:08:21.720 --> 01:08:23.480]   So it isn't capitalism.
[01:08:23.480 --> 01:08:28.480]   This is something where they ended up taking care of their friends that are also other
[01:08:28.480 --> 01:08:33.520]   at the time, multimillionaires and now those multimillionaires are billionaires.
[01:08:33.520 --> 01:08:35.720]   And it did not help the economy at all.
[01:08:35.720 --> 01:08:40.440]   It shrunk the economy and that's why people are trying to find ways to be able to make
[01:08:40.440 --> 01:08:41.440]   ends meet.
[01:08:41.440 --> 01:08:42.440]   It's absolutely ridiculous.
[01:08:42.440 --> 01:08:51.440]   Wow, I didn't know that you had such strong commie leanings to be honest with you.
[01:08:51.440 --> 01:08:55.200]   No, I agree with you 100%.
[01:08:55.200 --> 01:08:57.800]   But I try to hide it.
[01:08:57.800 --> 01:08:58.800]   You know, I don't.
[01:08:58.800 --> 01:09:05.240]   That's a funny thing that they say that because if they took true capitalistic ideals, this
[01:09:05.240 --> 01:09:06.240]   wouldn't happen either.
[01:09:06.240 --> 01:09:07.240]   This is not capitalistic.
[01:09:07.240 --> 01:09:10.240]   It's actually communistic for billionaires.
[01:09:10.240 --> 01:09:11.240]   Right.
[01:09:11.240 --> 01:09:13.920]   So we have to fund all of the infrastructure.
[01:09:13.920 --> 01:09:18.480]   We, the taxpayers, the governments have funded all the infrastructure to be able to have these
[01:09:18.480 --> 01:09:19.480]   telecom companies.
[01:09:19.480 --> 01:09:26.040]   So we pay, we paid for it, got all the telecom companies set up and then they ended up taking
[01:09:26.040 --> 01:09:30.280]   a monopoly over it so that they could make money from that and then charge us who paid
[01:09:30.280 --> 01:09:34.040]   them to be able to do this more money to be able to take care of it.
[01:09:34.040 --> 01:09:38.080]   That's quite a communistic thought for them.
[01:09:38.080 --> 01:09:42.120]   And then for us, it has to be capitalism and pull up yourself up by the bootstraps, which
[01:09:42.120 --> 01:09:44.400]   is fine if you have bootstraps.
[01:09:44.400 --> 01:09:46.160]   Socialism for the rich.
[01:09:46.160 --> 01:09:47.600]   Capitalism for the rest of us.
[01:09:47.600 --> 01:09:48.600]   Yeah.
[01:09:48.600 --> 01:09:50.120]   Let's take a little break.
[01:09:50.120 --> 01:09:54.960]   I got to do a little capitalistic thing we call advertising, but we'll come back in just
[01:09:54.960 --> 01:09:56.920]   a little bit with our great panel.
[01:09:56.920 --> 01:10:03.320]   Renee Richie is here creating a liaison from YouTube, Georgia Dow, who is the awesomest.
[01:10:03.320 --> 01:10:07.560]   And I really appreciate your thoughts.
[01:10:07.560 --> 01:10:08.560]   YouTube.com/georgia-dow.
[01:10:08.560 --> 01:10:14.080]   She's also Georgia at Westmounttherapy.com and follow this stuff as making you nuts.
[01:10:14.080 --> 01:10:17.840]   You probably in your professional capacity don't call it nuts.
[01:10:17.840 --> 01:10:19.680]   No, no, anxious.
[01:10:19.680 --> 01:10:20.680]   No.
[01:10:20.680 --> 01:10:22.680]   We all have our thing, right?
[01:10:22.680 --> 01:10:24.680]   We're all a little nuts.
[01:10:24.680 --> 01:10:25.680]   We all have.
[01:10:25.680 --> 01:10:26.680]   Yeah, that makes it more interesting.
[01:10:26.680 --> 01:10:28.840]   We'll be boring if it wasn't.
[01:10:28.840 --> 01:10:29.840]   Oh, I love it.
[01:10:29.840 --> 01:10:32.680]   I love all our nuts.
[01:10:32.680 --> 01:10:36.960]   And Mr. Ian Thompson is nothing but nuts from the register.com.
[01:10:36.960 --> 01:10:42.120]   We're not for you to say so.
[01:10:42.120 --> 01:10:48.680]   Our show they brought to you by Cole Line with a K. Cole Line is a device trust solution
[01:10:48.680 --> 01:10:52.320]   that ensures unsecured devices can't access your apps.
[01:10:52.320 --> 01:10:54.360]   Now you might say, well, what do you mean?
[01:10:54.360 --> 01:10:55.840]   What is the wise ad important?
[01:10:55.840 --> 01:10:57.240]   Well, let me explain.
[01:10:57.240 --> 01:11:01.000]   This is actually ripped from the headlines.
[01:11:01.000 --> 01:11:02.440]   Remember LastPass?
[01:11:02.440 --> 01:11:05.400]   How did the LastPass hack happen?
[01:11:05.400 --> 01:11:09.360]   LastPass had security, right?
[01:11:09.360 --> 01:11:11.660]   They had a zero trust architecture.
[01:11:11.660 --> 01:11:14.720]   You couldn't get into the LastPass network unless you authenticated.
[01:11:14.720 --> 01:11:16.640]   You proved you were who you said you were.
[01:11:16.640 --> 01:11:20.880]   But what it didn't do is make sure that that device you were getting into the network
[01:11:20.880 --> 01:11:23.840]   with was also secure.
[01:11:23.840 --> 01:11:28.040]   And in fact, one of their DevOps guys had an out of date version of Plex that was unpached,
[01:11:28.040 --> 01:11:29.880]   had a zero day on it.
[01:11:29.880 --> 01:11:33.600]   And it let the bad guys into the network and the rest is history.
[01:11:33.600 --> 01:11:37.520]   This can be a huge cost to your business.
[01:11:37.520 --> 01:11:39.400]   Cole Line has some big news for you.
[01:11:39.400 --> 01:11:46.080]   If you're an Octa user, Cole Line can get your entire fleet to 100% compliance because
[01:11:46.080 --> 01:11:51.560]   Cole Line patches this major hole in zero trust device compliance.
[01:11:51.560 --> 01:11:52.560]   Think about it.
[01:11:52.560 --> 01:11:56.520]   Your identity provider only lets known devices log in apps, right?
[01:11:56.520 --> 01:12:00.120]   But just because a device is known doesn't mean it's in a secure state.
[01:12:00.120 --> 01:12:04.480]   In fact, plenty of the devices in your fleet probably shouldn't be trusted like that DevOps
[01:12:04.480 --> 01:12:06.320]   guys laptop.
[01:12:06.320 --> 01:12:10.560]   Maybe they're running on out of date versions of the OS or maybe they've got unencrypted
[01:12:10.560 --> 01:12:15.840]   credentials lying around in their downloads folder or maybe they've got Plex unpatched
[01:12:15.840 --> 01:12:18.240]   on their hard drive.
[01:12:18.240 --> 01:12:23.440]   If a device isn't compliant or isn't running the collide agent, it just can't access the
[01:12:23.440 --> 01:12:26.720]   organization SaaS apps or other resources.
[01:12:26.720 --> 01:12:31.520]   It can't log into your company's cloud apps until they fix the problem on their end.
[01:12:31.520 --> 01:12:32.520]   It's that simple.
[01:12:32.520 --> 01:12:37.120]   And it's not a burden your IT department because the user does the fixing.
[01:12:37.120 --> 01:12:41.640]   For example, if a device will be blocked if an employee doesn't have an up to date browser.
[01:12:41.640 --> 01:12:42.640]   So what happens?
[01:12:42.640 --> 01:12:44.520]   Cole Line tells the user your browser is out of date.
[01:12:44.520 --> 01:12:46.840]   Let me show you how to fix it.
[01:12:46.840 --> 01:12:52.680]   You got your end users going to do the remediation driving your fleet to 100% compliance but not
[01:12:52.680 --> 01:12:55.480]   overwhelming your IT team.
[01:12:55.480 --> 01:12:57.120]   Plus there's a great side effect to this.
[01:12:57.120 --> 01:13:01.560]   The user now is part of the security process.
[01:13:01.560 --> 01:13:07.960]   The user is now educated and helps you reach 100% compliance.
[01:13:07.960 --> 01:13:11.680]   If you don't have Cole Line, you got no way to solve these compliance issues or stop
[01:13:11.680 --> 01:13:13.800]   insecure devices from logging in.
[01:13:13.800 --> 01:13:19.120]   If you have Cole Line, you can set and enforce compliance and oh by the way, completely cross
[01:13:19.120 --> 01:13:23.000]   platform Mac, Windows and Linux.
[01:13:23.000 --> 01:13:24.000]   Cole Line's unique.
[01:13:24.000 --> 01:13:27.960]   It makes device compliance part of the authentication process.
[01:13:27.960 --> 01:13:33.720]   So a user, when a user is logging in with Okta, Cole Line will then alert them to compliance
[01:13:33.720 --> 01:13:38.240]   issues and prevent them from logging in if they're not secure.
[01:13:38.240 --> 01:13:41.840]   And it's security you can feel good about because Cole Line puts transparency and respect
[01:13:41.840 --> 01:13:44.800]   for users at the center of their product.
[01:13:44.800 --> 01:13:46.400]   It's the best way to do this.
[01:13:46.400 --> 01:13:51.040]   To sum up, Cole Line's method means fewer support tickets, less frustration and most
[01:13:51.040 --> 01:13:53.880]   importantly 100% fleet compliance.
[01:13:53.880 --> 01:14:00.120]   K-O-L-I-D-E, visit colyde.com/twit to learn more or book a demo.
[01:14:00.120 --> 01:14:07.680]   Zero Trust for Okta, K-O-L-I-D-E.com/twit.
[01:14:07.680 --> 01:14:11.680]   And I have to say there's more than a few companies who right about now wish they had
[01:14:11.680 --> 01:14:13.840]   been using Coleide.
[01:14:13.840 --> 01:14:16.360]   Go to colyde.com/twit.
[01:14:16.360 --> 01:14:20.440]   We thank you so much for the support of this week in tech.
[01:14:20.440 --> 01:14:24.120]   I'm going to be on vacation next week and the week after and the week after that.
[01:14:24.120 --> 01:14:30.880]   But we've got some great shows planned for you with co-hosts, replacement hosts we've
[01:14:30.880 --> 01:14:33.480]   brought in next week.
[01:14:33.480 --> 01:14:38.720]   Michael Sargent will be hosting Christina Warren, Abral Al-Hidi and Alex Wilhelm.
[01:14:38.720 --> 01:14:41.640]   The following week, Devinda Harderwar will be our guest host.
[01:14:41.640 --> 01:14:42.640]   He's done it before.
[01:14:42.640 --> 01:14:43.640]   He's great.
[01:14:43.640 --> 01:14:47.520]   He's been on the panel lined up for you including Anthony Ha and Nicholas De Leon.
[01:14:47.520 --> 01:14:52.720]   And Jason Howell takes over on the 16th with Dan Patterson and Jason Heiner and Aunt Pruitt.
[01:14:52.720 --> 01:15:00.840]   I will be back on the 23rd and we will resume our previously scheduled programming.
[01:15:00.840 --> 01:15:02.640]   But thank you to Mike, Devinda and Jason.
[01:15:02.640 --> 01:15:08.080]   They're going to fill in for me for the next few weeks.
[01:15:08.080 --> 01:15:12.040]   I wish I were going to Montreal to get a bagel, but unfortunately.
[01:15:12.040 --> 01:15:14.440]   Anytime, Leo, thank you for the best.
[01:15:14.440 --> 01:15:16.240]   One of these days.
[01:15:16.240 --> 01:15:17.240]   Yeah.
[01:15:17.240 --> 01:15:19.240]   One of these days I swear.
[01:15:19.240 --> 01:15:24.440]   I've got a standing invite to a restaurant in Quebec from a Twit Viewer who was in the
[01:15:24.440 --> 01:15:25.440]   studio.
[01:15:25.440 --> 01:15:26.440]   No.
[01:15:26.440 --> 01:15:27.440]   And he owns a restaurant.
[01:15:27.440 --> 01:15:28.440]   Oh, do you know what your restaurant?
[01:15:28.440 --> 01:15:29.440]   Oh, wonderful.
[01:15:29.440 --> 01:15:31.480]   I've got to look it up.
[01:15:31.480 --> 01:15:35.520]   But no, he's a quite a quite a quite a quiet, quite a quiet, quiet owner.
[01:15:35.520 --> 01:15:37.880]   He came to the studio.
[01:15:37.880 --> 01:15:41.320]   His I suspect long suffering wife sat with him.
[01:15:41.320 --> 01:15:46.280]   But no, I mean, he really enjoyed the show and he was just like, anytime the Twit crew
[01:15:46.280 --> 01:15:48.280]   and Quebec, look us up.
[01:15:48.280 --> 01:15:49.280]   Nice.
[01:15:49.280 --> 01:15:51.000]   Here comes the TikTok band.
[01:15:51.000 --> 01:15:58.400]   Did any of you watch the congressional testimony on Thursday of a TikTok CEO?
[01:15:58.400 --> 01:15:59.400]   I started.
[01:15:59.400 --> 01:16:03.120]   That was a breakfast time here in California.
[01:16:03.120 --> 01:16:10.680]   And it was pretty hard for me to keep my breakfast down while I'm watching.
[01:16:10.680 --> 01:16:16.160]   And you know, look, I understand I'm not a fan of the People's Republic of China's Communist
[01:16:16.160 --> 01:16:17.160]   Party.
[01:16:17.160 --> 01:16:21.320]   I understand that they repress minorities.
[01:16:21.320 --> 01:16:25.920]   They are not a human rights bastion of democracy.
[01:16:25.920 --> 01:16:37.160]   But I don't know if banning TikTok is going to solve any of that CEO show GZ Chu who was
[01:16:37.160 --> 01:16:39.080]   Harvard educated Singaporean.
[01:16:39.080 --> 01:16:41.360]   It's not Chinese.
[01:16:41.360 --> 01:16:50.360]   It's been CEO of TikTok since 2021 was just bombarded from both the Democrats and the
[01:16:50.360 --> 01:16:58.320]   Republicans on the committee with practically insulting him.
[01:16:58.320 --> 01:17:08.560]   At one point, one of the members of Congress said, well, do you use your customers Wi-Fi
[01:17:08.560 --> 01:17:14.520]   and the CEO said, Chu said, well, what do you mean?
[01:17:14.520 --> 01:17:18.760]   Well, when I'm using TikTok at my house, are you looking at my Wi-Fi?
[01:17:18.760 --> 01:17:24.440]   Which, Chu said, well, yeah, that's how it works.
[01:17:24.440 --> 01:17:30.880]   I mean, around table count, how many people here have TikTok on their personal phones?
[01:17:30.880 --> 01:17:31.880]   I do.
[01:17:31.880 --> 01:17:32.880]   I'm the only one.
[01:17:32.880 --> 01:17:35.640]   Oh, and, okay, we have to.
[01:17:35.640 --> 01:17:38.320]   But Renee probably has to for work, right?
[01:17:38.320 --> 01:17:39.320]   Yes.
[01:17:39.320 --> 01:17:44.720]   Well, I mean, I use it for work, but I also want to create her and I post on every platform.
[01:17:44.720 --> 01:17:52.080]   I put my videos on Twitter on Twitter on Twitter on Instagram on TikTok on Facebook.
[01:17:52.080 --> 01:17:53.080]   Congress.
[01:17:53.080 --> 01:17:58.480]   I mean, after this, it looks pretty much like there's nonpartisan bipartisan support for
[01:17:58.480 --> 01:17:59.800]   a ban on TikTok.
[01:17:59.800 --> 01:18:01.000]   I mean, there's no question.
[01:18:01.000 --> 01:18:03.760]   And now it sounds like Ian and Georgia agree, right?
[01:18:03.760 --> 01:18:06.560]   Do you think TikTok should be bandying?
[01:18:06.560 --> 01:18:12.600]   I don't think you can honestly ban software that way without taking draconian steps to
[01:18:12.600 --> 01:18:13.600]   do so.
[01:18:13.600 --> 01:18:15.280]   I would say I'm not touching.
[01:18:15.280 --> 01:18:17.200]   I mean, I used TikTok on a burner phone.
[01:18:17.200 --> 01:18:20.360]   You know, it's the same phone I take to Defcon and the rest of it.
[01:18:20.360 --> 01:18:23.200]   You know, I don't want that on my personal phone.
[01:18:23.200 --> 01:18:26.720]   I think honestly, Scott Galloway this week was very strong on this.
[01:18:26.720 --> 01:18:31.480]   And he was saying, look, if you've got, I mean, and we know that Facebook has done research
[01:18:31.480 --> 01:18:37.200]   on this, you can influence people's thought process, or not thought process, but levels
[01:18:37.200 --> 01:18:38.200]   of happiness.
[01:18:38.200 --> 01:18:39.200]   But wait a minute.
[01:18:39.200 --> 01:18:40.320]   I face, but wait a minute.
[01:18:40.320 --> 01:18:41.320]   You just said it.
[01:18:41.320 --> 01:18:42.880]   Facebook's done it.
[01:18:42.880 --> 01:18:43.880]   Yeah.
[01:18:43.880 --> 01:18:45.440]   So what's been Facebook?
[01:18:45.440 --> 01:18:49.280]   Well, let's ban Facebook then.
[01:18:49.280 --> 01:18:50.280]   Facebook's actually done it.
[01:18:50.280 --> 01:18:52.400]   We don't have any evidence of TikTok ads.
[01:18:52.400 --> 01:18:54.160]   Facebook we know has.
[01:18:54.160 --> 01:18:56.520]   Are you proposing that Facebook should be banned?
[01:18:56.520 --> 01:18:58.920]   It was an egotistical sociopath.
[01:18:58.920 --> 01:19:02.040]   Oh, so you should be the Chinese government.
[01:19:02.040 --> 01:19:04.960]   And you know, there are a direct competitor to the US.
[01:19:04.960 --> 01:19:10.440]   I don't think banning it would work, but I do think I am very worried about, you know,
[01:19:10.440 --> 01:19:12.720]   the level of influence that it has.
[01:19:12.720 --> 01:19:18.000]   And I wish America could get a home-grown solution which worked as well.
[01:19:18.000 --> 01:19:23.360]   Is Congress going to create a US national TikTok?
[01:19:23.360 --> 01:19:26.200]   Give it all to Larry Ellis.
[01:19:26.200 --> 01:19:28.200]   And it's going to take off like crazy.
[01:19:28.200 --> 01:19:29.200]   Yeah.
[01:19:29.200 --> 01:19:32.080]   One thing I wanted to raise, like, and this is, again, this is personal opinion.
[01:19:32.080 --> 01:19:33.880]   I'm not speaking for anybody but myself here.
[01:19:33.880 --> 01:19:38.160]   But we had discussions on Macbreak for years about this when the French government wanted
[01:19:38.160 --> 01:19:42.120]   Microsoft to put, I think, it was health data or something on French servers.
[01:19:42.120 --> 01:19:45.880]   They didn't want information about French citizens to be on American servers.
[01:19:45.880 --> 01:19:51.240]   We had weeks and weeks, months of discussion about China forcing Apple to put Chinese citizens
[01:19:51.240 --> 01:19:54.760]   data, like iCloud data on Chinese servers.
[01:19:54.760 --> 01:19:58.960]   Now you have like the US wanting TikTok data to be on US servers.
[01:19:58.960 --> 01:20:04.120]   Data repatriation is a huge trend where a lot of countries post Snowden, you know, post
[01:20:04.120 --> 01:20:06.800]   China, like through a lot of different things.
[01:20:06.800 --> 01:20:11.360]   Just the internet is not like this big international, like, like a hippie thing anymore.
[01:20:11.360 --> 01:20:14.440]   A lot of countries are looking at it and saying, but we want our laws.
[01:20:14.440 --> 01:20:15.440]   We want our people.
[01:20:15.440 --> 01:20:16.440]   We want our control.
[01:20:16.440 --> 01:20:17.440]   We want our servers.
[01:20:17.440 --> 01:20:18.440]   We want our companies.
[01:20:18.440 --> 01:20:20.960]   And I fear we're racing towards that.
[01:20:20.960 --> 01:20:22.960]   Like we're starting to decompose the internet.
[01:20:22.960 --> 01:20:23.960]   Splintering this.
[01:20:23.960 --> 01:20:24.960]   Back into regional feudalism.
[01:20:24.960 --> 01:20:25.960]   Yeah.
[01:20:25.960 --> 01:20:26.960]   Yeah.
[01:20:26.960 --> 01:20:29.760]   Well, and Neil Roth, who was a former head of trust and safety at Twitter until Elon
[01:20:29.760 --> 01:20:35.920]   Fire and Mrotto interesting piece on Tech Dirt saying how forcing TikTok to completely
[01:20:35.920 --> 01:20:40.880]   separate its US operations could actually undermine national security.
[01:20:40.880 --> 01:20:46.880]   His point is when he was at Twitter, they were only able to really discover disinformation
[01:20:46.880 --> 01:20:50.120]   campaigns by looking at the global data set.
[01:20:50.120 --> 01:20:52.360]   It was very, it makes a very strong case.
[01:20:52.360 --> 01:20:58.080]   It was very important to see all the data at once in order to find disinformation campaigns,
[01:20:58.080 --> 01:21:05.440]   Iranian accounts, for instance, posting disinformation about Biden and Trump during the election.
[01:21:05.440 --> 01:21:09.280]   If you take TikTok and you say, okay, we're going to separate off all the US data and
[01:21:09.280 --> 01:21:15.240]   that's going to be hosted by Oracle and Texas, you may actually undermine its ability to
[01:21:15.240 --> 01:21:18.160]   discover disinformation campaigns.
[01:21:18.160 --> 01:21:24.720]   It's not as clear cut as members of Congress who don't understand how the internet works
[01:21:24.720 --> 01:21:25.720]   might think.
[01:21:25.720 --> 01:21:26.720]   Go ahead, George.
[01:21:26.720 --> 01:21:27.720]   I cut you off.
[01:21:27.720 --> 01:21:28.720]   No, no, no.
[01:21:28.720 --> 01:21:32.240]   I don't think that we can really ban TikTok.
[01:21:32.240 --> 01:21:33.400]   I don't think that that works out.
[01:21:33.400 --> 01:21:38.120]   I do think that government officials shouldn't be bringing TikTok into government buildings.
[01:21:38.120 --> 01:21:39.720]   Oh, I don't disagree with that.
[01:21:39.720 --> 01:21:44.480]   I mean, it's the right of the Department of Defense or any government to say, not on our
[01:21:44.480 --> 01:21:45.480]   phones.
[01:21:45.480 --> 01:21:49.240]   Yeah, not on our phones, not on our computers.
[01:21:49.240 --> 01:21:53.080]   If they want to block TikTok on the Wi-Fi, sure.
[01:21:53.080 --> 01:21:54.080]   But that's a lot of...
[01:21:54.080 --> 01:21:58.320]   Well, that's a big fan of phobies being taken into their office.
[01:21:58.320 --> 01:21:59.320]   I mean, TikTok is...
[01:21:59.320 --> 01:22:01.320]   Have you seen a naked phirby?
[01:22:01.320 --> 01:22:02.320]   Those things are scary.
[01:22:02.320 --> 01:22:04.320]   I totally got that.
[01:22:04.320 --> 01:22:06.320]   Why have you seen a naked phirby?
[01:22:06.320 --> 01:22:09.360]   That's something I've never thought I'd be asked.
[01:22:09.360 --> 01:22:10.360]   You see, well, there we go.
[01:22:10.360 --> 01:22:11.360]   There's always a first.
[01:22:11.360 --> 01:22:13.640]   That's why you know that they've been banned.
[01:22:13.640 --> 01:22:21.560]   Yeah, no, I'm not saying that if you're a spy, you should have TikTok on your phone.
[01:22:21.560 --> 01:22:22.560]   But I want to protect the right...
[01:22:22.560 --> 01:22:23.560]   You should have anything on your phone.
[01:22:23.560 --> 01:22:25.880]   Yeah, you probably should have a smartphone.
[01:22:25.880 --> 01:22:29.920]   Yeah, yeah.
[01:22:29.920 --> 01:22:31.560]   I just feel like that's none of their...
[01:22:31.560 --> 01:22:33.840]   Again, that seems to me kind of government overreach.
[01:22:33.840 --> 01:22:37.760]   What businesses are there as whether I have TikTok on my phone or not?
[01:22:37.760 --> 01:22:40.640]   You've done nothing to protect my privacy, Congress.
[01:22:40.640 --> 01:22:46.520]   In fact, you've done everything to undermine my privacy thanks to extensive lobbying efforts
[01:22:46.520 --> 01:22:48.760]   by the telecommunications industry.
[01:22:48.760 --> 01:22:52.600]   So now you want to ban TikTok to protect me?
[01:22:52.600 --> 01:22:54.600]   Seems like they want to protect Facebook.
[01:22:54.600 --> 01:22:58.560]   Well, it's the K-pop stands.
[01:22:58.560 --> 01:23:00.200]   They're terrified of the K-pop stands.
[01:23:00.200 --> 01:23:01.200]   Maybe.
[01:23:01.200 --> 01:23:02.200]   They're slightly joking.
[01:23:02.200 --> 01:23:03.200]   There's reasons to...
[01:23:03.200 --> 01:23:04.200]   That's a young person's movement.
[01:23:04.200 --> 01:23:05.200]   Yeah, of course.
[01:23:05.200 --> 01:23:08.520]   Well, that's why Trump, I think that is probably one of the reasons Trump wanted to...
[01:23:08.520 --> 01:23:12.280]   This is by the way been going on since President Trump wanted to ban them.
[01:23:12.280 --> 01:23:17.260]   At the time, I thought he wanted to ban them because TikTok was used to organize K-pop
[01:23:17.260 --> 01:23:20.400]   stands to undermine his rallies.
[01:23:20.400 --> 01:23:24.480]   But honestly, I think he had the same motivations as the Biden administration does.
[01:23:24.480 --> 01:23:26.520]   They're concerned about privacy.
[01:23:26.520 --> 01:23:28.080]   But I think they're also concerned about...
[01:23:28.080 --> 01:23:31.080]   This is a weird thing.
[01:23:31.080 --> 01:23:32.080]   And they just really talked about it.
[01:23:32.080 --> 01:23:39.800]   The hearing protecting our teenagers against the malign influence of social media.
[01:23:39.800 --> 01:23:43.800]   Well, you saw the USAR laws around this this week.
[01:23:43.800 --> 01:23:44.800]   Yeah.
[01:23:44.800 --> 01:23:45.800]   Yeah.
[01:23:45.800 --> 01:23:46.800]   That's just...
[01:23:46.800 --> 01:23:51.840]   Look, I applaud it in a way, but at the same time, it's completely unrealistic and
[01:23:51.840 --> 01:23:52.840]   unworkable.
[01:23:52.840 --> 01:24:00.520]   The idea that Utah can say that its teenagers cannot use social media between 10.30pm and
[01:24:00.520 --> 01:24:05.280]   6.30am, it's totally unenforceable.
[01:24:05.280 --> 01:24:07.520]   It makes no sense whatsoever.
[01:24:07.520 --> 01:24:14.480]   Yes, social media may have a detrimental effect on people's mental health.
[01:24:14.480 --> 01:24:18.320]   But the idea that you can just pass the law and expect everyone to follow it is utterly
[01:24:18.320 --> 01:24:19.320]   bonkers.
[01:24:19.320 --> 01:24:25.400]   By the way, the law of the state now, if you're under 18, you're barred from using social
[01:24:25.400 --> 01:24:30.200]   media between the hours of 10.30 and 6.30.
[01:24:30.200 --> 01:24:34.400]   They will require age verification for anyone who wants to use social media.
[01:24:34.400 --> 01:24:39.000]   You have to prove that you're over 18.
[01:24:39.000 --> 01:24:42.560]   And anybody under 18, you'll need parental consent before you can sign up for TikTok or
[01:24:42.560 --> 01:24:44.360]   Instagram or Facebook.
[01:24:44.360 --> 01:24:45.720]   That is actually the law of the state.
[01:24:45.720 --> 01:24:48.720]   The governor of Utah said, "We know kids are smart.
[01:24:48.720 --> 01:24:49.720]   We'll be able to get around this."
[01:24:49.720 --> 01:24:55.360]   What we really need is a federal law doing the same thing, banning social media.
[01:24:55.360 --> 01:24:57.800]   Here is the creep signing the bill.
[01:24:57.800 --> 01:24:58.800]   I don't know.
[01:24:58.800 --> 01:25:01.760]   Hang on, this is from the party of small governments.
[01:25:01.760 --> 01:25:02.760]   I got this wrong.
[01:25:02.760 --> 01:25:03.760]   He looks so happy.
[01:25:03.760 --> 01:25:04.760]   There are all.
[01:25:04.760 --> 01:25:05.760]   It's a bill joke.
[01:25:05.760 --> 01:25:08.800]   You want to small enough just to get into your bedroom?
[01:25:08.800 --> 01:25:09.800]   Yeah, right.
[01:25:09.800 --> 01:25:10.800]   Exactly.
[01:25:10.800 --> 01:25:15.960]   The party of small government turns out to be the party of big government knocking at
[01:25:15.960 --> 01:25:21.480]   your door for your social network and your reproductive rights and on and on and on.
[01:25:21.480 --> 01:25:23.440]   I'm sorry, I'm being political, but that's the party of small government.
[01:25:23.440 --> 01:25:24.440]   No facts.
[01:25:24.440 --> 01:25:25.440]   It's just power.
[01:25:25.440 --> 01:25:26.440]   It's all power, Leo.
[01:25:26.440 --> 01:25:28.680]   It's just like who has it, who wants it, who can get it, how they can manipulate to
[01:25:28.680 --> 01:25:29.680]   maintain it.
[01:25:29.680 --> 01:25:30.960]   I understand people's concern.
[01:25:30.960 --> 01:25:35.160]   I also think, though, you can't ignore how valuable TikTok has been.
[01:25:35.160 --> 01:25:40.520]   Look, I admit I'm biased because my son has started his career thanks to TikTok.
[01:25:40.520 --> 01:25:42.920]   He is now happily using YouTube shorts and Instagram reels.
[01:25:42.920 --> 01:25:45.440]   I see his collabs with other really big chefs.
[01:25:45.440 --> 01:25:46.440]   It's amazing.
[01:25:46.440 --> 01:25:47.440]   I saw him on Joshua Weisman's channel.
[01:25:47.440 --> 01:25:48.440]   He's everywhere.
[01:25:48.440 --> 01:25:49.440]   Yeah.
[01:25:49.440 --> 01:25:53.880]   He's going to Mexico, going to Oaxaca to work with a chef next month.
[01:25:53.880 --> 01:25:57.080]   It's going to be a chapter in his new cookbook.
[01:25:57.080 --> 01:25:58.360]   Thank you, TikTok.
[01:25:58.360 --> 01:26:00.520]   This has happened in a year.
[01:26:00.520 --> 01:26:01.800]   I remember having lunch with him.
[01:26:01.800 --> 01:26:05.240]   He said, "Well, I got about 30,000 on my TikTok page followers.
[01:26:05.240 --> 01:26:07.360]   Do you think I should pursue this?"
[01:26:07.360 --> 01:26:09.400]   I said, "Well, I'm the wrong guy to ask.
[01:26:09.400 --> 01:26:11.480]   I thought podcasting was going to be big.
[01:26:11.480 --> 01:26:12.480]   Go ahead.
[01:26:12.480 --> 01:26:13.480]   Give it a try."
[01:26:13.480 --> 01:26:15.480]   It was for a period of time.
[01:26:15.480 --> 01:26:19.000]   It's growing now.
[01:26:19.000 --> 01:26:22.760]   Within a year, he got to 2.1 million TikTok followers.
[01:26:22.760 --> 01:26:24.320]   Now, I've asked him.
[01:26:24.320 --> 01:26:26.640]   I said, "Well, would a ban on TikTok impact you?"
[01:26:26.640 --> 01:26:28.000]   He said, "Not anymore."
[01:26:28.000 --> 01:26:30.600]   But it would certainly impact the people behind him, right?
[01:26:30.600 --> 01:26:34.720]   Here's an article from the Washington Post Taylor Lauren's writing, "Hollywood and the
[01:26:34.720 --> 01:26:39.480]   music industry brace for a TikTok band.
[01:26:39.480 --> 01:26:40.560]   They are not happy either.
[01:26:40.560 --> 01:26:45.280]   The entertainment industry has become so reliant on TikTok that banning the app could hurt business
[01:26:45.280 --> 01:26:47.280]   industry insiders say."
[01:26:47.280 --> 01:26:50.400]   She quotes David Maho as a film director in Brooklyn.
[01:26:50.400 --> 01:26:51.960]   Never had the money to go to film school.
[01:26:51.960 --> 01:26:55.200]   He liked making films, but he could never get a job in Hollywood.
[01:26:55.200 --> 01:27:03.720]   So he joined TikTok in 2020, a master following for his unique directorial style, studio executives
[01:27:03.720 --> 01:27:10.320]   and Hollywood bigwigs noticed, and he is landing directing jobs in Hollywood.
[01:27:10.320 --> 01:27:12.400]   He says, "I was directing commercials.
[01:27:12.400 --> 01:27:15.480]   You're directing social media stuff, music, music, commercial directors."
[01:27:15.480 --> 01:27:19.000]   Let's not forget, Congress wanted to ban music videos too for a while.
[01:27:19.000 --> 01:27:20.200]   I was never on the radar.
[01:27:20.200 --> 01:27:21.200]   I was not real music.
[01:27:21.200 --> 01:27:22.200]   Yeah, rock.
[01:27:22.200 --> 01:27:23.200]   Yeah.
[01:27:23.200 --> 01:27:24.200]   Cut your hair, yeah.
[01:27:24.200 --> 01:27:25.200]   Eppy.
[01:27:25.200 --> 01:27:28.320]   I was never on the radar in places like Netflix or HBO Max or Paramount," he said.
[01:27:28.320 --> 01:27:30.160]   This is Taylor Lauren's quoting him.
[01:27:30.160 --> 01:27:34.640]   "Since I've been able to create work on the platform, my work has reached studio executives
[01:27:34.640 --> 01:27:35.800]   and marketing departments.
[01:27:35.800 --> 01:27:39.840]   TikTok allowed me to build that network without having the roster or resume."
[01:27:39.840 --> 01:27:40.840]   So that's two examples.
[01:27:40.840 --> 01:27:45.000]   My son and this guy, David Ma, but there's all of them.
[01:27:45.000 --> 01:27:46.000]   I would give you another.
[01:27:46.000 --> 01:27:47.000]   Yeah.
[01:27:47.000 --> 01:27:48.240]   No, I'd give you another in the form of Deadpool.
[01:27:48.240 --> 01:27:51.480]   We wouldn't have had a Deadpool, an original Deadpool movie.
[01:27:51.480 --> 01:27:57.520]   If they hadn't shot a test scene, put it out on the internet.
[01:27:57.520 --> 01:27:58.520]   No kidding.
[01:27:58.520 --> 01:28:00.520]   It just went mega.
[01:28:00.520 --> 01:28:06.320]   Talking to when they've given interviews about this, it's just like, if we hadn't leaked
[01:28:06.320 --> 01:28:10.200]   that footage online, then the movie would have never happened.
[01:28:10.200 --> 01:28:13.840]   They were never going to do an 18-rated superhero movie.
[01:28:13.840 --> 01:28:14.840]   Yeah.
[01:28:14.840 --> 01:28:19.000]   And now we've had one really good, one okay-ish movie out of it.
[01:28:19.000 --> 01:28:20.000]   So, you know.
[01:28:20.000 --> 01:28:21.000]   Well, I think the car was--
[01:28:21.000 --> 01:28:22.000]   I'm the car director of the segment.
[01:28:22.000 --> 01:28:28.640]   --which is like all of our entertainment to be up with people and Doris Day and Rock Hudson.
[01:28:28.640 --> 01:28:29.640]   Oh, wait a minute.
[01:28:29.640 --> 01:28:30.640]   He was gay.
[01:28:30.640 --> 01:28:32.000]   Stop that.
[01:28:32.000 --> 01:28:33.000]   So it's just hopeless.
[01:28:33.000 --> 01:28:34.000]   It's just a confirmed bachelor.
[01:28:34.000 --> 01:28:35.000]   Well, it's also--
[01:28:35.000 --> 01:28:36.000]   It's also--
[01:28:36.000 --> 01:28:41.640]   It's also that they can't grab the information from TikTok to be able to use for their own
[01:28:41.640 --> 01:28:47.440]   uses where they can actually subpoena and get information from Facebook and other American
[01:28:47.440 --> 01:28:48.440]   companies.
[01:28:48.440 --> 01:28:54.000]   So, I think that it's a little bit more self-serving than just trying to protect children from
[01:28:54.000 --> 01:28:55.000]   TikTok.
[01:28:55.000 --> 01:28:57.960]   Because if they really did want to do that, they would be doing that too.
[01:28:57.960 --> 01:29:01.360]   I do get, though, every time we talk about this, I get an email from people who say,
[01:29:01.360 --> 01:29:03.200]   "You must love China.
[01:29:03.200 --> 01:29:04.200]   Don't you know they're--"
[01:29:04.200 --> 01:29:05.200]   You know, they are our enemy.
[01:29:05.200 --> 01:29:09.480]   They're trying to invade our brains and take over our children.
[01:29:09.480 --> 01:29:12.240]   And I mean, what--
[01:29:12.240 --> 01:29:15.480]   Why don't people also say, "Well, China blends all US social networks so that the US should
[01:29:15.480 --> 01:29:16.480]   ban all Chinese social networks?"
[01:29:16.480 --> 01:29:17.480]   Yeah.
[01:29:17.480 --> 01:29:18.960]   Because that's actually a trade war that has nothing to do with--
[01:29:18.960 --> 01:29:19.960]   Right.
[01:29:19.960 --> 01:29:23.760]   --with like privacy, I think it's just part of the ongoing trade war.
[01:29:23.760 --> 01:29:25.480]   Yeah.
[01:29:25.480 --> 01:29:27.080]   And you raised a good point, Georgia.
[01:29:27.080 --> 01:29:31.720]   How do you ban an app?
[01:29:31.720 --> 01:29:32.720]   You can't.
[01:29:32.720 --> 01:29:37.840]   It's also, I find it very disingenuous and disrespectful to parents.
[01:29:37.840 --> 01:29:39.440]   This is something that--
[01:29:39.440 --> 01:29:43.320]   Which is one of the things that Republicans could want, that you should keep governments
[01:29:43.320 --> 01:29:47.720]   outside of your house and outside of your bedroom and also outside of your parenting.
[01:29:47.720 --> 01:29:51.200]   I think that it is up to parents to be able to decide what they want their children to
[01:29:51.200 --> 01:29:52.200]   do and not do.
[01:29:52.200 --> 01:29:53.560]   And I think that they should be dealing with it.
[01:29:53.560 --> 01:29:57.360]   I think that if it's something about something that goes all as far as health and safety,
[01:29:57.360 --> 01:30:00.720]   then maybe the government should step in to make sure that they're--
[01:30:00.720 --> 01:30:02.160]   All of their people are safe.
[01:30:02.160 --> 01:30:08.200]   But this isn't-- I don't really believe them because this would be something that would
[01:30:08.200 --> 01:30:09.200]   be far and wide.
[01:30:09.200 --> 01:30:13.880]   I think that this is about information leaking out and not wanting China to be able to have
[01:30:13.880 --> 01:30:16.680]   information and to be able to deal with that.
[01:30:16.680 --> 01:30:19.880]   But it's absolutely not going to be able to be done.
[01:30:19.880 --> 01:30:20.880]   So why bother?
[01:30:20.880 --> 01:30:22.960]   It's just going to harm the middle people.
[01:30:22.960 --> 01:30:23.960]   Right.
[01:30:23.960 --> 01:30:27.760]   I mean, we have the same problem in the UK because the UK government is now trying to ban
[01:30:27.760 --> 01:30:35.000]   end to end encryption without side-scanning signal, as said, they're moving out the country.
[01:30:35.000 --> 01:30:41.400]   A couple of other app vendors have said, basically, yeah, ban us if you want.
[01:30:41.400 --> 01:30:44.800]   We're still not going to block people from downloading and using this stuff.
[01:30:44.800 --> 01:30:51.480]   So you run up against the reality of software and governments have very limited powers in
[01:30:51.480 --> 01:30:52.480]   this respect.
[01:30:52.480 --> 01:30:55.320]   There are also websites, like their web pages as well.
[01:30:55.320 --> 01:30:58.880]   So you can force a company like Apple to take you off the App Store.
[01:30:58.880 --> 01:31:00.600]   But there's still a website behind it.
[01:31:00.600 --> 01:31:03.560]   And if you start banning websites, then what have we become?
[01:31:03.560 --> 01:31:06.000]   And there's VPNs and there's ways around.
[01:31:06.000 --> 01:31:11.320]   There's no way to be able to enforce this.
[01:31:11.320 --> 01:31:15.720]   Often when people post pictures of their driver's license, their credit cards, they
[01:31:15.720 --> 01:31:17.800]   redact the image.
[01:31:17.800 --> 01:31:20.640]   They draw a black bar across the credit card number.
[01:31:20.640 --> 01:31:23.920]   Sometimes people take pictures and crop them to only--
[01:31:23.920 --> 01:31:30.640]   well, some years ago at Tech TV, I worked with a host on one of our shows who had a picture.
[01:31:30.640 --> 01:31:32.760]   She had a professional photographer taking pictures of her.
[01:31:32.760 --> 01:31:36.520]   Her picture she loved as a headshot.
[01:31:36.520 --> 01:31:43.280]   The rest of the image was nude, but so she decided to crop it down to just her head and
[01:31:43.280 --> 01:31:44.960]   use that as her avatar.
[01:31:44.960 --> 01:31:49.760]   But somebody realized that the crop didn't delete the original photo data.
[01:31:49.760 --> 01:31:50.760]   And unfortunately--
[01:31:50.760 --> 01:31:51.760]   Oh, no.
[01:31:51.760 --> 01:31:54.640]   That was kind of a horrible thing.
[01:31:54.640 --> 01:31:56.840]   The nude picture was released.
[01:31:56.840 --> 01:32:00.160]   Turns out, 20 years later, it's still a problem.
[01:32:00.160 --> 01:32:02.760]   They're calling it a crop ellipse.
[01:32:02.760 --> 01:32:06.280]   We have some fun with headlines on this screen.
[01:32:06.280 --> 01:32:07.680]   That's all you can--
[01:32:07.680 --> 01:32:15.040]   First discovered in the Google Pixel's in-built screenshot editing tool markup, allowing partial
[01:32:15.040 --> 01:32:20.400]   recovery of the original unedited image of a cropped and/or redacted photo.
[01:32:20.400 --> 01:32:25.200]   And Microsoft took a look at its snipping tool and said, "Oh, whoops.
[01:32:25.200 --> 01:32:27.440]   I suspect a lot of cropping tools do that."
[01:32:27.440 --> 01:32:31.120]   Because it's the easiest thing to do is just say, "Well, no, the image is right here,"
[01:32:31.120 --> 01:32:36.720]   instead of recreating the image with the other material deleted.
[01:32:36.720 --> 01:32:41.200]   And I bet there are other cropping tools that this is going to crop up, if you will.
[01:32:41.200 --> 01:32:45.200]   Well, I mean, it originally popped up with Google, and then it's spread to--
[01:32:45.200 --> 01:32:48.200]   and then it says, "Hang on this works with Microsoft as well."
[01:32:48.200 --> 01:32:50.400]   And we'll dormant some interesting stuff on this.
[01:32:50.400 --> 01:32:51.760]   And they're basically just--
[01:32:51.760 --> 01:32:53.640]   they're not even overwriting the data.
[01:32:53.640 --> 01:32:54.160]   No.
[01:32:54.160 --> 01:32:56.000]   It's fundamentally bad security.
[01:32:56.000 --> 01:32:57.280]   Well, it's to save time.
[01:32:57.280 --> 01:32:58.640]   I mean, is it security?
[01:32:58.640 --> 01:33:01.280]   Or are we just assuming that--
[01:33:01.280 --> 01:33:04.200]   why-- what if you want to undo it, for instance?
[01:33:04.200 --> 01:33:05.640]   Why mung the image?
[01:33:05.640 --> 01:33:08.320]   We just can say only display this part of the image.
[01:33:08.320 --> 01:33:11.320]   Obviously, they've changed their tune on this.
[01:33:11.320 --> 01:33:12.120]   And they are now--
[01:33:12.120 --> 01:33:15.360]   We talked with this a long time ago on Macbreak, and I was considered paranoid at the time.
[01:33:15.360 --> 01:33:17.840]   But I always take a screenshot at the end.
[01:33:17.840 --> 01:33:22.160]   Before I post anything online, I take a screenshot of it, and then crop the screenshot.
[01:33:22.160 --> 01:33:25.680]   And after I've cropped everything, marked up everything, I screenshot it.
[01:33:25.680 --> 01:33:27.520]   And then that's what goes up, because that takes--
[01:33:27.520 --> 01:33:31.400]   as far as I know, so much of the data and underlying work and pixels out of it.
[01:33:31.400 --> 01:33:33.440]   And it seemed like a lot of extra effort at the time.
[01:33:33.440 --> 01:33:34.000]   But it's just--
[01:33:34.000 --> 01:33:34.600]   You were smart.
[01:33:34.600 --> 01:33:36.120]   --to me, it's investing in good paranoia.
[01:33:36.120 --> 01:33:37.240]   You were smart.
[01:33:37.240 --> 01:33:39.480]   You knew.
[01:33:39.480 --> 01:33:43.440]   I knew too, because of this horrible situation at Tech TV.
[01:33:43.440 --> 01:33:47.920]   But I didn't realize it was still going on.
[01:33:47.920 --> 01:33:51.680]   So anyway, just a word of warning.
[01:33:51.680 --> 01:33:52.600]   What do you do?
[01:33:52.600 --> 01:33:55.480]   You do a screenshot, and then you do a capture.
[01:33:55.480 --> 01:33:56.960]   I edit everything I want to.
[01:33:56.960 --> 01:33:58.320]   Like, I'll crop an image.
[01:33:58.320 --> 01:33:59.440]   I'll redact it.
[01:33:59.440 --> 01:34:03.360]   To take my numbers off of something, or to cover up a username for somebody.
[01:34:03.360 --> 01:34:08.720]   And then once it's all done, I take a screenshot of that, because I figured that really burns it in.
[01:34:08.720 --> 01:34:20.880]   And to demonstrate the problem, there is a web page, Acropolis.app, where you can take a picture and uncrop it.
[01:34:20.880 --> 01:34:21.640]   So just--
[01:34:21.640 --> 01:34:25.040]   What the film needs to be terminated with extreme prejudice?
[01:34:25.040 --> 01:34:26.000]   Yes.
[01:34:26.000 --> 01:34:28.280]   Microsoft has already done an emergency patch.
[01:34:28.280 --> 01:34:31.600]   I imagine Google has also for the Pixel phones.
[01:34:31.600 --> 01:34:37.560]   But just to be aware, this affects all Pixel phones from Pixel 3 to Pixel 7 Pro.
[01:34:37.560 --> 01:34:40.240]   If you use their tool.
[01:34:40.240 --> 01:34:42.480]   So don't.
[01:34:42.480 --> 01:34:43.320]   All right, a little break.
[01:34:43.320 --> 01:34:44.840]   All that age always applies.
[01:34:44.840 --> 01:34:48.120]   Never take-- I mean, we're lucky.
[01:34:48.120 --> 01:34:54.640]   Of the generation where we grew up through our teenagers and 20s without digital cameras around.
[01:34:54.640 --> 01:34:59.680]   But for the current generation of everyone, or would, they have to be careful.
[01:34:59.680 --> 01:35:00.680]   Yeah.
[01:35:00.680 --> 01:35:03.040]   You know, I don't know about you.
[01:35:03.040 --> 01:35:12.160]   But I think between consenting adults, the exchange of fun pictures is fine.
[01:35:12.160 --> 01:35:14.160]   It should be a lot of it.
[01:35:14.160 --> 01:35:15.640]   It's a shame.
[01:35:15.640 --> 01:35:16.880]   If you can't.
[01:35:16.880 --> 01:35:18.200]   If you know what I'm saying.
[01:35:18.200 --> 01:35:19.480]   I'm just assessing this right now.
[01:35:19.480 --> 01:35:23.000]   Never let a photo be sent online or put online that I've ever approved.
[01:35:23.000 --> 01:35:23.800]   Yeah.
[01:35:23.800 --> 01:35:24.320]   Yeah.
[01:35:24.320 --> 01:35:27.440]   I've learned that lesson.
[01:35:27.440 --> 01:35:33.360]   Let's take a little tini-tini time out with our fabulous panel.
[01:35:33.360 --> 01:35:35.640]   We also thank Corey for joining us earlier.
[01:35:35.640 --> 01:35:37.520]   That was great to have him on as well.
[01:35:37.520 --> 01:35:41.160]   We'll get Corey back for a full show when he's got some time.
[01:35:41.160 --> 01:35:43.160]   Our show today brought to you by Noom.
[01:35:43.160 --> 01:35:46.560]   Oh, Lisa and I are Noom love us.
[01:35:46.560 --> 01:35:47.760]   In fact, I started.
[01:35:47.760 --> 01:35:48.760]   I will take full credit.
[01:35:48.760 --> 01:35:49.920]   I've seen the ads for Noom.
[01:35:49.920 --> 01:35:52.160]   This is about a year and a half, almost two years ago.
[01:35:52.160 --> 01:35:53.280]   I've seen the ads for Noom.
[01:35:53.280 --> 01:35:58.200]   I have done every diet in the book, as you well know, because I share my journey with
[01:35:58.200 --> 01:35:59.600]   you.
[01:35:59.600 --> 01:36:01.360]   I saw the ads and I saw the ads.
[01:36:01.360 --> 01:36:04.960]   Finally, I said, "Well, I should give it a try."
[01:36:04.960 --> 01:36:05.960]   I did.
[01:36:05.960 --> 01:36:06.960]   I signed up for Noom.
[01:36:06.960 --> 01:36:07.960]   That's a great app.
[01:36:07.960 --> 01:36:14.520]   The idea is it helps you lose weight, not by dieting, but by understanding why you eat,
[01:36:14.520 --> 01:36:19.120]   how you eat, and what you do that is counterproductive.
[01:36:19.120 --> 01:36:21.680]   Lisa said, "Oh, she didn't really have anything.
[01:36:21.680 --> 01:36:22.680]   She lost maybe.
[01:36:22.680 --> 01:36:23.680]   Maybe five pounds."
[01:36:23.680 --> 01:36:24.680]   She said, "Oh, that's not...
[01:36:24.680 --> 01:36:27.680]   She may have also said that she wanted to support me, right, so that we're both doing
[01:36:27.680 --> 01:36:28.680]   Noom together."
[01:36:28.680 --> 01:36:32.600]   Of course, she lost 10 pounds and has kept it off ever since.
[01:36:32.600 --> 01:36:36.640]   I lost about 60, I think almost 20 pounds.
[01:36:36.640 --> 01:36:38.360]   I've kept most of it off.
[01:36:38.360 --> 01:36:43.360]   I have to every once in a while, re-up my Noom just to make sure I'm following it.
[01:36:43.360 --> 01:36:45.640]   Look, this is the thing.
[01:36:45.640 --> 01:36:48.960]   Trends in Feds come and go with dieting all the time.
[01:36:48.960 --> 01:36:51.280]   This is not a FED.
[01:36:51.280 --> 01:36:56.440]   This is re-learning how you eat and why you eat.
[01:36:56.440 --> 01:37:02.840]   Noom weight uses psychology, not Feds, to help you make intentional, thoughtful, sustainable
[01:37:02.840 --> 01:37:08.160]   choices that are aligned with your values and your weight loss goals.
[01:37:08.160 --> 01:37:10.600]   It's a psychology-based approach.
[01:37:10.600 --> 01:37:11.600]   Noom...
[01:37:11.600 --> 01:37:15.880]   Georgia, you know about CBT, cognitive behavioral therapy.
[01:37:15.880 --> 01:37:18.640]   That's one of the tools Noom uses.
[01:37:18.640 --> 01:37:22.240]   They'll help you understand the science behind your eating choices, why you have cravings.
[01:37:22.240 --> 01:37:29.440]   I learned, for instance, that I was what they call a fog eater, that I would eat and not
[01:37:29.440 --> 01:37:31.080]   even know I ate it.
[01:37:31.080 --> 01:37:36.000]   I would also get home from work and stuff my face unconsciously.
[01:37:36.000 --> 01:37:41.560]   I learned, both of us learned to sit down at dinner, turn off the TV, put away the phones,
[01:37:41.560 --> 01:37:44.120]   sit and enjoy our food.
[01:37:44.120 --> 01:37:47.600]   Sometimes Lisa and I will close our eyes as we chew a bite of food, put the fork down
[01:37:47.600 --> 01:37:48.800]   in between bites.
[01:37:48.800 --> 01:37:51.120]   All of that made a huge difference.
[01:37:51.120 --> 01:37:53.440]   I got to tell you, everybody's journey is different.
[01:37:53.440 --> 01:37:58.640]   That's why Noom, when you first sign up, they're going to ask a lot of questions and they personalize
[01:37:58.640 --> 01:38:03.440]   your Noom experience to you and your goals.
[01:38:03.440 --> 01:38:08.240]   For instance, you will get in the Noom app, you get a logging program.
[01:38:08.240 --> 01:38:10.200]   It helps you write down what you eat.
[01:38:10.200 --> 01:38:12.720]   You see, written out what you've been eating.
[01:38:12.720 --> 01:38:14.320]   That really helped me with a fog eating.
[01:38:14.320 --> 01:38:19.480]   You also get daily lessons that are personalized to you and your goals.
[01:38:19.480 --> 01:38:20.480]   It's really fascinating.
[01:38:20.480 --> 01:38:27.440]   You can learn a lot about what you're eating, why you're eating and learn new tools for handling
[01:38:27.440 --> 01:38:30.520]   things like social events.
[01:38:30.520 --> 01:38:33.680]   For instance, I learned from Noom that when we were eating out with other people, we tend
[01:38:33.680 --> 01:38:39.120]   to eat more than we would otherwise because we're watching the other people, we're socializing
[01:38:39.120 --> 01:38:40.800]   and there's a direct response.
[01:38:40.800 --> 01:38:42.880]   When they pick up their fork, you pick up your fork.
[01:38:42.880 --> 01:38:45.960]   You start to notice this behavior.
[01:38:45.960 --> 01:38:48.240]   It's like your eyes are opened.
[01:38:48.240 --> 01:38:49.400]   Noom is not restrictive.
[01:38:49.400 --> 01:38:51.040]   It's nourishing, not restrictive.
[01:38:51.040 --> 01:38:54.000]   Whatever your health goals are and it's a very flexible program.
[01:38:54.000 --> 01:38:56.640]   It focuses on progress, not perfection.
[01:38:56.640 --> 01:39:01.600]   In fact, after a while, I got bonus days where it said, "Go ahead, do whatever you want
[01:39:01.600 --> 01:39:02.600]   today."
[01:39:02.600 --> 01:39:07.640]   But I'd already learned, it's funny, once you absorb these habits, they're not hard.
[01:39:07.640 --> 01:39:11.120]   I've already learned all that so I didn't really go crazy on the off days.
[01:39:11.120 --> 01:39:13.080]   But hey, if you wanted to, you could.
[01:39:13.080 --> 01:39:14.760]   You also can get a personal coach.
[01:39:14.760 --> 01:39:16.320]   I love my coach.
[01:39:16.320 --> 01:39:19.440]   She really helps me focus and she answers questions.
[01:39:19.440 --> 01:39:22.560]   There's also a group you can use if you like groups.
[01:39:22.560 --> 01:39:27.160]   You can choose your level of support from five-minute daily check-ins to personal coaching.
[01:39:27.160 --> 01:39:28.160]   Off days are okay.
[01:39:28.160 --> 01:39:31.640]   Noom always helps you get back on track with lots of support.
[01:39:31.640 --> 01:39:36.040]   First time, Noomers lose an average of 15 pounds in 16 weeks.
[01:39:36.040 --> 01:39:38.520]   I lost a little bit more in about the same time.
[01:39:38.520 --> 01:39:44.600]   95% of customers say Noom is a good long-term solution, both Lisa and I'll testify to that.
[01:39:44.600 --> 01:39:49.560]   And by the way, ever since I started talking about Noom on the air, I've met many of our
[01:39:49.560 --> 01:39:51.800]   listeners and even our hosts that use Noom.
[01:39:51.800 --> 01:39:55.520]   Did you know that Brianna Wu, after I did the etch, said, "You know, that was how I lost
[01:39:55.520 --> 01:39:57.360]   100 pounds.
[01:39:57.360 --> 01:39:59.600]   100 pounds."
[01:39:59.600 --> 01:40:02.520]   One of our chatters lost 60 pounds in Noom.
[01:40:02.520 --> 01:40:04.160]   That's not typical.
[01:40:04.160 --> 01:40:06.120]   But it just shows you Noom really works.
[01:40:06.120 --> 01:40:12.880]   We've even published 50, five-zero peer-reviewed scientific articles describing their methods
[01:40:12.880 --> 01:40:13.880]   and effectiveness.
[01:40:13.880 --> 01:40:20.240]   And there's even a new Noom book, the first-ever book, The Noom Mindset, which is a great read.
[01:40:20.240 --> 01:40:23.760]   It'll help you understand the psychology of behavior change.
[01:40:23.760 --> 01:40:28.200]   So stop chasing health trends, diet fads.
[01:40:28.200 --> 01:40:29.200]   They don't work.
[01:40:29.200 --> 01:40:30.840]   You know they don't work.
[01:40:30.840 --> 01:40:36.080]   Build sustainable healthy habits with Noom's psychology-based approach.
[01:40:36.080 --> 01:40:41.760]   Sign up for a trial today, NoomNOM.com/twit.
[01:40:41.760 --> 01:40:44.840]   Sign up for your trial today.
[01:40:44.840 --> 01:40:47.160]   Check out that book, The Noom Mindset.
[01:40:47.160 --> 01:40:48.760]   It's wherever books are sold.
[01:40:48.760 --> 01:40:50.240]   You'll be able to find it.
[01:40:50.240 --> 01:40:53.080]   And share your journey with us.
[01:40:53.080 --> 01:40:56.200]   It's really been an amazing eye-opening experience for me.
[01:40:56.200 --> 01:40:57.680]   Noom.com/twit.
[01:40:57.680 --> 01:41:01.920]   We thank them not only for the support of the show, but the support they've given me
[01:41:01.920 --> 01:41:08.200]   and Lisa and many of our audience members to a healthier life.
[01:41:08.200 --> 01:41:10.440]   All right.
[01:41:10.440 --> 01:41:14.080]   That's time to do almost British Leo.
[01:41:14.080 --> 01:41:15.080]   All right.
[01:41:15.080 --> 01:41:19.720]   I told you, I've been listening to a lot of F1 races and F1 stories and F1 books.
[01:41:19.720 --> 01:41:21.640]   Ah, yeah, you've got into it at last.
[01:41:21.640 --> 01:41:23.640]   I'm so into it now.
[01:41:23.640 --> 01:41:30.680]   Dr. Sabai has done more to push people into learning about the formula one than 20 years
[01:41:30.680 --> 01:41:31.680]   of me.
[01:41:31.680 --> 01:41:32.680]   I should explain that.
[01:41:32.680 --> 01:41:33.680]   I'm not.
[01:41:33.680 --> 01:41:38.120]   Well, it's a very -- so it's a weird sport because it's a very complicated sport, very
[01:41:38.120 --> 01:41:39.880]   expensive.
[01:41:39.880 --> 01:41:43.840]   It's car racing, but they do like --
[01:41:43.840 --> 01:41:44.840]   It's also computerized.
[01:41:44.840 --> 01:41:45.840]   It's highly technical.
[01:41:45.840 --> 01:41:53.240]   I mean, they're basically having to reset settings on the car four or five times a lap,
[01:41:53.240 --> 01:41:56.400]   you know, break settings, and settings the rest of it.
[01:41:56.400 --> 01:41:58.040]   You'll probably enjoy that part.
[01:41:58.040 --> 01:41:59.040]   Yeah.
[01:41:59.040 --> 01:42:00.040]   It's a very complex.
[01:42:00.040 --> 01:42:03.480]   So I can see how people might be off-put because it's such a -- it's not just watching somebody
[01:42:03.480 --> 01:42:05.560]   go left for 400 laps.
[01:42:05.560 --> 01:42:07.520]   It's a very different kind of thing.
[01:42:07.520 --> 01:42:12.800]   Their road courses, the cars are incredibly sophisticated technical marvels.
[01:42:12.800 --> 01:42:15.160]   And I like it because it's kind of British.
[01:42:15.160 --> 01:42:16.880]   It's very European, right?
[01:42:16.880 --> 01:42:17.880]   It's not like NASCAR.
[01:42:17.880 --> 01:42:21.640]   Although, there are going to be, I think, four races in the U.S. this year.
[01:42:21.640 --> 01:42:23.200]   They've really -- drive to survive is really --
[01:42:23.200 --> 01:42:24.200]   No, they're expanding.
[01:42:24.200 --> 01:42:26.360]   And I got the new going out in August 4th as well.
[01:42:26.360 --> 01:42:27.360]   In November.
[01:42:27.360 --> 01:42:28.360]   It's something that's the jealous about.
[01:42:28.360 --> 01:42:30.720]   We're going to see the penultimate race down the street.
[01:42:30.720 --> 01:42:32.400]   There's a Montreal Grand Prix, Leo.
[01:42:32.400 --> 01:42:33.400]   Yes, I know.
[01:42:33.400 --> 01:42:35.840]   Maybe you come up for Montreal Grand Prix.
[01:42:35.840 --> 01:42:36.840]   That would be --
[01:42:36.840 --> 01:42:40.400]   The 2011 Canadian Grand Prix is one of the all-time classics.
[01:42:40.400 --> 01:42:42.400]   Oh, I have to watch it.
[01:42:42.400 --> 01:42:43.400]   I really --
[01:42:43.400 --> 01:42:44.400]   Don't tell me -- no spoilers.
[01:42:44.400 --> 01:42:45.400]   No spoilers.
[01:42:45.400 --> 01:42:47.240]   I wouldn't want to give --
[01:42:47.240 --> 01:42:49.080]   Twelve-year-old race.
[01:42:49.080 --> 01:42:51.800]   I'll tell you how far I've gone.
[01:42:51.800 --> 01:42:58.240]   I actually went and bought a Alonzo cap in an Aston Martin jersey so I can -- I can support
[01:42:58.240 --> 01:42:59.240]   my team.
[01:42:59.240 --> 01:43:02.840]   I have a Renault cap from when the Alonzo was there.
[01:43:02.840 --> 01:43:03.840]   Yeah, Renault.
[01:43:03.840 --> 01:43:04.840]   It's not a Renault anymore.
[01:43:04.840 --> 01:43:05.840]   It's Alpien.
[01:43:05.840 --> 01:43:06.840]   No, exactly.
[01:43:06.840 --> 01:43:08.840]   Oh, my Blackberry friends used to go.
[01:43:08.840 --> 01:43:09.840]   When that was the thing.
[01:43:09.840 --> 01:43:10.840]   Oh.
[01:43:10.840 --> 01:43:11.840]   All my Blackberry friends used to go all the time.
[01:43:11.840 --> 01:43:12.840]   Oh, they have a car.
[01:43:12.840 --> 01:43:13.840]   Because they used to sponsor it.
[01:43:13.840 --> 01:43:14.840]   Yeah.
[01:43:14.840 --> 01:43:15.840]   I think it was Mercedes.
[01:43:15.840 --> 01:43:16.840]   Wasn't it?
[01:43:16.840 --> 01:43:17.840]   Mercedes Blackberry.
[01:43:17.840 --> 01:43:18.840]   Oh, nice.
[01:43:18.840 --> 01:43:19.840]   Yeah.
[01:43:19.840 --> 01:43:23.600]   The classic was AMD sponsors Ferrari in a big way.
[01:43:23.600 --> 01:43:28.000]   And Martin Brundel was doing his grid walk and met Lisa.
[01:43:28.000 --> 01:43:30.000]   So what do you do?
[01:43:30.000 --> 01:43:32.000]   It's like I'm the champion.
[01:43:32.000 --> 01:43:33.000]   I'm the champion.
[01:43:33.000 --> 01:43:34.000]   I'm the champion.
[01:43:34.000 --> 01:43:39.600]   Like, okay, that's quite fist bad in terms of embarrassment.
[01:43:39.600 --> 01:43:41.800]   You saw that Android -- or is it Chrome?
[01:43:41.800 --> 01:43:43.600]   No, Chrome is a sponsor.
[01:43:43.600 --> 01:43:44.600]   Chrome is a sponsor.
[01:43:44.600 --> 01:43:46.600]   Of the McLaren cars.
[01:43:46.600 --> 01:43:47.600]   So much so --
[01:43:47.600 --> 01:43:48.600]   Boy, they picked the wrong team.
[01:43:48.600 --> 01:43:52.240]   Yeah, unfortunately, they're not having a great year.
[01:43:52.240 --> 01:43:56.840]   But so much so that the wheels on the McLaren car are the Google colors.
[01:43:56.840 --> 01:43:57.840]   Yep.
[01:43:57.840 --> 01:44:01.640]   And it says Chrome up along the top.
[01:44:01.640 --> 01:44:07.840]   McLaren, which for years was a dominant team, has not done very well in a while.
[01:44:07.840 --> 01:44:13.440]   It's kind of like when teams go up and down each season.
[01:44:13.440 --> 01:44:14.840]   It's the way of it.
[01:44:14.840 --> 01:44:17.640]   I mean, Mercedes dominated for the last seven days.
[01:44:17.640 --> 01:44:19.440]   Yeah, I'm glad they're not dominating so much.
[01:44:19.440 --> 01:44:21.160]   But Red Bull's going to dominate forever now.
[01:44:21.160 --> 01:44:24.360]   I don't think -- I don't think -- I think Red Bull's going to win every race this year,
[01:44:24.360 --> 01:44:26.360]   which is kind of disappointing.
[01:44:26.360 --> 01:44:27.760]   I'm borrowing mechanical difficulties.
[01:44:27.760 --> 01:44:28.760]   I think you're right.
[01:44:28.760 --> 01:44:29.760]   Yeah.
[01:44:29.760 --> 01:44:30.760]   I mean, Red Bull is --
[01:44:30.760 --> 01:44:35.960]   But that's why I'm rooting for Alonso because he is the old man and we old men have to support.
[01:44:35.960 --> 01:44:38.560]   He's all of 42 years old.
[01:44:38.560 --> 01:44:42.360]   He's the old -- for Formula One racing, that's ancient.
[01:44:42.360 --> 01:44:44.160]   And I love it.
[01:44:44.160 --> 01:44:45.760]   That's not a Villeneuve that didn't happen.
[01:44:45.760 --> 01:44:46.760]   Yeah, I love it.
[01:44:46.760 --> 01:44:47.760]   Yeah, well, I mean --
[01:44:47.760 --> 01:44:48.760]   It's not --
[01:44:48.760 --> 01:44:50.360]   He's not going, but yeah.
[01:44:50.360 --> 01:44:52.960]   It's not an F1, but it is an F bond.
[01:44:52.960 --> 01:44:54.960]   Did you see the Steve Jobs story this week?
[01:44:54.960 --> 01:44:56.960]   No, which one?
[01:44:56.960 --> 01:45:04.760]   So, Paul Kefassus wrote this blog post where he said that Adam Curry was talking to Steve Jobs at one point.
[01:45:04.760 --> 01:45:05.760]   Yeah.
[01:45:05.760 --> 01:45:09.160]   And they said the RIA asked Apple to ban audio hijack.
[01:45:09.160 --> 01:45:11.760]   And so Steve was like, "Do you need this for podcasting?"
[01:45:11.760 --> 01:45:13.360]   And Adam Curry said, "Currently, yes."
[01:45:13.360 --> 01:45:15.360]   And he said, "Well, then F them were not going to do it."
[01:45:15.360 --> 01:45:16.360]   He saved.
[01:45:16.360 --> 01:45:17.360]   There, but for the --
[01:45:17.360 --> 01:45:19.360]   He saved Roga Miba.
[01:45:19.360 --> 01:45:23.360]   Yep, from an early death.
[01:45:23.360 --> 01:45:27.360]   Which kind of maybe helped podcasting a bit, too.
[01:45:27.360 --> 01:45:28.360]   Yeah.
[01:45:28.360 --> 01:45:32.360]   So, you think that's an incredible story?
[01:45:32.360 --> 01:45:35.360]   I mean, like, you never know.
[01:45:35.360 --> 01:45:41.360]   It's like half the story about the iPod in the fish tank is apparently not a true story.
[01:45:41.360 --> 01:45:43.360]   But it's part of our urban mythology at this point.
[01:45:43.360 --> 01:45:44.360]   Yeah.
[01:45:44.360 --> 01:45:46.360]   Is our Silicon Valley a lower?
[01:45:46.360 --> 01:45:47.360]   It's from Adam Curry.
[01:45:47.360 --> 01:45:51.360]   So, I have some skepticism.
[01:45:51.360 --> 01:45:55.360]   It's a little self-aggrandizing, which is not unusual for Adam.
[01:45:55.360 --> 01:45:58.360]   Nevertheless, I wouldn't be surprised.
[01:45:58.360 --> 01:46:07.360]   Eddie Q reportedly told Curry, according to Adam, "The RIA wants us to disable audio hijack pro
[01:46:07.360 --> 01:46:11.360]   because with it you could record any sound off your Mac, any song, anything."
[01:46:11.360 --> 01:46:15.360]   Jobs then asked whether Curry and other podcasters needed audio hijack.
[01:46:15.360 --> 01:46:17.360]   The answer was an emphatic yes.
[01:46:17.360 --> 01:46:21.360]   So, Adam says, "Steve told the RIAA to get lost."
[01:46:21.360 --> 01:46:29.360]   I've heard enough apocryphal stories, Adam, out of Curry's mouth, that I'm slightly, slightly
[01:46:29.360 --> 01:46:31.360]   skeptical, but, you know, it's possible.
[01:46:31.360 --> 01:46:34.360]   Let's talk about the other big Apple story of the week.
[01:46:34.360 --> 01:46:41.360]   The New York Times says that there are people at Apple who do not want them to release the
[01:46:41.360 --> 01:46:46.360]   goggles.
[01:46:46.360 --> 01:46:51.360]   At Apple, rare descent over a new product interactive goggles.
[01:46:51.360 --> 01:46:56.360]   This confirms another story we'd heard from another source, I think, a german, that the
[01:46:56.360 --> 01:47:02.360]   design team did not want Apple to release the AR/VR goggles this year.
[01:47:02.360 --> 01:47:07.360]   They wanted them to wait until sometime in the distant future, the AR spectacles were
[01:47:07.360 --> 01:47:12.360]   completely different.
[01:47:12.360 --> 01:47:15.360]   I'm a little suspicious of that story as well.
[01:47:15.360 --> 01:47:20.360]   But, Donnie canceled the previous one because it needed a whole computer dongle.
[01:47:20.360 --> 01:47:25.360]   Remember when Apple did that whole show with WWDC with AMD when they showed off how you
[01:47:25.360 --> 01:47:30.360]   could do the Darth Vader, and that was all going to be slave to a box.
[01:47:30.360 --> 01:47:33.360]   And then, Donnie's like, "No, no, no, we're not doing that."
[01:47:33.360 --> 01:47:38.360]   I don't know if you're going to put the battery in the headset.
[01:47:38.360 --> 01:47:43.360]   We have with us, ladies and gentlemen, a strong advocate for virtual reality.
[01:47:43.360 --> 01:47:49.360]   Let me ask you, Georgia, would you spend $3,000 on this standalone headset?
[01:47:49.360 --> 01:47:54.360]   Sounds like it's somewhat like the Oculus Quest, maybe the pro version of Quest.
[01:47:54.360 --> 01:47:55.360]   What do you think?
[01:47:55.360 --> 01:47:56.360]   I'll say that I wouldn't.
[01:47:56.360 --> 01:47:57.360]   I won't say that I wouldn't.
[01:47:57.360 --> 01:47:59.360]   But that is a lot.
[01:47:59.360 --> 01:48:07.360]   How is it going to make my life better?
[01:48:07.360 --> 01:48:08.360]   Does it have a game that I really love?
[01:48:08.360 --> 01:48:15.360]   So I got the PlayStation 5 still for me at VR and I'm playing Horizon on it.
[01:48:15.360 --> 01:48:20.360]   And I find that the headset is very strangely made up where it's kind of like a headband
[01:48:20.360 --> 01:48:21.360]   and the headset fits on top.
[01:48:21.360 --> 01:48:23.360]   And I find it really uncomfortable.
[01:48:23.360 --> 01:48:27.360]   I find the gameplay wonderful and I think that they did a great job with it.
[01:48:27.360 --> 01:48:32.360]   But that they don't want it released is probably there's something that is not right.
[01:48:32.360 --> 01:48:37.360]   Like Apple shouldn't go into the market and produce something that's going to end up being not...
[01:48:37.360 --> 01:48:39.360]   I think that's the question.
[01:48:39.360 --> 01:48:44.360]   I think this is going to be a rare flop from Apple, to be honest.
[01:48:44.360 --> 01:48:49.360]   I mean, traditionally Apple has taken existing technologies and just made them slightly better.
[01:48:49.360 --> 01:48:53.360]   I mean, when the first iPhone came out, it was rubbish from a technical standpoint.
[01:48:53.360 --> 01:48:56.360]   But the interface was so good that people loved it.
[01:48:56.360 --> 01:48:58.360]   Same with the iPod and the rest of it.
[01:48:58.360 --> 01:49:00.360]   And the Apple Watch?
[01:49:00.360 --> 01:49:01.360]   Yeah.
[01:49:01.360 --> 01:49:05.360]   Well, some people are comparing it to the Apple Watch because when the Apple Watch came out,
[01:49:05.360 --> 01:49:07.360]   it wasn't a huge success.
[01:49:07.360 --> 01:49:08.360]   But it is gone.
[01:49:08.360 --> 01:49:13.360]   It is now a multi-billion, probably a 20 or 30 billion dollar business.
[01:49:13.360 --> 01:49:16.360]   Because over time, they've added functionality.
[01:49:16.360 --> 01:49:18.360]   People have found ways to use it.
[01:49:18.360 --> 01:49:20.360]   It's gotten better and better, right?
[01:49:20.360 --> 01:49:22.360]   I think we'd agree that it's gotten better and better.
[01:49:22.360 --> 01:49:23.360]   Yeah.
[01:49:23.360 --> 01:49:25.360]   So some people are saying this is going to be like that.
[01:49:25.360 --> 01:49:27.360]   It's going to be the first product.
[01:49:27.360 --> 01:49:30.360]   But the Apple Watch did not cost $3,000.
[01:49:30.360 --> 01:49:32.360]   Actually, one of them cost 25,000.
[01:49:32.360 --> 01:49:33.360]   Wow, yes.
[01:49:33.360 --> 01:49:36.360]   If you're going for the golds.
[01:49:36.360 --> 01:49:41.360]   But I think that for this, it's already such a niche market.
[01:49:41.360 --> 01:49:44.360]   There's very few people versus a watch where most people have a watch.
[01:49:44.360 --> 01:49:47.360]   Most people have a phone.
[01:49:47.360 --> 01:49:50.360]   I think that they have to make sure that this is going to knock it out of the park.
[01:49:50.360 --> 01:49:52.360]   And if not, they shouldn't waste it.
[01:49:52.360 --> 01:49:53.360]   They should wait.
[01:49:53.360 --> 01:49:55.360]   I paid $1,500 for the Quest Pro.
[01:49:55.360 --> 01:49:58.360]   Meta has dropped the price to $1,000.
[01:49:58.360 --> 01:50:01.360]   Not because it's a big best seller, I'm guessing.
[01:50:01.360 --> 01:50:02.360]   No.
[01:50:02.360 --> 01:50:05.360]   Which one do you like best, Georgia?
[01:50:05.360 --> 01:50:09.360]   Right now I'm using PlayStation 5 just because of the game.
[01:50:09.360 --> 01:50:11.360]   Not because I like the system itself.
[01:50:11.360 --> 01:50:13.360]   But I think that the interface is actually quite interesting.
[01:50:13.360 --> 01:50:19.360]   And the way that they have transformed the Horizon games in order to be really good in VR.
[01:50:19.360 --> 01:50:24.360]   So even though it's not the most comfortable of systems, it's easy to use.
[01:50:24.360 --> 01:50:26.360]   It's easy to set up.
[01:50:26.360 --> 01:50:29.360]   It runs on the PlayStation 5 really nicely.
[01:50:29.360 --> 01:50:32.360]   And for that, I think it has a really nice ease of use.
[01:50:32.360 --> 01:50:35.360]   And the gaming is quite well done.
[01:50:35.360 --> 01:50:44.360]   I just think that the headset itself is uncomfortable, especially compared to something like the Quest where it's really easy to put on and quite comfortable.
[01:50:44.360 --> 01:50:45.360]   I find it hurts me.
[01:50:45.360 --> 01:50:48.360]   And it gives me a headache because it's a headband.
[01:50:48.360 --> 01:50:51.360]   And because it's so heavy, it's really strapped your head so hard.
[01:50:51.360 --> 01:51:02.360]   And it's not supposed to rest on your nose, which means that the little nose part that has this little fold kind of tickles my nose and causes it to become really uncomfortable.
[01:51:02.360 --> 01:51:04.360]   And so I keep on adjusting my headset.
[01:51:04.360 --> 01:51:07.360]   And so I get a migraine.
[01:51:07.360 --> 01:51:13.360]   But the other side of it, they do a great job on the software side to make sure that you don't get sick playing it.
[01:51:13.360 --> 01:51:27.360]   And so when you're doing something like sliding down a zip line, which happens quite a lot, and it's a free movement game, which causes more people instead of a teleport where you zoom from one spot, and you pop into another spot, which you can travel quickly without getting sick.
[01:51:27.360 --> 01:51:39.360]   They end up changing the field of view into this kind of just a little tiny field of view every time you move very quickly, which works relatively effectively so you don't get very much vertigo when you play.
[01:51:39.360 --> 01:51:41.360]   And I think that they did a good job of that.
[01:51:41.360 --> 01:51:45.360]   I have another beautiful dream, and this is going to require this to be part of it.
[01:51:45.360 --> 01:51:49.360]   And like you cannot make fun of me because this might take years to happen.
[01:51:49.360 --> 01:52:00.360]   But I think when we get to like GPT six and Bard three and Facebook finally has their models off of the torrents and onto the real world and we see what Apple's doing with their language models.
[01:52:00.360 --> 01:52:10.360]   In order to get to like forget Star Trek for a minute, like to get to Tony Stark, like in the early Iron Man movies, even an end game where he's solving for time travel, you have this component of talking to a lion.
[01:52:10.360 --> 01:52:21.360]   Large language model, there it's Jarvis or Friday, but the other part of it is interacting with an environment and they use holographics and the Marvel movies, obviously, but that's like seems like a very hard problem to solve.
[01:52:21.360 --> 01:52:26.360]   And it seems like a task that AR glasses might be able to do, but not for a very long time.
[01:52:26.360 --> 01:52:39.360]   But if you want to have a really interactive large language model experience like generative AI experience, something that's beyond just the Zork interface, it's going to require very good natural language spoken words.
[01:52:39.360 --> 01:52:45.360]   But it's also going to have to have these virtual reality things that we can interact with.
[01:52:45.360 --> 01:52:49.360]   So like, of course, the games are going to be there and the entertainment and the fitness stuff.
[01:52:49.360 --> 01:53:00.360]   But to be able to sit there and model things and build things out is going to require both the computer and the next generation of interface, which we haven't really gotten for this stuff yet.
[01:53:00.360 --> 01:53:07.360]   We haven't solved the basic hardware problem.
[01:53:07.360 --> 01:53:13.360]   Oculus when 10 years ago was saying, we're going to be 15 years away from a pair of spectacles that you can put on and do this.
[01:53:13.360 --> 01:53:16.360]   Now, I've tried the latest Oculus kit.
[01:53:16.360 --> 01:53:22.360]   I gave a very open, enthusiastic review on Twitter of HoloLens when it first came out.
[01:53:22.360 --> 01:53:29.360]   But the fact of the matter, as Georgia points out, is you've got a heavy bit of kit on your head and the battery life is very low.
[01:53:29.360 --> 01:53:35.360]   And until we solve those hardware problems, I don't see this going forward.
[01:53:35.360 --> 01:53:36.360]   Do you still iron that?
[01:53:36.360 --> 01:53:39.360]   I tried the PlayStation VR when it first came out and you had to sit down.
[01:53:39.360 --> 01:53:43.360]   Do you still have to sit down in PlayStation VR or can you stand up now?
[01:53:43.360 --> 01:53:45.360]   The setup is actually really wonderful.
[01:53:45.360 --> 01:53:47.360]   So one is you can choose.
[01:53:47.360 --> 01:53:50.360]   It asks you whether you want to sit or if you want to stand.
[01:53:50.360 --> 01:53:54.360]   Two, it will track and map your room for you.
[01:53:54.360 --> 01:53:57.360]   That is incredible and really, really quick.
[01:53:57.360 --> 01:54:02.360]   And the other part is that it actually pulls you out of the game really quickly and effectively.
[01:54:02.360 --> 01:54:06.360]   Unfortunately, a little bit too soon into your room setup.
[01:54:06.360 --> 01:54:12.360]   So if you go outside before you're going to hit your TV or your wall, it's going to pull you out of the game, which stops you dead in your tracks.
[01:54:12.360 --> 01:54:15.360]   And then it has an eye tracking, which works really well.
[01:54:15.360 --> 01:54:19.360]   So you can, which you can choose to turn on or turn off depending on what you want.
[01:54:19.360 --> 01:54:24.360]   And so you can kind of track with your eyes where you want to be able to click and to be able to click in and out.
[01:54:24.360 --> 01:54:30.360]   And so I think that it's user interface is much more user friendly and much easier to set up.
[01:54:30.360 --> 01:54:37.360]   So the setup is very, very quick and to be able to flip that on or change something so I moved to table so I'd have a little bit more space.
[01:54:37.360 --> 01:54:47.360]   And I could just retract it and it took seconds and you don't have to be really technologically savvy or scan it out with the handset, which you have to do on the Vive.
[01:54:47.360 --> 01:54:49.360]   It really just does everything for you.
[01:54:49.360 --> 01:54:51.360]   And I think it does a great job of that.
[01:54:51.360 --> 01:55:01.360]   So one of the big problems, of course, is movement because we still don't have those ready player one every which way treadmills to go in every which way.
[01:55:01.360 --> 01:55:03.360]   They do. They're very expensive.
[01:55:03.360 --> 01:55:04.360]   Yeah, we tried one.
[01:55:04.360 --> 01:55:05.360]   All the games.
[01:55:05.360 --> 01:55:06.360]   Did you try one?
[01:55:06.360 --> 01:55:08.360]   Yeah, they're just not a good.
[01:55:08.360 --> 01:55:12.360]   If you thought you got sick with the glasses, whether you try to tread.
[01:55:12.360 --> 01:55:20.360]   So I, it's really curious to me that Apple may just be because there's a certain amount of momentum.
[01:55:20.360 --> 01:55:22.360]   You can't change on a dime.
[01:55:22.360 --> 01:55:26.360]   It really appears as if VR is a flop already.
[01:55:26.360 --> 01:55:33.360]   Like we already know it hasn't taken off and that AI is about to just explode.
[01:55:33.360 --> 01:55:35.360]   But Apple can't move on a dime.
[01:55:35.360 --> 01:55:39.360]   They put all their eggs in the VR basket, the AR basket.
[01:55:39.360 --> 01:55:41.360]   And now they're kind of stuck.
[01:55:41.360 --> 01:55:44.360]   And meanwhile, you know, AI is leaving them in the dust.
[01:55:44.360 --> 01:55:49.360]   Siri is not exactly the sharpest tool in the.
[01:55:49.360 --> 01:55:53.360]   I've got a book with a metaverse.
[01:55:53.360 --> 01:55:57.360]   You know, they, well, yeah, they put the gambled on that one and lost out big.
[01:55:57.360 --> 01:55:58.360]   Do you think they have?
[01:55:58.360 --> 01:55:59.360]   Is it?
[01:55:59.360 --> 01:56:00.360]   I mean, am I being premature?
[01:56:00.360 --> 01:56:02.360]   I think it's simpler than that.
[01:56:02.360 --> 01:56:03.360]   I think it's like simpler than that.
[01:56:03.360 --> 01:56:07.360]   I think like Apple is traditionally good at looking several years ahead.
[01:56:07.360 --> 01:56:13.360]   And when you look at the evolution of the Apple TV, the idea of paying like a hundred bucks for a box in the living room that everybody gets to share.
[01:56:13.360 --> 01:56:29.360]   Is probably not as enticing as people paying like right now 3000, but eventually like $500 for personal device you strap on your head that leverages like the movie, like all the Apple TV content they've built up the fitness plus content they've built up the educational content they've built up.
[01:56:29.360 --> 01:56:36.360]   Like it is a becomes a personal entertainment experience, the natural evolution of like the way the iPhone replaced the iPod.
[01:56:36.360 --> 01:56:39.360]   They're looking at this as a convergence device to replace the Apple TV.
[01:56:39.360 --> 01:56:47.360]   And then the AR glasses become the device that replaces the watch can't do as much as the phone like can't do as much as a headset, but it's really convenient.
[01:56:47.360 --> 01:56:48.360]   It's very light.
[01:56:48.360 --> 01:56:49.360]   You wear it.
[01:56:49.360 --> 01:57:03.360]   And so those two product lines they have now Apple watch and Apple TV go on to like the AR headset and the VR glasses and it wouldn't like count out Apple for AI either like they series like terrible, but they've got like a lot of really good AI for things.
[01:57:03.360 --> 01:57:08.360]   So is auto correct like anything that seems to involve languages is tough, but like there's a lot of AI going on for like photos and just like a lot of people.
[01:57:08.360 --> 01:57:11.360]   And for like photos and like photography and cameras and stuff.
[01:57:11.360 --> 01:57:14.360]   And they have enough money that they could have been building language models for years as well.
[01:57:14.360 --> 01:57:21.360]   We just don't know because they don't beta test in public the way every single other company does, but it's quite possible that AR is AI.
[01:57:21.360 --> 01:57:23.360]   In fact, I would be disappointed.
[01:57:23.360 --> 01:57:34.360]   Kevin Sorbo level disappointed if AI wasn't part of whatever they're building for wearables because it's it's the first thing that makes voice technology like like written word technology actually usable.
[01:57:34.360 --> 01:57:42.360]   And it is I mean that then you have something of much more interesting than just a headset you wear to play games.
[01:57:42.360 --> 01:57:56.360]   According to Mark Gurman in Bloomberg, Apple had an event at the Steve Jobs Theater for a hundred top executives to demo the device ahead of a unveiling at WWDC in June.
[01:57:56.360 --> 01:58:02.360]   No word about how they reacted to the unveiling.
[01:58:02.360 --> 01:58:04.360]   That tells you a lot.
[01:58:04.360 --> 01:58:09.360]   Yeah, no leaks about them jumping to their feet and cheering.
[01:58:09.360 --> 01:58:14.360]   I have to think Apple is running a big risk here.
[01:58:14.360 --> 01:58:15.360]   I mean, I don't know.
[01:58:15.360 --> 01:58:17.360]   What do you am I wrong?
[01:58:17.360 --> 01:58:19.360]   I don't think that they're running a risk.
[01:58:19.360 --> 01:58:20.360]   I think that they need to be in the game.
[01:58:20.360 --> 01:58:22.360]   And I think that it is a game changer.
[01:58:22.360 --> 01:58:26.360]   There's certain things that AI and VR can be very effective.
[01:58:26.360 --> 01:58:37.360]   And I think that there's some things that it doesn't. And I think that there's still a whole bunch of problems with setup and with weight and with the amount of processing and, you know, people feeling sick.
[01:58:37.360 --> 01:58:43.360]   I think that there's things that they need to be able to solve. And if they don't solve it well, I think that it's better that they wait and put it out.
[01:58:43.360 --> 01:58:55.360]   But I think that it is a way of a future that we can be able to do things, learn skills, take care of things, go through different situations, visit places and experience things that you can't do.
[01:58:55.360 --> 01:59:00.360]   And I think that it's just not the time yet.
[01:59:00.360 --> 01:59:03.360]   I think that it's a little bit ahead of its own curve.
[01:59:03.360 --> 01:59:15.360]   Speaking of the watch, according again to German, the Apple Watch with glucose sensing is up to seven years away from launch.
[01:59:15.360 --> 01:59:18.360]   I'm not surprised.
[01:59:18.360 --> 01:59:24.360]   It's like one diabetic and she was very hopeful initially, but it's not looking good.
[01:59:24.360 --> 01:59:25.360]   Yeah.
[01:59:25.360 --> 01:59:30.360]   They spoke about that years ago. Like they said that they thought they were close and they ended up being a lot longer out.
[01:59:30.360 --> 01:59:34.360]   They kept buying companies that kept saying within a year, we'll have it. And every company they bought swore that.
[01:59:34.360 --> 01:59:39.360]   And then it turned out they were years and years away. And it just looked like a very, very hard problem to solve.
[01:59:39.360 --> 01:59:44.360]   Is it time to short Apple stock if they finally reach their peak?
[01:59:44.360 --> 01:59:53.360]   I've stopped using stock advice because I was asked by a mate when Google went public, should I invest in Google?
[01:59:53.360 --> 01:59:57.360]   I was like, no, there'll be a mess that's belonging to India.
[01:59:57.360 --> 01:59:58.360]   I'll be making it.
[01:59:58.360 --> 02:00:02.360]   I mean, Microsoft is relevant again, right? Like Microsoft's kind of like...
[02:00:02.360 --> 02:00:05.360]   It's still a sore point for him.
[02:00:05.360 --> 02:00:11.360]   Yeah. I mean, look, even if so their plan is to release this headset for $3,000.
[02:00:11.360 --> 02:00:16.360]   I mean, it's aimed at, I guess, developers and companies and people want to see what they can do with it.
[02:00:16.360 --> 02:00:20.360]   It's not a consumer product yet. They aren't going to make a whole lot of them.
[02:00:20.360 --> 02:00:24.360]   They expect to sell about a million, which for Apple is nothing.
[02:00:24.360 --> 02:00:25.360]   Yeah.
[02:00:25.360 --> 02:00:26.360]   That would make it...
[02:00:26.360 --> 02:00:29.360]   Is it the $600 iPhone or the $600 HomePod?
[02:00:29.360 --> 02:00:31.360]   Yeah, that's a good question, isn't it?
[02:00:31.360 --> 02:00:39.360]   And of course, Bloomberg rubs Salton the Wound by posting this picture in the article of a MetaQuest Pro headset
[02:00:39.360 --> 02:00:44.360]   with a guy who looks like an absolute dork using it.
[02:00:44.360 --> 02:00:47.360]   He doesn't look bad. It's the hands.
[02:00:47.360 --> 02:00:49.360]   Yeah. What's he doing? Is he playing the piano?
[02:00:49.360 --> 02:00:51.360]   Why did he have a loss of raptor hands?
[02:00:51.360 --> 02:00:52.360]   Yeah, that's it.
[02:00:52.360 --> 02:00:56.360]   He's doing the Wednesday-outums dance.
[02:00:56.360 --> 02:01:01.360]   Almost universally stock art with people in VR headsets makes them look like idiots,
[02:01:01.360 --> 02:01:03.360]   but that's kind of what you learn from anime.
[02:01:03.360 --> 02:01:07.360]   Why didn't you learn from anime? We've got like 40 years of anime to draw on cool headsets.
[02:01:07.360 --> 02:01:11.360]   You always think you're cooler looking than you are when you're there.
[02:01:11.360 --> 02:01:14.360]   When you're wearing it, you feel pretty cool in this amazing world.
[02:01:14.360 --> 02:01:15.360]   Do you think...
[02:01:15.360 --> 02:01:16.360]   I don't know where it's like...
[02:01:16.360 --> 02:01:19.360]   I feel kind of sweaty, but no, it's true.
[02:01:19.360 --> 02:01:22.360]   Do you think without games that Apple, it's a non-starter?
[02:01:22.360 --> 02:01:24.360]   Do you think you have to have a game to sell these?
[02:01:24.360 --> 02:01:28.360]   I think the games help because that's what we've traditionally been using it for,
[02:01:28.360 --> 02:01:33.360]   but if they have a great AI interface where it's kind of that Terminator world
[02:01:33.360 --> 02:01:36.360]   where you're able to find out information that you need,
[02:01:36.360 --> 02:01:40.360]   if it actually increases, like gives you more time,
[02:01:40.360 --> 02:01:43.360]   is able to do things so that you can do things more effectively,
[02:01:43.360 --> 02:01:45.360]   more efficiently, more effortlessly,
[02:01:45.360 --> 02:01:48.360]   then it would be another way of going through it,
[02:01:48.360 --> 02:01:51.360]   but it's always that thing of changing people's perception of how they do things.
[02:01:51.360 --> 02:01:55.360]   It's really hard to be able to do, and so that's why it would be faster
[02:01:55.360 --> 02:01:57.360]   to be able to do it through gaming.
[02:01:57.360 --> 02:01:59.360]   That's more because we're...
[02:01:59.360 --> 02:02:02.360]   like we have these prehistoric brains where we get stuck on one way of doing things
[02:02:02.360 --> 02:02:04.360]   and change is hard for us.
[02:02:04.360 --> 02:02:06.360]   Yeah.
[02:02:06.360 --> 02:02:09.360]   I needed to make a particle like I used to power my arc reactor,
[02:02:09.360 --> 02:02:11.360]   so I stopped turning green this.
[02:02:11.360 --> 02:02:13.360]   It feels like you have to have a kill around.
[02:02:13.360 --> 02:02:14.360]   You're never going to be that cool.
[02:02:14.360 --> 02:02:16.360]   If you're going to sell $3,000 headset,
[02:02:16.360 --> 02:02:18.360]   there's got to be at least one thing that you want to do in it.
[02:02:18.360 --> 02:02:21.360]   And I don't want to meet people in virtual reality,
[02:02:21.360 --> 02:02:24.360]   even if Apple could figure out how to put legs on them.
[02:02:24.360 --> 02:02:26.360]   I... games...
[02:02:26.360 --> 02:02:28.360]   But they have a very different philosophy.
[02:02:28.360 --> 02:02:29.360]   Like, Facebook really...
[02:02:29.360 --> 02:02:31.360]   or at least, like, from what Facebook had said,
[02:02:31.360 --> 02:02:33.360]   they really want us to live in the metaverse.
[02:02:33.360 --> 02:02:36.360]   They want us to have meetings and converse and do shopping,
[02:02:36.360 --> 02:02:38.360]   where Tim Cook, I forget where he said it, but he's like,
[02:02:38.360 --> 02:02:40.360]   "We want you to... we want it to be like TV.
[02:02:40.360 --> 02:02:42.360]   You put it on, you have an enjoyable experience,
[02:02:42.360 --> 02:02:44.360]   you take it off and you interact with humans again."
[02:02:44.360 --> 02:02:47.360]   And they have, like, a fundamental difference in vision
[02:02:47.360 --> 02:02:49.360]   for what that technology should do than Facebook.
[02:02:49.360 --> 02:02:51.360]   But I think that that's why meta failed so poorly,
[02:02:51.360 --> 02:02:52.360]   is that it is a...
[02:02:52.360 --> 02:02:55.360]   and whenever you do something, be it a game or not,
[02:02:55.360 --> 02:02:58.360]   it has to be better than it is in reality,
[02:02:58.360 --> 02:03:00.360]   because if not, why do it?
[02:03:00.360 --> 02:03:03.360]   And for wearing a headset and not being able to have
[02:03:03.360 --> 02:03:06.360]   the facial reactions be appropriate,
[02:03:06.360 --> 02:03:09.360]   and things are weird, it right away pulls you out of the meeting,
[02:03:09.360 --> 02:03:12.360]   it's distracting, it's uncomfortable, it's heavy.
[02:03:12.360 --> 02:03:15.360]   And so that's where meta was never gonna...
[02:03:15.360 --> 02:03:17.360]   Sorry, I put people that are into it, but, like,
[02:03:17.360 --> 02:03:18.360]   it was never gonna happen.
[02:03:18.360 --> 02:03:21.360]   It's just the wrong situation for something that's wrong.
[02:03:21.360 --> 02:03:24.360]   It's much better to do something on Zoom,
[02:03:24.360 --> 02:03:27.360]   where you can actually see people closer to real time.
[02:03:27.360 --> 02:03:30.360]   And so, the facial reactions, which are so important
[02:03:30.360 --> 02:03:32.360]   for social interactions, are accurate.
[02:03:32.360 --> 02:03:34.360]   And so because of that, we're getting all of these force
[02:03:34.360 --> 02:03:36.360]   feedings and people's hands are floating around
[02:03:36.360 --> 02:03:39.360]   and their bodies look weird, and so you can't really relax
[02:03:39.360 --> 02:03:41.360]   and be able to deal with the meeting, and you're uncomfortable
[02:03:41.360 --> 02:03:43.360]   and you have to itch, and there's something on your eye,
[02:03:43.360 --> 02:03:46.360]   and it's suddenly fuzzy, and you've created condensation
[02:03:46.360 --> 02:03:49.360]   and you can't find the little tiny cloth to be able to clean
[02:03:49.360 --> 02:03:51.360]   your eye thing, and then your head's fallen off,
[02:03:51.360 --> 02:03:53.360]   because you've taken your headset off, you have to go to the
[02:03:53.360 --> 02:03:56.360]   washroom, everyone notices that your head's floating around.
[02:03:56.360 --> 02:03:58.360]   - You do this to yourself.
[02:03:58.360 --> 02:04:00.360]   - It's not the verge meeting, right?
[02:04:00.360 --> 02:04:04.360]   They filmed Nielei and Alex and everybody doing the Horizon
[02:04:04.360 --> 02:04:06.360]   meeting, and it was just like their eyes bugging out.
[02:04:06.360 --> 02:04:08.360]   - It's not good. It's not good.
[02:04:08.360 --> 02:04:10.360]   - It's really awful. - Yeah.
[02:04:10.360 --> 02:04:13.360]   - It's sweet. - Like, I want it to be like Terminator.
[02:04:13.360 --> 02:04:16.360]   I want it to be able to give me all the stats and information
[02:04:16.360 --> 02:04:19.360]   of everything that I interact with, how to deal with it,
[02:04:19.360 --> 02:04:22.360]   how to buy it, where it is on Amazon.
[02:04:22.360 --> 02:04:25.360]   Like, I want that kind of information at my fingertips,
[02:04:25.360 --> 02:04:29.360]   and I want it to be comfortable, and I want myself to look cooler,
[02:04:29.360 --> 02:04:31.360]   not less cool while I wear it.
[02:04:31.360 --> 02:04:34.360]   Like, if it doesn't solve these problems, it's not going to be
[02:04:34.360 --> 02:04:36.360]   something that I'm going to wear for long periods of time.
[02:04:36.360 --> 02:04:39.360]   - I got to decide whether to spend $3,000 on it.
[02:04:39.360 --> 02:04:43.360]   I mean, I have a reason to do it, which is so we can talk about it,
[02:04:43.360 --> 02:04:46.360]   but I'm really reluctant to spend that much.
[02:04:46.360 --> 02:04:52.360]   I already spent $1,600, $1,500 on a Quest Pro that, you know,
[02:04:52.360 --> 02:04:56.360]   I am really reluctant to spend twice that on an Apple headset
[02:04:56.360 --> 02:04:57.360]   that will be...
[02:04:57.360 --> 02:05:01.360]   - Yeah, I mean, $3,000 is a lot to shell out.
[02:05:01.360 --> 02:05:04.360]   And I think one of the things that Microsoft found with HoloLens
[02:05:04.360 --> 02:05:07.360]   was you can build the hardware, but unless you've got the apps,
[02:05:07.360 --> 02:05:09.360]   you've got to have the apps. - You've got to have the apps.
[02:05:09.360 --> 02:05:11.360]   - Yeah. - You know, they tried to get the US military
[02:05:11.360 --> 02:05:13.360]   sold on this, and they tried it out and said,
[02:05:13.360 --> 02:05:15.360]   "Yeah, this is going to get sold just killed."
[02:05:15.360 --> 02:05:19.360]   So, you know, I mean, they've been trying to be the engineering field,
[02:05:19.360 --> 02:05:22.360]   surprisingly not pushing it for gamers.
[02:05:22.360 --> 02:05:25.360]   I mean, Georgia, I'm curious about your thoughts on this,
[02:05:25.360 --> 02:05:28.360]   but I mean, you would have thought, you know,
[02:05:28.360 --> 02:05:30.360]   "Halo saved Xbox." - Yes.
[02:05:30.360 --> 02:05:33.360]   - So why is this not something similar to HoloLens?
[02:05:33.360 --> 02:05:40.360]   - One Halo, because of Horizon is why I got the PlayStation VR,
[02:05:40.360 --> 02:05:43.360]   was just for that game, because I love Horizon.
[02:05:43.360 --> 02:05:46.360]   It's a really great game. It's quite well done.
[02:05:46.360 --> 02:05:48.360]   Not as it's more of a climbing game, like you're climbing a lot of the time,
[02:05:48.360 --> 02:05:50.360]   so it's a little bit disappointing.
[02:05:50.360 --> 02:05:52.360]   But that's what I got it for.
[02:05:52.360 --> 02:05:55.360]   And so I think that if they had Halo, and again,
[02:05:55.360 --> 02:05:59.360]   that would be a really great experience in VR,
[02:05:59.360 --> 02:06:02.360]   especially more hand-to-hand combat, like if we end up having,
[02:06:02.360 --> 02:06:04.360]   like, some sort of like a Whirlpool sort,
[02:06:04.360 --> 02:06:06.360]   like that's just an awesome feeling,
[02:06:06.360 --> 02:06:08.360]   and it's a great workout at the same time.
[02:06:08.360 --> 02:06:10.360]   People would purchase it, but without that,
[02:06:10.360 --> 02:06:15.360]   then you have to offer something that is better than what you would do naturally
[02:06:15.360 --> 02:06:18.360]   and interact with naturally in the real world.
[02:06:18.360 --> 02:06:20.360]   And so what would be better than that?
[02:06:20.360 --> 02:06:23.360]   'Cause I can't fight people in the real world.
[02:06:23.360 --> 02:06:25.360]   I enjoy doing it in gaming.
[02:06:25.360 --> 02:06:27.360]   I can't control different, you know,
[02:06:27.360 --> 02:06:30.360]   robotics and be able to build things and work in a Mac.
[02:06:30.360 --> 02:06:33.360]   Like, these are things that people would enjoy doing in VR.
[02:06:33.360 --> 02:06:37.360]   If they don't have that, I don't see it being a wide mass appeal.
[02:06:37.360 --> 02:06:39.360]   And they have to be able to make it profitable.
[02:06:39.360 --> 02:06:42.360]   And so then it ends up being something that's very niche, not very profitable.
[02:06:42.360 --> 02:06:46.360]   And then people have to warm up to doing things differently.
[02:06:46.360 --> 02:06:50.360]   And even just changing a steering wheel on a car ends up having a whole bunch of critical reviews,
[02:06:50.360 --> 02:06:52.360]   even if it's a better way to handle a steering wheel,
[02:06:52.360 --> 02:06:54.360]   because we're already used to it.
[02:06:54.360 --> 02:06:57.360]   We don't want the extra overhead of having to learn something new.
[02:06:57.360 --> 02:07:02.360]   So if they don't end up solving for the problem of our own human biology
[02:07:02.360 --> 02:07:05.360]   of the way that we work things, then it's not going to be successful.
[02:07:05.360 --> 02:07:06.360]   Yeah.
[02:07:06.360 --> 02:07:10.360]   Two of my biggest questions in technology was why Microsoft never made an Xbox phone
[02:07:10.360 --> 02:07:14.360]   instead of a Windows phone, just had it in carrier stores with Xbox and branding
[02:07:14.360 --> 02:07:16.360]   right next to an iPhone and a Samsung phone,
[02:07:16.360 --> 02:07:18.360]   and why they didn't make a Halo lens instead of a Halo lens.
[02:07:18.360 --> 02:07:20.360]   And had it all on all the shelves.
[02:07:20.360 --> 02:07:24.360]   It just seems like those were such a better, stronger consumer brand.
[02:07:24.360 --> 02:07:26.360]   But they want to sell Windows to kids.
[02:07:26.360 --> 02:07:28.360]   It looks like the helmet.
[02:07:28.360 --> 02:07:31.360]   It looks like the helmet goes awesome.
[02:07:31.360 --> 02:07:33.360]   How did that be awesome?
[02:07:33.360 --> 02:07:34.360]   Oh, yeah.
[02:07:34.360 --> 02:07:36.360]   I would love that.
[02:07:36.360 --> 02:07:39.360]   I would wear it just out because you would look so cool.
[02:07:39.360 --> 02:07:40.360]   I love that.
[02:07:40.360 --> 02:07:41.360]   I love that.
[02:07:41.360 --> 02:07:42.360]   I do.
[02:07:42.360 --> 02:07:43.360]   It's to cost players.
[02:07:43.360 --> 02:07:44.360]   You're exactly right.
[02:07:44.360 --> 02:07:45.360]   They want to be Master G.
[02:07:45.360 --> 02:07:46.360]   Oh, yeah.
[02:07:46.360 --> 02:07:47.360]   Hey, we got the helmet already.
[02:07:47.360 --> 02:07:48.360]   Why not?
[02:07:48.360 --> 02:07:49.360]   Yeah.
[02:07:49.360 --> 02:07:50.360]   Absolutely.
[02:07:50.360 --> 02:07:52.360]   I want to Master G.
[02:07:52.360 --> 02:07:55.360]   I mean, coming to your point, Renee, about mobile gaming phones,
[02:07:55.360 --> 02:08:00.360]   I think the endgates scared a lot of people, a lot of companies off that because, you know,
[02:08:00.360 --> 02:08:07.360]   all this work was put into doing a handset which could play games and it died on its backside.
[02:08:07.360 --> 02:08:10.360]   It turned out the iPhone was an amazing gaming machine.
[02:08:10.360 --> 02:08:11.360]   Who knew?
[02:08:11.360 --> 02:08:12.360]   I'm just saying branding.
[02:08:12.360 --> 02:08:13.360]   Like, brand it.
[02:08:13.360 --> 02:08:14.360]   Brand it like an Xbox for the kids.
[02:08:14.360 --> 02:08:16.360]   So it went way more than the office people.
[02:08:16.360 --> 02:08:24.360]   Speaking of which, it looks like the EU is going to force Apple to open up its App Store.
[02:08:24.360 --> 02:08:26.360]   And Microsoft is already planning, apparently.
[02:08:26.360 --> 02:08:28.360]   We've talked about this before.
[02:08:28.360 --> 02:08:34.360]   A store that would allow you to buy Microsoft games or it will be a competing store with
[02:08:34.360 --> 02:08:36.360]   the App Store, I guess, right?
[02:08:36.360 --> 02:08:38.360]   They won't be the only ones.
[02:08:38.360 --> 02:08:42.360]   I think there are quite a few other companies, including Epic, that want to do App Store
[02:08:42.360 --> 02:08:44.360]   on the iPhone.
[02:08:44.360 --> 02:08:48.360]   You think this is going to be a, this is not probably until next spring, right?
[02:08:48.360 --> 02:08:50.360]   Next year at some point.
[02:08:50.360 --> 02:08:51.360]   But that's going to be a...
[02:08:51.360 --> 02:08:55.360]   But the EU is actually taking the move on this rather than American.
[02:08:55.360 --> 02:08:56.360]   Yeah.
[02:08:56.360 --> 02:09:01.360]   It's just like, we've seen this time and again when it comes to enforcement, you go to the EU.
[02:09:01.360 --> 02:09:05.360]   But yeah, you know, honestly, the App Store thing is that...
[02:09:05.360 --> 02:09:11.360]   The idea that you've just got to pay over 30% or 15% as it is now, it's just...
[02:09:11.360 --> 02:09:13.360]   It's a basic rent seeking.
[02:09:13.360 --> 02:09:14.360]   Right.
[02:09:14.360 --> 02:09:17.360]   I just found it like super disappointing.
[02:09:17.360 --> 02:09:20.360]   Like, because like, a lot of this could have...
[02:09:20.360 --> 02:09:22.360]   I don't know, I obviously don't run Apple.
[02:09:22.360 --> 02:09:23.360]   I don't have those responsibilities.
[02:09:23.360 --> 02:09:25.360]   I don't know everything that's involved.
[02:09:25.360 --> 02:09:29.360]   But it just seems like with a lot of these things, like the iPhone was always designed to be a console.
[02:09:29.360 --> 02:09:34.360]   Like from the very first time they announced the SDK, it was clear like this is not an open computing platform.
[02:09:34.360 --> 02:09:35.360]   This is a console.
[02:09:35.360 --> 02:09:36.360]   But they stuck with it.
[02:09:36.360 --> 02:09:38.360]   And in 30% back then wasn't very much.
[02:09:38.360 --> 02:09:40.360]   But like, things change over the years.
[02:09:40.360 --> 02:09:44.360]   And it seems like either just allowing people to buy things over the web, like just...
[02:09:44.360 --> 02:09:50.360]   You have to use the App Store on the phone, but you can go make your payments on the web if you want to get Netflix or you want to get Amazon books.
[02:09:50.360 --> 02:09:54.360]   Like, this seems like a whole bunch of steps that could have been taken that would mitigate all of this.
[02:09:54.360 --> 02:10:00.360]   And let's say we're like supremely confident they would win and would never be challenged, even if they changed nothing for 50 years.
[02:10:00.360 --> 02:10:04.360]   It just seems like a bad bet to make, especially when...
[02:10:04.360 --> 02:10:06.360]   And this goes for a lot of companies.
[02:10:06.360 --> 02:10:09.360]   All the stuff that ends up hurting them is not their core business.
[02:10:09.360 --> 02:10:13.360]   Like the App Store is super successful, but it's not where Apple makes their money.
[02:10:13.360 --> 02:10:14.360]   It's not their core product.
[02:10:14.360 --> 02:10:17.360]   And it's a big part of the services revenue, though, isn't it?
[02:10:17.360 --> 02:10:19.360]   A big part of the services.
[02:10:19.360 --> 02:10:22.360]   Yeah, but like services revenue is still like compared to like iPhone money.
[02:10:22.360 --> 02:10:25.360]   It's nothing to bet the company.
[02:10:25.360 --> 02:10:26.360]   Maybe it is.
[02:10:26.360 --> 02:10:27.360]   Like again, like I'm ignorant about this stuff.
[02:10:27.360 --> 02:10:29.360]   Maybe it is something to bet the company over.
[02:10:29.360 --> 02:10:34.360]   But just like letting them do web payments would have taken so much of the pressure away from this.
[02:10:34.360 --> 02:10:36.360]   And now it's going to be forced to be another.
[02:10:36.360 --> 02:10:41.360]   And the thing that like that kind of bugs me a little bit is that as nerds, we have everything.
[02:10:41.360 --> 02:10:44.360]   Like we have Linux and Windows and the Mac and Android.
[02:10:44.360 --> 02:10:48.360]   And there's like a couple products that like people who just want a computing appliance have.
[02:10:48.360 --> 02:10:51.360]   They have like iPads and Chromebooks basically.
[02:10:51.360 --> 02:10:53.360]   And like we look at these things and salivates.
[02:10:53.360 --> 02:10:54.360]   They have the hardware is nice.
[02:10:54.360 --> 02:10:55.360]   We want them to be nerd products too.
[02:10:55.360 --> 02:11:00.360]   And then we make them complicated to the point where like I've seen my parents iPads,
[02:11:00.360 --> 02:11:01.360]   they're nightmares now.
[02:11:01.360 --> 02:11:03.360]   They used to be like these simple computers they could use.
[02:11:03.360 --> 02:11:05.360]   Now they have 18 versions of mail open at all times.
[02:11:05.360 --> 02:11:07.360]   And they don't even know that they're open.
[02:11:07.360 --> 02:11:09.360]   Like it's just like they're no longer consumer friendly.
[02:11:09.360 --> 02:11:10.360]   They're no longer nerd friendly.
[02:11:10.360 --> 02:11:18.360]   And I worry that we've sort of taken away an option from the market that we won't be able to give back to the mainstream computing people for whom they were designed.
[02:11:18.360 --> 02:11:19.360]   Let's take a little break.
[02:11:19.360 --> 02:11:23.360]   We have a bunch of final thoughts, final stories.
[02:11:23.360 --> 02:11:27.360]   Including some very, very good news about Mr. and Mrs. Pickles.
[02:11:27.360 --> 02:11:33.360]   But before we do that, let me tell you about Shopify.
[02:11:33.360 --> 02:11:37.360]   There's that sound.
[02:11:37.360 --> 02:11:41.360]   There's that sound of another sale on Shopify.
[02:11:41.360 --> 02:11:45.360]   The moment another person's business dream becomes a reality.
[02:11:45.360 --> 02:11:47.360]   Our show today brought to you by Shopify.
[02:11:47.360 --> 02:11:55.360]   I have a kind of a soft spot for Shopify because I was talking about my son, the chef.
[02:11:55.360 --> 02:12:00.360]   He was able to build an amazing website.
[02:12:00.360 --> 02:12:01.360]   It made it easy.
[02:12:01.360 --> 02:12:04.360]   He didn't know anything about building a website or selling online.
[02:12:04.360 --> 02:12:07.360]   But as soon as he was able to set it up with Shopify.
[02:12:07.360 --> 02:12:12.360]   And I asked Hank, I said, how important has Shopify been to your business?
[02:12:12.360 --> 02:12:13.360]   He said, it's everything.
[02:12:13.360 --> 02:12:16.360]   I had no idea how to do this.
[02:12:16.360 --> 02:12:22.360]   Shopify gave me a beautiful website that I could sell.
[02:12:22.360 --> 02:12:24.360]   In fact, he's been selling salt like crazy.
[02:12:24.360 --> 02:12:29.360]   It's sold out almost as soon as he can create it.
[02:12:29.360 --> 02:12:32.360]   Shopify is very empowering technology.
[02:12:32.360 --> 02:12:34.360]   No matter how big you want to grow.
[02:12:34.360 --> 02:12:38.360]   Shopify handled all of a sudden a huge amount of traffic.
[02:12:38.360 --> 02:12:39.360]   He sold a ton of salt.
[02:12:39.360 --> 02:12:42.360]   Shopify is there to empower you with the confidence and control
[02:12:42.360 --> 02:12:44.360]   to take your business to the next level.
[02:12:44.360 --> 02:12:51.360]   Whether you're selling fedoras or bike helmets, Shopify simplifies selling online and in person
[02:12:51.360 --> 02:12:55.360]   so you can focus on successfully growing your business.
[02:12:55.360 --> 02:12:59.360]   Shopify covers every sales channel from an in-person point of sale system.
[02:12:59.360 --> 02:13:07.360]   Yeah, you can use it in the real world, in the IRL, to an all-in-one e-commerce platform like Hank's.
[02:13:07.360 --> 02:13:11.360]   It even lets you sell across social media marketplaces like TikTok, Facebook, and Instagram
[02:13:11.360 --> 02:13:14.360]   and I can tell you, Hank's been using that too.
[02:13:14.360 --> 02:13:17.360]   Packed with industry leading tools ready to ignite your growth,
[02:13:17.360 --> 02:13:23.360]   Shopify gives you complete control over your business and your brand without having to learn any new skills
[02:13:23.360 --> 02:13:25.360]   and design or code.
[02:13:25.360 --> 02:13:29.360]   Thanks to 24/7 Help and an extensive business course library,
[02:13:29.360 --> 02:13:34.360]   Shopify is there to support your success every step of the way.
[02:13:34.360 --> 02:13:35.360]   It's not just my son.
[02:13:35.360 --> 02:13:37.360]   My daughter also has a Shopify site.
[02:13:37.360 --> 02:13:43.360]   It's the whole family is doing it on Shopify.
[02:13:43.360 --> 02:13:55.360]   Thank you, Shopify, for empowering the LaPorte children's kind of amazing getting off of Dad's payroll plan.
[02:13:55.360 --> 02:13:57.360]   Very well done.
[02:13:57.360 --> 02:13:59.360]   Couldn't have done it with that Shopify.
[02:13:59.360 --> 02:14:02.360]   Love Shopify for so many reasons.
[02:14:02.360 --> 02:14:04.360]   I want you to check it out.
[02:14:04.360 --> 02:14:07.360]   I'm going to help you get started with Shopify.
[02:14:07.360 --> 02:14:14.360]   Sign up for a dollar a month trial period, one buck a month at Shopify.com/Twit.
[02:14:14.360 --> 02:14:18.360]   Now's your turn to get serious about selling and try Shopify today.
[02:14:18.360 --> 02:14:21.360]   This is Possibility Powered by Shopify.
[02:14:21.360 --> 02:14:23.360]   It blows me away.
[02:14:23.360 --> 02:14:26.360]   Go to Shopify.com/Twit, all lowercase.
[02:14:26.360 --> 02:14:29.360]   Take your business to the next level today.
[02:14:29.360 --> 02:14:31.360]   Shopify.com/Twit.
[02:14:31.360 --> 02:14:44.360]   To me, as the promise of the internet, to give anybody the chance to sell online to a global marketplace, a global audience, to build beautiful websites that really work.
[02:14:44.360 --> 02:14:46.360]   Hank's done it.
[02:14:46.360 --> 02:14:47.360]   Abby's done it.
[02:14:47.360 --> 02:14:48.360]   Maybe you're next.
[02:14:48.360 --> 02:14:50.360]   Shopify.com/Twit.
[02:14:50.360 --> 02:14:53.360]   Get that $1 a month trial period started right now.
[02:14:53.360 --> 02:14:57.360]   What are you going to sell on Shopify?
[02:14:57.360 --> 02:14:58.360]   Let us see.
[02:14:58.360 --> 02:14:59.360]   Not the Twitter source code.
[02:14:59.360 --> 02:15:00.360]   That would be wrong.
[02:15:00.360 --> 02:15:01.360]   Don't sell it.
[02:15:01.360 --> 02:15:06.360]   Don't sell it.
[02:15:06.360 --> 02:15:08.360]   Here's Cara Swisher's tweet.
[02:15:08.360 --> 02:15:12.360]   "Intelligence has its limitations, but stupidity is infinite."
[02:15:12.360 --> 02:15:17.360]   Twitter says part of its source code was leaked online.
[02:15:17.360 --> 02:15:22.360]   Friday night, they were appealing to GitHub to take it down.
[02:15:22.360 --> 02:15:29.360]   Remember that Musk promised to release to open source the algorithm, but then he fired anybody who knew how to do it.
[02:15:29.360 --> 02:15:33.360]   Musk promises a lot of things.
[02:15:33.360 --> 02:15:41.360]   The thing that this showed up was, if you fire half the company, someone's going to get really messy about it.
[02:15:41.360 --> 02:15:48.360]   It's just stupidity, but it's easily observable that it's going to happen.
[02:15:48.360 --> 02:15:50.360]   I don't know what you would do with the source code.
[02:15:50.360 --> 02:15:52.360]   I don't know how valuable that is.
[02:15:52.360 --> 02:15:54.360]   Oh, I know exploits they're worried about.
[02:15:54.360 --> 02:15:57.360]   Yeah, and that's exactly what happened to LastPass.
[02:15:57.360 --> 02:16:02.360]   The hack of LastPass began with something seemingly innocuous.
[02:16:02.360 --> 02:16:03.360]   They said, "Oh, don't worry about it.
[02:16:03.360 --> 02:16:07.360]   They just got into our source code repositories."
[02:16:07.360 --> 02:16:09.360]   But that was the beginning of the big hack.
[02:16:09.360 --> 02:16:16.360]   I think this is always a cause for concern.
[02:16:16.360 --> 02:16:22.360]   Speaking of Elon, Semaphore has the story of how Elon left OpenAI.
[02:16:22.360 --> 02:16:32.360]   For Elon Musk, along with Reid Hoffman, joined Sam Altman, formerly of Y Cominated, a form OpenAI, a nonprofit in 2015.
[02:16:32.360 --> 02:16:40.360]   The idea is the group pledged a billion dollars to take development of artificial intelligence out of the closed,
[02:16:40.360 --> 02:16:45.360]   private hands of companies like Google and to do it in public.
[02:16:45.360 --> 02:16:56.360]   In early 2018, Musk told Sam Altman, current CEO of OpenAI, that he believed the venture had fallen fatally behind Google.
[02:16:56.360 --> 02:16:58.360]   Google's beaten us.
[02:16:58.360 --> 02:17:00.360]   He proposed a possible solution.
[02:17:00.360 --> 02:17:04.360]   This is, by the way, Reid Albergotti's scoop on Semaphore.
[02:17:04.360 --> 02:17:05.360]   Good job, Reid.
[02:17:05.360 --> 02:17:06.360]   He was on a couple of weeks ago.
[02:17:06.360 --> 02:17:10.360]   Elon said, "Let me take control of OpenAI and run it myself."
[02:17:10.360 --> 02:17:14.360]   Apparently, Sam knew something about Elon that we didn't know until recently and said no.
[02:17:14.360 --> 02:17:16.360]   They rejected his proposal.
[02:17:16.360 --> 02:17:22.360]   Musk walked away from the company, reneged on a massive planned donation.
[02:17:22.360 --> 02:17:24.360]   He's no longer a part of OpenAI.
[02:17:24.360 --> 02:17:26.360]   Maybe he wishes he was.
[02:17:26.360 --> 02:17:28.360]   He was tweeting about it earlier this week.
[02:17:28.360 --> 02:17:29.360]   He was tweeting about it earlier this week.
[02:17:29.360 --> 02:17:32.360]   He gave all this money to an open source charity.
[02:17:32.360 --> 02:17:34.360]   Now it's a full of profit.
[02:17:34.360 --> 02:17:36.360]   Well, he's got a point there.
[02:17:36.360 --> 02:17:42.360]   OpenAI is no longer an open project.
[02:17:42.360 --> 02:17:45.360]   It's a for-profit project, right?
[02:17:45.360 --> 02:17:46.360]   No, no.
[02:17:46.360 --> 02:17:48.360]   They're hugely Microsoft-funded.
[02:17:48.360 --> 02:17:53.360]   It's entirely for profit.
[02:17:53.360 --> 02:18:00.360]   They've got a fig leaf charitable side, which is about as useful as a fifth leg.
[02:18:00.360 --> 02:18:03.360]   It's not how Chet GPT is coming from, that's for sure.
[02:18:03.360 --> 02:18:06.360]   Elon did finally come through on his stock grants.
[02:18:06.360 --> 02:18:11.360]   Remember, he'd promised stock options to any employees that stayed at Twitter.
[02:18:11.360 --> 02:18:17.360]   He is, unfortunately, though, valuing the company at about $20 billion, less than half,
[02:18:17.360 --> 02:18:20.360]   but he paid for it.
[02:18:20.360 --> 02:18:26.360]   I'm still not convinced he wasn't having a really good do-be, and he just thought, "Yeah, I'll buy it."
[02:18:26.360 --> 02:18:27.360]   Yeah.
[02:18:27.360 --> 02:18:28.360]   And you know what?
[02:18:28.360 --> 02:18:29.360]   He's probably accurate.
[02:18:29.360 --> 02:18:33.360]   The $20 billion is a lot closer to the actual value.
[02:18:33.360 --> 02:18:36.360]   He did email the staff saying he's optimistic about the future.
[02:18:36.360 --> 02:18:45.360]   I see a clear but difficult path to a greater than $250 billion valuation, meaning stock-grated
[02:18:45.360 --> 02:18:46.360]   head on wallets.
[02:18:46.360 --> 02:18:48.360]   He wants that to be a currency exchange.
[02:18:48.360 --> 02:18:50.360]   Like, what's called an everything app?
[02:18:50.360 --> 02:18:54.360]   Yeah, I don't know if he really wants it to be that or he's just like everything else,
[02:18:54.360 --> 02:18:56.360]   talking off, shooting off the hip.
[02:18:56.360 --> 02:19:03.360]   I mean, I want Rainbow Unicorn that passes pure need to be real ill, but it's not going to happen.
[02:19:03.360 --> 02:19:07.360]   I actually ordered one of those on Indiegogo about seven or eight years ago, and I never
[02:19:07.360 --> 02:19:08.360]   got it.
[02:19:08.360 --> 02:19:09.360]   George, I got the last one.
[02:19:09.360 --> 02:19:11.360]   Did she turn it?
[02:19:11.360 --> 02:19:12.360]   Sorry.
[02:19:12.360 --> 02:19:13.360]   I'll mine.
[02:19:13.360 --> 02:19:14.360]   Shocks.
[02:19:14.360 --> 02:19:20.360]   Hey, before we get to the final stories of the day, I want to remind you that this is
[02:19:20.360 --> 02:19:25.360]   an all-week-long network with lots of shows besides Twit.
[02:19:25.360 --> 02:19:29.360]   In fact, we've made a little minimovie to tease you about some of the things you might have
[02:19:29.360 --> 02:19:31.360]   missed this week, watch.
[02:19:31.360 --> 02:19:41.360]   Let's imagine Leo LaPorte being rested after his bank heist.
[02:19:41.360 --> 02:19:43.360]   Here I am being arrested.
[02:19:43.360 --> 02:19:44.360]   I'm not happy about it.
[02:19:44.360 --> 02:19:45.360]   It's sweater, man.
[02:19:45.360 --> 02:19:46.360]   What are you doing?
[02:19:46.360 --> 02:19:47.360]   Which one you are right?
[02:19:47.360 --> 02:19:48.360]   You and your Britches.
[02:19:48.360 --> 02:19:49.360]   Me and my Britches.
[02:19:49.360 --> 02:19:50.360]   Mom, James.
[02:19:50.360 --> 02:19:51.360]   Yeah.
[02:19:51.360 --> 02:19:54.360]   Mom, James, I've never seen you in dinner.
[02:19:54.360 --> 02:19:57.360]   Previously on Twit.
[02:19:57.360 --> 02:19:58.360]   Twit news.
[02:19:58.360 --> 02:20:04.360]   I think our systems, cloud, and software partners, and especially our amazing employees, for
[02:20:04.360 --> 02:20:07.360]   building the NVIDIA Accelerated Computing ecosystem.
[02:20:07.360 --> 02:20:09.360]   That's the NVIDIA GTC keynote.
[02:20:09.360 --> 02:20:17.360]   And as we predicted, NVIDIA is firing on all cylinders in every area, and AI has just
[02:20:17.360 --> 02:20:19.360]   accelerated their growth.
[02:20:19.360 --> 02:20:21.360]   They're working with all the players.
[02:20:21.360 --> 02:20:25.360]   They're creating hardware that almost everybody uses.
[02:20:25.360 --> 02:20:28.360]   Even competitors like Google and Microsoft.
[02:20:28.360 --> 02:20:30.360]   Home theater geeks.
[02:20:30.360 --> 02:20:36.360]   Of course, I care deeply about audio and the quality of audio reproduction.
[02:20:36.360 --> 02:20:40.360]   And I can certainly hear the difference between good audio and bad audio.
[02:20:40.360 --> 02:20:45.360]   But there are several reasons why I don't think of myself as an audio file and why I
[02:20:45.360 --> 02:20:47.360]   don't spend a ton of money on it.
[02:20:47.360 --> 02:20:49.360]   This weekend Enterprise Tech.
[02:20:49.360 --> 02:20:54.360]   This PC Magazine article, it sounds like the script from a double-o-semin film.
[02:20:54.360 --> 02:21:01.360]   Someone in Ecuador has resorted to creating flash drives designed to explode once they
[02:21:01.360 --> 02:21:02.360]   connect to a computer.
[02:21:02.360 --> 02:21:03.360]   Twit.
[02:21:03.360 --> 02:21:06.360]   It's not your father's Twit.
[02:21:06.360 --> 02:21:07.360]   One more reason.
[02:21:07.360 --> 02:21:12.360]   Not to stick a hard drive or a USB thumb drive you found in the street into your computer.
[02:21:12.360 --> 02:21:15.360]   There's bad things that can happen.
[02:21:15.360 --> 02:21:20.360]   We thank, by the way, our club Twit members for making home theater geeks possible.
[02:21:20.360 --> 02:21:27.360]   They brought it back with their donations because your seven bucks a month helps us in so many
[02:21:27.360 --> 02:21:28.360]   ways.
[02:21:28.360 --> 02:21:31.360]   We're able to launch new shows this weekend space came out of the club.
[02:21:31.360 --> 02:21:35.360]   We do hands on Macintosh and hands on Windows now in the club with Paul Therat and Michael
[02:21:35.360 --> 02:21:36.360]   Sergeant.
[02:21:36.360 --> 02:21:38.360]   We do the Untitled Linux show in the Gizfiz.
[02:21:38.360 --> 02:21:42.360]   And yes, we just reintroduced home theater geeks.
[02:21:42.360 --> 02:21:48.360]   Not only do you get shows we don't put out anywhere else, you get ad-free versions of all
[02:21:48.360 --> 02:21:50.000]   of our shows.
[02:21:50.000 --> 02:21:55.080]   You also get access to the club Twit Discord, which is full of great stuff, including a
[02:21:55.080 --> 02:22:01.640]   lot of mid-journey and this animated GIFs and a bunch of mid-journey stuff.
[02:22:01.640 --> 02:22:10.360]   We have our own mid-journey channels including these ones of me going to prison.
[02:22:10.360 --> 02:22:14.120]   I didn't want to go to prison.
[02:22:14.120 --> 02:22:15.120]   You didn't go easily.
[02:22:15.120 --> 02:22:16.120]   I didn't go easily.
[02:22:16.120 --> 02:22:17.120]   I fought it.
[02:22:17.120 --> 02:22:21.520]   I think we took the, I think at one point, oh yeah, I know where it is.
[02:22:21.520 --> 02:22:26.920]   I think Anthony took the ones that were fake of Donald Trump and put my head on it.
[02:22:26.920 --> 02:22:30.160]   Actually, I think I wore it better to be honest with you.
[02:22:30.160 --> 02:22:33.760]   I think honestly Leo, you put your head on the Pope fake one, you know, with a puffy
[02:22:33.760 --> 02:22:34.760]   jacket.
[02:22:34.760 --> 02:22:36.160]   Oh, that was fake.
[02:22:36.160 --> 02:22:37.160]   Darn it.
[02:22:37.160 --> 02:22:38.160]   Oh, man.
[02:22:38.160 --> 02:22:42.400]   Now I saw the pope in a puffy jacket and I thought that is really cool.
[02:22:42.400 --> 02:22:47.080]   That is my understanding is that's fake, but I'm going to ask Rob.
[02:22:47.080 --> 02:22:48.080]   I'm going to ask Rob.
[02:22:48.080 --> 02:22:49.080]   Oh, man.
[02:22:49.080 --> 02:22:52.960]   See, it's already started where you see stuff and you don't know.
[02:22:52.960 --> 02:22:53.960]   You don't know.
[02:22:53.960 --> 02:22:54.960]   Is it real?
[02:22:54.960 --> 02:22:55.960]   Is it not?
[02:22:55.960 --> 02:22:58.440]   Have you seen the videos where they have the deep fake presidents playing magic, the gathering
[02:22:58.440 --> 02:23:00.360]   and kind of ones and dragons and stuff.
[02:23:00.360 --> 02:23:01.960]   They have like Trump.
[02:23:01.960 --> 02:23:02.960]   Yeah.
[02:23:02.960 --> 02:23:07.520]   So they basically you hear like discord coming on and then there's like Trump, Obama, Biden,
[02:23:07.520 --> 02:23:10.240]   sometimes the people else wearing their gamer headsets and they let's hurry up and do this
[02:23:10.240 --> 02:23:11.720]   or Michelle's not going to let me play anymore.
[02:23:11.720 --> 02:23:12.720]   All right.
[02:23:12.720 --> 02:23:13.720]   We're going to tear rank the transformers.
[02:23:13.720 --> 02:23:14.960]   Optimus Prime, he's S tier.
[02:23:14.960 --> 02:23:15.960]   No, he's not sleepy Joe.
[02:23:15.960 --> 02:23:16.960]   He's clearly D tier.
[02:23:16.960 --> 02:23:20.920]   Donald, would you like like and they just like they yelled each other the way you'd expect
[02:23:20.920 --> 02:23:24.680]   them to, but as friends who get together in game with each other all the time when their
[02:23:24.680 --> 02:23:26.480]   wives and like their families allow it.
[02:23:26.480 --> 02:23:27.480]   Of course.
[02:23:27.480 --> 02:23:29.000]   It's like it's way funnier than any rights to be.
[02:23:29.000 --> 02:23:30.000]   Of course.
[02:23:30.000 --> 02:23:35.760]   I don't know if these are deep fakes or just a good Photoshop's.
[02:23:35.760 --> 02:23:36.760]   The voices are deep fakes.
[02:23:36.760 --> 02:23:37.760]   Oh, are they?
[02:23:37.760 --> 02:23:38.760]   Let's see.
[02:23:38.760 --> 02:23:39.760]   All right.
[02:23:39.760 --> 02:23:40.760]   I'm going to turn on this.
[02:23:40.760 --> 02:23:42.840]   Will you promise that you guys at YouTube won't take a stand if I turn this on?
[02:23:42.840 --> 02:23:44.360]   No, you can't promise that.
[02:23:44.360 --> 02:23:45.360]   Well, you know what to call.
[02:23:45.360 --> 02:23:46.360]   I'm going to turn it on.
[02:23:46.360 --> 02:23:47.360]   We fought.
[02:23:47.360 --> 02:23:50.240]   I don't think you know how to operate a keyboard.
[02:23:50.240 --> 02:23:52.960]   Oh, Donald, I'm getting you banned.
[02:23:52.960 --> 02:23:57.200]   Okay, we got it right at the right place.
[02:23:57.200 --> 02:23:58.200]   Brilliant.
[02:23:58.200 --> 02:24:02.360]   We'll just stop right there, shall we?
[02:24:02.360 --> 02:24:06.880]   Yeah, like the longer versions where they play like D and D games or magic, the gathering
[02:24:06.880 --> 02:24:10.600]   games and they argue about their decks and Obama has like cards that should be illegal
[02:24:10.600 --> 02:24:12.920]   now and others have like overpowered.
[02:24:12.920 --> 02:24:16.200]   Just just, hey, we got to like that in real life.
[02:24:16.200 --> 02:24:17.960]   This is my night sorted now.
[02:24:17.960 --> 02:24:19.960]   I need to watch this.
[02:24:19.960 --> 02:24:20.960]   I'm not.
[02:24:20.960 --> 02:24:25.000]   I'm going to I really want to find out if the puffy Jack, I guess they wouldn't have
[02:24:25.000 --> 02:24:31.280]   a white people puffer jacket would probably not really be real.
[02:24:31.280 --> 02:24:32.280]   Probably not.
[02:24:32.280 --> 02:24:37.520]   I mean, it seems like it's AI, but you know, then again, it is the pope, you know, they
[02:24:37.520 --> 02:24:42.560]   he has the city with the six inches of bulletproof glass around it because there's faith in
[02:24:42.560 --> 02:24:43.560]   action.
[02:24:43.560 --> 02:24:46.760]   But yeah, I guess that's a fake.
[02:24:46.760 --> 02:24:51.280]   You know, this just shows you how it's got to the point where I'd have no critical faculties
[02:24:51.280 --> 02:24:52.280]   at all.
[02:24:52.280 --> 02:24:53.280]   I saw it and go, well, that's cool.
[02:24:53.280 --> 02:24:54.880]   They made him a puffer jacket.
[02:24:54.880 --> 02:24:56.880]   That's finally known not to believe anything.
[02:24:56.880 --> 02:24:59.800]   I finally had the AI for us to realize not to believe.
[02:24:59.800 --> 02:25:00.960]   I know I should have known.
[02:25:00.960 --> 02:25:01.960]   I mean, come on.
[02:25:01.960 --> 02:25:03.920]   The pope's not going to have a puffer jacket.
[02:25:03.920 --> 02:25:05.720]   You didn't need a puffer jacket.
[02:25:05.720 --> 02:25:12.320]   Yeah, I've got a song in the current papers made when I first saw it.
[02:25:12.320 --> 02:25:16.720]   I was kind of like, yeah, okay, he's progressive for wood thinking.
[02:25:16.720 --> 02:25:20.640]   He might be the person to drag the Catholic church, kicking and screaming into the 20th
[02:25:20.640 --> 02:25:21.640]   century.
[02:25:21.640 --> 02:25:23.640]   But you know, it was just like puffer jacket.
[02:25:23.640 --> 02:25:25.200]   Yeah, that doesn't ring right.
[02:25:25.200 --> 02:25:27.200]   And the cross on the outside.
[02:25:27.200 --> 02:25:29.400]   It's a miter too far.
[02:25:29.400 --> 02:25:33.480]   I like the tweet from Jake Flores.
[02:25:33.480 --> 02:25:39.280]   Hey, oh, blessed be.
[02:25:39.280 --> 02:25:40.280]   Some good news.
[02:25:40.280 --> 02:25:41.280]   I like to end the show with good news.
[02:25:41.280 --> 02:25:42.280]   Here's some good news stories.
[02:25:42.280 --> 02:25:43.280]   You feel good?
[02:25:43.280 --> 02:25:44.280]   Here we go.
[02:25:44.280 --> 02:25:45.760]   The feel good section of the show.
[02:25:45.760 --> 02:25:52.680]   FTC has decided it wants to ban those tough to cancel gym and cable subscriptions.
[02:25:52.680 --> 02:25:57.680]   They proposed click to cancel rule would require companies make it as easy to cancel
[02:25:57.680 --> 02:26:01.520]   a membership as it was to sign up.
[02:26:01.520 --> 02:26:04.720]   They're putting out a request for comments.
[02:26:04.720 --> 02:26:06.080]   They haven't made the rule yet.
[02:26:06.080 --> 02:26:10.160]   I can't imagine anybody not agreeing with it except the cable companies and the phone
[02:26:10.160 --> 02:26:13.520]   companies and all the other companies that don't want you to cancel.
[02:26:13.520 --> 02:26:14.520]   You have to call them.
[02:26:14.520 --> 02:26:15.520]   You have to call them.
[02:26:15.520 --> 02:26:18.160]   I guess you can sign up for three seconds on the friend newspaper.
[02:26:18.160 --> 02:26:20.240]   We have to call them up to cancel it.
[02:26:20.240 --> 02:26:25.040]   One of the people that the recording that the tattoo and I made waves was gone.
[02:26:25.040 --> 02:26:26.040]   Yeah.
[02:26:26.040 --> 02:26:27.040]   It's gone cast.
[02:26:27.040 --> 02:26:28.040]   Yeah.
[02:26:28.040 --> 02:26:29.040]   subscription 20 minutes.
[02:26:29.040 --> 02:26:30.200]   Oh, it's off.
[02:26:30.200 --> 02:26:31.760]   So here's some of the things.
[02:26:31.760 --> 02:26:36.240]   First of all, it'll be the business has to offer the same way to cancel services they
[02:26:36.240 --> 02:26:41.080]   did to sign up if it was a single button, you got to have a single button cancel.
[02:26:41.080 --> 02:26:46.680]   If the company is going to offer you special deals or perks, they're allowed to do that,
[02:26:46.680 --> 02:26:52.280]   but they must offer an upfront opt out that list customers bypass the sales pitches.
[02:26:52.280 --> 02:26:53.280]   Love that.
[02:26:53.280 --> 02:26:55.320]   That upsell is nuts.
[02:26:55.320 --> 02:27:00.800]   They also have to annually remind customers they're enrolled in an auto renewal option.
[02:27:00.800 --> 02:27:01.800]   Right?
[02:27:01.800 --> 02:27:02.800]   Mm hmm.
[02:27:02.800 --> 02:27:06.560]   We have to, you know, one of the things Apple does right in its app store, they list all
[02:27:06.560 --> 02:27:07.560]   your subscriptions.
[02:27:07.560 --> 02:27:08.920]   You can easily cancel them.
[02:27:08.920 --> 02:27:12.520]   Well, you'll have to be notified before those renew.
[02:27:12.520 --> 02:27:15.360]   How many times have you gotten a renewal and went, I still have that.
[02:27:15.360 --> 02:27:16.360]   I still subscribe.
[02:27:16.360 --> 02:27:17.720]   I had a guy come to my door yesterday.
[02:27:17.720 --> 02:27:18.720]   Like he got into my building.
[02:27:18.720 --> 02:27:20.000]   So I'm now came to my door, right?
[02:27:20.000 --> 02:27:21.600]   Like knocked on knocked on it.
[02:27:21.600 --> 02:27:23.800]   It was a salesperson for the local cable company.
[02:27:23.800 --> 02:27:25.520]   I said, I'm already a customer.
[02:27:25.520 --> 02:27:27.240]   He yelled at me for not listening to him.
[02:27:27.240 --> 02:27:31.560]   Oh, I was a terrible client ruined his day and stormed off.
[02:27:31.560 --> 02:27:35.160]   And then it's so I contacted them and I said, well, you can take home visit.
[02:27:35.160 --> 02:27:38.880]   You can fill out this form to prevent home visits, but you have to refill it out every
[02:27:38.880 --> 02:27:39.880]   12 months.
[02:27:39.880 --> 02:27:41.080]   Ridiculous, ridiculous.
[02:27:41.080 --> 02:27:42.080]   Goodness.
[02:27:42.080 --> 02:27:45.720]   So the FDC is asking for comment.
[02:27:45.720 --> 02:27:50.440]   Presumably the comments will be positive and they will make a rule.
[02:27:50.440 --> 02:27:52.120]   I hope I'm praying.
[02:27:52.120 --> 02:27:53.440]   I'm counting on that.
[02:27:53.440 --> 02:27:58.240]   Well, it seemed to be gearing up for action now, which is good because we need some regulation.
[02:27:58.240 --> 02:27:59.720]   Thank you, Veena Khan.
[02:27:59.720 --> 02:28:02.440]   Can you see this main license plate?
[02:28:02.440 --> 02:28:09.120]   L U V T O F U. I hear that laugh.
[02:28:09.120 --> 02:28:10.800]   I know what you're seeing.
[02:28:10.800 --> 02:28:13.000]   This guy loves tofu.
[02:28:13.000 --> 02:28:14.480]   Okay.
[02:28:14.480 --> 02:28:16.960]   His name is Peter Stereostekki.
[02:28:16.960 --> 02:28:18.760]   He is a vegan.
[02:28:18.760 --> 02:28:25.320]   He got the license approved, but got an email from the state of Maine saying, uh, no, no,
[02:28:25.320 --> 02:28:31.160]   we, uh, you cannot have loved tofu as your license plates.
[02:28:31.160 --> 02:28:32.160]   Love to follow up.
[02:28:32.160 --> 02:28:34.920]   Yeah, that's it.
[02:28:34.920 --> 02:28:38.720]   His appeal was rejected because the plates have to be looked at without context.
[02:28:38.720 --> 02:28:45.720]   The fact that there's a vegan driving does not enter into the whole thing.
[02:28:45.720 --> 02:28:49.600]   So just, you know, just remembered love to F U. I mean, love tofu.
[02:28:49.600 --> 02:28:52.120]   But isn't that equally like the context is equally either way.
[02:28:52.120 --> 02:28:57.120]   I agree.
[02:28:57.120 --> 02:28:59.120]   It says more about the other person.
[02:28:59.120 --> 02:29:00.120]   Yeah.
[02:29:00.120 --> 02:29:01.120]   It says more about the state.
[02:29:01.120 --> 02:29:02.120]   I love him.
[02:29:02.120 --> 02:29:04.120]   I mean, not going to allow the letter F in people's, you know, license plates.
[02:29:04.120 --> 02:29:05.120]   F no.
[02:29:05.120 --> 02:29:06.920]   I've seen much worse things than that.
[02:29:06.920 --> 02:29:07.920]   I agree.
[02:29:07.920 --> 02:29:08.920]   I agree.
[02:29:08.920 --> 02:29:14.800]   Like they allow to have people have like full banners telling, you know, presidents and
[02:29:14.800 --> 02:29:19.800]   other public officials to like die or to, you know, often.
[02:29:19.800 --> 02:29:22.120]   What a shoot license plate.
[02:29:22.120 --> 02:29:23.120]   Right.
[02:29:23.120 --> 02:29:24.120]   What if it was H eight tofu?
[02:29:24.120 --> 02:29:25.120]   Would that be okay?
[02:29:25.120 --> 02:29:30.000]   H A T O F U. Yeah.
[02:29:30.000 --> 02:29:31.080]   You know, he should apply for that.
[02:29:31.080 --> 02:29:32.440]   I'll apply for that one.
[02:29:32.440 --> 02:29:33.440]   Yeah.
[02:29:33.440 --> 02:29:34.440]   Yeah.
[02:29:34.440 --> 02:29:39.080]   And good news for Mr. and Mrs. Pickles.
[02:29:39.080 --> 02:29:43.920]   These are 90 year old tortoises at the Houston Zoo.
[02:29:43.920 --> 02:29:46.440]   They are an endangered species.
[02:29:46.440 --> 02:29:50.040]   They've been together since 1996.
[02:29:50.040 --> 02:29:57.920]   And now they are proud parents of three new baby tortoises, Dil, Gherkin and Jalapeno.
[02:29:57.920 --> 02:30:02.320]   Maybe this was inspired by Rupert Murdoch.
[02:30:02.320 --> 02:30:04.440]   This is from the New York Times, my friends.
[02:30:04.440 --> 02:30:05.880]   It was an astounding feat.
[02:30:05.880 --> 02:30:11.960]   Zoo officials say not only because Mr. Pickles is 90 years old, but also because the critically
[02:30:11.960 --> 02:30:16.400]   endangered species rarely produces offspring might be related.
[02:30:16.400 --> 02:30:19.360]   To the fact in Berlin, they're in danger.
[02:30:19.360 --> 02:30:24.520]   Mr. Pickles has been at the zoo since for 36 years, partnered with Mrs. Pickles.
[02:30:24.520 --> 02:30:25.520]   She's young.
[02:30:25.520 --> 02:30:26.520]   She's young.
[02:30:26.520 --> 02:30:34.080]   She's only 53 since her arrival in 96 radiated tortoises can live for up to 150 years.
[02:30:34.080 --> 02:30:38.480]   So like Rupert Murdoch, he's really only in the first half of his life, but it's
[02:30:38.480 --> 02:30:39.480]   unknown.
[02:30:39.480 --> 02:30:40.480]   The show came full circle.
[02:30:40.480 --> 02:30:44.680]   It's unknown how long they can reproduce.
[02:30:44.680 --> 02:30:47.360]   We know 92 is not out.
[02:30:47.360 --> 02:30:48.560]   They're looking at you.
[02:30:48.560 --> 02:30:49.560]   They are.
[02:30:49.560 --> 02:30:50.560]   They're cute.
[02:30:50.560 --> 02:30:56.920]   Having seen on YouTube the videos of Galapagos Turtles mating, it scars you for life.
[02:30:56.920 --> 02:30:57.920]   Oh, I've seen it.
[02:30:57.920 --> 02:30:58.920]   It's like a person never hurts.
[02:30:58.920 --> 02:30:59.920]   Oh, yeah.
[02:30:59.920 --> 02:31:00.920]   That before.
[02:31:00.920 --> 02:31:01.920]   Yeah.
[02:31:01.920 --> 02:31:05.000]   No, those Galapagos turtles are, but they're big.
[02:31:05.000 --> 02:31:06.000]   Yeah.
[02:31:06.000 --> 02:31:08.080]   They're creatures of legend.
[02:31:08.080 --> 02:31:10.160]   I was the only fonts of turtles.
[02:31:10.160 --> 02:31:12.680]   I was, I should see if I can find this picture.
[02:31:12.680 --> 02:31:17.640]   I was taking a picture of one on the Galapagos with my long lens when he started approached
[02:31:17.640 --> 02:31:20.200]   me and then the guys who told us you can't touch him.
[02:31:20.200 --> 02:31:21.200]   Don't approach him.
[02:31:21.200 --> 02:31:22.200]   Stay away from him.
[02:31:22.200 --> 02:31:24.040]   I said, he's coming towards me.
[02:31:24.040 --> 02:31:25.040]   They said, don't move.
[02:31:25.040 --> 02:31:26.040]   I said, he's getting closer.
[02:31:26.040 --> 02:31:27.040]   They said, don't move.
[02:31:27.040 --> 02:31:28.040]   He's so slowly.
[02:31:28.040 --> 02:31:29.040]   I can't move.
[02:31:29.040 --> 02:31:30.040]   You think that, but it's got to charge.
[02:31:30.040 --> 02:31:31.040]   It's got to charge a stack.
[02:31:31.040 --> 02:31:39.120]   Do you see those little shorts of like the hippo attacks?
[02:31:39.120 --> 02:31:42.520]   Well, you see a hippo and then suddenly it's like 800 feet tall and coming out of the water
[02:31:42.520 --> 02:31:45.360]   and almost go on the water.
[02:31:45.360 --> 02:31:46.360]   They're terrifying.
[02:31:46.360 --> 02:31:47.360]   The hippos aren't.
[02:31:47.360 --> 02:31:48.360]   The hippos aren't.
[02:31:48.360 --> 02:31:51.680]   It's so funny because I went to Galapagos and I saw the Galapagos turtles and we're not
[02:31:51.680 --> 02:31:53.040]   allowed to touch them in there and dink.
[02:31:53.040 --> 02:31:54.040]   They're very careful.
[02:31:54.040 --> 02:31:55.040]   They're very careful.
[02:31:55.040 --> 02:31:56.920]   But in Florida, you can breathe them.
[02:31:56.920 --> 02:32:01.320]   You can actually buy one, take it home, read it, sell it to other people.
[02:32:01.320 --> 02:32:02.680]   I did the Galapagos.
[02:32:02.680 --> 02:32:05.880]   I did the, we paid for a special tour of the turtles.
[02:32:05.880 --> 02:32:06.880]   Ladies and gentlemen, Florida.
[02:32:06.880 --> 02:32:09.200]   I had the turtles, hit the turtles.
[02:32:09.200 --> 02:32:13.120]   It tried, it followed me around because I had a very colorful skirt which they, so
[02:32:13.120 --> 02:32:18.120]   it very slowly followed me around and then they keep the male and female turtles separate
[02:32:18.120 --> 02:32:23.000]   because they sometimes fight and it was adorable because these two male turtles that have
[02:32:23.000 --> 02:32:29.160]   been together for like 35 years and have animosity towards each other would be aggressive towards
[02:32:29.160 --> 02:32:30.160]   each other.
[02:32:30.160 --> 02:32:33.720]   So one would bite but it was so slow and it was going to bite together and the other would
[02:32:33.720 --> 02:32:37.840]   just move out of this way and then the other would be angry and it was so slow.
[02:32:37.840 --> 02:32:39.440]   It was hilarious.
[02:32:39.440 --> 02:32:41.440]   It was just, they also, they live on guava.
[02:32:41.440 --> 02:32:43.560]   As you probably noticed this and they're very messy eaters.
[02:32:43.560 --> 02:32:46.640]   So they always have a lot of guava on their face.
[02:32:46.640 --> 02:32:49.880]   I've had them, there were these reeds that they also fed them.
[02:32:49.880 --> 02:32:55.160]   And so I got to feed them these reeds really, really slowly.
[02:32:55.160 --> 02:32:57.360]   But it was, it was kind of hilarious.
[02:32:57.360 --> 02:32:58.360]   This is the little baby.
[02:32:58.360 --> 02:33:04.080]   This is Mr. Pickle's little baby and apparently, quite at the times, the births were even more
[02:33:04.080 --> 02:33:09.760]   improbable because the hatchlings likely would not have survived if a zookeeper hadn't noticed
[02:33:09.760 --> 02:33:12.960]   Mrs. Pickle's laying her eggs.
[02:33:12.960 --> 02:33:18.960]   The soil in Houston isn't conducive to keeping the burrowed eggs at Madagascar native turtles
[02:33:18.960 --> 02:33:22.240]   lay at the tortoises, sorry, lay at the right temperature and humidity.
[02:33:22.240 --> 02:33:28.800]   So the keepers had to take them and move them to the reptile and amphibian home.
[02:33:28.800 --> 02:33:34.720]   The neutrillo will remain behind the scenes until they're big enough to join their parents.
[02:33:34.720 --> 02:33:38.600]   Congratulations to Mr. Parks Scientist and Mrs. Pickles.
[02:33:38.600 --> 02:33:42.680]   There are Mr. and Mrs. Pickles eating some celebratory lettuce.
[02:33:42.680 --> 02:33:45.800]   Oh, yeah, they worked hard.
[02:33:45.800 --> 02:33:46.800]   They worked hard.
[02:33:46.800 --> 02:33:47.800]   That's for them.
[02:33:47.800 --> 02:33:50.560]   Those are some lettuce.
[02:33:50.560 --> 02:33:53.280]   Tortoises are pretty amazing, I have to say.
[02:33:53.280 --> 02:33:55.680]   But not as amazing as you three.
[02:33:55.680 --> 02:33:56.680]   What a great show.
[02:33:56.680 --> 02:33:58.200]   Thank you so much.
[02:33:58.200 --> 02:34:01.600]   Georgia Dow, I've got your videos.
[02:34:01.600 --> 02:34:04.280]   Anxiety-videos.com.
[02:34:04.280 --> 02:34:05.520]   She of course does great.
[02:34:05.520 --> 02:34:08.120]   I think her YouTube videos are hysterical.
[02:34:08.120 --> 02:34:09.120]   Thank you.
[02:34:09.120 --> 02:34:10.120]   I know.
[02:34:10.120 --> 02:34:11.120]   Thank you.
[02:34:11.120 --> 02:34:12.360]   They're not supposed to be comedic probably.
[02:34:12.360 --> 02:34:13.360]   They're fun.
[02:34:13.360 --> 02:34:14.760]   They're supposed to be fun.
[02:34:14.760 --> 02:34:18.360]   I actually thought when you said Mr. and Mrs. Pickles, I thought you were talking about
[02:34:18.360 --> 02:34:19.920]   the post in Boots movie.
[02:34:19.920 --> 02:34:20.920]   Oh.
[02:34:20.920 --> 02:34:21.920]   I just covered that.
[02:34:21.920 --> 02:34:25.360]   And so I was thinking that that's where it was from and unlike that seems strange.
[02:34:25.360 --> 02:34:27.000]   I'm like, why are we talking?
[02:34:27.000 --> 02:34:30.720]   Okay, we could be covering that.
[02:34:30.720 --> 02:34:31.720]   I want to watch.
[02:34:31.720 --> 02:34:35.840]   I got to watch your therapist reacts to Last of Us because I loved Last of Us.
[02:34:35.840 --> 02:34:36.840]   Last of Us was so good.
[02:34:36.840 --> 02:34:39.360]   Yeah, I want to see what you have to say about that.
[02:34:39.360 --> 02:34:40.360]   It was quite good.
[02:34:40.360 --> 02:34:41.360]   It was quite good.
[02:34:41.360 --> 02:34:42.360]   Yeah.
[02:34:42.360 --> 02:34:45.800]   So I'm not wearing any ears in that, but one of them is, I dress up a little.
[02:34:45.800 --> 02:34:46.800]   So one of them.
[02:34:46.800 --> 02:34:48.800]   You dress up as Wednesday Adams, which is-
[02:34:48.800 --> 02:34:49.800]   I do.
[02:34:49.800 --> 02:34:50.800]   I have two Wednesday Adams.
[02:34:50.800 --> 02:34:51.800]   Really good.
[02:34:51.800 --> 02:34:54.240]   You are an excellent Wednesday Adams.
[02:34:54.240 --> 02:34:55.240]   Thank you.
[02:34:55.240 --> 02:34:56.240]   Thank you.
[02:34:56.240 --> 02:34:57.240]   I do try.
[02:34:57.240 --> 02:34:59.680]   You might feel Scott cookies out of proper Girl Scouts.
[02:34:59.680 --> 02:35:01.840]   I did a little actually in it.
[02:35:01.840 --> 02:35:02.840]   It's horrible.
[02:35:02.840 --> 02:35:03.840]   Oh.
[02:35:03.840 --> 02:35:04.840]   It's horrible.
[02:35:04.840 --> 02:35:05.840]   It's terrible.
[02:35:05.840 --> 02:35:13.160]   I made the ears for all the characters for like, see, I glued together like the little
[02:35:13.160 --> 02:35:17.400]   tiny ears for like, if you saw the post in Boots movie, you'll understand.
[02:35:17.400 --> 02:35:18.400]   It's pretty tough.
[02:35:18.400 --> 02:35:19.920]   But like, you know, like it's just-
[02:35:19.920 --> 02:35:21.260]   It's so cute.
[02:35:21.260 --> 02:35:23.200]   What you do is so cute.
[02:35:23.200 --> 02:35:24.880]   There she is in her ears.
[02:35:24.880 --> 02:35:26.880]   Oh, there's I am with my ears.
[02:35:26.880 --> 02:35:27.880]   Oh.
[02:35:27.880 --> 02:35:28.880]   And Petito.
[02:35:28.880 --> 02:35:29.880]   Petito.
[02:35:29.880 --> 02:35:30.880]   It's so cute.
[02:35:30.880 --> 02:35:31.880]   It's a good movie.
[02:35:31.880 --> 02:35:36.400]   You will both laugh, love, and learn with her videos.
[02:35:36.400 --> 02:35:38.160]   They're so good.
[02:35:38.160 --> 02:35:42.320]   I wish I had been watching your videos as I watched Last of Us because you've gone through
[02:35:42.320 --> 02:35:43.320]   all the episodes.
[02:35:43.320 --> 02:35:44.320]   So I have to go back.
[02:35:44.320 --> 02:35:45.320]   I did go through all the episodes.
[02:35:45.320 --> 02:35:46.320]   Yeah.
[02:35:46.320 --> 02:35:47.320]   It was so good.
[02:35:47.320 --> 02:35:48.320]   I really enjoyed that.
[02:35:48.320 --> 02:35:49.320]   I agree.
[02:35:49.320 --> 02:35:50.320]   Episode three, it couldn't stop crying.
[02:35:50.320 --> 02:35:51.320]   Couldn't stop crying.
[02:35:51.320 --> 02:35:52.320]   Nope.
[02:35:52.320 --> 02:35:55.840]   I think that they should have had another two or three episodes.
[02:35:55.840 --> 02:35:59.760]   I think that it was short in comparison to how much they had in the game.
[02:35:59.760 --> 02:36:00.760]   Yeah.
[02:36:00.760 --> 02:36:01.760]   Yeah.
[02:36:01.760 --> 02:36:02.760]   That cragmays and could write.
[02:36:02.760 --> 02:36:03.760]   Who would have thunked it?
[02:36:03.760 --> 02:36:05.080]   Well, that's what's interesting, right?
[02:36:05.080 --> 02:36:09.400]   The themes of the game ended up being excellent themes for a television series.
[02:36:09.400 --> 02:36:10.600]   I thought that was fascinating.
[02:36:10.600 --> 02:36:13.480]   One thing I heard that was really interesting was that one editor, like the main editor
[02:36:13.480 --> 02:36:14.480]   had never seen it.
[02:36:14.480 --> 02:36:16.600]   So he cut the whole thing without never playing the video game.
[02:36:16.600 --> 02:36:17.600]   Oh.
[02:36:17.600 --> 02:36:18.600]   Just trying to figure out it.
[02:36:18.600 --> 02:36:22.000]   But the co-editor had like, was an ultra fan and would go through it and look at it
[02:36:22.000 --> 02:36:26.160]   after to make sure he didn't forget anything or like something wasn't done better in the
[02:36:26.160 --> 02:36:27.160]   game.
[02:36:27.160 --> 02:36:29.520]   But they said it ended up being cut very somewhat of the game just because that was
[02:36:29.520 --> 02:36:31.520]   the best way to tell the story.
[02:36:31.520 --> 02:36:32.520]   Interesting.
[02:36:32.520 --> 02:36:33.520]   Wow.
[02:36:33.520 --> 02:36:38.680]   Well, George Adao is at youtube.com at George Adao.
[02:36:38.680 --> 02:36:42.880]   Renee Richie is at youtube.com/at Renee Richie.
[02:36:42.880 --> 02:36:45.120]   And of course, he's the creator liaison over there.
[02:36:45.120 --> 02:36:49.160]   So if you're a YouTube creator, you will be talking to Mr. Richie.
[02:36:49.160 --> 02:36:50.440]   Thank you so much for being here.
[02:36:50.440 --> 02:36:51.440]   I really appreciate it.
[02:36:51.440 --> 02:36:52.440]   Thank you.
[02:36:52.440 --> 02:36:56.360]   Anything you want to plug?
[02:36:56.360 --> 02:37:01.440]   If you go to the, well, the creator, YouTube.com/at youtube liaison, I recently got to interview
[02:37:01.440 --> 02:37:06.720]   Mr. Beast, Jimmy Donaldson, about multi-language audio on YouTube where you can now upload.
[02:37:06.720 --> 02:37:10.200]   If you dub tracks, you don't have to have a separate Spanish channel or a separate French
[02:37:10.200 --> 02:37:15.680]   channel or Portuguese channel, you can upload multiple audio tracks to the same channel.
[02:37:15.680 --> 02:37:18.400]   And then it will automatically pick the one that goes with the language preference of
[02:37:18.400 --> 02:37:19.400]   the viewer.
[02:37:19.400 --> 02:37:20.400]   That's super cool.
[02:37:20.400 --> 02:37:22.480]   Portuguese and Spanish.
[02:37:22.480 --> 02:37:23.920]   That's really, really cool.
[02:37:23.920 --> 02:37:25.920]   Good for him.
[02:37:25.920 --> 02:37:31.480]   And of course, Ian Thompson, he is US editor at the register.com.
[02:37:31.480 --> 02:37:35.160]   Always a pleasure, Ian, when you used to come up, you used to go to the English store and
[02:37:35.160 --> 02:37:37.320]   pick up some last minute Marmite.
[02:37:37.320 --> 02:37:45.080]   Ah, finally, Marmite, we, my fellow, I went to a British bloke's leaving do last night.
[02:37:45.080 --> 02:37:47.760]   Dave Lee is going up to Bloomberg in New York.
[02:37:47.760 --> 02:37:51.200]   And we have a competition who can find the most expensive Marmite.
[02:37:51.200 --> 02:37:54.280]   And my goodness, California really comes like coming through.
[02:37:54.280 --> 02:37:56.840]   We have the most expensive Marmite?
[02:37:56.840 --> 02:37:57.840]   Oh, yeah.
[02:37:57.840 --> 02:38:02.240]   I was charged $8 for a tiny little thing.
[02:38:02.240 --> 02:38:06.200]   Whereas, you know, with Amazon, I can actually buy the catering size packs.
[02:38:06.200 --> 02:38:07.200]   So it's so...
[02:38:07.200 --> 02:38:10.360]   Oh my God, how much Marmite do you eat?
[02:38:10.360 --> 02:38:14.560]   I go through about a kilogram a year.
[02:38:14.560 --> 02:38:20.880]   A kilogram a year of basically the byproduct of beer making.
[02:38:20.880 --> 02:38:23.640]   Yum, yum, yum.
[02:38:23.640 --> 02:38:28.000]   Oh, look, Sanitarium brand Marmite from New Zealand.
[02:38:28.000 --> 02:38:39.840]   Oh, here's the 600 gram tops, Pekka 2 for 48 bucks.
[02:38:39.840 --> 02:38:42.040]   That's a lot of Marmite.
[02:38:42.040 --> 02:38:47.480]   But you know, it's nice and a spreadable tub is a good idea, I think.
[02:38:47.480 --> 02:38:52.480]   Well, honestly, the lids on those are really bad because one of them fell off on the shelf
[02:38:52.480 --> 02:38:55.480]   and went on its side and I didn't notice it for a day or two.
[02:38:55.480 --> 02:38:57.680]   And it looked like Venom 3, the remake.
[02:38:57.680 --> 02:39:02.120]   You know, that's great.
[02:39:02.120 --> 02:39:03.880]   Let's eat that.
[02:39:03.880 --> 02:39:09.840]   Well, I may have mind did say I bet you cleaned that up with toast, but you know...
[02:39:09.840 --> 02:39:11.880]   That's the easiest way to do it.
[02:39:11.880 --> 02:39:12.880]   Absolutely.
[02:39:12.880 --> 02:39:14.440]   Thank you, Ian.
[02:39:14.440 --> 02:39:15.440]   Thank you, Georgia.
[02:39:15.440 --> 02:39:16.440]   Thank you, Renee.
[02:39:16.440 --> 02:39:17.440]   Always a pleasure.
[02:39:17.440 --> 02:39:20.080]   Thanks also to Corey, Dr. O. joined us earlier.
[02:39:20.080 --> 02:39:24.160]   And thanks to all of you who put up with us every Sunday for this week in tech.
[02:39:24.160 --> 02:39:27.840]   We do the show Sunday afternoons right after ask the tech guys.
[02:39:27.840 --> 02:39:33.000]   I throw Micah bodily out of the studio and then we do this week in tech about 2 PM Pacific,
[02:39:33.000 --> 02:39:39.240]   5 PM Eastern, 2100 UTC.
[02:39:39.240 --> 02:39:42.640]   You can watch us do it live at live.twit.tv.
[02:39:42.640 --> 02:39:46.160]   If you're watching live, you can chat with us live at IRC.twit.tv.
[02:39:46.160 --> 02:39:50.280]   Of course, club twit members have the Discord where they can chat, which is awesome.
[02:39:50.280 --> 02:39:52.040]   We love having you in there.
[02:39:52.040 --> 02:39:54.760]   After the fact, the website has shows.
[02:39:54.760 --> 02:39:57.840]   You can download them there or go to YouTube.
[02:39:57.840 --> 02:39:59.320]   We have a YouTube channel.
[02:39:59.320 --> 02:40:04.880]   Actually, probably the best thing is to go to the main twit channel, youtube.com/attwit.
[02:40:04.880 --> 02:40:08.120]   And then you can follow links to all the shows.
[02:40:08.120 --> 02:40:12.440]   Each show has its own channel, including this week in tech.
[02:40:12.440 --> 02:40:18.000]   And maybe the easiest thing to do is just find a good podcast program and subscribe.
[02:40:18.000 --> 02:40:23.280]   That way you'll get it the minute it's available of a Sunday evening.
[02:40:23.280 --> 02:40:26.040]   Reminder, I will be taking the next few weeks off.
[02:40:26.040 --> 02:40:30.240]   Micah Sargent will be taking over next Sunday to vendor Hardwar following that.
[02:40:30.240 --> 02:40:36.840]   And on the 16th, Jason Howell will be back on the 23rd.
[02:40:36.840 --> 02:40:39.120]   Thanks to all the people who behind the scenes who make.
[02:40:39.120 --> 02:40:40.280]   I never thank you guys.
[02:40:40.280 --> 02:40:43.480]   I should really thank you, Benito, who runs the board.
[02:40:43.480 --> 02:40:47.120]   John and Burke, who run the studio.
[02:40:47.120 --> 02:40:49.840]   Is Kevin King, our editor usually?
[02:40:49.840 --> 02:40:51.720]   Pardon me?
[02:40:51.720 --> 02:40:52.720]   Not today.
[02:40:52.720 --> 02:40:53.720]   Who will be editing the show?
[02:40:53.720 --> 02:40:54.720]   Well, Benito.
[02:40:54.720 --> 02:40:55.720]   Anthony, Anthony Nielsen.
[02:40:55.720 --> 02:40:58.200]   Of course, Anthony is kind of our creative director.
[02:40:58.200 --> 02:41:00.280]   There's a lot of great things.
[02:41:00.280 --> 02:41:01.280]   You know what?
[02:41:01.280 --> 02:41:03.480]   There is a crawl at the end of the show that shows all the people.
[02:41:03.480 --> 02:41:06.600]   It is not an easy thing to put this together every week.
[02:41:06.600 --> 02:41:09.720]   And I am eternally grateful to all the people who help and do it.
[02:41:09.720 --> 02:41:14.000]   And of course, Aunt Pruitt, filling in for Jason Howell, who's in Costa Rica this week
[02:41:14.000 --> 02:41:16.480]   and did the production on this show.
[02:41:16.480 --> 02:41:18.240]   So thank you, Aunt Pruitt.
[02:41:18.240 --> 02:41:19.240]   Thanks to all of you.
[02:41:19.240 --> 02:41:21.160]   We'll see you next time.
[02:41:21.160 --> 02:41:24.040]   But meanwhile, another twit is in the can.
[02:41:24.040 --> 02:41:25.040]   Bye-bye.
[02:41:25.040 --> 02:41:26.040]   This is amazing.
[02:41:26.040 --> 02:41:33.040]   Do the twit.
[02:41:33.040 --> 02:41:34.880]   ♪ Doin' the twit, baby ♪
[02:41:34.880 --> 02:41:35.880]   ♪ Doin' the twit ♪
[02:41:35.880 --> 02:41:36.880]   ♪ All right ♪

