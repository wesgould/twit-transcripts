;FFMETADATA1
title=Soused Women and $4 Potatoes
artist=Leo Laporte, Owen Thomas, Daniel Rubino, Glenn Fleishman
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-05-28
track=929
language=English
genre=Podcast
comment=<p>AI Spoof, Twitter and DeSantis</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:02.080]   It's time for Twit this week at Tech.
[00:00:02.080 --> 00:00:03.440]   Oh, we have a great panel for you.
[00:00:03.440 --> 00:00:06.600]   Owen Thomas is here from the San Francisco Examiner.
[00:00:06.600 --> 00:00:09.480]   Glenn Fleischman from The Incompable,
[00:00:09.480 --> 00:00:11.960]   and from Windows Central, the editor-in-chief.
[00:00:11.960 --> 00:00:14.960]   Daniel Rabino will talk about Microsoft's build,
[00:00:14.960 --> 00:00:17.560]   some big AI announcements there.
[00:00:17.560 --> 00:00:21.400]   Well, we'll talk about Elon Musk's disaster,
[00:00:21.400 --> 00:00:26.200]   and what happens when your lawyer uses chat GPT?
[00:00:26.200 --> 00:00:28.160]   It's all coming up next on Twit.
[00:00:28.160 --> 00:00:30.520]   [MUSIC PLAYING]
[00:00:30.520 --> 00:00:32.360]   Podcasts you love.
[00:00:32.360 --> 00:00:34.880]   From people you trust.
[00:00:34.880 --> 00:00:36.160]   This is Twit.
[00:00:36.160 --> 00:00:46.960]   This is Twit this week in Tech.
[00:00:46.960 --> 00:00:53.240]   Episode 929 recorded Sunday, May 28, 2023.
[00:00:53.240 --> 00:00:57.440]   Soused women and $4 potatoes.
[00:00:57.440 --> 00:01:00.560]   This week at Tech is brought to you by ACI Learning.
[00:01:00.560 --> 00:01:03.720]   IT skills are outdated in about 18 months.
[00:01:03.720 --> 00:01:06.000]   Stay ahead of the curve and future proof
[00:01:06.000 --> 00:01:09.840]   your business competitiveness with customizable entertaining
[00:01:09.840 --> 00:01:10.840]   training.
[00:01:10.840 --> 00:01:14.840]   Fill out the form at go.acilurning.com/twit
[00:01:14.840 --> 00:01:17.160]   for more information on a free two-week training
[00:01:17.160 --> 00:01:19.440]   trial for your team.
[00:01:19.440 --> 00:01:23.680]   And buy worldwide technology with an innovative culture,
[00:01:23.680 --> 00:01:27.120]   thousands of IT engineers, application developers,
[00:01:27.120 --> 00:01:29.320]   unmatched labs, and integration centers
[00:01:29.320 --> 00:01:32.920]   for testing and deploying technology at scale.
[00:01:32.920 --> 00:01:36.480]   WWT helps customers bridge the gap between strategy
[00:01:36.480 --> 00:01:37.760]   and execution.
[00:01:37.760 --> 00:01:43.040]   To learn more about WWT, visit www.com/twit.
[00:01:43.040 --> 00:01:44.760]   And buy Lookout.
[00:01:44.760 --> 00:01:47.200]   Whether on a device or in the cloud,
[00:01:47.200 --> 00:01:50.680]   your business data is always on the move, minimize risk,
[00:01:50.680 --> 00:01:53.400]   increase visibility, and ensure compliance
[00:01:53.400 --> 00:01:55.560]   with Lookout's unified platform.
[00:01:55.560 --> 00:01:57.840]   Visit Lookout.com today.
[00:01:57.840 --> 00:02:05.240]   It's time for Twit this week at Tech,
[00:02:05.240 --> 00:02:07.600]   the show we covers the weeks news.
[00:02:07.600 --> 00:02:11.520]   And we covers it today with three excellent panelists.
[00:02:11.520 --> 00:02:15.960]   Glenn Fleischman joins us from parts northeast.
[00:02:15.960 --> 00:02:19.200]   He's the editor of a brand new book, Kickstarter Northwest,
[00:02:19.200 --> 00:02:21.160]   what did I say, Northeast Crazy.
[00:02:21.160 --> 00:02:22.880]   I was thinking of you and Maine.
[00:02:22.880 --> 00:02:25.120]   You're going to Maine to write your book.
[00:02:25.120 --> 00:02:25.960]   You're north of, yeah.
[00:02:25.960 --> 00:02:26.880]   Hello, hello.
[00:02:26.880 --> 00:02:28.400]   Hello, Glenn Fleischman.
[00:02:28.400 --> 00:02:31.000]   Editor of Shift happens.
[00:02:31.000 --> 00:02:33.320]   Good to see you again.
[00:02:33.320 --> 00:02:38.160]   Also joining us from Windows Central Magazine,
[00:02:38.160 --> 00:02:39.800]   the editor in chief.
[00:02:39.800 --> 00:02:42.600]   Do you still call it a magazine, Daniel?
[00:02:42.600 --> 00:02:43.800]   No, no, no.
[00:02:43.800 --> 00:02:44.640]   Blog.
[00:02:44.640 --> 00:02:45.480]   Just Windows Central.
[00:02:45.480 --> 00:02:46.840]   Just a thing.
[00:02:46.840 --> 00:02:47.760]   It's a thing.
[00:02:47.760 --> 00:02:48.680]   Yeah, tell me why.
[00:02:48.680 --> 00:02:49.600]   You do joke around people.
[00:02:49.600 --> 00:02:51.240]   I'm like, yeah, I just, I blog.
[00:02:51.240 --> 00:02:52.080]   That's all I did.
[00:02:52.080 --> 00:02:53.080]   Blog.
[00:02:53.080 --> 00:02:54.440]   Are you in your mother's basement?
[00:02:54.440 --> 00:02:56.160]   Daniel, where are you?
[00:02:56.160 --> 00:02:56.680]   Exactly.
[00:02:56.680 --> 00:02:58.960]   What are you doing there?
[00:02:58.960 --> 00:02:59.960]   Good to have you here.
[00:02:59.960 --> 00:03:01.800]   Of course, Microsoft Build was this week.
[00:03:01.800 --> 00:03:03.080]   Lots of AI news.
[00:03:03.080 --> 00:03:04.960]   We'll get to that in a bit.
[00:03:04.960 --> 00:03:07.400]   But before we do, let's introduce panelists number three,
[00:03:07.400 --> 00:03:11.120]   my good friend Owen Thomas, columnist from San Francisco
[00:03:11.120 --> 00:03:12.920]   Examiner.
[00:03:12.920 --> 00:03:15.360]   Alex Wilhelm's Twitter daddy.
[00:03:15.360 --> 00:03:16.560]   Do you still say that now?
[00:03:16.560 --> 00:03:17.400]   Is Twitter over?
[00:03:17.400 --> 00:03:18.840]   It's done.
[00:03:18.840 --> 00:03:19.880]   Oh, you're muted.
[00:03:19.880 --> 00:03:22.840]   I think you need to say Twitter dad, not Twitter daddy.
[00:03:22.840 --> 00:03:23.520]   That's different.
[00:03:23.520 --> 00:03:24.520]   Two different things.
[00:03:24.520 --> 00:03:27.040]   That's more only fans.
[00:03:27.040 --> 00:03:28.040]   Okay.
[00:03:28.040 --> 00:03:30.840]   He's Twitter, his Twitter furry.
[00:03:30.840 --> 00:03:31.840]   Is that right?
[00:03:31.840 --> 00:03:32.840]   No, that's wrong.
[00:03:32.840 --> 00:03:34.000]   That's very wrong.
[00:03:34.000 --> 00:03:35.960]   That's very wrong.
[00:03:35.960 --> 00:03:47.000]   This week, Twitter had a little attention on Wednesday as Elon constantly reminded us.
[00:03:47.000 --> 00:03:53.800]   Ron DeSantis, Governor of Florida, was to announce his 2024 candidacy for presidency.
[00:03:53.800 --> 00:04:00.520]   Didn't go exactly as planned.
[00:04:00.520 --> 00:04:09.680]   Somebody noted that Twitter had fired 97 out of the 100 people who worked at spaces, which
[00:04:09.680 --> 00:04:12.520]   might have had something to do with it.
[00:04:12.520 --> 00:04:23.720]   They also stopped paying for the service that helped keep spaces online.
[00:04:23.720 --> 00:04:26.720]   Was it DDoS protection kind of a service or?
[00:04:26.720 --> 00:04:32.160]   I think it was more like a CDN or something.
[00:04:32.160 --> 00:04:33.920]   They literally didn't have enough servers spun up.
[00:04:33.920 --> 00:04:38.360]   I think they're going to when you look up on Purik victory and dictionary, they're going
[00:04:38.360 --> 00:04:41.040]   to put Elon's face in the next edition.
[00:04:41.040 --> 00:04:44.360]   It's pretty amazing the cell phone there.
[00:04:44.360 --> 00:04:48.760]   His cost containment strategy at Twitter seems to be, "I'm going to randomly stop paying
[00:04:48.760 --> 00:04:52.240]   for bills and randomly shutting servers down and see what breaks."
[00:04:52.240 --> 00:04:54.520]   This is an example.
[00:04:54.520 --> 00:05:00.200]   That story that came out was just last week about reporting what people inside the company
[00:05:00.200 --> 00:05:04.800]   responsible for real estate and other bills were talking about what they were asked to
[00:05:04.800 --> 00:05:10.880]   do, which seemed a little wild.
[00:05:10.880 --> 00:05:15.760]   In the filings, they were saying, "Look, my career is in real estate.
[00:05:15.760 --> 00:05:23.680]   I will not be able to work again in this industry if you have me try to stiff our landlord."
[00:05:23.680 --> 00:05:25.960]   The answer was, "Well, we don't care.
[00:05:25.960 --> 00:05:27.600]   You don't want to ask me.
[00:05:27.600 --> 00:05:28.600]   I don't care."
[00:05:28.600 --> 00:05:31.800]   Or I might be sentenced to jail or the system.
[00:05:31.800 --> 00:05:37.840]   Yeah, I might be persecuted or prosecuted or persecuted.
[00:05:37.840 --> 00:05:45.360]   I think he maintains when he's talked about it in public, this is what rich people allegedly
[00:05:45.360 --> 00:05:50.200]   do is that they don't pay bills strategically to see which ones they have to pay.
[00:05:50.200 --> 00:05:54.920]   I don't think that's ethically the case for everybody, but certainly some other prominent
[00:05:54.920 --> 00:05:55.920]   people have done that.
[00:05:55.920 --> 00:05:59.480]   It's amazing to see it so blatantly and on such a huge scale.
[00:05:59.480 --> 00:06:05.200]   Did something happen when Trump got elected president that it just completely changed?
[00:06:05.200 --> 00:06:11.560]   It slid the ethical over-10 window over about 1,000 places because he's very famous also
[00:06:11.560 --> 00:06:14.200]   for not paying bills.
[00:06:14.200 --> 00:06:18.120]   It feels like he made a lot of things okay.
[00:06:18.120 --> 00:06:20.840]   Yeah, and there's no repercussions.
[00:06:20.840 --> 00:06:21.840]   You get away with it.
[00:06:21.840 --> 00:06:22.840]   I don't pay bills.
[00:06:22.840 --> 00:06:26.240]   If I don't pay bills, I'm going to get in trouble.
[00:06:26.240 --> 00:06:28.320]   We talk about bad things laughing.
[00:06:28.320 --> 00:06:31.480]   We talk about politics and stuff, but there's also a...
[00:06:31.480 --> 00:06:36.800]   The president is also in a way a leader of what people think.
[00:06:36.800 --> 00:06:41.120]   You can really establish a point of view that, yeah, it doesn't matter.
[00:06:41.120 --> 00:06:43.720]   You can now say anything you want.
[00:06:43.720 --> 00:06:46.880]   You can do whatever you want and it doesn't matter.
[00:06:46.880 --> 00:06:47.880]   You get away with it.
[00:06:47.880 --> 00:06:53.720]   I think Elon must have watched this with interest and is now adopting the art of the deal as
[00:06:53.720 --> 00:06:56.800]   it were.
[00:06:56.800 --> 00:06:59.360]   On the other hand, there has been some very good news for Elon.
[00:06:59.360 --> 00:07:00.360]   That was the bad news.
[00:07:00.360 --> 00:07:06.920]   Anyway, I got to point out the sub-tweeting that the New York Times did.
[00:07:06.920 --> 00:07:09.600]   This is an article by Ryan Mack and Tiffany Shoe.
[00:07:09.600 --> 00:07:15.400]   "Desantis's Twitter event falls short of the reach of past livestreams.
[00:07:15.400 --> 00:07:21.400]   Within hours of the event, participants celebrated the achievement, David Sachs, who's been a
[00:07:21.400 --> 00:07:25.760]   big donor to the Desantis campaign, declared it, quote, "By far, the biggest room ever
[00:07:25.760 --> 00:07:32.160]   where oil held on social media, Desantis himself said there were probably about 10 million
[00:07:32.160 --> 00:07:35.840]   people who would have watched the event."
[00:07:35.840 --> 00:07:42.560]   The Times says in a very kind of crisp single paragraph sentence, "They were wrong on both
[00:07:42.560 --> 00:07:43.560]   counts."
[00:07:43.560 --> 00:07:46.120]   They were wrong on both counts.
[00:07:46.120 --> 00:07:53.320]   According to Twitter's metrics, high of 300,000 concurrent listeners, that is actually...
[00:07:53.320 --> 00:07:59.960]   3.4 million afterwards, listen to recording, not quite 10 million, nor the biggest room
[00:07:59.960 --> 00:08:01.480]   ever held on social media.
[00:08:01.480 --> 00:08:08.160]   Facebook had a 800,000 person Facebook live event, which by the way featured two BuzzFeed
[00:08:08.160 --> 00:08:12.200]   employees placing rubber bands around a watermelon until it exploded.
[00:08:12.200 --> 00:08:14.120]   Oh, that was great.
[00:08:14.120 --> 00:08:16.600]   That was some entertainment.
[00:08:16.600 --> 00:08:20.760]   Five million people watched the entire event at some point.
[00:08:20.760 --> 00:08:24.800]   The 2017 Times is really being mean here.
[00:08:24.800 --> 00:08:30.480]   Live stream of a pregnant giraffe on YouTube brought in 5 million viewers a day.
[00:08:30.480 --> 00:08:35.720]   In fact, even Twitter spaces had done more.
[00:08:35.720 --> 00:08:40.240]   3 million people last month listened to an interview of Elon by a BBC reporter in a Twitter
[00:08:40.240 --> 00:08:42.480]   space.
[00:08:42.480 --> 00:08:47.840]   So yeah, this is severe degradation from what spaces had been.
[00:08:47.840 --> 00:08:53.720]   And certainly nothing compared to other AOC had to tweet.
[00:08:53.720 --> 00:09:01.000]   More people watched me play a game on Twitch and watched your announcement, which is true.
[00:09:01.000 --> 00:09:02.000]   Was she playing Among Us?
[00:09:02.000 --> 00:09:04.640]   I think she was.
[00:09:04.640 --> 00:09:08.080]   But that's pretty exciting as is watching a watermelon about to explode.
[00:09:08.080 --> 00:09:11.280]   So either way, I can understand the interest.
[00:09:11.280 --> 00:09:12.280]   Good news for you.
[00:09:12.280 --> 00:09:16.960]   Are you making some kind of DeSantis metaphor there?
[00:09:16.960 --> 00:09:18.440]   Intermelons explosion?
[00:09:18.440 --> 00:09:19.440]   You think?
[00:09:19.440 --> 00:09:21.440]   No, I wouldn't.
[00:09:21.440 --> 00:09:22.440]   Never.
[00:09:22.440 --> 00:09:23.880]   No, I'm not.
[00:09:23.880 --> 00:09:31.800]   That's any any resemblance to a person living or dead is accidental.
[00:09:31.800 --> 00:09:34.320]   I should say that the became every show.
[00:09:34.320 --> 00:09:35.280]   Good news for Elon though.
[00:09:35.280 --> 00:09:41.520]   The FDA has approved neural links study of human brain implants.
[00:09:41.520 --> 00:09:43.520]   Woo hoo.
[00:09:43.520 --> 00:09:46.440]   And when you want to get in line for this one.
[00:09:46.440 --> 00:09:48.440]   It's going to be a real life blue check.
[00:09:48.440 --> 00:09:49.440]   People.
[00:09:49.440 --> 00:09:53.360]   Because you know, those give me the first people signed up.
[00:09:53.360 --> 00:09:55.920]   Oh, yeah, it's going to be Elon stands, right?
[00:09:55.920 --> 00:09:56.920]   They'll be on Twitter.
[00:09:56.920 --> 00:09:57.920]   They'll have their little icon.
[00:09:57.920 --> 00:10:00.320]   I'll be a blue check and then a little brain next to it.
[00:10:00.320 --> 00:10:01.960]   You know, it's going to be awesome.
[00:10:01.960 --> 00:10:02.960]   Go.
[00:10:02.960 --> 00:10:03.960]   It's fine.
[00:10:03.960 --> 00:10:04.960]   I have a lot in my mind.
[00:10:04.960 --> 00:10:11.720]   I mean, to his credit, the whole idea is really to help people with brain injuries live
[00:10:11.720 --> 00:10:12.960]   more normal lives.
[00:10:12.960 --> 00:10:16.360]   So it's a I'm sure that's why the FDA cleared it.
[00:10:16.360 --> 00:10:22.960]   They cleared neural link to use its brain implant and surgical robot.
[00:10:22.960 --> 00:10:28.000]   Just what could go what could possibly go wrong for on patients, but declined to provide
[00:10:28.000 --> 00:10:31.480]   more details should point out.
[00:10:31.480 --> 00:10:37.000]   Neural link has been in trouble there in trouble with the United States Department of Agriculture
[00:10:37.000 --> 00:10:39.520]   for animal welfare violations.
[00:10:39.520 --> 00:10:42.960]   They're in trouble with the Department of Transportation.
[00:10:42.960 --> 00:10:43.960]   This is a weird one.
[00:10:43.960 --> 00:10:50.120]   They're probing whether neural link illegally transported dangerous pathogens on chips removed
[00:10:50.120 --> 00:10:54.480]   from monkey brains without proper containment measures.
[00:10:54.480 --> 00:10:59.200]   It's like a walking set of science fiction movies about to be made.
[00:10:59.200 --> 00:11:04.080]   It's like, I don't know why he doesn't have a secret layer in a volcano.
[00:11:04.080 --> 00:11:05.080]   It doesn't.
[00:11:05.080 --> 00:11:06.400]   It doesn't know that he doesn't.
[00:11:06.400 --> 00:11:07.400]   He could.
[00:11:07.400 --> 00:11:09.800]   No, if it's secret, that's true.
[00:11:09.800 --> 00:11:13.520]   The exciting news though, the neural link thing is brain based.
[00:11:13.520 --> 00:11:18.760]   I realized that it was incredibly excited about the fellow who had been paralyzed who
[00:11:18.760 --> 00:11:27.480]   was able to regain the ability to use his legs, a bridge technology from spine to spine bypassing
[00:11:27.480 --> 00:11:28.480]   the disconnection.
[00:11:28.480 --> 00:11:32.840]   I'm like, that is, whenever we talk about the future, I'm always like, well, we don't
[00:11:32.840 --> 00:11:34.160]   need flying cars necessarily.
[00:11:34.160 --> 00:11:38.280]   We don't need, I don't know, AI based recipe generators.
[00:11:38.280 --> 00:11:42.360]   But it's like people being able with aphasia, being able to speak, people being able to
[00:11:42.360 --> 00:11:45.000]   walk again, this stuff is fantastic.
[00:11:45.000 --> 00:11:47.120]   The question is, will anybody be able to afford it?
[00:11:47.120 --> 00:11:53.640]   Or will it be $10 million solutions that insurance companies can't reasonably cover for
[00:11:53.640 --> 00:11:55.520]   the people who need it?
[00:11:55.520 --> 00:11:57.560]   I think that's, so I'm excited.
[00:11:57.560 --> 00:12:01.640]   Neural link as a concept, I don't know what his end game is, but I'm excited if it leads
[00:12:01.640 --> 00:12:05.000]   to people being able to resume the lives that they had before.
[00:12:05.000 --> 00:12:06.320]   Yeah, absolutely.
[00:12:06.320 --> 00:12:10.160]   And I'm sure that's why the FDA was a little more open to the work.
[00:12:10.160 --> 00:12:14.960]   It's not just so you can go and Mark Zuckerberg's metaverse.
[00:12:14.960 --> 00:12:21.160]   Swiss researchers implanted an electronic device in a paraplegic skull on top of the
[00:12:21.160 --> 00:12:24.400]   region of the brain responsible for controlling leg movements.
[00:12:24.400 --> 00:12:28.600]   And then this is the AI part using algorithms based on adaptive AI methods.
[00:12:28.600 --> 00:12:30.800]   This is from the Financial Times.
[00:12:30.800 --> 00:12:39.040]   And intentions are decoded in real time from the brain and then transmitted wirelessly
[00:12:39.040 --> 00:12:43.960]   to a neural stimulator connected to an electrode ray over the part of the spinal cord that
[00:12:43.960 --> 00:12:47.320]   controls leg movement that's below the injury site.
[00:12:47.320 --> 00:12:49.320]   And he was able to walk.
[00:12:49.320 --> 00:12:50.320]   Yeah.
[00:12:50.320 --> 00:12:52.440]   And then also even be able to do that.
[00:12:52.440 --> 00:12:53.680]   Do I remember this right?
[00:12:53.680 --> 00:12:58.320]   It was even after a lot of training, he was able to do it without the connection in place.
[00:12:58.320 --> 00:12:59.320]   I think was the case too.
[00:12:59.320 --> 00:13:00.320]   Oh, that's interesting.
[00:13:00.320 --> 00:13:06.120]   So even though the signals aren't there, he was able to produce the response required.
[00:13:06.120 --> 00:13:11.160]   I mean, just this is his science fiction that we're living in, which is the good kind,
[00:13:11.160 --> 00:13:12.360]   the non-distopian kind.
[00:13:12.360 --> 00:13:13.360]   Yeah.
[00:13:13.360 --> 00:13:14.360]   Occasionally you get it.
[00:13:14.360 --> 00:13:17.080]   The Dutch man's spine was injured 11 years ago in a bike accident.
[00:13:17.080 --> 00:13:19.360]   They upgraded his digital bridge.
[00:13:19.360 --> 00:13:21.840]   He said this feels radically different before.
[00:13:21.840 --> 00:13:25.200]   I felt that the stimulation was controlling me.
[00:13:25.200 --> 00:13:27.720]   And now I am controlling the stimulation myself.
[00:13:27.720 --> 00:13:31.120]   I could take steps that feel natural.
[00:13:31.120 --> 00:13:32.480]   And there's no external computer.
[00:13:32.480 --> 00:13:38.720]   It's all in his head, so to speak.
[00:13:38.720 --> 00:13:39.720]   That is...
[00:13:39.720 --> 00:13:40.720]   Yeah.
[00:13:40.720 --> 00:13:42.520]   Thank you for bringing that up.
[00:13:42.520 --> 00:13:44.760]   We should talk about the positive stuff really.
[00:13:44.760 --> 00:13:46.360]   Does this feel good story?
[00:13:46.360 --> 00:13:47.360]   Yeah.
[00:13:47.360 --> 00:13:51.400]   It is feel good, but I'll also point out that there's down the road security risks with
[00:13:51.400 --> 00:13:52.400]   this as well.
[00:13:52.400 --> 00:13:53.400]   Oh, yeah.
[00:13:53.400 --> 00:13:54.400]   Lord.
[00:13:54.400 --> 00:13:55.400]   I think it's a good thing.
[00:13:55.400 --> 00:13:56.400]   I think it's a good thing.
[00:13:56.400 --> 00:13:57.400]   I think it's a good thing.
[00:13:57.400 --> 00:13:58.400]   I think it's a good thing.
[00:13:58.400 --> 00:13:59.400]   I think it's a good thing.
[00:13:59.400 --> 00:14:00.400]   I think it's a good thing.
[00:14:00.400 --> 00:14:01.400]   I think it's a good thing.
[00:14:01.400 --> 00:14:02.400]   I think it's a good thing.
[00:14:02.400 --> 00:14:03.400]   I think it's a good thing.
[00:14:03.400 --> 00:14:04.400]   I think it's a good thing.
[00:14:04.400 --> 00:14:05.400]   I think it's a good thing.
[00:14:05.400 --> 00:14:06.400]   I think it's a good thing.
[00:14:06.400 --> 00:14:07.400]   I think it's a good thing.
[00:14:07.400 --> 00:14:08.400]   I think it's a good thing.
[00:14:08.400 --> 00:14:09.400]   I think it's a good thing.
[00:14:09.400 --> 00:14:10.400]   I think it's a good thing.
[00:14:10.400 --> 00:14:11.400]   I think it's a good thing.
[00:14:11.400 --> 00:14:12.400]   I think it's a good thing.
[00:14:12.400 --> 00:14:13.400]   I think it's a good thing.
[00:14:13.400 --> 00:14:14.400]   I think it's a good thing.
[00:14:14.400 --> 00:14:15.400]   I think it's a good thing.
[00:14:15.400 --> 00:14:16.400]   I think it's a good thing.
[00:14:16.400 --> 00:14:17.400]   I think it's a good thing.
[00:14:17.400 --> 00:14:18.400]   I think it's a good thing.
[00:14:18.400 --> 00:14:19.400]   I think it's a good thing.
[00:14:19.400 --> 00:14:20.400]   I think it's a good thing.
[00:14:20.400 --> 00:14:21.400]   I think it's a good thing.
[00:14:21.400 --> 00:14:22.400]   I think it's a good thing.
[00:14:22.400 --> 00:14:23.400]   I think it's a good thing.
[00:14:23.400 --> 00:14:24.400]   I think it's a good thing.
[00:14:24.400 --> 00:14:25.400]   I think it's a good thing.
[00:14:25.400 --> 00:14:26.400]   I think it's a good thing.
[00:14:26.400 --> 00:14:27.400]   I think it's a good thing.
[00:14:27.400 --> 00:14:28.400]   I think it's a good thing.
[00:14:28.400 --> 00:14:29.400]   I think it's a good thing.
[00:14:29.400 --> 00:14:30.400]   I think it's a good thing.
[00:14:30.400 --> 00:14:31.400]   I think it's a good thing.
[00:14:31.400 --> 00:14:32.400]   I think it's a good thing.
[00:14:32.400 --> 00:14:33.400]   I think it's a good thing.
[00:14:33.400 --> 00:14:34.400]   I think it's a good thing.
[00:14:34.400 --> 00:14:35.400]   I think it's a good thing.
[00:14:35.400 --> 00:14:36.400]   I think it's a good thing.
[00:14:36.400 --> 00:14:37.400]   I think it's a good thing.
[00:14:37.400 --> 00:14:38.400]   I think it's a good thing.
[00:14:38.400 --> 00:14:39.400]   I think it's a good thing.
[00:14:39.400 --> 00:14:40.400]   I think it's a good thing.
[00:14:40.400 --> 00:14:41.400]   I think it's a good thing.
[00:14:41.400 --> 00:14:42.400]   I think it's a good thing.
[00:14:42.400 --> 00:14:43.400]   I think it's a good thing.
[00:14:43.400 --> 00:14:44.400]   I think it's a good thing.
[00:14:44.400 --> 00:14:45.400]   I think it's a good thing.
[00:14:45.400 --> 00:14:46.400]   I think it's a good thing.
[00:14:46.400 --> 00:14:47.400]   I think it's a good thing.
[00:14:47.400 --> 00:14:48.400]   I think it's a good thing.
[00:14:48.400 --> 00:14:49.400]   I think it's a good thing.
[00:14:49.400 --> 00:14:50.400]   I think it's a good thing.
[00:14:50.400 --> 00:14:51.400]   I think it's a good thing.
[00:14:51.400 --> 00:14:52.400]   I think it's a good thing.
[00:14:52.400 --> 00:14:53.400]   I think it's a good thing.
[00:14:53.400 --> 00:14:54.400]   I think it's a good thing.
[00:14:54.400 --> 00:14:55.400]   I think it's a good thing.
[00:14:55.400 --> 00:14:56.400]   I think it's a good thing.
[00:14:56.400 --> 00:14:57.400]   I think it's a good thing.
[00:14:57.400 --> 00:14:58.400]   I think it's a good thing.
[00:14:58.400 --> 00:14:59.400]   I think it's a good thing.
[00:14:59.400 --> 00:15:00.400]   I think it's a good thing.
[00:15:00.400 --> 00:15:01.400]   I think it's a good thing.
[00:15:01.400 --> 00:15:02.400]   I think it's a good thing.
[00:15:02.400 --> 00:15:03.400]   I think it's a good thing.
[00:15:03.400 --> 00:15:04.400]   I think it's a good thing.
[00:15:04.400 --> 00:15:05.400]   I think it's a good thing.
[00:15:05.400 --> 00:15:06.400]   I think it's a good thing.
[00:15:06.400 --> 00:15:07.400]   I think it's a good thing.
[00:15:07.400 --> 00:15:08.400]   I think it's a good thing.
[00:15:08.400 --> 00:15:09.400]   I think it's a good thing.
[00:15:09.400 --> 00:15:10.400]   I think it's a good thing.
[00:15:10.400 --> 00:15:11.400]   I think it's a good thing.
[00:15:11.400 --> 00:15:12.400]   I think it's a good thing.
[00:15:12.400 --> 00:15:13.400]   I think it's a good thing.
[00:15:13.400 --> 00:15:14.400]   I think it's a good thing.
[00:15:14.400 --> 00:15:15.400]   I think it's a good thing.
[00:15:15.400 --> 00:15:16.400]   I think it's a good thing.
[00:15:16.400 --> 00:15:17.400]   I think it's a good thing.
[00:15:17.400 --> 00:15:18.400]   I think it's a good thing.
[00:15:18.400 --> 00:15:19.400]   I think it's a good thing.
[00:15:19.400 --> 00:15:20.400]   I think it's a good thing.
[00:15:20.400 --> 00:15:21.400]   I think it's a good thing.
[00:15:21.400 --> 00:15:22.400]   I think it's a good thing.
[00:15:22.400 --> 00:15:24.400]   I think it's a good thing.
[00:15:24.400 --> 00:15:25.400]   I think it's a good thing.
[00:15:25.400 --> 00:15:26.400]   I think it's a good thing.
[00:15:26.400 --> 00:15:27.400]   I think it's a good thing.
[00:15:27.400 --> 00:15:28.400]   I think it's a good thing.
[00:15:28.400 --> 00:15:29.400]   I think it's a good thing.
[00:15:29.400 --> 00:15:30.400]   I think it's a good thing.
[00:15:30.400 --> 00:15:31.400]   I think it's a good thing.
[00:15:31.400 --> 00:15:32.400]   I think it's a good thing.
[00:15:32.400 --> 00:15:33.400]   I think it's a good thing.
[00:15:33.400 --> 00:15:34.400]   I think it's a good thing.
[00:15:34.400 --> 00:15:35.400]   I think it's a good thing.
[00:15:35.400 --> 00:15:36.400]   I think it's a good thing.
[00:15:36.400 --> 00:15:37.400]   I think it's a good thing.
[00:15:37.400 --> 00:15:38.400]   I think it's a good thing.
[00:15:38.400 --> 00:15:39.400]   I think it's a good thing.
[00:15:39.400 --> 00:15:40.400]   I think it's a good thing.
[00:15:40.400 --> 00:15:41.400]   I think it's a good thing.
[00:15:41.400 --> 00:15:42.400]   I think it's a good thing.
[00:15:42.400 --> 00:15:43.400]   I think it's a good thing.
[00:15:43.400 --> 00:15:44.400]   I think it's a good thing.
[00:15:44.400 --> 00:15:45.400]   I think it's a good thing.
[00:15:45.400 --> 00:15:46.400]   I think it's a good thing.
[00:15:46.400 --> 00:15:47.400]   I think it's a good thing.
[00:15:47.400 --> 00:15:48.400]   I think it's a good thing.
[00:15:48.400 --> 00:15:49.400]   I think it's a good thing.
[00:15:49.400 --> 00:15:50.400]   I think it's a good thing.
[00:15:50.400 --> 00:15:51.400]   I think it's a good thing.
[00:15:51.400 --> 00:15:52.400]   I think it's a good thing.
[00:15:52.400 --> 00:15:53.400]   I think it's a good thing.
[00:15:53.400 --> 00:15:54.400]   I think it's a good thing.
[00:15:54.400 --> 00:15:55.400]   I think it's a good thing.
[00:15:55.400 --> 00:15:56.400]   I think it's a good thing.
[00:15:56.400 --> 00:15:57.400]   I think it's a good thing.
[00:15:57.400 --> 00:15:58.400]   I think it's a good thing.
[00:15:58.400 --> 00:15:59.400]   I think it's a good thing.
[00:15:59.400 --> 00:16:00.400]   I think it's a good thing.
[00:16:00.400 --> 00:16:01.400]   I think it's a good thing.
[00:16:01.400 --> 00:16:02.400]   I think it's a good thing.
[00:16:02.400 --> 00:16:03.400]   I think it's a good thing.
[00:16:03.400 --> 00:16:04.400]   I think it's a good thing.
[00:16:04.400 --> 00:16:05.400]   I think it's a good thing.
[00:16:05.400 --> 00:16:06.400]   I think it's a good thing.
[00:16:06.400 --> 00:16:07.400]   I think it's a good thing.
[00:16:07.400 --> 00:16:08.400]   I think it's a good thing.
[00:16:08.400 --> 00:16:09.400]   I think it's a good thing.
[00:16:09.400 --> 00:16:10.400]   I think it's a good thing.
[00:16:10.400 --> 00:16:11.400]   I think it's a good thing.
[00:16:11.400 --> 00:16:12.400]   I think it's a good thing.
[00:16:12.400 --> 00:16:13.400]   I think it's a good thing.
[00:16:13.400 --> 00:16:14.400]   I think it's a good thing.
[00:16:14.400 --> 00:16:15.400]   I think it's a good thing.
[00:16:15.400 --> 00:16:16.400]   I think it's a good thing.
[00:16:16.400 --> 00:16:17.400]   I think it's a good thing.
[00:16:17.400 --> 00:16:18.400]   I think it's a good thing.
[00:16:18.400 --> 00:16:19.400]   I think it's a good thing.
[00:16:19.400 --> 00:16:20.400]   I think it's a good thing.
[00:16:20.400 --> 00:16:21.400]   I think it's a good thing.
[00:16:21.400 --> 00:16:22.400]   I think it's a good thing.
[00:16:22.400 --> 00:16:23.400]   I think it's a good thing.
[00:16:23.400 --> 00:16:24.400]   I think it's a good thing.
[00:16:24.400 --> 00:16:25.400]   I think it's a good thing.
[00:16:25.400 --> 00:16:26.400]   I think it's a good thing.
[00:16:26.400 --> 00:16:27.400]   I think it's a good thing.
[00:16:27.400 --> 00:16:28.400]   I think it's a good thing.
[00:16:28.400 --> 00:16:29.400]   I think it's a good thing.
[00:16:29.400 --> 00:16:30.400]   I think it's a good thing.
[00:16:30.400 --> 00:16:31.400]   I think it's a good thing.
[00:16:31.400 --> 00:16:32.400]   I think it's a good thing.
[00:16:32.400 --> 00:16:33.400]   I think it's a good thing.
[00:16:33.400 --> 00:16:34.400]   I think it's a good thing.
[00:16:34.400 --> 00:16:35.400]   I think it's a good thing.
[00:16:35.400 --> 00:16:36.400]   I think it's a good thing.
[00:16:36.400 --> 00:16:37.400]   I think it's a good thing.
[00:16:37.400 --> 00:16:38.400]   I think it's a good thing.
[00:16:38.400 --> 00:16:39.400]   I think it's a good thing.
[00:16:39.400 --> 00:16:40.400]   I think it's a good thing.
[00:16:40.400 --> 00:16:41.400]   I think it's a good thing.
[00:16:41.400 --> 00:16:42.400]   I think it's a good thing.
[00:16:42.400 --> 00:16:43.400]   I think it's a good thing.
[00:16:43.400 --> 00:16:44.400]   I think it's a good thing.
[00:16:44.400 --> 00:16:45.400]   I think it's a good thing.
[00:16:45.400 --> 00:16:46.400]   I think it's a good thing.
[00:16:46.400 --> 00:16:47.400]   I think it's a good thing.
[00:16:47.400 --> 00:16:48.400]   I think it's a good thing.
[00:16:48.400 --> 00:16:49.400]   I think it's a good thing.
[00:16:49.400 --> 00:16:50.400]   I think it's a good thing.
[00:16:50.400 --> 00:16:51.400]   I think it's a good thing.
[00:16:51.400 --> 00:16:52.400]   I think it's a good thing.
[00:16:52.400 --> 00:16:53.400]   I think it's a good thing.
[00:16:53.400 --> 00:16:54.400]   I think it's a good thing.
[00:16:54.400 --> 00:16:55.400]   I think it's a good thing.
[00:16:55.400 --> 00:16:56.400]   I think it's a good thing.
[00:16:56.400 --> 00:16:57.400]   I think it's a good thing.
[00:16:57.400 --> 00:16:58.400]   I think it's a good thing.
[00:16:58.400 --> 00:16:59.400]   I think it's a good thing.
[00:16:59.400 --> 00:17:00.400]   I think it's a good thing.
[00:17:00.400 --> 00:17:01.400]   I think it's a good thing.
[00:17:01.400 --> 00:17:02.400]   I think it's a good thing.
[00:17:02.400 --> 00:17:03.400]   I think it's a good thing.
[00:17:03.400 --> 00:17:04.400]   I think it's a good thing.
[00:17:04.400 --> 00:17:05.400]   I think it's a good thing.
[00:17:05.400 --> 00:17:06.400]   I think it's a good thing.
[00:17:06.400 --> 00:17:07.400]   I think it's a good thing.
[00:17:07.400 --> 00:17:08.400]   I think it's a good thing.
[00:17:08.400 --> 00:17:09.400]   I think it's a good thing.
[00:17:09.400 --> 00:17:10.400]   I think it's a good thing.
[00:17:10.400 --> 00:17:11.400]   I think it's a good thing.
[00:17:11.400 --> 00:17:12.400]   I think it's a good thing.
[00:17:12.400 --> 00:17:13.400]   I think it's a good thing.
[00:17:13.400 --> 00:17:14.400]   I think it's a good thing.
[00:17:14.400 --> 00:17:15.400]   I think it's a good thing.
[00:17:15.400 --> 00:17:16.400]   I think it's a good thing.
[00:17:16.400 --> 00:17:17.400]   I think it's a good thing.
[00:17:17.400 --> 00:17:18.400]   I think it's a good thing.
[00:17:18.400 --> 00:17:19.400]   I think it's a good thing.
[00:17:19.400 --> 00:17:20.400]   I think it's a good thing.
[00:17:20.400 --> 00:17:21.400]   I think it's a good thing.
[00:17:21.400 --> 00:17:22.400]   I think it's a good thing.
[00:17:22.400 --> 00:17:23.400]   I think it's a good thing.
[00:17:23.400 --> 00:17:24.400]   I think it's a good thing.
[00:17:24.400 --> 00:17:25.400]   I think it's a good thing.
[00:17:25.400 --> 00:17:26.400]   I think it's a good thing.
[00:17:26.400 --> 00:17:27.400]   I think it's a good thing.
[00:17:27.400 --> 00:17:28.400]   I think it's a good thing.
[00:17:28.400 --> 00:17:29.400]   I think it's a good thing.
[00:17:29.400 --> 00:17:30.400]   I think it's a good thing.
[00:17:30.400 --> 00:17:31.400]   I think it's a good thing.
[00:17:31.400 --> 00:17:32.400]   I think it's a good thing.
[00:17:32.400 --> 00:17:33.400]   I think it's a good thing.
[00:17:33.400 --> 00:17:34.400]   I think it's a good thing.
[00:17:34.400 --> 00:17:35.400]   I think it's a good thing.
[00:17:35.400 --> 00:17:36.400]   I think it's a good thing.
[00:17:36.400 --> 00:17:37.400]   I think it's a good thing.
[00:17:37.400 --> 00:17:38.400]   I think it's a good thing.
[00:17:38.400 --> 00:17:39.400]   I think it's a good thing.
[00:17:39.400 --> 00:17:40.400]   I think it's a good thing.
[00:17:40.400 --> 00:17:41.400]   I think it's a good thing.
[00:17:41.400 --> 00:17:42.400]   I think it's a good thing.
[00:17:42.400 --> 00:17:43.400]   I think it's a good thing.
[00:17:43.400 --> 00:17:44.400]   I think it's a good thing.
[00:17:44.400 --> 00:17:45.400]   I think it's a good thing.
[00:17:45.400 --> 00:17:46.400]   I think it's a good thing.
[00:17:46.400 --> 00:17:47.400]   I think it's a good thing.
[00:17:47.400 --> 00:17:48.400]   I think it's a good thing.
[00:17:48.400 --> 00:17:49.400]   I think it's a good thing.
[00:17:49.400 --> 00:17:50.400]   I think it's a good thing.
[00:17:50.400 --> 00:17:51.400]   I think it's a good thing.
[00:17:51.400 --> 00:17:52.400]   I think it's a good thing.
[00:17:52.400 --> 00:17:53.400]   I think it's a good thing.
[00:17:53.400 --> 00:17:54.400]   I think it's a good thing.
[00:17:54.400 --> 00:17:55.400]   I think it's a good thing.
[00:17:55.400 --> 00:17:56.400]   I think it's a good thing.
[00:17:56.400 --> 00:17:57.400]   I think it's a good thing.
[00:17:57.400 --> 00:17:58.400]   I think it's a good thing.
[00:17:58.400 --> 00:17:59.400]   I think it's a good thing.
[00:17:59.400 --> 00:18:00.400]   I think it's a good thing.
[00:18:00.400 --> 00:18:01.400]   I think it's a good thing.
[00:18:01.400 --> 00:18:02.400]   I think it's a good thing.
[00:18:02.400 --> 00:18:03.400]   I think it's a good thing.
[00:18:03.400 --> 00:18:04.400]   I think it's a good thing.
[00:18:04.400 --> 00:18:05.400]   I think it's a good thing.
[00:18:05.400 --> 00:18:06.400]   I think it's a good thing.
[00:18:06.400 --> 00:18:07.400]   I think it's a good thing.
[00:18:07.400 --> 00:18:08.400]   I think it's a good thing.
[00:18:08.400 --> 00:18:09.400]   I think it's a good thing.
[00:18:09.400 --> 00:18:10.400]   I think it's a good thing.
[00:18:10.400 --> 00:18:11.400]   I think it's a good thing.
[00:18:11.400 --> 00:18:12.400]   I think it's a good thing.
[00:18:12.400 --> 00:18:13.400]   I think it's a good thing.
[00:18:13.400 --> 00:18:14.400]   I think it's a good thing.
[00:18:14.400 --> 00:18:15.400]   I think it's a good thing.
[00:18:15.400 --> 00:18:16.400]   I think it's a good thing.
[00:18:16.400 --> 00:18:17.400]   I think it's a good thing.
[00:18:17.400 --> 00:18:18.400]   I think it's a good thing.
[00:18:18.400 --> 00:18:19.400]   I think it's a good thing.
[00:18:19.400 --> 00:18:20.400]   I think it's a good thing.
[00:18:20.400 --> 00:18:21.400]   I think it's a good thing.
[00:18:21.400 --> 00:18:22.400]   I think it's a good thing.
[00:18:22.400 --> 00:18:23.400]   I think it's a good thing.
[00:18:23.400 --> 00:18:24.400]   I think it's a good thing.
[00:18:24.400 --> 00:18:25.400]   I think it's a good thing.
[00:18:25.400 --> 00:18:26.400]   I think it's a good thing.
[00:18:26.400 --> 00:18:27.400]   I think it's a good thing.
[00:18:27.400 --> 00:18:28.400]   I think it's a good thing.
[00:18:28.400 --> 00:18:29.400]   I think it's a good thing.
[00:18:29.400 --> 00:18:30.400]   I think it's a good thing.
[00:18:30.400 --> 00:18:31.400]   I think it's a good thing.
[00:18:31.400 --> 00:18:32.400]   I think it's a good thing.
[00:18:32.400 --> 00:18:33.400]   I think it's a good thing.
[00:18:33.400 --> 00:18:34.400]   I think it's a good thing.
[00:18:34.400 --> 00:18:35.400]   I think it's a good thing.
[00:18:35.400 --> 00:18:36.400]   I think it's a good thing.
[00:18:36.400 --> 00:18:37.400]   I think it's a good thing.
[00:18:37.400 --> 00:18:38.400]   I think it's a good thing.
[00:18:38.400 --> 00:18:39.400]   I think it's a good thing.
[00:18:39.400 --> 00:18:40.400]   I think it's a good thing.
[00:18:40.400 --> 00:18:41.400]   I think it's a good thing.
[00:18:41.400 --> 00:18:42.400]   I think it's a good thing.
[00:18:42.400 --> 00:18:43.400]   I think it's a good thing.
[00:18:43.400 --> 00:18:44.400]   I think it's a good thing.
[00:18:44.400 --> 00:18:45.400]   I think it's a good thing.
[00:18:45.400 --> 00:18:46.400]   I think it's a good thing.
[00:18:46.400 --> 00:18:47.400]   I think it's a good thing.
[00:18:47.400 --> 00:18:48.400]   I think it's a good thing.
[00:18:48.400 --> 00:18:49.400]   I think it's a good thing.
[00:18:49.400 --> 00:18:50.400]   I think it's a good thing.
[00:18:50.400 --> 00:18:51.400]   I think it's a good thing.
[00:18:51.400 --> 00:18:52.400]   I think it's a good thing.
[00:18:52.400 --> 00:18:53.400]   I think it's a good thing.
[00:18:53.400 --> 00:18:54.400]   I think it's a good thing.
[00:18:54.400 --> 00:18:55.400]   I think it's a good thing.
[00:18:55.400 --> 00:18:56.400]   I think it's a good thing.
[00:18:56.400 --> 00:18:57.400]   I think it's a good thing.
[00:18:57.400 --> 00:18:58.400]   I think it's a good thing.
[00:18:58.400 --> 00:18:59.400]   I think it's a good thing.
[00:18:59.400 --> 00:19:00.400]   I think it's a good thing.
[00:19:00.400 --> 00:19:01.400]   I think it's a good thing.
[00:19:01.400 --> 00:19:02.400]   I think it's a good thing.
[00:19:02.400 --> 00:19:03.400]   I think it's a good thing.
[00:19:03.400 --> 00:19:04.400]   I think it's a good thing.
[00:19:04.400 --> 00:19:05.400]   I think it's a good thing.
[00:19:05.400 --> 00:19:06.400]   I think it's a good thing.
[00:19:06.400 --> 00:19:07.400]   I think it's a good thing.
[00:19:07.400 --> 00:19:08.400]   I think it's a good thing.
[00:19:08.400 --> 00:19:09.400]   I think it's a good thing.
[00:19:09.400 --> 00:19:10.400]   I think it's a good thing.
[00:19:10.400 --> 00:19:11.400]   I think it's a good thing.
[00:19:11.400 --> 00:19:12.400]   I think it's a good thing.
[00:19:12.400 --> 00:19:13.400]   I think it's a good thing.
[00:19:13.400 --> 00:19:14.400]   I think it's a good thing.
[00:19:14.400 --> 00:19:15.400]   I think it's a good thing.
[00:19:15.400 --> 00:19:16.400]   I think it's a good thing.
[00:19:16.400 --> 00:19:17.400]   I think it's a good thing.
[00:19:17.400 --> 00:19:18.400]   I think it's a good thing.
[00:19:18.400 --> 00:19:19.400]   I think it's a good thing.
[00:19:19.400 --> 00:19:20.400]   I think it's a good thing.
[00:19:20.400 --> 00:19:21.400]   I think it's a good thing.
[00:19:21.400 --> 00:19:22.400]   I think it's a good thing.
[00:19:22.400 --> 00:19:23.400]   I think it's a good thing.
[00:19:23.400 --> 00:19:24.400]   I think it's a good thing.
[00:19:24.400 --> 00:19:25.400]   I think it's a good thing.
[00:19:25.400 --> 00:19:27.400]   I think it's a good thing.
[00:19:27.400 --> 00:19:28.400]   I think it's a good thing.
[00:19:28.400 --> 00:19:29.400]   I think it's a good thing.
[00:19:29.400 --> 00:19:30.400]   I think it's a good thing.
[00:19:30.400 --> 00:19:31.400]   I think it's a good thing.
[00:19:31.400 --> 00:19:32.400]   I think it's a good thing.
[00:19:32.400 --> 00:19:33.400]   I think it's a good thing.
[00:19:33.400 --> 00:19:34.400]   I think it's a good thing.
[00:19:34.400 --> 00:19:35.400]   I think it's a good thing.
[00:19:35.400 --> 00:19:36.400]   I think it's a good thing.
[00:19:36.400 --> 00:19:37.400]   I think it's a good thing.
[00:19:37.400 --> 00:19:38.400]   I think it's a good thing.
[00:19:38.400 --> 00:19:39.400]   I think it's a good thing.
[00:19:39.400 --> 00:19:40.400]   I think it's a good thing.
[00:19:40.400 --> 00:19:41.400]   I think it's a good thing.
[00:19:41.400 --> 00:19:42.400]   I think it's a good thing.
[00:19:42.400 --> 00:19:43.400]   I think it's a good thing.
[00:19:43.400 --> 00:19:44.400]   I think it's a good thing.
[00:19:44.400 --> 00:19:45.400]   I think it's a good thing.
[00:19:45.400 --> 00:19:46.400]   I think it's a good thing.
[00:19:46.400 --> 00:19:47.400]   I think it's a good thing.
[00:19:47.400 --> 00:19:48.400]   I think it's a good thing.
[00:19:48.400 --> 00:19:49.400]   I think it's a good thing.
[00:19:49.400 --> 00:19:50.400]   I think it's a good thing.
[00:19:50.400 --> 00:19:51.400]   I think it's a good thing.
[00:19:51.400 --> 00:19:52.400]   I think it's a good thing.
[00:19:52.400 --> 00:19:53.400]   I think it's a good thing.
[00:19:53.400 --> 00:19:54.400]   I think it's a good thing.
[00:19:54.400 --> 00:19:55.400]   I think it's a good thing.
[00:19:55.400 --> 00:19:56.400]   I think it's a good thing.
[00:19:56.400 --> 00:19:57.400]   I think it's a good thing.
[00:19:57.400 --> 00:19:58.400]   I think it's a good thing.
[00:19:58.400 --> 00:19:59.400]   I think it's a good thing.
[00:19:59.400 --> 00:20:00.400]   I think it's a good thing.
[00:20:00.400 --> 00:20:01.400]   I think it's a good thing.
[00:20:01.400 --> 00:20:02.400]   I think it's a good thing.
[00:20:02.400 --> 00:20:03.400]   I think it's a good thing.
[00:20:03.400 --> 00:20:04.400]   I think it's a good thing.
[00:20:04.400 --> 00:20:05.400]   I think it's a good thing.
[00:20:05.400 --> 00:20:06.400]   I think it's a good thing.
[00:20:06.400 --> 00:20:07.400]   I think it's a good thing.
[00:20:07.400 --> 00:20:08.400]   I think it's a good thing.
[00:20:08.400 --> 00:20:09.400]   I think it's a good thing.
[00:20:09.400 --> 00:20:10.400]   I think it's a good thing.
[00:20:10.400 --> 00:20:12.400]   I think it's a good thing.
[00:20:12.400 --> 00:20:13.400]   I think it's a good thing.
[00:20:13.400 --> 00:20:14.400]   I think it's a good thing.
[00:20:14.400 --> 00:20:15.400]   I think it's a good thing.
[00:20:15.400 --> 00:20:16.400]   I think it's a good thing.
[00:20:16.400 --> 00:20:17.400]   I think it's a good thing.
[00:20:17.400 --> 00:20:18.400]   I think it's a good thing.
[00:20:18.400 --> 00:20:19.400]   I think it's a good thing.
[00:20:19.400 --> 00:20:20.400]   I think it's a good thing.
[00:20:20.400 --> 00:20:21.400]   I think it's a good thing.
[00:20:21.400 --> 00:20:22.400]   I think it's a good thing.
[00:20:22.400 --> 00:20:23.400]   I think it's a good thing.
[00:20:23.400 --> 00:20:24.400]   I think it's a good thing.
[00:20:24.400 --> 00:20:25.400]   I think it's a good thing.
[00:20:25.400 --> 00:20:26.400]   I think it's a good thing.
[00:20:26.400 --> 00:20:27.400]   I think it's a good thing.
[00:20:27.400 --> 00:20:28.400]   I think it's a good thing.
[00:20:28.400 --> 00:20:29.400]   I think it's a good thing.
[00:20:29.400 --> 00:20:30.400]   I think it's a good thing.
[00:20:30.400 --> 00:20:31.400]   I think it's a good thing.
[00:20:31.400 --> 00:20:32.400]   I think it's a good thing.
[00:20:32.400 --> 00:20:33.400]   I think it's a good thing.
[00:20:33.400 --> 00:20:34.400]   I think it's a good thing.
[00:20:34.400 --> 00:20:35.400]   I think it's a good thing.
[00:20:35.400 --> 00:20:36.400]   I think it's a good thing.
[00:20:36.400 --> 00:20:37.400]   I think it's a good thing.
[00:20:37.400 --> 00:20:38.400]   I think it's a good thing.
[00:20:38.400 --> 00:20:39.400]   I think it's a good thing.
[00:20:39.400 --> 00:20:40.400]   I think it's a good thing.
[00:20:40.400 --> 00:20:41.400]   I think it's a good thing.
[00:20:41.400 --> 00:20:42.400]   I think it's a good thing.
[00:20:42.400 --> 00:20:43.400]   I think it's a good thing.
[00:20:43.400 --> 00:20:44.400]   I think it's a good thing.
[00:20:44.400 --> 00:20:45.400]   I think it's a good thing.
[00:20:45.400 --> 00:20:46.400]   I think it's a good thing.
[00:20:46.400 --> 00:20:47.400]   I think it's a good thing.
[00:20:47.400 --> 00:20:48.400]   I think it's a good thing.
[00:20:48.400 --> 00:20:49.400]   I think it's a good thing.
[00:20:49.400 --> 00:20:50.400]   I think it's a good thing.
[00:20:50.400 --> 00:20:51.400]   I think it's a good thing.
[00:20:51.400 --> 00:20:52.400]   I think it's a good thing.
[00:20:52.400 --> 00:20:53.400]   I think it's a good thing.
[00:20:53.400 --> 00:20:54.400]   I think it's a good thing.
[00:20:54.400 --> 00:20:55.400]   I think it's a good thing.
[00:20:55.400 --> 00:20:56.400]   I think it's a good thing.
[00:20:56.400 --> 00:20:57.400]   I think it's a good thing.
[00:20:57.400 --> 00:20:58.400]   I think it's a good thing.
[00:20:58.400 --> 00:20:59.400]   I think it's a good thing.
[00:20:59.400 --> 00:21:00.400]   I think it's a good thing.
[00:21:00.400 --> 00:21:01.400]   I think it's a good thing.
[00:21:01.400 --> 00:21:02.400]   I think it's a good thing.
[00:21:02.400 --> 00:21:03.400]   I think it's a good thing.
[00:21:03.400 --> 00:21:04.400]   I think it's a good thing.
[00:21:04.400 --> 00:21:05.400]   I think it's a good thing.
[00:21:05.400 --> 00:21:06.400]   I think it's a good thing.
[00:21:06.400 --> 00:21:07.400]   I think it's a good thing.
[00:21:07.400 --> 00:21:08.400]   I think it's a good thing.
[00:21:08.400 --> 00:21:09.400]   I think it's a good thing.
[00:21:09.400 --> 00:21:10.400]   I think it's a good thing.
[00:21:10.400 --> 00:21:11.400]   I think it's a good thing.
[00:21:11.400 --> 00:21:12.400]   I think it's a good thing.
[00:21:12.400 --> 00:21:13.400]   I think it's a good thing.
[00:21:13.400 --> 00:21:14.400]   I think it's a good thing.
[00:21:14.400 --> 00:21:15.400]   I think it's a good thing.
[00:21:15.400 --> 00:21:16.400]   I think it's a good thing.
[00:21:16.400 --> 00:21:17.400]   I think it's a good thing.
[00:21:17.400 --> 00:21:18.400]   I think it's a good thing.
[00:21:18.400 --> 00:21:19.400]   I think it's a good thing.
[00:21:19.400 --> 00:21:20.400]   I think it's a good thing.
[00:21:20.400 --> 00:21:21.400]   I think it's a good thing.
[00:21:21.400 --> 00:21:22.400]   I think it's a good thing.
[00:21:22.400 --> 00:21:23.400]   I think it's a good thing.
[00:21:23.400 --> 00:21:24.400]   I think it's a good thing.
[00:21:24.400 --> 00:21:25.400]   I think it's a good thing.
[00:21:25.400 --> 00:21:26.400]   I think it's a good thing.
[00:21:26.400 --> 00:21:27.400]   I think it's a good thing.
[00:21:27.400 --> 00:21:28.400]   I think it's a good thing.
[00:21:28.400 --> 00:21:29.400]   I think it's a good thing.
[00:21:29.400 --> 00:21:30.400]   I think it's a good thing.
[00:21:30.400 --> 00:21:31.400]   I think it's a good thing.
[00:21:31.400 --> 00:21:32.400]   I think it's a good thing.
[00:21:32.400 --> 00:21:33.400]   I think it's a good thing.
[00:21:33.400 --> 00:21:34.400]   I think it's a good thing.
[00:21:34.400 --> 00:21:35.400]   I think it's a good thing.
[00:21:35.400 --> 00:21:36.400]   I think it's a good thing.
[00:21:36.400 --> 00:21:37.400]   I think it's a good thing.
[00:21:37.400 --> 00:21:38.400]   I think it's a good thing.
[00:21:38.400 --> 00:21:39.400]   I think it's a good thing.
[00:21:39.400 --> 00:21:40.400]   I think it's a good thing.
[00:21:40.400 --> 00:21:41.400]   I think it's a good thing.
[00:21:41.400 --> 00:21:42.400]   I think it's a good thing.
[00:21:42.400 --> 00:21:43.400]   I think it's a good thing.
[00:21:43.400 --> 00:21:44.400]   I think it's a good thing.
[00:21:44.400 --> 00:21:45.400]   I think it's a good thing.
[00:21:45.400 --> 00:21:46.400]   I think it's a good thing.
[00:21:46.400 --> 00:21:47.400]   I think it's a good thing.
[00:21:47.400 --> 00:21:48.400]   I think it's a good thing.
[00:21:48.400 --> 00:21:49.400]   I think it's a good thing.
[00:21:49.400 --> 00:21:50.400]   I think it's a good thing.
[00:21:50.400 --> 00:21:51.400]   I think it's a good thing.
[00:21:51.400 --> 00:21:52.400]   I think it's a good thing.
[00:21:52.400 --> 00:21:53.400]   I think it's a good thing.
[00:21:53.400 --> 00:21:54.400]   I think it's a good thing.
[00:21:54.400 --> 00:21:55.400]   I think it's a good thing.
[00:21:55.400 --> 00:21:56.400]   I think it's a good thing.
[00:21:56.400 --> 00:21:57.400]   I think it's a good thing.
[00:21:57.400 --> 00:21:58.400]   I think it's a good thing.
[00:21:58.400 --> 00:21:59.400]   I think it's a good thing.
[00:21:59.400 --> 00:22:00.400]   I think it's a good thing.
[00:22:00.400 --> 00:22:01.400]   I think it's a good thing.
[00:22:01.400 --> 00:22:02.400]   I think it's a good thing.
[00:22:02.400 --> 00:22:03.400]   I think it's a good thing.
[00:22:03.400 --> 00:22:04.400]   I think it's a good thing.
[00:22:04.400 --> 00:22:05.400]   I think it's a good thing.
[00:22:05.400 --> 00:22:06.400]   I think it's a good thing.
[00:22:06.400 --> 00:22:07.400]   I think it's a good thing.
[00:22:07.400 --> 00:22:08.400]   I think it's a good thing.
[00:22:08.400 --> 00:22:09.400]   I think it's a good thing.
[00:22:09.400 --> 00:22:10.400]   I think it's a good thing.
[00:22:10.400 --> 00:22:11.400]   I think it's a good thing.
[00:22:11.400 --> 00:22:12.400]   I think it's a good thing.
[00:22:12.400 --> 00:22:13.400]   I think it's a good thing.
[00:22:13.400 --> 00:22:14.400]   I think it's a good thing.
[00:22:14.400 --> 00:22:15.400]   I think it's a good thing.
[00:22:15.400 --> 00:22:16.400]   I think it's a good thing.
[00:22:16.400 --> 00:22:17.400]   I think it's a good thing.
[00:22:17.400 --> 00:22:18.400]   I think it's a good thing.
[00:22:18.400 --> 00:22:19.400]   I think it's a good thing.
[00:22:19.400 --> 00:22:20.400]   I think it's a good thing.
[00:22:20.400 --> 00:22:21.400]   I think it's a good thing.
[00:22:21.400 --> 00:22:22.400]   I think it's a good thing.
[00:22:22.400 --> 00:22:23.400]   I think it's a good thing.
[00:22:23.400 --> 00:22:24.400]   I think it's a good thing.
[00:22:24.400 --> 00:22:25.400]   I think it's a good thing.
[00:22:25.400 --> 00:22:26.400]   I think it's a good thing.
[00:22:26.400 --> 00:22:27.400]   I think it's a good thing.
[00:22:27.400 --> 00:22:28.400]   I think it's a good thing.
[00:22:28.400 --> 00:22:29.400]   I think it's a good thing.
[00:22:29.400 --> 00:22:30.400]   I think it's a good thing.
[00:22:30.400 --> 00:22:31.400]   I think it's a good thing.
[00:22:31.400 --> 00:22:32.400]   I think it's a good thing.
[00:22:32.400 --> 00:22:33.400]   I think it's a good thing.
[00:22:33.400 --> 00:22:34.400]   I think it's a good thing.
[00:22:34.400 --> 00:22:35.400]   I think it's a good thing.
[00:22:35.400 --> 00:22:36.400]   I think it's a good thing.
[00:22:36.400 --> 00:22:37.400]   I think it's a good thing.
[00:22:37.400 --> 00:22:38.400]   I think it's a good thing.
[00:22:38.400 --> 00:22:39.400]   I think it's a good thing.
[00:22:39.400 --> 00:22:40.400]   I think it's a good thing.
[00:22:40.400 --> 00:22:41.400]   I think it's a good thing.
[00:22:41.400 --> 00:22:42.400]   I think it's a good thing.
[00:22:42.400 --> 00:22:43.400]   I think it's a good thing.
[00:22:43.400 --> 00:22:44.400]   I think it's a good thing.
[00:22:44.400 --> 00:22:45.400]   I think it's a good thing.
[00:22:45.400 --> 00:22:46.400]   I think it's a good thing.
[00:22:46.400 --> 00:22:47.400]   I think it's a good thing.
[00:22:47.400 --> 00:22:48.400]   I think it's a good thing.
[00:22:48.400 --> 00:22:49.400]   I think it's a good thing.
[00:22:49.400 --> 00:22:50.400]   I think it's a good thing.
[00:22:50.400 --> 00:22:51.400]   I think it's a good thing.
[00:22:51.400 --> 00:22:52.400]   I think it's a good thing.
[00:22:52.400 --> 00:22:53.400]   I think it's a good thing.
[00:22:53.400 --> 00:22:54.400]   I think it's a good thing.
[00:22:54.400 --> 00:22:55.400]   I think it's a good thing.
[00:22:55.400 --> 00:22:56.400]   I think it's a good thing.
[00:22:56.400 --> 00:22:57.400]   I think it's a good thing.
[00:22:57.400 --> 00:22:58.400]   I think it's a good thing.
[00:22:58.400 --> 00:22:59.400]   I think it's a good thing.
[00:22:59.400 --> 00:23:00.400]   I think it's a good thing.
[00:23:00.400 --> 00:23:01.400]   I think it's a good thing.
[00:23:01.400 --> 00:23:02.400]   I think it's a good thing.
[00:23:02.400 --> 00:23:03.400]   I think it's a good thing.
[00:23:03.400 --> 00:23:04.400]   I think it's a good thing.
[00:23:04.400 --> 00:23:05.400]   I think it's a good thing.
[00:23:05.400 --> 00:23:06.400]   I think it's a good thing.
[00:23:06.400 --> 00:23:07.400]   I think it's a good thing.
[00:23:07.400 --> 00:23:08.400]   I think it's a good thing.
[00:23:08.400 --> 00:23:09.400]   I think it's a good thing.
[00:23:09.400 --> 00:23:10.400]   I think it's a good thing.
[00:23:10.400 --> 00:23:11.400]   I think it's a good thing.
[00:23:11.400 --> 00:23:12.400]   I think it's a good thing.
[00:23:12.400 --> 00:23:13.400]   I think it's a good thing.
[00:23:13.400 --> 00:23:14.400]   I think it's a good thing.
[00:23:14.400 --> 00:23:15.400]   I think it's a good thing.
[00:23:15.400 --> 00:23:16.400]   I think it's a good thing.
[00:23:16.400 --> 00:23:17.400]   I think it's a good thing.
[00:23:17.400 --> 00:23:18.400]   I think it's a good thing.
[00:23:18.400 --> 00:23:19.400]   I think it's a good thing.
[00:23:19.400 --> 00:23:20.400]   I think it's a good thing.
[00:23:20.400 --> 00:23:21.400]   I think it's a good thing.
[00:23:21.400 --> 00:23:22.400]   I think it's a good thing.
[00:23:22.400 --> 00:23:23.400]   I think it's a good thing.
[00:23:23.400 --> 00:23:24.400]   I think it's a good thing.
[00:23:24.400 --> 00:23:25.400]   I think it's a good thing.
[00:23:25.400 --> 00:23:26.400]   I think it's a good thing.
[00:23:26.400 --> 00:23:27.400]   I think it's a good thing.
[00:23:27.400 --> 00:23:28.400]   I think it's a good thing.
[00:23:28.400 --> 00:23:29.400]   I think it's a good thing.
[00:23:29.400 --> 00:23:30.400]   I think it's a good thing.
[00:23:30.400 --> 00:23:31.400]   I think it's a good thing.
[00:23:31.400 --> 00:23:32.400]   I think it's a good thing.
[00:23:32.400 --> 00:23:33.400]   I think it's a good thing.
[00:23:33.400 --> 00:23:34.400]   I think it's a good thing.
[00:23:34.400 --> 00:23:35.400]   I think it's a good thing.
[00:23:35.400 --> 00:23:36.400]   I think it's a good thing.
[00:23:36.400 --> 00:23:37.400]   I think it's a good thing.
[00:23:37.400 --> 00:23:38.400]   I think it's a good thing.
[00:23:38.400 --> 00:23:39.400]   I think it's a good thing.
[00:23:39.400 --> 00:23:40.400]   I think it's a good thing.
[00:23:40.400 --> 00:23:41.400]   I think it's a good thing.
[00:23:41.400 --> 00:23:42.400]   I think it's a good thing.
[00:23:42.400 --> 00:23:43.400]   I think it's a good thing.
[00:23:43.400 --> 00:23:44.400]   I think it's a good thing.
[00:23:44.400 --> 00:23:45.400]   I think it's a good thing.
[00:23:45.400 --> 00:23:46.400]   I think it's a good thing.
[00:23:46.400 --> 00:23:47.400]   I think it's a good thing.
[00:23:47.400 --> 00:23:48.400]   I think it's a good thing.
[00:23:48.400 --> 00:23:49.400]   I think it's a good thing.
[00:23:49.400 --> 00:23:50.400]   I think it's a good thing.
[00:23:50.400 --> 00:23:51.400]   I think it's a good thing.
[00:23:51.400 --> 00:23:52.400]   I think it's a good thing.
[00:23:52.400 --> 00:23:53.400]   I think it's a good thing.
[00:23:53.400 --> 00:23:54.400]   I think it's a good thing.
[00:23:54.400 --> 00:23:55.400]   I think it's a good thing.
[00:23:55.400 --> 00:23:56.400]   I think it's a good thing.
[00:23:56.400 --> 00:23:57.400]   I think it's a good thing.
[00:23:57.400 --> 00:23:58.400]   I think it's a good thing.
[00:23:58.400 --> 00:23:59.400]   I think it's a good thing.
[00:23:59.400 --> 00:24:00.400]   I think it's a good thing.
[00:24:00.400 --> 00:24:01.400]   I think it's a good thing.
[00:24:01.400 --> 00:24:02.400]   I think it's a good thing.
[00:24:02.400 --> 00:24:03.400]   I think it's a good thing.
[00:24:03.400 --> 00:24:04.400]   I think it's a good thing.
[00:24:04.400 --> 00:24:05.400]   I think it's a good thing.
[00:24:05.400 --> 00:24:06.400]   I think it's a good thing.
[00:24:06.400 --> 00:24:07.400]   I think it's a good thing.
[00:24:07.400 --> 00:24:08.400]   I think it's a good thing.
[00:24:08.400 --> 00:24:09.400]   I think it's a good thing.
[00:24:09.400 --> 00:24:10.400]   I think it's a good thing.
[00:24:10.400 --> 00:24:11.400]   I think it's a good thing.
[00:24:11.400 --> 00:24:12.400]   I think it's a good thing.
[00:24:12.400 --> 00:24:13.400]   I think it's a good thing.
[00:24:13.400 --> 00:24:14.400]   I think it's a good thing.
[00:24:14.400 --> 00:24:15.400]   I think it's a good thing.
[00:24:15.400 --> 00:24:16.400]   I think it's a good thing.
[00:24:16.400 --> 00:24:17.400]   I think it's a good thing.
[00:24:17.400 --> 00:24:18.400]   I think it's a good thing.
[00:24:18.400 --> 00:24:19.400]   I think it's a good thing.
[00:24:19.400 --> 00:24:20.400]   I think it's a good thing.
[00:24:20.400 --> 00:24:21.400]   I think it's a good thing.
[00:24:21.400 --> 00:24:22.400]   I think it's a good thing.
[00:24:22.400 --> 00:24:23.400]   I think it's a good thing.
[00:24:23.400 --> 00:24:24.400]   I think it's a good thing.
[00:24:24.400 --> 00:24:25.400]   I think it's a good thing.
[00:24:25.400 --> 00:24:26.400]   I think it's a good thing.
[00:24:26.400 --> 00:24:27.400]   I think it's a good thing.
[00:24:27.400 --> 00:24:28.400]   I think it's a good thing.
[00:24:28.400 --> 00:24:29.400]   I think it's a good thing.
[00:24:29.400 --> 00:24:30.400]   I think it's a good thing.
[00:24:30.400 --> 00:24:31.400]   I think it's a good thing.
[00:24:31.400 --> 00:24:32.400]   I think it's a good thing.
[00:24:32.400 --> 00:24:33.400]   I think it's a good thing.
[00:24:33.400 --> 00:24:34.400]   I think it's a good thing.
[00:24:34.400 --> 00:24:35.400]   I think it's a good thing.
[00:24:35.400 --> 00:24:36.400]   I think it's a good thing.
[00:24:36.400 --> 00:24:37.400]   I think it's a good thing.
[00:24:37.400 --> 00:24:38.400]   I think it's a good thing.
[00:24:38.400 --> 00:24:39.400]   I think it's a good thing.
[00:24:39.400 --> 00:24:40.400]   I think it's a good thing.
[00:24:40.400 --> 00:24:41.400]   I think it's a good thing.
[00:24:41.400 --> 00:24:42.400]   I think it's a good thing.
[00:24:42.400 --> 00:24:43.400]   I think it's a good thing.
[00:24:43.400 --> 00:24:44.400]   I think it's a good thing.
[00:24:44.400 --> 00:24:45.400]   I think it's a good thing.
[00:24:45.400 --> 00:24:46.400]   I think it's a good thing.
[00:24:46.400 --> 00:24:47.400]   I think it's a good thing.
[00:24:47.400 --> 00:24:48.400]   I think it's a good thing.
[00:24:48.400 --> 00:24:49.400]   I think it's a good thing.
[00:24:49.400 --> 00:24:50.400]   I think it's a good thing.
[00:24:50.400 --> 00:24:51.400]   I think it's a good thing.
[00:24:51.400 --> 00:24:52.400]   I think it's a good thing.
[00:24:52.400 --> 00:24:53.400]   I think it's a good thing.
[00:24:53.400 --> 00:24:54.400]   I think it's a good thing.
[00:24:54.400 --> 00:24:55.400]   I think it's a good thing.
[00:24:55.400 --> 00:24:56.400]   I think it's a good thing.
[00:24:56.400 --> 00:24:57.400]   I think it's a good thing.
[00:24:57.400 --> 00:24:58.400]   I think it's a good thing.
[00:24:58.400 --> 00:24:59.400]   I think it's a good thing.
[00:24:59.400 --> 00:25:00.400]   I think it's a good thing.
[00:25:00.400 --> 00:25:01.400]   I think it's a good thing.
[00:25:01.400 --> 00:25:02.400]   I think it's a good thing.
[00:25:02.400 --> 00:25:03.400]   I think it's a good thing.
[00:25:03.400 --> 00:25:04.400]   I think it's a good thing.
[00:25:04.400 --> 00:25:05.400]   I think it's a good thing.
[00:25:05.400 --> 00:25:06.400]   I think it's a good thing.
[00:25:06.400 --> 00:25:07.400]   I think it's a good thing.
[00:25:07.400 --> 00:25:08.400]   I think it's a good thing.
[00:25:08.400 --> 00:25:09.400]   I think it's a good thing.
[00:25:09.400 --> 00:25:10.400]   I think it's a good thing.
[00:25:10.400 --> 00:25:11.400]   I think it's a good thing.
[00:25:11.400 --> 00:25:12.400]   I think it's a good thing.
[00:25:12.400 --> 00:25:13.400]   I think it's a good thing.
[00:25:13.400 --> 00:25:14.400]   I think it's a good thing.
[00:25:14.400 --> 00:25:15.400]   I think it's a good thing.
[00:25:15.400 --> 00:25:16.400]   I think it's a good thing.
[00:25:16.400 --> 00:25:17.400]   I think it's a good thing.
[00:25:17.400 --> 00:25:18.400]   I think it's a good thing.
[00:25:18.400 --> 00:25:19.400]   I think it's a good thing.
[00:25:20.400 --> 00:25:21.400]   I think it's a good thing.
[00:25:21.400 --> 00:25:22.400]   I think it's a good thing.
[00:25:22.400 --> 00:25:23.400]   I think it's a good thing.
[00:25:23.400 --> 00:25:24.400]   I think it's a good thing.
[00:25:24.400 --> 00:25:25.400]   I think it's a good thing.
[00:25:25.400 --> 00:25:26.400]   I think it's a good thing.
[00:25:26.400 --> 00:25:27.400]   I think it's a good thing.
[00:25:27.400 --> 00:25:28.400]   I think it's a good thing.
[00:25:28.400 --> 00:25:29.400]   I think it's a good thing.
[00:25:29.400 --> 00:25:30.400]   I think it's a good thing.
[00:25:30.400 --> 00:25:31.400]   I think it's a good thing.
[00:25:31.400 --> 00:25:32.400]   I think it's a good thing.
[00:25:32.400 --> 00:25:33.400]   I think it's a good thing.
[00:25:33.400 --> 00:25:34.400]   I think it's a good thing.
[00:25:34.400 --> 00:25:35.400]   I think it's a good thing.
[00:25:35.400 --> 00:25:36.400]   I think it's a good thing.
[00:25:36.400 --> 00:25:37.400]   I think it's a good thing.
[00:25:37.400 --> 00:25:38.400]   I think it's a good thing.
[00:25:38.400 --> 00:25:39.400]   I think it's a good thing.
[00:25:39.400 --> 00:25:40.400]   I think it's a good thing.
[00:25:40.400 --> 00:25:41.400]   I think it's a good thing.
[00:25:41.400 --> 00:25:42.400]   I think it's a good thing.
[00:25:43.400 --> 00:25:44.400]   I think it's a good thing.
[00:25:44.400 --> 00:25:45.400]   I think it's a good thing.
[00:25:45.400 --> 00:25:46.400]   I think it's a good thing.
[00:25:46.400 --> 00:25:47.400]   I think it's a good thing.
[00:25:47.400 --> 00:25:48.400]   I think it's a good thing.
[00:25:48.400 --> 00:25:49.400]   I think it's a good thing.
[00:25:49.400 --> 00:25:50.400]   I think it's a good thing.
[00:25:50.400 --> 00:25:51.400]   I think it's a good thing.
[00:25:51.400 --> 00:25:52.400]   I think it's a good thing.
[00:25:52.400 --> 00:25:53.400]   I think it's a good thing.
[00:25:53.400 --> 00:25:54.400]   I think it's a good thing.
[00:25:54.400 --> 00:25:55.400]   I think it's a good thing.
[00:25:55.400 --> 00:25:56.400]   I think it's a good thing.
[00:25:56.400 --> 00:25:57.400]   I think it's a good thing.
[00:25:57.400 --> 00:25:58.400]   I think it's a good thing.
[00:25:58.400 --> 00:25:59.400]   I think it's a good thing.
[00:25:59.400 --> 00:26:00.400]   I think it's a good thing.
[00:26:00.400 --> 00:26:01.400]   I think it's a good thing.
[00:26:01.400 --> 00:26:02.400]   I think it's a good thing.
[00:26:02.400 --> 00:26:03.400]   I think it's a good thing.
[00:26:03.400 --> 00:26:04.400]   I think it's a good thing.
[00:26:04.400 --> 00:26:05.400]   I think it's a good thing.
[00:26:06.400 --> 00:26:07.400]   I think it's a good thing.
[00:26:07.400 --> 00:26:08.400]   I think it's a good thing.
[00:26:08.400 --> 00:26:09.400]   I think it's a good thing.
[00:26:09.400 --> 00:26:10.400]   I think it's a good thing.
[00:26:10.400 --> 00:26:11.400]   I think it's a good thing.
[00:26:11.400 --> 00:26:12.400]   I think it's a good thing.
[00:26:12.400 --> 00:26:13.400]   I think it's a good thing.
[00:26:13.400 --> 00:26:14.400]   I think it's a good thing.
[00:26:14.400 --> 00:26:15.400]   I think it's a good thing.
[00:26:15.400 --> 00:26:16.400]   I think it's a good thing.
[00:26:16.400 --> 00:26:17.400]   I think it's a good thing.
[00:26:17.400 --> 00:26:18.400]   I think it's a good thing.
[00:26:18.400 --> 00:26:19.400]   I think it's a good thing.
[00:26:19.400 --> 00:26:20.400]   I think it's a good thing.
[00:26:20.400 --> 00:26:21.400]   I think it's a good thing.
[00:26:21.400 --> 00:26:22.400]   I think it's a good thing.
[00:26:22.400 --> 00:26:23.400]   I think it's a good thing.
[00:26:23.400 --> 00:26:24.400]   I think it's a good thing.
[00:26:24.400 --> 00:26:25.400]   I think it's a good thing.
[00:26:25.400 --> 00:26:26.400]   I think it's a good thing.
[00:26:26.400 --> 00:26:27.400]   I think it's a good thing.
[00:26:27.400 --> 00:26:28.400]   I think it's a good thing.
[00:26:28.400 --> 00:26:29.400]   I think it's a good thing.
[00:26:29.400 --> 00:26:30.400]   I think it's a good thing.
[00:26:30.400 --> 00:26:31.400]   I think it's a good thing.
[00:26:31.400 --> 00:26:32.400]   I think it's a good thing.
[00:26:32.400 --> 00:26:33.400]   I think it's a good thing.
[00:26:33.400 --> 00:26:34.400]   I think it's a good thing.
[00:26:34.400 --> 00:26:35.400]   I think it's a good thing.
[00:26:35.400 --> 00:26:36.400]   I think it's a good thing.
[00:26:36.400 --> 00:26:37.400]   I think it's a good thing.
[00:26:37.400 --> 00:26:38.400]   I think it's a good thing.
[00:26:38.400 --> 00:26:39.400]   I think it's a good thing.
[00:26:39.400 --> 00:26:40.400]   I think it's a good thing.
[00:26:40.400 --> 00:26:41.400]   I think it's a good thing.
[00:26:41.400 --> 00:26:42.400]   I think it's a good thing.
[00:26:42.400 --> 00:26:43.400]   I think it's a good thing.
[00:26:43.400 --> 00:26:44.400]   I think it's a good thing.
[00:26:44.400 --> 00:26:45.400]   I think it's a good thing.
[00:26:45.400 --> 00:26:46.400]   I think it's a good thing.
[00:26:46.400 --> 00:26:47.400]   I think it's a good thing.
[00:26:47.400 --> 00:26:48.400]   I think it's a good thing.
[00:26:48.400 --> 00:26:49.400]   I think it's a good thing.
[00:26:49.400 --> 00:26:50.400]   I think it's a good thing.
[00:26:50.400 --> 00:26:52.400]   I think it's a good thing.
[00:26:52.400 --> 00:26:53.400]   I think it's a good thing.
[00:26:53.400 --> 00:26:54.400]   I think it's a good thing.
[00:26:54.400 --> 00:26:55.400]   I think it's a good thing.
[00:26:55.400 --> 00:26:56.400]   I think it's a good thing.
[00:26:56.400 --> 00:26:57.400]   I think it's a good thing.
[00:26:57.400 --> 00:26:58.400]   I think it's a good thing.
[00:26:58.400 --> 00:26:59.400]   I think it's a good thing.
[00:26:59.400 --> 00:27:00.400]   I think it's a good thing.
[00:27:00.400 --> 00:27:01.400]   I think it's a good thing.
[00:27:01.400 --> 00:27:02.400]   I think it's a good thing.
[00:27:02.400 --> 00:27:03.400]   I think it's a good thing.
[00:27:03.400 --> 00:27:04.400]   I think it's a good thing.
[00:27:04.400 --> 00:27:05.400]   I think it's a good thing.
[00:27:05.400 --> 00:27:06.400]   I think it's a good thing.
[00:27:06.400 --> 00:27:07.400]   I think it's a good thing.
[00:27:07.400 --> 00:27:08.400]   I think it's a good thing.
[00:27:08.400 --> 00:27:09.400]   I think it's a good thing.
[00:27:09.400 --> 00:27:10.400]   I think it's a good thing.
[00:27:10.400 --> 00:27:11.400]   I think it's a good thing.
[00:27:11.400 --> 00:27:12.400]   I think it's a good thing.
[00:27:12.400 --> 00:27:13.400]   I think it's a good thing.
[00:27:13.400 --> 00:27:14.400]   I think it's a good thing.
[00:27:14.400 --> 00:27:15.400]   I think it's a good thing.
[00:27:15.400 --> 00:27:16.400]   I think it's a good thing.
[00:27:16.400 --> 00:27:17.400]   I think it's a good thing.
[00:27:17.400 --> 00:27:18.400]   I think it's a good thing.
[00:27:18.400 --> 00:27:19.400]   I think it's a good thing.
[00:27:19.400 --> 00:27:20.400]   I think it's a good thing.
[00:27:20.400 --> 00:27:21.400]   I think it's a good thing.
[00:27:21.400 --> 00:27:22.400]   I think it's a good thing.
[00:27:22.400 --> 00:27:23.400]   I think it's a good thing.
[00:27:23.400 --> 00:27:24.400]   I think it's a good thing.
[00:27:24.400 --> 00:27:25.400]   I think it's a good thing.
[00:27:25.400 --> 00:27:26.400]   I think it's a good thing.
[00:27:26.400 --> 00:27:27.400]   I think it's a good thing.
[00:27:27.400 --> 00:27:28.400]   I think it's a good thing.
[00:27:28.400 --> 00:27:29.400]   I think it's a good thing.
[00:27:29.400 --> 00:27:30.400]   I think it's a good thing.
[00:27:30.400 --> 00:27:31.400]   I think it's a good thing.
[00:27:31.400 --> 00:27:32.400]   I think it's a good thing.
[00:27:32.400 --> 00:27:33.400]   I think it's a good thing.
[00:27:33.400 --> 00:27:34.400]   I think it's a good thing.
[00:27:34.400 --> 00:27:35.400]   I think it's a good thing.
[00:27:35.400 --> 00:27:36.400]   I think it's a good thing.
[00:27:36.400 --> 00:27:37.400]   I think it's a good thing.
[00:27:37.400 --> 00:27:38.400]   I think it's a good thing.
[00:27:38.400 --> 00:27:39.400]   I think it's a good thing.
[00:27:39.400 --> 00:27:40.400]   I think it's a good thing.
[00:27:40.400 --> 00:27:41.400]   I think it's a good thing.
[00:27:41.400 --> 00:27:42.400]   I think it's a good thing.
[00:27:42.400 --> 00:27:43.400]   I think it's a good thing.
[00:27:43.400 --> 00:27:44.400]   I think it's a good thing.
[00:27:44.400 --> 00:27:45.400]   I think it's a good thing.
[00:27:45.400 --> 00:27:46.400]   I think it's a good thing.
[00:27:46.400 --> 00:27:47.400]   I think it's a good thing.
[00:27:47.400 --> 00:27:48.400]   I think it's a good thing.
[00:27:48.400 --> 00:27:49.400]   I think it's a good thing.
[00:27:49.400 --> 00:27:50.400]   I think it's a good thing.
[00:27:50.400 --> 00:27:51.400]   I think it's a good thing.
[00:27:51.400 --> 00:27:52.400]   I think it's a good thing.
[00:27:52.400 --> 00:27:53.400]   I think it's a good thing.
[00:27:53.400 --> 00:27:54.400]   I think it's a good thing.
[00:27:54.400 --> 00:27:55.400]   I think it's a good thing.
[00:27:55.400 --> 00:27:56.400]   I think it's a good thing.
[00:27:56.400 --> 00:27:57.400]   I think it's a good thing.
[00:27:57.400 --> 00:27:58.400]   I think it's a good thing.
[00:27:58.400 --> 00:27:59.400]   I think it's a good thing.
[00:27:59.400 --> 00:28:00.400]   I think it's a good thing.
[00:28:00.400 --> 00:28:01.400]   I think it's a good thing.
[00:28:01.400 --> 00:28:02.400]   I think it's a good thing.
[00:28:02.400 --> 00:28:03.400]   I think it's a good thing.
[00:28:03.400 --> 00:28:04.400]   I think it's a good thing.
[00:28:04.400 --> 00:28:05.400]   I think it's a good thing.
[00:28:05.400 --> 00:28:06.400]   I think it's a good thing.
[00:28:06.400 --> 00:28:07.400]   I think it's a good thing.
[00:28:07.400 --> 00:28:08.400]   I think it's a good thing.
[00:28:08.400 --> 00:28:09.400]   I think it's a good thing.
[00:28:09.400 --> 00:28:10.400]   I think it's a good thing.
[00:28:10.400 --> 00:28:11.400]   I think it's a good thing.
[00:28:11.400 --> 00:28:12.400]   I think it's a good thing.
[00:28:12.400 --> 00:28:13.400]   I think it's a good thing.
[00:28:13.400 --> 00:28:14.400]   I think it's a good thing.
[00:28:14.400 --> 00:28:15.400]   I think it's a good thing.
[00:28:15.400 --> 00:28:16.400]   I think it's a good thing.
[00:28:16.400 --> 00:28:17.400]   I think it's a good thing.
[00:28:17.400 --> 00:28:18.400]   I think it's a good thing.
[00:28:18.400 --> 00:28:19.400]   I think it's a good thing.
[00:28:19.400 --> 00:28:20.400]   I think it's a good thing.
[00:28:20.400 --> 00:28:21.400]   I think it's a good thing.
[00:28:21.400 --> 00:28:22.400]   I think it's a good thing.
[00:28:22.400 --> 00:28:23.400]   I think it's a good thing.
[00:28:23.400 --> 00:28:24.400]   I think it's a good thing.
[00:28:24.400 --> 00:28:25.400]   I think it's a good thing.
[00:28:25.400 --> 00:28:26.400]   I think it's a good thing.
[00:28:26.400 --> 00:28:27.400]   I think it's a good thing.
[00:28:27.400 --> 00:28:28.400]   I think it's a good thing.
[00:28:28.400 --> 00:28:29.400]   I think it's a good thing.
[00:28:29.400 --> 00:28:30.400]   I think it's a good thing.
[00:28:30.400 --> 00:28:31.400]   I think it's a good thing.
[00:28:31.400 --> 00:28:32.400]   I think it's a good thing.
[00:28:32.400 --> 00:28:33.400]   I think it's a good thing.
[00:28:33.400 --> 00:28:34.400]   I think it's a good thing.
[00:28:34.400 --> 00:28:35.400]   I think it's a good thing.
[00:28:35.400 --> 00:28:36.400]   I think it's a good thing.
[00:28:36.400 --> 00:28:37.400]   I think it's a good thing.
[00:28:37.400 --> 00:28:38.400]   I think it's a good thing.
[00:28:38.400 --> 00:28:39.400]   I think it's a good thing.
[00:28:39.400 --> 00:28:40.400]   I think it's a good thing.
[00:28:40.400 --> 00:28:41.400]   I think it's a good thing.
[00:28:41.400 --> 00:28:42.400]   I think it's a good thing.
[00:28:42.400 --> 00:28:43.400]   I think it's a good thing.
[00:28:43.400 --> 00:28:44.400]   I think it's a good thing.
[00:28:44.400 --> 00:28:45.400]   I think it's a good thing.
[00:28:45.400 --> 00:28:46.400]   I think it's a good thing.
[00:28:46.400 --> 00:28:47.400]   I think it's a good thing.
[00:28:47.400 --> 00:28:48.400]   I think it's a good thing.
[00:28:48.400 --> 00:28:49.400]   I think it's a good thing.
[00:28:49.400 --> 00:28:50.400]   I think it's a good thing.
[00:28:50.400 --> 00:28:51.400]   I think it's a good thing.
[00:28:51.400 --> 00:28:52.400]   I think it's a good thing.
[00:28:52.400 --> 00:28:53.400]   I think it's a good thing.
[00:28:53.400 --> 00:28:54.400]   I think it's a good thing.
[00:28:54.400 --> 00:28:55.400]   I think it's a good thing.
[00:28:55.400 --> 00:28:56.400]   I think it's a good thing.
[00:28:56.400 --> 00:28:57.400]   I think it's a good thing.
[00:28:57.400 --> 00:28:58.400]   I think it's a good thing.
[00:28:58.400 --> 00:28:59.400]   I think it's a good thing.
[00:28:59.400 --> 00:29:00.400]   I think it's a good thing.
[00:29:00.400 --> 00:29:01.400]   I think it's a good thing.
[00:29:01.400 --> 00:29:02.400]   I think it's a good thing.
[00:29:02.400 --> 00:29:03.400]   I think it's a good thing.
[00:29:03.400 --> 00:29:04.400]   I think it's a good thing.
[00:29:04.400 --> 00:29:05.400]   I think it's a good thing.
[00:29:05.400 --> 00:29:06.400]   I think it's a good thing.
[00:29:06.400 --> 00:29:07.400]   I think it's a good thing.
[00:29:07.400 --> 00:29:08.400]   I think it's a good thing.
[00:29:08.400 --> 00:29:09.400]   I think it's a good thing.
[00:29:09.400 --> 00:29:10.400]   I think it's a good thing.
[00:29:10.400 --> 00:29:11.400]   I think it's a good thing.
[00:29:11.400 --> 00:29:12.400]   I think it's a good thing.
[00:29:12.400 --> 00:29:13.400]   I think it's a good thing.
[00:29:13.400 --> 00:29:14.400]   I think it's a good thing.
[00:29:14.400 --> 00:29:15.400]   I think it's a good thing.
[00:29:15.400 --> 00:29:16.400]   I think it's a good thing.
[00:29:16.400 --> 00:29:17.400]   I think it's a good thing.
[00:29:17.400 --> 00:29:18.400]   I think it's a good thing.
[00:29:18.400 --> 00:29:19.400]   I think it's a good thing.
[00:29:19.400 --> 00:29:20.400]   I think it's a good thing.
[00:29:20.400 --> 00:29:21.400]   I think it's a good thing.
[00:29:21.400 --> 00:29:22.400]   I think it's a good thing.
[00:29:22.400 --> 00:29:23.400]   I think it's a good thing.
[00:29:23.400 --> 00:29:24.400]   I think it's a good thing.
[00:29:24.400 --> 00:29:25.400]   I think it's a good thing.
[00:29:25.400 --> 00:29:27.400]   I think it's a good thing.
[00:29:27.400 --> 00:29:28.400]   I think it's a good thing.
[00:29:28.400 --> 00:29:29.400]   I think it's a good thing.
[00:29:29.400 --> 00:29:30.400]   I think it's a good thing.
[00:29:30.400 --> 00:29:31.400]   I think it's a good thing.
[00:29:31.400 --> 00:29:32.400]   I think it's a good thing.
[00:29:32.400 --> 00:29:33.400]   I think it's a good thing.
[00:29:33.400 --> 00:29:34.400]   I think it's a good thing.
[00:29:34.400 --> 00:29:35.400]   I think it's a good thing.
[00:29:35.400 --> 00:29:36.400]   I think it's a good thing.
[00:29:36.400 --> 00:29:37.400]   I think it's a good thing.
[00:29:37.400 --> 00:29:38.400]   I think it's a good thing.
[00:29:38.400 --> 00:29:39.400]   I think it's a good thing.
[00:29:39.400 --> 00:29:40.400]   I think it's a good thing.
[00:29:40.400 --> 00:29:41.400]   I think it's a good thing.
[00:29:41.400 --> 00:29:42.400]   I think it's a good thing.
[00:29:42.400 --> 00:29:43.400]   I think it's a good thing.
[00:29:43.400 --> 00:29:44.400]   I think it's a good thing.
[00:29:44.400 --> 00:29:45.400]   I think it's a good thing.
[00:29:45.400 --> 00:29:46.400]   I think it's a good thing.
[00:29:46.400 --> 00:29:47.400]   I think it's a good thing.
[00:29:47.400 --> 00:29:48.400]   I think it's a good thing.
[00:29:48.400 --> 00:29:49.400]   I think it's a good thing.
[00:29:49.400 --> 00:29:50.400]   I think it's a good thing.
[00:29:50.400 --> 00:29:51.400]   I think it's a good thing.
[00:29:51.400 --> 00:29:52.400]   I think it's a good thing.
[00:29:52.400 --> 00:29:53.400]   I think it's a good thing.
[00:29:53.400 --> 00:29:54.400]   I think it's a good thing.
[00:29:54.400 --> 00:29:55.400]   I think it's a good thing.
[00:29:55.400 --> 00:29:56.400]   I think it's a good thing.
[00:29:56.400 --> 00:29:57.400]   I think it's a good thing.
[00:29:57.400 --> 00:29:58.400]   I think it's a good thing.
[00:29:58.400 --> 00:29:59.400]   I think it's a good thing.
[00:29:59.400 --> 00:30:00.400]   I think it's a good thing.
[00:30:00.400 --> 00:30:01.400]   I think it's a good thing.
[00:30:01.400 --> 00:30:02.400]   I think it's a good thing.
[00:30:02.400 --> 00:30:03.400]   I think it's a good thing.
[00:30:03.400 --> 00:30:04.400]   I think it's a good thing.
[00:30:04.400 --> 00:30:05.400]   I think it's a good thing.
[00:30:05.400 --> 00:30:06.400]   I think it's a good thing.
[00:30:06.400 --> 00:30:07.400]   I think it's a good thing.
[00:30:07.400 --> 00:30:08.400]   I think it's a good thing.
[00:30:08.400 --> 00:30:09.400]   I think it's a good thing.
[00:30:09.400 --> 00:30:10.400]   I think it's a good thing.
[00:30:10.400 --> 00:30:11.400]   I think it's a good thing.
[00:30:11.400 --> 00:30:12.400]   I think it's a good thing.
[00:30:12.400 --> 00:30:13.400]   I think it's a good thing.
[00:30:13.400 --> 00:30:14.400]   I think it's a good thing.
[00:30:14.400 --> 00:30:15.400]   I think it's a good thing.
[00:30:15.400 --> 00:30:16.400]   I think it's a good thing.
[00:30:16.400 --> 00:30:17.400]   I think it's a good thing.
[00:30:17.400 --> 00:30:18.400]   I think it's a good thing.
[00:30:18.400 --> 00:30:19.400]   I think it's a good thing.
[00:30:19.400 --> 00:30:20.400]   I think it's a good thing.
[00:30:20.400 --> 00:30:21.400]   I think it's a good thing.
[00:30:21.400 --> 00:30:22.400]   I think it's a good thing.
[00:30:22.400 --> 00:30:23.400]   I think it's a good thing.
[00:30:23.400 --> 00:30:24.400]   I think it's a good thing.
[00:30:24.400 --> 00:30:25.400]   I think it's a good thing.
[00:30:25.400 --> 00:30:26.400]   I think it's a good thing.
[00:30:26.400 --> 00:30:27.400]   I think it's a good thing.
[00:30:27.400 --> 00:30:28.400]   I think it's a good thing.
[00:30:28.400 --> 00:30:29.400]   I think it's a good thing.
[00:30:29.400 --> 00:30:30.400]   I think it's a good thing.
[00:30:30.400 --> 00:30:31.400]   I think it's a good thing.
[00:30:31.400 --> 00:30:32.400]   I think it's a good thing.
[00:30:32.400 --> 00:30:33.400]   I think it's a good thing.
[00:30:33.400 --> 00:30:34.400]   I think it's a good thing.
[00:30:34.400 --> 00:30:35.400]   I think it's a good thing.
[00:30:35.400 --> 00:30:36.400]   I think it's a good thing.
[00:30:36.400 --> 00:30:37.400]   I think it's a good thing.
[00:30:37.400 --> 00:30:38.400]   I think it's a good thing.
[00:30:38.400 --> 00:30:39.400]   I think it's a good thing.
[00:30:39.400 --> 00:30:40.400]   I think it's a good thing.
[00:30:40.400 --> 00:30:41.400]   I think it's a good thing.
[00:30:41.400 --> 00:30:42.400]   I think it's a good thing.
[00:30:42.400 --> 00:30:43.400]   I think it's a good thing.
[00:30:43.400 --> 00:30:44.400]   I think it's a good thing.
[00:30:44.400 --> 00:30:45.400]   I think it's a good thing.
[00:30:45.400 --> 00:30:46.400]   I think it's a good thing.
[00:30:46.400 --> 00:30:47.400]   I think it's a good thing.
[00:30:47.400 --> 00:30:48.400]   I think it's a good thing.
[00:30:48.400 --> 00:30:49.400]   I think it's a good thing.
[00:30:49.400 --> 00:30:50.400]   I think it's a good thing.
[00:30:50.400 --> 00:30:51.400]   I think it's a good thing.
[00:30:51.400 --> 00:30:52.400]   I think it's a good thing.
[00:30:52.400 --> 00:30:53.400]   I think it's a good thing.
[00:30:53.400 --> 00:30:54.400]   I think it's a good thing.
[00:30:54.400 --> 00:30:55.400]   I think it's a good thing.
[00:30:55.400 --> 00:30:56.400]   I think it's a good thing.
[00:30:56.400 --> 00:30:57.400]   I think it's a good thing.
[00:30:57.400 --> 00:30:58.400]   I think it's a good thing.
[00:30:58.400 --> 00:30:59.400]   I think it's a good thing.
[00:30:59.400 --> 00:31:00.400]   I think it's a good thing.
[00:31:00.400 --> 00:31:01.400]   I think it's a good thing.
[00:31:01.400 --> 00:31:02.400]   I think it's a good thing.
[00:31:02.400 --> 00:31:03.400]   I think it's a good thing.
[00:31:03.400 --> 00:31:04.400]   I think it's a good thing.
[00:31:04.400 --> 00:31:05.400]   I think it's a good thing.
[00:31:05.400 --> 00:31:06.400]   I think it's a good thing.
[00:31:06.400 --> 00:31:07.400]   I think it's a good thing.
[00:31:07.400 --> 00:31:08.400]   I think it's a good thing.
[00:31:08.400 --> 00:31:09.400]   I think it's a good thing.
[00:31:09.400 --> 00:31:10.400]   I think it's a good thing.
[00:31:10.400 --> 00:31:11.400]   I think it's a good thing.
[00:31:11.400 --> 00:31:12.400]   I think it's a good thing.
[00:31:12.400 --> 00:31:13.400]   I think it's a good thing.
[00:31:13.400 --> 00:31:14.400]   I think it's a good thing.
[00:31:14.400 --> 00:31:15.400]   I think it's a good thing.
[00:31:15.400 --> 00:31:16.400]   I think it's a good thing.
[00:31:16.400 --> 00:31:17.720]   It doesn't know frisbee.
[00:31:17.720 --> 00:31:21.820]   It doesn't understand the concept of frisbee.
[00:31:21.820 --> 00:31:34.440]   It's not thinking and I think we owe it to the unwashed masses, normal people, to kind of really belabor at that point.
[00:31:34.440 --> 00:31:42.980]   The problem is we're also excited about this and it fits our science fiction, you know, fantasies that I think we've oversold it to people.
[00:31:43.020 --> 00:31:47.120]   And I think it's just what humans do, right?
[00:31:47.120 --> 00:31:53.280]   Yeah, it's normal that we enter primorifies, you know, we're like, they're thinking and we do it.
[00:31:53.280 --> 00:31:57.140]   We see faces everywhere and we see with the face, we treat it differently.
[00:31:57.140 --> 00:32:02.380]   You know, so humans are just wired to sort of see this behavior and then extrapolate from it and believe that.
[00:32:02.380 --> 00:32:03.780]   So we're just naturally flawed.
[00:32:03.780 --> 00:32:04.320]   You're right, though.
[00:32:04.320 --> 00:32:08.160]   I mean, we do need to get it out there, you know, that this definitely has limits.
[00:32:08.180 --> 00:32:14.080]   But like I said, it's being rolled out in so many different ways that it's really starting to make an impact.
[00:32:14.080 --> 00:32:19.080]   It's just gone from a buzzword and there's research and here's a new paper to like, oh, here's this product.
[00:32:19.080 --> 00:32:20.120]   Oh, here's another product.
[00:32:20.120 --> 00:32:23.360]   And I think that's why it's just getting so much attention, right?
[00:32:23.360 --> 00:32:27.140]   I mean, if you look at stable diffusion, looking at video, it was just that.
[00:32:27.140 --> 00:32:32.740]   I don't know if you guys saw the video going around with the Santas in the off playing in the office on the show.
[00:32:32.740 --> 00:32:35.720]   They replaced Michael's face with the Santas's face.
[00:32:35.740 --> 00:32:40.740]   And it's not 100%, but it looks really pretty good.
[00:32:40.740 --> 00:32:45.440]   And so, you know, how these kinds of things affect elections coming up and everything.
[00:32:45.440 --> 00:32:48.400]   It's going to be real, you know.
[00:32:48.400 --> 00:32:50.480]   So I think that's the threat.
[00:32:50.480 --> 00:32:55.740]   The idea though, that these things are thinking and all this, it's, you know, we stick out a long way to do what that happens.
[00:32:55.740 --> 00:33:00.540]   So this is, by the way, it was tweeted by Donald Trump Jr.
[00:33:00.540 --> 00:33:02.180]   So I don't know.
[00:33:02.180 --> 00:33:04.380]   Furthermore, I don't know.
[00:33:04.400 --> 00:33:06.320]   Oh, it looks it's pretty uncanny.
[00:33:06.320 --> 00:33:07.880]   I mean, it is Ron DeSantis's.
[00:33:07.880 --> 00:33:09.320]   Wow.
[00:33:09.320 --> 00:33:12.040]   That's, that's a deep fake.
[00:33:12.040 --> 00:33:14.160]   That's really, really good.
[00:33:14.160 --> 00:33:14.960]   Does it have his voice?
[00:33:14.960 --> 00:33:17.800]   I don't want to turn on the sound because I know I'll get taken down.
[00:33:17.800 --> 00:33:18.640]   Yeah.
[00:33:18.640 --> 00:33:19.640]   It's like it.
[00:33:19.640 --> 00:33:21.200]   It's his voice.
[00:33:21.200 --> 00:33:21.680]   That's all that.
[00:33:21.680 --> 00:33:23.080]   But just doing Michael's wines.
[00:33:23.080 --> 00:33:23.400]   Okay.
[00:33:23.400 --> 00:33:23.680]   Let me.
[00:33:23.680 --> 00:33:24.040]   Let me.
[00:33:24.040 --> 00:33:24.400]   Let me.
[00:33:24.400 --> 00:33:29.920]   Hey, look, hey, I'm, please tell there all that this is not a woman suit.
[00:33:29.920 --> 00:33:31.760]   Oh my God.
[00:33:31.760 --> 00:33:32.880]   That's a woman suit.
[00:33:32.880 --> 00:33:34.200]   You're wearing a woman suit.
[00:33:34.200 --> 00:33:34.960]   No, I do.
[00:33:34.960 --> 00:33:36.800]   I even, I wear men suits.
[00:33:36.800 --> 00:33:39.000]   It is Ron's voice.
[00:33:39.000 --> 00:33:40.320]   Yeah.
[00:33:40.320 --> 00:33:43.720]   Oh, we're in a deep, we're a deep fake heart here.
[00:33:43.720 --> 00:33:46.040]   This is, this is just the beginning.
[00:33:46.040 --> 00:33:46.720]   Wow.
[00:33:46.720 --> 00:33:48.200]   You know, this is two years.
[00:33:48.200 --> 00:33:49.680]   It's a fun part, though.
[00:33:49.680 --> 00:33:53.400]   That's, I think why I got so much attention is it is a great demo, right?
[00:33:53.400 --> 00:33:57.120]   And things that have a great demo, you want to show off and, and then people get bored of them.
[00:33:57.120 --> 00:33:57.760]   What was the thing?
[00:33:57.760 --> 00:34:00.680]   The, um, Gibjab, that thing that, uh, yeah.
[00:34:00.680 --> 00:34:00.760]   Yeah.
[00:34:00.760 --> 00:34:01.520]   Yeah.
[00:34:01.520 --> 00:34:01.800]   Yeah.
[00:34:01.800 --> 00:34:02.000]   Yeah.
[00:34:02.200 --> 00:34:06.040]   And they created some tools where you can make here on Gibjab, it was like fun for 12 seconds.
[00:34:06.040 --> 00:34:06.360]   Right.
[00:34:06.360 --> 00:34:09.800]   Because of very specific style or everyone makes, I subscribe.
[00:34:09.800 --> 00:34:10.680]   Pieces of themselves.
[00:34:10.680 --> 00:34:11.880]   Gibjab for a year.
[00:34:11.880 --> 00:34:13.440]   Use it for a week.
[00:34:13.440 --> 00:34:22.480]   It's, but it's, I, I think we're seeing the hype curve, you know, it started to fade on it, but it's, there are things like I'm working on a website right now.
[00:34:22.480 --> 00:34:24.480]   And I have not found a graphical tool.
[00:34:24.480 --> 00:34:26.280]   I like I'm so old.
[00:34:26.280 --> 00:34:27.760]   I remember good graphical tools.
[00:34:27.760 --> 00:34:28.760]   They don't exist anymore.
[00:34:29.040 --> 00:34:32.560]   And I've kind of forgotten a lot of my HTML, CSS and JavaScript.
[00:34:32.560 --> 00:34:36.760]   So I'm using chat GPT forum, like, Hey, I, you know, don't say I've forgotten.
[00:34:36.760 --> 00:34:37.960]   Hey, dear Mr.
[00:34:37.960 --> 00:34:38.640]   Ms.
[00:34:38.640 --> 00:34:40.360]   Java, uh, chat GGP.
[00:34:40.360 --> 00:34:43.600]   I say, you know, how do I make a box that is centered?
[00:34:43.600 --> 00:34:44.240]   How do I do this?
[00:34:44.240 --> 00:34:44.640]   Whatever.
[00:34:44.640 --> 00:34:45.480]   And it spits out.
[00:34:45.480 --> 00:34:47.040]   And this is like co-pilot, right?
[00:34:47.040 --> 00:34:49.160]   But I'm asking kind of very specific questions.
[00:34:49.160 --> 00:34:51.120]   I had a problem on a page.
[00:34:51.120 --> 00:34:52.960]   I described it natural language.
[00:34:52.960 --> 00:34:57.480]   And I said, what is the page move back and forth when I've got this with this CSS property?
[00:34:57.480 --> 00:35:00.680]   He said, Oh, you need to do set overflow property to whatever.
[00:35:00.680 --> 00:35:01.960]   And I was like, Oh my God, that's it.
[00:35:01.960 --> 00:35:06.400]   But I gave it this like paragraph long description of a problem.
[00:35:06.400 --> 00:35:09.240]   And again, it's a verifiable solution.
[00:35:09.240 --> 00:35:11.400]   So this is the thing I tested and it worked.
[00:35:11.400 --> 00:35:14.960]   I didn't say like I, you know, I'm a human affiliate and I'm bleeding.
[00:35:14.960 --> 00:35:15.560]   What do I do?
[00:35:15.560 --> 00:35:16.560]   That's probably not a good.
[00:35:16.560 --> 00:35:17.440]   Probably not a good idea.
[00:35:17.440 --> 00:35:19.160]   Some people's use of it.
[00:35:19.160 --> 00:35:20.120]   That's an example.
[00:35:20.120 --> 00:35:21.160]   I look at this video.
[00:35:21.160 --> 00:35:23.400]   I think we are heading into an election.
[00:35:24.160 --> 00:35:28.720]   This originally was created by C3P meme and posts on Twitter.
[00:35:28.720 --> 00:35:31.600]   We're heading into election where it is rightfully through them.
[00:35:31.600 --> 00:35:33.400]   The crazy and I grabbed one.
[00:35:33.400 --> 00:35:34.560]   This is going to be.
[00:35:34.560 --> 00:35:37.680]   And it fit a problem.
[00:35:37.680 --> 00:35:41.280]   I think this is really going to be a problem.
[00:35:41.280 --> 00:35:45.080]   Funny, but it's going to be difficult for people to tell the difference.
[00:35:45.080 --> 00:35:49.360]   I mean, I mean, I mean, think of think of someone in their 70s on Facebook.
[00:35:49.360 --> 00:35:50.640]   Watch it.
[00:35:50.640 --> 00:35:51.560]   Watch it, buddy.
[00:35:51.560 --> 00:35:53.440]   I'm just saying.
[00:35:53.440 --> 00:35:54.080]   Careful.
[00:35:54.080 --> 00:35:55.360]   You're not going to see the difference.
[00:35:55.360 --> 00:35:57.680]   For them, the uncanny valley doesn't exist.
[00:35:57.680 --> 00:35:58.400]   They're going to leave.
[00:35:58.400 --> 00:35:58.840]   No, you're right.
[00:35:58.840 --> 00:36:05.800]   And they're going to, you know, and if it's a serious thing, you know, an event that's happened, you know, there was a right.
[00:36:05.800 --> 00:36:06.800]   We'll get to that story, right?
[00:36:06.800 --> 00:36:10.480]   About the, it was that the image that went around the AI generated image of the Pentagon.
[00:36:10.480 --> 00:36:11.760]   Yeah, let's take a break.
[00:36:11.760 --> 00:36:12.440]   I want to come back.
[00:36:12.440 --> 00:36:19.400]   And I still want to really talk about what Microsoft announced because I think these are all really important things, but I need to take a little break.
[00:36:19.400 --> 00:36:20.600]   We've got a great panel.
[00:36:21.000 --> 00:36:26.960]   It's so nice to have you, Daniel covering the water, the build, but everything else.
[00:36:26.960 --> 00:36:28.360]   Glenn Fleischman.
[00:36:28.360 --> 00:36:30.240]   Owen, I see you.
[00:36:30.240 --> 00:36:37.840]   I see you, man, and your beautiful painting, which was smuggled into the United States illegally, but we won't mention.
[00:36:37.840 --> 00:36:39.040]   We won't mention that.
[00:36:39.040 --> 00:36:41.800]   There is a story there, though, right?
[00:36:41.800 --> 00:36:43.400]   He got it through customs.
[00:36:43.400 --> 00:36:44.240]   Oh, yeah.
[00:36:44.240 --> 00:36:48.480]   It was it was work documents.
[00:36:48.480 --> 00:36:50.800]   Why do you have to smuggle a painting in, though?
[00:36:50.800 --> 00:36:51.800]   I don't understand.
[00:36:51.800 --> 00:36:54.600]   Oh, I forgot why.
[00:36:54.600 --> 00:36:57.280]   You just say maybe you didn't want to clear its value, but.
[00:36:57.280 --> 00:36:59.200]   I don't know.
[00:36:59.200 --> 00:37:03.160]   I think it was just it was just it was just a painting, man.
[00:37:03.160 --> 00:37:07.200]   Basically, that was the that was the advice given at the time.
[00:37:07.200 --> 00:37:09.080]   Yeah, when the money was acquired.
[00:37:09.080 --> 00:37:09.720]   I know.
[00:37:09.720 --> 00:37:10.200]   I know.
[00:37:10.200 --> 00:37:17.000]   I know I almost brought a Spanish hamman after visit to Barcelona, but I was talked out of that.
[00:37:17.000 --> 00:37:17.920]   That's a good thing.
[00:37:18.520 --> 00:37:21.400]   Our show today brought to you by ACI learning.
[00:37:21.400 --> 00:37:22.680]   They sponsor our studio.
[00:37:22.680 --> 00:37:23.640]   We're big fans.
[00:37:23.640 --> 00:37:25.440]   You may say, well, who's ACI learning?
[00:37:25.440 --> 00:37:27.440]   I know you know the name IT pro.
[00:37:27.440 --> 00:37:30.640]   They've been with us since they started in 2013.
[00:37:30.640 --> 00:37:34.960]   IT pro is now part of ACI learning's family.
[00:37:34.960 --> 00:37:37.520]   I T pro's capabilities continue to grow.
[00:37:37.520 --> 00:37:39.160]   They have those great.
[00:37:39.160 --> 00:37:39.600]   Highly.
[00:37:39.600 --> 00:37:42.440]   There's their studio have been there in Gainesville.
[00:37:42.440 --> 00:37:47.920]   Highly entertaining, literally bingeable short format content
[00:37:48.200 --> 00:37:52.880]   that teaches you IT skills so you can get the certifications so that you can get the job.
[00:37:52.880 --> 00:37:57.640]   There's over 7,000 hours now in their library and they're creating new content
[00:37:57.640 --> 00:37:59.520]   every day, Monday through Friday.
[00:37:59.520 --> 00:38:05.440]   And by the way, great content for enhancing your existing skills.
[00:38:05.440 --> 00:38:10.880]   If you've got an IT team for building their skills, if you're an MSP, you must love it pro.
[00:38:10.880 --> 00:38:17.000]   We know it and ACI learning 30% of ACI learners are managed service providers.
[00:38:17.480 --> 00:38:23.400]   ACI learning is dedicated to supporting your MSP team through any challenges and MSP.
[00:38:23.400 --> 00:38:28.720]   You're selling the expertise of your team, their ability to solve problems for your clients.
[00:38:28.720 --> 00:38:32.320]   They will love they will love ACI learning.
[00:38:32.320 --> 00:38:37.040]   For instance, the ACI learning practice labs, a place you could test and experiment
[00:38:37.040 --> 00:38:42.000]   before deploying new apps or updates without compromising your live system.
[00:38:42.000 --> 00:38:43.840]   That's pretty cool.
[00:38:44.280 --> 00:38:47.480]   Try out your skills on virtual machine labs. You could just do it with a browser
[00:38:47.480 --> 00:38:51.080]   and be running Windows Server, desktop clients.
[00:38:51.080 --> 00:38:55.560]   You can run anywhere Mac OS, Linux, iOS, Windows.
[00:38:55.560 --> 00:39:00.040]   You can prepare for challenging certification examinations with practice exams.
[00:39:00.040 --> 00:39:02.640]   It's always best to take the test before you take it for real.
[00:39:02.640 --> 00:39:07.720]   You could take and retake tests to make sure you're ready to sit for the certification exam.
[00:39:07.720 --> 00:39:12.920]   One happy MSP team leaders told us, quote, "I had 110 engineers in the field."
[00:39:12.920 --> 00:39:16.600]   Wow, we had dozens of ITPro accounts last year alone.
[00:39:16.600 --> 00:39:20.440]   They passed over 40 certs. That's real success.
[00:39:20.440 --> 00:39:26.720]   If your IT training isn't raising your team to the level you need, you need ACI learning.
[00:39:26.720 --> 00:39:31.560]   And by the way, ACI learning's content is so bingeable, is so high quality.
[00:39:31.560 --> 00:39:37.040]   The industry average completion rate on these on videos from other companies around 30%,
[00:39:37.040 --> 00:39:39.880]   70% of people bail out with ITPro.
[00:39:40.800 --> 00:39:43.280]   It's an over 80% completion rate.
[00:39:43.280 --> 00:39:45.800]   Why? Because these are great. They're entertaining. They're fun.
[00:39:45.800 --> 00:39:49.000]   And you're really learning. Don't settle for subpar training.
[00:39:49.000 --> 00:39:51.840]   This is the format ITPro's want.
[00:39:51.840 --> 00:39:55.120]   And with their Pro Portal, you can assign courses.
[00:39:55.120 --> 00:39:58.360]   You can manage seats. You can unassign and assign team members.
[00:39:58.360 --> 00:40:02.320]   You can even create custom programs for your team.
[00:40:02.320 --> 00:40:08.160]   It's a great way to affect a much needed way to stay compliant with regulations,
[00:40:08.160 --> 00:40:11.920]   to identify potential risks and weaknesses before they're a problem,
[00:40:11.920 --> 00:40:15.120]   to future proof your business, to retain top talent.
[00:40:15.120 --> 00:40:17.360]   It's a great benefit for your IT team.
[00:40:17.360 --> 00:40:22.200]   Upskill that team, gain essential insights into training for individuals, teams and leaders.
[00:40:22.200 --> 00:40:28.560]   While other team training companies may not comply with regulatory requirements,
[00:40:28.560 --> 00:40:32.480]   you'll be glad to know ACI learning is ISO certified.
[00:40:32.480 --> 00:40:37.400]   So you are getting the world class training. Your team deserves.
[00:40:37.400 --> 00:40:40.760]   Try it for yourself. Then bring the whole team along for individuals.
[00:40:40.760 --> 00:40:47.680]   Use the code TWIT30 for 30% off on a standard or premium individual ITPro membership.
[00:40:47.680 --> 00:40:51.400]   You can even get volume discounts for teams from 2 to 1,000.
[00:40:51.400 --> 00:40:53.360]   The volume discounts start at five seats. Look,
[00:40:53.360 --> 00:40:57.240]   the best thing to do is learn more about ACI learning's premium training options
[00:40:57.240 --> 00:41:04.160]   across audit, across IT, across cybersecurity readiness at go.acilearning.com/twit.
[00:41:04.160 --> 00:41:09.160]   Fill out the form there. You can learn about a free two-week trial for training your entire team.
[00:41:09.160 --> 00:41:14.160]   Go.acilearning.com/twit.
[00:41:14.160 --> 00:41:20.160]   So, somebody asked, I should mention, unfortunately, we are not sponsored by Cognizant, by Aramco,
[00:41:20.160 --> 00:41:25.160]   by any of the companies on my shirt or my hat.
[00:41:25.160 --> 00:41:28.160]   The boss does not sponsor this show.
[00:41:28.160 --> 00:41:32.160]   I'm just celebrating a very nice race from my team.
[00:41:32.160 --> 00:41:38.160]   I'm a very nice race from my man, Fernando Alonso in a Formula One.
[00:41:38.160 --> 00:41:43.160]   You know, you wear a team jersey during football games.
[00:41:43.160 --> 00:41:46.160]   It's okay, right? I can wear a fire suit.
[00:41:46.160 --> 00:41:49.160]   If it gets spicy in here, I'm ready.
[00:41:49.160 --> 00:41:54.160]   All right, let's get back to AI.
[00:41:54.160 --> 00:41:58.160]   You were mentioning the picture.
[00:41:58.160 --> 00:42:06.160]   It's funny because this picture of the Pentagon on fire, which was obviously a fake.
[00:42:06.160 --> 00:42:09.160]   Pentagon was not on fire.
[00:42:09.160 --> 00:42:13.160]   The New York Times blamed AI for it.
[00:42:13.160 --> 00:42:17.160]   And I really think that was kind of a mistake.
[00:42:17.160 --> 00:42:21.160]   It really was Twitter that spread it.
[00:42:21.160 --> 00:42:23.160]   Sure.
[00:42:23.160 --> 00:42:25.160]   You could have done that in Photoshop.
[00:42:25.160 --> 00:42:32.160]   Was that an AI thing or a Twitter thing or a social media thing?
[00:42:32.160 --> 00:42:33.160]   Yeah, that's true.
[00:42:33.160 --> 00:42:36.160]   I mean, they probably did latch onto the AI for the buzz, but yeah, you're right.
[00:42:36.160 --> 00:42:38.160]   I mean, this is the danger of social networks, right?
[00:42:38.160 --> 00:42:45.160]   I mean, we saw that when they had the verified accounts and, you know, what was it?
[00:42:45.160 --> 00:42:49.160]   I was at one who was trying to be someone who was a pharmaceutical company, so they're going
[00:42:49.160 --> 00:42:50.160]   to give away insulin for free.
[00:42:50.160 --> 00:42:52.160]   Well, that's exactly what happened here.
[00:42:52.160 --> 00:42:58.160]   It was a fake Bloomberg account, right, that had the blue check, tweeted this picture of
[00:42:58.160 --> 00:43:05.160]   the Pentagon burning and the stock market plummeted as a result.
[00:43:05.160 --> 00:43:07.160]   You know, I think CNN got it right.
[00:43:07.160 --> 00:43:11.160]   Verified Twitter accounts share fake image of explosion causing confusion.
[00:43:11.160 --> 00:43:14.160]   Donnie O'Sullivan knows what he's talking about.
[00:43:14.160 --> 00:43:18.160]   He's a smart guy who was banned from Twitter for a while.
[00:43:18.160 --> 00:43:19.160]   AI generated.
[00:43:19.160 --> 00:43:20.160]   Yeah.
[00:43:20.160 --> 00:43:21.160]   I don't know what if you look at it.
[00:43:21.160 --> 00:43:23.040]   It's not a good again.
[00:43:23.040 --> 00:43:29.880]   We 70 year olds might not know that that fence is all mung up, but it's pretty obviously
[00:43:29.880 --> 00:43:33.400]   a fake, whether it was Photoshop or AI is kind of irrelevant.
[00:43:33.400 --> 00:43:39.320]   The real issue was it was tweeted out by an account that looked like it was Bloomberg
[00:43:39.320 --> 00:43:41.160]   with a blue check.
[00:43:41.160 --> 00:43:48.160]   I mean, the issue right with AI is that you could imagine AI being used to generate like
[00:43:48.160 --> 00:43:52.160]   10 different angles of the same scene.
[00:43:52.160 --> 00:43:57.160]   There's an efficiency gained over human Photoshop.
[00:43:57.160 --> 00:44:03.160]   But yeah, I think the information environment is more the issue here.
[00:44:03.160 --> 00:44:08.160]   RT Russia today was one of the accounts that shared it.
[00:44:08.160 --> 00:44:15.160]   By the way, the legitimate RT account because they loved it, right?
[00:44:15.160 --> 00:44:24.520]   The legitimate share of legitimate misinformation shared actual made up, made up misinformation.
[00:44:24.520 --> 00:44:31.000]   At stock market Dow fell 80 points in four minutes, but it fully recovered because it
[00:44:31.000 --> 00:44:36.840]   became pretty obvious it was a fake, right?
[00:44:36.840 --> 00:44:42.400]   It took the fire department from Arlington, Virginia to tweet that it and the Pentagon
[00:44:42.400 --> 00:44:48.760]   force protection agency said, "There is no explosion or incident taking place anywhere
[00:44:48.760 --> 00:44:53.480]   near the Pentagon, no hazards to the public."
[00:44:53.480 --> 00:44:55.600]   And then the stock market recovered.
[00:44:55.600 --> 00:44:59.200]   Yeah, to me, that's really more about blue checks.
[00:44:59.200 --> 00:45:03.440]   You know, that's something people have to learn too.
[00:45:03.440 --> 00:45:07.040]   The blue check on Twitter doesn't mean the same thing as it used to.
[00:45:07.040 --> 00:45:10.960]   Yeah, no, that's definitely that's a whole other thing.
[00:45:10.960 --> 00:45:14.800]   Like I would say with AI too, what's interesting about AI is of course you could generate these
[00:45:14.800 --> 00:45:18.960]   images, but you can also use AI to detect these images, right?
[00:45:18.960 --> 00:45:24.120]   Like there are ways to do that, to analyze the photos and videos.
[00:45:24.120 --> 00:45:30.280]   And what should happen is all these companies, Microsoft included, should be investing in
[00:45:30.280 --> 00:45:37.840]   technologies that do that as part and parcel of making these other technologies.
[00:45:37.840 --> 00:45:44.120]   Social networks need to do and maybe be legislated into doing is implementing these safeguards
[00:45:44.120 --> 00:45:50.200]   into the system so that they're automatically detected and flagged as AI generated.
[00:45:50.200 --> 00:45:52.320]   Like this is going to be possible.
[00:45:52.320 --> 00:45:56.320]   This is not out of the realm of, you know, like science fiction.
[00:45:56.320 --> 00:46:00.120]   But right now, I mean Elon Musk doesn't care about any of that.
[00:46:00.120 --> 00:46:05.000]   He has a meme on his Twitter account right now attributed to Voltaire.
[00:46:05.000 --> 00:46:08.600]   Oh yeah, it wasn't Voltaire as a white supremacist.
[00:46:08.600 --> 00:46:13.000]   Yeah, and all his followers are like, yeah, but the quotes good.
[00:46:13.000 --> 00:46:15.920]   It's like, oh, the quote's not good.
[00:46:15.920 --> 00:46:17.800]   The quote is so wrong.
[00:46:17.800 --> 00:46:22.200]   In fact, when I read it and I didn't know it wasn't Voltaire, I thought boy Voltaire
[00:46:22.200 --> 00:46:24.520]   really got that wrong.
[00:46:24.520 --> 00:46:27.840]   I have to rethink my whole idea of the father of enlightenment.
[00:46:27.840 --> 00:46:30.120]   I mean, the age of reason.
[00:46:30.120 --> 00:46:36.960]   Didn't Elon Musk buy Twitter to conquer the bots and it feels like the bots are conquering
[00:46:36.960 --> 00:46:37.960]   now?
[00:46:37.960 --> 00:46:43.040]   No, he bought Twitter to do exactly what he's doing, which is to shake, to share fake Voltaire
[00:46:43.040 --> 00:46:46.040]   quotes from neo-Nazis.
[00:46:46.040 --> 00:46:51.080]   The quote, by the way, is to learn who rules over you.
[00:46:51.080 --> 00:46:55.440]   Simply find out who you are not allowed to criticize.
[00:46:55.440 --> 00:46:56.440]   And you know, if you're not paying it.
[00:46:56.440 --> 00:46:57.440]   It's hilarious on Twitter.
[00:46:57.440 --> 00:46:58.440]   Yeah, right.
[00:46:58.440 --> 00:46:59.440]   Free speech.
[00:46:59.440 --> 00:47:00.440]   You're not allowed to criticize.
[00:47:00.440 --> 00:47:06.040]   Then of course, it turns out that it wasn't Voltaire, but it was a neo-Nazi that tweeted
[00:47:06.040 --> 00:47:07.040]   it.
[00:47:07.040 --> 00:47:12.280]   And actually the first reply was, well, well, I guess we can't criticize children with
[00:47:12.280 --> 00:47:14.080]   leukemia then.
[00:47:14.080 --> 00:47:17.720]   So that's obviously a specious quote.
[00:47:17.720 --> 00:47:20.800]   It doesn't make it's, you know, well, I think Voltaire would not have said that, to be
[00:47:20.800 --> 00:47:22.400]   honest with you.
[00:47:22.400 --> 00:47:23.400]   Yeah.
[00:47:23.400 --> 00:47:24.400]   Yeah.
[00:47:24.400 --> 00:47:28.480]   I think what was his was, I disagree with what you say, but I will fight to the death
[00:47:28.480 --> 00:47:29.480]   you're right to say it.
[00:47:29.480 --> 00:47:30.480]   There you go.
[00:47:30.480 --> 00:47:31.480]   That's free speech.
[00:47:31.480 --> 00:47:32.480]   Right.
[00:47:32.480 --> 00:47:33.480]   That's a little bit Voltaire.
[00:47:33.480 --> 00:47:34.480]   That's a lot.
[00:47:34.480 --> 00:47:40.800]   By the way, this comes on the heels of Twitter pulling out of the EU disinformation code.
[00:47:40.800 --> 00:47:42.920]   Elon has essentially said, no, we loved it.
[00:47:42.920 --> 00:47:43.920]   What?
[00:47:43.920 --> 00:47:44.920]   No, what's wrong?
[00:47:44.920 --> 00:47:45.920]   Nothing wrong with that.
[00:47:45.920 --> 00:47:47.400]   And it's a bit voluntarily too.
[00:47:47.400 --> 00:47:49.920]   It wasn't even like a commitment, that kind of agreement.
[00:47:49.920 --> 00:47:54.240]   Like you didn't want forced to do anything, which is saying we'll try to do stuff.
[00:47:54.240 --> 00:47:55.760]   That was too much.
[00:47:55.760 --> 00:48:03.680]   It doesn't matter because after August 25th, the EU's internal market commissioner said,
[00:48:03.680 --> 00:48:05.280]   it doesn't matter.
[00:48:05.280 --> 00:48:07.560]   We're going to enforce this law.
[00:48:07.560 --> 00:48:10.680]   So you're going to have to do something about disinformation anyway.
[00:48:10.680 --> 00:48:13.120]   We, well, we'll see.
[00:48:13.120 --> 00:48:14.680]   And that's the right approach.
[00:48:14.680 --> 00:48:18.080]   I don't know if you guys see, I put in the show notes, there was another funny AI thing
[00:48:18.080 --> 00:48:19.080]   that went around.
[00:48:19.080 --> 00:48:21.040]   I think it was last night with politicians cheating.
[00:48:21.040 --> 00:48:24.760]   So in generated a bunch of images of famous politicians.
[00:48:24.760 --> 00:48:25.760]   Oh God.
[00:48:25.760 --> 00:48:26.760]   Heating on their spouse case.
[00:48:26.760 --> 00:48:28.760]   I just want to tell you, this is fake.
[00:48:28.760 --> 00:48:31.600]   This threat is, this threat is something else.
[00:48:31.600 --> 00:48:33.120]   This is not real.
[00:48:33.120 --> 00:48:34.120]   Okay.
[00:48:34.120 --> 00:48:38.320]   I'm just saying, oh my God, this is not.
[00:48:38.320 --> 00:48:39.320]   And you know what?
[00:48:39.320 --> 00:48:41.120]   You can kind of tell these are not real.
[00:48:41.120 --> 00:48:42.120]   Right.
[00:48:42.120 --> 00:48:43.120]   Yeah.
[00:48:43.120 --> 00:48:44.120]   Right.
[00:48:44.120 --> 00:48:45.120]   Yeah.
[00:48:45.120 --> 00:48:46.120]   But you know, give it a couple more years.
[00:48:46.120 --> 00:48:47.120]   Well, you and I can tell.
[00:48:47.120 --> 00:48:48.120]   But they're pretty good.
[00:48:48.120 --> 00:48:50.400]   They're pretty, they're pressively awkward.
[00:48:50.400 --> 00:48:52.120]   There's something really awkward and awful.
[00:48:52.120 --> 00:48:54.280]   Bernie doesn't look like he's having fun, does he?
[00:48:54.280 --> 00:48:58.160]   No, it makes it seem, is Joe Biden cheating with his own wife?
[00:48:58.160 --> 00:48:59.560]   That's what it looks like.
[00:48:59.560 --> 00:49:00.560]   Really?
[00:49:00.560 --> 00:49:02.560]   Honestly, he doesn't remember.
[00:49:02.560 --> 00:49:03.560]   Okay.
[00:49:03.560 --> 00:49:05.640]   It's like, it's just, they didn't make it prettier.
[00:49:05.640 --> 00:49:07.160]   If it was prettier, you wouldn't believe it.
[00:49:07.160 --> 00:49:09.640]   It's so bizarre like.
[00:49:09.640 --> 00:49:10.640]   Skis.
[00:49:10.640 --> 00:49:11.640]   Yeah.
[00:49:11.640 --> 00:49:12.640]   Skis.
[00:49:12.640 --> 00:49:13.640]   Yeah.
[00:49:13.640 --> 00:49:17.160]   You know what is, what is actually very, very well executed?
[00:49:17.160 --> 00:49:20.400]   And I believe it, I believe they used AI for this.
[00:49:20.400 --> 00:49:23.400]   There's an account called Roo Publicans.
[00:49:23.400 --> 00:49:25.400]   And it's Republicans.
[00:49:25.400 --> 00:49:26.400]   Indra.
[00:49:26.400 --> 00:49:27.400]   It's drag queen.
[00:49:27.400 --> 00:49:28.400]   Oh, yeah.
[00:49:28.400 --> 00:49:31.960]   Just Google Roo, like Rupal Publicans.
[00:49:31.960 --> 00:49:33.880]   And it should pop right up.
[00:49:33.880 --> 00:49:36.040]   It's really excellently done.
[00:49:36.040 --> 00:49:38.720]   I will leave that as an exercise for the listener.
[00:49:38.720 --> 00:49:39.720]   How about?
[00:49:39.720 --> 00:49:40.720]   Yeah.
[00:49:40.720 --> 00:49:41.720]   Yeah.
[00:49:41.720 --> 00:49:42.720]   It's on Insta.
[00:49:42.720 --> 00:49:43.720]   Yeah.
[00:49:43.720 --> 00:49:44.720]   Actually, I might follow it.
[00:49:44.720 --> 00:49:46.560]   Just, it's a good thing to have around.
[00:49:46.560 --> 00:49:47.560]   Okay.
[00:49:47.560 --> 00:49:48.800]   It is pretty funny.
[00:49:48.800 --> 00:49:49.800]   Okay.
[00:49:49.800 --> 00:49:50.800]   I'm sorry.
[00:49:50.800 --> 00:49:51.800]   Don't, no, no.
[00:49:51.800 --> 00:49:55.160]   No, no, no, no.
[00:49:55.160 --> 00:50:00.840]   The top five announcements from Microsoft Build that you need to know.
[00:50:00.840 --> 00:50:02.200]   All right.
[00:50:02.200 --> 00:50:04.200]   Whew.
[00:50:04.200 --> 00:50:07.120]   It was really an AI show, right?
[00:50:07.120 --> 00:50:11.880]   Chat GPT gets being integration.
[00:50:11.880 --> 00:50:13.640]   That's not new, is it?
[00:50:13.640 --> 00:50:15.720]   Well, so yeah, actually kind of is.
[00:50:15.720 --> 00:50:20.240]   So the problem with chat GPT was it was, you know, walked off basically to the end of
[00:50:20.240 --> 00:50:22.560]   2021 for information.
[00:50:22.560 --> 00:50:26.280]   And so if you asked about like the basketball game last night with the Celtics, it would
[00:50:26.280 --> 00:50:30.480]   have no idea what you're talking about, where Bing does Bing has access to the search engine.
[00:50:30.480 --> 00:50:32.760]   And so that's the difference between the two.
[00:50:32.760 --> 00:50:35.120]   Otherwise, they're effectively the same.
[00:50:35.120 --> 00:50:39.560]   So what happens now is there's a plugin and they're going to allow basically chat GPT
[00:50:39.560 --> 00:50:40.760]   to have access to Bing.
[00:50:40.760 --> 00:50:42.000]   So that becomes more effective.
[00:50:42.000 --> 00:50:47.640]   So the differences between chat GPT and Bing are really blurring now, which is good for Microsoft
[00:50:47.640 --> 00:50:52.840]   because users don't need to choose between and say with the plugins.
[00:50:52.840 --> 00:50:54.600]   Oh, see, OK.
[00:50:54.600 --> 00:51:01.640]   So I read the headline backwards because chat Bing had already had chat GPT integrated.
[00:51:01.640 --> 00:51:02.640]   Right.
[00:51:02.640 --> 00:51:05.320]   This is now chat GPT getting Bing integrated.
[00:51:05.320 --> 00:51:09.000]   So it has more up to date search results, not just Bing either.
[00:51:09.000 --> 00:51:12.720]   Quite a few extensions.
[00:51:12.720 --> 00:51:16.840]   They talked about OpenTable, which has been around for a while in Zapier, but they're
[00:51:16.840 --> 00:51:18.640]   adding quite a few extensions.
[00:51:18.640 --> 00:51:24.760]   And this kind of makes sense because now like Wolfram Alpha, that's not new, but that's
[00:51:24.760 --> 00:51:25.760]   in there.
[00:51:25.760 --> 00:51:26.760]   In Instacart.
[00:51:26.760 --> 00:51:28.320]   I thought the Instacart was out before.
[00:51:28.320 --> 00:51:29.480]   I thought you could use this.
[00:51:29.480 --> 00:51:30.480]   They announced it.
[00:51:30.480 --> 00:51:34.640]   I pay for chat GPT, but not everybody's getting all the plugins.
[00:51:34.640 --> 00:51:36.400]   I don't have access to any of these.
[00:51:36.400 --> 00:51:37.400]   Yeah.
[00:51:37.400 --> 00:51:40.800]   So what happened was chat GPT announced a plugin support week ago.
[00:51:40.800 --> 00:51:41.800]   It's like six weeks ago.
[00:51:41.800 --> 00:51:45.360]   And then Microsoft announced plugin support a couple of weeks ago.
[00:51:45.360 --> 00:51:50.680]   And then at build, what they basically announced was, hey, our plugins, they're interoperable.
[00:51:50.680 --> 00:51:51.880]   Oh, build them for Bing.
[00:51:51.880 --> 00:51:52.880]   They work on chat GPT.
[00:51:52.880 --> 00:51:53.880]   Build them for chat GPT.
[00:51:53.880 --> 00:51:57.800]   They're going to work on Bing, which is great because as a developer, you don't want to
[00:51:57.800 --> 00:52:01.360]   choose between these two systems if you have limited resources and you're not going to
[00:52:01.360 --> 00:52:02.760]   have to do that anymore.
[00:52:02.760 --> 00:52:06.320]   It's going to be the same system, which is going to be really beneficial for Microsoft
[00:52:06.320 --> 00:52:08.800]   here because they're going to reap the benefits.
[00:52:08.800 --> 00:52:12.560]   Yeah, that was what this makes me go ahead on.
[00:52:12.560 --> 00:52:14.600]   This makes me think of Facebook platform.
[00:52:14.600 --> 00:52:20.280]   Like, yeah, there's going to be a lot of excitement, a lot of buzz, probably, you know, some startups
[00:52:20.280 --> 00:52:25.320]   are going to get a hundred million dollars plus series A valuations because they build
[00:52:25.320 --> 00:52:27.360]   chat GPT plugins.
[00:52:27.360 --> 00:52:31.120]   And then we're going to have the privacy violations.
[00:52:31.120 --> 00:52:34.040]   Then we're going to have the, you know, runaway spam.
[00:52:34.040 --> 00:52:39.680]   Then we're going to have the, you know, the ethical problems and open AI is going to say,
[00:52:39.680 --> 00:52:42.680]   oh, we really should have thought about all of that.
[00:52:42.680 --> 00:52:47.880]   I'm just like task rabbit Instagram or Instacart.
[00:52:47.880 --> 00:52:51.760]   Like I want to see the mashup of this where you tell it something and your whole life
[00:52:51.760 --> 00:52:52.760]   is ruined.
[00:52:52.760 --> 00:52:55.920]   It reaches into the real world.
[00:52:55.920 --> 00:52:58.360]   It books you trick it to Mexico.
[00:52:58.360 --> 00:52:59.920]   It hires explosive company.
[00:52:59.920 --> 00:53:01.560]   800 pizzas delivered.
[00:53:01.560 --> 00:53:02.560]   Yeah.
[00:53:02.560 --> 00:53:03.560]   Yeah.
[00:53:03.560 --> 00:53:04.560]   It orders a sulfuric acid.
[00:53:04.560 --> 00:53:07.560]   You know, remember when Facebook started spamming your friends with all of your e-commerce
[00:53:07.560 --> 00:53:08.560]   purchases?
[00:53:08.560 --> 00:53:09.560]   Oh my God.
[00:53:09.560 --> 00:53:14.280]   You've ruined someone's proposal because, you know, it posted like so-and-so just bought
[00:53:14.280 --> 00:53:15.800]   an engagement ring.
[00:53:15.800 --> 00:53:16.800]   Wow.
[00:53:16.800 --> 00:53:17.800]   Wow.
[00:53:17.800 --> 00:53:20.080]   So yeah, I saw the effect platform.
[00:53:20.080 --> 00:53:23.840]   You said that, that it's like the AI platform.
[00:53:23.840 --> 00:53:26.760]   That's in fact, such an even explicitly addressed as he.
[00:53:26.760 --> 00:53:27.760]   It is a platform.
[00:53:27.760 --> 00:53:33.400]   He brought up Bill Gates definition of a platform, which is a platform is something that the
[00:53:33.400 --> 00:53:37.040]   people who use it make more money than the platform makers make in effect, right?
[00:53:37.040 --> 00:53:40.440]   Microsoft created Microsoft knows better than anybody.
[00:53:40.440 --> 00:53:47.280]   The power of a platform, it created Windows and the people who developed on top of Windows
[00:53:47.280 --> 00:53:52.680]   did very well and I guess made more money in aggregate than Microsoft did.
[00:53:52.680 --> 00:53:54.280]   But everybody benefited from that.
[00:53:54.280 --> 00:53:57.120]   So Microsoft says this is a new platform essentially.
[00:53:57.120 --> 00:53:58.120]   Yes.
[00:53:58.120 --> 00:53:59.120]   Yeah.
[00:53:59.120 --> 00:54:00.120]   Oh yeah, absolutely.
[00:54:00.120 --> 00:54:06.840]   Now they were able to get this plug-in format compatible with chat GPT because they own 47%
[00:54:06.840 --> 00:54:08.840]   of the chat GPT, right?
[00:54:08.840 --> 00:54:09.840]   Yeah.
[00:54:09.840 --> 00:54:15.480]   I mean, who knows exactly in those agreements, but yeah, obviously the relationship and
[00:54:15.480 --> 00:54:19.920]   I know that's Father Zilong must, but I think it's just more sour grapes because he pulled
[00:54:19.920 --> 00:54:20.920]   out of open.
[00:54:20.920 --> 00:54:23.520]   He found it and pulled out because they were going to commercialize it and he didn't want
[00:54:23.520 --> 00:54:25.080]   him to commercialize it.
[00:54:25.080 --> 00:54:29.720]   Well, if he go buy semaphore, they said he pulled out because he wanted to take over
[00:54:29.720 --> 00:54:31.520]   the company and he wanted to be put in charge.
[00:54:31.520 --> 00:54:32.520]   That makes a lot more sense.
[00:54:32.520 --> 00:54:33.920]   Yeah, that makes a lot of sense.
[00:54:33.920 --> 00:54:36.800]   No, he was like, "All right, well, I'm going to go," but then he said, "I'll still give
[00:54:36.800 --> 00:54:37.800]   you some money."
[00:54:37.800 --> 00:54:38.800]   And they're like, "All right, cool."
[00:54:38.800 --> 00:54:39.800]   And then he didn't give the money.
[00:54:39.800 --> 00:54:45.160]   And then all of a sudden they want to scale up because Google was doing these large LLMs.
[00:54:45.160 --> 00:54:47.360]   And then that's where Microsoft came in.
[00:54:47.360 --> 00:54:49.640]   So Microsoft's like, "Here's a couple billion dollars."
[00:54:49.640 --> 00:54:50.800]   That's how it all happened.
[00:54:50.800 --> 00:54:54.360]   So technically this is, yeah, it's all Musk's creation.
[00:54:54.360 --> 00:54:57.880]   It's its creation from the beginning of OpenAI and its creation where it is now because
[00:54:57.880 --> 00:54:59.440]   of his actions.
[00:54:59.440 --> 00:55:00.440]   Yeah.
[00:55:00.440 --> 00:55:05.880]   So Microsoft, they said, put in 10 billion, but I think we learned this week that the
[00:55:05.880 --> 00:55:07.760]   10 billion was really in kind.
[00:55:07.760 --> 00:55:11.280]   It was Azure time.
[00:55:11.280 --> 00:55:13.960]   But that's one thing that OpenAI really needs.
[00:55:13.960 --> 00:55:17.960]   It's very expensive to generate these large language models and do it at all.
[00:55:17.960 --> 00:55:19.320]   And they have a supercomputer.
[00:55:19.320 --> 00:55:20.320]   Yeah.
[00:55:20.320 --> 00:55:24.960]   Yeah, the supercomputer was, that was 2020, I believe, that Microsoft and OpenAI announced
[00:55:24.960 --> 00:55:27.960]   that they were going to use the, that to train their LLMs.
[00:55:27.960 --> 00:55:28.960]   Right.
[00:55:28.960 --> 00:55:31.720]   So the difference here with Microsoft versus some of the other, you know, this is why
[00:55:31.720 --> 00:55:32.720]   I worry about Apple.
[00:55:32.720 --> 00:55:35.360]   Here's what Apple's doing because they don't have quite have this infrastructure.
[00:55:35.360 --> 00:55:40.000]   Oh, we'll get to Apple because Apple's coming up with a big announcement.
[00:55:40.000 --> 00:55:42.680]   And I'll be very curious how they respond.
[00:55:42.680 --> 00:55:47.280]   You know, we've been really watching Google and Microsoft go back and forth over AI,
[00:55:47.280 --> 00:55:48.280]   right?
[00:55:48.280 --> 00:55:52.600]   There was Google I/O where it was AI every three seconds.
[00:55:52.600 --> 00:55:58.600]   I have to say, Bard, Google's chat engine, search engine, it seems a little bit laggard
[00:55:58.600 --> 00:56:03.040]   compared to chat GPT, but Google's now integrating it into their own search with SGE.
[00:56:03.040 --> 00:56:04.600]   It's a Google Labs thing.
[00:56:04.600 --> 00:56:05.600]   So there's it.
[00:56:05.600 --> 00:56:10.080]   And then Microsoft comes back with their own volley of AI application.
[00:56:10.080 --> 00:56:13.960]   So that was the first one, Bing integration.
[00:56:13.960 --> 00:56:15.560]   Bing chat is boosted with plugins.
[00:56:15.560 --> 00:56:16.960]   Oh, here's the plug-ins slide.
[00:56:16.960 --> 00:56:24.120]   This is, this is the one with, with all of the plugins from Adobe to Zillow and everything
[00:56:24.120 --> 00:56:25.120]   in between.
[00:56:25.120 --> 00:56:26.120]   Spotify.
[00:56:26.120 --> 00:56:27.120]   Yeah.
[00:56:27.120 --> 00:56:28.120]   Yeah.
[00:56:28.120 --> 00:56:30.920]   And they said the Spotify thing like four times.
[00:56:30.920 --> 00:56:37.400]   That was really, we fit, we did, we covered both keynotes with Rich Campbell on day one
[00:56:37.400 --> 00:56:40.160]   and Rich and Paul Theron on day two.
[00:56:40.160 --> 00:56:45.720]   And both times they announced that you could generate a Spotify playlist.
[00:56:45.720 --> 00:56:48.760]   Like this is the number one thing people want to do.
[00:56:48.760 --> 00:56:49.760]   Developers.
[00:56:49.760 --> 00:56:50.760]   Yeah.
[00:56:50.760 --> 00:56:51.760]   Right.
[00:56:51.760 --> 00:56:56.000]   You want to buy a personal computer so you can do recipes, you know, that's the primary
[00:56:56.000 --> 00:56:57.000]   use case.
[00:56:57.000 --> 00:56:59.360]   You can remember that and balance your checkbook, right?
[00:56:59.360 --> 00:57:00.360]   That's your checkbook.
[00:57:00.360 --> 00:57:03.920]   I mean, I'm still using quicken, but you know, I, I'm sure this has been talked about before,
[00:57:03.920 --> 00:57:08.720]   but it's, this is, continues the culmination of like Google search results are terrible.
[00:57:08.720 --> 00:57:13.680]   And I wonder if one of the reasons that Microsoft's been able to have so much, uh, place, a room
[00:57:13.680 --> 00:57:17.120]   to run on this is that, I mean, I am sick of Google results.
[00:57:17.120 --> 00:57:21.480]   Like I, even if chat GPT is making stuff up, it's better than when I got into Google.
[00:57:21.480 --> 00:57:26.120]   So I think Bing has an incredible opportunity if they get, if Microsoft doesn't mess it
[00:57:26.120 --> 00:57:32.560]   up with all the extensions with the whole approach that it could dramatically leapfrog.
[00:57:32.560 --> 00:57:37.520]   I mean, again, I am completely concerned about the shape of inaccurate results, but at the
[00:57:37.520 --> 00:57:39.680]   same time, I get an accurate results on Google.
[00:57:39.680 --> 00:57:42.120]   You search in, you search in almost anything on Google now.
[00:57:42.120 --> 00:57:46.960]   And often if the first or second result isn't good, for me, I have to go down pages or do
[00:57:46.960 --> 00:57:48.120]   a ton of refinement.
[00:57:48.120 --> 00:57:52.600]   It can take me five or 10 minutes to shape a Google search that actually gets me past
[00:57:52.600 --> 00:57:53.600]   the spam.
[00:57:53.600 --> 00:57:55.360]   And that is unacceptable.
[00:57:55.360 --> 00:57:59.800]   And I also don't blame them because I'm sure the, you know, there's a war in heaven that's
[00:57:59.800 --> 00:58:04.640]   always going on between the spamming search results and none, but it would be great.
[00:58:04.640 --> 00:58:08.720]   It would be great if this is one way to cut through it is that you can't spam the LLM
[00:58:08.720 --> 00:58:10.800]   is the way you can a Google search index.
[00:58:10.800 --> 00:58:11.800]   I, it's interesting.
[00:58:11.800 --> 00:58:13.120]   I've been hearing that a lot too.
[00:58:13.120 --> 00:58:15.760]   You know, and obviously I don't use Google budget anymore myself.
[00:58:15.760 --> 00:58:16.760]   I have to use Bing.
[00:58:16.760 --> 00:58:17.760]   Do you use Bing now, Daniel?
[00:58:17.760 --> 00:58:18.760]   Is that mine?
[00:58:18.760 --> 00:58:19.760]   You're probably not.
[00:58:19.760 --> 00:58:20.760]   He's also just a person.
[00:58:20.760 --> 00:58:21.760]   Yeah.
[00:58:21.760 --> 00:58:28.360]   I've been using Bing for many, many years, but because you have to, as a job occupation,
[00:58:28.360 --> 00:58:31.000]   I know you prefer it.
[00:58:31.000 --> 00:58:32.000]   I actually prefer.
[00:58:32.000 --> 00:58:35.440]   And plus I get, you know, there's the credits you get unbanked.
[00:58:35.440 --> 00:58:41.160]   So I have like 230,000 credits and I can go and read those for like, like, yeah, I can
[00:58:41.160 --> 00:58:44.360]   redeem them for Amazon cards and Spotify cards.
[00:58:44.360 --> 00:58:48.560]   Oh, man, I have this big rewards are better than I thought.
[00:58:48.560 --> 00:58:49.560]   They're really good.
[00:58:49.560 --> 00:58:51.200]   I mean, it literally does pay you to.
[00:58:51.200 --> 00:58:53.880]   It's a bit of a shopping and you get money back on shopping too.
[00:58:53.880 --> 00:58:56.480]   So it's really good in the US.
[00:58:56.480 --> 00:58:58.920]   I think Microsoft's problem is outside the US.
[00:58:58.920 --> 00:59:03.200]   It starts to not be as good where Google attempts have a little bit better global presence.
[00:59:03.200 --> 00:59:06.760]   I've been hearing this from a lot of people that Google search results have just been
[00:59:06.760 --> 00:59:08.200]   not as good.
[00:59:08.200 --> 00:59:13.640]   That said, is not a ton of evidence currently that Microsoft is gaining any significant
[00:59:13.640 --> 00:59:17.520]   traction with Bing even with all this hype.
[00:59:17.520 --> 00:59:21.920]   They were around two to 4% before they're maybe slightly higher now.
[00:59:21.920 --> 00:59:24.240]   This is the launch in pad.
[00:59:24.240 --> 00:59:27.840]   This is where I mean, this is what it's exciting to think we could have another search engine
[00:59:27.840 --> 00:59:28.840]   war.
[00:59:28.840 --> 00:59:29.840]   It was actually a valid.
[00:59:29.840 --> 00:59:34.360]   It's like we need Google as let it Google's let itself degrade to what extent it's it's
[00:59:34.360 --> 00:59:35.840]   direct responsibility or not.
[00:59:35.840 --> 00:59:38.520]   And it'd be great to have a great to have a real match up.
[00:59:38.520 --> 00:59:44.920]   I'm very sad because I was using Neva and they just sold and they're shutting down.
[00:59:44.920 --> 00:59:45.920]   I was paying for search.
[00:59:45.920 --> 00:59:52.960]   So let's let's let's put a pin in the announcements at at build because I kind of want to you said
[00:59:52.960 --> 00:59:57.400]   something I think that some listeners are going to say what Bing search.
[00:59:57.400 --> 00:59:59.360]   I mean, Google search has gone downhill.
[00:59:59.360 --> 01:00:03.640]   Yeah, don't you think we all agree that to that?
[01:00:03.640 --> 01:00:05.200]   That's I've seen it dramatically.
[01:00:05.200 --> 01:00:08.600]   It's what I mean, I don't know how other people feel when you're not using it, Daniel.
[01:00:08.600 --> 01:00:10.720]   So you're like, I use that occasion.
[01:00:10.720 --> 01:00:15.560]   But yeah, well, I see people complain about too is all the ads and you know, you know,
[01:00:15.560 --> 01:00:17.560]   you can type in something.
[01:00:17.560 --> 01:00:20.720]   Do you use what do you think?
[01:00:20.720 --> 01:00:22.520]   Is that premise acceptable?
[01:00:22.520 --> 01:00:27.000]   Do you Google's search results aren't good anymore?
[01:00:27.000 --> 01:00:33.480]   I mean, for commercial results, like I remember looking for, you know, looking for a hotel
[01:00:33.480 --> 01:00:35.840]   room last fall.
[01:00:35.840 --> 01:00:39.560]   And I just I finally found a site that had a good discount on a hotel.
[01:00:39.560 --> 01:00:40.480]   I wanted to say that.
[01:00:40.480 --> 01:00:45.400]   And I ended up picking up the phone because I just did not trust that the website Google
[01:00:45.400 --> 01:00:49.120]   was serving me was, you know, was reputable.
[01:00:49.120 --> 01:00:54.280]   You know, it's not it wasn't Expedia or Priceline or one that I'd heard of.
[01:00:54.280 --> 01:00:59.280]   And I think that, you know, I think that a lot of a lot of searches are kind of degraded
[01:00:59.280 --> 01:01:00.280]   at this point.
[01:01:00.280 --> 01:01:02.480]   Let me let me let's do an experiment.
[01:01:02.480 --> 01:01:03.480]   Let's do an experiment.
[01:01:03.480 --> 01:01:04.480]   So I'm going to search for Tina Turner.
[01:01:04.480 --> 01:01:07.040]   The late Tina Turner passed away this week.
[01:01:07.040 --> 01:01:09.800]   This is the Google search for Tina Turner.
[01:01:09.800 --> 01:01:15.240]   It says 394 million results, 0.61 seconds.
[01:01:15.240 --> 01:01:20.400]   All of all, all, only one of the results is on this page.
[01:01:20.400 --> 01:01:22.320]   They're all below the fold.
[01:01:22.320 --> 01:01:25.520]   In fact, way below the fold.
[01:01:25.520 --> 01:01:29.560]   And this is to get away from non-authoritativeness as you put the stuff that's very authoritative
[01:01:29.560 --> 01:01:30.560]   that you've curated.
[01:01:30.560 --> 01:01:31.560]   It's Yahoo.
[01:01:31.560 --> 01:01:32.560]   It's a Yahoo portal above.
[01:01:32.560 --> 01:01:34.760]   Yeah, there's pictures of Tina Turner.
[01:01:34.760 --> 01:01:37.000]   There's one article to the USA today.
[01:01:37.000 --> 01:01:38.840]   There's when she was born, when she died.
[01:01:38.840 --> 01:01:43.800]   Here's a video from YouTube, a Google property, and then the Wikipedia result.
[01:01:43.800 --> 01:01:48.560]   Plus a panel that shows where you can listen to her music on YouTube Spotify, Pandora,
[01:01:48.560 --> 01:01:49.800]   or YouTube music.
[01:01:49.800 --> 01:01:54.280]   A knowledge graph about box, which is cribbed directly from Wikipedia, I might add.
[01:01:54.280 --> 01:01:57.200]   So it's really the same as that Wikipedia result.
[01:01:57.200 --> 01:02:01.200]   People also ask, and now I'm below the fold, there's songs.
[01:02:01.200 --> 01:02:04.840]   People also search for there's more YouTube videos.
[01:02:04.840 --> 01:02:07.320]   Finally, I get top stories.
[01:02:07.320 --> 01:02:11.120]   Now Bing, let's look at Bing.
[01:02:11.120 --> 01:02:13.200]   It's links right away.
[01:02:13.200 --> 01:02:15.040]   There is a knowledge graph.
[01:02:15.040 --> 01:02:20.520]   There is a top searched artists.
[01:02:20.520 --> 01:02:23.040]   I feel like it's not much different.
[01:02:23.040 --> 01:02:26.280]   Well, but I feel like it's more, there are more links here.
[01:02:26.280 --> 01:02:31.920]   I mean, at least above the fold, there's one, two, three, four, five, six, seven pages
[01:02:31.920 --> 01:02:32.920]   that are linked.
[01:02:32.920 --> 01:02:37.680]   They're all obituaries, but that's actually not all obituaries.
[01:02:37.680 --> 01:02:41.200]   That's a better result.
[01:02:41.200 --> 01:02:45.360]   I would show you Niva, but I don't think they exist anymore.
[01:02:45.360 --> 01:02:47.080]   The worst one is you search for the how-to thing.
[01:02:47.080 --> 01:02:49.520]   You're like, "Why is my iCloud Drive so slow?"
[01:02:49.520 --> 01:02:51.320]   A question I've asked a lot lately.
[01:02:51.320 --> 01:02:53.160]   I think slow.
[01:02:53.160 --> 01:02:57.240]   You get maybe one or two good results, and you get hundreds of pages that are optimized,
[01:02:57.240 --> 01:03:02.440]   and they're often have, they're clearly maybe pre-AI generated text.
[01:03:02.440 --> 01:03:05.960]   That was copied and pasted as a wheelchair association.
[01:03:05.960 --> 01:03:12.800]   I'm going to ask you a question that I will then answer, which is, "Why is every how-to
[01:03:12.800 --> 01:03:16.200]   question that you search for a YouTube video?"
[01:03:16.200 --> 01:03:22.560]   I'm pretty sure the answer is money, because YouTube monetizes better than creating a,
[01:03:22.560 --> 01:03:24.360]   you know, how-to-
[01:03:24.360 --> 01:03:26.360]   I don't want to watch a video.
[01:03:26.360 --> 01:03:27.360]   I really don't.
[01:03:27.360 --> 01:03:28.360]   Exactly.
[01:03:28.360 --> 01:03:29.360]   Inefficient.
[01:03:29.360 --> 01:03:32.880]   I'm going to tell you about how it-
[01:03:32.880 --> 01:03:35.320]   But it's more profitable.
[01:03:35.320 --> 01:03:36.320]   It's more profitable.
[01:03:36.320 --> 01:03:37.320]   Oh, yeah.
[01:03:37.320 --> 01:03:38.320]   It's good for you, too.
[01:03:38.320 --> 01:03:40.800]   I'm going to tell you how I made $1200 off YouTube by accident.
[01:03:40.800 --> 01:03:48.960]   I made an airport utility run- a walk-through once, years ago, posted it on YouTube, and
[01:03:48.960 --> 01:03:52.400]   I made $1200 off without even knowing I was getting the money, because it was coming in
[01:03:52.400 --> 01:03:53.400]   in tiny amounts.
[01:03:53.400 --> 01:03:54.840]   They're like, "You've got a lot of you.
[01:03:54.840 --> 01:03:56.960]   You can monetize this before they impose the things."
[01:03:56.960 --> 01:03:57.960]   I said, "Yes."
[01:03:57.960 --> 01:04:01.800]   I already had a Google AdSense account, so it was just depositing small amounts over
[01:04:01.800 --> 01:04:02.800]   years.
[01:04:02.800 --> 01:04:04.960]   And one day I looked at where it was like, "Oh, here's so much money you made."
[01:04:04.960 --> 01:04:07.120]   I'm like, "How did I make $1200 off that?"
[01:04:07.120 --> 01:04:08.120]   So that's-
[01:04:08.120 --> 01:04:09.120]   That's why.
[01:04:09.120 --> 01:04:10.120]   That's why.
[01:04:10.120 --> 01:04:11.120]   That's why.
[01:04:11.120 --> 01:04:12.120]   Thank you.
[01:04:12.120 --> 01:04:16.720]   For years, John C. de Vorex said his number one YouTube video was one he made of a Barney
[01:04:16.720 --> 01:04:19.400]   doll singing in his bathroom floor.
[01:04:19.400 --> 01:04:24.520]   I'm looking for his- I don't think he has a channel anymore, so I can't find out.
[01:04:24.520 --> 01:04:25.960]   I bet you it's still number one.
[01:04:25.960 --> 01:04:29.120]   I have to say.
[01:04:29.120 --> 01:04:30.200]   So what do we do?
[01:04:30.200 --> 01:04:32.880]   I mean, that's not acceptable.
[01:04:32.880 --> 01:04:35.720]   Is there a replacement coming?
[01:04:35.720 --> 01:04:37.560]   Is Bing the right way to go?
[01:04:37.560 --> 01:04:43.400]   What is- I guess I'll use start using Bing.
[01:04:43.400 --> 01:04:46.720]   I think Bing still has its hang-ups with people.
[01:04:46.720 --> 01:04:50.480]   They think it's kind of a joke, but it is that, I think Glenn's right.
[01:04:50.480 --> 01:04:52.880]   There's going to be one opportunity for Bing.
[01:04:52.880 --> 01:04:53.880]   This is it.
[01:04:53.880 --> 01:04:59.040]   There was also the talk that Samsung may adopt Bing as its default browser, but it looks
[01:04:59.040 --> 01:05:00.040]   like that's happening.
[01:05:00.040 --> 01:05:01.040]   They changed their mind.
[01:05:01.040 --> 01:05:02.040]   Yeah.
[01:05:02.040 --> 01:05:05.000]   But stuff like that would be needed, you know, because even though most people would
[01:05:05.000 --> 01:05:09.760]   still switch away, the problem is is people are like, "What's your hooked into Google?
[01:05:09.760 --> 01:05:11.000]   It's just like browsers.
[01:05:11.000 --> 01:05:12.240]   You know the thing."
[01:05:12.240 --> 01:05:14.680]   But I'm just making a good effort here.
[01:05:14.680 --> 01:05:16.680]   They're doing all they can.
[01:05:16.680 --> 01:05:19.240]   It's just a changing sentiment.
[01:05:19.240 --> 01:05:22.520]   There's two things that I think are always required in these situations.
[01:05:22.520 --> 01:05:24.080]   And we almost have them.
[01:05:24.080 --> 01:05:29.160]   Google is going to be required to basically really trip up on itself and fail and annoy
[01:05:29.160 --> 01:05:31.680]   people enough that they want to move away.
[01:05:31.680 --> 01:05:35.080]   And then there needs to be a really good alternative for people to go to.
[01:05:35.080 --> 01:05:41.520]   And we're kind of almost there, but we haven't seen the actual push yet.
[01:05:41.520 --> 01:05:42.520]   I tried-
[01:05:42.520 --> 01:05:43.520]   Marissa Meyer has-
[01:05:43.520 --> 01:05:44.520]   Go ahead.
[01:05:44.520 --> 01:05:49.200]   Marissa Meyer has answered the question, by the way, of why not a former Yahoo CEO, but
[01:05:49.200 --> 01:05:52.640]   also at Google for many years before that.
[01:05:52.640 --> 01:05:56.800]   And she just says that Google results are bad because everything is bad on the web.
[01:05:56.800 --> 01:05:59.000]   And they're like, "All right, she has a point.
[01:05:59.000 --> 01:06:00.000]   It's like there's a lot of-
[01:06:00.000 --> 01:06:01.000]   That's kind of great.
[01:06:01.000 --> 01:06:05.240]   Today, the 20 years ago, there's just a lot more-
[01:06:05.240 --> 01:06:09.040]   I think because there's a lot of economic incentive for misinformation, for clicks, for
[01:06:09.040 --> 01:06:12.080]   perfect purchases, and this all makes sense.
[01:06:12.080 --> 01:06:16.800]   And this is on an interview with the Freakonomics Podcast where they asked, "Is Google getting
[01:06:16.800 --> 01:06:17.800]   worse?"
[01:06:17.800 --> 01:06:18.800]   The answer was, "Yeah."
[01:06:18.800 --> 01:06:22.680]   They talked to her because they figured she's actually a slightly disinterested third party
[01:06:22.680 --> 01:06:26.880]   now, not being at the company and having led it to where it was.
[01:06:26.880 --> 01:06:33.280]   What do you use Glenn as your phrase to test search engines?
[01:06:33.280 --> 01:06:35.000]   You said, "How do I back up my-
[01:06:35.000 --> 01:06:36.000]   Oh, yeah.
[01:06:36.000 --> 01:06:39.960]   Or what's the best hotel and whatever?
[01:06:39.960 --> 01:06:42.280]   What's the best gas grill?"
[01:06:42.280 --> 01:06:44.480]   I look for things like that.
[01:06:44.480 --> 01:06:48.120]   They're generic enough and you should get authoritative results because think how many-
[01:06:48.120 --> 01:06:52.360]   There's 20 publications that have really good roundup guides and gas grills.
[01:06:52.360 --> 01:06:58.160]   And often, not always, but often the top results for things like that are polluted by-
[01:06:58.160 --> 01:06:58.880]   In my-
[01:06:58.880 --> 01:07:03.560]   Maybe I have terrible results, but in my results are polluted by just the-
[01:07:03.560 --> 01:07:05.040]   This is the best blah, blah, blah.
[01:07:05.040 --> 01:07:10.280]   These nonsense things from sites that don't have any credibility, but they show up in-
[01:07:10.280 --> 01:07:11.280]   Right.
[01:07:11.280 --> 01:07:12.280]   - Google results.
[01:07:12.280 --> 01:07:13.280]   So at least watercutters showed up there.
[01:07:13.280 --> 01:07:14.280]   Oh, we see you.
[01:07:14.280 --> 01:07:15.280]   No, this is not-
[01:07:15.280 --> 01:07:16.280]   So I want to say this is not Google.
[01:07:16.280 --> 01:07:17.280]   Oh, this is not Google.
[01:07:17.280 --> 01:07:18.280]   This is so-
[01:07:18.280 --> 01:07:22.760]   So after Neva went down, I've been looking for replacements and I'm trying to one called
[01:07:22.760 --> 01:07:24.720]   Kagi, K-A-G-I.
[01:07:24.720 --> 01:07:27.960]   I think Jason Snell recommended it.
[01:07:27.960 --> 01:07:28.960]   It's not free.
[01:07:28.960 --> 01:07:29.960]   Neither was Neva.
[01:07:29.960 --> 01:07:32.840]   It's five bucks a month, but therefore no ads.
[01:07:32.840 --> 01:07:35.440]   And in theory, I guess no need to monetize.
[01:07:35.440 --> 01:07:37.240]   And these are good results, right?
[01:07:37.240 --> 01:07:38.880]   You would agree.
[01:07:38.880 --> 01:07:40.360]   These are the results you'd want.
[01:07:40.360 --> 01:07:41.360]   Yeah.
[01:07:41.360 --> 01:07:44.760]   I'm doing a Google search right now and I'm getting actually very good results for gas
[01:07:44.760 --> 01:07:45.760]   grills.
[01:07:45.760 --> 01:07:49.080]   I'm going to lay down before I start getting- Maybe that's the bad one.
[01:07:49.080 --> 01:07:50.280]   I do a lot of how-to writing.
[01:07:50.280 --> 01:07:53.440]   And so when I'm trying to get an answer, I will search forums and things like that.
[01:07:53.440 --> 01:07:58.040]   So I start on Google to see if anyone's posted anywhere a similar problem.
[01:07:58.040 --> 01:08:01.560]   Sometimes you'll find somebody writes in with a question and you'll find one other person
[01:08:01.560 --> 01:08:06.560]   in 2008 had this situation they described, maybe even how to fix it.
[01:08:06.560 --> 01:08:07.560]   Well, that's why people do-
[01:08:07.560 --> 01:08:08.560]   Right.
[01:08:08.560 --> 01:08:09.560]   There's sitecolinredit.com.
[01:08:09.560 --> 01:08:10.560]   Right?
[01:08:10.560 --> 01:08:11.560]   Yeah.
[01:08:11.560 --> 01:08:12.560]   Yeah.
[01:08:12.560 --> 01:08:17.120]   That's the best search engine for that kind of thing.
[01:08:17.120 --> 01:08:18.120]   That's the topic exchange.
[01:08:18.120 --> 01:08:19.120]   Yeah.
[01:08:19.120 --> 01:08:20.120]   It's a great-
[01:08:20.120 --> 01:08:24.480]   If I were looking for the best gas grills, I would search Reddit, not anything, not the
[01:08:24.480 --> 01:08:25.480]   white internet.
[01:08:25.480 --> 01:08:27.360]   And you know, it's interesting.
[01:08:27.360 --> 01:08:33.480]   Everyone from Wikipedia to Reddit to Twitter has been looking to extract more money from
[01:08:33.480 --> 01:08:38.560]   the use of their, you know, kind of data corpus, much of which is contributed by, you
[01:08:38.560 --> 01:08:39.840]   know, ordinary users.
[01:08:39.840 --> 01:08:42.240]   But we'll leave that aside.
[01:08:42.240 --> 01:08:48.560]   Among other things, AI companies looking to train their language models, which I think
[01:08:48.560 --> 01:08:49.560]   is smart.
[01:08:49.560 --> 01:08:50.560]   Oh, yeah.
[01:08:50.560 --> 01:08:51.560]   They're totally scraping Reddit, right?
[01:08:51.560 --> 01:08:52.560]   Of course.
[01:08:52.560 --> 01:08:53.560]   Yeah.
[01:08:53.560 --> 01:08:54.560]   And stack exchange.
[01:08:54.560 --> 01:08:55.560]   Yeah.
[01:08:55.560 --> 01:08:56.560]   And that should not be free.
[01:08:56.560 --> 01:08:57.560]   Yeah.
[01:08:57.560 --> 01:08:58.560]   That's a good point.
[01:08:58.560 --> 01:09:03.040]   Although, as you also point out, we made it.
[01:09:03.040 --> 01:09:04.880]   Damn it.
[01:09:04.880 --> 01:09:10.440]   The best, the thing about Reddit is there's 20 people who care more about gas grills than
[01:09:10.440 --> 01:09:12.040]   life itself.
[01:09:12.040 --> 01:09:13.680]   And those are the people you want to ask.
[01:09:13.680 --> 01:09:15.920]   What's the best gas grill, right?
[01:09:15.920 --> 01:09:16.920]   Yeah.
[01:09:16.920 --> 01:09:24.120]   I mean, it is, it is pretty gutsy for Elon Musk to, you know, look to charge everyone
[01:09:24.120 --> 01:09:25.120]   for.
[01:09:25.120 --> 01:09:26.120]   Yeah.
[01:09:26.120 --> 01:09:28.400]   Oh, he is the use of our data.
[01:09:28.400 --> 01:09:34.520]   He has really pissed off academics because he not only is charging now, he's saying if
[01:09:34.520 --> 01:09:39.320]   you had Twitter data on your servers that we were using for academic research, you must
[01:09:39.320 --> 01:09:42.720]   delete it now if you're not going to pay us for it.
[01:09:42.720 --> 01:09:48.200]   And again, that's not, you weren't even there, Elon, but most of that data was generated.
[01:09:48.200 --> 01:09:49.200]   Yeah.
[01:09:49.200 --> 01:09:50.200]   You weren't even there.
[01:09:50.200 --> 01:09:51.200]   I guess he wants it.
[01:09:51.200 --> 01:09:52.200]   Was it for a billion back?
[01:09:52.200 --> 01:09:53.200]   What was it?
[01:09:53.200 --> 01:09:54.200]   Yahoo guides.
[01:09:54.200 --> 01:09:56.280]   I feel like Yahoo had a pro at every product.
[01:09:56.280 --> 01:10:00.000]   They had so many products that you could find a Yahoo product that did everything.
[01:10:00.000 --> 01:10:04.240]   And I think there was a Yahoo guides or something like that that had product experts in different
[01:10:04.240 --> 01:10:08.480]   categories who specialized and got paid in some basis before affiliate programs and
[01:10:08.480 --> 01:10:10.360]   all that kind of referral revenue.
[01:10:10.360 --> 01:10:13.680]   That's the other thing, by the way, how is that formed?
[01:10:13.680 --> 01:10:14.680]   Oh, I love that.
[01:10:14.680 --> 01:10:20.640]   This is the other part is the reason Google results have become poor is because advertising
[01:10:20.640 --> 01:10:21.640]   rates went way down.
[01:10:21.640 --> 01:10:25.320]   I mean, particularly was accelerated early in the pandemic, but a lot of publications
[01:10:25.320 --> 01:10:26.320]   have found it.
[01:10:26.320 --> 01:10:31.600]   I think all of us understand this very well on this episode about it difficult for advertising
[01:10:31.600 --> 01:10:33.400]   dollars to support most publications.
[01:10:33.400 --> 01:10:38.040]   And so some went to subscribers, but a lot followed the wire cutter was a little cutting
[01:10:38.040 --> 01:10:43.480]   edge in looking at the how far you could push the we are totally editorial independent,
[01:10:43.480 --> 01:10:45.280]   which they were and are.
[01:10:45.280 --> 01:10:51.000]   And the we're taking money for referring you to a place in this slightly hands off transaction.
[01:10:51.000 --> 01:10:56.120]   And you know, I remember long ago back in 2000 or something, the Seattle Times said, we're
[01:10:56.120 --> 01:10:58.480]   never going to put advertisements on our cover.
[01:10:58.480 --> 01:11:01.720]   And you know, fast forward 15 years of Seattle times, of course, they've had some on their
[01:11:01.720 --> 01:11:02.720]   cover.
[01:11:02.720 --> 01:11:03.720]   It's the same thing.
[01:11:03.720 --> 01:11:07.360]   Every site now has affiliate relationships because it's a much more reliable form of
[01:11:07.360 --> 01:11:09.160]   income than anything else.
[01:11:09.160 --> 01:11:13.480]   So you go on, you're optimizing for Google because you want the affiliate revenue from
[01:11:13.480 --> 01:11:16.480]   Google so that you can make the money to run your whatever you do.
[01:11:16.480 --> 01:11:31.680]   You can't anymore go to, unfortunately Yahoo answers, but back in 2006, a user named Kavya
[01:11:31.680 --> 01:11:36.200]   asked the very famous query, how is Babby formed?
[01:11:36.200 --> 01:11:38.200]   How girl get pregnant?
[01:11:38.200 --> 01:11:42.240]   It was telling me the YouTube video like the animated.
[01:11:42.240 --> 01:11:43.520]   Oh, yeah, totally.
[01:11:43.520 --> 01:11:51.120]   I mean, this is from Know Your Meme and you can find site all of the memey responses to
[01:11:51.120 --> 01:11:52.120]   this.
[01:11:52.120 --> 01:11:54.720]   This is the famous YouTube.
[01:11:54.720 --> 01:12:09.360]   This is the famous YouTube.
[01:12:09.360 --> 01:12:12.720]   Let's move on to the top five announcements.
[01:12:12.720 --> 01:12:18.600]   Number three, Windows gets, Windows 11 gets its own AI assistant.
[01:12:18.600 --> 01:12:23.720]   Microsoft has been using the phrase co-pilot or the brand co-pilot, starting, I think,
[01:12:23.720 --> 01:12:25.360]   with GitHub co-pilot, right?
[01:12:25.360 --> 01:12:27.480]   That's now two years old.
[01:12:27.480 --> 01:12:29.480]   And it's now spreading.
[01:12:29.480 --> 01:12:32.280]   The latest co-pilot is Windows co-pilot.
[01:12:32.280 --> 01:12:34.280]   What does it do, Daniel?
[01:12:34.280 --> 01:12:37.360]   Yeah, so I mean, it's just kind of funny.
[01:12:37.360 --> 01:12:39.360]   We jokingly call it Cortana 2.0.
[01:12:39.360 --> 01:12:40.360]   I'm like herself.
[01:12:40.360 --> 01:12:43.280]   Yeah, did they have that in there?
[01:12:43.280 --> 01:12:44.280]   Yeah.
[01:12:44.280 --> 01:12:45.280]   Kind of, yeah.
[01:12:45.280 --> 01:12:46.680]   So it's not that radically different.
[01:12:46.680 --> 01:12:48.240]   Obviously doesn't look like Cortana.
[01:12:48.240 --> 01:12:49.600]   It's not called that.
[01:12:49.600 --> 01:12:54.600]   It does pop out from the side, though, and it's going to be access to the chat being
[01:12:54.600 --> 01:12:55.720]   chat as well.
[01:12:55.720 --> 01:12:57.160]   So that's not a huge thing.
[01:12:57.160 --> 01:13:01.080]   But the bigger deal is it can act upon Windows.
[01:13:01.080 --> 01:13:08.040]   You can ask it to do things, put my computer into dark mode and go to do my Windows update.
[01:13:08.040 --> 01:13:12.720]   And you can actually do more voice commands that are more advanced to get the operating
[01:13:12.720 --> 01:13:17.520]   system to do stuff that you may not have known where it is or whether it can do it or
[01:13:17.520 --> 01:13:18.520]   not.
[01:13:18.520 --> 01:13:23.560]   So this is, I think, where the first step of where these digital systems may actually
[01:13:23.560 --> 01:13:24.560]   become useful.
[01:13:24.560 --> 01:13:27.640]   I mean, yeah, like Siri's been around for so long now.
[01:13:27.640 --> 01:13:30.040]   And that really is not approved, right?
[01:13:30.040 --> 01:13:31.040]   None of them have.
[01:13:31.040 --> 01:13:36.760]   In fact, we were talking earlier, Muka and I, on Ask the Tech, somebody said, "In my
[01:13:36.760 --> 01:13:42.800]   nursing home, I used to set an alarm with the Google Assistant, set an alarm for 450.
[01:13:42.800 --> 01:13:45.960]   And then for a long time, I could say how much time is left in the alarm.
[01:13:45.960 --> 01:13:48.480]   Now it just says you have an alarm set for 450.
[01:13:48.480 --> 01:13:49.980]   It's gotten dumber."
[01:13:49.980 --> 01:13:52.160]   And he said, "Why is it getting dumber?"
[01:13:52.160 --> 01:13:56.680]   But this seems like all three assistants have been very disappointed.
[01:13:56.680 --> 01:13:59.440]   Serious, serious, serious terrible, but embarrassing.
[01:13:59.440 --> 01:14:00.920]   None of them are very good.
[01:14:00.920 --> 01:14:02.480]   Two multi-trillion dollar company.
[01:14:02.480 --> 01:14:03.480]   What are you doing?
[01:14:03.480 --> 01:14:08.080]   But I'm not sure I want chat GPT in there because this is going to go on and on and on.
[01:14:08.080 --> 01:14:09.640]   Well, let me tell you.
[01:14:09.640 --> 01:14:10.640]   You had that.
[01:14:10.640 --> 01:14:19.080]   I just like to be able to say, "I can't Siri currently, if I say, send a text message
[01:14:19.080 --> 01:14:22.680]   to my child's name, it'll say mobile or email."
[01:14:22.680 --> 01:14:23.840]   And I'm like, "Well, I don't know.
[01:14:23.840 --> 01:14:27.560]   What's the thing I've used for the last seven years to use them?
[01:14:27.560 --> 01:14:28.560]   Maybe that would be the method.
[01:14:28.560 --> 01:14:32.000]   And then if you say mobile or email, it's like, "I don't know what you're talking about.
[01:14:32.000 --> 01:14:33.000]   At least you chose Ask Me."
[01:14:33.000 --> 01:14:35.920]   You can't say, "See what I found on the web about that."
[01:14:35.920 --> 01:14:36.920]   That's right.
[01:14:36.920 --> 01:14:37.920]   Email is a meeting.
[01:14:37.920 --> 01:14:42.280]   We do really look forward to this stuff with machine learning and what we're calling AI
[01:14:42.280 --> 01:14:47.600]   is the ability to learn a user's behavior so that it can act upon stuff.
[01:14:47.600 --> 01:14:48.600]   Usually proactively.
[01:14:48.600 --> 01:14:52.680]   It's like, "Well, you usually do this around this time."
[01:14:52.680 --> 01:14:55.200]   Or say, "Right now you can get a notification.
[01:14:55.200 --> 01:14:56.520]   You have a meeting coming up.
[01:14:56.520 --> 01:14:57.520]   Cool."
[01:14:57.520 --> 01:14:58.520]   Why not?
[01:14:58.520 --> 01:15:02.080]   It actually did get, launches you into the meeting or gets you set up for it.
[01:15:02.080 --> 01:15:05.600]   It actually acts upon the thing before you need to do it yourself.
[01:15:05.600 --> 01:15:10.960]   I think that's the next level of this stuff that I'm really looking forward to.
[01:15:10.960 --> 01:15:13.800]   You can ask things probably to look at your documents.
[01:15:13.800 --> 01:15:16.200]   That's because they have co-pilot now coming out for office.
[01:15:16.200 --> 01:15:18.640]   Yeah, that's that document summary thing Glenn was talking about.
[01:15:18.640 --> 01:15:21.440]   I think it'll be good at that.
[01:15:21.440 --> 01:15:26.040]   They showed us an example of writer who has a bunch of notes.
[01:15:26.040 --> 01:15:31.800]   You said, "Can you summarize my notes on this topic so that I can start my novel or whatever?"
[01:15:31.800 --> 01:15:35.800]   Oh, I took 4,000 words of an interview just to see how replaceable I am.
[01:15:35.800 --> 01:15:37.800]   I took exonality for a thousand words.
[01:15:37.800 --> 01:15:42.800]   So the GPD4 write a 500 word article from this using direct quotes and don't use anything.
[01:15:42.800 --> 01:15:43.800]   It's not even good.
[01:15:43.800 --> 01:15:44.800]   It was not bad.
[01:15:44.800 --> 01:15:45.800]   It was not bad.
[01:15:45.800 --> 01:15:47.800]   That's by the way, that's it.
[01:15:47.800 --> 01:15:48.800]   It's not bad.
[01:15:48.800 --> 01:15:49.800]   It's not good.
[01:15:49.800 --> 01:15:55.000]   If someone had published it, it wouldn't have been entirely inaccurate because it did actually
[01:15:55.000 --> 01:15:59.640]   accurately only use quotes from the article and it used them essentially correctly.
[01:15:59.640 --> 01:16:01.320]   It was boring and it didn't have an eight.
[01:16:01.320 --> 01:16:03.560]   There was no reason for the article to exist.
[01:16:03.560 --> 01:16:05.800]   It didn't have a conclusive tone to it.
[01:16:05.800 --> 01:16:06.800]   It wasn't analytical.
[01:16:06.800 --> 01:16:07.800]   But you could add all that.
[01:16:07.800 --> 01:16:09.040]   I was like, after this.
[01:16:09.040 --> 01:16:13.400]   I mean, yeah, I'd look forward to that not being my job.
[01:16:13.400 --> 01:16:18.560]   That's why you know, you could have field of 19th century print history and centuries.
[01:16:18.560 --> 01:16:19.800]   It's a much better.
[01:16:19.800 --> 01:16:25.640]   But you don't want it to do all the things that you would do before you.
[01:16:25.640 --> 01:16:26.640]   No.
[01:16:26.640 --> 01:16:28.760]   I want it to be an enhancement and I hope it will be.
[01:16:28.760 --> 01:16:36.120]   I think again, as a writer, we all have particular needs that there are things in your like transcription
[01:16:36.120 --> 01:16:42.040]   is an incredible leg up like having a transcript of everything that's if it's not perfect,
[01:16:42.040 --> 01:16:46.200]   I can go listen the audio and fix it up so it's verbatim in a few seconds.
[01:16:46.200 --> 01:16:51.760]   That is an incredible enhancement over, you know, I mean that alone, like that could save
[01:16:51.760 --> 01:16:53.320]   hours on a long article.
[01:16:53.320 --> 01:16:59.720]   Even with Microsoft Teams, they've been advertising the feature where it will not only do a transcription
[01:16:59.720 --> 01:17:06.200]   of your meeting, which you expect at this point in technology, but it will also summarize
[01:17:06.200 --> 01:17:11.600]   that meeting and provide action points to the action items would be great, wouldn't it?
[01:17:11.600 --> 01:17:12.600]   Yeah.
[01:17:12.600 --> 01:17:13.600]   Yeah.
[01:17:13.600 --> 01:17:16.760]   And so if you miss this meeting, not only do you get a summary, but you get a quick list
[01:17:16.760 --> 01:17:19.160]   of the things that needs to be done.
[01:17:19.160 --> 01:17:22.400]   That's like the next level of like really now it's useful.
[01:17:22.400 --> 01:17:27.200]   And do that reliably without hallucinating or how accurate.
[01:17:27.200 --> 01:17:28.800]   They're rolling it out real soon.
[01:17:28.800 --> 01:17:29.800]   Yeah.
[01:17:29.800 --> 01:17:35.200]   The thing that Fireflies.ai is the transcription service I'm using and it puts action items
[01:17:35.200 --> 01:17:40.120]   at the bottom and they were again, it's come because it's taking a data input and not extrapolating.
[01:17:40.120 --> 01:17:42.800]   It seemed to do a fairly good job.
[01:17:42.800 --> 01:17:46.680]   Like if I were doing, I mean, for an average meeting where you don't need, you would never
[01:17:46.680 --> 01:17:50.600]   transcribe, you never pay for a transcriptionist, you never pay for someone to analyze it.
[01:17:50.600 --> 01:17:53.840]   Some might post some notes somewhere.
[01:17:53.840 --> 01:17:58.280]   This becomes an incredible company tool if every meeting can wind up being searchable
[01:17:58.280 --> 01:18:01.680]   and have some extracted stuff from it that helps.
[01:18:01.680 --> 01:18:05.520]   I mean, think about, I just think meetings are terrible to begin with, but having meetings
[01:18:05.520 --> 01:18:09.480]   and then having all the value lost because there's no way to harvest what was discussed
[01:18:09.480 --> 01:18:15.240]   except for people's imperfect note taking as it passes through the telephone game of
[01:18:15.240 --> 01:18:17.000]   office, you know, stuff.
[01:18:17.000 --> 01:18:19.400]   Like just a transcript of everything could be helpful.
[01:18:19.400 --> 01:18:20.400]   I mean, it might also be bad.
[01:18:20.400 --> 01:18:21.800]   It could cause a lot of hurt feelings too.
[01:18:21.800 --> 01:18:25.880]   Well, or if you missed an action item that was critical and it didn't know it was an
[01:18:25.880 --> 01:18:26.880]   action item.
[01:18:26.880 --> 01:18:31.440]   I mean, I guess it's figuring it out from the words you use or something.
[01:18:31.440 --> 01:18:35.000]   Anyway, I want to take a break because there is something we haven't addressed that is
[01:18:35.000 --> 01:18:38.320]   really the dirty little secret of all of this.
[01:18:38.320 --> 01:18:39.920]   And Microsoft didn't really address it either.
[01:18:39.920 --> 01:18:44.680]   I don't think they did, but Daniel Rabinos here from Windows Central, he will correct me
[01:18:44.680 --> 01:18:45.920]   if I'm wrong.
[01:18:45.920 --> 01:18:50.160]   Oh, and Thomas, always great to have you from the San Francisco Examiner.
[01:18:50.160 --> 01:18:53.800]   And we're going to keep putting pressure on you to bring Ditherati back because we love
[01:18:53.800 --> 01:18:54.800]   Ditherati.
[01:18:54.800 --> 01:18:59.200]   And while you're at it, bring back Yahoo pipes and then everybody will be happy.
[01:18:59.200 --> 01:19:00.200]   Yeah.
[01:19:00.200 --> 01:19:01.200]   Glenn Fleischman.
[01:19:01.200 --> 01:19:02.960]   I miss Yahoo pipes so much.
[01:19:02.960 --> 01:19:04.640]   I effing miss them.
[01:19:04.640 --> 01:19:06.640]   That was a great tool.
[01:19:06.640 --> 01:19:07.640]   It's a thing.
[01:19:07.640 --> 01:19:11.960]   I think I am trying to think of a piece of technology that people have more nostalgia
[01:19:11.960 --> 01:19:13.360]   for than Yahoo pipes.
[01:19:13.360 --> 01:19:17.160]   Partly because it showed a lot of promise and then it was frozen for like seven years before
[01:19:17.160 --> 01:19:18.240]   it died.
[01:19:18.240 --> 01:19:20.240]   So we never had a chance to decline.
[01:19:20.240 --> 01:19:25.080]   It just sat there and amber and everyone stared at it like a rare fossilized dinosaur
[01:19:25.080 --> 01:19:26.480]   bone or something.
[01:19:26.480 --> 01:19:28.160]   Anyway, they killed it.
[01:19:28.160 --> 01:19:34.200]   So no point in mourning it and you should probably learn Python because it's not coming
[01:19:34.200 --> 01:19:35.280]   back.
[01:19:35.280 --> 01:19:39.760]   Our showed anyway, great to have you to Glenn Fleischman for many, many points.
[01:19:39.760 --> 01:19:45.320]   But we will talk about your new book coming out with Marson, which you wrote called shift
[01:19:45.320 --> 01:19:46.320]   happens.
[01:19:46.320 --> 01:19:47.640]   It's imminent.
[01:19:47.640 --> 01:19:48.640]   It's imminent.
[01:19:48.640 --> 01:19:52.840]   And if you contributed to the Kickstarter, you'll, you'll hear some good news in just a
[01:19:52.840 --> 01:19:53.840]   bit.
[01:19:53.840 --> 01:19:58.720]   But first word from our sponsor, worldwide tech knowledge, WWT.
[01:19:58.720 --> 01:19:59.720]   We love WWT.
[01:19:59.720 --> 01:20:04.640]   Lisa and I went out there just before the pandemic broke before lockdown.
[01:20:04.640 --> 01:20:09.440]   And we saw the amazing advanced technology center.
[01:20:09.440 --> 01:20:12.480]   The ATC is at the heart of worldwide technology.
[01:20:12.480 --> 01:20:17.600]   It's a research and testing lab that brings together half a billion dollars of equipment
[01:20:17.600 --> 01:20:23.200]   from the leading OEMs and emerging disruptors, all the companies that enterprises might be
[01:20:23.200 --> 01:20:26.560]   using to get business done.
[01:20:26.560 --> 01:20:28.000]   Why do they build this?
[01:20:28.000 --> 01:20:31.880]   Well, initially, so their engineers could spin up proofs of concept, could get to know
[01:20:31.880 --> 01:20:36.960]   products, could use them in the lab before rolling them out to customers.
[01:20:36.960 --> 01:20:41.440]   But there's now a really great feature to the ATC.
[01:20:41.440 --> 01:20:49.600]   You can use it to the ATC offers hundreds of on-demand and schedulable labs, featuring
[01:20:49.600 --> 01:20:54.080]   solutions that include technologies representing the latest advances in enterprise, everything
[01:20:54.080 --> 01:21:01.000]   from cloud to security to networking, primary and secondary storage, data analytics, absolutely
[01:21:01.000 --> 01:21:02.000]   AI.
[01:21:02.000 --> 01:21:05.840]   If you want to learn what the cutting edge on AI is, for instance, AI used for business
[01:21:05.840 --> 01:21:06.840]   intelligence.
[01:21:06.840 --> 01:21:08.640]   They've got it all in the labs.
[01:21:08.640 --> 01:21:11.520]   DevOps, agile, so much more.
[01:21:11.520 --> 01:21:14.840]   WWT's engineers and partners use the ATC.
[01:21:14.840 --> 01:21:16.360]   But you can too.
[01:21:16.360 --> 01:21:19.960]   It'll cut your evaluation time for months to weeks.
[01:21:19.960 --> 01:21:23.400]   It'll introduce you to new ideas, new technologies.
[01:21:23.400 --> 01:21:30.160]   It'll help you do the thing you do best with technology that supports you.
[01:21:30.160 --> 01:21:34.840]   Before you go to market, you could access technical articles, expert insights, demonstration
[01:21:34.840 --> 01:21:39.400]   videos, white papers, of course, those hands-on labs, all the tools you need to stay up to
[01:21:39.400 --> 01:21:40.800]   date.
[01:21:40.800 --> 01:21:44.040]   And not only is it a physical lab space, but they've virtualized it so you don't even
[01:21:44.040 --> 01:21:48.200]   have to go to St. Louis like we did to use the ATC platform.
[01:21:48.200 --> 01:21:54.320]   Anybody can access these amazing resources anywhere in the world 365 days a year at no
[01:21:54.320 --> 01:21:56.360]   cost to you.
[01:21:56.360 --> 01:21:59.920]   So while you're there, make sure to check out WWT's events and communities.
[01:21:59.920 --> 01:22:01.200]   We did a great panel event.
[01:22:01.200 --> 01:22:02.560]   It was so much fun.
[01:22:02.560 --> 01:22:06.520]   For more opportunities to learn about technology trends, to hear about the latest research and
[01:22:06.520 --> 01:22:10.160]   insights from their experts, we've sent Lou Mareska from this week in Enterprise Tech
[01:22:10.160 --> 01:22:11.160]   out.
[01:22:11.160 --> 01:22:13.040]   He did a great keynote last year.
[01:22:13.040 --> 01:22:20.480]   Whatever your business needs, WWT can deliver scalable, tried and tested tailored solutions
[01:22:20.480 --> 01:22:23.040]   to help you succeed in business.
[01:22:23.040 --> 01:22:25.160]   Because WWT understands business.
[01:22:25.160 --> 01:22:29.320]   They bring strategy and execution together to make that new world happen.
[01:22:29.320 --> 01:22:33.680]   To learn more about worldwide technology, the Advanced Technology Center, and to gain access
[01:22:33.680 --> 01:22:37.040]   to all those great free resources, it's simple.
[01:22:37.040 --> 01:22:40.440]   Just go to www.t.com/twit.
[01:22:40.440 --> 01:22:43.920]   www.t.com/twit.
[01:22:43.920 --> 01:22:47.520]   Create a free account on their platform and you're in, man.
[01:22:47.520 --> 01:22:49.400]   And it is really a wonderful place.
[01:22:49.400 --> 01:22:51.160]   These are great people.
[01:22:51.160 --> 01:22:52.160]   www.t.com/twit.
[01:22:52.160 --> 01:22:58.920]   And by the way, please do us a favor and use the full URL so they know you saw it here.
[01:22:58.920 --> 01:23:05.200]   www.t.com/twit.
[01:23:05.200 --> 01:23:10.320]   To me, the dirty little secret of all of this is it's hideously expensive.
[01:23:10.320 --> 01:23:17.240]   And right now, it's being subsidized by Microsoft to gain a foothold into a new market, but
[01:23:17.240 --> 01:23:19.080]   that can't continue forever, Daniel.
[01:23:19.080 --> 01:23:20.920]   No, of course not.
[01:23:20.920 --> 01:23:23.080]   It's going to be charged.
[01:23:23.080 --> 01:23:27.640]   And I've heard this from Microsoft off the record that, you know, it shouldn't be
[01:23:27.640 --> 01:23:29.520]   a price, but like it's obvious, right?
[01:23:29.520 --> 01:23:34.840]   They're going to charge us somehow that who knows mechanism mechanism for it.
[01:23:34.840 --> 01:23:36.680]   Like I would say Microsoft 365.
[01:23:36.680 --> 01:23:41.080]   They already have a subscription program that it can come through.
[01:23:41.080 --> 01:23:44.640]   You know, we'll start probably with enterprise since they have deep pockets and typically
[01:23:44.640 --> 01:23:46.480]   don't mind spending that.
[01:23:46.480 --> 01:23:48.160]   How that trickles down to consumers.
[01:23:48.160 --> 01:23:52.920]   There's, you know, Microsoft 365 there where you get a terabyte of one drive access to office
[01:23:52.920 --> 01:23:54.600]   and all these other perks.
[01:23:54.600 --> 01:23:58.040]   You know, they could have another tier, it's an extra couple of dollars a month.
[01:23:58.040 --> 01:23:59.880]   But yeah, the economics.
[01:23:59.880 --> 01:24:01.520]   I think there's two issues here.
[01:24:01.520 --> 01:24:05.840]   One, they're currently not charging for it simply because the value isn't there, right?
[01:24:05.840 --> 01:24:08.440]   A lot of the stuff we just talked about isn't here yet.
[01:24:08.440 --> 01:24:11.520]   So cool pilots not here yet coming out maybe later this year.
[01:24:11.520 --> 01:24:13.280]   They'll be with Co-Pilot for office.
[01:24:13.280 --> 01:24:14.920]   So a lot of the stuff isn't here.
[01:24:14.920 --> 01:24:18.960]   So it'd be pretty mature to charge people for it when you're not really getting a lot
[01:24:18.960 --> 01:24:19.960]   out of it.
[01:24:19.960 --> 01:24:20.960]   You know, you have the big chat.
[01:24:20.960 --> 01:24:23.240]   Okay, it's cool, but is that worth money?
[01:24:23.240 --> 01:24:24.240]   Probably not.
[01:24:24.240 --> 01:24:29.040]   You have being image creator and it does have, they call them boosts.
[01:24:29.040 --> 01:24:32.840]   And if you get, you get to boost it, you get a priority on the server and your image
[01:24:32.840 --> 01:24:35.400]   renders more quickly because that's the issue here, right?
[01:24:35.400 --> 01:24:38.760]   So server time to server load and who gets access.
[01:24:38.760 --> 01:24:43.440]   If you pay for chat GPT, that's one of the things you get primary access to the server.
[01:24:43.440 --> 01:24:47.040]   And you, it goes really fast at chat GPT if you're paying for it.
[01:24:47.040 --> 01:24:49.360]   If you don't pay for it, yeah, you get that lag.
[01:24:49.360 --> 01:24:50.440]   So this will trickle down.
[01:24:50.440 --> 01:24:53.760]   I think they have time yet and to, you know, make sure that there's actual value there
[01:24:53.760 --> 01:24:55.080]   that people will pay for.
[01:24:55.080 --> 01:24:56.480]   We're not there yet.
[01:24:56.480 --> 01:24:57.480]   But you're right.
[01:24:57.480 --> 01:25:00.640]   And the other thing they're doing too is they're putting a lot of emphasis on Azure
[01:25:00.640 --> 01:25:02.520]   and optimization.
[01:25:02.520 --> 01:25:03.800]   So they really, really try that.
[01:25:03.800 --> 01:25:07.800]   And they've been lowering costs internally on this stuff.
[01:25:07.800 --> 01:25:11.120]   And they're going to continue to push that and continue to expect that to happen.
[01:25:11.120 --> 01:25:14.640]   So that finally, I think when they do roll this out, it won't be even as expensive as
[01:25:14.640 --> 01:25:16.920]   it would be today if they were to do it there.
[01:25:16.920 --> 01:25:19.040]   But yeah, obviously they have to charge for the stuff.
[01:25:19.040 --> 01:25:20.040]   It is very expensive.
[01:25:20.040 --> 01:25:21.600]   A lot of server space.
[01:25:21.600 --> 01:25:27.600]   I mean, when you're talking about building it into Windows 11, and we've talked a lot
[01:25:27.600 --> 01:25:31.880]   about Windows weekly, really being maybe that's what Windows 12 will be, is Windows
[01:25:31.880 --> 01:25:34.880]   with AI.
[01:25:34.880 --> 01:25:41.200]   Then you have a billion people, all of a sudden hitting, hammering your servers, plus
[01:25:41.200 --> 01:25:45.040]   the cost of building these large language models, which is steep, even though Microsoft
[01:25:45.040 --> 01:25:49.400]   owns Azure, it still costs some money to run it.
[01:25:49.400 --> 01:25:52.160]   Yeah.
[01:25:52.160 --> 01:25:53.680]   But it's your sense.
[01:25:53.680 --> 01:25:59.000]   I don't want you to breach your NDA, but it's your sense that they are thinking about
[01:25:59.000 --> 01:26:01.760]   this and they think it's going to be okay.
[01:26:01.760 --> 01:26:02.760]   Yeah.
[01:26:02.760 --> 01:26:08.200]   I mean, like I said, they already have a subscription plan for enterprise and consumers
[01:26:08.200 --> 01:26:10.280]   where they can just build these costs into it.
[01:26:10.280 --> 01:26:14.960]   I think the question right now is what's the perceived value that people would pay?
[01:26:14.960 --> 01:26:15.960]   How much would they pay?
[01:26:15.960 --> 01:26:16.960]   You can't charge them, yeah.
[01:26:16.960 --> 01:26:17.960]   And they would use.
[01:26:17.960 --> 01:26:18.960]   Yeah.
[01:26:18.960 --> 01:26:22.520]   How much can they reduce the actual costs before rolling that out?
[01:26:22.520 --> 01:26:27.200]   Glenn, you've been pointed a great use of it.
[01:26:27.200 --> 01:26:31.320]   How much would you have paid for that?
[01:26:31.320 --> 01:26:36.800]   For just the well, for what the transcription or for the transcription and the summary.
[01:26:36.800 --> 01:26:40.560]   I'm paying fireflies right now while I'm actively reporting.
[01:26:40.560 --> 01:26:42.800]   I'm playing because I don't always have a big article like that.
[01:26:42.800 --> 01:26:43.800]   Right.
[01:26:43.800 --> 01:26:44.800]   We have to tons of interviews.
[01:26:44.800 --> 01:26:46.600]   I think it's $19 a month.
[01:26:46.600 --> 01:26:50.120]   I think it'll be about 1000 minutes of uploads and unlimited zoom.
[01:26:50.120 --> 01:26:54.120]   I pay 20 for chats with chat GPT.
[01:26:54.120 --> 01:26:55.120]   Yeah.
[01:26:55.120 --> 01:27:01.960]   They're a 20 to mid journey or maybe more actually I think it's more like 60 for unlimited images.
[01:27:01.960 --> 01:27:05.360]   Do you think they make money on that or?
[01:27:05.360 --> 01:27:10.160]   I was wondering whether with the fireflies, I'm thinking what engine are they using?
[01:27:10.160 --> 01:27:14.440]   Maybe if it's proprietary, like what's the computational load storage load?
[01:27:14.440 --> 01:27:19.440]   It can't be, I mean, I don't know how many people are using a substantial amount of the
[01:27:19.440 --> 01:27:21.480]   process, but it can't be.
[01:27:21.480 --> 01:27:26.200]   AWS services, Amazon services are incredibly cheap per minute.
[01:27:26.200 --> 01:27:27.720]   I just don't know.
[01:27:27.720 --> 01:27:29.400]   I can't conceptualize it, I guess.
[01:27:29.400 --> 01:27:32.120]   I don't know what the, I mean, you can run some of the models.
[01:27:32.120 --> 01:27:33.440]   You can download and run on a phone.
[01:27:33.440 --> 01:27:38.360]   I know phones are super computers now anyway, but legitimately the fact that you can run
[01:27:38.360 --> 01:27:42.920]   some of this stuff in very low or low end hardware compared to what you can get at a data center
[01:27:42.920 --> 01:27:48.880]   makes me think that the computational cost must be relatively low, but it's not nothing
[01:27:48.880 --> 01:27:49.880]   when you multiply it by.
[01:27:49.880 --> 01:27:50.880]   Right.
[01:27:50.880 --> 01:27:56.520]   It's once, I mean, if I, if I, if I peg my CPU on an iPhone, well, that's fine.
[01:27:56.520 --> 01:27:59.440]   But it multiply that by a million people on your servers.
[01:27:59.440 --> 01:28:00.440]   That's another matter entirely.
[01:28:00.440 --> 01:28:01.440]   Yeah.
[01:28:01.440 --> 01:28:02.680]   I don't know what the growth is.
[01:28:02.680 --> 01:28:03.680]   It's a whole.
[01:28:03.680 --> 01:28:11.640]   Sam, it seems like a Sam Altman, the president and CEO of OpenAI says it's about 10 times
[01:28:11.640 --> 01:28:15.240]   the cost of a Google search, a chat GT query.
[01:28:15.240 --> 01:28:18.840]   That's a lot more expensive.
[01:28:18.840 --> 01:28:26.120]   I did see some fear from the same people who brought you a Palm pilot uses a free freezers
[01:28:26.120 --> 01:28:30.200]   worth of electricity to operate based on very bad calculations.
[01:28:30.200 --> 01:28:35.000]   There's a moral panic that AI will, you know, use more than a Bitcoin's worth of energy.
[01:28:35.000 --> 01:28:36.040]   Well, that's what I'm worried about.
[01:28:36.040 --> 01:28:37.040]   We got rid of.
[01:28:37.040 --> 01:28:41.880]   So we all figured out the crypto was really terrible for the environment.
[01:28:41.880 --> 01:28:48.080]   So all the, but all the crypto bros have now moved to AI bros and they continue to burn
[01:28:48.080 --> 01:28:50.920]   CPU cycles like crazy.
[01:28:50.920 --> 01:28:55.240]   By the way, Nvidia, how much did Nvidia's stock price go up this week?
[01:28:55.240 --> 01:28:58.080]   They became almost a trillion dollar company.
[01:28:58.080 --> 01:29:01.200]   And I think a lot of that is because of build.
[01:29:01.200 --> 01:29:02.200]   Yeah.
[01:29:02.200 --> 01:29:09.920]   Because they need their, their GPUs and everything for AI, their position and they do their own
[01:29:09.920 --> 01:29:10.920]   AI.
[01:29:10.920 --> 01:29:16.360]   So like, you know, Nvidia's position really well for the future.
[01:29:16.360 --> 01:29:20.880]   They're in automotive, they're in gaming, all these, they're in all every growth category,
[01:29:20.880 --> 01:29:21.880]   basically.
[01:29:21.880 --> 01:29:23.640]   Here's the releasing products.
[01:29:23.640 --> 01:29:26.920]   Here's the five day trend.
[01:29:26.920 --> 01:29:28.440]   This is well now.
[01:29:28.440 --> 01:29:36.400]   Well now, Wednesday, May 24th, right after the Microsoft keynote, keynote number two, I
[01:29:36.400 --> 01:29:44.080]   think that I, as I remember they, they went up, their market cap went up $200 billion,
[01:29:44.080 --> 01:29:46.120]   I think was, I remember.
[01:29:46.120 --> 01:29:50.920]   It's kind of crazy that the, you know, the GPU, which was this very kind of specialized
[01:29:50.920 --> 01:29:56.920]   niche thing, like mostly of interest to gamers or people who were, you know, like computer
[01:29:56.920 --> 01:30:04.320]   animators has become this like really desirable computing commodity.
[01:30:04.320 --> 01:30:08.120]   And you know, Intel has badly missed that transition.
[01:30:08.120 --> 01:30:14.040]   I mean, it's not the only, it's not the only transition that Intel missed, but I would,
[01:30:14.040 --> 01:30:16.280]   it's not that bad for Intel, but you're right about the GPU.
[01:30:16.280 --> 01:30:19.200]   I mean, for instance, the blurring in my background, I always talk about this.
[01:30:19.200 --> 01:30:20.200]   This is done by Nvidia.
[01:30:20.200 --> 01:30:21.200]   Oh, yeah.
[01:30:21.200 --> 01:30:23.640]   You use that, that Nvidia technology, don't you?
[01:30:23.640 --> 01:30:24.640]   That's really good.
[01:30:24.640 --> 01:30:25.640]   Yeah.
[01:30:25.640 --> 01:30:32.520]   It's also doing my voice in the sense that it filters out my background.
[01:30:32.520 --> 01:30:34.160]   Plus look away from us.
[01:30:34.160 --> 01:30:36.160]   Oh, I did do the eye.
[01:30:36.160 --> 01:30:37.160]   Oh, do the eye.
[01:30:37.160 --> 01:30:39.360]   Daniel, do the eye trick.
[01:30:39.360 --> 01:30:40.360]   Yeah.
[01:30:40.360 --> 01:30:44.880]   Give me a last time you were on, you had eyes looking through your hand.
[01:30:44.880 --> 01:30:47.040]   We made that the, the, the, the, the, the, the, the, the, the, the, the, the, the, the,
[01:30:47.040 --> 01:30:48.040]   okay.
[01:30:48.040 --> 01:30:49.040]   Okay.
[01:30:49.040 --> 01:30:50.040]   Okay.
[01:30:50.040 --> 01:30:51.040]   Daniel is worth, he can't look away from us.
[01:30:51.040 --> 01:30:52.040]   He's closing one eye.
[01:30:52.040 --> 01:30:54.040]   I'm trying to close one eye.
[01:30:54.040 --> 01:30:57.800]   And it doesn't like it, huh?
[01:30:57.800 --> 01:30:58.800]   You cannot look away.
[01:30:58.800 --> 01:30:59.800]   It tries to make.
[01:30:59.800 --> 01:31:00.800]   Oh, yeah.
[01:31:00.800 --> 01:31:01.800]   Yeah.
[01:31:01.800 --> 01:31:02.800]   He's trying.
[01:31:02.800 --> 01:31:03.800]   I can't.
[01:31:03.800 --> 01:31:05.680]   No, no eyes through your eyelids.
[01:31:05.680 --> 01:31:06.680]   It's gotten better.
[01:31:06.680 --> 01:31:09.320]   It actually, it's gotten better since the last time.
[01:31:09.320 --> 01:31:10.320]   Put your hand in front of your face.
[01:31:10.320 --> 01:31:12.480]   Let's see if it still does that.
[01:31:12.480 --> 01:31:16.160]   Oh, it's trying to look through your hand.
[01:31:16.160 --> 01:31:19.560]   This is like a loose, uh, video film suddenly.
[01:31:19.560 --> 01:31:25.600]   Yeah, that's going to be, yeah, and it's not that one before, but yeah, I mean, so in
[01:31:25.600 --> 01:31:26.880]   video's definitely positioned here.
[01:31:26.880 --> 01:31:28.680]   I will say that within until don't forget.
[01:31:28.680 --> 01:31:30.600]   So, you know, we're talking about server costs and all that.
[01:31:30.600 --> 01:31:33.200]   And that's true, but there's the other half of this that's coming out, right?
[01:31:33.200 --> 01:31:34.480]   And that's already kind of out.
[01:31:34.480 --> 01:31:35.480]   NPUs, right?
[01:31:35.480 --> 01:31:36.480]   Neural processing units.
[01:31:36.480 --> 01:31:38.320]   So you'll have localized processing.
[01:31:38.320 --> 01:31:40.360]   Well, this was a big, big thing.
[01:31:40.360 --> 01:31:44.000]   By the way, it build, uh, both, I think Paul Thorep brought this up.
[01:31:44.000 --> 01:31:49.520]   A lot of the technologies they were talking about on device technologies require an NPU
[01:31:49.520 --> 01:31:50.520]   system.
[01:31:50.520 --> 01:31:55.120]   Only, I mean, right now only Qualcomm makes NPUs.
[01:31:55.120 --> 01:31:59.640]   It's going to be the fall before the meteor lake, uh, Intel processors with neural processing
[01:31:59.640 --> 01:32:00.640]   units come out.
[01:32:00.640 --> 01:32:04.360]   Of course, Apple stuff all has MPUs now because they're using their own chips.
[01:32:04.360 --> 01:32:08.040]   Uh, Microsoft's going to be a little bit behind because of that.
[01:32:08.040 --> 01:32:09.280]   They've got to get MPUs out.
[01:32:09.280 --> 01:32:13.800]   I think just the Surface Go writer, Surface X, uh, uses.
[01:32:13.800 --> 01:32:15.040]   On Surface Pro 9.
[01:32:15.040 --> 01:32:16.040]   Yeah.
[01:32:16.040 --> 01:32:18.280]   Because well, the, uh, the 5G version.
[01:32:18.280 --> 01:32:23.760]   It's like, so that can do native background blurring AI tracking and all that kind of
[01:32:23.760 --> 01:32:24.760]   stuff.
[01:32:24.760 --> 01:32:27.200]   And that's NPU right now we're relying on GPUs, mostly to do that.
[01:32:27.200 --> 01:32:30.000]   Occasionally CPUs, fine, but it's brute force.
[01:32:30.000 --> 01:32:33.640]   NPUs are hard coded design to handle AI.
[01:32:33.640 --> 01:32:34.640]   Intel does have theirs coming out.
[01:32:34.640 --> 01:32:35.640]   But yeah, you're right.
[01:32:35.640 --> 01:32:39.160]   Technically right now it's just Qualcomm and they've been doing it for years because of
[01:32:39.160 --> 01:32:40.160]   smartphones.
[01:32:40.160 --> 01:32:41.160]   So, yeah.
[01:32:41.160 --> 01:32:42.160]   Yeah.
[01:32:42.160 --> 01:32:47.480]   I just want to point out that Qualcomm is, um, worth slightly more than Intel, uh, they're
[01:32:47.480 --> 01:32:53.320]   both around $120 billion, but Qualcomm's a little more, uh, AMD, which has long been
[01:32:53.320 --> 01:32:59.720]   the underdog runner up to Intel is worth $200 billion and, uh, Nvidia is worth $960
[01:32:59.720 --> 01:33:00.720]   billion.
[01:33:00.720 --> 01:33:01.960]   They've been flirting with that $1 trillion.
[01:33:01.960 --> 01:33:07.400]   They're almost, I mean, it's eight times as valuable as Intel.
[01:33:07.400 --> 01:33:08.400]   Yeah.
[01:33:08.400 --> 01:33:09.400]   Yeah.
[01:33:09.400 --> 01:33:11.640]   Does that, that might just be an overheated market, right?
[01:33:11.640 --> 01:33:14.800]   That's not necessarily what they really are worth.
[01:33:14.800 --> 01:33:17.400]   That's two, they're priced to earn a, a price to earn a, a price to earn a, a price
[01:33:17.400 --> 01:33:18.400]   to earn a price.
[01:33:18.400 --> 01:33:19.400]   And the earnings is 200, two.
[01:33:19.400 --> 01:33:23.640]   So, you know what, I think, ratio.
[01:33:23.640 --> 01:33:28.920]   I think Warren Buffett said that the stock market in the, in the short term is a popularity
[01:33:28.920 --> 01:33:30.480]   contest in the long run.
[01:33:30.480 --> 01:33:31.880]   It's a voting machine.
[01:33:31.880 --> 01:33:32.880]   Right.
[01:33:32.880 --> 01:33:36.940]   Um, so yeah, I mean, is any stock worth what it is?
[01:33:36.940 --> 01:33:41.200]   Well, yeah, because someone bought it and sold it at that price, but yeah.
[01:33:41.200 --> 01:33:42.360]   That's a good point.
[01:33:42.360 --> 01:33:44.980]   It's not really tied to the return of the company.
[01:33:44.980 --> 01:33:48.580]   It's only tied to the, what people think is the return at the company.
[01:33:48.580 --> 01:33:49.580]   Yeah.
[01:33:49.580 --> 01:33:52.460]   You can tell when the time travelers show up though, as they show up just before it does
[01:33:52.460 --> 01:33:56.380]   that and they buy calls on the stock.
[01:33:56.380 --> 01:33:59.860]   So if you watch for a lot of calls, then you can identify whether time travel exists.
[01:33:59.860 --> 01:34:04.380]   That's the, Intel saving grace right now is their foundries, you know, that they build
[01:34:04.380 --> 01:34:08.100]   chips and they're going to be building chips for other companies, including Qualcomm.
[01:34:08.100 --> 01:34:09.100]   Right.
[01:34:09.100 --> 01:34:14.020]   And that's their, you know, I think their next big business move as, but their processors
[01:34:14.020 --> 01:34:15.020]   are catching up there.
[01:34:15.020 --> 01:34:19.580]   I think their positions long term, they're going to do all right.
[01:34:19.580 --> 01:34:24.900]   One of the stories that got buried because it wasn't an AI story, but you guys call
[01:34:24.900 --> 01:34:25.980]   it as one of the top five.
[01:34:25.980 --> 01:34:31.980]   And I agree windows 11 is getting cloud powered OS backup and restore the kind of thing that
[01:34:31.980 --> 01:34:38.620]   we've got with an iPhone and you've got with your Android phone windows with windows eight.
[01:34:38.620 --> 01:34:42.180]   Well, really this was a windows eight feature that disappeared.
[01:34:42.180 --> 01:34:43.180]   Yeah.
[01:34:43.180 --> 01:34:44.180]   Yeah.
[01:34:44.180 --> 01:34:45.180]   A lot of people upset about that.
[01:34:45.180 --> 01:34:46.180]   Yeah.
[01:34:46.180 --> 01:34:47.180]   So it's a basic function, right?
[01:34:47.180 --> 01:34:52.300]   That when you say you get a new windows 11 PC, you install it, you log into your accounts,
[01:34:52.300 --> 01:34:55.380]   and then the question is, all right, what are the apps I need to install?
[01:34:55.380 --> 01:35:00.140]   You know, and if you have a pretty previous windows 11 device, this is just going to populate
[01:35:00.140 --> 01:35:04.700]   the stuff from the store on your start menu again, just like a smartphone would when you,
[01:35:04.700 --> 01:35:06.300]   you know, transfer your old account.
[01:35:06.300 --> 01:35:07.300]   That's awesome.
[01:35:07.300 --> 01:35:08.300]   Actually, that's a great feature.
[01:35:08.300 --> 01:35:09.300]   Yeah.
[01:35:09.300 --> 01:35:10.300]   Pretty basic.
[01:35:10.300 --> 01:35:13.380]   Yeah, windows eight used to have it and then it got rid of it and people have been wanting
[01:35:13.380 --> 01:35:14.460]   it back ever since.
[01:35:14.460 --> 01:35:17.900]   So that was a little palette cleanser in between AI stories.
[01:35:17.900 --> 01:35:21.100]   I just thought we throw that in just to refresh your palate.
[01:35:21.100 --> 01:35:26.860]   Actually, you said, I think really this whole arm thing is interesting because windows and
[01:35:26.860 --> 01:35:30.140]   arm historically is kind of not been great.
[01:35:30.140 --> 01:35:31.660]   I found it's getting better and better.
[01:35:31.660 --> 01:35:36.260]   I've started using it on my Mac and emulation or no, not emulation in VM.
[01:35:36.260 --> 01:35:37.260]   Well, it is emulation.
[01:35:37.260 --> 01:35:40.060]   I guess in hypervisor hypervisors, VMware.
[01:35:40.060 --> 01:35:42.140]   Oh, no, I'm sorry.
[01:35:42.140 --> 01:35:44.620]   It's parallels and it actually runs great.
[01:35:44.620 --> 01:35:45.900]   It really runs.
[01:35:45.900 --> 01:35:47.140]   It's very snappy.
[01:35:47.140 --> 01:35:48.140]   Yeah.
[01:35:48.140 --> 01:35:51.220]   Are is Microsoft going to?
[01:35:51.220 --> 01:35:54.540]   How important is arm to Microsoft?
[01:35:54.540 --> 01:35:55.620]   Very important.
[01:35:55.620 --> 01:35:58.140]   But don't forget, Apple has a different strategy.
[01:35:58.140 --> 01:36:03.580]   Apple is like, we're switching from x86 to arm and we're just going to throw that switch
[01:36:03.580 --> 01:36:05.620]   and everybody has to come over to it.
[01:36:05.620 --> 01:36:07.900]   And because they can do that, they do.
[01:36:07.900 --> 01:36:11.380]   And they were in just a better position long term with their chips.
[01:36:11.380 --> 01:36:16.260]   Microsoft is will support arm, but we're going to keep x86 round.
[01:36:16.260 --> 01:36:18.420]   And that's two different strategies.
[01:36:18.420 --> 01:36:24.500]   And so they're getting their windows 11 is much more optimized for arm than windows 10
[01:36:24.500 --> 01:36:27.340]   was and windows 12 will be even more of that.
[01:36:27.340 --> 01:36:32.380]   At build, we saw them bring over unity is now on arm and a bunch of other tools that are
[01:36:32.380 --> 01:36:35.020]   needed for developers to build apps, right?
[01:36:35.020 --> 01:36:36.700]   You're doing a basic app right now.
[01:36:36.700 --> 01:36:39.060]   You can do it in arm, not a problem.
[01:36:39.060 --> 01:36:42.300]   And you can even do hybrid apps now, x86 and arm.
[01:36:42.300 --> 01:36:46.500]   But the problem is we have very complicated apps that rely on other libraries.
[01:36:46.500 --> 01:36:48.420]   So those libraries don't support arm.
[01:36:48.420 --> 01:36:49.420]   Right.
[01:36:49.420 --> 01:36:50.420]   You know, it's a problem.
[01:36:50.420 --> 01:36:54.620]   And so that's been the journey there combined with Qualcomm's chips.
[01:36:54.620 --> 01:36:55.820]   Haven't been that impressive.
[01:36:55.820 --> 01:37:01.340]   They've been getting better, but they've always been like compared to what last gen Intel
[01:37:01.340 --> 01:37:02.540]   has done.
[01:37:02.540 --> 01:37:07.740]   The big thing now we're expecting is, of course, the new via chips, what are called the onix
[01:37:07.740 --> 01:37:08.740]   processors.
[01:37:08.740 --> 01:37:14.780]   I expect those to be debut later this year in the sense that they'll premiere them.
[01:37:14.780 --> 01:37:17.900]   They'll talk about them, the architecture, the processing speeds.
[01:37:17.900 --> 01:37:23.500]   I don't think we'll see them shipped until at least summer 2024, maybe in fall 2024.
[01:37:23.500 --> 01:37:24.820]   So we're still far out.
[01:37:24.820 --> 01:37:30.380]   But according to Qualcomm, they've had a lot of design wins, meaning a lot of OEMs are
[01:37:30.380 --> 01:37:36.180]   committed to building laptops and devices around these chips, more so than any other.
[01:37:36.180 --> 01:37:42.220]   But that Qualcomm says 2024 will be the pivot point for Windows on arm.
[01:37:42.220 --> 01:37:45.140]   That's going to be where we're going to really start to see it take off.
[01:37:45.140 --> 01:37:46.340]   Will that happen?
[01:37:46.340 --> 01:37:47.580]   Will onix deliver?
[01:37:47.580 --> 01:37:52.180]   I think it's a tough challenge, especially a gen one chip, but it is the X Apple engineers.
[01:37:52.180 --> 01:37:53.180]   So fingers crossed.
[01:37:53.180 --> 01:37:54.500]   Yeah, it's an interesting story.
[01:37:54.500 --> 01:38:00.540]   The guy left who was instrumental in the design of the Apple chips left Apple founded Nuvia
[01:38:00.540 --> 01:38:02.700]   Apple suit him.
[01:38:02.700 --> 01:38:03.700]   He countered suit.
[01:38:03.700 --> 01:38:07.740]   I think those suits have been dropped because he took a lot of employees with him to Nuvia
[01:38:07.740 --> 01:38:09.820]   and then Qualcomm bottom.
[01:38:09.820 --> 01:38:14.500]   And so that's how Qualcomm kind of got into the race.
[01:38:14.500 --> 01:38:20.700]   It's interesting because when Apple came out with these Apple Silicon chips, they were
[01:38:20.700 --> 01:38:26.100]   low-performance, they were low-power, they had NPUs built in, they had integrated memory.
[01:38:26.100 --> 01:38:29.420]   It looked like they had kind of leap-frogged the PC industry.
[01:38:29.420 --> 01:38:32.260]   And I imagine that all of this is in response to that.
[01:38:32.260 --> 01:38:33.260]   Yeah.
[01:38:33.260 --> 01:38:34.660]   Oh, yeah, absolutely.
[01:38:34.660 --> 01:38:40.580]   In fact, so I put up that story later about it's a much less than hiring people.
[01:38:40.580 --> 01:38:43.020]   You said stop being ridiculous.
[01:38:43.020 --> 01:38:46.540]   Microsoft is not building its own processor.
[01:38:46.540 --> 01:38:48.540]   So knock it off.
[01:38:48.540 --> 01:38:53.780]   And Windows fans have this weird fantasy that Microsoft is just going to build an ARM processor.
[01:38:53.780 --> 01:38:55.980]   It's going to come out the Gates Gen 1.
[01:38:55.980 --> 01:38:57.580]   It's going to blow everybody away.
[01:38:57.580 --> 01:38:58.580]   And that's going to be the end of it.
[01:38:58.580 --> 01:39:00.900]   And it's less like that's not how any of this works.
[01:39:00.900 --> 01:39:04.180]   Apple came out with the M1, but it took them 10 years.
[01:39:04.180 --> 01:39:05.180]   Yeah.
[01:39:05.180 --> 01:39:10.060]   They were doing it for a very long time and it had a lot of devices to test it on and
[01:39:10.060 --> 01:39:17.300]   scale that Microsoft does not have an A4 processor to start with to build up to an I1.
[01:39:17.300 --> 01:39:19.940]   Apple was putting these out on the iPhone for years.
[01:39:19.940 --> 01:39:20.940]   Yeah.
[01:39:20.940 --> 01:39:25.220]   So I think there's some truth to this in this sense that I do believe Microsoft experiments
[01:39:25.220 --> 01:39:27.460]   with chips and chip design.
[01:39:27.460 --> 01:39:29.820]   I think that's actually publicly known that they've done that.
[01:39:29.820 --> 01:39:32.940]   They've done it with, you know, for server chips and have explored that.
[01:39:32.940 --> 01:39:34.380]   They have people working on it.
[01:39:34.380 --> 01:39:36.700]   They build their own code processors.
[01:39:36.700 --> 01:39:37.700]   That's public knowledge.
[01:39:37.700 --> 01:39:39.220]   They have them in the Surface device.
[01:39:39.220 --> 01:39:41.620]   So they do and they use them in HoloLens.
[01:39:41.620 --> 01:39:42.620]   They use them in Xbox.
[01:39:42.620 --> 01:39:46.620]   So they do build microprocessors, but they don't do that the scale of like what Apple
[01:39:46.620 --> 01:39:47.620]   is doing.
[01:39:47.620 --> 01:39:53.020]   That said, I wouldn't be surprised that back in 2019 and 20 that they were, you know,
[01:39:53.020 --> 01:39:57.220]   looking into this, which is where the Bloomberg story came out saying that Microsoft was looking
[01:39:57.220 --> 01:40:02.300]   to build its own processors to use in Azure and they may trickle down to Surface devices.
[01:40:02.300 --> 01:40:07.060]   I think what happened though was they were hedging their bets if Qualcomm couldn't,
[01:40:07.060 --> 01:40:11.540]   wasn't up to the task of Windows and committing to it or they didn't either went back out
[01:40:11.540 --> 01:40:12.900]   of that market.
[01:40:12.900 --> 01:40:19.180]   So I think, but then what happened was Qualcomm in 2021 bought NUVA for $1.4 billion.
[01:40:19.180 --> 01:40:23.220]   I think they did that because it was a strategic decision, but it was also sending a message
[01:40:23.220 --> 01:40:24.220]   to Microsoft.
[01:40:24.220 --> 01:40:25.220]   Hey, we're serious.
[01:40:25.220 --> 01:40:26.220]   We're committing to this.
[01:40:26.220 --> 01:40:30.060]   And I think Microsoft was, okay, why would we need to build our own chip at this point
[01:40:30.060 --> 01:40:31.820]   when we're going get it from these people?
[01:40:31.820 --> 01:40:34.500]   It doesn't make sense for Apple to make their own processor.
[01:40:34.500 --> 01:40:35.500]   Microsoft.
[01:40:35.500 --> 01:40:36.500]   Yeah.
[01:40:36.500 --> 01:40:37.500]   Yeah.
[01:40:37.500 --> 01:40:42.780]   I should go in this beautiful Cooler Master sneaker X computer.
[01:40:42.780 --> 01:40:49.740]   That is, that's a key spot.
[01:40:49.740 --> 01:40:51.420]   It's only $6,000.
[01:40:51.420 --> 01:40:56.420]   I'm $3,000 over the cost of the parts for it.
[01:40:56.420 --> 01:40:57.420]   Yeah.
[01:40:57.420 --> 01:40:59.620]   No, it's definitely the case.
[01:40:59.620 --> 01:41:00.860]   You're paying for the case.
[01:41:00.860 --> 01:41:05.980]   I'd like to, they should put this next to the screen or a person or something because
[01:41:05.980 --> 01:41:06.980]   how big is that?
[01:41:06.980 --> 01:41:07.980]   Well, yeah, I don't know.
[01:41:07.980 --> 01:41:09.900]   Is it actually sneaker size?
[01:41:09.900 --> 01:41:10.900]   I don't know.
[01:41:10.900 --> 01:41:12.540]   No, it is not.
[01:41:12.540 --> 01:41:15.300]   It's a giant sneaker.
[01:41:15.300 --> 01:41:18.900]   All right, let's take a little, let's take a little break and then, Oh, and I'm going
[01:41:18.900 --> 01:41:24.940]   to get you back in the conversation because the Surgeon General has told us social media
[01:41:24.940 --> 01:41:28.260]   is bad for kids.
[01:41:28.260 --> 01:41:30.660]   And I'm going to shock.
[01:41:30.660 --> 01:41:35.860]   Oh, and it's great to have Owen Thomas here from the examiner.
[01:41:35.860 --> 01:41:39.500]   Of course, our great friend Glenn Fleischman from the incomparable and his new book, Shift
[01:41:39.500 --> 01:41:40.500]   Happens.
[01:41:40.500 --> 01:41:41.780]   We'll tell you about that.
[01:41:41.780 --> 01:41:44.460]   Glenn.fun is his website.
[01:41:44.460 --> 01:41:49.940]   And of course, Daniel Rabino editor in chief of windows central great panel.
[01:41:49.940 --> 01:41:53.620]   Our show today brought to you by lookout.
[01:41:53.620 --> 01:41:54.620]   Business has changed, right?
[01:41:54.620 --> 01:41:56.780]   I mean, the pandemic changed how we work.
[01:41:56.780 --> 01:42:00.340]   People don't, you know, don't necessarily have to be at the office these days boundaries
[01:42:00.340 --> 01:42:04.500]   to where we work or even how we work have disappeared.
[01:42:04.500 --> 01:42:10.260]   But if you are in charge of protecting your company's data, that's scary business has
[01:42:10.260 --> 01:42:13.500]   changed and your data is always on the move.
[01:42:13.500 --> 01:42:19.260]   Whether it's on a device in the cloud across networks at the local coffee shop, your employees
[01:42:19.260 --> 01:42:20.260]   love it.
[01:42:20.260 --> 01:42:22.780]   Users love it, but it's a challenge for IT security.
[01:42:22.780 --> 01:42:28.740]   That's why you need lookout lookout helps you control your data and free your workforce.
[01:42:28.740 --> 01:42:30.860]   It's possible to have both with lookout.
[01:42:30.860 --> 01:42:35.580]   You gain complete visibility into all your data so you can minimize risk from external
[01:42:35.580 --> 01:42:42.940]   and internal threats and very importantly, ensure compliance by seamlessly securing
[01:42:42.940 --> 01:42:43.940]   hybrid work.
[01:42:43.940 --> 01:42:48.660]   Your organization doesn't have to sacrifice productivity or even employee satisfaction
[01:42:48.660 --> 01:42:50.740]   for security.
[01:42:50.740 --> 01:42:56.660]   Look out makes it job easier to, you know, with it security these days, you might have
[01:42:56.660 --> 01:42:59.820]   multiple solutions, none of which work together.
[01:42:59.820 --> 01:43:03.740]   You're trying to get it all working and you're spending way too many cycles getting the tools
[01:43:03.740 --> 01:43:10.060]   to work together and too little time on keeping secure lookout solves that.
[01:43:10.060 --> 01:43:16.660]   It's a single unified platform that reduces IT complexity so you can focus those cycles,
[01:43:16.660 --> 01:43:21.980]   those brain cells on the things you need to, whatever comes your way.
[01:43:21.980 --> 01:43:24.500]   Good data protection, it does not have to be a cage.
[01:43:24.500 --> 01:43:29.940]   It could be a springboard letting you and your organization leap toward a future of your
[01:43:29.940 --> 01:43:35.220]   making visit lookout.com today and learn how to safeguard data, secure hybrid work, reduce
[01:43:35.220 --> 01:43:40.060]   IT complexity lookout.com lookout.
[01:43:40.060 --> 01:43:43.660]   Thank you look out for supporting this week in tech.
[01:43:43.660 --> 01:43:45.620]   Thanks to all of you for supporting it.
[01:43:45.620 --> 01:43:46.620]   We had a great week.
[01:43:46.620 --> 01:43:49.620]   We put together a little video for you to watch.
[01:43:49.620 --> 01:43:59.220]   Well, it's certainly an incredible honor to have Governor Santos make this stark announcement.
[01:43:59.220 --> 01:44:00.220]   What the hell?
[01:44:00.220 --> 01:44:02.900]   What the hell is that?
[01:44:02.900 --> 01:44:05.380]   Please wait.
[01:44:05.380 --> 01:44:08.860]   Previously on Twitch, AI news, I mean, "Twit News."
[01:44:08.860 --> 01:44:14.740]   Next, we're bringing the core pilot to the biggest canvas of all, Windows.
[01:44:14.740 --> 01:44:17.620]   I mean, it looks like a command line.
[01:44:17.620 --> 01:44:20.740]   I click on that and now we'll pop up the Windows core pilot on the right.
[01:44:20.740 --> 01:44:22.900]   I always wanted to make that right sidebar do something.
[01:44:22.900 --> 01:44:24.460]   Just like a thing.
[01:44:24.460 --> 01:44:25.460]   All about AI.
[01:44:25.460 --> 01:44:26.460]   I mean, Android.
[01:44:26.460 --> 01:44:31.420]   We've got Matthew McCullough and Jamal Eason from the developer team at Google talking
[01:44:31.420 --> 01:44:36.380]   all about how AI integrates with their developer tools makes things easier for developers.
[01:44:36.380 --> 01:44:39.620]   We can recognize code, we can recognize dependencies and imports.
[01:44:39.620 --> 01:44:42.540]   Me times you find a code sub online, it's like, well, how do I get this above our project?
[01:44:42.540 --> 01:44:43.540]   How do I try to relate to this?
[01:44:43.540 --> 01:44:45.260]   Is this not going to build past our current compile?
[01:44:45.260 --> 01:44:46.980]   We thought about those things.
[01:44:46.980 --> 01:44:48.820]   This week in AI, I mean, Google.
[01:44:48.820 --> 01:44:55.740]   Our special guest, Chris Messina, the inventor of the hashtag and discoverer of so many amazing
[01:44:55.740 --> 01:44:56.740]   technologies.
[01:44:56.740 --> 01:45:01.380]   The kids coming up now who are in school, who are learning to work with chat, Chippity,
[01:45:01.380 --> 01:45:04.700]   will have a completely different set of assumptions about what the interaction paradigm should
[01:45:04.700 --> 01:45:06.980]   be when they pick up a device.
[01:45:06.980 --> 01:45:12.420]   AI Leo says, "Because we are living in a generative world and I'm a generative girl."
[01:45:12.420 --> 01:45:14.420]   Oh, this is my world.
[01:45:14.420 --> 01:45:15.420]   My world.
[01:45:15.420 --> 01:45:18.260]   This is really fresh.
[01:45:18.260 --> 01:45:24.260]   So they've got this AI Leo and they can make it say anything.
[01:45:24.260 --> 01:45:25.260]   That wasn't me.
[01:45:25.260 --> 01:45:26.700]   That was AI Leo.
[01:45:26.700 --> 01:45:28.940]   I'm living in a generative world.
[01:45:28.940 --> 01:45:29.940]   Please.
[01:45:29.940 --> 01:45:31.500]   Holy cow.
[01:45:31.500 --> 01:45:35.740]   I would never expect a Madonna reference from you.
[01:45:35.740 --> 01:45:36.740]   If I'm honest.
[01:45:36.740 --> 01:45:38.740]   Oh, come on.
[01:45:38.740 --> 01:45:39.980]   What about Taylor Swift?
[01:45:39.980 --> 01:45:40.980]   You know I'm a Swiftie.
[01:45:40.980 --> 01:45:41.980]   Oh, yeah.
[01:45:41.980 --> 01:45:42.980]   That I can see.
[01:45:42.980 --> 01:45:43.980]   That I can see.
[01:45:43.980 --> 01:45:47.340]   The general has warned that Taylor Swift may harm children and adolescence.
[01:45:47.340 --> 01:45:48.340]   Wait a minute.
[01:45:48.340 --> 01:45:49.340]   No, sorry.
[01:45:49.340 --> 01:45:50.340]   Social media.
[01:45:50.340 --> 01:45:56.140]   Although, Surgeon General, there are things in this world that could harm them much worse
[01:45:56.140 --> 01:45:59.180]   than spending a little time on Instagram.
[01:45:59.180 --> 01:46:05.900]   Dr. Vivek Murthy cited a profound risk of harmed adolescent mental health and urged
[01:46:05.900 --> 01:46:11.460]   families to freak out, no, to set limits.
[01:46:11.460 --> 01:46:17.700]   And governments to set tougher standards for use.
[01:46:17.700 --> 01:46:21.940]   I mean, he is the Surgeon General and I guess he's supposed to warn us about things like
[01:46:21.940 --> 01:46:22.940]   this.
[01:46:22.940 --> 01:46:25.100]   Seems like there are worse things in this world.
[01:46:25.100 --> 01:46:27.380]   Well, this also seems like 10 years too late.
[01:46:27.380 --> 01:46:34.700]   I mean, it was years ago that a leak revealed that Facebook's own researchers had found
[01:46:34.700 --> 01:46:37.220]   that social media harmed adolescents.
[01:46:37.220 --> 01:46:41.100]   I mean, Facebook knew this and did nothing.
[01:46:41.100 --> 01:46:44.860]   So I think there's a lot to be said for it's time for government to step in because the
[01:46:44.860 --> 01:46:49.020]   companies are, you know, it's not like they're ignorant of the problem.
[01:46:49.020 --> 01:46:51.580]   They're fully aware of the problem.
[01:46:51.580 --> 01:46:52.580]   They don't seem to care.
[01:46:52.580 --> 01:46:56.340]   They don't seem to be willing to take the steps that might, you know, cost them money.
[01:46:56.340 --> 01:46:57.340]   Well, they shouldn't.
[01:46:57.340 --> 01:46:58.500]   I mean, this is their business.
[01:46:58.500 --> 01:46:59.500]   They're never going to admit.
[01:46:59.500 --> 01:47:00.500]   Oh, yeah.
[01:47:00.500 --> 01:47:04.140]   We're the, it's like fentanyl saying, yeah, I'm the problem.
[01:47:04.140 --> 01:47:06.220]   It's not going to admit it.
[01:47:06.220 --> 01:47:08.220]   That Taylor Swift reference.
[01:47:08.220 --> 01:47:09.220]   Yes.
[01:47:09.220 --> 01:47:11.100]   Facebook clicking the mirror and saying it's me.
[01:47:11.100 --> 01:47:12.100]   I'm the problem.
[01:47:12.100 --> 01:47:13.100]   It's me.
[01:47:13.100 --> 01:47:18.300]   But, but I don't know if government is the right tool to fix this.
[01:47:18.300 --> 01:47:24.020]   Montana has signed a bill banning TikTok starting on January 1st.
[01:47:24.020 --> 01:47:25.540]   Is that the right answer?
[01:47:25.540 --> 01:47:30.060]   Well, that's, you know, that has nothing to do with protecting children, right?
[01:47:30.060 --> 01:47:35.540]   It's about performative geopolitics at a, at a state level scale, which makes no sense
[01:47:35.540 --> 01:47:41.860]   given that our union, you know, seeds foreign relations to, right, to the federal government.
[01:47:41.860 --> 01:47:42.860]   Right.
[01:47:42.860 --> 01:47:44.860]   Montana should not be in this business.
[01:47:44.860 --> 01:47:50.460]   And it almost certainly violates the free speech rights of TikTok users in Montana.
[01:47:50.460 --> 01:47:54.140]   TikTok creators and TikTok itself of both sued.
[01:47:54.140 --> 01:47:57.860]   It will, I mean, on, on first amendment grounds alone, it'll probably get thrown out.
[01:47:57.860 --> 01:48:02.060]   Nevertheless, a lot of people that think TikTok is a, you know, a.
[01:48:02.060 --> 01:48:03.060]   Yeah.
[01:48:03.060 --> 01:48:06.980]   I think that, you know, the question of whether a state should have the authority to ban an
[01:48:06.980 --> 01:48:11.380]   app is separate from, is that app actively harming people?
[01:48:11.380 --> 01:48:15.860]   I also, you know, I also think that like humans are our own worst enemies.
[01:48:15.860 --> 01:48:21.980]   We will find ways to hurt each other, whether it's in, in person in writing, you know, the
[01:48:21.980 --> 01:48:28.380]   founding fathers used to like rip each other, you know, rip, rip, rip each other apart in
[01:48:28.380 --> 01:48:30.740]   like pamphlets that they circulated.
[01:48:30.740 --> 01:48:35.700]   If you've, you know, if you've watched Hamilton, you know, a little bit about this.
[01:48:35.700 --> 01:48:40.540]   But to the point, I mean, aside from being a decade too late, I think, I think there is
[01:48:40.540 --> 01:48:45.340]   a lot of value here to the Surgeon General, even, you know, even if it doesn't lead to
[01:48:45.340 --> 01:48:49.820]   regulation, just saying this is a real problem.
[01:48:49.820 --> 01:48:55.540]   The, the flip side is that this horse is out of the barn.
[01:48:55.540 --> 01:48:57.900]   Teens are going to use social media.
[01:48:57.900 --> 01:49:04.100]   All we need to do is adapt to it and feed, you know, like figure out how to, you know,
[01:49:04.100 --> 01:49:10.100]   figure out how to get our, you know, our youth, like in a healthy place where they can be
[01:49:10.100 --> 01:49:16.060]   on social media, know how to take it with a certain lightness and grace.
[01:49:16.060 --> 01:49:22.540]   And, you know, and, and also behave appropriately, because, you know, a lot of the problem is,
[01:49:22.540 --> 01:49:24.540]   you know, teens bullying other teens.
[01:49:24.540 --> 01:49:32.420]   You know, it's mean girls on, you know, internet, you know, on internet steroids.
[01:49:32.420 --> 01:49:37.620]   I have to point out the number one cause of death among children and adolescents in the
[01:49:37.620 --> 01:49:40.300]   United States is firearms.
[01:49:40.300 --> 01:49:44.100]   They're not doing anything about that.
[01:49:44.100 --> 01:49:47.260]   I think the Surgeon General put out a warning about firearms a lot of time ago.
[01:49:47.260 --> 01:49:48.540]   The firearms will kill you.
[01:49:48.540 --> 01:49:49.540]   So be careful.
[01:49:49.540 --> 01:49:50.540]   Yeah.
[01:49:50.540 --> 01:49:51.540]   Yeah.
[01:49:51.540 --> 01:49:54.820]   So this is actually, you know, it's a public health disaster.
[01:49:54.820 --> 01:49:56.140]   Firearms fall into that category.
[01:49:56.140 --> 01:49:59.860]   And there's been a lot of efforts at state and federal level to declare it that.
[01:49:59.860 --> 01:50:05.020]   And it's funny, the social media thing might fall in the same category where it's like,
[01:50:05.020 --> 01:50:08.980]   yeah, we all think, we all, but a significant number of people think it's a, it's a bad
[01:50:08.980 --> 01:50:11.420]   idea to laugh under federal access.
[01:50:11.420 --> 01:50:16.980]   But firearms a different issue in that regard, but how do you provide it with the, how do
[01:50:16.980 --> 01:50:21.380]   you not do it something that violates the constitution, whether or not you support the
[01:50:21.380 --> 01:50:25.140]   full range of it and the second amendment or really want it in the first.
[01:50:25.140 --> 01:50:26.380]   Well, I guess you're right.
[01:50:26.380 --> 01:50:31.660]   TikTok is not protected by the constitution, but the first amendment does kind of cover
[01:50:31.660 --> 01:50:32.660]   this.
[01:50:32.660 --> 01:50:33.660]   Yeah.
[01:50:33.660 --> 01:50:36.380]   Well, corporate, I mean, this is the double edged sort of giving corporate free speech
[01:50:36.380 --> 01:50:41.020]   rights is that corporations have expanded rights and they, they would have 20 to 50
[01:50:41.020 --> 01:50:45.220]   years ago that allowed them to defend themselves in that way.
[01:50:45.220 --> 01:50:49.420]   It's ironic when free speech maximalists are saying that corporations like Disney can't
[01:50:49.420 --> 01:50:51.660]   say what they want to say.
[01:50:51.660 --> 01:50:52.660]   It's sort of a funny thing.
[01:50:52.660 --> 01:50:55.940]   I mean, I guess it's always a, you don't need to freedom from repercussion is not what
[01:50:55.940 --> 01:50:59.660]   the first amendment says, the freedom from government intervention in your right to have
[01:50:59.660 --> 01:51:00.660]   speech.
[01:51:00.660 --> 01:51:05.260]   But I don't know, when we kept our kids off devices, we did not have full time devices
[01:51:05.260 --> 01:51:09.460]   until they were in their early teens and they didn't have access to devices overnight.
[01:51:09.460 --> 01:51:10.460]   And we set limits.
[01:51:10.460 --> 01:51:12.380]   That's probably a good idea.
[01:51:12.380 --> 01:51:17.460]   Contract with them that schools had suggested about usage and a couple of times like, oh,
[01:51:17.460 --> 01:51:18.900]   set up an Instagram account with that time.
[01:51:18.900 --> 01:51:21.060]   You were like, all right, well, we're going to disable it, but we're not going to, you
[01:51:21.060 --> 01:51:22.980]   know, you tell what's fine.
[01:51:22.980 --> 01:51:27.380]   And so now, of course, when I got off Twitter, they both got on both my 16 year old, 18 year
[01:51:27.380 --> 01:51:32.380]   old, just for the laws, but they're mature enough to deal with it because they laugh
[01:51:32.380 --> 01:51:36.380]   at Twitter as opposed to are sucked into its dramas.
[01:51:36.380 --> 01:51:44.500]   I would point out that there's an interesting new legal avenue for kind of, you know, like
[01:51:44.500 --> 01:51:48.500]   holding social media companies accountable for their actions.
[01:51:48.500 --> 01:51:52.420]   And it's, you know, it's not free speech because that's pretty kind of dried.
[01:51:52.420 --> 01:51:57.580]   And it's not section 230 because that's proved very tricky to kind of take apart.
[01:51:57.580 --> 01:52:02.340]   And Mike Masnick, among others, has laid out a very strong argument for why you don't
[01:52:02.340 --> 01:52:07.780]   want to, you know, take apart section 230 just to go after social media companies.
[01:52:07.780 --> 01:52:14.460]   The product liability has been an interesting kind of legal tactic.
[01:52:14.460 --> 01:52:19.620]   Snap, for example, was found liable, I believe, I'm not sure if this was a final ruling, but
[01:52:19.620 --> 01:52:24.100]   in some early rulings in the case, Snap was found liable for creating a filter that lets
[01:52:24.100 --> 01:52:28.140]   you post how fast you were going when you took a snap.
[01:52:28.140 --> 01:52:29.140]   Oh, yeah.
[01:52:29.140 --> 01:52:30.140]   Yeah.
[01:52:30.140 --> 01:52:31.140]   Oh, yeah.
[01:52:31.140 --> 01:52:34.020]   I use that on the bullet train in Japan, but also a number of people use it while they
[01:52:34.020 --> 01:52:35.020]   were driving.
[01:52:35.020 --> 01:52:36.020]   Yes.
[01:52:36.020 --> 01:52:43.100]   And, you know, and the plaintiffs lawyers argued that this encouraged people to speed to kind
[01:52:43.100 --> 01:52:44.340]   of show off.
[01:52:44.340 --> 01:52:48.900]   And I think that, you know, I think that this could be, you know, it could actually be a
[01:52:48.900 --> 01:52:54.060]   valuable corrective for people to pursue product liability claims against these social media
[01:52:54.060 --> 01:52:57.060]   companies to say, Snap, by the way, just continue to do this.
[01:52:57.060 --> 01:53:01.260]   To the design of that filter and probably would have even with absent a court case.
[01:53:01.260 --> 01:53:03.860]   I mean, I think these companies try, Snap does.
[01:53:03.860 --> 01:53:06.860]   I know they try to be good citizens.
[01:53:06.860 --> 01:53:11.860]   Well, I think should it have existed in the first place?
[01:53:11.860 --> 01:53:14.900]   Well, I think that mistakes happen.
[01:53:14.900 --> 01:53:21.820]   Well, Kara Swisher talks about how Facebook showed them, you know, their live video broadcasting
[01:53:21.820 --> 01:53:22.820]   features.
[01:53:22.820 --> 01:53:27.500]   And, you know, she said, you know, someone is going to shoot someone live on this.
[01:53:27.500 --> 01:53:28.860]   And they said, what are you talking about?
[01:53:28.860 --> 01:53:32.660]   That's, you know, like they didn't, it wasn't even conceivable to that.
[01:53:32.660 --> 01:53:33.660]   She was right.
[01:53:33.660 --> 01:53:34.660]   Yeah.
[01:53:34.660 --> 01:53:35.660]   She was absolutely right.
[01:53:35.660 --> 01:53:36.660]   Yeah.
[01:53:36.660 --> 01:53:44.780]   Jeff Jarvis, I'm going to channel our host from this week in Google posted this text from
[01:53:44.780 --> 01:53:48.100]   the Surgeon General 51 years ago.
[01:53:48.100 --> 01:53:53.500]   All the available statistics confirm the pervasive role television plays in the United
[01:53:53.500 --> 01:53:55.900]   States, if not throughout the world.
[01:53:55.900 --> 01:53:58.900]   More people own television sets and more people watch television than make use of any
[01:53:58.900 --> 01:54:03.260]   other single form of mass communication.
[01:54:03.260 --> 01:54:06.700]   The question of violence and television has been one issue that was raised almost immediately
[01:54:06.700 --> 01:54:11.700]   after television became a major contender, you know, warning about television.
[01:54:11.700 --> 01:54:16.580]   But after that, they warned about video games, violent video games.
[01:54:16.580 --> 01:54:22.660]   There's always been a new cultural phenomenon that we old people look at and say, you kids
[01:54:22.660 --> 01:54:26.260]   in your rock and roll, it's running your mind.
[01:54:26.260 --> 01:54:29.300]   It reminds me the, the pessimist archive.
[01:54:29.300 --> 01:54:30.300]   That's all they do.
[01:54:30.300 --> 01:54:35.340]   They show a lot of that where they, they'll show how books we're going to take down society.
[01:54:35.340 --> 01:54:36.340]   Yeah.
[01:54:36.340 --> 01:54:38.420]   And it was, you know, then it was the bicycle.
[01:54:38.420 --> 01:54:39.420]   Yeah.
[01:54:39.420 --> 01:54:45.100]   So there is, there is a certain irony here that, and we need to be careful that, yeah,
[01:54:45.100 --> 01:54:46.740]   we're just now overreacting to what's new.
[01:54:46.740 --> 01:54:51.740]   But I think social media is clearly different in some ways, right?
[01:54:51.740 --> 01:54:58.540]   The fact that it instantly accesses millions of people and its ability to spread disinformation
[01:54:58.540 --> 01:55:02.460]   or violence, you know, there was, I even hate talking about the story a couple of weeks
[01:55:02.460 --> 01:55:07.260]   ago with the torture, the animal torture videos on Twitter, you took down there, you know,
[01:55:07.260 --> 01:55:10.140]   like, I mean, no kids should ever see that.
[01:55:10.140 --> 01:55:11.140]   That's horrific.
[01:55:11.140 --> 01:55:12.140]   Right.
[01:55:12.140 --> 01:55:14.540]   I mean, that's worse than you were reading about it.
[01:55:14.540 --> 01:55:16.340]   I had like PTSD for a day.
[01:55:16.340 --> 01:55:17.340]   Yeah.
[01:55:17.340 --> 01:55:18.340]   Yeah.
[01:55:18.340 --> 01:55:19.340]   Yeah.
[01:55:19.340 --> 01:55:21.260]   And reading about it wasn't so disturbing.
[01:55:21.260 --> 01:55:23.900]   I can imagine children who saw that stuff.
[01:55:23.900 --> 01:55:28.140]   And it's like, what do you do about that, especially since currently, you know, with
[01:55:28.140 --> 01:55:32.940]   Elon, you know, I hate that we keep picking on him, but yeah, there's no reaction to this,
[01:55:32.940 --> 01:55:33.940]   right?
[01:55:33.940 --> 01:55:35.460]   There doesn't seem to be a concern.
[01:55:35.460 --> 01:55:39.660]   And then you have like with the Republicans, it's like, you have free speech and you can
[01:55:39.660 --> 01:55:41.940]   do whatever you want, but then TikTok's Chinese.
[01:55:41.940 --> 01:55:43.180]   So maybe we should ban it.
[01:55:43.180 --> 01:55:44.180]   Yeah.
[01:55:44.180 --> 01:55:46.500]   It's like, you know, it's all over the place.
[01:55:46.500 --> 01:55:50.060]   And I don't, meanwhile though, and to be fair, in China, what do they do?
[01:55:50.060 --> 01:55:51.060]   Right?
[01:55:51.060 --> 01:55:52.060]   They ban social media.
[01:55:52.060 --> 01:55:53.060]   They don't let the kids on stuff.
[01:55:53.060 --> 01:55:56.340]   They are limited to gaming just for a couple hours a week.
[01:55:56.340 --> 01:55:57.980]   And they force the kids to learn stuff.
[01:55:57.980 --> 01:56:03.620]   So their children are all like, you know, focused, education, engineering.
[01:56:03.620 --> 01:56:06.100]   And our kids are just like on social media all the time.
[01:56:06.100 --> 01:56:07.900]   I mean, TikTok's not even in China.
[01:56:07.900 --> 01:56:09.300]   I always thought that was interesting.
[01:56:09.300 --> 01:56:11.700]   Well, they have their own version of it.
[01:56:11.700 --> 01:56:12.700]   Yeah.
[01:56:12.700 --> 01:56:13.700]   Yeah.
[01:56:13.700 --> 01:56:14.700]   But TikTok is for the rest of the world, right?
[01:56:14.700 --> 01:56:15.700]   Right.
[01:56:15.700 --> 01:56:17.620]   And it's a fascinating thing.
[01:56:17.620 --> 01:56:20.580]   And I don't, you know, yeah, we're the first in that myth.
[01:56:20.580 --> 01:56:24.060]   So you can't just ban stuff, but we should ban candy.
[01:56:24.060 --> 01:56:25.060]   What is it?
[01:56:25.060 --> 01:56:26.060]   Rots your teeth.
[01:56:26.060 --> 01:56:27.060]   Right.
[01:56:27.060 --> 01:56:28.060]   Hell yeah.
[01:56:28.060 --> 01:56:29.060]   Hell yeah.
[01:56:29.060 --> 01:56:30.060]   No candy.
[01:56:30.060 --> 01:56:34.340]   I mean, you know, like as, as mayor of New York, Mike Bloomberg pushed the, oh yeah.
[01:56:34.340 --> 01:56:35.340]   He didn't want to bottle.
[01:56:35.340 --> 01:56:36.340]   He's a very beverage.
[01:56:36.340 --> 01:56:37.340]   I'll have to pop.
[01:56:37.340 --> 01:56:38.340]   Yeah.
[01:56:38.340 --> 01:56:39.340]   Here.
[01:56:39.340 --> 01:56:43.220]   By the way, thank you for referencing for name checking the pessimist archive, Daniel.
[01:56:43.220 --> 01:56:44.220]   This is fantastic.
[01:56:44.220 --> 01:56:45.220]   Yeah.
[01:56:45.220 --> 01:56:46.220]   Look at this one.
[01:56:46.220 --> 01:56:49.100]   Teddy bears doom race.
[01:56:49.100 --> 01:56:51.300]   The teddy bear is a, not kidding.
[01:56:51.300 --> 01:56:52.300]   1907.
[01:56:52.300 --> 01:56:57.580]   The teddy bear is a social menace and aater and a better of race suicide.
[01:56:57.580 --> 01:57:01.820]   Uh, I mean, we've always had this moral panic.
[01:57:01.820 --> 01:57:04.200]   I'm going to send this site to Jeff Jarvis.
[01:57:04.200 --> 01:57:05.460]   This is fantastic.
[01:57:05.460 --> 01:57:06.460]   Everything.
[01:57:06.460 --> 01:57:09.460]   Here's, you know, comic books.
[01:57:09.460 --> 01:57:11.340]   Oh my God.
[01:57:11.340 --> 01:57:16.300]   Comic good books called the answer to comic book addiction.
[01:57:16.300 --> 01:57:25.060]   Kids today, evil communications, comic books give Tots bad habits from 1949 and on and
[01:57:25.060 --> 01:57:26.060]   on and on.
[01:57:26.060 --> 01:57:27.060]   This is a great question.
[01:57:27.060 --> 01:57:28.060]   I don't think there's any bad, bad media.
[01:57:28.060 --> 01:57:29.820]   I think it's just bad parenting.
[01:57:29.820 --> 01:57:30.820]   There's my state.
[01:57:30.820 --> 01:57:31.820]   That's true too.
[01:57:31.820 --> 01:57:32.820]   Right.
[01:57:32.820 --> 01:57:35.340]   I mean, I'm being, I'm being evil by saying that, but it's reasonable.
[01:57:35.340 --> 01:57:41.300]   I guess if there's a hazard to all children that parents, you know, maybe, you know, maybe
[01:57:41.300 --> 01:57:46.540]   having trouble dealing with that the government could step in, you know, you don't, you, you
[01:57:46.540 --> 01:57:51.940]   want to make sure you label Tylenol, not appropriate for children under a certain age.
[01:57:51.940 --> 01:57:52.940]   I don't know.
[01:57:52.940 --> 01:57:57.660]   Tylen, I, I literally have a friend who lost her daughter to, to Tylenol overdose.
[01:57:57.660 --> 01:57:58.660]   Wow.
[01:57:58.660 --> 01:58:02.020]   I mean, it's a deadly, it's a terrible deadly medicine.
[01:58:02.020 --> 01:58:03.020]   Um, so.
[01:58:03.020 --> 01:58:07.140]   Parenting thing is true, but it's such a complicated issue, right?
[01:58:07.140 --> 01:58:12.540]   Just now, like when I grew up in the eighties, like I had a mom at home for a while, like
[01:58:12.540 --> 01:58:16.940]   parents were allowed to parent their kids and everybody's so busy and, you know, there's
[01:58:16.940 --> 01:58:17.980]   so much going on.
[01:58:17.980 --> 01:58:21.180]   Like even like everybody, both parents have to often work.
[01:58:21.180 --> 01:58:25.900]   It's like, it's very hard for parents to be true parents in that sense.
[01:58:25.900 --> 01:58:29.260]   So I, I, even that I don't know what to, they can do that.
[01:58:29.260 --> 01:58:30.860]   They can handle all this.
[01:58:30.860 --> 01:58:35.620]   I don't know how women have survived the telephone.
[01:58:35.620 --> 01:58:39.340]   This is from 1917 to talking machines.
[01:58:39.340 --> 01:58:45.500]   A woman at a telephone are to blame for Soused husbands and four dollar potatoes.
[01:58:45.500 --> 01:58:49.940]   That's the explanation for wrecked homes in the high cost of living being given out by
[01:58:49.940 --> 01:58:54.580]   Mrs. John C. Bly president of the housewives league.
[01:58:54.580 --> 01:58:56.080]   So there you go.
[01:58:56.080 --> 01:58:57.080]   There's the problem.
[01:58:57.080 --> 01:58:58.080]   The telephone.
[01:58:58.080 --> 01:58:59.080]   I don't know.
[01:58:59.080 --> 01:59:00.080]   I make light of this.
[01:59:00.080 --> 01:59:01.080]   This is a great site.
[01:59:01.080 --> 01:59:02.080]   I just lie.
[01:59:02.080 --> 01:59:03.080]   I could go on.
[01:59:03.080 --> 01:59:04.240]   I make light of this.
[01:59:04.240 --> 01:59:10.180]   I just, it bothers me the, the attorney general, I'm sorry, the surgeon general.
[01:59:10.180 --> 01:59:14.500]   Thank God he's not the attorney general is focusing on this when there are so many other
[01:59:14.500 --> 01:59:17.380]   hazards to young people.
[01:59:17.380 --> 01:59:18.380]   I'm not sure.
[01:59:18.380 --> 01:59:24.100]   Well, we can, you know, like we can do more than, you know, more than one.
[01:59:24.100 --> 01:59:25.100]   I agree.
[01:59:25.100 --> 01:59:26.100]   I agree.
[01:59:26.100 --> 01:59:31.940]   Um, but I think it's overstating the case to say, for instance, the mental health issues
[01:59:31.940 --> 01:59:36.980]   in young women in America today are due to Instagram and Facebook.
[01:59:36.980 --> 01:59:39.040]   I mean, that seems a stretch to me.
[01:59:39.040 --> 01:59:43.820]   Well, it's like, it's like, uh, accelerants are always a problem and they're hard to measure.
[01:59:43.820 --> 01:59:44.820]   Yes.
[01:59:44.820 --> 01:59:46.700]   For some people, it has no effect on other people.
[01:59:46.700 --> 01:59:51.820]   Uh, you know, it can be a devastating thing that it wouldn't otherwise have encountered.
[01:59:51.820 --> 01:59:56.660]   And so the question is, is the greater exposure to things that otherwise someone might not
[01:59:56.660 --> 01:59:57.660]   have seen?
[01:59:57.660 --> 01:59:58.740]   Is that worse than in real life?
[01:59:58.740 --> 02:00:02.940]   If the internet didn't exist, would they be facing the same hazards at this time?
[02:00:02.940 --> 02:00:04.780]   You can't really do an A/B test.
[02:00:04.780 --> 02:00:08.820]   Um, on that, I mean, we talked to our children, frankly, about drugs and it's like, we're
[02:00:08.820 --> 02:00:11.540]   like, look, fentanyl is a different category.
[02:00:11.540 --> 02:00:14.620]   So, you know, Instagram and Facebook didn't exist when we were children.
[02:00:14.620 --> 02:00:16.180]   They did fentanyl in this variety.
[02:00:16.180 --> 02:00:19.260]   And it's like fentanyl is destroying people and killing people.
[02:00:19.260 --> 02:00:23.100]   And it's, it's not like pot or even it's not even like heroin.
[02:00:23.100 --> 02:00:26.980]   It's not even like other of these super hard drugs when we were kids.
[02:00:26.980 --> 02:00:30.620]   Uh, you know, some of which were circulating around, I knew people who were engaged with.
[02:00:30.620 --> 02:00:32.220]   It's like, fentanyl is a whole new thing.
[02:00:32.220 --> 02:00:33.860]   So how do you respond to fentanyl?
[02:00:33.860 --> 02:00:37.900]   I think there is a little bit, not that Facebook is fentanyl, but it's like, there is a little
[02:00:37.900 --> 02:00:42.420]   bit of that like, well, it's a million times more powerful than the telephone.
[02:00:42.420 --> 02:00:43.900]   They're the comic book.
[02:00:43.900 --> 02:00:44.900]   Yeah.
[02:00:44.900 --> 02:00:48.020]   And it's free and your parents don't want you to do it.
[02:00:48.020 --> 02:00:50.540]   So I think you did the very sensible thing.
[02:00:50.540 --> 02:00:51.540]   And we used to do that.
[02:00:51.540 --> 02:00:53.220]   We turn off the router at 10 p.m.
[02:00:53.220 --> 02:00:55.860]   Because kids will, if they are, you know, it's addictive.
[02:00:55.860 --> 02:00:57.020]   I'll stay up all night.
[02:00:57.020 --> 02:01:01.900]   If I, if I start looking at Instagram, I can see doom scrolling and all that.
[02:01:01.900 --> 02:01:03.980]   And, uh, you know, I mean, there's certain things you can do.
[02:01:03.980 --> 02:01:05.540]   I've, fortunately, my kids are a little bit older.
[02:01:05.540 --> 02:01:07.180]   I didn't have the same problems.
[02:01:07.180 --> 02:01:14.260]   Um, Dr. Merti has also decried the public health crisis of loneliness, isolation and
[02:01:14.260 --> 02:01:16.300]   the lack of connection in our country.
[02:01:16.300 --> 02:01:19.140]   I think that's true.
[02:01:19.140 --> 02:01:23.300]   I don't know if it's government's job to fix it, I guess is my question.
[02:01:23.300 --> 02:01:24.300]   Yeah.
[02:01:24.300 --> 02:01:25.300]   Well, I think just highlighting it is important.
[02:01:25.300 --> 02:01:26.300]   Yeah.
[02:01:26.300 --> 02:01:28.340]   You know, Scott, he talks about this all the time.
[02:01:28.340 --> 02:01:34.500]   And I think it's, it's true, you know, like between the pandemic and the changing social
[02:01:34.500 --> 02:01:41.220]   structures and, you know, how men is being perceived, like there's a lot of moving parts
[02:01:41.220 --> 02:01:42.380]   here.
[02:01:42.380 --> 02:01:44.460]   And I take it where I love working from home.
[02:01:44.460 --> 02:01:47.620]   I love being an introvert, being a shut in, but I'm also my forties.
[02:01:47.620 --> 02:01:48.620]   I've done my talk.
[02:01:48.620 --> 02:01:49.620]   Right.
[02:01:49.620 --> 02:01:52.500]   I wasn't like that in my teens and twenties, but if you're in your teens and 20, those
[02:01:52.500 --> 02:01:55.740]   are formative years to build these skills out.
[02:01:55.740 --> 02:01:58.740]   And if you're not getting them, it's like it can cause a lot of problems.
[02:01:58.740 --> 02:02:03.140]   And there's that issue with, uh, I forgot the name already, but they have in Japan with,
[02:02:03.140 --> 02:02:05.140]   uh, pillow, pillow brides.
[02:02:05.140 --> 02:02:06.140]   Yeah.
[02:02:06.140 --> 02:02:10.020]   It's called like, I've got the name of it, but there's a name for it.
[02:02:10.020 --> 02:02:11.020]   Yeah.
[02:02:11.020 --> 02:02:12.020]   Yeah.
[02:02:12.020 --> 02:02:13.020]   They don't leave the house.
[02:02:13.020 --> 02:02:14.020]   Oh, yeah.
[02:02:14.020 --> 02:02:15.020]   Yeah.
[02:02:15.020 --> 02:02:16.020]   Yeah.
[02:02:16.020 --> 02:02:17.020]   Yeah.
[02:02:17.020 --> 02:02:18.020]   Yeah.
[02:02:18.020 --> 02:02:19.020]   Yeah.
[02:02:19.020 --> 02:02:20.020]   They pull out of society, basically.
[02:02:20.020 --> 02:02:21.020]   And that's starting to happen here for different reasons.
[02:02:21.020 --> 02:02:25.300]   There's a lot of other ways going up and young men because of now it's like body image stuff
[02:02:25.300 --> 02:02:28.900]   going on with men too, because everybody wants to, you know, they go on Instagram and they
[02:02:28.900 --> 02:02:30.420]   see all these guys who are jacked.
[02:02:30.420 --> 02:02:33.780]   And it's just like, there's a lot of stuff that's going on, which is why I'm not a parent,
[02:02:33.780 --> 02:02:38.340]   by the way, because even I'm like, well, next I, I don't know how I would handle this stuff
[02:02:38.340 --> 02:02:39.340]   either.
[02:02:39.340 --> 02:02:40.340]   I think it's very complicated.
[02:02:40.340 --> 02:02:44.180]   It's called, and by the way, he is making, go ahead.
[02:02:44.180 --> 02:02:45.180]   Oh, sorry.
[02:02:45.180 --> 02:02:47.140]   AI is making all of that worse, right?
[02:02:47.140 --> 02:02:48.140]   Right.
[02:02:48.140 --> 02:02:49.140]   Well, the filter.
[02:02:49.140 --> 02:02:52.180]   It makes everything worse, everything.
[02:02:52.180 --> 02:02:56.060]   It's called, and everything better too, at the same time.
[02:02:56.060 --> 02:02:57.060]   It's so amazing.
[02:02:57.060 --> 02:03:00.260]   It's like, listen, if I want to go on Instagram and see somebody who's jacked, then just go
[02:03:00.260 --> 02:03:02.260]   on and look at it when Wilson, I mean, come on.
[02:03:02.260 --> 02:03:03.260]   I mean, when Thomas, sorry.
[02:03:03.260 --> 02:03:08.940]   When Wilson, I had that joke all prepped that I still miss over.
[02:03:08.940 --> 02:03:14.980]   It's called Hikiko Mori, a Japanese culture-bound syndrome of social withdrawal.
[02:03:14.980 --> 02:03:19.420]   This is an article from the National Institutes of Health, recommending it be added to the
[02:03:19.420 --> 02:03:24.660]   DSM, the diagnostic manual for a mental illness.
[02:03:24.660 --> 02:03:26.620]   I don't know if that's, I don't know.
[02:03:26.620 --> 02:03:28.500]   Is it a new psychiatric disorder?
[02:03:28.500 --> 02:03:29.980]   I don't know.
[02:03:29.980 --> 02:03:34.420]   Next week, we'll talk, we didn't, I meant to, I thought I would with you, Glenn, talk
[02:03:34.420 --> 02:03:38.900]   about what Apple's going to do a week from Monday, but I guess we'll do that next Sunday.
[02:03:38.900 --> 02:03:41.100]   That's going to be, I might lose my bet.
[02:03:41.100 --> 02:03:42.620]   You might have to eat a half of some money.
[02:03:42.620 --> 02:03:46.500]   You just think that Apple is not going to release a VR helmet or what?
[02:03:46.500 --> 02:03:50.180]   Well, my argument was I didn't think they were going to lease one as a product that
[02:03:50.180 --> 02:03:53.740]   would ship this year for a reasonable sum of money, but I thought it's possible they'd
[02:03:53.740 --> 02:03:58.580]   do a developer release that would be anchored to a computer and only available to developers
[02:03:58.580 --> 02:03:59.780]   and in limited quantities.
[02:03:59.780 --> 02:04:05.220]   And so I think I'm right about that second part, but all the word that's coming out from
[02:04:05.220 --> 02:04:09.900]   people who seem to know birdies inside the company, which only happens when things are
[02:04:09.900 --> 02:04:14.540]   going actually close to being released that this information comes out seems like Apple
[02:04:14.540 --> 02:04:17.540]   will be ready to ship it this year as a consumer product.
[02:04:17.540 --> 02:04:19.860]   I still think it's ridiculous.
[02:04:19.860 --> 02:04:25.260]   I don't see how they're going to make it work, but I could very well be on.
[02:04:25.260 --> 02:04:29.300]   So let's just to get this on record.
[02:04:29.300 --> 02:04:30.300]   Exactly right.
[02:04:30.300 --> 02:04:32.060]   You're not saying they won't announce something.
[02:04:32.060 --> 02:04:33.540]   You're not saying it won't be a developer product.
[02:04:33.540 --> 02:04:36.940]   You just don't think it will ship as a consumer product in 2023.
[02:04:36.940 --> 02:04:39.720]   Yeah, I don't think it'll be out this year.
[02:04:39.720 --> 02:04:44.420]   It doesn't seem right from like a like like who would buy it is what you're saying.
[02:04:44.420 --> 02:04:47.100]   Yeah, well that and physics and hardware and what's developed for it.
[02:04:47.100 --> 02:04:50.620]   I don't think that developers get the thing if Apple and us in June, even if they're ready
[02:04:50.620 --> 02:04:55.820]   to ship a developer or test version, which they've done for multiple previous products
[02:04:55.820 --> 02:05:04.660]   in major transitions over time, then it's one thing to port software across generations
[02:05:04.660 --> 02:05:08.980]   of Intel hardware, Intel, and one or whatever, those things have happened before.
[02:05:08.980 --> 02:05:14.260]   It's another to develop stuff that's entirely new and compelling in an AR/VR environment.
[02:05:14.260 --> 02:05:16.460]   And I just don't think there's time to make it happen.
[02:05:16.460 --> 02:05:20.020]   But I also just don't think the hardware could exist the way it's described.
[02:05:20.020 --> 02:05:24.180]   It's either ridiculous if they're going to ship what's being described widely.
[02:05:24.180 --> 02:05:26.780]   Sounds like an absurd Homer Simpson's car to me.
[02:05:26.780 --> 02:05:28.300]   It's not that.
[02:05:28.300 --> 02:05:32.460]   I don't think the technology exists to make a thing that won't be like that.
[02:05:32.460 --> 02:05:34.540]   So that's my thinking.
[02:05:34.540 --> 02:05:35.540]   I'm not disagreeing.
[02:05:35.540 --> 02:05:36.540]   I'm not betting against you.
[02:05:36.540 --> 02:05:37.540]   Let me be wrong.
[02:05:37.540 --> 02:05:38.540]   I am not betting against you.
[02:05:38.540 --> 02:05:43.620]   I think you might be wrong, but not because you're not right.
[02:05:43.620 --> 02:05:49.620]   You're wrong because Apple's executives have put so much wood in those arrows that they
[02:05:49.620 --> 02:05:51.900]   can't pull back at this point.
[02:05:51.900 --> 02:05:56.940]   So even though you're absolutely right, Apple should not do this.
[02:05:56.940 --> 02:05:58.140]   They're going to do it.
[02:05:58.140 --> 02:06:01.900]   And I think it's going to be the worst product mistake Apple has ever made in its history.
[02:06:01.900 --> 02:06:02.900]   I mean, it's going to be better.
[02:06:02.900 --> 02:06:03.900]   Wow.
[02:06:03.900 --> 02:06:05.460]   Picker than the Newton.
[02:06:05.460 --> 02:06:06.540]   The Newton wasn't a mistake.
[02:06:06.540 --> 02:06:07.860]   I got four of them right there.
[02:06:07.860 --> 02:06:11.300]   The Newton was ahead of its time.
[02:06:11.300 --> 02:06:12.300]   Yeah.
[02:06:12.300 --> 02:06:17.900]   No, the Newton, you could reasonably say we wouldn't have the iPhone except for the Newton.
[02:06:17.900 --> 02:06:20.220]   Like the Newton was a precursor.
[02:06:20.220 --> 02:06:24.700]   Maybe this will be the Newton for what is to come in AR.
[02:06:24.700 --> 02:06:26.260]   That's my reaction.
[02:06:26.260 --> 02:06:31.260]   Keeps being if what I keep reading about being described is what were to ship.
[02:06:31.260 --> 02:06:32.820]   It seems like a complete failure.
[02:06:32.820 --> 02:06:36.220]   So I can't believe what they're going to ship will be what I keep reading about, in which
[02:06:36.220 --> 02:06:40.340]   case they have to violate the laws of physics or they just have some really clever hardware
[02:06:40.340 --> 02:06:45.300]   innovation we haven't seen before that it's going to wow people and I'm willing to be wrong
[02:06:45.300 --> 02:06:46.300]   about that because I don't know.
[02:06:46.300 --> 02:06:51.900]   I'm not in chip factories in China and across the US and Germany or something looking at
[02:06:51.900 --> 02:06:52.900]   what's being made.
[02:06:52.900 --> 02:06:56.900]   So it's plausible that it's going to be something that's going to be extraordinary and I'm just
[02:06:56.900 --> 02:06:57.900]   not ready for it.
[02:06:57.900 --> 02:06:59.740]   The future shock will hit me there.
[02:06:59.740 --> 02:07:01.260]   No, I think you're absolutely right.
[02:07:01.260 --> 02:07:05.100]   I'm just trying to find the sales figures for the Newton.
[02:07:05.100 --> 02:07:06.420]   So that's the only question.
[02:07:06.420 --> 02:07:08.780]   Will it be a bigger flop than the Newton?
[02:07:08.780 --> 02:07:09.780]   I like that.
[02:07:09.780 --> 02:07:11.220]   I'm not going to do an Apple Q.
[02:07:11.220 --> 02:07:12.220]   I was G4Q.
[02:07:12.220 --> 02:07:14.780]   Oh, I bought the cube on the last day.
[02:07:14.780 --> 02:07:16.940]   I got it home and the news came out.
[02:07:16.940 --> 02:07:19.340]   They were discontinuing it.
[02:07:19.340 --> 02:07:25.060]   The question is, do you think they're going to talk about AI because they've been so quiet
[02:07:25.060 --> 02:07:26.700]   about any of that?
[02:07:26.700 --> 02:07:27.700]   Yeah.
[02:07:27.700 --> 02:07:28.700]   Like, hope so.
[02:07:28.700 --> 02:07:30.100]   That's the weirdest part of this.
[02:07:30.100 --> 02:07:35.260]   It's like, if they're doing AR VR, it's like, all right, but everybody else is just like,
[02:07:35.260 --> 02:07:37.100]   we're switching positions now on that.
[02:07:37.100 --> 02:07:38.740]   That's last year's fad.
[02:07:38.740 --> 02:07:39.740]   We're moving on.
[02:07:39.740 --> 02:07:41.740]   We're AIs the new thing.
[02:07:41.740 --> 02:07:42.740]   And here's Apple.
[02:07:42.740 --> 02:07:44.140]   Like, we're going to do AR VR.
[02:07:44.140 --> 02:07:47.380]   Even if they're geniuses and really figure this out, like, you know, what Glenn is kind
[02:07:47.380 --> 02:07:54.740]   of looting to maybe or, yeah, this could be just a really big error in terms of guidance.
[02:07:54.740 --> 02:07:55.740]   I think it's an issue.
[02:07:55.740 --> 02:07:56.740]   I really do.
[02:07:56.740 --> 02:07:57.980]   They have the neural cores.
[02:07:57.980 --> 02:08:01.300]   They've got, they've been making chips with multiple cores.
[02:08:01.300 --> 02:08:03.340]   I mean, I like Apple's approach.
[02:08:03.340 --> 02:08:08.060]   They've done a lot of machine learning stuff that does not very flashy, but works actually
[02:08:08.060 --> 02:08:09.340]   quite well.
[02:08:09.340 --> 02:08:15.340]   That's the device to device and end-to-end encrypted and relies on, in some cases, dedicated cores.
[02:08:15.340 --> 02:08:19.340]   Like, Apple added OCR to photos library.
[02:08:19.340 --> 02:08:22.820]   So everything, every piece of text now, I can search on it.
[02:08:22.820 --> 02:08:23.820]   That's extraordinary.
[02:08:23.820 --> 02:08:25.380]   It's an incredible improvement.
[02:08:25.380 --> 02:08:29.580]   It's so quiet, most people don't know they did it because they want it just to be something
[02:08:29.580 --> 02:08:32.860]   that's emergent when you search and you just, oh, there's more things to it.
[02:08:32.860 --> 02:08:36.100]   That will be, maybe Tim Cook, that would be an interesting attack to take.
[02:08:36.100 --> 02:08:40.220]   Tim Cook comes out and says, we don't think AI is ready.
[02:08:40.220 --> 02:08:41.740]   We don't think it's safe.
[02:08:41.740 --> 02:08:46.500]   We think it should be incorporated into existing products in a way that's safe, reliable,
[02:08:46.500 --> 02:08:47.540]   and private.
[02:08:47.540 --> 02:08:50.900]   And that's what we do as the privacy company.
[02:08:50.900 --> 02:08:55.420]   Meanwhile, casting shade on Google and Microsoft is not carrying one bit about your privacy
[02:08:55.420 --> 02:08:57.180]   or accuracy.
[02:08:57.180 --> 02:09:03.060]   The Newton, according to ours, Technica, sold an estimated 200,000 units.
[02:09:03.060 --> 02:09:10.460]   Now it's interesting because Apple's saying they expect to sell 100,000 of these nerd
[02:09:10.460 --> 02:09:12.100]   helmets.
[02:09:12.100 --> 02:09:18.340]   So maybe Apple really is in their mind thinking, this is our Newton and we will, how long was
[02:09:18.340 --> 02:09:21.740]   it after the Newton came out that came out with the iPhone?
[02:09:21.740 --> 02:09:25.140]   The Newton was like 12, 15 years, but a while.
[02:09:25.140 --> 02:09:27.740]   1997 to 2007.
[02:09:27.740 --> 02:09:31.460]   So it's exactly 10 years.
[02:09:31.460 --> 02:09:35.740]   So it might be accurate.
[02:09:35.740 --> 02:09:40.580]   They might be saying in 10 years, we'll have the AR device, the equivalent of the iPhone
[02:09:40.580 --> 02:09:43.020]   to the Newton.
[02:09:43.020 --> 02:09:45.340]   That'd be interesting, wouldn't it?
[02:09:45.340 --> 02:09:47.980]   I actually have a little soft spot in my heart for the Newton.
[02:09:47.980 --> 02:09:50.180]   If this is a Newton, it wouldn't be so bad.
[02:09:50.180 --> 02:09:52.420]   Yeah, that wouldn't be so bad.
[02:09:52.420 --> 02:10:02.700]   But and by the way, let's not forget that one of the outcomes of Newton was the ARM processor,
[02:10:02.700 --> 02:10:08.860]   which was the processor used in the Newton, Apple bought or actually founded ARM based
[02:10:08.860 --> 02:10:14.420]   on the Acorn risk machine, ARM, and eventually sold off their stake.
[02:10:14.420 --> 02:10:16.620]   But there would be no ARM without Newton.
[02:10:16.620 --> 02:10:18.420]   I forgot about that.
[02:10:18.420 --> 02:10:21.540]   So these early, you know, and Apple's not Apple.
[02:10:21.540 --> 02:10:25.660]   I think Tim Cook is very aware of Apple history and the lessons learned.
[02:10:25.660 --> 02:10:26.660]   It might very well be.
[02:10:26.660 --> 02:10:27.660]   That's how they think of this.
[02:10:27.660 --> 02:10:30.980]   You've given me a new perspective on this.
[02:10:30.980 --> 02:10:32.540]   So it's so it's not that you're wrong.
[02:10:32.540 --> 02:10:34.180]   You just not right.
[02:10:34.180 --> 02:10:36.740]   I appreciate this very Stephen Colbert.
[02:10:36.740 --> 02:10:38.780]   You're so wrong.
[02:10:38.780 --> 02:10:40.500]   Why are you right?
[02:10:40.500 --> 02:10:43.460]   You're not that right.
[02:10:43.460 --> 02:10:46.900]   My analysis is correct and the patient is dead, but the patient should be alive according
[02:10:46.900 --> 02:10:47.900]   to my calculations.
[02:10:47.900 --> 02:10:49.540]   Go into my careful calculations.
[02:10:49.540 --> 02:10:53.700]   I'm glad that you all decided to come out and play today.
[02:10:53.700 --> 02:10:57.300]   That's what I'm going to say, even though you really didn't have to leave your home.
[02:10:57.300 --> 02:11:02.980]   So in Thomas, San Francisco examiner, bring back to the rati.
[02:11:02.980 --> 02:11:03.980]   Love you.
[02:11:03.980 --> 02:11:07.900]   It's great to see you and please give your favorite furry Alex a hug.
[02:11:07.900 --> 02:11:09.540]   No, I'm sorry.
[02:11:09.540 --> 02:11:10.540]   You're his daddy.
[02:11:10.540 --> 02:11:11.540]   I'm sorry.
[02:11:11.540 --> 02:11:12.540]   Let's get that right.
[02:11:12.540 --> 02:11:13.540]   No, I'm just teasing.
[02:11:13.540 --> 02:11:14.940]   I like to tease Alex.
[02:11:14.940 --> 02:11:16.580]   You know, he lives in my boyhood home.
[02:11:16.580 --> 02:11:18.060]   Have I mentioned that?
[02:11:18.060 --> 02:11:19.780]   How many times have I mentioned that?
[02:11:19.780 --> 02:11:21.740]   You didn't know that.
[02:11:21.740 --> 02:11:28.300]   Is there a plaque in Providence in Providence, Rhode Island, that house, that same house.
[02:11:28.300 --> 02:11:30.220]   Yeah, that is free.
[02:11:30.220 --> 02:11:34.700]   We found out it was complete coincidence is a fiance at the time now wife.
[02:11:34.700 --> 02:11:36.140]   Liza was out visiting.
[02:11:36.140 --> 02:11:38.220]   And she was doing her residency at Brown.
[02:11:38.220 --> 02:11:40.020]   I said, I used to live in Providence.
[02:11:40.020 --> 02:11:41.020]   She said, Oh, yeah.
[02:11:41.020 --> 02:11:42.020]   I grew up there.
[02:11:42.020 --> 02:11:43.020]   Where did you live?
[02:11:43.020 --> 02:11:46.660]   I said, I told her the streets and oh, yeah, what number?
[02:11:46.660 --> 02:11:47.940]   And I told her the numbers.
[02:11:47.940 --> 02:11:49.940]   She said, yeah, that's my house.
[02:11:49.940 --> 02:11:50.940]   Oh my gosh.
[02:11:50.940 --> 02:11:54.180]   Yeah, they gave me a tour last time I was out there.
[02:11:54.180 --> 02:11:56.180]   Anyway, that's amazing.
[02:11:56.180 --> 02:12:00.220]   So where's the is there like a velvet rope in front of your bedroom?
[02:12:00.220 --> 02:12:01.220]   There is a plaque.
[02:12:01.220 --> 02:12:02.460]   Yes, there is a plaque.
[02:12:02.460 --> 02:12:06.340]   Leoport slept here.
[02:12:06.340 --> 02:12:07.620]   It's actually they've done a nice job.
[02:12:07.620 --> 02:12:08.620]   They fixed it up.
[02:12:08.620 --> 02:12:09.620]   It's beautiful.
[02:12:09.620 --> 02:12:11.500]   Yeah, but I did visit my old bedroom.
[02:12:11.500 --> 02:12:12.500]   It's great.
[02:12:12.500 --> 02:12:13.500]   And it hasn't changed that much.
[02:12:13.500 --> 02:12:16.540]   It's kind of kind of a strange experience.
[02:12:16.540 --> 02:12:17.540]   That is Glenn Fleischman.
[02:12:17.540 --> 02:12:19.300]   We've got to give a plug to shift happens.
[02:12:19.300 --> 02:12:21.020]   Is it too late to get a copy?
[02:12:21.020 --> 02:12:26.020]   No, you can still get it to go to shift happens dot site.
[02:12:26.020 --> 02:12:28.420]   There's a site that lets you preorder.
[02:12:28.420 --> 02:12:32.460]   We have set the order of the number of books that will ever be printed probably because
[02:12:32.460 --> 02:12:34.260]   it's a big project.
[02:12:34.260 --> 02:12:36.500]   And then the printer is the printer.
[02:12:36.500 --> 02:12:39.180]   So I got an order paper because there's still a little bit of paper shortage.
[02:12:39.180 --> 02:12:40.180]   Oh, dear.
[02:12:40.180 --> 02:12:48.060]   And this is a it's a ultimately thirteen hundred and a lost track thirteen hundred seventy six
[02:12:48.060 --> 02:12:49.500]   pages across three volumes.
[02:12:49.500 --> 02:12:50.500]   It's a lot of paper.
[02:12:50.500 --> 02:12:51.780]   It's a lot of paper.
[02:12:51.780 --> 02:12:56.580]   So they got to go and you know, talk to the mills and the mills are still doing literally
[02:12:56.580 --> 02:12:59.700]   rationing like the printers can't order more than a certain age month.
[02:12:59.700 --> 02:13:00.940]   There's certain kinds of stock.
[02:13:00.940 --> 02:13:01.940]   So wow.
[02:13:01.940 --> 02:13:04.460]   And this is going to be a full color beautiful.
[02:13:04.460 --> 02:13:07.060]   You're probably using really good paper.
[02:13:07.060 --> 02:13:08.060]   I would imagine.
[02:13:08.060 --> 02:13:11.020]   Yeah, the paper is very nice, very nice paper.
[02:13:11.020 --> 02:13:12.620]   It's been a lot of time looking at papers.
[02:13:12.620 --> 02:13:15.260]   Let me tell you a lot of work.
[02:13:15.260 --> 02:13:16.260]   But yes, we're going on.
[02:13:16.260 --> 02:13:21.140]   We get to go out and go on press will be actually at the printers in Lewis and Maine
[02:13:21.140 --> 02:13:27.900]   in July watching sheets come off and every hour, hour and a half for at least a week.
[02:13:27.900 --> 02:13:32.100]   Maybe two weeks will be there 16 hours a day as they bring sheets off and go, how do you
[02:13:32.100 --> 02:13:33.100]   like this sheet?
[02:13:33.100 --> 02:13:34.100]   We go, oh, it's okay.
[02:13:34.100 --> 02:13:35.660]   Or maybe you need to fix that.
[02:13:35.660 --> 02:13:36.660]   Color patch isn't perfect.
[02:13:36.660 --> 02:13:39.580]   What's really awesome that you're doing that, I think that's really fantastic.
[02:13:39.580 --> 02:13:42.820]   It's going to be a lot of work, but it's going to be perfect.
[02:13:42.820 --> 02:13:45.220]   And if you give it, I mean, the printers are good.
[02:13:45.220 --> 02:13:47.460]   If you let them do it, they'll do it right.
[02:13:47.460 --> 02:13:53.060]   It's very digital end to end process for analog things, lots of color checking and so
[02:13:53.060 --> 02:13:55.420]   forth that happens all in line with the equipment.
[02:13:55.420 --> 02:13:59.740]   So they keep it on target, but still, this is a high touch book, as they say.
[02:13:59.740 --> 02:14:00.740]   So we're going to touch it a lot.
[02:14:00.740 --> 02:14:05.380]   And if you are in New England, you should go visit the Museum of Printing on Saturday,
[02:14:05.380 --> 02:14:06.380]   July 8th.
[02:14:06.380 --> 02:14:10.980]   Glenn will be there along with Martian Doug Wilson, the director of a film about line
[02:14:10.980 --> 02:14:15.460]   type and some guy named Jeff Jarvis to talk about.
[02:14:15.460 --> 02:14:16.460]   Jeff Jarvis.
[02:14:16.460 --> 02:14:20.140]   To talk about keyboards, typewriters and the line type.
[02:14:20.140 --> 02:14:25.460]   This will be, I want to just go and see the nerds that show up to be honest with you.
[02:14:25.460 --> 02:14:26.460]   It'll be great.
[02:14:26.460 --> 02:14:27.460]   It'll be great.
[02:14:27.460 --> 02:14:32.180]   Look, I sometimes go to conventions, ways goose, they're called ways geese, old term,
[02:14:32.180 --> 02:14:37.260]   and hundreds of people go to an old printing factory or a museum and we all talk about
[02:14:37.260 --> 02:14:39.340]   printing or letter press or type or whatever.
[02:14:39.340 --> 02:14:40.340]   It's a lot of fun.
[02:14:40.340 --> 02:14:41.340]   It's your goal.
[02:14:41.340 --> 02:14:42.340]   Glenn.
[02:14:42.340 --> 02:14:45.380]   It's your goal to devote your life to this subject.
[02:14:45.380 --> 02:14:48.780]   Yeah, it's kind of accidental, but I love it.
[02:14:48.780 --> 02:14:52.220]   And I joked at one point I was collecting obsolete professions.
[02:14:52.220 --> 02:14:56.100]   I was trained as a typesetter in the 80s and then I went into journalism.
[02:14:56.100 --> 02:15:00.780]   So maybe historian might be, I don't know how a little more staying power, we'll find
[02:15:00.780 --> 02:15:01.980]   out, I don't sure.
[02:15:01.980 --> 02:15:02.980]   So I didn't realize that.
[02:15:02.980 --> 02:15:06.020]   So you, because you've been a typesetter, you kind of came to this honestly.
[02:15:06.020 --> 02:15:11.100]   Yeah, I was a high school job and then in college I worked as a typesetter and always
[02:15:11.100 --> 02:15:14.980]   had kind of an interesting, I mean, printing history is the history of how we communicate,
[02:15:14.980 --> 02:15:15.980]   right?
[02:15:15.980 --> 02:15:20.020]   And there's this whole 1800s we went from being a low information society in some ways to
[02:15:20.020 --> 02:15:25.980]   a high because of improvements and printing allowed the rapid transformation of news from
[02:15:25.980 --> 02:15:29.620]   what happened into newspapers and it's an incredible thing.
[02:15:29.620 --> 02:15:33.860]   So I'm kind of studying aspects of that that are very obscure.
[02:15:33.860 --> 02:15:34.860]   Awesome.
[02:15:34.860 --> 02:15:38.180]   Shift happens dot site.
[02:15:38.180 --> 02:15:42.820]   I think the time to buy, you know, get this before it's at a print because it will be
[02:15:42.820 --> 02:15:45.380]   the one and only and you will have something.
[02:15:45.380 --> 02:15:46.380]   That's a great laser.
[02:15:46.380 --> 02:15:47.380]   What was that?
[02:15:47.380 --> 02:15:48.380]   I love that shot.
[02:15:48.380 --> 02:15:49.380]   That's a laser.
[02:15:49.380 --> 02:15:50.380]   It's a guy.
[02:15:50.380 --> 02:15:53.660]   One of the people who is responsible for the invention of the laser, he co won Nobel Prize
[02:15:53.660 --> 02:15:59.580]   for it also decided there should be a laser typo eraser and he actually made a model.
[02:15:59.580 --> 02:16:00.580]   He brought it on the air.
[02:16:00.580 --> 02:16:01.580]   It's a chad chandler.
[02:16:01.580 --> 02:16:04.380]   I'm like his name.
[02:16:04.380 --> 02:16:08.020]   He went on the air with Walter Cronkite and demoed it and you just zap it because the
[02:16:08.020 --> 02:16:10.940]   ink would vaporize and the paper wouldn't get as hot.
[02:16:10.940 --> 02:16:14.140]   And he thought this could be miniaturized and just built into typewriters and you just
[02:16:14.140 --> 02:16:21.300]   have a, you know, sadly the correcting Selectric was invented instead.
[02:16:21.300 --> 02:16:23.700]   Mike Nesma's mom came along with white out.
[02:16:23.700 --> 02:16:24.700]   Yes.
[02:16:24.700 --> 02:16:25.700]   Yes.
[02:16:25.700 --> 02:16:29.020]   And then eventually computers and you didn't use a typewriter.
[02:16:29.020 --> 02:16:30.020]   Yeah.
[02:16:30.020 --> 02:16:31.020]   It's amazing.
[02:16:31.020 --> 02:16:34.300]   I mean, there's a technology that lasted a hundred years and then just disappeared.
[02:16:34.300 --> 02:16:35.300]   It's over.
[02:16:35.300 --> 02:16:36.300]   Like buggy whips.
[02:16:36.300 --> 02:16:37.300]   Yeah.
[02:16:37.300 --> 02:16:38.300]   Like buggy whips.
[02:16:38.300 --> 02:16:39.300]   Thank you, Glenn.
[02:16:39.300 --> 02:16:40.300]   Always great to see you.
[02:16:40.300 --> 02:16:41.300]   Thank you.
[02:16:41.300 --> 02:16:42.300]   Pleasure to be here.
[02:16:42.300 --> 02:16:44.980]   Glenn fun is two ends in Glenn, G L E N N dot funds.
[02:16:44.980 --> 02:16:50.620]   Thank you, Daniel, Rubino editor in chief of Windows central great coverage on build.
[02:16:50.620 --> 02:16:53.260]   There was a lot more we didn't get to.
[02:16:53.260 --> 02:16:57.460]   It's build was, I think really interesting this year.
[02:16:57.460 --> 02:16:58.460]   And Microsoft.
[02:16:58.460 --> 02:16:59.460]   Time in a long time.
[02:16:59.460 --> 02:17:01.420]   Yeah, just fascinating.
[02:17:01.420 --> 02:17:06.500]   So I really appreciate you coming and filling us in on the best stuff.
[02:17:06.500 --> 02:17:07.500]   Thank you, Daniel.
[02:17:07.500 --> 02:17:08.500]   Thank you.
[02:17:08.500 --> 02:17:09.500]   I appreciate it.
[02:17:09.500 --> 02:17:10.500]   And now I'm excited about that book.
[02:17:10.500 --> 02:17:11.500]   That's a, that book looks amazing.
[02:17:11.500 --> 02:17:12.500]   Isn't it beautiful?
[02:17:12.500 --> 02:17:14.500]   What a work of art you've created.
[02:17:14.500 --> 02:17:16.500]   Glad I just, incredible.
[02:17:16.500 --> 02:17:18.300]   I'm just a project manager.
[02:17:18.300 --> 02:17:20.780]   All credit to Marching who is the designer.
[02:17:20.780 --> 02:17:21.980]   He took a lot of the pictures in it.
[02:17:21.980 --> 02:17:23.620]   He found the photos.
[02:17:23.620 --> 02:17:28.140]   He contacted people over the world to take pictures of obscure typewriters and terminals.
[02:17:28.140 --> 02:17:32.900]   But yeah, it's about half photos also one of the selling points.
[02:17:32.900 --> 02:17:35.180]   I'm going to, I'm going to pick one up right now.
[02:17:35.180 --> 02:17:36.780]   I should have ordered it before.
[02:17:36.780 --> 02:17:41.780]   We've been talking about it all this time and I just like put it off, put it off.
[02:17:41.780 --> 02:17:42.780]   I'm ready.
[02:17:42.780 --> 02:17:45.140]   Someday a collector's item possibly because we're not going to print the thing again.
[02:17:45.140 --> 02:17:47.140]   It's a huge, no, yeah.
[02:17:47.140 --> 02:17:48.700]   It's a huge, it's so much work.
[02:17:48.700 --> 02:17:49.700]   I can see that.
[02:17:49.700 --> 02:17:50.700]   Yeah.
[02:17:50.700 --> 02:17:51.700]   Might not ever go back.
[02:17:51.700 --> 02:17:53.900]   Well, and I think we should have one on our, in our studio.
[02:17:53.900 --> 02:17:57.020]   Honestly, I think that's a, that's a great piece of history.
[02:17:57.020 --> 02:18:00.660]   So thank you for that.
[02:18:00.660 --> 02:18:03.460]   Ladies and gentlemen, we do Twitch every Sunday afternoon.
[02:18:03.460 --> 02:18:05.580]   I'm glad you were here for this episode.
[02:18:05.580 --> 02:18:08.100]   You can watch us live if you want.
[02:18:08.100 --> 02:18:10.860]   The live stream is at live.tuit.tv.
[02:18:10.860 --> 02:18:12.300]   There's audio and video.
[02:18:12.300 --> 02:18:17.020]   We get started around 2 p.m. Pacific right after ask the tech guys.
[02:18:17.020 --> 02:18:19.860]   And we end usually around 5 or 6 p.m.
[02:18:19.860 --> 02:18:24.620]   We ending a little early tonight because this is the last episode of succession.
[02:18:24.620 --> 02:18:30.020]   And I have to put on my, my dark suit, my funeral, my suit and watch it.
[02:18:30.020 --> 02:18:32.580]   I'm very sad about that.
[02:18:32.580 --> 02:18:35.100]   Don't, don't get me started.
[02:18:35.100 --> 02:18:38.580]   Anyway, we're glad you're here 2 to 5 p.m. Pacific.
[02:18:38.580 --> 02:18:42.700]   That's 5 to 8 p.m. Eastern time 2100 UTC.
[02:18:42.700 --> 02:18:46.380]   After the fact you can get on to man versions of the show ad supported at Twitch.tv our
[02:18:46.380 --> 02:18:47.380]   website.
[02:18:47.380 --> 02:18:49.500]   And when you're there, you'll see a link to YouTube.
[02:18:49.500 --> 02:18:52.300]   There's a YouTube channel devoted to this week in tech.
[02:18:52.300 --> 02:18:57.180]   You also see links to podcast players so you can subscribe in them and RSS feed because
[02:18:57.180 --> 02:19:00.220]   you could basically any podcast player search for a Twitch.
[02:19:00.220 --> 02:19:02.020]   You'll find it right away.
[02:19:02.020 --> 02:19:03.020]   Subscribe to all our shows.
[02:19:03.020 --> 02:19:04.340]   We have a lot of great shows.
[02:19:04.340 --> 02:19:09.460]   Now if you are a club Twitch member, don't forget you have your own feeds.
[02:19:09.460 --> 02:19:11.620]   Those are the ad free versions of the feeds.
[02:19:11.620 --> 02:19:15.460]   When you subscribe to club Twitch, we send you an email with links to all the shows,
[02:19:15.460 --> 02:19:16.860]   the ad free versions.
[02:19:16.860 --> 02:19:20.940]   We send you a discord invite so you can join us in our discord chat room.
[02:19:20.940 --> 02:19:24.740]   We send you information about the Twitch plus feed which has shows we don't put out anywhere
[02:19:24.740 --> 02:19:30.100]   else like hands on Macintosh with my sergeant Paul Therot's hands on windows.
[02:19:30.100 --> 02:19:32.420]   We brought back Scott Wilkinson's home theater geeks.
[02:19:32.420 --> 02:19:35.300]   That's a wonderful show this week in space and on and on and on.
[02:19:35.300 --> 02:19:38.380]   So club Twitch members, $7 a month.
[02:19:38.380 --> 02:19:40.140]   It supports us.
[02:19:40.140 --> 02:19:41.260]   We appreciate it.
[02:19:41.260 --> 02:19:45.740]   If you can just take a little bit of money out of your shift happens funds and send that
[02:19:45.740 --> 02:19:49.580]   to us, we will be very glad to welcome you into the club.
[02:19:49.580 --> 02:19:53.100]   Check that TV slash club Twitch.
[02:19:53.100 --> 02:19:57.620]   And we are very grateful to all of our club Twitch members for making this possible.
[02:19:57.620 --> 02:19:58.860]   That's it for this week in tech.
[02:19:58.860 --> 02:20:00.420]   Thank you everybody for joining us.
[02:20:00.420 --> 02:20:01.420]   We'll see you next time.
[02:20:01.420 --> 02:20:03.100]   Another Twitch is in the game.
[02:20:03.100 --> 02:20:13.100]   [Music]

