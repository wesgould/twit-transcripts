;FFMETADATA1
title=Elon and L. Ron
artist=Leo Laporte, Amy Webb, David Spark
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-05-23
track=876
language=English
genre=Podcast
comment=NY goes after Twitch, Musk's real plan, Apple's DeepMind defection
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.000]   It's time for Twit this week in Tech. We've got a great show. My old friend from Tech TV,
[00:00:04.000 --> 00:00:10.080]   David Spark returns after a multi-year absence and Amy Webb is here. That means big thoughts,
[00:00:10.080 --> 00:00:15.280]   big thinking. We'll talk about the fact that the New York state attorney general wants to blame
[00:00:15.280 --> 00:00:22.960]   Twitch and 4chan and Discord for the Buffalo shooting. Elon Musk's real plans Amy explains all
[00:00:23.600 --> 00:00:30.240]   and then Apple losing perhaps its most important employee because they wouldn't let them work from
[00:00:30.240 --> 00:00:40.000]   home. Come on man it's this week in Tech coming up next. Podcasts you love from people you trust.
[00:00:40.000 --> 00:00:42.800]   This is Twit.
[00:00:50.800 --> 00:00:58.720]   This is Twit this week in Tech. Episode 876 recorded Sunday May 22nd 2022.
[00:00:58.720 --> 00:01:00.880]   Elon and Elrond.
[00:01:00.880 --> 00:01:08.880]   This episode of this week in Tech is brought to you by Stamps.com. Start mailing and shipping
[00:01:08.880 --> 00:01:14.640]   with Stamps.com. Sign up with the promo code TWIT. You'll get our special offer including a four-week
[00:01:14.640 --> 00:01:21.040]   trial, free postage and a digital scale. No long-term commitments or contracts. Go to Stamps.com,
[00:01:21.040 --> 00:01:28.960]   click the microphone at the top of the page and enter the code TWIT. And by ExpressVPN. Be smart.
[00:01:28.960 --> 00:01:34.400]   Stop paying full price for streaming services and only getting access to a fraction of their content.
[00:01:34.400 --> 00:01:41.360]   Get your money's worth. Get three extra months free with a one-year package at expressvpn.com/Twit.
[00:01:42.080 --> 00:01:48.960]   And by Noom. Unlike other programs, Noom Weight uses a psychology-based approach
[00:01:48.960 --> 00:01:53.840]   to help people better understand their relationship with food and gives you skills and knowledge you
[00:01:53.840 --> 00:02:01.120]   need to build long-lasting positive habits. Sign up for your trial at noom.com/Twit.
[00:02:01.120 --> 00:02:08.080]   And by Indochino. If you've got a big day coming up, getting the perfect look is no big deal with
[00:02:08.080 --> 00:02:16.560]   Indochino. Get $50 off any purchase of $3.99 or more by using the promo code TWIT at Indochino.com.
[00:02:16.560 --> 00:02:27.680]   It's time for TWIT this week at Tech. This show will be covered in weeks. Tech news.
[00:02:27.680 --> 00:02:32.560]   And we have such a good panel today. I thought, you know what? I'm just going to sit back,
[00:02:32.560 --> 00:02:39.200]   relax and enjoy the ride. Amy Webb is here, our favorite futurist from the Future Today Institute,
[00:02:39.200 --> 00:02:44.240]   author of the Genesis Machine, our quest to rewrite life in the age of synthetic biology.
[00:02:44.240 --> 00:02:47.040]   I haven't seen you since the book came out. Is it doing well?
[00:02:47.040 --> 00:02:56.640]   The book is doing really well. And I actually, Eric Schmidt gave me hives last week. We'd
[00:02:56.640 --> 00:03:02.000]   write a conference together on a panel talking about AI and we diverged and started talking about
[00:03:02.480 --> 00:03:08.640]   these said some very, very nice things about me. Wow. And I was so, it was in front of this
[00:03:08.640 --> 00:03:14.080]   big group. I was so embarrassed, like in a good way, but I was so embarrassed. I broke out into
[00:03:14.080 --> 00:03:20.480]   hives everywhere. I burned bright red and I was like three minutes into the session.
[00:03:20.480 --> 00:03:24.000]   Oh Lord. Oh Lord. Oh Lord. That was embarrassing.
[00:03:24.000 --> 00:03:29.120]   But that's Amy. You have set up the challenge for both Leo and I. We have to get you to break
[00:03:29.120 --> 00:03:35.440]   out at high. That's very often. It doesn't happen very often. We've got two hours to do this, Leo.
[00:03:35.440 --> 00:03:41.840]   David Spark is in the house. Oh, I love seeing David Spark. Long time friends since going back
[00:03:41.840 --> 00:03:45.920]   to tech TV. He's been on Twitter many times. Not in the long time though. So I'm really glad to get
[00:03:45.920 --> 00:03:51.280]   you in. Oh, it's been a while. You have started a new network on security called the CISO.
[00:03:51.280 --> 00:03:57.040]   Siri, do you pronounce it CISO or is that just me? Yes. But if you go, we did a show, a live show in
[00:03:57.040 --> 00:04:00.880]   Australia and they're down under. They pronounce it CISO.
[00:04:00.880 --> 00:04:04.560]   Router. So. Rooter. They call it Rooter. Oh, CISO. Okay.
[00:04:04.560 --> 00:04:10.000]   So, CISO. So I'm going to say CISO because that's a, that's a, most of us say,
[00:04:10.000 --> 00:04:16.960]   America. And so this is, you've got five podcasts all about security, which is great.
[00:04:16.960 --> 00:04:20.640]   It's defense and depth. Super Cyber Friday. Capture the CISO.
[00:04:20.640 --> 00:04:25.200]   Cyber secure headlines. Good job. Capture the CISO is our new show.
[00:04:25.200 --> 00:04:30.640]   I like the name. It's kind of like, it's kind of like shark tank and that there's CISOs judging
[00:04:30.640 --> 00:04:38.000]   security vendors. But instead of like seeing full pitches, CISOs, which are chief information
[00:04:38.000 --> 00:04:43.120]   security officers, they come on and they've already watched the demo videos of the products
[00:04:43.120 --> 00:04:48.720]   and they're just having conversations with these security vendors, asking them like,
[00:04:48.720 --> 00:04:52.560]   tell me more about your product and ask them relevant questions about it. Nice.
[00:04:54.640 --> 00:04:56.560]   And that's kind of like what everyone wants to hear in the,
[00:04:56.560 --> 00:04:59.040]   the kind of thing you would be asking. Yeah. Yeah.
[00:04:59.040 --> 00:05:03.760]   I think that's part of the problem with doing this kind of stuff is it tends to get very
[00:05:03.760 --> 00:05:08.640]   vendor heavy and ends up being an ad. So this is really good. This is a way to get some solid
[00:05:08.640 --> 00:05:15.120]   content. It's the conversation, not the pitch. I am drinking today some synthetic booze provided
[00:05:15.120 --> 00:05:24.160]   by Amy Webb. It's called glyph, GLYPH. It is, I want to say distilled in San Francisco, but I
[00:05:24.160 --> 00:05:29.920]   think it's probably created in a vat somewhere in San Francisco. All the spirit, none of the rules.
[00:05:29.920 --> 00:05:34.320]   So I think we talked about it as in a previous show, right? The idea that they could, maybe it
[00:05:34.320 --> 00:05:39.680]   was in the Genesis machine that they can make whiskey without actually putting whiskey barrels
[00:05:39.680 --> 00:05:45.920]   and all that. Right. Basically, if you've got the molecular formula, the construct, you know,
[00:05:45.920 --> 00:05:50.640]   you can, you can print out molecules and just distill in a different way.
[00:05:51.360 --> 00:05:55.520]   Hang on a second. I've got crazy different microphones going here.
[00:05:55.520 --> 00:06:00.800]   Printed in printed molecules, kind of like a Star Trek synth, the hall or, I mean,
[00:06:00.800 --> 00:06:04.640]   every effect, every science fiction has a synthetic whiskey. I'm just finishing the
[00:06:04.640 --> 00:06:08.960]   Bobovers. And that's one of the first things they try to figure out, how to make whiskey
[00:06:08.960 --> 00:06:11.680]   again. Right. So this is the show.
[00:06:11.680 --> 00:06:15.280]   It's, I think that's a good, that's a good goal.
[00:06:15.280 --> 00:06:21.200]   You established. We have a challenge. You have a challenge. No, I can't do that to poor
[00:06:21.200 --> 00:06:24.080]   baby. I just can't please don't.
[00:06:24.080 --> 00:06:25.280]   I had a theory.
[00:06:25.280 --> 00:06:26.800]   Very very right. Yeah.
[00:06:26.800 --> 00:06:30.160]   No, it was a compliment. So she's not going to hate that.
[00:06:30.160 --> 00:06:30.640]   No.
[00:06:30.640 --> 00:06:37.760]   I just, I'd never had anybody compliment. He just said very nice things and it seemed
[00:06:37.760 --> 00:06:41.680]   very sincere and it was kind of out of nowhere and we were deep in the weeds on AI at that point.
[00:06:41.680 --> 00:06:44.320]   Well, also coming from Eric Shand.
[00:06:44.320 --> 00:06:47.520]   Do you want us to go in the opposite direction?
[00:06:48.800 --> 00:06:50.240]   What's the opposite of hot?
[00:06:50.240 --> 00:06:50.880]   You.
[00:06:50.880 --> 00:06:51.920]   It's the opposite of hot.
[00:06:51.920 --> 00:06:53.440]   Well, no, it's like,
[00:06:53.440 --> 00:06:55.280]   got the monkey pox.
[00:06:55.280 --> 00:06:56.640]   Let's not give her a monkey pox.
[00:06:56.640 --> 00:06:58.080]   Monkey pox. Let's not do that.
[00:06:58.080 --> 00:06:59.040]   Let's not do that.
[00:06:59.040 --> 00:06:59.040]   Okay.
[00:06:59.040 --> 00:06:59.520]   Okay.
[00:06:59.520 --> 00:07:00.000]   Please.
[00:07:00.000 --> 00:07:00.400]   Okay.
[00:07:00.400 --> 00:07:03.600]   Monkey pox is the new, the new,
[00:07:03.600 --> 00:07:08.480]   I don't know, cause celeb, I don't want to celebrate it in any way.
[00:07:08.480 --> 00:07:11.840]   It's the cool virus everybody wants to take to prom this year.
[00:07:11.840 --> 00:07:13.120]   No, no, no.
[00:07:14.320 --> 00:07:19.440]   So, okay. So I know everybody's afraid and rightly so that we might start talking about Elon.
[00:07:19.440 --> 00:07:21.680]   We're just going to get out of the way.
[00:07:21.680 --> 00:07:22.480]   It's going to be quick.
[00:07:22.480 --> 00:07:22.880]   Knock it out.
[00:07:22.880 --> 00:07:23.680]   It's going to be brief.
[00:07:23.680 --> 00:07:25.200]   Like it like ripping off a bandaid.
[00:07:25.200 --> 00:07:28.160]   Just rip off the bandaid, get it over with quickly.
[00:07:28.160 --> 00:07:32.480]   Elon, what's the status of Elon's Twitter deal?
[00:07:32.480 --> 00:07:36.320]   Is it up in the air? Is it, is it, it's all?
[00:07:36.320 --> 00:07:43.600]   It is up in the air based on the percentage of bots that are, or the fake accounts that are on
[00:07:43.600 --> 00:07:48.800]   Twitter, of which it doesn't appear there's any third party that's verifying these.
[00:07:48.800 --> 00:07:49.760]   It just seems that they have a trust with them.
[00:07:49.760 --> 00:07:54.240]   Oh, there is absolutely, no, no, there are third parties that are verifying it.
[00:07:54.240 --> 00:07:58.800]   There's been a couple of academics that, and there's an independent group that,
[00:07:58.800 --> 00:08:02.000]   to be fair, like built some type of tool, it's not bot or not.
[00:08:02.000 --> 00:08:07.440]   I forgot who it was, but there are groups that are pulling as much data as they can by
[00:08:07.440 --> 00:08:09.280]   doing samples and to be fair.
[00:08:09.280 --> 00:08:12.640]   You know, it's hard to get a huge sample size, but
[00:08:13.280 --> 00:08:17.120]   I mean, I think we all know even, like I even know anecdotally,
[00:08:17.120 --> 00:08:20.640]   like there's a tremendous amount of bot activity and there always has.
[00:08:20.640 --> 00:08:23.200]   I think it's way over 20%, though. That's the thing.
[00:08:23.200 --> 00:08:25.760]   Well, Elon was told it was 5%.
[00:08:25.760 --> 00:08:30.080]   That's where he sent the poop emoji to the CEO of Twitter.
[00:08:30.080 --> 00:08:34.960]   Yeah, none of it matters. He's trying to use this as a wedge to get out of the deal.
[00:08:34.960 --> 00:08:38.080]   And why did he try to get, maybe he's, I don't know, maybe
[00:08:40.400 --> 00:08:46.480]   the number of bots thwarting his investment is sort of a red hearing, right?
[00:08:46.480 --> 00:08:50.080]   He just wants to get out of it. That's really what's going on.
[00:08:50.080 --> 00:08:55.360]   I think what happened is that he bought up a bunch of shares,
[00:08:55.360 --> 00:08:59.760]   he got invited to be on the board, realized that he had a fiduciary responsibility
[00:08:59.760 --> 00:09:04.240]   as a board of directors member, and he couldn't go around trash talking to the CEO or others,
[00:09:04.240 --> 00:09:07.440]   or the edit button, or pick whatever you want, free speech.
[00:09:08.960 --> 00:09:12.960]   Then left because Elon doesn't want people telling him what to do.
[00:09:12.960 --> 00:09:18.160]   And I now think he just, you know, he's got that had that had repercussions,
[00:09:18.160 --> 00:09:23.200]   it had repercussions on the other businesses where his finances are tied up.
[00:09:23.200 --> 00:09:29.920]   So why tank Tesla over a social media, which a lot of us use, but let's face it,
[00:09:29.920 --> 00:09:36.800]   has a limited number of DAUs and like, you know, it was never, it's never made more than a
[00:09:36.800 --> 00:09:42.800]   billion dollars in a year. If you pay 44 billion dollars for something that's a beta is less than
[00:09:42.800 --> 00:09:49.040]   a billion, it's going to take you a long time. No, but there is, I'm sorry, is there any history
[00:09:49.040 --> 00:09:54.240]   of any company being purchased at that level of multiple of yearly revenue? Oh, yeah.
[00:09:54.240 --> 00:10:00.720]   But that's that's 44 X. That seems. I mean, I don't think it's that. I don't know if anyone's
[00:10:00.720 --> 00:10:05.440]   purchased anything at that. And then I've heard people say, Oh, now that it's off,
[00:10:06.240 --> 00:10:11.040]   watch while Microsoft swoops in. No, Microsoft doesn't want Twitter. No one wants Twitter.
[00:10:11.040 --> 00:10:16.720]   That's the whole point. Twitter's been trying to sell itself for years and not succeeded.
[00:10:16.720 --> 00:10:22.640]   So I don't know if Elon legitimately wanted it. I think he did. I think he found out that when he
[00:10:22.640 --> 00:10:27.920]   would get down to it, he was going to have to come up with 21 billion of his own money.
[00:10:27.920 --> 00:10:33.680]   He have a leveraged himself against Tesla stock. That opens him up as the stock drops,
[00:10:33.680 --> 00:10:39.840]   which it has. It's plummeted as a result to a margin call, which could put him out of business.
[00:10:39.840 --> 00:10:45.600]   It's also hard to get the rest of it, although he seems to have found Larry Ellison gave him a
[00:10:45.600 --> 00:10:49.920]   billion. Seems to have found a few other suckers. I mean, investors who are willing to give him money
[00:10:49.920 --> 00:10:55.040]   for something that is never going to pay off. So he's got to also be a negotiation tactic.
[00:10:55.040 --> 00:10:59.920]   It could probably is an attempt to get the price down. Because the stock went down because of this,
[00:11:00.640 --> 00:11:03.920]   he, you know, his, what, $54 per share.
[00:11:03.920 --> 00:11:07.680]   5420 because it's got to have something to do with marijuana or he won't do it.
[00:11:07.680 --> 00:11:13.840]   Exactly. But he, you know, showing how much it's going down because of his suspicions of the
[00:11:13.840 --> 00:11:19.520]   high percentage of fake accounts, then he could say, well, it doesn't really have x million of
[00:11:19.520 --> 00:11:23.600]   users. It has this percentage of users. So does it matter?
[00:11:23.600 --> 00:11:28.560]   Here's some lowering. But it doesn't matter as he well knows because he made the deal, in fact,
[00:11:28.560 --> 00:11:33.440]   waving due diligence. You don't get to say, I don't do due diligence. Here's the amount of
[00:11:33.440 --> 00:11:37.120]   money I'm going to pay sign on the dotted line. And then, Oh, now I'm going to do due diligence too
[00:11:37.120 --> 00:11:42.080]   late. Clearly there's, he's got there, there have to be compliance people that he works with.
[00:11:42.080 --> 00:11:48.160]   There have to be lawyers and I cannot imagine that job having to run for the worst job ever.
[00:11:48.160 --> 00:11:51.280]   I mean, I can't even wrap my head around what that must be like.
[00:11:51.280 --> 00:11:58.400]   This is William Cohen, who is a M and A guy writing for Puck News. I like Puck News a lot.
[00:11:58.400 --> 00:12:05.360]   Elon's next moves. He says, Elon played it like a Jedi up to a point listening to his financial
[00:12:05.360 --> 00:12:12.560]   advisors, Saks Goldman Sachs, JP Morgan, Chase Allen and company, his advisor, Rob Kindler,
[00:12:12.560 --> 00:12:20.400]   who was Cohen's old boss. So Cohen knows him well at Morgan Stanley, now vice chairman,
[00:12:20.400 --> 00:12:24.720]   global head of the M and A group, followed all their instructions up to a point.
[00:12:26.080 --> 00:12:29.200]   But after, and this is what Cohen writes, after making, aftering one his prize,
[00:12:29.200 --> 00:12:33.440]   what seemed with what seemed like a minimum effort, Elon started to lose his deal making
[00:12:33.440 --> 00:12:47.440]   poise. He neglected to tell the SEC. He, he, he basically started, you know, trolling at Twitter,
[00:12:47.440 --> 00:12:54.480]   violating his non-disclosure. And he points out, Elon's equity hole is still $14 billion,
[00:12:54.480 --> 00:13:00.160]   even with a billion from Ellison and Prince Alaweed, who put in 1.9 billion.
[00:13:00.160 --> 00:13:05.520]   And of course, the 4 billion in shares that he's already got that he doesn't have to buy again.
[00:13:05.520 --> 00:13:09.120]   He still has to come up with 14 billion dollars. It could just be that Elon
[00:13:09.120 --> 00:13:16.240]   just kind of lost his head. And now it's trying to get out of it. Twitter's response is, yeah,
[00:13:16.240 --> 00:13:26.320]   no, we're holding you. They told the shareholders, let's go. That's a good price. We're going to
[00:13:26.320 --> 00:13:30.880]   take it. Elon says he can't move forward. He already, he already agreed says Twitter,
[00:13:30.880 --> 00:13:36.480]   they're going to make him. You know, Twitter doesn't have to make money because Twitter makes
[00:13:36.480 --> 00:13:42.560]   power. If you stop and think about the, the sort of Venn diagram of who's using Twitter,
[00:13:42.560 --> 00:13:47.120]   obviously, there are many, many bots, but it's media people like I'm a Twitter OG.
[00:13:47.120 --> 00:13:50.800]   Yeah. And you know, a lot. So it has an outside
[00:13:50.800 --> 00:13:55.680]   importance in the in the in the in the in the media community community, right? Yeah.
[00:13:55.680 --> 00:14:01.840]   And tech community and would you grant me a political that it is the public square? I don't
[00:14:01.840 --> 00:14:07.040]   think it's the public square. No, I, but here's the question that I would be raising, which is,
[00:14:07.040 --> 00:14:11.760]   think of the amount of influence he's wielded in the past or the market. I mean, he made a crypto
[00:14:11.760 --> 00:14:17.360]   that nobody cared about become a thing, right? For some people. Right. So another way to look at this
[00:14:17.360 --> 00:14:22.240]   is that Twitter doesn't have to make money. It just has to continue to make waves. And if you
[00:14:22.240 --> 00:14:29.120]   continue to get the right nexus of powerful people in that space talking, do you think Larry
[00:14:29.120 --> 00:14:33.680]   Ellis never has to earn revenue is putting in a billion dollars Prince Alaweed putting in 1.9
[00:14:33.680 --> 00:14:37.760]   billion, not because they expect return on their investment, but because they expect power,
[00:14:37.760 --> 00:14:44.320]   they expect access. Yeah. Yeah. That's exactly what I that that's no other scenario makes sense to
[00:14:44.320 --> 00:14:49.840]   me. There is a risk though, because if Taylor Swift isn't tweeting just Prince Alaweed and Larry
[00:14:49.840 --> 00:14:56.400]   Ellison, no, no, no, I'm not saying like, no, no, I'm not that they don't have to tweet, right?
[00:14:56.400 --> 00:15:01.440]   The point is if you so this is a sort of deep state stuff, if you want politics,
[00:15:01.440 --> 00:15:06.400]   political decisions, policy to go a certain way, you don't speak out loud. You have others speak
[00:15:06.400 --> 00:15:14.240]   for you. You want to be able to have influence, right? Control control the overton window as it
[00:15:14.240 --> 00:15:19.120]   were. That's right. And there could be an argument made at the moment that that's the real net
[00:15:19.120 --> 00:15:23.600]   benefit here that this is not at all about monetizing. That's pretty ugly. Whatever else.
[00:15:23.600 --> 00:15:28.320]   It's kind of dark. It's a dark point. It's well, that's my thing. I want to say,
[00:15:28.320 --> 00:15:34.480]   though, Amy, you make a very good point in that, you know, think about just all of media on the
[00:15:34.480 --> 00:15:43.120]   internet, how hyper decentralized and splintered it's become. And bizarrely, Twitter is the only
[00:15:43.120 --> 00:15:47.840]   or the largest megaphone, I think, left in all media. I mean, I don't even think, you know,
[00:15:47.840 --> 00:15:53.920]   like network television doesn't even have the megaphone power the Twitter has. And so,
[00:15:53.920 --> 00:16:00.960]   Amy, you make a really, really good point here. I mean, it's the last fashion of literally world
[00:16:00.960 --> 00:16:05.200]   power that that could be left. And I mean, you could actually buy it.
[00:16:05.200 --> 00:16:11.280]   Yeah, there's there are other places where people have sort of scurried off to discord
[00:16:11.280 --> 00:16:16.160]   key base, although I think I might literally be the only person. And I know on key base at this
[00:16:16.160 --> 00:16:20.080]   point. I know I left as soon as they got sold as soon, I left. But, you know,
[00:16:20.080 --> 00:16:26.240]   there are other spaces. But the point is there's there is still some something to be said for
[00:16:27.520 --> 00:16:37.840]   having a fairly high profile and you know, tweeting a poop emoji at a CEO. There is some power because
[00:16:37.840 --> 00:16:44.400]   it's being done in front of other people. And, you know, journalists, you know, and a billion
[00:16:44.400 --> 00:16:49.440]   years ago, that was my first career. You know, they're looking at Reddit, they're looking at
[00:16:49.440 --> 00:16:55.760]   Twitter to start developing stories. And so there is this sort of network, right, that goes between
[00:16:55.760 --> 00:17:02.080]   broadcast media, especially, and Twitter and really high profile powerful people like Elon on Twitter.
[00:17:02.080 --> 00:17:05.680]   I like your take. It makes the only thing that makes a lot of sense. Cohen writes,
[00:17:05.680 --> 00:17:10.400]   my view is the Elon's antics in the last week or so have made him into a fool who's grabbing
[00:17:10.400 --> 00:17:18.640]   defeat from the jaws of victory. But in fact, he is playing 40 chess. If he can get the price,
[00:17:18.640 --> 00:17:21.680]   he's going to buy Twitter. In other words, you think he's going to buy Twitter, he's trying to
[00:17:21.680 --> 00:17:26.480]   get the price down. He may end up buying it for 5420 because he doesn't have any choice.
[00:17:26.480 --> 00:17:32.560]   But it's worth it is your point. Right. Well, the other point is now that we've got a financial,
[00:17:32.560 --> 00:17:37.280]   like an honest conversation happening about financials, what other investors going to swoop in,
[00:17:37.280 --> 00:17:40.560]   right? And I mean, Twitter has been for sale for you.
[00:17:40.560 --> 00:17:43.280]   It's way overvalued to begin with. Yeah, nobody's going to buy Twitter.
[00:17:43.280 --> 00:17:49.600]   So he could force, he could force the price down, change some of the rules. And then there would
[00:17:49.600 --> 00:17:55.120]   be a net benefit to getting especially a lot of politicians back and getting them to talk
[00:17:55.120 --> 00:17:58.800]   openly because it's an opportunity to wield power behind the scenes.
[00:17:58.800 --> 00:18:04.000]   If I said to somebody, if you give me $44 billion, you could guarantee who the next president of
[00:18:04.000 --> 00:18:07.360]   the United States would be. That would be worth it to some people.
[00:18:07.360 --> 00:18:17.040]   Absolutely would. And there's huge, he's in two industries, space economy, space tech and
[00:18:17.040 --> 00:18:25.120]   autos that were not disrupted. There was no disruptive innovation happening really before him. And again,
[00:18:25.120 --> 00:18:35.920]   I am not an Elon fan girl at all. But the point is he's very good at both industries heavily
[00:18:35.920 --> 00:18:42.000]   regulated by the government. Both industries heavily subsidized by the federal government.
[00:18:43.120 --> 00:18:49.840]   For Elon, in those businesses, having control of the levers of power in the federal government
[00:18:49.840 --> 00:18:56.000]   are vital to both those businesses. So he doesn't need to make money on Twitter.
[00:18:56.000 --> 00:18:59.280]   He just needs the power Twitter confers. I think he nailed it.
[00:18:59.280 --> 00:19:06.720]   Right. Yeah. That's the only part of this that makes any sense to me whatsoever that. Or he is,
[00:19:09.280 --> 00:19:12.880]   he has a mental health issue that would cause him to make.
[00:19:12.880 --> 00:19:15.520]   I mean, we're always asking that question.
[00:19:15.520 --> 00:19:21.840]   I'm saying it delicately because, you know, listen, it's,
[00:19:21.840 --> 00:19:26.000]   well, he could be a troll. He could be a tension whore. He could be saying,
[00:19:26.000 --> 00:19:29.840]   well, I'm the richest man in the world. But I think he would not risk Tesla, which he has done
[00:19:29.840 --> 00:19:33.120]   unless there were something of real value there. I think that makes a lot.
[00:19:33.120 --> 00:19:34.080]   I think the shareholders would not.
[00:19:34.640 --> 00:19:39.280]   His whoever is that that board, the people that he answers to, again, everybody's got fiduciary
[00:19:39.280 --> 00:19:43.600]   responsibilities and it just wouldn't. There's no other reason that this would make sense.
[00:19:43.600 --> 00:19:46.320]   He'd like us to think he has billionaire brain and he's, you know, just,
[00:19:46.320 --> 00:19:52.240]   but I think maybe he's crazy like a fox. That's really an interesting point of view. David, if
[00:19:52.240 --> 00:19:59.120]   he did this and Twitter's price went down and suddenly others saw it as the potential power
[00:19:59.120 --> 00:20:03.120]   lever that it is, who else would buy it? Microsoft doesn't care about that, do they?
[00:20:03.120 --> 00:20:07.840]   Who else would buy it? No, but it's, it's, well, it's, you just have to look at the list of
[00:20:07.840 --> 00:20:11.840]   billionaires and who's willing to do something like that. I could see Jeff Bezos wanted to buy
[00:20:11.840 --> 00:20:16.160]   Bezos, possibly Larry Ellison, who already has a billing sunk in.
[00:20:16.160 --> 00:20:19.840]   Rupert Murdoch. Why does it? I would think Rupert would jump on the
[00:20:19.840 --> 00:20:23.920]   Yes. Good point. You know, that's the only list you can really look at.
[00:20:23.920 --> 00:20:28.400]   Somebody who has so much money, but this is what you get to when you get all the garks running a
[00:20:28.400 --> 00:20:35.840]   country. This is, this is the place you get to. But just think about it, power is can be
[00:20:35.840 --> 00:20:40.160]   purchased up to a point, but Amy made a great point there that
[00:20:40.160 --> 00:20:48.160]   Twitter's got a level of power that there's no other mechanism that has that level. And the thing
[00:20:48.160 --> 00:20:53.120]   that's also we've seen in the media happening, and this would always fascinate me, is just think
[00:20:53.120 --> 00:21:01.120]   of the sheer number of news stories that are about a Twitter feud, or somebody said this on Twitter.
[00:21:01.120 --> 00:21:06.800]   I'm astonished by just the sheer number of news stories that are just commenting.
[00:21:06.800 --> 00:21:08.240]   Totally.
[00:21:08.240 --> 00:21:08.960]   Lazy journalism.
[00:21:08.960 --> 00:21:10.720]   Totally lazy journalism, but it's happened.
[00:21:10.720 --> 00:21:11.200]   Completely lazy.
[00:21:11.200 --> 00:21:13.040]   Everywhere all the time. Yeah.
[00:21:13.040 --> 00:21:14.000]   Yes. And that's why.
[00:21:14.000 --> 00:21:15.760]   Anyway, Amy wins that story.
[00:21:15.760 --> 00:21:17.600]   Amy wins. We have a winner.
[00:21:18.640 --> 00:21:22.720]   You're amazing. You are so smart. You're incredible.
[00:21:22.720 --> 00:21:23.280]   Does it work?
[00:21:23.280 --> 00:21:24.160]   It's not happening.
[00:21:24.160 --> 00:21:25.440]   It's not happening.
[00:21:25.440 --> 00:21:29.360]   All right. I'll work her a little later.
[00:21:29.360 --> 00:21:30.240]   You know what? I wasn't going to.
[00:21:30.240 --> 00:21:31.200]   We'll get the hives coming out.
[00:21:31.200 --> 00:21:32.560]   I wasn't going to bring up Elon.
[00:21:32.560 --> 00:21:38.400]   But you know what? That's what's great about having Amy and David on is there's a new fresh
[00:21:38.400 --> 00:21:42.560]   take on it. And I think you're, I think after all this conversation of the last six weeks,
[00:21:42.560 --> 00:21:47.360]   you have finally come up with the real reason, the real story behind all of this.
[00:21:47.360 --> 00:21:52.000]   And frankly, it's the most chilling of all possible worlds.
[00:21:52.000 --> 00:21:56.560]   I have zero data, by the way, to back any up.
[00:21:56.560 --> 00:21:58.240]   Anything. It's pure speculation.
[00:21:58.240 --> 00:21:59.200]   Show me. What happened?
[00:21:59.200 --> 00:22:02.400]   I feel like it's 16. Social media happened in 2016.
[00:22:02.400 --> 00:22:03.040]   Yeah.
[00:22:03.040 --> 00:22:07.840]   Right? If Trump had 44 billion, he would buy Twitter.
[00:22:07.840 --> 00:22:09.040]   He's trying to create truth.
[00:22:09.040 --> 00:22:12.080]   Social to kind of create an alternate universe.
[00:22:12.080 --> 00:22:13.040]   But there's a mistake.
[00:22:13.040 --> 00:22:14.560]   All you've got is the base there.
[00:22:14.560 --> 00:22:16.640]   What you have at Twitter is so much more useful.
[00:22:17.600 --> 00:22:22.800]   You also have a working code at Twitter, which is not the case on the other.
[00:22:22.800 --> 00:22:25.040]   Yeah. Apparently two socials not exactly working yet.
[00:22:25.040 --> 00:22:29.520]   All right. I want to take a little break because then we are going to get into something a little
[00:22:29.520 --> 00:22:35.120]   more serious and a little more depressing, which is the Buffalo killing spree and the decision
[00:22:35.120 --> 00:22:37.440]   of the attorney general of the state of New York,
[00:22:38.800 --> 00:22:44.880]   goaded by the governor of New York to investigate gun stores. No.
[00:22:44.880 --> 00:22:49.440]   To investigate right wing terrorist organizations. No.
[00:22:49.440 --> 00:22:54.560]   To investigate Twitch and discord and 4chan.
[00:22:54.560 --> 00:22:58.400]   There's a lot to say there. We'll talk about that in just a bit.
[00:22:58.400 --> 00:23:01.200]   What a panel. So good to have you both.
[00:23:01.200 --> 00:23:02.880]   David, how long has it been since you've been on?
[00:23:02.880 --> 00:23:04.560]   Did you have your count on that?
[00:23:04.560 --> 00:23:06.000]   I was in many years.
[00:23:06.000 --> 00:23:09.440]   I'm trying to remember I was in your original cottage way back.
[00:23:09.440 --> 00:23:09.920]   I know.
[00:23:09.920 --> 00:23:13.120]   And you're the OG to it.
[00:23:13.120 --> 00:23:18.960]   Yeah, I was on that. And then I've done a few of your other shows, but it had been a long time.
[00:23:18.960 --> 00:23:21.360]   Yeah. And also now I'm back to doing traditional media.
[00:23:21.360 --> 00:23:24.960]   Yeah. Well, you know, I think that's what happened. You became a you were we went over the other
[00:23:24.960 --> 00:23:27.680]   side. You became I was in content marketing. I was in that.
[00:23:27.680 --> 00:23:29.680]   But now I'm back to.
[00:23:29.680 --> 00:23:33.840]   That's what happened. I try to I try to avoid marketers PR people.
[00:23:35.120 --> 00:23:39.440]   Because you know, this is supposed to be tech journalists. But I'm totally on board with that.
[00:23:39.440 --> 00:23:46.640]   I have rules for my own show too with with vendor. So I am completely in 100% respect for
[00:23:46.640 --> 00:23:47.120]   love.
[00:23:47.120 --> 00:23:50.880]   It's very old fashioned that concept. Nobody does it anymore.
[00:23:50.880 --> 00:23:56.640]   Let's take a little break and talk about our sponsor. That's that's who makes this all possible.
[00:23:56.640 --> 00:23:59.200]   Unless you're a member of club Twitter, you're watching this ad free.
[00:24:00.000 --> 00:24:05.840]   That case, thank you because of course you make this possible to stamps.com's been a sponsor of
[00:24:05.840 --> 00:24:11.920]   our shows for many years, probably since David was here last. We've been using it. Oh gosh,
[00:24:11.920 --> 00:24:17.120]   more than a decade. I think they started advertising 10 years ago. I have stamps,
[00:24:17.120 --> 00:24:22.560]   actual stamps with my head on them. But I made a mistake when I made them, I didn't make forever
[00:24:22.560 --> 00:24:27.280]   stamps. So every couple of months I have to go to Debbie and I said, could you print me some more
[00:24:27.280 --> 00:24:32.640]   stamps, penny stamps, two cents stamps? So I can still use the picture with my head on it.
[00:24:32.640 --> 00:24:39.760]   That's the problem, right? The postage changes stamps.com solves this. What is stamps.com? That's
[00:24:39.760 --> 00:24:46.000]   just the easiest way to if you're a business to do mailings, all the services of the post office
[00:24:46.000 --> 00:24:50.720]   without going to the post office. So with your computer and your printer, you don't even need a
[00:24:50.720 --> 00:24:56.960]   postage meter. You can print real US postage for packages, for envelopes, for anything you would
[00:24:56.960 --> 00:25:03.520]   want to mail discounts. You won't get at the post office. And now, and if you've been hearing these
[00:25:03.520 --> 00:25:08.480]   ads and saying, well, someday this should put you over the top. Now it's UPS as well as USPS.
[00:25:08.480 --> 00:25:16.240]   So you get the postal service and UPS and discounts. You can't get anywhere else. And stamps.com will
[00:25:16.240 --> 00:25:21.200]   even do the great thing. Stamps.com you'll you say, okay, I want to print this package. It'll say,
[00:25:21.200 --> 00:25:25.120]   you know, it'd be cheaper to ship the UPS or it'll be cheaper to set postal. So it'll actually
[00:25:25.840 --> 00:25:31.280]   pit them one against the other to get you the best rate. Discounts like crazy up to 30% off
[00:25:31.280 --> 00:25:38.000]   postal service rates, 86% off UPS. That's going to save you every single time you mail a package.
[00:25:38.000 --> 00:25:46.320]   More than 1 million businesses use stamps.com because it's easy. It's fast. It'll save you on
[00:25:46.320 --> 00:25:51.760]   shipping. It'll save your time. And it also looks more professional, whether you're on Amazon or
[00:25:51.760 --> 00:25:58.800]   Etsy or or eBay or your full blown warehouse shipping out truckloads of orders, those packages look
[00:25:58.800 --> 00:26:03.760]   more professional that you can print right on the envelopes with your company logo, the return
[00:26:03.760 --> 00:26:10.240]   address. It's just a great mailing and now shipping solution for you. And it'll take by the
[00:26:10.240 --> 00:26:13.520]   way, save you time too, because it'll take all the information from those websites.
[00:26:13.520 --> 00:26:20.240]   Automatically, you fill out forms that you need, whether it's international shipping forms or
[00:26:21.280 --> 00:26:26.960]   or certified mail, all that stuff just fills it out automatically. And by the way, easy. You'll
[00:26:26.960 --> 00:26:32.320]   be up and running in minutes. You could do it before the show's over printing official US postage
[00:26:32.320 --> 00:26:36.080]   for any letter, any package anywhere you want to send. They've been a partner of this show since
[00:26:36.080 --> 00:26:40.880]   2012. If you haven't signed up yet, well, I don't know what you're waiting for. Do it right now.
[00:26:40.880 --> 00:26:46.320]   So start mailing and shipping now with stamps.com. Keep more money in your pocket every day.
[00:26:46.320 --> 00:26:55.200]   Go to stamps.com. Click the link that says, "Hurt us on a podcast" and enter the offer code TWIT.
[00:26:55.200 --> 00:26:59.200]   Wow, what a deal you're going to get. Not only a four week trial, you're going to get a ton of
[00:26:59.200 --> 00:27:05.120]   free postage you can use over a period of time, a digital scale, stamps.com, no long term commitments,
[00:27:05.120 --> 00:27:11.520]   no contracts, just a chance to try it. Go to stamps.com, click the microphone at the top of the page,
[00:27:11.520 --> 00:27:18.160]   enter TWIT, and enter a new world of mailing and shipping stamps.com.
[00:27:18.160 --> 00:27:27.040]   I think Mike Masnick's headline on TechDirt.com kind of says it all, "New York launches ridiculous,
[00:27:27.040 --> 00:27:36.640]   blatantly unconstitutional, "investigations" into Twitch and Discord, semicolon deflecting blame
[00:27:36.640 --> 00:27:43.840]   from New York's own failings. We know that the shooter, I'm not going to, by the way,
[00:27:43.840 --> 00:27:50.000]   say his name, not going to give many publicity, but we know that he opened a Discord chatroom
[00:27:50.000 --> 00:27:56.000]   before the shooting. Anybody can do that. It's free. We have a Discord room. It couldn't be
[00:27:56.000 --> 00:28:03.440]   simpler set up. Invited people in to discuss his plans. The police say it was as many as 15 people
[00:28:03.440 --> 00:28:10.320]   in there before he did anything. Those people knew what he was about to do. He then started
[00:28:10.320 --> 00:28:15.280]   up a Twitch stream, which only ran for a couple of minutes as he launched into his attack.
[00:28:15.280 --> 00:28:20.480]   Twitch pulled it down right away. I think I saw 40 people saw it on Twitch, but since then,
[00:28:20.480 --> 00:28:28.160]   40 million people have seen it as it's been passed around. David, is Twitch or Discord
[00:28:28.800 --> 00:28:34.960]   to blame for any of this? Well, I don't think any of us believe this, and this is why we're all
[00:28:34.960 --> 00:28:42.320]   kind of shocked at the attacks of Twitch and Discord for that. But it's interesting that 15
[00:28:42.320 --> 00:28:51.120]   people in Discord heard what they were saying. Nothing. But this goes to, and this is something
[00:28:51.120 --> 00:28:56.640]   we talk about in our shows about cybersecurity and about security awareness training is,
[00:28:57.360 --> 00:29:03.440]   you know, there's this fear of, well, it's probably nothing. I don't want to cause a stir. I don't
[00:29:03.440 --> 00:29:10.640]   want to say anything. There's a fear of that. But man, there is a, you need to say something when
[00:29:10.640 --> 00:29:16.160]   you see something like this, and it's not for you to deal with it. It's for authorities to deal with
[00:29:16.160 --> 00:29:23.440]   it. And we need everybody's eyes and ears to act as, you know, monitors for these kinds of things.
[00:29:23.440 --> 00:29:29.360]   And, you know, don't cast blame when you may not know something that's going on. But if someone
[00:29:29.360 --> 00:29:35.200]   is talking about some serious, violent, dangerous things on a social channel, alert an authority.
[00:29:35.200 --> 00:29:41.840]   I have to say this shooter was clearly radicalized into believe he's only 18 years old. It's
[00:29:41.840 --> 00:29:47.440]   believing this BS replacement theory, which, you know, frankly, Fox News has been flogging.
[00:29:48.720 --> 00:29:53.680]   But I don't see any, but I don't, I don't see anybody investigating Fox News, nor do I think
[00:29:53.680 --> 00:30:03.120]   they should. This, the First Amendment protects that. Yes. I disagree. Good. I disagree. I didn't
[00:30:03.120 --> 00:30:11.040]   mean to interrupt. I'm sorry. No, please. Should is should Twitch discord for Chan and 8chan. They're
[00:30:11.040 --> 00:30:15.520]   also under investigation. Be a, now that's the internet. They didn't say anything about
[00:30:15.520 --> 00:30:19.520]   investigating news media, probably realizing that they're going to, you know, that's a
[00:30:19.520 --> 00:30:23.360]   potential problem with the First Amendment. But should they be held responsible?
[00:30:23.360 --> 00:30:31.040]   So I'm not a constitutional scholar or lawyer. I made a last minute decision not to go to law
[00:30:31.040 --> 00:30:35.840]   school. So I'm, I'm with those caveats being said, I did pretty well in the all set.
[00:30:37.360 --> 00:30:43.040]   Well, when those caveats being said, the First Amendment was written in 1791.
[00:30:43.040 --> 00:30:53.600]   And the point of it in 1791 was to, was to require Congress to restrict speech
[00:30:53.600 --> 00:31:01.040]   and to restrict the press in promotion of the public good. So what's happened in the, in the,
[00:31:01.040 --> 00:31:07.920]   to prevent, to require Congress to prevent Congress, it requires con basically,
[00:31:07.920 --> 00:31:13.680]   Congress shall make no law respecting, establish a religion or prohibiting free speech there of,
[00:31:13.680 --> 00:31:18.800]   free exercise there of, or a bridging the freedom of speech or of the press or of the right of
[00:31:18.800 --> 00:31:23.360]   peace, people to peaceably assemble and to petition the government for regressed. It's very simple.
[00:31:23.360 --> 00:31:28.160]   It says Congress shall make no law. By the way, we should point out that almost immediately after
[00:31:28.160 --> 00:31:33.280]   this, the Sedition Acts were passed, which did exactly that and was used to prosecute people
[00:31:33.280 --> 00:31:39.360]   like crazy. So the history of, of enforcement of the First Amendment is not, is very checker to say
[00:31:39.360 --> 00:31:44.720]   the least. No, the, the, again, I think the original intent of the founders, I wasn't there. So I
[00:31:44.720 --> 00:31:50.640]   don't have quantitative data on this, but it's my interpretation that the founders originally were
[00:31:50.640 --> 00:31:57.120]   just trying to make it so that the public good was what was preserved. And at the time, so again,
[00:31:57.120 --> 00:32:04.720]   this is like the seven, this is 1791. It was very expensive to create speech in written form and
[00:32:04.720 --> 00:32:10.480]   handed out. So like, you know, eight functioning newspapers were sort of just getting started.
[00:32:10.480 --> 00:32:18.720]   And it was very, very expensive. So free speech was meant to be not restricted in certain cases,
[00:32:18.720 --> 00:32:25.920]   but at that period of time when this was invented, there was no completely unfettered speech. And
[00:32:25.920 --> 00:32:33.440]   certainly there were no algorithms. I would ask the question, does a strict interpretation
[00:32:33.440 --> 00:32:38.000]   of the First Amendment, which is it becomes one of these like super polarizing things. There's just
[00:32:38.000 --> 00:32:46.960]   no room for nuance. I would ask in an age of algorithmic determinism and, um, unrest, you know,
[00:32:46.960 --> 00:32:54.640]   completely unfettered speech that's also unedited and can be taken out of context, you know, does,
[00:32:54.640 --> 00:33:00.080]   does our strict interpretation of free speech make sense? And I don't think, I know that I'm in a
[00:33:00.080 --> 00:33:05.120]   minority here, but I actually don't think that it does. I'm not saying we go the direction of China.
[00:33:05.120 --> 00:33:08.640]   So let's say we go down that direction, Amy, let's say we go down the direction,
[00:33:08.640 --> 00:33:15.840]   then becomes the issue of who then decides who is the grand decider. And that becomes a big
[00:33:15.840 --> 00:33:20.560]   issue in itself. I should mention that we have more updated laws since the First Amendment. And
[00:33:21.280 --> 00:33:27.680]   I was feeling that section 230 of the Communications Decency Act actually protects, you know, or,
[00:33:27.680 --> 00:33:31.600]   you know, groups like Discord and Twitch, which essentially, and I'm quoting here,
[00:33:31.600 --> 00:33:36.320]   no provider user of an interactive computer service shall be treated. I see publisher,
[00:33:36.320 --> 00:33:41.840]   publisher or speaker of any, any information provided by another information content provider.
[00:33:41.840 --> 00:33:46.800]   So essentially they're not, and we've dealt, and this has come up many times, and this is just for
[00:33:46.800 --> 00:33:52.240]   any like hate speech or pornography or anything else that appears on some type of service that
[00:33:52.240 --> 00:33:57.840]   they are protected for liability because someone else published something on that.
[00:33:57.840 --> 00:34:01.440]   That's why what you say makes me nervous because there have been a lot of people who said,
[00:34:01.440 --> 00:34:08.240]   let's get, let's abandon section 230. It's too broad. And I disagree wholeheartedly because
[00:34:08.240 --> 00:34:13.200]   with that section 230, there is no moderation. In fact, Mike Masnick, a protector points out,
[00:34:13.200 --> 00:34:20.000]   Twitch might have been by the New Texas Social Media Law, prevented by that law from pulling down
[00:34:20.000 --> 00:34:28.160]   the stream. They would have to justify it and then could be sued over it by the New Texas law.
[00:34:28.160 --> 00:34:34.400]   Again, I think the problem here is that we are thinking with too many constraints and parameters.
[00:34:34.400 --> 00:34:36.480]   Let's go back to the year 2000.
[00:34:36.480 --> 00:34:39.200]   Yeah, let's just start from scratch and let's rerun.
[00:34:39.200 --> 00:34:45.120]   What would you say as a founding mother today that should be our constitutional right?
[00:34:45.120 --> 00:34:52.320]   I want to address the problem. In the year 2000, I was at the Columbia Graduate School of Journalism.
[00:34:52.320 --> 00:34:56.800]   So great school, if you want to be a journalist and pay a lot of money for the privilege of having
[00:34:56.800 --> 00:35:03.040]   a degree. I'm in the first ethics class, and I had just moved back to the United States from
[00:35:03.040 --> 00:35:08.960]   Japan, and it was like I had traveled back in time from the future because of the technology that
[00:35:08.960 --> 00:35:15.520]   we had here, and it was not comparable in any way. So we spent an hour and a half looking at the
[00:35:15.520 --> 00:35:23.520]   Newsweek and Time magazine covers of OJ Simpson. Some of you will remember that it was Time magazine
[00:35:23.520 --> 00:35:30.000]   that ran the unedited version. One of the magazines ran an unedited photo, his mugshot.
[00:35:30.000 --> 00:35:30.640]   I remember this.
[00:35:30.640 --> 00:35:31.680]   The other one darkened it.
[00:35:31.680 --> 00:35:35.440]   Okay, so this is everybody's favorite. It was the first day of ethics class,
[00:35:35.440 --> 00:35:40.240]   and the question that we were being asked was, should journalists be allowed to use
[00:35:40.240 --> 00:35:46.000]   digital cameras and digital editing software? Was that the right thing to do?
[00:35:46.000 --> 00:35:52.000]   Here's the Time magazine cover. Here's in New York cover, and you could see the dark and OJ
[00:35:52.000 --> 00:35:58.880]   in the time cover. Here's what I said. My response to that question was, why are we having this
[00:35:58.880 --> 00:36:04.640]   conversation on my phone, which I realized doesn't exist yet in the United States, but this phone
[00:36:04.640 --> 00:36:09.360]   in the year 2000, I've got a camera in this phone. Pretty soon, literally everybody is
[00:36:09.360 --> 00:36:14.640]   going to be taking pictures with their phones, and we will be able to send things fast.
[00:36:14.640 --> 00:36:20.320]   And the question, should journalists be allowed to do this is the wrong question.
[00:36:20.320 --> 00:36:25.520]   To me, that is, and I like nobody wanted to have that. Nobody wanted to engage conversation.
[00:36:25.520 --> 00:36:30.480]   So Time's cover is protected, but Congress can't make a law saying you can't darken
[00:36:30.480 --> 00:36:36.800]   somebody's image on the cover of your magazine. My point is we're asking the wrong questions.
[00:36:36.800 --> 00:36:42.720]   It's not, should Section 230 be scrapped or rewritten. We have to
[00:36:42.720 --> 00:36:50.800]   approach these conversations with better questions and more nuance. And David, I don't have an
[00:36:50.800 --> 00:36:55.680]   answer to who gets to be the arbiter of what the news rules look like. What I'm actually in favor
[00:36:55.680 --> 00:37:03.040]   of is, in this particular case, an incremental approach that as technology changes, we should
[00:37:03.040 --> 00:37:08.800]   not be playing catch up. We should rather be creating policy and laws that are able to keep pace
[00:37:08.800 --> 00:37:14.160]   with the changes. That actually makes sense. That's a very reasonable thing to say, and I agree with
[00:37:14.160 --> 00:37:19.920]   it. Right, but historically, you know, it's hard to do. Yeah, it's policy and regulations keeping
[00:37:19.920 --> 00:37:27.360]   pace with technology does not have a good history at all. Sorry, go ahead.
[00:37:27.360 --> 00:37:32.960]   No, I interrupted you. I'm sorry. What I'll just complete is what happens is,
[00:37:32.960 --> 00:37:38.640]   organizations trying to protect their own brand develop their own sort of rules and
[00:37:38.640 --> 00:37:44.240]   regulations of what can and cannot be on the platform. And the hope is they can do that. Look,
[00:37:44.240 --> 00:37:49.680]   Twitch, if someone's doing something horrific on their platform, there's only a certain amount
[00:37:49.680 --> 00:37:55.440]   of speed that you can catch it and stop it. I mean, the number of people who are monitoring it is
[00:37:55.440 --> 00:38:02.640]   an infinite. So some stuff slips through for a period of time. And fortunately, Twitch was able
[00:38:02.640 --> 00:38:12.080]   to take this down. More my concern is, we need to educate the populace of when you see scary dialogue
[00:38:12.080 --> 00:38:20.160]   going online. We don't want to get into a state where we're reporting on our neighbors, but
[00:38:20.160 --> 00:38:26.000]   there has to be a state of if someone's going to yell fire in a movie theater, we need to report
[00:38:26.000 --> 00:38:33.840]   that. There is a very clear line at that point. Right, but so again, just to untangle a few things.
[00:38:33.840 --> 00:38:38.000]   First of all, from a technology point of view, there's absolutely a way to solve this problem.
[00:38:38.000 --> 00:38:43.200]   China is doing it every day through brute force and really smart machine learning.
[00:38:43.200 --> 00:38:47.200]   But you did say you don't want it. You don't. I mean, China just passed a law that said you can only
[00:38:47.200 --> 00:38:53.200]   be upbeat and positive. Right. I don't. I don't think that's the right way. But here's the issue.
[00:38:53.200 --> 00:38:58.400]   The challenge that we face in this country with just about everything is that there's a there's a
[00:38:58.400 --> 00:39:05.280]   monetary incentive tied to everything. Huge. So the reason there is a reason that between
[00:39:06.000 --> 00:39:12.320]   whatever 2010, 2011, 2014, over and over, I was at Twitter talking about
[00:39:12.320 --> 00:39:20.080]   some of these issues. And the problem is that if you make the bad parts of Twitter go away,
[00:39:20.080 --> 00:39:26.800]   that's partially what drives people to have those. That's what partially drives people there.
[00:39:26.800 --> 00:39:33.040]   So part of the challenge is that we've got the wrong incentive structure to try to compel
[00:39:33.040 --> 00:39:36.480]   these companies to fix things. There are parallels in artificial intelligence.
[00:39:36.480 --> 00:39:44.000]   We know that there are all kinds of huge transformer models that are very great,
[00:39:44.000 --> 00:39:49.680]   but that have been trained on data or bad data. Sorry. So like, we know that these problems exist,
[00:39:49.680 --> 00:39:55.920]   but there's no incentive to fix any of it. In fact, there's a it's it's going to be
[00:39:55.920 --> 00:40:00.800]   not with a penalty. There's a financial disincentive to fix these things. So what I would like to see
[00:40:01.440 --> 00:40:07.120]   is totally different legislation that doesn't focus on how are we going to define
[00:40:07.120 --> 00:40:12.400]   this particular act being shown on video at this particular moment in time, right? Because
[00:40:12.400 --> 00:40:16.800]   that's all it's a moving target. It's always going to change. Is there a way that we can compel
[00:40:16.800 --> 00:40:24.720]   the platforms to promote to attack what they know is bad stuff head on through incentives that are
[00:40:24.720 --> 00:40:29.600]   tied to revenue? I think that's the only one is nobody's going to come to the table and do
[00:40:29.600 --> 00:40:34.160]   this voluntarily. It's falling upon this Supreme Court at Facebook is falling apart.
[00:40:34.160 --> 00:40:40.320]   We've got our own government agency where where a misinformation disinformation board didn't work
[00:40:40.320 --> 00:40:46.320]   for reasons that are tied to disinformation itself, which is tied to how the platforms make money.
[00:40:46.320 --> 00:40:52.720]   But Amy, don't you think if if there could be financial incentives attached to this,
[00:40:52.720 --> 00:40:57.920]   don't you think we would have seen this by now? Because I mean, I like conceptually what you're
[00:40:57.920 --> 00:41:02.320]   saying, but I don't understand even I can't even grok how the heck that would happen.
[00:41:02.320 --> 00:41:10.880]   Yeah. So again, we just haven't done it before. So there's there's a carrot and stick model that
[00:41:10.880 --> 00:41:17.520]   can be accomplished through all kinds of things that are going to be wildly unpopular.
[00:41:17.520 --> 00:41:26.240]   But we know work, right? Tax incentives. There are lots of ways to achieve this
[00:41:26.240 --> 00:41:36.320]   if companies can commit to measure and enforce making sure that content is in that the sort of
[00:41:36.320 --> 00:41:41.600]   is being generated for the public good. And I know that this immediately, the easier thing to
[00:41:41.600 --> 00:41:46.400]   argue about and have a conversation about is censorship. The harder thing to do is to
[00:41:46.400 --> 00:41:50.960]   solution it, right? To get to a solution that involves a different type of incentive structure.
[00:41:50.960 --> 00:41:53.840]   Is it going to come from government? Is that where it's going to come from?
[00:41:55.440 --> 00:42:00.320]   You know, it's like a company. I don't think so. The companies are going to fight that and
[00:42:00.320 --> 00:42:04.160]   they're going to have, right? You've really raised a larger issue though. Larry Lessig,
[00:42:04.160 --> 00:42:11.040]   the great Larry Lessig, who was for a long time as a advocate, a strong advocate for eliminating
[00:42:11.040 --> 00:42:17.040]   copy protection and changing copyright law finally gave up and realized I can't do this
[00:42:17.040 --> 00:42:22.960]   because of the perverse financial incentives and the money in Congress. He said, we can't do
[00:42:22.960 --> 00:42:29.920]   anything until we get money out of politics. And that became his cause, by the way, he's still
[00:42:29.920 --> 00:42:34.480]   working on that. He's somewhere around that. I don't think that's going to ever happen.
[00:42:34.480 --> 00:42:39.360]   So I understand if you could say, well, if there were no perverse financial incentives,
[00:42:39.360 --> 00:42:44.560]   we might be able to come up with a framework. But that's like saying, you know, if the moon
[00:42:44.560 --> 00:42:49.680]   were made a cotton candy, we could all live on Mars. I mean, it doesn't. It's just different.
[00:42:49.680 --> 00:42:55.840]   It requires reperception. Reperception is the art of taking the data that surround you at any
[00:42:55.840 --> 00:43:00.320]   given moment and looking for the white space. And this is what you're so good at getting you
[00:43:00.320 --> 00:43:05.200]   out of the box. This is what you do. You know, all we have to do is spend a little time,
[00:43:05.200 --> 00:43:11.440]   not us right now. But like a huge group of people should just spend some time
[00:43:11.440 --> 00:43:16.560]   batting this different idea around. And if you get to the end and decide, this is not going to
[00:43:16.560 --> 00:43:20.480]   work fine. The problem is that we're not even thinking about it. We're not even going.
[00:43:20.480 --> 00:43:25.120]   No, and I feel like I know a lot of the people who listen to the show are the ones who have those
[00:43:25.120 --> 00:43:31.360]   great ideas. And sometimes you bring those ideas up at work and or you're afraid to because you
[00:43:31.360 --> 00:43:36.880]   don't want to be ostracized. And like, because it's different, different equals bad, right?
[00:43:36.880 --> 00:43:44.080]   Different should equal curiosity. Let's exercise our curiosity and see where we get. And at the end,
[00:43:44.080 --> 00:43:48.960]   we get to the bad place. Fine. We're going to be in no worse situation than we are today.
[00:43:48.960 --> 00:43:55.520]   I agree with you. We should do all that. But I also think given the exigencies of
[00:43:55.520 --> 00:44:01.680]   the world as it is the real, you know, real politic, the real world, we are lucky we have
[00:44:01.680 --> 00:44:11.760]   something so simple and so clear as the First Amendment and Section 230, which is, by the way,
[00:44:11.760 --> 00:44:18.400]   very short, very succinct description of and yes, there may be a better way than that.
[00:44:18.400 --> 00:44:24.400]   But because they're so simple and so clear and they have and I think they have worked for the most
[00:44:24.400 --> 00:44:31.440]   part for so long, I think that that's a good fall. If they were if they were simple and clear,
[00:44:31.440 --> 00:44:36.240]   there would be no ambiguity. Well, there's going to be debate. We would not have two centuries of case
[00:44:36.240 --> 00:44:40.720]   law. Yeah, arguing. There's debate. The interpretation actually means that's the nature of the
[00:44:40.720 --> 00:44:43.840]   Constitution and the nature of the world is and that's why you have courts.
[00:44:43.840 --> 00:44:48.960]   I don't think that that's the wrong thing. That's kind of more of what you were talking about.
[00:44:48.960 --> 00:44:55.520]   Let's refine this. But don't you think it is there any country in the world that is doing
[00:44:55.520 --> 00:45:00.800]   something that you think is a good direct? I understand nothing's perfect, but a good direction
[00:45:00.800 --> 00:45:05.920]   that we should be looking at is the Chinese. This new Chinese rule is very interesting.
[00:45:05.920 --> 00:45:10.320]   Is it a good rule? No, of course not. I think that the amount of policing that
[00:45:10.320 --> 00:45:14.960]   happens in China, I mean, for God's sake, there are people who are worried at this point about
[00:45:14.960 --> 00:45:21.200]   using VPNs to talk about the absolutist COVID policies, zero tolerance throughout the country.
[00:45:21.200 --> 00:45:26.960]   Even people who live outside of China, who use Weibo and some of the other social networks,
[00:45:26.960 --> 00:45:34.480]   who clearly have a different IP, they're being targeted by people. They're not bots. They're
[00:45:34.480 --> 00:45:42.560]   actual people. They're being targeted and attacked with propaganda and being ousted as foreign spies
[00:45:42.560 --> 00:45:50.240]   or people trying to take down the CCP. No, I don't think that model is good. I see Scandinavia,
[00:45:50.240 --> 00:45:53.200]   and I know that I bring it up a lot. I don't mean to sound like a broken record, but
[00:45:54.160 --> 00:46:05.040]   we don't see a lot of the serious extremes. There are problems, but Norway, Sweden, Finland,
[00:46:05.040 --> 00:46:09.520]   you don't see a lot of the same volume.
[00:46:09.520 --> 00:46:16.480]   It's a problem. Well, I mean, Anders Breivik killed 77 people in Norway about 20 years.
[00:46:16.480 --> 00:46:24.080]   Yes, that was a right. There was a horrific incident in Sweden, but it's not two weekends
[00:46:24.080 --> 00:46:29.280]   ago. We had awful, or I guess this was last weekend. There were multiple mass.
[00:46:29.280 --> 00:46:38.800]   The Chinese, this is a new rule. By the way, a very excellent thread from Kendra Schafer
[00:46:38.800 --> 00:46:45.360]   from a few months, I guess almost a year ago now about the Chinese cyberspace watchdog
[00:46:46.080 --> 00:46:52.960]   CAC regulations, many of which I think privacy advocates in the US would say, "Ooh, yeah, good."
[00:46:52.960 --> 00:46:59.280]   But the funny thing is, this is the funny quote, "The algorithm's recommendation service
[00:46:59.280 --> 00:47:04.320]   provider should adhere to mainstream values," by the way, when the CAC publishes that,
[00:47:04.320 --> 00:47:09.600]   you know what that means, mainstream values, "actively spread positive energy
[00:47:11.200 --> 00:47:16.400]   and promote the application of algorithms for the better." It's very communist, utopianism,
[00:47:16.400 --> 00:47:23.440]   speech. There is this paradox that has yet to be solved in the United States.
[00:47:23.440 --> 00:47:29.600]   Everybody seems to be very concerned, or at least they talk about being very concerned about their
[00:47:29.600 --> 00:47:36.560]   privacy. They don't want to be surveilled. At the same time, they want absolute free speech.
[00:47:36.560 --> 00:47:42.320]   Everybody else should be surveilled. There is a tension. There's also no acknowledgement
[00:47:42.320 --> 00:47:49.280]   that we are all constantly being surveilled all of the time by the same exact platforms.
[00:47:49.280 --> 00:47:59.760]   So in a cloaked way, there's huge piles of data, and all of these companies are using the data for
[00:47:59.760 --> 00:48:05.120]   the purpose of making more money, for advertising. That somehow isn't defined as speech.
[00:48:05.760 --> 00:48:14.640]   But there's a paradox between, again, what we get so upset about it. I wish there was a way to have
[00:48:14.640 --> 00:48:20.640]   a conversation that it did not immediately devolve to, "Well, who's going to decide this is
[00:48:20.640 --> 00:48:28.000]   censorship, China?" It's important that we have these conversations, and I really hope everybody
[00:48:28.000 --> 00:48:33.680]   listening will be willing to engage in a more nuanced version of that conversation, because a lot of
[00:48:33.680 --> 00:48:37.120]   you are the people who are going to help build this other future, I hope.
[00:48:37.120 --> 00:48:44.800]   Look, I want to. I agree with you, and I'm trying. But it's hard for me to imagine a framework
[00:48:44.800 --> 00:48:52.000]   that isn't just transferring control to another party.
[00:48:52.000 --> 00:48:59.840]   But there's two different ways also to do about this, and this goes into what you were talking
[00:48:59.840 --> 00:49:06.000]   about with regards to privacy. I want to give some credit to Davi Oppenheimer of Inrupt. That's
[00:49:06.000 --> 00:49:11.280]   Tim Burtner's lead. That's a TBL thing, yeah. Yes, his company, and Davi works from it. I
[00:49:11.280 --> 00:49:17.520]   literally just before this show, I talk with him, and this goes to another story that's on our list
[00:49:17.520 --> 00:49:26.960]   about the fact that these tools that we use are sharing our information hundreds of times,
[00:49:26.960 --> 00:49:35.680]   more so in the US than even in the UK for that matter. We don't get to decide that.
[00:49:35.680 --> 00:49:41.520]   They're deciding for us. There's so many decisions that are being made for us about our own information,
[00:49:41.520 --> 00:49:50.000]   which is kind of tragic. Now, this is Inrupt's whole charge is to reverse that, where you control
[00:49:50.000 --> 00:50:00.480]   your own information as to who it goes to. You know where it's going, and it's being used.
[00:50:00.480 --> 00:50:05.600]   And actually, and I thought quite the opposite, but what Davi said is advertisers really,
[00:50:05.600 --> 00:50:12.080]   really love this kind of concept where we know people truly like this because they
[00:50:12.080 --> 00:50:19.280]   individually approved, they wanted it. That shows that their money that they're spending on
[00:50:19.280 --> 00:50:24.320]   advertising, and I know I'm sort of directing this slightly differently, is actually being used
[00:50:24.320 --> 00:50:32.320]   appropriately. So if we get to a point where people are self-directed, can take ownership of
[00:50:32.320 --> 00:50:41.040]   their information, and it's not used unknowingly by them, then that is a good direction to go. Now,
[00:50:41.040 --> 00:50:48.080]   per what you were saying, Amy, as to somebody or having a conversation of how we're going to find
[00:50:48.080 --> 00:50:53.280]   the defining line of what can and can't be said, and how much it can be said, and on what platforms
[00:50:53.280 --> 00:50:57.680]   we've already been having this conversation. I don't think this is the first time it's been
[00:50:57.680 --> 00:51:03.680]   brought up. I feel like it's authoritarianism in any fashion that you could think of.
[00:51:03.680 --> 00:51:12.160]   If you're not an absolutist on free speech, then you're giving somebody control of speech.
[00:51:12.160 --> 00:51:17.760]   And I don't think in any situation that that's going to be acceptable, because even if you trust
[00:51:17.760 --> 00:51:23.440]   that person today, the reins of power could be passed on tomorrow.
[00:51:23.440 --> 00:51:31.440]   I feel like free speech absolutism is the only way to go here. I know that's exactly what you're
[00:51:31.440 --> 00:51:39.680]   arguing against, Amy. Am I right? Again, I'm well, it's me. So generally speaking, I'm not in favor
[00:51:39.680 --> 00:51:46.720]   of any absolutist regardless of what it is. But anything that would say, you can't say,
[00:51:47.280 --> 00:51:53.600]   I feel like if we just let everybody speak, algorithms are a problem because we are
[00:51:53.600 --> 00:51:58.320]   boost, we are amplifying artificially some speech. So I agree, that's a problem.
[00:51:58.320 --> 00:52:03.520]   But if you could say everybody can say whatever they want, and everybody else could
[00:52:03.520 --> 00:52:10.080]   judge everything that they say, if you get the algorithms out of it, that's true free speech.
[00:52:10.080 --> 00:52:15.040]   Nobody's amplified over anybody else. One of the things that the internet has given us
[00:52:15.040 --> 00:52:21.440]   is a much more democratic communications medium. So everybody can have a voice if they want.
[00:52:21.440 --> 00:52:29.440]   Let everybody speak. Teach people how to think critically about what they read.
[00:52:29.440 --> 00:52:36.160]   Do not algorithmically amplify stuff so that you don't get radicalized.
[00:52:36.160 --> 00:52:44.080]   Isn't that the only possible solution? Anything else is somebody, whether it's you or
[00:52:44.080 --> 00:52:49.840]   President Biden or Donald Trump saying, that's okay, that's not okay or Google saying that's
[00:52:49.840 --> 00:52:53.040]   okay. I don't want to see that. I think that's always fraught.
[00:52:53.040 --> 00:53:00.080]   Okay, so let me offer a counterpoint. So broadcast media in the United States,
[00:53:00.080 --> 00:53:04.320]   and in particular, national public radio. So actually there's a listener,
[00:53:04.320 --> 00:53:09.280]   Dominico Zafora, sorry if I mispronounced your name, who brought this up on Twitter,
[00:53:09.280 --> 00:53:13.840]   literally just now. We have standards and practices. You can't just broadcast anything
[00:53:13.840 --> 00:53:19.120]   you want on. Only on the airway. You can on, you can on, that's just an FCC.
[00:53:19.120 --> 00:53:21.840]   Different rules on the internet. You know, you're not anything you want.
[00:53:21.840 --> 00:53:27.920]   I'm well aware of that. But again, I'm trying to make an analogy here. So what,
[00:53:27.920 --> 00:53:33.440]   you know, the platform in a way is like this notebook that I'm holding up. This is also a
[00:53:33.440 --> 00:53:38.480]   platform and I can, and I can write anything that I want on it, right? And if I write horrible
[00:53:38.480 --> 00:53:43.600]   things, I guess, or if something horrible happens, nobody's going to sue the maker of this notebook,
[00:53:43.600 --> 00:53:46.480]   right? They're going to, I'm going to be the one that gets in trouble. And so I think that's the
[00:53:46.480 --> 00:53:52.400]   position that modern media has taken. We're just the platform we are just distributing.
[00:53:52.400 --> 00:53:57.040]   There have been questions about whether or not algorithms are entitled to the same rights to
[00:53:57.040 --> 00:54:03.040]   speech as humans are, and that has yet to be decided. But I have to ask a question. If we
[00:54:03.040 --> 00:54:08.960]   continue to, you know, you can't say certain words on NBC at eight o'clock at night, more can you
[00:54:08.960 --> 00:54:14.480]   show nudity or certain types of violence? Why do we hold, I mean, I know legally why we hold
[00:54:14.480 --> 00:54:21.360]   broadcast media to a different standard than a platform, but from a functional societal point
[00:54:21.360 --> 00:54:28.400]   of view, if we are free speech absolutists, then why hold broadcast media to some standards?
[00:54:28.400 --> 00:54:34.080]   Because that can't be a time of scarcity. Just that scarcity you were talking about.
[00:54:34.080 --> 00:54:37.680]   But as scarcity goes away and scarcity is going away, thanks to the internet,
[00:54:37.680 --> 00:54:39.920]   I don't think you'd have to do those regulations anymore.
[00:54:39.920 --> 00:54:45.760]   And so because there were three networks and there were a very, and it was a scarcity of the
[00:54:45.760 --> 00:54:50.720]   airwaves and there was no other way to do it, you had some regulation. But let's get rid of the
[00:54:50.720 --> 00:54:56.000]   scarcity which we have. I think the regulation goes away too. And it did. We don't have, we don't have
[00:54:56.000 --> 00:55:00.720]   equal time. We don't have the seven dirty words. None of that. And I think that that's a good thing.
[00:55:00.720 --> 00:55:06.720]   And if you don't, if you thought that Twitter would be full of expletives and you didn't want
[00:55:06.720 --> 00:55:09.920]   that for yourself or your family, you wouldn't listen. That's fine.
[00:55:09.920 --> 00:55:15.120]   I wouldn't listen, but I am not a 15 year old kid who's hanging out. Listen, my daughter,
[00:55:15.120 --> 00:55:19.360]   we did. Then that's up to the parents. If you want, I mean, a parent has the right to regulate
[00:55:19.360 --> 00:55:22.800]   their children. That's different. I would, I would, I would say there's.
[00:55:22.800 --> 00:55:28.960]   But we know that that's not happening. Well, that's not then big brother doesn't get to come in
[00:55:28.960 --> 00:55:33.040]   and fix it. I don't care if that's not happening. Here's the question that I would love for us to
[00:55:33.040 --> 00:55:37.360]   ask. If we go into the future 20 years, what kind of life do we want to have?
[00:55:37.360 --> 00:55:43.440]   Do we want to have more like, because we have these multi faceted problems,
[00:55:43.440 --> 00:55:47.520]   all of which at this point now intersect with speech, whether that's catastrophic climate change
[00:55:47.520 --> 00:55:54.240]   or horrific gun violence or economic insanity. We have all of these problems,
[00:55:54.240 --> 00:56:01.840]   all of them intersect with both algorithms, tech and speech. So we have to ask the challenging
[00:56:01.840 --> 00:56:06.960]   question 20 years from now, if we make no change, what's our baseline? What's the baseline scenario?
[00:56:06.960 --> 00:56:11.520]   And from my vantage point, the baseline scenario is not a good one. We are in a worse situation.
[00:56:11.520 --> 00:56:16.800]   I don't have an immediate answer. I'm not suggesting that the government or some
[00:56:16.800 --> 00:56:24.560]   arbitrary body of people start policing what we say, how to whom and where. But I also don't think
[00:56:24.560 --> 00:56:32.000]   we should continue to support total anonymity and totally unfettered.
[00:56:32.000 --> 00:56:39.760]   All anything goes without any type of consequence in a real way, because the problem here is that
[00:56:39.760 --> 00:56:47.920]   I don't see us mitigating these other problems. And so we wind up more politically polarized,
[00:56:47.920 --> 00:56:53.680]   we wind up with more economic uncertainty. Again, this is just my opinion. And I think we wind up
[00:56:53.680 --> 00:56:56.640]   with more totally unnecessary violence.
[00:56:56.640 --> 00:57:04.000]   I think the solution to that is small communities. Communities of
[00:57:04.000 --> 00:57:14.960]   affinity, there has to be communication between the communities. I think that's one of the reasons
[00:57:14.960 --> 00:57:19.920]   I think the Fediverse works, it's Mastodon works compared to Twitter. Those are individual
[00:57:20.480 --> 00:57:26.800]   communities that enforce their own by agreement rules that you participate with or not,
[00:57:26.800 --> 00:57:30.080]   depending on whether you agree with those rules. That seems to work quite well.
[00:57:30.080 --> 00:57:34.400]   Yeah, absolutely. They're federated models. I just want to say one other quick thing,
[00:57:34.400 --> 00:57:39.600]   because I think it's worth noting. I know all of this sounds really speculative.
[00:57:39.600 --> 00:57:45.520]   Like I'm thinking like worst possible cases here. For those of you who are not familiar with what
[00:57:45.520 --> 00:57:51.280]   I what people in my position do. So technically the term is futurist, but what we really do is
[00:57:51.280 --> 00:57:59.360]   strategic foresight. We use data sets and build different types of models to look for long-term
[00:57:59.360 --> 00:58:04.720]   indications of change. So my academic background is game theory and economics. This was before
[00:58:04.720 --> 00:58:12.160]   the journalism stuff. So this is not just a crazy lady on a Sunday afternoon. But I think
[00:58:12.160 --> 00:58:19.760]   that's important because again, there is data and evidence rooted in what I've been
[00:58:19.760 --> 00:58:24.560]   stating, what I've been talking about. A federated model works really well, but that
[00:58:24.560 --> 00:58:30.800]   requires boundaries. And maybe some type of DAO is a potential solution here. DAO is a government
[00:58:30.800 --> 00:58:34.800]   governance model versus an investment model. Could be actually kind of awesome.
[00:58:34.800 --> 00:58:40.160]   Smart contracts making a lot of this totally transparent. Maybe that's another,
[00:58:40.960 --> 00:58:46.000]   that's another, there is emerging technology that we could use to solve our current technology
[00:58:46.000 --> 00:58:51.840]   problems, which would be kind of awesome. Yeah, good. And by the way, nobody.
[00:58:51.840 --> 00:58:58.720]   If I read the chatroom to you, nobody second guesses your credentials or your.
[00:58:58.720 --> 00:59:04.720]   Well, I'm on the chatroom and that's why I'm like somebody totally within your right to say this,
[00:59:04.720 --> 00:59:07.120]   by the way, was like, I don't know what she talked about. Oh, there's always going to be
[00:59:07.120 --> 00:59:11.520]   a little there. But I think if you look in general, did somebody disagrees with someone in the
[00:59:11.520 --> 00:59:19.280]   room? In general, it is widely recognized that your contributions are always thought
[00:59:19.280 --> 00:59:26.320]   provoking and a real interest. And I'm always trying to pin you down and I can't because I
[00:59:26.320 --> 00:59:32.800]   don't know if you're a wide eyed optimist, a complete dystopian pessimist. I'm a pragmatist.
[00:59:32.800 --> 00:59:37.760]   I'm a pragmatist. I am. I am. And I'm not a techno utopian. I just,
[00:59:37.760 --> 00:59:43.200]   my job is to model plausible outcomes. That's it. That's all I do. It's very boring most of the time.
[00:59:43.200 --> 00:59:51.040]   I think it's appropriate to imagine. In fact, it's important to imagine a goal state where if,
[00:59:51.040 --> 00:59:54.640]   you know, we could forget all the. Yeah, totally. Just to think what would we like,
[00:59:54.640 --> 00:59:59.840]   I think is a really good exercise without worrying about how hard it's going to be to get there.
[00:59:59.840 --> 01:00:04.000]   What would we like? That's right. So, yeah. That's right. And that's a good exercise for everybody.
[01:00:04.000 --> 01:00:11.520]   Go into the future. If 20 years feels too wrong, like try 10. What should people like in an ideal
[01:00:11.520 --> 01:00:16.560]   world, what type of speech do we have? Because again, we have early talk about.
[01:00:16.560 --> 01:00:23.040]   I will provide one thing. One thing that I would like, one source of truth that we can all agree
[01:00:23.040 --> 01:00:29.600]   that is actually truth as if something actually happened. This has become our major, major problem.
[01:00:29.600 --> 01:00:35.680]   We've seen over the last number of years is that we don't agree that something is black,
[01:00:35.680 --> 01:00:40.480]   is black, and something as white as white. We can't agree on legitimate facts.
[01:00:40.480 --> 01:00:45.680]   So here's how you, in my opinion, scary, how you handle that. It's all in my opinion,
[01:00:45.680 --> 01:00:50.160]   it's all societal. It's all norms. That's why I like a federated model. I like a small community
[01:00:50.160 --> 01:00:55.280]   model because the best way to decide that is not for any individual to say that's facts. That's
[01:00:55.280 --> 01:01:04.160]   not facts. But for community to censure liars. And when the community says, you know what,
[01:01:04.160 --> 01:01:11.760]   that's a lie. That's sky is not green. We all agree about that. And those of you who say the
[01:01:11.760 --> 01:01:18.960]   sky is green, no, you're wrong. But it's no longer that. It's the argument of whether the sky is
[01:01:18.960 --> 01:01:22.320]   green is now become a political issue. No, I understand. It's no longer a factual.
[01:01:22.320 --> 01:01:25.520]   You know what, there may be polarization and there may be groups that then say,
[01:01:25.520 --> 01:01:28.320]   well, we believe the sky is green. We're going to go off and do our own thing.
[01:01:28.320 --> 01:01:35.120]   That's fine. That's fine. I don't think there's anything wrong with that. I think in every case
[01:01:35.120 --> 01:01:39.920]   you get in trouble when you become authoritarian and say, no, no, we're going to enforce the fact
[01:01:39.920 --> 01:01:44.960]   that this is a fact because David, as you well know, one man's fact is another man's fiction.
[01:01:44.960 --> 01:01:50.720]   It's impossible to say for many things, not everything, but for many things,
[01:01:50.720 --> 01:01:55.040]   it's impossible to say what if there are things that we all watch on video that people argue
[01:01:55.040 --> 01:02:00.320]   didn't actually happen. No, I know. And that is the sad reality. I know. And what are you going to do
[01:02:00.320 --> 01:02:04.720]   about that? Are you going to bash somebody in the head because they disagree? What are you going to do?
[01:02:04.720 --> 01:02:10.240]   Here's no, here's the let them have that opinion. Stop giving airtime to the people who point out
[01:02:10.240 --> 01:02:14.960]   and that's what I'm saying. The problem is not on vacation. And so that's the real problem.
[01:02:14.960 --> 01:02:19.520]   It's the amplification. That's always the problem. And algorithms amplify, media amplifies.
[01:02:19.520 --> 01:02:23.840]   And but I think we're going in a in a better direction, aren't we? But I think it's much more
[01:02:23.840 --> 01:02:29.680]   democratic. There are many more voices. Yes, right now it sounds chaotic. There's a lot of people
[01:02:29.680 --> 01:02:34.480]   talking at you. We're going to have to learn ways of dealing with that, learn ways to judge
[01:02:34.480 --> 01:02:39.360]   whether something's true or not. That's part of the strife that's going on right now. But in the
[01:02:39.360 --> 01:02:44.400]   long run, we're moving in the positive direction. But I think I've always said algorithms are a big
[01:02:44.400 --> 01:02:51.200]   problem. And I don't think you come up with a better algorithm, a fact fiction hot dog or not
[01:02:51.200 --> 01:02:56.400]   algorithm. I think you just stop algorithms. But there's also the training just journalism in
[01:02:56.400 --> 01:03:02.480]   general. The training of journalism in general says get both sides of the story. Like day one in
[01:03:02.480 --> 01:03:06.560]   journalism school, it's taught to you. I know and that's wrong too. I agree. Sometimes there's
[01:03:06.560 --> 01:03:10.480]   not a second side of the story. Yeah, I agree. And oftentimes there's five sides to the story.
[01:03:11.920 --> 01:03:18.000]   And this is the problem. And you'll see so many stories in just your local TV news like,
[01:03:18.000 --> 01:03:23.040]   what's the opposite side of the story? There is no opposite side in many cases.
[01:03:23.040 --> 01:03:25.600]   Sometimes it's just a fact reported be done.
[01:03:25.600 --> 01:03:27.520]   Amy, go ahead.
[01:03:27.520 --> 01:03:37.600]   So I was, yeah, so I found journalism school to be really challenging because
[01:03:39.280 --> 01:03:42.800]   there weren't so Leo, you were just talking about the sky is blue, the sky is green.
[01:03:42.800 --> 01:03:48.480]   Like my vision for the future is that we all walk outside and somebody says the sky is blue and
[01:03:48.480 --> 01:03:53.840]   somebody else is like the sky is green. And the sky is green person says like, come at me, bro.
[01:03:53.840 --> 01:04:00.800]   Let's talk. But like, let's why do you think the sky is blue?
[01:04:00.800 --> 01:04:01.920]   Let's talk about it. Yeah.
[01:04:01.920 --> 01:04:07.680]   Exactly. I think the central problem that we face and the reason that we like algorithms
[01:04:08.560 --> 01:04:14.640]   is because people are very uncomfortable with uncertainty. And part of what algorithmic
[01:04:14.640 --> 01:04:21.520]   selection does is it reduces the tyranny of choice. So we have more, we're goaded into thinking
[01:04:21.520 --> 01:04:26.000]   we have more certainty and that the decisions that are being made for us are good ones. And then
[01:04:26.000 --> 01:04:30.640]   our behavior starts to change to match. I think that's what drove a lot of people to discord,
[01:04:30.640 --> 01:04:38.480]   including me. I've spent tons less time on Twitter in the past year than I did five years ago.
[01:04:39.120 --> 01:04:47.440]   I'm in part because of like, just, you know, I'm getting like every day just crypto bots and
[01:04:47.440 --> 01:04:52.320]   crypto bros or just it's me with a whole bunch of hashtags and it's like meaningless, like
[01:04:52.320 --> 01:04:57.200]   horrible, right? Twitter goes through this stage. It doesn't it? Yeah, it does. Well, discord.
[01:04:57.200 --> 01:05:00.240]   So what you've done with this court is exactly what I was just talking about. Would you
[01:05:00.240 --> 01:05:03.080]   form your own community? Yeah. Right. So, you know, I've got a
[01:05:03.080 --> 01:05:07.720]   different community. I had a key base community. It's a shame because Twitter, what Twitter is
[01:05:07.720 --> 01:05:12.600]   the potential for Twitter is to bring people from different communities together. Just to discuss.
[01:05:12.600 --> 01:05:19.800]   It could be a great discussion ground, right? It could be. But you and I have both been harassed
[01:05:19.800 --> 01:05:25.240]   pretty. Yeah, it's nasty. prolifically. It's, you know, whatever, like, I, it's more inconvenient.
[01:05:25.240 --> 01:05:28.840]   I don't actually care. People have said some pretty horrible things. And I'm very,
[01:05:28.840 --> 01:05:35.160]   of course. Yeah. I'm a very confident person. So I'm quite fine. But it just makes the experience
[01:05:35.160 --> 01:05:41.400]   less. And every time I've reported to Twitter, the harassment that meets their own criteria,
[01:05:41.400 --> 01:05:45.240]   it's not dealt with. And even if it is dealt with, there's it's like a hydra, right? You got,
[01:05:45.240 --> 01:05:49.240]   like five new. Well, and that's what's disturbing is that all of the things
[01:05:49.240 --> 01:05:52.520]   Elon has said of show an ignorance of what the issues are.
[01:05:52.520 --> 01:05:57.880]   Twitter is, I think done a lot. Believe it or not, they've gotten better. I don't know if you've
[01:05:57.880 --> 01:06:04.360]   noticed, but it's it has gotten a little bit better. I think I do think we need a place.
[01:06:04.360 --> 01:06:10.440]   If we all go in a communities where the communities can come converse. I mean, I really like
[01:06:10.440 --> 01:06:15.640]   Mastodon for that reason. Yeah. There needs to be some porosity to the bubble that you're in.
[01:06:15.640 --> 01:06:22.440]   Otherwise, you all believe the sky is green. And so it's important that there be communication.
[01:06:23.960 --> 01:06:27.240]   Are you are you using Mastodon or David? Are you using Mastodon?
[01:06:27.240 --> 01:06:30.760]   I am not. No, I wonder.
[01:06:30.760 --> 01:06:33.880]   This is the other. Yeah. We are.
[01:06:33.880 --> 01:06:38.040]   We are on the other thing is instant. By the way, Twitter that social. So yeah.
[01:06:38.040 --> 01:06:44.520]   Yeah. What is aggravating me is we should never have a conversation about whether the sky is
[01:06:44.520 --> 01:06:50.840]   blue or green, like some certain facts just are facts. And we, but what's happening is we're being,
[01:06:50.840 --> 01:06:57.480]   we are wasting time on these conversations and these issues that should have never ever come
[01:06:57.480 --> 01:07:03.160]   up in the first place. And it's often to divert our attention from something that's that is.
[01:07:03.160 --> 01:07:08.600]   There is a lot of nasty manipulation going on. Yeah.
[01:07:08.600 --> 01:07:12.840]   Or that's your fear. Of course, if Elon gets Twitter is he would use it to manipulate the
[01:07:12.840 --> 01:07:18.360]   electorate. And so there is there are people willing to manipulate. And the solution to them
[01:07:18.360 --> 01:07:21.800]   is to solve this amplification algorithmic amplification issue.
[01:07:21.800 --> 01:07:30.120]   And I'm not sure what the answer to that is, but you're right. And maybe the other issue is for us
[01:07:30.120 --> 01:07:34.840]   to become more aware of manipulation, to raise our consciousness, to understand we're being
[01:07:34.840 --> 01:07:40.520]   manipulated. And maybe there's a lot of people probably is probably the majority of people who
[01:07:40.520 --> 01:07:47.240]   don't care. They like being. But we fall, but we fall for it all. Yeah. But just look at what
[01:07:47.240 --> 01:07:51.320]   remembering David, maybe you're not you're not you're kind of the three of us, I think, are natural
[01:07:51.320 --> 01:07:56.600]   skeptics. We're always even debating with ourselves about what we believe. But there are,
[01:07:56.600 --> 01:08:01.080]   I would say the vast majority of people just want to be told what to believe and love that
[01:08:01.080 --> 01:08:10.040]   warm bath of no doubt, right, of pure faith. And if if a charismatic leader comes along and tells
[01:08:10.040 --> 01:08:16.360]   them a fairy tale that that makes them feel good, they love the idea that they can believe.
[01:08:16.360 --> 01:08:23.800]   They can yes, but then they can turn off that very hard thing, which is thinking.
[01:08:23.800 --> 01:08:28.760]   And I unfortunately think that that's a natural human tendency. So that maybe.
[01:08:28.760 --> 01:08:33.960]   Here, let me ask. This is not on our show notes for tonight, but I got a good example.
[01:08:33.960 --> 01:08:39.480]   Why is everybody so fascinated with the Johnny Depp and Amber Heard trial? Because it is showing
[01:08:39.480 --> 01:08:44.200]   up that is the thing that shows up on Twitter, right? It's it's it's being live streamed. It's
[01:08:44.200 --> 01:08:47.000]   like, well, I could tell you why. Do you want to know why?
[01:08:47.000 --> 01:08:53.400]   Well, to me, this is an example that illustrates. Yeah, I mean, it's kind of what you just said.
[01:08:53.400 --> 01:08:58.760]   Right. So that's a biological imperative that we talk about.
[01:08:58.760 --> 01:09:04.440]   These things, emotions at a distance for feelings. It's why we have celebrity.
[01:09:04.440 --> 01:09:08.200]   We love that. And I think it's just biological, but we know we love.
[01:09:08.200 --> 01:09:11.240]   That is celebrities getting knocked down. Yeah, yeah, we love all that.
[01:09:11.240 --> 01:09:15.320]   Yeah, we love the guys. Yeah, someone at a high power falling.
[01:09:15.320 --> 01:09:18.360]   And just play to our tendencies.
[01:09:18.360 --> 01:09:24.680]   Right. So this is a challenge with artificial general intelligence and with algorithms in
[01:09:24.680 --> 01:09:27.880]   general. They're going to be as stupid as we are. Is that what you're saying?
[01:09:27.880 --> 01:09:36.200]   No, it's just it's really hard to parse the the the the things that draw us in. And I know there's
[01:09:36.200 --> 01:09:41.000]   like plenty of companies that have promised that they can like figure out intent and predictability
[01:09:41.000 --> 01:09:46.360]   and all these other things. But on a very base level, I mean, yes, we like we were drawn into
[01:09:46.360 --> 01:09:51.880]   celebrity gossip. But why why that trial and not some other trial? Because arguably, like,
[01:09:51.880 --> 01:09:58.040]   Johnny Depp's a little past his prime, you know, and like, really, you're still big celebrities.
[01:09:58.040 --> 01:10:03.240]   Right. So this is the challenge, you know, there. But this is the part that I think is really
[01:10:03.240 --> 01:10:10.200]   a family exist. Well, that I don't know. But from a if we were to try to like,
[01:10:10.200 --> 01:10:14.040]   who polices the algorithms, I think is a question, the conversation that we're having.
[01:10:14.040 --> 01:10:21.240]   Yeah. The problem is that to get to the end of that, we would want to have some kill switch so that
[01:10:21.240 --> 01:10:27.720]   we don't all get sucked into to something that is probably not great. But defining that would be
[01:10:27.720 --> 01:10:31.720]   really challenging. I mean, this is one of the core problems with with a Jan. So you wind up with
[01:10:31.720 --> 01:10:38.360]   the reverse of that, which is a system like GPT three, which makes a lot of assumptions based on
[01:10:38.360 --> 01:10:42.760]   like problematic data sets and a handful of people who are making the decisions during coding. So
[01:10:42.760 --> 01:10:47.160]   again, if I like play this forward, this is an interesting another place for a Dow.
[01:10:47.160 --> 01:10:53.240]   Again, not tied to finances, but what should those decisions be? What should the like,
[01:10:53.240 --> 01:11:00.040]   like, what should the data set be? What should some of the params be like, you know, and, you know,
[01:11:00.040 --> 01:11:04.520]   nobody would care, but maybe some people would care. And maybe that's another way to think about
[01:11:04.520 --> 01:11:12.920]   this. I don't know. This question is as old as time and has been asked, for instance,
[01:11:12.920 --> 01:11:23.240]   how is it that an entire nation would fall for Adolf Hitler and Nazism and follow in what was
[01:11:23.240 --> 01:11:29.720]   inevitably going to end in disaster? How did and this is, of course, the the work, Hannah Aaron's
[01:11:29.720 --> 01:11:35.720]   work, the origins of totalitarianism that tries to answer that question.
[01:11:35.720 --> 01:11:42.440]   And, you know, I think it, it essentially is, I mean, look, I probably should be the
[01:11:42.440 --> 01:11:46.760]   right. Have you read it? Yeah, but I'm the wrong person to synopsis it. It's kind of, it's kind of,
[01:11:46.760 --> 01:11:52.840]   it's kind of trivializing in a way, but she says it's loneliness.
[01:11:52.840 --> 01:11:58.760]   I think that I think there's something there for sure. She says loneliness is a precondition for
[01:11:58.760 --> 01:12:05.000]   totalitarian domination, that if you're socially isolated, you're more likely to be attracted to
[01:12:05.000 --> 01:12:12.600]   totalitarian ideology. And I'm not sure I ever, I'm not sure I agreed with it when I read it.
[01:12:12.600 --> 01:12:21.560]   It bugs me still, but you see evidence of it time and time again. And, but there's something in our
[01:12:21.560 --> 01:12:28.440]   nature that it draws us to a strong man and draws us to somebody who will tell us
[01:12:28.440 --> 01:12:33.800]   that mythology. It's certainty. We don't like thinking. We don't like uncertainty. We much
[01:12:33.800 --> 01:12:39.320]   prefer it if we're told what to think. There's another book called Cultish, which is somewhere in
[01:12:39.320 --> 01:12:44.120]   my office. I read that book. I enjoyed that. What do you think? Did you, did you like it?
[01:12:44.120 --> 01:12:53.880]   It's about the language is what's driving us to believe in cults. And the cults go from
[01:12:53.880 --> 01:13:00.760]   the branch to vidians all the way down to your peloton and structure. Crossfit.
[01:13:00.760 --> 01:13:04.280]   Yeah, and crossfit. The language. Yeah.
[01:13:04.280 --> 01:13:09.640]   And that they use the same language across all its quite fascinating.
[01:13:09.640 --> 01:13:13.240]   He talks about soul cycle, compares soul cycle to science. Soul cycle, excuse me.
[01:13:13.240 --> 01:13:18.360]   Yeah, soul cycle is definitely cult. Everybody knows that. But, but, you know,
[01:13:18.360 --> 01:13:24.760]   I think these are different facets of the same elephant that, I mean, language and Joseph Gebel's
[01:13:24.760 --> 01:13:33.240]   knew this, Hitler knew this. Language is a tool, a very powerful tool. And the language you choose
[01:13:33.240 --> 01:13:38.120]   is part of this. I don't think it's all of it. I don't think it's neuro linguistic programming.
[01:13:38.120 --> 01:13:42.440]   I don't think, you know, if I say the magic words, you're going to fall into bed with me. But
[01:13:42.440 --> 01:13:46.760]   it's certainly a tool in this, in this manipulation.
[01:13:46.760 --> 01:13:52.440]   The reason it works though, is we're so willing to go for it. We want to believe.
[01:13:52.440 --> 01:13:57.320]   Maybe run up something about, about challenging ourselves, which I loved. And,
[01:13:57.320 --> 01:14:02.120]   and here's something like often when you hear somebody speak, I'm going to read this book,
[01:14:02.120 --> 01:14:05.640]   by the way, thank you. This is great. Yeah, it's good. The phrase people say is,
[01:14:05.640 --> 01:14:09.640]   oh, I like what so and so said. I like what Amy said. I like what Leo said.
[01:14:09.640 --> 01:14:15.480]   But, and this is what we should be saying to ourselves, I'd like what Amy said because it
[01:14:15.480 --> 01:14:21.400]   challenged my thing. There you go. And people don't say that and they should. Never. Yeah.
[01:14:21.400 --> 01:14:27.800]   Well, here, again, if I just bring the book and the conversation back to sort of tech and the
[01:14:27.800 --> 01:14:33.000]   issue at hand, which is like, should New York be creating draconian social media laws?
[01:14:34.200 --> 01:14:38.120]   Well, you know, by the way, this was the story that we began with. And you know, nothing will
[01:14:38.120 --> 01:14:45.320]   come of that. Right. That's just them. No, I'm so mad at discord. I mean, I admit Fort
[01:14:45.320 --> 01:14:49.880]   Chan, which by the way, Fort Chan is exactly what Elon Musk wants to turn Twitter into.
[01:14:49.880 --> 01:14:56.520]   Fort Chan was very much part of this kid's radicalization. But he found like minds there,
[01:14:56.520 --> 01:15:01.080]   and he found the information he wanted, and it reinforced his own beliefs. So,
[01:15:01.080 --> 01:15:07.720]   right. So again, this is all structured data. This is the problem. So when you have radical ideas
[01:15:07.720 --> 01:15:13.080]   starting to take over, they tend to share a common linguistic. Yeah, which is partially what this
[01:15:13.080 --> 01:15:16.840]   book is about. That's fascinating. Yeah, that makes sense. Right. But the problem is that that's
[01:15:16.840 --> 01:15:22.200]   like straight up structured data. And if everybody's using the same words and phrases, then,
[01:15:22.200 --> 01:15:28.120]   you know, you have two options there. It becomes much easier to identify and remove
[01:15:28.120 --> 01:15:34.600]   or mitigate on any you can train any system to recognize those words. And they will evolve,
[01:15:34.600 --> 01:15:41.720]   because people will come up with additional code words then to right. But but it gives you
[01:15:41.720 --> 01:15:47.000]   optionality. If you're if you're looking at it as structured data, you can either deal with it
[01:15:47.000 --> 01:15:52.760]   and remove it. Or you're making a choice not to. So you know what I've got? Did I ever show you this?
[01:15:53.720 --> 01:15:58.680]   I have. Oh, actually, hold on. Go get it. And we're going to talk about it when I got to take a
[01:15:58.680 --> 01:16:03.080]   break. So hold on just a second. But when we come back, Amy will show us something.
[01:16:03.080 --> 01:16:11.160]   I don't know if I should. Yes, you should. Hold on. Our show today brought to you by Express VPN.
[01:16:11.160 --> 01:16:18.120]   Watching Netflix without using Express VPN is like buying tickets to a Taylor Swift concert,
[01:16:18.120 --> 01:16:23.560]   but only being allowed to watch the opening act. Oh, man, how disappointing is that?
[01:16:23.560 --> 01:16:34.760]   Well, let me explain. Express VPN unlocks the content you want. It lets you like like any good
[01:16:34.760 --> 01:16:40.440]   VPN change your online location. But there's a real reason you'd want to use Express VPN to do
[01:16:40.440 --> 01:16:45.720]   this. First of all, actually, there are a number of reasons. There's security. There's privacy.
[01:16:45.720 --> 01:16:50.440]   There's a limiting geographic restrictions. I do all three. Everyone to watch Dr. Who on Netflix.
[01:16:50.440 --> 01:16:55.800]   You can go to Netflix UK if you're already an Netflix subscriber in the US. And now you're in
[01:16:55.800 --> 01:17:01.640]   England. They invest in their infrastructure. So you can watch high def video because it's fast.
[01:17:01.640 --> 01:17:07.880]   One of the ways VPNs get blocked is because an address and IP address becomes associated with
[01:17:07.880 --> 01:17:12.680]   a VPN. And then the BBC or somebody else says, well, we're not going to allow that and blocks
[01:17:12.680 --> 01:17:20.280]   that IP address. Express VPN rotates their address. They do a really good job of not only hiding you,
[01:17:20.280 --> 01:17:25.640]   but hiding themselves. So with Express VPN, this is one of many reasons to use it. You can just open
[01:17:25.640 --> 01:17:32.360]   their app, select a country, go to the country, run Netflix. You're already logged in. Suddenly,
[01:17:32.360 --> 01:17:38.920]   you'll be seeing Netflix UK or Netflix, Japan. Beautiful, beautiful HD video without buffering
[01:17:38.920 --> 01:17:44.840]   that to mention the security factor of using a VPN. When you go, you know, on a Google search,
[01:17:44.840 --> 01:17:49.080]   even if you're in incognito mode, Google knows your IP address. They say, Oh, hi, Leo,
[01:17:49.080 --> 01:17:56.680]   not with Express VPN. It's not Leo. It's some other IP address. Express VPN works with everything
[01:17:56.680 --> 01:18:04.280]   you use phones or laptops, your your your consoles, your smart TVs. You can even put Express VPN on
[01:18:04.280 --> 01:18:09.880]   your router, protect the whole house. And because they've got servers all over the world, 94 different
[01:18:09.880 --> 01:18:17.240]   countries, man, the world is your oyster. Yes, it works with Netflix. It works with BBC's iPlayer,
[01:18:17.240 --> 01:18:24.280]   works with YouTube. It's really a remarkable VPN. Here's the final, final issue with the VPN.
[01:18:24.280 --> 01:18:29.640]   Sure, you're protecting your privacy against the other guys, but the VPN knows everything you're
[01:18:29.640 --> 01:18:35.080]   doing. You want a VPN service that protects your privacy the same way you protect it. I'll never
[01:18:35.080 --> 01:18:41.560]   forget the time that I fire up iTunes and my son is list of everybody's music in the hotel.
[01:18:41.560 --> 01:18:46.440]   I could listen to their music. That's when it hit me. You know, I probably should be using Express
[01:18:46.440 --> 01:18:53.800]   VPN. There was a great article on bleeping computer about Express VPN and how they do it. I was so
[01:18:53.800 --> 01:19:06.760]   impressed. They run a custom version of Debian. And it gets wiped completely every day. So there's
[01:19:06.760 --> 01:19:13.480]   nothing on the hard drive. Furthermore, with their trusted server technology, they are launching a
[01:19:13.480 --> 01:19:19.880]   RAM only server that can't that sandbox can't write to the hard drive. So it can't write to the
[01:19:19.880 --> 01:19:28.760]   hard drive. It's wiped regularly on reboot. It is a verification system that prevents insider
[01:19:28.760 --> 01:19:34.600]   code tampering. It's patched every week with clean installs on every Express VPN server.
[01:19:34.600 --> 01:19:40.280]   So when I read this on bleeping computer, I thought, you know what, this confirms what I always
[01:19:40.280 --> 01:19:45.640]   thought. Express VPN, it works better. They invest in infrastructure. They protect your privacy.
[01:19:45.640 --> 01:19:51.560]   And all of this for less than seven bucks a month when you go to express VPN.com/twit.
[01:19:51.560 --> 01:19:56.840]   Be smart. Stop paying full price for streaming services and only getting access to a fraction
[01:19:56.840 --> 01:20:03.400]   of their content. Get your money's worth Express VPN.com/twit and sign up for your package. You'll get
[01:20:03.400 --> 01:20:08.280]   three months free. Big discount, less than seven bucks a month to protect your privacy,
[01:20:08.280 --> 01:20:15.400]   to put yourself anywhere in the world. It's amazing. Worth every penny. It's the only VPN I use,
[01:20:15.400 --> 01:20:21.400]   the only one I trust Express VPN.com/twit. We thank them so much for their support of
[01:20:21.400 --> 01:20:29.080]   Twit. They've done a really good job of making this service. Now, Amy Webb is going to show us
[01:20:29.080 --> 01:20:39.400]   something. Ta-da. Is it a book? I should. So I collect, I like to collect rare books,
[01:20:39.400 --> 01:20:45.640]   hard to find books, books on weird topics, and obviously like any type of book that was predicting
[01:20:45.640 --> 01:20:51.320]   the future, you know, like foretelling the future. I also really like dictionaries. I got my hands.
[01:20:51.320 --> 01:20:58.360]   I love dictionaries. I'm not sure I'm supposed to have. This is the technical dictionary for
[01:20:58.360 --> 01:21:04.280]   Scientology. They're going to be coming to your door, knock knock knock. They might be. And I'll
[01:21:04.280 --> 01:21:09.640]   know what they're talking about because I read a lot of this. So Scientology, which is definitely
[01:21:09.640 --> 01:21:19.880]   a cult, has a lot of secret lore that you have to pay to get to. And I imagine that book is one of
[01:21:19.880 --> 01:21:27.080]   those. Yeah. There's a lot of, so again, like language is, language is so interesting because
[01:21:27.080 --> 01:21:32.040]   it's a way of creating in and out groups. It's a way to manipulate, create exercise power.
[01:21:32.760 --> 01:21:36.440]   And which again, why when we think of that, like a zenu, is zenu in there?
[01:21:36.440 --> 01:21:38.840]   Let me take a look. XENU.
[01:21:38.840 --> 01:21:48.440]   No, man. We could get so much trouble. I had a girlfriend who was an ex-scientologist. It is
[01:21:48.440 --> 01:21:55.240]   the hardest thing ever to get out of. No, there's four entries. X one, X two, X unit and X.
[01:21:55.240 --> 01:21:59.000]   X one is a code number of a process. X unit.
[01:22:00.600 --> 01:22:04.680]   Yeah, this is all about that. Maybe look under L for Lord, XENU.
[01:22:04.680 --> 01:22:13.320]   She the things they did to her to try to keep her in Scientology were mind-boggling,
[01:22:13.320 --> 01:22:23.960]   including putting a bullet in her mailbox of 45. No, no, Lord. So you do not have, I'm sorry to say,
[01:22:23.960 --> 01:22:31.640]   you have not paid sufficiently to get the final materials that was released. Somebody released
[01:22:31.640 --> 01:22:36.120]   the stuff in the safe that you have to get clear to get somebody released.
[01:22:36.120 --> 01:22:43.720]   Yeah. It's crazy. This was first printed in 1975. But again, it's just there's thousands and
[01:22:43.720 --> 01:22:49.080]   thousands of words in here. And if you're, if you want to continue to rise up, you have to know
[01:22:49.080 --> 01:22:55.240]   what all of this means. So again, think about this in terms of tech and structured data and
[01:22:55.240 --> 01:23:01.400]   taxonomies and the nomenclature and like the words that are getting used to train systems to learn
[01:23:01.400 --> 01:23:06.840]   and understand and talk. It's like, it starts to be like totally mind-bending. If you think about
[01:23:06.840 --> 01:23:12.600]   the power that we are giving to decisioning systems based on a handful of people's like language,
[01:23:12.600 --> 01:23:19.000]   like words selection, you know? I think the good news is Leo, wait a second, Leo, I think she just
[01:23:19.000 --> 01:23:25.320]   got this book so she could cheat on the test. You know, no, no, no looking into the soul of the
[01:23:25.320 --> 01:23:33.720]   person next to you, as Woody. Yes, I'm glad you have that, Amy. And I hope that you survived the night.
[01:23:33.720 --> 01:23:41.320]   So that's all I can say. Do you have other artifacts from religions worldwide?
[01:23:43.480 --> 01:23:50.200]   Do I'm very interested in religion? Again, like it bugs me a little bit that some people I really
[01:23:50.200 --> 01:23:57.880]   admire in science have become hardliners on religion or lack, you know, like there is no God,
[01:23:57.880 --> 01:24:05.800]   right? I think that just, I don't know, maybe I don't have sufficient data to make that decision.
[01:24:05.800 --> 01:24:12.280]   Well, no, when we're dead or not. So again, like I don't want to go through life. It took me a
[01:24:12.280 --> 01:24:16.520]   while to get to this point. But I want to be open to possibilities. I want to learn new things.
[01:24:16.520 --> 01:24:23.560]   Oh, I agree. I'm 100% there. Yeah. You know, I don't judge. I don't judge anybody for anything
[01:24:23.560 --> 01:24:30.520]   that they believe, even if they thought the sky was green. But hold away, if they thought the sky
[01:24:30.520 --> 01:24:34.920]   was green, how much would they you believe anything else they said? Well, it might be green to them.
[01:24:34.920 --> 01:24:39.240]   Green's just a construct. Well, somebody in the chat from the stuff. Well, if they're colorblind,
[01:24:39.240 --> 01:24:43.960]   that's a whole different issue. Well, that's the point we don't know. There you go. Yeah.
[01:24:43.960 --> 01:24:47.960]   Just in so I lived in Japan for a long time. And as I think a lot of people who listen to the
[01:24:47.960 --> 01:24:53.400]   show now, I speak Japanese fluently and somebody brought this up in the chat. But owl is the name.
[01:24:53.400 --> 01:25:00.360]   So so there is no the words for blue and green kind of get interchanged a lot in Japan. Like a
[01:25:00.360 --> 01:25:06.520]   stoplight is owl, which technically means blue, but it's definitely green. Again, like it's a
[01:25:06.520 --> 01:25:12.920]   constant we were a little, yeah, our creativity, our thinking is a little hamstrung by the absolute
[01:25:12.920 --> 01:25:17.640]  est language that we use. And I think that's again, we're training machines to do the same.
[01:25:17.640 --> 01:25:24.120]   But see, this is why I do think you're an optimist, because I am maybe I used to be an optimist. And
[01:25:24.120 --> 01:25:31.160]   now as a reformed optimist, I'm very pessimistic. And, you know, there's nothing worse than a
[01:25:31.160 --> 01:25:36.840]   lapsed Catholic or a reformed optimist. We're the worst. But the I kind of some thinking,
[01:25:36.840 --> 01:25:43.960]   it's hopeless. And we are so mired. We don't we do not know what we don't know to the nth degree.
[01:25:43.960 --> 01:25:51.400]   And we're so mired in our, you know, brain patterns that it's just it's just hopeless. And
[01:25:51.400 --> 01:25:58.280]   the good news is GPT three notwithstanding, we're never going to have self driving cars,
[01:25:58.280 --> 01:26:03.080]   or general artificial general artificial intelligence. That's impossible.
[01:26:03.080 --> 01:26:11.320]   Oh, I think we've already got AGI. Where's? Well, Ian Goodfellow just left and went to the place
[01:26:11.320 --> 01:26:15.800]   where it's being invented. The place that's actually another one of our stories. He was the
[01:26:15.800 --> 01:26:22.600]   Apple guy who'd come from Google to go to Apple. Yeah, to who is the king of GP three three or
[01:26:22.600 --> 01:26:28.440]   GAN, right? He's a GAN. So right? He was the first person to really generate his adversarial
[01:26:28.440 --> 01:26:36.120]   network network, which is a very deep form of machine learning. He has some of his
[01:26:36.120 --> 01:26:42.040]   papers, early papers are fascinating to read now. And he was unhappy with the requirement to come
[01:26:42.040 --> 01:26:48.520]   back to work. He liked working from home. Who knew? And said, for that and other reasons, I think
[01:26:48.520 --> 01:26:56.520]   decided to leave Apple and then go back to Google. There was a great quote. This is this is from
[01:26:56.520 --> 01:27:05.560]   TNW, Tristan Green. He said to put this into a sports analogy, what Apple's done is like letting
[01:27:05.560 --> 01:27:10.520]   Tom Brady or Michael Jordan leave your team over a disagreement between them and the team owner on
[01:27:10.520 --> 01:27:15.480]   how towels should be folded. That kind of happened at Jordan though, didn't it? Yeah,
[01:27:15.480 --> 01:27:21.480]   let good follow up. To be honest, it's happens. Let good fellow and his team work from wherever
[01:27:21.480 --> 01:27:24.760]   they want. If they think they can code a better machine learning model from the International
[01:27:24.760 --> 01:27:33.640]   Space Station, you should probably look into building a rocket. So you think that AGI
[01:27:33.640 --> 01:27:40.520]   is where? Absolutely. It's here. This is the problem. We first of all, the
[01:27:40.520 --> 01:27:44.840]   how do you define it? We know the Turing test is bogus.
[01:27:44.840 --> 01:27:51.320]   A lot of the benchmarks have typically to do with language. So there's something called the
[01:27:51.320 --> 01:27:57.720]   glue GLUE benchmark, which is an acronym that stands for stuff. That got surpassed. They're super
[01:27:57.720 --> 01:28:03.080]   glue. That's been surpassed. There was the Artari benchmark. We can't even define it.
[01:28:04.680 --> 01:28:11.480]   So this is the problem. We talk about systems being generally capable of performing multiple
[01:28:11.480 --> 01:28:18.360]   tasks at once at or above the level of humans. I would argue that we are already starting like
[01:28:18.360 --> 01:28:25.240]   alpha fold uses language, uses decision making, uses recognition, making decisions.
[01:28:25.240 --> 01:28:29.160]   We already have systems that I think evidence
[01:28:31.160 --> 01:28:37.400]   sort of the outer edges. We have edge cases now. But they're very specific domains.
[01:28:37.400 --> 01:28:44.040]   So I always think of AGI as more like computer. Well, no, I'm just not in any. I mean, you could
[01:28:44.040 --> 01:28:51.000]   say yeah, indistinguishable from human. Steve Wozniak said if a machine could come into your house
[01:28:51.000 --> 01:28:57.160]   and make a cup of coffee on the machine. Machine is in my house. I've got a machine that makes
[01:28:57.880 --> 01:29:02.680]   automate most of the stuff. And I do very little. I've got machines that make me coffee. I've got
[01:29:02.680 --> 01:29:08.280]   machines that are connecting me to you from half of them on the East Coast. Yeah, it's pretty amazing
[01:29:08.280 --> 01:29:13.320]   in a way. But these are all very narrow. That's why I say general intelligence because these are
[01:29:13.320 --> 01:29:17.480]   all like, well, yeah, of course, machines can do a lot of things. But honestly, that's algorithmic,
[01:29:17.480 --> 01:29:22.760]   in my opinion. The problem with the current benchmarks is that you're right. They are
[01:29:22.760 --> 01:29:29.640]   specific. They are mostly tied to specific types of language. But you have to have the benchmarks
[01:29:29.640 --> 01:29:33.480]   because that's what galvanizes activity in the research community because people want to beat
[01:29:33.480 --> 01:29:38.040]   them. I think we have to just define. So in the last book that I wrote, The Big Nine, I actually
[01:29:38.040 --> 01:29:43.320]   came up with a different my version of what a better benchmark would be. It's called the, I think
[01:29:43.320 --> 01:29:48.280]   I called it the meaningful contribution test, which is where you would have an AI system like the A
[01:29:48.280 --> 01:29:54.440]   word, for example, disembodied voice sitting in on a meeting with you. And it would sort of
[01:29:54.440 --> 01:30:00.120]   automatically intervene or be a part of conflict conversations. It would automatically-
[01:30:00.120 --> 01:30:03.800]   Yeah, I think that's super hard. Yeah. Much harder than we think. I think that's-
[01:30:03.800 --> 01:30:10.120]   Yeah. I feel like the first 80% is easy. It's the last 1% that's impossible. And I think that's-
[01:30:10.120 --> 01:30:13.880]   That's everything. Yeah, it's everything. And I think that that's really, we've underestimated
[01:30:13.880 --> 01:30:25.240]   that last 1% all this time. You know, right now the EU is proposing that every social network
[01:30:25.240 --> 01:30:31.560]   should be responsible for the content of that network, not only child sexual abuse material,
[01:30:31.560 --> 01:30:35.640]   CSAM, that's what Apple was proposing with a CSAM scanner for the iPhone,
[01:30:35.640 --> 01:30:41.320]   but of grooming behavior. So this raises a number of things. First of all, that means no end to
[01:30:41.320 --> 01:30:45.720]   an encryption because if WhatsApp is responsible for all the conversations on WhatsApp,
[01:30:45.720 --> 01:30:49.960]   they can't encrypt them, they've got to be able to see them. But then there's even a more
[01:30:49.960 --> 01:30:55.080]   difficult- What is grooming behavior? How do you know? Do you know-
[01:30:55.080 --> 01:30:58.440]   Can you identify it accurately enough to send somebody to jail?
[01:30:58.440 --> 01:31:03.240]   I didn't even know what that word was. I'd never heard that. Do you know now?
[01:31:03.240 --> 01:31:07.800]   I do, unfortunately. Yeah. No, it's what I've been grooming you for months.
[01:31:10.200 --> 01:31:18.520]   Five. They're coming. But that's the thing. If you say, if I said to David Spark ASL,
[01:31:18.520 --> 01:31:23.640]   Age Sex Location, which was the acronym on an AOL aim,
[01:31:23.640 --> 01:31:30.120]   some would say that's grooming behavior, right? It could certainly be the first step in grooming
[01:31:30.120 --> 01:31:36.920]   behavior. But maybe I just want to know, David, better. And how is it? So this is, by the way,
[01:31:36.920 --> 01:31:42.280]   it's not yet the law in the EU, but this is a serious proposal that could very well become law
[01:31:42.280 --> 01:31:47.160]   in the EU. And bizarre. I don't know how you would enforce it.
[01:31:47.160 --> 01:31:51.880]   I think we're going to look back in 100 years. I've been-
[01:31:51.880 --> 01:31:56.040]   Well, you're going to be around in 100 years? I wish. I hope not. I hope not.
[01:31:56.040 --> 01:31:59.000]   No, no, no. No, wouldn't you want to be if you could be poured into-
[01:31:59.000 --> 01:32:03.480]   Of course. Of course. Yes. Of course. Yeah. If I could go far enough in the future,
[01:32:03.480 --> 01:32:09.080]   absolutely. But maybe- I'm hoping- I'm hoping. I'm hoping. I'm hoping between now.
[01:32:09.080 --> 01:32:16.360]   And what is that? 2028, 2038. That somebody will figure out how to put my brain in a jar.
[01:32:16.360 --> 01:32:23.400]   And, you know, I could live for it. Well, right now your brain needs to figure out basic math
[01:32:23.400 --> 01:32:28.680]   because it'd be 2040. Okay, whatever. Here's what I've been thinking a lot about.
[01:32:31.560 --> 01:32:37.400]   I've been thinking a lot that you go 100- 150- 200 years into the future.
[01:32:37.400 --> 01:32:42.360]   Those people are going to look back at us, the generation that's alive right now.
[01:32:42.360 --> 01:32:49.240]   And we're the generation that- we're going to look so primitive, right? We are the ones who
[01:32:49.240 --> 01:32:54.680]   started things. We're like cave people in a good way. James Webb is going to start
[01:32:54.680 --> 01:32:57.160]   porting back images that are literally going to-
[01:32:57.960 --> 01:33:01.720]   They're going to- We are going to have to confront our interpretation of rally.
[01:33:01.720 --> 01:33:09.400]   I also think looking back at us now, we're going to look like barbarians. The weapons that we're
[01:33:09.400 --> 01:33:14.680]   using, the weapons are like legends, right? We look at the way people fought 1000 years ago and
[01:33:14.680 --> 01:33:18.600]   we say now, how could anybody have lived through this? I think they're going to look at our digital
[01:33:18.600 --> 01:33:23.240]   tools. Sure. Of course they are. And they're going to say the same types that's going to be the
[01:33:23.240 --> 01:33:29.960]   same analogy about us. Personally, I would love to be the generation that's like,
[01:33:29.960 --> 01:33:34.280]   hey, we figured out picks and shovels and how to make fire and then we did some good stuff with
[01:33:34.280 --> 01:33:42.280]   those tools versus we were the barbarians. But isn't it always those guys were really-
[01:33:42.280 --> 01:33:50.120]   They didn't take baths. They thought baths were unhelpful. Aren't they always barbarians in the
[01:33:50.120 --> 01:33:54.920]   distant past? Yes, but in a way that I think is- We're going to be worse.
[01:33:54.920 --> 01:34:00.600]   I think they're going to look back at us and say, what the hell were you thinking,
[01:34:00.600 --> 01:34:06.760]   Digit, you got energy coming in from the sun in almost infinite quantity and you thought it was a
[01:34:06.760 --> 01:34:13.000]   good idea to dig up animal matter from under the ground and burn it. What the hell were you thinking?
[01:34:13.000 --> 01:34:17.320]   That's what I think they're going to think. No, of course. But it's- I've been having fun
[01:34:17.320 --> 01:34:23.640]   going pretty far out thinking, what are the things that we are the first, really the first? I mean,
[01:34:23.640 --> 01:34:28.920]   yes, in '69 we like- Humans went to the moon. That's fine. But this is something different.
[01:34:28.920 --> 01:34:34.360]   That's pretty good. That's pretty good. But we're on the cusp of doing totally different types of
[01:34:34.360 --> 01:34:38.920]   building a space economy and becoming a multi-planet species, which is pretty amazing. We're
[01:34:38.920 --> 01:34:43.000]   engineering cells, we're programming cells, we're programming money. Well, that's what this book
[01:34:43.000 --> 01:34:46.440]   is about, right? We're this transition. This is what you're saying in the Genesis machine is,
[01:34:46.440 --> 01:34:52.840]   we're on the cusp, we're on the edge of a- Well, we are the edge. We are that we're the transition
[01:34:52.840 --> 01:34:59.160]   generation. So it makes me excited. It also makes me feel the sense of urgency. We got to all make
[01:34:59.160 --> 01:35:07.080]   good choices here. So that- Anyways. Well, that's right. If you're shooting at Mars,
[01:35:07.080 --> 01:35:15.160]   one degree off at the beginning could be a million miles at the end. We got to aim carefully.
[01:35:15.160 --> 01:35:21.880]   Of course, we're not habitually- Clearly, why do you want to blow up Mars?
[01:35:21.880 --> 01:35:29.080]   What did Mars do to you? Did you watch the- Did you watch the expanse? The Martians are-
[01:35:29.080 --> 01:35:35.000]   They really ended up- On the show. Yeah. Some of them. Yeah. So- No, but in real life, the Martians
[01:35:35.000 --> 01:35:40.680]   are very nice. So that's a really good point. So for instance, somebody like Elon and others
[01:35:40.680 --> 01:35:43.720]   just saying, we're going to- We got to go to Mars. We're going to live on Mars. We're going to
[01:35:44.440 --> 01:35:50.360]   inhabit Mars. Incredibly difficult. Probably not valuable. And at this point-
[01:35:50.360 --> 01:35:56.520]   We place this here on Earth and have oxygen. Yeah. At any time, it would be better to make sure that
[01:35:56.520 --> 01:36:03.240]   this place right here is in good shape. Is that the kind of aiming you're talking about?
[01:36:03.240 --> 01:36:08.520]   We could do both. It doesn't have to be an either or. This is the argument that gets made, right?
[01:36:08.520 --> 01:36:13.400]   Why do we have our sets that on Mars? We should make things better. Again, like, can't we operate
[01:36:13.400 --> 01:36:18.520]   in two cognitive spaces at once? It takes a little bit more energy, but why can't we work on both?
[01:36:18.520 --> 01:36:22.200]   I'm up for that. I don't mind that. I do think sometimes- I don't know why everybody is
[01:36:22.200 --> 01:36:26.040]   sometimes with Mars. We say, well, yeah, I don't- Yeah. But sometimes we use that as a way of
[01:36:26.040 --> 01:36:32.280]   avoiding our responsibilities on Earth. And that is a little risky, right? Yeah.
[01:36:32.280 --> 01:36:36.200]   Would you go? Would you get on a space- Oh, I'd go. Yeah. I'd go on a one-way trip.
[01:36:36.200 --> 01:36:41.000]   No. Would you make a- Would you make the return? Would you make one of the-
[01:36:41.000 --> 01:36:45.000]   You have 18 years left. Wait, wait a good 17 and a half-
[01:36:45.000 --> 01:36:46.280]   In 17 years, I'm on my way.
[01:36:46.280 --> 01:36:53.160]   If money was not an object, there's what I'm asking. Would you hop on a SpaceX capsule or
[01:36:53.160 --> 01:36:56.920]   Blue Origin? They're not really going to space-space. But would you go up?
[01:36:56.920 --> 01:37:01.800]   Yeah. I don't- I want to get my wings. So I want to do- I'm going to bring my gliss-
[01:37:01.800 --> 01:37:04.920]   They're not conferring wings. They're not conferring wings to-
[01:37:04.920 --> 01:37:09.000]   You're not an astronaut astronaut. No, you have to go. So I want to go to the ISS.
[01:37:09.800 --> 01:37:12.440]   I want to do an experiment to see if synthetic whiskey-
[01:37:12.440 --> 01:37:15.960]   I don't know, something- something in space.
[01:37:15.960 --> 01:37:20.760]   And I want to get my wings. Yes, I would- Better get to the ISS fast before it's
[01:37:20.760 --> 01:37:25.960]   decommissioned, since there's no funding for it. No, I would love- I've always wanted-
[01:37:25.960 --> 01:37:33.160]   It's a kid, because I grew up watching Gemini and Mercury and Apollo, and that to me was the-
[01:37:33.160 --> 01:37:40.120]   You know, that was the- the heights of the future technology. I always want to do that.
[01:37:40.120 --> 01:37:46.040]   Yeah, I'd do that in a minute. Let's take a little break. Boy, I- we haven't- we've got- we-
[01:37:46.040 --> 01:37:53.240]   I think we did two stories. That's my fault. I'm sorry. No, there's no rule that we have to do
[01:37:53.240 --> 01:37:57.800]   all the stories. I was a little worried when the show began that we had a light rundown. Apparently
[01:37:57.800 --> 01:38:05.320]   not. But we will get to- we will get to more- Filling up time is not the problem.
[01:38:05.320 --> 01:38:08.360]   That's not the problem. You're exactly right, David.
[01:38:08.360 --> 01:38:25.240]   So what- so between tech TV and your new CISO series, the CISO series.com, what are you even up to?
[01:38:26.760 --> 01:38:30.840]   But then- well, the most interesting thing is what I'm doing right now, the CISO series. It's-
[01:38:30.840 --> 01:38:36.200]   You know, if anyone's listening, they're in the cybersecurity industry or you want to be-
[01:38:36.200 --> 01:38:38.840]   You don't want to be talking about your time in the merchant marine and-
[01:38:38.840 --> 01:38:41.320]   I was not in the merchant marine. Okay. Okay.
[01:38:41.320 --> 01:38:44.360]   Why- no, but you know, that's-
[01:38:44.360 --> 01:38:48.040]   Fighting in- Look, here's bars across the globe or-
[01:38:48.040 --> 01:38:52.840]   You are doing this extraordinarily well in terms of making tech entertaining,
[01:38:52.840 --> 01:38:57.480]   interesting fun. Now you're grooming me and- Are you breaking out in the hive? Yes.
[01:38:57.480 --> 01:39:05.560]   The thing that we're trying to do in cybersecurity specifically, and I know you have your security
[01:39:05.560 --> 01:39:10.760]   now to show as well, we're just trying to make it fun. We're trying to make it entertaining.
[01:39:10.760 --> 01:39:15.880]   We've got a great community of people. Honestly, this whole concept started because-
[01:39:15.880 --> 01:39:18.120]   I don't know how much you've talked about it on your shows-
[01:39:18.120 --> 01:39:22.840]   Security vendors and security practitioners are like butting heads constantly.
[01:39:22.840 --> 01:39:23.480]   Yes.
[01:39:23.480 --> 01:39:29.320]   And you know, the problem is, and it's this weird dichotomy of how the industry works is,
[01:39:29.320 --> 01:39:31.960]   you've got somewhere- and we don't know exactly how many-
[01:39:31.960 --> 01:39:36.440]   But there's somewhere between 3,000 to 5,000 security vendors out there.
[01:39:36.440 --> 01:39:38.600]   And then they're all selling-
[01:39:38.600 --> 01:39:40.120]   They all advertise with us.
[01:39:40.120 --> 01:39:42.440]   Thank you- you would hope.
[01:39:44.120 --> 01:39:51.000]   But they're all selling to CISOs. And so what's happening is just CISOs can-
[01:39:51.000 --> 01:39:53.560]   They got to run their environment. They got to secure their environment.
[01:39:53.560 --> 01:39:57.160]   They got to work with their team. They're managing and also working with the business.
[01:39:57.160 --> 01:40:00.360]   And then at the same time, they have to look at new security solutions.
[01:40:00.360 --> 01:40:03.320]   So- It's a challenging space. Absolutely.
[01:40:03.320 --> 01:40:04.680]   It's very, very challenging.
[01:40:04.680 --> 01:40:05.640]   Yeah, absolutely.
[01:40:05.640 --> 01:40:14.040]   So we began the network with the original mission of how can we ease this conflict that's going on?
[01:40:14.520 --> 01:40:18.360]   And stop blaming and figure out a better way to communicate?
[01:40:18.360 --> 01:40:18.760]   Love it.
[01:40:18.760 --> 01:40:22.440]   And then it's sort of grown out of that, actually, is where it came.
[01:40:22.440 --> 01:40:27.000]   So nice to have you. CISO series.com, of course, Amy Webb,
[01:40:27.000 --> 01:40:32.600]   the greatest, the futurist, the author of so many good books,
[01:40:32.600 --> 01:40:36.600]   including Anna Mustard, The Genesis Machine, her latest, which is-
[01:40:36.600 --> 01:40:39.640]   It reads like a novel, except it's true, which is so fun.
[01:40:40.200 --> 01:40:43.640]   Actually, when we come back, this is an important day in my life,
[01:40:43.640 --> 01:40:48.920]   May 22nd. On the 22nd of each month, I get two new audible credits.
[01:40:48.920 --> 01:40:53.960]   And you already used up one with cultish. So maybe when we come back-
[01:40:53.960 --> 01:40:56.920]   I listened to the audiobook. That's how I read the book.
[01:40:56.920 --> 01:41:00.040]   That's how I listened to everything. I need a new book.
[01:41:00.040 --> 01:41:02.840]   So maybe you guys can come up with some recommendations.
[01:41:02.840 --> 01:41:08.040]   I'll show you what's in my wish list. You can maybe review those and decide which ones I should read.
[01:41:08.040 --> 01:41:12.440]   Our show today brought to you by my new thing,
[01:41:12.440 --> 01:41:16.600]   Nume. It's not new. I've been doing Nume for a year, Lisa, my wife,
[01:41:16.600 --> 01:41:23.080]   to support me. Isn't that great? Did it too? And she only supported me. Of course,
[01:41:23.080 --> 01:41:29.080]   she slapped me. She's looking great. And what's great about Nume is it's not a diet.
[01:41:29.080 --> 01:41:34.120]   It's a way of life that will lead to greater health.
[01:41:35.240 --> 01:41:42.520]   It's not just about losing weight, but Nume weight will help you in your path to feeling great,
[01:41:42.520 --> 01:41:48.680]   weighing the right amount for you, and most importantly, change the way you think about food.
[01:41:48.680 --> 01:41:51.480]   It is not a diet because you don't have to change your lifestyle.
[01:41:51.480 --> 01:41:55.160]   Nume doesn't believe in restricting what you can or can't eat. Instead,
[01:41:55.160 --> 01:42:00.760]   Nume gives you knowledge. This is, of course, the secret, isn't it? And the wisdom you need
[01:42:00.760 --> 01:42:06.840]   to make informed choices and then gives you the encouragement you need to stay on that path.
[01:42:06.840 --> 01:42:11.880]   It's a psychological approach based on things like cognitive behavioral therapy, which really,
[01:42:11.880 --> 01:42:16.680]   really works. With Nume, you get a number of tools. You get the app. You can track everything
[01:42:16.680 --> 01:42:21.160]   you're eating and drinking and all the exercise you're doing. You get a coach, personal coach,
[01:42:21.160 --> 01:42:26.360]   you get a group. It helps you understand your relationship with food and build sustainable habits
[01:42:26.360 --> 01:42:31.560]   the last of the lifetime. When I first started Nume and I told my coach, I said, "I feel bad.
[01:42:31.560 --> 01:42:36.040]   I had a hot dog." She said, "No, there's no bad foods. You can have a hot dog." I said, "What
[01:42:36.040 --> 01:42:41.000]   what kind of diet is this?" She said, "Leo, I've told you again and again. It's not a diet.
[01:42:41.000 --> 01:42:45.880]   There are no bad foods. It's about making informed choices. What I was really doing that was bad
[01:42:45.880 --> 01:42:51.720]   that I had to learn to change was fog eating, eating unconsciously, coming home and stuffing my
[01:42:51.720 --> 01:42:58.200]   mouth. Nume has really helped me build sustainable habits that are also fostering my health.
[01:42:58.200 --> 01:43:05.320]   Nume understands building long-term positive habits is hard. And yes, it has its ups and downs.
[01:43:05.320 --> 01:43:10.120]   They believe in progress, not perfection. Your journey is going to look different from everybody
[01:43:10.120 --> 01:43:13.720]   else, but it is grounded in science and it's the heart of everything they do. They've published
[01:43:13.720 --> 01:43:19.880]   more than 30 peer-reviewed scientific articles for users, practitioners, scientists, and the public
[01:43:19.880 --> 01:43:25.640]   about what they're doing and how it works. And I encourage you, if you want to know more to read
[01:43:25.640 --> 01:43:34.200]   up on that, you can start at the website, n-o-o-m.com/twit. When I first started Nume, they'd give you a
[01:43:34.200 --> 01:43:39.240]   fairly long questionnaire. They're really digging into what you're doing, your habits,
[01:43:39.240 --> 01:43:44.440]   how you feel about stuff. Do you stress eat? Do you fog eat? Do you let the elephant take control?
[01:43:45.160 --> 01:43:52.440]   All of that. And it's really, even in the onboarding process, a beginning of
[01:43:52.440 --> 01:43:58.120]   learning about yourself and knowing what's going on. You don't have to worry about ruining the whole
[01:43:58.120 --> 01:44:05.080]   program with one-off day. It helps you get back on track. You fit it in your life so you decide how
[01:44:05.080 --> 01:44:10.520]   much time you want to spend on Nume, how fast you want to lose weight, five, 10, 15 minutes a day.
[01:44:11.160 --> 01:44:16.760]   All of that is in your control. And that's really what Nume is about, is about being in control,
[01:44:16.760 --> 01:44:25.240]   not forcing anything. I'm such a fan. It works so well. And if Lisa ever sees you,
[01:44:25.240 --> 01:44:30.520]   you just ask her about Nume and prepare because she will tell you how great it is,
[01:44:30.520 --> 01:44:35.000]   the whole story. It's really fantastic. It's really fantastic. In fact, you can read it online.
[01:44:35.560 --> 01:44:41.800]   Go to Nume, N-O-O-M.com/Twit and start building better habits today. Of course, they have a trial.
[01:44:41.800 --> 01:44:50.840]   So at least do that for yourself. N-O-O-M.com/Twit. See if it works for you. I think you'll see why
[01:44:50.840 --> 01:44:59.240]   it is now becoming such a great choice for so many people. And I love the community that I had
[01:44:59.240 --> 01:45:06.360]   in Nume and my coach. And we communicate regularly. There's also a great Nume subreddit with lots
[01:45:06.360 --> 01:45:13.560]   of additional ideas. I just think Nume is fantastic. Nume, N-O-O-M.com/Twit. I want you to sign up for
[01:45:13.560 --> 01:45:17.880]   trial. If you've been looking for something, this may be the thing you're looking for. It was for
[01:45:17.880 --> 01:45:23.000]   me. Nume.com/Twit. Thank you, Nume, from the bottom of my heart, seriously.
[01:45:25.800 --> 01:45:31.320]   All right. By the way, one of the things that's kind of a through line in this whole conversation is
[01:45:31.320 --> 01:45:39.240]   we were talking even before the show started about Elon. Again, this week, saying we're going to
[01:45:39.240 --> 01:45:46.120]   have fully self-driving cars next year, which he's been saying every year since 2014.
[01:45:46.120 --> 01:45:51.320]   One of these years he'll be right. Well, but I think it's also a testament. I don't think Elon's,
[01:45:52.040 --> 01:45:58.200]   I think he believes it. It's a testament to how hard that last little bit is. That last little
[01:45:58.200 --> 01:46:10.920]   bit is almost impossible. This is the week that Apple, apparently according to Bloomberg and Mark
[01:46:10.920 --> 01:46:20.440]   German showed its glasses, VR, AR, I don't know, to the board. Apple's got the WWDC keynote coming up
[01:46:20.440 --> 01:46:28.280]   June 6th. I imagine they may say something then in a month. They may say something in the fall.
[01:46:28.280 --> 01:46:35.160]   Rumor is we'll sell something this year. This is a really good example. Google at Google I/O
[01:46:35.160 --> 01:46:41.880]   showed some thick spectacles that put translations up in your heads up display so that
[01:46:41.880 --> 01:46:47.480]   a young woman could talk to her mother. Her mother speaks Mandarin. The woman speaks English,
[01:46:47.480 --> 01:46:51.080]   and she could see the translation in English. On her glasses, the mother could see the Mandarin
[01:46:51.080 --> 01:46:58.280]   translation. They were able to talk to one another. Amy, are you? Do you think that's the killer app?
[01:46:58.280 --> 01:47:06.840]   That's my question. Because I have those quest goggles from Facebook. What do you think is the
[01:47:06.840 --> 01:47:12.200]   killer app? What is the tool that'll get people to buy these units? Not making people nauseated.
[01:47:13.400 --> 01:47:19.400]   Well, I think the quest doesn't make me nauseated. I must say that exercise apps is what I've used
[01:47:19.400 --> 01:47:23.560]   them for. But there are plenty of people that include myself who do get nauseated.
[01:47:23.560 --> 01:47:27.960]   And I think it's hard to sell other ones prior to Quest that the screen is.
[01:47:27.960 --> 01:47:30.840]   Did Father You have a question? I don't have a quest because I don't have a Facebook account.
[01:47:30.840 --> 01:47:37.320]   So I'm never going to have a quest. Apple's probably, I'll get the Apple one even though it's
[01:47:37.320 --> 01:47:43.720]   rumored to be expensive $3,000. Because it's not pre-consumer releases.
[01:47:43.720 --> 01:47:50.280]   Oh, no. It's not the... Yeah. I want AR badly. I want to be able to wear glasses around that
[01:47:50.280 --> 01:47:57.400]   will give me extra information as I walk around the world. So you want your world augmented?
[01:47:57.400 --> 01:48:01.560]   In fact, Amy, I know you believe in it because you consulted. What was the name of that show?
[01:48:01.560 --> 01:48:05.640]   I really liked that show. I don't know. It's been a lot of shows.
[01:48:05.640 --> 01:48:10.360]   I know. You liked it? You were in a very small number of shows.
[01:48:10.360 --> 01:48:13.960]   I know what we're talking about. Well, it was canceled. The idea was they were going to be the
[01:48:13.960 --> 01:48:20.600]   first on Mars. But one of the technologies, and I know it came from you, was the young woman's
[01:48:20.600 --> 01:48:24.200]   wearing some stylish glasses. They're talking about something. She said, "Well, let me show you."
[01:48:24.200 --> 01:48:29.160]   She gives the glasses to the guy so he can see it on the heads-up display, the augmented reality,
[01:48:29.160 --> 01:48:36.280]   what she's talking about. Your consulting for this show was to put them in the near future, right?
[01:48:36.280 --> 01:48:43.160]   Yeah. I do actually a lot of work on shows and movies that are set in the future. There's one
[01:48:43.160 --> 01:48:50.360]   coming out this summer that is... We will talk about offline. It's a secret. It's going to be terrible.
[01:48:50.360 --> 01:48:53.640]   It's a secret. It's going to be terrible. Yeah. That's not in your control, is it?
[01:48:53.640 --> 01:48:58.040]   It's not in my control. But let me tell you about glasses. Yeah, that's the problem.
[01:48:58.040 --> 01:49:02.280]   One of the problems. I thought the first was great. That was really sad because it ends in the first
[01:49:02.280 --> 01:49:07.880]   season with them launching to Mars, and they're never going to get there. Right. Here's the problem.
[01:49:07.880 --> 01:49:15.960]   It was marketed. The talking points were that it was the first group of people to live on Mars,
[01:49:15.960 --> 01:49:20.360]   to make it to Mars. And that's not what the story was about. Bill Williman, who did the
[01:49:20.360 --> 01:49:24.440]   Love Original Showrunner. Love Bill Williman. He's a great storyteller. Really great.
[01:49:24.440 --> 01:49:30.440]   Exceptional writer. He did. He has a card. He did. He also did a movie called "Eids of March." He's
[01:49:30.440 --> 01:49:37.480]   very smart. But the show was about getting to Mars, not being on Mars. So there was this enormous
[01:49:37.480 --> 01:49:42.680]   disappointment. Anyway, on classes... I love Natasha McAlone. Oh, she's great. I'm not sure I
[01:49:42.680 --> 01:49:47.800]   want Sean Penn to be the first person on Mars, but okay. You have to have a big name attached to
[01:49:47.800 --> 01:49:53.160]   anything anymore to get funding and to get production through. So when I have a lot of respect for him,
[01:49:53.160 --> 01:50:00.840]   he's a rugged handsome man. I guess I don't know. He's a name. He's a name. He's a name.
[01:50:00.840 --> 01:50:05.720]   Anyways, with the glasses. So a lot of people would say, "I'm never going to wear...
[01:50:05.720 --> 01:50:11.240]   I'm never going to use VR." And by the way, VR has a very limited set of use cases. Again, because
[01:50:13.080 --> 01:50:16.760]   significant percentage, at least 10, get nauseated every time they use VR.
[01:50:16.760 --> 01:50:21.080]   Right. So let's think about glasses like I'm wearing. And so I go... That's what I want.
[01:50:21.080 --> 01:50:28.840]   In 2018... I want your glasses. So in 2018, I built this model that seems to be holding,
[01:50:28.840 --> 01:50:35.000]   showing the sort of plateau of mobile devices to be replaced by a constellation of other devices
[01:50:35.000 --> 01:50:41.560]   that all rely on the cloud. So this one device goes away. It's apples and stress. Because 50% of
[01:50:41.560 --> 01:50:47.960]   their revenue this quarter was the iPhone. They got to think, "As this goes away, what's going to be
[01:50:47.960 --> 01:50:53.880]   next?" So the model, the math on it, is continuing to hold. And so the thing that I've been talking
[01:50:53.880 --> 01:51:00.680]   about since 2016, 2017, 2018 is that this is what comes next glasses, which has been met with
[01:51:00.680 --> 01:51:04.280]   abject hostility or... Like, just, you're wrong.
[01:51:04.280 --> 01:51:09.560]   Yeah, the history of violence in San Francisco with people literally beating them...
[01:51:09.560 --> 01:51:11.160]   Google Glass.
[01:51:11.160 --> 01:51:13.400]   The tech bros with Google Glass on.
[01:51:13.400 --> 01:51:14.920]   But they deserve to be beaten up. The Glass Holes.
[01:51:14.920 --> 01:51:15.880]   The Glass Holes.
[01:51:15.880 --> 01:51:16.680]   They deserved it.
[01:51:16.680 --> 01:51:19.240]   But here's the data that we have.
[01:51:19.240 --> 01:51:22.120]   Wait, are you promoting violence on a media channel?
[01:51:22.120 --> 01:51:22.680]   Yes.
[01:51:22.680 --> 01:51:23.240]   Leo?
[01:51:23.240 --> 01:51:23.640]   Yes.
[01:51:23.640 --> 01:51:26.680]   In the United States.
[01:51:26.680 --> 01:51:27.800]   Two people in a good punch.
[01:51:27.800 --> 01:51:29.640]   Natsies and Glass Holes. Go ahead.
[01:51:29.640 --> 01:51:31.080]   Yes, always.
[01:51:31.080 --> 01:51:31.720]   Yes.
[01:51:31.720 --> 01:51:38.040]   In the US, the number of people with myopia, which is the technical turn for nearsightedness,
[01:51:38.040 --> 01:51:40.760]   is like a hockey stick. It's going way up really fast.
[01:51:40.760 --> 01:51:41.320]   Is it?
[01:51:41.320 --> 01:51:42.360]   Totally makes sense.
[01:51:42.360 --> 01:51:43.720]   Because we're all staring at screens.
[01:51:43.720 --> 01:51:44.680]   Is that why?
[01:51:44.680 --> 01:51:48.680]   We are all staring at screens and the tech has evolved faster than our biology.
[01:51:48.680 --> 01:51:53.720]   Our eyes were built to see long distances so that we didn't get killed by animals coming at us.
[01:51:53.720 --> 01:51:55.160]   Not short distances.
[01:51:55.160 --> 01:51:55.400]   Right.
[01:51:55.400 --> 01:52:00.840]   So what we know to be true is most people are going to need some type of corrective lens anyway.
[01:52:00.840 --> 01:52:03.640]   And that is especially true of people who are very young.
[01:52:03.640 --> 01:52:09.480]   So kids especially. And after being through COVID that caused a second bump in acceleration.
[01:52:09.480 --> 01:52:11.880]   So we're already going to need to wear glasses.
[01:52:11.880 --> 01:52:16.360]   Now, if your choice is you can have analog glasses.
[01:52:16.360 --> 01:52:19.960]   You can have like plain old glasses, glasses like that don't do anything.
[01:52:19.960 --> 01:52:23.880]   Or you can have enhanced glasses with these other capabilities.
[01:52:23.880 --> 01:52:27.000]   And if those capabilities are basically a heads up display,
[01:52:27.000 --> 01:52:28.440]   what do you think people are going to choose?
[01:52:28.440 --> 01:52:33.400]   It's a it is a absolute certainty.
[01:52:33.400 --> 01:52:38.440]   I mean, I'm very I'm not certain about a lot of things this if they're but this.
[01:52:38.440 --> 01:52:39.880]   But much more certain.
[01:52:39.880 --> 01:52:42.680]   I'm they have to get the technical limitations of the problem.
[01:52:42.680 --> 01:52:43.320]   Battery.
[01:52:43.320 --> 01:52:43.720]   Yes.
[01:52:43.720 --> 01:52:44.280]   Heat.
[01:52:44.280 --> 01:52:44.600]   Yes.
[01:52:44.600 --> 01:52:44.840]   Right.
[01:52:44.840 --> 01:52:47.800]   And right now that build is happening back and I'm pointing.
[01:52:47.800 --> 01:52:49.480]   She showed us her air temple.
[01:52:49.480 --> 01:52:50.920]   Yeah.
[01:52:50.920 --> 01:52:51.960]   I'm actually speaking.
[01:52:51.960 --> 01:52:52.120]   Yeah.
[01:52:52.120 --> 01:52:53.560]   Why were my hearing aids?
[01:52:53.560 --> 01:52:57.160]   It's got a big old battery in the behind the ear and a little.
[01:52:57.160 --> 01:52:57.640]   Yeah.
[01:52:57.640 --> 01:52:58.120]   So.
[01:52:58.120 --> 01:53:00.440]   And that requires less compute.
[01:53:00.440 --> 01:53:04.200]   The hearing aids are amazing technology what they're able to do.
[01:53:04.200 --> 01:53:06.120]   But that's not that's not video.
[01:53:06.120 --> 01:53:08.360]   So that's a that's a different challenge.
[01:53:08.360 --> 01:53:08.680]   Right.
[01:53:08.680 --> 01:53:11.160]   The optical it might husband's an eye doctor.
[01:53:11.160 --> 01:53:13.080]   So like I've got some background on this.
[01:53:13.080 --> 01:53:18.920]   The optical limitations are not as challenging as things like battery life.
[01:53:18.920 --> 01:53:21.480]   And you know, all of that.
[01:53:21.480 --> 01:53:24.040]   But it's it will be overcome at some point.
[01:53:24.040 --> 01:53:27.640]   So it totally makes sense that Apple would be going all in.
[01:53:27.640 --> 01:53:29.800]   And there would be a market for it.
[01:53:29.800 --> 01:53:30.040]   Yep.
[01:53:30.040 --> 01:53:32.840]   And Qualcomm just announced this week.
[01:53:32.840 --> 01:53:35.880]   It's gotten a new platform for AR glasses.
[01:53:35.880 --> 01:53:36.120]   Yeah.
[01:53:36.120 --> 01:53:36.840]   Everybody's working.
[01:53:36.840 --> 01:53:40.040]   So again, I don't think this is about Pokemon go on steroids.
[01:53:40.040 --> 01:53:42.200]   But I think this is a heads up display.
[01:53:42.200 --> 01:53:46.200]   The kinds of stuff that you would be looking at your phone for like directions.
[01:53:46.200 --> 01:53:48.600]   I think translation is more likely to be.
[01:53:48.600 --> 01:53:52.040]   Well, I guess you could have a heads up on translation.
[01:53:52.040 --> 01:53:55.880]   But it's that like today I went on a I do really long distance bike riding.
[01:53:55.880 --> 01:53:57.480]   I saw you got a new gravel bike.
[01:53:57.480 --> 01:53:58.040]   Yeah.
[01:53:58.040 --> 01:53:58.680]   Oh my god.
[01:53:58.680 --> 01:54:01.400]   That bike is I don't tell anybody.
[01:54:01.400 --> 01:54:03.480]   I'm talking quietly because Brian's in the other room.
[01:54:03.480 --> 01:54:05.560]   I totally smoked him like hard.
[01:54:05.560 --> 01:54:08.360]   And he was it was demoralizing.
[01:54:08.360 --> 01:54:12.840]   I pushed him to go an extra six miles and he was like ticked off.
[01:54:12.840 --> 01:54:14.520]   And then I couldn't slow down.
[01:54:14.520 --> 01:54:15.960]   I've got so much.
[01:54:15.960 --> 01:54:19.640]   The cemetery is right now recording this at a lower level.
[01:54:19.640 --> 01:54:24.600]   At any rate, I've got a computer on my bike.
[01:54:24.600 --> 01:54:26.360]   I've also got my iPhone.
[01:54:26.360 --> 01:54:30.760]   It would be neat if the wrap around super ugly.
[01:54:30.760 --> 01:54:31.720]   That's fine.
[01:54:31.720 --> 01:54:37.800]   Glasses showed me all the data so that I wasn't having to look at my computer the whole time.
[01:54:37.800 --> 01:54:38.440]   Yeah, I know that's.
[01:54:38.440 --> 01:54:45.000]   And here is all of the left pedal, the right pedal, the all of the boy you talk about.
[01:54:45.000 --> 01:54:48.200]   If you're if you're like a data person,
[01:54:48.200 --> 01:54:50.040]   the quantified cyclist.
[01:54:50.040 --> 01:54:50.360]   Wow.
[01:54:50.360 --> 01:54:50.840]   Yeah.
[01:54:50.840 --> 01:54:51.320]   Yeah.
[01:54:51.320 --> 01:54:54.280]   Is that stuff you're wearing or is it all from the bike itself?
[01:54:55.560 --> 01:54:57.560]   I'm wearing I had a heart rate monitor on.
[01:54:57.560 --> 01:54:59.960]   I've got a wristband monitor on.
[01:54:59.960 --> 01:55:06.520]   I've got my I've got this really cool system from Garmin that I've got sensors on the wheels.
[01:55:06.520 --> 01:55:07.560]   I've got sensors on my.
[01:55:07.560 --> 01:55:09.240]   See David, she's living in the future.
[01:55:09.240 --> 01:55:11.080]   That's how she can be a futurist.
[01:55:11.080 --> 01:55:11.800]   It's pretty cool.
[01:55:11.800 --> 01:55:13.480]   I felt like Tron today.
[01:55:13.480 --> 01:55:15.480]   Today it was amazing.
[01:55:15.480 --> 01:55:19.320]   Now the question is, are you going to write it again tomorrow or is it just that it?
[01:55:19.320 --> 01:55:19.800]   Are you done?
[01:55:19.800 --> 01:55:19.960]   No, no.
[01:55:19.960 --> 01:55:21.240]   Tomorrow's a recovery day.
[01:55:21.240 --> 01:55:22.680]   I pushed it pretty hard today.
[01:55:22.680 --> 01:55:24.440]   I've got to do some recovery rides tomorrow.
[01:55:24.440 --> 01:55:25.880]   So I can get flush out my system.
[01:55:25.880 --> 01:55:28.200]   The diverged gravel bike from specialized.
[01:55:28.200 --> 01:55:31.480]   I like specialized bikes, but I bet it's not electric, right?
[01:55:31.480 --> 01:55:33.400]   Oh, no, it's not electric.
[01:55:33.400 --> 01:55:35.000]   You guys have to get them.
[01:55:35.000 --> 01:55:36.520]   You're going to have to wait a year.
[01:55:36.520 --> 01:55:37.640]   I actually bought it or.
[01:55:37.640 --> 01:55:39.640]   Electric bikes all over the place.
[01:55:39.640 --> 01:55:41.160]   I love my electric bikes.
[01:55:41.160 --> 01:55:41.640]   Yeah.
[01:55:41.640 --> 01:55:45.320]   If there weren't so many gosh darn cars around, it'd be great.
[01:55:45.320 --> 01:55:50.520]   Let me ask you, what percentage of bikes on the road where you live, Leo or electric?
[01:55:50.520 --> 01:55:51.640]   What percentage would you say?
[01:55:51.640 --> 01:55:57.480]   Uh, up here in Petaloma, it's probably a third, according to a third.
[01:55:57.480 --> 01:55:57.960]   It's not.
[01:55:57.960 --> 01:55:59.800]   95% of what I say.
[01:55:59.800 --> 01:56:00.280]   I moved down to.
[01:56:00.280 --> 01:56:01.240]   Where do you live?
[01:56:01.240 --> 01:56:01.800]   Carlsbad.
[01:56:01.800 --> 01:56:02.360]   I moved.
[01:56:02.360 --> 01:56:03.560]   I'm now in Carlsbad.
[01:56:03.560 --> 01:56:05.160]   I can stay in the ego area.
[01:56:05.160 --> 01:56:06.120]   I would say 95%.
[01:56:06.120 --> 01:56:08.120]   Everything's an electric bike.
[01:56:08.120 --> 01:56:08.680]   Yeah.
[01:56:08.680 --> 01:56:09.160]   Everything.
[01:56:09.160 --> 01:56:14.600]   If you look in our garage, we've got four electric bikes, two segues, three electric cars.
[01:56:14.600 --> 01:56:17.800]   We're all in on electric.
[01:56:17.800 --> 01:56:18.680]   I love it.
[01:56:18.680 --> 01:56:20.120]   You have solar panels on the roof?
[01:56:20.120 --> 01:56:20.760]   Lots of them.
[01:56:20.760 --> 01:56:21.960]   I got more than 60.
[01:56:21.960 --> 01:56:22.920]   I got a ton of them.
[01:56:22.920 --> 01:56:25.880]   And we got two Tesla power walls.
[01:56:25.880 --> 01:56:28.840]   So I just-
[01:56:28.840 --> 01:56:30.280]   So let's talk smack about Elon.
[01:56:30.280 --> 01:56:32.040]   Yeah, really.
[01:56:32.040 --> 01:56:34.360]   Well, you know, when I was- when I was-
[01:56:34.360 --> 01:56:38.520]   I got the- like one of the first Model X's.
[01:56:38.520 --> 01:56:40.120]   So I got on the list like a year before.
[01:56:40.120 --> 01:56:42.360]   And when I was on the list and I thought,
[01:56:42.360 --> 01:56:44.280]   I can't say anything bad about Tesla.
[01:56:44.280 --> 01:56:47.480]   Because there was a blogger who said something about-
[01:56:48.520 --> 01:56:50.600]   that Elon didn't like and Elon
[01:56:50.600 --> 01:56:53.320]   canceled his reservation.
[01:56:53.320 --> 01:56:57.320]   And I thought, I better be careful what I say.
[01:56:57.320 --> 01:57:01.000]   Now I can say anything I want because I don't drive a Tesla anymore.
[01:57:01.000 --> 01:57:03.000]   And also they need people to place orders.
[01:57:03.000 --> 01:57:03.480]   So-
[01:57:03.480 --> 01:57:03.800]   Do they-
[01:57:03.800 --> 01:57:05.320]   I think-
[01:57:05.320 --> 01:57:05.880]   Don't they?
[01:57:05.880 --> 01:57:06.680]   I'll tell you what.
[01:57:06.680 --> 01:57:08.760]   I don't know what it's like in Carlsbad.
[01:57:08.760 --> 01:57:10.440]   Or are you in Baltimore right now?
[01:57:10.440 --> 01:57:11.320]   Whatever you are.
[01:57:11.320 --> 01:57:11.880]   Me?
[01:57:11.880 --> 01:57:12.040]   Yeah.
[01:57:12.040 --> 01:57:13.800]   But-
[01:57:13.800 --> 01:57:14.520]   No, I mean, I-
[01:57:14.520 --> 01:57:15.080]   I see nothing but Tesla's-
[01:57:15.080 --> 01:57:16.840]   I see nothing but Tesla's everywhere.
[01:57:16.840 --> 01:57:18.840]   I've never-
[01:57:18.840 --> 01:57:20.600]   That's what's all over the place, Ryan.
[01:57:20.600 --> 01:57:22.760]   It's totally dominant all of a sudden.
[01:57:22.760 --> 01:57:24.600]   Everybody's driving Tesla's up here.
[01:57:24.600 --> 01:57:25.800]   I thought they missed their-
[01:57:25.800 --> 01:57:26.680]   I'm maybe-
[01:57:26.680 --> 01:57:27.400]   I shouldn't say.
[01:57:27.400 --> 01:57:29.800]   I thought maybe they missed their quarterly-
[01:57:29.800 --> 01:57:30.280]   Well, because of-
[01:57:30.280 --> 01:57:32.840]   Supply chain?
[01:57:32.840 --> 01:57:33.560]   Supply chain.
[01:57:33.560 --> 01:57:34.600]   Uh, you know,
[01:57:34.600 --> 01:57:37.160]   I feel like we're never going to be able to buy anything again.
[01:57:37.160 --> 01:57:39.000]   Better take good carrier iPhone kids.
[01:57:39.000 --> 01:57:40.840]   Uh, it's-
[01:57:40.840 --> 01:57:42.760]   It's going to get nasty out here, right?
[01:57:42.760 --> 01:57:45.400]   Yeah, anything with a semi-conductor.
[01:57:45.400 --> 01:57:49.140]   I guess we're not talking about semiconductors, but that's whole mess.
[01:57:49.140 --> 01:57:50.860]   We can talk about semiconductors.
[01:57:50.860 --> 01:57:53.500]   So you believe AR is a real thing.
[01:57:53.500 --> 01:57:56.860]   David, are you, you were spectacles?
[01:57:56.860 --> 01:57:59.060]   I wear contacts, but normally I wear spectacles.
[01:57:59.060 --> 01:57:59.880]   I can't believe.
[01:57:59.880 --> 01:58:04.280]   I would. In fact, I already when I buy my glasses, I choose all the options of
[01:58:04.280 --> 01:58:09.600]   the thinnest lenses protection, the glare, the this, the that.
[01:58:09.600 --> 01:58:11.780]   I just have to click make it AR.
[01:58:12.880 --> 01:58:16.640]   I will definitely make it AR. Absolutely.
[01:58:16.640 --> 01:58:22.000]   But I also think that's in the one that a working one is in the 1% that hard part.
[01:58:22.000 --> 01:58:25.460]   But I, but I'm, but I think what you said, Amy is,
[01:58:25.460 --> 01:58:29.800]   is key is that what we're already looking on our phone will be on our glasses.
[01:58:29.800 --> 01:58:33.200]   It obviously it's going to take some time for that.
[01:58:33.200 --> 01:58:38.480]   What that looks like to sort of work itself out because the experience on the
[01:58:38.480 --> 01:58:41.360]   phone is taking many years to work itself out too.
[01:58:41.760 --> 01:58:48.120]   But I still believe there needs to be that one compelling reason as is there is
[01:58:48.120 --> 01:58:51.240]   with pretty much all new hardware.
[01:58:51.240 --> 01:58:54.640]   What is that one compelling reason we're doing it?
[01:58:54.640 --> 01:58:57.080]   I don't exactly know.
[01:58:57.080 --> 01:58:58.880]   Do you have a sort of thought on that?
[01:58:58.880 --> 01:59:03.800]   Well, again, we've become habituated to talking more than typing.
[01:59:03.800 --> 01:59:08.040]   So I think there's some changes that are already underway that become accelerating
[01:59:08.040 --> 01:59:09.800]   factors. We're talking more than we type.
[01:59:10.880 --> 01:59:14.000]   We already have heads up displaying other places.
[01:59:14.000 --> 01:59:22.000]   You know, and this one device, a site, you know, there's different functions,
[01:59:22.000 --> 01:59:25.440]   but some of the sort of, again, everyday functions,
[01:59:25.440 --> 01:59:28.560]   quantifying some things that people are interested in,
[01:59:28.560 --> 01:59:30.600]   directions, translation.
[01:59:30.600 --> 01:59:34.760]   We don't need a phone to do a lot of that.
[01:59:34.760 --> 01:59:36.080]   It's like, I agree.
[01:59:36.080 --> 01:59:37.600]   All of those things. I don't know that you need a one.
[01:59:37.600 --> 01:59:40.520]   I do. I believe you need a one.
[01:59:40.600 --> 01:59:41.600]   Yeah, you need a one.
[01:59:41.600 --> 01:59:46.280]   There's always you just look at all the history of hardware technology.
[01:59:46.280 --> 01:59:50.040]   And there's always one, something two or three,
[01:59:50.040 --> 01:59:54.120]   but there's always a few things that just drive the masses.
[01:59:54.120 --> 01:59:57.320]   And I always start with, you know, the story of the television.
[01:59:57.320 --> 02:00:00.360]   People didn't buy a television to see television.
[02:00:00.360 --> 02:00:02.920]   They bought it so they could watch the Milton Burl show.
[02:00:02.920 --> 02:00:06.360]   And so there's the killer sort of evolution.
[02:00:06.360 --> 02:00:09.080]   Sir, what the killer app theory?
[02:00:09.640 --> 02:00:14.200]   You got to have the killer app or the killer content, whatever it is.
[02:00:14.200 --> 02:00:15.360]   Yeah.
[02:00:15.360 --> 02:00:19.520]   So Gary V had a big conference this weekend
[02:00:19.520 --> 02:00:21.880]   called Vee Con.
[02:00:21.880 --> 02:00:23.400]   A lot of pictures. Yeah.
[02:00:23.400 --> 02:00:24.760]   A lot of pictures from that thing. Yeah.
[02:00:24.760 --> 02:00:28.480]   Bringing together iconic leaders from Web3 and popular culture.
[02:00:28.480 --> 02:00:33.440]   Steve Ioki pushed a cake into Beeple's face.
[02:00:33.440 --> 02:00:36.880]   Yeah, when I think Web3, I definitely think Snoop Dogg.
[02:00:36.880 --> 02:00:38.680]   Snoop Dogg totally makes sense to me.
[02:00:38.680 --> 02:00:45.280]   So Gary Vee, who himself made $90 million last year on NFTs,
[02:00:45.280 --> 02:00:48.240]   promoting NFTs with big names.
[02:00:48.240 --> 02:00:49.440]   Paris Hilton was there.
[02:00:49.440 --> 02:00:53.360]   I don't know. She's not a big name anymore. I don't know.
[02:00:53.360 --> 02:00:58.000]   Deepak Chopra. Deepak Chopra was the keynote.
[02:00:58.000 --> 02:00:58.760]   Yeah. Yeah.
[02:00:58.760 --> 02:01:00.400]   It's all the scammers, basically.
[02:01:00.400 --> 02:01:08.040]   He said 98% of NFT projects will fail after the gold rush fades.
[02:01:08.760 --> 02:01:12.400]   I bet he did not say that when he took the stage at Vee Con this weekend.
[02:01:12.400 --> 02:01:16.000]   Maybe he did. Maybe he did.
[02:01:16.000 --> 02:01:19.400]   Who is the con here in Vee Con?
[02:01:19.400 --> 02:01:21.720]   And who is the con?
[02:01:21.720 --> 02:01:25.800]   Isn't con supposed to stand for conference?
[02:01:25.800 --> 02:01:29.640]   Yeah. Oh, I'm sorry. Oh, I thought it was like con job.
[02:01:29.640 --> 02:01:31.000]   OK.
[02:01:31.000 --> 02:01:33.440]   Well, this is what this is the second meaning.
[02:01:33.440 --> 02:01:37.040]   I don't believe that that's how Gary chose to sell it.
[02:01:37.040 --> 02:01:43.600]   No, because if you've got an NFT collection you're selling, which he does,
[02:01:43.600 --> 02:01:47.720]   they're currently at $41,000 apiece, but he hopes they'll go to the moon.
[02:01:47.720 --> 02:01:51.160]   Your whole goal in life is to convince the suckers.
[02:01:51.160 --> 02:01:54.720]   I mean, everybody else that these are going to be.
[02:01:54.720 --> 02:01:58.680]   Can we just pause for a moment on the whole NFT on the crypto,
[02:01:58.680 --> 02:02:01.440]   which I'm sure you've spent umpteen times on?
[02:02:01.440 --> 02:02:07.000]   This is all based on the theory of convincing enough people
[02:02:07.000 --> 02:02:12.480]   hosting events like this, throwing advertising that something has value,
[02:02:12.480 --> 02:02:16.160]   even before all this existed, just like a collectible item.
[02:02:16.160 --> 02:02:19.800]   The only reason some collectible item exists or it has value.
[02:02:19.800 --> 02:02:21.480]   To sell on somebody else. Yeah.
[02:02:21.480 --> 02:02:22.880]   No, it's because there's none.
[02:02:22.880 --> 02:02:26.520]   They're person who's willing to pay that price for that object.
[02:02:26.520 --> 02:02:28.600]   It doesn't intrinsically have value.
[02:02:28.600 --> 02:02:32.720]   It's because well, this is even less so because you don't even get the baseball
[02:02:32.720 --> 02:02:34.360]   card or the painting.
[02:02:34.560 --> 02:02:37.000]   Well, you do because you get the the
[02:02:37.000 --> 02:02:38.840]   you get bragging rights.
[02:02:38.840 --> 02:02:40.760]   You get bragging rights.
[02:02:40.760 --> 02:02:42.840]   You don't even get an object.
[02:02:42.840 --> 02:02:46.080]   You don't get bored apes or snocking.
[02:02:46.080 --> 02:02:49.400]   There's also the whole royalty payment theory down the line.
[02:02:49.400 --> 02:02:51.920]   I know how it's a speculative currency is what it is.
[02:02:51.920 --> 02:02:52.600]   I know.
[02:02:52.600 --> 02:02:55.600]   But it's all based on just think about it.
[02:02:55.600 --> 02:03:01.520]   If all the marketing for crypto and NFTs just stopped, they would all tank.
[02:03:01.600 --> 02:03:06.040]   That's right. They don't produce anything in the value.
[02:03:06.040 --> 02:03:07.080]   Yes, there's nothing.
[02:03:07.080 --> 02:03:11.440]   And so they have to have this is why we solve what for ads on the Super Bowl.
[02:03:11.440 --> 02:03:13.080]   Yep. For crypto.
[02:03:13.080 --> 02:03:15.920]   Yep. While you see endless stories, endless marketing.
[02:03:15.920 --> 02:03:21.200]   The only way this is the product they're selling is endless PR,
[02:03:21.200 --> 02:03:25.360]   endless marketing, endless getting stories in the news.
[02:03:25.360 --> 02:03:29.560]   That is the product you're starting to sound like me, like a boomer.
[02:03:29.920 --> 02:03:34.000]   David Spark, I bet you Amy Webb has a different point of view.
[02:03:34.000 --> 02:03:40.720]   Yeah, I actually had a really interesting conversation
[02:03:40.720 --> 02:03:49.640]   with the CTO of Roblox last week, who I met a couple of weeks ago at something
[02:03:49.640 --> 02:03:52.640]   else and I really loved his perspective.
[02:03:52.640 --> 02:03:55.840]   I don't think I'm betraying the conversation to him or the conversation we had
[02:03:55.840 --> 02:03:58.080]   to repeat when he said, you know, he
[02:03:58.560 --> 02:04:02.880]   is feeling about Web3 is that in a way, it's kind of like Web1, right?
[02:04:02.880 --> 02:04:06.040]   Which is to say, it's the foundation.
[02:04:06.040 --> 02:04:08.720]   It's the it's the TCP IP.
[02:04:08.720 --> 02:04:10.880]   It's the initial protocols.
[02:04:10.880 --> 02:04:12.200]   It's the beginning.
[02:04:12.200 --> 02:04:16.000]   And the application layer really comes in at Web4.
[02:04:16.000 --> 02:04:22.280]   And when I see the FOMO and curiosity and insanity around NFTs,
[02:04:22.280 --> 02:04:27.240]   I think I think it's a little like
[02:04:27.240 --> 02:04:31.720]   it's Bob Metcalf said, I have just invented TCP.
[02:04:31.720 --> 02:04:32.240]   Yeah.
[02:04:32.240 --> 02:04:34.120]   And would you like to invest?
[02:04:34.120 --> 02:04:35.800]   Oh, he did anything.
[02:04:35.800 --> 02:04:37.080]   Yeah, I'm sorry.
[02:04:37.080 --> 02:04:38.840]   It's been been served.
[02:04:38.840 --> 02:04:39.360]   Yeah.
[02:04:39.360 --> 02:04:39.840]   Yeah.
[02:04:39.840 --> 02:04:40.640]   It's been served.
[02:04:40.640 --> 02:04:45.640]   So for a small amount of money, you can get in on Web1.0, baby.
[02:04:45.640 --> 02:04:50.960]   I mean, the thing is we, again, like we have to continue to think about
[02:04:50.960 --> 02:04:54.200]   these evolutionary and developmental tracks, right?
[02:04:54.200 --> 02:05:01.760]   So I from the outset didn't, I understand what's driving the desire to invest.
[02:05:01.760 --> 02:05:04.520]   It's, I think it's the same emotional.
[02:05:04.520 --> 02:05:08.120]   I think it's the same thing that drives people to do day trading.
[02:05:08.120 --> 02:05:12.280]   And there is a pretty, pretty big crossover in the group that day trades
[02:05:12.280 --> 02:05:15.760]   and is on like, what's the name of that website?
[02:05:15.760 --> 02:05:18.040]   What's the name of the website where all the day traders hang out?
[02:05:18.040 --> 02:05:19.760]   Seeking alpha.
[02:05:19.760 --> 02:05:20.400]   No, not seeking.
[02:05:20.400 --> 02:05:25.840]   No, anyway, I think there's a, there's a Venn diagram because I am not a day trader.
[02:05:25.840 --> 02:05:28.560]   No, but there's a sort of day trading side.
[02:05:28.560 --> 02:05:29.800]   Robin Hood's one.
[02:05:29.800 --> 02:05:30.640]   Yeah.
[02:05:30.640 --> 02:05:33.720]   There's this, there's this Venn diagram.
[02:05:33.720 --> 02:05:39.240]   So I, and I think that the people, I mean, listen, I don't know, Gary V at all, but I,
[02:05:39.240 --> 02:05:44.400]   I know the people who were invited to speak and it's, you know, there's, there's
[02:05:44.400 --> 02:05:47.480]   like this collective of the same people sells people.
[02:05:47.480 --> 02:05:47.760]   Yeah.
[02:05:47.760 --> 02:05:48.280]   Yeah.
[02:05:48.280 --> 02:05:49.760]   And they go from event to event.
[02:05:49.760 --> 02:05:51.080]   It's like the old fire festival.
[02:05:51.080 --> 02:05:51.880]   Yeah.
[02:05:51.880 --> 02:05:54.040]   Again, this is going back to my own theory.
[02:05:54.040 --> 02:05:57.200]   The product they're selling is marketing and advertising, marketing,
[02:05:57.200 --> 02:05:59.160]   but carry these brand names.
[02:05:59.160 --> 02:05:59.680]   Yeah.
[02:05:59.680 --> 02:06:00.440]   Yeah.
[02:06:00.440 --> 02:06:06.120]   You know, you know, we saw what is, um, uh, what was said, the one ad that was
[02:06:06.120 --> 02:06:10.440]   getting everyone's credit, but anyway, Larry David, Larry David, Matt Damon.
[02:06:10.440 --> 02:06:11.440]   Oh, Matt Damon.
[02:06:11.440 --> 02:06:13.800]   Courage favors the brave.
[02:06:13.800 --> 02:06:15.520]   When I saw that, I almost vomited.
[02:06:15.520 --> 02:06:17.440]   That was just like, Oh my God.
[02:06:18.280 --> 02:06:19.360]   That was courage.
[02:06:19.360 --> 02:06:25.080]   It's the, the thing is everything else in our market that, that it's like in, you
[02:06:25.080 --> 02:06:30.480]   know, in the normal stock market, it is a company that is producing a product.
[02:06:30.480 --> 02:06:31.760]   It is producing something.
[02:06:31.760 --> 02:06:33.120]   There is something tangible.
[02:06:33.120 --> 02:06:34.000]   There's something there.
[02:06:34.000 --> 02:06:38.680]   And also in just the collectibles market, there is something tradable.
[02:06:38.680 --> 02:06:42.160]   Now it's up to the community decide whether that has value or not.
[02:06:42.160 --> 02:06:44.080]   The community is not deciding this.
[02:06:44.320 --> 02:06:49.520]   It is we are being told something has value through marketing and advertising
[02:06:49.520 --> 02:06:50.800]   and celebrities and who want.
[02:06:50.800 --> 02:06:54.880]   And the more of that goes on, the more they can keep the price up.
[02:06:54.880 --> 02:07:01.200]   Again, this, the, the currency will stay high as long as the advertising, the PR continues.
[02:07:01.200 --> 02:07:03.280]   I, you know, and I have no problem.
[02:07:03.280 --> 02:07:08.320]   If those people want to put their hard earned dollars in some sort of cryptocurrency
[02:07:08.320 --> 02:07:12.520]   or invest in NFTs or, or I don't know how you invest in web three.
[02:07:12.520 --> 02:07:13.240]   I don't have a problem.
[02:07:13.240 --> 02:07:13.800]   If they do that.
[02:07:13.800 --> 02:07:14.720]   I just have a problem.
[02:07:14.720 --> 02:07:18.920]   If they sucker people who are putting, putting the rent money into it.
[02:07:18.920 --> 02:07:21.040]   So what did they, what did they are?
[02:07:21.040 --> 02:07:21.600]   They are.
[02:07:21.600 --> 02:07:22.800]   I mean, here's the other thing.
[02:07:22.800 --> 02:07:26.960]   For everybody who wins or somebody who loses, you can't all win.
[02:07:26.960 --> 02:07:27.320]   Right.
[02:07:27.320 --> 02:07:29.880]   They're, they're making something from nothing.
[02:07:29.880 --> 02:07:33.720]   And I actually, I'm ashamed to say some of the people who've been on this show,
[02:07:33.720 --> 02:07:38.800]   some of the people I really like and know, and including Gary V have become
[02:07:38.800 --> 02:07:39.880]   shills for this.
[02:07:39.880 --> 02:07:42.760]   And because they made money.
[02:07:43.280 --> 02:07:45.520]   And almost all of them made a lot of money.
[02:07:45.520 --> 02:07:47.000]   And yeah, you're right.
[02:07:47.000 --> 02:07:47.840]   You need to show.
[02:07:47.840 --> 02:07:49.240]   You need to continue to show.
[02:07:49.240 --> 02:07:53.680]   Um, and I feel bad about it.
[02:07:53.680 --> 02:07:57.480]   I don't think it's so normally I would just say live and let live.
[02:07:57.480 --> 02:08:02.360]   But I'm starting to feel like there is this avalanche and I feel like there are a
[02:08:02.360 --> 02:08:04.000]   lot of people going to get buried by it.
[02:08:04.000 --> 02:08:05.440]   And it's well, it's there are.
[02:08:05.440 --> 02:08:07.520]   And so I live in high-level impacts.
[02:08:07.520 --> 02:08:08.760]   Right.
[02:08:08.760 --> 02:08:12.080]   What are the next order impacts of a whole bunch of people feeling very
[02:08:12.080 --> 02:08:15.480]   emotionally connected and excited and then losing a lot of money?
[02:08:15.480 --> 02:08:21.240]   But that's not, that's not, it's not going to be good when that happens and it's going to happen.
[02:08:21.240 --> 02:08:24.400]   Do you think there'll be a revolution in this country in the next five years?
[02:08:24.400 --> 02:08:25.560]   Like a violent.
[02:08:25.560 --> 02:08:26.680]   No, I don't.
[02:08:26.680 --> 02:08:28.640]   Oh, revolution.
[02:08:28.640 --> 02:08:29.720]   Now, are we going to?
[02:08:29.720 --> 02:08:32.960]   I think what winds up happening is Leo, what you talked about a couple of
[02:08:32.960 --> 02:08:37.120]   books ago, um, the totalitarian, the birth of totalitarianism.
[02:08:37.120 --> 02:08:37.400]   Yeah.
[02:08:37.400 --> 02:08:38.320]   The name of the book.
[02:08:38.320 --> 02:08:39.160]   Well, that's happening.
[02:08:39.160 --> 02:08:41.440]   That's going to happen 2024.
[02:08:42.120 --> 02:08:46.840]   I think people feel very dejected and you wind up with polarization and you wind up
[02:08:46.840 --> 02:08:50.560]   with people desiring certainty and espousing certainty.
[02:08:50.560 --> 02:08:53.080]   These things are kind of interconnected.
[02:08:53.080 --> 02:08:57.040]   And again, like, um, the government, not that the government should be.
[02:08:57.040 --> 02:09:05.080]   The, our government was way too slow on this to really really think about, um,
[02:09:05.080 --> 02:09:10.160]   policy, Biden's executive order, whenever that was a couple of months ago, like
[02:09:10.160 --> 02:09:13.240]   finally telling federal agencies like, Hey, you're going to have to come up with
[02:09:13.240 --> 02:09:16.840]   a policy on crypto and blockchain was a little, he was a little late to the party.
[02:09:16.840 --> 02:09:21.120]   Um, that, um, why do you think the reluctance was to regulate?
[02:09:21.120 --> 02:09:22.320]   It just that it's too new.
[02:09:22.320 --> 02:09:22.880]   We don't understand.
[02:09:22.880 --> 02:09:28.120]   Um, because part of, uh, there's an existential piece of this.
[02:09:28.120 --> 02:09:32.360]   I, this is just my opinion, but like China is moving pretty quickly with its
[02:09:32.360 --> 02:09:33.480]   digital currency.
[02:09:33.480 --> 02:09:39.600]   Um, as an ultra, they're, I think creating an alternative to the dollar right now,
[02:09:39.600 --> 02:09:42.560]   a lot of other, like the global economy is kind of pegged to the dollar and
[02:09:42.560 --> 02:09:45.880]   they're presenting an alternative future in which maybe that doesn't happen or
[02:09:45.880 --> 02:09:46.800]   need to be the case.
[02:09:46.800 --> 02:09:49.880]   The European central bank says we want to create a digital euro.
[02:09:49.880 --> 02:09:51.040]   Yeah.
[02:09:51.040 --> 02:09:52.840]   So everybody wants to have the dollar.
[02:09:52.840 --> 02:09:56.320]   Everybody wants to have that, that, that currency that everything else is pegged
[02:09:56.320 --> 02:09:56.840]   to, right?
[02:09:56.840 --> 02:10:01.760]   And right now, yeah, it may not be the dollar in the long run.
[02:10:01.760 --> 02:10:02.760]   It might be the renmin bow.
[02:10:02.760 --> 02:10:06.880]   I don't know, but, but, uh, it probably will be digital.
[02:10:06.880 --> 02:10:07.440]   Yes.
[02:10:08.880 --> 02:10:10.960]   It's probably going to be your good.
[02:10:10.960 --> 02:10:13.000]   Look, my good looks.
[02:10:13.000 --> 02:10:14.680]   You know what you,
[02:10:14.680 --> 02:10:16.480]   stable winds are problems.
[02:10:16.480 --> 02:10:17.120]   Yes.
[02:10:17.120 --> 02:10:18.080]   Table, Heather.
[02:10:18.080 --> 02:10:20.480]   Whoa, stand back.
[02:10:20.480 --> 02:10:22.080]   We thought they were stable.
[02:10:22.080 --> 02:10:24.000]   Apparently they're not.
[02:10:24.000 --> 02:10:28.480]   Uh, yeah, I feel like this is, uh, this is not going.
[02:10:28.480 --> 02:10:31.080]   And web three is not going.
[02:10:31.080 --> 02:10:34.640]   Well, again, like all of these things right now are like the meta versus a
[02:10:34.640 --> 02:10:35.440]   technology.
[02:10:35.440 --> 02:10:36.280]   It's a concept.
[02:10:36.280 --> 02:10:38.480]   Web three isn't a technology.
[02:10:38.480 --> 02:10:41.000]   It's just the name of the next generation.
[02:10:41.000 --> 02:10:41.360]   Yeah.
[02:10:41.360 --> 02:10:41.840]   Yeah.
[02:10:41.840 --> 02:10:45.280]   Of the web, it's we're all constantly evolving.
[02:10:45.280 --> 02:10:46.960]   And so that's a better way.
[02:10:46.960 --> 02:10:51.880]   The I, the TCP IP of all this is that in your enough.
[02:10:51.880 --> 02:10:52.480]   There are a block.
[02:10:52.480 --> 02:10:55.200]   I mean, there's a lot of blockchains and they're public blockchains.
[02:10:55.200 --> 02:10:56.080]   So I don't know.
[02:10:56.080 --> 02:11:00.640]   I'm not like, this is an area of tech that I, I sort of float around in,
[02:11:00.640 --> 02:11:02.240]   but I don't have any real depth.
[02:11:02.240 --> 02:11:03.600]   Um, so I don't know.
[02:11:03.600 --> 02:11:05.040]   Well, nobody does.
[02:11:05.040 --> 02:11:07.160]   It's all a interesting mystery.
[02:11:08.240 --> 02:11:10.640]   You did say something though that kind of triggered me there.
[02:11:10.640 --> 02:11:14.360]   I think, uh, David, you must have noticed my lovely jacket.
[02:11:14.360 --> 02:11:15.040]   Did you?
[02:11:15.040 --> 02:11:15.680]   I did.
[02:11:15.680 --> 02:11:16.120]   Yeah.
[02:11:16.120 --> 02:11:17.640]   I haven't seen you in that color.
[02:11:17.640 --> 02:11:20.360]   Most looking like a smoking jacket, but not a smoke.
[02:11:20.360 --> 02:11:24.680]   I actually have a smoking jacket also from our sponsor, Indochino.
[02:11:24.680 --> 02:11:30.840]   This was a hand tailored jacket to fit my perfect body perfectly.
[02:11:30.840 --> 02:11:34.560]   Whether you're going to be a groom in a wedding party, like our son just was
[02:11:34.560 --> 02:11:35.960]   or a lucky, not a groom.
[02:11:35.960 --> 02:11:36.440]   He wasn't a.
[02:11:37.000 --> 02:11:38.480]   So whoa, hold on.
[02:11:38.480 --> 02:11:39.120]   Hold on.
[02:11:39.120 --> 02:11:39.960]   Didn't mean to start it.
[02:11:39.960 --> 02:11:45.920]   Oh, anybody Michael got his first suit because he was in a wedding party and he
[02:11:45.920 --> 02:11:47.400]   looked awfully good.
[02:11:47.400 --> 02:11:50.880]   And we said, go on out there and that beautiful.
[02:11:50.880 --> 02:11:52.160]   Look at how good those suits look.
[02:11:52.160 --> 02:11:56.000]   Everyone wants to look their best for a wedding or a special event.
[02:11:56.000 --> 02:11:59.960]   Well, with a custom fitted suit from Indochino, you will look great.
[02:11:59.960 --> 02:12:01.560]   You will feel confident.
[02:12:01.560 --> 02:12:04.080]   You will enjoy the big day without fussing over your clothes.
[02:12:04.080 --> 02:12:06.560]   I love it because it's like a constr-
[02:12:06.560 --> 02:12:09.120]   It's kind of like Roblox for clothing.
[02:12:09.120 --> 02:12:10.680]   It's a construction set.
[02:12:10.680 --> 02:12:14.640]   You could choose every detail on your suit, on your shirt, on your dinner jacket.
[02:12:14.640 --> 02:12:19.800]   Affordable prices on custom tailored stuff.
[02:12:19.800 --> 02:12:25.040]   This was made for me and it by the way, fits perfectly, fits perfectly.
[02:12:25.040 --> 02:12:28.560]   You can customize every detail, monograms, fabrics.
[02:12:28.560 --> 02:12:30.960]   I love this beautiful purple silk.
[02:12:30.960 --> 02:12:36.240]   I got a kind of a velvet smoking jacket with satin lapels.
[02:12:36.240 --> 02:12:37.080]   It looks so good.
[02:12:37.080 --> 02:12:38.320]   I'm going to wear that on the cruise.
[02:12:38.320 --> 02:12:40.680]   I just think these are gorgeous.
[02:12:40.680 --> 02:12:43.640]   I got shirts that fit perfectly.
[02:12:43.640 --> 02:12:45.800]   Monogrammed.
[02:12:45.800 --> 02:12:51.560]   You can get statement linings, lapel shapes, custom tailored.
[02:12:51.560 --> 02:12:52.000]   Look at that.
[02:12:52.000 --> 02:12:55.360]   I might get like a red suit or a yellow.
[02:12:55.360 --> 02:12:56.600]   I might get something really wild.
[02:12:56.600 --> 02:12:59.040]   $429 for suits.
[02:12:59.040 --> 02:13:03.760]   Shirts started 79 bucks, but these are tailored to your exact measurements.
[02:13:04.120 --> 02:13:07.400]   They offer completely custom fitted shirts, casual wear and more.
[02:13:07.400 --> 02:13:16.760]   I think your husband would like for Father's Day, a beautiful new suit or a wardrobe
[02:13:16.760 --> 02:13:20.320]   personalized to a style and taste without spending a fortune.
[02:13:20.320 --> 02:13:22.120]   Look how that's by the way.
[02:13:22.120 --> 02:13:25.760]   That's the Tuxedo I got, I think on the right there.
[02:13:25.760 --> 02:13:27.640]   Doesn't that look nice?
[02:13:27.640 --> 02:13:28.720]   I'll wear it.
[02:13:28.720 --> 02:13:29.480]   I'll wear it next week.
[02:13:29.480 --> 02:13:32.920]   They're always adding new pieces and options so you can stay on trend and in style.
[02:13:33.720 --> 02:13:38.160]   They've got a, I think a very nice relaxed, but refined approach to spring suits.
[02:13:38.160 --> 02:13:39.240]   I love this jacket.
[02:13:39.240 --> 02:13:41.280]   I've always wanted a purple jacket.
[02:13:41.280 --> 02:13:42.400]   I just love it.
[02:13:42.400 --> 02:13:45.360]   They just, yeah, they have perfect suits.
[02:13:45.360 --> 02:13:46.080]   Look at that.
[02:13:46.080 --> 02:13:53.000]   $50 off any purchase of $399 or more when you use the promo code twit at indochino.com.
[02:13:53.000 --> 02:13:56.200]   I-N-D-O-C-H-I-N-O.com.
[02:13:56.200 --> 02:14:00.960]   If you've got a big day coming up or just a big date, getting the perfect
[02:14:00.960 --> 02:14:02.800]   look is no big deal with Indochino.
[02:14:03.440 --> 02:14:04.880]   It was really fun.
[02:14:04.880 --> 02:14:08.080]   Now, and by the way, now that they've got my measurements and they have, you know,
[02:14:08.080 --> 02:14:10.760]   and I've, they've got, they've got me nailed.
[02:14:10.760 --> 02:14:13.880]   I love going in there and saying, you know, I'd like to maybe not get another
[02:14:13.880 --> 02:14:17.000]   cement, another jacket, maybe some, some shirts.
[02:14:17.000 --> 02:14:17.840]   It's really fun.
[02:14:17.840 --> 02:14:22.440]   I-N-D-O-C-H-I-N-O, Indochino.com.
[02:14:22.440 --> 02:14:27.400]   Don't forget that promo code twit to save $50 on a purchase of $399 or more.
[02:14:27.400 --> 02:14:30.120]   Get a luxury suit or a Tuxedo.
[02:14:30.120 --> 02:14:32.520]   You know, every, every man.
[02:14:33.360 --> 02:14:37.040]   You know, should have a dark suit in his collection.
[02:14:37.040 --> 02:14:43.080]   And I think everyone should have a Tuxedo suit in your collection with a nice shirt
[02:14:43.080 --> 02:14:47.840]   and collar and stays and, you know, cufflinks just because you never know.
[02:14:47.840 --> 02:14:49.280]   You never know.
[02:14:49.280 --> 02:14:50.040]   Could be your wedding.
[02:14:50.040 --> 02:14:52.120]   Could be somebody else's, but you want to look sharp.
[02:14:52.120 --> 02:14:52.920]   Don't you?
[02:14:52.920 --> 02:14:53.680]   I love this.
[02:14:53.680 --> 02:14:57.320]   This is, I have been saving it because I didn't want to show anybody until we
[02:14:57.320 --> 02:15:00.560]   were talking about the ad because I, but I just love this jacket.
[02:15:00.560 --> 02:15:01.280]   That's beautiful.
[02:15:01.680 --> 02:15:04.160]   Indochino.com.
[02:15:04.160 --> 02:15:06.320]   You're going to see more of it on me.
[02:15:06.320 --> 02:15:08.480]   We had a great week this week on Twit.
[02:15:08.480 --> 02:15:11.440]   We have a little video for you to watch and enjoy.
[02:15:11.440 --> 02:15:15.960]   I can't fit you and Micah into the TV shot.
[02:15:15.960 --> 02:15:18.120]   So those of you watching the video.
[02:15:18.120 --> 02:15:18.920]   There he is.
[02:15:18.920 --> 02:15:19.800]   There he is.
[02:15:19.800 --> 02:15:20.520]   He's still here.
[02:15:20.520 --> 02:15:21.520]   Hey!
[02:15:21.520 --> 02:15:23.440]   He just doesn't fit in.
[02:15:23.440 --> 02:15:27.520]   I need a super, I need one of those iPhone wide angle cameras.
[02:15:27.520 --> 02:15:29.960]   Previously on Twit.
[02:15:30.920 --> 02:15:32.160]   Twit Live Events.
[02:15:32.160 --> 02:15:36.760]   How Apple became a trillion dollar company and lost its solar guest as the
[02:15:36.760 --> 02:15:37.720]   author, Tripp Mickel.
[02:15:37.720 --> 02:15:42.880]   I think the difference that occurred in the wake of Steve Jobs' death was that
[02:15:42.880 --> 02:15:47.360]   this became much more of a leadership by committee for somebody like Johnny who
[02:15:47.360 --> 02:15:51.120]   didn't particularly enjoy or all of a sudden he was thrust in a world where that
[02:15:51.120 --> 02:15:52.800]   was, that was what you had to do.
[02:15:52.800 --> 02:15:54.480]   iOS today.
[02:15:54.480 --> 02:16:00.360]   I am joined by my pal, my good buddy, Dan Morin, who is here to help me talk
[02:16:00.360 --> 02:16:03.240]   about apps for tabletop gaming.
[02:16:03.240 --> 02:16:04.920]   Security now.
[02:16:04.920 --> 02:16:10.920]   When a ransom-wear gang gets so big for their britches that they suggest that
[02:16:10.920 --> 02:16:16.560]   perhaps a government which is refusing to pay their ransom should be overthrown.
[02:16:16.560 --> 02:16:22.720]   They said if your current government cannot stabilize the situation, maybe it's
[02:16:22.720 --> 02:16:23.840]   worth changing it.
[02:16:23.840 --> 02:16:24.880]   Twit.
[02:16:24.880 --> 02:16:27.840]   Well, like I said, too big for their britches.
[02:16:27.840 --> 02:16:29.080]   Oh my God.
[02:16:29.480 --> 02:16:31.080]   That was an amazing story.
[02:16:31.080 --> 02:16:37.440]   David, the Conti gang, well-known ransomware gang, apparently is really hit
[02:16:37.440 --> 02:16:43.400]   Costa Rica hard and Costa Rica doesn't want to pay the ransom as they shouldn't.
[02:16:43.400 --> 02:16:47.360]   So the Conti gang said to the people of Costa Rica overthrow your government.
[02:16:47.360 --> 02:16:50.800]   When did that happen?
[02:16:50.800 --> 02:16:52.640]   Just last week.
[02:16:52.640 --> 02:16:56.040]   What was the aftermath?
[02:16:56.040 --> 02:16:58.600]   Well, I don't think they overthrew the government.
[02:16:58.880 --> 02:16:59.440]   Let me see.
[02:16:59.440 --> 02:16:59.880]   I don't know.
[02:16:59.880 --> 02:17:01.600]   What's what's
[02:17:01.600 --> 02:17:07.520]   Costa Conti shut down.
[02:17:07.520 --> 02:17:09.160]   Oh, interesting.
[02:17:09.160 --> 02:17:14.720]   The ransomware group appears to be offline, but the criminals behind and
[02:17:14.720 --> 02:17:15.640]   aren't going anywhere.
[02:17:15.640 --> 02:17:16.760]   This is from tech monitor.
[02:17:16.760 --> 02:17:19.920]   Conti has taken its infrastructure offline.
[02:17:19.920 --> 02:17:25.000]   This is as of two days ago after the after the show, members of the gang, which
[02:17:25.000 --> 02:17:28.840]   are currently engaged in a high profile ransomware campaign against Costa Rica.
[02:17:29.520 --> 02:17:35.000]   Are thought to be forming alliances with other smaller groups as a way of rebranding.
[02:17:35.000 --> 02:17:40.160]   You know, they probably went to V con and got some ideas and some marketing
[02:17:40.160 --> 02:17:40.720]   thoughts.
[02:17:40.720 --> 02:17:43.280]   That's another thing, by the way, if we didn't have cryptocurrency,
[02:17:43.280 --> 02:17:45.760]   ransomware wouldn't be so prevalent because.
[02:17:45.760 --> 02:17:48.600]   Yeah, well, that's it.
[02:17:48.600 --> 02:17:49.240]   No, it's interesting.
[02:17:49.240 --> 02:17:50.320]   You got a combination.
[02:17:50.320 --> 02:17:52.560]   Here's the whole thing with ransomware, why it's so popular.
[02:17:52.560 --> 02:17:58.360]   It's extremely low risk, extremely high reward and takes
[02:17:58.480 --> 02:18:02.640]   little to no skill to do it because now they have like ransomware as a service.
[02:18:02.640 --> 02:18:06.880]   So yeah, we're else in any other kind of crime.
[02:18:06.880 --> 02:18:10.160]   Any other crime, do you have that wonderful combination?
[02:18:10.160 --> 02:18:13.920]   Low risk, high reward, little skill.
[02:18:13.920 --> 02:18:20.160]   Effect, you'd have to be an idiot to rob a bank these days.
[02:18:20.160 --> 02:18:20.960]   Yes.
[02:18:20.960 --> 02:18:23.600]   Yes, because it's very high risk.
[02:18:23.600 --> 02:18:27.520]   The reward isn't nearly the level and it takes a lot of skill.
[02:18:27.880 --> 02:18:28.880]   Quite the opposite.
[02:18:28.880 --> 02:18:34.280]   I saw on Reddit last week a picture at a target or somewhere where they, you know,
[02:18:34.280 --> 02:18:37.720]   they have the gift card rack and a big handwritten sign saying.
[02:18:37.720 --> 02:18:43.360]   Do not buy and send these to whoever AT&T they are.
[02:18:43.360 --> 02:18:47.640]   They are not asking for gift cards there.
[02:18:47.640 --> 02:18:48.280]   But you know what?
[02:18:48.280 --> 02:18:50.880]   If if the world weren't full of suckers, I guess.
[02:18:50.880 --> 02:18:52.360]   So it's interesting.
[02:18:52.360 --> 02:18:57.600]   One of my hosts, Andy Ellis, who a former CISO for Akama,
[02:18:57.600 --> 02:19:02.720]   I feel like 20 years, he did this experiment where he tried to actually
[02:19:02.720 --> 02:19:07.520]   legitimately buy gift cards on multiple different sites.
[02:19:07.520 --> 02:19:12.760]   And astonishingly, he got rejected many times.
[02:19:12.760 --> 02:19:17.760]   So gift cards is a traditional technique for fishing to try to get money out of a,
[02:19:17.760 --> 02:19:22.360]   out of a company, you target certain individuals, you know, I often see level people.
[02:19:22.800 --> 02:19:29.120]   But even when you try to legitimately purchase gift cards, because this is such a common
[02:19:29.120 --> 02:19:32.200]   vector for theft, those get rejected.
[02:19:32.200 --> 02:19:33.280]   I didn't even know that.
[02:19:33.280 --> 02:19:33.720]   It's amazing.
[02:19:33.720 --> 02:19:34.720]   I had a credit card company.
[02:19:34.720 --> 02:19:35.800]   It was the credit card.
[02:19:35.800 --> 02:19:36.960]   Well, yes.
[02:19:36.960 --> 02:19:41.920]   I mean, but actually, I don't know where the source is coming from, but the,
[02:19:41.920 --> 02:19:48.160]   the different sites that are offering gift cards will often reject a legitimate purchase.
[02:19:48.160 --> 02:19:54.040]   Now more often if you sign up brand new, you know, you go to walmart.com for the very first
[02:19:54.040 --> 02:19:56.960]   time, try to get a gift card for the very first time.
[02:19:56.960 --> 02:20:00.240]   This is your first purchase that will probably be rejected.
[02:20:00.240 --> 02:20:01.480]   And that's what happened to Andy.
[02:20:01.480 --> 02:20:02.520]   Interesting.
[02:20:02.520 --> 02:20:03.280]   Wow.
[02:20:03.280 --> 02:20:07.600]   This is the new thing with the ransom.
[02:20:07.600 --> 02:20:10.040]   What was the ransomware group that disappeared?
[02:20:10.040 --> 02:20:11.200]   We thought, Oh, it's all over.
[02:20:11.200 --> 02:20:13.960]   And instead they just re re emerged rebranded.
[02:20:13.960 --> 02:20:17.560]   So this is, if it gets a little hot, they're all in Russia, by the way.
[02:20:17.760 --> 02:20:20.920]   If it gets a little hot, if Putin says, you're not giving me a big enough cut,
[02:20:20.920 --> 02:20:22.560]   they just disappear.
[02:20:22.560 --> 02:20:26.760]   They keep every all the software and everything and they're rebrand, get new
[02:20:26.760 --> 02:20:30.160]   servers, new name, resume operation.
[02:20:30.160 --> 02:20:33.320]   They run like a normal business.
[02:20:33.320 --> 02:20:36.400]   There is even customer service at these ransomware.
[02:20:36.400 --> 02:20:36.880]   Oh, yeah.
[02:20:36.880 --> 02:20:37.640]   Places.
[02:20:37.640 --> 02:20:38.080]   Yes.
[02:20:38.080 --> 02:20:41.360]   Hey, I bought a decryption code, but doesn't work.
[02:20:41.360 --> 02:20:42.160]   What do I do?
[02:20:42.160 --> 02:20:44.560]   What's your customer number?
[02:20:44.560 --> 02:20:45.000]   Yeah.
[02:20:45.000 --> 02:20:46.200]   That's incredible.
[02:20:46.400 --> 02:20:49.840]   It is, it is, it's a strange world we live in.
[02:20:49.840 --> 02:20:51.920]   Um,
[02:20:51.920 --> 02:20:54.960]   let's see.
[02:20:54.960 --> 02:20:55.320]   I don't know.
[02:20:55.320 --> 02:20:57.560]   So many stories we haven't gotten to.
[02:20:57.560 --> 02:21:01.600]   I don't want to let either of you go without recommending books because I got
[02:21:01.600 --> 02:21:02.320]   really smart.
[02:21:02.320 --> 02:21:03.240]   Oh, one.
[02:21:03.240 --> 02:21:04.560]   I was going to do that.
[02:21:04.560 --> 02:21:06.880]   OK, I got one fiction book.
[02:21:06.880 --> 02:21:07.360]   OK.
[02:21:07.360 --> 02:21:11.520]   And the reason I'm recommending it via audible is because the actor who does
[02:21:11.520 --> 02:21:13.360]   the read is spectacular.
[02:21:13.360 --> 02:21:13.920]   OK.
[02:21:14.400 --> 02:21:18.760]   And it's the, uh, Oh God, I think it's called hail project.
[02:21:18.760 --> 02:21:19.760]   Man, the project.
[02:21:19.760 --> 02:21:20.560]   Hail Mary.
[02:21:20.560 --> 02:21:21.200]   Project.
[02:21:21.200 --> 02:21:23.880]   I'm way ahead of you because we've not only read the book, but
[02:21:23.880 --> 02:21:28.120]   interviewed the author Andy Weir, guy who did the Martian and the guy who read
[02:21:28.120 --> 02:21:32.040]   it, Ray Porter, I completely agree with you is one of the great actor.
[02:21:32.040 --> 02:21:38.880]   He is in fact, OK, I'm going to turn tables on you because I am reading a series.
[02:21:38.880 --> 02:21:44.280]   I'm in volume four of something called the Bobber verse also narrated by
[02:21:44.280 --> 02:21:45.160]   Ray Porter.
[02:21:45.160 --> 02:21:48.440]   It's a Dennis E. Taylor science fiction series.
[02:21:48.440 --> 02:21:55.640]   And that snarky voice that he has is perfect for the premise of the premise of
[02:21:55.640 --> 02:21:59.280]   the Bobber verse is it's roughly modern times.
[02:21:59.280 --> 02:22:03.480]   Uh, guy sells his company gaming company for a lot of money.
[02:22:03.480 --> 02:22:06.600]   First thing he does is sign of the dotted line to have his head frozen.
[02:22:06.600 --> 02:22:10.360]   If anything should happen to him because he's got money immediately goes out the
[02:22:10.360 --> 02:22:12.400]   door, it gets run over and killed.
[02:22:13.160 --> 02:22:15.480]   Wakes wakes up a hundred years later.
[02:22:15.480 --> 02:22:17.560]   He's this is what we were talking about.
[02:22:17.560 --> 02:22:19.240]   Yeah, he's in a box.
[02:22:19.240 --> 02:22:23.920]   He is an AGI, but he has all his memories, his personality.
[02:22:23.920 --> 02:22:30.280]   Uh, and it's in, and then of course, uh, he clones himself and then now
[02:22:30.280 --> 02:22:31.560]   hundreds of Bob's.
[02:22:31.560 --> 02:22:33.560]   It's really a great story.
[02:22:33.560 --> 02:22:34.800]   And listen to this guy's voice.
[02:22:34.800 --> 02:22:36.720]   Ray Porter is so good.
[02:22:36.720 --> 02:22:40.520]   I tried to respond, but what came out was something like a cross between a
[02:22:40.520 --> 02:22:45.560]   cough and static, for God's sake, that sounded like a voice synthesizer, having
[02:22:45.560 --> 02:22:46.120]   a breakdown.
[02:22:46.120 --> 02:22:50.600]   Freud put down the tablet, leaned forward and rested his arms on the desk or
[02:22:50.600 --> 02:22:54.400]   that's that same voice is in Project Hail Mary completely agree with you.
[02:22:54.400 --> 02:22:55.240]   It's great actor.
[02:22:55.240 --> 02:22:58.360]   I, I, half of the enjoyment was how good he did that.
[02:22:58.360 --> 02:22:59.720]   I think he works.
[02:22:59.720 --> 02:23:04.200]   He's, I think he's on contract to audible because Project Hail Mary and this
[02:23:04.200 --> 02:23:06.920]   were both done as part of audible originals.
[02:23:06.920 --> 02:23:10.360]   So I have a feeling that he is like he's got, he's got an in there.
[02:23:10.680 --> 02:23:11.240]   With audible.
[02:23:11.240 --> 02:23:11.600]   Yeah.
[02:23:11.600 --> 02:23:12.760]   Great recommendation though.
[02:23:12.760 --> 02:23:15.440]   If you haven't read Project Hail Mary, you must end.
[02:23:15.440 --> 02:23:16.360]   And I put them both on.
[02:23:16.360 --> 02:23:16.960]   I haven't yet.
[02:23:16.960 --> 02:23:19.360]   Oh, Boba versus just fun.
[02:23:19.360 --> 02:23:24.320]   And, and Ray Porter has a long list of books that he's read, uh, that I, you
[02:23:24.320 --> 02:23:26.440]   know, some nonfiction actually, which is interesting.
[02:23:26.440 --> 02:23:30.520]   Uh, I think this is one of the things that happens with an audio book is you
[02:23:30.520 --> 02:23:32.760]   start to like the reader ghost in the wires.
[02:23:32.760 --> 02:23:33.760]   I saw you go by.
[02:23:33.760 --> 02:23:37.120]   I, I, I listened to that ghost in the wires.
[02:23:37.120 --> 02:23:37.600]   Okay.
[02:23:37.600 --> 02:23:39.080]   That's the Kevin Mintnick book.
[02:23:39.080 --> 02:23:39.720]   Oh, yeah.
[02:23:39.720 --> 02:23:40.240]   Yeah.
[02:23:40.240 --> 02:23:42.400]   Of course we know Kevin very well.
[02:23:42.400 --> 02:23:49.880]   Uh, Amy now I, now I got to show you before you say anything, I have a very
[02:23:49.880 --> 02:23:54.360]   lengthy wish list with some very good books in it that I only get one credit.
[02:23:54.360 --> 02:23:55.360]   Now I've already used it.
[02:23:55.360 --> 02:24:00.160]   Uh, the first one on cultist cult-ish cult-ish.
[02:24:00.160 --> 02:24:04.160]   Exploding the phone, the untoward story of the teenagers and outlaws who hacked
[02:24:04.160 --> 02:24:04.840]   my bell.
[02:24:04.840 --> 02:24:08.160]   Uh, yeah, doesn't that sound good?
[02:24:08.360 --> 02:24:10.840]   I got a couple of novels, Louise Erdic is newest.
[02:24:10.840 --> 02:24:12.360]   Uh, I love Amor tolls.
[02:24:12.360 --> 02:24:14.240]   His newest, the Lincoln highway.
[02:24:14.240 --> 02:24:18.840]   Uh, Anthony Dore has written a book called cloud cuckoo land.
[02:24:18.840 --> 02:24:19.240]   I don't know.
[02:24:19.240 --> 02:24:19.760]   We'll see.
[02:24:19.760 --> 02:24:24.600]   Um, because internet from Gretchen McCulloch, understanding the new rules of
[02:24:24.600 --> 02:24:30.440]   language, I'm pretty close to hitting the buy on that one on tyranny lessons
[02:24:30.440 --> 02:24:33.120]   from the 20th century to depressing.
[02:24:33.120 --> 02:24:34.200]   So what do you recommend?
[02:24:34.200 --> 02:24:37.200]   Uh, I can give you a couple.
[02:24:37.200 --> 02:24:38.600]   I don't know if they're on audible or not.
[02:24:38.600 --> 02:24:40.720]   No, I don't have to be audible just to read anything.
[02:24:40.720 --> 02:24:41.360]   I don't know.
[02:24:41.360 --> 02:24:41.760]   Right.
[02:24:41.760 --> 02:24:43.600]   This is not an audible ad by the way.
[02:24:43.600 --> 02:24:46.120]   This is just just looking for stuff to read.
[02:24:46.120 --> 02:24:47.800]   I'll give you three.
[02:24:47.800 --> 02:24:48.200]   Okay.
[02:24:48.200 --> 02:24:51.760]   Um, the first one is the nineties.
[02:24:51.760 --> 02:24:53.240]   So I'm a Gen Xer.
[02:24:53.240 --> 02:24:58.040]   Um, so Chuck Klosterman wrote this book and it's sort of like what happened in
[02:24:58.040 --> 02:25:00.600]   the nineties that set the stage for today and what preceded it.
[02:25:00.600 --> 02:25:05.920]   Oh, um, I think he got quite a bit wrong, uh, including some stuff, having to do
[02:25:05.920 --> 02:25:10.080]   with like sound garden, uh, which I think people know, I'm who know me, know
[02:25:10.080 --> 02:25:10.800]   I'm obsessed with.
[02:25:10.800 --> 02:25:14.280]   He got some details, like wrong on some of these things.
[02:25:14.280 --> 02:25:18.800]   I think I know you pretty well and I did not know you have so many obsessions.
[02:25:18.800 --> 02:25:20.640]   I didn't know sound gardens.
[02:25:20.640 --> 02:25:23.000]   My favorite, uh, I love rush.
[02:25:23.000 --> 02:25:24.880]   I think everybody listens to the show.
[02:25:24.880 --> 02:25:27.400]   It doesn't look like it is like quote a little too much.
[02:25:27.400 --> 02:25:30.640]   Um, now you played the rush pinball machine, by the way.
[02:25:30.640 --> 02:25:31.280]   Have you played it?
[02:25:31.280 --> 02:25:34.840]   Oh, now I'm not a fan of pinball machine by Stern.
[02:25:35.080 --> 02:25:36.200]   It is excellent.
[02:25:36.200 --> 02:25:38.720]   Highly recommend this Getty Lee.
[02:25:38.720 --> 02:25:39.120]   My hand.
[02:25:39.120 --> 02:25:40.760]   I coordination, not great.
[02:25:40.760 --> 02:25:47.000]   Uh, not great, but, uh, anyhow, that book is a fun cause it's nostalgic,
[02:25:47.000 --> 02:25:51.360]   but also will be a little infuriating, but I've kind of enjoyed like hate reading.
[02:25:51.360 --> 02:25:57.400]   And did you ever read the book in 1968, which was actually a classic work about.
[02:25:57.400 --> 02:26:01.200]   A year that everything changed dramatically.
[02:26:01.200 --> 02:26:01.360]   Yeah.
[02:26:01.360 --> 02:26:01.720]   Yeah.
[02:26:01.720 --> 02:26:02.920]   Um, similar idea.
[02:26:02.920 --> 02:26:03.360]   Have you read it?
[02:26:03.360 --> 02:26:04.960]   Yeah, it's really, really good.
[02:26:04.960 --> 02:26:05.280]   Highly.
[02:26:05.280 --> 02:26:07.400]   You feel like it's like, okay, I'll put that on my list.
[02:26:07.400 --> 02:26:11.680]   Well, but you know, I don't know if you would hate read it, but I thought I was quite good.
[02:26:11.680 --> 02:26:16.680]   Yeah, sometimes, you know, like you, you just like, I don't know, like I'm having a dialogue in my head.
[02:26:16.680 --> 02:26:19.520]   Well, it was, it was a super fast read and he's a pretty good writer.
[02:26:19.520 --> 02:26:20.640]   You're so smart.
[02:26:20.640 --> 02:26:24.680]   I get every book you read, you go like you have this dialogue as you're reading it.
[02:26:24.680 --> 02:26:29.360]   Um, I mean, I try to, um, I try to spend a couple hours a day reading and I try to get
[02:26:29.360 --> 02:26:31.360]   through a book every couple of days or so.
[02:26:31.360 --> 02:26:35.360]   Cause I, you know, um, anyways, yes.
[02:26:35.360 --> 02:26:37.240]   And so I usually, I, I'm an active reader.
[02:26:37.240 --> 02:26:39.520]   So I tend to write while I'm reading notes.
[02:26:39.520 --> 02:26:40.920]   Oh, yeah.
[02:26:40.920 --> 02:26:43.080]   Do you do the Z Z Edelkasten thing or?
[02:26:43.080 --> 02:26:45.760]   I do the Amy Web thing.
[02:26:45.760 --> 02:26:46.000]   Yeah.
[02:26:46.000 --> 02:26:51.600]   Just this is, this is all the rage now with nerds is, uh, is, you know, make little slips of paper
[02:26:51.600 --> 02:26:58.360]   or digital versions of either of you use those apps blinched or a headway to do those sort of
[02:26:58.360 --> 02:27:00.320]   abbreviated versions of book.
[02:27:00.320 --> 02:27:00.960]   I,
[02:27:01.200 --> 02:27:06.000]   I use blank, which I, or blinks, which I like a lot, but I, I treat that more as like a
[02:27:06.000 --> 02:27:07.000]   cliff's notes version.
[02:27:07.000 --> 02:27:07.680]   And if the,
[02:27:07.680 --> 02:27:08.000]   Right.
[02:27:08.000 --> 02:27:08.560]   Exactly.
[02:27:08.560 --> 02:27:08.800]   Yeah.
[02:27:08.800 --> 02:27:10.960]   At least a very long time for me to show.
[02:27:10.960 --> 02:27:14.960]   Audible has like 15 minute versions of every book.
[02:27:14.960 --> 02:27:16.800]   And I feel like that's really cheating.
[02:27:16.800 --> 02:27:20.560]   The problem I have with blinks is they keep tweeting in my tweet storm.
[02:27:20.560 --> 02:27:22.720]   The nine books Elon Musk read that.
[02:27:22.720 --> 02:27:23.760]   And I don't know.
[02:27:23.760 --> 02:27:24.640]   I'm not.
[02:27:24.640 --> 02:27:27.120]   So I treat it more like, um,
[02:27:28.800 --> 02:27:32.560]   so it takes me a long time to dry my hair in the morning and I have to hang my head up,
[02:27:32.560 --> 02:27:33.440]   sign down to do it.
[02:27:33.440 --> 02:27:35.360]   So I read blinks.
[02:27:35.360 --> 02:27:38.080]   I read the blinks books while I'm doing that.
[02:27:38.080 --> 02:27:40.960]   Nobody tell Amy about towels and towel heads.
[02:27:40.960 --> 02:27:41.200]   Okay.
[02:27:41.200 --> 02:27:41.520]   Go ahead.
[02:27:41.520 --> 02:27:46.240]   It's like, there's a lot of, you know, washing every morning, do you?
[02:27:46.240 --> 02:27:46.960]   No, I don't.
[02:27:46.960 --> 02:27:48.800]   I just know, you need to shower.
[02:27:48.800 --> 02:27:50.400]   At any rate, but then it would kind of read.
[02:27:50.400 --> 02:27:50.960]   You read.
[02:27:50.960 --> 02:27:51.200]   Okay.
[02:27:51.200 --> 02:27:55.360]   My point is if I like, can we do a whole show on Amy's hair, retching?
[02:27:55.360 --> 02:27:56.160]   Yeah.
[02:27:56.160 --> 02:27:56.720]   Let's do it.
[02:27:57.360 --> 02:27:58.000]   Bad in AI.
[02:27:58.000 --> 02:27:59.760]   That's groovy.
[02:27:59.760 --> 02:28:00.800]   It's a sample.
[02:28:00.800 --> 02:28:01.920]   It's a sample.
[02:28:01.920 --> 02:28:04.480]   And then if I like what's there, I read the rest of the book.
[02:28:04.480 --> 02:28:05.440]   Like I read the whole book.
[02:28:05.440 --> 02:28:05.600]   Yeah.
[02:28:05.600 --> 02:28:06.960]   I don't use it as a substitute.
[02:28:06.960 --> 02:28:07.520]   Just if I like it.
[02:28:07.520 --> 02:28:08.640]   Or I use it as like a.
[02:28:08.640 --> 02:28:10.400]   I did that with being mortal.
[02:28:10.400 --> 02:28:15.280]   I tool go one day's amazing book, which I highly recommend about
[02:28:15.280 --> 02:28:17.920]   gerontology and, uh, end of life.
[02:28:17.920 --> 02:28:21.520]   And I listened to the short version and then I said, this is so good.
[02:28:21.520 --> 02:28:22.240]   I read the whole thing.
[02:28:22.240 --> 02:28:22.400]   Yeah.
[02:28:22.400 --> 02:28:23.040]   It's really good.
[02:28:23.040 --> 02:28:23.600]   He's a wonderful.
[02:28:23.600 --> 02:28:24.000]   Right.
[02:28:24.000 --> 02:28:29.440]   Anyway, the 90s, Pull Earth by John Markoff, which is about.
[02:28:29.440 --> 02:28:31.520]   I read everything John's a good friend of the show.
[02:28:31.520 --> 02:28:32.400]   And he's been on our show.
[02:28:32.400 --> 02:28:33.680]   He's a wonderful writer.
[02:28:33.680 --> 02:28:34.160]   What's his new one?
[02:28:34.160 --> 02:28:34.560]   Great.
[02:28:34.560 --> 02:28:36.480]   So it's called Whole Earth.
[02:28:36.480 --> 02:28:37.440]   Oh, yeah.
[02:28:37.440 --> 02:28:39.760]   It's all about Stuart Brand.
[02:28:39.760 --> 02:28:39.920]   Yeah.
[02:28:39.920 --> 02:28:40.080]   Yeah.
[02:28:40.080 --> 02:28:42.320]   I've got to read that because I know Stuart as well.
[02:28:42.320 --> 02:28:44.240]   I was a regular on the well.
[02:28:44.240 --> 02:28:45.440]   Yeah.
[02:28:45.440 --> 02:28:45.760]   Yeah.
[02:28:45.760 --> 02:28:46.560]   No, I know.
[02:28:46.560 --> 02:28:47.440]   I want to read all of it.
[02:28:47.440 --> 02:28:47.920]   Definitely.
[02:28:47.920 --> 02:28:48.720]   It's on my list.
[02:28:48.720 --> 02:28:48.960]   Yeah.
[02:28:48.960 --> 02:28:49.680]   The many lines.
[02:28:49.680 --> 02:28:50.480]   I think those are good.
[02:28:50.480 --> 02:28:51.280]   Stuart Brand.
[02:28:51.280 --> 02:28:51.680]   Okay.
[02:28:51.680 --> 02:28:52.320]   That's a good one.
[02:28:52.320 --> 02:28:53.440]   I'm going to put that one on my list.
[02:28:53.440 --> 02:28:56.400]   And then for like super dense, heavier reading,
[02:28:56.400 --> 02:29:00.880]   but still I think a great book to have is The Ideas That Created the Future by Harry Lewis,
[02:29:00.880 --> 02:29:05.200]   which is a collection of the most important academic papers that got written that created the
[02:29:05.200 --> 02:29:06.560]   technology we use today.
[02:29:06.560 --> 02:29:08.000]   So it's like good to have on your desk.
[02:29:08.000 --> 02:29:10.080]   Is that an older book?
[02:29:10.080 --> 02:29:11.520]   Yeah.
[02:29:11.520 --> 02:29:14.400]   I've got it over on my shelf, on that shelf over there.
[02:29:14.400 --> 02:29:15.120]   It's a, yeah.
[02:29:15.120 --> 02:29:18.640]   It's the kind of book that I'd put on the shelf, not read,
[02:29:18.640 --> 02:29:21.600]   but put on the shelf to show that I have intellectual heft.
[02:29:22.400 --> 02:29:24.080]   Um, no, I read it.
[02:29:24.080 --> 02:29:27.520]   And it's again, like it's with all this stuff happening right now with,
[02:29:27.520 --> 02:29:28.960]   I should read this.
[02:29:28.960 --> 02:29:30.160]   And some of their, honestly.
[02:29:30.160 --> 02:29:34.160]   You can go back and like read piece together how we got to now.
[02:29:34.160 --> 02:29:35.280]   I just, I think it's useful.
[02:29:35.280 --> 02:29:37.360]   It's not like a fun weekend reading.
[02:29:37.360 --> 02:29:42.720]   46 classic papers in computer science, although since it has Aristotle and Leibniz in it,
[02:29:42.720 --> 02:29:47.680]   it's not exactly all computer science, but, uh, Norbert Wiener, Gordon Moore.
[02:29:47.680 --> 02:29:48.880]   Oh, I have to, you know, what?
[02:29:48.880 --> 02:29:53.520]   I mean, you probably already read most of those, but I think it's just useful to have them in one spot.
[02:29:53.520 --> 02:29:54.240]   You know, and sort of.
[02:29:54.240 --> 02:29:54.480]   Yeah.
[02:29:54.480 --> 02:29:55.520]   No, I'm going to order this.
[02:29:55.520 --> 02:29:56.720]   Yeah.
[02:29:56.720 --> 02:29:59.680]   This is not what you would call light listening for audible.com.
[02:29:59.680 --> 02:30:01.600]   This is, no, I wouldn't listen.
[02:30:01.600 --> 02:30:02.720]   I wouldn't want to listen to that.
[02:30:02.720 --> 02:30:03.200]   No.
[02:30:03.200 --> 02:30:07.920]   And then also I am definitely going to buy the rush pinball game.
[02:30:07.920 --> 02:30:12.800]   It's a great game.
[02:30:12.800 --> 02:30:16.080]   Oh, you don't have to have borders anymore.
[02:30:16.080 --> 02:30:17.120]   You can pay with your phone.
[02:30:17.120 --> 02:30:18.400]   That's, I, I'm.
[02:30:18.400 --> 02:30:18.960]   Yes.
[02:30:18.960 --> 02:30:20.320]   I got to find somewhere.
[02:30:20.320 --> 02:30:21.840]   There's an app called pay range.
[02:30:21.840 --> 02:30:24.080]   Where you pay via phone.
[02:30:24.080 --> 02:30:26.800]   We got to stop the sound or, uh, or Getty Lees Estates.
[02:30:26.800 --> 02:30:27.360]   Can I, uh,
[02:30:27.360 --> 02:30:30.000]   I feel like Getty's cool.
[02:30:30.000 --> 02:30:32.400]   I don't feel like they would know Neil Pertz Estate maybe though.
[02:30:32.400 --> 02:30:32.640]   Yeah.
[02:30:32.640 --> 02:30:34.880]   No, I love rush.
[02:30:34.880 --> 02:30:36.800]   And now, you know what?
[02:30:36.800 --> 02:30:37.920]   Can I confess something?
[02:30:37.920 --> 02:30:41.520]   I don't think I could recognize a single sound garden song.
[02:30:41.520 --> 02:30:42.720]   So I'm going to have to.
[02:30:42.720 --> 02:30:43.280]   I'm sorry.
[02:30:43.280 --> 02:30:45.440]   There is no sound garden pinball machine.
[02:30:45.440 --> 02:30:45.680]   No.
[02:30:45.680 --> 02:30:47.360]   Sound garden.
[02:30:47.840 --> 02:30:49.040]   Do they have some hits?
[02:30:49.040 --> 02:30:51.520]   No, here's the thing that, yeah, the good.
[02:30:51.520 --> 02:30:53.440]   Black hole, son.
[02:30:53.440 --> 02:30:53.600]   Black hole, son.
[02:30:53.600 --> 02:30:56.720]   But my, my favorite song of the two favorites.
[02:30:56.720 --> 02:30:58.560]   Outshined, which is wonderful.
[02:30:58.560 --> 02:31:03.360]   And also Rusty Cage, which is a very technically complicated song to play.
[02:31:03.360 --> 02:31:06.320]   But, um, Johnny Cash covered it.
[02:31:06.320 --> 02:31:09.280]   Oh, and it's a wonderful, crazy, weird, interesting.
[02:31:09.280 --> 02:31:10.480]   That sounds really interesting.
[02:31:10.480 --> 02:31:12.800]   I saw them live in Chicago years ago.
[02:31:12.800 --> 02:31:13.200]   Did you?
[02:31:13.200 --> 02:31:17.440]   Chris Cornell died like it was five years ago this past week.
[02:31:17.440 --> 02:31:18.160]   I remember that.
[02:31:18.160 --> 02:31:18.800]   Pre-sense.
[02:31:18.800 --> 02:31:19.280]   Yeah, yeah.
[02:31:19.280 --> 02:31:21.040]   All right.
[02:31:21.040 --> 02:31:22.880]   Go play the rush pinball machine, Leo.
[02:31:22.880 --> 02:31:23.200]   Yeah.
[02:31:23.200 --> 02:31:26.560]   While listening to outshined by sound garden.
[02:31:26.560 --> 02:31:27.680]   Yes.
[02:31:27.680 --> 02:31:28.240]   With headphones.
[02:31:28.240 --> 02:31:28.400]   I love that.
[02:31:28.400 --> 02:31:29.360]   It's a good song.
[02:31:29.360 --> 02:31:29.840]   Yeah.
[02:31:29.840 --> 02:31:31.520]   Here's another recommendation for you.
[02:31:31.520 --> 02:31:31.760]   Yes.
[02:31:31.760 --> 02:31:36.000]   Not book the pinball map, pinballmap.com.
[02:31:36.000 --> 02:31:37.120]   You can also get into the map.
[02:31:37.120 --> 02:31:40.640]   You can find where all the pinball machines are located near you.
[02:31:40.640 --> 02:31:41.280]   Oh.
[02:31:41.280 --> 02:31:43.200]   So I could find that rush pinball game.
[02:31:43.200 --> 02:31:46.000]   You could find you can, you can literally look up rush pinball
[02:31:46.000 --> 02:31:49.520]   and find where what bar arcade, whatever,
[02:31:49.520 --> 02:31:51.120]   has a rush pinball machine near you.
[02:31:51.120 --> 02:31:52.800]   Oh my god.
[02:31:52.800 --> 02:31:54.160]   It's a great, great app.
[02:31:54.160 --> 02:31:56.640]   Let me just quickly type in a rush pinball.
[02:31:56.640 --> 02:31:57.440]   Just type in rush.
[02:31:57.440 --> 02:31:58.080]   Just type in rush.
[02:31:58.080 --> 02:31:59.360]   Yeah, because it's all pinball.
[02:31:59.360 --> 02:32:00.160]   You don't, the pinball.
[02:32:00.160 --> 02:32:00.720]   It's all pinball.
[02:32:00.720 --> 02:32:04.560]   That would be, I'll just search for that here.
[02:32:04.560 --> 02:32:07.280]   Not found.
[02:32:07.280 --> 02:32:08.080]   No, it's somewhere.
[02:32:08.080 --> 02:32:09.040]   It's got to be.
[02:32:09.040 --> 02:32:10.000]   No, I'm sure it is.
[02:32:10.000 --> 02:32:13.120]   That's probably I'm typing the wrong thing.
[02:32:13.120 --> 02:32:15.040]   No, well, rush.
[02:32:15.040 --> 02:32:18.800]   Is it finding the machines in your area?
[02:32:18.800 --> 02:32:20.080]   Rush Pro by Stern.
[02:32:20.080 --> 02:32:20.960]   You're typing it in.
[02:32:20.960 --> 02:32:21.600]   That's the one I want.
[02:32:21.600 --> 02:32:22.640]   Rush Pro by Stern.
[02:32:22.640 --> 02:32:24.800]   I think I just didn't qualify it sufficiently.
[02:32:24.800 --> 02:32:25.920]   Look, it's all over.
[02:32:25.920 --> 02:32:27.360]   Now you can end down zero.
[02:32:27.360 --> 02:32:30.400]   Zero in on Petaloma or Carlsby.
[02:32:30.400 --> 02:32:36.640]   Oh, look at this pinball death valley between San Francisco.
[02:32:36.640 --> 02:32:38.800]   You're only looking up just the one machine.
[02:32:38.800 --> 02:32:40.320]   Oh, that's really looking up the one machine.
[02:32:40.320 --> 02:32:41.680]   I might have to go down to the city.
[02:32:41.680 --> 02:32:44.720]   Oh, I know exactly that location in San Francisco.
[02:32:44.720 --> 02:32:45.360]   Uptown.
[02:32:45.360 --> 02:32:47.120]   Oh, I know the Uptown.
[02:32:47.120 --> 02:32:48.240]   Been there many times.
[02:32:48.240 --> 02:32:49.520]   Oh, no, I thought it was something up.
[02:32:49.520 --> 02:32:51.200]   Oh, no, that's not the true hill.
[02:32:51.200 --> 02:32:52.400]   It's like over 50 feet.
[02:32:52.400 --> 02:32:54.720]   Uptown is a great bar on Cap Street.
[02:32:54.720 --> 02:32:55.920]   You just go there all the time.
[02:32:55.920 --> 02:32:58.560]   Okay, very excited.
[02:32:58.560 --> 02:33:01.360]   Go play it.
[02:33:01.360 --> 02:33:04.480]   You never know what you're going to learn on this show.
[02:33:04.480 --> 02:33:07.680]   Amy Webb, you are fantastic.
[02:33:07.680 --> 02:33:09.120]   Enjoy your gravel bike.
[02:33:09.120 --> 02:33:11.440]   I will.
[02:33:11.440 --> 02:33:12.160]   Thank you, Leo.
[02:33:12.160 --> 02:33:13.120]   You are fantastic.
[02:33:13.120 --> 02:33:13.680]   Oh, no, no.
[02:33:13.680 --> 02:33:14.160]   I love that.
[02:33:14.160 --> 02:33:14.720]   And I do.
[02:33:14.720 --> 02:33:17.680]   I noticed that jacket right away because I haven't seen you.
[02:33:17.680 --> 02:33:18.480]   It's not typical.
[02:33:18.480 --> 02:33:19.360]   It's wonderful on you.
[02:33:19.360 --> 02:33:20.480]   Yeah, it's wonderful.
[02:33:20.480 --> 02:33:22.400]   I thought I wear it with a black shirt.
[02:33:22.400 --> 02:33:23.840]   Yeah, it's a pretty cooler.
[02:33:23.840 --> 02:33:24.320]   Thank you.
[02:33:24.320 --> 02:33:25.280]   Yeah, I like it a lot.
[02:33:25.280 --> 02:33:28.080]   You're trying to make all of us break out in hives.
[02:33:28.080 --> 02:33:30.640]   I am now.
[02:33:30.640 --> 02:33:32.960]   You can't tell I am hiving.
[02:33:32.960 --> 02:33:36.800]   No, I love that color on you.
[02:33:36.800 --> 02:33:37.360]   It looks good.
[02:33:37.360 --> 02:33:41.200]   The book is the Genesis machine.
[02:33:42.240 --> 02:33:43.920]   Definitely, if you've not read this yet,
[02:33:43.920 --> 02:33:45.120]   I know many of you have,
[02:33:45.120 --> 02:33:46.560]   because we had a great triangulation,
[02:33:46.560 --> 02:33:47.840]   which you can go back and listen to.
[02:33:47.840 --> 02:33:49.600]   Amy Webb, Andrew Hassel,
[02:33:49.600 --> 02:33:51.360]   if you haven't read it yet, get it.
[02:33:51.360 --> 02:33:55.520]   It is all about the future of biotech,
[02:33:55.520 --> 02:33:57.840]   synthetic, what you call synthetic biology.
[02:33:57.840 --> 02:33:58.880]   And it's fun.
[02:33:58.880 --> 02:34:02.560]   It's a light and easy read with a lot of heavy information
[02:34:02.560 --> 02:34:03.280]   and ideas.
[02:34:03.280 --> 02:34:03.840]   I love it.
[02:34:03.840 --> 02:34:04.960]   Thanks.
[02:34:04.960 --> 02:34:07.200]   And there's actually five, I think,
[02:34:07.200 --> 02:34:09.680]   actors who read that book because the middle section
[02:34:09.680 --> 02:34:11.440]   are scenarios about the future.
[02:34:11.440 --> 02:34:12.080]   Yeah.
[02:34:12.080 --> 02:34:15.360]   One is acted out by a different actor.
[02:34:15.360 --> 02:34:16.960]   It's kind of cool the way they did it.
[02:34:16.960 --> 02:34:18.800]   And then the Tech Trends Report is out.
[02:34:18.800 --> 02:34:21.680]   You can go to futuretodayinstitute.com.
[02:34:21.680 --> 02:34:23.200]   That's Amy's company.
[02:34:23.200 --> 02:34:25.520]   Still haven't got the front page fixed, huh?
[02:34:25.520 --> 02:34:28.400]   Yeah, I broke the code pretty badly.
[02:34:28.400 --> 02:34:33.520]   Ben, I destroyed a couple of other plugins and scripts.
[02:34:33.520 --> 02:34:35.600]   And we're going to get the whole thing and start over
[02:34:35.600 --> 02:34:36.320]   a couple of weeks.
[02:34:36.320 --> 02:34:38.400]   Don't be messing with the code, Amy.
[02:34:38.400 --> 02:34:38.800]   I know.
[02:34:38.800 --> 02:34:39.520]   Are you a coder?
[02:34:39.520 --> 02:34:40.000]   Not my job.
[02:34:40.000 --> 02:34:41.040]   Or you just thought--
[02:34:41.040 --> 02:34:43.920]   Yeah, I mean, sometimes I'm just like,
[02:34:43.920 --> 02:34:45.920]   it'll just be faster if I do it rather than--
[02:34:45.920 --> 02:34:46.000]   Yeah.
[02:34:46.000 --> 02:34:47.280]   It's a delegation issue.
[02:34:47.280 --> 02:34:48.400]   I should not have--
[02:34:48.400 --> 02:34:48.880]   No.
[02:34:48.880 --> 02:34:49.680]   I have a team.
[02:34:49.680 --> 02:34:51.200]   I just screwing around and--
[02:34:51.200 --> 02:34:51.440]   No.
[02:34:51.440 --> 02:34:52.160]   There you go.
[02:34:52.160 --> 02:34:52.640]   It's your site.
[02:34:52.640 --> 02:34:55.040]   You get to screw around the 15th anniversary.
[02:34:55.040 --> 02:35:01.920]   Tech Trends Report is free for download at futuretodayinstitute.com.
[02:35:01.920 --> 02:35:06.320]   And of course, Amy is available for consulting your Fortune 500
[02:35:06.320 --> 02:35:10.400]   firm and steering it in the right direction.
[02:35:10.400 --> 02:35:12.720]   Steering it into the flames of hell.
[02:35:12.720 --> 02:35:13.200]   No.
[02:35:13.200 --> 02:35:14.000]   You're post-unliptic.
[02:35:14.000 --> 02:35:14.800]   Hell's stations.
[02:35:14.800 --> 02:35:15.200]   No.
[02:35:15.200 --> 02:35:16.240]   That's bad marketing.
[02:35:16.240 --> 02:35:17.040]   No.
[02:35:17.040 --> 02:35:18.560]   No.
[02:35:18.560 --> 02:35:20.960]   All right, we violated David's full thing about marketing.
[02:35:20.960 --> 02:35:26.560]   David, it's so great to see my old friend David Spar.
[02:35:26.560 --> 02:35:28.160]   Awesome to see you, Leo.
[02:35:28.160 --> 02:35:32.560]   It's been many, many years since we first worked at the Tech TV days.
[02:35:32.560 --> 02:35:34.400]   Now that you've returned from the dark side,
[02:35:34.400 --> 02:35:38.080]   we'll have you on more CISO series.
[02:35:38.080 --> 02:35:39.200]   CISO series.
[02:35:39.200 --> 02:35:41.440]   The Australians can call it CISO series.
[02:35:41.440 --> 02:35:42.240]   That's OK.
[02:35:42.240 --> 02:35:44.960]   It's CISO series.com.
[02:35:44.960 --> 02:35:48.160]   C-I-S-O-S-E-R-I-E-S.com.
[02:35:48.160 --> 02:35:51.600]   That's his new endeavor and great.
[02:35:51.600 --> 02:35:55.200]   Hey, what I recommend is you see the woman sort of cleaning there
[02:35:55.200 --> 02:35:56.640]   at the top, the little logo.
[02:35:56.640 --> 02:35:59.360]   That's-- we do a really fun event every Friday.
[02:35:59.360 --> 02:35:59.920]   Yeah.
[02:35:59.920 --> 02:36:02.080]   And just go go to the-- you click on that.
[02:36:02.080 --> 02:36:03.600]   You can register for that.
[02:36:03.600 --> 02:36:05.600]   Actually, we're not doing this Friday because of the holiday weekend,
[02:36:05.600 --> 02:36:07.600]   but that's the following Friday, June 3rd.
[02:36:07.600 --> 02:36:10.640]   And we just have a great community.
[02:36:10.640 --> 02:36:11.680]   There are a ton of fun.
[02:36:11.680 --> 02:36:15.760]   You know, one of the things I love about the cybersecurity community,
[02:36:15.760 --> 02:36:17.280]   that a lot of people realize,
[02:36:17.280 --> 02:36:19.040]   they're actually extraordinarily funny people.
[02:36:19.040 --> 02:36:19.760]   I do know that.
[02:36:19.760 --> 02:36:20.480]   Really, really funny.
[02:36:20.480 --> 02:36:21.440]   I do know that Wendy.
[02:36:21.440 --> 02:36:23.120]   I did a panel with Wendy Nather.
[02:36:23.120 --> 02:36:23.760]   She was great.
[02:36:23.760 --> 02:36:24.640]   She's awesome.
[02:36:24.640 --> 02:36:25.040]   Love her.
[02:36:25.040 --> 02:36:25.840]   I adore Wendy.
[02:36:25.840 --> 02:36:26.080]   Yeah.
[02:36:26.080 --> 02:36:27.520]   I've interviewed her many times.
[02:36:27.520 --> 02:36:28.240]   She's the best.
[02:36:28.240 --> 02:36:31.200]   She's the one who coined the term.
[02:36:33.280 --> 02:36:35.200]   Cyber-- the security poverty level.
[02:36:35.200 --> 02:36:36.480]   Did you talk about that?
[02:36:36.480 --> 02:36:36.880]   Maybe.
[02:36:36.880 --> 02:36:37.760]   That sounds familiar.
[02:36:37.760 --> 02:36:38.320]   Yeah.
[02:36:38.320 --> 02:36:39.360]   She coined that term.
[02:36:39.360 --> 02:36:40.080]   And it's a great term.
[02:36:40.080 --> 02:36:42.000]   And just very briefly I'll mention,
[02:36:42.000 --> 02:36:46.000]   it's about what the bare, bare minimum you need
[02:36:46.000 --> 02:36:50.560]   to just be at sort of the poverty level of having a security level.
[02:36:50.560 --> 02:36:53.360]   And that is-- it's a really important concept.
[02:36:53.360 --> 02:36:54.160]   No, it is.
[02:36:54.160 --> 02:36:54.400]   Yeah.
[02:36:54.400 --> 02:36:55.440]   We have to do full--
[02:36:55.440 --> 02:36:55.920]   Yeah.
[02:36:55.920 --> 02:36:56.080]   Yeah.
[02:36:56.080 --> 02:36:56.080]   Yeah.
[02:36:56.080 --> 02:36:56.080]   Yeah.
[02:36:56.080 --> 02:36:56.800]   At least do that.
[02:36:56.800 --> 02:36:57.280]   At least do that.
[02:36:57.280 --> 02:36:58.000]   Security poverty level.
[02:36:58.000 --> 02:36:58.560]   Yeah, yeah, yeah.
[02:36:58.560 --> 02:36:59.600]   Yeah.
[02:36:59.600 --> 02:36:59.840]   Yeah.
[02:36:59.840 --> 02:37:01.920]   We-- Amy, I'm her brother.
[02:37:01.920 --> 02:37:06.000]   Wendy and I and a few others did a event at South
[02:37:06.000 --> 02:37:10.160]   by a couple of-- a few years ago before South by closed
[02:37:10.160 --> 02:37:11.840]   for the pandemic.
[02:37:11.840 --> 02:37:13.600]   And you were at South by this year, Amy.
[02:37:13.600 --> 02:37:14.240]   Did it go well?
[02:37:14.240 --> 02:37:14.800]   Was it nice?
[02:37:14.800 --> 02:37:15.600]   Was it good to be back?
[02:37:15.600 --> 02:37:17.600]   Oh, it was amazing.
[02:37:17.600 --> 02:37:18.400]   Yeah.
[02:37:18.400 --> 02:37:21.200]   The South by community is a second family for me.
[02:37:21.200 --> 02:37:21.840]   Yeah, I know.
[02:37:21.840 --> 02:37:23.120]   I've been going on and off for 20 years.
[02:37:23.120 --> 02:37:28.000]   And I think there were fewer people than in the past.
[02:37:28.000 --> 02:37:29.760]   And I think it's going to--
[02:37:29.760 --> 02:37:31.040]   Sometimes that's not bad.
[02:37:31.040 --> 02:37:31.600]   Yes.
[02:37:31.600 --> 02:37:34.160]   I think there's the right number for any event.
[02:37:34.160 --> 02:37:36.560]   And most events, if they're good, get too big.
[02:37:36.560 --> 02:37:40.560]   Yeah, I mean, pre-pandemic, I think they have like 100,000 people.
[02:37:40.560 --> 02:37:42.080]   Yeah, that's too big.
[02:37:42.080 --> 02:37:43.920]   So now my session--
[02:37:43.920 --> 02:37:46.160]   So I always launch our Tech Trends Report there.
[02:37:46.160 --> 02:37:51.840]   And I'm always humbled and honored that so many people show up to my session.
[02:37:51.840 --> 02:37:56.400]   But I think I opened for Lizzo, because Lizzo was in the room right after me.
[02:37:56.400 --> 02:37:57.600]   So jealous.
[02:37:57.600 --> 02:37:58.160]   That's so cool.
[02:37:58.160 --> 02:38:00.080]   You should just put that on your resume.
[02:38:00.080 --> 02:38:01.120]   I opened for Lizzo.
[02:38:01.120 --> 02:38:03.760]   Well, I was like crap.
[02:38:03.760 --> 02:38:05.120]   All these people are here to see her.
[02:38:05.120 --> 02:38:08.000]   They're going to have to like grin and bear it through my Tech Trends.
[02:38:08.000 --> 02:38:09.440]   And then it turns out they weren't.
[02:38:09.440 --> 02:38:09.920]   Oh.
[02:38:09.920 --> 02:38:11.680]   Like most of the people got up and left.
[02:38:11.680 --> 02:38:12.800]   They left.
[02:38:12.800 --> 02:38:13.280]   Well, that's like--
[02:38:13.280 --> 02:38:13.600]   That was cool.
[02:38:13.600 --> 02:38:16.400]   And then like she totally packed the house and I stave.
[02:38:16.400 --> 02:38:16.880]   Oh, lovely.
[02:38:16.880 --> 02:38:17.520]   And she was amazing.
[02:38:17.520 --> 02:38:18.880]   Oh, she's such a fan.
[02:38:18.880 --> 02:38:20.640]   She is a goddess.
[02:38:20.640 --> 02:38:22.640]   She's cool.
[02:38:22.640 --> 02:38:23.040]   Yeah.
[02:38:23.040 --> 02:38:24.480]   I'm a fan of yours, Amy.
[02:38:24.480 --> 02:38:25.680]   Yeah.
[02:38:25.680 --> 02:38:26.240]   We all are--
[02:38:26.240 --> 02:38:27.200]   I'm a fan of yours, David.
[02:38:27.200 --> 02:38:27.600]   Stop it.
[02:38:27.600 --> 02:38:28.320]   No, no, no, no.
[02:38:28.320 --> 02:38:30.720]   You don't have to just reciprocate for the academic.
[02:38:30.720 --> 02:38:36.400]   I want you to know that you made me think differently, Amy, on this show.
[02:38:36.400 --> 02:38:37.760]   Every single time she does that.
[02:38:37.760 --> 02:38:38.880]   That's what was important.
[02:38:38.880 --> 02:38:40.160]   I wanted to think--
[02:38:40.160 --> 02:38:41.520]   I wanted to be challenged.
[02:38:41.520 --> 02:38:42.640]   I felt you challenged the audience.
[02:38:42.640 --> 02:38:43.440]   Every single time.
[02:38:43.440 --> 02:38:45.120]   And myself, personally.
[02:38:45.120 --> 02:38:46.080]   And I appreciate that.
[02:38:46.080 --> 02:38:46.960]   I appreciate that.
[02:38:46.960 --> 02:38:47.520]   Thank you.
[02:38:47.520 --> 02:38:48.720]   This has been really fun.
[02:38:48.720 --> 02:38:50.480]   I hate to end it, but we must.
[02:38:50.480 --> 02:38:53.600]   We do Twitch on a Sunday afternoon.
[02:38:53.600 --> 02:38:55.200]   You see why you don't want to miss an episode.
[02:38:55.200 --> 02:38:59.920]   Every Sunday about 2 p.m. Pacific, 5 p.m. Eastern 2100 UTC.
[02:38:59.920 --> 02:39:03.280]   Livestream, audio and video at live.twit.tv.
[02:39:03.280 --> 02:39:08.640]   If you're watching live, chat live with us at irc.twit.tv.
[02:39:08.640 --> 02:39:11.440]   Of course, club Twitch members get their very own
[02:39:11.440 --> 02:39:17.280]   enhanced chat room full of animated gifts at--
[02:39:17.280 --> 02:39:18.800]   like this.
[02:39:18.800 --> 02:39:22.240]   All you have to do is pay $7 a month.
[02:39:22.240 --> 02:39:25.040]   You'll get access to the Discord where conversations go on.
[02:39:25.040 --> 02:39:28.080]   All the time about all sorts of geeky subjects.
[02:39:28.080 --> 02:39:30.960]   You also get the ad-free versions of every show we do.
[02:39:30.960 --> 02:39:32.880]   Every single show we do.
[02:39:32.880 --> 02:39:35.840]   And you get access to the Twitch plus feed,
[02:39:35.840 --> 02:39:38.240]   which has shows we don't put anywhere else.
[02:39:38.240 --> 02:39:40.880]   That's where we launched this week in space.
[02:39:40.880 --> 02:39:43.360]   If you will, launched it.
[02:39:43.360 --> 02:39:45.760]   It's where we do the untitled Linux show.
[02:39:45.760 --> 02:39:47.920]   Stacey Higginbotham's book club.
[02:39:47.920 --> 02:39:49.040]   It's going to be good this month.
[02:39:49.040 --> 02:39:52.240]   It's going to be Neil Stevenson's termination shock.
[02:39:52.240 --> 02:39:55.440]   Another good book, well worth reading.
[02:39:55.440 --> 02:39:57.360]   We also do ask me anything.
[02:39:57.360 --> 02:40:00.000]   Alex Lindsay will be doing an AMA in July 14th.
[02:40:00.000 --> 02:40:02.240]   There's a lot of reasons to join the club.
[02:40:02.240 --> 02:40:06.000]   I think it's a great community of smart people.
[02:40:06.000 --> 02:40:07.840]   And you're obviously one of them.
[02:40:07.840 --> 02:40:12.080]   So come on in, twit.tv/clubtwit.
[02:40:12.080 --> 02:40:17.040]   After the fact, of course, we put shows out on demand
[02:40:17.040 --> 02:40:18.800]   on our website, twit.tv.
[02:40:18.800 --> 02:40:20.240]   On YouTube, there's a Twitch channel.
[02:40:20.240 --> 02:40:21.360]   There's a channel for all the shows.
[02:40:22.000 --> 02:40:25.200]   And you can subscribe to your favorite podcast
[02:40:25.200 --> 02:40:26.640]   and your favorite podcast player.
[02:40:26.640 --> 02:40:28.480]   But do us a favor.
[02:40:28.480 --> 02:40:31.280]   You know, we've been doing twit now for 17 years,
[02:40:31.280 --> 02:40:34.480]   when our 18th year and sometimes we know something's
[02:40:34.480 --> 02:40:36.320]   been around that long, people forget about it.
[02:40:36.320 --> 02:40:39.120]   So put a five-star review in there.
[02:40:39.120 --> 02:40:43.520]   Let remind people about the goodness that is this weekend tech.
[02:40:43.520 --> 02:40:44.560]   Thanks everybody.
[02:40:44.560 --> 02:40:45.520]   We'll see you next time.
[02:40:45.520 --> 02:40:47.200]   Another Twitch is in the game.
[02:40:47.200 --> 02:40:57.200]   [Music]
[02:40:57.200 --> 02:40:59.200]   Baby

