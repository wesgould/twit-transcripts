;FFMETADATA1
album=This Week in Tech
genre=Podcast
encoded_by=Uniblab 5.3
title=Gotta Sleep 'Em All
language=English
artist=Leo Laporte, Ben Parr, Reed Albergotti, Seth Weintraub
album_artist=TWiT
publisher=TWiT
track=935
date=2023
TRDA=2023-07-10
comment=<p>Threads by Meta, TweetDeck returns, Causal AI, thumbs-up emoji lawsuit</p>\

encoder=Lavf60.3.100

[00:00:00.000 --> 00:00:02.000]   It's time for Twit this week in tech.
[00:00:02.000 --> 00:00:08.840]   We've got a great panel for you with some big stories. Ben Parr is here, Seth Windrum from 9 to 5 in electric and
[00:00:08.840 --> 00:00:16.000]   Reed Albergotti from Semaphore. Of course, we're gonna talk about threads. The hottest, I think the hottest app launch in
[00:00:16.000 --> 00:00:20.200]   history was this week. We'll also talk about AI,
[00:00:20.200 --> 00:00:26.640]   used for everything from writing articles to zapping pests and is at the end of the line forever note.
[00:00:26.640 --> 00:00:29.140]   It's all coming up next on Twit.
[00:00:29.140 --> 00:00:34.860]   Podcasts you love from people you trust.
[00:00:34.860 --> 00:00:37.980]   This is Twit.
[00:00:37.980 --> 00:00:50.280]   This is Twit this week at Tech. Episode 935 recorded Sunday, July 9th,
[00:00:50.280 --> 00:00:56.080]   2023. Gotta sleep 'em all. This week at Tech is brought to you by Mint Mobile.
[00:00:56.620 --> 00:01:01.640]   Reflation is everywhere whether it's gas utilities or your favorite streaming services.
[00:01:01.640 --> 00:01:08.600]   Thankfully, Mint Mobile can give you a much needed break. Get your new wireless plan for just 15 bucks a month and
[00:01:08.600 --> 00:01:15.020]   get the plan shipped to your door for free. Go to mintmobile.com/twit and
[00:01:15.020 --> 00:01:22.060]   buy HelloFresh America's number one meal kit. Get farm fresh pre-portion ingredients and
[00:01:22.460 --> 00:01:29.500]   seasonal recipes delivered right to your doorstep. Skip the grocery store and count on HelloFresh to make home-cooking easy,
[00:01:29.500 --> 00:01:39.100]   fun and affordable. Go to HelloFresh.com/twit50 and use the code Twit50 for 50% off plus free shipping.
[00:01:39.100 --> 00:01:41.180]   And buy
[00:01:41.180 --> 00:01:45.580]   ACI learning. IT skills are outdated in about 18 months.
[00:01:45.660 --> 00:01:53.100]   So stay ahead of the curve and strengthen your IT expertise with affordable certification based learning that will advance your career.
[00:01:53.100 --> 00:02:04.140]   Visit go.acilurning.com/twit and use the code Twit30 for 30% off a standard or premium individual IT Pro membership.
[00:02:04.140 --> 00:02:15.180]   It's time for Twit this week in Tech. Although we might be calling it this week in threads this week anyway.
[00:02:15.180 --> 00:02:22.180]   I'm Leo LaPorte. This is the show where we cover the latest the latest tech news and no one more elite than Ben Parr
[00:02:22.180 --> 00:02:29.020]   joining us AI guru author of the AI analyst and AI columnist for the informacion.
[00:02:29.020 --> 00:02:31.340]   Is that a new job Ben?
[00:02:31.340 --> 00:02:38.180]   There's a new column. Yes, I write a monthly column where I get to say many brain thoughts on AI.
[00:02:38.180 --> 00:02:44.420]   That will be what I spend my time after this trying to get the thoughts out of my head onto a piece of paper.
[00:02:44.420 --> 00:02:49.140]   No AI being used for that yet. Yeah, I stopped doing that because it's painful.
[00:02:49.140 --> 00:02:51.980]   Those thoughts are really painful. They don't want to leave my head.
[00:02:51.980 --> 00:02:56.660]   So I found it easier to talk. Anyway, great to have you especially this week because you were
[00:02:56.660 --> 00:03:01.260]   instantly on threads and very active there. And so I'm
[00:03:01.260 --> 00:03:08.020]   looking forward to talking to you about it. The technology editor from Semaphore is also here the great read Albergotti. Hi, Reed.
[00:03:08.020 --> 00:03:11.140]   Hi, how's it going? Good. Are you on threads?
[00:03:12.380 --> 00:03:15.940]   Yes, I am on threads. What's your threads count?
[00:03:15.940 --> 00:03:24.900]   That's gonna be a thing. It has no name. I don't know. I have reached Egyptian cotton.
[00:03:24.900 --> 00:03:33.300]   I hope it's long staple. That's all I could say. I have no idea. I am not like I'm not as good as Ben.
[00:03:33.300 --> 00:03:41.380]   I'm sure I get notifications that someone's following me every once in a while. Yeah, I'm not really right away.
[00:03:41.380 --> 00:03:47.660]   Right away. I am. It's great to see you though. And I love your, I love a Semaphore and I love your newsletter.
[00:03:47.660 --> 00:03:51.740]   You guys are doing a great job. Yeah, it's really pleased to see you move over there.
[00:03:51.740 --> 00:03:57.780]   Also, a guy who's founded more than a few publications, the great Seth Wine Trob is here.
[00:03:57.780 --> 00:04:00.660]   You know him from initially was nine to five Mac, right?
[00:04:00.660 --> 00:04:05.900]   Right. And then nine to five Google nine to five. What else? You got a nine to five AI yet?
[00:04:05.900 --> 00:04:10.580]   Not yet. But you do have electric, which is electric is my favorite
[00:04:10.980 --> 00:04:16.380]   publication because I'm an EV fan. And he's on the threads with the same handle.
[00:04:16.380 --> 00:04:23.420]   He was on everything else with L L S L L S. I get L L Seth J. Yes.
[00:04:23.420 --> 00:04:32.020]   Get it. Get it. I love it. It's an old AOL name that's stuck with me forever. Nice.
[00:04:32.020 --> 00:04:39.420]   I just haven't been able to shake it. Do you get on threads and go ASL and ask people for their age,
[00:04:39.420 --> 00:04:42.220]   sex and location or no, you probably don't do that.
[00:04:42.220 --> 00:04:45.380]   Stop doing that. Good. Actually, Seth is legendary.
[00:04:45.380 --> 00:04:53.300]   And I was very pleased I was that you electric sponsors a Grand Prix race of
[00:04:53.300 --> 00:04:57.780]   solar powered vehicles, which is really cool.
[00:04:57.780 --> 00:05:01.500]   It's called the formula sun, not to be confused with the formula one.
[00:05:01.500 --> 00:05:08.380]   And it's a bunch of colleges every year, get together and race their cars.
[00:05:08.380 --> 00:05:10.620]   And it's a lot of fun. It's a lot of nerdy.
[00:05:10.620 --> 00:05:14.700]   And look at these. These are so cool looking. Yeah. Yeah.
[00:05:14.700 --> 00:05:16.980]   And they're fast. I mean, relatively fast. They're not.
[00:05:16.980 --> 00:05:21.500]   Well, you were telling me before the show, they were too fast.
[00:05:21.500 --> 00:05:23.580]   Initially, they had to cut down the size of the panels.
[00:05:23.580 --> 00:05:28.180]   Yeah, they went from six square meters to four square meters because they were going, you know, 60,
[00:05:28.180 --> 00:05:30.380]   70 miles per hour.
[00:05:30.380 --> 00:05:33.660]   And when it's essentially a bike. Pretty much.
[00:05:33.660 --> 00:05:37.620]   Yeah, a bike with a fairing, big old fairing and solar panels on it.
[00:05:38.260 --> 00:05:40.700]   Very cool. Very, very cool.
[00:05:40.700 --> 00:05:42.740]   Who, what team won this year?
[00:05:42.740 --> 00:05:46.780]   So Florida State won the single occupant vehicle race.
[00:05:46.780 --> 00:05:53.180]   And then a team from Montreal won the multi user or multi person vehicle.
[00:05:53.180 --> 00:05:55.500]   How many people in a multi person?
[00:05:55.500 --> 00:05:58.900]   Well, there's just two. Oh, I guess they could call it a family car or something.
[00:05:58.900 --> 00:06:02.060]   Yeah. OK. No, but in the world solar challenge,
[00:06:02.060 --> 00:06:08.220]   the team from the Netherlands actually turned their multi car or multi person
[00:06:08.220 --> 00:06:13.340]   car into what became light year, the company light year,
[00:06:13.340 --> 00:06:17.340]   which recently went out of business, but it was going good for a while.
[00:06:17.340 --> 00:06:21.420]   It's always going good for a while until you're out of business.
[00:06:21.420 --> 00:06:25.620]   That's the start of the multi multi time startup founder.
[00:06:25.620 --> 00:06:29.140]   That's a great idea. Gosh, I would buy a solar powered.
[00:06:29.140 --> 00:06:32.140]   Evie, I'd be very happy.
[00:06:32.140 --> 00:06:36.380]   The problem is you can't get enough panels on there to charge you 80 or 90 kilowatt hour
[00:06:36.380 --> 00:06:38.060]   battery. It's just the two biz.
[00:06:38.060 --> 00:06:42.660]   There's a company out of San Diego called Apterra that's making some headway.
[00:06:42.660 --> 00:06:48.780]   It's not entirely solar powered, but it's such a aerodynamic car that the solar
[00:06:48.780 --> 00:06:51.580]   that you get from it will add about 40 miles a day.
[00:06:51.580 --> 00:06:53.380]   Oh, that's great. If you're lucky.
[00:06:53.380 --> 00:06:56.980]   Well, you know, 90% of trips are under 30 miles.
[00:06:56.980 --> 00:06:58.460]   So that's good.
[00:06:58.460 --> 00:07:01.020]   That how to do it. That a doer.
[00:07:01.020 --> 00:07:03.860]   So we were joking around at the beginning of the show about threads.
[00:07:03.860 --> 00:07:07.660]   I think that's really clearly the top story of the week.
[00:07:07.660 --> 00:07:13.900]   And I mean, OK, I have mixed feelings about it, but let me tell you what happened.
[00:07:13.900 --> 00:07:15.060]   And we've been teasing this.
[00:07:15.060 --> 00:07:20.860]   So as meta for some time, that they were going to add a Twitter like platform to
[00:07:20.860 --> 00:07:23.780]   Instagram. The name is threads.
[00:07:23.780 --> 00:07:24.860]   They did it this week.
[00:07:24.860 --> 00:07:26.660]   They said they were going to do it on Thursday.
[00:07:26.660 --> 00:07:29.100]   They jumped the gun and launched it Wednesday night.
[00:07:29.100 --> 00:07:35.060]   And even though I got on as quickly as possible, I was the in the five millions
[00:07:35.940 --> 00:07:39.020]   of new people. Mark Zuckerberg
[00:07:39.020 --> 00:07:44.140]   posted this, I think, was Saturday or Friday that they have 70 million.
[00:07:44.140 --> 00:07:47.100]   I'm sure it's higher. I'm sure it's over a hundred million by now.
[00:07:47.100 --> 00:07:50.780]   So it's probably the fastest growing app of all time.
[00:07:50.780 --> 00:07:53.340]   It is very enough. I'll show you.
[00:07:53.340 --> 00:07:55.380]   It's very much like Twitter.
[00:07:55.380 --> 00:07:58.340]   And it has a couple of advantages.
[00:07:58.340 --> 00:08:03.540]   Well, one advantage is Twitter is rapidly swirling the the drain.
[00:08:04.020 --> 00:08:10.220]   But the oh, but the other advantage is it's just like Twitter and you join it by
[00:08:10.220 --> 00:08:16.500]   going to Instagram and then import your Instagram follows and followers and your
[00:08:16.500 --> 00:08:19.660]   handle and you're pretty much on and going.
[00:08:19.660 --> 00:08:25.340]   The other thing they did that was brilliant was instead of doing the Twitter
[00:08:25.340 --> 00:08:29.220]   style following feed where you have to follow people before you see their posts,
[00:08:29.220 --> 00:08:33.940]   it did a TikTok style algorithmic feed, which when you first join it,
[00:08:33.940 --> 00:08:37.180]   it is brilliant because you are instantly you're seeing a ton of content.
[00:08:37.180 --> 00:08:40.820]   And if the algorithm works content, you're interested in.
[00:08:40.820 --> 00:08:43.020]   And so you could quickly add people.
[00:08:43.020 --> 00:08:48.820]   It immediately caught on with not just regular users, but brands and
[00:08:48.820 --> 00:08:54.540]   celebrities, everybody, there's AOC, there's Howie Mandel, there's Ben Parr,
[00:08:54.540 --> 00:08:56.980]   there's everybody is on here.
[00:08:56.980 --> 00:09:01.700]   And I think this is a very interesting play.
[00:09:01.700 --> 00:09:03.060]   Now it's missing a lot of features.
[00:09:03.060 --> 00:09:05.420]   It doesn't in fact have a following feed.
[00:09:05.420 --> 00:09:07.780]   You know, you're stuck with the algorithmic feed.
[00:09:07.780 --> 00:09:10.900]   It does not have direct messaging.
[00:09:10.900 --> 00:09:16.260]   It does have tweet quotes, tweet, tweet, tweet, tweet quotes,
[00:09:16.260 --> 00:09:20.580]   quotes, threads, you know, where you quote, you can say, look, I like this.
[00:09:20.580 --> 00:09:23.820]   I'm going to retweet, repost it and I'm going to put a quote in above it.
[00:09:23.820 --> 00:09:28.900]   You know, the traditional quote tweet that Mastodon refuses to do because they
[00:09:28.900 --> 00:09:29.500]   don't like it.
[00:09:30.580 --> 00:09:35.420]   But it is missing some other features that Twitter users might like.
[00:09:35.420 --> 00:09:39.460]   It certainly doesn't show you how many people have looked at your post, which
[00:09:39.460 --> 00:09:42.260]   was a late ad from Elon Musk.
[00:09:42.260 --> 00:09:44.860]   It shows you though how many replies you've gotten, how many likes.
[00:09:44.860 --> 00:09:49.500]   And you can see on a lot of these, there are, there's a lot of engagement, a
[00:09:49.500 --> 00:09:50.580]   lot of engagement.
[00:09:50.580 --> 00:09:56.380]   Mepin, you were very good picture of a picture that I think Mark Zuckerberg
[00:09:56.380 --> 00:10:01.940]   would prefer not to see, I'm saying as a link to some of himself as I don't know
[00:10:01.940 --> 00:10:02.300]   what.
[00:10:02.300 --> 00:10:05.020]   It's like a juggalo.
[00:10:05.020 --> 00:10:06.340]   There he has a juggalo.
[00:10:06.340 --> 00:10:08.020]   I think you're right there.
[00:10:08.020 --> 00:10:08.740]   I'll show you.
[00:10:08.740 --> 00:10:14.740]   I will go to the, the app store to show you the thing that most people, you know,
[00:10:14.740 --> 00:10:22.140]   before it launched, complained about, which was the privacy settings.
[00:10:22.180 --> 00:10:26.860]   It has the same privacy settings as Instagram.
[00:10:26.860 --> 00:10:33.900]   And I presume as the Facebook app, in other words, it sucks it all down, baby,
[00:10:33.900 --> 00:10:36.100]   including health and fitness.
[00:10:36.100 --> 00:10:40.020]   In theory, we don't know in practice what they're going to do, but in theory,
[00:10:40.020 --> 00:10:44.900]   they could know, for instance, if you were, if you were pregnant, if you didn't
[00:10:44.900 --> 00:10:48.820]   like to exercise, if you like to eat donuts, it knows your purchases, your
[00:10:48.820 --> 00:10:54.540]   locations, your financial info, your sensitive info, basically they get, they
[00:10:54.540 --> 00:10:55.580]   get everything.
[00:10:55.580 --> 00:10:58.620]   I thought maybe that would slow down adoption.
[00:10:58.620 --> 00:11:03.300]   Clearly it has not nor has the relationship to meta.
[00:11:03.300 --> 00:11:06.100]   I think there's a lot of people I don't, I'm not on Facebook.
[00:11:06.100 --> 00:11:09.060]   I don't want to have anything to do with Facebook, but for some reason,
[00:11:09.060 --> 00:11:10.620]   Instagram never bothered me.
[00:11:10.620 --> 00:11:14.540]   And I don't, I don't use Instagram anymore because it's turned into some sort
[00:11:14.540 --> 00:11:20.820]   of weird shopping mall, but I, but threads was easy to join and I love the engagement.
[00:11:20.820 --> 00:11:23.260]   So Ben, what's happening?
[00:11:23.260 --> 00:11:24.500]   Why is threads?
[00:11:24.500 --> 00:11:29.740]   People are just over over ruling their own better interests and joining.
[00:11:29.740 --> 00:11:37.100]   I mean, there's one main reason that threads is taken off and it's Elon.
[00:11:37.300 --> 00:11:45.580]   And Elon ever since he took over has alienated a large section of Twitter.
[00:11:45.580 --> 00:11:49.620]   And I think, you know, now we know how clearly that happened.
[00:11:49.620 --> 00:11:55.340]   And it was a lot of people more than, let's say the Elon stands would be willing to admit.
[00:11:55.340 --> 00:12:00.300]   And the result is that people were walking and looking for a place where they didn't
[00:12:00.300 --> 00:12:04.820]   feel like they would be trolled all the time or have those issues.
[00:12:04.820 --> 00:12:08.660]   And look, I love Twitter and I like, so love the community there.
[00:12:08.660 --> 00:12:10.500]   And there's a difference between the two.
[00:12:10.500 --> 00:12:15.540]   And I think what will end up happening is that threads will become the more mainstream
[00:12:15.540 --> 00:12:18.580]   app with a little bit of the more mainstream conversation just purely from the fact
[00:12:18.580 --> 00:12:22.380]   it's connected to Instagram and Twitter will have a little bit more of a business
[00:12:22.380 --> 00:12:24.580]   bent and maybe a little bit of a right wing bend.
[00:12:24.580 --> 00:12:29.980]   But reality is like Elon made missteps and it turned people off and they were
[00:12:29.980 --> 00:12:34.540]   looking for an alternative and Macedon wasn't really it and blue sky wasn't really
[00:12:34.540 --> 00:12:38.300]   it. But meta just knows Mark Zuckerberg knows social.
[00:12:38.300 --> 00:12:40.420]   You could hate on him as much as you want.
[00:12:40.420 --> 00:12:42.820]   He knows how to build social media companies.
[00:12:42.820 --> 00:12:47.500]   Does it quite know how to do better verse, but he knows social media and he has nailed it.
[00:12:47.500 --> 00:12:54.380]   And by the way, as proof that Elon is feeling the pain, he immediately, July 5th,
[00:12:54.380 --> 00:13:01.180]   sent out a cease and desist email from his attorneys saying that A, threads is a
[00:13:01.180 --> 00:13:06.380]   copy of Twitter and B, that they hire a bunch of former Twitter employees.
[00:13:06.380 --> 00:13:12.660]   People, Elon fired, I should point out and is using them to copy threads.
[00:13:12.660 --> 00:13:16.900]   Adam, a Siri who's the head of Instagram and the head of threads said, no, in fact,
[00:13:16.900 --> 00:13:19.700]   we are not using any Twitter employees on threads.
[00:13:19.700 --> 00:13:25.140]   And who knows if this cease and desist will actually carry a lawsuit behind it,
[00:13:25.140 --> 00:13:29.820]   but it sure is a direct sign of Elon's serious pain.
[00:13:30.660 --> 00:13:31.980]   He is not happy.
[00:13:31.980 --> 00:13:34.900]   And of course, he probably sees what's happening, right?
[00:13:34.900 --> 00:13:37.100]   I mean, we don't know the numbers.
[00:13:37.100 --> 00:13:38.460]   We know how many people have joined threads.
[00:13:38.460 --> 00:13:42.540]   Well, we don't know as how many people have abandoned Twitter in favor of threads.
[00:13:42.540 --> 00:13:43.540]   We may never know that.
[00:13:43.540 --> 00:13:49.660]   Well, there's always these little signs happening right now of people banding in
[00:13:49.660 --> 00:13:53.980]   like Elon, at least resetting, or you could say panicking.
[00:13:53.980 --> 00:13:58.940]   One example was, I think today, they brought back tweet deck, which was a
[00:13:58.940 --> 00:14:01.780]   favorite of power users of Twitter, including myself.
[00:14:01.780 --> 00:14:06.460]   I was so upset when they removed tweet deck, which was before Elon's time bringing it
[00:14:06.460 --> 00:14:07.220]   back.
[00:14:07.220 --> 00:14:09.060]   That's the kind of thing that could bring people back.
[00:14:09.060 --> 00:14:14.940]   My general thought, hopefully, is that this spurs real competition, which means
[00:14:14.940 --> 00:14:18.860]   better things for users because competition is always good for the end user.
[00:14:18.860 --> 00:14:20.500]   That's my hope, at least.
[00:14:20.500 --> 00:14:22.740]   This is how fast the story is breaking.
[00:14:22.740 --> 00:14:26.420]   We would have reported the story that tweet deck no longer worked.
[00:14:28.460 --> 00:14:31.060]   Because of the rate limiting paywall.
[00:14:31.060 --> 00:14:35.380]   And then this morning, all of a sudden it starts working again.
[00:14:35.380 --> 00:14:41.500]   And according to one developer, Twitter switched to the old version one API to get
[00:14:41.500 --> 00:14:42.700]   it working again.
[00:14:42.700 --> 00:14:45.580]   Don't know how long it will work.
[00:14:45.580 --> 00:14:52.020]   The old tweet deck is actually kind of more important because you can, you can
[00:14:52.020 --> 00:14:53.900]   act as different sites.
[00:14:53.900 --> 00:14:58.100]   So, you know, me as a publisher, we have a bunch of different
[00:14:58.100 --> 00:15:00.980]   writers that have a bunch of different accounts.
[00:15:00.980 --> 00:15:06.140]   And the old tweet deck was the only thing that we could get to, you know, multiple
[00:15:06.140 --> 00:15:08.260]   users using multiple accounts.
[00:15:08.260 --> 00:15:10.620]   Plus you could schedule posts too, right?
[00:15:10.620 --> 00:15:11.860]   Which I'm sure was useful for you.
[00:15:11.860 --> 00:15:14.020]   Yep.
[00:15:14.020 --> 00:15:16.940]   Um, yeah, I agree.
[00:15:16.940 --> 00:15:18.700]   I, that was the only way I use Twitter.
[00:15:18.700 --> 00:15:20.180]   I loved the old tweet deck.
[00:15:20.180 --> 00:15:25.500]   Um, so Twitter clearly is reeling a little bit.
[00:15:26.580 --> 00:15:28.780]   But threads is missing a lot of features.
[00:15:28.780 --> 00:15:29.860]   There's no web interface.
[00:15:29.860 --> 00:15:34.020]   I have to show you on my phone because I can't put it up on a well, you've got a
[00:15:34.020 --> 00:15:36.580]   hack, I guess, Ben, but normal people.
[00:15:36.580 --> 00:15:44.580]   I mean, look, the, uh, they, the, the story is that they rushed to get this out early.
[00:15:44.580 --> 00:15:48.620]   It was going to launch probably a full month from now with more of those features.
[00:15:48.620 --> 00:15:53.300]   But when the rate limiting thing happened, which was just like the cherry on top,
[00:15:53.620 --> 00:15:57.460]   Mark told, uh, Adam, sorry, and the team launched now.
[00:15:57.460 --> 00:16:00.060]   Uh, and I think his instincts were right.
[00:16:00.060 --> 00:16:04.020]   Most people were just looking for an outlet and look now every day they're going to
[00:16:04.020 --> 00:16:06.140]   launch something that is just a basic functionality.
[00:16:06.140 --> 00:16:08.860]   You now have the ability to have multiple feeds.
[00:16:08.860 --> 00:16:12.060]   You now have lists and people will be excited over it.
[00:16:12.060 --> 00:16:16.300]   Just as a like side that we are like just, it's the great.
[00:16:16.300 --> 00:16:18.340]   Eli is the greatest PR person.
[00:16:18.340 --> 00:16:20.580]   Bart Zuckerberg has ever had.
[00:16:20.980 --> 00:16:25.180]   Like think about how we talked about Zuck a year ago.
[00:16:25.180 --> 00:16:25.580]   Yeah.
[00:16:25.580 --> 00:16:26.500]   No years ago.
[00:16:26.500 --> 00:16:26.900]   No.
[00:16:26.900 --> 00:16:27.500]   Yes.
[00:16:27.500 --> 00:16:27.780]   You did.
[00:16:27.780 --> 00:16:28.660]   Yeah.
[00:16:28.660 --> 00:16:31.660]   Oh, what?
[00:16:31.660 --> 00:16:35.140]   It's, it is such a power move.
[00:16:35.140 --> 00:16:41.180]   Cause I don't know if we like the, he tweeted for the first time in 10 years when he launched threads.
[00:16:41.180 --> 00:16:44.820]   And it was just that spider man meme of like pointing at each other.
[00:16:44.820 --> 00:16:46.980]   He's having a blast.
[00:16:46.980 --> 00:16:48.700]   He's having the best we get ever.
[00:16:48.700 --> 00:16:49.260]   It's good.
[00:16:49.260 --> 00:16:52.380]   But she had paid whatever he's having the great time.
[00:16:52.380 --> 00:16:58.780]   How quickly we forget though that meta's long term strategy was to find something that was succeeding on the web.
[00:16:58.780 --> 00:17:03.100]   And if they couldn't buy it like Instagram, they would copy it like Snapchat.
[00:17:03.100 --> 00:17:10.580]   And how many dozens of clones has met a maid over the years, but this one took.
[00:17:10.580 --> 00:17:15.260]   Unlike all the others, this one took, although I guess you could argue Instagram's
[00:17:15.260 --> 00:17:18.140]   reels probably did hurt TikTok a little bit.
[00:17:18.780 --> 00:17:20.820]   Read, are you on threads?
[00:17:20.820 --> 00:17:22.700]   Yes.
[00:17:22.700 --> 00:17:24.460]   As I said before, I am on threads.
[00:17:24.460 --> 00:17:25.580]   I've played around with it.
[00:17:25.580 --> 00:17:26.220]   Do you care?
[00:17:26.220 --> 00:17:27.300]   Do you think that there's?
[00:17:27.300 --> 00:17:28.420]   I did.
[00:17:28.420 --> 00:17:31.740]   I mean, yes, I obviously, I agree with you.
[00:17:31.740 --> 00:17:32.580]   It's the top story.
[00:17:32.580 --> 00:17:34.740]   I think there's this like question though.
[00:17:34.740 --> 00:17:39.100]   I have in the back of my head, like, has anybody ever been able to launch a successful
[00:17:39.100 --> 00:17:40.100]   social media app?
[00:17:40.100 --> 00:17:46.620]   Like with this kind of fanfare based on just sort of people wanting to leave another platform.
[00:17:46.620 --> 00:17:47.380]   Well, I'm not.
[00:17:47.380 --> 00:17:48.220]   I don't know if I'm not.
[00:17:48.220 --> 00:17:50.500]   My space read it dig.
[00:17:50.500 --> 00:17:51.460]   Yeah.
[00:17:51.460 --> 00:17:52.340]   That was not.
[00:17:52.340 --> 00:17:57.020]   I mean, my space was sort of like, it wasn't like people said, oh, I don't like the people
[00:17:57.020 --> 00:17:58.460]   running my space and I'm going to go to.
[00:17:58.460 --> 00:17:59.300]   I mean, that's true.
[00:17:59.300 --> 00:18:03.860]   It's just it's it had a better kind of rollout on college campus.
[00:18:03.860 --> 00:18:08.940]   I mean, there's always these things always kind of happen like organically and not to say
[00:18:08.940 --> 00:18:12.020]   that this this won't have staying power.
[00:18:12.020 --> 00:18:16.980]   But I do kind of wonder whether we're whether this may be kind of a flash in the pan.
[00:18:17.700 --> 00:18:18.660]   It's hard to get.
[00:18:18.660 --> 00:18:21.980]   I'm not sure if you get 70 million or 100 million users.
[00:18:21.980 --> 00:18:25.740]   That's critical mass, isn't it?
[00:18:25.740 --> 00:18:31.220]   I mean, yeah, but a lot of that is like a lot of that is just their their ability to
[00:18:31.220 --> 00:18:32.940]   leverage that Instagram, right?
[00:18:32.940 --> 00:18:35.700]   You know, graph, I guess, if you will.
[00:18:35.700 --> 00:18:41.500]   So I don't know if it's like a pure 70 million, like, and it's and it's like, what does it
[00:18:41.500 --> 00:18:45.340]   actually mean for Facebook and what does it actually mean for for Twitter?
[00:18:45.380 --> 00:18:52.180]   Like, I don't know if this will meaningfully change Facebook's or meta's, you know, underlying business.
[00:18:52.180 --> 00:18:57.540]   And I don't know, like, we'll see what is met as underlying business by the way.
[00:18:57.540 --> 00:19:02.700]   Do you is it is still, you know, it's still just selling ads on Facebook, right?
[00:19:02.700 --> 00:19:07.780]   And they want it to be, they want it to be, you know, a different sort of, you know, they're
[00:19:07.780 --> 00:19:08.740]   looking to the future.
[00:19:08.740 --> 00:19:12.700]   That's the other part of it that I wonder is like, is this really the future?
[00:19:12.700 --> 00:19:20.700]   Like, is the future of tech and media like just another, just more and more copycat apps of, sorry,
[00:19:20.700 --> 00:19:21.700]   I'm like shaking my camera.
[00:19:21.700 --> 00:19:25.660]   Are there, they're like more and more copycat apps that just do the same thing?
[00:19:25.660 --> 00:19:32.620]   Like, I sort of, I'm kind of wondering whether Gen Z is going to like start using
[00:19:32.620 --> 00:19:35.100]   something completely different and like, what is that?
[00:19:35.100 --> 00:19:37.540]   Like, how is, how is Gen Z communicate?
[00:19:37.540 --> 00:19:41.140]   It was TikTok and they, that did create a new paradigm.
[00:19:41.380 --> 00:19:45.300]   But unfortunately, some of the other new social networks they've tried to do, like
[00:19:45.300 --> 00:19:47.060]   the Twitter copies for example,
[00:19:47.060 --> 00:19:49.100]   Oh, be real was like a new idea.
[00:19:49.100 --> 00:19:51.820]   And like that completely has lost all momentum.
[00:19:51.820 --> 00:19:57.820]   And I agree with you that, you know, like, so the only reason threads works is because
[00:19:57.820 --> 00:20:01.820]   of what Elon has done with Twitter and like the, like response to that.
[00:20:01.820 --> 00:20:05.100]   He, if market launched threads a year ago, it would have failed.
[00:20:05.100 --> 00:20:06.180]   Two years ago would have failed.
[00:20:06.180 --> 00:20:07.860]   It would only work right now.
[00:20:07.860 --> 00:20:08.340]   And he knows.
[00:20:08.340 --> 00:20:12.020]   And like, I think the reality is there will still be two and he can convince these
[00:20:12.020 --> 00:20:16.100]   specific celebrities who are not tweeting anymore to post on threads because it's
[00:20:16.100 --> 00:20:17.060]   part of Instagram.
[00:20:17.060 --> 00:20:20.300]   And so there will be an audience and both are going to still be there and survive.
[00:20:20.300 --> 00:20:24.780]   I actually think that it will make Twitter better and like,
[00:20:24.780 --> 00:20:27.460]   threaders will make Twitter better and threads will have an audience.
[00:20:27.460 --> 00:20:33.060]   And it's not like my guess and I can be wrong is that it's not like clubhouse.
[00:20:33.460 --> 00:20:39.140]   And it's more has more real staying power, but it's purely by circumstance in a lot of ways.
[00:20:39.140 --> 00:20:44.780]   And look, you know, if it works, don't, I sometimes if it works, don't try to like reinvent
[00:20:44.780 --> 00:20:46.100]   a wheel just because you can.
[00:20:46.100 --> 00:20:50.580]   It would be interesting to see if there's like another generational shift in the way
[00:20:50.580 --> 00:20:52.700]   social media works or something else.
[00:20:52.700 --> 00:20:54.540]   That is like the hardest thing to predict.
[00:20:54.540 --> 00:20:59.060]   All I can say right now, yeah, Elon, if Elon messed up.
[00:20:59.060 --> 00:21:00.820]   We should.
[00:21:00.820 --> 00:21:01.380]   I mean, we.
[00:21:01.380 --> 00:21:07.500]   And when I say Gen Z, it's like TikTok is like the older Gen Z slash millennial audience.
[00:21:07.500 --> 00:21:09.100]   I feel like the younger Gen Z.
[00:21:09.100 --> 00:21:10.140]   What a 20 year old.
[00:21:10.140 --> 00:21:10.660]   Yeah.
[00:21:10.660 --> 00:21:16.260]   Yeah, I think I think there's going to be something else and it's it's not going to be threads.
[00:21:16.260 --> 00:21:19.740]   So one thing one thing that this has a couple of things come to mind.
[00:21:19.740 --> 00:21:23.420]   One is we were last week all proclaiming the end of social media.
[00:21:23.420 --> 00:21:29.020]   Those articles look have aged like milk to.
[00:21:29.980 --> 00:21:32.020]   It's still the same people use Twitter.
[00:21:32.020 --> 00:21:35.020]   It is not the general or maybe it is.
[00:21:35.020 --> 00:21:38.660]   I don't know, but I don't think it's the normal group of people.
[00:21:38.660 --> 00:21:42.420]   It's journalists, it's influencers, it's brands, it's celebrities.
[00:21:42.420 --> 00:21:47.820]   The, the same group that used Twitter, we're looking for a Twitter replacement, but that is
[00:21:47.820 --> 00:21:50.900]   still a fraction of the total world.
[00:21:50.900 --> 00:21:56.220]   And then the third question I have actually said this might apply to you because you
[00:21:56.220 --> 00:21:56.980]   have a bunch of brands.
[00:21:56.980 --> 00:22:02.500]   I presume because you know, you're doing brand marketing, you're going to immediately
[00:22:02.500 --> 00:22:05.900]   join whatever network is people are on, right?
[00:22:05.900 --> 00:22:06.460]   You have to.
[00:22:06.460 --> 00:22:06.820]   Sure.
[00:22:06.820 --> 00:22:07.060]   Yeah.
[00:22:07.060 --> 00:22:07.140]   Yeah.
[00:22:07.140 --> 00:22:11.740]   And we joined threads right away and we immediately saw like some big engagement.
[00:22:11.740 --> 00:22:17.180]   Um, and also like, you know, as Ben said, like Twitter has been really rough on us.
[00:22:17.180 --> 00:22:20.700]   Uh, recently, um, you know, we can't auto tweet anymore.
[00:22:20.700 --> 00:22:23.740]   They pulled our tweet deck like, and we're, and we're verified.
[00:22:23.740 --> 00:22:26.900]   Um, so like, I don't, I don't know how.
[00:22:26.900 --> 00:22:31.660]   People are being treated that aren't, you know, well, they only can look at 600 tweets.
[00:22:31.660 --> 00:22:33.300]   And that's that and you're done and goodbye.
[00:22:33.300 --> 00:22:33.900]   Yeah.
[00:22:33.900 --> 00:22:39.700]   So we, we were treated pretty poorly by Twitter and, you know, every, every author there has
[00:22:39.700 --> 00:22:45.260]   to like, you know, go through with these extra hoops, uh, to, to tweet or reply or whatever.
[00:22:45.260 --> 00:22:48.700]   So when threads came up, everybody's like, let's just do this.
[00:22:48.700 --> 00:22:50.060]   You know, let's, let's be there.
[00:22:50.060 --> 00:22:51.740]   Let's see if this is the next thing.
[00:22:51.740 --> 00:22:54.340]   And, and, um, the engagement has been great.
[00:22:54.340 --> 00:22:54.900]   Yeah.
[00:22:55.540 --> 00:23:02.140]   What, uh, does this mean from your point of view to for blue sky, T two spill?
[00:23:02.140 --> 00:23:04.500]   I mean, I can go on and on and on.
[00:23:04.500 --> 00:23:08.780]   Uh, I'm going to leave massive on, uh, out for the moment, because there is a further
[00:23:08.780 --> 00:23:11.260]   story with the Fediverse, but what is it?
[00:23:11.260 --> 00:23:13.180]   What the other Twitter clones are they?
[00:23:13.180 --> 00:23:15.900]   I mean, they were in the same position.
[00:23:15.900 --> 00:23:19.980]   They didn't, they didn't have the Instagram social graph to, to glom onto.
[00:23:19.980 --> 00:23:24.020]   But I think, is it done for blue sky, for instance?
[00:23:24.020 --> 00:23:27.420]   It feels kind of done to me.
[00:23:27.420 --> 00:23:30.500]   I, like, uh, I don't think I've checked blue skies.
[00:23:30.500 --> 00:23:31.060]   I haven't.
[00:23:31.060 --> 00:23:31.580]   Nope.
[00:23:31.580 --> 00:23:35.420]   It's, it's hard for me and post news and like, look, you know,
[00:23:35.420 --> 00:23:36.660]   post it already killed itself.
[00:23:36.660 --> 00:23:37.060]   I'm sorry.
[00:23:37.060 --> 00:23:38.060]   Post was already done.
[00:23:38.060 --> 00:23:43.060]   Uh, uh, blue sky did just announce a new funding round.
[00:23:43.060 --> 00:23:47.180]   They have, they think they have a very small loyal audience, but I mean,
[00:23:47.180 --> 00:23:51.220]   those people went to blue sky because it wasn't Twitter and now there's
[00:23:51.220 --> 00:23:53.100]   something better and they're going to go to the threads.
[00:23:53.420 --> 00:23:57.140]   There's no reason, there's no stickiness to blue sky or two or any of the other.
[00:23:57.140 --> 00:23:57.380]   Yeah.
[00:23:57.380 --> 00:24:00.740]   You just go where the people are and it's the people have voted.
[00:24:00.740 --> 00:24:01.540]   It seems like.
[00:24:01.540 --> 00:24:02.660]   Yeah.
[00:24:02.660 --> 00:24:06.780]   I mean, threads suck the air out of the room for all the, you know,
[00:24:06.780 --> 00:24:09.980]   Elon hating Twitter people, like they're just all over it.
[00:24:09.980 --> 00:24:11.060]   That's not doing anything.
[00:24:11.060 --> 00:24:17.300]   Well, Elon hating is interesting because like there was, we had what,
[00:24:17.300 --> 00:24:20.580]   like six years of Zuckerberg hating right now, which is crazy.
[00:24:20.580 --> 00:24:23.140]   Like, like you were saying, everything.
[00:24:23.620 --> 00:24:24.580]   It really loves the guy now.
[00:24:24.580 --> 00:24:27.940]   So you think that Zuck hate was unmerited, uh, says?
[00:24:27.940 --> 00:24:33.060]   No, no, I, I can't believe that I'm like sort of rooting for
[00:24:33.060 --> 00:24:35.860]   friends to work because like I hate it.
[00:24:35.860 --> 00:24:37.860]   I hated Facebook.
[00:24:37.860 --> 00:24:39.940]   Like, they say I'm, they do.
[00:24:39.940 --> 00:24:41.980]   Nobody treated publishers worse than that.
[00:24:41.980 --> 00:24:44.820]   Uh, like there was no, like, well, how do you feel when Adam
[00:24:44.820 --> 00:24:48.220]   Assyri says, yeah, news and politics were, we're not going to favor that.
[00:24:48.220 --> 00:24:52.940]   Uh, I, so I mean, I don't, I don't even know what that means.
[00:24:52.940 --> 00:24:53.940]   I don't know what that means.
[00:24:53.940 --> 00:24:54.980]   Coming out of there.
[00:24:54.980 --> 00:24:57.900]   Uh, so we'll keep an eye on it.
[00:24:57.900 --> 00:25:03.500]   I guess, like Ben said, I, I think competition between the two, hopefully
[00:25:03.500 --> 00:25:07.460]   keeps them honest, hopefully keeps them, you know, you know, fighting each other
[00:25:07.460 --> 00:25:09.340]   and making the user experience better.
[00:25:09.340 --> 00:25:15.340]   But I mean, you know, Facebook and Instagram have not been great to users or,
[00:25:15.340 --> 00:25:19.860]   you know, when they're in the position of power, they're not great to users
[00:25:19.860 --> 00:25:21.140]   or publishers or anybody.
[00:25:21.140 --> 00:25:21.580]   Yeah.
[00:25:21.580 --> 00:25:30.220]   So I left out Mastodon because there is one, I think fascinating element to this
[00:25:30.220 --> 00:25:32.500]   that still remains unknown.
[00:25:32.500 --> 00:25:38.220]   Wha, before they launched FED threads, they promised that they would
[00:25:38.220 --> 00:25:44.980]   federate, that they would support activity pub, which if they do it means
[00:25:44.980 --> 00:25:49.220]   that, uh, we know, we run our own Mastodon instance at twit.social.
[00:25:49.220 --> 00:25:56.340]   I could follow anybody on threads safely without joining threads on Mastodon,
[00:25:56.340 --> 00:26:01.900]   which means all the brands and celebrities that refuse to join Mastodon and the
[00:26:01.900 --> 00:26:05.660]   subgroups like black Twitter that wasn't, didn't feel safe on Mastodon.
[00:26:05.660 --> 00:26:08.980]   They joined, they've all joined threads.
[00:26:08.980 --> 00:26:11.340]   They could all be followed from Mastodon.
[00:26:11.340 --> 00:26:13.300]   Now there are a lot of masks for some reason.
[00:26:13.300 --> 00:26:18.140]   I'm not sure I understand why a lot of Mastodon admins say, we'll never
[00:26:18.140 --> 00:26:19.260]   federate with meta.
[00:26:19.260 --> 00:26:20.460]   We're going to affect this one guy.
[00:26:20.460 --> 00:26:22.020]   I said, I just blocked it already.
[00:26:22.020 --> 00:26:22.700]   It doesn't even exist.
[00:26:22.700 --> 00:26:23.420]   I already blocked it.
[00:26:23.420 --> 00:26:28.100]   Uh, but as a Mastodon admin, I, I think it's great.
[00:26:28.100 --> 00:26:34.060]   Uh, Eugene Rock, Rachko, who wrote Mastodon, wrote a blog post this week,
[00:26:34.060 --> 00:26:37.820]   what to know about threads in which he said exactly what I've felt, which is
[00:26:37.820 --> 00:26:41.100]   it's not, there's no risk to Mastodon from threads.
[00:26:41.100 --> 00:26:42.500]   There's nothing but benefit.
[00:26:42.500 --> 00:26:45.340]   It threads meta doesn't get any of your data.
[00:26:46.020 --> 00:26:47.460]   Uh, you won't see ads.
[00:26:47.460 --> 00:26:50.020]   There's no embrace and extinguish going on.
[00:26:50.020 --> 00:26:54.740]   The activity pub is open and is out there and is not going to disappear.
[00:26:54.740 --> 00:26:58.300]   So this is very good for Mastodon, which makes me think that.
[00:26:58.300 --> 00:27:01.180]   Meta will absolutely back down.
[00:27:01.180 --> 00:27:03.060]   It's promise it will never, never.
[00:27:03.060 --> 00:27:05.980]   They have, there's no reason for them to.
[00:27:05.980 --> 00:27:07.620]   They've been successful, so successful.
[00:27:07.620 --> 00:27:08.780]   They don't need to, right?
[00:27:08.780 --> 00:27:11.860]   I think they do.
[00:27:11.860 --> 00:27:12.820]   I think they will.
[00:27:12.820 --> 00:27:17.060]   That will be the real impact of threats is to, is to make the feta, the
[00:27:17.060 --> 00:27:19.300]   fetivers happen, which is really interesting.
[00:27:19.300 --> 00:27:19.780]   Isn't that?
[00:27:19.780 --> 00:27:20.660]   I think, yes.
[00:27:20.660 --> 00:27:21.820]   Yeah.
[00:27:21.820 --> 00:27:23.300]   That's my thought exactly.
[00:27:23.300 --> 00:27:25.260]   Complete side note.
[00:27:25.260 --> 00:27:29.700]   When I hear the phrase fetiverse, I always first always think like the
[00:27:29.700 --> 00:27:31.820]   feds are going to come knocking down my door.
[00:27:31.820 --> 00:27:34.580]   It's, it is the universe of federal agents.
[00:27:34.580 --> 00:27:39.660]   It is like the Marvel cinematic universe, but you got, uh, the CIA and you
[00:27:39.660 --> 00:27:42.460]   got the FBI agent and they're going to all combine forces.
[00:27:42.780 --> 00:27:46.140]   I want to be very clear that Ben is a terrible name cookie.
[00:27:46.140 --> 00:27:50.380]   Maybe it's a terrible name, but the point of it is that it is it really
[00:27:50.380 --> 00:27:51.340]   interesting concept.
[00:27:51.340 --> 00:27:53.540]   I think it makes a lot of sense.
[00:27:53.540 --> 00:27:54.060]   Yeah.
[00:27:54.060 --> 00:27:56.180]   It's a, it's a federated social network.
[00:27:56.180 --> 00:27:58.980]   So, uh, you know, it doesn't, it's not all Mastodon, by the way.
[00:27:58.980 --> 00:28:00.500]   That's just a small fraction of it.
[00:28:00.500 --> 00:28:04.380]   You can follow people from any part of the fetiverse to any other part.
[00:28:04.380 --> 00:28:09.940]   If threads joins it, yes, they'll be by far the largest social network in
[00:28:09.940 --> 00:28:12.860]   the federation, but that doesn't mean they take it over.
[00:28:12.860 --> 00:28:17.940]   It merely means we have access to me the best way to use threads, the
[00:28:17.940 --> 00:28:22.580]   privacy forward way to use threads would be to join a Mastodon instance and follow
[00:28:22.580 --> 00:28:29.300]   Ben, par and, you know, and, uh, Kim Kardashian and, and, uh, you know, Cheetos
[00:28:29.300 --> 00:28:33.660]   and just follow them on a Mastodon and you get exactly what you want and no more.
[00:28:33.660 --> 00:28:34.660]   No less there.
[00:28:34.660 --> 00:28:35.020]   There.
[00:28:35.020 --> 00:28:38.860]   I think there are some other issues and I really do wonder if Mark will just say,
[00:28:39.460 --> 00:28:42.740]   yeah, that was a good thing to say when we weren't sure if it was going to take off,
[00:28:42.740 --> 00:28:45.020]   but we don't need, we don't need them now.
[00:28:45.020 --> 00:28:50.140]   It feels like a two thirds chance now it backs out and a one third chance.
[00:28:50.140 --> 00:28:55.140]   He just says, I said, I might as well do it anyway, because it doesn't really hurt
[00:28:55.140 --> 00:28:55.700]   that meta.
[00:28:55.700 --> 00:29:00.980]   No, no, the only way it hurts them is they, if a large percentage of their users
[00:29:00.980 --> 00:29:06.540]   are looking at threads from another Mastodon instance and they can't serve ads,
[00:29:06.780 --> 00:29:09.660]   that's how it could hurt and that would be the reason they wouldn't do it.
[00:29:09.660 --> 00:29:11.420]   But who knows?
[00:29:11.420 --> 00:29:13.220]   I would love to see it.
[00:29:13.220 --> 00:29:14.420]   It should happen.
[00:29:14.420 --> 00:29:15.860]   Will it happen now?
[00:29:15.860 --> 00:29:17.420]   He's not thinking about that at all.
[00:29:17.420 --> 00:29:19.780]   The Fediverse would be a symbiote.
[00:29:19.780 --> 00:29:23.100]   It would be, I don't know, what is the, what is the bird that sits on the hip
[00:29:23.100 --> 00:29:26.660]   upon him is back and picks its teeth or crocodiles back and picks his teeth.
[00:29:26.660 --> 00:29:31.260]   It would be the smallest little thing on the back of the giant behemoths that is
[00:29:31.260 --> 00:29:33.420]   threads and I don't think it would harm it or help it.
[00:29:33.420 --> 00:29:36.620]   It's just there and it would help in the sense that it would
[00:29:36.620 --> 00:29:39.740]   make it look like it's not a centralized network.
[00:29:39.740 --> 00:29:47.020]   Yeah, but Facebook not being able to monetize a group of users.
[00:29:47.020 --> 00:29:48.860]   I just don't see that happening.
[00:29:48.860 --> 00:29:50.540]   That's the third party problem, isn't it?
[00:29:50.540 --> 00:29:50.900]   Yeah.
[00:29:50.900 --> 00:29:51.580]   Right.
[00:29:51.580 --> 00:29:52.580]   I have to.
[00:29:52.580 --> 00:29:57.980]   Somebody needs to come in and make the Fediverse sort of accessible to everyone,
[00:29:57.980 --> 00:29:58.180]   right?
[00:29:58.180 --> 00:29:59.820]   Mastodon is just not that.
[00:29:59.820 --> 00:30:01.300]   It's never going to be that.
[00:30:01.300 --> 00:30:01.740]   Yeah.
[00:30:01.740 --> 00:30:05.980]   Something needs to happen to kind of get that idea off the ground because it's,
[00:30:06.340 --> 00:30:10.020]   it's so ridiculous that if people, people have this network that they've
[00:30:10.020 --> 00:30:14.740]   built up for years on Twitter and, you know, they all, so they don't like Twitter
[00:30:14.740 --> 00:30:19.340]   anymore, they can't just take that network and go somewhere else and still be able
[00:30:19.340 --> 00:30:20.300]   to access it, right?
[00:30:20.300 --> 00:30:24.220]   I mean, that's like you really don't own your contacts.
[00:30:24.220 --> 00:30:27.820]   I think there's something just inherently backwards about that.
[00:30:27.820 --> 00:30:34.060]   Here is a picture of a water buffalo with a symbiote oxpecker.
[00:30:35.820 --> 00:30:42.500]   Mastodon in this, in this metaphor would be the oxpecker and threads would be
[00:30:42.500 --> 00:30:43.580]   your college.
[00:30:43.580 --> 00:30:44.500]   I'm starting to get used to that.
[00:30:44.500 --> 00:30:44.580]   Yeah.
[00:30:44.580 --> 00:30:45.180]   Oxpecker.
[00:30:45.180 --> 00:30:46.020]   That's what they call me.
[00:30:46.020 --> 00:30:49.620]   Oxpecker.com.
[00:30:49.620 --> 00:30:50.620]   Oxpecker.com.
[00:30:50.620 --> 00:30:54.300]   I'm going to quickly reserve that name.
[00:30:54.300 --> 00:30:58.060]   Okay.
[00:30:58.060 --> 00:31:01.340]   Well, I guess we've said everything amazing in less than half an hour.
[00:31:01.340 --> 00:31:03.300]   Everything there is to be said about threads.
[00:31:03.300 --> 00:31:04.220]   Yes.
[00:31:05.220 --> 00:31:11.380]   What would you, what will we look for over the next couple of weeks to,
[00:31:11.380 --> 00:31:15.420]   what wins should we look for?
[00:31:15.420 --> 00:31:17.340]   Where should we, where should we stick our finger?
[00:31:17.340 --> 00:31:18.620]   No, that's not right.
[00:31:18.620 --> 00:31:24.340]   It's, I meant, you know, in the air like that.
[00:31:24.340 --> 00:31:32.900]   I mean, the next, like closing thought, which is just kind of like, we're still
[00:31:32.900 --> 00:31:33.740]   like a weekend.
[00:31:33.740 --> 00:31:36.860]   So like fourth of July weekend, it feels like fourth of July is a long time ago.
[00:31:36.860 --> 00:31:36.860]   Yeah.
[00:31:36.860 --> 00:31:37.860]   Right.
[00:31:37.860 --> 00:31:42.020]   And like, we have no news, but you just threw giant news.
[00:31:42.020 --> 00:31:46.860]   Well, we have to wait a couple of months to see if it did stick.
[00:31:46.860 --> 00:31:47.940]   It's going to be a while.
[00:31:47.940 --> 00:31:48.460]   Okay.
[00:31:48.460 --> 00:31:49.220]   Right.
[00:31:49.220 --> 00:31:50.820]   And what's your prediction?
[00:31:50.820 --> 00:31:51.100]   This.
[00:31:51.100 --> 00:31:58.140]   My prediction is that threads and Twitter coexist threads.
[00:31:58.140 --> 00:31:59.580]   This like is bigger.
[00:31:59.580 --> 00:32:01.740]   How much bigger?
[00:32:01.740 --> 00:32:02.540]   I don't know.
[00:32:02.780 --> 00:32:05.780]   Threads is just a different kind of conversation.
[00:32:05.780 --> 00:32:11.420]   A little bit more to hit talk like a little bit more unhinged, less business.
[00:32:11.420 --> 00:32:13.780]   And Twitter becomes a little bit more business.
[00:32:13.780 --> 00:32:17.820]   And Twitter is a little bit more like politics.
[00:32:17.820 --> 00:32:20.820]   And Twitter gets better because of this.
[00:32:20.820 --> 00:32:23.220]   Those are my safe predictions.
[00:32:23.220 --> 00:32:28.260]   Does, does threads get to a billion users by the end of the year?
[00:32:28.260 --> 00:32:30.860]   I don't think so, but we'll see.
[00:32:32.140 --> 00:32:33.580]   Reed, you want to make a prediction?
[00:32:33.580 --> 00:32:34.100]   Scattered.
[00:32:34.100 --> 00:32:34.860]   Oh, go ahead, Seth.
[00:32:34.860 --> 00:32:35.060]   You.
[00:32:35.060 --> 00:32:38.620]   Well, I mean, I don't necessarily disagree.
[00:32:38.620 --> 00:32:43.780]   I mean, I think what you're kind of saying is it's like, you know, the future of social
[00:32:43.780 --> 00:32:45.260]   networking is just scattered, right?
[00:32:45.260 --> 00:32:51.060]   Like people are just, you know, because there's discord and, you know, some people are using.
[00:32:51.060 --> 00:32:52.740]   I know families that use Slack.
[00:32:52.740 --> 00:32:57.940]   You know, there are messages of telegram.
[00:32:58.380 --> 00:33:02.220]   There are all these places that are more private, no advertising.
[00:33:02.220 --> 00:33:05.900]   This is where the Fediverse is kind of interesting, too.
[00:33:05.900 --> 00:33:10.540]   It's like if the world is really scattered and there's like 18 different social networks
[00:33:10.540 --> 00:33:14.340]   that you're using at any given time, it just becomes really unwieldy.
[00:33:14.340 --> 00:33:17.380]   And like then you sort of need something to kind of like unify it.
[00:33:17.380 --> 00:33:22.580]   I've been, I've been using this app called Beeper, which is kind of interesting
[00:33:22.580 --> 00:33:24.860]   that like takes all your messages.
[00:33:24.860 --> 00:33:26.100]   I'm just playing with it.
[00:33:26.100 --> 00:33:31.940]   But like, you know, you have Slack and I message and, you know, every, you know, LinkedIn,
[00:33:31.940 --> 00:33:34.140]   whatever, it's all like in one place.
[00:33:34.140 --> 00:33:36.340]   And it's, it's really useful.
[00:33:36.340 --> 00:33:41.740]   I mean, it's just, it's so hard to keep track of like everything that you, you know,
[00:33:41.740 --> 00:33:44.780]   that you're posting and messaging these days, I think.
[00:33:44.780 --> 00:33:47.060]   So that is one complaint people had.
[00:33:47.060 --> 00:33:49.620]   I imagine you have two Seth, which is there.
[00:33:49.620 --> 00:33:51.900]   Does Hoot suite work with threads?
[00:33:51.900 --> 00:33:53.260]   I mean, how do I?
[00:33:53.260 --> 00:33:54.660]   I was just going to comment on that.
[00:33:54.660 --> 00:33:54.900]   Yeah.
[00:33:54.900 --> 00:33:55.580]   Doesn't yet.
[00:33:55.580 --> 00:34:00.140]   And, you know, like it's just going to become more complicated.
[00:34:00.140 --> 00:34:01.740]   Like we're going in that direction.
[00:34:01.740 --> 00:34:05.340]   It would be nice if there was like a Hoot suite type top level thing that would
[00:34:05.340 --> 00:34:08.780]   shoot out to all the, you know, other stuff and maybe meta will make something
[00:34:08.780 --> 00:34:15.180]   like that that does its programs and then, you know, Twitter and some other groups
[00:34:15.180 --> 00:34:16.180]   will do something else.
[00:34:16.180 --> 00:34:20.260]   You know, it's hard to say like right now, I know that.
[00:34:20.260 --> 00:34:23.020]   Threads is here to stay.
[00:34:23.020 --> 00:34:24.780]   I think it'll be around for a while.
[00:34:24.780 --> 00:34:26.340]   I don't think it's going to be like Google plus.
[00:34:26.340 --> 00:34:29.260]   And I think and meta, I know.
[00:34:29.260 --> 00:34:33.460]   I don't think I don't think that it's going to give up on give up on anything
[00:34:33.460 --> 00:34:34.260]   that quickly.
[00:34:34.260 --> 00:34:37.740]   I'm ashamed that I loved Google plus.
[00:34:37.740 --> 00:34:38.300]   I did say it.
[00:34:38.300 --> 00:34:42.460]   And look, look, look, I look, they gave, I got half a million followers on Google
[00:34:42.460 --> 00:34:42.780]   plus.
[00:34:42.780 --> 00:34:43.700]   Of course I loved it.
[00:34:43.700 --> 00:34:46.780]   Please, I am not above.
[00:34:46.780 --> 00:34:48.260]   Be honest, suggested user list.
[00:34:48.260 --> 00:34:49.340]   I am not above pegging.
[00:34:50.700 --> 00:34:55.620]   But if Google had just not turned that off and had just kept going with it,
[00:34:55.620 --> 00:34:58.500]   it could have taken advantage.
[00:34:58.500 --> 00:35:02.900]   Like Google could have absolutely done it to this is just timing.
[00:35:02.900 --> 00:35:06.940]   But the reality is Mark, like Google is not an expert at social networks.
[00:35:06.940 --> 00:35:09.740]   Meta is that is their core competency.
[00:35:09.740 --> 00:35:13.540]   Yeah, but ironically, the one thing that made threads take off is something they
[00:35:13.540 --> 00:35:17.300]   stole from TikTok, which is a pure algorithmic feed as that of all feet.
[00:35:17.740 --> 00:35:22.540]   And you know what had had people known about that in Google's time,
[00:35:22.540 --> 00:35:25.620]   Google plus time, maybe they could have saved Google plus with that.
[00:35:25.620 --> 00:35:27.100]   That's how YouTube works.
[00:35:27.100 --> 00:35:30.580]   When you go to YouTube, you don't have to follow a channel before you can see
[00:35:30.580 --> 00:35:32.580]   the videos it recommends immediately.
[00:35:32.580 --> 00:35:37.180]   And so maybe YouTube gets credit for inventing that certainly TikTok perfected
[00:35:37.180 --> 00:35:41.820]   it and threads is doing exactly what TikTok does, which is you don't have to
[00:35:41.820 --> 00:35:43.300]   have any friends on threads.
[00:35:43.300 --> 00:35:46.260]   You just go and there's stuff already happening.
[00:35:46.260 --> 00:35:47.980]   You can see people, you can add them easily.
[00:35:47.980 --> 00:35:52.780]   They have everybody's head has a plus sign in it, which means you just tap their
[00:35:52.780 --> 00:35:54.260]   head and now you follow them.
[00:35:54.260 --> 00:36:04.900]   They, if anything, Meta has learned the lesson from TikTok and Google and what
[00:36:04.900 --> 00:36:06.580]   Twitter did right and what Twitter did wrong.
[00:36:06.580 --> 00:36:08.420]   And this is the neck to me.
[00:36:08.420 --> 00:36:13.340]   This is next generation social using everything we've learned over the last 10
[00:36:13.340 --> 00:36:15.300]   years. Google plus didn't have the advantage of that.
[00:36:15.300 --> 00:36:16.820]   They were at the other end of those 10 years.
[00:36:16.820 --> 00:36:25.340]   I think in my opinion, threads by cleverly using, they can go wrong.
[00:36:25.340 --> 00:36:28.340]   And they still have a long way to go.
[00:36:28.340 --> 00:36:34.260]   But I think if they make the right steps, this is the next big thing.
[00:36:34.260 --> 00:36:36.140]   I don't have any doubt about that.
[00:36:36.140 --> 00:36:38.340]   Seth, you haven't made a prediction yet.
[00:36:38.340 --> 00:36:39.180]   What's your prediction?
[00:36:39.180 --> 00:36:41.260]   Yeah, I think I think this is going to be a thing.
[00:36:41.260 --> 00:36:44.860]   I don't know if it's going to be the next big thing, but it's going to be a thing.
[00:36:44.900 --> 00:36:46.660]   Is it? We post next.
[00:36:46.660 --> 00:36:49.340]   Are we posted the big thing on the internet now?
[00:36:49.340 --> 00:36:52.580]   It's just too fragmented and big to have somebody in things.
[00:36:52.580 --> 00:36:54.420]   You can't win the internet anymore.
[00:36:54.420 --> 00:37:00.380]   Until the super talented AI is invented and integrates all of us into one super
[00:37:00.380 --> 00:37:02.100]   being one giant paperclip.
[00:37:02.100 --> 00:37:02.780]   Are you kidding me?
[00:37:02.780 --> 00:37:03.620]   That's the problem.
[00:37:03.620 --> 00:37:07.140]   We'll all be paper clips.
[00:37:07.140 --> 00:37:08.420]   All right.
[00:37:08.420 --> 00:37:09.060]   Yeah.
[00:37:09.060 --> 00:37:14.540]   I I have to say, having seen quite a bit of this in my 30 or 40 years
[00:37:14.540 --> 00:37:20.780]   covering technology, this is I've never seen anything quite like threads.
[00:37:20.780 --> 00:37:24.340]   And I've never seen anything be quite so appealing so quickly.
[00:37:24.340 --> 00:37:28.940]   I joined Instagram on day one and it was very appealing very quickly, but it
[00:37:28.940 --> 00:37:32.380]   didn't have the same kind of resonance.
[00:37:32.380 --> 00:37:35.180]   And I think partly it's because we already know about we know Twitter.
[00:37:35.180 --> 00:37:36.180]   We understand that.
[00:37:36.180 --> 00:37:38.740]   And it's like a Twitter done right.
[00:37:38.740 --> 00:37:42.500]   I I do disagree with you, Ben.
[00:37:42.500 --> 00:37:43.780]   I don't think Twitter has a history.
[00:37:43.780 --> 00:37:45.340]   I think it's over.
[00:37:45.340 --> 00:37:49.380]   I think I think that's like nice and that's a nice spicy take.
[00:37:49.380 --> 00:37:52.540]   I I mean, look, I I did the
[00:37:52.540 --> 00:37:53.940]   Twitter is the next gap.
[00:37:53.940 --> 00:37:56.060]   How about that?
[00:37:56.060 --> 00:38:03.140]   I I do disagree, but it's it depends the way that Elon responds.
[00:38:03.140 --> 00:38:05.460]   And I think Elon is competitive.
[00:38:05.460 --> 00:38:05.660]   Yeah.
[00:38:05.660 --> 00:38:09.180]   What's his history, though, is his petulant and foolish.
[00:38:09.180 --> 00:38:10.180]   It's not brilliant.
[00:38:10.180 --> 00:38:12.220]   You think he's brilliant?
[00:38:12.220 --> 00:38:12.580]   No.
[00:38:13.180 --> 00:38:16.540]   There's he look, look, look, I'm going to try to give some credit, even though, like
[00:38:16.540 --> 00:38:21.740]   overall, like I like embrace threads because I was unhappy with the way Elon ran things.
[00:38:21.740 --> 00:38:25.980]   Like there are things he did, like being able to see the number of impressions on a
[00:38:25.980 --> 00:38:28.980]   tweet that were good moves that did help engagement.
[00:38:28.980 --> 00:38:33.100]   And I posted the exact same thing on both threads and on Twitter, of course, this is a
[00:38:33.100 --> 00:38:37.060]   weekend and my Twitter went super viral.
[00:38:37.060 --> 00:38:39.020]   My threads was like minorly viral.
[00:38:39.300 --> 00:38:43.740]   And so there's a bunch of people who aren't going to change behavior and it's really
[00:38:43.740 --> 00:38:47.540]   hard for them to change behavior and they're not going to go and do it or they don't
[00:38:47.540 --> 00:38:48.340]   trust meta.
[00:38:48.340 --> 00:38:50.660]   And so I think there is a world for both now.
[00:38:50.660 --> 00:38:54.900]   Does do I think that threads will be bigger than Twitter?
[00:38:54.900 --> 00:38:55.780]   Yeah, I do.
[00:38:55.780 --> 00:38:57.180]   I think that's what will happen.
[00:38:57.180 --> 00:39:03.220]   But I do think that there is a world like Elon's too proud to give up like that.
[00:39:03.220 --> 00:39:04.580]   He's not going to go and shut it down.
[00:39:04.580 --> 00:39:08.380]   Like I just can't imagine Twitter going away, at least in the next 10 years.
[00:39:08.380 --> 00:39:13.060]   It had to take, I don't know, you'd have to burn all the servers, which I guess is not
[00:39:13.060 --> 00:39:13.900]   out of the question.
[00:39:13.900 --> 00:39:19.180]   Is this your Twitter's always been tiny compared to Facebook, right?
[00:39:19.180 --> 00:39:22.020]   I mean, so how do you define bigger, right?
[00:39:22.020 --> 00:39:23.180]   It's a good point.
[00:39:23.180 --> 00:39:23.660]   That's a good point.
[00:39:23.660 --> 00:39:25.020]   Yeah, influential, right.
[00:39:25.020 --> 00:39:29.500]   Compared to it's because Twitter is like 300 million daily active users compared to
[00:39:29.500 --> 00:39:30.380]   several.
[00:39:30.380 --> 00:39:30.860]   Yeah.
[00:39:30.860 --> 00:39:34.980]   I mean, Facebook eclipsed that like a decade more than a decade ago.
[00:39:34.980 --> 00:39:35.260]   Right.
[00:39:35.260 --> 00:39:37.180]   I mean, it's not your numbers.
[00:39:37.180 --> 00:39:37.780]   I think it's the.
[00:39:37.780 --> 00:39:39.820]   I think it's mind share, right?
[00:39:39.820 --> 00:39:40.380]   I'll give it.
[00:39:40.380 --> 00:39:41.460]   Let's say mind share.
[00:39:41.460 --> 00:39:41.860]   Yeah.
[00:39:41.860 --> 00:39:43.460]   Yeah.
[00:39:43.460 --> 00:39:44.060]   Right.
[00:39:44.060 --> 00:39:46.420]   I mean, I guess what the numbers will be.
[00:39:46.420 --> 00:39:47.700]   I guess I'm on grandpa's.
[00:39:47.700 --> 00:39:50.340]   Meta's got the mind share Facebook has a mind share.
[00:39:50.340 --> 00:39:53.420]   And I say this as somebody old enough to be a grandpa.
[00:39:53.420 --> 00:39:58.260]   But Twitter's mind share was all with the kind of didgerati.
[00:39:58.260 --> 00:40:01.940]   You you have a politicians too.
[00:40:01.940 --> 00:40:02.540]   Yeah.
[00:40:02.540 --> 00:40:03.100]   No, that's true.
[00:40:03.100 --> 00:40:05.820]   But I know politician wants no, no, no.
[00:40:06.260 --> 00:40:09.260]   Non right wing politician wants to be on Twitter anymore.
[00:40:09.260 --> 00:40:09.740]   Right.
[00:40:09.740 --> 00:40:14.220]   It's become a haven for the right, which isn't going to bode well.
[00:40:14.220 --> 00:40:15.460]   And you don't get those people back.
[00:40:15.460 --> 00:40:16.780]   AOC is not going to come.
[00:40:16.780 --> 00:40:18.300]   Oh, yeah, I should be back on Twitter.
[00:40:18.300 --> 00:40:20.260]   That isn't going to happen.
[00:40:20.260 --> 00:40:25.220]   Biden still posts there, but that's because, you know, he's the president of all the people.
[00:40:25.220 --> 00:40:29.260]   Well, I mean, if voters are on Twitter, they'll go on Twitter, won't they?
[00:40:29.260 --> 00:40:29.580]   Yeah.
[00:40:29.580 --> 00:40:30.180]   I guess so.
[00:40:30.180 --> 00:40:30.660]   Yeah.
[00:40:30.660 --> 00:40:33.220]   Don't give me too proud to just ignore.
[00:40:34.820 --> 00:40:39.940]   One thing here that we are also still forgetting, which is threads are still mostly US.
[00:40:39.940 --> 00:40:41.380]   It's not even available to EU.
[00:40:41.380 --> 00:40:47.180]   And when that happens, it may never be available to you because of privacy concerns.
[00:40:47.180 --> 00:40:52.380]   And if it is not available to you for some reason, then Twitter still does absolutely have a future.
[00:40:52.380 --> 00:40:52.820]   That's a good point.
[00:40:52.820 --> 00:40:53.780]   Look, they're going to figure it out.
[00:40:53.780 --> 00:40:55.580]   Threads will be there at some point.
[00:40:55.580 --> 00:40:57.180]   They will figure it out.
[00:40:57.180 --> 00:40:58.700]   It's just going to happen.
[00:40:58.700 --> 00:41:04.740]   But in the interim, you know, Twitter, like there's reasons why Twitter will still be
[00:41:04.740 --> 00:41:08.780]   around. And even if it's just internationally in certain countries, Twitter is just
[00:41:08.780 --> 00:41:11.860]   immensely popular in places like Japan.
[00:41:11.860 --> 00:41:17.300]   And who knows, like if threads will become popular in a place like Japan, I don't know
[00:41:17.300 --> 00:41:18.060]   enough of the numbers.
[00:41:18.060 --> 00:41:22.780]   It's a really interesting, complex topic all around.
[00:41:22.780 --> 00:41:24.500]   Big in Japan.
[00:41:24.500 --> 00:41:26.500]   Yeah, I'm big in Japan.
[00:41:26.500 --> 00:41:26.820]   Yeah.
[00:41:26.820 --> 00:41:33.900]   I guess because you publish and our editor in chief of electric
[00:41:33.900 --> 00:41:38.260]   success, you might have a higher regard for Elon than I do.
[00:41:38.260 --> 00:41:40.340]   That's not correct.
[00:41:40.340 --> 00:41:43.620]   Just giving you the option.
[00:41:43.620 --> 00:41:50.740]   We used to have a pretty good relationship with Tesla and and Elon as well.
[00:41:50.740 --> 00:41:57.940]   And we wrote something about when they put out the hardware, two point five.
[00:41:57.940 --> 00:42:00.820]   So test is going in the weeds a little bit.
[00:42:00.980 --> 00:42:06.780]   Tesla's had when when the Model 3 came out, the hardware for like self driving
[00:42:06.780 --> 00:42:08.100]   was 2.5.
[00:42:08.100 --> 00:42:08.420]   Right.
[00:42:08.420 --> 00:42:13.620]   And when the Model 3 was launched, Elon and Tesla even wrote a blog post about it
[00:42:13.620 --> 00:42:18.500]   said, every car we build from now on is going to be capable of full self driving.
[00:42:18.500 --> 00:42:20.780]   Well, you know, obviously that didn't come true.
[00:42:20.780 --> 00:42:27.940]   But in the interim, Tesla released a 3.0 version of the hardware that everybody
[00:42:27.940 --> 00:42:30.700]   was getting and actually did full self driving a little bit.
[00:42:30.740 --> 00:42:35.020]   Like the, you know, the beta stuff that, you know, you only crashed every 30 minutes
[00:42:35.020 --> 00:42:35.420]   or whatever.
[00:42:35.420 --> 00:42:44.140]   So we wrote a story saying, Hey, you know, Tesla promised this in 2018, said
[00:42:44.140 --> 00:42:47.780]   every car being built from now on is going to have full self, you know, the
[00:42:47.780 --> 00:42:50.580]   cap, hardware capable of doing full self driving.
[00:42:50.580 --> 00:42:52.180]   What are you going to do about it?
[00:42:52.180 --> 00:42:55.580]   Because that was, you know, that was what you said.
[00:42:56.020 --> 00:43:03.780]   And that that day, Elon blocked, electric blocked the wrong rider, but one of the
[00:43:03.780 --> 00:43:10.460]   riders of electric and never like we were like persona on Grotta at that point.
[00:43:10.460 --> 00:43:10.460]   Yeah.
[00:43:10.460 --> 00:43:11.660]   How dare you call him out?
[00:43:11.660 --> 00:43:12.020]   Yeah.
[00:43:12.020 --> 00:43:19.140]   Well, somebody got $5,000 for for the potential right to buy full self driving on my
[00:43:19.140 --> 00:43:19.980]   Model X.
[00:43:19.980 --> 00:43:23.060]   And that was $5,000 fully wasted.
[00:43:23.740 --> 00:43:25.860]   I'm a little sympathetic.
[00:43:25.860 --> 00:43:30.980]   Would you keep the details of your purchase and sell the car?
[00:43:30.980 --> 00:43:33.300]   I think at some point in the future, there'll be a settlement.
[00:43:33.300 --> 00:43:33.900]   You think?
[00:43:33.900 --> 00:43:34.540]   Yeah, maybe.
[00:43:34.540 --> 00:43:34.980]   Yeah.
[00:43:34.980 --> 00:43:35.940]   Yeah, maybe for sure.
[00:43:35.940 --> 00:43:36.300]   Yeah.
[00:43:36.300 --> 00:43:41.900]   That has, well, this is actually one of the things that like also like why threads
[00:43:41.900 --> 00:43:48.380]   is working because if you like publicly criticized Elon on Twitter, you actually
[00:43:48.380 --> 00:43:52.100]   have to worry about the risk of just a secret versus a button and you're not
[00:43:52.100 --> 00:43:53.100]   seen anymore.
[00:43:53.100 --> 00:43:57.540]   And look, for all the things about like Zuck, like I'm not worried about that.
[00:43:57.540 --> 00:44:01.740]   If I were to criticize Zuck on threads, I don't have the worry that they're
[00:44:01.740 --> 00:44:04.260]   going to throttle me for some like reason.
[00:44:04.260 --> 00:44:09.020]   And that that is a big thing despite the fact that like they want Twitter to feel
[00:44:09.020 --> 00:44:10.100]   like free speech.
[00:44:10.100 --> 00:44:13.580]   It's like that's like part of the pitch.
[00:44:13.580 --> 00:44:15.660]   It doesn't feel that way for.
[00:44:15.660 --> 00:44:16.260]   Obviously.
[00:44:16.260 --> 00:44:18.300]   It's I'm for a lot of people.
[00:44:18.300 --> 00:44:24.380]   Back in 2014, when I put it in an order for my Model X, there was a real chilling
[00:44:24.380 --> 00:44:29.660]   effect because Elon had canceled a journalist's order because he was critical of
[00:44:29.660 --> 00:44:34.780]   Elon and it was like, oh man, I could lose my order.
[00:44:34.780 --> 00:44:38.820]   I could lose my place in line if I'm critical of Elon.
[00:44:38.820 --> 00:44:41.900]   I mean, this is, there's a long, that was eight years ago.
[00:44:41.900 --> 00:44:43.300]   There's a long history of this.
[00:44:43.300 --> 00:44:44.340]   Yeah.
[00:44:44.340 --> 00:44:44.580]   With the.
[00:44:44.980 --> 00:44:49.860]   When he started pulling journalists off Twitter for the private jet location thing,
[00:44:49.860 --> 00:44:54.740]   I think that was a turning point where it's just that the Twitter was not going to be
[00:44:54.740 --> 00:44:55.260]   the same.
[00:44:55.260 --> 00:45:00.220]   I don't, I don't agree that it's going to go away completely, but I do think it,
[00:45:00.220 --> 00:45:03.420]   it sort of changed its, its whole meaning.
[00:45:03.420 --> 00:45:10.740]   And incidentally, just kind of related, Elon Musk's jet is now on, on, on threads.
[00:45:10.740 --> 00:45:14.940]   So if you, if you want to know where Elon is, it's, it's on threads.
[00:45:15.300 --> 00:45:16.700]   He moved to Mastinum briefly.
[00:45:16.700 --> 00:45:20.740]   I don't know if he's still there, but clearly he knows this is the place to be.
[00:45:20.740 --> 00:45:21.660]   This is where the people are.
[00:45:21.660 --> 00:45:25.900]   One other quick, one of their follow up read, read your top mentioned in paper.
[00:45:25.900 --> 00:45:26.740]   Do you like people?
[00:45:26.740 --> 00:45:31.540]   I, yeah, I was on the wait list for like two years.
[00:45:31.540 --> 00:45:32.260]   I have been to.
[00:45:32.260 --> 00:45:32.820]   Yeah.
[00:45:32.820 --> 00:45:33.380]   Yeah.
[00:45:33.380 --> 00:45:39.460]   I was really excited when I like suddenly randomly got a, you know, a message that
[00:45:39.460 --> 00:45:41.820]   was off the waitlist and it's, I think it works great.
[00:45:42.220 --> 00:45:46.380]   My only problem is now I get, I'm not ready yet to turn off like notifications for
[00:45:46.380 --> 00:45:47.180]   the other apps.
[00:45:47.180 --> 00:45:51.500]   So now I get to notifications for everything, which is sort of annoying, but
[00:45:51.500 --> 00:45:53.340]   that's like, I just haven't been.
[00:45:53.340 --> 00:45:59.700]   This is like a pigeon or there were a bunch of back in the days of early days of
[00:45:59.700 --> 00:46:03.420]   messaging apps, like ICQ and AIM.
[00:46:03.420 --> 00:46:07.740]   There were a number of apps that would get all, all of them.
[00:46:07.740 --> 00:46:12.060]   That's the thing that's interesting about this is they've got a message.
[00:46:12.060 --> 00:46:14.260]   So you can like, I message from other people.
[00:46:14.260 --> 00:46:14.860]   You see that.
[00:46:14.860 --> 00:46:16.300]   There's, I mean, I don't know how they're doing it.
[00:46:16.300 --> 00:46:18.580]   They say they have Twitter too, which there's no API.
[00:46:18.580 --> 00:46:19.620]   So I don't know.
[00:46:19.620 --> 00:46:20.260]   Yeah, they do.
[00:46:20.260 --> 00:46:20.740]   It works.
[00:46:20.740 --> 00:46:23.580]   I mean, it's, they must be scraping it or something.
[00:46:23.580 --> 00:46:26.580]   How could they know how they can't scrape by messages?
[00:46:26.580 --> 00:46:29.580]   Like, how does Apple like they've somehow got?
[00:46:29.580 --> 00:46:31.340]   Oh, I remember with Beeper.
[00:46:31.340 --> 00:46:33.020]   Didn't they give you an iPhone?
[00:46:33.020 --> 00:46:33.500]   Didn't they?
[00:46:33.500 --> 00:46:34.820]   Didn't you have to get that?
[00:46:34.820 --> 00:46:36.100]   That was originally how they did it.
[00:46:36.100 --> 00:46:36.540]   Yeah.
[00:46:36.540 --> 00:46:39.340]   But they're not enough to get out of way to do it without that.
[00:46:39.340 --> 00:46:39.980]   It works.
[00:46:39.980 --> 00:46:40.380]   Yeah.
[00:46:40.380 --> 00:46:41.540]   It works across platforms.
[00:46:41.540 --> 00:46:43.260]   So you can I message there.
[00:46:43.260 --> 00:46:45.620]   Well, I tried some other solutions too.
[00:46:45.620 --> 00:46:48.020]   I was just interested in there.
[00:46:48.020 --> 00:46:50.620]   There are some other ways to do it, but this is definitely the easiest.
[00:46:50.620 --> 00:46:51.860]   All right.
[00:46:51.860 --> 00:46:52.860]   Well, I'm glad to get the update.
[00:46:52.860 --> 00:46:53.060]   Yeah.
[00:46:53.060 --> 00:46:55.700]   I asked for an invite a long time ago.
[00:46:55.700 --> 00:46:57.340]   I should check my email.
[00:46:57.340 --> 00:46:58.580]   I stopped looking.
[00:46:58.580 --> 00:47:01.340]   All right.
[00:47:01.340 --> 00:47:01.940]   Well, I have more.
[00:47:01.940 --> 00:47:04.900]   I think on the Elon thing, I just want to be one point on the Elon.
[00:47:04.900 --> 00:47:05.180]   Please.
[00:47:05.180 --> 00:47:07.020]   I mean, I agree.
[00:47:07.020 --> 00:47:11.260]   Like he's, you know, he's childish and, and you know, sort of.
[00:47:11.700 --> 00:47:17.180]   I think petty at times and reminds me of, I, you know, I wrote a book about the
[00:47:17.180 --> 00:47:22.860]   Lance Armstrong and a lot of that reminds me of Armstrong too, the way he treats his
[00:47:22.860 --> 00:47:27.620]   critics, but, but I mean, you know, the guys like you have to have some respect
[00:47:27.620 --> 00:47:28.860]   for what he's accomplished, right?
[00:47:28.860 --> 00:47:32.060]   And I think, yeah, I mean, Ben was kind of making his point too.
[00:47:32.060 --> 00:47:37.060]   But I mean, the guy is like, I would not count him out on Twitter.
[00:47:37.060 --> 00:47:40.940]   If that means like him stepping back or whatever, whatever he has to do.
[00:47:41.940 --> 00:47:44.940]   Um, it's, I, I'm not ready to write it off.
[00:47:44.940 --> 00:47:45.940]   Oh my God.
[00:47:45.940 --> 00:47:47.940]   I got a beeper invite last month.
[00:47:47.940 --> 00:47:49.940]   I was just signing up.
[00:47:49.940 --> 00:47:51.940]   I just checked my spam.
[00:47:51.940 --> 00:47:53.940]   I just took my spam.
[00:47:53.940 --> 00:47:55.940]   Thank you, Reed.
[00:47:55.940 --> 00:47:56.940]   Thank you.
[00:47:56.940 --> 00:47:58.940]   Downloading it right now.
[00:47:58.940 --> 00:47:59.940]   Those are amazing.
[00:47:59.940 --> 00:48:00.940]   You invited me.
[00:48:00.940 --> 00:48:05.940]   Well, I just gave up and, uh, you know, stop looking for the invite.
[00:48:05.940 --> 00:48:08.940]   And I just checked it was in my spam folder, but it's there.
[00:48:08.940 --> 00:48:09.940]   Wow.
[00:48:09.940 --> 00:48:10.940]   So I'm downloading it right.
[00:48:10.940 --> 00:48:11.940]   It's on Linux too.
[00:48:11.940 --> 00:48:12.940]   I'm downloading it right now.
[00:48:12.940 --> 00:48:18.500]   That would be huge if I could get Android and messages and, uh, I message and slack and
[00:48:18.500 --> 00:48:20.660]   everything all in one app on Linux.
[00:48:20.660 --> 00:48:21.660]   That'd be amazing.
[00:48:21.660 --> 00:48:22.660]   Yeah.
[00:48:22.660 --> 00:48:29.180]   Let me know how I will our show today brought to you by Mint Mobile.
[00:48:29.180 --> 00:48:33.020]   Love these guys from the gas pump to the grocery store.
[00:48:33.020 --> 00:48:37.500]   Your utility bills, your favorite streaming services, everything costs more these days,
[00:48:37.500 --> 00:48:38.500]   right?
[00:48:38.500 --> 00:48:39.500]   Make it stop.
[00:48:39.500 --> 00:48:42.220]   That's going the one thing that's going the opposite direction.
[00:48:42.220 --> 00:48:43.460]   A much needed break.
[00:48:43.460 --> 00:48:44.940]   It's Mint Mobile.
[00:48:44.940 --> 00:48:49.660]   Mint Mobile is the first company to sell premium wireless service online only.
[00:48:49.660 --> 00:48:52.060]   They pass the savings along to you.
[00:48:52.060 --> 00:48:53.780]   No retail stores.
[00:48:53.780 --> 00:48:54.980]   They let you order from home.
[00:48:54.980 --> 00:48:56.940]   You save a ton.
[00:48:56.940 --> 00:48:59.700]   Their phone plan started just $15 a month.
[00:48:59.700 --> 00:49:04.100]   I've been a Mint Mobile user for two years now.
[00:49:04.100 --> 00:49:06.100]   Saved a huge amount of money compared.
[00:49:06.100 --> 00:49:11.100]   You know, I have, I have to for work have accounts with Verizon, AT&T and T-Mobile.
[00:49:11.100 --> 00:49:15.980]   And I have to say, if I add up how much I'm spending for those guys and how much for Mint
[00:49:15.980 --> 00:49:20.820]   Mobile, I've saved thousands of dollars with Mint Mobile.
[00:49:20.820 --> 00:49:23.140]   And yes, they have inflation, but it's going the other way.
[00:49:23.140 --> 00:49:29.260]   Mint Mobile's inflation is they give you now more data for your $15 a month than before.
[00:49:29.260 --> 00:49:33.340]   For everyone looking for extra savings this year, Mint Mobile offers premium wireless for
[00:49:33.340 --> 00:49:37.180]   just $15 a month.
[00:49:37.180 --> 00:49:41.420]   By eliminating the traditional cost of retail selling online only, they pass significant
[00:49:41.420 --> 00:49:42.740]   savings on to you.
[00:49:42.740 --> 00:49:44.260]   Now, let me tell you what you get.
[00:49:44.260 --> 00:49:49.700]   All plans come with unlimited talk and text nationwide plus high speed data delivered in
[00:49:49.700 --> 00:49:53.620]   the nation's largest 5G network.
[00:49:53.620 --> 00:49:54.620]   You can use your own phone.
[00:49:54.620 --> 00:50:00.340]   They'll send you a SIM or use an ECM and you'll be up and running on your existing phone for
[00:50:00.340 --> 00:50:03.260]   no charge or buy a phone from them.
[00:50:03.260 --> 00:50:04.820]   I bought an iPhone SE from them.
[00:50:04.820 --> 00:50:06.900]   It was very affordable.
[00:50:06.900 --> 00:50:12.100]   And with the $15 a month plan, man, I was saving big time.
[00:50:12.100 --> 00:50:16.580]   Mint Mobile, switch to Mint Mobile, you get premium wireless service starting at $15 a
[00:50:16.580 --> 00:50:17.580]   month.
[00:50:17.580 --> 00:50:18.580]   Bring your own phone.
[00:50:18.580 --> 00:50:22.020]   You could port your phone number ever, keep your contacts.
[00:50:22.020 --> 00:50:24.220]   It's just a great deal.
[00:50:24.220 --> 00:50:27.900]   In fact, honestly, you'd be crazy not to do this.
[00:50:27.900 --> 00:50:30.660]   Get your new wireless plan for just $15 a month.
[00:50:30.660 --> 00:50:31.660]   That's all in.
[00:50:31.660 --> 00:50:33.380]   How much are you spending right now?
[00:50:33.380 --> 00:50:34.700]   Get the plan shipped to your door for free.
[00:50:34.700 --> 00:50:36.700]   Go to mintmobile.com/twit.
[00:50:36.700 --> 00:50:43.580]   Mint, M-I-N-T, like that minty fresh flavor, mintmobile.com/twit.
[00:50:43.580 --> 00:50:46.300]   I even wear mint green mint mobile socks to celebrate it.
[00:50:46.300 --> 00:50:48.820]   I'm wearing a mint green shirt today.
[00:50:48.820 --> 00:50:54.060]   Cut your wireless bill to $15 a month, mintmobile.com/twit.
[00:50:54.060 --> 00:50:59.420]   Thank you so much for their support.
[00:50:59.420 --> 00:51:02.100]   Reid Albergotti is here from Semaphore.
[00:51:02.100 --> 00:51:03.740]   Ben Parr, Ben, you got a new column.
[00:51:03.740 --> 00:51:07.300]   I'm so happy for you on the information on about AI.
[00:51:07.300 --> 00:51:08.300]   Tell me about the book, though.
[00:51:08.300 --> 00:51:10.660]   What is this book?
[00:51:10.660 --> 00:51:11.860]   The book, which one?
[00:51:11.860 --> 00:51:13.660]   The AI analyst.
[00:51:13.660 --> 00:51:15.900]   Oh, that is my column.
[00:51:15.900 --> 00:51:17.900]   That's the column.
[00:51:17.900 --> 00:51:18.900]   Oh.
[00:51:18.900 --> 00:51:20.820]   I have two.
[00:51:20.820 --> 00:51:26.380]   I have my column on the information, so you can go to the information and just like find
[00:51:26.380 --> 00:51:27.380]   my name.
[00:51:27.380 --> 00:51:28.860]   I have my first two columns up there.
[00:51:28.860 --> 00:51:29.860]   It's a new thing.
[00:51:29.860 --> 00:51:32.340]   Oh, and then you have a sub-stack kind of, or is it a sub-stack?
[00:51:32.340 --> 00:51:33.340]   What is it?
[00:51:33.340 --> 00:51:34.340]   It's a beehive.
[00:51:34.340 --> 00:51:35.340]   Beehive.
[00:51:35.340 --> 00:51:40.180]   Hey, how do you like beehive compared to sub-stack?
[00:51:40.180 --> 00:51:43.860]   For me who wants a tinker, it's much better because you can tinker and you can change things.
[00:51:43.860 --> 00:51:44.860]   I'm hearing a lot.
[00:51:44.860 --> 00:51:48.660]   Beehive's new, but I've hearing a lot of good things about it compared to the other newsletter
[00:51:48.660 --> 00:51:49.660]   platforms.
[00:51:49.660 --> 00:51:53.780]   Oh, look at all the stuff you've written here.
[00:51:53.780 --> 00:51:57.620]   And you pretty much, by the way, there's your appearance on episode 915.
[00:51:57.620 --> 00:52:02.140]   You pretty much focus on AI nowadays?
[00:52:02.140 --> 00:52:09.020]   Focus mostly, although my most recent one about threads did upset some people.
[00:52:09.020 --> 00:52:11.380]   Elon should be worried?
[00:52:11.380 --> 00:52:14.140]   Yes, that one upset some people.
[00:52:14.140 --> 00:52:15.140]   That's fine.
[00:52:15.140 --> 00:52:20.260]   Look, I'm going to still occasionally drop in on other topics, but AI is the thing that
[00:52:20.260 --> 00:52:22.220]   I've been talking about.
[00:52:22.220 --> 00:52:25.300]   For those who don't know, I started an AI company seven years ago.
[00:52:25.300 --> 00:52:31.780]   It's fascinating how you suddenly become part of a hot trend and I am not complaining,
[00:52:31.780 --> 00:52:35.620]   but also I've been doing it for a very long time and thinking about these topics for a
[00:52:35.620 --> 00:52:36.620]   very long time.
[00:52:36.620 --> 00:52:37.620]   More than anything but-
[00:52:37.620 --> 00:52:38.620]   So it's nice to actually be able to write about it.
[00:52:38.620 --> 00:52:39.780]   Yeah, you really have been.
[00:52:39.780 --> 00:52:42.580]   And congratulations to you and Deborah.
[00:52:42.580 --> 00:52:46.180]   We had dinner with you guys when you were in town a while ago.
[00:52:46.180 --> 00:52:51.260]   She's a playwright and fabulous and you're very lucky man.
[00:52:51.260 --> 00:52:52.260]   That's great.
[00:52:52.260 --> 00:52:53.260]   Congratulations.
[00:52:53.260 --> 00:52:54.260]   That's really-
[00:52:54.260 --> 00:52:55.260]   Thank you.
[00:52:55.260 --> 00:53:01.580]   Benpart.com for your Beehive, Obehive baby.
[00:53:01.580 --> 00:53:06.460]   So I'm sorry I said that.
[00:53:06.460 --> 00:53:12.100]   Wired Magazine says today, why we don't recommend ring cameras.
[00:53:12.100 --> 00:53:14.180]   Kind of a big deal.
[00:53:14.180 --> 00:53:15.180]   Wired does testing.
[00:53:15.180 --> 00:53:19.460]   Kind of like the wire cutter and everybody else.
[00:53:19.460 --> 00:53:26.940]   They say, well, yeah, ring cameras from Amazon are affordable and ubiquitous, but we don't
[00:53:26.940 --> 00:53:30.260]   think homeowners should be able to act as vigilantes.
[00:53:30.260 --> 00:53:31.260]   Yikes.
[00:53:31.260 --> 00:53:38.620]   When you set up a ring camera, it says wired, you automatically are enrolled in their neighbors
[00:53:38.620 --> 00:53:39.820]   service.
[00:53:39.820 --> 00:53:42.980]   You can turn it off, but of course it's the default.
[00:53:42.980 --> 00:53:45.780]   So nobody does.
[00:53:45.780 --> 00:53:54.060]   Neighbors, which is a standalone app, shows you activity from all nearby ring camera owners.
[00:53:54.060 --> 00:53:59.940]   And a safety report that shows how many calls for safety services were made in the past
[00:53:59.940 --> 00:54:02.460]   week in your neighborhood.
[00:54:02.460 --> 00:54:07.660]   It also allows you as a ring owner to send videos you've captured with your doorbell to
[00:54:07.660 --> 00:54:13.100]   law enforcement.
[00:54:13.100 --> 00:54:15.540]   And they say, hold on there.
[00:54:15.540 --> 00:54:18.180]   That's turning everybody into vigilantes.
[00:54:18.180 --> 00:54:25.900]   We believe this feature should not exist because of course the biggest problem is it increases
[00:54:25.900 --> 00:54:29.220]   the possibility of racial profiling.
[00:54:29.220 --> 00:54:33.580]   Makes it easier for both private citizens and law enforcement to target certain groups
[00:54:33.580 --> 00:54:40.860]   for suspicion of crime based on skin color, ethnicity, religion, or country of origin.
[00:54:40.860 --> 00:54:43.140]   Thoughts?
[00:54:43.140 --> 00:54:45.660]   Ring says we do it because our customers love it.
[00:54:45.660 --> 00:54:46.980]   And they're probably right.
[00:54:46.980 --> 00:54:47.980]   They're probably right.
[00:54:47.980 --> 00:54:50.660]   You know, I'm sure customers have a ring camera.
[00:54:50.660 --> 00:54:51.660]   I love it.
[00:54:51.660 --> 00:54:52.660]   See, there you go.
[00:54:52.660 --> 00:54:53.660]   Could you turn it off?
[00:54:53.660 --> 00:54:59.060]   I opted out of the neighbors thing, but you know, it's just and it's like, as far as crime
[00:54:59.060 --> 00:55:01.180]   prevention, I think it's completely useless.
[00:55:01.180 --> 00:55:06.180]   Like, I mean, what are they going to do with the footage, like of whatever people stealing
[00:55:06.180 --> 00:55:07.580]   packages or breaking in?
[00:55:07.580 --> 00:55:08.580]   I mean, come on.
[00:55:08.580 --> 00:55:14.020]   But I think it's really useful like when you're not home and somebody, you know, I have a
[00:55:14.020 --> 00:55:18.620]   Google Hello camera and it's just as intrusive, right?
[00:55:18.620 --> 00:55:23.340]   It captures everything going by on the street in front of my house, including neighbors and
[00:55:23.340 --> 00:55:25.660]   people walking their dogs and stuff.
[00:55:25.660 --> 00:55:29.980]   It doesn't offer to send it back to law enforcement.
[00:55:29.980 --> 00:55:32.180]   I used to be, I used to use name.
[00:55:32.180 --> 00:55:33.940]   Was it next door?
[00:55:33.940 --> 00:55:35.500]   Used to use next door.
[00:55:35.500 --> 00:55:37.460]   And that ended up really scaring people.
[00:55:37.460 --> 00:55:39.380]   What was the other one that was really terrifying?
[00:55:39.380 --> 00:55:42.380]   They would tell you all the law enforcement citizen citizen.
[00:55:42.380 --> 00:55:54.380]   Oh, it does scare you and turn you into kind of a racist, a gorephobe.
[00:55:54.380 --> 00:55:59.300]   I agree with you, but like, isn't that just, isn't that more a problem with people rather
[00:55:59.300 --> 00:56:00.300]   than technology?
[00:56:00.300 --> 00:56:01.300]   Yeah.
[00:56:01.300 --> 00:56:05.460]   I mean, like, I don't, I mean, I use this stuff and I'm not like a racist and I'm not
[00:56:05.460 --> 00:56:08.140]   checking crime stats and being scared all the time.
[00:56:08.140 --> 00:56:11.660]   Like, I shouldn't say this, but half the time my doors aren't even locked.
[00:56:11.660 --> 00:56:13.300]   But like, I don't know.
[00:56:13.300 --> 00:56:15.820]   I mean, it just seems like it's a problem.
[00:56:15.820 --> 00:56:17.300]   It is a people problem.
[00:56:17.300 --> 00:56:18.300]   My stuff.
[00:56:18.300 --> 00:56:19.300]   People are terrible.
[00:56:19.300 --> 00:56:22.660]   But it's like people just people are people, right?
[00:56:22.660 --> 00:56:26.260]   Like they're, you know, whether they have the ring camera or not, they're going to.
[00:56:26.260 --> 00:56:28.660]   Do you have any magic on your ring camera?
[00:56:28.660 --> 00:56:30.100]   See, I think that's cool.
[00:56:30.100 --> 00:56:33.180]   I would go around the neighborhood doing magic.
[00:56:33.180 --> 00:56:35.180]   Yeah.
[00:56:35.180 --> 00:56:38.940]   I had to agree with Reed on that one.
[00:56:38.940 --> 00:56:42.820]   Just for most people, I could like, that's a useful feature.
[00:56:42.820 --> 00:56:47.220]   I have, I have one for my home when I'm like away to just like check on stuff.
[00:56:47.220 --> 00:56:48.580]   Like it's useful.
[00:56:48.580 --> 00:56:51.660]   And like the Wired article doesn't have like, here's the alternative.
[00:56:51.660 --> 00:56:52.660]   Right.
[00:56:52.660 --> 00:56:54.380]   So like, yeah, I need something.
[00:56:54.380 --> 00:56:56.580]   I want something.
[00:56:56.580 --> 00:57:02.780]   And I still fall under the thesis that just the vast majority of people do not care about
[00:57:02.780 --> 00:57:09.260]   some of these like larger societal issues or privacy issues or things like that.
[00:57:09.260 --> 00:57:13.940]   They're like, what can solve my problem of who's at my door and who's trying to steal
[00:57:13.940 --> 00:57:15.300]   my packages?
[00:57:15.300 --> 00:57:16.980]   And Ring does a very good job of it.
[00:57:16.980 --> 00:57:17.980]   All right.
[00:57:17.980 --> 00:57:21.900]   I had a ring camera for a long time and they were an advertiser for a long time before Amazon
[00:57:21.900 --> 00:57:23.180]   bought them.
[00:57:23.180 --> 00:57:25.260]   I do like a doorbell camera.
[00:57:25.260 --> 00:57:27.220]   What do you have, Seth?
[00:57:27.220 --> 00:57:33.020]   So I have a Google one just because we have, you know, a bunch of Google stuff.
[00:57:33.020 --> 00:57:40.060]   I, so I think the problem here is that it defaults to sending video to the cops.
[00:57:40.060 --> 00:57:41.540]   Right.
[00:57:41.540 --> 00:57:46.860]   Because we know like, for instance, I was told that if the default on your license was
[00:57:46.860 --> 00:57:52.140]   organ donor and not, not organ donor, that we wouldn't have any organ donor problems because
[00:57:52.140 --> 00:57:53.500]   everybody would just be like, whatever.
[00:57:53.500 --> 00:57:54.500]   Right.
[00:57:54.500 --> 00:57:55.500]   Yeah.
[00:57:55.500 --> 00:57:56.500]   Right.
[00:57:56.500 --> 00:57:57.780]   So the default is a big deal.
[00:57:57.780 --> 00:58:03.100]   Like, you know, the default web browser on a phone or a default search engine.
[00:58:03.100 --> 00:58:06.180]   That's, that is, you know, the search engine.
[00:58:06.180 --> 00:58:10.660]   So I think Ring could probably say, Hey, look, let's just not make the default or during
[00:58:10.660 --> 00:58:14.460]   this setup, you can choose one or the other.
[00:58:14.460 --> 00:58:18.380]   But you know, I think the default people know that everybody's going to be sending their
[00:58:18.380 --> 00:58:25.540]   video to the cops, maybe cops in some certain areas aren't, you know, the best civil rights
[00:58:25.540 --> 00:58:27.620]   people.
[00:58:27.620 --> 00:58:33.820]   So you know, I understand like, why it's concerned that a lot of cops are going to probably get
[00:58:33.820 --> 00:58:38.500]   a lot of footage that maybe they shouldn't have the right to have by default.
[00:58:38.500 --> 00:58:44.860]   So the easy fixes Ring could just say, you know, during a setup to say, send it to the
[00:58:44.860 --> 00:58:46.780]   cops, don't send it to the cops.
[00:58:46.780 --> 00:58:49.580]   And I'm happy cops are trying to solve crimes at all.
[00:58:49.580 --> 00:58:54.220]   Like I, no, it's not going to hold.
[00:58:54.220 --> 00:58:56.420]   You must live in San Francisco, Reed.
[00:58:56.420 --> 00:59:00.860]   And some areas, I mean, I don't live in San Francisco, but I mean, yeah, like San Francisco
[00:59:00.860 --> 00:59:05.980]   is going through, you know, hell right now, because nobody wants to stop crime.
[00:59:05.980 --> 00:59:07.580]   It's like, yeah, I mean, I don't know.
[00:59:07.580 --> 00:59:09.380]   What are they exactly doing with the footage?
[00:59:09.380 --> 00:59:15.900]   Like, there's so much, I mean, I don't know, like we villainize these cops, you know, and
[00:59:15.900 --> 00:59:18.660]   there are some bad ones, obviously some bad apples.
[00:59:18.660 --> 00:59:23.580]   But I mean, it's like they're probably just trying to get footage to solve some crime.
[00:59:23.580 --> 00:59:24.980]   Like that's what else.
[00:59:24.980 --> 00:59:27.100]   Yeah, I don't mind it so much.
[00:59:27.100 --> 00:59:35.180]   It's just I noticed that it turns people, it turns people into kind of paranoid.
[00:59:35.180 --> 00:59:40.220]   You know, I stopped looking at next door for that reason because it just was like, there's
[00:59:40.220 --> 00:59:43.580]   a strange person, you know, strange black man walking around in the neighborhood.
[00:59:43.580 --> 00:59:44.580]   What is he doing?
[00:59:44.580 --> 00:59:45.580]   What is going on?
[00:59:45.580 --> 00:59:48.940]   It makes you kind of crazy and concerned.
[00:59:48.940 --> 00:59:55.420]   And I think citizen even worse, you start, you start turning into, you know, one of those
[00:59:55.420 --> 00:59:58.060]   people cowering in their homes with shotguns.
[00:59:58.060 --> 01:00:01.420]   Somebody knocks at your door, you blow their heads off.
[01:00:01.420 --> 01:00:03.300]   It's not a good thing.
[01:00:03.300 --> 01:00:05.060]   I turned that notifications off.
[01:00:05.060 --> 01:00:08.100]   I couldn't, I didn't want to have all that negativity.
[01:00:08.100 --> 01:00:09.100]   It's negativity.
[01:00:09.100 --> 01:00:10.100]   Yeah.
[01:00:10.100 --> 01:00:13.260]   And it gets eventually kind of gets into your head, I think you start.
[01:00:13.260 --> 01:00:17.900]   And that's why you should probably turn off network news and 24 hour news channels because
[01:00:17.900 --> 01:00:19.900]   it just gets in your head after a while.
[01:00:19.900 --> 01:00:20.900]   You start out.
[01:00:20.900 --> 01:00:23.180]   I was going to say people used to say the same thing about the news.
[01:00:23.180 --> 01:00:24.180]   Yeah.
[01:00:24.180 --> 01:00:26.580]   You started to think the world is a horrible place.
[01:00:26.580 --> 01:00:29.660]   You watch the news and you think there's way more crime than there actually is.
[01:00:29.660 --> 01:00:33.740]   And then you watch, you know, the cop shows and you think that way more crimes get solved
[01:00:33.740 --> 01:00:34.740]   than actually do.
[01:00:34.740 --> 01:00:35.740]   Right.
[01:00:35.740 --> 01:00:37.540]   You know, your whole view of the world is completely wrong.
[01:00:37.540 --> 01:00:38.540]   Easily skewed.
[01:00:38.540 --> 01:00:42.740]   And I guess the problem is people again, the problem is people, not, not those things, but
[01:00:42.740 --> 01:00:43.740]   still.
[01:00:43.740 --> 01:00:47.340]   We blame technology now for like every problem with society and humanity.
[01:00:47.340 --> 01:00:48.340]   Good point.
[01:00:48.340 --> 01:00:49.140]   And it's like, I don't know.
[01:00:49.140 --> 01:00:53.300]   I mean, we should be asking these questions and I think it's important.
[01:00:53.300 --> 01:00:54.940]   It's an important discussion to have for sure.
[01:00:54.940 --> 01:00:59.940]   I don't mean to minimize it, but I mean, right at some point, we're flawed.
[01:00:59.940 --> 01:01:05.380]   Well, that's why I for one welcome our AI over lords.
[01:01:05.380 --> 01:01:10.940]   You're, they say that AI should have human values, but that scares the hell out.
[01:01:10.940 --> 01:01:11.940]   There's a terrible idea.
[01:01:11.940 --> 01:01:15.940]   Let's hope it does better than we do.
[01:01:15.940 --> 01:01:21.020]   Your, this from the Wall Street Journal, your school's next security guard may be an AI
[01:01:21.020 --> 01:01:22.020]   enabled robot.
[01:01:22.020 --> 01:01:25.460]   Talk about a store, a headline designed to scare you, right?
[01:01:25.460 --> 01:01:34.060]   One school in Santa Fe, Santa Fe High School using these 360 degree camera enabled patrol
[01:01:34.060 --> 01:01:35.060]   robots.
[01:01:35.060 --> 01:01:37.940]   Look, it's got a big loud speaker on it.
[01:01:37.940 --> 01:01:40.940]   Hey, you kids get away from there.
[01:01:40.940 --> 01:01:49.660]   It's among other things, it does face recognition.
[01:01:49.660 --> 01:01:58.100]   It starts to notice when kids gather and when certain kids gather, maybe that's a, I think
[01:01:58.100 --> 01:02:02.780]   some of this is a response to all the school shootings, right?
[01:02:02.780 --> 01:02:09.820]   In Andy Sanchez, who does sales for the robots distributor in North America, team first technologies
[01:02:09.820 --> 01:02:14.620]   says, in the case of an active shooter, the robot could alert the security team.
[01:02:14.620 --> 01:02:19.060]   It could move toward the intruder in a transmit video footage that informs the officer's course
[01:02:19.060 --> 01:02:20.060]   of action.
[01:02:20.060 --> 01:02:25.540]   It's not armed, but it can confront intruders and human security team members would be able
[01:02:25.540 --> 01:02:31.980]   to speak to the intruder through the robots communication system.
[01:02:31.980 --> 01:02:37.940]   They did have, they turned them off weapon detection features.
[01:02:37.940 --> 01:02:39.420]   Does not have face recognition?
[01:02:39.420 --> 01:02:42.460]   I should clarify that.
[01:02:42.460 --> 01:02:45.380]   And the high school maintains control of the footage.
[01:02:45.380 --> 01:02:49.020]   It doesn't go back to the home office.
[01:02:49.020 --> 01:02:52.900]   The robot, according to the journal has not yet detected intruders on campus, but it
[01:02:52.900 --> 01:02:58.540]   has alerted the security team to new workers entering the school's construction site.
[01:02:58.540 --> 01:03:03.540]   Individuals attempting to open locked doors in harmless attempts to enter buildings, I
[01:03:03.540 --> 01:03:04.540]   might add.
[01:03:04.540 --> 01:03:05.540]   It's good.
[01:03:05.540 --> 01:03:06.540]   It's not armed.
[01:03:06.540 --> 01:03:12.140]   It's cameras have also caught faculty members waving and students making peace signs and
[01:03:12.140 --> 01:03:16.980]   passing.
[01:03:16.980 --> 01:03:20.820]   One student said, I don't think the robot will change our behavior.
[01:03:20.820 --> 01:03:25.380]   It'll just be funnier, just different.
[01:03:25.380 --> 01:03:28.100]   And a drama teacher says, I already feel safer.
[01:03:28.100 --> 01:03:29.780]   Oh no, I'm sorry.
[01:03:29.780 --> 01:03:35.700]   I already feel safe at school without the new surveillance measures.
[01:03:35.700 --> 01:03:38.780]   What do you think?
[01:03:38.780 --> 01:03:42.700]   Security robots, I don't know if I want to see this in our anywhere.
[01:03:42.700 --> 01:03:45.820]   Security robots on patrol in this area.
[01:03:45.820 --> 01:03:49.340]   But there's more and more of it happening.
[01:03:49.340 --> 01:03:55.820]   It is cheaper than a human or multiple humans.
[01:03:55.820 --> 01:03:58.380]   But I'm shocked that thing hasn't been vandalized yet.
[01:03:58.380 --> 01:04:01.580]   That's like a $60,000 robot.
[01:04:01.580 --> 01:04:06.660]   Like teenagers could have just like spray paint.
[01:04:06.660 --> 01:04:10.660]   It should be full of TB by the end of the week, I'm sure.
[01:04:10.660 --> 01:04:14.940]   I mean, interestingly, I think a lot of it is just based off of like the design.
[01:04:14.940 --> 01:04:21.940]   And so it actually reminds me of a story I saw recently where people judge different
[01:04:21.940 --> 01:04:27.580]   pieces of art and they changed their rating based off of if they were told it was made
[01:04:27.580 --> 01:04:29.660]   by AI or not.
[01:04:29.660 --> 01:04:35.420]   Whether or not actually AI made it and they wouldn't tell them either way, like the reality.
[01:04:35.420 --> 01:04:42.820]   This thing, it evokes some of those feelings of Terminator style, that sort of thing versus
[01:04:42.820 --> 01:04:48.900]   like, you don't think about just like cameras in like the quarters or like, you know, even
[01:04:48.900 --> 01:04:49.900]   it's true.
[01:04:49.900 --> 01:04:51.500]   They're already there, aren't they?
[01:04:51.500 --> 01:04:52.500]   Yeah.
[01:04:52.500 --> 01:04:53.500]   Right.
[01:04:53.500 --> 01:04:58.020]   So it's the form factor that like gets people and that like maybe you could turn it into
[01:04:58.020 --> 01:04:59.020]   a puppy.
[01:04:59.020 --> 01:05:04.740]   You know, but it's going to, it'll be tested more and more and it'll get more and more
[01:05:04.740 --> 01:05:07.020]   sophisticated and it'll get cheaper and cheaper.
[01:05:07.020 --> 01:05:15.220]   But to be widespread adoption feels decades away, if ever, because like the other pieces,
[01:05:15.220 --> 01:05:20.340]   like a robot can't solve some of the human problems that people have.
[01:05:20.340 --> 01:05:23.740]   So interesting supplement, interesting experiment.
[01:05:23.740 --> 01:05:29.620]   The biggest one is the stat indictment of the state of American safety in American schools.
[01:05:29.620 --> 01:05:32.140]   That is my political perch.
[01:05:32.140 --> 01:05:34.620]   Fair enough.
[01:05:34.620 --> 01:05:37.420]   We were talking about speaking of AI.
[01:05:37.420 --> 01:05:44.100]   We were talking about today on the Ask the Tech guy, Samable Samad, our car guy was on
[01:05:44.100 --> 01:05:51.020]   talking about San Francisco, which is inundated with the self-driving Waymo and Cruz robo
[01:05:51.020 --> 01:05:52.020]   taxis.
[01:05:52.020 --> 01:05:58.820]   San Franciscans have discovered that the way to disable them is to put an orange cone on
[01:05:58.820 --> 01:06:07.220]   its hood because the vehicle thinks, you know, oh, that's a traffic safety area, but it can't
[01:06:07.220 --> 01:06:08.820]   drive around it.
[01:06:08.820 --> 01:06:13.620]   So it just stops.
[01:06:13.620 --> 01:06:18.660]   Let me, this is a TikTok video that describes this technique.
[01:06:18.660 --> 01:06:20.540]   And I don't know.
[01:06:20.540 --> 01:06:21.980]   We're kind of distant from San Francisco.
[01:06:21.980 --> 01:06:27.420]   I did see, when I was in the city yesterday, I did see quite a few Waymo's and Cruz's
[01:06:27.420 --> 01:06:28.420]   and they do see.
[01:06:28.420 --> 01:06:35.220]   But see, if you just put a cone, just find a traffic cone.
[01:06:35.220 --> 01:06:39.340]   They're all all over the place, of course, but don't take it from anywhere important.
[01:06:39.340 --> 01:06:42.940]   Then gently, they say, place it on the hood.
[01:06:42.940 --> 01:06:46.460]   And you know, as long as there's nobody in the car, obviously, if there's a safety driver,
[01:06:46.460 --> 01:06:48.340]   he'll come and take it off the hood.
[01:06:48.340 --> 01:06:53.340]   But if it's just driving by itself, it's terrible.
[01:06:53.340 --> 01:06:55.700]   It just said,
[01:06:55.700 --> 01:06:58.740]   Mine said he almost did that the other day, but then he felt bad because it would have
[01:06:58.740 --> 01:07:01.740]   caused a traffic jam.
[01:07:01.740 --> 01:07:10.860]   Oh, yeah, that's the problem because that disabled Cruz is just sitting there in traffic.
[01:07:10.860 --> 01:07:12.620]   And that we see enough of that anyway.
[01:07:12.620 --> 01:07:20.220]   I'm sure Google and or Waymo and Cruz will send a software update that tells it to back
[01:07:20.220 --> 01:07:24.180]   up really quickly and it'll fall off.
[01:07:24.180 --> 01:07:26.340]   Wiggle, wiggle the hood.
[01:07:26.340 --> 01:07:27.340]   Yeah.
[01:07:27.340 --> 01:07:30.180]   It's a robotic arm to take it off.
[01:07:30.180 --> 01:07:31.180]   Right.
[01:07:31.180 --> 01:07:32.180]   It's the Ben's point though.
[01:07:32.180 --> 01:07:35.900]   Some people feel really like, I don't know, like violated somehow because they're these
[01:07:35.900 --> 01:07:41.380]   like robo taxis just driving around their city and you know, like who's responsible,
[01:07:41.380 --> 01:07:46.300]   who's accountable and you know, so far there haven't been any like, you know, major safety.
[01:07:46.300 --> 01:07:47.300]   Like, you know,
[01:07:47.300 --> 01:07:48.300]   There's been small ones though.
[01:07:48.300 --> 01:07:53.780]   There were emergency vehicles were blocked by Cruz and San Francisco some weeks ago.
[01:07:53.780 --> 01:07:57.340]   I was talking to a friend who lives on a hilly area of San Francisco.
[01:07:57.340 --> 01:08:01.700]   He said, yeah, for like several weeks, a lot of a lot of self driving vehicles.
[01:08:01.700 --> 01:08:06.060]   I guess they were training on the hills would we just were all over the place.
[01:08:06.060 --> 01:08:09.860]   Then there are people who live in San Francisco and call the sex.
[01:08:09.860 --> 01:08:17.180]   And because the Cruz vehicles follow traffic laws, you know, any any any human will just
[01:08:17.180 --> 01:08:21.220]   go, okay, I am turning left here because I'm not going down that cul-de-sac, but these
[01:08:21.220 --> 01:08:25.180]   vehicles go down the cul-de-sac, make a U-turn and then come back out so they don't make
[01:08:25.180 --> 01:08:26.180]   a left turn.
[01:08:26.180 --> 01:08:31.940]   They only make right turns and it and they've been lined up in the cul-de-sac like dozens
[01:08:31.940 --> 01:08:35.340]   of them making this this crazy turn.
[01:08:35.340 --> 01:08:38.460]   I think they must have fixed that because I haven't heard about that in a while, but
[01:08:38.460 --> 01:08:41.620]   they're not it's not merely that.
[01:08:41.620 --> 01:08:43.900]   Oh, the creeps me out.
[01:08:43.900 --> 01:08:47.380]   They can cause problems.
[01:08:47.380 --> 01:08:48.380]   But it's kind of amazing.
[01:08:48.380 --> 01:08:52.060]   They've gotten them to work so well in San Francisco.
[01:08:52.060 --> 01:08:56.180]   I think that the thing that I keep wondering every time and I see them every time I go
[01:08:56.180 --> 01:08:59.500]   in the city for a dinner or whatever, they're just there.
[01:08:59.500 --> 01:09:00.500]   There's more of them at night.
[01:09:00.500 --> 01:09:05.660]   I just see them all the time and yeah thing I keep thinking is like when are they going
[01:09:05.660 --> 01:09:07.300]   to have these in other cities, right?
[01:09:07.300 --> 01:09:12.980]   Like how hard is it to bring this now to like every major metropolitan area?
[01:09:12.980 --> 01:09:18.580]   And I think that's the fact that it hasn't happened yet is like it's either because they're
[01:09:18.580 --> 01:09:23.580]   worried that there's going to be some safety issue and something bad's going to happen
[01:09:23.580 --> 01:09:31.260]   and it's going to shut the whole thing down or those little edge cases in San Francisco
[01:09:31.260 --> 01:09:33.540]   are in every single city and they're different.
[01:09:33.540 --> 01:09:40.460]   And so it's just going to be so difficult to scale this, you know, to other places.
[01:09:40.460 --> 01:09:41.780]   They are in some other places.
[01:09:41.780 --> 01:09:44.900]   I think they're both in Arizona, Phoenix.
[01:09:44.900 --> 01:09:48.540]   Austin has some now.
[01:09:48.540 --> 01:09:49.540]   In fact, Austin.
[01:09:49.540 --> 01:09:51.740]   Yeah, they're just expanding now.
[01:09:51.740 --> 01:09:55.140]   A lot of local laws and those pieces, they're being cautious, right?
[01:09:55.140 --> 01:09:58.660]   You don't want to just like, if you don't want to do what Bird did and just drop a whole
[01:09:58.660 --> 01:10:01.260]   bunch of self-driving cars, happy driving.
[01:10:01.260 --> 01:10:03.020]   Oh, those scooters, you mean?
[01:10:03.020 --> 01:10:04.820]   Oh, yeah, would you now worth?
[01:10:04.820 --> 01:10:05.820]   Oh, yeah.
[01:10:05.820 --> 01:10:08.300]   Went from what, a billion valuation to 25 billion now?
[01:10:08.300 --> 01:10:09.300]   Yeah.
[01:10:09.300 --> 01:10:10.300]   Just the whole another story.
[01:10:10.300 --> 01:10:11.300]   Yeah.
[01:10:11.300 --> 01:10:12.300]   But you don't want to just drop a thing like that.
[01:10:12.300 --> 01:10:17.380]   You got to work with the local government, like Google and the other companies in this
[01:10:17.380 --> 01:10:20.740]   space and crews are much more cautious as they should be.
[01:10:20.740 --> 01:10:21.900]   And so you think they could.
[01:10:21.900 --> 01:10:27.780]   You think it's not, there's no, there's no hurdle to scaling these other than laws and
[01:10:27.780 --> 01:10:28.780]   regulations.
[01:10:28.780 --> 01:10:32.980]   Oh, I'm sure in some cities, there's some.
[01:10:32.980 --> 01:10:38.500]   Because of very unique circumstances, like snow is probably a very unique circumstance.
[01:10:38.500 --> 01:10:42.420]   Note all the cities we talked about, Austin others are not usually snow cities.
[01:10:42.420 --> 01:10:46.020]   They are warm cities that don't have a lot of snow.
[01:10:46.020 --> 01:10:51.220]   Snow is your biggest problem where like cuts off all the lines and things.
[01:10:51.220 --> 01:10:52.420]   And rain, how about rain?
[01:10:52.420 --> 01:10:54.940]   I mean, they all rain in these cities.
[01:10:54.940 --> 01:10:55.940]   Rain is easier.
[01:10:55.940 --> 01:11:00.020]   You can still see the lines like you can detect, like in three to 60 degrees.
[01:11:00.020 --> 01:11:05.260]   This one snow completely covers like the middle of lines and the sidelines and like humans
[01:11:05.260 --> 01:11:10.620]   can kind of understand where like the edge of the road is by feel.
[01:11:10.620 --> 01:11:13.980]   The machines, you know, that's just a much harder problem.
[01:11:13.980 --> 01:11:19.140]   So when you start seeing self-driving cars in the middle of winter in the twin cities,
[01:11:19.140 --> 01:11:21.060]   that's what you know, the technology has done it.
[01:11:21.060 --> 01:11:22.060]   Right.
[01:11:22.060 --> 01:11:23.980]   And as I talked to someone at cruise, okay.
[01:11:23.980 --> 01:11:24.980]   Go ahead.
[01:11:24.980 --> 01:11:25.980]   No, no.
[01:11:25.980 --> 01:11:28.380]   I was just going to say I talked to someone at cruise about it the other day.
[01:11:28.380 --> 01:11:35.720]   And it was like, I think the message was, you know, don't underestimate how difficult
[01:11:35.720 --> 01:11:43.980]   it was to make it work in San Francisco and, you know, how long that took and sort of
[01:11:43.980 --> 01:11:46.940]   how bespoke it is, I guess.
[01:11:46.940 --> 01:11:52.500]   And I think that's an interesting, it'll be interesting to see because nobody wants to
[01:11:52.500 --> 01:11:58.740]   like none of these companies want to just invest all that manpower in every single city.
[01:11:58.740 --> 01:12:01.420]   Like they want to, they want to figure out how to make it turn key.
[01:12:01.420 --> 01:12:07.420]   Well, and that's important because we, we forget they need to make money in the long
[01:12:07.420 --> 01:12:08.420]   run.
[01:12:08.420 --> 01:12:11.300]   They can't, they're not just doing this just because they can.
[01:12:11.300 --> 01:12:13.220]   They want to make it a profitable business.
[01:12:13.220 --> 01:12:14.220]   And if it's prohibitively.
[01:12:14.220 --> 01:12:15.540]   I think each new city is good.
[01:12:15.540 --> 01:12:18.660]   I think each new city is going to be a little bit easier that like they're going to learn
[01:12:18.660 --> 01:12:24.180]   some stuff from every new city and theoretically the fifth or the eighth or the tenth city is
[01:12:24.180 --> 01:12:26.020]   going to be a little bit easier.
[01:12:26.020 --> 01:12:32.100]   By the time they get to smaller cities in South Dakota, it's going to be turn key.
[01:12:32.100 --> 01:12:37.100]   So this is one use of AI that I think probably is okay, right?
[01:12:37.100 --> 01:12:40.820]   It's not that doesn't pose an existential threat to mankind.
[01:12:40.820 --> 01:12:44.100]   As long as you're not a cab driver.
[01:12:44.100 --> 01:12:45.740]   Yeah, true.
[01:12:45.740 --> 01:12:50.780]   It's probably bad for, although I think about, we were talking about this earlier, I think
[01:12:50.780 --> 01:12:53.820]   about what happened in New York City when Uber took over.
[01:12:53.820 --> 01:12:55.140]   It didn't get rid of cab drivers.
[01:12:55.140 --> 01:13:01.340]   It just meant there were far more cars in the street and it was worse for everybody except
[01:13:01.340 --> 01:13:05.900]   because it was probably was good for subway use.
[01:13:05.900 --> 01:13:13.180]   It got really gridlocked for a while because of all the Ubers on the city streets.
[01:13:13.180 --> 01:13:16.580]   Is Moto another publication?
[01:13:16.580 --> 01:13:23.100]   Do any of your publications, Seth, use AI to write articles?
[01:13:23.100 --> 01:13:25.460]   So we use Grammarly sometimes.
[01:13:25.460 --> 01:13:26.460]   That's fine.
[01:13:26.460 --> 01:13:27.460]   We use Grammarly too.
[01:13:27.460 --> 01:13:28.460]   That's good.
[01:13:28.460 --> 01:13:34.660]   So I kind of do AI is like the next step in that regard.
[01:13:34.660 --> 01:13:36.820]   So we don't use AI to write articles.
[01:13:36.820 --> 01:13:39.780]   We have no plans to do anything like that.
[01:13:39.780 --> 01:13:45.060]   We know that some of our competitors are doing it, you know, red ventures and see that.
[01:13:45.060 --> 01:13:47.220]   They love it.
[01:13:47.220 --> 01:13:56.300]   It's painful for me since I wrote a column for see that it's so painful for me because
[01:13:56.300 --> 01:14:00.460]   if it were because the initial stuff was just it's not good.
[01:14:00.460 --> 01:14:01.860]   It's not good.
[01:14:01.860 --> 01:14:04.740]   I mean, it's a little branding move.
[01:14:04.740 --> 01:14:13.780]   Why are you announcing that you're going to use like a not mature technology to write,
[01:14:13.780 --> 01:14:17.100]   you know, basically garbage?
[01:14:17.100 --> 01:14:22.380]   We had Connie Guillermo on right after that came out and she said, well, these are articles
[01:14:22.380 --> 01:14:23.620]   no human wants to write.
[01:14:23.620 --> 01:14:28.620]   They were in the personal finance section, basic stuff that no person wants to write.
[01:14:28.620 --> 01:14:32.220]   So we have the AI write it and then we have an editor check it.
[01:14:32.220 --> 01:14:36.420]   But what did come out later was there were some real concerns about accuracy.
[01:14:36.420 --> 01:14:39.020]   Yeah, there were big big mistakes.
[01:14:39.020 --> 01:14:40.020]   Yeah.
[01:14:40.020 --> 01:14:48.700]   So here's the story from the Washington Post about an AI written Star Wars story at Gizmodo.
[01:14:48.700 --> 01:14:57.460]   Now one group you don't mess with is Star Wars nerds, right?
[01:14:57.460 --> 01:15:02.860]   A few hours the story goes after James Whitbrook clocked into work at Gizmodo on Wednesday,
[01:15:02.860 --> 01:15:05.980]   he got a note from his editor in chief.
[01:15:05.980 --> 01:15:10.340]   Within 12 hours, the company would roll out articles written by artificial intelligence.
[01:15:10.340 --> 01:15:16.740]   10 minutes later, a story by quote Gizmodo bought and quote, post on the site about a
[01:15:16.740 --> 01:15:22.900]   chronological order of Star Wars movies and TV shows.
[01:15:22.900 --> 01:15:27.140]   Whitbrook who is a Star Wars fan and writes about sci-fi among other things that Gizmodo
[01:15:27.140 --> 01:15:28.140]   said.
[01:15:28.140 --> 01:15:34.820]   He cataloged 18 concerns, corrections and comments about the story in the email to the editor
[01:15:34.820 --> 01:15:40.420]   in chief noting, for instance, that the bot put the Star Wars TV series, the Clone Wars
[01:15:40.420 --> 01:15:44.940]   in the wrong order, admitted any mention of and or.
[01:15:44.940 --> 01:15:47.540]   In fact, completely.
[01:15:47.540 --> 01:15:52.580]   And the 2008 film entitled Star Wars the Clone Wars, inaccurately formatted movie titles
[01:15:52.580 --> 01:15:57.980]   and the story's headline had repetitive descriptions and contained no explicit, this is the biggest
[01:15:57.980 --> 01:16:04.460]   problem in my opinion, explicit disclaimer that was written by AI.
[01:16:04.460 --> 01:16:11.500]   The story was error riddled and more importantly to the staffers actively heard our reputation
[01:16:11.500 --> 01:16:14.780]   and credibility.
[01:16:14.780 --> 01:16:25.460]   They used Google bar and chat GPT to write it.
[01:16:25.460 --> 01:16:29.140]   Whitbrook said I had never had to deal with this basic level of incompetence with any
[01:16:29.140 --> 01:16:32.500]   of the colleagues that I've ever worked with.
[01:16:32.500 --> 01:16:36.860]   If these chatbots can't even do something as basic as put a Star Wars movie in order
[01:16:36.860 --> 01:16:40.300]   one after the other, I don't think you can report it.
[01:16:40.300 --> 01:16:41.740]   Use it to report on anything else.
[01:16:41.740 --> 01:16:43.380]   I think it's got a point.
[01:16:43.380 --> 01:16:49.260]   I mean, that's a factual matter.
[01:16:49.260 --> 01:16:51.500]   You cover AI.
[01:16:51.500 --> 01:16:53.380]   Can you defend Gizmodo Benner?
[01:16:53.380 --> 01:16:56.900]   Was this just an inappropriate use of it?
[01:16:56.900 --> 01:16:59.780]   It's a bad use of it because there are much better uses.
[01:16:59.780 --> 01:17:06.620]   And look, I thought a lot about AI and journalism kind of over the short and long term.
[01:17:06.620 --> 01:17:08.860]   There's this great report from before.
[01:17:08.860 --> 01:17:14.020]   Chat GPT came out by the AP showing where AI is already being used for certain things.
[01:17:14.020 --> 01:17:18.300]   And so I'll give you my general stance, which is like we're already seeing AI being used
[01:17:18.300 --> 01:17:22.420]   for the really straightforward stuff.
[01:17:22.420 --> 01:17:28.140]   If you're reporting on a local sports game, the format is straightforward.
[01:17:28.140 --> 01:17:31.500]   Put in the numbers, there's names of players.
[01:17:31.500 --> 01:17:33.340]   Those things are already automated a lot by AI.
[01:17:33.340 --> 01:17:37.420]   And it's in part because there are no local reporters to report on those things anymore,
[01:17:37.420 --> 01:17:39.260]   which is unfortunate, but the reality.
[01:17:39.260 --> 01:17:42.140]   And so there's a whole world of that.
[01:17:42.140 --> 01:17:44.300]   But we're definitely going to see more of it.
[01:17:44.300 --> 01:17:48.220]   And it's in its infancy, but it is also the worst it will ever be.
[01:17:48.220 --> 01:17:51.060]   And so we're going to see more experiments like this.
[01:17:51.060 --> 01:17:54.980]   This is not a good experiment because they didn't have a support of the staff.
[01:17:54.980 --> 01:18:00.180]   They didn't do any fact checking out, which you must do with AI.
[01:18:00.180 --> 01:18:02.460]   The AI's of today don't fact check.
[01:18:02.460 --> 01:18:04.060]   They're not that smart.
[01:18:04.060 --> 01:18:09.980]   But this is why the writers, for example, are striking in Hollywood right now.
[01:18:09.980 --> 01:18:14.780]   Because the future could very well be that more newsrooms do use more AI.
[01:18:14.780 --> 01:18:18.180]   I'm going to plug my fiance Deborah because she wrote a play.
[01:18:18.180 --> 01:18:20.260]   Why was a sonoma last year?
[01:18:20.260 --> 01:18:24.580]   Her play last year called Atlas the Lonely Gibbon was about two journalists, one of whom
[01:18:24.580 --> 01:18:29.140]   lost their job to AI and the other was desperately trying to keep his job.
[01:18:29.140 --> 01:18:32.340]   He was a cybersecurity reporter in this play.
[01:18:32.340 --> 01:18:37.740]   And that's kind of what the reality is going to be.
[01:18:37.740 --> 01:18:42.180]   I think down the line for certain types of reporters, the types that are going to have
[01:18:42.180 --> 01:18:48.500]   jobs long term are the ones who are doing investigative journalism because the AI cannot
[01:18:48.500 --> 01:18:51.300]   do that, cannot make the phone calls.
[01:18:51.300 --> 01:18:56.500]   And those who are on the fourth, in terms of columns and opinion, because AI's don't
[01:18:56.500 --> 01:18:59.060]   have strong or good opinions.
[01:18:59.060 --> 01:19:02.700]   And it's not going to be some immediate change where half the jobs are going to be gone.
[01:19:02.700 --> 01:19:06.180]   But more of this experimentation is going to come just because the economic incentives
[01:19:06.180 --> 01:19:08.780]   are unfortunately there.
[01:19:08.780 --> 01:19:09.780]   Here is a good use.
[01:19:09.780 --> 01:19:10.780]   Well, that's true.
[01:19:10.780 --> 01:19:11.780]   Yeah.
[01:19:11.780 --> 01:19:14.660]   And I think I just wanted to say the CNET thing.
[01:19:14.660 --> 01:19:17.060]   And I think this is motivating as well.
[01:19:17.060 --> 01:19:22.420]   I mean, they're having it right articles that basically they used to pay people like $30
[01:19:22.420 --> 01:19:28.100]   to write something that's essentially just to try to game the Google algorithms.
[01:19:28.100 --> 01:19:31.620]   And that's not journalism.
[01:19:31.620 --> 01:19:37.380]   I just wouldn't put it in the same categories reporting as you basically said, Ben.
[01:19:37.380 --> 01:19:39.180]   What we do is make phone calls.
[01:19:39.180 --> 01:19:45.900]   The writing part of it is almost like my boss, Ben Smith, made this point at one of his book
[01:19:45.900 --> 01:19:47.700]   parties that I was at.
[01:19:47.700 --> 01:19:50.860]   The writing part of this stuff is kind of an afterthought.
[01:19:50.860 --> 01:19:54.100]   There are a lot of great journalists who really can't write.
[01:19:54.100 --> 01:19:55.100]   Wow.
[01:19:55.100 --> 01:19:56.100]   And editors have to do.
[01:19:56.100 --> 01:19:57.100]   That's true.
[01:19:57.100 --> 01:19:58.100]   Wow.
[01:19:58.100 --> 01:19:59.100]   That's not the strong cap.
[01:19:59.100 --> 01:20:00.100]   That's interesting.
[01:20:00.100 --> 01:20:01.100]   I never thought of that.
[01:20:01.100 --> 01:20:04.500]   How can you build yourself a writer?
[01:20:04.500 --> 01:20:05.500]   You're not a writer.
[01:20:05.500 --> 01:20:07.100]   You're an investigator.
[01:20:07.100 --> 01:20:08.100]   You're a fact collector.
[01:20:08.100 --> 01:20:12.340]   I mean, any real reporting is kind of, I mean, I guess, like is investigative, right?
[01:20:12.340 --> 01:20:15.500]   You're trying to get information that hasn't been out there before.
[01:20:15.500 --> 01:20:20.980]   And the only way to do that is to make human relationships with people.
[01:20:20.980 --> 01:20:24.700]   I would love for AI to be able to help me with the writing process.
[01:20:24.700 --> 01:20:29.940]   We do use it for grammar checks and things like that.
[01:20:29.940 --> 01:20:34.020]   But like, you know, as it gets better, it could help write.
[01:20:34.020 --> 01:20:38.340]   You could give, you could provide it with the facts and say, here, put this into a coherent
[01:20:38.340 --> 01:20:39.340]   article.
[01:20:39.340 --> 01:20:40.340]   I've tried.
[01:20:40.340 --> 01:20:41.340]   Yeah.
[01:20:41.340 --> 01:20:42.340]   I've tried to do it.
[01:20:42.340 --> 01:20:46.420]   I've tried to do it with Q and A's where like, okay, I've got a transcript of a conversation.
[01:20:46.420 --> 01:20:47.420]   Can you edit this down?
[01:20:47.420 --> 01:20:50.420]   And you give it a whole big string of instructions with a prompt.
[01:20:50.420 --> 01:20:51.980]   And it just isn't there yet.
[01:20:51.980 --> 01:20:56.060]   It doesn't, can't do what a human can do yet there.
[01:20:56.060 --> 01:21:00.820]   And it can't take a, you can't take your notebook and turn it into an article.
[01:21:00.820 --> 01:21:05.300]   Like you'll end up spending more time than you would have if you just, if you just wrote
[01:21:05.300 --> 01:21:06.940]   the thing from scratch.
[01:21:06.940 --> 01:21:12.380]   So, you know, it's just good at certain things.
[01:21:12.380 --> 01:21:16.380]   And the, you can get it to a point where it'll do that, not in chat.
[01:21:16.380 --> 01:21:18.380]   You're going to have to like build your own side.
[01:21:18.380 --> 01:21:23.220]   I think like my co-founder Matt and I built our own like side stuff just to like do very
[01:21:23.220 --> 01:21:26.180]   specific AI things for our company and for ourselves.
[01:21:26.180 --> 01:21:27.700]   That's the only way we could get it to do those things.
[01:21:27.700 --> 01:21:32.620]   It's just a lot of tweaking to make it work for very specific use cases.
[01:21:32.620 --> 01:21:37.340]   But with AI is not yet at the point where you could just be like, fix this and it fixes
[01:21:37.340 --> 01:21:39.020]   it like a human would.
[01:21:39.020 --> 01:21:40.460]   That's what it has to get to.
[01:21:40.460 --> 01:21:44.140]   It, but it will at some point I do believe.
[01:21:44.140 --> 01:21:51.260]   I just don't know the timeframe, but some point, but it will not be making the phone calls,
[01:21:51.260 --> 01:22:00.220]   building human relationships, sourcing reporting out of Google or Tesla or meta or the US government.
[01:22:00.220 --> 01:22:04.540]   And that's where we need more reporters focusing more of their time anyway.
[01:22:04.540 --> 01:22:06.780]   Actually, and we need those local reporters.
[01:22:06.780 --> 01:22:07.780]   Yeah.
[01:22:07.780 --> 01:22:08.780]   Yes.
[01:22:08.780 --> 01:22:09.780]   We need those.
[01:22:09.780 --> 01:22:10.780]   Yeah.
[01:22:10.780 --> 01:22:11.780]   And there's no money for those.
[01:22:11.780 --> 01:22:16.100]   It strikes me actually that the AI might be a good reporter.
[01:22:16.100 --> 01:22:23.140]   I was playing with a friend yesterday who was a former Facebook engineer.
[01:22:23.140 --> 01:22:25.980]   We were having a conversation with the three of us and he put our little recorder on the
[01:22:25.980 --> 01:22:30.780]   table and he said, watch this and he had it set up so that the recording would automatically
[01:22:30.780 --> 01:22:37.860]   go to Dropbox where whisper AI would turn it into a transcription and then chat GPT would
[01:22:37.860 --> 01:22:44.660]   take the transcription and return notes on the conversation and action items on the conversation.
[01:22:44.660 --> 01:22:45.660]   Exactly.
[01:22:45.660 --> 01:22:50.220]   The notes would be the kind of thing a reporter would do on listening and the action items
[01:22:50.220 --> 01:22:52.380]   the kind of thing you might do in a business meeting.
[01:22:52.380 --> 01:22:54.180]   And it was really, it was quite good.
[01:22:54.180 --> 01:22:55.660]   It was almost instantaneous.
[01:22:55.660 --> 01:22:58.860]   We'd finished the conversation and he said, okay, now here's the notes.
[01:22:58.860 --> 01:23:04.860]   And it was actually really good at picking up tidbits.
[01:23:04.860 --> 01:23:09.540]   I guess there's a lot of judgment for a reporter on what to keep track of.
[01:23:09.540 --> 01:23:13.580]   So maybe it would lack that, but it might save some time on that.
[01:23:13.580 --> 01:23:16.540]   Maybe it's better on that and then on the writing end.
[01:23:16.540 --> 01:23:20.580]   Well, any profession you should be trying to figure out how to use these tools, right?
[01:23:20.580 --> 01:23:21.580]   Yes.
[01:23:21.580 --> 01:23:22.580]   That's right.
[01:23:22.580 --> 01:23:26.500]   I mean, you're going to fall behind and we've been using, like you said, Ben, like we've
[01:23:26.500 --> 01:23:28.460]   been using AI for a long time.
[01:23:28.460 --> 01:23:31.420]   If you define it that way, right, it's really machine learning.
[01:23:31.420 --> 01:23:35.460]   But like, I remember, I mean, when I was at the Washington Post, like I trained an algorithm
[01:23:35.460 --> 01:23:42.460]   to scan apps in the, you know, tapped into the API, you could scan app store reviews.
[01:23:42.460 --> 01:23:46.860]   And we were trying to find like certain, you know, certain types of content.
[01:23:46.860 --> 01:23:49.060]   And it works, you know, works incredibly well.
[01:23:49.060 --> 01:23:51.180]   It's a powerful tool.
[01:23:51.180 --> 01:23:56.300]   And all of these large language models will be very powerful as well.
[01:23:56.300 --> 01:24:00.700]   When you're talking about it being a good reporter, it kind of makes me think a lot
[01:24:00.700 --> 01:24:03.340]   of it, there's like the LinkedIn era sort of journalist.
[01:24:03.340 --> 01:24:08.420]   And there's the strategy of just basically linked in messaging, like every single person
[01:24:08.420 --> 01:24:10.860]   who could possibly give you information.
[01:24:10.860 --> 01:24:14.140]   Like that actually, very good at that.
[01:24:14.140 --> 01:24:15.140]   Yeah.
[01:24:15.140 --> 01:24:19.060]   I mean, and then the people who respond, the one out of whatever 20 people who respond,
[01:24:19.060 --> 01:24:22.260]   like you just, you know, you then you start talking with them.
[01:24:22.260 --> 01:24:27.180]   Do you identify yourself as an AI that's contacting people or do you pretend you're
[01:24:27.180 --> 01:24:28.180]   human?
[01:24:28.180 --> 01:24:31.540]   I mean, I think you'd have to pretend you would be unethical.
[01:24:31.540 --> 01:24:33.620]   So I really do.
[01:24:33.620 --> 01:24:38.580]   But like, I mean, you're not far from an AI if that's how you're doing your reporting,
[01:24:38.580 --> 01:24:39.580]   right?
[01:24:39.580 --> 01:24:45.020]   So yeah, I guess there's already companies automating using AI to automate like all their outbound
[01:24:45.020 --> 01:24:46.020]   email.
[01:24:46.020 --> 01:24:48.620]   I know it's happening to success for some.
[01:24:48.620 --> 01:24:50.780]   There's all sorts of crazy stuff happening.
[01:24:50.780 --> 01:24:54.740]   Like, you know, superhuman the email app now has like AI feature.
[01:24:54.740 --> 01:24:55.740]   I have it.
[01:24:55.740 --> 01:24:56.740]   You click a button.
[01:24:56.740 --> 01:25:01.060]   Like, we'll write your rate from your random thoughts into a pretty, yeah, yeah, into a
[01:25:01.060 --> 01:25:02.060]   decent email.
[01:25:02.060 --> 01:25:03.060]   Yeah.
[01:25:03.060 --> 01:25:07.300]   So it's like, you have to just expect that what a quarter of the emails you write or
[01:25:07.300 --> 01:25:14.580]   at least 50%, quarter percent, some percentage written by AI, a whole different topic of
[01:25:14.580 --> 01:25:16.620]   once like most everything is just written.
[01:25:16.620 --> 01:25:20.180]   AI is talking to other AIs whole different world.
[01:25:20.180 --> 01:25:21.700]   I'm sure it's happening.
[01:25:21.700 --> 01:25:25.860]   I was saying when I got this demonstration of, you know, our conversation being transcribed
[01:25:25.860 --> 01:25:29.660]   and then noted, I thought this would be a great thing to do with all our podcasts.
[01:25:29.660 --> 01:25:31.620]   We don't have the manpower to do.
[01:25:31.620 --> 01:25:34.980]   Nobody would lose a job because we don't have the manpower to have somebody take really
[01:25:34.980 --> 01:25:37.140]   thorough show notes and publish them.
[01:25:37.140 --> 01:25:42.660]   But if an AI could do that quickly and easily and basically for free, that would be a huge
[01:25:42.660 --> 01:25:44.580]   benefit to listeners.
[01:25:44.580 --> 01:25:45.580]   Made me think.
[01:25:45.580 --> 01:25:49.540]   But do you publish transcripts of the whole thing?
[01:25:49.540 --> 01:25:50.540]   We do.
[01:25:50.540 --> 01:25:51.540]   We do.
[01:25:51.540 --> 01:25:53.380]   I mean, you could just scan those.
[01:25:53.380 --> 01:25:58.820]   And then, yeah, you scan those and then summarize or make action items or whatever.
[01:25:58.820 --> 01:25:59.820]   Here's a-
[01:25:59.820 --> 01:26:00.900]   I think the go ahead.
[01:26:00.900 --> 01:26:06.780]   I think one thing that like it's kind of scary is like what does a world look like,
[01:26:06.780 --> 01:26:12.340]   you know, publishing world look like when every single publisher has access to a fairly
[01:26:12.340 --> 01:26:13.900]   good AI.
[01:26:13.900 --> 01:26:22.540]   So we could publish 750 articles on, you know, the new iPhone in a second.
[01:26:22.540 --> 01:26:25.300]   And you know, all of our competitors could do that.
[01:26:25.300 --> 01:26:30.660]   And 500 blogs and publications that we don't even know about could do the same thing.
[01:26:30.660 --> 01:26:36.540]   So then all of a sudden you have a glut of content that all kind of looks and acts, you
[01:26:36.540 --> 01:26:42.900]   know, very generic and very similar, theoretically.
[01:26:42.900 --> 01:26:44.060]   That's not going to be good for anyone.
[01:26:44.060 --> 01:26:46.300]   Like, who's going to want that?
[01:26:46.300 --> 01:26:48.180]   Like Google certainly doesn't want that.
[01:26:48.180 --> 01:26:52.340]   Like Google would become useless at that point because anybody who's doing original reporting
[01:26:52.340 --> 01:26:58.660]   is going to be buried under 700 different AIs doing, you know, like large language model
[01:26:58.660 --> 01:26:59.900]   reporting.
[01:26:59.900 --> 01:27:06.820]   So I think the human piece is going to be more and more important as, you know, that's the
[01:27:06.820 --> 01:27:12.980]   senates of the world go in and just, you know, lay off their writers and build up an AI.
[01:27:12.980 --> 01:27:13.980]   Huge.
[01:27:13.980 --> 01:27:14.980]   So yeah.
[01:27:14.980 --> 01:27:18.540]   I think that what you're saying is true.
[01:27:18.540 --> 01:27:24.220]   But I think to me, the natural conclusion there is like people are just going to crave
[01:27:24.220 --> 01:27:26.020]   places like the information, right?
[01:27:26.020 --> 01:27:30.700]   That have, you know, it's a paywall, but it's differentiated content that you know is going
[01:27:30.700 --> 01:27:33.340]   to be high quality.
[01:27:33.340 --> 01:27:36.860]   You know, I worked at the information for four years, so full disclosure.
[01:27:36.860 --> 01:27:39.060]   But I think it's great.
[01:27:39.060 --> 01:27:41.540]   But I, you know, isn't that what's going to happen?
[01:27:41.540 --> 01:27:43.140]   Like, like I think you're right.
[01:27:43.140 --> 01:27:46.500]   Google's, you know, if the internet's just flooded with this stuff, like Google's going
[01:27:46.500 --> 01:27:50.780]   to be in trouble, but it kind of, it's kind of been moving in that direction anyway.
[01:27:50.780 --> 01:27:53.580]   And like AI is just going to like finish the job.
[01:27:53.580 --> 01:27:54.580]   I guess.
[01:27:54.580 --> 01:27:55.580]   Yeah.
[01:27:55.580 --> 01:27:58.460]   I mean, the Google results have been bad and getting worse.
[01:27:58.460 --> 01:28:04.700]   And you know, there's a hundred very low quality, you know, set sites on top of some
[01:28:04.700 --> 01:28:06.060]   better ones.
[01:28:06.060 --> 01:28:09.020]   So you know, yes, that's the direction it's going with.
[01:28:09.020 --> 01:28:12.060]   I don't think that's, I don't think that's a good ending.
[01:28:12.060 --> 01:28:14.140]   Like, I don't, I don't like that world.
[01:28:14.140 --> 01:28:18.460]   I mean, look, we're ending it in a world of lots of behavior change right now.
[01:28:18.460 --> 01:28:25.180]   There's a lot of articles about how Gen Z, their first instinct is not to go and search
[01:28:25.180 --> 01:28:26.180]   on Google.
[01:28:26.180 --> 01:28:27.500]   It's to go on TikTok and search.
[01:28:27.500 --> 01:28:30.900]   And like, you know, that's going to be made by a person for the most part.
[01:28:30.900 --> 01:28:33.900]   Although that might not be true to a couple of years.
[01:28:33.900 --> 01:28:36.780]   But you can go and like find travel tips on all sorts of things.
[01:28:36.780 --> 01:28:39.460]   And that's like how they find their information.
[01:28:39.460 --> 01:28:41.820]   Now you have a chat GPT, no sorts of things.
[01:28:41.820 --> 01:28:46.460]   And the, you know, I have a generation where I wrote about this on Benpar.com, how the
[01:28:46.460 --> 01:28:51.620]   new generation is solving their education problems, like trying to solve a complex,
[01:28:51.620 --> 01:28:53.300]   like, how do I solve this math problem?
[01:28:53.300 --> 01:28:56.820]   Instead of going and asking a student or googling, they just ask chat GPT.
[01:28:56.820 --> 01:29:00.020]   And it's really good at that sort of thing, give you an explanation of the step by step
[01:29:00.020 --> 01:29:01.620]   of what you should be doing.
[01:29:01.620 --> 01:29:03.620]   And so just dramatic behavior change happens.
[01:29:03.620 --> 01:29:07.860]   And I think the reality is that long term, we are probably entering an era where the
[01:29:07.860 --> 01:29:12.060]   Google search will be on a very, very slow decline.
[01:29:12.060 --> 01:29:14.580]   And this is an existential crisis for Google.
[01:29:14.580 --> 01:29:16.540]   That's why they rallied so much.
[01:29:16.540 --> 01:29:18.420]   That's why they're investing in Bard.
[01:29:18.420 --> 01:29:21.020]   And it's not going to happen overnight.
[01:29:21.020 --> 01:29:24.700]   But it's, yeah, the behavior change, I'm already changing my behavior.
[01:29:24.700 --> 01:29:29.180]   I just Google results are less useful for me than they used to be.
[01:29:29.180 --> 01:29:36.060]   And results from AIs and results from TikTok and other places are much more useful to me.
[01:29:36.060 --> 01:29:37.060]   Yeah.
[01:29:37.060 --> 01:29:38.660]   I want to take a little break when we come back.
[01:29:38.660 --> 01:29:43.460]   Rita has a story about something not generative AI, but causal AI.
[01:29:43.460 --> 01:29:47.980]   We'll find out what the heck that is in just a little bit as we continue.
[01:29:47.980 --> 01:29:50.900]   What a great panel, especially for AI.
[01:29:50.900 --> 01:29:52.780]   Ben Parr is here.
[01:29:52.780 --> 01:29:56.060]   His column at Benparr.com is the AI analyst.
[01:29:56.060 --> 01:30:00.060]   And he's writing now for the information.
[01:30:00.060 --> 01:30:03.500]   And also a founder and been in the AI space for some time.
[01:30:03.500 --> 01:30:08.500]   Pete Albergotti is here.
[01:30:08.500 --> 01:30:11.500]   He is now with semaphore.com.
[01:30:11.500 --> 01:30:13.500]   Technology editor there.
[01:30:13.500 --> 01:30:14.500]   Great newsletter.
[01:30:14.500 --> 01:30:15.500]   Must subscribe.
[01:30:15.500 --> 01:30:16.500]   And it's free, right?
[01:30:16.500 --> 01:30:17.500]   Did they charge for new?
[01:30:17.500 --> 01:30:18.500]   It's free.
[01:30:18.500 --> 01:30:19.500]   Yeah.
[01:30:19.500 --> 01:30:20.500]   Thank you for mentioning that.
[01:30:20.500 --> 01:30:21.500]   Yeah.
[01:30:21.500 --> 01:30:22.500]   It's free.
[01:30:22.500 --> 01:30:23.500]   Yeah.
[01:30:23.500 --> 01:30:24.500]   Can I plug semaphore?
[01:30:24.500 --> 01:30:27.500]   We broke that story about Elon threatening to sue.
[01:30:27.500 --> 01:30:29.500]   Yes, you get credit for that.
[01:30:29.500 --> 01:30:30.500]   That's right.
[01:30:30.500 --> 01:30:31.500]   I had the PDF.
[01:30:31.500 --> 01:30:32.500]   But I should run over there.
[01:30:32.500 --> 01:30:33.500]   How did he do that?
[01:30:33.500 --> 01:30:34.500]   Was that shoe leather?
[01:30:34.500 --> 01:30:39.500]   Did he go down to the courthouse and chat up the clerk or what?
[01:30:39.500 --> 01:30:42.500]   No, he asked chat GPT and it pulled him back.
[01:30:42.500 --> 01:30:46.500]   He's planning his shoe leather salesman.
[01:30:46.500 --> 01:30:49.500]   I'm going to be in trouble if this keeps up.
[01:30:49.500 --> 01:30:51.500]   I got to tell you right now.
[01:30:51.500 --> 01:30:52.500]   Does it at least see you?
[01:30:52.500 --> 01:30:54.000]   Mark Zuckerberg made fun of me.
[01:30:54.000 --> 01:30:56.500]   What should I do, chat GPT?
[01:30:56.500 --> 01:31:01.500]   I hope at least he wears a fedora hat with press in the side there.
[01:31:01.500 --> 01:31:04.500]   I can't reveal Max Taney's sources.
[01:31:04.500 --> 01:31:05.500]   Secret.
[01:31:05.500 --> 01:31:06.500]   Secret.
[01:31:06.500 --> 01:31:11.500]   And good old fashioned journalism and nothing but nine to five Google nine to five Mac.
[01:31:11.500 --> 01:31:14.500]   And of course, electric Seth Wine tribe.
[01:31:14.500 --> 01:31:15.500]   It's founder.
[01:31:15.500 --> 01:31:18.500]   He's a guy who discovered Mark Gurman.
[01:31:18.500 --> 01:31:20.500]   Put that on your business card.
[01:31:20.500 --> 01:31:23.500]   He was like 12 probably when you discovered it.
[01:31:23.500 --> 01:31:24.500]   Something like that.
[01:31:24.500 --> 01:31:25.500]   Yeah.
[01:31:25.500 --> 01:31:26.500]   High school kid.
[01:31:26.500 --> 01:31:30.500]   Our show today brought to you by HelloFresh.
[01:31:30.500 --> 01:31:33.500]   Oh, I love it with my HelloFresh box.
[01:31:33.500 --> 01:31:40.060]   Comes take a bite out of summer with HelloFresh from chef crafted seasonal recipes to this
[01:31:40.060 --> 01:31:43.500]   brand new and I love it fresh and fit summer menu.
[01:31:43.500 --> 01:31:47.500]   HelloFresh brings flavor right to your door.
[01:31:47.500 --> 01:31:50.500]   Pre portion ingredients that cups down on food waste.
[01:31:50.500 --> 01:31:55.620]   You always have exactly what you need to make the recipes beautiful recipe cards to with step
[01:31:55.620 --> 01:32:02.620]   by step instruction.
[01:32:02.620 --> 01:32:04.620]   So cooking is a breeze.
[01:32:04.620 --> 01:32:05.620]   Not a chore.
[01:32:05.620 --> 01:32:10.620]   Look at that creamy lemon herb pork chops that could be for dinner tonight.
[01:32:10.620 --> 01:32:13.620]   Actually, it might be at our house.
[01:32:13.620 --> 01:32:14.620]   Come to think of it.
[01:32:14.620 --> 01:32:16.620]   Pork sausage and bell pepper risotto.
[01:32:16.620 --> 01:32:18.620]   When I made that, that was a huge hit.
[01:32:18.620 --> 01:32:20.620]   Tuscan pork tenderloin.
[01:32:20.620 --> 01:32:24.620]   Make your home hang out place this summer.
[01:32:24.620 --> 01:32:29.620]   First bar would that be fun to tangy key lime pie.
[01:32:29.620 --> 01:32:32.620]   HelloFresh market makes summer entertaining a cinch.
[01:32:32.620 --> 01:32:36.620]   HelloFresh, make sure you get peak time summer produce all season long.
[01:32:36.620 --> 01:32:44.620]   This is the time for delicious tomatoes, ripe, plump, sweet or delicious sweet corn.
[01:32:44.620 --> 01:32:49.620]   Their ingredients travel from the farm to your door in less than seven days.
[01:32:49.620 --> 01:32:53.620]   And that really means fresh ingredients with quality you can taste.
[01:32:53.620 --> 01:32:55.620]   HelloFresh offers more than just delicious dinners.
[01:32:55.620 --> 01:32:59.620]   It's now easier than ever to add snacks, sides and more to your weekly order.
[01:32:59.620 --> 01:33:01.620]   Get that brought worst bar.
[01:33:01.620 --> 01:33:02.620]   It sounds incredible.
[01:33:02.620 --> 01:33:04.620]   I want to do that.
[01:33:04.620 --> 01:33:09.620]   HelloFresh market offers a curated selection of more than a hundred items.
[01:33:09.620 --> 01:33:12.620]   So really you can get your shopping done each week.
[01:33:12.620 --> 01:33:17.620]   HelloFresh's menu features calorie smart and protein smart lunch and dinner options.
[01:33:17.620 --> 01:33:20.620]   They now have vegan dinners to choose from.
[01:33:20.620 --> 01:33:22.620]   They make it easy to eat healthy with delicious recipes.
[01:33:22.620 --> 01:33:26.620]   Let HelloFresh take care of the meal planning and deliver the ingredients.
[01:33:26.620 --> 01:33:31.620]   So you'll have everything you need to whip up a delicious meal and rise right at your door.
[01:33:31.620 --> 01:33:37.620]   HelloFresh gets that you want options when it comes to making dinner, not just the same old thing all the time.
[01:33:37.620 --> 01:33:42.620]   That's why they offer 40 recipes to choose from every single week so you never get bored.
[01:33:42.620 --> 01:33:46.620]   And they've got great substitutions too to fit the needs of you and your family.
[01:33:46.620 --> 01:33:51.620]   HelloFresh's fast and fresh recipes can be ready in 15 minutes or less.
[01:33:51.620 --> 01:33:53.620]   And yeah, don't worry about your budget.
[01:33:53.620 --> 01:33:56.620]   It's 25% less than takeout.
[01:33:56.620 --> 01:34:01.620]   If you're eating takeout, no, get the HelloFresh America's number one meal kit.
[01:34:01.620 --> 01:34:10.620]   Go to hellofresh.com/twit50 and use the offer code TWIT50, TWIT50 for 50% off and free shipping.
[01:34:10.620 --> 01:34:12.620]   That's a heck of a deal.
[01:34:12.620 --> 01:34:19.620]   HelloFresh.com/twit50 offer code TWIT50 to get 50% off plus free shipping.
[01:34:19.620 --> 01:34:20.620]   Thank you.
[01:34:20.620 --> 01:34:23.620]   HelloFresh for your support of This Week in Tech.
[01:34:23.620 --> 01:34:25.620]   And thanks to all of our This Week in Tech listeners.
[01:34:25.620 --> 01:34:27.620]   When you use that address, they hear it.
[01:34:27.620 --> 01:34:28.620]   They know it.
[01:34:28.620 --> 01:34:29.620]   That helps us a lot.
[01:34:29.620 --> 01:34:31.620]   HelloFresh.com/twit50.
[01:34:31.620 --> 01:34:36.620]   Don't forget the offer code TWIT50.
[01:34:36.620 --> 01:34:41.620]   What read Albergotti is causal AI?
[01:34:41.620 --> 01:34:45.300]   I've never heard of that.
[01:34:45.300 --> 01:34:48.620]   You say it's the next artificial intelligence frontier.
[01:34:48.620 --> 01:34:51.980]   Yeah, I mean, it was really interesting.
[01:34:51.980 --> 01:34:57.340]   I was in London and meeting with a bunch of AI companies there, which is, you know,
[01:34:57.340 --> 01:35:01.620]   if you don't know, London's like a big hot spot for AI.
[01:35:01.620 --> 01:35:07.540]   It's where DeepMind, now Google DeepMind started.
[01:35:07.540 --> 01:35:13.020]   And I met with this sort of interesting company that I think a few people have heard of called
[01:35:13.020 --> 01:35:14.020]   Causalens.
[01:35:14.020 --> 01:35:15.380]   Maybe you've heard of it then.
[01:35:15.380 --> 01:35:20.340]   But it was a really interesting conversation about what they're doing, which is like very
[01:35:20.340 --> 01:35:22.660]   different from these large language models.
[01:35:22.660 --> 01:35:30.620]   They're trying to tell companies, you know, like predictive analytics, but then like give
[01:35:30.620 --> 01:35:37.740]   them suggestions with these sort of frontier kind of cutting edge AI algorithms that determine
[01:35:37.740 --> 01:35:40.180]   cause and effect relationships.
[01:35:40.180 --> 01:35:44.140]   And that's something that large language models cannot do right now, right?
[01:35:44.140 --> 01:35:47.980]   And that's the stuff we were talking about earlier.
[01:35:47.980 --> 01:35:49.580]   You know, it gets all these things wrong.
[01:35:49.580 --> 01:35:53.420]   And that's because really what it's doing is just sort of trying to predict what the
[01:35:53.420 --> 01:36:00.260]   next word should be based on billions of other words that it's scraped from the internet,
[01:36:00.260 --> 01:36:01.260]   right?
[01:36:01.260 --> 01:36:05.540]   It's kind of a, you know, in that sense, it's not very intelligent.
[01:36:05.540 --> 01:36:10.900]   And if you, this idea, and it's sort of a science fiction kind of idea at this point,
[01:36:10.900 --> 01:36:17.460]   but like if you could somehow combine causal AI where like, you know, algorithms understand
[01:36:17.460 --> 01:36:21.780]   can sort of, you know, grasp, and the grasp is the wrong term, right?
[01:36:21.780 --> 01:36:23.420]   Because these are not brains.
[01:36:23.420 --> 01:36:24.980]   These are just math problems.
[01:36:24.980 --> 01:36:31.140]   But you know, math problems that can determine cause and effect relationships, you combine
[01:36:31.140 --> 01:36:36.860]   that with large language models, you get something that is much more powerful than anything we
[01:36:36.860 --> 01:36:37.860]   have today.
[01:36:37.860 --> 01:36:41.980]   So give me an example of how that would be used.
[01:36:41.980 --> 01:36:48.820]   Well, I mean, right, like basically, I mean, what they're doing right now is actually the
[01:36:48.820 --> 01:36:51.900]   founder of cause and lens had this great, this great example.
[01:36:51.900 --> 01:36:58.340]   It was like an analogy, but for a made up example, but like, you know, if you go to like, you
[01:36:58.340 --> 01:37:03.500]   know, Bondi Beach in Sydney, and you look at ice cream sales and shark attacks, those
[01:37:03.500 --> 01:37:05.940]   two statistics are correlated, right?
[01:37:05.940 --> 01:37:09.900]   Like when ice cream sales go up, shark attacks go up.
[01:37:09.900 --> 01:37:14.620]   Obviously, they have nothing to do with one another, but you know, there are more people
[01:37:14.620 --> 01:37:17.980]   in the water in the summer when it's hot and they're buying ice cream.
[01:37:17.980 --> 01:37:23.900]   So they get, you know, but if you run sort of traditional predictive analytics, it might
[01:37:23.900 --> 01:37:30.540]   not figure out that that that correlation is completely unrelated with, you know, cause
[01:37:30.540 --> 01:37:36.980]   the AI, you can sort of say, okay, you know, when should I increase, you know, buy more
[01:37:36.980 --> 01:37:38.820]   ice cream for the ice cream truck?
[01:37:38.820 --> 01:37:44.100]   And it will, it will understand, you know, that it has nothing to do with shark attacks,
[01:37:44.100 --> 01:37:45.100]   right?
[01:37:45.100 --> 01:37:50.420]   And that therefore will apply like anywhere, not just in Sydney, right?
[01:37:50.420 --> 01:37:53.380]   We often make that mistake.
[01:37:53.380 --> 01:37:57.660]   I mean, I, we often have to say, you know, correlation does not equal causation, but
[01:37:57.660 --> 01:38:02.540]   when it comes to, I mean, people make that mistake all the time when it comes to diet
[01:38:02.540 --> 01:38:07.860]   and all sorts of things, they go, oh, there's a correlation there must be the cause.
[01:38:07.860 --> 01:38:10.620]   So you're saying this AI is smart enough to know the difference.
[01:38:10.620 --> 01:38:11.620]   Right.
[01:38:11.620 --> 01:38:12.940]   So they had a customer.
[01:38:12.940 --> 01:38:15.180]   I thought this is an interesting anecdote.
[01:38:15.180 --> 01:38:21.540]   They had one customer where they, they were trying to figure out why they, there was all
[01:38:21.540 --> 01:38:27.860]   this churn, like why customers were, were leaving right after they, their usage went
[01:38:27.860 --> 01:38:28.860]   up.
[01:38:28.860 --> 01:38:29.860]   It was a SaaS company.
[01:38:29.860 --> 01:38:32.860]   So they, they'd start using the product more and then they would leave.
[01:38:32.860 --> 01:38:33.860]   Yeah.
[01:38:33.860 --> 01:38:34.860]   That's not good.
[01:38:34.860 --> 01:38:38.140]   And they were like, well, how do we, like, how do we sort of solve this problem?
[01:38:38.140 --> 01:38:39.140]   Yeah.
[01:38:39.140 --> 01:38:41.780]   We can predict when people are leaving, but we don't really understand why.
[01:38:41.780 --> 01:38:43.860]   We don't really know what to do about it.
[01:38:43.860 --> 01:38:48.820]   And it turned out that they were, they ran these numbers and it turned out that they
[01:38:48.820 --> 01:38:54.900]   were giving people like free that their, their own algorithms, the company's existing
[01:38:54.900 --> 01:38:58.060]   algorithms were predicting people would churn and then they would automatically get these
[01:38:58.060 --> 01:38:59.540]   like free credit.
[01:38:59.540 --> 01:39:02.100]   Oh, they were causing the churn.
[01:39:02.100 --> 01:39:06.020]   So they were sort of, yeah, or they were predicting the churn, but then they had this
[01:39:06.020 --> 01:39:08.500]   number, it turned out to be completely unrelated.
[01:39:08.500 --> 01:39:11.740]   Like it had nothing to do with what they were, you know, trying to solve.
[01:39:11.740 --> 01:39:19.300]   And the, the, the solution, which, you know, came out in these algorithms was that actually,
[01:39:19.300 --> 01:39:24.060]   you know, maybe they should offer some other, you know, some other solution that the free
[01:39:24.060 --> 01:39:25.460]   credits were not working.
[01:39:25.460 --> 01:39:26.460]   Yeah.
[01:39:26.460 --> 01:39:27.820]   Like, you know, give them a phone call.
[01:39:27.820 --> 01:39:32.020]   And there was another one with a bank where they were trying to figure out it was kind
[01:39:32.020 --> 01:39:33.020]   of the same thing.
[01:39:33.020 --> 01:39:35.620]   Like, how do we not lose customers?
[01:39:35.620 --> 01:39:39.580]   And it turned out that some of the customers that they were called, they would like call
[01:39:39.580 --> 01:39:45.020]   these people and try to like touch base with customers and turn out that that actually
[01:39:45.020 --> 01:39:46.780]   was causing people to leave.
[01:39:46.780 --> 01:39:47.780]   It's chasing them.
[01:39:47.780 --> 01:39:48.780]   There's like this.
[01:39:48.780 --> 01:39:53.100]   There's like certain, there's a certain customer that like, if you call them, they
[01:39:53.100 --> 01:39:56.060]   will actually like they'll think about it and they'll leave.
[01:39:56.060 --> 01:40:00.260]   But if you just do nothing, yeah, and they'll just, they won't, they'll forget it or that.
[01:40:00.260 --> 01:40:01.260]   I don't know for every reason.
[01:40:01.260 --> 01:40:04.460]   Don't remind them they're paying for the service by calling them.
[01:40:04.460 --> 01:40:05.460]   Right.
[01:40:05.460 --> 01:40:08.700]   So they saved this bank like two or three.
[01:40:08.700 --> 01:40:12.220]   So the, I mean, those are some examples, but I think that, that what's fascinating about
[01:40:12.220 --> 01:40:19.060]   it is that it's this idea that like, I mean, understanding cause and effect is like a,
[01:40:19.060 --> 01:40:21.380]   it's an essential part of intelligence, right?
[01:40:21.380 --> 01:40:25.780]   And if we do want to get, I don't think that we're going to get to AGI as we've sort of
[01:40:25.780 --> 01:40:26.980]   been joking about on this shit.
[01:40:26.980 --> 01:40:29.300]   Like, I don't think, I don't think we're getting there.
[01:40:29.300 --> 01:40:35.900]   But if we do want to make these tools much more powerful and much more, you know, I guess
[01:40:35.900 --> 01:40:38.660]   trustworthy, useful.
[01:40:38.660 --> 01:40:40.180]   There are going to have to be breakthroughs.
[01:40:40.180 --> 01:40:44.260]   And I don't think it's just going to be, you know, large language models getting bigger
[01:40:44.260 --> 01:40:45.260]   and bigger.
[01:40:45.260 --> 01:40:49.620]   I think it's going to be other areas of, you know, artificial intelligence where there
[01:40:49.620 --> 01:40:51.460]   are breakthroughs.
[01:40:51.460 --> 01:40:53.700]   And those things sort of combining.
[01:40:53.700 --> 01:40:54.700]   I agree.
[01:40:54.700 --> 01:40:59.180]   I think that's the risk of the success of chat GPT in large language models is that people
[01:40:59.180 --> 01:41:04.860]   will then focus on these instead of trying to be, think of more appropriate and better
[01:41:04.860 --> 01:41:11.340]   use of more useful ways to use AI than the ones that everybody's all excited about,
[01:41:11.340 --> 01:41:15.260]   you know, there is a big risk, I think in that.
[01:41:15.260 --> 01:41:21.740]   And we're worrying so much about, you know, existential hazards to humans that you miss
[01:41:21.740 --> 01:41:24.340]   the fact that they, I love this.
[01:41:24.340 --> 01:41:26.540]   They called you have this in your interview.
[01:41:26.540 --> 01:41:32.340]   They called the, the customers who were just kind of happily paying for something, sleeping
[01:41:32.340 --> 01:41:38.020]   dogs, and you don't want to wake the sleeping dogs up with a call saying, how do you like
[01:41:38.020 --> 01:41:39.020]   our surface?
[01:41:39.020 --> 01:41:42.380]   Because, well, better to let sleeping dogs lie, right?
[01:41:42.380 --> 01:41:44.900]   You know, it's actually very clever.
[01:41:44.900 --> 01:41:46.540]   But it's that's that's something.
[01:41:46.540 --> 01:41:50.540]   I mean, I really believe in the use of AI for analysis of existing data sets.
[01:41:50.540 --> 01:41:52.220]   You don't get the hallucinations.
[01:41:52.220 --> 01:41:55.540]   Ben, you must, I presume you're going to be looking at this.
[01:41:55.540 --> 01:41:57.140]   And I hope we look at it.
[01:41:57.140 --> 01:42:00.820]   We're going to do an AI show in the club with Jason and Jeff Jarvis.
[01:42:00.820 --> 01:42:02.420]   I hope you'll be looking at this too, though.
[01:42:02.420 --> 01:42:09.300]   These other kind of non-LLM uses of AI, I think some of them are even more interesting,
[01:42:09.300 --> 01:42:14.060]   but they're more narrow so they don't have the broad general appeal.
[01:42:14.060 --> 01:42:20.140]   Generative AI is just one subset, like one approach and large language models are just
[01:42:20.140 --> 01:42:25.900]   one approach of using deep learning and machine learning, which is like, like I, I'd have
[01:42:25.900 --> 01:42:29.380]   to like put up like the graph because you have like AI is this weird umbrella term for
[01:42:29.380 --> 01:42:34.020]   all these different terms for machine learning neural networks, all sorts of things.
[01:42:34.020 --> 01:42:37.220]   And then you have like, you know, machine learning neural networks within it, deep learning
[01:42:37.220 --> 01:42:38.580]   within it.
[01:42:38.580 --> 01:42:43.100]   And one piece of that is the generative AI slash large and large language models is a
[01:42:43.100 --> 01:42:44.780]   subset of that.
[01:42:44.780 --> 01:42:46.820]   And they're very good at a very specific thing.
[01:42:46.820 --> 01:42:50.420]   And like, I could go into the explanation of embeddings because I just find it fascinating.
[01:42:50.420 --> 01:42:55.580]   But at the end of the day, it's just a predictive technology to predict the next most likely
[01:42:55.580 --> 01:42:56.580]   word.
[01:42:56.580 --> 01:42:58.540]   It doesn't really well.
[01:42:58.540 --> 01:43:05.820]   But that is not enough of an approach to approximate a human, for example, the HCI discussion.
[01:43:05.820 --> 01:43:09.740]   You have to have other types of learning.
[01:43:09.740 --> 01:43:13.140]   Causal AI is one example of that.
[01:43:13.140 --> 01:43:15.900]   And as you know, generative AI has lots of weaknesses.
[01:43:15.900 --> 01:43:18.180]   It's not very good at doing math.
[01:43:18.180 --> 01:43:20.220]   It was not made for that, for example.
[01:43:20.220 --> 01:43:24.620]   And so it can guess the number, but it's not doing actual computation.
[01:43:24.620 --> 01:43:30.940]   But there's other AI systems that actually can and causal AI can actually like find cause
[01:43:30.940 --> 01:43:33.460]   relationship in a way that generative AI can't.
[01:43:33.460 --> 01:43:37.420]   So at some point, like, if you think of a human as just a bunch of AI is like a bunch
[01:43:37.420 --> 01:43:43.740]   of the eyes and AI's put together, you know, there's like 170 of them or some ridiculous
[01:43:43.740 --> 01:43:46.060]   number inside of our brains.
[01:43:46.060 --> 01:43:50.820]   And generative, like the ability to predict the next most likely word is probably 1/170
[01:43:50.820 --> 01:43:54.020]   of human capacity and potential.
[01:43:54.020 --> 01:43:57.660]   So we need more of those.
[01:43:57.660 --> 01:44:03.100]   Causal is very interesting in particular, but I think we'll see more fields of AI start
[01:44:03.100 --> 01:44:08.100]   to pop up as we hit the limits of what you can do with the large language model.
[01:44:08.100 --> 01:44:12.020]   I put this in the show notes and it's really too much to ask anybody to read.
[01:44:12.020 --> 01:44:15.100]   And it's certainly far too much for me to comment on the show.
[01:44:15.100 --> 01:44:17.420]   But I encourage our listeners to read it.
[01:44:17.420 --> 01:44:23.340]   It's from an engineer at DeepMind in London, Jen Dong Wang.
[01:44:23.340 --> 01:44:30.700]   He posted last month, why transformative AI is really, really hard to achieve.
[01:44:30.700 --> 01:44:32.740]   And he's saying the same thing in a way.
[01:44:32.740 --> 01:44:40.380]   He's saying people are very optimistic that AI is a kind of a different category of innovation,
[01:44:40.380 --> 01:44:48.700]   that it's going to transform things in a way that other great inventions didn't.
[01:44:48.700 --> 01:44:54.820]   And he cautions people that probably AI will be transformative in the same way, say, the
[01:44:54.820 --> 01:44:58.300]   internet was, or the Industrial Revolution.
[01:44:58.300 --> 01:45:02.980]   But he said, "Transforming transformative AI is very difficult."
[01:45:02.980 --> 01:45:05.420]   And he brings up three arguments.
[01:45:05.420 --> 01:45:06.420]   Why?
[01:45:06.420 --> 01:45:11.580]   One is that the transformational potential of AI is constrained by its hardest problems.
[01:45:11.580 --> 01:45:18.380]   He quotes a well-known aphorism from, it's called, "More of X Paradox."
[01:45:18.380 --> 01:45:21.260]   Stephen Pinker wrote about it in 1994.
[01:45:21.260 --> 01:45:27.700]   The main lessons of 35 years of AI research is the hard problems are easy and the easy
[01:45:27.700 --> 01:45:29.980]   problems are hard.
[01:45:29.980 --> 01:45:35.860]   And I think we're already seeing that with self-driving vehicles and handwriting recognition,
[01:45:35.860 --> 01:45:37.020]   face recognition.
[01:45:37.020 --> 01:45:43.300]   He also says, "Despite rapid progress in some AI subfields, major technical hurdles remain."
[01:45:43.300 --> 01:45:46.900]   And finally, and this is, I thought, the most interesting, even if technical AI progress
[01:45:46.900 --> 01:45:52.700]   continues, social and economic hurdles, not technical hurdles, but social and economic
[01:45:52.700 --> 01:45:55.500]   hurdles, may limit its impact.
[01:45:55.500 --> 01:46:04.660]   And he uses, as an example, how the Industrial Revolution has, you know, technology has reduced
[01:46:04.660 --> 01:46:10.380]   the price of a huge number of goods and services, new cars, household furnishings, clothing,
[01:46:10.380 --> 01:46:13.620]   cell phones, computer software, toys and TVs.
[01:46:13.620 --> 01:46:22.620]   But it hasn't had an overall impact on inflation because non-technology supported products
[01:46:22.620 --> 01:46:30.220]   like hospital services, college tuition, medical care, food and beverages, housing have gone
[01:46:30.220 --> 01:46:32.700]   up commensurately.
[01:46:32.700 --> 01:46:39.500]   So there's this kind of perverse cycle that happens that, yeah, your AI is bringing down
[01:46:39.500 --> 01:46:43.900]   the cost of these things, but it's bringing up because it's not solving other problems,
[01:46:43.900 --> 01:46:46.580]   it's bringing up the cost of those things.
[01:46:46.580 --> 01:46:52.260]   So a really good article, we don't need to debate it, but I thought I'd put it in people's
[01:46:52.260 --> 01:46:55.700]   in-tray inbox to look at.
[01:46:55.700 --> 01:46:57.300]   I thought it was quite good.
[01:46:57.300 --> 01:47:04.340]   Even its author, as a deep mind engineer, is obviously deeply involved in this, very provocative.
[01:47:04.340 --> 01:47:10.460]   He has a good picture too of King Charles operating the London tube, and he points out
[01:47:10.460 --> 01:47:15.780]   that train drivers are paid close to twice the national median in London, even though
[01:47:15.780 --> 01:47:21.300]   the technology to replace them wholly has existed for decades.
[01:47:21.300 --> 01:47:24.500]   Even King Charles can drive one of these.
[01:47:24.500 --> 01:47:31.100]   But it can easily be automated, but it hasn't for a variety of social reasons and other
[01:47:31.100 --> 01:47:32.900]   reasons.
[01:47:32.900 --> 01:47:36.660]   Automation is the right word, right?
[01:47:36.660 --> 01:47:41.940]   AI is to your Ben's point earlier, I think AI is a really bad term.
[01:47:41.940 --> 01:47:42.940]   It's terrible.
[01:47:42.940 --> 01:47:43.940]   It's terrible.
[01:47:43.940 --> 01:47:51.420]   Even though I use it every day in the newsletter, it's just not intelligence, it's automation.
[01:47:51.420 --> 01:47:56.980]   I think Ben, the one thing I disagreed with what you said was that the brain is all these
[01:47:56.980 --> 01:47:59.700]   different algorithms put together.
[01:47:59.700 --> 01:48:05.180]   I don't think we have any idea how the human brain works.
[01:48:05.180 --> 01:48:11.660]   And yet, all these AI technologies or automation technologies are named after the human brain,
[01:48:11.660 --> 01:48:12.660]   like neural nets.
[01:48:12.660 --> 01:48:18.100]   You read it all the time, like neural nets are like, it's an architecture based on how
[01:48:18.100 --> 01:48:19.300]   the human brain works.
[01:48:19.300 --> 01:48:21.100]   It's really not.
[01:48:21.100 --> 01:48:22.820]   It has nothing to do with that.
[01:48:22.820 --> 01:48:25.540]   We don't really know how the human brain works.
[01:48:25.540 --> 01:48:26.540]   That's part of the problem.
[01:48:26.540 --> 01:48:27.540]   Right.
[01:48:27.540 --> 01:48:31.380]   I just think it's actually kind of, it would be helpful if we could use different vocabulary
[01:48:31.380 --> 01:48:37.260]   for this because I think people would like, they would freak out less and it would be
[01:48:37.260 --> 01:48:42.500]   like, look at this in a much more sort of practical way, which is like, here are these
[01:48:42.500 --> 01:48:47.140]   new tools that people are creating, just like they have forever.
[01:48:47.140 --> 01:48:48.700]   We answer, it premorpifies it.
[01:48:48.700 --> 01:48:54.740]   And by calling it intelligence or saying it's thinking or saying it's a he or it's hallucinating,
[01:48:54.740 --> 01:48:58.940]   all implies some sort of human mechanism that isn't really present.
[01:48:58.940 --> 01:48:59.940]   Right.
[01:48:59.940 --> 01:49:01.580]   And we all do it.
[01:49:01.580 --> 01:49:02.580]   I do it.
[01:49:02.580 --> 01:49:03.580]   Yeah.
[01:49:03.580 --> 01:49:07.700]   I mean, I do it all every day, but it's just Kevin Roost did it when he said, we need language.
[01:49:07.700 --> 01:49:10.860]   He said, chat, you be dead, fallen in love with him in the New York Times.
[01:49:10.860 --> 01:49:11.860]   No, everybody.
[01:49:11.860 --> 01:49:14.220]   I mean, how ridiculous is that?
[01:49:14.220 --> 01:49:22.220]   Look, there's, there's an entire community and like you can find and tell you the subreddits
[01:49:22.220 --> 01:49:28.180]   that are just hoping and praying that open AI secretly has like the super intelligence
[01:49:28.180 --> 01:49:34.460]   AGI and they're going back forth and they're just so scared to release it to the public.
[01:49:34.460 --> 01:49:36.380]   And they're just trying to repair us for it.
[01:49:36.380 --> 01:49:40.140]   And also there's UFO bodies in Roswell.
[01:49:40.140 --> 01:49:41.380]   We know that for a fact.
[01:49:41.380 --> 01:49:43.220]   So, you know, it's a fact.
[01:49:43.220 --> 01:49:44.660]   That's the subreddit right next door.
[01:49:44.660 --> 01:49:46.020]   I think that that's a bit.
[01:49:46.020 --> 01:49:48.660]   I mean, it's just again, back to being human, right?
[01:49:48.660 --> 01:49:53.500]   Like we're just, you know, humans are, you know, we go a little crazy.
[01:49:53.500 --> 01:49:54.860]   I've had this conversation.
[01:49:54.860 --> 01:49:58.740]   I'm trying to talk to all the smart people I know and I know a lot of smart people.
[01:49:58.740 --> 01:50:01.180]   And that's one of the questions that I have.
[01:50:01.180 --> 01:50:04.060]   And there really are two answers to this.
[01:50:04.060 --> 01:50:11.020]   I mean, you know, people like Ray Kurzweil who say the singularity is near who really does
[01:50:11.020 --> 01:50:14.060]   believe that an artificial general intelligence will be.
[01:50:14.060 --> 01:50:19.380]   He defines a singularity as indistinguishable from humans so that we won't really know
[01:50:19.380 --> 01:50:20.540]   if it's a human.
[01:50:20.540 --> 01:50:25.860]   It's not merely the Turing test, but they will literally be, you know, human, so human-like
[01:50:25.860 --> 01:50:26.860]   that we won't know the difference.
[01:50:26.860 --> 01:50:28.060]   I said, but they're not human, right?
[01:50:28.060 --> 01:50:33.300]   He said, but it doesn't matter if you can't tell the difference.
[01:50:33.300 --> 01:50:39.860]   And of course, the real threat there is once they become that adept, then they can design
[01:50:39.860 --> 01:50:45.820]   themselves and their acceleration and advancement of a, and I think this is what this whole,
[01:50:45.820 --> 01:50:50.340]   you know, threat to humanity is become so fast that we don't, you know, just they exceed
[01:50:50.340 --> 01:50:55.940]   us by such a great margin so quickly that we're just little worms to them.
[01:50:55.940 --> 01:50:58.660]   I think there's, and this is really where it comes down to.
[01:50:58.660 --> 01:51:00.220]   I'm curious what you all think.
[01:51:00.220 --> 01:51:07.100]   I think there is a gap and I'm not a religious person and I'm not talking about the soul,
[01:51:07.100 --> 01:51:12.660]   but I think there's a gap between the most that you can do with a kind of a von Neumann
[01:51:12.660 --> 01:51:17.060]   machine with a mechanized intelligence.
[01:51:17.060 --> 01:51:20.060]   There's a gap between that and what we do as humans.
[01:51:20.060 --> 01:51:24.460]   Now some, some of my friends, Steve Gibson, our security guy says, no, no, consciousness
[01:51:24.460 --> 01:51:26.380]   is an emergent property.
[01:51:26.380 --> 01:51:31.420]   You just have to get the intelligent machines fast enough, give them enough RAM, and they
[01:51:31.420 --> 01:51:32.740]   will be conscious.
[01:51:32.740 --> 01:51:34.020]   They'll be indistinguishable.
[01:51:34.020 --> 01:51:37.420]   You're just machines in other words.
[01:51:37.420 --> 01:51:41.820]   I am not religious, but I don't, I don't feel like that's true.
[01:51:41.820 --> 01:51:44.660]   Where do you, what do you think of that?
[01:51:44.660 --> 01:51:46.300]   I mean, let me ask you, Seth.
[01:51:46.300 --> 01:51:47.300]   I don't think it's true.
[01:51:47.300 --> 01:51:48.300]   You don't think it's true.
[01:51:48.300 --> 01:51:50.700]   Reed says no.
[01:51:50.700 --> 01:51:53.020]   I mean, I think that's the big question, right?
[01:51:53.020 --> 01:51:54.020]   We don't know.
[01:51:54.020 --> 01:51:55.020]   We don't know.
[01:51:55.020 --> 01:51:56.620]   Like what's going on in our head is just like a.
[01:51:56.620 --> 01:51:59.860]   Are we machines or is there something special that we do?
[01:51:59.860 --> 01:52:05.340]   This is just a down to semantics and definitions because there is some machinery like things
[01:52:05.340 --> 01:52:07.340]   that's making me move my arms.
[01:52:07.340 --> 01:52:08.340]   Yeah.
[01:52:08.340 --> 01:52:09.340]   That's a chemical reaction.
[01:52:09.340 --> 01:52:10.340]   Yeah.
[01:52:10.340 --> 01:52:11.340]   Yeah.
[01:52:11.340 --> 01:52:12.340]   Right.
[01:52:12.340 --> 01:52:15.500]   But like what point is like which and like how is the cross over?
[01:52:15.500 --> 01:52:16.500]   Yeah.
[01:52:16.500 --> 01:52:17.500]   What?
[01:52:17.500 --> 01:52:18.500]   Yeah.
[01:52:18.500 --> 01:52:19.500]   What are you doing?
[01:52:19.500 --> 01:52:20.500]   It's better.
[01:52:20.500 --> 01:52:21.500]   It's not.
[01:52:21.500 --> 01:52:22.500]   It's not.
[01:52:22.500 --> 01:52:23.500]   Yes.
[01:52:23.500 --> 01:52:23.500]   Now I will point out that, and this is from Scientific American, a 25 year old bed about
[01:52:23.500 --> 01:52:25.820]   consciousness has finally been settled.
[01:52:25.820 --> 01:52:33.740]   The brain scientist and a philosopher 25 years ago made a bet.
[01:52:33.740 --> 01:52:39.540]   The brain, this was at an event called toward a scientific basis for consciousness.
[01:52:39.540 --> 01:52:42.460]   Christoph Koch was the neuroscientist.
[01:52:42.460 --> 01:52:48.060]   David Chalmers was the philosopher.
[01:52:48.060 --> 01:52:56.660]   The neuroscientist believed that consciousness was an emergent property and that we would
[01:52:56.660 --> 01:52:57.660]   correct.
[01:52:57.660 --> 01:53:03.220]   He said scientists would crack consciousness by discovering its neural underpinnings or
[01:53:03.220 --> 01:53:06.500]   correlates by now.
[01:53:06.500 --> 01:53:14.420]   The philosopher said neither 40 Hertz oscillations or any other strictly physical process could
[01:53:14.420 --> 01:53:22.020]   account for conscious sensations and that consciousness was not something we would solve.
[01:53:22.020 --> 01:53:23.020]   He won.
[01:53:23.020 --> 01:53:24.020]   Obviously we don't know what.
[01:53:24.020 --> 01:53:25.740]   Here we are 25 years later.
[01:53:25.740 --> 01:53:31.940]   We are no closer to understanding consciousness than we were 25 years ago in 1994.
[01:53:31.940 --> 01:53:35.700]   So actually, Koch paid that wager.
[01:53:35.700 --> 01:53:41.780]   Well, you mentioned more of X paradox in the article.
[01:53:41.780 --> 01:53:48.740]   I think more of X paradox is really, I've long thought that it always turns out to fit that
[01:53:48.740 --> 01:53:53.740]   and maybe we overthink what he really meant by it.
[01:53:53.740 --> 01:53:57.780]   He was a brilliant thinker.
[01:53:57.780 --> 01:54:01.900]   He also believed that we were going to have consciousness pretty soon.
[01:54:01.900 --> 01:54:08.100]   Computers have become conscious fairly soon, I think.
[01:54:08.100 --> 01:54:13.100]   And obviously, he was totally wrong, which I think is a reminder that there are people
[01:54:13.100 --> 01:54:19.260]   who are absolutely brilliant, who are pioneers in this technology, who can also believe in
[01:54:19.260 --> 01:54:28.740]   an almost supernatural philosophy, I guess, that this will eventually become human.
[01:54:28.740 --> 01:54:31.260]   That's what gives me pause.
[01:54:31.260 --> 01:54:35.780]   Because the people who signed that letter saying AI was an existential threat to humanity
[01:54:35.780 --> 01:54:41.500]   where some of the top, I mean, wasn't just Sam Alman, it was Jeffrey Hinton, it was some
[01:54:41.500 --> 01:54:47.220]   of the top people in the field, people I respect, who presumably know a lot more about AI than
[01:54:47.220 --> 01:54:49.060]   I ever will.
[01:54:49.060 --> 01:54:51.820]   And they're worried, but I don't understand why.
[01:54:51.820 --> 01:54:52.820]   It's a big bit more interesting.
[01:54:52.820 --> 01:54:53.820]   Yeah.
[01:54:53.820 --> 01:54:54.900]   Like even previous generations, right?
[01:54:54.900 --> 01:54:56.300]   It's something that happens.
[01:54:56.300 --> 01:54:58.660]   I think they read too much sci-fi, personally.
[01:54:58.660 --> 01:55:01.820]   I think it's science fiction.
[01:55:01.820 --> 01:55:05.100]   I think it's also, it's like the language we use to describe it, right?
[01:55:05.100 --> 01:55:10.900]   But if the language you're using to describe your field is like neural nets and all this
[01:55:10.900 --> 01:55:11.980]   brain related-
[01:55:11.980 --> 01:55:14.420]   They painted themselves in that corner.
[01:55:14.420 --> 01:55:15.420]   Yeah.
[01:55:15.420 --> 01:55:16.420]   Yeah.
[01:55:16.420 --> 01:55:19.820]   I think you start to believe that you're creating consciousness.
[01:55:19.820 --> 01:55:23.260]   And that's like a very powerful thing and it's very emotional.
[01:55:23.260 --> 01:55:30.380]   I think, but if you look at the way this technology actually works, it's so clearly not-
[01:55:30.380 --> 01:55:31.380]   It's mechanistic.
[01:55:31.380 --> 01:55:32.380]   You know, headed in that.
[01:55:32.380 --> 01:55:33.380]   Yeah.
[01:55:33.380 --> 01:55:34.380]   Right.
[01:55:34.380 --> 01:55:39.580]   You wouldn't, you know, you look at a self-drive, you look at a cruise.
[01:55:39.580 --> 01:55:43.700]   It's not confused by the cone on its hood.
[01:55:43.700 --> 01:55:46.260]   Like, it's not an existential crisis.
[01:55:46.260 --> 01:55:47.260]   Why am I here?
[01:55:47.260 --> 01:55:48.260]   What am I doing?
[01:55:48.260 --> 01:55:49.580]   I can't drive.
[01:55:49.580 --> 01:55:50.580]   It's just the-
[01:55:50.580 --> 01:55:53.780]   Why can't I fly?
[01:55:53.780 --> 01:55:54.780]   The-
[01:55:54.780 --> 01:55:59.780]   If you put chat GPT in charge of like really important stuff, like, you know-
[01:55:59.780 --> 01:56:00.780]   Right.
[01:56:00.780 --> 01:56:04.340]   Well, don't give it lasers and don't give it nuclear weapons.
[01:56:04.340 --> 01:56:09.540]   Not because it's going to decide, oh, we don't need these worms anymore, but because mistakes
[01:56:09.540 --> 01:56:10.540]   happen.
[01:56:10.540 --> 01:56:11.540]   Right.
[01:56:11.540 --> 01:56:13.420]   Like really bad stuff would happen if you did that.
[01:56:13.420 --> 01:56:15.940]   But I don't think anybody is saying that you should.
[01:56:15.940 --> 01:56:16.940]   Right.
[01:56:16.940 --> 01:56:19.540]   You know, I think it's kind of a, you know-
[01:56:19.540 --> 01:56:20.540]   Nobody's proposing that.
[01:56:20.540 --> 01:56:21.540]   That's kind of a meetup problem.
[01:56:21.540 --> 01:56:23.540]   And if they are, well, don't.
[01:56:23.540 --> 01:56:24.540]   They're in there's a-
[01:56:24.540 --> 01:56:25.540]   Yeah, right.
[01:56:25.540 --> 01:56:26.540]   Don't.
[01:56:26.540 --> 01:56:31.540]   They're- and Gen Dong Wang's article, he has a picture of a very famous, well, it was an
[01:56:31.540 --> 01:56:39.700]   art project in 1971 where he took an autonomous vehicle and put it in a circle with dotted
[01:56:39.700 --> 01:56:40.700]   lines.
[01:56:40.700 --> 01:56:45.420]   In other words, you know, a do not pass zone all the way around it.
[01:56:45.420 --> 01:56:50.540]   This is equivalent to putting an orange cone on the hood of a cruise.
[01:56:50.540 --> 01:56:56.700]   And the autonomous vehicle said, I can't cross those lines.
[01:56:56.700 --> 01:57:01.980]   The easy stuff is hard, but the hard stuff is easy.
[01:57:01.980 --> 01:57:05.540]   The easy stuff, hard stuff is easy, but the easy stuff is hard.
[01:57:05.540 --> 01:57:08.340]   I mean, a three-year-old can recognize its mom.
[01:57:08.340 --> 01:57:09.340]   Right?
[01:57:09.340 --> 01:57:10.340]   Yeah.
[01:57:10.340 --> 01:57:11.340]   All right.
[01:57:11.340 --> 01:57:12.340]   Let's take a little break.
[01:57:12.340 --> 01:57:13.980]   That was very heavy and philosophical.
[01:57:13.980 --> 01:57:15.060]   We're going to stop that.
[01:57:15.060 --> 01:57:16.060]   Stop that right now.
[01:57:16.060 --> 01:57:20.660]   And when we come back, actual tech news, but first a word from our sponsor, the sponsor,
[01:57:20.660 --> 01:57:24.540]   the studio, you've seen the signage, ACI learning.
[01:57:24.540 --> 01:57:27.620]   ITPro is now a part of ACI learning.
[01:57:27.620 --> 01:57:28.620]   And you know ITPro.
[01:57:28.620 --> 01:57:32.140]   They provided our listeners with engaging and entertaining IT training for more than
[01:57:32.140 --> 01:57:33.140]   10 years.
[01:57:33.140 --> 01:57:34.980]   We've been with them since every step of the way.
[01:57:34.980 --> 01:57:40.180]   Now they're part of ACI learning, which means they have really elevated their capabilities,
[01:57:40.180 --> 01:57:46.460]   bringing you better and more highly entertaining, bingeable, short-form content, more than 7,000
[01:57:46.460 --> 01:57:51.740]   hours to help you become better at IT, whether you're an individual looking to get into the
[01:57:51.740 --> 01:57:57.860]   business or a business with an IT department, you want to get better.
[01:57:57.860 --> 01:58:01.660]   Maybe you're an MSP and you have a bunch of engineers who need to learn this stuff.
[01:58:01.660 --> 01:58:04.340]   There's no better place than ACI learning.
[01:58:04.340 --> 01:58:06.060]   They provide you with world-class service.
[01:58:06.060 --> 01:58:10.380]   They'll help you choose which learning path suits you best all the way through helping
[01:58:10.380 --> 01:58:12.580]   you find the right career opportunity.
[01:58:12.580 --> 01:58:14.900]   So if you're just getting started, you can get some instruction.
[01:58:14.900 --> 01:58:16.380]   They'll help you get that first job.
[01:58:16.380 --> 01:58:21.100]   And if you're already in IT or you have an IT team, you could fortify your expertise
[01:58:21.100 --> 01:58:28.500]   with access to self-paced IT training videos, really wonderful interactive practice labs.
[01:58:28.500 --> 01:58:31.260]   Those are fantastic and certification practice tests.
[01:58:31.260 --> 01:58:32.940]   The practice labs are great.
[01:58:32.940 --> 01:58:36.780]   MSPs love them because they can install stuff, try stuff.
[01:58:36.780 --> 01:58:41.940]   All it takes is an HTML5 browser, so it works on a Chromebook or Mac OS or Windows or Linux,
[01:58:41.940 --> 01:58:44.620]   even iOS or Android.
[01:58:44.620 --> 01:58:49.220]   And you can set up multiple instances of Windows Server and Windows clients.
[01:58:49.220 --> 01:58:51.140]   You can test and experiment.
[01:58:51.140 --> 01:58:54.060]   It's a great way to learn if something goes wrong.
[01:58:54.060 --> 01:58:57.180]   What I do what I do, which is you close the tab.
[01:58:57.180 --> 01:58:59.140]   And it's all gone.
[01:58:59.140 --> 01:59:00.500]   We can start over.
[01:59:00.500 --> 01:59:04.380]   You can also think and retake practice exams.
[01:59:04.380 --> 01:59:06.500]   So you're ready when you sit for the real thing.
[01:59:06.500 --> 01:59:12.580]   ACI learning brings you IT practice exam questions from Microsoft, CompTIA, EC Council, PMI and
[01:59:12.580 --> 01:59:13.580]   more.
[01:59:13.580 --> 01:59:17.860]   ACI learning is with you every step of the way to help you choose the best certifications
[01:59:17.860 --> 01:59:19.500]   for your needs.
[01:59:19.500 --> 01:59:25.660]   Whether it's Microsoft, Azure, administrator, CompTIA, SISA, security plus or A plus, the
[01:59:25.660 --> 01:59:29.860]   accredited ITIL-4 Foundation, the Cisco CCNA.
[01:59:29.860 --> 01:59:35.620]   They've got classes in AWS certified cloud practitioner, AWS certified solutions architect
[01:59:35.620 --> 01:59:36.620]   associate.
[01:59:36.620 --> 01:59:38.300]   Listen to all these certs you can study for.
[01:59:38.300 --> 01:59:42.420]   CISSP, that's Certified Information Systems Security Professional.
[01:59:42.420 --> 01:59:48.900]   I love from the EC2, the CEH Certified Ethical Hackers cert.
[01:59:48.900 --> 01:59:52.180]   That's such a, I want to get that, hang that on my wall.
[01:59:52.180 --> 01:59:54.460]   But it's also a great way to get a job, right?
[01:59:54.460 --> 01:59:55.460]   Try it for yourself.
[01:59:55.460 --> 01:59:59.820]   Then bring your whole team along for teams of two to one thousand volume discounts started
[01:59:59.820 --> 02:00:01.300]   five seats.
[02:00:01.300 --> 02:00:03.580]   Visit go.acilarning.com/twit.
[02:00:03.580 --> 02:00:09.100]   Get your teams two week trial at go.acilarning.com/twit.
[02:00:09.100 --> 02:00:14.460]   Learn more about ACI learning premium training options across audit, IT and cybersecurity readiness
[02:00:14.460 --> 02:00:18.940]   at go.acilarning.com/twit.
[02:00:18.940 --> 02:00:24.060]   And if you're an individual, that offer code, TWIT30, gets you 30% off a standard of premium
[02:00:24.060 --> 02:00:26.020]   individual IT Pro membership.
[02:00:26.020 --> 02:00:31.500]   TWIT30, the offer code, go.acilarning.com/twit.
[02:00:31.500 --> 02:00:35.300]   We thank them so much for the support they've given us over the years.
[02:00:35.300 --> 02:00:40.500]   As IT Pro and now as ACI learning our studio sponsors.
[02:00:40.500 --> 02:00:43.260]   This is late breaking news, but kind of disappointing.
[02:00:43.260 --> 02:00:45.700]   For many years, I was a big Evernote fan.
[02:00:45.700 --> 02:00:48.820]   Phil Libman, who's been on our show many times, was the CEO.
[02:00:48.820 --> 02:00:54.860]   He told me at one point that more than half of their signups came from me, people I recommended
[02:00:54.860 --> 02:00:58.220]   use Evernote, loved Evernote.
[02:00:58.220 --> 02:01:02.300]   They've gone through a lot of troubles and eventually were bought by an Italian company
[02:01:02.300 --> 02:01:06.740]   called Bending Spoons.
[02:01:06.740 --> 02:01:12.020]   They have fired 129 employees now.
[02:01:12.020 --> 02:01:16.820]   It looks like, I don't know if it's the end of the line, but the Milan-based Bending Spoons
[02:01:16.820 --> 02:01:22.820]   has shut down their US operations.
[02:01:22.820 --> 02:01:23.820]   Kind of sad.
[02:01:23.820 --> 02:01:24.820]   Kind of the end of the line, I think.
[02:01:24.820 --> 02:01:29.060]   I've said this before forever now, but maybe this is the real deal.
[02:01:29.060 --> 02:01:30.460]   The real deal.
[02:01:30.460 --> 02:01:31.860]   End of the line.
[02:01:31.860 --> 02:01:34.340]   Did you use Evernote, Ben?
[02:01:34.340 --> 02:01:35.340]   All the time.
[02:01:35.340 --> 02:01:36.340]   Yeah.
[02:01:36.340 --> 02:01:37.340]   And look, Phil's a friend too.
[02:01:37.340 --> 02:01:40.060]   Phil was one of the very first investors.
[02:01:40.060 --> 02:01:44.620]   In Octane, back in the day, this is, of course, after he left Evernote.
[02:01:44.620 --> 02:01:46.420]   But I switched over.
[02:01:46.420 --> 02:01:51.980]   I put my stuff into Apple Notes and I put my stuff over into Notion a little while ago
[02:01:51.980 --> 02:01:55.300]   because Notion's fantastic.
[02:01:55.300 --> 02:02:03.940]   The Rags been on the wall for Evernote for a while, unfortunately.
[02:02:03.940 --> 02:02:10.900]   And it is sad because it was a great product and it could have very well come out with
[02:02:10.900 --> 02:02:12.900]   Notion features and other things.
[02:02:12.900 --> 02:02:16.860]   But once you didn't have someone like Phil at the helm, you didn't have founders, you
[02:02:16.860 --> 02:02:19.060]   had all those kinds of things.
[02:02:19.060 --> 02:02:23.980]   It just felt like nothing ever changed and no one really wanted to change things.
[02:02:23.980 --> 02:02:26.660]   Eventually, you can't win if you do that.
[02:02:26.660 --> 02:02:31.900]   Ironically, one of the nails in the coffin was they moved to subscription some years ago.
[02:02:31.900 --> 02:02:36.060]   Nowadays, I mean, Notion's a subscription product, everything's a subscription, including
[02:02:36.060 --> 02:02:38.900]   Microsoft Office.
[02:02:38.900 --> 02:02:43.260]   Had they done it recently, it might have not been in such a negative, but I think it really
[02:02:43.260 --> 02:02:45.140]   hurt them back when they did it.
[02:02:45.140 --> 02:02:49.300]   They were early on the subscription train.
[02:02:49.300 --> 02:02:52.620]   And I think it's true that there are so many other choices.
[02:02:52.620 --> 02:02:53.620]   I like Notion.
[02:02:53.620 --> 02:02:54.620]   I like Rome.
[02:02:54.620 --> 02:02:56.180]   I like Obsidian.
[02:02:56.180 --> 02:03:00.340]   I've been using a free open source product called LogSeqNow for a while.
[02:03:00.340 --> 02:03:01.940]   L-O-G-S-E-Q.
[02:03:01.940 --> 02:03:05.740]   It's fantastic and it's free.
[02:03:05.740 --> 02:03:07.860]   There's so many good choices out there.
[02:03:07.860 --> 02:03:14.300]   And it's come along with this idea of the personal knowledge management system, PKMS.
[02:03:14.300 --> 02:03:17.900]   And a lot of people, students, it started with college kids, but a lot of people now
[02:03:17.900 --> 02:03:24.740]   are starting to adopt these systems like ZetlKastin, which ties your thoughts together in such
[02:03:24.740 --> 02:03:30.860]   a way that you can take notes on what you're learning and then create new ideas based on
[02:03:30.860 --> 02:03:33.420]   the synergy of all these notes.
[02:03:33.420 --> 02:03:38.340]   It's a really interesting model these days.
[02:03:38.340 --> 02:03:43.180]   And I think Evernote, which really was the first, I mean, one note preceded it, but Evernote
[02:03:43.180 --> 02:03:45.140]   was so much better.
[02:03:45.140 --> 02:03:46.140]   It's kind of sad.
[02:03:46.140 --> 02:03:48.340]   I don't know if it's the end of the line, but it doesn't feel good.
[02:03:48.340 --> 02:03:52.500]   When you lay off 129 people, that's usually not a good sign.
[02:03:52.500 --> 02:03:54.940]   I hope they all land on their feet.
[02:03:54.940 --> 02:03:55.940]   That's what I hope.
[02:03:55.940 --> 02:03:59.340]   TechCrunch says they're getting good severance.
[02:03:59.340 --> 02:04:03.300]   They're not being dumped out.
[02:04:03.300 --> 02:04:07.340]   Actually, it's not TechCrunch, it's the San Francisco Chronicle.
[02:04:07.340 --> 02:04:09.260]   I guess they're all in San Francisco.
[02:04:09.260 --> 02:04:14.140]   The company will provide 16 weeks of salary, pro-rated performance benefit up to a year
[02:04:14.140 --> 02:04:19.780]   of health insurance and visa support to affected employees.
[02:04:19.780 --> 02:04:21.300]   It sounds like an Italian company.
[02:04:21.300 --> 02:04:22.300]   It does.
[02:04:22.300 --> 02:04:26.380]   They're probably required to, aren't they, by law?
[02:04:26.380 --> 02:04:30.980]   They say our plans forever note are as ambitious as ever.
[02:04:30.980 --> 02:04:35.900]   Going forward, a growing dedicated team based in Europe will continue to a some ownership
[02:04:35.900 --> 02:04:37.820]   of the Evernote product.
[02:04:37.820 --> 02:04:39.740]   Now, Eastern need to a vampire.
[02:04:39.740 --> 02:04:40.740]   I'm sorry.
[02:04:40.740 --> 02:04:43.340]   That wasn't my intent.
[02:04:43.340 --> 02:04:45.660]   You transported me to Lake Como.
[02:04:45.660 --> 02:04:47.100]   Yeah, I go to go home.
[02:04:47.100 --> 02:04:51.060]   I see banding spoons that had $100 million in.
[02:04:51.060 --> 02:04:52.860]   Now I'm Rosanne Rosanna Danna.
[02:04:52.860 --> 02:04:53.860]   I don't know.
[02:04:53.860 --> 02:04:54.860]   It's all over the place.
[02:04:54.860 --> 02:04:55.860]   They also got-
[02:04:55.860 --> 02:04:57.860]   I didn't say pivot to AI in Italian.
[02:04:57.860 --> 02:04:58.860]   That's fine.
[02:04:58.860 --> 02:04:59.860]   Pv.
[02:04:59.860 --> 02:05:08.900]   That's the way I, uh, uh, here in the annals of court judgments, last week was all about
[02:05:08.900 --> 02:05:09.900]   the Supreme Court.
[02:05:09.900 --> 02:05:19.420]   It's got a somewhat smaller court, uh, King's Bench for Saskatchewan, Canada.
[02:05:19.420 --> 02:05:25.940]   A Canadian farmer was sued for a breach of contract.
[02:05:25.940 --> 02:05:31.940]   So the story is it's all about, uh, flax, you know, flax, FLAX.
[02:05:31.940 --> 02:05:36.140]   It's like a, it's like a wheat thing.
[02:05:36.140 --> 02:05:44.180]   Green purchasers, uh, from a company called Southwest Terminal, we'll call them SWT, uh,
[02:05:44.180 --> 02:05:50.860]   wanted to buy flax for $17 a bushel for delivery in October, November, December.
[02:05:50.860 --> 02:05:53.860]   There's a futures contract, right?
[02:05:53.860 --> 02:06:02.620]   Uh, after phone calls with Bob and Chris actor, SWT, uh, drafted a contract for Chris
[02:06:02.620 --> 02:06:11.340]   to sell 86 metric tons of flax for $17 a bushel and deliver the flax in November.
[02:06:11.340 --> 02:06:17.260]   Uh, they had a real printed contract signed in ink.
[02:06:17.260 --> 02:06:19.380]   This is where they might have gone wrong.
[02:06:19.380 --> 02:06:23.780]   They sent, they took a picture of it with their camera phone and took a picture of the
[02:06:23.780 --> 02:06:32.180]   contract, sent him the photo with a message, please confirm flax contract to which the
[02:06:32.180 --> 02:06:36.780]   farmer responded with the thumbs up emoji.
[02:06:36.780 --> 02:06:47.700]   However, by November of 2021, the price of flax went from $17 to $41 a bushel.
[02:06:47.700 --> 02:06:51.380]   Farmer decided, no, thumbs down.
[02:06:51.380 --> 02:06:54.340]   And he, he did not deliver.
[02:06:54.340 --> 02:06:58.020]   So SWT went to court and said, well, wait a minute, he sent us the thumbs up and the
[02:06:58.020 --> 02:07:05.980]   dispute was, does the thumbs up emoji bind you to a contract?
[02:07:05.980 --> 02:07:09.220]   The farmer said, no, no, no, it just means I got it.
[02:07:09.220 --> 02:07:11.140]   I got the contract and I'll let you know.
[02:07:11.140 --> 02:07:12.980]   I'll follow up.
[02:07:12.980 --> 02:07:17.380]   Uh, no, the court said, no, no, no.
[02:07:17.380 --> 02:07:19.940]   That is a, that is a verbal contract.
[02:07:19.940 --> 02:07:23.660]   It's not, it's a thumbs up emoji contract.
[02:07:23.660 --> 02:07:28.300]   It's binding the judge wrote, I'm satisfied on the balance of probabilities that Chris
[02:07:28.300 --> 02:07:30.780]   okay to approve the contract.
[02:07:30.780 --> 02:07:36.260]   Just as he had done before, except this time he used a, and this is in the actual judgment,
[02:07:36.260 --> 02:07:38.940]   the thumbs up emoji.
[02:07:38.940 --> 02:07:44.180]   In my opinion, when considering all the circumstances that meant approval of the flax contract,
[02:07:44.180 --> 02:07:48.340]   not simply that he received it and was going to think about it, a reasonable bystander
[02:07:48.340 --> 02:07:52.980]   knowing all of the backward come to the objective understanding that they had received reached
[02:07:52.980 --> 02:07:59.860]   consensus, a meaning of the minds, the judge ruled in favor of SWT.
[02:07:59.860 --> 02:08:03.660]   Yes, you owe me $22,000 for that flax contract.
[02:08:03.660 --> 02:08:06.180]   You didn't deliver on.
[02:08:06.180 --> 02:08:10.980]   So just be aware when you use the thumbs up, that is legally binding.
[02:08:10.980 --> 02:08:12.220]   Emojis are language everyone.
[02:08:12.220 --> 02:08:13.220]   They are.
[02:08:13.220 --> 02:08:15.220]   Just the same as any other word.
[02:08:15.220 --> 02:08:16.220]   They are.
[02:08:16.220 --> 02:08:17.220]   The judge got this one right.
[02:08:17.220 --> 02:08:18.220]   You think so?
[02:08:18.220 --> 02:08:19.220]   I think so.
[02:08:19.220 --> 02:08:20.220]   Oh, for sure.
[02:08:20.220 --> 02:08:21.220]   Yeah.
[02:08:21.220 --> 02:08:23.220]   I agree.
[02:08:23.220 --> 02:08:24.380]   Okay.
[02:08:24.380 --> 02:08:33.460]   So we all agree that we're the court of appeals here and it is now, I guess, read you agree?
[02:08:33.460 --> 02:08:35.460]   Yeah, totally agree.
[02:08:35.460 --> 02:08:36.460]   Seth, you agree?
[02:08:36.460 --> 02:08:37.460]   All right.
[02:08:37.460 --> 02:08:39.460]   I have a little bit of a problem.
[02:08:39.460 --> 02:08:40.460]   Oh, no.
[02:08:40.460 --> 02:08:42.140]   What is somebody like?
[02:08:42.140 --> 02:08:47.740]   There's a lot of stuff like what if you know, you're an e-breated or what if your kids got
[02:08:47.740 --> 02:08:49.540]   your phone, you know?
[02:08:49.540 --> 02:08:50.540]   Oh, okay.
[02:08:50.540 --> 02:08:53.580]   Well, you didn't even assert that the kids got the phone.
[02:08:53.580 --> 02:08:55.100]   Because you can.
[02:08:55.100 --> 02:08:59.740]   If you can send afterwards like, hey, kid took the phone or something.
[02:08:59.740 --> 02:09:01.100]   That does change a thing.
[02:09:01.100 --> 02:09:04.900]   But it's like under reasonable circumstances.
[02:09:04.900 --> 02:09:06.460]   Yeah, I agree.
[02:09:06.460 --> 02:09:07.460]   I agree.
[02:09:07.460 --> 02:09:14.500]   But like, I don't want this to be a precedent because I've said things that I don't necessarily
[02:09:14.500 --> 02:09:15.780]   want to be.
[02:09:15.780 --> 02:09:20.540]   I mean, I guess if it's an $80 million contract, I'm probably not going to be dealing with
[02:09:20.540 --> 02:09:21.540]   emojis.
[02:09:21.540 --> 02:09:23.420]   So maybe I don't have to.
[02:09:23.420 --> 02:09:24.740]   And frankly, think about that.
[02:09:24.740 --> 02:09:29.380]   I feel for the farmer because he sold it at 17, a bushel and it went to 41.
[02:09:29.380 --> 02:09:32.140]   I might be saying, yeah, it's not a contract.
[02:09:32.140 --> 02:09:33.780]   That's just a thumbs up emoji.
[02:09:33.780 --> 02:09:34.780]   Right.
[02:09:34.780 --> 02:09:35.780]   It's worth a shot.
[02:09:35.780 --> 02:09:36.940]   It was worth a shot.
[02:09:36.940 --> 02:09:37.940]   Yes.
[02:09:37.940 --> 02:09:44.900]   If anyone out there has proved a contract using an emoji, please tell us on threads because
[02:09:44.900 --> 02:09:45.900]   why not?
[02:09:45.900 --> 02:09:49.300]   I mean, the question is, what does this mean for DocuSign?
[02:09:49.300 --> 02:09:52.780]   Oh, you know, that's a good point.
[02:09:52.780 --> 02:09:53.780]   That's a good point.
[02:09:53.780 --> 02:09:58.820]   I mean, actually, I think Elon accepted the Twitter offer with a thumbs up, right?
[02:09:58.820 --> 02:10:01.580]   And that's why it went through.
[02:10:01.580 --> 02:10:02.580]   Is that true?
[02:10:02.580 --> 02:10:04.420]   Or you make you joking?
[02:10:04.420 --> 02:10:06.620]   I could see him doing that.
[02:10:06.620 --> 02:10:10.020]   No, Elon only sends rocket emojis.
[02:10:10.020 --> 02:10:11.020]   And poop emojis.
[02:10:11.020 --> 02:10:16.940]   If he would have moved, he said press department and had it been a thumbs up, he still would
[02:10:16.940 --> 02:10:18.740]   have had to buy Twitter.
[02:10:18.740 --> 02:10:21.740]   We now know.
[02:10:21.740 --> 02:10:27.060]   Let that sink in.
[02:10:27.060 --> 02:10:31.540]   You wrote a or no, you actually put this in the show notes, Ben, the Vegas fear.
[02:10:31.540 --> 02:10:33.780]   Are you all excited about the MSG?
[02:10:33.780 --> 02:10:39.420]   The world's largest free standing sphere structure in Las Vegas.
[02:10:39.420 --> 02:10:40.420]   Yeah.
[02:10:40.420 --> 02:10:46.220]   And for those who haven't seen it, go and just like Google Vegas sphere thing is nuts.
[02:10:46.220 --> 02:10:47.220]   I saw it.
[02:10:47.220 --> 02:10:51.260]   I was in Vegas two weeks ago before they turned it on just before that already that thing
[02:10:51.260 --> 02:10:52.260]   looked impressive.
[02:10:52.260 --> 02:10:56.140]   Oh, I've been driving by it in Vegas for with four years now, right?
[02:10:56.140 --> 02:10:57.780]   They've been building it forever.
[02:10:57.780 --> 02:11:03.580]   Was stalled a little bit in COVID ended up costing more than the Raiders stadium.
[02:11:03.580 --> 02:11:05.940]   Two billion dollars.
[02:11:05.940 --> 02:11:08.700]   James Dolan and the MSG group paying for it.
[02:11:08.700 --> 02:11:14.940]   It's going to be an 18,000 seat concert venue with tens of thousands of speakers.
[02:11:14.940 --> 02:11:16.420]   It's really interesting.
[02:11:16.420 --> 02:11:23.540]   And on the outside, a 50,000 square foot LED screen, which can turn into the moon, the
[02:11:23.540 --> 02:11:25.500]   sun, the planet earth.
[02:11:25.500 --> 02:11:28.740]   There was its hello world message, which was posted last week.
[02:11:28.740 --> 02:11:34.300]   And then on the 4th of July, they had fireworks on it and real fireworks behind it, which
[02:11:34.300 --> 02:11:37.020]   is kind of cool.
[02:11:37.020 --> 02:11:38.700]   I think this will be worth seeing.
[02:11:38.700 --> 02:11:39.900]   I tried to buy tickets.
[02:11:39.900 --> 02:11:42.420]   You too, is the first concert there.
[02:11:42.420 --> 02:11:46.940]   There's been an event there before them, but you too be the first concert in October.
[02:11:46.940 --> 02:11:51.060]   And I maybe fortunately or unfortunately, my credit card wouldn't let me.
[02:11:51.060 --> 02:11:59.060]   So they were a little bit pricey, shall I say, you could sell your left arm and you might
[02:11:59.060 --> 02:12:02.540]   be able to get one ticket expensive.
[02:12:02.540 --> 02:12:07.340]   But it would be a good.
[02:12:07.340 --> 02:12:08.340]   No, you go.
[02:12:08.340 --> 02:12:13.260]   I was going to say the one that got me the picture was the basketball.
[02:12:13.260 --> 02:12:14.260]   Yes.
[02:12:14.260 --> 02:12:18.140]   And it was like you could see the bars and then it looked like a gigantic basketball sort
[02:12:18.140 --> 02:12:20.340]   of hovering over Las Vegas.
[02:12:20.340 --> 02:12:21.820]   Amazing.
[02:12:21.820 --> 02:12:25.340]   You know, there are a lot of high rises around this sphere that will have this as their
[02:12:25.340 --> 02:12:26.340]   view.
[02:12:26.340 --> 02:12:28.740]   I don't know how happy they are about that.
[02:12:28.740 --> 02:12:30.420]   People love seeing it.
[02:12:30.420 --> 02:12:31.420]   I've seen from TikTok.
[02:12:31.420 --> 02:12:35.020]   I don't know if you'd want to see it every day from your apartment, but if you're on
[02:12:35.020 --> 02:12:38.220]   the strip, that's I think you'd want to see.
[02:12:38.220 --> 02:12:44.500]   It just makes me think how Abu Dhabi and Dubai and Saudi Arabia are putting up the crazy
[02:12:44.500 --> 02:12:45.500]   stuff.
[02:12:45.500 --> 02:12:47.940]   And then America was just like, oh, you have a tower?
[02:12:47.940 --> 02:12:48.940]   Hold my beer.
[02:12:48.940 --> 02:12:49.940]   Giant sphere.
[02:12:49.940 --> 02:12:55.380]   So, I mean, look, it's going to drive people.
[02:12:55.380 --> 02:12:57.580]   It's going to drive that tourism thing.
[02:12:57.580 --> 02:13:00.740]   Now you're going to have all these sports teams there.
[02:13:00.740 --> 02:13:03.540]   There's something crazy happening over at Vegas for sure.
[02:13:03.540 --> 02:13:04.540]   Yeah.
[02:13:04.540 --> 02:13:05.540]   Yeah.
[02:13:05.540 --> 02:13:07.540]   How long until it gets hacked?
[02:13:07.540 --> 02:13:14.100]   Oh, here's a, by the way, here is the, here's the basketball, I think, on the horizon.
[02:13:14.100 --> 02:13:16.340]   Yeah, there you are driving down the strip.
[02:13:16.340 --> 02:13:18.180]   The giant is just crazy.
[02:13:18.180 --> 02:13:19.540]   It looks like a giant.
[02:13:19.540 --> 02:13:22.260]   I mean, it really looks real.
[02:13:22.260 --> 02:13:23.660]   It's insane.
[02:13:23.660 --> 02:13:30.100]   The technological work to make that work is just mind-boggling too.
[02:13:30.100 --> 02:13:31.860]   It's, yeah, it's kind of amazing.
[02:13:31.860 --> 02:13:32.860]   It's LEDs, right?
[02:13:32.860 --> 02:13:33.860]   Is that the?
[02:13:33.860 --> 02:13:34.860]   It's LEDs.
[02:13:34.860 --> 02:13:37.940]   100 square, 160,000 square feet inside.
[02:13:37.940 --> 02:13:39.940]   That's about the size of three football fields.
[02:13:39.940 --> 02:13:46.300]   I mean, on the outside, it says three football fields worth of programmable LEDs.
[02:13:46.300 --> 02:13:47.300]   Yeah.
[02:13:47.300 --> 02:13:49.380]   How many iPhone screens is that?
[02:13:49.380 --> 02:13:50.380]   That's, yeah.
[02:13:50.380 --> 02:13:52.060]   Somebody do some math.
[02:13:52.060 --> 02:13:53.060]   Somebody do some math.
[02:13:53.060 --> 02:13:54.060]   Someone let us know, please.
[02:13:54.060 --> 02:13:55.060]   Yeah.
[02:13:55.060 --> 02:13:56.060]   Look at this.
[02:13:56.060 --> 02:13:57.060]   How about this?
[02:13:57.060 --> 02:13:59.900]   A giant eyeball on the Vegas strip.
[02:13:59.900 --> 02:14:03.380]   I mean, it looks very real, doesn't it?
[02:14:03.380 --> 02:14:06.380]   It's terrifying.
[02:14:06.380 --> 02:14:09.020]   The eye of Sauron.
[02:14:09.020 --> 02:14:11.940]   They've been building it for years.
[02:14:11.940 --> 02:14:17.580]   Madison Square Garden, I don't know how they make money on this, but they're planning
[02:14:17.580 --> 02:14:20.700]   to do another one in London.
[02:14:20.700 --> 02:14:23.340]   That's the real London eye.
[02:14:23.340 --> 02:14:29.700]   So we will see now, if you plan to sue MSG any time soon, you might want to think
[02:14:29.700 --> 02:14:34.980]   twice because you wouldn't want the dolens to stop you from getting into the Las Vegas
[02:14:34.980 --> 02:14:35.980]   sphere.
[02:14:35.980 --> 02:14:36.980]   Maybe that's my credit card.
[02:14:36.980 --> 02:14:38.140]   Wouldn't let me buy a ticket.
[02:14:38.140 --> 02:14:39.140]   Maybe a new.
[02:14:39.140 --> 02:14:41.500]   I don't know.
[02:14:41.500 --> 02:14:43.500]   I don't know.
[02:14:43.500 --> 02:14:44.940]   You play Pokemon Go?
[02:14:44.940 --> 02:14:49.940]   Get ready for Pokemon Sleep.
[02:14:49.940 --> 02:14:52.700]   The Niantic companies laid off a good number of employees.
[02:14:52.700 --> 02:14:53.860]   They've been struggling a little bit.
[02:14:53.860 --> 02:14:57.660]   Pokemon Go, though, still a big success came out in 2016.
[02:14:57.660 --> 02:15:03.620]   I mean, it wasn't as big as threads, but it was a pretty big launch.
[02:15:03.620 --> 02:15:07.880]   The whole idea of Pokemon Go was you had to walk around, like for reals.
[02:15:07.880 --> 02:15:17.460]   My wife has accumulated more than 11,000 kilometers searching for Pokemon, walking around.
[02:15:17.460 --> 02:15:19.460]   Their claim is this has been really good for people's health.
[02:15:19.460 --> 02:15:20.620]   I think that's probably true.
[02:15:20.620 --> 02:15:25.820]   I don't know how much, but I've walked quite a bit more than I would normally looking for
[02:15:25.820 --> 02:15:26.820]   Pokemon.
[02:15:26.820 --> 02:15:31.580]   We've launched Pokemon Sleep, or we'll launch this summer.
[02:15:31.580 --> 02:15:34.900]   Here's a video from Niantic.
[02:15:34.900 --> 02:15:35.900]   How do you play?
[02:15:35.900 --> 02:15:39.340]   Well, you go to sleep.
[02:15:39.340 --> 02:15:47.980]   You catch Pokemon by sleeping and having a quality night's sleep.
[02:15:47.980 --> 02:15:49.500]   I guess the same idea.
[02:15:49.500 --> 02:15:53.420]   They want to do good things for their users.
[02:15:53.420 --> 02:15:55.700]   There are some sleepy style Pokemon.
[02:15:55.700 --> 02:15:59.700]   There's a sleeping pok pikachu.
[02:15:59.700 --> 02:16:04.180]   You have a sleep type decks, like a pokie decks.
[02:16:04.180 --> 02:16:09.780]   Once players collect a certain Pokemon, they can feed it pokie biscuits to make it friendlier.
[02:16:09.780 --> 02:16:18.380]   There are three types of sleeping pokie mons, dozing, snoozing, and slumbering.
[02:16:18.380 --> 02:16:20.020]   My wife plays competition, right?
[02:16:20.020 --> 02:16:21.420]   Yeah, of course it is.
[02:16:21.420 --> 02:16:22.900]   So who can better?
[02:16:22.900 --> 02:16:24.500]   Do they give you a handicap?
[02:16:24.500 --> 02:16:28.180]   If you're a teenager, you shouldn't get the same number of Pokemon.
[02:16:28.180 --> 02:16:29.180]   That's a good point.
[02:16:29.180 --> 02:16:30.780]   As a parent or something.
[02:16:30.780 --> 02:16:31.780]   That's right.
[02:16:31.780 --> 02:16:34.020]   It's a simple sleep tracker.
[02:16:34.020 --> 02:16:43.420]   Users can monitor their sleep patterns and even listen to recordings of their sleep.
[02:16:43.420 --> 02:16:46.460]   It's a good way to do a sleep tracker for kids.
[02:16:46.460 --> 02:16:48.460]   Because for a...
[02:16:48.460 --> 02:16:52.820]   Look, the Aura Ring, which I have one, is a fantastic sleep tracker.
[02:16:52.820 --> 02:16:55.900]   And viral among a lot of adults.
[02:16:55.900 --> 02:16:57.900]   I see everybody wearing them now.
[02:16:57.900 --> 02:17:00.180]   They were an early advertiser.
[02:17:00.180 --> 02:17:04.780]   We interviewed them when they first came out and they advertised for many months.
[02:17:04.780 --> 02:17:06.420]   I still have mine.
[02:17:06.420 --> 02:17:08.420]   Do you have the version 3?
[02:17:08.420 --> 02:17:11.020]   Yeah, I have the latest version.
[02:17:11.020 --> 02:17:14.820]   It doesn't seem to be super accurate, is my only.
[02:17:14.820 --> 02:17:17.500]   I have so many sleep trackers.
[02:17:17.500 --> 02:17:19.300]   My mattress tracks me.
[02:17:19.300 --> 02:17:20.300]   My wife's tracks me.
[02:17:20.300 --> 02:17:21.300]   That's one more.
[02:17:21.300 --> 02:17:22.300]   See what sleep taking are.
[02:17:22.300 --> 02:17:23.300]   They're all different.
[02:17:23.300 --> 02:17:24.300]   They all have different results.
[02:17:24.300 --> 02:17:28.580]   I have that Google hub that tracks your sleep, records your snoring.
[02:17:28.580 --> 02:17:29.580]   I don't...
[02:17:29.580 --> 02:17:32.380]   But this is the only one where you can sleep them all.
[02:17:32.380 --> 02:17:34.140]   Gotta sleep them all.
[02:17:34.140 --> 02:17:36.820]   That sounds wrong on a contact.
[02:17:36.820 --> 02:17:38.580]   Gotta sleep them all.
[02:17:38.580 --> 02:17:39.580]   Oh, God.
[02:17:39.580 --> 02:17:40.820]   There's a show title for you.
[02:17:40.820 --> 02:17:41.820]   I think so.
[02:17:41.820 --> 02:17:43.900]   I think you gave us a show title.
[02:17:43.900 --> 02:17:48.780]   We'll wrap things up in just a bit.
[02:17:48.780 --> 02:17:52.620]   In just a moment, stay right here.
[02:17:52.620 --> 02:17:56.500]   Before we finish things up, just to look at some of the things that happened this week
[02:17:56.500 --> 02:17:57.500]   on Twitch.
[02:17:57.500 --> 02:18:01.380]   I finally do my best thinking in the shower.
[02:18:01.380 --> 02:18:02.380]   Oh, me too.
[02:18:02.380 --> 02:18:03.580]   I used to have shower crayons for...
[02:18:03.580 --> 02:18:04.580]   Yeah.
[02:18:04.580 --> 02:18:07.260]   I would write things like I would get leads or ideas and I'd write them all over.
[02:18:07.260 --> 02:18:08.260]   See, that's smart.
[02:18:08.260 --> 02:18:11.420]   I had shower chalk and by the time the shower was done, it was all washed away.
[02:18:11.420 --> 02:18:12.420]   Yeah.
[02:18:12.420 --> 02:18:13.420]   You got a ring around the tub.
[02:18:13.420 --> 02:18:14.420]   Smart.
[02:18:14.420 --> 02:18:15.820]   You put it on crayon.
[02:18:15.820 --> 02:18:18.220]   Previously on Twitch.
[02:18:18.220 --> 02:18:19.220]   News weekly.
[02:18:19.220 --> 02:18:23.100]   Holy moly, we have another social media network to track.
[02:18:23.100 --> 02:18:27.580]   It's called Threads launched by this scrappy upstart you've never heard of.
[02:18:27.580 --> 02:18:28.580]   Neta.
[02:18:28.580 --> 02:18:33.420]   It's not necessarily ready to entirely replace Twitter, but you can see the very basics of
[02:18:33.420 --> 02:18:36.900]   what it has and it has all of those and most importantly, it has people.
[02:18:36.900 --> 02:18:40.100]   That's the thing that I think every other platform that is trying to take over for Twitter
[02:18:40.100 --> 02:18:42.500]   doesn't have this week in space.
[02:18:42.500 --> 02:18:46.920]   Joining us today, Jeffrey Nodkin, who's among many other achievements starred in three
[02:18:46.920 --> 02:18:49.300]   seasons of "Beater Right Man" for Discovery Networks.
[02:18:49.300 --> 02:18:54.460]   The scariest would definitely be in Kansas.
[02:18:54.460 --> 02:18:55.460]   So we're digging, we're digging.
[02:18:55.460 --> 02:19:00.420]   Fortunately, we're both out of the hole when the entire bottom of the hole collapsed into
[02:19:00.420 --> 02:19:01.420]   a void.
[02:19:01.420 --> 02:19:08.340]   When there was this, this like, and I thought, oh, we've accidentally uncovered the entrance
[02:19:08.340 --> 02:19:09.500]   to the underworld.
[02:19:09.500 --> 02:19:10.980]   This week in Google.
[02:19:10.980 --> 02:19:12.780]   Mr. Jeff Jarvis.
[02:19:12.780 --> 02:19:13.780]   Hello, hello, hello.
[02:19:13.780 --> 02:19:17.200]   Who is a author of a brand new book.
[02:19:17.200 --> 02:19:18.200]   Everybody show your book.
[02:19:18.200 --> 02:19:20.200]   Everybody in the audience hold on here.
[02:19:20.200 --> 02:19:22.300]   Gutenberg, parentheses.
[02:19:22.300 --> 02:19:24.920]   And I also note that you have dedicated it to Craig Newmark.
[02:19:24.920 --> 02:19:28.680]   The Craig Newmark Graduate School of Journalism at the City of First Eve, New York.
[02:19:28.680 --> 02:19:32.520]   And even the book sings when you open it.
[02:19:32.520 --> 02:19:33.520]   It's amazing.
[02:19:33.520 --> 02:19:39.280]   See, I didn't get that in the Kindle.
[02:19:39.280 --> 02:19:40.280]   We had a good week.
[02:19:40.280 --> 02:19:43.420]   Like this week in space is a good reason to join Club Twit.
[02:19:43.420 --> 02:19:48.360]   We launched this week in space, if you will forgive the phrase, in our club, because the
[02:19:48.360 --> 02:19:50.480]   club members supported it.
[02:19:50.480 --> 02:19:52.000]   And it garnered an audience.
[02:19:52.000 --> 02:19:54.120]   We were able to put it out in the public.
[02:19:54.120 --> 02:19:58.880]   That's kind of our model going forward as ad sales kind of dwindle.
[02:19:58.880 --> 02:20:04.120]   We really rely on you, our audience, to support our growth.
[02:20:04.120 --> 02:20:06.900]   And we need it because this month we just broke even.
[02:20:06.900 --> 02:20:08.500]   We didn't make any money.
[02:20:08.500 --> 02:20:10.860]   That's a tough position to be in for any company.
[02:20:10.860 --> 02:20:13.720]   We could keep this up, but I don't want to lose money.
[02:20:13.720 --> 02:20:15.760]   The Club Twit makes all the difference.
[02:20:15.760 --> 02:20:18.120]   And it's just $7 a month.
[02:20:18.120 --> 02:20:19.120]   Hardly anything.
[02:20:19.120 --> 02:20:24.920]   You get access to all the shows we do, including this one, ad-free, tracker-free.
[02:20:24.920 --> 02:20:27.000]   So you completely private.
[02:20:27.000 --> 02:20:31.500]   You also get access to the wonderful Club Twit Discord, which is a place to hang with
[02:20:31.500 --> 02:20:34.360]   other Club Twit members.
[02:20:34.360 --> 02:20:37.000]   It's a great place to be.
[02:20:37.000 --> 02:20:42.620]   Lots of animated gifts, conversations about all the topics, geeks-like, and special events
[02:20:42.620 --> 02:20:46.060]   that our community manager, Ant Prout, puts together.
[02:20:46.060 --> 02:20:47.420]   We just had Stacy's book club.
[02:20:47.420 --> 02:20:52.180]   He just interviewed Hugh Howie, the author of "Wooled, the Inspiration for the Apple TV
[02:20:52.180 --> 02:20:54.420]   Plus Silo series."
[02:20:54.420 --> 02:21:01.500]   We've got other events coming up in the works.
[02:21:01.500 --> 02:21:03.180]   The animated gifts are part of the fun.
[02:21:03.180 --> 02:21:07.800]   You also get shows that we don't put out in public, shows we're launching or trying,
[02:21:07.800 --> 02:21:11.120]   like Hands on Mac, Tash with Michael Sargent, Hands on Windows.
[02:21:11.120 --> 02:21:15.720]   And coming soon, our new AI show with Jeff Jarvis, Jason Howell.
[02:21:15.720 --> 02:21:17.480]   We're in development right now.
[02:21:17.480 --> 02:21:19.960]   That's because the Club members support it.
[02:21:19.960 --> 02:21:22.360]   That $7 a month makes a huge difference to us.
[02:21:22.360 --> 02:21:25.960]   If you're not yet a member of Club Twit, I implore you.
[02:21:25.960 --> 02:21:27.320]   If you can't afford it, don't worry.
[02:21:27.320 --> 02:21:30.480]   There's plenty of free stuff ad-supported still.
[02:21:30.480 --> 02:21:35.040]   But the future, I think, is going to be with the club.
[02:21:35.040 --> 02:21:38.880]   So get on in there and go to twit.tv/clubtwit.
[02:21:38.880 --> 02:21:44.220]   We have monthly plans, annual plans, family plans, and corporate memberships as well.
[02:21:44.220 --> 02:21:48.620]   You can even give a gift of Club Twit, which is an awfully nice gift for the geek in your
[02:21:48.620 --> 02:21:49.620]   life.
[02:21:49.620 --> 02:21:50.620]   Twit.tv/clubtwit.
[02:21:50.620 --> 02:21:56.980]   We thank you so much for your support to all the Club Twit members.
[02:21:56.980 --> 02:22:00.300]   You're keeping us on the air, and we really appreciate it.
[02:22:00.300 --> 02:22:01.500]   Seth, it's so great to have you.
[02:22:01.500 --> 02:22:06.420]   Seth Wine-Trub is the founder, publisher, editorial director of the 9-5 sites and of
[02:22:06.420 --> 02:22:10.620]   course, Electric, which I think is the best.
[02:22:10.620 --> 02:22:11.620]   It's my go-to.
[02:22:11.620 --> 02:22:12.620]   I'm a big EV fan.
[02:22:12.620 --> 02:22:14.220]   I just love reading about it.
[02:22:14.220 --> 02:22:19.580]   Congratulations on the sponsorship of the Formula Sun.
[02:22:19.580 --> 02:22:21.740]   I think that's a great thing that you did.
[02:22:21.740 --> 02:22:23.980]   I look forward to next year, too.
[02:22:23.980 --> 02:22:24.980]   Grand Prix.
[02:22:24.980 --> 02:22:30.860]   You can read all about it in electric.com.
[02:22:30.860 --> 02:22:32.220]   Anything else you want to plug, Seth?
[02:22:32.220 --> 02:22:33.220]   This is your chance.
[02:22:33.220 --> 02:22:35.380]   Well, electric.co is somebody else got the .co.
[02:22:35.380 --> 02:22:36.380]   Oh, damn, now.
[02:22:36.380 --> 02:22:37.380]   Yeah.
[02:22:37.380 --> 02:22:38.380]   Co, electric.co.
[02:22:38.380 --> 02:22:39.380]   Yes.
[02:22:39.380 --> 02:22:40.380]   That's okay.
[02:22:40.380 --> 02:22:41.380]   It reminds me of the electric company, right?
[02:22:41.380 --> 02:22:42.380]   Exactly.
[02:22:42.380 --> 02:22:43.380]   Yeah.
[02:22:43.380 --> 02:22:45.380]   Anything else you want to plug?
[02:22:45.380 --> 02:22:47.900]   We just, the whole network is...
[02:22:47.900 --> 02:22:48.900]   It's great stuff.
[02:22:48.900 --> 02:22:49.900]   I have good stuff.
[02:22:49.900 --> 02:22:50.900]   Yeah, you've done a great job.
[02:22:50.900 --> 02:22:51.900]   I'm very grateful.
[02:22:51.900 --> 02:22:52.900]   Thank you.
[02:22:52.900 --> 02:22:59.460]   As somebody in the contact business myself, I kind of know how hard it is now.
[02:22:59.460 --> 02:23:02.980]   And it really takes dedication and inspiration.
[02:23:02.980 --> 02:23:05.860]   I thank you for what you've done all these years.
[02:23:05.860 --> 02:23:07.260]   Thanks, too, to reap albagani.
[02:23:07.260 --> 02:23:08.420]   It's great to see you again.
[02:23:08.420 --> 02:23:10.500]   And Semaphore is going so well.
[02:23:10.500 --> 02:23:17.900]   Subscribe to Reed's free technology newsletter, SEMAFOR.com.
[02:23:17.900 --> 02:23:20.140]   Did I mention it's free?
[02:23:20.140 --> 02:23:23.500]   Semaphore is going to be good.
[02:23:23.500 --> 02:23:24.500]   For now, when is it?
[02:23:24.500 --> 02:23:25.500]   Get on the ground floor.
[02:23:25.500 --> 02:23:26.500]   Is that the plan?
[02:23:26.500 --> 02:23:29.580]   Is it at some point, start a charge for these things?
[02:23:29.580 --> 02:23:34.500]   No, I mean, I can't give away our long-term plans, but I was just trying to create urgency.
[02:23:34.500 --> 02:23:35.500]   Okay, that's it.
[02:23:35.500 --> 02:23:37.260]   That's the way to do it.
[02:23:37.260 --> 02:23:38.260]   It's about...
[02:23:38.260 --> 02:23:41.660]   Semaphore is great because it's global, which is really important.
[02:23:41.660 --> 02:23:46.780]   I don't think we get enough international coverage in our national news media.
[02:23:46.780 --> 02:23:53.020]   News, politics, business, technology, net zero Africa security media.
[02:23:53.020 --> 02:23:55.100]   I really like what the bends are doing.
[02:23:55.100 --> 02:23:58.260]   And I'm so glad they snagged you because I think you're fantastic.
[02:23:58.260 --> 02:23:59.820]   Thank you for being here, Reed.
[02:23:59.820 --> 02:24:00.820]   Me, too.
[02:24:00.820 --> 02:24:01.820]   Appreciate it.
[02:24:01.820 --> 02:24:02.820]   Thanks for having me.
[02:24:02.820 --> 02:24:03.820]   Yeah, we love having you on.
[02:24:03.820 --> 02:24:05.300]   And Ben, old friend, great to see you.
[02:24:05.300 --> 02:24:06.500]   Congratulations on your engagement.
[02:24:06.500 --> 02:24:08.500]   When's the wedding?
[02:24:08.500 --> 02:24:11.300]   The wedding is April next year.
[02:24:11.300 --> 02:24:12.300]   Okay.
[02:24:12.300 --> 02:24:13.380]   Okay.
[02:24:13.380 --> 02:24:17.500]   Do and Deborah have to come by sometime, say hello.
[02:24:17.500 --> 02:24:20.500]   We might be up in your neck of the woods in a month or so.
[02:24:20.500 --> 02:24:21.500]   Good.
[02:24:21.500 --> 02:24:23.180]   Well, you know where to call.
[02:24:23.180 --> 02:24:24.340]   You know who to call.
[02:24:24.340 --> 02:24:25.740]   Go to BenParr.com.
[02:24:25.740 --> 02:24:26.740]   Ghostbusters.
[02:24:26.740 --> 02:24:27.740]   Ghostbusters.
[02:24:27.740 --> 02:24:34.860]   To read his fabulous AI analyst newsletter.
[02:24:34.860 --> 02:24:35.860]   Do you charge for this?
[02:24:35.860 --> 02:24:36.860]   Yeah, you subscribe.
[02:24:36.860 --> 02:24:37.860]   No.
[02:24:37.860 --> 02:24:38.860]   Also free.
[02:24:38.860 --> 02:24:39.860]   Thank you.
[02:24:39.860 --> 02:24:40.860]   Yes, free as well.
[02:24:40.860 --> 02:24:41.860]   Come.
[02:24:41.860 --> 02:24:45.940]   And follow me on threads afterwards because that's apparently the thing this week.
[02:24:45.940 --> 02:24:50.940]   I really appreciate, I'll be honest, I really appreciate free newsletters because more and
[02:24:50.940 --> 02:24:54.820]   more as I go around, you know, what part of my job is to go all over the net and look
[02:24:54.820 --> 02:24:57.620]   for news stories for our shows.
[02:24:57.620 --> 02:25:02.700]   And increasingly I am, I am, you know, pay walled out of a lot of great stuff.
[02:25:02.700 --> 02:25:04.140]   And yeah, you're good.
[02:25:04.140 --> 02:25:07.580]   You deserve my money, but I can't, what am I going to, I eventually am going to have
[02:25:07.580 --> 02:25:12.140]   to put a, like make a budget like a hundred bucks a month, subscribe to 20 newsletters
[02:25:12.140 --> 02:25:13.380]   or something.
[02:25:13.380 --> 02:25:16.580]   Because, you know, everybody's charging now.
[02:25:16.580 --> 02:25:18.980]   I'm not sure that's a great thing.
[02:25:18.980 --> 02:25:20.060]   I like what we do.
[02:25:20.060 --> 02:25:23.100]   Have a patronage site.
[02:25:23.100 --> 02:25:25.340]   Have a club, but then have free stuff too.
[02:25:25.340 --> 02:25:27.380]   Anyway, I'm glad you're doing it for free.
[02:25:27.380 --> 02:25:32.780]   You know, the AI analyst at BenParr.com.
[02:25:32.780 --> 02:25:36.900]   Anything else you want to promote?
[02:25:36.900 --> 02:25:38.420]   I said threads already.
[02:25:38.420 --> 02:25:39.740]   That was a threat again.
[02:25:39.740 --> 02:25:43.420]   Threads, threads, threads, threads, threads, threads, threads, threads.
[02:25:43.420 --> 02:25:44.420]   Let's go.
[02:25:44.420 --> 02:25:47.220]   And I need to like be able to like get some shiny threads.
[02:25:47.220 --> 02:25:49.620]   I'm trying to get to 10,000 followers.
[02:25:49.620 --> 02:25:51.700]   You know, I got to do the whole game.
[02:25:51.700 --> 02:25:55.780]   It was funny when I first joined, I was just posting nonstop and my fiance.
[02:25:55.780 --> 02:25:57.660]   You were a friend that I was Ben.
[02:25:57.660 --> 02:25:58.660]   You were.
[02:25:58.660 --> 02:25:59.660]   Paring was what she described it as.
[02:25:59.660 --> 02:26:00.660]   It was too much Ben.
[02:26:00.660 --> 02:26:01.660]   It was too much.
[02:26:01.660 --> 02:26:04.340]   No, now it's just now it's just right.
[02:26:04.340 --> 02:26:07.580]   Yeah, can you get my still on the.
[02:26:07.580 --> 02:26:09.580]   Yeah, you should be able to see my.
[02:26:09.580 --> 02:26:10.580]   Can you see?
[02:26:10.580 --> 02:26:11.580]   Can you see there?
[02:26:11.580 --> 02:26:13.620]   Nope, nope, nope, my phone.
[02:26:13.620 --> 02:26:14.620]   Can you show it?
[02:26:14.620 --> 02:26:15.620]   There it is.
[02:26:15.620 --> 02:26:16.620]   There it is.
[02:26:16.620 --> 02:26:17.620]   There's Benny.
[02:26:17.620 --> 02:26:20.100]   Little Benji, little Benji par.
[02:26:20.100 --> 02:26:23.940]   I post threads about AI sternups, tech media and hot sauce.
[02:26:23.940 --> 02:26:26.820]   I miss that.
[02:26:26.820 --> 02:26:27.820]   That's great.
[02:26:27.820 --> 02:26:34.300]   I have to say because everybody I know is on threads now, it's become.
[02:26:34.300 --> 02:26:38.420]   It's become my, what like Twitter used to be for me.
[02:26:38.420 --> 02:26:43.940]   All my old Twitter buddies are on there, but I only have like a thousand followers.
[02:26:43.940 --> 02:26:46.300]   I really got to work on them.
[02:26:46.300 --> 02:26:47.460]   I'm Twitleo.
[02:26:47.460 --> 02:26:50.260]   I'm Twitleo on the, let me show you mine.
[02:26:50.260 --> 02:26:51.260]   That's me.
[02:26:51.260 --> 02:26:54.100]   Podcast you, bread, cast you, tick, pundit.
[02:26:54.100 --> 02:26:56.140]   Oh, I'm up to 1650.
[02:26:56.140 --> 02:26:58.620]   Look at that.
[02:26:58.620 --> 02:26:59.900]   And there is Ben par.
[02:26:59.900 --> 02:27:01.940]   You mentioned, oh no, I reposted you.
[02:27:01.940 --> 02:27:02.940]   That's it.
[02:27:02.940 --> 02:27:05.900]   Let me know if there's a topic we should cover.
[02:27:05.900 --> 02:27:08.700]   Here's some pictures from the Computer History Museum.
[02:27:08.700 --> 02:27:10.500]   This is the book I wish I'd bought.
[02:27:10.500 --> 02:27:13.380]   How to make a fortune on the information superhighway.
[02:27:13.380 --> 02:27:14.380]   Dang, Nabbit.
[02:27:14.380 --> 02:27:16.460]   I missed that one.
[02:27:16.460 --> 02:27:19.260]   It's in the museum now.
[02:27:19.260 --> 02:27:20.940]   We thank all of you for joining us.
[02:27:20.940 --> 02:27:25.700]   We do this week in tech every Sunday afternoon, right after I asked the tech guys with Mike
[02:27:25.700 --> 02:27:28.420]   a sergeant and me, 2 p.m.
[02:27:28.420 --> 02:27:31.700]   Pacific 5 p.m. Eastern 2100 UTC.
[02:27:31.700 --> 02:27:33.740]   I mentioned that because you can watch us do it live.
[02:27:33.740 --> 02:27:40.300]   All of our shows, not all, but most of our shows, you know, we, we stream the production
[02:27:40.300 --> 02:27:42.820]   so you can watch it like it's a live TV show.
[02:27:42.820 --> 02:27:46.220]   We like it because we like the interactivity and the chat rooms and so forth.
[02:27:46.220 --> 02:27:48.980]   Live.twit.tv for live audio or video.
[02:27:48.980 --> 02:27:53.260]   If you are watching live, IRC.twit.tv is open to all.
[02:27:53.260 --> 02:27:54.620]   That's our internet relay chat.
[02:27:54.620 --> 02:27:56.340]   And you can do that with a browser.
[02:27:56.340 --> 02:27:58.860]   Just put your, point your browser to IRC.twit.tv.
[02:27:58.860 --> 02:28:02.940]   Of course, club twit members can be in the discord at the same time.
[02:28:02.940 --> 02:28:07.780]   After the fact, on-demand versions of this show and all the shows we do are available
[02:28:07.780 --> 02:28:10.180]   at twit.tv.
[02:28:10.180 --> 02:28:13.460]   There's a YouTube channel dedicated to it as well.
[02:28:13.460 --> 02:28:17.980]   And of course, the best way to get it is to subscribe and your favorite podcast application.
[02:28:17.980 --> 02:28:19.500]   Just search for Twit.
[02:28:19.500 --> 02:28:23.420]   You should find this, this and all the other shows we do is subscribe to them and all.
[02:28:23.420 --> 02:28:26.500]   That way you'll have it on your phone and you'll have something to listen to next time you're
[02:28:26.500 --> 02:28:28.820]   stuck on a long commute.
[02:28:28.820 --> 02:28:30.580]   Thanks for listening everybody.
[02:28:30.580 --> 02:28:35.860]   Thanks to our great guests Ben Parr, Reed Albergotti, Seth.
[02:28:35.860 --> 02:28:38.700]   I almost called you Seth MacFarlane.
[02:28:38.700 --> 02:28:39.860]   It's great to...
[02:28:39.860 --> 02:28:41.380]   Seth, wine drum.
[02:28:41.380 --> 02:28:43.020]   Great the only other Seth I know.
[02:28:43.020 --> 02:28:44.020]   Great to have you.
[02:28:44.020 --> 02:28:45.700]   Thanks for joining us everybody.
[02:28:45.700 --> 02:28:46.700]   We'll see you next time.
[02:28:46.700 --> 02:28:48.700]   Another Twit is in the can.
[02:28:48.700 --> 02:28:49.700]   Bye bye.
[02:28:49.700 --> 02:29:04.860]   If you're big plans this year include your big day, plan your look with Indochino.
[02:29:04.860 --> 02:29:09.580]   Customize every detail of a blazer, suit or tuxedo online or at a showroom with an expert
[02:29:09.580 --> 02:29:11.060]   style guide.
[02:29:11.060 --> 02:29:13.780]   Then sit back for delivery straight to your door.
[02:29:13.780 --> 02:29:18.260]   Suit star at just 449 and premium fitted shirts at just $89.
[02:29:18.260 --> 02:29:25.060]   Go to Indochino.com and use code NEWCHAPTER for 10% off any purchase of $3.99 or more.
[02:29:25.060 --> 02:29:29.100]   That's INDOCHINO.com code NEWCHAPTER.

