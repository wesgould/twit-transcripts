;FFMETADATA1
title=Scraped On the Back End
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=662
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:01.880]   It's time for Twit.
[00:00:01.880 --> 00:00:05.720]   This week in Tech, Amy Webb is here, our futurist, Lindsey Turrentine, editor in Chief
[00:00:05.720 --> 00:00:09.560]   at CNET, Jason Heiner, editor in Chief at Tech Republic.
[00:00:09.560 --> 00:00:14.560]   We are going to talk about a lot of very deep philosophical things.
[00:00:14.560 --> 00:00:21.640]   China versus the US, artificial intelligence, Mark Zuckerberg versus Congress, and Waymo
[00:00:21.640 --> 00:00:23.160]   versus the citizens of Mountain View.
[00:00:23.160 --> 00:00:28.520]   It's all coming up next on Twit.
[00:00:28.520 --> 00:00:30.520]   That casts you love.
[00:00:30.520 --> 00:00:32.520]   From people you trust.
[00:00:32.520 --> 00:00:38.160]   This is Twit.
[00:00:38.160 --> 00:00:50.040]   Bandwidth for this week in Tech is provided by CashFly at C-A-C-H-E-F-L-Y dot com.
[00:00:50.040 --> 00:00:52.080]   This is Twit this week in Tech.
[00:00:52.080 --> 00:01:00.080]   Episode 662, recorded Sunday, April 15, 2018, scraped on the back end.
[00:01:00.080 --> 00:01:02.440]   This week in Tech is brought to you by Aptiv.
[00:01:02.440 --> 00:01:07.560]   Aptiv produces audio-based workouts created by certified personal trainers available through
[00:01:07.560 --> 00:01:08.560]   a mobile app.
[00:01:08.560 --> 00:01:15.560]   Get 30% off new annual memberships when you visit Aptiv, A-A-P-T-I-V dot com slash Twit.
[00:01:15.560 --> 00:01:16.560]   And by Molecule.
[00:01:16.560 --> 00:01:21.560]   Molecule is the world's first molecular error purifier that reduces symptoms for allergy
[00:01:21.560 --> 00:01:23.360]   and asthma sufferers.
[00:01:23.360 --> 00:01:30.240]   $75 off your first order when you visit Molecule.com, M-O-L-E-K-U-L-E dot com and another promo
[00:01:30.240 --> 00:01:31.760]   code Twit.
[00:01:31.760 --> 00:01:34.640]   And by Rocket Mortgage by Quick and Loans.
[00:01:34.640 --> 00:01:35.960]   Home plays a big role in your life.
[00:01:35.960 --> 00:01:38.960]   That's why Quick and Loans created Rocket Mortgage.
[00:01:38.960 --> 00:01:43.160]   It lets you apply simply and understand the entire mortgage process fully so you can be
[00:01:43.160 --> 00:01:45.360]   confident you're getting the right mortgage for you.
[00:01:45.360 --> 00:01:49.840]   Get started at Rocket Mortgage dot com slash Twit 2.
[00:01:49.840 --> 00:01:51.200]   And by Audible.
[00:01:51.200 --> 00:01:57.200]   For a free audiobook with a 30 day free trial go to audible.com slash Twit or text Twit
[00:01:57.200 --> 00:01:58.600]   to 500-500.
[00:01:58.600 --> 00:02:03.520]   It's time for Twit this week in Tech the show.
[00:02:03.520 --> 00:02:07.440]   We get together with the best Tech journalists in the business and talk about the week's
[00:02:07.440 --> 00:02:08.640]   Tech news.
[00:02:08.640 --> 00:02:18.200]   And this is a special Twit because April 17, 2005, the very first Twit was recorded with
[00:02:18.200 --> 00:02:22.840]   Patrick Norton, Kevin Rose, Robert Herron, David Prager.
[00:02:22.840 --> 00:02:26.680]   And we are so this is as close as you get to our 13th anniversary episode.
[00:02:26.680 --> 00:02:27.680]   And you know what?
[00:02:27.680 --> 00:02:31.960]   We put together a panel of people I love and cupcakes.
[00:02:31.960 --> 00:02:34.080]   So we won't eat the cupcakes yet.
[00:02:34.080 --> 00:02:35.080]   They are here.
[00:02:35.080 --> 00:02:37.880]   But as soon as I introduce you, you may dive in.
[00:02:37.880 --> 00:02:42.800]   We'll start all the way over to my right with the author of Follow the Geeks, Tech Republic's
[00:02:42.800 --> 00:02:48.800]   editor in chief, my good buddy, the guy who got me elected president of the internet.
[00:02:48.800 --> 00:02:49.800]   Jason Heiner.
[00:02:49.800 --> 00:02:50.880]   Thank you, Jason, for being here.
[00:02:50.880 --> 00:02:51.880]   A pleasure.
[00:02:51.880 --> 00:02:52.880]   Very glad to be here.
[00:02:52.880 --> 00:02:54.680]   We'll bring you out to this coast.
[00:02:54.680 --> 00:02:59.080]   Got some meetings at work with many of the awesome people at CBS Interactive.
[00:02:59.080 --> 00:03:02.720]   People like maybe Lindsey Turned Time Editor in Chief at CNET.com perhaps.
[00:03:02.720 --> 00:03:05.440]   I hope I get to be in a meeting with you.
[00:03:05.440 --> 00:03:06.440]   Is this an annual?
[00:03:06.440 --> 00:03:08.440]   Yeah, let's make sure we do it.
[00:03:08.440 --> 00:03:09.440]   Oh, aren't they cute?
[00:03:09.440 --> 00:03:10.440]   All right.
[00:03:10.440 --> 00:03:12.680]   Can break it up you two.
[00:03:12.680 --> 00:03:14.480]   Is this an annual thing for CBS Interactive?
[00:03:14.480 --> 00:03:16.280]   I'm here for RSA.
[00:03:16.280 --> 00:03:19.560]   And then like a couple days at RSA and a couple days in the office.
[00:03:19.560 --> 00:03:20.720]   RSA is a big security conference.
[00:03:20.720 --> 00:03:21.720]   It is.
[00:03:21.720 --> 00:03:22.720]   Sorry.
[00:03:22.720 --> 00:03:23.720]   Yeah.
[00:03:23.720 --> 00:03:25.840]   Lindsey's also here and Lindsey brought her daughter.
[00:03:25.840 --> 00:03:28.520]   It's so nice to see how grown up your daughter is.
[00:03:28.520 --> 00:03:29.520]   Oh, she disappeared.
[00:03:29.520 --> 00:03:30.520]   She's back there.
[00:03:30.520 --> 00:03:33.520]   She's got her iPad.
[00:03:33.520 --> 00:03:35.960]   She's here to see the, I called it Nerd Hollywood.
[00:03:35.960 --> 00:03:38.040]   Nerd Hollywood.
[00:03:38.040 --> 00:03:40.240]   Not nearly as fun as real Hollywood.
[00:03:40.240 --> 00:03:42.480]   I'm going to assure you.
[00:03:42.480 --> 00:03:46.520]   So another book author and a good friend, Amy Webb, is here.
[00:03:46.520 --> 00:03:49.760]   She is the founder of the Future Today Institute, the author of a book.
[00:03:49.760 --> 00:03:52.720]   This is how I met Amy talking about her book The Signals are talking.
[00:03:52.720 --> 00:03:54.600]   Why today's fringe is tomorrow's mainstream.
[00:03:54.600 --> 00:03:55.760]   Hello, Amy Webb.
[00:03:55.760 --> 00:03:56.760]   Hello.
[00:03:56.760 --> 00:03:58.240]   Congrats on the anniversary.
[00:03:58.240 --> 00:03:59.240]   That's so cool.
[00:03:59.240 --> 00:04:00.240]   13.
[00:04:00.240 --> 00:04:01.240]   Can you believe it?
[00:04:01.240 --> 00:04:06.320]   I mean, this podcasting began really began in earnest in 2004, late 2004.
[00:04:06.320 --> 00:04:07.320]   We started in the spring.
[00:04:07.320 --> 00:04:09.120]   You were really at the beginning of all of this.
[00:04:09.120 --> 00:04:10.320]   Yeah, it's amazing.
[00:04:10.320 --> 00:04:14.440]   If you told me then that in 13 years you'll still be doing this show, I would have just,
[00:04:14.440 --> 00:04:17.760]   I would have killed myself.
[00:04:17.760 --> 00:04:20.360]   I would have said, oh, I hope we have cupcakes.
[00:04:20.360 --> 00:04:22.280]   Please, we got red velvet.
[00:04:22.280 --> 00:04:23.280]   I'm sorry, Amy.
[00:04:23.280 --> 00:04:26.960]   We don't have any way of sending them at keys, but we have get red velvet.
[00:04:26.960 --> 00:04:27.960]   I'm going to try one.
[00:04:27.960 --> 00:04:28.960]   I'm going to catch one to any man.
[00:04:28.960 --> 00:04:32.560]   I'm going to get the H. I remember being here for the 10th anniversary.
[00:04:32.560 --> 00:04:33.880]   Well, I was writing my book.
[00:04:33.880 --> 00:04:34.880]   I came here for the 10th anniversary.
[00:04:34.880 --> 00:04:35.880]   That's right.
[00:04:35.880 --> 00:04:36.880]   And what do we have?
[00:04:36.880 --> 00:04:37.880]   Cupcakes.
[00:04:37.880 --> 00:04:38.880]   You did have a couple of minutes.
[00:04:38.880 --> 00:04:39.880]   That's right.
[00:04:39.880 --> 00:04:40.880]   Cupcakes seem like.
[00:04:40.880 --> 00:04:41.880]   Can't just be crunchy.
[00:04:41.880 --> 00:04:49.320]   It didn't seem like so amazing that this thing was still going strong after 10 years.
[00:04:49.320 --> 00:04:54.960]   Now, to be fair, most of the podcasts that were going strong back then are long since
[00:04:54.960 --> 00:05:00.000]   history, but this one seems like it just keeps getting stronger.
[00:05:00.000 --> 00:05:04.880]   We've survived long enough to live through podcast renaissance, which is going out right
[00:05:04.880 --> 00:05:05.880]   now.
[00:05:05.880 --> 00:05:06.880]   Yeah.
[00:05:06.880 --> 00:05:07.880]   Yeah.
[00:05:07.880 --> 00:05:08.880]   Yeah.
[00:05:08.880 --> 00:05:10.880]   It's kind of funny.
[00:05:10.880 --> 00:05:18.160]   Well, in a way, I feel like we're now such old hat that, you know, all the new podcasts
[00:05:18.160 --> 00:05:23.200]   get all the attention serial and, you know, all the WNYC stuff and gimlet media and all
[00:05:23.200 --> 00:05:24.200]   that stuff.
[00:05:24.200 --> 00:05:25.760]   But they don't have cupcakes.
[00:05:25.760 --> 00:05:28.160]   They will in 13 years.
[00:05:28.160 --> 00:05:29.160]   Please be my guest.
[00:05:29.160 --> 00:05:30.160]   Red velvet.
[00:05:30.160 --> 00:05:31.160]   Yeah.
[00:05:31.160 --> 00:05:32.160]   All right.
[00:05:32.160 --> 00:05:33.160]   I'm taking the exclamation point.
[00:05:33.160 --> 00:05:35.680]   And your daughter, by the way, can have one.
[00:05:35.680 --> 00:05:37.200]   You want to come have a cupcake?
[00:05:37.200 --> 00:05:38.200]   No.
[00:05:38.200 --> 00:05:39.200]   Yeah.
[00:05:39.200 --> 00:05:40.200]   Yeah.
[00:05:40.200 --> 00:05:41.920]   Burke, Burke's going to have one.
[00:05:41.920 --> 00:05:42.920]   That's for sure.
[00:05:42.920 --> 00:05:43.920]   Excellent.
[00:05:43.920 --> 00:05:45.680]   Can I reach you a cupcake?
[00:05:45.680 --> 00:05:47.240]   I'm vegan.
[00:05:47.240 --> 00:05:48.240]   So I doubt that.
[00:05:48.240 --> 00:05:49.240]   Oh, these aren't vegan.
[00:05:49.240 --> 00:05:50.560]   Are they vegan cupcakes?
[00:05:50.560 --> 00:05:52.120]   The red velvet has coffee in them.
[00:05:52.120 --> 00:05:53.120]   That's almost vegan.
[00:05:53.120 --> 00:05:54.120]   All right.
[00:05:54.120 --> 00:05:55.880]   I didn't realize you were a vegan.
[00:05:55.880 --> 00:05:56.880]   Yeah.
[00:05:56.880 --> 00:05:59.880]   You know, with funny thing on YouTube, I think it was Zanep Tuficke.
[00:05:59.880 --> 00:06:03.720]   You'd notice this in an editorial on the New York Times a couple of weeks ago that
[00:06:03.720 --> 00:06:05.560]   there's just a natural thing in the algorithm.
[00:06:05.560 --> 00:06:10.360]   And really, one of the things we talk a lot about these days is algorithms and how algorithms
[00:06:10.360 --> 00:06:13.880]   are driving technology in kind of a weird way.
[00:06:13.880 --> 00:06:18.280]   But one of the things about YouTube's algorithms is no matter what you look at, it pushes you
[00:06:18.280 --> 00:06:19.440]   something more extreme.
[00:06:19.440 --> 00:06:23.360]   So if you go to look, she mentioned this, a vegetarian video, it'll suggest a vegan
[00:06:23.360 --> 00:06:24.360]   video.
[00:06:24.360 --> 00:06:28.000]   If it's vegan, it'll suggest a raw vegan.
[00:06:28.000 --> 00:06:29.160]   Raw vegans, right.
[00:06:29.160 --> 00:06:30.160]   And then, you know, nuts.
[00:06:30.160 --> 00:06:31.160]   You want to move on nuts?
[00:06:31.160 --> 00:06:32.160]   Yeah.
[00:06:32.160 --> 00:06:33.160]   Water and carrots.
[00:06:33.160 --> 00:06:34.160]   Water and air.
[00:06:34.160 --> 00:06:35.160]   Okay.
[00:06:35.160 --> 00:06:39.360]   But there is something that it's not that the engineers are doing that.
[00:06:39.360 --> 00:06:41.400]   It's not that any humans are thinking about that.
[00:06:41.400 --> 00:06:44.280]   It's just that this is what algorithms do.
[00:06:44.280 --> 00:06:47.080]   Their algorithms are tuned to improve engagement.
[00:06:47.080 --> 00:06:48.400]   And over time, they get better.
[00:06:48.400 --> 00:06:50.480]   Thanks to machine learning and big data.
[00:06:50.480 --> 00:06:53.160]   And what they learn is this is how you engage people.
[00:06:53.160 --> 00:06:57.960]   You get more and more extreme, but unfortunately, it's not just in dietary things.
[00:06:57.960 --> 00:07:01.640]   It's also in, you know, political and on and on.
[00:07:01.640 --> 00:07:07.000]   Ernie, is that something that you and your research kind of noticed in the past?
[00:07:07.000 --> 00:07:09.800]   Or is this something new and unexpected?
[00:07:09.800 --> 00:07:16.080]   Well, I mean, it's new in the digital realm, but it's certainly not unexpected.
[00:07:16.080 --> 00:07:20.280]   So from the digital realm, attention is currency, right?
[00:07:20.280 --> 00:07:25.400]   The way that you would liken this is to what's going to compel somebody to keep, I guess,
[00:07:25.400 --> 00:07:26.400]   spending.
[00:07:26.400 --> 00:07:27.400]   And in this case, it'd be their time.
[00:07:27.400 --> 00:07:31.960]   At some point, you reach a certain threshold and you need more, right?
[00:07:31.960 --> 00:07:37.320]   Which is why we see gradients of everything from porn to...
[00:07:37.320 --> 00:07:39.440]   Yeah, porn gets more extreme, doesn't it?
[00:07:39.440 --> 00:07:40.440]   Yeah.
[00:07:40.440 --> 00:07:41.440]   Right.
[00:07:41.440 --> 00:07:43.400]   So I mean, it's just like we cognitively like we need...
[00:07:43.400 --> 00:07:44.400]   Stinger's.
[00:07:44.400 --> 00:07:46.200]   Yeah, all the time.
[00:07:46.200 --> 00:07:52.600]   Yeah, actually, the prime example of this is television, which started fairly innocently
[00:07:52.600 --> 00:07:59.120]   and turned into Morton Downey Jr. and it's more and more extreme realism and reality shows.
[00:07:59.120 --> 00:08:00.480]   Or staged extreme realism.
[00:08:00.480 --> 00:08:01.480]   Staged, more things.
[00:08:01.480 --> 00:08:03.480]   And we think we know that it has to get more and more extreme.
[00:08:03.480 --> 00:08:05.320]   Newspakers tend to get more sensationalistic.
[00:08:05.320 --> 00:08:06.480]   They don't have to.
[00:08:06.480 --> 00:08:11.160]   When it doesn't happen, it doesn't happen in TV when you get somebody like CBS's Edward
[00:08:11.160 --> 00:08:13.400]   Armaro who said, "No, we're going to be the Tiffany network.
[00:08:13.400 --> 00:08:15.320]   We're not going to go down that road."
[00:08:15.320 --> 00:08:18.320]   Or somebody like the New York Times, who said, "We're not going to go down that road.
[00:08:18.320 --> 00:08:20.440]   Let the National Enquirer to go down that road."
[00:08:20.440 --> 00:08:26.760]   It doesn't seem in tech that there's anybody saying, "No, let's be responsible.
[00:08:26.760 --> 00:08:29.480]   Let's not maximize 100% for attention.
[00:08:29.480 --> 00:08:30.800]   Let's be more responsible."
[00:08:30.800 --> 00:08:33.920]   I think the pressure is going to be on for YouTube now that people know that they're
[00:08:33.920 --> 00:08:38.120]   doing this and Facebook and all these others to draw the line at some point.
[00:08:38.120 --> 00:08:39.120]   That's the problem.
[00:08:39.120 --> 00:08:40.120]   They're not drawing the line.
[00:08:40.120 --> 00:08:42.400]   But is there also a user responsibility part of that?
[00:08:42.400 --> 00:08:43.840]   I mean, there is this sort of...
[00:08:43.840 --> 00:08:45.960]   If you're offered a platter of...
[00:08:45.960 --> 00:08:46.960]   It's our responsibility.
[00:08:46.960 --> 00:08:47.960]   It's our responsibility.
[00:08:47.960 --> 00:08:48.960]   And we're never going to dream.
[00:08:48.960 --> 00:08:49.960]   Yeah.
[00:08:49.960 --> 00:08:52.040]   We have to stop picking the extreme ones.
[00:08:52.040 --> 00:08:54.920]   Do you think humans can do that?
[00:08:54.920 --> 00:09:00.520]   My son, who's not in the audience, he said the other day, he's like, "Oh, I know what
[00:09:00.520 --> 00:09:01.800]   Clickbait is."
[00:09:01.800 --> 00:09:04.240]   And I know he said, "I just considered a Clickbait tax."
[00:09:04.240 --> 00:09:07.360]   And so about once a week I decided, "I'm just going to go in on Clickbait.
[00:09:07.360 --> 00:09:08.360]   I know what this is.
[00:09:08.360 --> 00:09:09.360]   I know it's ridiculous."
[00:09:09.360 --> 00:09:10.360]   But it's a tax.
[00:09:10.360 --> 00:09:14.960]   And I'm just going to click on it and pay the tax and enjoy it and then limit myself in
[00:09:14.960 --> 00:09:15.960]   the future.
[00:09:15.960 --> 00:09:16.960]   Which is a problem.
[00:09:16.960 --> 00:09:17.960]   We need to look at it.
[00:09:17.960 --> 00:09:22.240]   It's not that simple of a transaction though, because on the one hand you're clicking and
[00:09:22.240 --> 00:09:26.880]   absorbing whatever the content is, but you're still being scraped on the back end.
[00:09:26.880 --> 00:09:30.000]   All of these systems, the business model for all of them is surveillance.
[00:09:30.000 --> 00:09:31.000]   That sounds painful.
[00:09:31.000 --> 00:09:34.280]   I don't want to be scraped on the back end.
[00:09:34.280 --> 00:09:38.000]   Very few people do, but that's kind of extreme.
[00:09:38.000 --> 00:09:39.000]   Some people do.
[00:09:39.000 --> 00:09:41.320]   I'm sure.
[00:09:41.320 --> 00:09:45.320]   If you watch Pat on the back end videos, you'll get scraped on the back end videos later.
[00:09:45.320 --> 00:09:46.320]   Yeah.
[00:09:46.320 --> 00:09:47.320]   I'm sorry.
[00:09:47.320 --> 00:09:49.320]   I should not have derailed that.
[00:09:49.320 --> 00:09:50.760]   Exist every time.
[00:09:50.760 --> 00:09:53.760]   No, but here's the example.
[00:09:53.760 --> 00:09:58.880]   Mark Zuckerberg testifying in Congress.
[00:09:58.880 --> 00:10:02.360]   Some members of Congress had different agendas, but a number of them wanted to know about
[00:10:02.360 --> 00:10:04.440]   what do they call them?
[00:10:04.440 --> 00:10:09.040]   The dark dossies that Facebook gathers on people who aren't even members.
[00:10:09.040 --> 00:10:11.320]   They're getting scraped on the back end.
[00:10:11.320 --> 00:10:16.880]   Well, the interesting thing is, you know, chasing the clicks, right, is kind of what
[00:10:16.880 --> 00:10:17.880]   we're getting at.
[00:10:17.880 --> 00:10:21.360]   Well, it's ruined blogging.
[00:10:21.360 --> 00:10:23.400]   You could say, certainly.
[00:10:23.400 --> 00:10:28.560]   There's a really good post on this on, I think it was on Facebook by somebody who's been in
[00:10:28.560 --> 00:10:31.840]   the industry a long time, been on the show many times.
[00:10:31.840 --> 00:10:32.840]   Lance, you're going off.
[00:10:32.840 --> 00:10:33.840]   Well, Lance, yeah.
[00:10:33.840 --> 00:10:34.840]   Right.
[00:10:34.840 --> 00:10:35.840]   Great guy was terrific.
[00:10:35.840 --> 00:10:37.760]   Lance is kind of a victim of this, isn't he?
[00:10:37.760 --> 00:10:38.760]   Yes.
[00:10:38.760 --> 00:10:43.600]   Because he went to Mashable to be editor in chief, and Mashable is the poster child for
[00:10:43.600 --> 00:10:44.600]   chasing the clicks.
[00:10:44.600 --> 00:10:45.600]   Chase and clicks.
[00:10:45.600 --> 00:10:46.600]   Or it was.
[00:10:46.600 --> 00:10:47.600]   And what happened?
[00:10:47.600 --> 00:10:48.600]   What happened, right?
[00:10:48.600 --> 00:10:49.600]   Yeah.
[00:10:49.600 --> 00:10:53.640]   And so, Lance wrote this great post very kind of introspective about the fact that I got
[00:10:53.640 --> 00:10:55.160]   really good at doing this.
[00:10:55.160 --> 00:10:57.040]   And he was one of the best.
[00:10:57.040 --> 00:11:01.200]   We used to, you know, joke, and he would send, sometimes we would do some really good at
[00:11:01.200 --> 00:11:02.960]   the height of when this was going on.
[00:11:02.960 --> 00:11:04.200]   We would do something really good.
[00:11:04.200 --> 00:11:09.480]   Like, I think one time we did 20 things to do that'll lose your geek card or something.
[00:11:09.480 --> 00:11:11.120]   Got a zillion pages in traffic.
[00:11:11.120 --> 00:11:15.520]   And I remember Lance and Lance sent me a message saying, "I just, you know, went in and
[00:11:15.520 --> 00:11:18.760]   excoriated my team for not thinking of this idea first."
[00:11:18.760 --> 00:11:19.760]   Oh.
[00:11:19.760 --> 00:11:20.760]   You know, great.
[00:11:20.760 --> 00:11:21.760]   He got stuff.
[00:11:21.760 --> 00:11:25.280]   But, you know, I understand why you feel like you have a responsibility to your staff.
[00:11:25.280 --> 00:11:26.280]   Right.
[00:11:26.280 --> 00:11:30.200]   That, you know, and your owners that you need to generate revenue that this is what they
[00:11:30.200 --> 00:11:31.400]   brought you here for.
[00:11:31.400 --> 00:11:35.760]   And even somebody like Lance, who is editor in chief at PC Magazine, who really has real
[00:11:35.760 --> 00:11:39.360]   strong journalistic credentials, got sucked in by this.
[00:11:39.360 --> 00:11:40.360]   Yes.
[00:11:40.360 --> 00:11:43.960]   And in fact, this is kind of what this may a culpa in medium that he wrote is.
[00:11:43.960 --> 00:11:44.960]   Yes.
[00:11:44.960 --> 00:11:47.880]   And he's just pondering new rules for an old profession.
[00:11:47.880 --> 00:11:54.800]   And the thing is, is Mashable is kind of the prime example, but it's not just Mashable,
[00:11:54.800 --> 00:12:00.680]   right, is that you follow the clicks and eventually what you find.
[00:12:00.680 --> 00:12:08.720]   And a lot of places are finding or having their comeuppance on this is that you can't
[00:12:08.720 --> 00:12:09.720]   monetize that.
[00:12:09.720 --> 00:12:10.720]   There's a great quote.
[00:12:10.720 --> 00:12:12.480]   Oh, God, I hope that's true.
[00:12:12.480 --> 00:12:16.760]   Where Lance goes to his editor and his editors like they're about to put up a paywall and
[00:12:16.760 --> 00:12:19.040]   he goes to and said, you got to stop.
[00:12:19.040 --> 00:12:20.040]   And he's like, why?
[00:12:20.040 --> 00:12:24.200]   And he said, because everything that we have, you can find anywhere else.
[00:12:24.200 --> 00:12:25.200]   Content is free.
[00:12:25.200 --> 00:12:32.000]   Well, and here's the thing it used to be before algorithm started to rule publishing that
[00:12:32.000 --> 00:12:35.640]   the simple way to get more traffic was to write more content.
[00:12:35.640 --> 00:12:36.640]   Right.
[00:12:36.640 --> 00:12:40.960]   It's almost like dumbly simple, but actually no longer.
[00:12:40.960 --> 00:12:42.320]   That's kind of demand media, right?
[00:12:42.320 --> 00:12:47.280]   That was a whole, there was a whole generation couple of years back of sites that were created
[00:12:47.280 --> 00:12:48.600]   around Google searches.
[00:12:48.600 --> 00:12:53.920]   Like you monetize the search for tortoise shell belt buckles and you're right, a dumb,
[00:12:53.920 --> 00:12:56.320]   meaningless article, but you know, you'll get traffic because people search for that.
[00:12:56.320 --> 00:13:00.680]   Yeah, it's like Wickey How or any of the big how to engines, which kind of got shut down
[00:13:00.680 --> 00:13:01.680]   algorithmically.
[00:13:01.680 --> 00:13:03.080]   But here's the interesting thing to me.
[00:13:03.080 --> 00:13:08.720]   I think that algorithms are very good at understanding what people want for a little while,
[00:13:08.720 --> 00:13:12.920]   but people are the smartest algorithm and we start to burn out.
[00:13:12.920 --> 00:13:17.280]   We all know exactly what a clickbait headline sounds like.
[00:13:17.280 --> 00:13:19.800]   And we now are starting to get bored and move.
[00:13:19.800 --> 00:13:20.800]   That's what hit Facebook.
[00:13:20.800 --> 00:13:22.640]   That's why Facebook deprecated the publishers, right?
[00:13:22.640 --> 00:13:25.240]   I think it depends on how you define the we.
[00:13:25.240 --> 00:13:27.400]   We who, right?
[00:13:27.400 --> 00:13:31.240]   Like we may understand what a clickbait headline is, but I mean.
[00:13:31.240 --> 00:13:33.960]   Well, ultimately, the only matters of traffic goes down.
[00:13:33.960 --> 00:13:38.280]   But I think the larger we are starting to understand what that looks like.
[00:13:38.280 --> 00:13:43.240]   The larger we totally got duped by Facebook on Facebook.
[00:13:43.240 --> 00:13:46.960]   So I'm not a big, you know, like, I don't know if I gave a lot of credence to the we
[00:13:46.960 --> 00:13:47.960]   here.
[00:13:47.960 --> 00:13:52.160]   I think that the algorithms are much more powerful than we give them credit for.
[00:13:52.160 --> 00:13:53.920]   So I don't know.
[00:13:53.920 --> 00:13:55.680]   I think maybe you're right.
[00:13:55.680 --> 00:13:58.040]   It worked, but maybe we won't get fooled again.
[00:13:58.040 --> 00:14:00.680]   You know, maybe now we go, Oh, that didn't work.
[00:14:00.680 --> 00:14:04.880]   I mean, Mashable is gone effectively, right?
[00:14:04.880 --> 00:14:08.120]   Facebook stopped dep, you know, deprecated link baiting.
[00:14:08.120 --> 00:14:11.720]   And I think not because I don't think Facebook did it altruistically because they want to
[00:14:11.720 --> 00:14:13.640]   protect the Facebook does nothing altruistic.
[00:14:13.640 --> 00:14:15.920]   No, they did it because it wasn't working anymore.
[00:14:15.920 --> 00:14:16.920]   Right.
[00:14:16.920 --> 00:14:19.920]   And I think it was trashing up the it was polluting the stream.
[00:14:19.920 --> 00:14:27.560]   I actually don't give we credit and sort of I don't think that the larger we was is has
[00:14:27.560 --> 00:14:29.720]   grown wiser necessarily.
[00:14:29.720 --> 00:14:32.200]   I just think that humans are susceptible to trends.
[00:14:32.200 --> 00:14:33.880]   We get bored very quickly.
[00:14:33.880 --> 00:14:39.440]   We recognize trends very quick and then we move on to new ones and there's something
[00:14:39.440 --> 00:14:41.240]   else out there that is going to replace it.
[00:14:41.240 --> 00:14:45.480]   And I think that there's a good reason people if you spend much time on Facebook and have
[00:14:45.480 --> 00:14:50.760]   continued to spend even before last week, Facebook's getting kind of dead.
[00:14:50.760 --> 00:14:53.840]   Yeah, it's just it's just not that interesting anymore.
[00:14:53.840 --> 00:14:55.240]   And I think people are starting to move on.
[00:14:55.240 --> 00:14:59.880]   I mean, there's a reason that nobody under the age of 25 spends any time on Facebook.
[00:14:59.880 --> 00:15:00.880]   Yeah.
[00:15:00.880 --> 00:15:01.880]   Yeah.
[00:15:01.880 --> 00:15:06.360]   So we sort of looked at this pretty hard because we were going, you know, we're like a smaller
[00:15:06.360 --> 00:15:08.160]   niche player, right?
[00:15:08.160 --> 00:15:09.720]   And so we're tech republic.
[00:15:09.720 --> 00:15:12.840]   Sorry, tech republic is a smaller niche player.
[00:15:12.840 --> 00:15:17.320]   I'm a zee net the other site that I spend, you know, a lot of time on.
[00:15:17.320 --> 00:15:22.840]   And we sort of got to this point where it's like, okay, we could go all in on like Lindsey
[00:15:22.840 --> 00:15:28.480]   saying like, get more put more content out there and try to keep up with the rat race.
[00:15:28.480 --> 00:15:35.320]   And what we decided at the time was to go in a different direction was that our users
[00:15:35.320 --> 00:15:38.240]   were just, we were, we don't want to produce more stuff.
[00:15:38.240 --> 00:15:41.800]   Our conclusion was, and when we asked our users, we're, we don't want to produce more
[00:15:41.800 --> 00:15:44.120]   stuff that really doesn't have a lot of value.
[00:15:44.120 --> 00:15:50.640]   So we did instead was really listen to what our users tell us, the stuff that we create
[00:15:50.640 --> 00:15:52.160]   that gives the brings them value.
[00:15:52.160 --> 00:15:53.760]   That's to me responsible journalism.
[00:15:53.760 --> 00:15:58.760]   I gather, Amy, you don't feel like that's widespread.
[00:15:58.760 --> 00:16:01.600]   Well, no, I mean, I can tell you that trunk.
[00:16:01.600 --> 00:16:04.360]   Oh, God, trunk lowercase trunk.
[00:16:04.360 --> 00:16:09.400]   So under what before Michael Farro, who was the chairman left, this was the old Chicago
[00:16:09.400 --> 00:16:10.400]   Tribune, right?
[00:16:10.400 --> 00:16:11.400]   Right.
[00:16:11.400 --> 00:16:14.320]   And as a Chicago, like I'm originally from Chicago and I grew up with the with the
[00:16:14.320 --> 00:16:19.320]   Chicago, you know, like it was heartbreaking watching this whole place get dismantled by
[00:16:19.320 --> 00:16:21.440]   somebody who had never worked inside of a newsroom.
[00:16:21.440 --> 00:16:24.520]   And quite frankly, he knows very little about AI.
[00:16:24.520 --> 00:16:30.840]   And the grand plan was that he had acquired over a hundred AI patents that were going
[00:16:30.840 --> 00:16:37.360]   to be used to automatically generate content because the business model was predicated
[00:16:37.360 --> 00:16:39.640]   on just like barfing up a lot of news.
[00:16:39.640 --> 00:16:42.440]   I remember the speech he gave to the troops.
[00:16:42.440 --> 00:16:44.000]   Well, I'll tell you something.
[00:16:44.000 --> 00:16:47.560]   I read every one of those patents.
[00:16:47.560 --> 00:16:52.680]   And not only did basically none of them have any direct applicability to anything that
[00:16:52.680 --> 00:16:57.880]   was happening at the Chicago Tribune, but any of the automation that was being discussed,
[00:16:57.880 --> 00:16:59.920]   like it was better spelled out in Magic Leap.
[00:16:59.920 --> 00:17:01.880]   You know, Amazon is already pretty far ahead.
[00:17:01.880 --> 00:17:04.120]   Like everybody else was pretty far ahead.
[00:17:04.120 --> 00:17:10.800]   But but in traditional media, I think there is still this idea that more like, like creating
[00:17:10.800 --> 00:17:16.400]   huge amounts of content auto magically through the miracles and wonders of machine learning,
[00:17:16.400 --> 00:17:17.400]   right?
[00:17:17.400 --> 00:17:22.000]   And somehow going to create enough traffic that that helps boost the numbers.
[00:17:22.000 --> 00:17:23.640]   So you saw that at Tronk.
[00:17:23.640 --> 00:17:24.640]   Let me show.
[00:17:24.640 --> 00:17:27.440]   Let me I got to show this because it's so crazy.
[00:17:27.440 --> 00:17:28.720]   That press release was insane.
[00:17:28.720 --> 00:17:35.160]   Well, let me just show you the the Tronk video with Malcolm Cassell, who I know for you've
[00:17:35.160 --> 00:17:36.160]   known for years.
[00:17:36.160 --> 00:17:38.080]   This is the future of journalism.
[00:17:38.080 --> 00:17:42.080]   The future of contact doesn't get much better than that.
[00:17:42.080 --> 00:17:46.400]   If you care about media technology, this is the place to see if you can identify any
[00:17:46.400 --> 00:17:51.520]   actual concrete ideas in this at all.
[00:17:51.520 --> 00:17:56.440]   Tronk stands for a Tribune Online Content and it also means pooling of resources.
[00:17:56.440 --> 00:18:02.320]   It's about meeting in the middle, having a tech startup culture meet a legacy corporate
[00:18:02.320 --> 00:18:04.600]   culture and then evolving and changing.
[00:18:04.600 --> 00:18:06.760]   That's really the fun part.
[00:18:06.760 --> 00:18:07.760]   That's exciting.
[00:18:07.760 --> 00:18:09.760]   You have great titles.
[00:18:09.760 --> 00:18:12.880]   We produce tons of great content every single day.
[00:18:12.880 --> 00:18:17.800]   We're really focused on how we deliver it to people in a way that they want to consume
[00:18:17.800 --> 00:18:19.800]   it more and more.
[00:18:19.800 --> 00:18:20.800]   More and more.
[00:18:20.800 --> 00:18:26.040]   We want to harness the power of our journalism is to have an optimization group.
[00:18:26.040 --> 00:18:30.840]   This Tronk team will work with all of the local markets to harness the power of our
[00:18:30.840 --> 00:18:31.840]   partners.
[00:18:31.840 --> 00:18:33.640]   It feels like a parody video from Ready Player One.
[00:18:33.640 --> 00:18:34.640]   I'm just saying.
[00:18:34.640 --> 00:18:37.560]   I'm trying to find the way that we reach the biggest global audience possible.
[00:18:37.560 --> 00:18:38.560]   This is I.O.I.
[00:18:38.560 --> 00:18:43.360]   Our content really valuable to the broadest possible audience.
[00:18:43.360 --> 00:18:44.360]   That's enough of that.
[00:18:44.360 --> 00:18:45.360]   I can't hear any more.
[00:18:45.360 --> 00:18:47.280]   I'll play the opposite of that.
[00:18:47.280 --> 00:18:50.440]   You know who's doing really well the information.
[00:18:50.440 --> 00:18:51.680]   I love the information.
[00:18:51.680 --> 00:18:53.880]   Jessica Lesson is a genius.
[00:18:53.880 --> 00:18:54.880]   That's right.
[00:18:54.880 --> 00:18:55.880]   They're not trying to.
[00:18:55.880 --> 00:18:56.880]   This is the opposite method, right?
[00:18:56.880 --> 00:18:58.720]   But this is that paywall.
[00:18:58.720 --> 00:19:00.120]   They charge 400 bucks a year.
[00:19:00.120 --> 00:19:01.880]   I pay it.
[00:19:01.880 --> 00:19:04.160]   They have some of the best journalists, right?
[00:19:04.160 --> 00:19:08.480]   And they really focus not on linkbait but on really what you said.
[00:19:08.480 --> 00:19:10.480]   They're valuable content users.
[00:19:10.480 --> 00:19:13.280]   Because you have to earn that 400 bucks.
[00:19:13.280 --> 00:19:16.200]   These big conglomeration, there was all this consolidation.
[00:19:16.200 --> 00:19:21.000]   And so you have these huge like Tronk, you know, there are too many news organizations
[00:19:21.000 --> 00:19:24.600]   within this big bubble and you just can't support all of them and they can't all be
[00:19:24.600 --> 00:19:26.000]   the same thing to all people.
[00:19:26.000 --> 00:19:29.920]   So the information I think is succeeding because it's not trying to be everything.
[00:19:29.920 --> 00:19:33.560]   It's trying to do one thing exceptionally well and it does.
[00:19:33.560 --> 00:19:36.560]   And so people are willing to pay for it and they're able to sustain that way.
[00:19:36.560 --> 00:19:39.160]   Doesn't feel like that's a model that can be used.
[00:19:39.160 --> 00:19:41.560]   It's a model that works for doing something specific.
[00:19:41.560 --> 00:19:47.520]   And I think that Jessica's a genius also and she's really straightforward about what she's
[00:19:47.520 --> 00:19:48.520]   doing.
[00:19:48.520 --> 00:19:49.520]   She's a real vision.
[00:19:49.520 --> 00:19:50.520]   I'm really good at it.
[00:19:50.520 --> 00:19:55.040]   And I think it's going to allow her to continue to do what she wants to do, which is pursue
[00:19:55.040 --> 00:20:00.280]   serious journalism about the Valley with the caveat that that's probably never going to
[00:20:00.280 --> 00:20:04.520]   extend beyond the Valley unless the same model gets applied to a different topic.
[00:20:04.520 --> 00:20:05.840]   It's kind of like what we do here.
[00:20:05.840 --> 00:20:08.320]   We're not trying to get a mass audience.
[00:20:08.320 --> 00:20:09.600]   She's not trying to get a mass audience.
[00:20:09.600 --> 00:20:14.280]   But is there a room for, I mean, CNET is, tech republic is to some degree.
[00:20:14.280 --> 00:20:15.840]   I mean, you have a focus.
[00:20:15.840 --> 00:20:21.400]   But certainly CBS Interactive is how there has to be room for mass media as well.
[00:20:21.400 --> 00:20:26.520]   Does mass media, can it find its way without descending the link bait?
[00:20:26.520 --> 00:20:27.520]   I think it has.
[00:20:27.520 --> 00:20:31.920]   I mean, if you look at what, I mean, I'm obviously going to say this, but if you look
[00:20:31.920 --> 00:20:37.480]   at what CBS Interactive has done, we have continued to serve an audience, the content
[00:20:37.480 --> 00:20:40.360]   that they want in a serious manner over time.
[00:20:40.360 --> 00:20:41.680]   And that doesn't mean we don't expand.
[00:20:41.680 --> 00:20:45.920]   We expand aggressively into lots of different parts of what that means.
[00:20:45.920 --> 00:20:47.680]   We cover culture, we cover science.
[00:20:47.680 --> 00:20:52.720]   And you don't get pressure from CBS to drive numbers and stuff like you must.
[00:20:52.720 --> 00:20:53.960]   Always looking to grow.
[00:20:53.960 --> 00:20:59.480]   But we look to grow by pleasing a broad audience and talking to them about what matters.
[00:20:59.480 --> 00:21:00.480]   Right.
[00:21:00.480 --> 00:21:01.960]   And what matters to them.
[00:21:01.960 --> 00:21:05.400]   The thing that is difficult is that what matters to them changes a lot.
[00:21:05.400 --> 00:21:07.600]   People change, generations change.
[00:21:07.600 --> 00:21:08.600]   That's hard to do.
[00:21:08.600 --> 00:21:10.400]   We are constantly, constantly looking very closely.
[00:21:10.400 --> 00:21:11.400]   I'm dealing with that now.
[00:21:11.400 --> 00:21:12.400]   Yeah.
[00:21:12.400 --> 00:21:13.400]   Oh, it's changing.
[00:21:13.400 --> 00:21:14.400]   Oh, it's changing.
[00:21:14.400 --> 00:21:16.280]   It's very different 13 years later.
[00:21:16.280 --> 00:21:20.800]   And as a broadcaster, I always told broadcasters, don't chase the audience because it's chasing
[00:21:20.800 --> 00:21:21.800]   your tail.
[00:21:21.800 --> 00:21:22.800]   You can't.
[00:21:22.800 --> 00:21:28.000]   So you, for me, and I guess for niche broadcasters or niche journalists like Jessica, having
[00:21:28.000 --> 00:21:31.520]   a vision and pursuing that vision and hoping that what you're doing, being true to yourself
[00:21:31.520 --> 00:21:35.840]   through that vision, will generate enough revenue enough of an audience that you can survive.
[00:21:35.840 --> 00:21:40.080]   But it's very different from a mass marketer or a mass media enterprise, which really does
[00:21:40.080 --> 00:21:41.440]   need to chase the audience, I guess.
[00:21:41.440 --> 00:21:43.760]   It needs to be nimble and understand what the audience wants.
[00:21:43.760 --> 00:21:44.760]   Understand the audience.
[00:21:44.760 --> 00:21:45.760]   Right.
[00:21:45.760 --> 00:21:48.040]   And I think that there's, I think we fell down a rabbit hole where we started to think
[00:21:48.040 --> 00:21:49.800]   as an industry.
[00:21:49.800 --> 00:21:52.840]   And this is the mashable topic that we're talking about that we could tell the audience
[00:21:52.840 --> 00:21:53.840]   what they wanted.
[00:21:53.840 --> 00:21:54.840]   Yeah.
[00:21:54.840 --> 00:21:59.760]   They're almost beating them at their own game, right, by using machines to tell us what they
[00:21:59.760 --> 00:22:02.520]   were going to want before they knew what they wanted.
[00:22:02.520 --> 00:22:06.880]   And the reality is that everybody gets wise to that eventually and says, no, I'm interested
[00:22:06.880 --> 00:22:12.080]   in something else now and circling all the way back to that Facebook issue that we were
[00:22:12.080 --> 00:22:13.080]   talking about a minute ago.
[00:22:13.080 --> 00:22:16.120]   I noticed that something in the chat room said, I don't think Facebook is going away.
[00:22:16.120 --> 00:22:19.720]   I have the same number of friends this week that I did last week.
[00:22:19.720 --> 00:22:22.880]   But what I found is interesting is that, yeah, that's because people are just walking
[00:22:22.880 --> 00:22:23.880]   away.
[00:22:23.880 --> 00:22:25.040]   They're just wandering off somewhere else.
[00:22:25.040 --> 00:22:28.240]   I actually removed the app from my phone.
[00:22:28.240 --> 00:22:29.240]   Me too.
[00:22:29.240 --> 00:22:32.200]   And my behavior radically changed within a week.
[00:22:32.200 --> 00:22:33.200]   So much happier.
[00:22:33.200 --> 00:22:34.200]   Right.
[00:22:34.200 --> 00:22:36.640]   I can go back and find it on the desktop if I want it to.
[00:22:36.640 --> 00:22:39.040]   Did everybody delete Instagram too?
[00:22:39.040 --> 00:22:40.040]   No.
[00:22:40.040 --> 00:22:41.040]   And that's the thing.
[00:22:41.040 --> 00:22:42.720]   I actually just switched right over to Instagram.
[00:22:42.720 --> 00:22:43.720]   And I know.
[00:22:43.720 --> 00:22:46.800]   But Facebook's getting the same, but they're not, I mean, they're not getting the same signals
[00:22:46.800 --> 00:22:50.680]   from Instagram that they got from Facebook, not nearly the richness of information.
[00:22:50.680 --> 00:22:55.040]   I mean, I don't do any of us actually know for sure.
[00:22:55.040 --> 00:22:59.920]   I am certain that my day to day behavior, the things that I search for, the things that
[00:22:59.920 --> 00:23:03.800]   I do online affects what shows up in my Instagram feed.
[00:23:03.800 --> 00:23:04.800]   Sure.
[00:23:04.800 --> 00:23:07.560]   The targeted ads that show up know exactly who I am and what I'm doing.
[00:23:07.560 --> 00:23:08.560]   Yeah.
[00:23:08.560 --> 00:23:10.800]   I buy more stuff from Instagram than any of us.
[00:23:10.800 --> 00:23:11.800]   They have to take Instagram.
[00:23:11.800 --> 00:23:12.800]   Oh, yeah.
[00:23:12.800 --> 00:23:15.400]   Is three to four X.
[00:23:15.400 --> 00:23:18.400]   I think any of the other social networks on ads.
[00:23:18.400 --> 00:23:19.400]   Sure.
[00:23:19.400 --> 00:23:23.840]   I don't presume that Instagram's not keeping a really close eye on what I'm doing.
[00:23:23.840 --> 00:23:26.440]   I just, that's where my friends are.
[00:23:26.440 --> 00:23:27.440]   Yeah.
[00:23:27.440 --> 00:23:28.440]   They are not on Facebook.
[00:23:28.440 --> 00:23:29.440]   Same.
[00:23:29.440 --> 00:23:30.440]   It's straight forward.
[00:23:30.440 --> 00:23:31.440]   This is like such a cool thing.
[00:23:31.440 --> 00:23:33.240]   Do you guys use Snap?
[00:23:33.240 --> 00:23:34.720]   No, not much.
[00:23:34.720 --> 00:23:36.280]   But I bet your daughter does.
[00:23:36.280 --> 00:23:38.280]   No, she's not allowed to.
[00:23:38.280 --> 00:23:40.240]   She's the smartest daughter ever.
[00:23:40.240 --> 00:23:41.760]   I think so.
[00:23:41.760 --> 00:23:44.480]   My kids do, but they're, you know, they're in their 20s.
[00:23:44.480 --> 00:23:47.640]   This user thing is such a push pull thing because yeah, we say.
[00:23:47.640 --> 00:23:48.640]   What's wrong with it before you.
[00:23:48.640 --> 00:23:53.440]   Before you go into this, what's wrong with Snap?
[00:23:53.440 --> 00:23:54.440]   Where should I start?
[00:23:54.440 --> 00:23:55.440]   Oh, boy.
[00:23:55.440 --> 00:23:56.720]   So, well, no, no.
[00:23:56.720 --> 00:24:03.760]   So I think it was in 26, 2014 or 15, 16.
[00:24:03.760 --> 00:24:06.320]   Snap filed some very, very interesting patents.
[00:24:06.320 --> 00:24:11.800]   I will see if I can pull them up, but they had to do with a online advertising marketplace
[00:24:11.800 --> 00:24:18.160]   that use machine learning and object recognition to automatically surface, sellable things
[00:24:18.160 --> 00:24:22.520]   within a photo that others could buy ads against.
[00:24:22.520 --> 00:24:27.320]   And to me, they were also describing in this thing what that system would look like.
[00:24:27.320 --> 00:24:36.160]   To me, that very, very much look like the next generation of auction-based ads for display.
[00:24:36.160 --> 00:24:41.120]   So are you saying that Snap would look at things in your pictures?
[00:24:41.120 --> 00:24:42.120]   Yeah.
[00:24:42.120 --> 00:24:43.120]   Yeah.
[00:24:43.120 --> 00:24:44.640]   So like you've got a bunch of cupcakes there.
[00:24:44.640 --> 00:24:48.120]   They're going to start selling me cupcakes?
[00:24:48.120 --> 00:24:56.160]   So yeah, so there may be filters that appear or like stories like part of the feed where
[00:24:56.160 --> 00:25:01.160]   I'm being targeted because you're being targeted with something related to a cupcake.
[00:25:01.160 --> 00:25:02.880]   You're not drinking what you go.
[00:25:02.880 --> 00:25:06.680]   I've always suspected that the reason Amazon keeps trying to get cameras in my house is
[00:25:06.680 --> 00:25:07.680]   that.
[00:25:07.680 --> 00:25:08.680]   Right?
[00:25:08.680 --> 00:25:17.960]   It's hard to note to what's driving the choices that targeting makes.
[00:25:17.960 --> 00:25:20.440]   But this is not just Instagram.
[00:25:20.440 --> 00:25:21.440]   It's not just Snapchat.
[00:25:21.440 --> 00:25:25.480]   I mean, I went on vacation to Hawaii last week, rented a Jeep.
[00:25:25.480 --> 00:25:27.720]   I did it through Touro.
[00:25:27.720 --> 00:25:29.440]   It's kind of like Airbnb for car rentals.
[00:25:29.440 --> 00:25:31.320]   It was a great experience.
[00:25:31.320 --> 00:25:38.640]   I know that I talked about Jeeps with my friends and on text and I was discussing it all the
[00:25:38.640 --> 00:25:40.680]   time and taking pictures of the Jeep.
[00:25:40.680 --> 00:25:43.080]   And now I am being advertised as Jeeps.
[00:25:43.080 --> 00:25:45.080]   It's just the best for the rest of Hawaii.
[00:25:45.080 --> 00:25:46.080]   Constantly in Twitter.
[00:25:46.080 --> 00:25:47.080]   Yeah.
[00:25:47.080 --> 00:25:49.200]   I don't think I actually tweeted about Jeeps.
[00:25:49.200 --> 00:25:51.080]   I'm just saying that this is pervasive.
[00:25:51.080 --> 00:25:52.080]   It's not all social.
[00:25:52.080 --> 00:25:53.080]   It's not just Instagram.
[00:25:53.080 --> 00:25:54.080]   It's not just Facebook.
[00:25:54.080 --> 00:25:55.080]   It's all social media.
[00:25:55.080 --> 00:26:01.200]   It's a fact I think there are a lot of companies saying thank God Mark is taking all the heat
[00:26:01.200 --> 00:26:03.400]   right now because we don't have to be.
[00:26:03.400 --> 00:26:07.800]   We could just kind of oh yeah, that's too bad about that Facebook thing.
[00:26:07.800 --> 00:26:10.200]   And I'm sure Google's number one on that list.
[00:26:10.200 --> 00:26:12.160]   Here's a Zuckerberg.
[00:26:12.160 --> 00:26:14.080]   I'm just going to play this.
[00:26:14.080 --> 00:26:15.080]   This is risky.
[00:26:15.080 --> 00:26:16.880]   I trust our chat room.
[00:26:16.880 --> 00:26:21.800]   This is a Zuckerberg meme called smile.exe.
[00:26:21.800 --> 00:26:22.800]   Let's see.
[00:26:22.800 --> 00:26:26.200]   Let's see that guy over there.
[00:26:26.200 --> 00:26:33.480]   That's a smile.
[00:26:33.480 --> 00:26:34.480]   Like the Terminator.
[00:26:34.480 --> 00:26:43.280]   He's trying out smiles.
[00:26:43.280 --> 00:26:46.880]   He could practice in front of a mirror or something.
[00:26:46.880 --> 00:26:51.280]   Okay, I just had to watch that.
[00:26:51.280 --> 00:26:54.480]   Zuck does have a somewhat robotic persona.
[00:26:54.480 --> 00:26:58.440]   He's not exactly the poster boy for a warm and fuzzy.
[00:26:58.440 --> 00:27:00.800]   Although the stock went up after Zuck's testimony.
[00:27:00.800 --> 00:27:02.600]   He kept it so even keel.
[00:27:02.600 --> 00:27:08.640]   It was impressive to me how able he was to just use the same answer over and over again
[00:27:08.640 --> 00:27:10.040]   apparently without shame.
[00:27:10.040 --> 00:27:12.080]   My team will get back to you on that Senator.
[00:27:12.080 --> 00:27:13.080]   I don't know.
[00:27:13.080 --> 00:27:19.360]   I think there was the drinking game at Facebook or maybe at Google on how many times he said
[00:27:19.360 --> 00:27:21.960]   Senator or Congress.
[00:27:21.960 --> 00:27:24.160]   Man, Congress will make.
[00:27:24.160 --> 00:27:30.160]   Here's a list of all of the things that Facebook's team will get back to.
[00:27:30.160 --> 00:27:33.000]   I'm glad somebody was making a list.
[00:27:33.000 --> 00:27:34.720]   There's quite a few.
[00:27:34.720 --> 00:27:36.120]   Where are the Russians hiding?
[00:27:36.120 --> 00:27:39.280]   If I delete my account, how long does Facebook keep my data?
[00:27:39.280 --> 00:27:41.080]   Does Facebook delete the backups?
[00:27:41.080 --> 00:27:42.440]   Does it have apps that Facebook ban?
[00:27:42.440 --> 00:27:43.440]   What about audits?
[00:27:43.440 --> 00:27:45.280]   How many bots has Facebook taken down?
[00:27:45.280 --> 00:27:48.120]   What regulations would Facebook collaborate with Congress on?
[00:27:48.120 --> 00:27:51.000]   Does Facebook use cross device tracking?
[00:27:51.000 --> 00:27:52.920]   Where do people affected live?
[00:27:52.920 --> 00:27:55.160]   Who has the data that Cambridge Analytica has?
[00:27:55.160 --> 00:27:56.160]   Who else has it?
[00:27:56.160 --> 00:27:57.640]   What are you doing about Russian interference?
[00:27:57.640 --> 00:27:59.640]   All of these more than...
[00:27:59.640 --> 00:28:01.280]   We knew the answers too.
[00:28:01.280 --> 00:28:02.480]   More than 20 of them.
[00:28:02.480 --> 00:28:03.480]   Most of those.
[00:28:03.480 --> 00:28:04.480]   I am so bad.
[00:28:04.480 --> 00:28:05.480]   Yeah, there was some disingenuous stuff.
[00:28:05.480 --> 00:28:09.320]   So was it Congress that was at fault then were the members of Congress who were at fault
[00:28:09.320 --> 00:28:11.320]   for not pursuing this?
[00:28:11.320 --> 00:28:12.320]   I think...
[00:28:12.320 --> 00:28:14.200]   Letting him off the hook, letting him say that.
[00:28:14.200 --> 00:28:20.800]   I think that journalists spent a lot of time last week making fun of Congress people who
[00:28:20.800 --> 00:28:23.640]   didn't understand how Facebook works.
[00:28:23.640 --> 00:28:27.880]   And I honestly think that the reason that happened is because Zack played it so cool.
[00:28:27.880 --> 00:28:34.280]   He was so dispassionate and a little bit disingenuous about his answers that I think
[00:28:34.280 --> 00:28:35.280]   they didn't even know.
[00:28:35.280 --> 00:28:40.040]   There were people who wanted to make fun of somebody and turned to Congress.
[00:28:40.040 --> 00:28:45.760]   Well while everybody was busy playing on Twitter and making fun, Facebook, legislation
[00:28:45.760 --> 00:28:50.960]   that Facebook wrote went into effect or went up for a vote and looks like we'll go into
[00:28:50.960 --> 00:28:52.360]   effect in the state of Maryland.
[00:28:52.360 --> 00:28:54.280]   Oh, tell us about that.
[00:28:54.280 --> 00:28:55.280]   That all happens simultaneously.
[00:28:55.280 --> 00:28:56.280]   What was that?
[00:28:56.280 --> 00:28:57.280]   Right.
[00:28:57.280 --> 00:29:01.880]   So if you'll remember, every big media outlet was touting their exclusive sit down with
[00:29:01.880 --> 00:29:04.920]   Cheryl Sandberg and of course she was on a media tour.
[00:29:04.920 --> 00:29:11.000]   Over and over again, she talked about industry leading thought, industry leading thoughts,
[00:29:11.000 --> 00:29:16.280]   industry leading ideas, industry leading policies, which was all lobbyist code for regulation.
[00:29:16.280 --> 00:29:23.320]   So days before, while she's doing this and days before Zuckerberg shows up on the Hill,
[00:29:23.320 --> 00:29:27.800]   Facebook was meeting with lawmakers in different states, one of them being Maryland where,
[00:29:27.800 --> 00:29:31.440]   and I can find the link to this, but Facebook wrote the legislation.
[00:29:31.440 --> 00:29:37.360]   See on the surface of this, this sounds like good legislation plans to regulate political
[00:29:37.360 --> 00:29:38.720]   ads on Facebook.
[00:29:38.720 --> 00:29:39.720]   Right.
[00:29:39.720 --> 00:29:47.120]   But they so, so like nothing is like everything that happened was Zuckerberg giving testimony
[00:29:47.120 --> 00:29:50.160]   that was all, it was all for show.
[00:29:50.160 --> 00:29:54.440]   That was all for show, but, but and so everybody was making fun of it and they were very distracted.
[00:29:54.440 --> 00:29:59.320]   But on the back end, we were getting scraped on the back end.
[00:29:59.320 --> 00:30:01.320]   We get scraped on the back end.
[00:30:01.320 --> 00:30:02.320]   It was done.
[00:30:02.320 --> 00:30:08.360]   So, a wheeled Castleberry, Facebook's vice president for state policy says, yeah, we
[00:30:08.360 --> 00:30:12.640]   helped draft the Maryland legislation and we look forward to implementing it.
[00:30:12.640 --> 00:30:15.360]   Now, that doesn't, that's not necessarily bad.
[00:30:15.360 --> 00:30:17.200]   That means they want to get the heat off of them.
[00:30:17.200 --> 00:30:19.320]   Maybe they found a way that they think will help.
[00:30:19.320 --> 00:30:22.120]   We believe, but Leo, look, go back.
[00:30:22.120 --> 00:30:26.560]   So, so this was, we believe this bill will be a national model for the other 49 states
[00:30:26.560 --> 00:30:30.160]   to follow avoiding, by the way, federal regulation, right?
[00:30:30.160 --> 00:30:35.840]   And if there's some regulation that gets started that takes all of the heat off of Zuckerberg,
[00:30:35.840 --> 00:30:41.680]   because technically nobody broke any laws, that's, that's, so this is all about avoiding,
[00:30:41.680 --> 00:30:46.760]   this is all about showing up in public, keeping the share price high and avoiding federal
[00:30:46.760 --> 00:30:47.760]   regulation.
[00:30:47.760 --> 00:30:53.280]   So, part of the problem is that the laws that affect broadcasters about political ads,
[00:30:53.280 --> 00:30:57.040]   that they have to, for instance, reveal who bought the ad, don't apply to digital.
[00:30:57.040 --> 00:30:58.040]   Right.
[00:30:58.040 --> 00:31:01.240]   So, this is the federal laws, the Honest Ads Act.
[00:31:01.240 --> 00:31:03.080]   Is Facebook supporting the Honest Ads Act?
[00:31:03.080 --> 00:31:05.000]   I think they are.
[00:31:05.000 --> 00:31:06.520]   Or not.
[00:31:06.520 --> 00:31:09.920]   This, this Maryland law looks a lot like the Honest Ads Act.
[00:31:09.920 --> 00:31:13.040]   It says, this is an article from the Baltimore Sun.
[00:31:13.040 --> 00:31:17.120]   It resembles legislation in Congress aimed at monitoring political advertising on social
[00:31:17.120 --> 00:31:18.120]   media.
[00:31:18.120 --> 00:31:20.040]   It would affect Google, by the way, as well as Facebook.
[00:31:20.040 --> 00:31:23.960]   New York, California, Connecticut also weighing their own measures.
[00:31:23.960 --> 00:31:26.800]   We want to keep the platforms accountable.
[00:31:26.800 --> 00:31:29.120]   I'm trying to find the part where it says what.
[00:31:29.120 --> 00:31:31.960]   Whether they are or not.
[00:31:31.960 --> 00:31:33.720]   They will, they will have to.
[00:31:33.720 --> 00:31:38.680]   Really what all of this is about at the end is that, and I know I've said this before
[00:31:38.680 --> 00:31:43.400]   on the show, but there's three companies that know data better than anybody else, and they're
[00:31:43.400 --> 00:31:50.720]   eating the world, they're disrupting industries, and they are undermining the privacy of people
[00:31:50.720 --> 00:31:52.960]   every single day, more and more, right?
[00:31:52.960 --> 00:31:55.120]   Facebook, Amazon, and Google.
[00:31:55.120 --> 00:32:03.000]   And they're doing it largely with a black box of data that they control, and that data
[00:32:03.000 --> 00:32:10.760]   being the new oil, they're using that to become one of the three of the most powerful
[00:32:10.760 --> 00:32:15.280]   companies in the history of humanity.
[00:32:15.280 --> 00:32:20.320]   The transparency issue is a big part of the issue.
[00:32:20.320 --> 00:32:25.920]   Facebook, the least transparent, Amazon somewhere in the middle, Google a little bit more transparent
[00:32:25.920 --> 00:32:27.920]   of what they do with the data and all that.
[00:32:27.920 --> 00:32:34.800]   So all of these companies you can bet over the next, over the coming years are going to
[00:32:34.800 --> 00:32:40.480]   be under enormous pressure to tell what data they're actually collecting, what they do
[00:32:40.480 --> 00:32:49.840]   with it, what it's worth to them, and what they can do to actually empower people to better
[00:32:49.840 --> 00:32:52.560]   understand this and take responsibility for it.
[00:32:52.560 --> 00:32:54.000]   Well, it is so interesting.
[00:32:54.000 --> 00:32:59.640]   I think what you said is such a great summary of what we're dealing with here, and the analogy
[00:32:59.640 --> 00:33:03.200]   of data as the new oil is a really compelling one.
[00:33:03.200 --> 00:33:06.800]   And what's fascinating about that is this is the first time probably in human history
[00:33:06.800 --> 00:33:11.800]   when the oil, or the diamonds, or the gold, or whatever it is, is actually us.
[00:33:11.800 --> 00:33:13.640]   Yes, we're being mined.
[00:33:13.640 --> 00:33:15.040]   We're being mined.
[00:33:15.040 --> 00:33:17.120]   We're being mined for our money.
[00:33:17.120 --> 00:33:23.320]   It's basically an outgrowth of the advertising industry plus a lot more on top of that.
[00:33:23.320 --> 00:33:27.120]   And shouldn't we at least know when we're being mined?
[00:33:27.120 --> 00:33:28.120]   Yes.
[00:33:28.120 --> 00:33:32.400]   And when we're offering up bits of ourselves to somebody who's going to then turn around
[00:33:32.400 --> 00:33:37.560]   and sell to us, it's almost like we're giving something away and then being asked to buy
[00:33:37.560 --> 00:33:40.920]   it back, the purchases that we make.
[00:33:40.920 --> 00:33:46.840]   I have to say though that on the face of it, the idea that Facebook would work with Maryland
[00:33:46.840 --> 00:33:53.080]   legislators to try to craft something that is both helpful and something that Facebook
[00:33:53.080 --> 00:33:56.320]   could live with is not necessarily a bad thing.
[00:33:56.320 --> 00:33:58.400]   That's how politics works, Amy.
[00:33:58.400 --> 00:33:59.400]   You're trying to reach consensus.
[00:33:59.400 --> 00:34:00.400]   Right.
[00:34:00.400 --> 00:34:01.400]   And it's not an unprecedented, right?
[00:34:01.400 --> 00:34:03.360]   That is what lobbying is.
[00:34:03.360 --> 00:34:04.360]   Yes.
[00:34:04.360 --> 00:34:12.360]   So I think that my job is to collect data and to model it and to figure out what all of
[00:34:12.360 --> 00:34:17.000]   it means and what the next order, risk, and opportunity is many, many years into the future.
[00:34:17.000 --> 00:34:20.920]   But one of the early signals that we look for are contradictions.
[00:34:20.920 --> 00:34:23.560]   So to me, it was very...
[00:34:23.560 --> 00:34:26.880]   These were some weak signals that I was paying attention to over the past couple of weeks.
[00:34:26.880 --> 00:34:33.360]   The fact that quietly this legislation was being written, it potentially becomes the model.
[00:34:33.360 --> 00:34:39.000]   It eludes some regulatory challenges right around the same time that it becomes apparent
[00:34:39.000 --> 00:34:44.160]   that our data is being continually mined, refined, and productized.
[00:34:44.160 --> 00:34:48.680]   But in ways that we may not have thought through or intended, all this sort of constellation
[00:34:48.680 --> 00:34:55.880]   of interesting facts and data over the past couple of weeks, I think points us in a direction
[00:34:55.880 --> 00:34:58.080]   that we should be concerned about.
[00:34:58.080 --> 00:35:02.360]   But it's hard to be concerned and to really think through things when the spectacle is
[00:35:02.360 --> 00:35:04.680]   so fascinating and fun.
[00:35:04.680 --> 00:35:07.640]   I mean, it was fun to watch all of this happening, right?
[00:35:07.640 --> 00:35:08.640]   Yeah.
[00:35:08.640 --> 00:35:11.240]   I mean, for everybody though, I think it was fun for us.
[00:35:11.240 --> 00:35:12.680]   It looked like it was fun for Mark.
[00:35:12.680 --> 00:35:17.440]   I think that most people, if you ask them, like, "Did you see what happened this week?"
[00:35:17.440 --> 00:35:19.240]   They'd be like, "Oh yeah, Mark Zuckerberg.
[00:35:19.240 --> 00:35:21.000]   He looks like data from Star Trek."
[00:35:21.000 --> 00:35:22.000]   Yeah, yeah.
[00:35:22.000 --> 00:35:23.000]   That's it.
[00:35:23.000 --> 00:35:24.000]   That was it.
[00:35:24.000 --> 00:35:25.000]   That's what most people got out of that.
[00:35:25.000 --> 00:35:26.000]   Mark Zuckerberg is boring.
[00:35:26.000 --> 00:35:31.120]   You think Amy that Mark walked away saying, "Yes, victory."
[00:35:31.120 --> 00:35:33.880]   I mean, I would have if I was him.
[00:35:33.880 --> 00:35:36.360]   He escaped any serious questions.
[00:35:36.360 --> 00:35:38.800]   Clearly, I mean, did you hear some of the questions?
[00:35:38.800 --> 00:35:44.520]   I think it was Senator Orrin Hatch who was trying to suss out whether or not Facebook
[00:35:44.520 --> 00:35:46.280]   and Twitter were kind of the same thing.
[00:35:46.280 --> 00:35:47.280]   I know.
[00:35:47.280 --> 00:35:48.280]   I think one of the questions-
[00:35:48.280 --> 00:35:49.280]   That was depressing.
[00:35:49.280 --> 00:35:53.080]   I might have him confused with Lindsey Graham, but somebody asked the question, "Is Twitter
[00:35:53.080 --> 00:35:54.360]   the same as Facebook?"
[00:35:54.360 --> 00:35:56.720]   And it's like, it wasn't even rhetorical.
[00:35:56.720 --> 00:35:57.720]   Yeah.
[00:35:57.720 --> 00:35:58.720]   Yeah.
[00:35:58.720 --> 00:36:00.520]   I mean, he escaped unscathed.
[00:36:00.520 --> 00:36:08.040]   If he had gone in front of people who wouldn't have had to ask, people were asking questions
[00:36:08.040 --> 00:36:13.920]   about whether or not even if you log out of Facebook, you're still being tracked.
[00:36:13.920 --> 00:36:17.040]   It's a clear indication that they don't understand cookies, right?
[00:36:17.040 --> 00:36:18.040]   Yeah.
[00:36:18.040 --> 00:36:22.760]   So if he had actually faced people that asked serious questions or if anybody who was up
[00:36:22.760 --> 00:36:27.920]   there that didn't understand the tech could have and would have pressed on the other sort
[00:36:27.920 --> 00:36:32.320]   of civil liberties piece of this, I think it would have been different.
[00:36:32.320 --> 00:36:37.200]   I think the platform was designed not to even allow that chatter somebody wanted to.
[00:36:37.200 --> 00:36:39.400]   And I observed this before it began.
[00:36:39.400 --> 00:36:44.120]   I said, "This is really just going to be a chance for a member of Congress to stand
[00:36:44.120 --> 00:36:46.360]   up, say something that they can use in an ad later."
[00:36:46.360 --> 00:36:48.840]   Because in the Senate, they got five minutes.
[00:36:48.840 --> 00:36:49.840]   Yeah.
[00:36:49.840 --> 00:36:50.840]   In the House, they got four minutes.
[00:36:50.840 --> 00:36:52.480]   There was no time for follow-up.
[00:36:52.480 --> 00:36:53.480]   Right.
[00:36:53.480 --> 00:36:59.760]   They deliberately used up the clock by doing longer answers, which many members of Congress
[00:36:59.760 --> 00:37:03.000]   were at pains to interrupt trying to get their second question in.
[00:37:03.000 --> 00:37:04.000]   Yeah.
[00:37:04.000 --> 00:37:05.000]   He even drank his water slowly.
[00:37:05.000 --> 00:37:06.000]   Yeah.
[00:37:06.000 --> 00:37:08.920]   Whoever designed it, designed it with Facebook in mind, frankly, wasn't...
[00:37:08.920 --> 00:37:09.920]   Yeah.
[00:37:09.920 --> 00:37:12.480]   So I don't know that the...
[00:37:12.480 --> 00:37:17.560]   I don't want to be Alex Jones here and a conspiracy theorist.
[00:37:17.560 --> 00:37:20.960]   I don't think it's conspiracy just to say that, you know...
[00:37:20.960 --> 00:37:23.160]   They're in each other's pockets.
[00:37:23.160 --> 00:37:24.640]   Well, they played the game.
[00:37:24.640 --> 00:37:27.120]   They literally relied on Facebook to get elected.
[00:37:27.120 --> 00:37:29.360]   I mean, that was a good...
[00:37:29.360 --> 00:37:31.000]   How much money did each one of those...
[00:37:31.000 --> 00:37:32.400]   There's a great story somewhere.
[00:37:32.400 --> 00:37:33.400]   Somebody catalog...
[00:37:33.400 --> 00:37:35.200]   They all got money from Mark.
[00:37:35.200 --> 00:37:39.120]   And not just the tangent, not the obvious money, but every one of them relied on Facebook
[00:37:39.120 --> 00:37:40.120]   to get elected.
[00:37:40.120 --> 00:37:41.120]   Every one of them.
[00:37:41.120 --> 00:37:45.240]   That's a larger issue that probably isn't for us.
[00:37:45.240 --> 00:37:46.880]   But there's the larger political issue.
[00:37:46.880 --> 00:37:48.920]   We talked about this on Wednesday.
[00:37:48.920 --> 00:37:49.920]   There's an economic theory.
[00:37:49.920 --> 00:37:52.120]   I think it's called the Peltzman Theory.
[00:37:52.120 --> 00:37:55.000]   There are three constituents here.
[00:37:55.000 --> 00:37:56.360]   There's Monopoly's big business.
[00:37:56.360 --> 00:38:00.560]   There's politicians and there's users.
[00:38:00.560 --> 00:38:03.080]   And they each have kind of differing goals.
[00:38:03.080 --> 00:38:04.480]   The politicians want to get reelected.
[00:38:04.480 --> 00:38:06.680]   The users want to pay less.
[00:38:06.680 --> 00:38:09.920]   Or in the case of Facebook where it's free, maybe...
[00:38:09.920 --> 00:38:12.440]   I don't know, get spied on less.
[00:38:12.440 --> 00:38:17.160]   And the big business is a minimal regulation as they can get to continue on with what they're
[00:38:17.160 --> 00:38:18.160]   doing.
[00:38:18.160 --> 00:38:21.920]   And what happens is you get this kind of nice little feedback loop and everybody gets
[00:38:21.920 --> 00:38:22.920]   kind of what they want.
[00:38:22.920 --> 00:38:25.120]   Nobody is 100% happy.
[00:38:25.120 --> 00:38:29.120]   But at the same time, we don't get any kind of...
[00:38:29.120 --> 00:38:33.480]   The voters really don't get any kind of privacy protection or regulation of these companies,
[00:38:33.480 --> 00:38:36.160]   because that's not what politicians are looking for.
[00:38:36.160 --> 00:38:38.600]   In fact, Facebook's very helpful to them.
[00:38:38.600 --> 00:38:39.920]   They're not trying to shut down Facebook.
[00:38:39.920 --> 00:38:40.920]   No, and there was some of the...
[00:38:40.920 --> 00:38:45.560]   I wish I could remember who wrote this piece, but there was a piece of documenting which
[00:38:45.560 --> 00:38:49.600]   politicians went up to Zuckerberg afterward and basically fawned over him.
[00:38:49.600 --> 00:38:50.600]   Yeah.
[00:38:50.600 --> 00:38:53.320]   So, we're proud of what you started in your dorm room.
[00:38:53.320 --> 00:38:54.960]   He's a kid maker.
[00:38:54.960 --> 00:38:57.640]   This is the symbol of American success.
[00:38:57.640 --> 00:39:02.160]   I don't think a lot of interest in actually angering or unsettling him outside of that
[00:39:02.160 --> 00:39:04.720]   grandstanding, for sure.
[00:39:04.720 --> 00:39:07.960]   But I think it would be interesting to talk for a second about whether or not people
[00:39:07.960 --> 00:39:11.920]   would have the appetite to pay for a service like Facebook.
[00:39:11.920 --> 00:39:12.920]   We were just talking about that.
[00:39:12.920 --> 00:39:13.920]   They brought that up.
[00:39:13.920 --> 00:39:14.920]   Journalism, they brought it up.
[00:39:14.920 --> 00:39:16.600]   People have been bringing it up for the whole week.
[00:39:16.600 --> 00:39:21.600]   They say, "What if we paid for it?
[00:39:21.600 --> 00:39:24.600]   Do you need to change your business model Facebook?"
[00:39:24.600 --> 00:39:28.600]   And I think this is, again, I keep coming back to this, but it's sort of a personal
[00:39:28.600 --> 00:39:29.600]   responsibility thing.
[00:39:29.600 --> 00:39:34.600]   I don't think that there's any actual appetite for paying for social media.
[00:39:34.600 --> 00:39:40.600]   As soon as any service starts to bring up a fee, people freak out and walk away, which
[00:39:40.600 --> 00:39:43.600]   is weird because we'll pay what?
[00:39:43.600 --> 00:39:45.600]   Kind of confounding.
[00:39:45.600 --> 00:39:47.480]   As much time as people spend on it.
[00:39:47.480 --> 00:39:49.480]   I agree completely.
[00:39:49.480 --> 00:39:52.600]   I never thought people would pay for the information either, that it would survive more than a
[00:39:52.600 --> 00:39:54.600]   year, but they'd eventually get targeted.
[00:39:54.600 --> 00:39:56.640]   Well, there's a key difference, though.
[00:39:56.640 --> 00:39:58.280]   There've been plenty of studies.
[00:39:58.280 --> 00:40:01.560]   The information was always a paid product.
[00:40:01.560 --> 00:40:09.200]   The challenge is there's a cognitive bias that happens when something was free and now
[00:40:09.200 --> 00:40:14.240]   you're being compelled for whatever reason to pay, that feels like something is being
[00:40:14.240 --> 00:40:15.480]   taken away from us.
[00:40:15.480 --> 00:40:19.200]   Historically speaking, regardless of what industry or field it is, we won't do it.
[00:40:19.200 --> 00:40:21.600]   It doesn't work sometimes.
[00:40:21.600 --> 00:40:24.120]   Think about with cell phones.
[00:40:24.120 --> 00:40:29.040]   We completely destroyed the value of cell phones for about five or six years there.
[00:40:29.040 --> 00:40:34.240]   And then eventually we got to the point where now we're paying whatever, 40 bucks a month
[00:40:34.240 --> 00:40:38.200]   or 30 bucks a month or having to buy one separately.
[00:40:38.200 --> 00:40:40.840]   But it was a different type of phone.
[00:40:40.840 --> 00:40:43.280]   The free you're talking about the free model?
[00:40:43.280 --> 00:40:46.240]   Right, well, you get a free upgrade.
[00:40:46.240 --> 00:40:50.720]   You get $200 for your phone, which was massively subsidized.
[00:40:50.720 --> 00:40:53.320]   Now you're paying everybody's pay for the phone.
[00:40:53.320 --> 00:40:54.760]   In Japan, you would get the phones for free.
[00:40:54.760 --> 00:40:57.800]   You would get a free phone when I lived there.
[00:40:57.800 --> 00:40:59.760]   The early smartphones were free.
[00:40:59.760 --> 00:41:01.720]   You just signed up for the service.
[00:41:01.720 --> 00:41:11.400]   But once the truly smartphones hit, the first smartphone that I had was in i-mode from entity
[00:41:11.400 --> 00:41:13.760]   dogamole, which was a totally different kind of phone.
[00:41:13.760 --> 00:41:19.000]   Everything else was free, but you had to pay for that.
[00:41:19.000 --> 00:41:25.000]   We've shifted over to the fancy models that people pay for because there's never been
[00:41:25.000 --> 00:41:26.000]   an alternative.
[00:41:26.000 --> 00:41:28.760]   There's never been a way not to pay for a brand new iPhone.
[00:41:28.760 --> 00:41:30.800]   There's also this sort of half-paid model.
[00:41:30.800 --> 00:41:32.600]   I live in Berkeley.
[00:41:32.600 --> 00:41:35.840]   There's a regional news site called BerkeleySide, which I'm a huge fan of.
[00:41:35.840 --> 00:41:43.240]   They've done a fantastic job creating a serious local publication that's internet only.
[00:41:43.240 --> 00:41:44.280]   It's first in the nation.
[00:41:44.280 --> 00:41:47.680]   They just raised a million dollars from its readers in a direct public offering.
[00:41:47.680 --> 00:41:48.680]   Wow.
[00:41:48.680 --> 00:41:49.680]   Interesting.
[00:41:49.680 --> 00:41:50.680]   Small local news.
[00:41:50.680 --> 00:41:51.680]   Isn't that interesting?
[00:41:51.680 --> 00:41:52.680]   High quality products.
[00:41:52.680 --> 00:41:53.680]   That's Berkeley, though.
[00:41:53.680 --> 00:41:54.680]   Totally free.
[00:41:54.680 --> 00:41:55.680]   That's still.
[00:41:55.680 --> 00:41:56.680]   No, it's Berkeley.
[00:41:56.680 --> 00:41:57.680]   There's something to be said, though.
[00:41:57.680 --> 00:42:01.440]   And then you say, "Look, it's kind of the NPR model, too."
[00:42:01.440 --> 00:42:03.640]   Like, "Please pay for this thing.
[00:42:03.640 --> 00:42:07.160]   We're going to give it to you for free, but we're going to use guilt."
[00:42:07.160 --> 00:42:08.160]   Guilt works.
[00:42:08.160 --> 00:42:10.040]   I mean, guilt absolutely works.
[00:42:10.040 --> 00:42:11.240]   It's not democratic, though.
[00:42:11.240 --> 00:42:13.040]   It's fundamentally undemocratic.
[00:42:13.040 --> 00:42:19.040]   That's part of the problem I have is that news especially, I'm kind of the opinion software.
[00:42:19.040 --> 00:42:20.200]   All software should be free.
[00:42:20.200 --> 00:42:22.680]   The paid software model is a broken model.
[00:42:22.680 --> 00:42:24.760]   And certainly, social networks should all be free.
[00:42:24.760 --> 00:42:26.160]   And news should be free.
[00:42:26.160 --> 00:42:29.720]   You've got to find other ways to monetize it because it's undemocratic to say that only
[00:42:29.720 --> 00:42:34.680]   people who can afford news should be able to get the good news and the rest of you are
[00:42:34.680 --> 00:42:37.960]   going to get fake news and crappy news and link-baity news.
[00:42:37.960 --> 00:42:38.960]   That's not right.
[00:42:38.960 --> 00:42:42.360]   And yet, when Freedom of the Press started, everybody had to pay to get that cheat.
[00:42:42.360 --> 00:42:43.360]   The broadsheets.
[00:42:43.360 --> 00:42:44.360]   The broadsheets.
[00:42:44.360 --> 00:42:45.360]   Right?
[00:42:45.360 --> 00:42:46.360]   So, some of the reasons we've never charged for it.
[00:42:46.360 --> 00:42:47.920]   The whole person bought it.
[00:42:47.920 --> 00:42:54.280]   And then other people said they shared it, talked about it, whatever, shared it around.
[00:42:54.280 --> 00:42:57.080]   So, I think that it's kind of HBO model.
[00:42:57.080 --> 00:43:01.680]   Like we know people steal it, but some of you are going to pay for it and it all works
[00:43:01.680 --> 00:43:02.680]   out in the end.
[00:43:02.680 --> 00:43:08.760]   The LA Times accidentally ran a new business model experiment without even realizing it.
[00:43:08.760 --> 00:43:13.680]   A couple weeks ago, their page loads times were like really, really slow and people couldn't
[00:43:13.680 --> 00:43:15.480]   fare what was going on.
[00:43:15.480 --> 00:43:20.600]   And a friend of mine was poking around and somebody was using their site to mine for
[00:43:20.600 --> 00:43:21.600]   bitcoins.
[00:43:21.600 --> 00:43:22.600]   Oh my God.
[00:43:22.600 --> 00:43:25.400]   So, they had been hacked in a weird way.
[00:43:25.400 --> 00:43:29.240]   But also maybe like a genius way to raise some cash.
[00:43:29.240 --> 00:43:30.240]   Yeah.
[00:43:30.240 --> 00:43:32.520]   Just thought for the LA Times, unfortunately.
[00:43:32.520 --> 00:43:40.520]   And to close the loop on the Facebook thing, I do think Facebook is primarily over and under
[00:43:40.520 --> 00:43:46.360]   30, there's hardly any people that are actively engaged on Facebook.
[00:43:46.360 --> 00:43:49.400]   So, what's next is the important question.
[00:43:49.400 --> 00:43:56.640]   But for the hearings themselves, I think that you have to take what happened not just in
[00:43:56.640 --> 00:44:02.720]   the event itself, which was somewhat sad and disappointing and not what it could have
[00:44:02.720 --> 00:44:04.400]   been or should have been.
[00:44:04.400 --> 00:44:07.400]   But think of it as a first step or maybe second step, right?
[00:44:07.400 --> 00:44:14.560]   Because they did call after the election a few of these tech heads before the Congress
[00:44:14.560 --> 00:44:16.120]   to testify.
[00:44:16.120 --> 00:44:23.280]   Again, it's about the scrutiny coming on to these folks about the way that they use data.
[00:44:23.280 --> 00:44:26.880]   And while this wasn't ideal, it wasn't what it could have been or should have been.
[00:44:26.880 --> 00:44:31.520]   It is a start and you're going to increasingly in the coming years see more pressure on them
[00:44:31.520 --> 00:44:37.560]   to be transparent and the Maryland legislation I think is another step in that direction.
[00:44:37.560 --> 00:44:41.640]   They're going to be more transparent about what data they take, how they use it, how
[00:44:41.640 --> 00:44:48.280]   much it's worth and then what they can do to give people control of their own data.
[00:44:48.280 --> 00:44:52.280]   And somebody in the chat room brought this up and maybe not exactly this way, but also
[00:44:52.280 --> 00:44:59.000]   there's the possibility that a transparency here, I'm going to sell your data to marketers.
[00:44:59.000 --> 00:45:02.400]   You will see ads related to the things that you do on this platform.
[00:45:02.400 --> 00:45:06.880]   If you would like to not receive these ads, it will cost you $10 a month to use the platform.
[00:45:06.880 --> 00:45:09.280]   Yeah, I bet most people say fine.
[00:45:09.280 --> 00:45:10.280]   I don't want to pay it.
[00:45:10.280 --> 00:45:11.280]   Yeah.
[00:45:11.280 --> 00:45:12.280]   Am I wrong?
[00:45:12.280 --> 00:45:16.080]   I have a problem with Facebook using my information to sell ads.
[00:45:16.080 --> 00:45:17.760]   That's not where the issue is.
[00:45:17.760 --> 00:45:24.120]   The issue is more using my mood to take advantage of me to sell ads or selling my information
[00:45:24.120 --> 00:45:30.240]   to a foreign government or using my information in kind of sleazier ways than just straight
[00:45:30.240 --> 00:45:33.520]   up you talked about jeeps, you should see jeep ads.
[00:45:33.520 --> 00:45:35.160]   What would be wrong with that?
[00:45:35.160 --> 00:45:36.560]   You're actually interested in jeeps.
[00:45:36.560 --> 00:45:38.720]   They're taking more data than what they're telling us.
[00:45:38.720 --> 00:45:43.600]   They're taking more data and they're using it in ways that are kind of not so nice.
[00:45:43.600 --> 00:45:47.880]   Well, and we're talking about what's, we're talking about what's already happened.
[00:45:47.880 --> 00:45:53.480]   I mean, you know, Facebook saying if at the end of all of this, the end result is we'll
[00:45:53.480 --> 00:45:59.680]   tell you, you know, we'll give you the opportunity to opt out so that we won't use your data
[00:45:59.680 --> 00:46:06.040]   to surface ads, sort of like not that that sort of shields us from everything that's coming.
[00:46:06.040 --> 00:46:10.480]   You know, the next social networks are going to be part of mixed reality, not text and
[00:46:10.480 --> 00:46:13.840]   photo and video based social networks that we have right now.
[00:46:13.840 --> 00:46:17.720]   Now's the time that we get to work on social networks that don't rely on surveillance
[00:46:17.720 --> 00:46:19.120]   capitalism to survive.
[00:46:19.120 --> 00:46:24.560]   There must be some other way to create the next thing that doesn't require this kind
[00:46:24.560 --> 00:46:27.720]   of skeezy invasion of privacy or is that not?
[00:46:27.720 --> 00:46:28.720]   Quit book.
[00:46:28.720 --> 00:46:29.720]   Quit book.
[00:46:29.720 --> 00:46:30.720]   I like it.
[00:46:30.720 --> 00:46:33.360]   Let me, let me take a little break.
[00:46:33.360 --> 00:46:35.400]   I am so thrilled that you guys are here.
[00:46:35.400 --> 00:46:36.400]   Great, smart people.
[00:46:36.400 --> 00:46:43.240]   Amy Webb, the signals are talking the future today Institute find out more at Amy Webb.io
[00:46:43.240 --> 00:46:49.240]   w e b b dot I O. Lindsay Turntine editor in chief had seen that.
[00:46:49.240 --> 00:46:51.880]   I mean, that's power.
[00:46:51.880 --> 00:46:53.440]   That's power.
[00:46:53.440 --> 00:46:57.200]   She is also at L Turntine on Twitter.
[00:46:57.200 --> 00:47:03.640]   And of course, Jason Heiner from the tech republic and CBS both CBS interactive and ziti net.
[00:47:03.640 --> 00:47:04.640]   Thank you all for being here.
[00:47:04.640 --> 00:47:06.960]   I want to introduce a new sponsor.
[00:47:06.960 --> 00:47:12.760]   Not to this show, but not new to people who've listened to me talk about this app for months
[00:47:12.760 --> 00:47:14.240]   because I love it.
[00:47:14.240 --> 00:47:16.440]   It's the front page of my phone.
[00:47:16.440 --> 00:47:24.560]   It's called apptiv a A A P T I V and it is a trainer in your phone.
[00:47:24.560 --> 00:47:26.240]   Tiv is audio based workouts.
[00:47:26.240 --> 00:47:31.880]   You put them in your in your ears with created by certified personal trainers.
[00:47:31.880 --> 00:47:33.960]   You get it on a mobile app.
[00:47:33.960 --> 00:47:35.440]   I love the trainers.
[00:47:35.440 --> 00:47:38.760]   There's a huge variety of trainings and trainers and a huge variety of training.
[00:47:38.760 --> 00:47:42.920]   Everything from riding a bike, taking a walk, getting on a treadmill, lifting weights.
[00:47:42.920 --> 00:47:44.760]   I have a medicine ball training.
[00:47:44.760 --> 00:47:45.760]   I do on this thing.
[00:47:45.760 --> 00:47:48.320]   I just love great music.
[00:47:48.320 --> 00:47:50.920]   Really good trainers for all personalities.
[00:47:50.920 --> 00:47:54.720]   I have a certain kind of trainer I like to use.
[00:47:54.720 --> 00:47:58.000]   Somebody who kind of kicks my butt a little bit and says work a little bit harder, but
[00:47:58.000 --> 00:47:59.520]   maybe you don't want that.
[00:47:59.520 --> 00:48:05.120]   There are more than 40 new classes every week in every kind of workout.
[00:48:05.120 --> 00:48:08.320]   If you've been getting on a stale climber and elliptical and just kind of doing it for
[00:48:08.320 --> 00:48:13.080]   20 minutes and not seeing a lot of results, what if you put a trainer in your ear or on
[00:48:13.080 --> 00:48:15.600]   your speakers that said, okay, now I want you to step it up.
[00:48:15.600 --> 00:48:17.160]   We're going to do some high intensity.
[00:48:17.160 --> 00:48:19.560]   Now slow it down.
[00:48:19.560 --> 00:48:23.160]   You can even get yoga and meditation in this.
[00:48:23.160 --> 00:48:27.680]   After trainers guide you in a flexible audio format, it is a great way to make fitness a
[00:48:27.680 --> 00:48:32.720]   lasting part of your routine and a lot less expensive than hiring a, trust me, I know
[00:48:32.720 --> 00:48:37.960]   that hiring a trainer or going to the gym classes for all fitness levels, beginning
[00:48:37.960 --> 00:48:42.440]   intermediate advanced, more than 2,500 workouts.
[00:48:42.440 --> 00:48:43.840]   You'll never get to the bottom of them.
[00:48:43.840 --> 00:48:48.360]   And as I said, there's always new workouts, many more than you could ever take.
[00:48:48.360 --> 00:48:51.320]   Training music, genre, every difficulty, every duration.
[00:48:51.320 --> 00:48:55.080]   And by the way, the active community is great.
[00:48:55.080 --> 00:48:57.560]   And they're working on their fitness goals and they support you.
[00:48:57.560 --> 00:48:59.080]   You support them.
[00:48:59.080 --> 00:49:00.240]   It's very affordable.
[00:49:00.240 --> 00:49:05.880]   I got the year membership a couple of months ago, but you can start at $14.99 a month, $100
[00:49:05.880 --> 00:49:09.680]   for an annual membership, and we've got a deal for you, 30% off.
[00:49:09.680 --> 00:49:13.320]   So if you've been looking at AFTiv, you can try it for free and I would do that.
[00:49:13.320 --> 00:49:22.480]   But 69.99, 30% off for a year of unlimited workouts, AFTiv.com/twit, AAPtiv.com/twit.
[00:49:22.480 --> 00:49:24.960]   I've been using this for several months now.
[00:49:24.960 --> 00:49:29.840]   And it is just a great, you know, as the weather turns and you can take walks, you can use
[00:49:29.840 --> 00:49:31.560]   it to get out there and run.
[00:49:31.560 --> 00:49:35.120]   They can walk you through a run or a walk.
[00:49:35.120 --> 00:49:37.760]   Everything you could ever imagine and you never have to repeat.
[00:49:37.760 --> 00:49:40.400]   You can always have a new workout every day.
[00:49:40.400 --> 00:49:41.400]   AFTiv.
[00:49:41.400 --> 00:49:42.400]   I love it.
[00:49:42.400 --> 00:49:43.400]   AFTiv.com/twit.
[00:49:43.400 --> 00:49:48.480]   I was really pleased because I've been giving them free plugs for months.
[00:49:48.480 --> 00:49:51.640]   I was really pleased when they said, "Hey, would you like to do an ad?"
[00:49:51.640 --> 00:49:55.440]   I said, "Yes, I'll be glad to."
[00:49:55.440 --> 00:50:00.480]   We're talking, it feels like we've been talking about Facebook for the last three months.
[00:50:00.480 --> 00:50:05.400]   It seems like also one of those conversations that doesn't have a logical end.
[00:50:05.400 --> 00:50:08.600]   Like I don't know what to say, what you could even say.
[00:50:08.600 --> 00:50:12.360]   We know we're not happy about it, but I don't think anything is ever going to happen.
[00:50:12.360 --> 00:50:13.360]   It's going to change.
[00:50:13.360 --> 00:50:17.280]   We just have to keep putting pressure on them to be more transparent.
[00:50:17.280 --> 00:50:18.280]   What about Twitter?
[00:50:18.280 --> 00:50:19.280]   What about Google?
[00:50:19.280 --> 00:50:20.280]   What about Amazon?
[00:50:20.280 --> 00:50:21.280]   What about Microsoft?
[00:50:21.280 --> 00:50:23.600]   Are they going to skate?
[00:50:23.600 --> 00:50:28.080]   And by the way, there's more than 300 companies gathering information just like Facebook.
[00:50:28.080 --> 00:50:29.560]   We don't even know about it.
[00:50:29.560 --> 00:50:32.040]   Not to mention, you're in a service provider.
[00:50:32.040 --> 00:50:34.120]   Well, but they don't use it in the same way.
[00:50:34.120 --> 00:50:35.120]   They're not smart.
[00:50:35.120 --> 00:50:37.360]   Oh, no, I don't take that back.
[00:50:37.360 --> 00:50:41.280]   Comcast and Verizon, AT&T have all sold information.
[00:50:41.280 --> 00:50:45.120]   They gained from you using their services to third parties.
[00:50:45.120 --> 00:50:52.200]   They also, as we know, are a revolving door for the NSA and spy agencies.
[00:50:52.200 --> 00:50:54.960]   They've climbed the hand over all that information.
[00:50:54.960 --> 00:50:59.560]   Thanks to a whistleblower, we know that the NSA had a room in San Francisco's AT&T headquarters
[00:50:59.560 --> 00:51:02.960]   where they monitored everything on the internet and on the phone system.
[00:51:02.960 --> 00:51:09.440]   I mean, I'd be more worried about them if they had, you know, five to, you know, 10,000
[00:51:09.440 --> 00:51:11.920]   engineers working on machine learning, but they don't.
[00:51:11.920 --> 00:51:12.920]   Well, that's true.
[00:51:12.920 --> 00:51:15.520]   Machine learning is a little scary because it's weaponizing this stuff.
[00:51:15.520 --> 00:51:16.520]   Yeah.
[00:51:16.520 --> 00:51:17.520]   And so...
[00:51:17.520 --> 00:51:18.520]   Who are you more afraid of?
[00:51:18.520 --> 00:51:19.520]   Government or industry, business?
[00:51:19.520 --> 00:51:20.520]   Oh, 100%.
[00:51:20.520 --> 00:51:21.520]   I mean, 100%.
[00:51:21.520 --> 00:51:22.520]   100%.
[00:51:22.520 --> 00:51:23.520]   They...
[00:51:23.520 --> 00:51:24.520]   100% both.
[00:51:24.520 --> 00:51:26.520]   Well, yeah, that's true.
[00:51:26.520 --> 00:51:27.520]   That's true.
[00:51:27.520 --> 00:51:29.520]   I'm scared.
[00:51:29.520 --> 00:51:30.520]   I'm more...
[00:51:30.520 --> 00:51:31.520]   It's more...
[00:51:31.520 --> 00:51:36.120]   The commercial is more scary right now, especially because they have...
[00:51:36.120 --> 00:51:38.520]   But see, I cannot use Facebook.
[00:51:38.520 --> 00:51:39.680]   I don't have to use Facebook.
[00:51:39.680 --> 00:51:40.680]   Right.
[00:51:40.680 --> 00:51:41.680]   True.
[00:51:41.680 --> 00:51:42.680]   I'm not moving out of the US.
[00:51:42.680 --> 00:51:46.400]   There are some people who really do have to use it, though, for their jobs or it is
[00:51:46.400 --> 00:51:47.920]   the primary way that they communicate.
[00:51:47.920 --> 00:51:48.920]   It's a free service.
[00:51:48.920 --> 00:51:50.920]   A lot of people now use Messenger specifically.
[00:51:50.920 --> 00:51:55.000]   Siva, by Janaas, I've said that it's white privilege that allows us to quit Facebook.
[00:51:55.000 --> 00:51:56.000]   It's some kind of privilege.
[00:51:56.000 --> 00:51:58.200]   I love you going a little far, but...
[00:51:58.200 --> 00:52:04.240]   Well, there are plenty of places around the world where, for various reasons, you know,
[00:52:04.240 --> 00:52:06.120]   like the press is being undermined.
[00:52:06.120 --> 00:52:07.200]   It's not free.
[00:52:07.200 --> 00:52:12.520]   So Facebook is the one way that they're able to publish certain news.
[00:52:12.520 --> 00:52:13.520]   But here's what I...
[00:52:13.520 --> 00:52:18.200]   So, there are legitimate reasons why in some cases it works.
[00:52:18.200 --> 00:52:21.040]   But here's what I would say.
[00:52:21.040 --> 00:52:22.720]   Our entire future...
[00:52:22.720 --> 00:52:26.480]   If we are entering the third era of computing, and by the way, this is my entire next book
[00:52:26.480 --> 00:52:32.240]   which is coming out, is all about this, so we're entering the third era of computing.
[00:52:32.240 --> 00:52:33.240]   Let's give us a background.
[00:52:33.240 --> 00:52:34.240]   What's the first two?
[00:52:34.240 --> 00:52:38.160]   So, the first was tabulation, and the second was programmable systems.
[00:52:38.160 --> 00:52:43.560]   So, the third era is AI, which has been in some form of development for the past 150
[00:52:43.560 --> 00:52:44.560]   years.
[00:52:44.560 --> 00:52:53.120]   So, the challenge is that as we now move from artificial narrow intelligence into what
[00:52:53.120 --> 00:53:01.000]   comes next, all of the primary research and work that's being done is all being done in
[00:53:01.000 --> 00:53:02.040]   the commercial sector.
[00:53:02.040 --> 00:53:05.640]   And so, in the book, I identify the big nine.
[00:53:05.640 --> 00:53:10.160]   There are nine big companies that basically control our future on Earth and any other
[00:53:10.160 --> 00:53:13.560]   planet in the place that we may venture to.
[00:53:13.560 --> 00:53:15.000]   Three of those companies are in China.
[00:53:15.000 --> 00:53:16.880]   That's Tencent, Baidu, and Alibaba.
[00:53:16.880 --> 00:53:18.760]   And the other six are here in the United States.
[00:53:18.760 --> 00:53:24.600]   Google and Amazon are the primary ones that matter, followed by Microsoft, Facebook, IBM,
[00:53:24.600 --> 00:53:26.560]   and to a far lesser degree Apple.
[00:53:26.560 --> 00:53:28.480]   And what about Tesla?
[00:53:28.480 --> 00:53:29.480]   Seriously.
[00:53:29.480 --> 00:53:32.480]   And related Musk.
[00:53:32.480 --> 00:53:40.840]   Tesla doesn't have all of the primary and basic research that's being done.
[00:53:40.840 --> 00:53:42.840]   Tesla is not one of the primary ones.
[00:53:42.840 --> 00:53:45.840]   Way more than Tesla.
[00:53:45.840 --> 00:53:47.840]   Way more than Tesla.
[00:53:47.840 --> 00:53:49.000]   But here's the problem.
[00:53:49.000 --> 00:53:55.320]   The problem is that these companies have fiduciary responsibilities to their shareholders,
[00:53:55.320 --> 00:53:59.280]   and they are profit-driven organizations.
[00:53:59.280 --> 00:54:02.000]   And even in China, technically they're independent companies, but they are very much tied to the
[00:54:02.000 --> 00:54:03.000]   Chinese government.
[00:54:03.000 --> 00:54:07.200]   Well, that's the thing that I'm very aware of is the fact that you could say Baidu and
[00:54:07.200 --> 00:54:15.000]   Tencent, but especially as she consolidates power, there is a definite move towards, for
[00:54:15.000 --> 00:54:22.640]   instance, putting Communist Party committees in these businesses requiring investment of
[00:54:22.640 --> 00:54:23.960]   the Chinese government.
[00:54:23.960 --> 00:54:29.520]   They're basically, this is an article from Bloomberg, it's an opinion piece, but basically
[00:54:29.520 --> 00:54:34.960]   there's some evidence that they're effectively nationalizing the tech sector, and they certainly
[00:54:34.960 --> 00:54:38.040]   want to have a lot to say about artificial intelligence.
[00:54:38.040 --> 00:54:43.400]   And they are using it against their citizenry right now.
[00:54:43.400 --> 00:54:44.400]   Right.
[00:54:44.400 --> 00:54:51.960]   So, just as an example, so there are two sense time and Megvi are two enormous AI companies
[00:54:51.960 --> 00:54:55.120]   that most people haven't heard of.
[00:54:55.120 --> 00:55:02.360]   The former has now just got a $4.5 billion valuation last week, making it the most highly
[00:55:02.360 --> 00:55:06.760]   valued AI tech startup in the entire world.
[00:55:06.760 --> 00:55:10.040]   The AI ecosystem is predicated on data, on people.
[00:55:10.040 --> 00:55:15.800]   And so, the reason that people say data is the new oil is partially because, like oil,
[00:55:15.800 --> 00:55:17.480]   it has to be mined, refined, and productized.
[00:55:17.480 --> 00:55:24.520]   But if you think about it in terms of us, we are the data, we are creating the data, then
[00:55:24.520 --> 00:55:29.680]   who's in charge of the largest natural resource going forward on Earth?
[00:55:29.680 --> 00:55:34.760]   It's China with 1.4 billion people and totally different ideas and restrictions around how
[00:55:34.760 --> 00:55:36.560]   their data is used.
[00:55:36.560 --> 00:55:43.720]   So, the needs and desires of these companies often run counter to the needs and desires
[00:55:43.720 --> 00:55:45.880]   and hopes of a democracy.
[00:55:45.880 --> 00:55:47.720]   Not just in China, but in the United States.
[00:55:47.720 --> 00:55:53.760]   And so, my argument is that we ought to change the developmental direction and track that
[00:55:53.760 --> 00:55:55.400]   AI is currently on.
[00:55:55.400 --> 00:56:00.360]   But that's going to force us all to make a lot of really difficult and possibly uncomfortable
[00:56:00.360 --> 00:56:01.600]   decisions.
[00:56:01.600 --> 00:56:08.160]   But if we wait 20 years, we're going to find that life is going to look really strange
[00:56:08.160 --> 00:56:10.800]   in ways that we don't like.
[00:56:10.800 --> 00:56:14.760]   And we have some agency and say in changing that developmental track now, we just have
[00:56:14.760 --> 00:56:17.320]   to decide we want a different future for ourselves.
[00:56:17.320 --> 00:56:21.000]   I would argue that life looks really strange in ways we don't like right now.
[00:56:21.000 --> 00:56:23.600]   We're just waking up to it.
[00:56:23.600 --> 00:56:27.520]   I just kind of get up and go, "What is this?
[00:56:27.520 --> 00:56:29.760]   This is so weird."
[00:56:29.760 --> 00:56:32.280]   But the other thing that I think needs to be mentioned, I think all of those points
[00:56:32.280 --> 00:56:35.360]   are so germane and smart.
[00:56:35.360 --> 00:56:40.400]   However, despite the size of the Chinese population, that value is only as good as
[00:56:40.400 --> 00:56:44.040]   the wealth held by the people who generate the data.
[00:56:44.040 --> 00:56:51.080]   Because presumably, the data is used to manipulate the way that we distribute our own wealth.
[00:56:51.080 --> 00:56:52.080]   That's why the data matters so much.
[00:56:52.080 --> 00:56:53.080]   The stuff we buy.
[00:56:53.080 --> 00:56:56.200]   And in some cases, right, are choices.
[00:56:56.200 --> 00:56:59.440]   And that's why we're here talking about it because of the elections.
[00:56:59.440 --> 00:57:01.040]   So there's also political power.
[00:57:01.040 --> 00:57:03.040]   In China, they're absolutely using this.
[00:57:03.040 --> 00:57:08.440]   They have the social score and they're keeping you from buying houses, taking trains.
[00:57:08.440 --> 00:57:09.440]   The social score.
[00:57:09.440 --> 00:57:14.840]   You're an expert on Asian cultural differences.
[00:57:14.840 --> 00:57:15.840]   That's a cultural difference.
[00:57:15.840 --> 00:57:20.120]   It seems something that would be very hard to do here in the States, but seems to be
[00:57:20.120 --> 00:57:22.480]   becoming rampant in China.
[00:57:22.480 --> 00:57:26.120]   Well, this is why you have to connect the broader dots.
[00:57:26.120 --> 00:57:30.600]   So all of this facial and posture, I mean, face recognition is interesting, but they're
[00:57:30.600 --> 00:57:33.080]   the world leaders in posture recognition.
[00:57:33.080 --> 00:57:35.880]   So it turns out everybody has a unique face print.
[00:57:35.880 --> 00:57:37.920]   We have a unique fingerprints.
[00:57:37.920 --> 00:57:42.880]   We also have unique face prints because the capillaries beneath our skin are all slightly
[00:57:42.880 --> 00:57:44.360]   different, right?
[00:57:44.360 --> 00:57:47.040]   We also have unique voice prints.
[00:57:47.040 --> 00:57:52.920]   So not just the particular intonation of our and cadence, right, when we speak, but all
[00:57:52.920 --> 00:58:00.960]   of the other elements that go into our voices, they're also leading the field in voice recognition
[00:58:00.960 --> 00:58:04.000]   technology and also posture and gesture.
[00:58:04.000 --> 00:58:08.960]   So you know, you've all this aimed at recognizing you with a camera.
[00:58:08.960 --> 00:58:11.520]   So yeah, that's right.
[00:58:11.520 --> 00:58:19.120]   So if look at all that, China has also been developing smart cameras, which are capable
[00:58:19.120 --> 00:58:26.880]   of identifying and locking onto somebody and then handing off, you've seen this in the
[00:58:26.880 --> 00:58:31.320]   Borne movies and like on CSI, but this is like a real thing where the cameras can lock
[00:58:31.320 --> 00:58:34.680]   on to somebody and follow them without a human in the loop, right?
[00:58:34.680 --> 00:58:39.000]   So you're not supposed to J-Walk in China.
[00:58:39.000 --> 00:58:40.840]   And if you do, that's frowned upon.
[00:58:40.840 --> 00:58:46.000]   Well, now they've used all of this technology that I just described to identify you and
[00:58:46.000 --> 00:58:51.880]   at intersections, they have electronic billboards and they will put your face up on the billboard
[00:58:51.880 --> 00:58:56.840]   and say, you know, Mr. Wong, just, you know, J-Walk to cross the street.
[00:58:56.840 --> 00:58:57.840]   Here's where he works.
[00:58:57.840 --> 00:59:00.720]   Here's, you know, here's his job.
[00:59:00.720 --> 00:59:08.120]   He'll broadcast it out on Weibo, which is the Chinese version of Facebook and make life
[00:59:08.120 --> 00:59:10.760]   generally pretty uncomfortable for that person.
[00:59:10.760 --> 00:59:12.640]   So it's not just about commerce.
[00:59:12.640 --> 00:59:13.640]   I guess is what I'm getting at.
[00:59:13.640 --> 00:59:14.640]   Yeah.
[00:59:14.640 --> 00:59:17.000]   Let me see if I can, and you can tell me if you agree with this.
[00:59:17.000 --> 00:59:18.560]   Cautify this a little bit.
[00:59:18.560 --> 00:59:23.720]   What Lindsey was talking about, which was using this information to manipulate people
[00:59:23.720 --> 00:59:28.400]   into participating in the economy more vigorously, spending more money.
[00:59:28.400 --> 00:59:31.960]   Or potentially things like elections and social choices.
[00:59:31.960 --> 00:59:33.960]   Or co-worsing them, making social choices.
[00:59:33.960 --> 00:59:38.040]   And then there's what governments like to do, partly social choices, but mostly what
[00:59:38.040 --> 00:59:42.000]   a government is really all about is maintaining order to maintain the status quo.
[00:59:42.000 --> 00:59:45.520]   I mean, let's be honest, that's all government has ever really been about, whether it's
[00:59:45.520 --> 00:59:52.760]   a king or a congress is maintaining the social order so that the people who got it can keep
[00:59:52.760 --> 00:59:53.760]   it.
[00:59:53.760 --> 00:59:57.320]   So, and both, you could have both of those.
[00:59:57.320 --> 01:00:00.520]   They're not mutually incompatible, as you point out, Lindsey.
[01:00:00.520 --> 01:00:06.160]   But from the point of view of an American citizen, they have very different import and
[01:00:06.160 --> 01:00:07.160]   means.
[01:00:07.160 --> 01:00:12.280]   Are there other ways, would you say that this is being used besides social control and
[01:00:12.280 --> 01:00:13.280]   economic purposes?
[01:00:13.280 --> 01:00:14.280]   Yeah.
[01:00:14.280 --> 01:00:19.480]   I think I would like to give a shout out to the mayor of Louisville who says that cities
[01:00:19.480 --> 01:00:21.880]   and governments are a platform for human potential.
[01:00:21.880 --> 01:00:22.880]   That's what they should be.
[01:00:22.880 --> 01:00:23.880]   See, that's interesting.
[01:00:23.880 --> 01:00:25.760]   I love that concept.
[01:00:25.760 --> 01:00:34.080]   They can foster the population to be more actualized, more efficient, more productive,
[01:00:34.080 --> 01:00:35.080]   happier.
[01:00:35.080 --> 01:00:36.080]   Yeah.
[01:00:36.080 --> 01:00:37.080]   That is a great vision.
[01:00:37.080 --> 01:00:38.080]   I love that.
[01:00:38.080 --> 01:00:39.080]   I love that.
[01:00:39.080 --> 01:00:40.080]   I love that possible.
[01:00:40.080 --> 01:00:42.080]   So, shout out to make fish that way.
[01:00:42.080 --> 01:00:43.080]   Or could go the other way.
[01:00:43.080 --> 01:00:44.080]   Yeah.
[01:00:44.080 --> 01:00:49.360]   Well, there is another piece of this, which is that, so if AI is the next era of computing,
[01:00:49.360 --> 01:00:50.920]   everything else is layered on top of that.
[01:00:50.920 --> 01:00:54.800]   So I would argue that AI is not the most important technology going forward.
[01:00:54.800 --> 01:00:59.600]   The biology is the most important technology platform of the whole 21st century.
[01:00:59.600 --> 01:01:02.160]   We're about to have it sold to no big revolution.
[01:01:02.160 --> 01:01:03.160]   Right.
[01:01:03.160 --> 01:01:07.360]   So that's why it's so important to look at all these different things and connect dots.
[01:01:07.360 --> 01:01:13.160]   Smart cities, Dubai and Saudi Arabia are both doing really interesting things to change
[01:01:13.160 --> 01:01:20.040]   the electric grid and to send power in new ways and optimize so that they're off traditional
[01:01:20.040 --> 01:01:22.640]   power sources, but that plugs into AI as well.
[01:01:22.640 --> 01:01:27.720]   So there's a vested interest in becoming the, you know, China is quickly becoming the global
[01:01:27.720 --> 01:01:32.840]   hegemon, which would mean that we're all much more reliant on China, not just for direct
[01:01:32.840 --> 01:01:38.240]   economic investment, but potentially for things like power or the soft robots that will live
[01:01:38.240 --> 01:01:40.040]   inside of our bodies.
[01:01:40.040 --> 01:01:41.680]   It may be this is a stereotype.
[01:01:41.680 --> 01:01:44.400]   Correct me if I'm completely mistaken.
[01:01:44.400 --> 01:01:47.760]   But my sense of Americans is we're rugged individualists.
[01:01:47.760 --> 01:01:51.360]   This nation was born on the notion of I'm going my own way.
[01:01:51.360 --> 01:01:53.480]   I'm going to believe what I want.
[01:01:53.480 --> 01:01:57.040]   But that billboard up on America, like so what don't tread on me.
[01:01:57.040 --> 01:01:58.040]   I cross the street.
[01:01:58.040 --> 01:01:59.040]   So yeah, so what?
[01:01:59.040 --> 01:02:01.800]   Well, I think that to be fair, that depends on who you are.
[01:02:01.800 --> 01:02:04.520]   That's like rugged individualism as long as you look like me.
[01:02:04.520 --> 01:02:05.520]   Yeah.
[01:02:05.520 --> 01:02:07.360]   Well, no, but that's part of it too.
[01:02:07.360 --> 01:02:08.360]   We have to face it.
[01:02:08.360 --> 01:02:10.080]   America is a country that was built on slavery.
[01:02:10.080 --> 01:02:14.480]   And so there is a kind of an endemic history of racism in this country that we have to
[01:02:14.480 --> 01:02:15.480]   own up to.
[01:02:15.480 --> 01:02:16.480]   Absolutely.
[01:02:16.480 --> 01:02:20.640]   Is this fundamental notion of the individual and individual rights?
[01:02:20.640 --> 01:02:25.920]   Whereas I, is this a complete stereotype, Amy, but I feel like China, because of confusion
[01:02:25.920 --> 01:02:31.240]   is in particularly, which really promoted the notion of society being the greater good.
[01:02:31.240 --> 01:02:34.480]   The individual is less important than society.
[01:02:34.480 --> 01:02:36.120]   Is that correct?
[01:02:36.120 --> 01:02:42.160]   That's that is very true in places like Japan and China and Korea.
[01:02:42.160 --> 01:02:45.600]   However, you know, it's 1.4 billion people in China.
[01:02:45.600 --> 01:02:48.240]   So not everybody's on the exact same page all the time about everything.
[01:02:48.240 --> 01:02:49.240]   Yeah.
[01:02:49.240 --> 01:02:51.080]   And not all Americans are rugged individualists.
[01:02:51.080 --> 01:02:56.920]   However, I think that the kinds of things we're talking about are anathema to rugged
[01:02:56.920 --> 01:03:05.040]   individualists and fit right into the worldview of a country where the society is paramount
[01:03:05.040 --> 01:03:06.040]   to the individual.
[01:03:06.040 --> 01:03:08.400]   I think that they would China we have to think of too.
[01:03:08.400 --> 01:03:10.200]   Do we want to live in a society like that?
[01:03:10.200 --> 01:03:11.200]   That's what I want to know.
[01:03:11.200 --> 01:03:12.200]   I don't.
[01:03:12.200 --> 01:03:15.400]   You know, China has this important work.
[01:03:15.400 --> 01:03:17.040]   They're doing an AI.
[01:03:17.040 --> 01:03:24.720]   Much of it is anathema to the values that that we have in the West in terms of privacy
[01:03:24.720 --> 01:03:31.120]   and expectations of data, security, all of these things.
[01:03:31.120 --> 01:03:38.040]   But we also have to remember that, you know, China is happy serving 1.4 billion people
[01:03:38.040 --> 01:03:41.760]   today and the companies there and the society.
[01:03:41.760 --> 01:03:46.400]   So they're happy having a certain myopic viewpoint.
[01:03:46.400 --> 01:03:48.400]   Collectivism has been very powerful for them.
[01:03:48.400 --> 01:03:49.400]   Yes.
[01:03:49.400 --> 01:03:56.320]   And that myopic viewpoint works for them, but it will get to a point where they will, even
[01:03:56.320 --> 01:04:01.720]   if it works perfectly, which I think you still have, there are still a lot of people to Amy's
[01:04:01.720 --> 01:04:09.040]   point in China who are dissenters to that point of view about, you know, the government having
[01:04:09.040 --> 01:04:10.040]   such a whole world.
[01:04:10.040 --> 01:04:11.040]   Which way is the world moving?
[01:04:11.040 --> 01:04:14.200]   Is it moving to collectivism or individualism?
[01:04:14.200 --> 01:04:19.200]   I mean, and what will these technologies do to the world?
[01:04:19.200 --> 01:04:23.320]   You know, I would, I would submit that the, the, the genomics revolution might move us
[01:04:23.320 --> 01:04:26.000]   more towards individualism.
[01:04:26.000 --> 01:04:29.680]   But that's going to really exacerbate the difference than the haves and the have nots.
[01:04:29.680 --> 01:04:37.360]   If I can, if I can with enough money, make sure my kids are smarter, faster, better than
[01:04:37.360 --> 01:04:38.360]   your kids.
[01:04:38.360 --> 01:04:39.840]   But that's happening in China.
[01:04:39.840 --> 01:04:40.840]   That's problematic.
[01:04:40.840 --> 01:04:43.080]   That's happening in China.
[01:04:43.080 --> 01:04:46.280]   I think that we're some, it's like elective collectivism.
[01:04:46.280 --> 01:04:47.280]   Oh boy.
[01:04:47.280 --> 01:04:52.120]   And it's, I mean, we get really wonky here, but I feel like there's a way in which we
[01:04:52.120 --> 01:04:57.360]   are moving beyond the bounds of typical government government because we're starting to build
[01:04:57.360 --> 01:04:58.520]   our own communities online.
[01:04:58.520 --> 01:05:00.000]   We need to save the planet.
[01:05:00.000 --> 01:05:03.760]   That may end up being so important that we have to abandon individualism.
[01:05:03.760 --> 01:05:09.840]   Well, and it could be that we take our individual cultures, apply them across regional boundaries
[01:05:09.840 --> 01:05:10.840]   and physical boundaries.
[01:05:10.840 --> 01:05:12.640]   There's been a lot of sci-fi about this, right?
[01:05:12.640 --> 01:05:13.640]   Yeah.
[01:05:13.640 --> 01:05:17.440]   This idea that we start to form nations that have to do more with our ideologies and our
[01:05:17.440 --> 01:05:21.480]   interests than they have to do with our governments.
[01:05:21.480 --> 01:05:26.640]   And to a certain degree, social media is kind of allowing us to do that and to block out
[01:05:26.640 --> 01:05:31.680]   the ideas and interests of anyone else physically around us, which I think is super weird.
[01:05:31.680 --> 01:05:35.080]   It's collectivism as long as we don't pay attention to our neighbors.
[01:05:35.080 --> 01:05:37.880]   I mean, the East and West are becoming more and more integrated, right?
[01:05:37.880 --> 01:05:45.840]   So the individualism and collectivism are going to meet and find a balance in there naturally,
[01:05:45.840 --> 01:05:50.360]   just by the fact that the world itself is becoming so much smaller.
[01:05:50.360 --> 01:05:52.520]   It's becoming so much more integrated.
[01:05:52.520 --> 01:05:58.120]   It's becoming so influenced by one another.
[01:05:58.120 --> 01:06:02.440]   And that's what I was getting to with China is while China is doing certain things with
[01:06:02.440 --> 01:06:08.400]   data today, eventually those companies are going to want to become global companies and
[01:06:08.400 --> 01:06:14.640]   those standards and those ways of dealing with people and their users are not going to
[01:06:14.640 --> 01:06:17.560]   be acceptable globally.
[01:06:17.560 --> 01:06:18.560]   Which companies?
[01:06:18.560 --> 01:06:20.200]   Are you talking about Tencent, Alibaba and Baidu?
[01:06:20.200 --> 01:06:21.200]   Yes.
[01:06:21.200 --> 01:06:22.200]   Yes.
[01:06:22.200 --> 01:06:23.200]   They are global companies.
[01:06:23.200 --> 01:06:24.200]   Well, they are.
[01:06:24.200 --> 01:06:25.200]   But...
[01:06:25.200 --> 01:06:26.200]   Well, are they?
[01:06:26.200 --> 01:06:28.200]   I mean, not in the same way that Google...
[01:06:28.200 --> 01:06:29.200]   Google's not global.
[01:06:29.200 --> 01:06:30.200]   Google's not...
[01:06:30.200 --> 01:06:32.440]   By revenue, by numbers?
[01:06:32.440 --> 01:06:34.440]   Absolutely not.
[01:06:34.440 --> 01:06:37.880]   By revenue and by numbers, those three companies are...
[01:06:37.880 --> 01:06:38.880]   Alibaba is Amazon.
[01:06:38.880 --> 01:06:42.600]   Yeah, but it doesn't have a presence in the West or doesn't.
[01:06:42.600 --> 01:06:44.120]   Do they need to have...
[01:06:44.120 --> 01:06:51.080]   Well, okay, so then the question has to do with how we are valuing these organizations.
[01:06:51.080 --> 01:06:57.440]   If these organizations are making as much or more in revenue and if they have as much
[01:06:57.440 --> 01:07:03.800]   or more influence over their designated population, then what does it matter that they're not
[01:07:03.800 --> 01:07:05.960]   in every single country around the world?
[01:07:05.960 --> 01:07:06.960]   It doesn't matter.
[01:07:06.960 --> 01:07:08.200]   What I'm saying, eventually they'll want to be.
[01:07:08.200 --> 01:07:13.560]   You know, when I was there and I talked to them, I asked this question across the board
[01:07:13.560 --> 01:07:16.800]   because they have products, they have very compelling things they've done.
[01:07:16.800 --> 01:07:23.680]   They are innovating very quickly in a number of ways and I asked them, you know, in each
[01:07:23.680 --> 01:07:28.920]   one and got a very similar answer, the question of, you know, when are you going to bring this
[01:07:28.920 --> 01:07:29.920]   to Europe?
[01:07:29.920 --> 01:07:34.680]   What are you going to do in the US and North America?
[01:07:34.680 --> 01:07:40.200]   And the question, the answer, each case was, right now we're really focused on China.
[01:07:40.200 --> 01:07:41.200]   Other than Huawei.
[01:07:41.200 --> 01:07:44.120]   Huawei is actually...
[01:07:44.120 --> 01:07:46.120]   And they've been at this longer as a global company.
[01:07:46.120 --> 01:07:48.520]   It's the same thing though with Amazon, right?
[01:07:48.520 --> 01:07:51.480]   Amazon's really focused on the US to somewhat lesser...
[01:07:51.480 --> 01:07:52.680]   A little bit to Europe.
[01:07:52.680 --> 01:07:56.480]   Sorry, but I know Alibaba wants to be here so badly.
[01:07:56.480 --> 01:07:59.520]   And I'm sure Jeff Bezos would love to be in China.
[01:07:59.520 --> 01:08:00.520]   And we'll figure it out.
[01:08:00.520 --> 01:08:01.520]   I actually think it's more...
[01:08:01.520 --> 01:08:06.080]   I personally think it's more likely that Alibaba will end up here than Amazon will end up in
[01:08:06.080 --> 01:08:07.840]   China for regulatory reasons.
[01:08:07.840 --> 01:08:08.840]   Right.
[01:08:08.840 --> 01:08:09.840]   I don't know.
[01:08:09.840 --> 01:08:10.840]   I...
[01:08:10.840 --> 01:08:14.480]   Given the tension right now, I have a feeling that they'll get hit with regulation because
[01:08:14.480 --> 01:08:18.920]   the US government will be concerned about IP bleed and our personal data bleed.
[01:08:18.920 --> 01:08:28.000]   I mean, I would say that we would have a national security interest in allowing Alibaba to
[01:08:28.000 --> 01:08:29.640]   gain a foothold in the United States.
[01:08:29.640 --> 01:08:35.160]   That would take some debate and discussion from a national security standpoint.
[01:08:35.160 --> 01:08:39.960]   It's really unfortunate because currently this will change, but we don't have an administration
[01:08:39.960 --> 01:08:43.880]   right now that shows any clear long-term vision.
[01:08:43.880 --> 01:08:47.840]   And you could just as easily see President Trump saying, "Yes, screw Jeff Bezos.
[01:08:47.840 --> 01:08:49.440]   Come on in, Alibaba."
[01:08:49.440 --> 01:08:52.240]   I mean, it could go any way.
[01:08:52.240 --> 01:08:53.240]   It could go any way.
[01:08:53.240 --> 01:08:54.240]   Who knows what's going on?
[01:08:54.240 --> 01:08:55.680]   You can change entirely in three years.
[01:08:55.680 --> 01:09:00.280]   But I can imagine also a time when we have almost a clash between the East and West where
[01:09:00.280 --> 01:09:05.760]   you've got these economic interests centered in Asia and centered in the West and they
[01:09:05.760 --> 01:09:08.120]   both want the other.
[01:09:08.120 --> 01:09:10.800]   And there's a massive clash between them.
[01:09:10.800 --> 01:09:11.800]   I think it's...
[01:09:11.800 --> 01:09:12.960]   A new kind of economic warfare on the West.
[01:09:12.960 --> 01:09:15.960]   I think it's less likely because there's so much more in the Navy.
[01:09:15.960 --> 01:09:17.720]   Do you think that's going to happen?
[01:09:17.720 --> 01:09:21.640]   Well, that would be the thing that I would be concerned about.
[01:09:21.640 --> 01:09:30.040]   And when Trump was going on and on, egging on China, not only was that a stupid economic
[01:09:30.040 --> 01:09:37.880]   arguments, but just displayed this deep cultural insensitivity, you can't challenge a Chinese
[01:09:37.880 --> 01:09:41.520]   man to a duel and think he's not going to show up.
[01:09:41.520 --> 01:09:43.240]   That's not the way it works.
[01:09:43.240 --> 01:09:46.280]   So absolutely, the wars of the future...
[01:09:46.280 --> 01:09:50.200]   And I can't say what I was going to say, so I won't.
[01:09:50.200 --> 01:09:52.880]   I'm starting some of the work that I do, which is...
[01:09:52.880 --> 01:09:53.880]   Secret.
[01:09:53.880 --> 01:09:54.880]   Secret.
[01:09:54.880 --> 01:09:56.200]   But here's what I would say.
[01:09:56.200 --> 01:10:04.200]   We think, our reference point for war is always what we've always seen in the past or what
[01:10:04.200 --> 01:10:05.840]   we've seen in movies.
[01:10:05.840 --> 01:10:09.880]   It's hard for us to conceive of something as being realistic or plausible when we've never
[01:10:09.880 --> 01:10:11.400]   seen it before.
[01:10:11.400 --> 01:10:18.680]   My models and my work would show that the wars that we're potentially headed to in the future
[01:10:18.680 --> 01:10:23.120]   have different contours and aspects of them that are more economic in nature but could
[01:10:23.120 --> 01:10:25.880]   be very disastrous.
[01:10:25.880 --> 01:10:31.560]   There is though the argument to be made that free trade is good for peace, that when China
[01:10:31.560 --> 01:10:35.600]   relies on us for its economic well-being and we rely on China for our economic well-being,
[01:10:35.600 --> 01:10:37.120]   they're much less likely to wage...
[01:10:37.120 --> 01:10:38.120]   Right, there's stability.
[01:10:38.120 --> 01:10:39.120]   More stability.
[01:10:39.120 --> 01:10:40.120]   That's right.
[01:10:40.120 --> 01:10:47.360]   First, Trump standing up to China in trade is not...
[01:10:47.360 --> 01:10:52.560]   He has some good points because we let every country in the world dump cheap goods here
[01:10:52.560 --> 01:10:58.640]   and all of them are very strict about what goods they allow to go in.
[01:10:58.640 --> 01:11:01.280]   We're all about free trade.
[01:11:01.280 --> 01:11:06.800]   Everybody else is very circumspect about free trades, which means it's not really free
[01:11:06.800 --> 01:11:07.800]   trade.
[01:11:07.800 --> 01:11:15.480]   The problem was, you do the embargo to make a point and then China's response was very
[01:11:15.480 --> 01:11:16.480]   measured.
[01:11:16.480 --> 01:11:19.880]   It was like, "Look, we're not going to make a big deal about this.
[01:11:19.880 --> 01:11:22.320]   They follow international laws.
[01:11:22.320 --> 01:11:24.200]   They're like, "You did 50 billion.
[01:11:24.200 --> 01:11:25.680]   We're going to do 50 billion."
[01:11:25.680 --> 01:11:28.880]   Then Trump comes back and says, "Well, we're going to do 100 million."
[01:11:28.880 --> 01:11:29.880]   So bear.
[01:11:29.880 --> 01:11:30.880]   You just played into his hand.
[01:11:30.880 --> 01:11:33.600]   He let you look like the bad guy.
[01:11:33.600 --> 01:11:37.880]   Now you made it even easier for you to be the bad guy and everybody else, the rest of the
[01:11:37.880 --> 01:11:41.080]   world to say, "You guys are the ones that are..."
[01:11:41.080 --> 01:11:46.040]   I hope that in years to come, there'll be a little bit more of a rational back and forth.
[01:11:46.040 --> 01:11:48.400]   I don't think right now we're in a rational period.
[01:11:48.400 --> 01:11:49.800]   So I'm hoping that at some point...
[01:11:49.800 --> 01:11:50.800]   What?
[01:11:50.800 --> 01:11:51.800]   What?
[01:11:51.800 --> 01:11:52.800]   We're talking about at the very beginning.
[01:11:52.800 --> 01:11:56.800]   I wonder if it's possible for us to return to rationality because we've already been ment...
[01:11:56.800 --> 01:12:01.040]   Like, sort of conditioned for craziness.
[01:12:01.040 --> 01:12:02.560]   Can we go back in the other direction?
[01:12:02.560 --> 01:12:04.560]   That's what we were talking about at the very beginning of today.
[01:12:04.560 --> 01:12:05.560]   I think so.
[01:12:05.560 --> 01:12:06.560]   It's a pendulum swing, right?
[01:12:06.560 --> 01:12:07.560]   You're the futureist.
[01:12:07.560 --> 01:12:08.560]   I hope you're wrong.
[01:12:08.560 --> 01:12:11.000]   Society is always a pendulum.
[01:12:11.000 --> 01:12:12.960]   I was more just curious to see what you all think.
[01:12:12.960 --> 01:12:14.960]   Is it possible to swing into the direction?
[01:12:14.960 --> 01:12:15.960]   Yes, it is.
[01:12:15.960 --> 01:12:19.920]   It's absolutely possible because society is always a pendulum swing, right?
[01:12:19.920 --> 01:12:22.120]   Because humans have this grass is always greener mentality.
[01:12:22.120 --> 01:12:27.600]   In fact, I would go a step farther and to say that just as...
[01:12:27.600 --> 01:12:33.120]   That there will be a flight from irrationality, having experienced it will be the best lesson
[01:12:33.120 --> 01:12:34.120]   of all.
[01:12:34.120 --> 01:12:37.720]   And then instead of conditioning us to irrationality, we will flee from it.
[01:12:37.720 --> 01:12:38.720]   I...
[01:12:38.720 --> 01:12:39.720]   And we will seek hyper-rationality.
[01:12:39.720 --> 01:12:44.560]   I hope so, but I am becoming convinced over time that humans...
[01:12:44.560 --> 01:12:49.680]   We are simply irrational because we're emotional creatures and that's being played to and
[01:12:49.680 --> 01:12:55.360]   that we may circle back or we may be charmed by a leader who speaks more rationally because
[01:12:55.360 --> 01:12:57.040]   it seems like an alternative.
[01:12:57.040 --> 01:13:01.160]   But the United States in which that happens may look completely different.
[01:13:01.160 --> 01:13:05.000]   Sunday driver says, "We're in the cray cray zone and we're staying there.
[01:13:05.000 --> 01:13:06.920]   Let's take a break."
[01:13:06.920 --> 01:13:07.920]   I want to talk tech.
[01:13:07.920 --> 01:13:08.920]   We've got some tech stories.
[01:13:08.920 --> 01:13:09.920]   I'm sure we do.
[01:13:09.920 --> 01:13:11.800]   We'll find them in just a second.
[01:13:11.800 --> 01:13:13.640]   I love the deep philosophic stuff though.
[01:13:13.640 --> 01:13:14.640]   I really do.
[01:13:14.640 --> 01:13:16.480]   It's my favorite Twit panel ever.
[01:13:16.480 --> 01:13:17.960]   Well, I really love that.
[01:13:17.960 --> 01:13:22.240]   I hope we're not turning people off because it is kind of amorphous, but it is really...
[01:13:22.240 --> 01:13:25.680]   I think these are the things you might have to think about if you want to think about
[01:13:25.680 --> 01:13:27.160]   what our future is to look like.
[01:13:27.160 --> 01:13:32.200]   If you're going to think about, "Well, what do we do about destructive social media?"
[01:13:32.200 --> 01:13:35.880]   You got to think on that larger scale of what its impact is and what is it?
[01:13:35.880 --> 01:13:38.720]   More importantly, what is the world we want to live in?
[01:13:38.720 --> 01:13:41.400]   I guess that's what I was kind of bringing up.
[01:13:41.400 --> 01:13:42.720]   What is that world?
[01:13:42.720 --> 01:13:44.680]   Is it the world of the rugged individualist?
[01:13:44.680 --> 01:13:46.080]   Is it the world of collectivism?
[01:13:46.080 --> 01:13:48.080]   Is there some third way?
[01:13:48.080 --> 01:13:49.080]   What is the world we want to live in?
[01:13:49.080 --> 01:13:55.640]   I don't know if we have enough of that conversation in general in the body politic.
[01:13:55.640 --> 01:13:58.240]   I feel like that's an important conversation I have.
[01:13:58.240 --> 01:14:00.320]   I think it's hugely important.
[01:14:00.320 --> 01:14:04.040]   What I have noticed recently is that every once in a while, coming back to the original
[01:14:04.040 --> 01:14:09.880]   conversation about publishing, every once in a while, somebody on the CNET staff or CNET
[01:14:09.880 --> 01:14:15.080]   itself will tweet something or share something that is just positive.
[01:14:15.080 --> 01:14:17.360]   Here's a great piece of news.
[01:14:17.360 --> 01:14:19.520]   Well, you don't need that very much anymore to get that.
[01:14:19.520 --> 01:14:22.760]   I tell you how much appetite there is for that.
[01:14:22.760 --> 01:14:26.720]   There's groundswell of excitement around something that is positive.
[01:14:26.720 --> 01:14:28.480]   We get tired of that too though.
[01:14:28.480 --> 01:14:30.520]   If it's Pollyanna, it sure.
[01:14:30.520 --> 01:14:32.120]   If it bleeds, it leads.
[01:14:32.120 --> 01:14:35.200]   But I think that there is a good reminder for optimism.
[01:14:35.200 --> 01:14:38.680]   It's a good reminder though, especially now when it feels like the world is crumbling
[01:14:38.680 --> 01:14:43.840]   and life is just happening at us.
[01:14:43.840 --> 01:14:49.680]   We forget that we all have the capability and the possibility and the skill set and
[01:14:49.680 --> 01:14:52.120]   the tools to create the futures that we want.
[01:14:52.120 --> 01:14:55.280]   The future doesn't just show up fully formed.
[01:14:55.280 --> 01:15:01.440]   We are creating it in the present, but we have to be willing to make some short-term sacrifices
[01:15:01.440 --> 01:15:03.480]   for long-term gain.
[01:15:03.480 --> 01:15:04.480]   That's the challenge.
[01:15:04.480 --> 01:15:08.680]   The challenge is that we tend to think in the now in the United States and we think in
[01:15:08.680 --> 01:15:15.720]   the now and sort of short-shrift ourselves in the future.
[01:15:15.720 --> 01:15:20.600]   I think it's a great idea for us all to be optimistic about the world that we're creating,
[01:15:20.600 --> 01:15:24.840]   but then we have to put the wheels in motion to make that optimistic future happen.
[01:15:24.840 --> 01:15:31.360]   What you said part of the present is I think we're living in the most affluent in general
[01:15:31.360 --> 01:15:36.200]   for not for everybody obviously, but in the most affluent times we've ever had that most
[01:15:36.200 --> 01:15:45.480]   people, even relatively poor people, are living better than people who have lived for millennia.
[01:15:45.480 --> 01:15:48.280]   Maybe the thing to acknowledge is how good we have it right now.
[01:15:48.280 --> 01:15:50.680]   Blue poverty is not making this...
[01:15:50.680 --> 01:15:52.920]   Violence all time long.
[01:15:52.920 --> 01:15:59.320]   This is the book I've been reading, The Angels of Our Better Nature.
[01:15:59.320 --> 01:16:02.360]   Angels of Our Better Nature is Bill Gates' favorite book, right?
[01:16:02.360 --> 01:16:03.360]   Fascinating.
[01:16:03.360 --> 01:16:04.360]   It really is.
[01:16:04.360 --> 01:16:07.880]   It's a miraculous time we live in.
[01:16:07.880 --> 01:16:08.880]   It doesn't feel that way.
[01:16:08.880 --> 01:16:09.880]   It doesn't feel that way.
[01:16:09.880 --> 01:16:10.880]   We've got to make it work.
[01:16:10.880 --> 01:16:15.000]   We've got lots of unfinished work still, and we're more conscious about it.
[01:16:15.000 --> 01:16:17.960]   We see it because we know what good looks like.
[01:16:17.960 --> 01:16:21.440]   We're more conscious about the things that we see, and we see a lot of unfinished work
[01:16:21.440 --> 01:16:24.640]   that we have to do as a country, as a people, as a world.
[01:16:24.640 --> 01:16:32.120]   I think there's also a sense when you're at the top that it looks like a long way down.
[01:16:32.120 --> 01:16:33.120]   Sure.
[01:16:33.120 --> 01:16:40.560]   When things are going really well, then the most predominant emotion is fear that you're
[01:16:40.560 --> 01:16:41.560]   going to lose it.
[01:16:41.560 --> 01:16:42.560]   Sure.
[01:16:42.560 --> 01:16:45.760]   As humans respond to fear, that's why, if it bleeds, it leads.
[01:16:45.760 --> 01:16:48.080]   Has always been a truism for a long time.
[01:16:48.080 --> 01:16:53.840]   But also, I think that we are so much more exposed through social media to what is going
[01:16:53.840 --> 01:16:55.200]   wrong.
[01:16:55.200 --> 01:16:58.440]   Even though we tend to personally give our Rosie as divisions of our social media.
[01:16:58.440 --> 01:16:59.440]   It's funny, isn't it?
[01:16:59.440 --> 01:17:01.840]   Just everybody's, you know, my life is great.
[01:17:01.840 --> 01:17:06.760]   But we are also very eager to share what's happening that's wrong next to us.
[01:17:06.760 --> 01:17:09.080]   We know when something bad happens to a friend.
[01:17:09.080 --> 01:17:10.680]   We might know faster.
[01:17:10.680 --> 01:17:15.240]   Or even not friends, just anything bad happening anywhere in the world, we know it.
[01:17:15.240 --> 01:17:16.240]   Immediately.
[01:17:16.240 --> 01:17:19.440]   And sometimes they're very specific stories.
[01:17:19.440 --> 01:17:22.720]   I don't want to say small because nothing is small when something tragic is happening
[01:17:22.720 --> 01:17:24.200]   to you.
[01:17:24.200 --> 01:17:27.600]   But we learn about a lot of specific single stories that we might not have heard about
[01:17:27.600 --> 01:17:28.600]   before.
[01:17:28.600 --> 01:17:31.280]   In the future, you are in the past, you would have never seen that, right?
[01:17:31.280 --> 01:17:32.280]   You live in a little village?
[01:17:32.280 --> 01:17:36.760]   You never been conscious of the bad things that were happening in small ways to people.
[01:17:36.760 --> 01:17:37.760]   Sure.
[01:17:37.760 --> 01:17:38.960]   And who cares who's in the castle?
[01:17:38.960 --> 01:17:42.480]   What does that mean to me?
[01:17:42.480 --> 01:17:46.560]   As long as they don't raise my taxes.
[01:17:46.560 --> 01:17:49.840]   Amy Webb is here, the future ist.
[01:17:49.840 --> 01:17:51.800]   Her book The Signals are Talking.
[01:17:51.800 --> 01:17:53.280]   And what's the new book going to be called?
[01:17:53.280 --> 01:17:54.280]   What is it out?
[01:17:54.280 --> 01:17:55.280]   And what can I interview about it?
[01:17:55.280 --> 01:17:56.280]   You about it.
[01:17:56.280 --> 01:18:01.360]   We were like on title revision number three, but it's a manifesto about the future of AI
[01:18:01.360 --> 01:18:05.480]   for everyday people out spring of 2019.
[01:18:05.480 --> 01:18:06.480]   This much.
[01:18:06.480 --> 01:18:07.800]   Oh, I have to wait a whole year.
[01:18:07.800 --> 01:18:09.160]   This really is important.
[01:18:09.160 --> 01:18:10.640]   I'm glad you're writing it.
[01:18:10.640 --> 01:18:11.640]   Thanks.
[01:18:11.640 --> 01:18:14.520]   And it is a conversation we have a lot here on our show.
[01:18:14.520 --> 01:18:15.520]   Absolutely.
[01:18:15.520 --> 01:18:16.520]   Going to continue to have it.
[01:18:16.520 --> 01:18:18.520]   It's not going to get any less important.
[01:18:18.520 --> 01:18:21.640]   No, Jason Heiner, he's the editor in chief of Tech Republic.
[01:18:21.640 --> 01:18:26.080]   Lindsay Turntine is also here from CNET.com where she's editor in chief.
[01:18:26.080 --> 01:18:28.080]   I got the editors in chief here.
[01:18:28.080 --> 01:18:29.440]   Top brass.
[01:18:29.440 --> 01:18:33.080]   Our show today brought to you by, just take a, take a, smells good in here.
[01:18:33.080 --> 01:18:36.600]   It smells a little bit like cupcakes, but it smells, it smells good in here.
[01:18:36.600 --> 01:18:39.760]   That's because our air is pure.
[01:18:39.760 --> 01:18:46.880]   We are breathing filtered air through molecule, the world's first molecular air purifier.
[01:18:46.880 --> 01:18:50.240]   We got our first molecule at home because Lisa was getting stuffed up in headaches every
[01:18:50.240 --> 01:18:51.240]   morning from allergies.
[01:18:51.240 --> 01:18:52.280]   We live out in the country.
[01:18:52.280 --> 01:18:53.280]   We got a molecule.
[01:18:53.280 --> 01:18:54.280]   Headaches were gone.
[01:18:54.280 --> 01:19:00.360]   In fact, I know it's real and not an imaginary thing because we had, we were away one night
[01:19:00.360 --> 01:19:02.640]   and we had a relative staying in our house.
[01:19:02.640 --> 01:19:03.760]   He turned off the molecule.
[01:19:03.760 --> 01:19:05.200]   I don't know why.
[01:19:05.200 --> 01:19:06.920]   We got home the next night, Lisa got a headache.
[01:19:06.920 --> 01:19:08.320]   She said, "What's wrong?
[01:19:08.320 --> 01:19:09.640]   Is the molecule broken?"
[01:19:09.640 --> 01:19:10.640]   It was turned off.
[01:19:10.640 --> 01:19:11.640]   We turned it on.
[01:19:11.640 --> 01:19:12.640]   She's better again.
[01:19:12.640 --> 01:19:13.640]   It really works.
[01:19:13.640 --> 01:19:16.000]   Molecule is not a HEPA filter.
[01:19:16.000 --> 01:19:20.200]   That help of filter technology you've heard about has been used to clean your air since
[01:19:20.200 --> 01:19:21.200]   the 1940s.
[01:19:21.200 --> 01:19:24.640]   There haven't been any major innovations since until now.
[01:19:24.640 --> 01:19:32.240]   Molecule's Pico, PECO technology, it's photo electrochemical oxidation and it goes way beyond
[01:19:32.240 --> 01:19:37.120]   what a HEPA filter can do to capture and eliminate, not just allergens, but stuff a
[01:19:37.120 --> 01:19:40.520]   thousand times smaller than what a HEPA filter can capture.
[01:19:40.520 --> 01:19:46.520]   Mold, bacteria, viruses, VOCs, airborne chemicals.
[01:19:46.520 --> 01:19:51.680]   We actually got one in here because of paint fumes and then the smoke from the fires and
[01:19:51.680 --> 01:19:53.680]   then tar on the roof.
[01:19:53.680 --> 01:19:59.400]   There's no windows in this studio and this molecule has saved our lives.
[01:19:59.400 --> 01:20:02.800]   Mole makes it easier to cope with asthma and allergies.
[01:20:02.800 --> 01:20:05.680]   Sedificantly reduces your symptoms.
[01:20:05.680 --> 01:20:08.800]   One customer said after using a molecule in our home, she was able to breathe through
[01:20:08.800 --> 01:20:10.400]   a nose for the first time in 15 years.
[01:20:10.400 --> 01:20:13.560]   That was not Lisa, but it could have been.
[01:20:13.560 --> 01:20:19.040]   Molecule's technology, funded by the EPA, extensively tested by not only real people,
[01:20:19.040 --> 01:20:25.400]   but university laboratories like the University of South Florida's Center for Biological Defense.
[01:20:25.400 --> 01:20:27.560]   You can read all about this on the Molecule webpage.
[01:20:27.560 --> 01:20:32.080]   The University of Minnesota's Particle Calibration Laboratory.
[01:20:32.080 --> 01:20:33.680]   It's easy to use.
[01:20:33.680 --> 01:20:34.680]   It's beautiful.
[01:20:34.680 --> 01:20:36.520]   It's kind of like the apple of air filters.
[01:20:36.520 --> 01:20:39.400]   It's a solid aluminum shell.
[01:20:39.400 --> 01:20:43.480]   You can connect it to the internet and when the filters get low, they'll automatically
[01:20:43.480 --> 01:20:44.480]   send you new filters.
[01:20:44.480 --> 01:20:45.480]   I love that.
[01:20:45.480 --> 01:20:47.440]   Although, I have to say that doesn't happen.
[01:20:47.440 --> 01:20:48.440]   It's not even monthly.
[01:20:48.440 --> 01:20:52.640]   It happens every few months, depending on how dirty your air is, but we've only changed
[01:20:52.640 --> 01:20:55.840]   filters once in about eight months, something like that.
[01:20:55.840 --> 01:21:03.240]   Some users have said that the Molecule is the one thing that's allowed them to breathe
[01:21:03.240 --> 01:21:05.400]   easy in their home.
[01:21:05.400 --> 01:21:09.360]   It's great for offices where windows are sealed and you can't open the windows.
[01:21:09.360 --> 01:21:13.400]   This fixes the sick air in offices.
[01:21:13.400 --> 01:21:16.760]   You don't have to use the Bluetooth or Wi-Fi if you're privacy concerned or whatever.
[01:21:16.760 --> 01:21:18.280]   It works just like a regular air fresher.
[01:21:18.280 --> 01:21:20.880]   Air fresher's got a button on the top you turn it on and so forth.
[01:21:20.880 --> 01:21:21.880]   It has a silent mode.
[01:21:21.880 --> 01:21:22.880]   I really like that.
[01:21:22.880 --> 01:21:23.880]   We use that in the bedroom.
[01:21:23.880 --> 01:21:25.200]   You don't even hear it.
[01:21:25.200 --> 01:21:26.800]   It's whisper quiet.
[01:21:26.800 --> 01:21:31.880]   If you have a lot of air to clear, you can turn it on high.
[01:21:31.880 --> 01:21:33.640]   You will love the Molecule.
[01:21:33.640 --> 01:21:34.640]   I want you to try it.
[01:21:34.640 --> 01:21:36.480]   We've got $75 off your first order.
[01:21:36.480 --> 01:21:40.440]   If you go to Molecule.com, that's Molecule with a K-M-O-L-E-K-U-L-E.com.
[01:21:40.440 --> 01:21:42.440]   Here's a promo code.
[01:21:42.440 --> 01:21:46.120]   Finally, technology has come to air purifiers.
[01:21:46.120 --> 01:21:52.720]   This thing is so much better than anything you've ever tried before.
[01:21:52.720 --> 01:21:53.720]   Molecule.com.
[01:21:53.720 --> 01:21:56.600]   A promo code "Twit".
[01:21:56.600 --> 01:22:03.080]   If you've got dangerous air in your house or your office, this is a really good solution.
[01:22:03.080 --> 01:22:04.880]   Molecule.com.
[01:22:04.880 --> 01:22:08.760]   Use a promo code "Twit" at the checkout.
[01:22:08.760 --> 01:22:09.760]   Let's see here.
[01:22:09.760 --> 01:22:14.160]   I don't have to admit though, the Molecule does not get rid of cupcake smell.
[01:22:14.160 --> 01:22:15.160]   This drive me crazy.
[01:22:15.160 --> 01:22:16.160]   That's a good thing.
[01:22:16.160 --> 01:22:17.160]   That's a good thing.
[01:22:17.160 --> 01:22:19.560]   I thought that maybe it just knows.
[01:22:19.560 --> 01:22:20.800]   It's so good.
[01:22:20.800 --> 01:22:21.800]   It knows.
[01:22:21.800 --> 01:22:22.800]   Keep the cupcake smell.
[01:22:22.800 --> 01:22:23.800]   Keep the cupcake smell.
[01:22:23.800 --> 01:22:24.800]   Keep the cupcake smell.
[01:22:24.800 --> 01:22:25.800]   It knows.
[01:22:25.800 --> 01:22:33.160]   I'm going to try to find something upbeat, something happy, something fun, an anadote.
[01:22:33.160 --> 01:22:34.160]   I can't.
[01:22:34.160 --> 01:22:35.160]   There's nothing.
[01:22:35.160 --> 01:22:36.160]   There's nothing.
[01:22:36.160 --> 01:22:37.160]   There's nothing.
[01:22:37.160 --> 01:22:38.160]   There's nothing.
[01:22:38.160 --> 01:22:39.160]   The HomePod flop.
[01:22:39.160 --> 01:22:41.600]   Is there the HomePod a flop?
[01:22:41.600 --> 01:22:43.160]   Is that set in the world on fire?
[01:22:43.160 --> 01:22:45.560]   It's not set in the world on fire.
[01:22:45.560 --> 01:22:46.560]   Is that the world on fire?
[01:22:46.560 --> 01:22:50.600]   Mark German who's really got his finger on the pulse of Apple news in Bloomberg Technologies
[01:22:50.600 --> 01:22:56.360]   say inventory is piling up, low sales have pushed the company to cut orders with the
[01:22:56.360 --> 01:22:58.360]   company that's manufacturing the HomePod.
[01:22:58.360 --> 01:23:02.400]   It'll be dropped to 199 pretty soon.
[01:23:02.400 --> 01:23:05.640]   At 199 it's a lot more interesting than at 350.
[01:23:05.640 --> 01:23:10.000]   People thought maybe if people can go to the Apple Store and listen to it, sales will
[01:23:10.000 --> 01:23:11.000]   go up.
[01:23:11.000 --> 01:23:13.800]   It had really good pre-orders which just fell off the cliff.
[01:23:13.800 --> 01:23:14.800]   I have one.
[01:23:14.800 --> 01:23:15.800]   I do two.
[01:23:15.800 --> 01:23:16.800]   I do two.
[01:23:16.800 --> 01:23:20.120]   I think for me, I like it.
[01:23:20.120 --> 01:23:23.200]   It works because I already was invested in Apple news.
[01:23:23.200 --> 01:23:24.200]   You have to be an Apple news.
[01:23:24.200 --> 01:23:25.200]   But I'm like the one.
[01:23:25.200 --> 01:23:27.200]   It's the only, you know, a so-nose or a-
[01:23:27.200 --> 01:23:32.840]   It's a really really expensive way to listen to your overpriced Apple music.
[01:23:32.840 --> 01:23:33.840]   Thank you.
[01:23:33.840 --> 01:23:34.840]   That is all true.
[01:23:34.840 --> 01:23:36.600]   However, I can corroborate.
[01:23:36.600 --> 01:23:38.160]   It sounds great.
[01:23:38.160 --> 01:23:39.360]   It does sound really good.
[01:23:39.360 --> 01:23:40.360]   It sounds great.
[01:23:40.360 --> 01:23:41.360]   Although-
[01:23:41.360 --> 01:23:43.520]   We cracked one open on Tech Republic which is also a CNET video.
[01:23:43.520 --> 01:23:46.960]   So it's a yeah.
[01:23:46.960 --> 01:23:52.680]   We talked about comparing the products during that.
[01:23:52.680 --> 01:23:57.240]   Better privacy because Apple is not very good at AI among other reasons.
[01:23:57.240 --> 01:23:59.120]   But they also are committed to privacy.
[01:23:59.120 --> 01:24:00.280]   They're really smart about this.
[01:24:00.280 --> 01:24:02.640]   They're like, "Look, we really stink at AI.
[01:24:02.640 --> 01:24:04.880]   So let's just double down on privacy.
[01:24:04.880 --> 01:24:07.240]   We're going to be the company that's all about privacy.
[01:24:07.240 --> 01:24:12.760]   We're not going to be the one that takes advantage of your data to monetize you.
[01:24:12.760 --> 01:24:15.080]   They weren't really doing that anyway.
[01:24:15.080 --> 01:24:17.880]   But whatever, you know, give them some kudos.
[01:24:17.880 --> 01:24:20.360]   But the product itself is a good product.
[01:24:20.360 --> 01:24:23.160]   It just feels, to me, it's too expensive.
[01:24:23.160 --> 01:24:26.720]   Well, I think Apple tried to make it super luxury.
[01:24:26.720 --> 01:24:29.280]   We're going to make this fantastic sounding thing.
[01:24:29.280 --> 01:24:31.080]   It's going to have this luxury price like all.
[01:24:31.080 --> 01:24:33.840]   I mean, this is Apple 2.80.
[01:24:33.840 --> 01:24:37.960]   We're going to launch later than others with something that we are going to tell you.
[01:24:37.960 --> 01:24:40.160]   We really hope you believe is better.
[01:24:40.160 --> 01:24:43.040]   We're just going to keep telling you that it's better.
[01:24:43.040 --> 01:24:47.280]   But the reality is, in this particular case, it's not better enough.
[01:24:47.280 --> 01:24:48.840]   It's not better enough.
[01:24:48.840 --> 01:24:50.880]   And Apple Music is the wedge.
[01:24:50.880 --> 01:24:51.880]   Yeah.
[01:24:51.880 --> 01:24:53.800]   They can't come in and pick off what they love to do.
[01:24:53.800 --> 01:24:57.120]   They come in and pick off the most lucrative 10% of the market, right?
[01:24:57.120 --> 01:24:58.800]   And they're so good at it.
[01:24:58.800 --> 01:25:01.040]   They're the place where all the money is made.
[01:25:01.040 --> 01:25:03.080]   They just come in and swoop that.
[01:25:03.080 --> 01:25:04.080]   It's just not happening.
[01:25:04.080 --> 01:25:06.800]   And the thing is that Sonos already did that.
[01:25:06.800 --> 01:25:07.800]   That's true.
[01:25:07.800 --> 01:25:09.800]   They're the home of a wealthy person in the Bay Area.
[01:25:09.800 --> 01:25:10.800]   They are full Sonos.
[01:25:10.800 --> 01:25:11.800]   I'm all in the Sonos.
[01:25:11.800 --> 01:25:12.800]   Rome's a room to room.
[01:25:12.800 --> 01:25:15.320]   And Sonos has its own problems as a business.
[01:25:15.320 --> 01:25:19.640]   I think the problem is that the big tech companies didn't see this coming.
[01:25:19.640 --> 01:25:23.880]   They didn't see Amazon's smart speaker coming and they didn't see it in a meaningful way.
[01:25:23.880 --> 01:25:28.960]   So Microsoft is still trying to get a smart speaker to market.
[01:25:28.960 --> 01:25:32.160]   And they're late to market.
[01:25:32.160 --> 01:25:33.560]   Apple was late to market.
[01:25:33.560 --> 01:25:35.400]   And yes, you could say that that was their strategy.
[01:25:35.400 --> 01:25:38.240]   It's always been their strategy.
[01:25:38.240 --> 01:25:43.800]   In this economy, you're going to have to provide some seriously amazing value for a product
[01:25:43.800 --> 01:25:49.640]   that Amazon, you can now get a sub $50 version.
[01:25:49.640 --> 01:25:51.760]   But it sounds terrible.
[01:25:51.760 --> 01:25:52.760]   It does.
[01:25:52.760 --> 01:25:53.760]   I would say there are--
[01:25:53.760 --> 01:25:58.040]   Well, but you can also-- the Google Home Max, which most blind testers say actually say
[01:25:58.040 --> 01:26:00.120]   it sounds better than the HomePod, by the way.
[01:26:00.120 --> 01:26:01.120]   It's a little more expensive.
[01:26:01.120 --> 01:26:03.120]   It's as big as this table.
[01:26:03.120 --> 01:26:04.120]   That's not that big.
[01:26:04.120 --> 01:26:05.120]   It's not that big.
[01:26:05.120 --> 01:26:07.760]   And it depends on where you're sitting in the room.
[01:26:07.760 --> 01:26:11.480]   Well, that's part of the problem is that Apple-- and this is actually-- I just read a really
[01:26:11.480 --> 01:26:13.400]   interesting article.
[01:26:13.400 --> 01:26:15.760]   Apple designed a speaker that sounds good no matter where you sit.
[01:26:15.760 --> 01:26:19.800]   It's the tweeters go all the way around it and the subwoofer fires straight up.
[01:26:19.800 --> 01:26:23.240]   That's fine in an environment where music is wallpaper.
[01:26:23.240 --> 01:26:27.080]   But if you're going to sit in front of speakers, the max actually sounds better.
[01:26:27.080 --> 01:26:33.360]   And the real question is, yeah, I mean, is the closed-- is Apple's closed ecosystem model
[01:26:33.360 --> 01:26:35.480]   going to survive going forward?
[01:26:35.480 --> 01:26:36.480]   Not for music.
[01:26:36.480 --> 01:26:37.480]   Are they reason--
[01:26:37.480 --> 01:26:39.200]   Well, I don't think it is for music and media.
[01:26:39.200 --> 01:26:41.920]   I think that it has been too slow to all sorts of media.
[01:26:41.920 --> 01:26:45.360]   And the same-- Apple's trying to make big moves when it comes to Hollywood, when it comes
[01:26:45.360 --> 01:26:46.360]   to Studio.
[01:26:46.360 --> 01:26:47.920]   It's well behind Netflix.
[01:26:47.920 --> 01:26:50.800]   It's well behind established providers of content.
[01:26:50.800 --> 01:26:52.680]   And that's where Apple's going to start to struggle.
[01:26:52.680 --> 01:26:53.960]   It's not just the content, though.
[01:26:53.960 --> 01:26:59.280]   So all the modeling that I've done shows that 50% of the interactions that people in
[01:26:59.280 --> 01:27:04.920]   the industrialized world have with machines will be so doing their voice between now and
[01:27:04.920 --> 01:27:06.680]   the next five years from now.
[01:27:06.680 --> 01:27:11.240]   So what that tells us is that this isn't like earth shattering news, but we're moving
[01:27:11.240 --> 01:27:16.960]   into these zero-based UIs where we're speaking to talking is the new typing.
[01:27:16.960 --> 01:27:17.960]   So talking is the timing.
[01:27:17.960 --> 01:27:18.960]   It feels pretty high, though, Amy.
[01:27:18.960 --> 01:27:19.960]   Sorry, go ahead.
[01:27:19.960 --> 01:27:20.960]   Go ahead.
[01:27:20.960 --> 01:27:22.200]   Well, here's the argument that I'm laying out.
[01:27:22.200 --> 01:27:24.520]   The speakers are just the beginning of this.
[01:27:24.520 --> 01:27:29.000]   So having a speaker that connects to your phone to do certain things is useful right
[01:27:29.000 --> 01:27:30.000]   now.
[01:27:30.000 --> 01:27:33.840]   But the real question is, what does that ecosystem look like as it matures?
[01:27:33.840 --> 01:27:38.840]   Because we could very well be heading into, instead of, are you a PC or a Mac?
[01:27:38.840 --> 01:27:40.640]   Are you Amazon or--
[01:27:40.640 --> 01:27:41.640]   That's right.
[01:27:41.640 --> 01:27:42.640]   You know, probably not--
[01:27:42.640 --> 01:27:46.640]   Microsoft is recognizing that it's not about Windows versus Mac.
[01:27:46.640 --> 01:27:47.640]   It's about cloud.
[01:27:47.640 --> 01:27:48.880]   It's about services.
[01:27:48.880 --> 01:27:49.880]   It's about voice.
[01:27:49.880 --> 01:27:50.880]   The platform.
[01:27:50.880 --> 01:27:56.840]   It's about a complete sea change in what technology manifests in our home.
[01:27:56.840 --> 01:28:00.800]   And I think, Amy, what you're talking about is ambient computing.
[01:28:00.800 --> 01:28:01.800]   Sure.
[01:28:01.800 --> 01:28:04.480]   I mean, I call it zero UI.
[01:28:04.480 --> 01:28:07.120]   There's conversational ambient, whatever you want.
[01:28:07.120 --> 01:28:16.200]   But speaking to and having other adjacently related, so like machine reading comprehension.
[01:28:16.200 --> 01:28:22.240]   So this Ray Kurzweil at TED this week debuted a new product.
[01:28:22.240 --> 01:28:24.840]   The Google search through books-- have a conversation with books.
[01:28:24.840 --> 01:28:25.840]   Wasn't that interesting?
[01:28:25.840 --> 01:28:29.440]   He said instead of searching for keywords, we're searching for ideas now.
[01:28:29.440 --> 01:28:30.440]   Right.
[01:28:30.440 --> 01:28:35.760]   Except that's an example of something called machine reading comprehension, which within--
[01:28:35.760 --> 01:28:39.960]   that's not earth shattering brand new ideas or technology.
[01:28:39.960 --> 01:28:45.120]   And in fact, Microsoft and Stanford were working on that a while ago.
[01:28:45.120 --> 01:28:49.760]   But what's different here is that this allows us to have more of a conversation.
[01:28:49.760 --> 01:28:55.960]   So rather than getting a bunch of links returned, machine reading comprehension, which is what
[01:28:55.960 --> 01:29:01.280]   that project is an example of, allows us to have those conversations with the devices.
[01:29:01.280 --> 01:29:03.400]   So this just brings me back to the speakers.
[01:29:03.400 --> 01:29:05.080]   So it's like, OK, great.
[01:29:05.080 --> 01:29:09.480]   Apple has an expensive speaker that sounds really good playing music in a particular circumstance.
[01:29:09.480 --> 01:29:17.440]   But going forward, which as the ecosystem matures, Apple's going to have to do something besides
[01:29:17.440 --> 01:29:22.080]   make a cool product that connects to your iPhone that plays music pretty well, given
[01:29:22.080 --> 01:29:24.520]   that its whole ecosystem is locked down and closed.
[01:29:24.520 --> 01:29:29.080]   And everybody else, Amazon is inside of cars now.
[01:29:29.080 --> 01:29:30.920]   Amazon is inside of banks.
[01:29:30.920 --> 01:29:33.920]   Amazon has this massive open platform.
[01:29:33.920 --> 01:29:40.880]   It's to be fair series inside of cars too.
[01:29:40.880 --> 01:29:44.360]   I don't think that most audiences understand the difference that you're talking about.
[01:29:44.360 --> 01:29:47.920]   And I mean, as a person who covers consumer technology day in and day out and sees how
[01:29:47.920 --> 01:29:55.280]   consumers respond to things, Apple has a chance with HomePod and any other AI.
[01:29:55.280 --> 01:29:59.520]   As long as Apple can find the thing that people don't know they need to solve yet.
[01:29:59.520 --> 01:30:00.960]   But look at, you know, Nest.
[01:30:00.960 --> 01:30:04.320]   Why did Nest beat all other connected thermostats they existed before?
[01:30:04.320 --> 01:30:05.600]   It wasn't a new thing.
[01:30:05.600 --> 01:30:10.040]   They made it look good and they made it easy and they identified that something needed
[01:30:10.040 --> 01:30:11.040]   to be fixed now.
[01:30:11.040 --> 01:30:12.040]   They've had problems since then.
[01:30:12.040 --> 01:30:16.520]   Yeah, because nobody wants to spend $400 on a thermostat that didn't do significantly
[01:30:16.520 --> 01:30:18.800]   anything more better than the existing.
[01:30:18.800 --> 01:30:21.080]   If HomePod could solve a problem that we don't know.
[01:30:21.080 --> 01:30:24.480]   And literally, it really solved it, not just on the surface.
[01:30:24.480 --> 01:30:25.480]   Sure.
[01:30:25.480 --> 01:30:28.280]   I do think that I just want to pick the one.
[01:30:28.280 --> 01:30:29.440]   By the way, that's a big if.
[01:30:29.440 --> 01:30:30.440]   That's a hard thing.
[01:30:30.440 --> 01:30:31.440]   It is a hard thing.
[01:30:31.440 --> 01:30:33.400]   Oh, I just think it can be done.
[01:30:33.400 --> 01:30:37.960]   That's basically build a better mouse trap and the world will beat a path to your door.
[01:30:37.960 --> 01:30:38.960]   Well, yeah.
[01:30:38.960 --> 01:30:39.960]   But that keeps happening.
[01:30:39.960 --> 01:30:40.960]   That keeps happening.
[01:30:40.960 --> 01:30:43.560]   But also it also happens from unexpected quarters.
[01:30:43.560 --> 01:30:44.960]   It happens from unexpected quarters.
[01:30:44.960 --> 01:30:48.560]   It's rarely the incumbent that comes up with the next thing, right?
[01:30:48.560 --> 01:30:49.560]   It's always.
[01:30:49.560 --> 01:30:50.560]   I want to talk about voice though.
[01:30:50.560 --> 01:30:58.360]   I want it for just a second and zero UI as Amy characterized it and sort of ambient interface
[01:30:58.360 --> 01:31:05.120]   and all that because there's part of this that I feel like gets missed and gets overstated.
[01:31:05.120 --> 01:31:10.040]   And that's I see a lot of these estimates, Amy, similar to like 50% of all computing
[01:31:10.040 --> 01:31:14.160]   is going to be with your voice in the future.
[01:31:14.160 --> 01:31:16.880]   And I take issue with that for a couple of times.
[01:31:16.880 --> 01:31:17.880]   I didn't say computing.
[01:31:17.880 --> 01:31:21.520]   I said interactions with devices, which is not like the kind of computing that you're
[01:31:21.520 --> 01:31:24.120]   doing with the laptop that you've got sitting in front of you right now.
[01:31:24.120 --> 01:31:28.120]   But this is why I prefer the term ambient because what to me, what's happening.
[01:31:28.120 --> 01:31:29.120]   And so you can see this.
[01:31:29.120 --> 01:31:32.000]   It's moving from the center to the edge, computing moving to the edge.
[01:31:32.000 --> 01:31:36.720]   And the idea that everything has some smarts and you interact with it in a natural way.
[01:31:36.720 --> 01:31:38.360]   Sometimes that's voice.
[01:31:38.360 --> 01:31:40.760]   Sometimes it's tapping your ear.
[01:31:40.760 --> 01:31:43.280]   Sometimes it's that the thermostat has intelligence.
[01:31:43.280 --> 01:31:46.520]   And to me, that's why ambient computing is a better definition.
[01:31:46.520 --> 01:31:49.040]   Certainly we will use speech, but it won't be.
[01:31:49.040 --> 01:31:52.240]   It's not what we're talking about as like this is the future of computing.
[01:31:52.240 --> 01:31:54.600]   The future of computing is computing it.
[01:31:54.600 --> 01:31:57.240]   Those cupcakes will have processors in them.
[01:31:57.240 --> 01:31:58.240]   They're on interface.
[01:31:58.240 --> 01:32:08.560]   No, so that I like better because these things we naturally train users because certain users
[01:32:08.560 --> 01:32:13.400]   that are new to computing and there are still a lot of users that are still just on the
[01:32:13.400 --> 01:32:15.720]   on-ramp.
[01:32:15.720 --> 01:32:21.600]   And some of those simpler interfaces appeal because it is better on-ramp.
[01:32:21.600 --> 01:32:29.680]   But as those users become more sophisticated and complex, they tend to become more what
[01:32:29.680 --> 01:32:31.960]   we think of today as like power users, right?
[01:32:31.960 --> 01:32:33.440]   And they don't want to use a voice interface.
[01:32:33.440 --> 01:32:37.640]   They don't want to use the sort of the dumb down interfaces.
[01:32:37.640 --> 01:32:40.320]   They want to use the ones where they can be more efficient.
[01:32:40.320 --> 01:32:41.680]   Here's what I would say.
[01:32:41.680 --> 01:32:48.440]   That is based on what will soon be an outdated paradigm that we have to adjust to the computer.
[01:32:48.440 --> 01:32:50.080]   That we have to adjust to the interface.
[01:32:50.080 --> 01:32:51.080]   Yes.
[01:32:51.080 --> 01:32:52.240]   I'm hopeful for it.
[01:32:52.240 --> 01:32:54.520]   We haven't gotten him close to it yet.
[01:32:54.520 --> 01:32:59.640]   But I'm hopeful for it is an interface that adjusts to us and that we operate at whatever.
[01:32:59.640 --> 01:33:04.440]   However, we operate and this would be critical for ambient computing to work, we operate
[01:33:04.440 --> 01:33:11.080]   as we would and the technology is smart enough to respond to us appropriately.
[01:33:11.080 --> 01:33:12.560]   We don't have to adjust ourselves.
[01:33:12.560 --> 01:33:16.440]   So last year I was interviewing Mark Spates, who's a head of product for Google and he
[01:33:16.440 --> 01:33:20.920]   works a lot on all the Google Home implementation, all of their smart home.
[01:33:20.920 --> 01:33:23.800]   If you want to call it that work.
[01:33:23.800 --> 01:33:27.960]   And he said exactly what you were saying and that he thinks that the future is all an
[01:33:27.960 --> 01:33:33.840]   anticipation of needs and preemption of those needs and it's not necessarily voice.
[01:33:33.840 --> 01:33:38.520]   And the examples that he would give, and I don't remember the specifics, but really
[01:33:38.520 --> 01:33:43.280]   surrounded what we expect to have happen on our home based on our patterns and then have
[01:33:43.280 --> 01:33:48.040]   that happening without a lot of interaction from us, simply through anticipating our needs.
[01:33:48.040 --> 01:33:52.160]   So to take this all the way back to HomePod, if Apple were able to create a HomePod that
[01:33:52.160 --> 01:33:57.040]   even though it cost $350, when I walked in the front door said, "Hey, Lindsay, you
[01:33:57.040 --> 01:34:01.800]   allowed your home," started playing what I was listening to in the car without me asking
[01:34:01.800 --> 01:34:08.680]   it to ask me if I wanted to re-lock the doors or keep them open, reminded me to do whatever
[01:34:08.680 --> 01:34:13.640]   it is that I always forget to do when I get home, that may be charming and useful enough
[01:34:13.640 --> 01:34:15.400]   to me that I want it.
[01:34:15.400 --> 01:34:20.960]   Here's why it won't happen from Apple because Apple is the king of the silo and interoperability
[01:34:20.960 --> 01:34:23.640]   is exactly the opposite of what Apple's looking to do.
[01:34:23.640 --> 01:34:28.080]   And so that would only work if you had an Apple car and an Apple music service, Apple
[01:34:28.080 --> 01:34:30.200]   refrigerator and Apple.
[01:34:30.200 --> 01:34:33.600]   And so if that's going to work, it's going to more likely come from somebody like Google
[01:34:33.600 --> 01:34:38.040]   that is more open to the idea of having everything work together.
[01:34:38.040 --> 01:34:39.040]   What is the name of the...
[01:34:39.040 --> 01:34:41.080]   It's just holding Apple back, frankly.
[01:34:41.080 --> 01:34:46.440]   Most certainly, there's a name for this concept that organizations often create products that
[01:34:46.440 --> 01:34:49.480]   look like their organization.
[01:34:49.480 --> 01:34:54.560]   So the old fashion example of this is that you would...
[01:34:54.560 --> 01:35:01.760]   If the stereo on the dashboard seems totally isolated and uninteroperable, if that a word,
[01:35:01.760 --> 01:35:04.960]   with the rest of the car, that's usually because a different department designed that.
[01:35:04.960 --> 01:35:08.800]   There's a totally different department within that auto manufacturer who designed that.
[01:35:08.800 --> 01:35:13.240]   And that the physical product ends up reflecting the organization's organization.
[01:35:13.240 --> 01:35:14.240]   Where's that?
[01:35:14.240 --> 01:35:18.240]   I don't remember the name of it, but I'm going to Google the heck out of it, just like Mark
[01:35:18.240 --> 01:35:19.240]   Zuckerberg.
[01:35:19.240 --> 01:35:20.240]   Yeah, that's interesting.
[01:35:20.240 --> 01:35:21.240]   When I get out of here so that I can share it with you all.
[01:35:21.240 --> 01:35:23.160]   You should use the new Google talk to books.
[01:35:23.160 --> 01:35:24.160]   Maybe it could help.
[01:35:24.160 --> 01:35:25.160]   Maybe.
[01:35:25.160 --> 01:35:26.160]   But it is a really interesting concept.
[01:35:26.160 --> 01:35:28.440]   And I think that's what you're seeing happen with Apple and what you're not seeing
[01:35:28.440 --> 01:35:29.440]   happening with Google.
[01:35:29.440 --> 01:35:30.440]   It's very interesting.
[01:35:30.440 --> 01:35:34.120]   Google has historically been very good at moving people around within its organization
[01:35:34.120 --> 01:35:35.120]   over time.
[01:35:35.120 --> 01:35:36.120]   Just break that up.
[01:35:36.120 --> 01:35:40.720]   You go into the Google HR organization and end up doing content at YouTube and then you
[01:35:40.720 --> 01:35:42.760]   end up moving over to a different part of Google.
[01:35:42.760 --> 01:35:43.760]   That's conscious.
[01:35:43.760 --> 01:35:44.960]   That's conscious.
[01:35:44.960 --> 01:35:50.040]   And then Google ends up becoming less bifurcated as a result because its organization is a
[01:35:50.040 --> 01:35:51.480]   little bit more organic.
[01:35:51.480 --> 01:35:54.120]   I think that was a problem at Microsoft.
[01:35:54.120 --> 01:35:57.560]   It's going to be a problem at Apple is this inward looking.
[01:35:57.560 --> 01:36:01.440]   And you can even see it in the metaphor of Apple's spaceship campus, which is inward
[01:36:01.440 --> 01:36:06.400]   looking and is a barrier to the outside world for something like Amazon's Downtown headquarters
[01:36:06.400 --> 01:36:14.080]   in Seattle, which is integrated in with the city of Seattle and is part of Seattle.
[01:36:14.080 --> 01:36:15.680]   I find that fascinating.
[01:36:15.680 --> 01:36:21.200]   This whole notion of a company reproducing its own bias.
[01:36:21.200 --> 01:36:22.200]   Yeah.
[01:36:22.200 --> 01:36:23.200]   I think that's really fascinating.
[01:36:23.200 --> 01:36:24.200]   It makes a lot of sense.
[01:36:24.200 --> 01:36:27.760]   And it is something maybe that's really starting to harm Apple.
[01:36:27.760 --> 01:36:32.560]   It's one of the reasons Siri hasn't really excelled.
[01:36:32.560 --> 01:36:33.560]   This talk to books thing.
[01:36:33.560 --> 01:36:40.960]   See, what I feel like is that we're in baby steps mode and that what we're seeing is slow.
[01:36:40.960 --> 01:36:44.160]   One of the reasons, for instance, that we don't like voice assistants is we have to learn
[01:36:44.160 --> 01:36:45.840]   a peculiar syntax.
[01:36:45.840 --> 01:36:48.040]   We have to learn how to ask them for what we want.
[01:36:48.040 --> 01:36:49.760]   The same way you used to have to learn a command line.
[01:36:49.760 --> 01:36:50.760]   Yeah.
[01:36:50.760 --> 01:36:52.680]   And it's not what it needs to do is artificial.
[01:36:52.680 --> 01:36:54.240]   It needs to understand what we're saying.
[01:36:54.240 --> 01:36:55.480]   It needs to be how 9,000.
[01:36:55.480 --> 01:36:56.960]   Well, that's a bad example.
[01:36:56.960 --> 01:37:02.120]   It needs to-- it better not lock the pie-vei door, but it needs to understand what we-- it
[01:37:02.120 --> 01:37:04.480]   needs to intuit what we're looking for.
[01:37:04.480 --> 01:37:06.320]   And I feel like this talk to books is interesting.
[01:37:06.320 --> 01:37:10.480]   Did you think, Amy, that this was not compelling or--
[01:37:10.480 --> 01:37:16.920]   Well, I mean, Ray is compelling when he speaks, but the project itself is just an example
[01:37:16.920 --> 01:37:20.280]   of something called machine reading comprehension.
[01:37:20.280 --> 01:37:25.040]   So right now-- so Google's gotten pretty good at semantic search.
[01:37:25.040 --> 01:37:28.700]   You type in a search or you speak a search and it may or may not give you the answer
[01:37:28.700 --> 01:37:30.200]   that you're looking for.
[01:37:30.200 --> 01:37:37.280]   But that's different from asking a question like, can I roller skate inside?
[01:37:37.280 --> 01:37:38.280]   I don't know.
[01:37:38.280 --> 01:37:42.800]   Here, I've asked talk to books, which will triumph, collectivism or individualism?
[01:37:42.800 --> 01:37:43.800]   [LAUGHTER]
[01:37:43.800 --> 01:37:44.800]   And actually--
[01:37:44.800 --> 01:37:45.800]   Yeah.
[01:37:45.800 --> 01:37:52.280]   --came up with some pretty good responses, including Ains Rand's Fountainhead, the Oxford
[01:37:52.280 --> 01:37:58.920]   Handbook of Emotions, Social Cognition, and Problem Solving, Saving Face in Business,
[01:37:58.920 --> 01:38:01.120]   and Cultural Differences in a Globalizing World.
[01:38:01.120 --> 01:38:04.920]   Speaking of advanced reasoning, before I forget, I found it.
[01:38:04.920 --> 01:38:05.920]   Oh, good.
[01:38:05.920 --> 01:38:07.440]   It's called Conway's Law.
[01:38:07.440 --> 01:38:08.440]   Oh.
[01:38:08.440 --> 01:38:10.440]   And I'll read it to you.
[01:38:10.440 --> 01:38:11.440]   So I'll read it to you.
[01:38:11.440 --> 01:38:15.520]   This is a law-- and it's an adage named after computer programmer Melvin Conway.
[01:38:15.520 --> 01:38:17.680]   I'm reading off of Wikipedia.
[01:38:17.680 --> 01:38:23.640]   It was dubbed Conway's Law and states that organizations which design systems are constrained
[01:38:23.640 --> 01:38:28.640]   to produce designs, which are copies of the communication structures of those organizations.
[01:38:28.640 --> 01:38:29.640]   Wow.
[01:38:29.640 --> 01:38:30.640]   I love that.
[01:38:30.640 --> 01:38:32.960]   As an editor, I would say that those witches should have been that's, but other than that,
[01:38:32.960 --> 01:38:33.960]   it's really interesting.
[01:38:33.960 --> 01:38:34.960]   [LAUGHTER]
[01:38:34.960 --> 01:38:35.960]   That's really interesting.
[01:38:35.960 --> 01:38:39.480]   Well, it's probably a Britishism or something like that.
[01:38:39.480 --> 01:38:41.240]   Isn't that really interesting?
[01:38:41.240 --> 01:38:42.800]   I wonder if it holds true.
[01:38:42.800 --> 01:38:55.640]   Eric Raymond, who's a well-known among open source advocates, restated Conway's Law in
[01:38:55.640 --> 01:39:01.440]   the New Hacker's Dictionary, he said, "The organization of the software and the organization
[01:39:01.440 --> 01:39:04.880]   of the softball team will be congruent.
[01:39:04.880 --> 01:39:09.440]   The software team and the software itself will be congruent.
[01:39:09.440 --> 01:39:13.320]   If you have four groups working on a compiler, you get a four-pass compiler."
[01:39:13.320 --> 01:39:14.320]   [LAUGHTER]
[01:39:14.320 --> 01:39:18.880]   But I think this is a really good argument for biodiversity.
[01:39:18.880 --> 01:39:20.480]   Yes, diversity.
[01:39:20.480 --> 01:39:21.480]   Diversity.
[01:39:21.480 --> 01:39:22.480]   Thank you.
[01:39:22.480 --> 01:39:28.360]   I mean, honestly, unless you're only-- and here's the mathematical explanation for that.
[01:39:28.360 --> 01:39:35.480]   If you are only selling your product to white dudes, white dudes make up only a certain percentage
[01:39:35.480 --> 01:39:42.600]   of the total population, which means that you're only addressing a tiny fraction of
[01:39:42.600 --> 01:39:43.600]   your addressable mark.
[01:39:43.600 --> 01:39:46.120]   Although as a white dude, I'm pretty happy about that.
[01:39:46.120 --> 01:39:52.360]   That's why I love software so much for us, by us.
[01:39:52.360 --> 01:39:53.360]   Right?
[01:39:53.360 --> 01:39:54.360]   Fibu.
[01:39:54.360 --> 01:39:59.080]   Here's Nigel Bevin, the usability expert, his version of Conway's Law.
[01:39:59.080 --> 01:40:03.200]   Organizations often produce websites with content and structure that mirrors the internal
[01:40:03.200 --> 01:40:07.920]   concerns of the organization rather than the needs of the users.
[01:40:07.920 --> 01:40:09.320]   I think-- I don't know.
[01:40:09.320 --> 01:40:10.320]   It feels right to me.
[01:40:10.320 --> 01:40:11.720]   Yeah, there's something there.
[01:40:11.720 --> 01:40:12.720]   Feels right to me.
[01:40:12.720 --> 01:40:15.400]   In some personal experiences, I've had to say absolutely.
[01:40:15.400 --> 01:40:16.400]   That happens.
[01:40:16.400 --> 01:40:17.400]   And it's completely unconscious.
[01:40:17.400 --> 01:40:20.880]   And sometimes you can look at-- and I love to use the dashboard example for this, but
[01:40:20.880 --> 01:40:26.400]   you can look at a dashboard and be like, "Why on earth does this knob exist over here when
[01:40:26.400 --> 01:40:27.480]   really it should be over here?"
[01:40:27.480 --> 01:40:29.480]   It's probably because that team was working over here.
[01:40:29.480 --> 01:40:30.480]   Yeah.
[01:40:30.480 --> 01:40:31.480]   There's a team for that knob.
[01:40:31.480 --> 01:40:32.480]   There were over there.
[01:40:32.480 --> 01:40:34.480]   Let's follow Conway's Law.
[01:40:34.480 --> 01:40:38.400]   AirPods, I can guarantee-- I don't know who worked-- who exactly was on that design team,
[01:40:38.400 --> 01:40:42.960]   but I can guarantee-- I would feel very confident that it was not a woman with curly hair or
[01:40:42.960 --> 01:40:44.240]   not a woman with long hair.
[01:40:44.240 --> 01:40:45.240]   Very good point.
[01:40:45.240 --> 01:40:46.240]   You know.
[01:40:46.240 --> 01:40:47.480]   And there's one size fits all.
[01:40:47.480 --> 01:40:50.280]   And my wife who has smaller ear holes-- I can't wear airpods.
[01:40:50.280 --> 01:40:51.280]   They're uncomfortable.
[01:40:51.280 --> 01:40:53.200]   Yeah, they don't fit-- they fall out of my ears.
[01:40:53.200 --> 01:40:54.200]   Yep.
[01:40:54.200 --> 01:40:56.360]   The Apple earphones in general fall in my ears.
[01:40:56.360 --> 01:40:57.800]   Most other-- if you have a small--
[01:40:57.800 --> 01:40:59.280]   The phone for a lot of it.
[01:40:59.280 --> 01:41:03.040]   Most other companies will make earbuds with the exception of Apple.
[01:41:03.040 --> 01:41:06.520]   We'll give you multi-sized rubber things.
[01:41:06.520 --> 01:41:11.200]   And Apple's the only one whether it's the iPhone earpods or the earpods.
[01:41:11.200 --> 01:41:12.200]   That's it.
[01:41:12.200 --> 01:41:14.720]   It better fit.
[01:41:14.720 --> 01:41:18.560]   If it doesn't fit-- I have yet to see-- You must get over it.
[01:41:18.560 --> 01:41:20.360]   I travel a lot for work.
[01:41:20.360 --> 01:41:24.680]   And so I'm seeing more and more people walking around airports and walking around cities with
[01:41:24.680 --> 01:41:25.680]   these things in.
[01:41:25.680 --> 01:41:26.680]   And they're all men.
[01:41:26.680 --> 01:41:29.240]   I really haven't seen women wearing them yet.
[01:41:29.240 --> 01:41:32.880]   I've never seen a woman in the airport with airpods.
[01:41:32.880 --> 01:41:33.380]   Oh, my gosh.
[01:41:33.380 --> 01:41:34.200]   I see it all the time.
[01:41:34.200 --> 01:41:35.200]   I'm sorry.
[01:41:35.200 --> 01:41:37.360]   Yeah, because I am fighting it.
[01:41:37.360 --> 01:41:40.640]   I think they do fit me, but I just can't do it.
[01:41:40.640 --> 01:41:42.480]   Because it looks so bad.
[01:41:42.480 --> 01:41:45.480]   And I realize that that's a bias that I'll probably get over over time because so many
[01:41:45.480 --> 01:41:46.480]   people are wearing them.
[01:41:46.480 --> 01:41:50.520]   But I was actually in the airport last week actively looking and being like, look at all
[01:41:50.520 --> 01:41:52.520]   those ladies wearing airpods.
[01:41:52.520 --> 01:41:54.440]   So maybe it's shifting or-- I don't know.
[01:41:54.440 --> 01:41:56.000]   I'm going to look when I go to SFO.
[01:41:56.000 --> 01:41:57.840]   When I'm flying back, I will--
[01:41:57.840 --> 01:42:00.440]   It does feel like a pathology of a company, though.
[01:42:00.440 --> 01:42:03.120]   They can't see beyond.
[01:42:03.120 --> 01:42:06.680]   It's actually been a problem in the computer industry since day one that the engineers
[01:42:06.680 --> 01:42:12.360]   designed products, they understood, but that normal people didn't understand.
[01:42:12.360 --> 01:42:14.880]   They were really by engineers, for engineers.
[01:42:14.880 --> 01:42:18.520]   And now it's by white male engineers.
[01:42:18.520 --> 01:42:19.520]   I think that that's a pathology.
[01:42:19.520 --> 01:42:25.320]   I think outside of the gender and diversity thing, though, doesn't that sort of-- I don't
[01:42:25.320 --> 01:42:26.320]   know.
[01:42:26.320 --> 01:42:29.160]   I keep finding it so interesting that in a lot of ways, our technology has gotten more
[01:42:29.160 --> 01:42:30.360]   and more complicated.
[01:42:30.360 --> 01:42:33.000]   And we have fewer and fewer user manuals.
[01:42:33.000 --> 01:42:36.280]   You're just sort of expected to understand how to use stuff.
[01:42:36.280 --> 01:42:41.920]   I was talking about this on the radio show today because on the radio show, I talked
[01:42:41.920 --> 01:42:43.840]   to normals.
[01:42:43.840 --> 01:42:47.240]   And they are in great pain.
[01:42:47.240 --> 01:42:49.000]   It's not getting better.
[01:42:49.000 --> 01:42:51.560]   We always thought, oh, it's the infancy of technology.
[01:42:51.560 --> 01:42:52.960]   It'll get easier to use.
[01:42:52.960 --> 01:42:54.760]   It'll be, it's not getting better.
[01:42:54.760 --> 01:42:56.920]   It's getting far worse.
[01:42:56.920 --> 01:43:02.280]   And now people are struggling with trying to keep Windows secure over a network and they're
[01:43:02.280 --> 01:43:06.840]   adding Internet of Things devices that are themselves insecure.
[01:43:06.840 --> 01:43:08.800]   Their routers are misbehaving.
[01:43:08.800 --> 01:43:11.280]   No, not routers on the next frontier.
[01:43:11.280 --> 01:43:13.040]   Your printer won't print.
[01:43:13.040 --> 01:43:17.360]   You can't change the channel on your TV.
[01:43:17.360 --> 01:43:21.800]   It's a maze of complexity that is not serving people at all.
[01:43:21.800 --> 01:43:24.360]   I understand how people might just get fed up and started.
[01:43:24.360 --> 01:43:27.800]   This is reminding us about China, though.
[01:43:27.800 --> 01:43:34.440]   One of the reasons that China does have a big advantage over America in AI and other
[01:43:34.440 --> 01:43:40.800]   fields right now is they don't have as much of a gender gap.
[01:43:40.800 --> 01:43:42.160]   It's a model culture.
[01:43:42.160 --> 01:43:50.160]   No, it's like they're close to almost all their companies are close to 40 to 50% female,
[01:43:50.160 --> 01:43:52.160]   all the big companies and their engineers.
[01:43:52.160 --> 01:43:53.160]   That's interesting.
[01:43:53.160 --> 01:43:54.680]   And they're in leadership positions.
[01:43:54.680 --> 01:43:55.680]   That's really interesting.
[01:43:55.680 --> 01:43:58.080]   In leadership positions as well.
[01:43:58.080 --> 01:43:59.080]   Not as many.
[01:43:59.080 --> 01:44:00.080]   And they start young.
[01:44:00.080 --> 01:44:01.080]   Yeah.
[01:44:01.080 --> 01:44:02.080]   They start young.
[01:44:02.080 --> 01:44:05.080]   Their schools popping up all over the country where kids are like AI schools for kids.
[01:44:05.080 --> 01:44:06.080]   How do they do that?
[01:44:06.080 --> 01:44:07.760]   Did they make a conscious effort?
[01:44:07.760 --> 01:44:09.840]   Was it the communist?
[01:44:09.840 --> 01:44:10.840]   Math.
[01:44:10.840 --> 01:44:11.840]   In math.
[01:44:11.840 --> 01:44:12.840]   That's right.
[01:44:12.840 --> 01:44:13.840]   Yeah.
[01:44:13.840 --> 01:44:18.680]   The men and women compete are equally skilled.
[01:44:18.680 --> 01:44:23.160]   There's no stigmas and it really starts with math.
[01:44:23.160 --> 01:44:24.160]   So it was cultural.
[01:44:24.160 --> 01:44:25.160]   And need.
[01:44:25.160 --> 01:44:26.160]   Need for a workforce.
[01:44:26.160 --> 01:44:27.160]   Need.
[01:44:27.160 --> 01:44:31.040]   The Chinese had needed a huge workforce.
[01:44:31.040 --> 01:44:35.800]   So if you think about it in terms of in the US, if we're maxed out with the amount of
[01:44:35.800 --> 01:44:39.720]   males, which may be that can go into tech fields, right?
[01:44:39.720 --> 01:44:47.600]   We're not nearly maxed out in our capacity and our available labor of women to go into
[01:44:47.600 --> 01:44:49.600]   and hold those positions.
[01:44:49.600 --> 01:44:50.600]   That's great.
[01:44:50.600 --> 01:44:51.600]   This is great.
[01:44:51.600 --> 01:44:52.600]   I had no idea.
[01:44:52.600 --> 01:44:57.360]   And so because of that, and that's the best argument that I've even been able to use on
[01:44:57.360 --> 01:45:03.040]   people who are real, either threatened or skeptical about the gender gap in the US.
[01:45:03.040 --> 01:45:09.360]   And of like, look, we're losing because we're not mobilizing enough women.
[01:45:09.360 --> 01:45:13.920]   And so of course, you know, we're fighting with one arm tied behind our back.
[01:45:13.920 --> 01:45:14.920]   Exactly.
[01:45:14.920 --> 01:45:15.920]   Right.
[01:45:15.920 --> 01:45:16.920]   And it's not just diversity for diversity's sake.
[01:45:16.920 --> 01:45:20.400]   I mean, again, like this is an easy argument to make with numbers.
[01:45:20.400 --> 01:45:22.160]   So you've got half the population.
[01:45:22.160 --> 01:45:27.880]   So your sort of plausible scenarios are either that that half of the population is on the
[01:45:27.880 --> 01:45:32.120]   right side of the bell curve towards the end, meaning that they are the most smart and the
[01:45:32.120 --> 01:45:35.880]   most capable, which is statistically unlikely, right?
[01:45:35.880 --> 01:45:41.440]   Or the other alternative is that we've lapped off, you know, a good chunk of our best and
[01:45:41.440 --> 01:45:42.440]   brightest.
[01:45:42.440 --> 01:45:48.720]   And China has taken the attitude that gender sort of doesn't matter, that they're just
[01:45:48.720 --> 01:45:52.720]   looking for the best and the brightest and to push them as hard as they can to achieve
[01:45:52.720 --> 01:45:53.960]   global dominance.
[01:45:53.960 --> 01:45:54.960]   Wow.
[01:45:54.960 --> 01:45:56.760]   You know, I mean.
[01:45:56.760 --> 01:45:59.720]   So important note in the in the China US debate.
[01:45:59.720 --> 01:46:01.280]   This has been an education.
[01:46:01.280 --> 01:46:03.080]   This is what I got to say.
[01:46:03.080 --> 01:46:04.440]   I am sorry, but I have to interrupt.
[01:46:04.440 --> 01:46:06.360]   We have to take a break.
[01:46:06.360 --> 01:46:09.480]   Keep those good, those brilliant thoughts coming.
[01:46:09.480 --> 01:46:11.080]   We're not done yet.
[01:46:11.080 --> 01:46:12.960]   But I did want to show you we made this little movie.
[01:46:12.960 --> 01:46:13.960]   It's really cute.
[01:46:13.960 --> 01:46:19.120]   I really wanted to show you from the week of things that we did this week on Twitch.
[01:46:19.120 --> 01:46:20.920]   Previously on Twitch.
[01:46:20.920 --> 01:46:21.920]   I love that.
[01:46:21.920 --> 01:46:23.320]   Well, your foil cap.
[01:46:23.320 --> 01:46:24.920]   It's by tin foil hat.
[01:46:24.920 --> 01:46:26.000]   That's fantastic.
[01:46:26.000 --> 01:46:28.400]   I'm a paranoid nut job.
[01:46:28.400 --> 01:46:31.800]   I don't think paranoid nut jobs admit that they're paranoid nut jobs.
[01:46:31.800 --> 01:46:32.800]   So you're fine.
[01:46:32.800 --> 01:46:34.880]   You can paranoid have enemies.
[01:46:34.880 --> 01:46:36.360]   The new screen savers.
[01:46:36.360 --> 01:46:37.360]   This is the ink display.
[01:46:37.360 --> 01:46:38.360]   Yes.
[01:46:38.360 --> 01:46:39.360]   Beautiful display.
[01:46:39.360 --> 01:46:40.360]   It's about three quarters of an inch.
[01:46:40.360 --> 01:46:42.320]   Can I call this a smart license plate?
[01:46:42.320 --> 01:46:43.320]   You can.
[01:46:43.320 --> 01:46:44.320]   Calling server.
[01:46:44.320 --> 01:46:45.320]   Yes.
[01:46:45.320 --> 01:46:46.320]   Why is it?
[01:46:46.320 --> 01:46:47.320]   What's it doing?
[01:46:47.320 --> 01:46:48.320]   It's going to change.
[01:46:48.320 --> 01:46:51.560]   You've got the new sticker and you've got the California background.
[01:46:51.560 --> 01:46:53.120]   I think you've got a handle on the future.
[01:46:53.120 --> 01:46:57.720]   I mean, I can really see that this at some point becomes kind of required, right?
[01:46:57.720 --> 01:46:59.440]   For things like this.
[01:46:59.440 --> 01:47:00.440]   This week in Google.
[01:47:00.440 --> 01:47:03.920]   This is the it's the new doorbell from the alphabet company Nest.
[01:47:03.920 --> 01:47:05.880]   This is my family coming home right now.
[01:47:05.880 --> 01:47:06.880]   Is that a blue?
[01:47:06.880 --> 01:47:07.880]   That's a blue.
[01:47:07.880 --> 01:47:08.880]   It's my birthday.
[01:47:08.880 --> 01:47:12.280]   Oh, I just spoiled their birthday balloons.
[01:47:12.280 --> 01:47:13.600]   You don't have to tell them.
[01:47:13.600 --> 01:47:15.120]   You invaded your own privacy.
[01:47:15.120 --> 01:47:16.600]   Wait, did we see a cake?
[01:47:16.600 --> 01:47:17.600]   Hold on.
[01:47:17.600 --> 01:47:18.600]   No, stop looking.
[01:47:18.600 --> 01:47:19.600]   Stop.
[01:47:19.600 --> 01:47:20.600]   Stop.
[01:47:20.600 --> 01:47:21.600]   To it.
[01:47:21.600 --> 01:47:23.760]   The happiest place on earth.
[01:47:23.760 --> 01:47:24.760]   Whoa.
[01:47:24.760 --> 01:47:25.760]   Stolen.
[01:47:25.760 --> 01:47:26.760]   Yes.
[01:47:26.760 --> 01:47:28.760]   How does it know?
[01:47:28.760 --> 01:47:35.360]   Well, if you if it's the attach from the vehicle, I like this.
[01:47:35.360 --> 01:47:36.360]   That's really cool.
[01:47:36.360 --> 01:47:37.360]   I didn't see that.
[01:47:37.360 --> 01:47:38.360]   Only in California.
[01:47:38.360 --> 01:47:39.360]   Are they legal?
[01:47:39.360 --> 01:47:40.840]   Are they being used?
[01:47:40.840 --> 01:47:43.800]   They're legal in California, Arizona.
[01:47:43.800 --> 01:47:45.800]   Many states are looking at it.
[01:47:45.800 --> 01:47:48.200]   There's actually got a lot of good reason for the state to adopt it.
[01:47:48.200 --> 01:47:49.200]   They're not cheap.
[01:47:49.200 --> 01:47:53.120]   They're hundreds of dollars right now, but they make more of them.
[01:47:53.120 --> 01:47:54.520]   It could do all sorts of interesting things.
[01:47:54.520 --> 01:47:58.720]   They worked with California and the CH, the California Highway Patrol to make sure it
[01:47:58.720 --> 01:48:01.760]   was compliant with everything the CHP needed.
[01:48:01.760 --> 01:48:05.280]   It can actually put advertising, but not while you're driving, only when you're legally
[01:48:05.280 --> 01:48:06.280]   parked.
[01:48:06.280 --> 01:48:07.280]   Oh, my gosh.
[01:48:07.280 --> 01:48:08.880]   It's meant to be a replacement for your life.
[01:48:08.880 --> 01:48:10.360]   Like that's where it goes where you're.
[01:48:10.360 --> 01:48:11.360]   Yeah.
[01:48:11.360 --> 01:48:12.360]   Of course.
[01:48:12.360 --> 01:48:13.360]   Then you're just like, so that's amazing.
[01:48:13.360 --> 01:48:16.360]   But then also like you're being tracked at literally every night.
[01:48:16.360 --> 01:48:17.360]   Oh, yeah.
[01:48:17.360 --> 01:48:18.360]   Oh, yeah.
[01:48:18.360 --> 01:48:19.360]   It's got tracking.
[01:48:19.360 --> 01:48:21.160]   All I can think about is weather.
[01:48:21.160 --> 01:48:22.600]   Like what happens when I get with what?
[01:48:22.600 --> 01:48:23.600]   What happens when I get hit?
[01:48:23.600 --> 01:48:24.600]   Nope.
[01:48:24.600 --> 01:48:26.640]   It's IP 67 waterproof when it gets hit.
[01:48:26.640 --> 01:48:28.080]   Well, it gets hit, but so does your car.
[01:48:28.080 --> 01:48:32.280]   Yeah, but bumpers are for bumping.
[01:48:32.280 --> 01:48:35.880]   When I was in when I was in Israel, car bumpers are not for bumping.
[01:48:35.880 --> 01:48:41.040]   I just want to say most cars nowadays when it might be, you might have like a big, big
[01:48:41.040 --> 01:48:45.080]   rubber thing on your car, but nowadays cars collapse.
[01:48:45.080 --> 01:48:46.080]   You tap them.
[01:48:46.080 --> 01:48:48.080]   They go, oh, yeah, that's true.
[01:48:48.080 --> 01:48:49.760]   I had a bumper that broke lately.
[01:48:49.760 --> 01:48:54.560]   I was in Israel last month though, and they've been trying to do the bumper car.
[01:48:54.560 --> 01:48:57.440]   They just did you do pin every car has a pin in it.
[01:48:57.440 --> 01:49:01.400]   So you not only do I had to, I rented a car and they're like, here's your pin, which
[01:49:01.400 --> 01:49:02.920]   was this is really interesting.
[01:49:02.920 --> 01:49:04.960]   You know, the Israelis are so practical.
[01:49:04.960 --> 01:49:07.040]   Because they had a problem with theft.
[01:49:07.040 --> 01:49:10.480]   And so they were like, this is your pin for the car.
[01:49:10.480 --> 01:49:12.200]   And they're like, you can't take that.
[01:49:12.200 --> 01:49:14.320]   Just take your phone out and take a picture of it.
[01:49:14.320 --> 01:49:16.400]   And I'm like, okay.
[01:49:16.400 --> 01:49:17.400]   And I take the pictures.
[01:49:17.400 --> 01:49:20.240]   Yeah, when you get in the car, you got to enter the pin.
[01:49:20.240 --> 01:49:24.280]   You put before you put the key in, and then you have to enter the pin.
[01:49:24.280 --> 01:49:25.280]   And then it lets you turn it on.
[01:49:25.280 --> 01:49:27.160]   And also if your drunk gets hard.
[01:49:27.160 --> 01:49:28.160]   And so that's true.
[01:49:28.160 --> 01:49:30.160]   That does make a difficult.
[01:49:30.160 --> 01:49:33.360]   You know, it is really, this is a little bit of a tangent, but it's really hard to debug
[01:49:33.360 --> 01:49:34.360]   cars.
[01:49:34.360 --> 01:49:42.280]   So when I drive, I drive the Lexus CT200H, which is no longer made, which is very sad.
[01:49:42.280 --> 01:49:47.040]   It's Lexus's baby, junior, hybrid, tiny car.
[01:49:47.040 --> 01:49:48.480]   It's built on the Prius drive train.
[01:49:48.480 --> 01:49:54.720]   And I'm leading you up to something, which is the key fob in my car and my car and some
[01:49:54.720 --> 01:49:58.480]   combination of those things has this bug in which everything's fine unless somebody who
[01:49:58.480 --> 01:50:00.960]   drives a Prius gets in my car with their key fob.
[01:50:00.960 --> 01:50:01.960]   What?
[01:50:01.960 --> 01:50:02.960]   Yep.
[01:50:02.960 --> 01:50:03.960]   Oh no.
[01:50:03.960 --> 01:50:04.960]   That's a good thing.
[01:50:04.960 --> 01:50:07.480]   My car thinks that my keys aren't there anymore.
[01:50:07.480 --> 01:50:08.480]   That's a good thing.
[01:50:08.480 --> 01:50:10.760]   It'll be like, keys not in the car.
[01:50:10.760 --> 01:50:14.160]   If my car engine is going, it will keep going.
[01:50:14.160 --> 01:50:17.360]   But if I haven't started my car yet, I actually have to make that person take their key fob
[01:50:17.360 --> 01:50:18.360]   out of the car.
[01:50:18.360 --> 01:50:19.360]   I need you to get out of the car.
[01:50:19.360 --> 01:50:20.360]   Yeah, I need you to get out of the car.
[01:50:20.360 --> 01:50:21.360]   Because the Prius has priority.
[01:50:21.360 --> 01:50:23.000]   The Prius apparently has priority.
[01:50:23.000 --> 01:50:27.080]   And this is not, I have a friend who has the same car and it happens to her car too.
[01:50:27.080 --> 01:50:28.080]   So they've Lexus has made this bug.
[01:50:28.080 --> 01:50:30.520]   And her car is many years newer than mine.
[01:50:30.520 --> 01:50:31.520]   Wow.
[01:50:31.520 --> 01:50:33.280]   So Lexus has had this bug for a long time.
[01:50:33.280 --> 01:50:34.280]   And not fixed it.
[01:50:34.280 --> 01:50:35.280]   It is super weird.
[01:50:35.280 --> 01:50:36.840]   Of the Prius Prius.
[01:50:36.840 --> 01:50:39.240]   The Prius Prius.
[01:50:39.240 --> 01:50:42.200]   Our show today brought to you by rocket mortgage from quick and loans.
[01:50:42.200 --> 01:50:45.200]   When you buy a house, you have a choice.
[01:50:45.200 --> 01:50:46.200]   You really do.
[01:50:46.200 --> 01:50:51.120]   You can go with the bank, which will treat it like, hey, welcome back to the 19th century.
[01:50:51.120 --> 01:50:55.400]   You got to put on a tie, a nice dress, go into the bank, say, please, sir, can I have
[01:50:55.400 --> 01:50:57.200]   some money?
[01:50:57.200 --> 01:50:58.200]   He'll look through.
[01:50:58.200 --> 01:50:59.920]   He's really, it's Dickensian.
[01:50:59.920 --> 01:51:02.000]   He'll look through a sheaf of papers.
[01:51:02.000 --> 01:51:07.280]   Well, I think you might qualify for this rate right here.
[01:51:07.280 --> 01:51:11.520]   They'll give you a big stack of papers to fill out an application form.
[01:51:11.520 --> 01:51:12.520]   You'll go back home.
[01:51:12.520 --> 01:51:13.520]   You'll have to go to the attic.
[01:51:13.520 --> 01:51:14.520]   You'll have to find pay stubs.
[01:51:14.520 --> 01:51:15.880]   You'll have to find bank statements.
[01:51:15.880 --> 01:51:20.120]   You'll probably have to call your bank saying, do you still have my bank statement from five
[01:51:20.120 --> 01:51:21.120]   years ago?
[01:51:21.120 --> 01:51:22.120]   Do you?
[01:51:22.120 --> 01:51:28.560]   You'll have to find a fax machine so that you can fax this stuff to Oliver Twist back
[01:51:28.560 --> 01:51:29.880]   at the office.
[01:51:29.880 --> 01:51:31.520]   That's nuts.
[01:51:31.520 --> 01:51:34.560]   Or you can go with the future.
[01:51:34.560 --> 01:51:37.840]   You can go with quick and loans, which is not only the biggest lender in the country.
[01:51:37.840 --> 01:51:42.080]   They're the best number one in customer satisfaction, eight years in a row, according
[01:51:42.080 --> 01:51:47.840]   to JD Power, and the most highly technologically savvy.
[01:51:47.840 --> 01:51:49.240]   Dan Harmon and his crew have always been.
[01:51:49.240 --> 01:51:50.520]   I mean, they're really amazing.
[01:51:50.520 --> 01:51:52.640]   They're revitalizing downtown Detroit.
[01:51:52.640 --> 01:51:53.960]   Quick and loans amazing company.
[01:51:53.960 --> 01:51:57.760]   And they realized that there was a better way because guess what?
[01:51:57.760 --> 01:52:00.720]   Here in the 21st century, we have the internet.
[01:52:00.720 --> 01:52:02.080]   We have computers.
[01:52:02.080 --> 01:52:03.920]   So they created rocket mortgage.
[01:52:03.920 --> 01:52:04.920]   You don't have to go to the bank.
[01:52:04.920 --> 01:52:05.920]   No, you can do this on your phone.
[01:52:05.920 --> 01:52:07.600]   You whip out your phone.
[01:52:07.600 --> 01:52:09.600]   Go to rocketmortgage.com/twit2.
[01:52:09.600 --> 01:52:11.080]   You don't even need a rocket mortgage app.
[01:52:11.080 --> 01:52:12.080]   I like that.
[01:52:12.080 --> 01:52:14.280]   Just do it in your browser rocketmortgage.com/twit2.
[01:52:14.280 --> 01:52:16.760]   Answer a few simple questions.
[01:52:16.760 --> 01:52:19.440]   You already know the answer to off the top of your head.
[01:52:19.440 --> 01:52:23.080]   They go out because they have relationships with all the financial institutions or quick
[01:52:23.080 --> 01:52:24.080]   and loans.
[01:52:24.080 --> 01:52:27.240]   They go out with your permission, get the information they need, crunch the numbers,
[01:52:27.240 --> 01:52:31.640]   and then based on income assets and credit, they will analyze all the loans for which
[01:52:31.640 --> 01:52:34.640]   you qualify for, give you the choice.
[01:52:34.640 --> 01:52:38.920]   You choose the rate, the term, the down payments, completely transparent, and you're approved.
[01:52:38.920 --> 01:52:41.560]   And all this took 10 minutes less.
[01:52:41.560 --> 01:52:42.560]   10 minutes.
[01:52:42.560 --> 01:52:44.880]   Like you didn't even have to get up off the couch.
[01:52:44.880 --> 01:52:46.360]   You could do this at an open house.
[01:52:46.360 --> 01:52:47.360]   Rocketmortgage.com/twit2.
[01:52:47.360 --> 01:52:53.320]   The next time you're going to buy or if you want to refi, they do refi's too.
[01:52:53.320 --> 01:52:55.640]   Go to rocketmortgage.com/twit2.
[01:52:55.640 --> 01:52:59.400]   You'll apply simply, you'll understand fully, you'll mortgage confidently.
[01:52:59.400 --> 01:53:05.160]   Equal housing, lender licensed in all 50 states and MLSconsumeraccess.org 3030.
[01:53:05.160 --> 01:53:06.160]   Book market.
[01:53:06.160 --> 01:53:08.520]   Rocketmortgage.com/twit2 or you know what?
[01:53:08.520 --> 01:53:09.520]   Even better.
[01:53:09.520 --> 01:53:12.280]   Just go there and start the process so that you'll be ready.
[01:53:12.280 --> 01:53:16.320]   If you go to an open house tomorrow and you see something you like, boom, you're minutes
[01:53:16.320 --> 01:53:17.680]   away from a home loan.
[01:53:17.680 --> 01:53:18.840]   You could show the realtor.
[01:53:18.840 --> 01:53:21.560]   It says here, we're approved.
[01:53:21.560 --> 01:53:23.760]   Rocketmortgage.com/twit2.
[01:53:23.760 --> 01:53:26.320]   We thank them for their support.
[01:53:26.320 --> 01:53:28.980]   Of Twit.
[01:53:28.980 --> 01:53:29.980]   This is depressing.
[01:53:29.980 --> 01:53:33.400]   80% of teenagers prefer iPhone to Android.
[01:53:33.400 --> 01:53:43.800]   80% and by the way, that's up from 78% last fall and 84% of teens say their next phone
[01:53:43.800 --> 01:53:45.000]   will be an iPhone.
[01:53:45.000 --> 01:53:46.000]   84%.
[01:53:46.000 --> 01:53:50.120]   I wonder if that has to do with their parents though and not them, right?
[01:53:50.120 --> 01:53:51.640]   Well, there's hair me downs, right?
[01:53:51.640 --> 01:53:54.240]   So their first phone might have been a hand me down.
[01:53:54.240 --> 01:53:55.240]   Right.
[01:53:55.240 --> 01:54:00.680]   So if that's been their only experience and Apple's been so big in schools that they're
[01:54:00.680 --> 01:54:03.720]   more likely to encounter an iPad or an Apple product than an iPhone.
[01:54:03.720 --> 01:54:04.960]   I don't think that's true.
[01:54:04.960 --> 01:54:07.400]   Chromebooks are winning in schools, by the way.
[01:54:07.400 --> 01:54:09.800]   Chrome is huge in schools everywhere.
[01:54:09.800 --> 01:54:10.960]   Chromebooks just Chromebooks.
[01:54:10.960 --> 01:54:13.800]   I think it depends on the school and the school system.
[01:54:13.800 --> 01:54:15.360]   No, no, actually stats bear this out.
[01:54:15.360 --> 01:54:16.360]   It is--
[01:54:16.360 --> 01:54:17.360]   Every class thing.
[01:54:17.360 --> 01:54:20.400]   Every school district we go to are looking at Chromebooks.
[01:54:20.400 --> 01:54:23.160]   A few are looking at Windows machines.
[01:54:23.160 --> 01:54:25.720]   Apple's education thing didn't change the--
[01:54:25.720 --> 01:54:27.360]   No, I mean, they're trying.
[01:54:27.360 --> 01:54:28.360]   And then, right?
[01:54:28.360 --> 01:54:32.080]   So iPads do really well in like K through three.
[01:54:32.080 --> 01:54:33.160]   Kids that are just coming in, right?
[01:54:33.160 --> 01:54:39.880]   Motor skills are an issue with keyboards in those ages.
[01:54:39.880 --> 01:54:45.080]   Apps are often very customized for those ages really well.
[01:54:45.080 --> 01:54:51.200]   Once they get into elementary school Chromebooks winning big time, the only place that we
[01:54:51.200 --> 01:54:58.080]   see Apple making any headway is in private schools where in high schools, they'll do
[01:54:58.080 --> 01:55:02.040]   Chromebooks up through eighth grade and then in high schools they'll do math.
[01:55:02.040 --> 01:55:06.200]   So what do you attribute then this kind of complete dominance of Apple?
[01:55:06.200 --> 01:55:09.480]   I think that the hand me down issue is a huge one.
[01:55:09.480 --> 01:55:10.480]   I think you're right.
[01:55:10.480 --> 01:55:11.480]   Sure.
[01:55:11.480 --> 01:55:12.480]   And I think the hand me downs are a big part of it.
[01:55:12.480 --> 01:55:13.480]   So adults--
[01:55:13.480 --> 01:55:17.080]   I'd say that 80% of adults would be using iPhones too, which I don't think is the case.
[01:55:17.080 --> 01:55:20.080]   I don't think so, but also--
[01:55:20.080 --> 01:55:21.800]   Peer pressure with friends.
[01:55:21.800 --> 01:55:24.000]   Peer pressure, the cameras.
[01:55:24.000 --> 01:55:25.520]   And I do think that there's--
[01:55:25.520 --> 01:55:29.160]   I think kids say, oh, you're using a Samsung ew.
[01:55:29.160 --> 01:55:30.160]   I don't know.
[01:55:30.160 --> 01:55:35.040]   I mean, again, parents always do this, so I'm going to apologize and then do it.
[01:55:35.040 --> 01:55:37.240]   We always are like, well, my kid.
[01:55:37.240 --> 01:55:38.240]   And so, my kid.
[01:55:38.240 --> 01:55:40.200]   Well, she's sitting right there, so it's OK.
[01:55:40.200 --> 01:55:41.200]   You're allowed.
[01:55:41.200 --> 01:55:42.200]   She may have taken a break, but--
[01:55:42.200 --> 01:55:43.200]   Oh, she left.
[01:55:43.200 --> 01:55:44.200]   I think I'll board with me.
[01:55:44.200 --> 01:55:45.760]   I said, I don't want to hear about it.
[01:55:45.760 --> 01:55:49.680]   No, my son is 14, and he has a little bit more agency of his own choice, and he has
[01:55:49.680 --> 01:55:50.680]   a pixel.
[01:55:50.680 --> 01:55:51.680]   Really?
[01:55:51.680 --> 01:55:52.680]   Good for him.
[01:55:52.680 --> 01:55:55.600]   Well, he's not 31, so he's like, I don't want those blue bubbles.
[01:55:55.600 --> 01:55:57.440]   I don't want you to know when I'm typing.
[01:55:57.440 --> 01:55:58.440]   That's very much--
[01:55:58.440 --> 01:55:59.440]   That's very much--
[01:55:59.440 --> 01:56:04.000]   But I do think that the find my iPhone ubiquity is what has driven parents to get
[01:56:04.000 --> 01:56:05.000]   iPhones for their kids.
[01:56:05.000 --> 01:56:06.000]   Oh, my gosh.
[01:56:06.000 --> 01:56:09.760]   Because even though there are apps to do that on Android, it is easy to do.
[01:56:09.760 --> 01:56:14.760]   It's easier to do it with a closed ecosystem of iPhones in your family to know where your
[01:56:14.760 --> 01:56:15.960]   kids are and track them.
[01:56:15.960 --> 01:56:18.960]   And I think parents love that.
[01:56:18.960 --> 01:56:23.400]   Wasn't there a study very recently that showed the decline of Apple among kids is not seeing
[01:56:23.400 --> 01:56:25.360]   that as a cool brand?
[01:56:25.360 --> 01:56:26.760]   I'm trying to find it.
[01:56:26.760 --> 01:56:27.760]   Yes.
[01:56:27.760 --> 01:56:28.760]   And it's not true.
[01:56:28.760 --> 01:56:31.600]   Yeah, I think at least they're not putting their money where their brand is.
[01:56:31.600 --> 01:56:34.280]   You know, kids are more fashion conscious, especially at that age.
[01:56:34.280 --> 01:56:37.600]   And iPhone is the biggest thing that you-- one of the biggest things you think about
[01:56:37.600 --> 01:56:42.840]   is what you look like and what you brands you wear and things like that.
[01:56:42.840 --> 01:56:49.200]   And an iPhone is a brand that is just like fashion among kids.
[01:56:49.200 --> 01:56:52.600]   Now, that said, those things change quickly.
[01:56:52.600 --> 01:57:00.560]   And you have to expect that at some point brands all run their life, have their life--
[01:57:00.560 --> 01:57:05.240]   Is there any chance the kids know that iPhones are more secure and more private than they--
[01:57:05.240 --> 01:57:06.240]   No.
[01:57:06.240 --> 01:57:07.240]   No.
[01:57:07.240 --> 01:57:10.520]   Also, you know what?
[01:57:10.520 --> 01:57:11.520]   I will--
[01:57:11.520 --> 01:57:13.280]   Spoken as parents, by the way.
[01:57:13.280 --> 01:57:15.280]   Make the argument that they don't care.
[01:57:15.280 --> 01:57:16.280]   They don't care.
[01:57:16.280 --> 01:57:17.280]   I think that for a lot of--
[01:57:17.280 --> 01:57:19.200]   Wait, they don't care about their privacy?
[01:57:19.200 --> 01:57:20.200]   Correct.
[01:57:20.200 --> 01:57:21.200]   That ship has sailed.
[01:57:21.200 --> 01:57:22.200]   They don't.
[01:57:22.200 --> 01:57:25.600]   They know that they're like, yeah, I know that everybody's spying on me.
[01:57:25.600 --> 01:57:29.040]   I think they care deeply about their privacy from their parents.
[01:57:29.040 --> 01:57:33.200]   I think they care deeply about privacy from certain clicks in kids at school.
[01:57:33.200 --> 01:57:35.200]   Yes, personal privacy.
[01:57:35.200 --> 01:57:39.240]   Talking about the kind of privacy we were discussing at the beginning of this show, right?
[01:57:39.240 --> 01:57:40.240]   Data privacy?
[01:57:40.240 --> 01:57:44.360]   I don't think they have any expectation of that.
[01:57:44.360 --> 01:57:48.160]   Dana Boyd says it's complicated.
[01:57:48.160 --> 01:57:49.960]   It's never easy.
[01:57:49.960 --> 01:57:50.960]   It is.
[01:57:50.960 --> 01:57:51.960]   It's interesting.
[01:57:51.960 --> 01:57:57.880]   She makes it her business to study teenagers in privacy.
[01:57:57.880 --> 01:57:58.880]   And you're right.
[01:57:58.880 --> 01:58:00.160]   You're both right.
[01:58:00.160 --> 01:58:07.640]   She says teenagers understand how they aren't living privately.
[01:58:07.640 --> 01:58:12.320]   But at the same time, they do many things to obfuscate.
[01:58:12.320 --> 01:58:14.160]   They don't use their real name on Facebook.
[01:58:14.160 --> 01:58:16.120]   They all have many different Instagram accounts.
[01:58:16.120 --> 01:58:19.360]   And they only tell their parents about one of them.
[01:58:19.360 --> 01:58:23.880]   So I think her main point-- I hope I'm not misstating it.
[01:58:23.880 --> 01:58:30.120]   But her main point is that young people don't think about privacy the same way.
[01:58:30.120 --> 01:58:33.280]   As we think about privacy.
[01:58:33.280 --> 01:58:37.000]   Remember when a couple months ago, Facebook was launching a product for kids?
[01:58:37.000 --> 01:58:38.000]   Yeah, yeah.
[01:58:38.000 --> 01:58:40.880]   Facebook Messenger for the under-13 set.
[01:58:40.880 --> 01:58:43.400]   Boy, I wonder what happened to that.
[01:58:43.400 --> 01:58:44.400]   Really?
[01:58:44.400 --> 01:58:47.080]   They're not talking about it very much right now.
[01:58:47.080 --> 01:58:49.320]   We can't really get those kids.
[01:58:49.320 --> 01:58:50.920]   Yeah, expand the "its" case.
[01:58:50.920 --> 01:58:51.920]   Yeah, exactly.
[01:58:51.920 --> 01:58:56.200]   That's just kind of amazing how quickly we live our lives these days, where if you remember
[01:58:56.200 --> 01:59:01.520]   that debate was raging on and on and on, it's like everybody's forgotten about it already.
[01:59:01.520 --> 01:59:05.720]   This boy says, "I've been overwhelmingly told kids these days don't care about privacy.
[01:59:05.720 --> 01:59:09.240]   And yet when I wandered around talking to young people, I found young people care deeply
[01:59:09.240 --> 01:59:13.680]   about privacy, even in an online environment.
[01:59:13.680 --> 01:59:19.000]   But how they strive to achieve that privacy is sometimes puzzling to outsiders."
[01:59:19.000 --> 01:59:23.840]   In other words, I think a lot of us look and say, "Well, you're using Snapchat non-stop."
[01:59:23.840 --> 01:59:26.160]   What do you mean-- what is the privacy?
[01:59:26.160 --> 01:59:31.320]   They want privacy from authority figures.
[01:59:31.320 --> 01:59:33.000]   Parents, teachers, social workers.
[01:59:33.000 --> 01:59:37.760]   They don't need privacy with their peers.
[01:59:37.760 --> 01:59:45.400]   The thing is, this is very much the way it was when previous generations, before there
[01:59:45.400 --> 01:59:50.680]   was social media, you went to school and kind of what you did at school and said at school.
[01:59:50.680 --> 01:59:52.200]   Most of it kind of stayed there, right?
[01:59:52.200 --> 01:59:53.680]   It was your sort of world.
[01:59:53.680 --> 01:59:56.120]   It was your kind of bubble.
[01:59:56.120 --> 02:00:01.320]   And there's a little bit of a healthy part of that.
[02:00:01.320 --> 02:00:07.240]   It's kids, your parents are both teachers at your school.
[02:00:07.240 --> 02:00:11.080]   That is rough.
[02:00:11.080 --> 02:00:15.760]   If you, kids have that little bit of space because it's part of them developing their
[02:00:15.760 --> 02:00:20.400]   own identity, I don't know being their own sense of self, and in a social media world
[02:00:20.400 --> 02:00:24.560]   where everybody's aware of everything you're doing, they do lose that.
[02:00:24.560 --> 02:00:28.760]   And so you have to look at it in one sense of them recreating it in the world that we
[02:00:28.760 --> 02:00:35.440]   live in, and that's a natural part of them establishing their own identity.
[02:00:35.440 --> 02:00:36.920]   I love this story.
[02:00:36.920 --> 02:00:42.160]   Apple sent out a memo to employees saying, "You got to stop leaking information.
[02:00:42.160 --> 02:00:46.160]   And in fact, last year we caught 29 leakers.
[02:00:46.160 --> 02:00:48.560]   12 of them were arrested."
[02:00:48.560 --> 02:00:52.400]   The memo said, "These people not only lose their jobs, they can face extreme difficulty
[02:00:52.400 --> 02:00:54.080]   finding employment elsewhere."
[02:00:54.080 --> 02:00:58.560]   I mean, this was a thinly veiled threat.
[02:00:58.560 --> 02:01:05.880]   They pointed out that you could be prosecuted using federal laws against network intrusion.
[02:01:05.880 --> 02:01:09.920]   You could face jail time, be convicted of a felony.
[02:01:09.920 --> 02:01:12.200]   And of course, the memo was immediately leaked.
[02:01:12.200 --> 02:01:15.960]   What's the difference between, I wonder what the line is, and I don't know the answer to
[02:01:15.960 --> 02:01:22.120]   this, but what's the line between whistleblowing and whistleblowing?
[02:01:22.120 --> 02:01:26.160]   And committing acts of corporate espionage.
[02:01:26.160 --> 02:01:33.280]   We may find out that there's a Google employee who was dismissed because he released a memo
[02:01:33.280 --> 02:01:35.880]   that was a corporate memo and he released it to the public.
[02:01:35.880 --> 02:01:37.520]   It was caught.
[02:01:37.520 --> 02:01:40.640]   These companies are very smart about how they catch people.
[02:01:40.640 --> 02:01:47.400]   And now he's suing, saying, "This is wrongful termination.
[02:01:47.400 --> 02:01:52.720]   You are violating my free speech."
[02:01:52.720 --> 02:01:54.640]   But I wonder how a court will react to that.
[02:01:54.640 --> 02:01:58.000]   I don't think you have a right to free speech inside a company.
[02:01:58.000 --> 02:01:59.000]   No.
[02:01:59.000 --> 02:02:00.200]   There's corporate secrets.
[02:02:00.200 --> 02:02:01.640]   But you raise a good point.
[02:02:01.640 --> 02:02:06.400]   We also, courts also have to protect the right of somebody to be a whistleblower to release
[02:02:06.400 --> 02:02:07.400]   that.
[02:02:07.400 --> 02:02:08.840]   And it may come down, you may be right.
[02:02:08.840 --> 02:02:12.200]   I think that's really an interesting question, and it may come down to intent.
[02:02:12.200 --> 02:02:13.200]   Why did you leak it?
[02:02:13.200 --> 02:02:16.600]   Did you leak it because you just like being part of a rumor mill?
[02:02:16.600 --> 02:02:18.760]   Did you leak it because you want to feel important?
[02:02:18.760 --> 02:02:21.880]   Or did you leak it because you think that there's something seriously going wrong at
[02:02:21.880 --> 02:02:25.320]   the company where you are and you've tried to deal with it internally?
[02:02:25.320 --> 02:02:27.640]   Nobody has taken it seriously, so you go to the press.
[02:02:27.640 --> 02:02:35.280]   Is there any other moment in history when I feel like 2017, 2018 will partially be remembered
[02:02:35.280 --> 02:02:37.200]   as the time we leaked?
[02:02:37.200 --> 02:02:41.480]   Is there other times in history where it was this prevalent?
[02:02:41.480 --> 02:02:45.360]   I can't think of any other analogous times.
[02:02:45.360 --> 02:02:46.800]   You're talking about governmental leaks?
[02:02:46.800 --> 02:02:47.800]   Are you talking about?
[02:02:47.800 --> 02:02:53.200]   Just like people, which I guess is part of just being alive in the age of social media.
[02:02:53.200 --> 02:02:55.560]   Yeah, we just have better ways to leak.
[02:02:55.560 --> 02:02:57.200]   We can leak all over the place.
[02:02:57.200 --> 02:03:01.760]   I might not do that unless you get scraped on the back end.
[02:03:01.760 --> 02:03:02.760]   Yeah.
[02:03:02.760 --> 02:03:06.000]   Well, sometimes it's hard not to leak if you get scraped on the back end.
[02:03:06.000 --> 02:03:07.000]   Here's in the Apple memo.
[02:03:07.000 --> 02:03:08.000]   I love this.
[02:03:08.000 --> 02:03:11.120]   In many cases, in many cases, we understand.
[02:03:11.120 --> 02:03:12.120]   Right, Apple.
[02:03:12.120 --> 02:03:13.680]   Leakers don't set out to leak.
[02:03:13.680 --> 02:03:19.480]   Instead, people who work for Apple are often targeted by press analysts.
[02:03:19.480 --> 02:03:20.480]   I love this part.
[02:03:20.480 --> 02:03:21.480]   I love this part.
[02:03:21.480 --> 02:03:25.720]   Who befriend them on professional social networks like LinkedIn, Twitter, and Facebook
[02:03:25.720 --> 02:03:28.400]   and begin to pry for information.
[02:03:28.400 --> 02:03:34.000]   While that may seem flattering to be approached, it's important to remember you're getting
[02:03:34.000 --> 02:03:35.000]   played.
[02:03:35.000 --> 02:03:38.960]   I would I laughed out loud when I read this.
[02:03:38.960 --> 02:03:45.720]   What's also funny is you could write this to members of the press, analysts and bloggers
[02:03:45.720 --> 02:03:48.120]   who receive calls from Apple executives.
[02:03:48.120 --> 02:03:52.520]   Just remember, it might be flattering to get a call from an Apple executive, but you're
[02:03:52.520 --> 02:03:53.520]   getting played.
[02:03:53.520 --> 02:03:57.960]   Oh my, I read half that stuff on the news stories on the Wall Street Journal.
[02:03:57.960 --> 02:03:59.800]   They're going to play.
[02:03:59.800 --> 02:04:04.160]   Apple leaks all the time intentionally into the Wall Street Journal.
[02:04:04.160 --> 02:04:05.160]   All the time.
[02:04:05.160 --> 02:04:10.840]   I think that's my conversation with the CEO recently.
[02:04:10.840 --> 02:04:15.760]   They were talking about how does Apple ever actually leak anything to the press?
[02:04:15.760 --> 02:04:16.760]   I'm like, they are notorious.
[02:04:16.760 --> 02:04:19.520]   I said, I'll give you an example.
[02:04:19.520 --> 02:04:24.760]   If you remember when the Apple Watch was first coming out, there were all these crazy
[02:04:24.760 --> 02:04:25.760]   analyst estimates.
[02:04:25.760 --> 02:04:30.680]   They're going to sell 30 million in iPhones.
[02:04:30.680 --> 02:04:32.520]   I can't remember what year it was.
[02:04:32.520 --> 02:04:34.400]   2015, maybe it came out.
[02:04:34.400 --> 02:04:35.760]   It was like, there's no way.
[02:04:35.760 --> 02:04:38.760]   You looked at even the numbers of Fitbits and other things, and it was going to be a
[02:04:38.760 --> 02:04:40.440]   super set.
[02:04:40.440 --> 02:04:43.600]   It's a luxury version of that.
[02:04:43.600 --> 02:04:44.800]   It was like, there's no way.
[02:04:44.800 --> 02:04:52.640]   Then all of a sudden, about a month before it was going to come out, the story in the
[02:04:52.640 --> 02:05:00.080]   Wall Street Journal about Apple reportedly has this internal memo that they only expect
[02:05:00.080 --> 02:05:04.040]   to sell 11 million or something like that.
[02:05:04.040 --> 02:05:07.960]   They're leaking that because they want to lower expectations so that they can define
[02:05:07.960 --> 02:05:09.200]   what success looks like.
[02:05:09.200 --> 02:05:12.800]   In fact, those of us who cover Apple pretty much can tell immediately from a Wall Street
[02:05:12.800 --> 02:05:16.840]   Journal headline whether this is Apple's spin on it or this is something some digging
[02:05:16.840 --> 02:05:17.840]   that they've done.
[02:05:17.840 --> 02:05:21.760]   Yeah, I felt like that and maybe part of the reason it was leaked.
[02:05:21.760 --> 02:05:23.440]   That memo might have been, who knows?
[02:05:23.440 --> 02:05:29.080]   It might have been intentionally leaked as a warning as a warning to anybody else in
[02:05:29.080 --> 02:05:30.080]   a bar of journalists.
[02:05:30.080 --> 02:05:31.080]   It's a very good point.
[02:05:31.080 --> 02:05:33.040]   We want to control our message and you stop meddling with us.
[02:05:33.040 --> 02:05:34.040]   They're playing me 29 people.
[02:05:34.040 --> 02:05:37.600]   And so that you can all see how seriously we take this.
[02:05:37.600 --> 02:05:38.600]   29 people.
[02:05:38.600 --> 02:05:41.920]   Out of 123,000 employees by the way.
[02:05:41.920 --> 02:05:44.360]   Hey, 12 of them are arrested.
[02:05:44.360 --> 02:05:45.840]   The police came.
[02:05:45.840 --> 02:05:51.640]   I mean, it is true that Apple is really focused on controlling its supply chain.
[02:05:51.640 --> 02:05:56.080]   And what it says in that memo, somewhere in that memo basically says, essentially, if
[02:05:56.080 --> 02:06:00.960]   you leak what we're doing, then journalists get to say what we're doing instead of us
[02:06:00.960 --> 02:06:01.960]   saying it better.
[02:06:01.960 --> 02:06:02.960]   Right.
[02:06:02.960 --> 02:06:03.960]   And leave the leaking to us.
[02:06:03.960 --> 02:06:06.880]   You know, I mean, it is very important to them to control the message.
[02:06:06.880 --> 02:06:09.640]   Well, of course, every company, every company, right?
[02:06:09.640 --> 02:06:10.640]   Especially Apple.
[02:06:10.640 --> 02:06:11.960]   Because they have more than any.
[02:06:11.960 --> 02:06:17.080]   They spend more time than any other company that I know crafting the way they're going
[02:06:17.080 --> 02:06:18.080]   to tell their story.
[02:06:18.080 --> 02:06:19.080]   Yeah.
[02:06:19.080 --> 02:06:20.080]   That's why they're better at it.
[02:06:20.080 --> 02:06:23.480]   I mean, they are frankly better at it telling their story than other companies.
[02:06:23.480 --> 02:06:25.080]   And so that undermines them.
[02:06:25.080 --> 02:06:26.640]   And they do have perspective.
[02:06:26.640 --> 02:06:29.920]   Pet journalist Matthew Panzarena of TechCrunch invited back to Apple.
[02:06:29.920 --> 02:06:34.600]   Last year he was part of the five who got invited for the Apple apology tour.
[02:06:34.600 --> 02:06:36.480]   We made the Mac Pro, but we blew it.
[02:06:36.480 --> 02:06:37.800]   We painted ourselves into a corner.
[02:06:37.800 --> 02:06:39.560]   We can't make another one.
[02:06:39.560 --> 02:06:41.360]   Not this year, but we're working on it.
[02:06:41.360 --> 02:06:42.360]   Not this year.
[02:06:42.360 --> 02:06:43.640]   Panzarena got invited to now.
[02:06:43.640 --> 02:06:48.760]   I don't, I would love to know, Matthew, if you're watching a great reporting, but I'd
[02:06:48.760 --> 02:06:53.480]   like to know if Apple called you and said, would you like to come to the campus and see
[02:06:53.480 --> 02:06:54.720]   what we're doing in the Mac Pro?
[02:06:54.720 --> 02:06:59.480]   Or if Matthew did some digging in any event, he did go to the campus and was shown, you
[02:06:59.480 --> 02:07:02.000]   know, all the work Apple's doing.
[02:07:02.000 --> 02:07:06.320]   And Apple used this as an opportunity to say, and by the way, it's not coming out this year.
[02:07:06.320 --> 02:07:08.760]   We're hoping 2019.
[02:07:08.760 --> 02:07:13.760]   And on the one hand, Matthew said, well, you know, I'm glad that Apple's telling people
[02:07:13.760 --> 02:07:18.320]   who are now in this process of thinking about what Mac to buy.
[02:07:18.320 --> 02:07:20.520]   Don't wait, buy it next year.
[02:07:20.520 --> 02:07:21.520]   That's a good thing.
[02:07:21.520 --> 02:07:26.360]   But it's also a very self-serving thing because Apple's saying buy an iMac Pro right now because
[02:07:26.360 --> 02:07:28.120]   we're going to have the Mac Pro for you.
[02:07:28.120 --> 02:07:29.120]   It's really, yeah.
[02:07:29.120 --> 02:07:31.000]   I mean, it always cuts both ways.
[02:07:31.000 --> 02:07:35.440]   And in this case, Apple's very much manipulating people saying, oh, yeah, don't wait, buy a
[02:07:35.440 --> 02:07:38.320]   Mac Pro now and we'll give you something next year to buy.
[02:07:38.320 --> 02:07:40.680]   They're not selling many iMac Pros, by the way.
[02:07:40.680 --> 02:07:42.040]   Yeah, I mean, that's our audience.
[02:07:42.040 --> 02:07:43.040]   You know, wouldn't.
[02:07:43.040 --> 02:07:46.720]   Yeah, I bought one, but that's what I'm kind of not am I crazy.
[02:07:46.720 --> 02:07:48.600]   But you did it for science, right?
[02:07:48.600 --> 02:07:49.600]   You're not using it.
[02:07:49.600 --> 02:07:50.600]   I have to review it.
[02:07:50.600 --> 02:07:55.400]   But then I get the distinct pleasure of getting to use it.
[02:07:55.400 --> 02:07:56.920]   Just what I need at home.
[02:07:56.920 --> 02:07:58.680]   Everyone's fairly fast.
[02:07:58.680 --> 02:08:00.160]   32 gigs of RAM.
[02:08:00.160 --> 02:08:01.160]   You know what?
[02:08:01.160 --> 02:08:02.160]   It's not a you can't tell it's fast.
[02:08:02.160 --> 02:08:03.840]   You don't run on Facebook anyway.
[02:08:03.840 --> 02:08:06.000]   You killed Facebook a little last time.
[02:08:06.000 --> 02:08:11.600]   But I hope you know, okay, I'm going to be I'm going to be honest here.
[02:08:11.600 --> 02:08:15.040]   I have gone through several stages of grief with Facebook.
[02:08:15.040 --> 02:08:18.800]   So a couple of weeks ago, I told you I I deleted my account.
[02:08:18.800 --> 02:08:20.840]   You get two weeks and this is new.
[02:08:20.840 --> 02:08:25.400]   This is the GDPR thing is that you can actually say delete it and they give you two weeks
[02:08:25.400 --> 02:08:26.760]   to change your mind.
[02:08:26.760 --> 02:08:30.960]   My wife said I mentioned this last week, but now I'm not married to anybody.
[02:08:30.960 --> 02:08:35.240]   It just says I'm married, but not to anybody because my name's gone.
[02:08:35.240 --> 02:08:36.240]   Right.
[02:08:36.240 --> 02:08:39.960]   So I said, oh, and I brought it back and then I went, no.
[02:08:39.960 --> 02:08:41.640]   So I deleted it again.
[02:08:41.640 --> 02:08:43.800]   And then get a new two weeks.
[02:08:43.800 --> 02:08:45.120]   You get a new two weeks.
[02:08:45.120 --> 02:08:47.400]   Every time you do this, you get a new two weeks.
[02:08:47.400 --> 02:08:50.440]   So then I thought, well, I shouldn't really delete it because what if somebody steals
[02:08:50.440 --> 02:08:52.880]   my name or I don't know.
[02:08:52.880 --> 02:08:59.840]   So I am now currently I'm deactivated, but it's pointless in every respect.
[02:08:59.840 --> 02:09:03.040]   A, even if I deleted it, Facebook has my data.
[02:09:03.040 --> 02:09:04.280]   It's too late.
[02:09:04.280 --> 02:09:05.280]   It's too late.
[02:09:05.280 --> 02:09:11.480]   B, this is in any way register is a blip of any kind in Facebook's bottom line.
[02:09:11.480 --> 02:09:12.880]   Absolutely not.
[02:09:12.880 --> 02:09:18.440]   More people join Facebook in the time it took to tell you this than everybody who quit
[02:09:18.440 --> 02:09:20.240]   totally so far, right?
[02:09:20.240 --> 02:09:22.600]   Delete in the delete Facebook movement.
[02:09:22.600 --> 02:09:24.080]   So I've got, I've accomplished nothing.
[02:09:24.080 --> 02:09:25.520]   Mark Zuckerberg's not getting a signal.
[02:09:25.520 --> 02:09:26.680]   I've accomplished nothing.
[02:09:26.680 --> 02:09:31.000]   It just makes me feel better because I frankly, it's purely emotional.
[02:09:31.000 --> 02:09:33.000]   I think he's kind of creepy.
[02:09:33.000 --> 02:09:35.440]   And I think Facebook's kind of creepy.
[02:09:35.440 --> 02:09:38.200]   More so than anybody else, more than Google, more than anybody else.
[02:09:38.200 --> 02:09:39.960]   Well, that's why I deleted the app from my phone.
[02:09:39.960 --> 02:09:41.960]   It wasn't, it wasn't out of protest.
[02:09:41.960 --> 02:09:44.240]   No, it was out of, it was a mental health move.
[02:09:44.240 --> 02:09:45.240]   It's mental health.
[02:09:45.240 --> 02:09:46.240]   Yeah.
[02:09:46.240 --> 02:09:47.240]   I just don't go.
[02:09:47.240 --> 02:09:49.240]   I haven't deleted my account, but I kind of just thought show up there.
[02:09:49.240 --> 02:09:50.840]   I've been deleting Facebook since 2010.
[02:09:50.840 --> 02:09:51.840]   I'm fresh.
[02:09:51.840 --> 02:09:57.800]   But I have been, I have been, you know, that's the place that I tell people to really be
[02:09:57.800 --> 02:10:00.600]   careful about what you do more than any of the others.
[02:10:00.600 --> 02:10:04.200]   Well, and every set you go to, whether you delete Facebook or not, registers, you know,
[02:10:04.200 --> 02:10:08.360]   if it's got a Facebook like or a login, registers your presence.
[02:10:08.360 --> 02:10:13.400]   So I got to, I just got to say, you know, over the past couple of weeks, there have been
[02:10:13.400 --> 02:10:18.320]   all these how to guides, how to delete Facebook, how to check if you work compromised.
[02:10:18.320 --> 02:10:19.320]   Delete Facebook.
[02:10:19.320 --> 02:10:22.480]   And other people, like my favorite one is how to have the talk.
[02:10:22.480 --> 02:10:23.480]   Yes.
[02:10:23.480 --> 02:10:26.760]   If your data was stolen, how to have that talk with your friends, we are literally talking
[02:10:26.760 --> 02:10:31.440]   about personal data in the same, using the same language in terms that we use to talk
[02:10:31.440 --> 02:10:32.440]   about STDs.
[02:10:32.440 --> 02:10:33.440]   It is.
[02:10:33.440 --> 02:10:34.440]   Right?
[02:10:34.440 --> 02:10:35.440]   It is.
[02:10:35.440 --> 02:10:42.560]   But, you know, we're not like engaging in like the equivalent of safe sex when we, you
[02:10:42.560 --> 02:10:46.120]   know, we're not protecting ourselves online in any real data hygiene.
[02:10:46.120 --> 02:10:47.440]   We definitely need data.
[02:10:47.440 --> 02:10:51.040]   It's like we're telling, we're telling people how to say you've got chlamydia without actually
[02:10:51.040 --> 02:10:52.280]   telling them how to use a content.
[02:10:52.280 --> 02:10:53.280]   It's so funny.
[02:10:53.280 --> 02:10:54.280]   It's because the new chlamydia.
[02:10:54.280 --> 02:10:56.360]   It's so funny.
[02:10:56.360 --> 02:11:00.480]   This is exactly, is exactly what I've been thinking about is.
[02:11:00.480 --> 02:11:02.320]   Because we get scraped on the back end.
[02:11:02.320 --> 02:11:05.320]   Unless you get scraped on the back end.
[02:11:05.320 --> 02:11:07.560]   This is exactly what I've been thinking about with this topic.
[02:11:07.560 --> 02:11:11.080]   I mean, this is so funny because I was thinking that's the content that's needed is like good
[02:11:11.080 --> 02:11:12.080]   data hygiene.
[02:11:12.080 --> 02:11:16.840]   So we've actually been doing a little bit on this and planning to do more because yeah,
[02:11:16.840 --> 02:11:17.840]   that's what's needed, right?
[02:11:17.840 --> 02:11:22.120]   Is the positive proactive angle?
[02:11:22.120 --> 02:11:26.240]   What does good data hygiene look like online for an individual?
[02:11:26.240 --> 02:11:28.200]   And then how do you teach that to kids?
[02:11:28.200 --> 02:11:31.400]   Do you teach people how to write your account deletion epitaph?
[02:11:31.400 --> 02:11:35.640]   And there is, there's such a thirst for that knowledge and that really important knowledge.
[02:11:35.640 --> 02:11:39.700]   I mean, I just wrote a little for my friends, a little thing on Facebook right after the
[02:11:39.700 --> 02:11:41.600]   camera, Jan, Lina, and the news started.
[02:11:41.600 --> 02:11:44.040]   And I was like, Hey guys, here's something to know.
[02:11:44.040 --> 02:11:49.880]   Like if you ever want to take a quiz or any take anything that looks like it is doing
[02:11:49.880 --> 02:11:51.640]   nothing in particular for itself.
[02:11:51.640 --> 02:11:55.440]   I was like, consider how that app within Facebook makes its money.
[02:11:55.440 --> 02:11:56.440]   Yeah.
[02:11:56.440 --> 02:11:57.440]   Is it?
[02:11:57.440 --> 02:11:58.440]   Yes.
[02:11:58.440 --> 02:12:00.240]   Is it obvious how this is making its money?
[02:12:00.240 --> 02:12:01.240]   Yes.
[02:12:01.240 --> 02:12:04.960]   If it's not obvious how it's making its money, it's making its money off of you and what you're
[02:12:04.960 --> 02:12:05.960]   sharing.
[02:12:05.960 --> 02:12:09.560]   So just think about that every single time you use something and when you have small
[02:12:09.560 --> 02:12:13.080]   children, you have to teach them how to wipe their Facebook.
[02:12:13.080 --> 02:12:14.080]   I think that's important.
[02:12:14.080 --> 02:12:16.760]   But my friends were so thankful.
[02:12:16.760 --> 02:12:19.400]   They were so like, Oh my gosh, I hadn't thought about it that way.
[02:12:19.400 --> 02:12:20.400]   Yeah.
[02:12:20.400 --> 02:12:21.400]   Okay.
[02:12:21.400 --> 02:12:22.400]   My business works.
[02:12:22.400 --> 02:12:23.400]   Yeah.
[02:12:23.400 --> 02:12:24.640]   Do your friends post photos of their kids on Facebook?
[02:12:24.640 --> 02:12:25.640]   Oh, sure.
[02:12:25.640 --> 02:12:26.640]   Yeah.
[02:12:26.640 --> 02:12:27.640]   All right.
[02:12:27.640 --> 02:12:32.200]   So then what the I was then then, then, then, kind of like, we need to have the talk.
[02:12:32.200 --> 02:12:35.680]   I've been posting your picture on Facebook for 13 years.
[02:12:35.680 --> 02:12:36.680]   Right.
[02:12:36.680 --> 02:12:41.760]   I mean, it's kind of like, you know, you don't put your picture, like if you're actually
[02:12:41.760 --> 02:12:47.000]   concerned about this stuff, then you have to exercise himself for strain and don't put
[02:12:47.000 --> 02:12:51.160]   pictures of your kids' twoshes in the bathtub on the Facebook.
[02:12:51.160 --> 02:12:53.480]   You know, I mean, it's part of this.
[02:12:53.480 --> 02:12:56.440]   This has been sort of a threat, a through line for this entire show tonight.
[02:12:56.440 --> 02:13:01.200]   But a lot of this just comes back to like, life moves pretty fast.
[02:13:01.200 --> 02:13:04.400]   Technology moves faster than we do.
[02:13:04.400 --> 02:13:08.360]   And we got to, you know, stop and think about what we're doing.
[02:13:08.360 --> 02:13:09.840]   You know who else ought to move fast?
[02:13:09.840 --> 02:13:15.840]   People in Mountain View, because Waymo has just received a permit for no driver testing
[02:13:15.840 --> 02:13:20.120]   in California and Mountain View is the first place.
[02:13:20.120 --> 02:13:21.120]   Watch out.
[02:13:21.120 --> 02:13:22.560]   There's Lexus is driving down the street.
[02:13:22.560 --> 02:13:25.320]   There's nobody inside.
[02:13:25.320 --> 02:13:27.360]   And I bet if it hits you, it just keeps going.
[02:13:27.360 --> 02:13:28.360]   Right?
[02:13:28.360 --> 02:13:31.280]   Who said they're driving what they've got like a trailer car behind it?
[02:13:31.280 --> 02:13:35.040]   No, they're they talk about moving too fast.
[02:13:35.040 --> 02:13:38.800]   We just, you know, had this Uber issue, the Tesla issue.
[02:13:38.800 --> 02:13:43.040]   Waymo confirmed on Friday, the California Department of Motor Vehicles has approved its
[02:13:43.040 --> 02:13:47.520]   permit to test without a safety driver behind the wheel.
[02:13:47.520 --> 02:13:49.160]   That's lobbying at work.
[02:13:49.160 --> 02:13:50.160]   That's amazing.
[02:13:50.160 --> 02:13:51.160]   Wow.
[02:13:51.160 --> 02:13:52.160]   Amazing.
[02:13:52.160 --> 02:13:53.160]   Yeah.
[02:13:53.160 --> 02:13:54.160]   Yeah.
[02:13:54.160 --> 02:13:58.400]   So if you see a Chrysler minivan with strange accoutrement on the roof coming at you, Ron,
[02:13:58.400 --> 02:14:00.400]   there's somebody at the wheel.
[02:14:00.400 --> 02:14:02.040]   That's actually, you know, it's interesting though.
[02:14:02.040 --> 02:14:07.320]   I wonder, I bet you nobody at the California Department of Motor Vehicles asked in depth
[02:14:07.320 --> 02:14:13.360]   questions about how secure the system is and if it's vulnerable to intrusion outside
[02:14:13.360 --> 02:14:14.360]   in Cleveland.
[02:14:14.360 --> 02:14:16.080]   No, I doubt it.
[02:14:16.080 --> 02:14:21.640]   I have to say, I mean, Waymo's done a lot better job than Uber with self-driving Uber.
[02:14:21.640 --> 02:14:28.200]   I think the Uber record of average was a driver had to take over every 13 miles, had
[02:14:28.200 --> 02:14:30.000]   overrided the system.
[02:14:30.000 --> 02:14:32.840]   And Waymo is something like, I can't remember what it was, hundreds of times.
[02:14:32.840 --> 02:14:37.080]   Well, Uber's having a hard time with object and image recognition and making some mistakes.
[02:14:37.080 --> 02:14:38.080]   Yeah.
[02:14:38.080 --> 02:14:39.640]   And I think this all comes back again.
[02:14:39.640 --> 02:14:41.920]   Like Waymo is part of a data company.
[02:14:41.920 --> 02:14:42.920]   Yeah.
[02:14:42.920 --> 02:14:44.920]   Uber is a transportation company that uses data.
[02:14:44.920 --> 02:14:45.920]   Right.
[02:14:45.920 --> 02:14:46.920]   It's kind of interesting.
[02:14:46.920 --> 02:14:52.920]   And I sort of wonder if part of Waymo's lobbying effort was just like, well, look at the statistics.
[02:14:52.920 --> 02:14:57.720]   We have actually many hours and miles of safe testing on a closed course.
[02:14:57.720 --> 02:15:00.120]   Yeah, they've been doing this for a long time.
[02:15:00.120 --> 02:15:01.120]   But you know what?
[02:15:01.120 --> 02:15:03.720]   Let's all hand it to the California Department of Motor Vehicles.
[02:15:03.720 --> 02:15:07.240]   That's probably literally the fastest they've ever moved on anything.
[02:15:07.240 --> 02:15:08.240]   Right.
[02:15:08.240 --> 02:15:09.440]   So we know it can jump.
[02:15:09.440 --> 02:15:11.240]   We're all standing online.
[02:15:11.240 --> 02:15:13.800]   It's really true.
[02:15:13.800 --> 02:15:20.400]   Waymo says that on average, once every 5,600 miles, the safety driver has had to take over
[02:15:20.400 --> 02:15:22.360]   compared to Uber's every 13 miles.
[02:15:22.360 --> 02:15:23.360]   It's pretty remarkable.
[02:15:23.360 --> 02:15:24.360]   Yeah, let me get that.
[02:15:24.360 --> 02:15:26.160]   Well, they've been at it for a long time though.
[02:15:26.160 --> 02:15:30.720]   Remember, this is a secret project inside Google for a long time before we knew about
[02:15:30.720 --> 02:15:31.720]   it.
[02:15:31.720 --> 02:15:32.720]   And then we've also known about it for a long time.
[02:15:32.720 --> 02:15:33.720]   Google is alphabet.
[02:15:33.720 --> 02:15:34.720]   Right.
[02:15:34.720 --> 02:15:35.720]   Before Google's alphabet, thank you.
[02:15:35.720 --> 02:15:40.040]   Should point out though that with no safety driver, there's not going to be somebody there
[02:15:40.040 --> 02:15:43.200]   every 5,600 miles to correct the car.
[02:15:43.200 --> 02:15:48.360]   I wonder who's ensuring them because there's no actual-- we don't have the insurance stuff
[02:15:48.360 --> 02:15:49.360]   figured out yet.
[02:15:49.360 --> 02:15:50.360]   I wonder how--
[02:15:50.360 --> 02:15:51.360]   Google's probably self-insuring.
[02:15:51.360 --> 02:15:53.200]   Yeah, I was just thinking the same thing.
[02:15:53.200 --> 02:15:54.200]   Google insurance company.
[02:15:54.200 --> 02:15:55.200]   I think so.
[02:15:55.200 --> 02:15:57.280]   Are we saying they have some money?
[02:15:57.280 --> 02:15:58.280]   Wow.
[02:15:58.280 --> 02:15:59.280]   Wow.
[02:15:59.280 --> 02:16:02.280]   That's just-- that's wild.
[02:16:02.280 --> 02:16:03.280]   Let's take a break.
[02:16:03.280 --> 02:16:04.280]   Surprise.
[02:16:04.280 --> 02:16:05.280]   Last ad.
[02:16:05.280 --> 02:16:08.480]   And then we can say a good night because it's as much as I love talking to you.
[02:16:08.480 --> 02:16:11.000]   This has probably gone on way too long.
[02:16:11.000 --> 02:16:13.240]   Our shooting front.
[02:16:13.240 --> 02:16:15.040]   You know, I discovered something really cool.
[02:16:15.040 --> 02:16:16.040]   I use Sonos.
[02:16:16.040 --> 02:16:17.440]   We were talking about Sonos.
[02:16:17.440 --> 02:16:20.200]   And the Sonos app has now added Audible back in.
[02:16:20.200 --> 02:16:22.720]   You can listen to your audio books on your Sonos.
[02:16:22.720 --> 02:16:27.240]   This morning to celebrate, I had Ready Player One playing throughout the entire house.
[02:16:27.240 --> 02:16:29.800]   It was so awesome.
[02:16:29.800 --> 02:16:32.120]   Audible makes great audio books.
[02:16:32.120 --> 02:16:37.000]   Whether you like to listen to books to get away from it all, like Ready Player One, you
[02:16:37.000 --> 02:16:38.640]   like to get-- learn.
[02:16:38.640 --> 02:16:39.640]   I do that too.
[02:16:39.640 --> 02:16:41.920]   I love reading history.
[02:16:41.920 --> 02:16:44.360]   I've been an Audible member since the year 2000.
[02:16:44.360 --> 02:16:45.360]   18 years.
[02:16:45.360 --> 02:16:46.640]   That's kind of hard to believe.
[02:16:46.640 --> 02:16:47.640]   18 years.
[02:16:47.640 --> 02:16:50.560]   Longer than I've been doing this show, I've been an Audible member.
[02:16:50.560 --> 02:16:52.840]   And I love Audible.
[02:16:52.840 --> 02:16:54.320]   You will too.
[02:16:54.320 --> 02:16:58.040]   The selection on Audible's phenomenal.
[02:16:58.040 --> 02:17:02.480]   Pretty much every book that comes out now comes out in audio as well as prints so that
[02:17:02.480 --> 02:17:05.160]   you can get it day and date on Audible.
[02:17:05.160 --> 02:17:06.160]   I do that a lot.
[02:17:06.160 --> 02:17:07.720]   I'll pre-order books that I know are coming out.
[02:17:07.720 --> 02:17:09.480]   I'm very excited about reading.
[02:17:09.480 --> 02:17:16.600]   When Bill Gates said my favorite book was Stephen Pinker's The Better Angel of Our--
[02:17:16.600 --> 02:17:17.600]   Angels of Our Better Nature.
[02:17:17.600 --> 02:17:18.680]   Angels of Our Better Nature.
[02:17:18.680 --> 02:17:20.400]   I immediately got it on Audible.
[02:17:20.400 --> 02:17:21.920]   And that's another nice thing.
[02:17:21.920 --> 02:17:23.920]   You get a book immediately.
[02:17:23.920 --> 02:17:27.320]   You go to Audible.com/twit.
[02:17:27.320 --> 02:17:30.160]   And you can get a book for free right now.
[02:17:30.160 --> 02:17:33.960]   And you're downloading it and you're listening within seconds.
[02:17:33.960 --> 02:17:36.440]   That's pretty cool.
[02:17:36.440 --> 02:17:38.000]   Jason, your book's Audible.
[02:17:38.000 --> 02:17:39.280]   It is on Audible.
[02:17:39.280 --> 02:17:44.240]   And as a matter of fact, if you go to Audible and you listen to the sample chapter, it is
[02:17:44.240 --> 02:17:46.400]   the chapter number nine.
[02:17:46.400 --> 02:17:47.400]   About me.
[02:17:47.400 --> 02:17:48.400]   You're on the report.
[02:17:48.400 --> 02:17:49.400]   Yeah.
[02:17:49.400 --> 02:17:50.400]   You're probably going to be sick of that by now.
[02:17:50.400 --> 02:17:51.400]   Yes.
[02:17:51.400 --> 02:17:52.400]   That's adorable.
[02:17:52.400 --> 02:17:53.400]   You guys.
[02:17:53.400 --> 02:17:55.600]   Read by yours truly as well.
[02:17:55.600 --> 02:17:56.600]   Yeah.
[02:17:56.600 --> 02:18:00.120]   My book is also on there, but not read by me.
[02:18:00.120 --> 02:18:05.360]   But a professional person who knows how to read a book out loud to other people.
[02:18:05.360 --> 02:18:06.360]   Really.
[02:18:06.360 --> 02:18:07.360]   Yeah.
[02:18:07.360 --> 02:18:08.360]   No, it's so good.
[02:18:08.360 --> 02:18:13.040]   I have to say every book on Audible is something you learn, something you can enjoy, something
[02:18:13.040 --> 02:18:14.040]   you can escape to.
[02:18:14.040 --> 02:18:17.040]   I was just looking at my Audible library.
[02:18:17.040 --> 02:18:19.640]   And I have a pretty good mix of fiction and nonfiction.
[02:18:19.640 --> 02:18:21.680]   I learn a lot from Audible.
[02:18:21.680 --> 02:18:26.960]   I'm going to Japan, so I'm just reading Haruki Murakami's Wind Up Bird Chronicle.
[02:18:26.960 --> 02:18:29.600]   I don't know if you've read that, Amy, but it's really funny.
[02:18:29.600 --> 02:18:33.000]   Hey, can I ask you a question since we're talking about books in Audible?
[02:18:33.000 --> 02:18:34.000]   Yeah.
[02:18:34.000 --> 02:18:35.800]   Is it what books is everybody?
[02:18:35.800 --> 02:18:39.240]   I need, I just finished Born by Jeff Vandermeer.
[02:18:39.240 --> 02:18:40.920]   I don't know that book.
[02:18:40.920 --> 02:18:42.240]   Tell me about that.
[02:18:42.240 --> 02:18:43.240]   It's weird and crazy.
[02:18:43.240 --> 02:18:44.840]   He's the guy who wrote Annihilation.
[02:18:44.840 --> 02:18:45.840]   Oh.
[02:18:45.840 --> 02:18:46.840]   And this one is another.
[02:18:46.840 --> 02:18:49.120]   So you want science fiction or horror?
[02:18:49.120 --> 02:18:52.320]   It's sort of dystopian futures.
[02:18:52.320 --> 02:18:53.320]   It's me.
[02:18:53.320 --> 02:18:59.360]   So I'm reading, I'm way back and I'm reading even Cowgirls Get the Blues.
[02:18:59.360 --> 02:19:00.360]   Love that.
[02:19:00.360 --> 02:19:01.360]   Oh, yeah.
[02:19:01.360 --> 02:19:02.360]   Great book.
[02:19:02.360 --> 02:19:03.360]   And you know what?
[02:19:03.360 --> 02:19:04.360]   It's working for me right now.
[02:19:04.360 --> 02:19:08.560]   It's totally escapist, but also the most fantastic use of metaphor I've ever encountered.
[02:19:08.560 --> 02:19:09.920]   Like, just the wildest metaphor.
[02:19:09.920 --> 02:19:13.000]   She had giant thumbs because she hitchhiked.
[02:19:13.000 --> 02:19:14.800]   She hitchhiked because she had giant thumbs.
[02:19:14.800 --> 02:19:15.800]   Avisa Versa.
[02:19:15.800 --> 02:19:17.440]   Yeah, that's a big distinction.
[02:19:17.440 --> 02:19:18.440]   Interesting.
[02:19:18.440 --> 02:19:23.720]   I do one that I'm about to do because the second season of Handmaid's Tale is about
[02:19:23.720 --> 02:19:24.720]   to be found.
[02:19:24.720 --> 02:19:26.840]   Oh, there's a new version.
[02:19:26.840 --> 02:19:32.600]   It's Claire Danes who did a reading of the original, but this one is a performance version
[02:19:32.600 --> 02:19:33.880]   where there's other voices.
[02:19:33.880 --> 02:19:34.880]   I love those.
[02:19:34.880 --> 02:19:38.320]   When Audible does that, it's really cool because it brings a book to life.
[02:19:38.320 --> 02:19:39.320]   I love Tom Robbins.
[02:19:39.320 --> 02:19:40.800]   You know, you got me kind of in the mood.
[02:19:40.800 --> 02:19:41.800]   I'm just--
[02:19:41.800 --> 02:19:42.800]   I love Tom Robbins.
[02:19:42.800 --> 02:19:43.800]   Yeah.
[02:19:43.800 --> 02:19:45.480]   Half of Sleep and Frog Pajamas is one of my favorite books of all time.
[02:19:45.480 --> 02:19:50.120]   I listened on Audible way back in 2010 to Fierce Envelopes from Hot Climate's Home
[02:19:50.120 --> 02:19:51.120]   from Hot Climate.
[02:19:51.120 --> 02:19:52.120]   That was a great book.
[02:19:52.120 --> 02:19:54.000]   Tom Robbins on audio has got to be great.
[02:19:54.000 --> 02:19:55.000]   Yeah, it is.
[02:19:55.000 --> 02:19:56.000]   Oh, did he read it?
[02:19:56.000 --> 02:19:57.000]   No.
[02:19:57.000 --> 02:19:58.000]   No, no, no.
[02:19:58.000 --> 02:19:59.000]   Okay.
[02:19:59.000 --> 02:20:00.000]   Just the words, right?
[02:20:00.000 --> 02:20:01.000]   Such fun words to read.
[02:20:01.000 --> 02:20:02.000]   Yeah.
[02:20:02.000 --> 02:20:04.000]   No, it's like, you know, who else is great in Audible?
[02:20:04.000 --> 02:20:06.520]   Vonnegut, kind of for the same reason.
[02:20:06.520 --> 02:20:07.520]   It just--
[02:20:07.520 --> 02:20:08.520]   Kind of--
[02:20:08.520 --> 02:20:12.120]   They write like they speak very naturally and you can hear the voice.
[02:20:12.120 --> 02:20:16.120]   I want Jeff Bridges to read Tom Robbins.
[02:20:16.120 --> 02:20:18.120]   The dude should read Tom Robbins.
[02:20:18.120 --> 02:20:19.120]   He should, right?
[02:20:19.120 --> 02:20:20.120]   Yeah, definitely.
[02:20:20.120 --> 02:20:21.120]   We're Vonnegut too.
[02:20:21.120 --> 02:20:22.120]   Yeah.
[02:20:22.120 --> 02:20:23.120]   Yeah.
[02:20:23.120 --> 02:20:28.720]   One of the recommendations, it's a nonfiction one though, Whiplash by Joey Ito and Jeff How.
[02:20:28.720 --> 02:20:29.720]   Love Joey.
[02:20:29.720 --> 02:20:31.520]   How to survive our faster future.
[02:20:31.520 --> 02:20:32.520]   Yeah.
[02:20:32.520 --> 02:20:37.240]   So a lot of this is how, you know, because we're in a world where we're absorbing so
[02:20:37.240 --> 02:20:44.960]   much data and things are moving so much faster, you know, a lot of the laws and principles
[02:20:44.960 --> 02:20:51.600]   that we've worked on in business and life and culture, you know, don't-- they're-- we're
[02:20:51.600 --> 02:20:57.480]   breaking them and we have to be comfortable with that and think about them differently.
[02:20:57.480 --> 02:20:58.480]   So Whiplash.
[02:20:58.480 --> 02:21:00.640]   Listening to business books on Audible is really cool.
[02:21:00.640 --> 02:21:01.640]   Yeah.
[02:21:01.640 --> 02:21:06.120]   It's a great way to kind of take the time that you're in the car or walk in the dog
[02:21:06.120 --> 02:21:08.000]   or doing the dishes and make some use of it.
[02:21:08.000 --> 02:21:09.000]   Get a little smarter.
[02:21:09.000 --> 02:21:10.000]   Yeah, exactly.
[02:21:10.000 --> 02:21:11.000]   Yeah.
[02:21:11.000 --> 02:21:12.000]   You know, it's cool about the Vonnegut books.
[02:21:12.000 --> 02:21:14.200]   They have movie stars reading them all.
[02:21:14.200 --> 02:21:15.880]   Audible's really good at choosing voices.
[02:21:15.880 --> 02:21:18.640]   James Franco reads science as Slaughterhouse 5.
[02:21:18.640 --> 02:21:20.120]   Tony Roberts reads Cats Cradle.
[02:21:20.120 --> 02:21:23.120]   I want to hear John Malkovich read Breakfast of Champions.
[02:21:23.120 --> 02:21:25.120]   Oh, that would be good.
[02:21:25.120 --> 02:21:26.120]   Wow.
[02:21:26.120 --> 02:21:27.120]   Yeah.
[02:21:27.120 --> 02:21:28.120]   Wouldn't that be something?
[02:21:28.120 --> 02:21:29.120]   Yeah.
[02:21:29.120 --> 02:21:30.120]   Yeah.
[02:21:30.120 --> 02:21:31.120]   So it goes.
[02:21:31.120 --> 02:21:32.960]   I listened to Galapagos before I went to the Galapagos.
[02:21:32.960 --> 02:21:35.960]   Turns out it's not really about the Galapagos, but it still is.
[02:21:35.960 --> 02:21:38.680]   It was a great book.
[02:21:38.680 --> 02:21:39.840]   I really enjoyed it.
[02:21:39.840 --> 02:21:41.640]   And the Galapagos does figure in it.
[02:21:41.640 --> 02:21:43.640]   It's just not a natural history or anything.
[02:21:43.640 --> 02:21:44.640]   It's not a trauma.
[02:21:44.640 --> 02:21:47.080]   I feel like you should have a book club, Leo.
[02:21:47.080 --> 02:21:51.480]   I've always wanted to do an Audio Audible book club because I listened to so many Audible
[02:21:51.480 --> 02:21:52.480]   books.
[02:21:52.480 --> 02:21:53.640]   Welcome to the Monarchy House.
[02:21:53.640 --> 02:21:59.040]   This is a dramatization with David Straythairn, Maria Tucci, Biller, when Tony Roberts and
[02:21:59.040 --> 02:22:00.040]   Dylan Baker.
[02:22:00.040 --> 02:22:01.040]   That would be awesome.
[02:22:01.040 --> 02:22:02.880]   See, this is the fun part.
[02:22:02.880 --> 02:22:06.440]   If you're an Audible subscriber, when you get together with other Audible subscribers,
[02:22:06.440 --> 02:22:11.720]   which apparently we all are, coincidentally, what you do is talk about what you hear lately.
[02:22:11.720 --> 02:22:13.560]   What's a great book?
[02:22:13.560 --> 02:22:14.560]   Audible facilitates this.
[02:22:14.560 --> 02:22:17.840]   You can now send books, share books from your library with anyone.
[02:22:17.840 --> 02:22:19.640]   If it's their first time, they listen free.
[02:22:19.640 --> 02:22:21.000]   You could share audio excerpts.
[02:22:21.000 --> 02:22:22.800]   I really like this from your favorite books.
[02:22:22.800 --> 02:22:23.800]   That's awesome.
[02:22:23.800 --> 02:22:24.800]   So if you listen to something and say, "Oh, this is really good.
[02:22:24.800 --> 02:22:25.800]   Jason's got to hear this.
[02:22:25.800 --> 02:22:26.960]   I can send this to you."
[02:22:26.960 --> 02:22:29.840]   A lot of people listen to our podcast faster.
[02:22:29.840 --> 02:22:32.280]   You can do that with Audible too or slower.
[02:22:32.280 --> 02:22:33.520]   Get the narration you want.
[02:22:33.520 --> 02:22:37.880]   If you're an Amazon Kindle user and you use Whisper Sync, you can buy the book on the
[02:22:37.880 --> 02:22:42.360]   Kindle as well as the audiobook and watch as it reads or switch back and forth.
[02:22:42.360 --> 02:22:43.360]   So listen.
[02:22:43.360 --> 02:22:44.360]   You get home.
[02:22:44.360 --> 02:22:46.760]   You can read it if you like to read books on a Kindle.
[02:22:46.760 --> 02:22:50.080]   Audible members get a credit every month, good for any audiobook, regardless of price.
[02:22:50.080 --> 02:22:53.000]   Unused credits roll over.
[02:22:53.000 --> 02:22:54.520]   Although I never have unused credits.
[02:22:54.520 --> 02:22:58.480]   I look forward to my reset date, which is the 22nd of every month.
[02:22:58.480 --> 02:23:01.360]   I immediately jump in because I have an Audible wish list.
[02:23:01.360 --> 02:23:05.200]   As long as my arm books have a great feature too that I recently discovered.
[02:23:05.200 --> 02:23:11.520]   If you used a credit on a book and you didn't like it, you can exchange that credit for
[02:23:11.520 --> 02:23:12.520]   another book.
[02:23:12.520 --> 02:23:13.520]   That's really nice.
[02:23:13.520 --> 02:23:14.520]   Yeah.
[02:23:14.520 --> 02:23:15.520]   So I...
[02:23:15.520 --> 02:23:16.520]   No questions asked.
[02:23:16.520 --> 02:23:17.520]   No questions asked.
[02:23:17.520 --> 02:23:18.600]   And with Audible, you're not renting.
[02:23:18.600 --> 02:23:19.600]   These books are yours to keep.
[02:23:19.600 --> 02:23:23.040]   I have my library because I've been an Audible member so long as hundreds and hundreds of
[02:23:23.040 --> 02:23:24.040]   books.
[02:23:24.040 --> 02:23:28.360]   And I often go back and reread favorites, especially when, you know, just I need a...
[02:23:28.360 --> 02:23:31.360]   I need even cowgroves get the blues to get me through the night.
[02:23:31.360 --> 02:23:33.960]   And then I got it, which is really, really nice.
[02:23:33.960 --> 02:23:34.960]   I forgot.
[02:23:34.960 --> 02:23:35.960]   I love that book.
[02:23:35.960 --> 02:23:36.960]   I'm going to re-download it.
[02:23:36.960 --> 02:23:37.960]   Me too.
[02:23:37.960 --> 02:23:40.760]   I did a lot of John Irving's too, like Cider House Rules.
[02:23:40.760 --> 02:23:42.360]   For some reason they were mine.
[02:23:42.360 --> 02:23:43.360]   In narrative.
[02:23:43.360 --> 02:23:47.160]   Yeah, they're great stories, but they're real and it's their fun.
[02:23:47.160 --> 02:23:48.760]   Get a free audiobook right now.
[02:23:48.760 --> 02:23:50.360]   You get a 30 day free trial.
[02:23:50.360 --> 02:23:51.360]   Here's two ways to do it.
[02:23:51.360 --> 02:23:53.360]   You can go to audible.com/twit.
[02:23:53.360 --> 02:23:54.360]   We've got a new way.
[02:23:54.360 --> 02:23:56.280]   You know, most people listen Audible on their phone.
[02:23:56.280 --> 02:23:57.360]   So you could just use your phone.
[02:23:57.360 --> 02:24:02.480]   And text "twit" TWIT TWIT 500 500.
[02:24:02.480 --> 02:24:03.480]   That's the short code.
[02:24:03.480 --> 02:24:04.480]   That's as easy as you can get.
[02:24:04.480 --> 02:24:05.480]   You get a link back.
[02:24:05.480 --> 02:24:06.480]   You click.
[02:24:06.480 --> 02:24:07.480]   That's it.
[02:24:07.480 --> 02:24:08.480]   You're done.
[02:24:08.480 --> 02:24:13.920]   Text "twit" to 500 500 or go to audibla.com/twit.
[02:24:13.920 --> 02:24:19.120]   Audible.com/twit or text "twit" to 500 500.
[02:24:19.120 --> 02:24:24.400]   You'll get a link, a free book and a whole world of amazing literature awaits you.
[02:24:24.400 --> 02:24:26.880]   That's how these people got so smart.
[02:24:26.880 --> 02:24:29.280]   I know why you all are so darn smart.
[02:24:29.280 --> 02:24:30.280]   I forgot.
[02:24:30.280 --> 02:24:32.440]   I just remembered another recommendation.
[02:24:32.440 --> 02:24:33.960]   Hamilton the Revolution.
[02:24:33.960 --> 02:24:38.240]   They did the story of how they created Hamilton.
[02:24:38.240 --> 02:24:39.240]   That I got a read.
[02:24:39.240 --> 02:24:40.240]   Terrific audio book.
[02:24:40.240 --> 02:24:41.240]   I got to read that.
[02:24:41.240 --> 02:24:42.240]   Yeah.
[02:24:42.240 --> 02:24:45.440]   I think you just made a great shout out to English majors.
[02:24:45.440 --> 02:24:50.760]   English majors are pretty much everybody because we couldn't get a real job.
[02:24:50.760 --> 02:24:56.640]   In the future people, people who have, we all focus on STEM, STEM, STEM, but the people
[02:24:56.640 --> 02:25:02.720]   who are going to fare pretty well going forward are people who have had very rigorous liberal
[02:25:02.720 --> 02:25:10.440]   arts degrees with super concentrated abilities to do critical thinking and comparative reasoning
[02:25:10.440 --> 02:25:11.440]   and logic.
[02:25:11.440 --> 02:25:12.440]   Analysis.
[02:25:12.440 --> 02:25:17.840]   I would submit why I reject the notion that you're one of the other, your left brain
[02:25:17.840 --> 02:25:18.840]   or right brain.
[02:25:18.840 --> 02:25:19.840]   Do both.
[02:25:19.840 --> 02:25:24.200]   Really, the secret to my success in life, and I bet you to all of yours too, is that
[02:25:24.200 --> 02:25:29.240]   you understand technology, that you have an affinity for STEM, for science, for technology,
[02:25:29.240 --> 02:25:35.160]   for engineering, for math, but you also can talk or write and explicate.
[02:25:35.160 --> 02:25:36.640]   We really need that.
[02:25:36.640 --> 02:25:42.600]   I do think, Amy, that STEM gives you a logic foundation in a way that no language thing.
[02:25:42.600 --> 02:25:43.600]   Sure.
[02:25:43.600 --> 02:25:48.680]   They're not in exchange for comparative, the best possible degree would be instead of
[02:25:48.680 --> 02:25:50.840]   algebra, like applied mathematics.
[02:25:50.840 --> 02:25:51.840]   Yeah.
[02:25:51.840 --> 02:25:52.840]   More practical.
[02:25:52.840 --> 02:25:53.840]   That's what coding is.
[02:25:53.840 --> 02:25:54.840]   And world religion.
[02:25:54.840 --> 02:25:55.840]   Yeah.
[02:25:55.840 --> 02:26:02.680]   And, you know, like that makes you a super, like a super powerful person if you're able
[02:26:02.680 --> 02:26:03.680]   to do all of that.
[02:26:03.680 --> 02:26:08.720]   And you know, one thing that this whole panel represents is lifelong learners, that you
[02:26:08.720 --> 02:26:12.480]   got a good foundation and you never stopped learning, that you still read, you still learn,
[02:26:12.480 --> 02:26:13.720]   you're still inquisitive.
[02:26:13.720 --> 02:26:14.720]   That's really cool.
[02:26:14.720 --> 02:26:17.560]   Well, because no matter what happens, nobody can take away what you've learned.
[02:26:17.560 --> 02:26:20.680]   Like nobody can take away your education, which is why education is paramount.
[02:26:20.680 --> 02:26:23.120]   I completely agree with you.
[02:26:23.120 --> 02:26:26.560]   Take a coding class as well as a comparative religions class.
[02:26:26.560 --> 02:26:27.560]   They're both great.
[02:26:27.560 --> 02:26:28.560]   Yeah.
[02:26:28.560 --> 02:26:29.560]   Well, we should do it all.
[02:26:29.560 --> 02:26:30.560]   That's the big, right?
[02:26:30.560 --> 02:26:32.000]   I say, I can't do that.
[02:26:32.000 --> 02:26:33.000]   I can't do that.
[02:26:33.000 --> 02:26:36.280]   That's why all good liberalized schools and most big, wonderful public universities too
[02:26:36.280 --> 02:26:40.280]   require you to take a number of different types of courses so that you can think that
[02:26:40.280 --> 02:26:41.280]   way.
[02:26:41.280 --> 02:26:48.600]   I am a big proponent of a lot of high school math curricula is not very good these days,
[02:26:48.600 --> 02:26:53.120]   but there is good coding curriculum out there.
[02:26:53.120 --> 02:26:58.560]   Learn math through coding that will help you learn math and logic and technology.
[02:26:58.560 --> 02:27:05.680]   I interviewed a guy who was a professor of computer science at Brown University.
[02:27:05.680 --> 02:27:07.920]   I'm trying to remember the name.
[02:27:07.920 --> 02:27:09.840]   It was on triangulation.
[02:27:09.840 --> 02:27:15.320]   The name of his, they have a school curriculum that any math teacher can teach.
[02:27:15.320 --> 02:27:17.880]   It doesn't matter if you don't know coding.
[02:27:17.880 --> 02:27:21.640]   It's really good and it's free and it's available.
[02:27:21.640 --> 02:27:24.680]   While you're looking that up, I have a funny story.
[02:27:24.680 --> 02:27:27.360]   When I was in high school calculus, my high school calculus teacher, and I took this as
[02:27:27.360 --> 02:27:35.240]   an insult at the time, he said, "You're the best non-math student I've had."
[02:27:35.240 --> 02:27:39.320]   Now much later, I'm like, "No, that was a compliment.
[02:27:39.320 --> 02:27:41.360]   I'm going to take that because..."
[02:27:41.360 --> 02:27:42.360]   Yeah.
[02:27:42.360 --> 02:27:43.360]   I know that.
[02:27:43.360 --> 02:27:44.360]   That's good.
[02:27:44.360 --> 02:27:45.360]   I'm going to own that.
[02:27:45.360 --> 02:27:46.360]   Yeah.
[02:27:46.360 --> 02:27:48.160]   I'm going to take that.
[02:27:48.160 --> 02:27:49.160]   This is the page.
[02:27:49.160 --> 02:27:52.240]   Bootstrapworld.org.
[02:27:52.240 --> 02:27:54.200]   I really love this idea.
[02:27:54.200 --> 02:27:56.520]   They have lesson plans.
[02:27:56.520 --> 02:28:01.160]   They have actually classes that you can take as a teacher.
[02:28:01.160 --> 02:28:04.840]   They've got ongoing support from the National Science Foundation.
[02:28:04.840 --> 02:28:12.120]   It is a way to bring math, physics, and coding into the classroom for basically for high
[02:28:12.120 --> 02:28:13.120]   schools.
[02:28:13.120 --> 02:28:17.680]   I would say smart middle schooler could benefit from this as well.
[02:28:17.680 --> 02:28:23.160]   By the way, they're one of the largest providers of formal computer science education to girls
[02:28:23.160 --> 02:28:25.200]   and underrepresented students nationwide.
[02:28:25.200 --> 02:28:28.320]   They really do a great job of getting diversity.
[02:28:28.320 --> 02:28:29.320]   I would just say...
[02:28:29.320 --> 02:28:33.320]   My high school teacher said, "You're the worst student I've ever had."
[02:28:33.320 --> 02:28:35.600]   I don't know why you're taking calculus.
[02:28:35.600 --> 02:28:37.160]   That's almost a direct quote.
[02:28:37.160 --> 02:28:38.160]   That's really insulting.
[02:28:38.160 --> 02:28:39.160]   Wow.
[02:28:39.160 --> 02:28:40.440]   That's almost a direct quote.
[02:28:40.440 --> 02:28:42.160]   So I dropped the class.
[02:28:42.160 --> 02:28:47.280]   It may not feel like a rigorous liberal arts program is going to prepare you for the job
[02:28:47.280 --> 02:28:50.840]   market over the next three to five years and it probably will not.
[02:28:50.840 --> 02:28:52.680]   I just want to clarify what I said.
[02:28:52.680 --> 02:28:56.040]   I'm talking about 20 years out.
[02:28:56.040 --> 02:28:58.520]   I've got a seven-year-old.
[02:28:58.520 --> 02:29:02.320]   By virtue of her being seven and growing up in my household, she's getting exposure
[02:29:02.320 --> 02:29:04.720]   to quite a bit of code and tech.
[02:29:04.720 --> 02:29:05.720]   Good for her.
[02:29:05.720 --> 02:29:06.720]   That's great.
[02:29:06.720 --> 02:29:11.080]   But if you think about the hybrid skill sets and all the really interesting new kinds of
[02:29:11.080 --> 02:29:16.640]   jobs that will never have existed before but will need to exist 20 years from now, humans
[02:29:16.640 --> 02:29:25.080]   who have really great abilities to think critically, understand logic, write, communicate.
[02:29:25.080 --> 02:29:26.080]   I agree 100%.
[02:29:26.080 --> 02:29:28.800]   That's going to be really important because a machine can.
[02:29:28.800 --> 02:29:30.280]   It's not going to be able to do that.
[02:29:30.280 --> 02:29:31.280]   To connect the dots.
[02:29:31.280 --> 02:29:32.280]   That reminds me of something too.
[02:29:32.280 --> 02:29:41.040]   We've been interviewing interns and we interviewed this young fella from Harvard.
[02:29:41.040 --> 02:29:44.740]   He told me something that I thought was really interesting that they don't allow you to
[02:29:44.740 --> 02:29:48.600]   declare a major now until your junior year.
[02:29:48.600 --> 02:29:50.100]   How will you?
[02:29:50.100 --> 02:29:53.680]   Which I thought is great because most kids change three times by then.
[02:29:53.680 --> 02:29:57.560]   There was a first trend going on for a while where you had to declare before you got in
[02:29:57.560 --> 02:29:58.560]   the college.
[02:29:58.560 --> 02:29:59.560]   No, terrible.
[02:29:59.560 --> 02:30:00.560]   Terrible.
[02:30:00.560 --> 02:30:05.440]   But their approach is actually similar to what Amy's saying is that they want you to
[02:30:05.440 --> 02:30:11.880]   come in and have whether you're going to major in computer science or drama or a medist pre-med
[02:30:11.880 --> 02:30:12.880]   or whatever.
[02:30:12.880 --> 02:30:18.360]   They want you to come in and get a broad based exposure to different things.
[02:30:18.360 --> 02:30:24.000]   As a matter of fact, they require that you are involved in an activity that's part of
[02:30:24.000 --> 02:30:26.080]   your credits.
[02:30:26.080 --> 02:30:30.880]   Instead of having to do it on your own time, you have that built in.
[02:30:30.880 --> 02:30:38.880]   It's cheer squad or the magazine or the newspaper, whatever it is.
[02:30:38.880 --> 02:30:44.600]   This fella that we were interviewing that was really smart, he said, "I had to pick something
[02:30:44.600 --> 02:30:51.760]   and so I picked a newspaper because I was like, 'Well, that looks easy, right?'
[02:30:51.760 --> 02:30:54.520]   Which is something to tell an editor in an interview.
[02:30:54.520 --> 02:30:57.680]   I couldn't do calculus, so I'm here."
[02:30:57.680 --> 02:30:59.400]   But he's like, "I love it."
[02:30:59.400 --> 02:31:01.160]   Did he find out, "This is a Harvard kid?
[02:31:01.160 --> 02:31:02.160]   This is a Harvard kid."
[02:31:02.160 --> 02:31:03.160]   Are you just writing for the Crimson?
[02:31:03.160 --> 02:31:04.160]   Yes.
[02:31:04.160 --> 02:31:09.160]   He found out that he loved it and he not only did the newspaper, but he did the magazine
[02:31:09.160 --> 02:31:10.160]   too.
[02:31:10.160 --> 02:31:13.560]   So very super cool.
[02:31:13.560 --> 02:31:19.800]   I thought that was a really good sign that at least one institution, and obviously where
[02:31:19.800 --> 02:31:25.360]   Harvard goes, a lot of colleges tend to follow, was embracing this idea that no matter what
[02:31:25.360 --> 02:31:33.600]   you learn, you critical thinking, analysis, having a broad learning profile is going
[02:31:33.600 --> 02:31:35.280]   to suit you really well.
[02:31:35.280 --> 02:31:36.840]   What was your major?
[02:31:36.840 --> 02:31:37.840]   My major?
[02:31:37.840 --> 02:31:42.240]   It was journalism until I had these fights with my journalism professors about that all
[02:31:42.240 --> 02:31:44.080]   the future was going to be online.
[02:31:44.080 --> 02:31:48.920]   They said, "No, they tried that in hyper-card down in Florida and people just don't want
[02:31:48.920 --> 02:31:49.920]   to read on screens."
[02:31:49.920 --> 02:31:50.920]   True.
[02:31:50.920 --> 02:31:54.880]   All people did in Florida with the interactive tests was by stamps.
[02:31:54.880 --> 02:31:57.600]   That shows you how long ago that was.
[02:31:57.600 --> 02:32:01.120]   This online thing is neat, but it's not going to ever happen.
[02:32:01.120 --> 02:32:04.240]   We tried it with prodigy.
[02:32:04.240 --> 02:32:06.320]   It's just not going to happen.
[02:32:06.320 --> 02:32:08.480]   And finally, I just can't learn anything from these people.
[02:32:08.480 --> 02:32:11.600]   In my very 18-year-old way, it was like, "I'm not going to learn anything from these
[02:32:11.600 --> 02:32:12.600]   people.
[02:32:12.600 --> 02:32:14.160]   They're complete idiots," which was your change too.
[02:32:14.160 --> 02:32:19.120]   So I went to Indiana University and they have a thing that's called create your own
[02:32:19.120 --> 02:32:22.000]   major, the College of Arts and Sciences.
[02:32:22.000 --> 02:32:27.880]   So I did a mix of history and English and Middle Eastern studies.
[02:32:27.880 --> 02:32:28.880]   That's really cool.
[02:32:28.880 --> 02:32:30.720]   That's what Amy would prescribe.
[02:32:30.720 --> 02:32:33.320]   Small world, Jason, where are you from?
[02:32:33.320 --> 02:32:38.360]   I'm from Fort Wayne, Indiana in Philadelphia, Pennsylvania.
[02:32:38.360 --> 02:32:40.280]   So I am a region rat.
[02:32:40.280 --> 02:32:43.320]   I'm from northwest Indiana.
[02:32:43.320 --> 02:32:46.760]   And I was on a full ride for the music school.
[02:32:46.760 --> 02:32:49.360]   So I went to IU, somewhat against me.
[02:32:49.360 --> 02:32:51.800]   You've got a couple of hoops, yeah.
[02:32:51.800 --> 02:32:52.800]   That's crazy.
[02:32:52.800 --> 02:32:56.360]   Yeah, my parents said I was allowed to go anywhere I wanted.
[02:32:56.360 --> 02:33:00.640]   But I had to have the same deal, which of course I wasn't going to get.
[02:33:00.640 --> 02:33:02.720]   So I went to the music school.
[02:33:02.720 --> 02:33:04.520]   I made it through my freshman year recital.
[02:33:04.520 --> 02:33:07.840]   I was a clarinet performance major.
[02:33:07.840 --> 02:33:09.360]   Dropped out, didn't tell anybody.
[02:33:09.360 --> 02:33:10.360]   Wow.
[02:33:10.360 --> 02:33:11.520]   And then points switched over.
[02:33:11.520 --> 02:33:13.400]   And I did the same kind of degree that you did.
[02:33:13.400 --> 02:33:16.360]   So mine was game theory, political science, and economics.
[02:33:16.360 --> 02:33:17.360]   Nice.
[02:33:17.360 --> 02:33:18.360]   Nice.
[02:33:18.360 --> 02:33:19.360]   Awesome.
[02:33:19.360 --> 02:33:20.360]   Yeah, so that's a crazy small world.
[02:33:20.360 --> 02:33:21.360]   Very cool, yes.
[02:33:21.360 --> 02:33:24.360]   I was a English major at Harvard at the time.
[02:33:24.360 --> 02:33:25.360]   Did you work with the Crimson?
[02:33:25.360 --> 02:33:26.360]   I did.
[02:33:26.360 --> 02:33:27.360]   I was a Crimson editor.
[02:33:27.360 --> 02:33:28.360]   In fact, I co-edited the magazine.
[02:33:28.360 --> 02:33:29.840]   You were the editor of the Crimson?
[02:33:29.840 --> 02:33:35.160]   No, I was the co-editor of 15 Minutes, which is the weekly magazine of the Harvard Crimson.
[02:33:35.160 --> 02:33:39.840]   But the point that I was getting to was that at the time, Harvard was pretty snobby about
[02:33:39.840 --> 02:33:41.000]   vocational anything.
[02:33:41.000 --> 02:33:43.920]   Like applied math was the only degree at Harvard.
[02:33:43.920 --> 02:33:45.920]   You're here to learn about the world.
[02:33:45.920 --> 02:33:46.920]   Right.
[02:33:46.920 --> 02:33:47.920]   There was a leader.
[02:33:47.920 --> 02:33:51.320]   There was very, like you could take computer science at Harvard, but it was really, really
[02:33:51.320 --> 02:33:52.320]   abstract.
[02:33:52.320 --> 02:33:55.920]   It wasn't very, it wasn't very hands on.
[02:33:55.920 --> 02:33:57.640]   But he was the interesting thing.
[02:33:57.640 --> 02:34:02.440]   Because of that, the Crimson has nothing to do with Harvard University.
[02:34:02.440 --> 02:34:05.040]   I mean, except obviously it has everything to do with Harvard University, but is not
[02:34:05.040 --> 02:34:08.040]   run by the universities completely funded by the students.
[02:34:08.040 --> 02:34:12.760]   We own our own presses, completely funded by the students, technically off campus.
[02:34:12.760 --> 02:34:16.920]   And what was interesting was that we were running a full-fledged website at the time,
[02:34:16.920 --> 02:34:19.160]   even though it was brand new, because there was nobody to tell us not to.
[02:34:19.160 --> 02:34:20.160]   Yeah.
[02:34:20.160 --> 02:34:24.720]   And the students kind of going like, "We have to have this, so we're going to start it."
[02:34:24.720 --> 02:34:25.720]   And that was cool.
[02:34:25.720 --> 02:34:30.560]   It was like, "Okay, well let's take that vocational part and move it kind of off campus."
[02:34:30.560 --> 02:34:35.120]   And I'm glad to hear that now they're kind of acknowledging that and helping people move
[02:34:35.120 --> 02:34:36.120]   into it.
[02:34:36.120 --> 02:34:39.040]   So you see kids you never know.
[02:34:39.040 --> 02:34:40.400]   Just get that education.
[02:34:40.400 --> 02:34:41.400]   Be cool.
[02:34:41.400 --> 02:34:42.400]   Stay in school.
[02:34:42.400 --> 02:34:43.400]   And someday--
[02:34:43.400 --> 02:34:44.400]   English is fine.
[02:34:44.400 --> 02:34:45.400]   Yeah, English is fine.
[02:34:45.400 --> 02:34:47.040]   It doesn't really matter.
[02:34:47.040 --> 02:34:49.720]   Learn as much as you can for as long as you can, really.
[02:34:49.720 --> 02:34:50.720]   Yes.
[02:34:50.720 --> 02:34:51.720]   What a great panel.
[02:34:51.720 --> 02:34:52.760]   Amy Webb, love having you on.
[02:34:52.760 --> 02:34:54.560]   The signals are talking.
[02:34:54.560 --> 02:34:55.560]   Why today's--
[02:34:55.560 --> 02:34:56.560]   Thanks.
[02:34:56.560 --> 02:34:57.560]   Can I--
[02:34:57.560 --> 02:34:58.560]   Oh, sorry.
[02:34:58.560 --> 02:34:59.560]   Plug.
[02:34:59.560 --> 02:35:00.560]   Can I plug?
[02:35:00.560 --> 02:35:01.560]   So our annual report just came out.
[02:35:01.560 --> 02:35:02.560]   It's almost 300 pages long.
[02:35:02.560 --> 02:35:03.560]   Where's mine?
[02:35:03.560 --> 02:35:04.560]   Free.
[02:35:04.560 --> 02:35:05.560]   Where's mine?
[02:35:05.560 --> 02:35:06.560]   I sent one.
[02:35:06.560 --> 02:35:07.560]   Oh, you did?
[02:35:07.560 --> 02:35:08.560]   Carsten.
[02:35:08.560 --> 02:35:10.360]   Are you boguarding the annual report?
[02:35:10.360 --> 02:35:11.360]   Don't go guard.
[02:35:11.360 --> 02:35:15.120]   This year-- so it's our annual report on emerging tech trends for the coming year.
[02:35:15.120 --> 02:35:20.440]   This year there's 225 across 20 industries.
[02:35:20.440 --> 02:35:21.440]   It's all free.
[02:35:21.440 --> 02:35:23.880]   So you can get it online at our website.
[02:35:23.880 --> 02:35:26.040]   And we printed like a handful of hard copies.
[02:35:26.040 --> 02:35:28.440]   But it debuted at South by Southwest.
[02:35:28.440 --> 02:35:30.240]   And now it's everywhere.
[02:35:30.240 --> 02:35:31.240]   So--
[02:35:31.240 --> 02:35:32.240]   Future.
[02:35:32.240 --> 02:35:33.240]   --for the future.
[02:35:33.240 --> 02:35:34.480]   --todayinstitute.com.
[02:35:34.480 --> 02:35:35.480]   And--
[02:35:35.480 --> 02:35:36.480]   Downloading it in.
[02:35:36.480 --> 02:35:38.640]   --tech trends report available for download.
[02:35:38.640 --> 02:35:39.640]   Yeah.
[02:35:39.640 --> 02:35:40.640]   How many months do you spend working on that?
[02:35:40.640 --> 02:35:41.640]   I probably a lot.
[02:35:41.640 --> 02:35:42.640]   That looks hard.
[02:35:42.640 --> 02:35:43.640]   No.
[02:35:43.640 --> 02:35:46.440]   This is our 11th annual-- like our 11th edition.
[02:35:46.440 --> 02:35:48.200]   So we've been doing this for 11 years.
[02:35:48.200 --> 02:35:49.560]   It takes a couple of months.
[02:35:49.560 --> 02:35:52.520]   But it's the research that we're always doing.
[02:35:52.520 --> 02:35:57.000]   Last year-- two years ago, I open sourced all of my methodology research tools and everything.
[02:35:57.000 --> 02:35:59.520]   So I want other people to use them and build on them.
[02:35:59.520 --> 02:36:00.520]   Wow.
[02:36:00.520 --> 02:36:01.520]   So we give everything away for free.
[02:36:01.520 --> 02:36:02.520]   That's--
[02:36:02.520 --> 02:36:03.520]   Wow.
[02:36:03.520 --> 02:36:04.520]   --so amazing.
[02:36:04.520 --> 02:36:05.520]   Isn't that nice?
[02:36:05.520 --> 02:36:06.520]   Look, here's your reading list.
[02:36:06.520 --> 02:36:10.760]   Oh, yeah, we put together our new ones coming out in two weeks.
[02:36:10.760 --> 02:36:15.320]   So every-- I think pretty strongly that if you want to understand the future, you also
[02:36:15.320 --> 02:36:16.680]   have to pay attention to sci-fi.
[02:36:16.680 --> 02:36:17.680]   OK.
[02:36:17.680 --> 02:36:19.680]   I can live with that.
[02:36:19.680 --> 02:36:20.680]   OK.
[02:36:20.680 --> 02:36:25.800]   So we put together a reading and listening list every quarter.
[02:36:25.800 --> 02:36:29.080]   And encourage people to read and watch and listen.
[02:36:29.080 --> 02:36:30.320]   It's really cool, Amy.
[02:36:30.320 --> 02:36:34.680]   You have not only created your own major, you've created your own job, really.
[02:36:34.680 --> 02:36:35.680]   And well done.
[02:36:35.680 --> 02:36:36.680]   Well done.
[02:36:36.680 --> 02:36:37.680]   Thanks.
[02:36:37.680 --> 02:36:38.680]   FutureTodayinstitute.com.
[02:36:38.680 --> 02:36:45.680]   Lindsay Turrentine took that career at the Harvard Crimson and brought it to us all, editor-in-chief
[02:36:45.680 --> 02:36:46.680]   at cnet.com.
[02:36:46.680 --> 02:36:50.200]   Anything you want to plug that's going on?
[02:36:50.200 --> 02:36:56.080]   I want to vaguely plug what we're doing later this week in the smart home territory, because
[02:36:56.080 --> 02:36:57.840]   it's a secret, and I can't talk about it yet.
[02:36:57.840 --> 02:37:00.280]   But I'll be able to talk about it later this week.
[02:37:00.280 --> 02:37:01.280]   OK.
[02:37:01.280 --> 02:37:02.920]   So Thursday, check out CNET.
[02:37:02.920 --> 02:37:03.920]   You won't be able to miss it.
[02:37:03.920 --> 02:37:04.920]   CNET.com.
[02:37:04.920 --> 02:37:05.920]   That's kind of cool.
[02:37:05.920 --> 02:37:06.920]   Yeah.
[02:37:06.920 --> 02:37:08.920]   Everything is coming.
[02:37:08.920 --> 02:37:09.920]   Something's coming.
[02:37:09.920 --> 02:37:10.920]   How exciting.
[02:37:10.920 --> 02:37:13.160]   Jason Hunter, he's here for RSA.
[02:37:13.160 --> 02:37:15.040]   You could find his work, of course, at Tech Republic.
[02:37:15.040 --> 02:37:16.040]   He's editor-in-chief.
[02:37:16.040 --> 02:37:17.800]   Anything you want to plug?
[02:37:17.800 --> 02:37:18.800]   Yeah.
[02:37:18.800 --> 02:37:24.360]   Every month, Zienia and Tech Republic come together, all of our editors across the globe.
[02:37:24.360 --> 02:37:29.000]   We have editors all over the world, contributors all over the world.
[02:37:29.000 --> 02:37:33.920]   We come together and we have 10 features, essentially, that we do on the most important
[02:37:33.920 --> 02:37:35.240]   topics in tech.
[02:37:35.240 --> 02:37:39.240]   And so this month is cybersecurity.
[02:37:39.240 --> 02:37:46.480]   And so you can go to ZDNET and go to-- you can see it sort of on the front page there
[02:37:46.480 --> 02:37:48.200]   if you scroll down.
[02:37:48.200 --> 02:37:49.840]   All of our monthly features are there.
[02:37:49.840 --> 02:37:55.400]   So if you come every month, you can get all the latest sort of our own research, as well
[02:37:55.400 --> 02:38:01.480]   as our journalists on the ground, you know, in all the English speaking parts of the world.
[02:38:01.480 --> 02:38:07.640]   So yeah, next month is the Internet of Things, the Industrial Internet of Things.
[02:38:07.640 --> 02:38:10.720]   Yes, you're all doing such exciting, interesting work.
[02:38:10.720 --> 02:38:12.240]   Now I'm off for vacation.
[02:38:12.240 --> 02:38:13.720]   I won't be here next week.
[02:38:13.720 --> 02:38:16.120]   Ian Thompson will be hosting Becky Worley.
[02:38:16.120 --> 02:38:17.960]   We'll be hosting the week following.
[02:38:17.960 --> 02:38:18.960]   So that'll be a lot of fun.
[02:38:18.960 --> 02:38:22.120]   I'll be back on May 6th, but it'll be a fun one to watch.
[02:38:22.120 --> 02:38:23.360]   I'll be just back.
[02:38:23.360 --> 02:38:25.320]   So I'll be incredibly jet lagged.
[02:38:25.320 --> 02:38:28.160]   That's full of Japanese words.
[02:38:28.160 --> 02:38:29.320]   And full of Japanese words.
[02:38:29.320 --> 02:38:31.320]   And I'm hoping sushi.
[02:38:31.320 --> 02:38:35.680]   And maybe a few other things, including a hot dog bun with brown spaghetti in it.
[02:38:35.680 --> 02:38:36.680]   I don't know.
[02:38:36.680 --> 02:38:37.680]   Amy, so delicious.
[02:38:37.680 --> 02:38:40.360]   Eat one for me.
[02:38:40.360 --> 02:38:43.480]   It sounds like the Japanese version of spaghetti tacos.
[02:38:43.480 --> 02:38:45.800]   Oh, this sounds like a kid's favorite.
[02:38:45.800 --> 02:38:46.960]   Like a nightmare.
[02:38:46.960 --> 02:38:50.080]   But I'm going to try it because Amy Webb knows all and tells all.
[02:38:50.080 --> 02:38:51.360]   And thank you all for being here.
[02:38:51.360 --> 02:38:55.760]   We do twit every Sunday afternoon, 3 p.m. Pacific, 6 p.m. Eastern, 2200 UTC.
[02:38:55.760 --> 02:39:00.600]   If you want to be in studio, we invite you to just email tickets@twit.tv.
[02:39:00.600 --> 02:39:01.960]   We love our in studio audience.
[02:39:01.960 --> 02:39:04.040]   You guys are always great.
[02:39:04.040 --> 02:39:08.800]   Michael O'Donnell is working on beating his record at the old screen savers show.
[02:39:08.800 --> 02:39:11.960]   How many screen savers did you see?
[02:39:11.960 --> 02:39:12.960]   Over a hundred.
[02:39:12.960 --> 02:39:15.880]   You're not close to that yet on Twitch, but you're getting there.
[02:39:15.880 --> 02:39:16.960]   We love having him.
[02:39:16.960 --> 02:39:21.000]   You could follow him on Twitter @photo, and he always posts fun photos from the episode.
[02:39:21.000 --> 02:39:24.160]   So look for those @photo on Twitter.
[02:39:24.160 --> 02:39:27.800]   If you can't be here in person, you can watch the stream live from wherever you are.
[02:39:27.800 --> 02:39:30.800]   Anywhere in the world, just go to twit.tv/live.
[02:39:30.800 --> 02:39:33.720]   If you do that though, I invite you to join us in the chat room.
[02:39:33.720 --> 02:39:39.240]   You could be the smart kids in the back of the class, the wise acres, the wise and primers,
[02:39:39.240 --> 02:39:41.640]   the chatroom is at irc.twit.tv.
[02:39:41.640 --> 02:39:47.280]   And of course, if you're busy Sunday evenings, you can always get an on demand version of
[02:39:47.280 --> 02:39:49.400]   our show and listen at your leisure.
[02:39:49.400 --> 02:39:50.960]   You'll find those at our website.
[02:39:50.960 --> 02:39:58.600]   We'll go all the way back to Twit #1 from April 17, 2005, 13 years.
[02:39:58.600 --> 02:40:00.440]   Mark Zuckerberg in the frame, isn't it there?
[02:40:00.440 --> 02:40:01.600]   I think that is.
[02:40:01.600 --> 02:40:02.600]   It's Mark Zuckerberg.
[02:40:02.600 --> 02:40:07.240]   He was just an eight-year-old at the time.
[02:40:07.240 --> 02:40:11.000]   Twit.tv, you'll find downloadable versions, audio and video there.
[02:40:11.000 --> 02:40:13.200]   You can also subscribe in your favorite podcatcher.
[02:40:13.200 --> 02:40:15.160]   In fact, I invite you to subscribe that way.
[02:40:15.160 --> 02:40:16.680]   You won't miss a single episode.
[02:40:16.680 --> 02:40:17.680]   Thanks for being here.
[02:40:17.680 --> 02:40:19.000]   I'll see you in a couple of weeks.
[02:40:19.000 --> 02:40:20.760]   Another Twit is in the camera.
[02:40:20.760 --> 02:40:21.760]   Easy.
[02:40:21.760 --> 02:40:22.760]   [MUSIC]
[02:40:22.760 --> 02:40:24.760]   [MUSIC]
[02:40:24.760 --> 02:40:25.760]   Do the Twit.
[02:40:25.760 --> 02:40:26.760]   All right.
[02:40:26.760 --> 02:40:27.760]   Do the Twit, baby.
[02:40:27.760 --> 02:40:28.760]   Do the Twit.
[02:40:28.760 --> 02:40:29.760]   All right.
[02:40:29.760 --> 02:40:30.760]   Do the Twit.

