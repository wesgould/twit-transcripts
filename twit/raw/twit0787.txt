;FFMETADATA1
title=Dreamliner
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=787
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:13.000]   Coming up, it's time for Twit. We've got a great panel for you. Dan Patterson from CBS News, our good friend, Luma Reska from this weekend Enterprise Tech, and novelist Rob Reed, who's going to talk about synthetic biology.
[00:00:13.000 --> 00:00:23.000]   He predicted the pandemic way back when? Last year, we're going to talk about a guy in a jetpack flying in the flight path at LAX.
[00:00:23.000 --> 00:00:32.000]   The U.S. Court finally vindicating Edward Snowden and a Fisher-priced baby toy that knows the Konami code. It's all coming up next.
[00:00:32.000 --> 00:00:41.000]   On Twit! This weekend Tech comes to you from Twit's LastPass studio, securing every access point in your company, does not have to be a challenge.
[00:00:41.000 --> 00:00:50.000]   LastPass unifies access and authentication to make securing your employees simple and secure, even when they're working remotely.
[00:00:50.000 --> 00:00:56.000]   Check out lastpass.com/twit to learn more.
[00:00:56.000 --> 00:01:00.000]   Podcasts you love from people you trust.
[00:01:00.000 --> 00:01:04.000]   This is Twit.
[00:01:04.000 --> 00:01:19.000]   This is Twit, this week at Tech. Episode 787, recorded Sunday, September 6, 2020. Dreamliner.
[00:01:19.000 --> 00:01:24.000]   This episode of This Week at Tech is brought to you by Forward Networks.
[00:01:24.000 --> 00:01:30.000]   Forward Networks reduces business risk by revolutionizing the way large networks are managed.
[00:01:30.000 --> 00:01:38.000]   Forward Networks Advanced Software delivers a digital twin of the network, a completely accurate mathematical model in software.
[00:01:38.000 --> 00:01:42.000]   Get a demo at forwardnetworks.com/twit.
[00:01:42.000 --> 00:01:44.000]   And by ERO.
[00:01:44.000 --> 00:01:48.000]   These days you need solid Wi-Fi in your whole house.
[00:01:48.000 --> 00:01:51.000]   So everyone isn't working on top of each other.
[00:01:51.000 --> 00:02:00.000]   Well that's why you need ERO. Go to ERO.com/twit and enter the code TWIT at checkout to get free overnight shipping with your order.
[00:02:00.000 --> 00:02:03.000]   And by Barracuda.
[00:02:03.000 --> 00:02:11.000]   Did you know that 91% of all cyber attacks start with an email to uncover the threats hiding in your Office 365 account?
[00:02:11.000 --> 00:02:17.000]   Get a secure and free email threat scan at barracuda.com/twit.
[00:02:17.000 --> 00:02:20.000]   And by LastPass.
[00:02:20.000 --> 00:02:26.000]   Let LastPass improve your employees experience while safeguarding your business from cyber threats.
[00:02:26.000 --> 00:02:33.000]   LastPass is the number one password manager. Visit LastPass.com/twit to find out how they can help you.
[00:02:38.000 --> 00:02:43.000]   It's time for TWIT this week at Tech, the show we cover the week's tech news.
[00:02:43.000 --> 00:02:52.000]   And we have brought in the big guns for this week. Dan Patterson is here from CBS and CNET senior producer.
[00:02:52.000 --> 00:02:54.000]   Hey Dan, how are you?
[00:02:54.000 --> 00:02:56.000]   It's great to see you Leo.
[00:02:56.000 --> 00:02:57.000]   Good to see you.
[00:02:57.000 --> 00:02:58.000]   I'm well.
[00:02:58.000 --> 00:03:02.000]   You've got a nice haircut. You actually went into Brooklyn and got an artisanal haircut.
[00:03:02.000 --> 00:03:04.000]   It was a process.
[00:03:04.000 --> 00:03:07.000]   It was a protected process, but a process none the last.
[00:03:07.000 --> 00:03:09.000]   I don't like the shaggy look.
[00:03:09.000 --> 00:03:14.000]   I've been getting haircuts too, but we have somebody come to the house and we do it outside.
[00:03:14.000 --> 00:03:16.000]   So that's not going to be very tenable much longer.
[00:03:16.000 --> 00:03:18.000]   Actually, it's not right now.
[00:03:18.000 --> 00:03:19.000]   It was 111.
[00:03:19.000 --> 00:03:25.000]   That was my first haircut was my wife took the clippers to my head.
[00:03:25.000 --> 00:03:31.000]   It grew over the summertime and we decided to test Brooklyn's infrastructure.
[00:03:31.000 --> 00:03:37.000]   Flowbeat stock has gone through the roof fight here. Big times for Flowbeat.
[00:03:37.000 --> 00:03:43.000]   Also with us from our show, TWIT this week in enterprise tech and Microsoft, Lou Mareska.
[00:03:43.000 --> 00:03:44.000]   Hi, Lou, M.M.
[00:03:44.000 --> 00:03:46.000]   Hey Leo, great to be here.
[00:03:46.000 --> 00:03:47.000]   Good to see you.
[00:03:47.000 --> 00:03:49.000]   Is this your behind you?
[00:03:49.000 --> 00:03:51.000]   Is that your flight simulator set up back there?
[00:03:51.000 --> 00:03:52.000]   It is.
[00:03:52.000 --> 00:03:53.000]   It is.
[00:03:53.000 --> 00:03:58.000]   It's my new machine, the 64 core beast that I decided to build because if you're going to work from home,
[00:03:58.000 --> 00:03:59.000]   might as well work with a little style.
[00:03:59.000 --> 00:04:03.000]   And I bet you built it right before Nvidia announced their new cards.
[00:04:03.000 --> 00:04:07.000]   You know, I actually bought a gap filling card.
[00:04:07.000 --> 00:04:09.000]   You smart for only like 150 bucks.
[00:04:09.000 --> 00:04:10.000]   So smart.
[00:04:10.000 --> 00:04:11.000]   I did it waiting for the 3090.
[00:04:11.000 --> 00:04:12.000]   Yes.
[00:04:12.000 --> 00:04:15.000]   Is it an Intel or is it an AMD core?
[00:04:15.000 --> 00:04:16.000]   It's got an AMD.
[00:04:16.000 --> 00:04:18.000]   It's AMD as AMD.
[00:04:18.000 --> 00:04:19.000]   64 core Ryzen.
[00:04:19.000 --> 00:04:20.000]   Yes.
[00:04:20.000 --> 00:04:22.000]   How does that feel?
[00:04:22.000 --> 00:04:23.000]   What is that like?
[00:04:23.000 --> 00:04:24.000]   Oh, it feels so nice.
[00:04:24.000 --> 00:04:27.000]   You know, going to shutting down is just as fast as going to sleep.
[00:04:27.000 --> 00:04:31.000]   So I can just shut the machine down at night and come back up and hit it go and boom.
[00:04:31.000 --> 00:04:33.000]   About 10 seconds later, I'm back in the windows.
[00:04:33.000 --> 00:04:34.000]   I love it.
[00:04:34.000 --> 00:04:35.000]   And did you go out?
[00:04:35.000 --> 00:04:40.000]   I understand there's been a run on flight sticks and pedals and all of the gear that, you know,
[00:04:40.000 --> 00:04:42.000]   if you really want to make flightsim real.
[00:04:42.000 --> 00:04:44.000]   Did you go out and get all that stuff?
[00:04:44.000 --> 00:04:48.000]   You know, I have I have an old logic pack.
[00:04:48.000 --> 00:04:49.000]   Flight stick.
[00:04:49.000 --> 00:04:50.000]   Oh, that's good.
[00:04:50.000 --> 00:04:51.000]   That'll do.
[00:04:51.000 --> 00:04:52.000]   15 years ago.
[00:04:52.000 --> 00:04:53.000]   This is the flight.
[00:04:53.000 --> 00:04:54.000]   And it still works.
[00:04:54.000 --> 00:04:57.000]   By the way, I can't help but notice your beautiful headphones stand back there.
[00:04:57.000 --> 00:04:59.000]   Is that what that is?
[00:04:59.000 --> 00:05:00.000]   Yes.
[00:05:00.000 --> 00:05:03.000]   I made that about six months ago with a little bit of a fly.
[00:05:03.000 --> 00:05:04.000]   Beautiful.
[00:05:04.000 --> 00:05:05.600]   And yeah, some some hand strength.
[00:05:05.600 --> 00:05:07.000]   Did you bend it yourself?
[00:05:07.000 --> 00:05:08.000]   I did.
[00:05:08.000 --> 00:05:11.280]   Yeah, it took about a week with a lot of water and a lot of mineral oil, but it actually
[00:05:11.280 --> 00:05:12.280]   worked out.
[00:05:12.280 --> 00:05:17.440]   Oh, that's so I didn't realize Louisa would worker, but earlier today in the Twit chat,
[00:05:17.440 --> 00:05:21.000]   he was sending out pictures of the little desk he's building for his kids.
[00:05:21.000 --> 00:05:22.000]   Yeah.
[00:05:22.000 --> 00:05:27.000]   And then you hit your thumb with a hammer and now he's on the show.
[00:05:27.000 --> 00:05:29.000]   You're not a woodworker until you smash your fingers.
[00:05:29.000 --> 00:05:33.000]   So yeah, that's definitely it's part of the coming of age process.
[00:05:33.000 --> 00:05:35.000]   Great to have you, Lou.
[00:05:35.000 --> 00:05:41.000]   Also with us, it's been a long time since we've seen Mr. Rob read on the show, author
[00:05:41.000 --> 00:05:43.680]   of after on and many other great books.
[00:05:43.680 --> 00:05:46.000]   He's working on an audible original right now.
[00:05:46.000 --> 00:05:48.000]   And it's so nice to see you, Rob.
[00:05:48.000 --> 00:05:50.000]   Great to see you as well.
[00:05:50.000 --> 00:05:51.000]   Yeah.
[00:05:51.000 --> 00:05:52.000]   You'll be back on the show.
[00:05:52.000 --> 00:05:53.000]   It's my first time remote.
[00:05:53.000 --> 00:05:55.800]   Yeah, I know normally we'd get you to come up.
[00:05:55.800 --> 00:06:00.000]   You're not so far away, but you know, nobody's...
[00:06:00.000 --> 00:06:01.000]   Pandemic.
[00:06:01.000 --> 00:06:04.520]   Oh, it's not that you'd stick to the roadway that the tar is melting.
[00:06:04.520 --> 00:06:05.520]   It's true.
[00:06:05.520 --> 00:06:06.600]   It's 111 degrees out.
[00:06:06.600 --> 00:06:10.080]   It's never literally in all the years that have been keeping records.
[00:06:10.080 --> 00:06:12.680]   It's never been this warm in Petaluma.
[00:06:12.680 --> 00:06:14.560]   We are record heat up here.
[00:06:14.560 --> 00:06:16.560]   And I imagine you're not so far away in Marin.
[00:06:16.560 --> 00:06:19.160]   I would imagine it's similarly warm there.
[00:06:19.160 --> 00:06:21.840]   Yeah, I think you've got about 20 degrees on us.
[00:06:21.840 --> 00:06:24.600]   I think it's in the neighborhood of the low 90s.
[00:06:24.600 --> 00:06:25.600]   So it's...
[00:06:25.600 --> 00:06:29.720]   You're definitely setting records that we're not quite taunting just yet.
[00:06:29.720 --> 00:06:30.800]   I can't believe it.
[00:06:30.800 --> 00:06:31.800]   It's just incredible.
[00:06:31.800 --> 00:06:33.360]   It's now dropped down a little bit.
[00:06:33.360 --> 00:06:34.840]   We did hit it 111.
[00:06:34.840 --> 00:06:35.840]   So...
[00:06:35.840 --> 00:06:36.840]   Ooh.
[00:06:36.840 --> 00:06:37.840]   Ooh.
[00:06:37.840 --> 00:06:38.840]   Ooh.
[00:06:38.840 --> 00:06:43.680]   Rob gets a little bit of credit because he did a TED talk.
[00:06:43.680 --> 00:06:45.440]   When was that last year?
[00:06:45.440 --> 00:06:46.440]   Yeah.
[00:06:46.440 --> 00:06:47.440]   It was April.
[00:06:47.440 --> 00:06:50.320]   It was the annual conference that would have been April of last year.
[00:06:50.320 --> 00:06:51.320]   April 2019.
[00:06:51.320 --> 00:06:53.920]   It was a little bit more than a year ago, yeah.
[00:06:53.920 --> 00:06:57.440]   In which you talked about pandemics?
[00:06:57.440 --> 00:06:58.960]   I did.
[00:06:58.960 --> 00:06:59.960]   How did you...
[00:06:59.960 --> 00:07:00.960]   What did you say?
[00:07:00.960 --> 00:07:01.960]   I haven't seen it yet.
[00:07:01.960 --> 00:07:03.720]   I've seen you earlier TED talks.
[00:07:03.720 --> 00:07:06.880]   But what was this one about?
[00:07:06.880 --> 00:07:12.000]   This one is about something called the Carlson Curve, which is basically the Moore's Law
[00:07:12.000 --> 00:07:14.680]   Curve for synthetic biology.
[00:07:14.680 --> 00:07:20.520]   And like Moore's Law, it is an exponential curve, which means that the things that practitioners
[00:07:20.520 --> 00:07:26.880]   of synthetic biology and life sciences in general can do are getting better and transforming
[00:07:26.880 --> 00:07:29.240]   with incredible speed.
[00:07:29.240 --> 00:07:36.000]   And one of the ramifications of that, that kind of alludes our intuitions because we're
[00:07:36.000 --> 00:07:40.800]   not really configured to intuitively understand exponential growth because we didn't...our
[00:07:40.800 --> 00:07:44.840]   ancestors didn't really encounter it on evolutionary timescales.
[00:07:44.840 --> 00:07:50.800]   One of the ramifications of that is the things that only the most brilliant and well-equipped
[00:07:50.800 --> 00:07:58.120]   professors can do in the most rarefied ivory towers is pretty much inevitably coming soon
[00:07:58.120 --> 00:08:03.920]   to a high school lab near you, much as the music could only be done in high-end labs.
[00:08:03.920 --> 00:08:04.920]   Just what you want.
[00:08:04.920 --> 00:08:10.640]   16-year-olds working on new paradigms in biology.
[00:08:10.640 --> 00:08:16.520]   By doing things that the entire synthetic biology industry cannot do today, that will
[00:08:16.520 --> 00:08:22.480]   definitely become possible in a high school lab within a very low, kind of a single digit,
[00:08:22.480 --> 00:08:25.920]   low single digit number of decades.
[00:08:25.920 --> 00:08:32.120]   Think about the kinds of things that could only be done in academic labs in computing
[00:08:32.120 --> 00:08:38.400]   in the 1970s and ask yourself, how long was it after that before anybody who cared to
[00:08:38.400 --> 00:08:40.600]   own a personal computer could do that thing?
[00:08:40.600 --> 00:08:41.600]   Whatever it was.
[00:08:41.600 --> 00:08:46.720]   And the answer was probably in most cases 15-ish years, 20 years.
[00:08:46.720 --> 00:08:51.160]   Think about a high-end academic computing lab in 1973.
[00:08:51.160 --> 00:08:55.360]   My guess is pretty much everything that was being done there could be done on a home computer
[00:08:55.360 --> 00:08:57.360]   by the early '90s.
[00:08:57.360 --> 00:09:01.800]   And there's no reason that this curve is going to move any slower than that.
[00:09:01.800 --> 00:09:09.120]   And what's frightening about that is that it is possible for brilliant academics if they
[00:09:09.120 --> 00:09:14.480]   wanted to, and luckily they don't, to create some really nasty pathogens.
[00:09:14.480 --> 00:09:20.600]   And because the admissions committee to becoming a brilliant academic usually involves many
[00:09:20.600 --> 00:09:26.800]   years of very stable career growth, that kind of weeds out people who might do something
[00:09:26.800 --> 00:09:32.360]   spectacularly bad for spectacularly bad reasons.
[00:09:32.360 --> 00:09:37.840]   But when that technology proliferates to the point where millions of people or even more
[00:09:37.840 --> 00:09:45.680]   have access to it, we can't count on absolutely everybody being perfectly stable and not doing
[00:09:45.680 --> 00:09:48.840]   something really rogue with an artificial pathogen.
[00:09:48.840 --> 00:09:52.360]   So that was the subject of the TED Talk, and that's also the subject of this audible original
[00:09:52.360 --> 00:09:53.520]   that I'm working on.
[00:09:53.520 --> 00:09:55.960]   It's basically an expansion of that TED Talk.
[00:09:55.960 --> 00:10:01.200]   It's an interesting topic because in the early days of the internet, and Moore's Law was
[00:10:01.200 --> 00:10:07.440]   well known, and we knew that we were going to see exponential growth in computer technology.
[00:10:07.440 --> 00:10:11.600]   Many people, and I'm going to include myself, were utopianists, we said, "Ah, this is the
[00:10:11.600 --> 00:10:12.600]   greatest thing.
[00:10:12.600 --> 00:10:17.440]   You're going to eliminate gatekeepers, everybody will have a voice."
[00:10:17.440 --> 00:10:21.880]   And we didn't do probably as much thinking as we should have about the potential for
[00:10:21.880 --> 00:10:27.000]   negative consequences, which we are starting to see today.
[00:10:27.000 --> 00:10:32.440]   Maybe it's time to start thinking about if you have a Moore's Law in biology thinking
[00:10:32.440 --> 00:10:33.680]   about the impact of that.
[00:10:33.680 --> 00:10:35.320]   And obviously some of it will be positive.
[00:10:35.320 --> 00:10:39.440]   I mean, again, utopianists that I am, I think, "Oh, that's great.
[00:10:39.440 --> 00:10:42.120]   Kids in high school will be solving, will be curing COVID.
[00:10:42.120 --> 00:10:43.120]   It'll be amazing."
[00:10:43.120 --> 00:10:47.960]   But they may also be creating deadly pathogens at the same time.
[00:10:47.960 --> 00:10:50.520]   And so it's important to kind of think about the consequences.
[00:10:50.520 --> 00:10:52.040]   This idea, though, Rob is not new.
[00:10:52.040 --> 00:10:57.920]   I remember Bill Gates when he was still running Microsoft saying the next big thing, you know,
[00:10:57.920 --> 00:11:03.700]   he used to go on these binges reading about topics, his summer reading list.
[00:11:03.700 --> 00:11:11.380]   One of them one year was exactly this was about genetics and biology and the kind of
[00:11:11.380 --> 00:11:13.700]   the future biotech revolution.
[00:11:13.700 --> 00:11:16.520]   So that was probably 25, 30 years ago.
[00:11:16.520 --> 00:11:19.480]   We've known this is coming.
[00:11:19.480 --> 00:11:23.880]   So it's your sense that it's actually getting closer, that it actually is starting.
[00:11:23.880 --> 00:11:25.160]   It's getting a lot closer.
[00:11:25.160 --> 00:11:32.000]   Yeah, it's like when you double something on a regular basis, you know, it's a very quiet
[00:11:32.000 --> 00:11:33.000]   process for a long time.
[00:11:33.000 --> 00:11:34.000]   Yeah.
[00:11:34.000 --> 00:11:36.820]   Yeah, then all of a sudden it becomes highly noticeable.
[00:11:36.820 --> 00:11:40.860]   It's kind of like that, you know, the puzzle that a lot of, you know, parents teach their
[00:11:40.860 --> 00:11:45.900]   kids, would you rather have a million dollars or a penny that doubles every day for a month?
[00:11:45.900 --> 00:11:49.980]   And if you pick the penny that doubles every day for a month, even around the 22nd or the
[00:11:49.980 --> 00:11:53.820]   23rd of the month, if you look at the sum of money, it's looking like you made the wrong
[00:11:53.820 --> 00:11:54.820]   choice.
[00:11:54.820 --> 00:11:59.340]   And, you know, then all of a sudden it's the 29th of the month.
[00:11:59.340 --> 00:12:04.200]   So I think with SinBio, it's maybe not quite the 29th of the month, but it's definitely
[00:12:04.200 --> 00:12:06.280]   getting well into the 20s.
[00:12:06.280 --> 00:12:07.280]   That's a good idea.
[00:12:07.280 --> 00:12:09.360]   And we need to start thinking about what happens.
[00:12:09.360 --> 00:12:12.960]   I'm sorry to interrupt, but I'm very curious as you're talking.
[00:12:12.960 --> 00:12:17.840]   I wonder if there are instances in history, or at least modern history, maybe anthropologists
[00:12:17.840 --> 00:12:25.100]   would know better, but have cultures decided to eliminate or elect to not use or for lack
[00:12:25.100 --> 00:12:29.960]   of better term delete technologies before they reach that exponential point, meaning
[00:12:29.960 --> 00:12:34.800]   have they identified potential dangers and said, this is the dangers outgrow outweigh
[00:12:34.800 --> 00:12:36.500]   the benefits for our society.
[00:12:36.500 --> 00:12:39.120]   Let's get rid of this technology.
[00:12:39.120 --> 00:12:40.800]   Nothing that I can think of.
[00:12:40.800 --> 00:12:45.040]   And in this particular case, I agree on the idealist front.
[00:12:45.040 --> 00:12:51.160]   I'm with Leo, and then I think that the promise of SinBio is spectacular.
[00:12:51.160 --> 00:12:54.400]   And a tech ban is essentially an impossibility.
[00:12:54.400 --> 00:12:57.900]   I mean, you think about what we've done with nuclear proliferation.
[00:12:57.900 --> 00:12:59.820]   That's been hard enough.
[00:12:59.820 --> 00:13:04.980]   Nuclear weapons since the non-proliferation agreement was signed whenever it was signed
[00:13:04.980 --> 00:13:07.980]   maybe in the '70s or earlier.
[00:13:07.980 --> 00:13:10.420]   India, Pakistan have acquired nukes.
[00:13:10.420 --> 00:13:12.860]   Iran is right on the cusp.
[00:13:12.860 --> 00:13:18.940]   And that is easy to monitor because it requires an enormous industrial complex to make nuclear
[00:13:18.940 --> 00:13:20.100]   weapons.
[00:13:20.100 --> 00:13:26.400]   Because biology can be practiced very much on the sly in tiny, tiny little laboratories
[00:13:26.400 --> 00:13:29.940]   that can be barely larger than a conference room.
[00:13:29.940 --> 00:13:37.120]   And so if there was some kind of a tech ban, not only would we be foreclosing an unbelievably
[00:13:37.120 --> 00:13:43.920]   wealthy aspect of our future or promising aspect of our future, but how could we count
[00:13:43.920 --> 00:13:48.760]   on China and Russia to honor the ban and how could they count on us?
[00:13:48.760 --> 00:13:51.840]   And how could any of us trust North Korea?
[00:13:51.840 --> 00:13:58.720]   If you implement a SinBio ban, you're almost by definition giving arrival a SinBio monopoly.
[00:13:58.720 --> 00:14:03.760]   So I think we kind of need to lean into this and say, "SinBio is upon us.
[00:14:03.760 --> 00:14:05.560]   It has incredible promise.
[00:14:05.560 --> 00:14:07.920]   There are some extraordinary dangers.
[00:14:07.920 --> 00:14:12.200]   Let's be as clear-eyed as possible about the dangers before they become clear and present
[00:14:12.200 --> 00:14:18.640]   and start today taking countermeasures against the kinds of things that we can very plainly
[00:14:18.640 --> 00:14:22.680]   imagine bad guys doing in 5, 10, 15, 20 years."
[00:14:22.680 --> 00:14:24.920]   And that's what I advocate for.
[00:14:24.920 --> 00:14:29.400]   And the good news is in this particular equation, like if you think of somebody wanting to make
[00:14:29.400 --> 00:14:31.280]   a doomsday bug, well, who would do that?
[00:14:31.280 --> 00:14:32.280]   Well, I don't know.
[00:14:32.280 --> 00:14:38.160]   The Vegas shooter, if he had the capability to create a doomsday bug might well have done
[00:14:38.160 --> 00:14:39.760]   that.
[00:14:39.760 --> 00:14:46.440]   But the bar to being a good guy versus a bad guy when the act is create doomsday bug that
[00:14:46.440 --> 00:14:49.120]   imperils humanity is incredibly low.
[00:14:49.120 --> 00:14:52.600]   Al Capone would be on our side on this one, right?
[00:14:52.600 --> 00:14:59.400]   So there is a history and to answer a little bit to answer your question, Dan, of us at
[00:14:59.400 --> 00:15:06.600]   least in warfare using things like the Geneva Convention to eschew things that would be so
[00:15:06.600 --> 00:15:13.720]   destructive nuclear anti-proliferation or the use of biological warfare that we all
[00:15:13.720 --> 00:15:17.160]   agree, yeah, let's not do that.
[00:15:17.160 --> 00:15:18.160]   That's the limit access.
[00:15:18.160 --> 00:15:21.400]   At least I don't mean a ban, but it's the limit access.
[00:15:21.400 --> 00:15:25.920]   But I have to say that's the only example I can think of and almost always the counter
[00:15:25.920 --> 00:15:31.640]   example is almost always the case where, hey, if we can invent it, let's do it.
[00:15:31.640 --> 00:15:36.720]   And we haven't been very good at controlling this stuff at all.
[00:15:36.720 --> 00:15:44.120]   Even nuclear anti-proliferation is kind of a back-filling and the long run probably will
[00:15:44.120 --> 00:15:47.000]   fail.
[00:15:47.000 --> 00:15:55.960]   So I don't want to be dystopian right off the bat, but it's a good warning, a really
[00:15:55.960 --> 00:15:56.960]   good warning, Rob.
[00:15:56.960 --> 00:16:01.200]   I'll look forward to hearing that audible original.
[00:16:01.200 --> 00:16:06.200]   It actually reminds me a little bit about, and we were talking everybody on this panel
[00:16:06.200 --> 00:16:11.520]   and myself included, we're kind of cocooning right now, right?
[00:16:11.520 --> 00:16:14.320]   We've all found places to go.
[00:16:14.320 --> 00:16:16.440]   We've all kind of battened down the hatches.
[00:16:16.440 --> 00:16:21.720]   A great piece by Doug Rushkoff in the medium.
[00:16:21.720 --> 00:16:25.000]   The privileged have entered their escape pods.
[00:16:25.000 --> 00:16:29.880]   This is kind of related, Rob, because it said technology gave us the dream of a cocooned
[00:16:29.880 --> 00:16:31.840]   future.
[00:16:31.840 --> 00:16:34.040]   And now we're living it.
[00:16:34.040 --> 00:16:38.280]   If you're privileged enough not to have to work as an essential worker in a meatpacking
[00:16:38.280 --> 00:16:44.640]   plant or a grocery store or a health care facility, if you can batten down the hatches,
[00:16:44.640 --> 00:16:48.560]   continue to make money, get yourself fed, get your family taken care of, we're all kind
[00:16:48.560 --> 00:16:49.760]   of gone back to the womb.
[00:16:49.760 --> 00:16:55.200]   He tells a really good, actually this is the best part of the article.
[00:16:55.200 --> 00:16:58.320]   He tells a story about Timothy Leary.
[00:16:58.320 --> 00:17:07.080]   He says, I remember back around 1990 when Tim Leary, proponent of LSD, of course, first
[00:17:07.080 --> 00:17:12.240]   read the media lab book by Stuart Brand about the new technology center that MIT had created
[00:17:12.240 --> 00:17:14.240]   in its architecture department.
[00:17:14.240 --> 00:17:19.320]   Leary read it all in one day around sunset, just as he was finishing, he threw it across
[00:17:19.320 --> 00:17:21.800]   the living room and discussed.
[00:17:21.800 --> 00:17:27.160]   Look at the index, he said, all of the names, less than 3% of all the names, less than 3%
[00:17:27.160 --> 00:17:28.160]   more women.
[00:17:28.160 --> 00:17:30.560]   That'll tell you something.
[00:17:30.560 --> 00:17:36.800]   He said, my problem with media lab, and maybe this is a problem with technology in general,
[00:17:36.800 --> 00:17:41.160]   is they want to recreate the womb.
[00:17:41.160 --> 00:17:45.400]   As Leary, who was a trained psychologist, saw it, the boys building our digital future
[00:17:45.400 --> 00:17:50.240]   were developing technology to simulate the ideal woman, the one their mothers could never
[00:17:50.240 --> 00:17:54.960]   be, unlike their human mothers, a predictive algorithm could anticipate their every need
[00:17:54.960 --> 00:18:00.840]   in advance and deliver it directly, removing every trace of friction and longing.
[00:18:00.840 --> 00:18:04.160]   These guys would be able to float in virtual bubbles.
[00:18:04.160 --> 00:18:08.760]   What the media lab called artificial ecology, and never have to face the messy harsh reality
[00:18:08.760 --> 00:18:13.760]   demanded of people living in a real world with, oh, let's say women and people of color,
[00:18:13.760 --> 00:18:16.720]   or people whose views differ.
[00:18:16.720 --> 00:18:17.720]   What do you think?
[00:18:17.720 --> 00:18:21.920]   Are we as COVID forced us into our escape pods?
[00:18:21.920 --> 00:18:24.520]   Sounds very timlier.
[00:18:24.520 --> 00:18:25.520]   It's wild.
[00:18:25.520 --> 00:18:30.000]   Yeah, 1990, Leary said that.
[00:18:30.000 --> 00:18:36.120]   He did pinpoint a big problem with this, which is the lack of diversity in Silicon Valley.
[00:18:36.120 --> 00:18:40.800]   We have been having very similar conversations, at least parallel conversations, at least going
[00:18:40.800 --> 00:18:42.160]   back the last half-decade.
[00:18:42.160 --> 00:18:46.840]   I wish that we had been prescient enough to talk about this for the last 20 or 30 years.
[00:18:46.840 --> 00:18:53.280]   We have been talking about, especially with sexism in Silicon Valley, how many of these
[00:18:53.280 --> 00:19:01.000]   companies are built by men to solve problems that men who are of upper middle class have.
[00:19:01.000 --> 00:19:06.720]   And I think that we really don't have to look much further than people like Cara Swisher
[00:19:06.720 --> 00:19:12.680]   and her Emily Chang is another great example, to find people who have really discussed this
[00:19:12.680 --> 00:19:14.200]   in pretty great detail.
[00:19:14.200 --> 00:19:19.360]   Well, it's easy if you're a woman or a person of color to say, "Look, it's all a bunch of
[00:19:19.360 --> 00:19:22.280]   white guys doing all this stuff."
[00:19:22.280 --> 00:19:26.240]   It's easy to say, "Look, the problem is getting the white guys to say, 'Come on in.
[00:19:26.240 --> 00:19:28.040]   Why don't you try it?'"
[00:19:28.040 --> 00:19:33.160]   And I think with all the diversity programs, with all the diversity programs we've seen
[00:19:33.160 --> 00:19:37.960]   in all these companies, I don't see a lot of results.
[00:19:37.960 --> 00:19:38.960]   Yeah.
[00:19:38.960 --> 00:19:40.920]   It takes time.
[00:19:40.920 --> 00:19:41.920]   It takes time.
[00:19:41.920 --> 00:19:43.200]   It says the guy works for Microsoft.
[00:19:43.200 --> 00:19:44.200]   Yeah, it takes time.
[00:19:44.200 --> 00:19:45.200]   Yeah, you know.
[00:19:45.200 --> 00:19:47.160]   I've been part of a lot of these programs.
[00:19:47.160 --> 00:19:48.400]   I mean, I did outreach.
[00:19:48.400 --> 00:19:54.120]   I've gone to places and recruited in places we've never recruited before.
[00:19:54.120 --> 00:20:01.080]   We put these, we tried to normalize how we post things and make sure they're more inclusive.
[00:20:01.080 --> 00:20:07.280]   And we still don't get the amount of applicants that we expect.
[00:20:07.280 --> 00:20:10.560]   We can't go out and find the applicants that we expect.
[00:20:10.560 --> 00:20:16.160]   It's a systemic problem because you're operating at the top of the pile, but you've got to
[00:20:16.160 --> 00:20:18.200]   start putting people in at the bottom.
[00:20:18.200 --> 00:20:24.400]   You've got to get girls in grade school, junior high school, to be embraced in math and science
[00:20:24.400 --> 00:20:26.680]   programs, right?
[00:20:26.680 --> 00:20:30.520]   Because they got to work their way up through it so that when you're at the top of the pile,
[00:20:30.520 --> 00:20:32.280]   you have some choice.
[00:20:32.280 --> 00:20:33.280]   Right?
[00:20:33.280 --> 00:20:34.280]   Yeah, absolutely.
[00:20:34.280 --> 00:20:35.280]   Absolutely.
[00:20:35.280 --> 00:20:37.760]   I think you definitely have to get them started sooner.
[00:20:37.760 --> 00:20:41.320]   A lot of high school programs we're starting to see, you know, we do a lot of high school
[00:20:41.320 --> 00:20:46.280]   programs at Microsoft and we're starting to see this a lot where we get more people enthusiastic
[00:20:46.280 --> 00:20:47.280]   about it.
[00:20:47.280 --> 00:20:48.280]   Absolutely.
[00:20:48.280 --> 00:20:49.280]   Yeah.
[00:20:49.280 --> 00:20:54.440]   Well, I don't know how we started with this dystopian.
[00:20:54.440 --> 00:20:55.440]   You know, it's funny.
[00:20:55.440 --> 00:21:04.160]   These days, it's so easy on all our shows to just kind of make it a downer because it's
[00:21:04.160 --> 00:21:08.440]   very easy to find plenty of stories about how technology's gone wrong and things are
[00:21:08.440 --> 00:21:10.480]   going wrong.
[00:21:10.480 --> 00:21:14.480]   It's a lot harder to find things to get excited about.
[00:21:14.480 --> 00:21:19.000]   I tell you one thing I am excited about and actually in the light of this, it all seems
[00:21:19.000 --> 00:21:22.480]   really trivial and stupid, but you've already got it, Lou.
[00:21:22.480 --> 00:21:24.920]   You've had it for a while and that's my duo phone.
[00:21:24.920 --> 00:21:25.920]   I'm sorry.
[00:21:25.920 --> 00:21:26.920]   He's not a phone.
[00:21:26.920 --> 00:21:30.360]   My Microsoft Duo folding do hickey.
[00:21:30.360 --> 00:21:32.760]   Can you still not talk about it?
[00:21:32.760 --> 00:21:34.440]   No, I mean, we could talk about it.
[00:21:34.440 --> 00:21:37.440]   I think the idea here is I can tell you how I feel about it.
[00:21:37.440 --> 00:21:40.520]   I think it's- But you can't turn it on and show me yet.
[00:21:40.520 --> 00:21:41.520]   Can turn it on.
[00:21:41.520 --> 00:21:47.160]   It's definitely an expensive device, but it's definitely feels to me like a device that
[00:21:47.160 --> 00:21:49.040]   people are going to really enjoy.
[00:21:49.040 --> 00:21:52.320]   Like in fact, the camera is actually better than you think.
[00:21:52.320 --> 00:21:58.920]   So there's two questions unanswered because reviewers who have the phone until September
[00:21:58.920 --> 00:22:02.200]   10th can't even turn it on and show you.
[00:22:02.200 --> 00:22:04.640]   And they can't post pictures, which is the thing.
[00:22:04.640 --> 00:22:10.280]   So the two questions I have are battery life and picture quality.
[00:22:10.280 --> 00:22:13.400]   We can't tell from the specs if it's going to be a decent camera.
[00:22:13.400 --> 00:22:18.040]   And I don't expect it to be pixel four quality or iPhone 11 quality.
[00:22:18.040 --> 00:22:22.760]   No, but it doesn't have to be if it's decent quality.
[00:22:22.760 --> 00:22:29.240]   And frankly, I guess that really the real question is the computational ability in the
[00:22:29.240 --> 00:22:31.040]   device.
[00:22:31.040 --> 00:22:34.080]   Because it's not the sensor anymore.
[00:22:34.080 --> 00:22:39.240]   It's how smart the software is about what it's captured, right?
[00:22:39.240 --> 00:22:41.040]   You can't respond to that, I know.
[00:22:41.040 --> 00:22:45.320]   Well, what I can tell you is there are actually a lot of people on Microsoft who've been self
[00:22:45.320 --> 00:22:47.560]   posting for a while that post pictures all the time.
[00:22:47.560 --> 00:22:51.000]   So I'm linked in recently from one of the CVPs.
[00:22:51.000 --> 00:22:53.760]   So you can go check them out for sure.
[00:22:53.760 --> 00:22:59.240]   But I can tell you, Microsoft focuses on the unification of software services and hardware.
[00:22:59.240 --> 00:23:03.360]   And I think what you'll find is that this device takes it to that level.
[00:23:03.360 --> 00:23:06.120]   And I'm not trying to throw out an advertisement here.
[00:23:06.120 --> 00:23:10.320]   I have an iPhone and iPod devices all over the iPad.
[00:23:10.320 --> 00:23:14.480]   No, in fact, Ralud, one of the things I love about you is you're not a shill for Microsoft.
[00:23:14.480 --> 00:23:16.680]   And it's great to have somebody on the inside.
[00:23:16.680 --> 00:23:18.640]   You work on the Azure team, right?
[00:23:18.640 --> 00:23:20.160]   I work on the Office team.
[00:23:20.160 --> 00:23:21.160]   Office team, sorry.
[00:23:21.160 --> 00:23:22.160]   Yeah, that's okay.
[00:23:22.160 --> 00:23:23.560]   I work on the Excel team, actually.
[00:23:23.560 --> 00:23:26.320]   I'm encapsulated within the Excel team for Office platform.
[00:23:26.320 --> 00:23:27.320]   But yeah.
[00:23:27.320 --> 00:23:28.480]   But you were always dead honest.
[00:23:28.480 --> 00:23:30.280]   And then otherwise, we wouldn't have you hosting Twiit.
[00:23:30.280 --> 00:23:31.840]   We wouldn't have you on our shows all the time.
[00:23:31.840 --> 00:23:33.200]   You're always really good about that.
[00:23:33.200 --> 00:23:36.880]   So it's nice to have somebody who knows this stuff, who could talk about it.
[00:23:36.880 --> 00:23:37.880]   So good.
[00:23:37.880 --> 00:23:40.120]   Well, I'll know on Friday.
[00:23:40.120 --> 00:23:42.560]   I can't wait.
[00:23:42.560 --> 00:23:46.680]   I'll be showing pictures next week.
[00:23:46.680 --> 00:23:51.600]   Yeah, I think for me, the reason I was willing to spend actually about $1,500 because I got
[00:23:51.600 --> 00:23:56.440]   the extra storage so much on that phone is because I think I'm dying for a new form
[00:23:56.440 --> 00:23:57.800]   factor.
[00:23:57.800 --> 00:24:00.520]   And I'm not-- it was really interesting that in the Z Fold.
[00:24:00.520 --> 00:24:05.720]   We now have seen the price and availability for Samsung's folding phone.
[00:24:05.720 --> 00:24:08.080]   They had an event this week.
[00:24:08.080 --> 00:24:17.160]   And I'm still not convinced that a folding screen is going to be anything but janky.
[00:24:17.160 --> 00:24:24.720]   So that's my-- either any of you guys have a Fold you can talk about?
[00:24:24.720 --> 00:24:25.720]   No.
[00:24:25.720 --> 00:24:26.720]   All right.
[00:24:26.720 --> 00:24:27.720]   No.
[00:24:27.720 --> 00:24:28.720]   I mean, come on.
[00:24:28.720 --> 00:24:29.720]   How long could that last?
[00:24:29.720 --> 00:24:30.720]   $2,000 phone.
[00:24:30.720 --> 00:24:31.720]   And yeah.
[00:24:31.720 --> 00:24:32.720]   How long?
[00:24:32.720 --> 00:24:33.720]   Yeah.
[00:24:33.720 --> 00:24:34.720]   And it's going to have a crease down the middle.
[00:24:34.720 --> 00:24:35.720]   Yeah.
[00:24:35.720 --> 00:24:36.720]   All right.
[00:24:36.720 --> 00:24:41.400]   So nobody I know has ordered one.
[00:24:41.400 --> 00:24:42.400]   Anybody in the chat room?
[00:24:42.400 --> 00:24:43.560]   Are you-- anybody?
[00:24:43.560 --> 00:24:44.560]   Anybody?
[00:24:44.560 --> 00:24:48.800]   You know what I think is interesting about the duo that didn't-- that kind of got under
[00:24:48.800 --> 00:24:54.280]   played is that Microsoft for the first time ever is swapped positions.
[00:24:54.280 --> 00:24:57.920]   It is now an OEM of somebody's else's operating system.
[00:24:57.920 --> 00:25:02.320]   For years, Microsoft made a living making operating systems for other manufacturers
[00:25:02.320 --> 00:25:03.640]   to put on their computers.
[00:25:03.640 --> 00:25:08.040]   For the first time ever, Microsoft is making a computer to run Android.
[00:25:08.040 --> 00:25:09.360]   Somebody else's operating system.
[00:25:09.360 --> 00:25:13.720]   That's just weird.
[00:25:13.720 --> 00:25:14.720]   Who knows?
[00:25:14.720 --> 00:25:15.720]   You might see more of that.
[00:25:15.720 --> 00:25:16.720]   The future?
[00:25:16.720 --> 00:25:17.720]   Maybe.
[00:25:17.720 --> 00:25:18.720]   Yeah.
[00:25:18.720 --> 00:25:19.720]   Yeah.
[00:25:19.720 --> 00:25:20.720]   Maybe it's the future.
[00:25:20.720 --> 00:25:21.720]   I don't know.
[00:25:21.720 --> 00:25:23.760]   Maybe we've been the blood and Linux in Azure for a long time now.
[00:25:23.760 --> 00:25:26.280]   Well, SQL Server is going to be installed on Azure.
[00:25:26.280 --> 00:25:28.360]   I mean, on Linux pretty soon.
[00:25:28.360 --> 00:25:29.360]   I mean, what is actually?
[00:25:29.360 --> 00:25:31.120]   So I think it's a new generation.
[00:25:31.120 --> 00:25:32.600]   It's a new world we live in here.
[00:25:32.600 --> 00:25:34.120]   It's a different world.
[00:25:34.120 --> 00:25:40.200]   Linux went from being a cancer to having a Microsoft kernel on Windows 10, a Microsoft
[00:25:40.200 --> 00:25:42.560]   Linux kernel on Windows 10.
[00:25:42.560 --> 00:25:43.560]   All right.
[00:25:43.560 --> 00:25:48.440]   We're going to take a little break and we'll actually get to some actual news in just a
[00:25:48.440 --> 00:25:49.440]   bit.
[00:25:49.440 --> 00:25:50.440]   Kind of a great panel though.
[00:25:50.440 --> 00:25:54.720]   When I have a panel like you guys, I often don't want to just do news.
[00:25:54.720 --> 00:25:59.560]   I like doing what we were doing with you, Rob, talking about, you know, futures and things
[00:25:59.560 --> 00:26:05.960]   like when a high school kid is going to make a pathogen that's going to kill us all.
[00:26:05.960 --> 00:26:06.960]   That's exciting.
[00:26:06.960 --> 00:26:07.960]   That's great stuff.
[00:26:07.960 --> 00:26:08.960]   That's fun stuff.
[00:26:08.960 --> 00:26:09.960]   Oh, God.
[00:26:09.960 --> 00:26:18.240]   Now, I am going to point out that it has happened in the past many times that we've
[00:26:18.240 --> 00:26:22.240]   been bullish on technologies that haven't taken off.
[00:26:22.240 --> 00:26:25.440]   We've talked about this exponential curve with AI.
[00:26:25.440 --> 00:26:27.640]   It hasn't taken off.
[00:26:27.640 --> 00:26:32.000]   Frankly, these voice assistants are plateaued for years.
[00:26:32.000 --> 00:26:33.000]   We're consumers.
[00:26:33.000 --> 00:26:35.840]   I mean, AI in the enterprise is fantastic.
[00:26:35.840 --> 00:26:36.840]   Is it?
[00:26:36.840 --> 00:26:39.840]   Or are they just falling for it?
[00:26:39.840 --> 00:26:43.720]   I think it, you know, fantastic is the wrong hyperbolic language to use.
[00:26:43.720 --> 00:26:46.160]   And I'm sure that others know the enterprise much better.
[00:26:46.160 --> 00:26:51.840]   But I think that it is something that has led to some considerable cost savings.
[00:26:51.840 --> 00:26:55.280]   But it hasn't created the Terminator.
[00:26:55.280 --> 00:26:57.240]   It hasn't created Westworld.
[00:26:57.240 --> 00:27:00.160]   Or even Hal 9000.
[00:27:00.160 --> 00:27:02.840]   So I don't know.
[00:27:02.840 --> 00:27:03.840]   Business intelligence.
[00:27:03.840 --> 00:27:05.840]   Okay, fine.
[00:27:05.840 --> 00:27:08.840]   AI is one example.
[00:27:08.840 --> 00:27:14.240]   There are others in technology where, you know, we never did get flying cars.
[00:27:14.240 --> 00:27:19.840]   They're, you know, autonomous self-driving vehicles are making strides.
[00:27:19.840 --> 00:27:25.360]   But I would have thought we'd be at the hockey stick curve by now.
[00:27:25.360 --> 00:27:27.520]   We're not there.
[00:27:27.520 --> 00:27:30.560]   I think some of these problems are more difficult.
[00:27:30.560 --> 00:27:32.720]   CRISPR is a good example, right?
[00:27:32.720 --> 00:27:37.800]   Rob, we really thought CRISPR would end up being an amazing gene splicing capability,
[00:27:37.800 --> 00:27:39.600]   cutting and pasting of genes.
[00:27:39.600 --> 00:27:43.720]   But it turns out we don't know enough about genes to know when we're cutting what looks
[00:27:43.720 --> 00:27:44.720]   like junk DNA.
[00:27:44.720 --> 00:27:51.080]   And it actually is vital for the survival of the organism.
[00:27:51.080 --> 00:27:54.760]   And so CRISPR is maybe not living up to its potential.
[00:27:54.760 --> 00:27:56.200]   Is that, that's my understanding?
[00:27:56.200 --> 00:27:57.200]   Is that right?
[00:27:57.200 --> 00:28:04.760]   You know, I would say that so much really skilled lab time was being lost to the sort
[00:28:04.760 --> 00:28:09.400]   of splicing that CRISPR makes possible, the cutting and splicing and pasting that CRISPR
[00:28:09.400 --> 00:28:11.760]   makes so much easier.
[00:28:11.760 --> 00:28:16.560]   But we didn't know enough about genes before CRISPR.
[00:28:16.560 --> 00:28:21.200]   What CRISPR's ad then is allowing us to do is to learn those lessons so much faster,
[00:28:21.200 --> 00:28:25.640]   to learn the limits of our knowledge, you know, because it was just unbelievably skilled
[00:28:25.640 --> 00:28:33.800]   people in labs were lavishing time on processes that really were ultimately just sort of wrote.
[00:28:33.800 --> 00:28:35.920]   But they took a great deal of skill to do.
[00:28:35.920 --> 00:28:39.480]   And CRISPR is really accelerated the learning that we can do.
[00:28:39.480 --> 00:28:43.960]   And again, like I said, finding out like, oh my God, junk DNA matters more than we thought
[00:28:43.960 --> 00:28:48.600]   because we've done all these splicing exercises that we never would have done before.
[00:28:48.600 --> 00:28:52.080]   Because it would have taken too much time to do something that would have seemed trivial.
[00:28:52.080 --> 00:28:54.240]   So I think it really makes a big difference.
[00:28:54.240 --> 00:28:59.280]   We may not have created a cut and paste for genes, but we at least we've learned a lot
[00:28:59.280 --> 00:29:00.280]   from that.
[00:29:00.280 --> 00:29:06.200]   For instance, there's some evidence that using CRISPR eliminates a cell's cancer resistance.
[00:29:06.200 --> 00:29:07.200]   Probably because it's-
[00:29:07.200 --> 00:29:08.960]   It can, depending on what you've cut, it can.
[00:29:08.960 --> 00:29:09.960]   Right.
[00:29:09.960 --> 00:29:10.960]   Whoops.
[00:29:10.960 --> 00:29:19.040]   But as for the Terminator, Leo, I interviewed a bunch of astrophysicists and astronomers
[00:29:19.040 --> 00:29:21.160]   for my podcast.
[00:29:21.160 --> 00:29:26.960]   And they have a famous saying in astronomy, which is, "It's never aliens until it's
[00:29:26.960 --> 00:29:27.960]   aliens."
[00:29:27.960 --> 00:29:28.960]   Yes.
[00:29:28.960 --> 00:29:33.320]   And that's meant to caution people when they find a weird phenomenon out in the sky to
[00:29:33.320 --> 00:29:34.760]   not default to aliens.
[00:29:34.760 --> 00:29:40.240]   But it also reminds people that in all likelihood, given this enormity of outer space, there
[00:29:40.240 --> 00:29:41.560]   are aliens out there.
[00:29:41.560 --> 00:29:45.360]   So I would translate that to the Terminator with AI.
[00:29:45.360 --> 00:29:48.960]   It's never the Terminator until it's the Terminator.
[00:29:48.960 --> 00:29:54.960]   Or in the case of LAX, it's never a guy in a jetpack until-
[00:29:54.960 --> 00:29:57.920]   It's a guy in a jetpack.
[00:29:57.920 --> 00:30:01.720]   This was the story earlier this week.
[00:30:01.720 --> 00:30:05.160]   We just saw the guys pass by us.
[00:30:05.160 --> 00:30:12.120]   We got blue 23, E. Scott and President Jetpack reported 300 yards down to the LA final at
[00:30:12.120 --> 00:30:13.560]   about 3000 feet.
[00:30:13.560 --> 00:30:18.680]   So we still don't know if there really was somebody in a jetpack flying 3,000 feet in
[00:30:18.680 --> 00:30:22.520]   the flight path at LAX.
[00:30:22.520 --> 00:30:31.040]   Seems like a bad idea, but more than two pilots reported visual sightings of it.
[00:30:31.040 --> 00:30:34.160]   Maybe it was just a drone with a costume.
[00:30:34.160 --> 00:30:36.120]   A drone with a dummy.
[00:30:36.120 --> 00:30:37.120]   A dummy.
[00:30:37.120 --> 00:30:38.120]   Maybe.
[00:30:38.120 --> 00:30:41.440]   I mean, honestly, I'm starting to think it was that because it would be very foolhardy
[00:30:41.440 --> 00:30:42.440]   to fly.
[00:30:42.440 --> 00:30:46.520]   First of all, we don't know of any jetpacks that can go 3000 feet and stay in the air
[00:30:46.520 --> 00:30:48.400]   for any length of time, right?
[00:30:48.400 --> 00:30:53.520]   Yeah, you have to just pop up and pop right back down like a bottle rocket.
[00:30:53.520 --> 00:31:00.240]   But they can get to 3000 feet, but only round trip very fast.
[00:31:00.240 --> 00:31:04.600]   So, well, maybe you never know.
[00:31:04.600 --> 00:31:08.280]   But a number of pilots reported it.
[00:31:08.280 --> 00:31:13.280]   People who know pilots say it's pretty likely that if they say we saw a guy passing us in
[00:31:13.280 --> 00:31:19.240]   a jetpack at 3000 feet on the flight path, that they did see that.
[00:31:19.240 --> 00:31:21.400]   And one of them said only in LA.
[00:31:21.400 --> 00:31:22.400]   Only.
[00:31:22.400 --> 00:31:23.400]   Yeah.
[00:31:23.400 --> 00:31:26.720]   One of the air traffic control you said, or maybe it was a pilot only in LA.
[00:31:26.720 --> 00:31:27.720]   I think it was a pilot.
[00:31:27.720 --> 00:31:31.200]   Yeah.
[00:31:31.200 --> 00:31:34.080]   Maybe it was David Blaine, you know, in a weather balloon.
[00:31:34.080 --> 00:31:37.400]   Carsten says, maybe, maybe, maybe that's what it was.
[00:31:37.400 --> 00:31:38.960]   Our show today, we got a brand new sponsor.
[00:31:38.960 --> 00:31:41.280]   I'm really pleased to welcome forward networks.
[00:31:41.280 --> 00:31:44.360]   I was on the phone with them earlier this week to get a demo.
[00:31:44.360 --> 00:31:52.800]   And I am so impressed for Stanford PhD students developed forward network seven years ago
[00:31:52.800 --> 00:31:59.680]   because they knew network operators and they saw the pain.
[00:31:59.680 --> 00:32:04.120]   There's so much mystery when you're looking at your network and your cloud, you know something's
[00:32:04.120 --> 00:32:05.120]   gone wrong.
[00:32:05.120 --> 00:32:09.040]   You know you're not getting data from this point to this point, but you're looking at
[00:32:09.040 --> 00:32:12.440]   this puffy little cloud and you don't know what the hell's going on in there.
[00:32:12.440 --> 00:32:16.960]   So they, these guys were math experts, software experts, they applied principles of modern
[00:32:16.960 --> 00:32:20.560]   software development to the network.
[00:32:20.560 --> 00:32:25.160]   And what their software does now is it builds, you're going to love this, Lou, a mathematical
[00:32:25.160 --> 00:32:30.280]   model in software of a physical network.
[00:32:30.280 --> 00:32:35.960]   It actually delivers, and you can see it on screen, a digital twin of the network.
[00:32:35.960 --> 00:32:36.920]   This is not an map.
[00:32:36.920 --> 00:32:38.280]   This is an end map on steroids.
[00:32:38.280 --> 00:32:39.960]   This is it can go out.
[00:32:39.960 --> 00:32:41.440]   It can see everything that's on your network.
[00:32:41.440 --> 00:32:45.680]   You start with what you think your network is because everybody who's running a big network
[00:32:45.680 --> 00:32:50.520]   has a map of what they think is on their network.
[00:32:50.520 --> 00:32:53.200]   Rarely is it accurate, rarely is it up to date.
[00:32:53.200 --> 00:32:56.240]   All you have to do is ask the guys at NASA.
[00:32:56.240 --> 00:32:59.120]   Remember when the guy, when somebody brought in a Raspberry Pi and connected it to the
[00:32:59.120 --> 00:33:02.360]   network at NASA and it was a huge, it was a huge problem?
[00:33:02.360 --> 00:33:04.840]   You just don't always know.
[00:33:04.840 --> 00:33:10.520]   Now with forward networks, you know exactly what's going on.
[00:33:10.520 --> 00:33:12.840]   You can verify your networks configured correctly.
[00:33:12.840 --> 00:33:17.480]   No more mistakes with your BGP rules.
[00:33:17.480 --> 00:33:19.240]   That it's in compliance with policies.
[00:33:19.240 --> 00:33:23.000]   It's behaving like you thought it would, like you intended it would.
[00:33:23.000 --> 00:33:25.520]   You can predict, this is actually the most valuable.
[00:33:25.520 --> 00:33:29.240]   You can, you can say, look, we're thinking of doing this and you can apply it to the
[00:33:29.240 --> 00:33:35.160]   mathematical model to predict the impact of that proposed chain across every possible
[00:33:35.160 --> 00:33:37.320]   traffic path.
[00:33:37.320 --> 00:33:40.200]   Wouldn't you love to have that?
[00:33:40.200 --> 00:33:43.840]   If there's something wrong in your network, you can go see what it is.
[00:33:43.840 --> 00:33:47.120]   If you're about to do something in your network, you can go see what the result will be.
[00:33:47.120 --> 00:33:51.080]   Forward networks dashboard provides key network insights with visualizations.
[00:33:51.080 --> 00:33:55.080]   You can actually look at their consumable and exportable.
[00:33:55.080 --> 00:33:59.960]   It automatically creates an always accurate network diagram with full details about the
[00:33:59.960 --> 00:34:01.400]   topology.
[00:34:01.400 --> 00:34:08.280]   You could search network behavior, configuration, and state network wide.
[00:34:08.280 --> 00:34:12.280]   With an intuitive and powerful tool, you could perform end-to-end path analyses across the
[00:34:12.280 --> 00:34:15.880]   network on-prem and cloud doesn't matter.
[00:34:15.880 --> 00:34:20.360]   And cloud, even all the way directly down to the customer, proactively identify potential
[00:34:20.360 --> 00:34:22.960]   connectivity and security policy violations.
[00:34:22.960 --> 00:34:28.440]   You can verify your networks configured and behaving as you intended on-prem, cloud, virtual
[00:34:28.440 --> 00:34:29.440]   overlay networks.
[00:34:29.440 --> 00:34:32.800]   You could set check and customize policies for your entire network.
[00:34:32.800 --> 00:34:35.200]   One of the cool features they have, and you've got to try this out.
[00:34:35.200 --> 00:34:37.200]   By the way, it's used by people all over the world.
[00:34:37.200 --> 00:34:39.160]   Verizon uses this.
[00:34:39.160 --> 00:34:41.320]   It's a bank of America.
[00:34:41.320 --> 00:34:43.200]   Goldman Sachs is a perfect example.
[00:34:43.200 --> 00:34:44.200]   These are software guys.
[00:34:44.200 --> 00:34:45.200]   These are quants.
[00:34:45.200 --> 00:34:46.200]   These are nerds.
[00:34:46.200 --> 00:34:51.000]   They thought, "Ah, we know exactly what our network's doing," until they didn't.
[00:34:51.000 --> 00:34:53.120]   They got forward networks on there.
[00:34:53.120 --> 00:34:58.960]   They were blown away to see what's actually going on in real time.
[00:34:58.960 --> 00:34:59.960]   PayPal.
[00:34:59.960 --> 00:35:01.360]   They were having an issue with their network.
[00:35:01.360 --> 00:35:03.120]   They thought about starting over.
[00:35:03.120 --> 00:35:04.200]   This is the thing.
[00:35:04.200 --> 00:35:09.240]   I mean, people like Goldman Sachs, these guys say, "We can write.
[00:35:09.240 --> 00:35:10.240]   We can build it ourselves.
[00:35:10.240 --> 00:35:11.240]   We don't need it."
[00:35:11.240 --> 00:35:12.240]   So you build it.
[00:35:12.240 --> 00:35:13.240]   You do your own software.
[00:35:13.240 --> 00:35:14.240]   You do your own thing until it doesn't work.
[00:35:14.240 --> 00:35:16.240]   And then you go, "Oh, yeah."
[00:35:16.240 --> 00:35:17.240]   Forward networks.
[00:35:17.240 --> 00:35:18.240]   PayPal.
[00:35:18.240 --> 00:35:20.560]   Turn to forward networks instead of building their own system.
[00:35:20.560 --> 00:35:21.560]   Found out.
[00:35:21.560 --> 00:35:24.480]   All they needed was software that could find the trouble spots that saved them time and
[00:35:24.480 --> 00:35:25.920]   saved them money.
[00:35:25.920 --> 00:35:29.840]   And by the way, if you're a network operator and you now have this map, you're never going
[00:35:29.840 --> 00:35:31.440]   to want to live without it again.
[00:35:31.440 --> 00:35:38.000]   The faster resolution of network trouble tickets, 50% faster, 90% faster fixes related to audit
[00:35:38.000 --> 00:35:43.960]   processes, 33% reduction in aborted network updates due to identified errors.
[00:35:43.960 --> 00:35:48.520]   Imagine you could search your network for what's causing problems.
[00:35:48.520 --> 00:35:52.760]   They just got a series C last October.
[00:35:52.760 --> 00:35:56.560]   Goldman Sachs led it, which tells you something about how happy they were about the results.
[00:35:56.560 --> 00:36:03.560]   Andresen Horowitz.
[00:36:03.560 --> 00:36:04.480]   Mark Andresen loves this threshold, the FJ. $65 million in funding.
[00:36:04.480 --> 00:36:09.920]   This is something that you're going to say, "I wish I'd known about this sooner."
[00:36:09.920 --> 00:36:15.200]   Get network automation and verification for your intent-based network right now with forward
[00:36:15.200 --> 00:36:16.200]   networks.
[00:36:16.200 --> 00:36:17.200]   You know what you should do?
[00:36:17.200 --> 00:36:18.200]   Get a demo.
[00:36:18.200 --> 00:36:19.400]   They've got a great demo at the website.
[00:36:19.400 --> 00:36:22.160]   Forward networks.com/twit.
[00:36:22.160 --> 00:36:25.000]   Please use that URL so they know you saw it here.
[00:36:25.000 --> 00:36:27.480]   Forward networks.com/twit.
[00:36:27.480 --> 00:36:28.800]   Maybe you haven't heard about them before.
[00:36:28.800 --> 00:36:31.640]   They're starting to become better and better known.
[00:36:31.640 --> 00:36:34.280]   We're doing our best to help the world know about it.
[00:36:34.280 --> 00:36:36.000]   This is such a great tool.
[00:36:36.000 --> 00:36:40.200]   Forward networks.com/twit.
[00:36:40.200 --> 00:36:44.600]   Thank you for your support forward and thank you for supporting us by using that special
[00:36:44.600 --> 00:36:49.080]   URL.
[00:36:49.080 --> 00:36:53.920]   Facebook says we're not going to add political ads on Facebook the week of the election.
[00:36:53.920 --> 00:37:01.200]   Pinterest says no.
[00:37:01.200 --> 00:37:04.080]   I didn't even know you could buy political ads on Pinterest.
[00:37:04.080 --> 00:37:08.800]   The thing that was most interesting to me is reading about the Trump campaign.
[00:37:08.800 --> 00:37:15.200]   You may remember they stopped television advertising a couple of weeks ago.
[00:37:15.200 --> 00:37:21.120]   It turns out almost all of that money has been redirected to YouTube.
[00:37:21.120 --> 00:37:27.000]   Trump in 2016 demonstrated a mastery, I think, of Facebook.
[00:37:27.000 --> 00:37:34.000]   Some argument about that, Politico's story today saying Trump deploys YouTube as his secret
[00:37:34.000 --> 00:37:38.160]   weapon in 2020.
[00:37:38.160 --> 00:37:45.840]   I've seen people say, "Oh, you know, all Trump didn't use Facebook perfectly in 2016.
[00:37:45.840 --> 00:37:47.000]   Facebook helped him do that.
[00:37:47.000 --> 00:37:49.840]   There's some debate about whether that's true."
[00:37:49.840 --> 00:37:50.840]   We don't.
[00:37:50.840 --> 00:37:54.200]   There were Facebook employees embedded at the Trump campaign, right?
[00:37:54.200 --> 00:37:55.200]   In both campaigns.
[00:37:55.200 --> 00:37:56.760]   In both campaigns.
[00:37:56.760 --> 00:37:59.000]   If this was not a secret, you could talk to them.
[00:37:59.000 --> 00:38:03.680]   And remember this- Brad Cariscal is the guy who gets the evil genius award for how he
[00:38:03.680 --> 00:38:04.680]   used Facebook.
[00:38:04.680 --> 00:38:07.400]   Well, it's kind of, but he just got fired or demoted.
[00:38:07.400 --> 00:38:09.440]   Well, that's because he did some other...
[00:38:09.440 --> 00:38:11.040]   He didn't understand TikTok.
[00:38:11.040 --> 00:38:13.440]   He didn't understand data.
[00:38:13.440 --> 00:38:15.320]   He oversold it.
[00:38:15.320 --> 00:38:16.320]   That's exactly right.
[00:38:16.320 --> 00:38:21.240]   And TikTok is partly responsible, but he is not the evil genius that...
[00:38:21.240 --> 00:38:24.200]   Look, this is my beat and I've been covering data and politics.
[00:38:24.200 --> 00:38:25.400]   That's why I'm bringing it up.
[00:38:25.400 --> 00:38:27.040]   I got the expert here.
[00:38:27.040 --> 00:38:30.240]   Yeah, and I'll try not to go too far down the street.
[00:38:30.240 --> 00:38:32.240]   No, I'm fascinating.
[00:38:32.240 --> 00:38:34.120]   It's pretty well known.
[00:38:34.120 --> 00:38:40.120]   It's not even a secret or an open secret that Pascal certainly oversold his capabilities
[00:38:40.120 --> 00:38:41.600]   in 2016.
[00:38:41.600 --> 00:38:46.800]   And he was certainly rubbing the president the wrong way even before the campaign kicked
[00:38:46.800 --> 00:38:51.360]   off, which was officially the day he was inaugurated or the day after and unofficially
[00:38:51.360 --> 00:38:53.120]   at the beginning of this year.
[00:38:53.120 --> 00:38:58.840]   And look, he did miss TikTok and he did miss big data operations that were kind of aligned
[00:38:58.840 --> 00:39:02.960]   against him and he paid a price for that.
[00:39:02.960 --> 00:39:07.000]   That's not the only reason he was demoted is relationship with Hope Hicks is another reason.
[00:39:07.000 --> 00:39:12.480]   But data is not...we have a story coming out on CNET about this very soon, but data is
[00:39:12.480 --> 00:39:14.640]   not the be-all-end-all.
[00:39:14.640 --> 00:39:19.080]   You go to war with the data you have and if you have bad data and you're targeting wrong
[00:39:19.080 --> 00:39:21.240]   on Facebook, you're going to mess up.
[00:39:21.240 --> 00:39:29.680]   If you're targeting wrong in YouTube, you might be effective, but the wrong data will
[00:39:29.680 --> 00:39:31.600]   yield the wrong results.
[00:39:31.600 --> 00:39:38.600]   We don't...YouTube doesn't report spending so we don't know exactly...with television
[00:39:38.600 --> 00:39:41.600]   advertising, you know, but not with YouTube.
[00:39:41.600 --> 00:39:46.840]   But according to Politico, the Trump re-election campaign is pouring money and staff time into
[00:39:46.840 --> 00:39:49.240]   Google's video platform.
[00:39:49.240 --> 00:39:54.680]   $65 million on YouTube and Google, $30 million since July.
[00:39:54.680 --> 00:40:03.000]   The Biden campaign has spent only $33 million on YouTube and Google the entire campaign.
[00:40:03.000 --> 00:40:04.560]   It's really interesting.
[00:40:04.560 --> 00:40:09.000]   So first of all, let me ask you, is my premise wrong?
[00:40:09.000 --> 00:40:15.480]   In 2016, how important Dan was Facebook to the election?
[00:40:15.480 --> 00:40:18.760]   Boy, I don't know.
[00:40:18.760 --> 00:40:19.760]   We don't know.
[00:40:19.760 --> 00:40:20.760]   We just can't tell.
[00:40:20.760 --> 00:40:30.080]   I have a really good editor at the network who always...I mean, he drives me nuts because
[00:40:30.080 --> 00:40:34.040]   he does what an editor is supposed to do, which is when I come with tech and I come
[00:40:34.040 --> 00:40:36.680]   with my own data and I say, "Look, Lexus proves it."
[00:40:36.680 --> 00:40:39.080]   He says, "That doesn't prove anything."
[00:40:39.080 --> 00:40:43.680]   You can't prove that Facebook was able to, air quotes, activate, which is the language
[00:40:43.680 --> 00:40:44.680]   they used.
[00:40:44.680 --> 00:40:51.760]   They want us to believe that they can send signals to their audience and air quotes,
[00:40:51.760 --> 00:40:53.480]   activate them at the right moment.
[00:40:53.480 --> 00:40:55.000]   There just is no proof of that.
[00:40:55.000 --> 00:41:01.600]   It's generally agreed, I think, that the Trump base, if you can get them to turn out the
[00:41:01.600 --> 00:41:04.800]   vote, is very, very powerful.
[00:41:04.800 --> 00:41:06.760]   That's what you mean by activating, right?
[00:41:06.760 --> 00:41:10.280]   It's getting those base voters to go to the polls.
[00:41:10.280 --> 00:41:11.720]   It's called GOTV.
[00:41:11.720 --> 00:41:13.960]   They can GOTV get out the vote.
[00:41:13.960 --> 00:41:17.880]   They say when they want to.
[00:41:17.880 --> 00:41:23.160]   And traditional GOTV, it's like the campaign Biden has been running, which is you have
[00:41:23.160 --> 00:41:28.040]   to keep your base motivated and make sure that they do get out to the polls, make sure
[00:41:28.040 --> 00:41:32.520]   they have all the tools that they need to get to the polls.
[00:41:32.520 --> 00:41:37.320]   But what the Trump campaign has been selling, at least until now, is that we can do this
[00:41:37.320 --> 00:41:38.880]   on a dime.
[00:41:38.880 --> 00:41:41.360]   We can activate people when we want them to.
[00:41:41.360 --> 00:41:46.920]   They've been selling this capability, not just to politics, but to all sorts of third
[00:41:46.920 --> 00:41:53.040]   parties who would like very much, including advertisers, would like that capability.
[00:41:53.040 --> 00:41:58.920]   Trump's campaign, according to Politico, spent $10 million total on YouTube in 2016.
[00:41:58.920 --> 00:42:03.080]   They've already spent much three times that amount.
[00:42:03.080 --> 00:42:09.960]   Trump officials say YouTube has been more effective at times in Facebook at not just a
[00:42:09.960 --> 00:42:13.800]   mobilization, getting out the vote, but also fundraising.
[00:42:13.800 --> 00:42:20.880]   And then persuasion in 2020, YouTube has become an increasingly influential force.
[00:42:20.880 --> 00:42:22.960]   And they're better able to use it.
[00:42:22.960 --> 00:42:29.800]   One of the things they're doing is, of course, they have a Trump channel campaign channel,
[00:42:29.800 --> 00:42:36.680]   but they also have ad buys, negative ads paired with livestream series like, quote, "Black
[00:42:36.680 --> 00:42:42.000]   voices for Trump," real talk online, exclamation mark, end quote, end quote, the right view,
[00:42:42.000 --> 00:42:43.000]   end quote.
[00:42:43.000 --> 00:42:47.640]   The campaign uploads, I'm reading from Politico, and then tests hundreds of short videos of
[00:42:47.640 --> 00:42:52.360]   the president speaking while also posting news clips about things like the jobs report
[00:42:52.360 --> 00:42:56.280]   and the recent Serbia Kosovo deal.
[00:42:56.280 --> 00:43:04.480]   I think it's the case four years later that more and more of us watch YouTube and get,
[00:43:04.480 --> 00:43:09.360]   you know, it was the case in 2016, people got their news from Facebook.
[00:43:09.360 --> 00:43:13.800]   I think YouTube is starting to become more and more important in that regard.
[00:43:13.800 --> 00:43:14.800]   Yes.
[00:43:14.800 --> 00:43:15.800]   Yeah.
[00:43:15.800 --> 00:43:16.800]   Yeah.
[00:43:16.800 --> 00:43:17.800]   You think about it.
[00:43:17.800 --> 00:43:18.800]   I mean, think about it just from a platform perspective.
[00:43:18.800 --> 00:43:24.720]   I mean, platforms like Pinterest, they have like only 350 million monthly active users
[00:43:24.720 --> 00:43:27.040]   and their demographic is very focused.
[00:43:27.040 --> 00:43:30.800]   But then you look at Facebook, it's very similar is you might have in the billions of
[00:43:30.800 --> 00:43:36.080]   two and a half billion plus users a month, but it's also focused on a specific demographic.
[00:43:36.080 --> 00:43:38.440]   But YouTube doesn't everybody use Facebook?
[00:43:38.440 --> 00:43:42.680]   Well, I mean, the younger generation starting to step away from it.
[00:43:42.680 --> 00:43:43.680]   Yeah.
[00:43:43.680 --> 00:43:44.680]   Yeah.
[00:43:44.680 --> 00:43:47.800]   So I think, but I think, but YouTube is where the demographic is across the spectrum.
[00:43:47.800 --> 00:43:53.480]   And so you have 2.5 or plus billion users a month on YouTube watching all different
[00:43:53.480 --> 00:43:54.480]   types of things.
[00:43:54.480 --> 00:43:58.760]   And sometimes you get ads from things that you're not even interested in or that are
[00:43:58.760 --> 00:44:01.320]   being, you know, that are upselled to you.
[00:44:01.320 --> 00:44:05.160]   And you might be a younger generation like kids get Minecraft videos and they'll get
[00:44:05.160 --> 00:44:06.160]   Trump ads.
[00:44:06.160 --> 00:44:09.600]   So I mean, like, you know, I mean, it's one of those things that I think they chose the
[00:44:09.600 --> 00:44:10.800]   right platform here.
[00:44:10.800 --> 00:44:11.800]   Wow.
[00:44:11.800 --> 00:44:19.040]   It's funny because I don't see YouTube videos because I have a YouTube premium account.
[00:44:19.040 --> 00:44:20.040]   Doesn't everybody?
[00:44:20.040 --> 00:44:21.520]   I guess not.
[00:44:21.520 --> 00:44:24.080]   I don't see ads on YouTube.
[00:44:24.080 --> 00:44:28.600]   So I'm not aware of it.
[00:44:28.600 --> 00:44:29.600]   That's a problem, isn't it?
[00:44:29.600 --> 00:44:34.560]   I mean, you can't buy your way out of ads on Facebook.
[00:44:34.560 --> 00:44:35.560]   You can on YouTube.
[00:44:35.560 --> 00:44:40.480]   That would be an interesting product for Facebook to launch.
[00:44:40.480 --> 00:44:45.040]   I wonder if there are average, I'd buy it too, just to see how it operated.
[00:44:45.040 --> 00:44:49.040]   And I wonder what the monthly revenue per Facebook user is.
[00:44:49.040 --> 00:44:51.840]   Like, there's got to be a break even point.
[00:44:51.840 --> 00:44:55.480]   Although maybe the value of their higher demographic people who would be more likely
[00:44:55.480 --> 00:44:59.080]   to afford such a service is high enough that it would price them out.
[00:44:59.080 --> 00:45:00.080]   I mean, who knows?
[00:45:00.080 --> 00:45:02.320]   But that's an intriguing notion.
[00:45:02.320 --> 00:45:09.440]   Each user according to arknea.com, each user of Facebook is worth $150.
[00:45:09.440 --> 00:45:11.280]   Well based on their market cap is worth that.
[00:45:11.280 --> 00:45:12.280]   Market cap, yeah.
[00:45:12.280 --> 00:45:13.360]   But revenue is a question.
[00:45:13.360 --> 00:45:14.360]   Or revenue.
[00:45:14.360 --> 00:45:18.800]   I've read somewhere.
[00:45:18.800 --> 00:45:24.720]   It's, let's see, somebody's saying $6 per quarter, $24 a year.
[00:45:24.720 --> 00:45:26.400]   I read somewhere $41 a year.
[00:45:26.400 --> 00:45:28.360]   It's probably in that ballpark.
[00:45:28.360 --> 00:45:31.680]   Not a huge amount, but multiply that times two and a half billion.
[00:45:31.680 --> 00:45:32.680]   It starts that up.
[00:45:32.680 --> 00:45:33.680]   It's starting to work.
[00:45:33.680 --> 00:45:34.680]   The real talent money.
[00:45:34.680 --> 00:45:36.760]   It's facing it.
[00:45:36.760 --> 00:45:40.080]   Aren't necessarily the ads or the ads targeting.
[00:45:40.080 --> 00:45:41.080]   It's trust.
[00:45:41.080 --> 00:45:45.160]   And can you trust that the platform isn't using your, I mean, even if I were to buy my way
[00:45:45.160 --> 00:45:51.440]   out of ads on Facebook, can you trust that they're not using my data in ways that I
[00:45:51.440 --> 00:45:52.440]   might find objectionable?
[00:45:52.440 --> 00:45:58.480]   Well, I don't even have a Facebook Instagram or a, or what's app account?
[00:45:58.480 --> 00:46:01.040]   But I know that there's still gathering information about me.
[00:46:01.040 --> 00:46:06.800]   Actually, I use a Firefox browser, which geote, which fences off, actually have a really kind
[00:46:06.800 --> 00:46:09.240]   of nice privacy fence.
[00:46:09.240 --> 00:46:12.080]   Fences off, they call it the Facebook container.
[00:46:12.080 --> 00:46:15.760]   And when I go to a page, I'll see it light up saying, yeah, Facebook has a tracker on
[00:46:15.760 --> 00:46:17.960]   this page, which we've blocked.
[00:46:17.960 --> 00:46:18.960]   Wow.
[00:46:18.960 --> 00:46:19.960]   You've been part of the baby.
[00:46:19.960 --> 00:46:20.960]   You've been part of the graph.
[00:46:20.960 --> 00:46:26.240]   You're in like you have connections in the graph that talk about you, that link to you,
[00:46:26.240 --> 00:46:27.840]   that have previously known you.
[00:46:27.840 --> 00:46:29.920]   Once they already have that, they already have your information.
[00:46:29.920 --> 00:46:30.920]   That's right.
[00:46:30.920 --> 00:46:31.920]   It's too late.
[00:46:31.920 --> 00:46:32.920]   Because my wife's on Facebook.
[00:46:32.920 --> 00:46:37.320]   So whatever, you know, they know, Lisa's on Facebook so we can figure everything we
[00:46:37.320 --> 00:46:38.400]   know about hers.
[00:46:38.400 --> 00:46:39.960]   Sort of true about Leo.
[00:46:39.960 --> 00:46:43.840]   Yeah, you can't really get away from it.
[00:46:43.840 --> 00:46:45.400]   I'm actually kind of curious.
[00:46:45.400 --> 00:46:46.680]   And I've been thinking a lot about this.
[00:46:46.680 --> 00:46:56.200]   We had some texts come out over the past few days to Ethernet wire all of our house because
[00:46:56.200 --> 00:46:57.760]   Lisa said, I hate Wi-Fi.
[00:46:57.760 --> 00:46:59.880]   You have to stop this.
[00:46:59.880 --> 00:47:00.880]   She's got to work at home.
[00:47:00.880 --> 00:47:02.480]   She's zooming all day.
[00:47:02.480 --> 00:47:08.240]   And so we got, she's got nice now, nice little wall thing in her desk with four Ethernet ports
[00:47:08.240 --> 00:47:09.240]   on it.
[00:47:09.240 --> 00:47:10.840]   We're going to do the whole thing.
[00:47:10.840 --> 00:47:16.640]   And in the process, the guy said, you know, you really should upgrade your Comcast Internet.
[00:47:16.640 --> 00:47:17.640]   It'll go up to a gig.
[00:47:17.640 --> 00:47:18.640]   I said, really?
[00:47:18.640 --> 00:47:19.640]   Okay.
[00:47:19.640 --> 00:47:20.640]   What about bandwidth caps?
[00:47:20.640 --> 00:47:21.640]   I said, you can buy your way out of that too.
[00:47:21.640 --> 00:47:22.640]   I said, oh, I'll do it.
[00:47:22.640 --> 00:47:25.640]   I called him up and I was going to cancel TV.
[00:47:25.640 --> 00:47:30.680]   I said, well, if I have a gigabit and I have ports everywhere, what would I need TV for?
[00:47:30.680 --> 00:47:32.680]   But they gave me such a good deal.
[00:47:32.680 --> 00:47:34.680]   I'm not such an idiot.
[00:47:34.680 --> 00:47:35.680]   You're one of those guys.
[00:47:35.680 --> 00:47:36.680]   I don't know.
[00:47:36.680 --> 00:47:37.680]   I think he gave me such a good deal.
[00:47:37.680 --> 00:47:40.480]   Look, we could take your bill.
[00:47:40.480 --> 00:47:44.720]   We could cut $50 off of it, give you a gigabit with no bandwidth cap.
[00:47:44.720 --> 00:47:50.440]   And every cable channel and phone service for $50 less.
[00:47:50.440 --> 00:47:53.040]   And then he said, yeah.
[00:47:53.040 --> 00:47:58.440]   And then he said under his breath for one year.
[00:47:58.440 --> 00:48:01.440]   And then it goes up to a thousand.
[00:48:01.440 --> 00:48:03.640]   It goes up a lot.
[00:48:03.640 --> 00:48:09.120]   So what I didn't realize is if you say, well, I don't use the phone.
[00:48:09.120 --> 00:48:10.360]   I don't need phone service.
[00:48:10.360 --> 00:48:11.360]   Okay.
[00:48:11.360 --> 00:48:13.240]   But that's going to raise the cost $50.
[00:48:13.240 --> 00:48:16.360]   It costs you more not to have a triple play bundle.
[00:48:16.360 --> 00:48:19.120]   These guys are so sneaky.
[00:48:19.120 --> 00:48:22.240]   But more and more, I'm thinking, okay, it's now I can't do it now for a year.
[00:48:22.240 --> 00:48:27.320]   But in September 2021, I think I'm just going to have internet and go over the top.
[00:48:27.320 --> 00:48:33.240]   And I really wonder how much longer cable companies will be able to operate as pay TV
[00:48:33.240 --> 00:48:34.740]   companies.
[00:48:34.740 --> 00:48:36.600]   How much longer?
[00:48:36.600 --> 00:48:40.440]   It can't be more than a few years before everything's over the top, right?
[00:48:40.440 --> 00:48:42.240]   Oh, OTT is a future.
[00:48:42.240 --> 00:48:43.240]   Yeah.
[00:48:43.240 --> 00:48:44.760]   I bet you're a young guy.
[00:48:44.760 --> 00:48:46.880]   I bet you don't have a cable subscription.
[00:48:46.880 --> 00:48:49.920]   No, but I have Fios, which is a gig up and down.
[00:48:49.920 --> 00:48:54.480]   And I just bought the Verizon Wi-Fi six router, which is tremendous.
[00:48:54.480 --> 00:48:58.720]   I put all of my IoT devices on kind of a side network.
[00:48:58.720 --> 00:49:02.640]   So it doesn't limit or impede my primary network.
[00:49:02.640 --> 00:49:05.000]   And I mean, I'm quite a distance.
[00:49:05.000 --> 00:49:10.760]   I mean, I'm a long way away from my router and it's not perfect, but it's not terrible.
[00:49:10.760 --> 00:49:17.400]   So and Netflix, YouTube, Peacock, HBO Max, you don't need to tell.
[00:49:17.400 --> 00:49:22.560]   And if you want locals, I guess you get YouTube TV or sling TV, you don't really need a cable
[00:49:22.560 --> 00:49:24.560]   subscription.
[00:49:24.560 --> 00:49:29.280]   And who the hell has all phones?
[00:49:29.280 --> 00:49:30.280]   I've had this.
[00:49:30.280 --> 00:49:31.280]   I didn't even know it.
[00:49:31.280 --> 00:49:33.000]   I looked and said, Oh, I have phone service from XFINITE.
[00:49:33.000 --> 00:49:34.600]   I've had that for years.
[00:49:34.600 --> 00:49:38.000]   I've never hooked up a phone.
[00:49:38.000 --> 00:49:43.280]   So it must be people my age and older that are keeping cable television alive, right?
[00:49:43.280 --> 00:49:44.280]   Absolutely.
[00:49:44.280 --> 00:49:45.720]   It'll be a demographic shift.
[00:49:45.720 --> 00:49:48.360]   It'll be like readers digest in the 90s or something.
[00:49:48.360 --> 00:49:49.360]   I remember the scientists.
[00:49:49.360 --> 00:49:50.360]   It's a readers digest.
[00:49:50.360 --> 00:49:51.360]   It is.
[00:49:51.360 --> 00:49:54.600]   Yeah, they had they had an issue in the 90s where they realized that their demographic
[00:49:54.600 --> 00:50:00.640]   was slowly leaving the stage and there was really just sort of a timer on the business.
[00:50:00.640 --> 00:50:03.120]   And cable will probably end up being kind of like that.
[00:50:03.120 --> 00:50:04.120]   Well, they're not stupid.
[00:50:04.120 --> 00:50:05.120]   They know that.
[00:50:05.120 --> 00:50:07.040]   That's why my internet access is going up and up and up.
[00:50:07.040 --> 00:50:12.240]   And it's cheaper, so much cheaper if I get the triple play for the first one that you're
[00:50:12.240 --> 00:50:13.240]   going to get it.
[00:50:13.240 --> 00:50:18.600]   So I kept TV and telephone even though I don't need it because I have over the top everything.
[00:50:18.600 --> 00:50:20.680]   Everything we do at CBS News is OTT.
[00:50:20.680 --> 00:50:22.720]   We do not have a cable, at least the network.
[00:50:22.720 --> 00:50:26.200]   We do not actually CBS has an excellent app.
[00:50:26.200 --> 00:50:28.040]   We are CBS News.com/live.
[00:50:28.040 --> 00:50:29.160]   I'm not logrolling.
[00:50:29.160 --> 00:50:33.560]   If you want OTT and you want good news, yeah.
[00:50:33.560 --> 00:50:38.160]   We are 100% in with OTT because it works for our audience.
[00:50:38.160 --> 00:50:40.520]   It works for our news gathering for the network.
[00:50:40.520 --> 00:50:45.280]   It's the way we break news without being on cable.
[00:50:45.280 --> 00:50:49.920]   Cousin of Yann, our chatroom says cable TV is the rotary phone of the 21st century.
[00:50:49.920 --> 00:50:52.440]   The phone in general.
[00:50:52.440 --> 00:50:54.480]   Yeah, I don't even need a phone.
[00:50:54.480 --> 00:50:57.840]   Who needs a phone?
[00:50:57.840 --> 00:50:59.200]   So this is a challenge though.
[00:50:59.200 --> 00:51:01.720]   Get back to original conversation for political campaigns.
[00:51:01.720 --> 00:51:03.920]   I mean, it's a challenge for pollsters, right?
[00:51:03.920 --> 00:51:07.200]   Pollsters don't include cell phones for the most part, which means they're missing like
[00:51:07.200 --> 00:51:09.200]   everybody in the country.
[00:51:09.200 --> 00:51:10.200]   For 80% of the country.
[00:51:10.200 --> 00:51:11.640]   They use all kinds of different data.
[00:51:11.640 --> 00:51:15.960]   I mean, you can talk to one of the most well-known and widely used polling companies.
[00:51:15.960 --> 00:51:17.800]   It's called Morning Consult.
[00:51:17.800 --> 00:51:19.840]   And they do bespoke polling for--
[00:51:19.840 --> 00:51:23.160]   So they'll do cell phones now.
[00:51:23.160 --> 00:51:24.800]   Well, they won't tell you what they do.
[00:51:24.800 --> 00:51:30.560]   I bet they don't because who answers an unknown number on their cell phone anymore?
[00:51:30.560 --> 00:51:31.560]   Never.
[00:51:31.560 --> 00:51:32.560]   Data.
[00:51:32.560 --> 00:51:33.560]   Data.
[00:51:33.560 --> 00:51:34.560]   Oh, I'm sorry.
[00:51:34.560 --> 00:51:38.000]   By the way, you missed the air quotes if you're listening.
[00:51:38.000 --> 00:51:40.720]   What is air quote data?
[00:51:40.720 --> 00:51:42.200]   They won't tell us.
[00:51:42.200 --> 00:51:44.920]   They won't talk about their tactics.
[00:51:44.920 --> 00:51:45.920]   Secret sauce.
[00:51:45.920 --> 00:51:47.160]   Well, we'll find out.
[00:51:47.160 --> 00:51:50.040]   I think pollsters didn't do such a good job in 2016.
[00:51:50.040 --> 00:51:52.920]   We'll find out how well they do in 2020.
[00:51:52.920 --> 00:51:59.120]   At some point, it's going to be a problem because at least our audience and most people
[00:51:59.120 --> 00:52:03.000]   I think under 25 are aware of these privacy issues.
[00:52:03.000 --> 00:52:05.360]   They don't have home phones.
[00:52:05.360 --> 00:52:09.360]   They don't watch cable TV.
[00:52:09.360 --> 00:52:14.600]   I feel like the world is changing rapidly and campaigns are going to have to struggle
[00:52:14.600 --> 00:52:18.880]   to advertise to and get to especially younger voters.
[00:52:18.880 --> 00:52:21.840]   And polling, forget it.
[00:52:21.840 --> 00:52:22.840]   Right?
[00:52:22.840 --> 00:52:23.840]   Yeah.
[00:52:23.840 --> 00:52:26.240]   You have different data streams.
[00:52:26.240 --> 00:52:31.200]   I mean, like they were saying, who knows if credit card companies are selling their data
[00:52:31.200 --> 00:52:34.680]   about demographics to pollsters or what you're buying?
[00:52:34.680 --> 00:52:36.120]   How would they help you know?
[00:52:36.120 --> 00:52:37.120]   And they are.
[00:52:37.120 --> 00:52:38.120]   Of course they are.
[00:52:38.120 --> 00:52:39.120]   Well, okay.
[00:52:39.120 --> 00:52:45.120]   So the first thing, first rule is assume anybody who can collect data is selling it because
[00:52:45.120 --> 00:52:47.720]   there's no limitation on that.
[00:52:47.720 --> 00:52:51.120]   So everybody who's got data about you is selling it.
[00:52:51.120 --> 00:52:54.040]   But how can that help people know who I'm going to vote for?
[00:52:54.040 --> 00:52:57.720]   Is it so obvious for my credit card purchases?
[00:52:57.720 --> 00:52:58.720]   It could be.
[00:52:58.720 --> 00:52:59.720]   It could be.
[00:52:59.720 --> 00:53:01.520]   I mean, it would have depended on that.
[00:53:01.520 --> 00:53:04.920]   In question by a maggot just to screw with them.
[00:53:04.920 --> 00:53:11.560]   Well, if you want a really good example and I mean, this is their pure tech company.
[00:53:11.560 --> 00:53:16.600]   There's a company called L2 Political that I mean, they're right out in the open.
[00:53:16.600 --> 00:53:21.360]   Do a Google image search for L2 Political and you can see how precise their maps give
[00:53:21.360 --> 00:53:22.360]   you.
[00:53:22.360 --> 00:53:28.440]   Everybody uses L2 and you would use this for again, it's called G.O.TV.
[00:53:28.440 --> 00:53:33.560]   But if you want to send you know, the people who knock on your doors or do call you, I
[00:53:33.560 --> 00:53:37.200]   have a reporter access to L2.
[00:53:37.200 --> 00:53:39.720]   You can log in and see you.
[00:53:39.720 --> 00:53:41.920]   I don't mean like a representation of you.
[00:53:41.920 --> 00:53:43.200]   I can look at you.
[00:53:43.200 --> 00:53:44.600]   I can use their maps.
[00:53:44.600 --> 00:53:50.760]   It's a big map and I can zoom right into your block, your neighborhood and I can see precisely
[00:53:50.760 --> 00:53:53.400]   how or what you care about.
[00:53:53.400 --> 00:53:58.760]   They use data from a source called haystack, which came from the Obama campaigns.
[00:53:58.760 --> 00:54:00.080]   But again, it's not partisan.
[00:54:00.080 --> 00:54:01.360]   It's just a data source.
[00:54:01.360 --> 00:54:04.920]   On the right hand side of that image there, you can see how you're able to filter down
[00:54:04.920 --> 00:54:05.920]   and see.
[00:54:05.920 --> 00:54:09.360]   Democratic nonpartisan Republican, American-dependent, libertarian, other green, peace
[00:54:09.360 --> 00:54:14.360]   and freedom, unknown, reform, natural law, voter has changed parties, changed party
[00:54:14.360 --> 00:54:20.240]   from R to NP, from NP to R, from D to NP, from NP to D, female male, unknown marital status,
[00:54:20.240 --> 00:54:24.520]   military veteran, voters name suffix, voting history, registration.
[00:54:24.520 --> 00:54:29.640]   It goes into controversial issues like your stance on medical marijuana or legal weed.
[00:54:29.640 --> 00:54:32.760]   It goes into how you likely feel about abortion.
[00:54:32.760 --> 00:54:34.800]   Again, this is not a secret.
[00:54:34.800 --> 00:54:36.840]   I mean, you can call L2 and talk to them.
[00:54:36.840 --> 00:54:38.200]   They'll tell you what they do.
[00:54:38.200 --> 00:54:42.480]   On that map that you pulled up, you can zoom right in and see the voter.
[00:54:42.480 --> 00:54:47.880]   You click on the voter's name, you expand their role and you see how much you probably make
[00:54:47.880 --> 00:54:49.680]   where you live, your address.
[00:54:49.680 --> 00:54:51.640]   I mean, it's a perfect doxing tool.
[00:54:51.640 --> 00:54:55.240]   You can scroll down and see how many elections you voted in.
[00:54:55.240 --> 00:54:57.040]   Where did you register the vote?
[00:54:57.040 --> 00:54:58.040]   Oh, I'm not your individual.
[00:54:58.040 --> 00:54:59.040]   All of that's public.
[00:54:59.040 --> 00:55:00.040]   On the individual.
[00:55:00.040 --> 00:55:01.040]   Yeah, but all of that's public, right?
[00:55:01.040 --> 00:55:02.040]   It's not like it's a purchase.
[00:55:02.040 --> 00:55:07.680]   It's a, well, your voter file is public information or it is easily accessed through your secretary
[00:55:07.680 --> 00:55:09.120]   of state to office.
[00:55:09.120 --> 00:55:15.280]   Each state is kind of different, but it's, it is, look, one way people get doxed is because
[00:55:15.280 --> 00:55:19.440]   your voter, the information on your voter file is public information.
[00:55:19.440 --> 00:55:26.640]   So if you can access somebody's voter file, then you have a really terrifying amount of
[00:55:26.640 --> 00:55:28.640]   information about that person.
[00:55:28.640 --> 00:55:33.360]   And then I'm not trying to go into the secretary of state and then there would be some way
[00:55:33.360 --> 00:55:35.120]   of requesting that information.
[00:55:35.120 --> 00:55:36.880]   Yeah, in almost every state.
[00:55:36.880 --> 00:55:41.640]   In fact, I'm not trying to take this into politics, but just to show the capabilities
[00:55:41.640 --> 00:55:44.240]   of the tools that are available right now.
[00:55:44.240 --> 00:55:49.760]   So you would take L2 and you would give it to like your local grassroots organizer or
[00:55:49.760 --> 00:55:53.160]   whatever and they would then carve up territory.
[00:55:53.160 --> 00:55:58.880]   So I used to live in South Dakota and they would say, well, there's only 7,000 people
[00:55:58.880 --> 00:56:01.200]   who live in the town of Sturgis.
[00:56:01.200 --> 00:56:02.200]   But you know what?
[00:56:02.200 --> 00:56:04.880]   I think that there are some persuadable voters there.
[00:56:04.880 --> 00:56:08.640]   So I'm going to find those people who might be okay with legal weed.
[00:56:08.640 --> 00:56:13.520]   They might be okay with abortion and I'm going to maybe try to convince them to vote Democratic
[00:56:13.520 --> 00:56:14.520]   this year.
[00:56:14.520 --> 00:56:17.400]   Now you wouldn't do that in South Dakota because South Dakota is always going to go
[00:56:17.400 --> 00:56:25.280]   red, but you would do that in say Texas, North Carolina, Georgia, some of these swing states.
[00:56:25.280 --> 00:56:31.720]   These are when they say battlegrounds, the good data shows that you can do battle on
[00:56:31.720 --> 00:56:35.120]   the ground over voters' hearts and minds.
[00:56:35.120 --> 00:56:36.160]   Wow.
[00:56:36.160 --> 00:56:41.080]   So this is particularly used for politics, but it's the same information marketers use
[00:56:41.080 --> 00:56:42.080]   and have access to.
[00:56:42.080 --> 00:56:43.080]   Exactly.
[00:56:43.080 --> 00:56:49.320]   And I think that's about with Brad Parscale and what he was maybe doing and is doing, Parscale.com
[00:56:49.320 --> 00:56:54.080]   will show you exactly how he's trying to leverage his capabilities.
[00:56:54.080 --> 00:56:57.600]   If I can get a political campaign to do this, I can certainly sell the same capabilities
[00:56:57.600 --> 00:56:59.160]   to an advertiser.
[00:56:59.160 --> 00:57:00.160]   Yeah.
[00:57:00.160 --> 00:57:01.160]   Wow.
[00:57:01.160 --> 00:57:07.200]   And of course, speaking from my perspective, it's one of the reasons advertisers are less
[00:57:07.200 --> 00:57:14.560]   and less likely to buy ads on podcasts because we can't give them any of that information.
[00:57:14.560 --> 00:57:20.400]   And it's so much more efficient for them just to buy a Facebook or Google or Spotify, which
[00:57:20.400 --> 00:57:21.960]   can give them that information.
[00:57:21.960 --> 00:57:26.280]   This is one of the reasons Spotify is a half billion dollar war chest to buy podcasts and
[00:57:26.280 --> 00:57:27.680]   take them exclusive.
[00:57:27.680 --> 00:57:33.200]   They reportedly gave Joe Rogan a hundred million dollars to make his podcast, not a YouTube
[00:57:33.200 --> 00:57:37.480]   show, not an RSS feed, but a Spotify exclusive.
[00:57:37.480 --> 00:57:41.240]   And you can see why there's a hundred million dollars in it because everybody who listens
[00:57:41.240 --> 00:57:45.560]   to Joe Rogan, all the tens of millions of people who listen to Joe Rogan every month,
[00:57:45.560 --> 00:57:46.560]   will have to do it through Spotify.
[00:57:46.560 --> 00:57:49.920]   And of course, Spotify is an app that it will know geo location.
[00:57:49.920 --> 00:57:52.520]   It will know all sorts of demographic information.
[00:57:52.520 --> 00:57:57.800]   So let me let's take a break and let's see how this applies to TikTok because we thought
[00:57:57.800 --> 00:57:59.960]   somebody would buy TikTok this week.
[00:57:59.960 --> 00:58:00.960]   Didn't happen.
[00:58:00.960 --> 00:58:05.720]   I think I know maybe why it didn't happen, but we could talk more about that.
[00:58:05.720 --> 00:58:06.720]   This is a good panel.
[00:58:06.720 --> 00:58:07.720]   Good to have you here.
[00:58:07.720 --> 00:58:10.600]   Dan Patterson, this is his beat.
[00:58:10.600 --> 00:58:14.760]   Big data and how it's used in a variety of ways.
[00:58:14.760 --> 00:58:17.640]   He's a senior producer at CNET and CBS News.
[00:58:17.640 --> 00:58:18.640]   Great to have you.
[00:58:18.640 --> 00:58:25.920]   Dan, from our "Twiet Show This Week in Enterprise Tech," the lovely and talented, Lou Maresca,
[00:58:25.920 --> 00:58:27.080]   always a pleasure to have you on.
[00:58:27.080 --> 00:58:30.640]   You have an excellent choice in T-shirt, Lou.
[00:58:30.640 --> 00:58:35.280]   It looks like somebody really took a logo against your chest and then splattered you
[00:58:35.280 --> 00:58:36.280]   with paint.
[00:58:36.280 --> 00:58:37.560]   Yeah, I really love this one.
[00:58:37.560 --> 00:58:38.560]   Yeah, I know.
[00:58:38.560 --> 00:58:39.480]   That's one of my favorites.
[00:58:39.480 --> 00:58:41.440]   I think Anthony Nielsen designed that.
[00:58:41.440 --> 00:58:42.440]   I don't know.
[00:58:42.440 --> 00:58:44.520]   Do you know offhand, John, who designed that?
[00:58:44.520 --> 00:58:47.680]   Anthony is our in-house graphics guru.
[00:58:47.680 --> 00:58:51.120]   So I'm going to guess it's Anthony, but if it's somebody else, let me know so I can
[00:58:51.120 --> 00:58:52.120]   give him credit.
[00:58:52.120 --> 00:58:55.160]   A long time friend, I haven't seen him in ages.
[00:58:55.160 --> 00:58:58.560]   Rob Reed, his book "After On" was amazing.
[00:58:58.560 --> 00:59:01.120]   Really, two great books in a row.
[00:59:01.120 --> 00:59:05.920]   Are you going to stop writing fiction or is there a novel in your future too?
[00:59:05.920 --> 00:59:08.320]   I think there's a novel in my near future.
[00:59:08.320 --> 00:59:09.320]   Good.
[00:59:09.320 --> 00:59:12.520]   Yeah, I think there's another season of the podcast coming up, but I think there's a novel
[00:59:12.520 --> 00:59:13.520]   coming up.
[00:59:13.520 --> 00:59:16.840]   Just right now, I'm in the throes of this Audible original.
[00:59:16.840 --> 00:59:21.360]   And when the decks are clear to that, I'm going to figure out exactly what the sequence
[00:59:21.360 --> 00:59:22.960]   of the next couple of things are going to be.
[00:59:22.960 --> 00:59:26.960]   Yeah, the "After On" podcast was great, but you're on hiatus during COVID, right?
[00:59:26.960 --> 00:59:27.960]   That's what's going to--
[00:59:27.960 --> 00:59:28.960]   I could bring that thing back.
[00:59:28.960 --> 00:59:33.160]   I'm thinking about bringing it back in conjunction with the release of the audiobook because
[00:59:33.160 --> 00:59:38.400]   a lot of the scientists, I've interviewed about 20 scientists for the Audible book,
[00:59:38.400 --> 00:59:43.560]   and a lot of those relationships that I've developed through researching that book would
[00:59:43.560 --> 00:59:45.800]   be great interviewees for the podcast.
[00:59:45.800 --> 00:59:49.800]   So I think there's pretty good odds I'll bring it back in conjunction with the release of
[00:59:49.800 --> 00:59:50.800]   the audiobook.
[00:59:50.800 --> 00:59:56.400]   And you were prescient not just with the cyber, with the pandemic, but "After On" was very
[00:59:56.400 --> 01:00:00.480]   prescient with this whole Facebook targeting and knowing everything.
[01:00:00.480 --> 01:00:04.600]   I remember the scene in the bar where it knew what drink you were going to have and offered
[01:00:04.600 --> 01:00:07.360]   it to you ahead of time.
[01:00:07.360 --> 01:00:09.800]   You've been writing about this for a long time.
[01:00:09.800 --> 01:00:15.000]   In a way, writing fiction gives you a chance to talk about these things in a way that it's
[01:00:15.000 --> 01:00:16.400]   not an argument.
[01:00:16.400 --> 01:00:17.400]   It's just what if?
[01:00:17.400 --> 01:00:18.400]   Right?
[01:00:18.400 --> 01:00:23.280]   Yeah, it's kind of like "Black Mirror" does a great job of it on TV.
[01:00:23.280 --> 01:00:29.240]   Where you take the very near future and you just tweak one variable, and that to me is
[01:00:29.240 --> 01:00:34.880]   a more interesting exercise than setting something in the deep future when you know essentially
[01:00:34.880 --> 01:00:38.040]   everything about society is going to be completely transformed.
[01:00:38.040 --> 01:00:43.520]   So you're almost creating a fantasy world because if you're writing science fiction
[01:00:43.520 --> 01:00:47.520]   that really takes technology into account, you can't really write something that's 300
[01:00:47.520 --> 01:00:51.440]   years of the future and have the society look very similar to our own.
[01:00:51.440 --> 01:00:52.440]   Yeah.
[01:00:52.440 --> 01:00:56.040]   So similar to we look to the society of the 1600s.
[01:00:56.040 --> 01:00:57.040]   We don't.
[01:00:57.040 --> 01:01:00.400]   Well, the funny thing though is the future is catching up with your science fiction.
[01:01:00.400 --> 01:01:01.400]   I know.
[01:01:01.400 --> 01:01:07.400]   Oh, after on was, I thought I was being all speculative when I was talking about a social
[01:01:07.400 --> 01:01:10.520]   network having gained conscious and run amok.
[01:01:10.520 --> 01:01:15.840]   And there was almost nothing in that book that Facebook hasn't done.
[01:01:15.840 --> 01:01:19.600]   By the time it came out, because there's always a 12 month lag with printed books, which is
[01:01:19.600 --> 01:01:20.600]   maddening.
[01:01:20.600 --> 01:01:26.400]   And the time that book came out, it was virtually not science fiction anymore aside from the
[01:01:26.400 --> 01:01:30.040]   AI attaining consciousness, which by the way is not a spoiler because you see that coming
[01:01:30.040 --> 01:01:31.200]   from the first page.
[01:01:31.200 --> 01:01:32.200]   Yeah.
[01:01:32.200 --> 01:01:33.200]   Yeah.
[01:01:33.200 --> 01:01:34.200]   It's so good though.
[01:01:34.200 --> 01:01:35.200]   Thank you.
[01:01:35.200 --> 01:01:36.200]   It's such a great story.
[01:01:36.200 --> 01:01:39.280]   So that's why I'm glad to hear you fiction that you haven't abandoned fiction, but they
[01:01:39.280 --> 01:01:43.640]   really they mesh because you really are as any great novelist.
[01:01:43.640 --> 01:01:49.520]   You are writing about current events in a fictional framework and it actually works
[01:01:49.520 --> 01:01:50.520]   really well.
[01:01:50.520 --> 01:01:51.520]   Really.
[01:01:51.520 --> 01:01:56.480]   I'm actually listening to 1984 the last time I read it had to be 20 years ago.
[01:01:56.480 --> 01:01:57.480]   Yeah.
[01:01:57.480 --> 01:01:58.680]   And I'm listening to it right now.
[01:01:58.680 --> 01:01:59.680]   I did the same thing.
[01:01:59.680 --> 01:02:00.680]   Yeah.
[01:02:00.680 --> 01:02:01.680]   It's fabulous.
[01:02:01.680 --> 01:02:02.680]   I love Audible.
[01:02:02.680 --> 01:02:05.320]   That was one of one of the great delights off of Audible.
[01:02:05.320 --> 01:02:09.760]   And he just inverted the last two years of two numbers in the year that he was writing
[01:02:09.760 --> 01:02:13.480]   1948 to call it 1984.
[01:02:13.480 --> 01:02:17.160]   And it was very, very much a commentary, although it was a science fiction novel.
[01:02:17.160 --> 01:02:22.520]   It was set in the future and had technologies that were at the time still impossible, two
[01:02:22.520 --> 01:02:23.520]   way telescreens.
[01:02:23.520 --> 01:02:24.520]   Right.
[01:02:24.520 --> 01:02:30.280]   But he was very much commentating on his present day and the rise of, you know, totalitarian
[01:02:30.280 --> 01:02:31.520]   communism and so forth.
[01:02:31.520 --> 01:02:35.680]   And it's, it's really fascinating to go back to that one.
[01:02:35.680 --> 01:02:36.680]   No kidding.
[01:02:36.680 --> 01:02:40.560]   I also have been listening to Brave New World because same thing, right?
[01:02:40.560 --> 01:02:41.560]   Yeah.
[01:02:41.560 --> 01:02:46.880]   It a little bit less dystopian, but but actually more dystopian if you really read between
[01:02:46.880 --> 01:02:50.880]   the lines, I would certainly rather live in the world of Brave New World, which is
[01:02:50.880 --> 01:02:56.800]   some some, yeah, give me some some and some of those great parties that they have.
[01:02:56.800 --> 01:03:03.560]   I'd rather live in that world given a choice, but it's, it's, it's a, it's a dystopia vapidity,
[01:03:03.560 --> 01:03:08.800]   like everybody is so vapid in that world, exactly that it would drive anybody on this,
[01:03:08.800 --> 01:03:11.040]   on this panel crazy to hear.
[01:03:11.040 --> 01:03:12.040]   Yes.
[01:03:12.040 --> 01:03:15.920]   But it would be better than being tortured by the thought police.
[01:03:15.920 --> 01:03:19.080]   I think that's the kind of the bottom line for this show.
[01:03:19.080 --> 01:03:21.360]   That's what we're going, what we're going for.
[01:03:21.360 --> 01:03:23.840]   Our show today brought to you by Eero.
[01:03:23.840 --> 01:03:29.000]   I tell you what, I still on the radio show almost every day, I get somebody calling me,
[01:03:29.000 --> 01:03:31.280]   complaining about their Wi-Fi.
[01:03:31.280 --> 01:03:36.000]   Wi-Fi was never designed for how we're using it with three kids on zoom, you know, looking
[01:03:36.000 --> 01:03:37.000]   up recipes.
[01:03:37.000 --> 01:03:39.200]   I'm looking up recipes in the kitchen.
[01:03:39.200 --> 01:03:41.520]   Somebody's watching YouTube videos.
[01:03:41.520 --> 01:03:43.960]   And, and I think that's, it's a big thing.
[01:03:43.960 --> 01:03:49.040]   YouTube videos and, and, and that's just in our neck of the woods and then we got the
[01:03:49.040 --> 01:03:53.960]   neighbors, their Wi-Fi, it's just a mess.
[01:03:53.960 --> 01:03:55.680]   That's why Eero is such a godsend.
[01:03:55.680 --> 01:04:03.640]   It's what we use at home and I have to say, uh, Eero solves your Wi-Fi issues fast, reliable
[01:04:03.640 --> 01:04:06.040]   Wi-Fi inside and out.
[01:04:06.040 --> 01:04:12.000]   If you've got rooms with bad to know Wi-Fi or dropouts on the back deck or as we call
[01:04:12.000 --> 01:04:15.080]   it back, he's the patio.
[01:04:15.080 --> 01:04:20.160]   Eero makes every square foot of your house usable by eliminating poor coverage and a
[01:04:20.160 --> 01:04:21.160]   dead spots.
[01:04:21.160 --> 01:04:23.920]   You'll have a strong signal wherever you need it.
[01:04:23.920 --> 01:04:29.400]   And then what's nice is you can add beacons to fill in the gaps, get the beer, Eero base
[01:04:29.400 --> 01:04:31.000]   station.
[01:04:31.000 --> 01:04:37.400]   And with everybody using the internet, Eero automatically adjusts with quality of service
[01:04:37.400 --> 01:04:39.400]   to the right places at the right times.
[01:04:39.400 --> 01:04:40.400]   It's fast.
[01:04:40.400 --> 01:04:41.400]   It's easy to set it up.
[01:04:41.400 --> 01:04:44.000]   You can plug it in your modem and you're good to go.
[01:04:44.000 --> 01:04:45.680]   And I love the Eero app.
[01:04:45.680 --> 01:04:50.960]   I've been using it a lot lately as we, as we, as I said, as we go wired, I'm still going
[01:04:50.960 --> 01:04:56.960]   to have to have a lot of Wi-Fi because even if you have a wired setup and, uh, and of
[01:04:56.960 --> 01:05:01.280]   course some of our workstations at home will be, I've got a ton of internet of things,
[01:05:01.280 --> 01:05:03.840]   devices, Google homes and echoes.
[01:05:03.840 --> 01:05:04.840]   I've got laptops.
[01:05:04.840 --> 01:05:05.840]   I've got phones.
[01:05:05.840 --> 01:05:09.120]   You still need a great Wi-Fi and that's Eero.
[01:05:09.120 --> 01:05:10.960]   We ask a lot of our Wi-Fi these days.
[01:05:10.960 --> 01:05:12.120]   Eero can help yours do more.
[01:05:12.120 --> 01:05:15.960]   Go to Eero, Eero, Eero.com/twit.
[01:05:15.960 --> 01:05:17.360]   If you use a go, Twitter, check out.
[01:05:17.360 --> 01:05:19.560]   You'll get free next day shipping with your order.
[01:05:19.560 --> 01:05:21.640]   Do not suffer another day.
[01:05:21.640 --> 01:05:25.800]   The more than you have to Eero.com/twit.
[01:05:25.800 --> 01:05:31.200]   And don't forget to use the offer code, Twitter, check out to get that free next day shipping.
[01:05:31.200 --> 01:05:33.440]   I tell you, Eero is such a boon.
[01:05:33.440 --> 01:05:34.440]   It saved our lives.
[01:05:34.440 --> 01:05:38.520]   It will save yours to Eero.com/twit.
[01:05:38.520 --> 01:05:41.560]   The offer code is Twit.
[01:05:41.560 --> 01:05:42.720]   Eero.com/twit.
[01:05:42.720 --> 01:05:43.720]   I think you're going to like it.
[01:05:43.720 --> 01:05:48.160]   Don't forget the offer code Twit for free overnight shipping.
[01:05:48.160 --> 01:05:49.640]   All right.
[01:05:49.640 --> 01:05:50.640]   Let's see here.
[01:05:50.640 --> 01:05:52.160]   What did I say we were going to talk about?
[01:05:52.160 --> 01:05:53.160]   I forgot already.
[01:05:53.160 --> 01:05:54.160]   TikTok.
[01:05:54.160 --> 01:05:55.160]   TikTok.
[01:05:55.160 --> 01:05:56.160]   TikTok.
[01:05:56.160 --> 01:05:57.160]   TikTok.
[01:05:57.160 --> 01:05:58.160]   Nobody's bought it yet.
[01:05:58.160 --> 01:06:04.960]   Do you think so the Chinese last week announced that they were going to add the intellectual
[01:06:04.960 --> 01:06:11.320]   property, the AI that TikTok uses to their export restrictions, in effect hinting that
[01:06:11.320 --> 01:06:14.040]   whoever buys TikTok ain't going to get the secret sauce.
[01:06:14.040 --> 01:06:16.280]   Do you think that's what's holding it up?
[01:06:16.280 --> 01:06:17.280]   Oh, yeah.
[01:06:17.280 --> 01:06:18.280]   Yeah.
[01:06:18.280 --> 01:06:20.280]   Why would you buy it?
[01:06:20.280 --> 01:06:24.520]   Well, you might buy it without the secret sauce, but then you pay a lot less money.
[01:06:24.520 --> 01:06:28.120]   And you would get the sauceless TikTok, which may not be worth a whole lot.
[01:06:28.120 --> 01:06:29.120]   Yeah.
[01:06:29.120 --> 01:06:31.200]   But you'd hopefully pay commensurately.
[01:06:31.200 --> 01:06:37.640]   Or you have to go on bend to the Chinese government to seek an export like grant.
[01:06:37.640 --> 01:06:40.720]   But that would completely mess with the timeline that Trump's put on this.
[01:06:40.720 --> 01:06:42.720]   I think he has a September 20th deadline.
[01:06:42.720 --> 01:06:43.720]   15th.
[01:06:43.720 --> 01:06:44.880]   When he wants us of 15th.
[01:06:44.880 --> 01:06:45.880]   15th.
[01:06:45.880 --> 01:06:46.880]   Yeah.
[01:06:46.880 --> 01:06:51.520]   Or even worse from the standpoint of the administration, you could license the algorithms from bite
[01:06:51.520 --> 01:06:53.080]   dance.
[01:06:53.080 --> 01:06:56.840]   But if you license the algorithms from bite dance, then you haven't achieved what the
[01:06:56.840 --> 01:07:00.480]   administration is seeking to achieve, which is getting China out of TikTok.
[01:07:00.480 --> 01:07:02.240]   Is that you're making a worse?
[01:07:02.240 --> 01:07:03.800]   You're making a worse.
[01:07:03.800 --> 01:07:04.800]   Yeah.
[01:07:04.800 --> 01:07:08.520]   So it's an interesting chess move by the Chinese government.
[01:07:08.520 --> 01:07:10.600]   And it'll be very interesting to see how it plays out.
[01:07:10.600 --> 01:07:14.960]   I don't know if anybody else shares this with me, but does anybody else find it bizarre
[01:07:14.960 --> 01:07:19.680]   that Oracle of all companies is buying TikTok?
[01:07:19.680 --> 01:07:22.920]   Like since when are they interested in reaching tweens and teens?
[01:07:22.920 --> 01:07:25.720]   I mean, nobody like the post office buying MTV.
[01:07:25.720 --> 01:07:26.720]   I just don't understand.
[01:07:26.720 --> 01:07:29.240]   It might tell you that the AI is valuable.
[01:07:29.240 --> 01:07:32.960]   It might tell you that there's a lot more to TikTok than just dancing people on videos
[01:07:32.960 --> 01:07:36.320]   that there is actually some secret sauce that Oracle could use.
[01:07:36.320 --> 01:07:38.480]   They certainly have AI efforts.
[01:07:38.480 --> 01:07:42.600]   And that's my assumption for Walmart too, that they need some sort of machine learning
[01:07:42.600 --> 01:07:45.200]   or AI that would help them be a little more competitive.
[01:07:45.200 --> 01:07:46.200]   Walmart is in the deal.
[01:07:46.200 --> 01:07:49.880]   And by the way, the last rumor was that Walmart is now teamed up with Microsoft.
[01:07:49.880 --> 01:07:52.200]   Originally, it was Walmart Softbank.
[01:07:52.200 --> 01:07:56.360]   And that was kind of a non-starter because there's no technology company involved.
[01:07:56.360 --> 01:07:58.960]   But Walmart makes sense for e-commerce too, right?
[01:07:58.960 --> 01:07:59.960]   Sure.
[01:07:59.960 --> 01:08:03.160]   TikTok is potentially a great e-commerce platform.
[01:08:03.160 --> 01:08:07.200]   I mean, Instagram, I bought way too much crap through Instagram.
[01:08:07.200 --> 01:08:09.680]   That's the real reason I don't have an Instagram account.
[01:08:09.680 --> 01:08:13.760]   I was kept buying stuff in the middle of the night, like weird underwear.
[01:08:13.760 --> 01:08:19.800]   Yeah, Walmart makes sense because both Walmart and TikTok reach inordinately high numbers
[01:08:19.800 --> 01:08:20.800]   of consumers.
[01:08:20.800 --> 01:08:21.800]   So that I get.
[01:08:21.800 --> 01:08:22.800]   Right.
[01:08:22.800 --> 01:08:26.960]   But I guess the AI dimension must be the only explanation for Oracle because also have
[01:08:26.960 --> 01:08:27.960]   a media business.
[01:08:27.960 --> 01:08:30.840]   Larry Ellison is a buddy of the president.
[01:08:30.840 --> 01:08:33.560]   And it could, I mean, it's still money.
[01:08:33.560 --> 01:08:38.480]   And among these guys, I think money talks more than friendship.
[01:08:38.480 --> 01:08:44.200]   But it could well be that the president said, look, Larry, let's work something out here.
[01:08:44.200 --> 01:08:46.520]   I don't know.
[01:08:46.520 --> 01:08:50.840]   That's one of the things that makes it so hard in this day and age because there's so
[01:08:50.840 --> 01:08:51.840]   much uncertainty.
[01:08:51.840 --> 01:08:54.840]   You feel like there's, this is why QAnon is so big.
[01:08:54.840 --> 01:08:58.600]   You just feel like there's machinery going on in the background that we just know nothing
[01:08:58.600 --> 01:08:59.600]   about.
[01:08:59.600 --> 01:09:01.640]   Oh, that's all something like that.
[01:09:01.640 --> 01:09:02.640]   You could tease it out though.
[01:09:02.640 --> 01:09:04.000]   Is that your beat, Dan?
[01:09:04.000 --> 01:09:05.840]   The machinery in the background?
[01:09:05.840 --> 01:09:07.520]   Nobody knows anything about.
[01:09:07.520 --> 01:09:11.560]   No, but QAnon people know a lot about and it's not.
[01:09:11.560 --> 01:09:12.840]   Well, they make up stuff.
[01:09:12.840 --> 01:09:16.800]   But I understand why people become conspiracy theorists.
[01:09:16.800 --> 01:09:18.680]   Some of it comes from this sense.
[01:09:18.680 --> 01:09:21.080]   Well, we were talking, it was really interesting.
[01:09:21.080 --> 01:09:22.920]   I can't remember what I was talking to.
[01:09:22.920 --> 01:09:25.480]   It's two things.
[01:09:25.480 --> 01:09:26.800]   Jeff Jarvis was referring to a book.
[01:09:26.800 --> 01:09:28.200]   I just can't remember the name of the book.
[01:09:28.200 --> 01:09:29.200]   It's two things.
[01:09:29.200 --> 01:09:32.520]   One, a mistrust of authority, clearly, right?
[01:09:32.520 --> 01:09:34.800]   But two, trauma.
[01:09:34.800 --> 01:09:36.920]   Trauma from being misled.
[01:09:36.920 --> 01:09:40.000]   Trauma from not understanding.
[01:09:40.000 --> 01:09:42.000]   Trauma from being beat down.
[01:09:42.000 --> 01:09:46.080]   And so you're looking for anything that will make you feel like you understand what's really
[01:09:46.080 --> 01:09:47.960]   going on because it isn't.
[01:09:47.960 --> 01:09:48.960]   You don't.
[01:09:48.960 --> 01:09:50.800]   And I completely understand that.
[01:09:50.800 --> 01:09:51.800]   I really do.
[01:09:51.800 --> 01:09:56.600]   I can see how seductive something like QAnon is.
[01:09:56.600 --> 01:10:02.840]   Even as bizarre and patently nonsensical as it is.
[01:10:02.840 --> 01:10:05.880]   So okay, TikTok.
[01:10:05.880 --> 01:10:06.880]   So will it be sold?
[01:10:06.880 --> 01:10:07.960]   Will it not be sold?
[01:10:07.960 --> 01:10:09.600]   What's the prognosis?
[01:10:09.600 --> 01:10:11.760]   I'll bet it gets sold.
[01:10:11.760 --> 01:10:12.760]   You think?
[01:10:12.760 --> 01:10:15.200]   Yeah, I bet it gets sold.
[01:10:15.200 --> 01:10:18.280]   Everybody on all sides of the transaction ultimately want that to happen.
[01:10:18.280 --> 01:10:19.680]   It's going to happen soon.
[01:10:19.680 --> 01:10:22.440]   You got a couple of weeks.
[01:10:22.440 --> 01:10:23.640]   What does Satya tell you, Lou?
[01:10:23.640 --> 01:10:27.160]   Are you going to be on the TikTok team?
[01:10:27.160 --> 01:10:28.160]   Oh, yeah.
[01:10:28.160 --> 01:10:30.840]   I mean, my point of view is this.
[01:10:30.840 --> 01:10:31.840]   Think about it.
[01:10:31.840 --> 01:10:35.080]   LinkedIn was a purchase a while back because they know that's where all the professionals
[01:10:35.080 --> 01:10:38.880]   are over the age of 25 or whatever.
[01:10:38.880 --> 01:10:41.320]   And so obviously this is a way for them to get into that.
[01:10:41.320 --> 01:10:45.440]   I mean, 60% of all US users are between 16 and 24 of TikTok.
[01:10:45.440 --> 01:10:46.440]   So like there you go.
[01:10:46.440 --> 01:10:50.760]   I guarantee Microsoft is interested because of that and that's where they're going to
[01:10:50.760 --> 01:10:51.760]   push hard on it.
[01:10:51.760 --> 01:10:57.760]   And it kind of completes their portfolio if you think about it.
[01:10:57.760 --> 01:10:59.840]   They bought LinkedIn.
[01:10:59.840 --> 01:11:01.240]   That's right.
[01:11:01.240 --> 01:11:02.240]   Same reason?
[01:11:02.240 --> 01:11:04.680]   They brought Minecraft.
[01:11:04.680 --> 01:11:08.000]   Minecraft was a good purchase.
[01:11:08.000 --> 01:11:13.280]   And I think Microsoft has done, has been a great steward of Minecraft.
[01:11:13.280 --> 01:11:16.280]   That has gotten better since Microsoft bought it.
[01:11:16.280 --> 01:11:17.280]   Yes.
[01:11:17.280 --> 01:11:19.880]   Although they've been eclipsed by Roblox, I believe in users.
[01:11:19.880 --> 01:11:21.680]   I think so.
[01:11:21.680 --> 01:11:24.680]   Roblox is big on TikTok with the kids.
[01:11:24.680 --> 01:11:25.680]   Yeah.
[01:11:25.680 --> 01:11:32.760]   I know that because Michael, who's become a big part of this show, are 17-year-olds,
[01:11:32.760 --> 01:11:36.280]   went to grade school with a young girl.
[01:11:36.280 --> 01:11:39.880]   He had a crush on it at the time who now is like a major TikTok star.
[01:11:39.880 --> 01:11:41.600]   She's the queen of Roblox.
[01:11:41.600 --> 01:11:44.440]   So I know that.
[01:11:44.440 --> 01:11:45.640]   So there is some sort of nexus.
[01:11:45.640 --> 01:11:47.040]   I never got into Roblox.
[01:11:47.040 --> 01:11:49.760]   Is that something an adult would be interested in?
[01:11:49.760 --> 01:11:52.440]   I think it's a pretty young demographic.
[01:11:52.440 --> 01:11:53.440]   Yeah.
[01:11:53.440 --> 01:11:54.440]   Sorry?
[01:11:54.440 --> 01:11:55.440]   There are creep.
[01:11:55.440 --> 01:11:57.760]   I mean, it is used by predators.
[01:11:57.760 --> 01:11:58.760]   Oh, really?
[01:11:58.760 --> 01:11:59.760]   Oh, God.
[01:11:59.760 --> 01:12:00.760]   Absolutely.
[01:12:00.760 --> 01:12:01.760]   Yeah.
[01:12:01.760 --> 01:12:02.760]   Because that's where kids are.
[01:12:02.760 --> 01:12:03.760]   And it's an online service.
[01:12:03.760 --> 01:12:07.320]   Supposed to Minecraft, you really couldn't be a predator in Minecraft.
[01:12:07.320 --> 01:12:08.800]   You'd be an 8-bit predator.
[01:12:08.800 --> 01:12:11.160]   Roblox is the same thing.
[01:12:11.160 --> 01:12:12.960]   These are avatars.
[01:12:12.960 --> 01:12:15.000]   You're avatars can be creepy.
[01:12:15.000 --> 01:12:16.760]   No, you're exactly right.
[01:12:16.760 --> 01:12:18.400]   These are social networks.
[01:12:18.400 --> 01:12:20.560]   I mean, where are there social networks?
[01:12:20.560 --> 01:12:22.240]   Where are people that are creepy?
[01:12:22.240 --> 01:12:23.240]   Yeah.
[01:12:23.240 --> 01:12:24.680]   Isn't that nice?
[01:12:24.680 --> 01:12:27.200]   Isn't that special?
[01:12:27.200 --> 01:12:30.360]   We're trying to figure out what to do.
[01:12:30.360 --> 01:12:42.360]   John Prosser, who is a pretty prolific and fairly accurate, Apple Tipster on Twitter,
[01:12:42.360 --> 01:12:47.800]   is 77% accurate, according to people who keep track of this thing.
[01:12:47.800 --> 01:12:53.080]   Prosser says that there will be an Apple announcement on Tuesday.
[01:12:53.080 --> 01:12:54.080]   Mm.
[01:12:54.080 --> 01:12:55.080]   Yes.
[01:12:55.080 --> 01:13:03.880]   Not clear if the announcement will be a live stream or merely a press release, his last
[01:13:03.880 --> 01:13:05.840]   tweet nine hours ago.
[01:13:05.840 --> 01:13:12.040]   Even Apple press release is currently scheduled for Tuesday at 9am Eastern.
[01:13:12.040 --> 01:13:16.080]   Though I should note, it's not locked in until the press has been briefed day of.
[01:13:16.080 --> 01:13:19.320]   I'll tweet early that morning to update you if it changes.
[01:13:19.320 --> 01:13:25.040]   We know that there are new iPads, at least one new iPad and iPad Air, but I think more
[01:13:25.040 --> 01:13:31.160]   than one, in the offing as well as Apple watches in the offing.
[01:13:31.160 --> 01:13:32.240]   But a press release.
[01:13:32.240 --> 01:13:37.160]   This would normally be the day Tuesday that Apple would announce the new iPhones.
[01:13:37.160 --> 01:13:42.680]   I think we're delayed on those because of COVID.
[01:13:42.680 --> 01:13:50.960]   Apple said in its last quarterly results that they would be delayed probably as much
[01:13:50.960 --> 01:13:54.680]   as a month, kind of more like the iPhone 10 timeframe.
[01:13:54.680 --> 01:13:58.680]   Prosser last August.
[01:13:58.680 --> 01:14:04.600]   So it's a really old tweet, but he did update at September 1st said the Apple watch and
[01:14:04.600 --> 01:14:08.600]   iPad via press release this week.
[01:14:08.600 --> 01:14:13.320]   iPhone 12 event next month, the week of October 12th.
[01:14:13.320 --> 01:14:19.440]   The devices will be available for pre-order that day, but not shipping till October 19th.
[01:14:19.440 --> 01:14:27.280]   And then the iPhone 12 pro device, the high end iPhone with 5G, not till November.
[01:14:27.280 --> 01:14:29.200]   So that's the timeline.
[01:14:29.200 --> 01:14:30.200]   You know what?
[01:14:30.200 --> 01:14:35.160]   Whether that's accurate or made up, we'll find out probably Tuesday.
[01:14:35.160 --> 01:14:36.960]   I was hoping there'd be an event.
[01:14:36.960 --> 01:14:41.480]   I told the team, let's be here early and get ready and maybe they'll be an event.
[01:14:41.480 --> 01:14:42.480]   But maybe not.
[01:14:42.480 --> 01:14:44.200]   Maybe we'll just be a press release.
[01:14:44.200 --> 01:14:47.320]   I guess none of you care at all.
[01:14:47.320 --> 01:14:48.760]   You want to talk about TikTok some more?
[01:14:48.760 --> 01:14:50.080]   Because that's the choice, guys.
[01:14:50.080 --> 01:14:51.400]   I'm sorry.
[01:14:51.400 --> 01:14:55.720]   I'd be excited for an Apple glass announcement, but I don't think that's until a couple
[01:14:55.720 --> 01:14:56.720]   of years from now.
[01:14:56.720 --> 01:14:57.720]   Yeah.
[01:14:57.720 --> 01:14:58.720]   It's through the year after.
[01:14:58.720 --> 01:14:59.720]   Yeah.
[01:14:59.720 --> 01:15:00.720]   Wouldn't that be?
[01:15:00.720 --> 01:15:03.320]   Just don't want another device to come out, so I have to pay for more devices.
[01:15:03.320 --> 01:15:06.600]   Oh, join my Spain.
[01:15:06.600 --> 01:15:09.080]   I hate this time of year.
[01:15:09.080 --> 01:15:12.080]   Fortunately, I can now kind of palm this.
[01:15:12.080 --> 01:15:14.560]   I still have to buy it, but at least I don't have to.
[01:15:14.560 --> 01:15:20.800]   I can have Jason Howell do the note 20 and the S20.
[01:15:20.800 --> 01:15:23.520]   And I don't know if we'll do the fold.
[01:15:23.520 --> 01:15:27.280]   I decided to take the duo, but then yeah, then there's going to be new iPhones to buy.
[01:15:27.280 --> 01:15:29.480]   Maybe I'll make Micah use those.
[01:15:29.480 --> 01:15:32.080]   I palm it off on the other guys.
[01:15:32.080 --> 01:15:34.480]   Do you feel obligated to buy all this stuff?
[01:15:34.480 --> 01:15:35.640]   You don't have to buy this stuff, Lou.
[01:15:35.640 --> 01:15:37.760]   Or do you feel like you should?
[01:15:37.760 --> 01:15:43.280]   You know, I don't feel obligated, but I do like to buy the latest tech or run our software
[01:15:43.280 --> 01:15:44.280]   on it only because--
[01:15:44.280 --> 01:15:45.280]   That makes sense.
[01:15:45.280 --> 01:15:46.280]   Yeah.
[01:15:46.280 --> 01:15:47.280]   I mean--
[01:15:47.280 --> 01:15:48.280]   Can you get the most to buy it?
[01:15:48.280 --> 01:15:49.280]   I mean, you don't have to--
[01:15:49.280 --> 01:15:51.280]   Do you have to lay out your own money?
[01:15:51.280 --> 01:15:52.280]   Most of the time, no.
[01:15:52.280 --> 01:15:53.280]   Most of the time I don't.
[01:15:53.280 --> 01:15:58.120]   Most of the time I do only because I want to be able to use on my own apps and--
[01:15:58.120 --> 01:15:59.120]   Exactly.
[01:15:59.120 --> 01:16:00.120]   --you may own software on it.
[01:16:00.120 --> 01:16:01.120]   Exactly.
[01:16:01.120 --> 01:16:03.960]   That's why I try to, if I can afford to buy it because I don't think you have a real
[01:16:03.960 --> 01:16:06.640]   experience with a two-week review that you don't really invest in.
[01:16:06.640 --> 01:16:07.640]   No, you really can't.
[01:16:07.640 --> 01:16:09.040]   I'm going to invest in the duo.
[01:16:09.040 --> 01:16:11.280]   It's going to be my-- we'll see.
[01:16:11.280 --> 01:16:12.280]   Micah primary theory.
[01:16:12.280 --> 01:16:13.280]   Did you get the pen with it?
[01:16:13.280 --> 01:16:14.280]   No, should I?
[01:16:14.280 --> 01:16:15.280]   Did you get the pen with it?
[01:16:15.280 --> 01:16:16.280]   Look how big that is.
[01:16:16.280 --> 01:16:17.840]   That's bigger than the duo.
[01:16:17.840 --> 01:16:22.880]   You don't get it with the duo, but this is by far the most superior pen of every--
[01:16:22.880 --> 01:16:23.880]   Which one?
[01:16:23.880 --> 01:16:25.880]   That's the one that comes with the X.
[01:16:25.880 --> 01:16:26.880]   Yes.
[01:16:26.880 --> 01:16:27.880]   Well, it doesn't--
[01:16:27.880 --> 01:16:28.880]   It hides inside.
[01:16:28.880 --> 01:16:33.200]   But Nx doesn't come with this cool little charging thing.
[01:16:33.200 --> 01:16:34.200]   Oh.
[01:16:34.200 --> 01:16:37.000]   Can you get that from Microsoft?
[01:16:37.000 --> 01:16:38.000]   You get that from Microsoft.
[01:16:38.000 --> 01:16:41.920]   I have a Lenovo sitting in front of me that comes with a pen.
[01:16:41.920 --> 01:16:43.960]   Will that pen work with a go?
[01:16:43.960 --> 01:16:45.760]   It works with Windows 10.
[01:16:45.760 --> 01:16:46.760]   No, that's a good question.
[01:16:46.760 --> 01:16:47.760]   I'm not trying that.
[01:16:47.760 --> 01:16:48.760]   I'll find out.
[01:16:48.760 --> 01:16:50.920]   I don't want to carry something this big.
[01:16:50.920 --> 01:16:54.080]   I want this tiny little thin thing.
[01:16:54.080 --> 01:16:58.880]   Now, this is actually important for you because you work on the Office team.
[01:16:58.880 --> 01:17:04.680]   I have to say, the touch first office on the iPad might be the best version of Office
[01:17:04.680 --> 01:17:05.680]   out there.
[01:17:05.680 --> 01:17:06.680]   It's really nice.
[01:17:06.680 --> 01:17:07.680]   Yeah.
[01:17:07.680 --> 01:17:09.480]   It's way better than Google Docs on the--
[01:17:09.480 --> 01:17:10.480]   Way better.
[01:17:10.480 --> 01:17:13.200]   Yeah, Google Docs is not good on the iOS.
[01:17:13.200 --> 01:17:16.920]   And I presume-- watch this, kids.
[01:17:16.920 --> 01:17:22.000]   I presume Office would work pretty nicely on a duo.
[01:17:22.000 --> 01:17:23.000]   Oh, very good.
[01:17:23.000 --> 01:17:24.000]   Oh, very good.
[01:17:24.000 --> 01:17:25.000]   Yeah.
[01:17:25.000 --> 01:17:29.520]   You know, there's been builds for a while of Office working on those devices.
[01:17:29.520 --> 01:17:32.360]   And I can tell you the experience is awesome.
[01:17:32.360 --> 01:17:33.360]   It's funny.
[01:17:33.360 --> 01:17:35.400]   I don't know if nobody say Outlook experience is awesome.
[01:17:35.400 --> 01:17:36.400]   No, I hate that.
[01:17:36.400 --> 01:17:37.400]   Sorry, Alex.
[01:17:37.400 --> 01:17:42.320]   But hey, I think that it really is awesome on a device like that.
[01:17:42.320 --> 01:17:46.120]   Because you have the list of emails on the left screen, you have the email on the right
[01:17:46.120 --> 01:17:47.760]   screen or vice versa.
[01:17:47.760 --> 01:17:49.680]   That looks pretty-- I saw Panos demonstrating that.
[01:17:49.680 --> 01:17:51.000]   It looks pretty nice.
[01:17:51.000 --> 01:17:56.480]   But I'm also really curious how many apps Microsoft has optimized for the two-screen
[01:17:56.480 --> 01:18:01.840]   experience, how screen and where it will be, how it will know if it's open all the way,
[01:18:01.840 --> 01:18:06.440]   or if it's flipped around, what it will look like when I scroll between two screens.
[01:18:06.440 --> 01:18:10.920]   All of that is really going to come down to what Microsoft has done to Android.
[01:18:10.920 --> 01:18:15.800]   I know that Android has some dual-screen capabilities built in.
[01:18:15.800 --> 01:18:19.720]   But I don't think it's so aware that it would know, for instance, what the configuration
[01:18:19.720 --> 01:18:21.640]   of the duo is, right?
[01:18:21.640 --> 01:18:22.640]   Right.
[01:18:22.640 --> 01:18:28.960]   No, they definitely had to do a bunch of low-level things to be able to support the 360-degree
[01:18:28.960 --> 01:18:32.360]   view and being able to do the various things.
[01:18:32.360 --> 01:18:34.840]   And some of those features that you're talking about where you can actually do something
[01:18:34.840 --> 01:18:39.040]   over here, where this something over here does the same thing or you scroll, is a big
[01:18:39.040 --> 01:18:40.040]   feature.
[01:18:40.040 --> 01:18:43.160]   It's one of those things, like kiosk-type features that they want.
[01:18:43.160 --> 01:18:48.120]   Yeah, there's no point making a two-screen device if you're not screen, two-screen aware
[01:18:48.120 --> 01:18:50.920]   and use it intelligently.
[01:18:50.920 --> 01:18:52.680]   And I'm very intrigued by all of that.
[01:18:52.680 --> 01:18:53.680]   I like that idea.
[01:18:53.680 --> 01:18:54.680]   So, I'll see.
[01:18:54.680 --> 01:18:58.680]   We'll have more to talk about next week.
[01:18:58.680 --> 01:19:02.640]   Google is in the crosshairs from the Department of Justice.
[01:19:02.640 --> 01:19:09.560]   And apparently, Bill Barr is rushing things along.
[01:19:09.560 --> 01:19:11.120]   In the next couple of...
[01:19:11.120 --> 01:19:12.600]   He's Russian.
[01:19:12.600 --> 01:19:14.600]   He's rushing things along.
[01:19:14.600 --> 01:19:15.600]   That's bad.
[01:19:15.600 --> 01:19:16.600]   Sorry.
[01:19:16.600 --> 01:19:20.600]   You had to do it, didn't you?
[01:19:20.600 --> 01:19:23.560]   Apparently, according to the New York Times, the Justice Department plans to file antitrust
[01:19:23.560 --> 01:19:26.280]   charges against Google in the coming weeks.
[01:19:26.280 --> 01:19:32.360]   The AG has said to have set a deadline over the objections, the objections of DOJ career
[01:19:32.360 --> 01:19:36.920]   lawyers who say they need more time to build the case.
[01:19:36.920 --> 01:19:38.080]   Why?
[01:19:38.080 --> 01:19:44.720]   What's the political payoff to going after Google now?
[01:19:44.720 --> 01:19:46.640]   Do Andy have a thought on that?
[01:19:46.640 --> 01:19:47.640]   No idea.
[01:19:47.640 --> 01:19:48.640]   No idea.
[01:19:48.640 --> 01:19:50.800]   Does it to looks...
[01:19:50.800 --> 01:19:57.960]   See, this is the trouble right now with any antitrust movement against any tech company.
[01:19:57.960 --> 01:20:01.640]   We're all reliant on tech companies right now more than ever.
[01:20:01.640 --> 01:20:06.360]   And in fact, if the president cares about the stock market, which he apparently does
[01:20:06.360 --> 01:20:07.880]   a lot, that's more than the stock market.
[01:20:07.880 --> 01:20:13.480]   It's more than ever dependent on the fortunes of the big five.
[01:20:13.480 --> 01:20:14.480]   Those are the ones...
[01:20:14.480 --> 01:20:15.480]   The stock...
[01:20:15.480 --> 01:20:16.480]   Microsoft...
[01:20:16.480 --> 01:20:17.480]   Congratulations, Lou.
[01:20:17.480 --> 01:20:18.480]   I hope you have a lot of stock.
[01:20:18.480 --> 01:20:27.400]   Microsoft, Apple, Amazon, Google, those stocks are powering the stock market.
[01:20:27.400 --> 01:20:29.200]   The other stock's not so much.
[01:20:29.200 --> 01:20:34.000]   Well, what they're investigating Google for is I think the same thing that they paid
[01:20:34.000 --> 01:20:37.160]   a $2.6 billion fine for in Europe.
[01:20:37.160 --> 01:20:42.040]   So if that's the case and those are the approximate dimensions of the consequences,
[01:20:42.040 --> 01:20:47.360]   I don't know how many days of earnings equate to $2.6 billion, but it really shouldn't
[01:20:47.360 --> 01:20:50.960]   lop that much off of Google's market cap.
[01:20:50.960 --> 01:20:51.960]   In terms of...
[01:20:51.960 --> 01:20:54.880]   It's for why Barr is rushing it.
[01:20:54.880 --> 01:20:58.360]   Maybe he feels that he's on a short clock now and he wants to make sure that he gets
[01:20:58.360 --> 01:21:01.480]   his licks in before there's an administration change.
[01:21:01.480 --> 01:21:02.480]   Interesting.
[01:21:02.480 --> 01:21:05.600]   That's just a guess because that surprised me as well.
[01:21:05.600 --> 01:21:11.000]   The Times says the former telecom industry executive who argued in an antitrust matter
[01:21:11.000 --> 01:21:13.840]   before the Supreme Court, Mr. Barr.
[01:21:13.840 --> 01:21:15.120]   He's talking about Barr.
[01:21:15.120 --> 01:21:17.800]   Barr has shown a deep interest in the Google investigation.
[01:21:17.800 --> 01:21:22.440]   He's requested regular briefing on the department's case taking thick binders of information
[01:21:22.440 --> 01:21:26.320]   about it on trips and vacations and returning it with ideas and nuts.
[01:21:26.320 --> 01:21:27.320]   Maybe you're right, Rob.
[01:21:27.320 --> 01:21:29.640]   Maybe he feels like it's his legacy.
[01:21:29.640 --> 01:21:34.760]   He wants to get this investigation off the ground before he leaves.
[01:21:34.760 --> 01:21:36.320]   Forget it past the point of no return.
[01:21:36.320 --> 01:21:41.760]   I think there's also a feeling amongst some Republicans that the tech companies are bastions
[01:21:41.760 --> 01:21:44.640]   of liberal thinking and liberal practices.
[01:21:44.640 --> 01:21:45.640]   That's what I wondered.
[01:21:45.640 --> 01:21:46.640]   It's punishment.
[01:21:46.640 --> 01:21:47.640]   If it's punishment.
[01:21:47.640 --> 01:21:50.880]   Yeah, there could be an element of in dictiveness there.
[01:21:50.880 --> 01:21:54.800]   Whatever it is, it certainly seems that he wants this thing to get going.
[01:21:54.800 --> 01:21:59.680]   If he has three months, if there is an administration change, and obviously that's not yet known,
[01:21:59.680 --> 01:22:05.600]   but if he has three months to get the thing ginned up, it's much, much harder for a subsequent
[01:22:05.600 --> 01:22:09.680]   administration to say, "Oh, never mind," than if he only had three weeks.
[01:22:09.680 --> 01:22:14.520]   Remember, the FTC was looking into Google and decided to drop the case quite famously
[01:22:14.520 --> 01:22:16.800]   a couple of years ago.
[01:22:16.800 --> 01:22:19.000]   They felt like there wasn't enough.
[01:22:19.000 --> 01:22:24.560]   Currently the 40 lawyers, according to the times who are working on the Google antitrust
[01:22:24.560 --> 01:22:28.280]   investigation, they say the time says most of the 40-odd lawyers have been working on
[01:22:28.280 --> 01:22:33.600]   the investigation, opposed the deadline to wrap it up by the end of September, according
[01:22:33.600 --> 01:22:34.800]   to three of the people.
[01:22:34.800 --> 01:22:40.480]   Some said they wouldn't sign the complaint, and several of them left the case this summer.
[01:22:40.480 --> 01:22:46.280]   He's getting some pushback from the DOJ career attorneys.
[01:22:46.280 --> 01:22:49.520]   Some argued this summer in a memo that ran hundreds of pages that they could bring a
[01:22:49.520 --> 01:22:54.040]   strong case, but needed more time.
[01:22:54.040 --> 01:22:55.520]   Maybe the DOJ does want to do this.
[01:22:55.520 --> 01:22:57.640]   This is what happened to the FTC.
[01:22:57.640 --> 01:23:00.680]   They dragged their heels, dragged their heels, and eventually dropped the case.
[01:23:00.680 --> 01:23:10.680]   Meanwhile, Amazon's furious over Jedi, the Pentagon contract, which Microsoft reaffirmed
[01:23:10.680 --> 01:23:19.680]   this week, the decision to keep Microsoft, Jeff Bezos, I'm sorry, Amazon says, "Why
[01:23:19.680 --> 01:23:26.480]   we will continue to protest this politically corrupted contract award."
[01:23:26.480 --> 01:23:30.440]   It's Amazon's position that the president doesn't like Jeff Bezos because he owns the
[01:23:30.440 --> 01:23:37.320]   Washington Post, and put his thumb on the scale, and they gave it to Microsoft.
[01:23:37.320 --> 01:23:44.680]   Amazon's point is we have a more sophisticated Amazon Web Services, more of what the military
[01:23:44.680 --> 01:23:45.680]   needs.
[01:23:45.680 --> 01:23:50.920]   We're the clear leader in cloud computing, and this is quote from the press release.
[01:23:50.920 --> 01:23:57.840]   By any objective measure, AWS has superior technology.
[01:23:57.840 --> 01:24:00.520]   There was some surprise when Microsoft won the Jedi contract.
[01:24:00.520 --> 01:24:06.040]   This is a $10 billion contract, 10 years, a billion dollar a year to provide cloud services
[01:24:06.040 --> 01:24:10.120]   for the Department of Defense.
[01:24:10.120 --> 01:24:15.960]   Most people I think thought AWS was a shoe-in.
[01:24:15.960 --> 01:24:19.320]   I'm not privileged to any information here, but I could definitely tell you that companies
[01:24:19.320 --> 01:24:23.840]   like Microsoft, the Oracle, the Googler world that had their bids in were willing to offer
[01:24:23.840 --> 01:24:29.200]   a significant amount of discounts and additional things to the government in order to buy to
[01:24:29.200 --> 01:24:32.240]   get this contract that AWS was.
[01:24:32.240 --> 01:24:36.200]   I'm not surprised that they went with number two versus number one.
[01:24:36.200 --> 01:24:37.200]   Just on price.
[01:24:37.200 --> 01:24:38.200]   Yes, exactly.
[01:24:38.200 --> 01:24:43.440]   Again, I don't have any knowledge here, but I can definitely tell you that that's one
[01:24:43.440 --> 01:24:44.440]   of the biggest reasons.
[01:24:44.440 --> 01:24:49.320]   You say that they have better technology is just kind of ridiculous if you think about
[01:24:49.320 --> 01:24:50.320]   it.
[01:24:50.320 --> 01:24:51.320]   I mean...
[01:24:51.320 --> 01:24:52.920]   Yeah, I have to think they really have better technology.
[01:24:52.920 --> 01:24:53.920]   Some parody.
[01:24:53.920 --> 01:24:56.400]   Amazon's been doing it for longer.
[01:24:56.400 --> 01:24:59.960]   AWS is definitely the leader in this field.
[01:24:59.960 --> 01:25:05.400]   Amazon, the press release said it's important to point out that the Department of Defense
[01:25:05.400 --> 01:25:11.000]   cited price, as you said, Lou, as a major factor in the previous decision, but this
[01:25:11.000 --> 01:25:16.680]   time during the reevaluation process, AWS offered a lower cost by several tens of millions
[01:25:16.680 --> 01:25:18.280]   of dollars.
[01:25:18.280 --> 01:25:24.560]   The DOD's decision to intentionally ignore the clear cost benefits offered by AWS reinforces
[01:25:24.560 --> 01:25:30.800]   the fact that this corrective action, this reevaluation, was never meant to be fair.
[01:25:30.800 --> 01:25:32.000]   They're hopping mad.
[01:25:32.000 --> 01:25:35.800]   They held back on calling it corrupt initially, I think.
[01:25:35.800 --> 01:25:37.400]   They're not holding back anymore.
[01:25:37.400 --> 01:25:42.840]   Yeah, increasingly the entrepreneurs and investors that I talk to about cloud computing, I've
[01:25:42.840 --> 01:25:47.720]   been hearing more and more and more about Microsoft and how they've done an incredible
[01:25:47.720 --> 01:25:48.720]   job here.
[01:25:48.720 --> 01:25:54.000]   And I think part of it, I mean, this is again just second hand from a diversity of sources,
[01:25:54.000 --> 01:25:57.160]   they just understand the enterprise in ways that Amazon doesn't.
[01:25:57.160 --> 01:26:02.000]   Amazon understands the American retail buyer better than any company in the world, but
[01:26:02.000 --> 01:26:04.800]   who understands the enterprise better than Microsoft?
[01:26:04.800 --> 01:26:07.040]   And ultimately cloud computing is an enterprise product.
[01:26:07.040 --> 01:26:13.880]   And it may just be sort of customer mentality, customer orientation that carried the day,
[01:26:13.880 --> 01:26:16.440]   or it could have been a political vendetta, who knows.
[01:26:16.440 --> 01:26:21.280]   But I definitely have been hearing a lot more about Microsoft's cloud offering over the
[01:26:21.280 --> 01:26:23.600]   last six months than ever before.
[01:26:23.600 --> 01:26:31.880]   I got to say though, we've heard reporting that President Trump ordered the former Secretary
[01:26:31.880 --> 01:26:37.360]   of Defense mad dog, Mattis to quote, screw Amazon.
[01:26:37.360 --> 01:26:43.240]   According to the Amazon press release, blatantly interfered in an active procurement, directed
[01:26:43.240 --> 01:26:48.360]   his subordinate to conduct an unorthodox review prior to a contract award announcement,
[01:26:48.360 --> 01:26:52.200]   and then stonewalled an investigation into his own political interference.
[01:26:52.200 --> 01:26:55.280]   Amazon really hopping mad.
[01:26:55.280 --> 01:26:58.800]   Yeah, hopping mad.
[01:26:58.800 --> 01:27:04.240]   They say we strongly disagree with the Department of Defense flawed evaluation and believe it's
[01:27:04.240 --> 01:27:08.240]   critical for our country that the government and its elected leaders administer procurements
[01:27:08.240 --> 01:27:09.440]   objectively.
[01:27:09.440 --> 01:27:10.960]   I think that's true.
[01:27:10.960 --> 01:27:13.080]   And in a manner that is free from political influence.
[01:27:13.080 --> 01:27:17.520]   And I guess that's the only debate is who was, who was technically the right, what was the
[01:27:17.520 --> 01:27:19.560]   right choice for the Pentagon?
[01:27:19.560 --> 01:27:20.720]   Or does it even matter?
[01:27:20.720 --> 01:27:24.680]   Maybe there's such, such a little difference between the two.
[01:27:24.680 --> 01:27:28.120]   The question we continue to ask ourselves is whether the President of the United States
[01:27:28.120 --> 01:27:32.720]   should be allowed to use the budget of the Department of Defense to pursue his own political
[01:27:32.720 --> 01:27:34.720]   and personal ends.
[01:27:34.720 --> 01:27:41.360]   Well, if Jeff Bezos and President Trump ever hoped to make up kiss and make up, that's all
[01:27:41.360 --> 01:27:43.280]   over.
[01:27:43.280 --> 01:27:47.880]   I think that's all over.
[01:27:47.880 --> 01:27:52.400]   Anybody going to buy the halo?
[01:27:52.400 --> 01:27:54.240]   Speaking of Amazon.
[01:27:54.240 --> 01:27:56.120]   I was fascinated.
[01:27:56.120 --> 01:27:58.640]   I asked for an invite.
[01:27:58.640 --> 01:28:00.240]   If I could get an invite, I'll buy it.
[01:28:00.240 --> 01:28:02.440]   It is interesting, isn't it?
[01:28:02.440 --> 01:28:03.920]   Subscription is interesting.
[01:28:03.920 --> 01:28:06.360]   Well, yeah, that's another thing.
[01:28:06.360 --> 01:28:11.800]   How much is it?
[01:28:11.800 --> 01:28:13.760]   Hardware is $65.
[01:28:13.760 --> 01:28:15.760]   It'll eventually be $100.
[01:28:15.760 --> 01:28:20.240]   The monthly subscription after the first six months, it's only four bucks a month.
[01:28:20.240 --> 01:28:25.120]   Although I can't think of any fitness band that requires you to pay monthly, right?
[01:28:25.120 --> 01:28:33.880]   What the halo will do, and the thing I'm going to have the most fun with, it uses AI to
[01:28:33.880 --> 01:28:36.680]   learn to judge you.
[01:28:36.680 --> 01:28:38.160]   Pretty much.
[01:28:38.160 --> 01:28:42.680]   Why wouldn't I wear a band that uses AI to judge me?
[01:28:42.680 --> 01:28:45.360]   I mean, come on.
[01:28:45.360 --> 01:28:46.840]   You get a halo app.
[01:28:46.840 --> 01:28:50.000]   It has some activity tracker.
[01:28:50.000 --> 01:28:57.280]   There's no screen on it.
[01:28:57.280 --> 01:28:57.280]   It's just a Velcro wristband with a little thing inside or an accelerometer, temperature sensor,
[01:28:57.280 --> 01:28:58.760]   heart rate monitor.
[01:28:58.760 --> 01:29:03.480]   But it also has, and this is why I bought it, two built-in microphones to measure how
[01:29:03.480 --> 01:29:06.360]   you speak.
[01:29:06.360 --> 01:29:10.760]   According to Amazon, how you speak, am I being a sucker?
[01:29:10.760 --> 01:29:13.040]   Am I just falling into this?
[01:29:13.040 --> 01:29:16.240]   Jeff Bezos really just wants to put a bug on my wrist.
[01:29:16.240 --> 01:29:17.720]   Is that really what's...
[01:29:17.720 --> 01:29:21.120]   I thought that sounded really speculative.
[01:29:21.120 --> 01:29:25.840]   The notion that your tone of voice could be so well understood by this device that it
[01:29:25.840 --> 01:29:31.720]   would be able to assess your wellness based on your tone of voice, that doesn't sound
[01:29:31.720 --> 01:29:35.480]   like something that most medical professionals would buy into.
[01:29:35.480 --> 01:29:36.480]   I can't.
[01:29:36.480 --> 01:29:37.480]   And you're a kill.
[01:29:37.480 --> 01:29:40.600]   I mean, it sounds like a really interesting big data project for an academic lab, but
[01:29:40.600 --> 01:29:43.160]   you would need to do...
[01:29:43.160 --> 01:29:46.840]   To train the AI, you would really need to train them on the individual.
[01:29:46.840 --> 01:29:50.520]   And what does the individual's voice sound like when they're sick and they're well?
[01:29:50.520 --> 01:29:52.520]   And they don't know that about you.
[01:29:52.520 --> 01:29:53.520]   That's AI.
[01:29:53.520 --> 01:29:55.680]   That struck me as goofy.
[01:29:55.680 --> 01:29:56.680]   That...
[01:29:56.680 --> 01:29:57.680]   I don't know.
[01:29:57.680 --> 01:29:58.680]   AI is...
[01:29:58.680 --> 01:29:59.680]   Go ahead.
[01:29:59.680 --> 01:30:00.680]   It's hard to interrupt you.
[01:30:00.680 --> 01:30:02.520]   Oh, your AI is so good.
[01:30:02.520 --> 01:30:04.680]   This is how you implement it.
[01:30:04.680 --> 01:30:05.680]   This is...
[01:30:05.680 --> 01:30:06.960]   You sound cranky today.
[01:30:06.960 --> 01:30:07.960]   Have you...
[01:30:07.960 --> 01:30:09.160]   Did you get enough sleep?
[01:30:09.160 --> 01:30:11.960]   Well, they'll know what it means.
[01:30:11.960 --> 01:30:12.960]   You got enough sleep.
[01:30:12.960 --> 01:30:13.960]   They'll know.
[01:30:13.960 --> 01:30:20.200]   And on how you speak is a useful indicator of your well-being, both emotionally and physically,
[01:30:20.200 --> 01:30:25.920]   so it will monitor your tone to determine if you're feeling positive enough to get through
[01:30:25.920 --> 01:30:26.920]   your day.
[01:30:26.920 --> 01:30:28.960]   I just think it'll be funny.
[01:30:28.960 --> 01:30:32.720]   Every day I'll come in here and I'll just check my tone.
[01:30:32.720 --> 01:30:38.120]   Each morning you'll get a breakdown of your tonal patterns with tips on how to sound warmer.
[01:30:38.120 --> 01:30:41.520]   Yeah, that doesn't sound like wellness to me.
[01:30:41.520 --> 01:30:44.840]   That sounds like a Black Mirror episode.
[01:30:44.840 --> 01:30:48.640]   With my wristband telling me, "Hey, Rob, you can sound more friendly even if you're not
[01:30:48.640 --> 01:30:49.800]   feeling that way."
[01:30:49.800 --> 01:30:52.640]   The other thing you might have noticed is...
[01:30:52.640 --> 01:30:54.960]   So this is Amazon's heel.
[01:30:54.960 --> 01:30:58.400]   We will give you a fifth mover wristband.
[01:30:58.400 --> 01:31:02.360]   There's lots of products that have been out there that are very mature, but we'll give
[01:31:02.360 --> 01:31:05.400]   you a sort of a last-of-the-party wristband.
[01:31:05.400 --> 01:31:09.600]   All you need to do in return is pay us a monthly subscription price, allow us to bug your
[01:31:09.600 --> 01:31:10.600]   wrist.
[01:31:10.600 --> 01:31:14.760]   And you need to send us naked pictures of yourself because the other thing they said
[01:31:14.760 --> 01:31:21.080]   is if you sell, it says to get your body fat percentage, they want you to take full-body
[01:31:21.080 --> 01:31:23.400]   selfie with your smartphone camera.
[01:31:23.400 --> 01:31:27.680]   Now I can't imagine that that's wearing a polar explorer suit, right?
[01:31:27.680 --> 01:31:32.560]   So it sounds like it's saying send a snake itself, he's going to put a bug on your wrist,
[01:31:32.560 --> 01:31:36.120]   and what could possibly go wrong with that?
[01:31:36.120 --> 01:31:37.400]   I have a theory now.
[01:31:37.400 --> 01:31:44.360]   Amazon is going to get more and more outrageous in the things it asks because eventually what
[01:31:44.360 --> 01:31:48.760]   will happen is they'll go, "Oh yeah, yeah, that was way too intrusive."
[01:31:48.760 --> 01:31:51.240]   So you can wear clothes during your naked selfie or something.
[01:31:51.240 --> 01:31:53.920]   They'll back it down a little bit and people go, "See, that's not so bad."
[01:31:53.920 --> 01:31:57.800]   Well, I don't know how they can tell your body fat percentage.
[01:31:57.800 --> 01:32:02.760]   I mean, first of all, from a selfie sounds reasonably speculative to begin with, but
[01:32:02.760 --> 01:32:07.360]   if you're clad in loose-fitting clothes, there's no way they can tell your body fat
[01:32:07.360 --> 01:32:08.360]   percentage.
[01:32:08.360 --> 01:32:11.240]   I can't wait to get this thing because I don't care.
[01:32:11.240 --> 01:32:15.560]   The Amazon, by the way, I've been informed by the chatroom, "Yes, Fitbit charges $8
[01:32:15.560 --> 01:32:16.560]   a month."
[01:32:16.560 --> 01:32:18.240]   So there's a Fitbit plus.
[01:32:18.240 --> 01:32:19.760]   I didn't know that either.
[01:32:19.760 --> 01:32:21.320]   Not having had a Fitbit since it came.
[01:32:21.320 --> 01:32:27.080]   I mean, I got the original Fitbits for, I lost quite a few in the laundry, but I didn't
[01:32:27.080 --> 01:32:28.440]   know you subscribed.
[01:32:28.440 --> 01:32:33.720]   Tone is powered by, this is Amazon, powered by advanced machine learning.
[01:32:33.720 --> 01:32:38.680]   You know, if they just had a blockchain to this, it'd be perfect.
[01:32:38.680 --> 01:32:43.560]   Advanced machine learning based speech processing technologies, the Halo band and Halo app use
[01:32:43.560 --> 01:32:48.680]   voice detection algorithms to pick up speech, remove background noise and optimize battery
[01:32:48.680 --> 01:32:49.680]   life.
[01:32:49.680 --> 01:32:51.920]   I don't know whether through that end.
[01:32:51.920 --> 01:32:52.920]   What?
[01:32:52.920 --> 01:32:59.640]   Our AI analyzes qualities of the customer's voice, pitch, intensity, tempo, and rhythm,
[01:32:59.640 --> 01:33:03.920]   to predict how others would perceive and describe the customer's tone of voice, which creates
[01:33:03.920 --> 01:33:09.200]   a summary you can see, "I cannot wait until my band tells me you sound a little cranky
[01:33:09.200 --> 01:33:10.600]   this morning.
[01:33:10.600 --> 01:33:13.520]   I cannot wait."
[01:33:13.520 --> 01:33:17.640]   You set up a personal voice profile within the Amazon Halo app.
[01:33:17.640 --> 01:33:24.880]   You can turn it off, but once you do that, creating a voice profile is super easy, says
[01:33:24.880 --> 01:33:25.880]   Amazon.
[01:33:25.880 --> 01:33:27.280]   You just read a few quotes.
[01:33:27.280 --> 01:33:32.480]   In this case, some of our favorites from classic books, this trains tone to only analyze your
[01:33:32.480 --> 01:33:34.480]   voice and not those around you.
[01:33:34.480 --> 01:33:38.920]   Once you've opted in by creating your voice profile, tone will run passively and intermittently
[01:33:38.920 --> 01:33:40.920]   in the background, so you don't have to think about it.
[01:33:40.920 --> 01:33:42.920]   Please don't think about it.
[01:33:42.920 --> 01:33:46.880]   Throughout the day, we'll take short samples of your speech and analyze the acoustic characteristics
[01:33:46.880 --> 01:33:50.720]   that represent how you sound to the people you interact with.
[01:33:50.720 --> 01:33:55.040]   I bet you if I do my Scottish accent a lot, it'll think I'm more cranky.
[01:33:55.040 --> 01:33:56.280]   You sound really angry.
[01:33:56.280 --> 01:33:57.280]   I'm scolish.
[01:33:57.280 --> 01:33:58.280]   'Cause I'm angry.
[01:33:58.280 --> 01:34:00.760]   It's bloody cold.
[01:34:00.760 --> 01:34:05.920]   This gives you a simple way to reflect on your interaction and communication throughout
[01:34:05.920 --> 01:34:06.920]   the day.
[01:34:06.920 --> 01:34:09.760]   See, I should have this.
[01:34:09.760 --> 01:34:10.760]   As you continue to use-
[01:34:10.760 --> 01:34:11.760]   It's the same.
[01:34:11.760 --> 01:34:12.760]   I'm angry all the time.
[01:34:12.760 --> 01:34:14.080]   I don't make kids too much.
[01:34:14.080 --> 01:34:15.080]   Yeah.
[01:34:15.080 --> 01:34:16.080]   Yeah.
[01:34:16.080 --> 01:34:17.080]   Well, yeah.
[01:34:17.080 --> 01:34:24.760]   In addition to the default mode, you can also bookmark important conversations by pressing
[01:34:24.760 --> 01:34:26.600]   and recording your-
[01:34:26.600 --> 01:34:27.600]   Ooh.
[01:34:27.600 --> 01:34:29.600]   The Amazon Halo Band.
[01:34:29.600 --> 01:34:33.880]   This allows you to get continuous tone analysis for up to 30 minutes.
[01:34:33.880 --> 01:34:40.400]   So how did I sound when I was talking to you, John, when I was yelling at you?
[01:34:40.400 --> 01:34:42.600]   How did I sound?
[01:34:42.600 --> 01:34:47.080]   This allows you to get- I don't know if I want this.
[01:34:47.080 --> 01:34:50.680]   This helps you practice for a presentation or a big toast.
[01:34:50.680 --> 01:34:54.720]   And again, what does that have to do with wellness?
[01:34:54.720 --> 01:34:56.920]   It sounds like they're selling you on this day to mind.
[01:34:56.920 --> 01:34:59.800]   It's so difficult to attract your health, but then all of a sudden they're talking about
[01:34:59.800 --> 01:35:04.480]   all these ways to make you sound more chipper when you're talking to your boss.
[01:35:04.480 --> 01:35:09.400]   I don't know if I want that kind of coaching every day.
[01:35:09.400 --> 01:35:13.440]   Well, I'm sure you'll be happy to hear Rob read.
[01:35:13.440 --> 01:35:16.960]   The way you communicate is incredibly personal.
[01:35:16.960 --> 01:35:21.760]   That's why we designed Tone with your privacy in mind.
[01:35:21.760 --> 01:35:26.400]   Wasn't there a Bloomberg investigation that showed that the Alexa, they had people-
[01:35:26.400 --> 01:35:27.400]   Yeah, listen to it.
[01:35:27.400 --> 01:35:28.400]   ...to the Alexa conversations.
[01:35:28.400 --> 01:35:29.400]   Yeah.
[01:35:29.400 --> 01:35:32.280]   Why anybody would trust Amazon at this point?
[01:35:32.280 --> 01:35:35.360]   Any kind of company says, "We've designed this with your privacy in mind."
[01:35:35.360 --> 01:35:36.360]   Yeah.
[01:35:36.360 --> 01:35:37.360]   That's how just naked pictures.
[01:35:37.360 --> 01:35:38.840]   Yeah, sentence naked pictures.
[01:35:38.840 --> 01:35:43.800]   If you choose to turn on the feature, your Tone speech samples are processed locally
[01:35:43.800 --> 01:35:47.560]   on the phone and deleted automatically after processing.
[01:35:47.560 --> 01:35:48.560]   Never sent to the cloud.
[01:35:48.560 --> 01:35:49.760]   Nobody ever hears them.
[01:35:49.760 --> 01:35:54.560]   And you have full control of your voice data, including the ability to delete Tone results
[01:35:54.560 --> 01:35:57.160]   in your entire voice profile from the app.
[01:35:57.160 --> 01:35:58.160]   I don't know.
[01:35:58.160 --> 01:35:59.400]   What could possibly go wrong?
[01:35:59.400 --> 01:36:01.480]   I think this is going to be fun.
[01:36:01.480 --> 01:36:04.360]   I can't wait.
[01:36:04.360 --> 01:36:07.360]   I would be nice if I had a little electroshock capability.
[01:36:07.360 --> 01:36:08.360]   I can't wait.
[01:36:08.360 --> 01:36:09.360]   I'd be nice if I had a little electroshock capability.
[01:36:09.360 --> 01:36:12.800]   I could have been kicking you under the table when you see something wrong.
[01:36:12.800 --> 01:36:13.800]   Yeah.
[01:36:13.800 --> 01:36:15.320]   Basically, it's a wife on your wrist.
[01:36:15.320 --> 01:36:16.320]   Yeah.
[01:36:16.320 --> 01:36:17.320]   Perfect.
[01:36:17.320 --> 01:36:18.320]   Perfect.
[01:36:18.320 --> 01:36:22.720]   Let's take a little break.
[01:36:22.720 --> 01:36:25.400]   Actually what I'd like to do right now, John, do we have a...
[01:36:25.400 --> 01:36:26.400]   Yeah.
[01:36:26.400 --> 01:36:33.600]   I thought it'd be kind of fun to run this little educational video about the week gone by.
[01:36:33.600 --> 01:36:34.600]   John?
[01:36:34.600 --> 01:36:38.920]   Mr. Renee Richie, he's got the silver button, boys and girls.
[01:36:38.920 --> 01:36:43.560]   And the real impressive award isn't like the plaque, it's when you can willfully spread
[01:36:43.560 --> 01:36:47.120]   coronavirus disinformation and they won't block your channel or do you mind?
[01:36:47.120 --> 01:36:48.120]   That's when you know.
[01:36:48.120 --> 01:36:49.640]   That's when you know you've got...
[01:36:49.640 --> 01:36:51.440]   You've really succeeded on your YouTube.
[01:36:51.440 --> 01:36:52.440]   You've really made it.
[01:36:52.440 --> 01:36:54.000]   Previously on Twitter.
[01:36:54.000 --> 01:36:55.240]   Hands on Android.
[01:36:55.240 --> 01:36:57.680]   Two different phones, a two different ends of the price spectrum.
[01:36:57.680 --> 01:37:03.200]   I have the Pixel 4A and the Samsung Galaxy Note 20 Ultra, and I take them on a photo walk
[01:37:03.200 --> 01:37:04.200]   to see how they compare.
[01:37:04.200 --> 01:37:05.200]   Tech news weekly.
[01:37:05.200 --> 01:37:08.200]   We have a hands on with the new Samsung Galaxy Z Fold 2.
[01:37:08.200 --> 01:37:13.360]   The biggest difference to me is the fact that the hinge can now open and hold its position
[01:37:13.360 --> 01:37:14.360]   at basically any angle.
[01:37:14.360 --> 01:37:18.280]   And that lets you do some really unique things with this phone where you can sort of prop
[01:37:18.280 --> 01:37:20.600]   it up like a laptop against the table.
[01:37:20.600 --> 01:37:21.600]   Security now.
[01:37:21.600 --> 01:37:30.400]   368,000+ internet users visited this history detection demo website which effectively sucked
[01:37:30.400 --> 01:37:37.840]   the browsing history out of their web browser and were uniquely identified by their histories
[01:37:37.840 --> 01:37:40.800]   in 97% of cases.
[01:37:40.800 --> 01:37:47.640]   In other words, where we steer our browsers is surprisingly unique to it.
[01:37:47.640 --> 01:37:49.400]   Download in place.
[01:37:49.400 --> 01:37:53.320]   I love that.
[01:37:53.320 --> 01:37:59.960]   After Steve Gibson told me about that, I foolishly deleted my browser history and cookies on
[01:37:59.960 --> 01:38:01.440]   this Firefox.
[01:38:01.440 --> 01:38:06.960]   Not knowing that it would sync with all the Firefoxes on all the devices I had.
[01:38:06.960 --> 01:38:11.360]   I had to log in 20 times to everything.
[01:38:11.360 --> 01:38:14.080]   I mean, it was like, "Oh, I got to log in here too?"
[01:38:14.080 --> 01:38:18.400]   And of course, I use a Ube key so everything is super secure.
[01:38:18.400 --> 01:38:19.400]   It's been a nightmare.
[01:38:19.400 --> 01:38:21.720]   Do not delete your cookies.
[01:38:21.720 --> 01:38:22.720]   It's a bad idea.
[01:38:22.720 --> 01:38:23.720]   I showed it.
[01:38:23.720 --> 01:38:25.520]   They brought to you by Barracuda.
[01:38:25.520 --> 01:38:26.680]   This is actually a good idea.
[01:38:26.680 --> 01:38:30.960]   Barracuda is a provider as I think anybody enterprise knows.
[01:38:30.960 --> 01:38:35.440]   The state of the art, the best cloud-enabled enterprise-grade security solutions.
[01:38:35.440 --> 01:38:41.520]   Hardware or software that protects your email, your networks, your data, your applications.
[01:38:41.520 --> 01:38:43.640]   And this is a really timely tool.
[01:38:43.640 --> 01:38:46.800]   The Barracuda Total Email Protection.
[01:38:46.800 --> 01:38:54.240]   Because it turns out that 91% of all cyber attacks, almost all of them, start in an email.
[01:38:54.240 --> 01:38:55.240]   That's spearfishing.
[01:38:55.240 --> 01:38:56.840]   That's ransomware.
[01:38:56.840 --> 01:38:57.840]   But it can be worse.
[01:38:57.840 --> 01:39:00.720]   It can be account takeover, conversation hijacking.
[01:39:00.720 --> 01:39:03.720]   Suddenly, you have a lot of employees working from home.
[01:39:03.720 --> 01:39:08.440]   They're outside the protection of the company network.
[01:39:08.440 --> 01:39:10.480]   They're getting all the email they've ever gotten.
[01:39:10.480 --> 01:39:12.840]   Tons of emails every day.
[01:39:12.840 --> 01:39:16.880]   At least some of them are going to be malicious.
[01:39:16.880 --> 01:39:22.600]   One click on the wrong email could cost your whole company money.
[01:39:22.600 --> 01:39:23.760]   It could cost you customers.
[01:39:23.760 --> 01:39:25.800]   It could cost you your reputation.
[01:39:25.800 --> 01:39:29.120]   You've got to protect your email, especially now.
[01:39:29.120 --> 01:39:31.120]   While employees are working from home.
[01:39:31.120 --> 01:39:36.880]   Barracuda has seen a big spike in the number of coronavirus related spearfishing attacks.
[01:39:36.880 --> 01:39:40.000]   667% since the end of February.
[01:39:40.000 --> 01:39:45.680]   Of course, they impersonate the World Health Organization or the boss.
[01:39:45.680 --> 01:39:47.520]   They utilize domain spoofing.
[01:39:47.520 --> 01:39:54.000]   So they say, "Come to the who.gov" or something like that.
[01:39:54.000 --> 01:39:55.440]   And they say, "Come here.
[01:39:55.440 --> 01:39:58.920]   You'll get information that'll help you stay safe."
[01:39:58.920 --> 01:40:00.920]   But it's a phishing scam.
[01:40:00.920 --> 01:40:02.520]   They're sneaky.
[01:40:02.520 --> 01:40:06.440]   And the thing is they're constantly changing the way they attack.
[01:40:06.440 --> 01:40:09.360]   That's why you need Barracuda total email protection.
[01:40:09.360 --> 01:40:16.200]   It uses AI-based protection that is constantly adapting to the new forms of attack.
[01:40:16.200 --> 01:40:17.200]   It protects you.
[01:40:17.200 --> 01:40:21.000]   It gets spearfishing, account takeover, business email, and compromise.
[01:40:21.000 --> 01:40:22.920]   That can be devastating.
[01:40:22.920 --> 01:40:24.760]   The conversation hijacking.
[01:40:24.760 --> 01:40:26.240]   That's what happened in North Skydro.
[01:40:26.240 --> 01:40:27.280]   Remember that last year?
[01:40:27.280 --> 01:40:30.600]   One of the biggest aluminum producers in the world.
[01:40:30.600 --> 01:40:31.920]   Factories all over the world.
[01:40:31.920 --> 01:40:35.560]   They were shut down for a couple of weeks, $60 million loss.
[01:40:35.560 --> 01:40:42.320]   And it all started with one email from an Italian customer, legit, with a legit attachment
[01:40:42.320 --> 01:40:48.440]   that was sent to a North Skydro employee who was expecting the email, expected the attachment,
[01:40:48.440 --> 01:40:55.960]   except that in between the Italian emailer and the company recipient, malware was injected,
[01:40:55.960 --> 01:41:00.240]   the original attachment was replaced, and then sent on.
[01:41:00.240 --> 01:41:06.040]   That was all it took to bring all of North Skydro down, cost them $60 million.
[01:41:06.040 --> 01:41:07.600]   Barracuda total email prediction.
[01:41:07.600 --> 01:41:12.760]   It's an all-in-one email security, backup, and archiving solution.
[01:41:12.760 --> 01:41:16.120]   Besides the scanning, you get an automated incident response.
[01:41:16.120 --> 01:41:19.800]   So when you see an attack, you can quickly and efficiently address it.
[01:41:19.800 --> 01:41:21.200]   Speed is really vital.
[01:41:21.200 --> 01:41:25.800]   As you know, the faster you respond, the less damage you're going to suffer.
[01:41:25.800 --> 01:41:27.840]   They even have security awareness training.
[01:41:27.840 --> 01:41:31.680]   So your employees can be trained, you know, they're the first line of defense.
[01:41:31.680 --> 01:41:35.920]   They can be trained into what to look for, what not to click on.
[01:41:35.920 --> 01:41:40.320]   Uncover the threats hiding in your inbox right now with a free scan.
[01:41:40.320 --> 01:41:42.280]   Look, here's the deal.
[01:41:42.280 --> 01:41:48.280]   Right now, if you go to barracuda.com/twit, you can get a free, secure email threat scan
[01:41:48.280 --> 01:41:51.080]   of your Office 365 account.
[01:41:51.080 --> 01:41:54.400]   Now I know a lot of you are going to say, "I don't want to know.
[01:41:54.400 --> 01:41:56.280]   I don't want to see what's in there."
[01:41:56.280 --> 01:41:57.640]   Yes, you do.
[01:41:57.640 --> 01:41:58.920]   Trust me.
[01:41:58.920 --> 01:42:01.280]   Go right now to barracuda.com/twit.
[01:42:01.280 --> 01:42:03.200]   You can just do it quietly.
[01:42:03.200 --> 01:42:05.080]   See what you find.
[01:42:05.080 --> 01:42:09.800]   I think the answer will surprise you and it will really encourage you to do the right
[01:42:09.800 --> 01:42:10.800]   thing.
[01:42:10.800 --> 01:42:12.320]   Protect yourself with barracuda.
[01:42:12.320 --> 01:42:19.600]   Barracuda.com/twitbarracuda is your journey, your journey secured.
[01:42:19.600 --> 01:42:22.440]   Thank you Barracuda for supporting the show.
[01:42:22.440 --> 01:42:28.760]   And thank you for supporting us by using that special address, barracuda.com/twit.
[01:42:28.760 --> 01:42:35.960]   I thought it was interesting to, they were showing on the All About Android show, the
[01:42:35.960 --> 01:42:42.000]   Z Fold opening and then staying in position like a little laptop.
[01:42:42.000 --> 01:42:43.600]   And actually the duo does that, right?
[01:42:43.600 --> 01:42:44.600]   Yeah, absolutely.
[01:42:44.600 --> 01:42:45.760]   It's got a great hinge for that.
[01:42:45.760 --> 01:42:49.200]   So I thought you could pass this along as such and next time you see them in the coffee,
[01:42:49.200 --> 01:42:52.000]   shop or whatever.
[01:42:52.000 --> 01:42:53.400]   It's like a little smurf computer.
[01:42:53.400 --> 01:42:56.560]   So I thought they could call it the Microsoft Smurfis.
[01:42:56.560 --> 01:42:59.120]   Don't you think?
[01:42:59.120 --> 01:43:00.120]   I like it.
[01:43:00.120 --> 01:43:01.120]   Yeah, just tell Satcha.
[01:43:01.120 --> 01:43:02.560]   I won't know no charges.
[01:43:02.560 --> 01:43:03.560]   It's my gift.
[01:43:03.560 --> 01:43:04.720]   Painos would be the one that was that.
[01:43:04.720 --> 01:43:05.720]   Oh yeah, tell Paris.
[01:43:05.720 --> 01:43:06.720]   Yeah, tell Paris.
[01:43:06.720 --> 01:43:07.720]   Yeah.
[01:43:07.720 --> 01:43:09.080]   Wow, I was surprised.
[01:43:09.080 --> 01:43:13.920]   How long ago was it Edward Snowden kind of took his life at his hands revealing what the
[01:43:13.920 --> 01:43:19.840]   NSA was up to and the NSA surveillance.
[01:43:19.840 --> 01:43:29.560]   A court, the US Court of Appeals has now ruled that the mass surveillance of Americans'
[01:43:29.560 --> 01:43:34.840]   telephone records, which we learned about, thanks to Edward Snowden seven years ago,
[01:43:34.840 --> 01:43:37.200]   was illegal.
[01:43:37.200 --> 01:43:45.200]   And that intelligence leaders who defended the program lied, lied.
[01:43:45.200 --> 01:43:49.360]   Edward Snowden released this statement.
[01:43:49.360 --> 01:43:56.600]   I never imagined I would live to see our courts condemn the NSA's activities as unlawful.
[01:43:56.600 --> 01:44:03.520]   And in the same ruling, credit me for exposing them yet that day has arrived.
[01:44:03.520 --> 01:44:10.480]   The court said there is no evidence of it saving any lives or preventing any terrorist
[01:44:10.480 --> 01:44:11.880]   attacks.
[01:44:11.880 --> 01:44:15.840]   He said that the claims that the surveillance program had played a crucial role in fighting
[01:44:15.840 --> 01:44:22.080]   domestic terrorism were inconsistent with the contents of the classified records that
[01:44:22.080 --> 01:44:26.640]   the program had violated the FISA Act.
[01:44:26.640 --> 01:44:29.160]   But the ruling doesn't do anything.
[01:44:29.160 --> 01:44:36.000]   It will not affect the convictions in 2013 that they claim were based on that information
[01:44:36.000 --> 01:44:37.000]   gathering.
[01:44:37.000 --> 01:44:41.120]   The ACLU said today is ruling is a victory for our privacy rights.
[01:44:41.120 --> 01:44:42.120]   So it's what we all knew.
[01:44:42.120 --> 01:44:48.440]   The NSA's bulk collection of American phone records was unconstitutional, but no harm,
[01:44:48.440 --> 01:44:49.760]   no foul.
[01:44:49.760 --> 01:44:54.240]   We'll just move along.
[01:44:54.240 --> 01:45:04.520]   Meanwhile, in a more recent court ruling, I guess part of the FISA Act is there is a regular
[01:45:04.520 --> 01:45:12.680]   yearly court review of how information is collected and whether it's lawful and all
[01:45:12.680 --> 01:45:14.280]   of that.
[01:45:14.280 --> 01:45:20.680]   We didn't learn about this when it came out, but in December after the review, a court
[01:45:20.680 --> 01:45:34.040]   ruling said that the FBI and others, let me read the statement.
[01:45:34.040 --> 01:45:39.800]   In the ruling from December, which we just learned about, Judge Boasburg, who was the
[01:45:39.800 --> 01:45:46.600]   chief judge of the FISA court, approved the new annual rules, but scolded the FBI over
[01:45:46.600 --> 01:45:52.880]   many instances in which its analysts had violated the previous rules, including requirements
[01:45:52.880 --> 01:45:58.200]   as searches of the repository have a foreign intelligence or criminal purpose.
[01:45:58.200 --> 01:46:02.320]   The judge wrote, "It must be noted that there still appear to be widespread violations
[01:46:02.320 --> 01:46:06.200]   of the querying standard by the FBI.
[01:46:06.200 --> 01:46:11.360]   For instance, in August of last year, the FBI made a query for information using the
[01:46:11.360 --> 01:46:18.400]   identifiers of 16,000 people, even though only seven of them, seven of them, had ties
[01:46:18.400 --> 01:46:20.560]   to an investigation.
[01:46:20.560 --> 01:46:25.440]   The FBI said the entire search meant the standard of being reasonably likely to retrieve foreign
[01:46:25.440 --> 01:46:29.280]   intelligence information or evidence of a crime, but the judge said that position was
[01:46:29.280 --> 01:46:39.920]   unsupportable and said, "All but the seven were broad, suspicionless queries, fishing
[01:46:39.920 --> 01:46:42.000]   expeditions."
[01:46:42.000 --> 01:46:44.840]   So even though the judge said, "I'm unhappy you did it wrong.
[01:46:44.840 --> 01:46:52.320]   It's illegal," he said, "Okay, but go ahead."
[01:46:52.320 --> 01:46:59.480]   So a couple of court rulings that accomplished nothing but at least vindicate Edward Snowden
[01:46:59.480 --> 01:47:08.160]   and those of us who said that there's no excuse for warrantless surveillance.
[01:47:08.160 --> 01:47:11.640]   Even the oversight isn't doing much to stop it.
[01:47:11.640 --> 01:47:19.960]   I talked to a source of mine about this year last July, and she said, just casually, she
[01:47:19.960 --> 01:47:25.240]   said, "That stuff was really useful, but we just use Palantir now."
[01:47:25.240 --> 01:47:29.480]   Yeah, great.
[01:47:29.480 --> 01:47:30.480]   We don't need it anymore.
[01:47:30.480 --> 01:47:31.480]   I'm not trying anymore.
[01:47:31.480 --> 01:47:32.480]   What aboutism?
[01:47:32.480 --> 01:47:33.480]   It's just bad.
[01:47:33.480 --> 01:47:39.080]   No, no, but we don't need it anymore because thanks to Peter Thiel and Alex Carp, we got
[01:47:39.080 --> 01:47:43.240]   all that information.
[01:47:43.240 --> 01:47:45.240]   All right.
[01:47:45.240 --> 01:47:49.680]   What else?
[01:47:49.680 --> 01:47:57.880]   I got an urgent email last week from Corey Doctorow, which I didn't respond too fast enough.
[01:47:57.880 --> 01:48:05.360]   The EFF was trying to get the California Assembly to hear a bill that would expand broadband
[01:48:05.360 --> 01:48:07.640]   to the people of California.
[01:48:07.640 --> 01:48:18.000]   The Assembly, they hoped before adjourning would hear SB 1130, but they adjurant.
[01:48:18.000 --> 01:48:20.040]   They didn't want to hear it.
[01:48:20.040 --> 01:48:22.640]   They refused, in fact, to hear it.
[01:48:22.640 --> 01:48:25.880]   The EFF is a little upset.
[01:48:25.880 --> 01:48:31.040]   Senator Lena Gonzalez, who had created the bill and really built a coalition for it,
[01:48:31.040 --> 01:48:36.520]   even had the support of the governor of California, Gavin Newsom, the California Senate.
[01:48:36.520 --> 01:48:39.960]   She said, "A press release on the bill during this crisis, children," and this is true.
[01:48:39.960 --> 01:48:40.960]   I've seen pictures of this.
[01:48:40.960 --> 01:48:47.120]   They're sitting outside Taco Bell so they can access the internet to do their homework,
[01:48:47.120 --> 01:48:52.440]   but the Assembly chose to kill SB 1130, the only viable solution in the state legislature,
[01:48:52.440 --> 01:48:57.520]   to help close the digital divide and provide reliable broadband infrastructure for California
[01:48:57.520 --> 01:49:00.960]   students, parents, educators, and first responders.
[01:49:00.960 --> 01:49:07.280]   The Assembly put in poison pill amendments that the big telecom industries wanted in
[01:49:07.280 --> 01:49:08.920]   there.
[01:49:08.920 --> 01:49:12.240]   There were hundreds of phone calls and letters of support.
[01:49:12.240 --> 01:49:17.680]   There was an EFF petition, EFF co-sponsored the bill with common sense media.
[01:49:17.680 --> 01:49:19.680]   So they're regrouping.
[01:49:19.680 --> 01:49:25.920]   And Corey asked me to help by signing the petition, but it didn't work.
[01:49:25.920 --> 01:49:32.320]   So even though the Senate passed the bill 30 to 9 in June, the Assembly just turned its
[01:49:32.320 --> 01:49:33.580]   back on.
[01:49:33.580 --> 01:49:42.200]   The idea was to stabilize and expand California's internet infrastructure, spend over half
[01:49:42.200 --> 01:49:47.080]   a billion dollars on broadband infrastructure as quickly as possible, enable local governments
[01:49:47.080 --> 01:49:53.680]   to finance with bonds a billion dollars for long-term low interest rates to support school
[01:49:53.680 --> 01:50:00.040]   districts, build broadband networks at a minimum of 100 megabits per second download with an
[01:50:00.040 --> 01:50:02.400]   emphasis on scalability.
[01:50:02.400 --> 01:50:05.400]   So they wouldn't have to finance new construction later.
[01:50:05.400 --> 01:50:09.400]   Support low-income neighborhoods that lacked broadband entirely.
[01:50:09.400 --> 01:50:13.640]   Availability for states of support to secure every rural Californian.
[01:50:13.640 --> 01:50:15.720]   It's internet access.
[01:50:15.720 --> 01:50:18.240]   So Corey talked a little bit about this last week.
[01:50:18.240 --> 01:50:19.640]   I just thought I'd give you an update.
[01:50:19.640 --> 01:50:27.360]   Unfortunately, we weren't able to get that approved in the Assembly.
[01:50:27.360 --> 01:50:30.760]   Do you, any of you use cable card?
[01:50:30.760 --> 01:50:37.000]   This is kind of a special narrow bit, but I want to mention it because I use cable card.
[01:50:37.000 --> 01:50:44.400]   FCC used to require cable companies to provide you with a PC's MCIA card, credit card size
[01:50:44.400 --> 01:50:49.440]   device that you could put in your Tivo, primarily as Tivo's, but other devices to make them
[01:50:49.440 --> 01:50:50.440]   cable boxes.
[01:50:50.440 --> 01:50:52.520]   So you didn't have to use the crappy cable boxes.
[01:50:52.520 --> 01:50:54.080]   The cable company offered.
[01:50:54.080 --> 01:50:55.520]   Cable companies hated it.
[01:50:55.520 --> 01:50:56.520]   They would hide it.
[01:50:56.520 --> 01:51:01.640]   I remember when I went to get cable cards, it was in the, the guy said, "Oh, let me see
[01:51:01.640 --> 01:51:02.640]   if we got any."
[01:51:02.640 --> 01:51:07.800]   You know, like digs through the back, dusty shelves, finally finds one.
[01:51:07.800 --> 01:51:14.600]   According to the FCC, there are about half a million Americans who use cable card, but
[01:51:14.600 --> 01:51:21.520]   they have changed the rules and cable companies no longer need to offer it.
[01:51:21.520 --> 01:51:22.520]   So bye-bye cable.
[01:51:22.520 --> 01:51:25.080]   My phone run is going to be useless now, you're saying.
[01:51:25.080 --> 01:51:26.080]   Exactly.
[01:51:26.080 --> 01:51:31.280]   Now, the question is, in the FCC said, "Don't worry, even if they're not required, they'll
[01:51:31.280 --> 01:51:33.400]   still do it for market reasons.
[01:51:33.400 --> 01:51:36.040]   I don't think that's the case."
[01:51:36.040 --> 01:51:39.160]   Cable companies have always hated cable card.
[01:51:39.160 --> 01:51:44.640]   There, Silicon Dust, makers of the HD Home Run, had promised, and I was waiting for this,
[01:51:44.640 --> 01:51:49.960]   this prime, HD Home Run Prime that would have six tuners in it.
[01:51:49.960 --> 01:51:54.840]   So you put a cable card in there, you could DVR up to six shows at the same time, put
[01:51:54.840 --> 01:51:58.000]   them on the network so you could watch them anywhere in the house.
[01:51:58.000 --> 01:52:03.120]   This is a cord cutter's dream, actually wouldn't be a cord cutter, you still need a cable service.
[01:52:03.120 --> 01:52:05.720]   But I suspect that that's why that's been put on hold.
[01:52:05.720 --> 01:52:10.360]   Well, I mean, plus you guys remember YouTube TV does this already, where you could go record
[01:52:10.360 --> 01:52:13.400]   whatever you want on any channel for, and they'll stay there for months.
[01:52:13.400 --> 01:52:16.600]   So like, you know, I'm an Apple TV.
[01:52:16.600 --> 01:52:18.320]   Only a month, but still, yeah.
[01:52:18.320 --> 01:52:19.320]   Yeah.
[01:52:19.320 --> 01:52:20.320]   Yeah.
[01:52:20.320 --> 01:52:21.920]   I think cable card is the end of the line for the cable card.
[01:52:21.920 --> 01:52:25.480]   I'm disappointed because I have four T-vos.
[01:52:25.480 --> 01:52:28.360]   It will be obsolete as soon as cable cards.
[01:52:28.360 --> 01:52:30.880]   I have four cable cards in my house.
[01:52:30.880 --> 01:52:32.560]   I was waiting for this HD prime.
[01:52:32.560 --> 01:52:36.120]   I would get rid of all but one and then it would be the solution, but I don't think
[01:52:36.120 --> 01:52:38.280]   that's going to happen.
[01:52:38.280 --> 01:52:42.960]   Maybe the Biden campaign doesn't understand spending on YouTube, but they know animal
[01:52:42.960 --> 01:52:44.680]   crossing.
[01:52:44.680 --> 01:52:55.000]   You can now get Biden Harris yard signs for your animal crossing new horizons.
[01:52:55.000 --> 01:52:56.000]   Game.
[01:52:56.000 --> 01:53:01.720]   There's something Trump doesn't have.
[01:53:01.720 --> 01:53:02.720]   That's pretty funny.
[01:53:02.720 --> 01:53:04.280]   It's a demographic for animal crossing.
[01:53:04.280 --> 01:53:06.320]   I don't think they're voters.
[01:53:06.320 --> 01:53:07.320]   Maybe they are.
[01:53:07.320 --> 01:53:09.600]   Actually, women, I take it back.
[01:53:09.600 --> 01:53:12.560]   Doesn't I think Christina Warren lives in animal crossing.
[01:53:12.560 --> 01:53:15.880]   I played animal farm like crazy the first six or eight weeks.
[01:53:15.880 --> 01:53:21.240]   I think it's an incredibly, terribly complex game hidden under cutesy animal.
[01:53:21.240 --> 01:53:22.240]   Yeah.
[01:53:22.240 --> 01:53:27.080]   I think it would play it, but I can't get my switch away from Michael.
[01:53:27.080 --> 01:53:28.600]   He has it and plays it.
[01:53:28.600 --> 01:53:29.600]   I said, "Well, how is it?"
[01:53:29.600 --> 01:53:31.320]   He said, "You wouldn't like it."
[01:53:31.320 --> 01:53:33.320]   He said, "John's Ion."
[01:53:33.320 --> 01:53:38.640]   That's who else plays it.
[01:53:38.640 --> 01:53:41.520]   I think this is going to be interesting too, not just YouTube, but I think they're going
[01:53:41.520 --> 01:53:46.920]   to be not just YouTube and Pinterest, but I think we're going to see streamed rallies
[01:53:46.920 --> 01:53:53.320]   in these places in Minecraft and maybe in animal crossing.
[01:53:53.320 --> 01:54:02.680]   Players can access the designs in game by scanning the QR codes on the Biden Harris website.
[01:54:02.680 --> 01:54:03.680]   Pick your sign.
[01:54:03.680 --> 01:54:06.360]   Here's one that says, "Joe."
[01:54:06.360 --> 01:54:09.560]   I like that with a little rainbow.
[01:54:09.560 --> 01:54:11.960]   Team Joe.
[01:54:11.960 --> 01:54:12.960]   I don't know.
[01:54:12.960 --> 01:54:13.960]   Is that clearly Joe?
[01:54:13.960 --> 01:54:16.400]   Three sunglasses, red, white and blue.
[01:54:16.400 --> 01:54:19.320]   He's famous for his aviator glasses.
[01:54:19.320 --> 01:54:20.320]   That's kind of...
[01:54:20.320 --> 01:54:23.080]   Yeah, that doesn't scream it to me, but I guess I see the reference.
[01:54:23.080 --> 01:54:24.080]   It's subtle.
[01:54:24.080 --> 01:54:25.080]   Yeah.
[01:54:25.080 --> 01:54:26.240]   Then Biden Harris.
[01:54:26.240 --> 01:54:27.320]   That's funny.
[01:54:27.320 --> 01:54:34.680]   As of late July, they had no director of cybersecurity, but they've launched this by the first week
[01:54:34.680 --> 01:54:36.640]   of September.
[01:54:36.640 --> 01:54:38.640]   Just shows you.
[01:54:38.640 --> 01:54:39.640]   Just shows you.
[01:54:39.640 --> 01:54:41.800]   How quickly you can get this stuff running.
[01:54:41.800 --> 01:54:46.080]   Mr. says, "It's no longer going to allow ads and election-related content in searches.
[01:54:46.080 --> 01:54:47.840]   It banned political ads in 2018."
[01:54:47.840 --> 01:54:53.960]   I wish Facebook would do that.
[01:54:53.960 --> 01:54:54.960]   Let's see.
[01:54:54.960 --> 01:54:57.080]   Oh, here's a story.
[01:54:57.080 --> 01:55:05.320]   Today I learned my Fisher Price gamepad that my kid plays with understands the Konami
[01:55:05.320 --> 01:55:10.400]   code.
[01:55:10.400 --> 01:55:13.320]   Up, up, down, down, left, right, left, right.
[01:55:13.320 --> 01:55:14.320]   BA.
[01:55:14.320 --> 01:55:15.320]   What do you get?
[01:55:15.320 --> 01:55:16.320]   What do you get?
[01:55:16.320 --> 01:55:17.320]   What happens?
[01:55:17.320 --> 01:55:18.320]   We've got start.
[01:55:18.320 --> 01:55:21.000]   Oh, there's no start button.
[01:55:21.000 --> 01:55:24.760]   But nevertheless, the controller erupted into sound.
[01:55:24.760 --> 01:55:29.600]   A Mario jump, a Mario mushroom power, up a Mario coin and declared you win.
[01:55:29.600 --> 01:55:32.320]   Oh, I got to play this.
[01:55:32.320 --> 01:55:33.320]   Can you hear it?
[01:55:33.320 --> 01:55:34.320]   Let me turn it on.
[01:55:34.320 --> 01:55:36.960]   Hey, it's in your Fisher Price video game controller thing for the kiddies.
[01:55:36.960 --> 01:55:37.960]   If you talk Scottish...
[01:55:37.960 --> 01:55:42.960]   I'm just going to press buttons and say stuff.
[01:55:42.960 --> 01:55:49.080]   And then it says numbers and colors and stuff.
[01:55:49.080 --> 01:55:55.080]   Don't give this to your kids, Lou.
[01:55:55.080 --> 01:56:01.840]   Amazingly, if you put in the Konami code, so it's up, up, down, down, left, right, left,
[01:56:01.840 --> 01:56:02.840]   right, right.
[01:56:02.840 --> 01:56:03.840]   BA.
[01:56:03.840 --> 01:56:10.840]   You beat the game.
[01:56:10.840 --> 01:56:12.840]   Awesome.
[01:56:12.840 --> 01:56:16.840]   Oh, Lardy, Lardy.
[01:56:16.840 --> 01:56:19.840]   Ah, it's a conchra.
[01:56:19.840 --> 01:56:20.840]   Let me see.
[01:56:20.840 --> 01:56:21.840]   I think I've...
[01:56:21.840 --> 01:56:22.840]   Can we have...
[01:56:22.840 --> 01:56:24.960]   As possibly we've run out of stories, I feel like there was more I wanted to talk about.
[01:56:24.960 --> 01:56:29.680]   But I think we've run out of stories.
[01:56:29.680 --> 01:56:34.520]   One thing I want to mention from one of the stories is there was a minor news from Google
[01:56:34.520 --> 01:56:37.560]   Maps, but it came directly from Google Maps.
[01:56:37.560 --> 01:56:42.480]   And they shared the intriguing statistic that a billion kilometers a day...
[01:56:42.480 --> 01:56:45.440]   I'm sorry, no, a billion kilometers per day, yeah.
[01:56:45.440 --> 01:56:48.040]   Are traveled by Google Maps users?
[01:56:48.040 --> 01:56:51.040]   And I did the back of the envelope math on that.
[01:56:51.040 --> 01:56:55.120]   And 24 years of that usage is a light year.
[01:56:55.120 --> 01:57:00.940]   So I'm always pleased when Cubans are doing things that can be measured only in light
[01:57:00.940 --> 01:57:01.940]   years.
[01:57:01.940 --> 01:57:02.940]   Thank you for doing the math.
[01:57:02.940 --> 01:57:03.940]   That's all I got.
[01:57:03.940 --> 01:57:04.940]   It's all I got.
[01:57:04.940 --> 01:57:06.680]   How long before we get a parsec?
[01:57:06.680 --> 01:57:07.680]   Can you figure that out while I...
[01:57:07.680 --> 01:57:10.080]   Yoursec is like three point something light years, right?
[01:57:10.080 --> 01:57:11.880]   Yeah, so it should be easy to think.
[01:57:11.880 --> 01:57:14.480]   So it's probably 80 years or so.
[01:57:14.480 --> 01:57:22.040]   But if we all start using Google Maps more, we will get to a parsec faster.
[01:57:22.040 --> 01:57:24.280]   The chat room has a question for the panel.
[01:57:24.280 --> 01:57:27.080]   We'll ask that in a minute, but first a word from the last pass.
[01:57:27.080 --> 01:57:29.400]   You know, we're here at the last pass.
[01:57:29.400 --> 01:57:31.800]   Studios, we love last pass.
[01:57:31.800 --> 01:57:35.160]   And I tell you what, so do IT professionals and security professionals.
[01:57:35.160 --> 01:57:41.000]   They surveyed 719 security professionals across a range of industries from financial
[01:57:41.000 --> 01:57:43.400]   services to IT to retail.
[01:57:43.400 --> 01:57:44.400]   Almost all of them.
[01:57:44.400 --> 01:57:51.280]   82% said their business had been exposed to a risk due to poor identity and access management.
[01:57:51.280 --> 01:57:54.480]   That's why you use last pass.
[01:57:54.480 --> 01:57:57.960]   Last pass makes sure that the right people are using the right resources at the right
[01:57:57.960 --> 01:58:00.080]   time from the right place.
[01:58:00.080 --> 01:58:03.160]   You get IT oversight and you know what's amazing?
[01:58:03.160 --> 01:58:05.200]   It does something I thought was impossible.
[01:58:05.200 --> 01:58:08.320]   It makes you more secure and it's more convenient.
[01:58:08.320 --> 01:58:10.000]   What?
[01:58:10.000 --> 01:58:12.720]   Usually security and convenience are a trade off.
[01:58:12.720 --> 01:58:19.520]   But last pass does everything to make it easier to create long, strong passwords.
[01:58:19.520 --> 01:58:25.760]   Usually random, storm safely, call them up automatically, automatically fill them in
[01:58:25.760 --> 01:58:28.560]   on web pages and on apps.
[01:58:28.560 --> 01:58:32.600]   Last pass has features for a last pass enterprise like single sign on, which is even easier
[01:58:32.600 --> 01:58:36.400]   to pass where it's just click a button on your phone that says, "Yeah, that's me."
[01:58:36.400 --> 01:58:38.840]   And you're logged in.
[01:58:38.840 --> 01:58:42.680]   Last pass makes it easy and safe for employees to share logins too.
[01:58:42.680 --> 01:58:45.440]   Not write them on posted notes or send them by a text or email.
[01:58:45.440 --> 01:58:46.440]   No, no, no.
[01:58:46.440 --> 01:58:52.040]   And then directly through last pass through their secure encrypted transmission, that
[01:58:52.040 --> 01:58:54.800]   is what you want.
[01:58:54.800 --> 01:59:00.080]   Enterprise password management ensures oversight of shadow IT and enforceable policies across
[01:59:00.080 --> 01:59:02.600]   all password protective accounts.
[01:59:02.600 --> 01:59:05.760]   Multi-factor authentication too, not just two-factor.
[01:59:05.760 --> 01:59:07.000]   Sure two-factor.
[01:59:07.000 --> 01:59:13.280]   You got biometrics, your fingerprint or face scan, but you also have contextual factors
[01:59:13.280 --> 01:59:16.200]   like location or IP address.
[01:59:16.200 --> 01:59:21.160]   These really help make sure the right people are accessing the right resources.
[01:59:21.160 --> 01:59:27.520]   And of course, we know because Steve Gibson checked it out for us 10 years ago now.
[01:59:27.520 --> 01:59:29.960]   That's how long we've been using last pass.
[01:59:29.960 --> 01:59:34.000]   We know that it uses the best AES-256 bit encryption.
[01:59:34.000 --> 01:59:41.720]   It uses the PBK DF2, the key derivative function to make sure brute forcing is virtually impossible.
[01:59:41.720 --> 01:59:45.440]   Salted hashes to ensure complete security in the cloud.
[01:59:45.440 --> 01:59:49.480]   Your data is never decrypted anywhere but on device.
[01:59:49.480 --> 01:59:52.200]   And your password is never transmitted to last pass.
[01:59:52.200 --> 01:59:53.960]   They don't have access to it.
[01:59:53.960 --> 01:59:57.760]   Only you have access to your data and your password.
[01:59:57.760 --> 01:59:59.000]   That's the way it should be done.
[01:59:59.000 --> 02:00:00.400]   Last pass does it all right.
[02:00:00.400 --> 02:00:02.800]   That's why it's the number one password manager.
[02:00:02.800 --> 02:00:08.640]   Let last pass securely manage your users identity and let your employees work more efficiently
[02:00:08.640 --> 02:00:11.960]   without making your business vulnerable to cyber threats.
[02:00:11.960 --> 02:00:15.880]   If you're in enterprise and you're not using last pass, you've got to do it.
[02:00:15.880 --> 02:00:17.120]   We've been using it for a long time.
[02:00:17.120 --> 02:00:23.120]   Makes it easy to add new employees automatically and take employees out.
[02:00:23.120 --> 02:00:25.840]   It makes it very easy to control who access is what when.
[02:00:25.840 --> 02:00:29.000]   We have folders for different business divisions.
[02:00:29.000 --> 02:00:32.480]   So the business office gets their passwords all in one place.
[02:00:32.480 --> 02:00:35.080]   The ops guys get their passwords in one place.
[02:00:35.080 --> 02:00:38.040]   The studio technicians get their passwords in one place.
[02:00:38.040 --> 02:00:40.360]   So it's so much better.
[02:00:40.360 --> 02:00:43.120]   That any other method of managing it.
[02:00:43.120 --> 02:00:44.760]   Lastpass.com/twit.
[02:00:44.760 --> 02:00:45.760]   We love it.
[02:00:45.760 --> 02:00:46.760]   You will too.
[02:00:46.760 --> 02:00:53.920]   Lastpass.com/twit because stronger security can't wait.
[02:00:53.920 --> 02:00:55.360]   Can't wait.
[02:00:55.360 --> 02:00:58.320]   So I didn't mention the Nvidia announcement.
[02:00:58.320 --> 02:01:01.720]   We talked about it a little bit before the show because you built blue.
[02:01:01.720 --> 02:01:02.720]   Maybe we talked about during the show.
[02:01:02.720 --> 02:01:06.280]   You built your new machine.
[02:01:06.280 --> 02:01:10.360]   You said you put a kind of a replaceable card in there.
[02:01:10.360 --> 02:01:11.360]   Gap filler.
[02:01:11.360 --> 02:01:12.360]   Gap filler.
[02:01:12.360 --> 02:01:18.480]   We still don't know what the new RTX 30 series GPUs will cost, do we?
[02:01:18.480 --> 02:01:23.160]   Well, they're saying that the 3090, which is the high end, it comes out on the 24th,
[02:01:23.160 --> 02:01:24.160]   will be about 1499.
[02:01:24.160 --> 02:01:27.720]   And if you think about it, it's high.
[02:01:27.720 --> 02:01:30.840]   But if you think about it, if you compare that to some of the previous cars like the
[02:01:30.840 --> 02:01:34.440]   2080 Ti, that's in the same ballpark.
[02:01:34.440 --> 02:01:36.480]   In fact, that's cheaper than the 2080 Ti.
[02:01:36.480 --> 02:01:38.560]   It's got more memory and it's faster.
[02:01:38.560 --> 02:01:43.720]   And so it almost, it almost sunsets those, those cars, which is why you're seeing buyout
[02:01:43.720 --> 02:01:48.080]   prices in places like Best Buy and, and Newegg and all those places.
[02:01:48.080 --> 02:01:51.080]   Because they know that these other cars are just going to completely eclipse the rest
[02:01:51.080 --> 02:01:52.080]   of them.
[02:01:52.080 --> 02:01:56.440]   Why is Nvidia being so aggressive about this?
[02:01:56.440 --> 02:01:58.360]   Well they had the technology.
[02:01:58.360 --> 02:01:59.360]   They've had it for awhile.
[02:01:59.360 --> 02:02:01.040]   Ray tracing and things like that, right?
[02:02:01.040 --> 02:02:06.760]   They've had it for awhile and they just know that with a lot more remote workers and the
[02:02:06.760 --> 02:02:12.640]   need for every Bitcoin, other of these other things coming, becoming more and more prevalent,
[02:02:12.640 --> 02:02:16.080]   they didn't think that it would be a bad time to release these things.
[02:02:16.080 --> 02:02:20.480]   And so, and they knew that people were going to start buying more machines because of the
[02:02:20.480 --> 02:02:23.240]   stretch out of the pandemic and so on.
[02:02:23.240 --> 02:02:24.800]   And so they just decided to do it.
[02:02:24.800 --> 02:02:30.000]   This seems like, but I'm surprised at the low, the pricing, because I've been doing a
[02:02:30.000 --> 02:02:32.880]   lot of research on graphics cards and they get up there.
[02:02:32.880 --> 02:02:38.480]   Like I think Newegg selling a 20t ti a couple weeks ago for like, for like 1800 bucks.
[02:02:38.480 --> 02:02:39.480]   Wow.
[02:02:39.480 --> 02:02:40.480]   So, yeah.
[02:02:40.480 --> 02:02:42.240]   So 14 is a heck of a deal.
[02:02:42.240 --> 02:02:43.240]   Yeah.
[02:02:43.240 --> 02:02:44.240]   Yeah.
[02:02:44.240 --> 02:02:47.360]   And are they worried about AMD's big Navi?
[02:02:47.360 --> 02:02:52.120]   Is that maybe part of the reason to get the, get the jump on that?
[02:02:52.120 --> 02:02:53.120]   You think?
[02:02:53.120 --> 02:02:54.120]   Oh yeah, obviously.
[02:02:54.120 --> 02:02:55.120]   I mean, they, they're always fighting.
[02:02:55.120 --> 02:02:56.120]   Should they be worried?
[02:02:56.120 --> 02:03:04.960]   I mean, it feels like Nvidia with ray tracing lapped, uh, Radeon, right?
[02:03:04.960 --> 02:03:05.960]   Right.
[02:03:05.960 --> 02:03:06.960]   Right.
[02:03:06.960 --> 02:03:07.960]   Right.
[02:03:07.960 --> 02:03:08.960]   That's true.
[02:03:08.960 --> 02:03:12.440]   And there's a lot more, uh, there's a lot more adoption of ray tracing.
[02:03:12.440 --> 02:03:14.600]   So I think that's one of the key players too.
[02:03:14.600 --> 02:03:19.240]   And so now you put out the card that can support it and it'll just make everything more smooth
[02:03:19.240 --> 02:03:21.520]   and, and, and, and more realistic.
[02:03:21.520 --> 02:03:23.960]   I think that's where they're, that's where AMD's wearing.
[02:03:23.960 --> 02:03:24.960]   Yeah.
[02:03:24.960 --> 02:03:30.160]   Um, not that consults matter as much anymore, but ray tracing will be in each new console.
[02:03:30.160 --> 02:03:31.160]   Right.
[02:03:31.160 --> 02:03:32.320]   I know I'm excited.
[02:03:32.320 --> 02:03:37.920]   Although I think the real thing to be excited about, especially within video, they've had
[02:03:37.920 --> 02:03:39.640]   some challenges, but it's streaming.
[02:03:39.640 --> 02:03:44.920]   I mean, Microsoft too, with, with, um, what's coming up with X cloud and game pass, but,
[02:03:44.920 --> 02:03:50.320]   um, Nvidia's, I got rid of my graphics card and got rid of my PC.
[02:03:50.320 --> 02:03:55.640]   I'm just down to an old Mac mini now because I stream everything using, um, G force now.
[02:03:55.640 --> 02:03:56.640]   Right.
[02:03:56.640 --> 02:03:57.960]   Yeah, that is pretty exciting.
[02:03:57.960 --> 02:04:03.000]   Although if you're an Apple user, you might be a little nervous about iOS's policy towards,
[02:04:03.000 --> 02:04:10.000]   uh, Epic with Fortnite because obviously X cloud, somebody said X cloud is running on
[02:04:10.000 --> 02:04:11.000]   iOS.
[02:04:11.000 --> 02:04:13.080]   I think a beta was, but it's not running.
[02:04:13.080 --> 02:04:14.080]   It's not going to come.
[02:04:14.080 --> 02:04:15.080]   Yeah, there's test flight.
[02:04:15.080 --> 02:04:17.000]   I think test flight, but I don't.
[02:04:17.000 --> 02:04:20.600]   Do you know anything about that Lou or have you been following X cloud?
[02:04:20.600 --> 02:04:22.800]   No, I mean, I'm not testing it.
[02:04:22.800 --> 02:04:24.480]   Yeah, never heard.
[02:04:24.480 --> 02:04:25.480]   No.
[02:04:25.480 --> 02:04:32.920]   So it pretty much looks like Apple is, uh, digging in on that and is not going to offer
[02:04:32.920 --> 02:04:40.760]   X cloud or Stadia or Epic's future store or any of that stuff on, uh, on iOS.
[02:04:40.760 --> 02:04:42.760]   That's going to be an interesting battle.
[02:04:42.760 --> 02:04:43.760]   And then Apple will be.
[02:04:43.760 --> 02:04:46.600]   Yeah, then Apple's going to have its own GPU.
[02:04:46.600 --> 02:04:48.200]   They're going to do that in the Apple Silicon.
[02:04:48.200 --> 02:04:53.400]   I can't imagine it competing against big Navi or the RTX 30.
[02:04:53.400 --> 02:04:54.840]   I'm going to make a prediction.
[02:04:54.840 --> 02:04:59.320]   I'm going to say that Apple's mistake here because then you can remember, Android's creeping
[02:04:59.320 --> 02:05:03.880]   up on them, which means that there's going to be some company that puts out some devices
[02:05:03.880 --> 02:05:07.840]   that allows you to do all of these that are that's inexpensive.
[02:05:07.840 --> 02:05:11.760]   And that's going to lie to X cloud and the Google cloud and all that and Stadia.
[02:05:11.760 --> 02:05:13.920]   And they're just going to eclipse the Amazon.
[02:05:13.920 --> 02:05:15.920]   I mean, the Apple gaming infrastructure.
[02:05:15.920 --> 02:05:18.600]   And I think that's why they think that's a big mistake.
[02:05:18.600 --> 02:05:22.000]   I mean, it should just be that they strike a deal here where they get a little bit of
[02:05:22.000 --> 02:05:27.280]   a cut from these services and then allow them, uh, rather than just completely preventing
[02:05:27.280 --> 02:05:28.480]   them from being there.
[02:05:28.480 --> 02:05:30.640]   I kind of think it's a mistake, but we'll have to see.
[02:05:30.640 --> 02:05:31.640]   I totally agree.
[02:05:31.640 --> 02:05:32.640]   I totally agree.
[02:05:32.640 --> 02:05:38.720]   I think this is a, I mean, just like we were talking about OTT with video, the same applies
[02:05:38.720 --> 02:05:41.840]   for OTT with gaming.
[02:05:41.840 --> 02:05:49.040]   And your, your iPhone and definitely your Android device can stream high quality games.
[02:05:49.040 --> 02:05:50.040]   Yeah.
[02:05:50.040 --> 02:05:55.160]   So there's, there's, I mean, if you, anyway, it is, Lou is exactly right.
[02:05:55.160 --> 02:05:58.240]   So Lou, you just built a monster gaming machine.
[02:05:58.240 --> 02:06:01.520]   Well, I didn't build it for gaming to be clear.
[02:06:01.520 --> 02:06:02.520]   Like I built it.
[02:06:02.520 --> 02:06:04.960]   I built it because I'm doing a lot more development at home.
[02:06:04.960 --> 02:06:05.960]   Oh, are you?
[02:06:05.960 --> 02:06:11.800]   But I mean, and again, you need a 64 core horizon to compile your code.
[02:06:11.800 --> 02:06:14.400]   Well, some, some code bases are big.
[02:06:14.400 --> 02:06:16.400]   How are you building windows?
[02:06:16.400 --> 02:06:18.760]   They're the office code.
[02:06:18.760 --> 02:06:24.000]   How, how many lines of code, how fast can you, can you, can you, can you, can you, you're
[02:06:24.000 --> 02:06:25.680]   not building all of office depends.
[02:06:25.680 --> 02:06:30.120]   I mean, it depends on what app you're talking about, but I mean, some, like my work machine,
[02:06:30.120 --> 02:06:35.920]   uh, that only has eight cores at the, at the time, could, would take, you know, minutes,
[02:06:35.920 --> 02:06:38.840]   maybe sometimes 30 minutes to run a complete bill.
[02:06:38.840 --> 02:06:42.400]   Now they have Microsoft does what they call cloud build, right, where they push gesture
[02:06:42.400 --> 02:06:45.920]   changes up to the cloud and then they build it for you and they build it across a hundred
[02:06:45.920 --> 02:06:46.920]   machines.
[02:06:46.920 --> 02:06:47.920]   So it takes seconds.
[02:06:47.920 --> 02:06:52.560]   But, um, I mean, for local devices, local apps that have a million lines of code, I
[02:06:52.560 --> 02:06:54.400]   mean, it'll take a while to go out the file.
[02:06:54.400 --> 02:06:56.480]   Do you guys, you must have a custom build tool.
[02:06:56.480 --> 02:06:59.040]   You don't use Maven or Jenkins, right?
[02:06:59.040 --> 02:07:01.520]   But, you know, um, it, it depends.
[02:07:01.520 --> 02:07:06.320]   I mean, a lot of older code, um, there's custom stuff, but a lot of it's moving to
[02:07:06.320 --> 02:07:11.320]   MS build, uh, and, um, and cloud build and, and, uh, it's really interesting.
[02:07:11.320 --> 02:07:15.320]   So, yeah, I like, so you're building the diff, the diffs, you're building the, the,
[02:07:15.320 --> 02:07:17.880]   uh, the, uh, changes up, which is not very much.
[02:07:17.880 --> 02:07:19.800]   And then they just run the build with your dips.
[02:07:19.800 --> 02:07:20.800]   Yep.
[02:07:20.800 --> 02:07:21.800]   Love that.
[02:07:21.800 --> 02:07:22.800]   That's kind of cool.
[02:07:22.800 --> 02:07:23.880]   It's more secure too.
[02:07:23.880 --> 02:07:29.680]   I'm always fascinated by the tooling used by Microsoft, Google and Facebook because they're
[02:07:29.680 --> 02:07:34.480]   operating a scale that is unimaginable to anybody even 10 years ago.
[02:07:34.480 --> 02:07:36.800]   I mean, just, it's unimaginable.
[02:07:36.800 --> 02:07:41.000]   And, uh, it's always fascinating to see what these, what these guys, what these cats are
[02:07:41.000 --> 02:07:42.000]   doing.
[02:07:42.000 --> 02:07:46.080]   It's really great to have you on the show, Lou, keep working, keep up the great work
[02:07:46.080 --> 02:07:47.080]   on Twi.
[02:07:47.080 --> 02:07:48.080]   I don't know how you have time.
[02:07:48.080 --> 02:07:49.880]   How many kids do you have now?
[02:07:49.880 --> 02:07:51.880]   I have four kids, one on the way.
[02:07:51.880 --> 02:07:53.160]   We'll need an end of November.
[02:07:53.160 --> 02:07:55.520]   I don't know how you have time to do that.
[02:07:55.520 --> 02:07:58.480]   Do woodworking, uh, fly.
[02:07:58.480 --> 02:08:03.240]   You, you, you said earlier today you flot your, you flew your Microsoft flight simulator
[02:08:03.240 --> 02:08:05.360]   and you didn't even know we had an airport.
[02:08:05.360 --> 02:08:06.560]   You landed a pedal room airport.
[02:08:06.560 --> 02:08:07.560]   You got my hopes up.
[02:08:07.560 --> 02:08:10.840]   I thought you were coming down, but no, it's just in the flight sim.
[02:08:10.840 --> 02:08:12.000]   Have you done what they're doing?
[02:08:12.000 --> 02:08:13.000]   Have you flown in Alora?
[02:08:13.000 --> 02:08:17.120]   Cause they, they're actually putting storm real time weather data in the, uh, Microsoft
[02:08:17.120 --> 02:08:18.120]   flight sim.
[02:08:18.120 --> 02:08:20.120]   Did you fly in Alora?
[02:08:20.120 --> 02:08:25.680]   I did not, but I, but I have, I have flown around, uh, you know, in that area and down,
[02:08:25.680 --> 02:08:29.440]   down in the area where they're having a lot of the storms and it's, it's pretty amazing.
[02:08:29.440 --> 02:08:33.040]   Especially when you have the monitors behind me and, you know, sooner or later, VR will
[02:08:33.040 --> 02:08:36.120]   be even more amazing, but nice, nice.
[02:08:36.120 --> 02:08:37.120]   That's awesome.
[02:08:37.120 --> 02:08:38.920]   Uh, follow Lou on the quiet.
[02:08:38.920 --> 02:08:40.920]   He's Lou MM on the Twitter.
[02:08:40.920 --> 02:08:43.280]   Um, anything you want to plug?
[02:08:43.280 --> 02:08:44.920]   We just love having you on and quiet.
[02:08:44.920 --> 02:08:45.920]   You're so good.
[02:08:45.920 --> 02:08:46.920]   Yeah.
[02:08:46.920 --> 02:08:47.920]   Come, come check out.
[02:08:47.920 --> 02:08:48.920]   Why?
[02:08:48.920 --> 02:08:50.360]   I mean, we, we, you know, you talk about about, about Barracuda, who is the sponsor, but I
[02:08:50.360 --> 02:08:54.920]   mean, we had the CTO on recently and he gave, he brought us through a lot of the, the attacks
[02:08:54.920 --> 02:08:56.440]   that are now where that's happening today.
[02:08:56.440 --> 02:08:58.440]   It's a really interesting self-force is recently on.
[02:08:58.440 --> 02:08:59.440]   Nice.
[02:08:59.440 --> 02:09:00.440]   Just, just come and check it out.
[02:09:00.440 --> 02:09:01.840]   I mean, it's a lot of fun.
[02:09:01.840 --> 02:09:02.840]   We have a lot of fun on there.
[02:09:02.840 --> 02:09:06.160]   We do a lot of list trends and you can find some really interesting stuff.
[02:09:06.160 --> 02:09:08.760]   We, if you're not interested in an enterprise stuff.
[02:09:08.760 --> 02:09:09.760]   So come check out.
[02:09:09.760 --> 02:09:10.760]   Thank you, Lou.
[02:09:10.760 --> 02:09:12.760]   This week at Enterprise Tech.
[02:09:12.760 --> 02:09:13.760]   Make sure you subscribe.
[02:09:13.760 --> 02:09:18.800]   Uh, it's great to see you Dan Patterson from CNET and CBS News.
[02:09:18.800 --> 02:09:19.800]   He's covered.
[02:09:19.800 --> 02:09:22.800]   So you, but I love the beats that you choose.
[02:09:22.800 --> 02:09:26.680]   You were covering, I'll occupy Wall Street and then you were doing the election.
[02:09:26.680 --> 02:09:28.840]   Are you going to be doing the election beat this year?
[02:09:28.840 --> 02:09:32.440]   Oh yeah, we have been for a while.
[02:09:32.440 --> 02:09:37.080]   I mean, this campaign has felt, it's been a long campaign.
[02:09:37.080 --> 02:09:38.760]   I have been very fortunate.
[02:09:38.760 --> 02:09:39.920]   It's been for four years.
[02:09:39.920 --> 02:09:40.920]   Yeah.
[02:09:40.920 --> 02:09:41.920]   Yeah.
[02:09:41.920 --> 02:09:43.800]   I covered the intersection of politics and technology.
[02:09:43.800 --> 02:09:48.400]   And as you know, I try really hard not to bring partisan stuff here to Twit.
[02:09:48.400 --> 02:09:51.960]   And we do spend a lot of time talking tech.
[02:09:51.960 --> 02:09:56.360]   But this, it really has been for years and we have seen these, these kind of horrifying
[02:09:56.360 --> 02:10:03.720]   emergence of groups like QAnon and, and some of the more violent groups that are, seem
[02:10:03.720 --> 02:10:05.040]   to be radicalized.
[02:10:05.040 --> 02:10:13.080]   So I, I spend a lot of time covering disinformation and the different routes it takes to the president.
[02:10:13.080 --> 02:10:14.080]   Yeah.
[02:10:14.080 --> 02:10:15.080]   How interesting.
[02:10:15.080 --> 02:10:20.880]   So I mean, I'm really looking forward to like only thinking about gaming and tech after
[02:10:20.880 --> 02:10:21.880]   the election.
[02:10:21.880 --> 02:10:24.000]   Although the election will not be one day.
[02:10:24.000 --> 02:10:25.000]   No.
[02:10:25.000 --> 02:10:29.840]   It's certainly be prepared for a look, I just interviewed Brian Krebs.
[02:10:29.840 --> 02:10:31.480]   Love Brian Krebs.
[02:10:31.480 --> 02:10:32.480]   Love him.
[02:10:32.480 --> 02:10:36.280]   Look, mail-in voting is safe.
[02:10:36.280 --> 02:10:41.160]   Do not buy any of this misinformation and disinformation about mail-in voting.
[02:10:41.160 --> 02:10:42.520]   It is safe and secure.
[02:10:42.520 --> 02:10:44.600]   It is resilient because it generates a paper trail.
[02:10:44.600 --> 02:10:49.000]   It might take a little time to count this, but you can be rest, you can rest assured
[02:10:49.000 --> 02:10:51.360]   if you vote my mail, your vote will be secure.
[02:10:51.360 --> 02:10:54.960]   Some have said that you should, and I think I'm going to do this, get an absentee ballot,
[02:10:54.960 --> 02:11:01.200]   though, on, save it in an election day, drop it, bring it to a polling place and drop it
[02:11:01.200 --> 02:11:02.800]   in a polling box.
[02:11:02.800 --> 02:11:06.960]   Is that a more, you think a safer way to do it?
[02:11:06.960 --> 02:11:12.320]   You know, I, I don't know that I actually shouldn't prescribe unsafety for election.
[02:11:12.320 --> 02:11:13.960]   I know what I'm talking about.
[02:11:13.960 --> 02:11:19.160]   I saw one expert, security experts say, you should crawl in broken glass if you have to,
[02:11:19.160 --> 02:11:23.520]   to vote in person, because it's the only way to guarantee your, your vote, your ballot,
[02:11:23.520 --> 02:11:25.720]   will get into the, into the, I misspoke.
[02:11:25.720 --> 02:11:29.040]   I interviewed Chris Krebs, who is the head of DHS cyber.
[02:11:29.040 --> 02:11:30.040]   Oh, okay.
[02:11:30.040 --> 02:11:31.040]   Not Brian Krebs.
[02:11:31.040 --> 02:11:32.040]   Okay.
[02:11:32.040 --> 02:11:35.560]   He's, he, yes, it is, he's the top cybersecurity official in the country.
[02:11:35.560 --> 02:11:41.560]   And he says that because mail-in voting generates a paper trail, it is safe and resilient.
[02:11:41.560 --> 02:11:43.720]   So that is, I'm not prescribing anything.
[02:11:43.720 --> 02:11:44.720]   I'm just using it.
[02:11:44.720 --> 02:11:47.480]   Everybody agrees the paper trail is critical.
[02:11:47.480 --> 02:11:48.480]   Critical.
[02:11:48.480 --> 02:11:49.480]   Critical.
[02:11:49.480 --> 02:11:50.960]   That's what, that's what counts.
[02:11:50.960 --> 02:11:51.960]   Yeah.
[02:11:51.960 --> 02:11:52.960]   But thanks.
[02:11:52.960 --> 02:11:54.840]   It's been great to be here.
[02:11:54.840 --> 02:11:55.840]   Really fantastic.
[02:11:55.840 --> 02:12:02.820]   Catch Dan, of course, on CBS news and, and whenever we can get him on our, our channel.
[02:12:02.820 --> 02:12:04.440]   And I'm so glad to see you again.
[02:12:04.440 --> 02:12:06.840]   Rob Reed, please give Morgan my love.
[02:12:06.840 --> 02:12:09.800]   Rob is a brilliant novelist.
[02:12:09.800 --> 02:12:13.200]   You've got to read his first two novels.
[02:12:13.200 --> 02:12:20.200]   The first is about aliens coming to earth with a huge copyright bill because apparently
[02:12:20.200 --> 02:12:22.840]   they love them, they love earth music.
[02:12:22.840 --> 02:12:24.360]   They love our pop music.
[02:12:24.360 --> 02:12:27.800]   We make the best music in the universe and we don't know it.
[02:12:27.800 --> 02:12:32.720]   But in the world of year zero, which is that book, this vast alien civilization knows how
[02:12:32.720 --> 02:12:34.080]   magnificent our music is.
[02:12:34.080 --> 02:12:38.160]   And they accidentally commit the biggest copyright infringement since the big bangs.
[02:12:38.160 --> 02:12:41.480]   They're by basically bankrupting the entire universe.
[02:12:41.480 --> 02:12:45.280]   So all the wealth in the universe is owed to our pop stars and their lawyers.
[02:12:45.280 --> 02:12:51.000]   And only one person knows about this, a low level media lawyer in New York.
[02:12:51.000 --> 02:12:54.680]   And he catches on to a nasty alien plot.
[02:12:54.680 --> 02:12:55.800]   So no spoilers here.
[02:12:55.800 --> 02:12:58.480]   That all happens in the first four pages.
[02:12:58.480 --> 02:13:04.440]   And if you want to read another great book, after on is fantastic.
[02:13:04.440 --> 02:13:10.400]   Also kind of a sci-fi novel about a social network that becomes intelligent.
[02:13:10.400 --> 02:13:11.400]   Self-aware.
[02:13:11.400 --> 02:13:12.400]   Self-aware.
[02:13:12.400 --> 02:13:17.400]   Self-aware.
[02:13:17.400 --> 02:13:19.040]   Really a pleasure to have you on Rob.
[02:13:19.040 --> 02:13:21.280]   And after on the podcast, we'll come back soon.
[02:13:21.280 --> 02:13:27.840]   I hope after dash on.com, make sure you watch his TED talk.
[02:13:27.840 --> 02:13:29.000]   That's fascinating.
[02:13:29.000 --> 02:13:31.080]   And we'll get you back real soon.
[02:13:31.080 --> 02:13:32.080]   Thank you, Rob.
[02:13:32.080 --> 02:13:33.560]   It's good to have you on here.
[02:13:33.560 --> 02:13:39.120]   I had no idea that you were a Fulbright scholar in Cairo.
[02:13:39.120 --> 02:13:40.680]   I was.
[02:13:40.680 --> 02:13:41.680]   That's wild.
[02:13:41.680 --> 02:13:46.640]   Yeah, my first year out of college, I went there to study Arabic and research the secular
[02:13:46.640 --> 02:13:48.520]   opposition.
[02:13:48.520 --> 02:13:53.040]   And sort of the political descendants of the people that I spent a lot of time with that
[02:13:53.040 --> 02:13:59.680]   year are the ones who were really behind Egypt's Arab Spring Movement in 2011.
[02:13:59.680 --> 02:14:03.360]   They did not end up, they carried the day in that they got rid of Mubarak, but they
[02:14:03.360 --> 02:14:07.720]   did not carry the day in that they did not ultimately take the reins of the government,
[02:14:07.720 --> 02:14:10.120]   which is tragic on a lot of levels.
[02:14:10.120 --> 02:14:11.120]   Yes.
[02:14:11.120 --> 02:14:12.320]   It's a very cool year that I spent there.
[02:14:12.320 --> 02:14:16.400]   And I still speak better Arabic than most folks in Marin County.
[02:14:16.400 --> 02:14:20.680]   Do you get much occasion to use it?
[02:14:20.680 --> 02:14:21.680]   Occasionally I do.
[02:14:21.680 --> 02:14:23.640]   It's a tough language.
[02:14:23.640 --> 02:14:24.640]   That's a tough language.
[02:14:24.640 --> 02:14:28.320]   And for some reason, it's really stuck in my brain pretty nicely.
[02:14:28.320 --> 02:14:34.280]   And when I do get an opportunity to dust it off, everybody is always completely shocked,
[02:14:34.280 --> 02:14:35.520]   which is fun.
[02:14:35.520 --> 02:14:36.520]   Yeah.
[02:14:36.520 --> 02:14:38.020]   Yeah.
[02:14:38.020 --> 02:14:39.520]   I would be.
[02:14:39.520 --> 02:14:45.360]   We've got to go to an Arab restaurant sometime and you could just show off.
[02:14:45.360 --> 02:14:46.360]   It'd be fun.
[02:14:46.360 --> 02:14:49.440]   I would love that in the after times, after the after times.
[02:14:49.440 --> 02:14:50.440]   In the after times.
[02:14:50.440 --> 02:14:51.440]   Yeah.
[02:14:51.440 --> 02:14:58.240]   We do Twitch every Sunday afternoon, about 2.30 Pacific, 5.30 Eastern, 21.30 UTC.
[02:14:58.240 --> 02:14:59.920]   You can watch us do it live.
[02:14:59.920 --> 02:15:02.400]   There's always interesting conversations before and after the show.
[02:15:02.400 --> 02:15:05.080]   Just go to twit.tv/live.
[02:15:05.080 --> 02:15:06.960]   There's live audio and video streams there.
[02:15:06.960 --> 02:15:11.000]   If you're watching live, there's a chat room you can talk with others who are watching
[02:15:11.000 --> 02:15:12.240]   live and it's really great.
[02:15:12.240 --> 02:15:14.000]   It's there 24/7.
[02:15:14.000 --> 02:15:16.520]   IRC.twit.tv.
[02:15:16.520 --> 02:15:21.720]   On demand versions of this show and everything we do are available in a variety of places.
[02:15:21.720 --> 02:15:24.120]   The podcast, that's what it means, right?
[02:15:24.120 --> 02:15:25.120]   It's a podcast.
[02:15:25.120 --> 02:15:28.320]   The podcast can be downloaded from our website, twit.tv.
[02:15:28.320 --> 02:15:29.800]   You can add, we're actually on YouTube.
[02:15:29.800 --> 02:15:30.800]   There's a whole YouTube channel.
[02:15:30.800 --> 02:15:35.400]   If you go to youtube.com/twit, you can see all the different show channels.
[02:15:35.400 --> 02:15:38.520]   We are also available in your favorite podcast application.
[02:15:38.520 --> 02:15:40.000]   That's probably the best thing to do.
[02:15:40.000 --> 02:15:45.000]   Subscribe and you will get it automatically the minute the show is available.
[02:15:45.000 --> 02:15:46.520]   We appreciate it if you do subscribe.
[02:15:46.520 --> 02:15:47.520]   Thanks for being here.
[02:15:47.520 --> 02:15:48.880]   We'll see you next time.
[02:15:48.880 --> 02:15:51.320]   Stay safe, stay healthy.
[02:15:51.320 --> 02:15:52.320]   Another Twitch.
[02:15:52.320 --> 02:15:53.320]   It's amazing.
[02:15:53.320 --> 02:15:54.320]   It's amazing.
[02:15:54.320 --> 02:15:55.320]   Do it the twit.
[02:15:55.320 --> 02:15:56.320]   All right.
[02:15:56.320 --> 02:15:57.320]   Do it the twit, baby.
[02:15:57.320 --> 02:15:58.320]   Do it the twit.
[02:15:58.320 --> 02:15:59.320]   All right.
[02:15:59.320 --> 02:16:00.320]   Do it the twit.
[02:16:00.320 --> 02:16:01.320]   All right.
[02:16:01.320 --> 02:16:02.320]   Do it the twit.
[02:16:02.320 --> 02:16:03.160]   - To it.
[02:16:03.160 --> 02:16:28.160]   [ Silence ]

