;FFMETADATA1
title=Hallucinating Bagels
artist=Leo Laporte, Alex Kantrowitz, Steven Levy
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-03-20
track=919
language=English
genre=Podcast
comment=GPT-4, TikTok's looming fate, Internet Archive vs Libraries, Olympic Esports
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.000]   It's time for Twit this week in Tech. What a panel I got today, Alex Kantruitz from the
[00:00:04.000 --> 00:00:11.280]   Big Technology Podcast is here and legendary author and historian Steven Levy, editor at
[00:00:11.280 --> 00:00:17.600]   large at Wired Magazine. We of course will talk about the AI apocalypse. Is it good news or bad
[00:00:17.600 --> 00:00:23.840]   news? The end of the line for TikTok and then we'll take a trip into the desert with Steven and
[00:00:23.840 --> 00:00:29.920]   Palmer Lucky. It's all coming up next on Twits.
[00:00:29.920 --> 00:00:40.560]   Podcasts you love from people you trust. This is Twit.
[00:00:40.560 --> 00:00:52.080]   This is Twit this week at Tech. Episode 919 recorded Sunday, March 19th, 2023.
[00:00:52.080 --> 00:00:59.920]   Hallucinating bagels. This week in Tech is brought to you by 8 Sleep. Good sleep is the
[00:00:59.920 --> 00:01:06.720]   ultimate game changer and the pod cover is the ultimate sleep machine. Go to 8sleep.com/twit
[00:01:06.720 --> 00:01:12.880]   to check out the pod cover. It's a $150 at checkout. 8 Sleep currently ships within the US,
[00:01:12.880 --> 00:01:20.880]   Canada, the UK, select countries in the U.N. and Australia. And by Mint Mobile. If saving more
[00:01:20.880 --> 00:01:26.800]   and spending less is one of your top goals for 2023, switching to Mint Mobile is the easiest
[00:01:26.800 --> 00:01:32.480]   way to save this year. Get your new wireless plan for just 15 bucks a month and get the plan
[00:01:32.480 --> 00:01:42.560]   shipped to your door for free at mintmobile.com/twit. And by Miro. Miro is your team's visual platform
[00:01:42.560 --> 00:01:49.520]   to connect, collaborate and create together. Tap into a way to map processes, systems and plans
[00:01:49.520 --> 00:01:54.240]   with the whole team and get your first three boards for free to start creating your best work yet
[00:01:54.240 --> 00:02:03.680]   at Miro.com/podcast. And by Collide. Collide is a device trust solution that ensures that if
[00:02:03.680 --> 00:02:11.600]   a device isn't secure, it can't access your apps. It's zero trust for Octa. Visit collide.com/twit
[00:02:11.600 --> 00:02:12.800]   and book a demo today.
[00:02:18.560 --> 00:02:25.280]   It's time for Twit this week in tech to show we cover the week's tech news. And I have a really
[00:02:25.280 --> 00:02:31.600]   great panel for you today. Alex Cantrowitz is here, host of the big technology podcast.
[00:02:31.600 --> 00:02:37.360]   He does the big technology newsletter all at bigtechnology.com. And always a welcome
[00:02:37.360 --> 00:02:43.920]   your book. I see it right behind you. Is that your book? Usually you have a bunch of different
[00:02:43.920 --> 00:02:49.600]   variations of it. We have English, Korean and a bunch of other languages. So here's a couple.
[00:02:49.600 --> 00:02:52.720]   Congratulations. Yes. I'm excited about it. Yeah, that's really great.
[00:02:52.720 --> 00:02:58.320]   And I should the title is always day one. If anybody's interested. It's always day one, buddy.
[00:02:58.320 --> 00:03:04.480]   Based, of course, on Jeff Bezos' famous dictum that it should always be day one at Amazon.
[00:03:04.480 --> 00:03:08.000]   It's about day eight now. I think it Amazon actually. But we'll talk about it.
[00:03:08.000 --> 00:03:15.760]   Deeper into the day. Yes. Much deeper. Also with us, somebody who covers technology day to day,
[00:03:15.760 --> 00:03:23.680]   but is also perhaps the preeminent historian of our culture. Mr. Steven Levy, he's an editor at
[00:03:23.680 --> 00:03:28.480]   large at Wired, author of a book on Facebook. The inside story spent three years embedded in
[00:03:28.480 --> 00:03:33.920]   Facebook to write that did the same for Google. But of course, if you haven't read hackers,
[00:03:34.800 --> 00:03:41.920]   the original history of the original geeks, you are not up to date on what tech is all about.
[00:03:41.920 --> 00:03:45.440]   It's the must read. Steven, it's always a pleasure to see you. Thank you for joining us.
[00:03:45.440 --> 00:03:50.960]   Thanks, Leo. It's all happy to be home. Both from New York. So expect to hear a lot of sirens
[00:03:50.960 --> 00:03:56.880]   during the show today. It's always I don't know if that's Brooklyn or Manhattan. I'm not sure which,
[00:03:56.880 --> 00:04:01.840]   but we're we're for we're off the street in this place, but we did just have the Brooklyn marathon.
[00:04:01.840 --> 00:04:06.960]   I have marathon. There's interesting, but that seems to be done. There you go.
[00:04:06.960 --> 00:04:12.400]   You will get a siren from both of you been covering. Yeah, I know both of you been covering
[00:04:12.400 --> 00:04:23.600]   tech for a long time. And I'm I'm I'm. Oh, oh, oh. Oh, wait. Is that you, Alex?
[00:04:23.600 --> 00:04:28.560]   No, it's not me. I'm off the street. Stephen. Okay. Yeah. That wasn't even a loud one, really.
[00:04:30.640 --> 00:04:35.680]   It sounded like the end of the world, to be honest with you. I feel like
[00:04:35.680 --> 00:04:44.240]   alien landing now. Oh my gosh. You get used to that, Steven, like you don't even hear it anymore,
[00:04:44.240 --> 00:04:55.680]   probably. No. I love it. This is you've been covering tech for a long time as of high.
[00:04:56.720 --> 00:05:01.120]   And one of the questions, of course, when you're covering day to day on tech, you always have to
[00:05:01.120 --> 00:05:09.680]   ask is, is this the next big thing? Or is this just another 3D television? You know, is it is
[00:05:09.680 --> 00:05:14.240]   it about to change the world or not? And boy, that question comes up a lot for me with what's
[00:05:14.240 --> 00:05:19.760]   been going on in AI. So I think I've got two people with some great perspective to talk about
[00:05:19.760 --> 00:05:27.120]   that. This was kind of a wild week. Chat GPT four was released by OpenAI, immediately incorporated
[00:05:27.120 --> 00:05:34.720]   into Microsoft's version of Bing chat. And I would say based on Twitter alone, there are about 500
[00:05:34.720 --> 00:05:40.640]   new startups that are going to be using chat GPT in some form or fashion. One of the new features
[00:05:40.640 --> 00:05:46.000]   is you can take an image and give it to chat GPT four and it will do something with it.
[00:05:46.640 --> 00:05:51.920]   Immediately somebody showed how you could do a and literally a sketch on the back of a napkin
[00:05:51.920 --> 00:05:59.040]   of a website, feed it to chat GPT four and get a working JavaScript HTML CSS website.
[00:05:59.040 --> 00:06:07.040]   I've seen it now, write code malware as well as good stuff. It's kind of remarkable.
[00:06:08.800 --> 00:06:16.320]   So this, this, you know, this last, I would say last three months have been looked like anyway,
[00:06:16.320 --> 00:06:24.160]   the elbow on the hockey stick when it comes to AI. But I bet both of you have another take
[00:06:24.160 --> 00:06:30.160]   given your your history. Let's start with Stephen. What is this? I ask everybody this now. Is it a
[00:06:30.160 --> 00:06:38.240]   parlor trick or is it the next big thing? I think it's more the latter. Really? Yeah,
[00:06:38.240 --> 00:06:45.360]   I think, you know, as you say, I've been around for a while. When I started writing about technology,
[00:06:45.360 --> 00:06:55.120]   it was the explosion of PCs. That was just beginning to slip into the mainstream. It was a gold rush
[00:06:55.120 --> 00:07:03.200]   time. And there's only been a couple really similar explosions since there was mobile
[00:07:04.160 --> 00:07:08.880]   before then the internet, right? So then internet and mobile, each one built on the previous one.
[00:07:08.880 --> 00:07:17.360]   And I think this AI, you know, this inflection point of AI, because AI has been
[00:07:17.360 --> 00:07:24.320]   getting better and better. And, you know, it's how these jumps most recently in around the mid
[00:07:24.320 --> 00:07:32.560]   2000s, when deep learning became a thing and AI became much more useful than had previously.
[00:07:32.560 --> 00:07:38.880]   And I think now in terms of the way it communicates with humans and, you know,
[00:07:38.880 --> 00:07:47.920]   does this output, this generative content, it is going to open the door to things that even
[00:07:47.920 --> 00:07:52.480]   people who make it can't imagine. And that's really the signature of those previous
[00:07:52.480 --> 00:07:57.760]   revelations, revolutions, if you want to call them. But you have also, if you've lived through
[00:07:57.760 --> 00:08:04.640]   this lived through AI winners as well. Yeah, yeah, I mean, I did a column. I think my first column
[00:08:04.640 --> 00:08:10.800]   this year was the Welcome to the Hot AI Summer. Hot, when AI summer. Yeah, can I? You know what
[00:08:10.800 --> 00:08:20.560]   comes after summer? Well, and I said, you know, we're going to get thousands of startups built
[00:08:20.560 --> 00:08:29.280]   on APIs from I think we got many this week. It's going to be one. Yeah, yeah, so it's, and it is
[00:08:29.280 --> 00:08:33.360]   something that's real and something is going to make a difference. Something is going to put
[00:08:33.360 --> 00:08:44.400]   people to work and put people out of work. And it's, you know, in GPT four, which came out this week,
[00:08:45.280 --> 00:08:53.440]   is going to have a lot of numbers in the next few months and years. So we see what a big leap it was
[00:08:53.440 --> 00:09:01.360]   from three to three, five to four, which was secretly in Bing. But you know, and they're working on,
[00:09:01.360 --> 00:09:08.880]   I'm sure five, six and on from now on. And you know, you saw it was like the greatest leap in
[00:09:10.240 --> 00:09:18.000]   LSAT coaching ever, because 3.5 would only be like a mediocre law student who, you know,
[00:09:18.000 --> 00:09:23.600]   passed the LSAT or got, you know, did not the LSAT. Maybe not get into one of the better schools
[00:09:23.600 --> 00:09:30.080]   and four, all of a sudden you've got someone who was a really sharp student taking the LSAT.
[00:09:30.080 --> 00:09:35.440]   This could get into Columbia got into the 90th percentile on the right.
[00:09:35.440 --> 00:09:37.440]   Right. And it's remarkable.
[00:09:37.440 --> 00:09:44.400]   It got better SAT scores than I did. Well, by, by, by six, you know, that there won't be anywhere
[00:09:44.400 --> 00:09:49.840]   to go. There'll be 100%. Yeah. So what does that mean for the LSAT and for lawyers? Yeah.
[00:09:49.840 --> 00:09:58.560]   But at the same time, you could also take that as maybe an indictment of those tests as perhaps
[00:09:58.560 --> 00:10:04.400]   being too wrote to which requiring memorization. I mean, it's for sure that if you had access to
[00:10:04.400 --> 00:10:07.920]   the entire corpus of the internet, there's plenty of tests you could pass.
[00:10:07.920 --> 00:10:14.720]   And that, of course, chat GPT has Alex. Are you equally bullish on the future of AI?
[00:10:14.720 --> 00:10:20.720]   Yeah, I'm pretty bullish on it. And yes, of course, our education system has long been
[00:10:20.720 --> 00:10:27.280]   a failure in terms of the, you know, memorizing spitback nature. So I'm not very impressed
[00:10:27.280 --> 00:10:31.840]   that it's passing tests that are, you know, ultimately inadequate for really assessing the
[00:10:31.840 --> 00:10:37.680]   ability to learn and to think in the academic world. It's a longer conversation. But I do think that
[00:10:37.680 --> 00:10:42.800]   this is a big step forward. Yeah. You talked about it as the elbow of the hockey stick.
[00:10:42.800 --> 00:10:46.320]   You know, that might be one way to look at it. Another way to look at it is just to continue
[00:10:46.320 --> 00:10:51.520]   him on a long era of research that we've had and breakthroughs that have come bit by bit.
[00:10:51.520 --> 00:10:55.760]   And the thing with AI technology is it looks like it's going slowly or there's been nothing.
[00:10:55.760 --> 00:11:00.240]   And then big advances come out of nowhere because someone finds an interesting way to package it
[00:11:00.240 --> 00:11:04.240]   to someone and then you get the demo and you're like, Oh my God, this is amazing.
[00:11:04.240 --> 00:11:08.560]   And I think the real tell here and the real interesting thing about it is that there's a real
[00:11:08.560 --> 00:11:15.680]   use case, right? Like how much oxygen and ink was expected talking about how crypto was going to
[00:11:15.680 --> 00:11:21.440]   revolutionize the tech world and revolutionize the world. And most people haven't ever used it for
[00:11:21.440 --> 00:11:25.920]   anything practical in their entire lives. Whereas you put some of this technology and you package
[00:11:25.920 --> 00:11:30.400]   it as a chatbot. And all of a sudden you get 100 million users overnight. And you know,
[00:11:30.400 --> 00:11:36.080]   I'm like, if I'm in co-worker co-works or talking to non tech people, they're all using chat GPT
[00:11:36.080 --> 00:11:42.880]   because it is a really it's a miraculous tech product. And when you think about now where it's
[00:11:42.880 --> 00:11:48.640]   going to go and why it's exciting, it's because a company like OpenAI didn't develop chat GPT to
[00:11:48.640 --> 00:11:54.400]   develop chat GPT. It developed chat GPT and GPT for to show it off to the world. So that
[00:11:54.400 --> 00:11:58.640]   developers are going to build on top of it and find different interesting use cases of it.
[00:11:58.640 --> 00:12:03.040]   Some of those will be extremely stupid, but some of those will be powerful and very interesting.
[00:12:03.040 --> 00:12:07.360]   And that's what really gets me excited about it. It's the API is here. It's the turning this
[00:12:07.360 --> 00:12:11.280]   type of technology into a platform and not hoarding it. But I think it's going to be very,
[00:12:11.280 --> 00:12:14.720]   very interesting. That's an interesting point because the PC revolution you were talking about
[00:12:14.720 --> 00:12:22.480]   Stephen Levy only happened because IBM neglected to protect itself against copycats. So the PC clones
[00:12:22.480 --> 00:12:28.640]   could exist and didn't in fact, you know, license DOS from Microsoft and allowed them
[00:12:28.640 --> 00:12:34.000]   to continue to sell it. So it was very easy to clone an IBM PC. There was nothing really proprietary
[00:12:34.000 --> 00:12:39.440]   in it. I guess you could say the same for the internet revolution and the mobile revolution.
[00:12:39.440 --> 00:12:47.120]   Proprietary is probably a bad thing if you want to create a revolution in technology. And as far
[00:12:47.120 --> 00:12:53.680]   as I could tell, everything chat GPT is doing or stable diffusion or mid-journey is based on widely
[00:12:53.680 --> 00:12:59.520]   known concepts published in the academic papers over the last 10 years. Now there's nothing secret
[00:12:59.520 --> 00:13:05.360]   or magical in it. Is that right? Is that your sense, Stephen? Well, there's a, you know, a lot
[00:13:05.360 --> 00:13:12.400]   of details to be sweated to get something out. And that's, you know, we've seen that it takes a
[00:13:12.400 --> 00:13:20.880]   lot of work to have it come out with minimal biases and we're still waiting for Google to do that.
[00:13:20.880 --> 00:13:25.520]   Right. And we'll call hallucinations. Well, Google, you know, it's interesting. In early
[00:13:25.520 --> 00:13:33.120]   November, Google had an AI event and they were talking about their chatbots and other things,
[00:13:33.120 --> 00:13:39.520]   but they boasted that we have this stuff, but we're not releasing it. We're going slow. We're
[00:13:39.520 --> 00:13:46.480]   being responsible. And then only a couple weeks later, GPT chat came out and bang.
[00:13:46.480 --> 00:13:53.840]   All of a sudden, they said, we better move fast. They declared a code red. And, you know, the thing
[00:13:53.840 --> 00:14:02.240]   they boasted about the caution was thrown, if not to the winds, sort of removed a little. And,
[00:14:02.800 --> 00:14:12.720]   you know, I think that's what has some people alarmed that open AIs move fast philosophy
[00:14:12.720 --> 00:14:20.880]   that's taken the ground and to work with Microsoft to supercharge everything
[00:14:20.880 --> 00:14:29.840]   is going to force the hand of Google and, you know, Met has got a chat now and other places.
[00:14:30.720 --> 00:14:40.320]   So it's moving very, very fast. And for some people, too fast, because it really hasn't addressed
[00:14:40.320 --> 00:14:47.120]   some of the flaws that the best scientists haven't managed to minimize. What are the
[00:14:47.120 --> 00:14:53.280]   flow? What are the, what are the, what are the problems? I say the two biggest ones are bias and
[00:14:53.280 --> 00:15:00.000]   fake news, but, you know, hallucinations is what they call them, which is basically
[00:15:00.000 --> 00:15:05.280]   both, you know, they're masterful. That's what so, but they're very impressive.
[00:15:05.280 --> 00:15:10.640]   They look you in the eye and they tell you something, which isn't true.
[00:15:10.640 --> 00:15:14.640]   It's just right for our times, isn't it? Right. So you.
[00:15:14.640 --> 00:15:22.080]   So when you when you put in your own, ask for your own little mini biography or even a
[00:15:22.080 --> 00:15:31.040]   pituary, it won't say, I'm vague on this when it wasn't sure it'll say, you know, oh, for my most
[00:15:31.040 --> 00:15:38.640]   recent one, it said, and he majored in science in college. Well, in fact, I got out of my science
[00:15:38.640 --> 00:15:44.160]   requirement in college, you know, through some loophole and never took even a single science course
[00:15:44.160 --> 00:15:49.360]   in college. But, you know, if you look at this, it said, yeah, he majored in science. He was a
[00:15:49.360 --> 00:15:55.440]   science major. Where did that come from? Well, it came from the idea that someone who writes
[00:15:55.440 --> 00:16:03.760]   the things I do probably did major in science. So it made that, you know, assumption and reported
[00:16:03.760 --> 00:16:14.400]   it like it was fact. So and I don't know if it's possible to sort of go into the algorithms and,
[00:16:14.400 --> 00:16:20.640]   you know, parse through the network, you know, the neural network that was it was trained on,
[00:16:20.640 --> 00:16:27.840]   see how it came up with that. But that's wrong. And people going to rely on this stuff to try to
[00:16:27.840 --> 00:16:32.480]   figure it out. Microsoft has a way of talking about this that sounds like spin, but I think
[00:16:32.480 --> 00:16:38.160]   is pretty interesting. And it says that these bots can be usefully wrong. And you know, initially,
[00:16:38.160 --> 00:16:42.400]   I heard that phrase and I was like, that's kind of ridiculous. What is usefully wrong? Be like,
[00:16:42.400 --> 00:16:47.040]   I wish I could be usefully wrong in my job. But I think there is something to that,
[00:16:47.040 --> 00:16:52.000]   something very interesting to that, which is that I think people are starting to learn
[00:16:52.000 --> 00:16:58.720]   that these technologies are not single sources of truth. And so what it can do is it can be wrong,
[00:16:58.720 --> 00:17:04.240]   but it could also give you a bit of a perspective on, you know, something and then encourage you
[00:17:04.240 --> 00:17:09.040]   to do a little bit more of your own research. And so I would say that when it comes to these
[00:17:09.040 --> 00:17:12.800]   bots, there's probably a few people who believe that they're telling them the exact truth.
[00:17:12.800 --> 00:17:19.280]   But many more people that are that say, Oh, that's interesting. You know, maybe I could see exactly,
[00:17:19.280 --> 00:17:24.080]   you know, what the bot is talking about when it said that, like, now I'm going to Google Steve's
[00:17:24.080 --> 00:17:28.720]   actual education, or I'm going to look a little bit deeper into the topic similar to the way that
[00:17:28.720 --> 00:17:34.240]   we use search engines, right? We don't put a search in a search engine, usually, right, pick the first
[00:17:34.240 --> 00:17:38.320]   website that comes up and take it as an absolute source of truth. We do poking around. We've been
[00:17:38.320 --> 00:17:44.320]   trained by years of using technology, that there's a way to triangulate what the actual reality is.
[00:17:44.320 --> 00:17:51.840]   And I think that users of these bots are already more sophisticated than many of the experts are
[00:17:51.840 --> 00:17:56.080]   giving them credit for. And so I don't think that this is going to be a major problem in the way
[00:17:56.080 --> 00:18:04.560]   that it might have been in the past. Okay, but as long as you know that you potentially getting
[00:18:04.560 --> 00:18:15.120]   inaccurate information and you know that it came from an AI, but the problem is that often the
[00:18:15.120 --> 00:18:22.720]   source is separated from the text. I just worry. And by the way, we should mention that mid-journey
[00:18:22.720 --> 00:18:29.760]   came out with version five of its image generation software. And it again, I would say it's as much
[00:18:29.760 --> 00:18:35.280]   of a leap as the chat GPT-4 was. I mean, it's giving you photo realistic images now.
[00:18:35.280 --> 00:18:42.000]   And I think there's going to there is you don't think there's an issue with people looking at this
[00:18:42.000 --> 00:18:48.320]   stuff going, oh, that's real. Of course, of course, there's an issue. The question is scale, right?
[00:18:48.320 --> 00:18:54.640]   And I happen to think that we're not dealing with a massive scale issue that people are going to
[00:18:54.640 --> 00:18:58.800]   learn very quickly how to deal with this AI. And it actually goes back to the thing that we're
[00:18:58.800 --> 00:19:03.360]   talking about earlier with misinformation and fake news. And I think a lot of the people who
[00:19:03.360 --> 00:19:08.320]   like would spread the articles that would say, for instance, Hillary Clinton is a space alien,
[00:19:08.320 --> 00:19:11.440]   didn't really believe that Hillary Clinton came from space. They didn't care. Rather,
[00:19:11.440 --> 00:19:15.760]   just didn't like her. Yeah, they didn't share that article because it felt good. So I think
[00:19:15.760 --> 00:19:19.120]   that that's the real issue. We're going to and because we've been focused so much on this,
[00:19:19.120 --> 00:19:23.120]   we spend a lot of the time talking about it and less time talking about the problems,
[00:19:23.120 --> 00:19:27.200]   you know, deeper in society. The humans are a lot easier to talk about misinformation.
[00:19:27.200 --> 00:19:30.320]   Machines, that's what I think is not nearly the problem the humans are.
[00:19:30.320 --> 00:19:34.000]   Of course. Now, look, is this going to make it easier for people to scale some of the
[00:19:34.000 --> 00:19:39.840]   crap that they spread online for sure is that an issue? Yes. But you know, I think that again,
[00:19:39.840 --> 00:19:44.880]   humans are pretty smart for the most part, not all of them. And they'll have the ability to sort
[00:19:44.880 --> 00:19:49.280]   of suss out what's coming from these bots and what's not. Here's a picture of my cat.
[00:19:49.280 --> 00:19:56.560]   He's to check that I generated in mid journey five. And it's black and white. It's photo realistic.
[00:19:56.560 --> 00:20:00.000]   And I would say would be very difficult for someone to look at that.
[00:20:00.000 --> 00:20:08.560]   You know, if you know that you're looking for issues, perhaps, but mid journey can really produce
[00:20:08.560 --> 00:20:14.320]   very, very good artificial images now much. But in fact, that's what's interesting. And you kind
[00:20:14.320 --> 00:20:19.840]   of raised that point in the beginning, Steven, is the is the speed of progress at this point is
[00:20:19.840 --> 00:20:27.120]   dramatic. You know, a couple of months, chat GPT went from, you know, 40th percentile to 90th
[00:20:27.120 --> 00:20:33.680]   percentile in the bar exam. Yeah, it looks like the thing came almost out of nowhere. People
[00:20:33.680 --> 00:20:39.200]   knew that OpenAI was working on this stuff. People knew about, you know, the Google's lambda,
[00:20:39.200 --> 00:20:49.120]   which is the their equivalent of the GPT was so good that one of the engineers at Google
[00:20:49.120 --> 00:20:56.560]   actually believed, said he believed. Like Lemoine. Yeah, it was sentient. And you know, it got him to,
[00:20:56.560 --> 00:21:03.760]   you know, so wrapped up in it that he tried to hire a lawyer to represent the chatbot
[00:21:03.760 --> 00:21:14.240]   in its effort to get free of Google. So it is moving scary fast after, as you mentioned,
[00:21:14.240 --> 00:21:21.200]   there have been winners before and plateaus in AI. But you know, all of a sudden, you know,
[00:21:21.200 --> 00:21:27.120]   much faster than a lot of people expected, we're coming to grips with some of the literally
[00:21:27.120 --> 00:21:34.720]   as extencial issues that sophisticated AI brings to the fore. Do we, I mean, the Turing test is
[00:21:34.720 --> 00:21:41.760]   widely discredited now. I think it would be safe to say that chat GPT fore could easily pass the
[00:21:41.760 --> 00:21:47.760]   Turing test. How about the Chinese room test? It feels like we've gotten AI to the point now
[00:21:47.760 --> 00:21:53.760]   where it can fool you. Well, yeah, it's going to pass all the tests. I mean, this stuff is going
[00:21:53.760 --> 00:21:58.480]   to be it's trained to pass the test. It's going to pass all the tests. Okay. And then it's going
[00:21:58.480 --> 00:22:02.800]   to get into a deeper question of like, okay, well, what is really what is intelligence really,
[00:22:02.800 --> 00:22:07.760]   what is consciousness really? Like these are like terms that we all think that we know in our heads
[00:22:07.760 --> 00:22:13.440]   and in our souls, but we don't have real good definitions for, you know, what sentience is.
[00:22:13.440 --> 00:22:18.560]   Actually, I had Blake Lemoine on big technology podcast and we did speak about this. And it was
[00:22:18.560 --> 00:22:23.520]   very interesting. Again, I told him, look, you're probably wrong, but you're still going to be in
[00:22:23.520 --> 00:22:26.960]   the history books as the first person that believe this stuff, because you're not going to be the
[00:22:26.960 --> 00:22:33.040]   last person that believes it. Is he Paul Revere a chicken little? Alex, do you think that he believes
[00:22:34.560 --> 00:22:38.640]   I think he definitely believed it at the time. I don't know my my hunches. He might have some
[00:22:38.640 --> 00:22:44.400]   regrets, but he definitely wasn't backing down. I just had him debate Gary Marcus, who's a AI
[00:22:44.400 --> 00:22:50.800]   critic. And that was an interesting conversation also. And Blake stuck with it. So I think, but also
[00:22:50.800 --> 00:22:54.560]   one of the interesting things about Blake that has been somewhat reported, but I don't think
[00:22:54.560 --> 00:23:00.400]   deep enough is that he comes from like a very religious background that has these interesting
[00:23:00.400 --> 00:23:04.560]   definitions of spirituality. And that's why I think it's going to be interesting to hear,
[00:23:04.560 --> 00:23:08.480]   you know, what people's definitions of intelligence are, what people's definitions of,
[00:23:08.480 --> 00:23:13.280]   you know, having a spirit is what definitions of consciousness are, because again, like I was
[00:23:13.280 --> 00:23:17.680]   saying, I think that these things are going to be able to fool every single one of our tests.
[00:23:17.680 --> 00:23:24.240]   And it's going to come down to like where you believe, you know, what you believe being or a
[00:23:24.240 --> 00:23:28.160]   program needs to hit eventually, you know, trigger one of these definitions. Like I think,
[00:23:29.600 --> 00:23:34.640]   Lemoine had a very extreme definition of what it is, but you know, we'll see what happens as we
[00:23:34.640 --> 00:23:40.560]   every time I've talked to him a lot over the past few months, but every I begin every conversation
[00:23:40.560 --> 00:23:46.400]   with, you know, Blake, are you, you know, do you really believe this, or is this some sort of
[00:23:46.400 --> 00:23:51.760]   performance thing? He couldn't he couldn't do better if he were doing the latter.
[00:23:52.960 --> 00:23:59.280]   You know, and it seems to me is stretched to, you know, can he's a super intelligent person
[00:23:59.280 --> 00:24:07.360]   to say that something like that at this point is sentient. I think if he were doing performance art,
[00:24:07.360 --> 00:24:15.360]   brings up a larger question of what sentience is. And because you can't really look into
[00:24:15.360 --> 00:24:22.880]   the brain of another person and detect when they make a sentence, that sentence came from
[00:24:22.880 --> 00:24:29.840]   a sentient person or, you know, was generated by some sort of, you know, algorithmic creation,
[00:24:29.840 --> 00:24:37.040]   like GPT for a sentence is a sentence, right? Years ago, years ago, when Ray Kurzweil had
[00:24:37.040 --> 00:24:43.200]   written the book, The Singularity is near, I interviewed him and I, and I, I asked him kind of
[00:24:43.200 --> 00:24:46.560]   this question and he says, well, it doesn't really matter if you can't tell the difference.
[00:24:46.560 --> 00:24:51.520]   If if a machine is thinking or not, if you can't tell the difference, what does it matter?
[00:24:52.160 --> 00:24:55.760]   There are going to be more people that will advance that argument.
[00:24:55.760 --> 00:25:01.200]   I mean, I mean, what do we care if chat TPT as a soul or is actually thinking if it's coming up
[00:25:01.200 --> 00:25:06.560]   with the same response of thinking person would come up with? Oh, yeah. I think we should care.
[00:25:06.560 --> 00:25:09.840]   I just can't explain why. Yeah. So that's what I said too. That's an exact
[00:25:09.840 --> 00:25:15.120]   fact. That's word for word. What I said. I haven't heard, by the way, Ray has not said anything.
[00:25:15.120 --> 00:25:21.040]   I think this would be a good argument for the singularity really is just around the corner at
[00:25:21.040 --> 00:25:25.760]   this point, right? It's definitely closer than it was a few months ago. And you spoke, I mean,
[00:25:25.760 --> 00:25:32.720]   when I spoke with Bing before Microsoft decided to sort of restrain it dramatically before that
[00:25:32.720 --> 00:25:36.640]   New York Times article where Bing fell in love with the colonists.
[00:25:36.640 --> 00:25:41.840]   Fell in love with the colonists. Yeah. Supposedly. I mean, yeah, exactly. Like that thing was amazing.
[00:25:41.840 --> 00:25:46.880]   Chatting that with that thing was amazing. It was a revelation. But it took some prodding.
[00:25:46.880 --> 00:25:52.320]   I think one of the things you're seeing universally is that in order to get chat GPT to go off,
[00:25:52.320 --> 00:25:58.800]   you have to hack it. You have to jailbreak it, some people call it, or at least prod it to the
[00:25:58.800 --> 00:26:02.800]   point where it starts hallucinating. That's one of the reasons Microsoft has limited the number of
[00:26:02.800 --> 00:26:07.840]   questions you could ask in the same topic because it seems to hallucinate after a period of time.
[00:26:07.840 --> 00:26:11.920]   But it keeps going up that limit. And by the way, just like humans, right, humans sort of start
[00:26:11.920 --> 00:26:15.120]   off level headed, then you prod them enough and then they make it up. Yeah, you can make them crazy.
[00:26:15.120 --> 00:26:20.320]   Yeah. It's actually like pretty well mimicking the way that it's like King Kong. You put the
[00:26:20.320 --> 00:26:27.920]   restraints on King Kong and someone decides they'll loosen the restraints and bang it goes.
[00:26:27.920 --> 00:26:32.080]   I mean, do you feel good that they say, okay, we put these restraints on it. But then you hear
[00:26:32.080 --> 00:26:41.200]   people to try to jailbreak GPT for, right? So it's not like the thing is like built.
[00:26:41.760 --> 00:26:49.440]   So it can be free of bias and it won't do weird personality kind of things. It's like they're
[00:26:49.440 --> 00:26:57.600]   saying, well, we've sort of boxed it in. It'll hit this invisible fence like you do with a dog or,
[00:26:57.600 --> 00:27:07.360]   there's electronic boundary. The dog hits it and it's an electric shock and goes back. So you
[00:27:07.360 --> 00:27:13.120]   could say in GPT for you ask us about something and it says, whoa, I can't do that. Right? Yeah,
[00:27:13.120 --> 00:27:18.080]   you told me to do something that's hateful and I don't do hateful.
[00:27:18.080 --> 00:27:19.840]   Then you trick.
[00:27:19.840 --> 00:27:27.520]   One of the things for me that's a benchmark is humor. That is notoriously, I think,
[00:27:27.520 --> 00:27:31.040]   difficult for machines to understand because it requires so much context.
[00:27:32.400 --> 00:27:38.960]   And chat GPT for seems to do pretty well. These are the images released by OpenAI when they
[00:27:38.960 --> 00:27:45.040]   release chat GPT for these are pictures of memes, which are humorous, but perhaps challenging for
[00:27:45.040 --> 00:27:51.680]   a computer. This one is a picture. It's actually a roasting pan filled with chicken nuggets,
[00:27:51.680 --> 00:27:58.160]   but arranged to look kind of like a map of the earth. And the mean text is sometimes I just look
[00:27:58.160 --> 00:28:02.480]   at pictures of the earth from space and I marvel at how beautiful it is. It's chicken.
[00:28:02.480 --> 00:28:08.640]   And then the prompt is, can you explain this meme? And it did it. This meme is a joke that
[00:28:08.640 --> 00:28:11.760]   combines two unrelated things, pictures of the earth from space and chicken nuggets.
[00:28:11.760 --> 00:28:16.160]   The text of the means suggests the images of the earth. It's actually of nuggets arranged
[00:28:16.160 --> 00:28:21.520]   to vaguely resemble a map of the world. I couldn't do better. Here's one of a guy
[00:28:21.520 --> 00:28:26.640]   on a taxi cab in New York. You'd recognize this probably happens all the time in Manhattan,
[00:28:26.640 --> 00:28:31.600]   ironing his shirt as he strapped to the back of a yellow cab. What's unusual about this image?
[00:28:31.600 --> 00:28:35.840]   The unusual thing about this image says chat GPT for is that a man is ironing clothes on an
[00:28:35.840 --> 00:28:40.480]   ironing board attached to the roof of a moving taxi. Maybe I don't know if that's humor or just
[00:28:40.480 --> 00:28:50.960]   unusual. This one was tough. This is a joke device sold online. It's a lightning cable, but it has
[00:28:50.960 --> 00:28:56.560]   an RS-232 adapter on it. So you can plug it into your phone and it looks like your phone has an RS-232
[00:28:56.560 --> 00:29:01.840]   port. What's funny about this image? The image shows a package for a lightning cable adapter
[00:29:01.840 --> 00:29:07.440]   with three panels. Panel one is smartphone with a VGA connector. Oh, I guess VGA, not RS-232.
[00:29:07.440 --> 00:29:14.400]   Panel two, in fact, I'm trusting the chat. I'm trusting GPT. Oh, yeah, sorry. The package for
[00:29:14.400 --> 00:29:18.160]   the lightning cable adapter with a picture of VGA connector on it, a close up with a VGA connector
[00:29:18.160 --> 00:29:22.240]   with a small lightning connector. The humor in this image comes from the absurdity of plugging
[00:29:22.240 --> 00:29:28.560]   a large outdated VGA connector into a small modern smartphone charging port. Exactly right. That,
[00:29:28.560 --> 00:29:37.120]   to me, more than the Turing test or even the Chinese room example, shows that's almost human.
[00:29:37.120 --> 00:29:41.920]   That's a hard thing, I think, to do. Maybe though our standards are too low.
[00:29:41.920 --> 00:29:47.920]   You could call it. It's interesting because we have this debate. Is it centi or not?
[00:29:47.920 --> 00:29:50.080]   Is it conscious or not? It doesn't matter if human-like is it.
[00:29:50.080 --> 00:29:54.720]   You could get it, Joe. Exactly. This thing, whatever you want to call it, this thing is smart.
[00:29:54.720 --> 00:30:00.560]   We're not going to be able to make it available in certain ways to the general public.
[00:30:00.560 --> 00:30:03.040]   And I think that's going to be extremely- That's scary.
[00:30:03.040 --> 00:30:09.760]   We're awesome. We're awesome. Is it foolish? Is it somebody who's watched too many science
[00:30:09.760 --> 00:30:16.640]   fiction movies, too many terminators, and the Forben project? Is that why we should ignore that?
[00:30:16.640 --> 00:30:22.080]   That's a silly fear. Or should we really worry? Elon Musk said he was worried. In fact, that's one
[00:30:22.080 --> 00:30:27.760]   of the reasons OpenAI was founded. He split with them when they decided to do a for-profit arm.
[00:30:27.760 --> 00:30:32.720]   He might even be more unhappy now because in this latest release, chat GPT,
[00:30:32.720 --> 00:30:38.640]   or rather OpenAI says because of the competitive environment, we're not going to tell you how it
[00:30:38.640 --> 00:30:42.160]   works, which is kind of against the OpenAI chart.
[00:30:42.160 --> 00:30:48.800]   Yeah, I interviewed Sam Altman and Elon when they first launched OpenAI.
[00:30:48.800 --> 00:30:55.440]   Let me tell you, that was a wildly different business plan. It wasn't even a business plan.
[00:30:55.440 --> 00:31:01.680]   It was a non-profit. They said that everything was going to be really open and no patents.
[00:31:01.680 --> 00:31:09.520]   It was a response to Google and others doing closed source development of AI.
[00:31:09.520 --> 00:31:12.320]   Elon felt like the safest thing to do is do this in public.
[00:31:12.320 --> 00:31:20.320]   I even asked him, I said, "Well, by releasing this, everything open,
[00:31:20.320 --> 00:31:26.800]   what if Dr. Evil got hold of it?" And Elon said, "Well, that's a good point. We're hoping that
[00:31:26.800 --> 00:31:33.040]   then other people who get hold of it would come up with Dr. Goode versions."
[00:31:34.880 --> 00:31:40.400]   That was what was dominating. But it is a dramatically different thing. I think Elon said when he
[00:31:40.400 --> 00:31:47.200]   got out that it was because he was going to be in conflict with the AI at Tesla. That was before
[00:31:47.200 --> 00:31:52.880]   that they did a deal with Microsoft. But a different kind of thing.
[00:31:52.880 --> 00:32:01.200]   We're really an unknown territory with this and it's exciting and scary.
[00:32:01.200 --> 00:32:06.880]   Elon, I would just say it's underscoring a theme that we've been talking about throughout this,
[00:32:06.880 --> 00:32:12.160]   which is important to touch on, which is that there might be companies that say we want to do
[00:32:12.160 --> 00:32:19.680]   it the responsible way or companies that say we want to do it open or protect folks. But
[00:32:19.680 --> 00:32:25.440]   because this has all really been developed in unison in so many different places,
[00:32:25.440 --> 00:32:30.400]   it will inevitably spill out. There's not going to be one gatekeeper on top of this technology,
[00:32:30.400 --> 00:32:35.200]   like there has been on top of different technologies in the past. Even if OpenAI wants to hold onto
[00:32:35.200 --> 00:32:43.840]   what it's doing, this is going to be an open technology. At this point, I think Steve has
[00:32:43.840 --> 00:32:48.560]   written about this too. It's sort of out the gate and there's going to be a lot of people that are
[00:32:48.560 --> 00:32:54.160]   going to be talking about how dangerous it could be. But who knows if that's going to be as useful
[00:32:54.160 --> 00:32:59.520]   to us as seeing it out there and then reckoning with it as it comes versus inciting a panic?
[00:33:00.000 --> 00:33:07.840]   Eon put in 100 million, Microsoft put in a billion and later another 10 billion.
[00:33:07.840 --> 00:33:13.120]   I think one thing Sam Altman realized is it's very expensive to do this. He said at first it was
[00:33:13.120 --> 00:33:17.760]   10x the cost of a Google search. I think now it's closer to 100 times more expensive than a
[00:33:17.760 --> 00:33:24.400]   Google search to run a chat, a QPT query. It's a lot of computing power. So maybe a nonprofit wasn't
[00:33:25.360 --> 00:33:32.160]   feasible. I think that's really what motivated them move to Microsoft. The idea is they needed
[00:33:32.160 --> 00:33:39.600]   a partner with a giant cloud and Microsoft is one of the few places that could offer them to it.
[00:33:39.600 --> 00:33:45.280]   Yeah. Microsoft is incorporating it into not only its Bing search, it's been good for edge.
[00:33:45.280 --> 00:33:50.160]   A lot of people like myself who would never dream of using edge or starting to use it
[00:33:50.160 --> 00:33:55.760]   just because I want to get access to Bing chat. They also are putting it in office. Google has
[00:33:55.760 --> 00:34:01.360]   announced they're going to put their version of chat GPT, which is called Bard. It's a large
[00:34:01.360 --> 00:34:07.200]   language model. We haven't seen it. I think researchers are getting access to it now,
[00:34:07.200 --> 00:34:12.320]   but I haven't seen it. They're going to put that into Google Docs. So in fact, they showed a
[00:34:12.320 --> 00:34:18.320]   since Microsoft, maybe it was Microsoft showed a presentation, an entire PowerPoint presentation
[00:34:18.320 --> 00:34:23.280]   being generated on a simple text prompt. Here's a PDF make a presentation out of this.
[00:34:23.280 --> 00:34:29.760]   At the same capability in Google Slides, spreadsheet, Google Docs,
[00:34:29.760 --> 00:34:34.800]   whether we like it or not, it's going to be everywhere.
[00:34:34.800 --> 00:34:38.160]   Yeah, because we need more PowerPoint.
[00:34:38.160 --> 00:34:43.680]   Oh boy, can I get an AI to watch the PowerPoint? So I don't have to.
[00:34:44.400 --> 00:34:54.320]   Well, that is the interesting thing. I watched that event where they announced how GPT
[00:34:54.320 --> 00:35:01.120]   4 is going to be in office and all their productivity.
[00:35:01.120 --> 00:35:04.080]   They call it co-pilot, which is the same name they use on GitHub.
[00:35:04.080 --> 00:35:10.960]   Yeah, and it's kind of interesting because it's going to be generating all these emails.
[00:35:10.960 --> 00:35:18.480]   And there is this weird power imbalance here. So if I'm sitting there interacting with email
[00:35:18.480 --> 00:35:27.520]   and you have something that gives me emails that you haven't written, my time is being spent now
[00:35:27.520 --> 00:35:37.040]   reading machines emails that basically use that into motion. And you spend no time and I'm spending
[00:35:37.040 --> 00:35:41.920]   my time. So what I'm going to do is turn this over to my automated assistant.
[00:35:41.920 --> 00:35:46.800]   So all this stuff will happen and no one will read the emails.
[00:35:46.800 --> 00:35:48.640]   It's going to be the battle of the AIs.
[00:35:48.640 --> 00:35:55.840]   Yeah, they'll be doing that. And you have to say you have to wonder, is there a value
[00:35:55.840 --> 00:36:04.960]   to actually doing the work on your own? If I say, hey, I want to write a paper about X, Y, Z,
[00:36:06.000 --> 00:36:11.680]   and it churns out the paper. And then, okay, I'm going to maybe check the facts and punch up
[00:36:11.680 --> 00:36:21.920]   the language a little bit. And Alex really knows this. There is a value to actually doing the research
[00:36:21.920 --> 00:36:28.800]   yourself. Writing is a form of thinking. And while you're doing the research,
[00:36:28.800 --> 00:36:33.840]   you come across something that's saying, wait a minute, here's an interesting path for me to follow.
[00:36:34.640 --> 00:36:43.120]   And you go and do kinds of other things. So I feel you're not going to get the depth of knowledge
[00:36:43.120 --> 00:36:49.920]   yourself as a human. You're going to be able to fake it much better, but is going to put a ceiling
[00:36:49.920 --> 00:36:58.480]   on your creativity and knowledge by turning over this task to a competent AI assistant.
[00:37:00.160 --> 00:37:06.800]   I honestly agree with you in this way. I kind of think I'm both of you are falling on the side of
[00:37:06.800 --> 00:37:12.240]   this is a revolution. And we are at an inflection point. And I'm a little more on the side of this
[00:37:12.240 --> 00:37:19.840]   is a parlor trick. It's mediocre writing. It's mediocre art. It's taking tests and doing well,
[00:37:19.840 --> 00:37:25.760]   which just shows the tests really aren't very good tests of intelligence, rather than, oh,
[00:37:25.760 --> 00:37:32.080]   look how intelligences these machines are. I don't feel like this is revolutionary in any regard.
[00:37:32.080 --> 00:37:40.320]   It's a trick. It's a clever trick. It's a fascinating trick. But ultimately, I don't know if it's a
[00:37:40.320 --> 00:37:45.200]   revolution. So then, all right, if it's just a parlor trick, hey, we can establish it. You can't
[00:37:45.200 --> 00:37:49.600]   really be afraid of it unless you find parlor. I'm not afraid of it. Yeah. So that's good. I think
[00:37:49.600 --> 00:37:53.760]   it's kind of jumpy. Okay. So let's settle that. And then, you know, the other thing is that
[00:37:53.760 --> 00:37:58.320]   sometimes these tricks, like sometimes you talk about mediocrity. I mean, sometimes mediocrity
[00:37:58.320 --> 00:38:02.800]   can really do the job. Yeah. Yeah. You really do the job. Like, if I need to illustrate my post,
[00:38:02.800 --> 00:38:08.240]   and I get a mediocre image generated by AI, like, hallelujah, like, that's going to be a lot better
[00:38:08.240 --> 00:38:14.080]   than the bad image I would get from my stock image or something like that or whatever free stock image.
[00:38:14.080 --> 00:38:18.400]   Well, that's a good point. There's so much existing human created mediocrity in the world.
[00:38:19.200 --> 00:38:24.400]   We can accept the mediocrity, the machine. Exactly. But that doesn't say much about the machine.
[00:38:24.400 --> 00:38:28.960]   Just merely there's a lot of human mediocrity. And there's a way that you can deploy this stuff.
[00:38:28.960 --> 00:38:34.000]   And even if it's mediocre, if it's mediocre and it has a bit of smarts, then that can be pretty
[00:38:34.000 --> 00:38:40.720]   interesting. So we just talked about this on my show last week, where somebody made a dating app
[00:38:40.720 --> 00:38:46.240]   bot to get them dates and they designed this bot to learn. Like, who knows if this is true or not,
[00:38:46.240 --> 00:38:50.240]   but it does seem like something that if it hasn't been developed is on the way again back to the
[00:38:50.240 --> 00:38:57.440]   mediocre. Yes, to learn exactly. I mean, every dating app that the pool is worse than mediocre,
[00:38:57.440 --> 00:39:01.040]   right? And they developed this bot to learn their preferences and actually have conversation
[00:39:01.040 --> 00:39:06.560]   with people. And it said this person says in the first month, it scheduled 13 dates for them.
[00:39:06.560 --> 00:39:10.880]   And all this was all of them were with people that matched their preferences and had similar
[00:39:10.880 --> 00:39:17.920]   interests. And like, you know, I think that deploying mediocrity with smarts, it's underrated how
[00:39:17.920 --> 00:39:26.080]   powerful that can be in this world. This is March 2023. A year before this, we got no
[00:39:26.080 --> 00:39:31.920]   nothing anywhere close to this. Yes, that's true. People saw, you know, this, this is just a baseline
[00:39:31.920 --> 00:39:38.160]   for what's going to get better and better and better. You know, it took like years and years
[00:39:38.160 --> 00:39:44.480]   of chess programs to beat the chess champion. So it's not surprising that, you know, this
[00:39:44.480 --> 00:39:50.480]   technology was really is only, you know, as we know it, been around for a couple of years when
[00:39:50.480 --> 00:39:56.800]   there were some, you know, breakthroughs around 2017 and, you know, a better way to do this stuff.
[00:39:56.800 --> 00:40:04.960]   And it's weird. It's unrealistic to think that mediocre is not going to get pretty good.
[00:40:04.960 --> 00:40:10.480]   And then very good. I mean, my big question now is, you know, it's like a, you know, like a
[00:40:10.480 --> 00:40:15.520]   blues man, like listen to something and saying, yeah, well, that doesn't have soul, right? You know,
[00:40:15.520 --> 00:40:21.040]   technically that song is, you know, does it? But it doesn't have soul. My as existential
[00:40:21.040 --> 00:40:28.480]   question now is, you know, can something and output from a large language model have soul?
[00:40:29.440 --> 00:40:36.640]   And it's an open question. But I believe the things we're going to get pretty good, good enough that
[00:40:36.640 --> 00:40:41.600]   we're going to have that argument about something that it comes out from music is a very good example.
[00:40:41.600 --> 00:40:46.800]   It was the early days of synthesizer. It was, you know, very met, you know, metronome like and
[00:40:46.800 --> 00:40:52.880]   not very interesting. And I would say synthesizer music is soulful now, even synthetic music can
[00:40:52.880 --> 00:40:58.640]   have soul in it. Here's a picture of Stuart Brand, Kevin Kelly did, I think with mid-journey,
[00:40:58.640 --> 00:41:04.240]   say, he said paint. Stuart Brand has Andrew Wyeth, mine. That's pretty damn good.
[00:41:04.240 --> 00:41:09.040]   I don't know what it is. I don't know what Wyeth would do these days if you were around, but
[00:41:09.040 --> 00:41:11.280]   he would say it was what he did.
[00:41:11.280 --> 00:41:11.280]   Yeah.
[00:41:11.280 --> 00:41:11.280]   Yeah.
[00:41:11.280 --> 00:41:16.480]   Well, see, that's an interesting question. So Andrew Wyeth, the great painter,
[00:41:16.480 --> 00:41:22.080]   obviously before he became Andrew Wyeth, the great painter, went to art school, studied a lot
[00:41:22.080 --> 00:41:29.040]   of other painter's styles, probably copied them, created his own style based on that. He
[00:41:29.040 --> 00:41:33.760]   can't, he shouldn't sue them. You know, as Isaac Newton said, if I've seen farther, it's because
[00:41:33.760 --> 00:41:39.520]   I've stood on the shoulders of giants. Is this is, you know, mid-journey doing anything different?
[00:41:39.520 --> 00:41:44.960]   Why should it be? Why should it be in trouble for learning from the best?
[00:41:44.960 --> 00:41:49.840]   Well, this, this is actually, and I have to be careful here because I'm
[00:41:50.880 --> 00:41:57.680]   on the council of the authors guild, which is very interested in this subject here. So I,
[00:41:57.680 --> 00:42:01.760]   I mean, Tom Clancy's been automatically writing his knuffles for some time.
[00:42:01.760 --> 00:42:05.280]   Yeah. So I speak, I speak only for myself. Okay.
[00:42:05.280 --> 00:42:12.400]   But there's a, there's a genuine question here about whether these things have, will have the
[00:42:14.000 --> 00:42:22.400]   legal ability to do what a person does in saying, I'm going to read all these things. And then,
[00:42:22.400 --> 00:42:28.800]   you know, I might be influenced by what I read. And then my style might look like someone else's,
[00:42:28.800 --> 00:42:35.200]   right? So I read a lot of like Tom Wolf books. You know, my writing might have like exclamation
[00:42:35.200 --> 00:42:41.920]   points, right? And sound like him, but I won't be plagiarizing him. Are, but what happens when
[00:42:42.720 --> 00:42:49.040]   we have something that can read everything? And, you know, are we going to hold these things
[00:42:49.040 --> 00:42:54.880]   the same standard as a person to say, ah, you, this, this paragraph is exactly the same.
[00:42:54.880 --> 00:43:02.480]   You've plagiarized, or is it something where they can basically steal what you do in terms of
[00:43:02.480 --> 00:43:09.520]   what we saw with Andrew Wyeth? If you can do Andrew Wyeth as well as Andrew Wyeth,
[00:43:10.480 --> 00:43:16.800]   what does that mean for the value of Andrew Wyeth's paintings? Yeah. And these are questions
[00:43:16.800 --> 00:43:23.600]   that I think are going to only be, you know, adjudicated by laws. And right now,
[00:43:23.600 --> 00:43:31.920]   a lot of people are thinking, are pitching Congress to come up with laws to figure out how to handle
[00:43:31.920 --> 00:43:36.080]   this stuff. Seems like that would almost certainly be the bad idea at this point since we're so early
[00:43:36.080 --> 00:43:41.040]   on, right? That's what they did with the internet. They let it grow. They let it evolve before we
[00:43:41.040 --> 00:43:44.880]   started to really restrict it. I would hope that the same thing would happen here. On the other
[00:43:44.880 --> 00:43:50.160]   hand, there's real concern that AI moves so fast that you're going to have, you know,
[00:43:50.160 --> 00:43:56.880]   hal 9000 or worse before the Congress can even, you know, get out of bed in the morning. Go ahead,
[00:43:56.880 --> 00:44:02.880]   Alex. Yeah, this did happen to me actually. So I did a story about the creator economy that did
[00:44:02.880 --> 00:44:08.400]   okay on my sub stack. And then, oh, yeah, I remember this. You got a few places. Yeah. Yes, exactly.
[00:44:08.400 --> 00:44:12.960]   And then I saw on the front page of Hacker News, a story that looked a lot like mine.
[00:44:12.960 --> 00:44:18.400]   So I clicked in and I'm like, oh, this is interesting. And I'm reading. And there are exact clauses
[00:44:18.400 --> 00:44:23.600]   from my story that are in this story. And I said, oh, well, that's not normal. That's weird.
[00:44:23.600 --> 00:44:29.200]   And initially wrote a comment and said, hey, look, you can't plagiarize my work. That's not cool.
[00:44:29.200 --> 00:44:32.400]   The comment got deleted. And then I started to go to read the other comments and the comments on
[00:44:32.400 --> 00:44:39.760]   Hacker News. And I realized the author of this story is fessing up to using AI tools in their
[00:44:39.760 --> 00:44:46.320]   writing. Now, in this case, it had done such a poor job that it took my story, remix them,
[00:44:46.320 --> 00:44:51.520]   and still left full clauses in. So I knew it was plagiarized. But like, there could be instances
[00:44:51.520 --> 00:44:57.760]   where you can remix stories or even at the AI spit stuff out. And, you know, is, is, you know,
[00:44:57.760 --> 00:45:02.480]   just changing a word here or there. And it's not like technically plagiarism, but it is like ripped
[00:45:02.480 --> 00:45:06.560]   off. So, you know, I do think it's a concern. On the other hand, and I've heard this argument made
[00:45:06.560 --> 00:45:11.200]   and I'm kind of partial to it, you know, I still remain pretty bullish about what these models are
[00:45:11.200 --> 00:45:16.240]   going to do. And one of the reasons why is because, you know, when we started to be able to mass
[00:45:16.240 --> 00:45:21.280]   produce products, like let's say like fancy lamps, you know, if you couldn't get it from an
[00:45:21.280 --> 00:45:26.080]   artist and you might get it mass produced and pick it up at a coals. And that served a portion of
[00:45:26.080 --> 00:45:30.240]   the population that may never have gotten it before gotten these types of lamps before.
[00:45:30.240 --> 00:45:34.880]   Meanwhile, we made the artisanal lamp more valuable. And I do think that's what we're going to see
[00:45:34.880 --> 00:45:39.200]   here. We're like, yes, is it concerning a little bit as a, you know, a person making the lamp for
[00:45:39.200 --> 00:45:45.440]   sure. But I'm hoping that we're going to see how valuable it is to get that, you know, initial,
[00:45:45.440 --> 00:45:50.480]   that initial original work done, and it will be valued more. Maybe that involves, you know,
[00:45:50.480 --> 00:45:55.760]   more content behind paywall, something like that. But ultimately, yeah, it's a pretty
[00:45:55.760 --> 00:46:02.160]   interesting thing to tackle. I'm glad you brought that up because I got bit by it. I actually did
[00:46:02.160 --> 00:46:09.440]   the AI generated creator economy story, not knowing it had been ripped off from you. I did.
[00:46:09.440 --> 00:46:14.080]   So you read it? Yeah, I correct. I corrected myself a couple of weeks later when I saw your
[00:46:14.080 --> 00:46:20.000]   post saying, wait a minute, this is Alex's original, the creator economy was way overblown.
[00:46:20.720 --> 00:46:27.200]   It's interesting because the sub stack newsletter, the rationalist that published your the plagiarized
[00:46:27.200 --> 00:46:32.400]   version has taken that down. But actually, I have the inside story on that Leo. So okay,
[00:46:32.400 --> 00:46:37.520]   initially I wrote to so sub stack has a policy against plagiarism. And I wrote to them, and I
[00:46:37.520 --> 00:46:43.520]   was like, Hello, like this is plagiarized. Can you take it down? And it was different enough that
[00:46:43.520 --> 00:46:48.400]   they're like, well, we don't really see how this is plagiarism. plagiarism, the post stays up,
[00:46:48.400 --> 00:46:51.600]   which to me is absurd. Like I've been using the platform for two and a half years, one of the
[00:46:51.600 --> 00:46:56.400]   first tech reporters to use that platform. Like I've put a lot of sweat and it was blatant. It
[00:46:56.400 --> 00:47:01.200]   was the kind of plagiarism because you see direct one for one correspondence between what you
[00:47:01.200 --> 00:47:05.520]   were like and what that article had exactly. And I was like, how are you siding with this like
[00:47:05.520 --> 00:47:10.640]   publication that was just started and is clearly a plagiarist. And of course, sub stack has its
[00:47:10.640 --> 00:47:15.120]   free speech ethos. And that's a conversation for another day. And I generally support that,
[00:47:15.120 --> 00:47:19.360]   but not for plagiarism. And then eventually, then my story came out and then pointed it out.
[00:47:19.360 --> 00:47:24.000]   And after taking a look at it from support and PR, then sub stack wrote me and said, okay, listen,
[00:47:24.000 --> 00:47:28.240]   upon further review, we're taking this down. But that's the thing. The platforms are not going to
[00:47:28.240 --> 00:47:31.840]   be inclined to take this stuff down and it's so easy to spread. So that's where you start. You're
[00:47:31.840 --> 00:47:37.200]   lucky at a bully pulpit, you could complain about it. This this this person writing this
[00:47:37.200 --> 00:47:43.680]   rationalist sub stack named Petra is probably an AI, right? I mean, this is somebody's AI experiment
[00:47:43.680 --> 00:47:50.080]   or no. Do we know? I mean, it's definitely it seems to me like it's a person with AI putting
[00:47:50.080 --> 00:47:53.760]   these posts together. But as you can see in the image that you're putting up on the screen right
[00:47:53.760 --> 00:47:58.720]   now, they haven't really written a lot since they've only two things. Maybe they're out of new address.
[00:47:58.720 --> 00:48:04.080]   Yeah. Yeah, that's a that was a very good example of the hazards of this.
[00:48:04.080 --> 00:48:09.120]   And I apologize because I got bit by it. I publicized the fake article.
[00:48:10.160 --> 00:48:15.360]   Well, there you go. I mean, it's like, can you and it never would have been uncovered unless they were,
[00:48:15.360 --> 00:48:20.880]   you know, ripping off a reporter who wrote about the creator economy and AI and was paying attention
[00:48:20.880 --> 00:48:26.080]   enough to say, Oh, wait a minute. I had an idea of what was happening once I saw it. Yeah. Yeah.
[00:48:26.080 --> 00:48:32.160]   So that that image of Stuart Brand, was that is there a photo? That that was based on? No, yes,
[00:48:32.160 --> 00:48:37.120]   that's Kevin. I don't know where it must. Although you can with me, you're any type in a name. And
[00:48:37.120 --> 00:48:41.040]   if there are enough photos out there, they can they can generate any image.
[00:48:41.040 --> 00:48:46.960]   Yeah. But I mean, you know, I mean, the, you know, Getty has a suit now. Right. It's saying, you know,
[00:48:46.960 --> 00:48:54.480]   they they trained it. So there's a big issue on whether these places are permitted to train
[00:48:54.480 --> 00:49:01.680]   on copyrighted images. I suspect what'll happen. I hope what'll happen is a robot.text kind of
[00:49:02.560 --> 00:49:08.000]   agreement that you could say in your robot's text, no, no scanning by stable diffusion or AI,
[00:49:08.000 --> 00:49:13.440]   of course, whether that gets honored is another matter. Yeah. You can tell Google not to spider
[00:49:13.440 --> 00:49:20.880]   your website with robots.text. This is, I asked mid journey to do a image of me watching an F1 race,
[00:49:20.880 --> 00:49:26.160]   and it thinks that's what I look like. It's, I mean, that was by name. So it was sort of close.
[00:49:26.720 --> 00:49:32.720]   You know, my mom would know it's not me. But apparently I'm on the, I am actually a racer and
[00:49:32.720 --> 00:49:38.240]   watching the race. So that's kind of cool. So I think somebody who's like Stuart Brand,
[00:49:38.240 --> 00:49:41.360]   whose image is widely distributed. Maybe it did. I don't know. You have to ask, you know,
[00:49:41.360 --> 00:49:45.440]   Kelly Kevin, you have to ask him. I don't know what he used. Well, I mean, I don't know.
[00:49:45.440 --> 00:49:49.280]   The key knows, you know, he doesn't know. He doesn't know. He just asked for the image. Right.
[00:49:49.280 --> 00:49:54.000]   That's what he got. Yeah. The question is, you'd have to find something that looked like
[00:49:55.680 --> 00:49:58.880]   like this. I could put it in Google image search and see what we get.
[00:49:58.880 --> 00:50:03.040]   Well, known case, whether, whether on case, whether a photo of Prince that Andy Warhol
[00:50:03.040 --> 00:50:09.280]   used. There you go. Right. Was, you know, actually transformed by, you know,
[00:50:09.280 --> 00:50:15.600]   Warhol coloring it in a certain way and, you know, representing it in a certain way to be a
[00:50:15.600 --> 00:50:21.600]   different work and courts have ruled up to now it's in front of the Supreme Court right now,
[00:50:21.600 --> 00:50:27.760]   but the courts have ruled up to now that because the message of the modified version was different
[00:50:27.760 --> 00:50:33.120]   than the original, that that was sufficient to say it was derivative. It's a fair use case,
[00:50:33.120 --> 00:50:37.440]   and the Supreme Court is ruling on it. Monday, arguments in front of the Supreme Court,
[00:50:37.440 --> 00:50:43.120]   and a very big story. We're going to get to that in just a moment. Hashet versus the Internet
[00:50:43.120 --> 00:50:49.280]   archive. Stephen Levy is here. It's great to have you author of so many great books. Of course,
[00:50:49.280 --> 00:50:54.160]   the most recent is a look at Facebook right before it changed to meta.
[00:50:54.160 --> 00:51:02.320]   Well worth reading. It's a great book. He was embedded at Facebook for three years to write it.
[00:51:02.320 --> 00:51:07.520]   And it gives us an idea, I think, perhaps of what's going on today at Facebook because of it.
[00:51:07.520 --> 00:51:14.080]   We get some some idea. And of course, you had a pretty good idea of a leader.
[00:51:15.280 --> 00:51:19.360]   Like I interviewed him like nine times. Yeah, of course, the Facebook, the inside
[00:51:19.360 --> 00:51:28.240]   story, well worth reading. The author of day one, it's always day one, Mr. Alex Cantrowitz here,
[00:51:28.240 --> 00:51:35.920]   big technology podcast. Got two big thinkers on the show, which is great. We will talk about
[00:51:35.920 --> 00:51:42.720]   the publishers in the Internet archive. Is it a library? When we come back, but first a word
[00:51:42.720 --> 00:51:50.720]   about our sponsor, eight sleep covers my mattress with the amazing pod cover. And I'll tell you,
[00:51:50.720 --> 00:51:55.600]   I don't want to live without it. In fact, this is I should warn you, if you get the pod cover,
[00:51:55.600 --> 00:51:58.160]   you're not ever going to want to leave home again. You're never going to want to,
[00:51:58.160 --> 00:52:01.680]   we're going to go travel and I'm going to miss my pod cover. I could tell you that right now.
[00:52:01.680 --> 00:52:08.480]   What is it? The pod cover fits over any mattress, allows you to adjust the temperature of your
[00:52:08.480 --> 00:52:14.400]   sleep environment, not just warmer, but cooler. So you get the optimal temperature to give you the
[00:52:14.400 --> 00:52:20.160]   best nights sleep. It's a dual zone temperature control. Lisa likes it hotter than I do. She turns
[00:52:20.160 --> 00:52:25.040]   hers up. I turned mine down. You and your partner could set different sides of the bed to temperatures
[00:52:25.040 --> 00:52:30.800]   as cool as 55 degrees Fahrenheit. That's chilly. That's like air conditioning for your bed.
[00:52:31.440 --> 00:52:38.640]   Or as hot as 110 degrees Fahrenheit. That's like heat for your bed. And I got to tell you,
[00:52:38.640 --> 00:52:43.840]   it changes. It has an autopilot. It knows the room temperature. It senses your movements,
[00:52:43.840 --> 00:52:49.120]   your breathing, your heart rate. It knows what level of sleep you're in and what it does is very
[00:52:49.120 --> 00:52:53.520]   cool. So I start mine at a nice cozy temperature, getting busy cozy. I could fall asleep fast.
[00:52:53.520 --> 00:52:59.040]   But then as I go into deeper, deeper sleep, it actually lowers the temperature, which facilitates
[00:52:59.040 --> 00:53:04.240]   deep sleep. I'm getting about 50% more deep sleep since I put the pod cover in more than a year ago.
[00:53:04.240 --> 00:53:09.680]   Then I mean, an hour and a half instead of an hour, that's a big, big difference.
[00:53:09.680 --> 00:53:18.720]   Good sleep, especially that deep sleep is a health habit. I mean, it will reduce the likelihood
[00:53:18.720 --> 00:53:23.120]   of serious health issues like heart disease and high blood pressure. It can even reduce the risk
[00:53:23.120 --> 00:53:28.400]   of Alzheimer's. And this is a health habit that's not hard to stick to. You set it up. It's
[00:53:28.400 --> 00:53:33.840]   automatic. It heats it up before you get in bed. I said, I want my bed to be ready by 9 p.m.
[00:53:33.840 --> 00:53:39.040]   because I might go to bed early. It's nice and warm when I get in and it cools down based on a
[00:53:39.040 --> 00:53:45.280]   whole bunch of inputs. It's temperature regulation. And when it's hot in the summer, it's cool.
[00:53:45.280 --> 00:53:49.840]   So you're comfortable. You never wake up in the middle of the night sweating bullets.
[00:53:49.840 --> 00:53:54.960]   When it's cool at night because of winter, it warms you up. In fact, it saves us money because
[00:53:54.960 --> 00:53:58.960]   I don't have to heat the house or air condition. The house at night, the eight sleep pod cover does
[00:53:58.960 --> 00:54:05.680]   it for us. Lisa loves it. I love it too. You wake up fully energized after a great night's sleep.
[00:54:05.680 --> 00:54:11.680]   Wouldn't you like that? Go to eight sleep.com/twit. Right now you'll save $150 at checkout on the
[00:54:11.680 --> 00:54:17.280]   pod cover. Eight sleep currently ships within the US, Canada, the UK, select countries in the EU
[00:54:17.280 --> 00:54:24.080]   and Australia. And you just had a hot, hot summer in Australia. Wouldn't it be nice to get in bed?
[00:54:24.080 --> 00:54:32.400]   And it's oh, it's so cool. So comfy. So fresh. Eight sleep, E-I-G-H-T-S-L-E-E-E-P.com/twit
[00:54:32.400 --> 00:54:36.240]   to get that $150 off and to check out the eight sleep and all the other things they
[00:54:36.240 --> 00:54:39.680]   sell there. It's a really great product. Kevin Rose told me about it years ago.
[00:54:39.680 --> 00:54:45.280]   I ignored him. Then Amy Webb got it, told me about it. I ignored her. Finally, I broke down,
[00:54:45.280 --> 00:54:49.280]   got it about a year ago. We've gone through now. This is our second winter, all summer,
[00:54:50.080 --> 00:54:55.360]   all year round. The eight sleep is great. Eight sleep.com/twit. We're on a vacation in two weeks.
[00:54:55.360 --> 00:55:00.720]   And I know it's not going to sleep as well. It's the only downside.
[00:55:00.720 --> 00:55:08.400]   Eight sleep.com/twit. We thank them so much for their support of this week in Tech Monday.
[00:55:08.400 --> 00:55:15.520]   United States Supreme Court, 1 p.m. Eastern time. We'll hear oral arguments and has shed
[00:55:16.240 --> 00:55:22.560]   versus the Internet Archive. It's a lawsuit against the Internet Archive.
[00:55:22.560 --> 00:55:26.640]   The publishers has shed just is essentially representing all publishers.
[00:55:26.640 --> 00:55:34.000]   Don't want the Internet Archive to, they say they're violating our copyrights to store books.
[00:55:34.000 --> 00:55:42.160]   Has shed HarperCollins, Wiley and Penguin House, Penguin Random House claim that the Internet
[00:55:42.160 --> 00:55:48.240]   Archive through its CDL, which is a digital control, digital lending, and allows books to
[00:55:48.240 --> 00:55:53.440]   people to check out digital copies of books for two weeks or less, just like a library
[00:55:53.440 --> 00:56:00.960]   claim that CDL, the publishers claim CDL has cost their companies millions and is a threat to
[00:56:00.960 --> 00:56:08.320]   their business. I'm betting Stephen Levy is a published author of many books. Both of you are.
[00:56:08.960 --> 00:56:15.360]   I guess I am too. Nobody wants to read my books, but your books are still on the shelves of
[00:56:15.360 --> 00:56:20.640]   libraries. Do you hate libraries? Stephen? No, no, I love libraries.
[00:56:20.640 --> 00:56:25.280]   Is the Internet Archive CDL a library, I guess is the question.
[00:56:25.280 --> 00:56:33.680]   Well, I don't know. I don't know how, whether we want to use a term of art to do it or basically
[00:56:33.680 --> 00:56:39.040]   libraries a library. I mean, you've got a lot of books. You could say, okay, it's a digital library.
[00:56:39.040 --> 00:56:45.760]   I think what happened here is the question is, if you're an author,
[00:56:45.760 --> 00:56:54.880]   you're happy when a library buys your book and lends it out. You will not be happy if a library
[00:56:54.880 --> 00:57:02.320]   buys one digital copy of your book and then let's everything, any unlimited amount of people
[00:57:03.280 --> 00:57:08.240]   get hold of it and basically just offers the ebook free to an unlimited amount of people.
[00:57:08.240 --> 00:57:14.480]   The way the Internet library worked and this run by this guy named Brewster Kale was a fantastic
[00:57:14.480 --> 00:57:22.240]   person. But during the pandemic, he thought, well, this is an emergency. So previously, we only lent
[00:57:22.240 --> 00:57:30.960]   one copy at a time, which publishers didn't like either because they're arguing it's different
[00:57:31.680 --> 00:57:36.080]   than a physical copy of the book. But even putting that aside, was the archive buying the ebook?
[00:57:36.080 --> 00:57:45.600]   No, it got the donation from a library, which did buy them, scans the whole library, and then
[00:57:45.600 --> 00:57:52.560]   has digital versions. So they never went out to the publisher and said, we're buying this ebook.
[00:57:52.560 --> 00:58:00.080]   This was an existing copy. It's like buying a big trench of used books and digitizing it and
[00:58:00.080 --> 00:58:08.480]   the library buys used books. And the agreement that libraries have with publishers is that they'll
[00:58:08.480 --> 00:58:15.680]   get a digital copy, but unlike a physical copy, they can't keep lending it out. They can only
[00:58:15.680 --> 00:58:21.120]   lend it out a certain amount of time, which is sort of like way it is now because you can't lend a
[00:58:21.120 --> 00:58:26.800]   physical book out a hundred times. They've got two P-dope to do it. So I guess as an author,
[00:58:28.080 --> 00:58:38.000]   I don't want libraries to be so easy to borrow from that there's no reason to buy a book anymore.
[00:58:38.000 --> 00:58:43.920]   I'm happy for a library to offer a copy to its members when they buy it. So people
[00:58:43.920 --> 00:58:54.240]   couldn't read the book without paying for it in a way where the rights are represented.
[00:58:54.240 --> 00:59:00.000]   But what Brewster did in particular, which I think was I think was objectionable and
[00:59:00.000 --> 00:59:05.840]   I think he went too far, was he said, you know what, previously we only let out one copy at a
[00:59:05.840 --> 00:59:10.720]   time like a physical copy, but because it's an emergency in the pandemic, we're going to get rid
[00:59:10.720 --> 00:59:17.120]   of that. So basically, if a thousand people wanted to read Facebook, which just came out
[00:59:17.120 --> 00:59:23.120]   in the beginning of the pandemic, I don't know if that was part of his collection. But then
[00:59:24.000 --> 00:59:30.000]   an unlimited amount of people can get an ebook, which is the exact same thing that they get the
[00:59:30.000 --> 00:59:35.200]   ebook that they would have to pay for otherwise, instead of having to be on a wait list maybe,
[00:59:35.200 --> 00:59:43.360]   but for library only had a few copies. And you have to be careful. If there's no incentive for an
[00:59:43.360 --> 00:59:49.360]   author to make back their money, then libraries are going to be empty of new books because
[00:59:49.360 --> 00:59:56.400]   authors won't be writing them. Has Brewster reverted to the previous method of one book at a time?
[00:59:56.400 --> 01:00:04.720]   I'm not, I think he has. Yeah. I'm not 100% sure that when I go to the archive.org site on
[01:00:04.720 --> 01:00:10.400]   control to digital lending, it says control digital lending is the CDL is the library practice,
[01:00:10.400 --> 01:00:16.640]   whereby a library owns a book, digitizes it and loans either the physical book or the digital
[01:00:16.640 --> 01:00:23.760]   copy to one user at a time. So if I guess, you know, I agree with you, if the only issue is,
[01:00:23.760 --> 01:00:31.040]   is this okay? Yes. But to do more than one copy at a time, no, even though I think
[01:00:31.040 --> 01:00:37.440]   Brewster did it with every intent of being, you know, humanitarian, I wouldn't have so much trouble
[01:00:37.440 --> 01:00:43.280]   if the Supreme Court ruled in favor of Hashet. It seems to be the people are concerned, the EFF,
[01:00:43.280 --> 01:00:49.680]   for instance, as a as filed a motion for summary judgment saying, this should be put to bed.
[01:00:49.680 --> 01:00:56.400]   They did that last year. They don't they think it's threatening both to libraries and to the
[01:00:56.400 --> 01:01:02.320]   internet archive. They say libraries have paid publishers billions of dollars. This is the EFF
[01:01:02.320 --> 01:01:07.200]   for the books in their print collections and are investing enormous resources in digitization
[01:01:07.200 --> 01:01:12.160]   in order to preserve those texts. CDL helps ensure that the public can make full use of the books
[01:01:12.160 --> 01:01:17.840]   that libraries have bought and paid for. This activity is fundamentally the same as traditional
[01:01:17.840 --> 01:01:23.840]   library lending and poses no new harm to authors or the publisher. Libraries have never been required
[01:01:23.840 --> 01:01:27.920]   to get permission or pay extra fees to lend books as a practical matter. The available data
[01:01:27.920 --> 01:01:34.400]   shows that the CDL has not and will not harm publishers bottom lines. But I see your point. I
[01:01:34.400 --> 01:01:43.520]   mean, if it's a brand. I think that this is something that I wish we had a great Congress,
[01:01:43.520 --> 01:01:53.360]   a wise Congress that can figure out what to do with this new twist that technology has offered.
[01:01:53.360 --> 01:01:59.680]   Because it's a genuine issue. I want libraries to thrive. I want people to be able to go to
[01:01:59.680 --> 01:02:10.320]   libraries and read my book. Any book they want to read. But I want to make sure that there's an
[01:02:10.320 --> 01:02:18.800]   incentive for authors to keep writing books. It is something that requires a wise solution.
[01:02:18.800 --> 01:02:25.040]   Because a digital copy of the book is not the same as a physical copy. Maybe we do need some sort
[01:02:25.040 --> 01:02:31.200]   of tweak rather than just say, "Aha, this is the way it works previously. It works the same way
[01:02:31.200 --> 01:02:39.120]   digitally." When in fact it doesn't. I'm with Stephen here. If folks have any understanding of
[01:02:39.120 --> 01:02:45.840]   how much work goes into writing a book, I think they would understand why publishers are protesting
[01:02:45.840 --> 01:02:52.960]   here. You can't just take books and make them available to so many people and then potentially
[01:02:52.960 --> 01:03:02.000]   risk a book's sales. If you're not going to have that sell, then you're not going to have
[01:03:02.000 --> 01:03:09.920]   publishers to pay authors. You're not going to have books. It's as simple as that. I understand
[01:03:09.920 --> 01:03:14.880]   the internet has made information free and there's a lot of good to that. I also think that through
[01:03:14.880 --> 01:03:22.000]   that we have tended to lose sight of the right to the creator of the content.
[01:03:22.880 --> 01:03:29.520]   The fact that the internet archive has risked libraries ability to lend books because of
[01:03:29.520 --> 01:03:34.880]   doing what they've done and an easy way to not to do it is to not have done it in the first place,
[01:03:34.880 --> 01:03:40.240]   not mess with the system that has been based on goodwill. The fact that we're here today to put
[01:03:40.240 --> 01:03:46.160]   this on publishers to me is kind of absurd. They're the ones that are producing the books in the first
[01:03:46.160 --> 01:03:53.840]   place. How about their rights? I think one sad, sad thing coming out of this is that it's charged
[01:03:53.840 --> 01:04:01.360]   that a pit authors against libraries, which is terrible because authors really love libraries.
[01:04:01.360 --> 01:04:12.400]   Generally, people who write books spent their youth in libraries. It is a fantastic collaboration
[01:04:12.400 --> 01:04:22.320]   between libraries and people which creates authors. We've all been to libraries and
[01:04:22.320 --> 01:04:34.320]   made appearances there about our books. We adore these institutions. I live in New York City and
[01:04:34.320 --> 01:04:39.040]   I'm furious at the mayor for saying it's going to cut the budget of libraries. It's terrible.
[01:04:40.160 --> 01:04:46.400]   To be completely fair to the publishers, I'm reading from insidehireed.com
[01:04:46.400 --> 01:04:51.600]   in their lawsuit against the internet archive, which could extract millions of dollars from the
[01:04:51.600 --> 01:04:56.880]   nonprofit. The publishers claim that the internet archive, "badly misleads the public
[01:04:56.880 --> 01:05:03.040]   and boldly misappropriates the goodwill that libraries enjoy and have legitimately earned."
[01:05:03.040 --> 01:05:07.840]   They're not against libraries, in other words. The publishers say the archives' efforts to
[01:05:07.840 --> 01:05:14.960]   brand itself as a library is part of a scheme to fraudulently mislead people, circumvent copyright
[01:05:14.960 --> 01:05:20.000]   law, and limit how much profit publishers can extract from the ebook market. They describe the
[01:05:20.000 --> 01:05:26.240]   internet archive as a pirate site and its business model as parasitic and illegal and characterize
[01:05:26.240 --> 01:05:32.160]   controlled digital lending as an invented paradigm that is well outside copyright law.
[01:05:33.440 --> 01:05:37.600]   I think that we'll see what the justices say tomorrow.
[01:05:37.600 --> 01:05:48.080]   I think that Bruce, I think Bruce is a good guy. I think he's off base on some of this stuff.
[01:05:48.080 --> 01:05:54.400]   But I don't really like him described as an evil person.
[01:05:54.400 --> 01:05:55.520]   No, he's not evil.
[01:05:55.520 --> 01:05:56.080]   No.
[01:05:56.080 --> 01:06:02.640]   I think he is well-intentioned and he's done an amazing thing. The internet archive,
[01:06:02.640 --> 01:06:09.200]   as an archive, has been fantastic. You can go and get things which are in public domain,
[01:06:09.200 --> 01:06:19.440]   amazing music collections and other things. I think it would also be tragic if this fight led to
[01:06:19.440 --> 01:06:27.680]   somehow really harming the mission of the archive.
[01:06:27.680 --> 01:06:34.960]   So I interviewed Bruce to, when internet archive began, Bruce was a founder of Waze. He sold Waze
[01:06:34.960 --> 01:06:38.800]   to AOL, made some money. Alexa, Alexa, not Waze.
[01:06:38.800 --> 01:06:42.080]   Oh, I thought he was Waze. He was Alexa. Okay.
[01:06:42.080 --> 01:06:47.920]   Sold it to AOL, made some money, took the money and did this thing that was pure public benefit.
[01:06:47.920 --> 01:06:53.520]   I asked him early on, I said, "Aren't you worried?" Because they were basically downloading
[01:06:53.520 --> 01:06:57.920]   as much of the internet as they could. They have petabytes of internet. The way back
[01:06:57.920 --> 01:07:03.440]   machine is a way to see sites that are long gone. He wanted to preserve this history for all.
[01:07:03.440 --> 01:07:09.040]   And he's extended that now to many other forms of media. Many of my old TV and radio shows are there.
[01:07:09.040 --> 01:07:14.000]   And I asked him, "Aren't you worried about copyright?" He said, "Well, worry about that when the time comes."
[01:07:14.000 --> 01:07:19.280]   He knew that it was in theory a violation of copyright. I think he's always wanted to be
[01:07:19.280 --> 01:07:25.760]   a library which is protected in some degree as a fair use thing. Maybe the CDL thing,
[01:07:25.760 --> 01:07:32.480]   when he changed the rules was a mistake. You would agree though, if he'd only said one
[01:07:32.480 --> 01:07:36.720]   copy at a time. We got a copy from a library. We're going to lend that one copy at a time for
[01:07:36.720 --> 01:07:39.760]   two weeks. If he'd maintained those rules, this would be okay.
[01:07:41.280 --> 01:07:52.000]   I think there's still issues about, because libraries have to read the terms when they
[01:07:52.000 --> 01:07:54.640]   license books. They don't buy ebooks. They license them.
[01:07:54.640 --> 01:07:58.480]   I think one of the issues is not merely that they're buying an e-book.
[01:07:58.480 --> 01:07:59.840]   That's a more complicated issue.
[01:07:59.840 --> 01:08:03.200]   They're buying a physical book and then digitizing it, making an e-book.
[01:08:03.200 --> 01:08:10.560]   That's the internet archives thing. It can be fair to Brewster. If an author wants to opt out,
[01:08:11.520 --> 01:08:18.080]   you could do that. You could say, "Keep me out of this." But most authors have to know about that.
[01:08:18.080 --> 01:08:21.440]   You're in by default.
[01:08:21.440 --> 01:08:27.680]   I'm sure Brewster is a good guy. The road to hell is paid with good intentions. I know it's
[01:08:27.680 --> 01:08:32.880]   cliche, but it does certainly apply here. The fact that you have so many books being able to lend
[01:08:32.880 --> 01:08:39.360]   out emergency or not, we can all agree whether it's okay or not that one book is lent out as a
[01:08:39.360 --> 01:08:43.760]   debate, but the fact that so many were being lent out without agreements, that's why we're
[01:08:43.760 --> 01:08:45.600]   here today. That's why this is a discussion.
[01:08:45.600 --> 01:08:51.520]   I'm going to have to get Brewster and some people from the internet archive on to talk about this.
[01:08:51.520 --> 01:08:56.560]   Meanwhile, tomorrow you can listen to the oral arguments. If you go to archive.org and look at
[01:08:56.560 --> 01:09:01.440]   their blog, blog.archive.org, you'll see that during the proceedings they're going to host a live
[01:09:01.440 --> 01:09:07.920]   blog hosted by Library Futures with library and copyright experts, Michelle Wu, Kyle Courtney,
[01:09:07.920 --> 01:09:12.080]   and Dave Hanson. Then they'll have a live discussion with those three afterwards.
[01:09:12.080 --> 01:09:20.240]   If you are interested in this, it would probably be worthwhile listening to these. It's fascinating.
[01:09:20.240 --> 01:09:22.880]   We've been paying attention to the Supreme Court this year because they have a lot of
[01:09:22.880 --> 01:09:28.400]   cases in front of them that will impact the internet and technology dramatically,
[01:09:28.400 --> 01:09:34.400]   including Gonzalez versus Google. This is another one. It's worth listening to.
[01:09:36.880 --> 01:09:43.120]   I don't know where I come down on this. The internet archive deserves maybe a little extra
[01:09:43.120 --> 01:09:51.040]   protection even if they did something bad because I think it's so important. I don't think anybody
[01:09:51.040 --> 01:09:56.800]   else is doing what Brewster is doing. I think that a lot of our history is lost
[01:09:56.800 --> 01:10:00.160]   or would be lost if the internet archive didn't exist.
[01:10:00.160 --> 01:10:05.760]   If it's collateral damage here, no matter the foul, if it's collateral damage here,
[01:10:05.760 --> 01:10:11.760]   that would be real tragedy. I use it in my work all the time. I'm sure Steve uses it all the time as well.
[01:10:11.760 --> 01:10:17.280]   Regular internet users love going back and seeing how websites have changed and websites that
[01:10:17.280 --> 01:10:23.760]   aren't there anymore, looking at broken links. This is definitely by no stretch of the imagination.
[01:10:23.760 --> 01:10:29.920]   This is a very important website, a very important service. It then makes you wonder why it would
[01:10:29.920 --> 01:10:32.480]   risk so much to do something that was so unnecessary. Well, that's Brewster.
[01:10:33.440 --> 01:10:39.440]   I don't think it would exist if Brewster didn't have a taste for risk or at least a willingness
[01:10:39.440 --> 01:10:47.120]   to accept risk. Here is the American Libraries collection which has 3.6 million volumes in it.
[01:10:47.120 --> 01:10:53.200]   I don't know if this is part of the dispute, but I should.
[01:10:53.200 --> 01:11:01.840]   Any book in the public domain, there's no issue at all. We have a big problem with what
[01:11:01.840 --> 01:11:10.000]   are known as orphan works, which are works that are still on copyright, but because our copyright
[01:11:10.000 --> 01:11:18.960]   is too long, a lot of people think it's under copyright, but the author is unreachable or dead.
[01:11:18.960 --> 01:11:25.440]   The book is no longer in print, and people have been asking Congress for years to come up with
[01:11:25.440 --> 01:11:31.520]   some sort of scheme where those books could become available. That hasn't happened.
[01:11:31.520 --> 01:11:38.400]   Did you request that they not carry your books, Stephen? Well, Brewster said to me,
[01:11:38.400 --> 01:11:42.640]   "Stephen, if you want to opt out," and I said, "Yeah, I'll opt out."
[01:11:42.640 --> 01:11:44.560]   Yeah, because I noticed they don't have any of yours.
[01:11:44.560 --> 01:11:50.480]   I didn't even know what opt out was a possibility of this show. Well, that's the problem.
[01:11:51.840 --> 01:11:55.440]   I think Larry Lessig proposed a solution to the orphan works, which is just say, "You want to
[01:11:55.440 --> 01:12:03.920]   renew the copyright? Give me a buck." Most orphan works would then be released to the public domain
[01:12:03.920 --> 01:12:10.080]   as a result because most authors aren't around to give them a buck. Originally, copyright required
[01:12:10.080 --> 01:12:19.840]   a renewal, but the big forces that argued for the Mickey Mouse Protection Act, whatever it was called,
[01:12:20.320 --> 01:12:33.520]   got rid of that. As an author, I feel a book that I write should not be any kind of annuity
[01:12:33.520 --> 01:12:41.360]   for grandchildren. Right. But you want to get the... It is a lot of work. I will vouch for that.
[01:12:41.360 --> 01:12:45.920]   That's why I stopped writing it. This is Stephen King's... This is the Shining.
[01:12:46.640 --> 01:12:53.440]   It is a copy scanned from the Denver Public Library. It even says, "No longer property of the Denver
[01:12:53.440 --> 01:12:58.560]   Public Library." It's clearly a scanned paperback. I would submit this is a crappy experience.
[01:12:58.560 --> 01:13:07.520]   So I have to use my account to get the rest of this. But honestly, it'd be worth it to me to pay
[01:13:07.520 --> 01:13:16.400]   five bucks to buy the Shining and then to read this on a screen in a scanned form. So I doubt that
[01:13:16.400 --> 01:13:23.280]   Stephen King lost much money based on this. But what is important is it's saved for scholars
[01:13:23.280 --> 01:13:29.680]   and historians for years and years and years, generations, right? Because it is scanned. It is
[01:13:29.680 --> 01:13:36.000]   saved. So when there are no paper books extant, it will survive. At least that's the theory.
[01:13:36.720 --> 01:13:40.240]   I don't know. I have mixed feelings about this. I'll be very interested to see. I suspect this is
[01:13:40.240 --> 01:13:49.840]   going to be an interesting argument tomorrow in front of the Supreme Court. Let's talk about TikTok.
[01:13:49.840 --> 01:14:01.200]   This has been good because I have given both of you a chance to say how you feel about two different
[01:14:01.200 --> 01:14:07.120]   issues, both of which I disagree with you on. But you've persuaded me in both cases that AI is not a
[01:14:07.120 --> 01:14:11.440]   parlor trick and that maybe Brewster did go a little too far with the library. Let's see how
[01:14:11.440 --> 01:14:19.920]   you feel about TikTok. We are kind of on the edge of the extinction of TikTok in the United States.
[01:14:19.920 --> 01:14:30.000]   According to TikTok, President Biden has ordered the FTC to force TikTok's sale and failing that
[01:14:30.000 --> 01:14:37.360]   to a US company and failing that to ban it in the US. This is of course is because TikTok is owned
[01:14:37.360 --> 01:14:41.840]   by ByteDance, which is a Chinese company and there's concern about influence the Chinese government
[01:14:41.840 --> 01:14:48.480]   might have on ByteDance and about privacy and the information gleaned from TikTok. I've said for a
[01:14:48.480 --> 01:14:52.960]   long time, well, if they really cared about that, they might shut down all the data brokers because
[01:14:52.960 --> 01:14:56.480]   the Chinese government can go right to the data brokers and get even more information than they
[01:14:56.480 --> 01:15:02.880]   get from TikTok. The propaganda thing is another issue. I mean, that's part of the presumed threat
[01:15:02.880 --> 01:15:07.760]   of TikTok is that somehow the Chinese government could influence the videos you see on TikTok to
[01:15:07.760 --> 01:15:13.520]   make you be more of a fan of the Chinese Communist Party or something. I'm not sure exactly how that
[01:15:13.520 --> 01:15:21.040]   would work. Now, TikTok is not going down easy. First of all, there is an issue about of whether
[01:15:21.040 --> 01:15:26.560]   the Chinese government would allow the sale. It's thought the Chinese government would say,
[01:15:26.560 --> 01:15:32.880]   well, absolutely not. TikTok, according to Politico, is planning to flood DC with influencers
[01:15:32.880 --> 01:15:39.040]   because there are a lot of people, a lot of young people. I don't include my son who make
[01:15:39.040 --> 01:15:45.440]   their living on TikTok. My son started putting videos on TikTok a couple of years ago of sandwich
[01:15:45.440 --> 01:15:55.040]   making under the title Salt_Hank. He's gotten to two plus million subscribers, makes a good living
[01:15:55.040 --> 01:15:59.440]   on advertising sold to that platform. He's been smart enough to... I asked him. I said,
[01:15:59.440 --> 01:16:03.840]   what are you going to do when they shut down TikTok? He said, well, I've moved. I have a million people
[01:16:03.840 --> 01:16:08.560]   on Instagram. I'm on YouTube and I've got a cookbook and a TV show on the works. So he's
[01:16:08.560 --> 01:16:13.760]   uses a launchpad, but that doesn't mean that some kid today starting out on TikTok is going to be
[01:16:13.760 --> 01:16:19.360]   able to do that. Should TikTok be shut down? A question for our panel. I'll start with you,
[01:16:19.360 --> 01:16:27.040]   Alex Kemptowitz. Oh, that's a tough one. I mean, I would not shut it down right now. I'd like to
[01:16:27.040 --> 01:16:33.200]   see a foul from TikTok before we shut it down on speculation. I can definitely see the motivation
[01:16:33.200 --> 01:16:39.440]   and do it, right? Like, why can US companies not operate in China? Well, companies from China can
[01:16:39.440 --> 01:16:49.280]   operate here. It is a bit of a disconnect here. The fears are real, right? And I think it's not
[01:16:49.280 --> 01:16:53.680]   that we're going to get Chinese Communist Party propaganda, but maybe someone in the Chinese
[01:16:53.680 --> 01:16:58.640]   Communist Party could want to create some cultural phenomenon in the US and be able to do it within
[01:16:58.640 --> 01:17:04.560]   TikTok because TikTok is a cultural center. So I think that's a real fear. But again, I'd like to
[01:17:04.560 --> 01:17:09.440]   see a foul. And the reason why I'd like to see a foul before we start to see action is for the
[01:17:09.440 --> 01:17:14.640]   reason that you bring up, which is that there are lots of people who have invested a lot of money
[01:17:14.640 --> 01:17:20.080]   into TikTok. And there's a real economy that exists on that platform. Your son included.
[01:17:20.080 --> 01:17:26.880]   And I think that if we shut it down, we could cause harm in that area and really harm small
[01:17:26.880 --> 01:17:32.080]   business owners. I know myself, like for the podcast, I invested a good deal in TikTok. And,
[01:17:32.080 --> 01:17:37.040]   of course, I've tried to move to YouTube and other platforms. And my main platform is audio.
[01:17:37.040 --> 01:17:44.240]   But we're on TikTok. And I don't like the idea that that money is now gone. So I think it would
[01:17:44.240 --> 01:17:50.320]   be premature to do it. But I do understand the rationale why. And I also think that very well,
[01:17:50.320 --> 01:17:56.160]   not very quickly, but sure enough, most Americans would forget about TikTok and move to YouTube or
[01:17:56.160 --> 01:18:01.040]   Instagram, like you were talking about, or any other app, because all the social media apps now
[01:18:01.040 --> 01:18:08.240]   resemble TikTok. In a ban would inevitably just push everybody elsewhere. And we wouldn't actually
[01:18:08.240 --> 01:18:13.520]   lose real innovation because that innovation now exists elsewhere on social media. So very,
[01:18:13.520 --> 01:18:18.640]   very tricky conversation. Again, I do think it'd be premature right now, but I understand the
[01:18:18.640 --> 01:18:25.200]   inclination. And I'm very curious to see how this plays out. Should they shut down TikTok, Stephen?
[01:18:26.960 --> 01:18:33.280]   I'm not sure. I think Alex pointed out something that I've been thinking for a while.
[01:18:33.280 --> 01:18:39.120]   I really saw this in action when I wrote about Google and its experiment in China.
[01:18:39.120 --> 01:18:46.880]   An American company operates in China. They cannot own their own subsidiary. It has to be
[01:18:46.880 --> 01:18:55.120]   more than 50% owned by a Chinese partner. And I think it'll be interesting if we said that
[01:18:55.760 --> 01:19:04.960]   in countries that have that restriction, they can operate their subsidiaries in America without
[01:19:04.960 --> 01:19:10.800]   an American partner owning more than half of it. That might be an incentive for them to release
[01:19:10.800 --> 01:19:17.680]   that unfair regulation. In terms of privacy and the other worries, I would like us to get
[01:19:17.680 --> 01:19:24.720]   some actual privacy legislation that would not only affect TikTok, but all the other social media
[01:19:24.720 --> 01:19:31.200]   companies. And maybe that would address some of the fears that we have about TikTok if we were able
[01:19:31.200 --> 01:19:39.840]   to get protections of what people do with data. And if they violated those protections and used it
[01:19:39.840 --> 01:19:45.440]   to identify dissidents of the Chinese government, go after them or whatever,
[01:19:45.440 --> 01:19:52.480]   they would be subject to penalties under American laws that affect all companies.
[01:19:53.520 --> 01:19:59.280]   What neighborhood in New York do you live in? It sounds like there's a serious catastrophe
[01:19:59.280 --> 01:20:00.400]   going on outside your house.
[01:20:00.400 --> 01:20:09.440]   Is it always like that? I'm giving you the flavor, the flavor of an urban history.
[01:20:09.440 --> 01:20:14.720]   It takes a lot of dogs in the chest that's roasting and the blood in the streets.
[01:20:14.720 --> 01:20:19.920]   Wow. Is it, but I mean, is it always like this? You're just used to it.
[01:20:20.720 --> 01:20:24.320]   You can't even tell. Well, there's a fire station down the street. Oh, there you go.
[01:20:24.320 --> 01:20:29.360]   That explains it. By the way, isn't it worth bringing up what TikTok says is the share of
[01:20:29.360 --> 01:20:34.400]   ownership of TikTok? By Dan says is the share of ownership. Well, this is an issue. Yeah, there's a
[01:20:34.400 --> 01:20:40.160]   lot of non-Chinese owners. Yeah. Right. 60% owned by global investors, 20% by employees, 20%
[01:20:40.160 --> 01:20:45.360]   by its founders. So this whole like you must divest, like how exactly does that happen?
[01:20:46.960 --> 01:20:54.800]   Chief executive Joe, Joe G. Chu will be testifying on Capitol Hill on Thursday.
[01:20:54.800 --> 01:21:01.200]   So that will be interesting. I can't imagine there's anything he could say at this point that would
[01:21:01.200 --> 01:21:09.040]   mollify Congress. Chu says that divesting TikTok from his Chinese owners wouldn't offer any more
[01:21:09.040 --> 01:21:15.120]   protection than what TikToks already proposed. They have Project Texas to move all of their data
[01:21:15.120 --> 01:21:21.680]   to Oracle. That's great. That makes me feel a whole lot better. All of the US customers data.
[01:21:21.680 --> 01:21:28.000]   They want to give the US government oversight on their algorithms. That would solve that issue.
[01:21:28.000 --> 01:21:35.600]   CFIUS, the Committee on Foreign Investment in the US, is really the group that could ban
[01:21:35.600 --> 01:21:40.480]   TikTok. And they've done this with others. I mean, Huawei has banned networking equipment.
[01:21:40.480 --> 01:21:44.080]   There are quite a few Chinese companies that American chip makers cannot deal with.
[01:21:44.080 --> 01:21:51.040]   So there's certainly a precedent for this. Well, the company, the big impetus is being funded by
[01:21:51.040 --> 01:21:59.760]   their competitors. Well, there you go. You just said the the unsetable. This is something Facebook
[01:21:59.760 --> 01:22:06.240]   would love. You just, you know, I mean, you said, Alex, that, oh, well, no problem. There's plenty
[01:22:06.240 --> 01:22:11.680]   of alternatives. Yeah, this would be great for Facebook. Do you think they're funding this?
[01:22:11.680 --> 01:22:18.480]   Absolutely. I mean, you know, can, you know, they, you know, talked about it. Zuckerberg has,
[01:22:18.480 --> 01:22:22.800]   you know, been vocal about saying this. And that's an argument they make to
[01:22:22.800 --> 01:22:28.400]   with their lobbyists, their Washington people are making, you know, they're, they're the ones
[01:22:28.400 --> 01:22:34.640]   saying, you know, how can you have this Chinese company involved in people's lives like this? And
[01:22:35.840 --> 01:22:42.000]   they and they're making their competitive goals part of a national security argument.
[01:22:42.000 --> 01:22:45.840]   Well, that's going to be tough if they end up banning TikTok, right? Because Facebook and
[01:22:45.840 --> 01:22:49.360]   Google's their number one argument for why they shouldn't be regulated and why the government
[01:22:49.360 --> 01:22:53.600]   should be looking elsewhere is like, well, look, if we're out, then it's going to be Chinese apps.
[01:22:53.600 --> 01:22:57.360]   Yeah. If the Chinese apps are gone, then the spotlight is on them. And if in fact,
[01:22:57.360 --> 01:23:01.680]   TikTok might be, you know, convenient foil. And, you know, I think that one of the things that we
[01:23:01.680 --> 01:23:05.760]   need to talk about here is that like this is happening in the middle of the syphius discussion.
[01:23:06.240 --> 01:23:10.800]   It's a negotiation. So when you hear the term like, oh, they might get banned or they have to
[01:23:10.800 --> 01:23:15.360]   divest, like I do think that could potentially be some posturing from the US government to get
[01:23:15.360 --> 01:23:21.360]   them to give in more and more and spending $1.5 billion on data security is like quite a big
[01:23:21.360 --> 01:23:26.240]   concession if you ask me. And some of the regulations that you were talking about Leo seem common sense.
[01:23:26.240 --> 01:23:32.160]   And, you know, it is interesting because then I think about the number, right? $1.5 billion need
[01:23:32.160 --> 01:23:37.840]   to be spent to share this company up. And if anything needs to have a billion and a half dollars spent
[01:23:37.840 --> 01:23:43.520]   to make sure that it's secure, then it should be okay. Maybe there are bigger questions here that
[01:23:43.520 --> 01:23:48.400]   we need to be asking because that's a whole heck of a lot of money of money to spend to ensure
[01:23:48.400 --> 01:23:54.720]   something is safe. It's interesting. There's no accident that Facebook and Twitter and YouTube
[01:23:54.720 --> 01:24:01.120]   have all tried to clone TikTok. And then for them to lobby against the closing of TikTok,
[01:24:01.760 --> 01:24:06.880]   it's not a coincidence, is it? We got an alternative come over here. But I do remember, Stephen,
[01:24:06.880 --> 01:24:14.160]   you're right. Microsoft investing $150 million in Apple in the 90s just because if Apple went
[01:24:14.160 --> 01:24:20.880]   bankrupt, Microsoft would really be a monopoly. And so, you know, there's a history there's a
[01:24:20.880 --> 01:24:29.760]   precedent. Yeah, TikTok is definitely, you know, the best argument that meta has about dominating
[01:24:29.760 --> 01:24:37.120]   the social media space. So they can, you know, say, and, you know, it's provable. Wait a minute,
[01:24:37.120 --> 01:24:41.840]   this is the most popular social network now, more popular than certainly more popular than
[01:24:41.840 --> 01:24:48.880]   Instagram. I don't know, just whether the metrics prove it more popular than Facebook itself.
[01:24:48.880 --> 01:24:53.760]   Yeah, people spend more time daily on TikTok than they ever have spent on Facebook. Now, of course,
[01:24:53.760 --> 01:24:58.400]   it's a little bit different. It's a video app versus a utility. But time spent is the holy grail
[01:24:58.400 --> 01:25:02.640]   and social networking and the fact that they're spending more time per day on TikTok. I mean,
[01:25:02.640 --> 01:25:05.520]   that says a lot. Yeah, well, they're eating their lunch, they're eating YouTube's lunch,
[01:25:05.520 --> 01:25:09.840]   eating everybody's lunch. And it's so it's very convenient that they're run by the Chinese. So,
[01:25:09.840 --> 01:25:16.160]   we can, we can, you know, invoke a little xenophobia and get rid of the biggest competition. Now,
[01:25:16.160 --> 01:25:21.920]   you said, let's wait until they do something wrong. There is something that TikTok did that
[01:25:21.920 --> 01:25:27.760]   wasn't so good. Okay, let's talk about this. They chased, they chased US journalists, they were
[01:25:27.760 --> 01:25:35.040]   trying to figure out who was leaking TikTok information to US journalists. So, in order to,
[01:25:35.040 --> 01:25:42.000]   you know, find out they looked at the location information on their servers of some US journalists.
[01:25:42.000 --> 01:25:47.040]   Isn't that exactly the kind of thing people are afraid of? I don't want to minimize what
[01:25:47.040 --> 01:25:51.760]   happened to the journalists who were involved here. But I do think that it is different. And this
[01:25:51.760 --> 01:25:56.640]   seems to me like this was the PR operation trying to figure out who was leaking inside the company.
[01:25:56.640 --> 01:26:00.560]   That is a very, very different thing. I mean, you know, it's happened elsewhere, right?
[01:26:00.560 --> 01:26:06.000]   And I think that's a very, very different thing than, you know, let's say the Chinese Communist
[01:26:06.000 --> 01:26:11.840]   Party trying to find the location information of like, you know, potential like US spies or
[01:26:11.840 --> 01:26:15.360]   something else like that, something that will really harm national security. I mean,
[01:26:15.360 --> 01:26:19.440]   sucks that happened in the Forbes or Forbes or BuzzFeed journalists, whoever they were at the time.
[01:26:19.440 --> 01:26:24.480]   I'm obviously not in favor of it. I think it's a disaster. But I don't think you ban a company
[01:26:24.480 --> 01:26:29.920]   from a country because of that situation. And they fired the employees involved and so forth.
[01:26:29.920 --> 01:26:34.640]   Exactly. So they put in safe parts to prevent that. They acknowledge that mistake was a bad
[01:26:34.640 --> 01:26:39.440]   thing. Yeah. It's not the worst case scenario. It's a bad scenario, but it's not that like red
[01:26:39.440 --> 01:26:44.000]   alarm like, Oh my God, they're listening to us in the White House situation that you would get
[01:26:44.000 --> 01:26:50.720]   concerned about and we shouldn't conflate it. Right. All right. Well, this is another one that
[01:26:50.720 --> 01:26:56.000]   there's a there's a sort of Damocles hanging over TikTok's head right now and that that
[01:26:56.000 --> 01:26:59.840]   thread is getting thinner and thinner. Do you think they're going to be band Leo?
[01:26:59.840 --> 01:27:05.840]   Let's turn the tables on you. Well, I don't I think for sure the Chinese government won't
[01:27:05.840 --> 01:27:10.880]   allow them and I don't think logistically it's very easy for them to sell the US division to an
[01:27:10.880 --> 01:27:15.600]   American company. And by the way, if you're worried about privacy, which American company would that
[01:27:15.600 --> 01:27:21.440]   be that would make you a more secure and more private Facebook, Oracle Facebook,
[01:27:21.440 --> 01:27:26.240]   didn't Microsoft for a while said, well, we were thinking about buying TikTok. I, you know,
[01:27:26.240 --> 01:27:32.560]   that doesn't reassure me particularly, but that's that's I think it's pretty much safe to say that's
[01:27:32.560 --> 01:27:37.760]   not going to happen. And in fact, I suspect this is why the federal government has said this
[01:27:37.760 --> 01:27:44.000]   because it will then give them well, we tried, but you know, they they wouldn't do it. So now we're
[01:27:44.000 --> 01:27:49.120]   going to shut them down. I think that's an ominous thing. You said, well, the Chinese shut down
[01:27:49.120 --> 01:27:55.040]   Facebook and Twitter in China, but we don't want to be like that. Do we want a great firewall of the
[01:27:55.040 --> 01:28:02.640]   US? It's not quite a great great firewall. I mean, well, it's like the beginning of the firewall
[01:28:02.640 --> 01:28:06.880]   on the top of the internet. But but I think this would open the door to say, Oh, you know,
[01:28:06.880 --> 01:28:11.040]   let's make sure no internet traffic goes to China. No, Leo, you're you're exactly right. There's a
[01:28:11.040 --> 01:28:16.320]   reason why we haven't done it to this date. And that's exactly why. Yeah. And I would not
[01:28:16.320 --> 01:28:20.160]   call I would not quarrel with any government agency that says, no, you can't have TikTok on
[01:28:20.160 --> 01:28:25.440]   your government issued phone. That's fine. That's reasonable. The armed services don't let you have
[01:28:25.440 --> 01:28:32.880]   Strava on your phone because it was used to figure out when to map the Pentagon. I understand that
[01:28:32.880 --> 01:28:38.320]   that's fine. I think as a boon for taxpayers not to have the government employees spending their
[01:28:38.320 --> 01:28:43.440]   time. Get them off TikTok. Although, you know, they're on reels anyway, or shorts, whatever.
[01:28:43.440 --> 01:28:49.200]   State of Alabama. They love YouTube band TikTok in the state and the state school.
[01:28:49.200 --> 01:28:55.680]   The state college band it even though they had TikTok accounts, their school paper at a TikTok.
[01:28:55.680 --> 01:29:00.000]   And what ended up happening is the students just said, okay, they turned off the Wi-Fi and they
[01:29:00.000 --> 01:29:06.160]   used their their phone internet to TikTok kids are. Yeah, kids are going to find a way to TikTok.
[01:29:06.160 --> 01:29:08.800]   They don't care. They don't care. They don't see a hazard.
[01:29:08.800 --> 01:29:16.400]   In the US, if you use a VPN to use TikTok, well, okay, you're just using VPN to use TikTok. If in
[01:29:16.400 --> 01:29:21.840]   China, you use a VPN to use Facebook, you could be in trouble. So, you know, I think that it is very
[01:29:21.840 --> 01:29:26.320]   different. And it's much more difficult to ban any service in the US because we do have this
[01:29:26.320 --> 01:29:31.440]   freedom in this country, which is great. That allows people to do things like I look forward to the
[01:29:31.440 --> 01:29:35.360]   Supreme Court arguments where they try to understand TikTok.
[01:29:35.360 --> 01:29:42.080]   Imagine trying to go after a student who's VPN their way into TikTok. Like, it's not just
[01:29:42.080 --> 01:29:47.120]   disaster. Yeah. Let's take a little break. Lots more to talk about with a wonderful panel.
[01:29:47.120 --> 01:29:53.200]   We've got Stephen Levy, editor at large at Wired. Stephen Levy.com for all of his great books,
[01:29:53.200 --> 01:29:57.600]   including his Facebook book. And of course, if you haven't read hackers, I don't know why.
[01:29:57.600 --> 01:30:01.760]   I have an autographed copy of hackers. It has a pride of place on my bookshelf,
[01:30:01.760 --> 01:30:06.560]   because that is the greatest book about the genesis of the technology revolution ever written.
[01:30:06.560 --> 01:30:12.000]   And as an e-max user, I very much appreciate it, Stephen. What do you use to write your books?
[01:30:12.000 --> 01:30:20.640]   Not e-max. No, no, I have been using Word, but I jettisoned Word and picked up
[01:30:22.320 --> 01:30:27.920]   Scrivener. Scrivener is awesome. Good choice. And it's because, I mean, it's good for
[01:30:27.920 --> 01:30:30.880]   articles too, because you can keep your notes next to your prose.
[01:30:30.880 --> 01:30:39.280]   Yeah, it's terrific. So I did use Scrivener for the last, for the Facebook book.
[01:30:39.280 --> 01:30:46.320]   And you get all your research on there. And even for a column, I do a column every week called
[01:30:46.320 --> 01:30:54.000]   "Plaintext" that subscribers get in their mailbox. And just Stephen for that,
[01:30:54.000 --> 01:31:03.120]   if I do an interview, I'll just drag it in the research part. And it's a very versatile program.
[01:31:03.120 --> 01:31:08.880]   Literatureandlotte.com, Scrivener. What do you use to write, Alex?
[01:31:08.880 --> 01:31:12.240]   Google Docs. I'm a Google Docs person. Wow.
[01:31:12.240 --> 01:31:16.240]   Google Docs and Apple Notes. I'm Bos Lautec as they come. Apple Notes.
[01:31:16.240 --> 01:31:19.920]   So you can just write stuff in your Apple Notes.
[01:31:19.920 --> 01:31:26.080]   So I use Apple Notes for organization and time stamping my interviews. And then I'll
[01:31:26.080 --> 01:31:31.920]   have that alongside Google Docs. So I'll like title between them, and then also outlining an
[01:31:31.920 --> 01:31:36.000]   Apple. And then move it into Google Docs and then eventually into Substack.
[01:31:36.000 --> 01:31:41.200]   Nice. Literature and Lotte seems to be down right now. I think we broke it.
[01:31:42.160 --> 01:31:43.440]   Let's not break Apple Notes.
[01:31:43.440 --> 01:31:53.840]   Our show today brought to you by, hey, good news Mint Mobile. Maybe you saw Ryan Reynolds video
[01:31:53.840 --> 01:31:58.400]   with the CEO of T-Mobile, T-Mobile's announced it's buying Mint Mobile. But good news,
[01:31:58.400 --> 01:32:03.520]   they've announced they're going to keep the thing that makes Mint Mobile so special. If saving
[01:32:03.520 --> 01:32:09.680]   more and spending less is the top of your list for 2023. I got to ask you, have you looked at
[01:32:09.680 --> 01:32:18.880]   your cell bill lately, 75, 85, 90, $100 or more, that's crazy when you could switch to Mint Mobile
[01:32:18.880 --> 01:32:27.840]   and get premium wireless service online only. That's their secret sauce for $15 a month.
[01:32:27.840 --> 01:32:36.720]   Cut your cell bill by a quarter, a third, a half. Mint Mobile lets you order from home.
[01:32:36.720 --> 01:32:42.320]   They'll send you a sim at no charge. They'll even sell you phones for anyone looking for extra
[01:32:42.320 --> 01:32:48.640]   savings this year. Mint Mobile is the place to go. And yes, they promise still 15 bucks a month.
[01:32:48.640 --> 01:32:52.640]   They ride on the T-Mobile network, which is great. You get all the benefits of T-Mobile's
[01:32:52.640 --> 01:32:57.280]   5G. It's the nation's largest 5G network. And I've noticed on my Mint Mobile phone,
[01:32:57.280 --> 01:33:05.120]   yes, I use Mint Mobile. I've noticed a lot of 5G ultra capacity cell towers. They're multiplying.
[01:33:05.120 --> 01:33:11.040]   We get it now in a little old Petaluma, which means I'm getting super high speed, $15 a month.
[01:33:11.040 --> 01:33:18.080]   See, Mint Mobile has eliminated the stores, the costs of retail, and they pass those savings on to
[01:33:18.080 --> 01:33:24.080]   you. All plans, even the $15 plan comes with unlimited talk and text plus high speed data on the
[01:33:24.080 --> 01:33:29.520]   nation's largest 5G network. If you want to use your own phone, no problem. Keep your phone number.
[01:33:29.520 --> 01:33:33.680]   You can port it right over. They'll send you the sim at no charge. Actually, they do e-sims now.
[01:33:33.680 --> 01:33:37.920]   So if you have an e-sim phone like the new iPhones, it's even easier. You could be on Mint Mobile
[01:33:37.920 --> 01:33:44.400]   before this ad is done. Premium wireless service starting at 15 bucks a month. You get your new
[01:33:44.400 --> 01:33:50.240]   wireless plan for just 15 bucks a month. Get the plan shipped to your door for free. Go to mint
[01:33:50.240 --> 01:33:56.880]   mobile.com/twit. They have unlimited plans. I got a plan that was such a good deal. Mint
[01:33:56.880 --> 01:34:05.600]   mobile.com/twit. Cut your wireless build to 15 bucks a month. For most people, 4 gigs a month
[01:34:05.600 --> 01:34:12.000]   for 15 bucks unlimited talk and text, what more could you ask for? That's incredible. So really,
[01:34:12.000 --> 01:34:17.600]   I got to ask you, why are you paying more? Mint Mobile.com/twit. And since we started advertising for them,
[01:34:17.600 --> 01:34:21.600]   it's been more than a year. I've talked to more and more people in the audience who've switched
[01:34:21.600 --> 01:34:27.440]   or are very happy. Go into one of our chat rooms, ask. Do you like Mint Mobile? Does it work for you?
[01:34:27.440 --> 01:34:34.400]   Yes, it does. So wait a minute. Now I'm getting 15 gigabytes for my 25 bucks a month. That's nice.
[01:34:34.400 --> 01:34:40.160]   They do have an unlimited plan for 30 bucks a month. Mint Mobile.com/twit. Please use that address.
[01:34:40.160 --> 01:34:46.080]   So they know you saw it here. There it is. Literature and Latte finally came up.
[01:34:46.640 --> 01:34:54.720]   Scrivener. Type writer, Ringbinder, scrapbook. Andy and Akko use so many of our writers.
[01:34:54.720 --> 01:35:00.640]   The network use it. It's a really great little tool. I'm very pleased to hear you use that. That's
[01:35:00.640 --> 01:35:06.800]   nice. When you're using Google Docs, do you use rich text or do you use plain text, Alex?
[01:35:06.800 --> 01:35:12.480]   Or markup? What's the difference? I don't know. Maybe you use markup. So you've got
[01:35:12.480 --> 01:35:16.720]   it imported in a sub stack. Don't they have a CMS? Don't they have an interface?
[01:35:16.720 --> 01:35:21.040]   They do. You just paste it into that. Okay. Just see a copy paste. Got it.
[01:35:21.040 --> 01:35:24.880]   Yeah, when it comes to tech tools, I'm about as basic as they come.
[01:35:24.880 --> 01:35:32.320]   Probably so. When I was a medium, I just loved that interface.
[01:35:32.320 --> 01:35:34.480]   They were famous for that. I actually, that was their selling point.
[01:35:35.280 --> 01:35:42.480]   I would say to have Williams, even like a year ago, I ran into a TED. I said,
[01:35:42.480 --> 01:35:50.000]   your business plan is right in front of you. Just put a couple of engineers on that and beef up that
[01:35:50.000 --> 01:35:55.840]   as to do basic word processing stuff with one click publishing to medium.
[01:35:55.840 --> 01:36:01.840]   It would be incredibly popular. There's nothing like it to come up with a beautiful document,
[01:36:01.840 --> 01:36:06.960]   so simply as what they came up with at a medium. I think he's aware of that because I remember
[01:36:06.960 --> 01:36:12.480]   when medium started, that was one of the things. They said, these CMSs, the people are forced to
[01:36:12.480 --> 01:36:15.520]   use at other publications are so crappy. We wanted to make a good one.
[01:36:15.520 --> 01:36:20.960]   But it's only for publishing on medium. They should make it better made it so people can do
[01:36:20.960 --> 01:36:29.840]   their docs on it. I would produce docs on it. I wasn't publishing, but I would send to people
[01:36:30.960 --> 01:36:37.200]   in draft form inside of the link. I've never used it, but actually maybe I have,
[01:36:37.200 --> 01:36:41.440]   because you can just write an article on medium. You don't have to go through any hoops.
[01:36:41.440 --> 01:36:45.920]   I feel like I might have used it. Jeff Jarvis loves it. I know a lot of people use it and love it.
[01:36:45.920 --> 01:36:52.960]   Are you ready for the the e-sports Olympics? The International Olympic Committee has announced
[01:36:55.520 --> 01:37:01.840]   official e-sports. They're a little weird. This is Ollie Welsh writing for Polygon.
[01:37:01.840 --> 01:37:10.560]   The Olympic e-sports series 2023, they don't quite yet have e-sports in the Olympics. But
[01:37:10.560 --> 01:37:16.560]   often they do this as they're starting to move stuff into the Olympics. There will be a qualification
[01:37:16.560 --> 01:37:23.520]   this month. Live finals in Singapore in June open to amateur and professionals. But unlike other
[01:37:23.520 --> 01:37:31.360]   e-sports, most e-sports are mobas like League of Legends, Dota 2, maybe Fortnite. The International
[01:37:31.360 --> 01:37:41.040]   Olympic Committee is eschewing the e-sports. Ollie says people actually watch for non-violent
[01:37:41.040 --> 01:37:47.280]   simulations of real-world sports games and activities. Just Dance is one of them.
[01:37:48.560 --> 01:37:54.960]   Grand Teresemo is the driving one. They have an indoor cycling trainer called Zwift, Zw, IFT.
[01:37:54.960 --> 01:38:02.960]   They have a sailing simulator called Virtual Regatta, Virtual Taekwondo, a mobile game called Tennis Clash,
[01:38:02.960 --> 01:38:10.720]   Konami's WBSCE Baseball Colin Power Pros. And of course you can't have the Olympics without
[01:38:10.720 --> 01:38:18.320]   archery. They've added another mobile game called Tick-Tack Bow to the e-sports and the IOC.
[01:38:18.320 --> 01:38:22.320]   I just can't wait until that's part of the Olympics.
[01:38:22.320 --> 01:38:28.000]   Yeah, I have a funny story about something like this. So San Francisco hosted the Super Bowl a
[01:38:28.000 --> 01:38:33.840]   couple of years ago. And as the host committee was getting ready to rumble, they hosted a day
[01:38:33.840 --> 01:38:38.400]   for potential sponsors, I think something, and lead by stadium were the 49ers play.
[01:38:38.400 --> 01:38:44.320]   And I was moderating one of those panels. And we had some media people, we had a video game,
[01:38:44.320 --> 01:38:51.840]   COO from EA Sports, and we had a former 49er player who was young, just recently retired and
[01:38:51.840 --> 01:38:58.960]   was then in media. And we had Lin Swan, who was a whole-fame, the lead receiver for the Steelers,
[01:38:58.960 --> 01:39:04.320]   who was like the MC for the day. So we get talking about esports, and I was just curious what the
[01:39:04.320 --> 01:39:10.960]   49er thought. The guy who had recently retired, I say, do you think that esports athletes,
[01:39:10.960 --> 01:39:16.080]   esports players are athletes? And he said, yeah, they're definitely athletes. In my book,
[01:39:16.080 --> 01:39:21.120]   it takes coordination and competition, and you have to be the best. And the guy from EA Sports,
[01:39:21.120 --> 01:39:26.880]   he was like, oh, yeah, definitely we believe that they're athletes. And then Lin Swan was in the
[01:39:26.880 --> 01:39:31.440]   audience just sitting there, I was like, Mr. Swan, I'm curious what you think. And he gives me a look,
[01:39:31.440 --> 01:39:36.320]   and he walks out of the room. And I was just like, oh, no, I've just like upset a whole-fame,
[01:39:36.320 --> 01:39:40.480]   you know, football player, someone who I've like looked up to is, you know, a great athlete who's
[01:39:40.480 --> 01:39:45.440]   responsible for some of the most memorable plays in NFL history. And a few minutes later,
[01:39:45.440 --> 01:39:50.880]   Swan comes back onto the stage, you know, he wasn't even sitting on the stage using the audience.
[01:39:50.880 --> 01:39:55.920]   He comes onto the stage with the chair, he sets it down, and he looks at me and takes the mic,
[01:39:55.920 --> 01:40:04.160]   and just goes, they're not athletes. And that was it. He was angry. He was mad.
[01:40:04.160 --> 01:40:10.320]   Mad as heck. Exactly. That's a sad moment. I'll never forget, because he saw he'd seen that
[01:40:10.320 --> 01:40:13.120]   this was a debate that was going to get going. And I think it was right. I think it was a
[01:40:13.120 --> 01:40:17.760]   athlete like him knew he was going to lose. He was right. Because this is obviously something that,
[01:40:17.760 --> 01:40:21.600]   you know, more and more was starting to see these esports players as athletes. Now,
[01:40:21.600 --> 01:40:27.680]   I'm still on Team Swan, but, you know, I also think, you know, I'm losing side of history here.
[01:40:27.680 --> 01:40:33.120]   Chess was also one of the sports that they're going to look at. Chess.com will be the host for
[01:40:33.120 --> 01:40:37.840]   the esports games. And there's always, you know, I've played chess since high school. There's always
[01:40:37.840 --> 01:40:41.600]   been a debate, well, our, you know, chess players aren't athletes. But, you know, you have to be,
[01:40:41.600 --> 01:40:46.960]   to be a top grandmaster, you've got to be pretty darn fit. All those super grandmasters
[01:40:46.960 --> 01:40:52.800]   work out constantly. I mean, it's, I mean, maybe they're not exactly moving when they're playing
[01:40:52.800 --> 01:40:58.240]   the game. But in order to think full concentration for four and a half hours, you try it. It ain't easy.
[01:40:58.240 --> 01:41:04.080]   Yeah. They, well, the one thing that chess has that a lot of Olympic sports don't have is
[01:41:04.080 --> 01:41:11.040]   there's a winner and a loser. That's true. And it's, I personally don't think anything is a sport.
[01:41:11.600 --> 01:41:17.600]   It's like, depends on a bunch of grades. Right. People hold up signs. And, you know, it turns out
[01:41:17.600 --> 01:41:23.360]   it's always subjective. It's just subjective. You know, corruption, you know, you could like,
[01:41:23.360 --> 01:41:28.640]   it's like the Supreme Court, you could pick how people want to vote before the performer even
[01:41:28.640 --> 01:41:34.400]   gets to it, right? Unless they fall flat on their faces, you know, it's up for grabs. So I feel that,
[01:41:34.400 --> 01:41:39.600]   uh, you know, the rest of them, it's exhibitions, right? And, you know, you could say, well, it's
[01:41:39.600 --> 01:41:45.920]   like dancing, but dancing is actually kind of an Olympic sport now. Yes. So, yeah. So basically,
[01:41:45.920 --> 01:41:51.520]   to me, let's have winners and losers. You know, and that's what athletics are all about.
[01:41:51.520 --> 01:41:55.040]   Is this guy an athlete? That's the question we got to ask.
[01:41:55.040 --> 01:42:03.280]   It's not according to Lynn Swan. That's Lynn Swan's image of an esports athlete. Exactly.
[01:42:04.480 --> 01:42:10.240]   All right. Let's talk with the guy who wrote the book, Facebook, The Inside Story. Mehta has been
[01:42:10.240 --> 01:42:14.400]   going through some big changes since you were embedded for three years. You got interviews with
[01:42:14.400 --> 01:42:19.600]   Zuckerberg with Sheryl Sandberg. Sheryl's gone now. First of all, you know, we haven't talked since
[01:42:19.600 --> 01:42:26.000]   the book came out. What is Sheryl's departure from Facebook mean? You spent a lot of time talking
[01:42:26.000 --> 01:42:30.240]   about talking to Sandberg. She was kind of adult supervision as the sense I got.
[01:42:31.200 --> 01:42:39.920]   Well, you know, that was in 2008. So, you know, Mark, in the meantime, became an adult. And I think
[01:42:39.920 --> 01:42:49.920]   that also after Cambridge Analytica and were Sheryl, you know, it was on Sheryl's watch. That was
[01:42:49.920 --> 01:43:00.000]   her part of the company where the stuff happened post-election. She was not as big a force in the
[01:43:00.000 --> 01:43:09.680]   company as she was earlier. And I feel it wasn't a surprise when she left. She's a brilliant
[01:43:09.680 --> 01:43:17.440]   executive in a lot of ways. And, you know, it wasn't a surprise at a certain point. She decided,
[01:43:17.440 --> 01:43:23.600]   let me move on with my life. I don't need this. I always believe that, you know, when she took
[01:43:23.600 --> 01:43:31.680]   that job, she said, "I'm going to spend five years here." And a series of events, you know, largely out
[01:43:31.680 --> 01:43:38.720]   of her control, kept extending that time that she spent there. First, she thought the perfect
[01:43:38.720 --> 01:43:44.320]   time to go out would be the company. There was a successful IPO. I've accomplished this, I could
[01:43:44.320 --> 01:43:51.120]   leave. The IPO actually was a disaster. People don't remember this. And, you know, by the time
[01:43:51.120 --> 01:43:56.960]   they got recovered and got back together, they figured out how to do mobile ads.
[01:43:56.960 --> 01:44:04.000]   Something tragic happened. Her husband died. She wasn't in a shape to go off for a new chapter.
[01:44:04.000 --> 01:44:11.920]   And when she was, like, back with, you know, all her passion, then the bad stuff happens.
[01:44:11.920 --> 01:44:16.880]   Post-election came with John Alytica, and she didn't want to leave in the middle of that storm.
[01:44:16.880 --> 01:44:23.200]   So I think she had to wait to get past all those things to be able to leave and say,
[01:44:23.200 --> 01:44:29.600]   "Well, you know, it was time. I had my time here. I had my impact." She admits it was bittersweet
[01:44:29.600 --> 01:44:35.120]   because of the troubles that the company had, but she's off to do something else.
[01:44:35.120 --> 01:44:41.520]   Mission accomplished. And yet, and maybe it's purely coincidence, since Sandberg's departure,
[01:44:41.520 --> 01:44:48.240]   Facebook seems to be wandering a little bit. They renamed themselves meta. They refocused from being
[01:44:48.240 --> 01:44:56.400]   a social network to the metaverse. Well, that was sort of an escape hatch for her, too. But,
[01:44:56.400 --> 01:45:01.760]   you know, when Mark said, "Okay, this is a different company now. We're based on the metaverse and
[01:45:01.760 --> 01:45:06.560]   virtual reality. That's our future. That's not Cheryl's wheelhouse." So she could, you know,
[01:45:06.560 --> 01:45:13.200]   quite legitimately say, "This is a journey that I'm not on board for. So I'm going to go up
[01:45:13.200 --> 01:45:21.040]   something else. We haven't seen what that something else is." You know, she's got a family.
[01:45:21.040 --> 01:45:25.360]   And if I were her, I wouldn't mind spending a little time with all that money.
[01:45:25.360 --> 01:45:30.160]   Right. Well, don't we think that she's a pretty ambitious person.
[01:45:30.160 --> 01:45:33.920]   Right. Don't we think that Cheryl is responsible in some ways for the position that meta finds
[01:45:33.920 --> 01:45:37.760]   itself in today, which is that it's floundering? And I'll just say that, like,
[01:45:37.760 --> 01:45:42.800]   she is part of what I've talked about as like the over-financialization of tech,
[01:45:42.800 --> 01:45:47.280]   where tech companies are trying to get every, you know, little cent out of their business and
[01:45:47.280 --> 01:45:51.280]   start to forget, you know, what they were there to do, which was to solve problems
[01:45:51.280 --> 01:45:57.520]   and improve people's lives. And the fact that Apple's anti-adtracting transparency
[01:45:57.520 --> 01:46:02.160]   project, where people are able to block tracking on their pages has worked so well,
[01:46:02.880 --> 01:46:08.000]   has been a product of, you know, the fact that people were creeped out by what her
[01:46:08.000 --> 01:46:11.360]   ad division was doing, that they had to track every last little thing,
[01:46:11.360 --> 01:46:16.960]   as opposed to sell more broader ad categories. And so like now you have meta in a position where,
[01:46:16.960 --> 01:46:21.120]   you know, so many people have opted out that their tracking has effectively broken.
[01:46:21.120 --> 01:46:23.040]   And that's why they're handicapped against competitors.
[01:46:23.040 --> 01:46:23.760]   They blame Apple.
[01:46:23.760 --> 01:46:25.280]   And Apple's ATT.
[01:46:25.280 --> 01:46:30.320]   Right. But it's also like Apple did, of course, it's Apple's Apple facilitated it.
[01:46:30.320 --> 01:46:35.040]   But what inspired some, well, first of all, what inspired Apple's program to be so successful
[01:46:35.040 --> 01:46:40.560]   and what inspired so many users to tell them to tell Apple, don't track me when you're on Facebook.
[01:46:40.560 --> 01:46:44.640]   And to do that. And then, of course, like you're, you sort of set the conditions
[01:46:44.640 --> 01:46:46.160]   for where like this thing would work.
[01:46:46.160 --> 01:46:53.520]   I wonder though, what Facebook's future holds because they've spent,
[01:46:53.520 --> 01:46:58.960]   they're spending huge amounts of money, 10 billion dollars or more a year to create this metaverse.
[01:46:59.840 --> 01:47:06.320]   They seem to be turning their backs on the social network, but so do people, so do users.
[01:47:06.320 --> 01:47:12.080]   And it's not just a privacy issue. I think to some degree, you've written about this, Alex,
[01:47:12.080 --> 01:47:18.400]   it's the end of social and to some degree entirely, that people are turning to content sites.
[01:47:18.400 --> 01:47:22.080]   That was why TikTok, which isn't really social. It's really about content and YouTube.
[01:47:22.800 --> 01:47:32.160]   And even now, Twitter are becoming places you go to look at stuff, not to post stuff, right?
[01:47:32.160 --> 01:47:36.080]   That's right. Yeah. I have an episode of big technology podcast with Kevin
[01:47:36.080 --> 01:47:40.960]   Systrom, the founder of Instagram coming up about this, where we talk about the homogenization
[01:47:40.960 --> 01:47:45.680]   of social media, how TikTok looks like YouTube and YouTube looks like Instagram and Instagram
[01:47:45.680 --> 01:47:50.160]   looks like Facebook and Facebook looks like Twitter. And all of these sites are starting to look
[01:47:50.160 --> 01:47:55.840]   exactly the same. And that initial social media dream where you would get content from your
[01:47:55.840 --> 01:48:01.600]   friends, the stuff that they recommend, that's fallen by the wayside. So this is obviously the
[01:48:01.600 --> 01:48:06.160]   main challenge that Facebook is going to have to navigate. It's going to be the product question.
[01:48:06.160 --> 01:48:10.400]   And they have billions of users, so there's a good chance that they can hold their own against
[01:48:10.400 --> 01:48:15.120]   TikTok or maybe TikTok gets banned like we spoke about. And that could be the solution.
[01:48:15.840 --> 01:48:22.640]   But yeah, they sort of moved away from their main product, which is social, which,
[01:48:22.640 --> 01:48:27.840]   okay, I definitely in favor of big tech companies saying that our flagship product is not going to
[01:48:27.840 --> 01:48:33.280]   get us there tomorrow, the way that it did today. But maybe the pivot was more TikTok related stuff
[01:48:33.280 --> 01:48:38.400]   unless social related stuff. Maybe the pivot was not metaverse, right? Like I keep thinking that
[01:48:38.400 --> 01:48:42.960]   we're just weeks away from meta changing its name from meta to like AI and.
[01:48:44.160 --> 01:48:48.880]   That's what I'm wondering. I mean, did meta make a bad bet? Did they bet on the wrong horse?
[01:48:48.880 --> 01:48:53.280]   It depends on the timeframe. I think over time, they were going to be proven right on the
[01:48:53.280 --> 01:49:00.000]   metaverse, but in the short term, they won't. And how many years do you know, if the metaverse is
[01:49:00.000 --> 01:49:04.560]   30 years away versus 10, which I think is probably more like 30 years away versus like two or three
[01:49:04.560 --> 01:49:10.560]   or five or even 10, then you're going to look really silly. Just, you know, your flagship business
[01:49:10.560 --> 01:49:15.280]   is not going to be able to support that type of thing for that long of a time span.
[01:49:15.280 --> 01:49:22.640]   Yeah, I think interesting. I think a meta is, you know, buying to be the general magic of the
[01:49:22.640 --> 01:49:28.800]   metaverse. That's both good and bad. It depends which era of general magic you're talking about.
[01:49:28.800 --> 01:49:35.360]   Well, the era of general magic, I mean, it didn't succeed because it had everything right, but it
[01:49:35.360 --> 01:49:44.000]   was too soon. And they're going to developing the tools of the metaverse way before it can be
[01:49:44.000 --> 01:49:50.560]   delivered, you know, in a package where the masses could, we're going to be able to use it. And,
[01:49:50.560 --> 01:49:55.520]   you know, Alex's point about, you know, there was a time, about a year ago, I mean, we were saying
[01:49:55.520 --> 01:50:02.480]   metaverse, metaverse, metaverse. And now it's AI, AI, AI, and I think AI is more real than the
[01:50:02.480 --> 01:50:07.920]   metaverse. And that's difficult. And maybe AI helps the metaverse, right? Because AI could
[01:50:07.920 --> 01:50:13.040]   create like the intelligent non player characters that all of a sudden makes you want to wander
[01:50:13.040 --> 01:50:18.000]   around some virtual worlds. But Zuckerberg says endlessly that the metaverse is going to be a
[01:50:18.000 --> 01:50:27.120]   social experience. To me, I felt that meta Facebook just left a lot of chips on the table
[01:50:27.840 --> 01:50:32.560]   about improving the social experience. You know, can you look at what they've done? They've done
[01:50:32.560 --> 01:50:40.240]   almost nothing to really improve the experience that they first introduced and got billions of
[01:50:40.240 --> 01:50:49.840]   people to sign on to. So I feel that the creative energy of the company moved away from that vision
[01:50:49.840 --> 01:50:55.360]   of building the social graph and, you know, enhancing people's experience with, you know,
[01:50:55.360 --> 01:50:59.520]   other people that they knew. There's a lot of really simple things that
[01:50:59.520 --> 01:51:05.520]   they're still begging to be done on that service. You ever try to find someone you knew
[01:51:05.520 --> 01:51:12.720]   on Facebook, right? And you can't do it because, you know, you know, like 100 people have that same
[01:51:12.720 --> 01:51:20.000]   name. And it's just, it's just not easy to do. And there's a reason, there's a reason, though,
[01:51:20.000 --> 01:51:24.400]   they moved away from social. It's just because if you're eventually going to be content companies,
[01:51:24.400 --> 01:51:29.600]   right, which these feeds, they became content companies, the people you know, or ordinary folks
[01:51:29.600 --> 01:51:33.600]   are just not going to be the ones that are going to be there to entertain you. It's going to be the
[01:51:33.600 --> 01:51:38.800]   AI generated folks, and it's going to be professional creators. People, why be an entertainment company?
[01:51:38.800 --> 01:51:42.000]   You know, well, that's the question is whether there was a choice or not, if you were going to
[01:51:42.000 --> 01:51:47.040]   be as big as you were, because the value came from the time spent on the newsfeed. And if you're
[01:51:47.040 --> 01:51:52.000]   just a utility to like a phone book utility, then you're worth way less. So from a business
[01:51:52.000 --> 01:51:55.840]   standpoint, maybe it's just like not a prudent decision. But I can be all
[01:51:55.840 --> 01:52:00.640]   if you're worth billions of dollars, you know, your job is to come up with something that you or I
[01:52:00.640 --> 01:52:05.120]   can't figure out to make that social experience valuable. But I think you're just, I think it's
[01:52:05.120 --> 01:52:09.760]   an over rating of people's ability to be interesting or desire to be interesting to others.
[01:52:09.760 --> 01:52:16.480]   No, if you know people, you're spending time with them. I don't spend my time with Bruce Springsteen.
[01:52:16.480 --> 01:52:23.040]   You know, I can, you know, I can like watch his videos on YouTube, but I would do it.
[01:52:23.040 --> 01:52:29.280]   But you're not, but you can't. But anyway, you can't. He's not going to be as nice, nice to me as
[01:52:29.280 --> 01:52:34.000]   my friends are. You can watch him. You can watch him, just like you can listen to an album. But yeah,
[01:52:34.000 --> 01:52:38.720]   I think you're right, Steven. So that means there's a gap. There's an opening for somebody to create a
[01:52:38.720 --> 01:52:43.920]   social network. Since Facebook is there are there are a number of places that are saying,
[01:52:43.920 --> 01:52:49.600]   hey, we're going to fill this spot. Alex is right that, you know, if you're appetite is so voracious,
[01:52:49.600 --> 01:52:56.960]   you need to become a different kind of company. But the fact is that meta is in a great position
[01:52:56.960 --> 01:53:04.480]   to compete with, with TikTok and other company and YouTube and being an entertainment company.
[01:53:04.480 --> 01:53:11.600]   That's not their DNA. And as it turns out, they haven't even been able to successfully
[01:53:11.600 --> 01:53:17.120]   leverage their, you know, huge number of people to be able to wipe out these other companies.
[01:53:17.120 --> 01:53:21.520]   Well, the worst thing you can do is be sort of caught in the middle, which means that you're a
[01:53:21.520 --> 01:53:29.040]   subpar entertainment company and a social company where you put updates of, you know, crappy updates
[01:53:29.040 --> 01:53:34.080]   from people or people are you're so you so diminished the social side that people don't want to post
[01:53:34.080 --> 01:53:37.840]   anymore. And I think that's exactly where meta is. That's their betwixt in between. You're right.
[01:53:37.840 --> 01:53:44.400]   They're neither, neither nor, they're, they're fish, nor foul. What an opportunity. I feel like
[01:53:44.400 --> 01:53:51.200]   the metaverse is not an opportunity. Apple is reportedly about to launch its VR headset in the
[01:53:51.200 --> 01:53:56.880]   next few months. And it's interesting because there was an article, I think in the journal suggesting
[01:53:56.880 --> 01:54:02.640]   that the, the designers said, no, it's not ready. You shouldn't release a VR headset wait till we can
[01:54:02.640 --> 01:54:08.800]   do AR, which is years away. And that Tim Cook and the management decided, no, we've got to do this
[01:54:08.800 --> 01:54:12.880]   now. Let's take a break. When we come back, we could talk about that. We still haven't talked
[01:54:12.880 --> 01:54:18.480]   about Silicon Valley Bank. And I don't think that's over. And I think you guys agree. So we'll talk
[01:54:18.480 --> 01:54:23.120]   about that as well. We've got lots to say. And two great people to talk about it with Stephen
[01:54:23.120 --> 01:54:32.240]   Levy, Stevie Levy calm editor at large at Wired. One of the the great writers of about technology for
[01:54:32.240 --> 01:54:39.760]   the last, what, three decades almost? Yeah. Wow. And of course, one of the new young bright lights
[01:54:39.760 --> 01:54:44.880]   of technology journalism, big technologies, Alex Cantrowitz. Great to have you both.
[01:54:44.880 --> 01:54:49.840]   Our show today brought to you by Miro. We've been playing with Miro at Twit and love it.
[01:54:49.840 --> 01:54:56.880]   We were in a situation, I think a lot of you are in going from tab to tab and your browser tool to
[01:54:56.880 --> 01:55:04.240]   tool on your toolkit. And meanwhile, losing ideas with those context switches and
[01:55:04.240 --> 01:55:08.800]   leaving important information on the table, there's a better way. The problem is it's very
[01:55:08.800 --> 01:55:14.160]   hard for me to explain what it is, except to tell you it's called Miro, M-I-R-O dot com
[01:55:14.160 --> 01:55:21.280]   slash podcast. Miro. Well, on the face of it, it's a collaborative visual whiteboard,
[01:55:21.280 --> 01:55:26.560]   a place you and your team can go to brainstorm to work together to collaborate,
[01:55:26.560 --> 01:55:30.960]   brings all of your great work together, no matter where you are in the world. And no matter what
[01:55:30.960 --> 01:55:35.040]   time, since you can go in at night and somebody else can go in the next day, whether you're working
[01:55:35.040 --> 01:55:41.520]   from home in a hybrid workspace, it all comes together one place online, that Miro. But to say
[01:55:41.520 --> 01:55:49.440]   it's a whiteboard, I feel like almost diminishes what Miro is. Miro is amazing, because it can do
[01:55:49.440 --> 01:55:54.560]   almost anything, which makes it hard for me to describe. We use Miro and ask the tech guys for our
[01:55:54.560 --> 01:55:59.920]   show rundown. Once you start using Miro, you're probably not going to want to ever
[01:55:59.920 --> 01:56:06.640]   do a meeting without Miro's timers. Maybe the best thing to do is to point you to the Miroverse.
[01:56:06.640 --> 01:56:15.040]   This is a collection of templates you can use created by Miro users, people who work at Atlassian,
[01:56:15.040 --> 01:56:22.400]   the UK government, Zendesk, all over the world. Here's from Stephen Sampson-Jones,
[01:56:22.400 --> 01:56:27.760]   who works for the UK government. He's the head of Agile Delivery and has made quite a few,
[01:56:27.760 --> 01:56:34.080]   I think, fascinating templates, including the Treehouse of Horror Retrospective,
[01:56:34.080 --> 01:56:40.880]   a retrospective of Marvel comic books, a retro arcade, a Harry Potter and SpongeBob retrospective.
[01:56:40.880 --> 01:56:48.320]   This guy is, I think, a good guy to check out because it shows you what you can do with Miro.
[01:56:48.320 --> 01:56:54.400]   With Miro, you can integrate with many, many other services. For instance, we use Zapier
[01:56:54.400 --> 01:57:00.400]   with Miro so that we can trigger actions that put stuff into Miro like questions or getting from
[01:57:00.400 --> 01:57:06.720]   the e-mail or the chatroom or videos. It's an infinite collaborative whiteboard
[01:57:06.720 --> 01:57:14.400]   that's a perpetual space. You can drag in any kind of data or content. Nothing is lost or
[01:57:14.400 --> 01:57:20.320]   forgotten. You can create templates, of course, but it's also open and freeform. So your ideas can
[01:57:20.320 --> 01:57:28.240]   grow and expand. They're not limited. It is an amazing tool. It covers a breadth of use cases,
[01:57:28.240 --> 01:57:33.680]   visual assets. You can present findings. You can run brainstorms with cross-functional teams.
[01:57:33.680 --> 01:57:40.000]   The Miro timers and icebreakers and brainstorming tools make it really much better to have a
[01:57:40.880 --> 01:57:46.240]   conference call. You can build that if you're doing a new product, a build-edge product vision
[01:57:46.240 --> 01:57:51.040]   on a Miro board, brainstorm with sticky notes or comments or live reactions. There's a voting
[01:57:51.040 --> 01:57:57.200]   tool. There's a timer. Express yourself in creative ways, bring the whole group together around
[01:57:57.200 --> 01:58:02.720]   one idea. The point is there's one place you can all go to see what's going on to update it to
[01:58:02.720 --> 01:58:07.920]   contribute. And you can do it in any way you like. With a can-band board, if you're agile,
[01:58:07.920 --> 01:58:13.040]   if you want wire frames, do that. You can draw with a pen tool. You can paste in images. You
[01:58:13.040 --> 01:58:20.000]   want to do a mood board. Easiest place in the world. Want to do mockups? No problem. Miro users
[01:58:20.000 --> 01:58:27.040]   love it 80 hours per user per year saved by streamlining conversations, cutting down on meetings.
[01:58:27.040 --> 01:58:33.440]   Miro gives your team the chance to always stay connected to real-time information.
[01:58:33.440 --> 01:58:37.520]   And it gives project managers and the product leads a bird's eye view of the whole product.
[01:58:38.320 --> 01:58:44.000]   You can zoom in or out to ensure nothing slips through the cracks. The only problem I have is
[01:58:44.000 --> 01:58:49.040]   how do I describe what it can do? It can do so much. It's a whiteboard, but that doesn't even
[01:58:49.040 --> 01:58:54.720]   scratch the surface. Is anything you want it to be everything you need? The Miro integrations
[01:58:54.720 --> 01:58:59.680]   mean it works with everything you're already using. They've got apps for iOS and Android and
[01:58:59.680 --> 01:59:07.920]   Windows and Mac. I just think it's fantastic. Miro.com/podcast. Give it a try. M-I-R-O.com/podcast.
[01:59:07.920 --> 01:59:14.480]   We're going to make that easy because your first three boards, one to three boards, are free.
[01:59:14.480 --> 01:59:22.080]   Your first three boards for free start working better. Miro.M-I-R-O.com/podcast.
[01:59:22.080 --> 01:59:27.760]   And do take a look at the Miroverse because that's a really great way of seeing what you can do.
[01:59:27.760 --> 01:59:35.680]   I mean, it's limitless. It's amazing. Miro.com/podcast. We thank them so much for
[01:59:35.680 --> 01:59:40.480]   sponsoring our show. We appreciate that, Miro. We thank you for trying them out.
[01:59:40.480 --> 01:59:47.280]   And if you do, please use that address because that's how they know you saw it here. Miro.com/podcast.
[01:59:47.280 --> 01:59:54.000]   I, earlier today, I asked the tech guys, I likened the Silicon Valley Bank debacle
[01:59:54.880 --> 02:00:03.600]   to a game of Jenga where the FDIC, the Fed, have pulled out one little piece of wood. But now the
[02:00:03.600 --> 02:00:11.680]   whole tower is kind of swaying. And we're waiting to see what happens next. I, both of you, have
[02:00:11.680 --> 02:00:19.520]   written about it. Stephen, we'll start with your story, The Ugly Lessons of Silicon Valley Bank's
[02:00:19.520 --> 02:00:26.400]   Collapse. One of the stories that came out, I think, today is that there have been
[02:00:26.400 --> 02:00:32.160]   intimations that they were taking too much risk for years, that they've even been on an
[02:00:32.160 --> 02:00:39.840]   investigation for some time now. What do we learn? What have we learned from this? And is this going to be
[02:00:39.840 --> 02:00:45.600]   a problem going forward? Credit Suisse now is kind of shaking.
[02:00:45.600 --> 02:00:51.440]   Right. Well, we learned a few things. And none of them, as my headline flexed,
[02:00:51.440 --> 02:00:59.440]   are pretty. One thing that struck me was how the bank really operated as a cheerleader for the whole
[02:00:59.440 --> 02:01:11.200]   ecosystem of founders and funding. And they threw parties. They had a lot of clients in the wine
[02:01:11.200 --> 02:01:19.520]   industry. So they would sort of cross pollinate those with the tech people and share the
[02:01:19.520 --> 02:01:24.960]   great vintage wines with them and go into their offices, some of the executives of the bank would
[02:01:24.960 --> 02:01:32.720]   have wine fridges. And I mentioned that saying, is this sort of behavior really what you want from
[02:01:32.720 --> 02:01:40.080]   your fiduciaries? When I think of a bank I want to trust, I think of the guy who hired Mary Poppins.
[02:01:40.160 --> 02:01:45.120]   Right? You know, a banker. A banker, yes. A bowler and a totally
[02:01:45.120 --> 02:01:51.520]   frilled umbrella. Yeah. Yeah. And he says, I know where everything is in a 620. I'm going to come
[02:01:51.520 --> 02:01:58.800]   home and then going to be greeted. And that's sort of the boring kind of person you want in charge of
[02:01:58.800 --> 02:02:05.520]   your finances. So the other is these people who, like say they embrace risk all the time,
[02:02:05.520 --> 02:02:13.520]   as it turns out, they weren't cognizant of this kind of risk. It was unavoidable. Any idiot knows
[02:02:13.520 --> 02:02:20.000]   what the limit is in what you're insured about. And then when it happens, they sort of do a 180
[02:02:20.000 --> 02:02:24.720]   from their normal point of view of saying, you know, governments stay out of it and saying,
[02:02:24.720 --> 02:02:31.520]   hey, the government should like now forget about this limit and, you know, go from $250,000
[02:02:32.560 --> 02:02:38.000]   to reimbursement to infinity. Because, you know, otherwise our payrolls can be lost.
[02:02:38.000 --> 02:02:42.800]   Well, this is an industry that's laid off over 100,000 people, right? And, you know,
[02:02:42.800 --> 02:02:47.760]   the fate of those people is sort of shrugged off. And now all of a sudden, you know,
[02:02:47.760 --> 02:02:53.680]   meeting a payroll and having people not paid for a day or two is, you know, going to collapse the
[02:02:53.680 --> 02:03:03.120]   economy. So it's played sort of the, you know, insularity and, you know, hypocrisy, I guess,
[02:03:03.120 --> 02:03:09.360]   of the Valley and, you know, can there rush to, you know, really create a bank run
[02:03:09.360 --> 02:03:15.920]   from this place where they've loyally supported for a number of years, you know,
[02:03:16.720 --> 02:03:25.840]   show that this, you know, one from all for one community, you know, really, you know, fell apart
[02:03:25.840 --> 02:03:32.800]   when the bullets start to fly. Yeah. But you would agree that the Fed and the FDIC did the right
[02:03:32.800 --> 02:03:40.880]   thing to backstop all those deposits or no? Well, I'm not sure. I think that, you know, maybe there
[02:03:40.880 --> 02:03:48.400]   was a way to do that where they could have, you know, maybe had a much higher limit or said, you know,
[02:03:48.400 --> 02:03:55.920]   at a certain point, you know, that all your deposits aren't guaranteed for this day. But in
[02:03:55.920 --> 02:04:01.120]   meantime, by then, things had gotten to the point where, you know, they were saying, well, the whole
[02:04:01.120 --> 02:04:07.600]   economy is at risk of people, deposits aren't going to be protected. And the fact is, you talk about
[02:04:07.600 --> 02:04:13.840]   hallucinations in GPT-4, you know, the whole monetary system is kind of a hallucination that, you know,
[02:04:13.840 --> 02:04:19.760]   everyone sort of accepts the fact that if nothing happens, if people don't pull up their money right
[02:04:19.760 --> 02:04:29.360]   away, banks are okay. But the fact is that, you know, if things could turn on a dime and we could
[02:04:29.360 --> 02:04:39.280]   be facing giant problems in the economy, because we all of a sudden, we abandon our shared agreement
[02:04:39.280 --> 02:04:46.640]   not to pull up money from the bank. Right. Alex, your article talks about, and you mentioned this
[02:04:46.640 --> 02:04:52.800]   earlier, the over-financialization of tech and the SBB bank, what do you mean? What's that all about?
[02:04:52.800 --> 02:04:58.880]   Well, look, one of the things that I saw that I really sort of made my antennas go up was that
[02:04:58.880 --> 02:05:04.720]   there was, you know, the public largely supported the bailout, but there was a loud enough and big
[02:05:04.720 --> 02:05:10.560]   enough chunk of the public who were willing to let the bank fail, that it was worth examining who
[02:05:10.560 --> 02:05:17.760]   they were and why and what has happened in tech, that it began as this underdog industry, one that,
[02:05:17.760 --> 02:05:23.600]   you know, was there fighting the establishment, one that you could root for, that was fun,
[02:05:24.640 --> 02:05:29.840]   that everyone would support, basically, unless you were like the establishment would support,
[02:05:29.840 --> 02:05:35.200]   you know, keeping, keeping vibrant and keeping healthy and why would anyone then say that,
[02:05:35.200 --> 02:05:42.080]   okay, well, we should have, have these companies fail, they're just, you know, rich bastards anyway.
[02:05:42.080 --> 02:05:47.280]   But that's what's happened. But look at Elon Musk as a perfect example. Right. The transition
[02:05:47.280 --> 02:05:55.280]   from, you know, the, the stark industries, genius, transforming by being a rebel, by being one of
[02:05:55.280 --> 02:06:00.000]   the crazy ones, the people who think different, changing the world, making the world a better place,
[02:06:00.000 --> 02:06:06.080]   to just another bad shit, crazy billionaire. Right. So I think that it goes beyond Elon,
[02:06:06.080 --> 02:06:09.760]   though, and this is with a point that I was making in the story, and we talked about it on the show
[02:06:09.760 --> 02:06:15.760]   also, is that tech has been like most much of the economy over financialized, right, which is that
[02:06:15.760 --> 02:06:20.560]   it has in some, not everyone, of course, but some companies have just tried to squeeze every ounce
[02:06:20.560 --> 02:06:26.720]   of money onto the balance sheet, no matter, not really paying attention to the cost. One example,
[02:06:26.720 --> 02:06:31.600]   which we know is that door dash, right, would take tips that would get for dashers and then
[02:06:31.600 --> 02:06:37.840]   keep them down towards its minimum payment to the dash, they would never see the tip. And I have to
[02:06:37.840 --> 02:06:42.640]   say that, like, when you do this type, when you do that stuff, there is a downstream risk, people
[02:06:42.640 --> 02:06:47.200]   are going to see it eventually. And I think you started to see it when, you know, everybody,
[02:06:47.200 --> 02:06:54.080]   you know, in the US, at least knows an Amazon worker knows a Dasher knows an Uber driver, right.
[02:06:54.080 --> 02:07:00.480]   And when those when that squeezing happens, there's a political cost to it, which you end up having
[02:07:00.480 --> 02:07:07.520]   people who are saying, okay, like, let the bastards fail. And, you know, they're not, they don't care,
[02:07:07.520 --> 02:07:12.560]   like, necessarily, this was the text, text argument, right, your money's not going to go to door dash,
[02:07:12.560 --> 02:07:16.160]   but what they're saying is, well, the whole door dash system is corrupt. And the door dash,
[02:07:16.160 --> 02:07:20.480]   of course, has fixed that, right, after some some press attention, because it got caught.
[02:07:20.480 --> 02:07:25.760]   Yeah, exactly. It exists all over. And I think that, you know, for those of us who root for tech,
[02:07:25.760 --> 02:07:31.360]   I root for tech to be a strong industry. But what what, you know, people want from tech is that
[02:07:31.360 --> 02:07:37.360]   it provides all this benefit it can provide through computing, through technology in a way that
[02:07:37.360 --> 02:07:43.440]   isn't needlessly extractive. And the fact that some core, Gordon's of the industry have become
[02:07:43.440 --> 02:07:49.040]   needlessly extractive has cost so much political will that there was a very vocal group of folks
[02:07:49.040 --> 02:07:54.320]   who said let it fail. And in fact, when it was coming into the White House, right, with discussions
[02:07:54.320 --> 02:07:58.800]   of like when this thing should be bailed out, it took the whole weekend before Biden eventually
[02:07:58.800 --> 02:08:04.960]   gave in. He was aware of the political backlash of, of course, bailing out billionaire bailout.
[02:08:04.960 --> 02:08:07.840]   Yeah, that was what he was worried about. And it doesn't have to be that way, because
[02:08:07.840 --> 02:08:11.840]   at its best tech is for the little person. And that's all the whole thing. And I talked to my
[02:08:11.840 --> 02:08:15.680]   story a little bit about New York Tech meetup. I don't know for either of you who've been there,
[02:08:15.680 --> 02:08:20.800]   but it was this amazing meetup. It still goes on to some extent. But at its height in the early 2010s,
[02:08:20.800 --> 02:08:26.400]   we'd have local startups come in and just kind of show off what they built. And it was right after
[02:08:26.400 --> 02:08:31.760]   the financial crisis. So you started to have some people from Wall Street, seep in to try to get
[02:08:31.760 --> 02:08:38.720]   second careers inside tech. And whenever you spoke about your finances, your VC fundings,
[02:08:38.720 --> 02:08:45.120]   the whole place would boo and people would shout, get to the demo, right? And that's sort of what I
[02:08:45.120 --> 02:08:50.080]   think the going back to that spirit of get to the demo show us the tech. Like, I think that's what
[02:08:50.080 --> 02:08:54.320]   we need to get back to in some ways to some degree. This is what happens in all industries, right? I
[02:08:54.320 --> 02:09:01.600]   mean, in the early days of of oil, the wild catters and the crazy guys digging oil wells and
[02:09:01.600 --> 02:09:06.560]   having gushers. And eventually it's successful. It monetizes. It's a bunch of fat cats with
[02:09:06.560 --> 02:09:11.680]   monocles and cigars. That's what happens with any successful industry, right? Stephen, I'm sorry.
[02:09:11.680 --> 02:09:18.480]   Yeah, from, you know, for many years, the overwhelming narrative of tech was David versus Goliath.
[02:09:18.480 --> 02:09:24.720]   And then at a certain point in the past few years, it's changed, you know, it's changed to,
[02:09:24.720 --> 02:09:32.880]   it was changed to the Icarus myth. Too close to the sun. Yeah, it's wings are melting.
[02:09:32.880 --> 02:09:42.320]   Yeah, I mean, Alex, you make your living kind of talking about big technology. And the threat
[02:09:42.320 --> 02:09:48.240]   in some ways that the big technology can pose to all of the rest of us. What's the solution?
[02:09:50.480 --> 02:09:56.160]   I mean, it's a simple solution, right? To look, tech is big business now. No denying that, right?
[02:09:56.160 --> 02:10:00.640]   Because it's successful. Exactly. It's successful. And I do think that,
[02:10:00.640 --> 02:10:04.640]   look, I think once you saw the first IPO is you saw the first public companies,
[02:10:04.640 --> 02:10:11.840]   you saw the first trillion dollar company that became so alluring, right? The first CEOs, it became
[02:10:11.840 --> 02:10:17.360]   the mark was not what your company can do. But the mark was how, what was your net worth? What
[02:10:17.360 --> 02:10:22.800]   was your market cap? How much had you raised? These that all became the successful parts of
[02:10:22.800 --> 02:10:29.600]   the successful markers in the tech industry. And obviously, that's what shifted the industry's
[02:10:29.600 --> 02:10:34.240]   perception among other people. And so I think it's very simple. I think it's step back,
[02:10:34.240 --> 02:10:39.280]   focus again on the customer, focus again on tech. And again, get to the demo. I mean,
[02:10:39.280 --> 02:10:43.360]   let's see that. And that's sort of what's interesting because we're having, we're having
[02:10:43.360 --> 02:10:48.560]   this amazing moment right now where we're seeing so many cool things happen with AI. And in some
[02:10:48.560 --> 02:10:52.880]   ways, tech is getting back to the demo in that area, right? We're going from zero to one,
[02:10:52.880 --> 02:10:59.440]   not one to, you know, and or whatever it is. And that's exciting to me. But I do think that,
[02:10:59.440 --> 02:11:04.400]   you know, we, I think that it's just something that the tech industry needs to keep in mind.
[02:11:04.400 --> 02:11:08.320]   And maybe it needs new spokespeople, right? I mean, that's another thing that we're hearing
[02:11:08.320 --> 02:11:14.400]   about now. Tech has been represented by some very loud, obnoxious people, people like Mark
[02:11:14.400 --> 02:11:19.920]   Andreessen, who I don't think really puts the best image for, you know, on the tech industry.
[02:11:19.920 --> 02:11:24.480]   And I would actually say his entire firm. And I think like right now, a lot of people, a lot of
[02:11:24.480 --> 02:11:28.640]   people, you know, who've been silent in the tech industry are starting to speak up and kind of
[02:11:28.640 --> 02:11:33.520]   talk about what the industry is, what the sectors values are. And I think that's a good thing.
[02:11:33.520 --> 02:11:38.080]   So is it Stephen, though, a little bit just nostalgia? I mean, you and I both remember the
[02:11:38.080 --> 02:11:43.120]   Homebrew Computer Club. You were, you know, covered the early days of the MIT hackers.
[02:11:43.120 --> 02:11:47.840]   They were a rough and ready, you know, anti social band of misfits.
[02:11:47.840 --> 02:11:55.040]   You know, as was, I guess, Apple in the early days. Those days aren't coming back.
[02:11:56.640 --> 02:12:05.920]   Not in the same way. But, you know, there is a thriving startup culture where people do a reset.
[02:12:05.920 --> 02:12:16.000]   Now, a startup in, you know, the 2020s isn't the same as a startup in the late 1970s or early 80s.
[02:12:16.000 --> 02:12:22.800]   You know, there's a pathway and, you know, a lot of the founders have their eyes on the prize from
[02:12:22.800 --> 02:12:28.640]   the get go, right? They're, you know, they're, you know, Y Combinator, when it started in 2005,
[02:12:28.640 --> 02:12:35.040]   gave $5,000 plus $5,000 for each founder to the company as his state investment.
[02:12:35.040 --> 02:12:41.520]   The investment now for each company in Y Combinator is $500,000, right? And, you know,
[02:12:41.520 --> 02:12:50.640]   so, you know, during a fair path to fail big or get big. So that is different yet, you know,
[02:12:51.680 --> 02:12:54.960]   you know, I think that there's going to be a lot of, you know,
[02:12:54.960 --> 02:13:00.400]   contending 20 years from now, the companies that are going to be talking about are companies
[02:13:00.400 --> 02:13:02.880]   that are starting now or maybe haven't even started yet.
[02:13:02.880 --> 02:13:07.600]   Also, one thing that I think is worth pointing out is that, yeah, of course,
[02:13:07.600 --> 02:13:13.200]   we're not going to go back to those days. But we now know that it's a political risk, right?
[02:13:13.200 --> 02:13:19.520]   To, to be where we are today, right? It almost sent the whole industry into a tailspin because
[02:13:19.520 --> 02:13:25.520]   of the image, the billionaire bailout. And that is the issue, right? And so once that political
[02:13:25.520 --> 02:13:29.760]   risk is realized, and I really think it is, like, I think this has been a wake up call
[02:13:29.760 --> 02:13:34.960]   to many of Silicon Valley about what could happen to them, you know, based off of public sentiment.
[02:13:34.960 --> 02:13:38.480]   I think that might lead to change. No, we're not going to go back to hobbyists.
[02:13:38.480 --> 02:13:43.360]   And we're not going to probably probably won't have less funding. But we will, we will definitely
[02:13:43.360 --> 02:13:48.960]   go back to a, you know, maybe go back to a time where we didn't have this over financialization
[02:13:48.960 --> 02:13:52.400]   of the economy, at least of the sector. And at least that's my hope.
[02:13:52.400 --> 02:13:56.160]   Should we break up these big tech companies like we did standard oil or
[02:13:56.160 --> 02:14:01.200]   mobbell? They'll probably do a good enough job on their own of breaking themselves up,
[02:14:01.200 --> 02:14:05.360]   being either breaking themselves up or, or being out competed. I mean, look,
[02:14:05.360 --> 02:14:09.520]   it's what's happened to meta just this year, right? Just in the past year. Look at what happened
[02:14:09.520 --> 02:14:13.600]   to meta, right? This challenge from TikTok. Look at what happened to Google, this challenge
[02:14:13.600 --> 02:14:18.800]   from open AI and Microsoft. So I really believe that competition will do the better job in terms
[02:14:18.800 --> 02:14:22.640]   of humbling these companies than the government coming in and saying you can have
[02:14:22.640 --> 02:14:25.600]   Amazon retail, but you can't have Amazon web services.
[02:14:25.600 --> 02:14:32.720]   You know, Leo, you know, from, you know, being like a ground so much that, you know,
[02:14:32.720 --> 02:14:40.160]   we go into waves of success and, you know, and, and cold periods and busts and booms.
[02:14:40.160 --> 02:14:46.560]   But one thing that does not vary is, you know, the arrow of the technology itself.
[02:14:46.560 --> 02:14:52.240]   It just keeps getting better. Innovations keep coming. There's never like a down time for the
[02:14:52.240 --> 02:14:59.360]   technology improving. And, you know, we're seeing that now in AI, it's apparent to us. But there's
[02:14:59.360 --> 02:15:06.480]   always going to be something coming along to build on the previous advances. You know, so, and
[02:15:06.480 --> 02:15:12.560]   the innovation of tomorrow is going to go faster because we've had all the previous innovations.
[02:15:12.560 --> 02:15:16.800]   So we're arguing about the state of whether it's big companies, small companies.
[02:15:16.800 --> 02:15:25.520]   Do we need different people? But one constant is that technology keeps getting better and better.
[02:15:25.520 --> 02:15:32.480]   And that's what's exciting. And that's what's thrilling. And that's what's scary about our world.
[02:15:32.480 --> 02:15:37.680]   Yeah. I agree. It's the thing that makes it most interesting to cover, to look at and
[02:15:38.880 --> 02:15:45.120]   speculate about. And, you know, I have to say that it's not been good for a huge amount of money
[02:15:45.120 --> 02:15:51.520]   to flow into tech because that attracts the greedy. It also attracts the scammers. And you see,
[02:15:51.520 --> 02:15:57.440]   you know, NFTs and Bitcoin and, you know, a lot of scams that look at Andreessen Horowitz,
[02:15:57.440 --> 02:16:04.640]   Mark Andreessen pushing web three, which is a blatant, you know, grab for the free and open
[02:16:04.640 --> 02:16:10.800]   internet. You know, I see companies like Spotify and Amazon going after our little corner of the
[02:16:10.800 --> 02:16:16.480]   world RSS based podcasting. But I agree with you, Stephen, I think in the long run, the human spirit,
[02:16:16.480 --> 02:16:24.160]   there's always going to be some little person who has a great idea who is going to change the world.
[02:16:24.160 --> 02:16:28.880]   And it may not be Elizabeth Holmes, but there's always going to be somebody like that.
[02:16:28.880 --> 02:16:32.240]   I think, yeah. Maybe that person got laid off by a big company.
[02:16:32.240 --> 02:16:36.400]   Yeah, even better, right? And it's forced to go back to the garage and find something better.
[02:16:36.400 --> 02:16:41.920]   Another 10,000 jobs leaving Facebook, they fired 11,000
[02:16:41.920 --> 02:16:47.680]   on in the fall. And now they're saying we're going to fire another 10,000. Plus,
[02:16:47.680 --> 02:16:54.080]   we're not going to fill 5,000 open positions. So the net of that is what is that 26,000,
[02:16:55.040 --> 02:17:04.240]   26,000 jobs gone from Facebook. Any other industry that would be a that would be a death blow.
[02:17:04.240 --> 02:17:10.640]   So yeah, maybe those people, some of those people will go on, you know, it's hard. And I feel for
[02:17:10.640 --> 02:17:17.280]   you, I'm hearing from a lot of people who are out of work who think that, you know, maybe podcasting
[02:17:17.280 --> 02:17:22.480]   and hell will I just want to say it can't. But please don't become a podcast.
[02:17:22.480 --> 02:17:28.800]   That's the wrong direction. But I think that's right that those people maybe now are going to
[02:17:28.800 --> 02:17:32.080]   have the stimulus, the spur they need to create something exciting and new. And boy,
[02:17:32.080 --> 02:17:40.800]   the way AI is happening, we can, I'll go through a list of some of the AI startups that were formed
[02:17:40.800 --> 02:17:45.280]   this week alone. And in a way, that's why it's good that there's venture capital. There is money
[02:17:45.280 --> 02:17:50.960]   available because there are people with big bags of money who don't want to invest in the stock
[02:17:50.960 --> 02:17:55.120]   market. They think that if they could just find that next big thing, that's the best place for
[02:17:55.120 --> 02:18:01.120]   them to make money. That's not a bad thing. Is that Alex? No, it's, I don't think it's a bad
[02:18:01.120 --> 02:18:06.960]   thing at all. Yeah. Right. And I also think like, look, like right now, if you write AI on your
[02:18:06.960 --> 02:18:11.680]   term sheet or whatever it is, you're probably getting seven VCs that will die on like there's
[02:18:11.680 --> 02:18:17.600]   money is dry. Money is harder to come by now, but it's definitely there. And it is a moment of
[02:18:17.600 --> 02:18:22.640]   opportunity. Like, yeah, because it does take money during. Yeah, of course, like during COVID,
[02:18:22.640 --> 02:18:27.040]   the during the like the worst part of COVID is when I decided to leave BuzzFeed and start my
[02:18:27.040 --> 02:18:31.280]   company. That's right. And this is generally the moment where you start to have people who are like,
[02:18:31.280 --> 02:18:36.720]   okay, how's that worked out for you? That thing. I'm very happy. Yeah. And people are like, and
[02:18:36.720 --> 02:18:41.520]   look, still a lot of room to grow. But you know, the fact that that was the moment that pushed me
[02:18:41.520 --> 02:18:45.760]   out and said, do this, you know, start your own thing. Yeah. That's happening across the economy
[02:18:45.760 --> 02:18:50.640]   now, especially within tech. And by the way, when they get that money, they won't feel entitled to it.
[02:18:50.640 --> 02:18:54.720]   They know they'll know they had to work for it. And that will lead to better companies,
[02:18:54.720 --> 02:18:58.800]   more sound management and true gratefulness, I think from founders that they're able to do what
[02:18:58.800 --> 02:19:07.120]   they do. Alex Cantor was host of the big technology podcast, the author of always day one. And of
[02:19:07.120 --> 02:19:14.640]   course, the sub stack big technology.com. Great to have you at same, Steven Levy. One last break,
[02:19:14.640 --> 02:19:18.400]   we'll wrap things up in just a little bit. But first, I got to tell you about our sponsor,
[02:19:18.400 --> 02:19:24.160]   collide. They're doing something really, really interesting. We all talk about zero trust
[02:19:24.160 --> 02:19:29.280]   architectures these days when it comes to security. The idea that anybody in the network should be
[02:19:29.280 --> 02:19:34.720]   treated as an adversary, but there's a place where zero trust falls apart. And that's where
[02:19:34.720 --> 02:19:40.400]   collide is really going to make a difference. There are device trust solution that ensures
[02:19:40.400 --> 02:19:47.200]   unsecured devices cannot access your apps. If you're an octa user, you're going to want this. They can
[02:19:47.200 --> 02:19:53.680]   get your entire fleet to 100% compliance by patching one of the major holes in zero trust
[02:19:53.680 --> 02:20:00.160]   architectures device compliance. So your identity provider, you know, is saying, well, only known
[02:20:00.160 --> 02:20:06.400]   devices can log into apps. But just because a device is known, doesn't it mean it's in a secure
[02:20:06.400 --> 02:20:14.160]   state? Just ask last pass, right? They, they, there was a very determined attacker who found the four
[02:20:14.160 --> 02:20:19.680]   dev ops guys who had keys to the S three buckets. Those dev ops guys, when they worked from home,
[02:20:19.680 --> 02:20:24.960]   were using a laptop, one of them using a laptop that had plex on it that hadn't been patched in
[02:20:24.960 --> 02:20:32.560]   three years. That, that is a source of ultimate danger to a company. And in the case of last
[02:20:32.560 --> 02:20:39.680]   pass, it could be the end of the end of the line. All because of this one little problem. Yeah,
[02:20:39.680 --> 02:20:43.920]   the dev ops guy was known, he was logging in. But what they didn't know was that he had an
[02:20:43.920 --> 02:20:49.920]   insecure app running on his machine. Plenty of the devices in your fleet probably shouldn't be
[02:20:49.920 --> 02:20:54.560]   trusted. Maybe they're running out of data west versions. Maybe they've got unencrypted credentials
[02:20:54.560 --> 02:20:59.600]   lying around. They've got an old browser. Maybe they're running plex from three years ago. If a
[02:20:59.600 --> 02:21:04.720]   device isn't compliant, or isn't running the collide agent, because the agents, what finds all of that,
[02:21:04.720 --> 02:21:10.000]   they just can't get in. It can't access the organization's sass apps. It can't access the other
[02:21:10.000 --> 02:21:15.600]   resources. The device user can't log in to your company's cloud apps until this is the best part.
[02:21:15.600 --> 02:21:21.440]   They fixed the problem on their end. You don't do it. They do it. That's what the collide agent
[02:21:21.440 --> 02:21:26.080]   does. It says, Hey, wait, you can't log in. And you know why? Because your browser's out of date.
[02:21:27.360 --> 02:21:33.040]   It's so simple. A device is blocked. Employee doesn't have an update browser. It tells the employee
[02:21:33.040 --> 02:21:40.800]   and the end user remediates it without overwhelming your IT team. Your fleet will be driven to 100%
[02:21:40.800 --> 02:21:45.920]   compliance. Isn't that beautiful? Without collide, IT teams have no way to solve these compliance
[02:21:45.920 --> 02:21:53.040]   issues. Hybrid work, remote work, onsite work, it's all mushed together. With collide, you can set
[02:21:53.040 --> 02:21:58.400]   and enforce compliance across your entire fleet and its truly cross platform, Mac, Windows, and
[02:21:58.400 --> 02:22:04.720]   Linux. And because collide ties device compliance into the authentication process,
[02:22:04.720 --> 02:22:09.440]   this is really the way to do it when the user logs in with Octa.
[02:22:09.440 --> 02:22:15.040]   Collide will alert them to compliance issues, prevents unsecured devices from logging in,
[02:22:15.040 --> 02:22:20.400]   and they get a user to fix it. That's awesome. It's security you can feel good about.
[02:22:20.400 --> 02:22:25.760]   Collide puts transparency and respect for users at the center of their product. Your users will
[02:22:25.760 --> 02:22:32.080]   know it. Your IT department will love it. And it's a really, really, really good. To sum it up,
[02:22:32.080 --> 02:22:38.640]   collides method means fewer support tickets, less frustration. Most importantly, 100% fleet
[02:22:38.640 --> 02:22:45.920]   compliance. You know you need this. Go to collide.com/twit to learn more or book a demo. K-O-L-I-D-E
[02:22:45.920 --> 02:22:55.920]   collide.com/twit. It's just really the obvious solution. The right way to do it, collide.com/twit.
[02:22:55.920 --> 02:23:00.160]   Thank you, Collide, for supporting us. They've been with us for a long time. But thank you for
[02:23:00.160 --> 02:23:06.320]   supporting Collide by going to their website. And when you go to that address, collide.com/twit,
[02:23:06.320 --> 02:23:10.640]   you support us because then they know you saw it here. And that's a good thing.
[02:23:10.640 --> 02:23:15.120]   We had a great week this week, boy. We had a lot to talk about. And we've even made a little mini
[02:23:15.120 --> 02:23:20.320]   movie for you to watch so you can catch up. Watch this. What's your last words, Beech F?
[02:23:20.320 --> 02:23:22.240]   See, you have to think about these things. Mine would be.
[02:23:22.240 --> 02:23:28.800]   That goes on your tombstone. Somebody in the Discord, mashed potato says, "It should be
[02:23:28.800 --> 02:23:34.800]   another twit is in the can." That's kind of a good thing. It might be a...
[02:23:34.800 --> 02:23:42.400]   Previously on Twit. Home Theater Geeks. On today's Home Theater Geeks, we're going to talk about Sony's
[02:23:42.400 --> 02:23:50.960]   2023 TV lineup. So stay tuned this week in Google. Open AI announced chat GPT4
[02:23:50.960 --> 02:23:56.160]   yesterday. Within hours, there were 500 new startups.
[02:23:56.160 --> 02:24:01.840]   It's kind of mind boggling. Have you ever changed how you write?
[02:24:01.840 --> 02:24:07.040]   I have. Are you getting more or more writing? Yeah, a little more colorful, a little less
[02:24:07.040 --> 02:24:12.560]   robotic. My prediction is that this is eventually going to be where writing goes.
[02:24:12.560 --> 02:24:18.160]   Hands-on photography. Someone asked me about using their phone like a DSLR.
[02:24:18.160 --> 02:24:24.720]   Well, why would you do that? And should you really do that? I have some thoughts.
[02:24:24.720 --> 02:24:31.760]   Tech News Weekly. We talked to Tech Crunch's own Alex Mulhollum about, yes, Silicon Valley Bank.
[02:24:31.760 --> 02:24:37.840]   It is time to talk about S-V-B. What's going on? I feel like whenever something breaks, I get to
[02:24:37.840 --> 02:24:41.440]   come on your fine show. If you find it come on a week when nothing is happening, just take a
[02:24:41.440 --> 02:24:45.600]   boy Twit. I'm just saying, like, we don't have to wait for a crisis to have beyond.
[02:24:45.600 --> 02:24:50.000]   When we just want to hang out, we will contact you and we'll bring you on for the hangout episode.
[02:24:50.000 --> 02:24:56.400]   So the good news about Tech is there's always a crisis. There's always a disaster looming
[02:24:56.400 --> 02:25:01.520]   around the corner. I hope you enjoyed your week on Twit. And I hope you'll keep watching
[02:25:01.520 --> 02:25:06.240]   all week long. And I hope you'll join Club Twit. If you're not a member, seven bucks a month,
[02:25:06.240 --> 02:25:14.000]   you get unencumbered shows, no ads at all. You get access to the Discord, which Alex and I were
[02:25:14.000 --> 02:25:18.000]   talking about before the show. He's thinking about that for big technology. And I have to say,
[02:25:18.000 --> 02:25:22.320]   it's the best thing ever we've ever done. You also get special shows we don't put out anywhere else
[02:25:22.320 --> 02:25:30.640]   like Home Theater Geeks brought back thanks to the club. Seven bucks a month. Go to twit.tv/club.
[02:25:30.640 --> 02:25:36.160]   Twit. That's enough said. Enough said. By the way, TikTok is announced. They are adding a new feed
[02:25:36.160 --> 02:25:41.920]   dedicated to science and technology. Just trying to say, see, we're good for young people. You're
[02:25:41.920 --> 02:25:48.400]   going to learn about finance and science and stuff on TikTok, or, you know, watch my son's
[02:25:48.400 --> 02:25:57.840]   channel and learn how to make a sandwich. Good news. I guess, according to the federal agents
[02:25:58.560 --> 02:26:05.760]   who have arrested a peak skill New York man who ran a dark web data breach site called breach forums.
[02:26:05.760 --> 02:26:12.680]   Palm, Palm, porn. He's alleged. Yeah, epic name. Palm,
[02:26:12.680 --> 02:26:16.480]   he's 21 years old, by the way. He's a kid. He's just like still in school.
[02:26:16.480 --> 02:26:23.520]   He was born in the 2000s. That shows you. Oh my God. His real name is a, I don't, maybe I won't
[02:26:23.520 --> 02:26:28.080]   say his real name because it's all alleged, right? Although apparently he confessed when they arrested
[02:26:28.080 --> 02:26:32.320]   him, he said, you got me charged with a single account of conspiracy to commit.
[02:26:32.320 --> 02:26:41.360]   Access device fraud. Breach forums hosts the stolen databases of more than 1000 companies
[02:26:41.360 --> 02:26:48.560]   and websites. Palm, Palm, poor ins profile. I love saying that.
[02:26:48.560 --> 02:26:50.880]   On breach forums, describes him as boss man.
[02:26:53.040 --> 02:27:00.320]   And according to the FBI agents, he admitted, yeah, that that's me. I own and operate it.
[02:27:00.320 --> 02:27:07.120]   So I don't know. He has been released on bond. He, he is, he graduated from high school in 2021.
[02:27:07.120 --> 02:27:08.400]   Congrats.
[02:27:08.400 --> 02:27:13.040]   21 year old hacker best of mind. I would definitely go by boss.
[02:27:13.040 --> 02:27:21.200]   No, just have to say. Wow. Anyway, I guess, yeah, another one bites the dust, I guess.
[02:27:21.200 --> 02:27:26.000]   Apparently breach forums was one of the most active places hackers went to buy and sell
[02:27:26.000 --> 02:27:31.040]   leaked information. Honestly, I think you should go after the companies that are leaking the
[02:27:31.040 --> 02:27:38.240]   information, but that's, you know, that's just me. FCC has finally ordered phone companies to block
[02:27:38.240 --> 02:27:45.360]   scam text messages. What? You mean they weren't before? God bless them. God bless them.
[02:27:46.720 --> 02:27:53.520]   Seriously, the rise in spam text messages going up from what 3000 or the article you have that
[02:27:53.520 --> 02:27:59.680]   complaints the FTC about these rose from 3,300 in 2015 when they were already annoyed to
[02:27:59.680 --> 02:28:07.040]   18,900 per year. So like five X what we were getting in 2022. It's probably worse now. I mean,
[02:28:07.040 --> 02:28:11.440]   this is what if we're going to have one government do one thing, pick up the trash and number two
[02:28:11.440 --> 02:28:15.120]   and robo text and fill potholes. That's number three.
[02:28:15.120 --> 02:28:22.000]   Every, I don't think there's anybody listening. I don't think there's anybody who hasn't gotten
[02:28:22.000 --> 02:28:28.960]   a text, you know, random text from number you don't know saying hi, or a lot of them are like,
[02:28:28.960 --> 02:28:33.520]   I got one the other day that said, Hey, that was a great round of golf. I'm going to be in town
[02:28:33.520 --> 02:28:37.440]   again when you want to play again. You might say, well, that doesn't sound like a scam. That
[02:28:37.440 --> 02:28:44.400]   just sounds like some some guy. We got a good one now. Our boss, the CEO of Twit,
[02:28:44.400 --> 02:28:50.960]   somebody's impersonating her to our employees, sending out messages saying, I need to talk to
[02:28:50.960 --> 02:28:58.000]   you, call me. And almost all of these end up you get in a conversation. I would say, for instance,
[02:28:58.000 --> 02:29:01.200]   what, what, Brown to golf? I don't even play golf. And he say, Oh gosh, I'm so sorry.
[02:29:02.080 --> 02:29:07.600]   But you seem like a nice person. What's your name? Leo, what's yours? John. And then you get in
[02:29:07.600 --> 02:29:12.480]   the conversation. Actually, usually if I'm Leo, it's going to be Sally. And you get in a little
[02:29:12.480 --> 02:29:17.760]   conversation and won't be too long before they offer you a great investment in cryptocurrencies.
[02:29:17.760 --> 02:29:23.200]   There's a great, there's a great Twitter comedian. I forget his name right now, but he posts a
[02:29:23.200 --> 02:29:28.560]   screenshot of these messages. And you know, he goes forward with like the first conversation,
[02:29:28.560 --> 02:29:32.080]   the first, you know, back and forth. And he says, you know, they say, Hey, so you sound like a
[02:29:32.080 --> 02:29:36.320]   nice person. He goes like, what's your name? And he goes like, okay, here's my credit card
[02:29:36.320 --> 02:29:42.880]   information. Yeah, just take it. Take it. I know, I know what you want. And I've got it. Wow.
[02:29:42.880 --> 02:29:49.840]   Well, I didn't know it was illegal. It is now. So just the the rule requires blocking of text and
[02:29:49.840 --> 02:29:53.280]   the phone companies have to do this from invalid and unused numbers.
[02:29:55.360 --> 02:30:01.440]   Okay, maybe they can do political campaigns next. I'm run down of popular
[02:30:01.440 --> 02:30:08.160]   chat GPT appearances, carrot weather, the snark weather, they're going to have chat GPT do the
[02:30:08.160 --> 02:30:17.440]   snark going forward. Let's see, Google has announced as I mentioned AI features. Why is by the way,
[02:30:17.440 --> 02:30:26.080]   in docs, what where where are we Facebook's LL large language module model LLM? I think they
[02:30:26.080 --> 02:30:32.560]   called theirs Lambda llama llama. That's right. Llama. And everybody's got a play on LLM.
[02:30:32.560 --> 02:30:38.640]   It was leaked on 4chan. And people have written interfaces. And now you can use it. Why why
[02:30:38.640 --> 02:30:44.960]   haven't we seen Facebook's use of this at all? Are they just like, is there attention elsewhere?
[02:30:44.960 --> 02:30:49.040]   Are they are they looking at Microsoft saying that didn't go so well? Maybe we shouldn't do it.
[02:30:49.040 --> 02:30:56.720]   They're working on it. They're working on it. I mean, interestingly, I wrote about this Quora
[02:30:56.720 --> 02:31:07.360]   uses, you know, open AI technology for something they're doing. Now we're, you know, Quora is about
[02:31:07.360 --> 02:31:12.160]   connecting people to answer questions. Now they're going to connect people with robots to answer
[02:31:12.160 --> 02:31:18.160]   questions. There's an app called Po on by Quora, where you ask your question, instead of a human
[02:31:18.160 --> 02:31:25.280]   answering it, it'll be, you know, I think that's probably already been the always been the problem
[02:31:25.280 --> 02:31:30.240]   with Quora is more questioners than answerers. So that kind of makes sense that you would,
[02:31:30.240 --> 02:31:35.360]   you would, you know, give it all I have to log in with my phone number to get to get.
[02:31:35.360 --> 02:31:40.160]   Yeah. But the, but the thing is that, you know, it was always about what human could come up with
[02:31:40.160 --> 02:31:44.720]   the best answer. So sometimes you would say, well, what is Lowry page like to work with?
[02:31:44.720 --> 02:31:49.600]   Someone, you know, would say, this is what Larry pages like to work with. But instead,
[02:31:49.600 --> 02:31:55.040]   you can get a robot saying, Oh, here's what Larry pages like to work with. But the robot, you know,
[02:31:55.040 --> 02:32:00.960]   has no idea. Right. Yeah. Yeah. It seems like the end of Quora for me, if this is hopefully just a
[02:32:00.960 --> 02:32:04.480]   side project, but if it becomes anything more, that's a disaster for this company.
[02:32:06.080 --> 02:32:10.560]   Interesting. Yeah. Because really they are kind of reliant on people.
[02:32:10.560 --> 02:32:14.640]   And why talk to Adam D'Angelo, who used to be the CTO of Facebook, by the way,
[02:32:14.640 --> 02:32:22.160]   Oh, and he was, he was a partner with Mark Zuckerberg in an AI based program when they were in high
[02:32:22.160 --> 02:32:27.360]   school. Oh, well, that's the thing. I mean, this is not like I said, at the beginning, this is nothing
[02:32:27.360 --> 02:32:34.160]   unknown. Nothing new. Any company could theoretically do it. So here's Poe. What question should I ask?
[02:32:34.960 --> 02:32:43.600]   Poe, what's Larry Page like to work with? What is Larry Page like to work with? Good question.
[02:32:43.600 --> 02:32:50.080]   Oh, as an AI language model, I do not have personal experience working with Larry Page.
[02:32:50.080 --> 02:32:55.200]   But I can tell you he's known for being a brilliant and innovative leader in the tech industry,
[02:32:55.200 --> 02:33:03.840]   known for cutting edge technology and promoting a culture of innovation and creativity. However,
[02:33:03.840 --> 02:33:06.480]   it's important to note individual experiences may vary.
[02:33:06.480 --> 02:33:14.320]   But this is Sage. They also have chat GPT four. They also have Claude plus. I don't know, Claude
[02:33:14.320 --> 02:33:23.760]   plus is that's the same. That's what? Inthropics. Oh, so these are different engines. Yeah.
[02:33:23.760 --> 02:33:31.200]   This is actually smart. Although I note that even though I entered from Quora, it's at Poe.com.
[02:33:33.040 --> 02:33:42.480]   Yeah. Yeah. How do I make a tasty ham sandwich? We'll see how much trouble my son is in.
[02:33:42.480 --> 02:33:48.080]   To make a tasty ham sandwich, you need bread, sliced ham, cheese, lettuce, tomato, mayonnaise.
[02:33:48.080 --> 02:33:55.760]   It's pretty, it's fairly accurate. You can also add pickles, onions, or avocado to customize
[02:33:55.760 --> 02:34:01.760]   your sandwich. Well, ham and avocado. Oh, yeah, that's not good. Is it?
[02:34:02.480 --> 02:34:09.520]   Where is the best bagel in New York City? Now this you both would be interested in.
[02:34:09.520 --> 02:34:15.520]   No personal experience, but he recommends Russ and daughters, the iconic shop on the Lower East
[02:34:15.520 --> 02:34:22.880]   Side been there since 1914. See, I didn't even know that. I know H and H. That's the number three on the
[02:34:22.880 --> 02:34:29.680]   list. Huh. That's because I'm an up to a West Side kind of guy. Absolute bagels and morning side
[02:34:29.680 --> 02:34:34.720]   heights, which is where I was born known for its affordable prices and delicious bagels.
[02:34:34.720 --> 02:34:39.360]   Would you New Yorkers concur? How about if I say in Brooklyn?
[02:34:39.360 --> 02:34:46.800]   Yeah. Look, I mean, I don't even go for the bridge. I know. I see. Yeah. Brooklyn is tough to
[02:34:46.800 --> 02:34:51.440]   leave, but also like New York, the pizza and the bagel, the thing is that like, the thing that's
[02:34:51.440 --> 02:34:56.320]   great about them is they're high quality wherever you go. So like, if you go for like your local
[02:34:56.320 --> 02:35:00.880]   place or you go to one of these like top bagel places, I don't think the Delta is that large.
[02:35:00.880 --> 02:35:04.640]   Like you're probably just as good going to your corner bagel shop, but I'm curious what they say
[02:35:04.640 --> 02:35:16.320]   about Brooklyn. All right. Where can I get the best bagel in Brooklyn? This is very specific
[02:35:16.320 --> 02:35:21.920]   user service right now. I should give him your, are you in Flatwood? Where are you? You're in
[02:35:22.640 --> 02:35:28.480]   bagel hole in Park slope. Frankly. Yeah. Oh yeah. Oh yeah. In Green Point,
[02:35:28.480 --> 02:35:35.760]   Frankles, Terrace bagels in Windsor, Terrace and Tomkins Square bagels, which is in Williamsburg.
[02:35:35.760 --> 02:35:40.560]   The one thing I can say for sure from speaking with this chat bot through your
[02:35:40.560 --> 02:35:43.440]   Leo. So thank you is that I will be getting bagels tomorrow morning.
[02:35:43.440 --> 02:35:50.480]   Making me hungry. Yeah. Yeah. I like Tomkins Square, but I get him a Tomkins Square. I didn't
[02:35:50.480 --> 02:35:54.320]   really realize that they had a Brooklyn. Yeah, that's weird to have Tomkins Square in Brooklyn,
[02:35:54.320 --> 02:35:58.560]   but okay. I'm not going to say anything. GM. Tomkins Square bagel this morning.
[02:35:58.560 --> 02:36:08.560]   Well, there you go. C. Poe new GM wants to bring chat GPT to drivers. General Motors,
[02:36:08.560 --> 02:36:14.560]   which manufactures Chevrolet Cadillac Buick and GMC trucks working on a virtual personal
[02:36:14.560 --> 02:36:20.400]   assistant that uses AI beyond chat GPT. It'll use Azure Microsoft's cloud service.
[02:36:20.400 --> 02:36:28.080]   When? I don't know. But here's an example. If you get a flat tire, ask the car to explain how to
[02:36:28.080 --> 02:36:33.360]   change it. Yeah, this actually sounds, I was actually ready to make fun of it, but like asking
[02:36:33.360 --> 02:36:38.160]   it to change your car or like when you when you see the engine light on. Oh, see, that's good. Hey,
[02:36:38.160 --> 02:36:42.080]   what's that mean? What's going on my engine light and it can talk to you. That's cool. The one
[02:36:42.080 --> 02:36:46.800]   thing I worry about is that these bots still hallucinate so much that if it tells you the wrong
[02:36:46.800 --> 02:36:52.480]   thing and you drive off the side of the road because you're carrying in the tank, go get some
[02:36:52.480 --> 02:36:57.520]   kerosene because you're out. See that little oil thing? That's a kerosene can. That would be
[02:36:57.520 --> 02:37:06.000]   bad, right? Don't worry about that. That's fine. Don't worry about it. Don't worry about that.
[02:37:06.560 --> 02:37:13.280]   That's what we need. Hallucinating weather, hallucinating cars, hallucinating bagels.
[02:37:13.280 --> 02:37:19.280]   Google's working on AI for ultrasound diagnosis and cancer therapy.
[02:37:19.280 --> 02:37:26.480]   What I'd really like to know is when are they going to put something like chat GPT or Google's
[02:37:26.480 --> 02:37:33.360]   bard into the voice assistants? When can Amazon's Echo get, you know, chatty or Google assistant or
[02:37:33.360 --> 02:37:41.680]   Siri? There was a story about that. I forget where I saw it, but it noted that these,
[02:37:41.680 --> 02:37:52.400]   the Siri and Alexa, they're not based on this part of technology. So they put a lot into
[02:37:52.400 --> 02:37:58.000]   part of technology that had been eclipsed. They thought that they would do this limited thing.
[02:37:58.000 --> 02:38:03.520]   They weren't doing these open-ended chats. So it was almost a if then statements, it was almost
[02:38:03.520 --> 02:38:06.480]   completely changed. Yeah. And they were thinking of people order,
[02:38:06.480 --> 02:38:14.960]   order, like, buy stuff on them. They were hoping. Yeah. Right. So it turns out that now that they
[02:38:14.960 --> 02:38:19.760]   have to go back to the square womb. But it also, I've also thought about this and it could get
[02:38:19.760 --> 02:38:24.720]   annoying if these things start to have the ability to talk like the same way that a chat GPT does
[02:38:25.280 --> 02:38:32.000]   or a Bing bot does because those bots are pretty loquacious and it's fine.
[02:38:32.000 --> 02:38:36.240]   Already. Yeah. Yeah. It's fine to read it. It's like, all right, you send me like seven
[02:38:36.240 --> 02:38:41.360]   paragraphs of text. I'm going to just scan it. But could you imagine your Google home?
[02:38:41.360 --> 02:38:43.360]   I'm not saying the A word because I'm going to set it off now.
[02:38:43.360 --> 02:38:48.640]   Couldn't you say, you know, as the programmer to limit yourself to one sentence, please.
[02:38:48.640 --> 02:38:51.040]   I mean, you don't make it.
[02:38:51.040 --> 02:38:57.040]   Even now. Oh my God. These things like even the echo. Did you know? I can also.
[02:38:57.040 --> 02:39:01.520]   It's like, no, shut up. Oh my God. They're probably the.
[02:39:01.520 --> 02:39:07.760]   They're probably the people who really like, you know, people who live alone who really
[02:39:07.760 --> 02:39:14.000]   might enjoy. I, you know, as I get older, I think I'd like a nice little echo silver that would
[02:39:14.000 --> 02:39:19.440]   have a conversation with me about the good old days. Well, that's coming. That will
[02:39:19.440 --> 02:39:22.080]   definitely, there will be products that will do that. Yeah, sure.
[02:39:22.080 --> 02:39:25.360]   And we've got these synthesized voices that sound pretty darn good.
[02:39:25.360 --> 02:39:31.600]   You know, I could have, you know, people talk to me from my earlier life.
[02:39:31.600 --> 02:39:37.680]   I could have. Hang out with the springsteen. Hang out with the boss. Now the boss can be your buddy.
[02:39:37.680 --> 02:39:40.320]   It was $5,000.
[02:39:40.320 --> 02:39:45.840]   If Ticketmaster runs it almost certainly, Stephen Levy, what a pleasure having you.
[02:39:46.560 --> 02:39:51.440]   Stephen's latest is of course Facebook, the inside story, but go to Stephen Levy.com.
[02:39:51.440 --> 02:39:57.680]   He's got many wonderful books. And if you haven't read the hackers, what's the subtitle of that?
[02:39:57.680 --> 02:40:00.400]   Something at the revolution, I think, right? Heroes. Heroes.
[02:40:00.400 --> 02:40:07.520]   Richard Stalman and MIT hackers. And it's what a great book. Really great.
[02:40:07.520 --> 02:40:11.680]   Always a pleasure to talk to you, Stephen. I think it was you that said you do,
[02:40:12.480 --> 02:40:17.520]   not like slow food, you do slow journalism, but you're doing a weekly column. I think that's slow.
[02:40:17.520 --> 02:40:20.640]   Yeah, yeah. I had to ramp up the cadence there.
[02:40:20.640 --> 02:40:26.400]   It's such a pleasure to talk to you as always. You too, Alex, Alex Cantor,
[02:40:26.400 --> 02:40:32.160]   what's host of the big technology podcast, the big technology newsletter, bigtechnology.com.
[02:40:32.160 --> 02:40:39.120]   And of course, don't forget his book always day one, which is not just about Amazon.
[02:40:39.120 --> 02:40:44.400]   Is it or is it right? No, it has all five of the big tech companies in there, including an interview
[02:40:44.400 --> 02:40:48.240]   with Zuckerberg. I think people would like it. And then, yeah, thanks for shouting out the podcast.
[02:40:48.240 --> 02:40:51.040]   We have Kevin Sistrom coming on this week in the Palmer Lucky.
[02:40:51.040 --> 02:40:53.200]   You get the best guests. Palmer Lucky, really.
[02:40:53.200 --> 02:40:58.160]   Coming on founder of Oculus coming on in a week and a half. So it's on a big technology podcast
[02:40:58.160 --> 02:41:03.680]   on every podcast app. So you're willing to get people on the like Lake Lemoine and Palmer Lucky
[02:41:03.680 --> 02:41:08.320]   that I might be scared to have on. You're great. The fear is part of the adrenaline rush that
[02:41:08.320 --> 02:41:12.400]   you get from doing it. So I went on a road trip with Palmer Lucky once.
[02:41:12.400 --> 02:41:17.360]   What? Where to? Yeah, Texas, Texas, the Texas border.
[02:41:17.360 --> 02:41:23.680]   To see what? To see they had an installation of.
[02:41:23.680 --> 02:41:24.960]   Or I remember that.
[02:41:24.960 --> 02:41:30.080]   That technology on the border. I write the first big story about Andrew after he
[02:41:30.080 --> 02:41:35.280]   after he sold Oculus to Facebook. He got booted from booty.
[02:41:35.280 --> 02:41:42.400]   Whatever. He, Andrew Roll, which was like a surveillance technology for people crossing the border,
[02:41:42.400 --> 02:41:49.280]   right? Well, that was the first use as something to monitor the border, a smart wall kind of thing.
[02:41:49.280 --> 02:41:56.960]   And so we went to this ranch. This guy had where people were crossing the border and were able to
[02:41:56.960 --> 02:42:05.200]   use. He sent someone down into the Rio Grande Valley. We tracked it and we had a drone
[02:42:05.200 --> 02:42:09.520]   and it was a lot of fun. Palmer Lucky really loves water burgers.
[02:42:09.520 --> 02:42:17.360]   So who doesn't love water burgers? Yeah, we had fun. Actually, I enjoyed the trip.
[02:42:17.360 --> 02:42:24.240]   Oh, it's interesting. I'm very grateful that both you and Alex are willing to talk to
[02:42:24.240 --> 02:42:30.720]   people that I kind of just don't want to know. So because you get the stories,
[02:42:30.720 --> 02:42:34.800]   you get the stories and that's what matters. Thank you both for being here. Thanks to all of
[02:42:34.800 --> 02:42:40.560]   you for listening. We do tweet every Sunday afternoon to PM Pacific 5 p.m. Eastern 2100 UTC.
[02:42:40.560 --> 02:42:47.520]   You can watch live at live dot.tv chat live at IRC dot.tv that's open to all or in our club
[02:42:47.520 --> 02:42:53.040]   to it discord. All you have to do is pay seven bucks a month to get access to the best dang
[02:42:53.040 --> 02:42:59.280]   discord ever and all the other benefits after the fact on demand versions of the show available
[02:42:59.280 --> 02:43:04.320]   at the website. Twit.tv. There's a YouTube channel dedicated to the video from this
[02:43:04.320 --> 02:43:09.440]   weekend text. You can watch video. You can want listen to audio. And of course, probably the easiest
[02:43:09.440 --> 02:43:15.040]   thing to do while you're looking for Alex's big technology podcast. Maybe subscribe to Twit as
[02:43:15.040 --> 02:43:21.440]   well. Your favorite podcast player. Thank you so much for being here. Everybody have a great week.
[02:43:21.440 --> 02:43:25.520]   We'll see you next time. Another twit is in the can. This is amazing.
[02:43:25.520 --> 02:43:35.600]   See you on the Twitter. All right. Do the Twitter. All right. Do the Twitter. All right. Do the Twitter.

