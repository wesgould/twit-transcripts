;FFMETADATA1
title=Eastern Blocks
artist=Leo Laporte, Alex Kantrowitz, Cory Doctorow
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-10-03
track=895
language=English
genre=Podcast
comment=Stadia is done, Zuck's UFC appearance, General AI, OG App, Amazon event
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:07.520]   It's time for "Dwight This Week" in Tech. We have two of my favorite people on two geniuses.
[00:00:07.520 --> 00:00:10.600]   Corey Dottro and Alex Cantrowitz.
[00:00:10.600 --> 00:00:14.600]   Pro Big Tech against Big Tech. It's going to be a great conversation.
[00:00:14.600 --> 00:00:18.600]   We'll talk about Google killing yet another service.
[00:00:18.600 --> 00:00:22.200]   Mark Zuckerberg fighting in the Octagon.
[00:00:22.200 --> 00:00:25.200]   Elon Musk's robot.
[00:00:25.200 --> 00:00:33.600]   And the plan, Peter Thiel's plan to buy into Britain's National Health Service that in a whole lot more coming up.
[00:00:33.600 --> 00:00:36.600]   Plus the big scam and podcast advertising.
[00:00:36.600 --> 00:00:40.600]   It's ahead on Twitch.
[00:00:40.600 --> 00:00:43.200]   Podcasts you love.
[00:00:43.200 --> 00:00:45.200]   From people you trust.
[00:00:45.200 --> 00:00:48.200]   This is Twitch.
[00:00:52.200 --> 00:00:56.200]   This is Twitch. This week in Tech.
[00:00:56.200 --> 00:01:01.200]   Episode 895 recorded Sunday, October 2nd, 2022.
[00:01:01.200 --> 00:01:03.200]   Eastern Blocks.
[00:01:03.200 --> 00:01:07.200]   This episode of This Week in Tech is brought to you by Nureva.
[00:01:07.200 --> 00:01:12.200]   Tired of the complexity and cost of traditional ProAV solutions for large spaces?
[00:01:12.200 --> 00:01:16.200]   Nureva has simplified everything about meetings and classroom audio.
[00:01:16.200 --> 00:01:21.200]   You get great audio in plug-and-play systems that are easy to install and manage
[00:01:21.200 --> 00:01:26.200]   and cost a fraction of in-ceiling systems. Visit Nureva.com/Twit.
[00:01:26.200 --> 00:01:31.200]   And by 8Sleep. Good sleep is the ultimate game changer.
[00:01:31.200 --> 00:01:34.200]   And the pod is the ultimate sleep machine.
[00:01:34.200 --> 00:01:39.200]   Go to 8sleep.com/Twit to check out the pod and save $150 at checkout.
[00:01:39.200 --> 00:01:42.200]   8Sleep currently ships within the US, Canada, the UK.
[00:01:42.200 --> 00:01:45.200]   Select countries in the EU and Australia.
[00:01:45.200 --> 00:01:47.200]   And by...
[00:01:47.200 --> 00:01:52.200]   Podium. Join more than 100,000 businesses that already use Podium
[00:01:52.200 --> 00:01:55.200]   to streamline their customer interactions.
[00:01:55.200 --> 00:01:57.200]   See how Podium can grow your business.
[00:01:57.200 --> 00:02:02.200]   Watch a demo today at Podium.com/Twit.
[00:02:02.200 --> 00:02:04.200]   And by...
[00:02:04.200 --> 00:02:09.200]   Policy Genius. Making it easy to compare your options from top companies.
[00:02:09.200 --> 00:02:13.200]   Policy Genius can help you make sure you're not paying a cent more than you have to
[00:02:13.200 --> 00:02:15.200]   for the coverage you need.
[00:02:15.200 --> 00:02:22.200]   Head to PolicyGenius.com/Twit to get your free life insurance quotes and see how much you could save.
[00:02:22.200 --> 00:02:29.200]   It's time for Twit, this week in Tech.
[00:02:29.200 --> 00:02:32.200]   The show we cover the week's Tech News. We have such a good panel today.
[00:02:32.200 --> 00:02:38.200]   I have limited it to just two people. Alex Kaptowitz is here from the big tech sub-stack
[00:02:38.200 --> 00:02:41.200]   newsletter and the big technology podcast.
[00:02:41.200 --> 00:02:43.200]   Hello, Alex. Good to see you.
[00:02:43.200 --> 00:02:45.200]   Hey Leo, great to see you.
[00:02:45.200 --> 00:02:48.200]   Are you plugging the Vancouver hockey team?
[00:02:48.200 --> 00:02:51.200]   This is the grizzly. So this is throwback.
[00:02:51.200 --> 00:02:55.200]   So for listeners, I'm wearing this throwback Vancouver grizzly's sweatshirts.
[00:02:55.200 --> 00:02:58.200]   It's pretty cool. It's by Mitchell and that's which is, I think, my favorite clothing brand.
[00:02:58.200 --> 00:02:59.200]   Nice.
[00:02:59.200 --> 00:03:03.200]   And I'm not a fashion guy, but I did see that the grizzly is wore the throwback jerseys.
[00:03:03.200 --> 00:03:04.200]   Memphis grizzly.
[00:03:04.200 --> 00:03:05.200]   Co-grizzly.
[00:03:05.200 --> 00:03:06.200]   Are you from?
[00:03:06.200 --> 00:03:07.200]   I got to get back here.
[00:03:07.200 --> 00:03:10.200]   No, the closest I've been to Canada is Washington State.
[00:03:10.200 --> 00:03:14.200]   Of course, I've been to Vancouver's Washington State on the West Coast.
[00:03:14.200 --> 00:03:18.200]   I do have some family in Toronto. Shout out Toronto.
[00:03:18.200 --> 00:03:22.200]   But no, I'm a New Yorker. Let's go. Let's go Mets.
[00:03:22.200 --> 00:03:27.200]   So there's nothing weirder than a New Yorker wearing a Vancouver grizzly shirt, but you know, we'll go.
[00:03:27.200 --> 00:03:28.200]   We'll go.
[00:03:28.200 --> 00:03:30.200]   I have one promise to make to you today.
[00:03:30.200 --> 00:03:31.200]   What's that?
[00:03:31.200 --> 00:03:32.200]   And that is that I'm here to bring the weird.
[00:03:32.200 --> 00:03:33.200]   Let's go.
[00:03:33.200 --> 00:03:34.200]   You're going to do it.
[00:03:34.200 --> 00:03:36.200]   Also here, I'm thrilled to have him.
[00:03:36.200 --> 00:03:39.200]   Corey, Dr. Rowe, you know Corey very well, I'm sure.
[00:03:39.200 --> 00:03:42.200]   He's got a new book. In fact, we did a triangulation.
[00:03:42.200 --> 00:03:50.200]   Corey and Rebecca Gibleg, his co-author and I, on Thursday, you might want to check that out on the Twit Triangulation feed or the Twit Event feed.
[00:03:50.200 --> 00:03:53.200]   Really fascinating conversation.
[00:03:53.200 --> 00:03:56.200]   Great to see you, Corey. You are a Canadian authentic.
[00:03:56.200 --> 00:04:01.200]   We walk among you. We are like serial killers. We look just like everyone else.
[00:04:01.200 --> 00:04:03.200]   He seems so normal.
[00:04:03.200 --> 00:04:05.200]   How could we have known?
[00:04:05.200 --> 00:04:08.200]   Although I became an American citizen about 10 weeks ago.
[00:04:08.200 --> 00:04:10.200]   I know. I remember that. Congratulations.
[00:04:10.200 --> 00:04:13.200]   Are you having regrets yet?
[00:04:13.200 --> 00:04:23.200]   No, opposite actually. Like the worst things get, the more glad I am that I have more rights that I would otherwise not be entitled to as someone who is merely a permanent resident.
[00:04:23.200 --> 00:04:26.200]   Now your wife is American, yes?
[00:04:26.200 --> 00:04:27.200]   No, she's British.
[00:04:27.200 --> 00:04:28.200]   She's British.
[00:04:28.200 --> 00:04:32.200]   But she's also American now. So she is an Anglo-American.
[00:04:32.200 --> 00:04:40.200]   My daughter and I are Anglo-Canadian Americans. I'm through my father entitled to Polish, Azerbaijani, Belaruzian, Russian citizenship.
[00:04:40.200 --> 00:04:41.200]   Great.
[00:04:41.200 --> 00:04:46.200]   I might get some of those. Most of them are countries I don't intend to ever do.
[00:04:46.200 --> 00:04:47.200]   Sure, I'd want to go.
[00:04:47.200 --> 00:04:56.200]   But every now and again, you want to go someplace that was like a non-elite nation during the Cold War and you got the right passport for it.
[00:04:56.200 --> 00:05:04.200]   Yeah, it's nice to have James Bond, a whole stack of passports you can go to in your go bag.
[00:05:04.200 --> 00:05:05.200]   That's right. Just in case.
[00:05:05.200 --> 00:05:09.200]   That's what I'm down for. I mean, even if it takes the rest of my life to get them.
[00:05:09.200 --> 00:05:13.200]   And hypothetically also I could get an Israeli passport. So like aid in total.
[00:05:13.200 --> 00:05:14.200]   Wow.
[00:05:14.200 --> 00:05:17.200]   I just like to have a big stack with a rubber band around it.
[00:05:17.200 --> 00:05:20.200]   Exactly. I have it in the safe with $10,000 in cash.
[00:05:20.200 --> 00:05:22.200]   And then just leave it open every once in a while.
[00:05:22.200 --> 00:05:29.200]   In case anybody is wondering by maybe a sidearm and you're set. You're ready to go.
[00:05:29.200 --> 00:05:33.200]   Someone in the IRC San Mateo says Putin might conscript me.
[00:05:33.200 --> 00:05:36.200]   Although I'm a 51 year old man with two artificial hips and cataracts.
[00:05:36.200 --> 00:05:37.200]   Doesn't matter.
[00:05:37.200 --> 00:05:38.200]   Doesn't matter.
[00:05:38.200 --> 00:05:43.200]   He'll take you. How's your hips by the way?
[00:05:43.200 --> 00:05:45.200]   Oh, they're getting better.
[00:05:45.200 --> 00:05:46.200]   Yeah, a little bit.
[00:05:46.200 --> 00:05:50.200]   Last time you were on you showed us your 3D model of your hip bone.
[00:05:50.200 --> 00:05:57.200]   Yeah. Now I should have brought them in. I made a brass cane topper cast from my hip bone.
[00:05:57.200 --> 00:05:58.200]   Yes.
[00:05:58.200 --> 00:06:02.200]   And then the bone itself is in a shadow box. But they're both out in our bar in the backyard.
[00:06:02.200 --> 00:06:03.200]   Rightly so.
[00:06:03.200 --> 00:06:05.200]   I think we can talk with it.
[00:06:05.200 --> 00:06:07.200]   Leave them there. That's good.
[00:06:07.200 --> 00:06:12.200]   Google, Google, Google. They're at it again.
[00:06:12.200 --> 00:06:17.200]   Stadia. Three years old. Google has announced.
[00:06:17.200 --> 00:06:21.200]   They're shutting it down. They're streaming gaming service.
[00:06:21.200 --> 00:06:26.200]   I guess really the only question is who ever thought Stadia would survive.
[00:06:26.200 --> 00:06:31.200]   The good news is they are giving you your money back.
[00:06:31.200 --> 00:06:36.200]   Why did Google Alex even think it could get into gaming?
[00:06:36.200 --> 00:06:39.200]   What was the process there?
[00:06:39.200 --> 00:06:44.200]   Well, I think that what we're seeing now is this demarcation between old Google and new Google.
[00:06:44.200 --> 00:06:50.200]   And old Google said let's experiment with everything we can, throw money behind it because why not?
[00:06:50.200 --> 00:06:51.200]   Right.
[00:06:51.200 --> 00:06:54.200]   And that's how you end up having a thing like Stadia get funded in the way that it got funded
[00:06:54.200 --> 00:07:02.200]   and become a big prog of Google's strategic vision is because they were going to make every experimental project part of that vision.
[00:07:02.200 --> 00:07:09.200]   And eventually some of it would stick. And that would be the potential next big reinvention for Google.
[00:07:09.200 --> 00:07:14.200]   Did Google get financial responsibility?
[00:07:14.200 --> 00:07:17.200]   Did they start getting it, did they grow up?
[00:07:17.200 --> 00:07:20.200]   Well, that's what I think, that's exactly what I think we're seeing right now.
[00:07:20.200 --> 00:07:23.200]   And the canceling of Stadia is like maybe just the beginning.
[00:07:23.200 --> 00:07:29.200]   We had soon to upper tie the CEO of Google and Alphabet at code a few weeks ago saying Google needed to get 20% more.
[00:07:29.200 --> 00:07:30.200]   That's right.
[00:07:30.200 --> 00:07:35.200]   And I think that rather than look at this as Google's bet on gaming,
[00:07:35.200 --> 00:07:38.200]   I think we can really see this as a signal for where this company is going.
[00:07:38.200 --> 00:07:45.200]   And maybe where all the big tech is going, which is that those experimental projects that used to get the money because, again, why not?
[00:07:45.200 --> 00:07:47.200]   They're not going to get the money anymore.
[00:07:47.200 --> 00:07:49.200]   And I think this is going to do two things.
[00:07:49.200 --> 00:07:52.200]   One, it's going to add bring needed focus to a lot of these companies.
[00:07:52.200 --> 00:08:02.200]   But two is going to open the door for companies who would have otherwise been in these experimental areas and gotten wiped out by big tech to start competing with big tech and the way that they haven't been before.
[00:08:02.200 --> 00:08:13.200]   So I think this is, again, just a very dynamic, interesting time for big tech and Stadia is one example of what we're seeing in terms of the change that's just going to accelerate, I think, over the next few months.
[00:08:13.200 --> 00:08:21.200]   Do you think, Corey, it's maybe also trepidation about government regulation that maybe Google's saying we should pull some of the tentacles back in?
[00:08:21.200 --> 00:08:33.200]   I mean, I don't think that that merger scrutiny is a lot easier to imagine the state imposing than merger review and unwinding.
[00:08:33.200 --> 00:08:35.200]   Unwinding mergers is really hard.
[00:08:35.200 --> 00:08:37.200]   It's very expensive and it takes a long time.
[00:08:37.200 --> 00:08:40.200]   I think it's probably a necessary corrective.
[00:08:40.200 --> 00:08:46.200]   And I do have one weird trick that we can talk about later for how I think we could do a lot of it very quickly.
[00:08:46.200 --> 00:08:53.200]   But merger scrutiny is far more likely. And since they didn't buy Stadia, I don't think they would be worried about that.
[00:08:53.200 --> 00:09:02.200]   But I do want to say that it's underappreciated the extent to which Google bought its way to glory instead of inventing its way to glory.
[00:09:02.200 --> 00:09:06.200]   This is a company that's had one and a half internal successes.
[00:09:06.200 --> 00:09:09.200]   They made a great search engine and a pretty good hotmail clone.
[00:09:09.200 --> 00:09:15.200]   All of the other things that they built internally crashed and burned and all of the successes that they have are things they bought from someone else.
[00:09:15.200 --> 00:09:17.200]   And this is just the latest example.
[00:09:17.200 --> 00:09:21.200]   Google Video, Stank, YouTube, succeeded.
[00:09:21.200 --> 00:09:24.200]   Google couldn't build a mobile OS, but Android came along.
[00:09:24.200 --> 00:09:28.200]   And people will say that Google Photos is an internal success.
[00:09:28.200 --> 00:09:34.200]   And it's true, but it's an internal success because it comes bundled on Android, which Google bought from someone else.
[00:09:34.200 --> 00:09:38.200]   And they bought Picasa, which is a lot of the back end as well.
[00:09:38.200 --> 00:09:43.200]   I'm looking at those Google's purchase starting in 2001 when they bought Deja News.
[00:09:43.200 --> 00:09:45.200]   I forgot about that.
[00:09:45.200 --> 00:09:50.200]   Yeah, remember Deja and Blogger, of course, was their third big acquisition.
[00:09:50.200 --> 00:09:52.200]   They bought AdSense. They didn't make that up.
[00:09:52.200 --> 00:09:57.200]   They bought their whole AdTech stack, their server management, their mobile platform, their video platform.
[00:09:57.200 --> 00:10:04.200]   You know, customer service, HR software, like all of it, their Docs platform, all came from other companies.
[00:10:04.200 --> 00:10:05.200]   What about Chrome?
[00:10:05.200 --> 00:10:06.200]   I mean, what about the browser?
[00:10:06.200 --> 00:10:10.200]   Chrome, you're right. Chrome is a browser they did build internally.
[00:10:10.200 --> 00:10:11.200]   You're right. That's an omission.
[00:10:11.200 --> 00:10:12.200]   Thank you.
[00:10:12.200 --> 00:10:15.200]   Well, I think a bad project is, of course, the CEO today.
[00:10:15.200 --> 00:10:16.200]   So, no, it was in charge.
[00:10:16.200 --> 00:10:17.200]   Right.
[00:10:17.200 --> 00:10:19.200]   Two and a half successful products. You're absolutely right.
[00:10:19.200 --> 00:10:20.200]   But still, I mean--
[00:10:20.200 --> 00:10:21.200]   That's an increase in 30%.
[00:10:21.200 --> 00:10:24.200]   And I don't mean to be flip here, right?
[00:10:24.200 --> 00:10:29.200]   But it's also true of lots of other companies that they're buying their way to glory.
[00:10:29.200 --> 00:10:41.200]   And the reason I bring that up, because you asked me about antitrust, is that historically, companies were prohibited from both merging with major competitors and also buying nascent competitors that they might neutralize on their way to become.
[00:10:41.200 --> 00:10:54.200]   And you know, the modern antitrust that was practiced for the 40 years of kind of Reagan-omics that seems to be coming to a close was extraordinarily tolerant of acquisitions as a growth strategy.
[00:10:54.200 --> 00:10:57.200]   But as I say, I think that's coming to a close.
[00:10:57.200 --> 00:11:00.200]   In the UK, the competition in market's authority is challenging.
[00:11:00.200 --> 00:11:05.200]   You know, even small acquisitions like Facebook buying Jiffy, which I refuse to call Giffy.
[00:11:05.200 --> 00:11:10.200]   And, you know, I think you're going to see more of this.
[00:11:10.200 --> 00:11:13.200]   And I think it's a necessary corrective.
[00:11:13.200 --> 00:11:33.200]   I mean, the extent to which VCs have become effectively like corporate recruiters who basically say, "All right, we're going to put a little money into a startup whose purpose is to basically produce a post-graduate portfolio piece that we're going to pretend as a product, just to prove that they can work together as a team."
[00:11:33.200 --> 00:11:38.200]   And then a tech company will buy them, throw away the product and just put them to work.
[00:11:38.200 --> 00:11:42.200]   And the VCs' equity will just be like a finder's fee.
[00:11:42.200 --> 00:11:50.200]   It's a grotesque and wasteful way to conduct business to say nothing of to get innovative products into the market.
[00:11:50.200 --> 00:11:55.200]   I'm just looking at the list of acquisitions Google has made.
[00:11:55.200 --> 00:12:02.200]   The many great companies that I've loved when they were around that have basically disappeared.
[00:12:02.200 --> 00:12:04.200]   Picasso's a good example.
[00:12:04.200 --> 00:12:06.200]   Jai Ku's a good example.
[00:12:06.200 --> 00:12:10.200]   You know, you can go on and on and on.
[00:12:10.200 --> 00:12:16.200]   They've basically been a graveyard feed burner, Grand Central, which became--
[00:12:16.200 --> 00:12:18.200]   I mean, feed burner's still in there.
[00:12:18.200 --> 00:12:21.200]   Yeah, and so is Google Voice as Grand Central.
[00:12:21.200 --> 00:12:27.200]   Although part of the problem with Google killing stuff is it makes people nervous about adopting Google services
[00:12:27.200 --> 00:12:31.200]   because there's always this risk that Google's going to lose interest.
[00:12:31.200 --> 00:12:40.200]   So Alex, you're saying this is kind of a salubrious adjustment of their financial maturity, but it's also risky, isn't it?
[00:12:40.200 --> 00:12:42.200]   I do think it's risky.
[00:12:42.200 --> 00:12:48.200]   Yeah, I think this is the-- I made this point in big technology a couple of weeks ago talking about how, you know,
[00:12:48.200 --> 00:12:52.200]   you might end up seeing stuff that Wall Street likes a lot in the short term.
[00:12:52.200 --> 00:12:55.200]   Things like greater profitability.
[00:12:55.200 --> 00:12:58.200]   They might even make some offensive moves to try to take out some competitors.
[00:12:58.200 --> 00:13:05.200]   But in the long term, I think focusing on profitability, especially when it comes to big tech companies that are 10, 20,
[00:13:05.200 --> 00:13:11.200]   30, 40 years old, leaves them vulnerable to outside challenges in a way that they weren't before.
[00:13:11.200 --> 00:13:19.200]   Google is at least trying to make people whole who bought into Stadia. They're going to refund your fancy controller
[00:13:19.200 --> 00:13:25.200]   and any games you bought, which is actually a big deal because you had to buy the games to play them on Stadia.
[00:13:25.200 --> 00:13:28.200]   But they are not going to refund your subscription fees.
[00:13:28.200 --> 00:13:36.200]   And there are some people who are a little miffed about Google pulling the plug like a YouTuber named ItsColorTV
[00:13:36.200 --> 00:13:46.200]   who says he had devoted 5,907 hours to building up a character in Red Dead Redemption 2, a character which will be lost.
[00:13:46.200 --> 00:13:59.200]   That's 246 days lost in January when Google pulls the plug because there's currently no way to transfer that character to your own copy of Red Dead Redemption.
[00:13:59.200 --> 00:14:10.200]   I'm sorry about it. For folks who think that it's a good idea to invest in products on other people's platforms without thinking that there's a risk that the platform might pull the rug,
[00:14:10.200 --> 00:14:17.200]   it's absurd at this point. Every single thing that people build on one of these big tech platforms, you have to understand that
[00:14:17.200 --> 00:14:23.200]   for some reason they decide that they don't want to support it anymore, it's done.
[00:14:23.200 --> 00:14:28.200]   But you don't have a choice is the problem.
[00:14:28.200 --> 00:14:35.200]   Where do you go if you're not going to build on, at this point you can't build on any, it's YouTube or TikTok.
[00:14:35.200 --> 00:14:48.200]   My son is 2.1 million followers on TikTok is starting to build a career as a TikTok creator, but it's not like you could do that on your own blog anymore.
[00:14:48.200 --> 00:14:53.200]   Of course, it's different. Even if you do it.
[00:14:53.200 --> 00:14:58.200]   You do it with your understanding that it might just be a moment in time.
[00:14:58.200 --> 00:15:03.200]   I think we all have to be okay with that when we're on these other platforms because there's no other way around it.
[00:15:03.200 --> 00:15:09.200]   They are also, by the way, say TikTok is providing that opportunity for your son to create that business.
[00:15:09.200 --> 00:15:11.200]   It's a great opportunity. It wouldn't be there.
[00:15:11.200 --> 00:15:20.200]   It goes both ways, but all this stuff really needs to be viewed as potentially temporary situations because that could always flip over time.
[00:15:20.200 --> 00:15:21.200]   Often it does.
[00:15:21.200 --> 00:15:25.200]   I agree with your cautionary note there.
[00:15:25.200 --> 00:15:32.200]   I do think that when we say there's no other way, you're right that there is no other way right now, but it's not like there's no conceivable other way.
[00:15:32.200 --> 00:15:40.200]   When RSS was designed and its core was this idea of blocking lock-in and orphaning and so on.
[00:15:40.200 --> 00:15:46.200]   There's an XML directive you can put with RSS that says this feed is permanently moved to a different address.
[00:15:46.200 --> 00:15:50.200]   If you and your hosting company part ways, you just send that directive out.
[00:15:50.200 --> 00:15:58.200]   The next time a pod catcher pulls down that XML file, it goes, "Oh, I'm just going to relocate my bookmark for this to a different server."
[00:15:58.200 --> 00:16:08.200]   You can just take your audience with you. I understand why YouTube hasn't built that, but I don't understand why we shouldn't want them to build it.
[00:16:08.200 --> 00:16:20.200]   If we're going to get a better deal for creators from YouTube, one of the ways that we'll do that is by YouTube having a legitimate fear that if they give creators a bad deal, that those creators will go elsewhere.
[00:16:20.200 --> 00:16:31.200]   It's a separate issue to the one about this Red Dead Redemption character, but again, at least within a single game, it's easy to see how those stats could be moved over.
[00:16:31.200 --> 00:16:44.200]   This is just a small database entry, but I think that firms don't like doing this because they like their app store to be a sole portal into payments and other sources of value.
[00:16:44.200 --> 00:16:50.200]   If they do create that interoperability, there is that possibility that customers will jump ship.
[00:16:50.200 --> 00:16:58.200]   If they're taking a 30% rake or a 15% rake or whatever it is, Stadia was taking out a Red Dead Redemption's publishers.
[00:16:58.200 --> 00:17:02.200]   The inability to port a character is a feature and not a bug.
[00:17:02.200 --> 00:17:05.200]   It's not a technical challenge.
[00:17:05.200 --> 00:17:16.200]   YouTube could disclose to Red Dead Redemption all of that material and give publishers the technical means to move that character over.
[00:17:16.200 --> 00:17:17.200]   Small file.
[00:17:17.200 --> 00:17:19.200]   They choose not to.
[00:17:19.200 --> 00:17:24.200]   Don't you think that if you spend the money creating this stuff, you should be able to make the rules?
[00:17:24.200 --> 00:17:26.200]   Let me give an example like brick and mortar store.
[00:17:26.200 --> 00:17:44.200]   If you have a restaurant and you shouldn't just be required to store all the person's food preferences so that they can then go to another restaurant and basically mimic exactly what you've been giving them without having that restaurant and having to work for it.
[00:17:44.200 --> 00:17:46.200]   That should be something that starts from the ground up.
[00:17:46.200 --> 00:17:49.200]   I mean, but the user should be able to.
[00:17:49.200 --> 00:17:54.200]   I mean, honestly, I know what my preferences are, so I can bring them with me.
[00:17:54.200 --> 00:17:59.200]   But if somehow the restaurant was able to lock up my preferences, that wouldn't be a good situation.
[00:17:59.200 --> 00:18:00.200]   Would it?
[00:18:00.200 --> 00:18:01.200]   Of course, yeah.
[00:18:01.200 --> 00:18:05.200]   Listen, I think that if you spend the effort, you run the business, you're spending the time.
[00:18:05.200 --> 00:18:07.200]   I'll never go to that restaurant then.
[00:18:07.200 --> 00:18:10.200]   If they're going to steal my preferences and keep them both.
[00:18:10.200 --> 00:18:15.200]   Let's say you go to pizza restaurant number one.
[00:18:15.200 --> 00:18:24.200]   And they name a pie for me, the Leo pie, which is got grapes, tomatoes, peep, pepperoni.
[00:18:24.200 --> 00:18:28.200]   And then I can't take that to another restaurant because it's now old.
[00:18:28.200 --> 00:18:36.200]   No, but other restaurants should be able to make that, but there shouldn't be requirement to go to this restaurant.
[00:18:36.200 --> 00:18:39.200]   And so please send the rest of download Leo's recipe.
[00:18:39.200 --> 00:18:40.200]   And then give it to the competitor.
[00:18:40.200 --> 00:18:42.200]   That competitor should have to figure out on their own.
[00:18:42.200 --> 00:18:47.200]   But RSS is a good example because, I mean, this whole network is dependent on RSS.
[00:18:47.200 --> 00:18:49.200]   That's why podcasting works.
[00:18:49.200 --> 00:19:01.200]   But didn't the platforms come along and say, yeah, we're going to kill RSS, creating some sort of spurious allegation that RSS isn't working or it's not right or it's not good.
[00:19:01.200 --> 00:19:02.200]   They did the same thing with XMPP.
[00:19:02.200 --> 00:19:04.200]   They don't want interoperability.
[00:19:04.200 --> 00:19:07.200]   So they actively kill it, don't they?
[00:19:07.200 --> 00:19:12.200]   And let me be clear, I'm not saying necessarily that I want YouTube to be forced to do this.
[00:19:12.200 --> 00:19:19.200]   But in the absence of meaningful competition, YouTube is neither being disciplined by firms nor by regulators.
[00:19:19.200 --> 00:19:24.200]   And that means that they can, their corporate preferences carry an enormous amount of weight.
[00:19:24.200 --> 00:19:35.200]   And that furthermore, the lack of competition, which arises in part out of this anti-competitive vertical integration that I was just talking about where Google just buys the companies that might later compete with it,
[00:19:35.200 --> 00:19:38.200]   means that it has enormous power over policy.
[00:19:38.200 --> 00:19:48.200]   So while I support absolutely YouTube's fair use claims, I was a staunch supporter of YouTube when Viacom was suing them for a billion dollars.
[00:19:48.200 --> 00:19:55.200]   I think that it's not YouTube's job to make sure Viacom's business model is intact or to respect their business model.
[00:19:55.200 --> 00:19:58.200]   I think what's sauce for the goose is sauce for the gander.
[00:19:58.200 --> 00:20:07.200]   And if you were to try and make a tool that allowed a YouTube broadcaster to take their audience with them to arrive a platform,
[00:20:07.200 --> 00:20:17.200]   which is, I think, analogizes to the kinds of things that people did, what YouTube did when they made a tool that allowed people to take the video they liked and put it on YouTube,
[00:20:17.200 --> 00:20:23.200]   and only have to respond to take down and so on, YouTube would reduce you to like radioactive rubble.
[00:20:23.200 --> 00:20:28.200]   They'd say you violated their terms of service. They'd say you violated the Computer Fraud and Abuse Act.
[00:20:28.200 --> 00:20:36.200]   They'd say you violated the DMCA, and they would put you out of business for doing unto them what they did unto others.
[00:20:36.200 --> 00:20:44.200]   I'm just saying that they should face the same competitive pressure that gave rise to something as innovative and great as YouTube,
[00:20:44.200 --> 00:20:47.200]   so that the next innovative great thing can come along.
[00:20:47.200 --> 00:20:52.200]   That seems fair. You wouldn't be against that, right, Alex?
[00:20:52.200 --> 00:20:56.200]   I mean, you don't have to--
[00:20:56.200 --> 00:21:01.200]   Yeah, well, I mean, is there a bill in the Senate to promote interoperability?
[00:21:01.200 --> 00:21:02.200]   Is that the--
[00:21:02.200 --> 00:21:05.200]   Yeah, that's the Access Act. It wouldn't touch this, but it's--
[00:21:05.200 --> 00:21:10.200]   The Access Act is about exposing APIs for social media and a few other kinds of platforms, App Stores as well.
[00:21:10.200 --> 00:21:15.200]   And then the EU, there's the Digital Markets Act.
[00:21:15.200 --> 00:21:20.200]   They embody the punchline of that Irish joke. If you wanted to get there, I wouldn't start from here.
[00:21:20.200 --> 00:21:29.200]   You have this situation where these firms are very dominant and where they do act as gatekeepers, right,
[00:21:29.200 --> 00:21:31.200]   where they're--
[00:21:31.200 --> 00:21:37.200]   Matt Mullingweg's post about why Tumblr doesn't have porn anymore and why it never will was pretty instructive here.
[00:21:37.200 --> 00:21:41.200]   And then there's like, you know, we submit updates to Apple three times a month,
[00:21:41.200 --> 00:21:46.200]   and at any time they might just arbitrarily decide that the filters we had last month are no longer good enough,
[00:21:46.200 --> 00:21:48.200]   and then we just go out of business.
[00:21:48.200 --> 00:21:52.200]   And he's like, "I don't know how Twitter gets away with it. I don't know how Reddit gets away with it,
[00:21:52.200 --> 00:21:54.200]   but we don't, and we couldn't.
[00:21:54.200 --> 00:21:58.200]   Apple has chosen to make exceptions for some firms and not others.
[00:21:58.200 --> 00:22:02.200]   We only have 100 million users instead of, you know, however--
[00:22:02.200 --> 00:22:07.200]   We only have 10 million users instead of 100 million users, so maybe that's why.
[00:22:07.200 --> 00:22:12.200]   But it just puts Apple in the position of picking winners and losers in the marketplace.
[00:22:12.200 --> 00:22:17.200]   And I don't think the answer is to say, "Apple, you must carry all apps no matter whether you feel they're good or bad."
[00:22:17.200 --> 00:22:22.200]   But I think that its customers should be allowed to choose a different app store, right?
[00:22:22.200 --> 00:22:24.200]   It is, after all, their phone. It belongs to them.
[00:22:24.200 --> 00:22:30.200]   You know, as you say, if you add the value to it, e.g. by opening your wallet and buying it,
[00:22:30.200 --> 00:22:32.200]   it would be yours to use as you feel like.
[00:22:32.200 --> 00:22:39.200]   And, you know, the fact that Apple doesn't have to face meaningful competition from other app stores
[00:22:39.200 --> 00:22:44.200]   for the hardware it sold means that it can act in this very high-handed and opaque way.
[00:22:44.200 --> 00:22:50.200]   And it does just, you know, it gets to structure the entire mobile market, kind of--
[00:22:50.200 --> 00:22:54.200]   Or half of it, the other half being structured by Google, but no one elected them.
[00:22:54.200 --> 00:23:00.200]   And, you know, mostly what they use to attain that structuring is not technology, but the law.
[00:23:00.200 --> 00:23:05.200]   It's the fact that if you were to try to unlock an Apple phone and, you know, sell a dongle that Jill broke your phone
[00:23:05.200 --> 00:23:08.200]   and let you choose another app store, Apple would sue you.
[00:23:08.200 --> 00:23:13.200]   So they're happy to have the state regulator step in and prevent people from offering, you know,
[00:23:13.200 --> 00:23:17.200]   more choice to people who own vices who want to use their property in different ways.
[00:23:17.200 --> 00:23:22.200]   But they abhor regulation when someone steps in and tells them how to use their property.
[00:23:22.200 --> 00:23:29.200]   I would actually prefer to just withdraw the legal protections from Apple, not impose new obligations on them.
[00:23:29.200 --> 00:23:34.200]   You say the Access Act should be modified to allow a right of private lawsuit.
[00:23:34.200 --> 00:23:35.200]   Yeah.
[00:23:35.200 --> 00:23:39.200]   Which is what Texas has done with their gun laws and their abortion laws.
[00:23:39.200 --> 00:23:42.200]   Actually, California did it with a gun laws, Texas.
[00:23:42.200 --> 00:23:43.200]   Yeah, with the gun laws.
[00:23:43.200 --> 00:23:46.200]   It's pretty-- that's slightly different because it's disinterested third parties.
[00:23:46.200 --> 00:23:54.200]   No, this is just like, you know, there are some statutes that only a public prosecutor can invoke and some that the public can invoke.
[00:23:54.200 --> 00:24:02.200]   So imagine if you went to the muffler shop and they wrecked your car, and the only way you could sue them is if you could get your local attorney general to sue them.
[00:24:02.200 --> 00:24:04.200]   That's what it's like when there's no private right of action.
[00:24:04.200 --> 00:24:11.200]   So private right of action is if you individually were harmed by someone who violated a statute, you can hire a lawyer to sue them.
[00:24:11.200 --> 00:24:16.200]   Statutes that don't have a private right of action require a public prosecutor to bring action.
[00:24:16.200 --> 00:24:23.200]   And sometimes that's appropriate, but I think like with both privacy and with the Access Act, a private right of action makes sense.
[00:24:23.200 --> 00:24:24.200]   Yeah.
[00:24:24.200 --> 00:24:29.200]   I think the fear is that it would jam the courts with a bunch of frivolous action as well.
[00:24:29.200 --> 00:24:33.200]   Although, you know, I don't see that in Texas. I don't see it in California, so maybe--
[00:24:33.200 --> 00:24:39.200]   And you can just-- you can have something like a slap act where you can have early motions to dismiss.
[00:24:39.200 --> 00:24:43.200]   You can also have fee recovery, which discourages that kind of thing.
[00:24:43.200 --> 00:24:53.200]   If it's loser pays, people aren't going to bring spurious lawsuits because the other side will be like, great, I'm just going to hire a lawyer on contingency to rack up giant billings for your frivolous lawsuit.
[00:24:53.200 --> 00:24:55.200]   At the end, I'll take it out of your hide.
[00:24:55.200 --> 00:24:59.200]   It's kind of telling that we'd have to rewrite our tort system in order for this to work.
[00:24:59.200 --> 00:25:04.200]   Well, it's mostly illegal protection and not a technical one.
[00:25:04.200 --> 00:25:13.200]   It's that as a technical matter, jailbreaking has been of varying degrees of difficulty at different times in Apple's devices history.
[00:25:13.200 --> 00:25:18.200]   But as a legal matter, the difficulty has stayed constant.
[00:25:18.200 --> 00:25:31.200]   You know, you have like checkmate, which is a jailbreak against all of the secure enclaves for eight years with iPhone models that cannot be remediated because the secure enclave is not field-up-datable because that's the whole point.
[00:25:31.200 --> 00:25:35.200]   If you can modify the secure enclave, then it's not secure.
[00:25:35.200 --> 00:25:50.200]   And so, you know, hypothetically, someone could develop a jailbreak-based third-party app store that leverage checkmate for anyone who bought an iPhone over the first eight years of its existence or, you know, year four through 12 of its existence.
[00:25:50.200 --> 00:25:54.200]   But they can't because Apple would come after you under the DMCA.
[00:25:54.200 --> 00:26:02.200]   So Matt Mullenweg, in fact, I wish I'd asked him this because he was on our show a couple of weeks ago about the porn thing.
[00:26:02.200 --> 00:26:12.200]   There was a kind of a movement on Tumblr people saying, "Oh, Tumblr's bringing porn back. Look at this. Look at this. Look at this." And Matt had to write a blog post saying, "No, no, it's not coming back. It's never coming back."
[00:26:12.200 --> 00:26:19.200]   Is it fair for Apple or for Matt to blame Apple for this, Alex?
[00:26:19.200 --> 00:26:24.200]   I'm not familiar with the controversy here. I know that Tumblr used to be Philip porn.
[00:26:24.200 --> 00:26:26.200]   Yeah, Verizon killed it.
[00:26:26.200 --> 00:26:43.200]   Matt said, "Basically, that's the old Tumblr. Back in 2006, you could do that. But nowadays, because Apple would just knock us out of the App Store, and that's 40% of our sign-ups and 85% of our page views from mobile, we'd be out of business."
[00:26:43.200 --> 00:26:53.200]   Yeah, I think I look at a certain point, you've got to let the companies that are running these products make their own decisions. Apple has a reason for not wanting to have a porn point.
[00:26:53.200 --> 00:26:58.200]   Yeah, but his point is they let Twitter and Reddit both have considerable amount of adult content.
[00:26:58.200 --> 00:27:02.200]   Well, I do think that they're going to want to be consistent in the application of the rules that they're putting up.
[00:27:02.200 --> 00:27:07.200]   He says they're too big for Apple to block. So they decided to make an example out of Tumblr.
[00:27:07.200 --> 00:27:13.200]   If that's the case, which I think is right, I think Matt knows better than anybody. And I like Matt and I trust him.
[00:27:13.200 --> 00:27:17.200]   That's a really good example of Apple misusing its market power.
[00:27:17.200 --> 00:27:23.200]   Well, I think the one way that Apple could really do a better job is make sure that it gets some of the scams out of the App Store.
[00:27:23.200 --> 00:27:32.200]   I don't know if Apple's making the best use, if it's time being the morality, please, on apps like Tumblr, and if it's going to be better, be consistent.
[00:27:32.200 --> 00:27:39.200]   But there's so many scam apps in the App Store. These are well documented. They exist in the US and outside, largely outside.
[00:27:39.200 --> 00:27:47.200]   And the companies trying to make us think that 30% fee is worth it. Work on getting those scams out of the App Store first.
[00:27:47.200 --> 00:27:52.200]   And then we can go the level down and talk about decency on the apps.
[00:27:52.200 --> 00:28:09.200]   Yeah, and thanks to Costa Altherio, who exposes, this makes, he's made this his job ever since Apple blocked his very useful tool for writing text on an Apple Watch, allowing others through.
[00:28:09.200 --> 00:28:21.200]   He's made it his life work to find scams on the App Store. Apparently he had a deal with, he made a deal with Apple over his apps being blocked.
[00:28:21.200 --> 00:28:24.200]   So he's had to stop talking about that.
[00:28:24.200 --> 00:28:25.200]   Well, that's terrible.
[00:28:25.200 --> 00:28:33.200]   By the way, can I ask you a question? So I think that there's been a very interesting arc to these conversations that we've had about Big Tech.
[00:28:33.200 --> 00:28:39.200]   The first one was a recognition of the fact that these apps and companies have just become too big.
[00:28:39.200 --> 00:28:47.200]   And it all happened so fast, right? Facebook went from like 500 million users to a billion to a couple billion users, like in a blink of an eye.
[00:28:47.200 --> 00:28:53.200]   Amazon went from being a percentage of online retail to being online retail effectively.
[00:28:53.200 --> 00:29:00.200]   In a moment, notice Apple from one trillion or a couple hundred billion dollar company to a three trillion dollar company in a moment.
[00:29:00.200 --> 00:29:04.200]   So then everyone's like, okay, these companies are the biggest companies in the world.
[00:29:04.200 --> 00:29:09.200]   And they are squeezing out competition, which is definitely true. We need to regulate them.
[00:29:09.200 --> 00:29:15.200]   And then we had this flood of ideas and bills that have come and tried to rein them in.
[00:29:15.200 --> 00:29:20.200]   I just wonder if they're going too far. Some of them seem like they make a lot of sense to me, right?
[00:29:20.200 --> 00:29:27.200]   Like the idea that a platform cannot privilege its own products by using the data that it gets from companies that have to go through them.
[00:29:27.200 --> 00:29:28.200]   That makes sense.
[00:29:28.200 --> 00:29:32.200]   But the whole idea of cutting off acquisitions, okay, most M&A fails.
[00:29:32.200 --> 00:29:38.200]   So the idea that M&A is the only thing that's made these companies successful doesn't really make sense to me.
[00:29:38.200 --> 00:29:40.200]   The idea of data portability.
[00:29:40.200 --> 00:29:45.200]   I understand the tenets of data portability, but people are on Facebook, people are on Twitter for the network.
[00:29:45.200 --> 00:29:49.200]   It's not like you can just take your data and go somewhere else and be okay.
[00:29:49.200 --> 00:29:54.200]   And all this is happening in the context of a market that's punishing these companies ruthlessly.
[00:29:54.200 --> 00:30:00.200]   I mean, Facebook's down 57% this year in the stock market and getting kneecapped by TikTok.
[00:30:00.200 --> 00:30:09.200]   So I would just wonder if how far, if we've gone too far on, let's regulate the fair competition back in the market without it.
[00:30:09.200 --> 00:30:16.200]   Remembering that we have a market economy and letting the market do its work as it seems to be doing this year.
[00:30:16.200 --> 00:30:18.200]   I mean, I agree.
[00:30:18.200 --> 00:30:25.200]   I think that it's important to distinguish between the dynamics that drive growth and the dynamics that maintain growth.
[00:30:25.200 --> 00:30:29.200]   So growth, I think, is driven, it's well understood by network effects.
[00:30:29.200 --> 00:30:32.200]   You know, you joined Facebook because you wanted to talk to people who were there.
[00:30:32.200 --> 00:30:34.200]   They joined Facebook because they wanted to talk to you.
[00:30:34.200 --> 00:30:38.200]   You made an app for the App Store because you wanted to sell it to Apple customers.
[00:30:38.200 --> 00:30:42.200]   Apple customers bought iPhones because they wanted to use your app.
[00:30:42.200 --> 00:30:45.200]   And so there's this virtuous cycle that drives growth.
[00:30:45.200 --> 00:30:50.200]   But intrinsically, technology has really low switching costs because computers are universal.
[00:30:50.200 --> 00:30:54.200]   The only computer we know how to make is the computer that can run all the software.
[00:30:54.200 --> 00:30:55.200]   We know how to write.
[00:30:55.200 --> 00:30:58.200]   It's the, you know, Turing machine, the von Neumann machine.
[00:30:58.200 --> 00:31:02.200]   And so historically, you know, when you had firms that had choke points in the market,
[00:31:02.200 --> 00:31:07.200]   the way Microsoft did in the early 2000s, when it wouldn't maintain the Mac office product.
[00:31:07.200 --> 00:31:11.200]   And it became harder and harder for CIOs to justify having Macs in the office.
[00:31:11.200 --> 00:31:13.200]   I was a CIO back then.
[00:31:13.200 --> 00:31:20.200]   We started putting Windows machines on designers' desk so that they could access Word files and Excel files and PowerPoint files without corrupting them.
[00:31:20.200 --> 00:31:27.200]   Eventually, we just put bigger graphics cards in them through away their Macs and bought them Adobe Suite for Windows.
[00:31:27.200 --> 00:31:38.200]   And the way Apple resolved that was not by asking the law to regulate Microsoft, nor was it by telling people that they like Macs better,
[00:31:38.200 --> 00:31:40.200]   and so they should just hang in there.
[00:31:40.200 --> 00:31:42.200]   They reverse engineered Microsoft's products.
[00:31:42.200 --> 00:31:50.200]   They made pages, keynote and numbers, which are reverse engineered, the file formats of Excel, PowerPoint and Word.
[00:31:50.200 --> 00:31:56.200]   And they made them feature compatible and they kept a team on that so that every time our Microsoft updated their file formats,
[00:31:56.200 --> 00:32:01.200]   Apple updated their file formats in parallel so that they could maintain compatibility,
[00:32:01.200 --> 00:32:04.200]   that's a thing that has been ended.
[00:32:04.200 --> 00:32:11.200]   The mechanisms under which we used to do that have now been made illegal under tortious interference with contract,
[00:32:11.200 --> 00:32:15.200]   under non-disclosure and non-compete, under the Computer Fraud and Abuse Act,
[00:32:15.200 --> 00:32:18.200]   Section 12, one of the Digital Millennium Copyright Act and so on.
[00:32:18.200 --> 00:32:23.200]   We've created this like a number of laws that boil down to felony contempt of business model.
[00:32:23.200 --> 00:32:28.200]   And what that's done is it's made it possible for firms that have attained dominance through network effects,
[00:32:28.200 --> 00:32:34.200]   but would historically have faced the risk of losing that dominance also through network effects,
[00:32:34.200 --> 00:32:39.200]   because the corollary of a service that gets more valuable every time joins, someone joins,
[00:32:39.200 --> 00:32:43.200]   is a service that gets less valuable every time someone leaves.
[00:32:43.200 --> 00:32:46.200]   And so you're prone to these bank runs on your users.
[00:32:46.200 --> 00:32:51.200]   As we sort of see happening with Facebook now, where people are leaving Facebook and advertisers are leaving
[00:32:51.200 --> 00:32:54.200]   and then people leaving and advertisers are leaving.
[00:32:54.200 --> 00:32:58.200]   So restoring that interoperability, the right to interoperate,
[00:32:58.200 --> 00:33:03.200]   when Facebook extended membership to non-EDU addresses,
[00:33:03.200 --> 00:33:06.200]   all of the users that had hoped to court were already on MySpace,
[00:33:06.200 --> 00:33:11.200]   and rather than telling them you should pick a day when all of you quit MySpace and come to Facebook
[00:33:11.200 --> 00:33:14.200]   or you should maintain two separate clients, it gave them a bot,
[00:33:14.200 --> 00:33:18.200]   and you could load that bot with your login and password for MySpace
[00:33:18.200 --> 00:33:22.200]   and it would go and scrape your waiting messages and put them in your Facebook inbox,
[00:33:22.200 --> 00:33:25.200]   and then it would let you reply to them and push them into your MySpace outbox.
[00:33:25.200 --> 00:33:28.200]   And if you tried to do that to Facebook today,
[00:33:28.200 --> 00:33:33.200]   or if you tried to reverse engineer Apple's App Store and produce a feature compatible App Store today,
[00:33:33.200 --> 00:33:37.200]   the way Apple did to Microsoft, they would destroy you.
[00:33:37.200 --> 00:33:41.200]   And so you're right, there's a market dynamic that drove this growth,
[00:33:41.200 --> 00:33:43.200]   but it's not a market dynamic that maintained the growth.
[00:33:43.200 --> 00:33:47.200]   The thing that maintained the growth was the capture of regulation
[00:33:47.200 --> 00:33:52.200]   to prevent new firms from doing to these firms what they did when they were new firms.
[00:33:52.200 --> 00:33:57.200]   Well, I also wonder, sorry, Leo, you can tell me you could rate me in, but...
[00:33:57.200 --> 00:33:58.200]   No, no, no, this is good.
[00:33:58.200 --> 00:34:02.200]   And then I'm going to bring up some other congressional legislation
[00:34:02.200 --> 00:34:05.200]   and a bit that is probably misguided, but go ahead.
[00:34:05.200 --> 00:34:07.200]   If this is the case, then how do you explain...
[00:34:07.200 --> 00:34:10.200]   And by the way, you know, just for the... I'm trying to learn here.
[00:34:10.200 --> 00:34:15.200]   So how do you explain the notion that Figmo, which effectively does what Adobe does
[00:34:15.200 --> 00:34:20.200]   but does it on the browser, just sold to Adobe for $20 billion,
[00:34:20.200 --> 00:34:27.200]   and what was absolutely a defensive move, because Adobe knew that Figmo was going to kick its butt
[00:34:27.200 --> 00:34:29.200]   if it let it continue to grow.
[00:34:29.200 --> 00:34:31.200]   And another thing...
[00:34:31.200 --> 00:34:35.200]   And by the way, I'm surprised, but the consensus seems to be that that's going to be allowed to go through.
[00:34:35.200 --> 00:34:39.200]   When, to me, Corey, this is exactly what you're talking about.
[00:34:39.200 --> 00:34:44.200]   But Figmo's ability to succeed is effectively also like pretty impressive, right?
[00:34:44.200 --> 00:34:49.200]   And then there's the other... Like with this acquisition, Adobe is going to become Figmo effectively.
[00:34:49.200 --> 00:34:55.200]   The other thing that I wonder about is what happens when we, you know, we have these conversations
[00:34:55.200 --> 00:34:59.200]   in the context that, you know, nothing's going to change.
[00:34:59.200 --> 00:35:04.200]   But what happens when we move platforms and we go to, you know, augmented reality, for instance.
[00:35:04.200 --> 00:35:10.200]   Like the people who are building the operating systems and the hardware for augmented reality right now,
[00:35:10.200 --> 00:35:13.200]   you know, we don't know who's going to win that.
[00:35:13.200 --> 00:35:20.200]   And that can, again, just like we move from desktop to mobile and, you know, even downloaded to the cloud,
[00:35:20.200 --> 00:35:22.200]   you know, that could also throw things out.
[00:35:22.200 --> 00:35:27.200]   Well, this is an opportunity with the metaverse to say, let's do this differently,
[00:35:27.200 --> 00:35:32.200]   because otherwise you're going to have Apple's metaverse, you're going to have a meta's metaverse,
[00:35:32.200 --> 00:35:37.200]   you're going to have maybe Microsoft's metaverse, and you're going to have to choose one of the other one
[00:35:37.200 --> 00:35:39.200]   we'll win because that's second life metaverse.
[00:35:39.200 --> 00:35:40.200]   Second life.
[00:35:40.200 --> 00:35:46.200]   And don't laugh at me, but I think that the metaverse is going to be largely enterprise,
[00:35:46.200 --> 00:35:50.200]   and there's this Facebook advertisement that I think I'm going to write about soon.
[00:35:50.200 --> 00:35:54.200]   That's all about enterprise use of enterprise uses of the metaverse.
[00:35:54.200 --> 00:35:58.200]   I really believe that the metaverse is going to be enterprise, not social.
[00:35:58.200 --> 00:36:03.200]   And if it's, well, I think Zuck is hoping it will be more than just enterprise,
[00:36:03.200 --> 00:36:07.200]   but well, exactly, but you look at their advertising and they're starting to see what it all.
[00:36:07.200 --> 00:36:09.200]   And Microsoft clearly made the decision.
[00:36:09.200 --> 00:36:12.200]   They said, yeah, the HoloLens isn't going to be a consumer product.
[00:36:12.200 --> 00:36:13.200]   Exactly.
[00:36:13.200 --> 00:36:15.200]   So, but let's throw another competitor in.
[00:36:15.200 --> 00:36:19.200]   And again, this is where I'm hoping no one laughs, but magic leap.
[00:36:19.200 --> 00:36:21.200]   You know, their second generation device is not bad.
[00:36:21.200 --> 00:36:22.200]   Really?
[00:36:22.200 --> 00:36:25.200]   And it's geared entirely towards enterprises.
[00:36:25.200 --> 00:36:26.200]   Yeah.
[00:36:26.200 --> 00:36:28.200]   And it's a capital overhang though.
[00:36:28.200 --> 00:36:30.200]   They have a lot of money they own to see.
[00:36:30.200 --> 00:36:33.200]   But when you go through shifts, little things, openings for competitors.
[00:36:33.200 --> 00:36:34.200]   Yeah.
[00:36:34.200 --> 00:36:35.200]   Sorry.
[00:36:35.200 --> 00:36:40.200]   I mean, I think that the, like starting for the end of working backwards, I think there's
[00:36:40.200 --> 00:36:45.200]   a good case to be made that the metaverse, if it ever succeeds, won't be for entertainment.
[00:36:45.200 --> 00:36:49.200]   If for no other reason, then walking around with a brick on your face is an invitation for someone
[00:36:49.200 --> 00:36:51.200]   to come up and kick you in the ass.
[00:36:51.200 --> 00:36:54.200]   So, I could see why it would only be used by people who are like sitting comfortably.
[00:36:54.200 --> 00:36:57.200]   But what about augmented reality where you're kind of...
[00:36:57.200 --> 00:36:58.200]   Maybe.
[00:36:58.200 --> 00:37:00.200]   But I don't want to get too caught up in there.
[00:37:00.200 --> 00:37:05.200]   I think that the Figma story is really interesting, that what you see is exactly what I'm describing
[00:37:05.200 --> 00:37:12.200]   where there were elements of PSD that were not, which is the Photoshop document file format,
[00:37:12.200 --> 00:37:15.200]   that were not within this containment vessel.
[00:37:15.200 --> 00:37:22.200]   And so Figma was able to make feature compatible Photoshop replacement that could read and
[00:37:22.200 --> 00:37:23.200]   read your Photoshop files.
[00:37:23.200 --> 00:37:24.200]   That was key, right?
[00:37:24.200 --> 00:37:28.200]   Because people have a lot invested in their photo, existing Photoshop files.
[00:37:28.200 --> 00:37:29.920]   They can't just abandon them.
[00:37:29.920 --> 00:37:32.200]   They need to be able to open them and read them.
[00:37:32.200 --> 00:37:35.680]   PSD was reverse engineered, and that's why you can read it in the GIMP, and that's why
[00:37:35.680 --> 00:37:39.120]   you can read it in other programs and so on.
[00:37:39.120 --> 00:37:44.760]   But, you know, their response was to use their access to the capital markets to snuff out
[00:37:44.760 --> 00:37:49.520]   a competitor before it could grow to become a significant and meaningful business all
[00:37:49.520 --> 00:37:50.960]   on its own.
[00:37:50.960 --> 00:37:56.520]   And so, you know, we're left with this kind of tautology, which is that if the capital
[00:37:56.520 --> 00:38:00.840]   markets will give you enough money to buy a company, you must be the best person to run
[00:38:00.840 --> 00:38:01.840]   it.
[00:38:01.840 --> 00:38:04.000]   And the way that we can tell you're the best person to run it is you have enough money
[00:38:04.000 --> 00:38:05.080]   to buy it.
[00:38:05.080 --> 00:38:09.520]   But going back to the Google Graveyard, it's pretty clear that, you know, there are lots
[00:38:09.520 --> 00:38:14.240]   of people whom the capital markets will entrust with the money to buy a nascent competitor
[00:38:14.240 --> 00:38:19.160]   who either, you know, treat that as a predatory acquisition and snuff it out, or who are just
[00:38:19.160 --> 00:38:20.400]   not qualified to run it.
[00:38:20.400 --> 00:38:24.840]   We were making jokes about the Flickr URL at the beginning of the show before we went
[00:38:24.840 --> 00:38:25.840]   on the air.
[00:38:25.840 --> 00:38:29.560]   You know, Flickr was the first service that had mobile photos, right?
[00:38:29.560 --> 00:38:36.040]   Years before Instagram, Yahoo bought it, and then it became a plaything that was dooled
[00:38:36.040 --> 00:38:41.400]   over among the various, you know, venal princelings of the Yahoo Empire, and it suffered and fell
[00:38:41.400 --> 00:38:44.520]   into like, you know, neglect.
[00:38:44.520 --> 00:38:49.840]   And now it belongs to Smugmug who are gradually digging out more than, you know, decades of
[00:38:49.840 --> 00:38:54.600]   technology debt, but it was, you know, catastrophic to that to be acquired.
[00:38:54.600 --> 00:38:55.600]   Yeah.
[00:38:55.600 --> 00:38:59.400]   But don't you think that if a company gets acquired and then ruined that opens up the
[00:38:59.400 --> 00:39:01.920]   door for another company to come and compete?
[00:39:01.920 --> 00:39:09.120]   So for instance, you know, Figma is not the Adobe isn't killing the cloud developer design.
[00:39:09.120 --> 00:39:11.720]   You know, there's a lot of competition.
[00:39:11.720 --> 00:39:16.040]   The idea of Padpot, which is an open source version, just raised like 20 million.
[00:39:16.040 --> 00:39:17.040]   Yeah.
[00:39:17.040 --> 00:39:18.040]   So exactly.
[00:39:18.040 --> 00:39:19.040]   So this is my point.
[00:39:19.040 --> 00:39:22.440]   Those Figma can't, Penpot come in and start to compete.
[00:39:22.440 --> 00:39:27.200]   Like you, you, of course, there is always the risk when you do M and A that you're going
[00:39:27.200 --> 00:39:30.480]   to end up killing innovation, but it's not the end of the story.
[00:39:30.480 --> 00:39:31.480]   The story continues.
[00:39:31.480 --> 00:39:36.400]   If you mess something up, you can all you are going to open it up to competition.
[00:39:36.400 --> 00:39:40.520]   And if there is a need in the market for this thing to be built, it will be built.
[00:39:40.520 --> 00:39:43.080]   Well, but you get things like these.
[00:39:43.080 --> 00:39:47.720]   So there's a great efflore essence of RSS readers at one point and Google decided to launch
[00:39:47.720 --> 00:39:49.440]   its own reader.
[00:39:49.440 --> 00:39:54.920]   And that created what, what venture capitalists call the kill zone around RSS readers wanted
[00:39:54.920 --> 00:39:58.560]   to back them because there was something that was priced at a, you know, cross subsidized
[00:39:58.560 --> 00:40:03.120]   from another business at a price that was so low free that there was no way to compete
[00:40:03.120 --> 00:40:04.120]   with it.
[00:40:04.120 --> 00:40:08.960]   And as a consequence, we saw, you know, a decade of neglect, which would have been fine if
[00:40:08.960 --> 00:40:12.920]   Google had not then killed reader, but readers never recovered, right?
[00:40:12.920 --> 00:40:16.080]   We still, you know, that was the end of RSS effectively.
[00:40:16.080 --> 00:40:17.440]   It's now this kind of rump.
[00:40:17.440 --> 00:40:22.620]   I was actually just in New York doing a book event that, um, Neelike Patel, who's the editor
[00:40:22.620 --> 00:40:27.080]   and chief of the Verge was, was, um, hosting.
[00:40:27.080 --> 00:40:29.440]   And the Verge's nude redesign is amazing.
[00:40:29.440 --> 00:40:33.480]   And it had one of the cool things about it is it has a feed of articles that people in
[00:40:33.480 --> 00:40:36.240]   the Verge's newsroom think are cool.
[00:40:36.240 --> 00:40:39.160]   That's there on the front door of the Verge's new redesign.
[00:40:39.160 --> 00:40:42.280]   And I said, where can I get an RSS for that?
[00:40:42.280 --> 00:40:45.920]   And he was like, we discussed it and thought nobody would want to see an RSS.
[00:40:45.920 --> 00:40:49.160]   That's exactly what they're doing is an RSS feed.
[00:40:49.160 --> 00:40:51.680]   Can I, can I tell a funny story though?
[00:40:51.680 --> 00:40:56.800]   So I think that, um, I think there's a lot of, uh, the idea of a kill zone is, is real
[00:40:56.800 --> 00:40:57.800]   and legit.
[00:40:57.800 --> 00:40:59.720]   And I think there's a lot of truth to what you said.
[00:40:59.720 --> 00:41:05.640]   Um, okay, maybe I'm wrong here, but, but, um, I do think that, that we ended the tech
[00:41:05.640 --> 00:41:09.160]   world ended up building a replacement for RSS.
[00:41:09.160 --> 00:41:10.320]   And that was Twitter.
[00:41:10.320 --> 00:41:15.240]   And that the person who built Google reader ended up going and working with Twitter for
[00:41:15.240 --> 00:41:16.240]   a while.
[00:41:16.240 --> 00:41:20.520]   And he ended up, unfortunately bill, and he regrets this building the retweet button,
[00:41:20.520 --> 00:41:25.040]   which I think is a source of, and he thinks is a source of, you know, a lot of, uh, the
[00:41:25.040 --> 00:41:26.960]   negative effects of social media.
[00:41:26.960 --> 00:41:32.640]   So it's a mixed bag, but it, it also, the market did come in and say, here is another,
[00:41:32.640 --> 00:41:37.800]   uh, way that you can get, you know, your, your stories via a feed and that was Twitter.
[00:41:37.800 --> 00:41:43.080]   Reddit also has, I have to, by the way, I think, Neil, I, uh, answer to you, uh, Corey's
[00:41:43.080 --> 00:41:46.080]   disingenuous.
[00:41:46.080 --> 00:41:47.880]   I don't think that has, you know, how much does it cost to do an RSS feed?
[00:41:47.880 --> 00:41:48.880]   It's nothing.
[00:41:48.880 --> 00:41:50.440]   You can easily do an RSS feed.
[00:41:50.440 --> 00:41:56.040]   I think, I think it was just basically like, they, they built a lot of features.
[00:41:56.040 --> 00:41:58.160]   It's, you know, they're not an engineering organization.
[00:41:58.160 --> 00:41:59.160]   They had to prioritize them.
[00:41:59.160 --> 00:42:02.240]   And they like knows he knows better than that.
[00:42:02.240 --> 00:42:03.240]   And that's the reality.
[00:42:03.240 --> 00:42:04.240]   Yeah.
[00:42:04.240 --> 00:42:10.240]   Remember the, the verge launch, but to, to be a reference customer for its own CMS.
[00:42:10.240 --> 00:42:13.680]   And so we, like I have no idea, like if we were talking about Drupal, I'd say, yeah,
[00:42:13.680 --> 00:42:14.680]   for sure.
[00:42:14.680 --> 00:42:19.080]   You just like, you know, type in the obscure Google command, make this thing be an RSS
[00:42:19.080 --> 00:42:20.080]   feed now.
[00:42:20.080 --> 00:42:21.080]   Right.
[00:42:21.080 --> 00:42:24.520]   Uh, by the way, when, when, when, yeah, go ahead.
[00:42:24.520 --> 00:42:25.520]   No, go ahead.
[00:42:25.520 --> 00:42:28.360]   When Google killed reader, there were so many clones that came out.
[00:42:28.360 --> 00:42:32.760]   The market did try to end up, um, you know, building and replacement.
[00:42:32.760 --> 00:42:36.800]   I remember, I'll be honest, I wasn't really into RSS readers.
[00:42:36.800 --> 00:42:40.560]   And then Google killed reader and I saw the outpouring of all this English and said,
[00:42:40.560 --> 00:42:41.960]   Oh, I got to try that.
[00:42:41.960 --> 00:42:46.640]   And so I downloaded or, or set up an account on something called the old reader.
[00:42:46.640 --> 00:42:50.680]   And, um, I got that was, uh, I still use RSS.
[00:42:50.680 --> 00:42:52.080]   I use sumi.news.
[00:42:52.080 --> 00:42:56.000]   And by the way, I got a point out, sumi discovered an RSS feed at the verge.
[00:42:56.000 --> 00:43:01.560]   So it's not that feed on the column there on the right, but it's the all posts feed
[00:43:01.560 --> 00:43:02.560]   from the verge.
[00:43:02.560 --> 00:43:03.560]   So all right.
[00:43:03.560 --> 00:43:05.600]   We're still doing RSS.
[00:43:05.600 --> 00:43:07.440]   Maybe Neil, I didn't know that.
[00:43:07.440 --> 00:43:12.360]   No, no, no, they've got RSS for the main feed, the main feed, not for that side feed.
[00:43:12.360 --> 00:43:13.360]   Yeah.
[00:43:13.360 --> 00:43:14.360]   Not for that sidebar.
[00:43:14.360 --> 00:43:15.360]   That sidebar is amazing.
[00:43:15.360 --> 00:43:18.760]   It's just, it's just not convenient for me to keep a browser to open some.
[00:43:18.760 --> 00:43:19.760]   My screen.
[00:43:19.760 --> 00:43:20.760]   It's just, yeah.
[00:43:20.760 --> 00:43:24.720]   You know, uh, but, but I, I don't think he was be asking me.
[00:43:24.720 --> 00:43:29.160]   I think they just sat down and did a triage and they were like, nobody uses RSS anymore.
[00:43:29.160 --> 00:43:30.160]   Why would we bother?
[00:43:30.160 --> 00:43:34.760]   I tried to convince them that RSS is like, what the people who read the news to talk
[00:43:34.760 --> 00:43:36.240]   about the news and amplify the news.
[00:43:36.240 --> 00:43:37.240]   All I use.
[00:43:37.240 --> 00:43:38.240]   Yep.
[00:43:38.240 --> 00:43:42.680]   And so, you know, it's like it's a, we're, we're a small, but proud people, you know,
[00:43:42.680 --> 00:43:46.880]   uh, like an influential, I think influential, like Canadians.
[00:43:46.880 --> 00:43:47.880]   Yeah.
[00:43:47.880 --> 00:43:49.120]   Like Canadians.
[00:43:49.120 --> 00:43:51.600]   He seemed like such a nice guy who would have known.
[00:43:51.600 --> 00:43:53.320]   We're going to take a live version of this.
[00:43:53.320 --> 00:43:54.320]   We're going to have to take a break.
[00:43:54.320 --> 00:43:55.320]   Hold on, Alex.
[00:43:55.320 --> 00:44:00.120]   Otherwise we'll be here a little eight or nine or 10 in on your time.
[00:44:00.120 --> 00:44:01.640]   It's like three in the morning.
[00:44:01.640 --> 00:44:07.920]   So Alex Kantruitz is here, the big tech, big technology podcast technology.substack.com.
[00:44:07.920 --> 00:44:08.960]   I didn't think of it this way.
[00:44:08.960 --> 00:44:12.880]   I didn't intend it this way, but, uh, you're not here to defend big techs.
[00:44:12.880 --> 00:44:17.840]   I don't feel like you have to, uh, especially when you've got Corey Dr. Oh on the author
[00:44:17.840 --> 00:44:23.000]   of the latest choke point capitalism, which is absolutely an indictment of not just big
[00:44:23.000 --> 00:44:25.520]   tech, but big business in general.
[00:44:25.520 --> 00:44:30.040]   You point out in the book and I think it's, by the way, a great read, highly recommended.
[00:44:30.040 --> 00:44:33.240]   Uh, that it's not just tech.
[00:44:33.240 --> 00:44:35.240]   It's, it's publishing.
[00:44:35.240 --> 00:44:36.920]   It's the record in business.
[00:44:36.920 --> 00:44:38.920]   It's the live concert businesses.
[00:44:38.920 --> 00:44:40.080]   Every business there is out there.
[00:44:40.080 --> 00:44:44.520]   In fact, we've got a story we'll talk about a little bit about the podcasting business
[00:44:44.520 --> 00:44:47.640]   as well is becoming a big tech.
[00:44:47.640 --> 00:44:49.200]   I'm not a part of that, unfortunately.
[00:44:49.200 --> 00:44:51.000]   Uh, we'll have more in just a bit.
[00:44:51.000 --> 00:44:54.440]   Our show today brought to you by new Rayva.
[00:44:54.440 --> 00:44:56.720]   No Rayva's great audio simplifies.
[00:44:56.720 --> 00:44:57.720]   Today's IT pros.
[00:44:57.720 --> 00:44:58.720]   Yeah.
[00:44:58.720 --> 00:45:02.320]   But we're, we're now we're doing this hybrid thing, right?
[00:45:02.320 --> 00:45:06.840]   Hybrid working and learning means you have to equip and support more spaces with audio
[00:45:06.840 --> 00:45:11.080]   and video conferencing systems because you've got people in the office.
[00:45:11.080 --> 00:45:13.360]   You've got people elsewhere.
[00:45:13.360 --> 00:45:15.240]   They are all working together.
[00:45:15.240 --> 00:45:19.880]   Uh, and you know, your IT department still got to do network security, which is a big
[00:45:19.880 --> 00:45:20.880]   issue these days.
[00:45:20.880 --> 00:45:23.200]   And of course you're shifting to cloud based solutions.
[00:45:23.200 --> 00:45:27.040]   You got infrastructure issues, product shortages and delays.
[00:45:27.040 --> 00:45:31.840]   And then on top, it's put so much strain in IT's resources, people, time expertise and
[00:45:31.840 --> 00:45:33.200]   budgets.
[00:45:33.200 --> 00:45:36.600]   So here's a product customers will love.
[00:45:36.600 --> 00:45:41.400]   It requires minimal effort from IT to deploy and manage its scale with a bonus of requiring
[00:45:41.400 --> 00:45:47.600]   zero end user training, but it solves a big problem in hybrid work.
[00:45:47.600 --> 00:45:52.640]   When it comes to audio conferencing in larger spaces, it's very common to be faced with
[00:45:52.640 --> 00:45:55.760]   complex and expensive multi component systems.
[00:45:55.760 --> 00:45:58.320]   You got to bring in people to design them and install them.
[00:45:58.320 --> 00:46:01.880]   And then they've got to continue to maintain them and update them and manage them.
[00:46:01.880 --> 00:46:04.880]   Noreva is the opposite.
[00:46:04.880 --> 00:46:06.360]   Great audio simplifies.
[00:46:06.360 --> 00:46:12.000]   It's changed by offering solutions that deliver a high level of simplicity with Noreva.
[00:46:12.000 --> 00:46:17.720]   You put in what is, looks like a sound bar that gives you full room mic pickup.
[00:46:17.720 --> 00:46:19.400]   If you got a really big room, put in a couple.
[00:46:19.400 --> 00:46:21.600]   If you got a regular size room, put in one.
[00:46:21.600 --> 00:46:22.600]   It's a sound bar.
[00:46:22.600 --> 00:46:23.600]   It's got speakers.
[00:46:23.600 --> 00:46:30.360]   It's got microphones and it uses Noreva's patented microphone mist technology to in effect fill
[00:46:30.360 --> 00:46:33.120]   the space with microphones.
[00:46:33.120 --> 00:46:34.400]   And so easy to install.
[00:46:34.400 --> 00:46:35.400]   You could do it.
[00:46:35.400 --> 00:46:36.400]   You don't even have to call the IT department.
[00:46:36.400 --> 00:46:39.680]   Install Noreva system in less than 30 minutes.
[00:46:39.680 --> 00:46:41.440]   Maybe an hour if you've got a giant room.
[00:46:41.440 --> 00:46:42.520]   It's simple.
[00:46:42.520 --> 00:46:44.280]   You don't have to configure it.
[00:46:44.280 --> 00:46:47.600]   You don't have to tweak it.
[00:46:47.600 --> 00:46:52.320]   It's so much less expensive and so much easier than those traditional systems.
[00:46:52.320 --> 00:46:54.200]   It could take your room offline for days.
[00:46:54.200 --> 00:46:58.400]   In fact, you could start right now, get a Noreva system, install it, be ready for your
[00:46:58.400 --> 00:47:00.800]   very next meeting.
[00:47:00.800 --> 00:47:05.200]   With Noreva, you can monitor, manage, update and adjust all the systems.
[00:47:05.200 --> 00:47:10.760]   No matter how many rooms with a powerful cloud-based platform, Noreva console, IT doesn't even
[00:47:10.760 --> 00:47:12.960]   have to walk down the hall.
[00:47:12.960 --> 00:47:14.960]   Noreva is very scalable.
[00:47:14.960 --> 00:47:17.320]   They can bring their simplicity to large organizations too.
[00:47:17.320 --> 00:47:22.160]   Noreva systems, N-U-R-E-V-A, they cost a fraction of traditional systems, give you a
[00:47:22.160 --> 00:47:27.840]   better result, solves this problem of people in the office and people elsewhere trying
[00:47:27.840 --> 00:47:29.720]   to meet and work together.
[00:47:29.720 --> 00:47:31.360]   Great audio is so important to that.
[00:47:31.360 --> 00:47:32.360]   Check it out.
[00:47:32.360 --> 00:47:33.360]   Noreva.com/twit.
[00:47:33.360 --> 00:47:34.360]   N-U-R-E-V-A, Noreva.com/twit.
[00:47:34.360 --> 00:47:35.360]   This is a solution.
[00:47:35.360 --> 00:47:42.960]   I don't think people, I mean, I guess if you've been watching our shows, you know about it,
[00:47:42.960 --> 00:47:47.520]   but I wish it were better known because I still see conference room so poorly equipped for
[00:47:47.520 --> 00:47:48.520]   audio.
[00:47:48.520 --> 00:47:50.200]   You could do so much better.
[00:47:50.200 --> 00:47:52.320]   Noreva.com/twit.
[00:47:52.320 --> 00:47:55.360]   Thank you, Noreva, for supporting this week in tech.
[00:47:55.360 --> 00:48:00.960]   And thank you listeners and viewers for supporting us by going to that special address.
[00:48:00.960 --> 00:48:02.640]   I know you saw it here.
[00:48:02.640 --> 00:48:05.720]   Noreva.com/twit.
[00:48:05.720 --> 00:48:13.400]   Did Mark Zuckerberg fight in the UFC show?
[00:48:13.400 --> 00:48:17.000]   He should have.
[00:48:17.000 --> 00:48:20.720]   So Saturday, I don't know what happened.
[00:48:20.720 --> 00:48:23.360]   I'm helping somebody subscribe to this.
[00:48:23.360 --> 00:48:27.720]   UFC closed its fight card at the apex of facility owns in Vegas or the press and the
[00:48:27.720 --> 00:48:29.420]   public.
[00:48:29.420 --> 00:48:34.920]   They wouldn't say initially who did it, but an MMA insider, Ariel Hawani, said a very
[00:48:34.920 --> 00:48:40.160]   good close to the event had told him it's something to do with Mark Zuckerberg.
[00:48:40.160 --> 00:48:45.960]   Speculation is Mark's written out the event maybe just so they could watch.
[00:48:45.960 --> 00:48:48.480]   Maybe they could record it for the metaverse.
[00:48:48.480 --> 00:48:50.240]   Maybe Mark's going to get into the octagon.
[00:48:50.240 --> 00:48:51.240]   I don't know.
[00:48:51.240 --> 00:48:52.560]   Wouldn't put it past him.
[00:48:52.560 --> 00:48:55.040]   Well, the event already happened.
[00:48:55.040 --> 00:48:57.960]   Yeah, I'm asking what happened.
[00:48:57.960 --> 00:48:58.960]   It was just marks.
[00:48:58.960 --> 00:49:04.600]   I go to see if it was Mark Zuckerberg in his cronies watching, smoking those mites and
[00:49:04.600 --> 00:49:10.440]   there's this amazing reaction, Jeff of his wife, Priscilla, just kind of losing her mind
[00:49:10.440 --> 00:49:12.520]   as the fight goes on.
[00:49:12.520 --> 00:49:16.880]   But I think the more telling thing is that there were all the UFC fighters who were talking
[00:49:16.880 --> 00:49:21.280]   about how awful it was that the entire fight could be bought out by one person.
[00:49:21.280 --> 00:49:24.040]   And I think that that spot on.
[00:49:24.040 --> 00:49:26.760]   And I don't know what Mark Zuckerberg is trying to do there.
[00:49:26.760 --> 00:49:27.760]   You got enough money.
[00:49:27.760 --> 00:49:28.760]   What could you be about?
[00:49:28.760 --> 00:49:32.160]   Of course, but just the A, the optics, B, the act itself, I find like fairly, fairly
[00:49:32.160 --> 00:49:35.800]   wrong and sports is a game for the people.
[00:49:35.800 --> 00:49:43.080]   I just, you know, I can't really see any justification for wanting to, you know, buy, you know,
[00:49:43.080 --> 00:49:46.960]   the entire seating and then leave the public out of it.
[00:49:46.960 --> 00:49:51.360]   If you want to go watch, if I go watch a fight, but don't buy it, has anyone else ever done?
[00:49:51.360 --> 00:49:53.800]   I've never seen it cost to buy the octagon.
[00:49:53.800 --> 00:49:54.800]   I don't know.
[00:49:54.800 --> 00:49:58.120]   You can probably, I guess you probably do it for a million dollars.
[00:49:58.120 --> 00:50:02.760]   They make most of, they make most of their money on the, on the cable for the streams.
[00:50:02.760 --> 00:50:03.760]   Yeah.
[00:50:03.760 --> 00:50:04.760]   Yeah.
[00:50:04.760 --> 00:50:09.720]   It's like getting Kanye to play your corporate event or your kids, but minutes or, or, you
[00:50:09.720 --> 00:50:11.760]   know, Kinsenyarer or something.
[00:50:11.760 --> 00:50:14.160]   It's just such a, it's such an oligarch.
[00:50:14.160 --> 00:50:15.880]   It really is.
[00:50:15.880 --> 00:50:17.280]   Yeah, it's awesome.
[00:50:17.280 --> 00:50:23.360]   You know, I was told that when the Sultan of Brunei visited Disneyland, he had 13 tour
[00:50:23.360 --> 00:50:27.360]   guides and the vice president of operations with a retinue of over a hundred people and
[00:50:27.360 --> 00:50:30.600]   a flying wedge ahead of them.
[00:50:30.600 --> 00:50:32.280]   And they just went from ride to ride.
[00:50:32.280 --> 00:50:35.600]   You're in parts of Disneyland as they went, which is hilarious.
[00:50:35.600 --> 00:50:40.160]   It's a military to like 10 grand a day.
[00:50:40.160 --> 00:50:41.160]   You know what?
[00:50:41.160 --> 00:50:42.680]   It'd be worth it not to have to get in line.
[00:50:42.680 --> 00:50:43.680]   That's all I'm saying.
[00:50:43.680 --> 00:50:44.680]   Absolutely.
[00:50:44.680 --> 00:50:45.680]   Yeah.
[00:50:45.680 --> 00:50:46.680]   Best deal at Disneyland.
[00:50:46.680 --> 00:50:51.440]   You see every once in a while, you see videos on Insta and, and TikTok of celebrities
[00:50:51.440 --> 00:50:56.440]   getting ushered to the front of the line, you know, you just have to, you know, it's good
[00:50:56.440 --> 00:50:58.160]   business.
[00:50:58.160 --> 00:51:01.800]   I did like how David Beckham waited the full, what was it?
[00:51:01.800 --> 00:51:02.800]   He waited for the queen.
[00:51:02.800 --> 00:51:03.800]   God bless him.
[00:51:03.800 --> 00:51:04.800]   Yeah.
[00:51:04.800 --> 00:51:05.800]   God bless him.
[00:51:05.800 --> 00:51:09.880]   You know, yeah, I think a number of celebrities actually got in line for that.
[00:51:09.880 --> 00:51:10.880]   That's respectful.
[00:51:10.880 --> 00:51:11.880]   I like that.
[00:51:11.880 --> 00:51:12.880]   That is respectful.
[00:51:12.880 --> 00:51:17.140]   Well, William Gibson once told me that he feels like he has just the right amount of
[00:51:17.140 --> 00:51:21.360]   fame, you know, like what is the right amount of fame?
[00:51:21.360 --> 00:51:25.320]   People enjoy his work and they tell him so he can earn a living from it, but he, it's
[00:51:25.320 --> 00:51:27.560]   not like he can't eat dinner in a restaurant.
[00:51:27.560 --> 00:51:28.560]   Right.
[00:51:28.560 --> 00:51:34.360]   I, I once, uh, Penn Gillette was visiting London and we went out for lunch and like got interrupted
[00:51:34.360 --> 00:51:35.360]   like three times.
[00:51:35.360 --> 00:51:36.360]   No fun.
[00:51:36.360 --> 00:51:40.840]   And Penn is a moderately famous person, but you know, we're eating at a restaurant in London.
[00:51:40.840 --> 00:51:44.160]   It's not like Penn, Penn and tell are really well known in the UK.
[00:51:44.160 --> 00:51:47.440]   And nevertheless, like he couldn't get through a 40 minute meal without being interrupted
[00:51:47.440 --> 00:51:48.440]   three times.
[00:51:48.440 --> 00:51:53.000]   So, you know, I, I feel like there's probably, it's, there's, there's definitely a threshold
[00:51:53.000 --> 00:51:55.640]   where the fame gets pretty toxic.
[00:51:55.640 --> 00:52:01.440]   Steve Martin once told me that he knew it was over for him as a real person when he attempted
[00:52:01.440 --> 00:52:02.760]   to ride the subway.
[00:52:02.760 --> 00:52:08.720]   This was many, many years ago to go see a show in Brooklyn or the Queens or somewhere.
[00:52:08.720 --> 00:52:11.440]   And he said, I can never, he said on that, I can't ride.
[00:52:11.440 --> 00:52:12.440]   I can't do it.
[00:52:12.440 --> 00:52:13.440]   And I've been to dinner with him.
[00:52:13.440 --> 00:52:15.760]   He rents basically you take over a private room.
[00:52:15.760 --> 00:52:17.800]   You don't eat in public.
[00:52:17.800 --> 00:52:19.800]   And you still harassed by the chef.
[00:52:19.800 --> 00:52:20.800]   I made your tea.
[00:52:20.800 --> 00:52:23.440]   How do you, uh, talk about this relationship.
[00:52:23.440 --> 00:52:24.440]   You have a steep.
[00:52:24.440 --> 00:52:25.440]   Oh, it's well known.
[00:52:25.440 --> 00:52:26.920]   I've told people about this before.
[00:52:26.920 --> 00:52:28.520]   He, he used to listen to my radio show.
[00:52:28.520 --> 00:52:30.040]   I don't think he does any more.
[00:52:30.040 --> 00:52:35.200]   And he, he DM'd me about 10 years ago saying, you don't, you don't have to respond to this,
[00:52:35.200 --> 00:52:36.440]   but I really like your radio show.
[00:52:36.440 --> 00:52:37.840]   I said, yeah, I'm not going to respond.
[00:52:37.840 --> 00:52:39.000]   Steve Martin, who cares?
[00:52:39.000 --> 00:52:43.040]   No, I responded and kind of struck up a friendship.
[00:52:43.040 --> 00:52:44.320]   That's awesome friendship.
[00:52:44.320 --> 00:52:45.320]   Yeah.
[00:52:45.320 --> 00:52:49.880]   And, uh, it is, it is the only person who's that famous that I've ever spent time with.
[00:52:49.880 --> 00:52:52.320]   And it really, it's, it's, he can eat.
[00:52:52.320 --> 00:52:53.480]   I bet now he can't.
[00:52:53.480 --> 00:52:55.280]   Now that murder is in the building's big.
[00:52:55.280 --> 00:52:57.800]   I bet you it's actually gotten worse for him again.
[00:52:57.800 --> 00:53:04.080]   Um, but he very famously stopped doing a stadium comedy because he said it's not, it's not
[00:53:04.080 --> 00:53:05.080]   a show anymore.
[00:53:05.080 --> 00:53:07.080]   It's, um, it's like the zoo.
[00:53:07.080 --> 00:53:11.480]   It's like, you know, uh, he talks about it and born standing up in his, his book.
[00:53:11.480 --> 00:53:15.360]   Um, he did, I think he really didn't like that level of fame, but once you get there,
[00:53:15.360 --> 00:53:17.200]   you can't, there's no turning back.
[00:53:17.200 --> 00:53:19.800]   Here's Mark, by the way, practicing.
[00:53:19.800 --> 00:53:26.280]   Uh, and apparently, uh, this is his sparring partner made his debut in the octagon and
[00:53:26.280 --> 00:53:28.280]   maybe that's why he bought it up.
[00:53:28.280 --> 00:53:29.280]   Yeah.
[00:53:29.280 --> 00:53:30.280]   Well, you could just buy a ticket to it.
[00:53:30.280 --> 00:53:31.280]   I'm not.
[00:53:31.280 --> 00:53:32.280]   You could just buy one ticket.
[00:53:32.280 --> 00:53:33.280]   I guess.
[00:53:33.280 --> 00:53:36.240]   But, you know, can I just say, sorry, go ahead.
[00:53:36.240 --> 00:53:39.960]   If you mark Zuckerberg, you don't want to be harassed the whole fight.
[00:53:39.960 --> 00:53:45.160]   You know, maybe you need that flying wedge of cronies and I think that he should just,
[00:53:45.160 --> 00:53:48.800]   you want to go watch sports, go, go suck it up and go watch it in person.
[00:53:48.800 --> 00:53:49.800]   With the people.
[00:53:49.800 --> 00:53:52.320]   I mean, it's not like they don't have booths, right?
[00:53:52.320 --> 00:53:56.160]   Like they have sky booths, sex, leech, whatever.
[00:53:56.160 --> 00:53:57.160]   Private booths.
[00:53:57.160 --> 00:53:58.160]   Yeah.
[00:53:58.160 --> 00:53:59.160]   Yeah.
[00:53:59.160 --> 00:54:00.160]   I mean, the champagne room.
[00:54:00.160 --> 00:54:01.800]   It's a solved problem, right?
[00:54:01.800 --> 00:54:05.520]   Like this is like a famously solved problem, right?
[00:54:05.520 --> 00:54:10.720]   Of sports stadiums having VIPs who want to sit in a fancy booth.
[00:54:10.720 --> 00:54:14.560]   Where a mustache of a, you know, a hat, just a little disguise.
[00:54:14.560 --> 00:54:17.080]   You can rent out the private room.
[00:54:17.080 --> 00:54:18.080]   Yeah.
[00:54:18.080 --> 00:54:19.080]   That exit.
[00:54:19.080 --> 00:54:23.240]   I don't know if it exists at this UOC thing, but.
[00:54:23.240 --> 00:54:25.520]   I mean, who says yes, that's a good idea.
[00:54:25.520 --> 00:54:30.000]   It's just again, you're getting bad advice if you think that's a good idea to do this.
[00:54:30.000 --> 00:54:33.240]   Maybe not a good idea for Mark, but remember, this is also the guy who on the 4th of July
[00:54:33.240 --> 00:54:40.480]   last year posted a picture of him wearing sunscreen holding the American flag on a motorized
[00:54:40.480 --> 00:54:41.480]   foil.
[00:54:41.480 --> 00:54:45.040]   You know, Leo, that I have less of a problem with.
[00:54:45.040 --> 00:54:47.560]   You want to go make an idiot of yourself in the ocean.
[00:54:47.560 --> 00:54:49.160]   By all means people do it every day.
[00:54:49.160 --> 00:54:54.280]   I remember when he posted this that we had somebody on and said, you know, you don't,
[00:54:54.280 --> 00:55:00.280]   if you're Mark Zuckerberg, you have a phalanx of PR professionals guiding you, protecting
[00:55:00.280 --> 00:55:02.440]   your image at all times.
[00:55:02.440 --> 00:55:04.680]   How did that, I don't think it bad.
[00:55:04.680 --> 00:55:08.640]   I'm saying, you know, like, like, no one goes, Hey, you might look a little out of touch
[00:55:08.640 --> 00:55:11.520]   if you end up, you know, buying out the whole UFC game.
[00:55:11.520 --> 00:55:16.520]   And I love the fact that all the UFC fighters called, you know, BS on it because deservedly
[00:55:16.520 --> 00:55:17.520]   so.
[00:55:17.520 --> 00:55:19.800]   So that, okay, no problem with that.
[00:55:19.800 --> 00:55:25.520]   If you want to go hold a flag, make an idiot of yourself in the ocean, like have a, there's
[00:55:25.520 --> 00:55:28.400]   a big ocean, but there's only limited seats on the UFC.
[00:55:28.400 --> 00:55:30.600]   People were offended by this though.
[00:55:30.600 --> 00:55:32.280]   I do remember they were very offended.
[00:55:32.280 --> 00:55:33.920]   I'm not sure why people are offended.
[00:55:33.920 --> 00:55:35.760]   Leo, people are offended by everything.
[00:55:35.760 --> 00:55:41.400]   So, yeah, you know, that's true.
[00:55:41.400 --> 00:55:43.280]   It's not, it's not dignified.
[00:55:43.280 --> 00:55:44.280]   It's not dignified.
[00:55:44.280 --> 00:55:46.880]   Maybe that's, that's all that's wrong with that.
[00:55:46.880 --> 00:55:50.320]   Look, is it, is it something I would do?
[00:55:50.320 --> 00:55:52.320]   No, whatever.
[00:55:52.320 --> 00:55:53.320]   Whatever.
[00:55:53.320 --> 00:55:54.320]   He's a robot.
[00:55:54.320 --> 00:55:55.320]   It's okay.
[00:55:55.320 --> 00:55:56.320]   It's okay.
[00:55:56.320 --> 00:55:57.320]   On that one.
[00:55:57.320 --> 00:55:58.320]   You know what's fun?
[00:55:58.320 --> 00:56:03.180]   We are now thanks to the ongoing action between Twitter and Elon Musk and the Delaware
[00:56:03.180 --> 00:56:12.960]   Court of Chancery, privy to the fascinating texts going back and forth between Elon and
[00:56:12.960 --> 00:56:20.600]   other wealthy individuals during his attempt to purchase Twitter.
[00:56:20.600 --> 00:56:24.640]   And I like the, I think it was the Atlantic's take on it that just shows you these guys
[00:56:24.640 --> 00:56:29.480]   are as stupid as you might even imagine.
[00:56:29.480 --> 00:56:35.920]   They, Elon Musk's texts shatter the myth of the text, tech genius.
[00:56:35.920 --> 00:56:42.240]   This is Charlie Wartzel writing, "The world's richest man has some embarrassing friends."
[00:56:42.240 --> 00:56:53.240]   A number of whom have been on this show, including Jason Calicanis, who volunteered to run Twitter
[00:56:53.240 --> 00:56:54.240]   for Elon.
[00:56:54.240 --> 00:57:00.400]   And then at one point said, I, you know, let me ask my, you know, I'll ask around.
[00:57:00.400 --> 00:57:01.400]   I remember he did this.
[00:57:01.400 --> 00:57:05.120]   He asked around and said, "Anybody want to invest, you know, put some money into Musk's
[00:57:05.120 --> 00:57:06.400]   acquisition?
[00:57:06.400 --> 00:57:10.440]   I can, I can help."
[00:57:10.440 --> 00:57:15.520]   Wartzel writes, "Fuan Musk's phone appeared as excitable as the angel investor Jason Calicanis,
[00:57:15.520 --> 00:57:20.120]   who peppered his friend with flattery and random ideas for the service.
[00:57:20.120 --> 00:57:25.600]   In the span of 30 minutes, Calicanis suggested a five point plan for Twitter that would introduce
[00:57:25.600 --> 00:57:29.560]   a membership to your creator revenue splits, algorithmic transparency and changes to the
[00:57:29.560 --> 00:57:31.000]   company's operation.
[00:57:31.000 --> 00:57:34.560]   After pledging his loyalty, you have my sword."
[00:57:34.560 --> 00:57:36.160]   He texted Musk.
[00:57:36.160 --> 00:57:40.200]   Calicanis pushed new ideas for weeks.
[00:57:40.200 --> 00:57:48.520]   Imagine we asked Justin Beaver, not Beaver, Beaver to come back and let him DM his fans.
[00:57:48.520 --> 00:57:52.720]   He could sell a million emergency tickets, would be insane.
[00:57:52.720 --> 00:57:59.640]   Finally, Musk says, sends a message back.
[00:57:59.640 --> 00:58:03.760]   "Morgan Stanley and Jared think you are using our friendship not in a good way.
[00:58:03.760 --> 00:58:06.400]   This makes it seem like I'm desperate.
[00:58:06.400 --> 00:58:07.400]   Please stop."
[00:58:07.400 --> 00:58:11.680]   Poor old Jason.
[00:58:11.680 --> 00:58:14.000]   Only ever want to support you, Elon.
[00:58:14.000 --> 00:58:15.000]   Love you, man."
[00:58:15.000 --> 00:58:18.960]   He said he'd jump on a grand day for him.
[00:58:18.960 --> 00:58:19.960]   Yeah.
[00:58:19.960 --> 00:58:21.800]   I wrote about this today.
[00:58:21.800 --> 00:58:22.800]   I just put the link in the IRC.
[00:58:22.800 --> 00:58:25.440]   I wrote about this today for my column on Medium.
[00:58:25.440 --> 00:58:35.280]   I think that the way to understand how this works is that it's to be an innovator is
[00:58:35.280 --> 00:58:37.280]   not to have a unique genius.
[00:58:37.280 --> 00:58:40.400]   It's to have good timing.
[00:58:40.400 --> 00:58:45.040]   If you scroll down a little there, Leo, you'll see these two diagrams I have about what it
[00:58:45.040 --> 00:58:46.960]   takes to invent the helicopter.
[00:58:46.960 --> 00:58:51.040]   For 500 years, people invented helicopters.
[00:58:51.040 --> 00:58:53.680]   They were like, "Oh, I've seen a screw press and I've seen a maple key.
[00:58:53.680 --> 00:58:55.560]   I've invented the helicopter."
[00:58:55.560 --> 00:59:02.960]   But it wasn't until someone else had invented a criminal, internal combustion that you could
[00:59:02.960 --> 00:59:04.440]   get a helicopter.
[00:59:04.440 --> 00:59:07.720]   Kevin Kelly calls us the adjacent possible.
[00:59:07.720 --> 00:59:11.600]   This is why when it's row-roading time, you get railroads and why six people invented
[00:59:11.600 --> 00:59:14.760]   the radio within a year, each other, and so on.
[00:59:14.760 --> 00:59:16.560]   An idea whose time has come.
[00:59:16.560 --> 00:59:17.560]   That's what that means.
[00:59:17.560 --> 00:59:18.560]   Yeah.
[00:59:18.560 --> 00:59:19.560]   Yeah.
[00:59:19.560 --> 00:59:21.320]   These guys, they had a good idea.
[00:59:21.320 --> 00:59:24.760]   They were not unique in having that good idea.
[00:59:24.760 --> 00:59:28.760]   What they did have was access to the capital markets after edging out other people, after
[00:59:28.760 --> 00:59:34.800]   getting a little bit of advantage, that they could use to buy other people's good ideas,
[00:59:34.800 --> 00:59:39.880]   to suppress good ideas before they could take hold independently by buying out rivals or
[00:59:39.880 --> 00:59:43.600]   by using predatory pricing, all of that other stuff.
[00:59:43.600 --> 00:59:48.000]   What you end up with is people who are just sort of mediocre donkeys, no better than you
[00:59:48.000 --> 00:59:49.000]   or me.
[00:59:49.000 --> 00:59:54.360]   I'm not claiming to be better than any of these people, but they're not better than me either.
[00:59:54.360 --> 00:59:58.280]   No one should put me in charge of the lives of 100 million Twitter users or 3 billion
[00:59:58.280 --> 01:00:01.640]   Facebook users.
[01:00:01.640 --> 01:00:06.460]   I think we often focus too much on whether these people have the right stuff for that
[01:00:06.460 --> 01:00:10.320]   job and not enough on whether that job should exist.
[01:00:10.320 --> 01:00:11.320]   I've talked about this before.
[01:00:11.320 --> 01:00:15.320]   There's this great man hypothesis.
[01:00:15.320 --> 01:00:22.920]   Because Elon is a billionaire or Mark Cuban's a billionaire, they become the Alexander the
[01:00:22.920 --> 01:00:23.920]   Greats.
[01:00:23.920 --> 01:00:25.720]   They become the great man.
[01:00:25.720 --> 01:00:27.920]   In fact, maybe they didn't earn that.
[01:00:27.920 --> 01:00:28.920]   So, brookay.
[01:00:28.920 --> 01:00:31.360]   Maybe they just the right person at the right time.
[01:00:31.360 --> 01:00:32.680]   That's what you're saying.
[01:00:32.680 --> 01:00:34.520]   It's the providential doctrine, right?
[01:00:34.520 --> 01:00:36.400]   If you are rich, you must be great.
[01:00:36.400 --> 01:00:39.080]   If you are not rich, you aren't great.
[01:00:39.080 --> 01:00:41.520]   And the way to tell whether someone is great is whether they're rich.
[01:00:41.520 --> 01:00:45.320]   I think we've come to a time when we've stopped worshiping billionaires or we're starting
[01:00:45.320 --> 01:00:46.720]   to stop worshiping billionaires.
[01:00:46.720 --> 01:00:47.720]   I hope so.
[01:00:47.720 --> 01:00:48.720]   You say that.
[01:00:48.720 --> 01:00:51.840]   I wrote something unflattering about Palantir yesterday.
[01:00:51.840 --> 01:00:56.240]   And it turns out that Palantir is the latest meme stock and there's a whole bunch of weird
[01:00:56.240 --> 01:00:57.240]   Peter Teal.
[01:00:57.240 --> 01:00:58.800]   He must be a short seller, man.
[01:00:58.800 --> 01:00:59.800]   They're in my mentions.
[01:00:59.800 --> 01:01:00.800]   Yeah.
[01:01:00.800 --> 01:01:02.800]   Who is paying you to write this?
[01:01:02.800 --> 01:01:04.160]   And so on and so on.
[01:01:04.160 --> 01:01:06.880]   Palantir should absolutely run the NHS.
[01:01:06.880 --> 01:01:10.160]   I've actually I did a thread and I post a link in there.
[01:01:10.160 --> 01:01:13.400]   Palantir should run the National Health Service of Britain.
[01:01:13.400 --> 01:01:14.400]   Yeah.
[01:01:14.400 --> 01:01:17.320]   So that's the thing is Palantir is so Palantir won a.
[01:01:17.320 --> 01:01:18.320]   They didn't win.
[01:01:18.320 --> 01:01:23.000]   They're got a no bid 26 million pound contract to do work for the NHS and they're trying to
[01:01:23.000 --> 01:01:29.440]   leverage it to this big 360 odd million, but 23 and 20 million pound contract for the
[01:01:29.440 --> 01:01:31.080]   NHS.
[01:01:31.080 --> 01:01:36.240]   And they're pretty clear that no one is going to no one is going to green like that because
[01:01:36.240 --> 01:01:37.680]   they're Palantir.
[01:01:37.680 --> 01:01:42.440]   And so their new strategy is they're buying all the companies that have NHS contracts.
[01:01:42.440 --> 01:01:48.280]   So this is this is kind of maybe a little like Amazon's planned by one medical.
[01:01:48.280 --> 01:01:50.520]   And that's really interesting.
[01:01:50.520 --> 01:01:56.240]   So for people who don't know, I'm sure most people know Palantir is basically a surveillance
[01:01:56.240 --> 01:02:03.120]   system, much like the Palantir in the Lord of the Rings, an eye of Sauron that collects
[01:02:03.120 --> 01:02:08.480]   data and then sells it to who governments law enforcement.
[01:02:08.480 --> 01:02:09.640]   Are they a data broker?
[01:02:09.640 --> 01:02:10.640]   Is that I mean, are they?
[01:02:10.640 --> 01:02:12.040]   I mean, are they that to Denmark?
[01:02:12.040 --> 01:02:13.960]   They're an analytics platform.
[01:02:13.960 --> 01:02:20.240]   I mean, they do a lot of like turnkey, you know, human rights abuses as a service.
[01:02:20.240 --> 01:02:25.760]   So like if you want to figure out how to do how to do algorithmic racism with refugees,
[01:02:25.760 --> 01:02:30.560]   yeah, you know, you can you can buy their service and feed it into them and their, you
[01:02:30.560 --> 01:02:35.000]   know, phrenology robot will tell you that all the bad refugees are brown.
[01:02:35.000 --> 01:02:40.640]   It strikes me that letting some company like that own or not own obviously, nobody can
[01:02:40.640 --> 01:02:46.720]   own the NHS, but participate in any way with a service that has the health records of millions
[01:02:46.720 --> 01:02:47.720]   of Britons.
[01:02:47.720 --> 01:02:49.720]   It seems like a bad idea.
[01:02:49.720 --> 01:02:50.720]   Terrible.
[01:02:50.720 --> 01:02:53.000]   Not least because there's actually a really good.
[01:02:53.000 --> 01:02:54.560]   Sorry, go ahead.
[01:02:54.560 --> 01:02:56.200]   No, you go ahead.
[01:02:56.200 --> 01:03:00.360]   I was just going to say there's a there's an amazing proposal in the offing Ben Goldacre
[01:03:00.360 --> 01:03:05.160]   who's an evidence based medicine specialist and has done a bunch of important interventions
[01:03:05.160 --> 01:03:09.440]   at the kind of national level in the British healthcare system, like the register of all
[01:03:09.440 --> 01:03:14.760]   trials and stuff that have absolutely revolutionized evidence based medicine in the UK.
[01:03:14.760 --> 01:03:19.520]   Did this thing called the Goldacre report and it's how you would build a research platform
[01:03:19.520 --> 01:03:25.560]   that allow you to extract insights from the collected health records of NHS service users
[01:03:25.560 --> 01:03:26.880]   without violating their privacy.
[01:03:26.880 --> 01:03:31.960]   And he's like, first you build an open platform that anyone can audit anyone can use and anyone
[01:03:31.960 --> 01:03:33.200]   can implement.
[01:03:33.200 --> 01:03:38.520]   And then you host that platform and close it off from everyone else with the NHS data
[01:03:38.520 --> 01:03:43.600]   in it and you tell researchers, send me a query and I will run it on the platform and
[01:03:43.600 --> 01:03:45.080]   send you back the data.
[01:03:45.080 --> 01:03:46.680]   But you can't ever see that data.
[01:03:46.680 --> 01:03:48.640]   It's owned by the public managed by the public.
[01:03:48.640 --> 01:03:51.360]   No, no, no, no, no, no, this is a really good idea.
[01:03:51.360 --> 01:03:52.840]   This is this is up.
[01:03:52.840 --> 01:03:58.200]   This is a good, like an auditable evidence private public service.
[01:03:58.200 --> 01:04:02.000]   It just reminds me of IMDB where you get everybody to create this great database.
[01:04:02.000 --> 01:04:03.480]   But it's owned by the public.
[01:04:03.480 --> 01:04:04.480]   Okay.
[01:04:04.480 --> 01:04:05.480]   NHS.
[01:04:05.480 --> 01:04:06.480]   All right.
[01:04:06.480 --> 01:04:07.480]   So the idea doesn't own it.
[01:04:07.480 --> 01:04:10.520]   And they wouldn't they wouldn't get like if Arthur Anderson built it for them or someone
[01:04:10.520 --> 01:04:13.240]   at uni-press waterhouse Cooper built it for them, they would have to make it open.
[01:04:13.240 --> 01:04:14.840]   They'd have to put the code on GitHub.
[01:04:14.840 --> 01:04:15.840]   Right.
[01:04:15.840 --> 01:04:16.840]   Anyone can see this code.
[01:04:16.840 --> 01:04:17.840]   Anyone can audit this code.
[01:04:17.840 --> 01:04:22.280]   But the service itself, the only person people with a login for it would be the people
[01:04:22.280 --> 01:04:24.240]   who ran research for the NHS.
[01:04:24.240 --> 01:04:26.280]   This is the only way you could keep it anonymous.
[01:04:26.280 --> 01:04:27.280]   Yeah.
[01:04:27.280 --> 01:04:31.680]   Well, and and and and but also productive, right?
[01:04:31.680 --> 01:04:35.680]   So if you're a private researcher at a university or firm a company anywhere else and you're
[01:04:35.680 --> 01:04:39.840]   like, I want to know what happens when these two interventions are paired.
[01:04:39.840 --> 01:04:40.840]   What does the data tell us?
[01:04:40.840 --> 01:04:44.520]   You got to give someone this drug when they're doing this this PT physiotherapy.
[01:04:44.520 --> 01:04:45.520]   It's huge value.
[01:04:45.520 --> 01:04:46.520]   Something else.
[01:04:46.520 --> 01:04:47.520]   Huge value.
[01:04:47.520 --> 01:04:50.760]   So you can then get back useful data, right?
[01:04:50.760 --> 01:04:53.600]   Useful conclusions, but you never handle the data.
[01:04:53.600 --> 01:04:55.080]   So the Goldacre reports really good.
[01:04:55.080 --> 01:05:01.160]   And it's such a it's the opposite of the Palantir approach, which is like we will you give
[01:05:01.160 --> 01:05:05.640]   us your data will apply our magic proprietary stuff that no one has allowed to know.
[01:05:05.640 --> 01:05:08.280]   About and then we'll tell you what's in your data.
[01:05:08.280 --> 01:05:14.720]   We'll tell you what, you know, we'll tell the LAPD where to go and do stop and frisk,
[01:05:14.720 --> 01:05:19.080]   which is, you know, like by an incredible coincidence that none of us could have predicted neighborhoods
[01:05:19.080 --> 01:05:21.840]   where a lot of brown people live.
[01:05:21.840 --> 01:05:22.840]   Yeah.
[01:05:22.840 --> 01:05:23.840]   Is this what Amazon's doing?
[01:05:23.840 --> 01:05:28.280]   Alex, is this why Amazon's acquiring, you know, one medical and others?
[01:05:28.280 --> 01:05:30.800]   I mean, I just want to make a comment on Palantir.
[01:05:30.800 --> 01:05:35.600]   I think that we need to understand that this is a consulting company, not a data company.
[01:05:35.600 --> 01:05:41.280]   And, you know, they're more in line with Deloitte than they are in line with any like cloud
[01:05:41.280 --> 01:05:42.280]   services provider.
[01:05:42.280 --> 01:05:44.360]   It's the analytics that they sell.
[01:05:44.360 --> 01:05:48.640]   Well, they sell the consulting, you know, they talk about the analytics.
[01:05:48.640 --> 01:05:50.760]   They're sort of a dressed up consulting company.
[01:05:50.760 --> 01:05:55.920]   My heart take is that we're going to end up seeing that, you know, they might be a meme
[01:05:55.920 --> 01:06:01.840]   stock today, but we will see the company end up in overtime, you know, being worth, you
[01:06:01.840 --> 01:06:03.680]   know, what they should be worth, which is a lot less.
[01:06:03.680 --> 01:06:07.520]   And I think their worth is largely inflated through due to the government connections
[01:06:07.520 --> 01:06:09.080]   that they have actually in the Elon text.
[01:06:09.080 --> 01:06:14.040]   There was something really interesting about Joe Lonsdale, making who was the Palantir co-founder,
[01:06:14.040 --> 01:06:17.680]   you know, hanging out with, you know, 100 Republican congressmen.
[01:06:17.680 --> 01:06:19.680]   We all know about Peter Teal.
[01:06:19.680 --> 01:06:24.080]   So, you know, those political connections help a lot, but overtime, I think we're going
[01:06:24.080 --> 01:06:26.840]   to see them for what they are, which is that they should, we should address them up in
[01:06:26.840 --> 01:06:31.440]   this, like, you know, putting their special sauce on the analytics, like they are just
[01:06:31.440 --> 01:06:32.880]   a thing in the end of the day.
[01:06:32.880 --> 01:06:33.880]   It's actually a good question.
[01:06:33.880 --> 01:06:34.960]   Oh, yeah, I'm not saying it's good.
[01:06:34.960 --> 01:06:38.520]   I'm not saying that it's useful insights, but I'm saying that, like, I think that there
[01:06:38.520 --> 01:06:42.400]   are people who will sell you confirmation biases of service, right?
[01:06:42.400 --> 01:06:43.400]   That's right.
[01:06:43.400 --> 01:06:44.400]   I have a thing I wanted.
[01:06:44.400 --> 01:06:48.400]   I have, I need some policy-based evidence, please, right?
[01:06:48.400 --> 01:06:50.160]   So I know what I want to do.
[01:06:50.160 --> 01:06:54.400]   Please produce the insights that will let me prove to my board or to other people or
[01:06:54.400 --> 01:06:56.440]   the market or whatever that what I want to do is good.
[01:06:56.440 --> 01:06:57.440]   Yeah.
[01:06:57.440 --> 01:06:58.440]   Yeah.
[01:06:58.440 --> 01:06:59.440]   Yeah.
[01:06:59.440 --> 01:07:00.440]   Well, that's interesting.
[01:07:00.440 --> 01:07:05.400]   I was giving Palantir all of this superpower to see into our lives and it's maybe the it's
[01:07:05.400 --> 01:07:06.400]   like Cambridge Analytica.
[01:07:06.400 --> 01:07:07.400]   Yeah.
[01:07:07.400 --> 01:07:09.640]   There is exactly the analytica I was making.
[01:07:09.640 --> 01:07:10.640]   Yeah.
[01:07:10.640 --> 01:07:11.640]   Yeah.
[01:07:11.640 --> 01:07:12.640]   Yeah.
[01:07:12.640 --> 01:07:13.640]   And I also think, yeah.
[01:07:13.640 --> 01:07:14.640]   Anyway, I'll leave it there.
[01:07:14.640 --> 01:07:15.640]   Why are they buying up?
[01:07:15.640 --> 01:07:16.640]   I think I'll take it.
[01:07:16.640 --> 01:07:22.200]   Why are they buying up these suppliers to the NHS then?
[01:07:22.200 --> 01:07:23.200]   What's their goal here?
[01:07:23.200 --> 01:07:24.200]   Palantir?
[01:07:24.200 --> 01:07:26.040]   Yeah, they're a leader.
[01:07:26.040 --> 01:07:27.040]   Right.
[01:07:27.040 --> 01:07:30.240]   They just want to they want to suck up a bunch of public private partnership money.
[01:07:30.240 --> 01:07:31.240]   Oh, okay.
[01:07:31.240 --> 01:07:35.560]   Because that's why Willie Sutton robbed banks, right?
[01:07:35.560 --> 01:07:36.560]   Because that's where the private public.
[01:07:36.560 --> 01:07:37.560]   Yeah.
[01:07:37.560 --> 01:07:38.560]   That's where the money money is.
[01:07:38.560 --> 01:07:42.840]   If you don't have the software that's going to take you there, you need the partnerships
[01:07:42.840 --> 01:07:44.640]   because you're a consulting company.
[01:07:44.640 --> 01:07:45.640]   That's right.
[01:07:45.640 --> 01:07:46.640]   Boy, thank you for that.
[01:07:46.640 --> 01:07:51.160]   You know, I've I've missed and really kind of given them a lot more credit than they deserve
[01:07:51.160 --> 01:07:52.160]   all this time.
[01:07:52.160 --> 01:07:56.840]   I just want to shut up the people in the chat saying I'm here to defend big tech.
[01:07:56.840 --> 01:07:59.040]   Look, let's look at it reasonably.
[01:07:59.040 --> 01:08:00.040]   Right.
[01:08:00.040 --> 01:08:03.040]   And the more reasonable our conversation, the more progress we're going to make.
[01:08:03.040 --> 01:08:08.760]   I think what I really admire actually about Corey's book is that it isn't it doesn't
[01:08:08.760 --> 01:08:11.120]   paint a target on big techs back.
[01:08:11.120 --> 01:08:17.160]   It paints a target on companies that have gotten too big, basically with predatory tech
[01:08:17.160 --> 01:08:18.560]   not predatory methods.
[01:08:18.560 --> 01:08:19.560]   Yes.
[01:08:19.560 --> 01:08:20.560]   Is that fair?
[01:08:20.560 --> 01:08:27.400]   I would say that the what it does is it identifies the source of the pain that artists are feeling.
[01:08:27.400 --> 01:08:28.400]   Right.
[01:08:28.400 --> 01:08:34.520]   So I spent 40 years giving artists longer duration of copyright, easier and forced copyright,
[01:08:34.520 --> 01:08:38.720]   stiffer penalties for copyright and increase scope of copyright.
[01:08:38.720 --> 01:08:40.960]   The entertainment industry has grown.
[01:08:40.960 --> 01:08:41.960]   It's gotten bigger.
[01:08:41.960 --> 01:08:46.360]   The share of income accruing to the workers who produce the materials that produce those
[01:08:46.360 --> 01:08:50.280]   profits has shrunk over the same time.
[01:08:50.280 --> 01:08:56.000]   And the reason is that these firms have between them created checkpoints where if you want to
[01:08:56.000 --> 01:09:00.320]   reach your audience, you have to negotiate with them.
[01:09:00.320 --> 01:09:04.000]   And typically, if there's another firm you can negotiate, it's another firm that has
[01:09:04.000 --> 01:09:07.120]   nearly identical or actually identical terms.
[01:09:07.120 --> 01:09:11.040]   And they've all converged on a set of negotiating terms that say whatever copyright you've
[01:09:11.040 --> 01:09:13.480]   been given, you have to hand over to us.
[01:09:13.480 --> 01:09:18.000]   And so giving creators more copyright won't help in the same way that giving your bullied
[01:09:18.000 --> 01:09:22.480]   kid more lunch money won't help, not even if the bullies are like running a national campaign
[01:09:22.480 --> 01:09:26.520]   saying won't someone think of America's hungry children give them more lunch money.
[01:09:26.520 --> 01:09:27.520]   Right.
[01:09:27.520 --> 01:09:29.600]   They're just going to take whatever lunch money you give them.
[01:09:29.600 --> 01:09:35.680]   And that's the first half of the book is just showing how these monocynistic, which
[01:09:35.680 --> 01:09:40.400]   when there's a small number of buyers who can control their sellers, this monocynistic
[01:09:40.400 --> 01:09:47.040]   dynamic works and how it enables tactics that range from simply unethical to just illegal,
[01:09:47.040 --> 01:09:49.520]   but no one can take them on even though they are illegal.
[01:09:49.520 --> 01:09:54.400]   We document multiple hundreds of millions of dollars in wage theft from audible creators
[01:09:54.400 --> 01:09:56.600]   by Amazon.
[01:09:56.600 --> 01:10:01.600]   But the second half of the book are interventions that aren't just more copyright that actually
[01:10:01.600 --> 01:10:03.400]   do widen out these choke points.
[01:10:03.400 --> 01:10:08.000]   And we talked about one on triangulation that I really like, which is that if you audit
[01:10:08.000 --> 01:10:12.920]   your royalty statements, which generally you're contractually allowed to do, if you audit
[01:10:12.920 --> 01:10:19.560]   your royalty statements, in order to get the money that you find is owed to you, the firm
[01:10:19.560 --> 01:10:24.680]   will generally say either you have to sue us, or if you want a voluntary settlement,
[01:10:24.680 --> 01:10:26.960]   you have to submit to non disclosure.
[01:10:26.960 --> 01:10:32.280]   And so we cite research from a firm that specializes in auditing recording industry contracts.
[01:10:32.280 --> 01:10:37.880]   They've done tens of thousands in all but one instance over decades.
[01:10:37.880 --> 01:10:41.760]   Every accounting area they located was in favor of the label, not the artist.
[01:10:41.760 --> 01:10:43.080]   This is an amazing coincidence.
[01:10:43.080 --> 01:10:47.680]   As always say, it's the most incredible localized probability storm you can imagine.
[01:10:47.680 --> 01:10:51.880]   There's no other possible explanation for wildest accounting errors, but would just favor the
[01:10:51.880 --> 01:10:53.840]   company making the royalty statement.
[01:10:53.840 --> 01:10:57.440]   But if you go and you find missing money and we have a source that found a six figure
[01:10:57.440 --> 01:11:02.560]   error in their favor when they audited their royalty statement, you have to agree not to
[01:11:02.560 --> 01:11:06.640]   tell anyone else who's being ripped off in the same way where they should go and look
[01:11:06.640 --> 01:11:09.000]   for the money that's being stolen from them.
[01:11:09.000 --> 01:11:12.920]   And there's an actual fix for this which is relatively straightforward.
[01:11:12.920 --> 01:11:17.560]   Because the industry is so concentrated, all of its contracts are consummated in New York,
[01:11:17.560 --> 01:11:23.240]   California and because of Amazon, Washington state, contract being a matter of state regulation,
[01:11:23.240 --> 01:11:27.560]   you could introduce laws at the state level that say, as a matter of public policy, non
[01:11:27.560 --> 01:11:32.440]   disclosure is not enforceable when it relates to material emissions or errors in royalty
[01:11:32.440 --> 01:11:36.080]   statements that negatively affect people who are owed royalties.
[01:11:36.080 --> 01:11:40.280]   And then at the stroke of a pen, every artist in the world, because all of their contracts
[01:11:40.280 --> 01:11:46.360]   are governed by Washington, New York or California law, every creator in the world would suddenly
[01:11:46.360 --> 01:11:50.440]   have money following on the more money than all of the copyright extensions the last 40
[01:11:50.440 --> 01:11:52.000]   years have ever provided.
[01:11:52.000 --> 01:11:53.640]   And we just fill the back half of the book with this.
[01:11:53.640 --> 01:11:59.800]   We've got like 12 or 14 of these interventions that actually rather than just giving artists
[01:11:59.800 --> 01:12:03.840]   the rep to feel aggrieved that their copyright is being violated, will let them like buy
[01:12:03.840 --> 01:12:07.440]   groceries and put braces on their kids teeth.
[01:12:07.440 --> 01:12:13.560]   Yeah, it's nice because you focus on artists, obviously it affects everybody from Amazon
[01:12:13.560 --> 01:12:17.960]   warehouse workers to burger flippers.
[01:12:17.960 --> 01:12:24.920]   But you know, we artists and I put myself in this category, we creators want to create
[01:12:24.920 --> 01:12:28.040]   and they take advantage of that fact, right?
[01:12:28.040 --> 01:12:30.360]   Let's say, fine, go ahead.
[01:12:30.360 --> 01:12:31.360]   You keep creating.
[01:12:31.360 --> 01:12:32.840]   We'll take care of the rest.
[01:12:32.840 --> 01:12:33.840]   Thanks.
[01:12:33.840 --> 01:12:34.840]   Let's take a little break.
[01:12:34.840 --> 01:12:35.840]   Want to talk more?
[01:12:35.840 --> 01:12:44.400]   We've got this new journalism competition and preservation act, which thanks to the Ted
[01:12:44.400 --> 01:12:46.800]   Cruz is moving forward.
[01:12:46.800 --> 01:12:49.600]   Lots, lots more to talk about, including Elon Musk's robot.
[01:12:49.600 --> 01:12:51.760]   We didn't even get to that yet.
[01:12:51.760 --> 01:12:55.880]   But first a word from our sponsor, Corey Doctor is here, Alex Cantrowitz from the Big
[01:12:55.880 --> 01:12:58.640]   Tech, Big Technology podcast.
[01:12:58.640 --> 01:13:00.920]   Our show today brought to you by Ate Sleep.
[01:13:00.920 --> 01:13:06.960]   If I am well rested tonight, you can thank my Ate Sleep Pod Pro cover too.
[01:13:06.960 --> 01:13:10.880]   Now they've got an even better Ate Sleep Pod cover.
[01:13:10.880 --> 01:13:14.840]   If you've got an Ate Sleep mattress or a pod cover, you already know.
[01:13:14.840 --> 01:13:15.840]   That's how I found out.
[01:13:15.840 --> 01:13:18.080]   Kevin Rose said, "Oh, you got to do this.
[01:13:18.080 --> 01:13:19.080]   Amy Webb got it."
[01:13:19.080 --> 01:13:20.080]   Then she said, "Oh, you got to do this."
[01:13:20.080 --> 01:13:22.200]   So finally, about a year ago, we got it.
[01:13:22.200 --> 01:13:27.720]   Ate Sleep is the only sleep technology that dynamically cools and heats each side of your
[01:13:27.720 --> 01:13:29.680]   bed.
[01:13:29.680 --> 01:13:33.720]   The reason it's sides is so your spouse can have her temperature as well to maintain
[01:13:33.720 --> 01:13:36.960]   the optimal sleeping temperature for what your body needs.
[01:13:36.960 --> 01:13:42.040]   Thanks to Ate Sleep, I'm getting more than half an hour more deep sleep every night up
[01:13:42.040 --> 01:13:44.960]   over up to an hour now.
[01:13:44.960 --> 01:13:48.160]   It combines dynamic cooling and heating with biometric tracking.
[01:13:48.160 --> 01:13:52.120]   So it's watching you as you sleep.
[01:13:52.120 --> 01:13:58.200]   So it turns out your body kind of wants it to get cool as you go into deeper sleep.
[01:13:58.200 --> 01:14:02.240]   I have my Ate Sleep set to be warm when I get in bed because it's nice and cozy.
[01:14:02.240 --> 01:14:03.240]   And then cool off.
[01:14:03.240 --> 01:14:08.040]   I go into deeper and deeper sleep from REM to deep sleep and then back up.
[01:14:08.040 --> 01:14:10.280]   And then it warms up again in the morning.
[01:14:10.280 --> 01:14:16.280]   But it'll adjust automatically or you can adjust it manually as cool as 55 degrees Fahrenheit,
[01:14:16.280 --> 01:14:21.000]   which was awesome during the hot heat wave we had this summer.
[01:14:21.000 --> 01:14:26.040]   And in the winter, as hot as 110 degrees Fahrenheit, it monitors not only your temperature,
[01:14:26.040 --> 01:14:29.120]   movements, but also the room temperature.
[01:14:29.120 --> 01:14:35.200]   Clinical data shows eight sleep users experience up to 19% increase in recovery, up to 32% improvement
[01:14:35.200 --> 01:14:39.840]   in sleep quality, up to 34% more deep sleep.
[01:14:39.840 --> 01:14:41.760]   More deep sleep makes a huge difference.
[01:14:41.760 --> 01:14:44.440]   I can, you know, I just feel it the next day.
[01:14:44.440 --> 01:14:50.400]   I feel like I am, you know, alive and awake and everything feels better.
[01:14:50.400 --> 01:14:53.320]   My mental clarity is better.
[01:14:53.320 --> 01:14:58.320]   Eight sleep just launched effect.
[01:14:58.320 --> 01:15:01.320]   We got to get one.
[01:15:01.320 --> 01:15:06.320]   This new next generation pod, the pod three, which has more sensors, double the amount of
[01:15:06.320 --> 01:15:11.320]   sensors so they've got more accurate sleep and health tracking, giving you the absolute
[01:15:11.320 --> 01:15:13.320]   best sleep experience on earth.
[01:15:13.320 --> 01:15:16.320]   I'm still loving our pod two pro cover.
[01:15:16.320 --> 01:15:18.320]   I'm very happy with it.
[01:15:18.320 --> 01:15:22.320]   It's not magic, but it definitely feels like it.
[01:15:22.320 --> 01:15:27.320]   One of the primary causes of poor sleep feeling hot at night.
[01:15:27.320 --> 01:15:28.640]   Feeling hot at night.
[01:15:28.640 --> 01:15:30.520]   You don't want to be hot at night.
[01:15:30.520 --> 01:15:31.520]   Go to eight sleep.com/twit.
[01:15:31.520 --> 01:15:36.160]   I'm going to get you $150 off a checkout.
[01:15:36.160 --> 01:15:37.400]   Sleep cozy this fall.
[01:15:37.400 --> 01:15:42.120]   Eight sleep currently ships within the US, Canada, the UK and the EU and Australia, some
[01:15:42.120 --> 01:15:43.480]   countries in the EU.
[01:15:43.480 --> 01:15:46.120]   Eight sleep.com/twit.
[01:15:46.120 --> 01:15:51.520]   And if you go there, you'll save $150 a check out on your pod.
[01:15:51.520 --> 01:15:52.760]   Get the pod.
[01:15:52.760 --> 01:15:53.760]   Trust me.
[01:15:53.760 --> 01:15:54.760]   Trust me.
[01:15:54.760 --> 01:15:56.960]   You'll sleep better and everybody needs a better night's sleep.
[01:15:56.960 --> 01:15:57.960]   Thank you.
[01:15:57.960 --> 01:16:00.760]   Eight sleep for your support.
[01:16:00.760 --> 01:16:01.760]   Keep me up at night.
[01:16:01.760 --> 01:16:05.760]   The idea of having a humanoid robot walking around through my house.
[01:16:05.760 --> 01:16:12.720]   Fortunately, it probably isn't going to be Elon Musk's robot.
[01:16:12.720 --> 01:16:17.280]   Elon and Tesla have an AI event every year.
[01:16:17.280 --> 01:16:24.960]   Last year, you may remember Musk talked about his humanoid robot and brought out a dancer
[01:16:24.960 --> 01:16:29.080]   in a costume to show it off.
[01:16:29.080 --> 01:16:33.800]   Let me see if I can find a video of Optimus is what he's calling it.
[01:16:33.800 --> 01:16:37.880]   I think because he's a fan of the Transformers, I don't know.
[01:16:37.880 --> 01:16:40.840]   It's a really made progress over the leotard person.
[01:16:40.840 --> 01:16:41.840]   Yeah.
[01:16:41.840 --> 01:16:45.120]   Definitely better than the leotard person.
[01:16:45.120 --> 01:16:47.320]   Apparently could walk by itself.
[01:16:47.320 --> 01:16:48.480]   Here it comes.
[01:16:48.480 --> 01:16:52.720]   I'll turn off the sound because it's just too annoying.
[01:16:52.720 --> 01:16:57.080]   Elon wants to put these in his factories, but I think he also feels he says it could
[01:16:57.080 --> 01:17:01.040]   be transformative for civilization.
[01:17:01.040 --> 01:17:04.040]   He's still a little bit behind Boston Dynamics.
[01:17:04.040 --> 01:17:10.040]   Not exactly doing backflips or open indoors for people, but do you really?
[01:17:10.040 --> 01:17:12.840]   That's the dancer from last year.
[01:17:12.840 --> 01:17:14.680]   It's not doing that.
[01:17:14.680 --> 01:17:17.680]   Elon also would take the dancer.
[01:17:17.680 --> 01:17:24.680]   This is Elon's video of the robot very shakily delivering.
[01:17:24.680 --> 01:17:27.040]   Maybe he can water your plants.
[01:17:27.040 --> 01:17:29.560]   Oh, that's good.
[01:17:29.560 --> 01:17:32.040]   Now I thought what's her name?
[01:17:32.040 --> 01:17:36.760]   Chelsea Steiner at the Mary Sue had a good summation here.
[01:17:36.760 --> 01:17:40.760]   Like all Musk promises, this one is vague, unimpressive and riddled with issues.
[01:17:40.760 --> 01:17:44.480]   It's also wildly unrealistic to imagine that the robot will be capable enough to replace
[01:17:44.480 --> 01:17:46.200]   the labor force as we know it.
[01:17:46.200 --> 01:17:49.400]   All it when Optimus can do half the moves of this superstar.
[01:17:49.400 --> 01:17:53.880]   Here's the Boston Dynamics robot doing backflips, dancing.
[01:17:53.880 --> 01:17:58.880]   Even this though, they're carefully curated videos because half the time it falls over.
[01:17:58.880 --> 01:18:00.840]   Yeah, I don't.
[01:18:00.840 --> 01:18:02.560]   But who wants this anyway?
[01:18:02.560 --> 01:18:07.480]   Elon's factory already is loaded with robots, but they're the traditional giant German and
[01:18:07.480 --> 01:18:12.280]   Japanese robots that can pick up a car, turn it around and put it back on the line and
[01:18:12.280 --> 01:18:13.280]   so forth.
[01:18:13.280 --> 01:18:16.800]   But can they automate the racism of his facts?
[01:18:16.800 --> 01:18:19.240]   No, no, that's only a human can really.
[01:18:19.240 --> 01:18:23.460]   Right now their wage bill for doing the racism is very high and they're going to need to
[01:18:23.460 --> 01:18:24.460]   automate that somehow.
[01:18:24.460 --> 01:18:27.320]   They're going to have to bring in hay.
[01:18:27.320 --> 01:18:28.880]   Facebook's bot.
[01:18:28.880 --> 01:18:29.880]   Let Tay do it.
[01:18:29.880 --> 01:18:30.880]   Yeah.
[01:18:30.880 --> 01:18:31.880]   Yeah, maybe.
[01:18:31.880 --> 01:18:32.880]   That was Microsoft's.
[01:18:32.880 --> 01:18:33.880]   Oh, Mike, but here's another.
[01:18:33.880 --> 01:18:37.560]   Here's another unpopular take since I guess that's my role on the show today.
[01:18:37.560 --> 01:18:42.960]   I think we should be hesitant before we discount this type of thing and it might not be the
[01:18:42.960 --> 01:18:47.800]   musk bot, okay, which by all accounts is not a very impressive machine.
[01:18:47.800 --> 01:18:54.320]   But we are seeing real interesting advances in robotics right now, humanoid style robotics.
[01:18:54.320 --> 01:18:58.760]   And like we've seen with AI, these things, you know, we see these breakthroughs of research.
[01:18:58.760 --> 01:19:02.920]   We get lots of duds and then all of a sudden we type in a sentence and the robot is drawing
[01:19:02.920 --> 01:19:08.400]   a picture for us or we talk to it and we have a Google engineer full engineer fooled that
[01:19:08.400 --> 01:19:09.400]   it's sentient.
[01:19:09.400 --> 01:19:13.640]   I think that the interesting report that no one's paying attention to everyone pays attention
[01:19:13.640 --> 01:19:15.000]   to what Musk does.
[01:19:15.000 --> 01:19:18.720]   But interesting report that no one's paying attention to along this line is that Amazon
[01:19:18.720 --> 01:19:25.640]   has been trying has been training robots to pick items out of boxes and then, you know,
[01:19:25.640 --> 01:19:27.360]   end up, you know, stowing them.
[01:19:27.360 --> 01:19:32.000]   So most of Amazon's workforce in the, in the warehouses are people that pick stuff out
[01:19:32.000 --> 01:19:35.680]   of boxes and then put them into shelves, pick them into from the shelves and put them into
[01:19:35.680 --> 01:19:39.320]   crates to be shipped off and they're making real progress there.
[01:19:39.320 --> 01:19:43.960]   So I think this idea of like, you know, Elon's robot is easy to laugh up, but the advances
[01:19:43.960 --> 01:19:48.640]   that we're seeing, you know, in this type of robotics are very, very real.
[01:19:48.640 --> 01:19:52.960]   And the fact that Musk is talking about disappearing in the warehouse, so that doesn't come out
[01:19:52.960 --> 01:19:53.960]   of nowhere.
[01:19:53.960 --> 01:19:57.000]   And there is definitely, you know, some progress here.
[01:19:57.000 --> 01:19:59.240]   And I'm not laughing.
[01:19:59.240 --> 01:20:03.800]   I think that this is some serious technology to be reckoned with, you know, maybe not coming
[01:20:03.800 --> 01:20:06.600]   from Elon Musk, but here today, for sure.
[01:20:06.600 --> 01:20:11.720]   So there's a term out of AI research, which is a centaur, which is when you have human
[01:20:11.720 --> 01:20:16.760]   AI collaboration, like a chess robot and a chess player playing together to do things
[01:20:16.760 --> 01:20:18.520]   that neither of them could do on their own.
[01:20:18.520 --> 01:20:22.560]   But there's also this, this sort of that of labor economics is this term, the reverse
[01:20:22.560 --> 01:20:29.160]   centaur, which is when the body is the inconvenient meat puppet for the AI.
[01:20:29.160 --> 01:20:31.040]   And Amazon are kind of the masters of that.
[01:20:31.040 --> 01:20:35.400]   I'm going to paste a link into the chat of a thing I wrote about reverse centaur is an
[01:20:35.400 --> 01:20:36.400]   Amazon.
[01:20:36.400 --> 01:20:43.640]   And things like their drivers being subjected to kind of like superhuman conditions or superhuman
[01:20:43.640 --> 01:20:51.400]   constraints by the, by the, uh, uh, uh, ais that are monitoring them in the cars or
[01:20:51.400 --> 01:20:58.880]   packers being driven to do, um, unrealistic working, uh, tempos in their warehouses.
[01:20:58.880 --> 01:21:03.640]   And, and Amazon leads the country in warehouse injuries.
[01:21:03.640 --> 01:21:09.600]   And the more automated on Amazon warehouse is the more injuries people incur.
[01:21:09.600 --> 01:21:16.720]   I'll, I'll stipulate that like automation is a thing and robots are cool and that, uh,
[01:21:16.720 --> 01:21:19.800]   Amazon has every motive in the world to try to make good robots.
[01:21:19.800 --> 01:21:24.520]   But I also want to sound the note of caution that a lot of what Amazon has booked to its
[01:21:24.520 --> 01:21:29.480]   shareholders as profits in automation have really been ways to get people to work in
[01:21:29.480 --> 01:21:32.840]   at an unsafe tempo in ways that put themselves at risk.
[01:21:32.840 --> 01:21:36.240]   And in the case of their drivers, put other people at risk, other users of the road at
[01:21:36.240 --> 01:21:37.240]   risk.
[01:21:37.240 --> 01:21:42.520]   I know a kid who works at Whole Foods owned by Amazon, uh, and it's a very different experience.
[01:21:42.520 --> 01:21:46.960]   Sometimes some hours he's working for Whole Foods, some hours he's working for Amazon.
[01:21:46.960 --> 01:21:54.040]   And the, the minute he's on the clock for Amazon, there are very clear, almost unachievable
[01:21:54.040 --> 01:21:56.160]   metrics for his performance.
[01:21:56.160 --> 01:22:00.560]   And they're made very clear and, uh, it is a very different experience.
[01:22:00.560 --> 01:22:02.400]   It's kind of you nailed it.
[01:22:02.400 --> 01:22:05.680]   It's a, you are, you are at the mercy of the machine.
[01:22:05.680 --> 01:22:06.680]   Yeah.
[01:22:06.680 --> 01:22:09.680]   And I need to state that I'm not celebrating this stuff, but when we look at the
[01:22:09.680 --> 01:22:10.520]   model, no, don't downplay it.
[01:22:10.520 --> 01:22:11.880]   Though I think you're right, Alex.
[01:22:11.880 --> 01:22:12.880]   Don't downplay it.
[01:22:12.880 --> 01:22:15.560]   And we've seen this very, very powerful technology.
[01:22:15.560 --> 01:22:16.560]   I agree.
[01:22:16.560 --> 01:22:17.880]   We've seen this with the explosion.
[01:22:17.880 --> 01:22:19.920]   Thanks to stable diffusion and Dali too.
[01:22:19.920 --> 01:22:24.200]   And mid journey, I mean, just explosion in AI art.
[01:22:24.200 --> 01:22:29.600]   Do you think though, you know, initially, my initial reaction was this is like a Cambrian
[01:22:29.600 --> 01:22:30.600]   explosion.
[01:22:30.600 --> 01:22:36.520]   So suddenly this is taken off and we've reached the hockey stick with AI in some interesting
[01:22:36.520 --> 01:22:37.760]   way.
[01:22:37.760 --> 01:22:45.240]   And now more and more, I'm thinking it's a parlor trick that the, the AI is just, it's
[01:22:45.240 --> 01:22:46.600]   not really AI almost.
[01:22:46.600 --> 01:22:50.840]   It's just combining images together in an interesting way.
[01:22:50.840 --> 01:22:52.240]   What's your take on it, Alex?
[01:22:52.240 --> 01:22:53.840]   I, I have you tried Dali before?
[01:22:53.840 --> 01:22:54.840]   Yeah.
[01:22:54.840 --> 01:22:56.960]   No, it's very impressive and stable diffusion and mid journey.
[01:22:56.960 --> 01:22:57.960]   Yeah, they're very stable.
[01:22:57.960 --> 01:23:01.720]   The future seems to be the fastest moving because it's open source.
[01:23:01.720 --> 01:23:03.040]   People can run it on their own servers.
[01:23:03.040 --> 01:23:05.200]   And there are a lot of people who've adopted it.
[01:23:05.200 --> 01:23:11.040]   I follow the stable diffusion Reddit subreddit and it's pretty amazing what, what they're
[01:23:11.040 --> 01:23:12.040]   doing.
[01:23:12.040 --> 01:23:17.800]   And yet I'm not, I'm not convinced it's, I don't know.
[01:23:17.800 --> 01:23:18.800]   Is it AI?
[01:23:18.800 --> 01:23:19.800]   Is it true AI?
[01:23:19.800 --> 01:23:20.800]   Of course.
[01:23:20.800 --> 01:23:21.800]   Yeah.
[01:23:21.800 --> 01:23:25.560]   And the images are going to be as creative as, you know, the prompts that you're going
[01:23:25.560 --> 01:23:26.560]   to need them.
[01:23:26.560 --> 01:23:29.960]   We've had anything like this to, you know, in our history before.
[01:23:29.960 --> 01:23:32.960]   And you know, there's, there's so many interesting applications.
[01:23:32.960 --> 01:23:33.960]   Here's one.
[01:23:33.960 --> 01:23:38.840]   I think that, so anyone who's worked with a marketing department, you know, knows that
[01:23:38.840 --> 01:23:42.840]   it's always a struggle to communicate to the creative department what you need them to,
[01:23:42.840 --> 01:23:47.480]   to create or what you'd like them to create because words are imprecise and are, you know,
[01:23:47.480 --> 01:23:49.120]   pictures worth a thousand words.
[01:23:49.120 --> 01:23:52.640]   So you can't just be like, I need a picture of this and they'll know exactly what you're
[01:23:52.640 --> 01:23:53.640]   doing.
[01:23:53.640 --> 01:23:58.280]   You know, graphic designer to product manager or marketing manager is one of the most difficult
[01:23:58.280 --> 01:24:00.760]   pieces of communication, I think, in the business world.
[01:24:00.760 --> 01:24:01.760]   And I've been there and it's tough.
[01:24:01.760 --> 01:24:02.760]   Yeah.
[01:24:02.760 --> 01:24:05.400]   And there are ways you get around to make this more prominent or we're trying to get this
[01:24:05.400 --> 01:24:06.400]   message across.
[01:24:06.400 --> 01:24:07.400]   We have a creative brief.
[01:24:07.400 --> 01:24:12.360]   Now I think what this stuff is the amateurs can make a version of the image, you know, with
[01:24:12.360 --> 01:24:16.680]   something like Dolly and then pass it to the professionals and then are, you know, be
[01:24:16.680 --> 01:24:18.480]   more vice versa.
[01:24:18.480 --> 01:24:22.280]   Both these systems really work well with a sketch.
[01:24:22.280 --> 01:24:28.200]   For years, an example of using old cartoons as in images says Fred Flintstone turned into
[01:24:28.200 --> 01:24:30.600]   this is, so start with it.
[01:24:30.600 --> 01:24:33.960]   If you give these a starting point, it is kind of an inspector.
[01:24:33.960 --> 01:24:35.640]   And like it, you know, I mean, I don't know.
[01:24:35.640 --> 01:24:39.760]   I continue to be blown away every time I do, you know, one of these searches.
[01:24:39.760 --> 01:24:44.280]   And I mean, Leo, it's sort of, it kind of shows you how much advances we've made.
[01:24:44.280 --> 01:24:47.400]   I mean, how serious of advanced, advanced we've made.
[01:24:47.400 --> 01:24:52.720]   If you're sitting there and typing in a sentence prompt, you know, on your browser.
[01:24:52.720 --> 01:24:55.920]   And the next thing you know, a machine will draw it for you.
[01:24:55.920 --> 01:24:56.920]   Yeah.
[01:24:56.920 --> 01:25:00.480]   And you're like, man, you know, like, think about how far we've come.
[01:25:00.480 --> 01:25:06.000]   You know, if that's your reaction, yeah, the stuff becomes unremarkable once it works,
[01:25:06.000 --> 01:25:07.000]   because we expect it.
[01:25:07.000 --> 01:25:09.600]   And I think that's the place we are today with these programs.
[01:25:09.600 --> 01:25:16.500]   Look at this progression from what is essentially a stick figure as this is why foo and stable
[01:25:16.500 --> 01:25:22.040]   diffusion combined together as it creates a more and more realistic with a little human
[01:25:22.040 --> 01:25:23.040]   help there.
[01:25:23.040 --> 01:25:27.360]   So it is a human partnership, I guess, that makes us the funniest thing I saw on Reddit
[01:25:27.360 --> 01:25:28.360]   last week.
[01:25:28.360 --> 01:25:33.980]   I just paste the URL for it into the chat is was a, um, a Drake meme whose title is what
[01:25:33.980 --> 01:25:39.420]   makes you a human and the, the pushing things away is to love and care about others.
[01:25:39.420 --> 01:25:40.760]   And the, yes, that's right.
[01:25:40.760 --> 01:25:42.920]   Is the selecting all images with bikes.
[01:25:42.920 --> 01:25:46.300]   Oh, isn't that interesting?
[01:25:46.300 --> 01:25:47.300]   Yeah.
[01:25:47.300 --> 01:25:48.500]   Talking about capture.
[01:25:48.500 --> 01:25:49.500]   Yeah.
[01:25:49.500 --> 01:25:50.500]   Yeah.
[01:25:50.500 --> 01:25:55.780]   So it's, it's, I mean, I, I agree that that this is a very powerful technology.
[01:25:55.780 --> 01:25:58.500]   It's, it's super interesting to work with.
[01:25:58.500 --> 01:26:03.100]   Um, you know, I've also been where you are trying to find art to use as reference to
[01:26:03.100 --> 01:26:04.900]   give to an illustrator.
[01:26:04.900 --> 01:26:06.620]   Being able to describe art is really great.
[01:26:06.620 --> 01:26:12.500]   I mean, Google image search was, was a huge, um, uh, phase change for that as well.
[01:26:12.500 --> 01:26:18.880]   Uh, and you know, I, I, uh, I was an imagineer for a while and in the imagineering archive,
[01:26:18.880 --> 01:26:21.860]   there's a room or I don't even know if it's still there, but there's a room with bankers
[01:26:21.860 --> 01:26:27.200]   boxes filled with magazine clippings of illustrations, organized by theater.
[01:26:27.200 --> 01:26:33.140]   And if you had to draw a water fountain, they just had a box of clipped out illustrations
[01:26:33.140 --> 01:26:37.920]   of water fountains that you would ring onto archives and they would send up a box of reference
[01:26:37.920 --> 01:26:38.920]   for you.
[01:26:38.920 --> 01:26:39.920]   Yeah.
[01:26:39.920 --> 01:26:45.200]   So this, this, this is definitely something that is making the lives of illustrators easier,
[01:26:45.200 --> 01:26:49.240]   making it easier for people who aren't illustrators to talk to people who are and say, that's
[01:26:49.240 --> 01:26:53.520]   what I mean when I say water fountain, this picture here is the, the kind of water fountain.
[01:26:53.520 --> 01:26:55.600]   I mean, not this picture over here.
[01:26:55.600 --> 01:27:00.320]   And, and certainly like Dolly and, and all the other ones, um, help there as well, but
[01:27:00.320 --> 01:27:08.480]   I will say that I don't see a path from statistical inference and, uh, you know, deep learning
[01:27:08.480 --> 01:27:10.960]   networks to GAI.
[01:27:10.960 --> 01:27:16.440]   I, I think that to say that if that's who it is, inference, we get GAIs like saying, if
[01:27:16.440 --> 01:27:19.880]   we read these horses carefully enough, we'll have a locomotive.
[01:27:19.880 --> 01:27:20.880]   Right.
[01:27:20.880 --> 01:27:21.880]   Right.
[01:27:21.880 --> 01:27:24.720]   General artificial intelligence, human style intelligence.
[01:27:24.720 --> 01:27:27.080]   But is there anybody actually arguing that?
[01:27:27.080 --> 01:27:28.080]   Corey?
[01:27:28.080 --> 01:27:29.080]   Oh, yeah, tons.
[01:27:29.080 --> 01:27:30.080]   Oh, yeah.
[01:27:30.080 --> 01:27:31.080]   That's the whole argument, right?
[01:27:31.080 --> 01:27:36.480]   The whole argument that, um, well, the whole like Nick Bostrom, Elon Musk, uh, sky net is
[01:27:36.480 --> 01:27:37.760]   coming out of our AI.
[01:27:37.760 --> 01:27:40.040]   I have looked at open AI and what it can do.
[01:27:40.040 --> 01:27:42.240]   And now I'm afraid for the human race.
[01:27:42.240 --> 01:27:46.680]   They're, they're basically saying we're going to selectively breed this horse long enough
[01:27:46.680 --> 01:27:50.520]   that eventually we're going to have a locomotive and then it's going to kill us all.
[01:27:50.520 --> 01:27:56.280]   And it is like such, um, obvious nonsense to me that I'm quite baffled by it.
[01:27:56.280 --> 01:27:59.040]   Uh, and you know, what is the discontinuity?
[01:27:59.040 --> 01:28:04.560]   So it's obviously horses and locomotives clear, but so, but this is about human cognition.
[01:28:04.560 --> 01:28:08.600]   Is there something about human cognition that is unreachable?
[01:28:08.600 --> 01:28:14.480]   No, no, it's just not, it's just not a human cognition is not statistical inference.
[01:28:14.480 --> 01:28:16.720]   You know, we, we don't know exactly entirely.
[01:28:16.720 --> 01:28:21.360]   It's not, it's not, I mean, statistical statistical inference might be a component of it.
[01:28:21.360 --> 01:28:22.360]   It probably is.
[01:28:22.360 --> 01:28:29.480]   Uh, but the idea that, that, um, increasing, uh, innovation in the realm of statistical
[01:28:29.480 --> 01:28:35.280]   inference eventually produces, uh, human cognition is just, it's, it's wrong.
[01:28:35.280 --> 01:28:41.400]   Uh, and you know, there's this, there's this corollary, which is, uh, uh, the, um, automation
[01:28:41.400 --> 01:28:43.480]   unemployment corollary, right?
[01:28:43.480 --> 01:28:47.800]   That the looming automation unemployment crisis, which again, I think is just like,
[01:28:47.800 --> 01:28:49.480]   doesn't, isn't right.
[01:28:49.480 --> 01:28:51.400]   Like it's, it's foundations are wrong.
[01:28:51.400 --> 01:28:56.640]   So like you look at the stories about automation and employment, you see things like, oh, the
[01:28:56.640 --> 01:29:01.080]   most popular job in America is truck driver and driving trucks is something that we can
[01:29:01.080 --> 01:29:06.000]   do with ML because we can give them a dedicated lane on the highway and set them to following
[01:29:06.000 --> 01:29:08.600]   each other and basically invent a shitty train.
[01:29:08.600 --> 01:29:12.120]   Uh, but, but, but, but, but, but, but, but.
[01:29:12.120 --> 01:29:17.360]   You know, the thing is that the Bureau of Labor Statistics category for truck driver incorporates
[01:29:17.360 --> 01:29:19.840]   anyone who operates a heavy goods vehicle.
[01:29:19.840 --> 01:29:25.120]   So it's not the most, like 16 wheeler driver is not the most popular job in America.
[01:29:25.120 --> 01:29:29.080]   It is a relatively small and unimportant part of our overall account, not that those people
[01:29:29.080 --> 01:29:32.360]   are unimportant, but like that, you know, if all of the truck drivers were unemployed
[01:29:32.360 --> 01:29:36.440]   tomorrow, the change in the unemployment figures would not be very large.
[01:29:36.440 --> 01:29:40.560]   Meanwhile, we're just not getting anywhere with the self-driving cars, right?
[01:29:40.560 --> 01:29:44.080]   Like the, the, there's so much smoke and mirrors in self-driving cars.
[01:29:44.080 --> 01:29:48.280]   The only ones that seem to perform at all are the ones that actually just have a human
[01:29:48.280 --> 01:29:53.680]   remotely driving the car, overseeing the car and at those people's attention wanders, which
[01:29:53.680 --> 01:29:57.720]   it will inevitably, uh, then those cars become murder boughts.
[01:29:57.720 --> 01:30:02.400]   Um, and you know, it raises an important point, which is that we already have a lot of human
[01:30:02.400 --> 01:30:06.040]   intelligence, like we have a billion humans.
[01:30:06.040 --> 01:30:08.920]   Um, we don't have enough non-human intelligence, right?
[01:30:08.920 --> 01:30:14.720]   Like the capacity to be vigilant for things that happens very rarely is, is not a capacity
[01:30:14.720 --> 01:30:16.680]   that humans mostly have.
[01:30:16.680 --> 01:30:21.520]   A few people may have it, but it's not a widespread trait in our population, which is why the
[01:30:21.520 --> 01:30:27.200]   TSA is really good at spotting, uh, water bottles and really bad at spotting guns because
[01:30:27.200 --> 01:30:28.480]   they never see a gun, right?
[01:30:28.480 --> 01:30:30.160]   But they see water bottles all day long.
[01:30:30.160 --> 01:30:35.240]   Like you cannot leave neurons trained to do a pattern recognition for a pattern that
[01:30:35.240 --> 01:30:40.680]   you never encounter because those neurons will be retrained to make you better at the pattern
[01:30:40.680 --> 01:30:43.200]   recognition that you do all day, right?
[01:30:43.200 --> 01:30:47.640]   And so they just forget like you can train them to spot guns on an X-ray, but then they'll
[01:30:47.640 --> 01:30:51.640]   forget not because they're lazy or whatever, but because they never see guns on an X-ray,
[01:30:51.640 --> 01:30:54.480]   they see water bottles all day long.
[01:30:54.480 --> 01:30:56.760]   That's interesting.
[01:30:56.760 --> 01:31:01.440]   So you're not, you're not, doesn't require a notion of a human soul or some sort of magical
[01:31:01.440 --> 01:31:03.400]   capability that cognizant human.
[01:31:03.400 --> 01:31:08.440]   Look, also, also the company is, yeah, the conversations that I think there are conversations
[01:31:08.440 --> 01:31:12.720]   that really occur on the fringes about this stuff leading to general intelligence.
[01:31:12.720 --> 01:31:17.280]   And I think that, that, um, you know, the mainstream conversation about this stuff looks
[01:31:17.280 --> 01:31:21.960]   at it rationally and says there's a lot of stuff that we can do humans and artificial
[01:31:21.960 --> 01:31:24.040]   intelligence combined.
[01:31:24.040 --> 01:31:26.000]   And I think sorry, go ahead.
[01:31:26.000 --> 01:31:27.640]   I beg your pardon.
[01:31:27.640 --> 01:31:28.640]   Go ahead.
[01:31:28.640 --> 01:31:29.960]   No, no, you finish.
[01:31:29.960 --> 01:31:30.960]   I'm sorry.
[01:31:30.960 --> 01:31:31.960]   I thought you were done.
[01:31:31.960 --> 01:31:32.960]   Yeah.
[01:31:32.960 --> 01:31:36.600]   I apologize because the latency in the, in the Skype is, is killing us as usual.
[01:31:36.600 --> 01:31:37.600]   Not Skype zoom.
[01:31:37.600 --> 01:31:39.600]   Uh, go ahead, Corey.
[01:31:39.600 --> 01:31:43.720]   I was just going to say during the lockdown, the world economic forum asked me to give
[01:31:43.720 --> 01:31:46.640]   them a talk on technological unemployment.
[01:31:46.640 --> 01:31:49.400]   And when I sent them the text of the talk, they withdrew the invitation.
[01:31:49.400 --> 01:31:52.280]   So it turned it into a column.
[01:31:52.280 --> 01:31:54.160]   And it's that it just put it in the URL there.
[01:31:54.160 --> 01:31:56.520]   You weren't saying what they wanted you to say, Corey.
[01:31:56.520 --> 01:32:00.740]   But basically I said, like, I don't think we're going to have AI driven unemployment
[01:32:00.740 --> 01:32:05.360]   because even if we automate some stuff, like we're going to have to, you know, relocate
[01:32:05.360 --> 01:32:09.320]   every city 20 kilometers inland over the next 300 years, that's full employment for
[01:32:09.320 --> 01:32:15.440]   everyone, no matter how many robots we build, there's just like more work than we can imagine.
[01:32:15.440 --> 01:32:17.440]   And they didn't like that at all.
[01:32:17.440 --> 01:32:18.440]   Really interesting.
[01:32:18.440 --> 01:32:19.440]   Yeah.
[01:32:19.440 --> 01:32:21.560]   Didn't fit their, uh, their model.
[01:32:21.560 --> 01:32:23.760]   But they, they really believe, and that's why I raised it.
[01:32:23.760 --> 01:32:29.080]   They really believe in technological employment, generally I break throughs on the immediate
[01:32:29.080 --> 01:32:30.240]   horizon.
[01:32:30.240 --> 01:32:35.800]   You know, they, they, they aren't fringe beliefs in the, in the, in the halls of power or in
[01:32:35.800 --> 01:32:38.240]   the halls of business or even in finance.
[01:32:38.240 --> 01:32:39.840]   They are accepted as gospel.
[01:32:39.840 --> 01:32:43.480]   You know, there are a few things like general, I don't know.
[01:32:43.480 --> 01:32:48.160]   There are a few things like generally, hold on a second, general AI, a fusion quantum
[01:32:48.160 --> 01:32:57.440]   computing, uh, it seems prudent to perhaps consider their eventual invention, even if
[01:32:57.440 --> 01:33:00.240]   they're not necessarily around the corner.
[01:33:00.240 --> 01:33:02.800]   Uh, go ahead, Alex.
[01:33:02.800 --> 01:33:04.320]   I mean, it's just not what I hear.
[01:33:04.320 --> 01:33:06.120]   You know, I mean, maybe you think it's going to happen?
[01:33:06.120 --> 01:33:07.120]   No, I'm sorry.
[01:33:07.120 --> 01:33:08.120]   I do.
[01:33:08.120 --> 01:33:09.120]   Will it happen eventually?
[01:33:09.120 --> 01:33:10.120]   Who knows?
[01:33:10.120 --> 01:33:14.000]   Um, but this, I, I just don't agree with Corey about, um, you know, this being an accepted
[01:33:14.000 --> 01:33:15.000]   thing.
[01:33:15.000 --> 01:33:18.600]   Um, at least not in the conversations I hear, you know, maybe the, uh, Oh, you're saying
[01:33:18.600 --> 01:33:22.320]   that people don't believe generally, I don't think so.
[01:33:22.320 --> 01:33:25.800]   No, I think look at the response to what happened with this Google engineer who said that the
[01:33:25.800 --> 01:33:27.600]   chat bot was, uh,
[01:33:27.600 --> 01:33:28.600]   Googled and like it.
[01:33:28.600 --> 01:33:30.480]   And Google fired him.
[01:33:30.480 --> 01:33:34.360]   Anyone who with any standing in the research can be all said that's, you know, called
[01:33:34.360 --> 01:33:35.920]   him and he did.
[01:33:35.920 --> 01:33:36.920]   Yeah.
[01:33:36.920 --> 01:33:37.920]   And yeah.
[01:33:37.920 --> 01:33:38.920]   And like, I don't know.
[01:33:38.920 --> 01:33:41.840]   I mean, Corey, maybe the, you know, the influencer class likes to talk about this at
[01:33:41.840 --> 01:33:43.400]   their conferences.
[01:33:43.400 --> 01:33:46.040]   Um, but this idea that AGI is right around the corner just to me.
[01:33:46.040 --> 01:33:47.480]   I've never heard that.
[01:33:47.480 --> 01:33:50.640]   Um, you know, from credible folks in the industry.
[01:33:50.640 --> 01:33:51.640]   Sure.
[01:33:51.640 --> 01:33:54.800]   Well, I mean, science fiction writers certainly think it's nonsense, right?
[01:33:54.800 --> 01:33:57.360]   Like it is a recurring theme at science fiction conventions.
[01:33:57.360 --> 01:34:02.920]   Why are all these CEOs out there saying this is, this is, uh, on the horizon, but they
[01:34:02.920 --> 01:34:03.920]   are saying.
[01:34:03.920 --> 01:34:05.840]   I think the CEOs aren't saying it.
[01:34:05.840 --> 01:34:08.960]   I think the CEOs are saying that it's very powerful technology.
[01:34:08.960 --> 01:34:13.240]   I don't think they're saying we're going to be hand in hand with artificial general
[01:34:13.240 --> 01:34:14.240]   intelligence.
[01:34:14.240 --> 01:34:15.480]   And that's what we're working to.
[01:34:15.480 --> 01:34:19.840]   I mean, you know, investing in and we can believe it.
[01:34:19.840 --> 01:34:24.160]   That's a hypothesis at the center for existential risk who are like, you know, have attracted
[01:34:24.160 --> 01:34:27.480]   like billions of dollars, hundreds of millions of dollars in capital, right?
[01:34:27.480 --> 01:34:31.080]   I mean, they were already saying that we shouldn't equate money with smarts.
[01:34:31.080 --> 01:34:32.240]   No, it's true.
[01:34:32.240 --> 01:34:33.240]   It's true.
[01:34:33.240 --> 01:34:37.120]   What I'm saying is that there may be a small number of very rich people who believe this,
[01:34:37.120 --> 01:34:40.480]   but there are, there are some very rich people who believe that and a bunch of weird stands
[01:34:40.480 --> 01:34:42.720]   for them who also believe it.
[01:34:42.720 --> 01:34:46.120]   I just think if you speak with people, credible folks in the industry, you know, folks who
[01:34:46.120 --> 01:34:48.520]   are actually doing the research, like Nick's boss.
[01:34:48.520 --> 01:34:50.240]   I mean, I like the guy.
[01:34:50.240 --> 01:34:51.240]   He's a philosopher.
[01:34:51.240 --> 01:34:52.240]   He's not.
[01:34:52.240 --> 01:34:53.840]   He doesn't work in machine learning.
[01:34:53.840 --> 01:34:54.840]   Yep.
[01:34:54.840 --> 01:34:55.840]   And you speak with researchers.
[01:34:55.840 --> 01:34:59.200]   You speak with the tech companies, you know, they might use marketing terms to talk about
[01:34:59.200 --> 01:35:03.400]   power of their artificial intelligence, soon to our Pichai calling it, you know, as powerful
[01:35:03.400 --> 01:35:04.400]   as fire.
[01:35:04.400 --> 01:35:06.520]   That sounds like marketing to me.
[01:35:06.520 --> 01:35:10.760]   But I never hear him or anybody at Google talking about, you know, us reaching artificial
[01:35:10.760 --> 01:35:14.640]   general intelligence outside of one guy, you know, who actually has an interesting story
[01:35:14.640 --> 01:35:15.640]   to tell.
[01:35:15.640 --> 01:35:17.120]   And I did have one on my podcast.
[01:35:17.120 --> 01:35:19.840]   We had an interesting conversation.
[01:35:19.840 --> 01:35:27.280]   That being said, you know, he's the extreme exception and not the rule.
[01:35:27.280 --> 01:35:29.280]   So I that's good.
[01:35:29.280 --> 01:35:32.960]   This is the Oh, yeah, here it is.
[01:35:32.960 --> 01:35:35.160]   Greg Lemoine, right?
[01:35:35.160 --> 01:35:36.160]   Blake Lemoine.
[01:35:36.160 --> 01:35:37.160]   Yeah, Blake Lemoine.
[01:35:37.160 --> 01:35:38.160]   Yeah.
[01:35:38.160 --> 01:35:39.160]   Yeah.
[01:35:39.160 --> 01:35:40.160]   Mm hmm.
[01:35:40.160 --> 01:35:41.160]   Nice.
[01:35:41.160 --> 01:35:43.080]   Yeah, I think the picture kind of says it all.
[01:35:43.080 --> 01:35:48.400]   If that's stable diffusion, I think it's a thing that's a thing that's a thing that's
[01:35:48.400 --> 01:35:49.400]   interesting.
[01:35:49.400 --> 01:35:50.400]   He's an interesting character.
[01:35:50.400 --> 01:35:55.280]   He's not dumb by any means, you know, obviously, but he's also a priest.
[01:35:55.280 --> 01:36:00.360]   He's spiritual in ways that, you know, I think are pretty relevant to the story.
[01:36:00.360 --> 01:36:02.800]   And he had some pretty neat interactions with Lambda.
[01:36:02.800 --> 01:36:06.400]   He drew the conclusion and he was, I think, predisposed to believe that there was going
[01:36:06.400 --> 01:36:10.440]   to be a general intelligence that is going to speak to him.
[01:36:10.440 --> 01:36:13.920]   You know, through a bot, he's like talked about it before he came out saying he believes
[01:36:13.920 --> 01:36:15.400]   Lambda is is this intelligent?
[01:36:15.400 --> 01:36:16.920]   He may also believe in fairies.
[01:36:16.920 --> 01:36:21.840]   I mean, it's not what I told him is, is that I think that he's wrong and he'll be in
[01:36:21.840 --> 01:36:22.840]   the history books.
[01:36:22.840 --> 01:36:25.440]   So I think that's like a self-described reason.
[01:36:25.440 --> 01:36:26.440]   Yeah, he's a mystic.
[01:36:26.440 --> 01:36:27.680]   He calls himself a mystic, right?
[01:36:27.680 --> 01:36:30.320]   Look, I encourage people who are on the so on here.
[01:36:30.320 --> 01:36:32.960]   So I had Blake on the big technology podcast.
[01:36:32.960 --> 01:36:36.200]   We spoke for an hour and a half about his interactions with Dolly.
[01:36:36.200 --> 01:36:39.560]   I kind of thought it was pretty interesting, you know, to your listen to perspective.
[01:36:39.560 --> 01:36:40.560]   Yeah.
[01:36:40.560 --> 01:36:44.960]   Talking about how the bot, you know, hired a lawyer, this again goes to like my comment
[01:36:44.960 --> 01:36:49.880]   before about how far we've come that if the technology can now fool Google and engineer
[01:36:49.880 --> 01:36:53.880]   into thinking it's sentient, then it's probably like pretty interesting technology that we
[01:36:53.880 --> 01:36:58.200]   should be focusing on, you know, what it can do, the dangers of it, et cetera, et cetera.
[01:36:58.200 --> 01:37:02.600]   And I do think that often these conversations about is it sentient or not, you know, kind
[01:37:02.600 --> 01:37:05.680]   of take our eye off the ball on that front.
[01:37:05.680 --> 01:37:10.580]   And then I also had Gary Marcus on who also like, well, anyway, I had Gary Marcus on who
[01:37:10.580 --> 01:37:15.880]   is he called it something like like foolishness on stilts or something like that.
[01:37:15.880 --> 01:37:21.320]   And we talked through all the other, you know, counter arguments to Blake.
[01:37:21.320 --> 01:37:26.240]   But yeah, it's look, I think the one thing that we can say is we're in a very interesting
[01:37:26.240 --> 01:37:30.960]   moment in technology where research is moving forward and the practical uses of the stuff
[01:37:30.960 --> 01:37:34.400]   that is being developed and it's being developed moving forward.
[01:37:34.400 --> 01:37:38.120]   Is it pointless to speculate about general AI though?
[01:37:38.120 --> 01:37:42.160]   I mean, is it so far off that why should we think about it?
[01:37:42.160 --> 01:37:43.160]   Yeah.
[01:37:43.160 --> 01:37:44.400]   Should we be thinking about it now?
[01:37:44.400 --> 01:37:45.680]   I mean, is it fun?
[01:37:45.680 --> 01:37:50.520]   You know, that's like we have to admit that like humans have a capacity and interest in
[01:37:50.520 --> 01:37:51.520]   fun.
[01:37:51.520 --> 01:37:55.280]   And it's fun to speculate about just like to say, think about how much energy goes into
[01:37:55.280 --> 01:37:58.280]   people trying to game out sports games before they happen.
[01:37:58.280 --> 01:38:00.200]   You know, as Mark Zuckerberg going to be in the ring.
[01:38:00.200 --> 01:38:01.200]   Yeah, we like it.
[01:38:01.200 --> 01:38:05.000]   But I don't think that is, you know, there's there's practical and then there's just enjoyable.
[01:38:05.000 --> 01:38:07.280]   Like we all love to have our minds wander.
[01:38:07.280 --> 01:38:10.880]   This is why this is such a big part of science fiction, you know, to think about where it
[01:38:10.880 --> 01:38:11.880]   could go.
[01:38:11.880 --> 01:38:15.880]   You know, the idea that it's, you know, it's coming.
[01:38:15.880 --> 01:38:19.800]   It's here and we better get ready to make our robot friends pretty quickly.
[01:38:19.800 --> 01:38:21.800]   You know, that seems outlandish.
[01:38:21.800 --> 01:38:22.800]   Yeah.
[01:38:22.800 --> 01:38:25.800]   Boston's simulation hypothesis is really just a fun thing to think about.
[01:38:25.800 --> 01:38:29.240]   But kind of pointless to spend any real energy on.
[01:38:29.240 --> 01:38:33.160]   I had a very interesting conversation with Boston for my book and, you know, I just was
[01:38:33.160 --> 01:38:35.360]   like, this was a black mirror chapter that I wrote.
[01:38:35.360 --> 01:38:39.080]   And I was like, all right, Nick, like go ahead and tell about all the terrible things that
[01:38:39.080 --> 01:38:40.080]   are going to happen.
[01:38:40.080 --> 01:38:44.440]   He's like, listen, I've kind of been like, have this bad brand because I did think about
[01:38:44.440 --> 01:38:47.880]   it, but maybe it's actually going to be good, you know, when it comes here.
[01:38:47.880 --> 01:38:54.320]   So general AI or the simulation is this is a new one.
[01:38:54.320 --> 01:38:56.800]   Leo, let's talk about all the simulation.
[01:38:56.800 --> 01:38:58.080]   You know, is there free will?
[01:38:58.080 --> 01:39:01.680]   Are you know, is there any difference from a simulation and what we're, what we're living
[01:39:01.680 --> 01:39:02.680]   here.
[01:39:02.680 --> 01:39:04.320]   I think the baiting that's in Saugusty.
[01:39:04.320 --> 01:39:07.680]   Yeah, I don't think there's nothing new about about it.
[01:39:07.680 --> 01:39:11.560]   But it does the cool thing about this is that it does help reframe, you know, the discussion
[01:39:11.560 --> 01:39:14.440]   or like, give us a new angle to think about it.
[01:39:14.440 --> 01:39:16.640]   But I think right now what it is is fun.
[01:39:16.640 --> 01:39:17.640]   It's fun.
[01:39:17.640 --> 01:39:22.040]   So there was a wonderful book this year from James Bridal, who's the, you may know him from
[01:39:22.040 --> 01:39:23.200]   some of his weird stunts.
[01:39:23.200 --> 01:39:27.640]   He's the guy who built a self driving car and then surrounded it in a circle of salt
[01:39:27.640 --> 01:39:31.880]   that it thought was a road marking that can cross.
[01:39:31.880 --> 01:39:36.440]   He's also the guy who did the research that found that YouTube kids was full of all these
[01:39:36.440 --> 01:39:38.200]   weird semi automated.
[01:39:38.200 --> 01:39:39.200]   Oh, yeah.
[01:39:39.200 --> 01:39:40.200]   Yeah.
[01:39:40.200 --> 01:39:41.200]   Yeah.
[01:39:41.200 --> 01:39:46.640]   So he wrote this book called ways of being that's about extending a view of personhood
[01:39:46.640 --> 01:39:52.440]   to the inanimate to regular software to machine learning and so on that makes quite an interesting
[01:39:52.440 --> 01:39:53.440]   case for it.
[01:39:53.440 --> 01:39:59.160]   I just just pasted a link to my review of the book into the chat there.
[01:39:59.160 --> 01:40:01.160]   He is a fascinating guy.
[01:40:01.160 --> 01:40:06.960]   And you know, he makes a great case for the idea that the fact that something isn't,
[01:40:06.960 --> 01:40:11.080]   isn't intelligent does not mean that we shouldn't think of it as a person.
[01:40:11.080 --> 01:40:12.080]   Wow.
[01:40:12.080 --> 01:40:16.480]   Like ecosystems and rocks and stuff.
[01:40:16.480 --> 01:40:19.680]   You don't need a Gaia hypothesis to treat the earth with.
[01:40:19.680 --> 01:40:24.000]   There's this car in the salt circle.
[01:40:24.000 --> 01:40:25.000]   What happened?
[01:40:25.000 --> 01:40:26.000]   It just sit there.
[01:40:26.000 --> 01:40:28.840]   It couldn't move.
[01:40:28.840 --> 01:40:31.040]   And he wrote about how it made him feel bad, right?
[01:40:31.040 --> 01:40:32.040]   That he invented a car.
[01:40:32.040 --> 01:40:34.040]   He heard its feelings.
[01:40:34.040 --> 01:40:36.840]   And well, it heard his feelings was his point.
[01:40:36.840 --> 01:40:45.200]   And that like the act that, you know, acknowledging that it feels bad to design a thing to do
[01:40:45.200 --> 01:40:51.160]   something and then frustrate it is a step towards a kind of wider empathy.
[01:40:51.160 --> 01:40:52.840]   I really like him.
[01:40:52.840 --> 01:40:58.280]   Do you think he apologizes to Amazon's echo if he swears at it?
[01:40:58.280 --> 01:41:01.000]   You know, there's a lot of people who think you should who think he should.
[01:41:01.000 --> 01:41:02.000]   Echo thinks you should.
[01:41:02.000 --> 01:41:03.000]   The voice assistants.
[01:41:03.000 --> 01:41:04.400]   Go ahead and swear at echo.
[01:41:04.400 --> 01:41:08.040]   It'll chastise you for mistreating it.
[01:41:08.040 --> 01:41:10.000]   Yeah, that's creepy to me.
[01:41:10.000 --> 01:41:11.000]   I don't.
[01:41:11.000 --> 01:41:14.200]   I think that's a bridge too far.
[01:41:14.200 --> 01:41:17.480]   But there's, you know, my wife and I have an argument.
[01:41:17.480 --> 01:41:23.800]   She says you shouldn't teach people to be, require people to be polite to inanimate objects.
[01:41:23.800 --> 01:41:28.240]   That's kind of imbuing it with more power than it deserves.
[01:41:28.240 --> 01:41:30.440]   It's in an inanimate object.
[01:41:30.440 --> 01:41:34.960]   You know, there's a vegan slash vegetarian argument that says that what whatever you
[01:41:34.960 --> 01:41:41.640]   treat with the least respect is the floor at the, at the, on that your respect for everyone
[01:41:41.640 --> 01:41:43.440]   else won't drop below.
[01:41:43.440 --> 01:41:48.640]   So whatever it is you respect least in the world, however much respect you afford that,
[01:41:48.640 --> 01:41:51.360]   that's how little respect you will treat anyone else with.
[01:41:51.360 --> 01:41:53.520]   You'll never treat them with less respect than that.
[01:41:53.520 --> 01:41:57.680]   And so if you raise the floor for how much respect you afford to the thing you respect
[01:41:57.680 --> 01:42:03.400]   least, then you end up raising the amount of respect you bring to everything else in
[01:42:03.400 --> 01:42:04.400]   the world.
[01:42:04.400 --> 01:42:05.400]   I like that.
[01:42:05.400 --> 01:42:06.400]   I like that.
[01:42:06.400 --> 01:42:07.400]   I think that's good.
[01:42:07.400 --> 01:42:10.200]   Do you, do you follow that?
[01:42:10.200 --> 01:42:11.200]   Probably not very well.
[01:42:11.200 --> 01:42:13.600]   Do you ever swear at echo?
[01:42:13.600 --> 01:42:16.960]   We don't have any voice assistants in my house.
[01:42:16.960 --> 01:42:19.680]   So that's how little respect you have for them.
[01:42:19.680 --> 01:42:20.680]   Yeah.
[01:42:20.680 --> 01:42:24.320]   That's a good fact for them not to have one.
[01:42:24.320 --> 01:42:25.440]   Let's take a little break.
[01:42:25.440 --> 01:42:30.680]   Having fun, I have to say with Alex Kaptor, what's big technology podcast, you could see
[01:42:30.680 --> 01:42:32.320]   why you want to listen to this boy.
[01:42:32.320 --> 01:42:34.400]   You have some great people on that's fantastic.
[01:42:34.400 --> 01:42:35.400]   Thanks, Leo.
[01:42:35.400 --> 01:42:36.400]   Yeah.
[01:42:36.400 --> 01:42:37.400]   We have a nice run these days.
[01:42:37.400 --> 01:42:38.400]   Yeah.
[01:42:38.400 --> 01:42:43.560]   And his book, of course, always day one, which is more than about Amazon, apparently,
[01:42:43.560 --> 01:42:47.480]   Amazon, Apple, Facebook, Google, Microsoft, child, there and each, Zuck speaks with me
[01:42:47.480 --> 01:42:48.480]   for it.
[01:42:48.480 --> 01:42:52.720]   I want Tech Titan's plan to stay on top forever.
[01:42:52.720 --> 01:42:53.720]   Forever.
[01:42:53.720 --> 01:42:54.720]   Forever.
[01:42:54.720 --> 01:42:57.160]   There's a fun subtitle.
[01:42:57.160 --> 01:42:58.560]   Yeah, I love it.
[01:42:58.560 --> 01:43:00.840]   Did you come up with that or your publisher?
[01:43:00.840 --> 01:43:02.480]   We had a bit of a back and forth about it.
[01:43:02.480 --> 01:43:03.480]   They came up with it.
[01:43:03.480 --> 01:43:04.480]   I'm like, I love that.
[01:43:04.480 --> 01:43:05.480]   We use it.
[01:43:05.480 --> 01:43:06.480]   And they said, no.
[01:43:06.480 --> 01:43:07.480]   That's not.
[01:43:07.480 --> 01:43:08.480]   They said, no.
[01:43:08.480 --> 01:43:09.480]   They said, no.
[01:43:09.480 --> 01:43:11.400]   They said, we don't, we don't think we should use it.
[01:43:11.400 --> 01:43:13.480]   And I was like, it's yours back and forth.
[01:43:13.480 --> 01:43:14.800]   And eventually it stuck.
[01:43:14.800 --> 01:43:15.880]   We used it.
[01:43:15.880 --> 01:43:17.480]   And it's fun.
[01:43:17.480 --> 01:43:19.080]   I think people read into it what they want.
[01:43:19.080 --> 01:43:20.640]   I love it.
[01:43:20.640 --> 01:43:24.240]   My first book, I wanted to call it, How to Get the Dog Here Out of the Disc Drive, the
[01:43:24.240 --> 01:43:31.600]   publisher renamed it 101 Computer Answers You Need to Know, which turned out to be a
[01:43:31.600 --> 01:43:36.720]   strategic mistake because shortly thereafter, Kim Commando came out with a book called 1000
[01:43:36.720 --> 01:43:38.920]   and 1 Computer Answers You Need to Know.
[01:43:38.920 --> 01:43:42.480]   And I think you can guess which one sold better, right?
[01:43:42.480 --> 01:43:46.440]   It should have been 101 Dalmatian hairs in this drive.
[01:43:46.440 --> 01:43:47.440]   There you go.
[01:43:47.440 --> 01:43:50.640]   I think How to Get the Dog Here Out of the Disc Drive was a pretty good title.
[01:43:50.640 --> 01:43:51.640]   But all right.
[01:43:51.640 --> 01:43:52.640]   That's good.
[01:43:52.640 --> 01:43:53.640]   Yeah.
[01:43:53.640 --> 01:43:54.640]   Yeah.
[01:43:54.640 --> 01:43:55.640]   Show Today brought to you by podium.
[01:43:55.640 --> 01:43:57.040]   You know text messaging works.
[01:43:57.040 --> 01:44:01.560]   If you want to reach Elon Musk, text, text him.
[01:44:01.560 --> 01:44:08.920]   Well, it turns out businesses can use text messaging to stay in touch with their customers.
[01:44:08.920 --> 01:44:11.160]   And this is, this has came at a very welcome time.
[01:44:11.160 --> 01:44:14.560]   It's been a tough couple of years for small businesses.
[01:44:14.560 --> 01:44:19.560]   You know, you know, supply chain issues, COVID.
[01:44:19.560 --> 01:44:25.880]   But one of the things I think we learned from COVID is that staying in touch with customers
[01:44:25.880 --> 01:44:29.800]   via text is kind of the way they prefer to communicate.
[01:44:29.800 --> 01:44:31.160]   You know, your food is ready.
[01:44:31.160 --> 01:44:32.160]   Come pick it up.
[01:44:32.160 --> 01:44:33.160]   Your groceries are ready.
[01:44:33.160 --> 01:44:34.160]   Come pick it up.
[01:44:34.160 --> 01:44:40.400]   A lot of us don't want to use the phone to call a business, whether plumber, landscaper,
[01:44:40.400 --> 01:44:42.240]   we don't want to play phone tag.
[01:44:42.240 --> 01:44:45.520]   We would prefer to leave a quick message.
[01:44:45.520 --> 01:44:48.800]   But if you're running a business, you can take advantage of that.
[01:44:48.800 --> 01:44:51.880]   If the only way to reach you is with a phone number, people are actually going to turn
[01:44:51.880 --> 01:44:52.880]   their back.
[01:44:52.880 --> 01:44:54.440]   You're going to walk away from you.
[01:44:54.440 --> 01:44:57.600]   But if that phone number can be texted or you could have on your website, a widget that
[01:44:57.600 --> 01:45:02.880]   says, send me a text or you can use text messaging to reach out to customers.
[01:45:02.880 --> 01:45:08.080]   For instance, when I, every time I leave my dentist now, I get a text messaging rate,
[01:45:08.080 --> 01:45:11.760]   rate us on Yelp or Google business.
[01:45:11.760 --> 01:45:15.120]   There's an ice cream shop in town that uses podium every three or four weeks.
[01:45:15.120 --> 01:45:16.440]   It says, oh, we haven't seen you in a while.
[01:45:16.440 --> 01:45:18.360]   Here's a coupon for ice cream.
[01:45:18.360 --> 01:45:21.120]   No, but it really works.
[01:45:21.120 --> 01:45:22.640]   Gets me in the door.
[01:45:22.640 --> 01:45:27.680]   podium gives businesses the tools to compete with the convenience offered by bigger businesses
[01:45:27.680 --> 01:45:28.840]   like Amazon.
[01:45:28.840 --> 01:45:34.600]   So this is really a boon for the small business from health care providers like my dentist
[01:45:34.600 --> 01:45:36.040]   to plumbers.
[01:45:36.040 --> 01:45:40.720]   Over a hundred thousand businesses are texting with customers using podium.
[01:45:40.720 --> 01:45:45.000]   Others love the convenience, but business, you will love the results when car dealer sold
[01:45:45.000 --> 01:45:47.600]   a $50,000 truck and four text messages.
[01:45:47.600 --> 01:45:53.280]   In fact, that's really, that's the only way I want to interact these days with my, my,
[01:45:53.280 --> 01:45:54.280]   my dealership, right?
[01:45:54.280 --> 01:45:58.600]   In fact, I just got a text because I'm bringing the car in tomorrow to confirm a jeweler sold
[01:45:58.600 --> 01:46:03.200]   a $5,000 ring coordinated curbside pickups did it all in text.
[01:46:03.200 --> 01:46:09.280]   A dentist who had gotten really behind on his collections decided to use text messages.
[01:46:09.280 --> 01:46:13.360]   70% of the outstanding collections came in in two weeks, but it's just easier for people.
[01:46:13.360 --> 01:46:14.960]   It's not that they didn't want to pay them.
[01:46:14.960 --> 01:46:16.120]   It just, it was convenient.
[01:46:16.120 --> 01:46:17.120]   It was easy.
[01:46:17.120 --> 01:46:19.200]   It was fast with podiums all in one inbox.
[01:46:19.200 --> 01:46:20.320]   You can do more than just chat.
[01:46:20.320 --> 01:46:22.080]   You can get online reviews.
[01:46:22.080 --> 01:46:25.320]   Just send a link that works so much better.
[01:46:25.320 --> 01:46:28.480]   Collect payments right through podium from anywhere.
[01:46:28.480 --> 01:46:30.880]   You could send marketing campaigns that actually get a response.
[01:46:30.880 --> 01:46:34.920]   It's just a quick text and your staff will love it too because all the communications
[01:46:34.920 --> 01:46:39.280]   that come into your customers come into one inbox makes it easy for you to keep track
[01:46:39.280 --> 01:46:41.800]   of what's going on with any given customer.
[01:46:41.800 --> 01:46:43.560]   I want you to try podium.
[01:46:43.560 --> 01:46:44.880]   It's really cool.
[01:46:44.880 --> 01:46:47.360]   See how podium can grow your business.
[01:46:47.360 --> 01:46:57.040]   There's a great demo video at podium.com/twit and a pretty good deal to podum.com/twit.
[01:46:57.040 --> 01:46:58.040]   podium.
[01:46:58.040 --> 01:47:00.640]   Let's grow podium.com/twit.
[01:47:00.640 --> 01:47:01.640]   Let's grow.
[01:47:01.640 --> 01:47:02.640]   Let's grow.
[01:47:02.640 --> 01:47:03.640]   Let's grow.
[01:47:03.640 --> 01:47:05.800]   Let's go to the next messaging platform.
[01:47:05.800 --> 01:47:08.080]   Thank you podium.
[01:47:08.080 --> 01:47:15.320]   You actually Alex had a good piece about the judge in the court of chanceery that Elon Musk
[01:47:15.320 --> 01:47:19.600]   is going to be facing.
[01:47:19.600 --> 01:47:26.920]   This is coming up October 17th and you did a profile of Kathleen McCormick.
[01:47:26.920 --> 01:47:31.520]   Who is she and is this good for Elon or bad?
[01:47:31.520 --> 01:47:33.240]   She's a fascinating character.
[01:47:33.240 --> 01:47:37.840]   I would say it's largely bad for Elon that she's going to be there.
[01:47:37.840 --> 01:47:43.120]   Just to give a little context as to who she is, she grew up in Delaware.
[01:47:43.120 --> 01:47:45.440]   She was the daughter of two public school teachers.
[01:47:45.440 --> 01:47:47.920]   Her dad coached football.
[01:47:47.920 --> 01:47:54.800]   Her mom eventually rose to become an administrator both duties they did in addition to the teaching.
[01:47:54.800 --> 01:48:01.720]   She is the first person from her town, Smirna, which is a middle-class town in Delaware,
[01:48:01.720 --> 01:48:04.400]   to go to Harvard.
[01:48:04.400 --> 01:48:09.400]   Think she's going to go back into education, gets involved in a legal nonprofit, starts
[01:48:09.400 --> 01:48:15.320]   to see how the law could be used for good, goes to Notre Dame because her dad was a massive
[01:48:15.320 --> 01:48:23.680]   Notre Dame fan and gets a law degree there working on human civil rights, actually takes
[01:48:23.680 --> 01:48:28.440]   a job at a nonprofit and goes to argue in front of the court and private practice and
[01:48:28.440 --> 01:48:32.120]   eventually becomes a vice chancellor.
[01:48:32.120 --> 01:48:37.080]   From there, she becomes the first female chancellor of the court in 229 years in its
[01:48:37.080 --> 01:48:39.280]   entire history.
[01:48:39.280 --> 01:48:44.440]   She has this very interesting ruling as vice chancellor where these two private equity
[01:48:44.440 --> 01:48:50.680]   companies, one sells this cake decorating company to the other and then COVID hits.
[01:48:50.680 --> 01:48:54.680]   They're like, "Well, listen, no one's going to want to decorate cakes anymore."
[01:48:54.680 --> 01:48:56.120]   The buyer said that.
[01:48:56.120 --> 01:49:00.120]   They said, "Okay, we're not going to buy this company anymore."
[01:49:00.120 --> 01:49:05.400]   The seller sued them and it lands in front of vice chancellor McCormick.
[01:49:05.400 --> 01:49:10.040]   She basically says, "Look, you signed a deal and I'm the judge and most important for me
[01:49:10.040 --> 01:49:13.080]   is deal certainty.
[01:49:13.080 --> 01:49:16.960]   That's what our job is here is to make sure that when you agree to a deal, the deal goes
[01:49:16.960 --> 01:49:18.400]   through."
[01:49:18.400 --> 01:49:21.120]   She forced that cake decorating deal through.
[01:49:21.120 --> 01:49:22.120]   Wow.
[01:49:22.120 --> 01:49:25.120]   That's going to be scary for Elon.
[01:49:25.120 --> 01:49:26.120]   Absolutely.
[01:49:26.120 --> 01:49:29.120]   I don't think Deco Pack went for $44 billion.
[01:49:29.120 --> 01:49:33.120]   No, it was $550 million, so it was a much smaller scale.
[01:49:33.120 --> 01:49:38.440]   That's actually a half a billion for a cake decorating company.
[01:49:38.440 --> 01:49:42.360]   It's a pretty legit cake decorating company.
[01:49:42.360 --> 01:49:43.520]   Their tagline is hilarious.
[01:49:43.520 --> 01:49:46.960]   It's like, "We decorate the world's best cakes or something like that."
[01:49:46.960 --> 01:49:47.960]   That's pretty good.
[01:49:47.960 --> 01:49:49.560]   Half a billion a decade.
[01:49:49.560 --> 01:49:50.560]   Wow.
[01:49:50.560 --> 01:49:51.560]   Not bad.
[01:49:51.560 --> 01:49:56.040]   No, this one is much bigger, $44 billion.
[01:49:56.040 --> 01:49:59.280]   I just don't think the size of the deal is going to come in.
[01:49:59.280 --> 01:50:02.680]   Everything I know about her doesn't need me to believe that the size of the deal or the
[01:50:02.680 --> 01:50:05.960]   court's ability to enforce is going to come into the rule he met.
[01:50:05.960 --> 01:50:06.960]   She's going to give.
[01:50:06.960 --> 01:50:13.160]   The question is, what happens with this whistleblower testimony, which I think can throw a wrench
[01:50:13.160 --> 01:50:16.760]   in Twitter's ability to force it to close?
[01:50:16.760 --> 01:50:20.200]   But Elon did sign the deal saying that he was going to buy it.
[01:50:20.200 --> 01:50:24.960]   If you look at the past precedent of this judge, we'll need you to believe that she'll make
[01:50:24.960 --> 01:50:25.960]   it go through.
[01:50:25.960 --> 01:50:27.560]   All things being equal.
[01:50:27.560 --> 01:50:29.240]   So very interesting person.
[01:50:29.240 --> 01:50:31.040]   It might be a scary proposition.
[01:50:31.040 --> 01:50:34.920]   I'm not sure I want Elon Musk to own Twitter, to be honest.
[01:50:34.920 --> 01:50:35.920]   Well, neither does Elon.
[01:50:35.920 --> 01:50:36.920]   Yeah.
[01:50:36.920 --> 01:50:39.440]   I feel like he should do something to make Twitter whole.
[01:50:39.440 --> 01:50:48.160]   This whole drama has cost Twitter staff a lot of upset, probably a lot of money.
[01:50:48.160 --> 01:50:51.920]   Elon was pretty flippant in the whole thing.
[01:50:51.920 --> 01:50:55.720]   So I feel like he needs to be somehow chasing.
[01:50:55.720 --> 01:50:59.720]   But boy, I'm not sure I want him to be forced to buy Twitter.
[01:50:59.720 --> 01:51:01.720]   But that won't come into consideration.
[01:51:01.720 --> 01:51:02.960]   No, that's not a consideration.
[01:51:02.960 --> 01:51:04.800]   It's just a contractual matter.
[01:51:04.800 --> 01:51:06.560]   Which makes it interesting.
[01:51:06.560 --> 01:51:09.640]   And then, okay, then you get into, okay, so maybe they said, well, how much will Elon
[01:51:09.640 --> 01:51:12.240]   need to pay Twitter to make it whole?
[01:51:12.240 --> 01:51:14.960]   Has to be $20 billion.
[01:51:14.960 --> 01:51:16.400]   Something in that range.
[01:51:16.400 --> 01:51:20.640]   Because you look at Facebook, which is maybe an analog, which is whose stock has gone down.
[01:51:20.640 --> 01:51:23.600]   I don't know, 57, 60% this year.
[01:51:23.600 --> 01:51:31.240]   And without Musk, Twitter's stock would probably fall by that same amount.
[01:51:31.240 --> 01:51:33.160]   And now they're going to really struggle to operate.
[01:51:33.160 --> 01:51:37.040]   Their CEO has very little credibility inside the company and everyone's leaving.
[01:51:37.040 --> 01:51:39.160]   So what's it going to take?
[01:51:39.160 --> 01:51:43.200]   It might end up having to, it might end up being, and I've always thought, okay, very
[01:51:43.200 --> 01:51:45.480]   little chance this deal actually goes through.
[01:51:45.480 --> 01:51:47.680]   I'm like, well, maybe there is a chance.
[01:51:47.680 --> 01:51:52.280]   Well, the markets might agree with you because the stock price has been slowly ratcheting
[01:51:52.280 --> 01:51:55.200]   up, not 54, 20 as Elon promised to pay.
[01:51:55.200 --> 01:51:57.640]   But it's been going up.
[01:51:57.640 --> 01:51:59.000]   And I'm, you know, it might be a bargain.
[01:51:59.000 --> 01:52:03.120]   I mean, this is an investment advice, but if they got, if they got some many billions
[01:52:03.120 --> 01:52:10.000]   of dollars in cash, who knows, they might find a way, a way forward.
[01:52:10.000 --> 01:52:16.200]   The question, I don't know the answer to, but maybe you guys do, given how leveraged
[01:52:16.200 --> 01:52:25.320]   and how great the profit to valuation or earnings to valuation ratios are and how much
[01:52:25.320 --> 01:52:31.240]   room there is for those stocks to move, if Elon has to have flog $20 billion worth of
[01:52:31.240 --> 01:52:36.760]   his Tesla stock, does that trigger a cascade of effects that could endanger all of his
[01:52:36.760 --> 01:52:39.200]   businesses, some of his businesses?
[01:52:39.200 --> 01:52:42.000]   And would the judge consider that?
[01:52:42.000 --> 01:52:45.840]   I don't think the judge would consider that, but yeah, I would say that.
[01:52:45.840 --> 01:52:47.760]   Remember, Tesla is a story stock.
[01:52:47.760 --> 01:52:51.680]   So what happens in Elon's business genius, right?
[01:52:51.680 --> 01:52:55.600]   If that takes a hit, that myth, well, he's a great businessman.
[01:52:55.600 --> 01:52:58.560]   He's built some amazing companies, no doubt about that.
[01:52:58.560 --> 01:53:03.480]   But if he makes a very stupid mistake, and this might end up proving to be one, you know,
[01:53:03.480 --> 01:53:06.400]   that could really cause an impact to his other businesses.
[01:53:06.400 --> 01:53:07.400]   Yeah.
[01:53:07.400 --> 01:53:13.800]   So despite being the richest man in the world, Elon, most of the 44 billion came from Elon's
[01:53:13.800 --> 01:53:19.400]   loans against Elon's Tesla stock and from people like Larry Ellison, who threw in a
[01:53:19.400 --> 01:53:21.520]   billion just because he's going to be tough for him.
[01:53:21.520 --> 01:53:22.520]   We're buds.
[01:53:22.520 --> 01:53:25.080]   Two billion from Ellison's going to be two.
[01:53:25.080 --> 01:53:30.680]   So can the judge have to tell those people to follow through or does the entire burden
[01:53:30.680 --> 01:53:33.520]   fall on Elon?
[01:53:33.520 --> 01:53:35.800]   I think that's got to be a separate court action.
[01:53:35.800 --> 01:53:36.800]   Yeah.
[01:53:36.800 --> 01:53:43.360]   I've got to say, do I does Larry Ellison sending a DM to Elon going, "Law 1 billion or two,
[01:53:43.360 --> 01:53:44.360]   you tell me."
[01:53:44.360 --> 01:53:45.360]   I constitute it.
[01:53:45.360 --> 01:53:48.960]   I was literally, by the way, that's pretty much word for word.
[01:53:48.960 --> 01:53:49.960]   Yeah.
[01:53:49.960 --> 01:53:50.960]   Unbelievable.
[01:53:50.960 --> 01:53:51.960]   Yes.
[01:53:51.960 --> 01:53:55.680]   This thing will definitely extend beyond October because they'll go to appeals.
[01:53:55.680 --> 01:53:58.880]   And of course, there's the money question.
[01:53:58.880 --> 01:54:03.080]   But it's, I don't know, it's definitely been fun and wild to follow.
[01:54:03.080 --> 01:54:04.080]   Wow.
[01:54:04.080 --> 01:54:05.440]   But it could do some serious damage to Elon.
[01:54:05.440 --> 01:54:06.440]   No doubt about it.
[01:54:06.440 --> 01:54:07.440]   Good piece of it.
[01:54:07.440 --> 01:54:12.040]   He's very rich, but $44 billion is nothing to sneeze at.
[01:54:12.040 --> 01:54:13.480]   No, and I think you're right.
[01:54:13.480 --> 01:54:20.280]   I think that having to sell that much stock, Tesla stock, or a significant portion of tech,
[01:54:20.280 --> 01:54:24.120]   you know, $20 billion in Tesla stock would tank the stock.
[01:54:24.120 --> 01:54:26.200]   And that's a really interesting...
[01:54:26.200 --> 01:54:28.640]   And Tesla shareholders hate this already.
[01:54:28.640 --> 01:54:29.640]   Oh, yeah.
[01:54:29.640 --> 01:54:32.160]   And it's not exactly like using the middle of the bull market where the market will just
[01:54:32.160 --> 01:54:33.240]   pick it right back up.
[01:54:33.240 --> 01:54:34.240]   Right.
[01:54:34.240 --> 01:54:35.240]   Market's unforgiving right now.
[01:54:35.240 --> 01:54:36.240]   Yeah.
[01:54:36.240 --> 01:54:41.440]   Better working that humanoid robot pretty darn hard, Elon.
[01:54:41.440 --> 01:54:47.440]   Oh, well, the market opportunities for a humanoid robot that can't do much are really,
[01:54:47.440 --> 01:54:48.440]   you know.
[01:54:48.440 --> 01:54:51.680]   It was not already selling the Astro.
[01:54:51.680 --> 01:54:59.800]   I mean, you combine that with the flamethrowers and the next thing you know.
[01:54:59.800 --> 01:55:03.520]   And putting holes in the ground for your Teslas to drive around.
[01:55:03.520 --> 01:55:08.600]   And somehow defeating geometry with the power of your mind so that you can add more cars
[01:55:08.600 --> 01:55:11.720]   to the roads without creating more congestion.
[01:55:11.720 --> 01:55:15.080]   You got to make the robot buy Twitter.
[01:55:15.080 --> 01:55:16.080]   That's the only solution.
[01:55:16.080 --> 01:55:17.080]   Oh, there you go.
[01:55:17.080 --> 01:55:22.080]   Well, do you remember that there was that Mesothelioma blog that it would just take stories from
[01:55:22.080 --> 01:55:27.840]   Google news about Mesothelioma and then put them on a blog spot blog with AdSense and
[01:55:27.840 --> 01:55:31.080]   then use the revenue from that to buy Google stock.
[01:55:31.080 --> 01:55:35.360]   And the idea was that if you ran it long enough, you would eventually own Google.
[01:55:35.360 --> 01:55:37.720]   It's like a paperclip AI.
[01:55:37.720 --> 01:55:38.720]   Yeah.
[01:55:38.720 --> 01:55:41.200]   Eventually you'll own the universe.
[01:55:41.200 --> 01:55:43.160]   Don't I stop at Google?
[01:55:43.160 --> 01:55:45.680]   I want to say Matt Howie from Metafilter built.
[01:55:45.680 --> 01:55:46.680]   Sounds like a man.
[01:55:46.680 --> 01:55:47.680]   How a joint.
[01:55:47.680 --> 01:55:48.680]   Absolutely.
[01:55:48.680 --> 01:55:50.240]   Absolutely.
[01:55:50.240 --> 01:55:51.240]   And how did it do?
[01:55:51.240 --> 01:55:53.240]   Did it run or was it a thought experiment?
[01:55:53.240 --> 01:55:54.240]   It made a bunch of money.
[01:55:54.240 --> 01:55:58.040]   No, for a while, Mesothelioma was the top AdSense word on Google.
[01:55:58.040 --> 01:55:59.040]   Right.
[01:55:59.040 --> 01:56:00.640]   Every one of those clicks was worth, I don't know, like 20 bucks or something.
[01:56:00.640 --> 01:56:08.720]   That's the asbestos lung disease that you see late night at TV ads for all the time.
[01:56:08.720 --> 01:56:14.280]   What about, what do you think about the Journalism Competition and Preservation Act?
[01:56:14.280 --> 01:56:23.280]   Amy Klobuchar's bill, which was initially blocked by Ted Cruz, Cruz last week, changed
[01:56:23.280 --> 01:56:25.920]   his mind and allowed it through.
[01:56:25.920 --> 01:56:28.760]   And it has advanced at a committee with a 15 to seven vote.
[01:56:28.760 --> 01:56:34.000]   The seven were Republicans voting against it, but there were enough Republicans voting
[01:56:34.000 --> 01:56:36.560]   for it that it went through.
[01:56:36.560 --> 01:56:41.080]   Ted Cruz says, "I think this amendment protects against this anti-trust liability being used
[01:56:41.080 --> 01:56:42.840]   as a shield for censorship.
[01:56:42.840 --> 01:56:45.120]   Big tech hates this bill.
[01:56:45.120 --> 01:56:49.200]   That to me is a strong positive for supporting it.
[01:56:49.200 --> 01:56:55.000]   There are a number of problems with this bill, including this thing called the First Amendment
[01:56:55.000 --> 01:56:58.440]   to the Constitution.
[01:56:58.440 --> 01:57:02.840]   I'm not sure if we'll ever get to a vote on the Senate floor, but it is at a committee.
[01:57:02.840 --> 01:57:04.600]   Should I be worried, Alex?
[01:57:04.600 --> 01:57:07.360]   Because I could see a lot of problems with this thing."
[01:57:07.360 --> 01:57:09.480]   I don't think you should be worried.
[01:57:09.480 --> 01:57:14.720]   I like where this bill comes from, which is that publishers don't have the ability to
[01:57:14.720 --> 01:57:17.800]   negotiate with platforms in a collective way.
[01:57:17.800 --> 01:57:20.920]   And the platforms have this disproportionate influence over that.
[01:57:20.920 --> 01:57:22.200]   Corey's been talking about that.
[01:57:22.200 --> 01:57:24.840]   That's in the book.
[01:57:24.840 --> 01:57:30.520]   You're enjoined by law not to collude, so they have no way to negotiate.
[01:57:30.520 --> 01:57:38.520]   This gives them essentially a safe harbor so that they can get together and negotiate
[01:57:38.520 --> 01:57:40.480]   with Google and Facebook.
[01:57:40.480 --> 01:57:43.800]   Now, here's my—I'm going to go out on a limb here also.
[01:57:43.800 --> 01:57:47.800]   I don't know what's in the water this week, but hey, let's go for it.
[01:57:47.800 --> 01:57:48.800]   Do it, man.
[01:57:48.800 --> 01:57:53.720]   I think that publisher is trying to make their livelihood over negotiating with Facebook
[01:57:53.720 --> 01:57:55.960]   and Google or playing losing game.
[01:57:55.960 --> 01:57:56.960]   Yes, I agree.
[01:57:56.960 --> 01:58:04.760]   I don't think you can depend on a platform for your distribution.
[01:58:04.760 --> 01:58:09.160]   I don't think you should wonder about a platform like Google that's sending you traffic.
[01:58:09.160 --> 01:58:10.920]   How much they should pay you for displaying the link.
[01:58:10.920 --> 01:58:12.720]   I think that's a net benefit.
[01:58:12.720 --> 01:58:13.720]   It's a publisher.
[01:58:13.720 --> 01:58:19.480]   I run a small media company and we don't depend at all on Facebook traffic or Google
[01:58:19.480 --> 01:58:20.480]   traffic.
[01:58:20.480 --> 01:58:24.560]   I think it's better that way because we make the decisions for the reader, not for the
[01:58:24.560 --> 01:58:25.560]   platform.
[01:58:25.560 --> 01:58:31.760]   Ultimately, I think that having this right to negotiate is good, but do I think that
[01:58:31.760 --> 01:58:33.920]   it's an earth-shattering thing?
[01:58:33.920 --> 01:58:38.960]   No, especially given that Facebook has made a real effort to reduce news links in the
[01:58:38.960 --> 01:58:44.920]   newsfeed where they used to be a big portion of what was going on on Facebook.
[01:58:44.920 --> 01:58:47.160]   Now they're getting close to non-existent.
[01:58:47.160 --> 01:58:48.160]   I think that's largely good.
[01:58:48.160 --> 01:58:50.160]   I think we should get our news through Facebook.
[01:58:50.160 --> 01:58:51.680]   I think they should find other ways to do it.
[01:58:51.680 --> 01:58:52.680]   There is the argument.
[01:58:52.680 --> 01:58:57.400]   This is based on the Australian bill which was promoted by Rupert Murdoch, who wanted
[01:58:57.400 --> 01:59:02.440]   to get a little more link money out of the big giants.
[01:59:02.440 --> 01:59:07.040]   There's the argument that it's somewhat worked in Australia.
[01:59:07.040 --> 01:59:11.240]   Despite Facebook's retaliatory attempt for a while, Facebook said, "Well, no more news
[01:59:11.240 --> 01:59:13.360]   links for you."
[01:59:13.360 --> 01:59:21.000]   There's also the concern, though, that it doesn't apply to anybody who employs more than 1,500
[01:59:21.000 --> 01:59:22.000]   people.
[01:59:22.000 --> 01:59:29.080]   There's some concern that in order to make this work, private equity will go around buying
[01:59:29.080 --> 01:59:32.240]   newsrooms and cut it down to 1,499 people.
[01:59:32.240 --> 01:59:37.040]   They can participate in wild imaginations.
[01:59:37.040 --> 01:59:40.400]   It's not going to be the main part of a company's business, the idea that private equity will
[01:59:40.400 --> 01:59:41.880]   go and trim.
[01:59:41.880 --> 01:59:46.160]   Private equity is in terms for its own reasons, but not to take advantage of the protections
[01:59:46.160 --> 01:59:47.160]   under this bill.
[01:59:47.160 --> 01:59:51.160]   Corey, you've pointed out that journalism has definitely suffered.
[01:59:51.160 --> 01:59:58.400]   Actually, you have a whole chapter talking about Craigslist versus Google links, and
[01:59:58.400 --> 02:00:04.200]   which is the most damaging to news.
[02:00:04.200 --> 02:00:13.040]   Yeah, I think that the problem with this solution is that it misunderstands what it is technology
[02:00:13.040 --> 02:00:14.600]   did to news.
[02:00:14.600 --> 02:00:21.400]   It's definitely true that Craigslist made a more efficient way of doing classified advertising
[02:00:21.400 --> 02:00:23.000]   than the newspapers had.
[02:00:23.000 --> 02:00:26.040]   There's a reason that Craigslist was better for classified advertising.
[02:00:26.040 --> 02:00:28.360]   It wasn't just a different cost basis.
[02:00:28.360 --> 02:00:34.200]   It's that in the run up to the Craigslist era, the web, I don't know if he's 1.5, web 1.5,
[02:00:34.200 --> 02:00:39.720]   something like that, in the run up to that, there was a series of media rollups, right?
[02:00:39.720 --> 02:00:46.400]   After the Telecommunications Act and the Clinton years that allowed radio stations and TV stations
[02:00:46.400 --> 02:00:50.560]   and newspapers in a single market to all come under a common ownership and also for there
[02:00:50.560 --> 02:00:52.720]   to be more cross market ownership.
[02:00:52.720 --> 02:00:59.680]   You saw lots and lots of regional local newspapers coming under a single owner, large corporate
[02:00:59.680 --> 02:01:05.040]   owners as opposed to the historic basis for news, which was outside of the big cities.
[02:01:05.040 --> 02:01:11.120]   The historic basis for news was you had a patrician family who owned the newspaper, who
[02:01:11.120 --> 02:01:18.440]   mostly ran it as a business to allow appliance retailers and grocers to reach people who are
[02:01:18.440 --> 02:01:22.160]   interested in the sports scores.
[02:01:22.160 --> 02:01:26.280]   In between, some of that money was peeled off to send a college kid to the town meeting
[02:01:26.280 --> 02:01:31.040]   to write down what people were saying and whatever controversy there was.
[02:01:31.040 --> 02:01:38.320]   Those papers were mostly rolled up into these big corporate national organizations.
[02:01:38.320 --> 02:01:44.040]   One of the ways that the new owners tried to justify those rollups was in part by trimming.
[02:01:44.040 --> 02:01:48.640]   They trimmed a lot of the regional sales force, and so the local shoe leather sales force
[02:01:48.640 --> 02:01:53.000]   who knew how to sell classified ads to local merchants were replaced by centralized call
[02:01:53.000 --> 02:01:58.160]   rooms where you would just call Chicago or New York or whatever or somewhere in the Midwest
[02:01:58.160 --> 02:02:02.200]   when you wanted to place an ad in the newspaper down the road.
[02:02:02.200 --> 02:02:08.040]   Another way that they realized new efficiencies was by selling off buildings, physical plant
[02:02:08.040 --> 02:02:13.040]   and outsourcing core functions, which exposed them to a bunch of shocks, rent shocks and
[02:02:13.040 --> 02:02:17.600]   other shocks, interest rate shocks and so on, where suddenly when things got bad, it got
[02:02:17.600 --> 02:02:21.840]   worse because when things got bad, suddenly the rent might go up or the cost of leasing
[02:02:21.840 --> 02:02:26.120]   their presses might go up and then they would be really exposed.
[02:02:26.120 --> 02:02:30.080]   You have this industry that had already weathered so many technological shocks.
[02:02:30.080 --> 02:02:37.440]   The newspapers survived the telegraph, radio, the television, cable, satellite, and suddenly
[02:02:37.440 --> 02:02:44.240]   they were uniquely vulnerable to Craig Newmark, who is a lovely guy and very smart and who
[02:02:44.240 --> 02:02:49.760]   did something really cool but was not, I think, intrinsically more disruptive to their business
[02:02:49.760 --> 02:02:51.920]   than cable television.
[02:02:51.920 --> 02:02:55.800]   The reason was that they had made themselves vulnerable.
[02:02:55.800 --> 02:03:00.760]   That's one of the things that's hurt the newspapers is this combination of consolidated
[02:03:00.760 --> 02:03:03.480]   ownership and changes in the technology.
[02:03:03.480 --> 02:03:07.280]   The other thing that's really hurt them and that is not addressed in this bill at all
[02:03:07.280 --> 02:03:09.480]   is fraud in the ad markets.
[02:03:09.480 --> 02:03:15.480]   The ad duopoly, Facebook and Google, have now there's a pretty strong evidentry basis
[02:03:15.480 --> 02:03:17.960]   to say that they steal from publishers.
[02:03:17.960 --> 02:03:23.800]   The Operation Jedi Blue, Project Jedi Blue, which was disclosed in the Texas AG case
[02:03:23.800 --> 02:03:28.840]   against Facebook, shows that the senior management team of Facebook and Google sat down and
[02:03:28.840 --> 02:03:34.120]   illegally colluded to rig the ad market so that publishers would get less and advertisers
[02:03:34.120 --> 02:03:35.800]   would pay more.
[02:03:35.800 --> 02:03:40.240]   You add to that other forms of fraud like the Pivot to Video, which was based on lies
[02:03:40.240 --> 02:03:45.000]   about how many people were watching videos, which cost the newsrooms of the country and
[02:03:45.000 --> 02:03:49.360]   around the world billions in aggregate and made them even more vulnerable.
[02:03:49.360 --> 02:03:56.000]   You have this thing where you have tech platforms that are stealing money from news organizations
[02:03:56.000 --> 02:04:00.200]   and we're acting like the problem is that they're stealing content from news organizations
[02:04:00.200 --> 02:04:04.840]   and allowing your users to talk about the news or providing links to the news so people
[02:04:04.840 --> 02:04:07.520]   can talk about it is not stealing the news.
[02:04:07.520 --> 02:04:09.640]   If it's a secret, it's not the news.
[02:04:09.640 --> 02:04:12.880]   The thing that it makes it the news is that we talk about it.
[02:04:12.880 --> 02:04:17.480]   But stealing your ad money is an actual problem that we can put our fingers on and that we
[02:04:17.480 --> 02:04:19.000]   can actually put our hands around.
[02:04:19.000 --> 02:04:21.760]   We could say, "All right, we're going to have transparency rights.
[02:04:21.760 --> 02:04:25.560]   We're going to follow the model of surveillance oxling, create individual criminal liability
[02:04:25.560 --> 02:04:32.120]   for executives who knowingly sign or produce false reports on financials."
[02:04:32.120 --> 02:04:35.800]   Things that actually address themselves to the problem instead of trying to take something
[02:04:35.800 --> 02:04:41.320]   off to one side, which is creating a stream of payments based on the bizarre idea that
[02:04:41.320 --> 02:04:45.760]   you should pay to link to the news or let your users talk about the news instead of unrolling
[02:04:45.760 --> 02:04:51.440]   the fraud, which is a thing that would benefit all kinds of creators, including newsrooms,
[02:04:51.440 --> 02:04:53.360]   but also individuals and so on.
[02:04:53.360 --> 02:04:54.360]   Agreed.
[02:04:54.360 --> 02:04:58.960]   And in the meantime, I would say publishers should not do business with these companies,
[02:04:58.960 --> 02:05:05.080]   do whatever you can to stay away from them and be immune to their ability.
[02:05:05.080 --> 02:05:06.080]   And there's ways to do that.
[02:05:06.080 --> 02:05:08.080]   They own the stack, right?
[02:05:08.080 --> 02:05:09.080]   They own like you.
[02:05:09.080 --> 02:05:10.720]   It's very hard not to do business with them.
[02:05:10.720 --> 02:05:11.720]   I disagree with you.
[02:05:11.720 --> 02:05:12.720]   I disagree.
[02:05:12.720 --> 02:05:16.000]   I think that there's independent ad tech that you can use out there.
[02:05:16.000 --> 02:05:17.360]   There's also subscriptions.
[02:05:17.360 --> 02:05:21.720]   And I think podcasts are also another revenue source that you don't need to go through
[02:05:21.720 --> 02:05:22.720]   them.
[02:05:22.720 --> 02:05:29.360]   I have a great story about fraud in the ad world, and it has to do with podcasts.
[02:05:29.360 --> 02:05:35.000]   And this is this story that's going to get me fired from my employer, the fabulous
[02:05:35.000 --> 02:05:36.560]   iHeart Media Corporation.
[02:05:36.560 --> 02:05:44.520]   According to Bloomberg, podcasters have been buying ads in mobile games that when you click
[02:05:44.520 --> 02:05:51.680]   on the ad, downloads the podcast every time a player taps on one of these fleeting in-game
[02:05:51.680 --> 02:05:57.160]   ads and you get some virtual loot for doing it, a podcast episode in the background begins
[02:05:57.160 --> 02:06:04.360]   downloading, which means the podcast company can claim the gamer as a listener and add a
[02:06:04.360 --> 02:06:08.120]   download to its overall tally.
[02:06:08.120 --> 02:06:12.920]   You might think, well, how big a deal can that be?
[02:06:12.920 --> 02:06:15.160]   Obviously, those people are not listening.
[02:06:15.160 --> 02:06:20.400]   They don't even know in many most cases that they downloaded the podcast.
[02:06:20.400 --> 02:06:24.480]   It just goes out into the, as Kerry explained to me, goes into the ether.
[02:06:24.480 --> 02:06:30.800]   It doesn't, the bit bucket doesn't even get saved onto your phone.
[02:06:30.800 --> 02:06:38.760]   One game referenced in this paper, this study came from a company called Deep C S E E Bloomberg's
[02:06:38.760 --> 02:06:41.000]   writing about it.
[02:06:41.000 --> 02:06:46.760]   One company, a popular mobile app from SIBO is called Subway Surfers.
[02:06:46.760 --> 02:06:51.680]   If you played it, download it three billion times since it came out 10 years ago.
[02:06:51.680 --> 02:06:56.680]   Over a period of two weeks in August, Bloomberg found multiple publishers using the game to
[02:06:56.680 --> 02:07:08.600]   rack up podcast downloads, including, I'm sad to say, iHeart Media, which is my player
[02:07:08.600 --> 02:07:10.920]   for the radio show.
[02:07:10.920 --> 02:07:19.200]   iHeart Media apparently was one of the number one users of this, and they shelled out more
[02:07:19.200 --> 02:07:23.840]   than $10 million gain to 6 million unique listeners a month.
[02:07:23.840 --> 02:07:27.840]   And they've been doing it since 2018.
[02:07:27.840 --> 02:07:30.680]   Always, you know, it's funny because we don't lie about our numbers.
[02:07:30.680 --> 02:07:34.880]   In fact, if anything, we're conservative about our numbers.
[02:07:34.880 --> 02:07:41.120]   And I always was puzzled when I see numbers from some of these big podcast companies like,
[02:07:41.120 --> 02:07:45.120]   really, you get 40 million downloads a month's release.
[02:07:45.120 --> 02:07:49.760]   Well now we know they don't.
[02:07:49.760 --> 02:07:56.040]   This is just one more example of the of the click fraud that you were talking about, Corey.
[02:07:56.040 --> 02:08:01.320]   Yeah, I mean, apps because they're so opaque are a natural environment for this stuff.
[02:08:01.320 --> 02:08:06.200]   Yeah, you do have to get through the app store heuristics and analytics.
[02:08:06.200 --> 02:08:12.360]   But if you can smuggle something through the platforms by design, don't let users closely
[02:08:12.360 --> 02:08:14.640]   monitor how the apps themselves are working.
[02:08:14.640 --> 02:08:20.080]   Like, you know, it's Apple famously sued a company that made a VM that you could run
[02:08:20.080 --> 02:08:25.920]   iOS apps and that would allow you to do like really deep forensics on it.
[02:08:25.920 --> 02:08:31.680]   Because they don't want you to think of this as software that you can like stick your own
[02:08:31.680 --> 02:08:32.680]   controls on.
[02:08:32.680 --> 02:08:36.880]   I saw you had on our rundown for today, something a little later on about the OG app, which is
[02:08:36.880 --> 02:08:42.880]   I understand it is a very similar kind of thing to in terms of allowing users to gain
[02:08:42.880 --> 02:08:49.400]   more control in that it kind of acts as an overlay to your social media and then loads
[02:08:49.400 --> 02:08:53.960]   the feed that the social media company wants to send you, but throws away the ads.
[02:08:53.960 --> 02:08:58.760]   And you know, this is Apple chucked it out of the app store and this is the point, right?
[02:08:58.760 --> 02:09:03.200]   It's to be able to exercise control so that when your interest diverge from the app store's
[02:09:03.200 --> 02:09:09.000]   interest, their interest comes first and that produces this space in which all this mischief
[02:09:09.000 --> 02:09:14.200]   can take place because as soon as you design computers to treat their owners as adversaries,
[02:09:14.200 --> 02:09:15.200]   right?
[02:09:15.200 --> 02:09:20.920]   Adversaries the manufacturer, then you make design decisions that, you know, necessarily
[02:09:20.920 --> 02:09:22.840]   increase their opacity.
[02:09:22.840 --> 02:09:27.280]   There's a lot of the reasons OG app exists because there's a lot of dissatisfaction with
[02:09:27.280 --> 02:09:32.240]   what meta has decided to do with Instagram, which is essentially take it from a very lovely
[02:09:32.240 --> 02:09:37.560]   site where you could share photos with your friends and family and turn it into TikTok
[02:09:37.560 --> 02:09:39.720]   because that's the flavor of the month.
[02:09:39.720 --> 02:09:42.000]   The OG app eliminated ads.
[02:09:42.000 --> 02:09:47.040]   That was probably part of the problem from meta's point of view, but also eliminated everything,
[02:09:47.040 --> 02:09:49.960]   all the algorithmic recommendations turned it.
[02:09:49.960 --> 02:09:52.960]   Frankly, I loved it.
[02:09:52.960 --> 02:09:55.920]   It turned my Instagram back into my Instagram like it used to be.
[02:09:55.920 --> 02:09:57.480]   It immediately caused problems.
[02:09:57.480 --> 02:10:02.680]   If one of the first things Instagram did after I installed the OG app, I recommended it on
[02:10:02.680 --> 02:10:04.000]   Macbreak Weekly on Tuesday.
[02:10:04.000 --> 02:10:05.720]   It's gone today, by the way.
[02:10:05.720 --> 02:10:11.880]   One of the reasons I recommended it and liked it was because it gave me my traditional Instagram,
[02:10:11.880 --> 02:10:16.680]   but as soon as I went back to Instagram, Instagram said, well, there's been a security event in
[02:10:16.680 --> 02:10:18.640]   your Instagram app.
[02:10:18.640 --> 02:10:20.680]   These you have to re-authenticate.
[02:10:20.680 --> 02:10:22.680]   We got to make sure you are you.
[02:10:22.680 --> 02:10:23.680]   Okay, fine.
[02:10:23.680 --> 02:10:25.880]   But that happened every time I use the OG app.
[02:10:25.880 --> 02:10:31.040]   And then, of course, they somehow, somebody got Apple to pull it from the app store, so
[02:10:31.040 --> 02:10:32.040]   it's gone.
[02:10:32.040 --> 02:10:35.160]   I don't know if it's, I think it's still on the Android.
[02:10:35.160 --> 02:10:36.160]   Yeah.
[02:10:36.160 --> 02:10:40.920]   I mean, Apple is a good proxy for defending your interest when they're co-terminal with
[02:10:40.920 --> 02:10:43.200]   Apple's interests.
[02:10:43.200 --> 02:10:47.840]   And they do have enormous resources and very skilled personnel who do that.
[02:10:47.840 --> 02:10:53.400]   But when your interests diverge from theirs, and this is not unique to Apple, this is true
[02:10:53.400 --> 02:10:54.760]   of all the big firms.
[02:10:54.760 --> 02:10:58.400]   Facebook has an incredible security team that defends you from all kinds of threat
[02:10:58.400 --> 02:10:59.400]   actors.
[02:10:59.400 --> 02:11:02.800]   The one threat actor that they want to defend you from is Facebook.
[02:11:02.800 --> 02:11:04.120]   And Apple is the same.
[02:11:04.120 --> 02:11:09.760]   So if you're a Chinese iOS user, and Apple has decided that access to Chinese consumers
[02:11:09.760 --> 02:11:15.600]   and Chinese manufacturing is more important than the integrity of Chinese users, you have
[02:11:15.600 --> 02:11:19.720]   no recourse when they remove all the working VPNs from the app store and add a backdoor
[02:11:19.720 --> 02:11:24.040]   to their cloud servers for the Chinese state to use.
[02:11:24.040 --> 02:11:27.960]   Because by design, you can't modify and intervene.
[02:11:27.960 --> 02:11:33.800]   One of the things that I think we need to understand is that the outer periphery of how
[02:11:33.800 --> 02:11:39.320]   badly a firm can treat you as historically, and I think is still determined by what you
[02:11:39.320 --> 02:11:41.120]   can do if you're dissatisfied.
[02:11:41.120 --> 02:11:44.800]   Yeah, you can just leave and there's someone in the chat saying, if you don't like Apple,
[02:11:44.800 --> 02:11:46.000]   you should just leave.
[02:11:46.000 --> 02:11:48.760]   Well leaving Apple causes a switching cost, right?
[02:11:48.760 --> 02:11:52.000]   There's the technological cost of adjusting to something new.
[02:11:52.000 --> 02:11:55.440]   There's throwing away the media that is specific to your Apple device.
[02:11:55.440 --> 02:12:02.160]   There's other intangible problems like losing the ability to do rich IM sessions with your
[02:12:02.160 --> 02:12:03.640]   fellow Apple users.
[02:12:03.640 --> 02:12:07.720]   You remember Tim Cook recently said, you know, if you want to share videos and pictures with
[02:12:07.720 --> 02:12:10.520]   your mom, you should just buy her an iPhone.
[02:12:10.520 --> 02:12:17.520]   So the corollary of that is if you switch away from iOS, then you can say goodbye to doing
[02:12:17.520 --> 02:12:19.360]   that kind of messaging with your mom.
[02:12:19.360 --> 02:12:24.600]   So all of those switching costs have to be weighed against the benefit that you get from
[02:12:24.600 --> 02:12:25.880]   going somewhere else.
[02:12:25.880 --> 02:12:28.200]   And the firms understand this very intimately.
[02:12:28.200 --> 02:12:34.320]   Again, in the Texas AG case against Facebook, one of the documents that was released are
[02:12:34.320 --> 02:12:40.000]   very frank memos between product design teams who are saying, we are going to design, for
[02:12:40.000 --> 02:12:48.400]   example, Facebook photos such that it is very good to use not because we think that is something
[02:12:48.400 --> 02:12:53.380]   people will value, but because we think it will lure people into adding their family photos
[02:12:53.380 --> 02:12:54.680]   to Facebook.
[02:12:54.680 --> 02:12:59.760]   And once they do, they will endure the high switching cost of leaving behind their cherished
[02:12:59.760 --> 02:13:04.120]   family photos if they quit Facebook and go to Google+, which is the rival they were worried
[02:13:04.120 --> 02:13:05.920]   about at the time.
[02:13:05.920 --> 02:13:10.480]   And so firms very deliberately add very high switching costs to their products.
[02:13:10.480 --> 02:13:17.600]   And one of the things that these interoperable technologies do, like OGE app and VMs and all
[02:13:17.600 --> 02:13:22.640]   these other things that people build that are part of the long history of how technology
[02:13:22.640 --> 02:13:27.640]   companies, including Apple and Facebook, have confronted their own competitive challenges
[02:13:27.640 --> 02:13:32.560]   is they allow users to have an intermediate state between leaving the firm altogether,
[02:13:32.560 --> 02:13:36.600]   leaving the service altogether and enduring those high switching costs and enduring whatever
[02:13:36.600 --> 02:13:40.320]   ration of crap the firm wants to shovel down their neck, right?
[02:13:40.320 --> 02:13:42.200]   Add blockers are a great example.
[02:13:42.200 --> 02:13:44.560]   Pop-up blockers are really good example.
[02:13:44.560 --> 02:13:46.560]   You know, pop-ups were once everywhere.
[02:13:46.560 --> 02:13:48.640]   The browsers added them by default, right?
[02:13:48.640 --> 02:13:52.640]   They were doing adversarial interoperability with the publishers whose content was being
[02:13:52.640 --> 02:13:54.760]   loaded in the browser.
[02:13:54.760 --> 02:13:58.560]   Publishers stop displaying pop-ups because advertisers stopped asking for them because
[02:13:58.560 --> 02:14:00.160]   users block them.
[02:14:00.160 --> 02:14:04.440]   What is one of the mechanisms by which we make technology better is by giving users control
[02:14:04.440 --> 02:14:10.400]   over their technological destiny so that firms own behavior is limited because when they
[02:14:10.400 --> 02:14:13.680]   act badly, the users just take a corrective.
[02:14:13.680 --> 02:14:19.120]   You called, I remember, add blockers, the largest consumer boycott in history.
[02:14:19.120 --> 02:14:20.120]   That's Doc Serals.
[02:14:20.120 --> 02:14:21.120]   Yeah, that's me.
[02:14:21.120 --> 02:14:22.120]   Doc said that.
[02:14:22.120 --> 02:14:23.120]   Oh, good.
[02:14:23.120 --> 02:14:24.120]   All right.
[02:14:24.120 --> 02:14:25.120]   Yeah.
[02:14:25.120 --> 02:14:26.120]   Yeah.
[02:14:26.120 --> 02:14:27.120]   Yeah.
[02:14:27.120 --> 02:14:32.200]   We're going to talk about Google doing what it can to make ad blockers no longer work at
[02:14:32.200 --> 02:14:33.200]   all.
[02:14:33.200 --> 02:14:42.680]   Alex, before I take the break, anything you want to add to the last few minutes of conversation?
[02:14:42.680 --> 02:14:43.680]   Thanks for the opening.
[02:14:43.680 --> 02:14:49.560]   I would say that if we're thinking about everybody just on the podcast tab, I thought
[02:14:49.560 --> 02:14:51.160]   the podcast scam is really interesting.
[02:14:51.160 --> 02:14:52.160]   Isn't that wild?
[02:14:52.160 --> 02:14:53.160]   Oh, it makes me mad news.
[02:14:53.160 --> 02:14:56.800]   It makes me so mad because it hurts us.
[02:14:56.800 --> 02:14:58.400]   You know, it really hurts us badly.
[02:14:58.400 --> 02:14:59.400]   Absolutely.
[02:14:59.400 --> 02:15:01.040]   We're doing the same thing or?
[02:15:01.040 --> 02:15:02.040]   Yeah.
[02:15:02.040 --> 02:15:03.040]   Yeah.
[02:15:03.040 --> 02:15:07.560]   The bad news is obviously like when you try to build a podcast and authentic real way,
[02:15:07.560 --> 02:15:10.520]   like you and I are doing, you know, you got to build a brick by brick.
[02:15:10.520 --> 02:15:11.680]   It's tough to build them.
[02:15:11.680 --> 02:15:14.240]   And so yeah, it does screw us.
[02:15:14.240 --> 02:15:18.240]   The good news, if you want to look at a silver lining, is that podcast ads work?
[02:15:18.240 --> 02:15:19.240]   Yes.
[02:15:19.240 --> 02:15:21.120]   They're very valuable business rules.
[02:15:21.120 --> 02:15:22.120]   Yes.
[02:15:22.120 --> 02:15:24.160]   So that's the positive note I'll give us on.
[02:15:24.160 --> 02:15:25.840]   I agree with you.
[02:15:25.840 --> 02:15:27.320]   Are you not a good tech winner?
[02:15:27.320 --> 02:15:28.320]   Yeah.
[02:15:28.320 --> 02:15:29.320]   Totally.
[02:15:29.320 --> 02:15:30.320]   I'm supported.
[02:15:30.320 --> 02:15:31.320]   Yeah.
[02:15:31.320 --> 02:15:34.280]   So I don't know what your experience has been, but lately it's been tougher and tougher
[02:15:34.280 --> 02:15:39.920]   because companies, mostly it's agencies, not individual companies, but they want ad tech
[02:15:39.920 --> 02:15:41.600]   and they'll raise the spectrum.
[02:15:41.600 --> 02:15:47.040]   Well, I can get all this information from Spotify or iHeart or they'll say to us, well,
[02:15:47.040 --> 02:15:51.360]   look, iHeart's CPMs, their cost for a thousand listeners, so much lower.
[02:15:51.360 --> 02:15:55.120]   Yeah, because eight million of those are fake.
[02:15:55.120 --> 02:16:03.240]   It becomes hard to compete with that kind of fraud and/or co-opting of the market.
[02:16:03.240 --> 02:16:10.000]   And I think honestly, independent podcasting like you and I do is really going to be facing
[02:16:10.000 --> 02:16:11.840]   an uphill battle over the next few years.
[02:16:11.840 --> 02:16:16.360]   But on the other hand, yeah, I think that we renew more easily because people see the
[02:16:16.360 --> 02:16:21.760]   stuff works and they know it's not fraud and they're more inclined to spend again.
[02:16:21.760 --> 02:16:22.760]   Yeah.
[02:16:22.760 --> 02:16:23.760]   That's good.
[02:16:23.760 --> 02:16:27.000]   I mean, if you're running a business, what you want to do, you want to deliver for customers,
[02:16:27.000 --> 02:16:29.080]   create loyalty and produce a good product.
[02:16:29.080 --> 02:16:30.080]   Yeah.
[02:16:30.080 --> 02:16:33.760]   I think that you can do that in an upstanding way in this industry and it's worth a heck
[02:16:33.760 --> 02:16:38.280]   of a lot to advertisers, which is why you see people wanting to mimic it.
[02:16:38.280 --> 02:16:39.280]   Yeah.
[02:16:39.280 --> 02:16:44.560]   So they should stop, but the headline for me is, you know, it's great news about podcast
[02:16:44.560 --> 02:16:45.560]   ads.
[02:16:45.560 --> 02:16:49.920]   Yeah, I want to get the word out to advertisers because if, you know, they get, they get, oh,
[02:16:49.920 --> 02:16:56.080]   it's cheap to buy iHeart, but then what they don't get is return because they're fake numbers.
[02:16:56.080 --> 02:17:02.520]   Unfortunately, the, the, sometimes the reaction is, well, I guess podcasting doesn't work.
[02:17:02.520 --> 02:17:05.240]   Let's put our ad dollars back in NFL football, right?
[02:17:05.240 --> 02:17:06.240]   Right.
[02:17:06.240 --> 02:17:09.920]   But clearly there's enough interest in podcasting that I hope people are going to these great
[02:17:09.920 --> 02:17:11.440]   links in order to fulfill it.
[02:17:11.440 --> 02:17:15.880]   So look, I think that there's, I still believe that podcasting is the, you know, more than
[02:17:15.880 --> 02:17:19.480]   radio, more than television, more than print.
[02:17:19.480 --> 02:17:23.760]   You know, I think it's the most intimate form of media.
[02:17:23.760 --> 02:17:27.840]   You're there with a person, you know, in my case for an hour a week, in your case for
[02:17:27.840 --> 02:17:30.400]   many hours a week and you're there with them on their walls.
[02:17:30.400 --> 02:17:31.400]   People get tired of me.
[02:17:31.400 --> 02:17:32.400]   I could tell you.
[02:17:32.400 --> 02:17:33.400]   No, no, no, no.
[02:17:33.400 --> 02:17:36.120]   But you're there with them on your, I don't, I don't think so.
[02:17:36.120 --> 02:17:40.160]   Every time I'm with you, man, I hear you know what I would be thinking about to be kind
[02:17:40.160 --> 02:17:44.080]   of fun is, um, I was talking about this with Micah.
[02:17:44.080 --> 02:17:49.160]   I haven't told my wife yet, but, uh, be kind of interesting to just make it more like a
[02:17:49.160 --> 02:17:53.640]   radio station where you just have hosts come on for two or three hours and you just, you're
[02:17:53.640 --> 02:17:57.720]   always on like 24 seven always that.
[02:17:57.720 --> 02:18:01.600]   Wouldn't that be interesting kind of like a live, that's a continuous live stream.
[02:18:01.600 --> 02:18:07.080]   That was my first, I started out, um, my first real media gig was working, uh, a sports
[02:18:07.080 --> 02:18:09.560]   show at a NASA community college radio.
[02:18:09.560 --> 02:18:13.800]   Um, before I was in college, actually, I was in high school and I wrote to these guys.
[02:18:13.800 --> 02:18:15.040]   And I was a blast.
[02:18:15.040 --> 02:18:16.040]   It's amazing.
[02:18:16.040 --> 02:18:17.240]   So I, I recommend it.
[02:18:17.240 --> 02:18:18.240]   Yeah.
[02:18:18.240 --> 02:18:19.400]   That's just a thought we have.
[02:18:19.400 --> 02:18:21.640]   That's one of the reasons we've got our fabulous club twit.
[02:18:21.640 --> 02:18:27.720]   Uh, I want to give it a little bit of a plug because it makes a big difference to our bottom
[02:18:27.720 --> 02:18:28.720]   line.
[02:18:28.720 --> 02:18:31.800]   In fact, right now it's about 25% of our, uh, total revenue.
[02:18:31.800 --> 02:18:35.720]   And that helps a lot as ads start to get harder and harder to sell.
[02:18:35.720 --> 02:18:41.040]   If I think if there is a longterm future for us, it's through something like club twit.
[02:18:41.040 --> 02:18:45.560]   Now here's what you get, seven bucks a month, which I think is a pretty affordable dollar
[02:18:45.560 --> 02:18:46.560]   figure.
[02:18:46.560 --> 02:18:51.440]   You get all ad free versions of our shows, all the shows ad free.
[02:18:51.440 --> 02:18:55.880]   You also get access to the club twit discord, which a lot of people are saying that's really
[02:18:55.880 --> 02:18:57.560]   the benefit we like.
[02:18:57.560 --> 02:19:01.160]   Uh, not everybody who joins a club ends up in the discord, but a lot of us use it and
[02:19:01.160 --> 02:19:02.160]   love it.
[02:19:02.160 --> 02:19:07.320]   Uh, it's a place to discuss not only the shows, but you know, everything geeks like books
[02:19:07.320 --> 02:19:11.920]   and comics and ham radio and hardware and Linux and music.
[02:19:11.920 --> 02:19:14.480]   Uh, that discord is a lot of fun.
[02:19:14.480 --> 02:19:16.160]   Here's our tick tock corner.
[02:19:16.160 --> 02:19:18.240]   I guess they're talking about tick tock in there.
[02:19:18.240 --> 02:19:21.800]   And of course here's the twit discussion going on right now.
[02:19:21.800 --> 02:19:25.800]   We have animated gifts, which makes it very much more fun.
[02:19:25.800 --> 02:19:29.600]   Also, it's a great place to put links in to other shows.
[02:19:29.600 --> 02:19:34.320]   This interview I did with you, Corey, uh, on a triangulation was not ad supported.
[02:19:34.320 --> 02:19:35.320]   It was club supported.
[02:19:35.320 --> 02:19:40.240]   There's the only way we could do it because it's a ad hoc show that, you know, we bring
[02:19:40.240 --> 02:19:42.480]   in people when we want to talk to them.
[02:19:42.480 --> 02:19:45.120]   And so no advertisers going to buy a show where they don't know when it's on, but the
[02:19:45.120 --> 02:19:46.400]   club makes it possible.
[02:19:46.400 --> 02:19:50.640]   That's why we also do hands on Mac in the club with Micah and hands on windows with
[02:19:50.640 --> 02:19:54.160]   Paul Therrott and our untitled Linux shows in there.
[02:19:54.160 --> 02:19:57.720]   Uh, the club has really given us a chance to do a lot more interesting stuff.
[02:19:57.720 --> 02:20:02.400]   We launched our new space show in there this week in space, which has since gone public.
[02:20:02.400 --> 02:20:04.400]   Uh, also the twit plus feed.
[02:20:04.400 --> 02:20:06.760]   That's where those other shows show up seven bucks a month.
[02:20:06.760 --> 02:20:08.840]   If you're interested, it really helps us.
[02:20:08.840 --> 02:20:10.920]   There's a yearly, uh, membership too.
[02:20:10.920 --> 02:20:14.880]   If you don't want to be nickeled and dined every month, uh, you can even get, uh, corporate
[02:20:14.880 --> 02:20:18.640]   memberships if you want to, uh, have everybody in your company listen.
[02:20:18.640 --> 02:20:19.960]   That would be good.
[02:20:19.960 --> 02:20:23.960]   You can also buy individual shows for $2.99 a month.
[02:20:23.960 --> 02:20:27.720]   All at twit.tv/club twit.
[02:20:27.720 --> 02:20:29.520]   And we thank you, uh, for your support.
[02:20:29.520 --> 02:20:31.280]   We do have great advertisers.
[02:20:31.280 --> 02:20:39.400]   And goodness, uh, because, you know, it's expensive to do, uh, what we do, including policy genius.
[02:20:39.400 --> 02:20:41.920]   Policy genius is genius.
[02:20:41.920 --> 02:20:47.040]   We pay hundreds of dollars a year to, uh, for insurance for our fire insurance for our house
[02:20:47.040 --> 02:20:49.920]   or, you know, you have to for your car, right?
[02:20:49.920 --> 02:20:51.840]   Some of us even buy insurance for our phones.
[02:20:51.840 --> 02:20:57.120]   The fumble thing among us, but how many of you are taking steps to protect your family?
[02:20:57.120 --> 02:21:04.800]   You know, think about your costs to your family, mortgage payments, student costs,
[02:21:04.800 --> 02:21:08.200]   uh, student loans and other loans that don't disappear.
[02:21:08.200 --> 02:21:13.200]   If something happens to you, a life insurance policy, it was the, when we had kids, the
[02:21:13.200 --> 02:21:16.240]   first thing I did is I want to make sure my family was taken care of.
[02:21:16.240 --> 02:21:21.000]   If something bad happened to me, it can provide your loved ones with a financial cushion.
[02:21:21.000 --> 02:21:23.480]   They can use to cover those ongoing costs.
[02:21:23.480 --> 02:21:28.920]   And it gives you peace of mind to know that they're going to be protected.
[02:21:28.920 --> 02:21:31.640]   Um, having life insurance through your job.
[02:21:31.640 --> 02:21:32.840]   Yeah, that's nice.
[02:21:32.840 --> 02:21:36.520]   It never was enough to really solve this problem.
[02:21:36.520 --> 02:21:41.080]   Like most people, it turns out need about 10 times more coverage than their jobs offer
[02:21:41.080 --> 02:21:43.080]   to provide for their families.
[02:21:43.080 --> 02:21:47.040]   Inflation is driving up prices for everything these days, life insurance rates actually
[02:21:47.040 --> 02:21:50.000]   down from this time last year.
[02:21:50.000 --> 02:21:54.360]   And since life insurance typically gets more expensive as you age, the best time to buy
[02:21:54.360 --> 02:21:57.000]   that life insurance is today.
[02:21:57.000 --> 02:21:59.160]   And the best way to do it, policy genius.
[02:21:59.160 --> 02:22:01.160]   It's not an insurer.
[02:22:01.160 --> 02:22:02.920]   They're an insurance marketplace.
[02:22:02.920 --> 02:22:08.160]   So you can get quotes from all the best companies, AIG, Prudential, all of the best best companies,
[02:22:08.160 --> 02:22:13.200]   one place and get the lowest price on your life insurance, which means you could save
[02:22:13.200 --> 02:22:19.920]   as much as 50% or even more on life insurance just by comparing quotes on, on policy genius.
[02:22:19.920 --> 02:22:23.280]   Options started $17 a month for half a million dollars of coverage.
[02:22:23.280 --> 02:22:25.520]   Of course, it's going to depend on you, your agents, so forth.
[02:22:25.520 --> 02:22:26.520]   That's why.
[02:22:26.520 --> 02:22:27.520]   But it's not getting cheaper.
[02:22:27.520 --> 02:22:28.600]   So do it right now.
[02:22:28.600 --> 02:22:33.720]   Click the link on our show notes or head to policy genius.com/twit.
[02:22:33.720 --> 02:22:36.120]   And you can get personalized quotes in minutes.
[02:22:36.120 --> 02:22:37.800]   Find the right policy for your needs.
[02:22:37.800 --> 02:22:42.280]   Now, I should tell you the people you're talking to a policy genius are licensed agents.
[02:22:42.280 --> 02:22:45.280]   They have to be in order to advise you about insurance.
[02:22:45.280 --> 02:22:48.840]   You're buying insurance from these big companies, but policy genius is not working for those
[02:22:48.840 --> 02:22:49.840]   companies.
[02:22:49.840 --> 02:22:54.400]   They're on hand through the entire process to help you understand your options, help
[02:22:54.400 --> 02:22:56.720]   you make decisions with confidence.
[02:22:56.720 --> 02:22:58.680]   They do not add extra fees.
[02:22:58.680 --> 02:22:59.680]   They do not.
[02:22:59.680 --> 02:23:00.680]   You'll be glad to know.
[02:23:00.680 --> 02:23:02.680]   Sell your info to third parties.
[02:23:02.680 --> 02:23:06.960]   Check the reviews on Google Trust Pilot and elsewhere, thousands of five star reviews.
[02:23:06.960 --> 02:23:09.240]   People really love this service.
[02:23:09.240 --> 02:23:12.360]   They have options that offer coverage in as little as a week.
[02:23:12.360 --> 02:23:14.400]   Some avoid unnecessary medical exams.
[02:23:14.400 --> 02:23:17.200]   30 million people have shopped for insurance.
[02:23:17.200 --> 02:23:21.040]   That policy genius, they've placed more than $150 billion in coverage.
[02:23:21.040 --> 02:23:24.200]   And again, people love it while you're there.
[02:23:24.200 --> 02:23:27.720]   Of course, they do offer quotes for home, auto, pet, renters and more.
[02:23:27.720 --> 02:23:29.560]   So you should check that out as well.
[02:23:29.560 --> 02:23:31.880]   But I thought it was appropriate to talk about life insurance.
[02:23:31.880 --> 02:23:35.920]   I think so many of us, especially you younger folks, you don't think about that, but you
[02:23:35.920 --> 02:23:43.160]   got kids, you got a spouse, you've got a family, you need that life insurance policy genius.com/twit.
[02:23:43.160 --> 02:23:44.920]   Get those life insurance quotes free.
[02:23:44.920 --> 02:23:51.920]   There's no cost and see how much you can save policy genius, policy genius.com/twit.
[02:23:51.920 --> 02:23:57.800]   Oh, gosh, there's so many stories and so little time.
[02:23:57.800 --> 02:23:58.960]   Let me see.
[02:23:58.960 --> 02:24:05.520]   Before we go to talk about that story, I wanted to ask you about the Amazon hardware event,
[02:24:05.520 --> 02:24:07.000]   Alex.
[02:24:07.000 --> 02:24:14.840]   Before we go to the A17 chip price increase, before all of that, let's run a quick promo
[02:24:14.840 --> 02:24:16.880]   for some of the things that happened this week on Twitter.
[02:24:16.880 --> 02:24:18.440]   I think you might recognize some of the brands.
[02:24:18.440 --> 02:24:22.120]   It's only the people in the C-suites and the stockholders that are really getting rich
[02:24:22.120 --> 02:24:27.280]   on this and the people are waking up and we got to get there before the guillotine's
[02:24:27.280 --> 02:24:30.040]   go up because, I mean, I have one over here.
[02:24:30.040 --> 02:24:31.040]   No, no.
[02:24:31.040 --> 02:24:32.040]   There we go.
[02:24:32.040 --> 02:24:33.040]   Oh, no, no.
[02:24:33.040 --> 02:24:37.560]   Previously on Twitter, triangulation.
[02:24:37.560 --> 02:24:43.240]   Corey, Dr. O, just co-authored a book now out called Choke Point Capitalism.
[02:24:43.240 --> 02:24:46.720]   Corey and his co-author Rebecca Giblin are joining us.
[02:24:46.720 --> 02:24:50.040]   So, Amazon's got this flywheel that it loves to boast about.
[02:24:50.040 --> 02:24:51.880]   It says we've got this lower cost structure.
[02:24:51.880 --> 02:24:54.440]   It leads to lower prices and everybody is super happy.
[02:24:54.440 --> 02:24:56.440]   That's not really what's happening.
[02:24:56.440 --> 02:24:57.440]   Yeah.
[02:24:57.440 --> 02:25:01.200]   Because everything that Amazon has ever done in its business has been designed, first
[02:25:01.200 --> 02:25:02.840]   and foremost, to lock in its customers.
[02:25:02.840 --> 02:25:06.720]   But also, so it could squeeze other competitors out of the market.
[02:25:06.720 --> 02:25:08.600]   That news weekly.
[02:25:08.600 --> 02:25:12.320]   Stadia Google's cloud gaming service is done.
[02:25:12.320 --> 02:25:14.760]   Google's basically announced that it's going to be done.
[02:25:14.760 --> 02:25:15.760]   It's killing it.
[02:25:15.760 --> 02:25:19.480]   Time and time again, they prove that this is just part of their brand identity.
[02:25:19.480 --> 02:25:22.480]   And that's a bad thing to be part of your brand identity.
[02:25:22.480 --> 02:25:23.480]   Mac Break Weekly.
[02:25:23.480 --> 02:25:24.480]   Yeah, that's-
[02:25:24.480 --> 02:25:29.080]   Also, I'll be testing this Apple Watch Ultra in the hurricane that's actually headed my
[02:25:29.080 --> 02:25:30.080]   way.
[02:25:30.080 --> 02:25:31.080]   Oh, where are you right now?
[02:25:31.080 --> 02:25:32.080]   My word.
[02:25:32.080 --> 02:25:33.080]   I am near Tampa, Florida.
[02:25:33.080 --> 02:25:36.640]   The kind of the bottom line review, Steven, on the Ultra.
[02:25:36.640 --> 02:25:37.640]   Worth it?
[02:25:37.640 --> 02:25:38.960]   I do like the bigger screen.
[02:25:38.960 --> 02:25:41.000]   And I was always a titanium watch guy.
[02:25:41.000 --> 02:25:44.400]   So if you want, well, there it goes.
[02:25:44.400 --> 02:25:45.960]   Oh, bye bye, Steven.
[02:25:45.960 --> 02:25:48.240]   My pleasure talking to you.
[02:25:48.240 --> 02:25:49.880]   Good luck in the hurricane.
[02:25:49.880 --> 02:25:53.960]   To it, it keeps going and going and going.
[02:25:53.960 --> 02:25:54.960]   Show this.
[02:25:54.960 --> 02:25:55.960]   Show the wide shot.
[02:25:55.960 --> 02:25:56.960]   The hurricane.
[02:25:56.960 --> 02:26:01.160]   Because what happens now is, comes in and removes you from the presence.
[02:26:01.160 --> 02:26:02.960]   Way still there.
[02:26:02.960 --> 02:26:03.960]   Way back.
[02:26:03.960 --> 02:26:07.360]   Well, last as long as my APC runs on my Mac student.
[02:26:07.360 --> 02:26:08.360]   I hear the big.
[02:26:08.360 --> 02:26:10.120]   I hear the big.
[02:26:10.120 --> 02:26:16.160]   The good news is the hurricane dodged Tampa and Steven is just fine.
[02:26:16.160 --> 02:26:17.160]   Just in case.
[02:26:17.160 --> 02:26:19.400]   And now our friends in Fort Myers, maybe that's another story.
[02:26:19.400 --> 02:26:22.400]   But we are thoughts going out to all of you.
[02:26:22.400 --> 02:26:25.760]   Of course, I hope you did survive and do well in the hurricane.
[02:26:25.760 --> 02:26:28.480]   If you didn't, I'm sorry.
[02:26:28.480 --> 02:26:29.480]   Sorry.
[02:26:29.480 --> 02:26:35.920]   Did you watch any of the or breed any of the coverage of Amazon's big event this week,
[02:26:35.920 --> 02:26:36.920]   Alex?
[02:26:36.920 --> 02:26:37.960]   And what were your thoughts?
[02:26:37.960 --> 02:26:42.040]   I was most intrigued by that sleep monitor that they have.
[02:26:42.040 --> 02:26:45.640]   I don't know about you, but I think you said earlier in the show, maybe for the age sleep
[02:26:45.640 --> 02:26:47.880]   bad that we could all use a better night sleep.
[02:26:47.880 --> 02:26:49.440]   Yeah, the halo, they call it, right?
[02:26:49.440 --> 02:26:52.840]   Because it, you know, they, it doesn't have a camera as far as I could tell.
[02:26:52.840 --> 02:26:53.840]   Yeah.
[02:26:53.840 --> 02:26:59.160]   Now, I think it doesn't have a camera and I'm always interested in trying to use like,
[02:26:59.160 --> 02:27:03.640]   you know, any way I can to sleep better and learn a little bit more about how I sleep.
[02:27:03.640 --> 02:27:06.840]   And the things that are on your wrist, I don't sleep always anything on my wrist.
[02:27:06.840 --> 02:27:12.440]   So, you know, I'm, I, I guess I'm out on those, but something like this is, is interesting.
[02:27:12.440 --> 02:27:16.000]   However, I just don't trust Amazon with this type of thing.
[02:27:16.000 --> 02:27:21.240]   So, you know, maybe someone can take a, do you go the reverse route and copy this from
[02:27:21.240 --> 02:27:22.720]   Big Tech and so I can use it.
[02:27:22.720 --> 02:27:24.160]   I should Google with their nest hub.
[02:27:24.160 --> 02:27:27.000]   We'll also do that because that one has a camera.
[02:27:27.000 --> 02:27:29.520]   It also listens to the snowers and stuff like that.
[02:27:29.520 --> 02:27:30.520]   It's out for me.
[02:27:30.520 --> 02:27:31.520]   Yeah.
[02:27:31.520 --> 02:27:32.520]   So, so I don't know.
[02:27:32.520 --> 02:27:33.520]   I like these.
[02:27:33.520 --> 02:27:34.520]   I like the idea.
[02:27:34.520 --> 02:27:39.080]   I like use this idea of we can use technology to improve our wellness and improve our health.
[02:27:39.080 --> 02:27:40.080]   It's interesting.
[02:27:40.080 --> 02:27:41.080]   This is part of Amazon.
[02:27:41.080 --> 02:27:42.080]   Do it.
[02:27:42.080 --> 02:27:48.920]   Amazon's halo series because they have this halo band too, which is a fitness tracker.
[02:27:48.920 --> 02:27:56.200]   And then now this is the halo view, which is, which doesn't see more of a watch, right?
[02:27:56.200 --> 02:27:58.560]   And then a halo rise, which doesn't have a camera.
[02:27:58.560 --> 02:27:59.560]   That doesn't see.
[02:27:59.560 --> 02:28:00.560]   Yeah, it doesn't see.
[02:28:00.560 --> 02:28:01.560]   Yeah.
[02:28:01.560 --> 02:28:02.560]   And it has a cool, it wakes you up with some light in the morning.
[02:28:02.560 --> 02:28:03.560]   That's what that's.
[02:28:03.560 --> 02:28:07.080]   I'm not going to buy it, but a lot of people are going to buy this thing for sure.
[02:28:07.080 --> 02:28:12.200]   Amazon also announced that they are going to let your Amazon Echo act as an Euro beacon.
[02:28:12.200 --> 02:28:15.440]   Amazon owns the mesh networking company, Euro.
[02:28:15.440 --> 02:28:20.720]   And in a very interesting synergy, when you buy one of the new fifth generation Amazon
[02:28:20.720 --> 02:28:27.400]   Echoes, it can be used to extend your Wi-Fi using Euro.
[02:28:27.400 --> 02:28:33.360]   I have to say all of these things look like Amazon, thinly veiled attempts at Amazon to
[02:28:33.360 --> 02:28:35.320]   get more sensors into your home.
[02:28:35.320 --> 02:28:37.320]   Corey, are they all?
[02:28:37.320 --> 02:28:39.800]   Yeah, any thoughts on that?
[02:28:39.800 --> 02:28:40.800]   Yeah.
[02:28:40.800 --> 02:28:47.520]   I mean, I think that is exactly what this is as is the the robot Astro and position.
[02:28:47.520 --> 02:28:48.520]   Yeah.
[02:28:48.520 --> 02:28:54.760]   I think that for me, this drives home the problem or the poverty of just don't do business with
[02:28:54.760 --> 02:28:57.040]   them because I own some Eros.
[02:28:57.040 --> 02:28:59.200]   In fact, I own many Eros.
[02:28:59.200 --> 02:29:03.160]   I bought them before they were an Amazon company.
[02:29:03.160 --> 02:29:09.360]   So if the answer to if you don't like Amazon, don't do business with them, if that's the
[02:29:09.360 --> 02:29:12.080]   answer, then what do I do when Amazon buys the company?
[02:29:12.080 --> 02:29:15.280]   What do you do when Google buys Fitbit?
[02:29:15.280 --> 02:29:20.280]   What do you do when Facebook buys your beloved daily visit Jiffy?
[02:29:20.280 --> 02:29:22.440]   You are your stock, right?
[02:29:22.440 --> 02:29:26.800]   And so, you know, it's not like I can add my own firmware.
[02:29:26.800 --> 02:29:28.680]   It's not like I can untether it.
[02:29:28.680 --> 02:29:34.180]   In fact, the firmware updates for the Eros that I own have become increasingly Amazon
[02:29:34.180 --> 02:29:35.280]   linked.
[02:29:35.280 --> 02:29:40.600]   Every time I get one, I get a bunch of notices about how this is becoming less and less functional
[02:29:40.600 --> 02:29:41.760]   with that Alexa.
[02:29:41.760 --> 02:29:44.480]   They build that as it becoming more and more functional with Alexa.
[02:29:44.480 --> 02:29:49.760]   But obviously, these are the opposite sides of the same coin.
[02:29:49.760 --> 02:29:57.400]   And you know, it tells you the problem with acquisition driven growth in a market where
[02:29:57.400 --> 02:30:02.520]   we choose winners based on what people buy because companies who have access to the capital
[02:30:02.520 --> 02:30:06.080]   markets can just buy the companies that are successful.
[02:30:06.080 --> 02:30:11.080]   You know, if everything in your grocery store is made by Proctor and Gamble or Unilever.
[02:30:11.080 --> 02:30:15.800]   And if there is a local artisanal, oatmeal cookie place that takes off, one of them will
[02:30:15.800 --> 02:30:16.800]   buy it.
[02:30:16.800 --> 02:30:21.720]   And when they do their press release, they will say, "We here at Proctor and Gamble understand
[02:30:21.720 --> 02:30:23.560]   that our customers value choice."
[02:30:23.560 --> 02:30:27.960]   And that's why we bought the company you chose to buy things from instead of us.
[02:30:27.960 --> 02:30:28.960]   Yeah.
[02:30:28.960 --> 02:30:30.360]   I don't know.
[02:30:30.360 --> 02:30:34.760]   This is pointing to an interesting battle, a slightly different lens, but interesting
[02:30:34.760 --> 02:30:37.520]   battle between Amazon and Apple.
[02:30:37.520 --> 02:30:43.680]   And I think that Apple has largely given up in the battle for ambient computing.
[02:30:43.680 --> 02:30:46.280]   Remember, it sort of rolled back home pod.
[02:30:46.280 --> 02:30:47.680]   Series still is garbage.
[02:30:47.680 --> 02:30:50.880]   Meanwhile, Amazon is going to be everywhere in your home.
[02:30:50.880 --> 02:30:53.120]   That's a whole new computing layer.
[02:30:53.120 --> 02:30:58.600]   And I think as this stuff gets more and more embedded in our lives, the fact that we're
[02:30:58.600 --> 02:31:02.480]   walking around the sensors, the fact that you have echoes in your house, there are also
[02:31:02.480 --> 02:31:03.680]   mesh networks.
[02:31:03.680 --> 02:31:04.680]   You can speak to it.
[02:31:04.680 --> 02:31:06.280]   It will enhance your Wi-Fi, by the way.
[02:31:06.280 --> 02:31:09.920]   I would like to hear what Corey thinks about this, but now they're going to acquire Roomba
[02:31:09.920 --> 02:31:12.880]   or try to at least get their vacu...
[02:31:12.880 --> 02:31:15.440]   I'll go at the latest news with the FTC.
[02:31:15.440 --> 02:31:16.440]   Right.
[02:31:16.440 --> 02:31:17.440]   It was with Warren, doesn't like that.
[02:31:17.440 --> 02:31:19.240]   And he'll just be FTC.
[02:31:19.240 --> 02:31:21.960]   But anyway, I think that it's interesting.
[02:31:21.960 --> 02:31:29.400]   And I wonder what will happen as Apple sees that Amazon run away with this screenless computing
[02:31:29.400 --> 02:31:30.800]   in your home.
[02:31:30.800 --> 02:31:37.280]   And then pushes Apple to try to basically do what it's done in every other level, which
[02:31:37.280 --> 02:31:41.360]   is say, "We're going to do this, but we're going to do privacy focused," and whether
[02:31:41.360 --> 02:31:42.360]   that will work.
[02:31:42.360 --> 02:31:45.760]   And it's an interesting battle that I'm looking at with this.
[02:31:45.760 --> 02:31:52.880]   I think Apple and Amazon just inch closer and closer together the farther we get towards
[02:31:52.880 --> 02:31:54.920]   them pushing beyond the markets that they own.
[02:31:54.920 --> 02:31:57.000]   And I think that's going to be a really interesting battle.
[02:31:57.000 --> 02:32:01.240]   But anyway, why don't we tee up Corey on the Roomba question?
[02:32:01.240 --> 02:32:02.800]   That's what I...
[02:32:02.800 --> 02:32:03.800]   That's what I...
[02:32:03.800 --> 02:32:04.800]   That's what I...
[02:32:04.800 --> 02:32:05.800]   The iRobot acquisition.
[02:32:05.800 --> 02:32:07.600]   I mean, I think it's the same thing.
[02:32:07.600 --> 02:32:12.880]   Amazon would like to know about the inside of your home and its geometry.
[02:32:12.880 --> 02:32:17.640]   And they'd like to know that so that they can leverage it for parochial advantage, like
[02:32:17.640 --> 02:32:24.160]   to make you use that or to make it so that the only company that you can use that geometry
[02:32:24.160 --> 02:32:26.480]   with is Amazon.
[02:32:26.480 --> 02:32:29.160]   That is to your benefit if you own a mobile robot.
[02:32:29.160 --> 02:32:32.840]   And this is the kind of thing that you're interested in to get accurate maps of your
[02:32:32.840 --> 02:32:34.400]   home from that robot.
[02:32:34.400 --> 02:32:39.000]   But it's not to your benefit to have that robot only allow you to take the data that
[02:32:39.000 --> 02:32:43.600]   was generated by the robot that you bought, that you charge with your electricity, going
[02:32:43.600 --> 02:32:48.680]   around your home that you own a rent, but that data not being yours to use to your maximal
[02:32:48.680 --> 02:32:54.080]   advantage, that data's use is being constrained to uses that are good for the shareholders
[02:32:54.080 --> 02:32:56.000]   of the company that made the robot.
[02:32:56.000 --> 02:32:59.640]   In fact, not even the shareholders that company made the robot, the shareholders of the company
[02:32:59.640 --> 02:33:01.520]   that bought the company that made the robot.
[02:33:01.520 --> 02:33:06.600]   I don't understand why we as the owners of that robot should feel like it's our duty
[02:33:06.600 --> 02:33:09.480]   to make sure that those shareholders are happy.
[02:33:09.480 --> 02:33:10.920]   I'm on team Corey here.
[02:33:10.920 --> 02:33:13.480]   Let's have Kumbai at a moment here towards the end.
[02:33:13.480 --> 02:33:17.280]   But I bought the Roomba on Prime Day, so go figure.
[02:33:17.280 --> 02:33:18.280]   Amazon got me to buy it.
[02:33:18.280 --> 02:33:23.840]   I think this is really the lesson of all of this is that we are willing captors.
[02:33:23.840 --> 02:33:24.840]   Captors.
[02:33:24.840 --> 02:33:25.840]   Exactly.
[02:33:25.840 --> 02:33:27.480]   I think that's a great lesson.
[02:33:27.480 --> 02:33:32.920]   You point out in your book, Corey, that only 1% of people who have Amazon Prime ever shop
[02:33:32.920 --> 02:33:36.440]   for better deals outside of Amazon.
[02:33:36.440 --> 02:33:39.920]   Once you pay for Prime, you're there for life.
[02:33:39.920 --> 02:33:41.160]   Go ahead, Alex.
[02:33:41.160 --> 02:33:45.440]   I'll just do a quick ode to the Roomba that is a very helpful device.
[02:33:45.440 --> 02:33:46.440]   Does it work nice for you?
[02:33:46.440 --> 02:33:47.440]   You like it?
[02:33:47.440 --> 02:33:48.440]   I love it.
[02:33:48.440 --> 02:33:49.440]   I'm a Roombaholic.
[02:33:49.440 --> 02:33:52.160]   I run a couple of times a week and my floors are real clean.
[02:33:52.160 --> 02:33:55.760]   Can I ask you as a practical matter?
[02:33:55.760 --> 02:33:56.760]   How does that work?
[02:33:56.760 --> 02:34:03.720]   I have never owned a Roomba that didn't immediately get hopelessly lost in a crack in the pavement
[02:34:03.720 --> 02:34:05.680]   or a cable or something.
[02:34:05.680 --> 02:34:15.680]   It does have a very square build house with no wires.
[02:34:15.680 --> 02:34:16.680]   It does eat my wires.
[02:34:16.680 --> 02:34:19.000]   I have to be a little careful about it.
[02:34:19.000 --> 02:34:21.080]   My house is pretty boxy.
[02:34:21.080 --> 02:34:22.080]   It's an apartment.
[02:34:22.080 --> 02:34:23.080]   It's pretty small.
[02:34:23.080 --> 02:34:25.080]   We had a Roomba that would get stuck.
[02:34:25.080 --> 02:34:26.080]   Roomba, no stairs.
[02:34:26.080 --> 02:34:32.760]   A little Etta Cher side table thing that had just enough clearance on the floor for the
[02:34:32.760 --> 02:34:37.120]   Roomba to think it could get under it, but not quite enough clearance for it to continue
[02:34:37.120 --> 02:34:38.120]   to get stuck.
[02:34:38.120 --> 02:34:39.920]   Oh, yeah, it's amazing.
[02:34:39.920 --> 02:34:43.680]   Every night, Roomba is the only thing in my house that's more stubborn than me.
[02:34:43.680 --> 02:34:44.680]   It's pretty amazing.
[02:34:44.680 --> 02:34:45.680]   And then we go, bang.
[02:34:45.680 --> 02:34:46.680]   Bang.
[02:34:46.680 --> 02:34:47.680]   Bang.
[02:34:47.680 --> 02:34:48.680]   Try to get in there.
[02:34:48.680 --> 02:34:49.680]   It does.
[02:34:49.680 --> 02:34:50.680]   Three in the morning.
[02:34:50.680 --> 02:34:51.680]   Sorry, go ahead.
[02:34:51.680 --> 02:34:52.680]   Every three in the morning.
[02:34:52.680 --> 02:34:55.040]   Every night, three in the morning, I'm up picking up the little Roomba.
[02:34:55.040 --> 02:34:56.840]   Bringing it back to its home.
[02:34:56.840 --> 02:34:57.840]   Yeah.
[02:34:57.840 --> 02:35:01.480]   I'll look at those these moments where the Roomba just goes for it and keeps going for
[02:35:01.480 --> 02:35:02.480]   it.
[02:35:02.480 --> 02:35:03.480]   And then eventually it wins.
[02:35:03.480 --> 02:35:05.400]   And it's quite a celebratory moment for me.
[02:35:05.400 --> 02:35:06.960]   I'm like, you did it.
[02:35:06.960 --> 02:35:07.960]   You did it.
[02:35:07.960 --> 02:35:16.720]   There's a great machine learning kind of a teachable moment, cautionary tale where an
[02:35:16.720 --> 02:35:23.640]   engineer used an ML algorithm to get his Roomba to minimize forward crashes with its
[02:35:23.640 --> 02:35:29.440]   forward bumper so that it wouldn't bang into the walls and it just started going in reverse.
[02:35:29.440 --> 02:35:32.720]   It's given up entirely on it.
[02:35:32.720 --> 02:35:34.280]   It's not Roomba.
[02:35:34.280 --> 02:35:37.360]   By the way, I'm just nominating Roomba Hollis for this show title.
[02:35:37.360 --> 02:35:38.360]   I like it.
[02:35:38.360 --> 02:35:39.360]   Roomba Hollis.
[02:35:39.360 --> 02:35:40.360]   Yes.
[02:35:40.360 --> 02:35:41.360]   Makes it happen.
[02:35:41.360 --> 02:35:44.920]   Surround your Roomba with a little bit of salt and see what happens.
[02:35:44.920 --> 02:35:45.920]   Yeah.
[02:35:45.920 --> 02:35:52.120]   Talking about browser extensions and browser ad blockers, Google had announced the evil
[02:35:52.120 --> 02:35:59.960]   technology and manifest V3, which allowed for something called the web content API, was
[02:35:59.960 --> 02:36:06.480]   going to be eliminated from Chrome and hence Chromium, its open source parent, and hence
[02:36:06.480 --> 02:36:13.040]   probably from many open source projects based on Chromium, which means that ad blockers
[02:36:13.040 --> 02:36:17.400]   like my favorite Gore Hills U block origin would no longer work.
[02:36:17.400 --> 02:36:23.520]   They require this manifest V2 and access to the web content via the API.
[02:36:23.520 --> 02:36:26.560]   Google has some good reasons to dump it.
[02:36:26.560 --> 02:36:31.240]   It can slow your, it can really hit performance if all the, if every single extension starts
[02:36:31.240 --> 02:36:32.840]   asking for the content.
[02:36:32.840 --> 02:36:40.520]   It can also be a privacy problem, but I think people install the honey Chrome extension really
[02:36:40.520 --> 02:36:41.520]   want it.
[02:36:41.520 --> 02:36:44.280]   Know that honey is watching every move they make.
[02:36:44.280 --> 02:36:49.240]   A lot of criticism, Google has said we're going to delay this till 2024.
[02:36:49.240 --> 02:36:52.280]   This has been by the way kind of a standard for Google.
[02:36:52.280 --> 02:36:55.520]   They'll announce some big change to something or other.
[02:36:55.520 --> 02:36:57.840]   Everybody will complain.
[02:36:57.840 --> 02:36:59.360]   And then Google says, Oh, well, never mind.
[02:36:59.360 --> 02:37:00.360]   We're going to do topics.
[02:37:00.360 --> 02:37:03.780]   We're not going to do, we're not going to do that other thing.
[02:37:03.780 --> 02:37:08.120]   So I hope that this is delayed forever, but it's just one more reason you should not use
[02:37:08.120 --> 02:37:09.800]   Chrome or Chromium based browsers.
[02:37:09.800 --> 02:37:14.000]   So if Aldi says, our ad blocker will continue to work, look at his sport V2.
[02:37:14.000 --> 02:37:18.640]   I think brave has his own ad blocker in there, but I use Firefox for that reason.
[02:37:18.640 --> 02:37:24.080]   I think it's good to have a competitor to Google.
[02:37:24.080 --> 02:37:33.360]   And finally, as you know, McDonald's has left Russia, which has given rise, I think,
[02:37:33.360 --> 02:37:40.920]   to a number of stores that look just like McDonald's called Tasty and that's it.
[02:37:40.920 --> 02:37:44.360]   And now Russia's former Lego stores.
[02:37:44.360 --> 02:37:50.080]   Lego has also left the country have a rebranded as world of cubes.
[02:37:50.080 --> 02:37:55.880]   But as Rob is just points out in Boing Boing, the Lego patent has expired.
[02:37:55.880 --> 02:38:02.120]   So it making a Lego clone is not hard to do unclear whether they stick with world of
[02:38:02.120 --> 02:38:03.120]   cubes.
[02:38:03.120 --> 02:38:04.120]   It was pretty good.
[02:38:04.120 --> 02:38:05.120]   Yeah.
[02:38:05.120 --> 02:38:06.120]   Yeah.
[02:38:06.120 --> 02:38:07.360]   Although Rob says he should have called it Eastern block.
[02:38:07.360 --> 02:38:09.360]   I mean, really, come on.
[02:38:09.360 --> 02:38:13.480]   Oh, Robert Skitza is a national treasure.
[02:38:13.480 --> 02:38:14.480]   Is it Pascisa?
[02:38:14.480 --> 02:38:15.480]   I'll say it right from now on.
[02:38:15.480 --> 02:38:16.480]   I say Pascisa.
[02:38:16.480 --> 02:38:17.480]   I don't know what he says.
[02:38:17.480 --> 02:38:18.480]   Oh, OK.
[02:38:18.480 --> 02:38:19.480]   Pascisa.
[02:38:19.480 --> 02:38:20.480]   Yeah.
[02:38:20.480 --> 02:38:22.120]   Well, that's the correct Italian pronunciation.
[02:38:22.120 --> 02:38:23.120]   Pascisa.
[02:38:23.120 --> 02:38:24.280]   Pascisa.
[02:38:24.280 --> 02:38:28.680]   He suggests also it'd be nice if there were some locally themed replacement product lines
[02:38:28.680 --> 02:38:35.480]   such as sets for barriers execution in the Lubianca buildings basement and so forth.
[02:38:35.480 --> 02:38:37.520]   Oh, my God.
[02:38:37.520 --> 02:38:40.520]   OK.
[02:38:40.520 --> 02:38:44.240]   We can laugh as we watch the world burn.
[02:38:44.240 --> 02:38:47.000]   That's pretty much the story there.
[02:38:47.000 --> 02:38:52.760]   I hope we don't have a nuclear war, World War III or any of that.
[02:38:52.760 --> 02:38:59.640]   If with any luck we don't, you can listen to the fantastic big technology podcast as
[02:38:59.640 --> 02:39:04.720]   created by the wonderful Alex Cantor Witz boy, you get some great people.
[02:39:04.720 --> 02:39:07.440]   Probably our rock of on is on the most recent one.
[02:39:07.440 --> 02:39:10.640]   Google's senior vice president of search.
[02:39:10.640 --> 02:39:11.640]   Really good stuff.
[02:39:11.640 --> 02:39:16.840]   Yeah, I'm about to drop an episode with Francis Halligan, the Facebook whistle.
[02:39:16.840 --> 02:39:17.840]   Awesome.
[02:39:17.840 --> 02:39:18.840]   Awesome.
[02:39:18.840 --> 02:39:20.360]   So that's coming by the time this is live, that will be live.
[02:39:20.360 --> 02:39:26.520]   And then later this week I have Tom Allison, who many people don't know, but he runs Facebook.
[02:39:26.520 --> 02:39:27.520]   Yeah.
[02:39:27.520 --> 02:39:28.520]   Wow.
[02:39:28.520 --> 02:39:34.280]   So it'll be some fun conversations coming up and lots of AI stuff on the way as well.
[02:39:34.280 --> 02:39:38.760]   So people like that conversation we had about sentinians and stuff like that, trying to
[02:39:38.760 --> 02:39:39.760]   bring all of you in.
[02:39:39.760 --> 02:39:40.760]   It's going to be fun.
[02:39:40.760 --> 02:39:42.360]   Kevin Kelly in August.
[02:39:42.360 --> 02:39:44.920]   So this is the second time you've dropped his name.
[02:39:44.920 --> 02:39:45.920]   Right.
[02:39:45.920 --> 02:39:47.640]   And Kevin Kelly's episode was just his life advice.
[02:39:47.640 --> 02:39:49.280]   We didn't really talk about technology at all.
[02:39:49.280 --> 02:39:50.280]   It was just his life advice.
[02:39:50.280 --> 02:39:51.280]   That's his new, isn't it?
[02:39:51.280 --> 02:39:52.280]   It's amazing.
[02:39:52.280 --> 02:39:54.360]   Yeah, he has a book coming out about it.
[02:39:54.360 --> 02:39:58.960]   And he has these lists of 100 things, you know, from my hundred and 70th birthday or
[02:39:58.960 --> 02:39:59.960]   something like that.
[02:39:59.960 --> 02:40:00.960]   Right.
[02:40:00.960 --> 02:40:01.960]   Right.
[02:40:01.960 --> 02:40:02.960]   Right.
[02:40:02.960 --> 02:40:04.440]   I just loved it so much as I kept on when you come on, I'm going to ask you about these
[02:40:04.440 --> 02:40:05.440]   things.
[02:40:05.440 --> 02:40:06.440]   Good.
[02:40:06.440 --> 02:40:07.440]   So I was a blast.
[02:40:07.440 --> 02:40:08.440]   Oh, I'll listen to that one for sure.
[02:40:08.440 --> 02:40:09.440]   I love it.
[02:40:09.440 --> 02:40:13.080]   Also, of course, the big technology newsletter at bigtechnology.substack.com.
[02:40:13.080 --> 02:40:18.480]   There's Dolly images right there up top.
[02:40:18.480 --> 02:40:21.720]   Oh, you're using Dolly for your illustrations.
[02:40:21.720 --> 02:40:22.720]   I am.
[02:40:22.720 --> 02:40:24.840]   I'm not a big business.
[02:40:24.840 --> 02:40:31.320]   So I don't have money for illustrators, but I was using Unsplash before the free stock
[02:40:31.320 --> 02:40:32.320]   photos.
[02:40:32.320 --> 02:40:36.480]   And I think this is my first week trying to Dolly lines out and it's dope.
[02:40:36.480 --> 02:40:37.480]   It's dope, man.
[02:40:37.480 --> 02:40:38.480]   It's dope.
[02:40:38.480 --> 02:40:43.400]   And don't forget always day one, how the tech Titans plan to stay on top.
[02:40:43.400 --> 02:40:46.520]   Alex is excellent book speaking of books.
[02:40:46.520 --> 02:40:52.160]   Before he doctors, choke point capitalism is now available.
[02:40:52.160 --> 02:40:56.360]   I'm ashamed to say I read it on Kindle unlimited, but I understand that you've pulled a plug
[02:40:56.360 --> 02:40:57.360]   on that.
[02:40:57.360 --> 02:40:59.680]   That's the thing to do.
[02:40:59.680 --> 02:41:08.160]   Buy it from from choke point capitalism.com or Corey's really fantastic pluralistic blog,
[02:41:08.160 --> 02:41:12.440]   which I love or anywhere else books are sold just to be clear.
[02:41:12.440 --> 02:41:13.440]   Okay.
[02:41:13.440 --> 02:41:14.440]   You don't mind.
[02:41:14.440 --> 02:41:15.440]   You don't mind if it's somewhere else.
[02:41:15.440 --> 02:41:16.440]   Okay.
[02:41:16.440 --> 02:41:19.040]   I'm sure where finer books are sold.
[02:41:19.040 --> 02:41:21.720]   And don't forget pluralistic.net.
[02:41:21.720 --> 02:41:24.400]   Corey has yet to use Dolly for his illustrations.
[02:41:24.400 --> 02:41:27.800]   Oh, no, I use them in bits and pieces.
[02:41:27.800 --> 02:41:28.800]   So do you?
[02:41:28.800 --> 02:41:29.800]   Oh, okay.
[02:41:29.800 --> 02:41:30.800]   Yeah.
[02:41:30.800 --> 02:41:37.840]   Like, and that illustration for today's medium column with the with the TED talk stage, I
[02:41:37.840 --> 02:41:39.960]   couldn't find a good image of a TED talk stage.
[02:41:39.960 --> 02:41:42.960]   So I said empty TED talk stage.
[02:41:42.960 --> 02:41:44.040]   And I got that.
[02:41:44.040 --> 02:41:46.160]   And then everything else came from it.
[02:41:46.160 --> 02:41:52.800]   And my thread about Palantir in the NHS haunted NHS hospital was my prompt to Dolly.
[02:41:52.800 --> 02:41:53.800]   And then everything else.
[02:41:53.800 --> 02:41:59.080]   That was really creepy that that one is really really creepy.
[02:41:59.080 --> 02:42:03.600]   And the Ted, did you ask for a donkey in the TED talk or did it just no, no, no, I shoot
[02:42:03.600 --> 02:42:04.600]   that.
[02:42:04.600 --> 02:42:09.000]   So the TED stage is is is Dolly.
[02:42:09.000 --> 02:42:12.840]   The jeans are from a public domain source.
[02:42:12.840 --> 02:42:15.400]   The torso is Steve Jobs's torso.
[02:42:15.400 --> 02:42:16.400]   I thought it might be.
[02:42:16.400 --> 02:42:17.400]   Yeah.
[02:42:17.400 --> 02:42:19.880]   Or that looks like the zombie eternal neck.
[02:42:19.880 --> 02:42:20.880]   Yeah.
[02:42:20.880 --> 02:42:24.920]   And the donkey and the clown shoes are both creative commons images.
[02:42:24.920 --> 02:42:25.920]   And they're all.
[02:42:25.920 --> 02:42:27.360]   This is what you mean by a centaur.
[02:42:27.360 --> 02:42:28.360]   I think right.
[02:42:28.360 --> 02:42:29.360]   Human that.
[02:42:29.360 --> 02:42:30.360]   And with the.
[02:42:30.360 --> 02:42:32.600]   The chicken is a shit of Ted.
[02:42:32.600 --> 02:42:34.120]   Do you and shoot?
[02:42:34.120 --> 02:42:36.480]   Is that the official terminology?
[02:42:36.480 --> 02:42:37.480]   That's the verb for.
[02:42:37.480 --> 02:42:39.400]   Yeah, like you can tell by the pixels.
[02:42:39.400 --> 02:42:42.480]   That's when you that's when you go in and mod an image with Photoshop.
[02:42:42.480 --> 02:42:43.680]   You have shooped it.
[02:42:43.680 --> 02:42:46.080]   But in my case, I gimped it.
[02:42:46.080 --> 02:42:49.560]   Because you are the open source guy.
[02:42:49.560 --> 02:42:51.480]   Always a pleasure to have you on.
[02:42:51.480 --> 02:42:52.480]   Yeah.
[02:42:52.480 --> 02:42:53.480]   Thank you.
[02:42:53.480 --> 02:42:54.480]   Thank you, Leo.
[02:42:54.480 --> 02:42:55.480]   Yeah.
[02:42:55.480 --> 02:42:57.520]   It was a and I liked that you ended this with your own controversial take that World War
[02:42:57.520 --> 02:42:59.160]   Three is bad.
[02:42:59.160 --> 02:43:03.720]   It's going to be a bad thing for children and other and other living creatures.
[02:43:03.720 --> 02:43:04.720]   Yeah.
[02:43:04.720 --> 02:43:05.720]   Unhealthy for children.
[02:43:05.720 --> 02:43:06.720]   Yeah.
[02:43:06.720 --> 02:43:08.480]   You remember that poster you old hippie you.
[02:43:08.480 --> 02:43:09.480]   Yeah.
[02:43:09.480 --> 02:43:11.000]   I had one in my bedroom growing up.
[02:43:11.000 --> 02:43:13.520]   I figured you did a little flower drawing.
[02:43:13.520 --> 02:43:14.520]   Yeah.
[02:43:14.520 --> 02:43:15.520]   Yeah.
[02:43:15.520 --> 02:43:16.520]   Hey, thank you.
[02:43:16.520 --> 02:43:17.520]   Both of you.
[02:43:17.520 --> 02:43:18.520]   Really a pleasure.
[02:43:18.520 --> 02:43:21.320]   I knew that with the two of you, I didn't need anybody else.
[02:43:21.320 --> 02:43:22.320]   What a great show.
[02:43:22.320 --> 02:43:24.320]   Alex Cantor with Corey Doctorow.
[02:43:24.320 --> 02:43:25.320]   Thanks for being here.
[02:43:25.320 --> 02:43:26.320]   Thanks to all of you.
[02:43:26.320 --> 02:43:28.440]   I think you're probably glad you were here for this as well.
[02:43:28.440 --> 02:43:33.920]   We do Twitter every Sunday afternoon, 2 p.m. Pacific 5 p.m. Eastern 2100 UTC.
[02:43:33.920 --> 02:43:36.960]   You can tune in and watch it live at live.twit.tv.
[02:43:36.960 --> 02:43:39.480]   If you're watching live, our IRC is open.
[02:43:39.480 --> 02:43:43.240]   Both Alex and Corey were in the IRC actively participating.
[02:43:43.240 --> 02:43:44.240]   That's fantastic.
[02:43:44.240 --> 02:43:45.720]   IRC.twit.tv.
[02:43:45.720 --> 02:43:51.040]   They're even putting in plugs for where you can buy the book, Corey.
[02:43:51.040 --> 02:43:52.840]   They love you.
[02:43:52.840 --> 02:43:57.120]   And also, if you're a member of Club Twit, you can do it in the Discord.
[02:43:57.120 --> 02:44:02.480]   After the fact, shows are available at the website at supported twit.tv.
[02:44:02.480 --> 02:44:05.720]   Also on YouTube, there's dedicated YouTube channel for all of our shows.
[02:44:05.720 --> 02:44:07.560]   And of course, the best way to get it.
[02:44:07.560 --> 02:44:11.760]   In my opinion, be to find a podcast player and subscribe.
[02:44:11.760 --> 02:44:15.560]   And that way you get it automatically every Sunday night, just in time for your Monday
[02:44:15.560 --> 02:44:22.160]   morning commute from the bedroom to the living room.
[02:44:22.160 --> 02:44:26.520]   Oh, it's a sad old world.
[02:44:26.520 --> 02:44:28.440]   It's a sad old world these days.
[02:44:28.440 --> 02:44:29.440]   I'll tell you.
[02:44:29.440 --> 02:44:30.440]   Thank you everybody for joining us.
[02:44:30.440 --> 02:44:31.440]   I'll see you next time.
[02:44:31.440 --> 02:44:32.440]   Another twit is in the game.
[02:44:32.440 --> 02:44:33.440]   It's amazing.
[02:44:33.440 --> 02:44:34.440]   It's amazing.
[02:44:34.440 --> 02:44:44.440]   [MUSIC]

