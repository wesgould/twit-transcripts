;FFMETADATA1
title=The J to J Protocol
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=793
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:07.680]   The time for Twit this week in Tech. Oh boy. Oh boy. Do not listen to the show while driving.
[00:00:07.680 --> 00:00:13.120]   I want you to put your thinking caps on. I have two of the smartest people we ever
[00:00:13.120 --> 00:00:18.720]   have had on the network. Two of my favorite guests. Amy Webb's here. Corey Doctorow's here.
[00:00:18.720 --> 00:00:24.080]   I've been trying to get these two together. We are going to talk about privacy. Big Tech.
[00:00:24.080 --> 00:00:29.840]   What we can do about it. What we should do about it. The role of artists. The role of intellectuals.
[00:00:29.840 --> 00:00:35.840]   This is going to be a wild ride. Fasten your seat belts. Twit is next.
[00:00:35.840 --> 00:00:41.760]   This week in Tech comes to you from Twit's LastPass Studios. Securing every access point in your
[00:00:41.760 --> 00:00:47.360]   company does not have to be a challenge. LastPass unifies access and authentication to make securing
[00:00:47.360 --> 00:00:54.080]   your employees simple, and secure. Even when they're working remotely, check out lastpass.com/twit
[00:00:54.080 --> 00:01:03.440]   to learn more. Podcasts you love. From people you trust. This is Twit.
[00:01:03.440 --> 00:01:18.720]   This is Twit this week in Tech. Episode 793 recorded Sunday, October 18th 2020,
[00:01:18.720 --> 00:01:25.840]   the J2J Protocol. This episode of this week in Tech is brought to you by ExpressVPN.
[00:01:25.840 --> 00:01:30.880]   Protect your online privacy with one click for three extra months free with a one-year package.
[00:01:30.880 --> 00:01:40.880]   Go to expressvpn.com/twit. And by ExtraHop. ExtraHop helps you keep your business secure and
[00:01:40.880 --> 00:01:46.160]   available with SaaS-based cloud native network detection and response. Learn more about how
[00:01:46.160 --> 00:01:54.320]   ExtraHop stops breaches 70% faster and experience a free trial for yourself at extrahop.com/twit.
[00:01:54.320 --> 00:02:01.520]   And by Wasabi Hot Cloud Storage. Thinking about moving your data storage to the cloud? Wasabi
[00:02:01.520 --> 00:02:07.280]   his enterprise class cloud storage at one-fifth the price of Amazon S3 and faster than the competition
[00:02:07.280 --> 00:02:13.840]   with no fees for egress or API requests and no complex storage tiers. Start your free trial at
[00:02:13.840 --> 00:02:17.920]   wasabi.com. And please don't forget to use the offer code TWIT.
[00:02:17.920 --> 00:02:30.320]   It's time for Twit this week in Tech. The show will recover the week's news and technology.
[00:02:30.320 --> 00:02:35.600]   Stand back kids. This is one for the ages. You're going to want to put your thinking caps on because
[00:02:35.600 --> 00:02:40.720]   I have the two smartest people I know in the room. Well they're not there only figuratively in
[00:02:40.720 --> 00:02:45.280]   the room be there in their own rooms because you know nobody can be in the same room with me anymore.
[00:02:45.280 --> 00:02:51.600]   Apparently. Joining us right now from the Future Today Institute. The wonderful Amy Webb. It's great
[00:02:51.600 --> 00:02:58.800]   to see you. Amy Webb.io. Hey Leo. The signals are talking her book about being a futurist.
[00:02:58.800 --> 00:03:05.360]   Her latest that what is the gang of five? No what is the name of the big nine? The big nine. There's
[00:03:05.360 --> 00:03:13.680]   nine not five. What am I saying? The big nine and soon a microbiology or not micro. What kind of
[00:03:13.680 --> 00:03:19.520]   biology synthetic. It's a book about the futures of synthetic biology. It's called re I think it's
[00:03:19.520 --> 00:03:25.600]   called re re writing life. That's why it should not be called rewriting life because you can't say
[00:03:25.600 --> 00:03:33.760]   no I like it rewriting life is good just practice. The big big nine is currently out and available
[00:03:33.760 --> 00:03:38.560]   everywhere. Amy's been on triangulation a couple of times. So has this guy Corey Doctorow who is
[00:03:38.560 --> 00:03:46.080]   the author of too many books to count. Welcome. Plur at his new site is pluralistic.net. This is
[00:03:46.080 --> 00:03:52.000]   the new one. A tax surface and he's doing something very unusual with this. He's doing the audiobook
[00:03:52.000 --> 00:03:57.440]   via Kickstarter. Why is that Corey? Welcome by the way. I have done I should say is it over?
[00:03:57.440 --> 00:04:02.960]   It the Kickstarter is over. Yeah. It did very well. $270,000. Wow.
[00:04:02.960 --> 00:04:11.840]   The I as a doctrinal matter don't allow DRM on my work. There is one company that dominates
[00:04:11.840 --> 00:04:16.640]   the audiobook market audible and they have a mandatory DRM requirement. I say that that's there
[00:04:16.640 --> 00:04:20.800]   to protect us authors. And of course if you want to remove the DRM from an audio audible book you
[00:04:20.800 --> 00:04:26.400]   just type into your favorite search engine. Very easy. But under section 12 one of the
[00:04:26.400 --> 00:04:30.400]   Digital Learning Copyright Act it's a felony punishable by five-year prison sentence and a
[00:04:30.400 --> 00:04:36.880]   $500,000 fine to tell someone how to remove their DRM. And so what that means is that I as the author
[00:04:36.880 --> 00:04:44.240]   if I sell you an audiobook on audible and locking you to Audible's platform forever. And I can't
[00:04:44.240 --> 00:04:48.960]   authorize you to take my work and leave it. So if I ever get into a situation where I feel like
[00:04:48.960 --> 00:04:54.160]   Amazon's offering me a deal I don't want to take and I try to go to one of its rivals you have to
[00:04:54.160 --> 00:04:58.880]   surrender your books to come follow me somewhere else or maintain two separate and parallel sets
[00:04:58.880 --> 00:05:03.680]   of infrastructure to do it. So this is terrible and I don't like it. Amazon promised they would
[00:05:03.680 --> 00:05:08.480]   drop Audible's DRM when they bought Audible in 2008. I've been waiting for 12 years. I'm not
[00:05:08.480 --> 00:05:13.920]   holding my breath anymore. And you know my publishers Macmillan they're very nice people but they had
[00:05:13.920 --> 00:05:19.360]   the very sensible position that if I wouldn't allow them to sell my audiobooks in the place where
[00:05:19.360 --> 00:05:24.720]   90% of the customers were they didn't want to pay me for those rights. And I was like that is
[00:05:24.720 --> 00:05:28.080]   perfectly fair. And they said but you can keep them and you can make your own audiobook. That's
[00:05:28.080 --> 00:05:34.640]   very generous. Yeah. Yeah. So I went out I got Amber Benson who you may know as Tara from Buffy but
[00:05:34.640 --> 00:05:40.720]   she's also a brilliant novelist. I got her to record it at Skyboat Media which is a tremendous
[00:05:40.720 --> 00:05:45.280]   audiobook studio here in Southern California. And we had my sound editor John Taylor Williams
[00:05:45.280 --> 00:05:52.000]   who's been editing my stuff for 15 years do the edit. And I presold it on Kickstarter along with
[00:05:52.000 --> 00:05:58.080]   the ebook which I sell on behalf of my publishers. So if you buy it for me I give Macmillan 70% I
[00:05:58.080 --> 00:06:03.040]   take the 30% Jeff Bezos would normally get. And then my publisher takes 25% of that which is the
[00:06:03.040 --> 00:06:07.520]   author's share and sends it back to me as a royalty comes out to like 48 and a half percent.
[00:06:08.640 --> 00:06:14.240]   Which is a it's a it's a it's a it's not bad. So that's not bad. So I sold it. I sold the first
[00:06:14.240 --> 00:06:21.200]   two audiobooks. I sold all three ebooks. I sold the new audiobook and all together we raised $270,000
[00:06:21.200 --> 00:06:27.040]   including three $10,000 pledges for commissions to write new little brother short stories which
[00:06:27.040 --> 00:06:31.520]   are going to be the nucleus of a new short story collection. So I got another book out of it as well.
[00:06:31.520 --> 00:06:36.800]   Nice. This newest attack surface is the third in the Little Brother series. Yeah.
[00:06:36.800 --> 00:06:40.720]   And a highly book for adults. Highly recommend. Yeah. Highly recommended.
[00:06:40.720 --> 00:06:45.600]   So that's good. So if people want the audiobook do they go to the Kickstarter pages or somewhere
[00:06:45.600 --> 00:06:51.200]   else? No, that's that window has closed. So that was your $15 preorder price. Now it's a $25
[00:06:51.200 --> 00:06:58.880]   audiobook that now that the preorder advance price is over. And you can get that it's an audible
[00:06:58.880 --> 00:07:02.960]   exclusive which is to say it's exclusive of audible. You can get anywhere that isn't audible.
[00:07:04.000 --> 00:07:09.120]   You recommended the last time you're on Libro.fm which gives a little kick back to your local
[00:07:09.120 --> 00:07:12.960]   bookstore. I chose copper fields my local bookstore. So that's kind of a nice feeling
[00:07:12.960 --> 00:07:18.240]   when you have a book store at craphound.com/shop where you can buy all of my audiobooks and all
[00:07:18.240 --> 00:07:25.120]   of my ebooks. And again, it's that deal where I get Jeff Bezos's cut and then my publisher sends
[00:07:25.120 --> 00:07:32.320]   me back my royalty. It's only fair that you should get Jeff Bezos's cut. Yes. I promise to use it
[00:07:32.320 --> 00:07:36.720]   in ways that are more sensible than he does. He says he doesn't know what to do with his money. He'll
[00:07:36.720 --> 00:07:43.360]   have to do space exploration. And I have ideas for what he could do with his money. And if you give
[00:07:43.360 --> 00:07:51.120]   that money to me, I will spend it accordingly. craphound.com/amiweb.io. And now that ends the show. So thank
[00:07:51.120 --> 00:07:55.040]   you very, no, I have a lot more to talk about. I don't know where to start. Normally, you know,
[00:07:55.040 --> 00:08:00.480]   we would talk about the iPhone event. But when I have two big minds, big brains here like this,
[00:08:00.480 --> 00:08:07.600]   that seems kind of trivial to talk about. I want to talk actually about the story you put in slate
[00:08:07.600 --> 00:08:14.960]   this week, Corey, if you don't mind. Because it kind of rang a bell with me, it kind of hit home.
[00:08:14.960 --> 00:08:25.600]   The dangers of cynical sci-fi disaster stories. It seems like that's all we're seeing these days.
[00:08:25.600 --> 00:08:31.920]   And certainly, if you're watching sci-fi on TV, on Amazon, Primer, Netflix, or HBO Max,
[00:08:31.920 --> 00:08:40.800]   it's dystopia. As far as the eye can see, you've written your share of cynical sci-fi disaster stories.
[00:08:40.800 --> 00:08:48.080]   Why are you going to stop? Well, I would say that I have written my share of cynical sci-fi
[00:08:48.080 --> 00:08:53.520]   disasters. If not, the whole story was cynical. I tend to be a pretty idealistic and optimistic,
[00:08:53.520 --> 00:09:00.720]   or at least hopeful person. I, like everybody, really enjoy a zombie apocalypse story.
[00:09:00.720 --> 00:09:03.840]   Right. There is something really exciting. We're living in it. Yeah.
[00:09:03.840 --> 00:09:10.800]   Sure. Yes. And you know, I'm a pulp writer. I write science fiction. I'm a pulp reader. I read
[00:09:10.800 --> 00:09:15.200]   science fiction. That means I like a lot of plot in my fiction. And usually, you know,
[00:09:15.200 --> 00:09:20.400]   the two plots are person versus nature and person versus person. And if you're trying to
[00:09:20.400 --> 00:09:24.080]   really lean into the plot, you could do person versus nature versus person.
[00:09:24.080 --> 00:09:28.000]   And that's the story where the tsunami knocks your house down, your neighbors come over to eat you.
[00:09:28.000 --> 00:09:31.840]   Yeah. There you go. And those stories, I've really enjoyed that. There's so many of those
[00:09:31.840 --> 00:09:36.240]   that I've read that I've enjoyed. And there's a little brother actually opens with a scene like
[00:09:36.240 --> 00:09:40.560]   that where they're in a jury after a terrorist attack, they're down in a BART station. And
[00:09:40.560 --> 00:09:46.640]   someone just randomly stabs someone else just because they can. And I think the problem with
[00:09:46.640 --> 00:09:51.280]   that is twofold. One is like a verusimilitude problem. Actually, let's call it threefold.
[00:09:51.280 --> 00:09:54.080]   One's a verusimilitude problem because generally, we're pretty good to each other.
[00:09:54.080 --> 00:10:00.240]   You know, Rebecca Solnit's history of disasters, Paradise, Built in Hell, very closely researched
[00:10:00.240 --> 00:10:05.920]   history of just people being really kind to each other and really stepping up during disasters.
[00:10:05.920 --> 00:10:11.440]   And of the countervailing force, which is not humanity's cruelty, but rather what she calls
[00:10:11.440 --> 00:10:16.400]   elite panic, the conviction on the part of wealthy people that when the lights go out,
[00:10:16.400 --> 00:10:20.640]   the pores are coming for them. And so this preemptive strike, you may remember like
[00:10:20.640 --> 00:10:27.840]   Blackwater mercenary shooting people in New Orleans after the hurricane. That sort of preemptive
[00:10:27.840 --> 00:10:34.560]   reaction, getting your revenge in first is really what turns crises into catastrophes.
[00:10:35.600 --> 00:10:44.000]   So it's wrong. It's also the wrong intuition pump that when crisis strikes and we have to
[00:10:44.000 --> 00:10:48.720]   make a very quick decision about what's going to go on, we lean on the things that are most
[00:10:48.720 --> 00:10:54.080]   easy to imagine. And if every time you have seen a story about a disaster, you've been primed to
[00:10:54.080 --> 00:10:58.560]   believe that this is the time when your neighbor is coming over with a shotgun gun, you would be
[00:10:58.560 --> 00:11:03.360]   an idiot to take the thawing food in your freezer, make a stew, and go over to your neighbor's
[00:11:03.360 --> 00:11:06.960]   house with a covered dish. But if you think that they're coming over with a covered dish,
[00:11:06.960 --> 00:11:11.920]   then everybody's good, right? Everybody shares. And that's how you get started, because of course,
[00:11:11.920 --> 00:11:17.440]   the answer to a disaster is not a total war of all against all. The answer to a disaster is like
[00:11:17.440 --> 00:11:21.120]   getting the sanitation running again and how to distribute masks.
[00:11:21.120 --> 00:11:24.560]   It kind of feels like we're in that crux right now where we could go either way.
[00:11:24.560 --> 00:11:30.080]   It sure feels like it. It really does. And so, and then that's the last part is that it's bad art,
[00:11:30.080 --> 00:11:35.440]   because there are lots of ways that people of goodwill can disagree with each other in ways
[00:11:35.440 --> 00:11:43.520]   that are irreconcilable, and that can make for stories that are really tense and really good,
[00:11:43.520 --> 00:11:49.200]   really plot driven, but that are truer to our human nature, which is that for the most part,
[00:11:49.200 --> 00:11:54.400]   people are good and want to help, but they may have a genuinely irreconcilable difference about
[00:11:54.400 --> 00:12:00.000]   what that help amounts to. So, a tax surface is a book about this young woman, Masha, who
[00:12:00.000 --> 00:12:04.000]   lives through the terrorist attack, that is the inciting incident in Little Brother,
[00:12:04.000 --> 00:12:09.600]   and rather than becoming an opponent of the DHS becomes a helper of the DHS to try and catch
[00:12:09.600 --> 00:12:14.560]   the terrorists, she then ends up in the second book, working in a forward operations base in Iraq as a
[00:12:14.560 --> 00:12:20.080]   like a beltway bandit, hunting jihadi's. And when we meet her in the third book, she's just
[00:12:20.080 --> 00:12:24.960]   gone full on cyber mercenary and she's working for a company much like Palantir or the NSO group,
[00:12:24.960 --> 00:12:31.040]   helping post-Soviet dictators crush pro-democracy movements. And she has to tell herself a story
[00:12:31.040 --> 00:12:36.560]   about why this is all okay. And that story is slowly collapsing and she has to do more and
[00:12:36.560 --> 00:12:41.280]   more extreme things. And eventually the story collapses and she ends up back in San Francisco,
[00:12:41.280 --> 00:12:45.600]   no longer employed doing this. And she discovers to her horror that her childhood best friend,
[00:12:45.600 --> 00:12:49.680]   who's now an activist and a successor movement of the movement for Black Lives,
[00:12:49.680 --> 00:12:54.720]   is being targeted with the same cyber weapons that she spent her career developing. And she
[00:12:54.720 --> 00:12:58.800]   has to like work out what it means to tell a story in which she's a good person, when she can still
[00:12:58.800 --> 00:13:05.600]   see the blowback and what she's going to do next to redeem herself. And I think that that's a much
[00:13:05.600 --> 00:13:12.640]   chewier, more interesting, higher stake story, a story in which winning may involve permanently
[00:13:12.640 --> 00:13:18.240]   alienating yourself from people you like and who you want to be on the same side of. But
[00:13:18.240 --> 00:13:23.520]   there's no way that both of you can have it your way and the issue matters so much that neither of
[00:13:23.520 --> 00:13:32.400]   you can give on it. That's a really tense, important, great, pulpy story. And it's a story that I'm
[00:13:32.400 --> 00:13:36.400]   much more interested in reading these days, not least because it primes us to understand
[00:13:36.400 --> 00:13:41.840]   what it is our neighbors are doing when crisis hits and they have a different idea
[00:13:41.840 --> 00:13:48.560]   to our ideas about what we should do about it. Because we're in the path of many crises to come.
[00:13:49.760 --> 00:13:55.600]   And so we really do need to figure out what our intuition around our neighbors during crisis
[00:13:55.600 --> 00:14:01.520]   should be. Otherwise, it could go much worse than even the negative projections that I think keep
[00:14:01.520 --> 00:14:08.320]   all of us at night. I can't wait to read it. It sounds really interesting. What a premise.
[00:14:08.320 --> 00:14:13.760]   Right, Amy? I mean, that's deep. By the way, I don't know. I should have introduced you.
[00:14:13.760 --> 00:14:17.600]   Do you have you guys ever met before Amy? We just met briefly before?
[00:14:17.600 --> 00:14:23.600]   Now, okay. I mean, I think we are sort of oscillating around the same circles of other people,
[00:14:23.600 --> 00:14:28.000]   but we haven't met yet. Well, it's great to get you the two of you together. What do you think
[00:14:28.000 --> 00:14:35.360]   about the new iPhone? Never mind. Never mind. It's hopeless. I like that they finally brought back
[00:14:35.360 --> 00:14:39.840]   headphones. That's cool. Yeah. Go ahead, Amy. What?
[00:14:39.840 --> 00:14:44.960]   I was going to say, I'd love to pick up on the thread about dystopian futures. And
[00:14:45.520 --> 00:14:51.440]   just as a sort of reminder that we tend to go throughout history, that the pendulum swings
[00:14:51.440 --> 00:14:58.160]   and rarely does it fall in the center where we're satisfied with pragmatism.
[00:14:58.160 --> 00:15:07.120]   So in a sense, it's not strange to me that we've spent so long telling each other stories about
[00:15:07.120 --> 00:15:14.720]   utter horror stories, zombie apocalypse is the dystopia somehow is getting darker and darker.
[00:15:15.520 --> 00:15:23.120]   I've been watching the boys the second season. I got through that whatever second episode.
[00:15:23.120 --> 00:15:29.440]   And it was I remember I did not get to this kind of a strange name for a feminist character.
[00:15:29.440 --> 00:15:37.120]   So I was momentarily going to bed that night feeling like somewhat vindicated. And then
[00:15:37.120 --> 00:15:45.440]   obviously things take a real turn. Here's the point though. I think we arrived in this place,
[00:15:45.440 --> 00:15:50.560]   not just in the stories that we tell, but the actual real world problems that we have because
[00:15:50.560 --> 00:15:57.680]   it began with a total utopian outlook, a utopian thinking. Certainly the internet and technology.
[00:15:57.680 --> 00:16:01.040]   We really thought this was going to make the world a better place. That's right. And the problem with
[00:16:02.320 --> 00:16:12.880]   I think anchoring ourselves in optimism or utopia is that we then fail to recognize that there can
[00:16:12.880 --> 00:16:20.080]   be also negative next order impacts when either you're not planning for real world issues, real
[00:16:20.080 --> 00:16:25.200]   world problems that could result or you think that somehow everything will be okay. And I don't
[00:16:25.200 --> 00:16:31.120]   mean the sort of norm and Vincent Peale power positive thinking, but I do think back to Wells
[00:16:31.760 --> 00:16:38.560]   and at the turn of the century, a lot of those very, very, very early science fiction writers
[00:16:38.560 --> 00:16:45.120]   were writing about utopia. They were writing about science and evidence based utopia where
[00:16:45.120 --> 00:16:49.680]   everything would be amazing. And part of why everything was so amazing was because of eugenics.
[00:16:49.680 --> 00:17:00.560]   So I agree that maybe we don't need to continue. We need a better balance. But my concern is I
[00:17:00.560 --> 00:17:06.720]   watch us sort of go back and forth and people who are writing about science and they're writing
[00:17:06.720 --> 00:17:15.360]   sci-fi or they're working in the area of futures, it seems to be zero sum. And if we swing totally
[00:17:15.360 --> 00:17:21.920]   in the other direction, I think we wind up to some extent 20 years from now back at where we started.
[00:17:23.760 --> 00:17:32.160]   Yeah, I have a complicated relationship with technological utopianism because it's a criticism
[00:17:32.160 --> 00:17:36.560]   that's often leveled at the sort of what you might call the digital rights community that
[00:17:36.560 --> 00:17:41.760]   back in the old days, we thought if we could just connect everybody, it would be fine. And then
[00:17:41.760 --> 00:17:46.080]   we failed to understand that it would end up like it did. And I do think that the digital
[00:17:46.080 --> 00:17:51.760]   rights community made some missteps. But I don't think that they were in failing to understand the
[00:17:51.760 --> 00:17:56.800]   potential risks of technology. Remember that organizations like the Electronic Frontier
[00:17:56.800 --> 00:18:04.320]   Foundation or the Free Software Foundation have their grounding not merely in aspirations for the
[00:18:04.320 --> 00:18:09.920]   kinds of self-realization and self-actualization and community that we could get with technology,
[00:18:09.920 --> 00:18:16.400]   but also in fear for what it would mean to have a technological society without them. What it would
[00:18:16.400 --> 00:18:21.680]   mean to have ubiquitous networks, but no cryptography so that universal surveillance would become a
[00:18:21.680 --> 00:18:29.200]   commonplace of the future. That is what animates those movements. And if you think that the future
[00:18:29.200 --> 00:18:33.840]   will take care of itself, the technology is going to sort out all the problems, you don't start an
[00:18:33.840 --> 00:18:39.120]   organization like Electronic Frontier Foundation, you just go off and do a startup or something
[00:18:39.120 --> 00:18:44.640]   because it's all going to take care of itself. It's this idea that Michael Weinberger had when
[00:18:44.640 --> 00:18:49.600]   he wrote this paper for Public Knowledge on 3D Printing, which is it will all be so great if
[00:18:49.600 --> 00:18:55.840]   we don't screw it up. I really feel like that's the animating ideology of at least the digital
[00:18:55.840 --> 00:19:00.960]   rights world. But I do think that we did get one thing wrong, which is that we failed to understand
[00:19:00.960 --> 00:19:10.320]   that we had passed the heroic age of anti-monopoly enforcement. And we didn't understand exactly
[00:19:10.320 --> 00:19:15.280]   what that would mean. After all, if you got your first computer around the time of the IBM PC,
[00:19:15.280 --> 00:19:22.640]   you were enjoying a computer ecosystem in which the largest tech company in the history of the
[00:19:22.640 --> 00:19:28.160]   world was too frightened to bundle its own operating system with its flagship computer
[00:19:28.160 --> 00:19:32.880]   because it might attract antitrust attention. They had just gone through a 12-year antitrust
[00:19:32.880 --> 00:19:38.800]   enforcement action in which they had outspent the entire DOJ's budget for antitrust for all
[00:19:38.800 --> 00:19:45.200]   DOJ cases in that one case every year IBM spent that much. And in which IBM had exercised for
[00:19:45.200 --> 00:19:50.240]   parents when Tom Jennings, the brilliant computer engineer who also started FidoNet,
[00:19:50.240 --> 00:19:56.320]   reverse engineered the ROM and the PC for Phoenix computers so that they could sell it to Dell and
[00:19:56.320 --> 00:20:00.320]   Gateway and all those other companies to make their own computers. And I think if you grew up
[00:20:00.320 --> 00:20:06.480]   with computers in that era, you quite naturally might have assumed that if a company became a
[00:20:06.480 --> 00:20:11.360]   monopolist that prevented the kind of dynamic turnover that was common in an era where one
[00:20:11.360 --> 00:20:16.320]   day Altavista was on top and the next day was Yahoo, the next day was Google that the DOJ might
[00:20:16.320 --> 00:20:23.680]   exercise some enforcement against them. And instead, after Microsoft petered out, I still think the
[00:20:23.680 --> 00:20:28.480]   Microsoft case did some good, but after Microsoft petered out, that was it. We just let companies
[00:20:28.480 --> 00:20:33.920]   do the most nakedly anti-competitive stuff by their competitors, merge with major competitors,
[00:20:33.920 --> 00:20:40.080]   create vertical monopolies and end up in a world where the web is five giant websites filled with
[00:20:40.080 --> 00:20:45.840]   screenshots from the other four and surveillance is ubiquitous, not least because states rely on
[00:20:45.840 --> 00:20:51.440]   private companies to gather the data that they then plunder for state surveillance. And so there's
[00:20:51.440 --> 00:20:59.200]   no hope of states curbing the surveillance impulse of tech companies for so long as there are prime
[00:20:59.200 --> 00:21:06.000]   beneficiary of it. Corey, I totally agree with you. However, when I hear the same argument being made,
[00:21:06.000 --> 00:21:11.920]   it is with reference to the current investigations, at least the ones being levied in the United States,
[00:21:11.920 --> 00:21:17.760]   where the calls tend to be, let's break up these companies. The challenge, of course, is that,
[00:21:17.760 --> 00:21:26.320]   again, are the mechanisms that we have for regulation were developed in a time when company
[00:21:26.320 --> 00:21:33.840]   structures were different, and the IP that companies were producing weren't so deeply embedded into
[00:21:33.840 --> 00:21:39.760]   all of the rest of their operations. The United States, as a lot of countries,
[00:21:39.760 --> 00:21:47.840]   the US doesn't have its own cloud. The federal government runs on AWS. The United States has
[00:21:47.840 --> 00:21:54.480]   stopped heavily investing in R&D, and quite frankly, not just within tech, but in science as well.
[00:21:55.520 --> 00:22:05.840]   Everybody carries much, much more about the D than they are. So with funding for basic science
[00:22:05.840 --> 00:22:10.720]   and research being stripped, the question that we have to ask ourselves is if we don't want these
[00:22:10.720 --> 00:22:15.440]   monopolies, which I think everybody at this point generally agrees that we do not, because today
[00:22:15.440 --> 00:22:20.320]   we're talking about tech tomorrow, it will be the frontiers of synthetic biology and other types of
[00:22:21.680 --> 00:22:28.480]   technologies. The question is, how do we keep the research moving forward when we've essentially
[00:22:28.480 --> 00:22:35.200]   outsourced it to the private sector and engineer the future so that it looks different? And I think
[00:22:35.200 --> 00:22:40.240]   the problem is that we don't have the right, we're not engaged in the right conversation. So
[00:22:40.240 --> 00:22:46.720]   it can't be as simple as we screwed up over the past 40 years, the way we're going to fix it is
[00:22:46.720 --> 00:22:51.600]   that the way we fixed it before, which is through the traditional antitrust and anti-competitiveness
[00:22:52.400 --> 00:22:55.680]   regulatory frameworks, because I just don't think they're going to, I think what's going to happen
[00:22:55.680 --> 00:23:00.800]   is we're going to wind up in court for a whole bunch of years. And I don't know that we will have
[00:23:00.800 --> 00:23:04.800]   solved the problems waiting for us on the other side of the horizon.
[00:23:04.800 --> 00:23:12.160]   Don't you feel that whatever Congress does, they're impotent against the vertical monopolies
[00:23:12.160 --> 00:23:16.800]   created by Facebook and Google and Amazon and Apple, and I'll add Microsoft into the mix,
[00:23:16.800 --> 00:23:22.720]   even though the government. We don't have, so I've been fighting hard and lobbying for an office of
[00:23:22.720 --> 00:23:27.520]   what I call a national office of strategic foresight. There was something called the Office of Technology
[00:23:27.520 --> 00:23:34.320]   Assessment, which Gingrich defunded in the 90s with his contract on America. And the challenge
[00:23:34.320 --> 00:23:43.200]   is that that left a significant void. And, you know, Ajit Pai, suddenly toying around with
[00:23:43.200 --> 00:23:48.160]   Section 230, I was just, I was actually having no legal basis to do, I might add.
[00:23:48.160 --> 00:23:54.000]   Now he and I were in a meeting on Thursday, Wednesday and in the green.
[00:23:54.000 --> 00:23:56.400]   Chairman of the FCC. Yeah. Did you confront him?
[00:23:56.400 --> 00:24:02.960]   Well, so he and I were speaking on a panel and we were in the green room before that. And, you know,
[00:24:02.960 --> 00:24:08.800]   listen, he's, this is the fundamental problem that we have is that these are all political
[00:24:08.800 --> 00:24:13.680]   appointees. Those are political appointments and we don't have a lot of,
[00:24:13.680 --> 00:24:19.760]   we don't have, I mean, we technically have some frameworks in how long people have to stay in
[00:24:19.760 --> 00:24:25.440]   office before they can swing back into the private sector. But it never works out that way. And
[00:24:25.440 --> 00:24:32.960]   what I think we need is some type of strategic office in place where people are in that space,
[00:24:32.960 --> 00:24:36.640]   they are highly educated and their job is to fill, you know,
[00:24:36.640 --> 00:24:41.120]   Yeah. But then they're not accountable. The whole point of political appointees in theory is that
[00:24:41.120 --> 00:24:46.480]   they are accountable ultimately to the voters, right? If you have this, some sort of strategic
[00:24:46.480 --> 00:24:51.040]   office that is just in there for life, who are they accountable to?
[00:24:51.040 --> 00:24:56.800]   Not, not for life, but for very long terms. We don't do long term planning in the US. And so,
[00:24:56.800 --> 00:25:01.520]   the reason why I brought this up was because we're not going to solve the current,
[00:25:02.080 --> 00:25:09.040]   we're not going to solve the past monopoly problem using our current situation in a way that makes
[00:25:09.040 --> 00:25:15.040]   sense for the future. Even the house report called for new monopoly rules.
[00:25:15.040 --> 00:25:23.040]   So, can I, I think that that's true but incomplete because I do think that there's a lot of space
[00:25:23.040 --> 00:25:28.720]   for our traditional antitrust remedies in existing competition problems. And one of the reasons I
[00:25:28.720 --> 00:25:35.120]   think that is when you look at the way the companies grew, you don't see the kind of exotic growth that
[00:25:35.120 --> 00:25:39.680]   that is character that is often spoken of when we hear about the tech companies. And we're told
[00:25:39.680 --> 00:25:43.600]   that they're large because of their network effects or their first move or advantage or
[00:25:43.600 --> 00:25:48.080]   something to do with, you know, longitudinal data sets or what have you. Instead, when you
[00:25:48.080 --> 00:25:53.280]   look at these companies, you find companies that by and large have grown through unlawful means,
[00:25:53.280 --> 00:25:57.120]   means that are unlawful under the plain language of the Sherman Act and the Clayton Act.
[00:25:57.120 --> 00:26:03.120]   Like additive acquisitions and it's leveraging their monopoly power in one market and another
[00:26:03.120 --> 00:26:08.960]   and that kind of thing. 100%. I mean, Google is a company that has made, depending on how you count
[00:26:08.960 --> 00:26:13.520]   one or two and a half successful products, right? They made a great search engine, a good hot mail
[00:26:13.520 --> 00:26:18.640]   clone. Maybe Android is something that was in a house because it was a camera OS when they bought
[00:26:18.640 --> 00:26:23.520]   it and then they fixed it. And maybe Google Photos, but of course it comes bundled with all of their
[00:26:23.520 --> 00:26:28.480]   devices. So can we really, like if you have a billion devices and you pre-install Google Photos on it,
[00:26:28.480 --> 00:26:35.120]   is that a success? But everything else that they've done that's successful was in house or rather
[00:26:35.120 --> 00:26:40.000]   was bought in rather and everything they've done in house crash and burnt, right? Wave and
[00:26:40.000 --> 00:26:45.600]   Plus and Orchid and all of those other things. And so to at least a first approximation,
[00:26:46.320 --> 00:26:54.720]   I think that it wouldn't hurt to try, not least because the existing rules to attempt at least to
[00:26:54.720 --> 00:27:00.960]   enforce them. Yeah, yeah, not least because we see concentration in other industries. And when you
[00:27:00.960 --> 00:27:05.360]   pick up the concentration, it got there the same way. That's how Pro Wrestling became one league.
[00:27:05.360 --> 00:27:09.440]   It's how beer became two companies. It's how movie studios became four. It's how we ended up with
[00:27:09.440 --> 00:27:14.480]   one movie theater chain. It's how we ended up with three record labels. I mean, it's five publishers,
[00:27:14.480 --> 00:27:18.800]   all of this stuff that they all have the same growth pattern. And so there would be an enormous
[00:27:18.800 --> 00:27:26.720]   social benefit irrespective of the specifics of the tech industry to restoring a vigorous
[00:27:26.720 --> 00:27:31.920]   presumption that predatory acquisitions, mergers to monopoly and vertical monopoly should not be
[00:27:31.920 --> 00:27:39.120]   permitted. And even the very muscular DOJ framework of the 30 glorious years, the post-war years,
[00:27:39.120 --> 00:27:44.400]   where if there was a five year period in which the structure of an industry remains static,
[00:27:44.400 --> 00:27:51.200]   where the company on top stayed on top, they would presumptively open an investigation into
[00:27:51.200 --> 00:27:55.520]   that company because if you manage to stay number one for five years, you were probably up to no good.
[00:27:55.520 --> 00:28:01.360]   Right? So that's like a good start. And I agree with Amy that this will produce long,
[00:28:01.360 --> 00:28:08.160]   difficult lawsuits. But they are a feature and not a bug because what they do is they terrorize
[00:28:08.160 --> 00:28:14.160]   executives, right? That Bill Gates being humiliated on the stand and having that go viral really
[00:28:14.160 --> 00:28:19.520]   changed the calculus within Microsoft. And Microsoft insiders and chroniclers at Microsoft and even
[00:28:19.520 --> 00:28:24.960]   Bill Gates own account suggest that the reason that the tech industry grew without the kind of
[00:28:24.960 --> 00:28:31.360]   stuff that Microsoft had done to Netscape in the years after the antitrust suit was in large part
[00:28:31.360 --> 00:28:37.760]   because they had lost their predatory instincts because whenever someone in the boardroom said,
[00:28:37.760 --> 00:28:42.640]   hey, let's go do something bad, the other people in the boardroom said, didn't you see what happened
[00:28:42.640 --> 00:28:47.280]   to Bill the last time they put him on the stand, go wash your mouth, that was so. And Bill Gates
[00:28:47.280 --> 00:28:53.600]   told Care Swisher last year that he didn't buy Android because he was distracted by the antitrust,
[00:28:53.600 --> 00:28:57.360]   but that was seven years after the antitrust. What he meant was that seven years later,
[00:28:57.360 --> 00:29:02.480]   he still had PTSD and he couldn't bring himself to do it. But there is something about tech that
[00:29:02.480 --> 00:29:08.880]   is suagenesis that's unique where we could bring remedies in, which is interoperability. And we
[00:29:08.880 --> 00:29:16.880]   have in the history of tech so many stories where tech giants were forced to reckon with competition
[00:29:16.880 --> 00:29:22.560]   and were forced to up their game or found themselves relegated to the trash heap of history because a
[00:29:22.560 --> 00:29:26.720]   new market entrant would come along. They had the seven dwarves who used to interoperate with
[00:29:26.720 --> 00:29:34.720]   AT&T servers. You had Microsoft doing the same to IBM, you had Apple cloning the iWorks suite,
[00:29:34.720 --> 00:29:40.880]   you had Google impersonating a browser and asking every web server in the world for a copy of every
[00:29:40.880 --> 00:29:47.360]   page it had, right? All of that interoperability where one thing plugs into an existing thing,
[00:29:47.360 --> 00:29:51.840]   irrespective of the wishes of the people who made it, and even if it gores their ox. And what we've
[00:29:51.840 --> 00:29:58.400]   seen in the years since monopolization is that the companies that relied on this have stripped
[00:29:58.400 --> 00:30:04.080]   future competitors of the same tools they used to grow, whether that's Facebook suing over
[00:30:04.080 --> 00:30:10.080]   enforceable terms of service after violating MySpace's terms of service, or Apple arguing for
[00:30:10.080 --> 00:30:15.440]   the enforceability of DRM laws so that no one can reverse engineer iTunes after they reverse
[00:30:15.440 --> 00:30:19.840]   engineered Microsoft's flagship products to make an interoperable version and so on.
[00:30:20.560 --> 00:30:25.920]   Every one of these companies has gone up the ladder and kicked it away and just stripping
[00:30:25.920 --> 00:30:31.200]   them of their legal power to block interoperators would go a long way I think to restoring some of
[00:30:31.200 --> 00:30:35.920]   the vigor that we used to get and force them to devote more energy to R&D because Amy's totally
[00:30:35.920 --> 00:30:41.520]   right. These companies with trillion dollar valuations, if you add up all their capital and then you
[00:30:41.520 --> 00:30:47.920]   add up their R&D, you still get something like 15 to 20% of their total market cap. The other
[00:30:47.920 --> 00:30:52.160]   80% is the expectation of the market that they will extract monopoly rents. That's what the
[00:30:52.160 --> 00:30:56.800]   intangibles are. And so we really do need to change that balance.
[00:30:56.800 --> 00:31:07.360]   Is your concern, Amy, that the political arm can't really effectively act because of politics?
[00:31:07.360 --> 00:31:12.640]   You know, we have conservatives on the one hand saying, well, the problem with big tech is they're
[00:31:12.640 --> 00:31:19.920]   censoring our speech. So they wouldn't sign off on the Democrats' proposals to break up these
[00:31:19.920 --> 00:31:25.360]   companies. The politics get in the way of a sensible solution. So is that why you prefer this
[00:31:25.360 --> 00:31:30.080]   kind of strategic planning committee? Right. I mean, listen, I'm a systems level thinker,
[00:31:30.080 --> 00:31:38.800]   so I tend to naturally think through many orders of, you know, so I think in terms of
[00:31:39.760 --> 00:31:48.000]   sort of many next steps in advance, right? So the challenge that I am seeing is if we continue
[00:31:48.000 --> 00:31:53.600]   down the path, trying to bake up, you know, with the rallying cry to break up big tech,
[00:31:53.600 --> 00:31:59.680]   the question is, how do you reverse engineer an outcome that is desirable, but that doesn't set
[00:31:59.680 --> 00:32:06.480]   us again on the wrong path? Instead of solving the problem, you just create a bunch more monolithic
[00:32:06.480 --> 00:32:11.920]   teammates. Or you, right, you wind up in court, you're a baby bells in effect. Interoperability,
[00:32:11.920 --> 00:32:16.640]   right. And well, but let's stop and think for a moment. You know, at that point, we were talking
[00:32:16.640 --> 00:32:22.640]   about telephony and what was then complicated, but not that sophisticated.
[00:32:22.640 --> 00:32:26.080]   Compared to this, yeah, communications infrastructure compared to now, right?
[00:32:26.080 --> 00:32:32.720]   You know, when you look at a company like Amazon, which everybody has missed what I think is the
[00:32:32.720 --> 00:32:37.200]   most compelling story and the most compelling story for Amazon has to do with the future of
[00:32:37.200 --> 00:32:46.720]   our health data that that, you know, nibbling at the corners of that without overtly pursuing it.
[00:32:46.720 --> 00:32:53.040]   Right. I don't think I listen, I dropped out of law school just before I was supposed to start.
[00:32:53.040 --> 00:32:57.840]   So I never went to law school. I'm totally unqualified to give my legal assessment on this.
[00:32:58.880 --> 00:33:05.920]   From my outsider's vantage point, you know, it strikes me that they have done an incredibly good
[00:33:05.920 --> 00:33:13.360]   job not overtaking any other market segment on their own in at least the health and medical
[00:33:13.360 --> 00:33:20.960]   and wearables data space, which means that if you try to go to court or regulate them,
[00:33:20.960 --> 00:33:23.760]   they're going to be able to prove that there are plenty of other competitors.
[00:33:24.400 --> 00:33:30.880]   Challenges when you zoom out and you see the constellation of how everything fits together,
[00:33:30.880 --> 00:33:36.160]   at that point, you know, yes, that is very scary to me. But we don't have a legal mechanism to
[00:33:36.160 --> 00:33:41.440]   deal with something like that. I don't think the shining this direction. My concern is that 10
[00:33:41.440 --> 00:33:46.800]   years from now, you know, all of our, I've been writing and talking about what I see is the
[00:33:46.800 --> 00:33:53.040]   all of our PII is sort of morphing into sync to a single data record, which I call a PDR or
[00:33:53.040 --> 00:34:00.160]   personal data record. We saw clear about a week ago, two weeks ago, making an announcement that it
[00:34:00.160 --> 00:34:06.720]   was going to be the arbiter of your data. We got all your biometric data already. Why don't we just
[00:34:06.720 --> 00:34:12.640]   we just can handle it from here. Right. So it is concerning to me that we don't have it is pretty
[00:34:12.640 --> 00:34:19.520]   clear why Google and Amazon are rushing in that direction. Right. So this is what I'm concerned
[00:34:19.520 --> 00:34:24.480]   about. I'm concerned that we get down in the weeds arguing about these other things when
[00:34:24.480 --> 00:34:30.880]   there are much bigger fish to fry. And when Elizabeth Warren entered the race and was talking so much
[00:34:30.880 --> 00:34:38.400]   about breaking up big tech, again, I understand why this is important, but it becomes politicized.
[00:34:38.400 --> 00:34:43.120]   And then the people who, depending on their politics, are talking about these things,
[00:34:43.120 --> 00:34:47.520]   you know, the whole conversation becomes toxic. So I somehow we have to be able to decouple.
[00:34:48.160 --> 00:34:52.720]   The future, you know, we have to decouple these things so that we can solve the issues
[00:34:52.720 --> 00:34:57.840]   that are coming. We have to, we have to like figure out these future problems that we're
[00:34:57.840 --> 00:35:02.240]   trying to solve and reverse engineer them so that we're on the right path going on.
[00:35:02.240 --> 00:35:04.880]   That's why it would be a mistake actually for you to worry about the legal
[00:35:04.880 --> 00:35:10.880]   ditty gritty of this, because that's later. We need to first figure out what's our framework. What
[00:35:10.880 --> 00:35:16.640]   are we, what's our plan? What do we want to accomplish? And then at that point, it's a technical matter
[00:35:16.640 --> 00:35:21.680]   of what laws you make and how you implement them. And this is where I go back to just, again,
[00:35:21.680 --> 00:35:26.880]   let me sing the praises of the OTA that got defunded by Gingrich. You know, the problem is that we
[00:35:26.880 --> 00:35:32.720]   do have, like, I can rattle off names of some very smart people at OSTP and its state, you know,
[00:35:32.720 --> 00:35:38.240]   and at the CIA, the Federal Reserve. But we don't have a central mechanism to do this,
[00:35:38.240 --> 00:35:45.840]   this type of long-term planning so that we have the ability to decouple the investigations and the
[00:35:45.840 --> 00:35:50.000]   regulatory frameworks and do some more forward thinking without politics intervening.
[00:35:50.000 --> 00:35:56.880]   You know, and I would submit that the real problem is who do we, where does this
[00:35:56.880 --> 00:36:03.520]   strategic thinking come from? In the past, Corey, you've suggested that it's up to technologists to
[00:36:03.520 --> 00:36:10.960]   push us in an interoperable direction, in a positive direction, because technologists are the one
[00:36:10.960 --> 00:36:16.320]   actually implementing these technologies. I think you've even said, you know, you guys at Google and
[00:36:16.320 --> 00:36:22.480]   Facebook, now's the time for you to subvert the dominant paradigm. Would you agree?
[00:36:22.480 --> 00:36:29.360]   Well, in the sense that technologists today are in such demand, particularly depending on their
[00:36:29.360 --> 00:36:33.680]   discipline and specialty, that there are some things that if they refuse to build them, they won't get
[00:36:33.680 --> 00:36:38.800]   built. That's why the tech won't build it. Movement is so important. And you know, the reason their
[00:36:38.800 --> 00:36:43.440]   bosses are giving them kombucha on tap and massages on Wednesdays is not because they're big-hearted
[00:36:43.440 --> 00:36:47.200]   guys. It's because they're terrified that they'll go work for someone across the street.
[00:36:47.200 --> 00:36:53.680]   It goes right back to your protagonist in a tech surface. And the rationalizations that she had to
[00:36:53.680 --> 00:36:58.080]   put herself through to get her to do what she was doing.
[00:36:58.080 --> 00:37:04.720]   Well, you know, the origin story of so many technologists is I wrote some code and a computer
[00:37:04.720 --> 00:37:09.520]   did the thing that I specified in my code perfectly. And then I gave that code to someone else and
[00:37:09.520 --> 00:37:13.360]   they could do it too without having to know what I knew. And so I could kind of project my will
[00:37:13.360 --> 00:37:17.760]   around the world. And then I found a network. And on that network, I found people who shared my
[00:37:17.760 --> 00:37:23.120]   point of view of my interest in a kind of society of mind where I wasn't a weirdo and where, you
[00:37:23.120 --> 00:37:27.200]   know, I could make common cause with other people, whether that's about my gender identity or my
[00:37:27.200 --> 00:37:34.000]   politics or what have you. And the idea that all of those people who's inciting incidents or
[00:37:34.000 --> 00:37:39.760]   as origin stories, rests in the realization that technology could give them so much more self
[00:37:39.760 --> 00:37:45.280]   determination and pluralism where the power wasn't concentrated into a few hands, but rather anyone
[00:37:45.280 --> 00:37:50.640]   who wanted it could get enough power to do what it was that they needed to make themselves happy.
[00:37:50.640 --> 00:37:55.760]   And then they've devoted their career to taking that away from other people is I think a fracture
[00:37:55.760 --> 00:38:01.040]   line that's under, theorized and underexploited, right? That there is a natural constituency.
[00:38:01.040 --> 00:38:05.440]   And we're seeing it. We're seeing it, for example, in Amazon, where you have workers who became
[00:38:05.440 --> 00:38:12.480]   energized around tech work issues around facial recognition or around the climate footprint of
[00:38:12.480 --> 00:38:17.600]   Amazon, but who then found themselves in solidarity with Amazon warehouse workers. And who said, you
[00:38:17.600 --> 00:38:24.560]   know, that these are actually elements of the same rule. And I think that we don't lack for legal
[00:38:24.560 --> 00:38:31.280]   tools to address some of these concerns. What we lack is political will that the plain language
[00:38:31.280 --> 00:38:34.960]   of the Sherman Act, the plain language of the Clayton Act, they've never been repealed.
[00:38:34.960 --> 00:38:39.600]   It's the same law that was used to break up AT&T and standard oil. And, you know, to prevent so
[00:38:39.600 --> 00:38:45.280]   many mergers. What's changed is that Ronald Reagan's court sorcerer, a guy named Robert Bork, who is
[00:38:45.280 --> 00:38:49.920]   kind of a predecessor of Gingrich. He was a Nixonite criminal who's denied a seat on the Supreme
[00:38:49.920 --> 00:38:55.120]   Court because back in the 80s, you couldn't get a seat on the Supreme Court if you were an illegitimate
[00:38:55.120 --> 00:39:02.320]   jerk, unlike now. And he had this ideology that we shouldn't pursue antitrust action unless you
[00:39:02.320 --> 00:39:08.080]   could show an immediate consumer harm resulting from anti-competitive conduct. But the language
[00:39:08.080 --> 00:39:12.960]   of the act is pretty clear that the reason that we resist monopoly is because monopoly itself
[00:39:12.960 --> 00:39:18.960]   distorts our policy outcomes that even with an office of technology assessment, which I'm 100%
[00:39:18.960 --> 00:39:24.960]   for and Gingrich, the long list of things that we should hate Gingrich for, that's one of the ones
[00:39:24.960 --> 00:39:30.080]   near the top, you know, blinding the American Congress to its ability to understand technical
[00:39:30.080 --> 00:39:35.280]   issues at a moment in which those issues are very salient. That was an enormously short-sighted
[00:39:35.280 --> 00:39:40.160]   and terrible wicked thing he did. But even if you know what the policy should be,
[00:39:40.160 --> 00:39:47.760]   making the policy happen is hard when there are monopolies who can exert pressure because they
[00:39:47.760 --> 00:39:52.720]   have monopoly rents, right? They have these profits, windfall profits, and they have a small enough
[00:39:52.720 --> 00:39:56.400]   group of executives that they can agree on how to spend them. That, you know, when there's only
[00:39:56.400 --> 00:40:00.720]   five companies, you can all get together and decide on what your lobbying party should be.
[00:40:00.720 --> 00:40:05.760]   And it's really no coincidence that Silicon Valley has come to Washington, DC in the last five years.
[00:40:05.760 --> 00:40:11.680]   And, you know, they were, they seemed to be apolitical in the early years and they learned very
[00:40:11.680 --> 00:40:18.160]   quickly that was not a way to- Absolutely. Yeah. Absolutely. And you don't- Go ahead, Amy.
[00:40:18.160 --> 00:40:21.600]   I was just going to say, just on the subject of DC, a lot of people don't know this, but
[00:40:21.600 --> 00:40:28.960]   Microsoft was actively poaching from state. They have a fairly robust corporate foreign policy
[00:40:28.960 --> 00:40:35.200]   group. Microsoft, yeah, I mean- It's not. I mean, look, every science fiction dystopian or not
[00:40:35.200 --> 00:40:41.200]   eventually ends up being about corporate government, right? The world is run by the
[00:40:41.200 --> 00:40:45.520]   corporations, not by government, by the people, for the people and of the people.
[00:40:45.520 --> 00:40:51.920]   And it sure feels like we're headed in that direction. You know, you said we lack the will.
[00:40:51.920 --> 00:40:56.160]   I think it's even more than just lacking the will, Corey. Perhaps we have the means.
[00:40:56.160 --> 00:41:01.360]   Perhaps we could even gather the will. I think the biggest problem is a lack of agreement on what
[00:41:01.360 --> 00:41:06.560]   needs to be done. And knowledge. Part of the reason is knowledge. If you- Listen,
[00:41:06.560 --> 00:41:11.120]   I like to play this game anytime I meet with a bunch of lawmakers and I don't mean to
[00:41:11.120 --> 00:41:16.240]   throw lawmakers under the bus at this point, but the fact remains that if you ask any of them to
[00:41:16.240 --> 00:41:21.520]   explain what Amazon is, they cannot do it. They just can't. So that's where an OTA or a
[00:41:21.520 --> 00:41:25.360]   strategic office would be very, very valuable. That was the intent of the NTA.
[00:41:25.360 --> 00:41:33.920]   The problem is that the way to break up a monopoly, you need teams of people working together.
[00:41:33.920 --> 00:41:40.240]   And part of the political will comes from strong leadership under incredibly difficult circumstances.
[00:41:40.960 --> 00:41:48.960]   And that is going to be influenced by what people are saying, right? By the public.
[00:41:48.960 --> 00:41:52.960]   So we kind of have this vicious circle and let's not forget these are also all publicly
[00:41:52.960 --> 00:41:57.200]   traded companies who have fiduciary responsibility to their shareholders.
[00:41:57.200 --> 00:42:02.720]   So we also have an economic issue here. And I'm not saying at all that we need to protect
[00:42:02.720 --> 00:42:07.760]   shareholders at all costs, but there are financial issues at play.
[00:42:07.760 --> 00:42:10.240]   Well, no president wants to tank the stock market.
[00:42:10.240 --> 00:42:14.080]   Right. And Microsoft right now, which doesn't get,
[00:42:14.080 --> 00:42:19.120]   whose name is not often evoked when we're talking about current monopolies,
[00:42:19.120 --> 00:42:24.400]   but Microsoft is one of the United States's most valuable companies.
[00:42:24.400 --> 00:42:31.200]   Well, you forgive me. This is why dystopian is in rules in the long run,
[00:42:31.200 --> 00:42:37.440]   because I'm going to call myself the guy in the middle. You guys are operating in a much higher
[00:42:37.440 --> 00:42:41.040]   philosophic level and a much greater understanding than I am.
[00:42:41.040 --> 00:42:49.040]   Most of us are just kind of feel at a loss, feel impotent in the face of all this.
[00:42:49.040 --> 00:42:55.760]   You're faced with giant corporations with infinite resources, a government that has
[00:42:55.760 --> 00:43:01.120]   completely different priorities, which either are consonant or inconsident with yours, but
[00:43:01.120 --> 00:43:09.760]   nevertheless have nothing to do with these corporate conglomerates. You have tech people like us
[00:43:09.760 --> 00:43:16.000]   debating what should happen, what's going on. And you don't really, I think most people,
[00:43:16.000 --> 00:43:18.960]   I'm going to include myself, don't really know what's going on anyway.
[00:43:18.960 --> 00:43:20.880]   But I think it's a little challenge.
[00:43:20.880 --> 00:43:26.720]   I think it's a willful ignorance and a learned helplessness.
[00:43:26.720 --> 00:43:31.760]   I was in Whole Foods just really quickly. There's a brand new Whole Foods that opened up downtown
[00:43:31.760 --> 00:43:34.960]   and for reasons that are too long to explain, we went shopping there.
[00:43:34.960 --> 00:43:40.080]   This Whole Foods, I don't think is intended for the general public. We were
[00:43:40.080 --> 00:43:45.680]   probably one of, it was a crowded store. We were one of maybe two shoppers,
[00:43:45.680 --> 00:43:50.640]   two sets of shoppers who were shopping for themselves. It was like I was in an
[00:43:50.640 --> 00:43:55.360]   air. I felt like I was in a warehouse. I was surrounded by people who were picking.
[00:43:55.360 --> 00:44:02.080]   They were pickers, shopping off for shelves. It was eerily quiet. I felt like I was an
[00:44:02.080 --> 00:44:04.160]   interloper in some other.
[00:44:04.160 --> 00:44:09.680]   You're one of the last humans that will be going in that store, I promise you.
[00:44:09.680 --> 00:44:15.440]   Well, and so that was one of the more profound experiences that I've had in the year where COVID
[00:44:15.440 --> 00:44:18.720]   has shifted everything that we know about. I don't think it's will fill in ignorance.
[00:44:18.720 --> 00:44:25.600]   I think it's moving so fast. It's very hard for us. These conversations have been the meat of
[00:44:25.600 --> 00:44:30.080]   most of the shows we've done over the last two or three years. I don't think we've ever come to a
[00:44:30.080 --> 00:44:34.320]   consensus about what should be done or how it should be done.
[00:44:34.320 --> 00:44:42.160]   That's amongst us. We're fairly like-minded people who have, I guess, a better understanding of
[00:44:42.160 --> 00:44:48.320]   technology than a lot of people. We can't agree. Maybe you and Corey can agree. Let me take a break.
[00:44:48.960 --> 00:44:54.560]   I want to- there's no good place to interrupt, so I'm going to interrupt right now. Hold that
[00:44:54.560 --> 00:44:59.520]   thought, Amy. We can pick up right where we left off, but I do have to take a break.
[00:44:59.520 --> 00:45:05.040]   I knew this would be a challenge. I didn't drink enough coffee this morning, and I apologize.
[00:45:05.040 --> 00:45:09.360]   I'm doing the best I can to keep up with two of the smartest people I know. Amy Webb from the
[00:45:09.360 --> 00:45:17.440]   Future Today Institute, she is a quantitative futurist, whatever the hell that is. But we're so-
[00:45:17.440 --> 00:45:23.280]   we just know she's smart and we're so thrilled, as always, to have her. The legitimately inspiring
[00:45:23.280 --> 00:45:30.160]   Corey Doctorow, who is not only a great novelist, works at the Electronic Frontier Foundation
[00:45:30.160 --> 00:45:38.880]   on competitive issues, so this is right up his alley. Together, they are easily the highest IQ
[00:45:38.880 --> 00:45:44.480]   panel we've ever had. The average, I'm bringing it down, and I apologize, but if you just take them
[00:45:45.760 --> 00:45:49.920]   and leave me out of it, it's pretty high here. Our show today brought to you by Express
[00:45:49.920 --> 00:45:55.680]   VPN every day on the news. There's a story about technology. We're just doing it right now.
[00:45:55.680 --> 00:46:01.200]   It's creeping into our private lives. I don't like the idea of somebody paying attention to every
[00:46:01.200 --> 00:46:11.360]   place I go on the Internet of every place I surf. We know that there's monitoring going on,
[00:46:11.920 --> 00:46:18.320]   and one of the really good uses of a VPN is to encrypt what you're doing, not just so that
[00:46:18.320 --> 00:46:23.440]   guy in the hoodie in the corner of the coffee shop can't see what you're doing. We're not in
[00:46:23.440 --> 00:46:30.560]   coffee shops that much anymore, but even so that your ISP or your carrier can't see what you're doing.
[00:46:30.560 --> 00:46:36.240]   This is why you want to use a good VPN. Now, the thing everybody's going to
[00:46:36.960 --> 00:46:42.640]   make a note of this, you should think about it. That VPN now becomes a proxy for that carrier or
[00:46:42.640 --> 00:46:49.360]   that ISP. All the stuff that you put through your ISP is now going past the ISP to the VPN service.
[00:46:49.360 --> 00:46:56.320]   So you really need to pick well, pick wisely. That's why I use Express VPN. They secure everything.
[00:46:56.320 --> 00:47:03.440]   My phone, my computer, my smart TV, I even can put it on my router to protect my entire household.
[00:47:03.440 --> 00:47:08.960]   That's a VPN that really works. And yes, you can trust it. Third party independent audits of
[00:47:08.960 --> 00:47:14.240]   their privacy policy and their trusted server technology show that they do not log, they do not
[00:47:14.240 --> 00:47:20.000]   track, they do not know what you are doing when you're on an Express VPN server. You're running in
[00:47:20.000 --> 00:47:24.640]   RAM on their trusted server technology sandbox. So it can't write to the disk, it can't log your
[00:47:24.640 --> 00:47:31.520]   activity. And as soon as you log out, it's gone, it's gone. And that means there is no way Express
[00:47:31.520 --> 00:47:36.160]   VPN can keep track of what you're doing, which means there's no way anyone else can. It's easy
[00:47:36.160 --> 00:47:43.280]   to use to fire up the app, tap the button, you're protected. In fact, it's the number one rated VPN
[00:47:43.280 --> 00:47:49.680]   provider on the market. If you just search, you'll see everywhere Express VPN is very highly rated
[00:47:49.680 --> 00:47:57.040]   because it is a good VPN provider that really protects your privacy. It's also really fast.
[00:47:57.040 --> 00:48:02.960]   They put the money into having many, many servers in almost 100 countries. So you can watch HD video
[00:48:02.960 --> 00:48:06.880]   on Express VPN. A lot of times people say, I don't want to use a VPN, it's going to slow me down,
[00:48:06.880 --> 00:48:12.480]   not Express VPN. So much so that you can use it to watch video from other countries, watch your
[00:48:12.480 --> 00:48:18.720]   Netflix in the UK and get all those Doctor Who series or get your anime fix your manga fix by
[00:48:18.720 --> 00:48:25.040]   watching Netflix Japan, just by pointing your Express VPN server to those locales. It's really
[00:48:25.040 --> 00:48:30.880]   kind of amazing. We've got a deal for you now. Look, you shouldn't be using free VPNs. It's
[00:48:30.880 --> 00:48:35.200]   expensive to run a VPN. And if they aren't charging you, there's another way they're making money.
[00:48:35.200 --> 00:48:40.880]   But Express VPN is very affordable, less than seven bucks a month. Take advantage of our best deal
[00:48:40.880 --> 00:48:47.840]   offer, which is a year of Express VPN plus an extra three months free. Go to express VPN.com
[00:48:47.840 --> 00:48:52.960]   slash Twitter. And if everything I haven't told you is in 100% true, you have a 30 day money back
[00:48:52.960 --> 00:48:58.160]   guarantee for three extra months free with a one year package, express VPN.com slash
[00:48:58.160 --> 00:49:03.120]   Twitter. Protect yourself online with Express VPN.com slash.
[00:49:03.120 --> 00:49:09.840]   Twitter. We go back to the ring for round three. Actually, it's not a fight. There's a considerable
[00:49:09.840 --> 00:49:15.360]   agreement here, which actually encourages me, Amy Webb, Corey Doctorow. I interrupted you,
[00:49:15.360 --> 00:49:20.240]   Amy. I don't know where you able to hold that thought. Yeah, no, my it was a question. I,
[00:49:20.240 --> 00:49:25.760]   you were disagreeing, I think with me slash us and I wanted to know what it was about. Okay,
[00:49:25.760 --> 00:49:32.560]   we should move on. I have no idea. Well, I guess my only point was that for most of us,
[00:49:32.560 --> 00:49:36.960]   this is baffling. And I don't know what the solution is. And I certainly, if I don't, I don't
[00:49:36.960 --> 00:49:41.760]   expect government to come up with a great solution either. Where, where should this conversation be
[00:49:41.760 --> 00:49:45.680]   held? That's why I posed that question to Corey. Because I think you're right, Corey, the people
[00:49:45.680 --> 00:49:50.720]   who are writing the technology, creating the technology bear considerable amount of responsibility. And
[00:49:50.720 --> 00:49:56.240]   we, and we should encourage them to, to let their conscience be the guy. But who else, where else
[00:49:56.240 --> 00:50:02.160]   should this conversation happen? Is it the EFF boardroom? I think even in the F boardroom,
[00:50:02.160 --> 00:50:07.920]   there's not a lot of agreement, is there? Amy, you go ahead first. I was just going to say,
[00:50:07.920 --> 00:50:14.400]   there's a certain amount of self-legulation that's happening right now in the valley,
[00:50:14.960 --> 00:50:20.560]   where it's become cool to talk about how bad everybody is. Yeah, just watch the social dilemma
[00:50:20.560 --> 00:50:25.120]   on Netflix. And you'll, you'll see an example. So that's exactly what I'm talking about. Yeah.
[00:50:25.120 --> 00:50:35.520]   And I don't, I find that, I find that fascinating. I find it fascinating that everybody's feeling
[00:50:35.520 --> 00:50:42.240]   great about, well, it's the easy way out. It's the easy way out. Guilt is the easy way out,
[00:50:42.240 --> 00:50:46.080]   self-flagellation. But it's not guilt. It's as bad as blaming the other guy,
[00:50:46.080 --> 00:50:50.880]   it's just a way of abrogating your own responsibility. But it's more than that. It is,
[00:50:50.880 --> 00:50:55.360]   it's a club. It's a, I don't know, Corey, maybe you know what I'm trying to.
[00:50:55.360 --> 00:50:59.040]   There's a lot of group thinking Silicon Valley, isn't there? But it's a,
[00:50:59.040 --> 00:51:06.320]   the cool kid, like it's cool now to, to talk about how, what Silicon Valley has done is bad.
[00:51:06.320 --> 00:51:08.640]   Right. I don't, I don't understand how, how,
[00:51:09.280 --> 00:51:14.720]   as if you didn't have anything to do with it. Yeah. As if. So I, I, it reminds me,
[00:51:14.720 --> 00:51:17.200]   there's a young Kapoor joke, which I guess is very seasonal.
[00:51:17.200 --> 00:51:24.000]   In which the rabbi and the, yeah, the rabbi and the richest man in town are praying at the altar
[00:51:24.000 --> 00:51:28.320]   on young Kapoor and the rabbi says, Oh Lord, I'm nothing. I am nothing. And the richest man in town
[00:51:28.320 --> 00:51:33.360]   says, Oh Lord, I'm nothing. I'm nothing. And the janitor who's been cleaning the pews comes to the,
[00:51:33.360 --> 00:51:38.240]   comes to the altar and says, Oh Lord, I'm nothing. I'm nothing. And the richest man in town turns to
[00:51:38.240 --> 00:51:41.840]   the rabbi and says, huh, look who thinks he's nothing. You know,
[00:51:41.840 --> 00:51:48.320]   and I think that there is a humble brag.
[00:51:48.320 --> 00:51:52.320]   It's a humble brag. Yes. The, the social dilemma,
[00:51:52.320 --> 00:52:00.400]   self-flagellation, which is we geniuses turned out to be evil geniuses.
[00:52:00.400 --> 00:52:04.080]   But there's no question about whether we're geniuses. Yeah.
[00:52:04.080 --> 00:52:08.640]   And I think that that, that, you know, if the critique starts with look at these geniuses,
[00:52:08.640 --> 00:52:16.720]   then who have, who have done this remarkable thing instead of this commonplace thing, right?
[00:52:16.720 --> 00:52:24.160]   This, the, the, the monopolization that was undertaken using tactics that are not so different
[00:52:24.160 --> 00:52:28.320]   from the tactics that Vince McMahon used to turn 30 wrestling leagues into one. And no one's
[00:52:28.320 --> 00:52:34.320]   going to call him a genius. Then what you're left with is, Oh, look at these greedy mediocrities,
[00:52:34.320 --> 00:52:41.440]   whose self rationalization allowed them to take the digital nervous system of the 21st century
[00:52:41.440 --> 00:52:52.560]   and turn it into this brittle, anti robust monopolized, surveilling, compromised thing,
[00:52:53.520 --> 00:52:58.720]   despite the manifest dangers that that gives to us as a species in a civilization.
[00:52:58.720 --> 00:53:03.600]   I agree. So that answers your question because that, that analogy actually sticks really well.
[00:53:03.600 --> 00:53:08.480]   Because I think everybody feels like they understand wrestling or at least it's approachable.
[00:53:08.480 --> 00:53:12.480]   I think that there, that we have been made to feel like we can't possibly understand.
[00:53:12.480 --> 00:53:17.920]   Yes. Yes. It feels hopeless. It feels like, well, if you guys can't figure it out, I don't like,
[00:53:17.920 --> 00:53:24.480]   well, how could I? No, but having, what would, what would it take to have an informed conversation?
[00:53:24.480 --> 00:53:28.640]   It would, it would take admitting, at least from the outset, that you're missing a critical,
[00:53:28.640 --> 00:53:32.800]   massive knowledge to have that informed conversation and nobody wants to be made to feel stupid.
[00:53:32.800 --> 00:53:37.280]   Right. So I think part of what's happening, that was what I meant when I was talking about a
[00:53:37.280 --> 00:53:43.200]   willful ignorance. I think that we don't want to be confronted with our lack of understanding.
[00:53:43.200 --> 00:53:50.160]   And listen, especially if you feel as though your age means that technology and therefore the world
[00:53:50.160 --> 00:53:55.600]   are passing you by, you're not going to want to have an informed conversation. And so the easier
[00:53:55.600 --> 00:54:03.600]   thing to do is either to ignore it or to join in a chorus, which feels right because you're
[00:54:03.600 --> 00:54:07.840]   politically aligned with whoever's ideology is shouting the things.
[00:54:07.840 --> 00:54:08.400]   Right. Yep.
[00:54:09.440 --> 00:54:15.200]   Yep. So let me, let me, let me try framing this around a different policy dysfunction.
[00:54:15.200 --> 00:54:20.640]   Let's talk about lead in the water in Flint. Okay. So as far as I know, no one in Congress is a
[00:54:20.640 --> 00:54:28.000]   public health expert or a chemist or a biologist. I'm sure no one on the Flint Council is, and no
[00:54:28.000 --> 00:54:31.600]   one in the Michigan State House, pretty, pretty unlikely if anyone in the Michigan State House
[00:54:31.600 --> 00:54:37.360]   had that background. And yet, for most of Flint's history, they didn't have lead in their water.
[00:54:37.360 --> 00:54:41.280]   And most other cities in America don't have lead in their water. And the question of weather and
[00:54:41.280 --> 00:54:45.920]   how much lead you should have is an intensely technical question. And yet it's one that they
[00:54:45.920 --> 00:54:55.280]   were able to answer without mastering the chemistry or the biology. It was one they managed to have
[00:54:55.280 --> 00:55:03.360]   by adopting an adjudication process, a truth seeking process by which people with rival ideas
[00:55:03.360 --> 00:55:08.720]   would go before a neutral adjudicator present their evidence. And the adjudicator would make
[00:55:08.720 --> 00:55:14.320]   some empirical findings and then ask some political questions about how they should be interpreted.
[00:55:14.320 --> 00:55:20.080]   So like in the UK, when David Nutt was the drug czar, he had a panel of experts rate all of the
[00:55:20.080 --> 00:55:25.600]   drugs that that had been scheduled as dangerous or not dangerous. According to how dangerous they
[00:55:25.600 --> 00:55:30.400]   were to the person who used them to their family and to society. And then he came up with different
[00:55:30.400 --> 00:55:35.920]   models to see which ones of those drugs changed in their ranking as to how dangerous they were
[00:55:35.920 --> 00:55:40.080]   depending on your political priorities. Some of them were always going to be very high on the list of
[00:55:40.080 --> 00:55:44.160]   dangerous, some would always be very low, but some would move depending on your political priorities.
[00:55:44.160 --> 00:55:48.640]   And then he went to Parliament and he said, I have an empirical basis to answer your political
[00:55:48.640 --> 00:55:54.560]   question. If you have a political consensus that we want to protect society rather than users or
[00:55:54.560 --> 00:55:59.280]   families, then I'll tell you where these drugs go. And if you have a different political consensus
[00:55:59.280 --> 00:56:04.400]   that I'll tell you where the drugs go on the basis of that. And so we can merge both the political
[00:56:04.400 --> 00:56:10.160]   and the empirical, but it requires that we have an honest process. And David Nutt was fired
[00:56:10.160 --> 00:56:15.200]   because he refused to say that cannabis was more dangerous than alcohol. And the alcohol
[00:56:15.200 --> 00:56:20.560]   industry in the UK by its own reckoning is only profitable because of binge drinking, unsafe
[00:56:20.560 --> 00:56:25.120]   binge drinking. If unsafe binge drinking were eliminated, that's where the profit margin is.
[00:56:25.120 --> 00:56:31.680]   Oh Lord. And the drinks industry operates its own anti-binged drinking education program,
[00:56:31.680 --> 00:56:36.240]   which is very unsuccessful. Yeah, we're one. And they say it's unsuccessful because they can't fix it.
[00:56:36.240 --> 00:56:40.400]   And yet he created one that did fix it. He created a rival one. And when people were inoculated
[00:56:40.400 --> 00:56:44.720]   with this, they did less binge drinking. And in the background of that, right, where you have this
[00:56:44.720 --> 00:56:50.400]   very powerful industry that is very profitable and highly concentrated, the evidence was secondary
[00:56:50.960 --> 00:56:55.600]   to their ability to distort political outcomes. And that's why I think monopolies should be at
[00:56:55.600 --> 00:57:01.360]   the front of this discourse. And also, but you have a very... The amount of lead in water is a very
[00:57:01.360 --> 00:57:10.720]   clear cut and easily measured criterion. Is there something like that for the problem of big tech,
[00:57:10.720 --> 00:57:18.000]   even to say the problem of big tech sounds reductionist? Well, okay, retaining long-term data
[00:57:18.000 --> 00:57:24.720]   retention has no good evidentiary basis, right? I mean, knowing that I clicked a link 10 years ago
[00:57:24.720 --> 00:57:30.400]   gives you nothing except for Compromot, unless it's like you're hoping to sell me a roof and the
[00:57:30.400 --> 00:57:37.520]   link was to replace my roof. 30 years years ago. So the only reason to hold that is because hard
[00:57:37.520 --> 00:57:43.120]   drives are cheap and there's no statutory damages for breaches. So we know that data that is collected
[00:57:43.120 --> 00:57:51.280]   tends to leak. We know that leaks have unquantifiable risks and that you should have a policy of data
[00:57:51.280 --> 00:57:55.920]   minimization or retention minimization. But that policy is not operated by any of the firms,
[00:57:55.920 --> 00:58:04.320]   quite the opposite happens. Or whether people like being surveilled for advertising. We know they
[00:58:04.320 --> 00:58:11.360]   don't. So I make a bill that requires data minimization. You can hold the data for three months,
[00:58:12.160 --> 00:58:16.080]   something like GDPR actually doesn't that kind of what GDPR or the California privacy?
[00:58:16.080 --> 00:58:22.000]   Or elements of it. Yeah, for sure. So I make that bill. First of all, could we make a bill like
[00:58:22.000 --> 00:58:27.200]   that? Could we get it passed? And if we, if we did, would that be sufficient to solve the problem of
[00:58:27.200 --> 00:58:33.920]   big tech? No, it'd be an iterative stuff. It's a little tiny thing. Yeah, look at what a GDPR has.
[00:58:33.920 --> 00:58:39.680]   Gee, all I know is I get cookie announcements every five seconds. We are going to have an
[00:58:39.680 --> 00:58:48.480]   enormous problem in the United States. The more that our homes produce data and devices produce
[00:58:48.480 --> 00:58:54.240]   data and the more federated these privacy laws are. And I'll give you a clear example. So
[00:58:54.240 --> 00:59:00.480]   Walmart has a product that it's been researching. It is not in market, but it is a connected shopping
[00:59:00.480 --> 00:59:05.040]   cart. There's a patent that they've won. Basically, you put your hands on the shopping cart. Yeah.
[00:59:05.840 --> 00:59:11.600]   You go into a store, it is looking for a bio, it takes a biometric baseline. And then as you
[00:59:11.600 --> 00:59:16.960]   shop throughout the store, it checks for anomalies. So if you're a pulses racing, you're sweating a
[00:59:16.960 --> 00:59:21.280]   little bit more. The shopping cart knows the shopping cart pings the store associate to hopefully
[00:59:21.280 --> 00:59:26.160]   help you have a better time shopping throughout Walmart. There are some tangible use cases that
[00:59:26.160 --> 00:59:31.120]   might make sense if you understand how those data are being collected and mined and refined and
[00:59:31.120 --> 00:59:37.040]   what that ultimately means for you. However, the fact remains that a shopping cart may not be able
[00:59:37.040 --> 00:59:44.160]   to cross state lines. A shopping cart may be more scary and illegal than a bottle of alcohol.
[00:59:44.160 --> 00:59:50.960]   And the challenge is that there are myriad devices coming to market that do the same thing.
[00:59:50.960 --> 00:59:59.360]   And we just don't have a clear understanding of what those data can be collected by whom and
[00:59:59.360 --> 01:00:06.480]   for what purpose. And the EU has been debating this. I was at a couple of high level meetings
[01:00:06.480 --> 01:00:12.640]   over the past 18 months or so. And there are lots of global leaders and organizations
[01:00:12.640 --> 01:00:21.280]   with enforcement powers who are trying to define data ownership. But again, this to me diverges from
[01:00:21.760 --> 01:00:30.400]   the great story about lead in the water and big alcohol and I guess tiny cannabis.
[01:00:30.400 --> 01:00:39.200]   The challenge is that when it comes to questions about technology where we have to have conversations
[01:00:39.200 --> 01:00:44.800]   regarding data and the people who are having those conversations don't, they can't explain
[01:00:44.800 --> 01:00:49.360]   or understand what it is they're talking about. That to me is fundamentally different
[01:00:50.800 --> 01:00:57.040]   from having conversations about lead in water or blood alcohol levels. I mean, you don't have to
[01:00:57.040 --> 01:01:02.560]   be a doctor to have a basic understanding of a toxin in your body. I don't think we can.
[01:01:02.560 --> 01:01:06.320]   You have a mode of come of. Yeah, we can't even identify what the toxin is.
[01:01:06.320 --> 01:01:11.440]   Very well in this big tech. I just feel like I don't know. I think you guys have got
[01:01:11.440 --> 01:01:16.800]   you're engaging in the sin of subject that is not you're assuming that the subject that's not
[01:01:16.800 --> 01:01:20.720]   your discipline has relatively clear cut lines and little debate within it.
[01:01:20.720 --> 01:01:25.360]   I guarantee you that within addiction research, there are enormous. I'm sure that's true.
[01:01:25.360 --> 01:01:30.960]   Debates about what amount of alcohol is safe, what frequency of alcohol used to
[01:01:30.960 --> 01:01:37.760]   say, what the danger signs are and so on. These are in fact hotly contested issues all around.
[01:01:37.760 --> 01:01:43.040]   As the question of like how much lead can be, I mean, there is going to be trace amounts of lead
[01:01:43.040 --> 01:01:49.040]   in your diet, right? That's a fact. So how much lead is too much lead and what steps do we take
[01:01:49.040 --> 01:01:54.720]   and what's the cost benefit? Can we find a common ground then between the three of us and what we
[01:01:54.720 --> 01:01:59.680]   should do about big tech? I like data minimization. I think that's good. Oh, I know we agree.
[01:01:59.680 --> 01:02:07.760]   I agree with the framework. I think we need to project much further out into the future because
[01:02:09.280 --> 01:02:15.680]   we do not tend to create policy or regulatory frameworks. This doesn't work on norms and standards.
[01:02:15.680 --> 01:02:22.320]   We don't have processes in place until there's a problem. And at that point, we're trying to fix
[01:02:22.320 --> 01:02:31.280]   or levy punitive. It would be much better if there was a way to incent the different parties,
[01:02:31.280 --> 01:02:36.960]   which in this case are big tech, also the market. So investors also everyday people in the government
[01:02:36.960 --> 01:02:41.520]   to come together and collaborate. I know that sounds pretty naive, but I do think that that's
[01:02:41.520 --> 01:02:49.520]   I think that pursuing something that incense tech companies to reframe their work as a public
[01:02:49.520 --> 01:02:56.880]   good or at least a lot of that work, it doesn't lead to the loss of revenue necessarily and it does
[01:02:56.880 --> 01:03:02.960]   allow us all to plan for the further future, I think. Yeah. Yeah. I mean, some of this is just
[01:03:02.960 --> 01:03:08.880]   corporate reform altogether. Amy mentioned the fiduciary principle, which traces back to this
[01:03:08.880 --> 01:03:14.400]   Milton Friedman editorial. And the problem with the fiduciary rule for corporate governance
[01:03:14.400 --> 01:03:21.680]   is that it has no empirical benchmark. So like I could say, it was my duty to put lead in the
[01:03:21.680 --> 01:03:28.560]   water of Flint because we only paid a $1 billion fine and we made $1 billion and $1 in profits.
[01:03:29.440 --> 01:03:34.880]   Or I could say it was my duty to engage in a string of predatory mergers without regard to
[01:03:34.880 --> 01:03:41.040]   what might happen in the future. Or I could say I had to exercise forbearance and not engage in
[01:03:41.040 --> 01:03:47.920]   any predatory mergers. Last five years down the line, this provokes so much anger among regulators
[01:03:47.920 --> 01:03:51.680]   that they broke up the company in a way that cost us more than we would have made. Right? I had
[01:03:51.680 --> 01:03:57.600]   to hire that person because doing so created new efficiencies that could have led to new business.
[01:03:57.600 --> 01:04:01.520]   I had to lay that person off because the cost savings could go to my shareholders.
[01:04:01.520 --> 01:04:06.560]   There is no course of action that cannot be framed as beneficial to shareholders
[01:04:06.560 --> 01:04:13.360]   if you get to name the timeframe and assign your own weights to hypotheticals. And so I think that
[01:04:13.360 --> 01:04:20.560]   the shareholder rule has always been a sociopath's charter, right? Just a thing that lets you do
[01:04:20.560 --> 01:04:25.520]   greedy things and insist that it's your legal responsibility to do them. And that a stakeholder
[01:04:25.520 --> 01:04:30.720]   rule, which is closer to the German proposition, the stuff Verfakis is pushing these days,
[01:04:30.720 --> 01:04:36.400]   is a law closer where at least what you say is, okay, well, there are competing priorities.
[01:04:36.400 --> 01:04:42.720]   There is no one empirical answer. We tend to rely on multi-stakeholderism as a way of resolving
[01:04:42.720 --> 01:04:46.480]   them, right? Rather than saying, okay, well, we'll just trust bosses to have their workers'
[01:04:46.480 --> 01:04:52.400]   best interests. We put the workers in the room too so that at least they have to come up with a
[01:04:52.400 --> 01:04:57.280]   thing where they can swing enough votes or find a compromise to govern the company in a way that
[01:04:57.280 --> 01:05:02.160]   accounts for both of their interests. So the stakeholder model has environmental regulators,
[01:05:02.160 --> 01:05:08.720]   consumers, workers, bosses, and investors all in the room, all with a seat at the table,
[01:05:08.720 --> 01:05:14.880]   all with what amounts to board votes. And it produces an outcome that at least gets a look in
[01:05:14.880 --> 01:05:19.280]   from other people's priorities. I don't think that it's necessarily good, but I think that it is a
[01:05:19.280 --> 01:05:22.880]   necessary precondition for something being good. That you'll never get something that's good
[01:05:22.880 --> 01:05:26.960]   for all of those stakeholders if one of them doesn't get a seat at the table.
[01:05:26.960 --> 01:05:34.240]   I think that that is largely correct. And I would also just add that I think companies,
[01:05:34.240 --> 01:05:41.600]   it's my observation that companies are a little shy, a little gun shy. I hate using that word now,
[01:05:41.600 --> 01:05:48.960]   a little concerned about doing risk modeling and incorporating more voices because they're
[01:05:48.960 --> 01:05:57.120]   concerned about the future of legal action. And there's a very large company that has some smart
[01:05:57.120 --> 01:06:02.240]   people working with it that I talked to a couple months ago that wanted to do some longer-term
[01:06:02.240 --> 01:06:07.360]   risk modeling to figure out how could they mitigate some problems that they were likely causing having
[01:06:07.360 --> 01:06:13.680]   to do with the environment. And we were all positioned to do that work together, and their
[01:06:13.680 --> 01:06:18.560]   legal department shut it down. And they shut it down because in the United States, if word ever
[01:06:18.560 --> 01:06:25.280]   got out that this company was engaging in work to fix the future. So they were doing a lot of
[01:06:25.280 --> 01:06:32.080]   what Corey just explained. Their legal department was concerned that that would open them up for
[01:06:32.080 --> 01:06:40.240]   some kind of big tobacco style lawsuit at some point. And so they were, from their vantage point,
[01:06:40.240 --> 01:06:47.680]   it was less of a problem to simply do the bad stuff and deal with it later on than to have sorted
[01:06:47.680 --> 01:06:52.800]   it out, how to fix the problems that plausibly were being caused in advance, mitigate that risk,
[01:06:52.800 --> 01:06:59.360]   and then do things a little differently. And the very fact that we are so litigious and that this
[01:06:59.360 --> 01:07:05.920]   the fact that somebody is concerned about being sued 10 years from now is going to prevent them
[01:07:05.920 --> 01:07:11.600]   from doing the risk modeling that would create a better outcome for everybody. To me, that is
[01:07:11.600 --> 01:07:16.160]   absolutely-- You could do it, but just don't write any of it down. Just remember it.
[01:07:16.160 --> 01:07:23.440]   >> Okay, so let me frame that slightly differently and say what if the countervailing force was not
[01:07:23.440 --> 01:07:27.920]   to deprive people who've been harmed by companies of their right to legal redress,
[01:07:27.920 --> 01:07:34.720]   but rather to make the penalty for not taking action to address your pollution so dire that
[01:07:34.720 --> 01:07:38.720]   the risk of litigation 10 years on because you show that you had actual knowledge and didn't take
[01:07:38.720 --> 01:07:42.640]   action on it. I mean, frankly, I think that's an overblown risk, right? We have Exxon's memos,
[01:07:42.640 --> 01:07:47.680]   right? Exxon knew 30 years ago that they were going to render the entire planet uninhabitable
[01:07:47.680 --> 01:07:53.600]   for the human species and took no action on it. In fact, the only action they took was a disinformation
[01:07:53.600 --> 01:07:58.560]   campaign to bury the facts that they knew that their own scientists had told them that they're
[01:07:58.560 --> 01:08:03.680]   in litigation, but it's not like that litigation has a hope of going anywhere unless there's some
[01:08:03.680 --> 01:08:10.160]   real shifts in how we proceed. So when I started at EFF, our legal director is now our executive
[01:08:10.160 --> 01:08:15.760]   director, Cindy Cohen, was engaged in a lawsuit called Boero that she brought with her from her
[01:08:15.760 --> 01:08:21.520]   old firm when she came to EFF. She was representing Larry Boero, who's the sole survivor of massacre
[01:08:21.520 --> 01:08:29.280]   in Nigeria by mercenaries hired by Chevron. And she was suing Chevron in a U.S. court on his behalf
[01:08:29.280 --> 01:08:35.200]   civilly and its directors and officers. The case collapsed when all of her key witnesses were
[01:08:35.200 --> 01:08:43.920]   assassinated. It didn't moderate Chevron's conduct, right? So I guess I would be more sympathetic to
[01:08:43.920 --> 01:08:50.160]   the plight of companies who are risk-averse because of the risk of litigation. If the Sacklers weren't
[01:08:50.160 --> 01:08:59.760]   saying to their farmer reps, go find me the pill doctors and reward them for pushing pills on people
[01:08:59.760 --> 01:09:06.320]   in ways that are dangerous, but increases our profits. There are certain firms.
[01:09:06.320 --> 01:09:07.840]   Sure, go ahead.
[01:09:07.840 --> 01:09:12.000]   No, but before you move on, I just wanted to, and I don't mean to interrupt, but I think there's
[01:09:12.000 --> 01:09:17.760]   an important point to be made right now. And that is, in industries where there has been
[01:09:17.760 --> 01:09:24.800]   tremendous consolidation of power and bad things happening, I just want to note that they're very
[01:09:24.800 --> 01:09:32.800]   different. So the Sacklers pushing, knowingly pushing highly addictive medications on people,
[01:09:32.800 --> 01:09:38.160]   I think is different from some, and again, I'm not defending Chevron. Some of the things that
[01:09:38.160 --> 01:09:46.400]   have happened with Chevron, but I would also say that those big oil and petroleum companies
[01:09:47.200 --> 01:09:52.960]   tend to work with local regional offices. It's very, it's unlike anyhow.
[01:09:52.960 --> 01:09:58.080]   I would just say that there is a direct parallel between the Sacklers pushing
[01:09:58.080 --> 01:10:04.400]   addictive narcotics on physicians for prescriptions is not so very different from, say,
[01:10:04.400 --> 01:10:12.080]   YouTube pushing addictive algorithms to keep you watching more YouTube videos or Facebook
[01:10:12.080 --> 01:10:18.800]   adjusting its newsfeed to keep you stuck to Facebook. There is a certain similarity between the two.
[01:10:18.800 --> 01:10:25.280]   Right. So Corey made a really good point that I loved, which was, can't we, can't the punitive
[01:10:25.280 --> 01:10:29.440]   measure for not doing something in advance to, to mitigate the risk?
[01:10:29.440 --> 01:10:37.280]   Can't that be a more compelling outcome than the opposite?
[01:10:37.280 --> 01:10:40.800]   What about just doing the right thing? How about that as a motivation?
[01:10:41.440 --> 01:10:46.480]   You know, well, I know we're exposing ourselves to potential lawsuit in 10 years,
[01:10:46.480 --> 01:10:50.160]   but you know, this is doing the right thing. Maybe we should just do that.
[01:10:50.160 --> 01:10:57.600]   But again, like we're in a situation where the nothing incense that, and in fact, if anything,
[01:10:57.600 --> 01:11:02.080]   but that's our problem. I would submit that's the problem right there.
[01:11:02.080 --> 01:11:06.720]   So Corey had a really great suggestion, which was, can't we make it so that the
[01:11:08.560 --> 01:11:14.000]   threat of a lawsuit for ignoring it's hopeless. If you're going to use government incentives
[01:11:14.000 --> 01:11:19.440]   to get people to do the right thing, it's hopeless because those go, look at the tax code.
[01:11:19.440 --> 01:11:24.000]   That's government incentives trying to get people to do the quote right thing, depending on who,
[01:11:24.000 --> 01:11:29.120]   with power parties in power and what legislators are writing the laws. That's hopeless.
[01:11:29.120 --> 01:11:33.920]   If people aren't going to do the right thing, because they're not legally incentive,
[01:11:34.560 --> 01:11:39.280]   we have real problems. We have a moral. We have a moral.
[01:11:39.280 --> 01:11:44.160]   It's true, but also, you know, think about the,
[01:11:44.160 --> 01:11:49.040]   think about the microeconomics of a room in which decision makers have gathered and they have
[01:11:49.040 --> 01:11:53.520]   different priorities and different urgency about how much they need the money and how much they
[01:11:53.520 --> 01:12:00.480]   care about different priorities. You can make a moral pitch compelling if it comes with an
[01:12:00.480 --> 01:12:04.320]   instrumental pitch, right? If it's both, this is an argument that comes up a lot in the
[01:12:04.320 --> 01:12:08.720]   stitcher debate. Yeah, the carrot in the stick. Carrot in the stick. Yeah, I mean,
[01:12:08.720 --> 01:12:13.120]   this comes up in the torture debate, right? On the one hand, torture generally gives us bad intel,
[01:12:13.120 --> 01:12:18.320]   but when I raised this with Bart Gellman, who's one of the Snowden journalists, he said, well,
[01:12:18.320 --> 01:12:23.280]   torture gives you bad intel, but if I've got a safe and I need to open it and I can tie you to a
[01:12:23.280 --> 01:12:26.720]   chair and go after you with a pair of tin snips, I'm going to get into that safe.
[01:12:26.720 --> 01:12:30.000]   So there's another reason we don't torture. It's not just because it gives us bad intel,
[01:12:30.000 --> 01:12:34.960]   it's because it's wrong. And both of those things being true, bad intel and it's wrong,
[01:12:34.960 --> 01:12:42.960]   is more powerful than either of them on their own. And so, I think in respect of the boardroom,
[01:12:42.960 --> 01:12:47.760]   look at the Americans with Disability Act. I think it's a really good example of how you can
[01:12:47.760 --> 01:12:55.040]   blend both the moral case for accommodating people of varying abilities. And as my friend,
[01:12:55.040 --> 01:13:02.160]   Liz Henry says, we are all of us only temporarily able-bodied. And so, you're really only benefiting
[01:13:02.160 --> 01:13:09.680]   yourself when you create that norm. But also, we created a regime with stiff damages and loser pays
[01:13:09.680 --> 01:13:17.600]   system that invited the Know When Know fee bar to show up and say, like, were you discriminated
[01:13:17.600 --> 01:13:22.640]   against, I will sue them, the deeper their pockets are, the harder I will sue them because the more
[01:13:22.640 --> 01:13:28.080]   legal fees I'll get back. And I will pray that they drag out the case and try to bankrupt you,
[01:13:28.080 --> 01:13:32.960]   because that's just more fees for me. And so, it created a different equilibrium.
[01:13:32.960 --> 01:13:36.320]   I would agree, we don't want to create incentives to misbehave.
[01:13:36.320 --> 01:13:43.360]   But I don't think incentives not to misbehave can replace a moral compass.
[01:13:43.360 --> 01:13:46.800]   I just think that replace is the wrong word. I think augment.
[01:13:46.800 --> 01:13:47.760]   Augments?
[01:13:47.760 --> 01:13:57.440]   Okay. So, probably should eliminate the incentives to misbehave, for sure. Let's eliminate those.
[01:13:57.440 --> 01:14:03.200]   If we can track those down, let's get those out of the legal code. But, I don't know,
[01:14:03.200 --> 01:14:07.360]   I mean, I'm starting to sound like a preacher here. We really have to have a moral compass.
[01:14:07.360 --> 01:14:09.440]   All right, maybe you can-
[01:14:09.440 --> 01:14:11.840]   Okay, but here's the problem.
[01:14:11.840 --> 01:14:14.080]   Any moral compass would be better than none.
[01:14:14.960 --> 01:14:17.440]   I don't know that I agree with that because-
[01:14:17.440 --> 01:14:18.240]   Yeah, very well.
[01:14:18.240 --> 01:14:23.440]   And part of the challenge that we keep running into is, who's morals?
[01:14:23.440 --> 01:14:24.800]   Who's worldview?
[01:14:24.800 --> 01:14:31.200]   Yeah, but right now we have an amoral situation where corporations are not incentive,
[01:14:31.200 --> 01:14:39.600]   incented in any way to act morally at all. Their entire organization is to maximize profits,
[01:14:39.600 --> 01:14:43.120]   regardless of whatever ethical considerations there would be.
[01:14:44.000 --> 01:14:47.760]   And I don't know if you can teach Sundar Pachai or Satya Nadella.
[01:14:47.760 --> 01:14:56.080]   I would guess Sundar Pachai and Satya Nadella and maybe even Jeff Bezos have some sort of moral code.
[01:14:56.080 --> 01:15:02.640]   But maybe that is the problem, is that the incentives are all there to make a money,
[01:15:02.640 --> 01:15:07.840]   to your fiduciary responsibility, to your shareholders, stakeholders,
[01:15:07.840 --> 01:15:10.960]   overwhelms whatever moral code you have. I don't know, is that the problem?
[01:15:12.320 --> 01:15:15.040]   What is the goal then? What would be the replacement goal?
[01:15:15.040 --> 01:15:20.640]   I understand. You and I may differ on some of the details of what's right and wrong.
[01:15:20.640 --> 01:15:27.280]   But I think at least in the Judeo-Christian world, we have some sense
[01:15:27.280 --> 01:15:32.400]   of what direction would be good and what would not. You disagree?
[01:15:32.400 --> 01:15:38.560]   I think, again, if we're talking with this level of ambiguity, we're probably on the same page.
[01:15:38.560 --> 01:15:42.880]   I think that the challenge comes when you have to make a decision on the fly in that moment.
[01:15:42.880 --> 01:15:50.240]   I agree that there isn't going to be a perfect litmus test, but that you would tend the arc,
[01:15:50.240 --> 01:15:55.280]   as Martin Luther King said, the arc of history would lean towards moral justice.
[01:15:55.280 --> 01:16:01.440]   If it wouldn't be perfect, we might disagree on the details, but it would tend toward moral justice.
[01:16:01.440 --> 01:16:05.040]   It does not seem to be tending that way. It seems to be tending towards profit,
[01:16:05.040 --> 01:16:08.160]   greater profit. We also don't allow people to change their minds.
[01:16:08.160 --> 01:16:15.920]   Listen, the whole Earth catalog, Stuart Brand has changed his mind over the years about lots
[01:16:15.920 --> 01:16:22.240]   of different things. Some of those early tech visionaries change their mind.
[01:16:22.240 --> 01:16:26.080]   He doesn't believe in... What would you say?
[01:16:26.080 --> 01:16:27.520]   I'm not going to talk about what he believes in.
[01:16:27.520 --> 01:16:27.840]   Okay.
[01:16:27.840 --> 01:16:28.560]   What I'm saying is...
[01:16:28.560 --> 01:16:35.040]   It evolves. I used to have changes in evolves.
[01:16:35.040 --> 01:16:38.880]   Yeah. If we're talking about the strategic direction of an organization
[01:16:38.880 --> 01:16:45.600]   whose products and services ultimately rely on our data, to me, that's what all of this comes down
[01:16:45.600 --> 01:16:52.480]   to is data. The fact that Apple has launched a financial instrument where the traditional
[01:16:52.480 --> 01:16:57.440]   ways of making money on that instrument, i.e. a credit card, is not through the transaction.
[01:16:57.440 --> 01:17:02.640]   In fact, they make no money on that transaction, which tells us what, that the data are more
[01:17:02.640 --> 01:17:06.160]   valuable to this organization than the transaction itself.
[01:17:06.160 --> 01:17:07.120]   Isn't that fascinating?
[01:17:07.120 --> 01:17:07.680]   Yeah.
[01:17:07.680 --> 01:17:13.280]   Right. So, we have to ask... Therefore, the question that we have to ask is,
[01:17:13.280 --> 01:17:22.400]   who should have the right to those data under what circumstances?
[01:17:22.400 --> 01:17:26.720]   And the answers to those questions, I think, are a little challenging.
[01:17:26.720 --> 01:17:30.320]   So let's start there. We all seem to agree.
[01:17:30.320 --> 01:17:35.440]   That's a good starting place. It may not be 100% of the problem, but this issue of the data,
[01:17:35.440 --> 01:17:42.400]   our data, seems to be consensus, right? That this is one of the big problems, big tech,
[01:17:42.400 --> 01:17:49.280]   is imposing on us. Let me take a break. And then we'll come back and discuss this.
[01:17:49.280 --> 01:17:53.920]   And I want... Now, Corey, you're a California voter. Amy, you're not. I don't know how well...
[01:17:53.920 --> 01:17:55.360]   I'm a Canadian, so I'm a California...
[01:17:55.360 --> 01:17:57.440]   Oh, you don't get to float by standard.
[01:17:58.000 --> 01:18:00.240]   So, well, you could tell me... I voted a month ago.
[01:18:00.240 --> 01:18:04.400]   I did this week. But there is a proposition on the ballot.
[01:18:04.400 --> 01:18:11.120]   California has a very aggressive privacy law, but the guy who wrote this originally,
[01:18:11.120 --> 01:18:15.280]   it's a complicated story, had made it the Alistair McTaggart.
[01:18:15.280 --> 01:18:20.320]   A good Scot's name had apparently made a deal with the California legislature that if you pass
[01:18:20.320 --> 01:18:26.640]   this law, I won't put my proposition on the ballot. So they did. He has now come back and it is back
[01:18:26.640 --> 01:18:33.120]   on the ballot, Proposition 24. I don't know if you've read up on it. I'm just curious
[01:18:33.120 --> 01:18:41.360]   if this consumer personal information law and agency initiative is a step in the direction that
[01:18:41.360 --> 01:18:49.520]   you two can agree we need to make, or if GDPR is, or if the current California privacy legislation is.
[01:18:49.520 --> 01:18:55.040]   I actually have a lot of people asking me about Prop 24 and what I think they should do.
[01:18:56.160 --> 01:19:02.480]   I can tell you I voted for it, but it is just like lead in the water and binge drinking.
[01:19:02.480 --> 01:19:08.160]   There can be a definite diversity of opinion on this thing.
[01:19:08.160 --> 01:19:15.600]   I would love to talk about that. Maybe we can narrow it down. Let's talk about the data.
[01:19:15.600 --> 01:19:20.960]   What do we do? There's Tim Berners-Lee has an initiative. Is it en-rupt to
[01:19:22.160 --> 01:19:28.080]   somehow contain our data and control it so that we can then sell it off if we wish to or not,
[01:19:28.080 --> 01:19:32.960]   somehow mine our own data for our benefit instead of for the benefit of these
[01:19:32.960 --> 01:19:37.440]   big tech companies. Maybe that's a solution. Let's talk about solutions when we come back.
[01:19:37.440 --> 01:19:42.800]   Right now though, I want to talk about our sponsor ExtraHOP, the new IT reality.
[01:19:42.800 --> 01:19:50.320]   Pretty obvious remote access on a massive scale. Of course, we're seeing rapid cloud and even
[01:19:50.320 --> 01:19:57.360]   multi-cloud adoption and all on top of a steady increase of internet of things, devices, and
[01:19:57.360 --> 01:20:04.560]   rife cybercrime. It's an interesting environment. If you are managing a cloud enterprise,
[01:20:04.560 --> 01:20:10.000]   it is an interesting place to be doing business. In this post-compromised world, it's more important
[01:20:10.000 --> 01:20:16.400]   than ever that organizations can see everything going on in their environment. From the cloud to
[01:20:16.400 --> 01:20:21.840]   the data center all the way down to the customer. In order to prevent an incident from becoming a
[01:20:21.840 --> 01:20:25.760]   full-scale data breach, you need more than just the information, more than unified visibility.
[01:20:25.760 --> 01:20:31.360]   You need context for your detections and then intelligent response workflows so you can
[01:20:31.360 --> 01:20:37.520]   act quickly. Teams can collaborate. You can get the job done. That's what ExtraHOP is all about.
[01:20:37.520 --> 01:20:44.720]   ExtraHOP stops, breaches 70 percent faster. ExtraHOP eliminates blind spots in your business and
[01:20:44.720 --> 01:20:50.560]   detects threats. Other tools miss to keep you secure and available with SaaS-based cloud,
[01:20:50.560 --> 01:20:54.800]   native network detection and response. They really were originally all about performance
[01:20:54.800 --> 01:21:00.000]   monitoring. They realized, "Hey, we're monitoring the performance. We can also monitor threats."
[01:21:00.000 --> 01:21:08.880]   That's when things really clicked. Threat detection and response in the cloud, on the network,
[01:21:09.920 --> 01:21:16.080]   tool consolidation. You get the information you need and you can act on it. Here's some examples.
[01:21:16.080 --> 01:21:22.320]   Wizards of the Coast uses AWS cloud and they use it with ExtraHOP. Their chief architect and
[01:21:22.320 --> 01:21:26.560]   information security officer, Daniel said, and this is the quote, "There's no other company that
[01:21:26.560 --> 01:21:31.760]   aligns to supporting our DevOps model, the speed and the lack of friction than ExtraHOP."
[01:21:31.760 --> 01:21:37.600]   Alt-to-beauty, you've probably seen them in the mall. They use ExtraHOP to secure their Google Cloud
[01:21:37.600 --> 01:21:41.600]   as well as to keep networking and security teams closely aligned. So, engineers have more time to
[01:21:41.600 --> 01:21:46.720]   focus on innovation. Their senior IT engineer, John Kreese, said, quote, "Before ExtraHOP,
[01:21:46.720 --> 01:21:51.280]   we had limited visibility to what was going on in the cloud. But now we can quickly identify
[01:21:51.280 --> 01:21:56.000]   vulnerabilities and exploits and understand how our applications are performing in the cloud.
[01:21:56.000 --> 01:22:02.720]   You've got to see the demo. It's on the site, extrahop.com/twit, to get a sense of what ExtraHOP can do for
[01:22:02.720 --> 01:22:07.680]   you and look at all the companies that use ExtraHOP. The ability to see what's going on, to see the
[01:22:07.680 --> 01:22:14.080]   threats, to respond coherently effectively in a timely way to whatever's happening on your network.
[01:22:14.080 --> 01:22:20.960]   This is the key. ExtraHOP.com/twit. You'll find out how ExtraHOP stops, breaches,
[01:22:20.960 --> 01:22:25.760]   70 percent faster. There's a free trial there and a complete demo so you can see the ExtraHOP
[01:22:25.760 --> 01:22:32.480]   dashboard at work. ExtraHOP.com/twit. This is a tool you want to use. This is a tool you want to
[01:22:32.480 --> 01:22:38.560]   know about. If you are managing a complex multi-cloud environment, if you've got customers out there,
[01:22:38.560 --> 01:22:46.640]   if you've got data in here, you've got to have ExtraHOP. ExtraHOP.com/twit. All right, I figured I
[01:22:46.640 --> 01:22:57.840]   give you a minute or two to read Prop 24. Word for Word. I already voted so you can influence me,
[01:22:57.840 --> 01:23:02.080]   Corey. What do you think? I know you're a Canadian. You don't have to worry about this stuff.
[01:23:02.080 --> 01:23:07.600]   Well, there's two different parts to it. I'd be interested in what you're more interested in.
[01:23:07.600 --> 01:23:13.680]   The first is the pluses and minuses of 24, which, like you have, had a lot of people ask me about.
[01:23:13.680 --> 01:23:19.360]   And then the other one is this idea of whether putting prices on data will solve the data issue.
[01:23:19.360 --> 01:23:24.160]   I mean, I can answer either or both, but you tell me what you're more interested in.
[01:23:24.160 --> 01:23:33.760]   Well, the controversy over 24, well, one of the things McTaggart, the reason McTaggart came back
[01:23:33.760 --> 01:23:39.840]   to the proposition, and it's kind of a broken system in California that really comes from a
[01:23:39.840 --> 01:23:46.400]   legislature that prefers to stand back and let democracy take control. It's fairly easy to get
[01:23:46.400 --> 01:23:50.800]   these propositions on the ballot. You only need to collect a small number of signatures, which
[01:23:50.800 --> 01:23:55.920]   can be easily done if you've got the money to do it. So we get all sorts of kind of oddball
[01:23:55.920 --> 01:24:02.400]   conflicting propositions, but even kind of disingenuously worded propositions. So
[01:24:02.400 --> 01:24:07.520]   poor voters in California, and this might be true in other states now, have really got a challenge
[01:24:07.520 --> 01:24:14.240]   ahead of them to read these, to understand what they do. We have a very good California Consumer
[01:24:14.240 --> 01:24:22.800]   Privacy Act, but it did apparently cut out, you know, some carve outs for some companies.
[01:24:22.800 --> 01:24:28.240]   On the other hand, people have complained that Facebook had a hand in writing this ballot
[01:24:28.240 --> 01:24:36.080]   proposition. So this was this was their way of saying, oh, yeah, yeah. Oh, look, see, we're
[01:24:36.080 --> 01:24:42.960]   we're protecting you. So I don't I just it's impossible. It's really impossible for the average
[01:24:42.960 --> 01:24:50.000]   person to figure this out. And you know, given the amount of time we have and number of propositions
[01:24:50.000 --> 01:24:55.680]   we have to vote on, I often wonder if this is a system that's that's got any hope.
[01:24:55.680 --> 01:25:01.440]   But which would you like to know about? Would you like my views on 24?
[01:25:01.440 --> 01:25:08.320]   Start with 24. Start with 24. We'll get to in next. All right. So 24. The history of this is that
[01:25:08.320 --> 01:25:15.760]   the first time around CCPA was passed. It was passed in somewhat in haste. It had a few defects.
[01:25:15.760 --> 01:25:19.920]   They were really trying to get the initial proposition off the ballot two years ago.
[01:25:19.920 --> 01:25:26.800]   Sure. That's right. And then what I think from its backers perspective was more disturbing,
[01:25:26.800 --> 01:25:32.880]   was that it then became the target of just an endless slew of reform bills that would have
[01:25:32.880 --> 01:25:38.000]   made it worse, not better, that were heavily financially backed. And I think that they were
[01:25:38.000 --> 01:25:41.760]   just made at the thought that they were just going to have to spend the rest of their lives
[01:25:41.760 --> 01:25:47.120]   babysitting this bill and staving off this kind of attack. And so the ballot initiative, the new one,
[01:25:47.120 --> 01:25:52.960]   creates a requirement that the bill not be amended except in furtherance of its purpose,
[01:25:53.520 --> 01:25:56.800]   which on its face sounds like a good way to avoid that kind of skirmishing.
[01:25:56.800 --> 01:26:04.160]   I've been in the middle of people who I respect who are really ferociously disagreeing with each
[01:26:04.160 --> 01:26:08.080]   other about this who are normally on the same side. That's what I suspect. I have been taking
[01:26:08.080 --> 01:26:12.960]   these arguments, right? The person who tells me, "Well, the furtherance of the original purpose
[01:26:12.960 --> 01:26:17.760]   solves that problem to people who are skeptical of it." And they say, "Well, except that the way the
[01:26:17.760 --> 01:26:22.640]   bill is worded is it says to promote the competitiveness of Californians and the privacy of Californian
[01:26:22.640 --> 01:26:28.400]   people and the growth of California's economy and fundamental equities." So everything fits the purpose.
[01:26:28.400 --> 01:26:33.760]   So also, what is in the purpose? Well, I mean, there are lots of bad faith answers to that,
[01:26:33.760 --> 01:26:37.120]   but the question is, how much time do you want to spend litigating it? So rather than spending
[01:26:37.120 --> 01:26:40.960]   all your time in Sacramento lobbying over it, you might spend all your time in the Ninth Circuit
[01:26:40.960 --> 01:26:47.200]   litigating over it. And it's like it's six of one half dozen the other. And the idea that you make
[01:26:47.200 --> 01:26:52.320]   legislation that is bulletproof and that no one can come along and amend- That's just a bad idea
[01:26:52.320 --> 01:26:55.680]   on the face of that. Well, it's a lovely idea if you support the legislations.
[01:26:55.680 --> 01:27:05.760]   But it's not- It seems a recipe for disaster. It's wishful thinking, right? And so I think that
[01:27:05.760 --> 01:27:11.040]   24 makes some compromises that I wouldn't make. In particular, there's two things that I'm
[01:27:11.040 --> 01:27:16.560]   concerned about. The first is that it doesn't have a private right of action. And that means that
[01:27:16.560 --> 01:27:21.680]   if your rights are violated, you have to convince an attorney general to bring a case on your behalf.
[01:27:21.680 --> 01:27:25.920]   And the thinking was that there was no way that a California Chamber of Commerce and other big
[01:27:25.920 --> 01:27:32.960]   business lobbies would allow this to pass unless private right of action were stripped from it.
[01:27:32.960 --> 01:27:36.240]   Because once there's a private right of action, they're worried about so-called nuisance suits.
[01:27:36.240 --> 01:27:43.200]   And so I think that that's a pretty grave defect. And then the other grave defect for me
[01:27:43.200 --> 01:27:50.640]   is the- Sorry, I just blew my buffer on what the other grave defect is. That's a big one.
[01:27:51.520 --> 01:27:57.200]   It should tell you something that the people against Prop 20 floor include not only the
[01:27:57.200 --> 01:28:01.520]   Republican Party, but the Green Party, the Libertarian Party, the co-founder of the United
[01:28:01.520 --> 01:28:07.920]   Farm Workers, Dolores Squirta, the ACLU, the League of Women Voters, they don't want it.
[01:28:07.920 --> 01:28:10.960]   But then- So I don't want to say- I don't want to counsel you to vote against it because I'm
[01:28:10.960 --> 01:28:15.280]   thoroughly on the fence about it. I know. And I remember what the other thing was, which is that
[01:28:16.800 --> 01:28:21.600]   the thinking of the backers is that if it had a default to privacy where you had to opt in to
[01:28:21.600 --> 01:28:25.680]   have your data collected instead of opting out to not have your data collected, that this wouldn't
[01:28:25.680 --> 01:28:28.560]   pass First Amendment muster. And there are some first-
[01:28:28.560 --> 01:28:29.040]   Ah, that's interesting.
[01:28:29.040 --> 01:28:29.520]   ...ementric.
[01:28:29.520 --> 01:28:33.440]   ... animators I know who say that's wrong. And there are people with a good track record
[01:28:33.440 --> 01:28:38.800]   of litigating difficult tech-related First Amendment questions. And so I'm not a lawyer.
[01:28:38.800 --> 01:28:42.560]   I'm not a First Amendment lawyer, and I'm not a litigator. And so I'm left with these people.
[01:28:42.560 --> 01:28:47.440]   Normally the way that I resolve these issues is like you. I find a technical expert and I,
[01:28:47.440 --> 01:28:51.680]   who I trust and I ask them for their judgment. And generally the pool of technical experts I
[01:28:51.680 --> 01:28:56.240]   rely on agree in the main. And this is an area where they're very split.
[01:28:56.240 --> 01:28:56.720]   Right.
[01:28:56.720 --> 01:28:58.240]   And I'm, you know, it's-
[01:28:58.240 --> 01:28:59.440]   By the way, why I ask you-
[01:28:59.440 --> 01:29:00.080]   ... people I don't get to vote.
[01:29:00.080 --> 01:29:01.200]   That's why I asked you, Corey.
[01:29:01.200 --> 01:29:02.480]   I understand.
[01:29:02.480 --> 01:29:03.520]   And you have not helped me.
[01:29:03.520 --> 01:29:07.680]   I had a local candidate for City Council asked me this morning.
[01:29:07.680 --> 01:29:08.160]   Hey.
[01:29:08.160 --> 01:29:09.360]   How he should vote.
[01:29:09.360 --> 01:29:14.800]   I just want you to know I supported your candidate for the Texas Railroad Commission.
[01:29:14.800 --> 01:29:16.480]   Good. She's amazing.
[01:29:16.480 --> 01:29:17.520]   I sent her some money.
[01:29:17.520 --> 01:29:18.640]   Me too.
[01:29:18.640 --> 01:29:22.960]   This Amy, in case you weren't familiar with this, I can't remember. I think you might have
[01:29:22.960 --> 01:29:25.040]   mentioned this or maybe I read something that you wrote.
[01:29:25.040 --> 01:29:26.320]   Who picked this old lawyer?
[01:29:26.320 --> 01:29:26.560]   Yeah.
[01:29:26.560 --> 01:29:30.080]   T-Boon Pickens lawyer. That's who she is.
[01:29:30.080 --> 01:29:36.160]   That's funny because weirdly in Texas, it's the Railroad Commission that decides
[01:29:36.160 --> 01:29:40.080]   whether oil wells can burn off the natural gas.
[01:29:40.080 --> 01:29:41.600]   You've seen it all the time in movies.
[01:29:41.600 --> 01:29:44.320]   These flares coming off of the oil wells,
[01:29:44.320 --> 01:29:49.520]   which of course is a huge pollutant, a major contributor to greenhouse gases.
[01:29:49.520 --> 01:29:53.840]   And it's the Railroad Commission that decides whether you can do that or not.
[01:29:53.840 --> 01:29:57.840]   And she could turn the entire Texas Railroad Commission
[01:29:57.840 --> 01:30:02.960]   on its ear and stop this practice of flaring.
[01:30:02.960 --> 01:30:10.560]   They basically have Texas' emissions and the commission is structured so that it requires
[01:30:10.560 --> 01:30:13.440]   consensus to grant a permit so one person can block it.
[01:30:13.440 --> 01:30:14.560]   It's a three person commission.
[01:30:14.560 --> 01:30:14.960]   Amazing.
[01:30:14.960 --> 01:30:16.480]   It's this amazing race.
[01:30:16.480 --> 01:30:17.840]   I'll be watching that race.
[01:30:17.840 --> 01:30:21.920]   You wouldn't think I'd be watching the Texas Railroad Commission race come November 3.
[01:30:21.920 --> 01:30:24.400]   I'm interested in what Amy has to say about Prop 24.
[01:30:24.400 --> 01:30:24.960]   Yes.
[01:30:24.960 --> 01:30:27.120]   And you're not a voter in California, Amy.
[01:30:27.120 --> 01:30:30.960]   But and probably haven't been following it that closely.
[01:30:30.960 --> 01:30:33.840]   The reason it's important is as California goes, so goes the nation.
[01:30:33.840 --> 01:30:38.320]   It's a big enough economy that if California has these strong privacy laws,
[01:30:38.320 --> 01:30:41.920]   just as GD PUP, he really influenced everybody in the world,
[01:30:41.920 --> 01:30:44.960]   it would very much influence the United States' countries,
[01:30:44.960 --> 01:30:47.600]   companies, especially those who operate in California.
[01:30:47.600 --> 01:30:51.520]   So it's of importance not just to those of us in California, but to everybody.
[01:30:51.520 --> 01:30:58.080]   Yeah, but I guess I would put that within the context of what does the House and Senate look like
[01:30:58.080 --> 01:31:01.440]   and who's in power over the next, you know, so much time.
[01:31:01.440 --> 01:31:02.560]   The federal government.
[01:31:02.560 --> 01:31:03.920]   Here's why.
[01:31:03.920 --> 01:31:06.160]   We continue to have these.
[01:31:06.160 --> 01:31:07.840]   Listen, or maybe we should just make a compromise.
[01:31:07.840 --> 01:31:09.120]   You can't do it state by state.
[01:31:09.120 --> 01:31:10.800]   You're saying you need a federal law.
[01:31:10.800 --> 01:31:16.400]   All I can tell you is Amazon has a wristband out called the halo,
[01:31:16.400 --> 01:31:18.160]   which listens to you all the time.
[01:31:18.160 --> 01:31:19.920]   And you know, it's not quite a fit bit.
[01:31:19.920 --> 01:31:22.240]   It's not quite an Apple watch.
[01:31:22.240 --> 01:31:27.520]   This does some of the same things, but it also scrapes your biometric data
[01:31:27.520 --> 01:31:30.640]   to make inferences like, are you feeling hopeful right now?
[01:31:30.640 --> 01:31:31.760]   And are you exhausted?
[01:31:31.760 --> 01:31:32.800]   Things like that.
[01:31:32.800 --> 01:31:35.520]   You know, there's ambient noise that gets picked up on the other side.
[01:31:35.520 --> 01:31:37.920]   Yeah, you're wearing a microphone all the time.
[01:31:37.920 --> 01:31:38.960]   Right.
[01:31:38.960 --> 01:31:43.520]   Well, so not just when you say, hey, a word, but all the time.
[01:31:43.520 --> 01:31:46.720]   On the other hand, it works so poorly.
[01:31:46.720 --> 01:31:48.880]   I've talked to people who've used it one of our.
[01:31:48.880 --> 01:31:49.600]   It doesn't matter.
[01:31:49.600 --> 01:31:51.920]   The use that that doesn't matter to me.
[01:31:51.920 --> 01:31:52.800]   That doesn't matter.
[01:31:52.800 --> 01:31:56.720]   What matters to me is how do we manage consent?
[01:31:57.680 --> 01:32:02.400]   In a multi-state system where we've got state by state laws that differ.
[01:32:02.400 --> 01:32:05.840]   Yeah, but Amazon's never going to make something that you can't sell in California.
[01:32:05.840 --> 01:32:07.760]   Right.
[01:32:07.760 --> 01:32:11.360]   So what you're saying is that if this passes in California,
[01:32:11.360 --> 01:32:14.800]   then every other state privacy goes in the same direction.
[01:32:14.800 --> 01:32:16.800]   I don't think that's I don't think that's the case.
[01:32:16.800 --> 01:32:20.800]   I think it would impact companies that would like to sell a product.
[01:32:20.800 --> 01:32:24.160]   If they know that that just as California's emissions laws,
[01:32:24.160 --> 01:32:27.040]   California's emissions laws affected the world.
[01:32:27.040 --> 01:32:31.600]   It did, but even GDPR is managed differently,
[01:32:31.600 --> 01:32:35.600]   country by country in the EU and those the enforcement.
[01:32:35.600 --> 01:32:39.280]   And that's really what the whacked myself on the face with my own microphone.
[01:32:39.280 --> 01:32:43.920]   It's the real question comes down to enforcement.
[01:32:43.920 --> 01:32:49.440]   And GDPR is enforced differently within countries, within parts of countries,
[01:32:49.440 --> 01:32:52.560]   where you've got different local agents interpreting things differently.
[01:32:52.560 --> 01:32:54.640]   So by the way, I just tricked you guys.
[01:32:54.640 --> 01:32:59.920]   Because we've said all along, there's a solution, there's a solution,
[01:32:59.920 --> 01:33:02.880]   there's a legislative solution, there's a governmental solution.
[01:33:02.880 --> 01:33:07.520]   Here is a possible solution and neither one of you can agree
[01:33:07.520 --> 01:33:09.920]   on whether that's a good idea or not.
[01:33:09.920 --> 01:33:15.200]   I don't understand how, but I don't understand how we can even hope for a government solution.
[01:33:15.200 --> 01:33:16.560]   I want to hear Amy's other solution.
[01:33:16.560 --> 01:33:17.760]   Okay, what's your other solution?
[01:33:18.720 --> 01:33:22.240]   So a couple of years ago, I'm always trying to think through,
[01:33:22.240 --> 01:33:24.400]   I understand what the problems are.
[01:33:24.400 --> 01:33:27.760]   How do we, like what's the most clever solution?
[01:33:27.760 --> 01:33:29.360]   I like that.
[01:33:29.360 --> 01:33:34.320]   That's rarely, by the way, the calculus.
[01:33:34.320 --> 01:33:37.120]   It's usually more what can we get politically, what can we get through,
[01:33:37.120 --> 01:33:39.840]   what can we write an ad for.
[01:33:39.840 --> 01:33:43.440]   So what the most clever one is often the one that gets passed by.
[01:33:43.440 --> 01:33:44.800]   The cool thing everybody wants to talk about,
[01:33:44.800 --> 01:33:48.480]   all the government leaders are talking about data ownership as though
[01:33:49.120 --> 01:33:49.840]   that makes sense.
[01:33:49.840 --> 01:33:52.320]   But what would it take for there to be data ownership anyhow?
[01:33:52.320 --> 01:33:53.840]   So here's what I have been mulling.
[01:33:53.840 --> 01:33:56.640]   What I've been mulling is for a while.
[01:33:56.640 --> 01:34:03.200]   So Alaska has, the state of Alaska, if you live in Alaska and if you are a resident,
[01:34:03.200 --> 01:34:07.680]   I think you have to have been born there, Norway does something similar.
[01:34:07.680 --> 01:34:13.760]   You get sort of a basic universal basic income dividend check from oil.
[01:34:14.720 --> 01:34:19.680]   And my thinking was, we have a social safety net in the United States that's going to collapse.
[01:34:19.680 --> 01:34:22.720]   And that was true pre-COVID.
[01:34:22.720 --> 01:34:30.000]   And we have long-term medical, the medical needs of people who are aging are not going to be met.
[01:34:30.000 --> 01:34:32.640]   Also true before COVID.
[01:34:32.640 --> 01:34:36.800]   So is there some way that we can solve some of those problems while also solving the problems of
[01:34:36.800 --> 01:34:44.160]   data and where I came back to was, if it's the case that all of our PIs are at this point,
[01:34:44.160 --> 01:34:47.360]   increasingly being consolidated by just a few companies,
[01:34:47.360 --> 01:34:54.640]   wouldn't there be a way to consolidate not all of them, but a lot of them, such that they were
[01:34:54.640 --> 01:34:59.280]   anonymized, but available and permissions could be given as needed.
[01:34:59.280 --> 01:35:01.920]   So I was thinking, is there some kind of.
[01:35:01.920 --> 01:35:02.880]   So that's interrupt.
[01:35:02.880 --> 01:35:07.200]   So now we've moved to the second part of Corey's question, which is, all right,
[01:35:07.200 --> 01:35:12.640]   and I agree with you, maybe the clever way is, I don't know how you would enforce that though,
[01:35:12.640 --> 01:35:14.560]   Amy. How would you make that work?
[01:35:14.560 --> 01:35:20.000]   Right, because if my thought was that with every, so with every,
[01:35:20.000 --> 01:35:25.840]   let's call it a packet transfer for just for the sake of conversation, that there,
[01:35:25.840 --> 01:35:32.640]   anytime that anybody's data were monetized, that there would be some way, some fraction of ascent
[01:35:32.640 --> 01:35:37.360]   that would go back to the individual and the individual would have the ability to determine
[01:35:37.360 --> 01:35:39.120]   how those data were being used.
[01:35:39.120 --> 01:35:43.760]   And again, this is a thorny, challenging issue because there are so many,
[01:35:43.760 --> 01:35:45.360]   how would you define data at that point?
[01:35:45.360 --> 01:35:50.000]   But so anyhow, my idea was that this does a couple of things.
[01:35:50.000 --> 01:35:54.000]   One, it forces more data literacy and more technological literacy,
[01:35:54.000 --> 01:35:56.560]   which I think is a good thing.
[01:35:56.560 --> 01:36:01.040]   Two, it helps solve some of these future social safety net issues, which are coming,
[01:36:01.040 --> 01:36:02.320]   and we are not prepared for.
[01:36:02.320 --> 01:36:08.560]   And three, it potentially incents the big tech companies to do better because it avoids
[01:36:08.560 --> 01:36:14.240]   regular, it avoids what they're currently facing and forces them to give up a little bit and forces
[01:36:14.240 --> 01:36:18.960]   the market to give up a little bit, but not in a way that's going to cause anything to crash.
[01:36:18.960 --> 01:36:21.920]   So that was the idea that I had been playing with.
[01:36:21.920 --> 01:36:28.240]   And it seemed as though a DLT distributed ledger, which would give everybody the ability
[01:36:28.240 --> 01:36:32.880]   to make some of those decisions would also force them to have to come to grips with
[01:36:32.880 --> 01:36:34.960]   what data are being collected in one way.
[01:36:34.960 --> 01:36:41.760]   But how do you convince Facebook and Google and Amazon to adopt this DLT?
[01:36:41.760 --> 01:36:42.560]   Well, hang on.
[01:36:42.560 --> 01:36:49.600]   I think you're getting ahead of the game here by starting with, let's go ahead with it.
[01:36:49.600 --> 01:36:54.720]   I'm skeptical of the, I think maybe we found the area where Amy and I disagree.
[01:36:54.720 --> 01:37:00.800]   With all due respect and I understand where, I hope I understand where you're coming from,
[01:37:00.800 --> 01:37:04.560]   but I think that there are a couple of pretty important flaws in that proposal.
[01:37:05.040 --> 01:37:10.240]   The first one is the idea that we can have clear title to personal identifying information.
[01:37:10.240 --> 01:37:15.040]   Most of the most valuable PII is in relation to other people.
[01:37:15.040 --> 01:37:21.440]   So the fact that you are your mother's son is a fact that in theory, both of you own,
[01:37:21.440 --> 01:37:26.560]   right? And that at the same time, your father probably has some claim over as well.
[01:37:26.560 --> 01:37:31.040]   And that I might know as an unrelated third party.
[01:37:31.040 --> 01:37:36.800]   And so the idea that we could have a market in which we either arbitrarily assign clear title to
[01:37:36.800 --> 01:37:41.040]   one of those people, which you could imagine incredible disfunctions, right? What if your
[01:37:41.040 --> 01:37:45.600]   mother owns the fact that you're her son? And that means that you can't post on Facebook about
[01:37:45.600 --> 01:37:50.880]   how she abused you as a child, right? Or your former boss has clear title to the fact that
[01:37:50.880 --> 01:37:55.120]   she's your former boss or he's your former boss. And you can't warn other people about the workplace
[01:37:55.120 --> 01:38:00.320]   conditions because they own that fact, right? So this is the first problem. We had this blended
[01:38:00.320 --> 01:38:04.960]   ownership that doesn't lend itself to market transactions to begin with.
[01:38:04.960 --> 01:38:10.640]   And the second problem is that if we acknowledge that privacy abuses are themselves harmful,
[01:38:10.640 --> 01:38:17.840]   we do not want to make some social good contingent on their expansion, right? It's like funding
[01:38:17.840 --> 01:38:23.360]   cancer research with tobacco sales. The best thing for cancer would be for tobacco sales to be
[01:38:23.360 --> 01:38:30.160]   eliminated. We do not want the cancer research industry in some way reliant on increases in
[01:38:30.240 --> 01:38:33.920]   tobacco sales. We want to decouple those things. We were talking before about incentives.
[01:38:33.920 --> 01:38:39.920]   I think this is a very powerful, poor incentive. And Ed Felton, who used to be the CTO or Deputy
[01:38:39.920 --> 01:38:43.360]   CTO of the Federal Trade Commission, who was in the White House Office of Tech Policy,
[01:38:43.360 --> 01:38:48.720]   and has laid a Princeton, he talked about this once in an article where he talked about interests.
[01:38:48.720 --> 01:38:54.800]   About how when we talk about human life, for example, we don't create property rights to
[01:38:54.800 --> 01:38:58.960]   regulate it. And in fact, creating property rights in human life is kind of the mark of
[01:38:58.960 --> 01:39:06.000]   sociopathy, right? A murder is not theft of life. Murder is a crime unto itself. We have a whole
[01:39:06.000 --> 01:39:11.360]   language of describing it. And for that reason, we also have a language of describing a relationship
[01:39:11.360 --> 01:39:16.560]   to one another that's not grounded in exclusive rights. My daughter and I have a relationship to
[01:39:16.560 --> 01:39:20.560]   one another in which I have an interest in her. And she has an interest in herself. And her
[01:39:20.560 --> 01:39:25.600]   school has an interest in her grandparents, my wife, who is her mother, her friends, and many
[01:39:25.600 --> 01:39:30.560]   other people all have varying degrees about legal and ethical claim to her because she is
[01:39:30.560 --> 01:39:36.720]   sui genera. She's a human and in no way like a thing that we make markets in. And I think information,
[01:39:36.720 --> 01:39:41.840]   person identifying information is much the same. And that so much of the dysfunction that we have
[01:39:41.840 --> 01:39:49.200]   arises from the fact that PII is not amenable to propertization. And then finally, I worry that
[01:39:49.200 --> 01:39:53.680]   although it would be great to have more information literacy, that you would mostly get information
[01:39:53.680 --> 01:39:58.720]   fatigue, that having to understand all of the different transactions that you opted into just
[01:39:58.720 --> 01:40:02.960]   by moving through time and space would create a kind of nihilism and that you would end up
[01:40:02.960 --> 01:40:07.360]   with privacy as a luxury good where the wealthy could afford the professional class who would
[01:40:07.360 --> 01:40:13.600]   help them navigate that. And everybody else would just be opted into adhesion contracts where all
[01:40:13.600 --> 01:40:20.240]   of their data was harvested nonconsensually with the pretense of consent and abused without
[01:40:21.280 --> 01:40:27.120]   effectively without limit. Right, so privacy is currently a luxury good. I don't think we should
[01:40:27.120 --> 01:40:32.560]   kid ourselves or try to convince ourselves otherwise. It's those people who both have knowledge
[01:40:32.560 --> 01:40:40.880]   and the financial wherewithal to use tools and systems to prevent others from tracking them.
[01:40:40.880 --> 01:40:46.960]   You know, it is that group of people. It is the wealthy who have the ability to be anonymous
[01:40:48.400 --> 01:40:54.720]   or to be private as they need. So I think that that's an addressable issue,
[01:40:54.720 --> 01:41:00.640]   but I don't think that that is so meaningful right with this argument. Now,
[01:41:00.640 --> 01:41:07.920]   in terms of all the PII, I don't think that we are and Corey rightfully
[01:41:07.920 --> 01:41:16.640]   explained. I don't think that tracking every single piece of this makes feasible or logistical
[01:41:16.640 --> 01:41:26.080]   sense. That being said, my favorite parlor trick is having people go to Google and open up their
[01:41:26.080 --> 01:41:33.680]   locations and the number of people who have absolutely no idea just how much they're being
[01:41:33.680 --> 01:41:40.800]   tracked all the time to me means that we are creating future problems for ourselves.
[01:41:40.800 --> 01:41:47.760]   So out of ignorance. Listen, my husband is not my husband.
[01:41:47.760 --> 01:41:48.560]   No smart.
[01:41:48.560 --> 01:41:53.360]   He's a successful guy. I remember sitting on the couch with him just after our daughter
[01:41:53.360 --> 01:41:58.960]   was born 10 years ago. He had something open and I was like, "Hey, you want me to show you
[01:41:58.960 --> 01:42:05.120]   everywhere you've been for the past, whatever?" And I pulled that open and we didn't speak for two
[01:42:05.120 --> 01:42:05.440]   days.
[01:42:05.440 --> 01:42:06.880]   What? He was pissed.
[01:42:06.880 --> 01:42:15.600]   I mean, there was nothing wrong. I think it was just this crushing sense of having not known that.
[01:42:15.600 --> 01:42:23.920]   And this is a highly educated person. So I think we need to decouple the education and understanding
[01:42:23.920 --> 01:42:31.280]   of how technology and most importantly, our data work from the business of sorting out who owns
[01:42:31.280 --> 01:42:36.240]   and has access to those data and how they can be used. And while I totally agree, we can't
[01:42:36.240 --> 01:42:43.760]   incent the companies to force us to create more data in order to earn more revenue.
[01:42:43.760 --> 01:42:49.680]   If that becomes a feasible UBI because then we all game the system and then the whole thing
[01:42:49.680 --> 01:42:57.040]   collapses. My point was I think that we are moving in this direction anyway. I believe that PII
[01:42:57.040 --> 01:43:01.360]   are quickly morphing into what, again, I call a PDR, which is personal data record, which is sort
[01:43:01.360 --> 01:43:06.160]   of one-stop shopping for all your stuff. I know that Salesforce has started to lay the groundwork
[01:43:06.160 --> 01:43:10.240]   when it comes to health and education. I know we all know what Google, Apple,
[01:43:10.240 --> 01:43:17.360]   and Amazon are doing some of the same things. Wouldn't you rather have a non-commercial entity
[01:43:17.360 --> 01:43:23.840]   housing your data in a way that is less icky? And I think the answer to that is yes.
[01:43:23.840 --> 01:43:32.320]   Because to me, this seems to be an inevitability. And again, why I think we ought to address some
[01:43:32.320 --> 01:43:37.680]   of these, the future plausible problems that we're going to be facing at the same time that we deal
[01:43:37.680 --> 01:43:43.280]   with the current issues around monopolies that we face today. I feel like if in a country where we
[01:43:43.280 --> 01:43:50.000]   can't agree with or amask causes COVID or not, we're going to have a lot of trouble getting a
[01:43:50.000 --> 01:43:56.960]   consensus on what privacy is, what companies should be allowed to do. Honest to God, most people
[01:43:56.960 --> 01:44:03.840]   would rather we were talking about the iPhone 12 than this. Because it feels hopeless.
[01:44:03.840 --> 01:44:07.440]   No, it's not that it's hopeless. It's hopeless.
[01:44:07.440 --> 01:44:09.200]   Listen, I think everybody-
[01:44:09.200 --> 01:44:15.120]   I was going to say, I think if everybody cared as deeply about privacy as the journalists do,
[01:44:15.120 --> 01:44:20.160]   I think we would see significant behavioral shifts.
[01:44:20.160 --> 01:44:23.600]   So is that where we start? Is getting people to care?
[01:44:23.600 --> 01:44:28.320]   No, I don't think we do get people to care. I lived in Japan for a really long time.
[01:44:28.320 --> 01:44:34.720]   And the way that you say the word privacy in Japanese is put Aibashi. There was no word for
[01:44:34.720 --> 01:44:36.480]   privacy. I'm not making this up.
[01:44:36.480 --> 01:44:38.640]   Well, privacy is a fairly modern concept.
[01:44:38.640 --> 01:44:45.680]   Well, it's a very modern concept because of modern technology in Japan. They never had to
[01:44:45.680 --> 01:44:52.480]   talk about it before. And I guess what I'm trying to say is I just don't think this
[01:44:52.480 --> 01:44:58.960]   fascination that this fetishized privacy at this point when we talk about it.
[01:44:58.960 --> 01:45:05.840]   But our behaviors don't mirror the conversations. And I don't think the everyday person,
[01:45:05.840 --> 01:45:14.880]   I'm on the IRC right now, or the IRC channel. And some of the same questions are being asked
[01:45:14.880 --> 01:45:20.640]   there. I don't think the everyday person cares so much about how they're being tracked.
[01:45:20.640 --> 01:45:23.840]   Or if they do the conveniences of the systems that they're using
[01:45:23.840 --> 01:45:30.640]   are so overwhelmingly positive that they are happy to give up that privacy in exchange for
[01:45:30.640 --> 01:45:32.480]   the convenience of having access to all that.
[01:45:32.480 --> 01:45:36.160]   They may be faced with that, crux, even in Japan, because Japan, India,
[01:45:36.160 --> 01:45:42.560]   and the Five Eyes governments, which include the US and the UK, have all called at the Five Eyes
[01:45:42.560 --> 01:45:50.240]   meeting for back doors in encryption. It's essentially the US, the UK,
[01:45:50.240 --> 01:45:57.280]   Canada, Australia, New Zealand, Japan, and India agreeing that end-to-end encryption is a threat to
[01:45:57.280 --> 01:46:03.440]   national security. National security. Yeah. Well, for an end point of view, it is.
[01:46:06.240 --> 01:46:12.560]   So I would love to kind of take a step back and talk about people's revealed preferences and
[01:46:12.560 --> 01:46:18.880]   privacy. So that the widest adopted consumer boycott in the history of the world is tracker
[01:46:18.880 --> 01:46:22.960]   blockers. One in four web users worldwide. Isn't that interesting? That's a very good point.
[01:46:22.960 --> 01:46:27.360]   Right. I don't know if that's about privacy. But privacy is much, Corey, as I just don't want
[01:46:27.360 --> 01:46:30.960]   to be slowed down, I don't want to see the ads. I bet you that's half the reason.
[01:46:31.440 --> 01:46:36.880]   Well, I mean, it's hard to disentangle, but there's certainly an awful lot of it. And
[01:46:36.880 --> 01:46:41.440]   one of the things that we know from the GDPR experiment, which has gotten us some facts and
[01:46:41.440 --> 01:46:47.040]   evidence, is that if you serve first-party context ads instead of third-party behavioral ads,
[01:46:47.040 --> 01:46:51.600]   more of your ads get through the blockers that users choose, about twice as many. So
[01:46:51.600 --> 01:46:58.320]   users are choosing the kind of blocker that blocks third-party behavioral ads, but not first-party
[01:46:58.320 --> 01:47:02.560]   context ads. And those are the privacy invasive versus non-privacy invasive ads.
[01:47:02.560 --> 01:47:08.720]   Well, I only do first-party contextual ads, as you know. But I have to say, that's probably more
[01:47:08.720 --> 01:47:12.480]   because it's easier technologically to block third-party ads than it is to block first-party ads.
[01:47:12.480 --> 01:47:16.640]   But you also get, I don't know about that, but you also get twice as much money because you
[01:47:16.640 --> 01:47:22.480]   don't have to give a vig to the ad broker when you're doing first-party ads. The other thing is
[01:47:23.440 --> 01:47:30.000]   that the most successful challengers to social networks have been privacy forward. So
[01:47:30.000 --> 01:47:33.520]   Snap, for example, was the most successful challenger to Facebook. They were only able to
[01:47:33.520 --> 01:47:39.120]   defeat it ironically by spying on users with a deceptive fake battery monitor called Unavo
[01:47:39.120 --> 01:47:44.320]   that spied on how people were using Snap and replicated Snap's functionality and Insta.
[01:47:44.320 --> 01:47:49.200]   Snap's pitch was, "We're like Facebook except your data doesn't hang around." Ironically,
[01:47:49.200 --> 01:47:54.240]   Facebook's pitch for its first 10 years was, "We're like MySpace except we don't like Google
[01:47:54.240 --> 01:48:00.960]   index it or third-party see it. It's private." Right? Facebook built its fortune on being the
[01:48:00.960 --> 01:48:07.200]   privacy forward network. So I think that it's true that people tend to underweight the future
[01:48:07.200 --> 01:48:13.040]   costs of privacy disclosures because it's hard to understand the connection between a privacy
[01:48:13.040 --> 01:48:18.400]   disclosure and a future harm. But people retrospectively regret privacy disclosures.
[01:48:18.400 --> 01:48:22.640]   I think it makes sense. When those harms arise. And so you could say that people like smoking
[01:48:22.640 --> 01:48:27.840]   because we have a lot of expensively sown doubt about the relationship between tobacco and cancer
[01:48:27.840 --> 01:48:33.440]   and the only people who are angry about tobacco are dying of lung cancer. And now it's too late.
[01:48:33.440 --> 01:48:42.960]   It actually turned out that when we prohibited firms from lying about the harms of their products
[01:48:43.600 --> 01:48:50.400]   and when we instituted vigorous public education about those harms that the public's consciousness
[01:48:50.400 --> 01:48:56.400]   shifted. The same is true of climate. The reveal preferences of Americans is we want
[01:48:56.400 --> 01:49:02.640]   trucks bigger than God. We don't care how much dinosaurs we have to pour in the gas tank.
[01:49:02.640 --> 01:49:06.320]   But when you ask Americans, "How do you feel about your red sky, the hurricane,
[01:49:06.320 --> 01:49:12.800]   the invasive mosquitoes, the refugee crisis?" None of them are going to say that that was a
[01:49:12.800 --> 01:49:16.320]   good bargain for their SUV. But then it's too late.
[01:49:16.320 --> 01:49:24.320]   So this is why I think you see journalists and activists who are ahead of the public consensus.
[01:49:24.320 --> 01:49:27.920]   Because what they're trying to do is reach the point of peak indifference before the point of no
[01:49:27.920 --> 01:49:34.160]   return. And if peak indifference only arises after you've got a lung full of tumors,
[01:49:34.160 --> 01:49:37.280]   then you might as well keep smoking and enjoying it for the years you got left.
[01:49:37.280 --> 01:49:42.400]   By the time you care about crashing animal populations is when there's only one rhino
[01:49:42.400 --> 01:49:45.360]   left, we might as well find out what he tastes like because he's not coming back.
[01:49:45.360 --> 01:49:51.760]   So this is why you have an activist cohort that tries to lead the consensus.
[01:49:51.760 --> 01:49:57.280]   What do you think? I think that sounds about right. Amy?
[01:49:57.280 --> 01:50:00.800]   I wonder how we do that.
[01:50:00.800 --> 01:50:02.480]   I don't have anything substantive to add.
[01:50:02.480 --> 01:50:07.280]   I guess we're in the activist cohort. So is it our job to come up with better and more
[01:50:07.280 --> 01:50:14.080]   subversive propaganda to achieve our goals? Maybe do some advertising or write a book?
[01:50:14.080 --> 01:50:16.960]   How about a book called a tax surface? What about that?
[01:50:16.960 --> 01:50:19.840]   I mean, I try to achieve that's what you do.
[01:50:19.840 --> 01:50:21.040]   That's what you're trying to do.
[01:50:21.040 --> 01:50:21.600]   That's how you're trying to do.
[01:50:21.600 --> 01:50:22.640]   It's not what you're trying to do.
[01:50:22.640 --> 01:50:22.640]   Yeah.
[01:50:22.640 --> 01:50:22.640]   Yeah.
[01:50:22.640 --> 01:50:26.800]   Through argument. I mean, it's just another way of presenting the argument.
[01:50:26.800 --> 01:50:26.800]   Yeah.
[01:50:26.800 --> 01:50:33.040]   Or well gave us 50 years of being able to head off all abstract policy discussions about
[01:50:33.040 --> 01:50:40.960]   surveillance by saying, "Stopping so Orwellian." And that I think is the thing that art can
[01:50:40.960 --> 01:50:43.040]   infuse our policy discussions with.
[01:50:43.040 --> 01:50:45.840]   It's funny that we should come down to that because ultimately that's what art is about.
[01:50:45.840 --> 01:50:49.280]   And that is what artists are about.
[01:50:49.280 --> 01:50:53.920]   And futurists. I imagine futurists have a little bit of that going on too.
[01:50:53.920 --> 01:50:59.040]   I showed it a brought to you by Wasabi. Not the green stuff on your sushi,
[01:50:59.040 --> 01:51:04.160]   although delicious as that might be. I'm talking about Wasabi hot cloud storage.
[01:51:04.160 --> 01:51:06.320]   It's the perfect solution.
[01:51:06.320 --> 01:51:11.840]   If you're a company buying more hard drives all the time, more local storage,
[01:51:11.840 --> 01:51:13.200]   what are you doing?
[01:51:13.200 --> 01:51:19.600]   If you know, we need another terabyte this week, next week and every week or a petabyte
[01:51:19.600 --> 01:51:22.560]   a month or whatever it is, you should be going to the cloud.
[01:51:22.560 --> 01:51:23.760]   You should be going to Wasabi.
[01:51:23.760 --> 01:51:27.520]   First of all, you might say, "Well, wait a minute. What about those other guys?"
[01:51:27.520 --> 01:51:31.200]   No, Wasabi is a fifth the cost of Amazon S3.
[01:51:31.200 --> 01:51:39.040]   It is significantly faster. It's so affordable, and it actually will cost you less than the
[01:51:39.040 --> 01:51:43.040]   maintenance fees of the on-prem storage you're looking at buying.
[01:51:43.040 --> 01:51:45.200]   Less than just the maintenance fees.
[01:51:45.200 --> 01:51:50.320]   And I would argue it's more secure. It's more reliable than anything on premises.
[01:51:50.320 --> 01:51:55.360]   11 nines of durability. I don't think you get more than that.
[01:51:55.360 --> 01:51:59.040]   They're hosted in Premier Tier 4 data centers, highly secure.
[01:51:59.040 --> 01:52:03.040]   They're fully redundant. That's good because Wasabi has active integrity checking.
[01:52:03.040 --> 01:52:07.360]   Every object stored on Wasabi is checked for integrity every 90 days.
[01:52:07.360 --> 01:52:09.120]   If one bit goes missing, no problem.
[01:52:09.120 --> 01:52:11.440]   These are redundant data centers.
[01:52:11.440 --> 01:52:13.360]   Remember, we can get the data back.
[01:52:13.360 --> 01:52:15.600]   You're never going to lose not even one bit.
[01:52:15.600 --> 01:52:17.440]   It's secure by default.
[01:52:17.440 --> 01:52:22.320]   Even if you don't request it, all data is stored, encrypted at rest.
[01:52:23.040 --> 01:52:28.320]   They follow every industry best practice for security models and design.
[01:52:28.320 --> 01:52:32.480]   They have things like access control mechanisms, bucket policies, ACLs.
[01:52:32.480 --> 01:52:36.000]   My favorite thing is your data is immutable.
[01:52:36.000 --> 01:52:40.240]   You can say, "I don't want this data to be erased or altered by anyone.
[01:52:40.240 --> 01:52:42.080]   Bad guys, malware, even me."
[01:52:42.080 --> 01:52:45.520]   Obviously, you could turn that off, but the malware can't.
[01:52:45.520 --> 01:52:47.200]   That is a great feature.
[01:52:47.200 --> 01:52:50.720]   That's why I say Wasabi is better than on-prem storage.
[01:52:50.720 --> 01:52:53.280]   Not just less expensive, but actually better.
[01:52:53.280 --> 01:52:57.120]   Hippo-compliant, FINRA-compliant, CJIS-compliant.
[01:52:57.120 --> 01:52:58.160]   It's a flat rate.
[01:52:58.160 --> 01:53:01.440]   There's no weird tiering system like on S3.
[01:53:01.440 --> 01:53:05.840]   It's just a basic flat 599 per terabyte per month.
[01:53:05.840 --> 01:53:09.440]   They've even got a better way to save.
[01:53:09.440 --> 01:53:11.440]   They call it reserved capacity storage.
[01:53:11.440 --> 01:53:15.040]   If you are in this situation where you just know, I know I need a certain amount every month,
[01:53:15.040 --> 01:53:16.720]   you can buy ahead.
[01:53:16.720 --> 01:53:18.640]   You can reserve capacity ahead.
[01:53:18.640 --> 01:53:21.840]   Purchase storage in one, three, or five-year increments.
[01:53:21.840 --> 01:53:25.920]   You'll save more for more storage and for a longer term.
[01:53:25.920 --> 01:53:29.360]   It's a great way to get that price even lower.
[01:53:29.360 --> 01:53:32.640]   Wasabi is not just highly secure disruptive technology
[01:53:32.640 --> 01:53:34.000]   that's turning the industry on its ear.
[01:53:34.000 --> 01:53:41.040]   It's storage that's 80% less expensive and six times faster than Amazon's S3.
[01:53:41.040 --> 01:53:43.360]   Oh, did I mention no charge for egress?
[01:53:43.360 --> 01:53:45.040]   It always gets me with S3.
[01:53:45.040 --> 01:53:48.240]   Oh, it's cheap to store it here until you want it back.
[01:53:48.240 --> 01:53:49.360]   Then they're going to charge you.
[01:53:49.360 --> 01:53:50.720]   No charge with a sabi.
[01:53:50.720 --> 01:53:52.320]   egress is free.
[01:53:52.320 --> 01:53:56.640]   So is API access and their API is 100% compatible with Amazon's S3.
[01:53:56.640 --> 01:53:58.640]   You're going to love wasabi.
[01:53:58.640 --> 01:54:00.320]   Just calculate the savings for yourself.
[01:54:00.320 --> 01:54:04.720]   Just go to wasabi.com, get a free trial of storage for a month.
[01:54:04.720 --> 01:54:06.240]   Use our offer code TWIT if you will.
[01:54:06.240 --> 01:54:07.360]   Let them know you saw it here.
[01:54:07.360 --> 01:54:08.080]   Join the movement.
[01:54:08.080 --> 01:54:10.240]   Migrate your data to the cloud with confidence.
[01:54:10.240 --> 01:54:11.680]   Go to wasabi.com.
[01:54:11.680 --> 01:54:14.080]   And again, don't forget the offer code TWIT.
[01:54:14.080 --> 01:54:17.840]   And thank you wasabi for supporting this week in tech.
[01:54:17.840 --> 01:54:22.560]   Thank you for supporting the show by using that TWIT offer code that'll let them know you saw it here.
[01:54:22.560 --> 01:54:26.080]   I love having both of you on Amy Webb as a futurist, quantitative.
[01:54:26.080 --> 01:54:28.880]   Futurist or qualitative, I can never remember.
[01:54:28.880 --> 01:54:31.440]   It's quantitative.
[01:54:31.440 --> 01:54:33.440]   I work with a lot of data and build models.
[01:54:33.440 --> 01:54:33.920]   Big data.
[01:54:33.920 --> 01:54:37.200]   Future Today Institute is her company.
[01:54:37.200 --> 01:54:38.000]   By the way, go there.
[01:54:38.000 --> 01:54:38.720]   It's so much fun.
[01:54:38.720 --> 01:54:41.200]   That front page, better than refrigerator magnets.
[01:54:42.880 --> 01:54:47.760]   Play with the various trends.
[01:54:47.760 --> 01:54:49.360]   See how it's changing the world.
[01:54:49.360 --> 01:54:53.200]   And of course, her book, which is well worth reading.
[01:54:53.200 --> 01:54:55.920]   I can't wait to read the new one on biotech.
[01:54:55.920 --> 01:54:56.000]   Thanks.
[01:54:56.000 --> 01:54:56.640]   Yeah.
[01:54:56.640 --> 01:54:59.760]   But the big nine is an excellent book.
[01:54:59.760 --> 01:55:01.680]   Go ahead.
[01:55:01.680 --> 01:55:03.040]   It's not about German refusal.
[01:55:03.040 --> 01:55:05.680]   Not that I know of.
[01:55:05.680 --> 01:55:07.600]   I just say one other thing.
[01:55:07.600 --> 01:55:07.760]   Yes.
[01:55:07.760 --> 01:55:11.360]   So we open sourced all of our research and our methods and tools.
[01:55:11.360 --> 01:55:13.040]   And we give everything away for free.
[01:55:13.040 --> 01:55:19.120]   So in all honesty, if any of what we have said tonight
[01:55:19.120 --> 01:55:21.200]   has sparked any interest whatsoever,
[01:55:21.200 --> 01:55:24.720]   the most important thing that you do from here on out is act.
[01:55:24.720 --> 01:55:27.600]   And the smallest act is to simply go chase down
[01:55:27.600 --> 01:55:29.360]   some of the things we talked about and learn.
[01:55:29.360 --> 01:55:29.360]   I agree.
[01:55:29.360 --> 01:55:29.920]   I agree.
[01:55:29.920 --> 01:55:30.320]   Learn.
[01:55:30.320 --> 01:55:32.560]   But you can also model next order impacts.
[01:55:32.560 --> 01:55:34.560]   And you can use the tools that we've got on the site to do that.
[01:55:34.560 --> 01:55:37.520]   Or show your husband all the places he's been.
[01:55:37.520 --> 01:55:38.400]   Then watch the film.
[01:55:40.080 --> 01:55:40.480]   Oh my.
[01:55:40.480 --> 01:55:43.680]   My his friends all listen to this little hacker group that's on Keybase
[01:55:43.680 --> 01:55:44.720]   that I keep talking about.
[01:55:44.720 --> 01:55:45.280]   Yeah.
[01:55:45.280 --> 01:55:46.560]   Where did they go, by the way?
[01:55:46.560 --> 01:55:47.520]   Are they still on Keybase?
[01:55:47.520 --> 01:55:49.280]   They're all around.
[01:55:49.280 --> 01:55:51.600]   And I'm sure it's like on Wednesday.
[01:55:51.600 --> 01:55:54.880]   When you find a replacement for Keybase, will you call me and tell me?
[01:55:54.880 --> 01:55:56.960]   I haven't found one yet.
[01:55:56.960 --> 01:55:57.680]   They got bought by Zoom.
[01:55:57.680 --> 01:55:59.280]   I don't want to use them anymore.
[01:55:59.280 --> 01:56:00.160]   I don't know what to do.
[01:56:00.160 --> 01:56:00.960]   I don't know what to do.
[01:56:00.960 --> 01:56:03.440]   Corey Doctorow is also here.
[01:56:03.440 --> 01:56:04.560]   Don't forget.
[01:56:04.560 --> 01:56:05.840]   It's too late for the Kickstarter.
[01:56:05.840 --> 01:56:06.880]   But don't worry.
[01:56:06.880 --> 01:56:08.880]   Because you can go to craphound.net.
[01:56:08.880 --> 01:56:09.920]   Is it craphound.net?
[01:56:09.920 --> 01:56:10.880]   Dot com.
[01:56:10.880 --> 01:56:11.760]   Dot com.
[01:56:11.760 --> 01:56:16.000]   Craphound.com and get yourself the audiobook or a hardcover.
[01:56:16.000 --> 01:56:19.760]   I bet you you even sign those hardcovers before you sign them out.
[01:56:19.760 --> 01:56:24.720]   So actually, if you go to attacksurface.com, which is domain I didn't own,
[01:56:24.720 --> 01:56:27.520]   but which a reader just gave to me out of the blue, which is very important to them.
[01:56:27.520 --> 01:56:31.920]   I'm doing a virtual tour, eight bookstores, eight nights,
[01:56:31.920 --> 01:56:34.480]   with different guests every night.
[01:56:34.480 --> 01:56:36.240]   And it comes with signed copies.
[01:56:36.240 --> 01:56:39.280]   In fact, if I had the lights on in my office and turned the camera on,
[01:56:39.280 --> 01:56:44.080]   you'd see the mountain of boxes of books that I am signing and fulfilling for the authors.
[01:56:44.080 --> 01:56:47.200]   So the four that remain, there's Bruce Sterling and Chris Brown
[01:56:47.200 --> 01:56:49.520]   talking about Cyberpunk and the Cyberpunk on Monday.
[01:56:49.520 --> 01:56:50.560]   Oh, I'd love that.
[01:56:50.560 --> 01:56:52.640]   And Ken Liu on Tuesday.
[01:56:52.640 --> 01:56:59.680]   Tochio Onyubuchi and Bethany Simorro talking about young adults and revolutions on Wednesday.
[01:56:59.680 --> 01:57:02.320]   And then we finish with Runa Sanvik from the tour project.
[01:57:02.320 --> 01:57:05.520]   He used to be the frontline defender for the New York Times Newsroom.
[01:57:05.520 --> 01:57:10.800]   And window Snyder, who's a security expert, she was a very senior
[01:57:10.800 --> 01:57:15.200]   a cartographer at Apple for many years, started a loft heavy industry, was in Cult of the Dead
[01:57:15.200 --> 01:57:15.760]   Count.
[01:57:15.760 --> 01:57:18.880]   And the two of them were talking about practical personal cybersecurity.
[01:57:18.880 --> 01:57:20.080]   Now, are you there too?
[01:57:20.080 --> 01:57:20.880]   Are you coming?
[01:57:20.880 --> 01:57:22.800]   So you had no, it's me and them.
[01:57:22.800 --> 01:57:24.640]   And they all come with signed copies of the book.
[01:57:24.640 --> 01:57:29.360]   If you want to go to more than one of them, and you don't want two copies of the books,
[01:57:29.360 --> 01:57:34.720]   if you go to craphound.com, you'll see a list of bookstores or rather of libraries and prisons
[01:57:34.720 --> 01:57:38.560]   and halfway houses and classrooms and universities that will take your copies.
[01:57:38.560 --> 01:57:38.880]   Nice.
[01:57:38.880 --> 01:57:43.840]   So you just put their address down instead of yours and all sign it to them and mail it to them instead.
[01:57:43.840 --> 01:57:44.480]   Wow.
[01:57:44.480 --> 01:57:47.680]   Well, if you like this conversation, these are going to be even better.
[01:57:47.680 --> 01:57:48.800]   This sounds really great.
[01:57:48.800 --> 01:57:49.280]   This is great.
[01:57:49.280 --> 01:57:51.680]   That's an amazing point of a few sounds.
[01:57:51.680 --> 01:57:52.400]   Thank you.
[01:57:52.400 --> 01:57:52.800]   Yeah.
[01:57:52.800 --> 01:57:53.600]   Really, really good.
[01:57:53.600 --> 01:57:55.040]   Thetechsurface.com.
[01:57:56.640 --> 01:58:03.920]   I had last week, you missed Malchia Cyril from Media Justice and Meredith Whitaker,
[01:58:03.920 --> 01:58:07.280]   who led the Google Uprising and founded the ANO Institute.
[01:58:07.280 --> 01:58:12.000]   I had Amber Benson and John Rogers, Ron Deiber, Daniva Galpren.
[01:58:12.000 --> 01:58:14.960]   The videos will eventually all be released on the podcast series as well.
[01:58:14.960 --> 01:58:16.400]   I was just going to ask you, where can we go to--
[01:58:16.400 --> 01:58:17.920]   Oh, this would be a great podcast.
[01:58:17.920 --> 01:58:19.120]   The dates not announced.
[01:58:19.120 --> 01:58:23.520]   Different bookstores have different policies about when they want the video embargo to.
[01:58:23.520 --> 01:58:25.280]   But we will make them all public.
[01:58:25.280 --> 01:58:26.000]   Nice.
[01:58:26.000 --> 01:58:26.880]   What a good idea.
[01:58:26.880 --> 01:58:28.800]   Why, how much better is that than a book tour?
[01:58:28.800 --> 01:58:30.240]   So much better.
[01:58:30.240 --> 01:58:31.200]   Well, I don't know.
[01:58:31.200 --> 01:58:36.320]   I miss being crammed in a fart tube and having given my morning government genital massage.
[01:58:36.320 --> 01:58:39.600]   Those were the days, were they, Corey?
[01:58:39.600 --> 01:58:40.480]   Oh, that good stuff.
[01:58:40.480 --> 01:58:41.040]   Yeah.
[01:58:41.040 --> 01:58:42.160]   Mini-bar cashews.
[01:58:42.160 --> 01:58:44.560]   Nothing like them.
[01:58:44.560 --> 01:58:45.840]   Let's take a break.
[01:58:45.840 --> 01:58:46.400]   We're not done.
[01:58:46.400 --> 01:58:47.280]   I still got some more time.
[01:58:47.280 --> 01:58:48.880]   I know you have to get to a birthday party.
[01:58:48.880 --> 01:58:50.560]   Is it your daughter's, Amy, or--
[01:58:50.560 --> 01:58:51.280]   No, it's mine.
[01:58:51.280 --> 01:58:52.160]   It's my birthday today.
[01:58:52.160 --> 01:58:52.800]   Hat won!
[01:58:52.800 --> 01:58:53.520]   Happy birthday!
[01:58:53.520 --> 01:58:54.560]   What?
[01:58:54.560 --> 01:58:55.200]   What?
[01:58:55.200 --> 01:58:56.240]   It's my birthday.
[01:58:56.240 --> 01:58:57.760]   It's my birthday.
[01:58:57.760 --> 01:58:59.040]   That's awesome.
[01:58:59.040 --> 01:59:00.160]   And I am here with you.
[01:59:00.160 --> 01:59:01.760]   That's awesome.
[01:59:01.760 --> 01:59:02.720]   Happy birthday.
[01:59:02.720 --> 01:59:05.440]   So are you going to have a party after the show?
[01:59:05.440 --> 01:59:06.480]   Happy birthday.
[01:59:06.480 --> 01:59:07.200]   What's that?
[01:59:07.200 --> 01:59:09.200]   Are you going to have a party after the show?
[01:59:09.200 --> 01:59:10.960]   Or is your husband not talking to you still?
[01:59:10.960 --> 01:59:11.600]   My daughter is a little--
[01:59:11.600 --> 01:59:12.240]   No, no, no.
[01:59:12.240 --> 01:59:13.280]   She's the world talking.
[01:59:13.280 --> 01:59:13.760]   I don't--
[01:59:13.760 --> 01:59:16.480]   We've got literally nowhere else to go, so we have to talk to each other.
[01:59:16.480 --> 01:59:17.120]   He has to talk to each other.
[01:59:17.120 --> 01:59:22.080]   My daughter made me a little something, so I'm going to go retrieve it before bedtime.
[01:59:22.080 --> 01:59:24.880]   Thank her for letting us have mommy for us so long.
[01:59:24.880 --> 01:59:26.240]   I really appreciate it.
[01:59:26.240 --> 01:59:26.800]   That's really--
[01:59:26.800 --> 01:59:28.400]   Happy birthday, Amy.
[01:59:28.400 --> 01:59:28.800]   Thank you.
[01:59:28.800 --> 01:59:29.360]   That's my wife.
[01:59:29.360 --> 01:59:30.640]   She calls me her cellmate.
[01:59:30.640 --> 01:59:33.600]   Yes, I feel that.
[01:59:33.600 --> 01:59:35.280]   I feel that.
[01:59:35.280 --> 01:59:35.760]   I'm talking--
[01:59:35.760 --> 01:59:38.560]   Now, listen, I am grateful to be--
[01:59:38.560 --> 01:59:40.480]   We're working.
[01:59:40.480 --> 01:59:41.840]   We're alive.
[01:59:41.840 --> 01:59:43.600]   We're with friends.
[01:59:43.600 --> 01:59:44.560]   We're eating.
[01:59:44.560 --> 01:59:47.760]   I mean, it could be a whole lot worse than it is for so many people.
[01:59:47.760 --> 01:59:50.000]   And that's such a tragedy.
[01:59:50.000 --> 01:59:51.760]   Actually, can I give a plug for my birthday?
[01:59:51.760 --> 01:59:57.040]   I am raising money for local food banks everywhere, so this is me telling you that
[01:59:57.040 --> 02:00:01.040]   food insecurity is a real thing all around the world, but also in the United States.
[02:00:01.040 --> 02:00:06.480]   Even in seemingly affluent communities, if you have $5, $10, whatever, find yourself,
[02:00:06.480 --> 02:00:08.320]   your local food bank, and please give them money.
[02:00:08.320 --> 02:00:09.120]   That's what I'm doing.
[02:00:09.120 --> 02:00:09.840]   Couldn't agree with you.
[02:00:09.840 --> 02:00:13.280]   You just reminded me, I haven't made my monthly donation to the Burbank Temporates
[02:00:13.280 --> 02:00:14.720]   Center, so I'm making that right now.
[02:00:14.720 --> 02:00:15.280]   There you go.
[02:00:15.280 --> 02:00:15.760]   Thank you.
[02:00:15.760 --> 02:00:16.320]   That's awesome.
[02:00:16.320 --> 02:00:17.360]   Wow, you guys are great.
[02:00:17.360 --> 02:00:19.920]   We're going to take a little break.
[02:00:20.480 --> 02:00:21.200]   Come back with more.
[02:00:21.200 --> 02:00:23.120]   We had, do we have a promo for this week?
[02:00:23.120 --> 02:00:27.280]   We had a good week on that to it, and John is going to push a button now that will show you
[02:00:27.280 --> 02:00:28.480]   exactly what you missed.
[02:00:28.480 --> 02:00:32.160]   Today, we're bringing 5G to iPhone.
[02:00:32.160 --> 02:00:32.720]   5G.
[02:00:32.720 --> 02:00:33.280]   5G.
[02:00:33.280 --> 02:00:33.840]   5G.
[02:00:33.840 --> 02:00:34.480]   5G.
[02:00:34.480 --> 02:00:35.200]   5G.
[02:00:35.200 --> 02:00:35.600]   5G.
[02:00:35.600 --> 02:00:36.000]   5G.
[02:00:36.000 --> 02:00:36.320]   5G.
[02:00:36.320 --> 02:00:39.040]   And 5G is going to unlock even more opportunities.
[02:00:39.040 --> 02:00:41.200]   What does 5G have to do with a camera?
[02:00:41.200 --> 02:00:41.680]   5G.
[02:00:41.680 --> 02:00:42.240]   5G.
[02:00:42.240 --> 02:00:42.720]   5G.
[02:00:42.720 --> 02:00:43.680]   Verizon 5G.
[02:00:43.680 --> 02:00:44.480]   Well, there you have it.
[02:00:44.480 --> 02:00:48.560]   The 2020 5G iPhone 5G event from Verizon IV.
[02:00:49.280 --> 02:00:52.800]   5G Apple Campus, Sun 5G.
[02:00:52.800 --> 02:00:56.960]   Previously on Twitch.
[02:00:56.960 --> 02:00:59.440]   All about Android.
[02:00:59.440 --> 02:01:04.240]   Got one back now in to talk the new Pixel 5, Sony's new Android phones,
[02:01:04.240 --> 02:01:06.080]   and the latest tick watch.
[02:01:06.080 --> 02:01:09.440]   I think from the Pixel 4a to here, it's definitely a step up.
[02:01:09.440 --> 02:01:12.800]   The 30 frame per second 4K looks phenomenal.
[02:01:12.800 --> 02:01:15.200]   iOS today.
[02:01:15.200 --> 02:01:15.520]   You know?
[02:01:17.040 --> 02:01:21.120]   It's getting a little, the water is raising in the studio here.
[02:01:21.120 --> 02:01:23.760]   So if you just don't mind, we can finish this up.
[02:01:23.760 --> 02:01:24.960]   Oh, God.
[02:01:24.960 --> 02:01:27.840]   Hands-on Android.
[02:01:27.840 --> 02:01:31.760]   And you thought that Samsung's devices couldn't get any more customized.
[02:01:31.760 --> 02:01:34.960]   Well, Samsung has an app called Goodlock that you want to check out.
[02:01:34.960 --> 02:01:37.520]   It'll take it even deeper, and I'm going to show you how to use it next.
[02:01:37.520 --> 02:01:39.680]   This week in Google.
[02:01:39.680 --> 02:01:42.800]   Google's giving data to police based on search keywords.
[02:01:42.800 --> 02:01:43.360]   Oh, crap.
[02:01:43.360 --> 02:01:45.680]   If you get caught up in something like this,
[02:01:45.680 --> 02:01:57.300]   But somebody like DuckDuckGo, that's their selling point, right? We de-identify. We don't know who you are. I hope that's true.
[02:01:57.300 --> 02:02:00.180]   Is that where you're doing your fertilizer bomb searches?
[02:02:00.180 --> 02:02:02.180]   [laughter]
[02:02:02.180 --> 02:02:04.680]   Twin. Unbelievable as always.
[02:02:04.680 --> 02:02:08.180]   5G.
[02:02:08.180 --> 02:02:08.680]   5G.
[02:02:08.680 --> 02:02:09.180]   5G.
[02:02:09.180 --> 02:02:09.680]   5G.
[02:02:09.680 --> 02:02:10.180]   5G.
[02:02:10.180 --> 02:02:10.680]   5G.
[02:02:10.680 --> 02:02:11.180]   5G.
[02:02:11.180 --> 02:02:11.680]   5G.
[02:02:11.680 --> 02:02:12.180]   5G.
[02:02:12.180 --> 02:02:12.680]   5G.
[02:02:12.680 --> 02:02:13.180]   5G.
[02:02:13.180 --> 02:02:13.680]   5G.
[02:02:13.680 --> 02:02:14.180]   5G.
[02:02:14.180 --> 02:02:14.680]   5G.
[02:02:14.680 --> 02:02:15.180]   5G.
[02:02:15.180 --> 02:02:15.680]   5G.
[02:02:15.680 --> 02:02:16.180]   5G.
[02:02:16.180 --> 02:02:16.680]   5G.
[02:02:16.680 --> 02:02:17.680]   5G.
[02:02:17.680 --> 02:02:18.180]   5G.
[02:02:18.180 --> 02:02:19.180]   5G.
[02:02:19.180 --> 02:02:20.180]   Birthday girl.
[02:02:20.180 --> 02:02:21.180]   Amy Webb.
[02:02:21.180 --> 02:02:23.180]   Yes, I didn't even...
[02:02:23.180 --> 02:02:24.180]   Shh.
[02:02:24.180 --> 02:02:25.180]   Shh.
[02:02:25.180 --> 02:02:26.180]   Talk about...
[02:02:26.180 --> 02:02:27.180]   [laughter]
[02:02:27.180 --> 02:02:29.180]   ...hiding your light under a bushel.
[02:02:29.180 --> 02:02:31.180]   Ice, whatever that means.
[02:02:31.180 --> 02:02:36.180]   Korea, are you Jewish? You keep saying Jewish things. I don't think Dr. Rose is a Jewish name. What is that?
[02:02:36.180 --> 02:02:37.180]   [unintelligible]
[02:02:37.180 --> 02:02:38.180]   Huh?
[02:02:38.180 --> 02:02:39.180]   Dr. Oggish.
[02:02:39.180 --> 02:02:43.180]   I was raised by APF Jews. I went to socialist theater school.
[02:02:43.180 --> 02:02:44.180]   Oh, there you go.
[02:02:44.180 --> 02:02:47.180]   I'm terrible Yiddish and I'm irreligious.
[02:02:47.180 --> 02:02:54.180]   I want that my wife who is not Jewish is insistent to the Passover Theater where I bought anarchist hagatas
[02:02:54.180 --> 02:02:59.180]   and replaced every instance of the word God with a vast uncaring universe.
[02:02:59.180 --> 02:03:00.180]   [laughter]
[02:03:00.180 --> 02:03:05.180]   You know what they really love is when you replace the word Israel with Palestine.
[02:03:05.180 --> 02:03:06.180]   It's always...
[02:03:06.180 --> 02:03:09.180]   I replaced it with just two state solutions.
[02:03:09.180 --> 02:03:10.180]   Yeah, two state solutions.
[02:03:10.180 --> 02:03:11.180]   Love, they love that.
[02:03:11.180 --> 02:03:12.180]   I know.
[02:03:12.180 --> 02:03:13.180]   Who's the "they"?
[02:03:13.180 --> 02:03:14.180]   I am Jewish.
[02:03:14.180 --> 02:03:15.180]   [laughter]
[02:03:15.180 --> 02:03:16.180]   Is that a...
[02:03:16.180 --> 02:03:18.180]   The chosen people, you know.
[02:03:18.180 --> 02:03:19.180]   [laughter]
[02:03:19.180 --> 02:03:23.180]   I bet you use a pretty liberal hagata, I would guess.
[02:03:23.180 --> 02:03:27.180]   We are gastronomically Jewish.
[02:03:27.180 --> 02:03:28.180]   I don't know.
[02:03:28.180 --> 02:03:29.180]   This whole thing.
[02:03:29.180 --> 02:03:32.180]   You like pastrami, but you know, you don't drink milk.
[02:03:32.180 --> 02:03:33.180]   I think you like pastrami.
[02:03:33.180 --> 02:03:34.180]   Yeah.
[02:03:34.180 --> 02:03:39.180]   I feel politically disenfranchised and religion's a tricky one.
[02:03:39.180 --> 02:03:40.180]   Yeah, I...
[02:03:40.180 --> 02:03:46.180]   But see, I don't know about the religious part, but culturally, I wish I were Jewish.
[02:03:46.180 --> 02:03:47.180]   It's the best.
[02:03:47.180 --> 02:03:50.180]   Did you hear Corey's joke about the Yom Kippur?
[02:03:50.180 --> 02:03:51.180]   Yeah.
[02:03:51.180 --> 02:03:52.180]   Well, there you go.
[02:03:52.180 --> 02:03:56.180]   That just said any religion that has its own jokes, that's great.
[02:03:56.180 --> 02:03:57.180]   I love that.
[02:03:57.180 --> 02:04:00.180]   There's like five million of us left, I think, on the planet.
[02:04:00.180 --> 02:04:02.180]   Is that really true?
[02:04:02.180 --> 02:04:03.180]   No.
[02:04:03.180 --> 02:04:06.180]   There's a shocking few number of people left.
[02:04:06.180 --> 02:04:07.180]   There were six million children.
[02:04:07.180 --> 02:04:09.180]   The Holocaust, because there's only five million left.
[02:04:09.180 --> 02:04:11.180]   I think there might be eleven, but there's...
[02:04:11.180 --> 02:04:13.180]   That's worse than the rhinoceruses.
[02:04:13.180 --> 02:04:16.180]   There's very, very few of us left.
[02:04:16.180 --> 02:04:19.180]   And I think increasingly, I don't...
[02:04:19.180 --> 02:04:21.180]   Is that because of assimilation?
[02:04:21.180 --> 02:04:24.180]   Is that assimilation and secularism?
[02:04:24.180 --> 02:04:27.180]   I think that's probably part of it.
[02:04:27.180 --> 02:04:29.180]   That's too bad.
[02:04:29.180 --> 02:04:30.180]   I have one child.
[02:04:30.180 --> 02:04:31.180]   I mean, it's not like we're...
[02:04:31.180 --> 02:04:33.180]   Yeah, you got to get going.
[02:04:33.180 --> 02:04:35.180]   You got to make the kids...
[02:04:35.180 --> 02:04:37.180]   And you named her after an era landmark.
[02:04:37.180 --> 02:04:38.180]   I did.
[02:04:38.180 --> 02:04:39.180]   It's so controversial.
[02:04:39.180 --> 02:04:43.180]   [laughter]
[02:04:43.180 --> 02:04:45.180]   People are saying, "What, the blue mosque?
[02:04:45.180 --> 02:04:46.180]   What do you call her?"
[02:04:46.180 --> 02:04:47.180]   Her name is Petra.
[02:04:47.180 --> 02:04:48.180]   Petra.
[02:04:48.180 --> 02:04:50.180]   One of the most amazing...
[02:04:50.180 --> 02:04:53.180]   Have you been to Petra, Corey?
[02:04:53.180 --> 02:04:54.180]   Did we lose Corey?
[02:04:54.180 --> 02:04:55.180]   His signal is...
[02:04:55.180 --> 02:04:56.180]   Her step in my problems.
[02:04:56.180 --> 02:04:57.180]   That's my mic.
[02:04:57.180 --> 02:04:58.180]   Now it's okay.
[02:04:58.180 --> 02:04:59.180]   It got bad for a bit.
[02:04:59.180 --> 02:05:00.180]   I thought it was a bit of a picture.
[02:05:00.180 --> 02:05:01.180]   Is that better?
[02:05:01.180 --> 02:05:02.180]   Yeah.
[02:05:02.180 --> 02:05:03.180]   Yeah.
[02:05:03.180 --> 02:05:04.180]   I mean, my kid is Jewish-ish.
[02:05:04.180 --> 02:05:06.180]   She asked for Russia Shonoff, and I said,
[02:05:06.180 --> 02:05:10.180]   "If you spend at least two hours this morning researching what it is,
[02:05:10.180 --> 02:05:12.180]   and what it's about, then you can have it."
[02:05:12.180 --> 02:05:13.180]   And she did.
[02:05:13.180 --> 02:05:14.180]   And I gave her the day off school.
[02:05:14.180 --> 02:05:15.180]   That's nice.
[02:05:15.180 --> 02:05:16.180]   Yeah.
[02:05:16.180 --> 02:05:17.180]   I think it depends...
[02:05:17.180 --> 02:05:19.180]   It all depends on how you count.
[02:05:19.180 --> 02:05:21.180]   I think one or both of you may know,
[02:05:21.180 --> 02:05:24.180]   Rael Dornfest, who was in a row for many years,
[02:05:24.180 --> 02:05:25.180]   and then a Twitter.
[02:05:25.180 --> 02:05:27.180]   And when I first met him,
[02:05:27.180 --> 02:05:29.180]   I was at an O'Reilly event,
[02:05:29.180 --> 02:05:30.180]   and Tim O'Reilly was there,
[02:05:30.180 --> 02:05:32.180]   and Rael and I told each other Jewish jokes
[02:05:32.180 --> 02:05:35.180]   for about 20 minutes solid.
[02:05:35.180 --> 02:05:39.180]   And Tim was both aghast and amazed,
[02:05:39.180 --> 02:05:40.180]   and said, "What is this?"
[02:05:40.180 --> 02:05:42.180]   And Rael, totally straight-faced, said,
[02:05:42.180 --> 02:05:45.180]   "Well, it's the J2J protocol when two Jews meet them
[02:05:45.180 --> 02:05:48.180]   after exchange all their Jewish jokes to synchronize."
[02:05:48.180 --> 02:05:50.180]   That's awesome.
[02:05:50.180 --> 02:05:52.180]   Oh my God, that's amazing.
[02:05:52.180 --> 02:05:54.180]   Oh, I love it.
[02:05:54.180 --> 02:05:56.180]   I love it.
[02:05:56.180 --> 02:05:58.180]   That's hilarious.
[02:05:58.180 --> 02:06:00.180]   The best thing, we just had Sakot.
[02:06:00.180 --> 02:06:03.180]   And do you build the little...
[02:06:03.180 --> 02:06:04.180]   You built a...
[02:06:04.180 --> 02:06:05.180]   I didn't.
[02:06:05.180 --> 02:06:06.180]   I'm a little confused.
[02:06:06.180 --> 02:06:07.180]   I have friends who did,
[02:06:07.180 --> 02:06:09.180]   but I think it's the cutest thing ever.
[02:06:09.180 --> 02:06:10.180]   I wanted to do that.
[02:06:10.180 --> 02:06:11.180]   You build a little hut.
[02:06:11.180 --> 02:06:12.180]   If you...
[02:06:12.180 --> 02:06:14.180]   Yeah, so, okay.
[02:06:14.180 --> 02:06:17.180]   Like, we don't have time for context,
[02:06:17.180 --> 02:06:19.180]   and if we start talking about some of our...
[02:06:19.180 --> 02:06:20.180]   I know, it's gonna get weird.
[02:06:20.180 --> 02:06:22.180]   I think people are gonna get...
[02:06:22.180 --> 02:06:23.180]   Yeah.
[02:06:23.180 --> 02:06:24.180]   I think there's...
[02:06:24.180 --> 02:06:27.180]   It's not gonna help me and myself in our cause.
[02:06:27.180 --> 02:06:29.180]   One of the things, though,
[02:06:29.180 --> 02:06:34.180]   that the Jewish faith really embraces is an intellectualism.
[02:06:34.180 --> 02:06:38.180]   And one of the things I was thinking when Corey was talking is...
[02:06:38.180 --> 02:06:41.180]   Actually, both of you is what we need is...
[02:06:41.180 --> 02:06:43.180]   Something that we used to have in the United States,
[02:06:43.180 --> 02:06:44.180]   they still have in Europe,
[02:06:44.180 --> 02:06:47.180]   and Europe is this notion of a public intellectual.
[02:06:47.180 --> 02:06:49.180]   Public intellectuals who bring up these issues,
[02:06:49.180 --> 02:06:52.180]   who talk at this more lofty level,
[02:06:52.180 --> 02:06:57.180]   and really kind of can frame the political conversation.
[02:06:57.180 --> 02:07:03.180]   And right now, what we have is just lots of yelling and conflict,
[02:07:03.180 --> 02:07:05.180]   and nobody rising above the fray.
[02:07:05.180 --> 02:07:10.180]   I feel like that's something that is kind of missing from our discourse in this.
[02:07:10.180 --> 02:07:14.180]   intellectualism has a little bit of a challenging past with...
[02:07:14.180 --> 02:07:16.180]   We've always hated this in America.
[02:07:16.180 --> 02:07:17.180]   Jewish people.
[02:07:17.180 --> 02:07:19.180]   Let me just say this.
[02:07:19.180 --> 02:07:23.180]   The thing that...
[02:07:23.180 --> 02:07:26.180]   So my family came to the United States.
[02:07:26.180 --> 02:07:30.180]   They were escaping like everybody,
[02:07:30.180 --> 02:07:35.180]   and I grew up around people that had numbers on their arms from concentration camps.
[02:07:35.180 --> 02:07:37.180]   The thing that I learned,
[02:07:37.180 --> 02:07:41.180]   that every... or a lot of people that I know learn from a very early age,
[02:07:41.180 --> 02:07:45.180]   is that when they come for your...
[02:07:45.180 --> 02:07:47.180]   They can come and take your things,
[02:07:47.180 --> 02:07:50.180]   but they can never take away your education.
[02:07:50.180 --> 02:07:55.180]   And so education matters more than almost anything else.
[02:07:55.180 --> 02:07:57.180]   Education and hard work.
[02:07:57.180 --> 02:07:58.180]   And if you are...
[02:07:58.180 --> 02:08:02.180]   If you have learned things and you are willing to work harder
[02:08:02.180 --> 02:08:04.180]   than everybody else around you,
[02:08:04.180 --> 02:08:08.180]   but then no matter what happens, you'll be okay.
[02:08:08.180 --> 02:08:11.180]   So I think that...
[02:08:11.180 --> 02:08:14.180]   I think those are good values.
[02:08:14.180 --> 02:08:15.180]   We certainly...
[02:08:15.180 --> 02:08:17.180]   I live by those values.
[02:08:17.180 --> 02:08:19.180]   Education is more important than anything else,
[02:08:19.180 --> 02:08:22.180]   so we make sure our daughter knows stuff.
[02:08:22.180 --> 02:08:27.180]   But there is a little bit of a line with the sort of intellectual...
[02:08:27.180 --> 02:08:29.180]   It's a...
[02:08:29.180 --> 02:08:33.180]   The Jewish intellectual trope
[02:08:33.180 --> 02:08:37.180]   is real prevalent in propaganda all around the world.
[02:08:37.180 --> 02:08:38.180]   So it's a little...
[02:08:38.180 --> 02:08:40.180]   Oh, it's a bad thing?
[02:08:40.180 --> 02:08:42.180]   It's a double-edged sword.
[02:08:42.180 --> 02:08:44.180]   I will tell you...
[02:08:44.180 --> 02:08:48.180]   So the forward is a Jewish news organization,
[02:08:48.180 --> 02:08:50.180]   and I got voted...
[02:08:50.180 --> 02:08:55.180]   They make a list every year of the hottest Jewish intellectuals,
[02:08:55.180 --> 02:08:57.180]   and I was on that list.
[02:08:57.180 --> 02:08:59.180]   And...
[02:08:59.180 --> 02:09:01.180]   Which I kind of thought was funny.
[02:09:01.180 --> 02:09:03.180]   And then I realized, like,
[02:09:03.180 --> 02:09:05.180]   this could potentially be a problem later on,
[02:09:05.180 --> 02:09:07.180]   and I worked kind of hard to bury it.
[02:09:07.180 --> 02:09:08.180]   Oh, dear.
[02:09:08.180 --> 02:09:09.180]   Oh, dear.
[02:09:09.180 --> 02:09:12.180]   Were you worried about any semitism?
[02:09:12.180 --> 02:09:15.180]   Yeah, my college roommate was pretty horrifically anti-Semitic
[02:09:15.180 --> 02:09:19.180]   and got expelled for doing some pretty bad stuff.
[02:09:19.180 --> 02:09:23.180]   You know, listen, it's the year 2020.
[02:09:23.180 --> 02:09:29.180]   We have a president in office who is unabashedly racist
[02:09:29.180 --> 02:09:31.180]   and anti-Semitic.
[02:09:31.180 --> 02:09:33.180]   Anti-a-lot.
[02:09:33.180 --> 02:09:38.180]   It's plausible that he won that election because of those views.
[02:09:38.180 --> 02:09:44.180]   And there is no question that there's a rise in...
[02:09:44.180 --> 02:09:49.180]   You know, there's a rise in public displays
[02:09:49.180 --> 02:09:53.180]   of anti-Semitism and racism and anti-ism.
[02:09:53.180 --> 02:09:56.180]   And I guess we're kind of circling back to where we began,
[02:09:56.180 --> 02:09:59.180]   which was Corey's, I think,
[02:09:59.180 --> 02:10:04.180]   admonishment of just awful dystopian futures
[02:10:04.180 --> 02:10:07.180]   and dystopian fiction because it reinforces some of those beliefs.
[02:10:07.180 --> 02:10:09.180]   And I guess I kind of agree with that.
[02:10:09.180 --> 02:10:11.180]   I'm changing my mind.
[02:10:11.180 --> 02:10:12.180]   Yeah.
[02:10:12.180 --> 02:10:13.180]   Yeah.
[02:10:13.180 --> 02:10:14.180]   I guess, you know, when I think of...
[02:10:14.180 --> 02:10:15.180]   I'm sorry, Amy, I didn't mean to interrupt.
[02:10:15.180 --> 02:10:16.180]   I thought you were done.
[02:10:16.180 --> 02:10:17.180]   Sorry.
[02:10:17.180 --> 02:10:18.180]   No, please.
[02:10:18.180 --> 02:10:19.180]   Oh.
[02:10:19.180 --> 02:10:20.180]   So...
[02:10:20.180 --> 02:10:21.180]   It's...
[02:10:21.180 --> 02:10:22.180]   I apologize.
[02:10:22.180 --> 02:10:23.180]   It's the Skype thing.
[02:10:23.180 --> 02:10:24.180]   Go ahead, Corey.
[02:10:24.180 --> 02:10:29.180]   I just wanted to say, you know, my father came to Canada on a DP boat.
[02:10:29.180 --> 02:10:32.180]   And when he got to Canada, he was racialized.
[02:10:32.180 --> 02:10:38.180]   He was spoken of as someone who was of a race that was not white.
[02:10:38.180 --> 02:10:39.180]   Right.
[02:10:39.180 --> 02:10:40.180]   And he was...
[02:10:40.180 --> 02:10:41.180]   He was from Russia.
[02:10:41.180 --> 02:10:42.180]   He was a Russian.
[02:10:42.180 --> 02:10:44.180]   His father was Polish and Belarusian.
[02:10:44.180 --> 02:10:45.180]   His mom was Russian.
[02:10:45.180 --> 02:10:46.180]   Okay.
[02:10:46.180 --> 02:10:48.180]   And he was born in Azerbaijan.
[02:10:48.180 --> 02:10:50.180]   And he...
[02:10:50.180 --> 02:10:53.180]   By the time I was born,
[02:10:53.180 --> 02:10:59.180]   we had become white, right, in terms of the way that we appear in the discourse.
[02:10:59.180 --> 02:11:08.180]   And I look around at family members and friends of mine who, like Amy experienced firsthand,
[02:11:08.180 --> 02:11:16.180]   the horrors of violent industrial-scale racism and who, upon attaining whiteness, decided
[02:11:16.180 --> 02:11:22.180]   that their allegiance lay with the system that divides us into white and not white and not
[02:11:22.180 --> 02:11:24.180]   with opposing racism everywhere.
[02:11:24.180 --> 02:11:29.180]   And I feel like what we're learning now in 2020 is what we should have intuited, which
[02:11:29.180 --> 02:11:33.180]   is the last people who get pulled into the whiteness boat are the first ones pushed over
[02:11:33.180 --> 02:11:41.180]   the gunnels when the seas get stormy and that are allegiance as people who are descended
[02:11:41.180 --> 02:11:48.180]   from and part of a culture that was shaped by this horrific violent racism is to be aligned
[02:11:48.180 --> 02:11:55.980]   with anti-racism, especially posing anti-black racism and the xenophobia that is directed
[02:11:55.980 --> 02:11:59.020]   at people from Central and South America.
[02:11:59.020 --> 02:12:06.620]   And I'm proud to the extent that I see Jews remembering that in their history and making
[02:12:06.620 --> 02:12:09.020]   that their lesson.
[02:12:09.020 --> 02:12:13.380]   And I just made to the extent that it isn't as universal as I would hope it would be.
[02:12:13.380 --> 02:12:14.380]   Yeah.
[02:12:14.380 --> 02:12:19.780]   Well, I don't want, I think it would be a mistake for us to deprecate intellectualism.
[02:12:19.780 --> 02:12:21.300]   I think we have done that.
[02:12:21.300 --> 02:12:27.300]   Going back to Adley Stevenson, who was declared an egghead and not elected president, that's,
[02:12:27.300 --> 02:12:29.260]   we have a long history of that in the United States.
[02:12:29.260 --> 02:12:33.540]   But I think maybe we need smart people more than we did ever before and we need people
[02:12:33.540 --> 02:12:39.300]   who are willing to think big thoughts and be public about that and talk about that.
[02:12:39.300 --> 02:12:42.740]   Both of you are my models for this.
[02:12:42.740 --> 02:12:45.820]   You're both so eloquent and so smart.
[02:12:45.820 --> 02:12:49.300]   And your, and Amy, the forward was right.
[02:12:49.300 --> 02:12:51.300]   You're willing to step out and talk about it.
[02:12:51.300 --> 02:12:54.460]   And I thank you for doing that, especially on your birthday.
[02:12:54.460 --> 02:13:00.100]   Amy Webb, Future Today Institute, the big nine, can't wait for the new book, can't wait
[02:13:00.100 --> 02:13:02.860]   till you come back and be on the show with us.
[02:13:02.860 --> 02:13:04.620]   Did you hear there's a new iPhone?
[02:13:04.620 --> 02:13:05.620]   Did you hear that?
[02:13:05.620 --> 02:13:07.420]   Did you hear anything about that?
[02:13:07.420 --> 02:13:08.420]   Because I heard that happen.
[02:13:08.420 --> 02:13:10.300]   I heard the 5G is bad.
[02:13:10.300 --> 02:13:11.300]   It's bad.
[02:13:11.300 --> 02:13:12.300]   It's 5G.
[02:13:12.300 --> 02:13:13.300]   It's 5G.
[02:13:13.300 --> 02:13:14.300]   I've been told that.
[02:13:14.300 --> 02:13:15.300]   Thank you, Amy, for being here.
[02:13:15.300 --> 02:13:16.300]   Have a happy, happy birthday.
[02:13:16.300 --> 02:13:17.300]   I really appreciate it.
[02:13:17.300 --> 02:13:18.300]   Thanks.
[02:13:18.300 --> 02:13:19.300]   Thank you.
[02:13:19.300 --> 02:13:21.860]   Corey Doctorow or Dr. Rovitch or what?
[02:13:21.860 --> 02:13:22.860]   I liked that.
[02:13:22.860 --> 02:13:23.860]   Dr. Rovitch.
[02:13:23.860 --> 02:13:24.860]   I liked it.
[02:13:24.860 --> 02:13:25.860]   Yeah.
[02:13:25.860 --> 02:13:27.540]   It's always a pleasure.
[02:13:27.540 --> 02:13:33.260]   Corey's new website, pluralistic.net is they have a, he has a great newsletter which
[02:13:33.260 --> 02:13:35.220]   you should subscribe to.
[02:13:35.220 --> 02:13:41.340]   He is, if nothing, if not eclectic in his interests and tastes, and it's all reflected
[02:13:41.340 --> 02:13:43.380]   at pluralistic.net.
[02:13:43.380 --> 02:13:46.020]   It's just, it's the way a blog used to be.
[02:13:46.020 --> 02:13:50.980]   I have an appreciation for a new Apple product as well since we're going out, which is that
[02:13:50.980 --> 02:13:55.220]   I was very delighted to see that they finally introduced a product for dad jokes, the eye
[02:13:55.220 --> 02:13:56.220]   roll.
[02:13:56.220 --> 02:13:57.220]   It's only.
[02:13:57.220 --> 02:14:03.420]   I stole that joke from DJBC of the Beastels.
[02:14:03.420 --> 02:14:05.020]   It's a good joke.
[02:14:05.020 --> 02:14:07.060]   The eye roll is here.
[02:14:07.060 --> 02:14:10.700]   Actually, they did introduce a cent in candle that talks to you.
[02:14:10.700 --> 02:14:11.700]   That's pretty cool.
[02:14:11.700 --> 02:14:14.420]   It'll even follow you around.
[02:14:14.420 --> 02:14:21.580]   We which sometime will get down to the privacy marketing that Apple does and how accurate
[02:14:21.580 --> 02:14:22.580]   that is.
[02:14:22.580 --> 02:14:29.220]   I have to say some of the smoking gun emails that came from the house, a judiciary investigation,
[02:14:29.220 --> 02:14:34.540]   the antitrust subcommittee were pretty damning of Apple.
[02:14:34.540 --> 02:14:39.820]   I think as much as we'd like to think of them as the shining fruit on the hill, it's
[02:14:39.820 --> 02:14:42.340]   not quite the case there.
[02:14:42.340 --> 02:14:46.620]   I think there are a lot of Uighurs who don't have VPNs anymore who are pretty skeptical
[02:14:46.620 --> 02:14:49.420]   of Apple's privacy claims.
[02:14:49.420 --> 02:14:50.420]   Exactly.
[02:14:50.420 --> 02:14:51.420]   That's for another day.
[02:14:51.420 --> 02:14:52.420]   We are done here.
[02:14:52.420 --> 02:14:54.900]   Plurioistic.net, craphound.com.
[02:14:54.900 --> 02:15:02.620]   Don't forget, by the way, to get your autographed copy and go to those virtual readings.
[02:15:02.620 --> 02:15:04.820]   That's really cool, really exciting.
[02:15:04.820 --> 02:15:06.740]   What's the website for that again?
[02:15:06.740 --> 02:15:07.740]   AttackSurface.com.
[02:15:07.740 --> 02:15:11.060]   AttackSurface.com.
[02:15:11.060 --> 02:15:14.060]   There must be a fee for those because you're going to get a book, right, for the readings.
[02:15:14.060 --> 02:15:16.700]   Yeah, it's the cost of the book plus a couple bucks for shipping.
[02:15:16.700 --> 02:15:17.740]   That's the ticket price.
[02:15:17.740 --> 02:15:18.740]   That's very fair.
[02:15:18.740 --> 02:15:21.860]   That's virtual and it's all online and there are a few more still to come.
[02:15:21.860 --> 02:15:22.860]   We haven't missed them all.
[02:15:22.860 --> 02:15:26.140]   There's one tomorrow on the 20th, the 21st and the 22nd.
[02:15:26.140 --> 02:15:28.940]   Boy, these sound like great conversations.
[02:15:28.940 --> 02:15:29.940]   Thank you, Corey.
[02:15:29.940 --> 02:15:30.940]   Happy birthday.
[02:15:30.940 --> 02:15:31.940]   Thank you, Amy.
[02:15:31.940 --> 02:15:32.940]   Thank you.
[02:15:32.940 --> 02:15:35.940]   And hey, the new book is amazing.
[02:15:35.940 --> 02:15:36.940]   So congrats.
[02:15:36.940 --> 02:15:38.220]   Yeah, thank you very much.
[02:15:38.220 --> 02:15:39.220]   I appreciate it.
[02:15:39.220 --> 02:15:40.660]   I'm really looking forward to reading your next one.
[02:15:40.660 --> 02:15:44.020]   Synth bio is definitely subject near and dear to my heart.
[02:15:44.020 --> 02:15:45.020]   Really interesting.
[02:15:45.020 --> 02:15:46.020]   I'm really looking forward to finishing it.
[02:15:46.020 --> 02:15:47.020]   Yeah.
[02:15:47.020 --> 02:15:48.020]   On my deadlines.
[02:15:48.020 --> 02:15:52.780]   Good luck with that.
[02:15:52.780 --> 02:15:54.140]   Those days long gone for me.
[02:15:54.140 --> 02:15:55.140]   Thank God.
[02:15:55.140 --> 02:15:56.140]   That was a nightmare.
[02:15:56.140 --> 02:16:01.660]   We do Twitter every Sunday about 230 Pacific 530 Eastern 2130 UTC.
[02:16:01.660 --> 02:16:06.740]   If you want to watch or listen live, we do have streams at twitter.tv/live.
[02:16:06.740 --> 02:16:10.180]   But honestly, there's no reason for you to disrupt your Sunday evening.
[02:16:10.180 --> 02:16:18.020]   You can easily just get the on demand versions, which are available on our website, twitter.tv.
[02:16:18.020 --> 02:16:19.020]   There's a YouTube channel.
[02:16:19.020 --> 02:16:21.140]   You could watch or listen there.
[02:16:21.140 --> 02:16:23.660]   Best thing to do is get a podcast application and subscribe.
[02:16:23.660 --> 02:16:29.260]   That way you get the every episode, the minute it's available of a Sunday evening.
[02:16:29.260 --> 02:16:32.540]   Thank you all so much for being here.
[02:16:32.540 --> 02:16:33.540]   My brain is swelling.
[02:16:33.540 --> 02:16:38.540]   I'm going to have to go get a cold compress and think about what we've learned today.
[02:16:38.540 --> 02:16:39.540]   Another Twitch.
[02:16:39.540 --> 02:16:40.540]   Synth bio.
[02:16:40.540 --> 02:16:40.540]   [music]
[02:16:40.540 --> 02:16:50.220]   [music]
[02:16:50.220 --> 02:17:13.820]   [ Silence ]

