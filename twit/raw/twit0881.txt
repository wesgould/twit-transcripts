;FFMETADATA1
title=A Cowboy Hat For Your Dog
artist=Leo Laporte, Brianna Wu, Iain Thomson, Florence Ion
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-06-27
track=881
language=English
genre=Podcast
comment=Roe vs. Wade, Amazon robots, Twitter Notes, Nothing Phone (1)
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:01.760]   It's time for Twit this week in Tech.
[00:00:01.760 --> 00:00:04.040]   Ian Thompson's here from the Register, Brianna Wu,
[00:00:04.040 --> 00:00:07.040]   from Rebellion Back, Florence Ion from Gizmodo.
[00:00:07.040 --> 00:00:09.360]   We'll talk about the latest Supreme Court decision
[00:00:09.360 --> 00:00:13.120]   and what that means for big tech and privacy.
[00:00:13.120 --> 00:00:17.120]   Florence Ion says the nothing phone lives up to its name.
[00:00:17.120 --> 00:00:19.800]   We'll find out what she means by that.
[00:00:19.800 --> 00:00:23.480]   And Google, coming back to Spain.
[00:00:23.480 --> 00:00:24.240]   What a thought.
[00:00:24.240 --> 00:00:26.480]   It's all coming up next on Twit.
[00:00:29.240 --> 00:00:31.040]   Podcasts you love.
[00:00:31.040 --> 00:00:33.520]   From people you trust.
[00:00:33.520 --> 00:00:34.840]   This is Twit.
[00:00:34.840 --> 00:00:48.440]   This is Twit this week in Tech, episode 881, recorded Sunday,
[00:00:48.440 --> 00:00:51.320]   June 26, 2022.
[00:00:51.320 --> 00:00:53.920]   A cowboy hat for your dog.
[00:00:53.920 --> 00:00:57.280]   This episode of This Week in Tech is brought to you by Audible.
[00:00:57.280 --> 00:01:01.560]   Audible lets you enjoy all of your audio entertainment in one app.
[00:01:01.560 --> 00:01:04.480]   Let Audible help you discover new ways to laugh,
[00:01:04.480 --> 00:01:06.720]   be inspired, or be entertained.
[00:01:06.720 --> 00:01:09.360]   New members can try it free for 30 days.
[00:01:09.360 --> 00:01:15.960]   Visit audible.com/twit or text Twit to 500-500.
[00:01:15.960 --> 00:01:18.120]   And by Wealthfront.
[00:01:18.120 --> 00:01:21.360]   To start building your wealth and get your first $5,000
[00:01:21.360 --> 00:01:27.160]   managed free for life, go to wealthfront.com/twit.
[00:01:27.160 --> 00:01:30.280]   And by ExpressVPN.
[00:01:30.280 --> 00:01:33.120]   Make sure your online activity and data is protected
[00:01:33.120 --> 00:01:35.640]   with the best VPN money can buy.
[00:01:35.640 --> 00:01:38.800]   Visit ExpressVPN.com/twit right now
[00:01:38.800 --> 00:01:42.360]   and get three extra months free through our special link.
[00:01:42.360 --> 00:01:45.120]   And by userway.org.
[00:01:45.120 --> 00:01:48.040]   Userway is the world's number one accessibility solution
[00:01:48.040 --> 00:01:51.640]   and it's committed to enabling the fundamental human right
[00:01:51.640 --> 00:01:54.080]   of digital accessibility for everyone.
[00:01:54.080 --> 00:01:56.280]   When you're ready to make your site compliant,
[00:01:56.280 --> 00:01:59.560]   deciding which solution to use is an easy choice to make.
[00:01:59.560 --> 00:02:05.040]   Go to userway.org/twit for 30% off userway's AI-powered
[00:02:05.040 --> 00:02:06.640]   accessibility solution.
[00:02:06.640 --> 00:02:09.640]   [MUSIC PLAYING]
[00:02:09.640 --> 00:02:14.240]   It's time for Twit.
[00:02:14.240 --> 00:02:17.640]   This week in Tech, the show we cover the latest Tech News
[00:02:17.640 --> 00:02:19.520]   with people I really like.
[00:02:19.520 --> 00:02:22.480]   That's really the only criterion for being on Twit, I think.
[00:02:22.480 --> 00:02:24.440]   We used to have this, you know, oh, they've
[00:02:24.440 --> 00:02:25.280]   got to be Tech journalists.
[00:02:25.280 --> 00:02:28.200]   Well, in this case, two out of three ain't bad.
[00:02:28.200 --> 00:02:29.480]   They're all good friends.
[00:02:29.480 --> 00:02:30.680]   That's the most important thing.
[00:02:30.680 --> 00:02:33.520]   In fact, it's wonderful that Florence Ion on,
[00:02:33.520 --> 00:02:35.600]   a longtime host on all about Android.
[00:02:35.600 --> 00:02:37.560]   She's now at Gizmodo.
[00:02:37.560 --> 00:02:41.920]   And it looks like in the nursery, which is good,
[00:02:41.920 --> 00:02:43.160]   because if you're needed.
[00:02:43.160 --> 00:02:45.400]   No, this is the flow lab.
[00:02:45.400 --> 00:02:46.400]   Oh.
[00:02:46.400 --> 00:02:47.400]   [LAUGHTER]
[00:02:47.400 --> 00:02:47.920]   The flow lab.
[00:02:47.920 --> 00:02:50.400]   Already I put my foot in my mouth and the show is just
[00:02:50.400 --> 00:02:51.400]   keep going.
[00:02:51.400 --> 00:02:52.880]   [LAUGHTER]
[00:02:52.880 --> 00:02:53.880]   Hello, welcome to the flow lab.
[00:02:53.880 --> 00:02:54.800]   This is my candy room.
[00:02:54.800 --> 00:02:55.300]   It is.
[00:02:55.300 --> 00:02:56.600]   It's a candy room.
[00:02:56.600 --> 00:02:58.000]   It's Sailor Moon.
[00:02:58.000 --> 00:02:59.000]   It's Sailor Moon.
[00:02:59.000 --> 00:03:00.000]   It's Sailor Moon.
[00:03:00.000 --> 00:03:01.000]   It's a critical, staff.
[00:03:01.000 --> 00:03:03.680]   An original 1990s poster.
[00:03:03.680 --> 00:03:05.120]   I have my Microsoft bag over here.
[00:03:05.120 --> 00:03:05.800]   I see that.
[00:03:05.800 --> 00:03:05.800]   Yeah.
[00:03:05.800 --> 00:03:07.800]   From a show that I went to over here.
[00:03:07.800 --> 00:03:09.720]   I have my desk mats.
[00:03:09.720 --> 00:03:10.280]   Very nice.
[00:03:10.280 --> 00:03:13.760]   This is where I take a nap sometimes on my Pikachu pillow.
[00:03:13.760 --> 00:03:15.600]   She's in the flow lab.
[00:03:15.600 --> 00:03:19.720]   I tell you, I have rare Sailor Moon art books from the '90s.
[00:03:19.720 --> 00:03:21.200]   Like the originals that came out.
[00:03:21.200 --> 00:03:22.560]   That's how I learned to draw.
[00:03:22.560 --> 00:03:24.640]   So you come to Boston.
[00:03:24.640 --> 00:03:25.640]   You can party.
[00:03:25.640 --> 00:03:26.640]   I'll show you all my rare stuff.
[00:03:26.640 --> 00:03:27.200]   No, absolutely.
[00:03:27.200 --> 00:03:28.120]   That's Brianna.
[00:03:28.120 --> 00:03:30.000]   Whoa, of course, as if you didn't know,
[00:03:30.000 --> 00:03:33.560]   Executive Director of Rebellion PAC, former candidate
[00:03:33.560 --> 00:03:37.000]   for Congress, and a good friend, of course.
[00:03:37.000 --> 00:03:39.000]   Thank you for joining us, Brianna.
[00:03:39.000 --> 00:03:41.120]   Somebody stepped on our ethernet cable.
[00:03:41.120 --> 00:03:41.560]   I know.
[00:03:41.560 --> 00:03:42.640]   I delayed the whole show.
[00:03:42.640 --> 00:03:44.000]   Sorry.
[00:03:44.000 --> 00:03:45.200]   No, not at all.
[00:03:45.200 --> 00:03:48.080]   Also with us, the US editor of the register.com.
[00:03:48.080 --> 00:03:50.280]   Good friend, Ian Thompson.
[00:03:50.280 --> 00:03:52.560]   Wow, this is like old home week for all three of you.
[00:03:52.560 --> 00:03:54.480]   I'm really thrilled to have all three of you on.
[00:03:54.480 --> 00:03:57.040]   On a week that has been a little bit tough,
[00:03:57.040 --> 00:04:00.360]   a little bit difficult for those of us in blue states.
[00:04:00.360 --> 00:04:02.360]   You know?
[00:04:02.360 --> 00:04:05.600]   And of course, we don't want to be particularly
[00:04:05.600 --> 00:04:07.080]   political on this show, but it does.
[00:04:07.080 --> 00:04:09.000]   It's very much part of the tech industry
[00:04:09.000 --> 00:04:12.520]   and a number of interesting ways.
[00:04:12.520 --> 00:04:15.360]   A number of tech companies, including Apple and Google,
[00:04:15.360 --> 00:04:18.640]   have told their employees.
[00:04:18.640 --> 00:04:22.400]   The we, A, if you want to relocate, you can.
[00:04:22.400 --> 00:04:27.800]   And B, we will pay for your trip to a state
[00:04:27.800 --> 00:04:29.680]   where you can get an abortion, states
[00:04:29.680 --> 00:04:32.400]   where the abortions are illegal.
[00:04:32.400 --> 00:04:35.800]   I think a lot of the tech community is saying that.
[00:04:35.800 --> 00:04:38.240]   Is that illegal, Brianna?
[00:04:38.240 --> 00:04:39.960]   Is it OK to say that?
[00:04:39.960 --> 00:04:45.040]   It seems like that might be committing a crime.
[00:04:45.040 --> 00:04:46.440]   I certainly think it's--
[00:04:46.440 --> 00:04:50.880]   Excuse me, it's opening them up to liability, certainly.
[00:04:50.880 --> 00:04:53.480]   Because many of these states have laws, in fact,
[00:04:53.480 --> 00:04:55.760]   saying if you help someone with an abortion,
[00:04:55.760 --> 00:04:57.720]   you're also liable.
[00:04:57.720 --> 00:04:59.360]   So I think the difference is--
[00:04:59.360 --> 00:05:00.720]   Texas, chiefly.
[00:05:00.720 --> 00:05:01.400]   Yeah.
[00:05:01.400 --> 00:05:02.760]   Yep, correct.
[00:05:02.760 --> 00:05:05.200]   So I think the difference is many of these--
[00:05:05.200 --> 00:05:07.720]   many of these large companies have more resources
[00:05:07.720 --> 00:05:11.440]   to fight this in court than say maybe the average person in Texas
[00:05:11.440 --> 00:05:14.960]   living under the poverty line might have.
[00:05:14.960 --> 00:05:22.640]   So I do just have to say, while I appreciate this gesture for it,
[00:05:22.640 --> 00:05:27.280]   it can't help but feel a little bit empty to me.
[00:05:27.280 --> 00:05:30.520]   I cannot imagine a woman in the tech industry
[00:05:30.520 --> 00:05:35.040]   wanting to go to her male boss and talking about her abortion
[00:05:35.040 --> 00:05:39.240]   and asking for reimbursement for a trip out of town and time
[00:05:39.240 --> 00:05:39.760]   off.
[00:05:39.760 --> 00:05:41.360]   It's appreciated.
[00:05:41.360 --> 00:05:43.880]   But it's entirely insufficient, especially
[00:05:43.880 --> 00:05:46.480]   when you consider many of these tech companies.
[00:05:46.480 --> 00:05:49.480]   Their packs literally donate to the politicians
[00:05:49.480 --> 00:05:52.160]   that now we're talking about a federal base.
[00:05:52.160 --> 00:05:56.600]   It's kind of the least you could do category.
[00:05:56.600 --> 00:06:00.040]   Meta's done even almost a little bit less.
[00:06:00.040 --> 00:06:01.520]   Well, they've banned this, haven't they?
[00:06:01.520 --> 00:06:03.520]   They've banned discussion about the issue.
[00:06:03.520 --> 00:06:04.040]   Yeah.
[00:06:04.040 --> 00:06:06.680]   So first they said, we intend to offer--
[00:06:06.680 --> 00:06:08.680]   and by the way, listen carefully, the verbiage--
[00:06:08.680 --> 00:06:12.200]   travel expense reimbursements to the extent permitted
[00:06:12.200 --> 00:06:15.280]   by law for employees who will need them to access out
[00:06:15.280 --> 00:06:17.920]   of state health care and reproductive services.
[00:06:17.920 --> 00:06:19.160]   We're in the process of assessing
[00:06:19.160 --> 00:06:22.560]   how best to do that given the legal complexities involved.
[00:06:22.560 --> 00:06:24.360]   So that's really only a half promise.
[00:06:24.360 --> 00:06:25.520]   And yes, you're right.
[00:06:25.520 --> 00:06:29.240]   They abandoned any discussion of the Roe v. Wade Supreme Court
[00:06:29.240 --> 00:06:31.440]   decision, which came down this week
[00:06:31.440 --> 00:06:33.080]   from their internal discussion.
[00:06:33.080 --> 00:06:38.080]   Although I kind of understand it's such a heated topic,
[00:06:38.080 --> 00:06:39.520]   they're afraid of--
[00:06:39.520 --> 00:06:42.640]   I don't know, a fight's breaking out on their slack.
[00:06:42.640 --> 00:06:43.480]   I don't know.
[00:06:43.480 --> 00:06:46.600]   Well, you see, I mean, we talked about this in the office
[00:06:46.600 --> 00:06:49.680]   because I don't often comment on Twitter
[00:06:49.680 --> 00:06:51.600]   on American politics because I'm a guest here
[00:06:51.600 --> 00:06:53.440]   and I feel it's kind of rude.
[00:06:53.440 --> 00:06:55.120]   Although soon to be a citizen.
[00:06:55.120 --> 00:06:58.240]   This just, well, maybe.
[00:06:58.240 --> 00:07:00.960]   Maybe soon to be going back to New York.
[00:07:00.960 --> 00:07:03.360]   Maybe change your mind.
[00:07:03.360 --> 00:07:07.200]   Honestly, that a lot of people who do have exit routes
[00:07:07.200 --> 00:07:08.760]   are taking them at the moment.
[00:07:08.760 --> 00:07:10.240]   We're seeing--
[00:07:10.240 --> 00:07:13.000]   I'm already seeing people moving out,
[00:07:13.000 --> 00:07:15.960]   or at least getting into the first stages of moving out.
[00:07:15.960 --> 00:07:18.280]   Look, I don't feel this is my position to comment.
[00:07:18.280 --> 00:07:21.120]   But I talked to my editor and said, look,
[00:07:21.120 --> 00:07:22.960]   I'm going to be talking about this.
[00:07:22.960 --> 00:07:23.720]   Is that a problem?
[00:07:23.720 --> 00:07:25.360]   And he's like, no, we're not the Washington Post.
[00:07:25.360 --> 00:07:27.640]   Go for it.
[00:07:27.640 --> 00:07:31.880]   But it is shameful how some media outlets have said to journalists,
[00:07:31.880 --> 00:07:34.240]   you can't talk about this online,
[00:07:34.240 --> 00:07:36.520]   except in the context of your paper.
[00:07:36.520 --> 00:07:38.320]   Otherwise, you'll be seen as biased.
[00:07:38.320 --> 00:07:44.760]   When the hell did personal opinion become just not allowed?
[00:07:44.760 --> 00:07:48.360]   So yeah, I applaud tech companies for doing this.
[00:07:48.360 --> 00:07:49.960]   There are some, as has been pointed out,
[00:07:49.960 --> 00:07:52.360]   some massive holes in their strategy,
[00:07:52.360 --> 00:07:55.200]   particularly as they're funding the very political candidates
[00:07:55.200 --> 00:07:56.440]   that are pushing this forward.
[00:07:56.440 --> 00:08:00.480]   But there is more that can be done by the tech industry.
[00:08:00.480 --> 00:08:03.400]   And it's about time it stepped up to the plate and did it.
[00:08:03.400 --> 00:08:04.680]   Yeah.
[00:08:04.680 --> 00:08:05.200]   Yeah.
[00:08:05.200 --> 00:08:10.040]   It's also a very exclusive and privileged to be able to--
[00:08:10.040 --> 00:08:12.480]   if you have this job to work at a tech company that
[00:08:12.480 --> 00:08:15.480]   will offer you this sort of thing,
[00:08:15.480 --> 00:08:19.040]   a lot of people don't have an exit strategy at all.
[00:08:19.040 --> 00:08:24.360]   A lot of people have to stay where they are and buckle in.
[00:08:24.360 --> 00:08:28.040]   I used to have this mindset of, well, if things just get bad,
[00:08:28.040 --> 00:08:29.120]   I guess I'll just leave.
[00:08:29.120 --> 00:08:32.040]   But then I really, really--
[00:08:32.040 --> 00:08:33.320]   It's super privileged.
[00:08:33.320 --> 00:08:35.280]   It's super privileged, right?
[00:08:35.280 --> 00:08:40.520]   And the people, frankly, the women who will have the hardest
[00:08:40.520 --> 00:08:44.400]   time with this are people who can't relocate, right?
[00:08:44.400 --> 00:08:45.360]   It's a privileged--
[00:08:45.360 --> 00:08:46.200]   It's a privileged--
[00:08:46.200 --> 00:08:47.040]   It's a privileged--
[00:08:47.040 --> 00:08:47.880]   It's broke.
[00:08:47.880 --> 00:08:48.880]   Yeah.
[00:08:48.880 --> 00:08:49.320]   Yeah.
[00:08:49.320 --> 00:08:55.480]   Those are the people that this is going to harm most of all.
[00:08:55.480 --> 00:08:57.720]   Yeah, we can say from a point of privilege,
[00:08:57.720 --> 00:08:59.320]   well, I'm just leaving.
[00:08:59.320 --> 00:09:01.760]   But that's not an option for most of us.
[00:09:01.760 --> 00:09:03.000]   Well, I mean, it's not even that.
[00:09:03.000 --> 00:09:07.480]   I mean, we did a piece on looking at sort of pregnancy
[00:09:07.480 --> 00:09:09.560]   and menstrual tracking apps.
[00:09:09.560 --> 00:09:12.880]   And somebody just said to say, well, look, what you need to do
[00:09:12.880 --> 00:09:16.360]   is get a burner phone and just use these apps up here.
[00:09:16.360 --> 00:09:17.920]   And if you're running out of staff,
[00:09:17.920 --> 00:09:18.840]   it's like, really?
[00:09:18.840 --> 00:09:20.680]   You're living paycheck to paycheck.
[00:09:20.680 --> 00:09:24.520]   You're having to go 500, 600 miles from an abortion.
[00:09:24.520 --> 00:09:28.040]   And now you've got to get a burner phone as well.
[00:09:28.040 --> 00:09:29.080]   It's just not going to happen.
[00:09:29.080 --> 00:09:30.440]   You shouldn't need an app.
[00:09:30.440 --> 00:09:33.320]   It's sick to get reproductive health care.
[00:09:33.320 --> 00:09:34.720]   Excuse me.
[00:09:34.720 --> 00:09:35.640]   I'm sorry.
[00:09:35.640 --> 00:09:38.240]   Google has told its employees, you
[00:09:38.240 --> 00:09:42.920]   can relocate without justification following the decision.
[00:09:42.920 --> 00:09:43.880]   You can move--
[00:09:43.880 --> 00:09:46.560]   They'll still sell the data to identify these women
[00:09:46.560 --> 00:09:48.720]   to other people, however.
[00:09:48.720 --> 00:09:50.840]   It's like, yeah, we'll look after our own.
[00:09:50.840 --> 00:09:53.160]   But everyone else, yeah, we've got a business to run.
[00:09:53.160 --> 00:09:54.360]   Thanks very much.
[00:09:54.360 --> 00:09:56.040]   Well, that's the second tech angle on this.
[00:09:56.040 --> 00:09:59.000]   And actually, a very, very big tech angle on this
[00:09:59.000 --> 00:10:01.360]   is the issue of privacy.
[00:10:01.360 --> 00:10:07.360]   And I've said this last few shows.
[00:10:07.360 --> 00:10:09.640]   I've been kind of blasé about privacy.
[00:10:09.640 --> 00:10:11.680]   Look, if they want to do targeted ads,
[00:10:11.680 --> 00:10:14.360]   I understand the ad business wants targeting.
[00:10:14.360 --> 00:10:16.000]   I'm not crazy about it.
[00:10:16.000 --> 00:10:18.040]   But I don't see much of the harm.
[00:10:18.040 --> 00:10:20.640]   But when you are in a living in a state where the government
[00:10:20.640 --> 00:10:24.680]   is actively searching your social media posts
[00:10:24.680 --> 00:10:27.200]   or your search posts, trying to find out
[00:10:27.200 --> 00:10:31.920]   if you're doing something they don't like, that's problematic.
[00:10:31.920 --> 00:10:33.800]   And that's exactly, it seems, what's
[00:10:33.800 --> 00:10:37.160]   going to happen in many states.
[00:10:37.160 --> 00:10:37.680]   Yeah.
[00:10:37.680 --> 00:10:40.760]   I mean, the difference between now and the last time abortion
[00:10:40.760 --> 00:10:44.960]   was illegal is today we live in a surveillance state, right?
[00:10:44.960 --> 00:10:47.360]   Where all this information is very quantifiable
[00:10:47.360 --> 00:10:53.000]   and prosecutors can far more easily build a case against you.
[00:10:53.000 --> 00:10:55.840]   It's fairly infuriating.
[00:10:55.840 --> 00:10:57.160]   Yeah.
[00:10:57.160 --> 00:11:00.800]   And it's worth mentioning also that not every--
[00:11:00.800 --> 00:11:02.480]   this is something I actually learned yesterday,
[00:11:02.480 --> 00:11:04.120]   because I was trying to be helpful,
[00:11:04.120 --> 00:11:05.480]   because I had friends who were like, well,
[00:11:05.480 --> 00:11:06.840]   what are we supposed to do now?
[00:11:06.840 --> 00:11:08.080]   We live in California.
[00:11:08.080 --> 00:11:09.240]   We have this privilege.
[00:11:09.240 --> 00:11:10.600]   We should do something.
[00:11:10.600 --> 00:11:12.200]   And I was like, oh, we should probably
[00:11:12.200 --> 00:11:15.920]   get people to move to encrypted messaging.
[00:11:15.920 --> 00:11:19.640]   Then I had a friend of mine who is an engineer,
[00:11:19.640 --> 00:11:20.520]   and he reached out to me.
[00:11:20.520 --> 00:11:23.680]   And he's like, by the way, look up what
[00:11:23.680 --> 00:11:26.480]   happened to Signal late last year
[00:11:26.480 --> 00:11:30.240]   and how their data got used by law enforcement
[00:11:30.240 --> 00:11:32.120]   to sort of implicate people.
[00:11:32.120 --> 00:11:33.160]   How did that happen?
[00:11:33.160 --> 00:11:33.680]   Sharing.
[00:11:33.680 --> 00:11:36.200]   Signal says we don't know anything.
[00:11:36.200 --> 00:11:38.960]   Was because people set their phones up badly.
[00:11:38.960 --> 00:11:39.720]   Oh, yeah.
[00:11:39.720 --> 00:11:40.520]   You know, OK.
[00:11:40.520 --> 00:11:41.520]   So they were deleting messages.
[00:11:41.520 --> 00:11:44.960]   If you're backing up messages to iCloud,
[00:11:44.960 --> 00:11:46.800]   they're no longer encrypted.
[00:11:46.800 --> 00:11:50.240]   But also, signal by default will keep you messages
[00:11:50.240 --> 00:11:51.200]   as long as you have them.
[00:11:51.200 --> 00:11:55.960]   And the case that you mentioned was just like people
[00:11:55.960 --> 00:11:58.160]   that just kept these messages on their phones.
[00:11:58.160 --> 00:12:00.600]   And yeah, their phones.
[00:12:00.600 --> 00:12:01.640]   So it's unencrypted.
[00:12:01.640 --> 00:12:03.680]   Unfortunately, it's on.
[00:12:03.680 --> 00:12:08.440]   So floor is the issue that it's unencrypted on the phone.
[00:12:08.440 --> 00:12:09.600]   I mean, also--
[00:12:09.600 --> 00:12:13.840]   If I have co-opted your phone, everything's in the clear.
[00:12:13.840 --> 00:12:18.600]   It's in transit that it's safe, but on your phone.
[00:12:18.600 --> 00:12:20.080]   Well, Signal-- so Signal still
[00:12:20.080 --> 00:12:22.000]   had to give up the timestamps.
[00:12:22.000 --> 00:12:22.880]   They did.
[00:12:22.880 --> 00:12:23.360]   OK.
[00:12:23.360 --> 00:12:24.360]   Yeah.
[00:12:24.360 --> 00:12:25.360]   Oh, OK.
[00:12:25.360 --> 00:12:26.280]   Not exactly what had happened.
[00:12:26.280 --> 00:12:28.280]   But they don't know metadata.
[00:12:28.280 --> 00:12:29.240]   They don't-- or do they.
[00:12:29.240 --> 00:12:31.880]   Do they know who you sent messages to?
[00:12:31.880 --> 00:12:32.600]   I guess they do.
[00:12:32.600 --> 00:12:35.440]   So PCMag was where I read this yesterday,
[00:12:35.440 --> 00:12:37.720]   because I just was like Googling it real quick.
[00:12:37.720 --> 00:12:39.040]   And I had completely forgot.
[00:12:39.040 --> 00:12:41.680]   They had talked about this is what law enforcement asks
[00:12:41.680 --> 00:12:45.960]   from us when they give us a search warrant.
[00:12:45.960 --> 00:12:48.400]   And they were saying one of the things is like timestamps
[00:12:48.400 --> 00:12:51.440]   could be used to implicate you and just, you know--
[00:12:51.440 --> 00:12:52.880]   It's like a frickin' serial case.
[00:12:52.880 --> 00:12:56.120]   It's one more bit of information
[00:12:56.120 --> 00:12:59.720]   that law enforcement can use and then try--
[00:12:59.720 --> 00:13:04.720]   And a lawyer could use to come after you.
[00:13:04.720 --> 00:13:09.120]   I do just want to say here, and look, the techno angle,
[00:13:09.120 --> 00:13:12.560]   like the Set Tech show, obviously it's important to hit.
[00:13:12.560 --> 00:13:17.320]   I literally used to live right next to the abortion clinic
[00:13:17.320 --> 00:13:22.440]   that this Mississippi case is based on, right?
[00:13:22.440 --> 00:13:25.560]   You know, this is-- I didn't grow up in Silicon Valley
[00:13:25.560 --> 00:13:27.000]   or Massachusetts.
[00:13:27.000 --> 00:13:29.680]   I grew up in Hattiesburg, Mississippi.
[00:13:29.680 --> 00:13:32.760]   And I really-- I say this with respect,
[00:13:32.760 --> 00:13:35.040]   because I know engineers and tech people,
[00:13:35.040 --> 00:13:36.560]   we see this problem and we're like,
[00:13:36.560 --> 00:13:38.680]   how can we address this?
[00:13:38.680 --> 00:13:41.000]   We naturally want to use the skills that we have
[00:13:41.000 --> 00:13:42.880]   and the things that we're thinking about.
[00:13:42.880 --> 00:13:45.680]   But I have to share with you, like,
[00:13:45.680 --> 00:13:48.200]   think about the people in Mississippi
[00:13:48.200 --> 00:13:51.400]   that this court case is about, right?
[00:13:51.400 --> 00:13:54.480]   They live in the Delta, which has the poorest literacy
[00:13:54.480 --> 00:13:56.640]   in the entire United States.
[00:13:56.640 --> 00:13:58.840]   Many of them are never going to leave the town
[00:13:58.840 --> 00:14:00.640]   that they were born in.
[00:14:00.640 --> 00:14:05.080]   I mean, access to electricity is an issue in the Delta,
[00:14:05.080 --> 00:14:07.080]   let alone school.
[00:14:07.080 --> 00:14:10.080]   And then you're talking about, like, your encrypted message
[00:14:10.080 --> 00:14:10.560]   app.
[00:14:10.560 --> 00:14:16.480]   It just-- it feels so respectfully toned up, in my opinion.
[00:14:16.480 --> 00:14:21.960]   I mean, what we need is, you know, this is going to be solved
[00:14:21.960 --> 00:14:23.520]   through an electoral process.
[00:14:23.520 --> 00:14:26.920]   And frankly, it's going to be solved through politicians
[00:14:26.920 --> 00:14:30.840]   that are more willing to use their power than, unfortunately,
[00:14:30.840 --> 00:14:32.800]   the people that we have right now.
[00:14:32.800 --> 00:14:34.560]   They're executive orders that we could do.
[00:14:34.560 --> 00:14:37.040]   They're things at a state level we can do.
[00:14:37.040 --> 00:14:40.680]   There are things that we can do with having abortion clinics
[00:14:40.680 --> 00:14:41.920]   built on federal lands.
[00:14:41.920 --> 00:14:45.360]   Like, there are any number of actions we can be taking.
[00:14:45.360 --> 00:14:48.960]   Encrypted, like, signal apps are not the solution here.
[00:14:48.960 --> 00:14:50.520]   In fact, I would argue it gives you
[00:14:50.520 --> 00:14:53.520]   the appearance of having power while not really
[00:14:53.520 --> 00:14:55.840]   being addressing the central problem.
[00:14:55.840 --> 00:14:56.440]   Respectful.
[00:14:56.440 --> 00:14:57.720]   That's a good point, Brianna.
[00:14:57.720 --> 00:14:59.200]   Yeah, that's a good point.
[00:14:59.200 --> 00:14:59.880]   Thank you for that.
[00:14:59.880 --> 00:15:00.840]   And you're right, Brianna.
[00:15:00.840 --> 00:15:05.520]   It really is like-- for me, it was an emotional reaction
[00:15:05.520 --> 00:15:08.600]   because I'm living in a state where it didn't just
[00:15:08.600 --> 00:15:09.600]   turn immediately.
[00:15:09.600 --> 00:15:13.560]   So I was trying to look for something tangible
[00:15:13.560 --> 00:15:16.920]   that I could contribute, but at the core of it,
[00:15:16.920 --> 00:15:19.480]   it's not really addressing the people who
[00:15:19.480 --> 00:15:22.400]   are really going to need this help.
[00:15:22.400 --> 00:15:23.400]   So given that--
[00:15:23.400 --> 00:15:24.480]   And I accept that.
[00:15:24.480 --> 00:15:26.920]   I do still think this is an opportunity for us
[00:15:26.920 --> 00:15:30.960]   to look at privacy and how much big tech knows about us
[00:15:30.960 --> 00:15:36.360]   and now say, as contrary to what my position has been for years,
[00:15:36.360 --> 00:15:38.320]   this is actually a big issue.
[00:15:41.760 --> 00:15:44.040]   Has our privacy disappeared?
[00:15:44.040 --> 00:15:46.920]   And what's criminalized next?
[00:15:46.920 --> 00:15:53.240]   And is government willing to weaponize big tech?
[00:15:53.240 --> 00:15:55.680]   And I think they are.
[00:15:55.680 --> 00:15:59.040]   And what is big tech going to do about it?
[00:15:59.040 --> 00:16:00.560]   Well, I think individuals are going
[00:16:00.560 --> 00:16:02.200]   to start weaponizing big tech.
[00:16:02.200 --> 00:16:05.280]   As we've seen in the Texas and Mississippi laws, I believe,
[00:16:05.280 --> 00:16:08.800]   they're now offering a $10,000 bounty on someone who can--
[00:16:08.800 --> 00:16:11.920]   if you can sue someone who is out in an abortion.
[00:16:11.920 --> 00:16:15.160]   And location data firms are happy to sell you
[00:16:15.160 --> 00:16:17.360]   the information that a particular person has
[00:16:17.360 --> 00:16:19.120]   been to a particular area.
[00:16:19.120 --> 00:16:22.400]   I think there is going to be a very nasty trend
[00:16:22.400 --> 00:16:23.800]   in these kind of things.
[00:16:23.800 --> 00:16:28.200]   And OK, Scott Minnily said a long time ago, privacy is dead.
[00:16:28.200 --> 00:16:30.560]   When the Edward Snowden revelations came out,
[00:16:30.560 --> 00:16:33.840]   he tweeted out, I didn't realize quite how right I was.
[00:16:33.840 --> 00:16:36.720]   Now we're about to see the real world applications of that.
[00:16:36.720 --> 00:16:38.760]   It's not going to be pretty.
[00:16:38.760 --> 00:16:41.440]   We're also sitting in a situation--
[00:16:41.440 --> 00:16:42.560]   happened in the UK.
[00:16:42.560 --> 00:16:44.840]   It's happened in Australia, where the government
[00:16:44.840 --> 00:16:50.440]   is really weighing whether encryption should be even allowed,
[00:16:50.440 --> 00:16:53.040]   whether privacy should be even allowed.
[00:16:53.040 --> 00:16:58.240]   And this is becoming a scary issue here.
[00:16:58.240 --> 00:17:00.560]   I mean, think about it in a wider sense.
[00:17:00.560 --> 00:17:03.800]   Clarence Thomas and his dissent very clearly indicated
[00:17:03.800 --> 00:17:09.160]   that gay marriage is up for debate.
[00:17:09.160 --> 00:17:10.600]   There are a lot of these civil rights
[00:17:10.600 --> 00:17:14.560]   going beyond reproductive access that are very much in the air.
[00:17:14.560 --> 00:17:18.920]   Can you imagine a future in this country where
[00:17:18.920 --> 00:17:24.080]   people that, like everyone that's queer now came out online,
[00:17:24.080 --> 00:17:26.000]   or there's an electronic trail?
[00:17:26.000 --> 00:17:29.040]   It's just-- I mean, for my generation,
[00:17:29.040 --> 00:17:31.800]   I went to the library to read books on it.
[00:17:31.800 --> 00:17:34.080]   That's just completely different today.
[00:17:34.080 --> 00:17:38.600]   It's so quantified that if there is this indication
[00:17:38.600 --> 00:17:43.720]   that the federal protection, which is based on the same law,
[00:17:43.720 --> 00:17:45.920]   which is based on Roe, that allows
[00:17:45.920 --> 00:17:48.160]   private sex acts between individuals,
[00:17:48.160 --> 00:17:50.880]   that's out of-- it's a sexual privacy issue.
[00:17:50.880 --> 00:17:54.400]   There are cases that can be brought now with that.
[00:17:54.400 --> 00:17:57.600]   Gay marriage, or forcefully outing people, transgender people,
[00:17:57.600 --> 00:18:03.360]   or increasingly in the sites of the religious right.
[00:18:03.360 --> 00:18:07.560]   So I think that this talk about privacy
[00:18:07.560 --> 00:18:10.600]   is going to become increasingly important
[00:18:10.600 --> 00:18:14.240]   as these tools are weaponized against these individuals
[00:18:14.240 --> 00:18:16.040]   if this Supreme Court stays on this court.
[00:18:16.040 --> 00:18:20.560]   You remember last year a Catholic priest was outed
[00:18:20.560 --> 00:18:25.640]   because a public Asia Catholic sub-stack publication
[00:18:25.640 --> 00:18:29.920]   got his movements from Grindr.
[00:18:29.920 --> 00:18:34.240]   Grindr historically had terrible privacy protections.
[00:18:34.240 --> 00:18:36.640]   It's widely used by gay men to hook up.
[00:18:36.640 --> 00:18:42.720]   What kind of chill does this send over the use of technology
[00:18:42.720 --> 00:18:43.400]   in general?
[00:18:43.400 --> 00:18:44.560]   I would submit technology.
[00:18:44.560 --> 00:18:45.600]   I've got to pay attention to this,
[00:18:45.600 --> 00:18:49.520]   because there's a real worry that people will stop using it,
[00:18:49.520 --> 00:18:52.560]   or they will start fuzzing their information,
[00:18:52.560 --> 00:18:56.280]   or this is going to rebound badly
[00:18:56.280 --> 00:19:00.240]   a big tech that doesn't take privacy seriously, isn't it?
[00:19:00.240 --> 00:19:01.720]   Or do they not care?
[00:19:01.720 --> 00:19:03.880]   I honestly think they don't care.
[00:19:03.880 --> 00:19:07.560]   They think at this point you're so bought into the system,
[00:19:07.560 --> 00:19:09.640]   there is no way for you to get out of it.
[00:19:09.640 --> 00:19:12.200]   We'll do some nice PR moves.
[00:19:12.200 --> 00:19:14.080]   We'll make it seem like we care,
[00:19:14.080 --> 00:19:15.840]   but we're still going to sell that data.
[00:19:15.840 --> 00:19:17.760]   We're still going to invade your privacy
[00:19:17.760 --> 00:19:19.640]   because you are the product.
[00:19:19.640 --> 00:19:21.520]   And that's the way it's going now.
[00:19:21.520 --> 00:19:26.520]   I think it seems to have been a shift in the last five years,
[00:19:26.520 --> 00:19:29.720]   maybe, particularly notices with Google,
[00:19:29.720 --> 00:19:33.360]   where they've moved from innovation to just pure rent seeking.
[00:19:33.360 --> 00:19:37.040]   And it's now just like, yeah, you bought into the ecosystem,
[00:19:37.040 --> 00:19:38.840]   we've got 10 years of your data,
[00:19:38.840 --> 00:19:41.360]   now we're going to take our money. Thank you.
[00:19:41.360 --> 00:19:43.200]   You have an article in the Register,
[00:19:43.200 --> 00:19:44.760]   Jessica Lyons, Hardcastle.
[00:19:44.760 --> 00:19:49.280]   She did a, she requested Amazon,
[00:19:49.280 --> 00:19:51.720]   Microsoft, Google, Meta and Twitter and said,
[00:19:51.720 --> 00:19:53.720]   "On Friday morning, what will your company do
[00:19:53.720 --> 00:19:56.120]   to ensure the data you collect isn't going to be used
[00:19:56.120 --> 00:19:59.560]   to build a case against women seeking abortions,
[00:19:59.560 --> 00:20:02.560]   and people organizations providing abortion support,
[00:20:02.560 --> 00:20:05.160]   a whole day went by, none of them responded."
[00:20:05.160 --> 00:20:09.480]   Not a sausage, absolutely nothing, and it was shameful.
[00:20:09.480 --> 00:20:11.440]   They haven't figured it out yet, that's why.
[00:20:11.440 --> 00:20:13.680]   They're PRs, they're writing up the press release.
[00:20:13.680 --> 00:20:15.760]   And I can guarantee you that PR will come up
[00:20:15.760 --> 00:20:18.960]   with a press release that Meeley moths it to the degree
[00:20:18.960 --> 00:20:19.960]   that it gives them quite a little bit.
[00:20:19.960 --> 00:20:22.160]   It's not like they didn't have notice on this though.
[00:20:22.160 --> 00:20:23.640]   You know, this thing was leaked.
[00:20:23.640 --> 00:20:24.880]   We've been talking about it for a week.
[00:20:24.880 --> 00:20:26.800]   We knew it was coming. Yeah.
[00:20:26.800 --> 00:20:31.800]   So they should have had their ducks in a row and be...
[00:20:31.800 --> 00:20:33.960]   Like writing in a bickery.
[00:20:33.960 --> 00:20:37.080]   Well, yeah, unfortunately you'll write on that one.
[00:20:37.080 --> 00:20:39.160]   No, we told people a month ago,
[00:20:39.160 --> 00:20:42.160]   after the leak of the Allioto decision,
[00:20:42.160 --> 00:20:45.320]   stop using any period tracker apps that you're using,
[00:20:46.520 --> 00:20:49.720]   because that will announce to the world
[00:20:49.720 --> 00:20:52.440]   in law enforcement that you're pregnant.
[00:20:52.440 --> 00:20:54.160]   Yeah, I mean, although, having said,
[00:20:54.160 --> 00:20:57.280]   we talk to a lot of the period tracker people,
[00:20:57.280 --> 00:20:59.040]   and they're kind of absolutely,
[00:20:59.040 --> 00:21:01.320]   we will not share this data with anyone.
[00:21:01.320 --> 00:21:04.400]   Yeah, because that's the end of their business, if that...
[00:21:04.400 --> 00:21:06.960]   Yeah, but the bigger risk is that this data
[00:21:06.960 --> 00:21:10.280]   is being sold by any everyone else.
[00:21:10.280 --> 00:21:12.880]   And also, I mean, there was a marvelous idea
[00:21:12.880 --> 00:21:15.600]   that maybe men should download these period apps
[00:21:15.600 --> 00:21:18.160]   and start really screwing with their data sets,
[00:21:18.160 --> 00:21:19.440]   which is kind of cute,
[00:21:19.440 --> 00:21:21.200]   but I don't think it's going to help that much.
[00:21:21.200 --> 00:21:25.800]   No, no, no, because if someone's pursuing a case against you,
[00:21:25.800 --> 00:21:28.000]   they're going to go get your...
[00:21:28.000 --> 00:21:30.720]   They're going to subpoena that app directly.
[00:21:30.720 --> 00:21:34.800]   A corrupted data set is not going to help that in any way.
[00:21:34.800 --> 00:21:38.160]   Yeah. I also just do want to add to this conversation.
[00:21:38.160 --> 00:21:39.960]   And yeah, this is uncomfortable.
[00:21:39.960 --> 00:21:41.480]   Yeah, I say this respectfully,
[00:21:41.480 --> 00:21:44.000]   but this is an important show today.
[00:21:45.080 --> 00:21:49.280]   I have been raising the alarm on structural sexism
[00:21:49.280 --> 00:21:52.520]   in the technology industry for a long time.
[00:21:52.520 --> 00:21:56.440]   Gamer kid, of course, was a huge issue for you.
[00:21:56.440 --> 00:21:57.440]   100%.
[00:21:57.440 --> 00:22:01.560]   So if you're talking about the same men
[00:22:01.560 --> 00:22:06.680]   that literally can't treat them unfairly at work,
[00:22:06.680 --> 00:22:11.840]   like asking them to come to the rescue on like period apps,
[00:22:11.840 --> 00:22:14.840]   I mean, I'm sorry, even right before this show,
[00:22:14.840 --> 00:22:17.760]   I had a thoroughly unpleasant Twitter row
[00:22:17.760 --> 00:22:20.640]   with someone that's the head of AR at Google, right?
[00:22:20.640 --> 00:22:25.120]   And Elizabeth Warren was posting a really lovely...
[00:22:25.120 --> 00:22:28.920]   She's mad, you know, posted a video about it.
[00:22:28.920 --> 00:22:31.720]   And he's like, "Well, she wants to regulate big tech."
[00:22:31.720 --> 00:22:34.000]   You know, like that's the beginning and end
[00:22:34.000 --> 00:22:36.240]   of where a lot of people care about this.
[00:22:36.240 --> 00:22:37.600]   It's at their own door.
[00:22:37.600 --> 00:22:41.800]   So I really think that these structural problems
[00:22:41.800 --> 00:22:44.040]   when it comes to how tech treats women,
[00:22:44.040 --> 00:22:45.640]   how tech treats trans people,
[00:22:45.640 --> 00:22:49.400]   how tech treats gay people and black people,
[00:22:49.400 --> 00:22:51.040]   when you're coming to those things,
[00:22:51.040 --> 00:22:54.760]   force really being weaponized by prosecutors
[00:22:54.760 --> 00:22:57.000]   because of the Supreme Court.
[00:22:57.000 --> 00:22:58.520]   I really...
[00:22:58.520 --> 00:23:00.800]   I'm ultra worried about that.
[00:23:00.800 --> 00:23:02.200]   Yeah.
[00:23:02.200 --> 00:23:03.600]   And you have a talk in this, huh?
[00:23:03.600 --> 00:23:07.080]   I mean, you are at risk as much, you know, as anybody.
[00:23:07.080 --> 00:23:08.440]   Sure.
[00:23:08.440 --> 00:23:13.760]   And that's the problem with this enumerated rights doctrine
[00:23:13.760 --> 00:23:15.880]   that this originalism doctrine,
[00:23:15.880 --> 00:23:17.480]   which is really being used as a cover.
[00:23:17.480 --> 00:23:20.320]   I don't think it's actually a genuine belief
[00:23:20.320 --> 00:23:22.040]   on the part of these justices, but they say,
[00:23:22.040 --> 00:23:24.520]   "Well, it's not the Constitution."
[00:23:24.520 --> 00:23:26.560]   Then it's not a right.
[00:23:26.560 --> 00:23:28.400]   Well, I mean, correct me if I'm wrong
[00:23:28.400 --> 00:23:29.600]   because you're all Americans,
[00:23:29.600 --> 00:23:32.080]   but my understanding in the Constitution,
[00:23:32.080 --> 00:23:34.240]   black people are three-fifths of the citizen.
[00:23:34.240 --> 00:23:35.480]   Is that correct?
[00:23:35.480 --> 00:23:36.320]   That's right.
[00:23:36.320 --> 00:23:38.880]   Now, if we're going to go full constitution,
[00:23:38.880 --> 00:23:41.360]   institutional enumeration,
[00:23:41.360 --> 00:23:43.720]   I suspect a lot of people would have a problem with that.
[00:23:43.720 --> 00:23:47.240]   Not at least one Supreme Court justice, although...
[00:23:47.240 --> 00:23:50.560]   Well, ironically, Clarence Thomas, who is black
[00:23:50.560 --> 00:23:52.760]   and who is married to a white woman,
[00:23:52.760 --> 00:23:57.960]   did in fact, in his majority opinion,
[00:23:57.960 --> 00:23:59.960]   his additional opinion,
[00:23:59.960 --> 00:24:04.200]   bring up birth control, gay marriage.
[00:24:04.200 --> 00:24:05.200]   He didn't mention the love...
[00:24:05.200 --> 00:24:06.800]   But strangely not the loving, yeah.
[00:24:06.800 --> 00:24:08.600]   So you didn't mention the interracial marriage.
[00:24:08.600 --> 00:24:09.840]   All that, isn't it?
[00:24:09.840 --> 00:24:10.680]   Yeah.
[00:24:10.680 --> 00:24:13.240]   But because this decision,
[00:24:13.240 --> 00:24:15.720]   that's what this decision leads to.
[00:24:15.720 --> 00:24:17.440]   If it's not an enumerated right,
[00:24:17.440 --> 00:24:19.200]   well, that puts a lot of things that we take
[00:24:19.200 --> 00:24:23.120]   for granted in the 21st century off the table.
[00:24:23.120 --> 00:24:28.000]   And frankly, I'll take the mores and beliefs
[00:24:28.000 --> 00:24:30.840]   of the founders in the mid-18th century,
[00:24:30.840 --> 00:24:32.720]   many of them slaveholders,
[00:24:32.720 --> 00:24:34.880]   really should be informing how we operate
[00:24:34.880 --> 00:24:36.240]   in the 21st century.
[00:24:36.240 --> 00:24:38.240]   Mm-hmm.
[00:24:38.240 --> 00:24:42.480]   And then, you know, well, I don't...
[00:24:42.480 --> 00:24:43.720]   Like, I don't want to get political.
[00:24:43.720 --> 00:24:46.360]   I think that it really is, obviously, too late.
[00:24:46.360 --> 00:24:47.200]   That's your percent.
[00:24:47.200 --> 00:24:48.200]   (laughing)
[00:24:48.200 --> 00:24:51.960]   Hey, look, with technology, we upgrade stuff.
[00:24:51.960 --> 00:24:54.240]   You know, we move on.
[00:24:54.240 --> 00:24:57.720]   The very idea that a document written in the 1700s
[00:24:57.720 --> 00:25:01.600]   should be applied to the 21st century seems...
[00:25:01.600 --> 00:25:05.040]   I admit, okay, I'm a foreigner, but it seems bonkers to me.
[00:25:05.040 --> 00:25:06.760]   You know, it's just you cannot apply
[00:25:06.760 --> 00:25:12.040]   the 18th century rules to 21st century society
[00:25:12.040 --> 00:25:15.040]   and expect everything to be perfect.
[00:25:15.040 --> 00:25:19.920]   I think that's what's disturbing about this particular court.
[00:25:19.920 --> 00:25:22.920]   You know, there are main lawyers who have commented this
[00:25:22.920 --> 00:25:27.560]   that, you know, before this Supreme Court,
[00:25:27.560 --> 00:25:30.040]   like you had conservative justices,
[00:25:30.040 --> 00:25:34.440]   and you could disagree with what they said,
[00:25:34.440 --> 00:25:37.320]   because obviously they have a point of view,
[00:25:37.320 --> 00:25:40.200]   but they were committed to, you know, starting to crisis.
[00:25:40.200 --> 00:25:43.800]   They were committed to let the president stand before, right?
[00:25:43.800 --> 00:25:46.440]   They were committed to the rule of law
[00:25:46.440 --> 00:25:49.600]   and not upending things and having the Supreme Court
[00:25:49.600 --> 00:25:51.080]   be respected.
[00:25:51.080 --> 00:25:53.560]   You know, with this ruling,
[00:25:53.560 --> 00:25:56.520]   the Supreme Court has truly gone to a radical place
[00:25:56.520 --> 00:25:59.960]   where they are upending all of that.
[00:25:59.960 --> 00:26:03.960]   And there's not that continuity that I think even...
[00:26:03.960 --> 00:26:07.400]   I'm seeing a real crisis of conscience with many lawyers,
[00:26:07.400 --> 00:26:11.880]   I know, because it's gone from a process that,
[00:26:11.880 --> 00:26:14.600]   while imperfect, they have faith in
[00:26:14.600 --> 00:26:18.840]   to really feeling like extremists or running the show.
[00:26:18.840 --> 00:26:23.080]   It's become politicized as opposed to some sort of attempt
[00:26:23.080 --> 00:26:26.120]   to at least follow precedent and things like that.
[00:26:26.120 --> 00:26:26.760]   I agree with it.
[00:26:26.760 --> 00:26:28.760]   Although I have to say, I mean,
[00:26:28.760 --> 00:26:31.800]   the court has often in the past ignored starry decisis,
[00:26:34.040 --> 00:26:38.040]   you know, and sometimes for a very good reason,
[00:26:38.040 --> 00:26:41.720]   the John Cornyn brought up,
[00:26:41.720 --> 00:26:45.080]   the Senator from Texas brought up Brown versus the Board of Education,
[00:26:45.080 --> 00:26:49.080]   which overturned the idea of separate but equal,
[00:26:49.080 --> 00:26:56.440]   effectively overturned set law, saying,
[00:26:56.440 --> 00:26:57.480]   "No, that's not enough.
[00:26:57.480 --> 00:27:00.360]   You've got to do more."
[00:27:01.000 --> 00:27:04.680]   So let's call it what it was, Leo.
[00:27:04.680 --> 00:27:05.800]   This was apartheid.
[00:27:05.800 --> 00:27:06.520]   Yeah.
[00:27:06.520 --> 00:27:09.960]   You know, it's not a pleasant word to use in the United States,
[00:27:09.960 --> 00:27:13.480]   but it was exactly the same with what was going on in South Africa.
[00:27:13.480 --> 00:27:17.400]   So let's call it what it is, and let's not go back to it.
[00:27:17.400 --> 00:27:24.040]   He compared reversing Roe versus Wade in a tweet.
[00:27:24.040 --> 00:27:28.200]   He says, "Now do Plessy versus Ferguson and Brown versus Board of Education."
[00:27:28.200 --> 00:27:30.040]   I have a shameful, I know.
[00:27:30.040 --> 00:27:33.000]   And unfortunately, that can really be interpreted to do.
[00:27:33.000 --> 00:27:35.560]   Let's go back to segregation.
[00:27:35.560 --> 00:27:38.680]   Coming from a Senator from Texas, it's not hard.
[00:27:38.680 --> 00:27:39.560]   It's not a stretch.
[00:27:39.560 --> 00:27:43.480]   I think his real intent was, well, I don't know.
[00:27:43.480 --> 00:27:45.960]   I'm not going to explain his real intent.
[00:27:45.960 --> 00:27:49.400]   I do wonder what all those tech people who are moving to Texas
[00:27:49.400 --> 00:27:51.000]   are now thinking in crisis.
[00:27:51.000 --> 00:27:54.600]   Like, yeah, let's go to Austin and build a new tech community.
[00:27:54.600 --> 00:27:55.080]   Yeah.
[00:27:55.080 --> 00:28:01.240]   Oh, right. My daughter might be sued for actually exercising her own bodily autonomy.
[00:28:01.240 --> 00:28:01.880]   Yeah.
[00:28:01.880 --> 00:28:03.560]   I'm not that.
[00:28:03.560 --> 00:28:12.280]   Of course, Alan Butler, Executive Director, President of Electronic Privacy Information
[00:28:12.280 --> 00:28:21.080]   Center said law enforcement has a lot of data, credit card records, cell phone towers
[00:28:21.080 --> 00:28:24.200]   that can get you in a lot of trouble.
[00:28:24.200 --> 00:28:31.480]   I mean, right now we send off a lot of data smog in our everyday transactions
[00:28:31.480 --> 00:28:36.440]   that could be used against us, particularly women.
[00:28:36.440 --> 00:28:41.480]   Although, as I said, I suspect it's going to extend to past just women.
[00:28:41.480 --> 00:28:49.160]   And so clearly, when you have a government that is willing and ready to use that information,
[00:28:49.160 --> 00:28:54.440]   it seems like we got to do something about privacy.
[00:28:54.440 --> 00:28:58.360]   But this has been an intractable problem for years.
[00:28:58.360 --> 00:29:00.360]   What do we do?
[00:29:00.360 --> 00:29:07.640]   Well, I mean, your internet searches are public.
[00:29:07.640 --> 00:29:13.080]   You're traveling, you go to an abortion clinic, it's public.
[00:29:13.080 --> 00:29:18.520]   You know, your messages, your emails, it's all public.
[00:29:18.520 --> 00:29:19.240]   GPS.
[00:29:19.240 --> 00:29:20.760]   GPS, it's all public.
[00:29:20.760 --> 00:29:23.800]   Quick straw poll, actually.
[00:29:23.800 --> 00:29:25.880]   I mean, a quick straw poll.
[00:29:25.880 --> 00:29:30.680]   How many of you keep your location data, your location tabs, which are on the phones?
[00:29:30.680 --> 00:29:33.320]   I do, but I'm an old white man.
[00:29:33.320 --> 00:29:37.320]   I'm the only one not at risk on this panel.
[00:29:37.320 --> 00:29:41.160]   So it's safe for me because you should.
[00:29:41.160 --> 00:29:42.200]   I'm tracking.
[00:29:42.200 --> 00:29:43.800]   I'm tracking a lot of people.
[00:29:43.800 --> 00:29:46.360]   I track my mom, I track my cousin.
[00:29:46.360 --> 00:29:46.920]   That's right.
[00:29:46.920 --> 00:29:47.480]   Okay.
[00:29:47.480 --> 00:29:48.840]   Yeah, I track my husband.
[00:29:48.840 --> 00:29:49.960]   I track my daughter.
[00:29:49.960 --> 00:29:51.640]   I tell my daughter, you got to leave that on.
[00:29:51.640 --> 00:29:53.240]   And she says, I'm not leaving that on.
[00:29:53.240 --> 00:29:55.400]   Yes.
[00:29:55.400 --> 00:29:57.400]   Exactly.
[00:29:57.400 --> 00:29:57.960]   Yeah, right.
[00:29:57.960 --> 00:30:01.720]   Mate, I mean, I'm just paranoid and childless, but yes, it's.
[00:30:01.720 --> 00:30:07.240]   I mean, I haven't turned on like a per-app basis, right?
[00:30:07.240 --> 00:30:11.160]   And for the answer, I trust, but you know, like everyone else,
[00:30:11.160 --> 00:30:13.720]   there's there's plenty of dirt you can find on me.
[00:30:13.720 --> 00:30:17.480]   And as to what can be done, Leo, I really think this is
[00:30:17.480 --> 00:30:20.440]   for looking for historical parallels.
[00:30:20.440 --> 00:30:26.120]   If you look at the Democratic Party after Jimmy Carter's loss, right,
[00:30:26.120 --> 00:30:30.600]   the Democrats have controlled the house for a long time before that,
[00:30:30.600 --> 00:30:36.680]   it really took the party 15, 20 years to really find its footing again and move forward.
[00:30:36.680 --> 00:30:43.000]   I really think that what we're seeing right now is kind of the last cries of this last Democratic
[00:30:43.000 --> 00:30:49.480]   party. And I think that moving forward, I think it's going to look more like,
[00:30:49.480 --> 00:30:55.160]   these, and I'm not even talking politics like progressive versus more centrist Democrats.
[00:30:55.160 --> 00:30:57.240]   I'm talking about communication styles.
[00:30:57.240 --> 00:31:02.520]   If you look at an Elizabeth Warren or an AOC, they're just much better communicators.
[00:31:02.520 --> 00:31:05.960]   Like they use these tools more natively that all of us use.
[00:31:05.960 --> 00:31:12.360]   They're able to tap into the emotion and they really come from this more activist
[00:31:12.360 --> 00:31:20.040]   grassroots background. I think that what we're going to see is rather than this kind of more
[00:31:20.040 --> 00:31:29.720]   centrist, let's respect civility. I think the party is going to change to something much more
[00:31:29.720 --> 00:31:33.320]   focused on the state house and kind of more activist power.
[00:31:33.320 --> 00:31:37.080]   Hopefully, Stacey Abrams type.
[00:31:37.080 --> 00:31:41.400]   Yeah, Stacey Abrams is the future 100%. She's much more of a centrist Democrat.
[00:31:41.400 --> 00:31:46.360]   I have to say I've somewhat lost a longtime Democrat, but I've somewhat lost faith in both
[00:31:46.360 --> 00:31:54.760]   parties. And unfortunately, what that leads to often is just a kind of nihilism about
[00:31:54.760 --> 00:31:59.400]   government in general, like government ain't going to do it. They have failed us.
[00:31:59.400 --> 00:32:05.560]   For a long time, I fought against that. A lot of people in the tech community have that feeling
[00:32:05.560 --> 00:32:09.320]   about government ass, grew it. Government has nothing to do with me. And there's nothing you
[00:32:09.320 --> 00:32:15.480]   can do. They're corrupt. And I'm starting to feel that way, but I know that there's no way forward
[00:32:15.480 --> 00:32:21.640]   with that. That's just nihilism. Let it all burn. And I don't think that's a solution either.
[00:32:21.640 --> 00:32:27.960]   St. Epp Toficki had an excellent opinion piece back when the original decision was leaked.
[00:32:27.960 --> 00:32:33.320]   We need to take back our privacy. This is in the New York Times. She actually talks about an
[00:32:33.320 --> 00:32:45.400]   interesting point in time 130 years ago, when the first portable cameras were invented.
[00:32:45.400 --> 00:32:52.520]   Louis Brandeis, who was to become a Supreme Court Justice later, wrote recent inventions
[00:32:52.520 --> 00:32:57.960]   and business methods call attention to the next step, which must be taken for the protection
[00:32:57.960 --> 00:33:03.720]   of the person. He warned the laws were needed to keep up with technology 130 years ago,
[00:33:03.720 --> 00:33:06.600]   or Americans would lose their right to be let alone.
[00:33:06.600 --> 00:33:15.240]   It's coming. I mean, in terms of involvement, though, check out Katie Missouri,
[00:33:15.240 --> 00:33:23.560]   classic security specialist. But she's now a delegate in her local state.
[00:33:24.200 --> 00:33:27.400]   But that may be enough to happen to people. People with technology.
[00:33:27.400 --> 00:33:30.760]   You need to get the ground route need to start getting involved. And of course,
[00:33:30.760 --> 00:33:35.240]   it has to have that's what you were doing, Brande. It starts locally. It starts.
[00:33:35.240 --> 00:33:42.200]   It starts local. And I didn't win, but you know, I took my skills to go on to build a very
[00:33:42.200 --> 00:33:48.040]   successful pack that donates tons of money to races that otherwise would be forfeit. You know,
[00:33:48.040 --> 00:33:55.400]   like, I mean, even if you lose, you're still building up valuable skills. So we need more people like
[00:33:55.400 --> 00:34:01.880]   me to basically get involved and stay involved because the country is literally on the line right
[00:34:01.880 --> 00:34:09.880]   now. And you know, the thing that's so important is I feel so strongly, we've got to,
[00:34:09.880 --> 00:34:17.960]   my perspective on growing up in Mississippi is this. A lot of the people that are supporting
[00:34:17.960 --> 00:34:23.320]   this movement that's going increasingly fascist, it's because we failed them.
[00:34:23.320 --> 00:34:29.080]   Ordinary solutions have failed them. They don't see their lives getting better. They don't see
[00:34:29.080 --> 00:34:34.440]   their income like going up. And just like growing up in Mississippi, you have really
[00:34:34.440 --> 00:34:40.600]   give the worst politicians basically taking advantage of that. If we are really interested in
[00:34:40.600 --> 00:34:46.200]   reverting a civil war in this country, we've got to reach out to them and make the case to those
[00:34:46.200 --> 00:34:50.680]   voters about the difference that we can make in their lives. And we need to do it in real ways
[00:34:50.680 --> 00:34:56.200]   because, Leo, this this is a democratic field. That's where Democrats have failed. Yeah, 100%.
[00:34:56.200 --> 00:35:02.040]   And this stalemate that we have with this culture war, it's not good for women being
[00:35:02.040 --> 00:35:07.160]   thrown out to be the ball in a game of kickballs, not good for trans people's, not good for gay
[00:35:07.160 --> 00:35:12.360]   people's, not good for black people. Like, we've got to go out there and make the case to ordinary
[00:35:12.360 --> 00:35:17.960]   Americans or else the entire country is just going to end up like Mississippi and this cultural
[00:35:17.960 --> 00:35:21.480]   political quagmire forever where nothing gets better.
[00:35:21.480 --> 00:35:27.720]   Well, I mean, I remember when you ran for office and they ran those photos of here are the candidates.
[00:35:27.720 --> 00:35:33.480]   And it was like, yeah, two white guys in suits. And then you, and it was just like, they picked the,
[00:35:33.480 --> 00:35:38.680]   you know, they picked a shot, which, okay, as a journalist, you look at it and you're just kind of
[00:35:38.680 --> 00:35:44.280]   like, yeah, they did that deliberately. You know, it was just, you've got to get in there, though,
[00:35:44.280 --> 00:35:50.120]   and all credit you for doing it and actually making it work. But it, it just, it's,
[00:35:50.120 --> 00:35:54.520]   it's, I think you start picture and they told me after the fact when I had it out with the
[00:35:54.520 --> 00:36:01.000]   ball, really, because they said because that was a picture they found a line where I was smiling
[00:36:01.000 --> 00:36:06.040]   because of my standard shot. I was looking very seriously at the camera.
[00:36:06.040 --> 00:36:12.440]   She says, okay, my homework school, the worst thing you can tell any woman is you ought to smile more.
[00:36:12.440 --> 00:36:19.080]   That is so horrific, so horrific. By the way, no one has ever told me I should smile more.
[00:36:19.080 --> 00:36:25.160]   What a surprise, right? Leo, you should smile more. Oh, thanks, Flo.
[00:36:25.160 --> 00:36:32.440]   All right, let's, that, okay, we, you know, look, we have to talk about it.
[00:36:34.840 --> 00:36:40.680]   It's, it's challenging. You are listening to a podcast that is mostly blue
[00:36:40.680 --> 00:36:46.600]   in a country that is very much divided between blue and red. I don't, I don't know what else to say.
[00:36:46.600 --> 00:36:54.280]   Many of you may not agree, maybe celebrating what's happened and maybe looking forward to a world
[00:36:54.280 --> 00:36:59.080]   where contraception is no longer available and gay people are thrown in jail and black people are
[00:36:59.080 --> 00:37:03.400]   kept from drinking in the same fountain as white people. That may be something you think is a good
[00:37:03.400 --> 00:37:10.440]   thing. It's just not, we don't agree on that. And unfortunately, technology has a lot to do with
[00:37:10.440 --> 00:37:15.880]   how this is going to happen. And so it is part of our mandate to talk about it.
[00:37:15.880 --> 00:37:22.360]   But I think the point that was made earlier was, was very clear. It's like back in the
[00:37:22.360 --> 00:37:27.160]   anti-boy, you know, back when abortion and gay people and trans people were illegal,
[00:37:27.160 --> 00:37:32.600]   there was no technology. And now there is, and it's going to be a massive problem. We have to
[00:37:32.600 --> 00:37:37.960]   deal with it. Let's take a little break. We're going to come back. We have much more to talk about,
[00:37:37.960 --> 00:37:45.640]   including Florence will explain this at grandma, your late grandma appearing in your Amazon Echo.
[00:37:45.640 --> 00:37:57.480]   That's a creepy idea. But first a word from our sponsor, audible. Now I tell you what,
[00:37:58.280 --> 00:38:03.080]   audible is a great way to stay up on what's going on in the world around us.
[00:38:03.080 --> 00:38:13.640]   There are some fantastic audio programs you can get at audible.com on these very subjects,
[00:38:13.640 --> 00:38:19.000]   say reproductive history, the story of Jane, the legendary underground feminist,
[00:38:19.000 --> 00:38:27.320]   the Jane in Jane Rowe, Roe v. Wade, bodies on the line, Jane against the world.
[00:38:28.040 --> 00:38:33.560]   Some really great stuff. If you want to become educated, if you want to learn about the world
[00:38:33.560 --> 00:38:39.880]   around you, if you want to be entertained, audible is the place to go. I listen to audible all the
[00:38:39.880 --> 00:38:48.280]   time. In fact, I actually would like to recommend it's, it's all about the man who made modern
[00:38:48.280 --> 00:38:55.320]   computing possible, John von Neumann, a brilliant Hungarian, a Jewish Hungarian mathematician who
[00:38:55.320 --> 00:39:02.200]   came to the United States. And an interesting figure was very involved in the creation of the
[00:39:02.200 --> 00:39:08.600]   atom bomb, but also worked with touring to create the foundation for modern computing.
[00:39:08.600 --> 00:39:15.080]   Fascinating. This is by Anonio Bhattacharya and the audiobook, which is really, I got to play a
[00:39:15.080 --> 00:39:19.240]   little bit of this narrated by Nicholas Cam is just, this is why I listen to audible.
[00:39:19.800 --> 00:39:26.440]   Eerily prescient with every passing year to fully understand the intellectual currents running
[00:39:26.440 --> 00:39:33.480]   through our century from politics to economics, technology to psychology. One has to understand
[00:39:33.480 --> 00:39:40.520]   von Neumann's life and work in the last. My recommendation, but let me tell you audible has so many
[00:39:40.520 --> 00:39:45.960]   audio, wonderful audio programs. Of course, they've got bestsellers, celebrity memoirs,
[00:39:45.960 --> 00:39:49.800]   mysteries. We're listening to a dar, Lisa and I are listening together to a dar, the
[00:39:49.800 --> 00:39:58.840]   sares mystery that is so good. We're listening to a dar, the sares, classic, strong poison. It's a
[00:39:58.840 --> 00:40:07.240]   Lord Peter whimsy mystery. And it's just part of our audible subscription, which makes it so much
[00:40:07.240 --> 00:40:13.560]   fun as part of the plus catalog, hundreds and hundreds of audiobooks, performances and more.
[00:40:13.560 --> 00:40:18.120]   So when you're in the mood for something, I always go through the audible plus catalog.
[00:40:18.120 --> 00:40:27.320]   Audible has the most amazing collection of performances. Podcasts, advertising free podcasts,
[00:40:27.320 --> 00:40:32.280]   audible originals, they record some of the best science fiction, science fiction that would
[00:40:32.280 --> 00:40:37.640]   otherwise not be recorded like the Bobiverse. Those are audible originals. They're wonderful,
[00:40:37.640 --> 00:40:42.280]   beautifully produced. Ray Porter is one of the best readers in the world. He does such a good
[00:40:42.280 --> 00:40:48.600]   job with a Bobiverse highly recommended. Audible originals for top celebrities, renowned experts,
[00:40:48.600 --> 00:40:53.080]   new voices and audio. As an audible member, you get a title a month from their entire catalog.
[00:40:53.080 --> 00:40:57.880]   That's yours to keep forever. I have a library of 500, including the bestsellers, the newest
[00:40:57.880 --> 00:41:03.960]   releases. You also get access to the audible plus catalog. So you can always supplement those.
[00:41:03.960 --> 00:41:10.360]   You can listen to all you want and get more every month, more get added every month.
[00:41:11.560 --> 00:41:16.520]   So that's fantastic. The audible app on your phone is great, but you can put it on your watch.
[00:41:16.520 --> 00:41:23.000]   I listen on Amazon. I listen on my Sonos now too, which is great. Audible, let Audible help you
[00:41:23.000 --> 00:41:28.120]   discover the ways, new ways to laugh, to be inspired, to be entertained, to learn. New
[00:41:28.120 --> 00:41:34.600]   members can try Audible free for 30 days. All you got to do is go to audible.com/twit, A-U-D-I,
[00:41:34.600 --> 00:41:43.800]   B-L-E, Audible.com/twit, or text Twit to 500 500. They'll send you a link. You can download the app.
[00:41:43.800 --> 00:41:52.120]   Audible, A-U-D-I, B-L-E.com/twit, text Twit to 500 500 to try Audible free for 30 days. Audible,
[00:41:52.120 --> 00:41:58.040]   we love you. I got an email from somebody who says, "Heaven't listened much to the podcast in the last
[00:41:58.040 --> 00:42:02.440]   five years because you told me about Audible and now I just listen to audio books. Okay,
[00:42:03.720 --> 00:42:06.760]   I understand. I understand that. That's okay. That's just my your own pet art.
[00:42:06.760 --> 00:42:13.080]   Oh, a hoist by our own pet art. But you know what? It's fine. It's fine. We love Audible. Audible.
[00:42:13.080 --> 00:42:19.320]   Dotcom/twit. Thank you Audible for your support and for stealing all our audience.
[00:42:19.320 --> 00:42:24.840]   It's kind of nice. There's so much you can listen to nowadays. I almost wish I had a long
[00:42:24.840 --> 00:42:32.680]   commute again. I would get two or three hours every day in the car. Now I'm like you. I don't have a
[00:42:32.680 --> 00:42:37.560]   flow. I don't have a commute anymore. I just have to find ways to listen to Audible. But I bet with
[00:42:37.560 --> 00:42:43.000]   the baby, maybe there's times when you're playing with the kid and she doesn't know you've got an
[00:42:43.000 --> 00:42:47.400]   Audible in your ear listening to a book. Oh, yeah, I did that today. Yeah. To go play with her at
[00:42:47.400 --> 00:42:51.640]   the park. I was listening to. Oh, yeah, honey. That's nice. Have fun at this slide. Yeah.
[00:42:51.640 --> 00:43:00.680]   What are you listening to? Well, I am listening to a lot of cult.
[00:43:01.960 --> 00:43:05.720]   You don't have to tell us if you cult stuff. That's why I don't. Yes. Yes.
[00:43:05.720 --> 00:43:08.600]   Oh, interesting. I actually forgot the name of the book I'm listening to.
[00:43:08.600 --> 00:43:15.560]   All about cults. I have a book I can share. Please. I just so Tokyo Vice
[00:43:15.560 --> 00:43:23.240]   this show, the book, the HBO shows based on excellent. In the day, row versus way came out,
[00:43:23.240 --> 00:43:28.120]   I just could not think about it. Sometimes you need a break. Yeah. 100%. So I
[00:43:28.120 --> 00:43:34.440]   mainlineed a book by Craig Allinson. There's about a talking beer can in space that goes on
[00:43:34.440 --> 00:43:40.360]   space battles with the people. It's the trashiest science fiction. What? I love him. He's a good
[00:43:40.360 --> 00:43:45.720]   friend of mine. He has written 15 books of these on Audible. It was called Match Came.
[00:43:45.720 --> 00:43:49.800]   It's such a silly novel. Book 14 of expeditionary force series.
[00:43:49.800 --> 00:43:54.440]   I see. It's an ordinary force. Oh, and RC brain arrows. There it's it. He's one of my favorite
[00:43:54.440 --> 00:43:59.000]   narrators. It's one of the things that happens with Audible is you start to find these narrators
[00:43:59.000 --> 00:44:04.360]   you just love. You search for the not the author of the book. You search for the narrators.
[00:44:04.360 --> 00:44:10.760]   Yeah. But unfortunately, that can backfire because, for example, I love Bill Bryson's books.
[00:44:10.760 --> 00:44:17.320]   Yes. He's done some really good ones. Yes. But his accent as a Brit is so annoying.
[00:44:17.320 --> 00:44:21.320]   I love it when Bill Bryson narrates his own books. You don't like that. Oh, my God. No,
[00:44:21.320 --> 00:44:26.840]   because he sounds like the worst kind of transatlantic wanker on the planet. It's just like,
[00:44:26.840 --> 00:44:32.280]   he gets words wrong. He gets Americanizations in there. And it's just a dull voice.
[00:44:32.280 --> 00:44:37.160]   He lived in the UK for a long time, right? I think he's. Yeah. He moved. Well, he moved there,
[00:44:37.160 --> 00:44:39.960]   moved back to the US and then moved back to the UK again.
[00:44:39.960 --> 00:44:47.560]   And yeah, he lived there for a long time. And it kind of, he's got that transatlantic thing going.
[00:44:48.680 --> 00:44:53.000]   Let me play a little bit. This is a book my trainer actually is relating right now or listening to
[00:44:53.000 --> 00:44:57.560]   right now. And he strongly recommended the body a guide for occupants. Let me just,
[00:44:57.560 --> 00:45:01.800]   this is Bill Bryson talking. Let's just want to hear his voice. When I was a junior high school
[00:45:01.800 --> 00:45:06.040]   student in America, I see what you're talking about. I remember being taught by a biology teacher
[00:45:06.040 --> 00:45:09.960]   at all the chemicals that make up a human body. See, I don't think this is a bother mind. But I
[00:45:09.960 --> 00:45:15.880]   can see you're saying he's got a little mixed up. His accent is a little mixed up. It's kind of like
[00:45:15.880 --> 00:45:21.320]   it's a bit try hard. You know, I can't do an American accent. I don't try to do it.
[00:45:21.320 --> 00:45:24.440]   Because you all really take the piss out of me.
[00:45:24.440 --> 00:45:30.120]   Just like the Brits do when I do my British accents. Oh God, yeah.
[00:45:30.120 --> 00:45:34.040]   Dick Van Dyke will never live that down. No, Mary loves Mary Poppins.
[00:45:34.040 --> 00:45:43.240]   Oh, Mary. All right. Well, I like to be listening to Bill Bryson's books because they're really,
[00:45:43.240 --> 00:45:49.240]   really good. He's got a whole bunch of good books. All right, let's talk about Amazon Echo.
[00:45:49.240 --> 00:45:55.000]   This is the Remar's conference was this week. Amazon does this. I see dead people.
[00:45:55.000 --> 00:46:01.320]   I hear them anyway. I hear them. I hear them. Every year they had Adam Savage was one of the
[00:46:01.320 --> 00:46:09.000]   speakers at Remar's from, of course, the Mythbusters and I guess his new site, The Tested.
[00:46:10.440 --> 00:46:15.160]   But one of the things Amazon announced did not sit well with me, but you know more than I do.
[00:46:15.160 --> 00:46:24.360]   What is Amazon proposing? Well, unfortunately, I was not in attendance for this conference for me
[00:46:24.360 --> 00:46:31.320]   to get the full picture, but from what from what I read and research. Amazon is just putting that
[00:46:31.320 --> 00:46:38.920]   caveat. Amazon is proposing that you feed it your dead relative's voice. Let's just say it's
[00:46:38.920 --> 00:46:45.560]   your relative. It could be anybody. And then it's Al Gore. It's AI will repeat it back to you.
[00:46:45.560 --> 00:46:51.240]   In just a few minutes of recording, you don't need like you could if you wanted. I probably
[00:46:51.240 --> 00:46:56.040]   easy for Amazon to simulate me. I've got a hundred a thousand hours of recording available online,
[00:46:56.040 --> 00:47:00.200]   but you don't need that much of grandma couple of minutes and then.
[00:47:00.200 --> 00:47:03.160]   A voice mail. Just a voice mail. And Echo could talk like her.
[00:47:05.320 --> 00:47:09.560]   I don't know which is worse. If it sounds exactly like her and she's passed,
[00:47:09.560 --> 00:47:15.880]   is that creepier than if it sounds sort of like her, but it's a robot, which is worse.
[00:47:15.880 --> 00:47:24.200]   Right. I had the same thought and actually so so my thought was, oh God, is this something that
[00:47:24.200 --> 00:47:31.800]   would interfere with the grieving process? Because if you're like, that he grandma's not dead,
[00:47:31.800 --> 00:47:40.280]   she's in the echo. You. I mean, there's a reason you give a goldfish to a kid as their first.
[00:47:40.280 --> 00:47:46.520]   That's oh boy. Is that true? People do that? I mean, you know, time to learn about death. Have a fish.
[00:47:46.520 --> 00:47:54.280]   Yeah. Wow. But this thing is this isn't a new idea. So actually in my Gizmodo article,
[00:47:54.280 --> 00:47:59.720]   I brought up there's this toy from Takara Tomi, which is a big toy manufacturer in Japan.
[00:47:59.720 --> 00:48:04.360]   And they have a smart speaker that actually can imitate a parent's voice. So like a parent who
[00:48:04.360 --> 00:48:09.160]   is traveling a lot, a parent who maybe isn't at home for bedtime could like theoretically, you
[00:48:09.160 --> 00:48:15.640]   know, read a story to their kid, which is nice. But the idea of somebody deceased, first of all,
[00:48:15.640 --> 00:48:23.800]   it removes the consent from that deceased person to exist in your life beyond the dead.
[00:48:24.760 --> 00:48:29.880]   And it also, again, my question was, how does this interfere with the actual grief process,
[00:48:29.880 --> 00:48:37.720]   which I am told by professionals in my life is a multi step process. Sometimes it takes many,
[00:48:37.720 --> 00:48:43.960]   many, many years. Sometimes you never get over the death of somebody. So how will this affect that?
[00:48:43.960 --> 00:48:49.240]   But then there is this school of thought, and this is completely valid and fair from those who
[00:48:49.240 --> 00:48:54.040]   have lost loved ones like a parent, let's say, that this is actually something that could be
[00:48:54.040 --> 00:48:57.320]   quite comforting. So there's a lot of
[00:48:57.320 --> 00:49:03.640]   it's just unknown. Yeah. And there could be a lot of psychiatric
[00:49:03.640 --> 00:49:09.640]   work that you might need later later in life. The first thing I thought about when I read about
[00:49:09.640 --> 00:49:14.200]   this, I'm like, okay, how is this going to interfere with the psychology of a person?
[00:49:14.200 --> 00:49:19.160]   And especially because we just got off the heels of that is AI human debate,
[00:49:19.800 --> 00:49:25.480]   which is kind of still happening. And so to kind of hear this, it's like, I don't know if I want
[00:49:25.480 --> 00:49:32.040]   to immortalize a dead one's voice, but there are people out there who do. So what do you do then?
[00:49:32.040 --> 00:49:40.600]   Wasn't there a black mirror episode where there was a robot or an android that was a
[00:49:40.600 --> 00:49:49.000]   somebody's ex wife or something had died? I didn't really remember this. It's very black mirror,
[00:49:49.000 --> 00:49:54.120]   right? See, and then that scares me. What if somebody takes, like you said, Leo, what if somebody
[00:49:54.120 --> 00:49:58.840]   takes your voice from the internet and wants to just continue that parasocial relationship?
[00:49:58.840 --> 00:50:03.480]   I think it's a good possibility. It's already happened. And then I don't even know it that I'm
[00:50:03.480 --> 00:50:11.960]   just a simulation. If I could go ahead, go ahead. No, I was just going to say my really good friend
[00:50:11.960 --> 00:50:18.440]   Amanda Windleesh is a famous voice actress, right? She did even, even Jillian. She did
[00:50:18.440 --> 00:50:26.280]   persona. She's like, if there's plastic on a may from the 80s and 90s, she's in it. I have no doubt
[00:50:26.280 --> 00:50:32.840]   she did a Sailor Moon episode probably at some point. And she was talking about how her fear with
[00:50:32.840 --> 00:50:40.600]   this was it was going to commodify what she does and let people use her instantly recognizable
[00:50:40.600 --> 00:50:47.080]   voice right off the bat without paying any rights. I think that for her, she's a professional.
[00:50:47.080 --> 00:50:53.080]   They would have to license her voice. Wouldn't they? Yeah. Well, couldn't you just go in and
[00:50:53.080 --> 00:51:00.280]   like tech usually when they're bringing on new technology, they try to like put their best foot
[00:51:00.280 --> 00:51:05.400]   forward. They don't put forward the dystopian stuff it's going to do. They put forward their
[00:51:05.400 --> 00:51:12.120]   best case for it. And, you know, we started thinking like, how can you use this technology that
[00:51:12.920 --> 00:51:19.880]   can basically take us when speech patterns and recreated like, okay, maybe having a loved one
[00:51:19.880 --> 00:51:26.360]   near you. But I think a lot of the use cases are deep fakes. You know, it's revenge porn that
[00:51:26.360 --> 00:51:32.600]   may be false. It's making it harder for people to make a living their voice acting professionals.
[00:51:32.600 --> 00:51:37.400]   And also a simple fake. Yeah. I could call up and say, Hey, it's Frank.
[00:51:38.440 --> 00:51:44.360]   Could you wire me some money because I'm stuck here in Memphis with the mobile blues again and
[00:51:44.360 --> 00:51:48.840]   you wouldn't know it. I mean, this is being used already in business email compromise.
[00:51:48.840 --> 00:51:55.960]   You know, we're already seeing. Yeah. We're already seeing this coming through where deep fakes are
[00:51:55.960 --> 00:52:01.640]   being used. It's very early days yet, but that, you know, it's getting worse. But, you know,
[00:52:01.640 --> 00:52:07.400]   these kind of deep fake voices are being used in business email, business email compromise,
[00:52:07.400 --> 00:52:12.600]   which costs, and I need to check the figures, but about 5 billion over the last five years.
[00:52:12.600 --> 00:52:19.320]   So, you know, people are making a shed load of money about the out of this. And it's going to
[00:52:19.320 --> 00:52:25.320]   get worse because the money's there. As usual, Black Mirror was there. First Jason Howell tells
[00:52:25.320 --> 00:52:31.000]   me it was the first episode of season two called Be Right Back. After learning about a new service
[00:52:31.000 --> 00:52:36.680]   that lets people stay in touch with the deceased, a lonely grieving Martha reconnects with her late
[00:52:36.680 --> 00:52:45.000]   lover. And Mirth and Merriman Sue, it's. Well, there's a short story by Rogers Alazni,
[00:52:45.000 --> 00:52:51.720]   famous science professor. Yeah. Where yes, though, you had gravestones with your
[00:52:51.720 --> 00:52:59.880]   dead, you know, with the dead relative's voice and brain installed in them. And he was trying to do
[00:52:59.880 --> 00:53:05.720]   share options to get his son, a dead person, was trying to get his son to connect him with another
[00:53:05.720 --> 00:53:11.640]   dead widow. It was a deeply twisted and very, very funny story. I highly recommend it.
[00:53:11.640 --> 00:53:16.600]   I'm, you know what, I might download all these old Black mirrors and watch them again,
[00:53:16.600 --> 00:53:21.720]   because it's what a great series that was. Here's another episode from the same season.
[00:53:21.720 --> 00:53:28.200]   A failed comedian who voices a popular cartoon bear named Waldo finds himself mixing in politics
[00:53:28.200 --> 00:53:34.120]   when TV executives want Waldo to run for office. Just like you said, Bran.
[00:53:35.080 --> 00:53:39.400]   Yeah. Well, wasn't it the Simpsons? I mean,
[00:53:39.400 --> 00:53:44.280]   A, the Simpsons actually predicted President Trump. Yeah. And also, didn't they have Ralph
[00:53:44.280 --> 00:53:54.680]   as running for president? Maybe. So I don't think we're very far off from this. I mean,
[00:53:54.680 --> 00:53:59.960]   I have Samuel L Jackson in my Amazon Echo. I have Melissa McCarthy in my Amazon Echo.
[00:53:59.960 --> 00:54:03.320]   I love asking Samuel for the weather because he swears up a blue streak.
[00:54:03.320 --> 00:54:08.760]   And then you ask him, why do you swear so much? And his response is even more blue.
[00:54:08.760 --> 00:54:16.840]   It's hysterical. We see with the old GPS, Tom, Tom, things. John Cleese. There was a John Cleese
[00:54:16.840 --> 00:54:23.800]   Easter egg in there. Yeah. If you if you went against its navigation things enough times,
[00:54:23.800 --> 00:54:27.800]   John Cleese's voice would go saying, well, if you're not going to pay attention, I'm not going to
[00:54:27.800 --> 00:54:34.280]   take you. I've said this before, but I had a Tom Tom voice that was Glados from Portal.
[00:54:34.280 --> 00:54:39.000]   And she'd always tell you the wrong direction. She'd say turn left when he was supposed to turn
[00:54:39.000 --> 00:54:47.720]   right. She it was complete chaos. It was terrible. Yeah. Tom Tom really was the first to do all that
[00:54:47.720 --> 00:54:53.240]   stuff. It's very, I guess the technique at this point is they have the robotic voice and they can
[00:54:53.240 --> 00:54:58.680]   apply what they call prosody, which is the intonation, the inflection, the sound of a
[00:54:58.680 --> 00:55:04.440]   human's voice up on top of it. And they've gotten, I think, pretty good at that to the point that
[00:55:04.440 --> 00:55:09.080]   one company approached me saying, you know, you never have to read any more commercials.
[00:55:09.080 --> 00:55:17.800]   We can synthesize them all. And I said, that's some ethically very fat issues in area.
[00:55:19.080 --> 00:55:26.600]   They're literally trying to sell this to radio and TV that you just write the copy and then
[00:55:26.600 --> 00:55:31.720]   it'll read it in somebody else's voice. And you're going to bet that some people with fewer issues
[00:55:31.720 --> 00:55:37.240]   on ethics are going to do that as well. Yeah. Isn't this what Google's doing though with the
[00:55:37.240 --> 00:55:43.400]   service that calls the restaurant for you? Yeah. I forgot the name. Although it's not trying to
[00:55:43.400 --> 00:55:48.600]   sound like you, but it is sounding pretty realistic, even to the point where it's synthetic, right?
[00:55:48.600 --> 00:55:53.000]   It's synthetic when it's when it's thinking, because sometimes computer isn't thinking as fast as a
[00:55:53.000 --> 00:55:59.560]   human, when it's thinking, it will add a just like a human would. Yeah, they did that when they
[00:55:59.560 --> 00:56:08.200]   demoed it down so creepy. I remember my own down. Yeah. You know, they demoed it. And they
[00:56:08.200 --> 00:56:14.680]   presented it as a feature. It's kind of like, yeah, okay, it'll sound like a human because it'll
[00:56:14.680 --> 00:56:20.440]   are more little are. This is a feature apparently, not something that is deeply deeply
[00:56:20.440 --> 00:56:26.840]   deeply proud of. Here's this is Google I/O in 2018. It's to help you get things done.
[00:56:26.840 --> 00:56:32.600]   Send her for Chai. It turns out a big part of getting things done is making a phone call.
[00:56:32.600 --> 00:56:37.400]   You may want to get an oil change schedule. So this is five, four years ago in the middle of the week.
[00:56:37.400 --> 00:56:41.800]   Yep. Or even scheduling the house at a point. Sitting in that auditorium baking in the sun.
[00:56:41.800 --> 00:56:48.840]   I remember that. Yes, it was very hot. We want to connect. Let me see if I can just skip ahead to the call.
[00:56:48.840 --> 00:56:54.680]   What happens is the Google Assistant makes the call seamlessly in the background for you.
[00:56:54.680 --> 00:57:00.840]   So what you're going to hear is the Google Assistant actually calling a real salon
[00:57:00.840 --> 00:57:08.760]   to schedule the appointment for you. Let's listen. Oh, the crowd goes wild. Oh, okay, he smells bad
[00:57:08.760 --> 00:57:14.120]   as Apple, but it was pretty bad. Hi, I'm calling the Google Women's haircut for a client.
[00:57:14.120 --> 00:57:18.920]   I'm looking for something on May 3rd. It's working. We are.
[00:57:18.920 --> 00:57:26.600]   Those are still in use, by the way. Are they? If you go to Google Maps, you will see if you're
[00:57:26.600 --> 00:57:33.400]   looking at the information for a restaurant or whatever. It'll show you hours confirmed two
[00:57:33.400 --> 00:57:39.000]   weeks ago, which is because they were calling. That's when somebody called after this demo. I
[00:57:39.000 --> 00:57:45.000]   remember there was a furor and the biggest issue was are you. We felt like you're messing with that
[00:57:45.000 --> 00:57:51.800]   human by pretending to be a human. Are you going to let people and after that, Google did start
[00:57:51.800 --> 00:57:59.960]   putting an announcement. Hello. It didn't say I'm a robot. This is like Google messaging or whatever.
[00:57:59.960 --> 00:58:06.680]   Yeah, but it kind of, you may not be talking to a human at this point. It's a little creepy.
[00:58:06.680 --> 00:58:11.960]   There's a scene in Westworld from the last season. Westworld's coming back tonight, so I'm
[00:58:11.960 --> 00:58:17.080]   trying to figure out what's that tonight? Tonight. Yeah. But season three was such a
[00:58:17.080 --> 00:58:22.040]   model. I had no idea what happened. So I'm trying to rewatch it. And in the very first episode of
[00:58:22.040 --> 00:58:29.160]   last season, Aaron Paul, who's a character in it, is talking, has been turned, being turned down
[00:58:29.160 --> 00:58:34.360]   for a job. Hi, I'm sorry. We just don't have anything right for you. And they have a conversation.
[00:58:34.360 --> 00:58:39.880]   And at some point, Aaron says, are you a robot? Because he's becoming, he's like, I don't think
[00:58:39.880 --> 00:58:44.440]   this is a human I'm talking to. He said, because he starts begging. Can you know, I can't, if there
[00:58:44.440 --> 00:58:49.400]   anything I can do, I really need this job. And the thing is pretty non-responsive. So finally,
[00:58:49.400 --> 00:58:55.240]   says, are you a robot? And instead of saying yes or no, it says, I can help you with many things.
[00:58:55.240 --> 00:59:01.640]   I am your representative for this corporation, just like Google saying, I'm not going to say
[00:59:01.640 --> 00:59:08.520]   I'm a robot. But I'm not going to do that with spam calls anyway, because I get a lot of spam
[00:59:08.520 --> 00:59:14.760]   calls. And I'm a journalist, I have to answer calls. And you get a recorded message from someone
[00:59:14.760 --> 00:59:21.720]   who says, Hi, I'm Joe from the American Military Whatever Association. And your first question
[00:59:21.720 --> 00:59:27.400]   used to be, are you a robot? And they've got wise to that now. It's like, no, I'm not a robot.
[00:59:27.400 --> 00:59:35.080]   But if you say, do you love me? Or, you know, what is two plus three? Yeah, then that foxes them
[00:59:35.080 --> 00:59:42.280]   completely. You should make you identify trains, cars and light, your traffic lights. And then
[00:59:42.280 --> 00:59:49.320]   maybe we'd know if they were robots or not. Did you guys? Sorry, go on. Sorry. Go ahead, Flo.
[00:59:49.320 --> 00:59:53.080]   Okay, this is going to be an absolute digression. Ian's going to be like, you interrupted me for
[00:59:53.080 --> 00:59:58.040]   this. Did you guys see the Paris Hilton deep fake that she made with her and Tom Cruise dancing?
[00:59:58.040 --> 01:00:04.840]   No, okay, is it the real Tom Cruise or is it fake Tom Cruise? It's fake Tom Cruise, but real
[01:00:04.840 --> 01:00:09.640]   Paris Hilton. I was going to say, yes. And now I love with a woman, but no, sorry. Yes.
[01:00:09.640 --> 01:00:16.040]   The Tom Cruise. No, no. You knew it wasn't him because, and I love short men. So I'm married to
[01:00:16.040 --> 01:00:19.960]   one, but this person was Paris Hilton's height. And we know that she's very tall.
[01:00:19.960 --> 01:00:26.840]   So you can tell, but it's such a good deep fake that I was. I will play it. I'll probably
[01:00:26.840 --> 01:00:32.680]   get in trouble for it because every time I did. Yeah, every time I play video, I get taken down.
[01:00:32.680 --> 01:00:37.960]   We're still we have an episode for three or four weeks ago where I played the video of the queen
[01:00:37.960 --> 01:00:44.680]   having tea with patting into the bear and pat the pattings in the bear people took the show down.
[01:00:45.240 --> 01:00:51.480]   It's still this. That's okay. Leo, because under the current British immigration rules,
[01:00:51.480 --> 01:00:56.600]   pattington is being deported as an illegal Chilean immigrant. Sorry, Peruvian immigrant.
[01:00:56.600 --> 01:01:02.520]   And he we sent to Rwanda for processing and then back to his home country. So you
[01:01:02.520 --> 01:01:07.720]   Wow. And on that, wow, government are bastards. I just figured
[01:01:09.960 --> 01:01:15.400]   that since every news organization in the world, wait a minute, this is her having cereal
[01:01:15.400 --> 01:01:20.200]   with Tom Cruise. Is that another one? There's there's that's another one. Oh my God. That's
[01:01:20.200 --> 01:01:25.640]   another one. Let's see. This is Paris having cereal with Tom Cruise. He's sitting down. So we
[01:01:25.640 --> 01:01:32.600]   can't tell if he's short. Just after one day, it just cereal hits the spot. Totally, guys.
[01:01:32.600 --> 01:01:38.920]   Totally not him. Oh, God. It looks just like it looks just like him.
[01:01:39.560 --> 01:01:44.120]   Oh, this is just but this is going to be a massive problem over the next five,
[01:01:44.120 --> 01:01:54.280]   10, 20 years, this is hot. This is spread around. So so this is a guy named Miles,
[01:01:54.280 --> 01:02:00.920]   who is a kind of Tom Cruise look alike, but they put his Tom Cruise face on top of him. So he does
[01:02:00.920 --> 01:02:05.720]   the smile. He does a lot of the gestures. Is that but is that the real person? I'm sorry. I'm
[01:02:05.720 --> 01:02:10.680]   still not. Yeah, I believe that's her. I believe she signed up for this grift.
[01:02:10.680 --> 01:02:18.920]   Wow. This guy is actually quite good. His tic-tac is called the Tom Cruise. There's the dancing one.
[01:02:18.920 --> 01:02:21.400]   Which one? This one? The first one where he yeah, that one.
[01:02:21.400 --> 01:02:25.880]   I don't want to be late to this premiere. We got to go. He doesn't sound exactly like Tom Cruise.
[01:02:25.880 --> 01:02:27.000]   You should always run right.
[01:02:27.000 --> 01:02:34.040]   Absolutely. It's your night. You're so absolutely beautiful. See, he's right. He's too tall. He should
[01:02:34.040 --> 01:02:37.960]   be shorter. He'll get a love short band, but not Tom Cruise.
[01:02:37.960 --> 01:02:45.160]   I think we're really in a wilder world. Yeah, this really creeped me out.
[01:02:45.160 --> 01:02:49.960]   I think most people. That's really nasty.
[01:02:49.960 --> 01:03:01.080]   It reminds me of the the second Iraq war, where Saddam Hussein had all these lucky likeies.
[01:03:01.080 --> 01:03:04.680]   Oh, yeah. That's right. Protect him for assassination.
[01:03:04.680 --> 01:03:09.480]   Yeah. There's a famous joke that they they were called into one room and said,
[01:03:09.480 --> 01:03:15.000]   Saddam Hussein has had survived an assassination attempt and they all applauded. Bad news is,
[01:03:15.000 --> 01:03:20.760]   he's lost an arm. So, you know, you've all got to go into the surgery room now.
[01:03:20.760 --> 01:03:26.680]   This is really deep. Here's another one. This has 13 million views of tic-tac,
[01:03:26.680 --> 01:03:32.920]   of deep fake Tom Cruise jumping over Jordan Peele, who's, I believe real, but what,
[01:03:32.920 --> 01:03:34.600]   how do you know anymore?
[01:03:34.600 --> 01:03:37.800]   Just want you to jump into a shoe.
[01:03:37.800 --> 01:03:39.320]   I'm glad you're not hanging by.
[01:03:39.320 --> 01:03:40.200]   I'm glad you're not hanging by.
[01:03:40.200 --> 01:03:42.600]   Doesn't that look so much like Tom Cruise?
[01:03:42.600 --> 01:03:47.800]   I am so glad for this though, because let me tell you, I am just mourning
[01:03:47.800 --> 01:03:49.320]   I'm sorry. That's not Jordan Peele.
[01:03:49.320 --> 01:03:50.280]   That's Scientology.
[01:03:50.280 --> 01:03:54.280]   That's Keegan Michael Key. Sorry. I refuse the two. Yes. Go ahead.
[01:03:54.280 --> 01:03:59.480]   I'm just so sad that we lost Tom Cruise to Scientology that I feel like there should be a
[01:03:59.480 --> 01:04:01.640]   a non- Scientology Tom Cruise.
[01:04:01.640 --> 01:04:02.760]   Yeah. Yeah.
[01:04:02.760 --> 01:04:03.400]   Yeah.
[01:04:03.400 --> 01:04:05.400]   Like the actual Hollywood, you know,
[01:04:05.400 --> 01:04:06.600]   "Chum, let's just..."
[01:04:06.600 --> 01:04:10.440]   Because that's the Tom Cruise we wish we had, but he's not allowed to do that.
[01:04:10.440 --> 01:04:17.080]   Oh, like for the production of this one though, I mean, I hear a lot of the worry that this is
[01:04:17.080 --> 01:04:23.880]   going to like end up, I think if you looked at the actual production that these videos take,
[01:04:24.360 --> 01:04:26.840]   it's not like five minutes with the phone.
[01:04:26.840 --> 01:04:30.440]   Like these have extremely elaborate setup times.
[01:04:30.440 --> 01:04:35.720]   You know, additionally, you've got someone that's really, really, really darn good at doing
[01:04:35.720 --> 01:04:38.360]   a Tom Cruise like impression. Of course.
[01:04:38.360 --> 01:04:46.440]   So, they gave him a head start, but it's only a few years before you could have deep fake grandma
[01:04:46.440 --> 01:04:47.320]   in your TikTok.
[01:04:48.200 --> 01:04:54.760]   Well, I think more specifically, I mean, disinformation targeted at women. Imagine how we see it.
[01:04:54.760 --> 01:04:55.720]   Oh, yeah, that's right.
[01:04:55.720 --> 01:05:01.240]   One of the things I uncovered during Geemer was there was actually an operation setup
[01:05:01.240 --> 01:05:08.040]   that you could actually buy destroying your ex's Google results, right?
[01:05:08.040 --> 01:05:12.360]   And flood it with like, you know, allegations of porn or whatever.
[01:05:12.360 --> 01:05:16.360]   I imagine this technology being used for that because in that case,
[01:05:16.360 --> 01:05:19.400]   it wouldn't even have to be that convincing.
[01:05:19.400 --> 01:05:23.480]   You know, my initial thought was, well, then nobody will ever run for office again,
[01:05:23.480 --> 01:05:27.720]   because, you know, immediately deep fakes, you know, if you doing something reprehensible,
[01:05:27.720 --> 01:05:30.200]   we'll show up. And then I just thought about it a little bit longer.
[01:05:30.200 --> 01:05:33.080]   And I realized, oh, no, that's just going to open the floodgates because
[01:05:33.080 --> 01:05:38.440]   now you can do anything you want and say, oh, it's a deep fake, right?
[01:05:38.440 --> 01:05:41.560]   Or maybe the only people will run for office or people don't care.
[01:05:42.440 --> 01:05:47.800]   I was watching an old documentary and the CIA came up with an idea to create a
[01:05:47.800 --> 01:05:53.560]   porn film with Saddam Hussein and a young boy.
[01:05:53.560 --> 01:05:59.160]   And that this would apparently demoralize the right position.
[01:05:59.160 --> 01:06:01.560]   Yeah. And the rest of it. And it's like, well, first off,
[01:06:01.560 --> 01:06:07.000]   that's showing a deep lack of knowledge of human psychology, but also,
[01:06:07.000 --> 01:06:12.200]   really, when it comes to deep fakes, this is what's going to come through the line.
[01:06:12.680 --> 01:06:16.840]   You're going to get partitioned saying things they haven't said, or, you know,
[01:06:16.840 --> 01:06:19.720]   people would like them to say. It would matter.
[01:06:19.720 --> 01:06:23.640]   And we would have a trust problem on social, you know, on internet media.
[01:06:23.640 --> 01:06:28.360]   And this is only going to make it catastrophically worse.
[01:06:28.360 --> 01:06:29.800]   There's a turning point.
[01:06:29.800 --> 01:06:31.160]   You thought fake news is bad.
[01:06:31.160 --> 01:06:32.600]   Fake video is going to be.
[01:06:32.600 --> 01:06:33.720]   Fake fake fake.
[01:06:33.720 --> 01:06:37.640]   There's a turning point in Neil Stevenson's latest book, which we did a book club called
[01:06:37.640 --> 01:06:38.520]   Termination Shock.
[01:06:38.520 --> 01:06:39.960]   Oh, it's so good.
[01:06:39.960 --> 01:06:41.560]   Isn't it good? And the queen of.
[01:06:41.560 --> 01:06:44.520]   Endings right yet, but no, no, no, no.
[01:06:44.520 --> 01:06:44.760]   Something.
[01:06:44.760 --> 01:06:48.840]   You know what's good about this book is there's no attempt even to do it.
[01:06:48.840 --> 01:06:54.840]   He's not trying to wrap it up. It's a slice of life, right?
[01:06:54.840 --> 01:06:57.080]   It has no beginning, middle or end.
[01:06:57.080 --> 01:06:58.920]   It's just like life.
[01:06:58.920 --> 01:06:59.880]   It's just normal.
[01:06:59.880 --> 01:07:05.480]   But the queen of the Netherlands at one point, who has been very careful not to influence this,
[01:07:07.000 --> 01:07:13.240]   the constitutional monarchy. So she doesn't want to influence in any way, national politics.
[01:07:13.240 --> 01:07:15.320]   So she's like any good queen.
[01:07:15.320 --> 01:07:18.200]   She doesn't make political statements.
[01:07:18.200 --> 01:07:23.160]   But a deep fake comes out in which she says, you know, look at, I don't want to say this.
[01:07:23.160 --> 01:07:26.760]   And I know we have a longstanding history of not getting involved politics,
[01:07:26.760 --> 01:07:29.160]   but we've really got to do some geoengineering.
[01:07:29.160 --> 01:07:35.080]   And I'm all in favor and the government falls as a result.
[01:07:35.080 --> 01:07:39.880]   And she goes, so she immediately rushes to the palace to make a denial video saying,
[01:07:39.880 --> 01:07:44.520]   look, that wasn't me. It was a deep fake before she shows, she tapes it before she can be released.
[01:07:44.520 --> 01:07:52.760]   A second deep fake comes out completely disarming any real denial possibility.
[01:07:52.760 --> 01:07:57.640]   In other words, gaming it in such a way that there's literally no denying it.
[01:07:57.640 --> 01:07:59.320]   She has to abdicate as a result.
[01:08:00.440 --> 01:08:07.160]   Because you filled the zone with BS, you don't know what's real and now it doesn't matter anymore.
[01:08:07.160 --> 01:08:09.400]   And that's where we're headed, I think absolutely.
[01:08:09.400 --> 01:08:13.400]   Amazon has announced its first fully autonomous mobile warehouse robot.
[01:08:13.400 --> 01:08:17.720]   You want to unionize? I got your union right here.
[01:08:17.720 --> 01:08:25.880]   But unfortunately, right now all I can do is pick things up, put them down and move them
[01:08:25.880 --> 01:08:26.360]   around.
[01:08:26.360 --> 01:08:31.640]   Have you seen the video of it in action? Because basically it looks like a rule about it.
[01:08:31.640 --> 01:08:32.840]   It lifted up.
[01:08:32.840 --> 01:08:33.320]   Yeah.
[01:08:33.320 --> 01:08:38.360]   And I looked at that and I was like, if you're moving it more than a mile an hour,
[01:08:38.360 --> 01:08:40.760]   then that thing's going to topple over.
[01:08:40.760 --> 01:08:44.920]   Because it's a robot, so it's not going to screw up.
[01:08:44.920 --> 01:08:46.040]   That knows what it's doing.
[01:08:46.040 --> 01:08:46.920]   And I get it.
[01:08:46.920 --> 01:08:49.560]   It's Amazon. They don't want to actually pay workers.
[01:08:49.560 --> 01:08:50.760]   Anything.
[01:08:50.760 --> 01:08:50.760]   Anything.
[01:08:50.760 --> 01:08:52.840]   Anything not to pay workers.
[01:08:52.840 --> 01:08:54.360]   Oh, jeez, you're right.
[01:08:54.360 --> 01:08:56.120]   I have to say, my friend-
[01:08:56.120 --> 01:08:56.760]   Look at that.
[01:08:56.760 --> 01:08:57.800]   It's so unstable.
[01:08:57.800 --> 01:08:58.760]   Sorry.
[01:08:58.760 --> 01:09:01.240]   My friend Christopher Mims, he's a-
[01:09:01.240 --> 01:09:02.600]   I love street journal reporter.
[01:09:02.600 --> 01:09:04.280]   Yeah, fantastic.
[01:09:04.280 --> 01:09:04.760]   Yeah.
[01:09:04.760 --> 01:09:05.640]   Fantastic guy.
[01:09:05.640 --> 01:09:08.600]   He writes about supply chains.
[01:09:08.600 --> 01:09:17.720]   And I used to have this opinion as well that Amazon's robots were going to destroy their warehouses
[01:09:17.720 --> 01:09:20.280]   or destroy jobs at their warehouses.
[01:09:20.280 --> 01:09:22.600]   He's really changed my mind on that.
[01:09:22.600 --> 01:09:27.000]   And he has actually gone to these warehouses.
[01:09:27.000 --> 01:09:32.680]   He literally, in his last book, tracked a USB key as it was sent around the world.
[01:09:32.680 --> 01:09:35.720]   It's fascinating journalism.
[01:09:35.720 --> 01:09:40.600]   But the conclusion he came to is that these robots are really built for
[01:09:40.600 --> 01:09:44.520]   very, very, very specialized tasks.
[01:09:44.520 --> 01:09:49.240]   And that we're so far away from a point where humans are completely taken out of the loop.
[01:09:49.240 --> 01:09:54.280]   It's just almost theoretical at this point that it's really, these robots are built to
[01:09:54.280 --> 01:09:57.240]   basically aid human labor rather than replace it.
[01:09:57.240 --> 01:10:00.360]   In an ideal world, these robots should do-
[01:10:00.360 --> 01:10:02.920]   I mean, no human should have to do a lot of these jobs, right?
[01:10:02.920 --> 01:10:03.400]   Yeah.
[01:10:03.400 --> 01:10:08.120]   These are mindless soulless jobs that people do just because they have to pay the rent.
[01:10:08.120 --> 01:10:12.360]   In a perfect world, humans wouldn't have to do these jobs.
[01:10:12.360 --> 01:10:13.320]   Robots would do them.
[01:10:13.320 --> 01:10:16.760]   But here's- this is where the imperfection happens.
[01:10:17.480 --> 01:10:20.600]   And instead of just merely enriching Jeff Bezos,
[01:10:20.600 --> 01:10:28.680]   that there would be such a surplus created by these highly productive machines
[01:10:28.680 --> 01:10:32.200]   that people wouldn't have to work or could they could work less.
[01:10:32.200 --> 01:10:35.480]   Or in other words, get rid of the soul crushing jobs.
[01:10:35.480 --> 01:10:39.080]   As long as people can eat, that's fine.
[01:10:39.080 --> 01:10:40.280]   Oh, really?
[01:10:40.280 --> 01:10:41.480]   We told fashion-
[01:10:41.480 --> 01:10:41.960]   I know.
[01:10:41.960 --> 01:10:42.840]   It just enrud-
[01:10:42.840 --> 01:10:43.320]   That's really-
[01:10:43.320 --> 01:10:44.840]   It rips all the garks.
[01:10:45.560 --> 01:10:47.560]   And they don't care that- great.
[01:10:47.560 --> 01:10:50.120]   Don't have to wear- don't have to have unions anymore.
[01:10:50.120 --> 01:10:50.840]   Thank goodness.
[01:10:50.840 --> 01:10:52.520]   We can just have robots do it.
[01:10:52.520 --> 01:10:55.080]   Well, I mean, Bezos has done this before.
[01:10:55.080 --> 01:10:58.920]   He got an award from a German magazine,
[01:10:58.920 --> 01:11:00.040]   which he just happens to own,
[01:11:00.040 --> 01:11:02.920]   "Stake in for being the best Adam entrepreneur in the world."
[01:11:02.920 --> 01:11:03.480]   That's hysterical.
[01:11:03.480 --> 01:11:06.760]   He owns a part of the magazine.
[01:11:06.760 --> 01:11:07.800]   That's hysterical.
[01:11:07.800 --> 01:11:09.480]   Oh, yeah, no, no.
[01:11:09.480 --> 01:11:11.320]   He owned it and they gave me this one.
[01:11:11.320 --> 01:11:13.160]   And he was like, well, if you look at German,
[01:11:13.160 --> 01:11:18.040]   Amazon workers, they owned above national levels.
[01:11:18.040 --> 01:11:21.320]   And he neglected to point out that it took three strikes
[01:11:21.320 --> 01:11:23.080]   and good union laws to do that.
[01:11:23.080 --> 01:11:23.560]   That's right.
[01:11:23.560 --> 01:11:29.960]   In America, however, Amazon warehouses cut the standard wage
[01:11:29.960 --> 01:11:31.880]   for compared to competitors.
[01:11:31.880 --> 01:11:35.240]   As we saw this week,
[01:11:35.240 --> 01:11:39.160]   are burning through workers faster than they can actually recruit them.
[01:11:39.160 --> 01:11:42.760]   So this kind of robot is definitely needed for Amazon.
[01:11:43.240 --> 01:11:45.320]   Because they treat people like that.
[01:11:45.320 --> 01:11:49.800]   I mean, I've never yet met someone who worked for Amazon,
[01:11:49.800 --> 01:11:52.760]   who's kind of like, yeah, it's a great culture.
[01:11:52.760 --> 01:11:54.920]   Everyone cares for each other.
[01:11:54.920 --> 01:11:55.160]   Yeah.
[01:11:55.160 --> 01:11:59.800]   This is purely, let's economize costs.
[01:11:59.800 --> 01:12:05.640]   But the commercials, they pay 20 weeks of leave.
[01:12:05.640 --> 01:12:08.040]   Pay for some dollars an hour.
[01:12:08.040 --> 01:12:10.440]   People seem so happy.
[01:12:10.440 --> 01:12:13.080]   You know, the worst commercials, almost as bad as those,
[01:12:13.080 --> 01:12:15.480]   actually maybe a little worse, and those are the meta commercials.
[01:12:15.480 --> 01:12:18.440]   This is the new thing for big tech.
[01:12:18.440 --> 01:12:21.720]   Google and Microsoft and Apple have so far avoided doing this,
[01:12:21.720 --> 01:12:25.640]   where they're really the self-serving political pieces essentially.
[01:12:25.640 --> 01:12:25.800]   Yeah.
[01:12:25.800 --> 01:12:27.640]   Google does it too.
[01:12:27.640 --> 01:12:28.360]   Does Google do it?
[01:12:28.360 --> 01:12:28.840]   Yeah.
[01:12:28.840 --> 01:12:29.320]   Yeah.
[01:12:29.320 --> 01:12:30.200]   Yes, they do.
[01:12:30.200 --> 01:12:34.280]   Because they know the search engine is their big money maker.
[01:12:34.280 --> 01:12:37.320]   And so there was like, the search engine is what gets your life going.
[01:12:37.320 --> 01:12:42.120]   Which is great.
[01:12:42.120 --> 01:12:42.840]   True for me.
[01:12:42.840 --> 01:12:46.840]   We dug into the Amazon, when the Amazon announced,
[01:12:46.840 --> 01:12:49.960]   yeah, we're sponsoring education for our workers,
[01:12:49.960 --> 01:12:51.240]   and we dug into it.
[01:12:51.240 --> 01:12:55.160]   And it's like, yeah, they're willing to pay five grand
[01:12:55.160 --> 01:12:58.920]   per year to educate people.
[01:12:58.920 --> 01:13:01.480]   That might get you two or three textbooks.
[01:13:01.480 --> 01:13:02.760]   Forget tuition fees.
[01:13:02.760 --> 01:13:03.400]   Forget.
[01:13:03.400 --> 01:13:04.040]   Yeah.
[01:13:04.040 --> 01:13:04.600]   Exactly.
[01:13:04.600 --> 01:13:05.080]   Have some estimate.
[01:13:05.080 --> 01:13:06.440]   You know, it was just kind of like,
[01:13:06.440 --> 01:13:09.560]   yeah, we're sponsoring education for our workers.
[01:13:09.560 --> 01:13:10.760]   Yeah.
[01:13:10.760 --> 01:13:11.080]   Right.
[01:13:11.080 --> 01:13:14.360]   And in the meantime, they cripple themselves working for you.
[01:13:14.360 --> 01:13:18.440]   Maybe they're building a time machine so you can go back to the 90s,
[01:13:18.440 --> 01:13:21.240]   where tuition was $5,000.
[01:13:21.240 --> 01:13:21.240]   Yeah.
[01:13:21.240 --> 01:13:24.440]   When I went to Yale, it was $7,000 a year.
[01:13:24.440 --> 01:13:25.080]   It was nothing.
[01:13:25.080 --> 01:13:26.040]   It was...
[01:13:26.040 --> 01:13:27.800]   Americans don't believe me on this.
[01:13:27.800 --> 01:13:28.600]   Back in the last six.
[01:13:28.600 --> 01:13:31.400]   I was actually paid to go to university.
[01:13:31.400 --> 01:13:34.040]   The UK government paid my tuition.
[01:13:34.040 --> 01:13:37.560]   Because I was raised in a single parent family.
[01:13:37.560 --> 01:13:40.600]   The UK government paid my tuition.
[01:13:40.600 --> 01:13:43.320]   They gave me $2,000 a year to live on.
[01:13:43.320 --> 01:13:45.480]   It is subsidized education.
[01:13:45.480 --> 01:13:49.880]   And yeah, Americans don't believe that actually happened.
[01:13:49.880 --> 01:13:52.440]   And trust me, in Europe, it happens.
[01:13:52.440 --> 01:13:56.600]   This is not the best way to do this stuff.
[01:13:56.600 --> 01:13:57.480]   Now, I have to say...
[01:13:57.480 --> 01:13:59.080]   Thanks for rubbing it in our face.
[01:13:59.080 --> 01:14:00.200]   I really appreciate this.
[01:14:00.200 --> 01:14:01.880]   This is a great week for that.
[01:14:01.880 --> 01:14:03.400]   How are your...
[01:14:03.400 --> 01:14:04.120]   How are your buy?
[01:14:04.120 --> 01:14:05.640]   How's your buy on the autonomy?
[01:14:05.640 --> 01:14:06.520]   Where are you coming from?
[01:14:06.520 --> 01:14:07.080]   Yeah.
[01:14:07.080 --> 01:14:07.720]   How's that going?
[01:14:07.720 --> 01:14:08.680]   How's that going?
[01:14:08.680 --> 01:14:11.800]   But this is a stupid thing because, you know,
[01:14:11.800 --> 01:14:18.200]   Steve Jobs was able to go to UC Berkeley courses for a pittance.
[01:14:18.200 --> 01:14:22.520]   My brother-in-law was able to fund his New York University education
[01:14:22.520 --> 01:14:24.680]   by being a taxi driver part-time.
[01:14:24.680 --> 01:14:25.000]   Yeah.
[01:14:25.000 --> 01:14:26.760]   You can't do that anymore.
[01:14:26.760 --> 01:14:31.160]   That has been shut out because, you know, prices have gone up.
[01:14:31.160 --> 01:14:32.360]   It's not even education.
[01:14:32.360 --> 01:14:33.960]   It's ridiculous.
[01:14:33.960 --> 01:14:36.280]   It's the rent of education.
[01:14:36.280 --> 01:14:37.240]   It's the rent.
[01:14:37.240 --> 01:14:43.320]   You cannot live on minimum wage in any part of this country.
[01:14:43.320 --> 01:14:46.120]   But if we're going to progress as a society,
[01:14:46.120 --> 01:14:48.920]   we need to educate people cheaply.
[01:14:48.920 --> 01:14:52.680]   And at the moment, this is education is the privilege of the rich.
[01:14:52.680 --> 01:14:56.920]   The cynic in me says that people like Jeff Bezos
[01:14:56.920 --> 01:14:58.760]   don't want educated people.
[01:14:58.760 --> 01:15:00.840]   They want all you women to have more babies.
[01:15:01.320 --> 01:15:04.440]   And they want those people to go to work in those warehouses.
[01:15:04.440 --> 01:15:07.320]   By the way, Asmatic in our chatroom has friends who work in Amazon
[01:15:07.320 --> 01:15:07.800]   warehouses.
[01:15:07.800 --> 01:15:11.480]   They say they love the robots probably because they're doing heavy labor
[01:15:11.480 --> 01:15:13.240]   that they don't want to do.
[01:15:13.240 --> 01:15:13.720]   Yeah.
[01:15:13.720 --> 01:15:14.280]   Yeah.
[01:15:14.280 --> 01:15:14.840]   Yeah.
[01:15:14.840 --> 01:15:15.400]   Yeah.
[01:15:15.400 --> 01:15:18.760]   But I have to say in the long run, I don't care what Christopher
[01:15:18.760 --> 01:15:23.080]   M says in the long run, the plan is to replace you with those robots, right?
[01:15:23.080 --> 01:15:24.360]   Yeah.
[01:15:24.360 --> 01:15:27.800]   I mean, they're literally running out of people that can do this job.
[01:15:27.800 --> 01:15:29.480]   That was the most wild story
[01:15:30.280 --> 01:15:31.480]   of the week. I'm going to take a break.
[01:15:31.480 --> 01:15:33.480]   But that was the most wild story of the week.
[01:15:33.480 --> 01:15:36.920]   I couldn't believe it, but we'll tell you in just a second.
[01:15:36.920 --> 01:15:37.480]   Great.
[01:15:37.480 --> 01:15:39.160]   So nice to have Ian Thompson here.
[01:15:39.160 --> 01:15:40.440]   Register.com.
[01:15:40.440 --> 01:15:42.200]   He's the news editor there.
[01:15:42.200 --> 01:15:45.800]   Last minute we asked Brianna Wu to join us.
[01:15:45.800 --> 01:15:47.560]   It is wonderful to have you.
[01:15:47.560 --> 01:15:51.720]   Thank Frank for getting these in there.
[01:15:51.720 --> 01:15:55.240]   I'm super push poster in the background.
[01:15:55.240 --> 01:15:55.800]   Yes.
[01:15:55.800 --> 01:15:56.600]   Thank you.
[01:15:56.600 --> 01:15:57.080]   Thank you.
[01:15:57.080 --> 01:15:57.960]   I actually own that.
[01:15:57.960 --> 01:15:59.080]   Is that your boxer?
[01:15:59.080 --> 01:15:59.560]   Wow.
[01:15:59.560 --> 01:16:01.240]   No, that's that's my 911.
[01:16:01.240 --> 01:16:03.720]   That's an air-cooled 911 from 1986.
[01:16:03.720 --> 01:16:06.120]   Oh, does it sound like a Volkswagen?
[01:16:06.120 --> 01:16:07.720]   No.
[01:16:07.720 --> 01:16:08.200]   No.
[01:16:08.200 --> 01:16:08.680]   No.
[01:16:08.680 --> 01:16:09.720]   [laughter]
[01:16:09.720 --> 01:16:09.720]   Ouch.
[01:16:09.720 --> 01:16:10.680]   Ouch.
[01:16:10.680 --> 01:16:11.160]   Ouch.
[01:16:11.160 --> 01:16:13.640]   Well, Eric, you're on me.
[01:16:13.640 --> 01:16:16.760]   Oh, oh, yeah.
[01:16:16.760 --> 01:16:18.600]   That's cool.
[01:16:18.600 --> 01:16:19.160]   Oh, come on.
[01:16:19.160 --> 01:16:21.080]   But Leo, do you want to race Brianna?
[01:16:21.080 --> 01:16:21.640]   No.
[01:16:21.640 --> 01:16:22.120]   Then.
[01:16:22.120 --> 01:16:25.400]   Well, he's got to watch the vehicle.
[01:16:25.400 --> 01:16:26.680]   I have some good torque.
[01:16:26.680 --> 01:16:28.360]   I have some very good torque right from the...
[01:16:28.360 --> 01:16:30.120]   Honestly, I'd go into race Brianna,
[01:16:30.120 --> 01:16:32.200]   but just on a racetrack where we both...
[01:16:32.200 --> 01:16:32.600]   Yeah.
[01:16:32.600 --> 01:16:33.240]   Wouldn't that be fun?
[01:16:33.240 --> 01:16:33.800]   We'd kill ourselves.
[01:16:33.800 --> 01:16:34.360]   That's fine.
[01:16:34.360 --> 01:16:36.040]   That would be really fun.
[01:16:36.040 --> 01:16:39.320]   I'll bring my motorcycle and I will be able to beat you,
[01:16:39.320 --> 01:16:40.600]   even in any vehicle you have.
[01:16:40.600 --> 01:16:41.240]   Oh, really?
[01:16:41.240 --> 01:16:41.800]   Nice.
[01:16:41.800 --> 01:16:42.280]   Yeah.
[01:16:42.280 --> 01:16:44.840]   I wonder if we could rent out Sonoma Raceway and do a...
[01:16:44.840 --> 01:16:45.480]   We sure could.
[01:16:45.480 --> 01:16:46.200]   ...an Twitch race.
[01:16:46.200 --> 01:16:46.760]   Lisa.
[01:16:46.760 --> 01:16:47.960]   Lisa.
[01:16:47.960 --> 01:16:48.920]   I'm going to let Lisa race you.
[01:16:48.920 --> 01:16:50.040]   I would buy tires for that.
[01:16:50.040 --> 01:16:50.760]   Though, because I...
[01:16:50.760 --> 01:16:53.800]   She's a speed demon and I drive like an almighty.
[01:16:53.800 --> 01:16:54.840]   Ooh.
[01:16:54.840 --> 01:16:55.800]   I am an old man.
[01:16:56.280 --> 01:16:59.320]   Also with us from Gizmodo, Florence Ion.
[01:16:59.320 --> 01:17:01.800]   So nice to see you and Sailor Moon.
[01:17:01.800 --> 01:17:02.360]   I'm happy to be here.
[01:17:02.360 --> 01:17:03.320]   Yeah, we love having you here.
[01:17:03.320 --> 01:17:03.960]   Thank you.
[01:17:03.960 --> 01:17:06.600]   Also, a last minute call because I thought,
[01:17:06.600 --> 01:17:10.120]   you know, we really probably should have some women on the show
[01:17:10.120 --> 01:17:12.520]   because, you know, maybe, just maybe,
[01:17:12.520 --> 01:17:16.120]   our show today brought to you by Wealthfront.
[01:17:16.120 --> 01:17:19.080]   Stock trading can be a wild ride.
[01:17:19.080 --> 01:17:20.600]   I have this conversation with my kids
[01:17:20.600 --> 01:17:24.120]   because they're young and they love the idea of meme stocks
[01:17:24.120 --> 01:17:26.280]   and crypto currency.
[01:17:26.280 --> 01:17:28.200]   And, you know, they see people around them
[01:17:28.200 --> 01:17:31.400]   seemingly getting very rich
[01:17:31.400 --> 01:17:33.960]   because nobody mentions the fact that their crypto investments
[01:17:33.960 --> 01:17:35.400]   just dropped 40%.
[01:17:35.400 --> 01:17:37.800]   They just talk about when it's a good thing.
[01:17:37.800 --> 01:17:38.280]   It's a...
[01:17:38.280 --> 01:17:40.600]   It's a stock trading crypto investing.
[01:17:40.600 --> 01:17:41.880]   It could be a wild ride.
[01:17:41.880 --> 01:17:44.760]   I would submit and I tell my kids,
[01:17:44.760 --> 01:17:46.200]   this is the thrill of risking it all
[01:17:46.200 --> 01:17:47.960]   is best enjoyed in moderation.
[01:17:47.960 --> 01:17:50.680]   Like, it's like casino gambling
[01:17:50.680 --> 01:17:52.600]   or eating questionable street food.
[01:17:53.240 --> 01:17:54.920]   Fun, but careful.
[01:17:54.920 --> 01:17:56.440]   If you're playing the market,
[01:17:56.440 --> 01:17:57.240]   I hope for your sake,
[01:17:57.240 --> 01:17:59.400]   you're stashing some safer money
[01:17:59.400 --> 01:18:01.480]   in a place like Wealthfront.
[01:18:01.480 --> 01:18:01.960]   It's taught...
[01:18:01.960 --> 01:18:04.680]   We're talking about building your wealth over time here,
[01:18:04.680 --> 01:18:09.000]   building it up for buying that first house or first car
[01:18:09.000 --> 01:18:11.240]   putting your kids through school,
[01:18:11.240 --> 01:18:13.080]   preparing for your retirement.
[01:18:13.080 --> 01:18:13.800]   You might say,
[01:18:13.800 --> 01:18:16.040]   day trading stocks are going to be the secret to my success,
[01:18:16.040 --> 01:18:18.440]   but Wealthfront has a lot of data to show that
[01:18:18.440 --> 01:18:22.040]   time in market almost always beats timing
[01:18:22.760 --> 01:18:23.640]   the market.
[01:18:23.640 --> 01:18:25.480]   They're globally diversified portfolios,
[01:18:25.480 --> 01:18:28.040]   automatically optimized to hit the goals you set,
[01:18:28.040 --> 01:18:30.360]   the risk level you choose.
[01:18:30.360 --> 01:18:33.480]   Without a lot of attention on your part,
[01:18:33.480 --> 01:18:35.000]   they automatically rebalance.
[01:18:35.000 --> 01:18:37.080]   They do things like tax loss harvesting.
[01:18:37.080 --> 01:18:39.800]   In fact, they actually invented the software for it.
[01:18:39.800 --> 01:18:41.880]   They give you automatic tax breaks,
[01:18:41.880 --> 01:18:44.040]   which when market goes down,
[01:18:44.040 --> 01:18:47.800]   you still can boost your returns with these tax breaks.
[01:18:47.800 --> 01:18:48.600]   It's really great.
[01:18:48.600 --> 01:18:50.360]   You can also personalize your portfolio.
[01:18:50.360 --> 01:18:51.160]   This is something new.
[01:18:52.200 --> 01:18:53.640]   They realize people...
[01:18:53.640 --> 01:18:54.840]   They like set it and forget it,
[01:18:54.840 --> 01:18:56.040]   but sometimes they want some input,
[01:18:56.040 --> 01:18:58.040]   so you can choose from health care funds,
[01:18:58.040 --> 01:18:58.760]   cannabis funds.
[01:18:58.760 --> 01:19:00.280]   Yes, crypto funds,
[01:19:00.280 --> 01:19:03.000]   all handpicked by Wealthfront's financial experts,
[01:19:03.000 --> 01:19:05.720]   social responsibility, clean energy.
[01:19:05.720 --> 01:19:07.720]   It works so well.
[01:19:07.720 --> 01:19:12.120]   Wealthfront is trusted with more than $27 billion in assets.
[01:19:12.120 --> 01:19:13.800]   Nearly half a million people
[01:19:13.800 --> 01:19:16.200]   are using Wealthfront to build their wealth.
[01:19:16.200 --> 01:19:18.360]   Investopedia named Wealthfront,
[01:19:18.360 --> 01:19:21.640]   the best robo advisor for 2022.
[01:19:22.520 --> 01:19:24.840]   You'll be amazed when you get to my age and go,
[01:19:24.840 --> 01:19:26.440]   "Wow, you could say thank you, Leo,
[01:19:26.440 --> 01:19:27.800]   to start building your wealth
[01:19:27.800 --> 01:19:30.840]   and get your first $5,000 managed for free for life."
[01:19:30.840 --> 01:19:33.800]   Go to wealthfront.com/twit.
[01:19:33.800 --> 01:19:40.440]   W-E-A-L-T-H, wealthfrontfr-o-n-t.com/twit.
[01:19:40.440 --> 01:19:44.600]   Start building your wealth wealthfront.com/twit,
[01:19:44.600 --> 01:19:46.600]   especially if you're younger.
[01:19:46.600 --> 01:19:48.040]   This is the time to do it.
[01:19:48.040 --> 01:19:51.320]   I know it's tough, but just a little bit each paycheck.
[01:19:51.320 --> 01:19:52.520]   You'll be amazed.
[01:19:52.520 --> 01:19:53.480]   You'll be amazed.
[01:19:53.480 --> 01:19:58.120]   Yeah, Amazon said,
[01:19:58.120 --> 01:20:00.120]   "Maybe this is why they're so big on robots."
[01:20:00.120 --> 01:20:02.120]   As we continue to grow,
[01:20:02.120 --> 01:20:04.040]   we're going to get to the point
[01:20:04.040 --> 01:20:06.040]   where there won't be enough people
[01:20:06.040 --> 01:20:09.400]   to staff our warehouses anymore.
[01:20:09.400 --> 01:20:14.520]   The company is because, partly because they chew through people,
[01:20:14.520 --> 01:20:16.440]   where they're automated hiring and firing,
[01:20:16.440 --> 01:20:19.240]   they actually chew through people,
[01:20:19.800 --> 01:20:23.400]   a leaked internal research report from mid-2021,
[01:20:23.400 --> 01:20:25.000]   recode got it,
[01:20:25.000 --> 01:20:26.600]   said that Amazon could run out of people
[01:20:26.600 --> 01:20:29.720]   to hire in U.S. warehouses by 2024.
[01:20:29.720 --> 01:20:31.880]   That's not far off.
[01:20:31.880 --> 01:20:33.400]   Nope.
[01:20:33.400 --> 01:20:34.200]   Not far off.
[01:20:34.200 --> 01:20:37.720]   Increasing wages could help.
[01:20:37.720 --> 01:20:40.040]   Certainly would expand the labor pool,
[01:20:40.040 --> 01:20:42.840]   but I think Amazon isn't too interested in that.
[01:20:42.840 --> 01:20:45.560]   Oh, increasing warehouse automation.
[01:20:45.560 --> 01:20:48.520]   Yeah, robots work cheap.
[01:20:49.640 --> 01:20:50.680]   24/7.
[01:20:50.680 --> 01:20:55.560]   Anyway, nothing more to say, except...
[01:20:55.560 --> 01:20:58.920]   Although I do, and I've told this story before,
[01:20:58.920 --> 01:21:01.960]   the phone company at the turn of the last century,
[01:21:01.960 --> 01:21:03.400]   in the early 1900s,
[01:21:03.400 --> 01:21:05.320]   some executive said,
[01:21:05.320 --> 01:21:07.880]   "If we keep growing at this rate,
[01:21:07.880 --> 01:21:10.920]   there won't be enough women in the U.S. to be operators.
[01:21:10.920 --> 01:21:12.920]   We're going to run out of women."
[01:21:12.920 --> 01:21:16.120]   I guess men couldn't do that job.
[01:21:19.480 --> 01:21:22.440]   They figured out a way to do it, automating, actually.
[01:21:22.440 --> 01:21:25.480]   And you didn't need operators anymore.
[01:21:25.480 --> 01:21:26.200]   You might be should still.
[01:21:26.200 --> 01:21:27.640]   Ah, you're back.
[01:21:27.640 --> 01:21:29.400]   Ah, okay.
[01:21:29.400 --> 01:21:33.720]   No, originally they hired men to do telephone-operated jobs,
[01:21:33.720 --> 01:21:35.560]   and they found they were really bad at it,
[01:21:35.560 --> 01:21:37.160]   so they got women in to do it.
[01:21:37.160 --> 01:21:38.360]   You did it.
[01:21:38.360 --> 01:21:39.320]   Do much more.
[01:21:39.320 --> 01:21:39.640]   No, no.
[01:21:39.640 --> 01:21:43.400]   When telephony came in,
[01:21:43.400 --> 01:21:47.960]   they took message boys who had been carrying time drums around.
[01:21:47.960 --> 01:21:49.720]   Call for a flop, my rest.
[01:21:49.720 --> 01:21:50.760]   And put them into cool things.
[01:21:50.760 --> 01:21:51.240]   Yeah.
[01:21:51.240 --> 01:21:55.800]   And then as the great hacker crackdown recounted,
[01:21:55.800 --> 01:21:59.640]   the boys were playing around with the system,
[01:21:59.640 --> 01:22:02.600]   and were basically causing havoc,
[01:22:02.600 --> 01:22:04.840]   they got women in who were far more responsible
[01:22:04.840 --> 01:22:06.200]   and got the job done properly.
[01:22:06.200 --> 01:22:10.920]   And yeah, so this was how the thing developed.
[01:22:10.920 --> 01:22:12.200]   Oh, I had no idea.
[01:22:12.200 --> 01:22:16.760]   We couldn't be trusted with the phones.
[01:22:16.760 --> 01:22:19.720]   Well, you don't trust teenage boys with a telephone connection.
[01:22:19.720 --> 01:22:20.360]   That's true.
[01:22:20.360 --> 01:22:23.080]   If anything over the last 100 years is taught us,
[01:22:23.080 --> 01:22:24.520]   that's one of the big ones.
[01:22:24.520 --> 01:22:26.520]   Wow.
[01:22:26.520 --> 01:22:29.640]   Elon Musk still trying to buy Twitter.
[01:22:29.640 --> 01:22:30.760]   That's it.
[01:22:30.760 --> 01:22:32.360]   Just that I tell you.
[01:22:32.360 --> 01:22:34.120]   Is that the update?
[01:22:34.120 --> 01:22:35.320]   Yeah, that's the update we got.
[01:22:35.320 --> 01:22:35.560]   Yeah.
[01:22:35.560 --> 01:22:37.800]   It seems to have fallen dormant.
[01:22:37.800 --> 01:22:38.760]   Still wants to buy it.
[01:22:38.760 --> 01:22:42.600]   He needs more data, says Gizmodo.
[01:22:42.600 --> 01:22:44.360]   Twitter gives it to him.
[01:22:44.360 --> 01:22:46.280]   He says, "It's still not enough."
[01:22:46.280 --> 01:22:47.640]   Twitter gave him the fire hose.
[01:22:47.640 --> 01:22:53.960]   His concern, he says, is there too many bots on Twitter?
[01:22:53.960 --> 01:22:56.120]   He's, this is a pump and dump scheme.
[01:22:56.120 --> 01:22:56.440]   Yeah.
[01:22:56.440 --> 01:22:59.400]   You know, it's basically, I bought a sha-
[01:22:59.400 --> 01:23:01.560]   I bought a shareholding in Twitter.
[01:23:01.560 --> 01:23:03.960]   Now I'm going to hype that up.
[01:23:03.960 --> 01:23:09.480]   I would like to see what shares he's sold since then, so I don't know,
[01:23:09.480 --> 01:23:11.160]   but the guys are fraudster.
[01:23:11.160 --> 01:23:15.400]   Well, I mean, I have to think if you're on the board,
[01:23:16.200 --> 01:23:20.440]   of Tesla, you might not be happy with your CEO saying,
[01:23:20.440 --> 01:23:23.880]   he's worried about keeping Tesla out of bankruptcy.
[01:23:23.880 --> 01:23:25.480]   That's the latest.
[01:23:25.480 --> 01:23:27.960]   You know, I've read-
[01:23:27.960 --> 01:23:28.520]   You know, I've read-
[01:23:28.520 --> 01:23:29.880]   I've read a bunch of amount of bitcoins.
[01:23:29.880 --> 01:23:31.400]   Sorry, Brianna, I'm sorry.
[01:23:31.400 --> 01:23:35.160]   I was just going to say, I read a book recently.
[01:23:35.160 --> 01:23:37.960]   It was called Ludicrous.
[01:23:37.960 --> 01:23:38.440]   Yes.
[01:23:38.440 --> 01:23:40.280]   Basically the history of Tesla.
[01:23:40.280 --> 01:23:40.680]   Great book.
[01:23:40.680 --> 01:23:42.840]   Excellent, excellent, excellent.
[01:23:42.840 --> 01:23:43.800]   10 out of 10 book.
[01:23:44.840 --> 01:23:49.240]   And you know, I had not really tuned into the Elon thing before
[01:23:49.240 --> 01:23:53.560]   Twitter because, you know, like I support EVs, but I don't own a Tesla.
[01:23:53.560 --> 01:23:59.320]   I'm not really a huge stakeholder in their success or failure.
[01:23:59.320 --> 01:24:03.240]   And really going through the entire history of the company.
[01:24:03.240 --> 01:24:05.400]   And you know, like some of it is-
[01:24:05.400 --> 01:24:11.320]   some of it admittedly, I think, is just the slight of hand that allows you to,
[01:24:11.320 --> 01:24:13.720]   you know, start a successful company, right?
[01:24:13.720 --> 01:24:19.000]   Like telling people you're going to have investors and then securing all the investors later.
[01:24:19.000 --> 01:24:20.600]   That's kind of low level stuff.
[01:24:20.600 --> 01:24:27.960]   But some of it, like there's very clearly a pattern with the Elon of beyond over promising
[01:24:27.960 --> 01:24:30.840]   and then just never ever ever delivery.
[01:24:30.840 --> 01:24:34.840]   A really good example, the Tesla Supercharger Network when it came out,
[01:24:34.840 --> 01:24:37.640]   they promised that not only was it going to be neutral,
[01:24:37.640 --> 01:24:41.080]   it was going to- because the solar panels are added to it,
[01:24:41.080 --> 01:24:44.040]   that was going to add back into the electrical grid.
[01:24:44.040 --> 01:24:45.000]   Never happened.
[01:24:45.000 --> 01:24:47.800]   And then it was battery swaps in Tesla.
[01:24:47.800 --> 01:24:51.880]   You know, it comes out on staging a society, filling it up with gas.
[01:24:51.880 --> 01:24:56.680]   These like the time it takes to do that, we're going to replace the battery pack in this Tesla.
[01:24:56.680 --> 01:24:58.360]   Let's do two of them, right?
[01:24:58.360 --> 01:25:02.840]   And it's this pattern over and over and over and over with the guy,
[01:25:02.840 --> 01:25:04.200]   like full self-driving.
[01:25:04.200 --> 01:25:06.040]   It never comes to fruition.
[01:25:06.040 --> 01:25:10.760]   And I think you have to come to the- at least I've come to the conclusion.
[01:25:10.760 --> 01:25:15.240]   This is someone with so much money that people can't say no to.
[01:25:15.240 --> 01:25:19.400]   They're just fundamentally not an honest actor at this point.
[01:25:19.400 --> 01:25:23.160]   And it's very troubling to me to have so many people.
[01:25:23.160 --> 01:25:27.000]   I know the work at Twitter and have him just destroying the company stock price.
[01:25:27.000 --> 01:25:28.600]   I do feel bad for them.
[01:25:28.600 --> 01:25:29.240]   Yeah.
[01:25:29.240 --> 01:25:29.720]   Yeah.
[01:25:29.720 --> 01:25:34.280]   We refer to Elon Deadlines in the industry.
[01:25:34.280 --> 01:25:38.760]   I mean, according to the Elon deadline from 2018,
[01:25:39.400 --> 01:25:41.160]   we're supposed to be on Mars right now.
[01:25:41.160 --> 01:25:41.720]   Yeah.
[01:25:41.720 --> 01:25:41.960]   Yeah.
[01:25:41.960 --> 01:25:46.040]   There are supposed to be equipment based on SpaceX technology on Mars.
[01:25:46.040 --> 01:25:51.960]   He goes through deadlines in a way that makes Douglas Adams look like a complete-
[01:25:51.960 --> 01:25:54.600]   sorry, a complete answer.
[01:25:54.600 --> 01:25:58.840]   He brings out the best of people, I must say.
[01:25:58.840 --> 01:26:04.120]   Well, no, I mean, just like everyone- anyone can promise anything.
[01:26:04.760 --> 01:26:09.880]   But Musk seems to make a habit of over-promising, over-stressing.
[01:26:09.880 --> 01:26:13.320]   And it's just so infuriating.
[01:26:13.320 --> 01:26:15.000]   There's another deadline looming.
[01:26:15.000 --> 01:26:22.120]   Tesla's AI day is September 3th and Elon has promised that their humanoid robot will appear.
[01:26:22.120 --> 01:26:30.200]   Now, he announced this robot a year ago with a human in a suit, a robot suit.
[01:26:33.000 --> 01:26:38.360]   He says the Tesla bot will be designed to handle dangerous and boring
[01:26:38.360 --> 01:26:39.560]   tests in a factory.
[01:26:39.560 --> 01:26:45.320]   He also sees the humanoid as a companion down the road.
[01:26:45.320 --> 01:26:48.120]   I think Elon reads a little too much science fiction.
[01:26:48.120 --> 01:26:51.560]   But I mean, DARPA has been working on this for 10 years.
[01:26:51.560 --> 01:26:52.040]   It's not easy.
[01:26:52.040 --> 01:26:52.200]   It's not easy.
[01:26:52.200 --> 01:26:53.000]   It's programmed.
[01:26:53.000 --> 01:26:53.000]   Yeah.
[01:26:53.000 --> 01:26:53.320]   Yeah.
[01:26:53.320 --> 01:26:54.360]   And it's really-
[01:26:54.360 --> 01:26:55.720]   What about Boston Dynamics?
[01:26:55.720 --> 01:26:56.200]   That's right.
[01:26:56.200 --> 01:26:57.000]   Yeah, exactly.
[01:26:57.000 --> 01:26:57.960]   For me, you know?
[01:26:57.960 --> 01:27:00.680]   Well, look at the Tesla Roadster.
[01:27:00.680 --> 01:27:03.720]   You know, if you have this, Tesla has this pattern.
[01:27:03.720 --> 01:27:07.560]   Well, they will, you know, with a car that they're trying to bring to market,
[01:27:07.560 --> 01:27:08.840]   like the Tesla Roadster.
[01:27:08.840 --> 01:27:13.160]   You know, they hand-build a model, right?
[01:27:13.160 --> 01:27:17.960]   And then they start trying to put all the pieces together to actually ship it and get it to market.
[01:27:17.960 --> 01:27:22.360]   You know, and you can cheat it in that way.
[01:27:22.360 --> 01:27:24.040]   But like, where's the Roadster?
[01:27:24.040 --> 01:27:26.040]   They promised that in what year was it?
[01:27:26.040 --> 01:27:27.320]   Was that 2018?
[01:27:27.320 --> 01:27:27.400]   Yeah.
[01:27:27.400 --> 01:27:30.440]   Like, it's a really long time ago that people that
[01:27:30.440 --> 01:27:34.120]   put down, what was it, a $10,000 deposit or something?
[01:27:34.120 --> 01:27:37.240]   Like, imagine if they'd actually put that money in the stock market.
[01:27:37.240 --> 01:27:41.960]   I don't believe Tesla is going to make headway on something Boston Dynamics,
[01:27:41.960 --> 01:27:48.280]   which is a huge factory here in Boston, like where they haven't been able to do that
[01:27:48.280 --> 01:27:52.840]   in like longer with the best people in the world doing it.
[01:27:52.840 --> 01:27:54.600]   I just- I flat out don't believe it.
[01:27:54.600 --> 01:27:58.360]   Meanwhile, Twitter does continue to innovate, surprisingly.
[01:27:59.640 --> 01:28:03.080]   If I were a Twitter employee, I would be a little nervous.
[01:28:03.080 --> 01:28:04.680]   Maybe they've been working on this for a while.
[01:28:04.680 --> 01:28:07.000]   They've added something new called notes.
[01:28:07.000 --> 01:28:11.160]   Remember, Twitter bought a newsletter service called Review,
[01:28:11.160 --> 01:28:15.720]   and I suspect this is their kind of version of Review.
[01:28:15.720 --> 01:28:22.360]   You can write a long piece, kind of a medium-length piece and then tweet about it.
[01:28:23.480 --> 01:28:29.720]   It's not open to all yet, but it will be soon. It's called Twitter notes.
[01:28:29.720 --> 01:28:33.320]   We're testing a way to write longer on Twitter.
[01:28:33.320 --> 01:28:34.200]   Is that a good thing?
[01:28:34.200 --> 01:28:38.520]   Yeah. I'm really excited about this feature.
[01:28:38.520 --> 01:28:42.440]   I've been activated. I've permissioned to do it.
[01:28:42.440 --> 01:28:46.520]   Like the thing where you know people pay you for content on Twitter.
[01:28:46.520 --> 01:28:47.160]   Like, sub-stack.
[01:28:47.160 --> 01:28:47.480]   Yeah.
[01:28:47.480 --> 01:28:49.320]   Right. Exactly.
[01:28:49.320 --> 01:28:55.320]   I think if you're marrying that with the ability to write long-form pieces,
[01:28:55.320 --> 01:28:59.160]   this is something I'm a thousand percent going to participate in.
[01:28:59.160 --> 01:29:02.120]   Because you had Twitter, that's where my home is.
[01:29:02.120 --> 01:29:05.720]   That's where I know most of my biggest professional network.
[01:29:05.720 --> 01:29:09.240]   If you are writing a blog post, of course, the first thing you do is you tweet it.
[01:29:09.240 --> 01:29:10.680]   Right.
[01:29:10.680 --> 01:29:12.920]   So why shouldn't the blog post live on Twitter?
[01:29:12.920 --> 01:29:13.400]   Then-
[01:29:13.400 --> 01:29:14.280]   A hundred percent.
[01:29:14.280 --> 01:29:14.840]   Yeah.
[01:29:14.840 --> 01:29:19.160]   They've actually shown that on sub-stack, all the sub-stacks that are large,
[01:29:19.160 --> 01:29:23.880]   happen because of basically Twitter controversies on your sub-stacks.
[01:29:23.880 --> 01:29:28.760]   Why not just cut out the middle person and just have all the outrage on Twitter?
[01:29:28.760 --> 01:29:32.520]   Is there a risk though for Twitter of adding too many features,
[01:29:32.520 --> 01:29:34.520]   making it more and more complicated?
[01:29:34.520 --> 01:29:38.360]   I mean, what really got Twitter started was it was very simple.
[01:29:38.360 --> 01:29:38.840]   Right.
[01:29:38.840 --> 01:29:39.080]   Yeah.
[01:29:39.080 --> 01:29:43.080]   I do miss the old Twitter news feed.
[01:29:43.080 --> 01:29:48.440]   But it used to be that this was a fire hose of information that was coming down.
[01:29:49.000 --> 01:29:55.320]   And then they did the Facebook thing and carved out certain areas.
[01:29:55.320 --> 01:29:57.000]   It's annoying as hell.
[01:29:57.000 --> 01:30:06.120]   Here is- let me pick- if you follow Twitter right on Twitter, you'll see some articles now.
[01:30:06.120 --> 01:30:08.520]   Here I'll click one.
[01:30:08.520 --> 01:30:13.960]   This is a published note, a new way to write beyond 280 characters on Twitter.
[01:30:13.960 --> 01:30:15.640]   It looks pretty nice.
[01:30:15.640 --> 01:30:20.280]   It still has the Twitter, it's wrapped on the left and the right with the Twitter
[01:30:20.280 --> 01:30:22.120]   menu and the trending and all that.
[01:30:22.120 --> 01:30:23.320]   You could put pictures in it.
[01:30:23.320 --> 01:30:25.080]   I think it's pretty cool.
[01:30:25.080 --> 01:30:25.960]   Yeah.
[01:30:25.960 --> 01:30:29.960]   So will you write on this, Brianna, you think?
[01:30:29.960 --> 01:30:30.680]   100%.
[01:30:30.680 --> 01:30:38.840]   I mean, you know, it's- I've noticed with the Twitter algorithm, it rewards you when you do fewer
[01:30:38.840 --> 01:30:41.960]   tweets and they all get more engagement.
[01:30:41.960 --> 01:30:46.840]   So I think with something like this, you could really gain the algorithm rather than doing
[01:30:46.840 --> 01:30:48.600]   you have 10 tweet threats.
[01:30:48.600 --> 01:30:51.000]   So I'm not a fan of the tweet threat.
[01:30:51.000 --> 01:30:51.960]   We argued about this.
[01:30:51.960 --> 01:30:57.400]   We always argue about this on this week in Google because Jeff Jarvis loves the idea of having 20
[01:30:57.400 --> 01:31:02.040]   tweets, one of 22 of 23 of 20.
[01:31:02.040 --> 01:31:05.560]   And I just say, why just- just write a medium post and link to it.
[01:31:05.560 --> 01:31:06.760]   I don't understand.
[01:31:06.760 --> 01:31:10.280]   But he says, but then you can respond to different parts of the thing.
[01:31:10.280 --> 01:31:11.800]   So I guess that makes sense.
[01:31:11.800 --> 01:31:14.120]   It becomes a tree of conversation.
[01:31:14.120 --> 01:31:17.080]   As a journalist, I'm really-
[01:31:17.080 --> 01:31:19.560]   Florence, I'd welcome your view on this.
[01:31:19.560 --> 01:31:24.040]   But as a journalist, if you're in a keynote and you're live-
[01:31:24.040 --> 01:31:25.080]   Well, that's different.
[01:31:25.080 --> 01:31:26.120]   That's live vlogging.
[01:31:26.120 --> 01:31:31.160]   And that is chunk by chunk because you're tweeting each thing that's announced.
[01:31:31.160 --> 01:31:36.600]   But so many people write like, think pieces as a 20 tweet, tweet storm.
[01:31:36.600 --> 01:31:38.120]   And I just- maybe it's just me.
[01:31:38.120 --> 01:31:39.080]   I'm an old-fashioned guy.
[01:31:39.080 --> 01:31:46.120]   It can't be hard to follow along, not to mention that I pay for Twitter blue, which I find to
[01:31:46.120 --> 01:31:48.200]   be worth the $3 a month, frankly, just because I-
[01:31:48.200 --> 01:31:49.960]   I just count on my subscription.
[01:31:49.960 --> 01:31:50.760]   Really?
[01:31:50.760 --> 01:31:51.240]   Interesting.
[01:31:51.240 --> 01:31:52.200]   I know, really?
[01:31:52.200 --> 01:31:52.680]   I just do this.
[01:31:52.680 --> 01:31:53.400]   Really?
[01:31:53.400 --> 01:31:54.600]   So it's half and half.
[01:31:54.600 --> 01:31:55.080]   I built it.
[01:31:55.080 --> 01:31:57.000]   Florence and I, both are Twitter blue.
[01:31:57.000 --> 01:31:58.360]   Happy Twitter blue users.
[01:31:58.360 --> 01:32:00.120]   And you guys just-
[01:32:00.120 --> 01:32:02.120]   I did it because I needed the countdown.
[01:32:02.120 --> 01:32:03.000]   I just might-
[01:32:03.000 --> 01:32:05.240]   The untweeted tweet.
[01:32:05.240 --> 01:32:06.200]   I need to tweet this.
[01:32:06.200 --> 01:32:07.240]   The untweeted tweet.
[01:32:07.240 --> 01:32:08.280]   Yeah, you might want to-
[01:32:08.280 --> 01:32:09.000]   No, no, that-
[01:32:09.000 --> 01:32:09.480]   I liked it.
[01:32:09.480 --> 01:32:10.440]   That was really good.
[01:32:10.440 --> 01:32:14.680]   But I've got to say, I did it for a few months.
[01:32:14.680 --> 01:32:19.480]   And then when Elon announced you can pay for a blue tick,
[01:32:19.480 --> 01:32:24.760]   I was just kind of like, I'm getting out of here because I don't want people to think that I
[01:32:24.760 --> 01:32:26.200]   actually paid for this.
[01:32:26.200 --> 01:32:27.880]   No, but it's not that way yet, right?
[01:32:27.880 --> 01:32:29.320]   This is just something Elon wants to do.
[01:32:29.320 --> 01:32:31.400]   I mean, I earned my verified badge.
[01:32:31.400 --> 01:32:32.760]   Yeah, likewise.
[01:32:32.760 --> 01:32:35.560]   We had to get it the old-fashioned way, bribing somebody.
[01:32:35.560 --> 01:32:37.720]   Yeah, you had to have a legitimate job that-
[01:32:37.720 --> 01:32:38.600]   Oh, well, that's right.
[01:32:38.600 --> 01:32:38.920]   Yeah.
[01:32:38.920 --> 01:32:39.640]   I didn't verify it.
[01:32:39.640 --> 01:32:40.280]   I didn't have to-
[01:32:40.280 --> 01:32:41.880]   Yeah, part of an organization.
[01:32:41.880 --> 01:32:45.000]   Yeah, I participated in a-
[01:32:45.000 --> 01:32:47.080]   Sorry, Amanda.
[01:32:47.080 --> 01:32:47.640]   Sorry, Amanda.
[01:32:47.640 --> 01:32:48.120]   Sorry.
[01:32:48.120 --> 01:32:52.920]   I was just going to say, I participated in a Twitter UI user group.
[01:32:52.920 --> 01:32:53.160]   Oh.
[01:32:53.160 --> 01:32:58.440]   We were trying to refine the Twitter blue product feature in the early days,
[01:32:58.440 --> 01:33:01.320]   because they were trying to figure out what people liked about it.
[01:33:01.320 --> 01:33:02.200]   And I-
[01:33:02.200 --> 01:33:05.720]   I'm not allowed, because I signed an MDA to talk about it.
[01:33:05.720 --> 01:33:11.640]   But I can say the questions that they asked me did not give me a live confidence that the
[01:33:11.640 --> 01:33:14.600]   product is going to be moving in a better direction.
[01:33:14.600 --> 01:33:19.400]   So, you know, when Elon announced he was going to buy Twitter, I was like,
[01:33:19.400 --> 01:33:21.560]   I'm not giving this jerkstore $5.
[01:33:21.560 --> 01:33:23.240]   He doesn't own it, though.
[01:33:23.240 --> 01:33:23.800]   He doesn't own it.
[01:33:23.800 --> 01:33:24.440]   I know.
[01:33:24.440 --> 01:33:25.880]   Now is the time to support it.
[01:33:25.880 --> 01:33:26.600]   Yes.
[01:33:26.600 --> 01:33:26.920]   Right?
[01:33:26.920 --> 01:33:28.840]   I don't know.
[01:33:28.840 --> 01:33:32.040]   I just- when that came up, it was just kind of like,
[01:33:33.320 --> 01:33:37.480]   oh, we- I don't even remember what I get for Twitter blue.
[01:33:37.480 --> 01:33:39.000]   I just want to support Twitter.
[01:33:39.000 --> 01:33:40.600]   Are you going to go when I'm getting-
[01:33:40.600 --> 01:33:41.080]   You get-
[01:33:41.080 --> 01:33:41.800]   You get to pay one.
[01:33:41.800 --> 01:33:47.400]   You know, I pay for certain publications, you know, because journalism costs money.
[01:33:47.400 --> 01:33:50.360]   And I'm quite happy to do that.
[01:33:50.360 --> 01:33:56.360]   But when Twitter started just like, okay, well, you can pay for certain things and
[01:33:56.360 --> 01:33:58.840]   pay for certain privilege, hell with that.
[01:33:58.840 --> 01:33:59.880]   I'll pay to read.
[01:33:59.880 --> 01:34:01.960]   I won't pay to get, you know,
[01:34:03.000 --> 01:34:04.840]   to be like aristocracy.
[01:34:04.840 --> 01:34:07.320]   I'm just done that.
[01:34:07.320 --> 01:34:08.920]   Look how well it worked in the US.
[01:34:08.920 --> 01:34:15.480]   I wouldn't mind being Sir Leo Laporte or your Duke ship.
[01:34:15.480 --> 01:34:17.400]   I wouldn't mind that.
[01:34:17.400 --> 01:34:19.880]   Yeah, I'll say Prince Andrew, how was that work?
[01:34:19.880 --> 01:34:23.480]   Oh, yeah, well, that's, you know, that's because that's hereditary.
[01:34:23.480 --> 01:34:25.400]   No, that's a hereditary problem.
[01:34:25.400 --> 01:34:29.080]   That's not, see, that's the hereditary titles that aren't good.
[01:34:29.080 --> 01:34:30.520]   I want to buy my title.
[01:34:31.160 --> 01:34:35.000]   No, no, you've got exactly the same problem in the US at the moment.
[01:34:35.000 --> 01:34:38.760]   I mean, everyone's just like, oh, well, Britain is so hereditary.
[01:34:38.760 --> 01:34:39.240]   Really?
[01:34:39.240 --> 01:34:43.480]   You had George Bush in second, the idiot son of the president.
[01:34:43.480 --> 01:34:45.480]   Oh, yeah, we have our own administration.
[01:34:45.480 --> 01:34:46.040]   Oh, it's true.
[01:34:46.040 --> 01:34:49.640]   Yeah, you've got oligarchs all the way through the route.
[01:34:49.640 --> 01:34:52.760]   And aristocracy is the death of society.
[01:34:52.760 --> 01:34:54.600]   We've learned this over at least-
[01:34:54.600 --> 01:34:55.320]   History.
[01:34:55.320 --> 01:34:57.560]   With the Queen, you get paddings in the baron tea.
[01:34:58.840 --> 01:35:03.960]   Oh, honestly, I have to tell you, I saw, you know, along with Sailor Moon,
[01:35:03.960 --> 01:35:06.840]   which I saw every episode, I read the princess diaries.
[01:35:06.840 --> 01:35:12.120]   This seems to me the being a royal figure has a lot more problems than I thought.
[01:35:12.120 --> 01:35:14.040]   Do you want to be a princess, Priya?
[01:35:14.040 --> 01:35:15.720]   No, no, no, I'm sorry.
[01:35:15.720 --> 01:35:19.800]   Oh, Sailor Moon was rough.
[01:35:19.800 --> 01:35:22.040]   Yeah, Sailor Moon.
[01:35:22.040 --> 01:35:22.760]   Oh my God.
[01:35:22.760 --> 01:35:23.720]   You never saw here?
[01:35:23.720 --> 01:35:24.600]   Oh my God.
[01:35:24.600 --> 01:35:27.560]   Wait a minute, there was a season that didn't come to the US?
[01:35:27.560 --> 01:35:29.320]   Yeah, Sailor Moon stars.
[01:35:29.320 --> 01:35:31.480]   Oh, I don't know what they're talking about.
[01:35:31.480 --> 01:35:36.920]   Yeah, you can watch it now on Netflix because Viz redid it,
[01:35:36.920 --> 01:35:40.200]   Sailor Moon crystal, but the original series is how we all got.
[01:35:40.200 --> 01:35:41.400]   Yeah.
[01:35:41.400 --> 01:35:41.400]   Yeah.
[01:35:41.400 --> 01:35:42.200]   Was it grim?
[01:35:42.200 --> 01:35:44.920]   Yeah, Sailor Moon star, dude.
[01:35:44.920 --> 01:35:47.000]   Very dark.
[01:35:47.000 --> 01:35:47.960]   Very dark.
[01:35:47.960 --> 01:35:48.440]   Very dark.
[01:35:48.440 --> 01:35:51.800]   This episode brought to you by ExpressVPN.
[01:35:51.800 --> 01:35:56.040]   You never run out of anything to watch when you have ExpressVPN.
[01:35:56.040 --> 01:36:00.120]   And let me tell you, nowadays we're talking about a privacy is very important.
[01:36:00.120 --> 01:36:06.200]   This is one way to really improve your privacy.
[01:36:06.200 --> 01:36:10.760]   You've heard we were talking about data brokers, the middlemen collecting and selling all the
[01:36:10.760 --> 01:36:13.400]   digital smog you're leaving behind you.
[01:36:13.400 --> 01:36:19.480]   They can easily collate all that information, stitch together detailed profiles, all about
[01:36:19.480 --> 01:36:24.040]   you, your browsing history, your online searches, your location data, and then they sell it
[01:36:25.080 --> 01:36:29.560]   to anybody who will want it for a surprisingly low amount of money, I might add.
[01:36:29.560 --> 01:36:34.120]   You might be surprised to learn the same data brokers also sell your information to the
[01:36:34.120 --> 01:36:40.040]   Department of Homeland Security, the IRS who wants the tax man showing up at the door because
[01:36:40.040 --> 01:36:41.560]   there's some search they did on their phone.
[01:36:41.560 --> 01:36:43.560]   Well, it could be a lot worse than that.
[01:36:43.560 --> 01:36:50.440]   So mask your digital footprints, protect yourself like I do with ExpressVPN.
[01:36:50.440 --> 01:36:54.840]   One of the nice things about ExpressVPN is you don't appear on the net as yourself.
[01:36:54.840 --> 01:36:58.360]   You have an IP address that is ExpressVPNs.
[01:36:58.360 --> 01:37:03.800]   So when you do that Google search, Google doesn't say Ohio, Leo, Google thinks it's ExpressVPN
[01:37:03.800 --> 01:37:06.280]   and hundreds of other people using that same IP address.
[01:37:06.280 --> 01:37:06.840]   I love it.
[01:37:06.840 --> 01:37:10.280]   The ExpressVPN also invests in their infrastructure.
[01:37:10.280 --> 01:37:12.440]   So they rotate IP addresses.
[01:37:12.440 --> 01:37:18.360]   So that's one of the reasons you can use ExpressVPN with things like the BBC iPlayer,
[01:37:18.360 --> 01:37:20.520]   which really try to stop you from doing that.
[01:37:20.520 --> 01:37:20.920]   It works.
[01:37:20.920 --> 01:37:21.640]   It still works.
[01:37:23.240 --> 01:37:25.400]   So get a unique IP address.
[01:37:25.400 --> 01:37:27.800]   Watch content anywhere you want.
[01:37:27.800 --> 01:37:30.040]   Stop big tech from tracking you.
[01:37:30.040 --> 01:37:33.800]   Protect yourself with an encrypted tunnel no matter where you are.
[01:37:33.800 --> 01:37:37.400]   I'm going to be traveling tomorrow, staying in a hotel all week.
[01:37:37.400 --> 01:37:40.040]   You better believe I'm going to bring an ExpressVPN with me.
[01:37:40.040 --> 01:37:43.320]   100% of the network traffic is encrypted.
[01:37:43.320 --> 01:37:45.400]   You can run ExpressVPN on almost everything.
[01:37:45.400 --> 01:37:50.760]   You've got max PCs, Linux, Windows, iOS, Android.
[01:37:50.760 --> 01:37:53.160]   You can even put it on your router, some routers.
[01:37:53.160 --> 01:37:54.680]   You can put it on a smart TV.
[01:37:54.680 --> 01:37:59.800]   Really protect your privacy, protect your security, and all you have to do is turn it on with one
[01:37:59.800 --> 01:38:00.760]   button and you're done.
[01:38:00.760 --> 01:38:05.960]   So make sure your online activity and data is protected with the best VPN money can buy.
[01:38:05.960 --> 01:38:10.280]   ExpressVPN.com/twit.
[01:38:10.280 --> 01:38:11.000]   Go there right now.
[01:38:11.000 --> 01:38:14.120]   You'll get three extra months free when you buy a one-year package.
[01:38:14.120 --> 01:38:17.480]   That brings the price down about $7 less than $7 a month.
[01:38:17.480 --> 01:38:18.840]   That's a heck of a deal.
[01:38:18.840 --> 01:38:24.680]   eXPR ESS ExpressVPN.com/twit.
[01:38:24.680 --> 01:38:29.000]   And if you're at all curious, there was a great article on bleeping computer
[01:38:29.000 --> 01:38:31.400]   about how ExpressVPN works.
[01:38:31.400 --> 01:38:32.440]   They're so smart.
[01:38:32.440 --> 01:38:39.160]   They use a custom Debian install that refreshes every time the computer reboots every single day.
[01:38:39.160 --> 01:38:41.640]   So nothing is saved on the hard drive.
[01:38:41.640 --> 01:38:44.360]   Their trusted server technology runs in RAM.
[01:38:44.360 --> 01:38:45.880]   Can't write to the hard drive.
[01:38:45.880 --> 01:38:49.400]   You're completely anonymous on ExpressVPN.
[01:38:49.400 --> 01:38:50.440]   They're doing God's work.
[01:38:50.440 --> 01:38:54.440]   ExpressVPN.com/twit.
[01:38:54.440 --> 01:38:56.840]   Thank you ExpressVPN for your support.
[01:38:56.840 --> 01:39:02.120]   Prime day, July 12th.
[01:39:02.120 --> 01:39:03.400]   Boy.
[01:39:03.400 --> 01:39:05.560]   Oh yeah, I forgot.
[01:39:05.560 --> 01:39:07.240]   Don't worry.
[01:39:07.240 --> 01:39:08.360]   You won't get to forget.
[01:39:08.360 --> 01:39:09.720]   You'll get plenty of reminders.
[01:39:09.720 --> 01:39:10.760]   National holiday.
[01:39:10.760 --> 01:39:12.360]   It's a national holiday.
[01:39:12.360 --> 01:39:12.760]   I don't know.
[01:39:12.760 --> 01:39:14.520]   I have never bought anything.
[01:39:14.520 --> 01:39:16.280]   Actually, I've never participated either.
[01:39:16.280 --> 01:39:18.840]   Yeah, I feel like it's not going to be a deal.
[01:39:18.840 --> 01:39:22.440]   You know who I feel bad for is all those writers at Gizmodo
[01:39:22.440 --> 01:39:25.560]   who are going to be working on the 12th and the 13th, right?
[01:39:25.560 --> 01:39:28.360]   Looking for deals on prime day.
[01:39:28.360 --> 01:39:31.480]   Actually, that's not quite true Leo.
[01:39:31.480 --> 01:39:36.760]   Prime day had the wire cutter does a special guide for prime day where they
[01:39:36.760 --> 01:39:38.040]   remember they were going to boycott.
[01:39:38.040 --> 01:39:39.240]   They boy cut it last year.
[01:39:39.240 --> 01:39:40.120]   They went on strike.
[01:39:40.120 --> 01:39:40.920]   Remember?
[01:39:40.920 --> 01:39:41.000]   Yeah.
[01:39:41.000 --> 01:39:42.120]   And then one last thing.
[01:39:42.120 --> 01:39:42.760]   Just give it.
[01:39:42.760 --> 01:39:43.480]   The power of union.
[01:39:43.480 --> 01:39:44.120]   Did they win?
[01:39:44.920 --> 01:39:45.560]   They did win.
[01:39:45.560 --> 01:39:46.200]   They did win.
[01:39:46.200 --> 01:39:48.520]   Does they say it's that union?
[01:39:48.520 --> 01:39:49.160]   They did.
[01:39:49.160 --> 01:39:49.560]   They did.
[01:39:49.560 --> 01:39:50.760]   Right on.
[01:39:50.760 --> 01:39:52.120]   In the union 100%.
[01:39:52.120 --> 01:39:55.000]   But they brought out something and they're like,
[01:39:55.000 --> 01:40:02.120]   wire cutter would like to announce we do not recommend this thing that happens to be on sale
[01:40:02.120 --> 01:40:04.840]   right now, which is a cowboy hat for your dog.
[01:40:04.840 --> 01:40:08.280]   They're so wrong.
[01:40:08.280 --> 01:40:09.800]   They're so wrong.
[01:40:10.760 --> 01:40:12.200]   Rocket needs.
[01:40:12.200 --> 01:40:14.520]   She needs a cowboy hat for your dog.
[01:40:14.520 --> 01:40:16.200]   I don't care how much it costs.
[01:40:16.200 --> 01:40:17.080]   It's worth it.
[01:40:17.080 --> 01:40:19.320]   Though I did fault.
[01:40:19.320 --> 01:40:20.280]   And that one that's 10.
[01:40:20.280 --> 01:40:22.280]   So you have bought something on prime day.
[01:40:22.280 --> 01:40:22.920]   I did.
[01:40:22.920 --> 01:40:26.200]   Cowboy hat for your dog.
[01:40:26.200 --> 01:40:27.240]   Is your dog's name rocket?
[01:40:27.240 --> 01:40:29.160]   My dog's name is rocket.
[01:40:29.160 --> 01:40:32.840]   Did rocket the podcast come before rocket the dog or vice versa?
[01:40:32.840 --> 01:40:36.360]   Yeah, I was going to name my I was going to name my dog optimus.
[01:40:36.360 --> 01:40:37.960]   And then we decide on the name.
[01:40:37.960 --> 01:40:39.560]   Like like optimus prime.
[01:40:39.560 --> 01:40:40.360]   Yeah.
[01:40:40.360 --> 01:40:41.000]   Yeah.
[01:40:41.000 --> 01:40:41.560]   Yeah.
[01:40:41.560 --> 01:40:42.440]   Optimus.
[01:40:42.440 --> 01:40:46.120]   And then we made the podcast rocket.
[01:40:46.120 --> 01:40:49.320]   And I was like, rocket's a good name for a dog.
[01:40:49.320 --> 01:40:50.680]   That's rocket.
[01:40:50.680 --> 01:40:51.640]   It's rocky.
[01:40:51.640 --> 01:40:52.280]   Row row.
[01:40:52.280 --> 01:40:52.760]   Yeah.
[01:40:52.760 --> 01:40:55.960]   Rocket is a very good name for a dog actually.
[01:40:55.960 --> 01:40:56.840]   Yeah.
[01:40:56.840 --> 01:40:57.800]   Now that you mention it.
[01:40:57.800 --> 01:40:59.480]   So I should.
[01:40:59.480 --> 01:41:00.760]   Oh, I steal my idea.
[01:41:00.760 --> 01:41:03.640]   I should give you a plug for your podcast at this point,
[01:41:03.640 --> 01:41:05.800]   which is named the same as her dog.
[01:41:05.800 --> 01:41:09.240]   Christina Warren, who's going to be on next week on Twitch.
[01:41:09.240 --> 01:41:14.200]   Brianna Wu, Simone de Roche for rocket accelerated geek conversation.
[01:41:14.200 --> 01:41:15.400]   Look at this.
[01:41:15.400 --> 01:41:17.160]   We're nearly at episode 400.
[01:41:17.160 --> 01:41:17.800]   Look at you.
[01:41:17.800 --> 01:41:19.400]   It's a great.
[01:41:19.400 --> 01:41:20.360]   400 out there.
[01:41:20.360 --> 01:41:20.920]   You deserve it.
[01:41:20.920 --> 01:41:21.720]   You're a great show.
[01:41:21.720 --> 01:41:23.800]   Will I.
[01:41:23.800 --> 01:41:29.000]   I actually get so rare like women to get to like have a voice in, you know,
[01:41:29.000 --> 01:41:30.600]   everything goes on.
[01:41:30.600 --> 01:41:35.240]   We're one of the oldest shows on relay and like 400 quality episodes.
[01:41:35.240 --> 01:41:37.000]   Like Simone is awesome.
[01:41:37.000 --> 01:41:37.560]   That's awesome.
[01:41:37.560 --> 01:41:38.760]   Christina is great.
[01:41:38.760 --> 01:41:39.720]   I mean, it's a.
[01:41:39.720 --> 01:41:40.200]   All three of you.
[01:41:40.200 --> 01:41:41.800]   But you got to put in a word with Simone.
[01:41:41.800 --> 01:41:43.320]   She doesn't return my calls anymore.
[01:41:43.320 --> 01:41:44.200]   I don't know if I said so.
[01:41:44.200 --> 01:41:47.160]   No, because we have you on.
[01:41:47.160 --> 01:41:49.640]   We have Christina and I want Simone on as well.
[01:41:49.640 --> 01:41:50.920]   I love Simone.
[01:41:50.920 --> 01:41:52.280]   Simone is she's.
[01:41:52.280 --> 01:41:54.600]   I joke that she's from another planet.
[01:41:54.600 --> 01:41:56.840]   Like I've, I've done 400 shows with her.
[01:41:56.840 --> 01:41:58.280]   I don't really understand her.
[01:41:58.280 --> 01:42:00.200]   So I just want her to do my ads.
[01:42:00.200 --> 01:42:01.560]   She does such a good job.
[01:42:01.560 --> 01:42:02.600]   Oh, she's.
[01:42:02.600 --> 01:42:05.080]   I just want to Simone read this ad.
[01:42:05.080 --> 01:42:05.800]   I used to go in.
[01:42:05.800 --> 01:42:07.880]   I used to go on a radio show in San Francisco.
[01:42:07.880 --> 01:42:10.280]   KGO hosted by a guy named Ron Owens.
[01:42:10.280 --> 01:42:14.280]   Every time I go in, you know, do my little 15 minutes tech segment.
[01:42:14.280 --> 01:42:15.720]   He'd make me read the ads.
[01:42:15.720 --> 01:42:17.480]   Pissed me off.
[01:42:17.480 --> 01:42:21.320]   It was a scam as it turned out.
[01:42:21.320 --> 01:42:27.160]   Google news has one in Spain.
[01:42:27.160 --> 01:42:31.800]   You may remember that the Spanish government.
[01:42:34.120 --> 01:42:37.800]   You know, wanted Google to pay for the snippets.
[01:42:37.800 --> 01:42:39.400]   This goes back to that whole snippet thing.
[01:42:39.400 --> 01:42:44.200]   After eight years, Google shut down saying,
[01:42:44.200 --> 01:42:49.080]   we are not going to pay publishers to put their name in our search results.
[01:42:49.080 --> 01:42:49.800]   That's nuts.
[01:42:49.800 --> 01:42:52.440]   And when the government said you have to.
[01:42:52.440 --> 01:42:54.520]   Yeah, they're going to steal a copy anyway.
[01:42:54.520 --> 01:43:00.120]   Spoken as a guy whose copy's probably getting stolen once in a while.
[01:43:00.120 --> 01:43:01.160]   No, I'm sorry.
[01:43:01.160 --> 01:43:03.560]   We are copy gets ripped off all the time.
[01:43:04.040 --> 01:43:04.280]   Yeah.
[01:43:04.280 --> 01:43:07.880]   And it's just like, yeah, okay, well, we're just going to take the stuff
[01:43:07.880 --> 01:43:09.560]   and you're going to have to deal with it.
[01:43:09.560 --> 01:43:11.400]   But doesn't Google send you traffic?
[01:43:11.400 --> 01:43:14.360]   I mean, honestly, doesn't most of your traffic come from Google?
[01:43:14.360 --> 01:43:22.760]   Actually, this is really interesting because when we looked at our traffic a couple of years ago
[01:43:22.760 --> 01:43:27.080]   and then today, okay, Facebook, we're trying to get us to pivot to video.
[01:43:27.080 --> 01:43:30.280]   And we told them to basically bugger off.
[01:43:31.800 --> 01:43:38.680]   And boy, hasn't that worked returns because everyone the pivot video got screwed so badly.
[01:43:38.680 --> 01:43:44.680]   Google is we've had our problems with Google when we changed our, you know,
[01:43:44.680 --> 01:43:46.840]   to from a dot go dot UK to dot com.
[01:43:46.840 --> 01:43:50.680]   But I'm sorry, these people are absolutely bastards.
[01:43:50.680 --> 01:43:54.920]   So you must do what you're told and say, no, thanks.
[01:43:54.920 --> 01:44:00.120]   Well, Gary, on as we are, it's worked out in the end, but I'm sorry.
[01:44:00.120 --> 01:44:03.240]   This is a really big issue for independent journalism.
[01:44:03.240 --> 01:44:10.360]   So in 2014, Spanish copyright law said publishers can charge Google News to use their links and
[01:44:10.360 --> 01:44:15.720]   headlines. Google's response very famously was to leave Spain.
[01:44:15.720 --> 01:44:19.720]   It's not this Spain related.
[01:44:19.720 --> 01:44:25.880]   It's just that a European copyright directive adopted in Spain in November
[01:44:26.760 --> 01:44:31.800]   allows online aggregators and platforms to use new snippets without permission from publishers
[01:44:31.800 --> 01:44:36.440]   if it's quote, a very short extract or just individual words.
[01:44:36.440 --> 01:44:42.360]   Is your concern that people will read the snippet and then not go to your article?
[01:44:42.360 --> 01:44:44.920]   No, it's not that at all.
[01:44:44.920 --> 01:44:51.960]   It's the fact that they are basically driving advertising information traffic using our copy
[01:44:51.960 --> 01:44:52.920]   and not paying for it.
[01:44:53.480 --> 01:44:57.640]   You know, it costs money to get good journalism out there.
[01:44:57.640 --> 01:45:00.680]   And at France, I'm hoping it will agree with me on this.
[01:45:00.680 --> 01:45:04.200]   We have to pay for journalism.
[01:45:04.200 --> 01:45:09.880]   And Google's saying, well, we just took the headlines and a couple of lines
[01:45:09.880 --> 01:45:12.360]   and we're not actually going to pay for it.
[01:45:12.360 --> 01:45:16.600]   But it's pretty shameful, to be honest.
[01:45:16.600 --> 01:45:19.640]   Yeah, by the way, go ahead.
[01:45:19.640 --> 01:45:20.120]   Sorry, go ahead.
[01:45:20.120 --> 01:45:22.200]   No, no, I'd love to know what you think.
[01:45:22.920 --> 01:45:26.600]   I was going to say it's very difficult to be a journalist in this climate right now
[01:45:26.600 --> 01:45:31.800]   because I am constantly finding myself having to justify why somebody should be paying money
[01:45:31.800 --> 01:45:34.280]   to read stuff that I effectively post for free.
[01:45:34.280 --> 01:45:41.640]   But the big thing that sucks is the SEO game of it all when you have these copycat sites
[01:45:41.640 --> 01:45:44.120]   that just re-aggregate your reporting.
[01:45:44.120 --> 01:45:50.840]   And that is where that is what Google's surfacing before the actual article.
[01:45:50.840 --> 01:45:53.880]   And it can get really frustrating.
[01:45:53.880 --> 01:45:56.280]   They've made a business plan out of this.
[01:45:56.280 --> 01:46:02.040]   It's basically screw the people that actually make the news,
[01:46:02.040 --> 01:46:03.800]   which is going to scrape it and run it.
[01:46:03.800 --> 01:46:08.200]   And Gizmodo has done some fantastic reporting.
[01:46:08.200 --> 01:46:13.000]   All models to require, the registers and pretty good stuff as well.
[01:46:13.000 --> 01:46:16.040]   But I mean, this stuff costs money.
[01:46:16.040 --> 01:46:17.880]   We have journalists to pay.
[01:46:18.600 --> 01:46:23.160]   And Google just scraping and showing.
[01:46:23.160 --> 01:46:26.200]   I'm sorry, it's just like you're a parasite.
[01:46:26.200 --> 01:46:28.600]   That's the way I see it.
[01:46:28.600 --> 01:46:31.400]   I'm looking at the, and by the way, this is just recently revamped,
[01:46:31.400 --> 01:46:33.960]   the Google News site for technology.
[01:46:33.960 --> 01:46:36.200]   I see articles.
[01:46:36.200 --> 01:46:40.360]   I mean, honestly, I think this drives traffic.
[01:46:40.360 --> 01:46:42.520]   I look at this and I say, oh, yeah, I want to read that.
[01:46:42.520 --> 01:46:44.680]   No, I don't want to read that.
[01:46:44.680 --> 01:46:49.400]   I mean, it's not enough that it doesn't give you enough information
[01:46:49.400 --> 01:46:51.000]   that I wouldn't read the article.
[01:46:51.000 --> 01:46:53.320]   It's sending traffic to you.
[01:46:53.320 --> 01:46:57.400]   Look at the title sort of there, and then you wonder who's paying for it.
[01:46:57.400 --> 01:47:01.160]   Oh, these aren't your headlines?
[01:47:01.160 --> 01:47:04.280]   Well, we don't pay for headlines.
[01:47:04.280 --> 01:47:05.320]   Other companies do.
[01:47:05.320 --> 01:47:08.360]   OK.
[01:47:08.360 --> 01:47:12.520]   I can't name names, but companies pay.
[01:47:12.520 --> 01:47:13.320]   Please do.
[01:47:13.320 --> 01:47:13.880]   Please do.
[01:47:13.880 --> 01:47:21.320]   So you're saying somebody like, I don't know, let's say, some fruit company in Southern Bay Area
[01:47:21.320 --> 01:47:25.000]   might pay the verge to say nice things in the headline,
[01:47:25.000 --> 01:47:26.840]   or did Stephen mention the minute liner?
[01:47:26.840 --> 01:47:29.240]   Well, I'm sorry.
[01:47:29.240 --> 01:47:36.040]   Oh, I was going to try and help being out by just saying, deals are made.
[01:47:36.040 --> 01:47:38.920]   Yeah, I had never heard of this.
[01:47:38.920 --> 01:47:40.840]   I've never made for play for placement.
[01:47:40.840 --> 01:47:45.560]   You know how, remember how in the entire 10 seasons of 902 and now there's always a
[01:47:45.560 --> 01:47:50.600]   Dr. Pepper or Diet Dr. Pepper in the scene and no other beverage could seem?
[01:47:50.600 --> 01:47:51.080]   But that's a little different.
[01:47:51.080 --> 01:47:54.920]   You're not going to that show to find out what beverage to drink.
[01:47:54.920 --> 01:48:02.360]   If I'm going to a site that for technology reviews and news and they're getting paid
[01:48:02.360 --> 01:48:07.560]   to show that content, that's putting your thumb on the scale.
[01:48:07.560 --> 01:48:10.040]   Well, no, I mean, OK.
[01:48:10.920 --> 01:48:11.480]   OK.
[01:48:11.480 --> 01:48:16.040]   I'm dealing under British libel law.
[01:48:16.040 --> 01:48:17.160]   So this is going to be careful.
[01:48:17.160 --> 01:48:18.040]   I don't want you saying.
[01:48:18.040 --> 01:48:18.920]   Yeah, I want to be.
[01:48:18.920 --> 01:48:20.600]   You don't have to name any names.
[01:48:20.600 --> 01:48:22.600]   This is a blind item.
[01:48:22.600 --> 01:48:24.040]   So do it as a blind item.
[01:48:24.040 --> 01:48:24.840]   OK.
[01:48:24.840 --> 01:48:32.680]   There are at least three publications I know of that when they do their top five,
[01:48:32.680 --> 01:48:38.200]   whatever it is, that's led by the advertising department, not by the editorial department.
[01:48:38.200 --> 01:48:43.320]   See, from my background, it's if Davis for years, I didn't mention that.
[01:48:43.320 --> 01:48:46.280]   No, no, no, I'm just saying that was verbot.
[01:48:46.280 --> 01:48:47.720]   You did not do that.
[01:48:47.720 --> 01:48:51.000]   And we, of course, we continue to follow those editorial standards at Twit.
[01:48:51.000 --> 01:48:56.840]   There is a there is a very clear bright line between advertising and editorial.
[01:48:56.840 --> 01:49:02.120]   And yeah, the way it advertises all the time will constantly say,
[01:49:02.120 --> 01:49:03.800]   when are you going to interview our CEO and stuff?
[01:49:03.800 --> 01:49:04.600]   We say, we're not.
[01:49:04.600 --> 01:49:07.880]   So that's I mean, but.
[01:49:08.040 --> 01:49:09.640]   You just have to draw that line.
[01:49:09.640 --> 01:49:12.040]   You certainly don't have the sales department going and saying,
[01:49:12.040 --> 01:49:17.640]   hey, how'd you like to be on our list of the top 10 gizmos for 1999?
[01:49:17.640 --> 01:49:19.720]   And then you pay me and I put you on that list.
[01:49:19.720 --> 01:49:21.320]   That's that's an ad.
[01:49:21.320 --> 01:49:27.320]   And then by the way, the FTC by law, you have to say sponsored or add.
[01:49:27.320 --> 01:49:28.200]   And they're not doing that.
[01:49:28.200 --> 01:49:29.960]   Absolutely not.
[01:49:29.960 --> 01:49:37.320]   No, I mean, look, when I was on PC, Mag, we spent two days going through our best of
[01:49:37.320 --> 01:49:39.160]   at the end of the year's series.
[01:49:39.160 --> 01:49:42.520]   And there were arguments on the one case of fist flight over there.
[01:49:42.520 --> 01:49:43.320]   It's editorial.
[01:49:43.320 --> 01:49:44.200]   Yeah.
[01:49:44.200 --> 01:49:47.960]   But at the moment, it's who pays.
[01:49:47.960 --> 01:49:50.600]   I missed the days when journalists would get into fist fights.
[01:49:50.600 --> 01:49:56.200]   I've had more than a few and they're fun.
[01:49:56.200 --> 01:49:57.000]   No, really?
[01:49:57.000 --> 01:49:57.960]   All right.
[01:49:57.960 --> 01:49:58.520]   Oh, God.
[01:49:58.520 --> 01:49:58.840]   Yeah.
[01:49:58.840 --> 01:50:03.000]   But we're not going to name we're not saying which companies and we're not saying which publications.
[01:50:03.000 --> 01:50:03.960]   I'm not surprised.
[01:50:03.960 --> 01:50:06.280]   This day and age, look, you go to YouTube.
[01:50:06.280 --> 01:50:16.360]   It's pretty clear if you watch any YouTube tech channel that there's there's something going on
[01:50:16.360 --> 01:50:17.240]   there.
[01:50:17.240 --> 01:50:20.840]   I miss you.
[01:50:20.840 --> 01:50:23.080]   It's just who is playing the game the right way.
[01:50:23.080 --> 01:50:23.320]   Yeah.
[01:50:23.320 --> 01:50:24.040]   Who is playing the game?
[01:50:24.040 --> 01:50:25.960]   Who has the money for the resources?
[01:50:25.960 --> 01:50:29.000]   Who has the money for an SEO team?
[01:50:29.000 --> 01:50:33.000]   Who has the money for a team that will like jump in and all the, you know,
[01:50:33.000 --> 01:50:33.800]   I'm disappointed.
[01:50:33.800 --> 01:50:35.720]   I had things that Google wants to add.
[01:50:35.720 --> 01:50:42.920]   It's like all of the stuff adds up to your overall ranking as an entity on the internet.
[01:50:42.920 --> 01:50:47.320]   And so I think I feel like it correct me where I'm wrong, but I feel like that's what kind of
[01:50:47.320 --> 01:50:48.360]   we're alluding to.
[01:50:48.360 --> 01:50:54.120]   No, no, no, no, that's not a tit for tat. That's not. Here's a dollar you put me there. You're saying they're they're actually paying for it.
[01:50:54.120 --> 01:50:57.400]   Oh, they're absolutely paying for it.
[01:50:57.400 --> 01:50:59.880]   That's bad, by the way.
[01:50:59.880 --> 01:51:01.560]   I mean, I agree with you Florence.
[01:51:01.560 --> 01:51:10.520]   Totally. It's it's not all publications, but publications with a good name with a good editorial
[01:51:10.520 --> 01:51:14.680]   history are basically selling themselves out on the street.
[01:51:14.680 --> 01:51:15.880]   I'm so disappointed to hear that.
[01:51:15.880 --> 01:51:23.560]   Cheap reviews. It's also very bad for our industry because as soon as people stop trusting
[01:51:23.560 --> 01:51:28.760]   that we're saying what we truly believe as opposed to what we're being paid to say,
[01:51:28.760 --> 01:51:30.200]   the whole thing collapses.
[01:51:30.200 --> 01:51:37.960]   We've got to understand that these magazines, these publications are being run by private equity
[01:51:37.960 --> 01:51:40.040]   companies who don't give a hell about the law.
[01:51:40.040 --> 01:51:41.400]   Don't care anymore about integrity.
[01:51:41.400 --> 01:51:47.160]   They want three to five years of good returns, and then they can dump it after creating
[01:51:47.160 --> 01:51:50.760]   huge management fees for themselves. So it's a win-win for them.
[01:51:50.760 --> 01:52:00.200]   I'm sorry to be Debbie down or on this.
[01:52:00.200 --> 01:52:02.280]   Brianna, does this shock you?
[01:52:02.280 --> 01:52:09.720]   What shocks me is I'm not a journalist like I write occasional pieces,
[01:52:10.680 --> 01:52:13.400]   but I'm not a writer as well.
[01:52:13.400 --> 01:52:15.480]   I appreciate that.
[01:52:15.480 --> 01:52:19.960]   I went to journalism school and I did investigative work.
[01:52:19.960 --> 01:52:24.280]   The biggest story I ever found was that coaches at my school are being paid millions of dollars
[01:52:24.280 --> 01:52:31.880]   for classes that did not exist. I did it through like hardcore, like investigating the budget,
[01:52:31.880 --> 01:52:34.440]   going through interviewing people, following the money.
[01:52:36.280 --> 01:52:43.480]   This was something I could do, but I quickly came to the conclusion that it was not going to be
[01:52:43.480 --> 01:52:49.160]   an industry where I could have a portion of collection if I kept on pursuing that field.
[01:52:49.160 --> 01:52:55.640]   And it just, the workplace is so abusive to every one of y'all, and you're being screwed over
[01:52:55.640 --> 01:53:03.400]   by everything involved. Like the companies you work for, Google, the people that mistreat you
[01:53:03.400 --> 01:53:07.480]   and y'all disproportionately win in nowadays. It's just a mystery to me.
[01:53:07.480 --> 01:53:12.760]   Like any of you stay in this field, and it's so clearly underpaid and abusive.
[01:53:12.760 --> 01:53:17.880]   So I just, I feel sorry for you. Thank you for your service, but it sucks.
[01:53:17.880 --> 01:53:21.560]   It's a tough job, but we love it.
[01:53:21.560 --> 01:53:29.240]   The thing is, democracy dies without information and take democracy dies without information.
[01:53:29.240 --> 01:53:35.000]   And yeah, I'm sure Florence has got the same PR companies are all,
[01:53:35.000 --> 01:53:39.080]   you know, come enjoy, does the rest of it. There's an important job to be done.
[01:53:39.080 --> 01:53:40.680]   You know, it's...
[01:53:40.680 --> 01:53:47.160]   I have some standards. I would never work in PR. I have some standards.
[01:53:47.160 --> 01:53:51.240]   I did three years there. I launched ActiveX in the UK.
[01:53:51.240 --> 01:53:52.280]   On the dark side.
[01:53:52.280 --> 01:53:54.280]   1996.
[01:53:54.280 --> 01:53:55.800]   ActiveX, Ian.
[01:53:55.800 --> 01:53:58.600]   I know. I still feel guilty about that one.
[01:53:59.080 --> 01:54:05.400]   Do you know how many times you were the somebody uttered a curse word toward you because of ActiveX?
[01:54:05.400 --> 01:54:08.280]   Ah, god. It was shaming.
[01:54:08.280 --> 01:54:08.920]   That means installs.
[01:54:08.920 --> 01:54:10.680]   No, no, no. I mean, seriously.
[01:54:10.680 --> 01:54:13.400]   Microsoft ActiveX?
[01:54:13.400 --> 01:54:17.960]   Yeah, no, I mean, I worked. I did three years in PR in the midnight.
[01:54:17.960 --> 01:54:21.320]   You promoted the use of Microsoft's ActiveX technology.
[01:54:21.320 --> 01:54:23.160]   Okay, this is a shameful approach.
[01:54:23.160 --> 01:54:24.120]   This is a shameful approach.
[01:54:24.120 --> 01:54:28.840]   But at the same time, they gave me a non-disclosure agreement,
[01:54:28.840 --> 01:54:31.640]   which I didn't sign and they forgot about it.
[01:54:31.640 --> 01:54:33.080]   So here's the rub down.
[01:54:33.080 --> 01:54:40.120]   Okay. This was the IE three years where they were really going against Netflix.
[01:54:40.120 --> 01:54:46.120]   And can you get the Bleat button ready because a senior Microsoft executive
[01:54:46.120 --> 01:54:50.280]   said our job was to Netflix so hard.
[01:54:50.280 --> 01:54:51.960]   They could never come back to us.
[01:54:52.920 --> 01:54:54.040]   That's good.
[01:54:54.040 --> 01:54:54.840]   That's good.
[01:54:54.840 --> 01:54:55.560]   That's good.
[01:54:55.560 --> 01:54:56.120]   That's good.
[01:54:56.120 --> 01:54:58.600]   Sorry, Netflix. Sorry.
[01:54:58.600 --> 01:55:00.040]   No, but you know what?
[01:55:00.040 --> 01:55:03.160]   I remember when IE three came out and Microsoft was giving it away.
[01:55:03.160 --> 01:55:04.760]   Netscape was charging.
[01:55:04.760 --> 01:55:07.000]   It was very clear that it was all over for Netscape.
[01:55:07.000 --> 01:55:09.480]   In fact, it wasn't much longer before Netscape did have to go on.
[01:55:09.480 --> 01:55:16.360]   Well, I mean, they paid us to send out screenshots with IE gaming screenshots with IE
[01:55:16.360 --> 01:55:17.480]   rather than Netscape.
[01:55:17.480 --> 01:55:21.720]   Basically, the idea was to bombard everyone.
[01:55:22.360 --> 01:55:24.360]   Kill Netscape completely.
[01:55:24.360 --> 01:55:25.800]   IE is free.
[01:55:25.800 --> 01:55:27.800]   Active X is great.
[01:55:27.800 --> 01:55:30.040]   It's a really good security solution.
[01:55:30.040 --> 01:55:32.600]   I tried to explain to you.
[01:55:32.600 --> 01:55:35.400]   Oh my God, it wasn't a security nightmare.
[01:55:35.400 --> 01:55:36.120]   Are you kidding?
[01:55:36.120 --> 01:55:37.160]   Oh, no, exactly.
[01:55:37.160 --> 01:55:41.400]   Random code from the internet running on your machine with full privilege.
[01:55:41.400 --> 01:55:43.720]   No, that was a big real one.
[01:55:43.720 --> 01:55:48.680]   You've got to understand Microsoft was running scared
[01:55:49.240 --> 01:55:51.640]   because they'd completely missed internet.
[01:55:51.640 --> 01:55:55.640]   And then they were trying to come back and use their position to dominate.
[01:55:55.640 --> 01:55:59.640]   And this was when they were using every particular...
[01:55:59.640 --> 01:56:01.720]   The bad old days of Microsoft, when it was...
[01:56:01.720 --> 01:56:03.720]   They were bloodthirsty.
[01:56:03.720 --> 01:56:05.240]   They were Darth Vader.
[01:56:05.240 --> 01:56:06.120]   Darth Vader.
[01:56:06.120 --> 01:56:07.000]   Absolutely.
[01:56:07.000 --> 01:56:10.520]   Honestly, that's still a bit bloodthirsty, but you know...
[01:56:10.520 --> 01:56:16.680]   You guys are making me feel like I've had my eyes closed to all this stuff.
[01:56:16.680 --> 01:56:19.560]   And it sounds pretty corrupt.
[01:56:19.560 --> 01:56:24.680]   Wow, I just want to reassure anybody who listens to any of our shows.
[01:56:24.680 --> 01:56:27.880]   That we do not engage in any of those practices.
[01:56:27.880 --> 01:56:30.520]   For years, I've refused even to talk to PR people.
[01:56:30.520 --> 01:56:33.000]   We try to...
[01:56:33.000 --> 01:56:36.040]   We've always tried to be objective about everything we talk about.
[01:56:36.040 --> 01:56:38.200]   This is so depressing.
[01:56:38.200 --> 01:56:42.200]   Okay, Leo, you should start a game show.
[01:56:42.200 --> 01:56:45.480]   You should start a video game show where you were doing that
[01:56:45.480 --> 01:56:47.480]   and see how the video game industry can be.
[01:56:47.480 --> 01:56:48.680]   Yeah, but that's weird.
[01:56:48.680 --> 01:56:51.320]   Because if you think the tech industry is bad,
[01:56:51.320 --> 01:56:55.720]   they will freeze you out from copies of the game.
[01:56:55.720 --> 01:56:56.360]   They will.
[01:56:56.360 --> 01:56:57.320]   Oh, it is.
[01:56:57.320 --> 01:56:57.880]   The cut-through.
[01:56:57.880 --> 01:56:58.760]   It is the absolute worst.
[01:56:58.760 --> 01:57:02.360]   And I always assumed that car magazines, for instance,
[01:57:02.360 --> 01:57:05.080]   probably were in the pockets.
[01:57:05.080 --> 01:57:05.880]   This is the problem.
[01:57:05.880 --> 01:57:09.480]   When you're covering an industry, you have to have a relationship.
[01:57:09.480 --> 01:57:14.120]   It's the old Beltway politics problem is you have to have a relationship with the people you're covering.
[01:57:14.120 --> 01:57:17.000]   And it pretty soon becomes difficult to be completely objective.
[01:57:17.000 --> 01:57:22.600]   Because you're friends with these people and eventually maybe
[01:57:22.600 --> 01:57:29.000]   you're getting paid by them or sleeping with them or they're taking you out to dinner or whatever,
[01:57:29.000 --> 01:57:32.040]   it gets pretty incestuous.
[01:57:32.040 --> 01:57:39.000]   This is the problem with Apple for so many years in that there were the 12 disciples of Apple who
[01:57:41.000 --> 01:57:43.240]   people who got Apple kit ahead of.
[01:57:43.240 --> 01:57:43.880]   Wasn't 12.
[01:57:43.880 --> 01:57:45.400]   It was four.
[01:57:45.400 --> 01:57:51.560]   The iPhone went to Walt Mossberg, Ed Big at USA Today, David Pogue at the New York Times.
[01:57:51.560 --> 01:57:52.120]   And who else?
[01:57:52.120 --> 01:57:54.040]   Four men.
[01:57:54.040 --> 01:57:54.600]   Four men.
[01:57:54.600 --> 01:57:55.320]   Of course, men.
[01:57:55.320 --> 01:57:57.400]   We wouldn't give it to women.
[01:57:57.400 --> 01:57:58.600]   But that's changed, by the way.
[01:57:58.600 --> 01:58:02.440]   Joanna Stern now gets plenty of love.
[01:58:02.440 --> 01:58:03.000]   Yes, more good.
[01:58:03.000 --> 01:58:03.480]   Yes, more.
[01:58:03.480 --> 01:58:03.720]   Yes, more.
[01:58:03.720 --> 01:58:06.360]   Plenty of love from the big companies.
[01:58:06.360 --> 01:58:09.240]   You, Florence, I don't know over at Gizmodo.
[01:58:09.800 --> 01:58:13.000]   I heard she just got a free home pod.
[01:58:13.000 --> 01:58:15.960]   For testing.
[01:58:15.960 --> 01:58:16.920]   For testing.
[01:58:16.920 --> 01:58:18.120]   For temporary.
[01:58:18.120 --> 01:58:19.320]   And you're going to send it back.
[01:58:19.320 --> 01:58:20.280]   It doesn't belong to me.
[01:58:20.280 --> 01:58:21.240]   You're going to send it back.
[01:58:21.240 --> 01:58:21.320]   You're going to send it back.
[01:58:21.320 --> 01:58:22.840]   I had to sign a contract.
[01:58:22.840 --> 01:58:23.560]   That's a fine.
[01:58:23.560 --> 01:58:24.680]   In fact, that's appropriate.
[01:58:24.680 --> 01:58:27.640]   When I say I get something in, I had to sign a contract.
[01:58:27.640 --> 01:58:27.640]   Yes.
[01:58:27.640 --> 01:58:30.600]   I have to have a story plan.
[01:58:30.600 --> 01:58:31.880]   You cannot just get this.
[01:58:31.880 --> 01:58:35.080]   No, but that was always, in fact, I'm glad that you're covering Apple because that was
[01:58:35.080 --> 01:58:36.760]   always my problem with Apple coverage.
[01:58:37.400 --> 01:58:41.960]   There are these people around the inside who gets, you know,
[01:58:41.960 --> 01:58:43.800]   basically groomed by Apple.
[01:58:43.800 --> 01:58:47.640]   Walt Mossberg was groomed by Steve Jobs.
[01:58:47.640 --> 01:58:51.000]   And as a result, was very friendly to Apple.
[01:58:51.000 --> 01:58:53.320]   And Apple's very good at doing that.
[01:58:53.320 --> 01:58:57.640]   And I never was on the inside in this Apple crowd.
[01:58:57.640 --> 01:58:59.960]   You know, when you go to a Mac world expo,
[01:58:59.960 --> 01:59:01.960]   I wasn't part of the party crowd.
[01:59:01.960 --> 01:59:06.120]   I got so much hate for calling Walt Mossberg and Apple Hall.
[01:59:07.080 --> 01:59:10.200]   That was just the amount.
[01:59:10.200 --> 01:59:12.920]   I mean, I literally got a death threat from that one.
[01:59:12.920 --> 01:59:13.480]   Yeah.
[01:59:13.480 --> 01:59:14.520]   Of course, you didn't like.
[01:59:14.520 --> 01:59:15.240]   I mean, I got a--
[01:59:15.240 --> 01:59:17.000]   I never saw a fight with him too.
[01:59:17.000 --> 01:59:19.160]   No, I have a lot of respect for Walt.
[01:59:19.160 --> 01:59:20.360]   I think he's a great guy.
[01:59:20.360 --> 01:59:24.600]   He probably knew he was being groomed, but he got access.
[01:59:24.600 --> 01:59:25.400]   And this is the problem.
[01:59:25.400 --> 01:59:26.760]   It's a tit for tat.
[01:59:26.760 --> 01:59:27.720]   Now he's got access.
[01:59:27.720 --> 01:59:29.000]   He's talking with Steve Jobs.
[01:59:29.000 --> 01:59:30.280]   He's sitting down with him.
[01:59:30.280 --> 01:59:34.920]   And this is, you know, unfortunately, you're not going to see CEOs on our shows.
[01:59:34.920 --> 01:59:38.680]   You're not going to see people from companies on my shows anyway.
[01:59:38.680 --> 01:59:41.000]   Because I don't want to do that.
[01:59:41.000 --> 01:59:41.720]   I won't do that.
[01:59:41.720 --> 01:59:44.040]   But this is why I love this show.
[01:59:44.040 --> 01:59:46.680]   But I mean, this is why I love this show.
[01:59:46.680 --> 01:59:50.120]   You actually allow people to speak, honestly.
[01:59:50.120 --> 01:59:53.960]   Whereas, you know, for so much of this stuff is couch.
[01:59:53.960 --> 01:59:55.720]   I don't want to lose my access.
[01:59:55.720 --> 01:59:58.360]   Access journalism is a losing game.
[01:59:58.360 --> 02:00:02.840]   All you're giving them is the ability to put their particular opinion out there.
[02:00:02.840 --> 02:00:07.720]   And Gizmodo has done this better than many, of course, on Reg.
[02:00:07.720 --> 02:00:14.520]   So we argue, but, you know, access journalism is just it gives you nothing.
[02:00:14.520 --> 02:00:16.840]   You just basically repeat.
[02:00:16.840 --> 02:00:20.920]   And it's all happening all over again, in my opinion on YouTube.
[02:00:20.920 --> 02:00:23.560]   And it's become very difficult to tell who's.
[02:00:23.560 --> 02:00:25.160]   Oh, it's very, yes.
[02:00:25.160 --> 02:00:26.520]   It's happening there.
[02:00:26.520 --> 02:00:32.040]   It's also happening on TikTok because I know like a lot of people want to dismiss TikTok
[02:00:32.040 --> 02:00:34.760]   is just another social media, but it is essentially a YouTube.
[02:00:34.760 --> 02:00:36.600]   A lot of people go there to get information.
[02:00:36.600 --> 02:00:39.960]   And, you know, the universe is a resource.
[02:00:39.960 --> 02:00:44.840]   In some defense, your generation, most of the journalists of your generation,
[02:00:44.840 --> 02:00:46.200]   didn't come from J School.
[02:00:46.200 --> 02:00:51.960]   They weren't schooled in this whole notion of separation of church and state and stuff.
[02:00:51.960 --> 02:00:53.640]   They literally don't know any better.
[02:00:53.640 --> 02:00:54.520]   They're just, you.
[02:00:54.520 --> 02:00:56.600]   Which is hard for me because I did go to school.
[02:00:56.600 --> 02:00:57.080]   You did.
[02:00:57.080 --> 02:00:57.800]   You did.
[02:00:57.800 --> 02:00:58.120]   Right.
[02:00:58.120 --> 02:01:00.920]   I graduated from San Francisco State University.
[02:01:00.920 --> 02:01:01.560]   Very good school.
[02:01:01.560 --> 02:01:02.040]   It's a general department.
[02:01:02.040 --> 02:01:02.200]   Yep.
[02:01:02.200 --> 02:01:03.320]   No.
[02:01:03.320 --> 02:01:04.200]   And that's the problem.
[02:01:04.200 --> 02:01:08.280]   You get most YouTubers, they're just some guy or gal who, you know,
[02:01:08.280 --> 02:01:10.040]   started to turn on the camera.
[02:01:10.040 --> 02:01:15.240]   And so if their products are proffered to them, or free trips or any of that,
[02:01:15.240 --> 02:01:17.320]   they say, yes, they don't know any different.
[02:01:17.320 --> 02:01:18.920]   So I don't blame them.
[02:01:18.920 --> 02:01:24.680]   Yeah, but also, you know, somebody actually brought this up last night because on TikTok,
[02:01:24.680 --> 02:01:26.520]   it's like, it's very hard being an influencer.
[02:01:26.520 --> 02:01:30.280]   And it's true because it is a lot of work.
[02:01:30.280 --> 02:01:33.080]   And I'm thinking about all the work I had to do to become a journalist,
[02:01:33.080 --> 02:01:37.480]   all the jobs I had to have to be able to pay rent and go to school and get the internships.
[02:01:37.480 --> 02:01:41.560]   But I, you know, there's privilege behind it.
[02:01:41.560 --> 02:01:44.360]   And sometimes like getting this stuff,
[02:01:44.360 --> 02:01:47.880]   having access to this stuff, if it gets you in front of people,
[02:01:47.880 --> 02:01:51.880]   you know, it's, it's hard.
[02:01:51.880 --> 02:01:52.600]   I get it.
[02:01:52.600 --> 02:01:54.120]   It's a balance.
[02:01:54.120 --> 02:01:56.440]   I get it.
[02:01:56.440 --> 02:02:01.480]   To kind of bring this back around to the beginning of the show, something I think about a lot
[02:02:01.480 --> 02:02:08.600]   and something I would love to get y'all's like opinions on this as journalists.
[02:02:08.600 --> 02:02:16.360]   It really seems to me that in the last, really since the Bush administration and the Iraq war,
[02:02:16.360 --> 02:02:25.720]   we've seen a dilution in the power of journalism to create outrage and change with the public,
[02:02:25.720 --> 02:02:30.120]   you know, it's, there's a, there's a lot out there for like entertainment journalism.
[02:02:30.120 --> 02:02:30.360]   Yeah.
[02:02:30.360 --> 02:02:36.120]   I think tech journalism has done well because you're ultimately like buying a product based on that.
[02:02:36.120 --> 02:02:39.480]   We're keeping, you know, up to date on culture.
[02:02:39.480 --> 02:02:49.160]   But I really can't figure out why journalism is an institution has so much less power than it had
[02:02:49.160 --> 02:02:51.960]   when, I mean, I remember an age where it was different.
[02:02:51.960 --> 02:02:57.960]   And I don't know if it's, I don't know if it's just the changing of America where
[02:02:57.960 --> 02:03:00.920]   shame doesn't have the same function that it used to.
[02:03:00.920 --> 02:03:06.120]   I don't know if it's a problem with the institution itself, where there was so much horse race that,
[02:03:06.120 --> 02:03:10.040]   you know, like really holding people accountable is less of what you do.
[02:03:10.040 --> 02:03:17.400]   But like, why is journalism not able to solve the many problems that we have in America the way
[02:03:17.400 --> 02:03:17.960]   it used to?
[02:03:17.960 --> 02:03:20.360]   And we do, and we so desperately need, right?
[02:03:20.360 --> 02:03:20.600]   Yeah.
[02:03:21.720 --> 02:03:24.760]   And there are a lot of reasons for economic reasons.
[02:03:24.760 --> 02:03:25.480]   Absolutely.
[02:03:25.480 --> 02:03:27.400]   Newspapers have failed all over the country.
[02:03:27.400 --> 02:03:33.960]   There is also the problem of this flooding the zone with BS.
[02:03:33.960 --> 02:03:42.840]   There are many people who know that if they can spread enough misinformation that you can't
[02:03:42.840 --> 02:03:49.080]   tell what's right or wrong, if you can undermine the trust people have in their journalism,
[02:03:50.040 --> 02:03:56.520]   if you tell them all it's fake news, then there's no one to tell the truth anymore.
[02:03:56.520 --> 02:03:59.400]   There's no one to speak truth to power.
[02:03:59.400 --> 02:04:01.240]   And then you get away with whatever you want.
[02:04:01.240 --> 02:04:02.840]   And that's clearly happening.
[02:04:02.840 --> 02:04:04.520]   I'm sorry.
[02:04:04.520 --> 02:04:08.680]   I didn't mean to be free or finished, but there are people who don't trust.
[02:04:08.680 --> 02:04:13.400]   Listen, there's a lot of things I don't like about sometimes the New York Times,
[02:04:13.400 --> 02:04:14.520]   like the opinion column.
[02:04:14.520 --> 02:04:17.320]   Oh, yeah, you're right.
[02:04:17.320 --> 02:04:18.440]   I'm hot or cold there.
[02:04:18.440 --> 02:04:19.320]   It's a mixed bag.
[02:04:19.320 --> 02:04:20.280]   It definitely is.
[02:04:20.280 --> 02:04:25.560]   It's a mixed bag, but there are people who just don't trust that there's a paper of record anymore.
[02:04:25.560 --> 02:04:25.880]   No, no.
[02:04:25.880 --> 02:04:28.680]   People hate the view of the record.
[02:04:28.680 --> 02:04:29.240]   People have been...
[02:04:29.240 --> 02:04:34.280]   I mean, I generally think we're in the middle of information shock at the moment.
[02:04:34.280 --> 02:04:35.000]   That's exactly right.
[02:04:35.000 --> 02:04:43.320]   If you think for 60 years ago, the amount of data that you got from outside sources was
[02:04:43.320 --> 02:04:48.520]   your a couple of newspapers, radio station, maybe a couple of TV stations.
[02:04:48.520 --> 02:04:56.360]   Right now, anyone can put a plausibly looking website on their spread spam and get it out.
[02:04:56.360 --> 02:05:00.760]   Journals of record are very, very hard to find.
[02:05:00.760 --> 02:05:06.440]   And I think honestly, humans aren't quite designed to deal with that yet.
[02:05:07.480 --> 02:05:13.240]   So we naturally go for things which coddle our particular opinion.
[02:05:13.240 --> 02:05:13.720]   That's right.
[02:05:13.720 --> 02:05:15.400]   That's right.
[02:05:15.400 --> 02:05:18.920]   But this isn't the first time historically we've had that phenomena.
[02:05:18.920 --> 02:05:22.200]   Like if you look at the, like Samuel Adams, it wasn't a beer.
[02:05:22.200 --> 02:05:23.880]   It was a radical propaganda.
[02:05:23.880 --> 02:05:24.280]   Right.
[02:05:24.280 --> 02:05:27.400]   Like at the beginning of this country, you had the Muckrakers.
[02:05:27.400 --> 02:05:30.360]   I mean, this is not the first time in American history.
[02:05:30.360 --> 02:05:37.320]   You've had disinformation as a large force, even by independent people that are
[02:05:37.320 --> 02:05:40.360]   deeply invested in telling people what they want to hear.
[02:05:40.360 --> 02:05:45.720]   I would point you towards the anti abolition papers during slavery in this country.
[02:05:45.720 --> 02:05:51.960]   And something has changed though, where none of it seems to be having a real effect.
[02:05:51.960 --> 02:05:57.720]   Like the things that we argue about are all kind of talked about in the same spheres by
[02:05:57.720 --> 02:06:01.240]   people that generally speaking hold the same opinions about things.
[02:06:01.240 --> 02:06:02.920]   Yeah.
[02:06:02.920 --> 02:06:04.040]   Yeah.
[02:06:04.040 --> 02:06:04.520]   Yeah.
[02:06:04.520 --> 02:06:05.000]   Yeah.
[02:06:05.000 --> 02:06:05.720]   I agree.
[02:06:06.200 --> 02:06:11.320]   I mean, there's an interesting corollary in 1847, where, you know,
[02:06:11.320 --> 02:06:16.920]   the telegraph and newspapers had started to spread.
[02:06:16.920 --> 02:06:19.240]   The trains had allowed people to move.
[02:06:19.240 --> 02:06:26.520]   And in 1848, we lost three monocles in Europe, just in three days, because
[02:06:26.520 --> 02:06:30.600]   information was spreading and people were getting out there.
[02:06:30.600 --> 02:06:32.680]   Now the amount of disinformation out there,
[02:06:34.280 --> 02:06:36.280]   I've got to say, I'm seriously worried.
[02:06:36.280 --> 02:06:39.480]   You know, this is not good for a society.
[02:06:39.480 --> 02:06:46.200]   And we need to sort out how to, I mean, we had, when I was growing up, we had
[02:06:46.200 --> 02:06:53.720]   media education, which is looking at how the news is spread and how it's done.
[02:06:53.720 --> 02:06:57.480]   We need social media education.
[02:06:57.480 --> 02:07:01.080]   I mean, it's just you need quality.
[02:07:01.800 --> 02:07:05.960]   And, you know, analysis of what is coming on.
[02:07:05.960 --> 02:07:07.160]   And that's not coming.
[02:07:07.160 --> 02:07:11.400]   Let me take a little break, because we are running out of time.
[02:07:11.400 --> 02:07:14.040]   I hate to because I love this conversation.
[02:07:14.040 --> 02:07:17.800]   And I have so many questions for you, especially you.
[02:07:17.800 --> 02:07:20.600]   I want to know what you're talking about there, about pay to play.
[02:07:20.600 --> 02:07:22.920]   We have to have a conversation over.
[02:07:22.920 --> 02:07:24.680]   I'm happy to talk.
[02:07:24.680 --> 02:07:25.160]   Yeah.
[02:07:25.160 --> 02:07:25.480]   Yeah.
[02:07:25.480 --> 02:07:29.640]   But we do need to take a break because we have sponsors.
[02:07:29.640 --> 02:07:31.080]   That's how we pay the bills.
[02:07:31.080 --> 02:07:36.920]   And this is a sponsor I'm proud to be representing, actually, userway.org.
[02:07:36.920 --> 02:07:41.240]   You may not know this, but every website, every single website is a public entity,
[02:07:41.240 --> 02:07:46.680]   which means in the United States, you are bound by the Americans with Disabilities Act,
[02:07:46.680 --> 02:07:47.480]   the ADA.
[02:07:47.480 --> 02:07:50.760]   Your website has to be accessible.
[02:07:50.760 --> 02:07:54.920]   That's not just a good idea, although it is a darn good idea.
[02:07:54.920 --> 02:07:58.120]   It's the law that not easy.
[02:07:58.120 --> 02:08:01.160]   And in fact, if you have a website, you might be a little challenged by the idea of,
[02:08:01.160 --> 02:08:02.760]   well, how do I make my website,
[02:08:02.760 --> 02:08:09.080]   comport with the hundreds, and there are literally hundreds of WCAG guidelines?
[02:08:09.080 --> 02:08:13.080]   WCAG, that's the web content accessibility guidelines.
[02:08:13.080 --> 02:08:15.480]   That's your job.
[02:08:15.480 --> 02:08:18.040]   That's what you got to do, but I got an easy way to do it.
[02:08:18.040 --> 02:08:19.080]   Userway.
[02:08:19.080 --> 02:08:23.880]   In a matter of seconds, userway can do more than an entire team of developer can do in months.
[02:08:25.000 --> 02:08:30.520]   Userway, a simple line of JavaScript can make your site accessible.
[02:08:30.520 --> 02:08:31.960]   In fact, start right now.
[02:08:31.960 --> 02:08:35.800]   Use their free scanning tool at userway.org/twit
[02:08:35.800 --> 02:08:41.640]   just to see what the issues are on your website, how compliant with the ADA your website is.
[02:08:41.640 --> 02:08:48.680]   Then take a look at what userway can do for you, not just for big enterprise sites
[02:08:48.680 --> 02:08:50.120]   with thousands of pages.
[02:08:50.120 --> 02:08:53.880]   And for those sites, they do have a great managed solution where their team can do it all for you.
[02:08:53.880 --> 02:08:58.760]   But even for little sites like ours, we recently added userway to our website.
[02:08:58.760 --> 02:09:02.040]   In fact, you could go to twit.tv, you'll see in the lower right hand corner
[02:09:02.040 --> 02:09:04.840]   that the true VIN man button, you just click on that.
[02:09:04.840 --> 02:09:08.040]   And you'll see what you can do to our website to make it accessible.
[02:09:08.040 --> 02:09:13.640]   We think that's great because we want everybody to be able to use our website.
[02:09:13.640 --> 02:09:22.280]   Userway powers accessibility for over a million websites, Coca-Cola, Disney, eBay,
[02:09:22.840 --> 02:09:24.440]   FedEx, Twit.
[02:09:24.440 --> 02:09:29.080]   And now they're making their best-in-class enterprise level accessibility tools available to small and
[02:09:29.080 --> 02:09:30.040]   medium businesses.
[02:09:30.040 --> 02:09:32.760]   And let me tell you, it was very affordable.
[02:09:32.760 --> 02:09:39.560]   It costs less than the web font we pay for to put on our page so you can see a nice font.
[02:09:39.560 --> 02:09:40.920]   It's that affordable.
[02:09:40.920 --> 02:09:41.960]   And it scales with you.
[02:09:41.960 --> 02:09:45.320]   If it can handle Coca-Cola and Disney, it can handle any site, right?
[02:09:45.320 --> 02:09:49.800]   Userway is the leading accessibility solution on the market today, but 61% market share.
[02:09:49.800 --> 02:09:50.840]   It's the biggest in the world.
[02:09:51.560 --> 02:09:58.440]   As an example, the Motley Fool, big investment advisor site, they had 1911 pages on their website,
[02:09:58.440 --> 02:09:59.960]   20 million page views a month.
[02:09:59.960 --> 02:10:02.280]   They had designed for accessibility.
[02:10:02.280 --> 02:10:06.120]   They knew that was part of the deal, but the development team was spending a lot of time
[02:10:06.120 --> 02:10:07.400]   keeping it up to current standards.
[02:10:07.400 --> 02:10:09.480]   Those standards have really been changing a lot.
[02:10:09.480 --> 02:10:13.080]   Of course, every time you add a page, you have to make sure it's compliant.
[02:10:13.080 --> 02:10:17.880]   So they used Userway as an extra layer of accessibility to make sure the browsing experience
[02:10:17.880 --> 02:10:19.880]   is accessible to everyone.
[02:10:20.360 --> 02:10:23.560]   It uses AI, machine learning, computer vision.
[02:10:23.560 --> 02:10:26.920]   For instance, you know you have to have alt tags on all the images, right?
[02:10:26.920 --> 02:10:30.680]   The computer vision looks at an image and says what it is and writes the description for you.
[02:10:30.680 --> 02:10:32.120]   You can add to that if you want.
[02:10:32.120 --> 02:10:36.840]   In fact, with Userway, you'll get a detailed report of all the things fixed on your website
[02:10:36.840 --> 02:10:40.120]   so you know what was changed and you know what you can do.
[02:10:40.120 --> 02:10:46.760]   If you want to enhance it, it remediates complex nav menus, ensures that all pop-ups are accessible,
[02:10:46.760 --> 02:10:49.880]   fixes vague link violations, fixes broken links,
[02:10:49.880 --> 02:10:53.080]   even makes sure your website makes use of accessible colors.
[02:10:53.080 --> 02:10:56.200]   Still true to your brand, but accessible.
[02:10:56.200 --> 02:10:57.400]   And that's what it's all about.
[02:10:57.400 --> 02:10:59.880]   There are 60 million people with disabilities.
[02:10:59.880 --> 02:11:01.320]   You don't want to leave them out.
[02:11:01.320 --> 02:11:03.160]   And here's the good news.
[02:11:03.160 --> 02:11:04.920]   Userway is easy to use.
[02:11:04.920 --> 02:11:09.640]   A single line of JavaScript, they have plugins for WordPress, Shopify, Wix,
[02:11:09.640 --> 02:11:11.320]   site course, SharePoint.
[02:11:11.320 --> 02:11:17.240]   Every site, every site, Userway can help your business meet its compliance goals
[02:11:17.240 --> 02:11:19.160]   and improve the experience for your users.
[02:11:19.960 --> 02:11:20.840]   The Voice of Series.
[02:11:20.840 --> 02:11:23.000]   Susan Bennett has this to say about Userway.
[02:11:23.000 --> 02:11:26.200]   Hi, I'm Susan Bennett, the original Voice of Series.
[02:11:26.200 --> 02:11:29.160]   You won't hear me say something like this too often.
[02:11:29.160 --> 02:11:33.000]   I'm sorry, I don't understand what you're looking for.
[02:11:33.000 --> 02:11:39.240]   But every day, that's what the internet is like for millions of people with disabilities.
[02:11:39.240 --> 02:11:43.480]   Userway fixes all of that with just one line of code.
[02:11:44.600 --> 02:11:47.240]   I love the idea and I think you will too.
[02:11:47.240 --> 02:11:53.160]   Userway can make any website fully accessible and ADA compliant for a lot less than you think.
[02:11:53.160 --> 02:11:55.480]   With Userway, everyone who visits your site can browse,
[02:11:55.480 --> 02:11:58.040]   seamlessly customize it to fit your needs.
[02:11:58.040 --> 02:12:00.200]   It's how it should be, right?
[02:12:00.200 --> 02:12:04.760]   Just showcase your brand's commitment to the millions of people with disabilities.
[02:12:04.760 --> 02:12:07.160]   Go to userway.org/twit.
[02:12:07.160 --> 02:12:12.200]   You right now get 30% off Userway's AI-powered accessibility solution.
[02:12:12.200 --> 02:12:15.560]   Userway, making the internet accessible for everyone.
[02:12:15.560 --> 02:12:17.960]   This is one I can really get behind.
[02:12:17.960 --> 02:12:21.000]   Userway.org/twit.
[02:12:21.000 --> 02:12:24.120]   Thank you, Userway.org/twit.
[02:12:24.120 --> 02:12:29.240]   All right, we're talking about, oh, before we get to the next subject,
[02:12:29.240 --> 02:12:34.760]   I should mention we have a little movie we made about the week that was on Twitch.
[02:12:34.760 --> 02:12:38.120]   How often did a dad each of you use your sports speaker?
[02:12:38.120 --> 02:12:38.920]   Every day.
[02:12:38.920 --> 02:12:40.360]   It's the most expensive clock.
[02:12:40.360 --> 02:12:41.480]   Oh, I use it.
[02:12:41.480 --> 02:12:45.400]   I get in the bathroom, I play music or I do my vocal warmups, I say,
[02:12:45.400 --> 02:12:46.520]   wait, wait, wait, wait, wait.
[02:12:46.520 --> 02:12:48.600]   We got to hear you do vocal warmups.
[02:12:48.600 --> 02:12:49.720]   I need to hear this.
[02:12:49.720 --> 02:12:50.360]   Ready?
[02:12:50.360 --> 02:12:51.960]   We are starting with the hum.
[02:12:51.960 --> 02:12:59.240]   Mommy, me, me, me, match my M&Ms.
[02:12:59.240 --> 02:13:01.560]   Pre-easily, on to it.
[02:13:01.560 --> 02:13:07.720]   It is time to take a look at the Steam Deck from Valve.
[02:13:07.720 --> 02:13:15.800]   I got this and wanted to give it a real try to see if this could serve as my gaming PC.
[02:13:15.800 --> 02:13:20.120]   I've only got my Mac and I wanted to see if this would work as that replacement.
[02:13:20.120 --> 02:13:22.040]   Security now.
[02:13:22.040 --> 02:13:31.000]   Last Tuesday, Mozilla's headline, red Firefox, rolls out total cookie protection by default
[02:13:31.000 --> 02:13:33.000]   to all users worldwide.
[02:13:33.000 --> 02:13:34.920]   I know.
[02:13:34.920 --> 02:13:38.120]   What's most shocking is that to me,
[02:13:38.120 --> 02:13:41.080]   is that it took us this long to get here.
[02:13:41.080 --> 02:13:43.320]   Mac Break Weekly.
[02:13:43.320 --> 02:13:49.400]   Those of us who have been using Macs since 1984 are very familiar with Claris, the cow dog.
[02:13:49.400 --> 02:13:53.240]   Claris, dog cow, pardon me, oh god.
[02:13:53.240 --> 02:13:59.800]   Oh, Claris is back, baby, in the new Mac OS Ventura printer dialogue.
[02:13:59.800 --> 02:14:01.640]   I think it's kind of like someone got a facelift and you're like,
[02:14:01.640 --> 02:14:06.040]   "Oh, you should have done that."
[02:14:06.040 --> 02:14:08.440]   You know, you lost the character of who you are.
[02:14:08.440 --> 02:14:11.960]   Two-it technology isn't always pretty, but we are.
[02:14:11.960 --> 02:14:15.800]   Maybe you shouldn't have done that.
[02:14:15.800 --> 02:14:19.160]   It was a fun week on Twitch.
[02:14:19.160 --> 02:14:24.600]   By the way, Club Twitch members know that because Club Twitch is where you go to have a good time.
[02:14:24.600 --> 02:14:28.920]   Ad-free versions of all of our shows, the fabulous Club Twitch Discord,
[02:14:29.560 --> 02:14:33.320]   where you can not only chat about shows, but see shows that we don't put out.
[02:14:33.320 --> 02:14:37.480]   Things like our untitled Linux show, the Gizfiz, Stacey's Book Club,
[02:14:37.480 --> 02:14:38.680]   our Sci-Fi Book Club.
[02:14:38.680 --> 02:14:43.640]   I got to see what book are we going to read next?
[02:14:43.640 --> 02:14:44.280]   I can't wait.
[02:14:44.280 --> 02:14:45.160]   I'm so excited.
[02:14:45.160 --> 02:14:49.960]   Plus events like members, Fireside Chat coming up.
[02:14:49.960 --> 02:14:52.680]   Alex Lindsey in asked me anything, July 14th.
[02:14:52.680 --> 02:14:56.360]   It's just great fun and the Twi-plus feed makes it even better.
[02:14:56.360 --> 02:14:58.040]   And all of that, all of that,
[02:14:58.600 --> 02:14:59.560]   seven bucks a month.
[02:14:59.560 --> 02:15:02.120]   A couple of cups of coffee and you're in.
[02:15:02.120 --> 02:15:04.360]   Please, it helps us.
[02:15:04.360 --> 02:15:08.200]   It helps us.
[02:15:08.200 --> 02:15:09.400]   It's also-
[02:15:09.400 --> 02:15:11.240]   Showing this thing is really weird.
[02:15:11.240 --> 02:15:13.160]   The home of animated GIFs.
[02:15:13.160 --> 02:15:15.720]   It helps us launch new shows.
[02:15:15.720 --> 02:15:18.360]   We're going to launch a new show with one of our favorite hosts soon
[02:15:18.360 --> 02:15:19.800]   that will be great.
[02:15:19.800 --> 02:15:23.800]   And because new shows don't have advertisers, it's out of pocket,
[02:15:23.800 --> 02:15:25.880]   but with the help of Club Twitch, we can do a lot more.
[02:15:25.880 --> 02:15:31.000]   So please go to twit.tv/clubtwit and join the fun.
[02:15:31.000 --> 02:15:33.480]   I don't see where I-
[02:15:33.480 --> 02:15:36.280]   Maybe it's under the book club.
[02:15:36.280 --> 02:15:37.320]   I want to see what the-
[02:15:37.320 --> 02:15:40.040]   They had a poll.
[02:15:40.040 --> 02:15:43.960]   Oh, looks like it's going to be Clara in the sun.
[02:15:43.960 --> 02:15:45.640]   It was close though.
[02:15:45.640 --> 02:15:46.600]   That was a close vote.
[02:15:46.600 --> 02:15:48.120]   Have you read that?
[02:15:48.120 --> 02:15:50.120]   No, I haven't read any of this.
[02:15:50.120 --> 02:15:54.040]   It's the guy who did, um, remains of the day.
[02:15:55.560 --> 02:15:57.480]   Which was one of my favorite books.
[02:15:57.480 --> 02:15:59.160]   Uh, I really love that.
[02:15:59.160 --> 02:16:00.040]   Ishigawa.
[02:16:00.040 --> 02:16:01.640]   Ishigawa, I think is his name.
[02:16:01.640 --> 02:16:02.920]   And-
[02:16:02.920 --> 02:16:03.640]   Sorry, Leo.
[02:16:03.640 --> 02:16:05.880]   I only read books about talking beer cans.
[02:16:05.880 --> 02:16:06.680]   That's right.
[02:16:06.680 --> 02:16:08.360]   Secret of the talking beer cans.
[02:16:08.360 --> 02:16:09.640]   This is probably, yeah.
[02:16:09.640 --> 02:16:10.840]   This is sci-fi though.
[02:16:10.840 --> 02:16:15.480]   Our book club sci-fi Clara and the sun by Kazuo Ishiguro.
[02:16:15.480 --> 02:16:16.280]   That's right.
[02:16:16.280 --> 02:16:17.000]   Oh, wow.
[02:16:17.000 --> 02:16:19.560]   Oh, they're all their favorite authors.
[02:16:19.560 --> 02:16:20.120]   Yes.
[02:16:20.120 --> 02:16:23.960]   And it's about how artificial intelligence might
[02:16:23.960 --> 02:16:25.480]   play a role in our futures.
[02:16:25.480 --> 02:16:29.000]   A poignant meditation on love and loneliness.
[02:16:29.000 --> 02:16:32.200]   And we picked last time it was Neil Stevenson's termination check.
[02:16:32.200 --> 02:16:34.760]   It was so long that Ant gave up.
[02:16:34.760 --> 02:16:40.360]   So this time, this time we got a little bit shorter, shorter books.
[02:16:40.360 --> 02:16:41.720]   Only 320 pages.
[02:16:41.720 --> 02:16:42.520]   Wait a minute.
[02:16:42.520 --> 02:16:44.520]   I was promised a short book.
[02:16:44.520 --> 02:16:47.240]   I love Neil Stevenson so much.
[02:16:47.240 --> 02:16:48.200]   I do too.
[02:16:48.200 --> 02:16:48.760]   He just-
[02:16:48.760 --> 02:16:51.080]   A) he can't nail endings.
[02:16:51.080 --> 02:16:51.720]   No.
[02:16:51.720 --> 02:16:56.120]   And B) he just goes too far.
[02:16:56.120 --> 02:16:56.760]   This is-
[02:16:56.760 --> 02:16:59.640]   The termination shock is a perfect example of that.
[02:16:59.640 --> 02:17:02.600]   But I love his research.
[02:17:02.600 --> 02:17:04.200]   I love like-
[02:17:04.200 --> 02:17:05.400]   I learned stuff from him.
[02:17:05.400 --> 02:17:07.320]   I learned about meth alligators.
[02:17:07.320 --> 02:17:15.400]   I learned about the line of actual control between China and India.
[02:17:15.400 --> 02:17:18.520]   Up in the Himalayas, I learned about a-
[02:17:19.080 --> 02:17:21.880]   Using sulfur dioxide to end climate change.
[02:17:21.880 --> 02:17:25.240]   I mean, there's a character in there, a cowboy,
[02:17:25.240 --> 02:17:28.600]   who's built a giant six-shooter that's shooting sulfur dioxide-
[02:17:28.600 --> 02:17:32.280]   sulfur into the sky, into the stratosphere to
[02:17:32.280 --> 02:17:34.040]   increase the risk of albedo.
[02:17:34.040 --> 02:17:36.440]   So climate change is ending.
[02:17:36.440 --> 02:17:37.640]   I'm still waiting for a sword-
[02:17:37.640 --> 02:17:38.200]   I have a lot of-
[02:17:38.200 --> 02:17:38.680]   I have a lot of-
[02:17:38.680 --> 02:17:39.000]   Sorry.
[02:17:39.000 --> 02:17:40.680]   No, no, no, it's okay.
[02:17:40.680 --> 02:17:41.000]   God.
[02:17:41.000 --> 02:17:44.920]   I was going to say, I'm still waiting for his sword-fliting game because he was-
[02:17:44.920 --> 02:17:47.880]   Well, he has a cryptocurrency now.
[02:17:48.520 --> 02:17:51.560]   Well, I mean, I saw him black out a couple of years ago and he was like,
[02:17:51.560 --> 02:17:55.640]   "I'm working on a proper physical sword-fighting game."
[02:17:55.640 --> 02:17:55.960]   Wow.
[02:17:55.960 --> 02:17:57.560]   And that hasn't happened.
[02:17:57.560 --> 02:17:59.880]   And cryptocurrency I've read about.
[02:17:59.880 --> 02:18:03.880]   Yeah, I mean, he's a great author in so many ways.
[02:18:03.880 --> 02:18:05.960]   He makes lousy endings to his books.
[02:18:05.960 --> 02:18:07.080]   Yeah, but who cares?
[02:18:07.080 --> 02:18:08.040]   I don't care how it ends.
[02:18:08.040 --> 02:18:09.400]   Hey, come on.
[02:18:09.400 --> 02:18:11.640]   Oh, this is Clark screwed up so many times.
[02:18:11.640 --> 02:18:12.600]   Yes, there you go.
[02:18:12.600 --> 02:18:13.560]   I mean, he actually-
[02:18:13.560 --> 02:18:15.160]   What's the ending of the 3,000-
[02:18:15.160 --> 02:18:16.600]   What the hell was that work?
[02:18:16.600 --> 02:18:17.080]   Yeah.
[02:18:18.200 --> 02:18:21.160]   I have to- I just want to push back on that a little bit.
[02:18:21.160 --> 02:18:26.840]   I have come to appreciate his endings.
[02:18:26.840 --> 02:18:28.760]   They are unconventional.
[02:18:28.760 --> 02:18:33.800]   I think his ending for Snow Crash is one of the greatest endings of any novel ever.
[02:18:33.800 --> 02:18:34.280]   It certainly-
[02:18:34.280 --> 02:18:34.280]   It certainly-
[02:18:34.280 --> 02:18:36.840]   I don't remember the ending but the beginning.
[02:18:36.840 --> 02:18:37.720]   Oh, wow.
[02:18:37.720 --> 02:18:38.280]   It's the beginning of it.
[02:18:38.280 --> 02:18:38.760]   Oh, really?
[02:18:38.760 --> 02:18:39.800]   Oh, it's wonderful.
[02:18:39.800 --> 02:18:40.520]   But why do you-
[02:18:40.520 --> 02:18:43.880]   The final line of it is like, "Home, take me home sounds good."
[02:18:43.880 --> 02:18:44.360]   You know?
[02:18:44.360 --> 02:18:46.680]   And she's got this huge adventure and she's like-
[02:18:46.680 --> 02:18:47.720]   Yeah.
[02:18:47.720 --> 02:18:48.440]   With her mom.
[02:18:48.440 --> 02:18:50.360]   Or Diamond Age,
[02:18:50.360 --> 02:18:53.480]   like where now Princess Nell is under the ocean,
[02:18:53.480 --> 02:18:55.800]   and she kisses the drummers and Miranda,
[02:18:55.800 --> 02:18:59.160]   and then she's the new queen of this new republic that came.
[02:18:59.160 --> 02:19:00.280]   That was awesome.
[02:19:00.280 --> 02:19:02.360]   That took me a while to accept.
[02:19:02.360 --> 02:19:04.520]   But I think termination shock,
[02:19:04.520 --> 02:19:05.400]   it's the same way.
[02:19:05.400 --> 02:19:07.880]   I mean, I don't list foil it because it's a newer book,
[02:19:07.880 --> 02:19:08.920]   but it's like,
[02:19:08.920 --> 02:19:10.440]   you have this character that's like,
[02:19:10.440 --> 02:19:11.960]   "What do we do now?"
[02:19:11.960 --> 02:19:15.880]   And it was so in her character that I kind of liked that.
[02:19:15.880 --> 02:19:17.400]   So I don't-
[02:19:17.400 --> 02:19:19.400]   You see, I mean, I thought Snow Crash was-
[02:19:19.400 --> 02:19:27.880]   It kind of obviated all the badass stuff that she'd done.
[02:19:27.880 --> 02:19:31.400]   And I kind of get it that,
[02:19:31.400 --> 02:19:35.320]   you know, she went home and the rest of it.
[02:19:35.320 --> 02:19:39.000]   Diamond Age, yeah, very good.
[02:19:39.000 --> 02:19:39.960]   But-
[02:19:39.960 --> 02:19:41.960]   Seven Eaves.
[02:19:41.960 --> 02:19:42.920]   I said to them.
[02:19:42.920 --> 02:19:44.520]   People go off like the ending of that.
[02:19:45.240 --> 02:19:46.600]   I loved it. I didn't mind.
[02:19:46.600 --> 02:19:48.920]   But see, part of the problem is you want more.
[02:19:48.920 --> 02:19:51.640]   Oh, of course.
[02:19:51.640 --> 02:19:52.840]   He's a great writer.
[02:19:52.840 --> 02:19:54.440]   You don't want it to end.
[02:19:54.440 --> 02:19:56.600]   So that's part of the problem, right?
[02:19:56.600 --> 02:19:59.560]   No, I'm the same way we were sort of grinning,
[02:19:59.560 --> 02:20:00.200]   with-
[02:20:00.200 --> 02:20:01.720]   as you were grinning the rest of it.
[02:20:01.720 --> 02:20:02.600]   I want more.
[02:20:02.600 --> 02:20:03.080]   Yeah.
[02:20:03.080 --> 02:20:05.560]   But, you know, there was that kind of feeling
[02:20:05.560 --> 02:20:08.360]   that Stevenson couldn't do endings well.
[02:20:08.360 --> 02:20:12.600]   I don't know if I'm at this endings.
[02:20:12.600 --> 02:20:17.480]   I think his problem is his best books were earlier in his career,
[02:20:17.480 --> 02:20:20.920]   where he was not successful enough that he could not tell his editor,
[02:20:20.920 --> 02:20:23.160]   we're going to do it my way.
[02:20:23.160 --> 02:20:26.600]   And because of that, all of his books have become books by the pound,
[02:20:26.600 --> 02:20:30.920]   which is one of seven Eaves is so too long.
[02:20:30.920 --> 02:20:33.880]   And the broke cycle is way, way, way too long.
[02:20:33.880 --> 02:20:34.360]   Yes.
[02:20:34.360 --> 02:20:36.040]   No, I agree with you.
[02:20:36.040 --> 02:20:36.440]   Yeah.
[02:20:36.440 --> 02:20:37.480]   I agree with you, totally.
[02:20:37.480 --> 02:20:37.880]   I have-
[02:20:37.880 --> 02:20:41.240]   I'm not going to get up, but I have the first-
[02:20:42.040 --> 02:20:45.800]   first editions of the broke cycle down in that bookcase.
[02:20:45.800 --> 02:20:49.800]   And he's got Dickens syndrome.
[02:20:49.800 --> 02:20:52.200]   No editor is going to say to him,
[02:20:52.200 --> 02:20:58.360]   "Look, just cut this down and, you know, just get it together."
[02:20:58.360 --> 02:21:03.240]   Dickens got paid by the word, so Dickens had a good excuse for it.
[02:21:03.240 --> 02:21:04.520]   Well, no, exactly.
[02:21:04.520 --> 02:21:10.840]   But, you know, I mean, I literally camped out outside a bookshop in London,
[02:21:10.840 --> 02:21:15.640]   just to get the first editions of the broke cycle, and I was so disappointed.
[02:21:15.640 --> 02:21:18.440]   Yeah, I had high hopes for that too, and it was a-
[02:21:18.440 --> 02:21:19.480]   But you know what?
[02:21:19.480 --> 02:21:21.880]   It's one of- like a lot of his books, the journey is-
[02:21:21.880 --> 02:21:23.480]   you have to really appreciate the journey.
[02:21:23.480 --> 02:21:27.480]   So the broke cycle, if you just don't-
[02:21:27.480 --> 02:21:29.240]   you don't feel like you're trying to get to the end or-
[02:21:29.240 --> 02:21:32.840]   they're trying to be a story or anything, you're just there.
[02:21:32.840 --> 02:21:35.000]   It's just happening all around you.
[02:21:35.000 --> 02:21:37.080]   Ah, he screwed it in the second book.
[02:21:37.080 --> 02:21:40.200]   You know, the whole recovering from Sifla's thing was-
[02:21:41.160 --> 02:21:42.920]   I- sorry, no spoilers.
[02:21:42.920 --> 02:21:45.160]   It's hard when you're recovering from Sifla's.
[02:21:45.160 --> 02:21:46.200]   It's hard to make that it.
[02:21:46.200 --> 02:21:50.120]   An exciting part of any novel, I think.
[02:21:50.120 --> 02:21:51.560]   But who knows?
[02:21:51.560 --> 02:21:54.120]   The show is now this week, and Neil Stevens says-
[02:21:54.120 --> 02:21:54.840]   Yeah, sorry.
[02:21:54.840 --> 02:21:56.520]   Sorry.
[02:21:56.520 --> 02:21:57.160]   You're more hydrated.
[02:21:57.160 --> 02:22:00.920]   Hey, what do you think of the "nothing" phone, Florence Ion?
[02:22:00.920 --> 02:22:01.640]   This was the phone-
[02:22:01.640 --> 02:22:03.800]   I think nothing about it.
[02:22:03.800 --> 02:22:04.760]   It doesn't exist.
[02:22:04.760 --> 02:22:05.400]   It's nothing.
[02:22:05.400 --> 02:22:05.720]   Oh.
[02:22:05.720 --> 02:22:07.480]   Broly.
[02:22:07.480 --> 02:22:08.120]   Well, Mark has-
[02:22:08.120 --> 02:22:09.320]   Brownlee has one.
[02:22:09.320 --> 02:22:10.920]   Okay.
[02:22:10.920 --> 02:22:13.960]   I mean, Mark has Brownlee has a lot of things-
[02:22:13.960 --> 02:22:15.080]   That no one else has.
[02:22:15.080 --> 02:22:15.960]   Including-
[02:22:15.960 --> 02:22:18.520]   I do plan on going full force.
[02:22:18.520 --> 02:22:22.680]   I feel like this is a good time for me to bring up that I'll be on all about Android
[02:22:22.680 --> 02:22:24.120]   this coming week.
[02:22:24.120 --> 02:22:27.000]   Are you going to show it off on all about Android?
[02:22:27.000 --> 02:22:30.520]   No, I don't have the phone, but I have a lot of opinions.
[02:22:30.520 --> 02:22:30.840]   Yes.
[02:22:30.840 --> 02:22:35.560]   This- this for some reason, they put lights on the back, even though-
[02:22:36.600 --> 02:22:39.160]   Who doesn't put a case on the phone, right?
[02:22:39.160 --> 02:22:41.080]   900 LEDs?
[02:22:41.080 --> 02:22:42.280]   Wow.
[02:22:42.280 --> 02:22:43.880]   I mean, that's kind of cool.
[02:22:43.880 --> 02:22:45.080]   I guess I won't put it.
[02:22:45.080 --> 02:22:51.240]   So this "nothing" phone, Carl Pei created nothing out of nothing.
[02:22:51.240 --> 02:22:52.440]   No, not out of nothing.
[02:22:52.440 --> 02:22:54.520]   Didn't he buy Andy Rubin's essential?
[02:22:54.520 --> 02:23:00.840]   Yeah, which let's also not forget what Andy Rubin disgraced Andy Rubin.
[02:23:00.840 --> 02:23:02.200]   Yes, the disgraced Andy Rubin.
[02:23:02.840 --> 02:23:09.080]   And by the way, the essential phone was a disgrace as well because I bought it and it never did anything.
[02:23:09.080 --> 02:23:11.800]   Never came out with any of those modules.
[02:23:11.800 --> 02:23:17.320]   I remember when you let us play with it for all about Android and I remember just like-
[02:23:17.320 --> 02:23:17.640]   What's the point?
[02:23:17.640 --> 02:23:22.840]   I want to get excited about these phone releases, but it really is like a Samsung
[02:23:22.840 --> 02:23:28.440]   Google world in the Android realm and a lot of what I'm seeing with the marketing around the
[02:23:28.440 --> 02:23:33.640]   nothing phone reminds me. It just feels like Deja Vu because I was there when OnePlus
[02:23:33.640 --> 02:23:34.840]   originally announced.
[02:23:34.840 --> 02:23:41.800]   And he was the founder of OnePlus, which has now been sucked back into the body cavity of Oppo.
[02:23:41.800 --> 02:23:50.200]   Oppo, a parent company, BBK, but at the same time, one plus is in carriers here in the United
[02:23:50.200 --> 02:23:53.480]   States and the nothing phone will not be.
[02:23:53.480 --> 02:23:55.480]   They're not going to sell it here at all, are they?
[02:23:56.680 --> 02:24:01.400]   They're not going to sell it here at all. No, you could technically buy it, I'm sure,
[02:24:01.400 --> 02:24:02.600]   and use it in the United States.
[02:24:02.600 --> 02:24:07.480]   Well, they say it's not fully supported in North America, which means you might be unhappy.
[02:24:07.480 --> 02:24:08.280]   Which do they have to say?
[02:24:08.280 --> 02:24:12.280]   All right.
[02:24:12.280 --> 02:24:17.480]   I'm not impressed with that. It's good luck taking on the Apple.
[02:24:17.480 --> 02:24:21.480]   Android 2, Opoly, best of luck.
[02:24:23.720 --> 02:24:26.840]   Well, do we need another phone? I mean, we've got some good choices.
[02:24:26.840 --> 02:24:29.960]   We've got Google. We've got Samsung. We've got Apple. Do we need?
[02:24:29.960 --> 02:24:33.400]   I mean, we don't want competition in this space. We just talked about like,
[02:24:33.400 --> 02:24:37.000]   the being the show privacy policy and why that's important.
[02:24:37.000 --> 02:24:40.600]   And, you know, like, I think competition would be good.
[02:24:40.600 --> 02:24:45.800]   I just don't know how this space expands to really have competition at this point.
[02:24:45.800 --> 02:24:51.480]   Resources, which they don't have, which is why a lot of what we see that's new is just a
[02:24:51.480 --> 02:24:55.000]   re-prised Android phone with a different skin on top of it.
[02:24:55.000 --> 02:24:57.720]   Even if it is, like, this is our version of Android.
[02:24:57.720 --> 02:25:02.360]   It's like, well, it's still built on top of Google's Android and it still has Google Play
[02:25:02.360 --> 02:25:06.040]   Services, which means it's another Android phone.
[02:25:06.040 --> 02:25:13.480]   I mean, obviously, as tech reporters, we want excitement and whizzy new things.
[02:25:13.480 --> 02:25:18.360]   Do people care? I mean, frankly, if you're looking at Android, all they care about is prices.
[02:25:18.360 --> 02:25:21.080]   No, people care hugely.
[02:25:21.080 --> 02:25:21.640]   Oh.
[02:25:21.640 --> 02:25:25.960]   And I've got to say, one of the things that really pissed me off about the move from
[02:25:25.960 --> 02:25:30.920]   Nexus to Pixel was they went from a stripped down operating system to,
[02:25:30.920 --> 02:25:36.440]   we're going to tarp this up like a, you know, a 1998 MySpace page.
[02:25:36.440 --> 02:25:41.000]   They're going to stick all this stuff up there.
[02:25:41.000 --> 02:25:50.760]   But to see, Fairie and Pixel is a much better device than the Nexus's were in terms of
[02:25:51.080 --> 02:25:52.200]   what it can actually do.
[02:25:52.200 --> 02:25:53.480]   The cameras are much better.
[02:25:53.480 --> 02:25:54.280]   Absolutely.
[02:25:54.280 --> 02:25:54.280]   Yeah.
[02:25:54.280 --> 02:25:57.560]   I mean, the Nexus was a bog-standard phone.
[02:25:57.560 --> 02:25:59.800]   It was developed at the end of the day.
[02:25:59.800 --> 02:26:02.920]   What do you want from a phone?
[02:26:02.920 --> 02:26:07.160]   Do you want something that makes calls and allows you to access the internet?
[02:26:07.160 --> 02:26:10.600]   Or do you want all the Hoopy features that are out there?
[02:26:10.600 --> 02:26:11.720]   I'm sorry.
[02:26:11.720 --> 02:26:17.880]   I just want a phone which is secure and which does the basic job.
[02:26:17.880 --> 02:26:22.120]   And I understand that's not entirely popular view.
[02:26:22.120 --> 02:26:28.840]   But at the same time, you know, Nexus was a great developer platform.
[02:26:28.840 --> 02:26:33.640]   Pixel, once they started tarding it up, I have no choice.
[02:26:33.640 --> 02:26:35.160]   Because it's about the assistant.
[02:26:35.160 --> 02:26:36.280]   It's not about Android.
[02:26:36.280 --> 02:26:38.920]   It's about the assistant and the ecosystem.
[02:26:38.920 --> 02:26:41.640]   The hot new phone in for all mankind.
[02:26:41.640 --> 02:26:43.320]   What it.
[02:26:43.320 --> 02:26:45.400]   God.
[02:26:45.400 --> 02:26:47.480]   So this is Apple TV.
[02:26:47.480 --> 02:26:48.680]   Put it with five.
[02:26:48.680 --> 02:26:51.320]   This is Apple TV's revisionist show.
[02:26:51.320 --> 02:26:51.880]   That's hilarious.
[02:26:51.880 --> 02:26:53.880]   Where the Soviets won the space race.
[02:26:53.880 --> 02:26:55.080]   They got to the moon first.
[02:26:55.080 --> 02:26:58.280]   It's actually kind of fun for that reason.
[02:26:58.280 --> 02:27:00.440]   I think it's in its third season now.
[02:27:00.440 --> 02:27:05.480]   And at one point, so it takes place in that era in the 60s and 70s.
[02:27:05.480 --> 02:27:08.920]   And at one point, they're making a video call.
[02:27:08.920 --> 02:27:10.840]   It's actually kind of clever.
[02:27:10.840 --> 02:27:15.160]   They took a Newton message pad, put a fake camera which it never had
[02:27:15.160 --> 02:27:15.800]   on top of it.
[02:27:15.800 --> 02:27:18.440]   Although that's what it would have looked like if they'd made it.
[02:27:18.440 --> 02:27:22.280]   But it's embodied in there is that there's actually an iPhone inside
[02:27:22.280 --> 02:27:25.720]   so that it can actually make a video call.
[02:27:25.720 --> 02:27:26.680]   Isn't that a terrible?
[02:27:26.680 --> 02:27:27.080]   Right.
[02:27:27.080 --> 02:27:29.560]   It's like half a play between Apple shows.
[02:27:29.560 --> 02:27:30.200]   Yeah.
[02:27:30.200 --> 02:27:31.000]   What a surprise.
[02:27:31.000 --> 02:27:35.480]   It's half Newton, half, half iPhone 12, Pro Max.
[02:27:35.480 --> 02:27:36.600]   Apple, Palm, Ham Spring,
[02:27:36.600 --> 02:27:40.920]   yeah, yeah, yeah, yeah, I miss handspring so much.
[02:27:40.920 --> 02:27:47.320]   You know, I mean, I loved my Palm three X because it had,
[02:27:47.320 --> 02:27:51.800]   you could plug it into a keyboard and be you had double a battery,
[02:27:51.800 --> 02:27:53.960]   triple a batteries that you put into the back of it.
[02:27:53.960 --> 02:27:56.920]   And it was great as a journalist.
[02:27:56.920 --> 02:28:01.320]   Yeah, but have you gone back and actually tried to use one?
[02:28:01.320 --> 02:28:05.800]   Because I made the mistake of, okay, so just full disclosure.
[02:28:05.800 --> 02:28:08.120]   If you ever read my physical handwriting,
[02:28:08.120 --> 02:28:10.920]   it is destroyed because of Palm graffiti.
[02:28:10.920 --> 02:28:13.000]   Because I write in Palm graffiti.
[02:28:13.000 --> 02:28:14.440]   You write Palm graffiti.
[02:28:14.440 --> 02:28:14.760]   Wow.
[02:28:14.760 --> 02:28:18.680]   I just use it for so many years when I was coming of age.
[02:28:18.680 --> 02:28:19.000]   Yeah.
[02:28:19.000 --> 02:28:23.480]   It is that's how I write my T's and how I write my E's all of it.
[02:28:23.480 --> 02:28:28.680]   So I was very curious to go back and retry this because, you know,
[02:28:28.680 --> 02:28:31.320]   when I was in college, like that Palm tones didn't see,
[02:28:31.320 --> 02:28:32.600]   I took all my notes on it.
[02:28:32.600 --> 02:28:33.960]   It was absolutely amazing.
[02:28:34.760 --> 02:28:35.960]   Don't do this.
[02:28:35.960 --> 02:28:37.000]   Leave us memories.
[02:28:37.000 --> 02:28:37.880]   Unders!
[02:28:37.880 --> 02:28:39.560]   Does happy movie.
[02:28:39.560 --> 02:28:41.080]   That's wonderful.
[02:28:41.080 --> 02:28:42.280]   This is not hold up, y'all.
[02:28:42.280 --> 02:28:44.120]   She writes in graffiti.
[02:28:44.120 --> 02:28:44.840]   That's so cool.
[02:28:44.840 --> 02:28:47.000]   Seriously, I would sit,
[02:28:47.000 --> 02:28:52.280]   I was doing a lot of work with Nokia and Ericsson in Scandinavia.
[02:28:52.280 --> 02:28:58.920]   So I was sitting on a plane writing using my keyboard with the Palm three X plugged in.
[02:28:58.920 --> 02:29:02.440]   And everyone was just like, what the hell is that?
[02:29:02.440 --> 02:29:05.160]   And I was just like, no, this is the way, this is the future.
[02:29:05.160 --> 02:29:08.280]   How about keyboards?
[02:29:08.280 --> 02:29:10.120]   Yeah, they're not the way of the future.
[02:29:10.120 --> 02:29:13.560]   But if you want to see Brandon's handwriting,
[02:29:13.560 --> 02:29:16.920]   here is the Palm pilot graffiti reference card.
[02:29:16.920 --> 02:29:17.320]   That's what it looks like.
[02:29:17.320 --> 02:29:21.640]   So that's how they teach you to do shorthand and J's.
[02:29:21.640 --> 02:29:23.880]   It's kind of like, yeah, the K is just that loopy thing
[02:29:23.880 --> 02:29:29.000]   because they realized that computer handwriting recognition,
[02:29:29.000 --> 02:29:35.400]   which was first attempted in 1993 by the Newton, famously a famous fail,
[02:29:35.400 --> 02:29:37.320]   wasn't going to work.
[02:29:37.320 --> 02:29:40.600]   So when they created the Palm, they said, you know,
[02:29:40.600 --> 02:29:45.560]   we should really have a modified, less ambiguous English alphabet
[02:29:45.560 --> 02:29:46.840]   that the computer can read.
[02:29:46.840 --> 02:29:48.920]   So you see the T is just that left bend.
[02:29:48.920 --> 02:29:51.400]   But actually, if you use graffiti, you can get very fast at it.
[02:29:51.400 --> 02:29:53.560]   It was actually quite well designed.
[02:29:53.560 --> 02:29:54.600]   Oh, yeah, no, definitely.
[02:29:54.600 --> 02:29:55.560]   Yeah, yeah.
[02:29:55.560 --> 02:29:57.240]   The four was a little weird.
[02:29:57.240 --> 02:30:02.120]   But there was a certain eat up Martin element to this.
[02:30:02.120 --> 02:30:06.520]   But yeah, it just, it was good.
[02:30:06.520 --> 02:30:09.400]   They added keyboards to it.
[02:30:09.400 --> 02:30:12.200]   But yeah, technology overcame it.
[02:30:12.200 --> 02:30:12.440]   Yeah.
[02:30:12.440 --> 02:30:17.560]   Oh, you know, speaking of overcoming, I think it's time to go to dinner.
[02:30:17.560 --> 02:30:20.360]   I want to thank all three of you.
[02:30:20.360 --> 02:30:24.200]   You are marvelous, especially Florence and Brianna,
[02:30:24.200 --> 02:30:26.440]   who showed up a short notice.
[02:30:27.160 --> 02:30:29.560]   So that we could have the conversation we did on the show.
[02:30:29.560 --> 02:30:31.160]   And I really appreciate your input.
[02:30:31.160 --> 02:30:33.000]   You guys are wonderful.
[02:30:33.000 --> 02:30:36.920]   Florence, I on long time stalwart at all about Android.
[02:30:36.920 --> 02:30:40.200]   Now, it gives Moto, you do show up on A.A. all the time.
[02:30:40.200 --> 02:30:43.160]   In fact, you said you're going to be on this week to talk about the nothing.
[02:30:43.160 --> 02:30:44.920]   She'll be talking about nothing.
[02:30:44.920 --> 02:30:49.960]   Once a month and this month, next this week is my time.
[02:30:49.960 --> 02:30:52.120]   So glad to have you Florence.
[02:30:52.120 --> 02:30:53.160]   Thank you very much.
[02:30:53.160 --> 02:30:53.640]   Yeah.
[02:30:53.640 --> 02:30:56.280]   And I'm your baby didn't wake up.
[02:30:56.280 --> 02:30:57.480]   So I guess we're okay.
[02:30:57.480 --> 02:30:59.640]   Oh, no, she she was whistling earlier.
[02:30:59.640 --> 02:31:03.560]   I actually apologize to her seat chat if they can hear her whistling.
[02:31:03.560 --> 02:31:04.520]   Oh, wow.
[02:31:04.520 --> 02:31:05.160]   Yeah.
[02:31:05.160 --> 02:31:05.480]   Cute.
[02:31:05.480 --> 02:31:06.680]   I'm sorry.
[02:31:06.680 --> 02:31:10.280]   She didn't toddle in and sit in your lap like the BBC.
[02:31:10.280 --> 02:31:13.480]   But next time we could look forward to that.
[02:31:13.480 --> 02:31:17.080]   Brianna, where you are marvelous, please apologize to Frank.
[02:31:17.080 --> 02:31:20.920]   Once again, for the kerfuffle I caused.
[02:31:20.920 --> 02:31:23.640]   You didn't cause it.
[02:31:23.640 --> 02:31:24.440]   I caused it.
[02:31:24.440 --> 02:31:25.160]   Oh, whatever.
[02:31:25.160 --> 02:31:26.520]   You know, we caused it together.
[02:31:26.520 --> 02:31:27.480]   Let's put it that way.
[02:31:27.480 --> 02:31:28.120]   We got it done.
[02:31:28.120 --> 02:31:32.760]   We made Brianna go out and get a hard wire for her.
[02:31:32.760 --> 02:31:33.240]   And you know what?
[02:31:33.240 --> 02:31:33.720]   It worked.
[02:31:33.720 --> 02:31:34.520]   You sounded great.
[02:31:34.520 --> 02:31:37.000]   Rebellionpack.com.
[02:31:37.000 --> 02:31:38.200]   What is Rebellionpack?
[02:31:38.200 --> 02:31:45.000]   Rebellionpack, if you are interested in doing something about basically the state of this
[02:31:45.000 --> 02:31:54.360]   country without impact, we use large data to basically target voters that other people
[02:31:54.360 --> 02:31:55.080]   don't.
[02:31:55.080 --> 02:32:00.120]   We, you know, campaigns are set up to target what we call triple primes.
[02:32:00.120 --> 02:32:05.640]   People that vote in, you know, national elections, local elections and state elections.
[02:32:05.640 --> 02:32:06.760]   We don't.
[02:32:06.760 --> 02:32:09.320]   We figure those people are going to be contacted.
[02:32:09.320 --> 02:32:13.720]   This is the reason why if you vote, you get people coming to campus you five times.
[02:32:13.720 --> 02:32:16.520]   Because that data is so easily available.
[02:32:16.520 --> 02:32:22.680]   We actually do analysis to figure out the people who are registered to vote but aren't really that
[02:32:22.680 --> 02:32:23.480]   activated.
[02:32:23.480 --> 02:32:24.840]   Oh, that's good.
[02:32:24.840 --> 02:32:25.800]   Yeah.
[02:32:25.800 --> 02:32:26.280]   Yeah.
[02:32:26.280 --> 02:32:33.320]   And we, we create ads that are based on issues that will make a very measurable
[02:32:33.320 --> 02:32:34.760]   difference in their life.
[02:32:34.760 --> 02:32:37.800]   Things like unionization, minimum wage.
[02:32:37.800 --> 02:32:40.840]   I'm sure this time around we will do a ton of ads on choice.
[02:32:40.840 --> 02:32:47.000]   So one of the things I learned when I ran for Congress is that Democratic operatives are
[02:32:47.000 --> 02:32:49.320]   not very good on technology.
[02:32:50.280 --> 02:32:53.320]   They suck and our tools are based in the 90s.
[02:32:53.320 --> 02:32:55.320]   So that's why we started the PAC.
[02:32:55.320 --> 02:33:01.240]   There's a huge need for people there technologically literate to kind of do the data segmentation.
[02:33:01.240 --> 02:33:05.960]   And you have something that more tech journals should have an ethics page.
[02:33:05.960 --> 02:33:07.000]   Yes, we do.
[02:33:07.000 --> 02:33:11.800]   We want to read more about the ethics of a rebellion PAC.
[02:33:11.800 --> 02:33:15.640]   The only do I have to donate through Act Blue?
[02:33:15.640 --> 02:33:16.920]   Can I give it to you directly?
[02:33:16.920 --> 02:33:18.360]   Just give you a check.
[02:33:19.480 --> 02:33:20.360]   Yes, you can.
[02:33:20.360 --> 02:33:22.040]   I'll mail you a check.
[02:33:22.040 --> 02:33:23.240]   It's kind of old fashioned.
[02:33:23.240 --> 02:33:25.800]   But that way you get it all.
[02:33:25.800 --> 02:33:26.760]   I have blues problems.
[02:33:26.760 --> 02:33:29.000]   I've had discussions with them about that.
[02:33:29.000 --> 02:33:30.360]   Yeah, no.
[02:33:30.360 --> 02:33:32.040]   I mean, I've used Act Blue.
[02:33:32.040 --> 02:33:34.520]   That was how I donated in the last few cycles.
[02:33:34.520 --> 02:33:38.760]   But I feel like they take a big cut and I don't know.
[02:33:38.760 --> 02:33:43.400]   I'm not sure I really like the way the dark patterns they use on their site and things like that.
[02:33:43.400 --> 02:33:46.520]   They have their Massachusetts company.
[02:33:46.520 --> 02:33:49.720]   And I'm always thankful for the help they've given us over the years.
[02:33:49.720 --> 02:33:49.720]   Yes.
[02:33:49.720 --> 02:33:51.560]   But they have some issues they need.
[02:33:51.560 --> 02:33:52.600]   They make it easy.
[02:33:52.600 --> 02:33:53.320]   That's the good news.
[02:33:53.320 --> 02:33:53.800]   They do.
[02:33:53.800 --> 02:33:54.280]   Yeah.
[02:33:54.280 --> 02:33:55.320]   Their API is good.
[02:33:55.320 --> 02:33:56.360]   It's secure.
[02:33:56.360 --> 02:33:58.040]   But there are other problems.
[02:33:58.040 --> 02:33:58.520]   All right.
[02:33:58.520 --> 02:34:00.840]   Well, then I'll give I'll give to you through Act Blue.
[02:34:00.840 --> 02:34:02.280]   It's good.
[02:34:02.280 --> 02:34:02.760]   Why not?
[02:34:02.760 --> 02:34:06.920]   Ease of Access is a big part of this, I'm sure.
[02:34:06.920 --> 02:34:09.480]   Rebellion PAC.com.
[02:34:09.480 --> 02:34:10.840]   Thanks for the work you're doing.
[02:34:10.840 --> 02:34:12.280]   We really appreciate it, Browna.
[02:34:12.280 --> 02:34:16.200]   Ian, someday you're going to come up here.
[02:34:16.200 --> 02:34:20.040]   You and I will have a Scotch egg and a pint of Guinness is best.
[02:34:20.040 --> 02:34:21.720]   And we'll talk about it a little bit.
[02:34:21.720 --> 02:34:22.760]   It's such a troll.
[02:34:22.760 --> 02:34:23.560]   We're all that.
[02:34:23.560 --> 02:34:27.800]   Don't leave our fine country.
[02:34:27.800 --> 02:34:29.560]   Stay here and help us make it better.
[02:34:29.560 --> 02:34:30.200]   How about that?
[02:34:30.200 --> 02:34:37.000]   Honestly, I mean, I'm going for citizenship, but at the same time,
[02:34:37.000 --> 02:34:37.720]   I know.
[02:34:37.720 --> 02:34:43.800]   It's kind of like, although let's face it, Boris's Britain isn't great.
[02:34:43.800 --> 02:34:47.640]   No, I think what we should do is we should all move to Vancouver, Canada.
[02:34:47.640 --> 02:34:49.240]   Beautiful area.
[02:34:49.240 --> 02:34:53.560]   No, no, we only need to move San Sebastian.
[02:34:53.560 --> 02:34:55.560]   Oh, Spain.
[02:34:55.560 --> 02:34:56.440]   Spain.
[02:34:56.440 --> 02:34:56.760]   All right.
[02:34:56.760 --> 02:34:58.920]   It is marvelous.
[02:34:58.920 --> 02:35:00.440]   That Marcos guy is long gone.
[02:35:00.440 --> 02:35:02.920]   We don't have to worry about the fascist anymore, right?
[02:35:02.920 --> 02:35:03.480]   It's OK.
[02:35:03.480 --> 02:35:05.880]   Ah, yeah, but I mean,
[02:35:05.880 --> 02:35:11.080]   honestly, no, I'm staying in the US because we need to sort this stuff out.
[02:35:11.080 --> 02:35:12.120]   Going to sort it out.
[02:35:12.120 --> 02:35:14.120]   Yeah, I'm not going to run away.
[02:35:14.120 --> 02:35:14.920]   No, good.
[02:35:14.920 --> 02:35:17.400]   We're going to sort this stuff out.
[02:35:17.400 --> 02:35:17.720]   Yes.
[02:35:17.720 --> 02:35:22.440]   And we need the British to help.
[02:35:22.440 --> 02:35:26.200]   There's a phrase which has been written for 300 years.
[02:35:26.200 --> 02:35:29.320]   You'll be back.
[02:35:29.320 --> 02:35:30.040]   Wait and see.
[02:35:30.040 --> 02:35:32.680]   Just remember, you belong to me.
[02:35:32.680 --> 02:35:36.760]   As long as you spell "anim" correctly, then, you know,
[02:35:36.760 --> 02:35:38.680]   I'm telling, I can't even say it.
[02:35:38.680 --> 02:35:39.080]   All right.
[02:35:40.520 --> 02:35:44.360]   We do it every Sunday afternoon, about 2 Pacific 5 p.m.
[02:35:44.360 --> 02:35:46.120]   Eastern 2100 UTC.
[02:35:46.120 --> 02:35:47.720]   You can watch us do it live.
[02:35:47.720 --> 02:35:54.360]   Get all the swear words un un-bowd-lurized at live.twit.tv.
[02:35:54.360 --> 02:35:56.040]   After the fact.
[02:35:56.040 --> 02:35:57.880]   Am I supposed to say you wanker at this point?
[02:35:57.880 --> 02:35:59.480]   You didn't say wanker.
[02:35:59.480 --> 02:36:00.200]   You didn't say any.
[02:36:00.200 --> 02:36:01.240]   It caught a swallow up.
[02:36:01.240 --> 02:36:08.120]   You didn't say any of those funny, funny British words that you, you, you, people say.
[02:36:08.360 --> 02:36:10.360]   [laughter]
[02:36:10.360 --> 02:36:12.120]   Try not to say anything bad.
[02:36:12.120 --> 02:36:19.720]   If you want an on-demand version, it's all, all the shows are available at twit.tv.
[02:36:19.720 --> 02:36:24.520]   Also on YouTube, there's video and each show has its own dedicated channel.
[02:36:24.520 --> 02:36:27.080]   So just search for this week at tech.
[02:36:27.080 --> 02:36:31.800]   And then, of course, you can subscribe, audio or video available in your favorite podcast.
[02:36:31.800 --> 02:36:37.480]   We like to call it the first podcast of the week and your last word in tech
[02:36:37.480 --> 02:36:38.600]   this week in tech.
[02:36:38.600 --> 02:36:42.600]   Now, when it's 18th year making people hop in mad.
[02:36:42.600 --> 02:36:43.560]   Thank you for being here.
[02:36:43.560 --> 02:36:44.840]   We'll see you next time.
[02:36:44.840 --> 02:36:46.760]   Another twit is in the can.
[02:36:46.760 --> 02:36:47.480]   This is me.
[02:36:47.480 --> 02:36:48.200]   Bloody hell.
[02:36:48.200 --> 02:36:50.200]   [music]
[02:36:51.080 --> 02:36:51.640]   Do the twit.
[02:36:51.640 --> 02:36:52.520]   Do the twit.
[02:36:52.520 --> 02:36:53.320]   All right.
[02:36:53.320 --> 02:36:54.600]   Do the twit, baby.
[02:36:54.600 --> 02:36:56.200]   Do the twit.
[02:36:56.200 --> 02:36:57.160]   All right.
[02:36:57.160 --> 02:36:58.520]   Do the twit, baby.
[02:36:58.520 --> 02:36:59.580]   - Do it!

