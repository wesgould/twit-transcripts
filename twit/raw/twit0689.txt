;FFMETADATA1
title=Nobody Expects the Scooter Inquisition
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=689
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.000]   It's time for Twit this week in Tech.
[00:00:03.000 --> 00:00:07.620]   Greg Farrell from the Packet Pushers Network joins us along with Dan Patterson from CBS
[00:00:07.620 --> 00:00:10.520]   News, Seth Weintraub from 9 to 5.
[00:00:10.520 --> 00:00:11.520]   We've got a great panel.
[00:00:11.520 --> 00:00:19.200]   We'll talk about election hacking, skip and scoot, the Apple iPad event, and the passing
[00:00:19.200 --> 00:00:22.080]   of one of the greats in personal computing.
[00:00:22.080 --> 00:00:27.200]   It's all coming up next on Twit.
[00:00:27.200 --> 00:00:29.800]   This is Twit.
[00:00:29.800 --> 00:00:34.200]   This is Twit.
[00:00:34.200 --> 00:00:37.200]   This is Twit.
[00:00:37.200 --> 00:00:43.800]   This is Twit.
[00:00:43.800 --> 00:00:49.800]   This week in Tech, Episode 689, recorded Sunday, October 21, 2018.
[00:00:49.800 --> 00:00:53.400]   Nobody expects the scooter inquisition.
[00:00:53.400 --> 00:00:57.680]   This week in Tech is brought to you by Rocket Mortgage from Quick and Loans, introducing
[00:00:57.680 --> 00:01:00.800]   rate shield approval if you're in the market to buy a home.
[00:01:00.800 --> 00:01:04.480]   Rate shield approval locks up your rate for up to 90 days while you shop.
[00:01:04.480 --> 00:01:10.800]   It's a real game changer, learn more and get started at rocketmortgage.com/twit2.
[00:01:10.800 --> 00:01:17.120]   And by ZipRecruiter, most job boards overwhelm you with tons of wrong resumes, not smart.
[00:01:17.120 --> 00:01:21.560]   ZipRecruiter finds the right people for you and actively invites them to apply.
[00:01:21.560 --> 00:01:22.560]   Smart.
[00:01:22.560 --> 00:01:25.680]   You're free at zippercruiter.com/twit.
[00:01:25.680 --> 00:01:30.440]   And by Wasabi Hot Cloud Storage from the founders of Carbonite comes the cloud storage
[00:01:30.440 --> 00:01:33.200]   technology that's disrupting the industry.
[00:01:33.200 --> 00:01:35.960]   See for yourself with free unlimited storage for a month.
[00:01:35.960 --> 00:01:40.840]   Go to wasabi.com, click free trial, and use the offer code Twit.
[00:01:40.840 --> 00:01:42.560]   And by Blue Apron.
[00:01:42.560 --> 00:01:45.520]   Blue Apron makes cooking fun and easy.
[00:01:45.520 --> 00:01:50.480]   Check out this week's menu and get your first three meals free at Blue Apron.com/twit.
[00:01:50.480 --> 00:01:57.280]   It's time for Twit this week in tech to show where we cover the weeks.
[00:01:57.280 --> 00:02:00.560]   Geeky news with Uber Geeks.
[00:02:00.560 --> 00:02:02.880]   I've got Uber Geeks in house.
[00:02:02.880 --> 00:02:06.400]   Nobody's more Uber than the host of the packet pushers network.
[00:02:06.400 --> 00:02:07.400]   Mr. Greg Farrow.
[00:02:07.400 --> 00:02:08.400]   Hello, Greg.
[00:02:08.400 --> 00:02:10.400]   Hello, Leo.
[00:02:10.400 --> 00:02:11.400]   I'm glad to be back.
[00:02:11.400 --> 00:02:12.720]   I'm even alive and kicking.
[00:02:12.720 --> 00:02:13.960]   It's 11.30 at night.
[00:02:13.960 --> 00:02:14.960]   It's great.
[00:02:14.960 --> 00:02:15.960]   Oh, man.
[00:02:15.960 --> 00:02:16.960]   That's right.
[00:02:16.960 --> 00:02:19.160]   Well, we had to do this before summer time ended or something.
[00:02:19.160 --> 00:02:20.160]   I don't know.
[00:02:20.160 --> 00:02:23.240]   Has summer time ended where you are?
[00:02:23.240 --> 00:02:24.240]   Yes.
[00:02:24.240 --> 00:02:25.240]   Oh, yes.
[00:02:25.240 --> 00:02:26.240]   Mid-October timeframe.
[00:02:26.240 --> 00:02:27.240]   You fell back already.
[00:02:27.240 --> 00:02:28.240]   Yeah.
[00:02:28.240 --> 00:02:32.320]   See, in the States, for some reason, we moved it to November.
[00:02:32.320 --> 00:02:33.320]   Yeah.
[00:02:33.320 --> 00:02:36.240]   I know there's a lot of people who are complaining about daylight saving.
[00:02:36.240 --> 00:02:40.160]   They find it very confusing that the time changes.
[00:02:40.160 --> 00:02:41.880]   And there's a substantial movement.
[00:02:41.880 --> 00:02:43.120]   It's a global movement, by the way.
[00:02:43.120 --> 00:02:44.120]   I don't know if you'd be thinking of them.
[00:02:44.120 --> 00:02:45.120]   Yes, it should be.
[00:02:45.120 --> 00:02:46.120]   Yep.
[00:02:46.120 --> 00:02:47.120]   Yep.
[00:02:47.120 --> 00:02:48.120]   It's ridiculous.
[00:02:48.120 --> 00:02:51.280]   We've got to get rid of summer time.
[00:02:51.280 --> 00:02:52.280]   So there's actually a...
[00:02:52.280 --> 00:02:54.600]   I have to wait at 8 o'clock in the morning when they're...
[00:02:54.600 --> 00:02:56.600]   In California, we're voting on that.
[00:02:56.600 --> 00:02:58.840]   But they got it all wrong.
[00:02:58.840 --> 00:03:05.920]   Proposition 7 says California will stay on daylight saving time.
[00:03:05.920 --> 00:03:10.960]   But that requires a two-thirds vote of the state legislature and federal approval.
[00:03:10.960 --> 00:03:12.840]   Like that's going to happen.
[00:03:12.840 --> 00:03:16.080]   So like, why even bother?
[00:03:16.080 --> 00:03:17.800]   Why even bother?
[00:03:17.800 --> 00:03:21.720]   I'm just looking forward to all the computer systems going tits up when the time changes.
[00:03:21.720 --> 00:03:26.120]   Well, that's why they want to change because Apple doesn't seem to be able to figure out.
[00:03:26.120 --> 00:03:33.000]   Again, the new series for Apple Watch and WatchOS, whatever it is, five...
[00:03:33.000 --> 00:03:34.480]   It's not handling the change.
[00:03:34.480 --> 00:03:38.240]   Australia did it changed a couple of weeks ago and it's crashed Apple watches.
[00:03:38.240 --> 00:03:39.560]   They were in boot loops.
[00:03:39.560 --> 00:03:44.760]   And Apple's fix was, "We'll wait till tomorrow."
[00:03:44.760 --> 00:03:48.760]   In 30 years of IT, we've had constant stream.
[00:03:48.760 --> 00:03:49.760]   I could...
[00:03:49.760 --> 00:03:53.080]   Like hundreds of times there's bugs around clock changes and daylight saving.
[00:03:53.080 --> 00:03:54.080]   Doesn't seem right.
[00:03:54.080 --> 00:03:55.080]   Doesn't seem right.
[00:03:55.080 --> 00:03:56.080]   That's it.
[00:03:56.080 --> 00:03:57.080]   Yes.
[00:03:57.080 --> 00:03:59.400]   Speaking of elections, we're thrilled to have Dan Patterson here.
[00:03:59.400 --> 00:04:00.400]   Dan is doing...
[00:04:00.400 --> 00:04:01.880]   How many pieces said?
[00:04:01.880 --> 00:04:07.720]   10-piece series on voter fraud?
[00:04:07.720 --> 00:04:08.720]   On what?
[00:04:08.720 --> 00:04:09.720]   On election technology?
[00:04:09.720 --> 00:04:11.480]   Yeah, election hacking.
[00:04:11.480 --> 00:04:14.200]   So this is a 10-episode series.
[00:04:14.200 --> 00:04:15.200]   We started...
[00:04:15.200 --> 00:04:18.840]   We are on week eight, I think, of election hacking.
[00:04:18.840 --> 00:04:20.720]   So we are examining...
[00:04:20.720 --> 00:04:26.160]   When I say we, this is for CBS News at cbsnews.com/electionhacking.
[00:04:26.160 --> 00:04:30.040]   So this is 10 stories as well as 10 produced segments.
[00:04:30.040 --> 00:04:35.600]   And then another 10 stories ever and seen it about the mechanics of election hacking.
[00:04:35.600 --> 00:04:39.800]   One thing we've learned is that 10 is certainly not enough.
[00:04:39.800 --> 00:04:45.400]   We will inevitably miss something that people, especially people in IT, look at us and go,
[00:04:45.400 --> 00:04:47.440]   "Well, what about this?"
[00:04:47.440 --> 00:04:52.000]   So we have in the US, we have what they call the midterms elections.
[00:04:52.000 --> 00:04:56.040]   They're coming up two weeks from Tuesday.
[00:04:56.040 --> 00:04:58.960]   So we're just about two weeks out.
[00:04:58.960 --> 00:05:04.400]   And a lot of alarm bells about election security.
[00:05:04.400 --> 00:05:08.360]   I will be listening to all of this at cbsnews.com.
[00:05:08.360 --> 00:05:11.760]   But what's just the top level take on this?
[00:05:11.760 --> 00:05:15.880]   Is the election going to be hacked?
[00:05:15.880 --> 00:05:21.760]   It is quite likely that various actors will engage in different methods, different tactics
[00:05:21.760 --> 00:05:23.280]   of election hacking.
[00:05:23.280 --> 00:05:27.520]   But that is not to discourage you from voting.
[00:05:27.520 --> 00:05:29.520]   You should go and vote.
[00:05:29.520 --> 00:05:34.640]   So when I say tactics, we look at everything from voting machine hacking because if you
[00:05:34.640 --> 00:05:38.400]   go to Black Hat or Defcon every year, there is a voter village.
[00:05:38.400 --> 00:05:41.280]   And we look at how vulnerable voting machines are.
[00:05:41.280 --> 00:05:46.040]   And they're incredibly vulnerable as we know they are simply computers and all computers
[00:05:46.040 --> 00:05:47.280]   can be hacked.
[00:05:47.280 --> 00:05:53.560]   However, I had a great talk with IBM's space rogue, Chris Thomas, who is part of a loft
[00:05:53.560 --> 00:05:55.720]   heavy industry, is a hacking collective.
[00:05:55.720 --> 00:05:59.640]   And he had a great thought, which was, look, yes, computers can be hacked.
[00:05:59.640 --> 00:06:03.720]   But one of the wonderful things about the American Bocker Sea is that we are decentralized.
[00:06:03.720 --> 00:06:08.160]   It is not a federal system, it's a statewide system, and every state does things a little
[00:06:08.160 --> 00:06:09.160]   differently.
[00:06:09.160 --> 00:06:16.200]   And that kind of security baked into the democratic process means that we have diversity in how
[00:06:16.200 --> 00:06:21.720]   we vote, or at least the mechanics of the technology involved in how we vote.
[00:06:21.720 --> 00:06:28.440]   And that's protected us from various forms of nefarious actors meddling with the election.
[00:06:28.440 --> 00:06:35.760]   Right now, it seems as though the biggest threat to election security is and will remain social
[00:06:35.760 --> 00:06:36.760]   media.
[00:06:36.760 --> 00:06:38.760]   It is influence campaigns that will--
[00:06:38.760 --> 00:06:40.400]   Yeah, that's not hacking per se.
[00:06:40.400 --> 00:06:46.160]   That's attempting to influence voters, attempting to suppress voting, a variety of things like
[00:06:46.160 --> 00:06:47.160]   that.
[00:06:47.160 --> 00:06:52.200]   And yeah, that seems to work a lot better because you can target a lot more people than trying
[00:06:52.200 --> 00:06:59.320]   to figure out which debold machine is used in Monroe County, Louisiana.
[00:06:59.320 --> 00:07:03.560]   So we can hack that one and change the vote there, but does that going to change anything
[00:07:03.560 --> 00:07:05.360]   in the state?
[00:07:05.360 --> 00:07:07.120]   It's a much more cognitive hacking.
[00:07:07.120 --> 00:07:10.920]   Yeah, cognitive hacking is a lot easier.
[00:07:10.920 --> 00:07:19.600]   If people are trying to hack voting machines, are these nation state, are these governmental
[00:07:19.600 --> 00:07:20.600]   hackers?
[00:07:20.600 --> 00:07:25.640]   Yeah, so attribution, the number one thing, if you talk to and we spend a lot of time
[00:07:25.640 --> 00:07:31.880]   talking to Facebook, Google, Microsoft, Twitter, the large technology firms, as well as experts
[00:07:31.880 --> 00:07:36.760]   in cybersecurity, as well as people who actually run elections in various states, including
[00:07:36.760 --> 00:07:39.560]   Colorado, Georgia, Florida.
[00:07:39.560 --> 00:07:44.600]   And they will tell us that, first of all, attribution is incredibly difficult because
[00:07:44.600 --> 00:07:47.680]   the forensics take a long time to figure out.
[00:07:47.680 --> 00:07:48.680]   Uh-oh.
[00:07:48.680 --> 00:07:52.880]   Uh-oh, we've lost your audio.
[00:07:52.880 --> 00:07:57.760]   I think Little Limbago has gotten a hold of your audio.
[00:07:57.760 --> 00:07:59.240]   Yeah.
[00:07:59.240 --> 00:08:01.400]   And obviously he can't hear us either.
[00:08:01.400 --> 00:08:03.760]   Yeah, he's going on and on and on and on.
[00:08:03.760 --> 00:08:05.760]   I can't hear a thing.
[00:08:05.760 --> 00:08:06.760]   Thank you, Skype.
[00:08:06.760 --> 00:08:09.200]   That's honestly Greg Ferro.
[00:08:09.200 --> 00:08:13.960]   The real threat here is not nation state actors, but just the crap level of technology being
[00:08:13.960 --> 00:08:18.040]   used by everybody, including companies like debold.
[00:08:18.040 --> 00:08:19.120]   I think it's complicated.
[00:08:19.120 --> 00:08:23.200]   I think that, I think what he was trying to get to is that there's more than one way
[00:08:23.200 --> 00:08:24.240]   an attack can happen.
[00:08:24.240 --> 00:08:29.280]   You can have an attack on the actual infrastructure itself, so the voting machines.
[00:08:29.280 --> 00:08:33.200]   But the point would be is that to actually do it to all voting machines across the US
[00:08:33.200 --> 00:08:34.200]   much more difficult.
[00:08:34.200 --> 00:08:39.840]   Yeah, would you scale that attack is incredibly difficult and improbable.
[00:08:39.840 --> 00:08:47.120]   And to really impact the US elections, you would need to be impacting a substantial number
[00:08:47.120 --> 00:08:48.120]   of people.
[00:08:48.120 --> 00:08:54.360]   Now, that's true, but the thing that strikes me about that particular discussion is in
[00:08:54.360 --> 00:08:59.120]   the current electoral environment, the left and the right is evenly balanced.
[00:08:59.120 --> 00:09:03.200]   And if you can swing 5 to 10% of the vote, you can actually get the outcome that you
[00:09:03.200 --> 00:09:05.720]   want by targeting certain areas.
[00:09:05.720 --> 00:09:10.200]   So that's in fact, you might say that's what we saw in the presidential election two
[00:09:10.200 --> 00:09:15.440]   years ago, because it was a swing of 77,000 votes that turned the election.
[00:09:15.440 --> 00:09:16.440]   Exactly.
[00:09:16.440 --> 00:09:17.960]   But that's a national election.
[00:09:17.960 --> 00:09:19.200]   This is not a national election.
[00:09:19.200 --> 00:09:21.960]   This is at best statewide in 2018.
[00:09:21.960 --> 00:09:22.960]   Yeah.
[00:09:22.960 --> 00:09:27.360]   Well, I think that I think the election hacking per se that's been happening in the US has
[00:09:27.360 --> 00:09:29.520]   much more been systemic.
[00:09:29.520 --> 00:09:34.760]   So we've seen, you know, various left and right wing parties gerrymandering the vote.
[00:09:34.760 --> 00:09:39.240]   Yeah, that kind of thing makes it very that kind of thing or stacking the rolls with people
[00:09:39.240 --> 00:09:44.320]   from, you know, this area gets zoned and there's like, and that's all white people or voter
[00:09:44.320 --> 00:09:49.240]   suppression making it harder for minorities to go out cleaning the rolls, which just happens
[00:09:49.240 --> 00:09:54.000]   to clean out all of the people who come from left leaning districts and so forth.
[00:09:54.000 --> 00:09:59.520]   So I think, and now that's all that seems to be happening at very much more scale.
[00:09:59.520 --> 00:10:03.840]   So the articles that I've been reading about the US election system is the actual system
[00:10:03.840 --> 00:10:10.800]   inside the US is actually being compromised by the people inside the system.
[00:10:10.800 --> 00:10:15.760]   But they're using the threat of social media and the threat of election hacking to hide
[00:10:15.760 --> 00:10:17.040]   what they're doing.
[00:10:17.040 --> 00:10:22.360]   So if you can actually say, you know, our fault, it's not our fault.
[00:10:22.360 --> 00:10:25.040]   It's all those trolls on the troll farm on Twitter.
[00:10:25.040 --> 00:10:26.400]   Yes, indeed.
[00:10:26.400 --> 00:10:30.040]   And so why wouldn't you blame them and point the finger at somebody else?
[00:10:30.040 --> 00:10:34.440]   But in fact, it was you that cleansed the rolls, which got rid of a lot of people in
[00:10:34.440 --> 00:10:38.280]   poorer counties who move a lot and therefore they're suddenly not eligible to that.
[00:10:38.280 --> 00:10:40.320]   You know, in Dan, we're going to get Dan back.
[00:10:40.320 --> 00:10:41.520]   Something's gone wrong with this guy.
[00:10:41.520 --> 00:10:49.880]   But in Dan's report on CBSN, CBS news.com, there's an article that he's involved in called
[00:10:49.880 --> 00:10:52.320]   These are the hackers targeting the midterm election.
[00:10:52.320 --> 00:10:57.040]   He points out it's not just nation states, but multinational corporations like Cambridge
[00:10:57.040 --> 00:10:59.080]   Analytica.
[00:10:59.080 --> 00:11:02.040]   Are we seeing evidence that Brexit might have also been influenced?
[00:11:02.040 --> 00:11:04.880]   The Brexit vote in the UK might have been influenced because that's where you.
[00:11:04.880 --> 00:11:05.880]   Yeah, we certainly are.
[00:11:05.880 --> 00:11:11.520]   In fact, all across Europe seeing the same thing happen in Italy, Spain, the German elections
[00:11:11.520 --> 00:11:14.840]   that came out, people aren't entirely sure even what happened there.
[00:11:14.840 --> 00:11:20.200]   A few points out Cambridge Analytica was involved in the 2013 Kenya elections.
[00:11:20.200 --> 00:11:23.640]   I mean, this has been going on for five years, at least.
[00:11:23.640 --> 00:11:29.120]   Yes, I would think by this point that any data that Facebook has got has been stolen or
[00:11:29.120 --> 00:11:30.920]   take an unauthorized theft.
[00:11:30.920 --> 00:11:31.920]   We know that.
[00:11:31.920 --> 00:11:36.120]   Yeah, and I don't think it was done by companies like Cambridge Analytica.
[00:11:36.120 --> 00:11:40.080]   It would much more easily be done by employees, quite honestly.
[00:11:40.080 --> 00:11:46.080]   So one of the reasons that the tech companies are very reluctant to say, "I will control.
[00:11:46.080 --> 00:11:47.080]   We don't want to restrict data.
[00:11:47.080 --> 00:11:48.080]   We don't want to share it."
[00:11:48.080 --> 00:11:52.280]   One of the reasons that I'm coming to realize is companies like Google and Facebook for
[00:11:52.280 --> 00:11:56.240]   them to put controls around the data is incredibly difficult.
[00:11:56.240 --> 00:11:59.160]   More importantly, it actually prevents them from functioning correctly.
[00:11:59.160 --> 00:12:04.160]   So they have things like a developer starts working on project A, they'll work it for
[00:12:04.160 --> 00:12:07.080]   two or three months until the project's finished, and then they go to project B.
[00:12:07.080 --> 00:12:11.240]   Well, project B might be on a completely different data system, but if you have controls on
[00:12:11.240 --> 00:12:15.040]   what data they can access, it might take them three months to get permissions and approvals
[00:12:15.040 --> 00:12:16.400]   and reviews.
[00:12:16.400 --> 00:12:20.160]   And so a lot of the reasons that they won't put controls on the data and enact privacy
[00:12:20.160 --> 00:12:24.240]   controls is because it would actually slow them down internally.
[00:12:24.240 --> 00:12:30.720]   And then the pace of innovation, most of what Facebook and Google these days is copying
[00:12:30.720 --> 00:12:35.280]   each other or copying Instagram or Snapchat or whatever somebody else is doing.
[00:12:35.280 --> 00:12:37.120]   Whatever it takes, man, whatever it takes.
[00:12:37.120 --> 00:12:40.280]   Yeah, it's always called innovation though, Leo.
[00:12:40.280 --> 00:12:43.320]   It's always innovation.
[00:12:43.320 --> 00:12:44.320]   We did something new, innovation.
[00:12:44.320 --> 00:12:47.360]   We have innovative ways of stealing the other guys' ideas.
[00:12:47.360 --> 00:12:48.360]   Some of the best.
[00:12:48.360 --> 00:12:49.360]   That's right.
[00:12:49.360 --> 00:12:50.360]   Yes.
[00:12:50.360 --> 00:12:51.360]   We spend a lot of time thinking about that.
[00:12:51.360 --> 00:12:54.760]   Because I do not copy it, especially when they're doing better than I was saying.
[00:12:54.760 --> 00:12:55.960]   Actually I thought this was interesting.
[00:12:55.960 --> 00:13:03.800]   Facebook has actually hired the former Deputy Prime Minister of England, Sir Nick Clegg,
[00:13:03.800 --> 00:13:08.240]   as they're ahead of global affairs and communications.
[00:13:08.240 --> 00:13:12.480]   Yeah, this is a rather unusual.
[00:13:12.480 --> 00:13:17.760]   Now Nick Clegg is something of a butt of many jokes here in the UK.
[00:13:17.760 --> 00:13:20.000]   He's the Liberal Democrats party.
[00:13:20.000 --> 00:13:25.440]   Yeah, which is now defunct, by the way, after his tenure as the leader.
[00:13:25.440 --> 00:13:27.480]   They'll tell you something.
[00:13:27.480 --> 00:13:33.120]   Yeah, so about two or three elections ago, the Liberal Democrats came out of nowhere to
[00:13:33.120 --> 00:13:34.680]   be a centrist party.
[00:13:34.680 --> 00:13:38.160]   So there was a right wing, a left wing, and then the Liberal Democrats emerged to occupy
[00:13:38.160 --> 00:13:41.400]   the centre.
[00:13:41.400 --> 00:13:44.960]   They took a few votes and then all of a sudden they got enough momentum.
[00:13:44.960 --> 00:13:48.040]   They had a really charismatic leader.
[00:13:48.040 --> 00:13:52.520]   They were able to form an alliance with the right wing party, the Tories.
[00:13:52.520 --> 00:13:59.880]   And then they were given the Deputy Prime Minister's ship and so forth in a number of
[00:13:59.880 --> 00:14:02.160]   various positions of power.
[00:14:02.160 --> 00:14:03.480]   And then they died.
[00:14:03.480 --> 00:14:04.680]   They weren't able to act.
[00:14:04.680 --> 00:14:06.240]   They didn't make any innovation.
[00:14:06.240 --> 00:14:07.960]   They didn't make anything change.
[00:14:07.960 --> 00:14:14.280]   So he was kind of the prototypical reformer who's beloved by the young people, we're going
[00:14:14.280 --> 00:14:15.920]   to make things better.
[00:14:15.920 --> 00:14:20.920]   And then did nothing and lost the confidence of the voters.
[00:14:20.920 --> 00:14:24.880]   And then after the loss of confidence of the voters, the voters abandoned them at the
[00:14:24.880 --> 00:14:26.400]   next general election.
[00:14:26.400 --> 00:14:29.600]   And the Liberal Democrats basically just banned it a couple of years later.
[00:14:29.600 --> 00:14:32.960]   So not exactly covered himself in glory here.
[00:14:32.960 --> 00:14:38.520]   This is not your typical burning sprite arc of career glory.
[00:14:38.520 --> 00:14:44.040]   This is a man who was given the party after the head of the party stood down and then
[00:14:44.040 --> 00:14:45.400]   proceeded to run it into the ground.
[00:14:45.400 --> 00:14:49.640]   So I actually think this is great for me because I hate Facebook with a passion and having
[00:14:49.640 --> 00:14:52.680]   somebody like this in charge of global policy should just be fantastic.
[00:14:52.680 --> 00:14:54.480]   I'm expecting it to blow up.
[00:14:54.480 --> 00:14:55.480]   I'm expecting Facebook.
[00:14:55.480 --> 00:14:57.640]   What do you give him about 10 years to bring it down?
[00:14:57.640 --> 00:14:59.600]   It took him 10 years to kill the Liberal Democrats.
[00:14:59.600 --> 00:15:02.640]   I think he'll take him about that in the long to defeat Facebook.
[00:15:02.640 --> 00:15:03.720]   I hope he does it.
[00:15:03.720 --> 00:15:05.760]   I hope he does it a hell of a lot quicker.
[00:15:05.760 --> 00:15:10.480]   Hugo Rifkin, who calls himself the funniest journalist in Britain, tweeted, "My timeline
[00:15:10.480 --> 00:15:15.360]   is intriguingly divided between people disappointed with Nick Clegg for going to work for Facebook
[00:15:15.360 --> 00:15:19.760]   and people disappointed with Facebook for hiring Nick Clegg."
[00:15:19.760 --> 00:15:21.000]   I think it's great.
[00:15:21.000 --> 00:15:22.000]   Honestly seeing Facebook.
[00:15:22.000 --> 00:15:23.000]   Because you hate Facebook.
[00:15:23.000 --> 00:15:24.000]   Yeah.
[00:15:24.000 --> 00:15:29.160]   I mean, last week we saw the whole tech, the beginning of a tech clash, $200 billion was
[00:15:29.160 --> 00:15:34.840]   wiped off the NASDAQ as the Google Facebook et al. all dropped sort of 5 to 10% as the
[00:15:34.840 --> 00:15:39.840]   investors go for the door because they definitely see that these, those companies are now under
[00:15:39.840 --> 00:15:40.840]   fire.
[00:15:40.840 --> 00:15:41.840]   You think that's lasting?
[00:15:41.840 --> 00:15:42.840]   That's not profit taking.
[00:15:42.840 --> 00:15:43.840]   That's not just a blip.
[00:15:43.840 --> 00:15:46.240]   What are you going to be a lasting abandoning of tech?
[00:15:46.240 --> 00:15:47.240]   Yes, I do.
[00:15:47.240 --> 00:15:48.800]   I think there's far too much stacking up against them.
[00:15:48.800 --> 00:15:53.040]   I mean, the European governments are now talking about mandatory taxation for companies like
[00:15:53.040 --> 00:15:56.240]   Facebook and Google.
[00:15:56.240 --> 00:16:02.040]   Facebook paid 10 million in tax on $1.5 billion worth of revenue taken out of the country
[00:16:02.040 --> 00:16:03.040]   in the UK.
[00:16:03.040 --> 00:16:04.040]   That's not right.
[00:16:04.040 --> 00:16:05.040]   Right.
[00:16:05.040 --> 00:16:09.080]   So all the infrastructure, consumed all the infrastructure, all the employees, they don't
[00:16:09.080 --> 00:16:13.280]   even pay indirect taxation because the internet is on tax.
[00:16:13.280 --> 00:16:17.160]   So there's going to be, there's no reason for the governments not to attack Facebook
[00:16:17.160 --> 00:16:19.000]   and Google and Apple.
[00:16:19.000 --> 00:16:22.520]   And they're basically going to say, we're going to have to put a consumption tax on companies
[00:16:22.520 --> 00:16:23.520]   like yours.
[00:16:23.520 --> 00:16:28.440]   And it's going to take a few years because governments move slowly and this sort of taxation can
[00:16:28.440 --> 00:16:29.960]   have unexpected impact.
[00:16:29.960 --> 00:16:36.960]   So if you start to move to that, I mean, many governments move to services tax or goods
[00:16:36.960 --> 00:16:40.240]   taxes and you know, where you pay like in the US estate tax.
[00:16:40.240 --> 00:16:41.240]   That's a value.
[00:16:41.240 --> 00:16:42.640]   How do you guys call it a value added tax event?
[00:16:42.640 --> 00:16:44.840]   We call it a value added tax, but you call it state tax.
[00:16:44.840 --> 00:16:45.840]   Right.
[00:16:45.840 --> 00:16:49.560]   So if you do have sales tax, but that's a taxation on consumption.
[00:16:49.560 --> 00:16:53.800]   When you buy something, you pay five to eight percent on consumption.
[00:16:53.800 --> 00:16:56.760]   But a VAT is a completely different idea of tax.
[00:16:56.760 --> 00:16:57.880]   No, it's a consumption.
[00:16:57.880 --> 00:16:58.880]   It's a consumption.
[00:16:58.880 --> 00:17:03.920]   But no, but there is the idea of a taxing production, taxing it all along the way.
[00:17:03.920 --> 00:17:05.400]   That is not what your guys are doing.
[00:17:05.400 --> 00:17:08.160]   No, it's only a tax at point of sale.
[00:17:08.160 --> 00:17:09.720]   So it's 21 cents.
[00:17:09.720 --> 00:17:11.640]   It's a ridiculously high tax tax.
[00:17:11.640 --> 00:17:13.680]   Yeah, those sales taxes work.
[00:17:13.680 --> 00:17:16.080]   What they do is they capture the black economy.
[00:17:16.080 --> 00:17:19.160]   Someone has to pay the sales tax at the final point of sale.
[00:17:19.160 --> 00:17:25.200]   So even if you're doing, you know, under the cable cash deals, you're still buying the
[00:17:25.200 --> 00:17:26.200]   goods.
[00:17:26.200 --> 00:17:29.360]   So if you're, let's say you're a carpenter and you're putting an extension on a house,
[00:17:29.360 --> 00:17:33.640]   but you're getting paid cash, then when you buy the wood, you're paying sales tax.
[00:17:33.640 --> 00:17:34.640]   So some of the tax.
[00:17:34.640 --> 00:17:40.160]   So Bill Gates said a weird thing a few months ago where he said, well, you know, if robots
[00:17:40.160 --> 00:17:42.840]   take all our jobs, we should really tax robots.
[00:17:42.840 --> 00:17:45.880]   But in fact, what he was saying, I think was smarter than that was the notion that you
[00:17:45.880 --> 00:17:53.800]   should tax companies on production all along the line instead of, which is a much easier,
[00:17:53.800 --> 00:17:57.360]   simpler thing to do instead of taxing consumption at the end.
[00:17:57.360 --> 00:17:59.200]   Well, it's not in a tax.
[00:17:59.200 --> 00:18:00.200]   It's not.
[00:18:00.200 --> 00:18:01.200]   I don't think it's as regressive either.
[00:18:01.200 --> 00:18:06.080]   Well, the challenge here is that today we tax traditional companies on profit.
[00:18:06.080 --> 00:18:10.440]   So if the company doesn't make a profit, the company doesn't pay tax and that's, that
[00:18:10.440 --> 00:18:14.400]   helps to protect jobs and helps to protect the people employed there.
[00:18:14.400 --> 00:18:18.520]   Because if you ask the company to pay tax and it's not making a profit, then people start
[00:18:18.520 --> 00:18:22.000]   to get unemployed and that's bad for government and bad for society general.
[00:18:22.000 --> 00:18:27.600]   Now, keep in mind here, my thesis would be is that a good capitalist, a good capitalist
[00:18:27.600 --> 00:18:33.360]   should always remember that the job of capitalism is to keep people employed and fed and clothed.
[00:18:33.360 --> 00:18:35.440]   And that's why we have a capitalist society.
[00:18:35.440 --> 00:18:36.440]   What do you nuts?
[00:18:36.440 --> 00:18:39.760]   The job of capitalism is to make as much money as you possibly can.
[00:18:39.760 --> 00:18:40.760]   Yes.
[00:18:40.760 --> 00:18:44.440]   No, the job of capitalism was to make sure that people aren't on the street shooting each
[00:18:44.440 --> 00:18:46.920]   other and stealing food from those thousands.
[00:18:46.920 --> 00:18:47.920]   Yeah.
[00:18:47.920 --> 00:18:48.920]   Yeah.
[00:18:48.920 --> 00:18:50.400]   So that we can make as much money as we possibly can.
[00:18:50.400 --> 00:18:51.400]   That's right.
[00:18:51.400 --> 00:18:54.840]   And part of the part of the value of capitalism is it has to be a redistribution from the top
[00:18:54.840 --> 00:18:55.840]   to the bottom.
[00:18:55.840 --> 00:18:56.840]   That's what taxation does.
[00:18:56.840 --> 00:18:57.840]   You are a communist.
[00:18:57.840 --> 00:18:58.840]   That is not capitalism.
[00:18:58.840 --> 00:18:59.840]   Actually, I'm a centrist.
[00:18:59.840 --> 00:19:03.800]   So don't don't curse me with the communists.
[00:19:03.800 --> 00:19:07.120]   I really want to get damned back in the conversation.
[00:19:07.120 --> 00:19:08.120]   We're having a little trouble with him technically.
[00:19:08.120 --> 00:19:11.160]   You know what I'm going to do is take a break and do an ad.
[00:19:11.160 --> 00:19:12.360]   And then we will come back because I do.
[00:19:12.360 --> 00:19:13.560]   There's much more to say about this.
[00:19:13.560 --> 00:19:20.040]   In fact, Google has decided that in the face of regulation from the EU, they're going to
[00:19:20.040 --> 00:19:24.720]   hit back and hit back in the consumers' pockets, which I think is very interesting.
[00:19:24.720 --> 00:19:26.240]   That's one way to fight back on this.
[00:19:26.240 --> 00:19:28.800]   Now, I'd like to talk more about the tech lash.
[00:19:28.800 --> 00:19:30.440]   Greg Farrow is here.
[00:19:30.440 --> 00:19:36.960]   He is somebody saying in the chat, "I'm a centrist only by the definition of the EU."
[00:19:36.960 --> 00:19:40.960]   Here in the United States, you'd be considered a leftist, lefty commie.
[00:19:40.960 --> 00:19:43.480]   Yeah, that's probably...
[00:19:43.480 --> 00:19:47.960]   He is at the Packet Pusher Network and an expert in all things technology, especially
[00:19:47.960 --> 00:19:48.960]   networking.
[00:19:48.960 --> 00:19:50.120]   We're thrilled to have you.
[00:19:50.120 --> 00:19:51.720]   We will get Dan Patterson back.
[00:19:51.720 --> 00:19:52.720]   He's with the tech republic.
[00:19:52.720 --> 00:19:54.360]   We're as a senior writer there.
[00:19:54.360 --> 00:20:04.240]   And in the midst of a 10-piece series on CBS News, on election hacking specifically, what's
[00:20:04.240 --> 00:20:06.840]   going to be happening in a couple of weeks here in the United States.
[00:20:06.840 --> 00:20:08.680]   We'll get back to that in just a second.
[00:20:08.680 --> 00:20:13.280]   Right now, I want to talk about our sponsor, Rocket Mortgage.
[00:20:13.280 --> 00:20:16.800]   If you're buying a house, you know there's some anxiety associated with writing that
[00:20:16.800 --> 00:20:18.640]   big check, the biggest check you'll ever write.
[00:20:18.640 --> 00:20:20.080]   And that's just the down payment.
[00:20:20.080 --> 00:20:22.880]   You're actually borrowing even more.
[00:20:22.880 --> 00:20:25.800]   It's 70% more, 80% more.
[00:20:25.800 --> 00:20:29.600]   And that's a plus you're buying something that you and your family are going to live
[00:20:29.600 --> 00:20:32.960]   in that's going to be your home for some time.
[00:20:32.960 --> 00:20:36.320]   Rocket Mortgage comes from quick and loans, the largest lender in the country.
[00:20:36.320 --> 00:20:38.840]   They are the experts in mortgages.
[00:20:38.840 --> 00:20:41.720]   Number one in customer satisfaction, eight years in a row.
[00:20:41.720 --> 00:20:43.240]   This is all according to JD Power.
[00:20:43.240 --> 00:20:46.840]   So this is independently rated that way.
[00:20:46.840 --> 00:20:51.080]   And one of the reasons they're so good, one of the reasons people like them so much is
[00:20:51.080 --> 00:20:52.840]   because they're very customer focused.
[00:20:52.840 --> 00:20:56.360]   That's why they created something called Rocket Mortgage, which really speaks to people like
[00:20:56.360 --> 00:20:58.240]   me, people like us.
[00:20:58.240 --> 00:21:00.720]   It's an entirely online mortgage approval process.
[00:21:00.720 --> 00:21:02.240]   I love that.
[00:21:02.240 --> 00:21:03.760]   That means you don't have to go to a bank.
[00:21:03.760 --> 00:21:05.640]   You don't have to fill out a lot of paperwork.
[00:21:05.640 --> 00:21:06.640]   You don't have to go to the ad.
[00:21:06.640 --> 00:21:07.800]   I can find pay stubs.
[00:21:07.800 --> 00:21:11.040]   They can do it all on your phone or on your computer.
[00:21:11.040 --> 00:21:16.480]   They call it the power buying process at rocketmortgage.com/twit2.
[00:21:16.480 --> 00:21:17.720]   There's three steps actually.
[00:21:17.720 --> 00:21:20.000]   First step, answer a few simple questions.
[00:21:20.000 --> 00:21:22.280]   They'll check your credit and you get pre-qualified approval.
[00:21:22.280 --> 00:21:24.920]   And all of that's just in a few minutes.
[00:21:24.920 --> 00:21:27.360]   Then the next step kicks in.
[00:21:27.360 --> 00:21:29.200]   They'll verify income, assets and credit.
[00:21:29.200 --> 00:21:32.920]   They'll do it within about 24 hours to give you verified approval.
[00:21:32.920 --> 00:21:35.040]   That's when you get that button that says, "Print the letter.
[00:21:35.040 --> 00:21:36.560]   You can show it to your realtor.
[00:21:36.560 --> 00:21:38.840]   You can show it to the seller when you're making an offer on a home.
[00:21:38.840 --> 00:21:40.680]   You are basically a cash buyer.
[00:21:40.680 --> 00:21:42.080]   You're approved, baby."
[00:21:42.080 --> 00:21:43.080]   And that is important.
[00:21:43.080 --> 00:21:46.520]   I don't know what it's like in the rest of the world, but in the United States, it's really
[00:21:46.520 --> 00:21:47.520]   a seller's market.
[00:21:47.520 --> 00:21:51.680]   And when you come in as a buyer and you've already got the loan, that makes a huge difference.
[00:21:51.680 --> 00:21:53.200]   It moves you right to the front of the line.
[00:21:53.200 --> 00:21:54.200]   But that's not all.
[00:21:54.200 --> 00:21:56.320]   That's one way to take out the anxiety.
[00:21:56.320 --> 00:22:00.960]   But the third step, once you're verified, you qualify for their all new exclusive rate
[00:22:00.960 --> 00:22:03.360]   shield approval, this really helps.
[00:22:03.360 --> 00:22:06.600]   Nowadays interest rates are going up.
[00:22:06.600 --> 00:22:08.760]   And that means there is some anxiety, there's some pressure.
[00:22:08.760 --> 00:22:11.560]   Buy that house fast before they go up again.
[00:22:11.560 --> 00:22:15.240]   But you don't have to worry with rate shield approval, quick and loans and rocket mortgage
[00:22:15.240 --> 00:22:17.520]   lock your rate for up to 90 days while you're shopping.
[00:22:17.520 --> 00:22:21.400]   It cannot go up for up to three months.
[00:22:21.400 --> 00:22:22.480]   If rates go up, it doesn't matter.
[00:22:22.480 --> 00:22:23.880]   Your rate stays the same.
[00:22:23.880 --> 00:22:26.320]   Oh, if rates go down, your rate will drop.
[00:22:26.320 --> 00:22:30.920]   But either way, it's a victory for you and it's the kind of thing you'd expect from
[00:22:30.920 --> 00:22:32.680]   America's best mortgage lender.
[00:22:32.680 --> 00:22:34.760]   Take the anxiety out of buying a new home.
[00:22:34.760 --> 00:22:37.840]   Go to rocketmortgage.com/twit2.
[00:22:37.840 --> 00:22:40.560]   Get your ducks in a row, as it were.
[00:22:40.560 --> 00:22:44.440]   Rate shield approval is only valid on certain 30-year purchase transactions, additional conditions
[00:22:44.440 --> 00:22:48.560]   or exclusions may apply based on quick and loans data in comparison to public data records.
[00:22:48.560 --> 00:22:55.120]   Local housing lender, licensed in all 50 states, and MLS consumer access.org number 30-30.
[00:22:55.120 --> 00:23:00.400]   But all you have to remember is rocketmortgage.com/twit and the number two.
[00:23:00.400 --> 00:23:01.560]   Dan Patterson is back.
[00:23:01.560 --> 00:23:02.560]   We've rebooted him.
[00:23:02.560 --> 00:23:03.560]   Yeah.
[00:23:03.560 --> 00:23:08.480]   You know, every time we think when does this stable, it gives me the best time.
[00:23:08.480 --> 00:23:09.480]   Oh, man.
[00:23:09.480 --> 00:23:10.480]   Why do you use windows?
[00:23:10.480 --> 00:23:11.480]   What is wrong with?
[00:23:11.480 --> 00:23:12.840]   So you're now on the wrong microphone.
[00:23:12.840 --> 00:23:18.000]   You're on your headsets instead of on the road podcaster.
[00:23:18.000 --> 00:23:19.000]   Nice road, Mike.
[00:23:19.000 --> 00:23:20.000]   You have there?
[00:23:20.000 --> 00:23:21.000]   Yeah.
[00:23:21.000 --> 00:23:22.400]   So I actually dual boot.
[00:23:22.400 --> 00:23:31.840]   I run boot camp on my Mac and I have an eGPU and I run Windows so I can process things
[00:23:31.840 --> 00:23:34.720]   that simply, you know, the Mac is turning.
[00:23:34.720 --> 00:23:37.000]   Sometimes you just have to have Windows, unfortunately.
[00:23:37.000 --> 00:23:38.000]   Yeah, unfortunately.
[00:23:38.000 --> 00:23:41.240]   I'm not, you know, these days, I bet you Greg uses Windows.
[00:23:41.240 --> 00:23:43.000]   Use Windows, Greg, you must.
[00:23:43.000 --> 00:23:45.360]   No, not in over a decade.
[00:23:45.360 --> 00:23:48.680]   Well, I know says wine tribe doesn't.
[00:23:48.680 --> 00:23:51.040]   He's going to join us in about 20 minutes.
[00:23:51.040 --> 00:23:53.320]   I use Windows, but I'm using it right now.
[00:23:53.320 --> 00:23:55.640]   I'm using the Surface Studio.
[00:23:55.640 --> 00:23:59.160]   And I think Microsoft's done a lot to make Windows 10 more reliable, although they've
[00:23:59.160 --> 00:24:01.800]   had some nasty problems lately with the latest.
[00:24:01.800 --> 00:24:06.320]   I would point out that after 25 years of Microsoft producing an operating system, it's
[00:24:06.320 --> 00:24:07.320]   still below his chunks.
[00:24:07.320 --> 00:24:09.320]   There's no opportunity ever going to work at all.
[00:24:09.320 --> 00:24:13.720]   That's a technical term, ladies and gentlemen.
[00:24:13.720 --> 00:24:19.080]   We were talking about election hacking before we lost Dan.
[00:24:19.080 --> 00:24:25.280]   We ended up talking about Nick Clegg running the global, whatever it is, global policy
[00:24:25.280 --> 00:24:31.200]   policy at Facebook, which I thought was kind of interesting.
[00:24:31.200 --> 00:24:35.120]   But I do want to, now that we got you back, I do want to get back to the election hacking.
[00:24:35.120 --> 00:24:38.760]   There's some really interesting pieces here on the CBS News site.
[00:24:38.760 --> 00:24:42.560]   How AI is creating new threats to election security.
[00:24:42.560 --> 00:24:49.200]   So artificial intelligence to target campaigns, how's that work?
[00:24:49.200 --> 00:24:56.600]   So with every conversation in Tekken IT, when we talk about AI, we go into the futuristic,
[00:24:56.600 --> 00:25:00.800]   very interesting Elon Musk world or the Nick Bostrom world.
[00:25:00.800 --> 00:25:07.560]   This is about automation and stuff that we think of as pretty almost rote or mundane.
[00:25:07.560 --> 00:25:13.080]   So automation is taking many of the phishing tactics that we saw in 2016.
[00:25:13.080 --> 00:25:19.600]   Of course, if you remember, John Podesta, the chairperson of the Clinton campaign, was
[00:25:19.600 --> 00:25:21.480]   phished and many other members.
[00:25:21.480 --> 00:25:27.520]   I think there are almost 100, about 96 members of the Clinton campaign were phished in 2016.
[00:25:27.520 --> 00:25:33.320]   Automation is scaling that and creating, look, if we think about a general election campaign,
[00:25:33.320 --> 00:25:40.120]   these are well-funded organizations that have the time, money, and really the human capital
[00:25:40.120 --> 00:25:45.440]   to spend defending the campaigns, but down-ticket, and particularly during midterm elections here
[00:25:45.440 --> 00:25:49.480]   in the United States, these campaigns often lack those resources.
[00:25:49.480 --> 00:25:56.120]   So they are the targets of phishing, of email scams, and of other ways to infiltrate your
[00:25:56.120 --> 00:25:57.120]   accounts.
[00:25:57.120 --> 00:26:03.320]   So, the question is creating exponentially larger threats because every campaign has valuable
[00:26:03.320 --> 00:26:10.360]   secrets and important information, and they just don't have the resources to defend themselves.
[00:26:10.360 --> 00:26:17.920]   So before Windows died on me, one thing I'd like to point out is that with every story
[00:26:17.920 --> 00:26:24.520]   that we do in this series, we discover more and more interesting little crevices that
[00:26:24.520 --> 00:26:30.320]   we could poke our head into, and there simply is so many vulnerabilities as everyone who
[00:26:30.320 --> 00:26:37.160]   works in IT or security knows, every digital device, and especially with the growth of
[00:26:37.160 --> 00:26:40.600]   the Internet of Things, everything is vulnerable.
[00:26:40.600 --> 00:26:44.320]   So it's impossible to cover everything, and what we wanted to do is cover the biggest
[00:26:44.320 --> 00:26:46.400]   threats to election security.
[00:26:46.400 --> 00:26:51.000]   I think once again, right before Windows died there, we were talking about influence campaigns
[00:26:51.000 --> 00:26:55.840]   and the role of the social web and cognitive hacking.
[00:26:55.840 --> 00:27:03.600]   We see state-sponsored cognitive hacking pop up in almost every country, or at least
[00:27:03.600 --> 00:27:05.600]   every powerful country.
[00:27:05.600 --> 00:27:06.600]   But how?
[00:27:06.600 --> 00:27:12.360]   So for years, we've had advertising, which is a form of cognitive hacking.
[00:27:12.360 --> 00:27:19.680]   In fact, right now, we're in the middle of the hottest season for TV ads for the election
[00:27:19.680 --> 00:27:21.080]   in California.
[00:27:21.080 --> 00:27:28.280]   And when you watch these ads, it almost feels like they're blatantly misrepresenting the
[00:27:28.280 --> 00:27:29.280]   subject.
[00:27:29.280 --> 00:27:30.840]   They're full of scare tactics.
[00:27:30.840 --> 00:27:36.840]   They're full of statements that aren't technically wrong but are absolutely misstating the subject.
[00:27:36.840 --> 00:27:38.400]   And I'm sure this happens in every state.
[00:27:38.400 --> 00:27:40.240]   It's kind of mind-boggling.
[00:27:40.240 --> 00:27:42.240]   How is that different?
[00:27:42.240 --> 00:27:46.840]   This legal advertising, how is that different from the cognitive hacking you're talking
[00:27:46.840 --> 00:27:47.840]   about?
[00:27:47.840 --> 00:27:53.080]   It's just another blade of the same tool.
[00:27:53.080 --> 00:27:54.960]   Scale coordination and intent.
[00:27:54.960 --> 00:28:00.360]   Look, the scale that we see with social media is much larger than television.
[00:28:00.360 --> 00:28:01.360]   Oh, really?
[00:28:01.360 --> 00:28:04.960]   Do you think more effective, Facebook's more effective than a TV ad?
[00:28:04.960 --> 00:28:10.280]   Well, more people can access the internet and social media than access advertising on
[00:28:10.280 --> 00:28:11.760]   television.
[00:28:11.760 --> 00:28:17.800]   And you can certainly access groups and different clusters of individuals.
[00:28:17.800 --> 00:28:19.240]   You could be more efficient, can't you?
[00:28:19.240 --> 00:28:23.760]   I mean, if you buy an ad on a football game on a Sunday afternoon, you're getting a vast
[00:28:23.760 --> 00:28:24.760]   audience.
[00:28:24.760 --> 00:28:29.800]   But with the Facebook, you can buy, I want to buy 35-year-old men in Mendocino County
[00:28:29.800 --> 00:28:33.800]   because I know that I can either convince them not to vote, which I think is really
[00:28:33.800 --> 00:28:38.120]   sleazy, or I can convince them to vote in another way.
[00:28:38.120 --> 00:28:39.600]   Or you can repress votes.
[00:28:39.600 --> 00:28:40.600]   Yeah.
[00:28:40.600 --> 00:28:43.760]   Suppression really is very effective, isn't it?
[00:28:43.760 --> 00:28:48.600]   Yeah, Saudi Arabia has been running influence campaigns against, and we're running influence
[00:28:48.600 --> 00:28:50.840]   campaigns against Khashoggi.
[00:28:50.840 --> 00:28:55.280]   And we now see the data that has popped up proving this.
[00:28:55.280 --> 00:28:59.160]   So when you- But they would just call that.
[00:28:59.160 --> 00:29:00.600]   That's not, he's not running for office, obviously.
[00:29:00.600 --> 00:29:02.200]   He was a Washington Post columnist.
[00:29:02.200 --> 00:29:05.000]   They would just call that counter-programming, right?
[00:29:05.000 --> 00:29:06.000]   Yeah.
[00:29:06.000 --> 00:29:07.160]   Khashoggi said one thing.
[00:29:07.160 --> 00:29:08.320]   We don't think that's true.
[00:29:08.320 --> 00:29:11.760]   So we're going to state our point of view and use social media to do so.
[00:29:11.760 --> 00:29:12.760]   Yeah.
[00:29:12.760 --> 00:29:18.040]   According to the Times story, the Saudi is hired individuals, humans, because on the flip side
[00:29:18.040 --> 00:29:24.840]   of automation, look, all of these companies, Facebook, most notoriously, repeats the line,
[00:29:24.840 --> 00:29:30.640]   we are going to hire 20,000 employees plus artificial intelligence to tamp down on these
[00:29:30.640 --> 00:29:32.200]   influence campaigns.
[00:29:32.200 --> 00:29:37.720]   In the- On Saturday, yesterday, The New York Times published this story, "Troll Army" and
[00:29:37.720 --> 00:29:42.000]   Twitter insider, in which they talk about the fact that the Saudis hired-
[00:29:42.000 --> 00:29:43.560]   Hired humans.
[00:29:43.560 --> 00:29:51.200]   A human, an employee of Twitter, basically, just like you would hire a mole, a spy, a
[00:29:51.200 --> 00:29:53.200]   counter-intelligence agent.
[00:29:53.200 --> 00:29:54.640]   They support an employee.
[00:29:54.640 --> 00:29:56.280]   They support an employee.
[00:29:56.280 --> 00:29:57.280]   Yeah.
[00:29:57.280 --> 00:30:00.520]   Twitter eventually fired him when they were told by American intelligence agencies what
[00:30:00.520 --> 00:30:01.880]   the Saudis were up to.
[00:30:01.880 --> 00:30:03.640]   But that is fascinating.
[00:30:03.640 --> 00:30:10.040]   Well, they also hired humans, people inside Saudi Arabia, they paid them the equivalent
[00:30:10.040 --> 00:30:16.760]   of $3,000 American dollars, because they found that automation was tamping down on their
[00:30:16.760 --> 00:30:19.080]   attempts to run bots and trolls.
[00:30:19.080 --> 00:30:25.200]   So they hired humans to write Twitter content that was designed to repress or attack Khashoggi.
[00:30:25.200 --> 00:30:31.760]   Now, if we look at the indictment that came out on Friday from the DOJ, this was very
[00:30:31.760 --> 00:30:35.720]   explicit about not just the money and the humans who were involved.
[00:30:35.720 --> 00:30:43.200]   This involved oligarchs, funneling money designed to repress groups here in the United States
[00:30:43.200 --> 00:30:44.800]   and inflame groups.
[00:30:44.800 --> 00:30:47.400]   This is directly from the indictment.
[00:30:47.400 --> 00:30:52.360]   But they pushed- They tried to repress African-American voters.
[00:30:52.360 --> 00:30:58.480]   They tried to inflame voters around topics like the Colin Kaepernick issue around Black
[00:30:58.480 --> 00:31:04.720]   Lives Matters, around the burning of the American flag, around the issues of removing
[00:31:04.720 --> 00:31:06.360]   Confederate statues.
[00:31:06.360 --> 00:31:07.600]   You can take all of these issues.
[00:31:07.600 --> 00:31:09.200]   This is the Saudis doing that?
[00:31:09.200 --> 00:31:11.520]   No, this is Russia.
[00:31:11.520 --> 00:31:13.720]   The Russia in 2016.
[00:31:13.720 --> 00:31:15.040]   What was their goal in that?
[00:31:15.040 --> 00:31:19.640]   I mean, I don't- The goal was to repress voters or repress particular groups.
[00:31:19.640 --> 00:31:24.000]   Especially African-Americans say, well, don't even bother to vote because it's hopeless.
[00:31:24.000 --> 00:31:30.040]   Precisely, but you can scale this to or apply this to countries.
[00:31:30.040 --> 00:31:32.360]   We also see this happening in Myanmar.
[00:31:32.360 --> 00:31:37.000]   We see this happening in Eastern Europe.
[00:31:37.000 --> 00:31:41.280]   We see this happening with powerful nations like China.
[00:31:41.280 --> 00:31:45.280]   China is incredibly good at running influence campaigns.
[00:31:45.280 --> 00:31:50.540]   They find this to be almost as effective as we were saying earlier, targeting or hacking
[00:31:50.540 --> 00:31:52.360]   voting machines.
[00:31:52.360 --> 00:32:00.040]   The goal is to undermine faith and confidence in institutions and are American faith and
[00:32:00.040 --> 00:32:01.800]   confidence in each other.
[00:32:01.800 --> 00:32:04.760]   So if we wonder, why are we so inflamed?
[00:32:04.760 --> 00:32:09.000]   Why did social media all of a sudden become such a dumpster fire?
[00:32:09.000 --> 00:32:11.680]   Well, one big reason is us.
[00:32:11.680 --> 00:32:13.560]   We respond to these signals.
[00:32:13.560 --> 00:32:20.000]   When you see that this is propaganda is a time tested tactic.
[00:32:20.000 --> 00:32:26.040]   When you see images over and over and over again, you are more inclined to be sympathetic
[00:32:26.040 --> 00:32:32.360]   or to have an emotional reaction as opposed to an intellectual or a slower reaction.
[00:32:32.360 --> 00:32:35.040]   What is the defense against something like that though?
[00:32:35.040 --> 00:32:38.840]   It seems like we're a prime for this kind of influence.
[00:32:38.840 --> 00:32:41.800]   Priming is the right word.
[00:32:41.800 --> 00:32:48.080]   One of the things we take a great pride in in the United States is freedom of the press,
[00:32:48.080 --> 00:32:54.280]   freedom of speech, the right of everybody to talk.
[00:32:54.280 --> 00:32:59.720]   That seems to open us up to any bad actor who wants to take advantage of that.
[00:32:59.720 --> 00:33:04.560]   Now we have social networks which amplify it dramatically in ways that we didn't really
[00:33:04.560 --> 00:33:05.560]   anticipate.
[00:33:05.560 --> 00:33:06.560]   What do you do?
[00:33:06.560 --> 00:33:07.560]   Shut down Facebook.
[00:33:07.560 --> 00:33:14.640]   I think the flip side here is the algorithms in the networks have been polarizing.
[00:33:14.640 --> 00:33:20.240]   A lot of the algorithms that we have in YouTube and Twitter and Facebook are driven by engagement.
[00:33:20.240 --> 00:33:26.480]   If you click on something, then it is a very simple, very dumb thing.
[00:33:26.480 --> 00:33:31.240]   That which is provocative, that which provokes outrage gets clicked on.
[00:33:31.240 --> 00:33:37.120]   Even if it's idle curiosity or that bystander, I want to see the drive past a road accident
[00:33:37.120 --> 00:33:38.680]   and look at it.
[00:33:38.680 --> 00:33:41.280]   These algorithms don't have any...
[00:33:41.280 --> 00:33:43.840]   CUMIS CRISNAL VOLNERABLE to this stuff.
[00:33:43.840 --> 00:33:48.480]   We basically little reaction, little skinner boxes.
[00:33:48.480 --> 00:33:49.480]   Yeah, that's right.
[00:33:49.480 --> 00:33:52.800]   And then you know, we have no ability to fight this at all.
[00:33:52.800 --> 00:33:53.800]   No.
[00:33:53.800 --> 00:33:55.760]   If it bleeds, it leads.
[00:33:55.760 --> 00:34:00.600]   If I can tell you a horrible story about election hacking in Missouri, I'm going to
[00:34:00.600 --> 00:34:02.240]   come on that and then be out right.
[00:34:02.240 --> 00:34:04.680]   This has been coming for a long time.
[00:34:04.680 --> 00:34:10.080]   We've watched this happen on TV in the 60s that this has been coming for a long time.
[00:34:10.080 --> 00:34:14.720]   I feel like it's an oncoming freight train that nobody has any ability to stop.
[00:34:14.720 --> 00:34:19.560]   And really has very little to do with any one particular election.
[00:34:19.560 --> 00:34:21.600]   Certainly has nothing to do with a partisan side.
[00:34:21.600 --> 00:34:26.880]   We saw evidence of bots propping up Bernie Sanders as much as Donald Trump during the
[00:34:26.880 --> 00:34:28.680]   2016 election.
[00:34:28.680 --> 00:34:32.200]   If your goal is to divide a country, then that's exactly what you do, right?
[00:34:32.200 --> 00:34:33.600]   Yeah, that's exactly right.
[00:34:33.600 --> 00:34:39.320]   So no matter what side you may be on in whatever country you may be listening or watching this,
[00:34:39.320 --> 00:34:40.960]   this isn't about politics.
[00:34:40.960 --> 00:34:48.160]   This is about the use, a novel, a new use of the social web to spread misinformation.
[00:34:48.160 --> 00:34:52.960]   And somebody who wants to be a bad actor and has enough money to do so like Russia or Saudi
[00:34:52.960 --> 00:34:57.160]   Arabia really can act in an almost unfettered way.
[00:34:57.160 --> 00:34:58.160]   Yes.
[00:34:58.160 --> 00:34:59.160]   That's correct.
[00:34:59.160 --> 00:35:01.680]   And we're seeing this, the Democrats, both the Democrats and the Republicans in the US
[00:35:01.680 --> 00:35:07.960]   and increasingly governments around the world are hiring and building specialist IT teams
[00:35:07.960 --> 00:35:15.160]   to produce those bots and to publish those articles, to blog and to cover them.
[00:35:15.160 --> 00:35:21.320]   And what we didn't see up until now is the mainstream politics taking action in its own
[00:35:21.320 --> 00:35:23.400]   right to own the ground that it covers.
[00:35:23.400 --> 00:35:30.360]   So they were out there using the press and doing face to face and walking down the streets.
[00:35:30.360 --> 00:35:35.240]   And other actors moved faster into new technologies and we're hacking up Facebook, Twitter and
[00:35:35.240 --> 00:35:37.680]   Google to get the results that they wanted, right?
[00:35:37.680 --> 00:35:43.800]   So now politics is catching up because it now knows that it has to react to that.
[00:35:43.800 --> 00:35:49.240]   And we saw the, I've seen various interviews with the Democrat IT and the Republican IT
[00:35:49.240 --> 00:35:51.440]   teams and just how effective they were.
[00:35:51.440 --> 00:35:53.600]   Yeah, Greg, you're exactly right.
[00:35:53.600 --> 00:35:57.560]   And this really, again, has very little to do with politics.
[00:35:57.560 --> 00:36:04.080]   This is, we say cognitive hacking, but it could easily be called algorithmic hacking.
[00:36:04.080 --> 00:36:11.000]   The issues are more important or at least more diverse than simply a partisan issue on the
[00:36:11.000 --> 00:36:14.320]   right or the left or even one particular election.
[00:36:14.320 --> 00:36:20.920]   If you think about the interests of not just state groups, but almost any group, information
[00:36:20.920 --> 00:36:25.240]   campaigns are a valuable asset in your arsenal.
[00:36:25.240 --> 00:36:30.640]   And technology now has, I mean, when we all started and certainly Leo and Greg, when you
[00:36:30.640 --> 00:36:34.080]   started, technology was far more of a vertical.
[00:36:34.080 --> 00:36:38.880]   Now it's a horizontal and impacts every component of life and culture.
[00:36:38.880 --> 00:36:41.920]   So it is not just about politics.
[00:36:41.920 --> 00:36:47.920]   It is about the power of technology and the influence and role of technology in life.
[00:36:47.920 --> 00:36:54.720]   And we really have to think about how we use technology and the leaders of technology
[00:36:54.720 --> 00:36:59.440]   companies have to think of themselves as cultural arbiters as much as they are technological
[00:36:59.440 --> 00:37:00.440]   arbiters.
[00:37:00.440 --> 00:37:04.760]   But if you go back in history and you look at when newspapers started in the early 1900s
[00:37:04.760 --> 00:37:10.000]   and when television started in the 1950s and 60s, exactly the same things happened.
[00:37:10.000 --> 00:37:14.040]   Everybody was hysterical about the fact that politicians were using the TV to reach out
[00:37:14.040 --> 00:37:15.680]   and publish ads.
[00:37:15.680 --> 00:37:19.280]   And that's why you have the disclaimers at the bottom of every political ad.
[00:37:19.280 --> 00:37:21.400]   Yeah, but it was primitive.
[00:37:21.400 --> 00:37:25.160]   I mean, yes, at the time it was disruptive, but it was primitive compared to what people
[00:37:25.160 --> 00:37:26.720]   can do today.
[00:37:26.720 --> 00:37:28.040]   And no, but it's still primitive.
[00:37:28.040 --> 00:37:30.160]   What we're seeing with fake
[00:37:30.160 --> 00:37:32.160]   words relatively primitive.
[00:37:32.160 --> 00:37:33.160]   No, if it's.
[00:37:33.160 --> 00:37:38.400]   So you're, I think what you're arguing for is, is laws, is government restrictions,
[00:37:38.400 --> 00:37:41.480]   is regulations on this kind of stuff.
[00:37:41.480 --> 00:37:46.000]   I'm, my point would be is that what we've seen is some fairly blunt and rather brutal
[00:37:46.000 --> 00:37:51.680]   sort of attempts to manipulate a population using a new tool before the before the system
[00:37:51.680 --> 00:37:52.680]   catches up.
[00:37:52.680 --> 00:37:53.680]   Okay.
[00:37:53.680 --> 00:37:54.680]   Great.
[00:37:54.680 --> 00:37:55.680]   That has happened.
[00:37:55.680 --> 00:37:57.880]   And now you think the system will catch up though.
[00:37:57.880 --> 00:37:59.360]   I think we're seeing signs of it.
[00:37:59.360 --> 00:38:00.920]   So there I don't have any hope for that.
[00:38:00.920 --> 00:38:04.960]   We've got articles in the show running list today about Facebook setting up literally
[00:38:04.960 --> 00:38:07.680]   war rooms with people to get this on the control.
[00:38:07.680 --> 00:38:12.160]   But blaming Facebook or Twitter is not at all addressing the problem.
[00:38:12.160 --> 00:38:13.160]   Those are just tools.
[00:38:13.160 --> 00:38:15.240]   But we have an actor who's willing to use.
[00:38:15.240 --> 00:38:16.880]   We'll have to blame the radio.
[00:38:16.880 --> 00:38:17.880]   Yeah.
[00:38:17.880 --> 00:38:21.200]   So that's the point is that it's the tools change constantly and technology is giving
[00:38:21.200 --> 00:38:22.840]   them more and more impact.
[00:38:22.840 --> 00:38:25.080]   But that's, that's going to never go away.
[00:38:25.080 --> 00:38:26.080]   That's not going to change.
[00:38:26.080 --> 00:38:28.080]   So we're just integrating the effect layer.
[00:38:28.080 --> 00:38:30.080]   What I'm saying is that the system takes some time to catch up.
[00:38:30.080 --> 00:38:35.080]   It takes time for people to adapt to the new situation and be able to decipher what is
[00:38:35.080 --> 00:38:38.080]   false information and what is genuine information.
[00:38:38.080 --> 00:38:39.960]   So you're bullish.
[00:38:39.960 --> 00:38:43.080]   You think people will figure it out and it'll stop having so much of that.
[00:38:43.080 --> 00:38:46.680]   No, I think people are dumb and it's going to take a good five to ten years and stupid.
[00:38:46.680 --> 00:38:52.800]   I mean, look at some of those political ads coming out of the deep south in the US.
[00:38:52.800 --> 00:38:55.240]   Those ads are obviously just full of misinformation.
[00:38:55.240 --> 00:38:57.240]   And those people are flat out lying.
[00:38:57.240 --> 00:39:01.120]   But you know, they're still getting published on the television and no one stopping them.
[00:39:01.120 --> 00:39:04.360]   We're going to add a third voice to the mixed Seth Wine Tribe is here.
[00:39:04.360 --> 00:39:05.880]   Seth, it's great to have you.
[00:39:05.880 --> 00:39:07.120]   Hey, thanks for having me, Liam.
[00:39:07.120 --> 00:39:08.680]   My pleasure.
[00:39:08.680 --> 00:39:14.560]   I let you sit and listen a little bit when you first got here just so you knew the tenor
[00:39:14.560 --> 00:39:16.560]   of the conversation.
[00:39:16.560 --> 00:39:19.800]   Seth founded nine to five Mac.
[00:39:19.800 --> 00:39:21.960]   Are you editor in chief at Electric?
[00:39:21.960 --> 00:39:22.960]   Yep.
[00:39:22.960 --> 00:39:25.160]   He's a publisher of both.
[00:39:25.160 --> 00:39:31.600]   He's kind of a legend in the blog community and always a welcome voice in all this.
[00:39:31.600 --> 00:39:34.840]   Did you have any thoughts about what we've been talking about?
[00:39:34.840 --> 00:39:35.840]   Yeah.
[00:39:35.840 --> 00:39:40.560]   So my, like, I don't know how deep you guys have gone into this, but my thing is always
[00:39:40.560 --> 00:39:43.840]   like, well, how intelligent are the politicians?
[00:39:43.840 --> 00:39:46.080]   Are people who are running the show right now?
[00:39:46.080 --> 00:39:48.840]   But the problem to me, Seth, is it's not the politics.
[00:39:48.840 --> 00:39:51.160]   I think the politicians are the least of the problem.
[00:39:51.160 --> 00:39:57.560]   It's state actors in Saudi Arabia and Russia and the Philippines and Myanmar.
[00:39:57.560 --> 00:40:00.320]   Those are the problems, not so much the politicians.
[00:40:00.320 --> 00:40:01.880]   They're just riding on the wave.
[00:40:01.880 --> 00:40:02.880]   Right.
[00:40:02.880 --> 00:40:05.880]   But somebody's got to make laws about you.
[00:40:05.880 --> 00:40:13.280]   And right now the lawmakers, not politicians, but lawmakers don't seem to really know what's
[00:40:13.280 --> 00:40:14.280]   going on.
[00:40:14.280 --> 00:40:15.280]   There's no real thinkers.
[00:40:15.280 --> 00:40:18.720]   Well, there's any, I'll tell you what though, there's an inequality because the people who
[00:40:18.720 --> 00:40:22.520]   benefit from this are then the ones who make the laws.
[00:40:22.520 --> 00:40:23.520]   Right.
[00:40:23.520 --> 00:40:25.000]   So it's like campaign finance reform.
[00:40:25.000 --> 00:40:29.120]   You're never going to have it because the people in charge of the ones who've benefited
[00:40:29.120 --> 00:40:30.520]   from a broken system.
[00:40:30.520 --> 00:40:31.520]   Right.
[00:40:31.520 --> 00:40:36.520]   It's almost like having a media mogul get elected president of Italy, for instance.
[00:40:36.520 --> 00:40:37.520]   Yeah.
[00:40:37.520 --> 00:40:38.520]   Yeah.
[00:40:38.520 --> 00:40:39.520]   That would never happen.
[00:40:39.520 --> 00:40:40.520]   Yeah.
[00:40:40.520 --> 00:40:41.520]   Right.
[00:40:41.520 --> 00:40:42.520]   Except it did.
[00:40:42.520 --> 00:40:43.520]   Oh, yeah.
[00:40:43.520 --> 00:40:44.520]   For those of you who don't know that did happen.
[00:40:44.520 --> 00:40:48.120]   Just in case your sense of irony is suppressed.
[00:40:48.120 --> 00:40:49.120]   Yeah.
[00:40:49.120 --> 00:40:56.800]   I'm glad that at least you, Greg and maybe others on the panel are optimistic.
[00:40:56.800 --> 00:41:00.000]   I find it hard to be too optimistic about this.
[00:41:00.000 --> 00:41:01.000]   I'm kind of with you.
[00:41:01.000 --> 00:41:02.320]   I think it's a little scary to me.
[00:41:02.320 --> 00:41:03.320]   Yeah.
[00:41:03.320 --> 00:41:05.120]   How about you?
[00:41:05.120 --> 00:41:11.240]   I'm optimistic about the fundamental tenets of democracy.
[00:41:11.240 --> 00:41:16.520]   And I'm optimistic about humans as optimists as well.
[00:41:16.520 --> 00:41:18.600]   We want the best.
[00:41:18.600 --> 00:41:19.600]   Yes.
[00:41:19.600 --> 00:41:20.600]   And I agree.
[00:41:20.600 --> 00:41:24.960]   We see this is an anecdote you probably have heard or experienced a million times.
[00:41:24.960 --> 00:41:28.200]   But you can flame online pretty easily.
[00:41:28.200 --> 00:41:30.440]   We look at Reddit or YouTube comments.
[00:41:30.440 --> 00:41:32.800]   It's pretty easy to see the flames.
[00:41:32.800 --> 00:41:36.080]   But when we talk to each other in person, it's a lot harder because we empathize with
[00:41:36.080 --> 00:41:37.080]   each other.
[00:41:37.080 --> 00:41:40.680]   And so my optimism comes from our humanity.
[00:41:40.680 --> 00:41:48.160]   I'm not particularly optimistic about our algorithmic future, at least in the short term.
[00:41:48.160 --> 00:41:50.160]   I think it will take more than policy.
[00:41:50.160 --> 00:41:51.920]   Look, I empathize.
[00:41:51.920 --> 00:41:55.880]   I'm on every single one of these calls with Zuckerberg and a Glaucher.
[00:41:55.880 --> 00:41:59.680]   And I empathize with them.
[00:41:59.680 --> 00:42:04.880]   Social media, many of us advocated in the early days as a democratization of the dissemination
[00:42:04.880 --> 00:42:07.000]   of information.
[00:42:07.000 --> 00:42:13.440]   But we have reached an inflection point where social media is potentially causing more harm
[00:42:13.440 --> 00:42:14.440]   than good.
[00:42:14.440 --> 00:42:19.800]   I think it is a cop out to point a finger and say that it is you know, you, Zuckerberg
[00:42:19.800 --> 00:42:21.960]   or Jack, it's your fault.
[00:42:21.960 --> 00:42:27.360]   I do think that technology companies look to the other way.
[00:42:27.360 --> 00:42:30.360]   In fact, we saw Mark Zuckerberg say this in 2016.
[00:42:30.360 --> 00:42:35.640]   It's ridiculous that anyone would use our platform for these types of purposes.
[00:42:35.640 --> 00:42:39.720]   And now of course, Facebook has done 180 on that.
[00:42:39.720 --> 00:42:45.040]   But I also think that it is pretty easy to say it's their fault, not our fault.
[00:42:45.040 --> 00:42:50.280]   The thing that we need to do is slow down, maybe stop using social media or use it less
[00:42:50.280 --> 00:42:52.280]   or react slower.
[00:42:52.280 --> 00:42:53.960]   >> Yeah, we're seeing that.
[00:42:53.960 --> 00:42:55.400]   >> Cool to each other.
[00:42:55.400 --> 00:42:57.840]   Stop giving Facebook and Google a pass here, right?
[00:42:57.840 --> 00:43:00.720]   Those guys are criminal, they're criminals as far as I'm concerned.
[00:43:00.720 --> 00:43:05.880]   They deliberately ran their businesses to drive profit margin at any cost.
[00:43:05.880 --> 00:43:12.600]   They did everything they could to override the best interests or the obvious good of
[00:43:12.600 --> 00:43:16.800]   the people in society, just so that they could go and brag that they made another billion
[00:43:16.800 --> 00:43:18.640]   dollars to the community, right?
[00:43:18.640 --> 00:43:20.840]   So- >> In a discreet conversation, Greg, I might
[00:43:20.840 --> 00:43:22.560]   agree with you.
[00:43:22.560 --> 00:43:25.800]   But the reporter in me says that it is more nuanced issue.
[00:43:25.800 --> 00:43:27.160]   >> Of course, it's nuanced.
[00:43:27.160 --> 00:43:29.840]   But first of all, I'm saying the first thing to do is to make sure that you understand
[00:43:29.840 --> 00:43:33.640]   that Silicon Valley is so caught up in its own self-importance, they can't actually see
[00:43:33.640 --> 00:43:35.680]   a rational grown-up discussion.
[00:43:35.680 --> 00:43:37.320]   And they still can't even accept that.
[00:43:37.320 --> 00:43:38.320]   They still won't apologize.
[00:43:38.320 --> 00:43:39.320]   >> What do you want them to do?
[00:43:39.320 --> 00:43:40.320]   >> I want them to apologize.
[00:43:40.320 --> 00:43:41.320]   I want them to apologize.
[00:43:41.320 --> 00:43:45.200]   I want them to say, I'm sorry, I did something wrong and now I'm going to make it right.
[00:43:45.200 --> 00:43:50.200]   >> Meanwhile, Sundar Pachai says, yeah, we're making a search engine for China.
[00:43:50.200 --> 00:43:51.440]   What about it?
[00:43:51.440 --> 00:43:52.440]   What of it?
[00:43:52.440 --> 00:43:54.280]   I think these companies are driven as they should be.
[00:43:54.280 --> 00:43:57.560]   They have a fiduciary responsibility to their stakeholders by profit.
[00:43:57.560 --> 00:44:02.520]   >> Yeah, but those stakeholders include me and the community and the society that they
[00:44:02.520 --> 00:44:03.520]   have created.
[00:44:03.520 --> 00:44:04.520]   >> Do you have Google stock?
[00:44:04.520 --> 00:44:06.000]   >> It's not about the stock.
[00:44:06.000 --> 00:44:07.480]   That's the point, right?
[00:44:07.480 --> 00:44:09.080]   >> Well, but that is the point.
[00:44:09.080 --> 00:44:13.400]   I think I agree with you morally, ethically, it's about more than the stock.
[00:44:13.400 --> 00:44:16.640]   But that's not how this is all configured.
[00:44:16.640 --> 00:44:17.640]   Is it?
[00:44:17.640 --> 00:44:22.880]   Can you talk an executive of a company that's got a responsibility to shareholders to raise
[00:44:22.880 --> 00:44:27.080]   value that they should really stop doing stuff that makes so much money?
[00:44:27.080 --> 00:44:29.080]   And that's how capitalism has worked for the last two years.
[00:44:29.080 --> 00:44:30.080]   >> Well, that's welcome.
[00:44:30.080 --> 00:44:31.080]   That's what I'm saying.
[00:44:31.080 --> 00:44:36.160]   And we are, by the way, in late stage capitalism, we're in the really predatory part of it.
[00:44:36.160 --> 00:44:38.160]   >> That's exactly right, yes.
[00:44:38.160 --> 00:44:43.920]   >> I would write, these are incredibly nuanced and challenging issues to solve.
[00:44:43.920 --> 00:44:50.360]   I certainly don't have solutions, but I would like to hear from the Twit Army.
[00:44:50.360 --> 00:44:53.120]   You guys are very smart, you're tech savvy.
[00:44:53.120 --> 00:44:58.200]   And if you have solutions or if you have thoughts on particular solutions, particularly not
[00:44:58.200 --> 00:45:01.200]   just about election hacking, again, this has nothing to do with politics.
[00:45:01.200 --> 00:45:04.240]   It's everything to do with the role of technology and our life.
[00:45:04.240 --> 00:45:07.000]   But if you have thoughts, I'd really like to hear from you.
[00:45:07.000 --> 00:45:09.240]   We will take these seriously.
[00:45:09.240 --> 00:45:16.440]   And I guarantee you we will take this into the newsroom at CBS News and examine particular
[00:45:16.440 --> 00:45:17.840]   solutions.
[00:45:17.840 --> 00:45:23.760]   And once again, this audience knows tech better than almost any audience.
[00:45:23.760 --> 00:45:28.280]   And your insights will be taken seriously.
[00:45:28.280 --> 00:45:30.000]   >> How should they get ahold of you, Dan?
[00:45:30.000 --> 00:45:31.000]   >> Just Twitter.
[00:45:31.000 --> 00:45:33.600]   >> You joined the bots.
[00:45:33.600 --> 00:45:40.600]   >> Somebody in the chatroom did say something.
[00:45:40.600 --> 00:45:43.960]   I thought, why don't we just shorten the election season?
[00:45:43.960 --> 00:45:48.520]   >> I mean, part of the reason this all works is we've got months to do this.
[00:45:48.520 --> 00:45:50.200]   >> Great.
[00:45:50.200 --> 00:45:51.200]   How do we do that?
[00:45:51.200 --> 00:45:52.200]   >> Yeah, I don't know.
[00:45:52.200 --> 00:45:54.200]   >> Oh, yeah, that.
[00:45:54.200 --> 00:46:00.200]   Seth, I feel bad for you because you got in the middle of a very hot conversation.
[00:46:00.200 --> 00:46:04.120]   >> But you can ever apply CBS News or Dan Patterson on Twitter.
[00:46:04.120 --> 00:46:05.120]   These are not plugs for us.
[00:46:05.120 --> 00:46:06.720]   >> No, no, I think it's great.
[00:46:06.720 --> 00:46:07.720]   I think you're right.
[00:46:07.720 --> 00:46:13.960]   People listen to this show are very engaged in technology and we'll have opinions.
[00:46:13.960 --> 00:46:16.360]   And by the way, come from across the political spectrum.
[00:46:16.360 --> 00:46:21.080]   >> And one thing I want to say here is this isn't just about the US.
[00:46:21.080 --> 00:46:25.400]   So everything that you've talked about here applies equally to Germany, to the UK, but
[00:46:25.400 --> 00:46:30.680]   also to places you probably aren't thinking about like Indonesia and China.
[00:46:30.680 --> 00:46:34.760]   I mean, what is happening with social media in China?
[00:46:34.760 --> 00:46:36.360]   Is it controlled by the government?
[00:46:36.360 --> 00:46:40.880]   And then you've got moral questions about do we have a responsibility to interfere in
[00:46:40.880 --> 00:46:41.880]   other countries operations?
[00:46:41.880 --> 00:46:46.200]   And at the same time, keep in mind that on one hand Facebook is being slammed by the
[00:46:46.200 --> 00:46:48.760]   US government and by US citizens.
[00:46:48.760 --> 00:46:52.960]   But at the same time, the UK government's coming after them as well as the European government
[00:46:52.960 --> 00:46:55.880]   as well as the Australian government and so on and so forth.
[00:46:55.880 --> 00:46:58.360]   So it is actually a complicated issue.
[00:46:58.360 --> 00:47:00.920]   But quite honestly, Facebook's going to get everything it deserves.
[00:47:00.920 --> 00:47:06.200]   >> There's only so much we can ask of Elon Musk.
[00:47:06.200 --> 00:47:09.880]   >> It sounds like that one.
[00:47:09.880 --> 00:47:13.240]   >> We could talk for hours about Elon.
[00:47:13.240 --> 00:47:14.440]   >> Elon, what a character.
[00:47:14.440 --> 00:47:15.440]   Huh?
[00:47:15.440 --> 00:47:16.440]   What's going on with Elon?
[00:47:16.440 --> 00:47:17.840]   I go back and forth.
[00:47:17.840 --> 00:47:21.360]   Sometimes I think, thank God Elon exists.
[00:47:21.360 --> 00:47:24.760]   And then sometimes I think, what the hell's going on with him?
[00:47:24.760 --> 00:47:26.320]   >> I think he needs more time off.
[00:47:26.320 --> 00:47:27.320]   >> Maybe that's it.
[00:47:27.320 --> 00:47:30.040]   >> Away from social media, probably time off.
[00:47:30.040 --> 00:47:34.360]   >> I think this one thing, if you get one thing from this conversation, we all need time
[00:47:34.360 --> 00:47:36.040]   off from social media.
[00:47:36.040 --> 00:47:39.640]   I've never been, I don't want to beat a dead horse.
[00:47:39.640 --> 00:47:43.160]   I know, because I've told our audience as many times and I'm sorry for repeating it,
[00:47:43.160 --> 00:47:44.600]   but I don't know if these guys know.
[00:47:44.600 --> 00:47:47.480]   I am completely off Facebook and Twitter and Instagram and that.
[00:47:47.480 --> 00:47:48.480]   >> That's great.
[00:47:48.480 --> 00:47:50.280]   >> I've never felt better.
[00:47:50.280 --> 00:47:53.560]   >> Yeah, I just went to.
[00:47:53.560 --> 00:47:57.560]   >> Yeah, I think probably some of you feel like, well as a journalist, I can't get off
[00:47:57.560 --> 00:47:58.560]   these.
[00:47:58.560 --> 00:48:01.160]   >> I can't get on the public for work.
[00:48:01.160 --> 00:48:05.760]   >> I'm going all in on Google+, I think there's a real future.
[00:48:05.760 --> 00:48:09.000]   >> On the Google+, did you?
[00:48:09.000 --> 00:48:14.480]   >> I know that the SEO, I know a bunch of people who are SEO fiends and they are really
[00:48:14.480 --> 00:48:19.080]   quite upset because one of the best ways to jack the SEO for a website was to post content
[00:48:19.080 --> 00:48:20.080]   into Google+.
[00:48:20.080 --> 00:48:24.760]   >> That explains a lot about why Google+ is the wasteland it is today.
[00:48:24.760 --> 00:48:28.720]   >> Yeah, you'll find a lot of links in there, but apparently the code that Google wrote
[00:48:28.720 --> 00:48:33.080]   to poll Google+ for links and then to prioritize it still existed.
[00:48:33.080 --> 00:48:38.520]   So it's one of the most popular SEO hacks is just pump rubbish into Google+, and you
[00:48:38.520 --> 00:48:39.560]   could jack your SEO.
[00:48:39.560 --> 00:48:42.520]   >> There's a wild Twitter storm.
[00:48:42.520 --> 00:48:45.960]   >> And it's great.
[00:48:45.960 --> 00:48:48.600]   >> It's about like 800 tweets.
[00:48:48.600 --> 00:48:54.040]   A guy named Morgan Mootson, who was a designer and he's now, I think he's done a couple of
[00:48:54.040 --> 00:48:59.360]   startups, he's now an angel investor, so he can speak freely.
[00:48:59.360 --> 00:49:03.040]   He says now the Google+ has been shuttered, that's a little premature, it actually doesn't
[00:49:03.040 --> 00:49:07.200]   go out of business to longest, although I feel like it's pretty much done.
[00:49:07.200 --> 00:49:13.760]   I should air my dirty laundry on how awful the project and the executive team was.
[00:49:13.760 --> 00:49:17.520]   I'm still pissed about the bait and switch he apparently was hired at Google, they said
[00:49:17.520 --> 00:49:21.960]   you're going to work on Chrome and then they said, oh by the way, no, you're working on
[00:49:21.960 --> 00:49:22.960]   Google+.
[00:49:22.960 --> 00:49:30.840]   Putting me on this God forsaken piece of poop on day one, he's very critical of the organization.
[00:49:30.840 --> 00:49:36.240]   He says some good things, he said, and he hurts, felt was there, the guy's a God, it was amazing,
[00:49:36.240 --> 00:49:43.160]   but he said that the real problem was, well, Sundar Pichai, the guy who was in charge of
[00:49:43.160 --> 00:49:48.440]   Google+, he says Google+ was situated in the main building, 1900, a floor away from Larry's
[00:49:48.440 --> 00:49:50.920]   office, Larry Page, the CEO.
[00:49:50.920 --> 00:49:54.240]   If you're one of the 12,000 people of Google in Mountain View who didn't work on Plus,
[00:49:54.240 --> 00:49:57.320]   you didn't have access to those floors.
[00:49:57.320 --> 00:50:01.000]   Each didn't just have an office, the entire floor was his.
[00:50:01.000 --> 00:50:05.240]   We all had access to it, but we're encouraged to use it sparingly, a warm room here and there.
[00:50:05.240 --> 00:50:10.040]   We had access to his cafe to a super fancy vegan cafe called Cloud.
[00:50:10.040 --> 00:50:11.400]   Have you been to Cloud?
[00:50:11.400 --> 00:50:12.400]   No.
[00:50:12.400 --> 00:50:17.760]   That wouldn't be sustainable in the real world.
[00:50:17.760 --> 00:50:20.120]   But the reason was, he says, why this exclusivity?
[00:50:20.120 --> 00:50:21.720]   What made Google+ so special?
[00:50:21.720 --> 00:50:25.080]   Why was it held so close to Google's chest?
[00:50:25.080 --> 00:50:28.400]   I'd find out later the senior vice president of Plus used his cloud to swing all of this.
[00:50:28.400 --> 00:50:30.480]   His name was Vic Gundotra.
[00:50:30.480 --> 00:50:32.600]   He was relatively charismatic.
[00:50:32.600 --> 00:50:37.000]   I remember him frequently flirting with the women on the team, gave me a compounded,
[00:50:37.000 --> 00:50:39.520]   horrible impression of him.
[00:50:39.520 --> 00:50:40.960]   He says I worked there eight months.
[00:50:40.960 --> 00:50:42.520]   Vic never said one word to me.
[00:50:42.520 --> 00:50:44.720]   No, hello, no, goodbye, no, thanks for staying late.
[00:50:44.720 --> 00:50:48.320]   No handshake, not even any eye contact.
[00:50:48.320 --> 00:50:50.080]   Vic's product vision was fear-based.
[00:50:50.080 --> 00:50:55.240]   Google built the knowledge graph and Facebook has swooped in and built the social graph.
[00:50:55.240 --> 00:50:58.200]   If Google doesn't own the social graph, then we can't claim to have indexed all the world's
[00:50:58.200 --> 00:50:59.200]   data.
[00:50:59.200 --> 00:51:00.520]   It made sense at the time.
[00:51:00.520 --> 00:51:06.160]   So much so that as we know, Google was giving people bonuses based on the success of Google
[00:51:06.160 --> 00:51:07.160]   Plus.
[00:51:07.160 --> 00:51:12.560]   If your team, say Android or Gmail was to integrate Google+ features, then you would
[00:51:12.560 --> 00:51:18.160]   get a one and a half to three times multiplier on top of your yearly bonus.
[00:51:18.160 --> 00:51:20.160]   We actually knew that at the time.
[00:51:20.160 --> 00:51:21.160]   I remember.
[00:51:21.160 --> 00:51:22.160]   Yeah.
[00:51:22.160 --> 00:51:26.840]   Not as controversial as it is in this particular context.
[00:51:26.840 --> 00:51:33.200]   He said he writes a F ton of money to ruin the product you were building with bloated
[00:51:33.200 --> 00:51:35.360]   garbage no one wanted.
[00:51:35.360 --> 00:51:38.280]   Nobody really liked this, but people drank the Kool-Aid because it was mostly green and
[00:51:38.280 --> 00:51:39.280]   made of paper.
[00:51:39.280 --> 00:51:41.320]   I've done that.
[00:51:41.320 --> 00:51:47.360]   Everything in this rant is like, "Sure, done that, done that, done that."
[00:51:47.360 --> 00:51:50.680]   I've worked in big companies all over the world.
[00:51:50.680 --> 00:51:55.840]   The CEO gets a hard on for a particular project or a senior executive decides that this is
[00:51:55.840 --> 00:51:59.000]   going to be his road to the top.
[00:51:59.000 --> 00:52:02.760]   What the road to the top might not actually be inside the company he's working for.
[00:52:02.760 --> 00:52:06.800]   It might be a project that he could put on his resume and carry over to the next company
[00:52:06.800 --> 00:52:08.240]   and say, "Look, I did this.
[00:52:08.240 --> 00:52:11.200]   You should hire me for five times what I'm currently earning."
[00:52:11.200 --> 00:52:16.720]   Nothing in this rant strikes me as anything but like, "Sure, whatever.
[00:52:16.720 --> 00:52:17.720]   It's normal."
[00:52:17.720 --> 00:52:23.640]   Business, as usual in American company, he does, Knudsen writes, "I had the comment and
[00:52:23.640 --> 00:52:25.360]   I want to make sure we say this.
[00:52:25.360 --> 00:52:29.880]   Common thread between 99.8% of the people that I interacted with at Google is they were ethical,
[00:52:29.880 --> 00:52:31.960]   highly intelligent and hardworking.
[00:52:31.960 --> 00:52:35.160]   I had a lot of admiration and respect for many of them.
[00:52:35.160 --> 00:52:39.560]   But this team that he was on, this Google+ team was horrifically dysfunctional.
[00:52:39.560 --> 00:52:43.160]   Furthermore, there was the kind of dysfunction that we saw at Microsoft that I think has
[00:52:43.160 --> 00:52:50.280]   been fixed since where different fiefdoms would basically say, "Our success is paramount.
[00:52:50.280 --> 00:52:51.880]   Your success just gets in our way."
[00:52:51.880 --> 00:53:02.800]   He talks about designing a way to get chat into Google+ and he said the whole vision
[00:53:02.800 --> 00:53:07.600]   was my vision was to integrate Google's other apps into this sidebar navigation.
[00:53:07.600 --> 00:53:14.360]   He approached the guy over at Gmail.
[00:53:14.360 --> 00:53:18.400]   The Gmail guy said, "Are you kidding?
[00:53:18.400 --> 00:53:19.560]   We're trying to put this in Gmail.
[00:53:19.560 --> 00:53:22.120]   If we let you do this from Google+ it's going to make us look bad.
[00:53:22.120 --> 00:53:24.480]   Google I/Os coming up the hell with that.
[00:53:24.480 --> 00:53:26.760]   We're not doing it."
[00:53:26.760 --> 00:53:29.000]   You see that all the time in these kinds of big corporations.
[00:53:29.000 --> 00:53:32.680]   We're different product lines basically.
[00:53:32.680 --> 00:53:38.040]   Over years, Office and Windows sabotaged each other at Microsoft.
[00:53:38.040 --> 00:53:39.200]   But you make a good point, Greg.
[00:53:39.200 --> 00:53:40.360]   This is normal.
[00:53:40.360 --> 00:53:41.360]   This is normal.
[00:53:41.360 --> 00:53:47.400]   It especially happens in companies that are making vast wads of cash where egos are large.
[00:53:47.400 --> 00:53:52.000]   I've worked in various.com companies since 1998.
[00:53:52.000 --> 00:53:57.520]   I've seen people come in and throw around the ego and the big words and spend up big
[00:53:57.520 --> 00:53:59.200]   on these projects.
[00:53:59.200 --> 00:54:03.880]   I've worked with plenty of dysfunctional projects where this is not new to me.
[00:54:03.880 --> 00:54:08.600]   The challenge here is that when companies are just awash with cash like this, this sort
[00:54:08.600 --> 00:54:11.040]   of stuff happens and nobody notices or cares.
[00:54:11.040 --> 00:54:14.760]   This is happening right the way through Silicon Valley.
[00:54:14.760 --> 00:54:20.160]   These companies have so much money, even startups who run around complaining how little money
[00:54:20.160 --> 00:54:21.160]   they have.
[00:54:21.160 --> 00:54:24.280]   I tell you what, you want to run a small business in the backwoods of Australia or the
[00:54:24.280 --> 00:54:25.280]   US.
[00:54:25.280 --> 00:54:27.280]   I promise you, you'll discover what a lack of cash is.
[00:54:27.280 --> 00:54:29.440]   Uber is planning an IBO for example.
[00:54:29.440 --> 00:54:33.080]   Uber is planning an IBO next year for an IPO for $120 billion.
[00:54:33.080 --> 00:54:34.080]   Yeah.
[00:54:34.080 --> 00:54:39.160]   So they think, but that's not the story, but we'll get to this.
[00:54:39.160 --> 00:54:43.000]   That's not the story, but that's just so much amazing money that this sort of dysfunction
[00:54:43.000 --> 00:54:46.000]   becomes institutionalized because you can throw away these projects.
[00:54:46.000 --> 00:54:50.000]   Remember how Google used to just let people work on stuff and they'd go off and work
[00:54:50.000 --> 00:54:53.000]   and then suddenly a group of people would come up with an idea and they'd spend two
[00:54:53.000 --> 00:54:56.280]   years working on something that had no purpose and no point?
[00:54:56.280 --> 00:54:58.280]   That's what this is, no big deal.
[00:54:58.280 --> 00:55:05.760]   Actually, what do you think of that Seth One, you cover of course electric vehicles and
[00:55:05.760 --> 00:55:07.440]   electric?
[00:55:07.440 --> 00:55:11.280]   Is Uber's $120 billion?
[00:55:11.280 --> 00:55:12.280]   It's not their valuation.
[00:55:12.280 --> 00:55:13.280]   Is there IPO?
[00:55:13.280 --> 00:55:18.280]   Yeah, that was reported by the Wall Street Journal, I believe.
[00:55:18.280 --> 00:55:19.280]   Yeah.
[00:55:19.280 --> 00:55:20.640]   And it was kind of quoting some bankers.
[00:55:20.640 --> 00:55:25.400]   So a little bit of grains of salt with that.
[00:55:25.400 --> 00:55:30.320]   You know, and another thing that kind of interests me about Uber a little bit more is that their
[00:55:30.320 --> 00:55:36.240]   CEO Dana, can't remember his last name, mentioned that after...
[00:55:36.240 --> 00:55:38.160]   You can remember, you just don't know how to pronounce it.
[00:55:38.160 --> 00:55:39.160]   I know.
[00:55:39.160 --> 00:55:40.160]   Dara Khoswoshahi.
[00:55:40.160 --> 00:55:41.160]   Khoradeski.
[00:55:41.160 --> 00:55:42.160]   What?
[00:55:42.160 --> 00:55:43.160]   Yes.
[00:55:43.160 --> 00:55:46.560]   Oh, you're not talking about Uber now?
[00:55:46.560 --> 00:55:47.560]   Somebody else?
[00:55:47.560 --> 00:55:48.560]   I'm sorry.
[00:55:48.560 --> 00:55:49.560]   Anyway...
[00:55:49.560 --> 00:55:50.560]   Pardon me, excuse me.
[00:55:50.560 --> 00:55:52.040]   I didn't mean out there.
[00:55:52.040 --> 00:55:53.040]   Thank you.
[00:55:53.040 --> 00:55:54.960]   Half of the business would be scooters and bikes.
[00:55:54.960 --> 00:55:55.960]   Yeah.
[00:55:55.960 --> 00:55:59.600]   So Uber owns Lime.
[00:55:59.600 --> 00:56:03.240]   And they also have this jump electric bike company.
[00:56:03.240 --> 00:56:07.440]   So they're kind of expanding their breadth a little bit.
[00:56:07.440 --> 00:56:12.000]   $120 billion seems a little bit crazy to me because there's quite a bit of competition
[00:56:12.000 --> 00:56:19.680]   and that moats and castles thing isn't really too deep.
[00:56:19.680 --> 00:56:23.680]   Lime in particular on that side is very strong.
[00:56:23.680 --> 00:56:32.160]   You know, Lyft and there's a ton of other smaller car sharing companies.
[00:56:32.160 --> 00:56:36.920]   So I don't see it a hundred billion valuation for Uber.
[00:56:36.920 --> 00:56:39.080]   It seems a little bit crazy.
[00:56:39.080 --> 00:56:42.000]   They were talking in the 50 to 70 previously.
[00:56:42.000 --> 00:56:45.200]   So I think that's probably a little bit more realistic.
[00:56:45.200 --> 00:56:47.560]   But I'm not a banker.
[00:56:47.560 --> 00:56:51.760]   It strikes me that the valuation, certainly the amount of money they've raised and then
[00:56:51.760 --> 00:56:57.280]   if they have some inflated valuation at an IPO is not for their current business, the
[00:56:57.280 --> 00:57:01.920]   ride sharing business, but it is all about the future of Uber with the self-driving car
[00:57:01.920 --> 00:57:02.920]   business, right?
[00:57:02.920 --> 00:57:03.920]   Absolutely.
[00:57:03.920 --> 00:57:09.120]   So think of Uber as Netflix of way back when...
[00:57:09.120 --> 00:57:11.120]   Back when they were still sending CDs out.
[00:57:11.120 --> 00:57:12.680]   They're giving CDs out.
[00:57:12.680 --> 00:57:17.960]   Uber has no plans in the future to have humans or meet their vehicles.
[00:57:17.960 --> 00:57:19.600]   They're all about self-driving things.
[00:57:19.600 --> 00:57:21.600]   It's costing them the profit.
[00:57:21.600 --> 00:57:22.600]   Right.
[00:57:22.600 --> 00:57:24.120]   You can make money.
[00:57:24.120 --> 00:57:25.120]   Exactly.
[00:57:25.120 --> 00:57:26.120]   So that's...
[00:57:26.120 --> 00:57:29.040]   Right now they're sending DVDs to people.
[00:57:29.040 --> 00:57:30.040]   That's what they're doing.
[00:57:30.040 --> 00:57:32.200]   That's their current situation.
[00:57:32.200 --> 00:57:33.200]   But they don't...
[00:57:33.200 --> 00:57:34.200]   They're Uber.
[00:57:34.200 --> 00:57:41.880]   They're going to be a self-driving human delivery service or delivery of any kind of service.
[00:57:41.880 --> 00:57:42.880]   Speaking of which...
[00:57:42.880 --> 00:57:46.040]   Didn't they sell off that part of the business or shut it down recently?
[00:57:46.040 --> 00:57:47.040]   Well, it's...
[00:57:47.040 --> 00:57:48.040]   I think...
[00:57:48.040 --> 00:57:49.040]   Yes.
[00:57:49.040 --> 00:57:52.080]   They're in a lot of trouble when they ran over somebody and killed her.
[00:57:52.080 --> 00:57:55.120]   Yeah, I think that's on the back burner, but I don't think...
[00:57:55.120 --> 00:57:58.520]   But I don't think there's a business if you don't get rid of the meat.
[00:57:58.520 --> 00:58:01.680]   There's going to be a lot of competition.
[00:58:01.680 --> 00:58:02.680]   Right.
[00:58:02.680 --> 00:58:04.160]   I mean, clearly that's where Silicon Valley is.
[00:58:04.160 --> 00:58:05.160]   Look at...
[00:58:05.160 --> 00:58:06.160]   Just putting a lot of their engineers right now.
[00:58:06.160 --> 00:58:13.920]   Look at Instacart, which just raised another $600 million Instacart is a $7.6 billion company.
[00:58:13.920 --> 00:58:15.840]   They deliver groceries.
[00:58:15.840 --> 00:58:16.840]   Yeah.
[00:58:16.840 --> 00:58:21.120]   And by the way, there's a lot of competition in that space.
[00:58:21.120 --> 00:58:24.800]   In fact, from some of the biggest companies in the world like Amazon.
[00:58:24.800 --> 00:58:27.560]   Kind of feels a little bubbly right now.
[00:58:27.560 --> 00:58:28.560]   A little bubbly.
[00:58:28.560 --> 00:58:29.560]   Yeah.
[00:58:29.560 --> 00:58:31.560]   We don't really cover the market, but I just...
[00:58:31.560 --> 00:58:35.480]   Since you brought up these crazy amounts of money that they're throwing around in Silicon
[00:58:35.480 --> 00:58:37.280]   Valley, I thought I'd just mention.
[00:58:37.280 --> 00:58:39.560]   Palantir, $40 billion next year.
[00:58:39.560 --> 00:58:40.560]   $40 billion.
[00:58:40.560 --> 00:58:41.560]   What is it?
[00:58:41.560 --> 00:58:42.560]   Palantir is a security company, right?
[00:58:42.560 --> 00:58:43.560]   Yeah.
[00:58:43.560 --> 00:58:46.240]   Palantir is one of the most interesting companies in the world right now.
[00:58:46.240 --> 00:58:47.240]   They are...
[00:58:47.240 --> 00:58:48.800]   They're a game for Javanalitica.
[00:58:48.800 --> 00:58:49.800]   Yeah.
[00:58:49.800 --> 00:58:50.800]   Are they?
[00:58:50.800 --> 00:58:57.440]   Well, they're not because they don't engage in the same extra legal activities that...
[00:58:57.440 --> 00:58:59.920]   But this is Peter Thiel's company.
[00:58:59.920 --> 00:59:04.480]   And they are a data analysis firm and they work with law enforcement.
[00:59:04.480 --> 00:59:08.080]   So, I mean, we can just say data, right?
[00:59:08.080 --> 00:59:10.480]   And they have some of the smartest people in data.
[00:59:10.480 --> 00:59:12.960]   They're also a very tough egg to crack, but just on the...
[00:59:12.960 --> 00:59:13.960]   They're very secretive.
[00:59:13.960 --> 00:59:14.960]   On the IQ of training.
[00:59:14.960 --> 00:59:15.960]   Extremely super good.
[00:59:15.960 --> 00:59:21.640]   For a company that we have almost no visibility into a $40 billion IPO seems staggering.
[00:59:21.640 --> 00:59:22.640]   Yeah.
[00:59:22.640 --> 00:59:27.600]   Well, remember, there's a lot of money floating around and investors are just desperately
[00:59:27.600 --> 00:59:29.880]   trying to find the next big thing.
[00:59:29.880 --> 00:59:36.400]   And all you really see with these valuations and these IPOs is what a certain group of
[00:59:36.400 --> 00:59:40.000]   people think is going to be the next big thing and they may well be wrong.
[00:59:40.000 --> 00:59:41.600]   Looking for the next Facebook.
[00:59:41.600 --> 00:59:44.000]   There's a lot of Facebook getting the next Google+,.
[00:59:44.000 --> 00:59:48.640]   That's the story right in that shell.
[00:59:48.640 --> 00:59:49.640]   Right there.
[00:59:49.640 --> 00:59:53.720]   There's a lot of fashion following going on in Silicon Valley at the moment.
[00:59:53.720 --> 00:59:57.760]   Everybody, once a particular thing looks like it's got legs, all the investors rush to
[00:59:57.760 --> 00:59:59.680]   get in there, especially the funds.
[00:59:59.680 --> 01:00:01.240]   Well, look at Hollywood.
[01:00:01.240 --> 01:00:02.600]   I mean, they don't make a new movie.
[01:00:02.600 --> 01:00:04.080]   They just remake Mary Poppins.
[01:00:04.080 --> 01:00:05.320]   Look at Broadway.
[01:00:05.320 --> 01:00:06.320]   They don't write a new play.
[01:00:06.320 --> 01:00:08.600]   They just take an old one and put music to it.
[01:00:08.600 --> 01:00:13.960]   I mean, it's not like innovation is the dominant thing.
[01:00:13.960 --> 01:00:20.240]   The trends that they follow is they all rushing back to the musical since the success of Mona.
[01:00:20.240 --> 01:00:24.320]   And then before that, it was rushing back to the comic books, DC and Marvel.
[01:00:24.320 --> 01:00:25.920]   You know, there was a whole rash of those.
[01:00:25.920 --> 01:00:29.720]   If there's a theme to that, it's artists will do what artists do.
[01:00:29.720 --> 01:00:35.760]   The problem is that businesses like Broadway and Hollywood and all these are not run by
[01:00:35.760 --> 01:00:37.320]   visionaries and creatives.
[01:00:37.320 --> 01:00:39.320]   They're run by bean counters.
[01:00:39.320 --> 01:00:41.160]   And bean counters say, well, that worked.
[01:00:41.160 --> 01:00:42.360]   Let's do that again.
[01:00:42.360 --> 01:00:43.360]   Yeah.
[01:00:43.360 --> 01:00:44.360]   They're not.
[01:00:44.360 --> 01:00:47.960]   We can allocate funds to that because I feel that the return on investment would be there.
[01:00:47.960 --> 01:00:49.520]   This is exactly.
[01:00:49.520 --> 01:00:53.320]   And the trick here is that back when Hollywood was making way too much money back in the
[01:00:53.320 --> 01:00:58.960]   50s and 60s, they basically couldn't stuff enough cash in their shirt, you know, to get
[01:00:58.960 --> 01:01:00.240]   they were able to do stupid things.
[01:01:00.240 --> 01:01:03.800]   Look at all the different crazy movies that they made in the 50s, 60s and 70s that made
[01:01:03.800 --> 01:01:04.800]   no sense.
[01:01:04.800 --> 01:01:07.800]   And that's what Silicon Valley is doing, same sort of thing.
[01:01:07.800 --> 01:01:11.920]   San Francisco had a problem with scooters.
[01:01:11.920 --> 01:01:16.040]   There were a lot of them and they were littering the streets.
[01:01:16.040 --> 01:01:17.040]   Are you are you in?
[01:01:17.040 --> 01:01:18.960]   I want to say, Seth, are you in Santa Monica?
[01:01:18.960 --> 01:01:19.960]   Do you down?
[01:01:19.960 --> 01:01:22.200]   I'm actually based in New York.
[01:01:22.200 --> 01:01:24.280]   Oh, you're in Brooklyn or somewhere like that.
[01:01:24.280 --> 01:01:25.280]   Yeah, North.
[01:01:25.280 --> 01:01:26.280]   But yeah.
[01:01:26.280 --> 01:01:27.280]   Yeah.
[01:01:27.280 --> 01:01:28.280]   Do you have a scooter issue there?
[01:01:28.280 --> 01:01:31.480]   Because if you go to Santa Monica, you can't hardly walk.
[01:01:31.480 --> 01:01:33.680]   Yeah, they're everywhere.
[01:01:33.680 --> 01:01:36.720]   I've been there quite a few times since they rolled out.
[01:01:36.720 --> 01:01:37.880]   Actually, they just came to Paris.
[01:01:37.880 --> 01:01:41.800]   I was at the Auto Show in Paris and it totally changed the city.
[01:01:41.800 --> 01:01:43.320]   Like made it so much better.
[01:01:43.320 --> 01:01:46.960]   Yeah, on the one hand, I love it because you can just get one and scoot somewhere.
[01:01:46.960 --> 01:01:47.960]   Yeah.
[01:01:47.960 --> 01:01:48.960]   Yeah.
[01:01:48.960 --> 01:01:49.960]   It's a great city for it except for the problems.
[01:01:49.960 --> 01:01:53.000]   A little dangerous in a city that is built around autos.
[01:01:53.000 --> 01:01:54.520]   Right.
[01:01:54.520 --> 01:01:56.240]   But the problem.
[01:01:56.240 --> 01:02:00.520]   So San Francisco had this problem that all these companies, including Uber's Lime, came
[01:02:00.520 --> 01:02:02.520]   in and just dumped scooters.
[01:02:02.520 --> 01:02:07.120]   Like big trucks pulled up and just threw scooters on the streets.
[01:02:07.120 --> 01:02:08.120]   And there was a mess.
[01:02:08.120 --> 01:02:10.560]   So they banned them all and they said, "We're going to pick two."
[01:02:10.560 --> 01:02:15.360]   On Monday, they picked Skip and Scoot.
[01:02:15.360 --> 01:02:23.200]   Which some of you, I think Heather Hommen in the radio show said sounds like a, right,
[01:02:23.200 --> 01:02:24.960]   Kavanaugh's drinking buddies.
[01:02:24.960 --> 01:02:30.040]   But Skip and Scoot.
[01:02:30.040 --> 01:02:34.480]   And then Lime got so pissed off, they went to the court and tried to get a injunction.
[01:02:34.480 --> 01:02:35.480]   And then...
[01:02:35.480 --> 01:02:36.480]   And did.
[01:02:36.480 --> 01:02:38.480]   And the court said no.
[01:02:38.480 --> 01:02:41.360]   The city has a right to do this.
[01:02:41.360 --> 01:02:42.360]   Yep.
[01:02:42.360 --> 01:02:44.720]   Are you, are you, are you electric going to cover scooters?
[01:02:44.720 --> 01:02:46.560]   Oh, we cover scooters quite a bit.
[01:02:46.560 --> 01:02:51.040]   You know, it's not something that we initially were thinking about when we built the site.
[01:02:51.040 --> 01:02:52.040]   No one was.
[01:02:52.040 --> 01:02:53.840]   Nobody expects the scooter inquisition.
[01:02:53.840 --> 01:02:54.840]   Right.
[01:02:54.840 --> 01:02:56.880]   But now it's like one of our big, big topic areas.
[01:02:56.880 --> 01:02:57.880]   Is it really?
[01:02:57.880 --> 01:02:58.880]   Yeah.
[01:02:58.880 --> 01:03:02.080]   And in general, are you positive on all this?
[01:03:02.080 --> 01:03:03.360]   I personally am.
[01:03:03.360 --> 01:03:08.080]   I think the bigger problem is the cities building the infrastructure around it.
[01:03:08.080 --> 01:03:12.280]   I think, you know, if cities want to have, you know, fewer cars.
[01:03:12.280 --> 01:03:13.280]   You need bike lanes.
[01:03:13.280 --> 01:03:14.280]   You need scooter lanes.
[01:03:14.280 --> 01:03:16.000]   You need places to safely scoot.
[01:03:16.000 --> 01:03:17.000]   Yeah.
[01:03:17.000 --> 01:03:18.000]   Exactly.
[01:03:18.000 --> 01:03:19.000]   Yeah.
[01:03:19.000 --> 01:03:20.000]   So...
[01:03:20.000 --> 01:03:22.920]   You also need to create societal rules around putting scooters back.
[01:03:22.920 --> 01:03:28.160]   No, I would, they arrived while I was in Lisbon for a few weeks last month.
[01:03:28.160 --> 01:03:30.800]   And the scooters, literally when I was first arrived, there was no scooters.
[01:03:30.800 --> 01:03:34.680]   And then all of a sudden there was this litter in the shape of a scooter all over the streets.
[01:03:34.680 --> 01:03:40.000]   And they were literally just being abandoned in the most interesting places.
[01:03:40.000 --> 01:03:43.520]   Lisbon does have a bit of a problem with youth and graffiti and stuff.
[01:03:43.520 --> 01:03:47.000]   And so there's quite a few of them sitting in that water where people are just tossed
[01:03:47.000 --> 01:03:48.000]   to my city.
[01:03:48.000 --> 01:03:49.000]   Same thing happened in San Francisco.
[01:03:49.000 --> 01:03:50.560]   And I think it's happened in LA.
[01:03:50.560 --> 01:03:52.400]   They just throw them in the ocean.
[01:03:52.400 --> 01:03:57.600]   Now, in Britain, I think, I try to remember the article, but I think it was electric bikes
[01:03:57.600 --> 01:04:02.120]   in Manchester, a company moved in there without any council permissions and just started
[01:04:02.120 --> 01:04:04.040]   putting bikes on the streets.
[01:04:04.040 --> 01:04:08.560]   And the loss rate on those bicycles was so high, they had to leave within three months.
[01:04:08.560 --> 01:04:09.560]   Google said that.
[01:04:09.560 --> 01:04:14.160]   Literally, people were just gas-axing off the bits and the batteries and turning them
[01:04:14.160 --> 01:04:15.160]   into the road.
[01:04:15.160 --> 01:04:17.200]   If you go down the mountain view and look at the creeks, you'll see on these Google
[01:04:17.200 --> 01:04:18.200]   bikes.
[01:04:18.200 --> 01:04:20.240]   It's kind of tossed in the creeks.
[01:04:20.240 --> 01:04:22.960]   There's a new kind of urban pollution.
[01:04:22.960 --> 01:04:23.960]   I like this.
[01:04:23.960 --> 01:04:26.920]   This is an article from Electric on Electric Mopedads.
[01:04:26.920 --> 01:04:28.520]   I see I like this idea.
[01:04:28.520 --> 01:04:32.480]   Yeah, and there's quite a few nice ones coming.
[01:04:32.480 --> 01:04:37.320]   A lot of them are based in China, but, you know, Bespas coming out with an electric scooter,
[01:04:37.320 --> 01:04:42.960]   which carries all that cashier, Piagio or whatever.
[01:04:42.960 --> 01:04:48.080]   There's quite a few like name-brand scooter companies coming out with electric versions.
[01:04:48.080 --> 01:04:50.040]   I would love an electric Vespa.
[01:04:50.040 --> 01:04:52.760]   That would be a height of cool, wouldn't it?
[01:04:52.760 --> 01:04:53.760]   This is a flux.
[01:04:53.760 --> 01:04:54.760]   It kind of looks like a Vespa.
[01:04:54.760 --> 01:04:55.760]   Yeah.
[01:04:55.760 --> 01:04:58.360]   Those are a little bit lower powered, but also a lot less expensive.
[01:04:58.360 --> 01:04:59.840]   I believe it's like $2,500.
[01:04:59.840 --> 01:05:03.640]   I just bought a Super73 electric bike because it looks-
[01:05:03.640 --> 01:05:05.160]   Oh, man, those are great, right?
[01:05:05.160 --> 01:05:06.160]   Yeah.
[01:05:06.160 --> 01:05:09.080]   It was a, I think it was a Kickstarter.
[01:05:09.080 --> 01:05:11.920]   And they look like Indian motorcycles, kind of.
[01:05:11.920 --> 01:05:15.720]   They look like old school motorcycles, but they're electric bikes.
[01:05:15.720 --> 01:05:19.920]   I wouldn't compare that to an Indian motorcycle, but I think they're awesome.
[01:05:19.920 --> 01:05:20.920]   Okay.
[01:05:20.920 --> 01:05:21.920]   Okay.
[01:05:21.920 --> 01:05:23.600]   They got fat tires.
[01:05:23.600 --> 01:05:25.280]   They're still awesome.
[01:05:25.280 --> 01:05:26.280]   I can't wait.
[01:05:26.280 --> 01:05:27.280]   I can't wait.
[01:05:27.280 --> 01:05:28.280]   I can't wait.
[01:05:28.280 --> 01:05:31.720]   I'd love the idea of an electric scooter.
[01:05:31.720 --> 01:05:35.160]   And I actually, because I don't own a car, I actually looked into buying these, but they're
[01:05:35.160 --> 01:05:38.320]   stupidly expensive and also stupidly insecure.
[01:05:38.320 --> 01:05:39.320]   Right.
[01:05:39.320 --> 01:05:42.600]   So I don't feel like I can drive this into town and just leave it parked up on the side
[01:05:42.600 --> 01:05:45.360]   of the road because people can just pick it up and put it in the back of my car and drive
[01:05:45.360 --> 01:05:46.360]   away with it.
[01:05:46.360 --> 01:05:47.360]   Yeah.
[01:05:47.360 --> 01:05:50.160]   There has to be some more safety.
[01:05:50.160 --> 01:05:53.040]   Some companies are actually putting GPS in them like a car.
[01:05:53.040 --> 01:05:54.040]   Right.
[01:05:54.040 --> 01:05:55.800]   They're actually trace where they're at.
[01:05:55.800 --> 01:05:58.000]   There's a company called Van Move out of the Netherlands.
[01:05:58.000 --> 01:06:03.520]   It has a nice one that you can trace with your smartphone app.
[01:06:03.520 --> 01:06:07.040]   But cities need to put infrastructure in.
[01:06:07.040 --> 01:06:11.240]   I don't know if sharing is going to eventually end up being the way to go.
[01:06:11.240 --> 01:06:17.560]   I mean, these scooters are getting so light and foldable that you can almost, five or
[01:06:17.560 --> 01:06:22.520]   ten pounds carry it with you, plug it in at your desk.
[01:06:22.520 --> 01:06:23.520]   Yeah.
[01:06:23.520 --> 01:06:26.760]   And our son uses a scooter to get to school.
[01:06:26.760 --> 01:06:29.520]   He's a junior or a sophomore in high school.
[01:06:29.520 --> 01:06:31.120]   And we have, we're very lucky.
[01:06:31.120 --> 01:06:33.240]   I don't know why because it makes no sense at all.
[01:06:33.240 --> 01:06:38.040]   But at some point, the area spent a billion dollars on a light rail service that goes
[01:06:38.040 --> 01:06:40.920]   basically from nowhere to nowhere, nowhere anyone wants to go.
[01:06:40.920 --> 01:06:43.360]   However, it stops in Berlin and stops.
[01:06:43.360 --> 01:06:47.360]   It goes from Santa Rosa to San Rafael.
[01:06:47.360 --> 01:06:49.400]   And it was like a huge expense, but it's fabulous.
[01:06:49.400 --> 01:06:50.680]   This is a nice light rail.
[01:06:50.680 --> 01:06:52.360]   You get on this smart train.
[01:06:52.360 --> 01:06:55.920]   So he scooters to the smart train, gets in the smart train.
[01:06:55.920 --> 01:06:59.120]   You can put your biker scooter there, gets off, scooters another, I think it's about a
[01:06:59.120 --> 01:07:00.880]   mile to school.
[01:07:00.880 --> 01:07:03.040]   It's the greatest thing ever, but he has his own scooter.
[01:07:03.040 --> 01:07:04.040]   He's not running.
[01:07:04.040 --> 01:07:11.880]   So one point, lime scooters, actually this became a big issue, screened, unlock me or
[01:07:11.880 --> 01:07:16.680]   I'll call the police when they're touched.
[01:07:16.680 --> 01:07:19.320]   And this was a big problem in San Francisco.
[01:07:19.320 --> 01:07:23.920]   A female voice from within lime's e-scooters shout the threat to anyone who tries to fiddle
[01:07:23.920 --> 01:07:26.920]   with them without downloading the app and paying.
[01:07:26.920 --> 01:07:31.840]   The companies also set their rights to blast cartoonish robot noises so loud that heads
[01:07:31.840 --> 01:07:33.800]   turn on busy streets.
[01:07:33.800 --> 01:07:39.200]   They disabled that because people figured out that actually they can't possibly call
[01:07:39.200 --> 01:07:40.880]   the police and just saying it.
[01:07:40.880 --> 01:07:42.480]   It's just an empty threat.
[01:07:42.480 --> 01:07:43.480]   Yeah.
[01:07:43.480 --> 01:07:44.480]   Yeah.
[01:07:44.480 --> 01:07:48.880]   So I mean, theoretically they could call the police, but it's a bad.
[01:07:48.880 --> 01:07:49.880]   It's a bad PR message.
[01:07:49.880 --> 01:07:51.320]   Yeah, don't call me the police.
[01:07:51.320 --> 01:07:54.920]   How dumb are these people to think that they're going to get away with that?
[01:07:54.920 --> 01:07:58.360]   Like how much stupid can you put in one place?
[01:07:58.360 --> 01:07:59.360]   That's not stupid.
[01:07:59.360 --> 01:08:00.360]   That's innovation.
[01:08:00.360 --> 01:08:01.360]   Yeah, exactly.
[01:08:01.360 --> 01:08:02.360]   We don't want to...
[01:08:02.360 --> 01:08:03.360]   Oh, my.
[01:08:03.360 --> 01:08:04.360]   Jesus.
[01:08:04.360 --> 01:08:05.360]   All right.
[01:08:05.360 --> 01:08:06.360]   These people are idiots.
[01:08:06.360 --> 01:08:10.200]   I mean, is there any signs that these people are anything but idiots?
[01:08:10.200 --> 01:08:14.240]   I mean, all the evidence points to them being complete, out of absolute idiots.
[01:08:14.240 --> 01:08:15.520]   They can't get it right.
[01:08:15.520 --> 01:08:16.800]   They don't even make the scooters.
[01:08:16.800 --> 01:08:20.280]   They buy them in bulk from China and then litter them around the streets and call it
[01:08:20.280 --> 01:08:22.200]   an innovation, honestly.
[01:08:22.200 --> 01:08:24.640]   But they have the $2 billion dollar valuations now.
[01:08:24.640 --> 01:08:25.640]   So...
[01:08:25.640 --> 01:08:26.640]   That sounds pretty smart to me.
[01:08:26.640 --> 01:08:27.640]   That's a big innovative.
[01:08:27.640 --> 01:08:28.640]   I'll do it.
[01:08:28.640 --> 01:08:30.520]   That doesn't mean they'll have $2 billion valuation tomorrow.
[01:08:30.520 --> 01:08:32.880]   I remember the 1999 dot-com bus.
[01:08:32.880 --> 01:08:33.880]   Yeah, yeah.
[01:08:33.880 --> 01:08:37.200]   But now who's stupid because they're going to take a little bit of that off the table.
[01:08:37.200 --> 01:08:43.000]   And here we are slogging away in the vital and growing podcast movement.
[01:08:43.000 --> 01:08:44.000]   That's...
[01:08:44.000 --> 01:08:45.000]   I'm doing okay.
[01:08:45.000 --> 01:08:46.000]   Oh, good.
[01:08:46.000 --> 01:08:47.000]   I'm glad to hear.
[01:08:47.000 --> 01:08:52.280]   I'm not exactly getting Facebook rich or Google rich, but I'm doing okay.
[01:08:52.280 --> 01:08:53.280]   I'm doing okay.
[01:08:53.280 --> 01:08:55.280]   We're doing okay.
[01:08:55.280 --> 01:08:56.280]   Yeah.
[01:08:56.280 --> 01:08:57.280]   Our show today, Brian.
[01:08:57.280 --> 01:08:58.760]   I just teasing you.
[01:08:58.760 --> 01:08:59.760]   Speaking of which...
[01:08:59.760 --> 01:09:02.880]   Packet pushers network, ladies and gentlemen, PacketPushers.net.
[01:09:02.880 --> 01:09:04.920]   Actually, great place for Enterprise.
[01:09:04.920 --> 01:09:06.080]   Podcasts, fabulous.
[01:09:06.080 --> 01:09:08.080]   Is that the right address?
[01:09:08.080 --> 01:09:09.080]   You want to have a win?
[01:09:09.080 --> 01:09:10.080]   PacketPushers.net.
[01:09:10.080 --> 01:09:11.280]   PacketPushers.net is us.
[01:09:11.280 --> 01:09:15.480]   Yeah, we're publishing eight channels of podcast content now across a wide spectrum of
[01:09:15.480 --> 01:09:17.840]   Enterprise IT.
[01:09:17.840 --> 01:09:21.640]   There's me on my snarky channels, but there's other people who say really intelligent things
[01:09:21.640 --> 01:09:22.640]   on those channels.
[01:09:22.640 --> 01:09:26.320]   So if you're interested in Enterprise IT, you should tune into those.
[01:09:26.320 --> 01:09:28.120]   I want to call out one channel if I can.
[01:09:28.120 --> 01:09:29.120]   Yes.
[01:09:29.120 --> 01:09:31.160]   Leo, which is our IPv6 buzz channel.
[01:09:31.160 --> 01:09:35.280]   For those people who are doing networking, IPv6 is the new Nirvana.
[01:09:35.280 --> 01:09:36.760]   It's the innovation.
[01:09:36.760 --> 01:09:39.240]   And all they talk about is IPv6 of all things.
[01:09:39.240 --> 01:09:40.240]   Good, cool.
[01:09:40.240 --> 01:09:41.240]   Amazing.
[01:09:41.240 --> 01:09:44.720]   When you say channel, are they on 24/7 talking about it?
[01:09:44.720 --> 01:09:50.200]   They publish twice every two weeks a show on just IPv6 technology and how to deploy it.
[01:09:50.200 --> 01:09:51.200]   My God.
[01:09:51.200 --> 01:09:52.200]   PacketPushers.net.
[01:09:52.200 --> 01:09:53.200]   That's pretty deep.
[01:09:53.200 --> 01:09:55.320]   Also, Dan Patterson from Tech Republic.
[01:09:55.320 --> 01:09:56.520]   He's a senior writer there.
[01:09:56.520 --> 01:10:00.880]   And do tune in to cbsnews.com/electionhacking.
[01:10:00.880 --> 01:10:03.120]   Is that the right URL, Dan?
[01:10:03.120 --> 01:10:04.120]   It is.
[01:10:04.120 --> 01:10:06.000]   I'm actually at CBS News and seeing it these days.
[01:10:06.000 --> 01:10:09.160]   But I dearly love our friends at Tech Republic.
[01:10:09.160 --> 01:10:11.520]   Oh, you've moved.
[01:10:11.520 --> 01:10:12.600]   Oh, I'm so confused.
[01:10:12.600 --> 01:10:14.520]   It's all owned by the same company.
[01:10:14.520 --> 01:10:15.520]   It's all owned.
[01:10:15.520 --> 01:10:17.640]   Look, my everyday is CBS News.
[01:10:17.640 --> 01:10:19.160]   Everything I write is for CBS News.
[01:10:19.160 --> 01:10:20.640]   My chiron says, "See net."
[01:10:20.640 --> 01:10:23.360]   And I also love my colleagues at CNET.
[01:10:23.360 --> 01:10:24.360]   But my--
[01:10:24.360 --> 01:10:25.360]   Your CBS News now.
[01:10:25.360 --> 01:10:26.360]   Your CBS News now.
[01:10:26.360 --> 01:10:27.360]   Good.
[01:10:27.360 --> 01:10:28.360]   Good.
[01:10:28.360 --> 01:10:29.360]   Good.
[01:10:29.360 --> 01:10:30.360]   CBSNews.com/live.
[01:10:30.360 --> 01:10:39.000]   And from Electric and 9 to 5 Mac, the publisher and legend in the business, Seth Weintraub.
[01:10:39.000 --> 01:10:40.800]   Great to have you, Seth.
[01:10:40.800 --> 01:10:41.800]   Thanks, Leo.
[01:10:41.800 --> 01:10:43.800]   Anything you want to plug?
[01:10:43.800 --> 01:10:45.960]   Yes, you know the sites.
[01:10:45.960 --> 01:10:49.080]   If you like electric cars and Lime Scooters, electric--
[01:10:49.080 --> 01:10:50.600]   I mean, electric.
[01:10:50.600 --> 01:10:52.600]   Should I buy a Model 3 or not?
[01:10:52.600 --> 01:10:53.920]   Yes, you should.
[01:10:53.920 --> 01:10:55.400]   Everybody loves it.
[01:10:55.400 --> 01:10:56.400]   Yeah.
[01:10:56.400 --> 01:11:01.600]   It's a nice-- I mean, there's every issue in the world that-- but the car is great.
[01:11:01.600 --> 01:11:04.000]   Yeah, I'm already a Model X owner.
[01:11:04.000 --> 01:11:05.000]   I know.
[01:11:05.000 --> 01:11:06.000]   Or is it a Model 10?
[01:11:06.000 --> 01:11:07.000]   I can never tell.
[01:11:07.000 --> 01:11:09.800]   I know what it means to own a Tesla.
[01:11:09.800 --> 01:11:11.600]   They're great cars.
[01:11:11.600 --> 01:11:15.840]   But every time I get into an internal combustion engine car these days, I'm always like, "Man,
[01:11:15.840 --> 01:11:16.840]   this isn't just silly."
[01:11:16.840 --> 01:11:18.560]   I won't drive a nice car anymore.
[01:11:18.560 --> 01:11:19.560]   That's it's out for me.
[01:11:19.560 --> 01:11:22.320]   I'm trying to get my wife out of them.
[01:11:22.320 --> 01:11:24.640]   Anyway, great to have all three of you.
[01:11:24.640 --> 01:11:28.000]   Our show today-- yes, this is where I monetize.
[01:11:28.000 --> 01:11:31.120]   Actually, we have great sponsors.
[01:11:31.120 --> 01:11:34.560]   And I love each and every one of them.
[01:11:34.560 --> 01:11:37.920]   And one of the things we try to do with our sponsors is pick companies we use and work
[01:11:37.920 --> 01:11:38.920]   with.
[01:11:38.920 --> 01:11:39.920]   And in this case, is it ProKruder?
[01:11:39.920 --> 01:11:41.560]   Man, we had an amazing experience.
[01:11:41.560 --> 01:11:46.160]   So ZipperKruder is-- it's not a job site exactly.
[01:11:46.160 --> 01:11:51.080]   It's a one place you go to post your job listing and get it on 100 job sites on Twitter, on
[01:11:51.080 --> 01:11:52.400]   Facebook.
[01:11:52.400 --> 01:11:57.920]   But it does even more because what ZipperKruder does is it actually brings you candidates.
[01:11:57.920 --> 01:11:58.920]   That's smart.
[01:11:58.920 --> 01:12:02.040]   It's not-- go to do a job site and wait for a candidate to apply to your job.
[01:12:02.040 --> 01:12:03.040]   That's not smart.
[01:12:03.040 --> 01:12:06.080]   Go to zipperkruder.com/twit.
[01:12:06.080 --> 01:12:10.560]   And the right person will get in touch because ZipperKruder doesn't depend on the candidates
[01:12:10.560 --> 01:12:11.560]   finding you.
[01:12:11.560 --> 01:12:12.560]   It finds them for you.
[01:12:12.560 --> 01:12:15.280]   It uses powerful matching technology.
[01:12:15.280 --> 01:12:19.400]   Once you put your listing up there, it scans thousands of resumes, identifies people who
[01:12:19.400 --> 01:12:23.800]   have the skills, the education, the experience for your job, and then says to them, "Hey,
[01:12:23.800 --> 01:12:24.800]   I got a great job for you."
[01:12:24.800 --> 01:12:27.920]   Actively invites them to apply so you get qualified candidates fast.
[01:12:27.920 --> 01:12:28.920]   And we know this.
[01:12:28.920 --> 01:12:34.080]   We-- about two months ago, our account manager, a bookkeeper, quit, gave us two weeks notice.
[01:12:34.080 --> 01:12:38.520]   Now, Lisa, my wife who runs the company, has a lot of experience.
[01:12:38.520 --> 01:12:40.080]   She's been a controller of big companies.
[01:12:40.080 --> 01:12:41.080]   She can do it.
[01:12:41.080 --> 01:12:44.080]   But she's going, "I have a million things to do.
[01:12:44.080 --> 01:12:45.080]   They're more important.
[01:12:45.080 --> 01:12:47.160]   I don't want to have to keep-- but I got to hire somebody.
[01:12:47.160 --> 01:12:51.600]   This is the problem is you're down a person and you've got to do the hiring.
[01:12:51.600 --> 01:12:54.600]   It's just twice as much work as she was really in the dumps.
[01:12:54.600 --> 01:12:55.600]   This is at breakfast.
[01:12:55.600 --> 01:12:58.760]   I'm saying, "Well, Lisa, ZipperKruder."
[01:12:58.760 --> 01:13:03.560]   She posted on ZipperKruder before lunch.
[01:13:03.560 --> 01:13:06.800]   She starts getting candidates and she's looking at each one and going, "Wow, this person's
[01:13:06.800 --> 01:13:07.800]   great.
[01:13:07.800 --> 01:13:09.640]   I'm bad-- fabulous."
[01:13:09.640 --> 01:13:12.360]   Before lunch, we had three or four applicants.
[01:13:12.360 --> 01:13:13.880]   That's how fast it was.
[01:13:13.880 --> 01:13:18.720]   On average, you're going to have at least one highly qualified applicant within the first
[01:13:18.720 --> 01:13:19.720]   24 hours.
[01:13:19.720 --> 01:13:21.200]   I'd say it's even faster than that.
[01:13:21.200 --> 01:13:24.640]   That's why ZipperKruder is number one with employers in the US.
[01:13:24.640 --> 01:13:25.640]   This is from TrustPilot.
[01:13:25.640 --> 01:13:27.040]   Over 1,000 reviews.
[01:13:27.040 --> 01:13:30.400]   Number one, it is a fantastic system.
[01:13:30.400 --> 01:13:33.040]   All the applicants go into the ZipperKruder interface.
[01:13:33.040 --> 01:13:37.120]   It's making it very easy to rank them, screen them and hire the right person fast.
[01:13:37.120 --> 01:13:38.880]   You can have screening questions.
[01:13:38.880 --> 01:13:43.080]   They free format all the resumes so it's easy to scan through them.
[01:13:43.080 --> 01:13:45.720]   It is a really great applicant.
[01:13:45.720 --> 01:13:49.800]   You see all the companies that use ZipperKruder, some of the biggest companies in the world,
[01:13:49.800 --> 01:13:51.800]   including Twit.
[01:13:51.800 --> 01:13:54.640]   Really, it made Lisa very happy.
[01:13:54.640 --> 01:13:56.720]   We got that person hired fast.
[01:13:56.720 --> 01:13:57.720]   You can do the same.
[01:13:57.720 --> 01:14:01.000]   Try it free at zipperkruder.com/twit.
[01:14:01.000 --> 01:14:03.800]   Yeah, free.
[01:14:03.800 --> 01:14:14.440]   ZipperKruder is the smartest way to hire.
[01:14:14.440 --> 01:14:18.400]   Let's see here so many things to talk about.
[01:14:18.400 --> 01:14:20.960]   Invitations went out to an Apple event.
[01:14:20.960 --> 01:14:24.520]   In your neck of the woods set, it's going to be in New York this time.
[01:14:24.520 --> 01:14:25.880]   It's very exciting.
[01:14:25.880 --> 01:14:31.600]   I don't think they've had a real New York hardware event since like Macworld exposed
[01:14:31.600 --> 01:14:32.680]   time type of stuff.
[01:14:32.680 --> 01:14:36.880]   When they did the updated iPads for education, they had that at a school in New York.
[01:14:36.880 --> 01:14:40.280]   This one's going to be in Brooklyn.
[01:14:40.280 --> 01:14:43.280]   It's going to be the Brooklyn School of Music.
[01:14:43.280 --> 01:14:48.240]   They sent out invitations to everybody but me.
[01:14:48.240 --> 01:14:51.960]   Oh, we're now on the Poopy list.
[01:14:51.960 --> 01:14:52.960]   Are you on the Poopy list?
[01:14:52.960 --> 01:14:53.960]   9 to 5 Mac?
[01:14:53.960 --> 01:14:55.200]   Are you kidding?
[01:14:55.200 --> 01:15:01.040]   You know how we released the Apple Watch for and the iPhones?
[01:15:01.040 --> 01:15:03.640]   We told people about you at least.
[01:15:03.640 --> 01:15:07.160]   We had all of it.
[01:15:07.160 --> 01:15:12.440]   Our invitations didn't come for that event and then our invitations didn't come for
[01:15:12.440 --> 01:15:13.440]   this event.
[01:15:13.440 --> 01:15:14.440]   Never again.
[01:15:14.440 --> 01:15:15.840]   We're kind of on the bad boy list.
[01:15:15.840 --> 01:15:17.840]   I'm on that list.
[01:15:17.840 --> 01:15:20.200]   I got a good company.
[01:15:20.200 --> 01:15:22.200]   I don't really care.
[01:15:22.200 --> 01:15:25.880]   Although this time they did something really interesting, they made different Apple logos
[01:15:25.880 --> 01:15:29.360]   like dozens of them for these invitations.
[01:15:29.360 --> 01:15:34.720]   I mean, I don't know how many, but there are so many of them.
[01:15:34.720 --> 01:15:39.920]   The event invitations all say the same thing, which is, you know, I don't know, come help
[01:15:39.920 --> 01:15:40.920]   us spend customers.
[01:15:40.920 --> 01:15:43.000]   Oh, I like that one.
[01:15:43.000 --> 01:15:44.000]   That's iron filings.
[01:15:44.000 --> 01:15:45.000]   It looks like that's cool.
[01:15:45.000 --> 01:15:46.680]   Look at all of them.
[01:15:46.680 --> 01:15:47.680]   This is from the Verge.
[01:15:47.680 --> 01:15:51.480]   They collected as many as they could.
[01:15:51.480 --> 01:15:56.560]   It is clearly going to be it's October 30th, which is funny because OnePlus was going to
[01:15:56.560 --> 01:16:00.560]   have their event on October 30th announcing the new OnePlus 16.
[01:16:00.560 --> 01:16:01.560]   Whoops.
[01:16:01.560 --> 01:16:03.840]   So they moved it a day earlier.
[01:16:03.840 --> 01:16:05.200]   You don't really want to be at the same time.
[01:16:05.200 --> 01:16:06.440]   It's going to be early in the morning.
[01:16:06.440 --> 01:16:10.720]   I'm going to come in here and do it for us on the West Coast, 7 AM Pacific.
[01:16:10.720 --> 01:16:15.000]   That's 10 AM Eastern October 30th.
[01:16:15.000 --> 01:16:21.320]   It's almost certain it'll be at least new iPads and iPad Pros, probably bezel lists with
[01:16:21.320 --> 01:16:23.720]   no home button.
[01:16:23.720 --> 01:16:25.840]   You guys probably have all you probably have seen them.
[01:16:25.840 --> 01:16:26.840]   You've held them.
[01:16:26.840 --> 01:16:28.840]   You know all about them, right?
[01:16:28.840 --> 01:16:32.240]   We haven't touched them, but we've heard quite a bit about them.
[01:16:32.240 --> 01:16:34.720]   And they're going to have the face ID as well.
[01:16:34.720 --> 01:16:38.680]   And what's kind of cool, and I think you'll appreciate this, Leo, is I think they're going
[01:16:38.680 --> 01:16:39.680]   to have USB-C.
[01:16:39.680 --> 01:16:41.480]   That's a real shock.
[01:16:41.480 --> 01:16:43.440]   I've seen that rumor.
[01:16:43.440 --> 01:16:45.560]   They're going to abandon the lightning port.
[01:16:45.560 --> 01:16:48.960]   I don't know if they're going to abandon it on this particular device.
[01:16:48.960 --> 01:16:50.640]   I think they're going to have both.
[01:16:50.640 --> 01:16:51.640]   Both.
[01:16:51.640 --> 01:16:54.840]   Which is super interesting for Apple, right?
[01:16:54.840 --> 01:16:55.800]   Yeah.
[01:16:55.800 --> 01:16:57.400]   I think they have to.
[01:16:57.400 --> 01:17:02.040]   I think you're right, Seth, because logically everybody's got lightning connectors and lightning
[01:17:02.040 --> 01:17:07.080]   accessories, you know, lightning to SD cards and lightning headphones and all sorts of
[01:17:07.080 --> 01:17:08.080]   things.
[01:17:08.080 --> 01:17:11.960]   If they suddenly switched over to USB, and keep in mind that a lot of those USB connectors
[01:17:11.960 --> 01:17:17.480]   that you need to plug into all the accessories that you've got is actually a big deal.
[01:17:17.480 --> 01:17:20.120]   I do think that that might be possible.
[01:17:20.120 --> 01:17:26.600]   What's like, what's really- Which most phones now are Type-C, except for Apple.
[01:17:26.600 --> 01:17:31.640]   And there is a European directive to drive them, to force them to use USB-C.
[01:17:31.640 --> 01:17:37.240]   What's so interesting is not just the money and time that Apple spent converting their
[01:17:37.240 --> 01:17:45.440]   audience from their consumers, from the 30-pin to the new connectors, but the- by all accounts
[01:17:45.440 --> 01:17:51.160]   they were pushing for wireless to be the replacement, we're not going to USB-C.
[01:17:51.160 --> 01:17:52.920]   Instead, we're going to wireless.
[01:17:52.920 --> 01:17:53.920]   No kidding.
[01:17:53.920 --> 01:17:58.240]   And I mean, this- That would be courage.
[01:17:58.240 --> 01:18:02.520]   That would be courage.
[01:18:02.520 --> 01:18:06.880]   But to abandon that, and it appears as though the charging pads are nowhere to be seen.
[01:18:06.880 --> 01:18:07.880]   Yeah, that was a flop.
[01:18:07.880 --> 01:18:14.000]   And to abandon that for USB-C is some sort of corporate mechanism happened behind the
[01:18:14.000 --> 01:18:20.080]   scene where they abandoned in one direction and are now pushing in this interesting USB-C
[01:18:20.080 --> 01:18:21.080]   direction.
[01:18:21.080 --> 01:18:22.080]   Wow.
[01:18:22.080 --> 01:18:29.160]   So I had heard the rumor they would do USB-C, but I did not know that people were thinking
[01:18:29.160 --> 01:18:32.160]   both lightning and USB-C on one device.
[01:18:32.160 --> 01:18:33.160]   That's interesting.
[01:18:33.160 --> 01:18:40.000]   Hey, and we're hearing for USB-C that we're- they're going to have like an export or sorry,
[01:18:40.000 --> 01:18:43.880]   like a mirrored display out via USB-C.
[01:18:43.880 --> 01:18:50.880]   So you can put up a 4K display and then theoretically you have a USB or sorry, a Bluetooth keyboard
[01:18:50.880 --> 01:18:53.360]   or whatever kind of keyboard and mouse.
[01:18:53.360 --> 01:18:57.000]   And if there's a mouse, then all of a sudden you have a cursor on the iPad and all these
[01:18:57.000 --> 01:19:02.800]   like computer-y things start happening and then all of a sudden Apple's not making Macs
[01:19:02.800 --> 01:19:03.800]   anymore.
[01:19:03.800 --> 01:19:04.800]   They're making iPads.
[01:19:04.800 --> 01:19:09.840]   Well, and I think that we've been thinking that that was the tenor of the- So this is
[01:19:09.840 --> 01:19:12.040]   the next step in that.
[01:19:12.040 --> 01:19:13.520]   That is very interesting.
[01:19:13.520 --> 01:19:18.400]   Yeah, I mean, after all, we all saw the ad where the girl's running around with her iPad
[01:19:18.400 --> 01:19:22.040]   and finally her dad says, "Okay, time to put away the PC."
[01:19:22.040 --> 01:19:23.880]   And she says, "What's a PC?"
[01:19:23.880 --> 01:19:25.280]   What's a computer?
[01:19:25.280 --> 01:19:26.280]   Maybe less.
[01:19:26.280 --> 01:19:27.280]   What's a computer?
[01:19:27.280 --> 01:19:28.320]   Maybe less slugger.
[01:19:28.320 --> 01:19:30.800]   Because I like Macs.
[01:19:30.800 --> 01:19:33.120]   I don't want to lose Mac and Tush.
[01:19:33.120 --> 01:19:38.640]   The big stopping point, the one thing that Apple has to solve is you need a Mac to develop
[01:19:38.640 --> 01:19:40.320]   apps for the iOS.
[01:19:40.320 --> 01:19:41.320]   Yeah.
[01:19:41.320 --> 01:19:45.480]   And it's also kind of interesting that now you can get Microsoft Office for iPad.
[01:19:45.480 --> 01:19:48.840]   You can get now Adobe Photoshop coming for iPad.
[01:19:48.840 --> 01:19:49.840]   Isn't that interesting?
[01:19:49.840 --> 01:19:50.840]   Yeah.
[01:19:50.840 --> 01:19:56.000]   But you can't get Final Cut Pro and you can't get Xcode and you can't get any of their
[01:19:56.000 --> 01:20:01.840]   Pro apps for iPad, which is like, what is going on there?
[01:20:01.840 --> 01:20:06.360]   Well, do you think they'll maybe have some announcements on the 30th?
[01:20:06.360 --> 01:20:07.360]   It would be nice.
[01:20:07.360 --> 01:20:08.520]   I think a lot of people are looking forward to that.
[01:20:08.520 --> 01:20:09.520]   What about the Mac?
[01:20:09.520 --> 01:20:10.720]   Does that mean there?
[01:20:10.720 --> 01:20:16.960]   Because one thing that Mac, a Ficinado's like me, are holding out for is an updated MacBook
[01:20:16.960 --> 01:20:21.360]   or MacBook Air or a Mac Mini.
[01:20:21.360 --> 01:20:23.280]   Any chance we'll see those announcements?
[01:20:23.280 --> 01:20:27.120]   I think there's going to be a big Mac portion of this event.
[01:20:27.120 --> 01:20:29.920]   I don't know if there's going to be a Mini.
[01:20:29.920 --> 01:20:32.400]   Would it spend like five years or something crazy?
[01:20:32.400 --> 01:20:33.400]   That's ridiculous.
[01:20:33.400 --> 01:20:38.240]   It kind of feels like the headless Mac is long gone.
[01:20:38.240 --> 01:20:45.720]   But it would be nice to get a Mac Mini and Apple has promised a Mac Pro next year.
[01:20:45.720 --> 01:20:53.480]   But those new Intel chips, those new workstation 265 watt workstation chips, the New Zions would
[01:20:53.480 --> 01:20:55.600]   be perfect for a Mac Pro.
[01:20:55.600 --> 01:21:00.640]   Especially if you want to heat up your 265 watts.
[01:21:00.640 --> 01:21:02.640]   Cheese Louise.
[01:21:02.640 --> 01:21:12.280]   I'm hanging on to my mid 2012 MacBook Pro, 15 inch.
[01:21:12.280 --> 01:21:17.640]   I think there's something coming on October 30 because I'll be all up in the grill of
[01:21:17.640 --> 01:21:18.640]   that.
[01:21:18.640 --> 01:21:20.480]   Six years as long as I can make that laptop last.
[01:21:20.480 --> 01:21:23.760]   They put the touch bar on it though.
[01:21:23.760 --> 01:21:26.360]   The Mac Mini is very important for the education market.
[01:21:26.360 --> 01:21:29.600]   Apple's losing a lot of share to the Chromebooks in particular.
[01:21:29.600 --> 01:21:35.600]   Is that because of price or because it's compact, it's headless as Seth said.
[01:21:35.600 --> 01:21:36.600]   Why Mac Mini?
[01:21:36.600 --> 01:21:42.600]   The advantage of the Chromebook in the education, I spoke to a number of IT CIOs of schools
[01:21:42.600 --> 01:21:44.800]   a couple of years ago about it.
[01:21:44.800 --> 01:21:47.920]   They said, first of all, Chromebooks are cheap.
[01:21:47.920 --> 01:21:49.800]   They can buy them for two to three hundred pounds.
[01:21:49.800 --> 01:21:52.000]   Apple has nothing in that price range.
[01:21:52.000 --> 01:21:57.160]   But in terms of maintaining them, they blow the pants off iPads because of Chromebook,
[01:21:57.160 --> 01:22:01.960]   you just side it up and then the person logs in and then everything comes from Google.
[01:22:01.960 --> 01:22:06.200]   If you've got a kid, they file in, they pick a Chromebook up off the thing and then they
[01:22:06.200 --> 01:22:08.120]   sit down at the desk and away they go.
[01:22:08.120 --> 01:22:11.760]   If that Chromebook flames out, they just put it on the pile to be repaired and pick up
[01:22:11.760 --> 01:22:14.080]   another one and log in and away they go.
[01:22:14.080 --> 01:22:18.160]   Apple is very much losing ground with its software here compared to Google.
[01:22:18.160 --> 01:22:24.440]   Google's winning the hearts and minds of schools very, very much so because you can
[01:22:24.440 --> 01:22:28.720]   get a classroom of laptops for under $10,000 basically.
[01:22:28.720 --> 01:22:29.720]   All right.
[01:22:29.720 --> 01:22:34.160]   Here's an interesting piece by Dave Smith in Business Insider.
[01:22:34.160 --> 01:22:37.000]   It's time for tech giants to retire the keynote.
[01:22:37.000 --> 01:22:41.000]   Apple, Google and others are all making the same mistake with their product launches.
[01:22:41.000 --> 01:22:43.160]   Dave says they're too long.
[01:22:43.160 --> 01:22:44.920]   They're not very efficient.
[01:22:44.920 --> 01:22:48.360]   It would make so much more sense to do these online.
[01:22:48.360 --> 01:22:52.360]   And because tech companies can't keep a secret and we saw this with Google's event, there
[01:22:52.360 --> 01:22:53.360]   was nothing.
[01:22:53.360 --> 01:22:56.440]   Nothing for Google to say that we didn't already know.
[01:22:56.440 --> 01:22:59.960]   I suspect that may be true for Apple's event on the 30th.
[01:22:59.960 --> 01:23:03.480]   He says keynote should evolve into online debuts.
[01:23:03.480 --> 01:23:05.240]   You agree, Dan Patterson?
[01:23:05.240 --> 01:23:06.240]   Yeah.
[01:23:06.240 --> 01:23:13.440]   I think that particularly given the challenges experienced by nearly every major technology
[01:23:13.440 --> 01:23:21.680]   firm in the last 18 months or so, look, there was excitement in a more naive era but we've
[01:23:21.680 --> 01:23:24.840]   obviously transitioned into a different time.
[01:23:24.840 --> 01:23:36.360]   And these big boastful, hubristic events communicate a hubrist that is hard for many
[01:23:36.360 --> 01:23:38.360]   people to relate to.
[01:23:38.360 --> 01:23:45.200]   And we need a dose of humility right now from technology firms that have invaded our lives
[01:23:45.200 --> 01:23:55.520]   with products and data that has arguably made life less fun and showing off, hey, look
[01:23:55.520 --> 01:24:01.640]   at our big fancy products, especially when we're long overdue for a recession, is maybe
[01:24:01.640 --> 01:24:02.640]   a misstep.
[01:24:02.640 --> 01:24:04.520]   I might be wrong about that.
[01:24:04.520 --> 01:24:08.000]   And I certainly love getting excited about technology products.
[01:24:08.000 --> 01:24:13.640]   And I kind of long for the days where these were few and far between.
[01:24:13.640 --> 01:24:21.880]   But I think these companies run the risk of appearing too big and alienating their consumers
[01:24:21.880 --> 01:24:24.760]   or seeming out of touch from their consumers.
[01:24:24.760 --> 01:24:27.760]   What do you think, Seth?
[01:24:27.760 --> 01:24:31.720]   So the purpose of a keynote has kind of evolved a little bit.
[01:24:31.720 --> 01:24:39.800]   And Steve Jobs' Apple keynote was kind of like the, I guess, the one to model yourself
[01:24:39.800 --> 01:24:40.800]   after.
[01:24:40.800 --> 01:24:46.880]   To actually learn quite a bit about the product, if you remember the iPhone keynote, he explained
[01:24:46.880 --> 01:24:49.000]   like how the scrolling worked.
[01:24:49.000 --> 01:24:52.160]   Because this was a new UX language.
[01:24:52.160 --> 01:24:54.640]   Nobody had seen anything like this before.
[01:24:54.640 --> 01:25:00.960]   And after show, you would kind of learn how the new stuff worked.
[01:25:00.960 --> 01:25:04.800]   And it was kind of like a big, like, this is it.
[01:25:04.800 --> 01:25:08.160]   This is what you're going to be learning for the next six months.
[01:25:08.160 --> 01:25:12.920]   It was also very valuable because Apple could position stuff.
[01:25:12.920 --> 01:25:15.600]   So not only would they say, this is what it does and how it does it, but they could,
[01:25:15.600 --> 01:25:18.160]   you know, it's a iPhone.
[01:25:18.160 --> 01:25:19.160]   It's a phone.
[01:25:19.160 --> 01:25:20.960]   It's an internet communicator.
[01:25:20.960 --> 01:25:21.960]   It's an iPod.
[01:25:21.960 --> 01:25:22.960]   It's an iPod.
[01:25:22.960 --> 01:25:23.960]   It's a communicator phone.
[01:25:23.960 --> 01:25:29.160]   He could really set the language for what these devices were.
[01:25:29.160 --> 01:25:33.480]   And you know, I don't know if in particular with Apple, I don't know if it's that their
[01:25:33.480 --> 01:25:39.720]   products aren't as, you know, motivational or like, you know, I don't know if.
[01:25:39.720 --> 01:25:40.960]   >> Awful is a good word.
[01:25:40.960 --> 01:25:41.960]   Yeah.
[01:25:41.960 --> 01:25:42.960]   >> Yeah.
[01:25:42.960 --> 01:25:45.160]   I mean, you know, the new iPhones, they're just like the iPhone 10.
[01:25:45.160 --> 01:25:46.160]   >> They're kind of like the old iPhone.
[01:25:46.160 --> 01:25:47.160]   >> Yeah.
[01:25:47.160 --> 01:25:48.640]   And the camera's a little better.
[01:25:48.640 --> 01:25:51.640]   And you know, the watch screen is a little bit bigger.
[01:25:51.640 --> 01:25:54.440]   And, you know, the keynote's over.
[01:25:54.440 --> 01:25:55.440]   Go home.
[01:25:55.440 --> 01:26:00.640]   >> I do agree with you, Dan, that I don't see in the tech community, we're all excited
[01:26:00.640 --> 01:26:01.640]   about these.
[01:26:01.640 --> 01:26:03.440]   And boy, do these companies get a lot of coverage.
[01:26:03.440 --> 01:26:06.040]   We actually stream them and comment on them.
[01:26:06.040 --> 01:26:08.760]   And it's a great way to get press.
[01:26:08.760 --> 01:26:11.320]   That's why OnePlus is moving its event, right?
[01:26:11.320 --> 01:26:15.040]   There's a lot of attention on these, especially on Apple events.
[01:26:15.040 --> 01:26:17.120]   Even if there's nothing novel to announce.
[01:26:17.120 --> 01:26:21.440]   But I do think you made an interesting point, Dan, that maybe the general public is going
[01:26:21.440 --> 01:26:23.880]   to start looking at scans at these.
[01:26:23.880 --> 01:26:28.720]   >> Well, I think the general public has already started doing that.
[01:26:28.720 --> 01:26:36.360]   And it can appear tone deaf to have a bunch of incredibly affluent people stand up on
[01:26:36.360 --> 01:26:40.080]   stage and present devices that are well out of your budget.
[01:26:40.080 --> 01:26:44.200]   And say, look at this fantastic thing.
[01:26:44.200 --> 01:26:47.960]   We might change your life, but it's probably going to be the same as the thing already
[01:26:47.960 --> 01:26:52.120]   in your pocket, but cost you more money.
[01:26:52.120 --> 01:26:56.440]   This is not me poo-pooing these events, but I think that we are simply in a different
[01:26:56.440 --> 01:26:57.600]   era.
[01:26:57.600 --> 01:27:01.520]   We don't need to have this new language explained to us anymore.
[01:27:01.520 --> 01:27:08.560]   We all understand this intuitively how to use our devices and what they do and the benefits
[01:27:08.560 --> 01:27:09.720]   that they can bring to our lives.
[01:27:09.720 --> 01:27:10.720]   >> 10 minutes.
[01:27:10.720 --> 01:27:14.480]   >> We're voting 10 minutes to me mojis, right?
[01:27:14.480 --> 01:27:15.480]   >> Yeah.
[01:27:15.480 --> 01:27:19.240]   >> But those, I think there's a difference here between what Apple does.
[01:27:19.240 --> 01:27:21.360]   And I think Apple gets it more or less right.
[01:27:21.360 --> 01:27:23.440]   They save four keynotes.
[01:27:23.440 --> 01:27:25.200]   They bring very visual things.
[01:27:25.200 --> 01:27:28.040]   And they tell stories and they do it reasonably well.
[01:27:28.040 --> 01:27:30.640]   But they also don't hold a key note for everything.
[01:27:30.640 --> 01:27:35.120]   It's not like sometimes they'll introduce minor refreshes of an iPad, but they don't
[01:27:35.120 --> 01:27:38.840]   hold a key note for it and make a big press event out of that.
[01:27:38.840 --> 01:27:41.120]   They generally have something to say.
[01:27:41.120 --> 01:27:46.080]   The challenge here is when Google stands up and announces the Pixel 3, which nobody buys
[01:27:46.080 --> 01:27:51.040]   and nobody uses and nobody should reasonably care about, but somehow the press is glommed
[01:27:51.040 --> 01:27:55.360]   around it like flies around a can of poop and treated it like it's the next nirvana
[01:27:55.360 --> 01:27:57.480]   and is the alternative to the iPhone.
[01:27:57.480 --> 01:28:03.120]   But the Pixel 3 is literally going to sell as many phones in a year as many iPhone.
[01:28:03.120 --> 01:28:08.400]   It'll sell in a year the number of phones that Apple sells in five days.
[01:28:08.400 --> 01:28:10.520]   Nobody should be commenting about the Pixel 3.
[01:28:10.520 --> 01:28:14.840]   The challenge here is that the media industry is chasing after the wrong thing.
[01:28:14.840 --> 01:28:16.400]   They see Apple doing a media event.
[01:28:16.400 --> 01:28:17.400]   They give it the coverage.
[01:28:17.400 --> 01:28:21.000]   So when somebody else does a media event, they give it the same level of coverage without
[01:28:21.000 --> 01:28:22.000]   qualifying it.
[01:28:22.000 --> 01:28:25.880]   They should be backing off from it and saying, "Google's going to announce a phone that
[01:28:25.880 --> 01:28:26.880]   nobody buys.
[01:28:26.880 --> 01:28:27.880]   I should not be covering that.
[01:28:27.880 --> 01:28:28.880]   It's not a big story."
[01:28:28.880 --> 01:28:34.720]   I have a data point along that line because I used to do every Friday morning I would
[01:28:34.720 --> 01:28:41.040]   talk to the big morning show in Los Angeles on KFI about stuff.
[01:28:41.040 --> 01:28:45.520]   And I was just told, "Let's stop talking about product releases.
[01:28:45.520 --> 01:28:46.960]   We want to talk about philosophy.
[01:28:46.960 --> 01:28:50.440]   We want to talk about spiggers stories."
[01:28:50.440 --> 01:28:55.960]   In fact, I remember I never wanted to cover cell phones and gadgets, but I felt forced
[01:28:55.960 --> 01:28:59.880]   into it by the huge interest in demand for that.
[01:28:59.880 --> 01:29:03.800]   Well, I think it's now shifting and it's gone away again.
[01:29:03.800 --> 01:29:04.800]   I think that's very interesting point.
[01:29:04.800 --> 01:29:09.520]   People are seeing the end of the product releases because it's become tedious, but it's also
[01:29:09.520 --> 01:29:10.520]   cheap journalism.
[01:29:10.520 --> 01:29:12.240]   Get a product, hold it up.
[01:29:12.240 --> 01:29:13.240]   What's the color of it?
[01:29:13.240 --> 01:29:14.240]   How good's the camera?
[01:29:14.240 --> 01:29:15.240]   I love a vlog.
[01:29:15.240 --> 01:29:19.800]   I'll do it only if people want it.
[01:29:19.800 --> 01:29:20.800]   But I think so.
[01:29:20.800 --> 01:29:21.800]   We've never had a photo.
[01:29:21.800 --> 01:29:22.800]   I think so.
[01:29:22.800 --> 01:29:23.800]   We've never had a photo.
[01:29:23.800 --> 01:29:24.800]   That's our bread and butter.
[01:29:24.800 --> 01:29:25.800]   This guy's hitting here.
[01:29:25.800 --> 01:29:31.240]   Yeah, but I mean, when you look at a phone anymore, this is a pixel.
[01:29:31.240 --> 01:29:32.920]   Some people do buy these things.
[01:29:32.920 --> 01:29:33.920]   It's just a screen.
[01:29:33.920 --> 01:29:35.640]   That's all it is anymore.
[01:29:35.640 --> 01:29:38.160]   Now it's got a notch.
[01:29:38.160 --> 01:29:44.080]   Yesterday on the screen savers, I put the Pixel 3 next to the iPhone X, XS, and next
[01:29:44.080 --> 01:29:46.040]   to the Note 9.
[01:29:46.040 --> 01:29:50.040]   And unless you knew what you were looking at, they were completely indistinguishable.
[01:29:50.040 --> 01:29:53.520]   They're all roughly the same six inch slabs of glass.
[01:29:53.520 --> 01:29:54.520]   Right.
[01:29:54.520 --> 01:29:59.880]   But in our network, we've never covered new product releases unless there's a generational
[01:29:59.880 --> 01:30:00.880]   change.
[01:30:00.880 --> 01:30:07.440]   So, just because Cisco, Juniper, extreme releases are 32 port switch with 100 gig interfaces.
[01:30:07.440 --> 01:30:11.160]   Yeah, but that's because you're covering an incredibly boring, mature industry.
[01:30:11.160 --> 01:30:12.160]   Yeah.
[01:30:12.160 --> 01:30:14.160]   But the product releases are boring.
[01:30:14.160 --> 01:30:15.160]   Right?
[01:30:15.160 --> 01:30:16.160]   Well, maybe not.
[01:30:16.160 --> 01:30:18.480]   I mean, for a while, they were pretty exciting.
[01:30:18.480 --> 01:30:21.440]   In fact, we've got a new iPhone coming out on Friday.
[01:30:21.440 --> 01:30:25.080]   The 10R, are your people Seth all excited about the 10R?
[01:30:25.080 --> 01:30:27.720]   Yeah, we're getting quite a bit of buzz about that.
[01:30:27.720 --> 01:30:32.040]   I feel like that's going to be the phone that sells, although, and I don't know if this
[01:30:32.040 --> 01:30:34.320]   is useful or not.
[01:30:34.320 --> 01:30:38.720]   But if you go to order a 10R, you can easily still get it on Friday.
[01:30:38.720 --> 01:30:40.680]   Yeah, unless you're on T-Mobile.
[01:30:40.680 --> 01:30:44.640]   That T-Mobile, I think, had a little bit of shortage, but everybody else is pretty solid.
[01:30:44.640 --> 01:30:49.360]   I went in to get the T-Mobile version the day after, and I couldn't get it fast enough.
[01:30:49.360 --> 01:30:53.720]   But the chatroom said, "I'll just get the ATD for ice," and when they're all unlocked,
[01:30:53.720 --> 01:30:56.280]   you just get whatever one you want.
[01:30:56.280 --> 01:30:57.280]   Yeah.
[01:30:57.280 --> 01:30:58.280]   Yeah.
[01:30:58.280 --> 01:31:01.880]   So, I haven't checked today, but there was no line.
[01:31:01.880 --> 01:31:03.320]   Let's put it that way.
[01:31:03.320 --> 01:31:06.920]   Yeah, I mean, they're able to build quite a few of these.
[01:31:06.920 --> 01:31:08.720]   So, maybe that's--
[01:31:08.720 --> 01:31:10.720]   Maybe that's a million sold.
[01:31:10.720 --> 01:31:11.720]   Yeah.
[01:31:11.720 --> 01:31:12.720]   Yeah.
[01:31:12.720 --> 01:31:16.240]   And they've had plenty of time to ramp this up since the announcement two months ago.
[01:31:16.240 --> 01:31:17.240]   Right.
[01:31:17.240 --> 01:31:20.200]   The only one that's available in the UK is the 64 gig.
[01:31:20.200 --> 01:31:25.360]   I can order the 128 and the 256 and it'll deliver on Friday.
[01:31:25.360 --> 01:31:28.760]   Can you get the banana phone?
[01:31:28.760 --> 01:31:29.760]   The yellow one?
[01:31:29.760 --> 01:31:31.760]   Let's have a look.
[01:31:31.760 --> 01:31:32.760]   I'll check for you.
[01:31:32.760 --> 01:31:37.440]   Check stock on the banana phone, because I think that's the one-- apparently that's the
[01:31:37.440 --> 01:31:39.040]   one everybody wants is the yellow one.
[01:31:39.040 --> 01:31:44.200]   So, two-week delay on the 64 gig and 128, but you can have the 256 gig.
[01:31:44.200 --> 01:31:48.320]   I want to ask you guys, because we talk about this a little bit.
[01:31:48.320 --> 01:31:54.720]   Do you notice that there's a big bezel around the edge of the XR or the 10R?
[01:31:54.720 --> 01:31:58.880]   That's what people are complaining about, that compared to the 10S, the 10R.
[01:31:58.880 --> 01:32:02.480]   Yeah, because it's an LCD display, so they can't go quite to the edge.
[01:32:02.480 --> 01:32:07.680]   But it almost feels like there's a big bezel all the way around.
[01:32:07.680 --> 01:32:08.680]   Oh, isn't that funny?
[01:32:08.680 --> 01:32:10.560]   They're kind of cheating it a little bit.
[01:32:10.560 --> 01:32:11.560]   Oh, isn't that funny?
[01:32:11.560 --> 01:32:13.320]   There's still a notch.
[01:32:13.320 --> 01:32:14.400]   But there's a bigger bezel.
[01:32:14.400 --> 01:32:15.400]   There's a bigger bezel.
[01:32:15.400 --> 01:32:18.040]   Quite a bit bigger, if you look at it.
[01:32:18.040 --> 01:32:19.040]   Now, here's the question.
[01:32:19.040 --> 01:32:20.920]   And people care about that?
[01:32:20.920 --> 01:32:23.920]   Well, they might, because this isn't that much cheaper.
[01:32:23.920 --> 01:32:26.680]   It's $750 for the 64 gig.
[01:32:26.680 --> 01:32:27.960]   It's a really expensive phone.
[01:32:27.960 --> 01:32:28.960]   It's pretty expensive.
[01:32:28.960 --> 01:32:29.960]   That was flagship.
[01:32:29.960 --> 01:32:30.960]   Yeah.
[01:32:30.960 --> 01:32:34.200]   So, I mean, if you're getting it for $500, you might say, "Well, who cares?"
[01:32:34.200 --> 01:32:35.200]   Sure.
[01:32:35.200 --> 01:32:38.360]   Hmm, that's interesting.
[01:32:38.360 --> 01:32:43.000]   I mean, I look at iPhone and phone reviews these days and go like, "What are you people
[01:32:43.000 --> 01:32:44.640]   on about it just to learn from it?"
[01:32:44.640 --> 01:32:45.640]   What do you carry?
[01:32:45.640 --> 01:32:46.640]   I really don't get it anymore.
[01:32:46.640 --> 01:32:48.040]   What's your phone?
[01:32:48.040 --> 01:32:49.040]   You know?
[01:32:49.040 --> 01:32:50.040]   What's your phone, Greg?
[01:32:50.040 --> 01:32:51.040]   I don't know.
[01:32:51.040 --> 01:32:52.040]   You don't care.
[01:32:52.040 --> 01:32:54.240]   He doesn't know, he doesn't care.
[01:32:54.240 --> 01:32:57.000]   I think it's an iPhone 8 thing.
[01:32:57.000 --> 01:33:00.480]   It doesn't really matter, does it?
[01:33:00.480 --> 01:33:02.200]   No, couldn't give a toss.
[01:33:02.200 --> 01:33:08.320]   And by the way, if you take Facebook and Twitter and Instagram off your phone, there's
[01:33:08.320 --> 01:33:10.560]   really no reason to look at it at all.
[01:33:10.560 --> 01:33:13.600]   Oh, well, I don't take telephone calls on it either.
[01:33:13.600 --> 01:33:14.600]   No.
[01:33:14.600 --> 01:33:17.720]   The truth is, I no longer need my phone.
[01:33:17.720 --> 01:33:19.360]   Nobody calls me.
[01:33:19.360 --> 01:33:20.360]   Text messages.
[01:33:20.360 --> 01:33:21.360]   If I had a little...
[01:33:21.360 --> 01:33:23.960]   Somebody should make a little thing that you could hook on your belt, maybe this...
[01:33:23.960 --> 01:33:27.680]   I don't know, the size of an Altoids tin that just had a screen on it would tell you
[01:33:27.680 --> 01:33:28.680]   your text messages.
[01:33:28.680 --> 01:33:29.680]   That's all I need.
[01:33:29.680 --> 01:33:30.680]   Like the size of a watch?
[01:33:30.680 --> 01:33:34.080]   Yeah, somebody could invent something like that.
[01:33:34.080 --> 01:33:36.840]   I use my watch more than I use my iPhone these days.
[01:33:36.840 --> 01:33:37.840]   But you're watchability.
[01:33:37.840 --> 01:33:40.000]   You're watch, unlike mine, tells the time.
[01:33:40.000 --> 01:33:41.680]   Mine doesn't tell the time unless you shake it.
[01:33:41.680 --> 01:33:44.360]   You give it a good shake.
[01:33:44.360 --> 01:33:45.360]   Maybe not even then.
[01:33:45.360 --> 01:33:46.360]   I'm so depressed.
[01:33:46.360 --> 01:33:47.360]   I'm so depressed.
[01:33:47.360 --> 01:33:52.200]   You guys are making me so depressed, there's used to be, I can at least say, well, the
[01:33:52.200 --> 01:33:55.120]   world's going to hell, but at least we've got cool gadgets.
[01:33:55.120 --> 01:33:56.120]   Yeah.
[01:33:56.120 --> 01:33:57.360]   We do have cool gadgets.
[01:33:57.360 --> 01:33:58.360]   Do we?
[01:33:58.360 --> 01:33:59.360]   You could take photos.
[01:33:59.360 --> 01:34:00.360]   Thank you.
[01:34:00.360 --> 01:34:03.320]   You can take photos of it as it goes to hell.
[01:34:03.320 --> 01:34:04.720]   Isn't that gonna be great?
[01:34:04.720 --> 01:34:06.760]   Scooters and Teslas, that's where it's at.
[01:34:06.760 --> 01:34:08.680]   Scooters and Teslas, they're the future.
[01:34:08.680 --> 01:34:14.720]   All right, let's take a break and then we'll talk about Google's plan to screw the EU.
[01:34:14.720 --> 01:34:15.720]   Mm.
[01:34:15.720 --> 01:34:18.160]   I got something you'll be interested in.
[01:34:18.160 --> 01:34:26.280]   My friend, David Friend, who was the CEO of Carbonite and his partner at Carbonite,
[01:34:26.280 --> 01:34:30.280]   Jeff Flowers, are very clever guys.
[01:34:30.280 --> 01:34:34.080]   And while they're still on, I'm sure they're still on the board at Carbonite, they realize
[01:34:34.080 --> 01:34:41.320]   they had a little bit of free time and they had also some really amazing technology that
[01:34:41.320 --> 01:34:52.400]   had allowed them to, in the days of Carbonite, store stuff really fast, really efficiently
[01:34:52.400 --> 01:34:53.400]   on hard drives.
[01:34:53.400 --> 01:34:57.720]   It was really the core technology, the patented technology that made Carbonite happen.
[01:34:57.720 --> 01:35:02.920]   And they said, you know, we shouldn't let this technology just be for Carbonite.
[01:35:02.920 --> 01:35:10.760]   What if we made this hot cloud storage and wasabi was born wasabi hot cloud storage?
[01:35:10.760 --> 01:35:15.200]   And by the way, this is in business, as you know, is a huge issue right now.
[01:35:15.200 --> 01:35:20.360]   According to, I guess I do see one of the analysts, there will be an estimated, I can't
[01:35:20.360 --> 01:35:26.960]   believe this, 163 zettabytes of data storage by 2025.
[01:35:26.960 --> 01:35:27.960]   That's 21 zeros.
[01:35:27.960 --> 01:35:29.920]   That's a crazy amount of data.
[01:35:29.920 --> 01:35:30.920]   And we're storing it in the cloud.
[01:35:30.920 --> 01:35:34.640]   And we're storing it essentially at three big companies.
[01:35:34.640 --> 01:35:36.440]   But there's a better way with sobe.
[01:35:36.440 --> 01:35:41.600]   And if you're looking at hot cloud storage right now, I want you to add a fourth company
[01:35:41.600 --> 01:35:45.880]   to the list that you're looking at with sobe results and storage, thanks to this technology
[01:35:45.880 --> 01:35:50.360]   that's 80% cheaper and six times faster than S3.
[01:35:50.360 --> 01:35:54.520]   Plus they don't charge for egress, free unlimited egress.
[01:35:54.520 --> 01:35:57.520]   They just have one basic tier of service.
[01:35:57.520 --> 01:36:01.400]   It's completely secure effect, really, I think you can make the case that it's more secure
[01:36:01.400 --> 01:36:03.880]   than you're on prem storage.
[01:36:03.880 --> 01:36:10.960]   It's HIPAA compliant, FINRA compliant, CJIS compliant, 80% less than S3, six times faster
[01:36:10.960 --> 01:36:12.920]   with 11 nines of data durability.
[01:36:12.920 --> 01:36:17.800]   And to add to that, they've created a category of immutable storage storage.
[01:36:17.800 --> 01:36:20.880]   You say, I don't want this ever to change.
[01:36:20.880 --> 01:36:24.680]   It is protected against accidental or malicious deletions or modifications.
[01:36:24.680 --> 01:36:26.280]   It's immutable.
[01:36:26.280 --> 01:36:30.400]   They have something called the wasabi ball that allows you to quickly migrate petabytes
[01:36:30.400 --> 01:36:34.680]   of data directly to the wasabi cloud.
[01:36:34.680 --> 01:36:35.840]   It is hot.
[01:36:35.840 --> 01:36:39.880]   W-A-S-A-B-I wasabi, just like wasabi.
[01:36:39.880 --> 01:36:40.880]   But it's cloud storage.
[01:36:40.880 --> 01:36:43.560]   It's not hot radish.
[01:36:43.560 --> 01:36:46.960]   If you go to wasabi.com and click the free trial link and use the code Twit, you'll get
[01:36:46.960 --> 01:36:50.400]   a month of unlimited storage for free so you can test it.
[01:36:50.400 --> 01:36:53.400]   When you see the price and when you go to your boss and say, look, we're going to save
[01:36:53.400 --> 01:36:59.160]   80% and it's six times faster and it's 11 nines durable and it's got free egress, no
[01:36:59.160 --> 01:37:03.320]   charge for API calls, immutable storage.
[01:37:03.320 --> 01:37:05.480]   I think you're going to see it as a great choice.
[01:37:05.480 --> 01:37:09.720]   Oh, by the way, it's fully S3 API compliant.
[01:37:09.720 --> 01:37:13.080]   So if you've already got S3 stuff that you're using, don't worry.
[01:37:13.080 --> 01:37:15.000]   It'll all work.
[01:37:15.000 --> 01:37:16.000]   Wasabi.
[01:37:16.000 --> 01:37:19.080]   I really, I love these guys.
[01:37:19.080 --> 01:37:22.240]   David and Jeff are brilliant.
[01:37:22.240 --> 01:37:24.480]   And I think they've done it again.
[01:37:24.480 --> 01:37:27.180]   And that's why I'm really happy to tell you about wasabi.
[01:37:27.180 --> 01:37:28.180]   Try it right now.
[01:37:28.180 --> 01:37:30.200]   Put it on your list with sabi.com.
[01:37:30.200 --> 01:37:33.880]   Don't forget the offer code Twit so they know about where you heard it.
[01:37:33.880 --> 01:37:34.880]   Ah!
[01:37:34.880 --> 01:37:38.680]   This is a segment of the market that's absolutely white hot.
[01:37:38.680 --> 01:37:42.440]   There's over 70 storage startups in the enterprise IT.
[01:37:42.440 --> 01:37:43.440]   That's just startups.
[01:37:43.440 --> 01:37:49.800]   In addition to the 15 to 20 odd billion dollar plus established players in this market, it's
[01:37:49.800 --> 01:37:54.880]   just a vital, there's so many different ways to store your data these days.
[01:37:54.880 --> 01:37:55.880]   It's amazing.
[01:37:55.880 --> 01:38:01.400]   And I think you would agree that in most cases these are more secure than your on-prem
[01:38:01.400 --> 01:38:02.400]   storage.
[01:38:02.400 --> 01:38:07.080]   You know, most of the time people got a couple of drives in the closet.
[01:38:07.080 --> 01:38:09.000]   It's complicated.
[01:38:09.000 --> 01:38:11.160]   It can certainly be much better.
[01:38:11.160 --> 01:38:16.040]   The actual arrays are much more, the data storage is much more reliable and stable.
[01:38:16.040 --> 01:38:19.040]   The challenge here is that a lot of people put data in the cloud but then forget to back
[01:38:19.040 --> 01:38:20.040]   it up.
[01:38:20.040 --> 01:38:21.040]   Right.
[01:38:21.040 --> 01:38:24.600]   It's got, if you put it in the cloud, yes, it's safe but the challenge is...
[01:38:24.600 --> 01:38:26.280]   You've got to have more than one copy.
[01:38:26.280 --> 01:38:27.280]   You've got to have one with the one copy.
[01:38:27.280 --> 01:38:28.280]   Yeah.
[01:38:28.280 --> 01:38:31.200]   If somebody corrupts the copy, how do you get back to with the un-grap the copy?
[01:38:31.200 --> 01:38:35.280]   I always talk about this when we talk about backup because what people do is they label
[01:38:35.280 --> 01:38:38.040]   the drive backup, they copy the data to it.
[01:38:38.040 --> 01:38:39.040]   They say, "Oh, it's backed up."
[01:38:39.040 --> 01:38:40.040]   And then delete the original.
[01:38:40.040 --> 01:38:41.280]   It's like, "No, it's not backed up.
[01:38:41.280 --> 01:38:42.280]   It's not backed up."
[01:38:42.280 --> 01:38:47.280]   Just because it's on a drive called backup does not make it backed up.
[01:38:47.280 --> 01:38:48.280]   No.
[01:38:48.280 --> 01:38:49.280]   So Google is...
[01:38:49.280 --> 01:38:53.800]   I think this is one of those screw you moments.
[01:38:53.800 --> 01:38:56.000]   Remember that the EU fined Google what was it?
[01:38:56.000 --> 01:38:58.320]   Five billion dollars.
[01:38:58.320 --> 01:38:59.480]   And he trust fine.
[01:38:59.480 --> 01:39:03.100]   And one of the things they said is, one of the things Google did wrong was they were
[01:39:03.100 --> 01:39:07.960]   telling companies that wanted the Google Apps on their Android devices, "Okay, but you
[01:39:07.960 --> 01:39:09.760]   can't make non...
[01:39:09.760 --> 01:39:11.480]   You can't make AOSP devices.
[01:39:11.480 --> 01:39:14.000]   You have to make all Google devices."
[01:39:14.000 --> 01:39:19.840]   You have to include Chrome and Google Search.
[01:39:19.840 --> 01:39:21.920]   And you said, "No, you can't do that anymore."
[01:39:21.920 --> 01:39:23.520]   So Google said, "Okay."
[01:39:23.520 --> 01:39:25.280]   But that was how we made money.
[01:39:25.280 --> 01:39:32.440]   So now if OEMs in Europe want to put Google services on Android, we're going to charge
[01:39:32.440 --> 01:39:36.960]   them as much as $40 a phone.
[01:39:36.960 --> 01:39:37.960]   Screw you, Europe!
[01:39:37.960 --> 01:39:42.520]   So Microsoft did with an Internet Explorer when they unbundled it.
[01:39:42.520 --> 01:39:43.520]   Yeah.
[01:39:43.520 --> 01:39:44.520]   So you...
[01:39:44.520 --> 01:39:47.280]   I mean, $5 billion is not a bad taxation gain.
[01:39:47.280 --> 01:39:50.840]   So it's still cheaper than paying the tax they should have paid for the last 15 years.
[01:39:50.840 --> 01:39:52.280]   It is a significant increase.
[01:39:52.280 --> 01:39:55.200]   And if you're in the EU, you might notice these phones are going to cost a little bit
[01:39:55.200 --> 01:39:56.200]   more.
[01:39:56.200 --> 01:40:00.360]   I don't think anybody cares because over here people buy the phones outright.
[01:40:00.360 --> 01:40:03.320]   They don't rent them from their carriers.
[01:40:03.320 --> 01:40:09.320]   It's very... getting increasingly uncommon to actually buy them as part of a package like
[01:40:09.320 --> 01:40:10.320]   a pay per month.
[01:40:10.320 --> 01:40:11.320]   Right.
[01:40:11.320 --> 01:40:12.320]   That's the same here in the States.
[01:40:12.320 --> 01:40:14.640]   So they hide that in various ways, you know?
[01:40:14.640 --> 01:40:15.800]   Yeah, there's lots of ways.
[01:40:15.800 --> 01:40:16.800]   I think...
[01:40:16.800 --> 01:40:18.320]   If I'm Apple, I'm thrilled.
[01:40:18.320 --> 01:40:19.320]   I'm going great.
[01:40:19.320 --> 01:40:21.720]   Finally, we can get some market share in Europe.
[01:40:21.720 --> 01:40:22.720]   Mm-hmm.
[01:40:22.720 --> 01:40:28.160]   The good part about this is though it does give Android users the chance to put other
[01:40:28.160 --> 01:40:29.160]   apps on it.
[01:40:29.160 --> 01:40:33.280]   Microsoft will be pleased because they can now start developing apps for the Android phone
[01:40:33.280 --> 01:40:35.120]   and might be able to get them on there.
[01:40:35.120 --> 01:40:38.720]   The challenge here, of course, is that the Google Play Store is a trained splash.
[01:40:38.720 --> 01:40:39.720]   It's insecure.
[01:40:39.720 --> 01:40:40.720]   It's full of rubbish.
[01:40:40.720 --> 01:40:41.720]   It's hard to hate it.
[01:40:41.720 --> 01:40:42.720]   It's hard to hate it.
[01:40:42.720 --> 01:40:43.720]   I really do.
[01:40:43.720 --> 01:40:44.720]   Yeah.
[01:40:44.720 --> 01:40:45.720]   I tried.
[01:40:45.720 --> 01:40:46.720]   I'm with Seth.
[01:40:46.720 --> 01:40:47.720]   I really like my Pixel 3.
[01:40:47.720 --> 01:40:48.720]   I bought some end-time.
[01:40:48.720 --> 01:40:53.960]   The pictures that the images are fantastic.
[01:40:53.960 --> 01:40:55.440]   And have you tried the digital zoom?
[01:40:55.440 --> 01:40:56.440]   It's amazing.
[01:40:56.440 --> 01:40:57.440]   It really is good.
[01:40:57.440 --> 01:40:58.440]   All right.
[01:40:58.440 --> 01:41:01.800]   But Greg doesn't use his phone for anything, so he doesn't really need it.
[01:41:01.800 --> 01:41:03.960]   I mean, like, you know...
[01:41:03.960 --> 01:41:05.800]   Whatever.
[01:41:05.800 --> 01:41:09.080]   This new screen, it's the OLED screen.
[01:41:09.080 --> 01:41:10.080]   Beautiful.
[01:41:10.080 --> 01:41:11.080]   Yeah.
[01:41:11.080 --> 01:41:12.080]   And it's wireless charging.
[01:41:12.080 --> 01:41:13.080]   I love that.
[01:41:13.080 --> 01:41:14.080]   Finally, it's back.
[01:41:14.080 --> 01:41:15.080]   Right.
[01:41:15.080 --> 01:41:16.080]   Yeah.
[01:41:16.080 --> 01:41:19.640]   So there is actually a little bit of that EU settlement that kind of went under the radar
[01:41:19.640 --> 01:41:21.800]   a little bit, but it's actually a big deal.
[01:41:21.800 --> 01:41:27.760]   And that's that Google used to say to like Samsung or LG or whatever, if you want to
[01:41:27.760 --> 01:41:32.080]   use the Google Play Store, which you have to use for Google Maps and like a lot of important
[01:41:32.080 --> 01:41:38.520]   apps, you can't put anything else on your phones on any kind of phones across like your
[01:41:38.520 --> 01:41:39.520]   line of stuff.
[01:41:39.520 --> 01:41:40.520]   You can't.
[01:41:40.520 --> 01:41:41.520]   But Samsung does.
[01:41:41.520 --> 01:41:44.960]   They have the Samsung versions of many of the Google apps.
[01:41:44.960 --> 01:41:45.960]   You're not talking about that.
[01:41:45.960 --> 01:41:46.960]   No, they...
[01:41:46.960 --> 01:41:52.880]   I mean, Samsung and LG have their own stores, but there's always the Google stuff on every
[01:41:52.880 --> 01:41:54.520]   single Samsung device.
[01:41:54.520 --> 01:41:56.680]   You can't sell a non-Google phone.
[01:41:56.680 --> 01:41:57.680]   Right.
[01:41:57.680 --> 01:41:59.520]   I mean, an Android non-Google phone.
[01:41:59.520 --> 01:42:00.520]   Right.
[01:42:00.520 --> 01:42:06.680]   So now part of the ruling was that Google can't enforce that.
[01:42:06.680 --> 01:42:14.360]   So if somebody wants to put Play Store on the Galaxy S10s and they don't want to put them
[01:42:14.360 --> 01:42:20.040]   on the low cost ones or whatever, Samsung can do that now.
[01:42:20.040 --> 01:42:22.040]   So that actually kind of...
[01:42:22.040 --> 01:42:23.440]   Well, think about Amazon.
[01:42:23.440 --> 01:42:27.240]   Amazon sells Android stuff, but it's not Google Android stuff.
[01:42:27.240 --> 01:42:28.240]   It's de-googled.
[01:42:28.240 --> 01:42:34.720]   They made a phone that was nobody bought because it was de-googled.
[01:42:34.720 --> 01:42:36.720]   Now Android could...
[01:42:36.720 --> 01:42:40.440]   Well, actually not now because they're not in the EU, but in the EU anyway, a company
[01:42:40.440 --> 01:42:41.520]   could make both.
[01:42:41.520 --> 01:42:46.040]   They could make a fire phone and a Google phone, which was never allowed before.
[01:42:46.040 --> 01:42:47.040]   Right.
[01:42:47.040 --> 01:42:55.880]   And in the US, Amazon was kind of forced because they sell prime special phones that
[01:42:55.880 --> 01:43:03.120]   used to have the Amazon Play Store as the default app where you get your apps.
[01:43:03.120 --> 01:43:09.520]   And they actually had to revert that to a different setup because Google was saying,
[01:43:09.520 --> 01:43:14.640]   "Look, if this manufacturer does this, we're not going to let them do any Play Store stuff."
[01:43:14.640 --> 01:43:19.160]   So had that been the EU in present day, they wouldn't be able to enforce that.
[01:43:19.160 --> 01:43:22.600]   But we're in America and we don't have any kind of reward that.
[01:43:22.600 --> 01:43:24.080]   God bless it.
[01:43:24.080 --> 01:43:26.000]   God bless it.
[01:43:26.000 --> 01:43:28.880]   It does feel a little bit like Google saying, "I will screw you, EU.
[01:43:28.880 --> 01:43:31.880]   We're going to just charge you."
[01:43:31.880 --> 01:43:33.800]   In a way, their explanation is reasonable.
[01:43:33.800 --> 01:43:36.720]   Yeah, you took away some of our revenue opportunities.
[01:43:36.720 --> 01:43:37.960]   We got to make it up.
[01:43:37.960 --> 01:43:38.960]   The cost...
[01:43:38.960 --> 01:43:39.960]   The how long...
[01:43:39.960 --> 01:43:41.760]   I mean, it's not like Google's suffering for cash, right?
[01:43:41.760 --> 01:43:42.760]   No.
[01:43:42.760 --> 01:43:46.040]   So it is a desperately greedy move to demand that I want my $40 per phone.
[01:43:46.040 --> 01:43:47.040]   Well, it's as little as $250.
[01:43:47.040 --> 01:43:51.480]   It depends on the country and it depends on the phone and the resolution of the screen
[01:43:51.480 --> 01:43:52.480]   and all that.
[01:43:52.480 --> 01:43:58.760]   It's just one of these situations where companies that are making vast wads of cash for almost
[01:43:58.760 --> 01:44:02.240]   zero effort and they still want their cash.
[01:44:02.240 --> 01:44:04.920]   This is Ebenezer Scrooge right in action.
[01:44:04.920 --> 01:44:09.000]   They could give away that and just go, "Right, whatever, phone."
[01:44:09.000 --> 01:44:13.240]   And no, they have to make it.
[01:44:13.240 --> 01:44:18.280]   So Seth, if you noticed, I won't ask Greg because he doesn't use a Pixel 3.
[01:44:18.280 --> 01:44:22.360]   But I noticed this when I was playing Pokemon Go in the city that every time I launched
[01:44:22.360 --> 01:44:27.480]   the camera, because I was ostensibly in San Francisco to take pictures to test the Google's
[01:44:27.480 --> 01:44:29.560]   camera in the Pixel 3.
[01:44:29.560 --> 01:44:33.520]   But I happen to be playing Pokemon Go in between every time I launched the camera, it would
[01:44:33.520 --> 01:44:34.520]   kill Pokemon Go.
[01:44:34.520 --> 01:44:36.520]   There's something going on with multitasking.
[01:44:36.520 --> 01:44:39.080]   Yeah, so there's a couple things.
[01:44:39.080 --> 01:44:45.280]   One is the, for whatever reason, the foreground app is using...
[01:44:45.280 --> 01:44:49.840]   It's the only part of the app that's kind of protected.
[01:44:49.840 --> 01:44:54.080]   So if it's not in the foreground, all bets are off.
[01:44:54.080 --> 01:44:55.920]   And I think this is a temporary problem.
[01:44:55.920 --> 01:44:56.920]   I think Google's going to go back.
[01:44:56.920 --> 01:44:58.800]   Intentional for battery life.
[01:44:58.800 --> 01:45:02.640]   Because we've come and gone on this for a long time.
[01:45:02.640 --> 01:45:05.560]   Apple didn't allow background tasks at all.
[01:45:05.560 --> 01:45:10.360]   You couldn't have a Spotify running in the background because they wanted to save battery
[01:45:10.360 --> 01:45:12.520]   because that's a real drain on battery.
[01:45:12.520 --> 01:45:16.040]   If it is, give users an option.
[01:45:16.040 --> 01:45:21.480]   I would rather lose battery than lose what I was working on basically.
[01:45:21.480 --> 01:45:25.920]   It seems unceremoniously closed the background application.
[01:45:25.920 --> 01:45:26.920]   Yeah.
[01:45:26.920 --> 01:45:28.160]   So maybe it's a bug.
[01:45:28.160 --> 01:45:30.680]   Something certainly Google could change.
[01:45:30.680 --> 01:45:34.280]   I think people notice it mostly with music apps because you're listening to music in
[01:45:34.280 --> 01:45:39.360]   the background and then you launch the camera and the music stops.
[01:45:39.360 --> 01:45:40.360]   According...
[01:45:40.360 --> 01:45:41.360]   This is from Android Police.
[01:45:41.360 --> 01:45:43.360]   They said in our own test, more than three.
[01:45:43.360 --> 01:45:49.440]   If you have more than three to four apps running, you'll run out of memory and background
[01:45:49.440 --> 01:45:50.440]   absolutely closed.
[01:45:50.440 --> 01:45:52.280]   Do you think the problem is that they just...
[01:45:52.280 --> 01:45:55.400]   It's a four gig phone, which seems like four gigs should be plenty.
[01:45:55.400 --> 01:45:58.200]   Four gigs should be enough for at least a few apps open.
[01:45:58.200 --> 01:46:02.320]   They compared that to the OnePlus and something else.
[01:46:02.320 --> 01:46:04.120]   The OnePlus says six.
[01:46:04.120 --> 01:46:05.120]   Yeah.
[01:46:05.120 --> 01:46:06.120]   There is a...
[01:46:06.120 --> 01:46:07.120]   You can get a note.
[01:46:07.120 --> 01:46:11.280]   If you want to spend a lot of money on the Note 9, you can get an 8 gig flavor.
[01:46:11.280 --> 01:46:16.720]   I think that that's more up to Google and this may be a bad memory management choice.
[01:46:16.720 --> 01:46:17.720]   I agree.
[01:46:17.720 --> 01:46:18.720]   I agree.
[01:46:18.720 --> 01:46:19.800]   That they could fix that.
[01:46:19.800 --> 01:46:21.400]   I don't think four gigs.
[01:46:21.400 --> 01:46:22.400]   That seems like an awful...
[01:46:22.400 --> 01:46:23.400]   What does the iPhone have?
[01:46:23.400 --> 01:46:24.400]   No one knows.
[01:46:24.400 --> 01:46:25.400]   They're like two gigs.
[01:46:25.400 --> 01:46:30.920]   They went to three gigs and then on the big one there.
[01:46:30.920 --> 01:46:32.880]   Nobody knows because Apple never talks about it.
[01:46:32.880 --> 01:46:33.880]   They never talk about it.
[01:46:33.880 --> 01:46:34.880]   You have to take it apart.
[01:46:34.880 --> 01:46:36.840]   I fix it as to tell us.
[01:46:36.840 --> 01:46:39.840]   They also do a better job of managing memory.
[01:46:39.840 --> 01:46:44.080]   They have the dedicated processors in there for memory management.
[01:46:44.080 --> 01:46:46.000]   They have memory expanded.
[01:46:46.000 --> 01:46:50.480]   We think there's memory expanders on the on board socks that they've programmed into
[01:46:50.480 --> 01:46:52.320]   the ARM chips to make it more viable.
[01:46:52.320 --> 01:46:57.160]   They've also done a lot with their own... with the Xcode stuff to make it efficient.
[01:46:57.160 --> 01:47:01.400]   Their memory handle is a bit of an... because Android is written in Java, you're kind of
[01:47:01.400 --> 01:47:02.400]   screwed.
[01:47:02.400 --> 01:47:03.400]   Yes.
[01:47:03.400 --> 01:47:04.400]   No kidding.
[01:47:04.400 --> 01:47:11.440]   Actually, there's a great piece in an on-tech on the A12 processor and the amazing things
[01:47:11.440 --> 01:47:12.440]   that Apple's did.
[01:47:12.440 --> 01:47:16.200]   A lot of things that they don't talk about that are just really smart, especially with...
[01:47:16.200 --> 01:47:17.200]   The JavaScript accelerator.
[01:47:17.200 --> 01:47:18.200]   Yeah.
[01:47:18.200 --> 01:47:19.200]   There's a hardware JavaScript accelerator.
[01:47:19.200 --> 01:47:20.200]   Yeah.
[01:47:20.200 --> 01:47:23.840]   And there's instructions to the ARM core just to solve a JavaScript.
[01:47:23.840 --> 01:47:28.120]   And now the iPhone is faster than a high Mac in rendering JavaScript somehow.
[01:47:28.120 --> 01:47:29.120]   Yeah.
[01:47:29.120 --> 01:47:30.120]   Isn't that wild?
[01:47:30.120 --> 01:47:31.120]   Yeah.
[01:47:31.120 --> 01:47:32.120]   It's actually...
[01:47:32.120 --> 01:47:35.320]   It's not like Intel could have done that.
[01:47:35.320 --> 01:47:40.000]   Remember last time I was on the show, I was sort of highlighting the fact that Intel hasn't
[01:47:40.000 --> 01:47:41.880]   really advanced following the market.
[01:47:41.880 --> 01:47:45.720]   They've gone down a path and sort of tried to drag the market along with them.
[01:47:45.720 --> 01:47:50.120]   This is kind of a hint that that's actually something that we're seeing.
[01:47:50.120 --> 01:47:51.120]   Let me ask me on the radio show.
[01:47:51.120 --> 01:47:55.800]   I'm going to ask you, Greg, should I get an Intel chip or should I be seduced away by
[01:47:55.800 --> 01:47:58.800]   the new AMD Ryzen?
[01:47:58.800 --> 01:48:00.840]   I don't care, really.
[01:48:00.840 --> 01:48:01.840]   It doesn't matter.
[01:48:01.840 --> 01:48:02.840]   It doesn't.
[01:48:02.840 --> 01:48:04.280]   They're all so fast.
[01:48:04.280 --> 01:48:09.200]   I think you could argue that this one's better than that one.
[01:48:09.200 --> 01:48:15.200]   We could go and look at cars and argue over a V6 versus a 4-speed with a turbocharger.
[01:48:15.200 --> 01:48:16.200]   You buy whatever you buy.
[01:48:16.200 --> 01:48:20.800]   But I think the main thing to keep in mind is that you only want to keep it for two to
[01:48:20.800 --> 01:48:24.600]   three years and then just don't waste too much of your life picking out a phone that's
[01:48:24.600 --> 01:48:27.760]   only going to be around for 24 months.
[01:48:27.760 --> 01:48:35.160]   Is the moral of this conversation or at least the modern era of technology that it is...
[01:48:35.160 --> 01:48:37.680]   The mobile era of technology is mature.
[01:48:37.680 --> 01:48:39.480]   Is IoT the next thing?
[01:48:39.480 --> 01:48:43.560]   I don't know about consumer IoT, but it seems as though there is a theme running through
[01:48:43.560 --> 01:48:50.560]   all of these conversations and that is that which was once revolutionary and fantastic
[01:48:50.560 --> 01:48:56.920]   and really thrilled us maybe 10 years or so ago is it's not necessarily bad.
[01:48:56.920 --> 01:49:03.200]   It's just reached an apex and the lines no longer do the hockey stick of excitement or
[01:49:03.200 --> 01:49:06.720]   of sales unless you're reaching the right market.
[01:49:06.720 --> 01:49:13.080]   Are we, even with processors, are we at a point where until we hit the next thing and
[01:49:13.080 --> 01:49:19.640]   I won't say quantum with any bit of seriousness, but are we in the next era of computing or
[01:49:19.640 --> 01:49:24.680]   at least at the pinnacle of the mobile era of computing?
[01:49:24.680 --> 01:49:29.040]   Well, I think when you say mobile and you mean phones, yes.
[01:49:29.040 --> 01:49:31.280]   I think we're definitely apex.
[01:49:31.280 --> 01:49:35.880]   It's hard to imagine something that you could do that would make everybody jump in and down,
[01:49:35.880 --> 01:49:37.880]   but I think we're going to see wearables, right?
[01:49:37.880 --> 01:49:39.520]   Yeah, I mean, I think the watch is right.
[01:49:39.520 --> 01:49:40.520]   Yeah, so IoT, right?
[01:49:40.520 --> 01:49:41.520]   Yeah.
[01:49:41.520 --> 01:49:43.680]   As wearables, as wearables IoT, I don't know.
[01:49:43.680 --> 01:49:44.680]   I don't know.
[01:49:44.680 --> 01:49:45.920]   I think it feels like the next.
[01:49:45.920 --> 01:49:48.080]   Now I played with the magic.
[01:49:48.080 --> 01:49:52.800]   I played with the magic leap last week and I think I felt that there.
[01:49:52.800 --> 01:49:56.640]   No, it's the same stupid.
[01:49:56.640 --> 01:49:58.400]   So at the time, I'm all excited.
[01:49:58.400 --> 01:50:00.520]   This is the same thing happened to me with the Oculus Rift.
[01:50:00.520 --> 01:50:01.520]   Oh, this is great.
[01:50:01.520 --> 01:50:02.520]   And the HoloLens.
[01:50:02.520 --> 01:50:04.320]   The whole thing is going to change the world.
[01:50:04.320 --> 01:50:05.320]   So exciting.
[01:50:05.320 --> 01:50:07.480]   But that is, I'm thinking about it.
[01:50:07.480 --> 01:50:14.400]   It was a fairly anemic experience and I mean that almost literally the images are washed
[01:50:14.400 --> 01:50:20.760]   out and watery and it wasn't really honestly, I don't, it's not going to change the world
[01:50:20.760 --> 01:50:21.760]   as it stands.
[01:50:21.760 --> 01:50:26.240]   Now I can see a day when you could get something amazing and something that looks like regular
[01:50:26.240 --> 01:50:29.920]   glasses, but I don't know that's going to happen.
[01:50:29.920 --> 01:50:35.200]   Yeah, a solution looking for a problem, a bit like Bitcoin and Ethereum still a solution
[01:50:35.200 --> 01:50:36.200]   looking for a problem.
[01:50:36.200 --> 01:50:37.200]   I'm going to be a better once we get blockchain.
[01:50:37.200 --> 01:50:48.240]   All right, I am going to show you the most exciting, the most exciting technological revolution
[01:50:48.240 --> 01:50:50.640]   that has happened this week in a moment.
[01:50:50.640 --> 01:50:55.200]   But first, I want to tell you what I'm having for dinner.
[01:50:55.200 --> 01:50:56.840]   Actually, I don't know.
[01:50:56.840 --> 01:50:57.960]   I know it's in the fridge.
[01:50:57.960 --> 01:50:59.720]   I'm having blue apron.
[01:50:59.720 --> 01:51:01.000]   Blue apron is awesome.
[01:51:01.000 --> 01:51:02.160]   We don't get the end of the day.
[01:51:02.160 --> 01:51:03.360]   This is a hard day for me.
[01:51:03.360 --> 01:51:04.680]   I do the radio shoot, do twit.
[01:51:04.680 --> 01:51:05.680]   I'm working hard.
[01:51:05.680 --> 01:51:07.880]   I get home around six in the evening.
[01:51:07.880 --> 01:51:08.880]   I'm starving.
[01:51:08.880 --> 01:51:09.880]   I don't want to go shopping.
[01:51:09.880 --> 01:51:12.880]   I don't want to do meal planning, but thank God for blue apron.
[01:51:12.880 --> 01:51:14.600]   Once a week we get our blue apron delivery.
[01:51:14.600 --> 01:51:16.600]   You choose, you know, three or four meals.
[01:51:16.600 --> 01:51:18.880]   I get three meals in a box.
[01:51:18.880 --> 01:51:23.160]   And I know Sunday night I'm going to cook and I'm going to cook a blue apron meal.
[01:51:23.160 --> 01:51:24.360]   Everything I need is there.
[01:51:24.360 --> 01:51:26.200]   All the ingredients, exactly the right amounts.
[01:51:26.200 --> 01:51:29.840]   If I need three tablespoons of soy sauce, there's a little bottle with three tablespoons
[01:51:29.840 --> 01:51:30.840]   of soy sauce.
[01:51:30.840 --> 01:51:31.840]   There's no waste.
[01:51:31.840 --> 01:51:36.800]   The recipes are great and they're easy to follow and they're just, they're just delicious.
[01:51:36.800 --> 01:51:40.560]   The best thing about blue apron for me is I get new ideas for things I'm going to make
[01:51:40.560 --> 01:51:41.560]   down the road.
[01:51:41.560 --> 01:51:44.720]   I cook something or I use an ingredient I've never tried before.
[01:51:44.720 --> 01:51:46.080]   Farrow, speaking of Greg.
[01:51:46.080 --> 01:51:47.080]   Farrow.
[01:51:47.080 --> 01:51:48.800]   I never had ferro before.
[01:51:48.800 --> 01:51:51.360]   Not Greg Ferro, the other kind, the wheat ferro.
[01:51:51.360 --> 01:51:53.120]   I cooked it the other day of blue apron.
[01:51:53.120 --> 01:51:54.120]   It was amazing.
[01:51:54.120 --> 01:51:55.520]   I'm going to buy it from now on.
[01:51:55.520 --> 01:51:58.160]   So it broadens your horizons.
[01:51:58.160 --> 01:51:59.680]   I really love blue apron.
[01:51:59.680 --> 01:52:04.160]   Whether you're looking for quick and easy meals or a full culinary cooking experience,
[01:52:04.160 --> 01:52:10.360]   blue apron, unless you choose from a range of recipe options, you go to the website, blueapron.com/twit
[01:52:10.360 --> 01:52:11.840]   and you can choose from the menus.
[01:52:11.840 --> 01:52:14.200]   They have some amazing dishes.
[01:52:14.200 --> 01:52:18.200]   Every week at least three recipes are designed with your schedule and mine where they've
[01:52:18.200 --> 01:52:19.360]   done the meal prep for you.
[01:52:19.360 --> 01:52:21.600]   They've done the sauces, the spices and the ingredients.
[01:52:21.600 --> 01:52:23.120]   So it's very quick and fast.
[01:52:23.120 --> 01:52:26.040]   That's really, that's something new and I love that.
[01:52:26.040 --> 01:52:27.160]   Get rid of your grocery list.
[01:52:27.160 --> 01:52:32.740]   Let blue apron do the meal prep for you as little as 20 minutes from getting home to
[01:52:32.740 --> 01:52:35.880]   sitting down and your family will love it.
[01:52:35.880 --> 01:52:40.600]   They have blue apron for couples, blue apron for families with kid friendly ingredients
[01:52:40.600 --> 01:52:46.280]   and the smell and it's so much better than take out or don't go to the olive garden.
[01:52:46.280 --> 01:52:49.080]   Just no one should ever go to the olive garden.
[01:52:49.080 --> 01:52:52.560]   Just make something, the house smells amazing.
[01:52:52.560 --> 01:52:53.720]   The kids go, "What's for dinner?
[01:52:53.720 --> 01:52:55.560]   That smells incredible.
[01:52:55.560 --> 01:52:56.560]   Get them to help.
[01:52:56.560 --> 01:52:57.560]   Get them to help.
[01:52:57.560 --> 01:52:58.560]   Learn them.
[01:52:58.560 --> 01:52:59.560]   Learn them how to cook."
[01:52:59.560 --> 01:53:00.920]   Get out of your cooking right.
[01:53:00.920 --> 01:53:05.280]   Experience the joy of chef design recipes, restaurant quality meals, bedding, that tomato
[01:53:05.280 --> 01:53:09.360]   and basil pesto pizza with roasted cauliflower.
[01:53:09.360 --> 01:53:14.480]   I can't do these ads because my mouth starts watering every single time.
[01:53:14.480 --> 01:53:15.480]   Stir fried.
[01:53:15.480 --> 01:53:16.480]   This sounds so good.
[01:53:16.480 --> 01:53:19.720]   Stir fried sweet chili chicken with broccoli and rice.
[01:53:19.720 --> 01:53:20.720]   You like steak?
[01:53:20.720 --> 01:53:24.680]   How about seared steaks and homemade, homemade steak sauce with mashed potatoes and sauteed
[01:53:24.680 --> 01:53:25.680]   carrots.
[01:53:25.680 --> 01:53:31.440]   Oh, seared beef dumplings and jasmine rice with sesame garlic bok choy salad.
[01:53:31.440 --> 01:53:33.560]   Just so wonderful.
[01:53:33.560 --> 01:53:34.960]   I love blue apron.
[01:53:34.960 --> 01:53:37.600]   The other day we cooked, it was so good.
[01:53:37.600 --> 01:53:41.440]   It was fettuccine, it was shrimp, spicy shrimp.
[01:53:41.440 --> 01:53:44.280]   It was a one-pot meal, it was easy to cook and it was delicious.
[01:53:44.280 --> 01:53:45.280]   Everybody ate it up.
[01:53:45.280 --> 01:53:47.880]   The other day, gnocchi and beef.
[01:53:47.880 --> 01:53:49.800]   I can go on and on.
[01:53:49.800 --> 01:53:50.800]   Skip meal planning.
[01:53:50.800 --> 01:53:52.400]   Get straight to cooking with blue apron.
[01:53:52.400 --> 01:53:54.040]   I know you're tired at the end of the day.
[01:53:54.040 --> 01:53:55.960]   Please, don't go to Olive Garden.
[01:53:55.960 --> 01:54:00.240]   Check out this week's menu or worst Mickey D's.
[01:54:00.240 --> 01:54:01.240]   Check it out.
[01:54:01.240 --> 01:54:04.600]   Get your first three meals free at blue apron.com/tuit.
[01:54:04.600 --> 01:54:05.640]   You're going to love it.
[01:54:05.640 --> 01:54:07.560]   Blue apron.com/tuit.
[01:54:07.560 --> 01:54:10.120]   Your first three meals free.
[01:54:10.120 --> 01:54:11.760]   It's just a better way to cook.
[01:54:11.760 --> 01:54:12.760]   I love it.
[01:54:12.760 --> 01:54:14.720]   All right.
[01:54:14.720 --> 01:54:18.440]   The big tech reveal.
[01:54:18.440 --> 01:54:21.200]   Echo, turn on whisper mode.
[01:54:21.200 --> 01:54:26.880]   Okay, I've turned on whispered responses.
[01:54:26.880 --> 01:54:28.720]   Echo, oh wait, I have to do it.
[01:54:28.720 --> 01:54:31.240]   What time is it?
[01:54:31.240 --> 01:54:32.680]   It's 5.09 p.m.
[01:54:32.680 --> 01:54:36.320]   How do you like that?
[01:54:36.320 --> 01:54:38.680]   Wait a minute, let me try that again.
[01:54:38.680 --> 01:54:40.680]   Echo, what time is it?
[01:54:40.680 --> 01:54:45.040]   I think he just whispered to me.
[01:54:45.040 --> 01:54:48.440]   From now on, when you whisper, I will whisper back.
[01:54:48.440 --> 01:54:51.280]   You can always say turn off whisper mode.
[01:54:51.280 --> 01:54:54.000]   Echo, it's 5.09 p.m.
[01:54:54.000 --> 01:54:59.480]   They should have called it Halloween mode.
[01:54:59.480 --> 01:55:01.880]   That's the best thing that's happened all week to me.
[01:55:01.880 --> 01:55:02.880]   Oh, wait, there you go.
[01:55:02.880 --> 01:55:03.880]   We could go home.
[01:55:03.880 --> 01:55:04.880]   Let's just pack this up.
[01:55:04.880 --> 01:55:05.880]   Let's go home everybody.
[01:55:05.880 --> 01:55:06.880]   We're done.
[01:55:06.880 --> 01:55:07.880]   Let's pack it up.
[01:55:07.880 --> 01:55:08.880]   I got a scram.
[01:55:08.880 --> 01:55:09.880]   Thank you guys.
[01:55:09.880 --> 01:55:10.880]   All right.
[01:55:10.880 --> 01:55:11.880]   Thank you, Dan.
[01:55:11.880 --> 01:55:12.880]   We'll let you in the future.
[01:55:12.880 --> 01:55:14.280]   He's running off to turn on whisper mode.
[01:55:14.280 --> 01:55:16.160]   Dan Patterson at Dan Patterson on the Twitter.
[01:55:16.160 --> 01:55:23.880]   You can tweet him there if you have input into their great hacking series at CBSNews.com/electionhacking.
[01:55:23.880 --> 01:55:24.880]   Thank you.
[01:55:24.880 --> 01:55:25.880]   Thanks all.
[01:55:25.880 --> 01:55:27.880]   Echo, I love you.
[01:55:27.880 --> 01:55:29.880]   That's really nice.
[01:55:29.880 --> 01:55:30.880]   Thanks.
[01:55:30.880 --> 01:55:36.520]   Yeah, you're welcome.
[01:55:36.520 --> 01:55:39.520]   Oh, yeah.
[01:55:39.520 --> 01:55:48.040]   Eight adult websites were hacked, exposing intimate user data.
[01:55:48.040 --> 01:55:51.400]   Actually what I got from this article on NARS Technica was how interesting some of these
[01:55:51.400 --> 01:55:55.800]   websites are wifelovers.com.
[01:55:55.800 --> 01:55:57.800]   Are you on wifelovers.com?
[01:55:57.800 --> 01:55:58.800]   Uh oh.
[01:55:58.800 --> 01:56:01.520]   It's amazing.
[01:56:01.520 --> 01:56:05.840]   These sites have never taken the security seriously to the point where.
[01:56:05.840 --> 01:56:06.840]   They don't care.
[01:56:06.840 --> 01:56:07.840]   Yeah.
[01:56:07.840 --> 01:56:09.160]   Do you think that they would be secure?
[01:56:09.160 --> 01:56:10.640]   It doesn't make them secure.
[01:56:10.640 --> 01:56:16.840]   No, IP addresses connected with the site's user passwords protected by a 40 year old cryptographic
[01:56:16.840 --> 01:56:21.840]   scheme that sounds like maybe a ROT 13 or something.
[01:56:21.840 --> 01:56:22.840]   Names.
[01:56:22.840 --> 01:56:24.840]   Captain Crunch decoder.
[01:56:24.840 --> 01:56:25.840]   Yeah.
[01:56:25.840 --> 01:56:26.840]   Yeah.
[01:56:26.840 --> 01:56:32.760]   Names and 1.4 million unique email addresses.
[01:56:32.760 --> 01:56:38.160]   Robert Angelini who owns wifelovers.com and the seven other sites told ours yesterday
[01:56:38.160 --> 01:56:47.560]   that in 21 years they've had 21 years fewer than 107,000 people have posted.
[01:56:47.560 --> 01:56:51.360]   So where did they think I obviously he's not been doing a very good job of managing spam
[01:56:51.360 --> 01:56:52.360]   bots.
[01:56:52.360 --> 01:56:58.920]   It's not exactly Ashley Madison, but yeah, they were using something called Descript or
[01:56:58.920 --> 01:57:01.640]   is it DEScript?
[01:57:01.640 --> 01:57:06.560]   But that was a hash function created in 1979 based on DES.
[01:57:06.560 --> 01:57:07.560]   Wow.
[01:57:07.560 --> 01:57:10.160]   And that's been obsolete for a very long time.
[01:57:10.160 --> 01:57:11.160]   Yeah.
[01:57:11.160 --> 01:57:14.880]   Well, DES was cracked shortly thereafter.
[01:57:14.880 --> 01:57:21.880]   So anyway, just want to if you're on wifelovers.com, you just, you know, I don't know.
[01:57:21.880 --> 01:57:24.240]   You probably aren't using your real email address.
[01:57:24.240 --> 01:57:26.240]   I would hope not.
[01:57:26.240 --> 01:57:32.960]   What did you think Leo about see the Bloomberg story this week?
[01:57:32.960 --> 01:57:35.440]   I'm still all fired up about this one.
[01:57:35.440 --> 01:57:36.520]   We could talk about this one.
[01:57:36.520 --> 01:57:40.640]   I think this is still, you know, in fact, I'm sorry, Dan's not here because I think this
[01:57:40.640 --> 01:57:43.080]   is still a hot topic.
[01:57:43.080 --> 01:57:46.000]   And now the latest is that Tim Cook is demanding a retraction.
[01:57:46.000 --> 01:57:50.180]   So in case you missed it somehow two weeks ago, Bloomberg Business Week and they put this
[01:57:50.180 --> 01:57:55.900]   on the cover of their print magazine declared that Super Micro had been hacked at the
[01:57:55.900 --> 01:57:56.900]   factory.
[01:57:56.900 --> 01:58:01.760]   I mean, they don't assert that Super Micro knew this was going on, but that small rice
[01:58:01.760 --> 01:58:08.500]   sized chips have been placed on Super Micro motherboards intended for companies like Amazon,
[01:58:08.500 --> 01:58:11.860]   Elemental Apple, the Department of Justice.
[01:58:11.860 --> 01:58:16.180]   These motherboards are used to control drones.
[01:58:16.180 --> 01:58:21.500]   And the elemental system, which we use, which is a streaming system, it was discovered apparently
[01:58:21.500 --> 01:58:25.380]   by Amazon during their security due diligence before acquiring Elemental, which they have
[01:58:25.380 --> 01:58:28.180]   since done that this chips was on there.
[01:58:28.180 --> 01:58:32.900]   The what it would do when you booted the operating system would still secure boot clean.
[01:58:32.900 --> 01:58:39.060]   But after the boot, the chip would get enough and modify the operating system, phone home.
[01:58:39.060 --> 01:58:41.180]   It's believed it was the Chinese military that we're doing it.
[01:58:41.180 --> 01:58:46.340]   I talked last week to Mark Millian who writes for Bloomberg Business Week, knows the authors
[01:58:46.340 --> 01:58:48.140]   of these articles.
[01:58:48.140 --> 01:58:49.460]   He says they're very credible.
[01:58:49.460 --> 01:58:53.300]   The story was in the works since the Obama administration for more than.
[01:58:53.300 --> 01:58:55.300]   They're not credible Leo.
[01:58:55.300 --> 01:58:59.540]   They actually have a history of blowing up stories that actually have no foundation in
[01:58:59.540 --> 01:59:00.540]   fact.
[01:59:00.540 --> 01:59:01.540]   Well, tell me what stories.
[01:59:01.540 --> 01:59:04.660]   I want to, you know, people have been saying this, but I want to see those stories and I
[01:59:04.660 --> 01:59:10.020]   want to see what they got wrong because Mark said George Richardson is very, very technical.
[01:59:10.020 --> 01:59:11.020]   Okay.
[01:59:11.020 --> 01:59:12.020]   So NSA Hartbleed.
[01:59:12.020 --> 01:59:18.260]   So back in 2014, there was a bug in open SSL.
[01:59:18.260 --> 01:59:24.580]   Which, yeah, and in theory, the NSA knew about that and they wrote an article claiming that
[01:59:24.580 --> 01:59:26.780]   that was complete falsehood.
[01:59:26.780 --> 01:59:29.300]   And they never resolved from that position.
[01:59:29.300 --> 01:59:32.980]   There's another one after that, the author Jordan.
[01:59:32.980 --> 01:59:33.980]   What's his name?
[01:59:33.980 --> 01:59:34.980]   The original article.
[01:59:34.980 --> 01:59:35.980]   Michael Riley and Jordan Richardson.
[01:59:35.980 --> 01:59:36.980]   Yeah.
[01:59:36.980 --> 01:59:37.980]   Yeah.
[01:59:37.980 --> 01:59:38.980]   And they've done it again in 2016.
[01:59:38.980 --> 01:59:39.980]   They boot up a big one.
[01:59:39.980 --> 01:59:42.860]   I can't think of the exact one right now because it's getting a bit late.
[01:59:42.860 --> 01:59:44.740]   And then they're doing it again in 2018.
[01:59:44.740 --> 01:59:49.460]   They're beating up a story that actually isn't true and saying, I don't know, but we were
[01:59:49.460 --> 01:59:54.260]   told by secret magic people while we were eating mushrooms down the back of the building
[01:59:54.260 --> 01:59:57.500]   that this is actually true.
[01:59:57.500 --> 01:59:59.860]   There's no evidence here.
[01:59:59.860 --> 02:00:04.260]   You could certainly make in my view, this is a story that I could tell you things about
[02:00:04.260 --> 02:00:08.060]   possible security breaches and you could run away and tell a story like this.
[02:00:08.060 --> 02:00:13.620]   But everybody in my community, like I work in enterprise IT and we actually look for this
[02:00:13.620 --> 02:00:16.020]   sort of stuff.
[02:00:16.020 --> 02:00:21.300]   And yes, it's all theoretically possible, but it's actually no one's ever seen it in real
[02:00:21.300 --> 02:00:22.300]   life.
[02:00:22.300 --> 02:00:23.860]   No one's ever seen it.
[02:00:23.860 --> 02:00:31.060]   The Heartbleed story is interesting because it is related in some ways.
[02:00:31.060 --> 02:00:35.540]   Michael Riley who wrote that was one of the authors on this and the Super Micro Story,
[02:00:35.540 --> 02:00:42.380]   he says, or he said in his article that the NSA was new about Heartbleed and used it.
[02:00:42.380 --> 02:00:43.380]   Yep.
[02:00:43.380 --> 02:00:49.460]   It's very similar because the reason you're saying it's debunked is because the NSA denied
[02:00:49.460 --> 02:00:51.180]   it and the government denied it.
[02:00:51.180 --> 02:00:55.020]   There's no knowledge of whether they did or did not.
[02:00:55.020 --> 02:00:59.460]   It's the same exact thing, which is that the intelligence agency said, no, we didn't do
[02:00:59.460 --> 02:01:00.460]   it.
[02:01:00.460 --> 02:01:01.460]   Like you believe them?
[02:01:01.460 --> 02:01:02.460]   Yes.
[02:01:02.460 --> 02:01:06.180]   And but more importantly, if they had have taken advantage of it, someone somewhere in
[02:01:06.180 --> 02:01:08.020]   the world would have seen that.
[02:01:08.020 --> 02:01:10.780]   And there's no signs of it ever being exploited by the NSA.
[02:01:10.780 --> 02:01:11.940]   I think that's not credible.
[02:01:11.940 --> 02:01:15.940]   I think that for instance, the NSA may have used it once in a targeted attack.
[02:01:15.940 --> 02:01:18.100]   The NSA collects these not to do them in mass.
[02:01:18.100 --> 02:01:19.940]   I've never seen you go to the toilet.
[02:01:19.940 --> 02:01:21.500]   Therefore, you never go to the toilet.
[02:01:21.500 --> 02:01:24.180]   And that would also be true because I've never seen you go to the toilet.
[02:01:24.180 --> 02:01:28.140]   Well, that's what I'm pointing out is that the NSA could very well have known about it.
[02:01:28.140 --> 02:01:29.540]   There's no way to know that they know.
[02:01:29.540 --> 02:01:31.900]   And you're saying that we don't know that they ever used it.
[02:01:31.900 --> 02:01:32.900]   Isn't it?
[02:01:32.900 --> 02:01:33.900]   Isn't it?
[02:01:33.900 --> 02:01:34.900]   It's an appropriate.
[02:01:34.900 --> 02:01:35.900]   It's exactly what you just said.
[02:01:35.900 --> 02:01:40.260]   It's comprehensively debunked over the years by any number of government employees who said,
[02:01:40.260 --> 02:01:43.060]   "I don't trust those number of government employees."
[02:01:43.060 --> 02:01:47.420]   But you can't publish articles at this scale claiming...
[02:01:47.420 --> 02:01:48.420]   Wait a minute, though.
[02:01:48.420 --> 02:01:49.900]   ...an empty secret fairies at the bottom of the garden.
[02:01:49.900 --> 02:01:50.980]   No, but wait a minute.
[02:01:50.980 --> 02:01:53.900]   What if you have, as they did in this case, 17 sources?
[02:01:53.900 --> 02:01:56.980]   Yeah, anonymous, but that's normal journalistic practice, right, Seth?
[02:01:56.980 --> 02:02:00.540]   You could also just have chatted to 17 people down the club and claimed them as an anonymous
[02:02:00.540 --> 02:02:01.540]   source.
[02:02:01.540 --> 02:02:04.180]   No, first of all, it's not the daily mail.
[02:02:04.180 --> 02:02:06.460]   We're talking about it here.
[02:02:06.460 --> 02:02:13.740]   So one of the sources said that, "Hey, I did talk to Bloomberg and I presented a couple
[02:02:13.740 --> 02:02:19.180]   of hypotheticals, but I know where did I say that this was happening or whatever."
[02:02:19.180 --> 02:02:21.140]   Well, that was one of them, yeah.
[02:02:21.140 --> 02:02:22.140]   I feel like that made me...
[02:02:22.140 --> 02:02:23.140]   No, all of them.
[02:02:23.140 --> 02:02:24.140]   There's five of them.
[02:02:24.140 --> 02:02:28.300]   Risky business had a podcast and he interviewed a couple more, and they were asked, "Would
[02:02:28.300 --> 02:02:30.740]   this be possible and what would be the possible ways?"
[02:02:30.740 --> 02:02:34.580]   And they gave them some hypotheticals, and they're now being reported as fact in the Bloomberg
[02:02:34.580 --> 02:02:35.580]   article.
[02:02:35.580 --> 02:02:39.020]   I kind of feel like this is what's happened.
[02:02:39.020 --> 02:02:40.020]   I don't.
[02:02:40.020 --> 02:02:44.540]   And the only reason is you've got a very respectable journalistic enterprise.
[02:02:44.540 --> 02:02:48.260]   You've got an editor in chief who came from the economist who's highly respected, who
[02:02:48.260 --> 02:02:52.700]   was intimately involved in this two years worth of reporting and oversaw it.
[02:02:52.700 --> 02:02:56.100]   Now I understand people don't like unknown sources, but when you're talking about national
[02:02:56.100 --> 02:03:01.540]   security stories, good luck getting somebody to talk, facing a lifetime in jail.
[02:03:01.540 --> 02:03:04.100]   So let me talk about it this way.
[02:03:04.100 --> 02:03:08.220]   If I was trying to compromise a server, I would not use a hardware implant.
[02:03:08.220 --> 02:03:09.220]   The first thing I would do...
[02:03:09.220 --> 02:03:10.220]   I acknowledge that.
[02:03:10.220 --> 02:03:13.300]   And Super Micro especially had tons of firmware flaws.
[02:03:13.300 --> 02:03:15.540]   There were all sorts of ways to do this.
[02:03:15.540 --> 02:03:19.980]   And so it's very easy to misinterpret a weakness in the BMC and call it a hardware implant,
[02:03:19.980 --> 02:03:22.220]   and that would be false, right?
[02:03:22.220 --> 02:03:27.420]   The second thing is, even if you had an implant on the board, in which case it would be easily
[02:03:27.420 --> 02:03:32.220]   found, and if they were manufactured onto the motherboard, there would be hundreds of
[02:03:32.220 --> 02:03:35.580]   thousands of these so-called implants somewhere in the world.
[02:03:35.580 --> 02:03:37.380]   And no one could find them, not...
[02:03:37.380 --> 02:03:42.460]   Yeah, that's going to ultimately prove the truth of the false sort of this, because somewhere
[02:03:42.460 --> 02:03:45.340]   somebody's got to have a Super Micro motherboard with his chip in.
[02:03:45.340 --> 02:03:49.860]   And people have been actively looking for these implants for over five years, right?
[02:03:49.860 --> 02:03:51.500]   As actively doing so.
[02:03:51.500 --> 02:03:53.740]   The second question is, how do you activate the implant?
[02:03:53.740 --> 02:03:56.980]   So once you've got an implant sitting in the BMC, how do you activate it?
[02:03:56.980 --> 02:03:58.820]   How do you tell it what you want to capture?
[02:03:58.820 --> 02:04:00.900]   And then how do you exfiltrate it?
[02:04:00.900 --> 02:04:03.620]   All right, you've got to make use of all these things.
[02:04:03.620 --> 02:04:05.180]   Quite honestly, doing these things...
[02:04:05.180 --> 02:04:09.300]   Wait a minute, now hold on there, because we know that that is possible.
[02:04:09.300 --> 02:04:14.780]   From the Snowden revelations, we know the NSA had a modified ethernet port that was
[02:04:14.780 --> 02:04:16.860]   able to do exactly this.
[02:04:16.860 --> 02:04:22.060]   No, what they actually did was modified Cisco routers at the time.
[02:04:22.060 --> 02:04:24.220]   Actually got into them and rehacked them.
[02:04:24.220 --> 02:04:28.300]   Now that was back in an era when technology was way less sophisticated.
[02:04:28.300 --> 02:04:32.420]   That is in the mid-2000s or so.
[02:04:32.420 --> 02:04:35.220]   So you're saying you couldn't do this today?
[02:04:35.220 --> 02:04:37.780]   It's incredibly difficult.
[02:04:37.780 --> 02:04:40.420]   In theory, it's possible to do it, right?
[02:04:40.420 --> 02:04:44.620]   In theory, it's possible to create an implant that would be very hard to detect.
[02:04:44.620 --> 02:04:48.900]   However, in reality, you've got to activate the implant.
[02:04:48.900 --> 02:04:50.900]   You've got to program it in some way to give you data.
[02:04:50.900 --> 02:04:52.060]   And then you've got to exfiltrate.
[02:04:52.060 --> 02:04:56.300]   So even if you had a motherboard that had an implant, whether it was in the BMC or in
[02:04:56.300 --> 02:05:01.820]   the ethernet chip, it still doesn't mean that it's actually a weakness or vulnerability
[02:05:01.820 --> 02:05:03.820]   because we put exfiltration controls.
[02:05:03.820 --> 02:05:06.020]   You can't exfiltrate easily.
[02:05:06.020 --> 02:05:08.260]   And the implants would be connected to a separate network.
[02:05:08.260 --> 02:05:12.300]   That would usually imply that there's an implant in the server and then you've got
[02:05:12.300 --> 02:05:14.620]   to get somebody on the inside to activate it.
[02:05:14.620 --> 02:05:18.900]   You're saying that it would have been obvious, even if this had been there, that they were
[02:05:18.900 --> 02:05:20.100]   exfiltrating data.
[02:05:20.100 --> 02:05:21.820]   They were somehow communicating with the answer.
[02:05:21.820 --> 02:05:22.820]   Exactly.
[02:05:22.820 --> 02:05:27.220]   If the hardware implant was in the BMC, then you would be on the out of band network.
[02:05:27.220 --> 02:05:31.140]   And when they tried to, we go to a lot of trouble when we built these data centers so
[02:05:31.140 --> 02:05:35.140]   that even if the out of band network woke up and started doing something, you would see
[02:05:35.140 --> 02:05:39.220]   it because you watch the out of band because that's the control engine.
[02:05:39.220 --> 02:05:44.500]   However, I got to point out, there's no question at all that these kinds of activities, whether
[02:05:44.500 --> 02:05:49.660]   in hardware or software occur all the time, they occur in the supply chain and everybody's
[02:05:49.660 --> 02:05:54.420]   agreeing that and they've been going on for at least 10 years in the supply chain.
[02:05:54.420 --> 02:05:56.580]   And there's not much protection against that.
[02:05:56.580 --> 02:06:01.420]   Well, Bruce Scheier said, or Brian Krebs said it, he reported something similar 10 years
[02:06:01.420 --> 02:06:03.300]   ago in the Washington Post.
[02:06:03.300 --> 02:06:04.300]   Yeah.
[02:06:04.300 --> 02:06:09.500]   We've seen lots and lots of suggestions of it, but we haven't actually seen too much evidence.
[02:06:09.500 --> 02:06:11.300]   There's only been a few in the IOT.
[02:06:11.300 --> 02:06:14.380]   I think you're obviously surprised at how credulous you are.
[02:06:14.380 --> 02:06:17.620]   The government agencies would tell the truth on this.
[02:06:17.620 --> 02:06:21.340]   I suspect that the people who are telling the truth are telling the truth.
[02:06:21.340 --> 02:06:24.180]   I think this is a beat up by Bloomberg.
[02:06:24.180 --> 02:06:26.980]   I do not believe that they actually have anything.
[02:06:26.980 --> 02:06:31.540]   And what they've done is they've taken talking to security experts of what might happen or
[02:06:31.540 --> 02:06:33.380]   what could happen.
[02:06:33.380 --> 02:06:37.940]   And I've seen or somebody got drunk and told him a big story and they've turned it into
[02:06:37.940 --> 02:06:38.940]   a beat up.
[02:06:38.940 --> 02:06:43.660]   Now, it turns out that Bloomberg journalists actually get paid when they move the market
[02:06:43.660 --> 02:06:44.660]   and get the extra bonus.
[02:06:44.660 --> 02:06:45.660]   Yeah, that is a little unfortunate.
[02:06:45.660 --> 02:06:46.660]   Yeah.
[02:06:46.660 --> 02:06:47.660]   It's a great problem.
[02:06:47.660 --> 02:06:51.220]   So it's very difficult to believe that these people have credibility when you know they're
[02:06:51.220 --> 02:06:54.140]   getting bonus by making a beat up out of an ordinary story.
[02:06:54.140 --> 02:06:55.140]   Well, we'll see.
[02:06:55.140 --> 02:06:57.380]   Tim Cook has called for a retraction, which is something unusual.
[02:06:57.380 --> 02:07:00.380]   Apple has never called for a publicly anyway called for a retraction.
[02:07:00.380 --> 02:07:01.380]   Not publicly.
[02:07:01.380 --> 02:07:02.380]   They called for a retraction.
[02:07:02.380 --> 02:07:04.060]   Have you ever experienced anything?
[02:07:04.060 --> 02:07:06.580]   Well, I don't, I won't put you on the spot, Seth.
[02:07:06.580 --> 02:07:09.300]   I find that extraordinary because the CEO of the...
[02:07:09.300 --> 02:07:10.820]   They've asked you to take down stories.
[02:07:10.820 --> 02:07:11.820]   Yeah.
[02:07:11.820 --> 02:07:12.820]   So they do this privately, normally.
[02:07:12.820 --> 02:07:13.820]   Of course.
[02:07:13.820 --> 02:07:14.820]   Yeah.
[02:07:14.820 --> 02:07:15.820]   Of course they do.
[02:07:15.820 --> 02:07:19.460]   They've published, you know, comprehensive, say they've done comprehensive reviews internally
[02:07:19.460 --> 02:07:22.060]   before the article was published and after the article was published.
[02:07:22.060 --> 02:07:28.220]   Do you think, Seth, that Apple, if this were a true story that Apple would lie because
[02:07:28.220 --> 02:07:32.220]   they denied this categorically and now they're asking for a retraction, is it...?
[02:07:32.220 --> 02:07:33.220]   I don't know.
[02:07:33.220 --> 02:07:38.220]   I kind of feel like they have to tell the truth in this scenario because the evidence will
[02:07:38.220 --> 02:07:39.900]   eventually come out or later.
[02:07:39.900 --> 02:07:43.020]   It would be very damaging if they did lie about it.
[02:07:43.020 --> 02:07:44.020]   Yeah.
[02:07:44.020 --> 02:07:45.020]   And I think...
[02:07:45.020 --> 02:07:46.780]   I don't think they'll ever be any evidence.
[02:07:46.780 --> 02:07:48.900]   Well, we don't know, but I think you might be writing.
[02:07:48.900 --> 02:07:49.900]   Yeah, we don't know.
[02:07:49.900 --> 02:07:54.540]   And I think Apple has actually gone through and double checked itself a little bit.
[02:07:54.540 --> 02:07:59.020]   You know, there's been some time since the story came out and Tim called for the public.
[02:07:59.020 --> 02:08:03.060]   And I think he probably went down and said, "Hey, you know, I'm going to say I want a
[02:08:03.060 --> 02:08:04.060]   retraction.
[02:08:04.060 --> 02:08:05.060]   You better not...
[02:08:05.060 --> 02:08:09.300]   You know, there better not be something here that's going to make me a liar."
[02:08:09.300 --> 02:08:11.500]   So you know, he went down in nature.
[02:08:11.500 --> 02:08:13.100]   All of his people were right.
[02:08:13.100 --> 02:08:18.460]   And I think the call for a public retraction, something that they haven't really done before
[02:08:18.460 --> 02:08:21.460]   would require them to be 100% sure that they're right.
[02:08:21.460 --> 02:08:22.620]   But we've also seen...
[02:08:22.620 --> 02:08:29.340]   So just to add to that, you know, AWS has responded saying that this is untrue.
[02:08:29.340 --> 02:08:32.660]   The NSA has responded and said it's untrue.
[02:08:32.660 --> 02:08:36.100]   We've seen various people who used to work for the NSA at the time that this happened,
[02:08:36.100 --> 02:08:38.540]   come out and say, "They were responding...
[02:08:38.540 --> 02:08:41.420]   They were covering this area at the time."
[02:08:41.420 --> 02:08:45.460]   And they're saying, "If this had happened, I would have known about it and it didn't
[02:08:45.460 --> 02:08:46.460]   happen."
[02:08:46.460 --> 02:08:50.940]   And to top it off, the Chinese government said it's not true.
[02:08:50.940 --> 02:08:51.940]   Yeah.
[02:08:51.940 --> 02:08:54.460]   So all that being said, I don't know if it's not a Chinese.
[02:08:54.460 --> 02:08:58.060]   And I mean, the other thing that makes me upset here is attribution.
[02:08:58.060 --> 02:08:59.860]   It doesn't have to be the Chinese government here.
[02:08:59.860 --> 02:09:05.300]   It would be much more plausible that somebody compromised employees at Super Micro.
[02:09:05.300 --> 02:09:06.300]   To put the implant here.
[02:09:06.300 --> 02:09:07.300]   Well, it wasn't Super Micro.
[02:09:07.300 --> 02:09:09.500]   It's had a supplier to Super Micro.
[02:09:09.500 --> 02:09:10.500]   Exactly.
[02:09:10.500 --> 02:09:14.020]   It could easily have been the Russians or the NSA themselves if you want to get all...
[02:09:14.020 --> 02:09:15.220]   Well, it was in China.
[02:09:15.220 --> 02:09:16.740]   So I mean, but yeah.
[02:09:16.740 --> 02:09:21.620]   I don't think that's the real import of the article is that these kinds of things happen
[02:09:21.620 --> 02:09:23.100]   all the time.
[02:09:23.100 --> 02:09:27.420]   And wouldn't it be easier to slip like a little...
[02:09:27.420 --> 02:09:29.060]   Some code onto some firmware?
[02:09:29.060 --> 02:09:30.060]   Absolutely.
[02:09:30.060 --> 02:09:31.060]   And that does happen.
[02:09:31.060 --> 02:09:32.060]   Well, okay.
[02:09:32.060 --> 02:09:33.580]   So just to talk about that.
[02:09:33.580 --> 02:09:36.460]   So this vulnerability happened in 2015.
[02:09:36.460 --> 02:09:42.220]   Since 2015, we have moved into things called Secure Enclaves.
[02:09:42.220 --> 02:09:46.460]   So in particular, your iPhone is a key issue here because it actually was one of the first
[02:09:46.460 --> 02:09:49.580]   devices to come out in the mainstream with Secure Enclaves.
[02:09:49.580 --> 02:09:52.300]   It's largely linked to the finger ID thing.
[02:09:52.300 --> 02:09:54.220]   And TPM's been around for a while.
[02:09:54.220 --> 02:09:55.220]   That's also secure.
[02:09:55.220 --> 02:09:56.220]   Yeah.
[02:09:56.220 --> 02:09:57.220]   So I talked...
[02:09:57.220 --> 02:09:59.660]   When this story broke, I talked to somebody in this business.
[02:09:59.660 --> 02:10:07.580]   He actually is a big, big supplier of hardware to big, big company, server farms.
[02:10:07.580 --> 02:10:13.100]   And as an expert in this area, he said, "Yes, it's completely possible and I believe it
[02:10:13.100 --> 02:10:14.100]   happens all the time."
[02:10:14.100 --> 02:10:19.460]   It happens though just as often in the mail, you got a server on its way to the Department
[02:10:19.460 --> 02:10:23.660]   of Defense and it gets interdicted on the way as it happens anywhere else.
[02:10:23.660 --> 02:10:26.420]   And I don't think he's convinced that any...
[02:10:26.420 --> 02:10:30.780]   And they do, by the way, he did talk a lot about the things you're talking about, the
[02:10:30.780 --> 02:10:33.900]   methods they use to secure these devices like Secure Boot and stuff.
[02:10:33.900 --> 02:10:34.900]   He said, "It's very difficult."
[02:10:34.900 --> 02:10:36.780]   Yeah, SDX and ARMS got its crypto line.
[02:10:36.780 --> 02:10:40.860]   It's very difficult to fully say something is secure.
[02:10:40.860 --> 02:10:44.660]   And there's a lot of incentive for nation state actors and others to get this stuff in
[02:10:44.660 --> 02:10:45.660]   there.
[02:10:45.660 --> 02:10:51.500]   You're going to have to work harder to convince me that this isn't happening.
[02:10:51.500 --> 02:10:52.500]   I think...
[02:10:52.500 --> 02:10:53.500]   You're saying...
[02:10:53.500 --> 02:10:59.420]   Our preventative measures are so successful that this isn't happening.
[02:10:59.420 --> 02:11:04.940]   I think the organizations that are being in question here, they take these very seriously.
[02:11:04.940 --> 02:11:09.140]   All of those organizations test the server, motherboards, they inspect them and indeed
[02:11:09.140 --> 02:11:14.380]   they do inspect the supply chain and have done since Snowden went public.
[02:11:14.380 --> 02:11:18.260]   And there are a range of features available to them to use.
[02:11:18.260 --> 02:11:24.700]   And most of these cloud companies do use SGX or TXT or Arm Crypto Island or AMD secure processing
[02:11:24.700 --> 02:11:25.700]   functions.
[02:11:25.700 --> 02:11:26.700]   They do implement them.
[02:11:26.700 --> 02:11:29.980]   Now, in enterprise IT more generally, now back out here in the real world where I live
[02:11:29.980 --> 02:11:33.340]   in, none of those secure features are actually used in the real world.
[02:11:33.340 --> 02:11:38.420]   The vendors do talk about them and promote the fact that they have these features, but
[02:11:38.420 --> 02:11:43.700]   only a very small percentage of their products actually have this secure boot functionality
[02:11:43.700 --> 02:11:44.700]   through these trustable ways.
[02:11:44.700 --> 02:11:46.220]   You probably missed this.
[02:11:46.220 --> 02:11:50.780]   But the United States GAO, the Government Accountability Office, issued a report to the
[02:11:50.780 --> 02:11:56.420]   Committee on Armed Services in the Senate this month, "Weaponsystems Cybersecurity DoD
[02:11:56.420 --> 02:12:00.500]   just beginning to grapple with the scale of vulnerabilities."
[02:12:00.500 --> 02:12:08.900]   And if you read it, it's a horrific indictment of the security on weapons systems in the
[02:12:08.900 --> 02:12:13.340]   United States which are sitting on public internet in some cases.
[02:12:13.340 --> 02:12:14.340]   I mean, it's...
[02:12:14.340 --> 02:12:15.340]   Yep.
[02:12:15.340 --> 02:12:16.340]   It's horrific.
[02:12:16.340 --> 02:12:17.340]   And they're key here.
[02:12:17.340 --> 02:12:18.340]   The Department of Defense...
[02:12:18.340 --> 02:12:19.340]   The Department of Defense...
[02:12:19.340 --> 02:12:20.340]   The Department of Defense...
[02:12:20.340 --> 02:12:24.700]   Well, the Department of Defense said DoD faces mounting challenges in protecting its weapon
[02:12:24.700 --> 02:12:27.860]   systems from increasingly sophisticated cyber threats.
[02:12:27.860 --> 02:12:31.020]   This state is due to the computerized nature of weapons systems.
[02:12:31.020 --> 02:12:36.620]   DoD's late start in prioritizing weapons systems cybersecurity and the Department of Defense's
[02:12:36.620 --> 02:12:40.540]   nascent understanding of how to develop more secure weapon systems.
[02:12:40.540 --> 02:12:43.420]   They are more software dependent, more network than ever before.
[02:12:43.420 --> 02:12:45.620]   According to the GAO, completely vulnerable.
[02:12:45.620 --> 02:12:50.060]   So I think your confidence in the security of...
[02:12:50.060 --> 02:12:51.060]   No.
[02:12:51.060 --> 02:12:53.820]   My confidence is that hardware security...
[02:12:53.820 --> 02:12:57.580]   My confidence level is that hardware implants is the wrong way to go about this.
[02:12:57.580 --> 02:12:59.860]   Well, there are many ways to go about it, but I think counting on...
[02:12:59.860 --> 02:13:02.780]   I would do it in the software where everyone could wipe or...
[02:13:02.780 --> 02:13:04.340]   ...you could blame it on malware.
[02:13:04.340 --> 02:13:08.700]   No, you're backing down because you implied that the defensive measures were effective.
[02:13:08.700 --> 02:13:12.660]   Bloomberg claims are entirely unfeasible and impractical.
[02:13:12.660 --> 02:13:15.100]   No one would want to do that.
[02:13:15.100 --> 02:13:18.140]   And I believe the whole claims by businesses inside are in that specific.
[02:13:18.140 --> 02:13:25.620]   Now, if you want to talk about the US Navy running Windows NT4 in the weapons system for
[02:13:25.620 --> 02:13:31.740]   a substantial chunk of their surface ships, yes, you do have a vulnerability problem,
[02:13:31.740 --> 02:13:32.740]   right?
[02:13:32.740 --> 02:13:33.740]   Yeah.
[02:13:33.740 --> 02:13:38.260]   And the networking hardware that's in there also dates from the early 2000s as well.
[02:13:38.260 --> 02:13:44.220]   And the service, they actually have 25-year supply agreements with traditional IT vendors
[02:13:44.220 --> 02:13:49.460]   to supply exactly the same kit for the next 25 years.
[02:13:49.460 --> 02:13:52.580]   Vulnerabilities and all, by the way.
[02:13:52.580 --> 02:13:54.460]   So yes, this is a problem.
[02:13:54.460 --> 02:13:56.980]   And it's not just limited to the Navy.
[02:13:56.980 --> 02:14:02.020]   I mean, all of the military stuff often runs on tremendously obsolete hardware.
[02:14:02.020 --> 02:14:08.820]   I mean, if this Bloomberg thing is true, won't there be a motherboard brought forward with
[02:14:08.820 --> 02:14:09.820]   a single chip?
[02:14:09.820 --> 02:14:10.820]   I would think so, yeah.
[02:14:10.820 --> 02:14:11.820]   And as a piece of that...
[02:14:11.820 --> 02:14:15.660]   So these elementals, just to use the specific ones they start the article with, we used
[02:14:15.660 --> 02:14:16.660]   for years.
[02:14:16.660 --> 02:14:17.660]   We still do.
[02:14:17.660 --> 02:14:18.660]   We use an elemental.
[02:14:18.660 --> 02:14:20.460]   Now it's from AWS and we sent the old one back.
[02:14:20.460 --> 02:14:25.780]   But I know people, including Alex Lindsey, who has many old pre-Amazon elementals.
[02:14:25.780 --> 02:14:28.940]   But does he have the wherewithal to scan the motherboard and find these implants?
[02:14:28.940 --> 02:14:30.420]   I mean, I don't know.
[02:14:30.420 --> 02:14:31.740]   I mean, that's the other question.
[02:14:31.740 --> 02:14:36.220]   All he's got to do is tell people that he's got one and they'll come and have a look.
[02:14:36.220 --> 02:14:38.260]   So email me and I will get you...
[02:14:38.260 --> 02:14:40.420]   I will have Alex get you one of these motherboards.
[02:14:40.420 --> 02:14:47.700]   If you know how to find this chip, I don't know what it takes to find it.
[02:14:47.700 --> 02:14:49.660]   It might take some sophisticated tools.
[02:14:49.660 --> 02:14:50.660]   But if you know how to...
[02:14:50.660 --> 02:14:55.980]   Now, one of the other things that struck me just to take this a step further is if hardware
[02:14:55.980 --> 02:15:00.160]   is a security risk, there was an article about two or three months ago where Apple was having
[02:15:00.160 --> 02:15:02.500]   a problem in China a couple of years ago.
[02:15:02.500 --> 02:15:04.980]   So the article was published a couple of months ago and the problem was two or three
[02:15:04.980 --> 02:15:10.220]   years ago, where people would buy iPhones and then take them outside disassemble and take
[02:15:10.220 --> 02:15:14.660]   the genuine components out, put third party components in and then take them back inside
[02:15:14.660 --> 02:15:15.660]   the store.
[02:15:15.660 --> 02:15:19.420]   And it was a way of defrauding the repair process, right?
[02:15:19.420 --> 02:15:23.740]   And so they would have this supply of genuine Apple components that they would then sell
[02:15:23.740 --> 02:15:26.180]   in the spare parts business.
[02:15:26.180 --> 02:15:29.900]   And apparently they used to stand outside the store with a bag of it and have all these
[02:15:29.900 --> 02:15:33.680]   people lined up and paid per phone to take them in to get them repaired.
[02:15:33.680 --> 02:15:37.280]   And that's part of the reason that Apple doesn't repair phones anymore is because it
[02:15:37.280 --> 02:15:40.440]   needs to have the integrity of the supply chain.
[02:15:40.440 --> 02:15:45.720]   And in the latest Macbooks, I believe that Apple is moving to actually lock down the
[02:15:45.720 --> 02:15:50.880]   repair inside of those so that if you try to use a non-genuine component, the box will
[02:15:50.880 --> 02:15:51.880]   not work.
[02:15:51.880 --> 02:15:56.160]   You actually have to have a special app to certify that each... to actually cross sign
[02:15:56.160 --> 02:15:59.000]   all of the components inside of the boxes.
[02:15:59.000 --> 02:16:01.260]   And that's the reaction to this.
[02:16:01.260 --> 02:16:05.220]   Now this is not new, this has been going on for five years or more.
[02:16:05.220 --> 02:16:08.580]   But part of the reason that we are losing our right to repair is because of the supply
[02:16:08.580 --> 02:16:10.380]   chain integrity issue.
[02:16:10.380 --> 02:16:14.660]   And if you say to vendors, you have to prevent this, then at the same time you are also losing
[02:16:14.660 --> 02:16:16.420]   your right to repair.
[02:16:16.420 --> 02:16:20.220]   So you need to be careful about what you ask for.
[02:16:20.220 --> 02:16:24.620]   Alex probably doesn't care whether you can repair his elemental server.
[02:16:24.620 --> 02:16:27.580]   He says he has one made in the spring of 2015.
[02:16:27.580 --> 02:16:28.580]   Yeah.
[02:16:28.580 --> 02:16:32.040]   And he had a little poll, should I void the warranty and break it open and see if we can
[02:16:32.040 --> 02:16:33.040]   find some?
[02:16:33.040 --> 02:16:38.480]   Now, I think finding this, by the way, 95% voted yes, we need to know.
[02:16:38.480 --> 02:16:42.680]   There were some interesting replies in here including Father Robert Baliser and so forth
[02:16:42.680 --> 02:16:44.200]   on what you would need to do.
[02:16:44.200 --> 02:16:46.760]   It's probably non-trivial.
[02:16:46.760 --> 02:16:52.120]   Robert said the best thing to do is just put it online and see what kind of traffic you
[02:16:52.120 --> 02:16:53.120]   get coming out of it.
[02:16:53.120 --> 02:16:55.120]   It'd be a lot easier than finding that chip.
[02:16:55.120 --> 02:16:59.700]   And all you'd have to do is take some high res photos and publish them.
[02:16:59.700 --> 02:17:00.700]   That's enough.
[02:17:00.700 --> 02:17:04.040]   People who know what they're looking for would be able to look at those and start to
[02:17:04.040 --> 02:17:05.480]   go, "Yeah, I know what that chip is.
[02:17:05.480 --> 02:17:06.480]   I know what that is.
[02:17:06.480 --> 02:17:07.480]   Hang on, what's that?"
[02:17:07.480 --> 02:17:08.480]   Yeah.
[02:17:08.480 --> 02:17:13.520]   If it's a chip, if it is indeed a chip, if it indeed is it not, somebody's put the code
[02:17:13.520 --> 02:17:15.920]   inside of a sock or another chip.
[02:17:15.920 --> 02:17:16.920]   Yeah.
[02:17:16.920 --> 02:17:17.920]   Who knows?
[02:17:17.920 --> 02:17:18.920]   That's the problem.
[02:17:18.920 --> 02:17:20.800]   It could even be nowadays the way they make motherboards with layers.
[02:17:20.800 --> 02:17:24.040]   It could even be in the motherboard layer.
[02:17:24.040 --> 02:17:26.760]   One of the layers were not even visible at all.
[02:17:26.760 --> 02:17:27.760]   Perhaps.
[02:17:27.760 --> 02:17:28.760]   I'm not so sure.
[02:17:28.760 --> 02:17:29.760]   I'm not an expert in that field.
[02:17:29.760 --> 02:17:35.480]   I would expect just post some high res photos and tell people to go nuts on them and certain
[02:17:35.480 --> 02:17:37.440]   people would look at it and go, "There it is."
[02:17:37.440 --> 02:17:40.720]   And they'd nudge the person over there who'd nudge the person over there to say, "Look
[02:17:40.720 --> 02:17:41.720]   at that.
[02:17:41.720 --> 02:17:42.720]   Yeah.
[02:17:42.720 --> 02:17:43.720]   That's how this sort of stuff is.
[02:17:43.720 --> 02:17:44.720]   Last story.
[02:17:44.720 --> 02:17:45.720]   Thank you, Greg.
[02:17:45.720 --> 02:17:52.600]   I'm glad you've piped up because we have covered over the last few weeks, but I think
[02:17:52.600 --> 02:17:54.560]   it's great to hear your point of view on it.
[02:17:54.560 --> 02:17:55.560]   Yeah.
[02:17:55.560 --> 02:17:56.800]   I think Bloomberg needs to publish a retraction.
[02:17:56.800 --> 02:17:57.800]   I think it's bunk.
[02:17:57.800 --> 02:18:04.760]   I think it's a complete beat up and that Bloomberg has probably lost its position as a platform
[02:18:04.760 --> 02:18:06.400]   of record.
[02:18:06.400 --> 02:18:10.480]   That would be a fairly dramatic price to pay.
[02:18:10.480 --> 02:18:16.280]   Microsoft co-founder Paul Allen passed away this week at the, sadly, at the age of 65,
[02:18:16.280 --> 02:18:21.240]   which is awfully young, says I, now in my 60s.
[02:18:21.240 --> 02:18:23.360]   He actually, I think it's his third bet with cancer.
[02:18:23.360 --> 02:18:25.280]   He had left Microsoft in 1982.
[02:18:25.280 --> 02:18:29.480]   He was the guy who woke Bill Gates up in his Harvard dorm room, slapping him around with
[02:18:29.480 --> 02:18:34.840]   a popular science article about the Mitzel, Terra computer, saying, "Bill, Bill, you got
[02:18:34.840 --> 02:18:36.760]   to drop out of Harvard.
[02:18:36.760 --> 02:18:39.560]   We got to write some basic for this program."
[02:18:39.560 --> 02:18:44.240]   Bill Gates wrote, "I'm heartbroken by the passing of one of my oldest and dearest friends,
[02:18:44.240 --> 02:18:48.040]   Paul Allen, from our early days together at Lakeside School, through our partnership
[02:18:48.040 --> 02:18:52.240]   in the creation of Microsoft, to some of our joint philanthropic projects of the year.
[02:18:52.240 --> 02:18:54.680]   Paul was a true partner and dear friend.
[02:18:54.680 --> 02:18:57.920]   Personal computing would not have existed without him."
[02:18:57.920 --> 02:19:02.000]   Of course, he was a great philanthropist, spending many millions of dollars and eventually
[02:19:02.000 --> 02:19:09.200]   even agreeing to join Bill Gates and Warren Buffett in their philanthropic pledge in 2010
[02:19:09.200 --> 02:19:13.960]   to give away at least half of his $20 billion fortune.
[02:19:13.960 --> 02:19:16.800]   I guess that is ongoing.
[02:19:16.800 --> 02:19:19.760]   I worked for Paul briefly.
[02:19:19.760 --> 02:19:23.480]   He owned tech TV for about four years.
[02:19:23.480 --> 02:19:26.680]   Ended up selling it to Comcast, but I only met him once.
[02:19:26.680 --> 02:19:32.160]   I don't have much to say about him, except that he seemed like a very sweet guy and kind
[02:19:32.160 --> 02:19:33.680]   of shy.
[02:19:33.680 --> 02:19:35.560]   He was kind of kept himself.
[02:19:35.560 --> 02:19:37.200]   I heard he could play guitar.
[02:19:37.200 --> 02:19:41.120]   He could rock the hell out of a guitar.
[02:19:41.120 --> 02:19:46.320]   Paul Allen, one of the pioneers in the computer industry, probably one that's not well-known
[02:19:46.320 --> 02:19:50.420]   because of, you know, he left it in the early '80s.
[02:19:50.420 --> 02:19:55.680]   But really somebody as Bill Gates with whom personal computing may not exist.
[02:19:55.680 --> 02:19:57.800]   Seth, it was great having you.
[02:19:57.800 --> 02:20:02.720]   Thank you for being here nine to five, nine to five, Mac, nine to five, Google and electric,
[02:20:02.720 --> 02:20:04.280]   all about electric vehicles.
[02:20:04.280 --> 02:20:05.440]   You can read his stuff there.
[02:20:05.440 --> 02:20:06.440]   Thank you, Seth, wine, Trump.
[02:20:06.440 --> 02:20:07.440]   Thanks for having me.
[02:20:07.440 --> 02:20:08.440]   Yeah.
[02:20:08.440 --> 02:20:09.440]   Really appreciate it.
[02:20:09.440 --> 02:20:13.920]   Greg Farrow, packet pushers.net.
[02:20:13.920 --> 02:20:15.480]   Subscribe today.
[02:20:15.480 --> 02:20:17.520]   I think you got some idea of what you're going to be getting.
[02:20:17.520 --> 02:20:18.520]   The theory of mind.
[02:20:18.520 --> 02:20:20.920]   And he wasn't even down to the pub earlier.
[02:20:20.920 --> 02:20:27.160]   No, no, I nearly thought about having a couple of windups, but I thought that wouldn't be
[02:20:27.160 --> 02:20:28.240]   very professional.
[02:20:28.240 --> 02:20:31.360]   Thanks also to Dan Patterson, who had to leave a little bit earlier, but it's great to have
[02:20:31.360 --> 02:20:32.680]   all three of you.
[02:20:32.680 --> 02:20:36.840]   We do this show every Sunday, 3 p.m. Pacific, 6 p.m. Eastern time.
[02:20:36.840 --> 02:20:41.920]   That's for now anyway, 2,200 UTC, although we will be leaving summertime in a couple of
[02:20:41.920 --> 02:20:43.960]   weeks and that will change it a little bit.
[02:20:43.960 --> 02:20:45.720]   You can tune in live at Twit.tv/live.
[02:20:45.720 --> 02:20:46.720]   You can even join us live.
[02:20:46.720 --> 02:20:50.760]   If you want to join us in the studio audience with a couple of very smart people in the
[02:20:50.760 --> 02:20:57.240]   studio audience today, one of whom works for the Google and infrastructure and one of whom
[02:20:57.240 --> 02:21:01.360]   is a PhD in mathematics, don't show them.
[02:21:01.360 --> 02:21:03.960]   They're secret.
[02:21:03.960 --> 02:21:05.600]   No names, but Drew and Zachary.
[02:21:05.600 --> 02:21:06.600]   It's nice to see you.
[02:21:06.600 --> 02:21:07.600]   Thank you.
[02:21:07.600 --> 02:21:09.960]   If you want to join them in the audience, do the same.
[02:21:09.960 --> 02:21:13.880]   All you have to do is email tickets at Twit.tv and be in Petaluma sometime when we're
[02:21:13.880 --> 02:21:16.240]   recording a show, tickets@twit.tv.
[02:21:16.240 --> 02:21:19.880]   You can download copies of everything we do at our website, Twit.tv.
[02:21:19.880 --> 02:21:24.080]   On demand versions of this show are available pretty much anywhere you go, including your
[02:21:24.080 --> 02:21:26.920]   favorite podcast or in fact, why don't you subscribe?
[02:21:26.920 --> 02:21:29.040]   That way you'll get it automatically.
[02:21:29.040 --> 02:21:34.360]   The minute it's done, if you want more information about subscribing, go to twit.tv/subscribe
[02:21:34.360 --> 02:21:39.200]   or just open up your favorite podcast program and search for this week in tech.
[02:21:39.200 --> 02:21:41.480]   I think you'll find it.
[02:21:41.480 --> 02:21:42.480]   Thanks for being here.
[02:21:42.480 --> 02:21:43.480]   We'll see you next time.
[02:21:43.480 --> 02:21:44.480]   Twits!
[02:21:44.480 --> 02:21:45.480]   This is amazing.
[02:21:45.480 --> 02:21:45.480]   Bye bye.
[02:21:45.480 --> 02:21:55.160]   Bye bye.

