;FFMETADATA1
title=No Future for Green Pants
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=779
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.000]   It's time for Twit this week in Tech. What a panel we have from Wired Magazine.
[00:00:04.000 --> 00:00:09.000]   Louise Mazzakis is here. Stacey Higginbotham is also here from Stacey on IOT.
[00:00:09.000 --> 00:00:12.000]   And Phil Libin, the former CEO of Evernote.
[00:00:12.000 --> 00:00:15.000]   He's got a new product aptly named him.
[00:00:15.000 --> 00:00:17.000]   I'll let him explain.
[00:00:17.000 --> 00:00:19.000]   We'll also talk about TikTok.
[00:00:19.000 --> 00:00:25.000]   And why that little bug where it's snooping your clipboard actually should be blamed on Google.
[00:00:25.000 --> 00:00:31.000]   We'll also talk about Amazon wanting to rule the world. Is that okay?
[00:00:31.000 --> 00:00:36.000]   And Quibi. Is there any hope? It's almost over, I think.
[00:00:36.000 --> 00:00:38.000]   All coming up next on Twit.
[00:00:38.000 --> 00:00:41.000]   This week in Tech comes to you from Twit's LastPass Studios.
[00:00:41.000 --> 00:00:47.000]   You're focused on security, but are your employees LastPass can ensure they are
[00:00:47.000 --> 00:00:49.000]   by making access and authentication seamless.
[00:00:49.000 --> 00:00:54.000]   Whether they're working in the office or remotely, visit lastpass.com/twit
[00:00:54.000 --> 00:00:57.000]   to learn more.
[00:00:57.000 --> 00:00:59.000]   Podcasts you love.
[00:00:59.000 --> 00:01:01.000]   From people you trust.
[00:01:01.000 --> 00:01:04.000]   This is Twit.
[00:01:04.000 --> 00:01:13.000]   This is Twit. This week in Tech, Episode 779.
[00:01:13.000 --> 00:01:16.000]   Recorded Sunday July 12, 2020.
[00:01:16.000 --> 00:01:18.000]   No future in green pants.
[00:01:18.000 --> 00:01:22.000]   This episode of This Week in Tech is brought to you by Casper.
[00:01:22.000 --> 00:01:28.000]   Casper is a sleep brand that makes expertly designed products to help you get your best rest
[00:01:28.000 --> 00:01:30.000]   one night at the time.
[00:01:30.000 --> 00:01:34.000]   Get $100 towards select mattresses by visiting casper.com/twit1
[00:01:34.000 --> 00:01:38.000]   and using the promo code Twit1 at checkout.
[00:01:38.000 --> 00:01:41.000]   And by World Wide Technology.
[00:01:41.000 --> 00:01:46.000]   WWT's Advanced Technology Center is like no other testing and research lab
[00:01:46.000 --> 00:01:49.000]   with more than half a billion dollars of equipment,
[00:01:49.000 --> 00:01:52.000]   including solutions from key partners like Intel Corporation
[00:01:52.000 --> 00:01:55.000]   and its virtual so you can access it 24/7.
[00:01:55.000 --> 00:02:01.000]   To learn more and get insights into wallet offers, go to www.twit.com/twit.
[00:02:01.000 --> 00:02:04.000]   And by Molecule.
[00:02:04.000 --> 00:02:09.000]   Molecule is reimagining the future of clean air starting with the air purifier.
[00:02:09.000 --> 00:02:15.000]   It's not just an innovation on existing technology, but a scientific breakthrough in air purification.
[00:02:15.000 --> 00:02:19.000]   For 10% off your first purifier order, visit Molecule.com
[00:02:19.000 --> 00:02:21.000]   and enter the code Twit10.
[00:02:21.000 --> 00:02:24.000]   And by Mint Mobile.
[00:02:24.000 --> 00:02:27.000]   Mint Mobile provides the same premium network coverage you're used to,
[00:02:27.000 --> 00:02:30.000]   but at a fraction of the cost because everything is online.
[00:02:30.000 --> 00:02:35.000]   Mint Mobile makes it easy to cut your wireless bill down to $15 a month
[00:02:35.000 --> 00:02:37.000]   with their three-month introductory plan.
[00:02:37.000 --> 00:02:43.000]   You'll also get the plan shipped to your door free at Mint Mobile.com/twit.
[00:02:43.000 --> 00:02:45.000]   It's time for Twit.
[00:02:45.000 --> 00:02:48.000]   This week at Tech, the show where we cover the latest Tech News
[00:02:48.000 --> 00:02:51.000]   with three of the best people I could find on short enough.
[00:02:51.000 --> 00:02:52.000]   It's not true.
[00:02:52.000 --> 00:02:54.000]   Three of the best people in the business.
[00:02:54.000 --> 00:02:57.000]   I am really thrilled to get Louise Mizzakis back.
[00:02:57.000 --> 00:03:01.000]   She is, of course, a senior writer at Wired, staff writer.
[00:03:01.000 --> 00:03:02.000]   I just promoted you.
[00:03:02.000 --> 00:03:03.000]   You're senior.
[00:03:03.000 --> 00:03:04.000]   Thank you. I appreciate that.
[00:03:04.000 --> 00:03:05.000]   It was like great.
[00:03:05.000 --> 00:03:07.000]   You're in charge of Wired.
[00:03:07.000 --> 00:03:09.000]   Wired, it's great to have you on.
[00:03:09.000 --> 00:03:13.000]   Actually, we have some topics that you are an expert in coming up.
[00:03:13.000 --> 00:03:14.000]   Can't wait.
[00:03:14.000 --> 00:03:15.000]   Yeah.
[00:03:15.000 --> 00:03:19.000]   We're going to -- Louise has been writing a lot about the latest stuff.
[00:03:19.000 --> 00:03:26.000]   I also want to welcome Stacey Higginbotham, who is normally relegated to this week in Google.
[00:03:26.000 --> 00:03:28.000]   Relegated?
[00:03:28.000 --> 00:03:30.000]   I think I elevate everything.
[00:03:30.000 --> 00:03:31.000]   Yeah.
[00:03:31.000 --> 00:03:33.000]   No, we are relegating, but you are elevating.
[00:03:33.000 --> 00:03:35.000]   So they go together.
[00:03:35.000 --> 00:03:41.000]   She is also an expert in many things, but primarily IOT.
[00:03:41.000 --> 00:03:45.000]   That's the name of her podcast and website, Stacey.ioT.com.
[00:03:45.000 --> 00:03:50.000]   The podcast is the IOT podcast that she does with a great Kevin Tofel.
[00:03:50.000 --> 00:03:53.000]   And you were in all about Android this week, so you've been very busy.
[00:03:53.000 --> 00:03:57.000]   Thank you for being part of our network this week.
[00:03:57.000 --> 00:03:58.000]   I did lie to you.
[00:03:58.000 --> 00:04:00.000]   I was on Tech News today.
[00:04:00.000 --> 00:04:01.000]   Tech News today.
[00:04:01.000 --> 00:04:02.000]   That's close.
[00:04:02.000 --> 00:04:04.000]   It's Jason Howell.
[00:04:04.000 --> 00:04:05.000]   Yeah, it's the same guy.
[00:04:05.000 --> 00:04:09.000]   It's the tall guy and the short guy together.
[00:04:09.000 --> 00:04:10.000]   Jason and Micah.
[00:04:10.000 --> 00:04:14.440]   And I want to welcome an old friend, a guy I've known for many, many years, back when he
[00:04:14.440 --> 00:04:15.880]   was CEO at Evernote.
[00:04:15.880 --> 00:04:16.880]   We became buddies.
[00:04:16.880 --> 00:04:18.880]   He's been on the show many times.
[00:04:18.880 --> 00:04:26.000]   He is now in charge of a group called All Turtles, which is kind of an AI think tank
[00:04:26.000 --> 00:04:30.120]   startup at AllTurtlesall-turtles.com.
[00:04:30.120 --> 00:04:31.120]   He's the CEO there.
[00:04:31.120 --> 00:04:33.000]   Phil Liben, it's great to have you on.
[00:04:33.000 --> 00:04:35.200]   It's super fun to be back with.
[00:04:35.200 --> 00:04:37.240]   Phil actually has a new product we're going to play with.
[00:04:37.240 --> 00:04:43.360]   In fact, you may notice everyone's in a while weird things happen with Phil.
[00:04:43.360 --> 00:04:45.400]   It's no weirder than usual.
[00:04:45.400 --> 00:04:49.080]   It's called, uh-huh.
[00:04:49.080 --> 00:04:54.120]   Which is really the culmination of many years of a startup naming.
[00:04:54.120 --> 00:04:57.400]   We've slowly eliminated all the vowels.
[00:04:57.400 --> 00:05:02.560]   And now it's just two M's, an H, and two more M's.
[00:05:02.560 --> 00:05:04.880]   Mm-hmm.
[00:05:04.880 --> 00:05:07.880]   And are you using Mm-hmm right now, Phil?
[00:05:07.880 --> 00:05:08.880]   He is.
[00:05:08.880 --> 00:05:09.880]   Look at that.
[00:05:09.880 --> 00:05:10.880]   Yeah, I know.
[00:05:10.880 --> 00:05:11.880]   Yep.
[00:05:11.880 --> 00:05:12.880]   I played with it.
[00:05:12.880 --> 00:05:15.080]   I was lucky enough to get an invite at, well, I think it might have something to do with
[00:05:15.080 --> 00:05:18.160]   knowing you, but I got to play with it last night.
[00:05:18.160 --> 00:05:19.160]   It's so much fun.
[00:05:19.160 --> 00:05:23.080]   It's a virtual backdrop, but it also gives you over the shoulder graphics.
[00:05:23.080 --> 00:05:28.680]   You can also do weird things to yourself, like make yourself translucent and make yourself
[00:05:28.680 --> 00:05:30.160]   smaller.
[00:05:30.160 --> 00:05:34.440]   So you're actually not in a blue room.
[00:05:34.440 --> 00:05:35.440]   I am not.
[00:05:35.440 --> 00:05:39.240]   I am in my, uh, stuck in my apartment in, in the mission in San Francisco.
[00:05:39.240 --> 00:05:40.720]   As many of us are these days.
[00:05:40.720 --> 00:05:44.400]   Well, not in your apartment, but our various abodes.
[00:05:44.400 --> 00:05:51.280]   And it's really nice to be able to get on a zoom call and not be in your normal environment.
[00:05:51.280 --> 00:05:52.720]   I love some of the environments you have.
[00:05:52.720 --> 00:05:54.160]   There's a brick wall.
[00:05:54.160 --> 00:05:55.560]   There's kind of an industrial space.
[00:05:55.560 --> 00:05:56.560]   There's bookshelves.
[00:05:56.560 --> 00:05:58.920]   Um, there's some really fun backgrounds.
[00:05:58.920 --> 00:06:01.520]   And there's also just plain backgrounds like that.
[00:06:01.520 --> 00:06:02.520]   I like this.
[00:06:02.520 --> 00:06:07.400]   This is like a, a newscaster, daily show kind of effect, the little side.
[00:06:07.400 --> 00:06:08.400]   Exactly.
[00:06:08.400 --> 00:06:09.400]   Whatever.
[00:06:09.400 --> 00:06:10.400]   What is that?
[00:06:10.400 --> 00:06:11.640]   Is there a word for that?
[00:06:11.640 --> 00:06:14.480]   Over the shoulder graphics is what we call so fancy.
[00:06:14.480 --> 00:06:15.640]   I knew you'd have it.
[00:06:15.640 --> 00:06:16.640]   Yeah.
[00:06:16.640 --> 00:06:17.640]   And that's exactly.
[00:06:17.640 --> 00:06:18.640]   And I think that was your intent, right?
[00:06:18.640 --> 00:06:23.720]   Fills to kind of give, give people the ability to do, uh, kind of newsy style presentations.
[00:06:23.720 --> 00:06:28.360]   You can even have slideshows and other things in that over the shoulder graphic, which is
[00:06:28.360 --> 00:06:32.000]   kind of the inspiration was very much instant weekend update.
[00:06:32.000 --> 00:06:34.040]   That's how we pitched it.
[00:06:34.040 --> 00:06:36.040]   All right.
[00:06:36.040 --> 00:06:38.720]   So I'm not good.
[00:06:38.720 --> 00:06:42.400]   And I don't have a good enough graphics department to be funny and talk about it.
[00:06:42.400 --> 00:06:43.400]   You're already there.
[00:06:43.400 --> 00:06:45.120]   Practically, you've got the backdrop.
[00:06:45.120 --> 00:06:47.440]   For all we know, this is not Stacy's new home.
[00:06:47.440 --> 00:06:52.240]   This could be just anywhere with a special backdrop put in there by Wednesday.
[00:06:52.240 --> 00:06:54.480]   I will have my actual shelves delivered.
[00:06:54.480 --> 00:06:55.480]   That's okay.
[00:06:55.480 --> 00:06:56.480]   That's okay.
[00:06:56.480 --> 00:06:57.480]   I'm not.
[00:06:57.480 --> 00:06:58.480]   Yeah.
[00:06:58.480 --> 00:06:59.480]   Isn't that the truth?
[00:06:59.480 --> 00:07:04.800]   Stacy just moved and is, uh, is slowly upgrading her, uh, her, uh, place.
[00:07:04.800 --> 00:07:08.640]   Let's, uh, talk TikTok because that's one of the stories.
[00:07:08.640 --> 00:07:12.760]   Louise, you've been covering and it's a very interesting story.
[00:07:12.760 --> 00:07:16.760]   This week, TikTok, we, a number of things happened.
[00:07:16.760 --> 00:07:20.360]   One because of iOS, it was a big week for TikTok.
[00:07:20.360 --> 00:07:26.920]   The first story was when iOS 14 became more widespread in its public beta.
[00:07:26.920 --> 00:07:33.520]   People started noticing that TikTok was snooping on their clipboard and the fear was that
[00:07:33.520 --> 00:07:37.000]   was actually trying to see passwords and things like that.
[00:07:37.000 --> 00:07:38.720]   Yeah, exactly.
[00:07:38.720 --> 00:07:43.200]   So that was the fear and TikTok actually put out this really long technical post explaining
[00:07:43.200 --> 00:07:46.520]   that the reason for that is that it was an anti spam measure.
[00:07:46.520 --> 00:07:50.400]   Um, you know, according to them, they are not storing that clipboard data and they're
[00:07:50.400 --> 00:07:56.200]   basically just looking to see, are you copying and pasting the same comment, basically over
[00:07:56.200 --> 00:08:00.080]   and over again onto a video or are you just like, you know, putting that comment on every
[00:08:00.080 --> 00:08:02.760]   single video you see, et cetera.
[00:08:02.760 --> 00:08:08.240]   But I think because of all of the other stuff going on with TikTok, this particular issue
[00:08:08.240 --> 00:08:12.720]   kind of, uh, raised a lot of fears for people and they shut it off basically immediately.
[00:08:12.720 --> 00:08:16.320]   I think the companies doing this have got to get together on their stories though because
[00:08:16.320 --> 00:08:24.040]   LinkedIn also got busted and their excuse was what they had a different reason.
[00:08:24.040 --> 00:08:26.840]   It was due to a software bug.
[00:08:26.840 --> 00:08:31.800]   Um, but they said the same thing, but we don't, we don't read it.
[00:08:31.800 --> 00:08:32.800]   We don't store it.
[00:08:32.800 --> 00:08:33.800]   We don't save it.
[00:08:33.800 --> 00:08:39.080]   LinkedIn is now, uh, the target of a class action lawsuit, uh, from people.
[00:08:39.080 --> 00:08:46.960]   There were a total, I think of 56 applications caught doing this, but honestly, now I could
[00:08:46.960 --> 00:08:50.840]   see to, I understand why people would assume the worst with TikTok.
[00:08:50.840 --> 00:08:52.520]   It's owned by a Chinese company.
[00:08:52.520 --> 00:08:55.400]   There's always been a lot of concern that it might be a spy tool that we're going to
[00:08:55.400 --> 00:08:58.880]   get to some more stories from other people who are worried about that.
[00:08:58.880 --> 00:09:01.240]   But LinkedIn is a Microsoft company.
[00:09:01.240 --> 00:09:07.520]   I don't think it seems likely that LinkedIn would take the reputational risk of doing
[00:09:07.520 --> 00:09:10.400]   this if they were, they get caught.
[00:09:10.400 --> 00:09:14.200]   Phil, you've overseen a lot of software development.
[00:09:14.200 --> 00:09:18.520]   How is it that this particular bug could get into at least 56 applications?
[00:09:18.520 --> 00:09:22.040]   I'm going to guess it's many, many more that we're going to discover that are doing this.
[00:09:22.040 --> 00:09:23.040]   Yeah.
[00:09:23.040 --> 00:09:27.320]   And it's, it's, you know, it's notable that all of these bugs that are, you know, super
[00:09:27.320 --> 00:09:32.280]   creepy, potentially privacy-vity bugs, they, you know, they'll go in that direction, right?
[00:09:32.280 --> 00:09:37.720]   You rarely hear a bug where companies like, Oh, due to a bug, we're not snooping on you.
[00:09:37.720 --> 00:09:39.680]   It's a good point.
[00:09:39.680 --> 00:09:40.680]   Yeah.
[00:09:40.680 --> 00:09:43.720]   You know, we wanted to snoop on you, but we don't seem to be able to.
[00:09:43.720 --> 00:09:44.720]   Yeah, it's a bug.
[00:09:44.720 --> 00:09:45.720]   Yeah.
[00:09:45.720 --> 00:09:46.720]   We just can't do it.
[00:09:46.720 --> 00:09:50.080]   You know, I was on LinkedIn literally today.
[00:09:50.080 --> 00:09:59.360]   And I saw this ad, which, Phil, explore irrelevant opportunities with urban salt, the egg.
[00:09:59.360 --> 00:10:00.360]   Yeah.
[00:10:00.360 --> 00:10:01.360]   What is that?
[00:10:01.360 --> 00:10:02.360]   Right.
[00:10:02.360 --> 00:10:07.360]   So now I'm not looking for a job right now, but this is my favorite brand, urban salted
[00:10:07.360 --> 00:10:12.600]   egg is my favorite brand of salted, eggy fish skins.
[00:10:12.600 --> 00:10:14.080]   And I literally like ate a crate of them.
[00:10:14.080 --> 00:10:15.720]   Last time I was in Singapore, it's a real thing.
[00:10:15.720 --> 00:10:17.120]   So how did they know that?
[00:10:17.120 --> 00:10:18.120]   Well, exactly.
[00:10:18.120 --> 00:10:21.000]   So probably they bought my credit card data because I've never said anything about it.
[00:10:21.000 --> 00:10:22.000]   Oh, interesting.
[00:10:22.000 --> 00:10:24.760]   But they probably bought, you know, I probably bought some with a credit card in Singapore,
[00:10:24.760 --> 00:10:28.680]   and they probably bought my credit card data and use it to serve up this ad.
[00:10:28.680 --> 00:10:34.160]   Now, look, if my company were to get acquired, I hope it gets acquired by urban salt, eggy.
[00:10:34.160 --> 00:10:40.120]   So maybe I would explore opportunities there, but it's creepy, right?
[00:10:40.120 --> 00:10:48.360]   So I'm 99% confident that the cut and paste bug really was a bug.
[00:10:48.360 --> 00:10:51.720]   But when you put it in this environment with all this other creepy stuff that they're doing
[00:10:51.720 --> 00:10:55.000]   intentionally, it's hard to say for sure.
[00:10:55.000 --> 00:10:59.640]   And I think it erodes trust and confidence in these companies, even when they're innocent
[00:10:59.640 --> 00:11:02.400]   of any sort of intentional malfeasance.
[00:11:02.400 --> 00:11:03.400]   Yeah.
[00:11:03.400 --> 00:11:04.400]   Those things.
[00:11:04.400 --> 00:11:05.400]   Really?
[00:11:05.400 --> 00:11:06.400]   But they're actually potato chips.
[00:11:06.400 --> 00:11:07.400]   No, no, no.
[00:11:07.400 --> 00:11:09.920]   So potato chips are for the amateur American market.
[00:11:09.920 --> 00:11:11.280]   You need the fish skins.
[00:11:11.280 --> 00:11:12.280]   Scroll down.
[00:11:12.280 --> 00:11:14.200]   I have many questions.
[00:11:14.200 --> 00:11:17.800]   Honestly, I just want to bag the taste.
[00:11:17.800 --> 00:11:18.800]   They're salty.
[00:11:18.800 --> 00:11:19.800]   Where are the eggs?
[00:11:19.800 --> 00:11:20.800]   Where's the egg?
[00:11:20.800 --> 00:11:21.800]   It's a good egg batter.
[00:11:21.800 --> 00:11:22.800]   Oh, I see.
[00:11:22.800 --> 00:11:23.800]   Okay.
[00:11:23.800 --> 00:11:25.280]   The potato chips are lame.
[00:11:25.280 --> 00:11:26.280]   The fish skins.
[00:11:26.280 --> 00:11:27.760]   You want the fried fish skins.
[00:11:27.760 --> 00:11:33.880]   I was ready for like, what are those eggs called that are banned in the US, those chocolate
[00:11:33.880 --> 00:11:35.680]   eggs, the UK.
[00:11:35.680 --> 00:11:39.880]   I was ready for like, the Cadbury, the Cadbury cream eggs.
[00:11:39.880 --> 00:11:40.880]   Yeah, exactly.
[00:11:40.880 --> 00:11:43.760]   We can't have them in the US because they're choking hazard because they have like little
[00:11:43.760 --> 00:11:44.760]   toys.
[00:11:44.760 --> 00:11:45.760]   It's the green one.
[00:11:45.760 --> 00:11:46.760]   Sorry.
[00:11:46.760 --> 00:11:47.760]   Orange and white.
[00:11:47.760 --> 00:11:48.760]   What are they?
[00:11:48.760 --> 00:11:49.760]   I used to bring them to my kid.
[00:11:49.760 --> 00:11:50.760]   Cadbury.
[00:11:50.760 --> 00:11:51.760]   Cadbury.
[00:11:51.760 --> 00:11:52.760]   Cadbury.
[00:11:52.760 --> 00:11:57.080]   Oh, Lord, because they have toys in them.
[00:11:57.080 --> 00:11:59.840]   And of course, that does sound kind of dangerous.
[00:11:59.840 --> 00:12:04.960]   That's a nice, small toys, but most kids, I feel like you're smart enough not to eat
[00:12:04.960 --> 00:12:05.960]   the Kinder Egg.
[00:12:05.960 --> 00:12:08.480]   If you knew the Kinder Egg had a toy in it.
[00:12:08.480 --> 00:12:09.480]   Right.
[00:12:09.480 --> 00:12:14.640]   Look, you know how like in the US, you buy like a pack of Q-tips for no mind tweeted about
[00:12:14.640 --> 00:12:15.640]   this.
[00:12:15.640 --> 00:12:16.640]   It says don't eat them.
[00:12:16.640 --> 00:12:19.400]   Yeah, it says like, don't put them in your ear for the love of God to not put these anywhere
[00:12:19.400 --> 00:12:20.400]   near you here.
[00:12:20.400 --> 00:12:21.400]   Right?
[00:12:21.400 --> 00:12:23.160]   But that's what you do with Q-tips.
[00:12:23.160 --> 00:12:28.040]   But you go to any other country and like Q-tip packaging is like photos of people blissfully
[00:12:28.040 --> 00:12:29.480]   putting them in their ears.
[00:12:29.480 --> 00:12:30.480]   Yeah.
[00:12:30.480 --> 00:12:33.320]   Because that's obviously what you're supposed to do with them, except like other countries
[00:12:33.320 --> 00:12:36.680]   are like, well, you know, when our citizens put stuff in their ear holes and it starts
[00:12:36.680 --> 00:12:38.680]   to hurt, they don't push it any further.
[00:12:38.680 --> 00:12:40.600]   But Americans can't trust it to do that.
[00:12:40.600 --> 00:12:44.720]   I think it more it has to do with class action lawsuits.
[00:12:44.720 --> 00:12:45.720]   Yeah, quite likely.
[00:12:45.720 --> 00:12:46.720]   Yeah.
[00:12:46.720 --> 00:12:47.720]   Yeah.
[00:12:47.720 --> 00:12:49.680]   Lawyers are saying, look, if you say on the box, don't put it in your ear and they put
[00:12:49.680 --> 00:12:52.320]   it in their ear, we're off the hook.
[00:12:52.320 --> 00:12:53.320]   It's on them.
[00:12:53.320 --> 00:12:55.320]   Oh my God.
[00:12:55.320 --> 00:12:59.840]   I was going to ask about is it the bugs, all of this?
[00:12:59.840 --> 00:13:00.840]   Is it both?
[00:13:00.840 --> 00:13:02.880]   Is it a software reuse thing?
[00:13:02.880 --> 00:13:03.880]   I mean, that's what my...
[00:13:03.880 --> 00:13:04.880]   Yes.
[00:13:04.880 --> 00:13:06.880]   That's exactly what it is.
[00:13:06.880 --> 00:13:07.880]   Great.
[00:13:07.880 --> 00:13:08.880]   Mine is.
[00:13:08.880 --> 00:13:09.880]   That's what I suspected.
[00:13:09.880 --> 00:13:10.880]   Okay.
[00:13:10.880 --> 00:13:11.880]   Yeah.
[00:13:11.880 --> 00:13:14.120]   It's a Google SDK feature that TikTok had used.
[00:13:14.120 --> 00:13:15.120]   There you go.
[00:13:15.120 --> 00:13:16.120]   Oh, yeah.
[00:13:16.120 --> 00:13:18.800]   So yeah, that's exactly what I really like encourage.
[00:13:18.800 --> 00:13:21.800]   If you want to know more about this, I thought the TikTok posts, like, you know, obviously
[00:13:21.800 --> 00:13:23.480]   they're trying to exonerate themselves.
[00:13:23.480 --> 00:13:27.560]   But I thought that the technical explanation of like how this got into the app and what
[00:13:27.560 --> 00:13:31.280]   they were using it for like was pretty clear.
[00:13:31.280 --> 00:13:32.280]   And like made a lot of sense.
[00:13:32.280 --> 00:13:35.600]   When you hear like copying clipboard, it's really scary.
[00:13:35.600 --> 00:13:39.400]   And I think this is why I think you won.
[00:13:39.400 --> 00:13:43.160]   You can't expect your engineers to be like, they're solving a problem, right?
[00:13:43.160 --> 00:13:44.320]   They're like, oh, I need to do this.
[00:13:44.320 --> 00:13:46.520]   Here's the easiest, fastest way to do this.
[00:13:46.520 --> 00:13:49.160]   You need to have someone whose job it is.
[00:13:49.160 --> 00:13:53.960]   Sure they should think about the other thing, but someone needs to come in and say, hey,
[00:13:53.960 --> 00:13:56.040]   what are all the ramifications of this?
[00:13:56.040 --> 00:13:57.280]   What is the worst that could happen?
[00:13:57.280 --> 00:14:01.600]   Because a lot of companies, I don't think they think that way because they're optimistic.
[00:14:01.600 --> 00:14:04.800]   They're Silicon Valley tech companies.
[00:14:04.800 --> 00:14:06.720]   Half of them are like trying to do good and change the world.
[00:14:06.720 --> 00:14:10.640]   The other half are probably a little bit more nefarious and, you know, working around
[00:14:10.640 --> 00:14:11.640]   it.
[00:14:11.640 --> 00:14:17.160]   But I do think companies really, given the heightened scrutiny and the freak out and the
[00:14:17.160 --> 00:14:23.560]   prominence in our lives really need to start having the people who are worried about class
[00:14:23.560 --> 00:14:24.560]   action lawsuits.
[00:14:24.560 --> 00:14:26.800]   I don't want it to go that far, but that's what they need.
[00:14:26.800 --> 00:14:30.360]   Louise, what is the intent of that Google SDK?
[00:14:30.360 --> 00:14:31.360]   What is it used for?
[00:14:31.360 --> 00:14:33.600]   I'm not exactly sure.
[00:14:33.600 --> 00:14:36.760]   I can pull up the post and see.
[00:14:36.760 --> 00:14:39.720]   I like looked at it yesterday, but we can look more at it.
[00:14:39.720 --> 00:14:42.440]   TikTok, call it out.
[00:14:42.440 --> 00:14:43.440]   I think they did.
[00:14:43.440 --> 00:14:45.280]   Yeah, they specifically called it out.
[00:14:45.280 --> 00:14:49.160]   It's funny because both Stacy and I had the same thought, which is actually, I didn't
[00:14:49.160 --> 00:14:50.680]   think it was like a Google SDK.
[00:14:50.680 --> 00:14:56.000]   I thought it was something on, you know, stack exchange that the developers said, how do
[00:14:56.000 --> 00:14:59.880]   you do this and found some code, which happens all the time?
[00:14:59.880 --> 00:15:01.040]   Why rewrite it?
[00:15:01.040 --> 00:15:02.400]   And they copied it and pasted it.
[00:15:02.400 --> 00:15:06.960]   And it just turned out 56 apps copy and pasted the same code with the same.
[00:15:06.960 --> 00:15:12.400]   It's not a bug, the same odd feature that was snooping on the clipboard.
[00:15:12.400 --> 00:15:13.960]   There is good reason to snoop on the clipboard.
[00:15:13.960 --> 00:15:18.920]   A lot of apps do that intentionally because you might leave passwords there, Bitcoin wallet
[00:15:18.920 --> 00:15:19.920]   numbers there.
[00:15:19.920 --> 00:15:23.960]   There's all sorts of stuff that is very likely on their clipboard effect.
[00:15:23.960 --> 00:15:28.920]   Our sponsor LastPass and most other password managers clear the clipboard pretty regularly
[00:15:28.920 --> 00:15:29.920]   for that very reason.
[00:15:29.920 --> 00:15:31.920]   You don't want to leave passwords on the clipboard.
[00:15:31.920 --> 00:15:39.080]   Yeah, so I just looked at the post and it says by the integration of Google ads SDK.
[00:15:39.080 --> 00:15:41.080]   It's an add SDK.
[00:15:41.080 --> 00:15:42.080]   Yeah.
[00:15:42.080 --> 00:15:43.080]   Yeah.
[00:15:43.080 --> 00:15:44.080]   So, okay.
[00:15:44.080 --> 00:15:45.080]   So there you go, right?
[00:15:45.080 --> 00:15:46.080]   That proves it.
[00:15:46.080 --> 00:15:48.080]   That's how they knew about the egg chips.
[00:15:48.080 --> 00:15:50.840]   So that's actually a little nefarious.
[00:15:50.840 --> 00:15:51.840]   Yes.
[00:15:51.840 --> 00:15:52.840]   I mean, they're.
[00:15:52.840 --> 00:15:54.920]   It's a lot nefarious, right?
[00:15:54.920 --> 00:15:58.120]   Like the the outrage here shouldn't be about the particular bug.
[00:15:58.120 --> 00:16:02.480]   But yeah, this bug was innocent in the sense probably in the sense that they probably
[00:16:02.480 --> 00:16:07.080]   weren't trying to spy on your clip on your pageboard and do anything nefarious with it.
[00:16:07.080 --> 00:16:10.440]   But the problem is all of these things happen because of ads.
[00:16:10.440 --> 00:16:11.440]   Right.
[00:16:11.440 --> 00:16:12.440]   Because of targeted ads.
[00:16:12.440 --> 00:16:13.440]   It's that tech.
[00:16:13.440 --> 00:16:14.440]   Yeah.
[00:16:14.440 --> 00:16:16.160]   Creepy advertising is going to eat the world.
[00:16:16.160 --> 00:16:23.200]   So just to kind of dot the eye and cross the tee here, you were using TikTok.
[00:16:23.200 --> 00:16:29.440]   And for some reason on your clipboard was perhaps well, no, in this case, you didn't
[00:16:29.440 --> 00:16:32.280]   you didn't order anything online from urban salted egg.
[00:16:32.280 --> 00:16:36.120]   The only time only information they could have gotten was from a credit card.
[00:16:36.120 --> 00:16:39.640]   But per my guess from a credit card, it's possibly texted something about it.
[00:16:39.640 --> 00:16:43.280]   But I don't think so because I'm not actually going to publicly admit to any of my friends
[00:16:43.280 --> 00:16:45.280]   that I ate my way.
[00:16:45.280 --> 00:16:47.440]   This is one problem.
[00:16:47.440 --> 00:16:51.040]   I don't think I told anyone one problem with fasting is that when you get off the fast,
[00:16:51.040 --> 00:16:53.160]   you will eat anything and a lot of.
[00:16:53.160 --> 00:16:55.960]   And honestly, it's not that it's not that cute or friendly because it's like, I think
[00:16:55.960 --> 00:16:57.600]   there's quite a lot of sugar in that.
[00:16:57.600 --> 00:16:59.400]   I'm just delicious.
[00:16:59.400 --> 00:17:02.600]   Yeah, they must have gotten my credit card, which is which is creepy.
[00:17:02.600 --> 00:17:03.600]   Another place.
[00:17:03.600 --> 00:17:04.600]   Another place.
[00:17:04.600 --> 00:17:05.600]   They said, right.
[00:17:05.600 --> 00:17:07.600]   That is the previous data.
[00:17:07.600 --> 00:17:08.600]   Yeah.
[00:17:08.600 --> 00:17:13.000]   I think it's actually kind of funny and interesting that LinkedIn would even think that there
[00:17:13.000 --> 00:17:14.560]   would necessarily be a correlation.
[00:17:14.560 --> 00:17:17.600]   I mean, maybe there is for a lot of people, but like, I don't think necessarily most
[00:17:17.600 --> 00:17:23.120]   of the stores that I frequent with my credit card purchases or places that I want to work.
[00:17:23.120 --> 00:17:26.200]   But this is exactly what's wrong with this ad tech.
[00:17:26.200 --> 00:17:34.000]   And we all know this is these ad recommendations are always very hilariously inaccurate, right?
[00:17:34.000 --> 00:17:39.080]   So they made a mechanical connection between the fact that you bought a lot of urban salted
[00:17:39.080 --> 00:17:40.080]   eggs.
[00:17:40.080 --> 00:17:43.240]   They made that connect, which are dangerously addictive.
[00:17:43.240 --> 00:17:45.760]   We agree, but they made that connection.
[00:17:45.760 --> 00:17:50.360]   And because LinkedIn is about job hunting, they said they put it's still an ad, but they
[00:17:50.360 --> 00:17:53.920]   put that two and two together and got eight.
[00:17:53.920 --> 00:17:57.320]   It's just a weird, but that's what happens all the time.
[00:17:57.320 --> 00:18:00.880]   You know, you know, what else is dangerously addictive is a creepy advertising.
[00:18:00.880 --> 00:18:01.880]   Yeah, no kidding.
[00:18:01.880 --> 00:18:05.520]   Lazy business models enabled by it, which are ultimately bad for the world.
[00:18:05.520 --> 00:18:11.240]   So just as Google used to perhaps sniff through your Gmail, looking for keywords so that they
[00:18:11.240 --> 00:18:17.800]   could advertise to you, they stopped doing that, but now, apparently they have an SDK
[00:18:17.800 --> 00:18:20.600]   and it's worse, it's it snoops on your clipboard.
[00:18:20.600 --> 00:18:25.640]   And then if a brand name goes by or they're not stealing your passwords, I'm sure your
[00:18:25.640 --> 00:18:30.600]   Bitcoin wall, but as as a brand name goes by or something like that, they'll make a note
[00:18:30.600 --> 00:18:33.400]   of that and they'll show you ads for that product.
[00:18:33.400 --> 00:18:38.880]   Yeah, Phil, to your exact point, I just was looking at the TikTok post and literally it
[00:18:38.880 --> 00:18:44.360]   says, the ubiquitous nature of third party ad programs helps explain why so many other
[00:18:44.360 --> 00:18:46.560]   apps indicated similar behavior.
[00:18:46.560 --> 00:18:48.760]   So it's like, yeah, exactly.
[00:18:48.760 --> 00:18:50.800]   All these apps were just caught doing advertising.
[00:18:50.800 --> 00:18:52.000]   They're caught.
[00:18:52.000 --> 00:18:53.000]   Yeah.
[00:18:53.000 --> 00:18:54.000]   Is it perfect?
[00:18:54.000 --> 00:18:56.920]   Do you think they're doing this knowingly?
[00:18:56.920 --> 00:19:01.520]   Like they know that Google's scraping the clipboard they must, right?
[00:19:01.520 --> 00:19:05.360]   Or no, maybe it's maybe it's part of being in the Google ad network that you get this
[00:19:05.360 --> 00:19:06.360]   code.
[00:19:06.360 --> 00:19:09.520]   Yeah, I don't think that it's like populating in a server or somewhere where they're like
[00:19:09.520 --> 00:19:15.080]   collecting all of it, you know, and then storing it, you know, indefinitely or anything like
[00:19:15.080 --> 00:19:16.080]   that.
[00:19:16.080 --> 00:19:20.000]   So it's potentially something that they're not particularly aware of actually until like,
[00:19:20.000 --> 00:19:22.080]   you know, this iOS notification came out.
[00:19:22.080 --> 00:19:24.640]   So two points to come out of this one.
[00:19:24.640 --> 00:19:25.640]   Thank you, Apple.
[00:19:25.640 --> 00:19:28.600]   Apple's doing more and more to surface this.
[00:19:28.600 --> 00:19:33.240]   Ironically, I think Apple was only doing it because their own I ads failed.
[00:19:33.240 --> 00:19:39.080]   So since they couldn't make money in ad tech, they said, don't look at it for everyone
[00:19:39.080 --> 00:19:40.080]   else.
[00:19:40.080 --> 00:19:41.080]   Yeah, it's a gift horse.
[00:19:41.080 --> 00:19:42.080]   Like, yeah.
[00:19:42.080 --> 00:19:43.080]   Here's why they're doing it.
[00:19:43.080 --> 00:19:44.080]   They're doing the right thing.
[00:19:44.080 --> 00:19:45.320]   Given a lemon, let's make some lemonade.
[00:19:45.320 --> 00:19:47.200]   But it is a gift horse.
[00:19:47.200 --> 00:19:49.720]   There's a lot of other things Apple's going to do in iOS 14.
[00:19:49.720 --> 00:19:55.680]   For instance, the permission for the Apple ad ID will pop up when you install an app.
[00:19:55.680 --> 00:19:58.880]   And you can at that point, you can turn it off now in the settings, but it's buried
[00:19:58.880 --> 00:19:59.880]   deep.
[00:19:59.880 --> 00:20:02.320]   But you can at that point every time you install an app, say, yeah, don't give that
[00:20:02.320 --> 00:20:04.440]   app any information.
[00:20:04.440 --> 00:20:09.880]   That's advertisers are terrified because that's going to be a huge loss of data.
[00:20:09.880 --> 00:20:10.880]   So you're right.
[00:20:10.880 --> 00:20:14.440]   This is a part of the war between ad tech and consumers.
[00:20:14.440 --> 00:20:15.840]   That's disappointing.
[00:20:15.840 --> 00:20:18.720]   All of the companies that were caught have said, oh, it's a bug.
[00:20:18.720 --> 00:20:21.240]   We'll fix it.
[00:20:21.240 --> 00:20:23.280]   In other words, we got caught.
[00:20:23.280 --> 00:20:25.240]   So we're going to fix it now.
[00:20:25.240 --> 00:20:27.400]   Kudos to Apple for surfacing it.
[00:20:27.400 --> 00:20:33.880]   And I suspect when iOS 14 gets widely in widespread use, it may be we find some other
[00:20:33.880 --> 00:20:35.640]   companies doing the same thing.
[00:20:35.640 --> 00:20:39.840]   So that was strike one against TikTok.
[00:20:39.840 --> 00:20:42.640]   I don't know if it's related, but a number of companies.
[00:20:42.640 --> 00:20:47.560]   Wells Fargo just said, you can't put TikTok on your company phone.
[00:20:47.560 --> 00:20:52.000]   Briefly, Amazon sent on a letter saying you can't use TikTok to Amazon employees.
[00:20:52.000 --> 00:20:53.520]   Then they said, no, that was a mistake.
[00:20:53.520 --> 00:20:55.520]   What was that about, Louise?
[00:20:55.520 --> 00:20:56.520]   Yeah.
[00:20:56.520 --> 00:21:01.240]   So my understanding is from like having talked to some sources in the last couple of days
[00:21:01.240 --> 00:21:09.000]   is that this was a decision that was made like not with the Amazon CSO's knowledge.
[00:21:09.000 --> 00:21:14.680]   So basically top C-suite security people didn't realize this was happening.
[00:21:14.680 --> 00:21:17.640]   So they basically would call it their pants down and this email went out.
[00:21:17.640 --> 00:21:22.280]   And it looked really dumb because Amazon is a major advertiser with TikTok.
[00:21:22.280 --> 00:21:28.120]   And they have been collaborating on a feature together that lets you shop directly on Amazon
[00:21:28.120 --> 00:21:29.600]   from TikTok.
[00:21:29.600 --> 00:21:32.680]   So their marketing people were really mad.
[00:21:32.680 --> 00:21:36.400]   I think basically immediately they got on calls with TikTok and they were like, hey, we're
[00:21:36.400 --> 00:21:38.600]   so sorry about this.
[00:21:38.600 --> 00:21:44.960]   They looked really bad, but I think it is indicative of kind of the wider concerns that people
[00:21:44.960 --> 00:21:45.960]   have about TikTok.
[00:21:45.960 --> 00:21:49.200]   But yeah, it was really embarrassing for Amazon honestly.
[00:21:49.200 --> 00:21:53.760]   It was obviously another nail in the coffin for TikTok potentially, but it was a very
[00:21:53.760 --> 00:22:00.800]   bizarre couple of hours on Friday for me as someone who primarily covers TikTok and Amazon.
[00:22:00.800 --> 00:22:04.680]   Together at last, it was a story made for Louise.
[00:22:04.680 --> 00:22:06.400]   I really felt that way.
[00:22:06.400 --> 00:22:09.840]   They were thinking of you when they sent out that letter.
[00:22:09.840 --> 00:22:17.200]   Mike Pompeo, the Secretary of State this week on Fox News said, we are thinking, we are
[00:22:17.200 --> 00:22:22.440]   thinking about banning TikTok, Chinese apps in general, but especially TikTok.
[00:22:22.440 --> 00:22:24.800]   Didn't say they're going to do it.
[00:22:24.800 --> 00:22:26.720]   They're thinking about it.
[00:22:26.720 --> 00:22:28.600]   India did.
[00:22:28.600 --> 00:22:33.520]   And there are a lot of Indian influencers who are very upset, a little lesson learned
[00:22:33.520 --> 00:22:34.520]   there.
[00:22:34.520 --> 00:22:35.680]   They're business.
[00:22:35.680 --> 00:22:38.680]   Some people over a couple of years making money on TikTok.
[00:22:38.680 --> 00:22:43.040]   Now they don't have that outlet anymore.
[00:22:43.040 --> 00:22:45.320]   Here's the question really.
[00:22:45.320 --> 00:22:47.800]   Is there anything to fear from TikTok?
[00:22:47.800 --> 00:22:49.880]   It's a Chinese company.
[00:22:49.880 --> 00:22:55.720]   They're trying to pretend they're not, for instance, in Hong Kong, they've withdrawn from
[00:22:55.720 --> 00:23:03.880]   Hong Kong because they're afraid that they would have to give information to China.
[00:23:03.880 --> 00:23:10.120]   There's stories going around that they're trying to get out of China somehow.
[00:23:10.120 --> 00:23:15.480]   Any company that operates in China, whether it's Huawei or TikTok, presumably is going
[00:23:15.480 --> 00:23:20.800]   to have to adhere to the laws of China, which would, just as in the United States, mean that
[00:23:20.800 --> 00:23:25.320]   they would give information to the Chinese authorities.
[00:23:25.320 --> 00:23:28.520]   Should we not be using, Louise, you use it?
[00:23:28.520 --> 00:23:29.960]   I do use TikTok.
[00:23:29.960 --> 00:23:32.200]   I have to for my job, honestly.
[00:23:32.200 --> 00:23:36.880]   I think that I'm not particularly more scared of TikTok than I am any other app that relies
[00:23:36.880 --> 00:23:39.240]   on advertising as a business model.
[00:23:39.240 --> 00:23:43.920]   I think that my threat model might be different if I were a US diplomat, if I were-
[00:23:43.920 --> 00:23:46.400]   Yeah, US diplomats shouldn't be using TikTok.
[00:23:46.400 --> 00:23:47.840]   I would bring you that.
[00:23:47.840 --> 00:23:48.840]   I like army personnel.
[00:23:48.840 --> 00:23:49.840]   Yeah.
[00:23:49.840 --> 00:23:50.840]   Or something like that.
[00:23:50.840 --> 00:23:54.760]   I don't think that there's been specific concrete security concerns that have been identified
[00:23:54.760 --> 00:23:55.760]   about this app.
[00:23:55.760 --> 00:24:01.480]   I think that anything that comes from US officials has to be viewed in the context of China-US
[00:24:01.480 --> 00:24:07.160]   relations, which I would very gently say are at an all-time low.
[00:24:07.160 --> 00:24:08.160]   Yeah.
[00:24:08.160 --> 00:24:10.320]   So I think it's important to keep that in mind.
[00:24:10.320 --> 00:24:15.120]   I think that a lot of people don't really understand ByteDance, which is the company that
[00:24:15.120 --> 00:24:18.080]   owns China, they own China.
[00:24:18.080 --> 00:24:24.680]   The company that owns TikTok and the role that it plays in China, it's kind of like the
[00:24:24.680 --> 00:24:26.520]   Facebook of China in a lot of ways.
[00:24:26.520 --> 00:24:30.280]   Very similarly to how Facebook pisses off US authorities.
[00:24:30.280 --> 00:24:36.220]   ByteDance has pissed off the CCP and gotten in trouble for things like letting what the
[00:24:36.220 --> 00:24:41.080]   CCP considers to be lewd content go viral.
[00:24:41.080 --> 00:24:46.040]   I think that it's best of you TikTok through the lens of capitalism and through the lens
[00:24:46.040 --> 00:24:52.920]   of another big tech giant that primarily makes its income through targeted advertising.
[00:24:52.920 --> 00:24:58.040]   It's incentivized to get you to spend as much time on the app as possible to make its features
[00:24:58.040 --> 00:25:00.520]   as addicting as possible.
[00:25:00.520 --> 00:25:04.920]   And that is the best way to think about the risks, I think, for the average person.
[00:25:04.920 --> 00:25:11.840]   But if you know, if anyone knows any specific security concerns related to its Chinese ownership,
[00:25:11.840 --> 00:25:12.840]   I'm open to that.
[00:25:12.840 --> 00:25:14.320]   But I don't think we've seen that.
[00:25:14.320 --> 00:25:16.160]   What's your signal account?
[00:25:16.160 --> 00:25:17.160]   Yes.
[00:25:17.160 --> 00:25:19.000]   Please DM me on Twitter.
[00:25:19.000 --> 00:25:21.880]   If you know anything about that, I'm open to it.
[00:25:21.880 --> 00:25:29.720]   I think it's naive to be dismissive, but I do get a large whiff of synophobia anytime
[00:25:29.720 --> 00:25:31.120]   that this topic comes up.
[00:25:31.120 --> 00:25:36.080]   I think a lot of the people shouting the most about it have the least knowledge about China.
[00:25:36.080 --> 00:25:37.880]   I think that that should be indicative.
[00:25:37.880 --> 00:25:39.480]   But that said, I'm open-minded.
[00:25:39.480 --> 00:25:46.880]   I'm not here to apologize for the CCP, but I think that they're an advertising company
[00:25:46.880 --> 00:25:50.880]   that wants as much data on you as possible to serve your advertisements.
[00:25:50.880 --> 00:25:57.120]   Well, and to be clear, now I think now we understand that the problem with these apps
[00:25:57.120 --> 00:26:02.320]   isn't what you put on these apps, the content you post, the videos kids post on TikTok.
[00:26:02.320 --> 00:26:05.880]   The problem is that it's on your phone and that because it's on your phone, it has access
[00:26:05.880 --> 00:26:11.200]   to all sorts of information about your location, what's on your clipboard, all sorts of things.
[00:26:11.200 --> 00:26:17.000]   And any, I think, honestly, Phil, you can tell me you're in this business.
[00:26:17.000 --> 00:26:20.600]   Any app you put on your phone is very likely collecting as much information as you'll let
[00:26:20.600 --> 00:26:21.600]   it.
[00:26:21.600 --> 00:26:28.040]   Well, I think that depends a lot on the design of the app.
[00:26:28.040 --> 00:26:34.760]   But isn't that one of the reasons you want to get on a phone?
[00:26:34.760 --> 00:26:39.800]   It's never motivated me or any of the stuff that we've built just because we have a philosophy
[00:26:39.800 --> 00:26:45.920]   that we're going to get data that is very clearly beneficial to the end users and we're
[00:26:45.920 --> 00:26:48.240]   going to explain why we're getting it.
[00:26:48.240 --> 00:26:51.320]   The philosophy here is it's called beneficent design.
[00:26:51.320 --> 00:26:56.000]   So the idea behind beneficent design is you make every single decision based solely on
[00:26:56.000 --> 00:26:59.080]   what's in the best interest of an informed end user.
[00:26:59.080 --> 00:27:03.400]   So you just say, well, if the user understood exactly what was happening, would they want
[00:27:03.400 --> 00:27:04.400]   to this?
[00:27:04.400 --> 00:27:05.400]   Would they be OK with it?
[00:27:05.400 --> 00:27:08.000]   And if the answer is ever in doubt, then you just don't do it.
[00:27:08.000 --> 00:27:13.480]   But yeah, a lot of-- and many companies operate that way, but none of the ad supported ones
[00:27:13.480 --> 00:27:14.480]   do.
[00:27:14.480 --> 00:27:19.920]   And I really do think it's an issue of the targeted ad supported business model, which
[00:27:19.920 --> 00:27:24.800]   puts a fundamental conflict of interest between what the customers want and where the money
[00:27:24.800 --> 00:27:28.640]   comes from, sorry, what the users want and where the money comes from.
[00:27:28.640 --> 00:27:32.040]   And when you have that, yeah, it's all sorts of bad behavior.
[00:27:32.040 --> 00:27:37.360]   So yeah, anything ad supported is-- I don't know, sketchy in my book.
[00:27:37.360 --> 00:27:40.600]   It's not unusual, though, for even a paid app to do some of this.
[00:27:40.600 --> 00:27:42.240]   No, of course not.
[00:27:42.240 --> 00:27:44.600]   But a lot of the stuff is like-- That's double dipping.
[00:27:44.600 --> 00:27:49.280]   You would want for the end user, right?
[00:27:49.280 --> 00:27:50.560]   There's legitimate functionality.
[00:27:50.560 --> 00:27:57.000]   So for example, if in iMessage, if you text-- if it's just in iMessage, text someone where
[00:27:57.000 --> 00:28:02.400]   you add, they will get an option in the order complete to just push a button to reply with
[00:28:02.400 --> 00:28:04.320]   their location.
[00:28:04.320 --> 00:28:05.320]   It's doing that.
[00:28:05.320 --> 00:28:08.600]   It obviously-- you have to enable that the first time.
[00:28:08.600 --> 00:28:13.360]   It's looking up iMessage on the phone who received it is looking at their location so
[00:28:13.360 --> 00:28:17.320]   that they can push the button in one tap reply to you where you are.
[00:28:17.320 --> 00:28:22.800]   That is a plausible thing that you can do, get your location in iMessage so that you can
[00:28:22.800 --> 00:28:25.600]   quickly communicate it, again, with an opt-in.
[00:28:25.600 --> 00:28:26.960]   So I think stuff like that is fine.
[00:28:26.960 --> 00:28:29.200]   Those are convenience features.
[00:28:29.200 --> 00:28:35.160]   But no one wants their information to be used for weird targeted advertising, and that
[00:28:35.160 --> 00:28:38.440]   becomes the root of a lot of the problems.
[00:28:38.440 --> 00:28:41.960]   Stacy, as I remember, you don't let your daughter do TikTok.
[00:28:41.960 --> 00:28:43.400]   She doesn't want to do TikTok.
[00:28:43.400 --> 00:28:45.400]   Good for her.
[00:28:45.400 --> 00:28:50.000]   Her golden life is to become a spy, and she wants to keep herself off the internet as
[00:28:50.000 --> 00:28:51.000]   long as possible.
[00:28:51.000 --> 00:28:52.000]   Is that true?
[00:28:52.000 --> 00:28:55.640]   I'm like, OK, sure.
[00:28:55.640 --> 00:28:57.240]   I have no problems with that.
[00:28:57.240 --> 00:29:03.480]   I do worry a lot, though, about like any time there's a move that sort of says, well, you
[00:29:03.480 --> 00:29:07.120]   shouldn't trust TikTok because it's a Chinese company, that kind of stuff.
[00:29:07.120 --> 00:29:09.800]   But radically, it turns me off.
[00:29:09.800 --> 00:29:13.280]   I think that's an extremely dangerous path to go down.
[00:29:13.280 --> 00:29:20.160]   We start basically excluding products and companies based on their national origin.
[00:29:20.160 --> 00:29:24.120]   That's just not going to end well, and it's not going to end well for American companies.
[00:29:24.120 --> 00:29:27.200]   Well, and it is a political thing going on in America right now.
[00:29:27.200 --> 00:29:28.200]   Go ahead, yes.
[00:29:28.200 --> 00:29:31.600]   Oh, I was going to say for 10 years, I've been hearing about Huawei because I was in
[00:29:31.600 --> 00:29:34.160]   the telecom industry as a reporter.
[00:29:34.160 --> 00:29:44.120]   And for 10 years, I have looked and yes, there was government ownership of Huawei.
[00:29:44.120 --> 00:29:48.960]   There are things that are questionable, but in 10 years of hunting, there has never been
[00:29:48.960 --> 00:29:50.680]   any proof.
[00:29:50.680 --> 00:29:53.520]   And every now and then, you just bring this up again and again.
[00:29:53.520 --> 00:29:59.520]   So to me, the whole issue with TikTok and by dance, it's similar.
[00:29:59.520 --> 00:30:04.640]   And I think we're going to have to get used to this because I think what we're seeing
[00:30:04.640 --> 00:30:11.680]   is lots of companies around the world, mostly China, they're creating products that people
[00:30:11.680 --> 00:30:13.400]   in the US want to consume.
[00:30:13.400 --> 00:30:15.400]   And that's actually good.
[00:30:15.400 --> 00:30:18.600]   That's, I mean, that's good for all of us.
[00:30:18.600 --> 00:30:21.200]   I am so torn because of course, first of all, I love China.
[00:30:21.200 --> 00:30:24.880]   I was a Chinese major college.
[00:30:24.880 --> 00:30:32.520]   So I don't like the jingoistic, vaguely racist idea of, oh, those people there, we don't
[00:30:32.520 --> 00:30:33.520]   like them.
[00:30:33.520 --> 00:30:35.680]   We don't want any of their products.
[00:30:35.680 --> 00:30:38.760]   But it is the case that China has economics.
[00:30:38.760 --> 00:30:43.320]   I don't think China has military designs on the world, but they definitely have economic
[00:30:43.320 --> 00:30:45.240]   designs on the world.
[00:30:45.240 --> 00:30:49.160]   They aren't a democratic country.
[00:30:49.160 --> 00:30:51.560]   They have a president for life.
[00:30:51.560 --> 00:30:56.960]   And it is certainly Huawei's behavior in other regards has sometimes been bad.
[00:30:56.960 --> 00:31:02.920]   I've heard lots of stories about them wandering the aisles at trade shows, trying to snarf
[00:31:02.920 --> 00:31:04.800]   up intellectual property.
[00:31:04.800 --> 00:31:06.000]   China does that.
[00:31:06.000 --> 00:31:07.920]   Many Chinese companies do that.
[00:31:07.920 --> 00:31:09.880]   Companies in the US do that.
[00:31:09.880 --> 00:31:10.880]   There's challenges.
[00:31:10.880 --> 00:31:11.880]   Yeah.
[00:31:11.880 --> 00:31:12.880]   Okay.
[00:31:12.880 --> 00:31:13.880]   Yeah.
[00:31:13.880 --> 00:31:14.880]   There's bad behavior on both sides.
[00:31:14.880 --> 00:31:16.520]   That's probably fair.
[00:31:16.520 --> 00:31:20.000]   So I don't, I want to rule out the jingoistic stuff at the same time.
[00:31:20.000 --> 00:31:25.840]   If you're putting Huawei gear and all of your 5G equipment and it's software to find radio
[00:31:25.840 --> 00:31:31.040]   equipment, so an update could completely change its behavior at any time.
[00:31:31.040 --> 00:31:36.040]   I don't, I'm not surprised that some governments would be concerned about that.
[00:31:36.040 --> 00:31:37.800]   So I don't know what the right answer is.
[00:31:37.800 --> 00:31:40.240]   Is that not something we should be concerned about?
[00:31:40.240 --> 00:31:41.240]   I mean, it is.
[00:31:41.240 --> 00:31:43.560]   So we should be concerned about that.
[00:31:43.560 --> 00:31:47.800]   But if we are concerned about that, we need to be concerned about that throughout our
[00:31:47.800 --> 00:31:53.080]   entire Silicon and software stack.
[00:31:53.080 --> 00:31:54.080]   Absolutely.
[00:31:54.080 --> 00:31:56.320]   On anything with, and we're not.
[00:31:56.320 --> 00:31:57.760]   And I don't think we can be.
[00:31:57.760 --> 00:32:00.320]   We don't have the economic incentives in place.
[00:32:00.320 --> 00:32:07.680]   So, and also, I would also say we need to think about that with regards to the phones
[00:32:07.680 --> 00:32:14.080]   that people who are our diplomats are, are leaders should maybe not be on Twitter.
[00:32:14.080 --> 00:32:16.800]   On actual phones that they use.
[00:32:16.800 --> 00:32:18.200]   I mean, then we have to be.
[00:32:18.200 --> 00:32:24.440]   That Android 7 phone is probably not the ideal tweet device.
[00:32:24.440 --> 00:32:29.320]   So I don't, it's a hard thing.
[00:32:29.320 --> 00:32:35.120]   And I'm not against us deciding that we're going to kick all of, you know, anything made
[00:32:35.120 --> 00:32:36.960]   in China that's strategic.
[00:32:36.960 --> 00:32:43.040]   Like if we decide a 5G network is strategic, which Trump certainly has, but we still send
[00:32:43.040 --> 00:32:47.640]   lots of data over 3G networks and, you know, those are.
[00:32:47.640 --> 00:32:49.120]   Good point.
[00:32:49.120 --> 00:32:50.120]   Good point.
[00:32:50.120 --> 00:32:51.120]   Yeah.
[00:32:51.120 --> 00:32:53.040]   And it does feel a little bit jingoistic.
[00:32:53.040 --> 00:32:57.180]   It doesn't, you know, it feels like it's more an arm of trade policy and anti-China
[00:32:57.180 --> 00:33:01.640]   sentiment than it does actual concern about security.
[00:33:01.640 --> 00:33:02.840]   I just feel it.
[00:33:02.840 --> 00:33:09.160]   Well, for instance, we know because of the economic climate, countries, companies that
[00:33:09.160 --> 00:33:13.440]   are now using China for manufacture, chiefly Apple are looking at other outlets.
[00:33:13.440 --> 00:33:20.280]   Foxconn is apparently looking at building an Indian factory to help Apple start to move
[00:33:20.280 --> 00:33:23.480]   iPhone production out of, perhaps out of China.
[00:33:23.480 --> 00:33:27.320]   I mean, one of the reasons you do that is because India has very steep tariffs against
[00:33:27.320 --> 00:33:30.280]   products not made in India.
[00:33:30.280 --> 00:33:33.440]   And same reason Foxconn has a Brazilian factory.
[00:33:33.440 --> 00:33:40.320]   But the most recent story implies that Apple is moving away from China.
[00:33:40.320 --> 00:33:42.360]   This comes from the times of India.
[00:33:42.360 --> 00:33:45.720]   So I don't know how credible it is.
[00:33:45.720 --> 00:33:50.560]   Well, and our national leadership at the moment is picking fights with the rest of the world.
[00:33:50.560 --> 00:33:51.560]   So it'd be true.
[00:33:51.560 --> 00:33:52.560]   And we do live in it.
[00:33:52.560 --> 00:33:53.560]   Yeah, be prudent.
[00:33:53.560 --> 00:33:54.560]   Yeah, we live in a global economy.
[00:33:54.560 --> 00:33:55.560]   Right.
[00:33:55.560 --> 00:33:57.920]   And when you're in a global economy, you can't just go around punching people in the face
[00:33:57.920 --> 00:34:00.520]   and calling them ugly and expect to work with them later.
[00:34:00.520 --> 00:34:01.520]   I'm serious.
[00:34:01.520 --> 00:34:02.520]   That's a good line.
[00:34:02.520 --> 00:34:03.360]   Yes.
[00:34:03.360 --> 00:34:04.360]   I agree.
[00:34:04.360 --> 00:34:05.360]   Yeah.
[00:34:05.360 --> 00:34:09.600]   And we know we can't make everything in the US.
[00:34:09.600 --> 00:34:13.280]   We might want to, but we don't have the ability yet to do that.
[00:34:13.280 --> 00:34:15.920]   And I don't think we'd want to.
[00:34:15.920 --> 00:34:17.280]   It's not economically viable.
[00:34:17.280 --> 00:34:20.760]   I mean, guns and butter people guns and butter.
[00:34:20.760 --> 00:34:24.800]   Anyway, I mean, you think that's impossible and impossible goal to say.
[00:34:24.800 --> 00:34:28.160]   Well, wouldn't it be great if we just did all our manufacture here in the US?
[00:34:28.160 --> 00:34:29.160]   We designed this.
[00:34:29.160 --> 00:34:32.040]   It's designed in California.
[00:34:32.040 --> 00:34:36.600]   I don't think it's impossible, but I don't think that's necessarily what we'd want to
[00:34:36.600 --> 00:34:37.600]   do.
[00:34:37.600 --> 00:34:39.560]   I mean, but this is the level of-
[00:34:39.560 --> 00:34:47.720]   But those countries have sources of expertise and materials and economic benefits.
[00:34:47.720 --> 00:34:49.840]   This is the conversation that we should be having.
[00:34:49.840 --> 00:34:54.000]   As opposed to these guys, you know, go ahead, Luis.
[00:34:54.000 --> 00:34:55.920]   I was just saying, like, keep an eye on the ball.
[00:34:55.920 --> 00:34:59.360]   Like I would love to see a national federal privacy law passed, right?
[00:34:59.360 --> 00:35:05.560]   Like, if we want to tackle these issues, I think we need to have robust regulation that
[00:35:05.560 --> 00:35:13.040]   is not just targeting companies from a specific country, but is deciding what kinds of privacy
[00:35:13.040 --> 00:35:15.360]   rights every American should be entitled to.
[00:35:15.360 --> 00:35:16.360]   Yeah.
[00:35:16.360 --> 00:35:17.360]   Yeah.
[00:35:17.360 --> 00:35:20.880]   Hear, hear, hear right on, Luis.
[00:35:20.880 --> 00:35:27.920]   I would support moving at least 50% of TikTok manufacturing to do.
[00:35:27.920 --> 00:35:32.640]   Well, Taylor Rowland's article in the New York Times says a lot of TikTokers in the US
[00:35:32.640 --> 00:35:34.200]   are very worried.
[00:35:34.200 --> 00:35:40.280]   Mike Pompeo's rhetoric because they make a lot of TikToks here in the US is very popular.
[00:35:40.280 --> 00:35:42.200]   Yeah, just the first time.
[00:35:42.200 --> 00:35:43.200]   Nope.
[00:35:43.200 --> 00:35:44.200]   Sorry.
[00:35:44.200 --> 00:35:48.120]   Go ahead, Phil, and then Stacey, and then Luis, I'll have to just traffic.
[00:35:48.120 --> 00:35:53.120]   Just Pompeo just can't do most of the dance moves.
[00:35:53.120 --> 00:35:55.320]   I think Mike needs a TikTok account.
[00:35:55.320 --> 00:35:57.600]   He'd feel a lot better about it if he did.
[00:35:57.600 --> 00:35:58.600]   Go ahead, Stacey.
[00:35:58.600 --> 00:36:01.640]   Yeah, no, I was going to say, I think this might be some of the first time or the first
[00:36:01.640 --> 00:36:05.920]   time that some of these kids has probably paid attention to Mike Pompeo other than his
[00:36:05.920 --> 00:36:08.520]   like, I think you're right.
[00:36:08.520 --> 00:36:13.680]   Luis, did you want to add anything before we move on?
[00:36:13.680 --> 00:36:15.000]   Oh, just that.
[00:36:15.000 --> 00:36:19.200]   There are a lot of really good memes on TikTok about Mike Pompeo's interview.
[00:36:19.200 --> 00:36:20.200]   Who are they?
[00:36:20.200 --> 00:36:21.200]   Oh, they're super funny.
[00:36:21.200 --> 00:36:22.200]   Yeah.
[00:36:22.200 --> 00:36:26.240]   There's a whole meme that's like people joking about like distracting Trump.
[00:36:26.240 --> 00:36:27.760]   So he doesn't delete TikTok.
[00:36:27.760 --> 00:36:32.600]   There's a girl who puts like orange makeup on and she like dances centrally while like
[00:36:32.600 --> 00:36:35.680]   stacking bricks, like pretending to be making a wall.
[00:36:35.680 --> 00:36:37.600]   And she's like, look over here, Trump.
[00:36:37.600 --> 00:36:39.360]   Like don't look at, don't delete TikTok.
[00:36:39.360 --> 00:36:41.120]   We need this app, which is pretty funny.
[00:36:41.120 --> 00:36:49.120]   Should I feel left out of the conversation if I don't have an active TikTok presence?
[00:36:49.120 --> 00:36:52.400]   You probably get more sleep than I do as a result.
[00:36:52.400 --> 00:36:54.920]   Are you doom scrolling or tick scrolling?
[00:36:54.920 --> 00:36:58.200]   I'm definitely doing a lot of TikTok scrolling late at night, which is not great because
[00:36:58.200 --> 00:37:02.040]   it's so like mobile friendly that even if I like close my laptop and you know, put away
[00:37:02.040 --> 00:37:03.040]   everything else.
[00:37:03.040 --> 00:37:04.040]   I don't know.
[00:37:04.040 --> 00:37:08.000]   I mean, why not get an account that you can just check out here or there?
[00:37:08.000 --> 00:37:13.320]   I think there's no, even if you're just like, that's exactly what I mean, who wouldn't
[00:37:13.320 --> 00:37:21.920]   love this picture of a dog relaxing on a lawn chair, but this is exactly why I got rid
[00:37:21.920 --> 00:37:24.000]   of my TikTok account is because you do.
[00:37:24.000 --> 00:37:25.000]   It is addictive.
[00:37:25.000 --> 00:37:26.000]   It's funny.
[00:37:26.000 --> 00:37:27.000]   Right.
[00:37:27.000 --> 00:37:31.120]   If Meg Whitman and Jeffrey Katzenberg had just done the right thing, they could have invested
[00:37:31.120 --> 00:37:32.680]   a lot of money.
[00:37:32.680 --> 00:37:34.400]   Instead of Quibi, they should have made TikTok.
[00:37:34.400 --> 00:37:39.200]   It is mobile friendly, but somehow I'm in the lounging dog.
[00:37:39.200 --> 00:37:41.320]   This is a good, this is a good area to be in.
[00:37:41.320 --> 00:37:44.440]   I'm one of the lounging down here.
[00:37:44.440 --> 00:37:46.520]   Wayman, here's the Bundt cake area.
[00:37:46.520 --> 00:37:48.000]   He's making an American.
[00:37:48.000 --> 00:37:49.320]   He can't get the Bundt cake.
[00:37:49.320 --> 00:37:50.760]   Oh, no.
[00:37:50.760 --> 00:37:51.760]   Ah.
[00:37:51.760 --> 00:37:55.800]   Okay, Bundt cake failure.
[00:37:55.800 --> 00:37:56.800]   This is, I don't know.
[00:37:56.800 --> 00:38:01.280]   We're not listening to music, which is a lot of what TikTok is all about.
[00:38:01.280 --> 00:38:05.760]   Yeah, I like understanding what is the cool music to be listening to.
[00:38:05.760 --> 00:38:08.160]   I'll be honest, as an old person.
[00:38:08.160 --> 00:38:09.160]   Jeez.
[00:38:09.160 --> 00:38:12.520]   See, I look at this and I go, oh, please.
[00:38:12.520 --> 00:38:17.880]   I can't stop and that's exactly why I deleted my TikTok.
[00:38:17.880 --> 00:38:20.800]   I feel for you, Louise.
[00:38:20.800 --> 00:38:23.120]   All those late nights.
[00:38:23.120 --> 00:38:24.480]   No mom, no, don't do it.
[00:38:24.480 --> 00:38:27.040]   Don't do it, mom.
[00:38:27.040 --> 00:38:29.960]   She duct taped his phone to his face.
[00:38:29.960 --> 00:38:33.680]   Now they stopped that TikTok too early because I sure would like to know what he did after
[00:38:33.680 --> 00:38:34.680]   that.
[00:38:34.680 --> 00:38:35.680]   Okay.
[00:38:35.680 --> 00:38:36.680]   All right.
[00:38:36.680 --> 00:38:37.680]   All right.
[00:38:37.680 --> 00:38:39.880]   We're going to, we're going to take a break.
[00:38:39.880 --> 00:38:44.280]   See, the mistake of doing this is now everybody's going to stop listening to the show and download
[00:38:44.280 --> 00:38:49.120]   TikTok and start looking at all these over and over and over and over again.
[00:38:49.120 --> 00:38:52.280]   It is highly addictive.
[00:38:52.280 --> 00:38:53.880]   I think it's fun to watch people.
[00:38:53.880 --> 00:38:57.320]   I mean, a good half of them are just people being goofy.
[00:38:57.320 --> 00:38:58.320]   Goofy.
[00:38:58.320 --> 00:38:59.320]   It's so weird.
[00:38:59.320 --> 00:39:02.240]   Why did what happened wasn't vine didn't vine own this?
[00:39:02.240 --> 00:39:03.240]   What happened?
[00:39:03.240 --> 00:39:06.640]   Vine was like a little rent.
[00:39:06.640 --> 00:39:08.880]   What's a more red TikTok mean?
[00:39:08.880 --> 00:39:10.880]   I'm so judgy.
[00:39:10.880 --> 00:39:13.480]   It's really judgy.
[00:39:13.480 --> 00:39:14.480]   What happened, Louise?
[00:39:14.480 --> 00:39:15.480]   My daughter loves vine.
[00:39:15.480 --> 00:39:16.480]   What happened or not?
[00:39:16.480 --> 00:39:17.480]   Vine is TikTok.
[00:39:17.480 --> 00:39:20.600]   I don't think they took it until they killed it till Twitter bought it and killed it.
[00:39:20.600 --> 00:39:22.200]   I think Twitter kind of messed it up.
[00:39:22.200 --> 00:39:24.320]   Honestly, I don't think that they really saw the potential.
[00:39:24.320 --> 00:39:27.080]   That was also like pre Instagram stories.
[00:39:27.080 --> 00:39:33.040]   I feel like we weren't really ready for like short term, short form video in that sense.
[00:39:33.040 --> 00:39:36.520]   Like TikTok came at the right time where everyone was like, Oh, I'm already watching
[00:39:36.520 --> 00:39:38.800]   short vertical video from my friends.
[00:39:38.800 --> 00:39:40.920]   Why not also watch it from funny strangers?
[00:39:40.920 --> 00:39:49.120]   So now the former founder of vine has a new thing called bite, which apparently all of
[00:39:49.120 --> 00:39:52.400]   the TikTokers in the US are thinking about moving to it.
[00:39:52.400 --> 00:39:56.360]   At least that's what Taylor Lawrence says in the New York Times.
[00:39:56.360 --> 00:39:57.400]   It's not going to happen.
[00:39:57.400 --> 00:39:59.920]   It's TikTok, right?
[00:39:59.920 --> 00:40:05.960]   I've had some fun on bite, but yeah, I checked it out when it first came out a couple months
[00:40:05.960 --> 00:40:09.840]   ago and I wasn't super impressed, but there also wasn't that many people on it yet.
[00:40:09.840 --> 00:40:11.160]   So I'll get by the chance.
[00:40:11.160 --> 00:40:12.520]   I'll get by the chance.
[00:40:12.520 --> 00:40:17.040]   One of the things that actually can ruin this and some people were worried with TikTok was
[00:40:17.040 --> 00:40:22.360]   going to get ruined is when the celebrities get on it.
[00:40:22.360 --> 00:40:25.480]   That did not seem to harm TikTok.
[00:40:25.480 --> 00:40:31.320]   I did like cringe a couple of times when I saw like Bob Saget and stuff on my brain
[00:40:31.320 --> 00:40:36.880]   page, but I think it's like segregated enough that like, I don't know, it's not like you
[00:40:36.880 --> 00:40:38.600]   don't have to see it.
[00:40:38.600 --> 00:40:39.600]   Yeah.
[00:40:39.600 --> 00:40:40.600]   Exactly.
[00:40:40.600 --> 00:40:43.760]   I think I might have actually blocked Bob Saget.
[00:40:43.760 --> 00:40:46.680]   You wouldn't be the only one, I'm sure.
[00:40:46.680 --> 00:40:47.680]   Let's take a little break.
[00:40:47.680 --> 00:40:53.240]   It's great to have Louis Mizzakis here from Wired Magazine, our great dear friend, Stacey
[00:40:53.240 --> 00:40:58.600]   Higginbotham from this week in Google and Stacey on IoT.
[00:40:58.600 --> 00:41:07.200]   And another old friend Phil Libben from all turtles, all-turtles.com and his new program.
[00:41:07.200 --> 00:41:08.800]   What's the deal with the name?
[00:41:08.800 --> 00:41:16.440]   Well, I mean, I think you said it last time.
[00:41:16.440 --> 00:41:18.800]   We wanted a memorable name.
[00:41:18.800 --> 00:41:23.840]   Yeah, we were talking about it on MacBray weekly.
[00:41:23.840 --> 00:41:25.240]   It's just MMH, MM.
[00:41:25.240 --> 00:41:26.240]   It is memorable.
[00:41:26.240 --> 00:41:28.240]   It's a palindrome.
[00:41:28.240 --> 00:41:30.360]   I always wanted a palindromic name.
[00:41:30.360 --> 00:41:31.360]   That's good.
[00:41:31.360 --> 00:41:32.360]   Yeah.
[00:41:32.360 --> 00:41:35.160]   And as you said, I think on another interview, you said it's performative.
[00:41:35.160 --> 00:41:37.720]   You tend to want to perform it.
[00:41:37.720 --> 00:41:38.720]   Yeah.
[00:41:38.720 --> 00:41:44.400]   That's really the main thing is this is a sound that anyone can make without thinking
[00:41:44.400 --> 00:41:45.400]   about it.
[00:41:45.400 --> 00:41:48.320]   Everyone just says, "Mm-hmm," all the time in conversation.
[00:41:48.320 --> 00:41:51.120]   But it's actually kind of tough to say intentionally.
[00:41:51.120 --> 00:41:55.720]   You have to pause for like half a second because you can't just say it.
[00:41:55.720 --> 00:41:56.720]   You have to perform it.
[00:41:56.720 --> 00:41:59.720]   You have to think about how you're going to inflect it, how you're going to pronounce
[00:41:59.720 --> 00:42:01.200]   it.
[00:42:01.200 --> 00:42:04.080]   And it's a name that you can't say thoughtlessly.
[00:42:04.080 --> 00:42:07.760]   Like every other company name or product name they've ever seen, you could just say it
[00:42:07.760 --> 00:42:09.200]   without thinking about it.
[00:42:09.200 --> 00:42:10.680]   And you can't.
[00:42:10.680 --> 00:42:13.080]   Every time you say it, it's like a little micro performance.
[00:42:13.080 --> 00:42:16.600]   And since the product is for performing, we kind of thought it's cool too.
[00:42:16.600 --> 00:42:18.240]   I like it.
[00:42:18.240 --> 00:42:26.760]   It is a virtual camera that you can use to create more interesting Zoom conversations.
[00:42:26.760 --> 00:42:29.640]   And you're on Skype right now.
[00:42:29.640 --> 00:42:31.920]   You said you had to do a little bit of hacking with Skype.
[00:42:31.920 --> 00:42:34.000]   Oh, look, it looks exactly the same.
[00:42:34.000 --> 00:42:37.120]   Oh, now you have a brick room.
[00:42:37.120 --> 00:42:38.920]   It's a virtual background.
[00:42:38.920 --> 00:42:40.840]   It does a very good job of the virtual background.
[00:42:40.840 --> 00:42:42.640]   That's hard to do well.
[00:42:42.640 --> 00:42:47.160]   Yeah, there's a bunch of really cool things that you can do with it.
[00:42:47.160 --> 00:42:51.280]   But yeah, people should just watch that YouTube video.
[00:42:51.280 --> 00:42:54.240]   We've had a tremendous amount of fun making it.
[00:42:54.240 --> 00:42:59.240]   And this was our first kind of post-COVID quarantine project.
[00:42:59.240 --> 00:43:04.520]   So this is the, you know, it is our first kind of lockdown native product.
[00:43:04.520 --> 00:43:05.520]   I like it.
[00:43:05.520 --> 00:43:07.720]   I don't think it would be good to meet this.
[00:43:07.720 --> 00:43:11.360]   Yeah, it is right now invite only.
[00:43:11.360 --> 00:43:12.360]   It is.
[00:43:12.360 --> 00:43:14.920]   Are you going to charge for it?
[00:43:14.920 --> 00:43:15.920]   What is the deal?
[00:43:15.920 --> 00:43:23.200]   Yeah, I mean, this is, you know, it's very much a Godfather three moment for me.
[00:43:23.200 --> 00:43:26.560]   This is, you know, every time I try to get out, they drag me back in.
[00:43:26.560 --> 00:43:28.520]   It's going to be a premium of some kind.
[00:43:28.520 --> 00:43:30.720]   Was Evernote your first?
[00:43:30.720 --> 00:43:31.720]   A first freemium.
[00:43:31.720 --> 00:43:33.120]   Yeah, I think so.
[00:43:33.120 --> 00:43:34.120]   Okay.
[00:43:34.120 --> 00:43:35.120]   So it'll be freemium.
[00:43:35.120 --> 00:43:38.240]   And then there'll be something that you pay for.
[00:43:38.240 --> 00:43:42.720]   But we haven't figured out exactly what the plan is to spend the summer in beta.
[00:43:42.720 --> 00:43:45.560]   So in beta, everything's going to be free.
[00:43:45.560 --> 00:43:48.960]   And we just want to work with people to do interesting things with it, kind of figure
[00:43:48.960 --> 00:43:51.520]   that out, invite more and more people into it.
[00:43:51.520 --> 00:43:53.680]   And then we'll go live in early fall.
[00:43:53.680 --> 00:43:56.160]   And by then we would have figured all this stuff out hopefully.
[00:43:56.160 --> 00:44:03.240]   You can go to mmhmm.app to sign up, get an invite when those are available.
[00:44:03.240 --> 00:44:06.440]   But app, I think it's fun to play with.
[00:44:06.440 --> 00:44:11.320]   There is a threat because virtual cameras don't always work.
[00:44:11.320 --> 00:44:12.560]   Like zoom, it comes and goes.
[00:44:12.560 --> 00:44:20.600]   Like we've used a snap has a virtual camera that we that we practically had the ban from
[00:44:20.600 --> 00:44:26.240]   our zoom meetings because people were coming in as giant fish and all sorts of sorts of
[00:44:26.240 --> 00:44:27.240]   things.
[00:44:27.240 --> 00:44:31.520]   But it can't it like it would work sometimes and it wouldn't work sometimes because it
[00:44:31.520 --> 00:44:35.600]   seems like the software sometimes allows virtual cameras and sometimes doesn't.
[00:44:35.600 --> 00:44:40.840]   What is you probably are very intimately familiar with this issue?
[00:44:40.840 --> 00:44:41.840]   What's going on?
[00:44:41.840 --> 00:44:48.280]   Yeah, so, you know, we don't, I guess the short answer is that you only need the virtual cameras
[00:44:48.280 --> 00:44:53.040]   for one of the use cases, which is the video conference meeting use case.
[00:44:53.040 --> 00:44:54.040]   And there, yeah, it works.
[00:44:54.040 --> 00:44:57.560]   It works great with all of the newer versions of zoom.
[00:44:57.560 --> 00:45:01.840]   They basically reintroduced virtual cameras just like a week ago.
[00:45:01.840 --> 00:45:04.800]   They were shut off for a while while they made some security improvements, but they're
[00:45:04.800 --> 00:45:05.800]   back.
[00:45:05.800 --> 00:45:10.480]   So it works with zoom, with Google Meet, with Microsoft Teams, with Blue Genes, with just
[00:45:10.480 --> 00:45:12.240]   about everything.
[00:45:12.240 --> 00:45:17.080]   But I think the video meetings are like one use case, but actually think that the streaming
[00:45:17.080 --> 00:45:22.600]   use case is probably just for people who want to make right and right in order to make
[00:45:22.600 --> 00:45:23.600]   presentations.
[00:45:23.600 --> 00:45:28.000]   So there's a lot of stuff that isn't just video meetings that I think people do.
[00:45:28.000 --> 00:45:31.600]   So we like, I feel pretty good about the fact of it.
[00:45:31.600 --> 00:45:32.600]   Yeah, yeah.
[00:45:32.600 --> 00:45:35.320]   Skype, Skype, it doesn't work with.
[00:45:35.320 --> 00:45:40.080]   So I've had to like, I've had to seriously abuse Skype on my Mac because it's work.
[00:45:40.080 --> 00:45:42.000]   I don't recommend other people do.
[00:45:42.000 --> 00:45:43.840]   So our virtual cameras is everything else.
[00:45:43.840 --> 00:45:45.000]   A security threat?
[00:45:45.000 --> 00:45:49.520]   Is that what is it that zoomed in like about them?
[00:45:49.520 --> 00:45:52.840]   I don't think virtual cameras themselves are a security threat.
[00:45:52.840 --> 00:45:58.040]   I think the way that apps handle plugins of any kind might be.
[00:45:58.040 --> 00:45:59.040]   Yeah, actually.
[00:45:59.040 --> 00:46:00.040]   They're not done correctly.
[00:46:00.040 --> 00:46:02.960]   So I think they took them out while they kind of hardened their whole plugin architecture.
[00:46:02.960 --> 00:46:03.960]   Got it.
[00:46:03.960 --> 00:46:04.960]   That makes sense.
[00:46:04.960 --> 00:46:07.800]   Yeah, they've been working hard to be more secure.
[00:46:07.800 --> 00:46:11.800]   They're number one feature, but yeah, they brought it all back.
[00:46:11.800 --> 00:46:14.000]   Because people like virtual cameras.
[00:46:14.000 --> 00:46:22.280]   Yeah, and again, I think the idea of a virtual camera, that's not a long-term concept, but
[00:46:22.280 --> 00:46:27.960]   you'll be able to do this sort of thing inside of any video presentation permanently.
[00:46:27.960 --> 00:46:33.800]   And we'll navigate through it just like we did with Evernote and all of the other companies
[00:46:33.800 --> 00:46:36.920]   and figure out the best ways to do it.
[00:46:36.920 --> 00:46:41.640]   For whatever year we happen to be in 2020 right now, it's virtual cameras in zoom.
[00:46:41.640 --> 00:46:44.520]   It's something else in other platforms.
[00:46:44.520 --> 00:46:46.240]   All that stuff are just technical details.
[00:46:46.240 --> 00:46:49.080]   Microsoft has added custom backgrounds to teams.
[00:46:49.080 --> 00:46:52.080]   And one of them is really absurd.
[00:46:52.080 --> 00:46:53.280]   Let me see if I can find it.
[00:46:53.280 --> 00:46:55.800]   It's not on this post.
[00:46:55.800 --> 00:47:00.000]   It looks like everybody in the large meeting is not on this post.
[00:47:00.000 --> 00:47:03.520]   Everybody in the large meeting is in an auditorium.
[00:47:03.520 --> 00:47:05.520]   If I can find it, it's just ridiculous.
[00:47:05.520 --> 00:47:07.880]   Oh, that's their press photos.
[00:47:07.880 --> 00:47:13.480]   If you see the, just Google the story and I bet you're going to come up with the teams
[00:47:13.480 --> 00:47:20.400]   auditorium virtual background, maybe.
[00:47:20.400 --> 00:47:23.360]   So yeah, here it is.
[00:47:23.360 --> 00:47:27.200]   It's called teacher mode or together mode.
[00:47:27.200 --> 00:47:28.200]   Sorry.
[00:47:28.200 --> 00:47:30.200]   I don't know.
[00:47:30.200 --> 00:47:32.560]   Or, you know, John Malkovich mode.
[00:47:32.560 --> 00:47:33.560]   Yeah.
[00:47:33.560 --> 00:47:34.560]   Yeah.
[00:47:34.560 --> 00:47:35.560]   Very weird.
[00:47:35.560 --> 00:47:36.560]   It's the same person.
[00:47:36.560 --> 00:47:39.720]   And then it's obviously that like just put John Malkovich's face and all of those.
[00:47:39.720 --> 00:47:43.000]   See, if we all look like John Malkovich, that would be great.
[00:47:43.000 --> 00:47:44.280]   That is John Malkovich mode.
[00:47:44.280 --> 00:47:46.000]   That would be John Malkovich mode.
[00:47:46.000 --> 00:47:47.000]   Yeah.
[00:47:47.000 --> 00:47:49.680]   This is, this is everybody's trying to figure out.
[00:47:49.680 --> 00:47:53.440]   But this is, as you point out, Phil, this is just for right now.
[00:47:53.440 --> 00:47:56.960]   So you have ambitions beyond just the zoom era.
[00:47:56.960 --> 00:48:00.080]   We're going to call this the zoom era that we're in.
[00:48:00.080 --> 00:48:01.080]   We do go.
[00:48:01.080 --> 00:48:03.960]   We'll play with it a little bit.
[00:48:03.960 --> 00:48:08.920]   Our show today brought to you by our good friends at Casper.
[00:48:08.920 --> 00:48:12.480]   As we all know these days, it's hard to get a good night's sleep.
[00:48:12.480 --> 00:48:14.320]   Too much doom scrolling.
[00:48:14.320 --> 00:48:17.480]   You got to get off the doom scrolling, Louise.
[00:48:17.480 --> 00:48:20.640]   If you have a nice mattress, maybe it'll be a little easier to get to sleep.
[00:48:20.640 --> 00:48:24.560]   Casper really innovated on this direct consumer model.
[00:48:24.560 --> 00:48:29.480]   Realized early on that the biggest problem in mattress buying is the mattress store.
[00:48:29.480 --> 00:48:31.600]   That's where all the markup happens.
[00:48:31.600 --> 00:48:34.680]   If only you could sell direct to customers.
[00:48:34.680 --> 00:48:38.360]   The problem is people like to sleep on their mattresses before they buy.
[00:48:38.360 --> 00:48:39.920]   Casper came up with a brilliant idea.
[00:48:39.920 --> 00:48:40.920]   Sure, then.
[00:48:40.920 --> 00:48:44.720]   We'll give you a 100 night risk free trial.
[00:48:44.720 --> 00:48:48.680]   If at any time in the first 100 nights you decide you don't like your Casper, they'll
[00:48:48.680 --> 00:48:50.480]   come and get it, refund you every penny.
[00:48:50.480 --> 00:48:51.720]   There's no risk at all.
[00:48:51.720 --> 00:48:55.000]   But I got to tell you, I don't think it happens very often.
[00:48:55.000 --> 00:48:56.840]   These Casper mattresses are phenomenal.
[00:48:56.840 --> 00:48:59.120]   My first Casper was the original Casper.
[00:48:59.120 --> 00:49:00.120]   They still sell it.
[00:49:00.120 --> 00:49:04.080]   It combines multiple supportive memory foams for a quality sleep surface.
[00:49:04.080 --> 00:49:06.280]   Just the right amount of sink and bounce.
[00:49:06.280 --> 00:49:07.280]   It's breathable design.
[00:49:07.280 --> 00:49:08.520]   Let's use sleep cool.
[00:49:08.520 --> 00:49:09.840]   That's really important.
[00:49:09.840 --> 00:49:14.360]   Hot summer night, if you can sleep cool, you're going to sleep much, much better.
[00:49:14.360 --> 00:49:18.600]   The original mattress now has 20,000 reviews across Casper, Amazon, and Google.
[00:49:18.600 --> 00:49:20.360]   An average of 4.8 stars.
[00:49:20.360 --> 00:49:22.160]   That's how much people love Casper.
[00:49:22.160 --> 00:49:24.520]   It's becoming the Internet's favorite mattress.
[00:49:24.520 --> 00:49:27.280]   I'm sleeping on my new Casper, the hybrid.
[00:49:27.280 --> 00:49:29.600]   They actually have a number of new models.
[00:49:29.600 --> 00:49:35.840]   The Wave, which features a patent-pended premium support system to mirror the natural
[00:49:35.840 --> 00:49:37.680]   shape of your body.
[00:49:37.680 --> 00:49:40.080]   The essential is great for low cost.
[00:49:40.080 --> 00:49:43.760]   It has a streamlined design and a price that won't keep you up at night.
[00:49:43.760 --> 00:49:47.800]   It's great for a guest bedroom, a student, that kind of thing.
[00:49:47.800 --> 00:49:50.880]   Then I'm on the hybrid right now with my kitty cat, Sammy.
[00:49:50.880 --> 00:49:55.080]   The hybrid combines the pressure relief of the award-winning foam with durable yet gentle
[00:49:55.080 --> 00:49:56.360]   springs.
[00:49:56.360 --> 00:49:57.840]   They also have great pillows.
[00:49:57.840 --> 00:49:59.760]   I love my Casper king-sized pillow.
[00:49:59.760 --> 00:50:03.640]   They have wonderful sheets, the Casper Glow Lamp.
[00:50:03.640 --> 00:50:09.280]   You can be sure of your purchase with Casper's 100-night risk-free sleep on a trial.
[00:50:09.280 --> 00:50:13.440]   They also offer free shipping and painless returns to the US and Canada.
[00:50:13.440 --> 00:50:16.440]   Right now, we're going to get you $100 off select mattresses.
[00:50:16.440 --> 00:50:24.240]   If you go to Casper.com/twit1 and using the promo code, "Twit1" at checkout.
[00:50:24.240 --> 00:50:29.440]   Casper.com/twit1, some terms and conditions apply.
[00:50:29.440 --> 00:50:33.120]   Don't forget to use the offer code "Twit" and the number one at checkout to get that
[00:50:33.120 --> 00:50:34.360]   $100 credit.
[00:50:34.360 --> 00:50:36.280]   Twit and the number code "1".
[00:50:36.280 --> 00:50:37.360]   I love my Casper.
[00:50:37.360 --> 00:50:38.360]   You will too.
[00:50:38.360 --> 00:50:39.360]   Good night's sleep.
[00:50:39.360 --> 00:50:40.520]   It's worth its weight in gold.
[00:50:40.520 --> 00:50:46.040]   I love the Casper.com/twit1 and the offer code "Twit1".
[00:50:46.040 --> 00:50:53.520]   Thank you, Casper, for your support.
[00:50:53.520 --> 00:50:57.560]   I'm just looking at all of the stories we could talk about.
[00:50:57.560 --> 00:51:00.280]   I love those TikTok stories.
[00:51:00.280 --> 00:51:02.960]   Did you ever try fill the magic leap?
[00:51:02.960 --> 00:51:05.040]   I did, yeah.
[00:51:05.040 --> 00:51:06.840]   Why do I see?
[00:51:06.840 --> 00:51:08.360]   Feel like we talked about this before.
[00:51:08.360 --> 00:51:10.360]   What did you think of it?
[00:51:10.360 --> 00:51:12.600]   I thought it was great.
[00:51:12.600 --> 00:51:17.160]   I have a high tolerance for wearing extremely dorky stuff on my face.
[00:51:17.160 --> 00:51:18.160]   I didn't bother.
[00:51:18.160 --> 00:51:19.600]   In fact, that's kind of a feature.
[00:51:19.600 --> 00:51:22.920]   I sort of looked for that.
[00:51:22.920 --> 00:51:26.520]   But I may be an outlier in that.
[00:51:26.520 --> 00:51:30.080]   The people who tried magic leap loved it.
[00:51:30.080 --> 00:51:31.800]   I remember talking to Amy Webb.
[00:51:31.800 --> 00:51:32.880]   She said, "You just don't get it.
[00:51:32.880 --> 00:51:34.720]   Steven Levy, you just don't get it.
[00:51:34.720 --> 00:51:36.760]   If you just could try it.
[00:51:36.760 --> 00:51:42.520]   I'm not talking about this is the developer version that they put out last year.
[00:51:42.520 --> 00:51:50.400]   I'm not talking about that, but the actual real magic leap apparently was something amazing.
[00:51:50.400 --> 00:51:52.720]   Maybe they oversold it a little bit on their website.
[00:51:52.720 --> 00:51:58.040]   Remember the school gym with a giant whale coming and splashing?
[00:51:58.040 --> 00:52:01.280]   They might have oversold it a little bit, but they were apparently trying something very
[00:52:01.280 --> 00:52:09.240]   different, a technology that was much more effective than other augmented reality glasses.
[00:52:09.240 --> 00:52:12.720]   The problem is it just took them too long.
[00:52:12.720 --> 00:52:16.240]   They raised almost $2 billion.
[00:52:16.240 --> 00:52:18.120]   They spent years developing this thing.
[00:52:18.120 --> 00:52:26.600]   I think there was some problem I suspect in getting it portable enough to be really useful.
[00:52:26.600 --> 00:52:29.480]   Company almost went out of business earlier this year.
[00:52:29.480 --> 00:52:32.280]   They were ready to lay off half their employees.
[00:52:32.280 --> 00:52:39.400]   Got an urgent round of additional funds, $350 million from the original investors.
[00:52:39.400 --> 00:52:42.360]   Now they have a new CEO.
[00:52:42.360 --> 00:52:49.880]   I realize this is total conspiracy theory, but the new CEO comes from Microsoft.
[00:52:49.880 --> 00:52:53.600]   She was there doing biz dev at Microsoft for six years.
[00:52:53.600 --> 00:52:55.760]   Maybe she just wanted a new challenge.
[00:52:55.760 --> 00:52:58.880]   Magic leap is struggling.
[00:52:58.880 --> 00:53:02.880]   Maybe she just thought it'd be, "Here's an opportunity either to turn it around and become
[00:53:02.880 --> 00:53:03.880]   a hero."
[00:53:03.880 --> 00:53:08.520]   If I don't, nobody's going to blame me because it's already fallen apart.
[00:53:08.520 --> 00:53:09.640]   I have another theory.
[00:53:09.640 --> 00:53:11.680]   This has happened once before with Microsoft.
[00:53:11.680 --> 00:53:18.840]   Microsoft executive Steven Eelop left Microsoft because he didn't look like he was going
[00:53:18.840 --> 00:53:26.120]   to get to be CEO and went to a company called Nokia, which Microsoft eventually acquired
[00:53:26.120 --> 00:53:28.240]   for billions of dollars.
[00:53:28.240 --> 00:53:31.080]   Remember, Microsoft is in the same business with their HoloLens.
[00:53:31.080 --> 00:53:37.760]   Am I not Stacey thinking that this may be the first move in an acquisition of Magic Leap?
[00:53:37.760 --> 00:53:40.680]   Why would Microsoft need?
[00:53:40.680 --> 00:53:43.160]   Maybe it's if they could get it cheap.
[00:53:43.160 --> 00:53:46.480]   I've done Magic Leap and I've done HoloLens.
[00:53:46.480 --> 00:53:52.920]   Both are good and HoloLens has actual customers and use cases that Microsoft can charge a
[00:53:52.920 --> 00:53:53.920]   lot of money for.
[00:53:53.920 --> 00:53:57.760]   I don't know if it needs Magic Leap.
[00:53:57.760 --> 00:54:00.360]   I think the Nokia deal was not one off.
[00:54:00.360 --> 00:54:03.160]   It was certainly nothing to trumpet.
[00:54:03.160 --> 00:54:07.160]   It was a bit right off in the end.
[00:54:07.160 --> 00:54:14.440]   Other people leave big, if you look at Cisco departures, you're going to see lots of acquisitions
[00:54:14.440 --> 00:54:17.080]   from the companies that their executives went to start.
[00:54:17.080 --> 00:54:20.040]   I don't think that's not conspiracy.
[00:54:20.040 --> 00:54:21.600]   It's just what you know.
[00:54:21.600 --> 00:54:27.400]   Maybe Peggy just felt like, "I've been here six years, time for another challenge, completely
[00:54:27.400 --> 00:54:28.400]   possible."
[00:54:28.400 --> 00:54:30.600]   Was she working on HoloLens with people?
[00:54:30.600 --> 00:54:32.600]   She was an EVP of Biz Dev.
[00:54:32.600 --> 00:54:35.720]   She was doing deals a lot.
[00:54:35.720 --> 00:54:42.960]   But that seems like that would be a reasonable person to put in there as a bridge to an acquisition.
[00:54:42.960 --> 00:54:46.400]   Sorry, I'm nodding my head.
[00:54:46.400 --> 00:54:47.400]   I'm like, "Yes."
[00:54:47.400 --> 00:54:48.560]   All right, maybe I'm nuts.
[00:54:48.560 --> 00:54:49.920]   I don't know.
[00:54:49.920 --> 00:54:54.680]   The only way this makes sense is if Magic Leap has some secret sauce, which many people
[00:54:54.680 --> 00:54:58.320]   have said they do, that is better than HoloLens.
[00:54:58.320 --> 00:55:02.520]   It's certainly got to be better than that Magic Leap one headset, which I have tried.
[00:55:02.520 --> 00:55:05.280]   That was silly.
[00:55:05.280 --> 00:55:07.640]   It was roughly the same as a HoloLens.
[00:55:07.640 --> 00:55:09.720]   But I think HoloLens is kind of silly, too.
[00:55:09.720 --> 00:55:11.800]   I don't think that's the end game.
[00:55:11.800 --> 00:55:17.120]   Louise, do you have an opinion on these AR glasses?
[00:55:17.120 --> 00:55:18.520]   Do you want some?
[00:55:18.520 --> 00:55:20.200]   I just have never gotten into it.
[00:55:20.200 --> 00:55:24.640]   I feel like the pandemic would have been the perfect moment for them to really take off.
[00:55:24.640 --> 00:55:29.920]   I don't know if we're just not ready for this tech yet, if it's not mobile enough.
[00:55:29.920 --> 00:55:36.600]   I'm curious and would be interested to delve deeper into some of this tech as it gets more
[00:55:36.600 --> 00:55:37.800]   mainstream.
[00:55:37.800 --> 00:55:41.040]   For now, it's not something I gravitate toward.
[00:55:41.040 --> 00:55:44.160]   I think that a lot of Americans feel the way that I do.
[00:55:44.160 --> 00:55:45.600]   I feel the way that you do.
[00:55:45.600 --> 00:55:49.760]   Even with virtual reality, I have a lot of friends who are big VR fans.
[00:55:49.760 --> 00:55:56.200]   When our 17-year-old spends a lot of time wearing his vibe with his buddies playing
[00:55:56.200 --> 00:55:59.400]   weird Beat Saber and things like that.
[00:55:59.400 --> 00:56:01.240]   Does it give him vertigo?
[00:56:01.240 --> 00:56:03.240]   It makes me sick.
[00:56:03.240 --> 00:56:04.240]   Me, too.
[00:56:04.240 --> 00:56:05.240]   It makes me nauseous.
[00:56:05.240 --> 00:56:09.640]   I actually think there's a different angle on this.
[00:56:09.640 --> 00:56:14.240]   This is one of our inspirations behind me.
[00:56:14.240 --> 00:56:21.920]   I spent 20 years and I've talked to you about the soladlio, fantasizing about all of the
[00:56:21.920 --> 00:56:23.520]   use cases that we're going to build.
[00:56:23.520 --> 00:56:28.960]   We finally have AR headsets that are small as normal glasses.
[00:56:28.960 --> 00:56:32.360]   Imagine all of the amazing functionality that we can have when we're walking around
[00:56:32.360 --> 00:56:34.880]   with these ubiquitous AR glasses.
[00:56:34.880 --> 00:56:40.600]   Then we just realized a few weeks ago, a few weeks into the lockdown, that now we all
[00:56:40.600 --> 00:56:42.640]   have giant AR glasses.
[00:56:42.640 --> 00:56:44.400]   They're just our big monitors.
[00:56:44.400 --> 00:56:45.400]   True.
[00:56:45.400 --> 00:56:47.800]   That is our window to the world, isn't it?
[00:56:47.800 --> 00:56:53.320]   We just started implementing a bunch of the fantasy use cases that I was saving for 10
[00:56:53.320 --> 00:56:54.320]   years from now.
[00:56:54.320 --> 00:56:57.080]   I thought, well, maybe now is actually a good time to do it.
[00:56:57.080 --> 00:56:58.080]   Interesting.
[00:56:58.080 --> 00:56:59.080]   Staring at monitors.
[00:56:59.080 --> 00:57:03.560]   Let's bring out those creative AR ideas and just stick them into monitors.
[00:57:03.560 --> 00:57:09.360]   There's a lot of ideas to be mined in that way.
[00:57:09.360 --> 00:57:13.520]   There's definitely the first version of that and there's a bunch more stuff.
[00:57:13.520 --> 00:57:17.040]   That's really interesting.
[00:57:17.040 --> 00:57:22.700]   I think I agree with you, Louise, if these were ready, anything like this were ready.
[00:57:22.700 --> 00:57:24.520]   This would be a perfect time.
[00:57:24.520 --> 00:57:29.240]   Although, I think a lot of what AR, as opposed to VR, what AR is going to do is give you
[00:57:29.240 --> 00:57:33.320]   information as you operate in the world.
[00:57:33.320 --> 00:57:37.360]   I think one thing we've seen, which is actually really fascinating, is that people like AR
[00:57:37.360 --> 00:57:43.960]   not on the outside world as much, not yet at least, but they actually like it on themselves.
[00:57:43.960 --> 00:57:49.600]   The biggest form of AR that everyone uses all the time is Snapchat filters, TikTok filters.
[00:57:49.600 --> 00:57:55.800]   I think that there's this idea of, "Oh, we want to put the glasses on.
[00:57:55.800 --> 00:57:57.600]   We'll be able to look at things and get information."
[00:57:57.600 --> 00:58:01.920]   I think that actually might be really useful, especially for something like Google Maps.
[00:58:01.920 --> 00:58:03.480]   Being told, "Just make it right here."
[00:58:03.480 --> 00:58:06.320]   That's so much better than just staring at your phone.
[00:58:06.320 --> 00:58:09.520]   It's interesting that so far what's been really big has been filters.
[00:58:09.520 --> 00:58:10.920]   That's the biggest AR application.
[00:58:10.920 --> 00:58:13.200]   I would say that's the most mainstream.
[00:58:13.200 --> 00:58:14.760]   I never even saw that.
[00:58:14.760 --> 00:58:15.760]   That's a good point.
[00:58:15.760 --> 00:58:16.760]   I don't know.
[00:58:16.760 --> 00:58:17.760]   Are you guys?
[00:58:17.760 --> 00:58:22.000]   It could be because of where I'm at, but I just moved in about a house.
[00:58:22.000 --> 00:58:28.680]   I've been playing crazy because I can't shop right now for things like, "I need a lamp
[00:58:28.680 --> 00:58:31.520]   because there are no lights in my house."
[00:58:31.520 --> 00:58:36.960]   I'm sitting here using the AR function on a lot of the shopping sites to see the furniture
[00:58:36.960 --> 00:58:40.400]   in my room, which I used to think was the stupidest thing in the world.
[00:58:40.400 --> 00:58:46.160]   Now it is not saving me, but it is actually quite helpful.
[00:58:46.160 --> 00:58:48.440]   I've been using that.
[00:58:48.440 --> 00:58:55.040]   I'll just say, as someone who has played with AR in factory settings, I think there's a
[00:58:55.040 --> 00:59:01.120]   huge opportunity for home repair that people could be using right now.
[00:59:01.120 --> 00:59:06.480]   I would love, again, I bought a 50-year-old house.
[00:59:06.480 --> 00:59:07.480]   I'm sitting here.
[00:59:07.480 --> 00:59:12.600]   I wish there were the AR videos I've tried for wiring airplane engines because those are
[00:59:12.600 --> 00:59:15.640]   the kind of demos I get to do.
[00:59:15.640 --> 00:59:22.000]   I wish I could bring that to fixing the leak in my sink.
[00:59:22.000 --> 00:59:24.280]   I think that would be awesome.
[00:59:24.280 --> 00:59:27.320]   I think the quarantine is probably a really good time for stuff like that.
[00:59:27.320 --> 00:59:28.320]   I don't know.
[00:59:28.320 --> 00:59:29.320]   All right.
[00:59:29.320 --> 00:59:31.640]   The technology we're ready, I think that's a part of the problem.
[00:59:31.640 --> 00:59:36.880]   One of the reasons magically ran out of runway is it's harder to do this stuff.
[00:59:36.880 --> 00:59:42.320]   One of the big stories this week comes from the information.
[00:59:42.320 --> 00:59:48.600]   According to the information, Apple is working with Foxconn, their big manufacturing arm,
[00:59:48.600 --> 00:59:54.400]   to develop semi-transparent lenses for AR glasses.
[00:59:54.400 --> 00:59:56.720]   They're actually in trial production.
[00:59:56.720 --> 00:59:59.960]   They're literally making thousands of these.
[00:59:59.960 --> 01:00:06.040]   According to the information, the lenses are one to two years away from mass production,
[01:00:06.040 --> 01:00:12.120]   but they are making them for testing and so forth.
[01:00:12.120 --> 01:00:14.200]   This confirms the story that Mark Gurman had.
[01:00:14.200 --> 01:00:19.360]   It was also a great Apple rumor distributor over at Bloomberg.
[01:00:19.360 --> 01:00:28.160]   He said that Apple will have a VR-like pair of glasses in 2021 and in 2022 or '23, they
[01:00:28.160 --> 01:00:33.880]   will have actual spectacles that are AR spectacles.
[01:00:33.880 --> 01:00:37.440]   Phil, is this match what you've been hearing?
[01:00:37.440 --> 01:00:43.240]   Yeah, but I've also, I'm still waiting for the transparent iPhone from 2016.
[01:00:43.240 --> 01:00:44.240]   Okay.
[01:00:44.240 --> 01:00:48.160]   Scoble owes me something for that because that never happened.
[01:00:48.160 --> 01:00:54.800]   Look, I think, so I always, I have a hard time thinking about technology advancements
[01:00:54.800 --> 01:00:56.240]   in the abstract.
[01:00:56.240 --> 01:01:00.760]   I prefer to think of what are the actual use cases that are going to be magical?
[01:01:00.760 --> 01:01:02.480]   Like, what do we want to do?
[01:01:02.480 --> 01:01:06.220]   So when we're thinking about AR, like it's easy to think about, okay, if I had this
[01:01:06.220 --> 01:01:10.760]   like AR headset that was, you know, that was on all the time, well, how would it improve
[01:01:10.760 --> 01:01:14.920]   meetings, for example?
[01:01:14.920 --> 01:01:19.120]   And, you know, we can, we actually designed a bunch of storyboards and features about
[01:01:19.120 --> 01:01:22.680]   what we would do to make meetings better with AR headsets.
[01:01:22.680 --> 01:01:25.080]   This was even back in never-no times.
[01:01:25.080 --> 01:01:28.720]   But now, all of my meetings are looking at a monitor, right?
[01:01:28.720 --> 01:01:33.320]   So I can like, I can take some of those ideas of how would we make meetings better if we
[01:01:33.320 --> 01:01:36.440]   had like ambient, always on information and we could just apply them.
[01:01:36.440 --> 01:01:39.320]   We don't need, we don't need AR, right?
[01:01:39.320 --> 01:01:45.560]   Our actual reality has moved into big monitors for at least, you know, the next months or
[01:01:45.560 --> 01:01:48.800]   a couple of years, or at least a large part of it will be.
[01:01:48.800 --> 01:01:53.020]   And so I think it's really interesting to sort of take the use cases that we've been
[01:01:53.020 --> 01:01:57.840]   storyboarding and designing for AR and just see, okay, well, now that our reality has
[01:01:57.840 --> 01:02:00.520]   moved online so much, how do we just introduce this?
[01:02:00.520 --> 01:02:03.720]   And we don't need the headset anymore.
[01:02:03.720 --> 01:02:09.880]   Yeah, you know, I've got a 37 inch monitor strapped to the front of my face for 14 hours
[01:02:09.880 --> 01:02:10.880]   a day now.
[01:02:10.880 --> 01:02:11.880]   Sadly, yes.
[01:02:11.880 --> 01:02:13.960]   You know, it all just kind of works.
[01:02:13.960 --> 01:02:17.520]   And then once we have those use cases figured out, whether they be in, you know, entertainment
[01:02:17.520 --> 01:02:24.160]   or, yeah, home repair or, you know, improving meetings or productivity or how we communicate,
[01:02:24.160 --> 01:02:27.000]   eventually, sure, we'll be getting out of the house more and more again.
[01:02:27.000 --> 01:02:32.000]   And hopefully by then we'll have, we'll have better AR headsets and hardware and we can,
[01:02:32.000 --> 01:02:34.960]   we can port some of those use cases over to it.
[01:02:34.960 --> 01:02:40.720]   The problem I had with Magic Leap, for example, was always that like, it was shockingly cool.
[01:02:40.720 --> 01:02:45.360]   Like I loved using it, but I never heard of like, well, okay, well, what are the actual
[01:02:45.360 --> 01:02:46.360]   use cases?
[01:02:46.360 --> 01:02:47.360]   Yeah, what do you do with this?
[01:02:47.360 --> 01:02:48.360]   Yeah.
[01:02:48.360 --> 01:02:49.360]   Great.
[01:02:49.360 --> 01:02:52.560]   Other than a couple of ones that, you know, everyone talked about, which was like training
[01:02:52.560 --> 01:02:53.560]   neurosurgeons.
[01:02:53.560 --> 01:02:56.200]   For some reason, there was always like, well, you can use it to like train brain surgeons.
[01:02:56.200 --> 01:03:01.440]   I don't, if you see us brain surgeon working on you and he's wearing Magic Leap, I'd
[01:03:01.440 --> 01:03:02.440]   run if you can.
[01:03:02.440 --> 01:03:03.440]   Yeah.
[01:03:03.440 --> 01:03:04.440]   Yeah.
[01:03:04.440 --> 01:03:07.440]   It's like somebody looking at the manual.
[01:03:07.440 --> 01:03:13.240]   Well, it's like, it's like, I think Leo, you and I are roughly, roughly the same age.
[01:03:13.240 --> 01:03:17.040]   I think maybe exactly.
[01:03:17.040 --> 01:03:21.960]   When I was growing up, when personal computers first started becoming a thing, there was,
[01:03:21.960 --> 01:03:24.880]   but there wasn't really like a lot of commercial software yet.
[01:03:24.880 --> 01:03:28.880]   The computer manufacturers would always like sell you the idea that like, oh, you could,
[01:03:28.880 --> 01:03:32.760]   if you buy this Atari computer, you can use it to, there's always two use cases.
[01:03:32.760 --> 01:03:35.800]   There's always a balance your checkbook and where your recipes.
[01:03:35.800 --> 01:03:36.800]   Yes.
[01:03:36.800 --> 01:03:38.800]   There's always like balance your checkbook and story recipes.
[01:03:38.800 --> 01:03:40.800]   Both categories have died, by the way.
[01:03:40.800 --> 01:03:43.240]   I'm still not sure what balancing a checkbook means.
[01:03:43.240 --> 01:03:44.880]   I don't think I've ever done it.
[01:03:44.880 --> 01:03:48.680]   But those were the two things and like for AR, it was always like, yeah, you can use
[01:03:48.680 --> 01:03:49.680]   it to train brain surgeons.
[01:03:49.680 --> 01:03:50.680]   Right.
[01:03:50.680 --> 01:03:53.080]   That is the balance your checkbook and story recipes.
[01:03:53.080 --> 01:03:55.440]   That's what happens when we don't know what's going to happen.
[01:03:55.440 --> 01:03:58.360]   We make up weird use cases.
[01:03:58.360 --> 01:04:02.880]   Focal which made North, which made the focal one glasses and was working on the focal two
[01:04:02.880 --> 01:04:08.200]   glasses, just got acquired or it's more like an Aqua hire by Google.
[01:04:08.200 --> 01:04:10.520]   But those were glasses that actually were real.
[01:04:10.520 --> 01:04:12.440]   We had a review on hands-on technology.
[01:04:12.440 --> 01:04:15.160]   Anthony Nielsen, one of our producers loves them.
[01:04:15.160 --> 01:04:16.480]   That was just notifications.
[01:04:16.480 --> 01:04:20.560]   It has a heads-up display and it was just putting notifications.
[01:04:20.560 --> 01:04:25.920]   That's the most obvious use is kind of an adjunct to your phone.
[01:04:25.920 --> 01:04:31.440]   There's a really cool pair of smart glasses made by Jins in Japan.
[01:04:31.440 --> 01:04:34.280]   These are normal glasses but these are by Jins.
[01:04:34.280 --> 01:04:38.480]   Jins is a really cool Japanese eyewear company.
[01:04:38.480 --> 01:04:41.200]   They have smart glasses that don't have any display.
[01:04:41.200 --> 01:04:46.000]   They just have a bunch of sensors including by, there's like a sensor and they look totally
[01:04:46.000 --> 01:04:47.000]   like normal glasses.
[01:04:47.000 --> 01:04:49.200]   Like you can't tell there's anything, like then it's a new generation.
[01:04:49.200 --> 01:04:51.360]   It just looks like glasses.
[01:04:51.360 --> 01:04:55.600]   There's a sensor that goes on the bridge of your nose and by basically measuring what's
[01:04:55.600 --> 01:05:01.640]   going on with the bridge of your nose, they can detect all sorts of stress, emotion, focus,
[01:05:01.640 --> 01:05:07.280]   fatigue and just gives you, yeah, so there's a next generation of that which this is the
[01:05:07.280 --> 01:05:08.280]   old generation.
[01:05:08.280 --> 01:05:10.640]   Maybe the next generation is an out yet.
[01:05:10.640 --> 01:05:12.640]   Maybe it is a new generation.
[01:05:12.640 --> 01:05:14.640]   This is the Jins meme-e-s.
[01:05:14.640 --> 01:05:21.880]   There's this and there's like a new generation of that either out or coming out, I think.
[01:05:21.880 --> 01:05:24.520]   And it's really cool but again it's a different direction.
[01:05:24.520 --> 01:05:25.520]   It's not AR at all.
[01:05:25.520 --> 01:05:32.240]   There's actually no display but it can do all sorts of things about telling you about
[01:05:32.240 --> 01:05:36.120]   your posture, your focus, your concentration, how TGR.
[01:05:36.120 --> 01:05:41.040]   So I think we've been focusing a lot on the immersive displays because those are like
[01:05:41.040 --> 01:05:46.240]   sexier but honestly the first gen, even the second gen of these wearables is probably
[01:05:46.240 --> 01:05:50.040]   the sensors are going to be more important than the display capabilities.
[01:05:50.040 --> 01:05:58.120]   One of the issues is privacy because a lot of early designs had cameras out front.
[01:05:58.120 --> 01:06:00.040]   In fact, super creepy.
[01:06:00.040 --> 01:06:01.640]   Yeah, that was too soon.
[01:06:01.640 --> 01:06:07.320]   I'm with Phil on this because we were testing the Echo Frames and I don't think many of
[01:06:07.320 --> 01:06:09.040]   us need Madam A on their face.
[01:06:09.040 --> 01:06:14.000]   But if you're that kind of person, it was really helpful just to say something and there's
[01:06:14.000 --> 01:06:20.360]   something always on you that is receptive to that and you look at what Apple's announcing
[01:06:20.360 --> 01:06:30.040]   with their AirPods and their monitors and things like being able to adjust your hearing,
[01:06:30.040 --> 01:06:32.200]   adjust your hearing to what you're seeing on the monitor.
[01:06:32.200 --> 01:06:33.480]   I can't remember.
[01:06:33.480 --> 01:06:40.280]   But basically tying your motions to things that are happening around you, so using that
[01:06:40.280 --> 01:06:46.400]   sort of sensor or sensor fusion just to say, "Hey, I think this is happening with this
[01:06:46.400 --> 01:06:47.400]   person.
[01:06:47.400 --> 01:06:50.400]   Let me make their life a little bit better by tweaking something."
[01:06:50.400 --> 01:06:55.920]   I think that's going to be far more exciting than super imposing the internet in front
[01:06:55.920 --> 01:06:57.760]   of us, in front of the real world, right?
[01:06:57.760 --> 01:07:01.400]   Because that's kind of a crappy thing to want to do all the time.
[01:07:01.400 --> 01:07:05.080]   So you see them, and by the way, these are my fine snapchat.
[01:07:05.080 --> 01:07:07.520]   Oh, Lord.
[01:07:07.520 --> 01:07:09.240]   I thought those were gone.
[01:07:09.240 --> 01:07:13.800]   You can see me coming a mile away with my snapchat glasses on.
[01:07:13.800 --> 01:07:14.800]   These are just a camera.
[01:07:14.800 --> 01:07:18.360]   In fact, I went back in my snapchat account and I found all the videos I shot four years
[01:07:18.360 --> 01:07:22.720]   ago and then never took anymore with these.
[01:07:22.720 --> 01:07:27.080]   I think that that's an interesting approach and it kind of goes along with what you were
[01:07:27.080 --> 01:07:28.120]   saying, Louise.
[01:07:28.120 --> 01:07:30.200]   It's more inward than outward.
[01:07:30.200 --> 01:07:39.480]   It's more about life law, the actual, what do they call it when you write all the information
[01:07:39.480 --> 01:07:41.120]   down about what you're doing or...
[01:07:41.120 --> 01:07:42.120]   Like logging?
[01:07:42.120 --> 01:07:48.640]   Like life logging or the quantified self, that kind of thing.
[01:07:48.640 --> 01:07:49.640]   Maybe that's what we want.
[01:07:49.640 --> 01:07:51.200]   That's kind of what these gins do, right?
[01:07:51.200 --> 01:07:53.360]   They're looking at your emotions.
[01:07:53.360 --> 01:07:54.360]   Yeah.
[01:07:54.360 --> 01:07:59.400]   So Bose has a set out of the Bose Frames, which have some sensors but also just really good
[01:07:59.400 --> 01:08:01.400]   sensors, which are pretty cool.
[01:08:01.400 --> 01:08:02.400]   They're open here.
[01:08:02.400 --> 01:08:05.480]   They're going to kill their AR, acoustic AR effort though.
[01:08:05.480 --> 01:08:06.480]   Oh, really?
[01:08:06.480 --> 01:08:07.960]   And I was sad about that, yeah.
[01:08:07.960 --> 01:08:08.960]   Yeah.
[01:08:08.960 --> 01:08:14.080]   But again, it never really made sense as audio AR.
[01:08:14.080 --> 01:08:16.800]   First of all, because that's clearly ought to be pronounced R.
[01:08:16.800 --> 01:08:17.800]   R.
[01:08:17.800 --> 01:08:22.560]   You didn't do it, so let's do the paper.
[01:08:22.560 --> 01:08:24.920]   But I've got two pairs of these.
[01:08:24.920 --> 01:08:25.920]   Really?
[01:08:25.920 --> 01:08:27.160]   I put prescription lenses in them.
[01:08:27.160 --> 01:08:31.160]   Yeah, we were hacking around on them to make some apps for them.
[01:08:31.160 --> 01:08:33.320]   And it's like a lot of things.
[01:08:33.320 --> 01:08:40.640]   The product itself is like early, but it's a really magical experience to have very high
[01:08:40.640 --> 01:08:45.480]   quality personal audio that doesn't cover your ears at all.
[01:08:45.480 --> 01:08:48.080]   And it's very interesting social signaling.
[01:08:48.080 --> 01:08:54.120]   Like people don't see your ears covered, so they're more likely to talk to you.
[01:08:54.120 --> 01:08:59.160]   And now I only vaguely remember interacting with other people, like in the same room.
[01:08:59.160 --> 01:09:02.440]   So none of this stuff, I think, makes sense in the lockdown world.
[01:09:02.440 --> 01:09:06.560]   But hopefully soon, again, it will be out of our apartments again, and that kind of stuff
[01:09:06.560 --> 01:09:07.560]   will make sense.
[01:09:07.560 --> 01:09:13.800]   But again, the Bose ones are very cool and no display, just sensors and speakers.
[01:09:13.800 --> 01:09:18.360]   Chatham says if I just had a bowler hat, I'd look like Roger Stone in these Snapchat
[01:09:18.360 --> 01:09:19.360]   spectacles.
[01:09:19.360 --> 01:09:23.160]   Yeah, I think they're right.
[01:09:23.160 --> 01:09:25.080]   So this is interesting.
[01:09:25.080 --> 01:09:29.280]   Is this market driven or is it company driven?
[01:09:29.280 --> 01:09:32.520]   You know, all the tech companies are looking for the next big thing.
[01:09:32.520 --> 01:09:33.520]   I mean, that's-
[01:09:33.520 --> 01:09:35.320]   I think it's just tech driven.
[01:09:35.320 --> 01:09:39.960]   Like, I mean, I think what it is is we're thanks to really cheap sensors.
[01:09:39.960 --> 01:09:42.560]   You've got amazing ability to process.
[01:09:42.560 --> 01:09:44.440]   You've got, you know, on the device.
[01:09:44.440 --> 01:09:45.440]   We can do it.
[01:09:45.440 --> 01:09:49.200]   And you've got everybody carries a freaking supercomputer that they can connect via Bluetooth
[01:09:49.200 --> 01:09:50.200]   to this.
[01:09:50.200 --> 01:09:52.640]   And you're like, wait, what can you do with this?
[01:09:52.640 --> 01:09:53.640]   Right.
[01:09:53.640 --> 01:09:55.560]   And I don't, I mean, I'm really excited.
[01:09:55.560 --> 01:09:59.880]   Like, you can tell, like, if you stick a earphone out and it's talking to somebody from Bose,
[01:09:59.880 --> 01:10:03.240]   and they were like, you can get galvanic skin response.
[01:10:03.240 --> 01:10:08.000]   You can get heart rate information that's super accurate.
[01:10:08.000 --> 01:10:12.520]   We got into this whole metaphysical conversation about like how everybody's hearing is different.
[01:10:12.520 --> 01:10:17.760]   And so what if you, like, the perception of the world that people have, you could actually
[01:10:17.760 --> 01:10:22.880]   change their perspective slash perception a little bit by, you know, tweaking the way
[01:10:22.880 --> 01:10:24.480]   they hear the world.
[01:10:24.480 --> 01:10:29.560]   And I don't, I mean, if you're a researcher studying this, that's fascinating.
[01:10:29.560 --> 01:10:33.120]   Of course you're going to try to build something that takes advantage of that.
[01:10:33.120 --> 01:10:39.920]   It was probably five years ago that we had Jerry Ellsworth on demonstrating her very early
[01:10:39.920 --> 01:10:41.160]   AR gaming.
[01:10:41.160 --> 01:10:45.920]   And she's getting close now with her product Tilt 5 there in preorder mode where it's a
[01:10:45.920 --> 01:10:46.920]   tabletop game.
[01:10:46.920 --> 01:10:49.640]   You're wearing these spectacles.
[01:10:49.640 --> 01:10:56.240]   You have this, this wand and you can play games that on the table in front of you that
[01:10:56.240 --> 01:11:03.080]   kind of come to life a little bit like Microsoft showed with the Minecraft on the table with
[01:11:03.080 --> 01:11:05.040]   a hollow lens.
[01:11:05.040 --> 01:11:08.440]   This is a kind of an interesting sleigh on it as well.
[01:11:08.440 --> 01:11:14.800]   I feel like everybody's kind of nibbling around the edges of this.
[01:11:14.800 --> 01:11:17.520]   But nobody has yet found the killer app.
[01:11:17.520 --> 01:11:20.240]   Maybe it'll be you, Phil.
[01:11:20.240 --> 01:11:22.040]   That's quite likely.
[01:11:22.040 --> 01:11:26.960]   You know, what, what, what these things are is, yeah, like a lot of them are tech driven,
[01:11:26.960 --> 01:11:28.840]   a lot of them are company driven.
[01:11:28.840 --> 01:11:31.440]   What very few of them are, are, are user driven.
[01:11:31.440 --> 01:11:32.440]   Well, that's my question.
[01:11:32.440 --> 01:11:33.440]   Yeah.
[01:11:33.440 --> 01:11:34.440]   These are user centric designs.
[01:11:34.440 --> 01:11:38.760]   This is the textbook definition of a not user centric design process.
[01:11:38.760 --> 01:11:39.760]   We can do it.
[01:11:39.760 --> 01:11:42.880]   So we're going to build it and hope that somebody wants it.
[01:11:42.880 --> 01:11:44.040]   I had a, I had a.
[01:11:44.040 --> 01:11:49.160]   And no one says like, well, what, what is the person like describes storyboard in a,
[01:11:49.160 --> 01:11:51.200]   in a high fidelity way?
[01:11:51.200 --> 01:11:53.920]   What is the magical experience for the end user?
[01:11:53.920 --> 01:11:54.920]   Right.
[01:11:54.920 --> 01:11:58.320]   And a lot of companies don't get around to that until they put a decade of R&D work into
[01:11:58.320 --> 01:11:59.320]   it.
[01:11:59.320 --> 01:12:00.320]   And then they just need to like shove something out the door.
[01:12:00.320 --> 01:12:01.320]   Right.
[01:12:01.320 --> 01:12:02.320]   Rarely is successful.
[01:12:02.320 --> 01:12:03.320]   Go ahead, Luis.
[01:12:03.320 --> 01:12:06.400]   I was just going to say that I think that's totally right.
[01:12:06.400 --> 01:12:07.520]   This is not user centric.
[01:12:07.520 --> 01:12:12.760]   I think that there's a huge race to sell people the next personal tech item, right?
[01:12:12.760 --> 01:12:17.480]   There's kind of been all of this fuss in the last couple of years about smart speakers,
[01:12:17.480 --> 01:12:19.480]   about things like that that you could have in your home.
[01:12:19.480 --> 01:12:24.880]   There was kind of the whole rush about air pods, you know, the next generation of headphones.
[01:12:24.880 --> 01:12:28.520]   But if there's going to be another device like glasses that we're going to take everywhere
[01:12:28.520 --> 01:12:31.000]   with us, that would be huge.
[01:12:31.000 --> 01:12:34.760]   So I think that there's really big push for like, let's sell you another form of hardware
[01:12:34.760 --> 01:12:37.480]   that you're going to need all the time.
[01:12:37.480 --> 01:12:38.800]   And it seems natural, right?
[01:12:38.800 --> 01:12:42.880]   Like, it's not very good to have a phone neck.
[01:12:42.880 --> 01:12:46.960]   You know, I think we would all love a kind of technology where you're looking straight,
[01:12:46.960 --> 01:12:47.960]   right?
[01:12:47.960 --> 01:12:50.840]   And like actually integrate the world because like your cell phone doesn't integrate with
[01:12:50.840 --> 01:12:51.840]   the world well.
[01:12:51.840 --> 01:12:55.480]   You know, it sucks to walk around looking down at your phone and trying to get where
[01:12:55.480 --> 01:12:58.480]   you're going or answer messages, et cetera.
[01:12:58.480 --> 01:13:01.080]   But you know, I don't know if anybody really asked for glasses.
[01:13:01.080 --> 01:13:05.680]   It just seems like the natural step and it'll be really up to these companies to see.
[01:13:05.680 --> 01:13:07.000]   But everyone's doing it, right?
[01:13:07.000 --> 01:13:08.680]   Like even Apple has these glasses now.
[01:13:08.680 --> 01:13:11.960]   I mean, Amazon has these glasses now that they're working on too.
[01:13:11.960 --> 01:13:13.360]   Yeah, the Alexa.
[01:13:13.360 --> 01:13:14.360]   Lisa.
[01:13:14.360 --> 01:13:15.360]   That's what we tried.
[01:13:15.360 --> 01:13:16.360]   Yeah.
[01:13:16.360 --> 01:13:17.360]   Yeah.
[01:13:17.360 --> 01:13:24.000]   Lisa, my wife just bought a long arm with a clamp on it that she could clamp to exercise
[01:13:24.000 --> 01:13:27.800]   equipment that would hold her iPad over her head.
[01:13:27.800 --> 01:13:29.320]   Beautiful.
[01:13:29.320 --> 01:13:30.480]   This is exactly it.
[01:13:30.480 --> 01:13:32.520]   This is consumer demand.
[01:13:32.520 --> 01:13:38.840]   And like imagine like an Uber driver, for example, like that's a great use case for
[01:13:38.840 --> 01:13:43.760]   this sort of stuff because every time I get in a ride hailing car or whatever, you see
[01:13:43.760 --> 01:13:49.240]   that they have that like, same, it's like an arm that they have attached to their car,
[01:13:49.240 --> 01:13:50.240]   right?
[01:13:50.240 --> 01:13:51.240]   It's the same thing.
[01:13:51.240 --> 01:13:54.920]   And it's like, seems like getting rid of all those weird, you know, movable arms would
[01:13:54.920 --> 01:13:55.920]   be awesome.
[01:13:55.920 --> 01:14:02.040]   But so in a way, I think that it is user driven, at least in the sense that we know users
[01:14:02.040 --> 01:14:07.480]   love their smartphones and are kind of viscerally connected to them.
[01:14:07.480 --> 01:14:09.640]   I would break it up just a little bit.
[01:14:09.640 --> 01:14:11.840]   Users love and need their screen.
[01:14:11.840 --> 01:14:13.520]   And but I think that's actually a mistake.
[01:14:13.520 --> 01:14:17.360]   I feel like we're like, you need the screen, right?
[01:14:17.360 --> 01:14:21.880]   The Uber, the television, but you can't put a screen on your glasses, which is where a
[01:14:21.880 --> 01:14:24.800]   lot of things have been kind of focused, right?
[01:14:24.800 --> 01:14:29.320]   But when we actually try that as people, we're like, yes, for certain jobs.
[01:14:29.320 --> 01:14:30.800]   So maybe if you're an Uber driver.
[01:14:30.800 --> 01:14:35.080]   But I think most of us probably, maybe I'm crazy, don't want to walk around looking at
[01:14:35.080 --> 01:14:36.080]   the screen.
[01:14:36.080 --> 01:14:41.560]   No, that's why we want something either in spectacles or in our ear that we want that
[01:14:41.560 --> 01:14:43.560]   if we want that whatever it was.
[01:14:43.560 --> 01:14:49.520]   And by the way, let's be honest, it's not an information thing.
[01:14:49.520 --> 01:14:59.040]   We're junkies and we want to somehow jack in all the time to that doom scroll feed, whatever
[01:14:59.040 --> 01:15:00.040]   it is, right?
[01:15:00.040 --> 01:15:01.040]   Yeah, I don't know.
[01:15:01.040 --> 01:15:05.080]   I mean, maybe that is, yeah, it's probably part of it for a lot of people.
[01:15:05.080 --> 01:15:09.040]   I would like it so I could like get some where because I have the world's worst sense of
[01:15:09.040 --> 01:15:10.040]   direction.
[01:15:10.040 --> 01:15:12.040]   I'm like, I would be happy if my glasses would just vibrate.
[01:15:12.040 --> 01:15:13.040]   Yeah.
[01:15:13.040 --> 01:15:14.640]   Actually, I know I'm with you.
[01:15:14.640 --> 01:15:17.880]   I love that in the Apple watch that you have to use Apple Maps.
[01:15:17.880 --> 01:15:21.800]   I never want to, but if you use Apple Maps, the direction show up on your watch while you're
[01:15:21.800 --> 01:15:25.280]   driving or better yet, even while you're while you're walking, that's really useful.
[01:15:25.280 --> 01:15:28.360]   And it buzzes at you and tells you turn left and all I like that.
[01:15:28.360 --> 01:15:29.360]   I agree with you.
[01:15:29.360 --> 01:15:34.480]   I can't point though because despite how useful that is in a lot of ways, watches, smart watches
[01:15:34.480 --> 01:15:38.200]   have also remained a relatively niche consumer product.
[01:15:38.200 --> 01:15:41.400]   It's like, you never meet anyone who doesn't have a smartphone anymore.
[01:15:41.400 --> 01:15:43.920]   But if you have an Apple watch, it's like, okay, cool.
[01:15:43.920 --> 01:15:44.920]   You have an Apple watch.
[01:15:44.920 --> 01:15:49.080]   It's not like out of the ordinary, but I can count my friends on one hand probably that
[01:15:49.080 --> 01:15:50.760]   use their Apple watch every day.
[01:15:50.760 --> 01:15:54.000]   I think we're at 20% on wearables actually.
[01:15:54.000 --> 01:15:56.320]   So Apple Fitbit, those kind of things.
[01:15:56.320 --> 01:15:59.320]   It's not anything that we thought it would perhaps.
[01:15:59.320 --> 01:16:00.320]   It's getting there.
[01:16:00.320 --> 01:16:01.320]   It's getting there.
[01:16:01.320 --> 01:16:02.320]   Yeah.
[01:16:02.320 --> 01:16:04.080]   I see a lot more people with Apple watches than before.
[01:16:04.080 --> 01:16:08.320]   And I even myself, at first the Apple watch was, eh, no big deal.
[01:16:08.320 --> 01:16:09.320]   Now I wear it every day.
[01:16:09.320 --> 01:16:10.320]   I have to have it on.
[01:16:10.320 --> 01:16:14.280]   I feel like a Fitbit, even though we're going to, it's obviously in the wearables category,
[01:16:14.280 --> 01:16:18.960]   I feel like it's a little different and you're not getting that same integration of Apple
[01:16:18.960 --> 01:16:20.360]   Maps the way that you would.
[01:16:20.360 --> 01:16:24.560]   But yeah, that's more than I would have expected, but it's still pretty small.
[01:16:24.560 --> 01:16:25.560]   It's maybe-
[01:16:25.560 --> 01:16:27.280]   It's failed because it's too small.
[01:16:27.280 --> 01:16:29.280]   It's not, you can't use it.
[01:16:29.280 --> 01:16:33.320]   So it's almost there, but it's not that usable, I think.
[01:16:33.320 --> 01:16:34.320]   It's the front.
[01:16:34.320 --> 01:16:37.240]   It's like the slightly more polite way to be addicted to tech.
[01:16:37.240 --> 01:16:39.760]   You're still glancing at your watch all the time.
[01:16:39.760 --> 01:16:42.200]   But I can't scroll through TikTok on my watch.
[01:16:42.200 --> 01:16:45.040]   If I could, maybe I would be doing that all the time.
[01:16:45.040 --> 01:16:47.280]   But you're wearing it all the time in terms of function.
[01:16:47.280 --> 01:16:48.280]   I'm sorry.
[01:16:48.280 --> 01:16:49.280]   Oh, it definitely does.
[01:16:49.280 --> 01:16:50.280]   Yeah, go ahead.
[01:16:50.280 --> 01:16:56.360]   I think there's two main approaches to product design that I've seen that I've tried to
[01:16:56.360 --> 01:17:02.240]   be involved with, and both of them take into the extreme, generally produce pretty crappy
[01:17:02.240 --> 01:17:03.240]   results.
[01:17:03.240 --> 01:17:11.000]   There's a design that is very user-centric that's specifically about solving real problems,
[01:17:11.000 --> 01:17:14.520]   often problems that are actually so common that they're hard to see because everyone
[01:17:14.520 --> 01:17:16.600]   just takes them for granted.
[01:17:16.600 --> 01:17:17.600]   And you just solve those problems.
[01:17:17.600 --> 01:17:21.080]   And you don't care if the resulting product is ugly or anything like that.
[01:17:21.080 --> 01:17:24.920]   And the ultimate example of that is a cup phone.
[01:17:24.920 --> 01:17:26.920]   This thing that you see like advertised all the time.
[01:17:26.920 --> 01:17:27.920]   Yeah, the weather tech cup phone.
[01:17:27.920 --> 01:17:29.560]   I actually bought one.
[01:17:29.560 --> 01:17:31.480]   First of all, it's not a cup phone.
[01:17:31.480 --> 01:17:32.480]   It's a phone cup.
[01:17:32.480 --> 01:17:33.480]   Right?
[01:17:33.480 --> 01:17:35.040]   Just let's get that straight.
[01:17:35.040 --> 01:17:39.320]   But second of all, like obviously that wasn't designed because somebody was like, okay, people
[01:17:39.320 --> 01:17:41.920]   are looking for something else to shove into their cup.
[01:17:41.920 --> 01:17:42.920]   No.
[01:17:42.920 --> 01:17:43.920]   Right?
[01:17:43.920 --> 01:17:44.920]   No, but it was a real problem.
[01:17:44.920 --> 01:17:45.920]   Like people have a hard time.
[01:17:45.920 --> 01:17:47.720]   There's no good way they get a phone holder.
[01:17:47.720 --> 01:17:48.720]   That's right.
[01:17:48.720 --> 01:17:50.600]   I never even knew this existed.
[01:17:50.600 --> 01:17:51.600]   You guys, wow.
[01:17:51.600 --> 01:17:52.600]   Oh, give you mine.
[01:17:52.600 --> 01:17:56.640]   But like I don't like, oh, I know so many people could look useless.
[01:17:56.640 --> 01:17:57.640]   Okay, sorry.
[01:17:57.640 --> 01:17:58.640]   Right.
[01:17:58.640 --> 01:18:00.800]   No, no disrespect to weather tech or whoever.
[01:18:00.800 --> 01:18:02.200]   It's a product is ugly.
[01:18:02.200 --> 01:18:03.720]   It's got a stupid name.
[01:18:03.720 --> 01:18:05.040]   It's all of that plastic.
[01:18:05.040 --> 01:18:06.040]   And I'm sure they're making more.
[01:18:06.040 --> 01:18:07.520]   Totally solves like a real problem.
[01:18:07.520 --> 01:18:08.520]   Yeah.
[01:18:08.520 --> 01:18:10.080]   And this is like great user centric design.
[01:18:10.080 --> 01:18:11.680]   The opposite example of the cut.
[01:18:11.680 --> 01:18:14.600]   And it was like, I'm not sure how much they spent on R&D, but I'm guessing it was like
[01:18:14.600 --> 01:18:15.760]   not under zero.
[01:18:15.760 --> 01:18:16.760]   Right?
[01:18:16.760 --> 01:18:17.760]   Yeah.
[01:18:17.760 --> 01:18:18.760]   Right.
[01:18:18.760 --> 01:18:23.240]   And the example of that is some of this like very tech driven stuff.
[01:18:23.240 --> 01:18:30.440]   It's the AR headsets as things that are entirely driven by a company thinking, okay, we need
[01:18:30.440 --> 01:18:32.920]   to capitalize on this tech trend.
[01:18:32.920 --> 01:18:34.120]   It's not solving any problem.
[01:18:34.120 --> 01:18:35.480]   No one asks for it.
[01:18:35.480 --> 01:18:42.520]   They're spending a lot of money on research and design.
[01:18:42.520 --> 01:18:47.080]   And often what's produced is like amazingly impressive and like thoughtful and sometimes
[01:18:47.080 --> 01:18:49.880]   beautiful, but it's not user centric.
[01:18:49.880 --> 01:18:50.880]   Right.
[01:18:50.880 --> 01:18:54.600]   And like the best products are in the Venn diagram intersection of those two things where
[01:18:54.600 --> 01:19:01.320]   it's like you've identified an actual real problem, often a problem that is so common
[01:19:01.320 --> 01:19:04.400]   that no one else is even talking about it because we all have it.
[01:19:04.400 --> 01:19:06.600]   Therefore, it's hard to even point out.
[01:19:06.600 --> 01:19:11.000]   And then you apply like, okay, just enough technology to solve it in an elegant way in
[01:19:11.000 --> 01:19:13.520]   a way that couldn't have been solved a few years ago.
[01:19:13.520 --> 01:19:16.680]   But you're not like, you're not bending over backwards to use the newest technology.
[01:19:16.680 --> 01:19:18.120]   That's what we try to do.
[01:19:18.120 --> 01:19:20.440]   That's what we try to do with this product with.
[01:19:20.440 --> 01:19:25.960]   We said like, all of us right now hate being on video meetings 14 hours a day.
[01:19:25.960 --> 01:19:27.160]   It's really tiresome.
[01:19:27.160 --> 01:19:28.160]   It's boring.
[01:19:28.160 --> 01:19:29.160]   Okay.
[01:19:29.160 --> 01:19:30.160]   That's a real problem.
[01:19:30.160 --> 01:19:32.600]   That is like, that is the cup phone problem.
[01:19:32.600 --> 01:19:35.560]   But let's try to solve that in a way that's like a little bit more elegant.
[01:19:35.560 --> 01:19:39.640]   And you know, and so like it's that Venn diagram intersection that we're always striving
[01:19:39.640 --> 01:19:43.520]   for and very often in a bunch of these other projects like Magic Leap, like they were
[01:19:43.520 --> 01:19:45.520]   way on one side of the spectrum or.
[01:19:45.520 --> 01:19:47.120]   That's a good point.
[01:19:47.120 --> 01:19:48.120]   Find the middle.
[01:19:48.120 --> 01:19:49.120]   I love this framework, Phil.
[01:19:49.120 --> 01:19:50.120]   It's so smart.
[01:19:50.120 --> 01:19:51.120]   I will never forget this.
[01:19:51.120 --> 01:19:52.120]   That's smart.
[01:19:52.120 --> 01:19:53.120]   Yeah.
[01:19:53.120 --> 01:19:54.120]   Although I will say I kind of want a cup phone now.
[01:19:54.120 --> 01:19:55.120]   Oh, I have one.
[01:19:55.120 --> 01:19:56.120]   They were pretty great.
[01:19:56.120 --> 01:19:57.120]   They've got one.
[01:19:57.120 --> 01:19:58.120]   They're pretty great.
[01:19:58.120 --> 01:20:05.200]   My partner and I just got a car on Craigslist and it's too old to have like whatever we
[01:20:05.200 --> 01:20:06.200]   don't have.
[01:20:06.200 --> 01:20:09.360]   You can't display directions in the middle console or whatever.
[01:20:09.360 --> 01:20:10.360]   So it's really annoying.
[01:20:10.360 --> 01:20:12.480]   But I think the cup phone would solve our issues.
[01:20:12.480 --> 01:20:15.920]   They also have those amounts that go in the air vent.
[01:20:15.920 --> 01:20:17.680]   Yeah, I've done those.
[01:20:17.680 --> 01:20:19.440]   It's a lot better than the cup holder.
[01:20:19.440 --> 01:20:21.480]   It's actually a good product.
[01:20:21.480 --> 01:20:26.480]   It's just, well, it comes with like six cups because you have to size.
[01:20:26.480 --> 01:20:27.480]   You have to size.
[01:20:27.480 --> 01:20:29.800]   It has to fit your cup holder.
[01:20:29.800 --> 01:20:30.960]   And apparently I didn't know this.
[01:20:30.960 --> 01:20:33.080]   There are many different sizes of cup holders.
[01:20:33.080 --> 01:20:38.160]   So you figure out which cup the now I have and I don't want to ever throw them out and
[01:20:38.160 --> 01:20:40.160]   keep thinking a different car.
[01:20:40.160 --> 01:20:42.520]   I have these extra cups.
[01:20:42.520 --> 01:20:45.600]   Well, I will let you know if we don't have the same cup.
[01:20:45.600 --> 01:20:46.600]   I will take your.
[01:20:46.600 --> 01:20:48.120]   You could take my extra cups.
[01:20:48.120 --> 01:20:49.120]   You could take my orders.
[01:20:49.120 --> 01:20:51.120]   Yeah, I have extra cups.
[01:20:51.120 --> 01:20:52.120]   Thank you.
[01:20:52.120 --> 01:20:53.120]   Good to know.
[01:20:53.120 --> 01:20:57.120]   It would be better if it weren't kind of cheap plastic.
[01:20:57.120 --> 01:21:01.880]   It's not should, you know, if it's your, if Apple made it would be milled aluminum.
[01:21:01.880 --> 01:21:04.960]   And it would cost like a thousand dollars.
[01:21:04.960 --> 01:21:06.960]   Yeah, but it would be five thousand dollars.
[01:21:06.960 --> 01:21:07.960]   Yeah.
[01:21:07.960 --> 01:21:08.960]   It would be okay.
[01:21:08.960 --> 01:21:09.960]   It would be perfect.
[01:21:09.960 --> 01:21:13.400]   And it would, you know, it would balance perfectly.
[01:21:13.400 --> 01:21:14.400]   This is not, it's not.
[01:21:14.400 --> 01:21:15.880]   I love that Venn diagram.
[01:21:15.880 --> 01:21:16.880]   Wait, close.
[01:21:16.880 --> 01:21:17.880]   Wait, wait, wait.
[01:21:17.880 --> 01:21:19.640]   Some segments of the economy will never recover.
[01:21:19.640 --> 01:21:22.680]   So that's a Venn diagram on the left.
[01:21:22.680 --> 01:21:23.680]   It says close.
[01:21:23.680 --> 01:21:26.160]   You don't need to work from home.
[01:21:26.160 --> 01:21:27.280]   And then on the right says close.
[01:21:27.280 --> 01:21:28.960]   You can't wear in front of a giant.
[01:21:28.960 --> 01:21:33.800]   Look how small he made himself in front of a giant, in front of a green screen.
[01:21:33.800 --> 01:21:39.240]   And then in the Venn diagram, the intersection of that is green pants.
[01:21:39.240 --> 01:21:42.840]   And I will say also, I appreciate the yellow and blue combining to green.
[01:21:42.840 --> 01:21:43.840]   It's a green.
[01:21:43.840 --> 01:21:44.840]   Oh, it's clever.
[01:21:44.840 --> 01:21:45.840]   Yeah.
[01:21:45.840 --> 01:21:48.640]   It was a nice, it was a nice touch of the, yeah, this is Carlos.
[01:21:48.640 --> 01:21:49.840]   One of our illustrators made it.
[01:21:49.840 --> 01:21:52.760]   And I'm like, Carlos, this is a lot nicer than I expected.
[01:21:52.760 --> 01:21:55.080]   There is no future for green pants.
[01:21:55.080 --> 01:21:56.080]   Yeah.
[01:21:56.080 --> 01:21:57.080]   But we did.
[01:21:57.080 --> 01:22:01.560]   We need, you know, green pants emoji for slack and now like in, in, in, you know, if
[01:22:01.560 --> 01:22:05.840]   something gets labeled with a green pants emoji, that means like, yeah, this is a ridiculously
[01:22:05.840 --> 01:22:09.760]   unnecessary thing that no one will ever need again.
[01:22:09.760 --> 01:22:13.320]   The green pants is the ultimate, the ultimate product design that you don't want to be.
[01:22:13.320 --> 01:22:15.280]   That is fantastic.
[01:22:15.280 --> 01:22:16.280]   That is so good.
[01:22:16.280 --> 01:22:21.440]   We're, you know, it's really nice to have smart people on the show who can, I can learn
[01:22:21.440 --> 01:22:23.320]   and I'm learning, I'm learning right now.
[01:22:23.320 --> 01:22:24.320]   I think that's great.
[01:22:24.320 --> 01:22:25.920]   Thank you, Phil Libin for being here.
[01:22:25.920 --> 01:22:31.760]   Louise Mazzakis from Wired magazine, Stacy Higginbotham, but a fantastic panel.
[01:22:31.760 --> 01:22:35.560]   I'll find some more meat you can sink your teeth into in just a little bit.
[01:22:35.560 --> 01:22:38.160]   But first a word from worldwide technology.
[01:22:38.160 --> 01:22:43.560]   Last trip, I'll always remember this is the last trip I took before shelter in place.
[01:22:43.560 --> 01:22:49.480]   We went out to St. Louis to visit with worldwide technology and I was actually blown away.
[01:22:49.480 --> 01:22:55.920]   This is a company that does enterprise technology for big and medium sized companies.
[01:22:55.920 --> 01:22:56.920]   They help them integrate it.
[01:22:56.920 --> 01:23:01.280]   They help them solve problems and they're amazing.
[01:23:01.280 --> 01:23:02.280]   They're amazing.
[01:23:02.280 --> 01:23:06.880]   Advanced Technology Center, we saw that was incredible.
[01:23:06.880 --> 01:23:08.800]   WWT, that's the initials.
[01:23:08.800 --> 01:23:13.680]   One of the top technology solution providers in the world delivering business and technology
[01:23:13.680 --> 01:23:19.880]   outcomes to large, private and public organizations all over the globe.
[01:23:19.880 --> 01:23:22.080]   They're really in the business of digital transformation.
[01:23:22.080 --> 01:23:27.480]   One of the ways they support organizations with this is helping them adopt a multi-cloud
[01:23:27.480 --> 01:23:28.960]   architecture.
[01:23:28.960 --> 01:23:30.600]   And this is something a lot of people want.
[01:23:30.600 --> 01:23:33.680]   Everybody knows we're moving to the cloud, but multi-cloud, the idea multi-cloud is,
[01:23:33.680 --> 01:23:36.280]   "Well, I don't want to be dependent on any one vendor."
[01:23:36.280 --> 01:23:38.640]   But of course it's complex.
[01:23:38.640 --> 01:23:45.680]   So you need a partner who understands this, who can help you implement it, who can get
[01:23:45.680 --> 01:23:46.760]   it working for you.
[01:23:46.760 --> 01:23:47.920]   And that's WWT.
[01:23:47.920 --> 01:23:51.400]   They stay with their customers, with their organizations, every step of the way from
[01:23:51.400 --> 01:23:56.160]   the vision and the strategy to the enablement to the migration to the optimization of the
[01:23:56.160 --> 01:23:58.680]   management all the way and everything in between.
[01:23:58.680 --> 01:24:03.480]   Their cloud consultants will sit down with you, formulate a vision and a strategy because
[01:24:03.480 --> 01:24:06.640]   they know you've got to do that upfront.
[01:24:06.640 --> 01:24:10.040]   One of the things WWT is brilliant on is process.
[01:24:10.040 --> 01:24:16.040]   They spend a lot of energy and time studying process and figuring out what's the best way
[01:24:16.040 --> 01:24:17.040]   to do this.
[01:24:17.040 --> 01:24:21.880]   They know that investments in these cloud architectures only pay off if they align to
[01:24:21.880 --> 01:24:24.560]   your business goals, for instance.
[01:24:24.560 --> 01:24:28.240]   So they'll work with you through briefings, through workshops.
[01:24:28.240 --> 01:24:33.320]   They do this with some of the biggest organizations in the world to unravel the complexities of
[01:24:33.320 --> 01:24:37.000]   the cloud so that you can do what you're really there to do.
[01:24:37.000 --> 01:24:38.480]   It's not about the cloud for you.
[01:24:38.480 --> 01:24:42.960]   It's about unlocking business opportunities and that's what WWT supports.
[01:24:42.960 --> 01:24:44.920]   They're experts in cloud migrations.
[01:24:44.920 --> 01:24:48.400]   They do application development too, in fact some amazing stuff.
[01:24:48.400 --> 01:24:50.280]   That completely surprised me.
[01:24:50.280 --> 01:24:52.640]   I had no idea about that part of the company.
[01:24:52.640 --> 01:24:57.360]   They can create for you a secure landing zone in any cloud with on-demand labs that give
[01:24:57.360 --> 01:25:01.720]   organizations access to tools for microservices and cloud native development.
[01:25:01.720 --> 01:25:04.720]   Once you're up in the cloud, WWT helps with management.
[01:25:04.720 --> 01:25:06.400]   That's also a problem, right?
[01:25:06.400 --> 01:25:08.840]   Optimization because they know it's a continuum.
[01:25:08.840 --> 01:25:10.720]   You don't just put it up there and forget it.
[01:25:10.720 --> 01:25:16.280]   You've got to continually pay attention to it, especially in a detailed way.
[01:25:16.280 --> 01:25:20.640]   Whether the strategy calls for cloud native, hybrid cloud or on-prem resources, it doesn't
[01:25:20.640 --> 01:25:21.640]   matter.
[01:25:21.640 --> 01:25:26.600]   WWT works closely with you, their customers and with Intel to optimize for the latest
[01:25:26.600 --> 01:25:33.280]   cloud smart solutions based on Intel technology, for security, for performance, for agility.
[01:25:33.280 --> 01:25:36.480]   They also feature Intel, Optane Persistent Memory.
[01:25:36.480 --> 01:25:39.200]   I love this stuff.
[01:25:39.200 --> 01:25:40.440]   Really interesting technology.
[01:25:40.440 --> 01:25:45.400]   Other Intel technologies work load optimized to deliver affordable large capacity and data
[01:25:45.400 --> 01:25:49.680]   persistence for solutions supporting everything from VDI to data analytics.
[01:25:49.680 --> 01:25:50.680]   Great company.
[01:25:50.680 --> 01:25:53.840]   They call themselves Silicon Valley and St. Louis but they're available anywhere in the
[01:25:53.840 --> 01:25:56.120]   world 24/7.
[01:25:56.120 --> 01:26:01.680]   If you want to know more about why organizations across the globe in every industry, in government
[01:26:01.680 --> 01:26:05.880]   and private, turn to WWT to guide them on their cloud journey.
[01:26:05.880 --> 01:26:06.880]   Just go to the website.
[01:26:06.880 --> 01:26:11.880]   There's lots of information there, including all the incredible info on the Advanced Technology
[01:26:11.880 --> 01:26:12.880]   Center.
[01:26:12.880 --> 01:26:13.880]   It's W.
[01:26:13.880 --> 01:26:14.880]   That's their lab.
[01:26:14.880 --> 01:26:15.880]   It's incredible.
[01:26:15.880 --> 01:26:16.880]   WWT.com/twit.
[01:26:16.880 --> 01:26:22.080]   Don't forget, if you create a MyWWT account there, you can access resources throughout WWT's
[01:26:22.080 --> 01:26:28.960]   ATC ecosystem, half a billion dollars in enterprise technology in the ATC.
[01:26:28.960 --> 01:26:30.600]   We visited it, started in one little building.
[01:26:30.600 --> 01:26:32.680]   It's spread to four or five now.
[01:26:32.680 --> 01:26:34.600]   It's kind of mind-boggling.
[01:26:34.600 --> 01:26:37.840]   www.wt.com/twit.
[01:26:37.840 --> 01:26:43.800]   Worldwide technology delivering business and technology outcomes all over the world.
[01:26:43.800 --> 01:26:52.040]   We had a fun week this week, including during our after hours on Friday, a game of clue
[01:26:52.040 --> 01:26:53.120]   on
[01:26:53.120 --> 01:26:57.320]   I thought we should all enjoy this little movie previously on Twitter.
[01:26:57.320 --> 01:27:00.360]   Oh shoot, I already knew that.
[01:27:00.360 --> 01:27:05.520]   This is how you're not supposed to play the game.
[01:27:05.520 --> 01:27:08.720]   Sometimes people playing clue badly.
[01:27:08.720 --> 01:27:11.560]   People without a clue.
[01:27:11.560 --> 01:27:12.800]   Hands on tech.
[01:27:12.800 --> 01:27:17.040]   If you want true wireless earphones that emphasize sound over everything else, then
[01:27:17.040 --> 01:27:20.840]   you're going to want to check this review out as the Sennheiser Momentum True Wireless
[01:27:20.840 --> 01:27:22.960]   Two Earphones.
[01:27:22.960 --> 01:27:24.160]   Tech News Weekly.
[01:27:24.160 --> 01:27:31.400]   We had this smart home summit and it kind of covered for developers what Google has brought
[01:27:31.400 --> 01:27:33.160]   forth in the smart home.
[01:27:33.160 --> 01:27:37.520]   The next Android 11 when it comes out is going, you're going to be able to touch the power
[01:27:37.520 --> 01:27:42.600]   button on your phone and you're going to be able to see your smart home stuff.
[01:27:42.600 --> 01:27:45.760]   I got to tell you, I love this feature.
[01:27:45.760 --> 01:27:46.760]   Security now.
[01:27:46.760 --> 01:27:49.760]   Still, in this day and age.
[01:27:49.760 --> 01:27:52.200]   One, two, three, four, five, six.
[01:27:52.200 --> 01:27:58.920]   Turns out to comprise one out of every 142 passwords.
[01:27:58.920 --> 01:27:59.920]   It's crazy.
[01:27:59.920 --> 01:28:01.440]   Found on the Internet.
[01:28:01.440 --> 01:28:02.440]   This week in Google.
[01:28:02.440 --> 01:28:07.880]   Yeah, amusement parks are reopening, but because screaming is dangerous in Japan, they're saying
[01:28:07.880 --> 01:28:12.600]   you may ride the roller coaster, but you cannot scream.
[01:28:12.600 --> 01:28:14.600]   Please scream inside your heart.
[01:28:14.600 --> 01:28:16.000]   That's exactly how I feel.
[01:28:16.000 --> 01:28:17.000]   To it.
[01:28:17.000 --> 01:28:19.440]   We're all screaming inside our heart right now.
[01:28:19.440 --> 01:28:24.080]   That is the phrase of the week.
[01:28:24.080 --> 01:28:26.480]   We're screaming in our hearts.
[01:28:26.480 --> 01:28:27.720]   Holy cow.
[01:28:27.720 --> 01:28:33.760]   Stacy, you paid close attention to that smart home event.
[01:28:33.760 --> 01:28:36.960]   I'm not all over the smart home, but could you summarize a little bit?
[01:28:36.960 --> 01:28:40.040]   I have to summarize it for the third time this week.
[01:28:40.040 --> 01:28:41.040]   Yeah, sorry.
[01:28:41.040 --> 01:28:42.040]   I'm going to.
[01:28:42.040 --> 01:28:45.840]   They've got a new program called works with Google, right?
[01:28:45.840 --> 01:28:47.960]   It's works with Hage.
[01:28:47.960 --> 01:28:48.960]   I'm going to.
[01:28:48.960 --> 01:28:54.360]   It's the whole I don't want to set anyone's phone off.
[01:28:54.360 --> 01:28:58.320]   The branding around the event was works with Hage.
[01:28:58.320 --> 01:29:04.240]   They still called it works with G when they were talking about it, which felt a little
[01:29:04.240 --> 01:29:07.480]   anti-Google because they usually are pretty good on their branding.
[01:29:07.480 --> 01:29:08.520]   They used to have works with Nest.
[01:29:08.520 --> 01:29:16.280]   The big news from that thing was actually one, I never want to do a media product lunch
[01:29:16.280 --> 01:29:17.280]   thing again.
[01:29:17.280 --> 01:29:20.600]   The virtual was awesome because it was so efficient.
[01:29:20.600 --> 01:29:21.600]   Yeah.
[01:29:21.600 --> 01:29:22.600]   That's just me.
[01:29:22.600 --> 01:29:23.600]   No, I'm with you.
[01:29:23.600 --> 01:29:26.440]   I'm never going to do a trade show ever again.
[01:29:26.440 --> 01:29:31.920]   They borrowed from Apple with putting the home kit home app on the phone that's easily
[01:29:31.920 --> 01:29:32.920]   accessible.
[01:29:32.920 --> 01:29:37.120]   When you're on your Android phone, you hit the power button and you will now see or in
[01:29:37.120 --> 01:29:41.840]   Android 11, you will see all of your smart home devices and you can control them really
[01:29:41.840 --> 01:29:43.920]   quickly and easily from the app.
[01:29:43.920 --> 01:29:44.920]   That's good.
[01:29:44.920 --> 01:29:48.120]   They also brought local control, which is good for latency.
[01:29:48.120 --> 01:29:50.640]   Things are happening faster in your home.
[01:29:50.640 --> 01:29:55.200]   Most of the partners are lighting focused and then there's also tile.
[01:29:55.200 --> 01:30:02.160]   Then they introduced shed devices, which is smart home entertainment devices.
[01:30:02.160 --> 01:30:10.520]   That's TVs, that sort of thing, bringing the AV component into the works with Google ecosystem.
[01:30:10.520 --> 01:30:11.920]   Those were the big things.
[01:30:11.920 --> 01:30:16.480]   I feel like there was one thing that I missed, but maybe you'll ask me about it.
[01:30:16.480 --> 01:30:17.880]   I'm done.
[01:30:17.880 --> 01:30:19.880]   Thank you very much, Stacy.
[01:30:19.880 --> 01:30:20.880]   Great.
[01:30:20.880 --> 01:30:23.800]   Third time's charm.
[01:30:23.800 --> 01:30:30.720]   Stacy is, at least when you lived in Austin, you had a very smart home.
[01:30:30.720 --> 01:30:34.000]   Are you going to reinstall all of that stuff in the new house?
[01:30:34.000 --> 01:30:35.000]   Yeah.
[01:30:35.000 --> 01:30:37.200]   Well, I'm going to reinstall new stuff in the new house.
[01:30:37.200 --> 01:30:38.200]   Nice.
[01:30:38.200 --> 01:30:42.600]   I will once again make my life miserable so nobody else has to.
[01:30:42.600 --> 01:30:45.560]   It's your husband I really feel for.
[01:30:45.560 --> 01:30:46.560]   You should.
[01:30:46.560 --> 01:30:47.560]   He's really, and my daughter.
[01:30:47.560 --> 01:30:50.280]   They're both very patient, very patient.
[01:30:50.280 --> 01:30:53.880]   Mom, tell me again how we opened the blinds.
[01:30:53.880 --> 01:30:57.240]   The nice thing is I'm no longer traveling.
[01:30:57.240 --> 01:31:00.840]   It's not like they have to call me up in the middle of the night wherever I am.
[01:31:00.840 --> 01:31:03.000]   I don't know how this works.
[01:31:03.000 --> 01:31:04.000]   All of our questions.
[01:31:04.000 --> 01:31:05.000]   I've all had one.
[01:31:05.000 --> 01:31:07.640]   I don't know what's going on.
[01:31:07.640 --> 01:31:08.640]   Yeah.
[01:31:08.640 --> 01:31:12.640]   I get the feeling Louise does not have a smart home.
[01:31:12.640 --> 01:31:13.640]   I do not.
[01:31:13.640 --> 01:31:15.960]   I don't have any smart home devices.
[01:31:15.960 --> 01:31:19.480]   I'm pretty low tech, which is boring sometimes.
[01:31:19.480 --> 01:31:23.560]   I do like some fun gadgets, but I don't even have an external monitor when I'm working
[01:31:23.560 --> 01:31:25.320]   from home, which a lot of people find.
[01:31:25.320 --> 01:31:27.280]   You work on a laptop, but you don't have anything else.
[01:31:27.280 --> 01:31:28.280]   It's just a laptop.
[01:31:28.280 --> 01:31:29.280]   It's just a laptop.
[01:31:29.280 --> 01:31:30.280]   Is that scary?
[01:31:30.280 --> 01:31:32.280]   No, we're going to be that's bad for you.
[01:31:32.280 --> 01:31:33.280]   Oh, I know.
[01:31:33.280 --> 01:31:35.160]   That's why you've got phone neck.
[01:31:35.160 --> 01:31:36.160]   It's laptop neck.
[01:31:36.160 --> 01:31:37.160]   I know.
[01:31:37.160 --> 01:31:38.360]   But I move around a lot.
[01:31:38.360 --> 01:31:41.760]   I feel like I've tried to use a monitor, but then I just end up on the couch or on the
[01:31:41.760 --> 01:31:44.000]   bed and it never really sticks.
[01:31:44.000 --> 01:31:47.680]   My partner has two external monitors and I think his back is in a lot better shape
[01:31:47.680 --> 01:31:48.680]   in mind.
[01:31:48.680 --> 01:31:50.960]   I will agree I'm in the wrong here.
[01:31:50.960 --> 01:31:52.480]   No, I don't think you're in the wrong.
[01:31:52.480 --> 01:31:54.280]   I think you're in the right.
[01:31:54.280 --> 01:31:58.680]   I think it's also generational because I think young, maybe it's because you're not yet
[01:31:58.680 --> 01:32:01.240]   fully settled.
[01:32:01.240 --> 01:32:06.120]   You don't want to put in a smart thermostat and a smart lock and smart lamps and all that
[01:32:06.120 --> 01:32:07.120]   stuff.
[01:32:07.120 --> 01:32:10.800]   That's kind of more middle-aged where you're kind of, "Okay, we're going to really trick
[01:32:10.800 --> 01:32:11.960]   this place out."
[01:32:11.960 --> 01:32:16.880]   But I don't think anybody in your generation cares one-witt about this stuff.
[01:32:16.880 --> 01:32:19.640]   It doesn't bode well for its future.
[01:32:19.640 --> 01:32:23.240]   So I was going to tell the homeowners, which is a good point.
[01:32:23.240 --> 01:32:24.240]   Yeah.
[01:32:24.240 --> 01:32:26.840]   Yeah, if you're renting, that's the last thing you wanted to do.
[01:32:26.840 --> 01:32:27.680]   But I even look at my daughter.
[01:32:27.680 --> 01:32:29.360]   I gave her, she's 28.
[01:32:29.360 --> 01:32:36.160]   I gave her a Google home, what are they?
[01:32:36.160 --> 01:32:37.760]   Humb Nest Home.
[01:32:37.760 --> 01:32:38.760]   The mini?
[01:32:38.760 --> 01:32:39.760]   The home or the mini?
[01:32:39.760 --> 01:32:40.760]   The screen.
[01:32:40.760 --> 01:32:41.760]   Oh, this is smart.
[01:32:41.760 --> 01:32:43.960]   It's the same that you talk to.
[01:32:43.960 --> 01:32:48.520]   And she likes it, but she uses it in a very minimal way.
[01:32:48.520 --> 01:32:51.720]   And then I look at myself and I kind of do too.
[01:32:51.720 --> 01:32:53.720]   I use it for a kitchen timer, mostly.
[01:32:53.720 --> 01:32:55.720]   What were you going to say, though, Cc?
[01:32:55.720 --> 01:32:59.920]   Do you think that my generation will catch on to these things more as we, like, perhaps
[01:32:59.920 --> 01:33:02.880]   maybe I doubt it, but maybe have more homeownership?
[01:33:02.880 --> 01:33:03.880]   Yeah.
[01:33:03.880 --> 01:33:08.040]   People, so that there are three times in your life that people apparently buy smart home
[01:33:08.040 --> 01:33:09.040]   devices.
[01:33:09.040 --> 01:33:14.720]   It's when you buy a new home, when you have a baby and right after you've been robbed.
[01:33:14.720 --> 01:33:16.120]   Makes total sense.
[01:33:16.120 --> 01:33:17.120]   It does.
[01:33:17.120 --> 01:33:18.120]   Yeah.
[01:33:18.120 --> 01:33:21.240]   I'm just going to recommend something for you because I just got one for my daughter
[01:33:21.240 --> 01:33:23.200]   and so far she's using it.
[01:33:23.200 --> 01:33:29.840]   So I just got her USB-C hub, like an onker one, but they come in like any normal things.
[01:33:29.840 --> 01:33:35.160]   And she just plugs everything into it and when she, and it carries power over too.
[01:33:35.160 --> 01:33:41.040]   So you can just, when you have a setup, so when you're like kind of feeling all icky,
[01:33:41.040 --> 01:33:44.440]   you can just plug right into it and you're then in an ergonomically correct place for
[01:33:44.440 --> 01:33:46.360]   like an hour and then you can go away again.
[01:33:46.360 --> 01:33:47.360]   It's like a doc.
[01:33:47.360 --> 01:33:51.160]   I can't pick mine up because I will disengage from the ethernet.
[01:33:51.160 --> 01:33:53.120]   If I do that, however, I have one right.
[01:33:53.120 --> 01:33:54.120]   It's a good thing to me and I love it.
[01:33:54.120 --> 01:33:58.800]   And it's the best thing ever, especially because I have a MacBook Pro, so I have no ports whatsoever.
[01:33:58.800 --> 01:33:59.800]   You need the dongles.
[01:33:59.800 --> 01:34:00.800]   Yes.
[01:34:00.800 --> 01:34:01.800]   That's why, yes.
[01:34:01.800 --> 01:34:02.800]   Okay.
[01:34:02.800 --> 01:34:03.800]   So, you know, it's so good.
[01:34:03.800 --> 01:34:07.920]   No, that's like the best suggestion and everyone should get one of them because at least
[01:34:07.920 --> 01:34:11.840]   you can just like, yeah, escape the ergonomic zone when I need to.
[01:34:11.840 --> 01:34:15.120]   That is going to be so depressing the Silicon Valley.
[01:34:15.120 --> 01:34:19.440]   We've done a lot of market research and we've discovered what millennials really want.
[01:34:19.440 --> 01:34:20.440]   USB hubs.
[01:34:20.440 --> 01:34:22.280]   I love mine.
[01:34:22.280 --> 01:34:23.280]   It's great.
[01:34:23.280 --> 01:34:24.280]   Oh my God.
[01:34:24.280 --> 01:34:25.840]   It's the worst news ever.
[01:34:25.840 --> 01:34:28.120]   Phil, where do you land on this?
[01:34:28.120 --> 01:34:30.960]   Are you automated?
[01:34:30.960 --> 01:34:34.720]   I am the dumbest thing in my house.
[01:34:34.720 --> 01:34:40.520]   Do you have smart lights, smart kitchen, all that stuff?
[01:34:40.520 --> 01:34:41.520]   Oh, no, no.
[01:34:41.520 --> 01:34:42.840]   Actually, I have no smart home stuff.
[01:34:42.840 --> 01:34:45.960]   It's just still a true statement, though.
[01:34:45.960 --> 01:34:48.120]   So even though you have a dumb home, you're even dumber?
[01:34:48.120 --> 01:34:50.120]   Is that what you're saying?
[01:34:50.120 --> 01:34:51.120]   I'm using the camera.
[01:34:51.120 --> 01:34:52.120]   I'm using the camera.
[01:34:52.120 --> 01:34:53.120]   I'm using the camera.
[01:34:53.120 --> 01:34:58.400]   I used to own a house and it had a lot of smart home stuff in it, but I think I kind
[01:34:58.400 --> 01:35:03.000]   of went through that phase and, you know, now on the other side of it.
[01:35:03.000 --> 01:35:09.760]   I do have a bunch of the thing that you can't say its name or else it'll wake up and start
[01:35:09.760 --> 01:35:10.760]   talking to.
[01:35:10.760 --> 01:35:11.760]   Yeah.
[01:35:11.760 --> 01:35:12.960]   And yeah, I guess I use that.
[01:35:12.960 --> 01:35:16.080]   Are you Amazon, Google, or Apple?
[01:35:16.080 --> 01:35:18.520]   I am mostly Amazon.
[01:35:18.520 --> 01:35:20.640]   Yeah.
[01:35:20.640 --> 01:35:25.600]   Do you worry that there was a really good piece we talked about on this week in Google on
[01:35:25.600 --> 01:35:32.600]   Wednesday in current affairs, the Bezos future, Nathan Robinson, and it really painted a very
[01:35:32.600 --> 01:35:38.520]   dystopian, bleak future in which essentially Amazon rules the world.
[01:35:38.520 --> 01:35:42.080]   Do you worry about Amazon's market dominance?
[01:35:42.080 --> 01:35:44.600]   And it's not just selling stuff, you know.
[01:35:44.600 --> 01:35:49.040]   The goal is to completely control all transactions everywhere.
[01:35:49.040 --> 01:35:56.840]   So I would be about tons of stuff, but I think Amazon might take and ultimately this question
[01:35:56.840 --> 01:36:01.800]   is above my pay grade, but I think Amazon is net positive for the world.
[01:36:01.800 --> 01:36:07.040]   I think the world is better off because Amazon exists in many ways, but that's not to say
[01:36:07.040 --> 01:36:12.040]   that it's not also pretty dangerous in the power that it has.
[01:36:12.040 --> 01:36:17.680]   And ultimately, I think the way I analyze these big companies is which ones have fundamentally
[01:36:17.680 --> 01:36:20.880]   aligned business models and which ones have misaligned business models.
[01:36:20.880 --> 01:36:24.800]   And I think Apple as an example has a pretty aligned business model.
[01:36:24.800 --> 01:36:31.000]   Like Apple wants you to desire their expensive phones and you want to buy their expensive
[01:36:31.000 --> 01:36:32.000]   phones.
[01:36:32.000 --> 01:36:33.000]   And so it's all kind of okay.
[01:36:33.000 --> 01:36:36.960]   They just want you to buy phones and other devices.
[01:36:36.960 --> 01:36:39.880]   So it's like, is it good or bad?
[01:36:39.880 --> 01:36:43.120]   It's like hard to say, but it's at least like everyone wants the same thing.
[01:36:43.120 --> 01:36:44.120]   All in alignment.
[01:36:44.120 --> 01:36:45.120]   Yeah.
[01:36:45.120 --> 01:36:48.000]   Amazon's model is pretty much in alignment.
[01:36:48.000 --> 01:36:49.160]   You want to buy?
[01:36:49.160 --> 01:36:50.160]   They want to sell.
[01:36:50.160 --> 01:36:52.160]   At low price and they want to sell it to you.
[01:36:52.160 --> 01:36:53.160]   So it's okay.
[01:36:53.160 --> 01:36:54.160]   Yeah.
[01:36:54.160 --> 01:36:56.240]   Facebook's business model is fundamentally misaligned.
[01:36:56.240 --> 01:36:58.040]   Oh, that's a good point.
[01:36:58.040 --> 01:36:59.040]   Fundamentally.
[01:36:59.040 --> 01:37:04.280]   Like you want to be there to talk to your friends or whatever and they want to target
[01:37:04.280 --> 01:37:05.280]   you with ads.
[01:37:05.280 --> 01:37:06.320]   You don't want the ads.
[01:37:06.320 --> 01:37:09.240]   And it's like a fundamental misalignment.
[01:37:09.240 --> 01:37:13.920]   Google's business model is very much aligned around their initial core business, which
[01:37:13.920 --> 01:37:14.920]   was search ads.
[01:37:14.920 --> 01:37:15.920]   Right?
[01:37:15.920 --> 01:37:17.760]   Like search ads are the only ads that are aligned.
[01:37:17.760 --> 01:37:21.560]   Like if I'm, if I'm googling for something, it's because I have a high intention of getting
[01:37:21.560 --> 01:37:22.560]   it.
[01:37:22.560 --> 01:37:24.000]   And so if they show me an ad for it, that's actually not bad.
[01:37:24.000 --> 01:37:25.240]   That's pretty good alignment.
[01:37:25.240 --> 01:37:27.360]   But they're pretty misaligned in almost every other business.
[01:37:27.360 --> 01:37:28.360]   Brilliant.
[01:37:28.360 --> 01:37:31.440]   And when you look at the places where they get into trouble, it's in their misaligned
[01:37:31.440 --> 01:37:32.440]   business.
[01:37:32.440 --> 01:37:33.440]   Right.
[01:37:33.440 --> 01:37:35.080]   I don't get into trouble with the search ads, even though that's like the bulk of their
[01:37:35.080 --> 01:37:39.000]   revenue, they get into trouble with like YouTube, which is fundamentally misaligned.
[01:37:39.000 --> 01:37:40.000]   Right?
[01:37:40.000 --> 01:37:42.760]   Like YouTube doesn't want what YouTube viewers want.
[01:37:42.760 --> 01:37:43.760]   Right.
[01:37:43.760 --> 01:37:47.760]   So in that sense, like I worry a lot less about Amazon because I think they're, yeah,
[01:37:47.760 --> 01:37:49.560]   they're a giant, very powerful company.
[01:37:49.560 --> 01:37:55.400]   But I kind of, I understand the business model and it's aligned with what customers want.
[01:37:55.400 --> 01:38:00.160]   I worry more about some of the, these other companies which have, which have no real alignment.
[01:38:00.160 --> 01:38:02.640]   The risk though with alignment is it's very effective.
[01:38:02.640 --> 01:38:04.360]   It's like a flywheel.
[01:38:04.360 --> 01:38:06.320]   And it becomes very, very powerful.
[01:38:06.320 --> 01:38:07.320]   Sorry, Louise.
[01:38:07.320 --> 01:38:08.320]   Go ahead.
[01:38:08.320 --> 01:38:12.040]   I think, yeah, I think your misalignment point is really interesting because I think
[01:38:12.040 --> 01:38:14.400]   that we also have to think about the scale of that, right?
[01:38:14.400 --> 01:38:19.360]   Like, so it seems aligned for customers in the short term to get a spatula in one day
[01:38:19.360 --> 01:38:20.360]   or less.
[01:38:20.360 --> 01:38:25.400]   However, the consequences of that for the environment, for the workers involved.
[01:38:25.400 --> 01:38:26.400]   Exactly.
[01:38:26.400 --> 01:38:30.640]   For the third party sellers whose intellectual property was ripped off and Amazon did nothing
[01:38:30.640 --> 01:38:31.640]   about it.
[01:38:31.640 --> 01:38:33.960]   I don't think they're really aligned.
[01:38:33.960 --> 01:38:34.960]   So I think it's like what-
[01:38:34.960 --> 01:38:36.960]   They put spatula city out of business as we-
[01:38:36.960 --> 01:38:37.960]   Yes.
[01:38:37.960 --> 01:38:44.520]   But I mean, I think like, you know, what happens when all of this, you know, retail innovation
[01:38:44.520 --> 01:38:48.840]   of these like small companies, which are basically forced to be Amazon sellers because that's
[01:38:48.840 --> 01:38:49.840]   the main platform.
[01:38:49.840 --> 01:38:55.120]   You know, I've just seen them be treated so poorly by Amazon because they're not really
[01:38:55.120 --> 01:38:59.760]   incentivized to, they are, they should be, but I think that they've gotten so big that
[01:38:59.760 --> 01:39:02.560]   they're not necessarily incentivized to help these small sellers.
[01:39:02.560 --> 01:39:06.360]   And a lot of them have moved away or tried to, but I think that Amazon is so big.
[01:39:06.360 --> 01:39:10.200]   But with that said, I do think some of the things that are held up as like the scariest
[01:39:10.200 --> 01:39:16.680]   things about Amazon, for example, like the fact that they're selling generic label products
[01:39:16.680 --> 01:39:20.920]   that are similar to, you know, other products for sale that people hold that up a lot is
[01:39:20.920 --> 01:39:23.880]   like, "Oh, well, like Amazon's trying to do everything."
[01:39:23.880 --> 01:39:25.760]   And you know, they're selling all these generic products.
[01:39:25.760 --> 01:39:27.480]   It's like, "Have you been inside Sephora?
[01:39:27.480 --> 01:39:29.240]   Have you been in a Walgreens?"
[01:39:29.240 --> 01:39:31.200]   Like I don't find that very scary.
[01:39:31.200 --> 01:39:36.600]   But I do think like, what is the end of like, you know, a drone is going to bring you whatever
[01:39:36.600 --> 01:39:38.000]   you want an hour or less.
[01:39:38.000 --> 01:39:40.400]   Like that's kind of what seems to be Bezos' vision.
[01:39:40.400 --> 01:39:45.240]   And I do worry about the consequences of that for, and so they're own employees, right?
[01:39:45.240 --> 01:39:50.120]   About for the environment for, you know, we're just going to have houses full of cheap stuff
[01:39:50.120 --> 01:39:52.440]   that we mindlessly bought on Amazon.
[01:39:52.440 --> 01:39:53.960]   I do worry about things like that.
[01:39:53.960 --> 01:39:58.440]   And their alignment is to do as much of that as possible.
[01:39:58.440 --> 01:40:00.360]   And that sort of thing I worry about.
[01:40:00.360 --> 01:40:05.960]   And I do worry that like, my values are not really aligned with Bezos' often and he has,
[01:40:05.960 --> 01:40:09.200]   you know, enormous, only increasing power.
[01:40:09.200 --> 01:40:10.520]   So there are things to be worried about.
[01:40:10.520 --> 01:40:14.400]   But yeah, I don't know if there's a, you know, it's like either giving us, Amazon's giving
[01:40:14.400 --> 01:40:17.800]   us what we want quote unquote, but is that what we're going to want in the long term
[01:40:17.800 --> 01:40:21.440]   and what's good for like the health of the planet and for ourselves.
[01:40:21.440 --> 01:40:22.440]   I don't know.
[01:40:22.440 --> 01:40:25.320]   I felt bad when I was ordering a lot of stuff on Prime.
[01:40:25.320 --> 01:40:27.160]   And I don't have a Prime account anymore.
[01:40:27.160 --> 01:40:32.320]   And I think I do a lot less like mindless consumption because of it.
[01:40:32.320 --> 01:40:33.320]   Interesting.
[01:40:33.320 --> 01:40:38.320]   How do you balance out though the concerns about the, you know, I think everything you
[01:40:38.320 --> 01:40:39.720]   said resonates with me.
[01:40:39.720 --> 01:40:45.520]   I wonder though, like how you weigh hypothetical future problems, like future dangers that
[01:40:45.520 --> 01:40:50.960]   Amazon could pose if they act in certain ways against like actual harm being done by other
[01:40:50.960 --> 01:40:53.560]   companies right now in the way that they are acting.
[01:40:53.560 --> 01:40:58.160]   I think Amazon is posting a tremendous risk to the environment right now and that global
[01:40:58.160 --> 01:40:59.360]   warming is happening right now.
[01:40:59.360 --> 01:41:02.120]   And it's not something we have to think about as hypothetical.
[01:41:02.120 --> 01:41:07.280]   And we know that like, you know, their injury rate is far above the average for, you know,
[01:41:07.280 --> 01:41:08.720]   in similar sorts of situations.
[01:41:08.720 --> 01:41:13.280]   I think that like sometimes Amazon is held up as this labor boogie man, perhaps because
[01:41:13.280 --> 01:41:16.840]   they're so big and I think that we have to look at like, you know, labor practices across
[01:41:16.840 --> 01:41:17.840]   the board.
[01:41:17.840 --> 01:41:20.120]   But I do think that these harms are already occurring.
[01:41:20.120 --> 01:41:21.760]   Like they definitely are already happening.
[01:41:21.760 --> 01:41:26.080]   And I think that, you know, to Amazon's credit after they got so much pushback about the
[01:41:26.080 --> 01:41:30.320]   environment, you know, it think it's phenomenal that Jeff Bezos wants to donate so much of
[01:41:30.320 --> 01:41:34.560]   his wealth to this issue and that they have, you know, really strengthened their climate
[01:41:34.560 --> 01:41:35.560]   policies.
[01:41:35.560 --> 01:41:41.080]   But for a long time, you know, Amazon just was like, it was like crickets about the environment,
[01:41:41.080 --> 01:41:44.840]   whereas a lot of their, you know, competitors, which I know it's hard to compare Apple or
[01:41:44.840 --> 01:41:48.640]   Google to Amazon because, you know, they're very fundamentally different businesses in
[01:41:48.640 --> 01:41:49.640]   some ways.
[01:41:49.640 --> 01:41:54.560]   You know, I think we're leading the way on climate and Amazon was just kind of silent
[01:41:54.560 --> 01:41:59.000]   until their own employees started really acting up about this.
[01:41:59.000 --> 01:42:02.960]   I would also prefer a more heterogeneous environment.
[01:42:02.960 --> 01:42:10.320]   I worry that if it's it becomes the company store, then that's when we see the most predacious
[01:42:10.320 --> 01:42:11.320]   behavior.
[01:42:11.320 --> 01:42:16.400]   Yeah, we like it now, but wouldn't it be in our interest to encourage competition to
[01:42:16.400 --> 01:42:20.280]   Amazon just so that we do have choice down the road?
[01:42:20.280 --> 01:42:24.080]   Well, that's hypothetical.
[01:42:24.080 --> 01:42:25.520]   So maybe, right?
[01:42:25.520 --> 01:42:26.760]   That would make sense.
[01:42:26.760 --> 01:42:33.440]   But that isn't, yeah, I think the environmental impact, the labor impact are very real and
[01:42:33.440 --> 01:42:38.320]   I think that, you know, need to be in happening pressure needs to continue being applied to
[01:42:38.320 --> 01:42:39.160]   it.
[01:42:39.160 --> 01:42:43.120]   A lot, but a lot of the other stuff is hypothetical likely.
[01:42:43.120 --> 01:42:46.360]   Well, are we going to regret it when there's no other options and they're going to do
[01:42:46.360 --> 01:42:47.360]   a jack of prices?
[01:42:47.360 --> 01:42:53.280]   Well, that's, you know, I think we saw a little bit of that though, during the coronavirus,
[01:42:53.280 --> 01:42:57.840]   when a lot of stuff couldn't be delivered via Amazon and suddenly we were like, okay,
[01:42:57.840 --> 01:42:58.840]   get it.
[01:42:58.840 --> 01:43:02.560]   Oh, I can't get my, well, we'll go with spatula because I actually had to buy one.
[01:43:02.560 --> 01:43:05.720]   And all the stories downtown are gone.
[01:43:05.720 --> 01:43:11.160]   Well, they were both closed and I didn't have a place anywhere nearby that I could get one,
[01:43:11.160 --> 01:43:16.160]   but I think you don't have a spatula city in your town.
[01:43:16.160 --> 01:43:20.400]   I don't, I do have a kitchen where they were.
[01:43:20.400 --> 01:43:24.560]   So here's a competitor coming along with Pike.
[01:43:24.560 --> 01:43:28.120]   Walmart is going to start an Amazon prime competitor this month.
[01:43:28.120 --> 01:43:35.480]   $98 a year, it'll be called Walmart plus same day, same day delivery of groceries,
[01:43:35.480 --> 01:43:36.920]   fuel discounts and other perks.
[01:43:36.920 --> 01:43:41.480]   Of course, Walmart can do the same day because they have retail outlets in a great part of
[01:43:41.480 --> 01:43:45.280]   the United States.
[01:43:45.280 --> 01:43:50.040]   So it's in Louise, you decided not to have a prime account.
[01:43:50.040 --> 01:43:53.160]   Would you consider a Walmart plus account?
[01:43:53.160 --> 01:43:55.040]   Absolutely not.
[01:43:55.040 --> 01:44:00.480]   And it's not, it wasn't like, you know, this political statement.
[01:44:00.480 --> 01:44:04.360]   I think sometimes when I tell people, yeah, I got rid of my prime account, it was not,
[01:44:04.360 --> 01:44:08.280]   you know, of course, like, I think that it's important, you know, especially as someone
[01:44:08.280 --> 01:44:13.520]   who covers this company, I think it's, you know, would be a little bit hypocritical if
[01:44:13.520 --> 01:44:16.680]   I was covering this company critically and, you know, I was ordering prime packages every
[01:44:16.680 --> 01:44:17.680]   single day.
[01:44:17.680 --> 01:44:21.120]   There are reporters who do that and that's fine, but it wasn't like a holier than that
[01:44:21.120 --> 01:44:22.120]   decision.
[01:44:22.120 --> 01:44:25.880]   I was just like, I was buying stuff when I thought about it and then I would order it
[01:44:25.880 --> 01:44:30.120]   so quickly on the Amazon app and then get it and be like, oh, you know, I really kind
[01:44:30.120 --> 01:44:33.360]   of wanted this two days ago, but I didn't really need it.
[01:44:33.360 --> 01:44:36.640]   You know, like, it's just so frictionless.
[01:44:36.640 --> 01:44:38.400]   And I think that that's what they're encouraging, right?
[01:44:38.400 --> 01:44:42.280]   That is the, that's the alignment that they want for you.
[01:44:42.280 --> 01:44:46.680]   So I don't want anything like that that just makes it way too easy to buy things.
[01:44:46.680 --> 01:44:47.680]   It's too good.
[01:44:47.680 --> 01:44:48.680]   It's too good.
[01:44:48.680 --> 01:44:49.680]   Yeah.
[01:44:49.680 --> 01:44:53.600]   I think it's like, I want to be more careful and more intentional about what I buy and
[01:44:53.600 --> 01:44:54.600]   where from.
[01:44:54.600 --> 01:44:55.600]   Yeah.
[01:44:55.600 --> 01:44:59.560]   But with that said, I have like waited three weeks for an Amazon order when I couldn't
[01:44:59.560 --> 01:45:01.160]   buy when I wanted anywhere else.
[01:45:01.160 --> 01:45:04.800]   And I'm like, they're really punishing me for not having prime now because now I've waited
[01:45:04.800 --> 01:45:08.040]   like four weeks for this thing that a prime person would have gotten.
[01:45:08.040 --> 01:45:09.040]   It's worse.
[01:45:09.040 --> 01:45:13.520]   Because you can even have prime and still be lately, that's been happening a lot.
[01:45:13.520 --> 01:45:14.520]   Still be right three weeks.
[01:45:14.520 --> 01:45:16.080]   And then you really feel like a sucker.
[01:45:16.080 --> 01:45:17.080]   Go ahead, Stacey.
[01:45:17.080 --> 01:45:19.800]   Well, that, and that's what I was going to say.
[01:45:19.800 --> 01:45:27.360]   This idea of being aligned, they are aligned with, yeah, consumers want to buy, you know,
[01:45:27.360 --> 01:45:28.960]   households of cheap stuff.
[01:45:28.960 --> 01:45:33.680]   And I don't think that that's, that's fine for businesses.
[01:45:33.680 --> 01:45:39.280]   But then if we go with that, then we have to say as a society, and this gets us into
[01:45:39.280 --> 01:45:43.280]   government, which I know everyone has a knee jerk reaction against government, but that's
[01:45:43.280 --> 01:45:46.560]   kind of the stuff that we want to have regulations around.
[01:45:46.560 --> 01:45:52.360]   So if we're going to say, hey, Amazon will deliver me the absolute cheapest, fastest,
[01:45:52.360 --> 01:45:56.480]   whatever I want, they're going to do it at the expense of maybe workers.
[01:45:56.480 --> 01:46:01.600]   They're going to do it at the expense of environmental policy.
[01:46:01.600 --> 01:46:05.760]   And we need to have more robust rules to protect the things we value and add cost to
[01:46:05.760 --> 01:46:10.120]   the things that, you know, in that, that stands in the way of our consumer wants.
[01:46:10.120 --> 01:46:12.680]   And that's kind of the goal of that sort of thing.
[01:46:12.680 --> 01:46:14.480]   But we just don't think like that.
[01:46:14.480 --> 01:46:15.480]   It's frustrating.
[01:46:15.480 --> 01:46:17.920]   Yeah, I agree with that definitely.
[01:46:17.920 --> 01:46:23.040]   And I don't, I don't think that business model alignment with the, with the, with the,
[01:46:23.040 --> 01:46:26.960]   with a customer with the end user is sufficient to, to make a virtuous company.
[01:46:26.960 --> 01:46:27.960]   But I think it's necessary.
[01:46:27.960 --> 01:46:33.160]   So I think if you don't, if you don't even have the alignment, then I can't even, I won't
[01:46:33.160 --> 01:46:35.600]   even know where to begin trying to fix that.
[01:46:35.600 --> 01:46:36.600]   That makes sense.
[01:46:36.600 --> 01:46:37.600]   Yeah.
[01:46:37.600 --> 01:46:38.600]   Yeah.
[01:46:38.600 --> 01:46:39.600]   Phil, what would you say now?
[01:46:39.600 --> 01:46:43.320]   Like I'm curious about what you think now, especially that Amazon's advertising business
[01:46:43.320 --> 01:46:44.400]   is growing so much.
[01:46:44.400 --> 01:46:48.040]   Like I do wonder about that because like, you know, in search results, for example,
[01:46:48.040 --> 01:46:51.960]   they're becoming more like Google, which is kind of interesting because you're getting
[01:46:51.960 --> 01:46:56.120]   potentially the person who paid the most to be that search results on Amazon, not necessarily
[01:46:56.120 --> 01:46:57.640]   like the best product anymore.
[01:46:57.640 --> 01:47:01.840]   So I think that they're kind of playing with fire there because people do trust that alignment
[01:47:01.840 --> 01:47:02.840]   a lot of ways.
[01:47:02.840 --> 01:47:06.320]   But now they're kind of like, we're going to mess with it a little bit by having, you
[01:47:06.320 --> 01:47:11.200]   know, paid search results, paid, you know, suggested items and that sort of thing, which
[01:47:11.200 --> 01:47:13.400]   is kind of interesting to watch.
[01:47:13.400 --> 01:47:14.400]   Yeah.
[01:47:14.400 --> 01:47:19.040]   And there's a ton of concerning things just around like fake reviews, but also like how
[01:47:19.040 --> 01:47:20.040]   they counter it.
[01:47:20.040 --> 01:47:21.040]   Yeah.
[01:47:21.040 --> 01:47:26.280]   How they pick the, the, like the Amazon choice thing, you know, that, that feels.
[01:47:26.280 --> 01:47:27.280]   Yeah.
[01:47:27.280 --> 01:47:28.280]   It's really sturdy.
[01:47:28.280 --> 01:47:29.280]   Yeah.
[01:47:29.280 --> 01:47:30.280]   Really.
[01:47:30.280 --> 01:47:31.280]   Yeah.
[01:47:31.280 --> 01:47:32.280]   Really.
[01:47:32.280 --> 01:47:33.280]   Yeah.
[01:47:33.280 --> 01:47:34.280]   There's lots of things there that are definitely, you know, sketchy.
[01:47:34.280 --> 01:47:38.120]   And I think we need to keep, you know, we need to keep kind of constant vigilance on a company
[01:47:38.120 --> 01:47:39.120]   this powerful.
[01:47:39.120 --> 01:47:44.240]   Do you think we've changed our tune in the last six months about government regulation
[01:47:44.240 --> 01:47:48.920]   that maybe people are a little more amenable to an effective government than they were
[01:47:48.920 --> 01:47:50.400]   before COVID-19?
[01:47:50.400 --> 01:47:54.760]   For the record, I'd be fine waiting at least another six months until we have new government
[01:47:54.760 --> 01:47:55.760]   regulations.
[01:47:55.760 --> 01:47:56.760]   Okay.
[01:47:56.760 --> 01:47:57.760]   Not a bad idea.
[01:47:57.760 --> 01:47:59.480]   We can pull off for just a little bit longer.
[01:47:59.480 --> 01:48:00.880]   Just wait a little bit longer.
[01:48:00.880 --> 01:48:01.880]   Yeah.
[01:48:01.880 --> 01:48:02.880]   I agree.
[01:48:02.880 --> 01:48:03.880]   We're getting new government regulations all the time.
[01:48:03.880 --> 01:48:04.880]   They're just scary.
[01:48:04.880 --> 01:48:06.880]   The wrong government regulations.
[01:48:06.880 --> 01:48:07.880]   Yeah.
[01:48:07.880 --> 01:48:09.400]   Let's say a little break come back.
[01:48:09.400 --> 01:48:10.960]   I, you guys are so smart.
[01:48:10.960 --> 01:48:12.520]   I am, I'm loving this.
[01:48:12.520 --> 01:48:13.960]   Phil Liben is here.
[01:48:13.960 --> 01:48:16.840]   All turtles is his company, his new product.
[01:48:16.840 --> 01:48:17.840]   Mm hmm.
[01:48:17.840 --> 01:48:18.840]   Are you on a green screen?
[01:48:18.840 --> 01:48:20.480]   Cause that is perfect.
[01:48:20.480 --> 01:48:21.480]   That is perfect.
[01:48:21.480 --> 01:48:22.480]   I am.
[01:48:22.480 --> 01:48:23.480]   Okay.
[01:48:23.480 --> 01:48:24.480]   It works.
[01:48:24.480 --> 01:48:26.480]   It works with and without it.
[01:48:26.480 --> 01:48:27.480]   There's like different modes.
[01:48:27.480 --> 01:48:30.760]   And so there's a bunch of stuff you can do that actually works better without a green
[01:48:30.760 --> 01:48:31.760]   screen.
[01:48:31.760 --> 01:48:32.760]   Like a lot of these.
[01:48:32.760 --> 01:48:33.760]   Oh, yeah.
[01:48:33.760 --> 01:48:36.880]   I can do a lot of like kind of head in a circle type of stuff.
[01:48:36.880 --> 01:48:37.880]   That's pretty cool.
[01:48:37.880 --> 01:48:38.880]   That's so cool.
[01:48:38.880 --> 01:48:39.880]   I really like it.
[01:48:39.880 --> 01:48:43.360]   I love the ghost, the ghost.
[01:48:43.360 --> 01:48:44.840]   All of this stuff works great.
[01:48:44.840 --> 01:48:45.840]   Yeah.
[01:48:45.840 --> 01:48:46.840]   Without without a green screen, even better.
[01:48:46.840 --> 01:48:50.920]   But the, the traditional like cut out thing is the background.
[01:48:50.920 --> 01:48:51.920]   Yeah.
[01:48:51.920 --> 01:48:52.920]   It's better with it.
[01:48:52.920 --> 01:48:54.920]   He looks like he's in his New York loft right now.
[01:48:54.920 --> 01:48:55.920]   Yeah.
[01:48:55.920 --> 01:48:57.720]   So it's just a Brooklyn loft.
[01:48:57.720 --> 01:48:58.720]   I love it.
[01:48:58.720 --> 01:49:04.120]   Stacy Higginbotham is in her hipster Seattle loft.
[01:49:04.120 --> 01:49:06.120]   It's my millennial pink background.
[01:49:06.120 --> 01:49:12.520]   It's so great to see you Stacy on IOT is her website Stacy on IOT.com.
[01:49:12.520 --> 01:49:14.720]   And of course sign up for the free newsletter there.
[01:49:14.720 --> 01:49:15.720]   It's really good.
[01:49:15.720 --> 01:49:17.800]   Louise Mizzak is also here from Wired.
[01:49:17.800 --> 01:49:20.360]   It's so nice to see you, Louise.
[01:49:20.360 --> 01:49:21.680]   And the things are going well.
[01:49:21.680 --> 01:49:22.680]   So smart.
[01:49:22.680 --> 01:49:27.000]   It's so fun to do a show with people who have, you know, where you just go.
[01:49:27.000 --> 01:49:28.000]   Oh, oh, that.
[01:49:28.000 --> 01:49:30.720]   Oh, oh, I think this is a great lineup.
[01:49:30.720 --> 01:49:31.920]   It is a good lineup.
[01:49:31.920 --> 01:49:32.920]   It is.
[01:49:32.920 --> 01:49:33.920]   I agree with you.
[01:49:33.920 --> 01:49:34.920]   That's because I'm part of it.
[01:49:34.920 --> 01:49:37.280]   But well, there's certainly a third of it.
[01:49:37.280 --> 01:49:40.200]   So I can say that much.
[01:49:40.200 --> 01:49:41.200]   So I showed it.
[01:49:41.200 --> 01:49:42.600]   I brought to you by Molecule.
[01:49:42.600 --> 01:49:44.640]   Now I am a Molecule fan.
[01:49:44.640 --> 01:49:45.640]   I really am.
[01:49:45.640 --> 01:49:47.480]   But Molecule is not what you think it is.
[01:49:47.480 --> 01:49:52.160]   And I think they maybe got some bad press from people thinking it was, oh, it's an air
[01:49:52.160 --> 01:49:53.480]   filtration system.
[01:49:53.480 --> 01:49:56.480]   Oh, it's so much more than that.
[01:49:56.480 --> 01:50:03.240]   It's for sick office buildings where the indoor air is actually five times worse than
[01:50:03.240 --> 01:50:04.240]   the outdoor air.
[01:50:04.240 --> 01:50:08.040]   EPA says that's very common in office spaces.
[01:50:08.040 --> 01:50:12.480]   It's an air purifier that's it's not about filtration.
[01:50:12.480 --> 01:50:16.520]   It's about pico or photo electrochemical oxidation.
[01:50:16.520 --> 01:50:22.120]   It actually destroys harmful pollutants in the air like viruses, bacteria, mole, and
[01:50:22.120 --> 01:50:23.120]   chemicals.
[01:50:23.120 --> 01:50:26.080]   It doesn't filter them or collect them on filters.
[01:50:26.080 --> 01:50:31.040]   Molecules air purifiers are designed to help protect homes, businesses, and medical spaces.
[01:50:31.040 --> 01:50:32.880]   Destroying pollutants, providing clean air.
[01:50:32.880 --> 01:50:34.720]   We have molecules at home.
[01:50:34.720 --> 01:50:38.040]   One in our bedroom and Lisa always knows when it's not on.
[01:50:38.040 --> 01:50:39.040]   We got one from Michael.
[01:50:39.040 --> 01:50:41.160]   We use one here in the studio.
[01:50:41.160 --> 01:50:42.600]   Molecule air purifiers are designed.
[01:50:42.600 --> 01:50:44.720]   Well, they have a variety of them.
[01:50:44.720 --> 01:50:47.720]   The big ones designed for large rooms up to 600 square feet.
[01:50:47.720 --> 01:50:49.000]   There's the new Molecule air.
[01:50:49.000 --> 01:50:52.200]   Many smaller rooms, 250 square feet.
[01:50:52.200 --> 01:50:57.680]   The Air Mini Plus, which helps protect small rooms with a particle sensor and auto protect
[01:50:57.680 --> 01:51:02.320]   mode so it'll adjust the fan speed based on what's in the air.
[01:51:02.320 --> 01:51:04.960]   And also brand new, the Molecule air Pro-RX.
[01:51:04.960 --> 01:51:10.560]   It's an FDA cleared as a 510k Class II medical device.
[01:51:10.560 --> 01:51:15.320]   It's intended for medical purposes to destroy bacteria and viruses in the air.
[01:51:15.320 --> 01:51:19.240]   It all relies on this Pico technology and you can go to the Molecule website, read the
[01:51:19.240 --> 01:51:20.240]   white papers.
[01:51:20.240 --> 01:51:26.720]   I just read one from Lawrence Labs, Department of Energy Labs, where they tested the Molecule
[01:51:26.720 --> 01:51:32.720]   collecting three really nasty chemicals that are in a lot of sick offices and other spaces
[01:51:32.720 --> 01:51:34.400]   with poor ventilation.
[01:51:34.400 --> 01:51:41.720]   Formaldehyde, it's volatile organic compounds like formaldehyde, ozone, and I forgot what
[01:51:41.720 --> 01:51:42.720]   the third was.
[01:51:42.720 --> 01:51:47.280]   These are things that are off-gassed by carpets and drapes and so forth.
[01:51:47.280 --> 01:51:52.440]   They cleared 95% of the ozone out of the air in minutes.
[01:51:52.440 --> 01:51:56.960]   And as soon as it came back on, they turned it off, the ozone came back up because it's
[01:51:56.960 --> 01:51:58.760]   being off-gassed all the time.
[01:51:58.760 --> 01:51:59.760]   It really works.
[01:51:59.760 --> 01:52:01.640]   You will notice the difference.
[01:52:01.640 --> 01:52:05.200]   Clean air if you're spending more time at home is so important.
[01:52:05.200 --> 01:52:07.720]   But when you go back to the office, it might even be more important.
[01:52:07.720 --> 01:52:13.200]   Pico technology meets the performance requirements in the FDA guidance for use in helping reduce
[01:52:13.200 --> 01:52:16.680]   the risk of exposure to SARS-COVID-2.
[01:52:16.680 --> 01:52:19.880]   That's the COVID-19 virus in healthcare settings.
[01:52:19.880 --> 01:52:21.600]   They got FDA approval.
[01:52:21.600 --> 01:52:24.480]   It meets the FDA guidance.
[01:52:24.480 --> 01:52:29.760]   In independent testing, they reduced concentrations of MS-2, which is a proxy virus they use for
[01:52:29.760 --> 01:52:33.320]   testing, over 99.9% in one hour.
[01:52:33.320 --> 01:52:37.880]   Of course, obviously, that's not enough to protect you.
[01:52:37.880 --> 01:52:42.160]   You still have to do all the other important things like masks and social distancing and
[01:52:42.160 --> 01:52:43.360]   washing your hands.
[01:52:43.360 --> 01:52:50.520]   But it's sure nice to know that you can run your molecule air purifier and clear the air
[01:52:50.520 --> 01:52:57.080]   of those aerosolized particles that quickly, 99.9% in an hour.
[01:52:57.080 --> 01:53:00.680]   Molecules technology and filtration systems have been tested and verified by independent
[01:53:00.680 --> 01:53:04.960]   third-party labs for the whole home and beyond.
[01:53:04.960 --> 01:53:09.320]   Molecule now offers its breakthrough Pico technology across a range of products, including
[01:53:09.320 --> 01:53:12.600]   clean air solutions for a variety of environments.
[01:53:12.600 --> 01:53:14.840]   Pick the unit that's best for your space.
[01:53:14.840 --> 01:53:17.880]   You can also, if you go to the website, create a discounted bundle to protect your entire
[01:53:17.880 --> 01:53:19.600]   home office.
[01:53:19.600 --> 01:53:23.600]   Medical or dental practice, you'll see a lot of them in dentist offices now.
[01:53:23.600 --> 01:53:27.480]   Many businesses, we're really glad we have them here in our studio.
[01:53:27.480 --> 01:53:29.240]   It is a must.
[01:53:29.240 --> 01:53:35.120]   For 10% off your first order, your purifier order at Molecule, visit Molecule.com.
[01:53:35.120 --> 01:53:37.960]   Enter the code TWIT10 at checkout.
[01:53:37.960 --> 01:53:40.240]   This is not your grandpa's air filter.
[01:53:40.240 --> 01:53:44.200]   This is something very new, very different and really works.
[01:53:44.200 --> 01:53:45.200]   M-O-L-E-K-U-L-E.com.
[01:53:45.200 --> 01:53:50.720]   Enter the offer code TWIT10.
[01:53:50.720 --> 01:53:52.520]   Molecule.
[01:53:52.520 --> 01:53:54.640]   Really nice.
[01:53:54.640 --> 01:54:04.160]   Let us, I know you guys are going to be starving any minute now.
[01:54:04.160 --> 01:54:10.080]   Let me do a rundown of some of the other stories, the mini stories that we can talk about.
[01:54:10.080 --> 01:54:16.640]   We mentioned that when Mixer closed down, a couple of streamers were left hanging, but
[01:54:16.640 --> 01:54:18.160]   they made a lot of money.
[01:54:18.160 --> 01:54:23.640]   Ninja, we think, made more than $20 million when Microsoft lured him away from Twitch.
[01:54:23.640 --> 01:54:27.800]   He's now back streaming, but he didn't go back to Twitch.
[01:54:27.800 --> 01:54:30.000]   He could have, but now he's on YouTube.
[01:54:30.000 --> 01:54:32.720]   You got to wonder, did he get another payday?
[01:54:32.720 --> 01:54:36.680]   Ninja is the king of Fortnite streamers.
[01:54:36.680 --> 01:54:42.040]   Mixer was a brief flirtation, but he's now on YouTube.
[01:54:42.040 --> 01:54:49.720]   I bet you, we don't know, but I bet you Google offered him a little something come over.
[01:54:49.720 --> 01:54:53.480]   I'm sure both Twitch and YouTube.
[01:54:53.480 --> 01:54:59.080]   By the way, Facebook too, because they now have a gaming platform, we're scrambling to
[01:54:59.080 --> 01:55:02.160]   make him an offer he could not refuse.
[01:55:02.160 --> 01:55:04.080]   I kind of feel bad for him, honestly.
[01:55:04.080 --> 01:55:05.640]   He's like, "Mucks suck."
[01:55:05.640 --> 01:55:11.880]   Yeah, I mean, it must suck to be the premier person on this new platform.
[01:55:11.880 --> 01:55:13.680]   And then, yeah.
[01:55:13.680 --> 01:55:18.120]   Yeah, I feel like it's like, if you made a Quibi show, it's not great.
[01:55:18.120 --> 01:55:21.280]   I don't feel bad for, there are a lot of Hollywood celebrities on Quibi.
[01:55:21.280 --> 01:55:22.280]   I don't feel bad.
[01:55:22.280 --> 01:55:23.280]   They got a paycheck.
[01:55:23.280 --> 01:55:24.280]   It doesn't.
[01:55:24.280 --> 01:55:25.280]   Nobody's like this.
[01:55:25.280 --> 01:55:28.920]   Yeah, but like there's a lot of journalists too, who like, no name journalists such as
[01:55:28.920 --> 01:55:31.680]   myself who made things on Quibi and it didn't really fit in out.
[01:55:31.680 --> 01:55:33.280]   Do you have a Quibi show?
[01:55:33.280 --> 01:55:34.800]   No, I do not have a Quibi show.
[01:55:34.800 --> 01:55:40.240]   But I mean, people at my level who took a risk in their normal career to be part of
[01:55:40.240 --> 01:55:41.240]   the platform.
[01:55:41.240 --> 01:55:42.240]   Oh, I do.
[01:55:42.240 --> 01:55:43.240]   I feel bad for them, yeah.
[01:55:43.240 --> 01:55:47.600]   Yeah, I think NINJA is a little bit bigger than that, but I think that's a lot of money.
[01:55:47.600 --> 01:55:48.600]   He made a whole lot of money.
[01:55:48.600 --> 01:55:51.920]   Yeah, he probably got a big chunk of change to go there, but I think that's like now
[01:55:51.920 --> 01:55:52.920]   what?
[01:55:52.920 --> 01:55:53.920]   I mean, he just crawled back.
[01:55:53.920 --> 01:55:58.200]   I mean, he has a lot of fans, I'm sure he'll be fine, but it is kind of like it shows that
[01:55:58.200 --> 01:56:02.000]   you can't just have one anchor person and there are a couple of anchor people and hope
[01:56:02.000 --> 01:56:03.000]   that that's enough.
[01:56:03.000 --> 01:56:06.440]   I think with these, another great example of that, you have to have like, I don't know.
[01:56:06.440 --> 01:56:10.480]   I think maybe this area is just too saturated the same way that streaming was too saturated.
[01:56:10.480 --> 01:56:14.480]   Be like if Howard Stern went to Sirius and then the satellite fell out of the sky.
[01:56:14.480 --> 01:56:16.800]   Be like, now you have to put a scale between his legs.
[01:56:16.800 --> 01:56:18.320]   Well, it sounds like just no one came.
[01:56:18.320 --> 01:56:21.640]   I mean, it's kind of like an, I mean, it's an ego thing is what it sounds like you're talking
[01:56:21.640 --> 01:56:22.640]   about.
[01:56:22.640 --> 01:56:25.960]   Like he was paid a lot of money to bring an audience and he didn't.
[01:56:25.960 --> 01:56:28.080]   You think that's what happened?
[01:56:28.080 --> 01:56:31.360]   I think maybe you brought some of his audience, but like that's one guy.
[01:56:31.360 --> 01:56:34.400]   It's like, it's just not enough to like sustain a platform.
[01:56:34.400 --> 01:56:35.920]   It is asking.
[01:56:35.920 --> 01:56:38.680]   So what about Quibi?
[01:56:38.680 --> 01:56:40.800]   It sounds like you wrote the obituary, Louise.
[01:56:40.800 --> 01:56:43.840]   Do you think it'll, there's, is it, there's no hope?
[01:56:43.840 --> 01:56:45.080]   I don't know.
[01:56:45.080 --> 01:56:48.960]   I mean, I think like it, there's not a lot of downloads.
[01:56:48.960 --> 01:56:49.960]   I mean, who knows?
[01:56:49.960 --> 01:56:50.960]   It might change.
[01:56:50.960 --> 01:56:55.840]   I don't want to be caught being the person who was kind of me into Quibi.
[01:56:55.840 --> 01:56:57.000]   But I just, I don't know.
[01:56:57.000 --> 01:57:02.800]   I think that that short kind of content that they were talking about is like not something
[01:57:02.800 --> 01:57:04.320]   I've ever heard anyone say.
[01:57:04.320 --> 01:57:05.880]   Like I've never heard anyone be like, I want.
[01:57:05.880 --> 01:57:08.760]   Yeah, I wish I had a seven minute TV show I could watch.
[01:57:08.760 --> 01:57:09.760]   Yeah.
[01:57:09.760 --> 01:57:11.080]   It's like that's called YouTube.
[01:57:11.080 --> 01:57:13.960]   I think that TikTok was smart and that they went even shorter than that.
[01:57:13.960 --> 01:57:18.240]   Like I don't think my brain is not, I don't want to watch a seven minute video to get this
[01:57:18.240 --> 01:57:19.240]   quick thing.
[01:57:19.240 --> 01:57:20.800]   It's called like a podcast.
[01:57:20.800 --> 01:57:26.680]   Like there's plenty of like short daily news podcasts that I can listen to while I like
[01:57:26.680 --> 01:57:27.920]   get ready for work or whatever.
[01:57:27.920 --> 01:57:29.800]   You know, that was in the before times at least now.
[01:57:29.800 --> 01:57:34.600]   It's like, well, I just brush my teeth and change it into my home work pajamas instead
[01:57:34.600 --> 01:57:36.680]   of my sleep ones.
[01:57:36.680 --> 01:57:37.840]   But yeah, I don't know.
[01:57:37.840 --> 01:57:39.360]   I, I'm not sure about Quibi.
[01:57:39.360 --> 01:57:40.800]   I've only seen some of the shows.
[01:57:40.800 --> 01:57:45.080]   A lot of it seems like over, over produced.
[01:57:45.080 --> 01:57:46.720]   But I think it's like an interesting concept.
[01:57:46.720 --> 01:57:48.600]   But I just think like, I don't know.
[01:57:48.600 --> 01:57:53.560]   I don't know if like having it also be like, it was really hard to share Quibi content
[01:57:53.560 --> 01:57:54.560]   elsewhere.
[01:57:54.560 --> 01:57:58.280]   And that was one thing that TikTok did really smart is like, just download a TikTok video
[01:57:58.280 --> 01:57:59.280]   easily.
[01:57:59.280 --> 01:58:00.280]   And you can send it to people who don't have TikTok.
[01:58:00.280 --> 01:58:03.800]   You can upload it to your Instagram story.
[01:58:03.800 --> 01:58:05.520]   And that's something that they really didn't do very well.
[01:58:05.520 --> 01:58:06.520]   It did mixer.
[01:58:06.520 --> 01:58:10.360]   Like I never saw like, you know, here's what happened on mixer today or on by like, I
[01:58:10.360 --> 01:58:13.840]   think all these like new platforms have to really think about like, you know, kind of
[01:58:13.840 --> 01:58:18.400]   paradoxically how are people going to share our stuff elsewhere?
[01:58:18.400 --> 01:58:24.120]   Well, this is a moment of truth for Quibi because they offered three month trials.
[01:58:24.120 --> 01:58:27.400]   And I think yesterday I signed up like day one.
[01:58:27.400 --> 01:58:30.240]   And I think mine expired yesterday.
[01:58:30.240 --> 01:58:34.800]   So this is a bad week if you're Jeffrey Katzberg and Meg Whitman, because you're waiting to
[01:58:34.800 --> 01:58:40.520]   see, well, how many of those people who who downloaded and signed up are staying with
[01:58:40.520 --> 01:58:41.520]   the platform?
[01:58:41.520 --> 01:58:43.080]   And I have to say, what are you going to do?
[01:58:43.080 --> 01:58:46.080]   Are you going to cancel it in the first week?
[01:58:46.080 --> 01:58:50.280]   So the flip side of that is it's a good week to check your subscription settings.
[01:58:50.280 --> 01:58:51.280]   Yes.
[01:58:51.280 --> 01:58:52.280]   Yes.
[01:58:52.280 --> 01:58:54.720]   You don't want to be paying for Quibi by accident.
[01:58:54.720 --> 01:58:55.720]   Yeah.
[01:58:55.720 --> 01:58:56.720]   They raised.
[01:58:56.720 --> 01:58:57.720]   Go ahead.
[01:58:57.720 --> 01:59:05.440]   I just for some reason it always, it sort of dripped in with inauthenticity for me.
[01:59:05.440 --> 01:59:08.560]   I don't know why I couldn't put my finger on it.
[01:59:08.560 --> 01:59:12.960]   But just even from the beginning, like from the announcement on onward, it just seemed
[01:59:12.960 --> 01:59:13.960]   inauthentic.
[01:59:13.960 --> 01:59:16.000]   I have to be careful because they don't apply why.
[01:59:16.000 --> 01:59:20.640]   I think I have a natural animus out of Schadenfreude almost.
[01:59:20.640 --> 01:59:26.920]   I wanted them to fail because what it was was Hollywood's idea of what digital content
[01:59:26.920 --> 01:59:28.160]   should be.
[01:59:28.160 --> 01:59:32.600]   And if we use the Hollywood model, they raised one and three quarters billion dollars.
[01:59:32.600 --> 01:59:33.600]   We throw money.
[01:59:33.600 --> 01:59:39.400]   We bring in Hollywood celebrities and we throw money at them and have super high production
[01:59:39.400 --> 01:59:41.080]   values.
[01:59:41.080 --> 01:59:47.600]   And all those dumb kids watching TikTok and YouTube videos will see what real production
[01:59:47.600 --> 01:59:51.040]   and real content looks like and they'll flock to us.
[01:59:51.040 --> 01:59:57.560]   And so I have a little bit of Schadenfreude that no, in fact, native digital content creators
[01:59:57.560 --> 02:00:02.280]   actually do know their audience and they actually creating something their audience wants.
[02:00:02.280 --> 02:00:06.480]   And maybe it isn't a slick and high production and you don't have tightest Burgess in your
[02:00:06.480 --> 02:00:07.580]   show.
[02:00:07.580 --> 02:00:09.500]   But maybe that's what people want.
[02:00:09.500 --> 02:00:11.900]   Maybe they want Tyler Blevins.
[02:00:11.900 --> 02:00:12.900]   Maybe they want Ninja.
[02:00:12.900 --> 02:00:13.900]   Maybe that's what they really want.
[02:00:13.900 --> 02:00:16.820]   I don't know.
[02:00:16.820 --> 02:00:22.780]   According to Karsten Quibi lost everybody but 72,000 subscribers this week.
[02:00:22.780 --> 02:00:25.100]   That's way more than I thought, honestly.
[02:00:25.100 --> 02:00:26.100]   That's bad.
[02:00:26.100 --> 02:00:27.100]   That's really bad.
[02:00:27.100 --> 02:00:28.100]   That's a lot.
[02:00:28.100 --> 02:00:30.180]   I mean, I can't believe how many people signed up.
[02:00:30.180 --> 02:00:32.660]   Oh, they hundreds of thousands signed up.
[02:00:32.660 --> 02:00:34.460]   I mean, yeah, lots of people signed up.
[02:00:34.460 --> 02:00:35.900]   Because it was free for three months.
[02:00:35.900 --> 02:00:36.900]   Yeah.
[02:00:36.900 --> 02:00:38.780]   And you wanted to see what it was about.
[02:00:38.780 --> 02:00:42.980]   There were some ridiculous show premises like shooting food at the chefs and making them
[02:00:42.980 --> 02:00:43.980]   recreate them.
[02:00:43.980 --> 02:00:46.460]   Like, I really enjoyed just the premises.
[02:00:46.460 --> 02:00:52.820]   But I was going to say, I feel like on the internet, even still, even as mature as it
[02:00:52.820 --> 02:00:57.300]   has gotten, people like the relationship they have with people.
[02:00:57.300 --> 02:00:59.060]   Oh, that's interesting.
[02:00:59.060 --> 02:01:00.060]   Yes.
[02:01:00.060 --> 02:01:05.740]   Quibi didn't, I'm just trying to even with YouTube and even with maybe less so with TikTok.
[02:01:05.740 --> 02:01:10.500]   So maybe we're just on this spectrum where we're no longer needing that relationship.
[02:01:10.500 --> 02:01:11.500]   I don't know.
[02:01:11.500 --> 02:01:15.980]   Louise, do you, I don't watch all that much TikTok content.
[02:01:15.980 --> 02:01:21.140]   But do you have, do you think that's true for people who are younger maybe?
[02:01:21.140 --> 02:01:26.540]   I'm going to share something disturbing about myself first to say it to before I to preface
[02:01:26.540 --> 02:01:28.300]   the fact that I feel like a TikTok expert.
[02:01:28.300 --> 02:01:32.860]   I got the notification today from Apple about my screen time and I spent 10 hours a day
[02:01:32.860 --> 02:01:33.860]   on my phone.
[02:01:33.860 --> 02:01:34.860]   Oh my God.
[02:01:34.860 --> 02:01:35.860]   Really bad.
[02:01:35.860 --> 02:01:37.340]   That's definitely a quarantine thing to be fair.
[02:01:37.340 --> 02:01:38.540]   I don't watch any TV.
[02:01:38.540 --> 02:01:39.940]   I don't watch any streaming.
[02:01:39.940 --> 02:01:42.340]   So like, you know, take that what you will.
[02:01:42.340 --> 02:01:43.340]   What are you doing?
[02:01:43.340 --> 02:01:45.260]   Were you doomscrolling Twitter?
[02:01:45.260 --> 02:01:46.260]   What are you doing?
[02:01:46.260 --> 02:01:47.260]   I'm doing Twitter.
[02:01:47.260 --> 02:01:51.060]   A lot of time on TikTok, honestly, I do a lot of my job from my phone because like you
[02:01:51.060 --> 02:01:53.940]   can't message people on TikTok from your desktop.
[02:01:53.940 --> 02:01:57.220]   So like a lot, that's a lot of what it is or like, you know, collecting content and stuff.
[02:01:57.220 --> 02:01:58.380]   You have a perfect excuse.
[02:01:58.380 --> 02:01:59.340]   That really doesn't justify it.
[02:01:59.340 --> 02:02:00.340]   This is your word.
[02:02:00.340 --> 02:02:01.340]   Yeah.
[02:02:01.340 --> 02:02:04.700]   However, given that, I would say to your question, no, I think you're going to say,
[02:02:04.700 --> 02:02:09.140]   I think that there still is a lot about like wanting the personal relationship.
[02:02:09.140 --> 02:02:13.820]   A lot of TikTok celebrities just started out as normal people sharing their lives.
[02:02:13.820 --> 02:02:17.700]   Like a lot of this stuff is almost just like micro vlogging.
[02:02:17.700 --> 02:02:19.380]   That's a very like technical way of putting it.
[02:02:19.380 --> 02:02:21.420]   But just people being like, here's what I did today.
[02:02:21.420 --> 02:02:24.820]   Here's like, you know, here's a funny thing my dog did or whatever.
[02:02:24.820 --> 02:02:28.500]   And I think it's very personal, very low production value.
[02:02:28.500 --> 02:02:30.420]   Much lower than YouTube actually.
[02:02:30.420 --> 02:02:33.420]   Like, you know, a lot of these people don't even have ring lights, you know.
[02:02:33.420 --> 02:02:38.180]   So I think that definitely there's definitely a pretty...
[02:02:38.180 --> 02:02:39.180]   Oh my God.
[02:02:39.180 --> 02:02:40.180]   What amateurs.
[02:02:40.180 --> 02:02:44.060]   I don't have one, but I'm saying like, you know, if you're making videos of the internet,
[02:02:44.060 --> 02:02:47.140]   you know, it's like the first thing you buy.
[02:02:47.140 --> 02:02:51.780]   So I think, I think, yeah, I think that personal connection is still really important.
[02:02:51.780 --> 02:02:56.780]   That's a great point about Quibi is that like, it was, yeah, maybe these household names,
[02:02:56.780 --> 02:03:00.140]   but they didn't really work to like have that relationship.
[02:03:00.140 --> 02:03:04.580]   I've never seen like, you know, Quibi native stars like engaging with people on Twitter
[02:03:04.580 --> 02:03:05.580]   or anything like that.
[02:03:05.580 --> 02:03:06.580]   I think it's just really hard.
[02:03:06.580 --> 02:03:07.580]   They're Hollywood.
[02:03:07.580 --> 02:03:08.580]   They're celebrities.
[02:03:08.580 --> 02:03:09.580]   Right exactly.
[02:03:09.580 --> 02:03:14.420]   Honestly, if I'm Katzenberg and Whitman, I'm looking at John Krasinski's massive success
[02:03:14.420 --> 02:03:19.300]   with some good news on YouTube and going, I don't...
[02:03:19.300 --> 02:03:22.660]   What did we do wrong?
[02:03:22.660 --> 02:03:24.540]   It's pathetic, honestly.
[02:03:24.540 --> 02:03:29.100]   By the way, I'm looking at my screen time for the last week, averaged an hour and three
[02:03:29.100 --> 02:03:30.100]   minutes a day.
[02:03:30.100 --> 02:03:32.500]   So Louise, you have a problem.
[02:03:32.500 --> 02:03:33.500]   I know, I know.
[02:03:33.500 --> 02:03:36.340]   I definitely have a problem and perhaps I should get like more into TV.
[02:03:36.340 --> 02:03:39.620]   To be fair, I also listen to a lot of podcasts and it includes that.
[02:03:39.620 --> 02:03:43.500]   So a lot of the time I'm actually looking at my phone, like I'm like cooking or something
[02:03:43.500 --> 02:03:45.100]   and it's playing a podcast.
[02:03:45.100 --> 02:03:46.820]   I'm going to be really fair.
[02:03:46.820 --> 02:03:51.540]   If you added all my screen time, not just my iPhone screen time, well over 10 hours a
[02:03:51.540 --> 02:03:52.540]   day.
[02:03:52.540 --> 02:03:53.540]   Oh, for sure.
[02:03:53.540 --> 02:03:58.620]   Almost all every waking minute is spent staring at a screen.
[02:03:58.620 --> 02:04:02.380]   So I'm at six hours, so five hours and 58 minutes.
[02:04:02.380 --> 02:04:04.020]   So you know what?
[02:04:04.020 --> 02:04:05.020]   You're fine.
[02:04:05.020 --> 02:04:12.020]   I feel like other people on this podcast are not, cannot tell me.
[02:04:12.020 --> 02:04:15.580]   Like they're not a good barometer, but I'm not to know you're at least 60% of the way
[02:04:15.580 --> 02:04:17.940]   there to me and that makes me feel a little better.
[02:04:17.940 --> 02:04:19.660]   So what do you got?
[02:04:19.660 --> 02:04:20.660]   38.
[02:04:20.660 --> 02:04:22.180]   I don't have to 38 hours a day.
[02:04:22.180 --> 02:04:23.900]   Yeah, I thought so.
[02:04:23.900 --> 02:04:27.260]   Clearly, clearly an overachiever in the business.
[02:04:27.260 --> 02:04:28.260]   Clearly.
[02:04:28.260 --> 02:04:30.660]   Two big monitors and then phones and iPads.
[02:04:30.660 --> 02:04:31.660]   Yeah, all the time.
[02:04:31.660 --> 02:04:33.740]   Got them up definitely more than a hour.
[02:04:33.740 --> 02:04:34.740]   Yeah.
[02:04:34.740 --> 02:04:37.420]   I thought you were going to say 38 minutes and you were going to be like, I actually
[02:04:37.420 --> 02:04:41.700]   meditate before I was a day and I was going to be like, oh, no, I feel horrible now.
[02:04:41.700 --> 02:04:42.700]   Who is that?
[02:04:42.700 --> 02:04:43.700]   Jack Dorsey who does that?
[02:04:43.700 --> 02:04:44.700]   Oh, no.
[02:04:44.700 --> 02:04:50.300]   Yuval Nora Harari, the author of all those great books like Homo.
[02:04:50.300 --> 02:04:52.620]   The author of one great book and a few okay books.
[02:04:52.620 --> 02:04:53.620]   Okay.
[02:04:53.620 --> 02:04:54.620]   Sapiens was the great one.
[02:04:54.620 --> 02:04:55.620]   Which one was the great one?
[02:04:55.620 --> 02:04:56.620]   Yeah.
[02:04:56.620 --> 02:04:57.620]   Sapiens was the great one.
[02:04:57.620 --> 02:05:01.220]   I like 20, 21 answers for the 21st century.
[02:05:01.220 --> 02:05:03.060]   I thought that was, but you're right.
[02:05:03.060 --> 02:05:04.580]   He's like Malcolm Gladwell.
[02:05:04.580 --> 02:05:05.820]   He's kind of recycling.
[02:05:05.820 --> 02:05:10.060]   You know, I think Sapiens was great because like it's not a field that I know much about
[02:05:10.060 --> 02:05:12.020]   and so I learned so much from it.
[02:05:12.020 --> 02:05:17.220]   But then the next one, you know, he starts writing about AI and a few other things and
[02:05:17.220 --> 02:05:20.900]   a little bit more familiar territory and I'm like, yeah, that's not really how any of
[02:05:20.900 --> 02:05:21.900]   this works.
[02:05:21.900 --> 02:05:22.900]   Yeah.
[02:05:22.900 --> 02:05:25.540]   But yeah, Sapiens was mine.
[02:05:25.540 --> 02:05:27.100]   And that was like his big thing.
[02:05:27.100 --> 02:05:32.900]   The New Yorker profile all about him was like, that was his grand unifying thesis of life,
[02:05:32.900 --> 02:05:33.900]   right?
[02:05:33.900 --> 02:05:36.940]   So, you know, I expect if that's what you're working on, but then you get the second book
[02:05:36.940 --> 02:05:39.540]   deal and you're like, well, I can't say no to that.
[02:05:39.540 --> 02:05:42.140]   How can I apply my thesis to something else?
[02:05:42.140 --> 02:05:46.100]   I've got a meditate for hours a day somehow.
[02:05:46.100 --> 02:05:50.580]   Let me quickly get one more add in before we get to our final stories.
[02:05:50.580 --> 02:05:51.580]   What a fun show.
[02:05:51.580 --> 02:05:53.420]   I don't ever want it to end.
[02:05:53.420 --> 02:05:54.660]   But some days it has to.
[02:05:54.660 --> 02:05:56.940]   Let me show you my, this is such a good deal.
[02:05:56.940 --> 02:06:01.220]   This is an iPhone SE, which I scored for Mint Mobile.
[02:06:01.220 --> 02:06:03.700]   It is $15 a month.
[02:06:03.700 --> 02:06:09.620]   And then I got my Mint Mobile service, unlimited nationwide text, talk, and 3 gigabytes of
[02:06:09.620 --> 02:06:13.340]   data, $15 a month for 30 bucks a month.
[02:06:13.340 --> 02:06:18.260]   A third what I'm paying for data, all for internet and, you know, phone calls alone
[02:06:18.260 --> 02:06:19.500]   from the big guys.
[02:06:19.500 --> 02:06:23.140]   I got a great little phone at a great little price.
[02:06:23.140 --> 02:06:24.460]   That's Mint Mobile.
[02:06:24.460 --> 02:06:26.420]   They're selling you the same service.
[02:06:26.420 --> 02:06:29.140]   They run on T-Mobile that you get from the big boys.
[02:06:29.140 --> 02:06:35.380]   But because they have no stores, they have no retail, there's no hidden fees, they give
[02:06:35.380 --> 02:06:38.420]   you an amazing deal.
[02:06:38.420 --> 02:06:43.180]   There's the green Fox who has a surprising resemblance to Ryan Reynolds.
[02:06:43.180 --> 02:06:44.460]   I just noticed that.
[02:06:44.460 --> 02:06:46.180]   Maybe it's the glasses.
[02:06:46.180 --> 02:06:51.780]   Mint Mobile is a no brainer, premium wireless, 15 bucks a month.
[02:06:51.780 --> 02:06:56.500]   By going online only, you're eliminating the cost of retail and they pass that savings
[02:06:56.500 --> 02:06:58.140]   along to you.
[02:06:58.140 --> 02:07:04.420]   Every plan comes with unlimited nationwide talk and text, crazy fast for GLTE.
[02:07:04.420 --> 02:07:06.220]   You can bring your own phone.
[02:07:06.220 --> 02:07:08.180]   They'll send you the SIM, put it in for free.
[02:07:08.180 --> 02:07:10.220]   They'll port your number if you want it.
[02:07:10.220 --> 02:07:16.780]   Or you can get phones, like I said, this iPhone SE, that's a great deal and a great phone.
[02:07:16.780 --> 02:07:17.780]   Switch now to Mint Mobile.
[02:07:17.780 --> 02:07:19.860]   You'll get premium wireless for 15 bucks a month.
[02:07:19.860 --> 02:07:27.420]   If you're not 100% satisfied, they've got your covered seven day money back guarantee.
[02:07:27.420 --> 02:07:29.580]   I actually ended up, they have a variety of plans.
[02:07:29.580 --> 02:07:31.140]   I did the three gigs for a while.
[02:07:31.140 --> 02:07:32.620]   I said, "This is great."
[02:07:32.620 --> 02:07:34.940]   But I thought, "I want more data."
[02:07:34.940 --> 02:07:39.740]   I got the 12 gigabytes a month on the Minute Nationwide Talk and text, 12 gigabytes.
[02:07:39.740 --> 02:07:42.740]   I never go through even half that, but it's nice to have it all.
[02:07:42.740 --> 02:07:43.740]   You can get eight gigabytes.
[02:07:43.740 --> 02:07:45.300]   They sell it in the chunks you want.
[02:07:45.300 --> 02:07:46.940]   25 bucks a month.
[02:07:46.940 --> 02:07:50.020]   So I paid once, $300, got a whole year.
[02:07:50.020 --> 02:07:52.020]   That's 25 bucks a month.
[02:07:52.020 --> 02:07:56.700]   That is a third, less than a third what I'm paying the big boys for the same service.
[02:07:56.700 --> 02:07:59.020]   I think Mint Mobile is awesome.
[02:07:59.020 --> 02:08:05.180]   If you are still overpaying for cell service, mintmobile.com/twit.
[02:08:05.180 --> 02:08:08.620]   Get that three month introductory plan, just $15 a month.
[02:08:08.620 --> 02:08:13.300]   Get the SIM shipped to your door, pop it in a phone or buy a phone from them.
[02:08:13.300 --> 02:08:14.940]   They've got great prices.
[02:08:14.940 --> 02:08:16.620]   You'll be very happy.
[02:08:16.620 --> 02:08:18.620]   Mintmobile.com/twit.
[02:08:18.620 --> 02:08:23.940]   Get your wireless bill down to $15 a month.
[02:08:23.940 --> 02:08:26.300]   Mintmobile.com/twit.
[02:08:26.300 --> 02:08:30.340]   We thank them so much for their support for the show.
[02:08:30.340 --> 02:08:37.460]   I have to say, when you're wearing face masks, touch ID, fingerprint, not a bad idea.
[02:08:37.460 --> 02:08:42.020]   This phone is so cool, plus it fits in my pocket.
[02:08:42.020 --> 02:08:44.020]   Mintmobile.com/twit.
[02:08:44.020 --> 02:08:50.900]   Boy, still a few stories to go, but we're getting long.
[02:08:50.900 --> 02:08:56.780]   Jack Dorsey donates $3 million to basic universal income.
[02:08:56.780 --> 02:09:00.740]   This is Silicon Valley's hope for the future.
[02:09:00.740 --> 02:09:08.180]   When Yang was running for president, it makes even more sense now, I think.
[02:09:08.180 --> 02:09:10.420]   Yes?
[02:09:10.420 --> 02:09:11.620]   No comment.
[02:09:11.620 --> 02:09:16.500]   Twitter hints at a future subscription service.
[02:09:16.500 --> 02:09:18.700]   Twit will not be doing that, by the way.
[02:09:18.700 --> 02:09:25.340]   But when Twitter mentioned this in a job listing, their stock price went up 10%.
[02:09:25.340 --> 02:09:27.940]   People like the idea of Twitter making money.
[02:09:27.940 --> 02:09:29.700]   Has Twitter responded to this?
[02:09:29.700 --> 02:09:31.460]   Are you covering this story, Louise?
[02:09:31.460 --> 02:09:32.460]   Has Twitter...
[02:09:32.460 --> 02:09:36.420]   I haven't covered this specifically, but I did see a lot of funny jokes because obviously
[02:09:36.420 --> 02:09:41.580]   the big thing everyone loves to say on Twitter is this website is free.
[02:09:41.580 --> 02:09:45.860]   So it will be funny when that website is $2.99 per month.
[02:09:45.860 --> 02:09:52.300]   If it would get rid of a lot of the bots if you charged even the smallest amount, right?
[02:09:52.300 --> 02:09:57.700]   I will say that I think they're leaning into the fact that Twitter has, including myself,
[02:09:57.700 --> 02:10:05.020]   like these power users that I think might pay for some enhanced features or whatever.
[02:10:05.020 --> 02:10:09.300]   I don't know if I would necessarily, but I can sort of see the use case for it, unlike
[02:10:09.300 --> 02:10:14.940]   some of these other apps where a lot of people are just kind of more casual users.
[02:10:14.940 --> 02:10:16.220]   I think Twitter has a rapid...
[02:10:16.220 --> 02:10:20.820]   I wouldn't call them fans, necessarily, but people who spend a lot of time dooms scrolling
[02:10:20.820 --> 02:10:21.820]   on there.
[02:10:21.820 --> 02:10:27.700]   I could also say some sort of news service that rounded up all the important tweets about
[02:10:27.700 --> 02:10:31.740]   a certain news topic or something that would be useful to a lot of Twitter's user base,
[02:10:31.740 --> 02:10:34.540]   which often a lot of people use the platform for news.
[02:10:34.540 --> 02:10:36.180]   That's how I use it.
[02:10:36.180 --> 02:10:37.940]   I don't use it for communication, really.
[02:10:37.940 --> 02:10:40.220]   I use it for news.
[02:10:40.220 --> 02:10:44.260]   It's like a news feed if you follow the right people.
[02:10:44.260 --> 02:10:48.460]   I would absolutely pay if they brought back the track feature, for instance, which let
[02:10:48.460 --> 02:10:53.220]   you track subject matter or hashtags.
[02:10:53.220 --> 02:10:57.500]   Honestly, if I could just get rid of the trolls, I'd pay money to use Twitter, just
[02:10:57.500 --> 02:10:59.180]   like have the good Twitter.
[02:10:59.180 --> 02:11:02.700]   I don't know how that would work, but if they could do that.
[02:11:02.700 --> 02:11:07.180]   Yeah, I think it's great that they need to experiment with this kind of stuff.
[02:11:07.180 --> 02:11:12.820]   I think Jack has been pretty serious about the whole healthy conversations.
[02:11:12.820 --> 02:11:14.260]   Aren't they?
[02:11:14.260 --> 02:11:16.580]   They're aligned, aren't they?
[02:11:16.580 --> 02:11:19.700]   I think they're really trying hard to come into alignment.
[02:11:19.700 --> 02:11:23.220]   I think they started in a place that was probably similar to Facebook, but I think they've really
[02:11:23.220 --> 02:11:25.020]   taken it seriously.
[02:11:25.020 --> 02:11:28.460]   I have no idea whether their subscription thing is real or whether it would succeed, but I
[02:11:28.460 --> 02:11:34.780]   think it's critical that they try anything to introduce business models that are fundamentally
[02:11:34.780 --> 02:11:37.020]   aligned with what end users want.
[02:11:37.020 --> 02:11:39.780]   It's not entirely targeted advertising dependent.
[02:11:39.780 --> 02:11:40.780]   Yeah.
[02:11:40.780 --> 02:11:43.180]   Well, there's another like Apple.
[02:11:43.180 --> 02:11:46.660]   Their advertising hasn't been really a goldmine for them.
[02:11:46.660 --> 02:11:48.860]   That's because it's terrible.
[02:11:48.860 --> 02:11:55.500]   If I get any more of those stupid, promoted stories about cute dogs or what is it?
[02:11:55.500 --> 02:11:59.380]   Makeup mistakes, women over 50 make.
[02:11:59.380 --> 02:12:01.540]   Why are you getting those?
[02:12:01.540 --> 02:12:03.540]   That's so brutal.
[02:12:03.540 --> 02:12:07.180]   I am getting all of those mistakes out of the way now.
[02:12:07.180 --> 02:12:09.100]   I'm just going to.
[02:12:09.100 --> 02:12:11.620]   I think your makeup is fantastic.
[02:12:11.620 --> 02:12:13.580]   I think you've done a wonderful job.
[02:12:13.580 --> 02:12:14.580]   That's sweet of you.
[02:12:14.580 --> 02:12:15.580]   I'm not over the age of 50.
[02:12:15.580 --> 02:12:21.020]   I'm like, "Jang, so mean."
[02:12:21.020 --> 02:12:22.420]   Let's see.
[02:12:22.420 --> 02:12:24.860]   I think that's all I really.
[02:12:24.860 --> 02:12:29.500]   Are there any stories you guys were anxious to talk about?
[02:12:29.500 --> 02:12:34.580]   I would ask Louise, I have a story that I would like to get outside takes on because
[02:12:34.580 --> 02:12:39.460]   I've been thinking about it for a while, which is the data miner's story and it's related
[02:12:39.460 --> 02:12:40.460]   to Twitter.
[02:12:40.460 --> 02:12:44.340]   I should have done a segue.
[02:12:44.340 --> 02:12:51.180]   The story that people were very upset that police officers were using a service called
[02:12:51.180 --> 02:12:58.140]   data miner that has access to the Twitter API to the police were using it to locate protests
[02:12:58.140 --> 02:13:02.780]   during the Black Lives Matter protest and people were like, "This is surveillance."
[02:13:02.780 --> 02:13:07.900]   As someone who, I know data miner, I sat through their presentations forever ago.
[02:13:07.900 --> 02:13:11.860]   I wanted access to their API so I could build a sentiment analysis thing with a smart light
[02:13:11.860 --> 02:13:19.020]   bulb to tell me when companies that I was excited about were popping on Twitter.
[02:13:19.020 --> 02:13:24.420]   I looked at the way the police were using it and I didn't feel nefarious or like surveillance.
[02:13:24.420 --> 02:13:30.340]   It doesn't feel great, but I would love to get a take from people who are maybe smarter
[02:13:30.340 --> 02:13:32.220]   or just have different opinions on this.
[02:13:32.220 --> 02:13:39.820]   So data miner has the famous Twitter fire hose, which is the everything feed.
[02:13:39.820 --> 02:13:42.740]   By now, must be a very fat fire hose.
[02:13:42.740 --> 02:13:44.100]   Do they pay Twitter for that?
[02:13:44.100 --> 02:13:46.540]   Very few people have access to it.
[02:13:46.540 --> 02:13:48.380]   I believe they pay for it.
[02:13:48.380 --> 02:13:53.140]   They must because it's got to cost Twitter a lot.
[02:13:53.140 --> 02:13:58.460]   Twitter says, "Yeah, they have it, but all people were doing with it was monitoring news
[02:13:58.460 --> 02:13:59.460]   stories.
[02:13:59.460 --> 02:14:02.220]   It's just the kind of thing we would want."
[02:14:02.220 --> 02:14:05.900]   Of course, the police can use that because if you're following the news, you know where
[02:14:05.900 --> 02:14:10.140]   protests are breaking out and so forth.
[02:14:10.140 --> 02:14:12.540]   Twitter says that's all they were doing, news alerting.
[02:14:12.540 --> 02:14:21.260]   Yeah, I mean, it's like where is the line between what is news alerting and what is surveillance?
[02:14:21.260 --> 02:14:26.180]   I mean, I think the line between those things is not necessarily clear.
[02:14:26.180 --> 02:14:28.100]   I think that this is intelligence gathering.
[02:14:28.100 --> 02:14:34.820]   I don't know if it's the same as secretly asking Twitter for data about users who are
[02:14:34.820 --> 02:14:37.420]   tweeting about the George Floyd protests.
[02:14:37.420 --> 02:14:40.620]   Is there a difference between this?
[02:14:40.620 --> 02:14:43.540]   Let's say flying a drone over the protest for sure.
[02:14:43.540 --> 02:14:49.060]   I think that Twitter's argument here is this is public information that's being aggregated
[02:14:49.060 --> 02:14:51.220]   and it's sentiment analysis.
[02:14:51.220 --> 02:14:55.700]   It's also something that a lot of researchers do, for example, is collect all the tweets
[02:14:55.700 --> 02:14:58.460]   on a certain topic and analyze them.
[02:14:58.460 --> 02:15:04.460]   But I do think that the fact that it's police really changes the dynamic here, especially
[02:15:04.460 --> 02:15:09.500]   with the way that the police have treated the protesters.
[02:15:09.500 --> 02:15:13.260]   But I do kind of, I think it's hard.
[02:15:13.260 --> 02:15:19.060]   I don't think it's a use that the average person tweeting about the protest understood
[02:15:19.060 --> 02:15:20.300]   it could be used for.
[02:15:20.300 --> 02:15:22.060]   And that's sort of like where I come down on this.
[02:15:22.060 --> 02:15:28.420]   It's like, for example, I've had a lot of debates about embedding tweets in news stories.
[02:15:28.420 --> 02:15:29.420]   Is that fair?
[02:15:29.420 --> 02:15:31.060]   Do you need to get the person's consent, et cetera?
[02:15:31.060 --> 02:15:34.060]   And there's a lot of people who say, okay, you tweeted it out.
[02:15:34.060 --> 02:15:35.780]   You had to have known that.
[02:15:35.780 --> 02:15:41.020]   However, I kind of fall on the side of that's not the intended audience.
[02:15:41.020 --> 02:15:43.580]   The intended audience of your tweet was not wired readers.
[02:15:43.580 --> 02:15:44.940]   It was your Twitter followers.
[02:15:44.940 --> 02:15:50.460]   So it's not really fair to assume that that's an audience you would have expected because
[02:15:50.460 --> 02:15:52.180]   it's not.
[02:15:52.180 --> 02:15:56.060]   I think if I quoted your tweet or something but didn't link to it, I think there are
[02:15:56.060 --> 02:15:57.260]   kind of shades of gray here.
[02:15:57.260 --> 02:16:01.820]   But I do think that the average person who was tweeting at a protest didn't understand
[02:16:01.820 --> 02:16:06.820]   that the police might be analyzing those tweets for intelligence purposes.
[02:16:06.820 --> 02:16:08.540]   But you were tweeting publicly.
[02:16:08.540 --> 02:16:15.540]   So I sort of do think that there's a little bit here or there, but it's not black and
[02:16:15.540 --> 02:16:16.540]   white.
[02:16:16.540 --> 02:16:19.540]   And I do think I understand why this was a news story and why this was something that
[02:16:19.540 --> 02:16:23.460]   some people would be upset about for sure.
[02:16:23.460 --> 02:16:25.060]   If people are interested, they should read.
[02:16:25.060 --> 02:16:26.060]   It's very detailed.
[02:16:26.060 --> 02:16:28.060]   The story that broke in the intercept.
[02:16:28.060 --> 02:16:29.620]   It was their story.
[02:16:29.620 --> 02:16:36.740]   And the reason it's of interest partly is because this came up four years ago in 2016,
[02:16:36.740 --> 02:16:44.140]   DataMiner, which is by the way, DataMiner is partly owned by Twitter as well as the CIA.
[02:16:44.140 --> 02:16:51.540]   The accusation four years ago was that DataMiner and other companies like it were being used
[02:16:51.540 --> 02:16:54.180]   to enable domestic surveillance.
[02:16:54.180 --> 02:16:56.540]   Twitter said, "No, no.
[02:16:56.540 --> 02:16:59.540]   We won't do that.
[02:16:59.540 --> 02:17:04.580]   Using Twitter's public APIs or data products to track or profile, protesters and activists
[02:17:04.580 --> 02:17:08.740]   is absolutely unacceptable and prohibited."
[02:17:08.740 --> 02:17:10.780]   And so this has come up again.
[02:17:10.780 --> 02:17:12.700]   And that's one of the reasons this is a story.
[02:17:12.700 --> 02:17:16.620]   Oh, I didn't know that they were saying that it had been prohibited from way back then.
[02:17:16.620 --> 02:17:17.620]   Four years ago.
[02:17:17.620 --> 02:17:18.620]   Yeah.
[02:17:18.620 --> 02:17:19.620]   Zendom.
[02:17:19.620 --> 02:17:22.940]   I mean, I don't agree with the way it's gone down, but I'm also like, the outreach here
[02:17:22.940 --> 02:17:29.700]   feels like it feels good in some ways because I think people need to understand what it
[02:17:29.700 --> 02:17:32.860]   means to have a digital footprint and how that can be used in them.
[02:17:32.860 --> 02:17:33.860]   Yes.
[02:17:33.860 --> 02:17:39.060]   I mean, I would like for them to understand it without having to be arrested.
[02:17:39.060 --> 02:17:41.340]   But these stories are important for that reason.
[02:17:41.340 --> 02:17:47.060]   But I also am kind of like, was this really a terrible thing or was just them just using
[02:17:47.060 --> 02:17:48.660]   it the way it was supposed to be.
[02:17:48.660 --> 02:17:55.780]   The Twitter has been a place for protesters to organize.
[02:17:55.780 --> 02:18:00.180]   The Arab Spring happened on Facebook and Twitter.
[02:18:00.180 --> 02:18:07.580]   So this has been a very useful tool for protesters, but I am not surprised that the police and
[02:18:07.580 --> 02:18:08.980]   others would try to use it.
[02:18:08.980 --> 02:18:12.060]   It is roughly public information, right?
[02:18:12.060 --> 02:18:13.660]   You're tweeting it publicly.
[02:18:13.660 --> 02:18:15.020]   So I disagree.
[02:18:15.020 --> 02:18:18.620]   I think, well, with the public information thing.
[02:18:18.620 --> 02:18:23.660]   And basically, I think there's three thoughts about this.
[02:18:23.660 --> 02:18:27.500]   The first is, it's a creepy company name.
[02:18:27.500 --> 02:18:30.780]   And let this be a lesson.
[02:18:30.780 --> 02:18:33.980]   But there's no hidden data minor.
[02:18:33.980 --> 02:18:34.980]   Creepy name.
[02:18:34.980 --> 02:18:38.300]   I mean, but then again, there's pound here.
[02:18:38.300 --> 02:18:39.940]   That's really creepy.
[02:18:39.940 --> 02:18:40.940]   That's super creepy.
[02:18:40.940 --> 02:18:45.060]   It's literally named after the thing in Lord of the Rings that corrupts everyone who looks
[02:18:45.060 --> 02:18:47.620]   at it by giving them too much information.
[02:18:47.620 --> 02:18:49.620]   Anyway, creepy names.
[02:18:49.620 --> 02:18:50.620]   Creepy name.
[02:18:50.620 --> 02:18:52.660]   In your lesson, people.
[02:18:52.660 --> 02:18:57.580]   But the second thing is like the fact, and I think Luis really, really hit this on the
[02:18:57.580 --> 02:18:58.580]   head.
[02:18:58.580 --> 02:19:02.580]   Yeah, it's public information, but that's no longer enough.
[02:19:02.580 --> 02:19:03.580]   Interesting.
[02:19:03.580 --> 02:19:04.580]   What's the intent in the audience?
[02:19:04.580 --> 02:19:05.580]   Yeah.
[02:19:05.580 --> 02:19:06.740]   Well, I would say more than that, right?
[02:19:06.740 --> 02:19:11.180]   So the traditional understanding of privacy, like the Supreme Court understanding of it,
[02:19:11.180 --> 02:19:16.380]   is you're not entitled to privacy if you are walking around outside, right?
[02:19:16.380 --> 02:19:22.260]   So like if the cops see you doing something illegal outside, outdoors in public and they
[02:19:22.260 --> 02:19:24.860]   don't have a warrant, they don't need a warrant, they can come after you.
[02:19:24.860 --> 02:19:26.660]   But they're not allowed to go inside your house, right?
[02:19:26.660 --> 02:19:28.460]   That was always like the distinction.
[02:19:28.460 --> 02:19:34.940]   But I think that distinction assumes human level capability of observation, not superhuman
[02:19:34.940 --> 02:19:37.660]   capability of observation.
[02:19:37.660 --> 02:19:42.620]   So when we have the ability to algorithmically and automatically see everything, when we
[02:19:42.620 --> 02:19:47.340]   literally are the eye of Sauron, just the fact that it's like public, I don't think that's
[02:19:47.340 --> 02:19:48.340]   enough.
[02:19:48.340 --> 02:19:50.820]   I don't think that actually means anything anymore.
[02:19:50.820 --> 02:19:56.220]   So yeah, like maybe the police ought to be allowed to scroll through Twitter manually
[02:19:56.220 --> 02:19:59.260]   looking for stuff like a person would.
[02:19:59.260 --> 02:20:03.100]   But as soon as they start using technology that literally looks at everything, I don't
[02:20:03.100 --> 02:20:06.140]   actually think that public versus private makes any distinction.
[02:20:06.140 --> 02:20:09.300]   It gets creepy and Orwellian to do it.
[02:20:09.300 --> 02:20:16.140]   And then kind of the third thing is, and I think you said this, this is like, well, is
[02:20:16.140 --> 02:20:20.820]   it terrible or is it like they were using it the way it was designed?
[02:20:20.820 --> 02:20:24.180]   I think it's terrible because they're using it in the way it was designed.
[02:20:24.180 --> 02:20:26.180]   Like yeah, it's not broken.
[02:20:26.180 --> 02:20:27.860]   It's in computer terms, right?
[02:20:27.860 --> 02:20:29.860]   It's a wad, it's working as designed.
[02:20:29.860 --> 02:20:35.420]   Like this is how they made it to do this kind of stuff with it, which makes it worse,
[02:20:35.420 --> 02:20:37.820]   that this isn't a bug or a problem.
[02:20:37.820 --> 02:20:38.820]   It's a feature.
[02:20:38.820 --> 02:20:43.300]   Yeah, I don't think it's necessarily illegal, but I think the outrage is kind of well placed.
[02:20:43.300 --> 02:20:44.300]   I love your distinction.
[02:20:44.300 --> 02:20:45.300]   I don't like it.
[02:20:45.300 --> 02:20:49.740]   I love your distinction between what a human could do and then what would be superhuman.
[02:20:49.740 --> 02:20:54.820]   So for a police officer to scroll through Twitter looking for Black Lives Matter protests
[02:20:54.820 --> 02:20:58.420]   and posts would be okay.
[02:20:58.420 --> 02:21:05.300]   But to do it by sucking the fire hose of Twitter and using powerful systems to analyze it and
[02:21:05.300 --> 02:21:07.780]   pop up alerts, that's not okay.
[02:21:07.780 --> 02:21:09.860]   Is that what you're saying?
[02:21:09.860 --> 02:21:13.340]   Yeah, I mean, I think the line is somewhere there, right?
[02:21:13.340 --> 02:21:21.220]   It's like if I tweet something and I tweet something that maybe is illegal or there's
[02:21:21.220 --> 02:21:27.380]   a crime in progress and somebody sees it and a human being sees it and reports it or maybe
[02:21:27.380 --> 02:21:33.500]   like a human police officer sees it and feels like that's okay.
[02:21:33.500 --> 02:21:37.980]   But if there's this super intelligent AI system that's looking at everything everybody
[02:21:37.980 --> 02:21:42.180]   says and automatically flagging and detecting things, then that's no longer.
[02:21:42.180 --> 02:21:45.820]   There's no longer any difference of that when me saying it publicly versus me saying it
[02:21:45.820 --> 02:21:47.180]   inside the house.
[02:21:47.180 --> 02:21:54.180]   It also introduces a possibility of error, misinterpretation and things like that, which
[02:21:54.180 --> 02:21:56.260]   make it too powerful in some way.
[02:21:56.260 --> 02:22:02.020]   I think there was this court case, I was probably a few decades ago at this point where I remember
[02:22:02.020 --> 02:22:08.580]   there was something about the police were using infrared cameras to detect marijuana
[02:22:08.580 --> 02:22:09.580]   growers.
[02:22:09.580 --> 02:22:13.940]   Back in the before times when marijuana growing was illegal, there was some case where the
[02:22:13.940 --> 02:22:18.900]   cops would roll up in front of a suspected house that was engaging in growing marijuana
[02:22:18.900 --> 02:22:22.380]   and they would point an infrared camera and they can see because of the heat difference,
[02:22:22.380 --> 02:22:26.940]   the heat signature of all the grow lamps and I think they made some arrests or some busts
[02:22:26.940 --> 02:22:29.940]   and the controversy was like, well, they didn't have a warrant.
[02:22:29.940 --> 02:22:32.060]   But the cops were saying, no, this is public.
[02:22:32.060 --> 02:22:33.580]   We were just looking from the outside.
[02:22:33.580 --> 02:22:38.060]   We were just passing by in our patrols and we just happened to see that your house was
[02:22:38.060 --> 02:22:40.940]   too hot and so we blah, blah, blah.
[02:22:40.940 --> 02:22:44.460]   And of course, the idea was like, yeah, you're looking from the outside, but you're not looking
[02:22:44.460 --> 02:22:48.100]   with human eyes, you're looking with an infrared camera.
[02:22:48.100 --> 02:22:54.660]   And the whole idea of that, well, it's publicly visible, those laws were created assuming a
[02:22:54.660 --> 02:22:58.060]   human level of capability, not a superhuman level of capability.
[02:22:58.060 --> 02:23:03.020]   And of course, now with this fire hose of data and with AI, it's much worse than infrared
[02:23:03.020 --> 02:23:05.700]   cameras, you can algorithmically see everything.
[02:23:05.700 --> 02:23:09.740]   So this whole hiding behind a fig leaf where, well, you tweeted it publicly should have
[02:23:09.740 --> 02:23:13.140]   known, it doesn't hold water for me.
[02:23:13.140 --> 02:23:18.740]   So the case that you're talking about is a good one because it exhibits where I think
[02:23:18.740 --> 02:23:21.220]   the law is going to break down on these.
[02:23:21.220 --> 02:23:23.980]   So that case wasn't because it wasn't their human eyes.
[02:23:23.980 --> 02:23:31.140]   It was actually an unreasonable search without a warrant because they decided that-
[02:23:31.140 --> 02:23:33.100]   >> I'm not sure what it was and I didn't just make it up because I wasn't sure when
[02:23:33.100 --> 02:23:34.100]   I was talking.
[02:23:34.100 --> 02:23:39.260]   So I think- >> No, no, it's Elliot was the name
[02:23:39.260 --> 02:23:40.260]   of the guy.
[02:23:40.260 --> 02:23:41.860]   But the legal basis for that-
[02:23:41.860 --> 02:23:42.860]   >> Kylo.
[02:23:42.860 --> 02:23:43.860]   >> What?
[02:23:43.860 --> 02:23:44.860]   >> It's a Kylo.
[02:23:44.860 --> 02:23:45.860]   >> Kylo.
[02:23:45.860 --> 02:23:49.900]   >> Kylo versus US as someone who was obsessed with the Fourth Amendment.
[02:23:49.900 --> 02:23:50.900]   >> Yeah.
[02:23:50.900 --> 02:23:51.900]   >> Yeah.
[02:23:51.900 --> 02:23:53.580]   >> Yeah, it's a Fourth Amendment case.
[02:23:53.580 --> 02:23:56.500]   And I think this is- because what you're talking about, and that's one of the things
[02:23:56.500 --> 02:24:00.260]   I wanted this to be public and talk about it because I think it's really important for
[02:24:00.260 --> 02:24:06.060]   people to understand the limits of their privacy when you have computer vision, when
[02:24:06.060 --> 02:24:11.380]   you have the massive powers of today's computing infrastructure.
[02:24:11.380 --> 02:24:15.180]   But as we're seeing, and I think we're going to see really interesting court cases come
[02:24:15.180 --> 02:24:20.340]   along because there are things like Google's getting the request for all phones that were
[02:24:20.340 --> 02:24:23.460]   in an area at the time of crime was committed.
[02:24:23.460 --> 02:24:27.140]   Now that is not a search and seizure issue.
[02:24:27.140 --> 02:24:30.820]   So we have to have- the laws we have now don't really address that.
[02:24:30.820 --> 02:24:33.380]   And I've been thinking about something like computer vision, right?
[02:24:33.380 --> 02:24:37.340]   So if you think about- it's the same argument we're making with Twitter.
[02:24:37.340 --> 02:24:46.100]   But if you say that somebody robbed someone in a public square and you did a search for
[02:24:46.100 --> 02:24:49.740]   their face and found them and maybe searched the whole country for them, I don't know how
[02:24:49.740 --> 02:24:50.900]   we would think about this.
[02:24:50.900 --> 02:24:57.780]   But we have to- I think we do need to make a distinction and we need to do it somehow
[02:24:57.780 --> 02:25:03.020]   within a legal framework that makes sense because I don't know of search and seizure.
[02:25:03.020 --> 02:25:08.500]   That's where most of these get searching- sorry, search without a warrant for amendment.
[02:25:08.500 --> 02:25:09.820]   That's where most of them fall into.
[02:25:09.820 --> 02:25:13.220]   But I don't know if that's robust enough for what we're looking for.
[02:25:13.220 --> 02:25:14.220]   Wow.
[02:25:14.220 --> 02:25:15.220]   That's a start.
[02:25:15.220 --> 02:25:17.220]   We can just ban whole companies with creepy names.
[02:25:17.220 --> 02:25:19.740]   I think we can just get the status of them over there.
[02:25:19.740 --> 02:25:21.860]   It's data minor and palantir, you're out.
[02:25:21.860 --> 02:25:22.860]   That's over.
[02:25:22.860 --> 02:25:25.300]   I wish I had another four hours to talk to you guys.
[02:25:25.300 --> 02:25:26.300]   So good.
[02:25:26.300 --> 02:25:31.060]   I am now fully intrigued by the fact that Louise, you're a fourth amendment fanatic.
[02:25:31.060 --> 02:25:33.980]   I'm excited about that too.
[02:25:33.980 --> 02:25:34.980]   Yeah.
[02:25:34.980 --> 02:25:38.220]   Is that a hobby for you?
[02:25:38.220 --> 02:25:40.180]   There was a big case.
[02:25:40.180 --> 02:25:43.660]   I think it was- I mean, it was last summer or the summer before.
[02:25:43.660 --> 02:25:47.980]   But there was a time when there was just a lot of movement in the fourth amendment.
[02:25:47.980 --> 02:25:52.700]   I just got- we have since hired someone who went to law school and is a DC reporter.
[02:25:52.700 --> 02:25:57.460]   But at the time I was doing a lot of legal reporting and it was just so fascinating in
[02:25:57.460 --> 02:25:58.460]   these cases.
[02:25:58.460 --> 02:26:05.020]   The other big case that always comes up is they put a GPS tracker on a guy's car.
[02:26:05.020 --> 02:26:09.780]   That was also ruled unconstitutional under the fourth amendment.
[02:26:09.780 --> 02:26:12.860]   Those are the two really famous tech cases.
[02:26:12.860 --> 02:26:14.460]   But yeah, I love the marijuana case.
[02:26:14.460 --> 02:26:15.460]   It was fascinating.
[02:26:15.460 --> 02:26:17.460]   That's the key to the case.
[02:26:17.460 --> 02:26:20.100]   There's a really smart on this and it's a good point.
[02:26:20.100 --> 02:26:26.540]   I'm blown away that three out of the four panelists remember the 2001 Supreme Court decision on
[02:26:26.540 --> 02:26:28.540]   unreasonable search and seizure.
[02:26:28.540 --> 02:26:29.540]   That's amazing.
[02:26:29.540 --> 02:26:32.380]   And that you could cite it is even more impressive, Louise.
[02:26:32.380 --> 02:26:37.980]   Louise, did you read Cyrus Faravar's book, "Habias Data"?
[02:26:37.980 --> 02:26:41.780]   I have not read the whole thing, but I had it on my desk in the before times and I've
[02:26:41.780 --> 02:26:46.060]   talked to him a couple of times for my stories.
[02:26:46.060 --> 02:26:49.460]   But yeah, he's like the guy on this for sure.
[02:26:49.460 --> 02:26:56.060]   Well, we're going to have to get him on privacy versus the rise of surveillance tech, Cyrus
[02:26:56.060 --> 02:26:57.060]   Faravar.
[02:26:57.060 --> 02:27:00.300]   Louise, thank you so much for being here.
[02:27:00.300 --> 02:27:06.100]   Louise Mazzakis is of course staff writer at Wired and fourth amendment junkie.
[02:27:06.100 --> 02:27:11.540]   You could find her on the Twitter @LMazzakis, M-A-T-S-A-K-I-S.
[02:27:11.540 --> 02:27:13.020]   Have I been saying that right the whole time?
[02:27:13.020 --> 02:27:14.020]   I hope.
[02:27:14.020 --> 02:27:15.020]   Yeah, Mazzakis.
[02:27:15.020 --> 02:27:16.020]   I think you got it.
[02:27:16.020 --> 02:27:17.020]   And you so much for having me.
[02:27:17.020 --> 02:27:21.060]   You tilted your head as if it was like, well, for an old guy, you did all right.
[02:27:21.060 --> 02:27:24.500]   No, no, I was laughing at being a fourth amendment junkie.
[02:27:24.500 --> 02:27:25.500]   No, no, that's good.
[02:27:25.500 --> 02:27:26.500]   That's your new title.
[02:27:26.500 --> 02:27:28.780]   We put that in the lower third from now on.
[02:27:28.780 --> 02:27:29.780]   Really a pleasure.
[02:27:29.780 --> 02:27:30.780]   I'll take it.
[02:27:30.780 --> 02:27:31.780]   Thank you so much for being here today.
[02:27:31.780 --> 02:27:32.780]   Thanks for having me.
[02:27:32.780 --> 02:27:33.780]   Stacy's been working overtime.
[02:27:33.780 --> 02:27:37.020]   She's been on three shows now on the Twitter network this week.
[02:27:37.020 --> 02:27:38.020]   Thank you.
[02:27:38.020 --> 02:27:39.020]   Bless you.
[02:27:39.020 --> 02:27:40.020]   We really appreciate it.
[02:27:40.020 --> 02:27:41.500]   We'll see you on Wednesday.
[02:27:41.500 --> 02:27:43.020]   Jeff Jarvis is birthday on Wednesday.
[02:27:43.020 --> 02:27:44.140]   We've got a surprise for him.
[02:27:44.140 --> 02:27:45.940]   So excited on this week in Google.
[02:27:45.940 --> 02:27:47.580]   That's going to be fun.
[02:27:47.580 --> 02:27:51.260]   Subscribe to Stacy's newsletter, Stacy on IOT.com.
[02:27:51.260 --> 02:27:55.140]   Listen to her podcast, the IOT podcast with Kevin Tofel.
[02:27:55.140 --> 02:27:57.300]   Follow her head, Giga Stacy on Twitter.
[02:27:57.300 --> 02:27:59.140]   Thank you, Stacy.
[02:27:59.140 --> 02:28:00.620]   Fine.
[02:28:00.620 --> 02:28:03.180]   And Phil Libman, really great to have you on.
[02:28:03.180 --> 02:28:05.820]   Man, you're coming back.
[02:28:05.820 --> 02:28:08.940]   You don't have to use, but you can.
[02:28:08.940 --> 02:28:14.820]   His new tool is at mmmm.app.
[02:28:14.820 --> 02:28:15.820]   But you can't get it yet.
[02:28:15.820 --> 02:28:17.260]   You got to apply for an invite.
[02:28:17.260 --> 02:28:19.300]   I just got lucky, I guess.
[02:28:19.300 --> 02:28:20.660]   Floted to the top of the heat.
[02:28:20.660 --> 02:28:21.660]   But thank you, Phil.
[02:28:21.660 --> 02:28:23.340]   I really appreciate it.
[02:28:23.340 --> 02:28:24.340]   Thank you.
[02:28:24.340 --> 02:28:25.340]   Yeah, really good.
[02:28:25.340 --> 02:28:32.860]   Do you think some of your, by the way, somebody pointed out, Palantir Data Minor, Bad Names
[02:28:32.860 --> 02:28:33.860]   Quibi.
[02:28:33.860 --> 02:28:34.860]   We could also ban Quibi.
[02:28:34.860 --> 02:28:37.340]   I don't know if Quibi is creepy though.
[02:28:37.340 --> 02:28:38.580]   It's just kind of, I don't know.
[02:28:38.580 --> 02:28:39.580]   Although, yeah, it's a little creepy.
[02:28:39.580 --> 02:28:44.180]   If it was like a good test is like, if a name was like a creature in Star Trek.
[02:28:44.180 --> 02:28:45.620]   It was like a better evil.
[02:28:45.620 --> 02:28:46.620]   Quibi is creepy.
[02:28:46.620 --> 02:28:47.620]   Yeah.
[02:28:47.620 --> 02:28:49.660]   It was like an E-walk then, you know.
[02:28:49.660 --> 02:28:50.660]   They'd be cute.
[02:28:50.660 --> 02:28:51.660]   Yeah, it would be cute.
[02:28:51.660 --> 02:28:52.660]   I could be.
[02:28:52.660 --> 02:28:57.220]   But they would be like, like cute, but deadly.
[02:28:57.220 --> 02:29:01.500]   Do you think that your attitude towards data mining a little influence by the fact that
[02:29:01.500 --> 02:29:03.740]   you were born in the Soviet Union?
[02:29:03.740 --> 02:29:08.900]   Yeah, I mean, I think, look, all of this is balancing out two harms, right?
[02:29:08.900 --> 02:29:17.420]   The harm of crime, of like individual crimes against other people and the need to reduce
[02:29:17.420 --> 02:29:21.340]   the harm that comes from those crimes, which is a very legitimate thing.
[02:29:21.340 --> 02:29:28.500]   And then balancing that out with the harm of state surveillance and crimes committed by
[02:29:28.500 --> 02:29:31.700]   states or by large organizations.
[02:29:31.700 --> 02:29:35.540]   And there's always a balance between which of those things gets overdramatized.
[02:29:35.540 --> 02:29:36.540]   Yeah.
[02:29:36.540 --> 02:29:37.540]   Right?
[02:29:37.540 --> 02:29:42.700]   And often, at least in the US, at least right now, the individual crime is like we're kind
[02:29:42.700 --> 02:29:43.700]   of over indexing on them.
[02:29:43.700 --> 02:29:48.020]   It's actually not that big a deal most of the time.
[02:29:48.020 --> 02:29:52.420]   And the harm done by massive surveillance at the state level has also not been that terrible
[02:29:52.420 --> 02:29:53.420]   yet.
[02:29:53.420 --> 02:29:54.900]   But there's definitely strong potential.
[02:29:54.900 --> 02:29:55.900]   Yeah.
[02:29:55.900 --> 02:29:56.900]   Yeah.
[02:29:56.900 --> 02:29:57.900]   The tools over there.
[02:29:57.900 --> 02:30:01.620]   And I think been born and, yeah, like lived a part of my life in a place where the balance
[02:30:01.620 --> 02:30:06.940]   very much was, there was a lot more harm committed by a powerful state in the Soviet
[02:30:06.940 --> 02:30:09.140]   Union than the harm committed by individual crimes.
[02:30:09.140 --> 02:30:10.140]   Yeah.
[02:30:10.140 --> 02:30:16.940]   I think I generally have some more, have some trauma around that that I usually err on the
[02:30:16.940 --> 02:30:17.940]   side of.
[02:30:17.940 --> 02:30:19.820]   Yeah, let's do a little bit less surveillance.
[02:30:19.820 --> 02:30:23.220]   Let's do a little bit less data mining, all of that kind of stuff.
[02:30:23.220 --> 02:30:24.580]   Couldn't agree more.
[02:30:24.580 --> 02:30:25.580]   Thank you, Phil.
[02:30:25.580 --> 02:30:26.580]   Really great to see you.
[02:30:26.580 --> 02:30:27.580]   Good luck with them.
[02:30:27.580 --> 02:30:28.580]   That's it.
[02:30:28.580 --> 02:30:29.580]   I love the title.
[02:30:29.580 --> 02:30:30.580]   I love the name.
[02:30:30.580 --> 02:30:31.580]   And I think it's a great app and I've been playing with it.
[02:30:31.580 --> 02:30:34.820]   And I think there's a lot we can be able to do with it.
[02:30:34.820 --> 02:30:36.820]   It's really, really cool.
[02:30:36.820 --> 02:30:38.940]   Thank you all for joining us.
[02:30:38.940 --> 02:30:40.420]   This has been a fantastic show.
[02:30:40.420 --> 02:30:44.580]   We do Twitter every Sunday afternoon, about 230 Pacific 530 Eastern.
[02:30:44.580 --> 02:30:45.580]   That's 2130 UTC.
[02:30:45.580 --> 02:30:49.380]   If you want to watch live, we do a live audio and video stream.
[02:30:49.380 --> 02:30:51.860]   It's really that kind of the behind the scene stream.
[02:30:51.860 --> 02:30:57.220]   But it's always on a twit.tv/live.
[02:30:57.220 --> 02:31:00.500]   If you're doing that though, join us in the chat room because they're live too, mostly
[02:31:00.500 --> 02:31:03.700]   IRC.twit.tv.
[02:31:03.700 --> 02:31:06.620]   That's where the interaction happens.
[02:31:06.620 --> 02:31:11.660]   Most of us listen on demand after the fact you'll find all of our podcasts at our website,
[02:31:11.660 --> 02:31:13.140]   twit.tv.
[02:31:13.140 --> 02:31:14.140]   They're also on YouTube.
[02:31:14.140 --> 02:31:16.860]   You can even ask your Amazon Echo or other voice device.
[02:31:16.860 --> 02:31:18.580]   Just play this week in tech.
[02:31:18.580 --> 02:31:20.980]   It'll play the most recent one.
[02:31:20.980 --> 02:31:26.620]   Or actually my preferred way for you to listen or watch is to get a podcast app and subscribe
[02:31:26.620 --> 02:31:31.020]   that way you'll have it the minute it's available of a Sunday evening.
[02:31:31.020 --> 02:31:32.020]   We appreciate that.
[02:31:32.020 --> 02:31:33.900]   Thank you all for being here.
[02:31:33.900 --> 02:31:34.900]   We'll see you next week.
[02:31:34.900 --> 02:31:35.900]   Another twit.
[02:31:35.900 --> 02:31:36.900]   Isn't it?
[02:31:36.900 --> 02:31:37.900]   It's amazing.
[02:31:37.900 --> 02:31:38.900]   Do the twit.
[02:31:38.900 --> 02:31:39.900]   Do the twit.
[02:31:39.900 --> 02:31:40.900]   All right.
[02:31:40.900 --> 02:31:41.900]   Do the twit, baby.
[02:31:41.900 --> 02:31:42.900]   Do the twit.
[02:31:42.900 --> 02:31:43.900]   All right.
[02:31:43.900 --> 02:31:44.900]   Do the twit.
[02:31:44.900 --> 02:31:45.900]   Do the twit.
[02:31:45.900 --> 02:31:46.900]   All right.
[02:31:46.900 --> 02:31:46.900]   Do the twit.
[02:31:46.900 --> 02:31:56.360]   [BLANK_AUDIO]

