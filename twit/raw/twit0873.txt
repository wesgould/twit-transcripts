;FFMETADATA1
title=A Little Patience and a Lot of Tape
artist=Leo Laporte, Cory Doctorow, Tim Stevens, Owen Thomas
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-05-02
track=873
language=English
genre=Podcast
comment=Musk buys Twitter, Ethereum stumbles, broadcasting to aliens
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.640]   It's time for Twit this weekend tech great panel for you science fiction author Corey
[00:00:04.640 --> 00:00:06.680]   Doctorow is here.
[00:00:06.680 --> 00:00:10.080]   He's joined by Tim Stevens from CNET and from protocol.com.
[00:00:10.080 --> 00:00:11.920]   Owen Thomas lots to talk about.
[00:00:11.920 --> 00:00:17.720]   We'll get the guys ideas on what Elon should do when he finally gets ahold of Twitter.
[00:00:17.720 --> 00:00:21.360]   Why web point three is not going that great.
[00:00:21.360 --> 00:00:24.920]   In fact, how the latest plan to sell what was it?
[00:00:24.920 --> 00:00:29.720]   Board apes brought ether down and Corey will show us.
[00:00:29.720 --> 00:00:32.480]   This femur is going to make a walking stick out of it.
[00:00:32.480 --> 00:00:35.840]   It's all coming up next on Twit.
[00:00:35.840 --> 00:00:41.600]   Podcasts you love from people you trust.
[00:00:41.600 --> 00:00:52.120]   This is Twit.
[00:00:52.120 --> 00:00:54.920]   This is Twit this weekend tech.
[00:00:54.920 --> 00:00:59.720]   At 873 recorded Sunday, May 1, 2022.
[00:00:59.720 --> 00:01:02.960]   A little patience and a lot of tape.
[00:01:02.960 --> 00:01:06.400]   This episode of This Week in Tech is brought to you by podium.
[00:01:06.400 --> 00:01:12.600]   Join more than 100,000 businesses that already use podium to streamline their customer interactions.
[00:01:12.600 --> 00:01:18.120]   Get started for free at podium.com/twit or sign up for a paid podium account and get
[00:01:18.120 --> 00:01:20.000]   a free credit card reader.
[00:01:20.000 --> 00:01:21.480]   Restrictions apply.
[00:01:21.480 --> 00:01:23.240]   And by our crowd.
[00:01:23.240 --> 00:01:28.560]   Our crowd helps accredited investors invest early in pre-IPO companies alongside professional
[00:01:28.560 --> 00:01:30.360]   venture capitalists.
[00:01:30.360 --> 00:01:37.240]   Join the fastest growing venture capital investment community at our crowd.com/twit.
[00:01:37.240 --> 00:01:38.960]   And by Noreva.
[00:01:38.960 --> 00:01:43.280]   Traditional audio conferencing systems can entail lots of components, installation could
[00:01:43.280 --> 00:01:47.040]   take days, and you might not get the mic coverage you need.
[00:01:47.040 --> 00:01:48.960]   That's complex-pensive.
[00:01:48.960 --> 00:01:54.520]   But Noreva Audio is easy to install and manage no technicians required, and you get true,
[00:01:54.520 --> 00:01:56.320]   full-room coverage.
[00:01:56.320 --> 00:01:58.080]   And that's easy-can-o-macle.
[00:01:58.080 --> 00:02:01.320]   Learn more at noreva.com.
[00:02:01.320 --> 00:02:04.480]   And by Userway.org.
[00:02:04.480 --> 00:02:09.080]   Userway is the world's number one accessibility solution, and it's committed to enabling the
[00:02:09.080 --> 00:02:13.600]   fundamental human right of digital accessibility for everyone.
[00:02:13.600 --> 00:02:17.920]   When you're ready to make your site compliant, deciding which solution to use is an easy
[00:02:17.920 --> 00:02:18.920]   choice.
[00:02:18.920 --> 00:02:31.720]   Go to Userway.org/twit for 30% off Userway's AI-powered accessibility solution.
[00:02:31.720 --> 00:02:34.840]   It's time for Twit this week in Tech, the show we cover the weeks.
[00:02:34.840 --> 00:02:37.880]   Tech news, I had to bring in the big brains for this one.
[00:02:37.880 --> 00:02:42.360]   No one could figure out what the hell's going on with Elon Musk and Twitter.
[00:02:42.360 --> 00:02:48.520]   Tim Stevens is here, editor in chief of the newly Chris and CNET Cars, formerly Roadshow.
[00:02:48.520 --> 00:02:50.320]   Still there, still editor in chief.
[00:02:50.320 --> 00:02:51.320]   Great to see you.
[00:02:51.320 --> 00:02:52.320]   Tim.
[00:02:52.320 --> 00:02:53.320]   Great to see you too, Leo.
[00:02:53.320 --> 00:02:54.880]   Thanks for having me on.
[00:02:54.880 --> 00:02:57.160]   And you told me you had a good winter.
[00:02:57.160 --> 00:03:00.560]   Here we are on May Day, so I guess the ice racing is over in upper New York state.
[00:03:00.560 --> 00:03:03.120]   Yeah, it's been a couple of weeks since we've had the ice.
[00:03:03.120 --> 00:03:04.880]   Yeah, we're still waiting for really things to thaw.
[00:03:04.880 --> 00:03:08.760]   We just had snow last week, so it's still hanging on there though.
[00:03:08.760 --> 00:03:09.760]   Wow.
[00:03:09.760 --> 00:03:12.560]   Colton says, no, I better not.
[00:03:12.560 --> 00:03:15.080]   It is all.
[00:03:15.080 --> 00:03:17.840]   It's May, it's May.
[00:03:17.840 --> 00:03:19.760]   No more snow driving begins today.
[00:03:19.760 --> 00:03:20.760]   That's what he says.
[00:03:20.760 --> 00:03:22.640]   Owen Thomas is here from Protocol.
[00:03:22.640 --> 00:03:24.400]   Hello, Owen Thomas.
[00:03:24.400 --> 00:03:27.400]   Yeah, he's doing the full Isaac.
[00:03:27.400 --> 00:03:28.680]   Good to see you.
[00:03:28.680 --> 00:03:29.680]   How are you?
[00:03:29.680 --> 00:03:30.920]   I'm great.
[00:03:30.920 --> 00:03:35.000]   You're in Virginia now, not in the Bay Area.
[00:03:35.000 --> 00:03:36.000]   My native land.
[00:03:36.000 --> 00:03:38.240]   Oh, Fairfax County, Virginia.
[00:03:38.240 --> 00:03:41.520]   Oh, that's horse country, isn't it?
[00:03:41.520 --> 00:03:43.160]   It was maybe 40 years ago.
[00:03:43.160 --> 00:03:44.840]   It's now suburbia.
[00:03:44.840 --> 00:03:50.600]   Everything for me, all my references are at least 40 years old, so you'll forgive me
[00:03:50.600 --> 00:03:52.040]   for that.
[00:03:52.040 --> 00:03:53.560]   All is forgiven.
[00:03:53.560 --> 00:04:00.480]   Also with this, the science fiction novelist and big thinker, Mr. Corey Doctorow from pluralistic.net.
[00:04:00.480 --> 00:04:01.760]   Always good to see you, Corey.
[00:04:01.760 --> 00:04:02.760]   Thank you for being here.
[00:04:02.760 --> 00:04:03.760]   It's my pleasure.
[00:04:03.760 --> 00:04:04.760]   Nice to see you too.
[00:04:04.760 --> 00:04:07.880]   If anybody could figure out what Elon Musk should do with Twitter, it would be Corey
[00:04:07.880 --> 00:04:08.880]   Doctorow.
[00:04:08.880 --> 00:04:14.000]   Yeah, I don't think he'd like my answer, but we'll talk.
[00:04:14.000 --> 00:04:15.800]   Let's talk.
[00:04:15.800 --> 00:04:16.800]   Let's talk.
[00:04:16.800 --> 00:04:21.440]   EFF has written already a piece.
[00:04:21.440 --> 00:04:22.480]   Twitter has a new owner.
[00:04:22.480 --> 00:04:27.280]   This is Jillian York and Jenny Gebhardt and Jason Kelly and David Green combining their
[00:04:27.280 --> 00:04:28.480]   great minds.
[00:04:28.480 --> 00:04:31.600]   Here's what he should do.
[00:04:31.600 --> 00:04:35.320]   It's pretty clear Elon doesn't have a clue what he should do.
[00:04:35.320 --> 00:04:40.300]   He knows what he wants to do, but they're mutually conflicting ideas, things like get
[00:04:40.300 --> 00:04:45.640]   rid of spam bots and protect free speech, which are mutually exclusive.
[00:04:45.640 --> 00:04:51.520]   Yeah, you know, there's this thing that people do where when they don't know much about a
[00:04:51.520 --> 00:04:54.400]   complicated subject, they say, "It's so simple.
[00:04:54.400 --> 00:04:56.160]   I don't know why you're not doing it.
[00:04:56.160 --> 00:04:59.320]   It's really obvious that all you should do is ask."
[00:04:59.320 --> 00:05:01.600]   I remember, I was just thinking about this the other day.
[00:05:01.600 --> 00:05:08.840]   I read a book when I was a kid, a kid's book where the premise was that the protagonist
[00:05:08.840 --> 00:05:12.280]   had figured out how to predict the weather 10 years out.
[00:05:12.280 --> 00:05:15.960]   And the way that they did it is they just asked the meteorologist what they did to predict
[00:05:15.960 --> 00:05:20.880]   the weather one day out, and then they just doubled it and tripled it until they got to
[00:05:20.880 --> 00:05:22.360]   10 years.
[00:05:22.360 --> 00:05:29.000]   And this whole thing, I want to reduce the amount of people who get banned, and I also
[00:05:29.000 --> 00:05:35.440]   want to ban anyone who I think is a bot, is a really good example of just not really
[00:05:35.440 --> 00:05:38.120]   understanding what you're talking about.
[00:05:38.120 --> 00:05:44.120]   And I think the argument that Twitter should have all the things that are legally allowed
[00:05:44.120 --> 00:05:47.560]   in America is clearly not a good plan.
[00:05:47.560 --> 00:05:51.080]   Well, for one thing, Twitter isn't just in America.
[00:05:51.080 --> 00:05:56.360]   That was what Joel Chris Anderson in his interview at Ted was, "Oh, it's easy.
[00:05:56.360 --> 00:05:58.960]   It's free speech, but anything that's not illegal."
[00:05:58.960 --> 00:06:00.440]   Yeah, a buy-by-buy.
[00:06:00.440 --> 00:06:05.600]   And so I think that every conversational space has some limits.
[00:06:05.600 --> 00:06:10.200]   And it is true that setting a limit for 100 million people is really hard.
[00:06:10.200 --> 00:06:14.640]   It's even harder to do it for $3 billion, which is one of the reasons Facebook sucks.
[00:06:14.640 --> 00:06:18.840]   And I think a lot of people are like, the problem with Facebook is that Mark Zuckerberg
[00:06:18.840 --> 00:06:23.720]   is the wrong person to set the conversational policies for 3 billion people speaking 1,000
[00:06:23.720 --> 00:06:25.560]   languages in 100 countries.
[00:06:25.560 --> 00:06:29.640]   And I think the right answer is like, nobody should have that job.
[00:06:29.640 --> 00:06:35.400]   And America has an approach to this that as someone who's not an American but was just
[00:06:35.400 --> 00:06:40.560]   called up for my citizenship interview, I'm quite fond of, which is federalism.
[00:06:40.560 --> 00:06:45.680]   Just the idea that there's some minimum standards that we have for what we want.
[00:06:45.680 --> 00:06:52.520]   And then we allow, we devolve control to smaller groups of people who set their own rules.
[00:06:52.520 --> 00:06:54.800]   And we make it real easy to go from one group to the other.
[00:06:54.800 --> 00:06:58.240]   So the states can set a bunch of rules for themselves that are different.
[00:06:58.240 --> 00:07:04.120]   I like living in California, for example, where non-compete agreements are illegal.
[00:07:04.120 --> 00:07:06.720]   And it's really easy to move to California.
[00:07:06.720 --> 00:07:11.080]   So you move to California and you just will never be subjected to a non-compete, which
[00:07:11.080 --> 00:07:12.080]   is good.
[00:07:12.080 --> 00:07:16.760]   And it's a way to find out how people want to live and what works and what doesn't.
[00:07:16.760 --> 00:07:19.520]   The laboratory of democracy, as they say.
[00:07:19.520 --> 00:07:26.040]   And allowing Twitter users to leave Twitter but continue to exchange messages with it
[00:07:26.040 --> 00:07:32.560]   by having a federated system where you had people who ran their own servers that if
[00:07:32.560 --> 00:07:37.160]   they met a certain standard about privacy and about certain other things, maybe doxing
[00:07:37.160 --> 00:07:40.680]   and so on, they could continue to interchange messages.
[00:07:40.680 --> 00:07:44.280]   And then within their own communities, they would set up their own rules.
[00:07:44.280 --> 00:07:46.600]   And you might have a Twitter community of your own.
[00:07:46.600 --> 00:07:47.600]   You might have a--
[00:07:47.600 --> 00:07:50.600]   You're describing Mastodon.
[00:07:50.600 --> 00:07:55.320]   Well the problem with Mastodon is that everyone is already on Twitter.
[00:07:55.320 --> 00:07:57.080]   And so there's a really high switching cost.
[00:07:57.080 --> 00:07:58.080]   Right.
[00:07:58.080 --> 00:08:02.040]   You have to convince all the people you want to talk to on Twitter to go to Mastodon.
[00:08:02.040 --> 00:08:05.960]   Don't you worry that part of the problem is simply scale that if everybody who was
[00:08:05.960 --> 00:08:11.200]   on Twitter went to Mastodon, similar problems with rear their ugly heads.
[00:08:11.200 --> 00:08:15.840]   If they went to one Mastodon server for sure, if they went to 10,000 Mastodon servers, there
[00:08:15.840 --> 00:08:20.920]   would be interesting new gnarly problems like, you know, is it white listing or black listing
[00:08:20.920 --> 00:08:23.320]   when people create a new server?
[00:08:23.320 --> 00:08:24.320]   What are the terms on which--
[00:08:24.320 --> 00:08:32.600]   Yeah, when I created my server, I banned-- I did not inter operate with gab.com, which
[00:08:32.600 --> 00:08:37.840]   is a right-wing version of Mastodon, and another server, which I shall remain nameless.
[00:08:37.840 --> 00:08:38.840]   Truthsocial or something?
[00:08:38.840 --> 00:08:42.200]   Yeah, well, truthsocial is running a Mastodon, but are they federating?
[00:08:42.200 --> 00:08:43.200]   They're not federating.
[00:08:43.200 --> 00:08:44.200]   I don't know if they're federating or not.
[00:08:44.200 --> 00:08:45.200]   You're right.
[00:08:45.200 --> 00:08:48.120]   If Mastodon's got a really interesting moderation--
[00:08:48.120 --> 00:08:51.240]   Yeah, I moderate my instance.
[00:08:51.240 --> 00:08:56.920]   Yeah, but as a user, so I use Quadrater to nets one, which is memo.fr.
[00:08:56.920 --> 00:09:00.760]   They're the French equivalent of EFF or French analog to EFF.
[00:09:00.760 --> 00:09:03.360]   And I like them because they stand up for your speech, right?
[00:09:03.360 --> 00:09:05.280]   I thought they'd be a good place to be.
[00:09:05.280 --> 00:09:07.160]   And they federate with a lot of people.
[00:09:07.160 --> 00:09:12.280]   And I noticed that there were people who were actual Nazis in my mentions, who were on
[00:09:12.280 --> 00:09:17.280]   servers with names like white supremacy dot some brother.
[00:09:17.280 --> 00:09:19.240]   And I could just block the whole server.
[00:09:19.240 --> 00:09:20.240]   So I just said, like--
[00:09:20.240 --> 00:09:22.600]   Boy, as an individual, you can block an entire server.
[00:09:22.600 --> 00:09:23.600]   See, that's great.
[00:09:23.600 --> 00:09:24.600]   Yeah.
[00:09:24.600 --> 00:09:27.120]   So they-- that's a set of policies that I like.
[00:09:27.120 --> 00:09:31.400]   It's really easy for me to quit Quadrater to net and go to a different server if I want
[00:09:31.400 --> 00:09:36.560]   a different set of policies, and to automatically have everyone who's following me there, move
[00:09:36.560 --> 00:09:38.680]   their follow to somewhere else.
[00:09:38.680 --> 00:09:40.960]   So my switching costs are really low.
[00:09:40.960 --> 00:09:44.680]   So there's a study, a preprint that just came out that's going to be presented at CHI this
[00:09:44.680 --> 00:09:51.400]   year, the Computer Human Interaction Conference that Usenix puts on, about people who left
[00:09:51.400 --> 00:09:55.880]   WhatsApp when they changed their privacy policy in 2021 and installed Signal.
[00:09:55.880 --> 00:09:58.240]   And there were several million people who did this.
[00:09:58.240 --> 00:10:03.160]   And the researchers surveyed over 1,000 of them to get a good representative sample.
[00:10:03.160 --> 00:10:07.680]   And what they found is that only about 20% managed to shift any of their conversations
[00:10:07.680 --> 00:10:12.320]   to Signal, like that has got anyone from WhatsApp that they wanted to talk to, to leave WhatsApp
[00:10:12.320 --> 00:10:13.760]   and move to Signal.
[00:10:13.760 --> 00:10:19.600]   And of those, half a percent of them managed to actually delete WhatsApp and move all the
[00:10:19.600 --> 00:10:22.240]   conversations that mattered to Signal.
[00:10:22.240 --> 00:10:24.720]   So switching costs are really high.
[00:10:24.720 --> 00:10:27.120]   They make it really hard to leave one service.
[00:10:27.120 --> 00:10:29.720]   They keep other services from popping up.
[00:10:29.720 --> 00:10:33.160]   Interoperability and federation make switching costs really low.
[00:10:33.160 --> 00:10:34.600]   You don't have to convince all your friends to leave.
[00:10:34.600 --> 00:10:38.760]   But just like literally 100% of the people who installed Signal could have immediately
[00:10:38.760 --> 00:10:43.040]   deleted WhatsApp because it wouldn't matter if the people they wanted to talk to had left
[00:10:43.040 --> 00:10:45.840]   WhatsApp because they could still talk to them from Signal.
[00:10:45.840 --> 00:10:46.840]   That's the key.
[00:10:46.840 --> 00:10:47.840]   Interoperability.
[00:10:47.840 --> 00:10:48.840]   Absolutely.
[00:10:48.840 --> 00:10:49.840]   Yeah.
[00:10:49.840 --> 00:10:52.480]   Tim Statham, are you a Twitter user?
[00:10:52.480 --> 00:10:54.360]   Yeah, I definitely am.
[00:10:54.360 --> 00:10:56.480]   And it's been an interesting week for sure.
[00:10:56.480 --> 00:10:59.400]   I've definitely lost a lot of followers this week, which I think is really unfortunate.
[00:10:59.400 --> 00:11:03.960]   I think it's very, very early for people to be making such a drastic measure to be, you
[00:11:03.960 --> 00:11:04.960]   know, deleting their accounts.
[00:11:04.960 --> 00:11:08.520]   We really don't know how this is going to shake out or even if it's going to pass.
[00:11:08.520 --> 00:11:11.920]   I mean, there's still every possibility that Musk is going to just change his mind.
[00:11:11.920 --> 00:11:15.640]   There is one thing that Musk will just walk away from this.
[00:11:15.640 --> 00:11:20.440]   He doesn't really, although he sold, I think, $8 billion with the Tesla stock since this
[00:11:20.440 --> 00:11:21.760]   offer.
[00:11:21.760 --> 00:11:24.360]   He's got to get to come up with $21 billion personally.
[00:11:24.360 --> 00:11:26.920]   So there's a lot more to go still.
[00:11:26.920 --> 00:11:29.880]   So and he said he's not selling more Tesla stock.
[00:11:29.880 --> 00:11:30.880]   Yeah, he can't.
[00:11:30.880 --> 00:11:33.160]   I'm a little curious about the math.
[00:11:33.160 --> 00:11:33.680]   Yeah.
[00:11:33.680 --> 00:11:34.680]   Yeah.
[00:11:34.680 --> 00:11:38.480]   So Tim, your point of view is the wait and see point of view.
[00:11:38.480 --> 00:11:39.920]   I'm going to see what happens.
[00:11:39.920 --> 00:11:41.280]   It is for now.
[00:11:41.280 --> 00:11:45.960]   And ultimately, you know, I think Twitter is an important enough platform that to simply
[00:11:45.960 --> 00:11:49.000]   walk away is to basically, you know, throw your hands into feet.
[00:11:49.000 --> 00:11:53.120]   If you want to have some say in what happens going forward, then I think that you need
[00:11:53.120 --> 00:11:56.200]   to ultimately stay in voice for opinion on what Twitter should be.
[00:11:56.200 --> 00:11:59.800]   I think always got a lot of great ideas and be wonderful to have some of those implemented
[00:11:59.800 --> 00:12:04.200]   for sure, if I could immediately step away to something like Mastodon and have my audience
[00:12:04.200 --> 00:12:06.440]   come with me, then I probably would do that as well.
[00:12:06.440 --> 00:12:08.720]   But obviously that's not something that we can do.
[00:12:08.720 --> 00:12:10.160]   I am definitely very concerned.
[00:12:10.160 --> 00:12:12.880]   I mean, most on one day was saying he didn't want to be political.
[00:12:12.880 --> 00:12:16.840]   And then the next day he's tweeting out political memes, which were quite shortsighted, in my
[00:12:16.840 --> 00:12:18.280]   opinion.
[00:12:18.280 --> 00:12:22.120]   So I have a lot of concerns about what that means for, you know, having someone like that
[00:12:22.120 --> 00:12:23.560]   at the core of the brand.
[00:12:23.560 --> 00:12:24.560]   Right.
[00:12:24.560 --> 00:12:29.120]   But it doesn't necessarily mean that it's going to turn this what I think is a very important
[00:12:29.120 --> 00:12:32.200]   social platform into an untenable solution.
[00:12:32.200 --> 00:12:37.240]   And I'm optimistic that if enough folks stay behind and that folks are keen on having it
[00:12:37.240 --> 00:12:42.280]   remain what it is, which is basically an open platform for discussion on all subjects
[00:12:42.280 --> 00:12:46.600]   with reasonably okay moderation, then I hope that we can continue to improve the situation
[00:12:46.600 --> 00:12:48.120]   and not make it worse.
[00:12:48.120 --> 00:12:52.120]   Twitter is made up of the people who tweet on it, not the people who own it.
[00:12:52.120 --> 00:12:54.000]   So far, yeah.
[00:12:54.000 --> 00:12:55.000]   How about you, Owen?
[00:12:55.000 --> 00:12:58.160]   Well, this is the insane thing.
[00:12:58.160 --> 00:13:02.760]   You know, Elon Musk says Twitter has a problem because like Taylor Swift isn't tweeting.
[00:13:02.760 --> 00:13:07.840]   And then he's doing everything he can to make it as unpleasant as possible for someone
[00:13:07.840 --> 00:13:11.960]   like Taylor Swift to be on Twitter by his stated plan.
[00:13:11.960 --> 00:13:17.440]   The reason those top 10 tweeters don't tweet very much is it's a hostile environment for
[00:13:17.440 --> 00:13:18.440]   them.
[00:13:18.440 --> 00:13:19.440]   Is that what you're saying?
[00:13:19.440 --> 00:13:20.440]   Yeah, there's no payback.
[00:13:20.440 --> 00:13:22.440]   It's a real benefit.
[00:13:22.440 --> 00:13:23.440]   Yeah.
[00:13:23.440 --> 00:13:26.160]   They use it as kind of one way broadcast.
[00:13:26.160 --> 00:13:29.040]   You know, they've got a new record that's dropping.
[00:13:29.040 --> 00:13:30.640]   You know, there's a new music video.
[00:13:30.640 --> 00:13:36.320]   They tell their followers, their followers, you know, good wild for a second and they don't
[00:13:36.320 --> 00:13:39.880]   stay to interact because there's nothing in it for them.
[00:13:39.880 --> 00:13:40.880]   They don't get value.
[00:13:40.880 --> 00:13:41.880]   Right.
[00:13:41.880 --> 00:13:42.880]   There are has zero.
[00:13:42.880 --> 00:13:45.440]   Zero plan to many people pointed this out.
[00:13:45.440 --> 00:13:48.120]   There are smaller communities on Twitter.
[00:13:48.120 --> 00:13:52.800]   Black Twitter is the one most people use as an example where there's somewhat insular
[00:13:52.800 --> 00:13:57.120]   and they are really important to the people who are members of those communities.
[00:13:57.120 --> 00:13:59.680]   I'm sure there are many more.
[00:13:59.680 --> 00:14:02.880]   I hate to lose that for those people.
[00:14:02.880 --> 00:14:08.760]   And I don't think anything Elon's thinking about doing is going to make that less
[00:14:08.760 --> 00:14:10.720]   tenable or is it Corey?
[00:14:10.720 --> 00:14:14.080]   What are the dangers of what Elon might do?
[00:14:14.080 --> 00:14:22.400]   Well, I mean, here's the thing that I think we should take away from this, which is that
[00:14:22.400 --> 00:14:30.920]   when Twitter was run by a board of like however many eight extremely wealthy tech people,
[00:14:30.920 --> 00:14:36.320]   it was not well managed to the extent that those were the eight people who decided to
[00:14:36.320 --> 00:14:37.840]   sell it to Elon Musk.
[00:14:37.840 --> 00:14:38.840]   Right.
[00:14:38.840 --> 00:14:43.320]   So if you think, by the way, he points out that none of them were real Twitter users.
[00:14:43.320 --> 00:14:44.320]   Right.
[00:14:44.320 --> 00:14:49.200]   So, but if you think Elon Musk shouldn't be in charge of Twitter, then perforce you
[00:14:49.200 --> 00:14:53.640]   shouldn't think you should think that the people who put him in charge of Twitter also
[00:14:53.640 --> 00:14:55.000]   shouldn't be in charge of Twitter.
[00:14:55.000 --> 00:14:56.000]   Right.
[00:14:56.000 --> 00:14:58.880]   Well, also the shareholders are who is going to decide whether he gets it.
[00:14:58.880 --> 00:15:00.800]   He still has to get a shareholder vote.
[00:15:00.800 --> 00:15:01.800]   Sure.
[00:15:01.800 --> 00:15:03.040]   That's very true.
[00:15:03.040 --> 00:15:08.200]   But I think that, you know, there are lots of ways that Twitter could become really awful.
[00:15:08.200 --> 00:15:12.960]   So, you know, this human verification thing, there are lots of ways that it can go very
[00:15:12.960 --> 00:15:19.160]   wrong, basically Musk is proposing to do in some way to gather a gigantic amount of
[00:15:19.160 --> 00:15:23.040]   potentially compromising personal identifying information because uniquely identifying and
[00:15:23.040 --> 00:15:27.960]   disambiguating a hundred million people in a thousand countries or a hundred countries
[00:15:27.960 --> 00:15:33.320]   rather is a really big job and it's going to involve either government ID or biometric
[00:15:33.320 --> 00:15:34.320]   or something.
[00:15:34.320 --> 00:15:39.560]   And he's not averse to that because Tesla gathers all the information about every drive.
[00:15:39.560 --> 00:15:40.560]   Sure.
[00:15:40.560 --> 00:15:44.840]   Remember when I had a Model X order in, I felt like I shouldn't say anything about Elon
[00:15:44.840 --> 00:15:49.600]   because a well-known blogger did say something negative about Tesla and Elon canceled his
[00:15:49.600 --> 00:15:50.600]   order.
[00:15:50.600 --> 00:15:54.560]   So, yeah, he's very, he's not a free speech.
[00:15:54.560 --> 00:15:55.560]   Very capricious, right?
[00:15:55.560 --> 00:15:56.560]   It's capricious.
[00:15:56.560 --> 00:15:57.560]   That's a better word than anything.
[00:15:57.560 --> 00:15:58.560]   Yeah.
[00:15:58.560 --> 00:16:05.520]   So, you know, if Musk says, well, what we should do is aggregate all this potentially
[00:16:05.520 --> 00:16:10.800]   sensitive personal identifying information and then never leak it, he's being extremely
[00:16:10.800 --> 00:16:12.160]   optimistic, right?
[00:16:12.160 --> 00:16:13.960]   And rather reckless.
[00:16:13.960 --> 00:16:19.200]   About, well, now 15 years ago, I wrote a column where I compare personal identifying
[00:16:19.200 --> 00:16:25.440]   information to uranium and plutonium because I grew up in Ontario where we mine a lot of
[00:16:25.440 --> 00:16:27.400]   the world's uranium.
[00:16:27.400 --> 00:16:30.560]   And when uranium comes out of the ground, it's pretty benign.
[00:16:30.560 --> 00:16:33.920]   You can actually like buy it on Amazon for science fair projects.
[00:16:33.920 --> 00:16:37.040]   But once you refine it into plutonium, it's immortal.
[00:16:37.040 --> 00:16:41.720]   Like, it will last longer than any human institution or built object.
[00:16:41.720 --> 00:16:44.160]   It is like pluripotent, right?
[00:16:44.160 --> 00:16:46.880]   There's no amount of it that's so small that it won't kill you.
[00:16:46.880 --> 00:16:51.680]   So you can't unmake it and it is infinitely toxic.
[00:16:51.680 --> 00:16:59.000]   And if you pile up all of that PII, it will end up kind of slithering around within and
[00:16:59.000 --> 00:17:00.240]   outside of Twitter.
[00:17:00.240 --> 00:17:06.560]   But the only way to keep data from leaking is to neither retain it nor collect it ideally.
[00:17:06.560 --> 00:17:09.240]   And that exposes a lot of people to risk.
[00:17:09.240 --> 00:17:14.560]   So that's just like one example of what it means to have like a dillotant show up.
[00:17:14.560 --> 00:17:19.280]   Well, even crosses my mind, that may be part of the, because one of the questions is, how
[00:17:19.280 --> 00:17:20.760]   is he going to make any money on this?
[00:17:20.760 --> 00:17:22.120]   This is a huge expense.
[00:17:22.120 --> 00:17:24.160]   No one's ever made money on Twitter.
[00:17:24.160 --> 00:17:25.840]   How does he justify this expense?
[00:17:25.840 --> 00:17:28.360]   Maybe the information is the point.
[00:17:28.360 --> 00:17:31.680]   Well, I think he doesn't know what he's doing.
[00:17:31.680 --> 00:17:35.320]   You know, I think that he's someone who's extremely narcissistic.
[00:17:35.320 --> 00:17:39.400]   You know, I'm not trying to diagnose him at a distance, but this is the guy who didn't
[00:17:39.400 --> 00:17:44.040]   found Tesla and bought it from people who did found Tesla and as part of the contract,
[00:17:44.040 --> 00:17:47.680]   made them promise that they would call him a co-founder even though he wasn't.
[00:17:47.680 --> 00:17:48.680]   Yeah.
[00:17:48.680 --> 00:17:49.880]   That's a really weird thing.
[00:17:49.880 --> 00:17:54.320]   He also calls himself the chief engineer of Tesla, even though he's not an engineer of
[00:17:54.320 --> 00:17:55.480]   Tesla.
[00:17:55.480 --> 00:17:59.520]   And when you write about, if you're a journalist and you write about the chief engineer of
[00:17:59.520 --> 00:18:04.200]   Tesla, their PR department will contact you and say, you'll find that Elon is the chief
[00:18:04.200 --> 00:18:07.200]   engineer of Tesla.
[00:18:07.200 --> 00:18:10.400]   The person you're talking about is like the chief operating engineer.
[00:18:10.400 --> 00:18:12.400]   He's the guy who does the work.
[00:18:12.400 --> 00:18:13.400]   Yeah.
[00:18:13.400 --> 00:18:17.760]   Elon wears the like the engineer hat and gets to pull the whistle on the railroad.
[00:18:17.760 --> 00:18:21.160]   So he thinks he's the engineer, right?
[00:18:21.160 --> 00:18:23.680]   It is a really bizarre circumstance.
[00:18:23.680 --> 00:18:25.960]   So I don't know that, I mean, he's pretty chaotic, right?
[00:18:25.960 --> 00:18:28.320]   He does a lot of chaotic things.
[00:18:28.320 --> 00:18:31.840]   I think that he's got poor impulse control.
[00:18:31.840 --> 00:18:34.400]   I think he's a very good showman.
[00:18:34.400 --> 00:18:38.800]   And I think that he doesn't really know what he's getting Twitter for.
[00:18:38.800 --> 00:18:41.400]   I've heard some theories that I think are kind of plausible.
[00:18:41.400 --> 00:18:47.200]   Like if he has Twitter, he can use it as like a PR vehicle for his other stuff.
[00:18:47.200 --> 00:18:48.200]   Yeah.
[00:18:48.200 --> 00:18:49.200]   Twitter.
[00:18:49.200 --> 00:18:50.200]   But not $44 billion worth.
[00:18:50.200 --> 00:18:51.200]   Well.
[00:18:51.200 --> 00:18:53.080]   I mean, you could do that for free right now.
[00:18:53.080 --> 00:18:54.080]   Yeah.
[00:18:54.080 --> 00:18:56.160]   He doesn't have to buy Twitter to do that.
[00:18:56.160 --> 00:19:00.440]   But you know, so here's, so here's my Elon Musk story.
[00:19:00.440 --> 00:19:07.840]   One day I was on Twitter and someone retweeted Elon Musk saying, I consider myself a utopian
[00:19:07.840 --> 00:19:09.880]   socialist in the mode of Ian Banks.
[00:19:09.880 --> 00:19:12.600]   So science fiction writer is dead.
[00:19:12.600 --> 00:19:13.920]   And I couldn't help it.
[00:19:13.920 --> 00:19:19.320]   And I replied and I said, I knew Ian, he was an ardent trade unionist.
[00:19:19.320 --> 00:19:24.000]   The National Labor Review Board has sanctioned you like four times in the last three months.
[00:19:24.000 --> 00:19:28.600]   I don't think you can call yourself a utopian socialist in the mode of Ian Banks.
[00:19:28.600 --> 00:19:32.440]   And we got into a really dumb argument where he said things like, well, there aren't any
[00:19:32.440 --> 00:19:34.840]   unions in banks as culture novels.
[00:19:34.840 --> 00:19:41.640]   And I said, yes, they're set on solar system size spaceships that travel thousands of times
[00:19:41.640 --> 00:19:45.800]   the speed of light that have a trillion people living on them and are piloted by superhuman
[00:19:45.800 --> 00:19:51.800]   AI's. And he said, you know, well, if if Ian could have seen one of my factories, he'd
[00:19:51.800 --> 00:19:53.760]   see that it doesn't need a union either.
[00:19:53.760 --> 00:19:54.760]   Oh, boy.
[00:19:54.760 --> 00:20:00.280]   You know, there is a ton of different fighting words difference between faster than light
[00:20:00.280 --> 00:20:04.120]   travel and he can get marginal gains in the production of electric vehicles.
[00:20:04.120 --> 00:20:07.640]   And then he called me an enemy of humanity, I think.
[00:20:07.640 --> 00:20:08.880]   Oh my God.
[00:20:08.880 --> 00:20:10.560]   But here's the thing that was really weird about it.
[00:20:10.560 --> 00:20:11.560]   That part wasn't weird.
[00:20:11.560 --> 00:20:13.120]   That was just like, oh, he's just a crank.
[00:20:13.120 --> 00:20:17.360]   But the part that was really weird was all the bots that follow Elon to try and put him
[00:20:17.360 --> 00:20:18.360]   on tilt.
[00:20:18.360 --> 00:20:20.480]   I actually think this is a lot of it.
[00:20:20.480 --> 00:20:26.640]   There are, there are for sure 100% shorts who try to put Elon on tilt with bots that
[00:20:26.640 --> 00:20:28.560]   just troll him night and day.
[00:20:28.560 --> 00:20:33.000]   And vice versa, I might add, he has his own bot army.
[00:20:33.000 --> 00:20:35.360]   I that is may entirely be true.
[00:20:35.360 --> 00:20:38.880]   I mean, I definitely there were a lot of like, notice me, senpai people in there who I don't
[00:20:38.880 --> 00:20:44.240]   think were bots, I think that they were, you know, sad fanboys, but that like the actual
[00:20:44.240 --> 00:20:46.360]   like inauthentic conduct, right?
[00:20:46.360 --> 00:20:50.680]   Like just hundreds of bots that showed up that I had to block before I could even read
[00:20:50.680 --> 00:20:53.920]   my timeline after being mentioned by mosque.
[00:20:53.920 --> 00:20:54.920]   I still get them.
[00:20:54.920 --> 00:20:55.920]   Yeah.
[00:20:55.920 --> 00:20:56.920]   Yeah.
[00:20:56.920 --> 00:20:58.000]   I still get them like this is five years later.
[00:20:58.000 --> 00:21:02.240]   And I still like a couple of times a week, we'll have to block a bot that just goes through
[00:21:02.240 --> 00:21:06.440]   everything mosque has ever tweeted and tweets weird horrendous garbage to see if they can
[00:21:06.440 --> 00:21:11.120]   put him on tilt so that short positions, I think so short positions can pay off maybe
[00:21:11.120 --> 00:21:12.520]   because they're just trolls.
[00:21:12.520 --> 00:21:13.520]   Who knows?
[00:21:13.520 --> 00:21:14.520]   Right.
[00:21:14.520 --> 00:21:17.240]   So I think that like some of what he's motivated by all this stuff about like we have to get
[00:21:17.240 --> 00:21:21.720]   rid of the bots is that he has got this unbelievably specific thing.
[00:21:21.720 --> 00:21:26.520]   It's like, it's like if he suddenly was like, really, like he bought all of CVS because
[00:21:26.520 --> 00:21:32.560]   he wanted the special shampoos used to treat hair plugs to be more, you know, prominently
[00:21:32.560 --> 00:21:33.560]   featured.
[00:21:33.560 --> 00:21:38.320]   It was really hard to find it.
[00:21:38.320 --> 00:21:41.320]   It's a problem for people.
[00:21:41.320 --> 00:21:43.320]   It was just a problem for him.
[00:21:43.320 --> 00:21:46.720]   You know, Corey, I think you're really onto something there because Elon really got active
[00:21:46.720 --> 00:21:49.400]   on Twitter to like fight with Tesla's short sellers.
[00:21:49.400 --> 00:21:50.400]   Yeah.
[00:21:50.400 --> 00:21:57.320]   So the idea that they're, you know, they're jitting him up, you know, like egging him on.
[00:21:57.320 --> 00:22:00.160]   It's actually pretty plausible.
[00:22:00.160 --> 00:22:04.960]   I think I asked Corey if they make his life miserable.
[00:22:04.960 --> 00:22:07.360]   So maybe that's all he's trying to do is get rid of that.
[00:22:07.360 --> 00:22:08.360]   Yeah.
[00:22:08.360 --> 00:22:09.360]   Yeah.
[00:22:09.360 --> 00:22:11.040]   I asked Corey what Elon could do to screw Twitter up.
[00:22:11.040 --> 00:22:15.720]   Let me ask you, Tim Stevens, what Elon could do to make Twitter better?
[00:22:15.720 --> 00:22:20.000]   I put you in the spot.
[00:22:20.000 --> 00:22:25.520]   I honestly feel like, you know, they always say, no, no plan survives the first encounter
[00:22:25.520 --> 00:22:26.520]   with the enemy.
[00:22:26.520 --> 00:22:32.800]   I really think that Elon's plans to free speech, open the algorithm and get rid of the bots
[00:22:32.800 --> 00:22:36.100]   is only going to last as long as until he gets in the chair.
[00:22:36.100 --> 00:22:38.480]   And then who knows what he's going to do.
[00:22:38.480 --> 00:22:39.480]   Yeah.
[00:22:39.480 --> 00:22:40.480]   What should he do?
[00:22:40.480 --> 00:22:42.560]   I mean, I think the number one thing is what Corey was talking about earlier, which would
[00:22:42.560 --> 00:22:46.040]   be the ability to take your audience with you to another platform if you want to.
[00:22:46.040 --> 00:22:49.400]   But since we've already covered that ground, I think ultimately has to be much, much better
[00:22:49.400 --> 00:22:53.760]   moderation tools and much smarter moderation as well.
[00:22:53.760 --> 00:22:58.640]   As I can see Twitter on my phone and on my desktop and a couple of platforms as well.
[00:22:58.640 --> 00:23:02.720]   And as I go from one of the other, I'll actually see different sets of tweets because my phone
[00:23:02.720 --> 00:23:07.160]   has a different level of basically profanity filter and other filters than other platforms
[00:23:07.160 --> 00:23:13.360]   do, which just kind of shows how weak Twitter is at actually keeping you from seeing abusive
[00:23:13.360 --> 00:23:15.840]   content, even if it's light stuff.
[00:23:15.840 --> 00:23:20.840]   So I think if Musk really wants to open the floodgates and let everybody back on, then
[00:23:20.840 --> 00:23:25.200]   ultimately they need a lot better tools to give me the power to control what I want to
[00:23:25.200 --> 00:23:28.960]   see and what I don't want to see, whether it be misinformation, whether it be hate speech,
[00:23:28.960 --> 00:23:32.800]   whether it be whatever, as of now, depending on which platform I go to, all that stuff
[00:23:32.800 --> 00:23:34.680]   gets through.
[00:23:34.680 --> 00:23:38.600]   And I have it relatively easy compared to a lot of folks who I know who have, you know,
[00:23:38.600 --> 00:23:41.640]   a lot of people who are saying very mean things to them.
[00:23:41.640 --> 00:23:46.200]   So if he's really going to open the floodgates in the name of quote unquote free speech, then
[00:23:46.200 --> 00:23:48.960]   moderation tools need to improve significantly.
[00:23:48.960 --> 00:23:53.360]   And it's for end users as well to control what they see.
[00:23:53.360 --> 00:23:57.680]   Mike Masnick made an excellent point of tech dirt saying Elon is essentially saying what
[00:23:57.680 --> 00:24:02.800]   people said at the beginning of Twitter before they really had Twitter and he's just going
[00:24:02.800 --> 00:24:08.800]   to throw out 15 years of struggling with moderating a platform like this and all the
[00:24:08.800 --> 00:24:10.360]   knowledge gained there.
[00:24:10.360 --> 00:24:14.800]   It sounds like Elon just saying, no, no, no, no, you guys, we, we, I got an idea.
[00:24:14.800 --> 00:24:16.960]   Let's do it better.
[00:24:16.960 --> 00:24:20.880]   I don't want to go on and on about Elon, but, but somebody in the chat, I'm saying somebody
[00:24:20.880 --> 00:24:22.760]   should defend Elon.
[00:24:22.760 --> 00:24:26.480]   I guess it's you, Owen.
[00:24:26.480 --> 00:24:28.560]   Defend the name.
[00:24:28.560 --> 00:24:30.720]   I mean, look, he created SpaceX.
[00:24:30.720 --> 00:24:32.520]   Amazing, right?
[00:24:32.520 --> 00:24:38.360]   Tesla has changed the world for electric vehicles, which someone else found it.
[00:24:38.360 --> 00:24:41.200]   I understand, but he certainly took it.
[00:24:41.200 --> 00:24:42.600]   And you know, you could also make the argument.
[00:24:42.600 --> 00:24:47.760]   He did it with government grants, but he's taken to electric vehicles into the mainstream.
[00:24:47.760 --> 00:24:50.680]   Thank you, Elon.
[00:24:50.680 --> 00:24:51.680]   SpaceX is pretty cool.
[00:24:51.680 --> 00:24:53.200]   They landed those two rockets like that.
[00:24:53.200 --> 00:24:54.200]   That was cool.
[00:24:54.200 --> 00:24:58.360]   They're certainly better doing better than NASA or the Russians.
[00:24:58.360 --> 00:25:00.480]   Using a lot of NASA technology.
[00:25:00.480 --> 00:25:01.480]   Okay.
[00:25:01.480 --> 00:25:02.480]   Yeah.
[00:25:02.480 --> 00:25:05.120]   Well, you know, they wouldn't exist without NASA the same way Apple wouldn't exist without
[00:25:05.120 --> 00:25:06.280]   the DoD.
[00:25:06.280 --> 00:25:11.920]   You know, I, to, Elon is basically kind of a Steve Jobsian character where, no, he didn't
[00:25:11.920 --> 00:25:14.960]   invent any of this, but he's a very, he's a master marketer.
[00:25:14.960 --> 00:25:15.960]   Yeah.
[00:25:15.960 --> 00:25:16.960]   Great.
[00:25:16.960 --> 00:25:17.960]   Absolutely.
[00:25:17.960 --> 00:25:24.720]   And, and I think as soon as Elon Musk can realize his plan to get a human to Mars, he
[00:25:24.720 --> 00:25:25.720]   should do it.
[00:25:25.720 --> 00:25:28.720]   Go, Elon, go.
[00:25:28.720 --> 00:25:38.080]   You know, we played that Penelope Scott song about Elon Musk.
[00:25:38.080 --> 00:25:39.080]   Yes.
[00:25:39.080 --> 00:25:44.640]   But that still is the thing that I, that I come back to whenever I think about him, because
[00:25:44.640 --> 00:25:47.320]   she does capture that ambivalence that you're expressing.
[00:25:47.320 --> 00:25:48.320]   Yeah.
[00:25:48.320 --> 00:25:49.320]   About, it's difficult.
[00:25:49.320 --> 00:25:52.560]   No, he's got, I drove a Tesla for a long time.
[00:25:52.560 --> 00:25:54.720]   I really love the fact that he made it.
[00:25:54.720 --> 00:25:56.880]   And I'll only drive electric vehicles from now on.
[00:25:56.880 --> 00:26:00.040]   I love the fact that he made that mainstream.
[00:26:00.040 --> 00:26:04.800]   I don't know if that's his past and his future is more chaotic, or if this is just the guy
[00:26:04.800 --> 00:26:05.800]   all along.
[00:26:05.800 --> 00:26:12.280]   I can tell you one thing, the, both the good and the bad of Twitter is it, it takes people
[00:26:12.280 --> 00:26:21.600]   like you, Corey, and, and brings you out into the, into the fracka of the modern world.
[00:26:21.600 --> 00:26:25.160]   And, and it's one of the reasons I stopped using Twitter is it's almost inevitable that
[00:26:25.160 --> 00:26:31.240]   you get in a battle eventually and maybe say things that you regret eventually.
[00:26:31.240 --> 00:26:35.880]   And I don't think anybody benefits from being on Twitter as much as they benefit from not
[00:26:35.880 --> 00:26:38.320]   being on Twitter.
[00:26:38.320 --> 00:26:40.960]   Maybe, maybe that's, that's what Elon will do for Twitter.
[00:26:40.960 --> 00:26:42.880]   Kick us all off.
[00:26:42.880 --> 00:26:44.320]   I wouldn't mind that either.
[00:26:44.320 --> 00:26:46.040]   I mean, I like Twitter.
[00:26:46.040 --> 00:26:49.080]   Maybe that's an unpopular opinion, but I know it's very popular.
[00:26:49.080 --> 00:26:52.360]   No, no, I try to convince people how awful Twitter is all the time.
[00:26:52.360 --> 00:26:53.360]   Nobody will.
[00:26:53.360 --> 00:26:56.640]   But I think people on Twitter like to talk about how much they don't like it.
[00:26:56.640 --> 00:27:01.440]   I can, I can name some things about Twitter that I think should be fixed in terms of like
[00:27:01.440 --> 00:27:03.000]   content moderation and stuff.
[00:27:03.000 --> 00:27:05.680]   I got dog piled at one point by this.
[00:27:05.680 --> 00:27:06.680]   It was hilarious.
[00:27:06.680 --> 00:27:10.560]   I wrote about, about police violence and this cop said, well, why don't you come along
[00:27:10.560 --> 00:27:11.920]   for a ride along?
[00:27:11.920 --> 00:27:13.800]   And, you know, and I'll show you what it's really like.
[00:27:13.800 --> 00:27:18.360]   And I was just like this kind of bad faith response to a pretty subtle and significant
[00:27:18.360 --> 00:27:19.360]   argument I made.
[00:27:19.360 --> 00:27:20.880]   And I just, I just blocked him.
[00:27:20.880 --> 00:27:27.560]   And so then he quote tweeted or he screen-capped the block and he had like 12,000 or 20,000
[00:27:27.560 --> 00:27:30.640]   followers and said, hey, this guy hates cops.
[00:27:30.640 --> 00:27:34.440]   Why don't you gang up on him and tell him a piece of shit is.
[00:27:34.440 --> 00:27:38.080]   And then I just got like, like literally thousands of like, I want to come over to your house
[00:27:38.080 --> 00:27:39.400]   and kill you kind of tweets.
[00:27:39.400 --> 00:27:43.400]   Like, I hope the Burbank police, I hope someone murders you in Burbank and the Burbank Police
[00:27:43.400 --> 00:27:45.480]   Department don't do anything about it.
[00:27:45.480 --> 00:27:48.680]   And I was like, yeah, this is definitely a guy I should have gone on a ride along with
[00:27:48.680 --> 00:27:52.320]   because he's really proved himself to be kind and the kind of person who I'd learned
[00:27:52.320 --> 00:27:53.320]   a lot from.
[00:27:53.320 --> 00:27:56.200]   But the point was there was no easy way to do to block it.
[00:27:56.200 --> 00:28:01.920]   I tried using a bunch of tools like block together to block all of this guy's followers
[00:28:01.920 --> 00:28:07.440]   to just say like, I don't, I just, if you're following this like ghastly monster of a human,
[00:28:07.440 --> 00:28:09.920]   I just don't want to ever hear from you.
[00:28:09.920 --> 00:28:17.480]   And all of those tools, Tripped Twitters, you are doing too much automated stuff, filters.
[00:28:17.480 --> 00:28:21.480]   And they locked me out of my account.
[00:28:21.480 --> 00:28:23.280]   That was like a compounding feeling.
[00:28:23.280 --> 00:28:25.600]   Like there are a bunch of things like that if you're like a Twitter power.
[00:28:25.600 --> 00:28:27.520]   I'm sure everyone's got like half a dozen of you.
[00:28:27.520 --> 00:28:32.520]   You should point out you accomplished your goal though because you're off to it.
[00:28:32.520 --> 00:28:33.520]   That worked.
[00:28:33.520 --> 00:28:34.520]   Right.
[00:28:34.520 --> 00:28:36.560]   Well, how you wanted to accomplish it?
[00:28:36.560 --> 00:28:38.000]   Well no, they could still reply to me.
[00:28:38.000 --> 00:28:39.320]   I just and I could read it.
[00:28:39.320 --> 00:28:40.320]   I couldn't block them.
[00:28:40.320 --> 00:28:41.320]   You couldn't do anything.
[00:28:41.320 --> 00:28:42.320]   Right.
[00:28:42.320 --> 00:28:43.320]   It just locked my account.
[00:28:43.320 --> 00:28:49.400]   It's stupid edge case, fail in there like moderation stuff.
[00:28:49.400 --> 00:28:52.720]   There are a bunch of things that I could completely see building that would be really
[00:28:52.720 --> 00:28:53.720]   good.
[00:28:53.720 --> 00:28:56.680]   You know, one of the things that Twitter did that I was very skeptical of and now I'm
[00:28:56.680 --> 00:29:01.440]   like, oh my God, that was brilliant is hide as an option.
[00:29:01.440 --> 00:29:04.560]   So someone replies to you can hide their reply.
[00:29:04.560 --> 00:29:08.120]   And so what people see when they go to your tweet is they see there are some hidden replies
[00:29:08.120 --> 00:29:10.200]   which they can click and see.
[00:29:10.200 --> 00:29:16.160]   And what that does is often the worst kinds of arguments are the ones where it's just
[00:29:16.160 --> 00:29:19.040]   people trolling in foolish ways.
[00:29:19.040 --> 00:29:23.640]   And it just means that everyone ends up talking about something that's not very interesting.
[00:29:23.640 --> 00:29:27.120]   And also like just generates a lot more heat than light.
[00:29:27.120 --> 00:29:31.320]   And what I can do, I think of myself as, you know, if I have a tweet that gets a lot
[00:29:31.320 --> 00:29:36.080]   of retweets and so on, I think of myself as someone who instigated a conversation and
[00:29:36.080 --> 00:29:41.960]   the way that I can be, you know, express my duty to that conversation to try and make
[00:29:41.960 --> 00:29:46.440]   it good is I can hide the stuff that I think is dumb and the people who want to read it
[00:29:46.440 --> 00:29:48.800]   can and they can reply to it if they want.
[00:29:48.800 --> 00:29:53.760]   But unless you take an extra step, you don't see it, you're not tempted to reply to it.
[00:29:53.760 --> 00:29:56.240]   And the actual caliber of the conversation goes up.
[00:29:56.240 --> 00:30:00.280]   So if you're someone who like kicks off big interesting Twitter conversations, they've
[00:30:00.280 --> 00:30:05.200]   produced some pretty simple and innovative tools that actually make those conversations
[00:30:05.200 --> 00:30:06.200]   better.
[00:30:06.200 --> 00:30:10.360]   And so, you know, there's a lot of stuff on Twitter that I really rather like.
[00:30:10.360 --> 00:30:14.200]   And then there's a bunch of stuff that I don't like too.
[00:30:14.200 --> 00:30:16.120]   I just would like Twitter.
[00:30:16.120 --> 00:30:21.480]   I just think that some of the hard Twitter calls to make about what shouldn't shouldn't
[00:30:21.480 --> 00:30:26.560]   be permitted are calls that are much better made among small communities, right?
[00:30:26.560 --> 00:30:32.000]   One of the constant failure modes of Twitter and all social media is the inability to distinguish
[00:30:32.000 --> 00:30:33.800]   speech from counter speech.
[00:30:33.800 --> 00:30:38.960]   So there's a big difference between that guy calling someone the N word and saying, I can't
[00:30:38.960 --> 00:30:41.320]   believe I just called the N word.
[00:30:41.320 --> 00:30:44.520]   And yet both of those tend to trip the same filter.
[00:30:44.520 --> 00:30:51.360]   And so that's a really big problem, especially since people who's like, you know, burning
[00:30:51.360 --> 00:30:56.640]   passion in life is figuring out how to hurl racial slurs at people can have all day long
[00:30:56.640 --> 00:31:01.520]   to think of euphemisms for the N word or putting, you know, words in brackets to indicate
[00:31:01.520 --> 00:31:03.200]   Jew or whatever.
[00:31:03.200 --> 00:31:07.560]   And then the people who they're actually targeting have just want to live their lives.
[00:31:07.560 --> 00:31:10.560]   And so they're the ones who aren't going to use the euphemisms and are going to get
[00:31:10.560 --> 00:31:15.520]   killed by the algorithm or by the moderators who are over busy or moderating in languages
[00:31:15.520 --> 00:31:17.040]   they don't speak.
[00:31:17.040 --> 00:31:21.360]   And the actual like bad actors are trying to catch are the ones who don't.
[00:31:21.360 --> 00:31:25.400]   And really the people who can distinguish speech from counter speech are the people in
[00:31:25.400 --> 00:31:31.880]   the affected community and letting them find their own place where they can talk and set
[00:31:31.880 --> 00:31:36.960]   their own rules and decide what they do and don't block is I think the only way we can
[00:31:36.960 --> 00:31:41.560]   answer this is it's not like we have to add more pages to the three ring binder that has
[00:31:41.560 --> 00:31:49.280]   the moderation policies and enough branching if thens that we cover all potential conversational
[00:31:49.280 --> 00:31:50.920]   possibilities, right?
[00:31:50.920 --> 00:31:55.800]   We just need people who understand the context to be the ones in charge of determining what
[00:31:55.800 --> 00:31:57.320]   what stays and what goes.
[00:31:57.320 --> 00:31:59.400]   That does not scale well and that's really the problem.
[00:31:59.400 --> 00:32:02.640]   So you're saying small massed on instances something like federates.
[00:32:02.640 --> 00:32:03.640]   Yeah, federates.
[00:32:03.640 --> 00:32:04.640]   We do it.
[00:32:04.640 --> 00:32:05.640]   Yeah, yeah.
[00:32:05.640 --> 00:32:06.640]   Right.
[00:32:06.640 --> 00:32:07.640]   Yeah.
[00:32:07.640 --> 00:32:08.640]   Figuring out what to have for dinner doesn't scale, which is why we don't try and come up
[00:32:08.640 --> 00:32:10.880]   with a single menu plan for the whole country.
[00:32:10.880 --> 00:32:11.880]   Right.
[00:32:11.880 --> 00:32:18.360]   We just Jack Dorsey and the blue sky project he was working on something that sounded
[00:32:18.360 --> 00:32:19.840]   like it was sort of federated.
[00:32:19.840 --> 00:32:22.280]   Do any of you know what blue sky?
[00:32:22.280 --> 00:32:23.280]   Yeah.
[00:32:23.280 --> 00:32:24.280]   Yeah.
[00:32:24.280 --> 00:32:29.520]   I learned something cool about blue sky, which is that they have a $15 million endowment
[00:32:29.520 --> 00:32:32.440]   and no oversight from Twitter.
[00:32:32.440 --> 00:32:35.960]   So it actually doesn't matter what who runs Twitter.
[00:32:35.960 --> 00:32:38.320]   Blue sky gets to go on now.
[00:32:38.320 --> 00:32:44.160]   Who runs Twitter matters a lot if blue sky makes something and Twitter decides not to
[00:32:44.160 --> 00:32:45.160]   use it, right?
[00:32:45.160 --> 00:32:50.240]   That Twitter is under no obligation to use whatever blue sky comes up with because they're
[00:32:50.240 --> 00:32:52.560]   not part of Twitter formally.
[00:32:52.560 --> 00:32:57.080]   But it also means that if Twitter changes their mind or comes under new management,
[00:32:57.080 --> 00:33:01.400]   they can't be hamstrung, at least not in their development work.
[00:33:01.400 --> 00:33:05.760]   Their rollout can obviously be hamstrung by whoever is running Twitter.
[00:33:05.760 --> 00:33:12.520]   And it's in blue skies mission to do it decentralized.
[00:33:12.520 --> 00:33:13.520]   Yeah.
[00:33:13.520 --> 00:33:15.200]   And they had a key to it, right?
[00:33:15.200 --> 00:33:20.560]   They had this idea they called an app store for moderation, which I mean, that's an interesting
[00:33:20.560 --> 00:33:21.840]   way of phrasing it.
[00:33:21.840 --> 00:33:25.120]   I don't know that I don't know exactly where that fits.
[00:33:25.120 --> 00:33:28.280]   I'd be interested to see what they come out with when they come out with it.
[00:33:28.280 --> 00:33:32.440]   I knew some of the people involved when it was getting started up and they were people
[00:33:32.440 --> 00:33:34.120]   I thought very highly of.
[00:33:34.120 --> 00:33:37.640]   So you know, I'm glad to see blue sky kind of rolling along.
[00:33:37.640 --> 00:33:40.680]   The other thing we should mention is the digital markets act in the European Union and the
[00:33:40.680 --> 00:33:49.280]   Access Act in the US, DMA initially only affects and send in or messenger services.
[00:33:49.280 --> 00:33:54.560]   And but eventually will cover social media and access act will cover social media, although
[00:33:54.560 --> 00:33:56.040]   not Twitter, it's not big enough people.
[00:33:56.040 --> 00:33:57.520]   We talk a lot about Twitter.
[00:33:57.520 --> 00:33:58.520]   Twitter is incredibly tiny.
[00:33:58.520 --> 00:34:01.760]   It's a hundred million users versus three billion on Facebook.
[00:34:01.760 --> 00:34:02.760]   Like it's not.
[00:34:02.760 --> 00:34:06.600]   These are laws to encourage interoperability, right?
[00:34:06.600 --> 00:34:07.600]   To require it.
[00:34:07.600 --> 00:34:14.440]   Yeah, to say like if someone shows up, so what it would do is it would say if Twitter showed
[00:34:14.440 --> 00:34:19.320]   up at Facebook's door or Mastodon or you or me and said, I want to interoperate, they
[00:34:19.320 --> 00:34:24.120]   would have to like expose an API to us that would let our users exchange messages and
[00:34:24.120 --> 00:34:26.320]   be in communities with Facebook users.
[00:34:26.320 --> 00:34:27.320]   Yeah.
[00:34:27.320 --> 00:34:31.840]   Unfortunately, you have to have 75 billion euros of worth.
[00:34:31.840 --> 00:34:35.200]   No, no, that's to be mandated.
[00:34:35.200 --> 00:34:36.200]   Oh, that's mandated.
[00:34:36.200 --> 00:34:37.200]   Okay.
[00:34:37.200 --> 00:34:39.560]   Yeah, no, that's the other end of it.
[00:34:39.560 --> 00:34:40.560]   So like there's this.
[00:34:40.560 --> 00:34:41.880]   So they can go to Facebook.
[00:34:41.880 --> 00:34:42.880]   I see what you're saying.
[00:34:42.880 --> 00:34:46.920]   Yeah, Facebook could have come to Twitter and say you have to interoperate, but Twitter
[00:34:46.920 --> 00:34:47.920]   could go to Facebook.
[00:34:47.920 --> 00:34:48.920]   Nice.
[00:34:48.920 --> 00:34:49.920]   Yeah.
[00:34:49.920 --> 00:34:50.920]   All right, I want to take a little break.
[00:34:50.920 --> 00:34:54.560]   We were done with the Elon Musk.
[00:34:54.560 --> 00:34:59.440]   Although I really was looking forward to your story, Owen Thomas about how you got in the
[00:34:59.440 --> 00:35:02.960]   battle with Elon over having to refile his S1.
[00:35:02.960 --> 00:35:07.080]   Oh, yeah, that was a saga.
[00:35:07.080 --> 00:35:10.760]   Was that on a Twitter stage or was that somewhere else?
[00:35:10.760 --> 00:35:15.080]   Well, the interesting thing is he's kind of in a similar pickle now in that he has been
[00:35:15.080 --> 00:35:17.760]   borrowing against his Tesla shares.
[00:35:17.760 --> 00:35:21.520]   And there could be a real problem if Tesla shares keep dropping in value.
[00:35:21.520 --> 00:35:24.600]   But the situation was Tesla was trying to go public.
[00:35:24.600 --> 00:35:33.560]   It had a big loan from that I think was backed by the US government, special bonds that it
[00:35:33.560 --> 00:35:34.720]   had issued.
[00:35:34.720 --> 00:35:41.080]   And one of the requirements was that Elon not sell or give away a certain number of his
[00:35:41.080 --> 00:35:42.080]   shares.
[00:35:42.080 --> 00:35:47.920]   Well, he was in the middle of a messy contested divorce and the divorce settlement could potentially
[00:35:47.920 --> 00:35:49.960]   have reached that agreement.
[00:35:49.960 --> 00:35:53.640]   And that would have had cascading bad effects on Tesla.
[00:35:53.640 --> 00:35:56.000]   And he wasn't disclosing it to shareholders.
[00:35:56.000 --> 00:35:57.000]   Does this sound familiar?
[00:35:57.000 --> 00:35:58.000]   Yeah.
[00:35:58.000 --> 00:35:59.520]   Plus not making the needed disclosures.
[00:35:59.520 --> 00:36:00.520]   Yeah.
[00:36:00.520 --> 00:36:06.040]   And I heard of that, he made it all about, oh, this, you know, SCSI journalist is digging
[00:36:06.040 --> 00:36:07.160]   into my divorce.
[00:36:07.160 --> 00:36:12.600]   And I'm like, no one would care about your divorce if it didn't have the risk of directly
[00:36:12.600 --> 00:36:15.360]   affecting your finances.
[00:36:15.360 --> 00:36:18.040]   So he had a write up post.
[00:36:18.040 --> 00:36:22.160]   Given the choice that other stick a fork in my hand and write about my personal life,
[00:36:22.160 --> 00:36:23.400]   says Elon Musk.
[00:36:23.400 --> 00:36:24.400]   Oh, yeah.
[00:36:24.400 --> 00:36:26.360]   Unfortunately, I have to.
[00:36:26.360 --> 00:36:29.320]   Thanks to you, Owen Thomas, you I blame you.
[00:36:29.320 --> 00:36:30.320]   All right.
[00:36:30.320 --> 00:36:34.400]   So it was, it was delightful fact checking that I, I, I wrote it in a piece.
[00:36:34.400 --> 00:36:39.720]   I got out my red pen and crossed through every false claim he made.
[00:36:39.720 --> 00:36:40.720]   Oh, Lord.
[00:36:40.720 --> 00:36:41.720]   Oh, Lord.
[00:36:41.720 --> 00:36:42.960]   So called correct in the record piece.
[00:36:42.960 --> 00:36:43.960]   Yes.
[00:36:43.960 --> 00:36:44.960]   It was entertaining.
[00:36:44.960 --> 00:36:45.960]   Yeah.
[00:36:45.960 --> 00:36:48.640]   Let's take a little break, come back with more great panel.
[00:36:48.640 --> 00:36:51.760]   Tim's, but we aren't, I promise you were done with Twitter.
[00:36:51.760 --> 00:36:58.000]   Tim Stevens editor in chief of the newly named CNET cars, formerly roadshow, same great
[00:36:58.000 --> 00:36:59.800]   content.
[00:36:59.800 --> 00:37:03.560]   We also thank you for being here as always from protocol.
[00:37:03.560 --> 00:37:09.120]   We've got Owen Thomas, new place to be relatively senior editor over there.
[00:37:09.120 --> 00:37:10.920]   And it's great to have you a love protocol.
[00:37:10.920 --> 00:37:16.880]   He'd been doing a great job free, which I love, although you can subscribe and get newsletters
[00:37:16.880 --> 00:37:18.360]   and stuff.
[00:37:18.360 --> 00:37:24.120]   And Corey Doctor, a sci fi author, his latest attack surface doing well.
[00:37:24.120 --> 00:37:26.360]   Yeah, doing very well.
[00:37:26.360 --> 00:37:30.600]   I mean, it's been a while since I've given a much thought I have eight books in production
[00:37:30.600 --> 00:37:31.600]   right now.
[00:37:31.600 --> 00:37:32.600]   Holy cow.
[00:37:32.600 --> 00:37:35.280]   So I'm kind of, I'm, I'm living as science fiction writers should in the future.
[00:37:35.280 --> 00:37:36.280]   Yes.
[00:37:36.280 --> 00:37:38.040]   You're right in your next one, I'm sure.
[00:37:38.040 --> 00:37:43.720]   On authorized bread is the, is the subject of our Stacey's book club in club.
[00:37:43.720 --> 00:37:45.960]   Twitter mentioned this before coming up later.
[00:37:45.960 --> 00:37:48.960]   Oh, it's our, did I miss it?
[00:37:48.960 --> 00:37:49.960]   Oh, shoot.
[00:37:49.960 --> 00:37:50.960]   It happened.
[00:37:50.960 --> 00:37:51.960]   Yeah.
[00:37:51.960 --> 00:37:52.960]   That was exciting.
[00:37:52.960 --> 00:37:53.960]   So, oh, oh, no.
[00:37:53.960 --> 00:37:54.960]   Next one is termination shock.
[00:37:54.960 --> 00:37:55.960]   That's right.
[00:37:55.960 --> 00:37:56.960]   That's also a good book.
[00:37:56.960 --> 00:37:57.960]   I read that book.
[00:37:57.960 --> 00:38:01.640]   I was in Rotterdam seeing the giant dam that he wrote it about.
[00:38:01.640 --> 00:38:02.640]   Oh, wow.
[00:38:02.640 --> 00:38:03.800]   That's kind of cool.
[00:38:03.800 --> 00:38:04.800]   Yeah.
[00:38:04.800 --> 00:38:07.280]   If you're not a club twit member, lots of stuff coming up.
[00:38:07.280 --> 00:38:11.040]   Thursday Scott Wilkinson hasn't asked me anything the following Thursday.
[00:38:11.040 --> 00:38:13.960]   Father Robert Ballisare with a fireside chat.
[00:38:13.960 --> 00:38:18.680]   Stacey's book club is coming up June 16th, but that's just part of what you get in club
[00:38:18.680 --> 00:38:23.080]   twit ad free versions of all of our shows access to the club twit discord where there
[00:38:23.080 --> 00:38:28.880]   is always a good conversation going on about all kinds of things and many, many animated
[00:38:28.880 --> 00:38:29.880]   gifs.
[00:38:29.880 --> 00:38:35.800]   Also, you get the twit plus feed with conversations from before and after the show and also shows
[00:38:35.800 --> 00:38:40.040]   that don't make it into the regular feeds like the untitled Linux show.
[00:38:40.040 --> 00:38:42.320]   Stacey's book club that gives fizz.
[00:38:42.320 --> 00:38:44.800]   We are launching a new show soon.
[00:38:44.800 --> 00:38:47.800]   Stay tuned for that because club members pay.
[00:38:47.800 --> 00:38:51.880]   We are able to launch shows without advertising that really helps us launch new shows.
[00:38:51.880 --> 00:38:53.440]   This week in space came out of the club.
[00:38:53.440 --> 00:38:54.920]   So thank you club members.
[00:38:54.920 --> 00:38:57.520]   Seven bucks a month club twit.
[00:38:57.520 --> 00:39:00.520]   Go to twit.tv/clubtwit.
[00:39:00.520 --> 00:39:02.240]   Thank you in advance.
[00:39:02.240 --> 00:39:05.280]   Our show today brought to you by podium.
[00:39:05.280 --> 00:39:09.920]   I'll tell you what, I think the merchants of Petaluma have discovered podium.
[00:39:09.920 --> 00:39:14.160]   I love it when I go to our local ice cream store and as I'm leaving, they say, would
[00:39:14.160 --> 00:39:16.800]   you like to leave review on Yelp or Google?
[00:39:16.800 --> 00:39:18.960]   And I can click a link and leave review just like that.
[00:39:18.960 --> 00:39:19.960]   I get messages.
[00:39:19.960 --> 00:39:21.160]   Hey, we haven't seen you in a while.
[00:39:21.160 --> 00:39:23.160]   Would you like at this kind of ice cream?
[00:39:23.160 --> 00:39:24.160]   Yes.
[00:39:24.160 --> 00:39:29.960]   It is a great way to stay in touch with your customers because text messaging is much more
[00:39:29.960 --> 00:39:30.960]   effective.
[00:39:30.960 --> 00:39:31.960]   It's what your customers want.
[00:39:31.960 --> 00:39:35.080]   It's kind of a habit we learned during the pandemic.
[00:39:35.080 --> 00:39:37.120]   So much better than playing phone tag.
[00:39:37.120 --> 00:39:40.680]   There aren't enough hours in the day for that.
[00:39:40.680 --> 00:39:43.680]   Podium makes every interaction as easy as sending a text.
[00:39:43.680 --> 00:39:46.760]   So everything that makes your business great can get done faster.
[00:39:46.760 --> 00:39:51.320]   I know is for instance, when we need somebody to do something around the house, the one
[00:39:51.320 --> 00:39:54.160]   that gets the job is the one that texts me back fastest.
[00:39:54.160 --> 00:39:55.720]   I love that.
[00:39:55.720 --> 00:39:57.360]   Podium isn't just a better way to communicate.
[00:39:57.360 --> 00:39:58.640]   It's a better way to do everything.
[00:39:58.640 --> 00:39:59.800]   You can get reviews.
[00:39:59.800 --> 00:40:02.400]   Yes, you can collect payments through podium.
[00:40:02.400 --> 00:40:03.760]   You can market to your customers.
[00:40:03.760 --> 00:40:06.800]   Podium makes it all as easy as pressing send.
[00:40:06.800 --> 00:40:08.040]   You won't just free up more time.
[00:40:08.040 --> 00:40:09.520]   You'll grow your business.
[00:40:09.520 --> 00:40:11.640]   You'll get more done with podium.
[00:40:11.640 --> 00:40:15.560]   You'll close deal with customers before the competition even has a chance to call them
[00:40:15.560 --> 00:40:16.560]   back.
[00:40:16.560 --> 00:40:19.640]   That's the case because it happens to me all the time.
[00:40:19.640 --> 00:40:26.440]   Join more than 100,000 businesses that already use podium to streamline their customer interaction.
[00:40:26.440 --> 00:40:27.800]   Get started for free.
[00:40:27.800 --> 00:40:32.560]   PodiumPODIUMPodium.com/twit.
[00:40:32.560 --> 00:40:36.280]   If you sign up for a paid podium account, you can get a free credit card reader as well.
[00:40:36.280 --> 00:40:37.280]   Restrictions apply.
[00:40:37.280 --> 00:40:40.280]   That's podium.com/twit.
[00:40:40.280 --> 00:40:42.960]   We thank you so much for supporting our show.
[00:40:42.960 --> 00:40:52.040]   Thank you for supporting it by using that address podium.com/twit.
[00:40:52.040 --> 00:40:53.040]   One more Elon.
[00:40:53.040 --> 00:40:54.040]   I said no more Elon.
[00:40:54.040 --> 00:40:55.560]   I should mention one more Elon story.
[00:40:55.560 --> 00:41:03.320]   During all of this last week, Judge rejected Elon's bid to truncate end his settlement
[00:41:03.320 --> 00:41:04.320]   with the SEC.
[00:41:04.320 --> 00:41:07.880]   That's the one where he got in trouble for tweeting that he was taking Tesla private,
[00:41:07.880 --> 00:41:08.880]   funding, arranged.
[00:41:08.880 --> 00:41:11.720]   The SEC said, "No, that's not true."
[00:41:11.720 --> 00:41:12.920]   You can't say that.
[00:41:12.920 --> 00:41:13.920]   Find him $20 million.
[00:41:13.920 --> 00:41:16.160]   Find Tesla $20 million.
[00:41:16.160 --> 00:41:21.720]   And furthermore, said that Elon from now on would have to have lawyers review every tweet
[00:41:21.720 --> 00:41:25.000]   to make sure he wasn't making illegal tweets.
[00:41:25.000 --> 00:41:29.840]   Elon says, "That's a violation of my free speech of federal judge," said, "No, it's
[00:41:29.840 --> 00:41:31.840]   not."
[00:41:31.840 --> 00:41:36.440]   You cannot complain that this violates your First Amendment rights.
[00:41:36.440 --> 00:41:43.880]   No, a little victory I think in this case for the SEC.
[00:41:43.880 --> 00:41:45.440]   Let's see.
[00:41:45.440 --> 00:41:47.880]   The point of a settlement is that it's voluntary.
[00:41:47.880 --> 00:41:48.880]   Yeah, you settled, dude.
[00:41:48.880 --> 00:41:50.680]   The government didn't open him not to do it.
[00:41:50.680 --> 00:41:54.200]   They said, "If you promise us, you will no longer do this.
[00:41:54.200 --> 00:41:56.280]   We won't punish you for this rule you broke."
[00:41:56.280 --> 00:41:58.400]   And he was like, "That sounds like a good deal to me.
[00:41:58.400 --> 00:42:00.440]   That is not a First Amendment violation."
[00:42:00.440 --> 00:42:03.760]   And then four years later, "Oh, I don't want to do that."
[00:42:03.760 --> 00:42:04.760]   Sorry, Elon.
[00:42:04.760 --> 00:42:12.480]   Also, he's not supposed to imply that he didn't do the behavior that he agreed that he did,
[00:42:12.480 --> 00:42:15.920]   which is what he's saying now that he had the funding.
[00:42:15.920 --> 00:42:17.000]   Everything was fine.
[00:42:17.000 --> 00:42:18.480]   He didn't deceive shareholders.
[00:42:18.480 --> 00:42:21.040]   Yeah, he's back to doing it again.
[00:42:21.040 --> 00:42:22.040]   Yeah.
[00:42:22.040 --> 00:42:26.520]   I mean, is anyone paying attention to this pattern?
[00:42:26.520 --> 00:42:31.280]   You know, in the Twitter boardroom, that's my question.
[00:42:31.280 --> 00:42:33.280]   Apparently not.
[00:42:33.280 --> 00:42:34.280]   What do you say?
[00:42:34.280 --> 00:42:35.280]   Apparently not.
[00:42:35.280 --> 00:42:38.360]   Apparently not.
[00:42:38.360 --> 00:42:44.360]   The board ape metaverse is at it again.
[00:42:44.360 --> 00:42:50.200]   A frenzy raises millions and crashes ethereum.
[00:42:50.200 --> 00:42:57.800]   People buying plots of virtual land in a still unreleased metaverse featuring board
[00:42:57.800 --> 00:43:03.760]   apes is from Yuga Labs, the creator of the board apes, Yacht Club collection.
[00:43:03.760 --> 00:43:12.280]   They raised $320 million worth of cryptocurrency selling 55,000 plots of virtual land demand
[00:43:12.280 --> 00:43:18.800]   was so strong that the ethereum blockchain was disrupted.
[00:43:18.800 --> 00:43:21.440]   I don't know.
[00:43:21.440 --> 00:43:22.720]   Maybe there's nothing to say about that.
[00:43:22.720 --> 00:43:24.200]   That story speaks for itself.
[00:43:24.200 --> 00:43:27.760]   Oh, I have so many thoughts.
[00:43:27.760 --> 00:43:29.240]   Thought away, Mr.
[00:43:29.240 --> 00:43:33.560]   If I may paraphrase Beyonce, I don't think you're ready for this crypto.
[00:43:33.560 --> 00:43:35.520]   I don't think you're ready for this crypto.
[00:43:35.520 --> 00:43:40.680]   I mean, it just shows you that crypto payments networks are not ready for prime time.
[00:43:40.680 --> 00:43:45.800]   Like they're not ready for a serious business board ape, which has promoted the heck out
[00:43:45.800 --> 00:43:47.280]   of its NFTs.
[00:43:47.280 --> 00:43:50.120]   You know, they swamped the system.
[00:43:50.120 --> 00:43:52.760]   That's, you know, that's pretty straightforward.
[00:43:52.760 --> 00:43:55.080]   But wouldn't they have thought about that?
[00:43:55.080 --> 00:43:59.400]   Wouldn't they have thought what a successful sale would look like and modeled out how that
[00:43:59.400 --> 00:44:01.400]   would impact gas fees?
[00:44:01.400 --> 00:44:02.880]   Yeah, but they didn't.
[00:44:02.880 --> 00:44:05.200]   Did gas fees go through the roof because of it?
[00:44:05.200 --> 00:44:06.200]   They did.
[00:44:06.200 --> 00:44:11.520]   And in fact, I think they, I think they, if I read, read this correctly, they temporarily
[00:44:11.520 --> 00:44:18.680]   stopped sales on ethereum because they saw the effect it was having transaction costs
[00:44:18.680 --> 00:44:23.480]   just to mint other deed NFTs after the launch.
[00:44:23.480 --> 00:44:28.360]   Now the plot of land itself was worth about $5,800 of ethereum.
[00:44:28.360 --> 00:44:36.880]   The gas fees, the fee to make that transaction $6,000 to ether.
[00:44:36.880 --> 00:44:41.000]   And so obviously that's a disruption if you have to cost you more money to make the transaction
[00:44:41.000 --> 00:44:43.000]   than pay for the transaction.
[00:44:43.000 --> 00:44:47.920]   I mean, this is a, a fractally stupid story.
[00:44:47.920 --> 00:44:50.560]   So Molly, Molly White at web three is going great.
[00:44:50.560 --> 00:44:54.800]   Did a pretty great page.
[00:44:54.800 --> 00:44:58.320]   One of the things that she points out is that they actually did very explicitly consider
[00:44:58.320 --> 00:45:03.520]   their auction design and whether or not it would have an impact on ethereum's ability
[00:45:03.520 --> 00:45:05.720]   to process, process transactions.
[00:45:05.720 --> 00:45:09.520]   And they discarded the Dutch auction plan, which I don't know if it would have been better
[00:45:09.520 --> 00:45:15.000]   or worse for ethereum as a system, but they discarded it because they said it would be
[00:45:15.000 --> 00:45:19.120]   too hard on their network and they explicitly chose this mechanism because they thought
[00:45:19.120 --> 00:45:23.840]   it would be better for the financial processing system underpinning it.
[00:45:23.840 --> 00:45:26.440]   But I mean, all of this is really awful, right?
[00:45:26.440 --> 00:45:32.720]   So they're selling plots of land for a game that doesn't exist and which may never exist.
[00:45:32.720 --> 00:45:38.360]   So it's, it's, this is very selling the Brooklyn Bridge, but their whole pitch, this pitch
[00:45:38.360 --> 00:45:45.400]   that board apes are, you know, are the vanguard of NFTs and NFTs of the vanguard of revitalizing
[00:45:45.400 --> 00:45:52.800]   the art market by decentralizing it is a really like disingenuous pitch because no one who
[00:45:52.800 --> 00:45:55.880]   owns a board ape could tell you who the artist was behind it.
[00:45:55.880 --> 00:45:58.800]   There are no royalties for those artists.
[00:45:58.800 --> 00:46:02.200]   These are works made for hire.
[00:46:02.200 --> 00:46:07.640]   The artists who did make it, the primary artists, the woman of color are struggling to capitalize
[00:46:07.640 --> 00:46:08.640]   on any of this.
[00:46:08.640 --> 00:46:13.320]   They're really just faceless entities behind the curtain there.
[00:46:13.320 --> 00:46:19.360]   And so the, the, all of the promises that are made about this just are, are literal like
[00:46:19.360 --> 00:46:25.600]   errant nonsense that is indefensible and, and inarguably wrong.
[00:46:25.600 --> 00:46:30.680]   And you know, that's, that's, you know, you land on top of that with, we are such financial
[00:46:30.680 --> 00:46:34.120]   geniuses that will come up with an auction designer, a sale design that won't crash
[00:46:34.120 --> 00:46:37.240]   Ethereum that promptly crashes Ethereum.
[00:46:37.240 --> 00:46:41.640]   And you know, and then on top of that, all this business about them getting doxed where
[00:46:41.640 --> 00:46:45.960]   like a journalist went and looked up their financial filings in which these people who
[00:46:45.960 --> 00:46:51.920]   have a multi billion dollar empire who are extracting hundreds of thousands, if not millions
[00:46:51.920 --> 00:46:57.480]   of dollars from investors and customers without telling you who they are, but who had told
[00:46:57.480 --> 00:47:01.160]   a regulator who they were as a matter of public record suddenly claimed to have been
[00:47:01.160 --> 00:47:06.960]   docked because someone looked at the public record and then threatened and harassed the
[00:47:06.960 --> 00:47:11.120]   journalist who did it. Like, you know, that doesn't permanently disqualify you from running
[00:47:11.120 --> 00:47:15.440]   even 11 aid stand, let alone a media empire. I don't know what does.
[00:47:15.440 --> 00:47:19.480]   Oh, there's so many things to be upset about.
[00:47:19.480 --> 00:47:23.960]   No, it's not like, it's not going to be great.
[00:47:23.960 --> 00:47:25.160]   Everything sucks.
[00:47:25.160 --> 00:47:31.720]   Uh, yeah, it's a anyway, I mean, if, if by now, if you're not, if alarm bells aren't going
[00:47:31.720 --> 00:47:38.440]   off, whenever you look at an NFT or a crypto transaction, yeah, Web three is going great.
[00:47:38.440 --> 00:47:40.320]   It's a great site to look at.
[00:47:40.320 --> 00:47:45.800]   I'll just give you some idea of what's, uh, what's going on out there.
[00:47:45.800 --> 00:47:49.840]   Web three is going great.com. And my favorite feature, we mentioned this on Wednesday in
[00:47:49.840 --> 00:47:55.960]   Twig is over here in the lower right, this little tracker, which is the grift counter
[00:47:55.960 --> 00:48:04.600]   trademark. And as you scroll through the various articles, the grift tracker adds up the amount
[00:48:04.600 --> 00:48:09.440]   of grift involved. So by the time you get to the end of the page, which you probably
[00:48:09.440 --> 00:48:15.520]   will never get to. It's a significant, uh, significant sum of money going on. This is,
[00:48:15.520 --> 00:48:21.480]   if you read this, I mean, I just, I feel like this kind of says all you need to know about
[00:48:21.480 --> 00:48:30.760]   all of this. Am I wrong? Do you think Owen to tell people it's just basically a speculative
[00:48:30.760 --> 00:48:37.200]   bubble of pyramid scam and stay away? Do you have NFTs? Are you invested heavily in crypto?
[00:48:37.200 --> 00:48:43.800]   I, I am a, uh, what they call a no-pointer and it's interesting. What is that? I want
[00:48:43.800 --> 00:48:49.400]   to be that you have no crypto holding. Oh, I do have action. I have a wallet. I can't
[00:48:49.400 --> 00:48:58.240]   get into it. Does that count? That's a, that's a dough pointer. There's a, uh, but yeah,
[00:48:58.240 --> 00:49:02.520]   there, there's some people in the crypto world who will not talk to journalists who don't
[00:49:02.520 --> 00:49:06.840]   hold crypto, which is bonkers when you think about like, you know, conventional, you got
[00:49:06.840 --> 00:49:14.400]   to have a dog in the hunt. Otherwise your coverage will be way too, uh, which is like
[00:49:14.400 --> 00:49:20.200]   not the expectation that like a Tesla reporter would hold shares of, of Tesla, for example.
[00:49:20.200 --> 00:49:28.760]   Right. It's the exact opposite. Right. Yeah. Um, but I, you know, I, I think that it is,
[00:49:28.760 --> 00:49:35.480]   I think it's fascinating that there are such utopian hopes around crypto and yet the actual
[00:49:35.480 --> 00:49:41.120]   realities when you dig into how many transactions they do a second, uh, what the transaction
[00:49:41.120 --> 00:49:47.920]   costs are that no one, no one's actually designing these things to like work in the real world.
[00:49:47.920 --> 00:49:52.880]   Right. Like if you're not as good as Visa or Mastercard, why are you claiming to be a
[00:49:52.880 --> 00:49:59.120]   payment snap? Right. Why are you claiming to upend, you know, the world of transactions?
[00:49:59.120 --> 00:50:08.720]   It, it boggles my mind. Are you a, a no-coiner, a Tim or a, or a get me, I'm a some-coiner,
[00:50:08.720 --> 00:50:15.040]   a few, um, mostly as an experiment, just more or less to kind of, um, teach myself a little
[00:50:15.040 --> 00:50:19.200]   bit more about the network. So I do have some, but it's a, you know, a fraction of a 10th
[00:50:19.200 --> 00:50:23.320]   of a season by my, my, my holding. Yeah. So that's about it. Yeah. I don't really have
[00:50:23.320 --> 00:50:26.920]   any, any large dogs in the game. I guess I have a few very small puppies in the game,
[00:50:26.920 --> 00:50:34.160]   um, but ultimately I, I tend to share your thoughts on NFTs in general. I, I think that
[00:50:34.160 --> 00:50:38.120]   there are some legitimate applications for the technology and I'm very interested to see
[00:50:38.120 --> 00:50:43.560]   where it goes, but 99% of what I see is, is basically grift and, and, uh, and pyramid
[00:50:43.560 --> 00:50:48.400]   schemes and finding that 1% that nugget, I think is, is a very difficult thing. I'm
[00:50:48.400 --> 00:50:53.720]   curious to see what we hear from auto manufacturers all the time we're talking about doing things
[00:50:53.720 --> 00:50:58.600]   like, uh, parts verification on, um, on blockchain and things like that, which I think are
[00:50:58.600 --> 00:51:02.480]   high. That makes sense. Some interesting applications where you can really validate the supply chain
[00:51:02.480 --> 00:51:07.640]   of a given product, um, both in terms of, you know, for a classic car, making sure that
[00:51:07.640 --> 00:51:12.040]   everything is valid, but also in terms of making sure that your battery materials are
[00:51:12.040 --> 00:51:16.960]   sourced in an ethical way or as ethically as they can be anyway. Um, so I think there's
[00:51:16.960 --> 00:51:20.080]   definitely some applications like that on the business side of things, but from a consumer
[00:51:20.080 --> 00:51:24.040]   standpoint, from a, you know, sticking it to the man and coming up with a new way of,
[00:51:24.040 --> 00:51:30.080]   of retiring because 401ks are boring. Um, that, that just really doesn't, um, I don't
[00:51:30.080 --> 00:51:34.680]   see a lot of ability there myself. There was a proposal some time ago to put, uh,
[00:51:34.680 --> 00:51:41.960]   VIN numbers on the blockchain and then all auto repair shops would add entries as they
[00:51:41.960 --> 00:51:46.840]   did repairs to the blockchain. That seems to me a sensible use of the blockchain. Uh,
[00:51:46.840 --> 00:51:49.320]   it definitely does, but it also seems like something that you could pretty easily do
[00:51:49.320 --> 00:51:52.440]   with a good old database. If you wanted to, you just needed to make it public. Um, you
[00:51:52.440 --> 00:51:57.720]   know, I don't know that it needs to exist necessarily in the blockchain. Um, but, but
[00:51:57.720 --> 00:52:01.480]   for sure, there are technologies like that, which, which could make that fundamental technology
[00:52:01.480 --> 00:52:06.040]   behind crypto worthwhile. Uh, the way three stuff, there's definitely applications there.
[00:52:06.040 --> 00:52:12.200]   But yeah, board apes, not one of them. Corey, you, you seem like you think blockchain is not
[00:52:12.200 --> 00:52:19.080]   a panacea. Tim just nailed it, which is, uh, what, what, what does the blockchain add to this?
[00:52:19.080 --> 00:52:23.560]   So I wrote an article about this, uh, called, I think something like, um, the inevitability
[00:52:23.560 --> 00:52:30.520]   of trusted third parties. So the, the thing is that VINs and repairs don't magically appear
[00:52:30.520 --> 00:52:34.520]   in the blockchain. As you said, if you were going to have a registry of repairs in the
[00:52:34.520 --> 00:52:40.040]   blockchain, you would need people to enter those repairs correctly into the blockchain. Otherwise,
[00:52:40.040 --> 00:52:44.040]   it doesn't work. So you're already trusting people to do that. Anyway,
[00:52:44.040 --> 00:52:50.040]   you want to do it. Do we think that the problem with the existing databases of VINs and service,
[00:52:50.040 --> 00:52:55.560]   uh, that has gone on is that people change the database after it's posted? Because there are
[00:52:55.560 --> 00:52:59.400]   those databases. Last time I bought a used car, I looked up the VIN of the cars that, uh,
[00:52:59.400 --> 00:53:03.320]   I was looking up on them. Or do we think that the problem is that sometimes people just don't
[00:53:03.320 --> 00:53:08.040]   enter the data? And if they just don't enter the data, then having an immutable thing that they
[00:53:08.040 --> 00:53:13.880]   just don't enter the data into doesn't make them enter the data more. And you know, more importantly,
[00:53:13.880 --> 00:53:17.880]   like, if, if you were going to trust, you know, when we talk about these supply chain issues,
[00:53:17.880 --> 00:53:23.240]   right? Like, um, I, I just before lockdown, I was in Brussels for an event. And I met someone who's,
[00:53:23.240 --> 00:53:28.440]   you know, very sincere about this blockchain for good project that she'd worked on or run with
[00:53:28.440 --> 00:53:35.080]   EU money, uh, where they were trying to, uh, guarantee fair trade produce. And so they were, you know,
[00:53:35.080 --> 00:53:41.160]   they would track the, the, the entire life cycle of a food commodity, like a bag of flour or potato
[00:53:41.160 --> 00:53:46.200]   or whatever. Uh, and they put it in the blockchain. And I said, so, um, I find a potato in the grocery
[00:53:46.200 --> 00:53:50.760]   store. How do I know it's the one that the blockchain is referencing and not a different potato? So
[00:53:50.760 --> 00:53:55.720]   well, you just have to trust us. I'm like, well, that's a fact, right? Like maybe you are trustworthy.
[00:53:55.720 --> 00:54:02.200]   In which case, why are we roasting the planet? You could tell me what I need to trust you on.
[00:54:02.200 --> 00:54:06.520]   And even if we're not roasting the planet, why are we, you know, even if you believe the proof of
[00:54:06.520 --> 00:54:11.240]   stake is, is not six months away as it has been for the last five years, but actually like imminent.
[00:54:11.240 --> 00:54:17.400]   Um, then, you know, why are we entrusting it to a system that can only process 15 transactions a
[00:54:17.400 --> 00:54:24.760]   second worldwide for all users, like the theory and virtual computer, rather than, uh, just having
[00:54:24.760 --> 00:54:29.800]   it as Tim said in a database, right? If you trust someone to run the database, or if you trust
[00:54:29.800 --> 00:54:34.840]   someone to correctly enter things into the database, because I don't know. That's how we've always done
[00:54:34.840 --> 00:54:40.520]   it, Corey, and we want to do something different, new and futuristic. So look, appendily ledgers
[00:54:40.520 --> 00:54:45.320]   are super cool. And I helped write up one, uh, that you use every day without knowing it,
[00:54:45.320 --> 00:54:50.200]   speaking of supply chain. Every time your browser gets a certificate from the world,
[00:54:50.760 --> 00:54:55.480]   uh, it, uh, signs, it gets a sign certificate that's signed by the certificate authority that
[00:54:55.480 --> 00:55:00.920]   issued it. It makes a hash, signs it again, and uploads it to one of several, one or more of
[00:55:00.920 --> 00:55:05.160]   several certificate transparency, uh, servers around the world. These are, these are not a
[00:55:05.160 --> 00:55:09.800]   blockchain. They're just Merkle trees. They're all over the place and they're appendily logs.
[00:55:09.800 --> 00:55:15.400]   And anyone who wants to can subscribe to them and see whether or not a certificate has been
[00:55:15.400 --> 00:55:22.040]   issued for their domain that they didn't authorize. And, um, this is a really powerful tool.
[00:55:22.040 --> 00:55:26.120]   I wrote it up for nature with the guy who led the project at Google, Ben Laurie is a
[00:55:26.120 --> 00:55:30.840]   cartographer, works on open SSL, a bunch of other projects. And, and it's really cool. And I do
[00:55:30.840 --> 00:55:36.760]   think public appendily ledgers have an application. But I think that the, that if we are going to
[00:55:36.760 --> 00:55:41.320]   talk about trust models, right, if we're going to say this is trustless, then it has to be trustless.
[00:55:41.320 --> 00:55:45.240]   And if you're going to introduce trust, if you're going to say there's a human oracle
[00:55:45.240 --> 00:55:49.320]   who could confound the entire transaction, who could break the entire transaction,
[00:55:49.320 --> 00:55:54.840]   you have to show why that person shouldn't just replace the cumbersome computing you're doing to
[00:55:54.840 --> 00:55:59.560]   get around having any human oracles in the loop. Otherwise it just makes sense to have that person
[00:55:59.560 --> 00:56:05.880]   run the server too. You haven't gained anything new by it. As you, uh, as you point out little
[00:56:05.880 --> 00:56:11.800]   mathematics, don't be afraid in your piece. If problem plus blockchain equals problem minus
[00:56:11.800 --> 00:56:18.520]   blockchain, then blockchain equals zero. Yeah, it's pretty, pretty, it's simple. It's math.
[00:56:18.520 --> 00:56:26.600]   Yeah. The blockchain hasn't added anything to the problem. Not in those cases anyways. I think
[00:56:26.600 --> 00:56:31.400]   there may be someone which it does. But I, this, this concept of append only ledger
[00:56:32.360 --> 00:56:36.520]   means stuff can't be deleted from the ledger. You can only add to it. Is that the? Yeah.
[00:56:36.520 --> 00:56:41.800]   Yeah. So you can, you can add a later record, right? You could say, well, you know, think about
[00:56:41.800 --> 00:56:45.400]   the deed for your. Twitter is an append only ledger without an edit button.
[00:56:45.400 --> 00:56:50.360]   Well, no, because you can delete. You could, yeah. No, think about the deed for your house,
[00:56:50.360 --> 00:56:55.160]   which in theory, there's actually like a piece of paper down at the, whatever the Petaluma property
[00:56:55.160 --> 00:57:00.360]   registry. Right. And it's like literally a sheet of paper with all the owners who own your house.
[00:57:00.360 --> 00:57:05.240]   Yeah. It says so at the bottom, right? So you can go back and see it. So you can add to the ledger.
[00:57:05.240 --> 00:57:08.760]   So it doesn't mean that once you sell your house, you can never sell it again, because you can
[00:57:08.760 --> 00:57:13.960]   append a new record to the bottom of the ledger. Right. But, you know, you, in this case, you're
[00:57:13.960 --> 00:57:19.880]   trusting the city to, to hold the deed and not go in and erase things or move things around and so on.
[00:57:19.880 --> 00:57:23.560]   And you know, basically we build institutions to make that trust work. And they're not always
[00:57:23.560 --> 00:57:28.280]   great. And there have been ways in which they failed us pretty grotesquely. But the way that we
[00:57:28.280 --> 00:57:33.400]   make those institutions work best is by taking them out of the market. Right. The thing that
[00:57:33.400 --> 00:57:38.680]   actually makes those things work best is when people feel unethical or a normative reason to do
[00:57:38.680 --> 00:57:43.720]   it. You know, the thing that keeps auditors honest is not the fear that they'll be discovered for
[00:57:43.720 --> 00:57:49.080]   colluding. It's their code of ethics. And, you know, when we, when we turn everything into a
[00:57:49.080 --> 00:57:54.680]   financial transaction, where we just say caveat, Mtor, if you can make $1 being the auditor and
[00:57:54.680 --> 00:58:01.240]   $1.01 cheating as the auditor, then you have like a moral duty to cheat. Instead of saying you have,
[00:58:01.240 --> 00:58:05.960]   your duty is first not to money, but to your profession, then that's the way we keep our
[00:58:05.960 --> 00:58:10.200]   institutions working. You know, we can laugh at politicians who say, I'm a public servant first,
[00:58:10.200 --> 00:58:14.680]   and, and, you know, here to earn a living second. But unless that's the way they feel, or unless
[00:58:14.680 --> 00:58:19.320]   that's the way a lot of them feel, the system does break down. And for me, the problem with,
[00:58:19.320 --> 00:58:23.720]   with the web three stuff is so much of it is grounded in the idea that we solve all of our problems
[00:58:23.720 --> 00:58:29.640]   by making the monetary, by just assigning values and selling stuff. And, and it explicitly getting
[00:58:29.640 --> 00:58:34.920]   rid of any normative claims about what people should do, and just saying people should do whatever
[00:58:34.920 --> 00:58:40.840]   the smart contract allows. I think we got a system. I think that's, that's where we are right now,
[00:58:40.840 --> 00:58:46.360]   an end stage capitalism, if you stop worrying about. Yeah, the triumph of Hayek. Yeah.
[00:58:48.600 --> 00:58:55.720]   What do you think about Owen Thomas Fidelity's plan to put Bitcoin in your 401k?
[00:58:55.720 --> 00:59:03.000]   I think that they are in hot water. Actually, we, we, we kind of called it in our piece on
[00:59:03.000 --> 00:59:09.080]   protocol.com because we said, Hey, you know, Fidelity is rolling out this plan. They're catering to
[00:59:09.080 --> 00:59:15.240]   micro strategy, which is a software company that is places really kind of frankly bizarre side
[00:59:15.240 --> 00:59:21.560]   bet on Bitcoin. It's not at all strategic to their software business, but the, the CEO, Michael Saylor,
[00:59:21.560 --> 00:59:28.520]   is, you know, is a Bitcoin fanatic anyway. So Fidelity is making Bitcoin an option for
[00:59:28.520 --> 00:59:33.320]   micro strategy employees in their 401k. The problem is the, the Department of Labor,
[00:59:33.320 --> 00:59:41.320]   which oversees 401k plans, said in March that they were going to investigate any fiduciaries like Fidelity
[00:59:43.080 --> 00:59:49.000]   who put cryptocurrencies in a retirement plan. So Fidelity was setting itself up for an
[00:59:49.000 --> 00:59:54.440]   investigation here and they, they knew full well, they had to have seen that warning that the
[00:59:54.440 --> 00:59:58.840]   Department of Labor, they were applauded by, of course, a lot of a lot of, what's the opposite of a
[00:59:58.840 --> 01:00:06.840]   no coin, a bro coin, they were a coin, go coin, they were applauded by the go coins for making
[01:00:06.840 --> 01:00:12.200]   this available to people. Now the employer would have to do it, right? So I would have to, as an
[01:00:12.200 --> 01:00:16.120]   employer say, Hey, good news, John, you can add Bitcoin to your retirement.
[01:00:16.120 --> 01:00:20.600]   That doesn't seem like on the face of it. A very good idea.
[01:00:20.600 --> 01:00:27.320]   And that's the difference. There have been Bitcoin like Roth iris and those are self directed
[01:00:27.320 --> 01:00:32.840]   plans. Yeah, that might not be so bad. Yeah. I mean, you can put, you can put real estate
[01:00:32.840 --> 01:00:38.600]   in a Roth IRA. That's how Peter Teal has, has sheltered his fortune, hasn't he? Yeah, you can,
[01:00:38.600 --> 01:00:44.360]   you can put private equity. If you have something that you think has a chance to really appreciate,
[01:00:44.360 --> 01:00:51.560]   you put it in after tax early in a Roth IRA and then you don't pay any taxes on the appreciation.
[01:00:51.560 --> 01:00:57.960]   But that's the difference under the law. A 401k plan, it's provided to you by, by your employer.
[01:00:57.960 --> 01:01:03.000]   Right. There's an implicit kind of recommendation there of the plan. And so it's got to be more
[01:01:03.000 --> 01:01:08.280]   conservative in what it offers. Yeah. All right. I want to take a little break. I've got to
[01:01:08.280 --> 01:01:16.280]   consider Corey's hips and all of this. I love it that he has a gold plated or solid gold hip
[01:01:16.280 --> 01:01:23.880]   joint brass, so it's nice and heavy. It's going on top of this cane here. You could beam somebody
[01:01:23.880 --> 01:01:28.600]   with that machine us to put in a screw. Yeah, that is fantastic. Oh man, I love that.
[01:01:29.240 --> 01:01:32.280]   Everybody gets hip surgery. You should save the top of the femur,
[01:01:32.280 --> 01:01:37.400]   get it cast and brass and make a cane out of it. You could get it. I mean, I don't know why you
[01:01:37.400 --> 01:01:45.240]   wouldn't. Right. It's obvious. John, I would. Would it, would it offend your views of intellectual
[01:01:45.240 --> 01:01:50.200]   property to patent that idea? Because well, that's the beauty of it. Corey has made it creative
[01:01:50.200 --> 01:01:56.680]   common zero. He is. Yeah, you can. There's a, I got some folks here in Burbank who run a prop house
[01:01:56.680 --> 01:02:02.520]   that have a super high res laser scanner to do a 1200 DPI laser scan and put it on the internet
[01:02:02.520 --> 01:02:08.120]   archive CC zero. So you can like, at that res, you can blow it up to like room size if you want.
[01:02:08.120 --> 01:02:16.680]   Do you like it? I'm on, I'm on an archive.org. What should I search for? Corey's hips. Dr. O
[01:02:16.680 --> 01:02:24.520]   femur. We're going to send that to my 3D printer right now. I love that band.
[01:02:25.720 --> 01:02:32.440]   Wait a minute. I'm sorry. My watch is about to call 911 because it thinks I fell. I don't, I'm
[01:02:32.440 --> 01:02:36.360]   sorry. I don't mean interrupt the show, but the watch was about to call 911. That's bizarre.
[01:02:36.360 --> 01:02:43.960]   All right. Here it is. Dr. O femur. Now this is gold genomes online database. I think I got the
[01:02:43.960 --> 01:02:51.800]   wrong, the wrong thing. I'll have to find it later. It's on, it's on the way. It's on the archive.org.
[01:02:51.800 --> 01:02:58.440]   Let me see. Dr. O femur internet archive. Oh, maybe just use Google instead. What a thought.
[01:02:58.440 --> 01:03:03.640]   No, that doesn't do it. Hang on. Yeah, I see it. I'll drop it in the chat. Oh, yeah. It'll take you
[01:03:03.640 --> 01:03:08.040]   to a Twitter, Twitter link, which will then take you to the internet archive. It turned out.
[01:03:08.040 --> 01:03:11.960]   Okay. I'm going to see this. This is awesome. Okay.
[01:03:11.960 --> 01:03:17.880]   It takes a while to load because they have a JavaScript 3D model thing. So it's going to,
[01:03:17.880 --> 01:03:20.840]   unless you've got a very fast computer, it might take a while to load.
[01:03:20.840 --> 01:03:24.120]   Oh, well, I don't. But if you look in that tweet, you can see some stills of it as well.
[01:03:24.120 --> 01:03:26.520]   Okay. That's cool. There it is on Twitter. Look at that.
[01:03:26.520 --> 01:03:31.320]   That weird thing that looks like a volcano coming off, that's the one tendon that they have to
[01:03:31.320 --> 01:03:38.120]   sever. Oh, yik. To, to, to just sand that down or? I did. I got it a little sanded down,
[01:03:38.120 --> 01:03:42.600]   not all the way down. I wanted to preserve it as like a memento mori there, but I got most of it
[01:03:42.600 --> 01:03:49.560]   taken off. Oh my God. I love it. Thank you, Corey, for being here. It's always a pleasure to have
[01:03:49.560 --> 01:03:56.280]   you on Tim Stevens. It's great to have you. And of course, Owen Thomas, our show today brought to you
[01:03:56.280 --> 01:04:03.480]   by our crowd all around the world tech companies are innovating and driving returns for investors.
[01:04:03.480 --> 01:04:09.240]   But you might wonder, you know, I've got my savings. I've been doing well. Well, how do I get involved?
[01:04:09.240 --> 01:04:14.280]   How do I get some deal flow? Because I'd like to get in on the, on the, on the, on the, on this
[01:04:14.280 --> 01:04:21.640]   exciting investment train. Well, our crowd is a venture capital firm, analyzes companies all across
[01:04:21.640 --> 01:04:26.040]   the global private market, selecting those with the greatest growth potential, but then they do
[01:04:26.040 --> 01:04:31.320]   something different. So they just put their money in and sitting back to enjoy. They allow you to
[01:04:31.320 --> 01:04:36.920]   participate as well. Now, this is, this is how you get into something that's not public yet,
[01:04:36.920 --> 01:04:42.680]   hasn't had an exit. This is really early, but that's the idea. Our crowd identifies innovators
[01:04:42.680 --> 01:04:48.120]   so you can invest when the growth potential is the greatest from personalized medicine to robotics
[01:04:48.120 --> 01:04:53.800]   to cybersecurity. By the way, that is a hot field right now. Companies spend 150 billion
[01:04:53.800 --> 01:05:00.040]   annually on cybersecurity. I'm not surprised. Our crowd is the fastest growing venture capital
[01:05:00.040 --> 01:05:04.680]   investment community. Now, to do this, you have to be an accredited investor. This is not for
[01:05:04.680 --> 01:05:12.440]   everyone. So go to our crowd.com/twit. Once you enter the country that you're in, you could find out
[01:05:12.440 --> 01:05:18.200]   what the rules are for accredited investors. And then you can participate. I should also mention
[01:05:18.200 --> 01:05:23.880]   the minimum investment is $10,000. So again, this is not for everybody, but if you've got your money
[01:05:23.880 --> 01:05:28.520]   for your retirement put away and you've got all you're doing all the right things and you want to
[01:05:28.520 --> 01:05:34.360]   have a little kind of fun finding out, participating. This is a great thing to do. By the way, it's
[01:05:34.360 --> 01:05:38.840]   free to join our crowd. So you can get the reports, you can get the information and then decide
[01:05:38.840 --> 01:05:44.200]   on your own. Our crowd's accredited investors have already invested a billion dollars more
[01:05:44.200 --> 01:05:50.040]   in growing tech companies. 21 of their portfolio companies are unicorns already. Many of our
[01:05:50.040 --> 01:05:55.880]   crowd's members have benefited from over 50 IPOs or sale exits of portfolio companies.
[01:05:55.880 --> 01:06:01.000]   You can get an a single company deal for as little as $10,000. Our crowd also has funds. The minimum
[01:06:01.000 --> 01:06:06.840]   investment there is $50,000. And again, investment terms will vary depending on where you are
[01:06:06.840 --> 01:06:12.200]   investing. So go to our crowd.com/twit and enter your country. As an example, though,
[01:06:12.200 --> 01:06:15.720]   of some of the deals and you'll see these all once you join, you can see all sorts of deals.
[01:06:15.720 --> 01:06:21.800]   You can invest in Satero. This is kind of cool. They've invented a patented approach to data
[01:06:21.800 --> 01:06:27.160]   protection that eliminates the gaps of traditional methods securing any data asset, whether it's
[01:06:27.160 --> 01:06:32.520]   on-prem or in the cloud. They already have as a customer one of the world's largest pharmaceutical
[01:06:32.520 --> 01:06:42.200]   companies. So, how do you get in? Our crowd.com/twit. Explore Satero's potential. Join our crowd for
[01:06:42.200 --> 01:06:50.680]   free at OURCROW.com/twit. Join the fastest growing venture capital investment community,
[01:06:50.680 --> 01:07:07.240]   our crowd.com/twit. I approve everybody we do ads for. I like this company because for years,
[01:07:07.240 --> 01:07:12.440]   I've sat on the sidelines and watched people like Kevin Rose get all this Jason Callikines,
[01:07:12.440 --> 01:07:15.960]   get all this deal flow investor early on Twitter and Facebook. And I'm thinking,
[01:07:15.960 --> 01:07:18.920]   here I am. I'm sitting here. Now, I can't do it because I'm covering the companies.
[01:07:18.920 --> 01:07:23.720]   I'm with Owen here. I'm not going to invest anything that we're covering.
[01:07:23.720 --> 01:07:28.920]   But I thought, this is a shame that only certain people get access to all this information. I think
[01:07:28.920 --> 01:07:33.400]   this is democratizing. I like it that our crowd is doing this. So, we're really happy to have
[01:07:33.400 --> 01:07:40.840]   them on the show. RCROW.com/twit. Corey's femur is still loading. You're right. My computer is not
[01:07:40.840 --> 01:07:47.320]   fast enough. Well, it's a 1200 DPI model. We're like a low-res. It would load right away.
[01:07:47.320 --> 01:07:54.120]   Dr. Rho femur, public domain, mark 1.0. Hip replacement anatomy arthritis,
[01:07:54.120 --> 01:08:01.000]   hip capsule and femur. Nice. Yeah, there's a Kaiser. They did it. So, you could get that's good to
[01:08:01.000 --> 01:08:06.120]   know. They do the coming in from the front thing there. Yeah, they do the anterior approach.
[01:08:06.760 --> 01:08:12.440]   And they were really, so it was funny because they didn't know how to deal with it. My surgeon
[01:08:12.440 --> 01:08:15.880]   and his team didn't know how to deal with it. But apparently, it's pretty common because
[01:08:15.880 --> 01:08:21.640]   what actually happens is you just tell them and the pathologist saves it instead of putting it
[01:08:21.640 --> 01:08:26.200]   in the incinerator after they look at it. Nice. And my folks who came down to help with the
[01:08:26.200 --> 01:08:29.880]   post-op recovery just drove down to Kaiser the next day and went to the pathologist who gave
[01:08:29.880 --> 01:08:34.760]   it to the menotup or where, and he said, "Oh, we do this all day long." Oh. So, I guess it's just the
[01:08:34.760 --> 01:08:38.920]   orthopedics department's not used to it. I think a lot of women save their placentas, for example.
[01:08:38.920 --> 01:08:44.520]   Sure. I know that. Yeah. So, yeah, it was, I mean, why not, right? Well, I wish I had the foresight
[01:08:44.520 --> 01:08:47.960]   to save the other one. It's about so much freezer space you have, I guess. Do they,
[01:08:47.960 --> 01:08:55.000]   this is indelicate. Do they clean it first? No, well, it gets wiped in back team before you,
[01:08:55.000 --> 01:09:02.200]   before they operate on it, right? It gets like, well, it's in situ. They clean it there. But did you
[01:09:02.200 --> 01:09:09.160]   have to then put it in a solution to, is it is? No, it doesn't have any wet stuff on it. It has,
[01:09:09.160 --> 01:09:14.920]   it does have marrow inside of it. Oh, yeah. Sure. What I'm going to do now,
[01:09:14.920 --> 01:09:19.880]   the real last though, I wouldn't hesitate. Yeah. Well, now it's come back from the caster. I'm
[01:09:19.880 --> 01:09:24.840]   going to soak it in a peroxide solution and own the solution. Then I'm debating, you can dip it in
[01:09:24.840 --> 01:09:29.400]   mop and glow to preserve it. That's a pretty common way of doing it. I found the website for
[01:09:30.440 --> 01:09:37.480]   mopping petal really. So I'm thinking about doing that. What I really would like to do is preserve it
[01:09:37.480 --> 01:09:43.240]   in like a methanol or ethanol rather on the back of our bar. We built this very elaborate backyard
[01:09:43.240 --> 01:09:47.960]   of the tiki bar. Yeah. Having it float there. Get one of those Frankenstein style jars that
[01:09:47.960 --> 01:09:53.480]   you can just have a yeah. Yeah. There's a thing called the sour toe. There's a bar in Dawson City
[01:09:53.480 --> 01:09:58.360]   in Yukon, in the Yukon where it's funny. I'm so trained not to say the Ukraine that now I don't
[01:09:58.360 --> 01:10:06.600]   say the Yukon. In the Yukon, there's this bar that is a prospector's bar and then someone gave them
[01:10:06.600 --> 01:10:11.720]   his amputated frostbitten toe and they kept it on a jar of alcohol in the back of the bar. Sure.
[01:10:11.720 --> 01:10:16.120]   And if you were drunk enough, you could order a sour toe and they would put the toe in a shot
[01:10:16.120 --> 01:10:21.720]   and you do the shot and inevitably someone drank the toe. And then one of their regulars
[01:10:21.720 --> 01:10:27.160]   willed them his toe when he died. So it's back on the menu. We're thinking you might offer the sour
[01:10:27.160 --> 01:10:34.200]   femur and like a lowball glass in the in our bar. Well, I don't I don't think that was in the John
[01:10:34.200 --> 01:10:41.480]   McGrew poem that I heard to the sour toe. Sour toe first. But maybe they should add that. You were
[01:10:41.480 --> 01:10:46.600]   talking about how there's a public record at the county seat of everybody who's ever owned your
[01:10:46.600 --> 01:10:52.200]   house. There's a lot of public records and there are companies out there who go around, send people
[01:10:52.200 --> 01:10:56.840]   to the county seats, record them and put them online so that a lot of people are quite shocked
[01:10:56.840 --> 01:11:01.240]   to learn a few search for information about yourself. You could find the home you own.
[01:11:01.240 --> 01:11:07.000]   You could find a lot of information. This is kind of a historic and historic problem. You can go to
[01:11:07.000 --> 01:11:11.720]   companies like Spokio and say, take me down. But there's so many of them. I you could easily miss
[01:11:11.720 --> 01:11:18.360]   a few. It's easy in other words to docs someone or docs yourself. Google has announced that they
[01:11:18.360 --> 01:11:25.000]   are going to allow people to remove at least the Google search result for personally identifiable
[01:11:25.560 --> 01:11:34.200]   information from Google search. I think this is a good move. There is a lot of different kinds
[01:11:34.200 --> 01:11:39.800]   of information. Obviously governmental ID numbers, bank account numbers, credit card numbers,
[01:11:39.800 --> 01:11:46.520]   pictures of your handwritten signature ID docs like your driver's license or passport,
[01:11:46.520 --> 01:11:51.880]   highly personal restricted and official records like medical records, personal contact info,
[01:11:51.880 --> 01:11:57.720]   including your address, phone number and email address or confidential login numbers. This is
[01:11:57.720 --> 01:12:06.520]   from a Google blog post on Wednesday. If you want to know more, Google has always had a set of
[01:12:06.520 --> 01:12:13.880]   policies to do this, but it sounds like this is a little bit easier to do. Thoughts? Anybody?
[01:12:13.880 --> 01:12:18.600]   I think it's Tim Stevens you've ever done a search for yourself on Spokio and been
[01:12:18.600 --> 01:12:24.120]   horrified? I have not actually. But yeah, I definitely think this is a positive move. It'll make it
[01:12:24.120 --> 01:12:32.440]   easier for people to take a first step to keeping the basics of your information being easily found,
[01:12:32.440 --> 01:12:36.360]   I guess. To me, I think this is kind of like putting a cheap padlock on something that you want
[01:12:36.360 --> 01:12:40.760]   to keep secure. It'll keep the honest people from getting in there, but anybody who wants in,
[01:12:40.760 --> 01:12:45.400]   they're going to find a way they'll cut the padlock off. So that information will still be out there
[01:12:45.400 --> 01:12:50.280]   obviously, and will be not that hard to find for people who know how to find it. But it will at
[01:12:50.280 --> 01:12:55.080]   least keep people from, we're only going to take the most basic steps in trying to find
[01:12:55.080 --> 01:12:57.640]   that information. It'll keep them from finding it easily. So I think that's a good thing.
[01:12:57.640 --> 01:13:01.800]   Yeah, why put it in the Google search results? As Google points out, it doesn't take it off the
[01:13:01.800 --> 01:13:07.800]   net. You still have to go and you should go to the site if you can to get it removed.
[01:13:09.320 --> 01:13:16.280]   Anyway, I have a horror story about this. Do you? From when we moved to back to California from
[01:13:16.280 --> 01:13:22.760]   the UK, we had a half container and the customs broker requires your passport. So I sent them a
[01:13:22.760 --> 01:13:27.400]   copy of my passport and they put that with the way bill on the outside of the container.
[01:13:27.400 --> 01:13:33.880]   And there are people who do market research at the port of Los Angeles. Oh, no, right down,
[01:13:34.600 --> 01:13:38.520]   transcribe all the all the way bills. They just stand there writing it down?
[01:13:38.520 --> 01:13:45.400]   Yeah, so they're public records, right? And so one day, someone, one day for some reason,
[01:13:45.400 --> 01:13:50.520]   I put my passport number into Google. I think I was just having a kind of paranoid moment.
[01:13:50.520 --> 01:13:55.640]   And I found literally hundreds of websites that had my passport number and my home address
[01:13:55.640 --> 01:14:00.680]   linked together because that's where the container was being delivered. So I actually
[01:14:00.680 --> 01:14:05.000]   have not used my home address for deliveries except for things like shipping containers.
[01:14:05.000 --> 01:14:10.760]   From about 15, 20 years now, I rent a post box down the road because I've had problems with
[01:14:10.760 --> 01:14:16.200]   stalkers and weirdos and whatever. And yet there are some things like you cannot have your shipping
[01:14:16.200 --> 01:14:21.800]   container dropped off at the post box a mile away. You know, you still have to get it home.
[01:14:21.800 --> 01:14:29.720]   And I never got rid of them all. We moved and my passport expired. Like that's why I'm now safe.
[01:14:29.720 --> 01:14:36.200]   How? Right. And it's pretty bad. I have a Google alert for my name and address.
[01:14:36.200 --> 01:14:41.800]   And whenever it pops up in a public search result, I go and ask the company to take it down.
[01:14:41.800 --> 01:14:45.880]   I do worry that if Google alerts were to leak, well, then that would expose a lot of my information.
[01:14:45.880 --> 01:14:48.760]   But if Google were to leak, it would expose a lot of my information anyway.
[01:14:48.760 --> 01:14:55.240]   So far, to my knowledge, Google has never had a breach. They're pretty good at detecting your
[01:14:55.240 --> 01:14:58.040]   information. Well, no, no, they were I mean, they were breached by the Chinese government.
[01:14:58.040 --> 01:15:04.280]   Oh, well, that's different. Ben, Ben, the guy that I co wrote that paper for nature for, he ran the
[01:15:04.280 --> 01:15:10.920]   forensics on the Chinese state hack. Holy cow. Yeah. When was that? And Google users get
[01:15:10.920 --> 01:15:15.880]   breached all the time. I just don't. And Google's had a bunch of insider attacks through their own
[01:15:15.880 --> 01:15:20.840]   through their own errors or yeah. And then Google's had a bunch of insider attacks. Oh, they have.
[01:15:20.840 --> 01:15:26.120]   Yeah. Where they've just had like people following around. Mostly it's dudes creeping on women.
[01:15:26.120 --> 01:15:30.680]   Oh my God. All right. I mean, a lot of these companies have it. Google's got actually pretty good
[01:15:30.680 --> 01:15:35.960]   internal access controls and forensics is to understand it. There was an amazing piece and wired
[01:15:35.960 --> 01:15:43.800]   about how Amazon in order to enable the agility of its teams has no had no controls over making
[01:15:43.800 --> 01:15:49.000]   whole copies of all of their user and or merchant data. And literally thousands of them were floating
[01:15:49.000 --> 01:15:53.640]   around within the company. Every team had its own local copy and there was no tracking and
[01:15:53.640 --> 01:15:59.080]   no forensics. They have tons of insider attacks. They have people who are like selling to merchants,
[01:15:59.080 --> 01:16:05.080]   how their rivals products were performing and they had people were creeping on customers and so on.
[01:16:05.080 --> 01:16:09.240]   Amazon's another one that I think a lot of people get delivered to their home and even if they have
[01:16:09.240 --> 01:16:15.240]   a mailbox because you know, it's you order heavy things right or big thing. Oh yeah. And so, you know,
[01:16:15.240 --> 01:16:19.480]   and what's the point of paying for prime and next day delivery? If you have it delivered to a mailbox
[01:16:19.480 --> 01:16:24.440]   that you then have to drive to and you only go to once a day. Right. Yeah, I have a post office
[01:16:24.440 --> 01:16:28.840]   box for the same reasons, Corey, but I'm afraid my ops sec is not great. I have so much stuff delivered
[01:16:28.840 --> 01:16:33.160]   at home. It's probably you can put your name. You can do the put your name into into Google with
[01:16:33.160 --> 01:16:38.280]   your address, put an alert up. That's it's not a terrible plan. Like I've I've I've the only reason
[01:16:38.280 --> 01:16:42.360]   I'm talking about it in public is I've mentioned it to a bunch of security experts and none of them
[01:16:42.360 --> 01:16:48.680]   were like you're doing what that's crazy. Don't do that. I know a security researcher who's at a
[01:16:48.680 --> 01:16:54.600]   high risk. He writes a lot about Carter's Eastern European Carter's and he's had like
[01:16:54.600 --> 01:17:01.000]   heroin sent to his home by Carter's who then called the police. He's been swatted and he when
[01:17:01.000 --> 01:17:06.840]   he bought a home registered an LLC out of state and then bought the house with his LLC
[01:17:06.840 --> 01:17:12.360]   so that it's there's it's not recorded. The deed recording just says that there's no
[01:17:12.360 --> 01:17:18.440]   estate LLC that owns it. So you put in the search terms, not just your name. You'd put your full
[01:17:18.440 --> 01:17:23.880]   address in the search term. Your home address. Yeah. Okay. I'm going to do that. And then you
[01:17:23.880 --> 01:17:27.560]   get all the results. You can even get it as an RSS feed. So you just put it in your feeder.
[01:17:27.560 --> 01:17:36.040]   And watch the fun happen right in front of your very very eyes. We thought that Spotify might
[01:17:36.040 --> 01:17:43.400]   suffer because of Joe Rogan. Not so much. Spotify's quarterly results are out. Even though they
[01:17:43.400 --> 01:17:49.160]   kicked off one and a half million premium users in Russia due to the invasion of Ukraine,
[01:17:49.160 --> 01:17:54.600]   even though the Joe Rogan controversy caused a lot of high profile people to say, well,
[01:17:54.600 --> 01:18:03.560]   I'm abandoning Spotify in the first quarter of this year. Spotify gained two million premium
[01:18:03.560 --> 01:18:08.680]   subscribers. So you subtract that one and a half million from Russia and you gain two million.
[01:18:09.400 --> 01:18:15.080]   16 million ads supported subscribers. Spotify is a juggernaut, which scares me a little bit
[01:18:15.080 --> 01:18:20.120]   because I feel like they're a little bit hostile to a free and open podcast ecosystem.
[01:18:20.120 --> 01:18:29.960]   Leo, I've got a musician friend. He says, just a couple years ago, his streaming checks were
[01:18:29.960 --> 01:18:35.560]   pretty evenly distributed. You'd get some from Apple Music, some from Amazon, some from Spotify.
[01:18:35.560 --> 01:18:43.640]   And he said in recent months, it is really consolidated to Spotify. Spotify is what is paying his bills.
[01:18:43.640 --> 01:18:51.640]   Look at my kids. I have Apple Music. I have Google YouTube Music. I have a lot of choices.
[01:18:51.640 --> 01:18:57.800]   And there could be on my family plan. They don't want Spotify. Everybody wants Spotify.
[01:18:57.800 --> 01:19:00.680]   That's kind of depressing.
[01:19:04.760 --> 01:19:15.800]   OK. OK. Data from fake legal requests has been used to extort miners sexually from alphabet,
[01:19:15.800 --> 01:19:23.960]   from Apple, from Facebook. Essentially, what happens is the bad guys, the stalkers, the creeps,
[01:19:23.960 --> 01:19:31.000]   pose as law enforcement and send emergency requests to these companies. Snaps also in there,
[01:19:31.000 --> 01:19:36.680]   Twitter is in there, Discord is in there. These are fraudulent legal requests. And then they use the
[01:19:36.680 --> 01:19:50.040]   data to do their bad guy things. Any thoughts? All of these things are just OK.
[01:19:50.040 --> 01:19:55.560]   So I mean, I got stuff to say, but I just I don't want to let other folks have a chance.
[01:19:56.600 --> 01:20:02.840]   Very kind of view. Alex Stavos says, I know emergency data requests get used for real life
[01:20:02.840 --> 01:20:07.960]   threatening emergencies every day. But it's tragic that this mechanism is also being abused
[01:20:07.960 --> 01:20:14.040]   to sexually exploit children. He's former chief security officer at Facebook. I don't know what
[01:20:14.040 --> 01:20:21.080]   you can do about this. That's the problem. Well, I mean, there is an argument that getting a warrant
[01:20:21.080 --> 01:20:25.960]   has really hard. And that's probably true in some places. We know it's not true in other places.
[01:20:25.960 --> 01:20:32.600]   So in some cases, these extraordinary data requests are really about a very minor amount of labor
[01:20:32.600 --> 01:20:39.080]   saving. And it produces an enormous risk. So you've got two different risks you're weighing, right?
[01:20:39.080 --> 01:20:43.720]   One is that you have these companies that have billions of users and have incredibly intimate and
[01:20:43.720 --> 01:20:51.240]   powerful pictures of our lives that can be used to really do enormous harm to us. And we create
[01:20:51.240 --> 01:20:56.280]   a system that is very easy to breach. I think there's something like 16,000, maybe 1600 US
[01:20:56.280 --> 01:21:00.520]   police departments. There's just no way they're going to be able to identify all the different
[01:21:00.520 --> 01:21:06.680]   police emails and so on. And in fact, a lot of the forged requests came from real police emails
[01:21:06.680 --> 01:21:11.960]   because the police websites had known vulnerabilities and they were able to open a web shell and take
[01:21:11.960 --> 01:21:16.840]   over accounts or create new accounts. A lot of this was done by children who attacked other
[01:21:16.840 --> 01:21:21.800]   children. So it's like we're not talking about a high degree of security here that was being
[01:21:21.800 --> 01:21:28.760]   breached. This is really like a very beginner level script kind of security defects that we are
[01:21:28.760 --> 01:21:34.440]   we're backstopping this very powerful bypass with. And against that, you have the security risks
[01:21:34.440 --> 01:21:39.240]   of making the police get a warrant. And because warrants are a lot harder to forge,
[01:21:39.240 --> 01:21:48.360]   right? Like warrants are a lot more recognizable and standardized than like a casual email from
[01:21:48.360 --> 01:21:53.480]   a cop that just says, you know, there's a bomb on the bus and it'll go off if it drops under 40
[01:21:53.480 --> 01:21:59.160]   miles an hour and I need to get into Sandra Bullock's email, right? And you just have to kind of
[01:21:59.800 --> 01:22:10.040]   curiously specific, by the way. We we we tour on Tony and we stick together. So we're like, we're
[01:22:10.040 --> 01:22:19.080]   like this. So yeah, that the I guess the I think that the security tradeoff is a bad one. And I
[01:22:19.080 --> 01:22:23.720]   think the companies could say no, by the way, they they are not legally obligated. If it's not
[01:22:23.720 --> 01:22:27.800]   signed by a judge, if it's not a legal warrant, they think blood will be on their hands, right?
[01:22:27.800 --> 01:22:32.280]   So cops are the worst thing that could happen is that later there's a press conference saying,
[01:22:32.280 --> 01:22:36.520]   well, you know, we went to Apple and they wouldn't give us this information and as a result,
[01:22:36.520 --> 01:22:39.400]   the child has died. That's that's what they don't want to have happen.
[01:22:39.400 --> 01:22:45.880]   And you know, we could we could make it if we could if we could show that it's actually too
[01:22:45.880 --> 01:22:49.560]   hard to get a warrant, we could make it easier to get a warrant. I'm not convinced it's too hard
[01:22:49.560 --> 01:22:54.360]   to get a warrant. I think there is supposed to be some due process there. And I and I don't think
[01:22:54.360 --> 01:23:00.440]   that judges are like, you know, unduly skeptical of cops requests for warrants.
[01:23:00.440 --> 01:23:05.000]   Yeah, make it make it make it be the judges decision, not the not apples or snaps or
[01:23:05.000 --> 01:23:09.720]   Facebooks. Make it make it wake a judge up say, Your Honor, I we need this information. We need
[01:23:09.720 --> 01:23:16.280]   it now sign this. I don't think that's unfair or unreasonable to ask that. I agree. Yeah, I mean,
[01:23:16.280 --> 01:23:20.760]   again, it will introduce a new security risk, which is the risk that it will take a little longer.
[01:23:20.760 --> 01:23:25.000]   And there might be some emergencies that are harmed. But what we're seeing is the
[01:23:25.000 --> 01:23:29.080]   visible side of the security risk here. And it's only the visible side. We don't know about the
[01:23:29.080 --> 01:23:33.720]   invisible side, right? Like if your bank account gets cleaned out and you never find out why,
[01:23:33.720 --> 01:23:41.000]   maybe it's because someone got the information needed to socially engineer your bank by doing
[01:23:41.000 --> 01:23:45.800]   one of these requests. And you never found out about it. Right. Well, so it's not just social
[01:23:45.800 --> 01:23:50.440]   media sites, a bank probably is also well, maybe, but I think the bank would probably say,
[01:23:50.440 --> 01:23:55.240]   no, but when you call up the bank and say, Hi, this is Corey Doctorow. And here are all the
[01:23:55.240 --> 01:23:59.960]   things you need to know to verify that I'm me, all of which were taken from my private account
[01:23:59.960 --> 01:24:04.120]   information called from Google and Facebook and whatever. If you can break into my Gmail
[01:24:04.120 --> 01:24:11.800]   and get my all my correspondence with my bank manager, and you can say, Hey, Fred, this is Corey.
[01:24:11.800 --> 01:24:16.120]   Remember last week when we were talking about the hot dog cookout? Well, anyway, funniest thing.
[01:24:16.120 --> 01:24:21.800]   I dropped my ATM card in the toilet. And, you know, I'm on my way to the airport. I'm sending my
[01:24:21.800 --> 01:24:27.240]   kid over now. Could you just give him another ATM card? Right. Like if you, if you, like, there's a
[01:24:27.240 --> 01:24:34.280]   lot of, of, of social engineering attacks, right, and semi automated social engineering attacks,
[01:24:34.280 --> 01:24:38.120]   where people are doing things like, like getting your password reset questions. So they're never
[01:24:38.120 --> 01:24:41.400]   talking to a human. They're just finding out what your first street was or whatever.
[01:24:41.400 --> 01:24:46.520]   There's a lot of that that is never solved. We don't know where the source came from. And so we
[01:24:46.520 --> 01:24:51.400]   have no way to know how widespread this is. And I'll remind you, the only reason we found out about
[01:24:51.400 --> 01:24:59.800]   this is because literal children had had started to use this en masse primarily to sexually
[01:24:59.800 --> 01:25:07.320]   exploit other children. Good Lord. And so there might have been other more sophisticated attackers
[01:25:07.320 --> 01:25:12.360]   who just like kept it on the DL. Yeah. And who might still be doing that, right? And we would never know.
[01:25:12.360 --> 01:25:19.320]   Yikes. All right, I'm going to save this one. We're going to take a little break. I'm going to
[01:25:19.320 --> 01:25:23.000]   save this one. All of you can join in. I have a feeling the science fiction author might have
[01:25:23.000 --> 01:25:28.840]   something to say. Scientists, astronomers are about to tell aliens where we are.
[01:25:28.840 --> 01:25:36.360]   For years, decades, we've been listening for sounds of extraterrestrial intelligence,
[01:25:36.360 --> 01:25:42.280]   radio signals, and the like. But there are some astronomers who think, you know, maybe we ought to
[01:25:42.280 --> 01:25:49.720]   be sending signals out trying to contact these folks directly. Maybe we ought to take this into
[01:25:49.720 --> 01:25:54.920]   our own hands. What could possibly go wrong? Well, there are a few people who think it's not such a
[01:25:54.920 --> 01:25:59.800]   good idea. We'll get to that in just a moment. Tim Stevens is here. Formerly Roadshow. Now,
[01:25:59.800 --> 01:26:09.320]   CNET cars. Is it CNET cars.com? It is CNET.com/cars. Much better than 1-800-CNET-Cars.com.
[01:26:09.320 --> 01:26:12.360]   That still works too. No, no. No, it doesn't.
[01:26:12.360 --> 01:26:20.760]   Owen Thomas from Protocol Protocol.com. That's right. Yeah, got to get it right.
[01:26:20.760 --> 01:26:27.320]   Great to have you. And of course, Corey Doctorow's latest book. Well, what is your latest book?
[01:26:27.320 --> 01:26:32.360]   Is it the Pozy book now? No, it's How to Destroy Surveillance Capitalism. But the next book,
[01:26:32.360 --> 01:26:36.920]   which is coming in September, is Choke Point Capitalism, which is the book I wrote with Rebecca
[01:26:36.920 --> 01:26:43.000]   Gibleen about the creative labor markets can't be solved just by adding more copyright,
[01:26:43.000 --> 01:26:47.800]   while you need a bunch of other stuff like antitrust and contracting rules and transparency
[01:26:47.800 --> 01:26:52.200]   and accounting and all that other stuff. Yes, we talked about that last time. That sounds really,
[01:26:52.920 --> 01:26:58.840]   really good. Our show today brought to you by Noreva, speaking of sounding good, if you are
[01:26:58.840 --> 01:27:03.080]   bringing people back into the office, you're starting to use the huddle room, the conference
[01:27:03.080 --> 01:27:07.240]   room, you have people, but you also have people still at home. And if you're still relying on
[01:27:07.240 --> 01:27:10.920]   that crappy little thing in the middle of the table, you know those people at home aren't here
[01:27:10.920 --> 01:27:15.880]   and what's going on. They're dialed out because it's impossible to understand. Or maybe you went
[01:27:15.880 --> 01:27:23.400]   to a big expensive AV company and got a complicated system with speakers and microphones and wires
[01:27:23.400 --> 01:27:29.000]   and DSPs everywhere. And they have to come in every time and calibrate the darn thing. And
[01:27:29.000 --> 01:27:37.720]   there is a third way, a much better way. It's Noreva. Noreva has patented their microphone
[01:27:37.720 --> 01:27:43.960]   missed technology. That means all you have to do is put in what looks like a speaker bar in your
[01:27:43.960 --> 01:27:50.120]   conference room, big room, maybe two of them. It's got integrated microphones. The microphone
[01:27:50.120 --> 01:27:55.800]   missed technology on that speaker fills the room with thousands of virtual microphones.
[01:27:55.800 --> 01:28:01.160]   So that there are no dead zones. Everyone could be heard clearly everywhere in the room no matter
[01:28:01.160 --> 01:28:05.960]   which way they're facing. If they're social distancing, if they're if it's a classroom,
[01:28:05.960 --> 01:28:10.840]   participants could just move around and talk naturally in the space and remote students
[01:28:10.840 --> 01:28:17.080]   can still hear and exactly what's going on. It is a revolution in medium room technology and it
[01:28:17.080 --> 01:28:22.360]   couldn't come at a better time. You don't have to have any any technicians. In fact, you could
[01:28:22.360 --> 01:28:26.680]   install this yourself if you ever installed a sound bar. It's easy 30 minute DIY job.
[01:28:26.680 --> 01:28:32.600]   Continuous audio calibration means your rooms are instantly and always ready with optimized
[01:28:32.600 --> 01:28:39.320]   audio. No outside technician is necessary. And management's easy too. Your IT team will love
[01:28:39.320 --> 01:28:45.800]   it, especially if you have many rooms. The Nareva console, let's IT monitor, manage and adjust the
[01:28:45.800 --> 01:28:53.240]   system anywhere they are, even offsite. So there's no need for IT to go from room to room. Look how
[01:28:53.240 --> 01:28:59.320]   easy it is to install your new Nareva sound bar. So ask yourself, if you want to go with a costly
[01:28:59.320 --> 01:29:04.840]   and complicated traditional system, are you tired of the little microphone in the middle of the
[01:29:04.840 --> 01:29:11.640]   table? That's the worst way to go. Make the leap. Simple economical, effective Nareva and you are
[01:29:11.640 --> 01:29:21.560]   eva.com. Nareva.com is the best way to do huddle room audio. They've got the patent on it and it
[01:29:21.560 --> 01:29:26.440]   really works. Thank you, Nareva, for your support of this week at Tech. You support us when you go
[01:29:26.440 --> 01:29:33.000]   to that website. Actually, I guess it just it's just Nareva.com. So you still support us by going
[01:29:33.000 --> 01:29:44.440]   so go ahead and do it. All right. ET, phone home. Two astronomers are two astronomer groups are
[01:29:44.440 --> 01:29:50.760]   planning to send messages from the world's largest radio telescope in China sometime in 2023.
[01:29:50.760 --> 01:29:55.320]   This is by the way, I think, Corey, isn't this the plot of the three body problem?
[01:29:55.320 --> 01:30:00.200]   I think so. Well, it's certainly, I mean, the plot's very complicated to three body problem.
[01:30:00.200 --> 01:30:03.960]   Chichen lose quite a complex plotter, but that's certainly a plot element of it.
[01:30:03.960 --> 01:30:10.440]   At the beginning, anyway, they're going to beam a signal of radio pulses over a broad swath of sky.
[01:30:10.440 --> 01:30:16.840]   They'll be on and off like the ones and zeros of digital information. The message is called
[01:30:16.840 --> 01:30:21.320]   the beacon in the galaxy. The first thing they'll do is send prime numbers,
[01:30:21.320 --> 01:30:28.440]   presuming that math is universal and mathematical operators, then the biochemistry of life,
[01:30:29.160 --> 01:30:35.720]   human forms, and then the controversial part, the Earth's location at a timestamp.
[01:30:35.720 --> 01:30:40.920]   They're sending the message toward a group of millions of stars near the center of the galaxy,
[01:30:40.920 --> 01:30:46.520]   10 to 20,000 light years from Earth. Now, the only good thing is it'll take them 10 to 20,000
[01:30:46.520 --> 01:30:50.440]   years to get this. And I'm going to hope it's going to take them at least this long to come back.
[01:30:50.440 --> 01:30:56.360]   So we don't have to worry too much about this, but still, theoretically,
[01:30:57.560 --> 01:31:04.280]   is it a bad idea to say, hey, guys, come on over. Let's get together. Let's talk.
[01:31:04.280 --> 01:31:10.760]   Cori, you must do you think about alien life? First contact things like that in any of your books.
[01:31:10.760 --> 01:31:17.400]   I can't remember. You're not that kind of sci-fi author, I don't think. Oh, you're muted.
[01:31:17.400 --> 01:31:23.560]   Sorry. I thought Kim Stanley Robinson's book Aurora did a really good job of this.
[01:31:23.560 --> 01:31:28.280]   Sorry, I had my mic off there. I only think about it as a literary device. I think that the
[01:31:28.280 --> 01:31:35.000]   distances are really big. They're kind of buzzkill big because they're 20,000 years to go.
[01:31:35.000 --> 01:31:38.040]   Well, no, that's 20,000 years at the speed of light.
[01:31:38.040 --> 01:31:43.080]   Anything that comes back is not going to come back at the speed of light or any appreciable
[01:31:43.080 --> 01:31:49.320]   fraction of it. So we're talking about significantly longer than behaviorally modern humans have
[01:31:49.320 --> 01:31:56.840]   existed at a point in which it's just very, very, very far away.
[01:31:56.840 --> 01:32:01.320]   And so I think that it's like these are interesting thought experiments, but
[01:32:01.320 --> 01:32:06.600]   and it's not like they're so far away that our light cone doesn't intersect with them so that
[01:32:06.600 --> 01:32:11.800]   we can never, in fact, influence each other. They might be watching. I love Lucy right now.
[01:32:11.800 --> 01:32:17.800]   Well, no, they're not going to wait about 9,550 years from now. But I just don't think we're
[01:32:17.800 --> 01:32:21.000]   going to make contact per se. Like, I don't think we're going to have a two way exchange.
[01:32:21.000 --> 01:32:25.160]   Yeah. It's the Fermi Paradox. Yeah.
[01:32:25.160 --> 01:32:29.000]   And he thought so and are you sci-fi fan?
[01:32:29.000 --> 01:32:35.800]   I am actually. It's kind of fun. I'm staying with my brother and he kind of has our childhood
[01:32:35.800 --> 01:32:41.880]   sci-fi collection. Oh, that's cool. Oh, nice. Yeah. So like visiting the wall of Heinlein
[01:32:43.720 --> 01:32:51.080]   is a, you know, a lot of memories. Yeah. Yeah. I think, you know, I don't know. I think that
[01:32:51.080 --> 01:32:58.280]   doesn't our atmosphere kind of radiate enough information to the press and aliens. Like,
[01:32:58.280 --> 01:33:03.160]   hey, there's a nice juicy oxygen, you know, oxygenated planet with water.
[01:33:03.160 --> 01:33:11.320]   And I love Lucy. Yeah. I mean, I think, I think that we've detected planets that might be able
[01:33:11.320 --> 01:33:16.360]   to sustain life. So if at our level of technology, we can do that. If there's an alien out there with,
[01:33:16.360 --> 01:33:19.960]   you know, sophisticated technology, they've probably spotted us.
[01:33:19.960 --> 01:33:22.360]   So very famous. I think they're worrying about nothing.
[01:33:22.360 --> 01:33:30.360]   Very famously, Stephen Hawking about 10 years ago said, you know, intelligent aliens could be
[01:33:30.360 --> 01:33:36.600]   rapacious marauders roaming the cosmos in search of resources to plunder and planets to conquer
[01:33:36.600 --> 01:33:44.600]   and colonize. So maybe we shouldn't be announcing our address. Maybe and see the other problem.
[01:33:44.600 --> 01:33:50.120]   Maybe they're not so far away, but they could still intercept the message and maybe
[01:33:50.120 --> 01:33:56.360]   come visit us. You know, and I'm not going to worry about it right now, not yet.
[01:33:56.360 --> 01:34:00.200]   Yeah. Like I would really worry about climate change.
[01:34:00.200 --> 01:34:02.920]   That's a little more concerning. Yeah.
[01:34:03.640 --> 01:34:08.200]   Think what what an amazing thing it would be if our race could endure long enough for aliens to
[01:34:08.200 --> 01:34:12.760]   come and wipe us out in 10,000. There you go. We're doing our best to make sure we can't.
[01:34:12.760 --> 01:34:16.920]   It would mean we got an extra 10,000 years above the current odds.
[01:34:16.920 --> 01:34:28.120]   Excellent. Excellent point. Let's see. Netflix had a very bad, no good earnings report. I should
[01:34:28.120 --> 01:34:32.920]   do. I'm going to, you know, it's funny. I avoid this, but I probably should go through all of the
[01:34:32.920 --> 01:34:39.080]   earnings this week because it was kind of an interesting mixed bag. The worst news was Netflix
[01:34:39.080 --> 01:34:46.120]   announcing on Tuesday that actually was a week ago, Tuesday, that for the first time in a decade,
[01:34:46.120 --> 01:34:53.240]   they'd lost subscribers. You know, 200,000 here, 200,000 there doesn't sound too bad. The stock
[01:34:53.240 --> 01:35:00.920]   market punished them. 35% lost more than $50 billion in the day immediately after it's actually
[01:35:00.920 --> 01:35:07.400]   continued to drop since Netflix has started laying off people as if like the whole thing is
[01:35:07.400 --> 01:35:12.600]   folding up and going home. Netflix did forecast they'd lose another 2 million subscribers over
[01:35:12.600 --> 01:35:18.120]   the next three months. So that may be what's on this. But didn't Netflix have a great two years
[01:35:18.120 --> 01:35:24.680]   with pandemic? Can they go ahead? I think if anything, as if we needed another indicator of just how
[01:35:24.680 --> 01:35:29.880]   speculative the market is right now, which was a pretty good one. Netflix has a kind of not great
[01:35:29.880 --> 01:35:35.080]   quarter, some bad news. And everyone just drops it immediately. Stock drops 35, 40% something like
[01:35:35.080 --> 01:35:41.880]   that. Yeah, I think that shows how much people are really looking for big continual gains. And
[01:35:41.880 --> 01:35:46.360]   ultimately, yeah, it does point to the fact that Netflix and disrupted the industry, they're way
[01:35:46.360 --> 01:35:50.440]   out ahead. And now a lot of other competitors have caught up. And there are so many options on the
[01:35:50.440 --> 01:35:56.120]   market that it is fairly sustainable for most people. So I know quite a few people, myself included,
[01:35:56.120 --> 01:36:00.680]   who are kind of floating between streaming services, they'll go here for a month or two and then
[01:36:00.680 --> 01:36:04.520]   jump over somewhere else for a while. And it used to be that Netflix was that one
[01:36:04.520 --> 01:36:10.360]   service that you needed to have on 12 months of the year. But that's not really the case anymore.
[01:36:10.360 --> 01:36:12.680]   There are compelling options for mother services.
[01:36:12.680 --> 01:36:17.240]   And Netflix is just a big deal. I mean, who needs
[01:36:19.400 --> 01:36:27.240]   some better than others? For sure. And I think that internally at Netflix,
[01:36:27.240 --> 01:36:31.800]   you have to remember that the execs themselves are heavily compensated with stock. And so this
[01:36:31.800 --> 01:36:37.160]   stuff really matters to them. But also, you have to remember that they're recruiting an extremely
[01:36:37.160 --> 01:36:43.880]   tight labor markets for, especially for technical staff and executive staff, and that they get an
[01:36:43.880 --> 01:36:50.120]   enormous discount on their wage bill provided that they can convince people that taking compensation
[01:36:50.120 --> 01:36:55.240]   and stock is a good idea because it'll go up and up and up. And if it doesn't, then you have to
[01:36:55.240 --> 01:37:01.640]   pay cash. There's a reason that Facebook has failed in its engineer recruiting numbers for the last
[01:37:01.640 --> 01:37:08.440]   three years, like significantly, I think by about a third. It's because engineers don't believe that
[01:37:08.440 --> 01:37:12.520]   the Facebook shares that they're getting are going to blow up the way the shares that they might get
[01:37:12.520 --> 01:37:17.800]   from somewhere else might be. And so Facebook's cash basis gets materially worse every time their
[01:37:17.800 --> 01:37:23.800]   share price goes down. And then investors go, "Oh, well, with a bad cash basis, we should sell our
[01:37:23.800 --> 01:37:28.760]   shares." And then the share price goes down there and they're in a kind of destructive spiral.
[01:37:28.760 --> 01:37:33.960]   That's probably why it's important to pay attention to quarterly results. I mean, I don't care what
[01:37:33.960 --> 01:37:39.400]   the stock market says about a company. I don't think it necessarily jives with how the company is
[01:37:39.400 --> 01:37:45.400]   going to do. But it does materially impact their ability to acquire talent. In fact,
[01:37:45.400 --> 01:37:49.960]   that's one reason taking Twitter private might not be a really great strategy long-term.
[01:37:49.960 --> 01:37:55.400]   I was just going to say, I know we said we were done talking about Twitter, but there's a lot of
[01:37:55.400 --> 01:37:59.160]   reasons why a lot of employees at Twitter are unhappy with this. And there's definitely expected
[01:37:59.160 --> 01:38:02.600]   to be a lot of losses within Twitter if this deal goes through. And that's one of the main
[01:38:02.600 --> 01:38:07.800]   reasons that that conversation has been hugely based on stock. And certainly, there are ways to
[01:38:07.800 --> 01:38:13.320]   provide similar proxies for private companies, but certainly nothing as visible or as seemingly
[01:38:13.320 --> 01:38:18.680]   attractive to new employees as the good old stock option. And if that's not on the table anymore,
[01:38:18.680 --> 01:38:22.840]   how is Twitter going to continue to hire good people? I think for some reason, that might
[01:38:22.840 --> 01:38:26.600]   actually be something that Musk is looking forward to, because one of the things he wants to do is
[01:38:26.600 --> 01:38:31.240]   cut staff to cut costs. But in the long run, that's going to make it really hard for them to recruit
[01:38:31.240 --> 01:38:37.480]   talented engineers and keep them. Alphabet, in a similar situation, YouTube ad revenue
[01:38:37.480 --> 01:38:43.480]   didn't go down. This is another stock market thing. It just didn't go up as dramatically. So,
[01:38:43.480 --> 01:38:49.640]   that's considered a loss from the point of view of the stock market. Growth's slowing dramatically
[01:38:49.640 --> 01:38:59.480]   on YouTube ad revenue. I think it only grew 14%, only 6.87 billion last quarter, but Wall Street
[01:38:59.480 --> 01:39:04.600]   wanted and hoped for more, and that's going to impact Google's ability to acquire talent.
[01:39:04.600 --> 01:39:09.560]   Maybe one of the reasons Google announced at the same time that they were going to spend $70
[01:39:09.560 --> 01:39:19.560]   billion to buy back stock, which puts them second in the tech sphere, Apple buying the most stock back.
[01:39:21.240 --> 01:39:28.440]   Apple's a big believer in stock repurchases, then Alphabet, then Facebook in 2021.
[01:39:28.440 --> 01:39:36.680]   I'm curious what you think, Owen, about stock buybacks. Is that a healthy thing for a company to do?
[01:39:36.680 --> 01:39:41.800]   It makes shareholders happy. I guess it probably makes employees happy, because it raises the stock
[01:39:41.800 --> 01:39:50.360]   value. So, there's two schools of thought. One is that a stock buyback is just a sign that you do
[01:39:50.360 --> 01:39:55.240]   not know how to take that cash and invest into new things that are going to grow fast.
[01:39:55.240 --> 01:39:58.520]   That's $70 billion they couldn't spend on R&D, for instance.
[01:39:58.520 --> 01:40:06.680]   Yeah, absolutely. Or buying things like YouTube, which they only paid $1.65 billion for,
[01:40:06.680 --> 01:40:11.720]   and invested more money in it along the ways. But I think that's been a great deal for them,
[01:40:11.720 --> 01:40:17.080]   overall, recent hiccups aside. The counter argument, and this is more of a technical thing,
[01:40:17.080 --> 01:40:23.560]   is when these companies are paying employees in stock, they're issuing new shares all the time
[01:40:23.560 --> 01:40:27.800]   to satisfy that compensation. That dilutes the stock, right?
[01:40:27.800 --> 01:40:36.120]   That dilutes the stock, so to keep the number of shares outstanding level in whatever range you want,
[01:40:36.120 --> 01:40:45.880]   you need to buyback stock. Otherwise, yeah, the share account keeps inflating.
[01:40:46.760 --> 01:40:52.360]   Google had strong growth, according to Sundar Pichai in Google Search and Cloud Revenue.
[01:40:52.360 --> 01:40:57.880]   He also said the Pixel 6 was the best-selling Pixel phone, which doesn't say anything at all,
[01:40:57.880 --> 01:41:07.000]   given how poorly the pixels have sold in the past. Alphabet has 163,000 employees,
[01:41:07.000 --> 01:41:11.400]   up 17% year over year. So, they are growing.
[01:41:11.400 --> 01:41:18.760]   I like my Pixel 6. It's a good phone. It's fine. I think that one of the things that
[01:41:18.760 --> 01:41:22.040]   was sexy, that's why it doesn't sell. It's not dead sexy.
[01:41:22.040 --> 01:41:28.200]   No. Yeah, it is the... I want a boring phone. I want a phone that just works every day,
[01:41:28.200 --> 01:41:33.640]   which more or less the Pixel 6 does. I switched to it after I dropped my old phone in water and
[01:41:33.640 --> 01:41:37.640]   killed it. This is the first time I'd done a phone upgrade in a long time without having a
[01:41:37.640 --> 01:41:42.200]   backup because I don't do cloud backups because I don't trust the cloud. So, I had to manually
[01:41:42.200 --> 01:41:46.760]   reassemble this phone and I'm getting all of the alerts that I'd already said yes to.
[01:41:46.760 --> 01:41:52.120]   And it's literally like whatever 15 years with the Android use alerts that I'm getting.
[01:41:52.120 --> 01:41:57.480]   And it's so annoying. Wait a minute. Wait a minute. You're telling me that in the past,
[01:41:57.480 --> 01:42:01.560]   your phone has been set no alerts. But now that you have a new phone,
[01:42:01.560 --> 01:42:04.520]   all the old alerts are coming back. It doesn't start from now.
[01:42:05.720 --> 01:42:10.760]   No, no, I mean things like tooltips. Oh, all that stuff you hadn't seen ever.
[01:42:10.760 --> 01:42:15.880]   Yeah. Yeah. And stuff where I think in that they've since eliminated from the UI,
[01:42:15.880 --> 01:42:22.360]   the ability to turn off certain kinds of nags. So, somehow I had Spotify configured before so
[01:42:22.360 --> 01:42:27.240]   that it wouldn't ask me to turn on Bluetooth every time I turned it on to quote, "improve service"
[01:42:27.240 --> 01:42:30.280]   because I don't want Spotify to know where I am. I don't want to... They're using it for location
[01:42:30.280 --> 01:42:37.800]   services. I use Spotify because I have the NoMic Sonos One systems, the non-spying ones.
[01:42:37.800 --> 01:42:43.160]   And you have to use something like Spotify for it. So, I have it. I pay for it. And I don't want
[01:42:43.160 --> 01:42:47.960]   to be nagged to turn on this thing. And all there is is like maybe later, there's not a never ask
[01:42:47.960 --> 01:42:57.240]   and there used to be somewhere in the UI a setting that persisted when I migrated between phones
[01:42:57.240 --> 01:43:03.800]   that let you turn that off. And that setting is gone. The ability to make that setting is gone.
[01:43:03.800 --> 01:43:10.200]   And so now I just get the nag every time I turn on Spotify, which you know, it's just the last
[01:43:10.200 --> 01:43:16.360]   thing I want. It's really annoying. The other thing you're identifying here is that share prices
[01:43:16.360 --> 01:43:21.000]   tumble partly because of the speculation effect. I think that's absolutely right.
[01:43:21.000 --> 01:43:29.640]   And partly because of bad surprises. It's not just that Google didn't grow as fast as they had before.
[01:43:29.640 --> 01:43:33.560]   It's that Google didn't grow as fast as they predicted. And I think this ties into the speculation
[01:43:33.560 --> 01:43:38.200]   question because when you're speculating, you're just gambling on the future. And you're like,
[01:43:38.200 --> 01:43:43.320]   "Who sounds like they know what they're talking about?" Well, Pinchai sounds like he knows what he's
[01:43:43.320 --> 01:43:47.560]   talking about. The last seven years, he's told us it would grow this much and it grew that much.
[01:43:47.560 --> 01:43:52.680]   And then he doesn't. You're like, "Oh, maybe he just got lucky. I better sell some of my stock because
[01:43:52.680 --> 01:43:57.000]   he's just pulling numbers out of his butt as it turns out. And he doesn't know what he's talking
[01:43:57.000 --> 01:44:02.360]   about. And next year, the earnings could be anything. They could be double and they could be half.
[01:44:02.360 --> 01:44:05.960]   Who knows? Because clearly he doesn't know what he's doing. He's lost his crystal ball.
[01:44:05.960 --> 01:44:13.000]   And I think that's what Stampede's investors out of company, out of these big tech stocks,
[01:44:13.000 --> 01:44:16.520]   when they fail. Because as you said, their fundamentals are terrible.
[01:44:17.400 --> 01:44:22.360]   For at least for Twitter and Netflix and stuff. Their fundamentals aren't great.
[01:44:22.360 --> 01:44:27.000]   And so all of their value is people buying the stock and the expectation that people will buy
[01:44:27.000 --> 01:44:34.840]   the stock. And that's entirely driven by confidence in whether or not their executives are baby
[01:44:34.840 --> 01:44:38.760]   Nostradomises who've got their own little crystal ball that can tell you how their company will
[01:44:38.760 --> 01:44:44.040]   perform next year. Typically, Apple does very well by, it seems like sandbagging, always saying,
[01:44:44.040 --> 01:44:48.280]   oh, we're not going to do that well. And then outperforming the analyst's predictions.
[01:44:48.280 --> 01:44:52.840]   And then everybody goes, see, they're even better than we thought. You would think analysts was
[01:44:52.840 --> 01:44:59.160]   stuck to catch on to this sandbagging. But apparently they never do. Apple revenue grew
[01:44:59.160 --> 01:45:05.720]   9% year over year in the quarter. They sold a lot of Macs. In fact, one of the stats that
[01:45:05.720 --> 01:45:10.520]   stood out to me is 50% of the new Macs were sold to people who had never owned a Macintosh before.
[01:45:11.160 --> 01:45:17.720]   That's very good for the Macintosh brand. But Apple shares did go down because
[01:45:17.720 --> 01:45:25.720]   Luca Maestry Apple CFO warned about supply constraints. He says could hurt sales between
[01:45:25.720 --> 01:45:33.800]   $4 and $8 billion. The shutdown in Shanghai particularly hard on Apple. Actually, a lot of
[01:45:33.800 --> 01:45:39.880]   companies. And in fact, I think Pat Gelsinger at Intel said, don't expect this chip shortage to
[01:45:39.880 --> 01:45:49.240]   ease up until 2025. That's starting to hit the bottom line too. You can't make it. You can't sell
[01:45:49.240 --> 01:45:54.520]   them. I think the markets are going to start responding to the threat of the Digital Markets
[01:45:54.520 --> 01:46:00.360]   Act and the Access Act as well. Oh, interesting. Yeah. In a positive or negative way.
[01:46:00.360 --> 01:46:04.600]   Oh, in a negative way, because it's going to be bad for their profits. It's going to be good for
[01:46:04.600 --> 01:46:10.840]   the world. It's just not going to be good for their profits. The idea is to reduce their
[01:46:10.840 --> 01:46:14.600]   monopoly rent extraction by making it easier to switch. We're just talking about switching costs.
[01:46:14.600 --> 01:46:23.160]   If you can leave iMessage without losing the whatever color triangle that you get when you're
[01:46:23.160 --> 01:46:29.000]   on iMessage and not an iOx user. I believe it's a, I believe it's a, but your blue triangle and
[01:46:29.000 --> 01:46:33.080]   then it's green if you're Android. I can't remember now. It's a little black turtle.
[01:46:33.560 --> 01:46:37.000]   [laughter]
[01:46:37.000 --> 01:46:40.280]   And all the stuff comes out in the Elizabeth Holmes voice.
[01:46:40.280 --> 01:46:47.160]   Yeah. I mean, I just think that that's going to hurt them. Also, the App Store bills
[01:46:47.160 --> 01:46:52.200]   are going to be particularly hard on Apple. There's going to be other App Stores.
[01:46:52.200 --> 01:46:58.360]   And Apple, like, their argument is so weird that Apple users don't want this.
[01:46:59.960 --> 01:47:04.360]   Who is going to use this if not Apple users? It's not like the alternate App Store is going to be a
[01:47:04.360 --> 01:47:10.120]   big hit among Android users because they won't be able to install those apps. It's literally what
[01:47:10.120 --> 01:47:15.720]   they're saying is like you aren't an Apple user. You are a bad Apple user. You're like Steve Jobs
[01:47:15.720 --> 01:47:20.200]   and you're holding the phone wrong. You're buying your apps from the wrong store. You're trusting
[01:47:20.200 --> 01:47:26.760]   the wrong people. It's a very weird argument. Yeah. But I think the reality is going to be that
[01:47:26.760 --> 01:47:31.400]   very few people actually use these other App Stores. No, that's right. It's going to be too
[01:47:31.400 --> 01:47:36.040]   much work. It's going to be confusing. I just want to buy it with my fingerprint or my face.
[01:47:36.040 --> 01:47:38.840]   I don't want to do anything more. I don't want to think about it.
[01:47:38.840 --> 01:47:44.760]   Yeah. It's not going to be forced to open up all of those access points and APIs.
[01:47:44.760 --> 01:47:49.320]   Like, you're not going to have the same smoothness. No. Apple will make sure you don't.
[01:47:49.320 --> 01:47:55.080]   The other day I paid with my Apple Watch for a burrito. I told my daughter, I said,
[01:47:55.080 --> 01:47:59.960]   I feel like I'm not even paying for this at all. It's just free. Apple's paying. That's what it's
[01:47:59.960 --> 01:48:06.280]   called. Apple Pay. Apple will pay. The psychology of it is dramatic. When you take out that friction
[01:48:06.280 --> 01:48:12.920]   of actually opening your bill full to pulling money out, people buy more stuff. I'm convinced of it.
[01:48:12.920 --> 01:48:19.080]   It just gets easier and easier. Leo, have you heard the story about Steve Jobs at the Apple
[01:48:19.080 --> 01:48:26.680]   Cafe Max cafeteria? No. Like many CEOs, Steve Jobs took a $1 a year salary.
[01:48:26.680 --> 01:48:33.080]   Whenever he had a guest and took them out to Cafe Max, he would offer to pay and get really
[01:48:33.080 --> 01:48:40.600]   insistent that he pay because it was deducted from his paycheck and because it was just $1
[01:48:40.600 --> 01:48:47.080]   a year, he'd basically get the pizza or whatever they were bought for free. He's worth a billion.
[01:48:47.080 --> 01:48:53.560]   That is just a sheep skate, man. He loves getting free pizza from his own company.
[01:48:53.560 --> 01:48:58.520]   From my own company. I mean, ultimately, well, that's ridiculous. That's nuts.
[01:48:58.520 --> 01:49:06.440]   Snap is making this. What could possibly go wrong with this? Snap is making a little flying drone
[01:49:06.440 --> 01:49:13.320]   called the Pixi that can do selfies when you can't ask somebody to take a video for you.
[01:49:13.880 --> 01:49:19.640]   Available in the US and France for 230 bucks. Fortunately, it's expensive. So I don't expect
[01:49:19.640 --> 01:49:28.120]   to see a lot of them around. There's no controller, no SD card. I don't even know how it could
[01:49:28.120 --> 01:49:33.640]   possibly work. There are four pre-configured flight paths. So I guess you turn it on, you say float,
[01:49:33.640 --> 01:49:40.840]   orbit, or follow. Press the button. Pixi takes off. When you want to stop recording,
[01:49:40.840 --> 01:49:45.320]   you catch the drone, which may not be easy, but you catch it, put your hand below it,
[01:49:45.320 --> 01:49:49.960]   and it just lands in your hand. I think this sounds kind of cool, actually.
[01:49:49.960 --> 01:49:56.920]   There was a company, I think it was called Lily that promised something like this,
[01:49:56.920 --> 01:50:02.280]   and it turned out that their demo video was completely fake. Oh, I remember that. Yeah.
[01:50:02.280 --> 01:50:08.440]   Yeah. I think Snap has a little more at stake. So they're going to be very clearly on this video
[01:50:08.440 --> 01:50:15.080]   created with Pixi. So you better believe that's kind of an interesting idea. I guess,
[01:50:15.080 --> 01:50:21.160]   five to eight flights on a single charge. It's got a replaceable battery. It doesn't weigh very
[01:50:21.160 --> 01:50:28.200]   much about three ounces, four ounces. They did a sky writing campaign over my house yesterday.
[01:50:28.200 --> 01:50:33.080]   Oh, really? For this thing. Yeah. There's an Olympic pull across the street that I swim in every
[01:50:33.080 --> 01:50:39.000]   day because I got bad back, bad joints. I was on my back and I looked up at the sky and it was one
[01:50:39.000 --> 01:50:46.920]   of those planes that does like dot matrix printing, where it just shoots a puff of cloud and it makes
[01:50:46.920 --> 01:50:53.320]   a matrix of letters. It was like, use Pixi. It's awesome. Something like that. Wow. Take it with Pixi.
[01:50:53.320 --> 01:51:02.360]   Well, let me buy one. Interestingly, it's going to take 16 to 17 weeks to get here.
[01:51:03.000 --> 01:51:07.560]   Maybe I'll have it in time for Christmas. That's the other thing the supply chain is.
[01:51:07.560 --> 01:51:11.000]   I mean, we're getting used to seeing things like that now. Oh, yeah. You'll get this.
[01:51:11.000 --> 01:51:19.000]   This is going to be devastating for influencer boyfriends. So all those boyfriends who are
[01:51:19.000 --> 01:51:25.400]   photographing those candid moments of their influencer. They're out of work. They are out of
[01:51:25.400 --> 01:51:30.920]   work. I've been replaced by a Pixi. That's right. Isn't there? Wasn't there a whole account,
[01:51:30.920 --> 01:51:34.200]   Instagram account about the boyfriends who have to take pictures?
[01:51:34.200 --> 01:51:41.000]   So August 22nd, I will get my Pixi and I'll tell you all about it.
[01:51:41.000 --> 01:51:46.360]   And you'll fire your influencer boyfriend. I will fire my influencer boyfriend. He's gone.
[01:51:46.360 --> 01:51:52.680]   He's history. Get ready. I don't know if you can use it on not snap. Like, if you give them 200
[01:51:52.680 --> 01:51:58.120]   and whatever dollars for this thing, they decide you're a spammer and kill your account.
[01:51:58.120 --> 01:52:01.240]   Do you now own like a 200 and something dollar brick?
[01:52:01.240 --> 01:52:06.760]   See, this is how Corey thinks, ladies and gentlemen. This is why he's such a bitter, unhappy human
[01:52:06.760 --> 01:52:12.760]   being. Just enjoy the consumer society as it was intended. This is what happened with people who
[01:52:12.760 --> 01:52:19.080]   bought Facebook's, Facebook's, right? Yeah. The quest. I don't have a quest because I don't have
[01:52:19.080 --> 01:52:23.480]   a Facebook account. And I'm not going to create one just so I can get a quest. And you're right.
[01:52:23.480 --> 01:52:26.760]   If you for some reason got booted off of Facebook, your quest would stop working.
[01:52:27.640 --> 01:52:33.240]   That's crazy. And you know, back to Elon and his weird business plans, like imagine if
[01:52:33.240 --> 01:52:39.000]   your switching cost, if you left Facebook was a bunch of proprietary hardware that you bought
[01:52:39.000 --> 01:52:43.400]   that would only work with or if you left Twitter, rather, it was a bunch of proprietary hardware
[01:52:43.400 --> 01:52:47.400]   that only worked with Twitter, right? So someone buys snap and says, oh, we've got a certain fraction
[01:52:47.400 --> 01:52:55.080]   of our users that have sunk 200 bucks into being a snap user, we can probably charge them,
[01:52:55.080 --> 01:53:00.840]   you know, X dollars before they're like, I guess I'll give up that $200 investment. And if we keep
[01:53:00.840 --> 01:53:05.080]   it below there, we can just melt them for it. And the more stuff we sell them that's locked to our
[01:53:05.080 --> 01:53:12.600]   platform, the more we can mistreat them before they leave. I fell for it. Oh, well, I think I have
[01:53:12.600 --> 01:53:21.640]   a snap account. I'm not sure. You've been perma band. I probably have Fort Worth, the first city in
[01:53:21.640 --> 01:53:30.440]   the US to mine Bitcoin. It's installed three Bitcoin mining rigs in the basement in the IT
[01:53:30.440 --> 01:53:35.960]   room, the climate controlled information technology wing, which is probably just a room of Fort
[01:53:35.960 --> 01:53:41.320]   Worth City Hall. The miners will be hosted on a private network to minimize the security risk.
[01:53:41.320 --> 01:53:49.480]   Three bit main, Ant minor S nine mining rings will run mining rigs will run 24 hours a day,
[01:53:49.480 --> 01:53:54.920]   seven days a week, Fort Worth mayor standing in front of them. She says, it's a gold mine. We're
[01:53:54.920 --> 01:54:05.400]   going to be rich or not. She said they say that the mining rigs uses much energy as a household
[01:54:05.400 --> 01:54:11.160]   vacuum cleaner. I don't think there's going to be a lot of Bitcoin flowing from that Bitcoin
[01:54:11.160 --> 01:54:17.240]   mining rig. I think they're about 20 years too late to get rich on that to see our mares laying
[01:54:17.240 --> 01:54:24.840]   in their strategic beanie baby reserves. I'm planning for the future.
[01:54:24.840 --> 01:54:31.560]   Let's see. Okay, let's take a little break and then we will have a couple of fun ones
[01:54:31.560 --> 01:54:37.560]   to wrap it up. TikTok, the number one app worldwide in the first quarter of 2022.
[01:54:37.560 --> 01:54:46.520]   Congratulations, TikTok. Are there some reason to fear our new Chinese overlords with TikTok?
[01:54:46.520 --> 01:54:49.560]   Is there anything to worry about there? I mean, I think that they
[01:54:49.560 --> 01:54:57.080]   are definitely supplying information to the Chinese state on the basis of lawful requests
[01:54:57.080 --> 01:55:02.280]   and maybe even in the sloppy way that we talked about before. I'm sure there's an equivalent
[01:55:02.280 --> 01:55:07.320]   for TikTok. Something more interesting, there's a little European group called Algorithms
[01:55:07.320 --> 01:55:14.040]   exposed their data scientists to crowdsource gathering information about how the recommender
[01:55:14.040 --> 01:55:19.400]   algorithms work. They started just to figure it out. You install a browser plugin depending
[01:55:19.400 --> 01:55:23.560]   on which services you use. There's one for TikTok, one for YouTube. There's one for Pornhub,
[01:55:23.560 --> 01:55:29.640]   one for Amazon, and one for Facebook. It just feeds anonymized data into this analytical
[01:55:29.640 --> 01:55:35.000]   platform. One of the reports they wrote was on how TikTok is performing in Russia right now.
[01:55:35.000 --> 01:55:43.240]   They said it's created a little pocket universe in which because they banned TikTok banned Russian
[01:55:43.880 --> 01:55:51.560]   new videos from Russia. All that's circulating on Russian TikTok right now is from Russia.
[01:55:51.560 --> 01:55:59.000]   You can't put new video into Russia. They're pre-banned videos about how Russia is defending
[01:55:59.000 --> 01:56:05.480]   itself against Nazis in Ukraine. It's a completely alternate universe. That's something that
[01:56:05.480 --> 01:56:10.600]   algorithms exposed was able to do, was able to learn by mining these algorithms.
[01:56:11.800 --> 01:56:16.520]   I think that there are pocket universes of all kinds. What Eli Paris are called the filter bubble,
[01:56:16.520 --> 01:56:21.560]   although that term gets thrown around a lot these days. I think that there's a lot of these
[01:56:21.560 --> 01:56:26.120]   little pocket universes that are created by recommender algorithms. Some of them are super benign.
[01:56:26.120 --> 01:56:33.560]   If you start in a joinery and carpentry, you will find your way into the pocket universe of
[01:56:33.560 --> 01:56:38.040]   Japanese nail list joinery and carpentry. From there, you can capitalize further.
[01:56:39.160 --> 01:56:43.080]   Specific kinds. That's awesome. That's the kind of esoteric I'm here for all day.
[01:56:43.080 --> 01:56:45.320]   Yeah. But some of them are less benign.
[01:56:45.320 --> 01:56:49.720]   Right. Well, I just hope Elon doesn't ever buy Reddit. That's all because that's how I learned
[01:56:49.720 --> 01:56:55.960]   about things like that. I don't imagine he would. Let's take a little break. And our final
[01:56:55.960 --> 01:57:00.360]   end of the day, we have a few more stories. We can wrap it up with our great panel. I want you
[01:57:00.360 --> 01:57:06.280]   to go to twit.tv, our website, and take a look. Let me just do it real quickly. At something we've
[01:57:06.280 --> 01:57:11.720]   got at the bottom of this, at the very bottom here, there's a little accessibility icon.
[01:57:11.720 --> 01:57:16.680]   This is from Userway. And this is the sweetest thing. Userway is our sponsor
[01:57:16.680 --> 01:57:23.160]   of this segment. This is the sweetest way to make the site more approachable, more accessible to
[01:57:23.160 --> 01:57:28.920]   everybody. If you click that button, you'll get an accessibility menu. You can also hit control
[01:57:28.920 --> 01:57:33.480]   you. It allows you to change the contrast to highlight links, make text bigger, change the
[01:57:33.480 --> 01:57:40.040]   text spacing, dyslexia-friendly font, and a lot more accessibility settings. This is one of the
[01:57:40.040 --> 01:57:48.280]   ways Userway makes websites easier to access all over the world. Userway is a truly exciting
[01:57:48.280 --> 01:57:55.000]   company. Every website, according to the law, without exception, has to be accessible. That's
[01:57:55.000 --> 01:58:01.320]   one of the features of the ADA. Userway is an AI-powered solution that enforces those web
[01:58:02.280 --> 01:58:10.040]   content accessibility guidelines, the WCAG guidelines, easily, in a matter of seconds. Userway,
[01:58:10.040 --> 01:58:15.400]   AI, can do more than an entire team of developers could do in months. It's just one line of JavaScript.
[01:58:15.400 --> 01:58:20.760]   It may seem overwhelming when you first figure out that you need to make your website accessible
[01:58:20.760 --> 01:58:26.680]   or want to make your website accessible. Userway solutions make it easy, simple, cost-effective.
[01:58:26.680 --> 01:58:31.960]   You can even use their free scanning tool at userway.org to see if your website is ADA
[01:58:31.960 --> 01:58:37.880]   compliant. Userway works with some of the biggest websites in the world. If you have a giant enterprise
[01:58:37.880 --> 01:58:42.760]   grade website with thousands of pages, they even have a managed solution where their team can handle
[01:58:42.760 --> 01:58:48.440]   everything for you. Userway's AI and machine learning solutions power accessibility for over a
[01:58:48.440 --> 01:58:57.720]   million websites, including Twit, Coca-Cola, Disney, eBay, and FedEx. Now, these enterprise-level
[01:58:57.720 --> 01:59:02.520]   accessibility tools are available to small and medium businesses as well. Of course, as you grow,
[01:59:02.520 --> 01:59:08.200]   they can grow with you. It is now the leading accessibility solution in the market, 61% of
[01:59:08.200 --> 01:59:12.120]   the market to the biggest in the world. I'll give you some examples. Motley Fool,
[01:59:12.120 --> 01:59:19.480]   you know, Motley Fool, Financial News and Investment Advisor, had 1,911 pages on their website.
[01:59:19.480 --> 01:59:24.040]   They were getting 20 million page views a month. They had designed the site smart. They were
[01:59:24.040 --> 01:59:28.280]   already structured for accessibility, but that was a lot of work for their dev team,
[01:59:28.280 --> 01:59:31.560]   spending a lot of time keeping it updated to the standards because the standards change.
[01:59:31.560 --> 01:59:37.640]   So they added Userway as an extra layer of accessibility to make sure their browsing experience
[01:59:37.640 --> 01:59:42.440]   was accessible to everyone. That's what we've done. We make sure our site was accessible,
[01:59:42.440 --> 01:59:48.040]   but by adding that Userway button, we just make it a little bit nicer for people with accessibility
[01:59:48.040 --> 01:59:52.840]   issues. For years, Userway has been on the cutting edge, creating innovative technologies
[01:59:52.840 --> 01:59:58.760]   for accessibility that push the envelope of what's possible with AI and machine learning and computer
[01:59:58.760 --> 02:00:04.600]   vision. That's one of the ways they auto-generate alt tags for images. They actually have,
[02:00:04.600 --> 02:00:10.760]   they can write image descriptions for you with their computer vision. They remediate complex
[02:00:10.760 --> 02:00:15.560]   nav menus, ensure that all the pop-ups are accessible, fix vague link violations, fix any
[02:00:15.560 --> 02:00:21.080]   broken links. They can make sure that your website supports the full accessibility layer that all
[02:00:21.080 --> 02:00:26.760]   web browsers offer, including the use of accessible colors while remaining true to your brand.
[02:00:26.760 --> 02:00:31.640]   And you'll get a detailed report of everything that was fixed on your website. Works with everything,
[02:00:31.640 --> 02:00:39.160]   WordPress, yes, Shopify, Wix, Site Core, yes, SharePoint. And of course, with your own hand-coded
[02:00:39.160 --> 02:00:43.480]   site, that's how we're doing it. Let Userway help your business meet its compliance goals
[02:00:43.480 --> 02:00:49.240]   and improve the experience for your users. The Voice of Siri, Susan Bennett, has a message about
[02:00:49.240 --> 02:00:54.760]   Userway, sir. Hi, I'm Susan Bennett, the original Voice of Siri. You won't hear me say
[02:00:54.760 --> 02:00:59.880]   something like this too often. I'm sorry, I don't understand what you're looking for.
[02:00:59.880 --> 02:01:06.120]   But every day, that's what the Internet is like for millions of people with disabilities.
[02:01:06.120 --> 02:01:13.160]   Userway fixes all of that with just one line of code. It's pretty amazing. One line of code does
[02:01:13.160 --> 02:01:19.720]   it all. And our web engineer, Patrick, will testify to that. It was a very easy fix for him to make
[02:01:19.720 --> 02:01:25.000]   to our site. Userway can make any website fully accessible and ADA compliant with the
[02:01:25.000 --> 02:01:30.200]   userway. Everyone who visits your site can browse seamlessly, can customize it to fit your needs.
[02:01:30.200 --> 02:01:35.320]   It's a great way to show your brand's commitment to millions of people with disabilities. You want
[02:01:35.320 --> 02:01:41.160]   to make your site accessible. I know you do. Well, here's an easy way to do it. Userway.org/twit,
[02:01:41.160 --> 02:01:50.280]   you get 30% off Userway's AI powered accessibility solution. 30% off userway.org/twit,
[02:01:50.280 --> 02:01:56.680]   Userway making the Internet accessible for everyone. Userway.org/twit. If you get a chance,
[02:01:56.680 --> 02:02:01.480]   go to the Twit website and click that widget in the lower right. That's pretty amazing. Even,
[02:02:01.480 --> 02:02:08.600]   you know, I'm an old guy. I like the bigger text. I like that. You can move the widget.
[02:02:09.160 --> 02:02:14.520]   You can hide it. It's a great solution. Userway. We thank you for your support
[02:02:14.520 --> 02:02:18.840]   and for this great accessibility menu on our site. If you want to try it to see.
[02:02:18.840 --> 02:02:26.440]   A couple of final stories before, oh, before we go to that though, I forgot, we've got a
[02:02:26.440 --> 02:02:32.120]   great little mini video about this week on TwitWatch. There was a PEPcom event and in-person
[02:02:33.080 --> 02:02:40.120]   PEPcom event. It's actually great to the press. It is a group. Yes, yes. But first time I ever saw
[02:02:40.120 --> 02:02:45.720]   a chocolate fountain was at a PEPcom showcase. But it wasn't for sale or you would have. No,
[02:02:45.720 --> 02:02:50.200]   but you could dip anything into it and then eat it. It doesn't work with a cell phone. Don't
[02:02:50.200 --> 02:02:59.480]   worry. No, I tried. No. Previously on Twit, Tech News Weekly. NAP has a new gadget that
[02:02:59.480 --> 02:03:06.520]   it's announcing officially. It's not another pair of spectacles for your face. It's a camera in the
[02:03:06.520 --> 02:03:14.440]   sky. iOS today. Coming up on iOS today, Rosemary Orchard and I have a special guest because we are
[02:03:14.440 --> 02:03:20.520]   going all in on shortcuts. So what does that mean? Of course, it means Matthew Cassinelli joining us.
[02:03:21.560 --> 02:03:29.720]   All about Android. Android 13, beta one is out. This is the first beta of Android 13 that doesn't
[02:03:29.720 --> 02:03:37.080]   require you to jump through hurdles in order to install it. I did this and my device got white.
[02:03:37.080 --> 02:03:44.120]   Twit. Technology isn't always pretty, but we are. Okay, so don't sit. I'm glad he said that.
[02:03:44.120 --> 02:03:50.360]   I won't be installing Android 13 on my Pixel 6. Not just yet. I didn't finish all of the
[02:03:50.360 --> 02:03:59.240]   quarterly results. Meta, first quarter, pretty successful generated revenue of $695 million.
[02:03:59.240 --> 02:04:09.000]   Oh, I'm sorry. That's wrong. $27.2 billion. That makes more sense. But they did have a loss on their,
[02:04:09.000 --> 02:04:19.880]   they call it the Facebook reality labs, the meta verse of $10.2 billion on revenue of $2.3 billion.
[02:04:20.840 --> 02:04:25.880]   But they knew that they, in fact, they estimated it would lose about $10 billion a year for some time
[02:04:25.880 --> 02:04:32.040]   until the metaverse actually happened. But the stock market liked the results.
[02:04:32.040 --> 02:04:40.040]   Meta's stock went up. Meta employees, happy Amazon didn't have a great quarter. Microsoft
[02:04:40.040 --> 02:04:44.360]   did have a good quarter. Actually, the most interesting story from my point of view of Microsoft is
[02:04:44.360 --> 02:04:50.360]   about half of its revenue came from cloud. Microsoft has become a cloud company.
[02:04:50.360 --> 02:04:58.280]   Are you surprised, Tim, you've been covering the tech business for some years.
[02:04:58.280 --> 02:05:02.120]   Microsoft basically is pivoted into a cloud company.
[02:05:02.120 --> 02:05:07.960]   Yeah, a services company, for sure. And that's definitely been something that
[02:05:07.960 --> 02:05:12.760]   a lot of people were predicting that they would have to do a decade ago. And we're concerned about
[02:05:12.760 --> 02:05:16.440]   with the operating system revenue really declining and ultimately going away.
[02:05:16.440 --> 02:05:20.680]   How are they going to be able to continue any level of growth? And they've done a remarkable job
[02:05:20.680 --> 02:05:25.640]   in being able to maintain that going forward. I'm pretty impressed actually at how well they've
[02:05:25.640 --> 02:05:29.400]   managed to make that pivot because it's not an easy one. We've seen companies like IBM do it
[02:05:29.400 --> 02:05:35.400]   in the past with some degrees of success. And for Microsoft to go from a really a largely
[02:05:35.400 --> 02:05:39.640]   consumer facing company with some business stuff to really pivot to being hugely business
[02:05:39.640 --> 02:05:45.320]   services related. Yeah, it's impressive. Azure 49% growth in the quarter.
[02:05:45.320 --> 02:05:51.160]   Yeah, it is very impressive. The number of companies that have not made that kind of pivot
[02:05:51.160 --> 02:05:58.760]   huge, in fact, the number that have, it's very rare. So good job. I think such in a
[02:05:58.760 --> 02:06:05.320]   dollar there's a lot of credit for that. Hollywood's fight against VPNs turns ugly.
[02:06:07.000 --> 02:06:12.840]   Now film companies are accusing VPNs of enabling illegal activity. It's not just copyright
[02:06:12.840 --> 02:06:19.880]   fringeman anymore. I'm sure Corey has something to say about Hollywood's war against Express VPN.
[02:06:19.880 --> 02:06:26.920]   I mean, I've been involved with this fight for a long time. It doesn't surprise me. I think that,
[02:06:27.560 --> 02:06:37.880]   you know, the unwillingness to acknowledge how many powerful and useful things VPNs do and the
[02:06:37.880 --> 02:06:44.760]   insistence that VPN that there's some magical way that you can design a VPN that only lets the good
[02:06:44.760 --> 02:06:51.000]   content through and not the bad content is pretty nonsensical. And you know, we are now at the end
[02:06:51.000 --> 02:06:57.960]   of the the copyright experiment. And it's been pretty much, I think, definitively proved that
[02:06:57.960 --> 02:07:04.760]   the best way to fight piracy is to offer people content in useful wrappers that's easy to get
[02:07:04.760 --> 02:07:10.200]   at a reasonable price without DRM. Yeah, well, without DRM. And without, I mean,
[02:07:10.200 --> 02:07:13.720]   their big thing is release windows. They want to do regional release windows. I understand that.
[02:07:13.720 --> 02:07:18.440]   I have an American publisher, I have a British publisher. I know what that's like. But you know,
[02:07:18.440 --> 02:07:24.840]   like the expecting the entire world to arrange itself to your shareholders convenience. And when
[02:07:24.840 --> 02:07:31.160]   they fail to do so accusing them of felony contempt of business model, it's not an adult posture. It's
[02:07:31.160 --> 02:07:36.760]   it is a like, there's no word for it, but childish as a way of thinking about how the world should
[02:07:36.760 --> 02:07:42.440]   work. Oh, and now they're saying VPNs are a response for hacking, stalking bomb threats,
[02:07:42.440 --> 02:07:47.080]   political assassinations, child pornography, P2P. They said, they said,
[02:07:47.080 --> 02:07:52.760]   that's right. Download an MP3. Yeah. And it's going to have a it's going to have a virus in the MP3.
[02:07:52.760 --> 02:07:58.360]   It's like that's not how MP3s work. But you know, you do you buddy. And and you know,
[02:07:58.360 --> 02:08:04.280]   and then the virus in the MP3 would have your bank account hacked. You said an interesting thing,
[02:08:04.280 --> 02:08:08.360]   though, we are at the end of the copyright wars. Do you think that's the case that companies are
[02:08:08.360 --> 02:08:14.360]   realizing that? I know music industries abandoned it. The thing that we're fighting over these days
[02:08:14.360 --> 02:08:21.880]   is automated filters. That's what they really want. So they they just want YouTube style content
[02:08:21.880 --> 02:08:27.160]   ID filters for all platforms. And then they wanted to be set up so that like they feed the filter
[02:08:27.160 --> 02:08:31.240]   with the things that are prohibited. So no one else gets to feed it and block their stuff, but
[02:08:31.240 --> 02:08:35.960]   they get to block your stuff. And that what they get to do is replace all the limitations and
[02:08:35.960 --> 02:08:41.160]   exceptions to copyright, which are super important. And just as big a part of copyright as the
[02:08:41.160 --> 02:08:47.160]   exclusive rights part with a kind of fiat system where it's like it's only fair use if we say it is.
[02:08:47.160 --> 02:08:52.920]   And since the whole point of fair use is all the things you can do if they without permission,
[02:08:52.920 --> 02:08:58.680]   that just effectively eliminates fair use. And you know, it's part of the extremely selective
[02:08:58.680 --> 02:09:02.120]   view that the entertainment industry has always taken of copyright. You know, you see Disney
[02:09:02.120 --> 02:09:07.560]   Marvel now fighting the original Marvel creators, including the heirs of Stan Lee,
[02:09:07.560 --> 02:09:12.600]   who are trying to exercise a bedrock of American copyright, which is the termination right,
[02:09:12.600 --> 02:09:17.000]   which is after 30 years, you can claw back the rights to your work and you can you can sell
[02:09:17.000 --> 02:09:22.440]   them to someone else. And they're basically arguing that that they shouldn't have to respect copyright.
[02:09:22.440 --> 02:09:27.400]   It's it's never been about respecting artists rights. It's always been about finding ways to
[02:09:27.400 --> 02:09:34.760]   secure commercial advantage. So you're just go ahead. Oh, sorry. This is streaming kind of make the
[02:09:34.760 --> 02:09:41.640]   old fashioned file sharing era copyright arguments irrelevant because, you know, really you're
[02:09:41.640 --> 02:09:46.840]   talking about not user generated content like YouTube where there are questions about
[02:09:46.840 --> 02:09:52.200]   the rights, but you know, it's just a package of content that Netflix gives you. They've licensed
[02:09:52.200 --> 02:09:58.440]   it or created it. And practically speaking, no one's trying to like grab it and download it.
[02:09:58.440 --> 02:10:04.600]   It's too much. Yeah, well, it's just. I mean, everyone can who can't afford Netflix has an
[02:10:04.600 --> 02:10:09.480]   Netflix account. And you know, we talked earlier about how some people are, you know, looking at
[02:10:09.480 --> 02:10:13.480]   the number of accounts they have every month and going, Oh, no, one wants to offer a bundle. And
[02:10:13.480 --> 02:10:16.920]   Disney decided to do this thing where they took everything out of Netflix and whatever. And then
[02:10:16.920 --> 02:10:21.000]   they bought all the media companies. So now I have to have D plus and it's all kind of a mess.
[02:10:21.000 --> 02:10:26.360]   And you know, like the the single biggest thing that drives people to password sharing and and
[02:10:26.360 --> 02:10:34.120]   downloading on torrents is when the packages available aren't well arrayed and convenient and
[02:10:34.120 --> 02:10:37.720]   efficient for them. And as soon as it's convenient and efficient and well arrayed, most of those
[02:10:37.720 --> 02:10:41.160]   people give it up. And the ones who don't are super price sensitive. And they weren't going to
[02:10:41.160 --> 02:10:47.800]   give you any money anyway. I think that's right. I think that's right. Which is one of the reasons
[02:10:47.800 --> 02:10:53.640]   I think Netflix is smart with their password sharing strategy, which is instead of saying to
[02:10:53.640 --> 02:10:58.840]   the person who isn't buying Netflix, but is using my password, we're going to cut you off. They just
[02:10:58.840 --> 02:11:05.720]   say to me, it's a couple extra bucks. Just pay it. Pay for your kids. Come on. And that's a, you know,
[02:11:05.720 --> 02:11:12.280]   that's not a hard thing to exceed too. And it's I think a much smarter way of dealing with the problem.
[02:11:12.280 --> 02:11:18.600]   Yeah. Final story, because I know we want to get everybody out of here on time,
[02:11:18.600 --> 02:11:25.000]   but I do want to give a pat on the back to our good friend and regular contributor, Christina Warren,
[02:11:25.000 --> 02:11:31.880]   featured on NPR this week, because she apparently collects memory of Billy from defunct companies.
[02:11:31.880 --> 02:11:36.680]   Companies that hardly ever made it movie pass. There she is in a movie pass t-shirt. She recently
[02:11:36.680 --> 02:11:43.320]   got a pop socket for CNN plus. She's looking for the Holy Grail, though, if you can help her out,
[02:11:43.320 --> 02:11:49.480]   which is any Theranos swag at all. She would very much like that there. She is wearing a fire
[02:11:49.480 --> 02:11:57.880]   festival shirt. She's still pursuing her so-called white whale, which is Theranos. Anybody get,
[02:11:57.880 --> 02:12:04.040]   oh, there's Enron and Enron pad. Where'd you get that, Corey? That's my friend Christopher Brown,
[02:12:04.040 --> 02:12:10.520]   the Austin science fiction writer and lawyer who one day sent me like a letter or a package or
[02:12:10.520 --> 02:12:16.200]   something. And in it was a pad of Enron paper. So that's real. That's not, that's not. Oh, yeah.
[02:12:16.200 --> 02:12:21.240]   No, it's actual Enron stationery. Can't use that whenever I write to my congressman,
[02:12:21.240 --> 02:12:25.560]   I write to Enron stationery. They listen, they sit up and take notice. Oh,
[02:12:25.560 --> 02:12:32.360]   you know that Elizabeth Holmes father worked at Enron? No. Oh, that's amazing.
[02:12:32.360 --> 02:12:37.080]   Well, it all comes around, doesn't it? Oh, and thank you so much for being here. Protocol,
[02:12:37.080 --> 02:12:45.000]   great publication is part of my regular daily news beat check protocol.com senior editor.
[02:12:45.720 --> 02:12:48.920]   And you don't charge a thing. I don't understand it, but
[02:12:48.920 --> 02:12:55.160]   they don't even have ads, do you? Oh, no, we do. Oh, you do. Maybe you've got a blocker.
[02:12:55.160 --> 02:13:02.040]   Oh, let me turn that ad blocker off for you. We do appreciate your time and signing up for
[02:13:02.040 --> 02:13:08.920]   our fine, fine newsletters, including FinTech, which I oversee in source code, which I occasionally
[02:13:08.920 --> 02:13:13.320]   contribute to. There you go. Yeah, you may have ads, but they're not intrusive. I can,
[02:13:13.960 --> 02:13:17.320]   it's a really great publication. And of course, that's where Megan Moroni works.
[02:13:17.320 --> 02:13:23.160]   So we'd like it for that reason to thank you, Owen. Great to have you on our stage this week.
[02:13:23.160 --> 02:13:28.280]   My pleasure. Also, Tim Stevens. He's the car guy, the editor in chief at CNET Cars.
[02:13:28.280 --> 02:13:34.200]   Anything going on at CNET Cars? You want to talk about plug? Yeah, actually, I'm heading down to
[02:13:34.200 --> 02:13:38.360]   Texas on Tuesday this week to drive the F-150 Lightning for the first time. So we'll be driving
[02:13:38.360 --> 02:13:43.560]   Ford's first EV truck. The impressions are going to be under barber for a little while, but we'll
[02:13:43.560 --> 02:13:46.440]   have that up on the side to the next couple of weeks so you can see what it's like to actually
[02:13:46.440 --> 02:13:50.280]   drive and even actually tell us some things with Ford's electric truck. First drive,
[02:13:50.280 --> 02:13:57.000]   I know President Biden got to drive it, but that's the only person ever. Yeah, he got a heck of
[02:13:57.000 --> 02:14:00.200]   an exclusive there. So, crude us to him. He's got good journalism jobs, obviously.
[02:14:00.200 --> 02:14:05.400]   I hope he was in bar and voting. I hope he had signed the DNA. No, I'm very interested in that.
[02:14:05.400 --> 02:14:10.440]   In fact, I think Jerry, our marketing director, our CMO, has put an order in.
[02:14:11.000 --> 02:14:14.200]   But now if you're ordering now, are they going to start shipping soon?
[02:14:14.200 --> 02:14:18.520]   They are. They're in production. In fact, they've just increased production. They just finally
[02:14:18.520 --> 02:14:22.440]   give us the actual final horsepower figures and everything else. So they are in the factory coming
[02:14:22.440 --> 02:14:26.120]   out now. But if you haven't put your order in now, you certainly won't get one until next year.
[02:14:26.120 --> 02:14:29.720]   He's waiting till next year. That's the truck that you can plug your house into.
[02:14:29.720 --> 02:14:33.960]   I can't. Or lots of other things too. Nice feature.
[02:14:33.960 --> 02:14:41.240]   CNET.com/cars. Right? You got it. All right. Thanks for being here, Tim.
[02:14:41.240 --> 02:14:47.320]   Corey Doctorow, science fiction author, blogger. His blog is now at pluralistic.net.
[02:14:47.320 --> 02:14:52.440]   Highly recommend subscribing to the newsletter. It's a must read. And if you want to know more
[02:14:52.440 --> 02:14:57.560]   about his book, Craphound.com is the place to go. I hope your hips are feeling better.
[02:14:57.560 --> 02:15:03.080]   Anything you want to mention, by the way, regular user amasted on.
[02:15:03.080 --> 02:15:07.000]   You just did like a 80s tweet thread or two thread, I should say.
[02:15:07.000 --> 02:15:13.480]   I do my all of the things that I post on pluralistic also show up at the same time as a Twitter thread.
[02:15:13.480 --> 02:15:18.920]   I'm asked it on thread. A Tumblr post, a Medium post, a Discourse post, an RSS feed,
[02:15:18.920 --> 02:15:23.000]   and a newsletter. So they all go. How do you do that? What do you use to do that?
[02:15:23.000 --> 02:15:24.680]   Manually. No.
[02:15:24.680 --> 02:15:31.960]   You know that scene in Batman, one of the Batman movies where the Penguin is being played by Danny
[02:15:31.960 --> 02:15:36.600]   DeVito. And he accuses Christopher Walken of having done something bad. And Christopher Walken
[02:15:36.600 --> 02:15:40.520]   says, I couldn't have possibly done it. And if I had, I would have shredded the evidence.
[02:15:40.520 --> 02:15:44.920]   And the Penguin pulls out the sheet of paper that's just got thousands of pieces of tape on it.
[02:15:44.920 --> 02:15:50.600]   And he says, all it took was a little patience and a lot of tape. That's kind of how my workflow
[02:15:51.960 --> 02:16:01.960]   holy cow pluralistic. He's on the mamat de France. server mamot.fr here is right here.
[02:16:01.960 --> 02:16:06.280]   Great picture of you too. That's great. Corey. And is that these tubes? What are they? What
[02:16:06.280 --> 02:16:10.040]   is that a picture of? That's from the computer history museum without the last out of town trip
[02:16:10.040 --> 02:16:14.600]   I took before lockdown. Nice. Was to the computer history museum. And I got to visit their secret
[02:16:14.600 --> 02:16:19.080]   warehouse full of incredibly valuable weird old junk. There's a lot of good pictures of it in my
[02:16:19.080 --> 02:16:22.680]   Flickr stream all CC by so you should check it out. I shall check it out. It isn't commercial if you
[02:16:22.680 --> 02:16:27.800]   need to illustrate something. Corey, thank you so much for joining us. Thanks. Owen. Thanks. Tim,
[02:16:27.800 --> 02:16:33.000]   always great to have all three of you on. We do Twitch every Sunday round about two Pacific
[02:16:33.000 --> 02:16:41.560]   230 530 Eastern. That would be 2130 UTC. So you can watch us live if you want at live.tv. There's
[02:16:41.560 --> 02:16:46.680]   also live audio streams there. If you're watching live chat with us live at our free
[02:16:47.480 --> 02:16:55.720]   community run IRC server IRC.twit.tv. You just a browser will work. But if you have an IRC client,
[02:16:55.720 --> 02:16:59.560]   that's even better. Of course, you can always chat with our Discord with all the other club
[02:16:59.560 --> 02:17:04.200]   Twit members if you're a member of club Twit. After the fact on demand versions of this show
[02:17:04.200 --> 02:17:09.960]   available at Twit.tv. There's a YouTube channel and you can also subscribe in your favorite podcast
[02:17:09.960 --> 02:17:15.560]   client. Please leave us a five star review. Let the world know about one of the longest running
[02:17:16.440 --> 02:17:25.160]   technology shows now in our 18th year. That's a full grown podcast. Thanks for joining us. We'll
[02:17:25.160 --> 02:17:27.560]   see you next time. Another Twitch. See you in the can.
[02:17:27.560 --> 02:17:38.840]   [Music]

