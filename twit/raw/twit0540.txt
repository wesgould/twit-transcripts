;FFMETADATA1
title=Filling the Room With Noble Gases
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=540
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2015
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.240]   It's time for Twit, this week in Tech.
[00:00:03.240 --> 00:00:06.600]   Becky Warley joins Dr. Kiki and Tom Merritt.
[00:00:06.600 --> 00:00:08.280]   It's an old-timers get-together.
[00:00:08.280 --> 00:00:10.320]   We're gonna have a lot of fun talking about the tech news,
[00:00:10.320 --> 00:00:13.080]   what's happening at Yahoo, Dow, DuPont,
[00:00:13.080 --> 00:00:16.320]   and you'll find out why I'm wearing a onesie.
[00:00:16.320 --> 00:00:17.760]   Yes, it's an adult onesie.
[00:00:17.760 --> 00:00:19.120]   You got a problem with that?
[00:00:19.120 --> 00:00:20.320]   Twit is next.
[00:00:20.320 --> 00:00:24.960]   Net-cast you love.
[00:00:24.960 --> 00:00:26.400]   - Front people you trust.
[00:00:26.400 --> 00:00:30.040]   ♪
[00:00:30.040 --> 00:00:32.440]   This is Twit.
[00:00:32.440 --> 00:00:35.720]   Bandwidth for this weekend tech is provided by CashFly
[00:00:35.720 --> 00:00:38.920]   at C-A-C-H-E-F-L-Y.com.
[00:00:38.920 --> 00:00:43.720]   ♪
[00:00:43.720 --> 00:00:45.880]   This is Twit this week at Tech,
[00:00:45.880 --> 00:00:51.360]   Episode 540, recorded Sunday, December 13, 2015,
[00:00:51.360 --> 00:00:54.600]   filling the room with noble gases.
[00:00:54.600 --> 00:00:57.640]   This week in Tech is brought to you by Bowlandbranch.com,
[00:00:57.640 --> 00:01:00.000]   the most comfortable sheets on the planet.
[00:01:00.000 --> 00:01:01.960]   If you order right now, they'll give you 20% off
[00:01:01.960 --> 00:01:04.280]   your entire order, plus free shipping.
[00:01:04.280 --> 00:01:08.840]   Just go to B-O-L-L-N-B-R-A-N-C-H.com
[00:01:08.840 --> 00:01:11.080]   and use the offer code Twit.
[00:01:11.080 --> 00:01:13.240]   And by Squarespace.
[00:01:13.240 --> 00:01:16.040]   Squarespace is the simplest way to create a beautiful
[00:01:16.040 --> 00:01:19.560]   website or cover page for your business, portfolio,
[00:01:19.560 --> 00:01:21.840]   or online store, and our offer code Twit
[00:01:21.840 --> 00:01:24.120]   and get 10% off Squarespace.
[00:01:24.120 --> 00:01:25.760]   Build it beautiful.
[00:01:25.760 --> 00:01:27.320]   And by Harry's.
[00:01:27.320 --> 00:01:29.080]   For guys who want a great shave experience
[00:01:29.080 --> 00:01:30.880]   for a fraction of what you're paying now,
[00:01:30.880 --> 00:01:34.480]   go to harries.com, get $5 off your first purchase
[00:01:34.480 --> 00:01:38.160]   by entering the code Twit 5 when you check out.
[00:01:38.160 --> 00:01:42.120]   And by Casper, an online retailer of premium mattresses
[00:01:42.120 --> 00:01:44.000]   for a fraction of the price,
[00:01:44.000 --> 00:01:46.240]   because everyone deserves a great night's sleep.
[00:01:46.240 --> 00:01:48.760]   Get $50 off any mattress purchase by visiting
[00:01:48.760 --> 00:01:53.120]   casper.com/twit, and into the promo code Twit.
[00:01:53.120 --> 00:01:55.040]   (upbeat music)
[00:01:55.040 --> 00:01:56.240]   It's time for Twit this week at Tech
[00:01:56.240 --> 00:01:58.160]   the show where we cover the week's Tech News.
[00:01:58.160 --> 00:02:02.040]   This is gonna be, I don't even know what this is gonna be.
[00:02:02.040 --> 00:02:06.560]   It's gonna be crazy because it's really three,
[00:02:06.560 --> 00:02:09.840]   well I'll include myself, four good old friends
[00:02:09.840 --> 00:02:12.440]   brought back together for a family reunion.
[00:02:12.440 --> 00:02:15.360]   And if you didn't have enough on Thanksgiving,
[00:02:15.360 --> 00:02:16.640]   you're ready now.
[00:02:16.640 --> 00:02:19.040]   Becky Worley in studio, great to see you.
[00:02:19.040 --> 00:02:20.440]   - So glad to be here.
[00:02:20.440 --> 00:02:22.240]   - I've seen you recently because we did that
[00:02:22.240 --> 00:02:24.160]   good morning America, thanks to you.
[00:02:24.160 --> 00:02:25.320]   - Oh, thanks to you.
[00:02:25.320 --> 00:02:27.720]   40 hours of good morning America, they said,
[00:02:27.720 --> 00:02:29.440]   and they said, why don't you just take two of them back?
[00:02:29.440 --> 00:02:30.520]   And I said, huh?
[00:02:30.520 --> 00:02:32.440]   - It was their 40th anniversary.
[00:02:32.440 --> 00:02:34.360]   Was any of it on TV or was all streamed?
[00:02:34.360 --> 00:02:37.160]   - It was almost all streamed and then they picked snippets
[00:02:37.160 --> 00:02:42.160]   and we did the 1 a.m. Tuesday night hour here.
[00:02:42.160 --> 00:02:44.360]   - That's real confidence in all of us.
[00:02:44.360 --> 00:02:46.760]   - Yes, and it was 1 a.m. East Coast time.
[00:02:46.760 --> 00:02:48.360]   - Right, and luckily it was only 10 here,
[00:02:48.360 --> 00:02:49.600]   but I will tell you I was back there
[00:02:49.600 --> 00:02:51.560]   and I saw the engineers and they all hugged me
[00:02:51.560 --> 00:02:53.800]   and they said, thank you so much to you and Leo,
[00:02:53.800 --> 00:02:57.360]   everybody there, because because of you, we got to pee.
[00:02:57.360 --> 00:02:59.720]   We were the only hour they could actually get up
[00:02:59.720 --> 00:03:01.680]   and walk away and not have to.
[00:03:01.680 --> 00:03:03.080]   - You don't know how often I hear that.
[00:03:03.080 --> 00:03:05.680]   - They could just, because of you.
[00:03:05.680 --> 00:03:08.400]   We got to pee, we were so low maintenance for them
[00:03:08.400 --> 00:03:10.440]   'cause it was all self-contained here at Twit,
[00:03:10.440 --> 00:03:11.280]   they loved it.
[00:03:11.280 --> 00:03:12.720]   - We're standing here in the studio watching
[00:03:12.720 --> 00:03:14.120]   as they're coming up to our time
[00:03:14.120 --> 00:03:15.960]   and they were doing some sort of barbecue thing.
[00:03:15.960 --> 00:03:17.760]   - Oh yeah, remember the, oh.
[00:03:17.760 --> 00:03:18.880]   - And then there was a chicken.
[00:03:18.880 --> 00:03:20.640]   I remember there was a chicken and.
[00:03:20.640 --> 00:03:22.480]   - They were doing a segment on E. coli
[00:03:22.480 --> 00:03:23.600]   and healthy food handling.
[00:03:23.600 --> 00:03:25.120]   - And some woman was cutting up a chicken
[00:03:25.120 --> 00:03:26.600]   like she was really into it.
[00:03:26.600 --> 00:03:27.680]   - I know.
[00:03:27.680 --> 00:03:28.920]   Well that's the other thing is then I went
[00:03:28.920 --> 00:03:30.920]   to the production services, people to talk to them
[00:03:30.920 --> 00:03:33.440]   and thank them and they had a picture up on their board
[00:03:33.440 --> 00:03:37.640]   of the chicken as the worst moment of the whole 40 hours.
[00:03:37.640 --> 00:03:39.480]   - It was shaky cam, it was like somebody was doing it,
[00:03:39.480 --> 00:03:41.240]   vertical video on an iPhone.
[00:03:41.240 --> 00:03:42.640]   I thought this is network television,
[00:03:42.640 --> 00:03:43.920]   this is what we've come to.
[00:03:43.920 --> 00:03:45.400]   - Hey, speaking of network television,
[00:03:45.400 --> 00:03:47.560]   also with us, I don't know what that segue means.
[00:03:47.560 --> 00:03:50.360]   Tom Merritt from DTMS.
[00:03:50.360 --> 00:03:52.360]   - Someone who's not doing any network television.
[00:03:52.360 --> 00:03:53.920]   (laughing)
[00:03:53.920 --> 00:03:56.360]   - Dailytechnewshow.com.
[00:03:56.360 --> 00:03:57.560]   Oh, love seeing you again.
[00:03:57.560 --> 00:04:00.920]   - Wait, did you say DTMS stands for Daily Tech News Show?
[00:04:00.920 --> 00:04:02.320]   - TTM, that's inventive.
[00:04:02.320 --> 00:04:04.680]   - I have seen you posting all this stuff about DWTS.
[00:04:04.680 --> 00:04:06.880]   I thought you were on Dancing with the Stars.
[00:04:06.880 --> 00:04:08.200]   - Aw man, no.
[00:04:08.200 --> 00:04:09.440]   - It's a little bit of click bait.
[00:04:09.440 --> 00:04:10.440]   - Oh, how does he dance?
[00:04:10.440 --> 00:04:11.440]   - How do I know?
[00:04:11.440 --> 00:04:13.880]   - During the daily, no, I leave that to Brian Brushwood.
[00:04:13.880 --> 00:04:15.480]   - Yeah, he's a dancer.
[00:04:15.480 --> 00:04:18.200]   - Still in Aliya Yacht-esque, I see.
[00:04:18.200 --> 00:04:20.200]   - Ah yeah, yeah, which is now just,
[00:04:20.200 --> 00:04:21.280]   it's not just a Warcraft guild,
[00:04:21.280 --> 00:04:25.040]   it's like a gaming guild, just for kicks, for fun.
[00:04:25.040 --> 00:04:26.480]   - For any of this stuff, I see your,
[00:04:26.480 --> 00:04:27.560]   I'm looking at your mantle.
[00:04:27.560 --> 00:04:29.480]   I see your, yeah, you're just here.
[00:04:29.480 --> 00:04:31.360]   Just see your podcast award up there.
[00:04:31.360 --> 00:04:34.160]   And what is that, your family from 1880?
[00:04:34.160 --> 00:04:36.640]   - That is, that's my great grandpa John.
[00:04:36.640 --> 00:04:38.680]   - Wow, that is cool.
[00:04:38.680 --> 00:04:39.520]   - Yeah.
[00:04:39.520 --> 00:04:40.360]   - That is really cool.
[00:04:40.360 --> 00:04:41.200]   - Well, nice to see you.
[00:04:41.200 --> 00:04:42.040]   - Yeah, good to see you.
[00:04:42.040 --> 00:04:42.880]   Thanks for having me back.
[00:04:42.880 --> 00:04:44.200]   - Always welcome.
[00:04:44.200 --> 00:04:45.040]   - Good to see you. - And crew.
[00:04:45.040 --> 00:04:47.640]   - Speaking of returns, many happy returns.
[00:04:47.640 --> 00:04:50.520]   Dr. Kiki is here, Kiki Sanford.
[00:04:50.520 --> 00:04:51.360]   - Hello.
[00:04:51.360 --> 00:04:53.640]   - For a twist this week in sciencetwist.org
[00:04:53.640 --> 00:04:55.880]   and broaderimpacts.tv
[00:04:55.880 --> 00:04:58.040]   and she's got her lab coat in the back.
[00:04:58.040 --> 00:04:59.880]   - That's right, two lab coats.
[00:04:59.880 --> 00:05:02.520]   - Is that set dressing or do you wear that?
[00:05:02.520 --> 00:05:05.120]   - Well, if I'm doing experiments,
[00:05:05.120 --> 00:05:07.440]   I have to have, protect my clothes,
[00:05:07.440 --> 00:05:09.480]   but very often it's just set dressing.
[00:05:09.480 --> 00:05:11.480]   - Nice to have lab coats around, just in case.
[00:05:11.480 --> 00:05:14.680]   I've got goggles, I've got some scales back here.
[00:05:14.680 --> 00:05:15.520]   - What's that?
[00:05:15.520 --> 00:05:16.360]   - I've got all sorts of stuff.
[00:05:16.360 --> 00:05:17.520]   - Since we've seen it,
[00:05:17.520 --> 00:05:20.000]   so the last you moved to Portland, Oregon.
[00:05:20.000 --> 00:05:21.520]   - Yes, I have.
[00:05:21.520 --> 00:05:23.120]   The home of the Happy Hipster.
[00:05:23.120 --> 00:05:25.120]   - Are you the Happy Hipster?
[00:05:25.120 --> 00:05:26.120]   - Happy to keep it weird.
[00:05:26.120 --> 00:05:26.960]   - Okay.
[00:05:26.960 --> 00:05:27.800]   (laughs)
[00:05:27.800 --> 00:05:28.640]   Nice to see you.
[00:05:28.640 --> 00:05:30.680]   It's great to have this panel.
[00:05:30.680 --> 00:05:32.960]   Just, I've been looking forward to this for months.
[00:05:32.960 --> 00:05:33.800]   It's really fun.
[00:05:33.800 --> 00:05:36.240]   Of course, Becky being the troublemaker
[00:05:36.240 --> 00:05:38.200]   that she has brought wine.
[00:05:38.200 --> 00:05:40.560]   - Yeah, that makes it for an interesting show.
[00:05:40.560 --> 00:05:41.400]   - I have no idea.
[00:05:41.400 --> 00:05:44.480]   - I've been working on other stuff this week,
[00:05:44.480 --> 00:05:46.840]   so I don't really know what's going on in the tech world.
[00:05:46.840 --> 00:05:49.680]   So that's a good sign I should bring wine.
[00:05:49.680 --> 00:05:51.120]   - I don't know what we're gonna talk about,
[00:05:51.120 --> 00:05:52.840]   but it's gonna be interesting.
[00:05:52.840 --> 00:05:55.120]   - What happened in this week in tech?
[00:05:55.120 --> 00:05:56.560]   I have no idea.
[00:05:56.560 --> 00:05:58.360]   - I'll tell you,
[00:05:58.360 --> 00:06:00.120]   one of the things we were talking about all week
[00:06:00.120 --> 00:06:01.800]   was Satoshi Nakamoto.
[00:06:01.800 --> 00:06:03.040]   - Oh yes, Bitcoin man.
[00:06:03.040 --> 00:06:06.360]   - The name you should remember from Bitcoin,
[00:06:06.360 --> 00:06:08.520]   we don't know if it's a person or a group of people,
[00:06:08.520 --> 00:06:10.240]   we don't really know anything about him.
[00:06:10.240 --> 00:06:13.000]   Newsweek very famously a couple of years ago,
[00:06:13.000 --> 00:06:15.160]   announced on its cover,
[00:06:15.160 --> 00:06:17.640]   they'd discovered it was some poor Japanese guy
[00:06:17.640 --> 00:06:20.840]   in Southern California didn't know anything about anything.
[00:06:20.840 --> 00:06:21.960]   He loved model trains.
[00:06:21.960 --> 00:06:25.240]   That guy was like, not even close.
[00:06:25.240 --> 00:06:26.760]   So this week on Tuesday,
[00:06:26.760 --> 00:06:30.440]   Wired magazine and Gizmodo both revealed
[00:06:30.440 --> 00:06:32.120]   that they had been in contact with a hacker
[00:06:32.120 --> 00:06:36.000]   who had provided them with significant number of documents.
[00:06:36.000 --> 00:06:37.960]   Convincingly, and if you read the article
[00:06:37.960 --> 00:06:42.480]   very convincingly pointing to an Australian
[00:06:42.480 --> 00:06:46.440]   as Satoshi Nakamoto,
[00:06:46.440 --> 00:06:50.480]   the documents were provided apparently,
[00:06:50.480 --> 00:06:53.560]   well, Wired magazine said by an unnamed source,
[00:06:53.560 --> 00:06:56.260]   Gizmodo said by a hacker,
[00:06:56.260 --> 00:07:00.760]   and they both pointed to Craig S. Wright.
[00:07:00.760 --> 00:07:03.920]   An Australian who at least has the unlike Newsweek's pick,
[00:07:03.920 --> 00:07:06.760]   at least has the credentials to perhaps
[00:07:06.760 --> 00:07:09.480]   be the creator of Bitcoin.
[00:07:09.480 --> 00:07:12.560]   Bitcoin of course requires great mathematical
[00:07:12.560 --> 00:07:14.040]   and crypto abilities.
[00:07:14.040 --> 00:07:17.480]   And allegedly $4 million worth of Bitcoin.
[00:07:17.480 --> 00:07:18.880]   Well, and of course, that's probably,
[00:07:18.880 --> 00:07:20.640]   there's many reasons why people would like to know
[00:07:20.640 --> 00:07:21.480]   who Satoshi is,
[00:07:21.480 --> 00:07:24.640]   but one of the reasons is something like 1.1 million
[00:07:24.640 --> 00:07:28.440]   Bitcoins are held in an account believed to be Nakamoto's.
[00:07:28.440 --> 00:07:30.320]   No one has touched, moved,
[00:07:30.320 --> 00:07:32.680]   or anyway, accessed that account in years.
[00:07:33.800 --> 00:07:37.040]   When you create a new cryptocurrency, it turns out,
[00:07:37.040 --> 00:07:40.960]   as the creator, you have the opportunity to amass
[00:07:40.960 --> 00:07:43.160]   quite a significant number of those.
[00:07:43.160 --> 00:07:46.000]   And should the currency take off as Bitcoin has,
[00:07:46.000 --> 00:07:48.880]   that cash of coins could be quite a bit.
[00:07:48.880 --> 00:07:52.040]   A million coins at $400 a coin
[00:07:52.040 --> 00:07:55.320]   is not an insignificant amount of money.
[00:07:55.320 --> 00:07:58.280]   However, and this is a motherboard article,
[00:07:58.280 --> 00:07:59.120]   but a number of others,
[00:07:59.120 --> 00:08:00.760]   including Gizmodo itself,
[00:08:00.760 --> 00:08:04.960]   are saying this might be a hoax.
[00:08:04.960 --> 00:08:08.000]   Well, haven't there been 16 alleged Bitcoin creators
[00:08:08.000 --> 00:08:09.320]   named so far?
[00:08:09.320 --> 00:08:10.880]   I think this is the 16th, right?
[00:08:10.880 --> 00:08:11.720]   Yeah.
[00:08:11.720 --> 00:08:17.600]   And an account thought to be belonged to Satoshi Nakamoto,
[00:08:17.600 --> 00:08:19.800]   but could very well have been hacked since,
[00:08:19.800 --> 00:08:21.920]   said that's not Satoshi.
[00:08:21.920 --> 00:08:23.440]   I'm Satoshi.
[00:08:23.440 --> 00:08:25.440]   This guy has been apparently going around saying
[00:08:25.440 --> 00:08:28.600]   he was Satoshi Nakamoto for some time.
[00:08:28.600 --> 00:08:32.320]   And it appears, although I don't know what the answer is,
[00:08:32.320 --> 00:08:34.600]   it appears that he maybe was the hacker
[00:08:34.600 --> 00:08:37.480]   who provided the leaked documents himself,
[00:08:37.480 --> 00:08:39.320]   the PGP key,
[00:08:39.320 --> 00:08:43.000]   which would be a very convincing piece of evidence,
[00:08:43.000 --> 00:08:44.440]   if it were true,
[00:08:44.440 --> 00:08:47.240]   that was supposedly matched.
[00:08:47.240 --> 00:08:50.100]   Nakamoto's is apparently forged and back-stated.
[00:08:50.100 --> 00:08:55.640]   But this guy is supposedly, he paid out
[00:08:55.640 --> 00:08:58.480]   millions and millions, 80 some million in Bitcoin
[00:08:58.480 --> 00:09:00.120]   to someone else, right?
[00:09:00.120 --> 00:09:01.240]   And that's one of the reasons.
[00:09:01.240 --> 00:09:02.080]   What's the coupon?
[00:09:02.080 --> 00:09:02.920]   He created a trust.
[00:09:02.920 --> 00:09:04.680]   He created a 1.1 million Bitcoin trust
[00:09:04.680 --> 00:09:06.200]   with his partner, Cleman,
[00:09:06.200 --> 00:09:07.040]   which is why everyone's like,
[00:09:07.040 --> 00:09:08.600]   hey, that's 1.1 million Bitcoins.
[00:09:08.600 --> 00:09:09.800]   That's the amount on the blockchain
[00:09:09.800 --> 00:09:11.000]   we don't know who's it is.
[00:09:11.000 --> 00:09:12.680]   And we think it might be Satoshi's.
[00:09:12.680 --> 00:09:16.440]   So it's coincidental evidence.
[00:09:16.440 --> 00:09:17.960]   Can I ask a dumb question?
[00:09:17.960 --> 00:09:20.640]   Is this just fun to try and figure out who this guy is?
[00:09:20.640 --> 00:09:23.280]   Or is there something deeper, more meaningful,
[00:09:23.280 --> 00:09:24.800]   more important that I'm missing?
[00:09:25.640 --> 00:09:27.800]   - Tom. - I think it's just fun.
[00:09:27.800 --> 00:09:29.960]   - Okay. - I don't think there's any,
[00:09:29.960 --> 00:09:32.360]   I mean, I think we all are curious
[00:09:32.360 --> 00:09:34.600]   whether Satoshi Nakamoto is a guy,
[00:09:34.600 --> 00:09:36.680]   a woman, a group of people,
[00:09:36.680 --> 00:09:37.640]   like we all wanna know
[00:09:37.640 --> 00:09:40.000]   because it's such a brilliant invention.
[00:09:40.000 --> 00:09:42.200]   I think that's just human nature, right?
[00:09:42.200 --> 00:09:44.440]   But I don't know that it matters
[00:09:44.440 --> 00:09:47.360]   other than who owns that 1.1 million block of Bitcoins,
[00:09:47.360 --> 00:09:49.720]   which could turn out not to be Satoshi Nakamoto.
[00:09:49.720 --> 00:09:50.840]   We don't know who owns that block.
[00:09:50.840 --> 00:09:52.760]   - I think we should do a cereal.
[00:09:52.760 --> 00:09:53.920]   - Oh yeah. - Yeah.
[00:09:53.920 --> 00:09:55.560]   - In search of Satoshi.
[00:09:55.560 --> 00:09:57.800]   - Season three. - Season three, right?
[00:09:57.800 --> 00:09:58.880]   - Yeah. - Yeah.
[00:09:58.880 --> 00:10:00.560]   - Why wouldn't you come forward?
[00:10:00.560 --> 00:10:03.480]   - Well, this guy immediately was arrested.
[00:10:03.480 --> 00:10:05.440]   - Well, he would know he was arrested beforehand.
[00:10:05.440 --> 00:10:07.200]   - Well, the Australian, it's,
[00:10:07.200 --> 00:10:08.440]   he was obviously coincidental.
[00:10:08.440 --> 00:10:09.520]   He was in trouble.
[00:10:09.520 --> 00:10:12.800]   The Australian tax authority say it has nothing to do with it,
[00:10:12.800 --> 00:10:14.880]   but they did raid his house that day.
[00:10:14.880 --> 00:10:18.160]   I don't know if it has nothing to do with it.
[00:10:18.160 --> 00:10:22.640]   If he really is the owner of the 1.1 million Bitcoin trove,
[00:10:23.600 --> 00:10:26.680]   I think that's more than academic interest, right?
[00:10:26.680 --> 00:10:30.960]   - Is the concern that he would then be trying,
[00:10:30.960 --> 00:10:33.280]   that he's someone, we wanna know who this person is
[00:10:33.280 --> 00:10:35.240]   because they may have some undue influence
[00:10:35.240 --> 00:10:36.440]   over the value of Bitcoin.
[00:10:36.440 --> 00:10:38.040]   Therefore, it's important to know who they are.
[00:10:38.040 --> 00:10:40.600]   - I don't think so, just that that guy's worth 40 million dollars.
[00:10:40.600 --> 00:10:41.760]   - Oh, come on.
[00:10:41.760 --> 00:10:43.760]   - I just love that. - It's too different.
[00:10:43.760 --> 00:10:45.080]   - To me, it's two different questions.
[00:10:45.080 --> 00:10:48.440]   Like, who invented Bitcoin is an academic question
[00:10:48.440 --> 00:10:49.440]   because that's here it is.
[00:10:49.440 --> 00:10:51.760]   - In and of itself, right. - So it owns 1.1 million Bitcoins
[00:10:51.760 --> 00:10:55.400]   is an influence question because, well, that's a lot of Bitcoins
[00:10:55.400 --> 00:10:56.720]   that could get thrown around by someone.
[00:10:56.720 --> 00:10:59.120]   - I think it would be very dangerous
[00:10:59.120 --> 00:11:00.560]   if you were the real Satoshi.
[00:11:00.560 --> 00:11:03.840]   I completely understand why he would prefer to stay anonymous.
[00:11:03.840 --> 00:11:06.440]   If you were real Satoshi, you really probably don't want
[00:11:06.440 --> 00:11:07.920]   the world to know who you are merely
[00:11:07.920 --> 00:11:10.920]   because you have a lot of money, you're fairly notorious,
[00:11:10.920 --> 00:11:14.120]   you may be subject to, I think part of the fear is that--
[00:11:14.120 --> 00:11:16.000]   - Sounds like you're just describing the Kardashians,
[00:11:16.000 --> 00:11:18.760]   so I don't quite understand that logic
[00:11:18.760 --> 00:11:20.320]   'cause they really wanna be known.
[00:11:20.320 --> 00:11:25.320]   - Part of the fear is that you would be hit,
[00:11:25.320 --> 00:11:27.360]   that you'd be taken out because, of course,
[00:11:27.360 --> 00:11:29.280]   people think that governments don't like the idea
[00:11:29.280 --> 00:11:31.520]   of this non-governmental currency
[00:11:31.520 --> 00:11:33.760]   and might do something to you.
[00:11:33.760 --> 00:11:37.240]   - Yeah, and there is a question as to whether
[00:11:37.240 --> 00:11:42.240]   this forging of the number, whether the key,
[00:11:42.240 --> 00:11:46.160]   whether or not it was to just basically get people
[00:11:46.160 --> 00:11:47.760]   going after Craig, I guess, right,
[00:11:47.760 --> 00:11:50.120]   that maybe somebody has something against him
[00:11:50.120 --> 00:11:50.960]   or somebody who's on.
[00:11:50.960 --> 00:11:52.960]   - I feel like it's, and we don't know,
[00:11:52.960 --> 00:11:55.480]   but I feel like it's far more likely that right
[00:11:55.480 --> 00:11:57.560]   is one of those guys who wants notoriety,
[00:11:57.560 --> 00:12:00.640]   honor and notoriety, and kind of set this whole thing up.
[00:12:00.640 --> 00:12:01.800]   He's been going around saying,
[00:12:01.800 --> 00:12:04.800]   "Hey, he's been kind of hinting, I'm not komodo."
[00:12:04.800 --> 00:12:05.720]   - And ladies.
[00:12:05.720 --> 00:12:07.040]   - Ladies.
[00:12:07.040 --> 00:12:08.120]   You know that big guy, and--
[00:12:08.120 --> 00:12:09.120]   (laughing)
[00:12:09.120 --> 00:12:10.040]   Would that work?
[00:12:10.040 --> 00:12:10.880]   - No.
[00:12:10.880 --> 00:12:12.560]   (laughing)
[00:12:12.560 --> 00:12:13.880]   - Hey ladies, what is he like?
[00:12:13.880 --> 00:12:15.560]   - Yeah, that would help.
[00:12:15.560 --> 00:12:17.520]   And Craig just being in the chat room says,
[00:12:17.520 --> 00:12:19.000]   maybe the reason why we wanna know
[00:12:19.000 --> 00:12:20.960]   is because people wanna keep an eye on him
[00:12:20.960 --> 00:12:22.440]   in case he has some sort of backdoor
[00:12:22.440 --> 00:12:25.680]   into the currency that nobody knows about.
[00:12:25.680 --> 00:12:28.120]   - That's unlikely because it's open source,
[00:12:28.120 --> 00:12:31.240]   it's well known how it works.
[00:12:31.240 --> 00:12:32.320]   - Yeah.
[00:12:32.320 --> 00:12:34.320]   - And blockchain really kind of prevents all that.
[00:12:34.320 --> 00:12:35.760]   There's no backdoor and Bitcoin.
[00:12:35.760 --> 00:12:39.200]   - I think Satoshi Nakamoto preserved on anonymity
[00:12:39.200 --> 00:12:41.560]   so that people would concentrate on the technology
[00:12:41.560 --> 00:12:42.840]   and not talk about the person,
[00:12:42.840 --> 00:12:44.640]   'cause we're talking about this sort of thing.
[00:12:44.640 --> 00:12:46.880]   It could become a cult of personality.
[00:12:46.880 --> 00:12:50.400]   That's why I don't think Wright is Satoshi Nakamoto,
[00:12:50.400 --> 00:12:52.400]   'cause either he wants the attention
[00:12:52.400 --> 00:12:55.040]   or someone wants to get him because he's a jerk,
[00:12:55.040 --> 00:12:57.120]   and either case, it just doesn't fit the profile,
[00:12:57.120 --> 00:12:57.960]   I don't think.
[00:12:57.960 --> 00:12:58.800]   - All right, another hope--
[00:12:58.800 --> 00:13:00.520]   - Why would you come out all of a sudden going,
[00:13:00.520 --> 00:13:02.560]   "Hey, I've been quiet for years.
[00:13:02.560 --> 00:13:04.520]   "Here I am now, and I want all this attention."
[00:13:04.520 --> 00:13:05.520]   - Yeah, exactly.
[00:13:05.520 --> 00:13:07.240]   That's why it's suspicious, I think.
[00:13:07.240 --> 00:13:10.920]   There's also another reason, Tom,
[00:13:10.920 --> 00:13:15.560]   which is if you want Bitcoin to,
[00:13:15.560 --> 00:13:17.160]   it's not merely the cult of personality,
[00:13:17.160 --> 00:13:19.160]   it's a question of trust.
[00:13:19.160 --> 00:13:20.240]   If you want Bitcoin to succeed,
[00:13:20.240 --> 00:13:22.920]   it needs to succeed purely on its own merits,
[00:13:22.920 --> 00:13:24.800]   mathematically.
[00:13:24.800 --> 00:13:29.800]   And there is, and I always kind of wary of this,
[00:13:29.800 --> 00:13:32.760]   but I think there's some case to be made
[00:13:32.760 --> 00:13:35.880]   that Bitcoin is like any classic pyramid scheme.
[00:13:35.880 --> 00:13:38.640]   If you, in the sense that the person
[00:13:38.640 --> 00:13:40.400]   who creates a new cryptocurrency
[00:13:40.400 --> 00:13:44.360]   as the opportunity to profit significantly--
[00:13:44.360 --> 00:13:45.400]   - Right, so that the tip up.
[00:13:45.400 --> 00:13:48.600]   - Because at the apex of the pyramid,
[00:13:48.600 --> 00:13:51.080]   and everybody else is kind of contributing to this.
[00:13:51.080 --> 00:13:52.560]   And if you're gonna be,
[00:13:52.560 --> 00:13:54.240]   if it is a pyramid scheme,
[00:13:54.240 --> 00:13:56.520]   really important that you not kind of rub it in that,
[00:13:56.520 --> 00:13:59.440]   "Yeah, I got 100 million of these," or whatever.
[00:13:59.440 --> 00:14:01.480]   It's not, you don't want to emphasize that fact.
[00:14:01.480 --> 00:14:02.560]   It should be a secret, right?
[00:14:02.560 --> 00:14:03.920]   - Maybe it's a marketing ploy,
[00:14:03.920 --> 00:14:05.240]   is that the more secret it is,
[00:14:05.240 --> 00:14:07.080]   the more intriguing and the more interesting,
[00:14:07.080 --> 00:14:08.560]   and people want to be a part of it.
[00:14:08.560 --> 00:14:09.920]   - Ken from Chicago in the chat room is saying,
[00:14:09.920 --> 00:14:10.920]   "Look what happened to Snowden.
[00:14:10.920 --> 00:14:13.080]   "It might be best not to be well known."
[00:14:13.080 --> 00:14:15.560]   And now in this day and age, probably,
[00:14:15.560 --> 00:14:17.000]   I don't know if it is the right.
[00:14:17.000 --> 00:14:20.240]   But it's got an irks Satoshi when somebody else
[00:14:20.240 --> 00:14:23.000]   comes forward and says, "Yeah, that's me."
[00:14:23.000 --> 00:14:25.240]   That's got a lot of--
[00:14:25.240 --> 00:14:28.920]   - Having this big of a trust too, that many Bitcoins,
[00:14:28.920 --> 00:14:30.440]   what kind of influence would it have
[00:14:30.440 --> 00:14:33.320]   on the Bitcoin market and value
[00:14:33.320 --> 00:14:37.480]   if suddenly it were just dumped into the market?
[00:14:37.480 --> 00:14:38.800]   - It's been very volatile up to now.
[00:14:38.800 --> 00:14:42.000]   It's been up well over a thousand dollars per Bitcoin.
[00:14:42.000 --> 00:14:43.840]   I don't know what it is currently.
[00:14:43.840 --> 00:14:45.240]   And there have been a lot of prosecutions,
[00:14:45.240 --> 00:14:46.800]   Matt Gox, which we used to go to
[00:14:46.800 --> 00:14:49.200]   to find out what Bitcoin was worth,
[00:14:49.200 --> 00:14:50.720]   turned out to be fraudulent.
[00:14:50.720 --> 00:14:53.800]   And its founder is headed to jail, I believe.
[00:14:53.800 --> 00:14:57.160]   $433 per coin,
[00:14:57.160 --> 00:14:59.760]   - According to the Wink Decks.
[00:14:59.760 --> 00:15:00.600]   - Wink Decks.
[00:15:00.600 --> 00:15:01.920]   - We interviewed the chief scientist
[00:15:01.920 --> 00:15:04.360]   at the Bitcoin Foundation on triangulation
[00:15:04.360 --> 00:15:06.560]   a couple of years ago, Gavin and Dereseun,
[00:15:06.560 --> 00:15:10.400]   who many have said, "Ez Satoshi," he denies it.
[00:15:10.400 --> 00:15:14.760]   But Satoshi in a very publicly handed over Bitcoin
[00:15:14.760 --> 00:15:17.240]   in effect to Gavin saying, "Okay, I'm done.
[00:15:17.240 --> 00:15:19.760]   You can run this from now on."
[00:15:19.760 --> 00:15:22.320]   So I asked him, he denied it.
[00:15:22.320 --> 00:15:24.320]   He said, "I'm not Satoshi."
[00:15:24.320 --> 00:15:25.320]   - This is such a good movie.
[00:15:25.320 --> 00:15:26.400]   - Exactly what Satoshi would say.
[00:15:26.400 --> 00:15:28.560]   - Exactly, that's what I said.
[00:15:28.560 --> 00:15:32.080]   That means you are, just tug your ear.
[00:15:32.080 --> 00:15:33.240]   Let me know.
[00:15:33.240 --> 00:15:36.880]   So it is fascinating.
[00:15:36.880 --> 00:15:39.560]   And I don't know really what Bitcoin is,
[00:15:39.560 --> 00:15:41.880]   but the future of Bitcoin is, I do not think though,
[00:15:41.880 --> 00:15:44.800]   there's a lot of attention on blockchain,
[00:15:44.800 --> 00:15:46.800]   the technology behind Bitcoin.
[00:15:46.800 --> 00:15:49.840]   And there's all sorts of interesting uses of blockchain,
[00:15:49.840 --> 00:15:53.880]   including as a music distribution system.
[00:15:53.880 --> 00:15:56.000]   So who was it?
[00:15:56.000 --> 00:15:58.960]   I'm trying to remember the name of the artist
[00:15:58.960 --> 00:16:03.960]   who wants to distribute her music via a blockchain technology.
[00:16:03.960 --> 00:16:06.240]   Imaging heap.
[00:16:06.240 --> 00:16:07.480]   - Imaging heap, yeah.
[00:16:08.240 --> 00:16:09.080]   - Mm.
[00:16:09.080 --> 00:16:12.600]   - And the idea is, one of the things blockchain could do
[00:16:12.600 --> 00:16:15.400]   is verify identity.
[00:16:15.400 --> 00:16:17.000]   Blockchain isn't perfect for this though,
[00:16:17.000 --> 00:16:18.960]   because one of the, I think the negatives of blockchain
[00:16:18.960 --> 00:16:21.080]   is everybody who has a Bitcoin wallet
[00:16:21.080 --> 00:16:24.320]   has the entire blockchain in the wallet.
[00:16:24.320 --> 00:16:26.920]   You have a record of everything, of everybody.
[00:16:26.920 --> 00:16:29.400]   And so you'd have to do that with a music system at well.
[00:16:29.400 --> 00:16:31.080]   But there are three companies at this point,
[00:16:31.080 --> 00:16:35.640]   PeerTracks, one of the other ones
[00:16:35.640 --> 00:16:37.880]   that are trying to use blockchain
[00:16:37.880 --> 00:16:39.960]   as a way of reinventing the music industry.
[00:16:39.960 --> 00:16:43.160]   BitTunes and Ujo Music.
[00:16:43.160 --> 00:16:49.240]   So the idea that Bitcoin could make a currency
[00:16:49.240 --> 00:16:52.120]   that's non-replicable using math.
[00:16:52.120 --> 00:16:52.960]   - It's amazing.
[00:16:52.960 --> 00:16:55.400]   - Is, you know, the music industry is like,
[00:16:55.400 --> 00:16:57.000]   that's exactly what we want.
[00:16:57.000 --> 00:17:00.760]   We want people to not be able to replicate our bits.
[00:17:00.760 --> 00:17:01.600]   - Right.
[00:17:02.640 --> 00:17:04.720]   I'm trying to get, I'm in Beckings, Steve Gibson,
[00:17:04.720 --> 00:17:06.040]   to do a blockchain segment.
[00:17:06.040 --> 00:17:07.640]   We did a Bitcoin segment on security now,
[00:17:07.640 --> 00:17:10.240]   but I really wanted to, please Steve,
[00:17:10.240 --> 00:17:12.680]   explain to me what is this blockchain.
[00:17:12.680 --> 00:17:15.240]   'Cause I think only he really understands the math.
[00:17:15.240 --> 00:17:18.000]   It's fascinating.
[00:17:18.000 --> 00:17:20.680]   Bitcoin has been around for seven years.
[00:17:20.680 --> 00:17:21.960]   - Oh, Nanny?
[00:17:21.960 --> 00:17:23.000]   - I have seven.
[00:17:23.000 --> 00:17:23.920]   - Seven.
[00:17:23.920 --> 00:17:24.760]   - You can't have them.
[00:17:24.760 --> 00:17:26.280]   (laughs)
[00:17:26.280 --> 00:17:28.720]   - I remember Devorak, one slagging off the hosts
[00:17:28.720 --> 00:17:30.480]   of the Money Machine on tech TV,
[00:17:30.480 --> 00:17:33.560]   because they said, "Oh yeah, our portfolios are down 10%
[00:17:33.560 --> 00:17:34.400]   "with this crash."
[00:17:34.400 --> 00:17:37.120]   And he said, "You can never be a finance expert
[00:17:37.120 --> 00:17:39.360]   "and say your portfolio is down."
[00:17:39.360 --> 00:17:41.400]   So I wonder, am I missing the boat
[00:17:41.400 --> 00:17:44.360]   as someone who's supposedly aware of the tech community?
[00:17:44.360 --> 00:17:46.120]   But by thinking, "Oh, it's too volatile.
[00:17:46.120 --> 00:17:46.960]   "It makes me nervous.
[00:17:46.960 --> 00:17:48.000]   "I don't wanna buy it."
[00:17:48.000 --> 00:17:49.280]   - Well, does this make you feel better?
[00:17:49.280 --> 00:17:52.120]   The Winklevoss twins are big into Bitcoin.
[00:17:52.120 --> 00:17:53.000]   - Oh yeah?
[00:17:53.000 --> 00:17:53.840]   No, I'm wanting to.
[00:17:53.840 --> 00:17:56.600]   (both laughing)
[00:17:58.200 --> 00:18:01.840]   - Well, they're all in on the Bitcoin.
[00:18:01.840 --> 00:18:03.240]   - Yeah, I think a lot of people, though,
[00:18:03.240 --> 00:18:06.320]   are really behind it in practice
[00:18:06.320 --> 00:18:09.320]   and also because of the technology that supports it,
[00:18:09.320 --> 00:18:11.920]   but a lot of people really don't have faith
[00:18:11.920 --> 00:18:13.560]   that it's going to pan out,
[00:18:13.560 --> 00:18:15.200]   that it's going to end up failing,
[00:18:15.200 --> 00:18:17.800]   but maybe it's a successful experiment
[00:18:17.800 --> 00:18:21.120]   because it shows us how the technology could work
[00:18:21.120 --> 00:18:24.360]   to create the next better currency.
[00:18:24.360 --> 00:18:25.240]   - Well, that's what I love.
[00:18:25.240 --> 00:18:26.800]   Is it what they call a cryptocurrency?
[00:18:26.800 --> 00:18:27.920]   And that's what I love about.
[00:18:27.920 --> 00:18:29.840]   I'm not sure you love about a Tookie.
[00:18:29.840 --> 00:18:31.400]   It's based on math.
[00:18:31.400 --> 00:18:32.760]   It's very rigorous.
[00:18:32.760 --> 00:18:34.360]   It's very clever in a lot of ways.
[00:18:34.360 --> 00:18:37.640]   For instance, they're creating a money supply
[00:18:37.640 --> 00:18:40.720]   and all the Bitcoins will be created
[00:18:40.720 --> 00:18:42.760]   at some point in the next 20 years.
[00:18:42.760 --> 00:18:44.200]   I can't remember the date.
[00:18:44.200 --> 00:18:46.200]   I think 2040, something like that.
[00:18:46.200 --> 00:18:47.800]   And then no more will be created.
[00:18:47.800 --> 00:18:50.360]   That will be a complete money supply.
[00:18:50.360 --> 00:18:53.480]   And that is created by work, mathematical work done.
[00:18:53.480 --> 00:18:55.440]   That's how you can mine Bitcoin
[00:18:55.440 --> 00:19:00.440]   by performing complex mathematical calculations.
[00:19:00.440 --> 00:19:03.440]   It's designed in such a way that it gets harder and harder
[00:19:03.440 --> 00:19:05.400]   to make a Bitcoin.
[00:19:05.400 --> 00:19:08.640]   The idea being as the supply increases,
[00:19:08.640 --> 00:19:10.520]   computing power is expected to increase.
[00:19:10.520 --> 00:19:12.520]   And as the value increases,
[00:19:12.520 --> 00:19:14.520]   people will be more interested in doing it.
[00:19:14.520 --> 00:19:16.440]   And so at this point,
[00:19:16.440 --> 00:19:18.480]   it seems to always be this as the case
[00:19:18.480 --> 00:19:21.280]   that it's kind of a push between the amount of power
[00:19:21.280 --> 00:19:22.560]   and hardware you need to buy
[00:19:22.560 --> 00:19:24.360]   and the amount of Bitcoin you would make.
[00:19:24.360 --> 00:19:25.360]   Exactly.
[00:19:25.360 --> 00:19:26.600]   It's brilliantly.
[00:19:26.600 --> 00:19:27.440]   Well, it's brilliant.
[00:19:27.440 --> 00:19:29.680]   And then Bitcoin is infinitely divisible.
[00:19:29.680 --> 00:19:31.000]   So sometimes you say,
[00:19:31.000 --> 00:19:32.240]   "Well, there's a fixed money supply.
[00:19:32.240 --> 00:19:35.200]   "Doesn't mean you're going to have incredible potential
[00:19:35.200 --> 00:19:36.680]   "for inflation and deflation."
[00:19:36.680 --> 00:19:38.600]   But no, because you can slice it infinitely.
[00:19:38.600 --> 00:19:40.080]   It's digital.
[00:19:40.080 --> 00:19:40.920]   You can handle that.
[00:19:40.920 --> 00:19:43.080]   You could, a tenth of a Bitcoin would be worth a thousand bucks
[00:19:43.080 --> 00:19:45.200]   and then you'd make it a hundredth of a Bitcoin.
[00:19:45.200 --> 00:19:46.040]   Come along.
[00:19:46.040 --> 00:19:49.480]   I'm going to give you my little infinity slice of Bitcoin.
[00:19:49.480 --> 00:19:50.560]   People do, when they tip me,
[00:19:50.560 --> 00:19:51.920]   one of the reasons I have seven Bitcoins
[00:19:51.920 --> 00:19:53.880]   is because I have a wallet and the wallet used to be
[00:19:53.880 --> 00:19:55.360]   on the website, people could donate.
[00:19:55.360 --> 00:19:57.280]   And they tip me, they don't tip you a Bitcoin.
[00:19:57.280 --> 00:19:59.160]   They tip you a hundredth or a thousandth of a Bitcoin.
[00:19:59.160 --> 00:20:00.000]   Tiny little, no.
[00:20:00.000 --> 00:20:00.920]   Do you take Bitcoin, Tom?
[00:20:00.920 --> 00:20:01.600]   I think you do, right?
[00:20:01.600 --> 00:20:02.760]   Yeah, I do.
[00:20:02.760 --> 00:20:05.600]   And somebody was nice enough to give me a bunch
[00:20:05.600 --> 00:20:08.760]   in the early days of Bitcoin when it first launched.
[00:20:08.760 --> 00:20:10.120]   So yeah, I've got a few too.
[00:20:10.120 --> 00:20:10.640]   But you're right.
[00:20:10.640 --> 00:20:13.960]   Somebody gives $20 and then the whatever system they're using
[00:20:13.960 --> 00:20:14.960]   to send the Bitcoin says,
[00:20:14.960 --> 00:20:18.840]   "Well, that's .43 Bitcoins and sends that along."
[00:20:18.840 --> 00:20:19.840]   There is some risk involved.
[00:20:19.840 --> 00:20:23.840]   Remember the guy who bought pizza?
[00:20:23.840 --> 00:20:26.160]   2010, he bought it.
[00:20:26.160 --> 00:20:27.080]   This was kind of cool.
[00:20:27.080 --> 00:20:27.840]   You can.
[00:20:27.840 --> 00:20:29.280]   There's a cupcake company in San Francisco
[00:20:29.280 --> 00:20:30.880]   that'll sell you cupcakes on Bitcoin.
[00:20:30.880 --> 00:20:35.320]   There was a guy who used Bitcoin to pay for pizza,
[00:20:35.320 --> 00:20:39.480]   Florida programmer, Laszlo Honech.
[00:20:39.480 --> 00:20:42.440]   And he paid for a pizza, two pizzas,
[00:20:42.440 --> 00:20:45.440]   scary, 10,000 Bitcoins.
[00:20:45.440 --> 00:20:47.000]   Well, damn.
[00:20:47.000 --> 00:20:50.840]   At the time, not worth a whole lot of money.
[00:20:50.840 --> 00:20:53.520]   But at one point worth $7 million.
[00:20:53.520 --> 00:20:54.520]   Jeez.
[00:20:54.520 --> 00:20:57.120]   And that pizza is not worth that much now.
[00:20:57.120 --> 00:20:58.040]   I don't know.
[00:20:58.040 --> 00:20:59.400]   I like cold pizza.
[00:20:59.400 --> 00:21:00.480]   I think by now.
[00:21:00.480 --> 00:21:01.280]   Pretty cold by now.
[00:21:01.280 --> 00:21:01.800]   Yeah.
[00:21:01.800 --> 00:21:02.640]   Can you believe that?
[00:21:02.640 --> 00:21:03.080]   And the guy--
[00:21:03.080 --> 00:21:05.400]   This served cold for a 10,000 Bitcoin.
[00:21:05.400 --> 00:21:06.840]   Guy must be kicking himself.
[00:21:06.840 --> 00:21:09.120]   I had 10,000 Bitcoins.
[00:21:09.120 --> 00:21:10.040]   I blew it.
[00:21:10.040 --> 00:21:11.840]   Well, you used to be able to sign up for Bitcoin
[00:21:11.840 --> 00:21:13.840]   and you'd get like, I think it was half a Bitcoin
[00:21:13.840 --> 00:21:16.920]   or a full Bitcoin free once you've downloaded the software.
[00:21:16.920 --> 00:21:18.160]   That was Gavin doing that.
[00:21:18.160 --> 00:21:20.000]   In fact, that's how Gavin became involved
[00:21:20.000 --> 00:21:23.280]   with the Bitcoin foundation was he wanted to really
[00:21:23.280 --> 00:21:24.600]   want to promote Bitcoin.
[00:21:24.600 --> 00:21:27.200]   By the way, somebody like Satoshi Nakamoto
[00:21:27.200 --> 00:21:29.640]   might reasonably want to do.
[00:21:29.640 --> 00:21:32.440]   And so he was giving away a Bitcoin.
[00:21:32.440 --> 00:21:34.680]   And that's how he became known to the Bitcoin foundation
[00:21:34.680 --> 00:21:35.400]   supposedly.
[00:21:35.400 --> 00:21:39.720]   And the Satoshi supposedly became their technical director
[00:21:39.720 --> 00:21:40.920]   supposedly.
[00:21:40.920 --> 00:21:42.720]   I think there's compelling evidence
[00:21:42.720 --> 00:21:44.160]   that a number of different people could be involved.
[00:21:44.160 --> 00:21:45.320]   It's a fascinating mystery.
[00:21:45.320 --> 00:21:46.960]   I think it's more than one person.
[00:21:46.960 --> 00:21:47.920]   I think it's a group.
[00:21:47.920 --> 00:21:50.080]   You're in a circuit movie waiting to happen.
[00:21:50.080 --> 00:21:50.600]   Definitely.
[00:21:50.600 --> 00:21:51.560]   What I think.
[00:21:51.560 --> 00:21:52.200]   You'll see it there.
[00:21:52.200 --> 00:21:54.720]   And we'll see Satoshi when he was young
[00:21:54.720 --> 00:21:55.880]   and then when he's middle aged
[00:21:55.880 --> 00:21:57.160]   and then near the end of his life.
[00:21:57.160 --> 00:21:58.720]   Did any of you see the Steve Jobs movie?
[00:21:58.720 --> 00:21:59.720]   Yeah, I saw it.
[00:21:59.720 --> 00:22:00.600]   I saw it too.
[00:22:00.600 --> 00:22:02.320]   What did you think, Tom?
[00:22:02.320 --> 00:22:06.520]   I thought it was a really good character examination
[00:22:06.520 --> 00:22:09.480]   that gave me an impression of what it might have been like
[00:22:09.480 --> 00:22:10.720]   to be around Steve Jobs.
[00:22:10.720 --> 00:22:13.080]   But it certainly wasn't accurate, right?
[00:22:13.080 --> 00:22:13.920]   Unfact.
[00:22:13.920 --> 00:22:14.760]   It was horrible.
[00:22:14.760 --> 00:22:15.600]   I thought.
[00:22:15.600 --> 00:22:17.800]   I thought he was like, well, OK, maybe--
[00:22:17.800 --> 00:22:19.640]   and you've been around Steve Jobs.
[00:22:19.640 --> 00:22:20.960]   So you could say this much better.
[00:22:20.960 --> 00:22:22.760]   I thought, well, maybe he could have been like that.
[00:22:22.760 --> 00:22:24.200]   I get the patna of it.
[00:22:24.200 --> 00:22:26.840]   But the facts were all wrong on me.
[00:22:26.840 --> 00:22:28.080]   I don't know.
[00:22:28.080 --> 00:22:31.360]   It was a good movie, I guess, if it had been about somebody
[00:22:31.360 --> 00:22:32.480]   not Steve Jobs as a--
[00:22:32.480 --> 00:22:34.040]   Had snappy dialogue.
[00:22:34.040 --> 00:22:35.040]   I love Aaron Sorkin.
[00:22:35.040 --> 00:22:35.720]   He's a great writer.
[00:22:35.720 --> 00:22:37.080]   But he does hate technology.
[00:22:37.080 --> 00:22:39.200]   Did you guys see the Alex Gibney documentary on Steve Jobs?
[00:22:39.200 --> 00:22:39.720]   No, I haven't.
[00:22:39.720 --> 00:22:40.520]   Have you seen it?
[00:22:40.520 --> 00:22:41.200]   Is that worth seeing?
[00:22:41.200 --> 00:22:42.280]   It's really worth seeing.
[00:22:42.280 --> 00:22:42.760]   Oh, for what you.
[00:22:42.760 --> 00:22:45.160]   Because it does not pull any punches.
[00:22:45.160 --> 00:22:49.720]   They-- he just really portrayed Steve Jobs that a-hole.
[00:22:49.720 --> 00:22:50.320]   Wow.
[00:22:50.320 --> 00:22:51.520]   I mean, he was really strong.
[00:22:51.520 --> 00:22:52.520]   It's really strong.
[00:22:52.520 --> 00:22:55.360]   If somebody's just a jerk, you cannot inspire people
[00:22:55.360 --> 00:22:58.000]   to create these great things that he inspired people to create.
[00:22:58.000 --> 00:23:00.960]   You cannot get people to work for you over a period of 10, 20
[00:23:00.960 --> 00:23:02.280]   years.
[00:23:02.280 --> 00:23:05.480]   I mean, that does not the whole picture by any means.
[00:23:05.480 --> 00:23:06.320]   No, no.
[00:23:06.320 --> 00:23:06.820]   No.
[00:23:06.820 --> 00:23:07.320]   I think you do.
[00:23:07.320 --> 00:23:08.520]   And that's one of the reasons I didn't like the movie.
[00:23:08.520 --> 00:23:11.920]   I felt like it doesn't really do a human.
[00:23:11.920 --> 00:23:14.960]   It doesn't mean disservice to portray him as any one thing.
[00:23:14.960 --> 00:23:15.800]   Yeah, yeah.
[00:23:15.800 --> 00:23:16.320]   And I think--
[00:23:16.320 --> 00:23:17.200]   I mean, I'm a little sad.
[00:23:17.200 --> 00:23:20.360]   But there's more to me than that, I hope.
[00:23:20.360 --> 00:23:21.680]   By the way, thank you for clarifying
[00:23:21.680 --> 00:23:23.120]   whether or not we could say that.
[00:23:23.120 --> 00:23:25.000]   I did the parental version.
[00:23:25.000 --> 00:23:25.520]   Thanks.
[00:23:25.520 --> 00:23:26.360]   Thanks.
[00:23:26.360 --> 00:23:26.680]   I'm glad not.
[00:23:26.680 --> 00:23:27.640]   Don't believe me out.
[00:23:27.640 --> 00:23:28.640]   We want to be with you.
[00:23:28.640 --> 00:23:31.680]   And I wait for the Aaron Sorkin Biopek of Leo@TechTV.
[00:23:31.680 --> 00:23:33.160]   Ah, you're so mad.
[00:23:33.160 --> 00:23:33.960]   Get out of here.
[00:23:33.960 --> 00:23:37.520]   Your people are ruining this company.
[00:23:37.520 --> 00:23:41.000]   Steve did say that, by the way, for the mobile me developers.
[00:23:41.000 --> 00:23:42.280]   Mobile Me was such a nightmare.
[00:23:42.280 --> 00:23:44.360]   He brought them-- and this is in his biography.
[00:23:44.360 --> 00:23:46.000]   This is reported by many people who were there.
[00:23:46.000 --> 00:23:48.680]   He brought them all into the auditorium at the Apple
[00:23:48.680 --> 00:23:49.680]   Campus.
[00:23:49.680 --> 00:23:53.120]   And Choonamout said, you have ruined my company.
[00:23:53.120 --> 00:23:55.480]   You have ruined our reputation, then Firenimal.
[00:23:55.480 --> 00:23:56.560]   Wow.
[00:23:56.560 --> 00:23:57.240]   All at once.
[00:23:57.240 --> 00:23:57.760]   All at once.
[00:23:57.760 --> 00:23:58.760]   Because it makes it easy.
[00:23:58.760 --> 00:23:59.260]   In an auditorium.
[00:23:59.260 --> 00:24:00.160]   It's fast.
[00:24:00.160 --> 00:24:02.040]   You got to admire his efficiency.
[00:24:02.040 --> 00:24:02.520]   Right.
[00:24:02.520 --> 00:24:08.120]   Coming up, Elon Musk and others are funding--
[00:24:08.120 --> 00:24:09.920]   damn you-- artificial intelligence.
[00:24:09.920 --> 00:24:12.040]   What's wrong with these people?
[00:24:12.040 --> 00:24:16.200]   A billion dollar AI fund.
[00:24:16.200 --> 00:24:19.720]   But first, a word from my sheets.
[00:24:19.720 --> 00:24:21.120]   Stop sleeping on me.
[00:24:21.120 --> 00:24:23.120]   No, I'm talking about my bowling branch sheets.
[00:24:23.120 --> 00:24:24.360]   You said you wanted to hear about this.
[00:24:24.360 --> 00:24:25.960]   Let me tell you about my bowling branch sheets.
[00:24:25.960 --> 00:24:28.520]   When you eat sheets, you're not talking about spread sheets.
[00:24:28.520 --> 00:24:29.520]   No.
[00:24:29.520 --> 00:24:30.480]   Or you mean bed sheets.
[00:24:30.480 --> 00:24:31.800]   I'm talking cotton, baby.
[00:24:31.800 --> 00:24:35.520]   And sometimes I got to-- let's lay this flat right here.
[00:24:35.520 --> 00:24:37.360]   People are going to say, what's the thread count?
[00:24:37.360 --> 00:24:38.400]   What is the thread count?
[00:24:38.400 --> 00:24:39.200]   See?
[00:24:39.200 --> 00:24:39.760]   No.
[00:24:39.760 --> 00:24:41.560]   That's a marketing term.
[00:24:41.560 --> 00:24:43.560]   I have very high thread count.
[00:24:43.560 --> 00:24:47.000]   You can buy at Walmart, very high thread count.
[00:24:47.000 --> 00:24:49.040]   Doesn't mean it's a great sheet.
[00:24:49.040 --> 00:24:51.960]   What makes it a great sheet is great cotton.
[00:24:51.960 --> 00:24:53.480]   Bowling branch is the first betting
[00:24:53.480 --> 00:24:55.400]   brand to get fair trade certification.
[00:24:55.400 --> 00:24:57.240]   It's organic cotton.
[00:24:57.240 --> 00:25:00.160]   It is-- I will tell you this-- the softest cotton
[00:25:00.160 --> 00:25:01.040]   I've ever slept on.
[00:25:01.040 --> 00:25:03.040]   It is amazing.
[00:25:03.040 --> 00:25:04.680]   And they don't sell in the department store,
[00:25:04.680 --> 00:25:07.120]   so they eliminate the 800% markup.
[00:25:07.120 --> 00:25:09.080]   They sell it direct to you from their website,
[00:25:09.080 --> 00:25:12.360]   bowlandbranch.com.
[00:25:12.360 --> 00:25:15.000]   Now, you're going to mock this, but it says it right here,
[00:25:15.000 --> 00:25:17.600]   slept on by three US presidents.
[00:25:17.600 --> 00:25:18.600]   Oh.
[00:25:18.600 --> 00:25:19.160]   That's the time?
[00:25:19.160 --> 00:25:20.560]   Is that the new metric?
[00:25:20.560 --> 00:25:21.920]   That's the new metric.
[00:25:21.920 --> 00:25:24.640]   Three US presidents, countless celebrities,
[00:25:24.640 --> 00:25:26.320]   I sleep on them, and I love them.
[00:25:26.320 --> 00:25:29.200]   And by the way, what a great gift for the holidays.
[00:25:29.200 --> 00:25:31.040]   Scroll down to the pictures, Jason,
[00:25:31.040 --> 00:25:32.040]   because look at the box.
[00:25:32.040 --> 00:25:35.600]   They come in this beautiful box with ribbons on it.
[00:25:35.600 --> 00:25:38.080]   When they came, I thought, somebody sent us a wedding present.
[00:25:38.080 --> 00:25:38.800]   It was gorgeous.
[00:25:38.800 --> 00:25:39.800]   No.
[00:25:39.800 --> 00:25:42.840]   That's what they come in, this beautiful, but ribbon box.
[00:25:42.840 --> 00:25:43.880]   They are the nicest sheets.
[00:25:43.880 --> 00:25:44.560]   And not just sheets.
[00:25:44.560 --> 00:25:48.280]   You can get pillowcases, towels, duvets.
[00:25:48.280 --> 00:25:51.120]   They even have beautiful cotton shawls.
[00:25:51.120 --> 00:25:55.000]   It is a luscious, luscious quality sheets
[00:25:55.000 --> 00:25:56.960]   without paying for department store overhead.
[00:25:56.960 --> 00:25:57.920]   Just a couple of hundred bucks.
[00:25:57.920 --> 00:26:01.760]   Treat yourself to these luxurious, incredibly comfortable
[00:26:01.760 --> 00:26:02.520]   sheets.
[00:26:02.520 --> 00:26:04.240]   And right now, when you use the offer code,
[00:26:04.240 --> 00:26:07.400]   "Twit at Checkout," you'll get 20% off your entire order.
[00:26:07.400 --> 00:26:10.040]   This would be a great gift, a wedding gift, a baby shower
[00:26:10.040 --> 00:26:15.640]   gift, a holiday gift, soft, pure, incredible linens.
[00:26:15.640 --> 00:26:17.520]   And they donate a portion of every sale to charity.
[00:26:17.520 --> 00:26:19.200]   They're really-- because it's fair trade,
[00:26:19.200 --> 00:26:21.800]   you can feel good about these sheets.
[00:26:21.800 --> 00:26:22.400]   I love them.
[00:26:22.400 --> 00:26:24.920]   Ball and branch, B-O-L-L, like cotton bowl.
[00:26:24.920 --> 00:26:25.760]   Yeah, oh, sure.
[00:26:25.760 --> 00:26:28.200]   B-O-L-L and branch.
[00:26:28.200 --> 00:26:31.760]   And don't forget the offer code, "Twit" for 20% off your entire order.
[00:26:31.760 --> 00:26:34.200]   Sheets, towels, blankets, duvets, covers, and more.
[00:26:34.200 --> 00:26:35.960]   Oh, and free shipping right now.
[00:26:35.960 --> 00:26:36.920]   All of their-- in the US.
[00:26:36.920 --> 00:26:39.600]   All of their products come beautifully packaged in those boxes.
[00:26:39.600 --> 00:26:42.800]   You'll love an even-- even if you don't love them,
[00:26:42.800 --> 00:26:47.880]   because you get 30 days risk-free, you can return them
[00:26:47.880 --> 00:26:48.840]   for your money back.
[00:26:48.840 --> 00:26:51.920]   If you're not completely sleepy,
[00:26:51.920 --> 00:26:52.880]   bowl and branch.com.
[00:26:52.880 --> 00:26:53.320]   I love them.
[00:26:53.320 --> 00:26:56.040]   Between the bowl and branch and, like, gas firm mattress,
[00:26:56.040 --> 00:26:57.840]   sleep is-- it's interesting.
[00:26:57.840 --> 00:27:00.160]   Geeks are realizing how you must--
[00:27:00.160 --> 00:27:01.920]   for good morning America, cover this, right?
[00:27:01.920 --> 00:27:04.040]   I've done tons of sleep stories, and I can tell you
[00:27:04.040 --> 00:27:05.400]   all the products that are out there.
[00:27:05.400 --> 00:27:10.480]   And I've done a sleep study at Stanford while wearing and using
[00:27:10.480 --> 00:27:12.000]   five of the most popular products.
[00:27:12.000 --> 00:27:15.160]   They're mostly within about 10% variance in terms
[00:27:15.160 --> 00:27:20.200]   of figuring out how-- things like your duration of sleep,
[00:27:20.200 --> 00:27:21.800]   how long it took you to get to sleep,
[00:27:21.800 --> 00:27:23.040]   and how many wake events.
[00:27:23.040 --> 00:27:24.200]   Although that's a little interesting,
[00:27:24.200 --> 00:27:27.800]   because everyone defines that differently.
[00:27:27.800 --> 00:27:29.200]   I don't think it's about accuracy.
[00:27:29.200 --> 00:27:31.040]   We were talking about the ZEO one time when we asked--
[00:27:31.040 --> 00:27:32.280]   So I'm using the ZEO.
[00:27:32.280 --> 00:27:33.040]   Yeah, what do you think?
[00:27:33.040 --> 00:27:37.160]   So ZEO was a startup, raised a lot of money,
[00:27:37.160 --> 00:27:38.440]   and they failed.
[00:27:38.440 --> 00:27:39.480]   They went out of business.
[00:27:39.480 --> 00:27:41.040]   And I think it's because of the Fitbit.
[00:27:41.040 --> 00:27:43.360]   I think it's because of the motion trackers.
[00:27:43.360 --> 00:27:45.880]   And it kind of-- in fact, they even say it--
[00:27:45.880 --> 00:27:48.880]   because this was a headband that measured your EEG.
[00:27:48.880 --> 00:27:50.840]   So this wasn't like--
[00:27:50.840 --> 00:27:52.320]   I see, I always thought, well, if--
[00:27:52.320 --> 00:27:54.240]   I mean, what it's telling-- it's not really telling me,
[00:27:54.240 --> 00:27:54.720]   I'm sleeping.
[00:27:54.720 --> 00:27:57.120]   It's telling me how my wrist is doing.
[00:27:57.120 --> 00:27:59.520]   Or you can get the bed at, which goes under your--
[00:27:59.520 --> 00:28:00.040]   Right.
[00:28:00.040 --> 00:28:01.440]   Yeah, we have the bed at.
[00:28:01.440 --> 00:28:04.280]   And that didn't go over very well.
[00:28:04.280 --> 00:28:06.640]   It wasn't actually that accurate.
[00:28:06.640 --> 00:28:08.320]   So I don't know how accurate this is.
[00:28:08.320 --> 00:28:09.680]   I found them on eBay.
[00:28:09.680 --> 00:28:13.480]   Steve Gibson said, quick, they've been liquidated,
[00:28:13.480 --> 00:28:15.280]   and you can buy them on eBay.
[00:28:15.280 --> 00:28:17.400]   And apparently, the software's still on Android.
[00:28:17.400 --> 00:28:17.880]   They never--
[00:28:17.880 --> 00:28:18.640]   The ZEO's great.
[00:28:18.640 --> 00:28:19.600]   I used it for a long time.
[00:28:19.600 --> 00:28:21.480]   The biggest problem was I'd wake up every morning
[00:28:21.480 --> 00:28:22.640]   with an indentation on my--
[00:28:22.640 --> 00:28:23.640]   Me too!
[00:28:23.640 --> 00:28:26.640]   I had to come to work with three dots on my head.
[00:28:26.640 --> 00:28:28.400]   And look at the size of our collective heads.
[00:28:28.400 --> 00:28:29.440]   You don't want that.
[00:28:29.440 --> 00:28:29.880]   Big.
[00:28:29.880 --> 00:28:30.800]   You don't want that.
[00:28:30.800 --> 00:28:32.600]   These things are massive domes.
[00:28:32.600 --> 00:28:33.080]   You may--
[00:28:33.080 --> 00:28:35.080]   And so people were saying, why are there three dots on your head?
[00:28:35.080 --> 00:28:36.840]   Because the ZEO thing, that's why it was.
[00:28:36.840 --> 00:28:37.360]   I know.
[00:28:37.360 --> 00:28:38.920]   They're like, it's pillow wrinkles.
[00:28:38.920 --> 00:28:39.920]   Yes.
[00:28:39.920 --> 00:28:40.920]   Oh.
[00:28:40.920 --> 00:28:41.440]   But the thing--
[00:28:41.440 --> 00:28:42.000]   Well, and you're a redhead.
[00:28:42.000 --> 00:28:44.160]   So you would really-- you would show up.
[00:28:44.160 --> 00:28:44.960]   You would people--
[00:28:44.960 --> 00:28:45.560]   You couldn't do this.
[00:28:45.560 --> 00:28:46.800]   You don't want to use the ZEO.
[00:28:46.800 --> 00:28:48.520]   Try the S+ from ResMed.
[00:28:48.520 --> 00:28:50.080]   I like that one a lot.
[00:28:50.080 --> 00:28:53.280]   It has-- it's-- it's-- uses motion.
[00:28:53.280 --> 00:28:55.280]   But I think their algorithm is better
[00:28:55.280 --> 00:28:58.120]   than some of the other ones.
[00:28:58.120 --> 00:29:01.800]   And the bottom line is, it doesn't necessarily
[00:29:01.800 --> 00:29:04.480]   matter how perfectly accurate they are.
[00:29:04.480 --> 00:29:06.360]   It's about cause and effect.
[00:29:06.360 --> 00:29:08.920]   You use these sleep devices, basically,
[00:29:08.920 --> 00:29:11.680]   to determine if I have three glasses of wine,
[00:29:11.680 --> 00:29:16.320]   I noticed that I wake up 15 times in the night.
[00:29:16.320 --> 00:29:19.680]   There's a bunch of different ways to use them.
[00:29:19.680 --> 00:29:23.120]   But basically, it's identifying what causes sleep problems
[00:29:23.120 --> 00:29:23.720]   for you.
[00:29:23.720 --> 00:29:24.680]   You know what this is, though?
[00:29:24.680 --> 00:29:27.040]   This is about cropping.
[00:29:27.040 --> 00:29:27.680]   I don't know.
[00:29:27.680 --> 00:29:33.200]   There's an out-something, outburst of the quantified self-movement.
[00:29:33.200 --> 00:29:35.920]   We want to measure everything.
[00:29:35.920 --> 00:29:38.760]   And I think that's a little self-centered.
[00:29:38.760 --> 00:29:40.480]   This is difficult geek activity.
[00:29:40.480 --> 00:29:43.080]   I need to know exactly how I sleep.
[00:29:43.080 --> 00:29:44.600]   But I have trouble shooting, right?
[00:29:44.600 --> 00:29:45.120]   Yeah.
[00:29:45.120 --> 00:29:46.960]   The people who use these are people who are like,
[00:29:46.960 --> 00:29:48.600]   I don't think I'm sleeping well enough.
[00:29:48.600 --> 00:29:51.640]   And I want to gather the data and figure it out.
[00:29:51.640 --> 00:29:52.960]   Well, I talked to one sleep expert
[00:29:52.960 --> 00:29:55.040]   at the University of Chicago, Dr. Garalnek.
[00:29:55.040 --> 00:29:57.360]   And she said she has a ton of people who come to her
[00:29:57.360 --> 00:29:58.720]   and say that they have insomnia.
[00:29:58.720 --> 00:30:00.480]   And then when she actually tests them, they don't have it.
[00:30:00.480 --> 00:30:01.320]   Really?
[00:30:01.320 --> 00:30:01.640]   Yeah.
[00:30:01.640 --> 00:30:02.320]   That's interesting.
[00:30:02.320 --> 00:30:03.240]   I think we're all just tired.
[00:30:03.240 --> 00:30:04.560]   If you think you have insomnia,
[00:30:04.560 --> 00:30:06.400]   go look up the clinical definition of it.
[00:30:06.400 --> 00:30:07.560]   It's not what you think.
[00:30:07.560 --> 00:30:08.080]   There is it is.
[00:30:08.080 --> 00:30:09.560]   It's not eating right.
[00:30:09.560 --> 00:30:12.480]   Yeah, it's a prolonged exposure.
[00:30:12.480 --> 00:30:15.480]   It's a prolonged-- it's like three nights
[00:30:15.480 --> 00:30:19.560]   a week of sleeping less than five hours for three to five months.
[00:30:19.560 --> 00:30:21.640]   I mean, it's really about the duration
[00:30:21.640 --> 00:30:23.520]   of sleep deprivation you have.
[00:30:23.520 --> 00:30:26.000]   So I think quantifying that and saying, OK,
[00:30:26.000 --> 00:30:27.680]   I'm not actually an insomniac.
[00:30:27.680 --> 00:30:29.760]   I just occasionally have trouble sleeping.
[00:30:29.760 --> 00:30:31.880]   Because one of the reasons that you get insomnia
[00:30:31.880 --> 00:30:35.520]   or perceived insomnia is anxiety about insomnia.
[00:30:35.520 --> 00:30:37.000]   So that's part of what happened.
[00:30:37.000 --> 00:30:39.280]   But you've heard about FFI, right?
[00:30:39.280 --> 00:30:40.200]   FFI?
[00:30:40.200 --> 00:30:42.200]   Fatal familial insomnia.
[00:30:42.200 --> 00:30:43.400]   No, this is a real disease.
[00:30:43.400 --> 00:30:44.360]   It's a genetic disease.
[00:30:44.360 --> 00:30:45.800]   You can't sleep.
[00:30:45.800 --> 00:30:48.280]   You can't sleep and you go psychotic and die.
[00:30:48.280 --> 00:30:49.720]   Well, it's real?
[00:30:49.720 --> 00:30:51.280]   Yes.
[00:30:51.280 --> 00:30:52.080]   I've never heard that.
[00:30:52.080 --> 00:30:53.840]   And you are going to get it.
[00:30:53.840 --> 00:30:54.800]   I don't have it.
[00:30:54.800 --> 00:30:56.280]   I don't have it.
[00:30:56.280 --> 00:30:58.040]   I know exactly how many deaths I've seen.
[00:30:58.040 --> 00:31:00.160]   A little brother tells-- or a big brother
[00:31:00.160 --> 00:31:01.600]   tells a little sister.
[00:31:01.600 --> 00:31:02.560]   Oh, yeah, you got it.
[00:31:02.560 --> 00:31:03.560]   You're going to die.
[00:31:03.560 --> 00:31:04.560]   No, you're going to die.
[00:31:04.560 --> 00:31:05.560]   And you're going to die.
[00:31:05.560 --> 00:31:06.560]   You're never going to sleep.
[00:31:06.560 --> 00:31:07.520]   I've got the FFA.
[00:31:07.520 --> 00:31:08.320]   It's not very common.
[00:31:08.320 --> 00:31:10.680]   40 families worldwide.
[00:31:10.680 --> 00:31:13.200]   But it is a genetic disease that causes death.
[00:31:13.200 --> 00:31:14.200]   Wow.
[00:31:14.200 --> 00:31:15.760]   That is-- I don't like that.
[00:31:15.760 --> 00:31:18.320]   You need-- anyway, that would be the extreme.
[00:31:18.320 --> 00:31:20.120]   You need sleep.
[00:31:20.120 --> 00:31:21.960]   And I think it is good to know if--
[00:31:21.960 --> 00:31:24.120]   coffee's keeping you up or--
[00:31:24.120 --> 00:31:25.640]   but I think we're all so stressed.
[00:31:25.640 --> 00:31:26.640]   I think that's--
[00:31:26.640 --> 00:31:29.800]   Coffee, anxiety over work or whatever it happens to be.
[00:31:29.800 --> 00:31:32.480]   Or whether it's the blue light from your device
[00:31:32.480 --> 00:31:33.480]   that you're using to read the--
[00:31:33.480 --> 00:31:34.240]   What do you think of that?
[00:31:34.240 --> 00:31:34.840]   Is that real?
[00:31:34.840 --> 00:31:36.440]   Because everybody's talking about that now.
[00:31:36.440 --> 00:31:39.400]   Your device is putting blue light in your eyes.
[00:31:39.400 --> 00:31:40.560]   Is that real?
[00:31:40.560 --> 00:31:42.440]   I think that there's something to it.
[00:31:42.440 --> 00:31:44.920]   I don't know that anybody has--
[00:31:44.920 --> 00:31:47.920]   it's been completely scientifically proven
[00:31:47.920 --> 00:31:50.400]   that people falling asleep with these devices
[00:31:50.400 --> 00:31:52.200]   or having more trouble sleeping.
[00:31:52.200 --> 00:31:54.440]   I know somebody wears blue blockers.
[00:31:54.440 --> 00:31:59.080]   Yeah, but we do know that blue light does stimulate the brain.
[00:31:59.080 --> 00:32:01.520]   And so that stimulation will have an effect.
[00:32:01.520 --> 00:32:03.200]   Like the blue light from the sun,
[00:32:03.200 --> 00:32:04.640]   so that's to tell you you're up?
[00:32:04.640 --> 00:32:05.560]   Is that like?
[00:32:05.560 --> 00:32:06.480]   That you're a link.
[00:32:06.480 --> 00:32:07.480]   Exactly.
[00:32:07.480 --> 00:32:11.560]   And so it affects your melatonin release.
[00:32:11.560 --> 00:32:12.120]   Are you a link?
[00:32:12.120 --> 00:32:13.720]   So further research is needed.
[00:32:13.720 --> 00:32:14.480]   Yes.
[00:32:14.480 --> 00:32:16.040]   And there are quite a few studies there.
[00:32:16.040 --> 00:32:18.120]   But this is one of those issues where I would say
[00:32:18.120 --> 00:32:19.080]   there are a lot of studies.
[00:32:19.080 --> 00:32:22.720]   But there's also some corollary factors
[00:32:22.720 --> 00:32:25.480]   that haven't totally been considered, which is if you're
[00:32:25.480 --> 00:32:28.720]   reading or checking Facebook or reading email,
[00:32:28.720 --> 00:32:31.920]   there's something that stimulates you emotionally
[00:32:31.920 --> 00:32:33.400]   or intellectually.
[00:32:33.400 --> 00:32:38.920]   And that, I think, is as much a factor as the blue light
[00:32:38.920 --> 00:32:40.280]   itself, personally.
[00:32:40.280 --> 00:32:40.760]   Totally.
[00:32:40.760 --> 00:32:41.520]   I totally agree.
[00:32:41.520 --> 00:32:42.920]   How about-- OK.
[00:32:42.920 --> 00:32:45.480]   By modal sleep, this is another hipster thing.
[00:32:45.480 --> 00:32:46.160]   Oh, what is this?
[00:32:46.160 --> 00:32:47.040]   Oh, yeah.
[00:32:47.040 --> 00:32:48.760]   Have you-- you know about this?
[00:32:48.760 --> 00:32:49.360]   Dr. Keke?
[00:32:49.360 --> 00:32:50.360]   I think-- yeah.
[00:32:50.360 --> 00:32:52.400]   This is where people don't sleep eight hours.
[00:32:52.400 --> 00:32:53.280]   They sleep four hours.
[00:32:53.280 --> 00:32:54.080]   They're up four hours.
[00:32:54.080 --> 00:32:56.400]   They sleep another four hours.
[00:32:56.400 --> 00:32:58.160]   It's split sleep.
[00:32:58.160 --> 00:33:00.000]   I do this a lot for work.
[00:33:00.000 --> 00:33:01.440]   So I'll go to bed at 10.
[00:33:01.440 --> 00:33:03.880]   I'll get up at 2 and do a live shot at 4.
[00:33:03.880 --> 00:33:05.440]   And I'll go back to bed at 5.
[00:33:05.440 --> 00:33:06.680]   And then I'll sleep till 9.
[00:33:06.680 --> 00:33:08.880]   I do this probably twice a week.
[00:33:08.880 --> 00:33:11.800]   It sucks.
[00:33:11.800 --> 00:33:13.680]   This is the paleo of sleeping.
[00:33:13.680 --> 00:33:14.680]   OK?
[00:33:14.680 --> 00:33:18.800]   The anthropologists say there's some evidence that sleeping--
[00:33:18.800 --> 00:33:22.600]   by sleeping or by modal sleeping was the pre-industrial norm.
[00:33:22.600 --> 00:33:23.520]   The sun goes down.
[00:33:23.520 --> 00:33:25.200]   There's no light, right?
[00:33:25.200 --> 00:33:26.760]   So you go to bed.
[00:33:26.760 --> 00:33:29.600]   You wake up a few hours later.
[00:33:29.600 --> 00:33:31.640]   You have a bowl.
[00:33:31.640 --> 00:33:32.760]   I don't know what you do.
[00:33:32.760 --> 00:33:34.640]   It's still dark.
[00:33:34.640 --> 00:33:36.320]   You talk.
[00:33:36.320 --> 00:33:37.000]   You're forinacate.
[00:33:37.000 --> 00:33:37.440]   I don't know.
[00:33:37.440 --> 00:33:38.000]   There's something.
[00:33:38.000 --> 00:33:38.760]   You're doing something.
[00:33:38.760 --> 00:33:39.480]   And then--
[00:33:39.480 --> 00:33:40.360]   Pain on the cave wall.
[00:33:40.360 --> 00:33:42.640]   Pain on the cave wall.
[00:33:42.640 --> 00:33:44.240]   Actually, it literally says, without a lecture,
[00:33:44.240 --> 00:33:45.560]   people went to bed at sunset.
[00:33:45.560 --> 00:33:47.680]   They woke a few hours later, relax, muse on their dreams,
[00:33:47.680 --> 00:33:49.040]   or have sex.
[00:33:49.040 --> 00:33:52.120]   Then they drift off again till sunrise.
[00:33:52.120 --> 00:33:53.360]   There's a famous--
[00:33:53.360 --> 00:33:55.280]   Who's showing this, though?
[00:33:55.280 --> 00:33:56.120]   How do we know?
[00:33:56.120 --> 00:33:56.960]   We don't know.
[00:33:56.960 --> 00:33:57.440]   We don't know.
[00:33:57.440 --> 00:33:57.720]   We did.
[00:33:57.720 --> 00:34:02.960]   In the caves, 10,000, 15, 20,000 years ago, we have no idea.
[00:34:02.960 --> 00:34:08.080]   We've always been watching a movie about what people did in the cave.
[00:34:08.080 --> 00:34:10.760]   In the '90s, a very famous study.
[00:34:10.760 --> 00:34:12.280]   No, there was a very famous sleep study
[00:34:12.280 --> 00:34:16.360]   where they gave people 14 hours of darkness.
[00:34:16.360 --> 00:34:19.160]   And then over a long period of time, months,
[00:34:19.160 --> 00:34:22.520]   and then let them get to a natural sleep cycle.
[00:34:22.520 --> 00:34:24.960]   And that, in many cases, the bi-modal cycle
[00:34:24.960 --> 00:34:26.040]   was the natural sleep.
[00:34:26.040 --> 00:34:29.080]   And I think for me-- so that's my case for why I wake up
[00:34:29.080 --> 00:34:30.240]   at 4 in the morning.
[00:34:30.240 --> 00:34:32.440]   I did a little round with my iPhone for a while.
[00:34:32.440 --> 00:34:33.920]   You did a little bit more?
[00:34:33.920 --> 00:34:34.680]   No, I don't.
[00:34:34.680 --> 00:34:35.600]   No, wrong.
[00:34:35.600 --> 00:34:36.840]   No, dirty mind.
[00:34:36.840 --> 00:34:39.800]   No, I read news.
[00:34:39.800 --> 00:34:42.240]   And actually, some of my best reading is at that hour.
[00:34:42.240 --> 00:34:42.760]   Really?
[00:34:42.760 --> 00:34:44.400]   And then about-- right, Lisa?
[00:34:44.400 --> 00:34:46.960]   And then about six or seven.
[00:34:46.960 --> 00:34:48.560]   I drift off, and I don't get up till nine.
[00:34:48.560 --> 00:34:49.680]   I sleep for a few more hours.
[00:34:49.680 --> 00:34:52.280]   And I find that very resting.
[00:34:52.280 --> 00:34:54.280]   Restful, I get work done in the middle of the night.
[00:34:54.280 --> 00:34:56.160]   That is so unique.
[00:34:56.160 --> 00:34:57.480]   I'm going to counter this with one thing.
[00:34:57.480 --> 00:35:00.880]   There was a study published in current biology back in October
[00:35:00.880 --> 00:35:02.960]   that studied basically what it asserted
[00:35:02.960 --> 00:35:06.880]   was that our ancestors didn't get as much sleep as we think they did.
[00:35:06.880 --> 00:35:09.440]   They didn't go to sleep when it went dark.
[00:35:09.440 --> 00:35:14.560]   And they studied a tribe, the Hads people in Tanzania.
[00:35:14.560 --> 00:35:15.800]   Oh, I know them well.
[00:35:15.800 --> 00:35:16.240]   Oh, yeah.
[00:35:16.240 --> 00:35:18.240]   Yeah.
[00:35:18.240 --> 00:35:18.800]   He just--
[00:35:18.800 --> 00:35:20.600]   It's just you read about them at 4 in the morning.
[00:35:20.600 --> 00:35:21.680]   I have a couple of buddies.
[00:35:21.680 --> 00:35:24.360]   I communicate with the Hads as if they were in the middle of the night.
[00:35:24.360 --> 00:35:25.280]   Yeah, yeah.
[00:35:25.280 --> 00:35:25.760]   Clearly.
[00:35:25.760 --> 00:35:26.480]   We're all up.
[00:35:26.480 --> 00:35:31.720]   Mean sleep time, 6.4 hours, which is only slightly less
[00:35:31.720 --> 00:35:32.920]   than the average American.
[00:35:32.920 --> 00:35:33.680]   But just think about it.
[00:35:33.680 --> 00:35:35.960]   What would you do if the sun-- it's dark.
[00:35:35.960 --> 00:35:37.360]   The sun goes down.
[00:35:37.360 --> 00:35:40.120]   You're not going to sleep sunset to sunrise.
[00:35:40.120 --> 00:35:41.240]   That's too much sleep.
[00:35:41.240 --> 00:35:43.880]   There are other studies, though.
[00:35:43.880 --> 00:35:46.800]   In addition to this, they had a couple of people
[00:35:46.800 --> 00:35:50.240]   who actually did go into caves and lived in caves in the dark
[00:35:50.240 --> 00:35:53.120]   for extended periods, weeks, weeks, weeks.
[00:35:53.120 --> 00:35:56.200]   And their sleep cycles completely changed to the point
[00:35:56.200 --> 00:36:02.480]   where they would sleep for sometimes up to a day or two at a time.
[00:36:02.480 --> 00:36:06.360]   But the other side of that, they'd stay up for 30 or 40 hours.
[00:36:06.360 --> 00:36:08.800]   They would stay up for the-- they completely lost.
[00:36:08.800 --> 00:36:11.560]   Their dioner cycle, which is normally your dioner
[00:36:11.560 --> 00:36:14.160]   diurnal cycle is based on sunrise sunset.
[00:36:14.160 --> 00:36:16.280]   Their dioner cycle-- I can't say it.
[00:36:16.280 --> 00:36:17.800]   Their diurnal cycle.
[00:36:17.800 --> 00:36:18.360]   Dioner cycle.
[00:36:18.360 --> 00:36:21.840]   Without the input of sunrise sunset completely changed.
[00:36:21.840 --> 00:36:24.640]   Yeah, but it's interesting, though, because there's circadian rhythm.
[00:36:24.640 --> 00:36:25.120]   That's easier to say.
[00:36:25.120 --> 00:36:26.440]   There's circadian rhythm.
[00:36:26.440 --> 00:36:28.360]   There have been numerous studies.
[00:36:28.360 --> 00:36:33.200]   We've been studying the circadian rhythm in humans, in mice, in monkeys,
[00:36:33.200 --> 00:36:35.640]   many animals for decades.
[00:36:35.640 --> 00:36:39.080]   The research goes back 1950s, 1940s.
[00:36:39.080 --> 00:36:41.240]   We've been putting people in dark rooms
[00:36:41.240 --> 00:36:43.920]   and looking to see what happens to their cycles.
[00:36:43.920 --> 00:36:49.400]   And it's interesting that just now there's this, oh, people went into a cave
[00:36:49.400 --> 00:36:53.000]   and they had this interesting result, which is not corroborated
[00:36:53.000 --> 00:36:55.840]   by any other research that I'm aware of.
[00:36:55.840 --> 00:36:56.920]   So--
[00:36:56.920 --> 00:36:58.240]   Isn't that interesting?
[00:36:58.240 --> 00:36:59.360]   No, but I remember that study.
[00:36:59.360 --> 00:37:02.240]   And I thought that was without any input.
[00:37:02.240 --> 00:37:03.920]   That's what happens in Vegas.
[00:37:03.920 --> 00:37:04.480]   Right.
[00:37:04.480 --> 00:37:05.440]   Basically.
[00:37:05.440 --> 00:37:06.640]   That's what they want to happen.
[00:37:06.640 --> 00:37:07.240]   Yeah.
[00:37:07.240 --> 00:37:07.880]   They want to just--
[00:37:07.880 --> 00:37:10.520]   Do they pump oxygen into your room in Vegas?
[00:37:10.520 --> 00:37:11.360]   That's what I've heard.
[00:37:11.360 --> 00:37:11.360]   I hope so.
[00:37:11.360 --> 00:37:11.360]   I hope so.
[00:37:11.360 --> 00:37:11.880]   I hope so.
[00:37:11.880 --> 00:37:12.640]   I hope so.
[00:37:12.640 --> 00:37:12.640]   Yeah.
[00:37:12.640 --> 00:37:13.120]   I hope so.
[00:37:13.120 --> 00:37:13.680]   Don't take it out.
[00:37:13.680 --> 00:37:15.320]   Do you remember time we'd go down and we'd do--
[00:37:15.320 --> 00:37:18.440]   [LAUGHTER]
[00:37:18.440 --> 00:37:20.800]   Probably mostly argon and nitrogen.
[00:37:20.800 --> 00:37:21.320]   No.
[00:37:21.320 --> 00:37:22.360]   We'd-- yeah, right.
[00:37:22.360 --> 00:37:27.080]   Let's just fill the room with noble gases.
[00:37:27.080 --> 00:37:28.080]   Or are those inert?
[00:37:28.080 --> 00:37:29.680]   I think they're inert.
[00:37:29.680 --> 00:37:32.560]   But no, I think we would go to Vegas to do Comdex, remember?
[00:37:32.560 --> 00:37:36.320]   And we'd be kind of wired at night.
[00:37:36.320 --> 00:37:38.880]   Again, it's hard to tell is it because there's
[00:37:38.880 --> 00:37:40.880]   just so much going on and you're stimulated,
[00:37:40.880 --> 00:37:42.720]   walking through the lobby and you're at CES
[00:37:42.720 --> 00:37:45.000]   and you got so much to work on.
[00:37:45.000 --> 00:37:47.720]   And maybe they're trying to artificially increase
[00:37:47.720 --> 00:37:48.640]   the oxygen in the room.
[00:37:48.640 --> 00:37:49.320]   I think so.
[00:37:49.320 --> 00:37:51.240]   I think they want you out of the room.
[00:37:51.240 --> 00:37:52.040]   They definitely do.
[00:37:52.040 --> 00:37:52.600]   Yeah.
[00:37:52.600 --> 00:37:53.840]   They don't give you a whole lot of--
[00:37:53.840 --> 00:37:55.800]   Kind of the cheap options for entertainment in your room.
[00:37:55.800 --> 00:37:57.520]   Well, they do now.
[00:37:57.520 --> 00:37:58.760]   Well, it depends on the hotel.
[00:37:58.760 --> 00:38:00.040]   So we were at a cheap hotel.
[00:38:00.040 --> 00:38:00.760]   There's nothing.
[00:38:00.760 --> 00:38:02.360]   There's nothing.
[00:38:02.360 --> 00:38:04.120]   Everything's-- remember the circus circus?
[00:38:04.120 --> 00:38:05.280]   Everything-- you were there.
[00:38:05.280 --> 00:38:06.480]   Everything was bolted.
[00:38:06.480 --> 00:38:07.560]   The remote control.
[00:38:07.560 --> 00:38:09.560]   Like you would go home with the hotel's remote control.
[00:38:09.560 --> 00:38:11.280]   It was bolted to the table.
[00:38:11.280 --> 00:38:13.520]   And it had a little swivel so you could aim it.
[00:38:13.520 --> 00:38:15.400]   And the lamp was bolted to the table.
[00:38:15.400 --> 00:38:17.840]   Like you would take the lamp and the remote control home
[00:38:17.840 --> 00:38:20.120]   with you if they didn't bolt it down.
[00:38:20.120 --> 00:38:21.840]   Everything was bolted down.
[00:38:21.840 --> 00:38:23.640]   And then there's nothing to do in the room.
[00:38:23.640 --> 00:38:24.440]   You can't open the window.
[00:38:24.440 --> 00:38:26.120]   You can't-- I mean, you could barely turn on the TV
[00:38:26.120 --> 00:38:27.400]   and there's certainly no mini bar.
[00:38:27.400 --> 00:38:28.040]   Yeah.
[00:38:28.040 --> 00:38:28.480]   You're going to leave--
[00:38:28.480 --> 00:38:29.440]   Maybe they--
[00:38:29.440 --> 00:38:30.040]   You're going to leave the lights?
[00:38:30.040 --> 00:38:31.880]   They watched the jerk too many times.
[00:38:31.880 --> 00:38:34.200]   You know, Steve Martin going, and I'll take this lamp
[00:38:34.200 --> 00:38:36.320]   in this chair.
[00:38:36.320 --> 00:38:38.080]   Anyway, I'm sorry.
[00:38:38.080 --> 00:38:39.680]   I got very distracted by sleep.
[00:38:39.680 --> 00:38:40.200]   Yeah.
[00:38:40.200 --> 00:38:41.640]   But it's fascinating to me, I think.
[00:38:41.640 --> 00:38:43.240]   Well, let's talk about Elon Musk.
[00:38:43.240 --> 00:38:48.520]   So there's a new organization, OpenAI.
[00:38:48.520 --> 00:38:52.120]   And the idea is Elon Musk funded this along
[00:38:52.120 --> 00:38:53.480]   with Reid Hoffman of LinkedIn.
[00:38:53.480 --> 00:38:55.480]   Peter Teal, the financier, the guy
[00:38:55.480 --> 00:38:58.080]   who put the initial investment into Facebook.
[00:38:58.080 --> 00:39:00.000]   I don't know who Jessica Livingston is.
[00:39:00.000 --> 00:39:00.480]   I don't know.
[00:39:00.480 --> 00:39:01.080]   I just saw that.
[00:39:01.080 --> 00:39:01.920]   I was surprised.
[00:39:01.920 --> 00:39:03.320]   Amazon Web Services.
[00:39:03.320 --> 00:39:06.360]   Collectively, they've pledged more than a billion dollars
[00:39:06.360 --> 00:39:08.040]   over a period of time.
[00:39:08.040 --> 00:39:11.160]   The co-chairs will be Musk and the CEO of Y Combinator,
[00:39:11.160 --> 00:39:12.920]   Sam Altman.
[00:39:12.920 --> 00:39:17.720]   And the idea is to open source AI artificial intelligence
[00:39:17.720 --> 00:39:22.480]   research so that Google, Microsoft, Apple, Uber
[00:39:22.480 --> 00:39:25.480]   don't kind of own this as a proprietary thing.
[00:39:25.480 --> 00:39:27.480]   She's a founding partner of Y Combinator.
[00:39:27.480 --> 00:39:29.760]   OK, there you go.
[00:39:29.760 --> 00:39:33.120]   Y Combinator is a very famous startup school
[00:39:33.120 --> 00:39:35.880]   and angel investment company.
[00:39:35.880 --> 00:39:37.480]   She looks sort of like her as a mire.
[00:39:37.480 --> 00:39:38.400]   She does, doesn't she?
[00:39:38.400 --> 00:39:39.560]   Mire mire, I never know.
[00:39:39.560 --> 00:39:40.720]   Everybody says it differently.
[00:39:40.720 --> 00:39:41.800]   I work at Yahoo.
[00:39:41.800 --> 00:39:42.440]   We'll get there.
[00:39:42.440 --> 00:39:43.960]   You work at Yahoo, I'm going to find out.
[00:39:43.960 --> 00:39:45.200]   We're going to get the inside scoop.
[00:39:45.200 --> 00:39:45.720]   Oh, yeah.
[00:39:45.720 --> 00:39:47.080]   I want to know the details, the deeds.
[00:39:47.080 --> 00:39:48.360]   We got to get there.
[00:39:48.360 --> 00:39:50.160]   So what do you think?
[00:39:50.160 --> 00:39:51.440]   Is this a good idea?
[00:39:51.440 --> 00:39:53.560]   I mean, I like the idea that no one company should
[00:39:53.560 --> 00:39:53.960]   own this.
[00:39:53.960 --> 00:39:55.800]   They say any patents we come up with
[00:39:55.800 --> 00:39:57.600]   will be royalty free.
[00:39:57.600 --> 00:40:00.080]   All our results will be public.
[00:40:00.080 --> 00:40:01.960]   So on that side, I think it's good.
[00:40:01.960 --> 00:40:04.360]   The other side though, I'm a little worried about too much
[00:40:04.360 --> 00:40:07.000]   money being pumped into AI research.
[00:40:07.000 --> 00:40:09.080]   But this is a standardizing it.
[00:40:09.080 --> 00:40:10.280]   No, but if it's going to happen--
[00:40:10.280 --> 00:40:11.800]   It's not going to be a finite.
[00:40:11.800 --> 00:40:13.160]   Why not?
[00:40:13.160 --> 00:40:20.280]   Because it's not going to be a single incident of AI.
[00:40:20.280 --> 00:40:23.040]   What they're doing is they're by making it open
[00:40:23.040 --> 00:40:25.200]   and everything open source, it's going
[00:40:25.200 --> 00:40:28.840]   to allow anybody to be able to hack on it, anybody
[00:40:28.840 --> 00:40:33.280]   to implement new ideas, to be creative,
[00:40:33.280 --> 00:40:35.720]   to actually make a new product off of the backbone
[00:40:35.720 --> 00:40:36.520]   of what they've created.
[00:40:36.520 --> 00:40:40.160]   So instead of having one Skynet that controls everything,
[00:40:40.160 --> 00:40:42.560]   we're going to have little tiny AI's popping up
[00:40:42.560 --> 00:40:44.200]   all over the place.
[00:40:44.200 --> 00:40:46.960]   So Stephen Levy, who wrote this article we're referring to
[00:40:46.960 --> 00:40:51.640]   on Medium, that's his now, his beat, his publication.
[00:40:51.640 --> 00:40:54.600]   Stephen Levy asks, Elon, if I'm Dr. Evil--
[00:40:54.600 --> 00:40:55.440]   [LAUGHTER]
[00:40:55.440 --> 00:40:58.800]   And I use it, won't you be overpowering me?
[00:40:58.800 --> 00:41:00.360]   Musk says, well, that's an excellent question.
[00:41:00.360 --> 00:41:02.000]   It's something we've debated quite a bit.
[00:41:02.000 --> 00:41:03.360]   So they two are worried about this.
[00:41:03.360 --> 00:41:06.000]   Altman says, there are a few different thoughts about this.
[00:41:06.000 --> 00:41:08.240]   Just like humans protect against Dr. Evil
[00:41:08.240 --> 00:41:10.080]   by the fact that most humans are good.
[00:41:10.080 --> 00:41:10.720]   Uh-oh.
[00:41:10.720 --> 00:41:11.200]   Well, I'm sorry.
[00:41:11.200 --> 00:41:13.840]   If they're relying on the most humans are good thesis,
[00:41:13.840 --> 00:41:14.600]   I don't know.
[00:41:14.600 --> 00:41:16.560]   And the collective force of humanity
[00:41:16.560 --> 00:41:18.600]   can contain the bad elements.
[00:41:18.600 --> 00:41:21.400]   We can't even contain Donald Trump.
[00:41:21.400 --> 00:41:24.120]   We think it's far more likely that many, many AI's
[00:41:24.120 --> 00:41:25.840]   will work to stop the occasional--
[00:41:25.840 --> 00:41:27.480]   oh, this is what you're saying, Kiki.
[00:41:27.480 --> 00:41:28.040]   It's going to be--
[00:41:28.040 --> 00:41:29.040]   It's going to be competition.
[00:41:29.040 --> 00:41:30.240]   We'll work to stop the occasional bad actors
[00:41:30.240 --> 00:41:33.440]   and the idea that there is a single AI a billion times more
[00:41:33.440 --> 00:41:36.720]   powerful than anything else is not going to happen.
[00:41:36.720 --> 00:41:37.240]   Right.
[00:41:37.240 --> 00:41:39.800]   Well, and the other part of this I think is really interesting
[00:41:39.800 --> 00:41:41.960]   is that as AI starts to take off,
[00:41:41.960 --> 00:41:45.040]   we're seeing people go right to open source,
[00:41:45.040 --> 00:41:48.320]   unlike operating systems, where it came along later.
[00:41:48.320 --> 00:41:49.840]   So you have Google open sourcing,
[00:41:49.840 --> 00:41:52.680]   their TensorFlow, deep learning software.
[00:41:52.680 --> 00:41:55.800]   You have Facebook open sourcing the design
[00:41:55.800 --> 00:41:58.080]   of their deep learning machines, which you could then
[00:41:58.080 --> 00:42:00.920]   run the TensorFlow stuff on and then take this and run on.
[00:42:00.920 --> 00:42:04.200]   So everybody is kind of pitching in and saying,
[00:42:04.200 --> 00:42:06.320]   let's do this as openly as possible,
[00:42:06.320 --> 00:42:07.680]   because we all benefit from it.
[00:42:07.680 --> 00:42:12.000]   Although neither of these Google or Facebook projects
[00:42:12.000 --> 00:42:15.000]   will be used by open AI, they're going to go over their own route
[00:42:15.000 --> 00:42:16.560]   and start from the ground up.
[00:42:16.560 --> 00:42:17.560]   Right, which is great.
[00:42:17.560 --> 00:42:18.080]   Yeah.
[00:42:18.080 --> 00:42:21.320]   Tom, is that-- do you think altruism or intelligence
[00:42:21.320 --> 00:42:23.760]   around deployment from the start,
[00:42:23.760 --> 00:42:26.480]   sort of a design ethos in the beginning?
[00:42:26.480 --> 00:42:28.840]   Or is that because the business model is just not clear?
[00:42:28.840 --> 00:42:32.520]   Therefore, the competitive spirit is not as high to protect IP?
[00:42:32.520 --> 00:42:34.280]   Yeah, it's probably a little of both, right?
[00:42:34.280 --> 00:42:36.200]   I mean, I know Google and Facebook both
[00:42:36.200 --> 00:42:37.920]   made a calculated decision to say,
[00:42:37.920 --> 00:42:40.840]   we'll probably make more money if we open source this,
[00:42:40.840 --> 00:42:42.360]   because it'll progress faster.
[00:42:42.360 --> 00:42:44.400]   But that's part of what drives open source development.
[00:42:44.400 --> 00:42:46.800]   I don't think TensorFlow is Google's state of the art
[00:42:46.800 --> 00:42:48.080]   is what I think.
[00:42:48.080 --> 00:42:50.200]   I think they gave away their old stuff.
[00:42:50.200 --> 00:42:51.720]   And open source that.
[00:42:51.720 --> 00:42:53.080]   And of course, remember, one of the things
[00:42:53.080 --> 00:42:54.600]   that you get when you open source something
[00:42:54.600 --> 00:42:57.120]   is contributors from outside Google,
[00:42:57.120 --> 00:42:59.360]   putting contributions back into it.
[00:42:59.360 --> 00:43:00.200]   Yeah, that's the whole--
[00:43:00.200 --> 00:43:03.960]   I fully believe that Google has their own proprietary stuff
[00:43:03.960 --> 00:43:05.040]   that they're not releasing.
[00:43:05.040 --> 00:43:06.360]   And that's pretty much how Google does.
[00:43:06.360 --> 00:43:07.040]   Very likely.
[00:43:07.040 --> 00:43:08.880]   Yeah.
[00:43:08.880 --> 00:43:10.680]   Elon Musk was one of the people who said,
[00:43:10.680 --> 00:43:12.400]   we've got to pay attention to AI.
[00:43:12.400 --> 00:43:16.520]   This is potentially very risky.
[00:43:16.520 --> 00:43:18.840]   We've all seen maybe too many movies
[00:43:18.840 --> 00:43:20.440]   like The Matrix and Terminator.
[00:43:20.440 --> 00:43:25.280]   And maybe going back to RUR, the very first movie
[00:43:25.280 --> 00:43:28.040]   about robots in the '20s, which was about robots taking over
[00:43:28.040 --> 00:43:28.720]   the world.
[00:43:28.720 --> 00:43:29.920]   Shopping, right?
[00:43:29.920 --> 00:43:30.880]   Yeah.
[00:43:30.880 --> 00:43:31.880]   I mean, this is not new.
[00:43:31.880 --> 00:43:33.240]   We've always been worried about that.
[00:43:33.240 --> 00:43:39.280]   And yet, I do feel like this is potentially a risk.
[00:43:39.280 --> 00:43:44.000]   I once asked Ray Kurzweil, who's now
[00:43:44.000 --> 00:43:47.680]   at Google, great artificial intelligence researcher,
[00:43:47.680 --> 00:43:50.400]   why should we trust these AIs?
[00:43:50.400 --> 00:43:53.280]   And he said, because they'll think of us as their parents,
[00:43:53.280 --> 00:43:57.560]   their progenitors, and so they will respect and protect us.
[00:43:57.560 --> 00:43:59.640]   But again, I don't find that compelling.
[00:43:59.640 --> 00:44:02.200]   I was talking to Paul Sappho at all.
[00:44:02.200 --> 00:44:03.680]   Yeah.
[00:44:03.680 --> 00:44:05.680]   Paul Sappho had a different philosophy about this.
[00:44:05.680 --> 00:44:06.760]   We were talking about it this week.
[00:44:06.760 --> 00:44:07.240]   We were talking about--
[00:44:07.240 --> 00:44:08.240]   The Institute for the Future.
[00:44:08.240 --> 00:44:08.740]   Yeah.
[00:44:08.740 --> 00:44:10.560]   We were talking about the future of tech for this--
[00:44:10.560 --> 00:44:12.200]   I was doing a piece for GMA about that.
[00:44:12.200 --> 00:44:13.720]   And he said, actually, he thinks
[00:44:13.720 --> 00:44:16.200]   that AI will fundamentally--
[00:44:16.200 --> 00:44:19.800]   AI devices will think of us as their pets, most likely.
[00:44:19.800 --> 00:44:20.240]   Yeah.
[00:44:20.240 --> 00:44:21.240]   That seems really--
[00:44:21.240 --> 00:44:22.520]   I mean, I think that's a little better.
[00:44:22.520 --> 00:44:25.040]   But you kill your pet if you get hungry, right?
[00:44:25.040 --> 00:44:26.560]   Yeah.
[00:44:26.560 --> 00:44:27.440]   Very likely.
[00:44:27.440 --> 00:44:28.200]   Or vice versa.
[00:44:28.200 --> 00:44:29.200]   Or it.
[00:44:29.200 --> 00:44:29.720]   Yeah.
[00:44:29.720 --> 00:44:30.280]   What?
[00:44:30.280 --> 00:44:34.200]   I know our cats are thinking any day now.
[00:44:34.200 --> 00:44:36.400]   Just a matter of time.
[00:44:36.400 --> 00:44:38.360]   They'll wipe out on their segues and they'll be loaded
[00:44:38.360 --> 00:44:38.840]   to ground.
[00:44:38.840 --> 00:44:39.880]   I mean, if that's always--
[00:44:39.880 --> 00:44:40.680]   You get the eyes.
[00:44:40.680 --> 00:44:41.840]   I'll eat the tongue.
[00:44:41.840 --> 00:44:43.880]   The pattern is always--
[00:44:43.880 --> 00:44:46.400]   We don't know what this technology will be used for.
[00:44:46.400 --> 00:44:48.720]   We don't know how powerful it might end up being.
[00:44:48.720 --> 00:44:51.000]   It's got the potential to do damage.
[00:44:51.000 --> 00:44:52.400]   We should be careful with it.
[00:44:52.400 --> 00:44:55.760]   And I think that's good and sensible.
[00:44:55.760 --> 00:44:57.680]   But I mean, we've developed things
[00:44:57.680 --> 00:44:59.280]   that can destroy our entire planet.
[00:44:59.280 --> 00:45:02.120]   And I'm not saying that that makes it OK to do anything.
[00:45:02.120 --> 00:45:07.680]   But maybe this is how we learn as a species
[00:45:07.680 --> 00:45:09.480]   not to destroy ourselves, is to keep
[00:45:09.480 --> 00:45:11.960]   trying on these sorts of things.
[00:45:11.960 --> 00:45:14.680]   Yeah. And over and over again, we've
[00:45:14.680 --> 00:45:17.680]   seen that we've done OK with technologies,
[00:45:17.680 --> 00:45:19.800]   that we have actually--
[00:45:19.800 --> 00:45:22.400]   that the internet has allowed people
[00:45:22.400 --> 00:45:27.320]   to connect all over the world, do business, to share ideas.
[00:45:27.320 --> 00:45:30.160]   It has improved society greatly.
[00:45:30.160 --> 00:45:31.760]   I mean, I don't know if you can put a number on it.
[00:45:31.760 --> 00:45:34.400]   That's fair.
[00:45:34.400 --> 00:45:35.240]   We're doing good things.
[00:45:35.240 --> 00:45:36.680]   How about nuclear bombs?
[00:45:36.680 --> 00:45:39.400]   I was just going to say, Leo, where's this groany skepticism
[00:45:39.400 --> 00:45:40.040]   coming from?
[00:45:40.040 --> 00:45:41.440]   This kind of change.
[00:45:41.440 --> 00:45:43.960]   So here's my actual genuine question, which is,
[00:45:43.960 --> 00:45:47.880]   is it just a science fiction version of AI that scares us?
[00:45:47.880 --> 00:45:52.040]   Or is it legitimate to think that, A, it's even possible
[00:45:52.040 --> 00:45:55.720]   to create a computer program that thinks like a human,
[00:45:55.720 --> 00:45:57.280]   or has its own--
[00:45:57.280 --> 00:46:00.440]   let's say sentience and its own will,
[00:46:00.440 --> 00:46:01.640]   because that's what would be scary.
[00:46:01.640 --> 00:46:03.520]   Is that even possible?
[00:46:03.520 --> 00:46:06.600]   I mean, I don't want to buy into some science fiction that
[00:46:06.600 --> 00:46:08.040]   isn't even possible.
[00:46:08.040 --> 00:46:09.040]   And worry about that.
[00:46:09.040 --> 00:46:09.840]   Of course it will.
[00:46:09.840 --> 00:46:11.880]   People like Stephen Hawking and Elon Musk
[00:46:11.880 --> 00:46:14.400]   are saying it might be possible.
[00:46:14.400 --> 00:46:16.600]   And even Ray Kurzweil is saying it might be possible.
[00:46:16.600 --> 00:46:19.960]   So the fact that it might be possible makes me say, yes,
[00:46:19.960 --> 00:46:23.200]   we should be careful, but not that we shouldn't investigate it.
[00:46:23.200 --> 00:46:24.120]   Well, here's the question.
[00:46:24.120 --> 00:46:26.600]   So we were talking about service robots earlier this week,
[00:46:26.600 --> 00:46:29.520]   and that the success of service robots is going to be--
[00:46:29.520 --> 00:46:30.240]   is going to come--
[00:46:30.240 --> 00:46:31.720]   Have you watched humans?
[00:46:31.720 --> 00:46:32.040]   No.
[00:46:32.040 --> 00:46:33.400]   Oh, what a great show.
[00:46:33.400 --> 00:46:34.080]   I'm a new mother.
[00:46:34.080 --> 00:46:36.560]   I haven't seen a movie in 10 years.
[00:46:36.560 --> 00:46:37.120]   This you can watch.
[00:46:37.120 --> 00:46:39.360]   It's a TV show you've seen this, Tom.
[00:46:39.360 --> 00:46:42.760]   Wonderful TV show about a service robots.
[00:46:42.760 --> 00:46:43.240]   Right.
[00:46:43.240 --> 00:46:45.560]   And they're human-oid.
[00:46:45.560 --> 00:46:46.000]   And they have--
[00:46:46.000 --> 00:46:46.840]   Is it a TV show?
[00:46:46.840 --> 00:46:48.040]   It's massively good.
[00:46:48.040 --> 00:46:48.480]   It's our TV show.
[00:46:48.480 --> 00:46:49.760]   I didn't even know it was a TV show.
[00:46:49.760 --> 00:46:50.520]   I thought it was a movie.
[00:46:50.520 --> 00:46:51.440]   Like that's how--
[00:46:51.440 --> 00:46:52.240]   OK, it's a few--
[00:46:52.240 --> 00:46:52.680]   Fabulous.
[00:46:52.680 --> 00:46:54.000]   OK, now I'm with you.
[00:46:54.000 --> 00:46:57.840]   And it's interesting, because well, there's
[00:46:57.840 --> 00:47:00.240]   a lot of implications that are raised by these.
[00:47:00.240 --> 00:47:02.640]   Well, the point was is that they're only
[00:47:02.640 --> 00:47:05.800]   going to succeed when we stop anthropomorphizing them.
[00:47:05.800 --> 00:47:06.800]   And that if we stop--
[00:47:06.800 --> 00:47:08.760]   Well, that's the mistake humans made.
[00:47:08.760 --> 00:47:09.880]   They look just like humans.
[00:47:09.880 --> 00:47:10.240]   Right.
[00:47:10.240 --> 00:47:13.520]   And therefore, our expectation is so high that they will--
[00:47:13.520 --> 00:47:14.680]   It's just AMC, not BBC.
[00:47:14.680 --> 00:47:17.120]   That they'll meet human standards.
[00:47:17.120 --> 00:47:19.440]   And maybe that's what's so wrong about this discussion
[00:47:19.440 --> 00:47:22.200]   about AI is not that they become human-like,
[00:47:22.200 --> 00:47:24.920]   but that they employ pieces of intelligence
[00:47:24.920 --> 00:47:26.240]   that meet our needs.
[00:47:26.240 --> 00:47:28.880]   And if we see them starting to exceed that in some way
[00:47:28.880 --> 00:47:32.640]   that's scary, that it'll still be so nascent
[00:47:32.640 --> 00:47:34.960]   that we can stop it and change it.
[00:47:34.960 --> 00:47:35.920]   You don't think so?
[00:47:35.920 --> 00:47:37.520]   You think it'll just snowball?
[00:47:37.520 --> 00:47:40.000]   So one of the things Kerzweil talks about
[00:47:40.000 --> 00:47:43.720]   is the singularity, that moment, that point in time
[00:47:43.720 --> 00:47:48.720]   when human intelligence is indistinguishable
[00:47:48.720 --> 00:47:51.000]   from human intelligence-- he makes it clear
[00:47:51.000 --> 00:47:53.400]   that he's not saying it will be the same as,
[00:47:53.400 --> 00:47:54.960]   but that we won't be able to tell the difference.
[00:47:54.960 --> 00:47:57.040]   It'll be indistinguishable.
[00:47:57.040 --> 00:48:00.360]   If that happens, what's the next logical step
[00:48:00.360 --> 00:48:01.840]   for this artificial intelligence
[00:48:01.840 --> 00:48:05.920]   is to start to take over design and construction
[00:48:05.920 --> 00:48:07.240]   of artificial intelligence.
[00:48:07.240 --> 00:48:08.880]   Because they'll do as good a job
[00:48:08.880 --> 00:48:10.880]   and then rapidly a much better job.
[00:48:10.880 --> 00:48:14.520]   And then geometrically and infinitely better job than we do.
[00:48:14.520 --> 00:48:17.080]   And at that point, it doesn't matter what we say, I think.
[00:48:17.080 --> 00:48:20.160]   We become those piece kind of chubby people in Walley.
[00:48:20.160 --> 00:48:24.600]   Yeah, but here once again, we're jumping to that end,
[00:48:24.600 --> 00:48:26.800]   that scary end point.
[00:48:26.800 --> 00:48:29.560]   When actually artificial intelligence,
[00:48:29.560 --> 00:48:31.600]   there are many, many different kinds.
[00:48:31.600 --> 00:48:35.840]   And like you said, Becky, different kinds of intelligence,
[00:48:35.840 --> 00:48:42.280]   that we program in what we want a particular AI to have
[00:48:42.280 --> 00:48:43.440]   for our needs.
[00:48:43.440 --> 00:48:47.840]   Does a service robot need to have artificial intelligence
[00:48:47.840 --> 00:48:50.000]   that contains emotions?
[00:48:50.000 --> 00:48:54.600]   Do we program in some kind of emotional social connectivity
[00:48:54.600 --> 00:48:55.640]   to that robot?
[00:48:55.640 --> 00:48:57.720]   Or is it just for working?
[00:48:57.720 --> 00:49:01.960]   Is it just AI to be able to notice when drugs need to be given,
[00:49:01.960 --> 00:49:05.160]   when care needs to be given of some form?
[00:49:05.160 --> 00:49:06.560]   And so that's the question.
[00:49:06.560 --> 00:49:07.680]   What are we designing for?
[00:49:07.680 --> 00:49:09.160]   What are we creating?
[00:49:09.160 --> 00:49:13.600]   Do we need to create this human-like intelligence?
[00:49:13.600 --> 00:49:16.560]   I mean, this is just people playing God, you know?
[00:49:16.560 --> 00:49:18.360]   But that's never stopped us before, right?
[00:49:18.360 --> 00:49:20.160]   No, of course it's not going to stop us.
[00:49:20.160 --> 00:49:21.200]   It's pretty cool, right?
[00:49:21.200 --> 00:49:22.040]   It wouldn't be cool.
[00:49:22.040 --> 00:49:24.040]   Don't you want Hal 9000?
[00:49:24.040 --> 00:49:25.360]   Don't you?
[00:49:25.360 --> 00:49:27.960]   Right at the point where it locks you out of the pod bay door.
[00:49:27.960 --> 00:49:29.360]   I think Kiki's making a good point,
[00:49:29.360 --> 00:49:31.360]   which is there's a couple of different questions.
[00:49:31.360 --> 00:49:32.480]   There are loads of different questions.
[00:49:32.480 --> 00:49:35.200]   But should we investigate this at all?
[00:49:35.200 --> 00:49:36.680]   Is like biomedical research.
[00:49:36.680 --> 00:49:41.640]   We play around with viruses and dangerous infectious agents
[00:49:41.640 --> 00:49:45.120]   because we think we can make something good for humanity out of it,
[00:49:45.120 --> 00:49:47.800]   even though it's dangerous to research on those.
[00:49:47.800 --> 00:49:50.280]   And I think that question is, yes, we should investigate
[00:49:50.280 --> 00:49:52.600]   deep learning, machine learning, artificial intelligence
[00:49:52.600 --> 00:49:53.440]   for those reasons.
[00:49:53.440 --> 00:49:55.560]   Then there's also the jumping to the end of like, yeah,
[00:49:55.560 --> 00:49:57.560]   but what if we end up making something that
[00:49:57.560 --> 00:49:59.600]   is a different kind of intelligence
[00:49:59.600 --> 00:50:01.120]   and can get off on its own?
[00:50:01.120 --> 00:50:03.040]   Will it be more intelligent than us?
[00:50:03.040 --> 00:50:04.360]   And will we let it take over?
[00:50:04.360 --> 00:50:07.800]   That's a different set of philosophical questions there, too.
[00:50:07.800 --> 00:50:08.480]   We interviewed.
[00:50:08.480 --> 00:50:10.680]   I've interviewed a number of people on this subject,
[00:50:10.680 --> 00:50:15.000]   including people who are very advanced researchers in this field.
[00:50:15.000 --> 00:50:17.400]   In fact, I interviewed one guy on triagulation
[00:50:17.400 --> 00:50:20.480]   who is an ethicist, a AI ethicist.
[00:50:20.480 --> 00:50:22.440]   His background is fascinating.
[00:50:22.440 --> 00:50:24.880]   I mean, philosophy, but also science.
[00:50:24.880 --> 00:50:28.720]   And he says, we don't even have an ethical framework
[00:50:28.720 --> 00:50:29.520]   for this at this point.
[00:50:29.520 --> 00:50:33.400]   He says, we need to do the groundwork now to even think about.
[00:50:33.400 --> 00:50:35.640]   He says, as an example, many people quote
[00:50:35.640 --> 00:50:39.440]   Asimov's rules for robots, rules for robotics,
[00:50:39.440 --> 00:50:40.880]   as a way to protect ourselves.
[00:50:40.880 --> 00:50:44.960]   He says, it's logically flawed.
[00:50:44.960 --> 00:50:46.120]   The whole thing is ridiculous.
[00:50:46.120 --> 00:50:48.000]   It's a fictional creation, and it's not
[00:50:48.000 --> 00:50:49.920]   going to protect us in any way.
[00:50:49.920 --> 00:50:51.600]   We need to do much, much better, and we
[00:50:51.600 --> 00:50:55.520]   need to start doing it now before we do the research.
[00:50:55.520 --> 00:50:56.080]   Right.
[00:50:56.080 --> 00:50:59.400]   And this is something that's not just in this field of technology
[00:50:59.400 --> 00:51:01.960]   for artificial intelligence, but also gene editing,
[00:51:01.960 --> 00:51:04.680]   whether or not we're going to be using the new CRISPR
[00:51:04.680 --> 00:51:08.080]   technology to be allowing us to edit human DNA
[00:51:08.080 --> 00:51:10.280]   at some point in the future.
[00:51:10.280 --> 00:51:12.680]   Recently, there was a summit in Washington, DC,
[00:51:12.680 --> 00:51:15.920]   where experts from all over the world, from China, from India,
[00:51:15.920 --> 00:51:18.400]   from Europe, from the United States,
[00:51:18.400 --> 00:51:22.960]   all came together to talk about what they thought was OK and not
[00:51:22.960 --> 00:51:23.680]   OK.
[00:51:23.680 --> 00:51:26.800]   And basically, everyone came out of it, going, all right,
[00:51:26.800 --> 00:51:33.760]   for now, we all promise not to mess with human DNA for now.
[00:51:33.760 --> 00:51:35.080]   Don't design your babies.
[00:51:35.080 --> 00:51:36.280]   No designer babies.
[00:51:36.280 --> 00:51:36.800]   In the CRISPR.
[00:51:36.800 --> 00:51:38.920]   Now, yeah.
[00:51:38.920 --> 00:51:39.760]   We have--
[00:51:39.760 --> 00:51:41.800]   It's the conversations that need to be happening.
[00:51:41.800 --> 00:51:43.480]   And so it's like getting the ethicists
[00:51:43.480 --> 00:51:45.680]   with the technologists and the engineers
[00:51:45.680 --> 00:51:49.680]   and getting everyone together to decide what they want to do.
[00:51:49.680 --> 00:51:52.000]   The reason I'm fascinated by this is we're really on the cusp,
[00:51:52.000 --> 00:51:54.000]   I think, of all this we had yesterday.
[00:51:54.000 --> 00:52:00.120]   A dentist on-- well, he's a researcher in dentist,
[00:52:00.120 --> 00:52:01.240]   or teeth or something.
[00:52:01.240 --> 00:52:02.800]   I don't remember.
[00:52:02.800 --> 00:52:06.200]   But he's talking about stem cells, using adult stem cells
[00:52:06.200 --> 00:52:08.800]   for regenerating your teeth in your mouth, growing teeth.
[00:52:08.800 --> 00:52:09.320]   Wow.
[00:52:09.320 --> 00:52:10.320]   And we're actually getting closer.
[00:52:10.320 --> 00:52:12.440]   It turns out that's the first step to growing organs.
[00:52:12.440 --> 00:52:14.320]   Teeth are easier-- they're organs,
[00:52:14.320 --> 00:52:16.720]   but they're a little easier than a liver.
[00:52:16.720 --> 00:52:21.280]   But we're surprisingly close to doing that.
[00:52:21.280 --> 00:52:23.640]   That was actually a fascinating interview yesterday
[00:52:23.640 --> 00:52:26.480]   on the new screensabers.
[00:52:26.480 --> 00:52:29.880]   We've done so many interviews on triangulation on AI.
[00:52:29.880 --> 00:52:31.520]   James Barrett, who was a journalist,
[00:52:31.520 --> 00:52:35.080]   but wrote a book called AI in the End of the Human Era.
[00:52:35.080 --> 00:52:40.280]   And he was very concerned and negative about this.
[00:52:40.280 --> 00:52:42.520]   But then I'm trying to find the name of the fellow
[00:52:42.520 --> 00:52:44.520]   we talked with, who's an academic, who's actually
[00:52:44.520 --> 00:52:46.840]   one of the few AI ethicists.
[00:52:46.840 --> 00:52:48.600]   And he wants to really grow that field.
[00:52:48.600 --> 00:52:52.040]   Because he says, this is where we should be starting.
[00:52:52.040 --> 00:52:52.400]   Before we--
[00:52:52.400 --> 00:52:53.200]   I think that's right.
[00:52:53.200 --> 00:52:55.960]   I mean, again, go back to my parallel to biomedical.
[00:52:55.960 --> 00:52:58.880]   Like, you don't want to research infectious agents
[00:52:58.880 --> 00:53:00.840]   without clear rules about what you're doing
[00:53:00.840 --> 00:53:01.680]   and how you handle them.
[00:53:01.680 --> 00:53:02.680]   Yeah, exactly.
[00:53:02.680 --> 00:53:06.960]   There's a presumption, just as we're anthropomorphizing robots,
[00:53:06.960 --> 00:53:09.360]   there's a presumption that they'll have our same desire
[00:53:09.360 --> 00:53:10.760]   for supremacy.
[00:53:10.760 --> 00:53:11.840]   And I'm not sure--
[00:53:11.840 --> 00:53:12.320]   Maybe not.
[00:53:12.320 --> 00:53:13.160]   That's an assumption that--
[00:53:13.160 --> 00:53:14.160]   Maybe we should be content to help.
[00:53:14.160 --> 00:53:14.880]   --that is an assumption.
[00:53:14.880 --> 00:53:15.880]   I'm not sure it's clear.
[00:53:15.880 --> 00:53:17.920]   There's an assumption there'll be only one kind of AI.
[00:53:17.920 --> 00:53:19.400]   There'll be all kinds.
[00:53:19.400 --> 00:53:20.400]   Here's what Barrett said.
[00:53:20.400 --> 00:53:22.960]   Or somebody said, maybe no-no is Jeff Hawkins, who's also
[00:53:22.960 --> 00:53:25.200]   doing this kind of research, the founder of graffiti
[00:53:25.200 --> 00:53:27.200]   and where he ran Palm.
[00:53:27.200 --> 00:53:28.880]   He's currently working on exactly this
[00:53:28.880 --> 00:53:31.840]   to create a microprocessor that can duplicate
[00:53:31.840 --> 00:53:34.480]   the massively parallel process of the human brain.
[00:53:34.480 --> 00:53:37.520]   He says, that's a better path than a von Neumann machine,
[00:53:37.520 --> 00:53:40.080]   a deterministic machine as our current computers are.
[00:53:40.080 --> 00:53:41.960]   His company, Namenta, is working on that.
[00:53:41.960 --> 00:53:47.480]   He said, nothing to worry about, just don't let them replicate.
[00:53:47.480 --> 00:53:49.680]   But that is an important point.
[00:53:49.680 --> 00:53:50.200]   Yeah.
[00:53:50.200 --> 00:53:51.880]   The AI that designs the AI.
[00:53:51.880 --> 00:53:56.360]   Yeah, because as soon as they can replicate--
[00:53:56.360 --> 00:54:01.600]   it's my opinion that that was the signal moment
[00:54:01.600 --> 00:54:05.360]   in human evolution was not the creation of life.
[00:54:05.360 --> 00:54:07.040]   It predates that.
[00:54:07.040 --> 00:54:09.760]   It's when the first replicator was created.
[00:54:09.760 --> 00:54:12.160]   Because as soon as something-- and it doesn't have to be alive--
[00:54:12.160 --> 00:54:15.360]   some physical process is created that's self-replicating--
[00:54:15.360 --> 00:54:19.560]   it will start competing with other processes for resources.
[00:54:19.560 --> 00:54:24.440]   The one that is the most efficient will succeed and regenerate.
[00:54:24.440 --> 00:54:28.040]   And in fact, if you follow that logic all you get life,
[00:54:28.040 --> 00:54:30.720]   you get diversification.
[00:54:30.720 --> 00:54:33.960]   You get the organizing principle of evolution.
[00:54:33.960 --> 00:54:35.120]   Of evolution?
[00:54:35.120 --> 00:54:35.640]   It starts--
[00:54:35.640 --> 00:54:36.960]   --speaking my language here, Leo.
[00:54:36.960 --> 00:54:39.160]   It starts with replication.
[00:54:39.160 --> 00:54:42.360]   And so you don't have to say they're malicious.
[00:54:42.360 --> 00:54:46.040]   You merely have to say, if they started somehow
[00:54:46.040 --> 00:54:48.120]   to replicate themselves, we'd be in deep trouble.
[00:54:48.120 --> 00:54:50.640]   Stick them on the blockchain.
[00:54:50.640 --> 00:54:51.480]   No replication.
[00:54:51.480 --> 00:54:52.080]   No where you are.
[00:54:52.080 --> 00:54:53.080]   No replication.
[00:54:53.080 --> 00:54:57.560]   I've got-- I got this book in the mail from the editor of edge.org.
[00:54:57.560 --> 00:55:00.200]   And it's what to think about machines that think.
[00:55:00.200 --> 00:55:06.200]   And it's a whole bunch of short essays by thinkers.
[00:55:06.200 --> 00:55:06.640]   Yeah.
[00:55:06.640 --> 00:55:07.280]   Thinkers, thinking about--
[00:55:07.280 --> 00:55:08.440]   I have to read that.
[00:55:08.440 --> 00:55:09.880]   That looks really interesting.
[00:55:09.880 --> 00:55:10.880]   Yeah.
[00:55:10.880 --> 00:55:11.760]   It's really good.
[00:55:11.760 --> 00:55:13.600]   It's some really neat reading in there.
[00:55:13.600 --> 00:55:15.560]   This is what Twitter's at its best
[00:55:15.560 --> 00:55:17.840]   is when we get really interesting smart people talking
[00:55:17.840 --> 00:55:19.920]   about more than just today's news.
[00:55:19.920 --> 00:55:21.400]   But what it means--
[00:55:21.400 --> 00:55:23.000]   I was just having a thought about the fact
[00:55:23.000 --> 00:55:24.680]   that wasn't your dad a paleontologist?
[00:55:24.680 --> 00:55:25.600]   Yes, he still is.
[00:55:25.600 --> 00:55:27.960]   That your dad was studying dinosaurs.
[00:55:27.960 --> 00:55:29.840]   And you're thinking about the future of robots that they--
[00:55:29.840 --> 00:55:30.720]   Is that fascinating?
[00:55:30.720 --> 00:55:31.840]   --going the other direction?
[00:55:31.840 --> 00:55:32.840]   It's in both directions.
[00:55:32.840 --> 00:55:34.400]   That is so old to me.
[00:55:34.400 --> 00:55:36.000]   What a world.
[00:55:36.000 --> 00:55:36.680]   Let's take a break.
[00:55:36.680 --> 00:55:39.120]   We've got a lot of stuff to talk about with some
[00:55:39.120 --> 00:55:40.400]   of the best people in the world.
[00:55:40.400 --> 00:55:43.000]   My great, great oldest friends.
[00:55:43.000 --> 00:55:45.920]   Becky Warley, who produced the old screen savers
[00:55:45.920 --> 00:55:48.320]   and called for help and has been a friend for 20 years.
[00:55:48.320 --> 00:55:49.160]   Almost 20 years.
[00:55:49.160 --> 00:55:49.680]   That's crazy.
[00:55:49.680 --> 00:55:51.720]   Nice to see you.
[00:55:51.720 --> 00:55:54.120]   She is currently at Good Morning America, GMA.
[00:55:54.120 --> 00:55:55.640]   And are you still doing Yahoo stuff?
[00:55:55.640 --> 00:55:56.800]   Well, we're going to talk about that.
[00:55:56.800 --> 00:55:58.080]   Not as much, so now I can dish.
[00:55:58.080 --> 00:55:59.640]   Mm, let's dish.
[00:55:59.640 --> 00:56:00.480]   That's next.
[00:56:00.480 --> 00:56:02.360]   @bwarley on the Twitter.
[00:56:02.360 --> 00:56:04.000]   Dr. Kirsten Sanford.
[00:56:04.000 --> 00:56:08.200]   Dr. Kiki would just adore her from this week in science.
[00:56:08.200 --> 00:56:10.920]   Still going on as it has been.
[00:56:10.920 --> 00:56:13.960]   How long has Twist been around?
[00:56:13.960 --> 00:56:15.000]   15 years.
[00:56:15.000 --> 00:56:18.200]   Before there was Twit, there was Twits.
[00:56:18.200 --> 00:56:19.560]   Absolutely the greatest.
[00:56:19.560 --> 00:56:21.560]   twis.org.
[00:56:21.560 --> 00:56:23.640]   What is broader impacts?
[00:56:23.640 --> 00:56:27.160]   I've started a production company to help scientists
[00:56:27.160 --> 00:56:32.880]   and organizations tell their science stories in video.
[00:56:32.880 --> 00:56:34.600]   Very nice.
[00:56:34.600 --> 00:56:37.920]   broaderimpacts.tv @DrKiki on Twitter.
[00:56:37.920 --> 00:56:42.600]   And of course, Tom Merritt, who is also a longtime tech TV
[00:56:42.600 --> 00:56:48.080]   year, was the original editor-in-chief of the Tech TV website.
[00:56:48.080 --> 00:56:49.360]   What was your title, though?
[00:56:49.360 --> 00:56:50.360]   What was it?
[00:56:50.360 --> 00:56:51.360]   Executive producer.
[00:56:51.360 --> 00:56:52.360]   Executive producer.
[00:56:52.360 --> 00:56:53.360]   Because we were TV people.
[00:56:53.360 --> 00:56:54.360]   What do we know?
[00:56:54.360 --> 00:56:55.360]   Yeah.
[00:56:55.360 --> 00:56:56.360]   Because they didn't give editors.
[00:56:56.360 --> 00:56:57.600]   What do we know?
[00:56:57.600 --> 00:56:59.560]   He's running the Daily Tech News Show.
[00:56:59.560 --> 00:57:02.000]   Dtns, you'll find it at DailyTechNewsShow.com.
[00:57:02.000 --> 00:57:06.400]   And don't forget to support it on Patreon.
[00:57:06.400 --> 00:57:09.200]   And you're doing-- it sounds like you're doing really great with that.
[00:57:09.200 --> 00:57:12.200]   Yeah, I just get to sit in my basement and talk about Tech News.
[00:57:12.200 --> 00:57:13.200]   It's awesome.
[00:57:13.200 --> 00:57:14.200]   It's the dream.
[00:57:14.200 --> 00:57:15.200]   You live in the dream?
[00:57:15.200 --> 00:57:16.200]   Yeah.
[00:57:16.200 --> 00:57:17.200]   It's just kind of like my basement.
[00:57:17.200 --> 00:57:20.200]   It's the best basement ever.
[00:57:20.200 --> 00:57:22.600]   It's been a nice basement.
[00:57:22.600 --> 00:57:23.600]   I kind of like the basement.
[00:57:23.600 --> 00:57:24.600]   Yeah.
[00:57:24.600 --> 00:57:26.200]   Well, we actually have a basement.
[00:57:26.200 --> 00:57:28.560]   But this is the basement above the basement.
[00:57:28.560 --> 00:57:29.960]   Leo's Man Cave basement.
[00:57:29.960 --> 00:57:30.960]   Yeah, really.
[00:57:30.960 --> 00:57:31.960]   You know what?
[00:57:31.960 --> 00:57:33.480]   This is just a really, really big man cave.
[00:57:33.480 --> 00:57:34.480]   Yes.
[00:57:34.480 --> 00:57:35.480]   That's nice.
[00:57:35.480 --> 00:57:36.480]   I totally nailed it.
[00:57:36.480 --> 00:57:38.640]   That is why I like to come here and drink.
[00:57:38.640 --> 00:57:41.000]   That is so you nailed it.
[00:57:41.000 --> 00:57:42.640]   Welcome to the man cave.
[00:57:42.640 --> 00:57:44.120]   Sunday Night Football next.
[00:57:44.120 --> 00:57:48.120]   Why don't we just call it the Twit Man Cave and be done with it?
[00:57:48.120 --> 00:57:49.120]   That's what I'm talking about.
[00:57:49.120 --> 00:57:50.120]   That's it.
[00:57:50.120 --> 00:57:52.120]   I missed the bet on that one.
[00:57:52.120 --> 00:57:55.280]   That is not a misogynistic title, by the way, because this lady likes being in this
[00:57:55.280 --> 00:57:56.280]   man cave.
[00:57:56.280 --> 00:57:59.400]   Hey, hey, what would a man cave be without the lead?
[00:57:59.400 --> 00:58:01.400]   We are going south so fast.
[00:58:01.400 --> 00:58:05.200]   You're the one who gave me wine, why?
[00:58:05.200 --> 00:58:07.160]   Our show today brought to you by Squarespace.
[00:58:07.160 --> 00:58:09.840]   If you watch Twit shows, you know about Squarespace.
[00:58:09.840 --> 00:58:16.080]   It's the place to make your next website a beautiful professional website without any,
[00:58:16.080 --> 00:58:18.040]   you know, you don't have to know CSS.
[00:58:18.040 --> 00:58:19.360]   You don't even have to know HTML.
[00:58:19.360 --> 00:58:21.640]   You just build it beautiful at squarespace.com.
[00:58:21.640 --> 00:58:25.640]   Of course, if you want to get geeky, the sky's the limit.
[00:58:25.640 --> 00:58:31.880]   They've got a developer platform customizable settings, fonts, colors, page configurations.
[00:58:31.880 --> 00:58:36.800]   The nice thing is you're going to start with a modern template designed by artists and
[00:58:36.800 --> 00:58:39.920]   not just artists, really good web engineers.
[00:58:39.920 --> 00:58:43.440]   So that template has a state of the art in it.
[00:58:43.440 --> 00:58:46.520]   Remember, Calacan is telling me, you'd be nuts.
[00:58:46.520 --> 00:58:49.320]   This was after I spent a quarter of a million dollars on a new website.
[00:58:49.320 --> 00:58:55.760]   You'd be nuts to do your own website, he said, because what you want to do is take this kind
[00:58:55.760 --> 00:59:01.040]   of great engineering and build your site on top of it, make it express you.
[00:59:01.040 --> 00:59:02.040]   Squarespace does that.
[00:59:02.040 --> 00:59:08.960]   Mobile responsive, the site looks great on any size screen from 55 inches to 4 inches.
[00:59:08.960 --> 00:59:09.960]   And that's built in.
[00:59:09.960 --> 00:59:11.920]   E-commerce is built in.
[00:59:11.920 --> 00:59:13.680]   And you know, it's gorgeous.
[00:59:13.680 --> 00:59:17.480]   If you go to leoville.com, my new Squarespace site, I'm just loving it.
[00:59:17.480 --> 00:59:22.200]   Now, and I've kind of used a simple template, but over time it's very easy to kind of customize
[00:59:22.200 --> 00:59:23.200]   it.
[00:59:23.200 --> 00:59:28.320]   Pricing is very straightforward when you register for a year, which I read as leoville.com.
[00:59:28.320 --> 00:59:32.480]   By the way, if you, everybody watching today right now went there at the same time, any
[00:59:32.480 --> 00:59:33.640]   other site would break.
[00:59:33.640 --> 00:59:35.960]   You cannot break a Squarespace site.
[00:59:35.960 --> 00:59:39.000]   Look at that great full bleed, brilliant images.
[00:59:39.000 --> 00:59:40.640]   I just love it.
[00:59:40.640 --> 00:59:43.680]   And Squarespace commerce built into every site.
[00:59:43.680 --> 00:59:47.800]   The only platform that you create, manage and brand your store in a beautiful way that
[00:59:47.800 --> 00:59:50.040]   reflects your ethos.
[00:59:50.040 --> 00:59:55.360]   There's a basic plan, there's an advanced plan, incredible 24/7 customer support, plus pages
[00:59:55.360 --> 00:59:57.600]   with webinars and all sorts of information.
[00:59:57.600 --> 01:00:02.560]   If you're a musician, they have a template for you that includes selling music, a set
[01:00:02.560 --> 01:00:06.480]   lists, performance, concert calendar, all of that.
[01:00:06.480 --> 01:00:09.400]   If you have a business, man, this is made for you.
[01:00:09.400 --> 01:00:14.960]   Even like things like a new baby or a getting married life events, Squarespace is fantastic.
[01:00:14.960 --> 01:00:19.880]   You can try it right now by going to squarespace.com and clip, they get started button and you're
[01:00:19.880 --> 01:00:20.880]   in.
[01:00:20.880 --> 01:00:21.880]   You can play with the template.
[01:00:21.880 --> 01:00:24.720]   The beauty part is you put the content in there.
[01:00:24.720 --> 01:00:26.560]   The template is completely separated.
[01:00:26.560 --> 01:00:27.560]   Of course it should be.
[01:00:27.560 --> 01:00:29.320]   This is modern engineering.
[01:00:29.320 --> 01:00:33.480]   You could change the template anytime, completely change your look with the click of a mouse,
[01:00:33.480 --> 01:00:35.920]   change it back, move around.
[01:00:35.920 --> 01:00:38.520]   It will always look the way you want it to.
[01:00:38.520 --> 01:00:39.520]   Start your free trial.
[01:00:39.520 --> 01:00:40.520]   No credit card needed.
[01:00:40.520 --> 01:00:42.520]   squarespace.com.
[01:00:42.520 --> 01:00:47.880]   All I ask when you decide to buy, and I forgot to do this when I bought it, is use the promo
[01:00:47.880 --> 01:00:49.640]   code Twit and you'll get 10% off.
[01:00:49.640 --> 01:00:50.640]   I was conflicted.
[01:00:50.640 --> 01:00:52.080]   I thought I shouldn't really use my own promo code.
[01:00:52.080 --> 01:00:53.080]   I used it.
[01:00:53.080 --> 01:00:54.080]   Did you?
[01:00:54.080 --> 01:00:55.880]   I used the Twit promo code when I did my Squarespace.
[01:00:55.880 --> 01:00:56.880]   Yeah, 10% off.
[01:00:56.880 --> 01:00:59.920]   It's a good deal, especially if you buy a year or two.
[01:00:59.920 --> 01:01:02.280]   That's going to be significant.
[01:01:02.280 --> 01:01:03.280]   Squarespace.com.
[01:01:03.280 --> 01:01:04.280]   We love them.
[01:01:04.280 --> 01:01:05.280]   They really do a great job.
[01:01:05.280 --> 01:01:06.680]   I wonder if that was Super Bowl ad again this year.
[01:01:06.680 --> 01:01:07.680]   They had one last year.
[01:01:07.680 --> 01:01:09.680]   Remember that Jeff Bridges had?
[01:01:09.680 --> 01:01:10.680]   Oh, the dude.
[01:01:10.680 --> 01:01:11.680]   The dude.
[01:01:11.680 --> 01:01:12.680]   Is that what they call the dude?
[01:01:12.680 --> 01:01:13.680]   The dude.
[01:01:13.680 --> 01:01:14.880]   Yeah, it was wild.
[01:01:14.880 --> 01:01:15.880]   It's so wild to see.
[01:01:15.880 --> 01:01:16.880]   The dude abides.
[01:01:16.880 --> 01:01:17.880]   I remember.
[01:01:17.880 --> 01:01:19.040]   What was the first Super Bowl ad?
[01:01:19.040 --> 01:01:20.200]   I shot the Ralphs.
[01:01:20.200 --> 01:01:24.600]   Do you buy vodka and cream?
[01:01:24.600 --> 01:01:25.600]   That's right.
[01:01:25.600 --> 01:01:27.600]   I have bought both of those things.
[01:01:27.600 --> 01:01:32.600]   That was, by the way, I'm glad he did it.
[01:01:32.600 --> 01:01:34.600]   Vodka and cream.
[01:01:34.600 --> 01:01:35.600]   Yeah.
[01:01:35.600 --> 01:01:36.600]   A white Russian.
[01:01:36.600 --> 01:01:38.720]   That's what the dude drinks.
[01:01:38.720 --> 01:01:46.000]   You saw, you saw, uh, Leonard Nimoy's incredible, uh, lazy day video that he did of the Brunner
[01:01:46.000 --> 01:01:49.080]   Mars song where he basically is the dude.
[01:01:49.080 --> 01:01:51.080]   Have you seen this?
[01:01:51.080 --> 01:01:54.200]   Can I play the audio or will I get in trouble with Brunner?
[01:01:54.200 --> 01:01:57.520]   This is shortly before he passed away.
[01:01:57.520 --> 01:01:58.520]   But he goes into this.
[01:01:58.520 --> 01:01:59.520]   He does the whole dude thing.
[01:01:59.520 --> 01:02:00.520]   Nice.
[01:02:00.520 --> 01:02:01.520]   Where's the audio?
[01:02:01.520 --> 01:02:02.520]   Are you not getting that, Jason?
[01:02:02.520 --> 01:02:04.600]   It's probably just as well right now.
[01:02:04.600 --> 01:02:06.600]   We don't need money.
[01:02:06.600 --> 01:02:07.600]   Oh, yeah.
[01:02:07.600 --> 01:02:08.600]   Yeah.
[01:02:08.600 --> 01:02:11.520]   This is totally the dude.
[01:02:11.520 --> 01:02:13.000]   That's probably something going on.
[01:02:13.000 --> 01:02:15.080]   Oh, it's not muted.
[01:02:15.080 --> 01:02:17.280]   I think he even goes to the grocery store in his bathroom.
[01:02:17.280 --> 01:02:19.840]   He yells at his, uh, yeah.
[01:02:19.840 --> 01:02:22.680]   You know that Leonard Nimoy did dishes, I would think.
[01:02:22.680 --> 01:02:27.560]   You, you would think I once when I was working at tech TV, I'd left the dishes so long and
[01:02:27.560 --> 01:02:31.680]   then I was going on a trip that I put the dishes in the refrigerator.
[01:02:31.680 --> 01:02:33.680]   Just because you want to keep that E. coli fresh.
[01:02:33.680 --> 01:02:35.720]   I figured it would smell less bad.
[01:02:35.720 --> 01:02:36.720]   That's what Chipotle did.
[01:02:36.720 --> 01:02:37.720]   It's a great technique.
[01:02:37.720 --> 01:02:38.720]   Yeah.
[01:02:38.720 --> 01:02:39.720]   That was great.
[01:02:39.720 --> 01:02:40.720]   Desperate measures for desperate time.
[01:02:40.720 --> 01:02:41.720]   You got to feel bad for Chipotle.
[01:02:41.720 --> 01:02:44.720]   You give a few people E. coli and it just ruined you.
[01:02:44.720 --> 01:02:45.720]   I knew it was Norovirus.
[01:02:45.720 --> 01:02:46.720]   It was Norovirus.
[01:02:46.720 --> 01:02:47.720]   It wasn't E. coli.
[01:02:47.720 --> 01:02:48.720]   It wasn't E. coli.
[01:02:48.720 --> 01:02:49.720]   Norovirus is better.
[01:02:49.720 --> 01:02:50.720]   I think.
[01:02:50.720 --> 01:02:53.840]   I think E. coli was here in Oregon, actually.
[01:02:53.840 --> 01:02:55.000]   Their lettuce was tainted.
[01:02:55.000 --> 01:02:56.000]   There he is.
[01:02:56.000 --> 01:02:57.000]   He's doing his dude.
[01:02:57.000 --> 01:02:58.000]   Look at that.
[01:02:58.000 --> 01:02:59.000]   That's straight out of the.
[01:02:59.000 --> 01:03:00.000]   It is.
[01:03:00.000 --> 01:03:02.000]   That's the shot from the dude.
[01:03:02.000 --> 01:03:04.440]   And then he yells at a kid.
[01:03:04.440 --> 01:03:05.440]   Watch this.
[01:03:05.440 --> 01:03:06.440]   You never saw a letter.
[01:03:06.440 --> 01:03:08.440]   He's doing a jello shot.
[01:03:08.440 --> 01:03:09.440]   All right.
[01:03:09.440 --> 01:03:10.440]   I don't know.
[01:03:10.440 --> 01:03:13.360]   I don't know why I got this.
[01:03:13.360 --> 01:03:15.320]   This is.
[01:03:15.320 --> 01:03:19.000]   So was it was equal to like an Norovirus is too separate.
[01:03:19.000 --> 01:03:22.440]   It's too separate incidences.
[01:03:22.440 --> 01:03:25.720]   We think that the sneeze guard really is going to protect us.
[01:03:25.720 --> 01:03:27.440]   We like to see our food prepared.
[01:03:27.440 --> 01:03:28.600]   No, wait a minute.
[01:03:28.600 --> 01:03:29.600]   Let me think.
[01:03:29.600 --> 01:03:32.160]   Have you done a GMAP on the sneeze guard?
[01:03:32.160 --> 01:03:33.160]   Great idea.
[01:03:33.160 --> 01:03:38.760]   You genius continues to.
[01:03:38.760 --> 01:03:39.760]   The sneeze guard.
[01:03:39.760 --> 01:03:40.760]   I'm on it.
[01:03:40.760 --> 01:03:42.760]   Basically, it's sixth grade science projects.
[01:03:42.760 --> 01:03:43.760]   Yeah.
[01:03:43.760 --> 01:03:44.760]   Now we're getting the audio.
[01:03:44.760 --> 01:03:46.720]   I don't know what happened.
[01:03:46.720 --> 01:03:48.720]   I think we have to.
[01:03:48.720 --> 01:03:49.720]   I think we have to cut it here.
[01:03:49.720 --> 01:03:53.040]   But at this point, I think there's a dirty hand gesture.
[01:03:53.040 --> 01:03:54.040]   Just in time.
[01:03:54.040 --> 01:03:55.040]   Oh, oh, oh, oh.
[01:03:55.040 --> 01:03:58.240]   You don't want to see Leonard Nimoy play with Nunchucks, do you?
[01:03:58.240 --> 01:03:59.240]   Right.
[01:03:59.240 --> 01:04:00.240]   And we got the sound work.
[01:04:00.240 --> 01:04:01.240]   OK.
[01:04:01.240 --> 01:04:02.240]   Yahoo.
[01:04:02.240 --> 01:04:04.040]   What the hell is going on?
[01:04:04.040 --> 01:04:05.680]   We've been talking about this for a couple of weeks.
[01:04:05.680 --> 01:04:09.400]   It started with a Wall Street Journal article, which now I'm convinced.
[01:04:09.400 --> 01:04:10.320]   Was it a plant?
[01:04:10.320 --> 01:04:17.040]   Was a plant by the activist investors who wanted Yahoo to make them some money.
[01:04:17.040 --> 01:04:24.480]   So the initial plan for Yahoo was to spin off their $35 billion stake in Alibaba to
[01:04:24.480 --> 01:04:28.240]   spin off their multi-billion dollar stake in Yahoo Japan.
[01:04:28.240 --> 01:04:30.240]   They paid a billion dollars for that in 2005.
[01:04:30.240 --> 01:04:31.240]   Yeah, no.
[01:04:31.240 --> 01:04:32.240]   It's a good investment.
[01:04:32.240 --> 01:04:33.240]   2005.
[01:04:33.240 --> 01:04:34.240]   So Marissa Mariah cannot take credit for that.
[01:04:34.240 --> 01:04:35.240]   No.
[01:04:35.240 --> 01:04:36.240]   Was that Terry Semmel?
[01:04:36.240 --> 01:04:37.240]   I think it might have been.
[01:04:37.240 --> 01:04:38.240]   Probably.
[01:04:38.240 --> 01:04:39.240]   Yeah.
[01:04:39.240 --> 01:04:40.240]   They've had five-- it's hard to deep track.
[01:04:40.240 --> 01:04:41.240]   They've had five CEOs in five years.
[01:04:41.240 --> 01:04:42.240]   Is it Tuesday?
[01:04:42.240 --> 01:04:43.240]   Because then it's Marissa.
[01:04:43.240 --> 01:04:45.240]   If it's Monday, it was terrible.
[01:04:45.240 --> 01:04:50.040]   But then the IRS told them we can't guarantee that that is a tax-free transaction.
[01:04:50.040 --> 01:04:51.840]   There might be a bill as big as $10 billion.
[01:04:51.840 --> 01:04:52.840]   All right.
[01:04:52.840 --> 01:04:55.640]   Is that like a cap gains basically for corporations that we're talking about?
[01:04:55.640 --> 01:04:56.640]   Okay.
[01:04:56.640 --> 01:04:57.640]   Got it.
[01:04:57.640 --> 01:04:58.640]   So and the IRS said we don't know.
[01:04:58.640 --> 01:05:01.360]   We're just not going to say-- we're not going to say the activist investors who are
[01:05:01.360 --> 01:05:07.400]   trying to get this done asked for a judgment by the IRS because they didn't want to pay
[01:05:07.400 --> 01:05:08.400]   one third of it in taxes.
[01:05:08.400 --> 01:05:09.400]   All right.
[01:05:09.400 --> 01:05:10.400]   Come on.
[01:05:10.400 --> 01:05:11.400]   I don't want to pay any taxes.
[01:05:11.400 --> 01:05:12.400]   No.
[01:05:12.400 --> 01:05:13.400]   Because it's--
[01:05:13.400 --> 01:05:14.400]   They thought it would be tax-free.
[01:05:14.400 --> 01:05:18.800]   Two bites at the apple because then the investors have to pay cap gains on all the--
[01:05:18.800 --> 01:05:19.800]   Yeah.
[01:05:19.800 --> 01:05:20.800]   That's probably why, right?
[01:05:20.800 --> 01:05:21.800]   It is.
[01:05:21.800 --> 01:05:22.800]   There's going to be a tax application.
[01:05:22.800 --> 01:05:25.480]   There's going to be a tax application to what they're doing, too.
[01:05:25.480 --> 01:05:29.880]   It's just less of a tax application and they didn't want to wait around for clarification
[01:05:29.880 --> 01:05:30.880]   on the Alibaba one.
[01:05:30.880 --> 01:05:33.520]   They wanted to go with the one they were sure was going to be lower.
[01:05:33.520 --> 01:05:35.080]   By the way, I just want to take a look at that.
[01:05:35.080 --> 01:05:38.880]   So instead of spinning off the peripherals, they're spinning off the core business.
[01:05:38.880 --> 01:05:42.640]   They-- well, we don't even know what they're doing because I feel like a lot of this is
[01:05:42.640 --> 01:05:44.600]   just leaked out.
[01:05:44.600 --> 01:05:45.600]   What does the board say?
[01:05:45.600 --> 01:05:47.480]   I know Max Leifchen is off the board.
[01:05:47.480 --> 01:05:51.280]   He was one of Meyer's supporters, former PayPal founder.
[01:05:51.280 --> 01:05:52.280]   Is Yahoo!
[01:05:52.280 --> 01:05:53.280]   High school or not?
[01:05:53.280 --> 01:05:54.280]   It is very much.
[01:05:54.280 --> 01:05:57.200]   I just can't even-- I mean, I don't work there anymore, basically.
[01:05:57.200 --> 01:05:58.200]   You basically don't.
[01:05:58.200 --> 01:05:59.200]   No.
[01:05:59.200 --> 01:06:00.200]   Did you enjoy working there?
[01:06:00.200 --> 01:06:01.840]   Um, yes and no.
[01:06:01.840 --> 01:06:02.840]   I mean, it's great.
[01:06:02.840 --> 01:06:04.400]   I always say, "mine the miners."
[01:06:04.400 --> 01:06:08.640]   So go where-- when you work in media, you go where the money is.
[01:06:08.640 --> 01:06:09.640]   And it's like a kind of--
[01:06:09.640 --> 01:06:10.640]   Well, you weren't alone.
[01:06:10.640 --> 01:06:12.600]   You followed Katie Couric with David Pogue.
[01:06:12.600 --> 01:06:16.840]   Well, no, that's an interesting story, but that's not about what we're talking about.
[01:06:16.840 --> 01:06:22.840]   Basically, this is a little dirt that I can tell, which is Marissa Meyer went to Davos.
[01:06:22.840 --> 01:06:25.400]   She met up with the head of one of the big banks.
[01:06:25.400 --> 01:06:27.200]   And he said, "Oh, I love Yahoo!
[01:06:27.200 --> 01:06:28.200]   Finance.
[01:06:28.200 --> 01:06:29.200]   It's so great."
[01:06:29.200 --> 01:06:30.200]   Me too.
[01:06:30.200 --> 01:06:31.200]   It is good.
[01:06:31.200 --> 01:06:32.200]   And she said, "Oh, which anchor do you listen to?
[01:06:32.200 --> 01:06:33.200]   Who's advised do you listen to?"
[01:06:33.200 --> 01:06:34.200]   Anchor.
[01:06:34.200 --> 01:06:35.200]   I just look at the graphs.
[01:06:35.200 --> 01:06:38.880]   Well, because he said, "I watch it all the time instead of Bloomberg or CNBC."
[01:06:38.880 --> 01:06:39.880]   And she said, "Well, which--"
[01:06:39.880 --> 01:06:40.880]   What do you like?
[01:06:40.880 --> 01:06:41.880]   Who do you like?
[01:06:41.880 --> 01:06:42.880]   No.
[01:06:42.880 --> 01:06:46.760]   Now, if you heard that, would you turn around on what she did, as she said?
[01:06:46.760 --> 01:06:47.760]   We need an anchor.
[01:06:47.760 --> 01:06:49.520]   We need one person for every vertical.
[01:06:49.520 --> 01:06:50.520]   Yeah.
[01:06:50.520 --> 01:06:52.520]   And that's the only talent that we have for that vertical.
[01:06:52.520 --> 01:06:53.520]   Oh.
[01:06:53.520 --> 01:06:58.800]   Which is such an interesting-- I mean, actually, I find it not the worst idea.
[01:06:58.800 --> 01:07:00.560]   Well, now you're a TV person.
[01:07:00.560 --> 01:07:01.560]   Right.
[01:07:01.560 --> 01:07:02.560]   Because it's--
[01:07:02.560 --> 01:07:05.760]   You have to have someone iconic that represents the brand in each vertical.
[01:07:05.760 --> 01:07:07.960]   So that was one of the things that she did.
[01:07:07.960 --> 01:07:08.960]   She came back--
[01:07:08.960 --> 01:07:10.520]   So you'd agree with that?
[01:07:10.520 --> 01:07:12.000]   I think it's smart because--
[01:07:12.000 --> 01:07:13.000]   It was perfect for news.
[01:07:13.000 --> 01:07:14.000]   It was perfect for tech.
[01:07:14.000 --> 01:07:16.840]   Perf for news, poke for tech.
[01:07:16.840 --> 01:07:18.120]   I don't even know for finance.
[01:07:18.120 --> 01:07:21.200]   I think they went maybe with the guys they had.
[01:07:21.200 --> 01:07:23.160]   They had two people that were really good.
[01:07:23.160 --> 01:07:24.160]   Yeah.
[01:07:24.160 --> 01:07:25.800]   But it just was an interesting move on her part.
[01:07:25.800 --> 01:07:26.800]   So anyways, I digress.
[01:07:26.800 --> 01:07:27.800]   That's smart.
[01:07:27.800 --> 01:07:31.880]   It's one thing she did, but that's not the news of today.
[01:07:31.880 --> 01:07:34.040]   In fact, I was listening to Cara Swisher.
[01:07:34.040 --> 01:07:39.360]   She was interviewed locally on KQED about this, along with Miguel Health from-- he was
[01:07:39.360 --> 01:07:40.360]   at the New York Times.
[01:07:40.360 --> 01:07:41.360]   Is it fusion?
[01:07:41.360 --> 01:07:47.120]   Anyway, she implied that she had heard from both Pogue and Quirk or heard from Purple
[01:07:47.120 --> 01:07:50.440]   who had heard from them that they were less than happy about what's going on at Yahoo
[01:07:50.440 --> 01:07:51.440]   right now.
[01:07:51.440 --> 01:07:52.440]   Oh, they're all-- I'm sure--
[01:07:52.440 --> 01:07:54.280]   Their career decision may not have panned out.
[01:07:54.280 --> 01:07:57.080]   Well, Yahoo said we're going big on media.
[01:07:57.080 --> 01:08:00.560]   And it's just not as easy as they made it seem.
[01:08:00.560 --> 01:08:04.480]   Aaron Task is the finance that they have.
[01:08:04.480 --> 01:08:05.480]   And Jeff Mackie.
[01:08:05.480 --> 01:08:09.800]   But I think that's the point is they underestimated how difficult the media landscape is now.
[01:08:09.800 --> 01:08:14.320]   And putting all their eggs in one basket and the money they spent on Pogue, Quirk, and
[01:08:14.320 --> 01:08:15.320]   the like.
[01:08:15.320 --> 01:08:20.320]   We concluded last week because we talked about this last week.
[01:08:20.320 --> 01:08:25.800]   In fact, we had a good panel to do it because we had from the New York Times-- who was it?
[01:08:25.800 --> 01:08:27.000]   Jason, that was from the New York Times.
[01:08:27.000 --> 01:08:28.000]   It was here.
[01:08:28.000 --> 01:08:29.000]   She was great.
[01:08:29.000 --> 01:08:30.000]   And I can't remember.
[01:08:30.000 --> 01:08:31.000]   I apologize.
[01:08:31.000 --> 01:08:32.000]   Sorry, Katie Banner.
[01:08:32.000 --> 01:08:33.880]   Katie Banner, of course.
[01:08:33.880 --> 01:08:38.040]   It was her opinion A that that Wall Street Journal story had been planted and that B
[01:08:38.040 --> 01:08:41.960]   that Marissa would never be fired because the board is Marissa's board.
[01:08:41.960 --> 01:08:42.960]   Right.
[01:08:42.960 --> 01:08:45.400]   And her severance is 150 million.
[01:08:45.400 --> 01:08:46.400]   Yeah, big deal.
[01:08:46.400 --> 01:08:47.400]   I know.
[01:08:47.400 --> 01:08:49.040]   But we're talking billions here.
[01:08:49.040 --> 01:08:50.040]   This is not--
[01:08:50.040 --> 01:08:51.040]   I know.
[01:08:51.040 --> 01:08:53.240]   But it's not going to get fixed with the CEO.
[01:08:53.240 --> 01:08:55.440]   She's not doing anything wrong.
[01:08:55.440 --> 01:08:57.640]   That's the question.
[01:08:57.640 --> 01:09:02.040]   You know, Swisher kind of said that she was.
[01:09:02.040 --> 01:09:03.040]   Swisher said--
[01:09:03.040 --> 01:09:04.280]   I mean, that's what I'm saying.
[01:09:04.280 --> 01:09:07.680]   And Cara, as you know, there's no one who spends more time.
[01:09:07.680 --> 01:09:13.760]   She's, of course, found her recode and probably the single-- the number one recipient of Leek
[01:09:13.760 --> 01:09:14.760]   Yahoo--
[01:09:14.760 --> 01:09:15.760]   Oh, yeah.
[01:09:15.760 --> 01:09:16.760]   --memorandum.
[01:09:16.760 --> 01:09:18.560]   Company memoranda.
[01:09:18.560 --> 01:09:26.880]   But she was the opinion that the board made this celebrity hire, that Marissa Meyer was
[01:09:26.880 --> 01:09:29.480]   the celebrity hire, the wrong person.
[01:09:29.480 --> 01:09:34.520]   She said they should have hired a turnaround specialist who'd come in, correctly assessed
[01:09:34.520 --> 01:09:39.680]   the value of each piece, turned the most important pieces around, dumped the rest.
[01:09:39.680 --> 01:09:41.680]   That was the only way to save Yahoo.
[01:09:41.680 --> 01:09:48.240]   Instead, they brought in a well-known Google executive who arguably didn't have a future
[01:09:48.240 --> 01:09:53.640]   at Google, paid her an awful lot of money, including a huge golden parachute.
[01:09:53.640 --> 01:09:58.040]   And she-- but she wasn't the right person, that she was merely a celebrity hire.
[01:09:58.040 --> 01:09:59.040]   OK.
[01:09:59.040 --> 01:10:01.880]   So then my question is, turn around to what?
[01:10:01.880 --> 01:10:06.120]   Well, like I said, OK, Yahoo mail.
[01:10:06.120 --> 01:10:07.120]   Great.
[01:10:07.120 --> 01:10:08.120]   Flicker.
[01:10:08.120 --> 01:10:10.040]   Let's think of some properties.
[01:10:10.040 --> 01:10:11.960]   Some valuable properties.
[01:10:11.960 --> 01:10:13.640]   Focus on those, make them valuable, again.
[01:10:13.640 --> 01:10:16.520]   Sell off the stuff that's not going to ever turn around.
[01:10:16.520 --> 01:10:17.920]   But none of those are going to satisfy--
[01:10:17.920 --> 01:10:19.440]   They're not going to make it big enough.
[01:10:19.440 --> 01:10:20.760]   I mean, I thought it's so interesting.
[01:10:20.760 --> 01:10:24.600]   On the day that all this Yahoo news came down was also the day that the Dow and DuPont
[01:10:24.600 --> 01:10:25.600]   merger were announced.
[01:10:25.600 --> 01:10:27.640]   Isn't that wild?
[01:10:27.640 --> 01:10:32.960]   And I mean, what that tells me is that everything in our economy and our stock market is around
[01:10:32.960 --> 01:10:34.440]   growth companies.
[01:10:34.440 --> 01:10:40.640]   And that stable companies that do certain things well but aren't super sexy are struggling.
[01:10:40.640 --> 01:10:42.080]   That was my takeaway.
[01:10:42.080 --> 01:10:45.720]   The lesson is don't go public, whatever you do.
[01:10:45.720 --> 01:10:48.480]   The expectations of growth are so high.
[01:10:48.480 --> 01:10:51.360]   Percentage of profit margin that people expect are ridiculous.
[01:10:51.360 --> 01:10:52.560]   And therefore-- I don't know.
[01:10:52.560 --> 01:10:58.960]   I'm not defending Yahoo, I'm more interested in what do we want from Yahoo and what's the
[01:10:58.960 --> 01:10:59.960]   reality?
[01:10:59.960 --> 01:11:04.720]   Well, I think Becky's right that Merced hasn't really done anything particularly wrong.
[01:11:04.720 --> 01:11:07.440]   She just hasn't done anything crazy right.
[01:11:07.440 --> 01:11:11.640]   And what Kara was saying is they should have hired like a John Chen who's gone in to turn
[01:11:11.640 --> 01:11:15.000]   around Blackberry or maybe sold to a venture capital.
[01:11:15.000 --> 01:11:17.480]   How's that working out by the way for John Chen?
[01:11:17.480 --> 01:11:18.640]   It's actually not working out bad.
[01:11:18.640 --> 01:11:20.720]   Blackberry just had a profitable quarter last time.
[01:11:20.720 --> 01:11:22.160]   Leo bought a priv.
[01:11:22.160 --> 01:11:24.080]   It's not going to get bought a priv.
[01:11:24.080 --> 01:11:27.320]   So it's working out really well for John Chen.
[01:11:27.320 --> 01:11:28.320]   Congratulations.
[01:11:28.320 --> 01:11:32.560]   You know, I mean, that's a tough, tough boat to turn around, but it is turning.
[01:11:32.560 --> 01:11:37.960]   And I think a better example would be Zif Davis where the Zif Davis properties, the magazines
[01:11:37.960 --> 01:11:42.600]   were considered just junk and venture capital companies came in and like, you know, slimmed
[01:11:42.600 --> 01:11:48.080]   them down, tidied them up and turned them into something that was actually worth owning,
[01:11:48.080 --> 01:11:50.280]   which is not an exciting or sexy thing to do.
[01:11:50.280 --> 01:11:52.880]   But I think that's what Kara might have been suggesting should happen to you.
[01:11:52.880 --> 01:11:56.720]   You just get a specialist to come in and strip it for parts, turn it into something that
[01:11:56.720 --> 01:11:57.800]   actually makes profit.
[01:11:57.800 --> 01:12:02.040]   You make such a good point because what they brought Marissa in to do is to keep it the
[01:12:02.040 --> 01:12:03.360]   Titanic.
[01:12:03.360 --> 01:12:05.840]   They didn't want to slim it down and turn it into parts.
[01:12:05.840 --> 01:12:11.080]   They wanted to add more acquire more, get more and be as big.
[01:12:11.080 --> 01:12:12.920]   And I'm not sure that's a realistic strategy.
[01:12:12.920 --> 01:12:19.040]   So maybe the point is is go big and then when you realize there's no big to go, go home.
[01:12:19.040 --> 01:12:21.920]   And that's kind of where I feel like we might be.
[01:12:21.920 --> 01:12:23.280]   It's not that Marissa did anything wrong.
[01:12:23.280 --> 01:12:26.880]   It's that they hired the wrong person.
[01:12:26.880 --> 01:12:27.880]   Maybe.
[01:12:27.880 --> 01:12:31.440]   There's a lot of smart people there.
[01:12:31.440 --> 01:12:34.160]   I have no idea what the solution is for Yahoo.
[01:12:34.160 --> 01:12:40.040]   And I thought media, just the power of the front page continues to be so strong.
[01:12:40.040 --> 01:12:42.400]   And it's such a global play.
[01:12:42.400 --> 01:12:46.880]   When I had a show there, 25% of our traffic came from the Philippines alone.
[01:12:46.880 --> 01:12:47.880]   What?
[01:12:47.880 --> 01:12:51.440]   Unbelievable amounts of global traffic.
[01:12:51.440 --> 01:12:55.960]   And it's because that front page is still really powerful there.
[01:12:55.960 --> 01:12:57.480]   It's the fire hose effect.
[01:12:57.480 --> 01:12:59.120]   You get on that front page, you're good to go.
[01:12:59.120 --> 01:13:05.120]   And they've actually done a decent job of creating some verticals that were self-sustaining,
[01:13:05.120 --> 01:13:06.680]   finance, sports.
[01:13:06.680 --> 01:13:11.720]   Tech was interesting, but I wouldn't say it's self-sustaining.
[01:13:11.720 --> 01:13:13.880]   Parenting has got a lot of legs.
[01:13:13.880 --> 01:13:16.960]   And they're really trying that media company play is what they're going on.
[01:13:16.960 --> 01:13:18.560]   Yahoo Mail is still huge, right?
[01:13:18.560 --> 01:13:19.560]   Oh, massive.
[01:13:19.560 --> 01:13:20.560]   Oh.
[01:13:20.560 --> 01:13:24.160]   So one of the things Yahoo Mail did this week is they allowed you to connect your Gmail
[01:13:24.160 --> 01:13:30.560]   and Outlook Mail accounts in so that you could use, if you like the Yahoo interface better,
[01:13:30.560 --> 01:13:32.120]   you could use it for Gmail.
[01:13:32.120 --> 01:13:33.760]   So bring the pop settings in or the--
[01:13:33.760 --> 01:13:39.080]   Yeah, the whole thing, a Viya iMap, which I did.
[01:13:39.080 --> 01:13:43.760]   And then I realized why I use Gmail because one of the reasons I didn't like about Yahoo
[01:13:43.760 --> 01:13:46.480]   Mail is just to, I mean, look at all the spam in here.
[01:13:46.480 --> 01:13:48.800]   It's a spam nightmare.
[01:13:48.800 --> 01:13:50.000]   So what did they do to my Gmail?
[01:13:50.000 --> 01:13:51.280]   They just loaded it up with spam.
[01:13:51.280 --> 01:13:57.160]   I am now, apparently, there's a ton of spam in my Gmail that I don't see because Google
[01:13:57.160 --> 01:13:58.160]   handles it.
[01:13:58.160 --> 01:14:00.320]   Viya who said, hey, guess what?
[01:14:00.320 --> 01:14:01.320]   This is really cool.
[01:14:01.320 --> 01:14:03.200]   Just bringing back the spam, baby.
[01:14:03.200 --> 01:14:06.040]   Because Yahoo's so good at showing you spam.
[01:14:06.040 --> 01:14:07.760]   Yeah, Yahoo Mail's a bad--
[01:14:07.760 --> 01:14:09.360]   It just needs to learn your new spam.
[01:14:09.360 --> 01:14:10.360]   That's all.
[01:14:10.360 --> 01:14:11.360]   Hey.
[01:14:11.360 --> 01:14:12.360]   Showing me spam.
[01:14:12.360 --> 01:14:14.600]   Can I make the point that we've buried the lead here?
[01:14:14.600 --> 01:14:15.600]   What's the lead?
[01:14:15.600 --> 01:14:22.680]   The lead is that the day after Marissa announced this huge spin-off and the tax implications
[01:14:22.680 --> 01:14:26.120]   and fielded questions, oh, she gave birth to twins.
[01:14:26.120 --> 01:14:27.600]   Oh, I know that was so sad.
[01:14:27.600 --> 01:14:32.680]   I was watching-- I was listening to this KQED interview with Swisher and Helped.
[01:14:32.680 --> 01:14:36.920]   And in the middle of it, Cara says, oh, yeah, I just got a text from Marissa.
[01:14:36.920 --> 01:14:37.920]   She just had twins.
[01:14:37.920 --> 01:14:38.920]   Oh, my God.
[01:14:38.920 --> 01:14:40.920]   It's like, as they're bashing her.
[01:14:40.920 --> 01:14:41.920]   Oh, my God.
[01:14:41.920 --> 01:14:42.920]   Oh, and by the way--
[01:14:42.920 --> 01:14:43.920]   I couldn't even imagine.
[01:14:43.920 --> 01:14:46.840]   She's been pregnant this whole time, and she just had twins.
[01:14:46.840 --> 01:14:48.480]   And she's going to back to work at two weeks.
[01:14:48.480 --> 01:14:49.480]   I just shudder.
[01:14:49.480 --> 01:14:53.520]   I mean, there was like a cold sweat that hit me when the realization of this came down.
[01:14:53.520 --> 01:14:57.960]   What'd you say Ms. Meyer has been for months hoping, fire me.
[01:14:57.960 --> 01:14:59.560]   Please, fire me, please.
[01:14:59.560 --> 01:15:00.560]   Please.
[01:15:00.560 --> 01:15:02.240]   Spin it off whatever direction you want.
[01:15:02.240 --> 01:15:03.240]   I don't fire me.
[01:15:03.240 --> 01:15:04.240]   Please.
[01:15:04.240 --> 01:15:05.240]   Yeah.
[01:15:05.240 --> 01:15:06.240]   $110 million, please, fire me.
[01:15:06.240 --> 01:15:07.240]   I got twins.
[01:15:07.240 --> 01:15:08.240]   I got to expect some times here.
[01:15:08.240 --> 01:15:09.240]   I just want to show you.
[01:15:09.240 --> 01:15:12.280]   We have been to God knows how many meetings and discussions.
[01:15:12.280 --> 01:15:14.040]   Like she has exasperated.
[01:15:14.040 --> 01:15:15.040]   Yeah.
[01:15:15.040 --> 01:15:17.040]   Do you not call--
[01:15:17.040 --> 01:15:18.040]   Uncomfortable.
[01:15:18.040 --> 01:15:20.680]   She must have been in every aspect of the word.
[01:15:20.680 --> 01:15:24.840]   Of course, you and Kiki know very well.
[01:15:24.840 --> 01:15:27.040]   This must have been very difficult for her, right?
[01:15:27.040 --> 01:15:28.040]   Oh, my God.
[01:15:28.040 --> 01:15:29.040]   Yeah.
[01:15:29.040 --> 01:15:30.040]   And look how huge she is.
[01:15:30.040 --> 01:15:31.040]   That's me.
[01:15:31.040 --> 01:15:32.040]   Oh, that's you.
[01:15:32.040 --> 01:15:33.040]   [LAUGHTER]
[01:15:33.040 --> 01:15:34.040]   You're so enormous.
[01:15:34.040 --> 01:15:35.040]   That is not that terrifying.
[01:15:35.040 --> 01:15:36.040]   Where's that from?
[01:15:36.040 --> 01:15:37.040]   That was the day before I gave birth.
[01:15:37.040 --> 01:15:38.040]   Oh, that's not a real picture.
[01:15:38.040 --> 01:15:39.040]   That is--
[01:15:39.040 --> 01:15:40.040]   That is--
[01:15:40.040 --> 01:15:41.040]   Oh, my God.
[01:15:41.040 --> 01:15:43.040]   That is a real video.
[01:15:43.040 --> 01:15:44.560]   Where can I find that?
[01:15:44.560 --> 01:15:45.960]   Here, plug your HDMI in here.
[01:15:45.960 --> 01:15:47.960]   Here, just plug it in because you got to see this.
[01:15:47.960 --> 01:15:50.400]   Is that not the most terrifying thing you've ever seen?
[01:15:50.400 --> 01:15:51.400]   You had twins, right?
[01:15:51.400 --> 01:15:52.400]   How big were the twins?
[01:15:52.400 --> 01:15:53.400]   Yes.
[01:15:53.400 --> 01:15:54.400]   Oh, my God.
[01:15:54.400 --> 01:15:56.680]   It's so-- that's when I just freaked out.
[01:15:56.680 --> 01:15:57.680]   OK.
[01:15:57.680 --> 01:15:58.680]   Where am I getting the audio from?
[01:15:58.680 --> 01:15:59.680]   Oh, this is my computer.
[01:15:59.680 --> 01:16:01.680]   I don't know if you can see this, but--
[01:16:01.680 --> 01:16:04.600]   Make it go full screen so we see the value, the giant value.
[01:16:04.600 --> 01:16:05.600]   Oh, God.
[01:16:05.600 --> 01:16:06.600]   It's terrifying.
[01:16:06.600 --> 01:16:08.480]   But that's where I was like, oh, my--
[01:16:08.480 --> 01:16:09.480]   Wow.
[01:16:09.480 --> 01:16:10.480]   Wow.
[01:16:10.480 --> 01:16:11.480]   I want to see this all.
[01:16:11.480 --> 01:16:12.480]   Go full screen here.
[01:16:12.480 --> 01:16:13.480]   Oh, this promo.
[01:16:13.480 --> 01:16:14.640]   I don't even know if it can here.
[01:16:14.640 --> 01:16:16.160]   This is like old school.
[01:16:16.160 --> 01:16:17.160]   This is--
[01:16:17.160 --> 01:16:18.840]   This is before they had full screen.
[01:16:18.840 --> 01:16:19.840]   They have full belly.
[01:16:19.840 --> 01:16:20.840]   Holy--
[01:16:20.840 --> 01:16:21.840]   That's not possible.
[01:16:21.840 --> 01:16:22.840]   That's not possible.
[01:16:22.840 --> 01:16:23.840]   That's 200 pounds.
[01:16:23.840 --> 01:16:24.840]   What?
[01:16:24.840 --> 01:16:25.840]   How did they--
[01:16:25.840 --> 01:16:26.840]   How did they--
[01:16:26.840 --> 01:16:27.840]   I mean, this woman--
[01:16:27.840 --> 01:16:30.040]   I mean, that's the part that I can't convey to you is that she's--
[01:16:30.040 --> 01:16:32.040]   She didn't look like that.
[01:16:32.040 --> 01:16:33.760]   I don't know why she wouldn't.
[01:16:33.760 --> 01:16:35.280]   She's having twins.
[01:16:35.280 --> 01:16:36.280]   Twins.
[01:16:36.280 --> 01:16:37.280]   Yeah.
[01:16:37.280 --> 01:16:38.280]   There's two children in there.
[01:16:38.280 --> 01:16:40.280]   Yes, that's the realization I'm trying to share with all of them.
[01:16:40.280 --> 01:16:41.280]   With all of you.
[01:16:41.280 --> 01:16:42.280]   Holy cow.
[01:16:42.280 --> 01:16:43.280]   Oh, my God.
[01:16:43.280 --> 01:16:44.280]   And I know who gives to him.
[01:16:44.280 --> 01:16:45.280]   When I say cow, I don't--
[01:16:45.280 --> 01:16:48.000]   It doesn't mean that disparagingly--
[01:16:48.000 --> 01:16:49.000]   You know, I don't--
[01:16:49.000 --> 01:16:50.000]   But that's an-- that's like there's an optical illusion.
[01:16:50.000 --> 01:16:51.000]   No.
[01:16:51.000 --> 01:16:52.000]   You're not sticking out that far.
[01:16:52.000 --> 01:16:53.000]   No, that is--
[01:16:53.000 --> 01:16:54.000]   You couldn't reach the end, could you?
[01:16:54.000 --> 01:16:55.000]   I could barely drive.
[01:16:55.000 --> 01:17:03.440]   I was doing a live shot at Black Friday, it was like two weeks prior to this.
[01:17:03.440 --> 01:17:07.280]   And I was talking about 50 inch TVs and the director said in my ear, "Get back.
[01:17:07.280 --> 01:17:09.280]   We can't see the TVs."
[01:17:09.280 --> 01:17:11.280]   [LAUGHTER]
[01:17:11.280 --> 01:17:14.280]   That's amazing.
[01:17:14.280 --> 01:17:17.920]   That's Marissa Mayer doing this.
[01:17:17.920 --> 01:17:21.760]   Yeah, I mean, it is kind of amazing.
[01:17:21.760 --> 01:17:23.760]   And I have huge respect for Ms. Mayer.
[01:17:23.760 --> 01:17:26.480]   I think she is very smart.
[01:17:26.480 --> 01:17:28.040]   You know, though, she's a product person.
[01:17:28.040 --> 01:17:29.280]   That's what she did at Google, right?
[01:17:29.280 --> 01:17:35.320]   She was famous for managing the Google front page and making sure that it didn't get cluttered.
[01:17:35.320 --> 01:17:37.760]   I don't understand why she couldn't bring that.
[01:17:37.760 --> 01:17:40.680]   She's a Yahoo! is just a ship that just couldn't be turned at this point.
[01:17:40.680 --> 01:17:42.520]   It's maybe a giant oil tanker.
[01:17:42.520 --> 01:17:44.280]   I thought I was saying it.
[01:17:44.280 --> 01:17:45.280]   It's a boat.
[01:17:45.280 --> 01:17:47.280]   Or Becky Worley with twins.
[01:17:47.280 --> 01:17:48.280]   It is.
[01:17:48.280 --> 01:17:49.760]   There was no turning that.
[01:17:49.760 --> 01:17:51.760]   That was not very agile.
[01:17:51.760 --> 01:17:52.760]   But it was quite--
[01:17:52.760 --> 01:17:54.760]   You don't want to see it in the next--
[01:17:54.760 --> 01:17:55.760]   I know, I think this is a fake picture.
[01:17:55.760 --> 01:17:57.320]   Do you really want to see this?
[01:17:57.320 --> 01:17:58.320]   Yes.
[01:17:58.320 --> 01:17:59.320]   Okay, all right.
[01:17:59.320 --> 01:18:00.320]   I'll--
[01:18:00.320 --> 01:18:01.320]   Can we go to the beginning?
[01:18:01.320 --> 01:18:02.320]   Yeah.
[01:18:02.320 --> 01:18:03.320]   So this was this--
[01:18:03.320 --> 01:18:04.320]   Okay, so it was kind of hidden through this--
[01:18:04.320 --> 01:18:05.320]   Is this a montage?
[01:18:05.320 --> 01:18:06.320]   No, it was this promo.
[01:18:06.320 --> 01:18:07.320]   I was going to do this show with Devorak.
[01:18:07.320 --> 01:18:09.000]   And so we made this promo.
[01:18:09.000 --> 01:18:12.320]   And they were like, we got to explain why you're not available right now.
[01:18:12.320 --> 01:18:14.400]   So they're like, you have years of experience.
[01:18:14.400 --> 01:18:16.200]   I'm a reporter, a tech reporter.
[01:18:16.200 --> 01:18:17.720]   You want your tech news now.
[01:18:17.720 --> 01:18:18.720]   Yes.
[01:18:18.720 --> 01:18:19.720]   And then I say--
[01:18:19.720 --> 01:18:20.720]   Pretending the type.
[01:18:20.720 --> 01:18:22.720]   Oh, like right now?
[01:18:22.720 --> 01:18:23.720]   Like--
[01:18:23.720 --> 01:18:26.280]   Oh my god!
[01:18:26.280 --> 01:18:29.440]   And then, you know, then my water breaks.
[01:18:29.440 --> 01:18:32.040]   And then we do the whole fake birthday.
[01:18:32.040 --> 01:18:33.040]   Oh, this is good.
[01:18:33.040 --> 01:18:34.040]   This is good.
[01:18:34.040 --> 01:18:35.040]   And how--
[01:18:35.040 --> 01:18:37.040]   Your technology news upload.
[01:18:37.040 --> 01:18:40.880]   And how shortly after that did you give birth?
[01:18:40.880 --> 01:18:41.880]   The next day.
[01:18:41.880 --> 01:18:42.880]   Oh my god.
[01:18:42.880 --> 01:18:43.880]   The next day.
[01:18:43.880 --> 01:18:44.880]   There's Finn and Amalia.
[01:18:44.880 --> 01:18:45.880]   Yeah.
[01:18:45.880 --> 01:18:46.880]   By the way, great kids.
[01:18:46.880 --> 01:18:47.880]   You have Marissa all over that.
[01:18:47.880 --> 01:18:48.880]   I see.
[01:18:48.880 --> 01:18:49.880]   Yeah.
[01:18:49.880 --> 01:18:51.360]   They had so much fun when we came to your house.
[01:18:51.360 --> 01:18:52.920]   Did you spin them out for tax reasons or--
[01:18:52.920 --> 01:18:53.920]   I did.
[01:18:53.920 --> 01:18:54.920]   I did.
[01:18:54.920 --> 01:18:55.920]   She started an LLC.
[01:18:55.920 --> 01:18:56.920]   I did.
[01:18:56.920 --> 01:18:57.920]   Yeah.
[01:18:57.920 --> 01:18:58.920]   The Finn and Warley--
[01:18:58.920 --> 01:18:59.920]   Finn Warley and Amalia Warley LLC.
[01:18:59.920 --> 01:19:00.920]   That's what I wanted.
[01:19:00.920 --> 01:19:03.400]   One is an S corp and the other is a C corp.
[01:19:03.400 --> 01:19:05.680]   So I kept it separate for tax reasons.
[01:19:05.680 --> 01:19:06.680]   Wow.
[01:19:06.680 --> 01:19:10.840]   When I just got cold sweats when I thought about the whole Marissa thing, like, wow.
[01:19:10.840 --> 01:19:14.040]   You guys aren't financial analysts, so there's really nothing to say about this spin off.
[01:19:14.040 --> 01:19:16.400]   I mean, I don't even know what they're spinning the spin.
[01:19:16.400 --> 01:19:18.080]   What does it mean?
[01:19:18.080 --> 01:19:19.080]   If you hold Yahoo!
[01:19:19.080 --> 01:19:21.600]   stock, you'll get parts of all of the new companies.
[01:19:21.600 --> 01:19:22.600]   Right.
[01:19:22.600 --> 01:19:25.560]   Far as I understand it, there's a little more transparency into Yahoo because they're
[01:19:25.560 --> 01:19:26.560]   spinning it out.
[01:19:26.560 --> 01:19:27.560]   Right.
[01:19:27.560 --> 01:19:28.560]   Yeah.
[01:19:28.560 --> 01:19:29.560]   So--
[01:19:29.560 --> 01:19:30.560]   It's like Google did with Alphabet.
[01:19:30.560 --> 01:19:31.560]   It's a-- yeah.
[01:19:31.560 --> 01:19:32.560]   For different reasons.
[01:19:32.560 --> 01:19:33.560]   But yeah.
[01:19:33.560 --> 01:19:34.720]   And there's no overarching company anymore.
[01:19:34.720 --> 01:19:37.320]   It will be Yahoo, Core, whatever they name these things.
[01:19:37.320 --> 01:19:38.320]   Yeah.
[01:19:38.320 --> 01:19:40.680]   Or like HP with HP Enterprise and Hewlett-Pack.
[01:19:40.680 --> 01:19:45.160]   But one question I have is, does the board still oversee Yahoo or does Yahoo--
[01:19:45.160 --> 01:19:46.160]   No.
[01:19:46.160 --> 01:19:47.160]   No, no.
[01:19:47.160 --> 01:19:50.160]   I think the board goes with one of the three, probably the core businesses.
[01:19:50.160 --> 01:19:51.160]   Right?
[01:19:51.160 --> 01:19:52.160]   That's cool.
[01:19:52.160 --> 01:19:54.200]   And Marissa probably goes with core businesses.
[01:19:54.200 --> 01:19:55.200]   Yeah.
[01:19:55.200 --> 01:19:56.200]   Yeah.
[01:19:56.200 --> 01:19:57.200]   That's what it's called.
[01:19:57.200 --> 01:19:59.000]   It's going to be a year before that even gets unsettled.
[01:19:59.000 --> 01:20:00.000]   So--
[01:20:00.000 --> 01:20:01.800]   Yeah, my guess is that's what it'll have.
[01:20:01.800 --> 01:20:02.800]   Yeah.
[01:20:02.800 --> 01:20:03.800]   All right.
[01:20:03.800 --> 01:20:05.320]   I'm still reeling from the future.
[01:20:05.320 --> 01:20:08.440]   I'm traumatized with visions of my body,
[01:20:08.440 --> 01:20:11.760]   morphing to job and hot stuff.
[01:20:11.760 --> 01:20:12.760]   Look how great you look.
[01:20:12.760 --> 01:20:15.440]   I mean, seriously, it took six years.
[01:20:15.440 --> 01:20:16.440]   But you were fabulous.
[01:20:16.440 --> 01:20:17.440]   I got there.
[01:20:17.440 --> 01:20:18.440]   That's actually--
[01:20:18.440 --> 01:20:19.440]   It's so good.
[01:20:19.440 --> 01:20:22.200]   No, you actually-- I never saw you that pregnant.
[01:20:22.200 --> 01:20:24.320]   And you look fabulous.
[01:20:24.320 --> 01:20:25.320]   Like instantly.
[01:20:25.320 --> 01:20:26.320]   Like--
[01:20:26.320 --> 01:20:27.320]   No.
[01:20:27.320 --> 01:20:28.320]   No.
[01:20:28.320 --> 01:20:29.880]   It was that cryogenic thing.
[01:20:29.880 --> 01:20:31.760]   People have died now doing that.
[01:20:31.760 --> 01:20:33.760]   I know.
[01:20:33.760 --> 01:20:37.200]   Are you regretting having risked your life in a cryogenic tank?
[01:20:37.200 --> 01:20:38.200]   No.
[01:20:38.200 --> 01:20:39.200]   No.
[01:20:39.200 --> 01:20:40.200]   No.
[01:20:40.200 --> 01:20:41.200]   No.
[01:20:41.200 --> 01:20:42.200]   Wasn't that cold?
[01:20:42.200 --> 01:20:43.200]   Wasn't that cold?
[01:20:43.200 --> 01:20:44.200]   Wasn't that cold?
[01:20:44.200 --> 01:20:45.200]   Wasn't that cold?
[01:20:45.200 --> 01:20:46.200]   Wasn't that cold?
[01:20:46.200 --> 01:20:47.200]   Wasn't that cold?
[01:20:47.200 --> 01:20:48.200]   Wasn't that cold?
[01:20:48.200 --> 01:20:49.200]   Wasn't that cold?
[01:20:49.200 --> 01:20:50.200]   Wasn't that cold?
[01:20:50.200 --> 01:20:51.200]   Wasn't that cold?
[01:20:51.200 --> 01:20:52.200]   Wasn't that cold?
[01:20:52.200 --> 01:20:53.200]   Wasn't that cold?
[01:20:53.200 --> 01:20:54.200]   Wasn't that cold?
[01:20:54.200 --> 01:20:55.200]   Wasn't that cold?
[01:20:55.200 --> 01:20:56.200]   Wasn't that cold?
[01:20:56.200 --> 01:20:57.200]   Wasn't that cold?
[01:20:57.200 --> 01:20:58.200]   Wasn't that cold?
[01:20:58.200 --> 01:20:59.200]   Wasn't that cold?
[01:20:59.200 --> 01:21:00.200]   Wasn't that cold?
[01:21:00.200 --> 01:21:01.200]   Wasn't that cold?
[01:21:01.200 --> 01:21:02.200]   Wasn't that cold?
[01:21:02.200 --> 01:21:03.200]   Wasn't that cold?
[01:21:03.200 --> 01:21:04.200]   Wasn't that cold?
[01:21:04.200 --> 01:21:05.200]   Wasn't that cold?
[01:21:05.200 --> 01:21:06.200]   Wasn't that cold?
[01:21:06.200 --> 01:21:07.200]   Wasn't that cold?
[01:21:07.200 --> 01:21:08.200]   Wasn't that cold?
[01:21:08.200 --> 01:21:09.200]   Wasn't that cold?
[01:21:09.200 --> 01:21:10.200]   Wasn't that cold?
[01:21:10.200 --> 01:21:11.200]   Wasn't that cold?
[01:21:11.200 --> 01:21:12.200]   Wasn't that cold?
[01:21:12.200 --> 01:21:13.200]   Wasn't that cold?
[01:21:13.200 --> 01:21:14.200]   Wasn't that cold?
[01:21:14.200 --> 01:21:15.200]   Wasn't that cold?
[01:21:15.200 --> 01:21:16.200]   Wasn't that cold?
[01:21:16.200 --> 01:21:17.200]   Wasn't that cold?
[01:21:17.200 --> 01:21:18.200]   Wasn't that cold?
[01:21:18.200 --> 01:21:19.200]   Wasn't that cold?
[01:21:19.200 --> 01:21:20.200]   Wasn't that cold?
[01:21:20.200 --> 01:21:21.200]   Wasn't that cold?
[01:21:21.200 --> 01:21:22.200]   Wasn't that cold?
[01:21:22.200 --> 01:21:23.200]   Wasn't that cold?
[01:21:23.200 --> 01:21:24.200]   Wasn't that cold?
[01:21:24.200 --> 01:21:25.200]   Wasn't that cold?
[01:21:25.200 --> 01:21:26.200]   Wasn't that cold?
[01:21:26.200 --> 01:21:27.200]   Wasn't that cold?
[01:21:27.200 --> 01:21:28.200]   Wasn't that cold?
[01:21:28.200 --> 01:21:29.200]   Wasn't that cold?
[01:21:29.200 --> 01:21:30.200]   Wasn't that cold?
[01:21:30.200 --> 01:21:32.200]   Wasn't that cold?
[01:21:32.200 --> 01:21:33.200]   Wasn't that cold?
[01:21:33.200 --> 01:21:34.200]   Wasn't that cold?
[01:21:34.200 --> 01:21:35.200]   Wasn't that cold?
[01:21:35.200 --> 01:21:36.200]   Wasn't that cold?
[01:21:36.200 --> 01:21:37.200]   Wasn't that cold?
[01:21:37.200 --> 01:21:38.200]   Wasn't that cold?
[01:21:38.200 --> 01:21:39.200]   Wasn't that cold?
[01:21:39.200 --> 01:21:40.200]   Wasn't that cold?
[01:21:40.200 --> 01:21:41.200]   Wasn't that cold?
[01:21:41.200 --> 01:21:42.200]   Wasn't that cold?
[01:21:42.200 --> 01:21:43.200]   Wasn't that cold?
[01:21:43.200 --> 01:21:44.200]   Wasn't that cold?
[01:21:44.200 --> 01:21:45.200]   Wasn't that cold?
[01:21:45.200 --> 01:21:46.200]   Wasn't that cold?
[01:21:46.200 --> 01:21:47.200]   Wasn't that cold?
[01:21:47.200 --> 01:21:48.200]   Wasn't that cold?
[01:21:48.200 --> 01:21:49.200]   Wasn't that cold?
[01:21:49.200 --> 01:21:50.200]   Wasn't that cold?
[01:21:50.200 --> 01:21:51.200]   Wasn't that cold?
[01:21:51.200 --> 01:21:52.200]   Wasn't that cold?
[01:21:52.200 --> 01:21:53.200]   Wasn't that cold?
[01:21:53.200 --> 01:21:54.200]   Wasn't that cold?
[01:21:54.200 --> 01:21:55.200]   Wasn't that cold?
[01:21:55.200 --> 01:21:56.200]   Wasn't that cold?
[01:21:56.200 --> 01:21:57.200]   Wasn't that cold?
[01:21:57.200 --> 01:21:58.200]   Wasn't that cold?
[01:21:58.200 --> 01:22:00.200]   Wasn't that cold?
[01:22:00.200 --> 01:22:01.200]   Wasn't that cold?
[01:22:01.200 --> 01:22:02.200]   Wasn't that cold?
[01:22:02.200 --> 01:22:03.200]   Wasn't that cold?
[01:22:03.200 --> 01:22:04.200]   Wasn't that cold?
[01:22:04.200 --> 01:22:05.200]   Wasn't that cold?
[01:22:05.200 --> 01:22:06.200]   Wasn't that cold?
[01:22:06.200 --> 01:22:07.200]   Wasn't that cold?
[01:22:07.200 --> 01:22:08.200]   Wasn't that cold?
[01:22:08.200 --> 01:22:09.200]   Wasn't that cold?
[01:22:09.200 --> 01:22:10.200]   Wasn't that cold?
[01:22:10.200 --> 01:22:11.200]   Wasn't that cold?
[01:22:11.200 --> 01:22:12.200]   Wasn't that cold?
[01:22:12.200 --> 01:22:13.200]   Wasn't that cold?
[01:22:13.200 --> 01:22:14.200]   Wasn't that cold?
[01:22:14.200 --> 01:22:15.200]   Wasn't that cold?
[01:22:15.200 --> 01:22:16.200]   Wasn't that cold?
[01:22:16.200 --> 01:22:17.200]   Wasn't that cold?
[01:22:17.200 --> 01:22:18.200]   Wasn't that cold?
[01:22:18.200 --> 01:22:19.200]   Wasn't that cold?
[01:22:19.200 --> 01:22:20.200]   Wasn't that cold?
[01:22:20.200 --> 01:22:21.200]   Wasn't that cold?
[01:22:21.200 --> 01:22:22.200]   Wasn't that cold?
[01:22:22.200 --> 01:22:23.200]   Wasn't that cold?
[01:22:23.200 --> 01:22:24.200]   Wasn't that cold?
[01:22:24.200 --> 01:22:25.200]   Wasn't that cold?
[01:22:25.200 --> 01:22:26.200]   Wasn't that cold?
[01:22:26.200 --> 01:22:28.200]   Wasn't that cold?
[01:22:28.200 --> 01:22:29.200]   Wasn't that cold?
[01:22:29.200 --> 01:22:30.200]   Wasn't that cold?
[01:22:30.200 --> 01:22:31.200]   Wasn't that cold?
[01:22:31.200 --> 01:22:32.200]   Wasn't that cold?
[01:22:32.200 --> 01:22:33.200]   Wasn't that cold?
[01:22:33.200 --> 01:22:34.200]   Wasn't that cold?
[01:22:34.200 --> 01:22:35.200]   Wasn't that cold?
[01:22:35.200 --> 01:22:36.200]   Wasn't that cold?
[01:22:36.200 --> 01:22:37.200]   Wasn't that cold?
[01:22:37.200 --> 01:22:38.200]   Wasn't that cold?
[01:22:38.200 --> 01:22:39.200]   Wasn't that cold?
[01:22:39.200 --> 01:22:40.200]   Wasn't that cold?
[01:22:40.200 --> 01:22:41.200]   Wasn't that cold?
[01:22:41.200 --> 01:22:42.200]   Wasn't that cold?
[01:22:42.200 --> 01:22:43.200]   Wasn't that cold?
[01:22:43.200 --> 01:22:44.200]   Wasn't that cold?
[01:22:44.200 --> 01:22:45.200]   Wasn't that cold?
[01:22:45.200 --> 01:22:46.200]   Wasn't that cold?
[01:22:46.200 --> 01:22:47.200]   Wasn't that cold?
[01:22:47.200 --> 01:22:48.200]   Wasn't that cold?
[01:22:48.200 --> 01:22:49.200]   Wasn't that cold?
[01:22:49.200 --> 01:22:50.200]   Wasn't that cold?
[01:22:50.200 --> 01:22:51.200]   Wasn't that cold?
[01:22:51.200 --> 01:22:52.200]   Wasn't that cold?
[01:22:52.200 --> 01:22:53.200]   Wasn't that cold?
[01:22:53.200 --> 01:22:54.200]   Wasn't that cold?
[01:22:54.200 --> 01:22:56.200]   Wasn't that cold?
[01:22:56.200 --> 01:22:57.200]   Wasn't that cold?
[01:22:57.200 --> 01:22:58.200]   Wasn't that cold?
[01:22:58.200 --> 01:22:59.200]   Wasn't that cold?
[01:22:59.200 --> 01:23:00.200]   Wasn't that cold?
[01:23:00.200 --> 01:23:01.200]   Wasn't that cold?
[01:23:01.200 --> 01:23:02.200]   Wasn't that cold?
[01:23:02.200 --> 01:23:03.200]   Wasn't that cold?
[01:23:03.200 --> 01:23:04.200]   Wasn't that cold?
[01:23:04.200 --> 01:23:05.200]   Wasn't that cold?
[01:23:05.200 --> 01:23:06.200]   Wasn't that cold?
[01:23:06.200 --> 01:23:07.200]   Wasn't that cold?
[01:23:07.200 --> 01:23:08.200]   Wasn't that cold?
[01:23:08.200 --> 01:23:09.200]   Wasn't that cold?
[01:23:09.200 --> 01:23:10.200]   Wasn't that cold?
[01:23:10.200 --> 01:23:11.200]   Wasn't that cold?
[01:23:11.200 --> 01:23:12.200]   Wasn't that cold?
[01:23:12.200 --> 01:23:13.200]   Wasn't that cold?
[01:23:13.200 --> 01:23:14.200]   Wasn't that cold?
[01:23:14.200 --> 01:23:15.200]   Wasn't that cold?
[01:23:15.200 --> 01:23:16.200]   Wasn't that cold?
[01:23:16.200 --> 01:23:17.200]   Wasn't that cold?
[01:23:17.200 --> 01:23:18.200]   Wasn't that cold?
[01:23:18.200 --> 01:23:19.200]   Wasn't that cold?
[01:23:19.200 --> 01:23:20.200]   Wasn't that cold?
[01:23:20.200 --> 01:23:21.200]   Wasn't that cold?
[01:23:21.200 --> 01:23:22.200]   Wasn't that cold?
[01:23:22.200 --> 01:23:24.200]   Wasn't that cold?
[01:23:24.200 --> 01:23:25.200]   Wasn't that cold?
[01:23:25.200 --> 01:23:26.200]   Wasn't that cold?
[01:23:26.200 --> 01:23:27.200]   Wasn't that cold?
[01:23:27.200 --> 01:23:28.200]   Wasn't that cold?
[01:23:28.200 --> 01:23:29.200]   Wasn't that cold?
[01:23:29.200 --> 01:23:30.200]   Wasn't that cold?
[01:23:30.200 --> 01:23:31.200]   Wasn't that cold?
[01:23:31.200 --> 01:23:32.200]   Wasn't that cold?
[01:23:32.200 --> 01:23:33.200]   Wasn't that cold?
[01:23:33.200 --> 01:23:34.200]   Wasn't that cold?
[01:23:34.200 --> 01:23:35.200]   Wasn't that cold?
[01:23:35.200 --> 01:23:36.200]   Wasn't that cold?
[01:23:36.200 --> 01:23:37.200]   Wasn't that cold?
[01:23:37.200 --> 01:23:38.200]   Wasn't that cold?
[01:23:38.200 --> 01:23:39.200]   Wasn't that cold?
[01:23:39.200 --> 01:23:40.200]   Wasn't that cold?
[01:23:40.200 --> 01:23:41.200]   Wasn't that cold?
[01:23:41.200 --> 01:23:42.200]   Wasn't that cold?
[01:23:42.200 --> 01:23:43.200]   Wasn't that cold?
[01:23:43.200 --> 01:23:44.200]   Wasn't that cold?
[01:23:44.200 --> 01:23:45.200]   Wasn't that cold?
[01:23:45.200 --> 01:23:46.200]   Wasn't that cold?
[01:23:46.200 --> 01:23:47.200]   Wasn't that cold?
[01:23:47.200 --> 01:23:48.200]   Wasn't that cold?
[01:23:48.200 --> 01:23:49.200]   Wasn't that cold?
[01:23:49.200 --> 01:23:50.200]   Wasn't that cold?
[01:23:50.200 --> 01:23:52.200]   Wasn't that cold?
[01:23:52.200 --> 01:23:53.200]   Wasn't that cold?
[01:23:53.200 --> 01:23:54.200]   Wasn't that cold?
[01:23:54.200 --> 01:23:55.200]   Wasn't that cold?
[01:23:55.200 --> 01:23:56.200]   Wasn't that cold?
[01:23:56.200 --> 01:23:57.200]   Wasn't that cold?
[01:23:57.200 --> 01:23:58.200]   Wasn't that cold?
[01:23:58.200 --> 01:23:59.200]   Wasn't that cold?
[01:23:59.200 --> 01:24:00.200]   Wasn't that cold?
[01:24:00.200 --> 01:24:01.200]   Wasn't that cold?
[01:24:01.200 --> 01:24:02.200]   Wasn't that cold?
[01:24:02.200 --> 01:24:03.200]   Wasn't that cold?
[01:24:03.200 --> 01:24:04.200]   Wasn't that cold?
[01:24:04.200 --> 01:24:05.200]   Wasn't that cold?
[01:24:05.200 --> 01:24:06.200]   Wasn't that cold?
[01:24:06.200 --> 01:24:07.200]   Wasn't that cold?
[01:24:07.200 --> 01:24:08.200]   Wasn't that cold?
[01:24:08.200 --> 01:24:09.200]   Wasn't that cold?
[01:24:09.200 --> 01:24:10.200]   Wasn't that cold?
[01:24:10.200 --> 01:24:11.200]   Wasn't that cold?
[01:24:11.200 --> 01:24:12.200]   Wasn't that cold?
[01:24:12.200 --> 01:24:13.200]   Wasn't that cold?
[01:24:13.200 --> 01:24:14.200]   Wasn't that cold?
[01:24:14.200 --> 01:24:15.200]   Wasn't that cold?
[01:24:15.200 --> 01:24:16.200]   Wasn't that cold?
[01:24:16.200 --> 01:24:17.200]   Wasn't that cold?
[01:24:17.200 --> 01:24:18.200]   Wasn't that cold?
[01:24:18.200 --> 01:24:19.200]   Wasn't that cold?
[01:24:19.200 --> 01:24:20.200]   Wasn't that cold?
[01:24:20.200 --> 01:24:21.200]   Wasn't that cold?
[01:24:21.200 --> 01:24:22.200]   Wasn't that cold?
[01:24:22.200 --> 01:24:23.200]   Wasn't that cold?
[01:24:23.200 --> 01:24:24.200]   Wasn't that cold?
[01:24:24.200 --> 01:24:25.200]   Wasn't that cold?
[01:24:25.200 --> 01:24:26.200]   Wasn't that cold?
[01:24:26.200 --> 01:24:27.200]   Wasn't that cold?
[01:24:27.200 --> 01:24:28.200]   Wasn't that cold?
[01:24:28.200 --> 01:24:29.200]   Wasn't that cold?
[01:24:29.200 --> 01:24:30.200]   Wasn't that cold?
[01:24:30.200 --> 01:24:31.200]   Wasn't that cold?
[01:24:31.200 --> 01:24:32.200]   Wasn't that cold?
[01:24:32.200 --> 01:24:33.200]   Wasn't that cold?
[01:24:33.200 --> 01:24:34.200]   Wasn't that cold?
[01:24:34.200 --> 01:24:35.200]   Wasn't that cold?
[01:24:35.200 --> 01:24:36.200]   Wasn't that cold?
[01:24:36.200 --> 01:24:37.200]   Wasn't that cold?
[01:24:37.200 --> 01:24:38.200]   Wasn't that cold?
[01:24:38.200 --> 01:24:39.200]   Wasn't that cold?
[01:24:39.200 --> 01:24:40.200]   Wasn't that cold?
[01:24:40.200 --> 01:24:41.200]   Wasn't that cold?
[01:24:41.200 --> 01:24:42.200]   Wasn't that cold?
[01:24:42.200 --> 01:24:43.200]   Wasn't that cold?
[01:24:43.200 --> 01:24:44.200]   Wasn't that cold?
[01:24:44.200 --> 01:24:45.200]   Wasn't that cold?
[01:24:45.200 --> 01:24:47.200]   Wasn't that cold?
[01:24:47.200 --> 01:24:48.200]   Wasn't that cold?
[01:24:48.200 --> 01:24:49.200]   Wasn't that cold?
[01:24:49.200 --> 01:24:50.200]   Wasn't that cold?
[01:24:50.200 --> 01:24:51.200]   Wasn't that cold?
[01:24:51.200 --> 01:24:52.200]   Wasn't that cold?
[01:24:52.200 --> 01:24:53.200]   Wasn't that cold?
[01:24:53.200 --> 01:24:54.200]   Wasn't that cold?
[01:24:54.200 --> 01:24:55.200]   Wasn't that cold?
[01:24:55.200 --> 01:24:56.200]   Wasn't that cold?
[01:24:56.200 --> 01:24:57.200]   Wasn't that cold?
[01:24:57.200 --> 01:24:58.200]   Wasn't that cold?
[01:24:58.200 --> 01:24:59.200]   Wasn't that cold?
[01:24:59.200 --> 01:25:00.200]   Wasn't that cold?
[01:25:00.200 --> 01:25:01.200]   Wasn't that cold?
[01:25:01.200 --> 01:25:02.200]   Wasn't that cold?
[01:25:02.200 --> 01:25:03.200]   Wasn't that cold?
[01:25:03.200 --> 01:25:04.200]   Wasn't that cold?
[01:25:04.200 --> 01:25:05.200]   Wasn't that cold?
[01:25:05.200 --> 01:25:06.200]   Wasn't that cold?
[01:25:06.200 --> 01:25:07.200]   Wasn't that cold?
[01:25:07.200 --> 01:25:08.200]   Wasn't that cold?
[01:25:08.200 --> 01:25:09.200]   Wasn't that cold?
[01:25:09.200 --> 01:25:10.200]   Wasn't that cold?
[01:25:10.200 --> 01:25:11.200]   Wasn't that cold?
[01:25:11.200 --> 01:25:12.200]   Wasn't that cold?
[01:25:12.200 --> 01:25:13.200]   Wasn't that cold?
[01:25:13.200 --> 01:25:15.200]   Wasn't that cold?
[01:25:15.200 --> 01:25:16.200]   Wasn't that cold?
[01:25:16.200 --> 01:25:17.200]   Wasn't that cold?
[01:25:17.200 --> 01:25:18.200]   Wasn't that cold?
[01:25:18.200 --> 01:25:19.200]   Wasn't that cold?
[01:25:19.200 --> 01:25:20.200]   Wasn't that cold?
[01:25:20.200 --> 01:25:21.200]   Wasn't that cold?
[01:25:21.200 --> 01:25:22.200]   Wasn't that cold?
[01:25:22.200 --> 01:25:23.200]   Wasn't that cold?
[01:25:23.200 --> 01:25:24.200]   Wasn't that cold?
[01:25:24.200 --> 01:25:25.200]   Wasn't that cold?
[01:25:25.200 --> 01:25:26.200]   Wasn't that cold?
[01:25:26.200 --> 01:25:27.200]   Wasn't that cold?
[01:25:27.200 --> 01:25:28.200]   Wasn't that cold?
[01:25:28.200 --> 01:25:29.200]   Wasn't that cold?
[01:25:29.200 --> 01:25:30.200]   Wasn't that cold?
[01:25:30.200 --> 01:25:31.200]   Wasn't that cold?
[01:25:31.200 --> 01:25:32.200]   Wasn't that cold?
[01:25:32.200 --> 01:25:33.200]   Wasn't that cold?
[01:25:33.200 --> 01:25:34.200]   Wasn't that cold?
[01:25:34.200 --> 01:25:35.200]   Wasn't that cold?
[01:25:35.200 --> 01:25:36.200]   Wasn't that cold?
[01:25:36.200 --> 01:25:37.200]   Wasn't that cold?
[01:25:37.200 --> 01:25:38.200]   Wasn't that cold?
[01:25:38.200 --> 01:25:39.200]   Wasn't that cold?
[01:25:39.200 --> 01:25:40.200]   Wasn't that cold?
[01:25:40.200 --> 01:25:41.200]   Wasn't that cold?
[01:25:41.200 --> 01:25:43.200]   Wasn't that cold?
[01:25:43.200 --> 01:25:44.200]   Wasn't that cold?
[01:25:44.200 --> 01:25:45.200]   Wasn't that cold?
[01:25:45.200 --> 01:25:46.200]   Wasn't that cold?
[01:25:46.200 --> 01:25:47.200]   Wasn't that cold?
[01:25:47.200 --> 01:25:48.200]   Wasn't that cold?
[01:25:48.200 --> 01:25:49.200]   Wasn't that cold?
[01:25:49.200 --> 01:25:50.200]   Wasn't that cold?
[01:25:50.200 --> 01:25:51.200]   Wasn't that cold?
[01:25:51.200 --> 01:25:52.200]   Wasn't that cold?
[01:25:52.200 --> 01:25:53.200]   Wasn't that cold?
[01:25:53.200 --> 01:25:54.200]   Wasn't that cold?
[01:25:54.200 --> 01:25:55.200]   Wasn't that cold?
[01:25:55.200 --> 01:25:56.200]   Wasn't that cold?
[01:25:56.200 --> 01:25:57.200]   Wasn't that cold?
[01:25:57.200 --> 01:25:58.200]   Wasn't that cold?
[01:25:58.200 --> 01:25:59.200]   Wasn't that cold?
[01:25:59.200 --> 01:26:00.200]   Wasn't that cold?
[01:26:00.200 --> 01:26:01.200]   Wasn't that cold?
[01:26:01.200 --> 01:26:02.200]   Wasn't that cold?
[01:26:02.200 --> 01:26:03.200]   Wasn't that cold?
[01:26:03.200 --> 01:26:04.200]   Wasn't that cold?
[01:26:04.200 --> 01:26:05.200]   Wasn't that cold?
[01:26:05.200 --> 01:26:06.200]   Wasn't that cold?
[01:26:06.200 --> 01:26:07.200]   Wasn't that cold?
[01:26:07.200 --> 01:26:08.200]   Wasn't that cold?
[01:26:08.200 --> 01:26:09.200]   Wasn't that cold?
[01:26:09.200 --> 01:26:10.200]   Wasn't that cold?
[01:26:10.200 --> 01:26:11.200]   Wasn't that cold?
[01:26:11.200 --> 01:26:12.200]   Wasn't that cold?
[01:26:12.200 --> 01:26:13.200]   Wasn't that cold?
[01:26:13.200 --> 01:26:14.200]   Wasn't that cold?
[01:26:14.200 --> 01:26:15.200]   Wasn't that cold?
[01:26:15.200 --> 01:26:16.200]   Wasn't that cold?
[01:26:16.200 --> 01:26:17.200]   Wasn't that cold?
[01:26:17.200 --> 01:26:18.200]   Wasn't that cold?
[01:26:18.200 --> 01:26:19.200]   Wasn't that cold?
[01:26:19.200 --> 01:26:20.200]   Wasn't that cold?
[01:26:20.200 --> 01:26:21.200]   Wasn't that cold?
[01:26:21.200 --> 01:26:22.200]   Wasn't that cold?
[01:26:22.200 --> 01:26:23.200]   Wasn't that cold?
[01:26:23.200 --> 01:26:24.200]   Wasn't that cold?
[01:26:24.200 --> 01:26:25.200]   Wasn't that cold?
[01:26:25.200 --> 01:26:26.200]   Wasn't that cold?
[01:26:26.200 --> 01:26:27.200]   Wasn't that cold?
[01:26:27.200 --> 01:26:28.200]   Wasn't that cold?
[01:26:28.200 --> 01:26:29.200]   Wasn't that cold?
[01:26:29.200 --> 01:26:30.200]   Wasn't that cold?
[01:26:30.200 --> 01:26:31.200]   Wasn't that cold?
[01:26:31.200 --> 01:26:32.200]   Wasn't that cold?
[01:26:32.200 --> 01:26:33.200]   Wasn't that cold?
[01:26:33.200 --> 01:26:34.200]   Wasn't that cold?
[01:26:34.200 --> 01:26:35.200]   Wasn't that cold?
[01:26:35.200 --> 01:26:36.200]   Wasn't that cold?
[01:26:36.200 --> 01:26:38.200]   Wasn't that cold?
[01:26:38.200 --> 01:26:39.200]   Wasn't that cold?
[01:26:39.200 --> 01:26:40.200]   Wasn't that cold?
[01:26:40.200 --> 01:26:41.200]   Wasn't that cold?
[01:26:41.200 --> 01:26:42.200]   Wasn't that cold?
[01:26:42.200 --> 01:26:43.200]   Wasn't that cold?
[01:26:43.200 --> 01:26:44.200]   Wasn't that cold?
[01:26:44.200 --> 01:26:45.200]   Wasn't that cold?
[01:26:45.200 --> 01:26:46.200]   Wasn't that cold?
[01:26:46.200 --> 01:26:47.200]   Wasn't that cold?
[01:26:47.200 --> 01:26:48.200]   Wasn't that cold?
[01:26:48.200 --> 01:26:49.200]   Wasn't that cold?
[01:26:49.200 --> 01:26:50.200]   Wasn't that cold?
[01:26:50.200 --> 01:26:51.200]   Wasn't that cold?
[01:26:51.200 --> 01:26:52.200]   Wasn't that cold?
[01:26:52.200 --> 01:26:53.200]   Wasn't that cold?
[01:26:53.200 --> 01:26:54.200]   Wasn't that cold?
[01:26:54.200 --> 01:26:55.200]   Wasn't that cold?
[01:26:55.200 --> 01:26:56.200]   Wasn't that cold?
[01:26:56.200 --> 01:26:57.200]   Wasn't that cold?
[01:26:57.200 --> 01:26:58.200]   Wasn't that cold?
[01:26:58.200 --> 01:26:59.200]   Wasn't that cold?
[01:26:59.200 --> 01:27:00.200]   Wasn't that cold?
[01:27:00.200 --> 01:27:01.200]   Wasn't that cold?
[01:27:01.200 --> 01:27:02.200]   Wasn't that cold?
[01:27:02.200 --> 01:27:03.200]   Wasn't that cold?
[01:27:03.200 --> 01:27:04.200]   Wasn't that cold?
[01:27:04.200 --> 01:27:05.200]   Wasn't that cold?
[01:27:05.200 --> 01:27:06.200]   Wasn't that cold?
[01:27:06.200 --> 01:27:07.200]   Wasn't that cold?
[01:27:07.200 --> 01:27:08.200]   Wasn't that cold?
[01:27:08.200 --> 01:27:09.200]   Wasn't that cold?
[01:27:09.200 --> 01:27:10.200]   Wasn't that cold?
[01:27:10.200 --> 01:27:11.200]   Wasn't that cold?
[01:27:11.200 --> 01:27:12.200]   Wasn't that cold?
[01:27:12.200 --> 01:27:13.200]   Wasn't that cold?
[01:27:13.200 --> 01:27:14.200]   Wasn't that cold?
[01:27:14.200 --> 01:27:15.200]   Wasn't that cold?
[01:27:15.200 --> 01:27:16.200]   Wasn't that cold?
[01:27:16.200 --> 01:27:17.200]   Wasn't that cold?
[01:27:17.200 --> 01:27:18.200]   Wasn't that cold?
[01:27:18.200 --> 01:27:19.200]   Wasn't that cold?
[01:27:19.200 --> 01:27:20.200]   Wasn't that cold?
[01:27:20.200 --> 01:27:21.200]   Wasn't that cold?
[01:27:21.200 --> 01:27:22.200]   Wasn't that cold?
[01:27:22.200 --> 01:27:23.200]   Wasn't that cold?
[01:27:23.200 --> 01:27:24.200]   Wasn't that cold?
[01:27:24.200 --> 01:27:25.200]   Wasn't that cold?
[01:27:25.200 --> 01:27:26.200]   Wasn't that cold?
[01:27:26.200 --> 01:27:27.200]   Wasn't that cold?
[01:27:27.200 --> 01:27:28.200]   Wasn't that cold?
[01:27:28.200 --> 01:27:29.200]   Wasn't that cold?
[01:27:29.200 --> 01:27:30.200]   Wasn't that cold?
[01:27:30.200 --> 01:27:31.200]   Wasn't that cold?
[01:27:31.200 --> 01:27:32.200]   Wasn't that cold?
[01:27:32.200 --> 01:27:33.200]   Wasn't that cold?
[01:27:33.200 --> 01:27:34.200]   Wasn't that cold?
[01:27:34.200 --> 01:27:35.200]   Wasn't that cold?
[01:27:35.200 --> 01:27:36.200]   Wasn't that cold?
[01:27:36.200 --> 01:27:37.200]   Wasn't that cold?
[01:27:37.200 --> 01:27:38.200]   Wasn't that cold?
[01:27:38.200 --> 01:27:39.200]   Wasn't that cold?
[01:27:39.200 --> 01:27:40.200]   Wasn't that cold?
[01:27:40.200 --> 01:27:41.200]   Wasn't that cold?
[01:27:41.200 --> 01:27:42.200]   Wasn't that cold?
[01:27:42.200 --> 01:27:43.200]   Wasn't that cold?
[01:27:43.200 --> 01:27:44.200]   Wasn't that cold?
[01:27:44.200 --> 01:27:45.200]   Wasn't that cold?
[01:27:45.200 --> 01:27:46.200]   Wasn't that cold?
[01:27:46.200 --> 01:27:47.200]   Wasn't that cold?
[01:27:47.200 --> 01:27:48.200]   Wasn't that cold?
[01:27:48.200 --> 01:27:49.200]   Wasn't that cold?
[01:27:49.200 --> 01:27:50.200]   Wasn't that cold?
[01:27:50.200 --> 01:27:51.200]   Wasn't that cold?
[01:27:51.200 --> 01:27:52.200]   Wasn't that cold?
[01:27:52.200 --> 01:27:53.200]   Wasn't that cold?
[01:27:53.200 --> 01:27:54.200]   Wasn't that cold?
[01:27:54.200 --> 01:27:55.200]   Wasn't that cold?
[01:27:55.200 --> 01:27:56.200]   Wasn't that cold?
[01:27:56.200 --> 01:27:57.200]   Wasn't that cold?
[01:27:57.200 --> 01:27:58.200]   Wasn't that cold?
[01:27:58.200 --> 01:28:05.200]   Wasn't that cold?
[01:28:05.200 --> 01:28:08.200]   Wasn't that cold?
[01:28:08.200 --> 01:28:09.200]   Wasn't that cold?
[01:28:09.200 --> 01:28:10.200]   Wasn't that cold?
[01:28:10.200 --> 01:28:11.200]   Wasn't that cold?
[01:28:11.200 --> 01:28:12.200]   Wasn't that cold?
[01:28:12.200 --> 01:28:13.200]   Wasn't that cold?
[01:28:13.200 --> 01:28:14.200]   Wasn't that cold?
[01:28:14.200 --> 01:28:15.200]   Wasn't that cold?
[01:28:15.200 --> 01:28:16.200]   Wasn't that cold?
[01:28:16.200 --> 01:28:17.200]   Wasn't that cold?
[01:28:17.200 --> 01:28:18.200]   Wasn't that cold?
[01:28:18.200 --> 01:28:19.200]   Wasn't that cold?
[01:28:19.200 --> 01:28:20.200]   Wasn't that cold?
[01:28:20.200 --> 01:28:21.200]   Wasn't that cold?
[01:28:21.200 --> 01:28:22.200]   Wasn't that cold?
[01:28:22.200 --> 01:28:23.200]   Wasn't that cold?
[01:28:23.200 --> 01:28:24.200]   Wasn't that cold?
[01:28:24.200 --> 01:28:25.200]   Wasn't that cold?
[01:28:25.200 --> 01:28:26.200]   Wasn't that cold?
[01:28:26.200 --> 01:28:27.200]   Wasn't that cold?
[01:28:27.200 --> 01:28:28.200]   Wasn't that cold?
[01:28:28.200 --> 01:28:29.200]   Wasn't that cold?
[01:28:29.200 --> 01:28:30.200]   Wasn't that cold?
[01:28:30.200 --> 01:28:31.200]   Wasn't that cold?
[01:28:31.200 --> 01:28:32.200]   Wasn't that cold?
[01:28:32.200 --> 01:28:33.200]   Wasn't that cold?
[01:28:33.200 --> 01:28:34.200]   Wasn't that cold?
[01:28:34.200 --> 01:28:35.200]   Wasn't that cold?
[01:28:35.200 --> 01:28:36.200]   Wasn't that cold?
[01:28:36.200 --> 01:28:37.200]   Wasn't that cold?
[01:28:37.200 --> 01:28:38.200]   Wasn't that cold?
[01:28:38.200 --> 01:28:39.200]   Wasn't that cold?
[01:28:39.200 --> 01:28:40.200]   Wasn't that cold?
[01:28:40.200 --> 01:28:41.200]   Wasn't that cold?
[01:28:41.200 --> 01:28:42.200]   Wasn't that cold?
[01:28:42.200 --> 01:28:43.200]   Wasn't that cold?
[01:28:43.200 --> 01:28:44.200]   Wasn't that cold?
[01:28:44.200 --> 01:28:45.200]   Wasn't that cold?
[01:28:45.200 --> 01:28:46.200]   Wasn't that cold?
[01:28:46.200 --> 01:28:47.200]   Wasn't that cold?
[01:28:47.200 --> 01:28:48.200]   Wasn't that cold?
[01:28:48.200 --> 01:28:49.200]   Wasn't that cold?
[01:28:49.200 --> 01:28:50.200]   Wasn't that cold?
[01:28:50.200 --> 01:28:51.200]   Wasn't that cold?
[01:28:51.200 --> 01:28:52.200]   Wasn't that cold?
[01:28:52.200 --> 01:28:53.200]   Wasn't that cold?
[01:28:53.200 --> 01:28:54.200]   Wasn't that cold?
[01:28:54.200 --> 01:28:56.200]   Wasn't that cold?
[01:28:56.200 --> 01:28:57.200]   Wasn't that cold?
[01:28:57.200 --> 01:28:58.200]   Wasn't that cold?
[01:28:58.200 --> 01:28:59.200]   Wasn't that cold?
[01:28:59.200 --> 01:29:00.200]   Wasn't that cold?
[01:29:00.200 --> 01:29:01.200]   Wasn't that cold?
[01:29:01.200 --> 01:29:02.200]   Wasn't that cold?
[01:29:02.200 --> 01:29:03.200]   Wasn't that cold?
[01:29:03.200 --> 01:29:04.200]   Wasn't that cold?
[01:29:04.200 --> 01:29:05.200]   Wasn't that cold?
[01:29:05.200 --> 01:29:06.200]   Wasn't that cold?
[01:29:06.200 --> 01:29:07.200]   Wasn't that cold?
[01:29:07.200 --> 01:29:08.200]   Wasn't that cold?
[01:29:08.200 --> 01:29:09.200]   Wasn't that cold?
[01:29:09.200 --> 01:29:10.200]   Wasn't that cold?
[01:29:10.200 --> 01:29:11.200]   Wasn't that cold?
[01:29:11.200 --> 01:29:12.200]   Wasn't that cold?
[01:29:12.200 --> 01:29:13.200]   Wasn't that cold?
[01:29:13.200 --> 01:29:14.200]   Wasn't that cold?
[01:29:14.200 --> 01:29:15.200]   Wasn't that cold?
[01:29:15.200 --> 01:29:16.200]   Wasn't that cold?
[01:29:16.200 --> 01:29:17.200]   Wasn't that cold?
[01:29:17.200 --> 01:29:18.200]   Wasn't that cold?
[01:29:18.200 --> 01:29:19.200]   Wasn't that cold?
[01:29:19.200 --> 01:29:20.200]   Wasn't that cold?
[01:29:20.200 --> 01:29:21.200]   Wasn't that cold?
[01:29:21.200 --> 01:29:22.200]   Wasn't that cold?
[01:29:22.200 --> 01:29:24.200]   Wasn't that cold?
[01:29:24.200 --> 01:29:25.200]   Wasn't that cold?
[01:29:25.200 --> 01:29:26.200]   Wasn't that cold?
[01:29:26.200 --> 01:29:27.200]   Wasn't that cold?
[01:29:27.200 --> 01:29:28.200]   Wasn't that cold?
[01:29:28.200 --> 01:29:29.200]   Wasn't that cold?
[01:29:29.200 --> 01:29:30.200]   Wasn't that cold?
[01:29:30.200 --> 01:29:31.200]   Wasn't that cold?
[01:29:31.200 --> 01:29:32.200]   Wasn't that cold?
[01:29:32.200 --> 01:29:33.200]   Wasn't that cold?
[01:29:33.200 --> 01:29:34.200]   Wasn't that cold?
[01:29:34.200 --> 01:29:35.200]   Wasn't that cold?
[01:29:35.200 --> 01:29:36.200]   Wasn't that cold?
[01:29:36.200 --> 01:29:37.200]   Wasn't that cold?
[01:29:37.200 --> 01:29:38.200]   Wasn't that cold?
[01:29:38.200 --> 01:29:39.200]   Wasn't that cold?
[01:29:39.200 --> 01:29:40.200]   Wasn't that cold?
[01:29:40.200 --> 01:29:41.200]   Wasn't that cold?
[01:29:41.200 --> 01:29:42.200]   Wasn't that cold?
[01:29:42.200 --> 01:29:43.200]   Wasn't that cold?
[01:29:43.200 --> 01:29:44.200]   Wasn't that cold?
[01:29:44.200 --> 01:29:45.200]   Wasn't that cold?
[01:29:45.200 --> 01:29:46.200]   Wasn't that cold?
[01:29:46.200 --> 01:29:47.200]   Wasn't that cold?
[01:29:47.200 --> 01:29:48.200]   Wasn't that cold?
[01:29:48.200 --> 01:29:49.200]   Wasn't that cold?
[01:29:49.200 --> 01:29:50.200]   Wasn't that cold?
[01:29:50.200 --> 01:29:52.200]   Wasn't that cold?
[01:29:52.200 --> 01:29:53.200]   Wasn't that cold?
[01:29:53.200 --> 01:29:54.200]   Wasn't that cold?
[01:29:54.200 --> 01:29:55.200]   Wasn't that cold?
[01:29:55.200 --> 01:29:56.200]   Wasn't that cold?
[01:29:56.200 --> 01:29:57.200]   Wasn't that cold?
[01:29:57.200 --> 01:29:58.200]   Wasn't that cold?
[01:29:58.200 --> 01:29:59.200]   Wasn't that cold?
[01:29:59.200 --> 01:30:00.200]   Wasn't that cold?
[01:30:00.200 --> 01:30:01.200]   Wasn't that cold?
[01:30:01.200 --> 01:30:02.200]   Wasn't that cold?
[01:30:02.200 --> 01:30:03.200]   Wasn't that cold?
[01:30:03.200 --> 01:30:04.200]   Wasn't that cold?
[01:30:04.200 --> 01:30:05.200]   Wasn't that cold?
[01:30:05.200 --> 01:30:06.200]   Wasn't that cold?
[01:30:06.200 --> 01:30:07.200]   Wasn't that cold?
[01:30:07.200 --> 01:30:08.200]   Wasn't that cold?
[01:30:08.200 --> 01:30:09.200]   Wasn't that cold?
[01:30:09.200 --> 01:30:10.200]   Wasn't that cold?
[01:30:10.200 --> 01:30:11.200]   Wasn't that cold?
[01:30:11.200 --> 01:30:12.200]   Wasn't that cold?
[01:30:12.200 --> 01:30:13.200]   Wasn't that cold?
[01:30:13.200 --> 01:30:14.200]   Wasn't that cold?
[01:30:14.200 --> 01:30:15.200]   Wasn't that cold?
[01:30:15.200 --> 01:30:16.200]   Wasn't that cold?
[01:30:16.200 --> 01:30:17.200]   Wasn't that cold?
[01:30:17.200 --> 01:30:18.200]   Wasn't that cold?
[01:30:18.200 --> 01:30:19.200]   Wasn't that cold?
[01:30:19.200 --> 01:30:20.200]   Wasn't that cold?
[01:30:20.200 --> 01:30:21.200]   Wasn't that cold?
[01:30:21.200 --> 01:30:22.200]   Wasn't that cold?
[01:30:22.200 --> 01:30:23.200]   Wasn't that cold?
[01:30:23.200 --> 01:30:24.200]   Wasn't that cold?
[01:30:24.200 --> 01:30:25.200]   Wasn't that cold?
[01:30:25.200 --> 01:30:26.200]   Wasn't that cold?
[01:30:26.200 --> 01:30:27.200]   Wasn't that cold?
[01:30:27.200 --> 01:30:28.200]   Wasn't that cold?
[01:30:28.200 --> 01:30:29.200]   Wasn't that cold?
[01:30:29.200 --> 01:30:30.200]   Wasn't that cold?
[01:30:30.200 --> 01:30:31.200]   Wasn't that cold?
[01:30:31.200 --> 01:30:32.200]   Wasn't that cold?
[01:30:32.200 --> 01:30:33.200]   Wasn't that cold?
[01:30:33.200 --> 01:30:34.200]   Wasn't that cold?
[01:30:34.200 --> 01:30:35.200]   Wasn't that cold?
[01:30:35.200 --> 01:30:36.200]   Wasn't that cold?
[01:30:36.200 --> 01:30:37.200]   Wasn't that cold?
[01:30:37.200 --> 01:30:38.200]   Wasn't that cold?
[01:30:38.200 --> 01:30:39.200]   Wasn't that cold?
[01:30:39.200 --> 01:30:40.200]   Wasn't that cold?
[01:30:40.200 --> 01:30:41.200]   Wasn't that cold?
[01:30:41.200 --> 01:30:42.200]   Wasn't that cold?
[01:30:42.200 --> 01:30:43.200]   Wasn't that cold?
[01:30:43.200 --> 01:30:44.200]   Wasn't that cold?
[01:30:44.200 --> 01:30:45.200]   Wasn't that cold?
[01:30:45.200 --> 01:30:46.200]   Wasn't that cold?
[01:30:46.200 --> 01:30:47.200]   Wasn't that cold?
[01:30:47.200 --> 01:30:48.200]   Wasn't that cold?
[01:30:48.200 --> 01:30:49.200]   Wasn't that cold?
[01:30:49.200 --> 01:30:50.200]   Wasn't that cold?
[01:30:50.200 --> 01:30:51.200]   Wasn't that cold?
[01:30:51.200 --> 01:30:52.200]   Wasn't that cold?
[01:30:52.200 --> 01:30:53.200]   Wasn't that cold?
[01:30:53.200 --> 01:30:54.200]   Wasn't that cold?
[01:30:54.200 --> 01:30:55.200]   Wasn't that cold?
[01:30:55.200 --> 01:30:56.200]   Wasn't that cold?
[01:30:56.200 --> 01:30:57.200]   Wasn't that cold?
[01:30:57.200 --> 01:30:58.200]   Wasn't that cold?
[01:30:58.200 --> 01:30:59.200]   Wasn't that cold?
[01:30:59.200 --> 01:31:00.200]   Wasn't that cold?
[01:31:00.200 --> 01:31:01.200]   Wasn't that cold?
[01:31:01.200 --> 01:31:02.200]   Wasn't that cold?
[01:31:02.200 --> 01:31:03.200]   Wasn't that cold?
[01:31:03.200 --> 01:31:04.200]   Wasn't that cold?
[01:31:04.200 --> 01:31:05.200]   Wasn't that cold?
[01:31:05.200 --> 01:31:06.200]   Wasn't that cold?
[01:31:06.200 --> 01:31:07.200]   Wasn't that cold?
[01:31:07.200 --> 01:31:08.200]   Wasn't that cold?
[01:31:08.200 --> 01:31:09.200]   Wasn't that cold?
[01:31:09.200 --> 01:31:10.200]   Wasn't that cold?
[01:31:10.200 --> 01:31:11.200]   Wasn't that cold?
[01:31:11.200 --> 01:31:12.200]   Wasn't that cold?
[01:31:12.200 --> 01:31:14.200]   Wasn't that cold?
[01:31:14.200 --> 01:31:15.200]   Wasn't that cold?
[01:31:15.200 --> 01:31:16.200]   Wasn't that cold?
[01:31:16.200 --> 01:31:17.200]   Wasn't that cold?
[01:31:17.200 --> 01:31:18.200]   Wasn't that cold?
[01:31:18.200 --> 01:31:19.200]   Wasn't that cold?
[01:31:19.200 --> 01:31:20.200]   Wasn't that cold?
[01:31:20.200 --> 01:31:21.200]   Wasn't that cold?
[01:31:21.200 --> 01:31:22.200]   Wasn't that cold?
[01:31:22.200 --> 01:31:23.200]   Wasn't that cold?
[01:31:23.200 --> 01:31:24.200]   Wasn't that cold?
[01:31:24.200 --> 01:31:25.200]   Wasn't that cold?
[01:31:25.200 --> 01:31:26.200]   Wasn't that cold?
[01:31:26.200 --> 01:31:27.200]   Wasn't that cold?
[01:31:27.200 --> 01:31:28.200]   Wasn't that cold?
[01:31:28.200 --> 01:31:29.200]   Wasn't that cold?
[01:31:29.200 --> 01:31:30.200]   Wasn't that cold?
[01:31:30.200 --> 01:31:31.200]   Wasn't that cold?
[01:31:31.200 --> 01:31:32.200]   Wasn't that cold?
[01:31:32.200 --> 01:31:33.200]   Wasn't that cold?
[01:31:33.200 --> 01:31:34.200]   Wasn't that cold?
[01:31:34.200 --> 01:31:35.200]   Wasn't that cold?
[01:31:35.200 --> 01:31:36.200]   Wasn't that cold?
[01:31:36.200 --> 01:31:37.200]   Wasn't that cold?
[01:31:37.200 --> 01:31:38.200]   Wasn't that cold?
[01:31:38.200 --> 01:31:39.200]   Wasn't that cold?
[01:31:39.200 --> 01:31:40.200]   Wasn't that cold?
[01:31:40.200 --> 01:31:42.200]   Wasn't that cold?
[01:31:42.200 --> 01:31:43.200]   Wasn't that cold?
[01:31:43.200 --> 01:31:44.200]   Wasn't that cold?
[01:31:44.200 --> 01:31:45.200]   Wasn't that cold?
[01:31:45.200 --> 01:31:46.200]   Wasn't that cold?
[01:31:46.200 --> 01:31:47.200]   Wasn't that cold?
[01:31:47.200 --> 01:31:48.200]   Wasn't that cold?
[01:31:48.200 --> 01:31:49.200]   Wasn't that cold?
[01:31:49.200 --> 01:31:50.200]   Wasn't that cold?
[01:31:50.200 --> 01:31:51.200]   Wasn't that cold?
[01:31:51.200 --> 01:31:52.200]   Wasn't that cold?
[01:31:52.200 --> 01:31:53.200]   Wasn't that cold?
[01:31:53.200 --> 01:31:54.200]   Wasn't that cold?
[01:31:54.200 --> 01:31:55.200]   Wasn't that cold?
[01:31:55.200 --> 01:31:56.200]   Wasn't that cold?
[01:31:56.200 --> 01:31:57.200]   Wasn't that cold?
[01:31:57.200 --> 01:31:58.200]   Wasn't that cold?
[01:31:58.200 --> 01:31:59.200]   Wasn't that cold?
[01:31:59.200 --> 01:32:00.200]   Wasn't that cold?
[01:32:00.200 --> 01:32:01.200]   Wasn't that cold?
[01:32:01.200 --> 01:32:02.200]   Wasn't that cold?
[01:32:02.200 --> 01:32:03.200]   Wasn't that cold?
[01:32:03.200 --> 01:32:04.200]   Wasn't that cold?
[01:32:04.200 --> 01:32:05.200]   Wasn't that cold?
[01:32:05.200 --> 01:32:06.200]   Wasn't that cold?
[01:32:06.200 --> 01:32:07.200]   Wasn't that cold?
[01:32:07.200 --> 01:32:08.200]   Wasn't that cold?
[01:32:08.200 --> 01:32:10.200]   Wasn't that cold?
[01:32:10.200 --> 01:32:11.200]   Wasn't that cold?
[01:32:11.200 --> 01:32:12.200]   Wasn't that cold?
[01:32:12.200 --> 01:32:13.200]   Wasn't that cold?
[01:32:13.200 --> 01:32:14.200]   Wasn't that cold?
[01:32:14.200 --> 01:32:15.200]   Wasn't that cold?
[01:32:15.200 --> 01:32:16.200]   Wasn't that cold?
[01:32:16.200 --> 01:32:17.200]   Wasn't that cold?
[01:32:17.200 --> 01:32:18.200]   Wasn't that cold?
[01:32:18.200 --> 01:32:19.200]   Wasn't that cold?
[01:32:19.200 --> 01:32:20.200]   Wasn't that cold?
[01:32:20.200 --> 01:32:21.200]   Wasn't that cold?
[01:32:21.200 --> 01:32:22.200]   Wasn't that cold?
[01:32:22.200 --> 01:32:23.200]   Wasn't that cold?
[01:32:23.200 --> 01:32:24.200]   Wasn't that cold?
[01:32:24.200 --> 01:32:25.200]   Wasn't that cold?
[01:32:25.200 --> 01:32:26.200]   Wasn't that cold?
[01:32:26.200 --> 01:32:27.200]   Wasn't that cold?
[01:32:27.200 --> 01:32:28.200]   Wasn't that cold?
[01:32:28.200 --> 01:32:29.200]   Wasn't that cold?
[01:32:29.200 --> 01:32:30.200]   Wasn't that cold?
[01:32:30.200 --> 01:32:31.200]   Wasn't that cold?
[01:32:31.200 --> 01:32:32.200]   Wasn't that cold?
[01:32:32.200 --> 01:32:33.200]   Wasn't that cold?
[01:32:33.200 --> 01:32:34.200]   Wasn't that cold?
[01:32:34.200 --> 01:32:35.200]   Wasn't that cold?
[01:32:35.200 --> 01:32:36.200]   Wasn't that cold?
[01:32:36.200 --> 01:32:38.200]   Wasn't that cold?
[01:32:38.200 --> 01:32:39.200]   Wasn't that cold?
[01:32:39.200 --> 01:32:40.200]   Wasn't that cold?
[01:32:40.200 --> 01:32:41.200]   Wasn't that cold?
[01:32:41.200 --> 01:32:42.200]   Wasn't that cold?
[01:32:42.200 --> 01:32:43.200]   Wasn't that cold?
[01:32:43.200 --> 01:32:44.200]   Wasn't that cold?
[01:32:44.200 --> 01:32:45.200]   Wasn't that cold?
[01:32:45.200 --> 01:32:46.200]   Wasn't that cold?
[01:32:46.200 --> 01:32:47.200]   Wasn't that cold?
[01:32:47.200 --> 01:32:48.200]   Wasn't that cold?
[01:32:48.200 --> 01:32:49.200]   Wasn't that cold?
[01:32:49.200 --> 01:32:50.200]   Wasn't that cold?
[01:32:50.200 --> 01:32:51.200]   Wasn't that cold?
[01:32:51.200 --> 01:32:52.200]   Wasn't that cold?
[01:32:52.200 --> 01:32:53.200]   Wasn't that cold?
[01:32:53.200 --> 01:32:54.200]   Wasn't that cold?
[01:32:54.200 --> 01:32:55.200]   Wasn't that cold?
[01:32:55.200 --> 01:32:56.200]   Wasn't that cold?
[01:32:56.200 --> 01:32:57.200]   Wasn't that cold?
[01:32:57.200 --> 01:32:58.200]   Wasn't that cold?
[01:32:58.200 --> 01:32:59.200]   Wasn't that cold?
[01:32:59.200 --> 01:33:00.200]   Wasn't that cold?
[01:33:00.200 --> 01:33:01.200]   Wasn't that cold?
[01:33:01.200 --> 01:33:02.200]   Wasn't that cold?
[01:33:02.200 --> 01:33:03.200]   Wasn't that cold?
[01:33:03.200 --> 01:33:04.200]   Wasn't that cold?
[01:33:04.200 --> 01:33:06.200]   Wasn't that cold?
[01:33:06.200 --> 01:33:07.200]   Wasn't that cold?
[01:33:07.200 --> 01:33:08.200]   Wasn't that cold?
[01:33:08.200 --> 01:33:09.200]   Wasn't that cold?
[01:33:09.200 --> 01:33:10.200]   Wasn't that cold?
[01:33:10.200 --> 01:33:11.200]   Wasn't that cold?
[01:33:11.200 --> 01:33:12.200]   Wasn't that cold?
[01:33:12.200 --> 01:33:13.200]   Wasn't that cold?
[01:33:13.200 --> 01:33:14.200]   Wasn't that cold?
[01:33:14.200 --> 01:33:15.200]   Wasn't that cold?
[01:33:15.200 --> 01:33:16.200]   Wasn't that cold?
[01:33:16.200 --> 01:33:17.200]   Wasn't that cold?
[01:33:17.200 --> 01:33:18.200]   Wasn't that cold?
[01:33:18.200 --> 01:33:19.200]   Wasn't that cold?
[01:33:19.200 --> 01:33:20.200]   Wasn't that cold?
[01:33:20.200 --> 01:33:21.200]   Wasn't that cold?
[01:33:21.200 --> 01:33:22.200]   Wasn't that cold?
[01:33:22.200 --> 01:33:23.200]   Wasn't that cold?
[01:33:23.200 --> 01:33:24.200]   Wasn't that cold?
[01:33:24.200 --> 01:33:25.200]   Wasn't that cold?
[01:33:25.200 --> 01:33:26.200]   Wasn't that cold?
[01:33:26.200 --> 01:33:27.200]   Wasn't that cold?
[01:33:27.200 --> 01:33:28.200]   Wasn't that cold?
[01:33:28.200 --> 01:33:29.200]   Wasn't that cold?
[01:33:29.200 --> 01:33:30.200]   Wasn't that cold?
[01:33:30.200 --> 01:33:31.200]   Wasn't that cold?
[01:33:31.200 --> 01:33:32.200]   Wasn't that cold?
[01:33:32.200 --> 01:33:34.200]   Wasn't that cold?
[01:33:34.200 --> 01:33:35.200]   Wasn't that cold?
[01:33:35.200 --> 01:33:36.200]   Wasn't that cold?
[01:33:36.200 --> 01:33:37.200]   Wasn't that cold?
[01:33:37.200 --> 01:33:38.200]   Wasn't that cold?
[01:33:38.200 --> 01:33:39.200]   Wasn't that cold?
[01:33:39.200 --> 01:33:40.200]   Wasn't that cold?
[01:33:40.200 --> 01:33:41.200]   Wasn't that cold?
[01:33:41.200 --> 01:33:42.200]   Wasn't that cold?
[01:33:42.200 --> 01:33:43.200]   Wasn't that cold?
[01:33:43.200 --> 01:33:44.200]   Wasn't that cold?
[01:33:44.200 --> 01:33:45.200]   Wasn't that cold?
[01:33:45.200 --> 01:33:46.200]   Wasn't that cold?
[01:33:46.200 --> 01:33:47.200]   Wasn't that cold?
[01:33:47.200 --> 01:33:48.200]   Wasn't that cold?
[01:33:48.200 --> 01:33:49.200]   Wasn't that cold?
[01:33:49.200 --> 01:33:50.200]   Wasn't that cold?
[01:33:50.200 --> 01:33:51.200]   Wasn't that cold?
[01:33:51.200 --> 01:33:52.200]   Wasn't that cold?
[01:33:52.200 --> 01:33:53.200]   Wasn't that cold?
[01:33:53.200 --> 01:33:54.200]   Wasn't that cold?
[01:33:54.200 --> 01:33:55.200]   Wasn't that cold?
[01:33:55.200 --> 01:33:56.200]   Wasn't that cold?
[01:33:56.200 --> 01:33:57.200]   Wasn't that cold?
[01:33:57.200 --> 01:33:58.200]   Wasn't that cold?
[01:33:58.200 --> 01:33:59.200]   Wasn't that cold?
[01:33:59.200 --> 01:34:00.200]   Wasn't that cold?
[01:34:00.200 --> 01:34:02.200]   Wasn't that cold?
[01:34:02.200 --> 01:34:03.200]   Wasn't that cold?
[01:34:03.200 --> 01:34:04.200]   Wasn't that cold?
[01:34:04.200 --> 01:34:05.200]   Wasn't that cold?
[01:34:05.200 --> 01:34:06.200]   Wasn't that cold?
[01:34:06.200 --> 01:34:07.200]   Wasn't that cold?
[01:34:07.200 --> 01:34:08.200]   Wasn't that cold?
[01:34:08.200 --> 01:34:09.200]   Wasn't that cold?
[01:34:09.200 --> 01:34:10.200]   Wasn't that cold?
[01:34:10.200 --> 01:34:11.200]   Wasn't that cold?
[01:34:11.200 --> 01:34:12.200]   Wasn't that cold?
[01:34:12.200 --> 01:34:13.200]   Wasn't that cold?
[01:34:13.200 --> 01:34:14.200]   Wasn't that cold?
[01:34:14.200 --> 01:34:15.200]   Wasn't that cold?
[01:34:15.200 --> 01:34:16.200]   Wasn't that cold?
[01:34:16.200 --> 01:34:17.200]   Wasn't that cold?
[01:34:17.200 --> 01:34:18.200]   Wasn't that cold?
[01:34:18.200 --> 01:34:19.200]   Wasn't that cold?
[01:34:19.200 --> 01:34:20.200]   Wasn't that cold?
[01:34:20.200 --> 01:34:21.200]   Wasn't that cold?
[01:34:21.200 --> 01:34:22.200]   Wasn't that cold?
[01:34:22.200 --> 01:34:23.200]   Wasn't that cold?
[01:34:23.200 --> 01:34:24.200]   Wasn't that cold?
[01:34:24.200 --> 01:34:25.200]   Wasn't that cold?
[01:34:25.200 --> 01:34:26.200]   Wasn't that cold?
[01:34:26.200 --> 01:34:27.200]   Wasn't that cold?
[01:34:27.200 --> 01:34:28.200]   Wasn't that cold?
[01:34:28.200 --> 01:34:30.200]   Wasn't that cold?
[01:34:30.200 --> 01:34:31.200]   Wasn't that cold?
[01:34:31.200 --> 01:34:32.200]   Wasn't that cold?
[01:34:32.200 --> 01:34:33.200]   Wasn't that cold?
[01:34:33.200 --> 01:34:34.200]   Wasn't that cold?
[01:34:34.200 --> 01:34:35.200]   Wasn't that cold?
[01:34:35.200 --> 01:34:36.200]   Wasn't that cold?
[01:34:36.200 --> 01:34:37.200]   Wasn't that cold?
[01:34:37.200 --> 01:34:38.200]   Wasn't that cold?
[01:34:38.200 --> 01:34:39.200]   Wasn't that cold?
[01:34:39.200 --> 01:34:40.200]   Wasn't that cold?
[01:34:40.200 --> 01:34:41.200]   Wasn't that cold?
[01:34:41.200 --> 01:34:42.200]   Wasn't that cold?
[01:34:42.200 --> 01:34:43.200]   Wasn't that cold?
[01:34:43.200 --> 01:34:44.200]   Wasn't that cold?
[01:34:44.200 --> 01:34:45.200]   Wasn't that cold?
[01:34:45.200 --> 01:34:46.200]   Wasn't that cold?
[01:34:46.200 --> 01:34:47.200]   Wasn't that cold?
[01:34:47.200 --> 01:34:48.200]   Wasn't that cold?
[01:34:48.200 --> 01:34:49.200]   Wasn't that cold?
[01:34:49.200 --> 01:34:50.200]   Wasn't that cold?
[01:34:50.200 --> 01:34:51.200]   Wasn't that cold?
[01:34:51.200 --> 01:34:52.200]   Wasn't that cold?
[01:34:52.200 --> 01:34:53.200]   Wasn't that cold?
[01:34:53.200 --> 01:34:54.200]   Wasn't that cold?
[01:34:54.200 --> 01:34:55.200]   Wasn't that cold?
[01:34:55.200 --> 01:34:56.200]   Wasn't that cold?
[01:34:56.200 --> 01:34:58.200]   Wasn't that cold?
[01:34:58.200 --> 01:34:59.200]   Wasn't that cold?
[01:34:59.200 --> 01:35:00.200]   Wasn't that cold?
[01:35:00.200 --> 01:35:01.200]   Wasn't that cold?
[01:35:01.200 --> 01:35:02.200]   Wasn't that cold?
[01:35:02.200 --> 01:35:03.200]   Wasn't that cold?
[01:35:03.200 --> 01:35:04.200]   Wasn't that cold?
[01:35:04.200 --> 01:35:05.200]   Wasn't that cold?
[01:35:05.200 --> 01:35:06.200]   Wasn't that cold?
[01:35:06.200 --> 01:35:07.200]   Wasn't that cold?
[01:35:07.200 --> 01:35:08.200]   Wasn't that cold?
[01:35:08.200 --> 01:35:09.200]   Wasn't that cold?
[01:35:09.200 --> 01:35:10.200]   Wasn't that cold?
[01:35:10.200 --> 01:35:11.200]   Wasn't that cold?
[01:35:11.200 --> 01:35:12.200]   Wasn't that cold?
[01:35:12.200 --> 01:35:13.200]   Wasn't that cold?
[01:35:13.200 --> 01:35:14.200]   Wasn't that cold?
[01:35:14.200 --> 01:35:15.200]   Wasn't that cold?
[01:35:15.200 --> 01:35:16.200]   Wasn't that cold?
[01:35:16.200 --> 01:35:17.200]   Wasn't that cold?
[01:35:17.200 --> 01:35:18.200]   Wasn't that cold?
[01:35:18.200 --> 01:35:19.200]   Wasn't that cold?
[01:35:19.200 --> 01:35:20.200]   Wasn't that cold?
[01:35:20.200 --> 01:35:21.200]   Wasn't that cold?
[01:35:21.200 --> 01:35:22.200]   Wasn't that cold?
[01:35:22.200 --> 01:35:23.200]   Wasn't that cold?
[01:35:23.200 --> 01:35:24.200]   Wasn't that cold?
[01:35:24.200 --> 01:35:26.200]   Was that your Oprah moment?
[01:35:26.200 --> 01:35:29.200]   And you get a helicopter and you get a helicopter?
[01:35:29.200 --> 01:35:30.200]   Well, I'm kind of glad.
[01:35:30.200 --> 01:35:33.200]   So I didn't, he got, he said, "Dad, did you mean to get me too?"
[01:35:33.200 --> 01:35:34.200]   I said, "You got two?"
[01:35:34.200 --> 01:35:36.200]   And he said, "Yeah."
[01:35:36.200 --> 01:35:38.200]   I said, "Well, I could set it back."
[01:35:38.200 --> 01:35:40.200]   I said, "No, you might as well keep it."
[01:35:40.200 --> 01:35:41.200]   And you know what?
[01:35:41.200 --> 01:35:48.200]   As a father, I'm paying a lot of money for my son to go to college.
[01:35:48.200 --> 01:35:52.200]   And I know he's studying hard, but really my proudest moment as a father
[01:35:52.200 --> 01:35:57.200]   is when he got on Tosh.0 in the hoverboard that I gave him.
[01:35:57.200 --> 01:35:58.200]   Let's see.
[01:35:58.200 --> 01:35:59.200]   Oh.
[01:35:59.200 --> 01:36:02.200]   You know the show on Comedy Central?
[01:36:02.200 --> 01:36:03.200]   Oh.
[01:36:03.200 --> 01:36:05.200]   Here he is on his hoverboard.
[01:36:05.200 --> 01:36:06.200]   Oh.
[01:36:06.200 --> 01:36:08.200]   I'm so glad I got him too.
[01:36:08.200 --> 01:36:09.200]   Wait.
[01:36:09.200 --> 01:36:10.200]   I'm so glad.
[01:36:10.200 --> 01:36:11.200]   That is a new stuff.
[01:36:11.200 --> 01:36:12.200]   That's a hoverboard justice.
[01:36:12.200 --> 01:36:13.200]   I've wanted to.
[01:36:13.200 --> 01:36:14.200]   It's called Hoverboard Jousting.
[01:36:14.200 --> 01:36:15.200]   He invented a new sport.
[01:36:15.200 --> 01:36:16.200]   Yeah.
[01:36:16.200 --> 01:36:19.200]   Okay, so my question is, that kid, who was the, who fell?
[01:36:19.200 --> 01:36:20.200]   His friend.
[01:36:20.200 --> 01:36:21.200]   Okay.
[01:36:21.200 --> 01:36:26.200]   So what Henry did that was genius is the other kid dropped his shoulder.
[01:36:26.200 --> 01:36:27.200]   Neither one is Henry.
[01:36:27.200 --> 01:36:29.200]   Henry's the cameraman.
[01:36:29.200 --> 01:36:30.200]   Oh.
[01:36:30.200 --> 01:36:31.200]   Okay, then Henry's really genius.
[01:36:31.200 --> 01:36:32.200]   He's really genius.
[01:36:32.200 --> 01:36:35.200]   He talked to other people into doing it.
[01:36:35.200 --> 01:36:37.200]   His friends are basically drunks.
[01:36:37.200 --> 01:36:40.200]   That's when you don't get hurt doing things like college.
[01:36:40.200 --> 01:36:41.200]   He just said he was in college.
[01:36:41.200 --> 01:36:42.200]   He's a frat brother.
[01:36:42.200 --> 01:36:43.200]   He's in a frat.
[01:36:43.200 --> 01:36:44.200]   These are frat brothers.
[01:36:44.200 --> 01:36:45.200]   He was sober and did that.
[01:36:45.200 --> 01:36:46.200]   You'd go down.
[01:36:46.200 --> 01:36:47.200]   You'd be in the ER.
[01:36:47.200 --> 01:36:48.200]   Speaking of it.
[01:36:48.200 --> 01:36:49.200]   Oh.
[01:36:49.200 --> 01:36:52.200]   Your ER doc do not sign up for the day after Christmas.
[01:36:52.200 --> 01:36:53.200]   Is it bad?
[01:36:53.200 --> 01:36:54.200]   Hoverboard.
[01:36:54.200 --> 01:36:55.200]   Oh, it's going to be bad.
[01:36:55.200 --> 01:36:58.200]   Oh, it's going to be broken wrist, broken shoulder is going to be bad.
[01:36:58.200 --> 01:36:59.200]   Don't tell me.
[01:36:59.200 --> 01:37:04.200]   We should, we should, of course, point out his creamy corn cob in the chat says these are
[01:37:04.200 --> 01:37:06.200]   not, these four boards do not actually hover.
[01:37:06.200 --> 01:37:07.200]   Well, what was your name for them?
[01:37:07.200 --> 01:37:09.200]   You had like some other name.
[01:37:09.200 --> 01:37:12.200]   Cyberboards or what do you call them, Tom?
[01:37:12.200 --> 01:37:13.200]   I, I call them hoverboards.
[01:37:13.200 --> 01:37:16.200]   That's what everybody knows what I'm talking about.
[01:37:16.200 --> 01:37:19.200]   He suggested a rover board.
[01:37:19.200 --> 01:37:20.200]   Rover board.
[01:37:20.200 --> 01:37:21.200]   Or rubber board.
[01:37:21.200 --> 01:37:23.200]   Propelled handleless segues.
[01:37:23.200 --> 01:37:26.200]   They like segues without the handle.
[01:37:26.200 --> 01:37:27.200]   Yeah.
[01:37:27.200 --> 01:37:28.200]   That was boards.
[01:37:28.200 --> 01:37:29.200]   That was boards.
[01:37:29.200 --> 01:37:30.200]   Shrips off the tongue.
[01:37:30.200 --> 01:37:31.200]   I like it.
[01:37:31.200 --> 01:37:33.200]   I think that's, that's a really important distinction, Tom.
[01:37:33.200 --> 01:37:34.200]   I appreciate your.
[01:37:34.200 --> 01:37:36.200]   Anybody do you're giving hoverboards for Christmas?
[01:37:36.200 --> 01:37:37.200]   Um, no.
[01:37:37.200 --> 01:37:38.200]   No.
[01:37:38.200 --> 01:37:39.200]   I'm not getting a rubber board.
[01:37:39.200 --> 01:37:40.200]   No.
[01:37:40.200 --> 01:37:42.200]   I say that was, I was, I haven't done it with some Marin moms.
[01:37:42.200 --> 01:37:45.200]   And for those of you who aren't from the barrier, Marin is like, fufuru.
[01:37:45.200 --> 01:37:46.200]   Like, yeah.
[01:37:46.200 --> 01:37:50.200]   And the Marin moms like, guys, are we getting the eight year olds?
[01:37:50.200 --> 01:37:53.200]   All the $900 hoverboards?
[01:37:53.200 --> 01:37:54.200]   And we all doing it.
[01:37:54.200 --> 01:37:56.200]   And I just, I lost my appetite.
[01:37:56.200 --> 01:37:57.200]   How old is your son now?
[01:37:57.200 --> 01:37:58.200]   Four and a half.
[01:37:58.200 --> 01:38:00.200]   Oh, he's too young for Christmas.
[01:38:00.200 --> 01:38:01.200]   Minecraft.
[01:38:01.200 --> 01:38:02.200]   Yeah.
[01:38:02.200 --> 01:38:03.200]   Yeah.
[01:38:03.200 --> 01:38:04.200]   Yeah.
[01:38:04.200 --> 01:38:05.200]   Yeah.
[01:38:05.200 --> 01:38:06.200]   Nice.
[01:38:06.200 --> 01:38:07.200]   Make your stuff.
[01:38:07.200 --> 01:38:08.200]   Make your stuff.
[01:38:08.200 --> 01:38:10.200]   What are Finn and Amaya?
[01:38:10.200 --> 01:38:14.200]   They are getting, um, Finn really wants a drone.
[01:38:14.200 --> 01:38:18.200]   And I've been doing all these drone stories and, uh, I lost a drone this week.
[01:38:18.200 --> 01:38:21.200]   I lost a drone every time I flew one.
[01:38:21.200 --> 01:38:22.200]   I'm telling you.
[01:38:22.200 --> 01:38:23.200]   They're not easy.
[01:38:23.200 --> 01:38:25.200]   Have you been to fries this week?
[01:38:25.200 --> 01:38:31.200]   Hey, there are, there's 50 yards of generic drones for sale right now.
[01:38:31.200 --> 01:38:33.200]   There's going to be a million drones sold this holiday.
[01:38:33.200 --> 01:38:34.200]   SEMA.
[01:38:34.200 --> 01:38:35.200]   S-Y-M-A.
[01:38:35.200 --> 01:38:36.200]   It's 40 bucks.
[01:38:36.200 --> 01:38:38.200]   It's the one I lost.
[01:38:38.200 --> 01:38:39.200]   Oh.
[01:38:39.200 --> 01:38:41.200]   Well, I, so, I mean, that's the thing.
[01:38:41.200 --> 01:38:43.200]   It's like, what's a good, what's a good drone to get into?
[01:38:43.200 --> 01:38:45.200]   An entry-level drone that's actually controlled.
[01:38:45.200 --> 01:38:46.200]   SEMA?
[01:38:46.200 --> 01:38:49.200]   This is what our drone father tells us.
[01:38:49.200 --> 01:38:52.200]   Uh, Father Robert, uh, Ballastair, this SEMA.
[01:38:52.200 --> 01:38:53.200]   Don't get the one with the camera.
[01:38:53.200 --> 01:38:54.200]   Oh, okay.
[01:38:54.200 --> 01:38:55.200]   SEMA.
[01:38:55.200 --> 01:38:56.200]   'Cause it's cheap.
[01:38:56.200 --> 01:38:57.200]   It's good for, uh, an eight-year-old.
[01:38:57.200 --> 01:38:59.200]   You know, it's not, you know, it's okay if they lose it.
[01:38:59.200 --> 01:39:00.200]   Nice.
[01:39:00.200 --> 01:39:01.200]   Okay.
[01:39:01.200 --> 01:39:03.200]   And it's small enough that if it hits somebody in the head, it's really not going to hurt anybody.
[01:39:03.200 --> 01:39:06.200]   Well, I lost mine to be, to be completely, um, clear.
[01:39:06.200 --> 01:39:10.200]   I lost my, we flew it over trees and I lost it in the trees.
[01:39:10.200 --> 01:39:12.200]   I didn't want to be an irresponsible drone person.
[01:39:12.200 --> 01:39:15.200]   And what is incredible to me is how difficult they are to control right out of the gates.
[01:39:15.200 --> 01:39:21.200]   And I didn't realize when you get a drone, you should fly it inside for the first five times.
[01:39:21.200 --> 01:39:22.200]   I had no idea.
[01:39:22.200 --> 01:39:26.200]   And apparently the FAA now has a website called Before You Fly.
[01:39:26.200 --> 01:39:27.200]   Yeah.
[01:39:27.200 --> 01:39:28.200]   That's for a drone.
[01:39:28.200 --> 01:39:29.200]   I should have read that.
[01:39:29.200 --> 01:39:31.200]   Because idiots like me are losing drones.
[01:39:31.200 --> 01:39:32.200]   So I go out on the back deck.
[01:39:32.200 --> 01:39:33.200]   I press one button.
[01:39:33.200 --> 01:39:36.200]   The drone starts going up and it keeps going up.
[01:39:36.200 --> 01:39:39.200]   Like a little balloon until I lose sight of it.
[01:39:39.200 --> 01:39:43.200]   And that's the last time I saw it.
[01:39:43.200 --> 01:39:44.200]   That happened to me with this.
[01:39:44.200 --> 01:39:46.200]   Does anybody want to buy a drone remote control?
[01:39:46.200 --> 01:39:47.200]   Because I got extras.
[01:39:47.200 --> 01:39:49.200]   I got one of those for those inflatable sharks.
[01:39:49.200 --> 01:39:50.200]   Remember these sharks?
[01:39:50.200 --> 01:39:51.200]   I lost mine.
[01:39:51.200 --> 01:39:52.200]   Shark, we could twit.
[01:39:52.200 --> 01:39:53.200]   It was great.
[01:39:53.200 --> 01:39:54.200]   Yeah.
[01:39:54.200 --> 01:39:55.200]   Tom, are you into the drone?
[01:39:55.200 --> 01:39:57.200]   You know, I don't play around with drones much.
[01:39:57.200 --> 01:40:02.200]   I've worked with them a couple of times on shoots because they're great for getting, essentially,
[01:40:02.200 --> 01:40:05.200]   what would have been a helicopter shot back in the day.
[01:40:05.200 --> 01:40:06.200]   Right.
[01:40:06.200 --> 01:40:08.200]   DJI has an entry level drone that's in the 500-600.
[01:40:08.200 --> 01:40:10.200]   Don't get your son that.
[01:40:10.200 --> 01:40:11.200]   I'm not trust me.
[01:40:11.200 --> 01:40:12.200]   I have the bebop.
[01:40:12.200 --> 01:40:13.200]   Is that what you're talking about?
[01:40:13.200 --> 01:40:14.200]   The parrot?
[01:40:14.200 --> 01:40:15.200]   No, the parrot was amazing.
[01:40:15.200 --> 01:40:17.200]   It was $394 on Black Friday, the bebop.
[01:40:17.200 --> 01:40:19.200]   But it's more like $500 now.
[01:40:19.200 --> 01:40:20.200]   I have that one.
[01:40:20.200 --> 01:40:22.200]   And I actually like it except I accidentally...
[01:40:22.200 --> 01:40:24.200]   Again, this time it didn't go straight up.
[01:40:24.200 --> 01:40:26.200]   I flew it into the side of a hill the first time.
[01:40:26.200 --> 01:40:27.200]   And it somehow broke its wing.
[01:40:27.200 --> 01:40:30.200]   And now if I fly it, it just goes like that every time it doesn't.
[01:40:30.200 --> 01:40:34.200]   I have to bring it in for repairs to the drone father because it won't go anywhere.
[01:40:34.200 --> 01:40:37.200]   The biggest difference between the bebop and the DJI is the bebop is all...
[01:40:37.200 --> 01:40:43.200]   iPhone control or device control whereas the DJI's are sticks.
[01:40:43.200 --> 01:40:44.200]   Right.
[01:40:44.200 --> 01:40:47.200]   And it has an iPhone interface where you can see the camera.
[01:40:47.200 --> 01:40:50.200]   So you can clip the iPhone in and see the camera and still control the device.
[01:40:50.200 --> 01:40:52.200]   I like having the iPhone interface.
[01:40:52.200 --> 01:40:53.200]   That's the difference.
[01:40:53.200 --> 01:40:54.200]   It's a little smarter.
[01:40:54.200 --> 01:40:55.200]   This is cool.
[01:40:55.200 --> 01:40:56.200]   This is the parrot.
[01:40:56.200 --> 01:40:57.200]   They call it a drone.
[01:40:57.200 --> 01:40:58.200]   I think this is good for an eight-year-old.
[01:40:58.200 --> 01:40:59.200]   It's not.
[01:40:59.200 --> 01:41:00.200]   It's a rolling vehicle.
[01:41:00.200 --> 01:41:01.200]   It seems awesome.
[01:41:01.200 --> 01:41:02.200]   And it jumps.
[01:41:02.200 --> 01:41:03.200]   Have you seen it jump?
[01:41:03.200 --> 01:41:04.200]   Yeah.
[01:41:04.200 --> 01:41:05.200]   It is so cool.
[01:41:05.200 --> 01:41:09.200]   We sent it back because we showed it on iOS today this earlier this week.
[01:41:09.200 --> 01:41:10.200]   It jumps.
[01:41:10.200 --> 01:41:12.200]   You press a button that goes like four feet high.
[01:41:12.200 --> 01:41:13.200]   Boom.
[01:41:13.200 --> 01:41:14.200]   It's awesome.
[01:41:14.200 --> 01:41:15.200]   No, that thing's cool.
[01:41:15.200 --> 01:41:16.200]   The BB-8 is cool.
[01:41:16.200 --> 01:41:17.200]   There's a lot of D-R's.
[01:41:17.200 --> 01:41:18.200]   This does not fly.
[01:41:18.200 --> 01:41:19.200]   I should be clear.
[01:41:19.200 --> 01:41:20.200]   It shouldn't be called a drone.
[01:41:20.200 --> 01:41:21.200]   It's just...
[01:41:21.200 --> 01:41:22.200]   And you have a BB-8.
[01:41:22.200 --> 01:41:23.200]   What do you think, Tom?
[01:41:23.200 --> 01:41:25.200]   It scares the crap out of my dog.
[01:41:25.200 --> 01:41:26.200]   Django doesn't like it.
[01:41:26.200 --> 01:41:28.200]   No, Django's fine with it.
[01:41:28.200 --> 01:41:29.200]   It's Sawyer.
[01:41:29.200 --> 01:41:30.200]   It's the one who gets scared.
[01:41:30.200 --> 01:41:31.200]   Aww.
[01:41:31.200 --> 01:41:32.200]   But yeah, they're great.
[01:41:32.200 --> 01:41:33.200]   They're fantastic.
[01:41:33.200 --> 01:41:37.200]   This is going to be a drone hoverboard Christmas.
[01:41:37.200 --> 01:41:38.200]   Yeah.
[01:41:38.200 --> 01:41:43.200]   Here's the brands that Eric Chang, my drone guy recommends Cheerson and the Blade Nano.
[01:41:43.200 --> 01:41:44.200]   They're both...
[01:41:44.200 --> 01:41:45.200]   Cheerson's like 29 bucks.
[01:41:45.200 --> 01:41:46.200]   It's like a mini quadcopter.
[01:41:46.200 --> 01:41:47.200]   Uh-huh.
[01:41:47.200 --> 01:41:48.200]   That's a lot of fun.
[01:41:48.200 --> 01:41:49.200]   And it flies with sticks.
[01:41:49.200 --> 01:41:54.200]   So basically you get used to the interface and you have very little to lose.
[01:41:54.200 --> 01:41:56.200]   And then the Blade Nano is also the same.
[01:41:56.200 --> 01:41:57.200]   It's $90.
[01:41:57.200 --> 01:41:58.200]   That's a little pricey.
[01:41:58.200 --> 01:41:59.200]   It is.
[01:41:59.200 --> 01:42:00.200]   It is.
[01:42:00.200 --> 01:42:01.200]   That one might have a camera.
[01:42:01.200 --> 01:42:02.200]   One of them doesn't answer a little bit cheaper.
[01:42:02.200 --> 01:42:03.200]   It's going to save your money.
[01:42:03.200 --> 01:42:04.200]   You don't need a camera.
[01:42:04.200 --> 01:42:05.200]   You're going to wreck it is the bottom line.
[01:42:05.200 --> 01:42:07.200]   So get one that you can fly it indoors.
[01:42:07.200 --> 01:42:08.200]   I had no idea.
[01:42:08.200 --> 01:42:10.200]   Cheerson's the H E R.
[01:42:10.200 --> 01:42:11.200]   Yeah.
[01:42:11.200 --> 01:42:12.200]   And it's teeny tiny.
[01:42:12.200 --> 01:42:13.200]   I have a little one.
[01:42:13.200 --> 01:42:14.200]   Oh, it's cute.
[01:42:14.200 --> 01:42:15.200]   And that's cheap.
[01:42:15.200 --> 01:42:16.200]   That's 15 bucks.
[01:42:16.200 --> 01:42:17.200]   Yeah.
[01:42:17.200 --> 01:42:18.200]   Oh.
[01:42:18.200 --> 01:42:19.200]   I have one of those in my purse.
[01:42:19.200 --> 01:42:20.200]   I'll go grab it.
[01:42:20.200 --> 01:42:21.200]   Do you really?
[01:42:21.200 --> 01:42:22.200]   Yeah.
[01:42:22.200 --> 01:42:23.200]   You just have it in your purse.
[01:42:23.200 --> 01:42:24.200]   Is that instead of me?
[01:42:24.200 --> 01:42:25.200]   I'm going to say with it.
[01:42:25.200 --> 01:42:26.200]   When I hear it.
[01:42:26.200 --> 01:42:27.200]   If somebody attacks you, you just go watch out.
[01:42:27.200 --> 01:42:28.200]   I got a drone.
[01:42:28.200 --> 01:42:29.200]   The sound is scary.
[01:42:29.200 --> 01:42:30.200]   It's...
[01:42:30.200 --> 01:42:31.200]   It's scary.
[01:42:31.200 --> 01:42:40.380]   So how is the FAA and the government going to regulate all the licensing and the individuals
[01:42:40.380 --> 01:42:41.520]   who are buying drones?
[01:42:41.520 --> 01:42:45.360]   Isn't that the state of policy at this point in time?
[01:42:45.360 --> 01:42:49.200]   Currently, I believe the FAA says you have to have a license for commercial flight, but
[01:42:49.200 --> 01:42:51.200]   you do not as a hobbyist.
[01:42:51.200 --> 01:42:54.760]   No, you don't have to have a license.
[01:42:54.760 --> 01:42:55.760]   You don't have to have a license.
[01:42:55.760 --> 01:42:56.760]   You have to register in some way.
[01:42:56.760 --> 01:42:58.760]   You have to register somewhere, right?
[01:42:58.760 --> 01:42:59.760]   Not yet.
[01:42:59.760 --> 01:43:05.800]   With this field just exploding and so many available on the market, are they just going
[01:43:05.800 --> 01:43:10.280]   to make it easy where you buy it and it just registers you immediately as you buy it or
[01:43:10.280 --> 01:43:12.480]   do you have to go register it?
[01:43:12.480 --> 01:43:13.480]   Here's the problem.
[01:43:13.480 --> 01:43:16.160]   The FAA doesn't have a police force.
[01:43:16.160 --> 01:43:17.160]   I know.
[01:43:17.160 --> 01:43:18.160]   It has no enforcement.
[01:43:18.160 --> 01:43:20.160]   What the what?
[01:43:20.160 --> 01:43:21.160]   That's cute.
[01:43:21.160 --> 01:43:22.160]   It is, right?
[01:43:22.160 --> 01:43:25.680]   Like put it on the close up cam.
[01:43:25.680 --> 01:43:26.680]   So before I wreck it.
[01:43:26.680 --> 01:43:27.680]   That is so cute.
[01:43:27.680 --> 01:43:28.680]   I know, right?
[01:43:28.680 --> 01:43:29.680]   I know, right?
[01:43:29.680 --> 01:43:30.680]   It is tiny.
[01:43:30.680 --> 01:43:31.680]   It is tiny.
[01:43:31.680 --> 01:43:32.680]   Go ahead, take off.
[01:43:32.680 --> 01:43:33.680]   I will try.
[01:43:33.680 --> 01:43:38.200]   Oh, you probably keep it in your purse right next to your toothpicks.
[01:43:38.200 --> 01:43:39.200]   I am terrible.
[01:43:39.200 --> 01:43:40.200]   I am so cute.
[01:43:40.200 --> 01:43:41.200]   But that is the thing is, look at the control.
[01:43:41.200 --> 01:43:44.200]   I think that is pronounced tampax, but you are not too cute.
[01:43:44.200 --> 01:43:46.360]   I think those are teeny tiny.
[01:43:46.360 --> 01:43:47.360]   Oh, it is so cute.
[01:43:47.360 --> 01:43:48.360]   I know that is 29 bucks.
[01:43:48.360 --> 01:43:49.840]   And how do I lose it?
[01:43:49.840 --> 01:43:50.840]   I press this button.
[01:43:50.840 --> 01:43:52.840]   The controller is bigger than this.
[01:43:52.840 --> 01:43:53.840]   There you go.
[01:43:53.840 --> 01:43:54.840]   Oh, it is going to go on your line.
[01:43:54.840 --> 01:43:55.840]   There it goes.
[01:43:55.840 --> 01:43:56.840]   There you go.
[01:43:56.840 --> 01:43:59.640]   It is a little sensitive.
[01:43:59.640 --> 01:44:06.520]   Okay, for those of you listening at home, it is about the size of, I don't know, a silver
[01:44:06.520 --> 01:44:07.520]   dollar.
[01:44:07.520 --> 01:44:08.520]   It is not very big.
[01:44:08.520 --> 01:44:09.520]   Oh, it is teeny tiny.
[01:44:09.520 --> 01:44:10.520]   But it is a quadcopter.
[01:44:10.520 --> 01:44:11.880]   It has built in bumpers.
[01:44:11.880 --> 01:44:12.880]   It lights up.
[01:44:12.880 --> 01:44:13.880]   LED lights all around.
[01:44:13.880 --> 01:44:15.880]   20 minute fly time battery.
[01:44:15.880 --> 01:44:16.880]   Really?
[01:44:16.880 --> 01:44:18.920]   That is better than a lot of bigger drones.
[01:44:18.920 --> 01:44:19.920]   No.
[01:44:19.920 --> 01:44:20.920]   The important thing is the control.
[01:44:20.920 --> 01:44:25.840]   So the left is up down, left right, and the right is the turning.
[01:44:25.840 --> 01:44:27.920]   Yeah, it is like 360 in space.
[01:44:27.920 --> 01:44:29.440]   Let's try it a little slower.
[01:44:29.440 --> 01:44:34.720]   Yeah, but this is the important thing is you have to start cheap and slow.
[01:44:34.720 --> 01:44:39.480]   The woman at Fry said, are you sure you want to buy that?
[01:44:39.480 --> 01:44:41.480]   She said, are you sure you want to buy that?
[01:44:41.480 --> 01:44:42.480]   She said they have.
[01:44:42.480 --> 01:44:43.480]   They come in.
[01:44:43.480 --> 01:44:45.480]   That is a good salesman ship.
[01:44:45.480 --> 01:44:48.480]   She also works in the customer returns department.
[01:44:48.480 --> 01:44:51.000]   Oh, she sees them coming back all the time.
[01:44:51.000 --> 01:44:55.400]   Now what is interesting is I called Walmart and Fry's neither one of them who are pushing
[01:44:55.400 --> 01:44:57.800]   constantly that extended warranty to you.
[01:44:57.800 --> 01:44:58.800]   Guess what?
[01:44:58.800 --> 01:45:00.600]   These are ineligible for the extended warranty.
[01:45:00.600 --> 01:45:01.600]   Of course not.
[01:45:01.600 --> 01:45:04.280]   Oh, good Leo, you are doing great.
[01:45:04.280 --> 01:45:06.120]   You are really good at this.
[01:45:06.120 --> 01:45:10.120]   Any of that size of drone will not be required to be registered.
[01:45:10.120 --> 01:45:12.720]   There is a weight limit under which you can just fly.
[01:45:12.720 --> 01:45:13.720]   Oh, there is.
[01:45:13.720 --> 01:45:14.720]   It seems to be losing altitude no matter what I do.
[01:45:14.720 --> 01:45:15.720]   I think the battery died.
[01:45:15.720 --> 01:45:16.720]   Well, you could buy that.
[01:45:16.720 --> 01:45:17.720]   But that was fun.
[01:45:17.720 --> 01:45:18.720]   I know.
[01:45:18.720 --> 01:45:20.520]   For 29 bucks, you know I always do props.
[01:45:20.520 --> 01:45:22.480]   I am like the Gallagher of Twit.
[01:45:22.480 --> 01:45:24.120]   What is that called again?
[01:45:24.120 --> 01:45:25.120]   Prop comedy.
[01:45:25.120 --> 01:45:26.120]   That is a...
[01:45:26.120 --> 01:45:27.120]   Oh, I am sorry.
[01:45:27.120 --> 01:45:28.120]   You weren't...
[01:45:28.120 --> 01:45:29.120]   I mean the drone.
[01:45:29.120 --> 01:45:30.120]   Yeah.
[01:45:30.120 --> 01:45:31.120]   It is the Quattro radical.
[01:45:31.120 --> 01:45:32.120]   It is called.
[01:45:32.120 --> 01:45:33.120]   Radical.
[01:45:33.120 --> 01:45:34.120]   You don't have to register them.
[01:45:34.120 --> 01:45:36.240]   You have to keep them under 400 feet elevation.
[01:45:36.240 --> 01:45:38.280]   You have to have them in line of sight at all times.
[01:45:38.280 --> 01:45:40.040]   Why do you have this in your purse?
[01:45:40.040 --> 01:45:41.040]   The current recommendation.
[01:45:41.040 --> 01:45:42.040]   I also have a one-seat.
[01:45:42.040 --> 01:45:43.040]   The current recommendation.
[01:45:43.040 --> 01:45:45.800]   But that is for later.
[01:45:45.800 --> 01:45:50.320]   The reason people are talking about registration is the FAA is currently considering it.
[01:45:50.320 --> 01:45:54.800]   So that is not a proposal that would say anything above a certain weight limit you would
[01:45:54.800 --> 01:45:57.720]   have to register yourself as a drone pilot.
[01:45:57.720 --> 01:46:00.720]   And then that would apply to all your drones and you would stick a sticker or you would
[01:46:00.720 --> 01:46:02.640]   stick a barcode on there to say yes.
[01:46:02.640 --> 01:46:05.760]   Because what they are after is not keeping track of everybody who flies drones.
[01:46:05.760 --> 01:46:09.480]   They are after educating people into where they can fly them and what they should be
[01:46:09.480 --> 01:46:10.480]   able to do with them.
[01:46:10.480 --> 01:46:15.720]   We were in the airport the other day and when you are doing the security line along with
[01:46:15.720 --> 01:46:16.720]   the...
[01:46:16.720 --> 01:46:19.800]   Don't take your laptop out and stuff.
[01:46:19.800 --> 01:46:26.680]   It had four screens on drones from the FAA including if you see a drone flying into airspace
[01:46:26.680 --> 01:46:29.720]   at this airport, please notify the FAA immediately.
[01:46:29.720 --> 01:46:32.560]   Here is the FAA website.
[01:46:32.560 --> 01:46:34.800]   No before you fly.
[01:46:34.800 --> 01:46:36.880]   Yeah, no before you fly.
[01:46:36.880 --> 01:46:37.880]   That is what they say.
[01:46:37.880 --> 01:46:38.880]   Fly safe with your drone.
[01:46:38.880 --> 01:46:43.360]   You are headed to the stores on Black Friday to buy that shiny new camera equipped drone.
[01:46:43.360 --> 01:46:47.080]   Do you know you are also going to become a pilot when you fly your drone anywhere in
[01:46:47.080 --> 01:46:48.280]   the nation's airspace?
[01:46:48.280 --> 01:46:51.000]   Do you automatically become part of the US aviation system?
[01:46:51.000 --> 01:46:54.600]   That is why flying inside is not.
[01:46:54.600 --> 01:46:56.760]   Under the law your drone is an aircraft.
[01:46:56.760 --> 01:47:00.760]   While the rules for drones may be different, you have the responsibility to operate safely
[01:47:00.760 --> 01:47:04.120]   just as a Cessna or a 747 pilot does.
[01:47:04.120 --> 01:47:06.480]   They have a checklist.
[01:47:06.480 --> 01:47:08.360]   That is a good response.
[01:47:08.360 --> 01:47:12.360]   What is interesting is that is putting the onus on the pilot.
[01:47:12.360 --> 01:47:18.280]   About three weeks ago the DJI folks announced that they are working with the FAA to create
[01:47:18.280 --> 01:47:19.280]   electronic fences.
[01:47:19.280 --> 01:47:21.080]   That is the solution.
[01:47:21.080 --> 01:47:25.640]   What you do is basically you create a jamming field around airports or any of the known fly
[01:47:25.640 --> 01:47:33.680]   zones and you work with the drone manufacturers to make sure that they have basically geo-located
[01:47:33.680 --> 01:47:37.080]   GPS locations and know where there is no fly zones are.
[01:47:37.080 --> 01:47:40.800]   Unless you are the Tokyo Metropolitan Police in which case you create a squad that has
[01:47:40.800 --> 01:47:42.080]   a drone with a net.
[01:47:42.080 --> 01:47:43.080]   The net drones.
[01:47:43.080 --> 01:47:44.560]   That is so cool.
[01:47:44.560 --> 01:47:46.000]   I think we need more of these.
[01:47:46.000 --> 01:47:50.200]   I either want a drone with a net or I want a shotgun.
[01:47:50.200 --> 01:47:54.440]   I just think I am going to have trained birds.
[01:47:54.440 --> 01:48:02.200]   I am going to get into hawking and use hawks to take down the net.
[01:48:02.200 --> 01:48:06.600]   So it is actually another drone but it has a long net on it and it can't watch.
[01:48:06.600 --> 01:48:08.400]   It is going to scoop up.
[01:48:08.400 --> 01:48:10.160]   It is like hawking for drones.
[01:48:10.160 --> 01:48:12.000]   That is exactly cool.
[01:48:12.000 --> 01:48:15.520]   We are going to have highly impractical by the way.
[01:48:15.520 --> 01:48:19.600]   My speed drone jazes in the sky.
[01:48:19.600 --> 01:48:20.600]   Oh my god.
[01:48:20.600 --> 01:48:22.600]   I think OJ is in that white drone.
[01:48:22.600 --> 01:48:24.080]   I said that is the same.
[01:48:24.080 --> 01:48:26.160]   Oh, that is a terrible idea.
[01:48:26.160 --> 01:48:27.160]   Okay, hypothesis.
[01:48:27.160 --> 01:48:29.360]   I can't see this going wrong in any way.
[01:48:29.360 --> 01:48:30.360]   No.
[01:48:30.360 --> 01:48:34.000]   I am going to throw this hypothesis out and see what you guys think.
[01:48:34.000 --> 01:48:38.640]   I have this over all philosophy about technology that started with the Xerox machine which
[01:48:38.640 --> 01:48:41.400]   is whenever new technology comes out.
[01:48:41.400 --> 01:48:45.000]   We tend to use and abuse it in the most base possible ways.
[01:48:45.000 --> 01:48:49.040]   Ergo photocopying your ass cheeks on the Xerox machine.
[01:48:49.040 --> 01:48:50.480]   We don't do that anymore.
[01:48:50.480 --> 01:48:51.480]   No, you learn.
[01:48:51.480 --> 01:48:52.840]   Yeah, we just get the old.
[01:48:52.840 --> 01:48:53.840]   It gets old.
[01:48:53.840 --> 01:48:54.840]   I might once in a while.
[01:48:54.840 --> 01:48:56.400]   Yeah, we don't.
[01:48:56.400 --> 01:49:01.320]   But like sticking the cell phone cameras under the system.
[01:49:01.320 --> 01:49:03.320]   Yeah, no, we do.
[01:49:03.320 --> 01:49:04.320]   We do.
[01:49:04.320 --> 01:49:05.320]   It has to.
[01:49:05.320 --> 01:49:08.760]   The initial use when laser writers came out, everybody did 100 font documents.
[01:49:08.760 --> 01:49:09.760]   Right.
[01:49:09.760 --> 01:49:14.040]   The Apple iPad now, everybody seems compelled to post their crappy sketches on Instagram.
[01:49:14.040 --> 01:49:15.040]   That kind of thing.
[01:49:15.040 --> 01:49:18.240]   Or really close parallel is the lasers that people were pointing at the airport.
[01:49:18.240 --> 01:49:19.800]   Yeah, that still happens though.
[01:49:19.800 --> 01:49:20.800]   That still happens.
[01:49:20.800 --> 01:49:21.800]   But hasn't it kind of?
[01:49:21.800 --> 01:49:22.800]   I don't know.
[01:49:22.800 --> 01:49:25.840]   I think that the teenagers have an infinite capacity for stupidity.
[01:49:25.840 --> 01:49:27.400]   But they want the newest stupidity.
[01:49:27.400 --> 01:49:28.400]   So this will be our...
[01:49:28.400 --> 01:49:29.400]   Oh, that's why we made the hoverboard.
[01:49:29.400 --> 01:49:31.400]   Yeah, we're always making new teenagers.
[01:49:31.400 --> 01:49:34.080]   That's why we have to because we'll run out if we...
[01:49:34.080 --> 01:49:35.080]   That's right.
[01:49:35.080 --> 01:49:36.080]   So we're giving...
[01:49:36.080 --> 01:49:37.560]   Stupidity is the never-ending supply.
[01:49:37.560 --> 01:49:40.360]   We're giving our 13 year old hoverboard.
[01:49:40.360 --> 01:49:41.360]   We...
[01:49:41.360 --> 01:49:43.040]   Is that so very wrong?
[01:49:43.040 --> 01:49:44.680]   No, Michael, I love it.
[01:49:44.680 --> 01:49:45.680]   Yeah.
[01:49:45.680 --> 01:49:46.680]   Henry loved his.
[01:49:46.680 --> 01:49:47.680]   Yeah.
[01:49:47.680 --> 01:49:48.680]   Jousting, that's so creative.
[01:49:48.680 --> 01:49:50.680]   That'll be the best thing he did in all of his college.
[01:49:50.680 --> 01:49:53.000]   They also wrote it down the stairs.
[01:49:53.000 --> 01:49:56.240]   There's no video exists.
[01:49:56.240 --> 01:49:58.520]   He swears of that one.
[01:49:58.520 --> 01:50:00.520]   Hey, we had a good week this week on Twitter.
[01:50:00.520 --> 01:50:03.600]   If you missed stupidity, it's everywhere.
[01:50:03.600 --> 01:50:04.600]   It reproduces.
[01:50:04.600 --> 01:50:09.240]   In fact, if you watch carefully, you might find some in this rendition of this week on
[01:50:09.240 --> 01:50:10.240]   Twitter.
[01:50:10.240 --> 01:50:12.320]   Previously on Twitter.
[01:50:12.320 --> 01:50:18.120]   The Jolly Tracker is a Santa beard that has electrodes that if you don't smile, there's
[01:50:18.120 --> 01:50:23.720]   other electrodes that shock you and give you a gentle reminder that you should be happy.
[01:50:23.720 --> 01:50:24.720]   Know how.
[01:50:24.720 --> 01:50:27.040]   We're actually going to be doing a gift-giving guide.
[01:50:27.040 --> 01:50:30.560]   We want to show you some of the tools, some of the gadgets, some of the gizmos that you
[01:50:30.560 --> 01:50:34.960]   should either buy for yourself or perhaps buy for that loved one who you're hoping to
[01:50:34.960 --> 01:50:39.240]   get into the maker spirit, the new screen savers.
[01:50:39.240 --> 01:50:40.520]   So I got this from Google.
[01:50:40.520 --> 01:50:47.760]   They said, we're going to send you a charging stand and it's Lego charging stand.
[01:50:47.760 --> 01:50:53.320]   And with any luck at the end of the show, I will have a charging stand for my next floor.
[01:50:53.320 --> 01:50:55.240]   I'll make a spaceship.
[01:50:55.240 --> 01:50:56.240]   Tech news today.
[01:50:56.240 --> 01:51:00.520]   Recently, we have seen a slew of tech tools to help you manage the end of a relationship
[01:51:00.520 --> 01:51:04.080]   I guess it was just a matter of time before someone made a business out of helping rid
[01:51:04.080 --> 01:51:06.920]   someone else from their digital lives completely.
[01:51:06.920 --> 01:51:10.680]   It might sound kind of like the silly convenience that I just don't want to look at pictures
[01:51:10.680 --> 01:51:11.680]   of my ex-boyfriend.
[01:51:11.680 --> 01:51:12.680]   Can you do it for me?
[01:51:12.680 --> 01:51:16.200]   But it can also come down to I don't want to have to look at the messages that I'm getting
[01:51:16.200 --> 01:51:19.320]   from my raster and can you help me deal with that?
[01:51:19.320 --> 01:51:20.320]   To it.
[01:51:20.320 --> 01:51:21.560]   We're no strangers to love.
[01:51:21.560 --> 01:51:23.960]   You know the rules and so do I.
[01:51:23.960 --> 01:51:25.720]   Were you smiling more afterwards?
[01:51:25.720 --> 01:51:26.720]   No, no.
[01:51:26.720 --> 01:51:27.720]   I was smiling in fear.
[01:51:27.720 --> 01:51:32.200]   So here it is by the way.
[01:51:32.200 --> 01:51:34.040]   This is the Lego.
[01:51:34.040 --> 01:51:39.280]   So if you're a Google Fi, that's their cell phone service customer.
[01:51:39.280 --> 01:51:40.720]   They send you they send it's cute.
[01:51:40.720 --> 01:51:41.720]   It's not real Lego.
[01:51:41.720 --> 01:51:43.920]   It's faux Lego.
[01:51:43.920 --> 01:51:47.440]   And but you know, talk about take it.
[01:51:47.440 --> 01:51:48.440]   I don't know.
[01:51:48.440 --> 01:51:49.680]   Make taking lemonade and making a lemon out of it.
[01:51:49.680 --> 01:51:51.800]   I'm not sure exactly what the opposite is.
[01:51:51.800 --> 01:51:56.240]   But I was this was cool, but they didn't send me enough pieces.
[01:51:56.240 --> 01:51:57.640]   And they sent me the wrong pieces.
[01:51:57.640 --> 01:52:02.560]   So it's it's like I was all excited and then I was incredibly frustrated.
[01:52:02.560 --> 01:52:04.080]   Oh, that's not good.
[01:52:04.080 --> 01:52:05.200]   So like what's going on?
[01:52:05.200 --> 01:52:06.200]   Why would you do that?
[01:52:06.200 --> 01:52:07.200]   That's terrible.
[01:52:07.200 --> 01:52:10.000]   That doesn't surprise and delight you as a user.
[01:52:10.000 --> 01:52:12.880]   But but but at the same time, it's a free gift.
[01:52:12.880 --> 01:52:15.280]   Yeah, but it's a free gift that kind of frustrates.
[01:52:15.280 --> 01:52:16.840]   And so I thought that was a little strange.
[01:52:16.840 --> 01:52:18.400]   Isn't that kind of Google?
[01:52:18.400 --> 01:52:20.240]   Yeah, maybe it is.
[01:52:20.240 --> 01:52:21.240]   It's a free.
[01:52:21.240 --> 01:52:23.600]   In a nutshell, kind of a mission statement, isn't it?
[01:52:23.600 --> 01:52:24.600]   Yeah.
[01:52:24.600 --> 01:52:25.600]   Yeah.
[01:52:25.600 --> 01:52:26.600]   It's free.
[01:52:26.600 --> 01:52:28.520]   So your expectation should be low.
[01:52:28.520 --> 01:52:30.800]   I mean, I use feed burner for free.
[01:52:30.800 --> 01:52:32.160]   Yeah, Google Docs.
[01:52:32.160 --> 01:52:33.160]   I never bought it.
[01:52:33.160 --> 01:52:34.160]   I mean, yeah.
[01:52:34.160 --> 01:52:35.160]   I don't have a license.
[01:52:35.160 --> 01:52:36.160]   I did not build it wrong.
[01:52:36.160 --> 01:52:39.360]   It literally did not include all the pieces you need.
[01:52:39.360 --> 01:52:41.360]   Has that been verified?
[01:52:41.360 --> 01:52:43.200]   By what a professional Lego bill?
[01:52:43.200 --> 01:52:46.640]   There's like a store here like down in Samra Feld.
[01:52:46.640 --> 01:52:47.640]   It has the Lego.
[01:52:47.640 --> 01:52:48.640]   Do we still have the instructions?
[01:52:48.640 --> 01:52:49.640]   I mean, I'm not doubting you.
[01:52:49.640 --> 01:52:51.440]   I think they're over there on the radio set.
[01:52:51.440 --> 01:52:53.200]   No, they might be on the radio set.
[01:52:53.200 --> 01:52:56.200]   I will.
[01:52:56.200 --> 01:52:58.280]   Okay, Warley.
[01:52:58.280 --> 01:52:59.280]   Challenge accepted.
[01:52:59.280 --> 01:53:00.280]   She got two Lego skills.
[01:53:00.280 --> 01:53:02.240]   I'm going to have to stay after the show and build with you.
[01:53:02.240 --> 01:53:03.640]   I'm going to give you.
[01:53:03.640 --> 01:53:08.080]   So they sent in a bag, supposedly all the pieces, and you know, the traditional Lego
[01:53:08.080 --> 01:53:10.080]   non-verbal instructions.
[01:53:10.080 --> 01:53:11.080]   Okay.
[01:53:11.080 --> 01:53:12.880]   I was able to make the stand.
[01:53:12.880 --> 01:53:15.960]   There's some differences in the color.
[01:53:15.960 --> 01:53:16.960]   Here's the instructions.
[01:53:16.960 --> 01:53:17.960]   Okay.
[01:53:17.960 --> 01:53:21.880]   I crumpled them up in frustration, so I apologize.
[01:53:21.880 --> 01:53:24.880]   So I built that stand.
[01:53:24.880 --> 01:53:29.120]   And even though the colors don't match, like that's supposed to be all green, here on the
[01:53:29.120 --> 01:53:37.800]   other side is what they call the cable route thing, the cable tidy.
[01:53:37.800 --> 01:53:42.200]   So see if you can build that from the remaining pieces.
[01:53:42.200 --> 01:53:44.760]   And I promise you, this is the complete set.
[01:53:44.760 --> 01:53:45.760]   Good luck.
[01:53:45.760 --> 01:53:46.760]   While you're doing that.
[01:53:46.760 --> 01:53:50.480]   Questioning you is the worst thing I've done all week.
[01:53:50.480 --> 01:53:52.480]   I'm just going to say, I'll be over here.
[01:53:52.480 --> 01:53:53.760]   This is the Google reader of Lego kits.
[01:53:53.760 --> 01:53:54.760]   That's what I'm saying.
[01:53:54.760 --> 01:53:58.280]   I also got the Google Pixel C, which is actually very nice.
[01:53:58.280 --> 01:53:59.280]   Look at this.
[01:53:59.280 --> 01:54:00.840]   It's kind of a magnet thing.
[01:54:00.840 --> 01:54:02.440]   It has all the pieces.
[01:54:02.440 --> 01:54:04.520]   Yeah, I believe so.
[01:54:04.520 --> 01:54:08.480]   So although Jason Howell ordered his same time effect a little earlier than mine, and
[01:54:08.480 --> 01:54:10.320]   he's only got the keyboard so far.
[01:54:10.320 --> 01:54:13.360]   So in fact, you got the same thing with Google.
[01:54:13.360 --> 01:54:14.360]   It's a thing.
[01:54:14.360 --> 01:54:15.360]   So this is it.
[01:54:15.360 --> 01:54:18.240]   It's a 10 inch tablet, a little pricey.
[01:54:18.240 --> 01:54:19.680]   But it's from Google.
[01:54:19.680 --> 01:54:23.440]   And then you order it for another $179 bucks, $500 bucks for the tablet, $179 bucks for
[01:54:23.440 --> 01:54:24.440]   the keyboard.
[01:54:24.440 --> 01:54:28.640]   And this is what they tell you to slide the keyboard off to break the magnetic attraction.
[01:54:28.640 --> 01:54:32.840]   And then you slap the tablet on this thing, and then it tilts up.
[01:54:32.840 --> 01:54:34.680]   And it's actually a very strong magnet.
[01:54:34.680 --> 01:54:36.760]   So it's not going to fall off.
[01:54:36.760 --> 01:54:37.760]   And now you have a key.
[01:54:37.760 --> 01:54:38.760]   It's Android.
[01:54:38.760 --> 01:54:39.760]   It's an Android tablet with a keyboard.
[01:54:39.760 --> 01:54:41.520]   It's kind of like a little bit makes a little laptop.
[01:54:41.520 --> 01:54:47.040]   How's the application switching compared to iOS?
[01:54:47.040 --> 01:54:48.560]   It's simply, you know, it's Android.
[01:54:48.560 --> 01:54:50.840]   If you've ever used Android, it's a very similar thing.
[01:54:50.840 --> 01:54:55.240]   With iOS, you double tap the home key.
[01:54:55.240 --> 01:55:00.080]   Or if you have a modern iPhone, you can force touch it over.
[01:55:00.080 --> 01:55:03.120]   With Android, it's Alt-Tab.
[01:55:03.120 --> 01:55:05.080]   And or you hit the or you hit the recents.
[01:55:05.080 --> 01:55:06.080]   Actually, let me do that.
[01:55:06.080 --> 01:55:08.480]   Let me hit the recents and you get you get with the recents.
[01:55:08.480 --> 01:55:11.480]   You get that sort of yeah.
[01:55:11.480 --> 01:55:15.040]   And there's a keyboard just as there is for the iPad Pro.
[01:55:15.040 --> 01:55:17.000]   There's a keyboard stroke, which is Alt-Tab.
[01:55:17.000 --> 01:55:20.160]   That does this unless you scroll through these.
[01:55:20.160 --> 01:55:24.040]   That's always my beef with mobile devices as productivity tools.
[01:55:24.040 --> 01:55:26.160]   Well, it is Android.
[01:55:26.160 --> 01:55:27.880]   So it's not really designed for a big screen.
[01:55:27.880 --> 01:55:30.120]   But I like the widgets, which makes, to me,
[01:55:30.120 --> 01:55:32.440]   this makes this a little bit better than an iPad.
[01:55:32.440 --> 01:55:35.520]   Instead of just a faceless grid of icons, which
[01:55:35.520 --> 01:55:39.080]   you get in iOS, admittedly, this is ugly as sin.
[01:55:39.080 --> 01:55:40.760]   And it looks like Times Square.
[01:55:40.760 --> 01:55:43.040]   But every one of these widgets is telling me something.
[01:55:43.040 --> 01:55:43.800]   Here's the chat room.
[01:55:43.800 --> 01:55:46.200]   That's a notification just came through.
[01:55:46.200 --> 01:55:48.880]   Those big text, that's just-- this is Google Plus.
[01:55:48.880 --> 01:55:53.120]   So I let it have a big screen.
[01:55:53.120 --> 01:55:55.440]   So more often it's pictures like that.
[01:55:55.440 --> 01:55:56.200]   I like this.
[01:55:56.200 --> 01:55:59.160]   I actually-- I'm pretty happy with it.
[01:55:59.160 --> 01:56:02.480]   I know it got very mixed reviews, partly because some apps,
[01:56:02.480 --> 01:56:04.120]   like Instagram, it's kind of pathetic.
[01:56:04.120 --> 01:56:06.080]   Instagram still thinks I'm on a phone.
[01:56:06.080 --> 01:56:08.280]   So it makes it all sideways.
[01:56:08.280 --> 01:56:09.560]   But that's Instagram's fault, right?
[01:56:09.560 --> 01:56:10.960]   You can't really fault Google for that.
[01:56:10.960 --> 01:56:12.240]   Most apps do a better job.
[01:56:12.240 --> 01:56:13.760]   This is our chat room.
[01:56:13.760 --> 01:56:15.640]   And it uses the space intelligently.
[01:56:15.640 --> 01:56:16.600]   It uses panes.
[01:56:16.600 --> 01:56:20.200]   This is actually more like a desktop experience would be.
[01:56:20.200 --> 01:56:23.600]   Here's-- this is Aqua Mail, which is very much like a desktop
[01:56:23.600 --> 01:56:24.520]   mail app.
[01:56:24.520 --> 01:56:28.360]   And when I use this, I feel like I have full productivity.
[01:56:28.360 --> 01:56:31.160]   I can slide-- I mean, I feel pretty good about this.
[01:56:31.160 --> 01:56:32.280]   This isn't a Google product.
[01:56:32.280 --> 01:56:34.600]   This is a third-party mail app.
[01:56:34.600 --> 01:56:35.320]   I like it.
[01:56:35.320 --> 01:56:37.680]   I personally think this is a great product.
[01:56:37.680 --> 01:56:38.600]   But--
[01:56:38.600 --> 01:56:39.440]   My heart will go.
[01:56:39.440 --> 01:56:42.920]   This and iPad and those kinds of things is no drag and drop.
[01:56:42.920 --> 01:56:45.040]   When I start actually wanting to be productive,
[01:56:45.040 --> 01:56:47.640]   I want to be able to have multiple windows, move things
[01:56:47.640 --> 01:56:48.440]   around.
[01:56:48.440 --> 01:56:49.640]   It's not a computer.
[01:56:49.640 --> 01:56:51.760]   It's getting better.
[01:56:51.760 --> 01:56:54.320]   And I can use the iPad Pro with a keyboard now
[01:56:54.320 --> 01:56:58.080]   and actually get some stuff done, which I couldn't before.
[01:56:58.080 --> 01:57:01.160]   For those of you listening, I want you to hear this snap.
[01:57:01.160 --> 01:57:02.000]   Yep, you hear that?
[01:57:02.000 --> 01:57:04.520]   I just put the last piece in, and I've done it correctly.
[01:57:04.520 --> 01:57:05.520]   Get out of here!
[01:57:05.520 --> 01:57:06.360]   [LAUGHTER]
[01:57:06.360 --> 01:57:07.360]   Hi, Wally.
[01:57:07.360 --> 01:57:08.280]   Zzaya!
[01:57:08.280 --> 01:57:10.520]   For those of you watching, please turn away.
[01:57:10.520 --> 01:57:12.160]   Zzaya.
[01:57:12.160 --> 01:57:13.640]   You haven't even started.
[01:57:13.640 --> 01:57:14.680]   I know.
[01:57:14.680 --> 01:57:16.520]   Are you not a LEGO user, doer?
[01:57:16.520 --> 01:57:17.600]   Can we get Finn in here?
[01:57:17.600 --> 01:57:19.920]   I'm just really regretting that I was questioning you
[01:57:19.920 --> 01:57:21.600]   in any way, shape, or form, because it's
[01:57:21.600 --> 01:57:23.560]   going to end up in my own personal shame.
[01:57:23.560 --> 01:57:25.360]   No, I would-- you know what's cool,
[01:57:25.360 --> 01:57:27.800]   is I was able to build a Star Wars speeder.
[01:57:27.800 --> 01:57:28.320]   [LAUGHTER]
[01:57:28.320 --> 01:57:30.400]   But I can't, for some reason, that cable tidy just
[01:57:30.400 --> 01:57:30.880]   doesn't have enough.
[01:57:30.880 --> 01:57:31.640]   I don't doubt you.
[01:57:31.640 --> 01:57:33.040]   Hey, I want to thank iTunes, which
[01:57:33.040 --> 01:57:37.240]   named us one of the best of 2015 podcasts.
[01:57:37.240 --> 01:57:38.680]   Duh.
[01:57:38.680 --> 01:57:39.520]   We're-- well--
[01:57:39.520 --> 01:57:41.040]   You guys are the best.
[01:57:41.040 --> 01:57:44.440]   It's kind of a mixed compliment, because they put us
[01:57:44.440 --> 01:57:45.760]   in the classics section.
[01:57:45.760 --> 01:57:47.840]   Oh, wow.
[01:57:47.840 --> 01:57:50.240]   Does Yo-Yo Ma also have a podcast that your next--
[01:57:50.240 --> 01:57:51.680]   Classic.
[01:57:51.680 --> 01:57:52.840]   This is a good time of year, though.
[01:57:52.840 --> 01:57:55.400]   This is when the Google and Twitter and YouTube
[01:57:55.400 --> 01:57:57.840]   do their end of year thing.
[01:57:57.840 --> 01:57:59.800]   I meant to do this last week, and I forgot
[01:57:59.800 --> 01:58:04.000]   to play the YouTube rewind video.
[01:58:04.000 --> 01:58:04.440]   It's not--
[01:58:04.440 --> 01:58:05.440]   My wife works there.
[01:58:05.440 --> 01:58:06.080]   Disclosure.
[01:58:06.080 --> 01:58:06.600]   Disclosure.
[01:58:06.600 --> 01:58:08.440]   She actually probably produced some of this, didn't she?
[01:58:08.440 --> 01:58:09.360]   No, she didn't.
[01:58:09.360 --> 01:58:10.360]   That's a totally different place.
[01:58:10.360 --> 01:58:11.640]   Wasn't done in LA?
[01:58:11.640 --> 01:58:12.040]   No.
[01:58:12.040 --> 01:58:14.240]   It's done by the team up in San Bruno.
[01:58:14.240 --> 01:58:15.800]   No kidding.
[01:58:15.800 --> 01:58:18.360]   Because I feel like this--
[01:58:18.360 --> 01:58:22.360]   the YouTube rewind video really has up the ante
[01:58:22.360 --> 01:58:23.400]   in terms of production value.
[01:58:23.400 --> 01:58:24.240]   This thing is--
[01:58:24.240 --> 01:58:25.800]   I mean, they shot some of it at the space.
[01:58:25.800 --> 01:58:26.920]   It just wasn't.
[01:58:26.920 --> 01:58:28.080]   I mean, it's amazing.
[01:58:28.080 --> 01:58:29.960]   I sure they used green screen a lot and stuff,
[01:58:29.960 --> 01:58:36.560]   but apparently they rented a barge with a pit, a ball pit.
[01:58:36.560 --> 01:58:37.760]   Well, didn't they have a barge?
[01:58:37.760 --> 01:58:38.520]   No, what they had parked?
[01:58:38.520 --> 01:58:41.040]   Maybe this is what the old barge was for.
[01:58:41.040 --> 01:58:42.640]   I mean, the production values on this
[01:58:42.640 --> 01:58:44.920]   are actually through the roof.
[01:58:44.920 --> 01:58:47.560]   And of course, they have a lot of YouTube stars in it,
[01:58:47.560 --> 01:58:50.240]   including what Sheerla's are will show up in just a second.
[01:58:50.240 --> 01:58:54.720]   She's going to be a little bit blurry because there she is.
[01:58:54.720 --> 01:58:55.080]   Wow.
[01:58:55.080 --> 01:58:57.000]   You have to have to pause really fast.
[01:58:57.000 --> 01:58:57.520]   She did.
[01:58:57.520 --> 01:59:00.280]   And there's a blurry image of her.
[01:59:00.280 --> 01:59:01.720]   These are all YouTube stars who I don't know.
[01:59:01.720 --> 01:59:02.640]   But, Tom, you might know them.
[01:59:02.640 --> 01:59:03.880]   I'm sure Eileen would know them.
[01:59:03.880 --> 01:59:06.040]   I know I'm a little more just because Eileen works there.
[01:59:06.040 --> 01:59:07.040]   Yeah.
[01:59:07.040 --> 01:59:08.920]   But if you're over 25 or 30--
[01:59:08.920 --> 01:59:10.240]   You won't know who these people are.
[01:59:10.240 --> 01:59:10.520]   Yeah.
[01:59:10.520 --> 01:59:12.600]   This is like watching MTV in the '80s, right?
[01:59:12.600 --> 01:59:15.200]   Marcus Brownlee, who's that?
[01:59:15.200 --> 01:59:15.640]   I don't know.
[01:59:15.640 --> 01:59:19.840]   Seriously, though, is there anybody over 40 in this video?
[01:59:19.840 --> 01:59:21.440]   Yes, John Oliver's in it.
[01:59:21.440 --> 01:59:23.400]   OK, one second.
[01:59:23.400 --> 01:59:25.240]   And actually, they had more--
[01:59:25.240 --> 01:59:27.000]   They had more real legitimate--
[01:59:27.000 --> 01:59:28.440]   I shouldn't say that is wrong.
[01:59:28.440 --> 01:59:31.560]   They had more mainstream stars last year.
[01:59:31.560 --> 01:59:32.720]   These are legitimate stars.
[01:59:32.720 --> 01:59:35.040]   When you talk PewDiePie, that's a legitimate star.
[01:59:35.040 --> 01:59:35.400]   He gets it.
[01:59:35.400 --> 01:59:36.960]   Well, and that's the thing, right?
[01:59:36.960 --> 01:59:39.800]   This kind of hit me when I was at the YouTube space
[01:59:39.800 --> 01:59:41.720]   for late night with James Corden taping.
[01:59:41.720 --> 01:59:44.760]   I was sitting next to these two folks talking about how they
[01:59:44.760 --> 01:59:45.880]   watch TV.
[01:59:45.880 --> 01:59:47.720]   And the one was saying, yeah, I actually
[01:59:47.720 --> 01:59:49.640]   do watch Netflix sometimes.
[01:59:49.640 --> 01:59:50.000]   Right.
[01:59:50.000 --> 01:59:50.920]   But it's mostly YouTube.
[01:59:50.920 --> 01:59:53.200]   And his girlfriend was like, yeah, I only watch YouTube.
[01:59:53.200 --> 01:59:55.160]   Yeah, and granted, they were in YouTube space.
[01:59:55.160 --> 01:59:59.280]   But that is the reality for a lot of folks under 30,
[01:59:59.280 --> 02:00:02.400]   not just about YouTube, but just like they don't watch TV.
[02:00:02.400 --> 02:00:04.880]   So our stars are not familiar to them,
[02:00:04.880 --> 02:00:07.480]   unless they're on those other shows.
[02:00:07.480 --> 02:00:10.160]   So interesting to think that the Verge and Gizmodo
[02:00:10.160 --> 02:00:13.480]   and all these big blogs, you surf sort of the Ziff Davis
[02:00:13.480 --> 02:00:14.880]   reviews and the--
[02:00:14.880 --> 02:00:17.680]   Because that's totally usurped everything, all mainstream media.
[02:00:17.680 --> 02:00:22.880]   But now YouTube is usurping the review space from the blogs.
[02:00:22.880 --> 02:00:23.440]   From everybody.
[02:00:23.440 --> 02:00:25.400]   That's where everybody's going to get advice on products.
[02:00:25.400 --> 02:00:28.280]   Well, again, as Kee, you said, if you're under 25, why not?
[02:00:28.280 --> 02:00:30.240]   [MUSIC PLAYING]
[02:00:30.240 --> 02:00:32.240]   [INTERPOSING VOICES]
[02:00:32.240 --> 02:00:33.240]   This was really interesting.
[02:00:33.240 --> 02:00:35.400]   This is a foreign language to me because I know--
[02:00:35.400 --> 02:00:36.400]   there's John Oliver.
[02:00:36.400 --> 02:00:41.320]   I know a lot of this is referring to YouTube.
[02:00:41.320 --> 02:00:43.840]   If you're a YouTube fanatic, you will recognize all these
[02:00:43.840 --> 02:00:46.160]   a lot of in-jugs.
[02:00:46.160 --> 02:00:47.480]   It's lost on me completely.
[02:00:47.480 --> 02:00:47.960]   Yeah.
[02:00:47.960 --> 02:00:49.080]   It's like hearing people talk about Warcraft.
[02:00:49.080 --> 02:00:50.400]   You didn't learn how to whip it.
[02:00:50.400 --> 02:00:52.240]   And Nene.
[02:00:52.240 --> 02:00:55.760]   I know how to Nene, but can you show me how to doggy?
[02:00:55.760 --> 02:00:57.320]   No.
[02:00:57.320 --> 02:01:00.080]   This Freddy Fozbear from a Five Nights at Freddy's
[02:01:00.080 --> 02:01:00.800]   shows up in this.
[02:01:00.800 --> 02:01:01.640]   I don't know.
[02:01:01.640 --> 02:01:03.080]   Oh, I guess that's Subway rat.
[02:01:03.080 --> 02:01:06.360]   So a few of these memes I'm recognizing, but very few.
[02:01:06.360 --> 02:01:07.840]   [MUSIC PLAYING]
[02:01:07.840 --> 02:01:09.400]   And I-- you know.
[02:01:09.400 --> 02:01:10.920]   I like the major laser, though.
[02:01:10.920 --> 02:01:12.960]   I got to sit down with my son to have him.
[02:01:12.960 --> 02:01:13.960]   I'll know.
[02:01:13.960 --> 02:01:16.000]   Yeah, I'll tell him about James Joyce's list season,
[02:01:16.000 --> 02:01:16.640]   what that means.
[02:01:16.640 --> 02:01:18.200]   And he could tell me what YouTube rewinds.
[02:01:18.200 --> 02:01:18.760]   Do you know?
[02:01:18.760 --> 02:01:19.680]   Oh, sure.
[02:01:19.680 --> 02:01:20.120]   Wow.
[02:01:20.120 --> 02:01:22.000]   Oh, that's all in the Bloomsday book, my friend.
[02:01:22.000 --> 02:01:22.520]   Wow.
[02:01:22.520 --> 02:01:25.120]   I'm Irish and I drink and I still don't know.
[02:01:25.120 --> 02:01:27.840]   There's I, Justine, for three seconds.
[02:01:27.840 --> 02:01:30.080]   Some of this isn't this--
[02:01:30.080 --> 02:01:34.560]   Tom, isn't this really about the YouTube stars in it
[02:01:34.560 --> 02:01:38.240]   and kind of giving them props and helping them feel good?
[02:01:38.240 --> 02:01:39.600]   Because really, this is how YouTube makes money
[02:01:39.600 --> 02:01:40.600]   is these people.
[02:01:40.600 --> 02:01:42.440]   It's making them feel good about what they do.
[02:01:42.440 --> 02:01:44.560]   And like, you're in the rewind video.
[02:01:44.560 --> 02:01:48.160]   For the people who give these people in this video millions
[02:01:48.160 --> 02:01:51.000]   of subscribers, it's about making them feel like, wow,
[02:01:51.000 --> 02:01:52.000]   yeah, I was a part of that.
[02:01:52.000 --> 02:01:52.520]   Right.
[02:01:52.520 --> 02:01:54.520]   It's just like any network promotion at the end of the year.
[02:01:54.520 --> 02:01:55.280]   I guess you're right.
[02:01:55.280 --> 02:01:55.480]   Yeah.
[02:01:55.480 --> 02:01:58.240]   I used to call them Pops, Proof of Performance.
[02:01:58.240 --> 02:02:00.600]   But the real key-- and there's PewDiePie and what's
[02:02:00.600 --> 02:02:01.240]   her name?
[02:02:01.240 --> 02:02:02.920]   PewDiePie, or I can't remember.
[02:02:02.920 --> 02:02:05.520]   The real key, though, for YouTube is not the audience,
[02:02:05.520 --> 02:02:07.680]   but these stars.
[02:02:07.680 --> 02:02:09.600]   They need to keep these guys in the stable.
[02:02:09.600 --> 02:02:11.840]   There's a lot of people going after them.
[02:02:11.840 --> 02:02:12.040]   Yeah.
[02:02:12.040 --> 02:02:13.000]   There's a lot of--
[02:02:13.000 --> 02:02:13.440]   Yeah.
[02:02:13.440 --> 02:02:14.760]   These are their cash casts.
[02:02:14.760 --> 02:02:16.560]   And in many cases--
[02:02:16.560 --> 02:02:18.040]   there's Casey Neistat in the middle there.
[02:02:18.040 --> 02:02:21.120]   In many cases, they don't make that much money.
[02:02:21.120 --> 02:02:22.360]   They work really hard.
[02:02:22.360 --> 02:02:23.920]   It's kind of like--
[02:02:23.920 --> 02:02:25.520]   Jason Calicanis, this newsletter,
[02:02:25.520 --> 02:02:27.960]   is saying, I'm going to work on YouTube's farm no more.
[02:02:27.960 --> 02:02:31.360]   It's kind of like, this is your spiff
[02:02:31.360 --> 02:02:33.040]   because you worked really hard this year
[02:02:33.040 --> 02:02:35.680]   and you made $3,000.
[02:02:35.680 --> 02:02:38.320]   Well, they need to sell this to the next generation.
[02:02:38.320 --> 02:02:40.120]   These are the people making hundreds of thousands.
[02:02:40.120 --> 02:02:41.160]   These are people making real money.
[02:02:41.160 --> 02:02:43.480]   And a lot of these people are out on vessel
[02:02:43.480 --> 02:02:47.040]   and doing their own studio stuff.
[02:02:47.040 --> 02:02:49.440]   The problem is it used to be just YouTube, right?
[02:02:49.440 --> 02:02:53.480]   And now there's all different levels of people at YouTube.
[02:02:53.480 --> 02:02:55.960]   There's people making money, people getting paid by YouTube
[02:02:55.960 --> 02:02:57.200]   for YouTube originals.
[02:02:57.200 --> 02:02:59.880]   There's people leaving YouTube to go make money elsewhere.
[02:02:59.880 --> 02:03:01.360]   It's just exploding.
[02:03:01.360 --> 02:03:03.240]   Well, and most of the people who make money, though,
[02:03:03.240 --> 02:03:05.560]   don't make it from YouTube plus.
[02:03:05.560 --> 02:03:07.840]   Like Michelle Fan, who sells cosmetic--
[02:03:07.840 --> 02:03:08.360]   Correct.
[02:03:08.360 --> 02:03:09.000]   Yeah, yeah.
[02:03:09.000 --> 02:03:09.800]   It's not just--
[02:03:09.800 --> 02:03:10.800]   They have merchandising.
[02:03:10.800 --> 02:03:12.520]   They have a nice sponsorship.
[02:03:12.520 --> 02:03:13.520]   Right, definitely.
[02:03:13.520 --> 02:03:13.920]   Yeah.
[02:03:13.920 --> 02:03:15.760]   So that's kind of my point is that YouTube
[02:03:15.760 --> 02:03:18.560]   is kind of needs to kind of make nice with these people.
[02:03:18.560 --> 02:03:20.480]   Which is why they started YouTube originals, right?
[02:03:20.480 --> 02:03:23.040]   Is to go like, OK, we'll actually give you direct money
[02:03:23.040 --> 02:03:23.520]   to make stuff like--
[02:03:23.520 --> 02:03:24.440]   That flops, you didn't?
[02:03:24.440 --> 02:03:24.960]   Can't you--
[02:03:24.960 --> 02:03:26.080]   No, no, no.
[02:03:26.080 --> 02:03:29.200]   There was the old thing where they tried just giving people
[02:03:29.200 --> 02:03:30.400]   money and seeing what happened.
[02:03:30.400 --> 02:03:33.600]   This coming year, there's like a dozen shows that are--
[02:03:33.600 --> 02:03:34.720]   High Netflix.
[02:03:34.720 --> 02:03:35.720]   Like Netflix is doing.
[02:03:35.720 --> 02:03:38.360]   Not quite there, but yeah, closer to that.
[02:03:38.360 --> 02:03:39.560]   Incubated.
[02:03:39.560 --> 02:03:40.560]   Interested.
[02:03:40.560 --> 02:03:41.840]   Direct productions.
[02:03:41.840 --> 02:03:43.320]   Will they be long form?
[02:03:43.320 --> 02:03:44.120]   Yeah.
[02:03:44.120 --> 02:03:46.280]   Lighty Pies got one that's about pranks.
[02:03:46.280 --> 02:03:47.960]   The Fine Brothers have one that's a parody
[02:03:47.960 --> 02:03:49.400]   of a singing competition.
[02:03:49.400 --> 02:03:50.760]   That's smart.
[02:03:50.760 --> 02:03:52.920]   Yeah.
[02:03:52.920 --> 02:03:54.640]   Because that's what you--
[02:03:54.640 --> 02:03:56.200]   I think that's what YouTube needs to do.
[02:03:56.200 --> 02:03:58.040]   And that's certainly what advertisers want them to do,
[02:03:58.040 --> 02:04:00.000]   which has proved there more than just
[02:04:00.000 --> 02:04:02.520]   quick viral videos.
[02:04:02.520 --> 02:04:03.640]   You know, when I was driving up here,
[02:04:03.640 --> 02:04:06.320]   I was thinking to myself that this was going to be like,
[02:04:06.320 --> 02:04:07.360]   going to a Christmas party.
[02:04:07.360 --> 02:04:09.800]   This was going to be so fun to sit with friends
[02:04:09.800 --> 02:04:10.720]   and talk about stuff.
[02:04:10.720 --> 02:04:11.960]   Have some more wine, I'm sorry.
[02:04:11.960 --> 02:04:12.560]   No, no.
[02:04:12.560 --> 02:04:13.320]   I was just thinking--
[02:04:13.320 --> 02:04:14.680]   And then we started talking about YouTube.
[02:04:14.680 --> 02:04:15.280]   I just noticed the clock.
[02:04:15.280 --> 02:04:16.120]   I'm like, oh my gosh.
[02:04:16.120 --> 02:04:16.920]   Sorry, Becky.
[02:04:16.920 --> 02:04:18.200]   No, we're two hours in.
[02:04:18.200 --> 02:04:19.440]   Because I was just about to ask you,
[02:04:19.440 --> 02:04:20.960]   like, how's it going for Eileen?
[02:04:20.960 --> 02:04:22.720]   And I went-- and then I'm like, this is not--
[02:04:22.720 --> 02:04:23.960]   Well, we're doing a show here.
[02:04:23.960 --> 02:04:25.120]   This is actually--
[02:04:25.120 --> 02:04:25.520]   You know what?
[02:04:25.520 --> 02:04:28.000]   It can be because people are interested in Eileen's doing it.
[02:04:28.000 --> 02:04:29.720]   You know, it's OK.
[02:04:29.720 --> 02:04:31.480]   Hey, let me do a quick Casper ad.
[02:04:31.480 --> 02:04:33.680]   And then we will wrap this up.
[02:04:33.680 --> 02:04:36.280]   Casper, of course, is an online retailer
[02:04:36.280 --> 02:04:39.040]   of great mattresses for a fraction of the price.
[02:04:39.040 --> 02:04:41.240]   You can kind of see a thread line
[02:04:41.240 --> 02:04:42.920]   through all of our advertisers.
[02:04:42.920 --> 02:04:45.640]   The whole idea is to take businesses, traditional brick
[02:04:45.640 --> 02:04:48.600]   and mortar businesses, disintermediate them,
[02:04:48.600 --> 02:04:50.240]   go from the factory direct to you,
[02:04:50.240 --> 02:04:52.360]   saving you money, giving you a better experience.
[02:04:52.360 --> 02:04:54.160]   Casper is a perfect example.
[02:04:54.160 --> 02:04:57.680]   The mattress business is a terrible business.
[02:04:57.680 --> 02:05:00.920]   If you go to a showroom, the markup is insane.
[02:05:00.920 --> 02:05:03.760]   They give you this fiction that you can owe a try before you buy.
[02:05:03.760 --> 02:05:04.880]   You've got to go to a showroom.
[02:05:04.880 --> 02:05:07.200]   You're lying on this thing in broad daylight
[02:05:07.200 --> 02:05:09.040]   under fluorescent lights for five minutes
[02:05:09.040 --> 02:05:11.680]   with the shopgirl looking at you.
[02:05:11.680 --> 02:05:14.160]   So it's not a good way to test a mattress.
[02:05:14.160 --> 02:05:14.560]   No.
[02:05:14.560 --> 02:05:19.440]   They make the mattresses very high quality in the US
[02:05:19.440 --> 02:05:21.680]   with really state of the art technology.
[02:05:21.680 --> 02:05:23.120]   Here's our-- if you're watching the video,
[02:05:23.120 --> 02:05:23.920]   our Casper mattress.
[02:05:23.920 --> 02:05:25.680]   It came in a very small box.
[02:05:25.680 --> 02:05:27.400]   That's a queen-sized mattress.
[02:05:27.400 --> 02:05:29.560]   Pay no attention to the fact that I'm wearing my jammies already.
[02:05:29.560 --> 02:05:31.360]   I just was-- I was excited.
[02:05:31.360 --> 02:05:31.920]   I like that.
[02:05:31.920 --> 02:05:32.680]   I was excited.
[02:05:32.680 --> 02:05:33.600]   I mean--
[02:05:33.600 --> 02:05:34.360]   You buy it online.
[02:05:34.360 --> 02:05:36.640]   And by the way, no risk, because you get 100 nights
[02:05:36.640 --> 02:05:37.480]   to lie on it.
[02:05:37.480 --> 02:05:39.360]   Do anything you want on it.
[02:05:39.360 --> 02:05:40.600]   Really test it out.
[02:05:40.600 --> 02:05:42.560]   And if you don't like it, they will come.
[02:05:42.560 --> 02:05:43.120]   They will get it.
[02:05:43.120 --> 02:05:45.840]   They will refund you every single penny.
[02:05:45.840 --> 02:05:49.240]   So basically, it's 100 days or better yet, 100 nights
[02:05:49.240 --> 02:05:50.080]   to try it out.
[02:05:50.080 --> 02:05:52.400]   So instead of spending-- oh, I got to see this.
[02:05:52.400 --> 02:05:52.840]   Oh.
[02:05:52.840 --> 02:05:54.480]   Oh, he's leaping on it.
[02:05:54.480 --> 02:05:55.520]   No, it's really comfy.
[02:05:55.520 --> 02:05:56.760]   In fact, Ozzy loved it, too.
[02:05:56.760 --> 02:05:57.760]   Everybody loved it.
[02:05:57.760 --> 02:05:58.720]   We all tried it out.
[02:05:58.720 --> 02:06:00.360]   So instead of that markup, and instead
[02:06:00.360 --> 02:06:02.680]   of spending money on the retail location,
[02:06:02.680 --> 02:06:04.800]   they're spending money on the customer service to say,
[02:06:04.800 --> 02:06:07.080]   we will take it back if you don't like it.
[02:06:07.080 --> 02:06:08.560]   And the mattress itself.
[02:06:08.560 --> 02:06:09.800]   No springs.
[02:06:09.800 --> 02:06:11.360]   It breathes.
[02:06:11.360 --> 02:06:13.840]   Sometimes people say, oh, well, it's latex and memory foam.
[02:06:13.840 --> 02:06:14.640]   That's not going to breathe.
[02:06:14.640 --> 02:06:16.520]   No, no, it's a very cool mattress.
[02:06:16.520 --> 02:06:17.880]   I really like it.
[02:06:17.880 --> 02:06:18.840]   Oh, what does he does like?
[02:06:18.840 --> 02:06:19.720]   Yeah, no, no, my kidding.
[02:06:19.720 --> 02:06:23.000]   $500 for a twin, $950 for a king size.
[02:06:23.000 --> 02:06:24.160]   That's the biggest size.
[02:06:24.160 --> 02:06:25.560]   And it comes in a small box.
[02:06:25.560 --> 02:06:26.560]   It's very easy.
[02:06:26.560 --> 02:06:28.920]   I ordered one for my son into college because it was easy
[02:06:28.920 --> 02:06:30.000]   for him to get upstairs.
[02:06:30.000 --> 02:06:33.120]   Casper.com/twit to find out more.
[02:06:33.120 --> 02:06:34.400]   And by the way, use the promo code
[02:06:34.400 --> 02:06:36.480]   "twit" and you'll get $50 off.
[02:06:36.480 --> 02:06:38.320]   Some terms in a condition supply.
[02:06:38.320 --> 02:06:41.320]   Casper.com/twit, promo code "twit"
[02:06:41.320 --> 02:06:49.160]   to get $50 off for a very good mattress at absolutely no risk.
[02:06:49.160 --> 02:06:54.560]   Yeah, I think it's really fun to watch the internet change
[02:06:54.560 --> 02:06:55.800]   all of these businesses.
[02:06:55.800 --> 02:06:56.800]   Just rub them.
[02:06:56.800 --> 02:06:58.800]   Just disrupting everything.
[02:06:58.800 --> 02:07:02.840]   Oh, man, we haven't gotten to hardly any of the stories.
[02:07:02.840 --> 02:07:06.120]   I'm going to go have a shave, lay down, and do a website.
[02:07:06.120 --> 02:07:06.560]   It makes you--
[02:07:06.560 --> 02:07:07.680]   OK, you got to get those sheets on.
[02:07:07.680 --> 02:07:08.680]   Get those sheets.
[02:07:08.680 --> 02:07:09.680]   Get the sheets.
[02:07:09.680 --> 02:07:10.440]   Get the sheets.
[02:07:10.440 --> 02:07:11.440]   Can I come be mattress?
[02:07:11.440 --> 02:07:12.120]   Yeah.
[02:07:12.120 --> 02:07:14.160]   I am wondering, though, I mean, the no risk
[02:07:14.160 --> 02:07:15.680]   to the consumer for the mattress.
[02:07:15.680 --> 02:07:17.680]   It's because it's hard to put the mattress back
[02:07:17.680 --> 02:07:18.680]   in the packaging.
[02:07:18.680 --> 02:07:19.200]   No, that's right.
[02:07:19.200 --> 02:07:20.480]   To be the way that they expanded--
[02:07:20.480 --> 02:07:23.720]   You're not required to put it back in the box.
[02:07:23.720 --> 02:07:25.040]   They actually send a courier.
[02:07:25.040 --> 02:07:25.840]   I actually inquired.
[02:07:25.840 --> 02:07:27.240]   I thought, well, how do they get back?
[02:07:27.240 --> 02:07:30.120]   A guy comes with a truck, and they give it a charity.
[02:07:30.120 --> 02:07:31.760]   They don't ship it all the way back to Casper.
[02:07:31.760 --> 02:07:33.880]   They give it to charity, and they give you your money back.
[02:07:33.880 --> 02:07:35.640]   So it's actually a pretty good deal.
[02:07:35.640 --> 02:07:37.960]   I know you're nervous that we didn't do any news today.
[02:07:37.960 --> 02:07:39.000]   So if I gave you--
[02:07:39.000 --> 02:07:41.200]   You looked at this and said great stories.
[02:07:41.200 --> 02:07:43.960]   No, I was just making that up because I didn't know any of it.
[02:07:43.960 --> 02:07:45.040]   I wanted you to tell me about it.
[02:07:45.040 --> 02:07:46.040]   Six stories.
[02:07:46.040 --> 02:07:48.320]   Give me six stories in 60 seconds
[02:07:48.320 --> 02:07:50.000]   that are important that people know about,
[02:07:50.000 --> 02:07:51.480]   even if they don't hear about them on this show,
[02:07:51.480 --> 02:07:52.720]   that they should go research.
[02:07:52.720 --> 02:07:55.000]   Racial discrimination by Airbnb.
[02:07:55.000 --> 02:07:55.500]   OK.
[02:07:55.500 --> 02:08:01.440]   Studied, offered Airbnb hosts, rentals.
[02:08:01.440 --> 02:08:04.000]   People with, apparently, African-American names
[02:08:04.000 --> 02:08:06.680]   are turned down at a significantly higher rate
[02:08:06.680 --> 02:08:08.640]   than people with white-sounding names.
[02:08:08.640 --> 02:08:10.200]   They didn't have pictures or anything.
[02:08:10.200 --> 02:08:11.760]   It was just by name.
[02:08:11.760 --> 02:08:14.520]   Tech giants say Verizon's new cellular tech
[02:08:14.520 --> 02:08:17.880]   could wreck Wi-Fi Verizon, among others,
[02:08:17.880 --> 02:08:22.360]   wants to use Wi-Fi networks, Wi-Fi mesh networks,
[02:08:22.360 --> 02:08:24.520]   for their cell network.
[02:08:24.520 --> 02:08:27.320]   But Google, Microsoft, and Comcast say,
[02:08:27.320 --> 02:08:31.120]   whoa, that's going to really screw up the spectrum.
[02:08:31.120 --> 02:08:35.320]   LTE in an unlicensed spectrum, it's called LTEU.
[02:08:35.320 --> 02:08:38.840]   And it is right there in the Wi-Fi spectrum, so beware.
[02:08:38.840 --> 02:08:39.760]   OK.
[02:08:39.760 --> 02:08:40.200]   That's two.
[02:08:40.200 --> 02:08:40.720]   You're doing great.
[02:08:40.720 --> 02:08:42.400]   Yeah.
[02:08:42.400 --> 02:08:47.320]   One fifth of Americans report going online, quote,
[02:08:47.320 --> 02:08:48.480]   almost constantly.
[02:08:48.480 --> 02:08:52.160]   [LAUGHTER]
[02:08:52.160 --> 02:08:54.200]   This is the great Pew study.
[02:08:54.200 --> 02:08:56.440]   There is an ongoing study of the internet.
[02:08:56.440 --> 02:08:59.880]   21% of Americans report they go online almost constantly.
[02:08:59.880 --> 02:09:04.120]   73% go online on a daily basis.
[02:09:04.120 --> 02:09:06.280]   42% go online several times a day.
[02:09:06.280 --> 02:09:09.600]   10% go online once a day.
[02:09:09.600 --> 02:09:12.120]   But I love the 21% who say I'm always on.
[02:09:12.120 --> 02:09:12.520]   I'm always on.
[02:09:12.520 --> 02:09:13.360]   Well, you got to watch on.
[02:09:13.360 --> 02:09:14.800]   You're always on.
[02:09:14.800 --> 02:09:15.520]   Yeah.
[02:09:15.520 --> 02:09:16.000]   Yeah.
[02:09:16.000 --> 02:09:17.360]   Leo's always on.
[02:09:17.360 --> 02:09:18.200]   I'm always on.
[02:09:18.200 --> 02:09:18.720]   I know.
[02:09:18.720 --> 02:09:19.080]   Right?
[02:09:19.080 --> 02:09:19.640]   Lisa?
[02:09:19.640 --> 02:09:20.120]   Always.
[02:09:20.120 --> 02:09:20.760]   Always on.
[02:09:20.760 --> 02:09:21.640]   Especially 4am.
[02:09:21.640 --> 02:09:22.480]   Watch it.
[02:09:22.480 --> 02:09:24.320]   Watch it on a TV show on the internet.
[02:09:24.320 --> 02:09:26.520]   It's one radio.
[02:09:26.520 --> 02:09:27.440]   OK, that's three stories.
[02:09:27.440 --> 02:09:30.200]   Like I'm the Beats One radio of internet.
[02:09:30.200 --> 02:09:30.640]   Totally.
[02:09:30.640 --> 02:09:31.520]   What else do I need to know?
[02:09:31.520 --> 02:09:33.800]   Ted Cruz used a firm that harvested data
[02:09:33.800 --> 02:09:36.240]   on millions of unwitting Facebook users.
[02:09:36.240 --> 02:09:38.760]   He won't be the only one.
[02:09:38.760 --> 02:09:40.920]   It's a donor-funded US startup.
[02:09:40.920 --> 02:09:45.000]   The Republicans campaign paid UK university academics
[02:09:45.000 --> 02:09:48.000]   to collect psychological profiles on potential voters
[02:09:48.000 --> 02:09:51.520]   from their Facebook profiles.
[02:09:51.520 --> 02:09:52.240]   This is bad.
[02:09:52.240 --> 02:09:54.040]   Largely without their permission.
[02:09:54.040 --> 02:09:55.320]   Yeah.
[02:09:55.320 --> 02:09:58.000]   But I don't think I can almost guarantee you, Ted Cruz is not
[02:09:58.000 --> 02:09:59.800]   the only guy doing this.
[02:09:59.800 --> 02:10:01.440]   This is valuable stuff, right?
[02:10:01.440 --> 02:10:03.120]   This is what Facebook was made to do.
[02:10:03.120 --> 02:10:05.080]   They got to find those conversion candidates.
[02:10:05.080 --> 02:10:06.680]   Who are the people that are on the phone?
[02:10:06.680 --> 02:10:07.680]   It's the swing voter, right?
[02:10:07.680 --> 02:10:10.120]   That's going to determine who wins the election.
[02:10:10.120 --> 02:10:12.720]   Cruz's campaign, according to FEC filings,
[02:10:12.720 --> 02:10:17.600]   paid Cambridge Analytica $750,000 this year.
[02:10:17.600 --> 02:10:19.800]   But they've received another $2.5 million
[02:10:19.800 --> 02:10:23.320]   of the last two years from other super PACs.
[02:10:23.320 --> 02:10:25.680]   And I think they say in there also that Ben Carson--
[02:10:25.680 --> 02:10:26.200]   Yeah.
[02:10:26.200 --> 02:10:27.080]   --aid for some data as well.
[02:10:27.080 --> 02:10:30.320]   I guarantee you once--
[02:10:30.320 --> 02:10:32.760]   you'd be dumb not to do this.
[02:10:32.760 --> 02:10:35.120]   If the data is there, wouldn't you want to know?
[02:10:35.120 --> 02:10:37.680]   Well, if you assume your opponents are doing it too,
[02:10:37.680 --> 02:10:38.640]   it's that kind of--
[02:10:38.640 --> 02:10:40.240]   And whose failure is it?
[02:10:40.240 --> 02:10:42.840]   Is it their failure or is it Facebook's failure?
[02:10:42.840 --> 02:10:43.960]   Right?
[02:10:43.960 --> 02:10:45.920]   If the data's there--
[02:10:45.920 --> 02:10:47.080]   I think that's four.
[02:10:47.080 --> 02:10:48.000]   Is that four stories?
[02:10:48.000 --> 02:10:48.600]   Four?
[02:10:48.600 --> 02:10:49.100]   Yeah.
[02:10:49.100 --> 02:10:50.000]   I need two more?
[02:10:50.000 --> 02:10:52.000]   Yeah, I mean, you are the--
[02:10:52.000 --> 02:10:56.000]   Walmart launches Walmart pay.
[02:10:56.000 --> 02:10:58.560]   Tom, you had some interesting thoughts on this.
[02:10:58.560 --> 02:10:59.560]   You got Apple Pay?
[02:10:59.560 --> 02:11:01.360]   You got your Android Pay now?
[02:11:01.360 --> 02:11:02.640]   You got your Walmart pay.
[02:11:02.640 --> 02:11:04.040]   Is anybody going to use this?
[02:11:04.040 --> 02:11:07.440]   Do you have to wear stretch pants?
[02:11:07.440 --> 02:11:09.040]   Jaggings, you have to wear jaggings.
[02:11:09.040 --> 02:11:10.840]   You have to wear jaggings.
[02:11:10.840 --> 02:11:14.800]   Well, it requires you to scan a QR code, essentially, to pay.
[02:11:14.800 --> 02:11:15.400]   It's worse.
[02:11:15.400 --> 02:11:17.080]   Don't you have to generate a QR code?
[02:11:17.080 --> 02:11:18.360]   They scan your QR code.
[02:11:18.360 --> 02:11:19.860]   Then they send a QR code, which you scan.
[02:11:19.860 --> 02:11:20.860]   Exactly.
[02:11:20.860 --> 02:11:21.360]   Yeah.
[02:11:21.360 --> 02:11:22.360]   It's worse.
[02:11:22.360 --> 02:11:23.200]   So it's not--
[02:11:23.200 --> 02:11:23.700]   All of this--
[02:11:23.700 --> 02:11:24.660]   Ridicously convenient.
[02:11:24.660 --> 02:11:26.460]   All of this is currency.
[02:11:26.460 --> 02:11:28.860]   By the way, you can't use a credit card.
[02:11:28.860 --> 02:11:31.540]   You have to use your Walmart pay card.
[02:11:31.540 --> 02:11:33.900]   All of this because the problem with Android Pay and Apple Pay
[02:11:33.900 --> 02:11:35.140]   is they don't learn enough about you.
[02:11:35.140 --> 02:11:39.300]   So this way, they can really get some information about you.
[02:11:39.300 --> 02:11:41.700]   Don't think it's an on-starter, don't you?
[02:11:41.700 --> 02:11:43.980]   Well, I always think it's interesting,
[02:11:43.980 --> 02:11:47.620]   because Walmart makes plays for their clientele
[02:11:47.620 --> 02:11:51.580]   that may be cash-based only.
[02:11:51.580 --> 02:11:53.140]   They've done some really interesting things with--
[02:11:53.140 --> 02:11:55.540]   That's right.
[02:11:55.540 --> 02:11:59.260]   What's it called when you pay on installment and--
[02:11:59.260 --> 02:11:59.760]   Layaway.
[02:11:59.760 --> 02:12:00.460]   Layaway, sorry.
[02:12:00.460 --> 02:12:01.140]   I had to burn first.
[02:12:01.140 --> 02:12:02.860]   You haven't been in a Walmart in a while, have you?
[02:12:02.860 --> 02:12:03.360]   No.
[02:12:03.360 --> 02:12:03.860]   OK.
[02:12:03.860 --> 02:12:04.660]   Actually, I have.
[02:12:04.660 --> 02:12:05.020]   I was just about five.
[02:12:05.020 --> 02:12:07.360]   I was in my first Walmart the other day.
[02:12:07.360 --> 02:12:07.860]   First.
[02:12:07.860 --> 02:12:09.100]   Yeah, oh, yeah.
[02:12:09.100 --> 02:12:12.380]   You know they have greeters at the door that say hello?
[02:12:12.380 --> 02:12:12.860]   Yeah, dude.
[02:12:12.860 --> 02:12:14.060]   And it's huge.
[02:12:14.060 --> 02:12:16.940]   I went to their e-commerce center down in Silicon Valley.
[02:12:16.940 --> 02:12:18.180]   Amazing.
[02:12:18.180 --> 02:12:22.540]   They have a command central of every skew and what's going.
[02:12:22.540 --> 02:12:24.740]   And it's a completely customizable display
[02:12:24.740 --> 02:12:25.940]   to see what it took.
[02:12:25.940 --> 02:12:28.300]   That's where dynamic pricing is happening.
[02:12:28.300 --> 02:12:28.940]   Amazing.
[02:12:28.940 --> 02:12:33.580]   Amazon is offering subscriptions, not just to Amazon itself,
[02:12:33.580 --> 02:12:35.500]   but to Showtime stars and even more.
[02:12:35.500 --> 02:12:38.140]   Amazon Prime is doing what Apple apparently could not do,
[02:12:38.140 --> 02:12:42.260]   which is build a Tele-Chordcutters Paradise.
[02:12:42.260 --> 02:12:44.860]   Apple is abandoned, apparently, or at least temporarily.
[02:12:44.860 --> 02:12:49.060]   It's a attempt to create live television on Apple TV.
[02:12:49.060 --> 02:12:50.900]   But Amazon seems to be--
[02:12:50.900 --> 02:12:52.940]   we are-- you cover Chordcutting, Tom.
[02:12:52.940 --> 02:12:55.380]   I know this is a big interest of yours.
[02:12:55.380 --> 02:12:56.220]   Yeah.
[02:12:56.220 --> 02:12:58.060]   Brian Brushwood and I do a show called Chord Killers.
[02:12:58.060 --> 02:13:01.300]   And this was something where a week before they announced it,
[02:13:01.300 --> 02:13:03.700]   we said, why doesn't somebody just take all these services
[02:13:03.700 --> 02:13:05.900]   and bundle them together to make it simple to subscribe to?
[02:13:05.900 --> 02:13:07.420]   And it looks like that's what Amazon's trying.
[02:13:07.420 --> 02:13:10.900]   They only have Showtime and then a bunch of networks that
[02:13:10.900 --> 02:13:12.940]   are kind of made up for Chordcutters
[02:13:12.940 --> 02:13:14.900]   from the properties of major networks.
[02:13:14.900 --> 02:13:16.380]   Like Crackle?
[02:13:16.380 --> 02:13:18.300]   Well, no, even weirder than that.
[02:13:18.300 --> 02:13:21.140]   Like an AMC network about indie films and stuff like that.
[02:13:21.140 --> 02:13:24.820]   But if this gets some traction, then they
[02:13:24.820 --> 02:13:27.900]   might start putting more prominent services up there,
[02:13:27.900 --> 02:13:28.580]   possibly.
[02:13:28.580 --> 02:13:31.260]   And Amazon providing the billing and the hosting.
[02:13:31.260 --> 02:13:31.700]   Interesting.
[02:13:31.700 --> 02:13:33.980]   So they're giving landing pages to companies.
[02:13:33.980 --> 02:13:36.660]   Somebody who might not want to roll their own, like HBO Now,
[02:13:36.660 --> 02:13:38.260]   might be willing to go to Amazon and say, hey,
[02:13:38.260 --> 02:13:39.260]   can we put a platform up to our--
[02:13:39.260 --> 02:13:41.780]   Well, Showtime already has that kind of service
[02:13:41.780 --> 02:13:43.860]   that over the top service that HBO Now is.
[02:13:43.860 --> 02:13:45.100]   Showtime's playing it both ways.
[02:13:45.100 --> 02:13:47.580]   And that's what Les Boonvus has said ever since he bought CNET,
[02:13:47.580 --> 02:13:48.900]   he's like, we want to authenticate.
[02:13:48.900 --> 02:13:49.940]   We don't care where.
[02:13:49.940 --> 02:13:51.100]   We'll do it on your platform.
[02:13:51.100 --> 02:13:52.540]   We'll do it on our platform as long as somebody's
[02:13:52.540 --> 02:13:53.740]   paying us for the stuff.
[02:13:53.740 --> 02:13:55.500]   That's why they didn't want to give it to Hulu, though,
[02:13:55.500 --> 02:13:58.060]   because they weren't authenticating people paying them for it.
[02:13:58.060 --> 02:13:59.980]   See, that's smart because I can think--
[02:13:59.980 --> 02:14:01.500]   I wouldn't mind me cork cutting, but I
[02:14:01.500 --> 02:14:04.060]   want 20 different bills from 20 different services.
[02:14:04.060 --> 02:14:07.260]   It'd be nice if somebody could bundle it for me.
[02:14:07.260 --> 02:14:09.460]   By the way, in related news, I went to a Comcast
[02:14:09.460 --> 02:14:11.500]   store this week and had a great experience.
[02:14:11.500 --> 02:14:12.220]   Really?
[02:14:12.220 --> 02:14:14.300]   Flash.
[02:14:14.300 --> 02:14:15.100]   Yeah.
[02:14:15.100 --> 02:14:15.900]   Flash?
[02:14:15.900 --> 02:14:16.660]   It's a news flash?
[02:14:16.660 --> 02:14:17.860]   No, no, that's a news flash.
[02:14:17.860 --> 02:14:19.300]   OK, now I'm with you.
[02:14:19.300 --> 02:14:20.500]   I was dreading it.
[02:14:20.500 --> 02:14:21.500]   I've been putting it off the month.
[02:14:21.500 --> 02:14:22.900]   I thought you were saying you had a great experience
[02:14:22.900 --> 02:14:24.100]   with Adobe Flash.
[02:14:24.100 --> 02:14:25.100]   Me too.
[02:14:25.100 --> 02:14:26.100]   No, no, no, I'm sorry.
[02:14:26.100 --> 02:14:27.420]   I'm not.
[02:14:27.420 --> 02:14:29.420]   I-- well, OK, here's the thing.
[02:14:29.420 --> 02:14:32.740]   So three years ago, I moved out of a department,
[02:14:32.740 --> 02:14:35.260]   three years ago-- literally three years ago--
[02:14:35.260 --> 02:14:37.980]   and carefully returned my Comcast gear to Comcast,
[02:14:37.980 --> 02:14:39.380]   thinking that was the end of it.
[02:14:39.380 --> 02:14:40.980]   I probably got a receipt.
[02:14:40.980 --> 02:14:42.420]   Maybe I even saved it.
[02:14:42.420 --> 02:14:45.740]   Three years later, I got a bill for the unreturned equipment.
[02:14:45.740 --> 02:14:47.100]   Three years later.
[02:14:47.100 --> 02:14:48.420]   Oh, really?
[02:14:48.420 --> 02:14:49.420]   I said, I went in there.
[02:14:49.420 --> 02:14:50.860]   I said, you know, I don't--
[02:14:50.860 --> 02:14:55.060]   I can't prove it, because it was three years ago.
[02:14:55.060 --> 02:14:55.780]   What happened?
[02:14:55.780 --> 02:14:57.220]   I said, did you guys do an audit?
[02:14:57.220 --> 02:15:01.020]   And suddenly, we realized, hey, we don't have Leo's modem.
[02:15:01.020 --> 02:15:02.900]   They said, yeah, maybe.
[02:15:02.900 --> 02:15:04.180]   So she looked in the computer.
[02:15:04.180 --> 02:15:06.380]   It wasn't there, so she said, all right, forget it.
[02:15:06.380 --> 02:15:07.380]   Wow.
[02:15:07.380 --> 02:15:07.780]   We'll see.
[02:15:07.780 --> 02:15:09.580]   Lisa, let me know if we get another bill.
[02:15:09.580 --> 02:15:11.100]   Wait till they send it to the collection agent.
[02:15:11.100 --> 02:15:13.700]   Yeah.
[02:15:13.700 --> 02:15:17.460]   iPhone-- actually, OK, I just came away with it.
[02:15:17.460 --> 02:15:18.660]   I was going to do a quiz.
[02:15:18.660 --> 02:15:19.220]   What do you think, though?
[02:15:19.220 --> 02:15:22.220]   iPhone.
[02:15:22.220 --> 02:15:23.460]   iPhone.
[02:15:23.460 --> 02:15:25.460]   I am so smart.
[02:15:25.460 --> 02:15:26.940]   I'm with you, Becky.
[02:15:26.940 --> 02:15:27.500]   You agree?
[02:15:27.500 --> 02:15:28.340]   You all agree?
[02:15:28.340 --> 02:15:28.860]   Yeah, great.
[02:15:28.860 --> 02:15:29.780]   That was such an awesome story.
[02:15:29.780 --> 02:15:30.380]   That was the sixth one.
[02:15:30.380 --> 02:15:33.460]   Actually, the answer is iPhone.
[02:15:33.460 --> 02:15:37.300]   Question is, what's the most popular camera on Flickr?
[02:15:37.300 --> 02:15:37.700]   That is--
[02:15:37.700 --> 02:15:39.100]   Ahead of Canon icon?
[02:15:39.100 --> 02:15:41.100]   Samsung, even?
[02:15:41.100 --> 02:15:43.300]   42% of all photos shared on Flickr,
[02:15:43.300 --> 02:15:45.580]   because you can tell from the information
[02:15:45.580 --> 02:15:47.140]   uploaded information.
[02:15:47.140 --> 02:15:50.660]   42% of all photos on Flickr are iPhone.
[02:15:50.660 --> 02:15:53.660]   I wonder if they grouped all the Android operating system
[02:15:53.660 --> 02:15:55.700]   phones together.
[02:15:55.700 --> 02:15:59.100]   Maybe if we're all-- yeah, because Samsung's just one.
[02:15:59.100 --> 02:16:00.020]   I mean, I don't doubt it.
[02:16:00.020 --> 02:16:02.180]   It looks like they're breaking it up by manufacturer.
[02:16:02.180 --> 02:16:03.540]   Yeah.
[02:16:03.540 --> 02:16:04.500]   And there's a little graph.
[02:16:04.500 --> 02:16:07.460]   You can see the iPhone 6 is kind of gaining.
[02:16:07.460 --> 02:16:10.900]   Have not pulled the DSLR out in two years.
[02:16:10.900 --> 02:16:11.460]   Why should you?
[02:16:11.460 --> 02:16:12.740]   And I have kids.
[02:16:12.740 --> 02:16:14.260]   I mean, but it's about timing.
[02:16:14.260 --> 02:16:14.700]   It's a little--
[02:16:14.700 --> 02:16:17.300]   I mean, you would think that you would want to get the--
[02:16:17.300 --> 02:16:18.780]   But the iPhone's very good, isn't it?
[02:16:18.780 --> 02:16:21.260]   Actually, all of the top-line camera phones
[02:16:21.260 --> 02:16:23.940]   are now very good, as good as any point you shoot probably.
[02:16:23.940 --> 02:16:25.500]   And additionally, with the DSLR, you
[02:16:25.500 --> 02:16:29.220]   have to have external photo processing, where
[02:16:29.220 --> 02:16:31.500]   with your phone, you've got it right there.
[02:16:31.500 --> 02:16:32.980]   And the apps are right in the phone,
[02:16:32.980 --> 02:16:33.900]   and you can process.
[02:16:33.900 --> 02:16:34.420]   You can do it.
[02:16:34.420 --> 02:16:35.260]   You can put it on Flickr.
[02:16:35.260 --> 02:16:36.820]   You can put it on Instagram.
[02:16:36.820 --> 02:16:37.820]   It's out.
[02:16:37.820 --> 02:16:38.740]   And you're done.
[02:16:38.740 --> 02:16:42.140]   Shocking-- we'll end this with a shocking rumor of the week.
[02:16:42.140 --> 02:16:42.540]   Yeah, go ahead.
[02:16:42.540 --> 02:16:44.740]   According to Mark German, who knows everything at 9 to 5,
[02:16:44.740 --> 02:16:47.140]   Mac, Apple's going to have an event in March at which they're
[02:16:47.140 --> 02:16:51.620]   going to announce a new Apple watch and a new iPhone.
[02:16:51.620 --> 02:16:52.700]   What?
[02:16:52.700 --> 02:16:57.380]   So for all of you who are getting or giving the Apple watch
[02:16:57.380 --> 02:16:57.620]   for--
[02:16:57.620 --> 02:16:59.300]   Three months later.
[02:16:59.300 --> 02:17:00.580]   That would be a slap in the face.
[02:17:00.580 --> 02:17:00.940]   Would be.
[02:17:00.940 --> 02:17:02.300]   I'd be a slap in the face for me,
[02:17:02.300 --> 02:17:03.500]   and I bought mine months ago.
[02:17:03.500 --> 02:17:04.460]   Yeah, it's too soon.
[02:17:04.460 --> 02:17:04.860]   Too soon.
[02:17:04.860 --> 02:17:07.580]   In March, if they announced it for a summer release,
[02:17:07.580 --> 02:17:08.820]   that wouldn't be so bad.
[02:17:08.820 --> 02:17:10.460]   It would be one year, right?
[02:17:10.460 --> 02:17:12.620]   Yeah.
[02:17:12.620 --> 02:17:17.620]   And last year's March event, they debuted the 12-inch MacBook
[02:17:17.620 --> 02:17:20.180]   and finalized the first version of the Apple Watch
[02:17:20.180 --> 02:17:24.820]   for, I think it was a June release, just as you said.
[02:17:24.820 --> 02:17:26.420]   Four-inch iPhone.
[02:17:26.420 --> 02:17:28.380]   That's what they're saying, a four-inch.
[02:17:28.380 --> 02:17:29.860]   It's like an iPod.
[02:17:29.860 --> 02:17:30.660]   It's not about the size.
[02:17:30.660 --> 02:17:32.580]   It's like they already did all the design work.
[02:17:32.580 --> 02:17:33.100]   Years ago.
[02:17:33.100 --> 02:17:33.700]   We know the way already did it.
[02:17:33.700 --> 02:17:34.180]   Let's do it again.
[02:17:34.180 --> 02:17:36.380]   Do it again.
[02:17:36.380 --> 02:17:38.420]   Doesn't have to have a lump in it, does it?
[02:17:38.420 --> 02:17:38.940]   No.
[02:17:38.940 --> 02:17:41.740]   Their release cycle is now new.
[02:17:41.740 --> 02:17:44.340]   Little newer, cheap.
[02:17:44.340 --> 02:17:46.420]   Dr. Kiki, can I just say, I miss you.
[02:17:46.420 --> 02:17:46.940]   I love you.
[02:17:46.940 --> 02:17:47.660]   You're doing great.
[02:17:47.660 --> 02:17:48.580]   I'm happy about that.
[02:17:48.580 --> 02:17:51.820]   But why the hell did you move to Portland after all?
[02:17:51.820 --> 02:17:52.340]   How the cool news.
[02:17:52.340 --> 02:17:53.620]   So nice to have you on.
[02:17:53.620 --> 02:17:55.220]   It's pretty awesome in Portland.
[02:17:55.220 --> 02:17:56.380]   But don't tell anyone I said that.
[02:17:56.380 --> 02:17:58.220]   Great beer.
[02:17:58.220 --> 02:18:02.060]   This week in sciencetwists.org, broader impacts TV.
[02:18:02.060 --> 02:18:04.620]   If you're a scientist and you want to get some help,
[02:18:04.620 --> 02:18:07.020]   put your name and your brand and your ideas out there,
[02:18:07.020 --> 02:18:13.460]   broader impacts.tv/follower on the Twitter @DrDrKiki.
[02:18:13.460 --> 02:18:14.540]   Thank you so much, Leo.
[02:18:14.540 --> 02:18:15.420]   So nice to see you.
[02:18:15.420 --> 02:18:16.300]   You look great.
[02:18:16.300 --> 02:18:18.620]   I follow you on Facebook, so I don't feel like I'm not,
[02:18:18.620 --> 02:18:19.460]   you know, keeping up.
[02:18:19.460 --> 02:18:21.900]   But it's just nice to see you.
[02:18:21.900 --> 02:18:22.260]   Yeah.
[02:18:22.260 --> 02:18:23.820]   Moving.
[02:18:23.820 --> 02:18:29.220]   Dr. Tom Merritt, professor of tech newsology.
[02:18:29.220 --> 02:18:32.660]   He is at the DailyTechNewsShow.com at ACE Detective.
[02:18:32.660 --> 02:18:35.900]   He's never changed the worst Twitter handle in history.
[02:18:35.900 --> 02:18:35.900]   Yeah.
[02:18:35.900 --> 02:18:37.380]   He's detached.
[02:18:37.380 --> 02:18:40.300]   Every follower has to earn it.
[02:18:40.300 --> 02:18:44.860]   A-C-E-D-T-E-C-T.
[02:18:44.860 --> 02:18:46.780]   Yeah.
[02:18:46.780 --> 02:18:48.340]   Yeah, tomorrow on Daily Tech News Show,
[02:18:48.340 --> 02:18:50.620]   we're going to have Peter Newell on with Veronica Belmont
[02:18:50.620 --> 02:18:53.420]   talking about making the transition from the military
[02:18:53.420 --> 02:18:55.700]   to Silicon Valley and the connection between the two.
[02:18:55.700 --> 02:18:56.780]   It should be fun.
[02:18:56.780 --> 02:18:58.540]   Gosh, it was just like I was flashing back
[02:18:58.540 --> 02:19:01.140]   to the week ahead.
[02:19:01.140 --> 02:19:02.300]   Blackberry earnings are coming up.
[02:19:02.300 --> 02:19:03.140]   The end of the world.
[02:19:03.140 --> 02:19:04.020]   Oh, how exciting.
[02:19:04.020 --> 02:19:06.020]   Yeah.
[02:19:06.020 --> 02:19:09.260]   And, of course, Becky Worley, you'll find her on Good Morning
[02:19:09.260 --> 02:19:09.700]   America.
[02:19:09.700 --> 02:19:10.860]   She's their Consumer Reporter.
[02:19:10.860 --> 02:19:14.340]   You'll find her on Twitter, B-W-O-R-L-E-Y.
[02:19:14.340 --> 02:19:14.900]   Oh, it's great.
[02:19:14.900 --> 02:19:16.740]   And how's it going on that Lego?
[02:19:16.740 --> 02:19:18.420]   Oh, it's shit.
[02:19:18.420 --> 02:19:20.100]   It's great.
[02:19:20.100 --> 02:19:22.020]   All next week, what do you be covening?
[02:19:22.020 --> 02:19:22.700]   Let's see.
[02:19:22.700 --> 02:19:24.140]   I'm just going to go through it real quick.
[02:19:24.140 --> 02:19:26.140]   I got the buyer's guide to headphones.
[02:19:26.140 --> 02:19:28.620]   I got the buyer's guide to adult onesies.
[02:19:28.620 --> 02:19:29.460]   I got the buyer's guide.
[02:19:29.460 --> 02:19:31.060]   What's an adult onesie?
[02:19:31.060 --> 02:19:32.620]   I love adult onesies.
[02:19:32.620 --> 02:19:35.260]   Hey, would you get me my adult onesie off the--
[02:19:35.260 --> 02:19:36.740]   it's on my coat wrap.
[02:19:36.740 --> 02:19:37.860]   Oh, that's sexy.
[02:19:37.860 --> 02:19:38.740]   That's leopard skin.
[02:19:38.740 --> 02:19:42.860]   This is the number four searched item in fashion on Google.
[02:19:42.860 --> 02:19:44.220]   And so not only does it have--
[02:19:44.220 --> 02:19:44.820]   Does it have feet?
[02:19:44.820 --> 02:19:45.780]   It has feet?
[02:19:45.780 --> 02:19:47.620]   No, this one doesn't have feet.
[02:19:47.620 --> 02:19:49.060]   It has-- I don't know if it does--
[02:19:49.060 --> 02:19:50.220]   No, it has holes for your feet.
[02:19:50.220 --> 02:19:50.820]   Minus feet.
[02:19:50.820 --> 02:19:51.620]   Minus.
[02:19:51.620 --> 02:19:53.260]   Let me put mine on.
[02:19:53.260 --> 02:19:54.700]   It also has hands.
[02:19:54.700 --> 02:19:55.220]   Look at this.
[02:19:55.220 --> 02:19:56.620]   It has paws.
[02:19:56.620 --> 02:19:58.140]   Oh, it has paws, but no feet.
[02:19:58.140 --> 02:19:58.500]   Yeah.
[02:19:58.500 --> 02:20:00.220]   So I've got adult onesies.
[02:20:00.220 --> 02:20:01.220]   I want to see you and yours.
[02:20:01.220 --> 02:20:03.220]   I got SpongeBob Squarepants.
[02:20:03.220 --> 02:20:04.380]   Oh, nice.
[02:20:04.380 --> 02:20:05.580]   I've got an in-depth--
[02:20:05.580 --> 02:20:06.140]   I feel like it's out.
[02:20:06.140 --> 02:20:06.660]   Mine has feet.
[02:20:06.660 --> 02:20:07.580]   Yours has feet.
[02:20:07.580 --> 02:20:09.820]   I have an in-depth look at battery technology.
[02:20:09.820 --> 02:20:12.780]   Then cashmere sweaters follows naturally right after that.
[02:20:12.780 --> 02:20:13.220]   Nice.
[02:20:13.220 --> 02:20:14.940]   Batteries and cashmere.
[02:20:14.940 --> 02:20:15.380]   Yep.
[02:20:15.380 --> 02:20:15.940]   And then--
[02:20:15.940 --> 02:20:17.380]   Do you know how they make cashmere?
[02:20:17.380 --> 02:20:19.380]   Yes, it's from the underbelly of the goat.
[02:20:19.380 --> 02:20:20.580]   And you know how they gather it?
[02:20:20.580 --> 02:20:22.020]   I don't-- do I want to know?
[02:20:22.020 --> 02:20:22.940]   Tickle goats.
[02:20:22.940 --> 02:20:24.660]   As the goats wander through the little fields,
[02:20:24.660 --> 02:20:27.460]   there's these bushes that their fur gets stuck on.
[02:20:27.460 --> 02:20:30.100]   And then they go out and they harvest the cashmere from the bush.
[02:20:30.100 --> 02:20:30.620]   Really?
[02:20:30.620 --> 02:20:31.300]   Yes.
[02:20:31.300 --> 02:20:33.580]   Well, I learned that the best Italian firms
[02:20:33.580 --> 02:20:36.100]   have their own herds in Mongolia.
[02:20:36.100 --> 02:20:36.620]   Yeah.
[02:20:36.620 --> 02:20:39.220]   And they can't have them, you know, like in Italy,
[02:20:39.220 --> 02:20:40.620]   because they won't produce the same--
[02:20:40.620 --> 02:20:40.620]   Yeah.
[02:20:40.620 --> 02:20:42.580]   Even the same types of goats.
[02:20:42.580 --> 02:20:44.100]   I love cashmere.
[02:20:44.100 --> 02:20:44.620]   Me too.
[02:20:44.620 --> 02:20:45.140]   I do too.
[02:20:45.140 --> 02:20:47.260]   I'm wearing it right now.
[02:20:47.260 --> 02:20:48.180]   Are you wearing it right now?
[02:20:48.180 --> 02:20:48.700]   You know what I'm getting.
[02:20:48.700 --> 02:20:49.220]   I am.
[02:20:49.220 --> 02:20:52.740]   This is a cashmere hoodie, and it makes me so happy every day.
[02:20:52.740 --> 02:20:55.180]   Well, I have an L.I.O. in Portland, Oregon.
[02:20:55.180 --> 02:20:58.820]   Lisa, you need to get Leo, a cashmere adult onesie.
[02:20:58.820 --> 02:20:59.780]   Lisa hates cashmere.
[02:20:59.780 --> 02:21:02.500]   She's allergic to cashmere.
[02:21:02.500 --> 02:21:04.900]   Oh my god.
[02:21:04.900 --> 02:21:07.540]   Look, go with the spongebob.
[02:21:07.540 --> 02:21:09.460]   Oh my lord, I'm taking your IFTL.
[02:21:09.460 --> 02:21:11.580]   Who lives in a pineapple under the sea?
[02:21:11.580 --> 02:21:13.780]   Spongebob Leo.
[02:21:13.780 --> 02:21:14.700]   Come on, audience.
[02:21:14.700 --> 02:21:15.900]   Have a look with you.
[02:21:15.900 --> 02:21:17.940]   Who lives in a pineapple under the sea?
[02:21:17.940 --> 02:21:19.340]   Spongebob Squarepants.
[02:21:19.340 --> 02:21:20.340]   Spongebob Squarepants.
[02:21:20.340 --> 02:21:22.340]   [LAUGHTER]
[02:21:22.980 --> 02:21:24.380]   I'm going to hand my headphones plugged in.
[02:21:24.380 --> 02:21:24.740]   I don't know.
[02:21:24.740 --> 02:21:25.300]   Was that good?
[02:21:25.300 --> 02:21:26.380]   It was so good.
[02:21:26.380 --> 02:21:27.620]   Was it beautiful?
[02:21:27.620 --> 02:21:30.500]   I'm going to finish up this show in my adult onesie,
[02:21:30.500 --> 02:21:32.060]   just like you.
[02:21:32.060 --> 02:21:34.740]   And we're going to be in really good shape here.
[02:21:34.740 --> 02:21:36.020]   Sorry, I mean to unplug you.
[02:21:36.020 --> 02:21:37.540]   I just worried you were getting--
[02:21:37.540 --> 02:21:39.860]   No, it was impeding the--
[02:21:39.860 --> 02:21:40.820]   I know.
[02:21:40.820 --> 02:21:41.340]   There we go.
[02:21:41.340 --> 02:21:42.540]   That's all better.
[02:21:42.540 --> 02:21:43.100]   All right.
[02:21:43.100 --> 02:21:44.100]   You got a hood, though.
[02:21:44.100 --> 02:21:44.300]   I know.
[02:21:44.300 --> 02:21:45.300]   I wish I had a hood.
[02:21:45.300 --> 02:21:46.140]   Wow.
[02:21:46.140 --> 02:21:48.180]   [LAUGHTER]
[02:21:48.180 --> 02:21:48.900]   We do a twit.
[02:21:48.900 --> 02:21:50.140]   I don't know why, but we do.
[02:21:50.140 --> 02:21:53.940]   Every Sunday, afternoon, 3 p.m., 6 p.m., Eastern time.
[02:21:53.940 --> 02:21:55.780]   That would be 2300 UTC.
[02:21:55.780 --> 02:21:57.500]   For those of you watching around the world,
[02:21:57.500 --> 02:21:58.540]   I do hope you watch live.
[02:21:58.540 --> 02:22:03.860]   And join us in the chatroom at IRC.tv.
[02:22:03.860 --> 02:22:05.380]   Doesn't fit as well as it did last.
[02:22:05.380 --> 02:22:06.260]   It looks great.
[02:22:06.260 --> 02:22:07.660]   I think one's the shrink over time.
[02:22:07.660 --> 02:22:09.100]   That's what it is.
[02:22:09.100 --> 02:22:10.100]   Oh, boy.
[02:22:10.100 --> 02:22:12.100]   [LAUGHTER]
[02:22:12.100 --> 02:22:13.860]   That's a Portland hat.
[02:22:13.860 --> 02:22:14.340]   That is.
[02:22:14.340 --> 02:22:15.820]   That's a-- it is.
[02:22:15.820 --> 02:22:16.820]   Knit beard.
[02:22:16.820 --> 02:22:17.460]   Knitted beard.
[02:22:17.460 --> 02:22:18.460]   It's a great track.
[02:22:18.460 --> 02:22:19.380]   Who's a Portland?
[02:22:19.380 --> 02:22:20.860]   It's very attractive.
[02:22:20.860 --> 02:22:23.100]   You would fit in here very well.
[02:22:23.100 --> 02:22:25.100]   But you can't.
[02:22:25.100 --> 02:22:26.940]   You can't watch live.
[02:22:26.940 --> 02:22:28.380]   You can always watch after the fact
[02:22:28.380 --> 02:22:30.780]   on demandtrit.tv or wherever you get your podcasts,
[02:22:30.780 --> 02:22:34.500]   including, yes, the iTunes best of 2015 list.
[02:22:34.500 --> 02:22:35.500]   We're in the classics section.
[02:22:35.500 --> 02:22:36.260]   Oh, boy.
[02:22:36.260 --> 02:22:36.780]   But you know what?
[02:22:36.780 --> 02:22:38.460]   So is WTF with Mark Marin.
[02:22:38.460 --> 02:22:40.780]   So that's a good place to be.
[02:22:40.780 --> 02:22:42.100]   That is great company.
[02:22:42.100 --> 02:22:43.620]   Thanks for being here.
[02:22:43.620 --> 02:22:45.620]   We appreciate it.
[02:22:45.620 --> 02:22:46.580]   We'll see you next time.
[02:22:46.580 --> 02:22:48.580]   Another twit is in the onesie.
[02:22:48.580 --> 02:22:49.580]   Woo!
[02:22:49.580 --> 02:22:52.060]   [MUSIC PLAYING]
[02:22:52.060 --> 02:22:52.560]   Wow.
[02:22:52.560 --> 02:22:53.060]   You are the twit.
[02:22:53.060 --> 02:22:54.060]   Right.
[02:22:54.060 --> 02:22:56.060]   You are the twit, baby.
[02:22:56.060 --> 02:22:56.980]   You are the twit.
[02:22:56.980 --> 02:22:57.980]   All right.
[02:22:57.980 --> 02:22:59.820]   You are the twit, baby.
[02:22:59.820 --> 02:23:00.620]   You are the twit.
[02:23:00.620 --> 02:23:01.620]   All right.
[02:23:01.620 --> 02:23:03.620]   You are the twit, baby.

