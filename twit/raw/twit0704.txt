;FFMETADATA1
title=To the Woodshed With You!
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=704
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.500]   It's time for "Dwight This Week" in Tech.
[00:00:02.500 --> 00:00:04.640]   We've found three people who weren't watching the Super Bowl.
[00:00:04.640 --> 00:00:08.000]   Georgia Dow from Imore, Christina Warren from Microsoft,
[00:00:08.000 --> 00:00:09.760]   and DeGadgetes to vendor a hardware
[00:00:09.760 --> 00:00:12.320]   of lots to talk about, including tech companies
[00:00:12.320 --> 00:00:16.960]   past and present at the Super Bowl, Apple's FaceTime flaw,
[00:00:16.960 --> 00:00:20.560]   and their spanking of Facebook and Google,
[00:00:20.560 --> 00:00:25.560]   and the dangers of giving your DNA to the FBI.
[00:00:25.560 --> 00:00:28.000]   It's all coming up next on "Twit."
[00:00:28.000 --> 00:00:32.000]   Netcast you love.
[00:00:32.000 --> 00:00:34.000]   From people you trust.
[00:00:34.000 --> 00:00:39.000]   This is "Twit."
[00:00:39.000 --> 00:00:47.000]   This is "Twit."
[00:00:47.000 --> 00:00:49.000]   This week in Tech, Episode 704,
[00:00:49.000 --> 00:00:52.000]   recorded Sunday, February 3, 2019
[00:00:52.000 --> 00:00:55.000]   to the woodshed with you.
[00:00:55.000 --> 00:00:57.000]   This week in Tech is brought to you by
[00:00:57.000 --> 00:00:59.000]   "Thousand Eyes."
[00:00:59.000 --> 00:01:02.000]   Companies that run in the cloud rely on "Thousand Eyes."
[00:01:02.000 --> 00:01:05.000]   It's the place they go first to see, understand,
[00:01:05.000 --> 00:01:07.000]   and improve the digital experience
[00:01:07.000 --> 00:01:10.000]   of their cloud-based applications and services.
[00:01:10.000 --> 00:01:12.000]   Do the cloud write and improve services
[00:01:12.000 --> 00:01:14.000]   for your customers and employees today.
[00:01:14.000 --> 00:01:17.000]   Visit "thousandeyes.com/twit."
[00:01:17.000 --> 00:01:20.000]   And buy Sophos Cybersecurity.
[00:01:20.000 --> 00:01:22.000]   In an age of evolving cyber threats,
[00:01:22.000 --> 00:01:25.000]   you need evolved cyber security,
[00:01:25.000 --> 00:01:27.000]   powered by artificial intelligence.
[00:01:27.000 --> 00:01:30.000]   Sophos can detect threats before they strike
[00:01:30.000 --> 00:01:32.000]   killing ransomware, viruses,
[00:01:32.000 --> 00:01:34.000]   and other cyber threats dead in their tracks.
[00:01:34.000 --> 00:01:37.000]   Get a free security scan and/or a free trial today
[00:01:37.000 --> 00:01:40.000]   at Sophos.com.
[00:01:40.000 --> 00:01:41.000]   And buy Quip!
[00:01:41.000 --> 00:01:43.000]   Get a fresh start every day with Quip,
[00:01:43.000 --> 00:01:45.000]   the first subscription electric toothbrush
[00:01:45.000 --> 00:01:47.000]   accepted by the American Dental Association.
[00:01:47.000 --> 00:01:51.000]   Visit getquip.com/twit to get your first refill pack
[00:01:51.000 --> 00:01:54.000]   free when you purchase any Quip electric toothbrush.
[00:01:55.000 --> 00:01:57.000]   And buy LastPass,
[00:01:57.000 --> 00:02:00.000]   the number one most preferred password manager.
[00:02:00.000 --> 00:02:02.000]   Secure every password-protected entry point
[00:02:02.000 --> 00:02:04.000]   to your business and reduce the threat of breach
[00:02:04.000 --> 00:02:07.000]   at lastpass.com/twit.
[00:02:07.000 --> 00:02:11.000]   It's time for Twit!
[00:02:11.000 --> 00:02:14.000]   This week at Tech, the show we cover the latest Tech News.
[00:02:14.000 --> 00:02:17.000]   And I am so glad that on a Super Bowl Sunday
[00:02:17.000 --> 00:02:19.000]   we were able to get three nerds
[00:02:19.000 --> 00:02:21.000]   who don't care about sports at all.
[00:02:21.000 --> 00:02:23.000]   On the show, I hope that's you, David.
[00:02:23.000 --> 00:02:25.000]   I hope that's you, Devinder Hardow,
[00:02:25.000 --> 00:02:27.000]   are falling over saying, "I do, I do, I do care about sports."
[00:02:27.000 --> 00:02:28.000]   No, it's pretty much me.
[00:02:28.000 --> 00:02:30.000]   I don't really care about our movie trailers right now.
[00:02:30.000 --> 00:02:31.000]   This is today.
[00:02:31.000 --> 00:02:32.000]   Yeah, 'cause a slash film.
[00:02:32.000 --> 00:02:34.000]   Is this a big day for movie trailers?
[00:02:34.000 --> 00:02:35.000]   Oh, yeah.
[00:02:35.000 --> 00:02:37.000]   All the biggest movie trailers of the years
[00:02:37.000 --> 00:02:38.000]   drop on Super Bowl.
[00:02:38.000 --> 00:02:39.000]   Oh, I didn't know that.
[00:02:39.000 --> 00:02:40.000]   It's fun.
[00:02:40.000 --> 00:02:43.000]   But the movies that are coming in the next few months
[00:02:43.000 --> 00:02:45.000]   are the crappiest movies of the year.
[00:02:45.000 --> 00:02:47.000]   It's not a good, like, normally January,
[00:02:47.000 --> 00:02:50.000]   it's the bad month, but yeah, this is not a good period
[00:02:50.000 --> 00:02:51.000]   for big buddies.
[00:02:51.000 --> 00:02:53.000]   It's a battle-angel movie.
[00:02:53.000 --> 00:02:55.000]   And they showed trailers last summer
[00:02:55.000 --> 00:02:57.000]   and it looked terrible then, but now I know
[00:02:57.000 --> 00:03:00.000]   that they didn't release it during the Christmas
[00:03:00.000 --> 00:03:03.000]   prestige season that it actually is terrible.
[00:03:03.000 --> 00:03:05.000]   I actually hear it's pretty good, though.
[00:03:05.000 --> 00:03:06.000]   Surprisingly.
[00:03:06.000 --> 00:03:07.000]   See?
[00:03:07.000 --> 00:03:08.000]   There you go.
[00:03:08.000 --> 00:03:09.000]   It's by Robert Rodriguez.
[00:03:09.000 --> 00:03:10.000]   I like a movie.
[00:03:10.000 --> 00:03:12.000]   And it's based on a comic book, and of course,
[00:03:12.000 --> 00:03:14.000]   everybody knows, movies based on comic books
[00:03:14.000 --> 00:03:15.000]   are uniformly perfect.
[00:03:15.000 --> 00:03:17.000]   Well, it's based on an anime,
[00:03:17.000 --> 00:03:19.000]   which is even more dangerous, right?
[00:03:19.000 --> 00:03:21.000]   And I think it's a great story.
[00:03:21.000 --> 00:03:22.000]   I think it's a great story.
[00:03:22.000 --> 00:03:23.000]   I think it's a great story.
[00:03:23.000 --> 00:03:24.000]   I think it's a great story.
[00:03:24.000 --> 00:03:25.000]   I think it's a great story.
[00:03:25.000 --> 00:03:26.000]   I think it's a great story.
[00:03:26.000 --> 00:03:27.000]   I think it's a great story.
[00:03:27.000 --> 00:03:28.000]   I think it's a great story.
[00:03:28.000 --> 00:03:29.000]   I think it's a great story.
[00:03:29.000 --> 00:03:30.000]   I think it's a great story.
[00:03:30.000 --> 00:03:31.000]   I think it's a great story.
[00:03:31.000 --> 00:03:32.000]   I think it's a great story.
[00:03:32.000 --> 00:03:33.000]   I think it's a great story.
[00:03:33.000 --> 00:03:34.000]   I think it's a great story.
[00:03:34.000 --> 00:03:35.000]   I think it's a great story.
[00:03:35.000 --> 00:03:36.000]   I think it's a great story.
[00:03:36.000 --> 00:03:37.000]   I think it's a great story.
[00:03:37.000 --> 00:03:38.000]   I think it's a great story.
[00:03:38.000 --> 00:03:39.000]   I think it's a great story.
[00:03:39.000 --> 00:03:40.000]   I think it's a great story.
[00:03:40.000 --> 00:03:41.000]   I think it's a great story.
[00:03:41.000 --> 00:03:48.000]   I think it's a great story.
[00:03:48.000 --> 00:03:49.000]   I think it's a great story.
[00:03:49.000 --> 00:03:50.000]   I think it's a great story.
[00:03:50.000 --> 00:03:51.000]   I think it's a great story.
[00:03:51.000 --> 00:03:52.000]   I think it's a great story.
[00:03:52.000 --> 00:03:53.000]   I think it's a great story.
[00:03:53.000 --> 00:03:54.000]   I think it's a great story.
[00:03:54.000 --> 00:03:55.000]   I think it's a great story.
[00:03:55.000 --> 00:03:56.000]   I think it's a great story.
[00:03:56.000 --> 00:03:57.000]   I think it's a great story.
[00:03:57.000 --> 00:03:58.000]   I think it's a great story.
[00:03:58.000 --> 00:03:59.000]   I think it's a great story.
[00:03:59.000 --> 00:04:00.000]   I think it's a great story.
[00:04:00.000 --> 00:04:01.000]   I think it's a great story.
[00:04:01.000 --> 00:04:02.000]   I think it's a great story.
[00:04:02.000 --> 00:04:03.000]   I think it's a great story.
[00:04:03.000 --> 00:04:04.000]   I think it's a great story.
[00:04:04.000 --> 00:04:05.000]   I think it's a great story.
[00:04:05.000 --> 00:04:06.000]   I think it's a great story.
[00:04:06.000 --> 00:04:07.000]   I think it's a great story.
[00:04:07.000 --> 00:04:08.000]   I think it's a great story.
[00:04:08.000 --> 00:04:09.000]   I think it's a great story.
[00:04:09.000 --> 00:04:10.000]   I think it's a great story.
[00:04:10.000 --> 00:04:11.000]   I think it's a great story.
[00:04:11.000 --> 00:04:12.000]   I think it's a great story.
[00:04:12.000 --> 00:04:13.000]   I think it's a great story.
[00:04:13.000 --> 00:04:14.000]   I think it's a great story.
[00:04:14.000 --> 00:04:15.000]   I think it's a great story.
[00:04:15.000 --> 00:04:16.000]   I think it's a great story.
[00:04:16.000 --> 00:04:17.000]   I think it's a great story.
[00:04:17.000 --> 00:04:18.000]   I think it's a great story.
[00:04:18.000 --> 00:04:19.000]   I think it's a great story.
[00:04:19.000 --> 00:04:20.000]   I think it's a great story.
[00:04:20.000 --> 00:04:21.000]   I think it's a great story.
[00:04:21.000 --> 00:04:22.000]   I think it's a great story.
[00:04:22.000 --> 00:04:23.000]   I think it's a great story.
[00:04:23.000 --> 00:04:24.000]   I think it's a great story.
[00:04:24.000 --> 00:04:25.000]   I think it's a great story.
[00:04:25.000 --> 00:04:26.000]   I think it's a great story.
[00:04:26.000 --> 00:04:27.000]   I think it's a great story.
[00:04:27.000 --> 00:04:28.000]   I think it's a great story.
[00:04:28.000 --> 00:04:29.000]   I think it's a great story.
[00:04:29.000 --> 00:04:30.000]   I think it's a great story.
[00:04:30.000 --> 00:04:31.000]   I think it's a great story.
[00:04:31.000 --> 00:04:32.000]   I think it's a great story.
[00:04:32.000 --> 00:04:33.000]   I think it's a great story.
[00:04:33.000 --> 00:04:34.000]   I think it's a great story.
[00:04:34.000 --> 00:04:35.000]   I think it's a great story.
[00:04:35.000 --> 00:04:36.000]   I think it's a great story.
[00:04:36.000 --> 00:04:37.000]   I think it's a great story.
[00:04:37.000 --> 00:04:38.000]   I think it's a great story.
[00:04:38.000 --> 00:04:39.000]   I think it's a great story.
[00:04:39.000 --> 00:04:40.000]   I think it's a great story.
[00:04:40.000 --> 00:04:41.000]   I think it's a great story.
[00:04:41.000 --> 00:04:42.000]   I think it's a great story.
[00:04:42.000 --> 00:04:43.000]   I think it's a great story.
[00:04:43.000 --> 00:04:44.000]   I think it's a great story.
[00:04:44.000 --> 00:04:45.000]   I think it's a great story.
[00:04:45.000 --> 00:04:46.000]   I think it's a great story.
[00:04:46.000 --> 00:04:47.000]   I think it's a great story.
[00:04:47.000 --> 00:04:48.000]   I think it's a great story.
[00:04:48.000 --> 00:04:49.000]   I think it's a great story.
[00:04:49.000 --> 00:04:50.000]   I think it's a great story.
[00:04:50.000 --> 00:04:51.000]   I think it's a great story.
[00:04:51.000 --> 00:04:52.000]   I think it's a great story.
[00:04:52.000 --> 00:04:53.000]   I think it's a great story.
[00:04:53.000 --> 00:04:54.000]   I think it's a great story.
[00:04:54.000 --> 00:04:55.000]   I think it's a great story.
[00:04:55.000 --> 00:04:56.000]   I think it's a great story.
[00:04:56.000 --> 00:04:57.000]   I think it's a great story.
[00:04:57.000 --> 00:04:58.000]   I think it's a great story.
[00:04:58.000 --> 00:04:59.000]   I think it's a great story.
[00:04:59.000 --> 00:05:00.000]   I think it's a great story.
[00:05:00.000 --> 00:05:01.000]   I think it's a great story.
[00:05:01.000 --> 00:05:02.000]   I think it's a great story.
[00:05:02.000 --> 00:05:03.000]   I think it's a great story.
[00:05:03.000 --> 00:05:04.000]   I think it's a great story.
[00:05:04.000 --> 00:05:05.000]   I think it's a great story.
[00:05:05.000 --> 00:05:06.000]   I think it's a great story.
[00:05:06.000 --> 00:05:07.000]   I think it's a great story.
[00:05:07.000 --> 00:05:08.000]   I think it's a great story.
[00:05:08.000 --> 00:05:09.000]   I think it's a great story.
[00:05:09.000 --> 00:05:10.000]   I think it's a great story.
[00:05:10.000 --> 00:05:11.000]   I think it's a great story.
[00:05:11.000 --> 00:05:12.000]   I think it's a great story.
[00:05:12.000 --> 00:05:13.000]   I think it's a great story.
[00:05:13.000 --> 00:05:14.000]   I think it's a great story.
[00:05:14.000 --> 00:05:15.000]   I think it's a great story.
[00:05:15.000 --> 00:05:16.000]   I think it's a great story.
[00:05:16.000 --> 00:05:17.000]   I think it's a great story.
[00:05:17.000 --> 00:05:18.000]   I think it's a great story.
[00:05:18.000 --> 00:05:19.000]   I think it's a great story.
[00:05:19.000 --> 00:05:20.000]   I think it's a great story.
[00:05:20.000 --> 00:05:21.000]   I think it's a great story.
[00:05:21.000 --> 00:05:22.000]   I think it's a great story.
[00:05:22.000 --> 00:05:23.000]   I think it's a great story.
[00:05:23.000 --> 00:05:24.000]   I think it's a great story.
[00:05:24.000 --> 00:05:25.000]   I think it's a great story.
[00:05:25.000 --> 00:05:26.000]   I think it's a great story.
[00:05:26.000 --> 00:05:27.000]   I think it's a great story.
[00:05:27.000 --> 00:05:28.000]   I think it's a great story.
[00:05:28.000 --> 00:05:29.000]   I think it's a great story.
[00:05:29.000 --> 00:05:30.000]   I think it's a great story.
[00:05:30.000 --> 00:05:31.000]   I think it's a great story.
[00:05:31.000 --> 00:05:32.000]   I think it's a great story.
[00:05:32.000 --> 00:05:33.000]   I think it's a great story.
[00:05:33.000 --> 00:05:34.000]   I think it's a great story.
[00:05:34.000 --> 00:05:35.000]   I think it's a great story.
[00:05:35.000 --> 00:05:36.000]   I think it's a great story.
[00:05:36.000 --> 00:05:37.000]   I think it's a great story.
[00:05:37.000 --> 00:05:38.000]   I think it's a great story.
[00:05:38.000 --> 00:05:39.000]   I think it's a great story.
[00:05:39.000 --> 00:05:40.000]   I think it's a great story.
[00:05:40.000 --> 00:05:41.000]   I think it's a great story.
[00:05:41.000 --> 00:05:42.000]   I think it's a great story.
[00:05:42.000 --> 00:05:43.000]   I think it's a great story.
[00:05:43.000 --> 00:05:44.000]   I think it's a great story.
[00:05:44.000 --> 00:05:45.000]   I think it's a great story.
[00:05:45.000 --> 00:05:46.000]   I think it's a great story.
[00:05:46.000 --> 00:05:47.000]   I think it's a great story.
[00:05:47.000 --> 00:05:48.000]   I think it's a great story.
[00:05:48.000 --> 00:05:49.000]   I think it's a great story.
[00:05:49.000 --> 00:05:50.000]   I think it's a great story.
[00:05:50.000 --> 00:05:51.000]   I think it's a great story.
[00:05:51.000 --> 00:05:52.000]   I think it's a great story.
[00:05:52.000 --> 00:05:53.000]   I think it's a great story.
[00:05:53.000 --> 00:05:54.000]   I think it's a great story.
[00:05:54.000 --> 00:05:55.000]   I think it's a great story.
[00:05:55.000 --> 00:05:56.000]   I think it's a great story.
[00:05:56.000 --> 00:05:57.000]   I think it's a great story.
[00:05:57.000 --> 00:05:58.000]   I think it's a great story.
[00:05:58.000 --> 00:05:59.000]   I think it's a great story.
[00:05:59.000 --> 00:06:00.000]   I think it's a great story.
[00:06:00.000 --> 00:06:01.000]   I think it's a great story.
[00:06:01.000 --> 00:06:02.000]   I think it's a great story.
[00:06:02.000 --> 00:06:03.000]   I think it's a great story.
[00:06:03.000 --> 00:06:04.000]   I think it's a great story.
[00:06:04.000 --> 00:06:05.000]   I think it's a great story.
[00:06:05.000 --> 00:06:06.000]   I think it's a great story.
[00:06:06.000 --> 00:06:07.000]   I think it's a great story.
[00:06:07.000 --> 00:06:08.000]   I think it's a great story.
[00:06:08.000 --> 00:06:09.000]   I think it's a great story.
[00:06:09.000 --> 00:06:10.000]   I think it's a great story.
[00:06:10.000 --> 00:06:11.000]   I think it's a great story.
[00:06:11.000 --> 00:06:12.000]   I think it's a great story.
[00:06:12.000 --> 00:06:13.000]   I think it's a great story.
[00:06:13.000 --> 00:06:14.000]   I think it's a great story.
[00:06:14.000 --> 00:06:15.000]   I think it's a great story.
[00:06:15.000 --> 00:06:16.000]   I think it's a great story.
[00:06:16.000 --> 00:06:17.000]   I think it's a great story.
[00:06:17.000 --> 00:06:18.000]   I think it's a great story.
[00:06:18.000 --> 00:06:19.000]   I think it's a great story.
[00:06:19.000 --> 00:06:20.000]   I think it's a great story.
[00:06:20.000 --> 00:06:21.000]   I think it's a great story.
[00:06:21.000 --> 00:06:22.000]   I think it's a great story.
[00:06:22.000 --> 00:06:23.000]   I think it's a great story.
[00:06:23.000 --> 00:06:24.000]   I think it's a great story.
[00:06:24.000 --> 00:06:25.000]   I think it's a great story.
[00:06:25.000 --> 00:06:26.000]   I think it's a great story.
[00:06:26.000 --> 00:06:27.000]   I think it's a great story.
[00:06:27.000 --> 00:06:28.000]   I think it's a great story.
[00:06:28.000 --> 00:06:29.000]   I think it's a great story.
[00:06:29.000 --> 00:06:30.000]   I think it's a great story.
[00:06:30.000 --> 00:06:31.000]   I think it's a great story.
[00:06:31.000 --> 00:06:32.000]   I think it's a great story.
[00:06:32.000 --> 00:06:33.000]   I think it's a great story.
[00:06:33.000 --> 00:06:34.000]   I think it's a great story.
[00:06:34.000 --> 00:06:35.000]   I think it's a great story.
[00:06:35.000 --> 00:06:36.000]   I think it's a great story.
[00:06:36.000 --> 00:06:37.000]   I think it's a great story.
[00:06:37.000 --> 00:06:38.000]   I think it's a great story.
[00:06:38.000 --> 00:06:39.000]   I think it's a great story.
[00:06:39.000 --> 00:06:40.000]   I think it's a great story.
[00:06:40.000 --> 00:06:41.000]   I think it's a great story.
[00:06:41.000 --> 00:06:42.000]   I think it's a great story.
[00:06:42.000 --> 00:06:43.000]   I think it's a great story.
[00:06:43.000 --> 00:06:44.000]   I think it's a great story.
[00:06:44.000 --> 00:06:45.000]   I think it's a great story.
[00:06:45.000 --> 00:06:46.000]   I think it's a great story.
[00:06:46.000 --> 00:06:47.000]   I think it's a great story.
[00:06:47.000 --> 00:06:48.000]   I think it's a great story.
[00:06:48.000 --> 00:06:49.000]   I think it's a great story.
[00:06:49.000 --> 00:06:50.000]   I think it's a great story.
[00:06:50.000 --> 00:06:51.000]   I think it's a great story.
[00:06:51.000 --> 00:06:52.000]   I think it's a great story.
[00:06:52.000 --> 00:06:53.000]   I think it's a great story.
[00:06:53.000 --> 00:06:54.000]   I think it's a great story.
[00:06:54.000 --> 00:06:55.000]   I think it's a great story.
[00:06:55.000 --> 00:06:56.000]   I think it's a great story.
[00:06:56.000 --> 00:06:57.000]   I think it's a great story.
[00:06:57.000 --> 00:06:58.000]   I think it's a great story.
[00:06:58.000 --> 00:06:59.000]   I think it's a great story.
[00:06:59.000 --> 00:07:00.000]   I think it's a great story.
[00:07:00.000 --> 00:07:01.000]   I think it's a great story.
[00:07:01.000 --> 00:07:02.000]   I think it's a great story.
[00:07:02.000 --> 00:07:03.000]   I think it's a great story.
[00:07:03.000 --> 00:07:04.000]   I think it's a great story.
[00:07:04.000 --> 00:07:05.000]   I think it's a great story.
[00:07:05.000 --> 00:07:06.000]   I think it's a great story.
[00:07:06.000 --> 00:07:07.000]   I think it's a great story.
[00:07:07.000 --> 00:07:08.000]   I think it's a great story.
[00:07:08.000 --> 00:07:09.000]   I think it's a great story.
[00:07:09.000 --> 00:07:10.000]   I think it's a great story.
[00:07:10.000 --> 00:07:11.000]   I think it's a great story.
[00:07:11.000 --> 00:07:12.000]   I think it's a great story.
[00:07:12.000 --> 00:07:13.000]   I think it's a great story.
[00:07:13.000 --> 00:07:14.000]   I think it's a great story.
[00:07:14.000 --> 00:07:15.000]   I think it's a great story.
[00:07:15.000 --> 00:07:16.000]   I think it's a great story.
[00:07:16.000 --> 00:07:17.000]   I think it's a great story.
[00:07:17.000 --> 00:07:18.000]   I think it's a great story.
[00:07:18.000 --> 00:07:19.000]   I think it's a great story.
[00:07:19.000 --> 00:07:20.000]   I think it's a great story.
[00:07:20.000 --> 00:07:21.000]   I think it's a great story.
[00:07:21.000 --> 00:07:22.000]   I think it's a great story.
[00:07:22.000 --> 00:07:23.000]   I think it's a great story.
[00:07:23.000 --> 00:07:24.000]   I think it's a great story.
[00:07:24.000 --> 00:07:25.000]   I think it's a great story.
[00:07:25.000 --> 00:07:26.000]   I think it's a great story.
[00:07:26.000 --> 00:07:27.000]   I think it's a great story.
[00:07:27.000 --> 00:07:28.000]   I think it's a great story.
[00:07:28.000 --> 00:07:29.000]   I think it's a great story.
[00:07:29.000 --> 00:07:30.000]   I think it's a great story.
[00:07:30.000 --> 00:07:31.000]   I think it's a great story.
[00:07:31.000 --> 00:07:32.000]   I think it's a great story.
[00:07:32.000 --> 00:07:33.000]   I think it's a great story.
[00:07:33.000 --> 00:07:34.000]   I think it's a great story.
[00:07:34.000 --> 00:07:35.000]   I think it's a great story.
[00:07:35.000 --> 00:07:36.000]   I think it's a great story.
[00:07:36.000 --> 00:07:37.000]   I think it's a great story.
[00:07:37.000 --> 00:07:38.000]   I think it's a great story.
[00:07:38.000 --> 00:07:39.000]   I think it's a great story.
[00:07:39.000 --> 00:07:40.000]   I think it's a great story.
[00:07:40.000 --> 00:07:41.000]   I think it's a great story.
[00:07:41.000 --> 00:07:42.000]   I think it's a great story.
[00:07:42.000 --> 00:07:43.000]   I think it's a great story.
[00:07:43.000 --> 00:07:44.000]   I think it's a great story.
[00:07:44.000 --> 00:07:45.000]   I think it's a great story.
[00:07:45.000 --> 00:07:46.000]   I think it's a great story.
[00:07:46.000 --> 00:07:47.000]   I think it's a great story.
[00:07:47.000 --> 00:07:48.000]   I think it's a great story.
[00:07:48.000 --> 00:07:49.000]   I think it's a great story.
[00:07:49.000 --> 00:07:54.000]   Harrison Ford, Forrest Whitaker, Elana Glazier.
[00:07:54.000 --> 00:07:55.000]   I don't know who that is.
[00:07:55.000 --> 00:07:56.000]   And Abby Jacobson.
[00:07:56.000 --> 00:07:57.000]   Oh, they're from Broad City.
[00:07:57.000 --> 00:07:58.000]   Yeah.
[00:07:58.000 --> 00:07:59.000]   Oh, that's nice.
[00:07:59.000 --> 00:08:00.000]   That's good.
[00:08:00.000 --> 00:08:03.000]   That's almost like a podcast.
[00:08:03.000 --> 00:08:05.000]   And astronaut twins Scott and Mark Kelly.
[00:08:05.000 --> 00:08:08.000]   They'll all be in the echo ads.
[00:08:08.000 --> 00:08:12.000]   ADT, Google will have ads again.
[00:08:12.000 --> 00:08:14.000]   Two commercials.
[00:08:14.000 --> 00:08:17.000]   Your company Microsoft, they're going to take --
[00:08:17.000 --> 00:08:20.000]   It looks like they're going to take that holiday ad and do a sequel to that.
[00:08:20.000 --> 00:08:21.000]   Yeah, they did.
[00:08:21.000 --> 00:08:23.000]   They released the extended version a couple of days ago.
[00:08:23.000 --> 00:08:27.000]   And I'm not -- I'm going to say this as unbiased as I possibly can.
[00:08:27.000 --> 00:08:29.000]   But I was -- I was -- I was taught when I was in the lawn.
[00:08:29.000 --> 00:08:31.000]   I was like, this is a good ad.
[00:08:31.000 --> 00:08:32.000]   This is really not a great thing.
[00:08:32.000 --> 00:08:34.000]   I loved the Christmas ad.
[00:08:34.000 --> 00:08:35.000]   I loved it.
[00:08:35.000 --> 00:08:43.000]   It was a beautiful ad about a kid -- a special needs kid who had this special accessible controller
[00:08:43.000 --> 00:08:45.000]   for the Xbox and all his friends.
[00:08:45.000 --> 00:08:46.000]   He's going to do it.
[00:08:46.000 --> 00:08:47.000]   He's going to do it.
[00:08:47.000 --> 00:08:50.000]   They were all running over and they saw him whatever it is, break his high score or whatever.
[00:08:50.000 --> 00:08:51.000]   And they were all cheering for him.
[00:08:51.000 --> 00:08:52.000]   It was a great ad.
[00:08:52.000 --> 00:08:54.000]   So we're going to see the -- I guess the sequel to that?
[00:08:54.000 --> 00:08:55.000]   Yeah.
[00:08:55.000 --> 00:09:00.000]   They -- he's visited again and there's some other kids that show off the accessible controller.
[00:09:00.000 --> 00:09:01.000]   Okay.
[00:09:01.000 --> 00:09:04.000]   Mint Mobile will do an ad.
[00:09:04.000 --> 00:09:06.000]   That's one of those MVNOs.
[00:09:06.000 --> 00:09:07.000]   Simply safe.
[00:09:07.000 --> 00:09:10.000]   I should mention is a sponsor of our content.
[00:09:10.000 --> 00:09:11.000]   Team Mobile.
[00:09:11.000 --> 00:09:12.000]   Verizon.
[00:09:12.000 --> 00:09:18.000]   So look for -- I'm not going to show any of these because I want you to enjoy your Super Bowl
[00:09:18.000 --> 00:09:20.000]   free of knowledge.
[00:09:20.000 --> 00:09:21.000]   You're almost --
[00:09:21.000 --> 00:09:22.000]   No ad.
[00:09:22.000 --> 00:09:23.000]   No ad.
[00:09:23.000 --> 00:09:24.000]   You know Tevo.
[00:09:24.000 --> 00:09:25.000]   I didn't know this.
[00:09:25.000 --> 00:09:27.000]   They've done this a couple of years in a row now.
[00:09:27.000 --> 00:09:30.000]   Tevo has a game skip button now.
[00:09:30.000 --> 00:09:31.000]   So --
[00:09:31.000 --> 00:09:32.000]   I love it.
[00:09:32.000 --> 00:09:35.000]   So I love this idea.
[00:09:35.000 --> 00:09:41.200]   You can -- if you're watching the Super Bowl for the ads, not the game, you Tevo it and then
[00:09:41.200 --> 00:09:44.320]   which I am doing and then you can just go watch the ads.
[00:09:44.320 --> 00:09:46.200]   I like that idea.
[00:09:46.200 --> 00:09:48.000]   I'll be you tubing later.
[00:09:48.000 --> 00:09:50.200]   Just want to watch the worst and the best.
[00:09:50.200 --> 00:09:51.200]   That's it.
[00:09:51.200 --> 00:09:55.120]   I like to watch him in -- in -- in Vivo.
[00:09:55.120 --> 00:09:56.360]   I like to watch -- Do you?
[00:09:56.360 --> 00:09:57.360]   Yeah.
[00:09:57.360 --> 00:09:59.720]   I like to watch him in the ambiance.
[00:09:59.720 --> 00:10:04.720]   In fact, I'd like to be going to get a sandwich and see the ad out of the corner of my eye.
[00:10:04.720 --> 00:10:11.920]   So then I really know what real people are seeing of this $5.5 million production.
[00:10:11.920 --> 00:10:12.920]   Was that Jeff Bridges?
[00:10:12.920 --> 00:10:14.280]   I thought that -- I think that was Jeff Bridges.
[00:10:14.280 --> 00:10:15.960]   And that's all I know.
[00:10:15.960 --> 00:10:18.520]   Anyway, so we will -- that's all we're going to talk about the Super Bowl.
[00:10:18.520 --> 00:10:19.520]   It's over.
[00:10:19.520 --> 00:10:20.520]   It's done.
[00:10:20.520 --> 00:10:21.520]   Somebody won.
[00:10:21.520 --> 00:10:23.080]   Somebody lost.
[00:10:23.080 --> 00:10:29.200]   There were probably bad calls and there was probably a wardrobe malfunction.
[00:10:29.200 --> 00:10:31.360]   Maroon 5.
[00:10:31.360 --> 00:10:32.360]   Their clothes fell off.
[00:10:32.360 --> 00:10:33.360]   I don't know.
[00:10:33.360 --> 00:10:34.360]   I'm just saying.
[00:10:34.360 --> 00:10:36.600]   First quarter results are in for Apple.
[00:10:36.600 --> 00:10:39.520]   This was going to be Apple's no good, very bad quarter.
[00:10:39.520 --> 00:10:45.160]   Tim Cook, remember a couple of weeks ago, downgraded their estimates for profit.
[00:10:45.160 --> 00:10:46.160]   You know what?
[00:10:46.160 --> 00:10:47.480]   They didn't have such a bad quarter.
[00:10:47.480 --> 00:10:49.200]   They did all right, right?
[00:10:49.200 --> 00:10:52.120]   Any takeaways from this to Vidron Hardin?
[00:10:52.120 --> 00:10:53.120]   $84 billion is okay.
[00:10:53.120 --> 00:10:54.120]   It's okay.
[00:10:54.120 --> 00:10:55.120]   It's okay.
[00:10:55.120 --> 00:10:56.120]   It's a lot of money.
[00:10:56.120 --> 00:10:57.120]   They're going to survive.
[00:10:57.120 --> 00:10:58.120]   I think they're going to survive.
[00:10:58.120 --> 00:10:59.120]   They'll be fine.
[00:10:59.120 --> 00:11:00.120]   Maybe.
[00:11:00.120 --> 00:11:01.120]   I'm just calling that.
[00:11:01.120 --> 00:11:02.120]   They're fine.
[00:11:02.120 --> 00:11:03.120]   They're fine.
[00:11:03.120 --> 00:11:05.120]   I've international sales up 62%.
[00:11:05.120 --> 00:11:06.120]   I guess that's okay.
[00:11:06.120 --> 00:11:14.240]   Now, this is the first one where they're not breaking down the individual sales for products,
[00:11:14.240 --> 00:11:15.880]   especially for the iPhones, right?
[00:11:15.880 --> 00:11:16.880]   Right.
[00:11:16.880 --> 00:11:22.040]   This is the first time that they have not the same number of units of iPhones sold, which
[00:11:22.040 --> 00:11:23.200]   I don't love that move.
[00:11:23.200 --> 00:11:25.000]   I get it, but I don't love it.
[00:11:25.000 --> 00:11:27.600]   I don't love it when any company does that.
[00:11:27.600 --> 00:11:28.600]   Yeah.
[00:11:28.600 --> 00:11:29.600]   It is what it is.
[00:11:29.600 --> 00:11:31.840]   It's clear you're hiding something, right?
[00:11:31.840 --> 00:11:34.240]   What shame is being hidden right now?
[00:11:34.240 --> 00:11:37.400]   Is it lower iPhone 10R sales?
[00:11:37.400 --> 00:11:38.640]   We don't know at this point.
[00:11:38.640 --> 00:11:39.640]   Yeah.
[00:11:39.640 --> 00:11:42.360]   Well, I mean, in fairness, they never actually broke out by the model series.
[00:11:42.360 --> 00:11:45.480]   They would never say you sold this many models of this or this or this.
[00:11:45.480 --> 00:11:52.400]   So usually what would happen is the mix would be extrapolated based on the margin on the
[00:11:52.400 --> 00:11:53.400]   devices.
[00:11:53.400 --> 00:11:57.360]   That's how you would figure out, well, they sold more of the newer devices than the older
[00:11:57.360 --> 00:11:58.360]   ones.
[00:11:58.360 --> 00:12:01.120]   But the fact that they're not breaking it out at all.
[00:12:01.120 --> 00:12:08.480]   I think what it indicates as a lot of people expected and as their own pre-quarter, I guess,
[00:12:08.480 --> 00:12:12.200]   declaration said is that they're selling fewer phones than they used to.
[00:12:12.200 --> 00:12:18.200]   Now, I think that's okay just because smartphones, we've reached a point where we're at saturation
[00:12:18.200 --> 00:12:22.960]   and the growth period is basically gone.
[00:12:22.960 --> 00:12:29.200]   So I understand that if you think the market is going to be basing its valuation of your
[00:12:29.200 --> 00:12:36.760]   company on certain growth percentages in sales and you start to see a decline, not because
[00:12:36.760 --> 00:12:40.560]   your product isn't so popular, but because everyone has a phone and because ownership
[00:12:40.560 --> 00:12:45.840]   cycles are becoming longer and longer, then I do understand the idea of saying, well, we
[00:12:45.840 --> 00:12:49.520]   would rather not break out these numbers.
[00:12:49.520 --> 00:12:53.600]   But I think that when people were trying to say, well, that's not what it's about.
[00:12:53.600 --> 00:12:59.040]   And then they have to lower their earnings estimates because things did not sell the
[00:12:59.040 --> 00:13:04.200]   way they were anticipating for whether it was China or pricing or what the reason may be,
[00:13:04.200 --> 00:13:10.240]   that just adds credence to the idea that, well, this is a convenient time to maybe not paper
[00:13:10.240 --> 00:13:15.640]   over the problem, but as you were saying, kind of hide something or make it less obvious
[00:13:15.640 --> 00:13:16.640]   what's happening.
[00:13:16.640 --> 00:13:17.640]   Yeah.
[00:13:17.640 --> 00:13:19.080]   I mean, there's multiple things going on too.
[00:13:19.080 --> 00:13:22.160]   Like as you were saying, phones are lasting longer and they're getting better.
[00:13:22.160 --> 00:13:27.080]   And we also know Apple's been trying to push up that average sales price with the 10s
[00:13:27.080 --> 00:13:28.800]   and the plus and everything.
[00:13:28.800 --> 00:13:32.120]   So I think that there may be some pushback to that as well.
[00:13:32.120 --> 00:13:33.520]   Things are getting so expensive.
[00:13:33.520 --> 00:13:36.920]   I still have my 10 from last year and I felt no real need to upgrade.
[00:13:36.920 --> 00:13:41.480]   I'm also seeing people go from the 10 last year to the 10R this year, which in many ways
[00:13:41.480 --> 00:13:43.400]   is to step down other than screen size.
[00:13:43.400 --> 00:13:46.280]   So there's a lot of confusion in the product line too, I think.
[00:13:46.280 --> 00:13:51.160]   So maybe this year with a full fledged redesign, we'll see something better.
[00:13:51.160 --> 00:13:55.480]   Cook was asked specifically about the theory that one of the reasons iPhone sales were
[00:13:55.480 --> 00:13:58.160]   down 15% this quarter.
[00:13:58.160 --> 00:14:00.760]   By the way, this is the quarter that you would see a big jump.
[00:14:00.760 --> 00:14:02.600]   This is their big quarter, right?
[00:14:02.600 --> 00:14:03.600]   Right.
[00:14:03.600 --> 00:14:05.800]   This is the holiday quarter.
[00:14:05.800 --> 00:14:11.680]   He was asked specifically is it prices and Cook said no, but his answer was kind of odd.
[00:14:11.680 --> 00:14:16.280]   He said the iPhone 10s match the price of the iPhone 10, the 10R was right in the middle
[00:14:16.280 --> 00:14:20.240]   of where the entry iPhone 8 and the entry iPhone plus had been priced.
[00:14:20.240 --> 00:14:25.120]   So actually it's a pretty small difference in the United States compared to last year.
[00:14:25.120 --> 00:14:29.800]   But actually, if you look at any individual phone, it's definitely more expensive.
[00:14:29.800 --> 00:14:32.240]   Nobody buys the average price phone.
[00:14:32.240 --> 00:14:34.280]   They buy a phone.
[00:14:34.280 --> 00:14:39.400]   And even the 10R was more expensive than the base model 8, which was itself more expensive
[00:14:39.400 --> 00:14:40.400]   than the iPhone 7.
[00:14:40.400 --> 00:14:46.640]   So Apple has looked at, they've been ratcheting the price up, but Cook seems at least in public
[00:14:46.640 --> 00:14:51.000]   to be fairly clear that he doesn't think that the drop in numbers is due to that.
[00:14:51.000 --> 00:14:56.120]   He does say the price was high in areas where the exchange rate wasn't so good like China.
[00:14:56.120 --> 00:14:58.240]   They also look a mystery.
[00:14:58.240 --> 00:15:02.080]   Their CFO highlighted the installed base.
[00:15:02.080 --> 00:15:05.080]   And this is something I think you're going to hear more and more from Apple because what
[00:15:05.080 --> 00:15:09.600]   Cook said was, well, you know, we make our phones to last a long time.
[00:15:09.600 --> 00:15:13.720]   So what that means is when somebody buys a new phone, they hand down the old phone.
[00:15:13.720 --> 00:15:17.000]   And as a result, the installed base is high.
[00:15:17.000 --> 00:15:24.000]   And for the first time, they gave a number, 900 million actively used iPhones up year over
[00:15:24.000 --> 00:15:26.240]   year in all five geographic segments.
[00:15:26.240 --> 00:15:30.880]   In fact, they say it grew almost 75 million in the last 12 months.
[00:15:30.880 --> 00:15:34.880]   This is very typical of a company, which is we're going to highlight the stat that makes
[00:15:34.880 --> 00:15:35.880]   us look good.
[00:15:35.880 --> 00:15:36.880]   Right.
[00:15:36.880 --> 00:15:40.880]   And here what this also indicates and what they would kind of push towards us to say,
[00:15:40.880 --> 00:15:42.840]   okay, this is saying two things.
[00:15:42.840 --> 00:15:45.320]   One, look at this huge number that makes us look good.
[00:15:45.320 --> 00:15:48.120]   Number two, the big thing that Apple has been focusing on.
[00:15:48.120 --> 00:15:50.840]   I guess I'd say about the last 12 quarters.
[00:15:50.840 --> 00:15:52.800]   It's really been about the last three years.
[00:15:52.800 --> 00:15:57.520]   They've been doing this massive push into services is that if you're looking at services
[00:15:57.520 --> 00:16:02.400]   and they've said, you know, it's going to be, you know, a $20 billion, you know, a business
[00:16:02.400 --> 00:16:10.720]   on its own or, you know, buy this date and whatnot, then that is going to you can say,
[00:16:10.720 --> 00:16:16.800]   well, if we have 958 million, you know, active install base, then you can start to extrapolate
[00:16:16.800 --> 00:16:20.360]   the potential of service revenue based on that number.
[00:16:20.360 --> 00:16:23.400]   And if that number continues to go up, then that's an indication that service revenue
[00:16:23.400 --> 00:16:28.000]   will continue to climb and will continue to grow because you have that install base where
[00:16:28.000 --> 00:16:33.160]   you can attach Apple music or iCloud or, you know, Apple Pay or whatever else you're
[00:16:33.160 --> 00:16:36.880]   wanting to kind of, you know, put on it.
[00:16:36.880 --> 00:16:39.200]   You have that user base to go after.
[00:16:39.200 --> 00:16:43.080]   So I think that it's both the big number that says, oh, this, look at how great this big
[00:16:43.080 --> 00:16:44.080]   number is.
[00:16:44.080 --> 00:16:48.480]   And also, it's also kind of an indicator to analysts to say, and this is this potential
[00:16:48.480 --> 00:16:54.760]   huge target number that we have to get additional money out of because they're in our ecosystem.
[00:16:54.760 --> 00:17:01.080]   And as a result, you know, we have the ability to attach, you know, X percentage of services
[00:17:01.080 --> 00:17:02.880]   onto the number of people.
[00:17:02.880 --> 00:17:06.840]   It's also why Apple goes after privacy because I think that as more and more people are
[00:17:06.840 --> 00:17:10.760]   caring about privacy, they're going to be heading down to Apple because Apple just has
[00:17:10.760 --> 00:17:12.800]   a better track record for that.
[00:17:12.800 --> 00:17:15.920]   And so they're going to be doing more and more services that are going to be on the
[00:17:15.920 --> 00:17:17.200]   Apple EcoSphere.
[00:17:17.200 --> 00:17:20.600]   So I think that this revenue is actually going to increase a lot more as people are more
[00:17:20.600 --> 00:17:26.120]   aware of what's happening and how companies are misusing, you know, their own data.
[00:17:26.120 --> 00:17:28.800]   Yeah, this was probably a bad week to double that.
[00:17:28.800 --> 00:17:33.560]   I was just going to say, yeah, because of course, there, there, there.
[00:17:33.560 --> 00:17:38.720]   We're talking about the FaceTime flaw that allowed people using group FaceTime to eavesdrop
[00:17:38.720 --> 00:17:46.200]   on other people, at least briefly, until they either didn't pick up or picked up the call.
[00:17:46.200 --> 00:17:49.240]   Did Tim Cook, I didn't listen to the analyst call and I didn't read the notes.
[00:17:49.240 --> 00:17:53.600]   Did he say anything about the FaceTime flaw in that call?
[00:17:53.600 --> 00:17:55.040]   They did apologize.
[00:17:55.040 --> 00:18:02.200]   Yeah, they apologized and removed the feature until they're able to roll out a software
[00:18:02.200 --> 00:18:03.200]   update.
[00:18:03.200 --> 00:18:08.640]   But yeah, I mean, look, Apple, rightly, I think has a really great reputation for privacy.
[00:18:08.640 --> 00:18:11.080]   Yeah, this doesn't, this doesn't ding that.
[00:18:11.080 --> 00:18:12.080]   This is a bug.
[00:18:12.080 --> 00:18:13.080]   No, it doesn't.
[00:18:13.080 --> 00:18:14.080]   This is a bug.
[00:18:14.080 --> 00:18:16.240]   They didn't get caught doing something wrong.
[00:18:16.240 --> 00:18:17.240]   Right.
[00:18:17.240 --> 00:18:19.240]   Facebook did that was a purpose.
[00:18:19.240 --> 00:18:20.240]   They made a mistake.
[00:18:20.240 --> 00:18:24.560]   They tried to fix it as quickly as possible.
[00:18:24.560 --> 00:18:26.840]   You know, they might have given a little bit of a run around.
[00:18:26.840 --> 00:18:27.840]   I get that.
[00:18:27.840 --> 00:18:29.600]   That's not great.
[00:18:29.600 --> 00:18:31.280]   And it's a really big deal.
[00:18:31.280 --> 00:18:35.520]   But this is really different from them choosing to sell your information and trying to be sneaky
[00:18:35.520 --> 00:18:36.520]   about it.
[00:18:36.520 --> 00:18:38.880]   Apple, again, wants to keep your business.
[00:18:38.880 --> 00:18:42.600]   So it makes every sense that they're going to try to make sure they're as secure as possible.
[00:18:42.600 --> 00:18:43.760]   They did respond quickly.
[00:18:43.760 --> 00:18:47.920]   Well, they responded quickly to the nine to five next stories.
[00:18:47.920 --> 00:18:50.160]   When they got caught, they were really fast about that.
[00:18:50.160 --> 00:18:52.000]   I don't think it's fair to say caught either.
[00:18:52.000 --> 00:18:53.000]   I mean, so.
[00:18:53.000 --> 00:18:56.920]   No, no, when they were when they were when the issue made it to their to their account.
[00:18:56.920 --> 00:19:01.200]   I mean, people tried, but you know, there's a family of people.
[00:19:01.200 --> 00:19:06.840]   Michelle Thompson, who's a 14 year old, discovered this a week before it was revealed a nine
[00:19:06.840 --> 00:19:11.400]   to five Mac, a security researcher, found it later, tried to get through to Apple.
[00:19:11.400 --> 00:19:14.360]   But she's, you know, not a developer.
[00:19:14.360 --> 00:19:15.360]   She's not somebody.
[00:19:15.360 --> 00:19:18.560]   Apple gets security reports from on a regular basis.
[00:19:18.560 --> 00:19:20.800]   So she says she never received a response from the company.
[00:19:20.800 --> 00:19:26.440]   She actually went to Fox News to tell them, look, there's a serious security privacy flaw
[00:19:26.440 --> 00:19:28.840]   in FaceTime.
[00:19:28.840 --> 00:19:30.480]   Apple did in its apology.
[00:19:30.480 --> 00:19:34.720]   They say, we thank the Thompson family from reporting the bug.
[00:19:34.720 --> 00:19:40.520]   And we're going to improve our process by which we receive and escalate these reports
[00:19:40.520 --> 00:19:42.720]   in order to get them the right people as fast as possible.
[00:19:42.720 --> 00:19:45.840]   All she got was a note saying, you should join that.
[00:19:45.840 --> 00:19:48.080]   If you want to report bugs, you need to join the developer program.
[00:19:48.080 --> 00:19:51.280]   It's kind of a tough thing to like coming from the things.
[00:19:51.280 --> 00:19:52.280]   Right.
[00:19:52.280 --> 00:19:55.120]   It's hard to tell when to take these things seriously and when not to.
[00:19:55.120 --> 00:19:56.120]   So that's understandable.
[00:19:56.120 --> 00:20:01.760]   It's just a shame that this was the one missed, you know, support ticket that was one of the
[00:20:01.760 --> 00:20:03.720]   most egregious privacy flaws.
[00:20:03.720 --> 00:20:09.040]   Like, I know this was not purposeful, but you know, the people who signed up for Facebook's
[00:20:09.040 --> 00:20:13.520]   privacy and information sharing thing, they at least knew like Facebook was doing this
[00:20:13.520 --> 00:20:14.520]   and they were getting paid for it.
[00:20:14.520 --> 00:20:15.760]   It was like a contract.
[00:20:15.760 --> 00:20:19.960]   This is an insane bit of like spy tech, basically.
[00:20:19.960 --> 00:20:25.000]   Like, this is something that I know like spy agencies have been trying to cook up things
[00:20:25.000 --> 00:20:27.240]   like this so that they can listen on your conversation.
[00:20:27.240 --> 00:20:32.800]   But how useful would it be if you were the NSA or the CIA, you knew about this.
[00:20:32.800 --> 00:20:36.640]   This only happened when Apple released group FaceTime, which was just a couple of versions
[00:20:36.640 --> 00:20:37.640]   of iOS 12 ago.
[00:20:37.640 --> 00:20:39.120]   It was like 12.1.
[00:20:39.120 --> 00:20:42.400]   So it hasn't been around that long, but let's say you're in the NSA.
[00:20:42.400 --> 00:20:47.520]   You're not saying rubbing your hands with Lee saying, oh, finally I can listen in Putin's
[00:20:47.520 --> 00:20:51.160]   conversations because it only rings the phone, right?
[00:20:51.160 --> 00:20:56.600]   And only they say is calling and yeah, says you're not answer the phone.
[00:20:56.600 --> 00:20:57.600]   Yeah.
[00:20:57.600 --> 00:21:01.080]   So it's not that useful, but you know, it's just it's embarrassing.
[00:21:01.080 --> 00:21:04.920]   Apple did respond once the United five Mac report came out.
[00:21:04.920 --> 00:21:07.560]   They responded immediately by turning off group FaceTime.
[00:21:07.560 --> 00:21:08.560]   It's still off.
[00:21:08.560 --> 00:21:11.960]   And as you said, Christina, they'll have they say they'll have a patch this week and group
[00:21:11.960 --> 00:21:12.960]   face.
[00:21:12.960 --> 00:21:13.960]   Yeah, no, totally.
[00:21:13.960 --> 00:21:17.960]   And I mean, in terms of bugs to have, I think you point out, like it's not good at all.
[00:21:17.960 --> 00:21:20.920]   It gets you believe the embarrassing, but it could be way worse.
[00:21:20.920 --> 00:21:22.160]   The phone is going to ring.
[00:21:22.160 --> 00:21:27.160]   So the longest you're talking about, you know, maybe 20 seconds that you might be able to
[00:21:27.160 --> 00:21:30.880]   hear something and and and I mean, it's not great.
[00:21:30.880 --> 00:21:34.200]   I think the video aspect was probably worse than maybe even the audio stuff, the fact that
[00:21:34.200 --> 00:21:35.560]   you could maybe see people.
[00:21:35.560 --> 00:21:37.160]   That's not great, but they fixed it.
[00:21:37.160 --> 00:21:40.600]   And I think that the good news here is that because they were able to fix things server
[00:21:40.600 --> 00:21:43.760]   side because of what this was, they could put a stop to it.
[00:21:43.760 --> 00:21:47.480]   You know, they on the server side level, they just disabled the group FaceTime feature
[00:21:47.480 --> 00:21:49.160]   and now they're going to issue a patch.
[00:21:49.160 --> 00:21:52.840]   So this wasn't even, I mean, this could have been really bad if this was something where
[00:21:52.840 --> 00:21:56.760]   they would have had to release an update that everybody's phone would have to be updated,
[00:21:56.760 --> 00:22:01.160]   you know, to anybody who's ever had an eight minute voicemail message from a butt dial.
[00:22:01.160 --> 00:22:04.840]   No, there's not that much going on.
[00:22:04.840 --> 00:22:07.160]   Have you ever sat and listened to all eight minutes?
[00:22:07.160 --> 00:22:08.160]   Oh, yeah.
[00:22:08.160 --> 00:22:09.160]   Oh, yeah.
[00:22:09.160 --> 00:22:12.440]   I mean, because you hear a lot of people.
[00:22:12.440 --> 00:22:17.000]   I usually, I usually get bored really quickly because you have to do something full eight
[00:22:17.000 --> 00:22:18.000]   minutes.
[00:22:18.000 --> 00:22:22.480]   Oh, I've done it just because you're kind of curious because sometimes, you know, again,
[00:22:22.480 --> 00:22:24.560]   there's that one time, most of the time you delete it, right?
[00:22:24.560 --> 00:22:26.320]   But you hear interesting conversations.
[00:22:26.320 --> 00:22:27.320]   You're like, wait a minute.
[00:22:27.320 --> 00:22:29.800]   All right, we're going to wrap the bank at eight tonight.
[00:22:29.800 --> 00:22:30.800]   Everybody in order.
[00:22:30.800 --> 00:22:31.800]   Yeah, about you.
[00:22:31.800 --> 00:22:32.800]   Yeah.
[00:22:32.800 --> 00:22:33.800]   Well, this is what I'm saying.
[00:22:33.800 --> 00:22:38.000]   That's what happens is this is like the worst case scenario where somebody's talking smack
[00:22:38.000 --> 00:22:41.280]   about you and unknowingly calling you.
[00:22:41.280 --> 00:22:45.040]   This is where voice assistance, I think, can be very dangerous because you can imagine
[00:22:45.040 --> 00:22:49.560]   if you're talking about someone you accidentally invoke the voice assistant that then calls
[00:22:49.560 --> 00:22:56.080]   that person and sort of leave a long message, you know, talking poorly about that person.
[00:22:56.080 --> 00:23:00.440]   No, I mean, I've maybe listened once like eight minute ones, usually just to leave, but you
[00:23:00.440 --> 00:23:05.040]   might hear a few occasional things are like, oh, so that's what they think about someone.
[00:23:05.040 --> 00:23:06.040]   It does happen.
[00:23:06.040 --> 00:23:11.000]   There's a guy who lost his job and is suing his ex-boss.
[00:23:11.000 --> 00:23:13.520]   This happened last spring.
[00:23:13.520 --> 00:23:20.560]   He accidentally but dialed his boss or pocket dialed him, as the post says, and he was talking
[00:23:20.560 --> 00:23:26.560]   to his wife about his boss and the boss heard him say negative things and he didn't like
[00:23:26.560 --> 00:23:27.560]   it.
[00:23:27.560 --> 00:23:31.520]   So he so he fired him and now there's a lot there.
[00:23:31.520 --> 00:23:32.520]   Not now, but I guess.
[00:23:32.520 --> 00:23:33.520]   Yeah.
[00:23:33.520 --> 00:23:39.640]   Now there's a lawsuit because he continued to listen and violated his privacy.
[00:23:39.640 --> 00:23:40.640]   Oh, that's interesting.
[00:23:40.640 --> 00:23:43.040]   Is there really violating your privacy?
[00:23:43.040 --> 00:23:44.040]   I don't know.
[00:23:44.040 --> 00:23:47.280]   I don't think that's going down though because there's no way like you call someone else.
[00:23:47.280 --> 00:23:48.560]   You sent me a message, dude.
[00:23:48.560 --> 00:23:50.120]   Not just listening to your message.
[00:23:50.120 --> 00:23:51.200]   Listening to the message.
[00:23:51.200 --> 00:23:53.680]   It does not sound like a violation of privacy.
[00:23:53.680 --> 00:23:54.680]   Yeah.
[00:23:54.680 --> 00:23:59.760]   And I didn't think you could say it's an unlawful dismissal because you know, you say something
[00:23:59.760 --> 00:24:00.760]   bad about someone.
[00:24:00.760 --> 00:24:04.040]   No one's going to want to work with you.
[00:24:04.040 --> 00:24:05.040]   So.
[00:24:05.040 --> 00:24:06.040]   Mm.
[00:24:06.040 --> 00:24:07.040]   Mm.
[00:24:07.040 --> 00:24:08.040]   Mm.
[00:24:08.040 --> 00:24:09.040]   Mm.
[00:24:09.040 --> 00:24:10.480]   All right, let's talk about you brought it up into the vendors.
[00:24:10.480 --> 00:24:16.280]   We should talk about it both Google and Facebook were spanked by Apple this week because they
[00:24:16.280 --> 00:24:23.000]   were using their enterprise certificates in a way that Apple said that's not appropriate.
[00:24:23.000 --> 00:24:24.400]   Apple has restored Facebook.
[00:24:24.400 --> 00:24:26.040]   I imagine by now they've restored Google's.
[00:24:26.040 --> 00:24:32.400]   The problem is that when you disable an enterprise certificate, you disable also in house apps.
[00:24:32.400 --> 00:24:35.880]   The Google bus app, the GLS app stopped working.
[00:24:35.880 --> 00:24:37.240]   So they're just stuck there.
[00:24:37.240 --> 00:24:38.240]   They couldn't go home.
[00:24:38.240 --> 00:24:39.240]   I can't believe.
[00:24:39.240 --> 00:24:41.960]   I don't know what the bus is coming.
[00:24:41.960 --> 00:24:44.680]   So what so, Devendra, you're obviously up on this.
[00:24:44.680 --> 00:24:46.520]   What was first of all, let's start with Facebook.
[00:24:46.520 --> 00:24:48.960]   What were they doing?
[00:24:48.960 --> 00:24:53.000]   From what I'm hearing, it was a research app.
[00:24:53.000 --> 00:24:56.720]   I haven't fully covered the story, but it was an internal research app.
[00:24:56.720 --> 00:25:00.640]   And it was something where Facebook was paying people basically to give them full access
[00:25:00.640 --> 00:25:02.040]   to everything on their phones.
[00:25:02.040 --> 00:25:03.200]   $20 a month.
[00:25:03.200 --> 00:25:08.800]   They gave root network access to the Facebook for all data, not just Facebook data, but
[00:25:08.800 --> 00:25:09.800]   all data.
[00:25:09.800 --> 00:25:10.800]   All data.
[00:25:10.800 --> 00:25:11.800]   But they were told.
[00:25:11.800 --> 00:25:12.800]   They were told.
[00:25:12.800 --> 00:25:13.800]   Right?
[00:25:13.800 --> 00:25:14.800]   They were told.
[00:25:14.800 --> 00:25:15.800]   Yeah.
[00:25:15.800 --> 00:25:16.800]   They're teenage olds.
[00:25:16.800 --> 00:25:17.800]   Yeah.
[00:25:17.800 --> 00:25:18.800]   This is not fair.
[00:25:18.800 --> 00:25:20.800]   They can't really give access.
[00:25:20.800 --> 00:25:24.480]   You're happy if your teen child had breakfast.
[00:25:24.480 --> 00:25:26.400]   They can't make these big decisions.
[00:25:26.400 --> 00:25:27.400]   They don't get it.
[00:25:27.400 --> 00:25:28.400]   They're like 25.
[00:25:28.400 --> 00:25:29.400]   Even if you accept that, right?
[00:25:29.400 --> 00:25:32.240]   Like, you could argue like if that's a worthy thing or not.
[00:25:32.240 --> 00:25:36.800]   But I think the really egregious part is that this is a continuation of something Facebook
[00:25:36.800 --> 00:25:37.800]   had before.
[00:25:37.800 --> 00:25:40.240]   It was a VPN app called Unavo.
[00:25:40.240 --> 00:25:44.400]   And they were using that app to track user behavior and track things that were happening
[00:25:44.400 --> 00:25:45.400]   on their phones.
[00:25:45.400 --> 00:25:48.200]   They were, you know, that was swatted down.
[00:25:48.200 --> 00:25:52.800]   This research app is basically a rebad of that technology, which Apple has already said
[00:25:52.800 --> 00:25:53.800]   no to.
[00:25:53.800 --> 00:25:58.200]   This is like, this is so Facebook, which is, yeah, you break the rules, you get caught,
[00:25:58.200 --> 00:26:00.120]   you apologize, then you do it again.
[00:26:00.120 --> 00:26:01.120]   Right.
[00:26:01.120 --> 00:26:04.920]   Well, and what they did, I mean, and in partially, I'm kind of happy that this whole instance
[00:26:04.920 --> 00:26:08.400]   happened, I obviously feel terrible for anybody who, any employees who worked at Facebook
[00:26:08.400 --> 00:26:13.280]   or Google who lost access to their stuff because, you know, I work at a big tech company and
[00:26:13.280 --> 00:26:18.680]   we have, you know, like internal enterprise apps and some of them are really important
[00:26:18.680 --> 00:26:22.360]   and it would be really frustrating to not have access to those things.
[00:26:22.360 --> 00:26:27.880]   But this has been a really common way around the app store for a long time that has kind
[00:26:27.880 --> 00:26:31.960]   of gone under the radar that hasn't had a lot of reporting around it where, you know,
[00:26:31.960 --> 00:26:37.520]   Apple won't allow certain types of applications in the app store for whatever reasons they
[00:26:37.520 --> 00:26:43.800]   say it violates, you know, collection information or, you know, maybe it's an emulator and we
[00:26:43.800 --> 00:26:45.640]   don't support this sort of thing or whatever.
[00:26:45.640 --> 00:26:51.040]   And so instead what happens is people get an enterprise certificate and, you know, they
[00:26:51.040 --> 00:26:54.480]   pay for one of those accounts or they just get a regular dev certificate where you can
[00:26:54.480 --> 00:27:00.760]   now have even more, you know, testers and they issue, you know, copies with the sort
[00:27:00.760 --> 00:27:04.800]   of round, you know, kind of in back channels and people install them, they install, you
[00:27:04.800 --> 00:27:10.120]   know, the certificate on their phone and then they're able to, you know, download the app
[00:27:10.120 --> 00:27:16.240]   directly from the developer and people use that as a way to, you know, do stuff that
[00:27:16.240 --> 00:27:19.520]   otherwise wouldn't be there, whether it's, you know, pirated, you know, sort of stuff
[00:27:19.520 --> 00:27:24.840]   or, you know, maybe something that, again, like in Facebook's case is rebatching an app
[00:27:24.840 --> 00:27:27.320]   that the apps for wouldn't let in and in some cases it might be legitimate.
[00:27:27.320 --> 00:27:32.440]   It might be like, I really want this SSH type of scenario or some sort of code editor on
[00:27:32.440 --> 00:27:33.680]   my phone.
[00:27:33.680 --> 00:27:38.560]   This won't be allowed at the app store, but I can get an enterprise certificate and release
[00:27:38.560 --> 00:27:41.000]   it to however many thousands of people.
[00:27:41.000 --> 00:27:42.000]   Cool.
[00:27:42.000 --> 00:27:45.160]   So this has been happening, but nothing's been done about it.
[00:27:45.160 --> 00:27:48.240]   And so if anything, I'm kind of glad that this is kind of drawing attention to the fact
[00:27:48.240 --> 00:27:55.240]   that there is this whole shadow, like app economy using these things and there and especially
[00:27:55.240 --> 00:27:59.040]   in other countries where, you know, people have to go around things, maybe to get around
[00:27:59.040 --> 00:28:02.880]   censorship or other stuff, like we should acknowledge that this is happening because
[00:28:02.880 --> 00:28:07.040]   Apple really loves to talk about how safe and insecure it store is and by the most part
[00:28:07.040 --> 00:28:12.320]   it is, but this is a really easy hole, like a really easy in run around those policies.
[00:28:12.320 --> 00:28:13.720]   Well, that's an important point.
[00:28:13.720 --> 00:28:20.400]   You can laud Apple for respecting privacy, but the minute you put any third party app
[00:28:20.400 --> 00:28:25.160]   on your iPhone, you're being monitored in some way or other, whether it's location,
[00:28:25.160 --> 00:28:27.760]   monitoring them in and you give them permission, I understand.
[00:28:27.760 --> 00:28:33.720]   But Apple's privacy policy only extends up to the point of Apple's apps and after that
[00:28:33.720 --> 00:28:34.960]   it's anything goes.
[00:28:34.960 --> 00:28:39.360]   Well, also it's the difference between the apps that are being available within the store
[00:28:39.360 --> 00:28:40.360]   and these apps that we're talking about.
[00:28:40.360 --> 00:28:41.360]   These are far worse.
[00:28:41.360 --> 00:28:42.360]   I understand.
[00:28:42.360 --> 00:28:43.360]   So let me just...
[00:28:43.360 --> 00:28:44.920]   There's nothing wrong with that.
[00:28:44.920 --> 00:28:45.920]   Like that's the thing.
[00:28:45.920 --> 00:28:46.920]   I don't...
[00:28:46.920 --> 00:28:48.800]   It sounds like we're getting to the point where it's like, oh, all these enterprise certificate
[00:28:48.800 --> 00:28:49.800]   apps are bad.
[00:28:49.800 --> 00:28:50.800]   Oh, no, no, no.
[00:28:50.800 --> 00:28:51.800]   Not at all.
[00:28:51.800 --> 00:28:52.800]   That's the way we have to...
[00:28:52.800 --> 00:28:56.000]   Like when I see apps early, this is the process I have to go through to see something
[00:28:56.000 --> 00:28:58.280]   before it's on the store.
[00:28:58.280 --> 00:28:59.280]   There are certain loopholes there.
[00:28:59.280 --> 00:29:01.000]   It's more like, yeah, what Facebook did.
[00:29:01.000 --> 00:29:04.360]   And to be clear, this is a novel VPN I'm talking about.
[00:29:04.360 --> 00:29:07.800]   I covered them when they launched like several years ago and it was a really interesting app
[00:29:07.800 --> 00:29:12.680]   idea because it was supposed to be this little app that would basically compress your data
[00:29:12.680 --> 00:29:15.720]   and save you cell phone bandwidth and stuff.
[00:29:15.720 --> 00:29:18.160]   They'll save your bandwidth cap.
[00:29:18.160 --> 00:29:21.200]   When Facebook bought it, everyone was like, oh, they're going to do something terrible
[00:29:21.200 --> 00:29:22.200]   with this.
[00:29:22.200 --> 00:29:24.600]   And not long after that, they did something terrible with it.
[00:29:24.600 --> 00:29:28.760]   So this is all an example of Facebook being the worst and trying to get away with being
[00:29:28.760 --> 00:29:30.240]   the worst like again and again.
[00:29:30.240 --> 00:29:31.840]   Yeah, at some point you have to...
[00:29:31.840 --> 00:29:33.600]   At some point you have to say, oh, this is intense.
[00:29:33.600 --> 00:29:34.600]   They get away.
[00:29:34.600 --> 00:29:37.160]   They keep on doing it and people...
[00:29:37.160 --> 00:29:41.080]   Again, it's like these companies too large to fail because Apple's going to take a whole
[00:29:41.080 --> 00:29:44.320]   bunch of heat if they just cut them out and say, listen, we can't trust you.
[00:29:44.320 --> 00:29:46.800]   We're going to have to check everything that you do beforehand.
[00:29:46.800 --> 00:29:50.240]   But the worst part is Facebook was doing this, got caught for it.
[00:29:50.240 --> 00:29:51.720]   They're already, I think, back in.
[00:29:51.720 --> 00:29:55.720]   I think that they've already said, okay, you're all restored to that.
[00:29:55.720 --> 00:29:59.640]   But they were doing this in order to catch what was the wave with the young generation
[00:29:59.640 --> 00:30:06.480]   and then be able to buy out whatever applications were important to do it so that they could
[00:30:06.480 --> 00:30:07.480]   exactly...
[00:30:07.480 --> 00:30:08.480]   Yeah.
[00:30:08.480 --> 00:30:12.200]   So let me, by the way, a lot of credit to John Konstein and TechCrunch because they broke
[00:30:12.200 --> 00:30:16.840]   the story and they did a lot of great reporting on this.
[00:30:16.840 --> 00:30:22.200]   Konstein had written, as you did, David, and many others about the Onavo Protect, Apple
[00:30:22.200 --> 00:30:27.520]   changed its policies to prohibit the way Onavo Protect worked.
[00:30:27.520 --> 00:30:30.520]   So Facebook...
[00:30:30.520 --> 00:30:32.000]   That app was taken out of the store.
[00:30:32.000 --> 00:30:36.360]   Facebook took it out of the store where Apple blocked it from the store, but it was gone.
[00:30:36.360 --> 00:30:41.000]   It looks like, according to Konstein, they got somebody to look at the code and the Facebook
[00:30:41.000 --> 00:30:43.320]   Research app is basically...
[00:30:43.320 --> 00:30:49.640]   He says, "We've found it featured tons of similar code and references to the Onavo Protect program,"
[00:30:49.640 --> 00:30:55.120]   which meant Facebook was intentionally disobeying Apple's privacy policy.
[00:30:55.120 --> 00:30:57.200]   Now, the idea of an enterprise...
[00:30:57.200 --> 00:31:01.480]   No, the idea of an enterprise certificate is completely normal.
[00:31:01.480 --> 00:31:05.640]   Many, many companies have internal apps that they want to put on iPhones.
[00:31:05.640 --> 00:31:06.640]   Absolutely.
[00:31:06.640 --> 00:31:08.360]   In order to do that, they have an enterprise certificate.
[00:31:08.360 --> 00:31:10.760]   Apple certainly allows this.
[00:31:10.760 --> 00:31:15.200]   There also is a test flight app that you can use if you want to test apps.
[00:31:15.200 --> 00:31:16.200]   This is normal.
[00:31:16.200 --> 00:31:23.440]   In fact, Facebook's enterprise certificate was used for seeing the day's lunch schedule,
[00:31:23.440 --> 00:31:26.880]   coordinating office, collaboration, commutes.
[00:31:26.880 --> 00:31:31.480]   They used it for pre-launch internal testing versions of Facebook's own app and Instagram.
[00:31:31.480 --> 00:31:40.160]   So Apple, which could have simply said, "Okay, project Atlas," which is this app, "is yanked.
[00:31:40.160 --> 00:31:41.760]   You can't do it anymore."
[00:31:41.760 --> 00:31:43.560]   Instead, they spanked...
[00:31:43.560 --> 00:31:45.400]   I'm sorry.
[00:31:45.400 --> 00:31:50.080]   They spanked Facebook by yanking this enterprise app, knowing that this would be completely
[00:31:50.080 --> 00:31:52.400]   disruptive as apparently it was.
[00:31:52.400 --> 00:31:53.920]   Well, they revoked the certificate.
[00:31:53.920 --> 00:31:57.120]   I mean, in this case, again, I think you could make the argument that the only way Apple
[00:31:57.120 --> 00:31:59.760]   knew that they could shut down access to the app.
[00:31:59.760 --> 00:32:02.880]   Couldn't they have a kill switch for the app?
[00:32:02.880 --> 00:32:05.000]   No, the kill switch is the certificate.
[00:32:05.000 --> 00:32:07.840]   The kill switch is enterprise certificate, right?
[00:32:07.840 --> 00:32:11.720]   So, honestly, the biggest mistake that Facebook made, if you're doing this, you know you're
[00:32:11.720 --> 00:32:13.200]   violating the App Store rules.
[00:32:13.200 --> 00:32:17.640]   You're using this enterprise certificate as a way to distribute apps outside the App Store,
[00:32:17.640 --> 00:32:19.560]   which is not the way it's supposed to be done.
[00:32:19.560 --> 00:32:22.360]   It's supposed to be for internal purposes, whatnot for testing.
[00:32:22.360 --> 00:32:23.600]   It's not supposed to be...
[00:32:23.600 --> 00:32:27.720]   We have a research project and we're paying people $20 a month and we're getting this
[00:32:27.720 --> 00:32:29.320]   information for our business.
[00:32:29.320 --> 00:32:33.640]   This is not how you're supposed to be using these types of tools.
[00:32:33.640 --> 00:32:37.160]   And rather than signing up for a new enterprise account, which would have been the best thing
[00:32:37.160 --> 00:32:41.560]   in having a different certificate to use it, they use their existing enterprise certificate.
[00:32:41.560 --> 00:32:43.760]   So you don't think it was punitive on Apple's part.
[00:32:43.760 --> 00:32:45.480]   This was just their only recourse.
[00:32:45.480 --> 00:32:46.880]   It was their only recourse.
[00:32:46.880 --> 00:32:50.040]   If Facebook and Google, when they ran this thing, had used...
[00:32:50.040 --> 00:32:51.720]   I mean, do I think it's punitive.
[00:32:51.720 --> 00:32:55.640]   I think Tim Cook calls Mark Zuckerberg and says, "Oh, yeah."
[00:32:55.640 --> 00:32:56.640]   Paul Project Atlas.
[00:32:56.640 --> 00:33:01.400]   You're not wrong, but I also feel like at this point, if you know, was there some punitive
[00:33:01.400 --> 00:33:02.400]   stuff to it?
[00:33:02.400 --> 00:33:05.320]   I'm sure there was, but I think you can also make the argument that says, "You've already
[00:33:05.320 --> 00:33:07.760]   banned this type of content from your app store.
[00:33:07.760 --> 00:33:09.600]   You've already said this sort of thing isn't allowed."
[00:33:09.600 --> 00:33:10.600]   You have...
[00:33:10.600 --> 00:33:11.600]   Yeah, they spammed them.
[00:33:11.600 --> 00:33:12.600]   Yeah.
[00:33:12.600 --> 00:33:13.600]   Yeah.
[00:33:13.600 --> 00:33:14.600]   But you have proof that this is doing it.
[00:33:14.600 --> 00:33:15.600]   I'm not saying it's inappropriate.
[00:33:15.600 --> 00:33:16.600]   I'm just curious.
[00:33:16.600 --> 00:33:19.640]   Did Apple say, "All right, to the woodshed with you?"
[00:33:19.640 --> 00:33:21.720]   I'm sure they did, but I'm sure that...
[00:33:21.720 --> 00:33:25.040]   I mean, you could make the argument where they could, I guess, claim that they're not
[00:33:25.040 --> 00:33:29.560]   being punitive by saying, "Well, the only way we can guarantee that this isn't being
[00:33:29.560 --> 00:33:31.840]   used anymore is to revoke the certificate."
[00:33:31.840 --> 00:33:37.560]   And if that has collateral damage with other apps that use the certificate, so be it.
[00:33:37.560 --> 00:33:38.560]   Yeah.
[00:33:38.560 --> 00:33:43.080]   See, I would have went after them much more strongly and been straight up.
[00:33:43.080 --> 00:33:45.280]   This is the plutonium tea.
[00:33:45.280 --> 00:33:46.720]   Listen, sorry.
[00:33:46.720 --> 00:33:50.960]   You do this, you get caught, this looks bad upon us, and we're going to punish the whole
[00:33:50.960 --> 00:33:52.480]   system to that because it does.
[00:33:52.480 --> 00:33:55.780]   It ends up looking bad upon Apple because people don't really understand the difference
[00:33:55.780 --> 00:33:58.320]   between what they have on their platform.
[00:33:58.320 --> 00:34:01.120]   It's a really big issue, and it was really embarrassing.
[00:34:01.120 --> 00:34:06.680]   Here's what Apple told TechCrunch, "We designed our enterprise developer program solely for
[00:34:06.680 --> 00:34:09.680]   the internal distribution of apps within an organization.
[00:34:09.680 --> 00:34:14.840]   Facebook has been using their membership to distribute a data collecting app to consumers,
[00:34:14.840 --> 00:34:16.880]   which is a clear breach of their agreement with Apple.
[00:34:16.880 --> 00:34:21.240]   Any developer using their enterprise certificates to distribute apps to consumers will have their
[00:34:21.240 --> 00:34:25.200]   certificates revoked, which is what we did, in this case, to protect our users and their
[00:34:25.200 --> 00:34:26.200]   data.
[00:34:26.200 --> 00:34:29.600]   And oh, by the way, they did it to Google too."
[00:34:29.600 --> 00:34:37.720]   Shortly after they did it to Facebook, TechCrunch discovered Google's Screenwise Meter Surveillance
[00:34:37.720 --> 00:34:40.120]   app.
[00:34:40.120 --> 00:34:45.720]   Very much like the Facebook research app was doing the same thing Apple canceled that enterprise,
[00:34:45.720 --> 00:34:49.400]   and that's when Google's G-bus program and others stopped working.
[00:34:49.400 --> 00:34:55.560]   Now this is another case of Google telling people, here's the message you'll see, "The
[00:34:55.560 --> 00:34:59.440]   Screenwise Meter app collects information on your internet usage from how long you visit
[00:34:59.440 --> 00:35:00.640]   a site to the apps you choose.
[00:35:00.640 --> 00:35:03.640]   It also manages your participation in a panel."
[00:35:03.640 --> 00:35:11.000]   It's kind of like, and I do this, the Google app to earn Google Opinion Rewards program
[00:35:11.000 --> 00:35:12.000]   to earn payments.
[00:35:12.000 --> 00:35:18.480]   And I do that once and while I answer a survey, Screenwise, like the Facebook research app,
[00:35:18.480 --> 00:35:21.360]   was open to users as young as 13.
[00:35:21.360 --> 00:35:27.120]   Konstein on TechCrunch says that about 5% of the users were under age of the Facebook
[00:35:27.120 --> 00:35:31.360]   app.
[00:35:31.360 --> 00:35:35.920]   So it wasn't particularly aimed at teenagers, although it was 13 to 35.
[00:35:35.920 --> 00:35:37.880]   So it was aimed at younger users.
[00:35:37.880 --> 00:35:41.720]   And it's clearly Facebook's intent to figure out what the hell are the kids doing.
[00:35:41.720 --> 00:35:44.480]   Because they ain't using Facebook.
[00:35:44.480 --> 00:35:45.480]   What is TikTok?
[00:35:45.480 --> 00:35:46.480]   I don't know.
[00:35:46.480 --> 00:35:47.480]   What is this TikTok?
[00:35:47.480 --> 00:35:51.320]   Why are they doing that?
[00:35:51.320 --> 00:35:56.040]   So Google quickly canceled its Screenwise Meter.
[00:35:56.040 --> 00:36:00.040]   And apparently, I don't know how long it took for their enterprise certificate to come
[00:36:00.040 --> 00:36:04.560]   back, but Facebook was turned back on under two days.
[00:36:04.560 --> 00:36:07.600]   I think Google got theirs fixed pretty quickly.
[00:36:07.600 --> 00:36:13.040]   Yeah, Apple actually made a statement that they were working, or maybe Google, they made
[00:36:13.040 --> 00:36:15.760]   a statement they were actively working with with Apple or Google.
[00:36:15.760 --> 00:36:18.840]   They made a statement that they were working together to get things back up and running.
[00:36:18.840 --> 00:36:21.640]   It's a pretty big stick that Apple wields, so isn't it?
[00:36:21.640 --> 00:36:22.640]   Oh, it is.
[00:36:22.640 --> 00:36:24.640]   I mean, there are questions on it.
[00:36:24.640 --> 00:36:25.640]   They're all us.
[00:36:25.640 --> 00:36:27.800]   It's your way of you expect it, right?
[00:36:27.800 --> 00:36:29.000]   Like you expect that.
[00:36:29.000 --> 00:36:33.200]   You go against another huge company and embarrass them.
[00:36:33.200 --> 00:36:35.120]   You expect to kind of get hit for it.
[00:36:35.120 --> 00:36:36.320]   And it wasn't a big hit.
[00:36:36.320 --> 00:36:38.720]   Like really, within a week, you're all back up.
[00:36:38.720 --> 00:36:40.560]   And again, it's a lot of money when you do it.
[00:36:40.560 --> 00:36:43.080]   But in the large run, it's really, really tiny.
[00:36:43.080 --> 00:36:45.000]   Like this was like, I don't think it was a big stick.
[00:36:45.000 --> 00:36:46.000]   I think it was a small post.
[00:36:46.000 --> 00:36:47.000]   I don't know.
[00:36:47.000 --> 00:36:48.000]   I don't know.
[00:36:48.000 --> 00:36:52.440]   It would be a shame if none of your employees was to know what was for lunch today.
[00:36:52.440 --> 00:36:53.440]   Yeah.
[00:36:53.440 --> 00:36:57.680]   Well, I'm going to disagree just because as somebody who, again, I have enterprise apps
[00:36:57.680 --> 00:36:58.680]   on my own--
[00:36:58.680 --> 00:37:00.240]   I'm sure because I have to use this too, right?
[00:37:00.240 --> 00:37:01.240]   Yes, we do.
[00:37:01.240 --> 00:37:04.280]   And we have both our own system.
[00:37:04.280 --> 00:37:08.960]   We bought HockeyApp, which is similar to TestFlight, but obviously how that works is using an
[00:37:08.960 --> 00:37:09.960]   enterprise certificate.
[00:37:09.960 --> 00:37:12.720]   And that controls things like my VPN access.
[00:37:12.720 --> 00:37:16.880]   And that controls things like, if I'm connecting to my corporate network on my device.
[00:37:16.880 --> 00:37:20.320]   And that connects certain profile information.
[00:37:20.320 --> 00:37:23.960]   And then I have other things like certain versions of my email client or certain things
[00:37:23.960 --> 00:37:25.320]   that are only internal.
[00:37:25.320 --> 00:37:27.080]   And yes, could I get my work done without it?
[00:37:27.080 --> 00:37:28.080]   Of course.
[00:37:28.080 --> 00:37:29.920]   But it's a lot more frustrating.
[00:37:29.920 --> 00:37:33.640]   And if stuff were just to break, I'm not going to discount the fact that if you look
[00:37:33.640 --> 00:37:40.840]   at the 20, 30,000 employees at each of these companies who were probably using at least
[00:37:40.840 --> 00:37:46.120]   one of these devices, one of these apps that now isn't working, that this isn't a big
[00:37:46.120 --> 00:37:47.120]   deal.
[00:37:47.120 --> 00:37:49.400]   Do I think Apple was justified in doing it?
[00:37:49.400 --> 00:37:50.400]   Absolutely.
[00:37:50.400 --> 00:37:53.720]   But I think this was a lot more disruptive than people might thinking.
[00:37:53.720 --> 00:37:56.760]   They might be rolling their eyes and saying, "Oh, well, you can't see what's for lunch."
[00:37:56.760 --> 00:38:01.800]   But what if this was something that was tied to your email or was tied to some sort of
[00:38:01.800 --> 00:38:06.360]   data entry app or some sort of other custom app that was really crucial to your day job
[00:38:06.360 --> 00:38:08.960]   and now you don't have access to that information?
[00:38:08.960 --> 00:38:13.920]   And when you do have access to it again, you have to reinstall each app, redo the certificates.
[00:38:13.920 --> 00:38:20.160]   I mean, it's people lose time and it's frustrating, especially when the reason for this is because
[00:38:20.160 --> 00:38:25.040]   the company you work for decided to go around the app store rules.
[00:38:25.040 --> 00:38:27.360]   So how bad does Facebook suck?
[00:38:27.360 --> 00:38:31.120]   I mean, they said, "No, wait a minute.
[00:38:31.120 --> 00:38:32.120]   This is their response.
[00:38:32.120 --> 00:38:35.160]   Key facts about this market research program are being ignored.
[00:38:35.160 --> 00:38:38.880]   I'm going to do this in a Tony Soprano voice if you don't mind.
[00:38:38.880 --> 00:38:42.320]   Despite early reports, there was nothing sacred about this.
[00:38:42.320 --> 00:38:46.080]   It was literally called the Facebook Research app.
[00:38:46.080 --> 00:38:51.640]   It wasn't spying as all the people signed up to participate, went through a clear onboarding
[00:38:51.640 --> 00:38:56.120]   process, asking for their permission, and it was paid to participate.
[00:38:56.120 --> 00:39:00.320]   Finally, less than 5% of the people who chose to participate in this market research program
[00:39:00.320 --> 00:39:03.400]   were teens, all of them with signed parental consent forms.
[00:39:03.400 --> 00:39:06.120]   You said you wanted a notice.
[00:39:06.120 --> 00:39:10.320]   The $20 gift card, we was given that to you.
[00:39:10.320 --> 00:39:16.080]   I mean, after all, this is kind of the foundational question.
[00:39:16.080 --> 00:39:20.640]   If you know that a company is invading your privacy and you're willing to trade it for
[00:39:20.640 --> 00:39:27.160]   some benefit, whether it's a Gmail or $20, what's wrong with that?
[00:39:27.160 --> 00:39:28.160]   Sure.
[00:39:28.160 --> 00:39:29.160]   Okay, that's mine.
[00:39:29.160 --> 00:39:33.200]   But if it is to do that, you're distributing it on a platform that says you can't do this
[00:39:33.200 --> 00:39:34.200]   sort of thing.
[00:39:34.200 --> 00:39:35.200]   No, I understand.
[00:39:35.200 --> 00:39:37.040]   But yeah, it could be pissed off.
[00:39:37.040 --> 00:39:38.560]   They shouldn't use an enterprise certificate.
[00:39:38.560 --> 00:39:40.480]   Is there any other mechanism though?
[00:39:40.480 --> 00:39:43.600]   By the way, both Google and Facebook's programs are still available on Android because they
[00:39:43.600 --> 00:39:44.600]   don't need any permission.
[00:39:44.600 --> 00:39:48.840]   I should point out you can still do it on Android.
[00:39:48.840 --> 00:39:53.480]   But is there, I mean, you could also say, well, look, this is the only way we can do
[00:39:53.480 --> 00:39:54.480]   it on Apple.
[00:39:54.480 --> 00:39:58.480]   And we think we should have access to people and their information.
[00:39:58.480 --> 00:40:00.200]   Apple says, no, you can't have any of this.
[00:40:00.200 --> 00:40:01.600]   Only we can have this.
[00:40:01.600 --> 00:40:02.600]   Yeah.
[00:40:02.600 --> 00:40:06.360]   I think it's more due to the way Facebook does business too, right?
[00:40:06.360 --> 00:40:11.520]   I think was it this week or recently there was the lawsuit about Facebook not doing anything
[00:40:11.520 --> 00:40:18.040]   to stop kids from buying things from buying in app purchases from within games?
[00:40:18.040 --> 00:40:19.040]   Yeah, exactly.
[00:40:19.040 --> 00:40:21.000]   They actively encouraged it.
[00:40:21.000 --> 00:40:22.560]   Just ignored it or whatever.
[00:40:22.560 --> 00:40:24.240]   I'm sure it's not putting these speed bumps.
[00:40:24.240 --> 00:40:28.120]   I think it's that mentality, that mentality of a company that would allow something like
[00:40:28.120 --> 00:40:31.320]   that, no matter the harm, it does to its users.
[00:40:31.320 --> 00:40:33.080]   That's what really bothers me about Facebook.
[00:40:33.080 --> 00:40:35.360]   Like selling it to people selling it to data.
[00:40:35.360 --> 00:40:36.360]   What you're just doing is not great.
[00:40:36.360 --> 00:40:37.680]   Kind of a hands-off thing.
[00:40:37.680 --> 00:40:39.680]   Well, look, this is free speech.
[00:40:39.680 --> 00:40:41.880]   It's the internet.
[00:40:41.880 --> 00:40:46.560]   You know, if people want to set themselves on fire, we're going to let them.
[00:40:46.560 --> 00:40:47.800]   It's not our job to stop them.
[00:40:47.800 --> 00:40:49.800]   And I kind of understand that.
[00:40:49.800 --> 00:40:53.960]   I mean, I kind of understand that except at the same time Facebook is not just saying,
[00:40:53.960 --> 00:40:55.640]   if you want to set yourself on fire, that's fine.
[00:40:55.640 --> 00:41:00.120]   They're saying, please set yourself on fire and we'll pay you $20 to do it.
[00:41:00.120 --> 00:41:02.480]   To me, that's where it kind of goes a little different.
[00:41:02.480 --> 00:41:04.680]   It's like one thing to be like, do whatever you want.
[00:41:04.680 --> 00:41:09.160]   It's another thing to be like, we're going to actively encourage you to do this thing,
[00:41:09.160 --> 00:41:14.160]   you know, because we want to see, we want to do a research project on, you know, what
[00:41:14.160 --> 00:41:15.160]   temperature, flesh burns.
[00:41:15.160 --> 00:41:19.320]   There's a little bit of a, don't people have personal responsibility?
[00:41:19.320 --> 00:41:20.320]   If somebody-
[00:41:20.320 --> 00:41:21.320]   Well, people do.
[00:41:21.320 --> 00:41:22.320]   13-year-olds don't.
[00:41:22.320 --> 00:41:23.320]   But they got the parents.
[00:41:23.320 --> 00:41:24.720]   They got the parents to say, okay.
[00:41:24.720 --> 00:41:25.720]   Please.
[00:41:25.720 --> 00:41:29.320]   Well, let's be honest, even like the amount of media literacy that's out there right now,
[00:41:29.320 --> 00:41:32.880]   people don't really understand what they're selling off their information to or how far
[00:41:32.880 --> 00:41:34.720]   that information goes.
[00:41:34.720 --> 00:41:38.040]   If, you know, Facebook comes off in this cute little app that we're going to kind of track
[00:41:38.040 --> 00:41:43.240]   you versus some creepy guy that's staring in on my window, writing down my every single
[00:41:43.240 --> 00:41:48.640]   move, when you really understand how this is not as invisible as what you think it is,
[00:41:48.640 --> 00:41:50.720]   then maybe you could make an informed choice.
[00:41:50.720 --> 00:41:54.840]   But it's the thing is that Facebook tries to come off as this front of a cute way to
[00:41:54.840 --> 00:41:56.280]   chat with people.
[00:41:56.280 --> 00:42:00.120]   And really all they're doing is they're trying to be a conduit to influence public opinion
[00:42:00.120 --> 00:42:03.880]   to the highest bidder and be able to gather as much information to be able to do that
[00:42:03.880 --> 00:42:05.600]   as efficiently as possible.
[00:42:05.600 --> 00:42:10.400]   And they really don't care for what purpose or cause that is.
[00:42:10.400 --> 00:42:13.960]   And that really bothers me about the way Facebook goes about business.
[00:42:13.960 --> 00:42:16.120]   I have no sympathy to them.
[00:42:16.120 --> 00:42:18.440]   And really they got handled with kid gloves.
[00:42:18.440 --> 00:42:22.240]   If this was a small dev that got caught making use of something-
[00:42:22.240 --> 00:42:23.240]   They'd never be on Apple's platform.
[00:42:23.240 --> 00:42:25.240]   They'd never seen it again.
[00:42:25.240 --> 00:42:28.880]   They'd be buried in the back with the trash.
[00:42:28.880 --> 00:42:30.120]   And Apple wouldn't feel that.
[00:42:30.120 --> 00:42:35.560]   It is only because Facebook is so big that they can get away with unabashedly making
[00:42:35.560 --> 00:42:40.280]   use of and just trying to destroy people's privacy, trying to hide things, being caught
[00:42:40.280 --> 00:42:42.880]   constantly and no one really does anything.
[00:42:42.880 --> 00:42:46.840]   Only now do I hear people really kind of getting upset and being like, Facebook is not
[00:42:46.840 --> 00:42:50.000]   that cute and friendly as what I thought it was.
[00:42:50.000 --> 00:42:51.360]   But it's taken so long.
[00:42:51.360 --> 00:42:55.200]   We've been talking about it in tech for a really long time, but now it's getting mainstream.
[00:42:55.200 --> 00:42:58.440]   I think Facebook, I've come to the conclusion.
[00:42:58.440 --> 00:43:00.120]   Facebook's pretty low.
[00:43:00.120 --> 00:43:01.560]   But I trust Google for some reason.
[00:43:01.560 --> 00:43:04.520]   And Google, as far as I could tell, invented this.
[00:43:04.520 --> 00:43:09.440]   And Cheryl Sandberg took the fire that the Google invented from Google to Facebook and
[00:43:09.440 --> 00:43:11.720]   showed them the way.
[00:43:11.720 --> 00:43:14.280]   Does Google get a pass?
[00:43:14.280 --> 00:43:17.240]   I'm on DuckDuckGo for almost everything now.
[00:43:17.240 --> 00:43:18.240]   So it's sad.
[00:43:18.240 --> 00:43:19.240]   It's not great.
[00:43:19.240 --> 00:43:20.760]   They even have a mapping system.
[00:43:20.760 --> 00:43:22.040]   It's really sad.
[00:43:22.040 --> 00:43:25.000]   But it makes me feel a little bit better.
[00:43:25.000 --> 00:43:27.120]   I'm not going to say I'm not on it for everything.
[00:43:27.120 --> 00:43:29.800]   It's hard to get out of Google.
[00:43:29.800 --> 00:43:31.400]   But yeah, I've started using DuckDuckGo.
[00:43:31.400 --> 00:43:33.120]   I stopped using Gmail.
[00:43:33.120 --> 00:43:34.120]   It works.
[00:43:34.120 --> 00:43:35.120]   Yeah.
[00:43:35.120 --> 00:43:36.120]   I'm with you.
[00:43:36.120 --> 00:43:38.000]   I'm moving towards the transition.
[00:43:38.000 --> 00:43:40.040]   My children are actually huge duck duck.
[00:43:40.040 --> 00:43:43.040]   They're fully on DuckDuckGo and they're like, "Mom, really?"
[00:43:43.040 --> 00:43:45.000]   I'm like, "Oh, I'd bet."
[00:43:45.000 --> 00:43:46.000]   That's interesting.
[00:43:46.000 --> 00:43:49.000]   They're more aware of privacy than you are.
[00:43:49.000 --> 00:43:50.000]   They just like think that's a go.
[00:43:50.000 --> 00:43:51.000]   Are they like the name Duck?
[00:43:51.000 --> 00:43:52.000]   Yeah.
[00:43:52.000 --> 00:43:53.000]   They do like ducks.
[00:43:53.000 --> 00:43:57.440]   But no, I think actually are doing it for their privacy.
[00:43:57.440 --> 00:43:58.440]   Yeah.
[00:43:58.440 --> 00:44:03.720]   So I think it was because once you get comfortable with something, you just don't want to change.
[00:44:03.720 --> 00:44:05.120]   Change is scary for us.
[00:44:05.120 --> 00:44:06.240]   And so you get comfortable.
[00:44:06.240 --> 00:44:07.920]   The same reason that Facebook survives.
[00:44:07.920 --> 00:44:10.080]   Facebook actually has increased in numbers.
[00:44:10.080 --> 00:44:11.080]   What is up?
[00:44:11.080 --> 00:44:14.520]   People, why do they have they larger numbers now with people using Facebook?
[00:44:14.520 --> 00:44:15.520]   I don't know.
[00:44:15.520 --> 00:44:16.520]   Yeah.
[00:44:16.520 --> 00:44:18.080]   But I think that we just get comfortable.
[00:44:18.080 --> 00:44:20.720]   I guess all this bad press was good for them.
[00:44:20.720 --> 00:44:24.400]   That's the craziest thing to speak about.
[00:44:24.400 --> 00:44:28.320]   After all this privacy stuff, Facebook's earnings were glorious.
[00:44:28.320 --> 00:44:29.320]   Amazing for them.
[00:44:29.320 --> 00:44:30.320]   And it was kind of depressing.
[00:44:30.320 --> 00:44:31.320]   Yeah.
[00:44:31.320 --> 00:44:32.320]   They're selling off this information, I guess.
[00:44:32.320 --> 00:44:33.320]   That's what I'm assuming.
[00:44:33.320 --> 00:44:36.560]   They're like, "They've made a ton of money selling off of this information."
[00:44:36.560 --> 00:44:40.080]   So this is not going to stop because of this.
[00:44:40.080 --> 00:44:41.320]   They're going to just get sneakier.
[00:44:41.320 --> 00:44:43.160]   That's really isn't that what Facebook just does.
[00:44:43.160 --> 00:44:45.360]   It just gets better at being sneaky.
[00:44:45.360 --> 00:44:49.560]   Well, I think the interesting thing too is that, I mean, Google's the same way and you
[00:44:49.560 --> 00:44:52.480]   could say the same thing about Microsoft and Amazon and any of these things is that it's
[00:44:52.480 --> 00:44:57.520]   really hard to cut any in Apple for sure too, even though Apple is better about certain
[00:44:57.520 --> 00:44:59.160]   things than other companies.
[00:44:59.160 --> 00:45:03.200]   It's really hard to extricate your life from any of these big places.
[00:45:03.200 --> 00:45:06.520]   And I would argue that you don't necessarily, it's not necessarily beneficial to.
[00:45:06.520 --> 00:45:07.520]   You know what I mean?
[00:45:07.520 --> 00:45:12.800]   I don't know if my life would be better if I didn't have YouTube or if I didn't have
[00:45:12.800 --> 00:45:13.800]   Google Maps.
[00:45:13.800 --> 00:45:14.800]   Like, I actually think that-
[00:45:14.800 --> 00:45:17.040]   But it would be better if you didn't have Facebook.
[00:45:17.040 --> 00:45:22.240]   Like the stats are through the roof of your levels of happiness or levels of self-loathing
[00:45:22.240 --> 00:45:23.240]   go down.
[00:45:23.240 --> 00:45:24.240]   I don't necessarily-
[00:45:24.240 --> 00:45:25.240]   I spoke with this-
[00:45:25.240 --> 00:45:26.240]   I don't necessarily disagree.
[00:45:26.240 --> 00:45:31.440]   I don't necessarily disagree with maybe Facebook, but I do get things out of Instagram.
[00:45:31.440 --> 00:45:32.440]   I do enjoy Instagram.
[00:45:32.440 --> 00:45:37.920]   I do appreciate for all of its many foibles, even Facebook Messenger as a way to communicate
[00:45:37.920 --> 00:45:38.920]   with people.
[00:45:38.920 --> 00:45:45.680]   I've been told that that attitude is really first world privilege speaking.
[00:45:45.680 --> 00:45:48.920]   There are many countries where you have to have WhatsApp.
[00:45:48.920 --> 00:45:49.920]   You're right.
[00:45:49.920 --> 00:45:50.920]   You have to have Facebook.
[00:45:50.920 --> 00:45:54.000]   I don't know if anybody has to have Instagram.
[00:45:54.000 --> 00:45:55.000]   But-
[00:45:55.000 --> 00:45:56.000]   Everyone has to have Facebook.
[00:45:56.000 --> 00:45:57.000]   Sorry, I'm just going to-
[00:45:57.000 --> 00:45:58.000]   It's how you stay in touch.
[00:45:58.000 --> 00:46:00.000]   I'm going to tell you, because not only-
[00:46:00.000 --> 00:46:02.000]   You don't have to have Facebook.
[00:46:02.000 --> 00:46:03.000]   There are other ways to-
[00:46:03.000 --> 00:46:04.000]   No.
[00:46:04.000 --> 00:46:05.000]   No, no, no, no.
[00:46:05.000 --> 00:46:07.240]   We stay in touch with other ways to that.
[00:46:07.240 --> 00:46:08.240]   I disagree.
[00:46:08.240 --> 00:46:09.240]   I agree with you.
[00:46:09.240 --> 00:46:10.240]   I think this is a first world privilege problem.
[00:46:10.240 --> 00:46:14.020]   I think it's easy for us to say we cannot have these things because we have the money
[00:46:14.020 --> 00:46:16.620]   and the privilege to communicate with people in other ways.
[00:46:16.620 --> 00:46:20.440]   If you live in a country, for instance, WhatsApp, where this is how people communicate, because
[00:46:20.440 --> 00:46:24.480]   the alternative would be really expensive phone systems, where they charge you per SMS
[00:46:24.480 --> 00:46:28.720]   message, and where you might have to have multiple phone services because of how rolling
[00:46:28.720 --> 00:46:29.720]   systems work.
[00:46:29.720 --> 00:46:33.480]   You don't have electricity all the time because of rolling outages.
[00:46:33.480 --> 00:46:37.080]   You're using the internet and you're using these tools to communicate.
[00:46:37.080 --> 00:46:41.920]   For better or worse, the main methods of communication are owned by these big companies.
[00:46:41.920 --> 00:46:46.900]   Facebook, for instance, in India, refused to let them do this, but they go into places
[00:46:46.900 --> 00:46:51.300]   and they say you can have access to the internet and we'll subsidize a certain amount of it.
[00:46:51.300 --> 00:46:54.860]   If we're going to try to- I'm not going to tell another person in another country that
[00:46:54.860 --> 00:46:59.740]   they can't have access to the internet, even if it's through Facebook's browser or whatever,
[00:46:59.740 --> 00:47:01.780]   because Facebook is evil.
[00:47:01.780 --> 00:47:04.180]   But the alternative is they have no access whatsoever.
[00:47:04.180 --> 00:47:05.460]   I'm not going to say that to someone.
[00:47:05.460 --> 00:47:06.460]   I'm not going to make that decision.
[00:47:06.460 --> 00:47:07.460]   The legend-
[00:47:07.460 --> 00:47:10.460]   Let's say that real places where we're talking about first world problems, they probably
[00:47:10.460 --> 00:47:14.440]   can't afford to have a phone or to have internet access at all to that.
[00:47:14.440 --> 00:47:15.440]   So it's already-
[00:47:15.440 --> 00:47:18.440]   There's people in the middle too.
[00:47:18.440 --> 00:47:21.880]   Of the digital divide, there are other ways to be able to do this.
[00:47:21.880 --> 00:47:26.820]   And in the end, the only way that our privacy will actually be secure is if instead of giving
[00:47:26.820 --> 00:47:31.720]   Facebook a slap on the hand, which is like a small find that's really pocket change for
[00:47:31.720 --> 00:47:36.900]   them, would be if they invade other people's privacy without being clear and concise with
[00:47:36.900 --> 00:47:39.200]   that manner would be criminal charges to that.
[00:47:39.200 --> 00:47:43.800]   Truly, if citizens united and corporations are people, then they should be tried as people
[00:47:43.800 --> 00:47:44.800]   as well.
[00:47:44.800 --> 00:47:50.040]   But they're acting the way that all the other big corporations, all the oil companies,
[00:47:50.040 --> 00:47:53.040]   all the gas companies, they're doing the bad thing.
[00:47:53.040 --> 00:47:55.240]   But that doesn't make it, let's be honest, it's all wrong.
[00:47:55.240 --> 00:47:56.240]   That doesn't-
[00:47:56.240 --> 00:47:57.240]   It's all terrible.
[00:47:57.240 --> 00:48:01.040]   I don't like that argument to say that everyone else does it, so this makes it okay.
[00:48:01.040 --> 00:48:02.040]   I think that people-
[00:48:02.040 --> 00:48:03.040]   No, but that's not-
[00:48:03.040 --> 00:48:04.040]   That's our goal here.
[00:48:04.040 --> 00:48:06.720]   But it means if you're going to hold one company to a criminal account, you should
[00:48:06.720 --> 00:48:09.120]   do all of them, and I don't disagree with that in theory.
[00:48:09.120 --> 00:48:10.120]   I agree, yeah.
[00:48:10.120 --> 00:48:11.120]   Sure.
[00:48:11.120 --> 00:48:12.120]   I think it's hard to do.
[00:48:12.120 --> 00:48:14.480]   I also feel like it's hard to say again, as that's in a position of privilege, to say
[00:48:14.480 --> 00:48:20.400]   what should be what other people who don't have our means, what decisions they should
[00:48:20.400 --> 00:48:21.960]   make.
[00:48:21.960 --> 00:48:24.480]   And I feel also weird and croaching.
[00:48:24.480 --> 00:48:30.240]   I got an argument with some of the conference I was speaking at back in November, who was
[00:48:30.240 --> 00:48:34.640]   kind of talking about we should all get rid of our smartphones and stop doing these things.
[00:48:34.640 --> 00:48:38.680]   And I said, okay, well, what about people in other countries who their lives have been
[00:48:38.680 --> 00:48:41.440]   demonstrably made better because of this technology?
[00:48:41.440 --> 00:48:43.640]   And his argument was, well, it's fine for them.
[00:48:43.640 --> 00:48:47.360]   And I find that really condescending for us to be like, well, if we have the means and
[00:48:47.360 --> 00:48:51.920]   we're good enough, we should get rid of this because it's better for our privacy and our
[00:48:51.920 --> 00:48:52.920]   means.
[00:48:52.920 --> 00:48:56.120]   But people who don't have access to anything else, oh, we should let them continue to be
[00:48:56.120 --> 00:48:57.120]   exploited.
[00:48:57.120 --> 00:48:58.280]   I feel like that's-
[00:48:58.280 --> 00:48:59.280]   I don't know.
[00:48:59.280 --> 00:49:00.480]   I have a problem with that kind of mixed message.
[00:49:00.480 --> 00:49:01.480]   It's either-
[00:49:01.480 --> 00:49:03.280]   It should be okay for everybody or it shouldn't be okay.
[00:49:03.280 --> 00:49:04.280]   Yeah.
[00:49:04.280 --> 00:49:08.720]   Another problem, though, is that Facebook basically has a monopoly on social networking,
[00:49:08.720 --> 00:49:10.720]   on these big groups.
[00:49:10.720 --> 00:49:13.760]   So you're going to use the service where your friends are, where your families are.
[00:49:13.760 --> 00:49:20.360]   I think it's easy for especially Western first world folks to just give up on Facebook
[00:49:20.360 --> 00:49:22.440]   and you'll have other ways to interact.
[00:49:22.440 --> 00:49:27.840]   But yeah, if you're in India and everybody's on WhatsApp, you can't really not be there.
[00:49:27.840 --> 00:49:32.440]   Otherwise you're missing out on a whole lot of social conversations.
[00:49:32.440 --> 00:49:34.680]   We need more real competition for Facebook.
[00:49:34.680 --> 00:49:38.960]   That's the biggest problem because Facebook's power, it's a superpower.
[00:49:38.960 --> 00:49:42.080]   It has the power of a country.
[00:49:42.080 --> 00:49:45.240]   I think that's kind of scary seeing what they do with it.
[00:49:45.240 --> 00:49:49.080]   In some ways they have more power than a lot of countries and that's the thing.
[00:49:49.080 --> 00:49:51.280]   These companies are too large to fail.
[00:49:51.280 --> 00:49:54.000]   That's a huge problem for everyone that's out there.
[00:49:54.000 --> 00:49:58.520]   All of us, that becomes a really huge problem to that, especially when they have been caught
[00:49:58.520 --> 00:50:00.000]   doing such nefarious things.
[00:50:00.000 --> 00:50:04.960]   I think the U has done a better job of trying to take a look into this and police that.
[00:50:04.960 --> 00:50:08.400]   The cutest part was when Facebook got caught with their emails and they're like, "No,
[00:50:08.400 --> 00:50:09.640]   please don't read our emails.
[00:50:09.640 --> 00:50:10.640]   Those are private."
[00:50:10.640 --> 00:50:11.640]   They did anyways.
[00:50:11.640 --> 00:50:12.640]   Thank God.
[00:50:12.640 --> 00:50:14.680]   But I think that we need to have that discussion.
[00:50:14.680 --> 00:50:17.640]   I think that we need to be going into it and I think that we really need to take a look
[00:50:17.640 --> 00:50:24.000]   at how do we be able to police companies that have huge lobbyists and they end up being
[00:50:24.000 --> 00:50:26.400]   able to control the governments that want to police them?
[00:50:26.400 --> 00:50:34.560]   Can the best solution, an on-governmental solution, be to provide a competitor?
[00:50:34.560 --> 00:50:35.560]   That's one thing.
[00:50:35.560 --> 00:50:37.080]   That would be great.
[00:50:37.080 --> 00:50:43.120]   I think time and again, what has helped us, looking across so many other industries, how
[00:50:43.120 --> 00:50:44.480]   is driving regulated?
[00:50:44.480 --> 00:50:45.800]   How is alcohol regulated?
[00:50:45.800 --> 00:50:49.000]   How are a lot of the major things we deal with?
[00:50:49.000 --> 00:50:51.920]   It's not the solution for everything.
[00:50:51.920 --> 00:50:56.080]   I think it's a combination of regulation plus competition that creates a healthy...
[00:50:56.080 --> 00:50:58.520]   You're trying to write a regulation for this though.
[00:50:58.520 --> 00:50:59.520]   What is that look?
[00:50:59.520 --> 00:51:00.520]   I don't know if it's hard.
[00:51:00.520 --> 00:51:01.520]   I don't know if it's hard.
[00:51:01.520 --> 00:51:02.520]   I think that we need to talk about it.
[00:51:02.520 --> 00:51:05.000]   We need to educate people that are becoming lawmakers.
[00:51:05.000 --> 00:51:10.440]   I think that, again, an education and technology and how is it used and why is it used is really
[00:51:10.440 --> 00:51:15.760]   important because a lot of maybe older lawmakers are people that are not that technically literate
[00:51:15.760 --> 00:51:18.440]   or then writing laws for this, which doesn't make sense.
[00:51:18.440 --> 00:51:19.440]   Yeah.
[00:51:19.440 --> 00:51:20.440]   And it's a problem.
[00:51:20.440 --> 00:51:21.440]   Definitely.
[00:51:21.440 --> 00:51:22.440]   You use people who don't know.
[00:51:22.440 --> 00:51:28.360]   It's a choice which I thought was very interesting not to allow internet.org.
[00:51:28.360 --> 00:51:33.280]   Facebook's attempt to provide internet access to areas that were completely non-served
[00:51:33.280 --> 00:51:35.360]   or underserved.
[00:51:35.360 --> 00:51:38.400]   The government of India refused them that license.
[00:51:38.400 --> 00:51:44.920]   I was told by some that it was seen almost as if the Raj had come back, if the English
[00:51:44.920 --> 00:51:47.680]   oppressor had come back and said, "It's okay.
[00:51:47.680 --> 00:51:52.040]   Let us handle it because, of course, the problem with internet.org was it was pretty much just
[00:51:52.040 --> 00:51:53.040]   Facebook.
[00:51:53.040 --> 00:51:54.040]   It had Wikipedia.
[00:51:54.040 --> 00:51:57.040]   But it was like, we'll provide you with Facebook."
[00:51:57.040 --> 00:51:59.240]   Also, they're tracking everything.
[00:51:59.240 --> 00:52:00.720]   I mean, so it's going to be--
[00:52:00.720 --> 00:52:06.880]   I thought that was interesting that they had the guts to do that is tell them, right?
[00:52:06.880 --> 00:52:11.160]   One thousand reasons, especially since Facebook, they have done things like that in another
[00:52:11.160 --> 00:52:12.160]   country.
[00:52:12.160 --> 00:52:14.760]   It's not to the extent of what they wanted to do with internet.org, but they do do it
[00:52:14.760 --> 00:52:16.600]   other places.
[00:52:16.600 --> 00:52:21.120]   A lot of phone carriers in other countries, you'll pay a certain amount of money for access
[00:52:21.120 --> 00:52:24.160]   to data, and the data, you'll get free Facebook.
[00:52:24.160 --> 00:52:27.000]   But other types of data will charge money.
[00:52:27.000 --> 00:52:30.240]   That's a huge incentive, obviously, for people to use Facebook.
[00:52:30.240 --> 00:52:36.920]   At the time, there was a lot of incredulity that India would turn it down.
[00:52:36.920 --> 00:52:37.920]   But I agree with it.
[00:52:37.920 --> 00:52:42.360]   I think it's a really ballsy move to say, "No, we're not going to allow that."
[00:52:42.360 --> 00:52:46.320]   The problem with competition for Facebook is anyone becomes competition to Facebook.
[00:52:46.320 --> 00:52:47.720]   They just buy them out.
[00:52:47.720 --> 00:52:51.000]   That's where regulations could help.
[00:52:51.000 --> 00:52:55.560]   Or worse, if you, like Evan Spiegel at Snapchat, decline.
[00:52:55.560 --> 00:52:58.440]   They'll drive you out.
[00:52:58.440 --> 00:53:03.000]   Snapchat is effectively being destroyed bit by bit by Facebook.
[00:53:03.000 --> 00:53:06.600]   Yeah, but I think that a lot of them is on Snapchat, too.
[00:53:06.600 --> 00:53:07.600]   Is it?
[00:53:07.600 --> 00:53:08.600]   Yeah.
[00:53:08.600 --> 00:53:11.720]   I think their product hasn't kept up.
[00:53:11.720 --> 00:53:17.560]   I think that a lot of-- they had famously very bad privacy and security issues.
[00:53:17.560 --> 00:53:20.320]   They weren't really focusing on.
[00:53:20.320 --> 00:53:23.320]   And look, did Facebook blatantly copy?
[00:53:23.320 --> 00:53:24.800]   Did Instagram blatantly copy stories?
[00:53:24.800 --> 00:53:25.800]   Yeah, they did.
[00:53:25.800 --> 00:53:26.800]   And made no qualms about it.
[00:53:26.800 --> 00:53:31.000]   I actually kind of appreciate the honesty in that.
[00:53:31.000 --> 00:53:37.240]   But I feel like, you know, snaps decline while certainly not helped by the fact that Instagram's
[00:53:37.240 --> 00:53:39.160]   product was a copy.
[00:53:39.160 --> 00:53:43.160]   The reality also is that Instagram not just copied the feature, but made it better and
[00:53:43.160 --> 00:53:44.160]   continued--
[00:53:44.160 --> 00:53:45.160]   It's tough, though.
[00:53:45.160 --> 00:53:50.360]   --as Snapchat to compete against something as integrated as Facebook WhatsApp Instagram,
[00:53:50.360 --> 00:53:51.360]   right?
[00:53:51.360 --> 00:53:52.360]   It absolutely is.
[00:53:52.360 --> 00:53:56.920]   And in fact, according to the New York Times last week, they're planning to integrate the
[00:53:56.920 --> 00:54:02.080]   platforms completely so that an advertiser could buy all three so that the information
[00:54:02.080 --> 00:54:07.000]   they glean from one will help the other.
[00:54:07.000 --> 00:54:11.320]   That is exactly-- in fact, one California legislator said, "This is why we should have
[00:54:11.320 --> 00:54:16.080]   been much more cautious about allowing Facebook to buy WhatsApp and Instagram."
[00:54:16.080 --> 00:54:17.080]   Well, completely.
[00:54:17.080 --> 00:54:20.800]   I mean, I think that it would have been impossible to make an argument, an antitrust argument
[00:54:20.800 --> 00:54:21.800]   for Instagram.
[00:54:21.800 --> 00:54:23.320]   It was a billion dollars.
[00:54:23.320 --> 00:54:25.240]   The best billion dollars Facebook ever spent.
[00:54:25.240 --> 00:54:28.960]   But at the time that it launched, you know, it didn't even-- I don't really think the Android
[00:54:28.960 --> 00:54:30.160]   app had officially come out yet.
[00:54:30.160 --> 00:54:33.080]   It wasn't even fully done when the acquisition happened.
[00:54:33.080 --> 00:54:34.080]   It was small.
[00:54:34.080 --> 00:54:35.080]   You know what I mean?
[00:54:35.080 --> 00:54:38.680]   Facebook was actually working on its own competitor that it released a few weeks after or a
[00:54:38.680 --> 00:54:42.560]   few days after it bought Instagram that was terrible.
[00:54:42.560 --> 00:54:47.480]   I remember this because I wrote a story in the headline with something like, you know,
[00:54:47.480 --> 00:54:51.840]   Facebook, whatever, proves why Instagram was worth a billion dollars.
[00:54:51.840 --> 00:54:52.840]   And--
[00:54:52.840 --> 00:54:53.840]   That's happened again, by the way.
[00:54:53.840 --> 00:54:54.840]   Instagram TV, Facebook.
[00:54:54.840 --> 00:54:55.840]   Totally.
[00:54:55.840 --> 00:54:56.840]   Completely.
[00:54:56.840 --> 00:54:57.840]   Facebook Watch.
[00:54:57.840 --> 00:54:58.840]   Remember that?
[00:54:58.840 --> 00:54:59.840]   This was their YouTube competitor.
[00:54:59.840 --> 00:55:00.840]   Yeah, exactly.
[00:55:00.840 --> 00:55:01.840]   So--
[00:55:01.840 --> 00:55:02.840]   Exactly.
[00:55:02.840 --> 00:55:03.840]   Yeah.
[00:55:03.840 --> 00:55:06.840]   But what I mean, though, is that-- so you're not wrong, but I think that Instagram-- well,
[00:55:06.840 --> 00:55:10.640]   what I was going to say that was like, Instagram, I don't think there was any antitrust thing
[00:55:10.640 --> 00:55:11.640]   at all.
[00:55:11.640 --> 00:55:15.480]   But I do think that for WhatsApp, when you know, when you're spending $20 billion and
[00:55:15.480 --> 00:55:19.160]   a big reason they spent $20 billion was because Google-- well, no, because Google was going
[00:55:19.160 --> 00:55:21.320]   to buy it and they had to outbid Google.
[00:55:21.320 --> 00:55:25.200]   I mean, that should have been looked at with a lot more scrutiny when you looked at the
[00:55:25.200 --> 00:55:30.200]   overlap that Facebook Messenger, which hadn't been separated in the same way and was already
[00:55:30.200 --> 00:55:33.840]   gearing up to compete with WhatsApp before they even bought the product.
[00:55:33.840 --> 00:55:35.720]   Like, I think the lawmaker is right.
[00:55:35.720 --> 00:55:39.640]   That's when the time for some of these things has passed us.
[00:55:39.640 --> 00:55:43.840]   And if we want to prevent this sort of consolidation in the future and this sort of overarching
[00:55:43.840 --> 00:55:50.440]   stuff in the future, then you have to look at when you're approving M&A stuff.
[00:55:50.440 --> 00:55:52.600]   But nobody cared when it happened.
[00:55:52.600 --> 00:55:54.320]   We didn't understand it.
[00:55:54.320 --> 00:56:00.840]   And they didn't have enough knowledge to be able to make a viable argument and discuss
[00:56:00.840 --> 00:56:01.840]   that properly.
[00:56:01.840 --> 00:56:04.400]   And I think that that's a problem with a lot of the lawmakers that are out there.
[00:56:04.400 --> 00:56:10.360]   They just don't understand enough to be able to make really good laws and good arguments
[00:56:10.360 --> 00:56:13.400]   when they're up there looking at it and taking a look.
[00:56:13.400 --> 00:56:16.040]   And again, Facebook pays them a lot of money.
[00:56:16.040 --> 00:56:20.160]   Like if you take a look at how much money Facebook has given to different people in
[00:56:20.160 --> 00:56:22.800]   the government, it's a lot of their money.
[00:56:22.800 --> 00:56:24.800]   And there's a reason for that.
[00:56:24.800 --> 00:56:26.640]   Let's take a break.
[00:56:26.640 --> 00:56:27.640]   Great panel.
[00:56:27.640 --> 00:56:29.120]   You guys are fantastic.
[00:56:29.120 --> 00:56:34.200]   On fire today, Georgia Dow from-- I guess senior editor I'm more, right?
[00:56:34.200 --> 00:56:35.200]   I could say that.
[00:56:35.200 --> 00:56:37.200]   Yeah, sometimes right, sometimes.
[00:56:37.200 --> 00:56:38.680]   Sometimes once in a while.
[00:56:38.680 --> 00:56:41.320]   Also anxiety-videos.com.
[00:56:41.320 --> 00:56:42.520]   We just love having you on Georgia.
[00:56:42.520 --> 00:56:44.760]   It's great to see you.
[00:56:44.760 --> 00:56:46.800]   Also a door, Christina.
[00:56:46.800 --> 00:56:47.800]   It's wonderful.
[00:56:47.800 --> 00:56:51.280]   I have Christina Warren, senior cloud developer advocate at Microsoft.
[00:56:51.280 --> 00:56:53.480]   You've known her as film girl.
[00:56:53.480 --> 00:56:55.760]   She's also within-- what shows do you do?
[00:56:55.760 --> 00:56:57.760]   You do that great show.
[00:56:57.760 --> 00:56:59.000]   Did you rock it?
[00:56:59.000 --> 00:57:00.680]   With Rocket with Simone.
[00:57:00.680 --> 00:57:07.680]   And the former candidate for Congress, Brianna-- although Brianna says she's still running,
[00:57:07.680 --> 00:57:08.680]   right?
[00:57:08.680 --> 00:57:09.680]   No, no.
[00:57:09.680 --> 00:57:10.680]   She's running for 2020.
[00:57:10.680 --> 00:57:11.680]   She said--
[00:57:11.680 --> 00:57:12.680]   2020.
[00:57:12.680 --> 00:57:13.680]   Here comes Brianna, 2020.
[00:57:13.680 --> 00:57:15.520]   That's a great show rocket on relay.fm.
[00:57:15.520 --> 00:57:17.360]   And in fact, you're all podcasters.
[00:57:17.360 --> 00:57:18.360]   You're all podcasters.
[00:57:18.360 --> 00:57:19.360]   Yeah.
[00:57:19.360 --> 00:57:23.400]   Devindra Hardwar/film is the fan senior editor in gadget, by the way.
[00:57:23.400 --> 00:57:25.560]   I don't think I mentioned-- it's funny.
[00:57:25.560 --> 00:57:27.920]   I don't think I mentioned any of your actual titles earlier.
[00:57:27.920 --> 00:57:30.560]   So I'll give you credit now.
[00:57:30.560 --> 00:57:34.200]   We just love having you on all three of you.
[00:57:34.200 --> 00:57:36.080]   You're always great on fire today.
[00:57:36.080 --> 00:57:37.080]   Always fun.
[00:57:37.080 --> 00:57:40.080]   Our show today brought to you by Thousand Eyes.
[00:57:40.080 --> 00:57:46.160]   This is a brand new sponsor and one I'm so happy to be on because I really think Thousand
[00:57:46.160 --> 00:57:49.360]   Eyes offers a solution that a lot of you don't even know exists.
[00:57:49.360 --> 00:57:55.680]   If you run a network, if you are online-- this is not a consumer product.
[00:57:55.680 --> 00:57:56.680]   This is a business product.
[00:57:56.680 --> 00:58:02.240]   If you have an online presence and stuff goes wrong, it's often impossible to figure out
[00:58:02.240 --> 00:58:05.960]   where it's going wrong.
[00:58:05.960 --> 00:58:09.120]   And I think many of us just say, well, we'll never know.
[00:58:09.120 --> 00:58:10.920]   What's happening in the network?
[00:58:10.920 --> 00:58:12.880]   It's a puffy little cloud.
[00:58:12.880 --> 00:58:14.400]   I don't know.
[00:58:14.400 --> 00:58:16.280]   Thousand Eyes knows.
[00:58:16.280 --> 00:58:22.720]   With Thousand Eyes, you get an immediate unmatched view of all the networks, all the dependencies
[00:58:22.720 --> 00:58:25.480]   that impact your users' digital experience.
[00:58:25.480 --> 00:58:30.680]   The problem is they should have named it a million eyes because it isn't a thousand.
[00:58:30.680 --> 00:58:36.000]   It's many hundreds of thousands of sensors at every point on the network from the edge
[00:58:36.000 --> 00:58:41.800]   on in that can tell you exactly where your traffic is and where it's going and what's
[00:58:41.800 --> 00:58:42.800]   going on.
[00:58:42.800 --> 00:58:47.480]   As we move to the cloud-- and I think cloud is so great for business, so many people are
[00:58:47.480 --> 00:58:49.320]   moving to the cloud.
[00:58:49.320 --> 00:58:50.880]   There are a lot of benefits to the cloud.
[00:58:50.880 --> 00:58:54.880]   You gain in that agility, but you increase in risk.
[00:58:54.880 --> 00:58:56.480]   You lose some control.
[00:58:56.480 --> 00:59:00.320]   And as I said, it's kind of mysterious sometimes.
[00:59:00.320 --> 00:59:05.200]   When your cloud or app or service goes down, do you know really what's going on?
[00:59:05.200 --> 00:59:08.960]   As you scramble to find the root cause of the problem, you're losing revenue.
[00:59:08.960 --> 00:59:10.640]   Your employee productivity goes down.
[00:59:10.640 --> 00:59:12.680]   Your brand is damaged.
[00:59:12.680 --> 00:59:14.280]   You need Thousand Eyes.
[00:59:14.280 --> 00:59:19.400]   You need instant visibility into the entire service delivery path from the cloud to your
[00:59:19.400 --> 00:59:22.080]   end user, including the portions you don't own or control.
[00:59:22.080 --> 00:59:27.120]   I think I am kind of on a mission because I am a believer in Thousand Eyes.
[00:59:27.120 --> 00:59:30.720]   I know these guys well and I think you need to know about them.
[00:59:30.720 --> 00:59:32.800]   It's unlike anything you've seen before.
[00:59:32.800 --> 00:59:39.800]   It's cloud-based software built to help organizations like yours do the cloud write a massive array
[00:59:39.800 --> 00:59:45.520]   of vantage points spanning the global internet, everywhere, China to cloud providers, even
[00:59:45.520 --> 00:59:48.160]   the Wi-Fi in your local coffee shop.
[00:59:48.160 --> 00:59:54.000]   This is not passive old school IT monitoring, which basically lives in a silo, can only see
[00:59:54.000 --> 00:59:55.960]   inside the data center.
[00:59:55.960 --> 01:00:00.760]   Thousand Eyes' unique path tracing technology expands beyond boundaries, allowing you to
[01:00:00.760 --> 01:00:07.160]   see, understand, and improve the experience for all your apps, services, and websites.
[01:00:07.160 --> 01:00:10.160]   Regain control ensures the best possible digital experience for your customers and
[01:00:10.160 --> 01:00:11.160]   employees.
[01:00:11.160 --> 01:00:13.080]   You've got to find out about Thousand Eyes.
[01:00:13.080 --> 01:00:14.440]   Big banks know about it.
[01:00:14.440 --> 01:00:19.000]   SaaS companies enterprises, the world's largest, fastest growing brands, rely on Thousand Eyes
[01:00:19.000 --> 01:00:21.200]   to do the cloud and do it right.
[01:00:21.200 --> 01:00:23.080]   But maybe you don't know about it.
[01:00:23.080 --> 01:00:24.800]   That's why I'm on a mission.
[01:00:24.800 --> 01:00:32.240]   Visit Thousand Eyes, all spelled out T-H-O-U-S-A-N-D-E-Y-E-S dot com slash twit to see what you've
[01:00:32.240 --> 01:00:33.240]   been missing.
[01:00:33.240 --> 01:00:35.920]   They're offering right now a handy book that everybody should have if you're thinking
[01:00:35.920 --> 01:00:41.080]   about cloud migration, five cloud migration challenges you shouldn't ignore.
[01:00:41.080 --> 01:00:45.280]   But there is a lot of data on the site, a way I just did an event with them that was just
[01:00:45.280 --> 01:00:51.520]   fantastic about where traffic from the big cloud providers get stopped, where it gets
[01:00:51.520 --> 01:00:52.520]   slowed down.
[01:00:52.520 --> 01:00:53.920]   They know all this stuff.
[01:00:53.920 --> 01:00:56.360]   Thousandeyes.com slash twit.
[01:00:56.360 --> 01:00:58.840]   Visit Thousandeyes.com slash twit.
[01:00:58.840 --> 01:01:02.320]   Thousand Eyes lets you thrive in a connected world.
[01:01:02.320 --> 01:01:06.560]   You're going to hear more about them on our shows in the future because I'm kind of a
[01:01:06.560 --> 01:01:10.120]   Thousand Eyes buff, a fan.
[01:01:10.120 --> 01:01:15.240]   I didn't mention this, but I should probably mention it that, you know, and it's funny
[01:01:15.240 --> 01:01:19.480]   because we've been just saying how, oh, government needs to step in.
[01:01:19.480 --> 01:01:24.400]   Facebook FaceTime's bug will be investigated by the New York Attorney General, Articia
[01:01:24.400 --> 01:01:26.400]   James and Governor Andrew Cuomo.
[01:01:26.400 --> 01:01:31.000]   They want, they, yeah, see this is, see, but then we just were saying, oh, I know what
[01:01:31.000 --> 01:01:34.640]   we do because we, I think, but this is exactly right.
[01:01:34.640 --> 01:01:35.640]   Oh, God.
[01:01:35.640 --> 01:01:36.800]   Well, no, because we need, we need.
[01:01:36.800 --> 01:01:42.680]   I think to Georgia's initial point, which is that we need more literacy across, you know,
[01:01:42.680 --> 01:01:44.760]   legislators about tech, we need better tech.
[01:01:44.760 --> 01:01:49.480]   Well, I think with people like, people like the right things, people like Alexandra,
[01:01:49.480 --> 01:01:57.000]   Acasio or Cortez and other new young legislators coming in and more all the time.
[01:01:57.000 --> 01:02:00.000]   I mean, she, it's to see her use social media.
[01:02:00.000 --> 01:02:01.480]   She gets it.
[01:02:01.480 --> 01:02:02.480]   She understands.
[01:02:02.480 --> 01:02:04.120]   I think we need more people.
[01:02:04.120 --> 01:02:06.920]   She's one of the few, we need, yeah, a lot more.
[01:02:06.920 --> 01:02:08.000]   Well, we will.
[01:02:08.000 --> 01:02:09.800]   I think this is a process.
[01:02:09.800 --> 01:02:14.240]   Remember my other issue, I think government's never been designed to move as fast as industry.
[01:02:14.240 --> 01:02:15.240]   That's intentional.
[01:02:15.240 --> 01:02:16.240]   Absolutely.
[01:02:16.240 --> 01:02:20.240]   What, what the worst thing in the world would be a super efficient government.
[01:02:20.240 --> 01:02:21.400]   Right?
[01:02:21.400 --> 01:02:27.000]   The founders knew that that's why we have three competing branches.
[01:02:27.000 --> 01:02:28.400]   There's oversight.
[01:02:28.400 --> 01:02:35.760]   I mean, it's intentionally deliberate, which means it's a little bit of a mismatch to a
[01:02:35.760 --> 01:02:40.960]   very fast moving world, not just by the way, not just info tech, but get ready for the
[01:02:40.960 --> 01:02:41.960]   genetic revolution.
[01:02:41.960 --> 01:02:42.960]   Yeah.
[01:02:42.960 --> 01:02:43.960]   It's going to be rough.
[01:02:43.960 --> 01:02:51.040]   I mean, if you think they don't understand, you know, it's in bites, we don't get to DNA
[01:02:51.040 --> 01:02:52.040]   and CRISPR.
[01:02:52.040 --> 01:02:57.920]   And by the way, in that respect, I know we roll our eyes when we see, you know, government
[01:02:57.920 --> 01:02:59.640]   is investigating something like this.
[01:02:59.640 --> 01:03:01.040]   This was still a serious flaw.
[01:03:01.040 --> 01:03:05.280]   I still think it's worth, if the words of the New York state taking a look at this and
[01:03:05.280 --> 01:03:09.920]   just being like, doing the rundown, seeing how this happened and how Apple didn't notice,
[01:03:09.920 --> 01:03:11.360]   I have no problem with that.
[01:03:11.360 --> 01:03:15.360]   We should be doing this for every security flaw for every data breach.
[01:03:15.360 --> 01:03:16.480]   I'm still like reeling.
[01:03:16.480 --> 01:03:19.160]   What was the widespread one that was shutting down?
[01:03:19.160 --> 01:03:20.280]   Wanna cry.
[01:03:20.280 --> 01:03:25.400]   Like after all the Wanna cry stuff too, which revealed that hospitals across the UK just
[01:03:25.400 --> 01:03:29.440]   had bad IT and that left them vulnerable to a lot of this.
[01:03:29.440 --> 01:03:31.440]   Those are problems we could fix.
[01:03:31.440 --> 01:03:32.800]   Like those are, well, that's what I'm saying.
[01:03:32.800 --> 01:03:38.360]   I don't think regulation is the fix for everything, but I think ensuring that a company is keeping
[01:03:38.360 --> 01:03:44.800]   their software up to date, you know, is securely patched as much as an OS maker has available.
[01:03:44.800 --> 01:03:47.960]   Like I think that sort of thing should be standardized.
[01:03:47.960 --> 01:03:51.480]   And if somebody is involving that should be punished for it because otherwise, yeah,
[01:03:51.480 --> 01:03:55.720]   you have secure so that you have critical systems like hospitals that could go down because
[01:03:55.720 --> 01:03:57.200]   somebody forgot a Windows update.
[01:03:57.200 --> 01:04:01.160]   Did you see this was actually it's old news, but I just saw it.
[01:04:01.160 --> 01:04:09.040]   Mondelez, which was bit by Petra, the big food company, had insurance with Zurich.
[01:04:09.040 --> 01:04:12.360]   Zurich said, oh, no, not Petra.
[01:04:12.360 --> 01:04:15.360]   The ransomware was an act of war.
[01:04:15.360 --> 01:04:18.480]   You're not covered.
[01:04:18.480 --> 01:04:20.680]   You're not covered.
[01:04:20.680 --> 01:04:27.720]   Mondelez is suing Zurich Insurance Group for $100 million in damages.
[01:04:27.720 --> 01:04:29.600]   It is an act of war, isn't it?
[01:04:29.600 --> 01:04:30.600]   Yeah.
[01:04:30.600 --> 01:04:39.160]   I mean, was launched against the world by national actors.
[01:04:39.160 --> 01:04:41.640]   Of course, the technology came from the NSA.
[01:04:41.640 --> 01:04:44.800]   Secure your system.
[01:04:44.800 --> 01:04:45.800]   Wow.
[01:04:45.800 --> 01:04:54.800]   That'll be an interesting lawsuit to follow speaking of court actions on Friday oral arguments.
[01:04:54.800 --> 01:05:00.840]   Very important oral arguments in a lawsuit challenging the repeal of net neutrality.
[01:05:00.840 --> 01:05:07.000]   The United States Court of Appeals for the District of Columbia Circuit is hearing these
[01:05:07.000 --> 01:05:09.920]   arguments, the plate is in the suit led by Mozilla.
[01:05:09.920 --> 01:05:17.880]   The Mozilla, you go Mozilla, or real and supported by 22 state attorneys general say the FCC lacked
[01:05:17.880 --> 01:05:24.120]   a sound legal reason for scrapping the title to regulations.
[01:05:24.120 --> 01:05:27.720]   The government will argue it's expected according to the New York Times that the rules were
[01:05:27.720 --> 01:05:32.880]   repealed because of the burden they provide they imposed on broadband providers like Verizon
[01:05:32.880 --> 01:05:34.720]   and Comcast.
[01:05:34.720 --> 01:05:40.720]   I'd be very interesting to hear the arguments and see what happens.
[01:05:40.720 --> 01:05:43.800]   This is not going to be a quick battle.
[01:05:43.800 --> 01:05:46.280]   This will go for years, no doubt.
[01:05:46.280 --> 01:05:50.640]   I think like with, by the way, like 5G is kind of a bigger thing like our all the Huawei
[01:05:50.640 --> 01:05:54.080]   stuff right now is centered around 5G and 5G infrastructure.
[01:05:54.080 --> 01:05:57.440]   And I think now that this conversation is happening in the government, maybe more people
[01:05:57.440 --> 01:06:02.480]   are paying attention to what this future infrastructure is and what it means and where it comes from.
[01:06:02.480 --> 01:06:06.280]   Maybe now they'll pay more attention to net neutrality and what that means too.
[01:06:06.280 --> 01:06:10.800]   Let's talk about Huawei because the news is bad.
[01:06:10.800 --> 01:06:14.440]   The United States has continued to prosecute the Huawei vice president who's currently
[01:06:14.440 --> 01:06:19.280]   in Canada but is being held for extradition for spying.
[01:06:19.280 --> 01:06:26.560]   There's a Polish Huawei executives but arrested in Poland for spying.
[01:06:26.560 --> 01:06:31.960]   Huawei which is a Chinese maker of it's the number two although it goes back and forth
[01:06:31.960 --> 01:06:32.960]   with Apple.
[01:06:32.960 --> 01:06:34.720]   I think they're number three this week.
[01:06:34.720 --> 01:06:36.520]   Manufacture of smartphones after Samsung.
[01:06:36.520 --> 01:06:43.120]   So it's at Samsung Apple, Huawei, sometimes Apple is third, sometimes second.
[01:06:43.120 --> 01:06:46.560]   So they're a big, this is a big Chinese company founded by a former member of the Chinese
[01:06:46.560 --> 01:06:50.600]   military.
[01:06:50.600 --> 01:06:58.520]   And the US government has already told two of the big US carriers, I think Verizon and
[01:06:58.520 --> 01:06:59.520]   AT&T.
[01:06:59.520 --> 01:07:00.520]   They may not.
[01:07:00.520 --> 01:07:06.560]   It's Huawei gear in their 5G network.
[01:07:06.560 --> 01:07:12.320]   What, this is, I just, I find this fascinating.
[01:07:12.320 --> 01:07:17.880]   This is a very difficult challenge because no one else is making 5G gear as good as Huawei
[01:07:17.880 --> 01:07:20.960]   as well as Huawei as inexpensively as Huawei.
[01:07:20.960 --> 01:07:23.960]   That's why we need that competition.
[01:07:23.960 --> 01:07:25.360]   But like this cuts deep, right?
[01:07:25.360 --> 01:07:31.120]   This cuts back to the cyber, the sort of like cyber cold war we've been having with China
[01:07:31.120 --> 01:07:34.920]   and a lot of other countries for the past decade.
[01:07:34.920 --> 01:07:36.400]   This goes way, way back.
[01:07:36.400 --> 01:07:40.840]   And I think the main worry now with Huawei is that, yeah, is this just another arm of
[01:07:40.840 --> 01:07:44.560]   the Chinese government trying to just have its tendrils there?
[01:07:44.560 --> 01:07:48.680]   Maybe not actually doing thing but having the capability to read data if they wanted
[01:07:48.680 --> 01:07:53.120]   to or have a certain amount of knowledge around what's happening around the world.
[01:07:53.120 --> 01:07:54.960]   It's a legit fear.
[01:07:54.960 --> 01:07:55.960]   I was skeptical.
[01:07:55.960 --> 01:08:00.360]   I wasn't clear because I like Huawei gear to be honest.
[01:08:00.360 --> 01:08:03.080]   Yeah, I have a love, I have a Huawei laptop so really.
[01:08:03.080 --> 01:08:04.080]   That may affect 20.
[01:08:04.080 --> 01:08:05.080]   It's awesome.
[01:08:05.080 --> 01:08:06.080]   They make proud.
[01:08:06.080 --> 01:08:07.080]   Yeah.
[01:08:07.080 --> 01:08:08.080]   I love their phones.
[01:08:08.080 --> 01:08:12.760]   So I was a little, I thought maybe they're a victim in the US Chinese trade war.
[01:08:12.760 --> 01:08:15.240]   But now my mind has a little bit been changed.
[01:08:15.240 --> 01:08:22.800]   The US unsealed the indictments against the Huawei chief financial officers being held
[01:08:22.800 --> 01:08:26.120]   in Canada among one Joe.
[01:08:26.120 --> 01:08:31.680]   And among the accusations, prosecutors say, "Wallway stole trade secrets including technology
[01:08:31.680 --> 01:08:36.640]   behind a robotic device T-Mobile used to test smartphones."
[01:08:36.640 --> 01:08:37.640]   Tappy.
[01:08:37.640 --> 01:08:41.360]   "Wallway engineers secretly took photos of the robot measured it and tried to steal a
[01:08:41.360 --> 01:08:43.000]   piece of it from the lab."
[01:08:43.000 --> 01:08:47.440]   Okay, well, maybe.
[01:08:47.440 --> 01:08:51.760]   There is no allegation in the unsealed documents that Huawei is working at the behest of the
[01:08:51.760 --> 01:08:52.760]   Chinese government.
[01:08:52.760 --> 01:08:53.760]   Right.
[01:08:53.760 --> 01:08:54.760]   I mean, this was interesting.
[01:08:54.760 --> 01:08:59.720]   This is slightly different than some of the ZTE scandals where there was, you know,
[01:08:59.720 --> 01:09:05.200]   ZTE, which is another Chinese manufacturer and they're often completed with Huawei because
[01:09:05.200 --> 01:09:08.680]   they make similar things and have had similar accusations against them.
[01:09:08.680 --> 01:09:13.840]   Yeah, the US Department of Commerce issued ban both ZTE and Huawei.
[01:09:13.840 --> 01:09:14.840]   Right, right, exactly.
[01:09:14.840 --> 01:09:19.600]   But in ZTE's case, I think there was actually some direct allegations to saying that their
[01:09:19.600 --> 01:09:23.160]   network equipment was spying or could potentially be spying.
[01:09:23.160 --> 01:09:29.680]   Whereas this seems to be more about like, are they capturing secrets or this or that?
[01:09:29.680 --> 01:09:32.800]   What's actually interesting, and it hasn't been got, it hasn't had as much traction,
[01:09:32.800 --> 01:09:38.640]   but I work with it a number of Canadians who used to work in the Canadian telecom industry
[01:09:38.640 --> 01:09:39.840]   for a long time.
[01:09:39.840 --> 01:09:44.080]   And what's sort of kind of an interesting wrinkle to the whole fact that the arrest happened
[01:09:44.080 --> 01:09:51.560]   in Canada is that a lot of people from the Nortell days in Canada have are angry about
[01:09:51.560 --> 01:09:59.000]   what would they perceive of Huawei's role in the downturn of the fall of Nortell.
[01:09:59.000 --> 01:10:00.000]   Interesting.
[01:10:00.000 --> 01:10:06.760]   Australia has banned Huawei gear from its 5G rollout.
[01:10:06.760 --> 01:10:13.360]   The Department of Defense bans both Huawei and ZTE from US military bases.
[01:10:13.360 --> 01:10:18.520]   Britain says, British telecom says they're going to strip Huawei equipment from its 4G
[01:10:18.520 --> 01:10:21.480]   network by 2021 and won't use it in 5G core.
[01:10:21.480 --> 01:10:22.600]   But see, this is the issue.
[01:10:22.600 --> 01:10:23.600]   It's everywhere.
[01:10:23.600 --> 01:10:24.600]   Right, exactly.
[01:10:24.600 --> 01:10:25.960]   It's like, well, who do you go to?
[01:10:25.960 --> 01:10:26.960]   I guess you have Ericsson.
[01:10:26.960 --> 01:10:30.240]   But who, and you have Intel maybe, but like, who else do you have?
[01:10:30.240 --> 01:10:31.560]   Who even makes some of this stuff?
[01:10:31.560 --> 01:10:32.920]   I'm not as familiar with vendor.
[01:10:32.920 --> 01:10:33.920]   Do you know?
[01:10:33.920 --> 01:10:35.880]   No, no, no, no, no, Ericsson are too big.
[01:10:35.880 --> 01:10:36.880]   Yeah.
[01:10:36.880 --> 01:10:37.880]   Yeah.
[01:10:37.880 --> 01:10:38.880]   Too big providers, of course, they're Scandinavia.
[01:10:38.880 --> 01:10:41.680]   So maybe, maybe the Finns like us.
[01:10:41.680 --> 01:10:42.680]   I don't know.
[01:10:42.680 --> 01:10:46.040]   This year at CES, Intel was saying, we want to be everywhere.
[01:10:46.040 --> 01:10:49.800]   We want to be in all the pieces because they missed out on mobile completely, right?
[01:10:49.800 --> 01:10:51.440]   By not powering the iPhone.
[01:10:51.440 --> 01:10:55.680]   So I think right now Intel's like, we don't want that to happen ever again.
[01:10:55.680 --> 01:10:57.360]   So there's going to be a big push by Intel.
[01:10:57.360 --> 01:11:01.080]   Who knows how well that works out, but they've been pushing like modems.
[01:11:01.080 --> 01:11:02.880]   They've been doing LT modems.
[01:11:02.880 --> 01:11:04.760]   And that's worked out well for them so far.
[01:11:04.760 --> 01:11:08.760]   Qualcomm has reached a licensing agreement with Huawei a couple of days ago.
[01:11:08.760 --> 01:11:11.760]   I'm going to get to my question.
[01:11:11.760 --> 01:11:16.280]   Like how much of this do we think is there any sort of like xenophobia involved in this?
[01:11:16.280 --> 01:11:17.280]   Yeah, it's happy.
[01:11:17.280 --> 01:11:23.640]   You know, taking pictures of Tappy, the T-Mobile robot doesn't sound like a cyber espionage.
[01:11:23.640 --> 01:11:27.560]   But it was because they were talking about either purchasing it or licensing it.
[01:11:27.560 --> 01:11:30.720]   And that is what that is what stealing is, right?
[01:11:30.720 --> 01:11:32.320]   Like they took that idea, they took photos.
[01:11:32.320 --> 01:11:34.720]   Yeah, but I think companies do that.
[01:11:34.720 --> 01:11:36.560]   I mean, okay, it's wrong.
[01:11:36.560 --> 01:11:37.560]   It's illegal.
[01:11:37.560 --> 01:11:38.560]   Yeah.
[01:11:38.560 --> 01:11:42.120]   But it is not a national security threat or I mean, is it?
[01:11:42.120 --> 01:11:43.120]   I don't think it is.
[01:11:43.120 --> 01:11:45.760]   Well, it's because it's a Chinese company doing that to a US company.
[01:11:45.760 --> 01:11:50.500]   And there are, I think there are a lot of reports about US companies like visiting and
[01:11:50.500 --> 01:11:52.320]   trying to partner with some Chinese companies.
[01:11:52.320 --> 01:11:55.440]   All of a sudden their tech, there were some really good.
[01:11:55.440 --> 01:11:59.160]   I think it was like all things considered had a good chat with some folks about this.
[01:11:59.160 --> 01:12:00.160]   I listened to that.
[01:12:00.160 --> 01:12:03.760]   There's a guy who wrote a book about this and it was interesting.
[01:12:03.760 --> 01:12:07.560]   Look, it's clear what the threats could be.
[01:12:07.560 --> 01:12:14.400]   If your network is built on, your network infrastructure is built on gear from a company
[01:12:14.400 --> 01:12:16.640]   controlled by the government of China.
[01:12:16.640 --> 01:12:21.000]   And it doesn't even have to be currently controlled because it is a Chinese company.
[01:12:21.000 --> 01:12:23.800]   At some point the government could say, okay, fine, thank you.
[01:12:23.800 --> 01:12:27.040]   We'll take over from here or we'd like access to that network.
[01:12:27.040 --> 01:12:28.440]   So that's legitimate.
[01:12:28.440 --> 01:12:34.840]   There's both industrial espionage threats and national security threats because if that,
[01:12:34.840 --> 01:12:39.840]   if our entire, if we start to rely on 5G infrastructure, our entire autonomous fleet
[01:12:39.840 --> 01:12:44.400]   is using 5G to communicate and all of a sudden the Chinese government flips a switch and
[01:12:44.400 --> 01:12:45.400]   it stops working.
[01:12:45.400 --> 01:12:46.400]   That's not good.
[01:12:46.400 --> 01:12:47.400]   Sure.
[01:12:47.400 --> 01:12:48.400]   That's cyber war.
[01:12:48.400 --> 01:12:49.400]   It isn't.
[01:12:49.400 --> 01:12:51.840]   There's a lot of pressure on Canada to ban Huawei.
[01:12:51.840 --> 01:12:52.840]   What's the Canada?
[01:12:52.840 --> 01:12:53.840]   Oh, they haven't yet.
[01:12:53.840 --> 01:12:54.840]   No, they have not.
[01:12:54.840 --> 01:13:00.080]   There's a lot of pressure on them too because, you know, again, there's a lot of like the
[01:13:00.080 --> 01:13:04.440]   big five are kind of looking at us as well because if our infrastructure has Huawei,
[01:13:04.440 --> 01:13:07.800]   like you go through us to be able to get information anyways.
[01:13:07.800 --> 01:13:11.920]   So that kind of compromises everyone else to be able to deal with it.
[01:13:11.920 --> 01:13:15.320]   But, you know, Huawei's already said like they deny that they're spying for the Chinese
[01:13:15.320 --> 01:13:19.800]   government or that they are using their technology for spying in any way.
[01:13:19.800 --> 01:13:23.800]   They didn't say that they couldn't or that they wouldn't, which is like also they were
[01:13:23.800 --> 01:13:27.480]   not really strong in their statements to that.
[01:13:27.480 --> 01:13:30.880]   And in Canada, they're still looking into it and they're going to make a decision dependent
[01:13:30.880 --> 01:13:35.320]   on what they find for security.
[01:13:35.320 --> 01:13:40.960]   Like we're lucky Trudeau actually has a good knowledge of technology and how it's used.
[01:13:40.960 --> 01:13:45.320]   And he did hire a whole bunch of people that work in the government that really are in
[01:13:45.320 --> 01:13:49.600]   the field like the agriculture minister actually works in agriculture.
[01:13:49.600 --> 01:13:52.960]   So we'll actually get some good answers to that.
[01:13:52.960 --> 01:13:54.960]   Nice humble bag, Georgia.
[01:13:54.960 --> 01:13:55.960]   Very nice.
[01:13:55.960 --> 01:13:56.960]   Seriously.
[01:13:56.960 --> 01:13:57.960]   So congratulations.
[01:13:57.960 --> 01:14:03.160]   It's great for you to know what they're doing.
[01:14:03.160 --> 01:14:06.520]   Yes, show off.
[01:14:06.520 --> 01:14:10.520]   So the person that was on fresh air is named David Sanger.
[01:14:10.520 --> 01:14:16.640]   He's a security correspondent for the New York Times and has written a book about cyber
[01:14:16.640 --> 01:14:19.240]   warfare called the Perfect Weapon.
[01:14:19.240 --> 01:14:24.800]   So his point, which was kind of interesting is that these switches that Huawei's selling
[01:14:24.800 --> 01:14:28.080]   are basically not hardware, they're software.
[01:14:28.080 --> 01:14:33.480]   So in the old days when switches were hardware, a company could buy them and presumably inspect
[01:14:33.480 --> 01:14:36.800]   them in such a way to make sure that they were safe.
[01:14:36.800 --> 01:14:40.520]   But the problem is it's software, not only is it software, it's software is updated fairly
[01:14:40.520 --> 01:14:41.840]   regularly.
[01:14:41.840 --> 01:14:47.160]   So there's no way to be sure that that software at some point won't be modified in such a
[01:14:47.160 --> 01:14:49.040]   way that it could be dangerous.
[01:14:49.040 --> 01:14:50.800]   There's just no way to guarantee that.
[01:14:50.800 --> 01:14:55.360]   How do you put any safeguards in place to be able to deal with that?
[01:14:55.360 --> 01:14:57.560]   So it's not hardware.
[01:14:57.560 --> 01:14:58.560]   It's important to understand.
[01:14:58.560 --> 01:15:01.720]   It's not just that don't put that hardware in there.
[01:15:01.720 --> 01:15:04.200]   It's the software that's the real concern.
[01:15:04.200 --> 01:15:05.200]   And I don't just agree with that.
[01:15:05.200 --> 01:15:06.400]   I mean, I think that's a valid concern.
[01:15:06.400 --> 01:15:11.880]   I guess the only thing I would push back on is you could make that claim about any provider.
[01:15:11.880 --> 01:15:12.880]   Right?
[01:15:12.880 --> 01:15:13.880]   Well, that's right.
[01:15:13.880 --> 01:15:14.880]   Right.
[01:15:14.880 --> 01:15:19.800]   So a little bit of it is probably because it's China versus the US got caught doing exactly
[01:15:19.800 --> 01:15:20.800]   the same thing.
[01:15:20.800 --> 01:15:21.800]   They were big enough company.
[01:15:21.800 --> 01:15:23.400]   They might get a pass on it.
[01:15:23.400 --> 01:15:24.400]   Right.
[01:15:24.400 --> 01:15:27.960]   Which I guess is kind of my point, which is that I don't doubt that there are real concerns.
[01:15:27.960 --> 01:15:33.120]   I just wonder in some cases how much there is a xenophobic component and an anti-China
[01:15:33.120 --> 01:15:35.200]   component that may not be warranted.
[01:15:35.200 --> 01:15:37.320]   And that bothers me if I mean totally.
[01:15:37.320 --> 01:15:39.240]   Like that bothers me.
[01:15:39.240 --> 01:15:40.240]   Exactly.
[01:15:40.240 --> 01:15:41.240]   Yeah.
[01:15:41.240 --> 01:15:45.120]   That's where my unease comes from exactly, Christina.
[01:15:45.120 --> 01:15:49.000]   Is this a product of xenophobia and trade war?
[01:15:49.000 --> 01:15:53.680]   Or is, I mean, there certainly is legitimate root cause for concern.
[01:15:53.680 --> 01:15:55.040]   I don't know.
[01:15:55.040 --> 01:15:59.240]   Is Erickson, I guess Erickson, we can trust them because they're not for China?
[01:15:59.240 --> 01:16:00.240]   Right.
[01:16:00.240 --> 01:16:01.240]   That's what I'm saying.
[01:16:01.240 --> 01:16:02.240]   But right.
[01:16:02.240 --> 01:16:03.240]   So that's the thing.
[01:16:03.240 --> 01:16:05.880]   It's like, are we putting the same scrutiny against Erickson, against Nokia and against
[01:16:05.880 --> 01:16:07.640]   others that we're doing here?
[01:16:07.640 --> 01:16:08.640]   I think we should, right?
[01:16:08.640 --> 01:16:10.200]   Like I think that we should.
[01:16:10.200 --> 01:16:13.480]   But if we're not, then we need to start, we need to discuss and have a real conversation
[01:16:13.480 --> 01:16:14.480]   about why.
[01:16:14.480 --> 01:16:18.680]   And when we're talking about holding people and arresting people in different countries
[01:16:18.680 --> 01:16:23.840]   and extraditing them, again, like you need to have in my opinion, very, very good reasons
[01:16:23.840 --> 01:16:24.840]   for that.
[01:16:24.840 --> 01:16:25.840]   You know?
[01:16:25.840 --> 01:16:26.840]   All right.
[01:16:26.840 --> 01:16:28.240]   Let's take a little break.
[01:16:28.240 --> 01:16:36.040]   We've lots more to talk about, including a DNA company giving its data to the FBI.
[01:16:36.040 --> 01:16:43.120]   But first a word from says, as long as we're talking cybersecurity, Sophos, we use Sophos.
[01:16:43.120 --> 01:16:44.680]   Everybody loves Sophos.
[01:16:44.680 --> 01:16:50.640]   Many is the time I've told people, use Sophos' online scanner to check your system.
[01:16:50.640 --> 01:16:57.360]   Sophos understands that it is a battle between the black hats and us.
[01:16:57.360 --> 01:17:02.400]   And it is a seesaw battle and the hackers are getting smarter all the time.
[01:17:02.400 --> 01:17:06.080]   That's why Sophos cybersecurity uses machine learning.
[01:17:06.080 --> 01:17:14.440]   We call it deep learning to interpret data and respond to threats better than any network
[01:17:14.440 --> 01:17:16.240]   protection can.
[01:17:16.240 --> 01:17:20.760]   The hardware and software Sophos recently ranked number one by an independent security test
[01:17:20.760 --> 01:17:25.840]   done by SE Labs with the best protection ratings across the board, both for large enterprises
[01:17:25.840 --> 01:17:29.760]   and small businesses is why we use Sophos here.
[01:17:29.760 --> 01:17:32.440]   They have actually now done something kind of interesting.
[01:17:32.440 --> 01:17:33.440]   And I've been using it.
[01:17:33.440 --> 01:17:34.920]   I find it very, very fascinating.
[01:17:34.920 --> 01:17:40.720]   They took the same deep learning software technology they're using in these big enterprises now.
[01:17:40.720 --> 01:17:44.280]   And they've made a premium version available for home users for Mac and PC.
[01:17:44.280 --> 01:17:46.720]   It's called Sophos Home.
[01:17:46.720 --> 01:17:48.440]   And it's not an antivirus.
[01:17:48.440 --> 01:17:50.640]   It's something so much more sophisticated.
[01:17:50.640 --> 01:17:56.360]   It delivers real time protection from the latest ransomware attacks, malicious software,
[01:17:56.360 --> 01:17:58.560]   hacking attempts and more.
[01:17:58.560 --> 01:18:01.200]   Very easy to use, always up to date.
[01:18:01.200 --> 01:18:05.680]   It's the first time I've ever seen anything that's faster than the bad guys.
[01:18:05.680 --> 01:18:07.480]   It's up to date faster than the bad guys.
[01:18:07.480 --> 01:18:09.640]   And because it's cloud based.
[01:18:09.640 --> 01:18:11.400]   And oh, I love this feature.
[01:18:11.400 --> 01:18:16.640]   And I know some of you are responsible for family and friends and their security.
[01:18:16.640 --> 01:18:18.280]   It's a cloud based protection.
[01:18:18.280 --> 01:18:21.240]   You can use to keep your relatives secure, no matter where they are, even if there are
[01:18:21.240 --> 01:18:22.080]   thousands of miles away.
[01:18:22.080 --> 01:18:26.320]   You can remotely manage their security, clean up threats, keep them safe.
[01:18:26.320 --> 01:18:31.600]   One account can protect all the Macs and PCs in your home and your family too.
[01:18:31.600 --> 01:18:36.040]   It's easy to use whether you're just securing a single laptop or managing the security of
[01:18:36.040 --> 01:18:39.040]   multiple devices around the world.
[01:18:39.040 --> 01:18:40.800]   Because that's Sophos.
[01:18:40.800 --> 01:18:42.640]   That's how they work in enterprise.
[01:18:42.640 --> 01:18:45.880]   And that's how they work with Sophos Home.
[01:18:45.880 --> 01:18:48.880]   Sophos Tagline is security made simple.
[01:18:48.880 --> 01:18:51.720]   And that's exactly what you're going to get.
[01:18:51.720 --> 01:18:53.280]   You work with your browser.
[01:18:53.280 --> 01:18:58.760]   You know, it's a browser interface, so you start securing a system right now.
[01:18:58.760 --> 01:19:05.640]   Whether you're a large enterprise or a home user, Sophos, S-O-P-H-O-S as you covered.
[01:19:05.640 --> 01:19:11.560]   Some of the largest businesses in the world use Sophos to protect their systems from ransomware.
[01:19:11.560 --> 01:19:16.240]   Third party reviewers consistently rank Sophos among the best cybersecurity providers.
[01:19:16.240 --> 01:19:20.480]   And I love it that synchronized security means you manage all your Sophos products from a single
[01:19:20.480 --> 01:19:26.040]   cloud-based console for your family as well as for yourself.
[01:19:26.040 --> 01:19:29.000]   Get a free trial right now of Sophos Home at Sophos.com.
[01:19:29.000 --> 01:19:32.240]   Actually, you can also go there and get that security scan too.
[01:19:32.240 --> 01:19:36.360]   And we've always recommended that as a great way of making sure your system is up to date
[01:19:36.360 --> 01:19:37.920]   and safe.
[01:19:37.920 --> 01:19:41.440]   Sophos.com.
[01:19:41.440 --> 01:19:43.720]   Great product.
[01:19:43.720 --> 01:19:49.400]   So you're going to have to tease the facts out on this one because it's not clear.
[01:19:49.400 --> 01:19:58.480]   You may remember the Golden State Killer case, which was a long cold case serial killer.
[01:19:58.480 --> 01:20:01.120]   Never caught him.
[01:20:01.120 --> 01:20:10.560]   Until the feds went to a database, a DNA genetic database run by FamilyTree DNA.
[01:20:10.560 --> 01:20:16.040]   Now, at the time this didn't bother me when I read about this because what had happened
[01:20:16.040 --> 01:20:23.200]   was relatives of the guy they eventually arrested, but this happened fairly recently, had posted,
[01:20:23.200 --> 01:20:28.160]   they had DNA tests on posted their data on the forum in an attempt to find other family
[01:20:28.160 --> 01:20:29.260]   members.
[01:20:29.260 --> 01:20:31.920]   The feds apparently were scanning through the forum.
[01:20:31.920 --> 01:20:34.080]   They had the DNA of the Golden State Killer.
[01:20:34.080 --> 01:20:35.640]   They just didn't have a suspect.
[01:20:35.640 --> 01:20:42.400]   They noticed the similarity and they actually were able to track down the relatives, say,
[01:20:42.400 --> 01:20:48.080]   tell us about other people in your family and they actually arrested the guy, which is
[01:20:48.080 --> 01:20:49.080]   remarkable.
[01:20:49.080 --> 01:20:54.680]   It's Portland police tied a, this is a 40 year old case.
[01:20:54.680 --> 01:20:57.320]   So they arrested him.
[01:20:57.320 --> 01:21:02.000]   This was in April, but what we've now learned since is something a little more disturbing.
[01:21:02.000 --> 01:21:03.080]   See, that didn't bother me.
[01:21:03.080 --> 01:21:04.560]   That was voluntary, right?
[01:21:04.560 --> 01:21:06.680]   People put their DNA information up.
[01:21:06.680 --> 01:21:07.920]   So you're putting it in public.
[01:21:07.920 --> 01:21:09.680]   You know, the FBI uses it.
[01:21:09.680 --> 01:21:11.720]   That's fine.
[01:21:11.720 --> 01:21:19.000]   What we're finding now is family tree DNA is also providing this access to people who haven't
[01:21:19.000 --> 01:21:23.080]   posted it publicly.
[01:21:23.080 --> 01:21:27.720]   The company, which was one of the first, I think they say they were the first to offer
[01:21:27.720 --> 01:21:34.040]   direct consumer test kit, told BuzzFeed News on Thursday that they have a relationship
[01:21:34.040 --> 01:21:41.200]   with the FBI that they will help law enforcement agencies solve violent crimes faster than ever.
[01:21:41.200 --> 01:21:46.200]   On a contract, but, and this is the thing that is disturbing, the firm has agreed to
[01:21:46.200 --> 01:21:51.360]   test DNA samples, upload the profiles to its database on a case by case basis.
[01:21:51.360 --> 01:21:59.360]   They've been doing it since the fall and they say that, you know, the only way to stop this,
[01:21:59.360 --> 01:22:05.400]   if you are a family tree DNA customer, is to turn off family sharing.
[01:22:05.400 --> 01:22:11.040]   That by saying, I want to find other relatives, you are giving them permission to share this
[01:22:11.040 --> 01:22:14.480]   information with law enforcement.
[01:22:14.480 --> 01:22:19.000]   Of course, if you're doing family tree DNA, that's the entire point of it.
[01:22:19.000 --> 01:22:20.960]   Right.
[01:22:20.960 --> 01:22:23.960]   Finding possible relatives through DNA testing.
[01:22:23.960 --> 01:22:30.200]   So family tree says, if you opt out of any familial match matching, notice no FBI in
[01:22:30.200 --> 01:22:35.480]   there, any familial matching, then we won't be able to, we won't, the FBI won't be able
[01:22:35.480 --> 01:22:37.160]   to search your files.
[01:22:37.160 --> 01:22:42.400]   But if you say, oh yeah, I'm looking for family members, that also means the FBI can look
[01:22:42.400 --> 01:22:43.400]   through your stuff.
[01:22:43.400 --> 01:22:44.400]   What do you think?
[01:22:44.400 --> 01:22:49.280]   I think this is the reason why no one should give these things.
[01:22:49.280 --> 01:22:52.080]   No one should give these things as gifts, you know, like every year.
[01:22:52.080 --> 01:22:54.080]   Sorry, that is true.
[01:22:54.080 --> 01:22:56.080]   You're trying to, no, but give you a 23andMe.
[01:22:56.080 --> 01:23:02.320]   Okay, first of all, 23andMe is a sponsor, but I'm not going to keep me from saying this.
[01:23:02.320 --> 01:23:08.640]   But I think there's a lot of value in genetic, in this direct to consumer DNA testing, but
[01:23:08.640 --> 01:23:13.480]   you should only do it with a company that says explicitly, no one's going to get this
[01:23:13.480 --> 01:23:15.480]   information except you.
[01:23:15.480 --> 01:23:19.040]   But the problem in Alex in the chat room actually mentioned it is that these companies
[01:23:19.040 --> 01:23:22.360]   don't fall, cover any HIPAA kind of covered entities.
[01:23:22.360 --> 01:23:25.320]   And so you don't have the same kind of legal protection.
[01:23:25.320 --> 01:23:29.200]   And even if they say they're not going to cover it, if the FBI comes after them and says,
[01:23:29.200 --> 01:23:33.480]   listen, we want this information, we know for some reason this person has come to you,
[01:23:33.480 --> 01:23:36.840]   it becomes one of these really big issues to that.
[01:23:36.840 --> 01:23:39.000]   And I think that Christina is absolutely right.
[01:23:39.000 --> 01:23:42.040]   Like you give this as a gift to someone else.
[01:23:42.040 --> 01:23:45.720]   It might not be that much of a gift unless you think they've fitted a dime to make sure
[01:23:45.720 --> 01:23:46.960]   you want to send it on purpose.
[01:23:46.960 --> 01:23:54.360]   Me a copa, but I have my entire family, my wife, my kids, my mother, my father, my wife's
[01:23:54.360 --> 01:23:56.560]   mother and father have all done.
[01:23:56.560 --> 01:24:01.880]   I think most of us did 23andMe, I've done 23andMe and Helix, both of which
[01:24:01.880 --> 01:24:06.160]   responses, actually 23andMe is still a response in the ratio.
[01:24:06.160 --> 01:24:07.160]   You know, I've done it.
[01:24:07.160 --> 01:24:11.760]   I like the stuff I get from it.
[01:24:11.760 --> 01:24:14.120]   So what do we need some sort of HIPAA?
[01:24:14.120 --> 01:24:16.120]   Do we need what do we need?
[01:24:16.120 --> 01:24:19.000]   Yeah, we need a lot more than what we have.
[01:24:19.000 --> 01:24:20.000]   All right.
[01:24:20.000 --> 01:24:22.960]   And let me play devil's advocate.
[01:24:22.960 --> 01:24:24.800]   Look if you didn't do anything wrong.
[01:24:24.800 --> 01:24:28.200]   Oh, my God.
[01:24:28.200 --> 01:24:30.600]   This is how the national database of DNA starts.
[01:24:30.600 --> 01:24:35.680]   Yeah, this could be used like for like like insurance companies and we can talk about
[01:24:35.680 --> 01:24:36.680]   insurance companies.
[01:24:36.680 --> 01:24:37.680]   Well, that's different.
[01:24:37.680 --> 01:24:38.680]   I agree.
[01:24:38.680 --> 01:24:45.240]   If you're in the house, this could be used against you in many different ways versus the
[01:24:45.240 --> 01:24:47.640]   benefits and you heard about the twins.
[01:24:47.640 --> 01:24:53.400]   Two twins ended up doing their DNA for their genetic ancestry and they only test 1% of the
[01:24:53.400 --> 01:24:54.400]   bloody DNA.
[01:24:54.400 --> 01:24:58.040]   So they ended up getting ancestry of like being in two different parts and they are
[01:24:58.040 --> 01:24:59.040]   identical twins.
[01:24:59.040 --> 01:25:04.760]   So their DNA is identical and they got to completely divergent.
[01:25:04.760 --> 01:25:08.720]   There's a lot that that CBC story was kind of BS.
[01:25:08.720 --> 01:25:13.520]   I want to say before you go on that CBC's where it was BS.
[01:25:13.520 --> 01:25:18.400]   It was statistically exactly in line with what you'd expect because but what people don't
[01:25:18.400 --> 01:25:23.600]   understand is that these DNA tests are not in fact testing every strand and comparing
[01:25:23.600 --> 01:25:24.800]   head to head.
[01:25:24.800 --> 01:25:28.760]   What they do is they take your DNA and they compare it to a statistical sample and within
[01:25:28.760 --> 01:25:30.920]   that limitation, this is exactly.
[01:25:30.920 --> 01:25:33.520]   But again, that shows how far it's going.
[01:25:33.520 --> 01:25:35.000]   But people need to understand that.
[01:25:35.000 --> 01:25:36.000]   Yeah, yeah, yeah.
[01:25:36.000 --> 01:25:37.000]   I understand.
[01:25:37.000 --> 01:25:39.720]   It's not really giving you what you think that you're getting from that.
[01:25:39.720 --> 01:25:41.840]   They're hoping to find out where they're for them.
[01:25:41.840 --> 01:25:47.000]   They were trying to find out where their ancestry was from and this is 1% of your DNA.
[01:25:47.000 --> 01:25:48.440]   It's like truly nothing.
[01:25:48.440 --> 01:25:52.720]   So you end up with an idea that is inaccurate for what you're getting and you're giving
[01:25:52.720 --> 01:25:53.720]   up your DNA.
[01:25:53.720 --> 01:25:54.720]   Yeah.
[01:25:54.720 --> 01:25:59.840]   And I would say that even if that, you know, because I think that the stuff will get better
[01:25:59.840 --> 01:26:02.120]   and even if you were to put that aside.
[01:26:02.120 --> 01:26:06.640]   If you look, I mean, this is the CBC story and you look at the statistical results that
[01:26:06.640 --> 01:26:07.640]   they got.
[01:26:07.640 --> 01:26:08.640]   They're pretty damn close.
[01:26:08.640 --> 01:26:11.640]   There's some little, I mean, it's nitpicky.
[01:26:11.640 --> 01:26:15.160]   But this, but I think if people think, oh no, this is an exact science, they're going to
[01:26:15.160 --> 01:26:16.720]   be disappointed by the results.
[01:26:16.720 --> 01:26:17.720]   But it's not.
[01:26:17.720 --> 01:26:18.720]   It's a statistical thing.
[01:26:18.720 --> 01:26:21.840]   The bigger thing is creating a database of people's DNA.
[01:26:21.840 --> 01:26:22.840]   Like that to me is the bigger.
[01:26:22.840 --> 01:26:23.920]   So that's okay, right.
[01:26:23.920 --> 01:26:29.840]   So, but no, I think what George is saying and I respect that is you're not getting that
[01:26:29.840 --> 01:26:32.880]   much and you're giving up everything.
[01:26:32.880 --> 01:26:33.880]   Yes.
[01:26:33.880 --> 01:26:34.880]   Yes.
[01:26:34.880 --> 01:26:38.520]   And I think it should also be pointed out that when we've shown this, this has happened
[01:26:38.520 --> 01:26:41.360]   before where you, it's not about you have nothing to hide.
[01:26:41.360 --> 01:26:44.680]   It's that, you know, DNA can be transferred across things.
[01:26:44.680 --> 01:26:47.120]   People have found DNA where people have never been in locations before.
[01:26:47.120 --> 01:26:51.120]   And now all of a sudden you have a database of individuals and you can have people framed
[01:26:51.120 --> 01:26:53.520]   or, you know, a significant of crimes.
[01:26:53.520 --> 01:26:59.000]   I wouldn't be able to get if, if, if, if I had the APOE gene, which is a marker, not
[01:26:59.000 --> 01:27:01.240]   it by the way, and this is the other problem with this.
[01:27:01.240 --> 01:27:04.000]   People often say, well, you got APOE, they're going to get Alzheimer's.
[01:27:04.000 --> 01:27:11.120]   No, no, no, but that is there, there is a relationship between that marker and Alzheimer's.
[01:27:11.120 --> 01:27:14.720]   If it ensures, then said, well, for what, I don't care, I'm not going to give you a
[01:27:14.720 --> 01:27:16.520]   short, long-term care insurance.
[01:27:16.520 --> 01:27:17.520]   Exactly.
[01:27:17.520 --> 01:27:18.520]   That would be bad.
[01:27:18.520 --> 01:27:19.520]   Yeah.
[01:27:19.520 --> 01:27:20.520]   No, totally.
[01:27:20.520 --> 01:27:21.520]   And I mean, this is where this all starts.
[01:27:21.520 --> 01:27:23.120]   So I mean, to me, it's, it's usually problematic.
[01:27:23.120 --> 01:27:27.080]   On the other hand, the insurance companies say, yes, but by doing so, we're reducing the
[01:27:27.080 --> 01:27:28.920]   cost of insurance for all of you.
[01:27:28.920 --> 01:27:29.920]   Oh, please.
[01:27:29.920 --> 01:27:33.080]   That's the same thing that the insurance companies say about like yours.
[01:27:33.080 --> 01:27:34.080]   Yeah.
[01:27:34.080 --> 01:27:35.080]   Yeah, they lie.
[01:27:35.080 --> 01:27:36.960]   They're just saying that because they want the information to it.
[01:27:36.960 --> 01:27:39.720]   And insurance companies are not there to help you out.
[01:27:39.720 --> 01:27:41.880]   I'm sorry, they're there to make profit.
[01:27:41.880 --> 01:27:46.120]   And so the more people they can say no to, the nice people get fired from insurance companies.
[01:27:46.120 --> 01:27:49.880]   And the more people that they can say no to, they actually have a quota to that, then that's
[01:27:49.880 --> 01:27:51.880]   how they get, move up in the company.
[01:27:51.880 --> 01:27:55.200]   So you're even on using it on your watch, they're like, you know what?
[01:27:55.200 --> 01:27:58.960]   If you're healthy and doing all of these things, you'll have less chance of needing to use
[01:27:58.960 --> 01:28:00.560]   the insurance company, which is true.
[01:28:00.560 --> 01:28:04.240]   That you end up finding out you have some sort of a rhythm, a rhythm, and they find out
[01:28:04.240 --> 01:28:05.720]   they could also say, you know what?
[01:28:05.720 --> 01:28:07.160]   Actually now that's no longer covered.
[01:28:07.160 --> 01:28:12.080]   But thanks to Family Tree DNA, this guy who killed 13 people, raped more than 50 people
[01:28:12.080 --> 01:28:17.360]   committed over 100 burglaries 40 years ago has finally been caught.
[01:28:17.360 --> 01:28:23.160]   You know, that doesn't fly with me because there is a backlog of rape kits that are out
[01:28:23.160 --> 01:28:28.200]   there that have never been looked at where they have all of the DNA there that the government
[01:28:28.200 --> 01:28:30.160]   could test and they choose not to.
[01:28:30.160 --> 01:28:35.080]   So when you get through all of those for murders and for rape and other violent crimes, then
[01:28:35.080 --> 01:28:37.280]   you can talk to me about needing to go to the FBI.
[01:28:37.280 --> 01:28:38.800]   Maybe that's what the FBI is doing.
[01:28:38.800 --> 01:28:43.840]   Maybe they're using, maybe they're going through those old rape kits and going through the database,
[01:28:43.840 --> 01:28:48.480]   the database, the Family Tree DNA database just to see what they could find.
[01:28:48.480 --> 01:28:49.960]   Would that be bad?
[01:28:49.960 --> 01:28:50.960]   Yes.
[01:28:50.960 --> 01:28:51.960]   Yes.
[01:28:51.960 --> 01:28:54.880]   Because it's bad for the greater good to that.
[01:28:54.880 --> 01:28:58.840]   We still need to have privacy is still important to everyone.
[01:28:58.840 --> 01:29:03.320]   And yes, it feels good to be able to say, well, you know, we've stopped this one bad
[01:29:03.320 --> 01:29:04.320]   guy.
[01:29:04.320 --> 01:29:07.440]   It did not be at the cost of the many.
[01:29:07.440 --> 01:29:12.760]   And at this point, you'd probably be wise to consider whether you want to do one of these,
[01:29:12.760 --> 01:29:13.760]   right?
[01:29:13.760 --> 01:29:17.240]   It's basically the minority report problem, right?
[01:29:17.240 --> 01:29:22.440]   Like here's a perfect system for predicting crime except how can this be used badly.
[01:29:22.440 --> 01:29:27.200]   And I love that movie because by the end, the solution is like, oh, I guess we kind of
[01:29:27.200 --> 01:29:29.120]   have to let crime happen again.
[01:29:29.120 --> 01:29:30.120]   I don't know.
[01:29:30.120 --> 01:29:31.440]   Like, there's no easy answer.
[01:29:31.440 --> 01:29:35.520]   So we need to have educated, smart people within these fields to really start asking
[01:29:35.520 --> 01:29:38.040]   these questions now because yeah, we're seeing.
[01:29:38.040 --> 01:29:41.520]   At the very least, at the very least, you need to have regulations.
[01:29:41.520 --> 01:29:48.120]   Like, you know, and like I'll roll my eyes about, you know, the state of New York looking
[01:29:48.120 --> 01:29:49.120]   into the FaceTime bug.
[01:29:49.120 --> 01:29:52.240]   But this is the sort of thing where you need HIPAA, where you need other sorts of regulations.
[01:29:52.240 --> 01:29:56.400]   It needs to be done very seriously because you're potentially talking about, yes, you
[01:29:56.400 --> 01:29:58.120]   could maybe catch a killer, which is great.
[01:29:58.120 --> 01:30:00.520]   But you could also conceivably ruin people's lives.
[01:30:00.520 --> 01:30:04.440]   And someone could be put in jail for something that they didn't do.
[01:30:04.440 --> 01:30:11.400]   And that's not a far out thought, you know, like the implications here are really, really
[01:30:11.400 --> 01:30:12.400]   dangerous.
[01:30:12.400 --> 01:30:13.400]   It's not just DNA.
[01:30:13.400 --> 01:30:18.120]   I mean, face recognition technology we know as a high rate of false positives.
[01:30:18.120 --> 01:30:19.120]   Exactly.
[01:30:19.120 --> 01:30:20.920]   And we already have, you know, databases for things like that.
[01:30:20.920 --> 01:30:27.240]   I mean, I feel annoyed enough with myself that I use clear the service ticket through
[01:30:27.240 --> 01:30:28.240]   airports.
[01:30:28.240 --> 01:30:29.240]   You've got your iris.
[01:30:29.240 --> 01:30:30.480]   I always say my fingerprints.
[01:30:30.480 --> 01:30:36.160]   And I'm mad at myself, but I'm an airport so often that, you know, and I also have global
[01:30:36.160 --> 01:30:37.160]   entry.
[01:30:37.160 --> 01:30:41.120]   So, you know, I'm already in a database, but it's, you know, it's just one other thing
[01:30:41.120 --> 01:30:46.600]   where you do worry about, okay, this is a private sector thing controlling these things
[01:30:46.600 --> 01:30:50.120]   and governments are using this to track exactly what it's about.
[01:30:50.120 --> 01:30:53.000]   There's huge benefit to DNA testing.
[01:30:53.000 --> 01:30:54.000]   Yes.
[01:30:54.000 --> 01:30:59.160]   Women who have the BRACA gene, for instance, that can be very, if done properly.
[01:30:59.160 --> 01:31:03.760]   By the way, the BRACA tests, but most of these consumer DNA tests are not very useful.
[01:31:03.760 --> 01:31:08.320]   But if you went to your physician, got a prescription and got a real BRACA genetic test, that is
[01:31:08.320 --> 01:31:12.040]   hugely valuable information.
[01:31:12.040 --> 01:31:13.880]   It's at that point protected by HIPAA.
[01:31:13.880 --> 01:31:23.080]   So I think the key is to extend the privacy protections of HIPAA to these DNA tests.
[01:31:23.080 --> 01:31:24.080]   Why not?
[01:31:24.080 --> 01:31:25.080]   Yeah.
[01:31:25.080 --> 01:31:27.760]   Why does this make life a lot harder for these companies?
[01:31:27.760 --> 01:31:29.760]   It should be harder for these companies.
[01:31:29.760 --> 01:31:31.320]   It's going to say you don't want to and they don't have to.
[01:31:31.320 --> 01:31:34.360]   They're making a lot of money by not having to do it on expensive.
[01:31:34.360 --> 01:31:39.400]   Well, remember the FDA went to 23andMe and said, you can't make that, you cannot make
[01:31:39.400 --> 01:31:41.960]   these health recommendations.
[01:31:41.960 --> 01:31:42.960]   Shut down 23andMe.
[01:31:42.960 --> 01:31:45.960]   They went through a long process, went back and forth.
[01:31:45.960 --> 01:31:49.360]   23andMe got at the point where the FDA said, okay, we'll now give you approval and they
[01:31:49.360 --> 01:31:50.560]   do have approval for it.
[01:31:50.560 --> 01:31:51.560]   Yes, which I think is great.
[01:31:51.560 --> 01:31:52.560]   Why can't that happen?
[01:31:52.560 --> 01:31:53.560]   That should happen.
[01:31:53.560 --> 01:31:54.560]   I think that should be the requirement.
[01:31:54.560 --> 01:31:55.560]   I mean, I think you pointed out.
[01:31:55.560 --> 01:32:00.080]   I mean, I think both Devindra and Georgia said, like the reason this doesn't happen is because
[01:32:00.080 --> 01:32:05.600]   by making them be responsible for HIPAA requirements and they would have to be classified as a
[01:32:05.600 --> 01:32:06.600]   medical.
[01:32:06.600 --> 01:32:07.600]   They should be.
[01:32:07.600 --> 01:32:08.600]   I agree.
[01:32:08.600 --> 01:32:12.280]   Because I don't want people not to do DNA testing because they're worried the FBI is going
[01:32:12.280 --> 01:32:14.000]   to arrest their cousin.
[01:32:14.000 --> 01:32:15.520]   I agree with you.
[01:32:15.520 --> 01:32:16.520]   I agree with you.
[01:32:16.520 --> 01:32:19.720]   What I'm saying though is that that makes it harder for these startups and these companies
[01:32:19.720 --> 01:32:20.720]   to scale.
[01:32:20.720 --> 01:32:24.880]   But I think 23andMe, I think that's a great point is that they were faced with that critique
[01:32:24.880 --> 01:32:28.320]   and other than shutting down, they went through the hoops.
[01:32:28.320 --> 01:32:33.640]   I almost feel like if you're in this space, look, disruption is great, but you need to
[01:32:33.640 --> 01:32:38.040]   follow the same rules that any other medical company would follow.
[01:32:38.040 --> 01:32:41.320]   And they're just simply saying, well, we're tech, we can break things.
[01:32:41.320 --> 01:32:42.320]   I mean, that's how it looks like.
[01:32:42.320 --> 01:32:45.560]   Well, Apple got approval for their ECG.
[01:32:45.560 --> 01:32:53.560]   According to a new FDA regulation system that allowed precisely for high-speed approval
[01:32:53.560 --> 01:33:00.320]   of technological devices, it's not been proven safe and effective, but it used this special
[01:33:00.320 --> 01:33:01.320]   regulation.
[01:33:01.320 --> 01:33:06.400]   I'm also thinking of like Theranos too.
[01:33:06.400 --> 01:33:07.840]   They didn't go through the FDA.
[01:33:07.840 --> 01:33:12.080]   I think there was a loophole where they didn't have to.
[01:33:12.080 --> 01:33:15.800]   Yeah, these are all things we need to fix because they could have been a really interesting
[01:33:15.800 --> 01:33:19.200]   company, but we could have also stopped them much earlier if somebody just took a look
[01:33:19.200 --> 01:33:21.800]   at their tech and was like, hey, you're not doing anything.
[01:33:21.800 --> 01:33:23.800]   It wasn't a lie, but yeah.
[01:33:23.800 --> 01:33:24.800]   Yeah.
[01:33:24.800 --> 01:33:29.480]   It would have had to not have been a complete lie from the beginning, but the idea was certainly
[01:33:29.480 --> 01:33:30.480]   interesting.
[01:33:30.480 --> 01:33:38.560]   It is interesting that because the FDA overhauled this process for medical devices because of
[01:33:38.560 --> 01:33:43.560]   Apple and because of demand and made it easier.
[01:33:43.560 --> 01:33:50.200]   So there's been this demand to do that so that we can do this because tech moves fast
[01:33:50.200 --> 01:33:54.560]   and we don't want these slow, expensive processes to hold us back, but then there's the other
[01:33:54.560 --> 01:33:55.560]   side of it.
[01:33:55.560 --> 01:33:59.080]   Well, there's a good reason for this to be slow and expensive.
[01:33:59.080 --> 01:34:00.080]   Yeah.
[01:34:00.080 --> 01:34:05.960]   The only problem is even once, if they get covered by HIPAA, like think about a data breach
[01:34:05.960 --> 01:34:07.640]   for your DNA, right?
[01:34:07.640 --> 01:34:10.680]   You can change your credit card numbers in your passwords.
[01:34:10.680 --> 01:34:15.520]   You can't change your DNA and once that's out there and it's known fact, then insurance
[01:34:15.520 --> 01:34:20.080]   companies can say it's a known fact and it's no fault of your own and it's been covered
[01:34:20.080 --> 01:34:21.080]   by anything.
[01:34:21.080 --> 01:34:24.480]   So even with that, there's still a discussion to be made.
[01:34:24.480 --> 01:34:29.040]   Yeah, I tried to do, they actually didn't accept me, but some years back there was a
[01:34:29.040 --> 01:34:34.800]   project, it was started in 2005, the Personal Genome Project, which I think was a great
[01:34:34.800 --> 01:34:35.800]   benefit to mankind.
[01:34:35.800 --> 01:34:36.800]   I think it was out of Harvard.
[01:34:36.800 --> 01:34:37.800]   Oh, yeah.
[01:34:37.800 --> 01:34:38.800]   And I applied for this.
[01:34:38.800 --> 01:34:43.960]   You had to pay like 1,500 bucks because this was back when DNA testing was very expensive.
[01:34:43.960 --> 01:34:47.840]   And what they said at the time was, look, we're going to collect all your DNA.
[01:34:47.840 --> 01:34:50.600]   We're going to actually, you're going to fill out long questionnaires about your medical
[01:34:50.600 --> 01:34:53.040]   history, your health history, your family health history.
[01:34:53.040 --> 01:34:55.200]   And we do not promise to keep this private.
[01:34:55.200 --> 01:35:01.080]   In fact, we're going to identify it with your name because we know we can't keep it private.
[01:35:01.080 --> 01:35:04.800]   People like Esther Dyson who did this said, yeah, this is good.
[01:35:04.800 --> 01:35:07.960]   And I said, yeah, fine, have my name.
[01:35:07.960 --> 01:35:11.120]   They never made a promise of privacy because they said that's going to be impossible.
[01:35:11.120 --> 01:35:13.040]   It will get out.
[01:35:13.040 --> 01:35:17.960]   So participants are invited to publicly share the genomic and trait data in an integrated,
[01:35:17.960 --> 01:35:20.280]   publicly accessible format.
[01:35:20.280 --> 01:35:23.480]   But you have to sign a fairly significant waiver.
[01:35:23.480 --> 01:35:24.480]   It's not anonymous.
[01:35:24.480 --> 01:35:28.520]   The risks of participant re-identification are addressed up front as an integral part
[01:35:28.520 --> 01:35:32.640]   of the consent and enrollment process, neither anonymity nor confidentially.
[01:35:32.640 --> 01:35:35.680]   Confidentiality of data is promised, period.
[01:35:35.680 --> 01:35:39.840]   And actually, I would have been happy to be accepted.
[01:35:39.840 --> 01:35:42.480]   I don't know why they didn't take me.
[01:35:42.480 --> 01:35:44.280]   That's the difference.
[01:35:44.280 --> 01:35:49.080]   By doing that, I'm an Esther Dyson and the people who have done it are giving to society.
[01:35:49.080 --> 01:35:50.080]   Totally.
[01:35:50.080 --> 01:35:51.760]   And is that fun about it, right?
[01:35:51.760 --> 01:35:57.360]   I think that's a completely different thing to me than saying, you were going to connect
[01:35:57.360 --> 01:35:58.840]   you with family members.
[01:35:58.840 --> 01:36:02.400]   But we're not going to let you know about these other things that might happen.
[01:36:02.400 --> 01:36:06.040]   Yeah, we don't know anything about that.
[01:36:06.040 --> 01:36:08.480]   As long as your blood pressure is up, let's get it up.
[01:36:08.480 --> 01:36:12.440]   A little higher Japanese government has said it's going to hack citizens.
[01:36:12.440 --> 01:36:19.080]   It's going to be the IoT devices to secure them before the Olympics.
[01:36:19.080 --> 01:36:22.840]   Japanese government approved a law on Friday that would allow government workers to hack
[01:36:22.840 --> 01:36:25.400]   into people's IoT devices.
[01:36:25.400 --> 01:36:32.120]   They're trying to survey them to see if you're using default passwords, you know, to find
[01:36:32.120 --> 01:36:37.280]   out if you're secure, if you've got something like the VPN filter on your router, that kind
[01:36:37.280 --> 01:36:40.440]   of thing.
[01:36:40.440 --> 01:36:44.720]   The Japanese government's decision, according to ZDNet, to log into users IoT devices, has
[01:36:44.720 --> 01:36:47.080]   sparked outrage in Japan.
[01:36:47.080 --> 01:36:49.920]   Many have argued it's an unnecessary step.
[01:36:49.920 --> 01:36:55.960]   However, the government plan does have its technical merits because after all, IoT devices
[01:36:55.960 --> 01:37:01.040]   and routers, particularly are notoriously insecure.
[01:37:01.040 --> 01:37:03.000]   What do you think?
[01:37:03.000 --> 01:37:07.240]   I'm surprised this is something ISPs aren't doing already to, like in a certain way.
[01:37:07.240 --> 01:37:11.200]   Like, hey, by the way, you have this thing on your system.
[01:37:11.200 --> 01:37:15.480]   They made you a system-wide ping to just look for default user names and passwords.
[01:37:15.480 --> 01:37:16.880]   I think that's a pretty simple harm thing.
[01:37:16.880 --> 01:37:17.880]   That's what they're going to do.
[01:37:17.880 --> 01:37:23.480]   The National Institute of Information Communications Technology will be allowed to use default
[01:37:23.480 --> 01:37:27.600]   passwords and password dictionaries to attack.
[01:37:27.600 --> 01:37:29.560]   That's interesting password dictionaries.
[01:37:29.560 --> 01:37:35.080]   It's like, I would not mind if ISPs were doing this because it would help a lot of consumers
[01:37:35.080 --> 01:37:37.840]   who aren't super technically savvy.
[01:37:37.840 --> 01:37:39.840]   There's less of a problem for them.
[01:37:39.840 --> 01:37:43.560]   There's certainly the worry, though, that all of a sudden you'll have these ISP specialists
[01:37:43.560 --> 01:37:47.240]   who know they can get into certain systems and who knows what happens.
[01:37:47.240 --> 01:37:51.600]   Well, the idea here is they're going to make a list of devices that use the default passwords
[01:37:51.600 --> 01:37:53.520]   or Monkey123.
[01:37:53.520 --> 01:37:59.080]   They send that to the Internet service provider who then contacts the customer and says,
[01:37:59.080 --> 01:38:00.080]   "Fix your device.
[01:38:00.080 --> 01:38:02.080]   The Olympics are coming.
[01:38:02.080 --> 01:38:03.080]   What are you crazy?"
[01:38:03.080 --> 01:38:05.160]   You could do this in Japan.
[01:38:05.160 --> 01:38:06.560]   It's a very different kind of society.
[01:38:06.560 --> 01:38:08.880]   You do this in the United States.
[01:38:08.880 --> 01:38:09.880]   Insane.
[01:38:09.880 --> 01:38:10.880]   Insane.
[01:38:10.880 --> 01:38:15.280]   Also, what I think the problem in the United States is that in many cases the router is
[01:38:15.280 --> 01:38:17.640]   actually provided by the ISP.
[01:38:17.640 --> 01:38:21.400]   A lot of these in-home devices, a lot of these things are actually controlled by the ISPs,
[01:38:21.400 --> 01:38:25.520]   which to me does kind of open up the idea of, yeah, to vendor explaining maybe they should
[01:38:25.520 --> 01:38:31.800]   be doing some of these scans or maybe we should just, as consumers or as people require
[01:38:31.800 --> 01:38:34.760]   that these things ship with better default.
[01:38:34.760 --> 01:38:36.560]   I don't know.
[01:38:36.560 --> 01:38:39.360]   I understand both concerns, but I guess I understand both sides.
[01:38:39.360 --> 01:38:42.440]   I think that this could potentially be a good thing if you could actually enforce this.
[01:38:42.440 --> 01:38:47.080]   But I also, you're right, this wouldn't work in the USA, the outrage and B, the reality
[01:38:47.080 --> 01:38:52.200]   is is that most people are getting their insecure router with their password or whatever from
[01:38:52.200 --> 01:38:55.360]   their Internet provider.
[01:38:55.360 --> 01:38:59.800]   Or in some cases, now you're seeing apartment complexes who are forcing people to install
[01:38:59.800 --> 01:39:02.560]   smart locks that could be hacked and have other things.
[01:39:02.560 --> 01:39:04.280]   What do you do in those cases?
[01:39:04.280 --> 01:39:08.280]   Where some of your IoT things that might be insecure have been provided to you or you're
[01:39:08.280 --> 01:39:11.240]   forced to use them because they kind of as part of something else that you didn't even
[01:39:11.240 --> 01:39:13.720]   knowingly put in your home.
[01:39:13.720 --> 01:39:18.760]   San Francisco Board of Supervisors has proposed an ordinance to stop secret surveillance
[01:39:18.760 --> 01:39:24.400]   ordinance that would make the city the first in the nation to ban government use of facial
[01:39:24.400 --> 01:39:25.400]   recognition.
[01:39:25.400 --> 01:39:29.680]   It's got to be approved by the board.
[01:39:29.680 --> 01:39:30.680]   Good idea?
[01:39:30.680 --> 01:39:34.560]   I mean, for instance, we know Taylor Swift, your beloved Tate.
[01:39:34.560 --> 01:39:35.560]   I know.
[01:39:35.560 --> 01:39:39.000]   Has used facial recognition at her concerts to keep stalkers at bay?
[01:39:39.000 --> 01:39:40.000]   Yeah.
[01:39:40.000 --> 01:39:43.240]   And I mean, the interesting thing there, I mean, do I love that?
[01:39:43.240 --> 01:39:45.320]   No, I also feel like it's a private event.
[01:39:45.320 --> 01:39:47.280]   So you have the ability to do those things.
[01:39:47.280 --> 01:39:49.240]   And I want Tate to be safe.
[01:39:49.240 --> 01:39:50.240]   Same.
[01:39:50.240 --> 01:39:54.280]   But I couldn't get as outraged about that just because it is in a private place.
[01:39:54.280 --> 01:39:59.320]   And it is not one of those instances where you're in the public and they're doing this.
[01:39:59.320 --> 01:40:02.280]   And do I understand why people are creeped out yet at the same time?
[01:40:02.280 --> 01:40:08.880]   I also understand why when someone has legitimate stalkers, why they might do something like
[01:40:08.880 --> 01:40:13.240]   that, you can be concerned about what happens with where those databases are being stored
[01:40:13.240 --> 01:40:14.240]   and that sort of thing.
[01:40:14.240 --> 01:40:16.400]   But again, you're entering a private event.
[01:40:16.400 --> 01:40:19.040]   I bet you at the Super Bowl, they are using face recognition.
[01:40:19.040 --> 01:40:20.040]   Of course they are.
[01:40:20.040 --> 01:40:21.680]   They're probably doing the digital kinds of things.
[01:40:21.680 --> 01:40:22.680]   The diversity Ben Stadium.
[01:40:22.680 --> 01:40:23.680]   Yeah.
[01:40:23.680 --> 01:40:27.360]   I'm actually more bothered about it in public spaces, if I'm being totally honest.
[01:40:27.360 --> 01:40:31.480]   Like I don't really feel like you necessarily have an expectation of privacy going to like
[01:40:31.480 --> 01:40:36.040]   especially because you don't really have an expectation of privacy in a public space
[01:40:36.040 --> 01:40:37.040]   either.
[01:40:37.040 --> 01:40:41.480]   So it kind of leaves you with nowhere that you really have an expectation of privacy.
[01:40:41.480 --> 01:40:46.280]   A lot of the face recognition, a lot of the face recognition software has been trained
[01:40:46.280 --> 01:40:47.280]   on white people.
[01:40:47.280 --> 01:40:54.280]   It is notoriously bad with people of color, which means a disproportionate number of false
[01:40:54.280 --> 01:40:56.720]   positives for people of color.
[01:40:56.720 --> 01:41:00.760]   So if you're African American, you're more likely to get stopped, detained, maybe even
[01:41:00.760 --> 01:41:04.240]   arrested for a false positive.
[01:41:04.240 --> 01:41:05.920]   And I don't care if it's public or private.
[01:41:05.920 --> 01:41:06.920]   That's not good.
[01:41:06.920 --> 01:41:08.600]   I agree.
[01:41:08.600 --> 01:41:12.920]   We tend to build these systems and think they're magic and kind of overlook these flaws.
[01:41:12.920 --> 01:41:15.320]   And that's the, I think that's the main worry.
[01:41:15.320 --> 01:41:20.040]   Like I have a lot of issues about facial recognition in general, but it's the false positives where
[01:41:20.040 --> 01:41:24.200]   and sort of like DNA testing to write for police, there are certainly issues with it.
[01:41:24.200 --> 01:41:27.840]   But I think in a certain sense, we tend to rely on it a lot.
[01:41:27.840 --> 01:41:31.040]   And that could lead to other sorts of issues.
[01:41:31.040 --> 01:41:32.040]   Let's take a little break.
[01:41:32.040 --> 01:41:36.440]   Devinda Hardewar, he's here from Engadget senior editor over there.
[01:41:36.440 --> 01:41:37.600]   Yes, yes.
[01:41:37.600 --> 01:41:39.240]   We love having you on every day.
[01:41:39.240 --> 01:41:40.240]   Every time.
[01:41:40.240 --> 01:41:41.240]   Congratulations.
[01:41:41.240 --> 01:41:42.240]   I understand.
[01:41:42.240 --> 01:41:43.240]   How old is your newborn now?
[01:41:43.240 --> 01:41:47.640]   Oh, so if he is three and a half months old and I'm beginning the second, it's going
[01:41:47.640 --> 01:41:48.640]   good.
[01:41:48.640 --> 01:41:49.640]   Like she's sleeping better.
[01:41:49.640 --> 01:41:50.640]   We're surviving.
[01:41:50.640 --> 01:41:51.640]   Yeah.
[01:41:51.640 --> 01:41:52.640]   Yeah.
[01:41:52.640 --> 01:41:53.640]   That's the big one.
[01:41:53.640 --> 01:41:54.640]   Oh, yeah.
[01:41:54.640 --> 01:41:55.640]   That's the huge thing.
[01:41:55.640 --> 01:41:56.640]   Oh, yeah.
[01:41:56.640 --> 01:41:57.640]   And I'm just wearing the second half of my paternity leave this month.
[01:41:57.640 --> 01:41:59.840]   So I just have February just to hang out with the baby.
[01:41:59.840 --> 01:42:01.320]   Oh, that's so great.
[01:42:01.320 --> 01:42:02.600]   On a time.
[01:42:02.600 --> 01:42:08.320]   I was very fortunate that I got semi-fired when our first baby was born and I only had
[01:42:08.320 --> 01:42:11.040]   to work a few days a week.
[01:42:11.040 --> 01:42:12.720]   And it was so much fun.
[01:42:12.720 --> 01:42:15.440]   I was the only daddy at Jimberry.
[01:42:15.440 --> 01:42:18.840]   You know, it was, I would go to mommy groups with Abby.
[01:42:18.840 --> 01:42:19.840]   It was so great.
[01:42:19.840 --> 01:42:22.000]   It's the best thing ever.
[01:42:22.000 --> 01:42:24.520]   I wish I could afford to just quit.
[01:42:24.520 --> 01:42:25.520]   I would have.
[01:42:25.520 --> 01:42:27.320]   On to Canada.
[01:42:27.320 --> 01:42:28.320]   We have one year.
[01:42:28.320 --> 01:42:29.320]   We're turning the eternity.
[01:42:29.320 --> 01:42:31.320]   We take care of you.
[01:42:31.320 --> 01:42:32.320]   We go get Georgia.
[01:42:32.320 --> 01:42:33.320]   And get paid.
[01:42:33.320 --> 01:42:34.320]   This one, I'm just going to Georgia.
[01:42:34.320 --> 01:42:36.560]   This one, you got to say this one.
[01:42:36.560 --> 01:42:37.560]   I'm not even.
[01:42:37.560 --> 01:42:39.280]   They're all, they're all benefits.
[01:42:39.280 --> 01:42:40.280]   They're all good ones.
[01:42:40.280 --> 01:42:41.560]   Like, take your childcare as well.
[01:42:41.560 --> 01:42:44.920]   Like once you start working again, like every other country has it better than the US.
[01:42:44.920 --> 01:42:48.480]   You know, this is the completely off topic, but this, but you know, there's a whole debate
[01:42:48.480 --> 01:42:51.960]   now about Medicare for all and this something that a number of Democratic candidates are
[01:42:51.960 --> 01:42:52.960]   proposing.
[01:42:52.960 --> 01:42:59.160]   And then the response from people like Howard Schultz, the CEO of Starbucks, who's running
[01:42:59.160 --> 01:43:02.760]   and Michael Bloomberg that, oh, we can't afford that.
[01:43:02.760 --> 01:43:04.840]   But then I wonder, but Canada can.
[01:43:04.840 --> 01:43:09.200]   In fact, every development in the world except the US can afford it.
[01:43:09.200 --> 01:43:10.200]   I don't.
[01:43:10.200 --> 01:43:12.520]   It's actually cheaper for you in the long run.
[01:43:12.520 --> 01:43:16.040]   So I think it's just people grab hold of a political thing.
[01:43:16.040 --> 01:43:19.120]   I think taking care of all of your people is really important.
[01:43:19.120 --> 01:43:24.000]   This shouldn't even be about politics when it comes to people's health.
[01:43:24.000 --> 01:43:28.320]   Your country is deemed on how you take care of the people that are the sickest, the weakest,
[01:43:28.320 --> 01:43:30.000]   the most needy to that.
[01:43:30.000 --> 01:43:35.000]   And it's, you know that it's actually more affordable because the more that you access
[01:43:35.000 --> 01:43:38.960]   healthcare earlier, the less chance that something big and expensive actually comes down the
[01:43:38.960 --> 01:43:39.960]   road to it.
[01:43:39.960 --> 01:43:43.560]   And then the more that you can stay into the workforce, the more that you contribute,
[01:43:43.560 --> 01:43:45.720]   the more taxes that you can pay.
[01:43:45.720 --> 01:43:48.720]   So I think that it's workable and I don't think that it should be a political issue.
[01:43:48.720 --> 01:43:52.320]   I think that everyone should be able to not have to think about if they have to take care
[01:43:52.320 --> 01:43:55.120]   of their sick child or pay the mortgage payment to that.
[01:43:55.120 --> 01:43:56.120]   It's terrible.
[01:43:56.120 --> 01:44:01.160]   And medical bankruptcy, this is the only country in the world where medical bankruptcy is prevalent
[01:44:01.160 --> 01:44:02.800]   and that shouldn't happen.
[01:44:02.800 --> 01:44:08.280]   And yet I also doubt very much will ever get single-payer health in the United States.
[01:44:08.280 --> 01:44:09.960]   It doesn't seem possible.
[01:44:09.960 --> 01:44:10.960]   Slowly but surely.
[01:44:10.960 --> 01:44:11.960]   Hopefully.
[01:44:11.960 --> 01:44:12.960]   I think you can definitely get it.
[01:44:12.960 --> 01:44:13.960]   Yeah.
[01:44:13.960 --> 01:44:14.960]   You're the only developed nature.
[01:44:14.960 --> 01:44:15.960]   The last one.
[01:44:15.960 --> 01:44:20.960]   I have it.
[01:44:20.960 --> 01:44:21.960]   For sure.
[01:44:21.960 --> 01:44:22.960]   This is going to happen.
[01:44:22.960 --> 01:44:25.960]   I think everyone can get on board with that.
[01:44:25.960 --> 01:44:29.960]   Hey, we have a new show we're launching on the Twitter.
[01:44:29.960 --> 01:44:30.960]   What's that?
[01:44:30.960 --> 01:44:31.960]   It's hot.
[01:44:31.960 --> 01:44:32.960]   Watch.
[01:44:32.960 --> 01:44:33.960]   This is Twitter.
[01:44:33.960 --> 01:44:34.960]   Hey, everybody.
[01:44:34.960 --> 01:44:38.960]   It's the brand new Twitch show, Hands on Tech.
[01:44:38.960 --> 01:44:39.960]   I'm Leo LeBort.
[01:44:39.960 --> 01:44:40.960]   I'm Megan Maroney.
[01:44:40.960 --> 01:44:41.960]   I'm Jason Hell.
[01:44:41.960 --> 01:44:46.960]   Oh, we're all here.
[01:44:46.960 --> 01:44:49.960]   You see, it turns out it's what we get a lot of gear.
[01:44:49.960 --> 01:44:56.960]   Always get a lot of gear to review laptops, phones, VR gear, stuff like robots.
[01:44:56.960 --> 01:44:57.960]   Robots.
[01:44:57.960 --> 01:44:58.960]   This thing.
[01:44:58.960 --> 01:45:00.960]   So we thought it'd be fun whatever that is.
[01:45:00.960 --> 01:45:01.960]   We don't even know, do you?
[01:45:01.960 --> 01:45:02.960]   No.
[01:45:02.960 --> 01:45:04.960]   So we thought it'd be fun to take some of the stuff we get and give you reviews.
[01:45:04.960 --> 01:45:09.520]   But instead of doing a regular review show, we're just going to review everything when
[01:45:09.520 --> 01:45:10.520]   it comes in.
[01:45:10.520 --> 01:45:11.520]   We're going to do a short review.
[01:45:11.520 --> 01:45:15.960]   It might be long reviews, but it'll always be a number of different reviews every week.
[01:45:15.960 --> 01:45:19.680]   So if you subscribe to the Hands on Tech channel, you'll get the latest reviews.
[01:45:19.680 --> 01:45:21.920]   And then you can always Google your favorite product.
[01:45:21.920 --> 01:45:23.040]   Find out what we think of it.
[01:45:23.040 --> 01:45:26.400]   The best way to get ready for Hands on Tech episode one's coming soon.
[01:45:26.400 --> 01:45:30.760]   In fact, it's going to be a review of the new Jaguar I-PACE electric vehicle is to go
[01:45:30.760 --> 01:45:31.760]   to twit.tv/hot.
[01:45:31.760 --> 01:45:36.040]   That stands for Hands on Tech.
[01:45:36.040 --> 01:45:37.040]   Is that robot?
[01:45:37.040 --> 01:45:38.040]   Yeah, it's hurting me.
[01:45:38.040 --> 01:45:39.040]   Sorry.
[01:45:39.040 --> 01:45:44.040]   We'll see you soon on Hands on Tech.
[01:45:44.040 --> 01:45:45.040]   Twit.tv/hot.
[01:45:45.040 --> 01:45:51.760]   You'd like to subscribe and be on YouTube as well.
[01:45:51.760 --> 01:45:52.760]   Our show today brought to you.
[01:45:52.760 --> 01:45:53.760]   That was great.
[01:45:53.760 --> 01:45:54.760]   That's going to be fun.
[01:45:54.760 --> 01:45:56.920]   You know, we realized we've got all this stuff.
[01:45:56.920 --> 01:45:59.120]   We review them on our individual shows.
[01:45:59.120 --> 01:46:02.080]   Why not just make it possible to get just the reviews?
[01:46:02.080 --> 01:46:03.480]   And Megan's on it.
[01:46:03.480 --> 01:46:05.360]   And we always love that.
[01:46:05.360 --> 01:46:11.200]   And actually, anybody who wants to review, I mean, you're already doing it for Engadget
[01:46:11.200 --> 01:46:12.200]   of Indra.
[01:46:12.200 --> 01:46:13.200]   But Christina or Georgia.
[01:46:13.200 --> 01:46:14.200]   Oh, yeah.
[01:46:14.200 --> 01:46:15.200]   I missed reviews.
[01:46:15.200 --> 01:46:16.200]   Let us know.
[01:46:16.200 --> 01:46:17.760]   We'll send you the product you can review it.
[01:46:17.760 --> 01:46:18.920]   We've got a lot of stuff.
[01:46:18.920 --> 01:46:20.200]   Far too much stuff for us to review.
[01:46:20.200 --> 01:46:23.560]   I just sent out a little handheld Osmo.
[01:46:23.560 --> 01:46:26.160]   I mean, how many people are reviewing products in Engadget?
[01:46:26.160 --> 01:46:27.160]   Probably a dozen, right?
[01:46:27.160 --> 01:46:28.160]   It's crazy.
[01:46:28.160 --> 01:46:29.160]   It's endless.
[01:46:29.160 --> 01:46:30.360]   Honestly, our reviews may also sync up.
[01:46:30.360 --> 01:46:31.360]   So let me know.
[01:46:31.360 --> 01:46:32.360]   We will probably get it.
[01:46:32.360 --> 01:46:33.360]   And we can just have that.
[01:46:33.360 --> 01:46:34.360]   They will sync up.
[01:46:34.360 --> 01:46:38.040]   Because honestly, there's a difference between a review and print where you can read the
[01:46:38.040 --> 01:46:39.040]   specs.
[01:46:39.040 --> 01:46:43.000]   And I really like those versus the kind of review we do, which is more like, well, here's what
[01:46:43.000 --> 01:46:44.000]   it's like to use it.
[01:46:44.000 --> 01:46:45.000]   Here's what it looks like.
[01:46:45.000 --> 01:46:46.000]   Here's how it feels.
[01:46:46.000 --> 01:46:47.000]   Nice to see you.
[01:46:47.000 --> 01:46:48.000]   Yeah.
[01:46:48.000 --> 01:46:49.000]   It's also the company.
[01:46:49.000 --> 01:46:50.000]   It's what people want to see right now, too.
[01:46:50.000 --> 01:46:53.840]   Like, I'm feeling the pressure of doing a really strict, traditional review video.
[01:46:53.840 --> 01:46:54.840]   It's very different.
[01:46:54.840 --> 01:46:57.640]   Meanwhile, everyone on YouTube is just doing like, hey, let's look at this cool thing.
[01:46:57.640 --> 01:46:59.600]   And it's really informal.
[01:46:59.600 --> 01:47:02.200]   It seems like the way people want to consume the stuff now, too.
[01:47:02.200 --> 01:47:03.200]   Yeah.
[01:47:03.200 --> 01:47:06.440]   So today we have over 40,000 subscribers on YouTube already.
[01:47:06.440 --> 01:47:07.440]   Nice.
[01:47:07.440 --> 01:47:08.440]   Awesome.
[01:47:08.440 --> 01:47:09.440]   That's hot.
[01:47:09.440 --> 01:47:10.440]   Yeah, that's hot.
[01:47:10.440 --> 01:47:11.440]   Hands on.
[01:47:11.440 --> 01:47:14.000]   I like the acronym though, Feeling Up Tech would have been good too.
[01:47:14.000 --> 01:47:17.960]   But the acronym for Feeling Up Tech is FUT.
[01:47:17.960 --> 01:47:18.960]   I don't know.
[01:47:18.960 --> 01:47:19.960]   Yeah, we have been in the Tatch.
[01:47:19.960 --> 01:47:22.760]   It's not as catchy as the cuter, but not as catchy.
[01:47:22.760 --> 01:47:27.040]   We will be feeling our tech up on HOT.
[01:47:27.040 --> 01:47:28.720]   This week at Tech brought to you by QwIP.
[01:47:28.720 --> 01:47:30.200]   Actually, this is a HOT device.
[01:47:30.200 --> 01:47:33.440]   At least then I just went to Vegas and we brought our QwIPs.
[01:47:33.440 --> 01:47:37.840]   So I love an electric toothbrush, but the QwIP is the best electric toothbrush ever.
[01:47:37.840 --> 01:47:41.800]   By the way, the first subscription electric toothbrush accepted by the American Dental
[01:47:41.800 --> 01:47:42.800]   Association.
[01:47:42.800 --> 01:47:45.360]   So that's a pretty nice thing.
[01:47:45.360 --> 01:47:47.720]   Brushing right, of course, is very important.
[01:47:47.720 --> 01:47:54.000]   The QwIP is a great, affordable electric toothbrush that's an effective clean.
[01:47:54.000 --> 01:47:56.840]   It's gentle and you're sensitive gums.
[01:47:56.840 --> 01:48:00.200]   It's in fact, sometimes people go, "Oh, it's too gentle."
[01:48:00.200 --> 01:48:01.680]   That was my first reaction.
[01:48:01.680 --> 01:48:03.000]   It turned out I've been brushing too hard.
[01:48:03.000 --> 01:48:06.760]   My dentist said, "No, no, please use the QwIP because it's just right.
[01:48:06.760 --> 01:48:11.640]   You've built in two-minute timer, which pulses every 30 seconds to remind you when to switch
[01:48:11.640 --> 01:48:14.840]   sides and help you clean your whole mouth evenly."
[01:48:14.840 --> 01:48:16.520]   Everybody around here started to use the QwIP.
[01:48:16.520 --> 01:48:17.520]   Debbie started using it.
[01:48:17.520 --> 01:48:21.720]   Her dentist said, "I don't know what you're doing, but keep up the good work."
[01:48:21.720 --> 01:48:25.920]   It's because most of us don't brush for a full two minutes or clean evenly.
[01:48:25.920 --> 01:48:26.920]   I love the QwIP.
[01:48:26.920 --> 01:48:29.760]   It's great for travel because it's got a AAA battery in it.
[01:48:29.760 --> 01:48:31.320]   There's no charger.
[01:48:31.320 --> 01:48:35.160]   So I just popped the QwIP in my suitcase and I'm good to go.
[01:48:35.160 --> 01:48:38.000]   It's great at home too because you see that little QwIP holder?
[01:48:38.000 --> 01:48:40.160]   It sticks right on the mirror so you'll never forget the brush.
[01:48:40.160 --> 01:48:43.800]   It's staring you right in the face when you go into the bathroom.
[01:48:43.800 --> 01:48:46.320]   It works as a stand or you can mount it to mirrors.
[01:48:46.320 --> 01:48:48.440]   You can slide over your bristles to pack.
[01:48:48.440 --> 01:48:50.840]   You turn it in upside down and so it's perfect.
[01:48:50.840 --> 01:48:54.480]   It's got everything it needs to pack.
[01:48:54.480 --> 01:48:59.240]   What's great about the QwIP is you automatically get new brush heads, new batteries delivered
[01:48:59.240 --> 01:49:00.920]   on a dentist recommended schedule.
[01:49:00.920 --> 01:49:05.800]   Every three months, just $5 and a friendly reminder when it's time for a refresh so you'll
[01:49:05.800 --> 01:49:08.120]   stay committed to your oral health.
[01:49:08.120 --> 01:49:13.240]   Most of us, whether you're using electric or regular toothbrushes, about 75% of us use
[01:49:13.240 --> 01:49:14.760]   brushes that are worn out.
[01:49:14.760 --> 01:49:16.800]   They are no longer effective.
[01:49:16.800 --> 01:49:18.320]   QwIP is the way to do it.
[01:49:18.320 --> 01:49:22.040]   One of the first electric toothbrushes accepted by the American Dental Association backed by
[01:49:22.040 --> 01:49:27.360]   over 25,000 dental professionals, thousands of verified five star reviews and a lot of
[01:49:27.360 --> 01:49:30.120]   people at Twitter who are happy to use it.
[01:49:30.120 --> 01:49:32.560]   One million happy healthy mouths.
[01:49:32.560 --> 01:49:33.560]   QwIP.
[01:49:33.560 --> 01:49:37.120]   Oh, did I mention it starts at $25?
[01:49:37.120 --> 01:49:39.480]   Go to getquip.com/twit right now.
[01:49:39.480 --> 01:49:43.440]   Get your first refill pack for free when you purchase any QwIP electric toothbrush.
[01:49:43.440 --> 01:49:45.160]   I got his and her toothbrushes.
[01:49:45.160 --> 01:49:46.160]   I'm copper.
[01:49:46.160 --> 01:49:47.320]   She's gold.
[01:49:47.320 --> 01:49:49.920]   So we know the difference and we love them.
[01:49:49.920 --> 01:49:51.200]   Get your first refill pack free.
[01:49:51.200 --> 01:49:53.200]   Getquip.com/twit.
[01:49:53.200 --> 01:49:56.200]   G-E-T-Q-U-I-P.com/twit.
[01:49:56.200 --> 01:49:58.800]   We love crap.
[01:49:58.800 --> 01:50:01.200]   Let's see.
[01:50:01.200 --> 01:50:02.200]   I jumped.
[01:50:02.200 --> 01:50:03.200]   Sometimes this happens.
[01:50:03.200 --> 01:50:07.040]   I jumped to an earlier episode and I have all the old stories.
[01:50:07.040 --> 01:50:10.960]   But here's one that gave me a little, you're all podcasters.
[01:50:10.960 --> 01:50:13.720]   So maybe this will give you a little thrill.
[01:50:13.720 --> 01:50:19.720]   Spotify, according to recode, is in talks to buy Gimlet Media for $200 million.
[01:50:19.720 --> 01:50:20.720]   Wow.
[01:50:20.720 --> 01:50:21.720]   Wow, wow, wow.
[01:50:21.720 --> 01:50:22.720]   Wow.
[01:50:22.720 --> 01:50:23.720]   Wow.
[01:50:23.720 --> 01:50:24.720]   Wow.
[01:50:24.720 --> 01:50:25.720]   Okay.
[01:50:25.720 --> 01:50:26.720]   So Gimlet started right with Alex.
[01:50:26.720 --> 01:50:28.240]   Was it Blumfeld who was?
[01:50:28.240 --> 01:50:29.240]   Yup, Blumberg.
[01:50:29.240 --> 01:50:30.240]   Blumberg.
[01:50:30.240 --> 01:50:32.760]   And it was all about I'm going to raise money to do podcast network, right?
[01:50:32.760 --> 01:50:33.760]   Startup.
[01:50:33.760 --> 01:50:34.760]   Startup.
[01:50:34.760 --> 01:50:37.800]   And they had this few other shows, including one called Homecoming, that they then sold
[01:50:37.800 --> 01:50:42.960]   the rights to the Netflix and it's become a widely beloved TV show in Netflix, One of
[01:50:42.960 --> 01:50:43.960]   Golding.
[01:50:43.960 --> 01:50:46.240]   Didn't it, When a Golded Globe, it was certainly nominated.
[01:50:46.240 --> 01:50:48.440]   It was a scripted podcast, which they turned it.
[01:50:48.440 --> 01:50:49.440]   Amazon Netflix.
[01:50:49.440 --> 01:50:51.440]   Amazon TV.
[01:50:51.440 --> 01:50:53.480]   I watched the first three episodes.
[01:50:53.480 --> 01:50:54.920]   Is it worth continuing?
[01:50:54.920 --> 01:50:55.920]   I just, it doesn't.
[01:50:55.920 --> 01:50:56.920]   I hear it's very good.
[01:50:56.920 --> 01:51:00.720]   I also like watch the first episode was kind of on it, but I hear it's great.
[01:51:00.720 --> 01:51:02.000]   It's slow.
[01:51:02.000 --> 01:51:06.640]   So Gimlet did raise money, you know, startup in 2017.
[01:51:06.640 --> 01:51:10.960]   A funding round valued the company at about $70 million, which even then blows my mind
[01:51:10.960 --> 01:51:13.520]   for a podcast network.
[01:51:13.520 --> 01:51:18.720]   But according to recode, a person familiar with the deal says Spotify will pay more
[01:51:18.720 --> 01:51:24.200]   than 200 million cash for the company.
[01:51:24.200 --> 01:51:27.520]   And the reason it's worth so much to Spotify, I think is pretty clear.
[01:51:27.520 --> 01:51:33.560]   Any company like Spotify that's kind of at the mercy of the record industry is desperately
[01:51:33.560 --> 01:51:37.120]   looking for another place for content, right?
[01:51:37.120 --> 01:51:38.120]   Yeah.
[01:51:38.120 --> 01:51:41.040]   Spotify has 200 million users.
[01:51:41.040 --> 01:51:45.400]   They listen to music, but the royalty rates aren't in their control.
[01:51:45.400 --> 01:51:46.960]   They don't know how much it's going to cost them.
[01:51:46.960 --> 01:51:49.520]   I know it's practically put Pandora out of business.
[01:51:49.520 --> 01:51:51.480]   Pandora, by the way, doing the same thing.
[01:51:51.480 --> 01:51:56.640]   In fact, they're wooing podcasters saying, "We want an exclusive."
[01:51:56.640 --> 01:51:57.640]   Yeah.
[01:51:57.640 --> 01:52:02.640]   Well, I was going to say that would be the interesting thing with if Spotify does buy
[01:52:02.640 --> 01:52:09.400]   Gimlet is do they continue to let Gimlet distribute this to Apple Podcasts and do Google and other
[01:52:09.400 --> 01:52:12.920]   places or do they make it a Spotify exclusive?
[01:52:12.920 --> 01:52:17.360]   Because if you're making that much of an investment, you could understand why you might say, "Well,
[01:52:17.360 --> 01:52:21.560]   we want to be the exclusive home," but you could also say, "Well, your reach is going
[01:52:21.560 --> 01:52:25.400]   to be significantly more limited if you cut off Apple Podcasts."
[01:52:25.400 --> 01:52:26.400]   That's an example.
[01:52:26.400 --> 01:52:27.400]   Right.
[01:52:27.400 --> 01:52:28.400]   Yeah.
[01:52:28.400 --> 01:52:31.880]   I would see, you know, you're all podcasters, so I include you this conversation.
[01:52:31.880 --> 01:52:36.880]   But as far as I'm concerned, I want my show to be everywhere you could possibly get it.
[01:52:36.880 --> 01:52:37.880]   I could see.
[01:52:37.880 --> 01:52:38.880]   Yeah.
[01:52:38.880 --> 01:52:40.360]   I think that's like the dream of podcasting, right?
[01:52:40.360 --> 01:52:42.440]   Because it's a spin off of RSS.
[01:52:42.440 --> 01:52:44.240]   It was about distributing audio everywhere, right?
[01:52:44.240 --> 01:52:50.800]   But I do think we are feeling the end of whatever that tech is so archaic.
[01:52:50.800 --> 01:52:54.200]   The way of distributing a podcast is very, it's just kind of insane.
[01:52:54.200 --> 01:52:55.680]   It's kind of hard to get.
[01:52:55.680 --> 01:52:57.600]   You have to train people.
[01:52:57.600 --> 01:53:01.760]   And if it's within an interface that people already use and understand, "I don't listen
[01:53:01.760 --> 01:53:06.520]   to podcasts within Spotify," but I think a lot of regular casual users, "I've talked
[01:53:06.520 --> 01:53:10.040]   to folks where that's where they listen to podcasts because that's just where they are."
[01:53:10.040 --> 01:53:11.040]   Totally.
[01:53:11.040 --> 01:53:12.040]   So it makes a certain amount of sense.
[01:53:12.040 --> 01:53:13.640]   They just need to fix that interface.
[01:53:13.640 --> 01:53:14.640]   Because like...
[01:53:14.640 --> 01:53:15.640]   I was going to say, yeah.
[01:53:15.640 --> 01:53:16.640]   Yeah.
[01:53:16.640 --> 01:53:18.640]   Discovery on Spotify is terrible.
[01:53:18.640 --> 01:53:19.640]   Yeah.
[01:53:19.640 --> 01:53:22.240]   At the same time, you have the potential to have really good discovery.
[01:53:22.240 --> 01:53:23.240]   Like that is the one benefit they have.
[01:53:23.240 --> 01:53:24.240]   It could make that better.
[01:53:24.240 --> 01:53:25.240]   Yeah.
[01:53:25.240 --> 01:53:27.080]   But they could make it so much better because they already have the analytics of what you
[01:53:27.080 --> 01:53:28.080]   listen to, what you like.
[01:53:28.080 --> 01:53:29.720]   And like, discover weekly is really good.
[01:53:29.720 --> 01:53:30.720]   I mean, you could...
[01:53:30.720 --> 01:53:31.720]   Oh, I love discover weekly.
[01:53:31.720 --> 01:53:36.040]   If Spotify wanted to, they could create a discover weekly for podcasts.
[01:53:36.040 --> 01:53:40.400]   That's a playlist Spotify creates every Monday based on your listening habits.
[01:53:40.400 --> 01:53:44.040]   And it's always got great stuff and a new stuff that I would like.
[01:53:44.040 --> 01:53:45.040]   Yeah.
[01:53:45.040 --> 01:53:48.400]   If they did that with podcasts, that would be very interesting.
[01:53:48.400 --> 01:53:52.000]   We were a launch partner when Spotify added podcasts.
[01:53:52.000 --> 01:53:53.000]   We were a launch partner.
[01:53:53.000 --> 01:53:56.360]   We were up there on stage with Daniel Equity announced it and all that stuff.
[01:53:56.360 --> 01:53:59.960]   But we don't notice a lot of traffic from Spotify because people can't find it.
[01:53:59.960 --> 01:54:00.960]   Yeah.
[01:54:00.960 --> 01:54:01.960]   Same.
[01:54:01.960 --> 01:54:06.160]   I mean, Rocket is also on Spotify.
[01:54:06.160 --> 01:54:09.280]   And I don't have a lot of direct analytic information about that.
[01:54:09.280 --> 01:54:11.000]   And that's actually one of the problems, right?
[01:54:11.000 --> 01:54:15.600]   But there's a problem of podcasts all up is that your analytic information is kind of
[01:54:15.600 --> 01:54:23.480]   a black box because it differs from platform to platform and you have to extrapolate a certain
[01:54:23.480 --> 01:54:25.840]   amount of information from that.
[01:54:25.840 --> 01:54:28.680]   Our number one downloads, I think, is Pocketcast.
[01:54:28.680 --> 01:54:30.320]   Maybe iTunes then Pocketcast.
[01:54:30.320 --> 01:54:33.880]   But that worries me because Pocketcast got bought by public radio.
[01:54:33.880 --> 01:54:38.760]   And I don't know what the future is for non-public radio podcasts, so Pocketcast.
[01:54:38.760 --> 01:54:44.440]   Apple is doing their thing where they're doing measurements and trying to sell that to advertisers.
[01:54:44.440 --> 01:54:51.280]   But you have to use an Apple product to get that an Apple Podcast app to get that information.
[01:54:51.280 --> 01:54:52.640]   I hope you're wrong, Devinda.
[01:54:52.640 --> 01:54:59.320]   I hope that RSS and the completely democratic method of distributing audio shows remains.
[01:54:59.320 --> 01:55:01.400]   Because I think it's really important.
[01:55:01.400 --> 01:55:07.240]   It's just we've seen, I think, for so long because our podcast just passed 500 episodes.
[01:55:07.240 --> 01:55:10.320]   We've been doing it for over 10 years at the last film cast.
[01:55:10.320 --> 01:55:12.400]   We've seen this tech for so long.
[01:55:12.400 --> 01:55:13.720]   The fundamental tech has not changed.
[01:55:13.720 --> 01:55:15.000]   I think that's the big problem.
[01:55:15.000 --> 01:55:18.840]   Analytics, doing something more interesting.
[01:55:18.840 --> 01:55:22.520]   What I like about Spotify too is that when you're playing music, occasionally a video
[01:55:22.520 --> 01:55:26.680]   will pop up too on your phone and maybe lyric info.
[01:55:26.680 --> 01:55:30.600]   Maybe there's some stuff from Rap Genius or Genius, right?
[01:55:30.600 --> 01:55:35.720]   There's more potential to do more with that interface than just have somebody listen to
[01:55:35.720 --> 01:55:38.440]   an audio file and let the screen be kind of useless.
[01:55:38.440 --> 01:55:40.120]   I've been looking forward to something like that too.
[01:55:40.120 --> 01:55:41.320]   Better use of metadata.
[01:55:41.320 --> 01:55:42.320]   Maybe throw up a link.
[01:55:42.320 --> 01:55:45.000]   Maybe throw up a video as we're talking about it.
[01:55:45.000 --> 01:55:47.240]   That could look really cool and really interesting.
[01:55:47.240 --> 01:55:49.720]   So at least Spotify could innovate on that front.
[01:55:49.720 --> 01:55:53.520]   I want RSS to succeed, but even Google Reader couldn't do it.
[01:55:53.520 --> 01:55:55.040]   So who knows how long this will end.
[01:55:55.040 --> 01:55:56.640]   Well, it's really been interesting for podcasts.
[01:55:56.640 --> 01:55:58.880]   I don't know if it's made a difference to you guys.
[01:55:58.880 --> 01:56:00.560]   I don't have any numbers to back this up.
[01:56:00.560 --> 01:56:06.560]   But the fact that you can ask Amazon's Echo or Google Home or Siri to play a podcast
[01:56:06.560 --> 01:56:11.440]   for you and just by name and you get the latest episode, that to me seems like a very
[01:56:11.440 --> 01:56:13.440]   nice way for consumers to listen.
[01:56:13.440 --> 01:56:14.440]   It's hard to control.
[01:56:14.440 --> 01:56:18.400]   It makes it so much easier for people to be able to just listen to something on the fly
[01:56:18.400 --> 01:56:19.400]   while they're doing something.
[01:56:19.400 --> 01:56:20.960]   They don't have to grab anything.
[01:56:20.960 --> 01:56:22.440]   They can just ask for it to play.
[01:56:22.440 --> 01:56:23.440]   It starts to play.
[01:56:23.440 --> 01:56:25.160]   You have to know what to ask for though.
[01:56:25.160 --> 01:56:26.160]   That's the problem.
[01:56:26.160 --> 01:56:27.160]   Yeah.
[01:56:27.160 --> 01:56:28.160]   Yeah.
[01:56:28.160 --> 01:56:32.160]   And you have to say it properly because if not, you get another podcast and that can be
[01:56:32.160 --> 01:56:33.160]   embarrassing.
[01:56:33.160 --> 01:56:34.160]   That's just the world of Echo.
[01:56:34.160 --> 01:56:35.160]   Yeah.
[01:56:35.160 --> 01:56:36.160]   You know what's weird?
[01:56:36.160 --> 01:56:37.160]   That's true.
[01:56:37.160 --> 01:56:42.560]   Every morning I'll get up and I say Echo, listen to the Daily Podcast on my bathroom
[01:56:42.560 --> 01:56:43.720]   Sonos.
[01:56:43.720 --> 01:56:45.600]   That's the exact sentence I say every day.
[01:56:45.600 --> 01:56:49.040]   And it usually plays the New York Times Daily Podcast, but it turns out there's a Daily
[01:56:49.040 --> 01:56:51.400]   Podcast from Australia or something.
[01:56:51.400 --> 01:56:55.120]   And I played that the other day and it's like, "What?"
[01:56:55.120 --> 01:56:56.120]   So this isn't...
[01:56:56.120 --> 01:56:57.120]   That might have been fun.
[01:56:57.120 --> 01:56:59.120]   You might have been a nice way to find an podcast.
[01:56:59.120 --> 01:57:01.160]   They were just trying to add variety.
[01:57:01.160 --> 01:57:02.160]   I should say.
[01:57:02.160 --> 01:57:04.200]   As long as it doesn't start playing, my dad wrote a porno, right?
[01:57:04.200 --> 01:57:05.200]   Like that.
[01:57:05.200 --> 01:57:06.200]   Don't make that mistake.
[01:57:06.200 --> 01:57:07.200]   Otherwise...
[01:57:07.200 --> 01:57:08.200]   Are you amazed?
[01:57:08.200 --> 01:57:10.040]   Don't make your podcast something close to that.
[01:57:10.040 --> 01:57:11.040]   Is what you say.
[01:57:11.040 --> 01:57:14.040]   Are you amazed by the number $200 million for a podcast network?
[01:57:14.040 --> 01:57:15.040]   No.
[01:57:15.040 --> 01:57:16.040]   I'm excited about it.
[01:57:16.040 --> 01:57:17.040]   I'm excited about it too.
[01:57:17.040 --> 01:57:18.040]   Yeah.
[01:57:18.040 --> 01:57:23.840]   And I think to be clear, I don't think that there are very few other, I think, producers
[01:57:23.840 --> 01:57:29.840]   and networks that could even conceive of getting within this realm of amount of money.
[01:57:29.840 --> 01:57:32.760]   But if anybody's going to, it's going to be gimlet.
[01:57:32.760 --> 01:57:39.240]   And I mean, it came out of this American life and people who really helped pioneer this
[01:57:39.240 --> 01:57:44.280]   format in terms of making money out of it and getting mass, mass, mass audiences.
[01:57:44.280 --> 01:57:49.480]   General audiences are not our more niche tech and film and cultural things, which are
[01:57:49.480 --> 01:57:54.560]   great but are not the same level of something like a serial.
[01:57:54.560 --> 01:58:00.600]   And so if anybody is going to get that kind of money, it makes sense it would be them.
[01:58:00.600 --> 01:58:03.720]   You could also say something like, like, you know, like earwolf or some of the others,
[01:58:03.720 --> 01:58:04.880]   although I think midroll might.
[01:58:04.880 --> 01:58:07.960]   How many podcasts do you think gimlet has just out of curiosity?
[01:58:07.960 --> 01:58:08.960]   Do you know?
[01:58:08.960 --> 01:58:09.960]   I don't know.
[01:58:09.960 --> 01:58:10.960]   Probably seven.
[01:58:10.960 --> 01:58:11.960]   Twenty.
[01:58:11.960 --> 01:58:12.960]   Twenty.
[01:58:12.960 --> 01:58:13.960]   That's pretty good.
[01:58:13.960 --> 01:58:15.600]   Can you name any of them?
[01:58:15.600 --> 01:58:16.600]   A startup.
[01:58:16.600 --> 01:58:17.600]   Besides startup?
[01:58:17.600 --> 01:58:18.600]   Yeah.
[01:58:18.600 --> 01:58:19.600]   And homecoming, of course.
[01:58:19.600 --> 01:58:20.600]   Homecoming.
[01:58:20.600 --> 01:58:21.600]   Yeah.
[01:58:21.600 --> 01:58:22.600]   Crime town was good.
[01:58:22.600 --> 01:58:23.600]   I listened to that.
[01:58:23.600 --> 01:58:24.600]   Crime town was really good.
[01:58:24.600 --> 01:58:25.600]   Yeah.
[01:58:25.600 --> 01:58:26.600]   So I don't know.
[01:58:26.600 --> 01:58:27.600]   Two hundred million.
[01:58:27.600 --> 01:58:28.600]   It's a lot.
[01:58:28.600 --> 01:58:30.600]   That's 10 million a show.
[01:58:30.600 --> 01:58:31.600]   It's giggly.
[01:58:31.600 --> 01:58:33.600]   Again, good for the future.
[01:58:33.600 --> 01:58:34.600]   Hopefully you have podcasts.
[01:58:34.600 --> 01:58:37.600]   Maybe that is one of the things we have 12 shows.
[01:58:37.600 --> 01:58:39.080]   I'd sell for 120 million.
[01:58:39.080 --> 01:58:40.080]   No problem.
[01:58:40.080 --> 01:58:41.080]   Spotify, Daniel.
[01:58:41.080 --> 01:58:43.480]   I love you, man.
[01:58:43.480 --> 01:58:44.480]   Call Leo.
[01:58:44.480 --> 01:58:46.800]   But I mean, you could also say that this is in.
[01:58:46.800 --> 01:58:50.360]   It's not just similar from what Netflix and Amazon are doing where they're paying really
[01:58:50.360 --> 01:58:52.800]   huge contracts out to showrunners.
[01:58:52.800 --> 01:58:58.560]   And this is as much about creating future content as it is about the existing lineup
[01:58:58.560 --> 01:59:03.800]   and saying we're going to have exclusive access potentially to these producers.
[01:59:03.800 --> 01:59:08.800]   And that doesn't seem inconceivable.
[01:59:08.800 --> 01:59:12.320]   Two hundred million dollars is inconceivable for that.
[01:59:12.320 --> 01:59:16.040]   I would love to know, and I don't know if this is published, what Gillette's revenue
[01:59:16.040 --> 01:59:17.040]   is.
[01:59:17.040 --> 01:59:21.200]   Oh, I'm sure it's not published because they're private, but I would love to know that as
[01:59:21.200 --> 01:59:22.200]   well.
[01:59:22.200 --> 01:59:28.520]   Well, so nobody knows what the podcast now industry is worth according to some three
[01:59:28.520 --> 01:59:32.440]   hundred fifteen million dollars in 2017 was the revenue.
[01:59:32.440 --> 01:59:33.840]   I don't know.
[01:59:33.840 --> 01:59:34.840]   It seems low.
[01:59:34.840 --> 01:59:35.840]   I could tell you.
[01:59:35.840 --> 01:59:36.840]   I mean, we're public.
[01:59:36.840 --> 01:59:41.160]   We made nine million dollars last year, 10 million the year before in 2017.
[01:59:41.160 --> 01:59:42.640]   So.
[01:59:42.640 --> 01:59:43.640]   But I wonder.
[01:59:43.640 --> 01:59:49.320]   I just know what the revenue is so that I can go to Spotify and say, Hey, hey, you want
[01:59:49.320 --> 01:59:50.320]   some shows?
[01:59:50.320 --> 01:59:53.560]   That would be interesting because if they do make this deal, I think it would have to
[01:59:53.560 --> 01:59:57.160]   be material enough that they would have to disclose it and maybe I think they would.
[01:59:57.160 --> 01:59:58.400]   They're financial things.
[01:59:58.400 --> 02:00:02.640]   So I mean, at the very least of this goes through this could give us some indicator of
[02:00:02.640 --> 02:00:03.640]   what sort of things are making.
[02:00:03.640 --> 02:00:05.400]   So the question is, is it based on hype?
[02:00:05.400 --> 02:00:07.960]   Is it based on realities based on actual revenue?
[02:00:07.960 --> 02:00:13.080]   One of the things that a gimlet does we don't do is they will make they make shows for brands.
[02:00:13.080 --> 02:00:14.080]   Right.
[02:00:14.080 --> 02:00:15.680]   And that's got to be a much better revenue.
[02:00:15.680 --> 02:00:16.680]   Yeah.
[02:00:16.680 --> 02:00:19.800]   No, I would think that your brand stuff that you're doing there, you could, yeah, you could
[02:00:19.800 --> 02:00:24.840]   very easily convince a brand to give you a million dollars, you know, to create a 20
[02:00:24.840 --> 02:00:26.960]   episode or 10 episode show.
[02:00:26.960 --> 02:00:27.960]   Yeah.
[02:00:27.960 --> 02:00:31.840]   Yeah, I could see that company.
[02:00:31.840 --> 02:00:36.920]   According to one of our chatters in 2015, gimlet was estimated to bring in two million
[02:00:36.920 --> 02:00:40.120]   dollars in advertising when we were making five times that.
[02:00:40.120 --> 02:00:42.680]   So I don't, I don't really.
[02:00:42.680 --> 02:00:43.680]   You're doing good.
[02:00:43.680 --> 02:00:45.480]   I'm just saying I would like 200 million dollars.
[02:00:45.480 --> 02:00:46.480]   Okay.
[02:00:46.480 --> 02:00:47.480]   And I know.
[02:00:47.480 --> 02:00:50.800]   This is a rare bit of good news, by the way, for the podcast world because wasn't it last
[02:00:50.800 --> 02:00:53.080]   year, a pan and plea shutdown?
[02:00:53.080 --> 02:00:54.080]   Yeah.
[02:00:54.080 --> 02:00:55.080]   Yeah.
[02:00:55.080 --> 02:00:57.600]   And that was kind of a brutal move because there are a lot of shows on that that I think
[02:00:57.600 --> 02:01:02.120]   were doing really well and yeah, didn't work out so well for that.
[02:01:02.120 --> 02:01:03.120]   It's just crazy.
[02:01:03.120 --> 02:01:04.120]   So this is better news.
[02:01:04.120 --> 02:01:05.120]   It's like a healthy.
[02:01:05.120 --> 02:01:09.960]   It's like when the house next door to you sells for a hundred million dollars.
[02:01:09.960 --> 02:01:12.960]   I had no idea.
[02:01:12.960 --> 02:01:17.320]   And you're like, hey, you know, my house is also available.
[02:01:17.320 --> 02:01:19.040]   But yeah, I know.
[02:01:19.040 --> 02:01:20.680]   Is that the house?
[02:01:20.680 --> 02:01:21.920]   Yeah.
[02:01:21.920 --> 02:01:25.720]   I think I'd move to the Bahamas if I got that kind of money, but I don't know.
[02:01:25.720 --> 02:01:26.720]   Now, don't worry.
[02:01:26.720 --> 02:01:27.720]   We're not for sale.
[02:01:27.720 --> 02:01:29.520]   Americans got 26.
[02:01:29.520 --> 02:01:31.520]   Depending on the numbers.
[02:01:31.520 --> 02:01:33.680]   Maybe with any number.
[02:01:33.680 --> 02:01:37.480]   I shouldn't speak for Lisa, should I?
[02:01:37.480 --> 02:01:40.640]   She's running in here in a second saying, no, no, we are.
[02:01:40.640 --> 02:01:41.640]   Yeah.
[02:01:41.640 --> 02:01:43.840]   300 million and Leo said, no, we're not doing it.
[02:01:43.840 --> 02:01:44.840]   400 million.
[02:01:44.840 --> 02:01:45.840]   We're not doing it.
[02:01:45.840 --> 02:01:46.840]   It doesn't matter.
[02:01:46.840 --> 02:01:51.280]   We saw what happened to Evan Spiegel, Leo, and you don't want to be Evan Spiegel.
[02:01:51.280 --> 02:01:54.880]   So he got offered, Facebook offered him how much?
[02:01:54.880 --> 02:01:56.400]   I think it was originally three billion.
[02:01:56.400 --> 02:02:00.080]   And then I think it went up to 10 and the 10 would have been a good move because they're
[02:02:00.080 --> 02:02:01.080]   not that.
[02:02:01.080 --> 02:02:02.080]   Wow.
[02:02:02.080 --> 02:02:03.880]   This is a little.
[02:02:03.880 --> 02:02:06.680]   But he was thinking, I really got to make these glasses.
[02:02:06.680 --> 02:02:08.600]   So he's got to do that.
[02:02:08.600 --> 02:02:09.600]   What a character.
[02:02:09.600 --> 02:02:12.560]   That's a hard thing to do is somebody, a big company comes to you.
[02:02:12.560 --> 02:02:13.920]   They've already spent what was it?
[02:02:13.920 --> 02:02:17.760]   $22 billion on WhatsApp and says, hey, we'd be interested.
[02:02:17.760 --> 02:02:19.560]   What is 10 billion a number?
[02:02:19.560 --> 02:02:21.560]   And you go, no.
[02:02:21.560 --> 02:02:26.200]   But some people do it for the love of creation.
[02:02:26.200 --> 02:02:27.200]   Yeah.
[02:02:27.200 --> 02:02:28.200]   Yeah.
[02:02:28.200 --> 02:02:30.200]   There we go.
[02:02:30.200 --> 02:02:31.200]   Yeah.
[02:02:31.200 --> 02:02:33.400]   I do it for love, not money.
[02:02:33.400 --> 02:02:34.400]   Literally.
[02:02:34.400 --> 02:02:41.400]   Americans got 26.3 billion robocalls last year.
[02:02:41.400 --> 02:02:42.400]   Yep.
[02:02:42.400 --> 02:02:43.400]   Yep.
[02:02:43.400 --> 02:02:44.320]   26.3 billion.
[02:02:44.320 --> 02:02:47.720]   There's only 300 million people in America.
[02:02:47.720 --> 02:02:49.160]   So do the math.
[02:02:49.160 --> 02:02:52.480]   That was up 46% from the year before.
[02:02:52.480 --> 02:02:54.600]   This is a report released by Haya.
[02:02:54.600 --> 02:02:55.600]   Haya.
[02:02:55.600 --> 02:02:59.320]   A Seattle based spa spam monitoring service.
[02:02:59.320 --> 02:03:01.200]   It has an app.
[02:03:01.200 --> 02:03:04.240]   Sometimes having an app for this kind of thing is good.
[02:03:04.240 --> 02:03:05.240]   In allies to activity.
[02:03:05.240 --> 02:03:07.480]   Which would Apple do what Android does?
[02:03:07.480 --> 02:03:08.480]   Right.
[02:03:08.480 --> 02:03:09.480]   Which lets you how to block this stuff.
[02:03:09.480 --> 02:03:10.480]   Yeah.
[02:03:10.480 --> 02:03:14.680]   From 450,000 users to determine the scope of unwanted robocalling.
[02:03:14.680 --> 02:03:20.800]   They also analyze how people reacted when they got a call.
[02:03:20.800 --> 02:03:25.480]   What's interesting is, according to Haya, people are just not answering the phone anymore.
[02:03:25.480 --> 02:03:26.480]   Yeah.
[02:03:26.480 --> 02:03:27.960]   I was going to say, I was going to ask you guys.
[02:03:27.960 --> 02:03:28.960]   Yeah.
[02:03:28.960 --> 02:03:30.960]   I don't answer my phone anymore unless I know exactly who it is.
[02:03:30.960 --> 02:03:33.360]   And even then, sometimes I'm like, no.
[02:03:33.360 --> 02:03:36.160]   And because I get the fake calls where they look just like your.
[02:03:36.160 --> 02:03:37.160]   Yeah.
[02:03:37.160 --> 02:03:38.160]   It's from your area code.
[02:03:38.160 --> 02:03:39.360]   And sometimes you're exchange.
[02:03:39.360 --> 02:03:40.360]   Totally.
[02:03:40.360 --> 02:03:41.600]   And here's the thing.
[02:03:41.600 --> 02:03:46.000]   Like the area code that my cell phone numbers from is a place I haven't lived in many, many,
[02:03:46.000 --> 02:03:47.400]   many, many years.
[02:03:47.400 --> 02:03:51.120]   And so, and it's also a different area code than like what my parents have, even though
[02:03:51.120 --> 02:03:52.520]   they're from the same city.
[02:03:52.520 --> 02:03:56.480]   And so it's one of those things if I see that area code, I know it's fake.
[02:03:56.480 --> 02:03:57.480]   Right.
[02:03:57.480 --> 02:03:58.480]   Yeah.
[02:03:58.480 --> 02:04:00.640]   But what would we see that's because you don't have kids.
[02:04:00.640 --> 02:04:01.640]   Right.
[02:04:01.640 --> 02:04:04.440]   Georgia, you know, if you get a call from your area code number, but you don't recognize
[02:04:04.440 --> 02:04:07.720]   the number and it's the first three digits, you're going to think that's the school.
[02:04:07.720 --> 02:04:08.720]   Yeah.
[02:04:08.720 --> 02:04:09.720]   That's a name.
[02:04:09.720 --> 02:04:14.080]   And also, yeah, with my job, I think that it might be a client or it might be a hospital
[02:04:14.080 --> 02:04:17.800]   or it might be something else, which I kind of am stuck calling.
[02:04:17.800 --> 02:04:23.640]   And we have a lot of in Canada, there's now an influx of calls from someone that's speaking
[02:04:23.640 --> 02:04:24.640]   Mandarin.
[02:04:24.640 --> 02:04:27.000]   So you don't really understand it at first.
[02:04:27.000 --> 02:04:32.600]   And if you listen in, then the second one that's really popular is a robotic voice that's
[02:04:32.600 --> 02:04:34.080]   saying that they're the police.
[02:04:34.080 --> 02:04:35.840]   They have a warrant for your arrest.
[02:04:35.840 --> 02:04:36.840]   They're coming after you.
[02:04:36.840 --> 02:04:37.840]   I got that one from the IRS.
[02:04:37.840 --> 02:04:38.840]   Yeah.
[02:04:38.840 --> 02:04:39.840]   That's really cute.
[02:04:39.840 --> 02:04:40.840]   I got that one from the IRS.
[02:04:40.840 --> 02:04:41.840]   And I was like, wait, what?
[02:04:41.840 --> 02:04:42.840]   That's a little bit scary.
[02:04:42.840 --> 02:04:45.320]   And then I got to Google it.
[02:04:45.320 --> 02:04:46.320]   Yeah.
[02:04:46.320 --> 02:04:47.320]   Yeah.
[02:04:47.320 --> 02:04:49.640]   It's a little bit scary because they say that they're going to come after you.
[02:04:49.640 --> 02:04:55.000]   And you know, it hits a lot of people and you know why it works is because a lot of
[02:04:55.000 --> 02:04:59.760]   people call back and then they get fed, you know, they don't want to have the RCMP or
[02:04:59.760 --> 02:05:01.000]   least coming up to them.
[02:05:01.000 --> 02:05:05.440]   And so they pay, but everyone's doing that once for the amount of people that do it.
[02:05:05.440 --> 02:05:08.840]   They're making enough money to keep these robocalls in action because it costs them
[02:05:08.840 --> 02:05:09.840]   almost nothing.
[02:05:09.840 --> 02:05:17.160]   I bought Viagra from a Chinese pharmacy once and my phone would not stop ringing.
[02:05:17.160 --> 02:05:19.120]   That's the worst part.
[02:05:19.120 --> 02:05:22.280]   The worst part is this is a few years ago.
[02:05:22.280 --> 02:05:24.600]   My ex wife was so pissed off.
[02:05:24.600 --> 02:05:26.800]   She gave my work number to the guys.
[02:05:26.800 --> 02:05:27.800]   So he's not here.
[02:05:27.800 --> 02:05:29.200]   Call him at work.
[02:05:29.200 --> 02:05:31.200]   So then remember that?
[02:05:31.200 --> 02:05:32.200]   John's going.
[02:05:32.200 --> 02:05:33.200]   Yeah.
[02:05:33.200 --> 02:05:36.760]   This Chinese pharmacy and then dights our work phones.
[02:05:36.760 --> 02:05:38.880]   How do we stop?
[02:05:38.880 --> 02:05:39.880]   I don't know.
[02:05:39.880 --> 02:05:41.480]   I think I said, no, I don't need it anymore.
[02:05:41.480 --> 02:05:42.720]   I'm all better.
[02:05:42.720 --> 02:05:43.720]   I don't know.
[02:05:43.720 --> 02:05:44.720]   It's terrible.
[02:05:44.720 --> 02:05:45.720]   Yeah.
[02:05:45.720 --> 02:05:46.720]   TMI.
[02:05:46.720 --> 02:05:47.720]   I over shared.
[02:05:47.720 --> 02:05:48.720]   Okay.
[02:05:48.720 --> 02:05:49.720]   Hi, yes.
[02:05:49.720 --> 02:05:52.040]   Just I mean, this is a cautionary tale.
[02:05:52.040 --> 02:05:53.800]   Don't answer the phone.
[02:05:53.800 --> 02:05:57.640]   And whatever you do, don't buy the time share.
[02:05:57.640 --> 02:06:00.200]   Be careful what you give who you give your number to.
[02:06:00.200 --> 02:06:01.200]   Yeah.
[02:06:01.200 --> 02:06:02.200]   But in the end, I like it doesn't.
[02:06:02.200 --> 02:06:03.200]   It doesn't mean your number.
[02:06:03.200 --> 02:06:04.200]   They don't need your number.
[02:06:04.200 --> 02:06:05.200]   These are random.
[02:06:05.200 --> 02:06:06.200]   They find you.
[02:06:06.200 --> 02:06:07.200]   That's not your number.
[02:06:07.200 --> 02:06:08.200]   It doesn't matter.
[02:06:08.200 --> 02:06:11.360]   So Haya says only about half of all cell phone calls are now being answered.
[02:06:11.360 --> 02:06:12.360]   Only half.
[02:06:12.360 --> 02:06:14.120]   I don't answer any.
[02:06:14.120 --> 02:06:16.480]   In fact, at this point, I don't care if you get my phone number because I'm not going
[02:06:16.480 --> 02:06:17.920]   to answer it anyway.
[02:06:17.920 --> 02:06:19.880]   Unless I know you, right?
[02:06:19.880 --> 02:06:21.600]   Yeah, totally.
[02:06:21.600 --> 02:06:22.600]   That's terrible.
[02:06:22.600 --> 02:06:28.320]   T-mobile is going to turn on a something they call shaken and stirred.
[02:06:28.320 --> 02:06:34.720]   We talked about this earlier, which is a caller identification system.
[02:06:34.720 --> 02:06:37.880]   AT&T Verizon Sprint said they're going to do it too.
[02:06:37.880 --> 02:06:40.600]   The FCC is endorsing.
[02:06:40.600 --> 02:06:44.080]   This will limit caller ID spoofing.
[02:06:44.080 --> 02:06:47.400]   So it will everybody has to do it or it's not going to work.
[02:06:47.400 --> 02:06:52.360]   But if it does it, then when you get a call with a caller ID, it will actually be the
[02:06:52.360 --> 02:06:54.120]   number of the caller.
[02:06:54.120 --> 02:06:55.960]   That would be nice.
[02:06:55.960 --> 02:06:59.120]   FCC has done nothing about this.
[02:06:59.120 --> 02:07:00.480]   They say they're doing stuff.
[02:07:00.480 --> 02:07:02.400]   There have been arrests.
[02:07:02.400 --> 02:07:11.640]   46% increase last year in robo calls.
[02:07:11.640 --> 02:07:15.480]   Did you see how many movies Netflix released last year?
[02:07:15.480 --> 02:07:16.480]   What was it?
[02:07:16.480 --> 02:07:17.480]   1500?
[02:07:17.480 --> 02:07:18.480]   Jeez.
[02:07:18.480 --> 02:07:19.480]   Originals.
[02:07:19.480 --> 02:07:24.040]   They said they were going to spend $8 billion in 2018.
[02:07:24.040 --> 02:07:25.880]   You can't watch that.
[02:07:25.880 --> 02:07:29.000]   It's some good shows, I have to say.
[02:07:29.000 --> 02:07:31.320]   Yeah, but how do you, this is another discovery issue.
[02:07:31.320 --> 02:07:33.120]   How do you find them?
[02:07:33.120 --> 02:07:35.280]   I have to ask from girl or Georgia.
[02:07:35.280 --> 02:07:36.280]   Exactly.
[02:07:36.280 --> 02:07:38.680]   You have to wait until someone else says something.
[02:07:38.680 --> 02:07:44.800]   Or I do use the, when it says recommended by like whatever 87% or something like that,
[02:07:44.800 --> 02:07:47.640]   I'm like, oh, it's similar to the other shows that I like.
[02:07:47.640 --> 02:07:49.840]   Right now I'm into Night Flyers.
[02:07:49.840 --> 02:07:51.080]   Night Flyers is amazing.
[02:07:51.080 --> 02:07:52.480]   Game of Thrones fans.
[02:07:52.480 --> 02:07:53.480]   Night Flyers.
[02:07:53.480 --> 02:07:54.480]   Let me like that.
[02:07:54.480 --> 02:07:55.480]   See, I never even heard of it.
[02:07:55.480 --> 02:07:57.320]   I think Christina, I think you would like this show.
[02:07:57.320 --> 02:07:58.320]   Okay, I'll check it out.
[02:07:58.320 --> 02:07:59.560]   I haven't heard of it either.
[02:07:59.560 --> 02:08:00.880]   I will definitely check it out.
[02:08:00.880 --> 02:08:06.240]   It is, it is George RR Martin's little novella show that's based and I'm not really into,
[02:08:06.240 --> 02:08:08.960]   well, I was like, I'm not so sure I'm going to be into this.
[02:08:08.960 --> 02:08:11.240]   We've watched five yesterday.
[02:08:11.240 --> 02:08:12.240]   So really watch.
[02:08:12.240 --> 02:08:13.440]   That's the show from sci-fi.
[02:08:13.440 --> 02:08:14.440]   Yeah.
[02:08:14.440 --> 02:08:16.360]   I don't think it's hit Netflix in the US yet.
[02:08:16.360 --> 02:08:17.360]   It has now.
[02:08:17.360 --> 02:08:18.360]   It's really good.
[02:08:18.360 --> 02:08:19.360]   But that's usually the way that I go through it.
[02:08:19.360 --> 02:08:20.640]   I want to turn all the time.
[02:08:20.640 --> 02:08:24.280]   So I've been using foreign Netflix this month for instance more than I've been using American
[02:08:24.280 --> 02:08:25.280]   Netflix.
[02:08:25.280 --> 02:08:26.280]   So that's fine.
[02:08:26.280 --> 02:08:27.640]   I'll be in Australia in a week.
[02:08:27.640 --> 02:08:28.640]   So it's fine.
[02:08:28.640 --> 02:08:30.400]   Wow, they're making you travel.
[02:08:30.400 --> 02:08:31.400]   Yeah.
[02:08:31.400 --> 02:08:33.680]   Do you mind?
[02:08:33.680 --> 02:08:36.520]   If I had to do this all the time, I would probably not love it.
[02:08:36.520 --> 02:08:41.200]   But for the, the, I knew going into this year that the first half of the year I'd be doing
[02:08:41.200 --> 02:08:42.200]   a lot of travel.
[02:08:42.200 --> 02:08:43.840]   Are you talking to developers at conferences?
[02:08:43.840 --> 02:08:44.840]   Yeah, exactly.
[02:08:44.840 --> 02:08:45.840]   Oh, that's cool.
[02:08:45.840 --> 02:08:50.160]   We've been doing a global tour called Microsoft Ignite the Tour where we've been going to
[02:08:50.160 --> 02:08:52.560]   a bunch of different cities all over the world.
[02:08:52.560 --> 02:08:55.400]   And so I've been so far.
[02:08:55.400 --> 02:08:58.160]   So we started, we kicked it off in December and we were in Berlin and then we were in
[02:08:58.160 --> 02:08:59.160]   Sao Paulo.
[02:08:59.160 --> 02:09:03.720]   And then this year I've been to Tel Aviv and Milan.
[02:09:03.720 --> 02:09:07.440]   But there've been, there was one in Johannesburg.
[02:09:07.440 --> 02:09:09.600]   There's one in DC like right now.
[02:09:09.600 --> 02:09:14.160]   There was one in Seoul, no, not Seoul, Singapore.
[02:09:14.160 --> 02:09:18.000]   I will get to see any of these places or is it just in and out?
[02:09:18.000 --> 02:09:19.000]   You know what?
[02:09:19.000 --> 02:09:20.000]   Some, some more than others.
[02:09:20.000 --> 02:09:24.080]   I was, for instance, able to see a little bit more of Milan even though it was raining.
[02:09:24.080 --> 02:09:29.360]   I also got to see a little bit of Tel Aviv, but sometimes it's just in and out.
[02:09:29.360 --> 02:09:32.720]   But I, the good thing is, the worst kind of business travel, you go to somewhere really
[02:09:32.720 --> 02:09:33.920]   cool.
[02:09:33.920 --> 02:09:34.920]   And then you have to leave immediately.
[02:09:34.920 --> 02:09:36.760]   And you don't get to see it.
[02:09:36.760 --> 02:09:40.680]   The cool thing is that when I, when I'm leaving for Australia, I'm actually going to be staying
[02:09:40.680 --> 02:09:44.320]   in extra couple of days because we're going to be doing something with the windows insider
[02:09:44.320 --> 02:09:46.400]   team at the Microsoft store in Sydney.
[02:09:46.400 --> 02:09:49.360]   And so I'm going to get to see more, more places.
[02:09:49.360 --> 02:09:52.920]   So Australia is one of my favorite places and Sydney is one of the best cities in the
[02:09:52.920 --> 02:09:55.120]   world is just awesome.
[02:09:55.120 --> 02:09:59.160]   And good coffee, which I know is important to you.
[02:09:59.160 --> 02:10:00.160]   Definitely.
[02:10:00.160 --> 02:10:01.160]   Let's take a little break.
[02:10:01.160 --> 02:10:03.560]   When we come back, I'm going to give you all assignment.
[02:10:03.560 --> 02:10:06.320]   What show should I watch next?
[02:10:06.320 --> 02:10:13.320]   Georgia Dow is here.
[02:10:13.320 --> 02:10:17.920]   She, and we'll talk about her videos, anxiety videos.com.
[02:10:17.920 --> 02:10:25.440]   She's senior editor, I'm more film girl herself, senior developer, cloud developer, advocate at Microsoft.
[02:10:25.440 --> 02:10:30.280]   Also the host of the fabulous rocket podcast with Simone and Brianna.
[02:10:30.280 --> 02:10:36.280]   And that's a relay.fm, which by the way, if Gimla's worth 200 million, I think relay's
[02:10:36.280 --> 02:10:37.280]   in.
[02:10:37.280 --> 02:10:40.120]   Tell him you're worth more.
[02:10:40.120 --> 02:10:43.200]   And the vendor hard of war slash film is his podcast.
[02:10:43.200 --> 02:10:46.280]   He's senior editor and in gadget, you must have something.
[02:10:46.280 --> 02:10:47.280]   It could be a movie.
[02:10:47.280 --> 02:10:52.240]   It could be a TV show, something everybody, some hidden secret gem.
[02:10:52.240 --> 02:10:53.240]   No one's heard of.
[02:10:53.240 --> 02:10:55.480]   Oh man, there's so many.
[02:10:55.480 --> 02:10:57.720]   I will say, don't you want it now?
[02:10:57.720 --> 02:10:59.040]   No, hold on to it.
[02:10:59.040 --> 02:11:03.760]   I'm giving you time to think about it while I talk about our past pass, which is not a
[02:11:03.760 --> 02:11:04.760]   hidden gem.
[02:11:04.760 --> 02:11:10.720]   Anybody who watches Twit for any length of time knows I'm a complete last pass advocate
[02:11:10.720 --> 02:11:12.600]   and fanatic.
[02:11:12.600 --> 02:11:16.040]   I've been using last pass for a decade ever since it started.
[02:11:16.040 --> 02:11:19.960]   You may remember about five years ago now, we had Steve Gibson interviewed the creator
[02:11:19.960 --> 02:11:21.280]   of last pass.
[02:11:21.280 --> 02:11:24.840]   Joe Seagress showed him the code, explained how everything worked.
[02:11:24.840 --> 02:11:27.440]   Steve Gibson, our security guru, said this is awesome.
[02:11:27.440 --> 02:11:29.240]   He started using it.
[02:11:29.240 --> 02:11:30.240]   And then something happened.
[02:11:30.240 --> 02:11:34.720]   One of our engineers posted all of this company's passwords, the database, the website,
[02:11:34.720 --> 02:11:38.600]   the everything on a public website, because he couldn't remember them.
[02:11:38.600 --> 02:11:41.780]   And we said, you know, maybe we should use last pass here.
[02:11:41.780 --> 02:11:43.580]   We got last pass enterprise.
[02:11:43.580 --> 02:11:47.680]   If you have a business, you know breaches are the worst thing that can happen to your
[02:11:47.680 --> 02:11:50.120]   reputation, to your company, to your bottom line.
[02:11:50.120 --> 02:11:51.520]   It's the worst.
[02:11:51.520 --> 02:11:53.040]   But where is the weak spot?
[02:11:53.040 --> 02:11:56.640]   Well, it's your employees and it's, and it's frankly, it's your passwords.
[02:11:56.640 --> 02:11:58.480]   That's why we use last pass enterprise.
[02:11:58.480 --> 02:12:02.800]   Reduce the threat of breach with organization wide password management.
[02:12:02.800 --> 02:12:06.480]   A hundred policies that you can set for your employees.
[02:12:06.480 --> 02:12:08.840]   For instance, we require two factor.
[02:12:08.840 --> 02:12:10.560]   You can set master password requirements.
[02:12:10.560 --> 02:12:13.320]   So it can't be monkey one, two, three.
[02:12:13.320 --> 02:12:14.720]   You've got password resets.
[02:12:14.720 --> 02:12:17.360]   You can restrict access of employee leaves.
[02:12:17.360 --> 02:12:18.720]   They can share passwords.
[02:12:18.720 --> 02:12:22.120]   And we know we know employees share passwords, not just with them, with other employees,
[02:12:22.120 --> 02:12:23.960]   but with the other people outside the company.
[02:12:23.960 --> 02:12:27.120]   They can share passwords, but they don't know the password.
[02:12:27.120 --> 02:12:28.160]   So they just see dots.
[02:12:28.160 --> 02:12:31.720]   So you can actually set it up so that they can log into the, you know, quick books or
[02:12:31.720 --> 02:12:36.440]   the bank accounts or the databases, but they don't even have access to the password.
[02:12:36.440 --> 02:12:40.960]   We use shared folders, which is a nice feature of enterprise that lets our ops team have
[02:12:40.960 --> 02:12:41.960]   all the passwords they need.
[02:12:41.960 --> 02:12:44.240]   We don't have to individually share it.
[02:12:44.240 --> 02:12:47.440]   Our business office has all the passwords they need.
[02:12:47.440 --> 02:12:48.480]   It's more of the passwords for me.
[02:12:48.480 --> 02:12:51.840]   I put everything in last pass database logins, SSH keys.
[02:12:51.840 --> 02:12:55.080]   Yes, I put my SSH keys in there.
[02:12:55.080 --> 02:12:56.080]   Software licenses.
[02:12:56.080 --> 02:13:01.680]   My driver's license, social security number, and pass port is in last pass.
[02:13:01.680 --> 02:13:03.480]   Because I know I have it everywhere.
[02:13:03.480 --> 02:13:08.480]   That's the beauty of LastPass on every device, every operating system, Windows, Mac, Linux,
[02:13:08.480 --> 02:13:09.840]   Android, iOS.
[02:13:09.840 --> 02:13:12.480]   The first thing I do when I get a new phone, I put LastPass on there.
[02:13:12.480 --> 02:13:13.720]   It helps me set up everything else.
[02:13:13.720 --> 02:13:19.600]   In fact, now thanks to iOS 12 and Android Pie, it's very easy because it just auto-fills
[02:13:19.600 --> 02:13:20.600]   the password.
[02:13:20.600 --> 02:13:23.840]   LastPass sits there and when you get to a site with needs a password or an app that needs
[02:13:23.840 --> 02:13:26.120]   a password, just fills it in.
[02:13:26.120 --> 02:13:28.000]   And it uses Face ID or Touch ID.
[02:13:28.000 --> 02:13:30.320]   So all the security is there.
[02:13:30.320 --> 02:13:35.960]   LastPass's password generator makes it easy for people to generate real, long, strong,
[02:13:35.960 --> 02:13:37.240]   random passwords.
[02:13:37.240 --> 02:13:38.240]   You don't have to remember them.
[02:13:38.240 --> 02:13:39.480]   You don't have to write them down.
[02:13:39.480 --> 02:13:41.360]   It stores them for you.
[02:13:41.360 --> 02:13:45.320]   It even has a security check to look for passwords, places where maybe you've used the same password
[02:13:45.320 --> 02:13:49.480]   more than once and helps you change those automatically.
[02:13:49.480 --> 02:13:50.640]   It's just the best.
[02:13:50.640 --> 02:13:52.600]   You've got to have LastPass.
[02:13:52.600 --> 02:13:58.880]   True Trust No One Zero Knowledge Security Model, the data in your vault is never decrypted
[02:13:58.880 --> 02:14:00.520]   anywhere except on device.
[02:14:00.520 --> 02:14:02.480]   It's encrypted and decrypted at the device level.
[02:14:02.480 --> 02:14:04.080]   LastPass doesn't have access to it.
[02:14:04.080 --> 02:14:06.320]   No one does just you.
[02:14:06.320 --> 02:14:11.080]   If you're an active directory user, and I know a lot of people use Microsoft Active Directory,
[02:14:11.080 --> 02:14:15.480]   you can use your AD credentials to log in a single sign-on.
[02:14:15.480 --> 02:14:19.960]   Whether your team just needs to share passwords or you're worried about a data breach, LastPass
[02:14:19.960 --> 02:14:22.200]   scales to fit your business needs.
[02:14:22.200 --> 02:14:27.280]   So if you're an individual, LastPass premium, great choice, the family, we use LastPass for
[02:14:27.280 --> 02:14:31.840]   families because that allows me to share with Lisa very easily and they all use LastPass
[02:14:31.840 --> 02:14:33.840]   even the kids.
[02:14:33.840 --> 02:14:39.320]   LastPass for teams is for a smaller enterprise, 50 or fewer, and of course we use the big boy,
[02:14:39.320 --> 02:14:41.720]   LastPass Enterprise.
[02:14:41.720 --> 02:14:42.720]   LastPass Enterprise.
[02:14:42.720 --> 02:14:45.480]   It's the first app I install on any device.
[02:14:45.480 --> 02:14:50.400]   It's the first app you should be using to protect yourself, LastPass.com/twit.
[02:14:50.400 --> 02:14:53.640]   If you're not using it, you're nuts.
[02:14:53.640 --> 02:14:57.520]   LastPass.com/twits.
[02:14:57.520 --> 02:15:02.000]   Streaming Observer, counted, they say Amazon Prime has four and a half times as many movies
[02:15:02.000 --> 02:15:04.240]   as Netflix.
[02:15:04.240 --> 02:15:11.520]   But look at that, 17,000 Netflix, only 3,800 Hulu, 2,000 HBO, no 800 or 15.
[02:15:11.520 --> 02:15:20.760]   But if you look at fresh Rotten Tomatoes, Netflix certified fresh movies, 596 compared
[02:15:20.760 --> 02:15:26.320]   to 232 on Amazon, 223 on Hulu and 38 on HBO.
[02:15:26.320 --> 02:15:32.400]   15% of all the Netflix content is certified fresh on Rotten Tomatoes.
[02:15:32.400 --> 02:15:33.560]   That's good.
[02:15:33.560 --> 02:15:34.560]   That's a killer.
[02:15:34.560 --> 02:15:35.920]   Yeah, that's their killer feature, right?
[02:15:35.920 --> 02:15:38.120]   There's always something vaguely good to watch.
[02:15:38.120 --> 02:15:39.120]   Vaguely.
[02:15:39.120 --> 02:15:40.120]   There's the word.
[02:15:40.120 --> 02:15:41.120]   Vaguely, right.
[02:15:41.120 --> 02:15:42.120]   That may not be the best thing yet.
[02:15:42.120 --> 02:15:43.120]   It's okay though.
[02:15:43.120 --> 02:15:44.120]   You know what I love.
[02:15:44.120 --> 02:15:45.120]   I'm going to tell I'll start us off.
[02:15:45.120 --> 02:15:48.080]   I've been watching on Netflix a documentary series called Seven Days.
[02:15:48.080 --> 02:15:49.520]   Have you guys seen this?
[02:15:49.520 --> 02:15:50.520]   No.
[02:15:50.520 --> 02:15:51.520]   Oh, I love--
[02:15:51.520 --> 02:15:54.840]   Seven Days, I'm tracking it down on my TV, new to TV.
[02:15:54.840 --> 02:15:56.440]   I love documentaries, right?
[02:15:56.440 --> 02:16:01.040]   And the premise of this is it's a variety of different events.
[02:16:01.040 --> 02:16:02.040]   I don't know how many there are.
[02:16:02.040 --> 02:16:04.800]   I've watched Seven or Eight.
[02:16:04.800 --> 02:16:09.760]   It's a variety of different events, but it's the Seven Days leading up to.
[02:16:09.760 --> 02:16:14.920]   So it's the Kentucky Derby and they talk to the traders and it's the Seven Days leading
[02:16:14.920 --> 02:16:16.800]   up to and then the event.
[02:16:16.800 --> 02:16:22.160]   They do the League of Legends grand championship.
[02:16:22.160 --> 02:16:23.320]   They talk to the different teams.
[02:16:23.320 --> 02:16:26.280]   It's really well done.
[02:16:26.280 --> 02:16:31.640]   The opening of a great restaurant in New York and all the stuff leading up to it.
[02:16:31.640 --> 02:16:36.600]   So if you like documentaries, I think it's-- I got to find the name of it.
[02:16:36.600 --> 02:16:37.600]   Seven Days before--
[02:16:37.600 --> 02:16:38.600]   Seven Days out.
[02:16:38.600 --> 02:16:39.600]   Seven Days out.
[02:16:39.600 --> 02:16:40.700]   Seven Days out.
[02:16:40.700 --> 02:16:42.200]   Thank you.
[02:16:42.200 --> 02:16:46.320]   It's-- they did-- yeah, there it is.
[02:16:46.320 --> 02:16:49.360]   Now, if you like fashion and even if you don't, this is fascinating.
[02:16:49.360 --> 02:16:52.640]   The Chanel Paris Fashion Show.
[02:16:52.640 --> 02:16:54.600]   Seven Days out.
[02:16:54.600 --> 02:16:57.120]   And they show the preparations.
[02:16:57.120 --> 02:16:59.440]   They go, "Oh, there's the dog show, the Westminster dog show."
[02:16:59.440 --> 02:17:00.440]   Is it a mistake?
[02:17:00.440 --> 02:17:01.440]   Yeah, I guess everything.
[02:17:01.440 --> 02:17:02.440]   A JPL launch.
[02:17:02.440 --> 02:17:03.440]   It's the best restaurant in the world.
[02:17:03.440 --> 02:17:04.440]   Newly, right?
[02:17:04.440 --> 02:17:05.440]   Oh, it's cool.
[02:17:05.440 --> 02:17:06.440]   I love documentaries.
[02:17:06.440 --> 02:17:07.440]   Seven Days.
[02:17:07.440 --> 02:17:08.440]   That's really cool.
[02:17:08.440 --> 02:17:09.440]   I do too.
[02:17:09.440 --> 02:17:10.440]   It's a great premise.
[02:17:10.440 --> 02:17:11.440]   I love documentaries.
[02:17:11.440 --> 02:17:12.440]   Yeah, it's a great premise.
[02:17:12.440 --> 02:17:13.440]   Seven Days out.
[02:17:13.440 --> 02:17:14.440]   I think you'll like it.
[02:17:14.440 --> 02:17:15.440]   Anyway, that's my trick.
[02:17:15.440 --> 02:17:19.720]   It's like, I feel like every documentary has that meaty moment of where the action is happening.
[02:17:19.720 --> 02:17:21.520]   And they just like hone down into that, right?
[02:17:21.520 --> 02:17:22.520]   Yeah.
[02:17:22.520 --> 02:17:23.520]   The big climax of every situation.
[02:17:23.520 --> 02:17:28.640]   Yeah, and you're prepared because you've been watching, you know, the Six Days before.
[02:17:28.640 --> 02:17:30.960]   And so it's really-- it's really got some drama.
[02:17:30.960 --> 02:17:31.960]   It's well done.
[02:17:31.960 --> 02:17:34.800]   All right, Georgia Dow, you already gave us one.
[02:17:34.800 --> 02:17:36.720]   You want to give us-- you said Nightflowers on SciFi.
[02:17:36.720 --> 02:17:39.520]   I'll give you-- yeah, that one's a lot of fun.
[02:17:39.520 --> 02:17:44.920]   But I'll give you another one that's non-educational, but a really great-- if you're into something
[02:17:44.920 --> 02:17:47.120]   scary, the haunting of Hill House is--
[02:17:47.120 --> 02:17:48.120]   Oh, yeah.
[02:17:48.120 --> 02:17:49.120]   --really--
[02:17:49.120 --> 02:17:50.120]   It's perfect.
[02:17:50.120 --> 02:17:51.200]   --exceptionally well done.
[02:17:51.200 --> 02:17:53.480]   The character development is superb.
[02:17:53.480 --> 02:17:55.140]   The way everything fits together.
[02:17:55.140 --> 02:17:58.600]   My biggest pet peeved the psychology behind it because I can't do it.
[02:17:58.600 --> 02:18:00.400]   If the psychology's wrong, it bothers me.
[02:18:00.400 --> 02:18:02.120]   I can't watch something.
[02:18:02.120 --> 02:18:04.120]   Psychology behind it is fabulous.
[02:18:04.120 --> 02:18:05.120]   It's gripping.
[02:18:05.120 --> 02:18:07.240]   It gets better as it goes along.
[02:18:07.240 --> 02:18:09.440]   You feel like you get it.
[02:18:09.440 --> 02:18:10.440]   It's lovely.
[02:18:10.440 --> 02:18:11.440]   Terrifying.
[02:18:11.440 --> 02:18:14.640]   If you have like night terrors, maybe don't watch this.
[02:18:14.640 --> 02:18:15.640]   But it's really good.
[02:18:15.640 --> 02:18:16.640]   It's a series or a--
[02:18:16.640 --> 02:18:17.640]   Yeah.
[02:18:17.640 --> 02:18:18.640]   --series.
[02:18:18.640 --> 02:18:24.600]   And Stephen King said it is the best in its series out there.
[02:18:24.600 --> 02:18:25.760]   So Stephen King loves it.
[02:18:25.760 --> 02:18:31.160]   See, I tried to watch Castle Rock, the Stephen King set miniseries on Hulu.
[02:18:31.160 --> 02:18:32.160]   This is--
[02:18:32.160 --> 02:18:33.160]   It wasn't good.
[02:18:33.160 --> 02:18:34.160]   This is so much better.
[02:18:34.160 --> 02:18:35.160]   It's OK.
[02:18:35.160 --> 02:18:36.160]   Yeah.
[02:18:36.160 --> 02:18:37.160]   This is it.
[02:18:37.160 --> 02:18:38.160]   All right.
[02:18:38.160 --> 02:18:41.040]   The cool part is there's like little background features, things that you didn't catch right
[02:18:41.040 --> 02:18:43.120]   away that you can like rewatch it.
[02:18:43.120 --> 02:18:44.120]   I'm surprised.
[02:18:44.120 --> 02:18:52.560]   Just that a psychotherapist, the founder of anxiety-videos.com, is watching a scary
[02:18:52.560 --> 02:18:54.360]   series like that.
[02:18:54.360 --> 02:18:55.360]   It's all in good ones.
[02:18:55.360 --> 02:18:57.520]   Once you dealt with your anxiety, you can watch that.
[02:18:57.520 --> 02:18:58.520]   You're going to enjoy it.
[02:18:58.520 --> 02:19:01.600]   Or you can get a geo and watch it together.
[02:19:01.600 --> 02:19:03.600]   You can go deep.
[02:19:03.600 --> 02:19:04.600]   You can't.
[02:19:04.600 --> 02:19:05.600]   You can't.
[02:19:05.600 --> 02:19:09.160]   It's really only scary up to the fourth episode that it does.
[02:19:09.160 --> 02:19:11.640]   It's not as scary because you understand more.
[02:19:11.640 --> 02:19:15.520]   So maybe just like if you can make it through the first four, you're good.
[02:19:15.520 --> 02:19:16.520]   Wow.
[02:19:16.520 --> 02:19:17.520]   Wow.
[02:19:17.520 --> 02:19:18.520]   This is exceptionally well done.
[02:19:18.520 --> 02:19:19.520]   I did see it many times.
[02:19:19.520 --> 02:19:20.520]   Yeah.
[02:19:20.520 --> 02:19:21.520]   All right.
[02:19:21.520 --> 02:19:22.520]   I'm right.
[02:19:22.520 --> 02:19:23.520]   I got my pencil.
[02:19:23.520 --> 02:19:24.520]   I'm writing this all down.
[02:19:24.520 --> 02:19:25.520]   You should watch this.
[02:19:25.520 --> 02:19:26.520]   I'm going to watch that.
[02:19:26.520 --> 02:19:27.520]   Lisa loves horror.
[02:19:27.520 --> 02:19:28.520]   I hate horror.
[02:19:28.520 --> 02:19:29.520]   I don't watch horror.
[02:19:29.520 --> 02:19:30.520]   Hello.
[02:19:30.520 --> 02:19:32.520]   But this is not like it's--
[02:19:32.520 --> 02:19:33.520]   I like suspense.
[02:19:33.520 --> 02:19:35.680]   It's not like someone jumps out of everything.
[02:19:35.680 --> 02:19:36.680]   It's suspenseful.
[02:19:36.680 --> 02:19:37.680]   That's what I would say.
[02:19:37.680 --> 02:19:39.360]   It's towards the horror.
[02:19:39.360 --> 02:19:43.480]   It's not like the bird box where it's not horror or suspenseful.
[02:19:43.480 --> 02:19:44.840]   Is bird box any good?
[02:19:44.840 --> 02:19:45.840]   It's just bad.
[02:19:45.840 --> 02:19:46.840]   It's not bad.
[02:19:46.840 --> 02:19:49.560]   Isn't it sad people are getting run over for a bad movie?
[02:19:49.560 --> 02:19:53.640]   Well, it's good to see you know what the memes are about, but it's not well done.
[02:19:53.640 --> 02:19:54.640]   No.
[02:19:54.640 --> 02:19:55.640]   Oh, I love it.
[02:19:55.640 --> 02:19:56.640]   I'm right in the stand.
[02:19:56.640 --> 02:19:57.640]   All right.
[02:19:57.640 --> 02:20:02.400]   Now, because this is great because we have three people who really are experts in pop
[02:20:02.400 --> 02:20:05.560]   culture and mass culture and film and TV.
[02:20:05.560 --> 02:20:06.960]   Couldn't have three better people to do this.
[02:20:06.960 --> 02:20:10.880]   How about you, Christina Warren Film Girl?
[02:20:10.880 --> 02:20:11.880]   Yes.
[02:20:11.880 --> 02:20:12.880]   All right.
[02:20:12.880 --> 02:20:14.880]   So I have to-- OK.
[02:20:14.880 --> 02:20:17.120]   So actually I first have a podcast recommendation.
[02:20:17.120 --> 02:20:18.120]   Good.
[02:20:18.120 --> 02:20:23.720]   Because I binged it all yesterday on my flight from Milan to New York.
[02:20:23.720 --> 02:20:26.040]   And it's called The Dream.
[02:20:26.040 --> 02:20:31.120]   And it's all about multi-level marketing scams, network marketing things.
[02:20:31.120 --> 02:20:32.120]   There's like--
[02:20:32.120 --> 02:20:33.120]   Oh, there's so many of those.
[02:20:33.120 --> 02:20:34.120]   I think--
[02:20:34.120 --> 02:20:37.120]   Oh, but it's so good.
[02:20:37.120 --> 02:20:41.360]   And Jane Marie, who's done stuff for this American life, she-- her company produced
[02:20:41.360 --> 02:20:42.360]   it.
[02:20:42.360 --> 02:20:43.360]   It's 11 episodes.
[02:20:43.360 --> 02:20:44.360]   It's fantastic.
[02:20:44.360 --> 02:20:45.360]   It's so good.
[02:20:45.360 --> 02:20:47.480]   Like it really, really, really good.
[02:20:47.480 --> 02:20:50.000]   I'm somebody who thought I knew a lot about that world.
[02:20:50.000 --> 02:20:53.640]   And I did-- like it enlightened things for me.
[02:20:53.640 --> 02:20:55.120]   And it's really, really good.
[02:20:55.120 --> 02:20:58.400]   So The Dream highly recommended.
[02:20:58.400 --> 02:21:04.080]   And as much as I'm mad about his revelation at the end of Gossip Girl, I really do
[02:21:04.080 --> 02:21:06.680]   like the new Netflix show You.
[02:21:06.680 --> 02:21:07.680]   OK.
[02:21:07.680 --> 02:21:08.680]   With that.
[02:21:08.680 --> 02:21:09.680]   It's a lifetime show.
[02:21:09.680 --> 02:21:10.680]   It's really good.
[02:21:10.680 --> 02:21:11.680]   Yeah.
[02:21:11.680 --> 02:21:12.680]   OK.
[02:21:12.680 --> 02:21:13.680]   Do I have to have liked Gossip Girl?
[02:21:13.680 --> 02:21:14.680]   I'll track that one down.
[02:21:14.680 --> 02:21:15.680]   No.
[02:21:15.680 --> 02:21:16.680]   OK.
[02:21:16.680 --> 02:21:18.720]   By the way, that show shows the power of Netflix, right?
[02:21:18.720 --> 02:21:20.240]   Because it aired on Lifetime.
[02:21:20.240 --> 02:21:21.240]   Nobody cared.
[02:21:21.240 --> 02:21:22.240]   It was canceled.
[02:21:22.240 --> 02:21:23.240]   And then it hit Netflix.
[02:21:23.240 --> 02:21:24.240]   It did really well.
[02:21:24.240 --> 02:21:26.520]   And Netflix is picking up for the next season, I think, or something like that.
[02:21:26.520 --> 02:21:27.520]   Yeah, exactly.
[02:21:27.520 --> 02:21:29.840]   It was picked up because I didn't even remember being on Lifetime.
[02:21:29.840 --> 02:21:31.040]   I wouldn't have watched it that way.
[02:21:31.040 --> 02:21:32.040]   And I watched on Netflix.
[02:21:32.040 --> 02:21:34.040]   And I was like, I think they were like, because you're like, "Gossip Girl."
[02:21:34.040 --> 02:21:35.040]   And I was like, well, yeah.
[02:21:35.040 --> 02:21:36.040]   But then I--
[02:21:36.040 --> 02:21:38.520]   I don't admit that in public, but OK.
[02:21:38.520 --> 02:21:40.680]   I mean, Gossip Girl was good.
[02:21:40.680 --> 02:21:45.080]   I actually-- I once had a DM conversation with John Leger, the T-Mobile CEO, about our
[02:21:45.080 --> 02:21:47.080]   mutual affection for Gossip Girl.
[02:21:47.080 --> 02:21:48.080]   Wow.
[02:21:48.080 --> 02:21:49.080]   It's a good show.
[02:21:49.080 --> 02:21:50.080]   Wow.
[02:21:50.080 --> 02:21:51.080]   It's a good show.
[02:21:51.080 --> 02:21:52.080]   Wow.
[02:21:52.080 --> 02:21:53.080]   All right.
[02:21:53.080 --> 02:21:54.080]   All right.
[02:21:54.080 --> 02:21:55.080]   I'm writing this down, you.
[02:21:55.080 --> 02:21:56.080]   And I maybe I'll watch Gossip Girl.
[02:21:56.080 --> 02:21:57.080]   I've never seen it, so I remember.
[02:21:57.080 --> 02:21:58.080]   You don't need to watch Gossip Girl, you know.
[02:21:58.080 --> 02:21:59.080]   You might not hold up.
[02:21:59.080 --> 02:22:00.080]   No, you would do more girls, though.
[02:22:00.080 --> 02:22:01.080]   I tried to watch Gilmore Girls.
[02:22:01.080 --> 02:22:02.320]   I tried to watch Gilmore Girls.
[02:22:02.320 --> 02:22:03.520]   I tried to watch Buffy.
[02:22:03.520 --> 02:22:05.520]   I tried to watch all of those chick TV shows.
[02:22:05.520 --> 02:22:06.520]   Buffy's great.
[02:22:06.520 --> 02:22:07.520]   Oh, I like to talk to you.
[02:22:07.520 --> 02:22:10.800]   You would give up about Gossip Girl in about the first 30 seconds.
[02:22:10.800 --> 02:22:13.240]   So don't put yourself through that.
[02:22:13.240 --> 02:22:14.640]   There are better things you can do with your time.
[02:22:14.640 --> 02:22:18.360]   It's a great show, but I know-- I feel like I've gotten to know you enough over the last
[02:22:18.360 --> 02:22:20.360]   decade, Leo, and you don't--
[02:22:20.360 --> 02:22:21.360]   Not for me.
[02:22:21.360 --> 02:22:22.720]   That's not for you, Leo.
[02:22:22.720 --> 02:22:23.720]   That's not for you.
[02:22:23.720 --> 02:22:24.720]   Thank you, Christina.
[02:22:24.720 --> 02:22:26.160]   How about you, Devindra?
[02:22:26.160 --> 02:22:27.160]   Yeah.
[02:22:27.160 --> 02:22:28.160]   I have so many.
[02:22:28.160 --> 02:22:30.960]   Let me split this up by streaming service, I guess.
[02:22:30.960 --> 02:22:31.960]   Wow.
[02:22:31.960 --> 02:22:32.960]   I'm gonna get a fresh page of my name.
[02:22:32.960 --> 02:22:33.960]   I'm gonna get a fresh page of my name.
[02:22:33.960 --> 02:22:34.960]   We'll be right back.
[02:22:34.960 --> 02:22:36.960]   Yes, sir, because I've been there, you need a newsletter.
[02:22:36.960 --> 02:22:37.960]   Yeah.
[02:22:37.960 --> 02:22:38.960]   Yeah.
[02:22:38.960 --> 02:22:39.960]   Well, you know, that's what the podcast--
[02:22:39.960 --> 02:22:40.960]   Slashfilm.
[02:22:40.960 --> 02:22:41.960]   Slashfilm.
[02:22:41.960 --> 02:22:42.960]   Yeah.
[02:22:42.960 --> 02:22:43.960]   Check out the podcast and Slashfilmcast.
[02:22:43.960 --> 02:22:46.800]   But yeah, on Amazon, Marvelous Mrs. Maisel is a perfect comedy.
[02:22:46.800 --> 02:22:47.800]   Yes.
[02:22:47.800 --> 02:22:48.800]   So much fun.
[02:22:48.800 --> 02:22:49.800]   It's so much fun.
[02:22:49.800 --> 02:22:50.800]   It's so--
[02:22:50.800 --> 02:22:51.800]   It started terrible.
[02:22:51.800 --> 02:22:52.800]   And now that they're up in the Berkshires, I'm loving it.
[02:22:52.800 --> 02:22:53.800]   I kind of hated that.
[02:22:53.800 --> 02:22:55.600]   But you know, whatever.
[02:22:55.600 --> 02:22:57.200]   The first season was great.
[02:22:57.200 --> 02:22:58.400]   I really enjoyed it.
[02:22:58.400 --> 02:23:01.840]   The first couple episodes of the next season, they were moving too fast.
[02:23:01.840 --> 02:23:02.840]   They're too crazy.
[02:23:02.840 --> 02:23:06.720]   And it felt like they were trying to be clever but weren't.
[02:23:06.720 --> 02:23:07.720]   Yeah.
[02:23:07.720 --> 02:23:08.720]   OK.
[02:23:08.720 --> 02:23:09.720]   So it's better a lot.
[02:23:09.720 --> 02:23:13.560]   Also on Amazon Patriot, which is a great spy show that nobody has really seen.
[02:23:13.560 --> 02:23:18.960]   But it's like if Wes Anderson was making a James Bond as a series and it's all about
[02:23:18.960 --> 02:23:24.240]   a spy, it's all about the existential doom of this job just bringing you down.
[02:23:24.240 --> 02:23:25.640]   Oh, I definitely want to see that.
[02:23:25.640 --> 02:23:28.800]   It's funny, because it's about this guy who's a spy.
[02:23:28.800 --> 02:23:29.800]   He's tired of being a spy.
[02:23:29.800 --> 02:23:30.800]   He wants to be a folk singer.
[02:23:30.800 --> 02:23:31.800]   Right?
[02:23:31.800 --> 02:23:33.320]   He's got some singing songs.
[02:23:33.320 --> 02:23:34.320]   That's why I like Barry.
[02:23:34.320 --> 02:23:38.040]   He's tired of being a hit man who wants to be a actor.
[02:23:38.040 --> 02:23:39.040]   Barry's very very.
[02:23:39.040 --> 02:23:40.040]   Yeah.
[02:23:40.040 --> 02:23:41.040]   Very Barry.
[02:23:41.040 --> 02:23:42.040]   So great show.
[02:23:42.040 --> 02:23:44.640]   We're checking out on Netflix Sex Education, which is a teen show.
[02:23:44.640 --> 02:23:45.640]   It was about a pair.
[02:23:45.640 --> 02:23:48.080]   I started that because it's Jillian Anderson in it.
[02:23:48.080 --> 02:23:49.080]   Yeah.
[02:23:49.080 --> 02:23:50.080]   So great.
[02:23:50.080 --> 02:23:51.080]   Yeah.
[02:23:51.080 --> 02:23:52.080]   That's about kids teaching.
[02:23:52.080 --> 02:23:55.440]   And so I think you'd appreciate this Georgia because it's about kids helping other kids
[02:23:55.440 --> 02:23:58.000]   with sex problems in high school.
[02:23:58.000 --> 02:23:59.560]   And I think it's really sweet.
[02:23:59.560 --> 02:24:03.540]   It's really like all about like coming to terms like with issues I think a lot of kids
[02:24:03.540 --> 02:24:05.120]   are having and even young adults.
[02:24:05.120 --> 02:24:07.200]   So there's a lot to live there with this show.
[02:24:07.200 --> 02:24:08.200]   And so it's genuinely funny.
[02:24:08.200 --> 02:24:09.200]   I love the whole cast.
[02:24:09.200 --> 02:24:11.920]   Sometimes I stay away from high school shows like that because I figure it's four high
[02:24:11.920 --> 02:24:13.400]   schoolers.
[02:24:13.400 --> 02:24:14.400]   But this is not one of those.
[02:24:14.400 --> 02:24:15.400]   There's so much.
[02:24:15.400 --> 02:24:16.400]   There's so much.
[02:24:16.400 --> 02:24:18.440]   And just briefly like CBS All Access.
[02:24:18.440 --> 02:24:19.640]   Nobody subscribing to this.
[02:24:19.640 --> 02:24:22.560]   But Star Trek Discovery is very very good.
[02:24:22.560 --> 02:24:25.240]   And I think the good fight is probably the best show on TV.
[02:24:25.240 --> 02:24:27.880]   I like the good fight because I like to good wife a lot.
[02:24:27.880 --> 02:24:28.880]   Yeah.
[02:24:28.880 --> 02:24:29.400]   Yeah.
[02:24:29.400 --> 02:24:30.400]   I like both of those.
[02:24:30.400 --> 02:24:34.000]   But I don't want to give CBS like money when I pay for cable.
[02:24:34.000 --> 02:24:35.000]   Me neither.
[02:24:35.000 --> 02:24:36.000]   I don't know.
[02:24:36.000 --> 02:24:38.600]   It's a bad system because like these are great shows.
[02:24:38.600 --> 02:24:39.680]   I want people to watch it.
[02:24:39.680 --> 02:24:43.440]   And then like I'm talking to Star Trek fans and they're refusing to pay a little bit
[02:24:43.440 --> 02:24:46.600]   to money the like two coffees a month to watch their new Star Trek.
[02:24:46.600 --> 02:24:48.400]   I don't if I can't sell it to them.
[02:24:48.400 --> 02:24:50.680]   I don't know how CBS is selling it to the rest of the country.
[02:24:50.680 --> 02:24:51.680]   Yeah.
[02:24:51.680 --> 02:24:55.560]   So the chat remains a recommendation and they've recommended this several times we have got
[02:24:55.560 --> 02:24:56.560]   to watch it.
[02:24:56.560 --> 02:24:59.360]   It's on the National Geographic channel but you can also watch it on their website.
[02:24:59.360 --> 02:25:01.760]   The Valley of the boom.
[02:25:01.760 --> 02:25:05.480]   It's the lives of tech world figures including their friendships, rivalries, victories and
[02:25:05.480 --> 02:25:06.720]   failures.
[02:25:06.720 --> 02:25:12.680]   It's very geeky looking when you have when you have show titles like seg fault, agile
[02:25:12.680 --> 02:25:18.440]   method and has Bradley with for as James Park sale which is kind of the.
[02:25:18.440 --> 02:25:21.120]   Oh, so this is the Netscape story.
[02:25:21.120 --> 02:25:22.120]   It's Netscape.
[02:25:22.120 --> 02:25:25.960]   It is what was the there was a video streaming company and something else in the globe.
[02:25:25.960 --> 02:25:29.840]   So it's like telling all the stories all at once.
[02:25:29.840 --> 02:25:33.880]   Is it a documentary or is it a drama?
[02:25:33.880 --> 02:25:34.880]   It's like half.
[02:25:34.880 --> 02:25:39.400]   So there's interviews with the actual folks and then there's like dramatized versions of
[02:25:39.400 --> 02:25:40.400]   the events.
[02:25:40.400 --> 02:25:41.400]   Interesting.
[02:25:41.400 --> 02:25:42.400]   Yeah.
[02:25:42.400 --> 02:25:46.040]   Because I see Mark Cuban in here and obviously broadcast.com is a streaming company that's
[02:25:46.040 --> 02:25:47.040]   in here.
[02:25:47.040 --> 02:25:48.040]   Yeah.
[02:25:48.040 --> 02:25:49.040]   But it's a nice show.
[02:25:49.040 --> 02:25:50.040]   It goes all over the place.
[02:25:50.040 --> 02:25:51.040]   Nice.
[02:25:51.040 --> 02:25:52.040]   I wasn't a huge fan.
[02:25:52.040 --> 02:25:54.040]   I would say watch Halton cast fire if you want something.
[02:25:54.040 --> 02:25:55.040]   Yeah, Halton cast fire.
[02:25:55.040 --> 02:25:56.040]   That was so good.
[02:25:56.040 --> 02:25:57.040]   That was a great show.
[02:25:57.040 --> 02:26:01.480]   You know, I started watching that when they had a sex pointless sex scene in the very
[02:26:01.480 --> 02:26:02.480]   first episode.
[02:26:02.480 --> 02:26:03.480]   I said, okay, you lost me.
[02:26:03.480 --> 02:26:04.480]   It gets better.
[02:26:04.480 --> 02:26:05.480]   It gets better.
[02:26:05.480 --> 02:26:06.480]   It does get better.
[02:26:06.480 --> 02:26:08.480]   You have to continue because like that was a really good show.
[02:26:08.480 --> 02:26:10.480]   I have to and they got the 80s stuff.
[02:26:10.480 --> 02:26:11.480]   Get on.
[02:26:11.480 --> 02:26:12.480]   Yeah.
[02:26:12.480 --> 02:26:16.520]   Black sales is also a good show for pirates.
[02:26:16.520 --> 02:26:19.440]   Hence pirates.
[02:26:19.440 --> 02:26:20.440]   Real pirates.
[02:26:20.440 --> 02:26:21.440]   Real pirates.
[02:26:21.440 --> 02:26:22.440]   Real pirates.
[02:26:22.440 --> 02:26:24.240]   I don't know if real pirate.
[02:26:24.240 --> 02:26:28.320]   I don't know if they were real pirates, the people in the show, but they're actors playing
[02:26:28.320 --> 02:26:29.320]   pirates.
[02:26:29.320 --> 02:26:30.320]   But it's not.
[02:26:30.320 --> 02:26:31.320]   It's just fun.
[02:26:31.320 --> 02:26:32.320]   It's just some of them are just brain candy.
[02:26:32.320 --> 02:26:33.320]   Okay.
[02:26:33.320 --> 02:26:34.320]   Yeah.
[02:26:34.320 --> 02:26:36.320]   We mean like pirates of the seas, not like pirates.
[02:26:36.320 --> 02:26:37.320]   Or pirates.
[02:26:37.320 --> 02:26:38.320]   Computer pirates.
[02:26:38.320 --> 02:26:39.320]   Oh, sorry.
[02:26:39.320 --> 02:26:40.320]   Yes.
[02:26:40.320 --> 02:26:41.320]   Sorry.
[02:26:41.320 --> 02:26:42.320]   Yeah.
[02:26:42.320 --> 02:26:43.320]   I know it's actors.
[02:26:43.320 --> 02:26:44.320]   Yeah.
[02:26:44.320 --> 02:26:46.320]   I don't think there's any real pirates anymore.
[02:26:46.320 --> 02:26:47.320]   Well, I think there are.
[02:26:47.320 --> 02:26:48.320]   There are.
[02:26:48.320 --> 02:26:49.320]   There are still real pirates.
[02:26:49.320 --> 02:26:50.320]   Yeah.
[02:26:50.320 --> 02:26:51.320]   Yeah.
[02:26:51.320 --> 02:26:52.800]   You remember that issue in Africa that happened?
[02:26:52.800 --> 02:26:53.800]   Yeah.
[02:26:53.800 --> 02:26:54.800]   There's real pirates.
[02:26:54.800 --> 02:26:55.800]   Yeah.
[02:26:55.800 --> 02:26:56.800]   There's real pirates.
[02:26:56.800 --> 02:26:58.960]   And the last kingdom is another candy, brain candy show.
[02:26:58.960 --> 02:26:59.960]   It's just brain candy.
[02:26:59.960 --> 02:27:05.720]   Like it's just, you know, you just watch it for silly time of, you know, what that
[02:27:05.720 --> 02:27:10.760]   one's like medieval, handsome people, you know, fighting for kingdoms.
[02:27:10.760 --> 02:27:12.200]   For time, watch all these shows.
[02:27:12.200 --> 02:27:15.320]   I won't be here for Twit next Sunday.
[02:27:15.320 --> 02:27:18.680]   You know, you watch, you watch them as you go if one takes your fancy.
[02:27:18.680 --> 02:27:22.520]   You have something entertaining to decompress with and sometimes you don't want to have
[02:27:22.520 --> 02:27:23.520]   to think.
[02:27:23.520 --> 02:27:27.800]   Some shows are about thinking, some throws are about just kind of relaxing and not having
[02:27:27.800 --> 02:27:29.880]   to think and enjoying whatever scandals.
[02:27:29.880 --> 02:27:30.880]   Exactly.
[02:27:30.880 --> 02:27:31.880]   We call that Vanderpump.
[02:27:31.880 --> 02:27:32.880]   And it's great.
[02:27:32.880 --> 02:27:36.320]   We call that Vanderpump rules and it's fantastic with you.
[02:27:36.320 --> 02:27:37.320]   I'm totally.
[02:27:37.320 --> 02:27:40.280]   I'm going to add one more because the first season was good.
[02:27:40.280 --> 02:27:44.240]   Second season was terrible, but the third season of True Detective, which is on right
[02:27:44.240 --> 02:27:46.680]   now with Monster Ollie Ollie is amazing.
[02:27:46.680 --> 02:27:47.680]   It's pretty good.
[02:27:47.680 --> 02:27:48.680]   I would agree that.
[02:27:48.680 --> 02:27:49.680]   Yeah, it is.
[02:27:49.680 --> 02:27:50.680]   Pretty good.
[02:27:50.680 --> 02:27:51.680]   Davindra, you don't think it's great.
[02:27:51.680 --> 02:27:52.680]   The next season was better.
[02:27:52.680 --> 02:27:53.680]   Like, Starbucks kind of did that.
[02:27:53.680 --> 02:27:54.680]   I didn't try to catch up with that.
[02:27:54.680 --> 02:27:56.240]   And it's just doing it so differently.
[02:27:56.240 --> 02:27:59.360]   This feels like a remake of season one, but it's still, it's Marshall Ollie.
[02:27:59.360 --> 02:28:00.360]   I love him so much.
[02:28:00.360 --> 02:28:01.520]   So I'll watch him in anything.
[02:28:01.520 --> 02:28:03.480]   He is so good in this.
[02:28:03.480 --> 02:28:05.760]   And the thing I can see, I like about it.
[02:28:05.760 --> 02:28:09.840]   I've seen other shows try to do this where it's three different time periods and they
[02:28:09.840 --> 02:28:15.760]   intermix them and it can either be Westworld where you go, what the hell is going on?
[02:28:15.760 --> 02:28:19.760]   Or they've done this so well and they've aged the actors amazingly.
[02:28:19.760 --> 02:28:21.960]   Do you know if it's back?
[02:28:21.960 --> 02:28:22.960]   Yeah.
[02:28:22.960 --> 02:28:23.960]   Yeah, it's really good.
[02:28:23.960 --> 02:28:25.440]   Anyway, enough TV watching.
[02:28:25.440 --> 02:28:27.360]   You guys are great.
[02:28:27.360 --> 02:28:30.440]   I want to thank everybody for joining us for Twit this week.
[02:28:30.440 --> 02:28:33.280]   And I think we should just bring the same panel back from now on.
[02:28:33.280 --> 02:28:34.440]   No, you've got lots.
[02:28:34.440 --> 02:28:35.440]   Super Bowl.
[02:28:35.440 --> 02:28:36.440]   Yeah.
[02:28:36.440 --> 02:28:37.440]   Every time there's a Super Bowl.
[02:28:37.440 --> 02:28:39.080]   Yeah, you know next year they'll be calling.
[02:28:39.080 --> 02:28:41.120]   Yeah, well, sooner than that, I hope.
[02:28:41.120 --> 02:28:42.840]   Thank you, Georgia Dow.
[02:28:42.840 --> 02:28:44.380]   Anxiety dash videos.
[02:28:44.380 --> 02:28:50.500]   If you want videos, not just about anxiety, but sleep, mastering, conflict resolution,
[02:28:50.500 --> 02:28:53.460]   emotional intelligence, great stuff.
[02:28:53.460 --> 02:28:54.460]   I have the whole set.
[02:28:54.460 --> 02:28:55.460]   I really love it.
[02:28:55.460 --> 02:28:56.980]   Georgia, actually you've added more.
[02:28:56.980 --> 02:28:58.420]   I do not have the whole set.
[02:28:58.420 --> 02:28:59.420]   There's more.
[02:28:59.420 --> 02:29:00.420]   We'll be sending them.
[02:29:00.420 --> 02:29:01.420]   Don't worry.
[02:29:01.420 --> 02:29:02.420]   You'll get the others.
[02:29:02.420 --> 02:29:03.420]   No, no, it's okay.
[02:29:03.420 --> 02:29:04.420]   It's okay.
[02:29:04.420 --> 02:29:10.780]   Georgia Dow is a family therapist and her partner, Sandra Reich, do these.
[02:29:10.780 --> 02:29:14.340]   She's the clinical director of the Montreal Center for Anxiety and Depression.
[02:29:14.340 --> 02:29:15.980]   And it really is well done.
[02:29:15.980 --> 02:29:20.300]   If you love Georgia, you will love Sandra and Georgia on anxiety videos.
[02:29:20.300 --> 02:29:21.660]   Thank you, Georgia, for being here.
[02:29:21.660 --> 02:29:22.660]   I'm more.com.
[02:29:22.660 --> 02:29:26.980]   We didn't talk that much about Apple, but we will next time.
[02:29:26.980 --> 02:29:29.180]   Christina, it's so wonderful to see you.
[02:29:29.180 --> 02:29:30.540]   I'm glad things are going well.
[02:29:30.540 --> 02:29:32.420]   You haven't been hit by a bus lately.
[02:29:32.420 --> 02:29:33.860]   No, it's been a year.
[02:29:33.860 --> 02:29:36.180]   It's been one year since I've been hit by a car.
[02:29:36.180 --> 02:29:40.900]   So, yes, everything's going well.
[02:29:40.900 --> 02:29:43.260]   She lives in New York for her whole life.
[02:29:43.260 --> 02:29:45.660]   Whatever gets hit by anything goes to Seattle almost instantly.
[02:29:45.660 --> 02:29:46.860]   A bus runs around her.
[02:29:46.860 --> 02:29:47.860]   Almost instantly.
[02:29:47.860 --> 02:29:48.860]   Almost instantly.
[02:29:48.860 --> 02:29:49.860]   I'm glad.
[02:29:49.860 --> 02:29:50.860]   So glad that you're getting that.
[02:29:50.860 --> 02:29:51.860]   I have a lot of fun at Microsoft.
[02:29:51.860 --> 02:29:53.660]   That was a big jump from Mashable to Microsoft.
[02:29:53.660 --> 02:29:54.660]   I'm glad it was.
[02:29:54.660 --> 02:29:55.660]   It was.
[02:29:55.660 --> 02:29:56.660]   It was.
[02:29:56.660 --> 02:29:57.660]   It's been going really well.
[02:29:57.660 --> 02:29:58.660]   And we're making videos.
[02:29:58.660 --> 02:29:59.660]   We're doing good content.
[02:29:59.660 --> 02:30:00.660]   And I'm having a great time.
[02:30:00.660 --> 02:30:04.940]   And it's amazing because they also let me continue to do things like this and be on a
[02:30:04.940 --> 02:30:06.820]   talk with brilliant people like you.
[02:30:06.820 --> 02:30:09.740]   Why attitude towards Microsoft is completely 180 degrees.
[02:30:09.740 --> 02:30:10.740]   Oh, yeah.
[02:30:10.740 --> 02:30:12.260]   I'm really impressed with the new Microsoft.
[02:30:12.260 --> 02:30:14.260]   It's not just a Nadella is brilliant.
[02:30:14.260 --> 02:30:15.980]   And I'm very impressed with the new Microsoft.
[02:30:15.980 --> 02:30:16.980]   I really am.
[02:30:16.980 --> 02:30:20.500]   Really, you picked a good company.
[02:30:20.500 --> 02:30:22.460]   Davender Harder War, he works for AOL.
[02:30:22.460 --> 02:30:23.460]   No, I'm sorry.
[02:30:23.460 --> 02:30:24.460]   He works for Verizon.
[02:30:24.460 --> 02:30:25.460]   No, I'm sorry.
[02:30:25.460 --> 02:30:26.460]   I don't know who the hell he works for.
[02:30:26.460 --> 02:30:27.460]   It used to be Oth.
[02:30:27.460 --> 02:30:28.460]   It was AOL.
[02:30:28.460 --> 02:30:29.460]   And now it's Verizon Media.
[02:30:29.460 --> 02:30:31.260]   You're all part of that.
[02:30:31.260 --> 02:30:32.260]   Yeah.
[02:30:32.260 --> 02:30:33.260]   I'm glad they got rid of the Oth.
[02:30:33.260 --> 02:30:37.220]   But everybody knows it better as in gadget, which is the number one gadget blog in the
[02:30:37.220 --> 02:30:38.220]   world.
[02:30:38.220 --> 02:30:39.220]   And Davender is senior editor there.
[02:30:39.220 --> 02:30:40.860]   It does a great job.
[02:30:40.860 --> 02:30:42.660]   How long have you been there?
[02:30:42.660 --> 02:30:45.060]   It's going to be four years this year, I think.
[02:30:45.060 --> 02:30:46.060]   Yeah.
[02:30:46.060 --> 02:30:48.700]   It feels like an eternity in web years.
[02:30:48.700 --> 02:30:49.700]   It's so much.
[02:30:49.700 --> 02:30:51.260]   But I've had a ton of fun here.
[02:30:51.260 --> 02:30:55.580]   It was great helping and gadget grow and stay awesome.
[02:30:55.580 --> 02:30:58.660]   But yeah, I also do the slash film cast and slash film.com.
[02:30:58.660 --> 02:31:03.980]   And I started a new podcast, a tech Q&A podcast at no more tech.net.
[02:31:03.980 --> 02:31:04.980]   Let's know with the K.
[02:31:04.980 --> 02:31:06.420]   Oh, I like that.
[02:31:06.420 --> 02:31:07.420]   I haven't heard about that.
[02:31:07.420 --> 02:31:08.420]   No more tech.
[02:31:08.420 --> 02:31:09.420]   Yeah.
[02:31:09.420 --> 02:31:10.420]   K.com.
[02:31:10.420 --> 02:31:12.420]   Yes, or .net.
[02:31:12.420 --> 02:31:13.420]   Not net.
[02:31:13.420 --> 02:31:14.420]   No more tech.net.
[02:31:14.420 --> 02:31:15.420]   Okay.
[02:31:15.420 --> 02:31:16.420]   Cool.
[02:31:16.420 --> 02:31:18.500]   And how do you get questions?
[02:31:18.500 --> 02:31:20.460]   I feel them through Twitter.
[02:31:20.460 --> 02:31:23.620]   There's just not loading.
[02:31:23.620 --> 02:31:27.420]   I feel them through Twitter and there's a Google sheet as well.
[02:31:27.420 --> 02:31:28.420]   Yeah.
[02:31:28.420 --> 02:31:29.860]   It's not loading because I put calm.
[02:31:29.860 --> 02:31:30.860]   It's net.
[02:31:30.860 --> 02:31:31.860]   Ha.
[02:31:31.860 --> 02:31:32.860]   That dummy.
[02:31:32.860 --> 02:31:34.660]   And of course, the most recent episode.
[02:31:34.660 --> 02:31:35.660]   CES.
[02:31:35.660 --> 02:31:36.660]   CES.
[02:31:36.660 --> 02:31:37.660]   Just dealing with all that.
[02:31:37.660 --> 02:31:41.740]   But it's actually, I'm taking a cue from Uleo just like, I love sitting back and answering
[02:31:41.740 --> 02:31:42.740]   questions from folks.
[02:31:42.740 --> 02:31:43.740]   I do too.
[02:31:43.740 --> 02:31:44.740]   It's really fun.
[02:31:44.740 --> 02:31:46.420]   It's just like a nice laid back thing.
[02:31:46.420 --> 02:31:48.060]   So this is what I do for fun, I guess.
[02:31:48.060 --> 02:31:49.060]   Yeah.
[02:31:49.060 --> 02:31:50.060]   And TV.
[02:31:50.060 --> 02:31:51.060]   Yeah.
[02:31:51.060 --> 02:31:54.220]   I love doing the radio show for that reason because it's nice to talk to normal people
[02:31:54.220 --> 02:31:55.220]   for a change.
[02:31:55.220 --> 02:31:56.900]   So the y'all, geeks.
[02:31:56.900 --> 02:32:02.060]   Thank you so much, Georgia, Christina, Devindra, you guys are great.
[02:32:02.060 --> 02:32:03.860]   Thank you all for being here.
[02:32:03.860 --> 02:32:04.860]   We do twit normally.
[02:32:04.860 --> 02:32:08.900]   We did a little early this week because the Super Bowl will be back at our normal time.
[02:32:08.900 --> 02:32:09.900]   3 p.m.
[02:32:09.900 --> 02:32:10.900]   Pacific, 6 p.m.
[02:32:10.900 --> 02:32:11.900]   Eastern.
[02:32:11.900 --> 02:32:14.340]   That's 2300 UTC on Sunday evening.
[02:32:14.340 --> 02:32:15.340]   Come by and watch.
[02:32:15.340 --> 02:32:16.420]   You can see it at youtube.com.
[02:32:16.420 --> 02:32:17.420]   Sorry.
[02:32:17.420 --> 02:32:24.820]   Twit.tv/live, which will give you access to the YouTube, Twitch, mixer, you stream,
[02:32:24.820 --> 02:32:26.140]   whatever stream you want.
[02:32:26.140 --> 02:32:29.300]   There's also two audio streams there so you can listen or watch.
[02:32:29.300 --> 02:32:35.380]   If you're doing it live, go in the chat room because they're doing it live to irc.twit.tv.
[02:32:35.380 --> 02:32:41.660]   But as always, on demand versions of everything we do available on the website, twit.tv or
[02:32:41.660 --> 02:32:44.020]   in your favorite podcast application.
[02:32:44.020 --> 02:32:50.060]   It's okay if you spotify slackers, stitcher, overcast, pocket casts, iTunes, whatever you
[02:32:50.060 --> 02:32:52.020]   use, just subscribe that way you'll get it.
[02:32:52.020 --> 02:32:54.900]   The minute it's available every Sunday night.
[02:32:54.900 --> 02:32:55.900]   Thanks for being here.
[02:32:55.900 --> 02:32:56.900]   We'll see you next time.
[02:32:56.900 --> 02:32:58.500]   Another twit is in the can.
[02:32:58.500 --> 02:33:00.820]   I didn't ask you your predictions.
[02:33:00.820 --> 02:33:01.980]   Patriots for Rams.
[02:33:01.980 --> 02:33:03.980]   Do it in the Twitter.
[02:33:03.980 --> 02:33:05.300]   No, but in the Twitter.
[02:33:05.300 --> 02:33:06.300]   Nobody.
[02:33:06.300 --> 02:33:07.300]   No idea.
[02:33:07.300 --> 02:33:08.300]   Ha!

