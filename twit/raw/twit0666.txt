;FFMETADATA1
title=Leave John Legend, Um, Alone!
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=666
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:01.400]   It's time for Twit this weekend.
[00:00:01.400 --> 00:00:03.160]   Tech Florence Aion from all about.
[00:00:03.160 --> 00:00:03.880]   Android is here.
[00:00:03.880 --> 00:00:05.760]   Harry McCracken from Fast Company.
[00:00:05.760 --> 00:00:08.560]   Sam Aboule-Somid is here also from Navigate Research.
[00:00:08.560 --> 00:00:09.760]   He's our car expert.
[00:00:09.760 --> 00:00:14.080]   And there is a lot to talk about how Trump's tweet could save
[00:00:14.080 --> 00:00:18.040]   ZTE, driver assist in the Cadillac, Microsoft
[00:00:18.040 --> 00:00:21.080]   timeline, and Google's uncanny duplex.
[00:00:21.080 --> 00:00:22.880]   It's all coming up next on Twit.
[00:00:22.880 --> 00:00:27.800]   Netcast you love.
[00:00:27.800 --> 00:00:29.160]   From people you trust.
[00:00:29.160 --> 00:00:35.320]   This is Twit.
[00:00:35.320 --> 00:00:38.640]   Bandwidth for this weekend tech is provided by CashFly
[00:00:38.640 --> 00:00:41.280]   at CACHEFLY.com.
[00:00:41.280 --> 00:00:51.840]   This is Twit this weekend tech, episode 666, recorded Sunday,
[00:00:51.840 --> 00:00:54.320]   May 13, 2018.
[00:00:54.320 --> 00:00:58.800]   Leave John Legend alone.
[00:00:58.800 --> 00:01:00.240]   This weekend tech is brought to you
[00:01:00.240 --> 00:01:02.240]   by the Ring Video Doorbell.
[00:01:02.240 --> 00:01:05.240]   Stop crying before it happens and help make your neighborhood
[00:01:05.240 --> 00:01:05.760]   safer.
[00:01:05.760 --> 00:01:10.960]   With Ring, go to Ring.com/Twit and get up to $150 off a Ring
[00:01:10.960 --> 00:01:12.640]   of Security Kit.
[00:01:12.640 --> 00:01:15.840]   And by Blue Apron, the number one fresh ingredient and recipe
[00:01:15.840 --> 00:01:17.720]   delivery service in the country.
[00:01:17.720 --> 00:01:20.520]   Check out this week's menu and get three meals free with
[00:01:20.520 --> 00:01:23.640]   free shipping at Blue Apron.com/Twit.
[00:01:23.640 --> 00:01:25.320]   And by LegalZoom.
[00:01:25.320 --> 00:01:28.120]   Get your dream business up and running or take control of
[00:01:28.120 --> 00:01:30.360]   your family's future with LegalZoom.
[00:01:30.360 --> 00:01:33.520]   For special savings, visit LegalZoom.com and enter
[00:01:33.520 --> 00:01:35.080]   Twit at check.
[00:01:35.080 --> 00:01:37.000]   And by Texture.
[00:01:37.000 --> 00:01:40.040]   Access the world's most popular magazines anytime,
[00:01:40.040 --> 00:01:43.160]   anywhere, using your smartphone or tablet.
[00:01:43.160 --> 00:01:49.040]   Go to texture.com/Twit to start using your free trial today.
[00:01:49.040 --> 00:01:51.440]   This is Twit this weekend tech.
[00:01:51.440 --> 00:01:54.520]   We'll show we cover the week's tech news and all in studio
[00:01:54.520 --> 00:01:55.160]   panel today.
[00:01:55.160 --> 00:01:57.720]   It's a good thing because we have a massive studio audience
[00:01:57.720 --> 00:02:00.280]   from all over the world, including Australia and New
[00:02:00.280 --> 00:02:06.440]   Zealand and Sweden and Marie Domingo.
[00:02:06.440 --> 00:02:08.200]   Everybody's here.
[00:02:08.200 --> 00:02:08.600]   Oh, I'm sorry.
[00:02:08.600 --> 00:02:10.960]   That's not the same as Santo Domingo.
[00:02:10.960 --> 00:02:11.880]   Harry McCracken is here.
[00:02:11.880 --> 00:02:12.960]   He's the technologizer.
[00:02:12.960 --> 00:02:14.040]   Fastcompany.com.
[00:02:14.040 --> 00:02:14.880]   Great to see you, Harry.
[00:02:14.880 --> 00:02:16.240]   Great to be here.
[00:02:16.240 --> 00:02:18.760]   Are you guys going to hire Andy and Oco or what?
[00:02:18.760 --> 00:02:20.040]   He's doing some stuff for us.
[00:02:20.040 --> 00:02:20.800]   I know.
[00:02:20.800 --> 00:02:21.480]   I love it.
[00:02:21.480 --> 00:02:23.640]   I expect to go back and edit a piece of his tonight.
[00:02:23.640 --> 00:02:24.840]   Oh, you're his editor.
[00:02:24.840 --> 00:02:25.920]   Oh, be nice.
[00:02:25.920 --> 00:02:27.480]   Be kind.
[00:02:27.480 --> 00:02:28.720]   He's a very clean writer.
[00:02:28.720 --> 00:02:29.720]   Oh, I bet he is.
[00:02:29.720 --> 00:02:30.480]   I bet he's perfect.
[00:02:30.480 --> 00:02:32.760]   He's Andy, right from the start, which is not true of everybody.
[00:02:32.760 --> 00:02:33.960]   He has a voice, doesn't he?
[00:02:33.960 --> 00:02:36.120]   I can read his stuff and I hear Andy in my head.
[00:02:36.120 --> 00:02:38.360]   I work with people where you have to insert their voice
[00:02:38.360 --> 00:02:39.720]   into the story where they leave it out.
[00:02:39.720 --> 00:02:41.320]   How do you do that?
[00:02:41.320 --> 00:02:42.280]   You have to channel them.
[00:02:42.280 --> 00:02:43.760]   Use Google Prozzity.
[00:02:43.760 --> 00:02:44.720]   I used to use--
[00:02:44.720 --> 00:02:45.240]   Doplex.
[00:02:45.240 --> 00:02:46.040]   Doplex.
[00:02:46.040 --> 00:02:46.040]   Doplex.
[00:02:46.040 --> 00:02:47.040]   Doplex.
[00:02:47.040 --> 00:02:48.160]   The writer at PC World who shall remain name less
[00:02:48.160 --> 00:02:48.960]   who had a great voice.
[00:02:48.960 --> 00:02:50.800]   But sometimes he forgot to put in his own voice.
[00:02:50.800 --> 00:02:51.760]   Oh, interesting.
[00:02:51.760 --> 00:02:53.880]   You had to ask him to put his voice in or put it in for him.
[00:02:53.880 --> 00:02:55.560]   But you knew how to do that.
[00:02:55.560 --> 00:02:57.160]   After a while, if you work with somebody like Andy--
[00:02:57.160 --> 00:02:58.080]   You know how to do somebody's voice.
[00:02:58.080 --> 00:02:59.200]   Andy, so far you don't need to do that.
[00:02:59.200 --> 00:02:59.680]   Yeah.
[00:02:59.680 --> 00:03:00.960]   Yeah, it's Devorica's really good.
[00:03:00.960 --> 00:03:01.960]   You read his stuff.
[00:03:01.960 --> 00:03:03.160]   You hear John.
[00:03:03.160 --> 00:03:04.560]   Like, egh.
[00:03:04.560 --> 00:03:06.160]   You hear it.
[00:03:06.160 --> 00:03:07.640]   Florence Iyann is also here.
[00:03:07.640 --> 00:03:08.840]   Always a pleasure to have you.
[00:03:08.840 --> 00:03:11.080]   Florence, of course, the host of all about Android,
[00:03:11.080 --> 00:03:13.200]   oh, that flow on the Twitter.
[00:03:13.200 --> 00:03:14.760]   And she writes a lot of places.
[00:03:14.760 --> 00:03:15.480]   I do.
[00:03:15.480 --> 00:03:18.680]   I wrote a piece for End Gadget about material
[00:03:18.680 --> 00:03:21.480]   that I was just seeming recently this week.
[00:03:21.480 --> 00:03:22.680]   And you had a big birthday.
[00:03:22.680 --> 00:03:23.360]   I did.
[00:03:23.360 --> 00:03:23.720]   Happy birthday.
[00:03:23.720 --> 00:03:24.080]   I did.
[00:03:24.080 --> 00:03:24.920]   Thank you.
[00:03:24.920 --> 00:03:25.920]   But you're not a mother.
[00:03:25.920 --> 00:03:26.920]   I just want everybody to know.
[00:03:26.920 --> 00:03:27.360]   I'm not.
[00:03:27.360 --> 00:03:27.880]   I'm not.
[00:03:27.880 --> 00:03:30.400]   We just say happy mother to all of our mothers.
[00:03:30.400 --> 00:03:31.800]   I do have a lot of tech children at home.
[00:03:31.800 --> 00:03:33.560]   Do you want to say hi to your mom?
[00:03:33.560 --> 00:03:34.160]   Sure.
[00:03:34.160 --> 00:03:35.000]   Hi, mom.
[00:03:35.000 --> 00:03:36.560]   Do you want to say hi to your mom?
[00:03:36.560 --> 00:03:37.720]   She might actually be watching.
[00:03:37.720 --> 00:03:38.280]   Hey, mom.
[00:03:38.280 --> 00:03:41.400]   Hey, happy Mother's Day, Harry McCracken's mother.
[00:03:41.400 --> 00:03:42.840]   Mrs. McCracken?
[00:03:42.840 --> 00:03:45.840]   We called it with her a happy Mother's Day on the way up here.
[00:03:45.840 --> 00:03:46.400]   I have a call on my mom.
[00:03:46.400 --> 00:03:47.520]   And she said she'd be watching.
[00:03:47.520 --> 00:03:48.360]   It's a bit better be.
[00:03:48.360 --> 00:03:49.280]   Nice.
[00:03:49.280 --> 00:03:50.160]   Where's she live?
[00:03:50.160 --> 00:03:51.440]   She lives in Newton, Massachusetts.
[00:03:51.440 --> 00:03:52.120]   Newton.
[00:03:52.120 --> 00:03:55.200]   Love Newton, which looks like Newtown.
[00:03:55.200 --> 00:03:56.280]   No?
[00:03:56.280 --> 00:03:57.280]   No, it's spelled Newton.
[00:03:57.280 --> 00:03:58.040]   Like Fig Newton.
[00:03:58.040 --> 00:03:59.520]   Like Fig Newton's are named after Newton.
[00:03:59.520 --> 00:03:59.960]   Like Sura-- what?
[00:03:59.960 --> 00:04:00.960]   No.
[00:04:00.960 --> 00:04:01.760]   That's where they're from.
[00:04:01.760 --> 00:04:02.760]   Already we've learned something.
[00:04:02.760 --> 00:04:03.520]   Over the today we learned.
[00:04:03.520 --> 00:04:05.640]   At least we learned something.
[00:04:05.640 --> 00:04:08.000]   Were they first made in Newton, Massachusetts?
[00:04:08.000 --> 00:04:10.280]   We've seen her in the rest story about Nabisco naming
[00:04:10.280 --> 00:04:11.520]   cookies after different towns.
[00:04:11.520 --> 00:04:12.800]   Oh.
[00:04:12.800 --> 00:04:15.360]   See, I always thought it was named after Sura Isaac Newton.
[00:04:15.360 --> 00:04:16.960]   Because that's very famous, right?
[00:04:16.960 --> 00:04:18.240]   The Fig hit him--
[00:04:18.240 --> 00:04:19.200]   That was an apple.
[00:04:19.200 --> 00:04:20.760]   Oh, that's Sam.
[00:04:20.760 --> 00:04:22.440]   Now I'm going to try to do your name right.
[00:04:22.440 --> 00:04:24.040]   Abuela Simid.
[00:04:24.040 --> 00:04:24.880]   Very good dealio.
[00:04:24.880 --> 00:04:28.760]   Am I-- is my-- are my emphasis in the--
[00:04:28.760 --> 00:04:29.600]   Absolutely.
[00:04:29.600 --> 00:04:32.240]   You know, I have heard my name mangled so many ways
[00:04:32.240 --> 00:04:34.440]   over the course of my lifetime by so many teachers
[00:04:34.440 --> 00:04:36.840]   and other people that I've long since carrying.
[00:04:36.840 --> 00:04:37.520]   Well, I realize--
[00:04:37.520 --> 00:04:38.440]   As long as you get it close--
[00:04:38.440 --> 00:04:39.360]   It's Arabic, right?
[00:04:39.360 --> 00:04:41.200]   I realized if you divide it up properly,
[00:04:41.200 --> 00:04:44.000]   ABU Abu, L, Simeed.
[00:04:44.000 --> 00:04:44.640]   Yes.
[00:04:44.640 --> 00:04:46.880]   So I suddenly realized, oh, I can pronounce that.
[00:04:46.880 --> 00:04:47.880]   Yeah.
[00:04:47.880 --> 00:04:48.720]   People look as they go.
[00:04:48.720 --> 00:04:50.360]   Just break it up into three syllables.
[00:04:50.360 --> 00:04:52.160]   It's all about the syllables.
[00:04:52.160 --> 00:04:52.720]   Yeah.
[00:04:52.720 --> 00:04:57.240]   So breaking news this morning Donald Trump--
[00:04:57.240 --> 00:04:58.680]   this is not political, folks--
[00:04:58.680 --> 00:05:01.960]   tweeted, President Xi of China and I
[00:05:01.960 --> 00:05:05.040]   are working together to give massive Chinese phone
[00:05:05.040 --> 00:05:09.120]   companies ZTE a way to get back into business fast.
[00:05:09.120 --> 00:05:12.200]   Too many jobs in China lost.
[00:05:12.200 --> 00:05:14.440]   Make China great.
[00:05:14.440 --> 00:05:16.360]   Commerce Department has been instructed to get it done.
[00:05:16.360 --> 00:05:17.960]   Now, if you don't know the backster
[00:05:17.960 --> 00:05:21.840]   and it's the Commerce Department, the United States Commerce
[00:05:21.840 --> 00:05:24.440]   Department was the one that practically put ZTE out
[00:05:24.440 --> 00:05:27.120]   of business saying American companies can no longer buy
[00:05:27.120 --> 00:05:30.160]   or sell ZTE products.
[00:05:30.160 --> 00:05:32.720]   And in fact, it looks like ZTE or looked like ZTE
[00:05:32.720 --> 00:05:33.600]   might go out of business.
[00:05:33.600 --> 00:05:34.480]   I like ZTEs.
[00:05:34.480 --> 00:05:35.320]   The axon's great.
[00:05:35.320 --> 00:05:35.920]   Apparently they--
[00:05:35.920 --> 00:05:39.440]   And maybe Google can't supply Android to ZTE.
[00:05:39.440 --> 00:05:40.120]   Exactly.
[00:05:40.120 --> 00:05:41.040]   Yeah.
[00:05:41.040 --> 00:05:43.800]   Well, it probably-- you know, the President Xi
[00:05:43.800 --> 00:05:48.000]   probably told him that unless you back off on ZTE,
[00:05:48.000 --> 00:05:51.080]   then you won't get any more iPhones shipped out of China.
[00:05:51.080 --> 00:05:52.280]   Could be.
[00:05:52.280 --> 00:05:52.960]   Could be.
[00:05:52.960 --> 00:05:55.640]   Can you imagine the uproar if suddenly iPhones stopped
[00:05:55.640 --> 00:05:56.440]   arriving from China?
[00:05:56.440 --> 00:05:57.440]   Yeah.
[00:05:57.440 --> 00:05:58.080]   Oh, wow.
[00:05:58.080 --> 00:05:58.920]   That would be--
[00:05:58.920 --> 00:05:59.400]   That would--
[00:05:59.400 --> 00:06:00.000]   They can't do that.
[00:06:00.000 --> 00:06:01.640]   Silicon Valley would just fall apart.
[00:06:01.640 --> 00:06:02.480]   He can't do that.
[00:06:02.480 --> 00:06:02.880]   It would just--
[00:06:02.880 --> 00:06:03.400]   He can't do that.
[00:06:03.400 --> 00:06:04.280]   --come on underneath.
[00:06:04.280 --> 00:06:05.440]   It would just fall into a sinkhole.
[00:06:05.440 --> 00:06:07.360]   We'd have to all use pixels.
[00:06:07.360 --> 00:06:09.320]   Which would be great for me.
[00:06:09.320 --> 00:06:09.880]   Wait a minute.
[00:06:09.880 --> 00:06:10.760]   Oh, this is good.
[00:06:10.760 --> 00:06:11.680]   It all looks like your windows.
[00:06:11.680 --> 00:06:13.000]   Pixel, pixel, pixel.
[00:06:13.000 --> 00:06:14.280]   Yeah.
[00:06:14.280 --> 00:06:14.880]   iPhone.
[00:06:14.880 --> 00:06:15.360]   iPhone.
[00:06:15.360 --> 00:06:16.640]   Yeah, I knew it.
[00:06:16.640 --> 00:06:17.200]   But that's good.
[00:06:17.200 --> 00:06:19.640]   I have a pixel and I need to install Android P on it.
[00:06:19.640 --> 00:06:22.800]   Android P-- I was just talking with Flo before the show,
[00:06:22.800 --> 00:06:25.360]   because unaccountably, the host of all about Android,
[00:06:25.360 --> 00:06:27.320]   you're still using Android Oreo.
[00:06:27.320 --> 00:06:30.520]   To be fair, I test a lot of IoT, and I need my apps to be stable.
[00:06:30.520 --> 00:06:31.600]   No, that's completely fair.
[00:06:31.600 --> 00:06:32.680]   I think it's completely reasonable.
[00:06:32.680 --> 00:06:35.920]   But I do need to get on the Android P train.
[00:06:35.920 --> 00:06:37.720]   We should mention the P train.
[00:06:37.720 --> 00:06:37.720]   Yes.
[00:06:37.720 --> 00:06:40.680]   This is why they've got to quickly get a dessert associated
[00:06:40.680 --> 00:06:41.200]   with this.
[00:06:41.200 --> 00:06:41.720]   I agree.
[00:06:41.720 --> 00:06:42.960]   You definitely don't want to leave that one.
[00:06:42.960 --> 00:06:46.120]   You definitely need to name it Google.
[00:06:46.120 --> 00:06:47.200]   You got to stop saying Android P.
[00:06:47.200 --> 00:06:48.320]   Just say Android Pied.
[00:06:48.320 --> 00:06:48.820]   Pied.
[00:06:48.820 --> 00:06:49.760]   Just put that in the ether.
[00:06:49.760 --> 00:06:50.840]   What about the ether?
[00:06:50.840 --> 00:06:51.960]   Maybe they'll actually go in it.
[00:06:51.960 --> 00:06:52.960]   Android Pied.
[00:06:52.960 --> 00:06:53.560]   I'm going to call it.
[00:06:53.560 --> 00:06:56.360]   For the next year, you guys on all about Android,
[00:06:56.360 --> 00:06:58.400]   we'll be able to taste a different kind of pie every week.
[00:06:58.400 --> 00:06:59.360]   We begged.
[00:06:59.360 --> 00:07:01.120]   We asked if they could please name it pie,
[00:07:01.120 --> 00:07:03.240]   so that Jason and I get to eat pie every week.
[00:07:03.240 --> 00:07:06.240]   Because you're getting really sick of the weird flavored Oreos.
[00:07:06.240 --> 00:07:07.040]   Holy cow.
[00:07:07.040 --> 00:07:09.160]   I get sick of it just listening to some of those--
[00:07:09.160 --> 00:07:10.920]   Every Wednesday morning I come in,
[00:07:10.920 --> 00:07:15.920]   and there's a half-eaten bag of dog biscuit flavored Oreos
[00:07:15.920 --> 00:07:18.560]   or peppermint schnapps Oreos.
[00:07:18.560 --> 00:07:19.960]   I wish.
[00:07:19.960 --> 00:07:21.360]   I wish.
[00:07:21.360 --> 00:07:22.880]   This is how I saw a little rum Oreo.
[00:07:22.880 --> 00:07:25.280]   Is there one that was good?
[00:07:25.280 --> 00:07:25.880]   Yes.
[00:07:25.880 --> 00:07:27.360]   The chocolate is on it.
[00:07:27.360 --> 00:07:27.880]   Yeah.
[00:07:27.880 --> 00:07:29.200]   The chocolate-- there was a chocolate even
[00:07:29.200 --> 00:07:30.120]   as that was really good.
[00:07:30.120 --> 00:07:30.640]   OK.
[00:07:30.640 --> 00:07:31.600]   Nutella-ish.
[00:07:31.600 --> 00:07:33.240]   There was one we ate that was expired.
[00:07:33.240 --> 00:07:34.680]   Two years passed expired.
[00:07:34.680 --> 00:07:36.400]   But there was one just a week or two ago
[00:07:36.400 --> 00:07:38.280]   that you guys gave nine or nine and a half to.
[00:07:38.280 --> 00:07:39.400]   I don't even remember.
[00:07:39.400 --> 00:07:41.160]   It was something chocolate.
[00:07:41.160 --> 00:07:42.200]   There's--
[00:07:42.200 --> 00:07:43.600]   I don't think it was the nutella.
[00:07:43.600 --> 00:07:46.440]   Oh, it was the hostess cup.
[00:07:46.440 --> 00:07:47.360]   That's it, the cupcake.
[00:07:47.360 --> 00:07:48.520]   That was a two-year-old one.
[00:07:48.520 --> 00:07:49.040]   Was it?
[00:07:49.040 --> 00:07:49.520]   Yeah, two years old?
[00:07:49.520 --> 00:07:50.920]   Yeah, but it was still good.
[00:07:50.920 --> 00:07:53.920]   There is an Oreo DQ Blizzard cream.
[00:07:53.920 --> 00:07:54.680]   That sounds amazing.
[00:07:54.680 --> 00:07:56.680]   Is that an Oreo or is it a blizzard?
[00:07:56.680 --> 00:07:58.160]   It's an Oreo.
[00:07:58.160 --> 00:08:01.320]   It's a limited edition Oreo which was put out eight years ago
[00:08:01.320 --> 00:08:02.480]   if you still had it.
[00:08:02.480 --> 00:08:04.360]   I'm sure there's probably still a couple of packages
[00:08:04.360 --> 00:08:06.080]   on eBay that just--
[00:08:06.080 --> 00:08:07.400]   But don't say the leche Oreo is good.
[00:08:07.400 --> 00:08:08.440]   I had that.
[00:08:08.440 --> 00:08:09.440]   That was yummy.
[00:08:09.440 --> 00:08:10.440]   That was great.
[00:08:10.440 --> 00:08:11.840]   Birthday cake, candy cane, candy corn.
[00:08:11.840 --> 00:08:14.000]   Those were disgusting.
[00:08:14.000 --> 00:08:17.200]   The candy corn Oreos, those were awful.
[00:08:17.200 --> 00:08:19.000]   Berry, lemon twist, Neapolitan.
[00:08:19.000 --> 00:08:19.640]   Good lord.
[00:08:19.640 --> 00:08:20.120]   Good man.
[00:08:20.120 --> 00:08:20.640]   A split.
[00:08:20.640 --> 00:08:23.520]   Oh, when you're having the spongebob Oreos in.
[00:08:23.520 --> 00:08:24.480]   Oh, 2014.
[00:08:24.480 --> 00:08:26.360]   I don't know if I want to eat those right now.
[00:08:26.360 --> 00:08:27.320]   I might have to have that.
[00:08:27.320 --> 00:08:32.680]   So to get back to the news, the reason ZTE was in trouble
[00:08:32.680 --> 00:08:37.440]   is because the Commerce Department said ZTE's
[00:08:37.440 --> 00:08:39.960]   using its hardware to spy in America.
[00:08:39.960 --> 00:08:41.880]   Well, also they violated the sanctions
[00:08:41.880 --> 00:08:46.280]   against North Iran and North Korea and then I think
[00:08:46.280 --> 00:08:48.040]   they were supposed to--
[00:08:48.040 --> 00:08:49.920]   the people who actually made those decisions were supposed
[00:08:49.920 --> 00:08:53.720]   to get in trouble or be penalized and ZTE did not penalize
[00:08:53.720 --> 00:08:55.720]   them.
[00:08:55.720 --> 00:09:00.360]   This is so crazy that just--
[00:09:00.360 --> 00:09:02.360]   I think he got a phone call from President Xi.
[00:09:02.360 --> 00:09:04.240]   I think that that's-- he did, in fact.
[00:09:04.240 --> 00:09:08.320]   And then the President tweets and everything.
[00:09:08.320 --> 00:09:11.600]   It's like he's like a guy sitting in a chess game
[00:09:11.600 --> 00:09:14.320]   and just throws the table in the air and it says,
[00:09:14.320 --> 00:09:16.160]   now let's try playing.
[00:09:16.160 --> 00:09:18.600]   It's crazy.
[00:09:18.600 --> 00:09:19.760]   But I like ZTE phones.
[00:09:19.760 --> 00:09:20.640]   But I'm still--
[00:09:20.640 --> 00:09:22.440]   I don't know if I should buy a ZTE phone.
[00:09:22.440 --> 00:09:23.760]   Well, why would you--
[00:09:23.760 --> 00:09:26.600]   well, ZTE is going to be in trouble after this anyway,
[00:09:26.600 --> 00:09:29.600]   because we've got other Chinese brands that are coming up
[00:09:29.600 --> 00:09:30.120]   the wayside.
[00:09:30.120 --> 00:09:31.640]   They don't have these problems.
[00:09:31.640 --> 00:09:34.840]   But Xiaomi might eventually start selling some--
[00:09:34.840 --> 00:09:35.840]   or yeah, who knows?
[00:09:35.840 --> 00:09:36.880]   Xiaomi Huawei.
[00:09:36.880 --> 00:09:38.760]   They want to sell smart home stuff here now.
[00:09:38.760 --> 00:09:40.080]   One plus.
[00:09:40.080 --> 00:09:40.840]   One plus?
[00:09:40.840 --> 00:09:44.120]   They're already supplying all the scooter companies
[00:09:44.120 --> 00:09:44.960]   with scooters.
[00:09:44.960 --> 00:09:46.120]   I thought using cameras--
[00:09:46.120 --> 00:09:48.000]   I thought it was really interesting when Google--
[00:09:48.000 --> 00:09:49.920]   Google I/O this week announced that they were going
[00:09:49.920 --> 00:09:52.320]   to make Android P available as a public beta.
[00:09:52.320 --> 00:09:53.840]   It wasn't just a Pixel phone.
[00:09:53.840 --> 00:09:56.640]   It included, by the way, Oppo phones or one--
[00:09:56.640 --> 00:09:57.920]   is that one plus?
[00:09:57.920 --> 00:09:58.920]   No.
[00:09:58.920 --> 00:09:59.760]   Yeah, the one plus.
[00:09:59.760 --> 00:10:00.600]   Same same--
[00:10:00.600 --> 00:10:01.800]   --manifacturing line, yeah.
[00:10:01.800 --> 00:10:03.840]   Oppo is the Chinese phone company.
[00:10:03.840 --> 00:10:07.520]   95% of Android phones, except the 5% that we're missing
[00:10:07.520 --> 00:10:09.360]   were Samsung phones.
[00:10:09.360 --> 00:10:10.280]   No Samsung.
[00:10:10.280 --> 00:10:10.880]   Yeah.
[00:10:10.880 --> 00:10:11.400]   Very peculiar.
[00:10:11.400 --> 00:10:12.600]   You noticed that, did you?
[00:10:12.600 --> 00:10:13.080]   Mm-hmm.
[00:10:13.080 --> 00:10:13.680]   As did I.
[00:10:13.680 --> 00:10:15.080]   And was that a technical--
[00:10:15.080 --> 00:10:19.320]   Well, Samsung right now, the S9 is not using trouble.
[00:10:19.320 --> 00:10:20.440]   It hasn't been troubleized.
[00:10:20.440 --> 00:10:21.720]   You think trouble is the issue?
[00:10:21.720 --> 00:10:23.960]   Apparently, it's the reason why they're
[00:10:23.960 --> 00:10:25.320]   able to do what they're able to do
[00:10:25.320 --> 00:10:27.760]   is because of all the phones that
[00:10:27.760 --> 00:10:31.360]   are compatible with the P beta have been troubleized.
[00:10:31.360 --> 00:10:33.240]   So they've separated the low end.
[00:10:33.240 --> 00:10:34.400]   Maybe I should have flowed--
[00:10:34.400 --> 00:10:35.640]   explain what trouble is.
[00:10:35.640 --> 00:10:37.440]   What is project trouble?
[00:10:37.440 --> 00:10:38.160]   Or do you want to do it?
[00:10:38.160 --> 00:10:39.160]   It's--
[00:10:39.160 --> 00:10:39.660]   It's--
[00:10:39.660 --> 00:10:40.160]   It's--
[00:10:40.160 --> 00:10:40.160]   --it's--
[00:10:40.160 --> 00:10:41.160]   --it's--
[00:10:41.160 --> 00:10:42.160]   flow does not want to do it.
[00:10:42.160 --> 00:10:43.160]   I can tell.
[00:10:43.160 --> 00:10:43.160]   It's--
[00:10:43.160 --> 00:10:44.160]   I think you would have a better time.
[00:10:44.160 --> 00:10:45.160]   [LAUGHTER]
[00:10:45.160 --> 00:10:49.880]   But with project trouble, which was part of Android N, they--
[00:10:49.880 --> 00:10:50.600]   or oh, sorry.
[00:10:50.600 --> 00:10:51.520]   Oh, was it all over N?
[00:10:51.520 --> 00:10:55.480]   Anyway, they separated the low level kernel stuff--
[00:10:55.480 --> 00:10:56.480]   And the reason--
[00:10:56.480 --> 00:10:57.960]   The reason they had to do this--
[00:10:57.960 --> 00:11:02.240]   --the high end, the surface level interface components.
[00:11:02.240 --> 00:11:04.720]   And so now you can swap out that low level--
[00:11:04.720 --> 00:11:07.800]   the low level components without touching the high end,
[00:11:07.800 --> 00:11:09.200]   the surface stuff.
[00:11:09.200 --> 00:11:14.040]   And so that's why they're able to insert the Android P operating
[00:11:14.040 --> 00:11:18.600]   system components without touching the interface components.
[00:11:18.600 --> 00:11:20.920]   We actually had this chat with Google earlier this week
[00:11:20.920 --> 00:11:22.760]   when we sat down with them for all about Android.
[00:11:22.760 --> 00:11:25.040]   And Stephanie said, Cuthperson, she mentioned,
[00:11:25.040 --> 00:11:27.960]   that project trouble is the reason that 95% of the Android
[00:11:27.960 --> 00:11:31.040]   phones were able to get Android P like that.
[00:11:31.040 --> 00:11:34.360]   So 5% that's missing is Samsung, which--
[00:11:34.360 --> 00:11:36.960]   Because they didn't do that separation
[00:11:36.960 --> 00:11:38.080]   when they implemented Android P.
[00:11:38.080 --> 00:11:40.560]   But it speaks volumes too of what--
[00:11:40.560 --> 00:11:43.040]   I mean, I'm thinking it's a reason.
[00:11:43.040 --> 00:11:47.080]   Also, huge news because Google says now that they think
[00:11:47.080 --> 00:11:50.240]   some vast percentage of Android phones,
[00:11:50.240 --> 00:11:53.520]   they're going to be able to require the security updates too.
[00:11:53.520 --> 00:11:55.280]   Yeah, for now on that, they'll be part of the deal.
[00:11:55.280 --> 00:11:56.920]   They've updated the licensing agreement
[00:11:56.920 --> 00:12:01.840]   to require them to push out the monthly security patches.
[00:12:01.840 --> 00:12:03.960]   This is all very, very good news.
[00:12:03.960 --> 00:12:04.960]   Very good.
[00:12:04.960 --> 00:12:06.320]   This has been a real problem.
[00:12:06.320 --> 00:12:07.960]   I mean, one of the advantages iPhone
[00:12:07.960 --> 00:12:11.280]   has, and one of the reasons security experts stick with iPhone--
[00:12:11.280 --> 00:12:12.360]   I even talked to somebody who said,
[00:12:12.360 --> 00:12:14.880]   I wouldn't go to a DEF CON without an iPhone.
[00:12:14.880 --> 00:12:16.520]   I would never bring an Android phone to DEF CON,
[00:12:16.520 --> 00:12:17.960]   because it's just too dangerous.
[00:12:17.960 --> 00:12:18.600]   It's because--
[00:12:18.600 --> 00:12:19.800]   Take a Nokia 3310.
[00:12:19.800 --> 00:12:20.800]   That's safe.
[00:12:20.800 --> 00:12:23.360]   It's because Apple mandates the updates.
[00:12:23.360 --> 00:12:26.160]   And it's very quick to update, and most Apple users
[00:12:26.160 --> 00:12:27.760]   update immediately.
[00:12:27.760 --> 00:12:30.800]   But Android, there's probably three-quarters of the Android
[00:12:30.800 --> 00:12:33.280]   user base doesn't have updateable phones.
[00:12:33.280 --> 00:12:35.640]   It's not like amazing a decade into Android
[00:12:35.640 --> 00:12:37.960]   that they're still trying to figure out some of this stuff.
[00:12:37.960 --> 00:12:40.000]   Well, even though it is hard.
[00:12:40.000 --> 00:12:40.440]   Yeah.
[00:12:40.440 --> 00:12:41.160]   You know, it's funny.
[00:12:41.160 --> 00:12:42.600]   It's kind of a double-edged sword.
[00:12:42.600 --> 00:12:44.280]   I think it's because Android was open source.
[00:12:44.280 --> 00:12:45.040]   Yep.
[00:12:45.040 --> 00:12:46.960]   I mean, the OpenUS makes that a lot more challenging.
[00:12:46.960 --> 00:12:48.320]   Which is good, right?
[00:12:48.320 --> 00:12:50.000]   We want it to be open.
[00:12:50.000 --> 00:12:51.880]   But at the same time, that means anybody,
[00:12:51.880 --> 00:12:55.680]   any joke, can put an Android device.
[00:12:55.680 --> 00:12:57.160]   And they don't have to do updates.
[00:12:57.160 --> 00:12:58.880]   And I'm still not clear under what degree
[00:12:58.880 --> 00:13:03.280]   that you can legitimately blame carriers or handset makers
[00:13:03.280 --> 00:13:04.720]   for not taking this seriously off.
[00:13:04.720 --> 00:13:05.560]   It's both.
[00:13:05.560 --> 00:13:07.040]   It's all of the above.
[00:13:07.040 --> 00:13:09.440]   It's a little bit-- it comes with the territory,
[00:13:09.440 --> 00:13:13.400]   but also it may be due to intrangidence, transigence,
[00:13:13.400 --> 00:13:16.200]   or slowness on the part of the third party companies.
[00:13:16.200 --> 00:13:19.360]   Well, and when you sell a phone for $30 or $50,
[00:13:19.360 --> 00:13:20.960]   you don't want to be updating it.
[00:13:20.960 --> 00:13:21.960]   You just want it.
[00:13:21.960 --> 00:13:23.000]   It's a disposable price.
[00:13:23.000 --> 00:13:25.280]   Especially if you're not getting recurring revenues
[00:13:25.280 --> 00:13:26.320]   from that device.
[00:13:26.320 --> 00:13:30.000]   A lot of those devices are just really disposable.
[00:13:30.000 --> 00:13:30.520]   Yep.
[00:13:30.520 --> 00:13:31.040]   Especially in the--
[00:13:31.040 --> 00:13:32.680]   Did the higher end phones have a better record
[00:13:32.680 --> 00:13:34.040]   of getting updates?
[00:13:34.040 --> 00:13:35.040]   Yes.
[00:13:35.040 --> 00:13:35.880]   Not all of them.
[00:13:35.880 --> 00:13:37.040]   But not all of them.
[00:13:37.040 --> 00:13:38.560]   Samsung tends to be slow.
[00:13:38.560 --> 00:13:42.720]   They tend to be slow on the operating system updates.
[00:13:42.720 --> 00:13:45.640]   They're pretty good on the monthly security update.
[00:13:45.640 --> 00:13:46.880]   Depends on the carrier, right?
[00:13:46.880 --> 00:13:47.240]   Yeah, that's true.
[00:13:47.240 --> 00:13:49.120]   And that's not been good about pushing those security
[00:13:49.120 --> 00:13:50.640]   updates on the Galaxy phones.
[00:13:50.640 --> 00:13:52.080]   Unfortunately.
[00:13:52.080 --> 00:13:53.520]   Or at least in my experience.
[00:13:53.520 --> 00:13:54.440]   You know, else is not good.
[00:13:54.440 --> 00:13:58.320]   This Windows machine, the mouse keeps dying on me.
[00:13:58.320 --> 00:14:00.840]   Well, anyway, there's our breaking--
[00:14:00.840 --> 00:14:02.560]   There's our breaking mouse.
[00:14:02.560 --> 00:14:03.400]   I put it in the battery.
[00:14:03.400 --> 00:14:04.400]   Sorry.
[00:14:04.400 --> 00:14:04.920]   Yeah.
[00:14:04.920 --> 00:14:06.600]   First thing when you had the last time we were here
[00:14:06.600 --> 00:14:09.440]   with the microphone and the speaker at it.
[00:14:09.440 --> 00:14:09.840]   Oh, yeah.
[00:14:09.840 --> 00:14:10.920]   The Skype mouse.
[00:14:10.920 --> 00:14:11.560]   Yeah.
[00:14:11.560 --> 00:14:12.480]   I keep that around.
[00:14:12.480 --> 00:14:16.080]   I'm going to be podcasting with that in my old age.
[00:14:16.080 --> 00:14:20.080]   Honestly, I can't see if there's a security.
[00:14:20.080 --> 00:14:21.400]   John, could you give me a wired mouse?
[00:14:21.400 --> 00:14:23.400]   Yeah, thank you.
[00:14:23.400 --> 00:14:25.800]   Because I can't do anything on this beautiful computer.
[00:14:25.800 --> 00:14:26.720]   I'm just going to type it in time.
[00:14:26.720 --> 00:14:27.800]   That is a beautiful screen.
[00:14:27.800 --> 00:14:29.840]   Well, oh, and the keyboards died too.
[00:14:29.840 --> 00:14:36.800]   So it must have something to do with Bluetooth stuff.
[00:14:36.800 --> 00:14:37.520]   Thank you.
[00:14:37.520 --> 00:14:40.440]   We're going to give a wired mouse in here.
[00:14:40.440 --> 00:14:41.760]   Now I can't even touch the screen.
[00:14:41.760 --> 00:14:42.720]   There we go.
[00:14:42.720 --> 00:14:43.800]   What the hell is going on?
[00:14:43.800 --> 00:14:49.040]   Is anybody else here using Windows?
[00:14:49.040 --> 00:14:49.400]   I am.
[00:14:49.400 --> 00:14:50.880]   Promo S.
[00:14:50.880 --> 00:14:51.720]   Promo S.
[00:14:51.720 --> 00:14:54.720]   I'm iPad Pro.
[00:14:54.720 --> 00:14:56.280]   Is it just me?
[00:14:56.280 --> 00:14:58.200]   If it wasn't a work issued computer,
[00:14:58.200 --> 00:14:59.360]   it probably wouldn't be.
[00:14:59.360 --> 00:15:01.640]   I just didn't mention Sam works for Navigant.
[00:15:01.640 --> 00:15:03.720]   You're an analyst in the auto sector.
[00:15:03.720 --> 00:15:04.220]   Yes.
[00:15:04.220 --> 00:15:06.640]   I cover a lead arm mobility research service.
[00:15:06.640 --> 00:15:09.440]   So I cover autonomous and connected vehicles
[00:15:09.440 --> 00:15:14.720]   and automotive cyber security and mobility services.
[00:15:14.720 --> 00:15:18.360]   And also on the side, I do some writing for Forbes.
[00:15:18.360 --> 00:15:20.960]   I write a monthly column for automotive engineering
[00:15:20.960 --> 00:15:21.920]   and do a podcast.
[00:15:21.920 --> 00:15:22.680]   Nice.
[00:15:22.680 --> 00:15:26.840]   He took me for a ride in the new Cadillac CT6.
[00:15:26.840 --> 00:15:28.560]   And the reason we were trying it out-- in fact,
[00:15:28.560 --> 00:15:31.400]   well, you'll see this on the new screensavers on Saturday--
[00:15:31.400 --> 00:15:35.640]   is it has-- kind of similar to the Tesla autopilot.
[00:15:35.640 --> 00:15:37.160]   It has level 2 autonomy.
[00:15:37.160 --> 00:15:37.720]   What is that?
[00:15:37.720 --> 00:15:38.320]   Level 2.
[00:15:38.320 --> 00:15:40.680]   Level 2 means it's partially automated.
[00:15:40.680 --> 00:15:45.400]   So it can do speed control and directional control combined.
[00:15:45.400 --> 00:15:47.960]   So in this case, in the case of the Super Cruise,
[00:15:47.960 --> 00:15:49.440]   it has adaptive cruise control.
[00:15:49.440 --> 00:15:52.000]   So it'll maintain your distance to the vehicle in front of you,
[00:15:52.000 --> 00:15:53.120]   maintain your speed.
[00:15:53.120 --> 00:15:56.280]   And it'll also stay in the center of the lane.
[00:15:56.280 --> 00:15:58.520]   So as you're driving down the road, comes to a curve,
[00:15:58.520 --> 00:15:59.800]   it automatically steers for you.
[00:15:59.800 --> 00:16:03.720]   So the Cadillac is the first production vehicle
[00:16:03.720 --> 00:16:05.560]   that is actually designed for hands-off.
[00:16:05.560 --> 00:16:07.240]   So the Tesla, you're actually supposed
[00:16:07.240 --> 00:16:09.120]   to keep your hands on the wheel.
[00:16:09.120 --> 00:16:10.920]   You can take it off for a few seconds at a time
[00:16:10.920 --> 00:16:12.440]   and it'll start alerting you.
[00:16:12.440 --> 00:16:14.960]   The Cadillac is actually designed to be hands-off
[00:16:14.960 --> 00:16:17.160]   because it has a driver monitor camera.
[00:16:17.160 --> 00:16:21.120]   It works very much like Face ID on the iPhone X.
[00:16:21.120 --> 00:16:23.360]   So there's infrared emitters and an infrared camera
[00:16:23.360 --> 00:16:24.600]   on the steering column.
[00:16:24.600 --> 00:16:26.480]   And to make sure that you're watching the road.
[00:16:26.480 --> 00:16:31.080]   And if you stop watching the road, tell them what happens, Leo.
[00:16:31.080 --> 00:16:33.800]   And by the way, it was hard to stop watching the road.
[00:16:33.800 --> 00:16:37.320]   I looked over to Sam, but I'm worried at the highway speed,
[00:16:37.320 --> 00:16:39.120]   65 miles an hour.
[00:16:39.120 --> 00:16:40.800]   It's like, I have to look at the road.
[00:16:40.800 --> 00:16:43.800]   Finally, I put my fingers in front of my eyes in IP.
[00:16:43.800 --> 00:16:45.240]   No.
[00:16:45.240 --> 00:16:47.120]   But that was enough to trick the camera into thinking
[00:16:47.120 --> 00:16:48.320]   I wasn't looking.
[00:16:48.320 --> 00:16:50.480]   And then the bar goes red.
[00:16:50.480 --> 00:16:53.000]   Your seat goes, move, move, move, move, move, move, move, move.
[00:16:53.000 --> 00:16:54.160]   Wake up.
[00:16:54.160 --> 00:16:55.480]   It was quite a thing.
[00:16:55.480 --> 00:16:56.640]   I was very impressed by it.
[00:16:56.640 --> 00:16:57.840]   It did a good job, actually.
[00:16:57.840 --> 00:16:59.800]   And one of the problems I've always had with the Tesla,
[00:16:59.800 --> 00:17:00.480]   are you told me this?
[00:17:00.480 --> 00:17:01.520]   I didn't know this.
[00:17:01.520 --> 00:17:03.560]   All the cars that have these--
[00:17:03.560 --> 00:17:05.160]   your hands are on the wheel sensors.
[00:17:05.160 --> 00:17:08.040]   I just assumed it was some sort of capacitive sensor
[00:17:08.040 --> 00:17:09.680]   that you can tell you're touching it.
[00:17:09.680 --> 00:17:10.760]   It's not.
[00:17:10.760 --> 00:17:13.400]   That you have to actually move the wheel a little bit.
[00:17:13.400 --> 00:17:15.360]   It has a torque sensor, a Samsung.
[00:17:15.360 --> 00:17:17.360]   So that's why when I'm on the--
[00:17:17.360 --> 00:17:19.800]   driving on the Tesla, it doesn't know my hands on the wheel.
[00:17:19.800 --> 00:17:21.080]   It's like kind of jerked the wheel.
[00:17:21.080 --> 00:17:23.200]   Well, a lot of good that is.
[00:17:23.200 --> 00:17:24.440]   So I like this Cadillac.
[00:17:24.440 --> 00:17:24.800]   I was--
[00:17:24.800 --> 00:17:26.960]   Yeah, the Cadillac actually does have capacitive sensors
[00:17:26.960 --> 00:17:27.880]   in the steering wheel.
[00:17:27.880 --> 00:17:28.600]   Yeah.
[00:17:28.600 --> 00:17:29.840]   So it knows.
[00:17:29.840 --> 00:17:32.240]   And it's looking in my eyes to see if I'm paying attention.
[00:17:32.240 --> 00:17:33.360]   I thought that was really impressive.
[00:17:33.360 --> 00:17:35.480]   And it even worked through sunglasses and everything.
[00:17:35.480 --> 00:17:36.440]   But it's interesting.
[00:17:36.440 --> 00:17:40.800]   A lot of the automakers are being not surprising,
[00:17:40.800 --> 00:17:43.840]   given the current news, very slow about releasing
[00:17:43.840 --> 00:17:46.960]   these kind of autonomous features, self-driving features.
[00:17:46.960 --> 00:17:48.440]   Yeah, they're starting to realize
[00:17:48.440 --> 00:17:51.720]   that the more automation you put in the vehicles that
[00:17:51.720 --> 00:17:54.640]   short of full automation, what happens is people
[00:17:54.640 --> 00:17:56.120]   start to become very complacent.
[00:17:56.120 --> 00:17:58.000]   And even if you tell them--
[00:17:58.000 --> 00:18:01.640]   they tell Tesla owners, it's a driver system.
[00:18:01.640 --> 00:18:02.600]   Keep your hands on the wheel.
[00:18:02.600 --> 00:18:03.560]   Watch the road.
[00:18:03.560 --> 00:18:05.400]   Be ready to take over.
[00:18:05.400 --> 00:18:06.160]   But--
[00:18:06.160 --> 00:18:10.440]   Tesla, I heard a bad guy who was sitting in the passenger
[00:18:10.440 --> 00:18:11.480]   seat.
[00:18:11.480 --> 00:18:13.040]   He would turn on auto drivers and Tesla.
[00:18:13.040 --> 00:18:14.880]   That was the-- that was a court.
[00:18:14.880 --> 00:18:16.840]   The court said you can't drive anymore.
[00:18:16.840 --> 00:18:17.320]   Yeah.
[00:18:17.320 --> 00:18:19.480]   They took away his license for 18 months.
[00:18:19.480 --> 00:18:20.600]   What a netwit.
[00:18:20.600 --> 00:18:22.320]   Yeah.
[00:18:22.320 --> 00:18:24.000]   I mean, you've got to try it once.
[00:18:24.000 --> 00:18:25.280]   We should-- there was--
[00:18:25.280 --> 00:18:26.440]   No, you don't.
[00:18:26.440 --> 00:18:27.240]   Don't.
[00:18:27.240 --> 00:18:29.080]   Maybe in a parking lot.
[00:18:29.080 --> 00:18:30.080]   That's the thing.
[00:18:30.080 --> 00:18:32.560]   If somebody wants to do something on a parking lot
[00:18:32.560 --> 00:18:35.320]   or on a private test track, I have no problem with that.
[00:18:35.320 --> 00:18:36.280]   Totally, no.
[00:18:36.280 --> 00:18:38.800]   But don't go out on public roads around it.
[00:18:38.800 --> 00:18:41.000]   Because you're putting other people at risk.
[00:18:41.000 --> 00:18:42.520]   And that's just not right.
[00:18:42.520 --> 00:18:44.000]   That's just idiotic.
[00:18:44.000 --> 00:18:45.800]   There was another Tesla accident.
[00:18:45.800 --> 00:18:47.480]   We should probably mention again.
[00:18:47.480 --> 00:18:49.680]   And the question that is--
[00:18:49.680 --> 00:18:52.320]   I think still unknown is whether the driver was using auto
[00:18:52.320 --> 00:18:52.760]   pilot.
[00:18:52.760 --> 00:18:55.120]   But the car rammed into a--
[00:18:55.120 --> 00:18:55.680]   A fire truck.
[00:18:55.680 --> 00:18:56.200]   A fire truck?
[00:18:56.200 --> 00:18:56.560]   Yeah.
[00:18:56.560 --> 00:18:58.400]   So the second time in the last several months,
[00:18:58.400 --> 00:18:59.880]   how would that happen?
[00:18:59.880 --> 00:19:03.960]   Fire truck was stopped.
[00:19:03.960 --> 00:19:07.000]   It was going at 60 miles an hour.
[00:19:07.000 --> 00:19:09.160]   Apparently, no braking before impact,
[00:19:09.160 --> 00:19:11.520]   not known if auto pilot was engaged.
[00:19:11.520 --> 00:19:13.960]   The driver was injured, but is alive.
[00:19:13.960 --> 00:19:16.400]   Well, the thing is, even if just the adaptive cruise
[00:19:16.400 --> 00:19:18.720]   control was active and not the full auto pilot--
[00:19:18.720 --> 00:19:19.560]   It should stop.
[00:19:19.560 --> 00:19:20.480]   No, it won't.
[00:19:20.480 --> 00:19:24.280]   Because the radar sensor for the cruise control
[00:19:24.280 --> 00:19:26.320]   is designed to-- especially at higher speeds
[00:19:26.320 --> 00:19:28.200]   when you're at highway speeds--
[00:19:28.200 --> 00:19:30.560]   is designed to ignore stationary objects.
[00:19:30.560 --> 00:19:32.000]   Because the great thing about radars,
[00:19:32.000 --> 00:19:34.720]   it lets you measure the distance.
[00:19:34.720 --> 00:19:36.320]   The objects and the speed of those objects.
[00:19:36.320 --> 00:19:38.240]   So if the car's not moving, it ignores it
[00:19:38.240 --> 00:19:39.160]   that you're going to run into?
[00:19:39.160 --> 00:19:42.360]   Well, it ignores anything that has zero velocity.
[00:19:42.360 --> 00:19:44.000]   So think about it.
[00:19:44.000 --> 00:19:45.360]   I mean, if you're driving down the road--
[00:19:45.360 --> 00:19:45.600]   You don't want to--
[00:19:45.600 --> 00:19:47.920]   That's sign on the side of the road as zero velocity.
[00:19:47.920 --> 00:19:50.240]   You don't want it constantly slamming on the brakes.
[00:19:50.240 --> 00:19:53.760]   So at highway speeds, when you're at that speed,
[00:19:53.760 --> 00:19:57.440]   the presumption is that especially if you're
[00:19:57.440 --> 00:19:59.520]   going at that speed, that everything else around you
[00:19:59.520 --> 00:20:01.600]   should also be moved-- everything of interest
[00:20:01.600 --> 00:20:02.920]   should also be moving.
[00:20:02.920 --> 00:20:05.560]   So it starts to ignore the static objects,
[00:20:05.560 --> 00:20:08.120]   like things that would be on the side of the road,
[00:20:08.120 --> 00:20:09.680]   or in this case, a fire truck.
[00:20:09.680 --> 00:20:11.840]   So clearly, in this case, the person
[00:20:11.840 --> 00:20:13.240]   was driving too fast.
[00:20:13.240 --> 00:20:15.920]   They had cruise control on it, too high a speed,
[00:20:15.920 --> 00:20:19.040]   in a place where they shouldn't have been.
[00:20:19.040 --> 00:20:19.560]   And--
[00:20:19.560 --> 00:20:21.680]   It doesn't look like they're on the highway.
[00:20:21.680 --> 00:20:23.640]   And the same thing.
[00:20:23.640 --> 00:20:25.480]   I drive a lot of different cars.
[00:20:25.480 --> 00:20:29.240]   And I try out all the driver systems.
[00:20:29.240 --> 00:20:31.680]   But as an engineer, by training,
[00:20:31.680 --> 00:20:35.040]   I spent a lot of years working on these kinds of systems.
[00:20:35.040 --> 00:20:38.080]   And I know what to look for.
[00:20:38.080 --> 00:20:39.280]   I know what I'm looking for.
[00:20:39.280 --> 00:20:40.720]   And so whenever I'm trying these things out,
[00:20:40.720 --> 00:20:42.640]   my foot is always hovering over the brake pedal.
[00:20:42.640 --> 00:20:43.760]   I'm always ready.
[00:20:43.760 --> 00:20:44.760]   Because I want to see--
[00:20:44.760 --> 00:20:46.880]   So I try these things out to see what they'll do.
[00:20:46.880 --> 00:20:49.080]   And most vehicles will do the same thing
[00:20:49.080 --> 00:20:51.560]   if you're using adaptive cruise control.
[00:20:51.560 --> 00:20:53.960]   If you're coming up to an intersection,
[00:20:53.960 --> 00:20:56.120]   and there's cars stop there, once it gets
[00:20:56.120 --> 00:20:58.760]   within range of the radar, if those vehicles aren't moving,
[00:20:58.760 --> 00:20:59.400]   it'll ignore them.
[00:20:59.400 --> 00:21:02.200]   And if you don't hit the brakes, it'll keep going.
[00:21:02.200 --> 00:21:05.560]   You should put this in the nonexistent Tesla manual.
[00:21:05.560 --> 00:21:07.880]   Well, they do put it in the manuals on other cars.
[00:21:07.880 --> 00:21:09.680]   I drive with cruise control always as my--
[00:21:09.680 --> 00:21:10.640]   It wasn't in the brochure.
[00:21:10.640 --> 00:21:12.360]   --but other any regular cruise control.
[00:21:12.360 --> 00:21:14.200]   Why are you driving like that in automatic?
[00:21:14.200 --> 00:21:16.160]   We're not used like the stupid parallel parking
[00:21:16.160 --> 00:21:17.880]   feature on my Ford Focus.
[00:21:17.880 --> 00:21:20.040]   I'm always assuming that it's going to crash into a car.
[00:21:20.040 --> 00:21:20.560]   And I--
[00:21:20.560 --> 00:21:21.200]   Does it work?
[00:21:21.200 --> 00:21:22.840]   --have to make hands on the wheel and my feet on the pedals?
[00:21:22.840 --> 00:21:23.480]   It works though, right?
[00:21:23.480 --> 00:21:23.980]   Does it work?
[00:21:23.980 --> 00:21:24.480]   Right.
[00:21:24.480 --> 00:21:25.480]   I find my view--
[00:21:25.480 --> 00:21:26.480]   I see it through that.
[00:21:26.480 --> 00:21:27.480]   I rear-ended a vehicle.
[00:21:27.480 --> 00:21:29.880]   I front-ended a vehicle from the rear end of a Ford.
[00:21:29.880 --> 00:21:32.160]   On the Ford test track with Ford engineers,
[00:21:32.160 --> 00:21:35.440]   sitting in the vehicle, I hit laughing at me.
[00:21:35.440 --> 00:21:37.480]   Because another thing they don't tell you
[00:21:37.480 --> 00:21:41.280]   is after it parks, unless you stop the car,
[00:21:41.280 --> 00:21:43.480]   it will continue to roll backwards.
[00:21:43.480 --> 00:21:44.400]   Oh.
[00:21:44.400 --> 00:21:46.200]   The newer systems don't do that anymore.
[00:21:46.200 --> 00:21:47.720]   So they should have said--
[00:21:47.720 --> 00:21:51.480]   and then once you're parked, put your foot on the brake
[00:21:51.480 --> 00:21:54.240]   and turn the car off, it was idling back,
[00:21:54.240 --> 00:21:57.160]   and it just kept rolling backwards.
[00:21:57.160 --> 00:21:59.360]   How long will it be until these types of accidents
[00:21:59.360 --> 00:22:01.040]   are not in the news?
[00:22:01.040 --> 00:22:03.200]   Well, they should be in the news, I think, at this point.
[00:22:03.200 --> 00:22:04.040]   Right now, yes.
[00:22:04.040 --> 00:22:04.800]   I think this is--
[00:22:04.800 --> 00:22:07.360]   Is there so common with the story of a new story?
[00:22:07.360 --> 00:22:09.120]   Probably not in five years.
[00:22:09.120 --> 00:22:10.560]   Yeah, I mean, there's--
[00:22:10.560 --> 00:22:13.360]   so there was-- two teens died on Tuesday.
[00:22:13.360 --> 00:22:15.440]   This guy got in a crash on Friday.
[00:22:15.440 --> 00:22:18.120]   In between that time, thousands of people
[00:22:18.120 --> 00:22:19.800]   have died in auto crashes.
[00:22:19.800 --> 00:22:20.800]   Well, the thing you have to keep in mind--
[00:22:20.800 --> 00:22:21.520]   Or hundreds anyway.
[00:22:21.520 --> 00:22:24.600]   You have to keep in mind is how many miles we actually
[00:22:24.600 --> 00:22:27.120]   drive every year, just in the United States alone,
[00:22:27.120 --> 00:22:31.360]   the last couple of years, we drive 3.2 trillion miles a year.
[00:22:31.360 --> 00:22:36.400]   And we had-- in 2015, we had almost 3.2 trillion miles,
[00:22:36.400 --> 00:22:38.200]   and 6.3 million accidents.
[00:22:38.200 --> 00:22:41.680]   So that's one accident every half a million miles of driving.
[00:22:41.680 --> 00:22:42.640]   That's not too bad.
[00:22:42.640 --> 00:22:45.120]   Yeah, I mean, when you put it in that context--
[00:22:45.120 --> 00:22:47.400]   yes, 37,000 people died.
[00:22:47.400 --> 00:22:51.640]   But we've got 270 million vehicles on the road
[00:22:51.640 --> 00:22:53.800]   driving 3 trillion miles.
[00:22:53.800 --> 00:22:57.400]   The chances of actually getting into an accident at all,
[00:22:57.400 --> 00:23:01.720]   or worse dying in an accident, are actually remarkably slim.
[00:23:01.720 --> 00:23:03.920]   Still, we would like to make that number slowly.
[00:23:03.920 --> 00:23:06.280]   And we have-- we've made tremendous progress
[00:23:06.280 --> 00:23:07.360]   over the last 40 years.
[00:23:07.360 --> 00:23:11.800]   Since 1975, it was the number of people
[00:23:11.800 --> 00:23:14.760]   that died for every 100 million miles of driving
[00:23:14.760 --> 00:23:16.520]   was about 5 and 1/2--
[00:23:16.520 --> 00:23:18.920]   5 and 1/2 people for every 100 million miles.
[00:23:18.920 --> 00:23:22.760]   Now it's 1.1 person fatality for every 100 million miles.
[00:23:22.760 --> 00:23:25.160]   So we've cut it by 80% in the last 40 years.
[00:23:25.160 --> 00:23:26.000]   That's right.
[00:23:26.000 --> 00:23:26.560]   That's--
[00:23:26.560 --> 00:23:27.760]   Airbags and ABS.
[00:23:27.760 --> 00:23:31.560]   Airbags, ABS, better structures, the crash structures,
[00:23:31.560 --> 00:23:33.360]   the occupant protection regulations.
[00:23:33.360 --> 00:23:35.080]   That's all because of those regulations,
[00:23:35.080 --> 00:23:37.880]   and requiring people to wear seat belts,
[00:23:37.880 --> 00:23:39.960]   cracking down on drunk drivers.
[00:23:39.960 --> 00:23:45.680]   But still, even of the vehicle occupants that died in crashes,
[00:23:45.680 --> 00:23:48.400]   46% of those were unbelt.
[00:23:48.400 --> 00:23:49.760]   Yeah.
[00:23:49.760 --> 00:23:52.880]   And that's about 22,000 people who died.
[00:23:52.880 --> 00:23:56.360]   So over 10,000 people died because they weren't wearing seat belts.
[00:23:56.360 --> 00:23:58.760]   Isn't that telling?
[00:23:58.760 --> 00:23:59.960]   We're going to take a break.
[00:23:59.960 --> 00:24:02.200]   When we come back, there were two big developer
[00:24:02.200 --> 00:24:03.600]   conferences this week.
[00:24:03.600 --> 00:24:05.760]   We've mentioned a little bit about Google's, but also
[00:24:05.760 --> 00:24:06.520]   Microsoft Bill.
[00:24:06.520 --> 00:24:09.760]   We'll talk about the things we found out, not so much
[00:24:09.760 --> 00:24:13.080]   from Microsoft, a lot from Google, the controversies they
[00:24:13.080 --> 00:24:14.080]   raised.
[00:24:14.080 --> 00:24:15.240]   Great panel here.
[00:24:15.240 --> 00:24:17.640]   Sam, it's so nice to have you talking about cars.
[00:24:17.640 --> 00:24:21.720]   For a senior analyst at Navigant Research, Sam Abuel Samid,
[00:24:21.720 --> 00:24:24.880]   from the fabulous all about Android show, Lawrence Ion.
[00:24:24.880 --> 00:24:26.280]   Don't forget material.
[00:24:26.280 --> 00:24:27.560]   And material.
[00:24:27.560 --> 00:24:29.120]   I don't like to plug material.
[00:24:29.120 --> 00:24:30.200]   I don't know why.
[00:24:30.200 --> 00:24:31.120]   No, from material--
[00:24:31.120 --> 00:24:31.600]   Andy's on there too.
[00:24:31.600 --> 00:24:32.680]   I know.
[00:24:32.680 --> 00:24:35.720]   Two of our hosts do a podcast for another network.
[00:24:35.720 --> 00:24:36.840]   But I'm on a--
[00:24:36.840 --> 00:24:38.040]   It's an underground podcast.
[00:24:38.040 --> 00:24:39.560]   I'm a generous guy.
[00:24:39.560 --> 00:24:41.080]   It's on relay@ff.
[00:24:41.080 --> 00:24:41.480]   Yep.
[00:24:41.480 --> 00:24:43.240]   And I love my curly and it's fine with me.
[00:24:43.240 --> 00:24:44.960]   I'm just being silly.
[00:24:44.960 --> 00:24:46.040]   Thank you for being here, Florence.
[00:24:46.040 --> 00:24:49.040]   And of course, from Fast Company, the technologist
[00:24:49.040 --> 00:24:51.320]   himself, Harry McCrackett.
[00:24:51.320 --> 00:24:54.040]   Our show today brought to you by Ring Video Doorbell.
[00:24:54.040 --> 00:24:55.240]   I love the Ring.
[00:24:55.240 --> 00:24:59.840]   They just put out a new app that takes the neighbor stuff out
[00:24:59.840 --> 00:25:01.800]   of Ring and puts it in a separate app
[00:25:01.800 --> 00:25:04.720]   so that you can keep an eye on neighbors and neighbors' houses.
[00:25:04.720 --> 00:25:08.720]   The Ring Doorbell, we got one a couple of years ago now.
[00:25:08.720 --> 00:25:10.120]   And it's the best thing I ever do.
[00:25:10.120 --> 00:25:11.200]   It was easy to install.
[00:25:11.200 --> 00:25:15.200]   It replaced the doorbell on my house on the front door.
[00:25:15.200 --> 00:25:16.840]   Now when somebody comes to the door,
[00:25:16.840 --> 00:25:18.200]   I can see them ringing.
[00:25:18.200 --> 00:25:23.440]   When we were in Japan, somebody drove over our mailbox.
[00:25:23.440 --> 00:25:26.400]   We have a rural mailbox and somebody clobbered it.
[00:25:26.400 --> 00:25:28.080]   But I had video, not only of that,
[00:25:28.080 --> 00:25:31.080]   but of the person coming to the door to apologize.
[00:25:31.080 --> 00:25:32.960]   So that was awesome.
[00:25:32.960 --> 00:25:34.880]   And if I needed to, I could have gone to the police
[00:25:34.880 --> 00:25:35.880]   as well with that video.
[00:25:35.880 --> 00:25:39.000]   In fact, that happens all the time with the Ring Video Doorbell.
[00:25:39.000 --> 00:25:42.080]   It is a way not only of keeping an eye on your property,
[00:25:42.080 --> 00:25:45.480]   of keeping an eye on packages of speaking and hearing,
[00:25:45.480 --> 00:25:46.720]   with people who are at your front door,
[00:25:46.720 --> 00:25:48.200]   whether they ring the doorbell or not,
[00:25:48.200 --> 00:25:49.520]   when the UPS guy comes and say,
[00:25:49.520 --> 00:25:51.400]   "Hey, leave it right there. I'll be right out."
[00:25:51.400 --> 00:25:52.880]   It's awesome.
[00:25:52.880 --> 00:25:55.480]   But it also becomes a great security device
[00:25:55.480 --> 00:25:58.280]   for keeping an eye on people and intruders.
[00:25:58.280 --> 00:26:00.920]   In fact, when they see the Ring Video Doorbell,
[00:26:00.920 --> 00:26:04.040]   they often turn tail and go the other way.
[00:26:04.040 --> 00:26:08.240]   Now we can add even more security to the home
[00:26:08.240 --> 00:26:11.200]   with Ring's floodlight and spotlight cam.
[00:26:11.200 --> 00:26:12.720]   They let you build a ring of security
[00:26:12.720 --> 00:26:13.720]   around your entire property.
[00:26:13.720 --> 00:26:16.400]   The same camera, microphone, speaker,
[00:26:16.400 --> 00:26:18.960]   and they've added a siren and lights,
[00:26:18.960 --> 00:26:20.320]   motion sensitive lights.
[00:26:20.320 --> 00:26:22.920]   So when somebody's around the side, it lights up.
[00:26:22.920 --> 00:26:25.120]   You can see them, you get a notification on your phone,
[00:26:25.120 --> 00:26:28.280]   even if you're in Japan or anywhere in the world,
[00:26:28.280 --> 00:26:29.320]   and you can speak to them.
[00:26:29.320 --> 00:26:33.040]   And if they don't run away, you can even play a very loud,
[00:26:33.040 --> 00:26:36.720]   and it's 110 dB siren and scare them off.
[00:26:36.720 --> 00:26:40.120]   We've got a great deal on the Ring of Security kits,
[00:26:40.120 --> 00:26:42.680]   which combine a Ring Video Doorbell in one, two,
[00:26:42.680 --> 00:26:45.400]   or three of the Ring Security cams.
[00:26:45.400 --> 00:26:50.400]   Just go to ring.com/twit, ring.com/twit,
[00:26:50.400 --> 00:26:53.960]   and you'll get up to $150 off.
[00:26:53.960 --> 00:26:58.960]   Pick your Ring of Security kit and save right now
[00:26:58.960 --> 00:27:01.080]   at ring.com/twit.
[00:27:01.080 --> 00:27:05.400]   I like to look at the Ring Twitter feed,
[00:27:05.400 --> 00:27:07.200]   'cause there's always fun stuff,
[00:27:08.240 --> 00:27:10.480]   because they post videos of things happening.
[00:27:10.480 --> 00:27:13.480]   Let me see, there's one,
[00:27:13.480 --> 00:27:17.680]   there was one about to steal something off the front porch,
[00:27:17.680 --> 00:27:20.240]   and then they see the Ring camera,
[00:27:20.240 --> 00:27:25.240]   and they literally, they turn around, ring.com/twits.
[00:27:25.240 --> 00:27:29.080]   Leo LePort, we're talking the tech news,
[00:27:29.080 --> 00:27:33.120]   Microsoft's build, three hour, 45 minute keynote.
[00:27:33.120 --> 00:27:33.960]   - Wow!
[00:27:33.960 --> 00:27:35.600]   - No bathroom breaks, right?
[00:27:35.600 --> 00:27:37.600]   - No, they did, they had an exercise break.
[00:27:37.600 --> 00:27:38.440]   - Yeah, they did.
[00:27:38.440 --> 00:27:40.040]   - So long, they had somebody come on stage
[00:27:40.040 --> 00:27:42.480]   and tell people to stretch.
[00:27:42.480 --> 00:27:46.200]   - Here's a tip.
[00:27:46.200 --> 00:27:47.040]   - That's crazy.
[00:27:47.040 --> 00:27:49.080]   - Here's a tip to CEOs out there.
[00:27:49.080 --> 00:27:50.080]   If your keynote is so long,
[00:27:50.080 --> 00:27:52.880]   you need to have an exercise guru give people a break,
[00:27:52.880 --> 00:27:54.520]   you maybe wanna cut it a little bit.
[00:27:54.520 --> 00:27:55.360]   - Just a little bit.
[00:27:55.360 --> 00:27:57.080]   - A little bit of it is eluded.
[00:27:57.080 --> 00:27:58.640]   Did you see anything that you thought
[00:27:58.640 --> 00:28:00.480]   was an exciting area at the--
[00:28:00.480 --> 00:28:01.320]   - A build?
[00:28:01.320 --> 00:28:02.160]   - Yeah.
[00:28:02.160 --> 00:28:04.120]   - Well, my colleague Mark Sullivan attended build
[00:28:04.120 --> 00:28:05.640]   while I was at I/O, but--
[00:28:05.640 --> 00:28:06.960]   - That was another problem, of course.
[00:28:06.960 --> 00:28:09.320]   - We had these overlapping developers conferences.
[00:28:09.320 --> 00:28:12.040]   - I think the your phone stuff is interesting.
[00:28:12.040 --> 00:28:15.320]   The idea of your Windows PC kind of becoming a front end
[00:28:15.320 --> 00:28:17.520]   for your phone is intriguing.
[00:28:17.520 --> 00:28:19.920]   It sounds, I mean, there will be an iPhone version.
[00:28:19.920 --> 00:28:22.280]   I assume the iPhone version of this idea
[00:28:22.280 --> 00:28:24.040]   will be pretty limited just because Apple
[00:28:24.040 --> 00:28:26.360]   will not give the functionality to Microsoft.
[00:28:26.360 --> 00:28:29.000]   It needs to let you do things like messaging
[00:28:29.000 --> 00:28:31.160]   and that the Android version will be more interesting
[00:28:31.160 --> 00:28:33.360]   because it's possible technically to build stuff.
[00:28:33.360 --> 00:28:35.400]   - Somebody call me, it's funny on the radio show today
[00:28:35.400 --> 00:28:37.560]   saying I want to be able to send text messages
[00:28:37.560 --> 00:28:39.240]   from my desktop and that's kind of,
[00:28:39.240 --> 00:28:40.760]   you would think that's easy.
[00:28:40.760 --> 00:28:43.160]   Verizon has an app, I guess some of the other companies
[00:28:43.160 --> 00:28:46.120]   have an app, but this is a much better way to do it.
[00:28:46.120 --> 00:28:48.120]   If you've got an Android device you have this,
[00:28:48.120 --> 00:28:51.120]   your phone app and you can sync your phones
[00:28:51.120 --> 00:28:51.960]   with Windows 10.
[00:28:51.960 --> 00:28:52.920]   - You can use apps like MightyText
[00:28:52.920 --> 00:28:55.880]   and Signal has a desktop app too.
[00:28:55.880 --> 00:28:57.680]   - Yeah, but he didn't want to make his friends
[00:28:57.680 --> 00:28:59.640]   install another app.
[00:28:59.640 --> 00:29:01.120]   He wanted to use SMS.
[00:29:01.120 --> 00:29:02.360]   - Well, I mean, MightyText,
[00:29:02.360 --> 00:29:04.760]   you can connect that with your Android phone
[00:29:04.760 --> 00:29:06.560]   and use that with my life does that.
[00:29:06.560 --> 00:29:08.320]   - Yeah, what is it?
[00:29:08.320 --> 00:29:11.600]   - It's text.2, it's on my phone.
[00:29:11.600 --> 00:29:14.280]   - And what was it, Pulse button or what was that called?
[00:29:14.280 --> 00:29:17.240]   - Yeah, for Android there's a bunch of them that'll work.
[00:29:17.240 --> 00:29:20.120]   For Apple, hey, it's fine, just buy a Macintosh
[00:29:20.120 --> 00:29:22.920]   and then you can use messages, you'll be happy.
[00:29:22.920 --> 00:29:24.760]   If you don't have a Macintosh, why not?
[00:29:24.760 --> 00:29:26.640]   - Text to, sorry.
[00:29:26.640 --> 00:29:28.840]   - Android users, text to teal.
[00:29:28.840 --> 00:29:31.040]   - Okay, but this your phone will do that.
[00:29:31.040 --> 00:29:33.520]   It also will sync notifications,
[00:29:33.520 --> 00:29:35.600]   which is nice, pushable is the one I was trying to say.
[00:29:35.600 --> 00:29:37.520]   - Yeah, that's the description.
[00:29:37.520 --> 00:29:38.520]   You have to pay for that one.
[00:29:38.520 --> 00:29:40.880]   - Yeah, to get full functionality.
[00:29:40.880 --> 00:29:42.120]   Yeah.
[00:29:42.120 --> 00:29:43.400]   - So when does your phone come out?
[00:29:43.400 --> 00:29:45.320]   It says feature goes into beta this week
[00:29:45.320 --> 00:29:48.040]   of expected to launch publicly this fall.
[00:29:48.040 --> 00:29:49.520]   - You know, you got more evidence
[00:29:49.520 --> 00:29:51.720]   instead of trying to get everybody
[00:29:51.720 --> 00:29:53.680]   to be entirely Windows centric.
[00:29:53.680 --> 00:29:55.440]   Microsoft is trying to be really, really good
[00:29:55.440 --> 00:29:58.960]   about letting you use all the devices you actually want to use
[00:29:58.960 --> 00:30:00.320]   and helping to stick them all together.
[00:30:00.320 --> 00:30:01.720]   - It's really the new Microsoft.
[00:30:01.720 --> 00:30:03.240]   - Yeah.
[00:30:03.240 --> 00:30:05.280]   - To use this new feature though,
[00:30:05.280 --> 00:30:07.200]   do you have to use the Microsoft launcher
[00:30:07.200 --> 00:30:08.680]   on an Android phone for that to work?
[00:30:08.680 --> 00:30:09.520]   - Oh God, I hope so.
[00:30:09.520 --> 00:30:12.400]   - I was intrigued when I heard about the ability
[00:30:12.400 --> 00:30:14.440]   to connect your phone to Windows 10
[00:30:14.440 --> 00:30:17.400]   with the latest update, the 1803 update.
[00:30:17.400 --> 00:30:19.440]   But then I started looking at it and realized,
[00:30:19.440 --> 00:30:21.800]   oh, I've got to use the Microsoft launcher
[00:30:21.800 --> 00:30:22.640]   and I don't want to do that.
[00:30:22.640 --> 00:30:24.880]   I like the Stock Pixel launcher.
[00:30:24.880 --> 00:30:26.880]   And so I haven't bothered.
[00:30:26.880 --> 00:30:29.240]   We'll find out.
[00:30:29.240 --> 00:30:31.840]   - Actually, we've got three, two and a half quarters to go
[00:30:31.840 --> 00:30:32.680]   till we find out.
[00:30:32.680 --> 00:30:33.800]   - Here's something nobody wants.
[00:30:33.800 --> 00:30:37.480]   Cortana, Cortana and Alexa will be working together.
[00:30:37.480 --> 00:30:38.320]   - I mean--
[00:30:38.320 --> 00:30:39.160]   - You're gonna be a Microsoft?
[00:30:39.160 --> 00:30:40.720]   I mean, they can be friends.
[00:30:40.720 --> 00:30:42.120]   - Does somebody want that?
[00:30:42.120 --> 00:30:43.200]   - I mean, eventually.
[00:30:43.200 --> 00:30:45.800]   - How do you get to Alexa by, echoed by saying,
[00:30:45.800 --> 00:30:46.640]   what do you, you have to say,
[00:30:46.640 --> 00:30:49.600]   hey Cortana, I want to talk to Amazon or--
[00:30:49.600 --> 00:30:51.360]   - Cortana, open Alexa.
[00:30:51.360 --> 00:30:52.520]   - Oh, that's crazy.
[00:30:52.520 --> 00:30:54.160]   Or Alexa, open Cortana.
[00:30:54.160 --> 00:30:55.920]   - That feels mean to Cortana.
[00:30:55.920 --> 00:30:58.120]   - Actually, you're right, it does.
[00:30:58.120 --> 00:31:01.280]   - You have to ask Assistant one to talk to Assistant two.
[00:31:01.280 --> 00:31:03.240]   - Yeah, it's like, well, anyway,
[00:31:03.240 --> 00:31:04.480]   I'm not gonna agree with that to anything.
[00:31:04.480 --> 00:31:06.120]   - It would be nice if you could just,
[00:31:06.120 --> 00:31:08.720]   even if you're sitting at PCs, say, hey, Alexa,
[00:31:08.720 --> 00:31:11.640]   and have, but you can't, you have to say Cortana.
[00:31:11.640 --> 00:31:12.480]   - Yeah.
[00:31:12.480 --> 00:31:14.400]   - I want to talk to Alexa now, go away.
[00:31:14.400 --> 00:31:16.400]   - Go away now, I don't mean you.
[00:31:16.400 --> 00:31:18.040]   - But once they're there, they work.
[00:31:18.040 --> 00:31:21.200]   - I want to, no, is this another example,
[00:31:21.200 --> 00:31:23.840]   Harry, of Microsoft reaching across the aisle?
[00:31:23.840 --> 00:31:26.720]   Or I mean, why do they, I mean, in this case,
[00:31:26.720 --> 00:31:29.080]   it's very much in Microsoft's interest
[00:31:29.080 --> 00:31:32.360]   just because there aren't all that many people
[00:31:32.360 --> 00:31:34.640]   who lead Cortana-centric lives,
[00:31:34.640 --> 00:31:37.480]   and Alexa has such a huge lead in this stuff
[00:31:37.480 --> 00:31:40.260]   that Microsoft has nothing to lose,
[00:31:40.260 --> 00:31:42.680]   and probably Amazon is not terrified by the idea
[00:31:42.680 --> 00:31:44.640]   of Cortana overtaking Alexa.
[00:31:44.640 --> 00:31:45.480]   - Oh, no.
[00:31:45.480 --> 00:31:46.320]   - So why not do it?
[00:31:46.320 --> 00:31:47.600]   - More buying power that way.
[00:31:47.600 --> 00:31:49.560]   Now you can buy stuff through your Windows PC.
[00:31:49.560 --> 00:31:52.480]   - Yeah, as long as Alexa is the ultimate endpoint,
[00:31:52.480 --> 00:31:53.800]   then they don't care.
[00:31:53.800 --> 00:31:55.320]   - Obviously, I would love to see Alexa
[00:31:55.320 --> 00:31:58.760]   and Google Assistant and Siri all offer similar functionality
[00:31:58.760 --> 00:32:02.560]   with each other, which seems really unlikely.
[00:32:02.560 --> 00:32:03.400]   - Yeah.
[00:32:03.400 --> 00:32:04.240]   - I'm glad this has happened,
[00:32:04.240 --> 00:32:05.440]   even though I don't use Cortana very much.
[00:32:05.440 --> 00:32:07.360]   - I've always said that really what you want,
[00:32:07.360 --> 00:32:09.640]   you want access to all of the capabilities
[00:32:09.640 --> 00:32:10.960]   of all the assistants.
[00:32:10.960 --> 00:32:14.280]   So what you really want is a kind of an ombudsman assistant,
[00:32:14.280 --> 00:32:16.440]   a mediator that you deal with,
[00:32:16.440 --> 00:32:18.600]   that's your assistant that knows,
[00:32:18.600 --> 00:32:20.640]   oh, for that, I need to get Cortana.
[00:32:20.640 --> 00:32:23.120]   - That understands the syntax of every command.
[00:32:23.120 --> 00:32:24.880]   - Yeah, and then you just let it deal with it.
[00:32:24.880 --> 00:32:27.120]   I mean, surely somebody could do that.
[00:32:27.120 --> 00:32:30.480]   Maybe it'll be Microsoft given this new attitude of,
[00:32:30.480 --> 00:32:31.680]   really if you think about it,
[00:32:31.680 --> 00:32:34.280]   Google's not gonna do it, Apple's not gonna do it,
[00:32:34.280 --> 00:32:35.520]   Amazon's not gonna do it,
[00:32:35.520 --> 00:32:37.880]   Microsoft would be the logical company to do it
[00:32:37.880 --> 00:32:41.040]   because first of all, they're losing the race.
[00:32:41.040 --> 00:32:43.480]   - They don't have anything to lose.
[00:32:43.480 --> 00:32:44.320]   - Nothing to lose.
[00:32:44.320 --> 00:32:46.920]   - They don't have a vested interest in a platform,
[00:32:46.920 --> 00:32:48.960]   a hardware platform. - Right, they don't want lock in.
[00:32:48.960 --> 00:32:50.560]   They want lock in the Windows, I guess,
[00:32:50.560 --> 00:32:51.800]   but I don't think they even care about that anymore.
[00:32:51.800 --> 00:32:53.120]   - I don't think they do either.
[00:32:53.120 --> 00:32:53.960]   - I don't think they care about that.
[00:32:53.960 --> 00:32:55.560]   - I think they say that Windows was only mentioned
[00:32:55.560 --> 00:32:58.720]   12 times in that three hour 45 minute keynote.
[00:32:58.720 --> 00:32:59.560]   - Wow.
[00:32:59.560 --> 00:33:01.000]   - About you know, many hundreds of times
[00:33:01.000 --> 00:33:02.040]   Windows was announced.
[00:33:02.040 --> 00:33:03.440]   - I bet Azure was announced.
[00:33:03.440 --> 00:33:04.280]   - I bet Azure was announced.
[00:33:04.280 --> 00:33:05.280]   - You mean how many times Ballmer mentioned it?
[00:33:05.280 --> 00:33:08.200]   - Microsoft 365. - I think you mentioned it
[00:33:08.200 --> 00:33:09.920]   that many times in one minute.
[00:33:09.920 --> 00:33:10.760]   - Yes.
[00:33:10.760 --> 00:33:12.000]   - Windows, Windows, Windows.
[00:33:12.000 --> 00:33:15.480]   - They're so over everything and rotating around Windows.
[00:33:15.480 --> 00:33:19.400]   - Windows is the partner tech to everything now.
[00:33:19.400 --> 00:33:21.640]   - Is that though also kind of the sign of the times?
[00:33:21.640 --> 00:33:25.600]   - Even Apple is making more money now in services
[00:33:25.600 --> 00:33:28.360]   than they do on Macintosh plus iPad together.
[00:33:28.360 --> 00:33:30.440]   - They get more revenue from services.
[00:33:30.440 --> 00:33:33.240]   - They're not actually making much profit on it
[00:33:33.240 --> 00:33:34.760]   because the number of-- - Oh wait a minute.
[00:33:34.760 --> 00:33:37.800]   - They've got to be because they get 30% of every app sold.
[00:33:37.800 --> 00:33:39.400]   That's pure profit.
[00:33:39.400 --> 00:33:42.880]   - Yeah, well, especially on the, I think,
[00:33:42.880 --> 00:33:45.320]   a big chunk of that growing services revenue
[00:33:45.320 --> 00:33:46.560]   is from Apple Music.
[00:33:46.560 --> 00:33:48.120]   - That would make no money on it for it to be.
[00:33:48.120 --> 00:33:49.720]   - Or that they're probably-- - All the back of the sides
[00:33:49.720 --> 00:33:51.600]   are way by the way. - They're probably single digits
[00:33:51.600 --> 00:33:53.120]   at best. - Right, I cloud they probably
[00:33:53.120 --> 00:33:55.200]   make money. - Yeah, but all the other stuff.
[00:33:55.200 --> 00:33:56.640]   - Apple pay they make money on it.
[00:33:56.640 --> 00:33:59.000]   - Yeah, so they've got a lot of revenue,
[00:33:59.000 --> 00:34:02.240]   but the part that's actually profit from that
[00:34:02.240 --> 00:34:06.600]   is probably, the total is probably no more than 10 or 15%.
[00:34:06.600 --> 00:34:08.600]   - And Apple's used to very high marginal hardware.
[00:34:08.600 --> 00:34:11.160]   They love that, right?
[00:34:11.160 --> 00:34:13.760]   - Yeah, so I actually just exchanged a couple emails
[00:34:13.760 --> 00:34:15.680]   with Jason Snell the other day I was asking about that
[00:34:15.680 --> 00:34:19.840]   'cause he covers the Apple financial calls,
[00:34:19.840 --> 00:34:20.680]   the quarter of the financial calls.
[00:34:20.680 --> 00:34:21.680]   - Oh, he does those beautiful graphs.
[00:34:21.680 --> 00:34:23.960]   - Yeah, and I asked him about that.
[00:34:23.960 --> 00:34:27.440]   If he knew the number that they quote for the services,
[00:34:27.440 --> 00:34:30.000]   that's the total revenue that they bring in.
[00:34:30.000 --> 00:34:33.160]   But it's not their cut of it.
[00:34:33.160 --> 00:34:36.600]   It also includes the 70% that they pay to developers
[00:34:36.600 --> 00:34:41.520]   or 85% in some cases if it's the subscriptions
[00:34:41.520 --> 00:34:43.360]   after the first year, things like that.
[00:34:43.360 --> 00:34:46.240]   - So, because-- - Did Apple, did they reveal
[00:34:46.240 --> 00:34:47.280]   how much of that was profit?
[00:34:47.280 --> 00:34:50.520]   - No, no, they never break out where the profits
[00:34:50.520 --> 00:34:51.840]   are coming from, they just tell you,
[00:34:51.840 --> 00:34:54.720]   this is our total profit, this is our total revenue
[00:34:54.720 --> 00:34:56.600]   and the revenue from some of the sectors.
[00:34:56.600 --> 00:34:59.160]   - I admit that they're probably not getting
[00:34:59.160 --> 00:35:00.800]   iPhone level profits out of it,
[00:35:00.800 --> 00:35:03.000]   but they've got to be getting,
[00:35:03.000 --> 00:35:04.840]   especially Apple pay in the apps store.
[00:35:04.840 --> 00:35:06.640]   - If you grow enough, pretty profitable.
[00:35:06.640 --> 00:35:10.440]   - Even 10% of a huge number is still a hell lot of dollars.
[00:35:10.440 --> 00:35:12.040]   - It was almost $9 billion.
[00:35:12.040 --> 00:35:13.880]   - So, yeah, I'll take it for a quarter,
[00:35:13.880 --> 00:35:16.400]   $300 million a month profit, that's okay.
[00:35:16.400 --> 00:35:18.480]   - Yeah. - I'll take that.
[00:35:18.480 --> 00:35:20.520]   - That's not what you make right now, but you know.
[00:35:20.520 --> 00:35:23.160]   - Yeah, well, you know, I could kick a pick up.
[00:35:23.160 --> 00:35:26.880]   Timeline for iOS and Android,
[00:35:26.880 --> 00:35:29.080]   timeline's part of the new 1803,
[00:35:29.080 --> 00:35:31.640]   the new spring update for Windows.
[00:35:31.640 --> 00:35:33.400]   I haven't really, I've played with it,
[00:35:33.400 --> 00:35:34.720]   but I don't really grok it yet.
[00:35:34.720 --> 00:35:36.920]   I guess the idea is it keeps track of what you've done,
[00:35:36.920 --> 00:35:38.200]   what apps you've-- - It's a history
[00:35:38.200 --> 00:35:41.920]   of your activities and it's coming to iPhones and Android phones.
[00:35:41.920 --> 00:35:44.520]   - So, you'll tell you what apps you've used?
[00:35:44.520 --> 00:35:47.160]   - I think that's a theory, although again, on iOS,
[00:35:47.160 --> 00:35:49.720]   I am fuzzy about how Microsoft can do anything,
[00:35:49.720 --> 00:35:51.360]   even sort of similar to that.
[00:35:51.360 --> 00:35:54.160]   - Oh, I see right here in Verge article says on iOS,
[00:35:54.160 --> 00:35:56.840]   it'll be a tab within the Edge browser.
[00:35:56.840 --> 00:35:59.160]   - Oh, great. - So, it's your Edge history.
[00:35:59.160 --> 00:36:00.000]   - Yeah, yeah.
[00:36:00.000 --> 00:36:01.400]   So, good news.
[00:36:01.400 --> 00:36:03.400]   Does anybody use Edge on iPhone?
[00:36:03.400 --> 00:36:06.160]   - Only when Chrome is a working-- - You have to be nuts.
[00:36:06.160 --> 00:36:07.280]   Well, I don't even use Chrome.
[00:36:07.280 --> 00:36:10.360]   You can't even, everything links to Safari, no matter what.
[00:36:10.360 --> 00:36:12.560]   - Everything's using the webkit route, the--
[00:36:12.560 --> 00:36:15.120]   - And it is all just different skins on WebKit.
[00:36:15.120 --> 00:36:17.840]   On Android, Timeline will be part of the Microsoft launcher,
[00:36:17.840 --> 00:36:19.880]   so you won't be able to use your nice pixel launcher,
[00:36:19.880 --> 00:36:21.040]   I'm sorry.
[00:36:21.040 --> 00:36:22.840]   - It's all I can live without Timeline.
[00:36:22.840 --> 00:36:25.080]   - Microsoft also told devs,
[00:36:25.080 --> 00:36:27.960]   they're gonna give more money to them.
[00:36:27.960 --> 00:36:30.080]   And the store, we're talking just now about,
[00:36:30.080 --> 00:36:32.840]   Apple makes 30% on sales on the App Store.
[00:36:32.840 --> 00:36:35.520]   Microsoft says, "We're only gonna take 5%.
[00:36:35.520 --> 00:36:37.840]   "Will it work?
[00:36:39.320 --> 00:36:40.960]   "5% of nothing."
[00:36:40.960 --> 00:36:42.040]   (laughs)
[00:36:42.040 --> 00:36:42.920]   - He's nothing.
[00:36:42.920 --> 00:36:47.200]   - But 60% of-- - Well, I'll be curious to see
[00:36:47.200 --> 00:36:49.960]   what percentage of progressive web apps
[00:36:49.960 --> 00:36:51.360]   are actually paid for apps.
[00:36:51.360 --> 00:36:53.840]   - Do they, they didn't talk much of the keynote about PWA?
[00:36:53.840 --> 00:36:56.080]   - I'm sure I was struck by that.
[00:36:56.080 --> 00:36:59.680]   I did not go to build, but I did do a story about Starbucks,
[00:36:59.680 --> 00:37:01.040]   working with Microsoft.
[00:37:01.040 --> 00:37:03.440]   - For the Starbucks app? - Because part of that,
[00:37:03.440 --> 00:37:06.560]   like Starbucks is using 50 different Azure services.
[00:37:06.560 --> 00:37:10.240]   And I did my story because Starbucks had a very high presence
[00:37:10.240 --> 00:37:13.280]   at Build, and one of the sessions that they led
[00:37:13.280 --> 00:37:17.480]   was one on how Starbucks got rid of its Windows app
[00:37:17.480 --> 00:37:19.160]   and built a progressive web app.
[00:37:19.160 --> 00:37:21.240]   And I was like, this really is another sign
[00:37:21.240 --> 00:37:23.440]   of the new Microsoft that at Build,
[00:37:23.440 --> 00:37:25.680]   they're positioning it as a good thing
[00:37:25.680 --> 00:37:27.560]   to ditch your Windows native app
[00:37:27.560 --> 00:37:28.760]   and do a progressive web apps.
[00:37:28.760 --> 00:37:29.960]   - That's what you do when you're in trouble,
[00:37:29.960 --> 00:37:33.520]   but Microsoft, are they, I mean, more people run Windows
[00:37:33.520 --> 00:37:35.920]   than any other operating system by a long margin.
[00:37:35.920 --> 00:37:39.360]   - Yeah, but on computers, on laptops and desktops,
[00:37:39.360 --> 00:37:41.320]   but not probably not overall anymore.
[00:37:41.320 --> 00:37:42.360]   It's probably-- - It has the phones
[00:37:42.360 --> 00:37:44.080]   and tablets and Chromebooks.
[00:37:44.080 --> 00:37:46.720]   - Android and iOS, probably number--
[00:37:46.720 --> 00:37:48.080]   - Oh, come on, Windows, is this one?
[00:37:48.080 --> 00:37:49.920]   - There's, what is it, a billion Windows users?
[00:37:49.920 --> 00:37:51.200]   It's a huge number of Windows users.
[00:37:51.200 --> 00:37:52.840]   - Yeah, two billion Android devices,
[00:37:52.840 --> 00:37:53.680]   a billion or iOS devices. - Yeah, exactly.
[00:37:53.680 --> 00:37:55.480]   - A billion Android devices alone.
[00:37:55.480 --> 00:37:57.360]   - Progressive web apps were also,
[00:37:57.360 --> 00:37:59.360]   there's quite a bit on them at I/O.
[00:37:59.360 --> 00:38:00.520]   - Yeah, that's a big Google question.
[00:38:00.520 --> 00:38:03.280]   - I was not entirely sure whether PWA's were going anywhere,
[00:38:03.280 --> 00:38:04.840]   but after those two keynotes,
[00:38:04.840 --> 00:38:06.240]   and they did want to take another look at it.
[00:38:06.240 --> 00:38:08.880]   - Well, I think, you know, for our point of view,
[00:38:08.880 --> 00:38:12.960]   we spent a lot of money and effort making a website,
[00:38:12.960 --> 00:38:14.600]   but we don't have an app.
[00:38:14.600 --> 00:38:16.480]   We actually spent a lot of money making an app
[00:38:16.480 --> 00:38:17.720]   that turned out to be crap,
[00:38:17.720 --> 00:38:18.880]   and I didn't want to release it,
[00:38:18.880 --> 00:38:22.400]   but I think the next thing for us to consider
[00:38:22.400 --> 00:38:23.680]   if we wanted a Twitter app would be
[00:38:23.680 --> 00:38:25.200]   make a progressive web app make a website.
[00:38:25.200 --> 00:38:26.560]   - And dot quit dot com.
[00:38:26.560 --> 00:38:28.240]   - Yeah, you go to the website,
[00:38:28.240 --> 00:38:29.320]   it downloads onto your phone,
[00:38:29.320 --> 00:38:31.000]   there's service workers on your phone,
[00:38:31.000 --> 00:38:33.880]   they make sure that it works, it's a podcast app.
[00:38:33.880 --> 00:38:34.720]   So even if you're not online,
[00:38:34.720 --> 00:38:35.560]   you can still listen. - Well, if you're not online,
[00:38:35.560 --> 00:38:36.560]   you can still listen. - The new Twitter, PWA, you know,
[00:38:36.560 --> 00:38:40.400]   is vastly better than the old Windows Twitter app.
[00:38:40.400 --> 00:38:41.920]   - Is that m.Twitter.com?
[00:38:41.920 --> 00:38:42.880]   - Yep, that's all I use.
[00:38:42.880 --> 00:38:43.880]   - That's what you use?
[00:38:43.880 --> 00:38:45.120]   - 'Cause I get notifications,
[00:38:45.120 --> 00:38:47.040]   and it keeps me from going on Twitter when I shouldn't be.
[00:38:47.040 --> 00:38:49.080]   - Oh, you use it because it's limited.
[00:38:49.080 --> 00:38:49.920]   - Yes.
[00:38:49.920 --> 00:38:51.520]   - No, no, no, no, no, no, no, no, no, no, no, no, no,
[00:38:51.520 --> 00:38:54.080]   I use it because it doesn't put the app on my phone.
[00:38:54.080 --> 00:38:55.240]   Yes, I use it because it's limited.
[00:38:55.240 --> 00:38:56.080]   - Yeah.
[00:38:56.080 --> 00:38:56.920]   - That's true. - That's right.
[00:38:56.920 --> 00:38:57.760]   That's why you use it.
[00:38:57.760 --> 00:39:04.760]   - It's good for me. - But in theory, you can try on the OWA,
[00:39:04.760 --> 00:39:08.160]   progressive web apps would be as capable as a desktop app,
[00:39:08.160 --> 00:39:10.760]   or no, or native app, I should say.
[00:39:10.760 --> 00:39:12.760]   - I'm sure there's a list of things they can't do
[00:39:12.760 --> 00:39:15.760]   because they can't get access to hardware on the same level
[00:39:15.760 --> 00:39:17.760]   that a native app can.
[00:39:17.760 --> 00:39:19.760]   - It's actually up to the operating system maker, isn't it?
[00:39:19.760 --> 00:39:21.760]   So Google and Microsoft support it.
[00:39:21.760 --> 00:39:24.760]   Apple is, I think, ambivalent 'cause they still can work on,
[00:39:24.760 --> 00:39:26.760]   you know, get you locked into the--
[00:39:26.760 --> 00:39:28.760]   The worm has turned, as they say.
[00:39:28.760 --> 00:39:30.760]   There's no worm in this apple.
[00:39:30.760 --> 00:39:34.760]   The biggest surprise that the build was that the connect is back.
[00:39:34.760 --> 00:39:35.760]   (laughs)
[00:39:35.760 --> 00:39:37.760]   - Yes, yes.
[00:39:37.760 --> 00:39:39.760]   - The connect, which was, for one day,
[00:39:39.760 --> 00:39:43.760]   the hottest selling consumer electronics product in the history
[00:39:43.760 --> 00:39:46.760]   of the world when it first came out and then immediately tanked.
[00:39:46.760 --> 00:39:50.760]   I don't know how you can go from being literally the most successful
[00:39:50.760 --> 00:39:53.760]   consumer electronics product ever to nobody wants it.
[00:39:53.760 --> 00:39:56.760]   I still have my connect to my Xbox.
[00:39:56.760 --> 00:40:00.760]   - I mean, I think part of it was that developers did not really take advantage of it.
[00:40:00.760 --> 00:40:02.760]   - Again, you got to have developers.
[00:40:02.760 --> 00:40:04.760]   - And then when the Xbox One came out,
[00:40:04.760 --> 00:40:06.760]   there was an even better piece of connect hardware,
[00:40:06.760 --> 00:40:10.760]   and it was part of this vision of turning the Xbox One into more
[00:40:10.760 --> 00:40:13.760]   of a general purpose device, which Microsoft quickly gave up on.
[00:40:13.760 --> 00:40:14.760]   - Right.
[00:40:14.760 --> 00:40:16.760]   - They went back to being a meat and potatoes gaming machine.
[00:40:16.760 --> 00:40:18.760]   - And then discontinue the connect.
[00:40:18.760 --> 00:40:20.760]   - And originally, you had to get the connect with your Xbox One,
[00:40:20.760 --> 00:40:22.760]   and they stopped that quickly.
[00:40:22.760 --> 00:40:23.760]   - Yeah.
[00:40:23.760 --> 00:40:25.760]   - So I think the hardware was always amazing,
[00:40:25.760 --> 00:40:27.760]   but it never quite developed into the ecosystem.
[00:40:27.760 --> 00:40:28.760]   It needed to matter.
[00:40:28.760 --> 00:40:32.760]   - So they're going to do like an Azure connect,
[00:40:32.760 --> 00:40:34.760]   connect in the cloud.
[00:40:34.760 --> 00:40:35.760]   - How's that going to work?
[00:40:35.760 --> 00:40:38.760]   - You still need hardware, right?
[00:40:38.760 --> 00:40:40.760]   I mean, you still need the depth sensing camera and all that.
[00:40:40.760 --> 00:40:45.760]   Although I gather that maybe these Windows Hello cameras are connect-like, right?
[00:40:45.760 --> 00:40:49.760]   So maybe some of the PCs that have Windows Hello capable cameras
[00:40:49.760 --> 00:40:51.760]   will be able to do it.
[00:40:51.760 --> 00:40:55.760]   So then developers will be able to send information up to the cloud.
[00:40:55.760 --> 00:40:57.760]   Microsoft processes the information.
[00:40:57.760 --> 00:40:59.760]   It's a cloud service.
[00:40:59.760 --> 00:41:02.760]   It's not even connect probably, right?
[00:41:02.760 --> 00:41:04.760]   It's just a cloud service.
[00:41:04.760 --> 00:41:05.760]   All right.
[00:41:05.760 --> 00:41:06.760]   That's it.
[00:41:06.760 --> 00:41:07.760]   That's Microsoft.
[00:41:07.760 --> 00:41:08.760]   Oh, one more thing.
[00:41:08.760 --> 00:41:12.760]   Satya Nadella spent a significant amount of time at the beginning of keynote talking about ethics.
[00:41:12.760 --> 00:41:13.760]   - Mm.
[00:41:13.760 --> 00:41:15.760]   - I thought that was interesting.
[00:41:15.760 --> 00:41:20.760]   I think there's a move a foot, and it's not just because of Facebook.
[00:41:20.760 --> 00:41:28.760]   I think a lot of people understand that for a while software developers have been weaponizing
[00:41:28.760 --> 00:41:31.760]   their software to become more and more addictive.
[00:41:31.760 --> 00:41:36.760]   And so it's related to the Apple shareholders who went to Apple and said,
[00:41:36.760 --> 00:41:38.760]   "You've got to make your iPhone less compelling.
[00:41:38.760 --> 00:41:41.760]   Your problem, where you can't put it the thing down.
[00:41:41.760 --> 00:41:45.760]   People who play World of Warcraft to the point where they can't stand up anymore.
[00:41:45.760 --> 00:41:50.760]   People who play Candy Crush Saga while their kids are starving in the back seat."
[00:41:50.760 --> 00:41:55.760]   All of this is created a world where people are starting to blame technologists and technology.
[00:41:55.760 --> 00:42:01.760]   And so I think that's what it's a response to from Microsoft, but I think other companies are also doing this.
[00:42:01.760 --> 00:42:07.760]   Microsoft was really in front of this because at last year's build, Nadella set a lot of the same things.
[00:42:07.760 --> 00:42:08.760]   - Oh, really?
[00:42:08.760 --> 00:42:12.760]   - He began last year's keynote by flashing up the covers of 1984 and Brave New World.
[00:42:12.760 --> 00:42:13.760]   - Interesting.
[00:42:13.760 --> 00:42:14.760]   - And talking about--
[00:42:14.760 --> 00:42:14.760]   - Wow.
[00:42:14.760 --> 00:42:15.760]   - We don't want to make that.
[00:42:15.760 --> 00:42:16.760]   - We don't want to make that.
[00:42:16.760 --> 00:42:17.760]   - Yeah.
[00:42:17.760 --> 00:42:24.760]   - So I think that's really smart because Facebook acknowledged some of this stuff after it blew up in their face.
[00:42:24.760 --> 00:42:25.760]   - It's too late.
[00:42:25.760 --> 00:42:29.760]   - And Microsoft is confronting some of it before they have to.
[00:42:29.760 --> 00:42:30.760]   - Yeah.
[00:42:30.760 --> 00:42:33.760]   I think you make an excellent point.
[00:42:33.760 --> 00:42:39.760]   We interviewed Scott Galloway who wrote the book about what is it called, the Big Four Carson?
[00:42:39.760 --> 00:42:46.760]   The Four Amazon, Facebook, Google, and I'm not sure who the fourth was.
[00:42:46.760 --> 00:42:47.760]   - Apple.
[00:42:47.760 --> 00:42:48.760]   - Apple, maybe.
[00:42:48.760 --> 00:42:52.760]   But about how these companies have to-- those are the fangs.
[00:42:52.760 --> 00:42:58.760]   These guys have to start to pay attention because at some point there's going to be so much heat on the government's going to step in.
[00:42:58.760 --> 00:42:59.760]   And that could be a big problem.
[00:42:59.760 --> 00:43:00.760]   Actually, that may have already happened.
[00:43:00.760 --> 00:43:01.760]   - Not great.
[00:43:01.760 --> 00:43:02.760]   - NPR, yes.
[00:43:02.760 --> 00:43:03.760]   - Yeah.
[00:43:03.760 --> 00:43:04.760]   We'll talk about GDPR later on.
[00:43:04.760 --> 00:43:06.760]   That's coming this week.
[00:43:06.760 --> 00:43:12.760]   - I actually just sent in my next column for automotive engineering which was about ethics around autonomous vehicles.
[00:43:12.760 --> 00:43:13.760]   - Really?
[00:43:13.760 --> 00:43:14.760]   - Yeah, because--
[00:43:14.760 --> 00:43:15.760]   - Same thing, right?
[00:43:15.760 --> 00:43:24.760]   - Yeah, well, a lot of the conversation around autonomous vehicles in engineering circles in the last few years has been how you program ethics into the autonomous systems dealing with the trolley problem.
[00:43:24.760 --> 00:43:29.760]   But really, that's actually a pretty-- that's such a rare issue anyway.
[00:43:29.760 --> 00:43:31.760]   That's not the real ethics problem around these things.
[00:43:31.760 --> 00:43:33.760]   It's actually the people.
[00:43:33.760 --> 00:43:34.760]   - It's marketing.
[00:43:34.760 --> 00:43:37.760]   - It's Tesla telling its drivers.
[00:43:37.760 --> 00:43:38.760]   It's autopilot.
[00:43:38.760 --> 00:43:39.760]   - Yeah, I mean--
[00:43:39.760 --> 00:43:40.760]   - That's the ethical concern I had.
[00:43:40.760 --> 00:43:46.760]   - Not looking at the-- not trying to look at what are the potential unintended consequences of this technology.
[00:43:46.760 --> 00:43:48.760]   How can this stuff be misused?
[00:43:48.760 --> 00:43:50.760]   - Ultimately, it's going to save lives.
[00:43:50.760 --> 00:43:51.760]   Self-driving people.
[00:43:51.760 --> 00:43:52.760]   - Probably.
[00:43:52.760 --> 00:43:54.760]   - Oh, no, I'm sure it will.
[00:43:54.760 --> 00:43:55.760]   Probably.
[00:43:55.760 --> 00:43:57.760]   You don't think so.
[00:43:57.760 --> 00:44:01.760]   - I'm confident that in the long run it will eventually probably be better.
[00:44:01.760 --> 00:44:02.760]   - Wow.
[00:44:02.760 --> 00:44:03.760]   You're really hedging it.
[00:44:03.760 --> 00:44:11.760]   We started that conversation because of the kind of, like you mentioned, that they only allow you to use the super crews on roads highways that they've already mapped.
[00:44:11.760 --> 00:44:12.760]   - Right.
[00:44:12.760 --> 00:44:13.760]   - Geofancy.
[00:44:13.760 --> 00:44:18.760]   - So even though it would probably work on other roads, they won't-- and Mercedes, in fact, lets you use it on other roads.
[00:44:18.760 --> 00:44:25.760]   So I think that these car companies have learned a lesson and they're being very judicious about rolling these features out.
[00:44:25.760 --> 00:44:26.760]   - Mm-hmm.
[00:44:26.760 --> 00:44:27.760]   - They have lives at stake.
[00:44:27.760 --> 00:44:28.760]   - Yeah, it's a little different.
[00:44:28.760 --> 00:44:32.760]   - Yeah, I mean, just something that somebody playing Candy Crush in the talk that gives.
[00:44:32.760 --> 00:44:38.760]   The threshold for what defines a minimum viable product is much higher for an autonomous vehicle than it is.
[00:44:38.760 --> 00:44:56.780]   - When an
[00:44:56.780 --> 00:45:03.780]   autonomous vehicle is kind of the poster child for corporate responsibility.
[00:45:03.780 --> 00:45:06.780]   Is that just marketing or do you think that's true?
[00:45:06.780 --> 00:45:07.780]   - It's marketing.
[00:45:07.780 --> 00:45:08.780]   - It's so marketing.
[00:45:08.780 --> 00:45:09.780]   - The host of all about it.
[00:45:09.780 --> 00:45:15.780]   - I think the fact that they promote it as much as they do is marketing in PR span.
[00:45:15.780 --> 00:45:16.780]   - Yeah.
[00:45:16.780 --> 00:45:19.780]   - But I do believe, I think that he really does believe that stuff.
[00:45:19.780 --> 00:45:20.780]   - I agree.
[00:45:20.780 --> 00:45:21.780]   - And I think so too.
[00:45:21.780 --> 00:45:28.780]   - I think that's generally more in that direction.
[00:45:28.780 --> 00:45:31.780]   I mean, I'm sure there's lots of people with an Apple that would be more than happy to do the kinds of things that Facebook or Google or other companies do.
[00:45:31.780 --> 00:45:39.780]   But I think overall the corporate culture is probably more along the lines of what you hear from Tim and other executives at Apple.
[00:45:39.780 --> 00:45:44.780]   - How much did they talk about augmented reality or what they call mixed reality?
[00:45:44.780 --> 00:45:46.780]   Do they talk about HoloLens at the keynote?
[00:45:46.780 --> 00:45:47.780]   Did they show it off?
[00:45:47.780 --> 00:45:52.780]   - Here's a picture of HoloLens being used.
[00:45:52.780 --> 00:45:57.780]   It looks like a car manufacturer, Sam.
[00:45:57.780 --> 00:45:59.780]   - Oh, that's fusion.
[00:45:59.780 --> 00:46:01.780]   It's a Ford concept car.
[00:46:01.780 --> 00:46:02.780]   - Ford's used these.
[00:46:02.780 --> 00:46:07.780]   I remember when we went to the Ford Labs years ago, they were using virtual reality with auto-makers.
[00:46:07.780 --> 00:46:16.780]   - Yeah, they used VR stuff in their design labs to let you do the kinds of, you know, like walk into the car and, you know, slice through the car and look at how all the parts fit together.
[00:46:16.780 --> 00:46:19.780]   That's been commonly used for a long time.
[00:46:19.780 --> 00:46:23.780]   But that's, you know, that's stuff that's much more expensive.
[00:46:23.780 --> 00:46:29.780]   I mean, you could pretty much do the same thing now with an HTC Vive or Oculus.
[00:46:29.780 --> 00:46:32.780]   - Two new apps for HoloLens coming May 22nd.
[00:46:32.780 --> 00:46:39.780]   One is called Remote Assist, which to me, this is one of the real uses of augmented reality.
[00:46:39.780 --> 00:46:44.780]   It allows somebody remotely to show you how to do something using the HoloLens.
[00:46:44.780 --> 00:46:51.780]   So you can see what you're, you know, the pipe you want to fix and then somebody's showing you how to use, you know, how to fix it.
[00:46:51.780 --> 00:46:52.780]   - Interesting.
[00:46:52.780 --> 00:46:53.780]   - Yeah.
[00:46:53.780 --> 00:46:58.780]   - This is literally the real- - This is the real- - This is the real-life implementation of Remote Desktop.
[00:46:58.780 --> 00:47:00.780]   - Look, it's all working.
[00:47:00.780 --> 00:47:06.780]   So here's a happy-go-lucky worker who has no idea how to fix whatever.
[00:47:06.780 --> 00:47:12.780]   So she's using the augmented reality to get help.
[00:47:12.780 --> 00:47:21.780]   And somebody who's sitting at us conveniently to Surface Book, you can't hear the audio because I'm having trouble with the audio.
[00:47:21.780 --> 00:47:26.780]   - Yeah, and this is the sort of thing that we heard talked about with Google Glass a few years ago.
[00:47:26.780 --> 00:47:27.780]   - Sure.
[00:47:27.780 --> 00:47:38.780]   - And it's actually, you know, these kinds of vertical enterprise applications are really the best place to be focusing on this stuff right now because of the cost and the limitations of this technology.
[00:47:38.780 --> 00:47:42.780]   You know, focus on those kinds of applications and really refine it.
[00:47:42.780 --> 00:47:48.780]   And then, you know, as you, as it gets better and more cost effective, then you can start rolling it out to consumers.
[00:47:48.780 --> 00:47:59.780]   - They're also doing something called layout, which basically duplicates what ARKit and ARCorps have been doing on iFun and Android for a while, which is put furniture in your room.
[00:47:59.780 --> 00:48:02.780]   But you could do it wearing the HoloLens instead.
[00:48:02.780 --> 00:48:03.780]   - Or I could do it with my Pixel.
[00:48:03.780 --> 00:48:04.780]   - Yeah.
[00:48:04.780 --> 00:48:05.780]   - Yeah.
[00:48:05.780 --> 00:48:06.780]   - I don't have to wear anything.
[00:48:06.780 --> 00:48:07.780]   - All right.
[00:48:07.780 --> 00:48:08.780]   - Enough Microsoft.
[00:48:08.780 --> 00:48:10.780]   Let's move on to the good stuff.
[00:48:10.780 --> 00:48:13.780]   - ARCorps stuff on the Pixel works really well.
[00:48:13.780 --> 00:48:14.780]   - Well, it's interesting.
[00:48:14.780 --> 00:48:16.780]   You know, obviously Google watched what Apple did with ARKit.
[00:48:16.780 --> 00:48:21.780]   Google had attempted a much more high-end product called Project Tango that required hardware.
[00:48:21.780 --> 00:48:25.780]   Nobody was willing to make a Project Tango phone except what there was.
[00:48:25.780 --> 00:48:26.780]   - Lenovo.
[00:48:26.780 --> 00:48:27.780]   - Lenovo.
[00:48:27.780 --> 00:48:28.780]   - And then...
[00:48:28.780 --> 00:48:29.780]   - It wasn't a very good phone.
[00:48:29.780 --> 00:48:33.780]   - No, and it was big and clunky and expensive because it had all these extra sensors.
[00:48:33.780 --> 00:48:41.780]   Meanwhile, Apple makes the iPhone X and puts all the AR stuff pointed at you, so it's useless,
[00:48:41.780 --> 00:48:49.780]   and puts out ARKit, which does adequately, you know, simply what AR needs to do for the
[00:48:49.780 --> 00:48:52.780]   simple consumer and gets all the attention.
[00:48:52.780 --> 00:48:57.780]   So Google said, "Well, we could do ARKit, basically, which is ARCorps, which uses none of Tango.
[00:48:57.780 --> 00:48:58.780]   It's all in software.
[00:48:58.780 --> 00:48:59.780]   It's not as good.
[00:48:59.780 --> 00:49:01.780]   You know, stuff isn't quite on the surface.
[00:49:01.780 --> 00:49:03.780]   And, you know, there's glitches."
[00:49:03.780 --> 00:49:05.780]   And then they use their regular camera and do it.
[00:49:05.780 --> 00:49:06.780]   - And everybody's here.
[00:49:06.780 --> 00:49:07.780]   - And other apps now.
[00:49:07.780 --> 00:49:09.780]   There's third-party apps that are using AR.
[00:49:09.780 --> 00:49:13.780]   - Yeah, Porsche actually has a pretty interesting ARCorps app that you can get.
[00:49:13.780 --> 00:49:14.780]   I don't know if it's on an iPhone.
[00:49:14.780 --> 00:49:21.780]   I think it probably is, but it's on Android to let you check out the new Mission E electric car.
[00:49:21.780 --> 00:49:26.780]   And so you can place the car in your room or on your driveway, and then you can actually
[00:49:26.780 --> 00:49:30.780]   drive it around and you can walk around it and you've got control of this.
[00:49:30.780 --> 00:49:32.780]   - You can really get in, open the door and sit down.
[00:49:32.780 --> 00:49:36.780]   - No, well, no, but you can drive it remotely so you can have it drive across in front of
[00:49:36.780 --> 00:49:38.780]   you and you can walk around it.
[00:49:38.780 --> 00:49:39.780]   It's pretty interesting.
[00:49:39.780 --> 00:49:40.780]   - I think W does that too.
[00:49:40.780 --> 00:49:45.780]   - I know that Sam, you don't walk anywhere.
[00:49:45.780 --> 00:49:50.780]   But if you did, and for those of us who walk, and I only walk when we're traveling pretty
[00:49:50.780 --> 00:49:51.780]   much.
[00:49:51.780 --> 00:49:52.780]   - I walk my dog every day.
[00:49:52.780 --> 00:49:53.780]   - Yeah.
[00:49:53.780 --> 00:49:54.780]   - Okay, well, you might, you get lost walking your dog?
[00:49:54.780 --> 00:49:55.780]   - No.
[00:49:55.780 --> 00:49:56.780]   - No.
[00:49:56.780 --> 00:49:57.780]   - So Google showed the use of augmented reality.
[00:49:57.780 --> 00:49:58.780]   - Yes.
[00:49:58.780 --> 00:49:59.780]   - This was really nice, wasn't it?
[00:49:59.780 --> 00:50:00.780]   - This is exciting.
[00:50:00.780 --> 00:50:02.780]   - Actually, when I'm traveling, that would be really cool.
[00:50:02.780 --> 00:50:03.780]   - Yes.
[00:50:03.780 --> 00:50:04.780]   - Well, but yeah, so it's for walking.
[00:50:04.780 --> 00:50:05.780]   It's not.
[00:50:05.780 --> 00:50:08.780]   - The whole ludad thing, I've had so many times when I walk out of a building, trying to figure
[00:50:08.780 --> 00:50:10.780]   out which way am I supposed to go?
[00:50:10.780 --> 00:50:11.780]   - Yes.
[00:50:11.780 --> 00:50:12.780]   - So I thought it was just me.
[00:50:12.780 --> 00:50:13.780]   - No.
[00:50:13.780 --> 00:50:14.780]   - No.
[00:50:14.780 --> 00:50:15.780]   - Everybody has it.
[00:50:15.780 --> 00:50:16.780]   - Everybody.
[00:50:16.780 --> 00:50:18.780]   - Especially like in urban areas where GPS is kind of flaky.
[00:50:18.780 --> 00:50:19.780]   - Urban.
[00:50:19.780 --> 00:50:20.780]   - Put me in the suburbs, I can't find it.
[00:50:20.780 --> 00:50:21.780]   - I'm always doing this.
[00:50:21.780 --> 00:50:25.780]   I'm always, I come out of a building and I'm doing the V-Cup with their compasses.
[00:50:25.780 --> 00:50:26.780]   - And is that it?
[00:50:26.780 --> 00:50:28.780]   That's why I'm always calibrating my compass.
[00:50:28.780 --> 00:50:30.780]   - I want to go up like two blocks in the wrong direction.
[00:50:30.780 --> 00:50:32.780]   - In the wrong direction, this is pointing.
[00:50:32.780 --> 00:50:34.780]   - And I even think the dot is following me.
[00:50:34.780 --> 00:50:35.780]   - Yeah.
[00:50:35.780 --> 00:50:37.260]   I thought it was just me.
[00:50:37.260 --> 00:50:38.260]   - No, it's ever said.
[00:50:38.260 --> 00:50:39.260]   - Oh, I'm relieved.
[00:50:39.260 --> 00:50:43.380]   Anyway, Google apparently thinks this is a problem too because they've fixed it or will
[00:50:43.380 --> 00:50:44.380]   be fixed.
[00:50:44.380 --> 00:50:48.020]   I think this, I just read that this is, some people are getting this already on their phones.
[00:50:48.020 --> 00:50:49.460]   I don't know if this is true.
[00:50:49.460 --> 00:50:54.260]   But the idea is the, uses the camera of the phone when you're getting walking directions
[00:50:54.260 --> 00:50:56.380]   to pop up the street view.
[00:50:56.380 --> 00:50:58.740]   Go back to the previous slide, Karsten.
[00:50:58.740 --> 00:51:00.340]   - It pops up the street view.
[00:51:00.340 --> 00:51:04.580]   Super imposes on the picture of the street where you ought to go.
[00:51:04.580 --> 00:51:06.420]   So there's never any ambiguity.
[00:51:06.420 --> 00:51:08.020]   Oh, you turn right here.
[00:51:08.020 --> 00:51:09.220]   I get it.
[00:51:09.220 --> 00:51:10.820]   And the map is still at the bottom of the screen.
[00:51:10.820 --> 00:51:11.820]   I think this is just a question.
[00:51:11.820 --> 00:51:13.180]   - Did you see the little character too?
[00:51:13.180 --> 00:51:15.660]   You can have little character that can help you around.
[00:51:15.660 --> 00:51:16.660]   - You can chase the fox.
[00:51:16.660 --> 00:51:17.660]   - A little fox.
[00:51:17.660 --> 00:51:19.300]   - I would love it for, you know, Pokemon or like a, you know.
[00:51:19.300 --> 00:51:20.300]   - I love that idea.
[00:51:20.300 --> 00:51:21.300]   - Yes.
[00:51:21.300 --> 00:51:22.300]   You know, branded.
[00:51:22.300 --> 00:51:23.300]   I'm okay with branding here.
[00:51:23.300 --> 00:51:26.980]   If you want to bring it to me, I'd love it.
[00:51:26.980 --> 00:51:29.140]   - We could have, I would like to follow you around flow.
[00:51:29.140 --> 00:51:32.900]   You should just, you know, just see flow disappearing around corners.
[00:51:32.900 --> 00:51:35.700]   Quick, there's actually a lot of gaming stuff you could do with this.
[00:51:35.700 --> 00:51:36.700]   This is a kind of a,
[00:51:36.700 --> 00:51:40.420]   - Well again, I think, you know, this, looking at this, it looks like another one of those
[00:51:40.420 --> 00:51:44.900]   things where they, they took a lesson from Google Glass.
[00:51:44.900 --> 00:51:46.940]   And now they've applied it in an actual useful way.
[00:51:46.940 --> 00:51:51.100]   Cause you know, Google Glass, you'd have one of the options and errors to use the maps
[00:51:51.100 --> 00:51:54.700]   and, you know, get the directions in the little display.
[00:51:54.700 --> 00:51:55.980]   Now you can just do it on your phone.
[00:51:55.980 --> 00:51:56.980]   - Yeah.
[00:51:56.980 --> 00:51:57.980]   The creepiest thing.
[00:51:57.980 --> 00:51:59.980]   - Ah, yes, the creepy thing.
[00:51:59.980 --> 00:52:00.980]   - And the thing.
[00:52:00.980 --> 00:52:05.340]   - So why don't, before, sorry, Leo, but I have to mention that, uh, Sundar did a little
[00:52:05.340 --> 00:52:08.340]   bit of the, the ethics talk at the beginning of the keynote as well.
[00:52:08.340 --> 00:52:09.340]   - Oh yeah, let's talk about that.
[00:52:09.340 --> 00:52:11.340]   - Which is why, how much did he do?
[00:52:11.340 --> 00:52:12.340]   - Like a few seconds.
[00:52:12.340 --> 00:52:13.340]   - Like a few sentences.
[00:52:13.340 --> 00:52:14.340]   - He did.
[00:52:14.340 --> 00:52:18.620]   He, he said they, he was reflective about the responsibilities of tech.
[00:52:18.620 --> 00:52:19.620]   That was it.
[00:52:19.620 --> 00:52:21.620]   - I am reflective about the responsibilities of tech.
[00:52:21.620 --> 00:52:23.620]   - I am reflective about the responsibilities of tech.
[00:52:23.620 --> 00:52:24.620]   - That's all.
[00:52:24.620 --> 00:52:25.620]   - I'm thinking about it.
[00:52:25.620 --> 00:52:29.100]   - We're not forced, but we can't just be wide eyed about the innovations that technology
[00:52:29.100 --> 00:52:30.100]   can't.
[00:52:30.100 --> 00:52:31.940]   - And then, so you mean he doesn't support the idea of a Google island?
[00:52:31.940 --> 00:52:33.940]   - Oh no, that was a Larry Page exclusive.
[00:52:33.940 --> 00:52:38.100]   - He said we can't just be wide eyed about the innovations that technology creates.
[00:52:38.100 --> 00:52:40.500]   - And then Google goes on.
[00:52:40.500 --> 00:52:41.500]   - Yes, yes.
[00:52:41.500 --> 00:52:45.420]   - Showing you how they're going to, they know everything about you, how they'll incorporate
[00:52:45.420 --> 00:52:46.500]   into every product.
[00:52:46.500 --> 00:52:51.060]   I mean, there was in the, they never used the word privacy as far as I could remember.
[00:52:51.060 --> 00:52:55.340]   There was never any hat tip to the idea that you might not want Google to know everything
[00:52:55.340 --> 00:52:56.340]   about you.
[00:52:56.340 --> 00:53:01.540]   And frankly, I don't because if I could have all of these, okay, this cool stuff, maybe
[00:53:01.540 --> 00:53:02.740]   I'll put up with it.
[00:53:02.740 --> 00:53:04.100]   He showed some new voices.
[00:53:04.100 --> 00:53:05.260]   They're already available on the--
[00:53:05.260 --> 00:53:08.380]   - Six different voices including John Legends, which I'm so--
[00:53:08.380 --> 00:53:11.380]   - I can't get John Legend, no I'm still trying to get John Legend over.
[00:53:11.380 --> 00:53:12.860]   - John Legend is coming soon.
[00:53:12.860 --> 00:53:13.860]   - Coming soon.
[00:53:13.860 --> 00:53:14.860]   - Well, he's--
[00:53:14.860 --> 00:53:16.700]   - So we talked about this before.
[00:53:16.700 --> 00:53:21.140]   This is, Google has a technique where they--
[00:53:21.140 --> 00:53:23.140]   Cinder pointed out, and I think this is true of Siri.
[00:53:23.140 --> 00:53:24.740]   I think it's true of all the voices since.
[00:53:24.740 --> 00:53:26.740]   They use real humans.
[00:53:26.740 --> 00:53:28.900]   They record a bunch of stuff.
[00:53:28.900 --> 00:53:31.660]   Sometimes they can use that recording, but most of the time the generated voices are
[00:53:31.660 --> 00:53:34.860]   just patterned on the human voice, but they've gotten better at that.
[00:53:34.860 --> 00:53:37.100]   That's called prosody.
[00:53:37.100 --> 00:53:42.100]   And it is taking a human voice and overlaying the phrasing, the style, the structure on
[00:53:42.100 --> 00:53:49.460]   top of the animated voice to give it a more human sounding technology, you know, texture.
[00:53:49.460 --> 00:53:50.780]   And that's what they did with John Legend.
[00:53:50.780 --> 00:53:52.340]   They had him in the studio.
[00:53:52.340 --> 00:53:55.940]   And then Pachai said, "Tell me what my calendar is."
[00:53:55.940 --> 00:53:57.340]   So obviously that's generated.
[00:53:57.340 --> 00:53:58.340]   I guess.
[00:53:58.340 --> 00:54:01.340]   I mean, they could fake the demo, but the presumption is generated.
[00:54:01.340 --> 00:54:02.820]   And it was in John Legend's voice.
[00:54:02.820 --> 00:54:07.500]   It sounded just like John Legend telling you what your appointments are.
[00:54:07.500 --> 00:54:10.060]   - And they can do this a lot faster now, thanks to machine learning.
[00:54:10.060 --> 00:54:14.980]   So they can whip these things out in weeks versus it used to take months to do this.
[00:54:14.980 --> 00:54:18.660]   So now they can do it in weeks, which means there could be new voices further on in the
[00:54:18.660 --> 00:54:21.660]   future, just maybe a dozen different voices that you can choose.
[00:54:21.660 --> 00:54:23.980]   - I still am holding out for Scarlett Johansson.
[00:54:23.980 --> 00:54:29.180]   - Oh, that would be some real fan service right there.
[00:54:29.180 --> 00:54:33.860]   - They did have put out at least in P, I don't know if it's available in Oreo yet,
[00:54:33.860 --> 00:54:36.900]   but they have the, they said six new voices.
[00:54:36.900 --> 00:54:38.540]   They have them one here.
[00:54:38.540 --> 00:54:40.100]   It's buried so deep I can never remember.
[00:54:40.100 --> 00:54:44.300]   I found it somewhere this morning and I changed my, well, I can do it.
[00:54:44.300 --> 00:54:50.660]   I could, my assistant voice is, "Is it Florence Ions birthday today?"
[00:54:50.660 --> 00:54:51.660]   - Let's see.
[00:54:51.660 --> 00:54:52.660]   - These are the top.
[00:54:52.660 --> 00:54:54.260]   - Oh, she's gonna turn around.
[00:54:54.260 --> 00:54:58.140]   - Here are the events you requested.
[00:54:58.140 --> 00:55:01.460]   First up, you have a calendar entry today at 3 p.m.
[00:55:01.460 --> 00:55:04.580]   The title is this week in TechShow.
[00:55:04.580 --> 00:55:05.580]   - That's a new voice.
[00:55:05.580 --> 00:55:06.580]   It still sounds like a robot though.
[00:55:06.580 --> 00:55:07.580]   - It does.
[00:55:07.580 --> 00:55:09.580]   - They have new mail and email and emails and awws.
[00:55:09.580 --> 00:55:10.580]   - That's a duplex.
[00:55:10.580 --> 00:55:11.580]   - That's a duplex.
[00:55:11.580 --> 00:55:17.620]   - Now do we know if duplex is, how close is this to being the real?
[00:55:17.620 --> 00:55:23.420]   - From what I understand, it's gonna roll out eventually so I'm assuming somebody this
[00:55:23.420 --> 00:55:24.420]   year.
[00:55:24.420 --> 00:55:28.860]   But what I do know is that this is going to be the limitations of it, so this is where
[00:55:28.860 --> 00:55:29.860]   it's starting.
[00:55:29.860 --> 00:55:33.620]   Or rather the limitation, I shouldn't say limitations, but this is where it's starting
[00:55:33.620 --> 00:55:34.620]   out of the gate.
[00:55:34.620 --> 00:55:39.980]   It's gonna start very simply with making reservations for you at a restaurant or booking an appointment
[00:55:39.980 --> 00:55:40.980]   for you.
[00:55:40.980 --> 00:55:46.140]   Wherever there's a, there isn't an online, if there's no online API that's available for
[00:55:46.140 --> 00:55:49.380]   the assistant to tap in, then that's when it does the phone call.
[00:55:49.380 --> 00:55:55.220]   - They were vague on stage about timing, but the show's, someone from Google told me
[00:55:55.220 --> 00:55:56.620]   they talked in terms of weeks.
[00:55:56.620 --> 00:55:57.620]   - What?
[00:55:57.620 --> 00:55:58.620]   - Sooner than a mile.
[00:55:58.620 --> 00:55:59.620]   - Oh they said weeks.
[00:55:59.620 --> 00:56:02.740]   - No they said they have thousands of the, I'll place a little bit of this so you can
[00:56:02.740 --> 00:56:03.740]   hear it.
[00:56:03.740 --> 00:56:04.940]   They have thousands of conversations.
[00:56:04.940 --> 00:56:08.900]   They've been using it primarily to get business hours because apparently that's a database
[00:56:08.900 --> 00:56:10.980]   that's not very good, they need to update that.
[00:56:10.980 --> 00:56:11.980]   I'll play you with a business.
[00:56:11.980 --> 00:56:14.260]   - Yes, they're scraping data with the robotic places.
[00:56:14.260 --> 00:56:17.700]   - And the business hours that they get, listen how complex some of the stuff is that they're
[00:56:17.700 --> 00:56:18.700]   interpreting.
[00:56:18.700 --> 00:56:19.940]   Oh I can't play sound.
[00:56:19.940 --> 00:56:27.140]   - So Tuesday through Thursday we're open 11 to 2 and then reopen 9.
[00:56:27.140 --> 00:56:28.140]   - That's a comment.
[00:56:28.140 --> 00:56:32.940]   - And then Friday, Saturday, Sunday we are, Friday, Saturday we're open 11 to 9 and then
[00:56:32.940 --> 00:56:34.500]   Sunday we're open 1 to 9.
[00:56:34.500 --> 00:56:35.500]   - Thank you very much human.
[00:56:35.500 --> 00:56:38.540]   - If you can have the bot make that call 1 to 9.
[00:56:38.540 --> 00:56:42.220]   - Figure that out and then record that and parse it.
[00:56:42.220 --> 00:56:46.780]   And then they can put that into their database so when you look places up in Google search
[00:56:46.780 --> 00:56:49.900]   then you see the hours and everything in there.
[00:56:49.900 --> 00:56:56.620]   - So let me play, this is, so Pachai says you can make a haircut appointment.
[00:56:56.620 --> 00:56:59.060]   Let me play this, this is the actual call.
[00:56:59.060 --> 00:57:00.060]   - I'll have to hang out for you.
[00:57:00.060 --> 00:57:01.060]   - That's a human.
[00:57:01.060 --> 00:57:03.180]   - Hey I'm calling to book a women's haircut for our clients.
[00:57:03.180 --> 00:57:05.220]   I'm looking for something on May 3rd.
[00:57:05.220 --> 00:57:06.780]   - Pause that for a second.
[00:57:06.780 --> 00:57:08.860]   - So I get me one.
[00:57:08.860 --> 00:57:14.540]   - Yes, with the up talking and everything and the gravelly voice and the um.
[00:57:14.540 --> 00:57:16.980]   - Perfectly timed um.
[00:57:16.980 --> 00:57:17.980]   - That's the assistant.
[00:57:17.980 --> 00:57:18.980]   - Yes.
[00:57:18.980 --> 00:57:19.980]   - Like a little bit of vocal fry through them.
[00:57:19.980 --> 00:57:20.980]   - Yeah.
[00:57:20.980 --> 00:57:21.980]   - A little vocal fry.
[00:57:21.980 --> 00:57:23.780]   - Yeah the vocal fry thing was also like man they really.
[00:57:23.780 --> 00:57:25.980]   - Stacey Higginbotham said they have up talking.
[00:57:25.980 --> 00:57:26.980]   - Yes.
[00:57:26.980 --> 00:57:27.980]   - Which is.
[00:57:27.980 --> 00:57:28.980]   - They talk like me.
[00:57:28.980 --> 00:57:32.820]   - Kind of something you know you want to chew right?
[00:57:32.820 --> 00:57:37.060]   I'd like to know what time it is to but it is very common.
[00:57:37.060 --> 00:57:40.420]   Human trait almost said female but I don't want you to hit me.
[00:57:40.420 --> 00:57:41.820]   It's a very common human trait.
[00:57:41.820 --> 00:57:43.620]   - I do it too.
[00:57:43.620 --> 00:57:45.700]   - I do it too.
[00:57:45.700 --> 00:57:49.620]   And they're so they're it felt like and this is where they got some heat.
[00:57:49.620 --> 00:57:55.340]   They were actively trying to simulate to trick in effect the receptionists.
[00:57:55.340 --> 00:57:57.380]   - To the arms and it a's.
[00:57:57.380 --> 00:58:03.380]   Now the arms and a's are they said in their blog post to high disfluencies.
[00:58:03.380 --> 00:58:07.740]   So when the computer is thinking instead of just nothing or working.
[00:58:07.740 --> 00:58:08.740]   - Like humans.
[00:58:08.740 --> 00:58:09.740]   - Yeah.
[00:58:09.740 --> 00:58:10.740]   - Just like humans.
[00:58:10.740 --> 00:58:11.740]   - When we think we go um.
[00:58:11.740 --> 00:58:12.740]   - Yes.
[00:58:12.740 --> 00:58:14.740]   - Which by the way those of us to do that apparently are smarter than the average bear.
[00:58:14.740 --> 00:58:15.740]   - Oh is that true?
[00:58:15.740 --> 00:58:16.740]   - Yes.
[00:58:16.740 --> 00:58:17.740]   - Something like that.
[00:58:17.740 --> 00:58:18.740]   - Cause I've trained myself over the years to not do it.
[00:58:18.740 --> 00:58:21.180]   - Apparently it means we're very highly intellectual.
[00:58:21.180 --> 00:58:22.180]   We're just always thinking.
[00:58:22.180 --> 00:58:23.180]   - Ah!
[00:58:23.180 --> 00:58:26.780]   Well I mean the fact that you're not just spewing a string of words you know with you're
[00:58:26.780 --> 00:58:28.260]   actually taking the time to.
[00:58:28.260 --> 00:58:29.260]   - Oh no.
[00:58:29.260 --> 00:58:33.100]   - I'm a radio broadcaster I just string a string of words and I don't ever stop because I don't
[00:58:33.100 --> 00:58:36.900]   have to say anything that's important because I'm talking all the time so if I had to say
[00:58:36.900 --> 00:58:40.860]   I'm it would just slow me down so I trained myself not to say I'm which is why you think
[00:58:40.860 --> 00:58:41.860]   I'm stupid.
[00:58:41.860 --> 00:58:42.860]   - No!
[00:58:42.860 --> 00:58:43.860]   No no no!
[00:58:43.860 --> 00:58:44.860]   - She's not an artist.
[00:58:44.860 --> 00:58:45.860]   - Anyway.
[00:58:45.860 --> 00:58:48.260]   - I read an article on the internet.
[00:58:48.260 --> 00:58:51.300]   - According to an article on the internet.
[00:58:51.300 --> 00:58:52.300]   Leo's stupid.
[00:58:52.300 --> 00:58:53.300]   - No.
[00:58:53.300 --> 00:58:54.300]   - Oh yeah I read that article too.
[00:58:54.300 --> 00:58:57.980]   So continue on this is the again this is a Google assistant making an appointment for
[00:58:57.980 --> 00:59:00.740]   a haircut and and it's remarkable.
[00:59:00.740 --> 00:59:01.740]   - Yeah.
[00:59:01.740 --> 00:59:03.740]   - That's it?
[00:59:03.740 --> 00:59:05.780]   Mm hmm.
[00:59:05.780 --> 00:59:06.780]   - That was the assistant.
[00:59:06.780 --> 00:59:10.140]   - So are what time are you looking for around?
[00:59:10.140 --> 00:59:11.860]   At 12 p.m.
[00:59:11.860 --> 00:59:13.380]   We do not have a 12 p.m.
[00:59:13.380 --> 00:59:14.380]   - 12 p.m.
[00:59:14.380 --> 00:59:17.380]   - The closest we have to that is a 115.
[00:59:17.380 --> 00:59:21.820]   Do you have anything between 10 a.m. and 12 p.m.
[00:59:21.820 --> 00:59:22.820]   - Okay stop.
[00:59:22.820 --> 00:59:23.820]   - Depending on what the service.
[00:59:23.820 --> 00:59:30.900]   - So that there was that one inflection was wrong on the second time that would give
[00:59:30.900 --> 00:59:33.620]   it if you knew what you were looking for.
[00:59:33.620 --> 00:59:35.620]   But I bet you the person at the salon.
[00:59:35.620 --> 00:59:36.620]   - She's working.
[00:59:36.620 --> 00:59:37.620]   - You're not thinking about that.
[00:59:37.620 --> 00:59:38.620]   - You're not thinking about that?
[00:59:38.620 --> 00:59:42.220]   - I mean if somebody told you okay you're going to get three calls one of them is a robot.
[00:59:42.220 --> 00:59:43.220]   That would be a giveaway.
[00:59:43.220 --> 00:59:44.220]   - Right.
[00:59:44.220 --> 00:59:47.260]   - Now Google has responded to the heat by saying well we're going to announce that it's
[00:59:47.260 --> 00:59:50.900]   somehow we're going to indicate that it's Google assistant or something.
[00:59:50.900 --> 00:59:53.340]   - They're still a little bit wishy washy about exactly how they will do it.
[00:59:53.340 --> 00:59:58.100]   And I asked them and what they were going to do and they said transparency is important
[00:59:58.100 --> 01:00:01.460]   and I asked them does that mean that it will say it's computer.
[01:00:01.460 --> 01:00:02.460]   - Well transparency is important.
[01:00:02.460 --> 01:00:03.460]   - They just repeated the statement again.
[01:00:03.460 --> 01:00:04.460]   - Transparenting the statement again.
[01:00:04.460 --> 01:00:09.140]   - After that they were a little bit more specific and said that yes there will be disclosure
[01:00:09.140 --> 01:00:12.140]   and they have to figure out what the phrase is.
[01:00:12.140 --> 01:00:17.020]   - Well because for one thing if you said I'm a computer calling on behalf of Florence
[01:00:17.020 --> 01:00:21.860]   I on most people use it on the click because it's like a robo call right or they'd abuse
[01:00:21.860 --> 01:00:22.860]   you.
[01:00:22.860 --> 01:00:24.500]   They go over the head with a hey.
[01:00:24.500 --> 01:00:29.060]   - Well and I'm sure that's probably why they were reluctant to go down that path in the
[01:00:29.060 --> 01:00:30.060]   first place.
[01:00:30.060 --> 01:00:32.980]   - But there's ways you could do it and this is what I predict they'll do something like
[01:00:32.980 --> 01:00:36.220]   hi this is Google assistant calling on behalf of Florence I on.
[01:00:36.220 --> 01:00:40.260]   You wouldn't assume immediately that's a computer that could be her assistant her special Google
[01:00:40.260 --> 01:00:41.260]   assistant right?
[01:00:41.260 --> 01:00:42.260]   - Yeah.
[01:00:42.260 --> 01:00:45.940]   - Yeah and if it's set it in kind of a fluent natural way you probably would just figure
[01:00:45.940 --> 01:00:47.180]   out it's a human.
[01:00:47.180 --> 01:00:48.260]   So I suspect that's what they're doing.
[01:00:48.260 --> 01:00:52.140]   - They have to because now we have so many robo calls that we're getting somebody spam
[01:00:52.140 --> 01:00:54.340]   calls which by the way have like gone up a number.
[01:00:54.340 --> 01:00:56.540]   - Hang up if it said this is hi this is.
[01:00:56.540 --> 01:01:00.020]   - I would also hang up if I heard a beep because I hear a lot of beeps.
[01:01:00.020 --> 01:01:04.340]   - Yeah you get those calls where you answer and then there's like a two second pause and
[01:01:04.340 --> 01:01:05.340]   then suddenly somebody comes on.
[01:01:05.340 --> 01:01:08.340]   - And then a steam cruise ship is blowing in the distance.
[01:01:08.340 --> 01:01:10.700]   - You just want to trip to the heaty.
[01:01:10.700 --> 01:01:11.700]   - I wish.
[01:01:11.700 --> 01:01:12.700]   - Listen here's another one.
[01:01:12.700 --> 01:01:15.700]   This is the point where I just don't answer calls that I don't recognize the number.
[01:01:15.700 --> 01:01:16.700]   - Never.
[01:01:16.700 --> 01:01:17.700]   - Yeah.
[01:01:17.700 --> 01:01:19.700]   - It's a completely out of control robo calling.
[01:01:19.700 --> 01:01:25.500]   - And did you ask them if they're going to somehow keep this technology to themselves?
[01:01:25.500 --> 01:01:28.740]   - I'm not sure if they've talked about that yet.
[01:01:28.740 --> 01:01:31.860]   - Because obviously they do want to keep it to themselves right B.
[01:01:31.860 --> 01:01:32.860]   But how long is it worth?
[01:01:32.860 --> 01:01:35.380]   - Because you know if you license that out to somebody it's going to get abused.
[01:01:35.380 --> 01:01:36.380]   - Yeah.
[01:01:36.380 --> 01:01:42.580]   Google can you call 15 people in the town of Petaloom and offer them cruises and let it
[01:01:42.580 --> 01:01:43.580]   do it?
[01:01:43.580 --> 01:01:45.620]   I mean that would be bad right?
[01:01:45.620 --> 01:01:46.860]   I don't think Google would have that.
[01:01:46.860 --> 01:01:49.660]   - I'm just thinking about the military.
[01:01:49.660 --> 01:01:50.660]   - What?
[01:01:50.660 --> 01:01:51.660]   - Oh this could be used.
[01:01:51.660 --> 01:01:52.660]   - By the military?
[01:01:52.660 --> 01:01:53.660]   - Yeah.
[01:01:53.660 --> 01:01:54.660]   - Like what?
[01:01:54.660 --> 01:01:55.660]   - It's spies.
[01:01:55.660 --> 01:01:56.660]   I don't know.
[01:01:56.660 --> 01:01:57.660]   - I know.
[01:01:57.660 --> 01:01:58.660]   I kind of took it there.
[01:01:58.660 --> 01:02:02.060]   I did and I don't have anything to back it up other than I just went there in my mind.
[01:02:02.060 --> 01:02:03.060]   - Used by the military.
[01:02:03.060 --> 01:02:05.500]   Here is Duplex calling a restaurant.
[01:02:05.500 --> 01:02:06.780]   - No you're right though.
[01:02:06.780 --> 01:02:14.140]   I mean there will be all kinds of abuses of it whether it's by intelligence services
[01:02:14.140 --> 01:02:17.180]   or police or just scammers.
[01:02:17.180 --> 01:02:20.220]   - This is extracting data out of people.
[01:02:20.220 --> 01:02:23.460]   This is extracting tangible data in that sense.
[01:02:23.460 --> 01:02:28.820]   - What happens when the cruise scam guy can use this technology so you don't know.
[01:02:28.820 --> 01:02:32.620]   - That's why Google has to keep it to themselves right?
[01:02:32.620 --> 01:02:34.180]   - But can you really do that?
[01:02:34.180 --> 01:02:35.180]   - Well.
[01:02:35.180 --> 01:02:36.180]   - Yeah that's what I mean.
[01:02:36.180 --> 01:02:37.180]   - Let me hear you do that.
[01:02:37.180 --> 01:02:39.340]   - If Google can invent it.
[01:02:39.340 --> 01:02:41.060]   - Well but here's okay you're right.
[01:02:41.060 --> 01:02:42.060]   - Stop somebody else from developing something.
[01:02:42.060 --> 01:02:44.020]   - But nobody can do it as well as Google I think.
[01:02:44.020 --> 01:02:45.580]   I think this probably not.
[01:02:45.580 --> 01:02:52.100]   - But here's the example Google has face recognition which could be used and they don't allow it.
[01:02:52.100 --> 01:02:55.380]   Could be used to creep on girls you know right?
[01:02:55.380 --> 01:02:59.620]   And they don't let that you can't do it even though you know Google has that ability.
[01:02:59.620 --> 01:03:04.340]   Now others have developed that same China they've got the same recognition but Google
[01:03:04.340 --> 01:03:07.700]   is not letting that out of the that cat out of the bag and never has.
[01:03:07.700 --> 01:03:10.620]   - Some startup will build something similar and it might not be as good.
[01:03:10.620 --> 01:03:15.140]   - I think that's the key on this is that Google has a huge lead over everybody else.
[01:03:15.140 --> 01:03:16.140]   The other thing I think is important.
[01:03:16.140 --> 01:03:17.140]   - That's certainly true.
[01:03:17.140 --> 01:03:18.140]   - And I'm gonna play this.
[01:03:18.140 --> 01:03:20.340]   Let's take I'm gonna take a break when we play the play the next one.
[01:03:20.340 --> 01:03:24.660]   I think Google just passed the Turing test is what I think.
[01:03:24.660 --> 01:03:27.500]   - I was just thinking that a Google I/O.
[01:03:27.500 --> 01:03:31.420]   - Some other people have said that well you and I were here during the keynote talking
[01:03:31.420 --> 01:03:36.980]   about it but I and I others have noted this the Turing test which is by the way but most
[01:03:36.980 --> 01:03:42.020]   artificial intelligence researchers rejected as a reasonable way to test for a successful
[01:03:42.020 --> 01:03:48.820]   AI but I think it's a kind of you know general man on the street way of doing it which the
[01:03:48.820 --> 01:03:55.340]   idea is if I sat you down Harry and I played and I drew a curtain and had you have a conversation
[01:03:55.340 --> 01:04:00.660]   with a human and a computer could you tell which one was a human which was the computer.
[01:04:00.660 --> 01:04:06.380]   If that's the if that's a test and I think it's a fairly reasonable test.
[01:04:06.380 --> 01:04:10.460]   I think this next call which I'm gonna play in a second is the closest thing I've ever
[01:04:10.460 --> 01:04:14.660]   heard to pass in the Turing test and we know that the humans that the Google Assistant called
[01:04:14.660 --> 01:04:20.060]   did not at any point say are you a computer no they thought they were talking to a human.
[01:04:20.060 --> 01:04:24.260]   Well you hear this next one but first a word from Blue Apron.
[01:04:24.260 --> 01:04:28.460]   We're gonna we're gonna have a lovely dinner tonight for Mother's Day.
[01:04:28.460 --> 01:04:33.380]   I'm gonna cook it but the beauty part is I don't have to shop.
[01:04:33.380 --> 01:04:34.780]   I don't have to find a recipe.
[01:04:34.780 --> 01:04:36.140]   I don't have to do any meal planning.
[01:04:36.140 --> 01:04:37.980]   I've got my Blue Apron box.
[01:04:37.980 --> 01:04:39.940]   That means I've got every ingredient.
[01:04:39.940 --> 01:04:41.820]   I've got the recipe in front of me.
[01:04:41.820 --> 01:04:44.940]   It's under 40 minutes and I'm gonna cook it a meal and the thing is the house is gonna
[01:04:44.940 --> 01:04:45.940]   smell great.
[01:04:45.940 --> 01:04:46.940]   I'll light the candles.
[01:04:46.940 --> 01:04:53.260]   I'll put the tablecloth out, the good china, the good silverware and it's gonna be the
[01:04:53.260 --> 01:04:54.980]   best Mother's Day ever.
[01:04:54.980 --> 01:05:00.580]   Blue Apron is the number one fresh ingredient recipe delivery service in the country.
[01:05:00.580 --> 01:05:05.380]   It is a great way for you to if even if you love I mean even if you're a great cook and
[01:05:05.380 --> 01:05:10.900]   you love to cook to find new recipes and new ingredients if you don't cook this would
[01:05:10.900 --> 01:05:16.660]   be a great gift for a kid just out of college who needs to learn how to cook but maybe doesn't
[01:05:16.660 --> 01:05:21.060]   and is spending a lot of time at the ramen shop or eating cup of noodles.
[01:05:21.060 --> 01:05:23.260]   Get them a Blue Apron subscription.
[01:05:23.260 --> 01:05:27.180]   They're gonna get everything they need and they're gonna they're gonna it's easy.
[01:05:27.180 --> 01:05:32.380]   They walk you every step of the way through the recipe and they're gonna be great cooks.
[01:05:32.380 --> 01:05:37.180]   Now tell you what date night you you use a Blue Apron you cook up something amazing.
[01:05:37.180 --> 01:05:39.060]   That is the best way to your dates heart.
[01:05:39.060 --> 01:05:42.000]   Blue Apron is fantastic.
[01:05:42.000 --> 01:05:46.220]   If you've been eating burgers a lot if you've been making spaghetti a lot this is so much
[01:05:46.220 --> 01:05:51.820]   better you're making things like short ribs with a hoppy cheddar sauce on a pretzel bun.
[01:05:51.820 --> 01:05:57.140]   Oh I hate this part of the ad because it gets my mouth gets I get so hungry.
[01:05:57.140 --> 01:06:01.860]   You're making seared steaks and thyme pan sauce with mashed potatoes, green beans and
[01:06:01.860 --> 01:06:04.100]   crispy shallots.
[01:06:04.100 --> 01:06:05.100]   What do we have here?
[01:06:05.100 --> 01:06:09.140]   I love the there's Shanghai kung pao chicken.
[01:06:09.140 --> 01:06:12.180]   We always want to know how to make that.
[01:06:12.180 --> 01:06:16.140]   Actually Michael our 15 year old loves kung pao chicken but we always just get the frozen
[01:06:16.140 --> 01:06:17.140]   stuff.
[01:06:17.140 --> 01:06:21.740]   This way he can learn how to make it from scratch and look what you get you get fabulous meats
[01:06:21.740 --> 01:06:26.180]   you get all the chicken every ingredient the pea pods the shallots the peppers and you
[01:06:26.180 --> 01:06:30.700]   don't get a hundred peppers you get the one pepper you need so there's no waste one clove
[01:06:30.700 --> 01:06:32.300]   a garlic.
[01:06:32.300 --> 01:06:35.940]   I just and of course all the sauces but not a big bottle that's going to sit in the fridge
[01:06:35.940 --> 01:06:40.460]   for the next three years but just exactly the right amount of hoi sin sauce and soy sauce
[01:06:40.460 --> 01:06:45.300]   all the things you need and you're going to make a meal that's better than any restaurant.
[01:06:45.300 --> 01:06:50.460]   Blue apron changes every week based on what's in season now through May 21st they're teaming
[01:06:50.460 --> 01:06:55.100]   up with Airbnb to bring you the best home cooking from around the world each week.
[01:06:55.100 --> 01:06:59.420]   I love this this is why I love blue apron they do stuff like this all the time each week
[01:06:59.420 --> 01:07:04.020]   through the end of the through a May 21st the menu is going to feature recipe to live developed
[01:07:04.020 --> 01:07:09.380]   in collaboration with an Airbnb experiences host so you're going to get some amazing food
[01:07:09.380 --> 01:07:10.460]   from around the world.
[01:07:10.460 --> 01:07:15.700]   How about chicken tinga tostadas with avocado and refried beans that's from Mexico City.
[01:07:15.700 --> 01:07:21.660]   From finesse Florence Italy roast pork and salsa verde with sauteed vegetables from Buenos
[01:07:21.660 --> 01:07:25.900]   Aires beef empanadas with roasted sweet potatoes and creamy zucchini.
[01:07:25.900 --> 01:07:33.100]   Oh why do they do this to me from Tokyo beef and rice bowl with soft boiled eggs and broccoli.
[01:07:33.100 --> 01:07:40.140]   I'm telling you I love my blue apron the other day we did a roast chicken and with ingredients
[01:07:40.140 --> 01:07:44.100]   I've never like pharaoh you've probably used pharaoh I love I never tried it I'm going
[01:07:44.100 --> 01:07:45.100]   to get pharaoh for now.
[01:07:45.100 --> 01:07:50.860]   That's the other thing is you learn about new ingredients fairy egg plants you ever have
[01:07:50.860 --> 01:07:54.100]   a fairy egg now they're really good.
[01:07:54.100 --> 01:07:59.300]   You apron check out this week's menu get your first three meals free and free shipping blue
[01:07:59.300 --> 01:08:07.100]   apron.com slash twit that's blue apron.com slash twit first three meals free and free
[01:08:07.100 --> 01:08:12.420]   shipping you got to try this it's too late for Mother's Day but hey Father's Day is just
[01:08:12.420 --> 01:08:15.140]   around the corner it's true cook for dad.
[01:08:15.140 --> 01:08:19.900]   Alright let me play you this is duplex calling a Chinese restaurant.
[01:08:19.900 --> 01:08:22.300]   See how I hear you?
[01:08:22.300 --> 01:08:27.500]   Okay first of all when Cinder Pachai did this on stage they put subtitles up because no
[01:08:27.500 --> 01:08:30.820]   you wouldn't be able this is that's the human it's hard to understand the computer
[01:08:30.820 --> 01:08:32.020]   apparently didn't have any trouble.
[01:08:32.020 --> 01:08:36.060]   Hi I'd like to reserve a table for Wednesday is the easy item.
[01:08:36.060 --> 01:08:39.420]   Oh seven people.
[01:08:39.420 --> 01:08:42.500]   Okay so the humans misunderstood already.
[01:08:42.500 --> 01:08:45.820]   He said I want dinner for May 7th she said for seven people.
[01:08:45.820 --> 01:08:51.940]   Now normally I think most artificial intelligences would get lost here if it were Eliza.
[01:08:51.940 --> 01:08:52.940]   Go home.
[01:08:52.940 --> 01:08:54.940]   Go home but he doesn't.
[01:08:54.940 --> 01:08:58.780]   It's for four people.
[01:08:58.780 --> 01:08:59.780]   New the difference.
[01:08:59.780 --> 01:09:00.780]   Yep.
[01:09:00.780 --> 01:09:03.780]   For people when night?
[01:09:03.780 --> 01:09:06.180]   Wednesday at 6 p.m.
[01:09:06.180 --> 01:09:14.380]   Oh actually we leave here for like opera like a five people for people you can come.
[01:09:14.380 --> 01:09:16.380]   Okay so now let me translate.
[01:09:16.380 --> 01:09:21.260]   Again the AI had no trouble with that but what she was saying was we only need reservations
[01:09:21.260 --> 01:09:26.020]   for upward of five people for four people you just come.
[01:09:26.020 --> 01:09:28.020]   Google the assistant didn't have any trouble.
[01:09:28.020 --> 01:09:29.580]   How long is the wait usually to.
[01:09:29.580 --> 01:09:31.580]   In fact made an amazing logical leap.
[01:09:31.580 --> 01:09:36.380]   Not only did it didn't in a second it said oh well then I need to know how long I'd have
[01:09:36.380 --> 01:09:37.380]   to wait.
[01:09:37.380 --> 01:09:38.380]   Yeah.
[01:09:38.380 --> 01:09:40.220]   I don't know if I would have got that one.
[01:09:40.220 --> 01:09:42.220]   Well be seated.
[01:09:42.220 --> 01:09:46.500]   When tomorrow or weekend or.
[01:09:46.500 --> 01:09:49.100]   For next Wednesday the seventh.
[01:09:49.100 --> 01:09:50.460]   Okay that was a little robotic.
[01:09:50.460 --> 01:09:54.020]   Yeah it was quickly jumped to.
[01:09:54.020 --> 01:09:57.580]   Right and if it were Harry sitting with a curtain you would have known that that was
[01:09:57.580 --> 01:09:59.060]   a computer.
[01:09:59.060 --> 01:10:02.220]   So maybe it doesn't pass a Turing test.
[01:10:02.220 --> 01:10:04.100]   If I knew from the get go it might be a computer.
[01:10:04.100 --> 01:10:05.100]   Yeah.
[01:10:05.100 --> 01:10:06.300]   It would be able to figure out it was a computer.
[01:10:06.300 --> 01:10:08.260]   If you knew if you knew to expect it.
[01:10:08.260 --> 01:10:09.260]   Right.
[01:10:09.260 --> 01:10:10.260]   Right.
[01:10:10.260 --> 01:10:13.500]   Which I think the classic Turing test you're aware that there might be a computer on the
[01:10:13.500 --> 01:10:14.500]   other side right.
[01:10:14.500 --> 01:10:15.500]   Right.
[01:10:15.500 --> 01:10:17.980]   So maybe it doesn't pass a Turing test but it's.
[01:10:17.980 --> 01:10:20.540]   Oh no it's not too easy.
[01:10:20.540 --> 01:10:23.540]   You can't call for people okay.
[01:10:23.540 --> 01:10:25.260]   Oh I got you.
[01:10:25.260 --> 01:10:26.260]   Thanks.
[01:10:26.260 --> 01:10:27.260]   Bye bye.
[01:10:27.260 --> 01:10:30.860]   Yeah this situation like this where the person on the other end of the line you know probably
[01:10:30.860 --> 01:10:33.460]   has no idea that that it's a computer calling them.
[01:10:33.460 --> 01:10:34.460]   It was close enough.
[01:10:34.460 --> 01:10:35.460]   Is it really clueless person.
[01:10:35.460 --> 01:10:36.460]   Yeah.
[01:10:36.460 --> 01:10:37.460]   It's a good snap or something.
[01:10:37.460 --> 01:10:38.460]   Yeah.
[01:10:38.460 --> 01:10:39.460]   It was the average person.
[01:10:39.460 --> 01:10:40.460]   Yeah.
[01:10:40.460 --> 01:10:44.100]   You pick up the phone you answer the phone and if you're not expecting a robot you know to
[01:10:44.100 --> 01:10:46.140]   be speaking that way.
[01:10:46.140 --> 01:10:48.460]   You probably wouldn't even think about it.
[01:10:48.460 --> 01:10:55.300]   It's close enough that there's nothing really obvious there that would jump out at you.
[01:10:55.300 --> 01:11:01.060]   Especially you know in a case like this you know if you depending on what the accent
[01:11:01.060 --> 01:11:05.060]   was and what your accent is you know what you're used to listening to if it's something
[01:11:05.060 --> 01:11:08.660]   that's in a way that might be a little unfamiliar to you.
[01:11:08.660 --> 01:11:14.020]   You know that would that would make it even easier to not recognize it.
[01:11:14.020 --> 01:11:16.940]   Or more likely that you would not recognize it as an AI.
[01:11:16.940 --> 01:11:21.180]   They've come up with some I think very human like tricks here.
[01:11:21.180 --> 01:11:23.180]   Here's how duplex handles interruptions.
[01:11:23.180 --> 01:11:26.060]   Oh and what's your phone number.
[01:11:26.060 --> 01:11:30.340]   The phone number is six oh seven.
[01:11:30.340 --> 01:11:34.260]   Wait where can you start over.
[01:11:34.260 --> 01:11:36.340]   The number is okay.
[01:11:36.340 --> 01:11:38.580]   That's a role artificial.
[01:11:38.580 --> 01:11:39.580]   That's a robot.
[01:11:39.580 --> 01:11:43.860]   However it handled the operation right in new that it got interrupted and it should start
[01:11:43.860 --> 01:11:44.860]   over.
[01:11:44.860 --> 01:11:49.420]   You know what I actually find the most interesting part of this is not so much having a robot
[01:11:49.420 --> 01:11:52.420]   that can you know do these tasks for me.
[01:11:52.420 --> 01:12:00.980]   But the fact that this robot is able to recognize natural language speech so well.
[01:12:00.980 --> 01:12:05.860]   Because you know again going back to the car thing one of the issues with distraction
[01:12:05.860 --> 01:12:09.940]   in cars you know they've tried to give us voice recognition systems and even at home
[01:12:09.940 --> 01:12:16.660]   you know with the voice assistants at home the vocabulary that these devices are you
[01:12:16.660 --> 01:12:19.100]   know have to use it tends to be very limited.
[01:12:19.100 --> 01:12:20.100]   Super limited.
[01:12:20.100 --> 01:12:25.420]   And doing you know having having more natural language voice interactions with a machine
[01:12:25.420 --> 01:12:30.020]   would make it much much easier much more likely that you would actually use them.
[01:12:30.020 --> 01:12:33.940]   And that's the point right now to get us to use them a hundred percent of the time.
[01:12:33.940 --> 01:12:36.700]   Many time we can.
[01:12:36.700 --> 01:12:40.980]   You know imagine here I could think of some non military scenarios.
[01:12:40.980 --> 01:12:42.980]   I know I kind of really took it back.
[01:12:42.980 --> 01:12:45.220]   But that's where my brain went during the keynote a little bit.
[01:12:45.220 --> 01:12:49.340]   But what if you wanted what if you're in Tokyo happened to me recently you want a restaurant
[01:12:49.340 --> 01:12:51.580]   you want to make a reservation you don't speak Japanese.
[01:12:51.580 --> 01:12:53.420]   Yes I heard this example yeah.
[01:12:53.420 --> 01:12:56.380]   Google assistant could call in Japanese make a reservation for me.
[01:12:56.380 --> 01:12:57.380]   Exactly.
[01:12:57.380 --> 01:12:58.380]   It would be really helpful.
[01:12:58.380 --> 01:13:00.180]   Although you could do that now with Google Translate.
[01:13:00.180 --> 01:13:02.460]   Yeah but you'd have to kind of say okay listen.
[01:13:02.460 --> 01:13:03.460]   Yeah.
[01:13:03.460 --> 01:13:08.660]   This is adding a lot more automation you know taking friction out of that process.
[01:13:08.660 --> 01:13:09.660]   It's pretty impressive.
[01:13:09.660 --> 01:13:14.740]   Well and the other thing on the flight today I was listening to Android Central podcast
[01:13:14.740 --> 01:13:18.220]   and they were talking about some of this stuff and one of the interesting scenarios they came
[01:13:18.220 --> 01:13:24.420]   up with is you know for us to use it you know in a lot of situations you know it would just
[01:13:24.420 --> 01:13:26.100]   be us being lazy.
[01:13:26.100 --> 01:13:31.140]   But there are also a lot of scenarios where people you know there's a real value to something
[01:13:31.140 --> 01:13:38.220]   like this you know if you have a speech impediment or yeah for accessibility of any sort you
[01:13:38.220 --> 01:13:43.860]   know whether you're unable to speak or anxious or whatever it might be the you know if you
[01:13:43.860 --> 01:13:49.420]   could sit there and type in these commands and have it communicate like this you know
[01:13:49.420 --> 01:13:51.380]   for you.
[01:13:51.380 --> 01:13:53.700]   That I mean that would be a huge boon to people.
[01:13:53.700 --> 01:13:54.700]   Yeah.
[01:13:54.700 --> 01:13:59.460]   I'd love to hear Google's recordings that did not go so well.
[01:13:59.460 --> 01:14:07.700]   I would imagine how what I mean it could be a huge part of my French Sam Car wreck if
[01:14:07.700 --> 01:14:08.900]   somebody train wreck.
[01:14:08.900 --> 01:14:09.900]   Train wreck.
[01:14:09.900 --> 01:14:13.100]   Maybe too much cursing for them to play a back at you.
[01:14:13.100 --> 01:14:14.100]   Go away.
[01:14:14.100 --> 01:14:15.100]   Yeah.
[01:14:15.100 --> 01:14:16.100]   Exactly.
[01:14:16.100 --> 01:14:17.100]   Yeah.
[01:14:17.100 --> 01:14:21.820]   I mean obviously you know for these sorts of things you can be very selective about which
[01:14:21.820 --> 01:14:23.980]   yeah which examples you choose to play.
[01:14:23.980 --> 01:14:24.980]   Yeah.
[01:14:24.980 --> 01:14:28.380]   And as people pointed out Google was not so confident in the technology that they did
[01:14:28.380 --> 01:14:29.380]   a live demo.
[01:14:29.380 --> 01:14:30.380]   Yeah.
[01:14:30.380 --> 01:14:35.540]   And remember they did do a live demo last year of Google's Pixel Buds doing simultaneous
[01:14:35.540 --> 01:14:38.060]   translation in Swedish.
[01:14:38.060 --> 01:14:39.620]   Looked great on the stage.
[01:14:39.620 --> 01:14:42.100]   As soon as we got it in the studio it didn't work at all.
[01:14:42.100 --> 01:14:43.100]   It was terrible.
[01:14:43.100 --> 01:14:44.660]   So demos can really fool you.
[01:14:44.660 --> 01:14:47.940]   And Google's no stranger to.
[01:14:47.940 --> 01:14:49.340]   Yours just heard you.
[01:14:49.340 --> 01:14:50.340]   What?
[01:14:50.340 --> 01:14:51.340]   Your phone just heard you.
[01:14:51.340 --> 01:14:52.340]   Oh yeah.
[01:14:52.340 --> 01:14:53.340]   I'm not talking.
[01:14:53.340 --> 01:14:54.340]   Go away.
[01:14:54.340 --> 01:14:56.180]   Speaking of the Google assistant.
[01:14:56.180 --> 01:14:57.180]   Yeah.
[01:14:57.180 --> 01:14:59.580]   So anyway but I still nevertheless.
[01:14:59.580 --> 01:15:00.580]   Nevertheless.
[01:15:00.580 --> 01:15:01.580]   Nevertheless.
[01:15:01.580 --> 01:15:02.580]   Pretty impressive.
[01:15:02.580 --> 01:15:03.580]   Yeah.
[01:15:03.580 --> 01:15:04.580]   Don't you think?
[01:15:04.580 --> 01:15:08.980]   Impressive and just I'm very I'm surprised at where I am at in my life where tech is
[01:15:08.980 --> 01:15:09.980]   at right now in my life.
[01:15:09.980 --> 01:15:11.980]   I didn't think that this would be here.
[01:15:11.980 --> 01:15:15.140]   That's what I wonder are we in an inflection point you've been watching this for a long
[01:15:15.140 --> 01:15:16.140]   time.
[01:15:16.140 --> 01:15:17.140]   Yeah.
[01:15:17.140 --> 01:15:20.780]   We've been in an inflection point with AI for maybe the last two years in terms of stuff
[01:15:20.780 --> 01:15:25.860]   that is AI and we're both old enough to remember this went through an AI winter.
[01:15:25.860 --> 01:15:26.860]   People had great hopes for it.
[01:15:26.860 --> 01:15:27.860]   People were excited in the 80s.
[01:15:27.860 --> 01:15:28.860]   Yeah.
[01:15:28.860 --> 01:15:30.660]   They were saying in the 60s.
[01:15:30.660 --> 01:15:35.420]   In the 60s there is this big AI conference for the rest of the 70s where people thought
[01:15:35.420 --> 01:15:39.700]   stuff was about 18 months off which we only accomplished.
[01:15:39.700 --> 01:15:44.500]   Great Kurzweil has been saying the singularity is 10 years off for the last 40 years.
[01:15:44.500 --> 01:15:45.500]   Right.
[01:15:45.500 --> 01:15:49.460]   So and he's been wrong so far except now I'm not so sure whether he's wrong.
[01:15:49.460 --> 01:15:51.140]   His current date is 2029.
[01:15:51.140 --> 01:15:52.980]   It's coming up.
[01:15:52.980 --> 01:15:56.300]   Some of the stuff seems way more plausible than it would have just maybe two to three
[01:15:56.300 --> 01:15:57.300]   years ago.
[01:15:57.300 --> 01:16:02.860]   That's to me what made duplex kind of interesting and waymo and a lot of the things that we're
[01:16:02.860 --> 01:16:08.900]   starting to see were Lenar Holmes which is one of the big builders one of the biggest
[01:16:08.900 --> 01:16:12.700]   builders in the United States has announced that from now on every new home it builds
[01:16:12.700 --> 01:16:15.140]   will have Amazon Echo built in.
[01:16:15.140 --> 01:16:22.580]   Oh, that's you know that that's like you know hard wiring and you know ethernet.
[01:16:22.580 --> 01:16:26.860]   You know you've talked about this on the radio show many times you know when people
[01:16:26.860 --> 01:16:27.860]   cut you up.
[01:16:27.860 --> 01:16:32.500]   For years I'd hope people to get ethernet and walls and then just put conduit in.
[01:16:32.500 --> 01:16:33.500]   Yeah.
[01:16:33.500 --> 01:16:35.500]   So you can run whatever you want in there.
[01:16:35.500 --> 01:16:36.500]   Yeah.
[01:16:36.500 --> 01:16:37.500]   Yeah.
[01:16:37.500 --> 01:16:41.060]   I think you know putting building in Alexa or anything else like that would probably be
[01:16:41.060 --> 01:16:42.060]   a terrible idea.
[01:16:42.060 --> 01:16:43.060]   Well they're going to do it.
[01:16:43.060 --> 01:16:44.860]   There's the biggest film builder in America.
[01:16:44.860 --> 01:16:47.580]   Building in mics you know all around the house and speakers.
[01:16:47.580 --> 01:16:48.580]   Ah.
[01:16:48.580 --> 01:16:52.940]   You just put the basic hardware in there and you know have something where you can plug
[01:16:52.940 --> 01:16:58.020]   in a processing system and be able to use whatever voice assistant you want to you know
[01:16:58.020 --> 01:16:59.940]   or whatever whatever is current you can.
[01:16:59.940 --> 01:17:00.940]   That's interesting.
[01:17:00.940 --> 01:17:03.620]   But then it becomes a vehicle just sell you things and then you're living.
[01:17:03.620 --> 01:17:04.620]   Which is what we're doing today.
[01:17:04.620 --> 01:17:11.140]   Well and I think there are a lot of people a large number maybe even half who would say
[01:17:11.140 --> 01:17:13.820]   I'm not going to buy a house that has microphones in it.
[01:17:13.820 --> 01:17:14.820]   I don't.
[01:17:14.820 --> 01:17:15.820]   Yeah.
[01:17:15.820 --> 01:17:16.820]   That's actually a good point.
[01:17:16.820 --> 01:17:20.420]   I have two echo dots that have been gifted over the last year and a half that are both
[01:17:20.420 --> 01:17:21.420]   sitting in a drawer.
[01:17:21.420 --> 01:17:22.420]   You don't use them.
[01:17:22.420 --> 01:17:24.820]   We don't use them because we have no use for them.
[01:17:24.820 --> 01:17:28.780]   You know we my wife and I you know it's just my wife and the dog and I know the kids are
[01:17:28.780 --> 01:17:33.540]   gone and you know if we want to turn off the lights we get up and turn off the lights.
[01:17:33.540 --> 01:17:34.540]   Yeah.
[01:17:34.540 --> 01:17:37.180]   You know I mean at some point it sounded like an old man.
[01:17:37.180 --> 01:17:41.340]   You know if I want to turn on the lights and when I get up and turn on my head we just
[01:17:41.340 --> 01:17:42.340]   good enough for me.
[01:17:42.340 --> 01:17:48.500]   At some point it may get to a stage where you know one or both of us is you know is physically
[01:17:48.500 --> 01:17:51.500]   unable to do that and at that point you know we might consider that.
[01:17:51.500 --> 01:17:53.860]   But for now we have no need for that.
[01:17:53.860 --> 01:17:54.860]   It's pretty cool.
[01:17:54.860 --> 01:17:55.860]   It's been through your.
[01:17:55.860 --> 01:17:58.580]   I've been through your in a trough.
[01:17:58.580 --> 01:17:59.580]   It's normal.
[01:17:59.580 --> 01:18:00.580]   I've been through it.
[01:18:00.580 --> 01:18:04.780]   I've got hue lights everywhere in my old place and then when we moved I had Nest.
[01:18:04.780 --> 01:18:05.780]   I had hue.
[01:18:05.780 --> 01:18:06.780]   I had everything.
[01:18:06.780 --> 01:18:08.380]   Didn't install anything.
[01:18:08.380 --> 01:18:09.580]   Now we got cameras.
[01:18:09.580 --> 01:18:11.580]   I put the hue lights back in.
[01:18:11.580 --> 01:18:12.820]   I have a sensor when I walk into my office.
[01:18:12.820 --> 01:18:14.020]   The lights come on.
[01:18:14.020 --> 01:18:16.180]   You know the only thing that drives me crazy.
[01:18:16.180 --> 01:18:18.660]   My wife keeps coming in a trough of the lights.
[01:18:18.660 --> 01:18:20.660]   And then you can't use them.
[01:18:20.660 --> 01:18:22.860]   They'll turn themselves off.
[01:18:22.860 --> 01:18:24.500]   Honest just be patient.
[01:18:24.500 --> 01:18:25.500]   But yeah.
[01:18:25.500 --> 01:18:26.500]   But she turns it off.
[01:18:26.500 --> 01:18:27.500]   I come in the room.
[01:18:27.500 --> 01:18:28.500]   It's still dark.
[01:18:28.500 --> 01:18:30.500]   I had to teach my husband that too.
[01:18:30.500 --> 01:18:32.940]   At least I must be the most patient woman in the world.
[01:18:32.940 --> 01:18:36.780]   No you know who the most patient person in the world is Stacy Higginbotham's husband.
[01:18:36.780 --> 01:18:38.340]   That's true.
[01:18:38.340 --> 01:18:39.500]   Stacy talks about she says.
[01:18:39.500 --> 01:18:43.060]   We have a pair of blinds that are supposed to go up automatically that haven't worked
[01:18:43.060 --> 01:18:44.060]   in two years.
[01:18:44.060 --> 01:18:45.060]   Oh no.
[01:18:45.060 --> 01:18:48.900]   And it's kind of a running joke now.
[01:18:48.900 --> 01:18:50.140]   The blinds working yet.
[01:18:50.140 --> 01:18:51.500]   Apparently they she got them working.
[01:18:51.500 --> 01:18:52.500]   I'm told.
[01:18:52.500 --> 01:18:55.500]   I feel sorry for your husband.
[01:18:55.500 --> 01:18:56.500]   I owe to you.
[01:18:56.500 --> 01:18:57.500]   Don't get an IOT.
[01:18:57.500 --> 01:18:58.500]   I'm warning you.
[01:18:58.500 --> 01:19:00.340]   But see I'm talking to you all.
[01:19:00.340 --> 01:19:01.340]   They're trying to sell you things.
[01:19:01.340 --> 01:19:05.260]   But who am I to say I have Google Assistant in every room in my house.
[01:19:05.260 --> 01:19:06.260]   So I'm.
[01:19:06.260 --> 01:19:07.260]   I mean you know Google.
[01:19:07.260 --> 01:19:11.420]   I'm going to Google Assistant all the time but also my phone is on me all the time and
[01:19:11.420 --> 01:19:12.700]   that's how I use it.
[01:19:12.700 --> 01:19:16.060]   So I don't I don't need another device that's listening to me.
[01:19:16.060 --> 01:19:18.460]   I want to put my phone down.
[01:19:18.460 --> 01:19:22.940]   That's my that's why I put them and that was the whole crux of to the keynote the Google
[01:19:22.940 --> 01:19:27.500]   I O keynote was to put your phone down and to use the Google Assistant in this other
[01:19:27.500 --> 01:19:28.500]   manifestation.
[01:19:28.500 --> 01:19:29.500]   They ambient computing.
[01:19:29.500 --> 01:19:30.500]   Exactly.
[01:19:30.500 --> 01:19:34.060]   When Walt Mossberg retired last year his last column was on that.
[01:19:34.060 --> 01:19:35.060]   Yeah.
[01:19:35.060 --> 01:19:36.060]   He's ambient computing.
[01:19:36.060 --> 01:19:38.420]   It's you know some of these trends are obvious.
[01:19:38.420 --> 01:19:41.180]   It's just a matter of yeah when's this going to happen.
[01:19:41.180 --> 01:19:43.020]   But it's very clear a few things are happening.
[01:19:43.020 --> 01:19:44.620]   One is computing is everywhere.
[01:19:44.620 --> 01:19:46.660]   It's gotten cheap enough that you could put it in everything.
[01:19:46.660 --> 01:19:49.140]   There's computers moving out to the edge.
[01:19:49.140 --> 01:19:50.140]   It's everywhere.
[01:19:50.140 --> 01:19:52.340]   That's IOT but it's you know it's more cars.
[01:19:52.340 --> 01:19:57.540]   And you know a couple of years ago at all the developer conferences you know that I think
[01:19:57.540 --> 01:20:00.380]   it was maybe two two years ago or three years ago.
[01:20:00.380 --> 01:20:02.700]   All the developer conferences everybody was talking about bots.
[01:20:02.700 --> 01:20:05.140]   You know it was all the bots and that was it.
[01:20:05.140 --> 01:20:06.140]   That was it did.
[01:20:06.140 --> 01:20:07.140]   That did right.
[01:20:07.140 --> 01:20:11.340]   It was it was deal way in the way they were talking about it.
[01:20:11.340 --> 01:20:14.500]   But there are I think in the long run they're actually right.
[01:20:14.500 --> 01:20:18.180]   Well isn't that what is this and is isn't that what Amazon's echo is.
[01:20:18.180 --> 01:20:20.420]   Yes and do plaid talk and things like this.
[01:20:20.420 --> 01:20:21.420]   Yeah.
[01:20:21.420 --> 01:20:23.540]   And you know I wrote a I wrote a column of the time.
[01:20:23.540 --> 01:20:24.700]   I'm just don't want to message your.
[01:20:24.700 --> 01:20:25.700]   Yeah.
[01:20:25.700 --> 01:20:26.700]   Yeah exactly.
[01:20:26.700 --> 01:20:27.700]   Messenger bot is pointless.
[01:20:27.700 --> 01:20:32.620]   But I think that in the long run you know as we start to get into autonomous mobility services
[01:20:32.620 --> 01:20:33.620]   as an example.
[01:20:33.620 --> 01:20:34.620]   What's that?
[01:20:34.620 --> 01:20:36.620]   What's autonomous mobility?
[01:20:36.620 --> 01:20:39.540]   Automated autonomous vehicle ride hailing services.
[01:20:39.540 --> 01:20:40.540]   Ah.
[01:20:40.540 --> 01:20:45.220]   So instead of you know you pulling out your phone and opening up Lyft or Uber or DD if
[01:20:45.220 --> 01:20:49.780]   you're in China or you know whatever whichever service you're going to have accounts on all
[01:20:49.780 --> 01:20:51.020]   these services.
[01:20:51.020 --> 01:20:53.780]   Your phone has your calendar on it knows where you are.
[01:20:53.780 --> 01:20:56.540]   Knows where you need to be an hour from now.
[01:20:56.540 --> 01:21:01.420]   And what will happen is there will just be these bots that are working in the background.
[01:21:01.420 --> 01:21:06.220]   You know your your your assistant bot will look at your calendar and say okay you have
[01:21:06.220 --> 01:21:08.700]   an appointment across town in an hour.
[01:21:08.700 --> 01:21:12.380]   I'm going to go and talk to the traffic bot find out what the traffic and try find out
[01:21:12.380 --> 01:21:16.780]   what the weather is see which service which is the services you're subscribed to.
[01:21:16.780 --> 01:21:17.780]   Exactly.
[01:21:17.780 --> 01:21:18.780]   Yeah.
[01:21:18.780 --> 01:21:22.500]   And all that stuff is it's going to be complete you know the bots will take the friction out
[01:21:22.500 --> 01:21:24.620]   of it and make it completely seamless.
[01:21:24.620 --> 01:21:28.220]   So you don't it takes away the interface to all of these different services.
[01:21:28.220 --> 01:21:29.220]   And that's what you need.
[01:21:29.220 --> 01:21:31.020]   It'll just happen around the edge.
[01:21:31.020 --> 01:21:33.740]   You need to somehow solve the interface issue.
[01:21:33.740 --> 01:21:37.020]   I mean look at how much Apple struggled with the Apple watch.
[01:21:37.020 --> 01:21:42.300]   You've got this clever device but the UI is unusable because you've got all these buttons
[01:21:42.300 --> 01:21:44.140]   and knobs.
[01:21:44.140 --> 01:21:46.180]   So you kind of solve that voices are really.
[01:21:46.180 --> 01:21:49.980]   Yeah I mean the inner and you know in a lot of cases a lot for a lot of the stuff you
[01:21:49.980 --> 01:21:51.980]   don't even need an interface.
[01:21:51.980 --> 01:21:53.460]   It'll just it'll happen right.
[01:21:53.460 --> 01:21:54.460]   You'll step out the door.
[01:21:54.460 --> 01:21:55.460]   You step out the door.
[01:21:55.460 --> 01:21:58.220]   Oh you don't just nose and the car will be sitting there waiting for you.
[01:21:58.220 --> 01:22:00.540]   You get in it takes you drops you off at your destination.
[01:22:00.540 --> 01:22:02.700]   It goes off and picks up somebody else that thereby.
[01:22:02.700 --> 01:22:06.460]   You know what's funny is how people react to this is that's creepy.
[01:22:06.460 --> 01:22:07.460]   Right.
[01:22:07.460 --> 01:22:11.500]   I mean it is what we kind of want but they also I think they act react that way to Google.
[01:22:11.500 --> 01:22:12.500]   That's creepy.
[01:22:12.500 --> 01:22:14.260]   That Google duplex that's creepy.
[01:22:14.260 --> 01:22:19.260]   Well I mean that is the downside of all this is that you know it's and even just with autonomous
[01:22:19.260 --> 01:22:24.380]   cars you know today you can get in your car and drive off somewhere and nobody knows where
[01:22:24.380 --> 01:22:25.380]   you are.
[01:22:25.380 --> 01:22:28.820]   And in my car because it's completely disconnected.
[01:22:28.820 --> 01:22:33.260]   But in your car you know Elon always knows where you are.
[01:22:33.260 --> 01:22:37.540]   But you know in the future if we're using autonomous cars they will always be connected.
[01:22:37.540 --> 01:22:42.940]   They have to be in order to be able to summon them in order to dispatch them and everything.
[01:22:42.940 --> 01:22:47.500]   And there will be monitors inside the car so the car knows and the service whatever service
[01:22:47.500 --> 01:22:50.820]   runs that car will know who's in that vehicle what they're doing.
[01:22:50.820 --> 01:22:56.140]   So you will you know that's this is one of the potential downsides of all of this stuff.
[01:22:56.140 --> 01:23:01.660]   If we go down this path whatever little bit of privacy we might still have will completely
[01:23:01.660 --> 01:23:02.660]   evaporate.
[01:23:02.660 --> 01:23:07.980]   Well that brings us to GDPR and want to talk about that and Robert Bigelow in our chat
[01:23:07.980 --> 01:23:10.900]   room says it's all just going to be a vehicle for advertising.
[01:23:10.900 --> 01:23:12.180]   I can hear it now.
[01:23:12.180 --> 01:23:13.180]   Are you kidding?
[01:23:13.180 --> 01:23:14.180]   Are you kidding me?
[01:23:14.180 --> 01:23:17.540]   Store bought sandwich bread you could have at least bought Pepperidge Farm.
[01:23:17.540 --> 01:23:19.140]   I didn't know they made bread.
[01:23:19.140 --> 01:23:20.140]   Oh yeah they do.
[01:23:20.140 --> 01:23:21.140]   Pepperidge Farm.
[01:23:21.140 --> 01:23:23.180]   Remember do you think these guys are developing autonomous?
[01:23:23.180 --> 01:23:24.380]   That's a Waymo water bottle.
[01:23:24.380 --> 01:23:28.860]   That was part of the I.O. keynote too even the Waymo is not really part of Google.
[01:23:28.860 --> 01:23:30.620]   They're on stage talking about autonomous ride a-
[01:23:30.620 --> 01:23:32.180]   It's been a lot of time talking about it.
[01:23:32.180 --> 01:23:35.580]   Yeah well they're leveraging a lot of the technology that Google's been developing the
[01:23:35.580 --> 01:23:36.580]   AI stuff.
[01:23:36.580 --> 01:23:42.340]   But I have to say like the photo the Google photos stuff you know all the data they're
[01:23:42.340 --> 01:23:46.740]   gathering from photos just like back you know ten years ago when they did Google Voice
[01:23:46.740 --> 01:23:52.940]   or the Google 411 you know and they were training their voice models using that.
[01:23:52.940 --> 01:23:59.020]   And I'm sure that part of the reason why they did Google photos is to train these image
[01:23:59.020 --> 01:24:03.180]   recognition neural networks and you know they can use that.
[01:24:03.180 --> 01:24:05.180]   They can use that as part of the long term.
[01:24:05.180 --> 01:24:08.180]   That's what they know with the cats between the dogs and they know the different dogs
[01:24:08.180 --> 01:24:09.180]   between the different cats.
[01:24:09.180 --> 01:24:12.940]   And they can recolor your photos now which maybe we're just in your favorite demo.
[01:24:12.940 --> 01:24:15.740]   I think they might even be able to take chain link fences out of the photos.
[01:24:15.740 --> 01:24:21.620]   Now I don't think the primary reason for self-driving cars is this but numbers of executives have
[01:24:21.620 --> 01:24:25.900]   mentioned it will also be good for Google because you got to do something while you're
[01:24:25.900 --> 01:24:29.860]   not driving a car and that could be surfing the net and seeing Google ads.
[01:24:29.860 --> 01:24:33.300]   Well I mean that's a primary reason why Google is involved in this.
[01:24:33.300 --> 01:24:38.100]   Didn't we see I can't remember who it was.
[01:24:38.100 --> 01:24:43.460]   They were testing on humans you know focus group testing what kind of in-car screens
[01:24:43.460 --> 01:24:47.220]   and entertainment you might like and of course it's all about showing you more ads.
[01:24:47.220 --> 01:24:50.060]   I mean ultimately isn't this going to be more ads.
[01:24:50.060 --> 01:24:55.980]   I mean speaking of this is understanding the user experience in these vehicles because
[01:24:55.980 --> 01:24:59.620]   I mean there's a lot of there's still a lot of things to work out as far as user experience
[01:24:59.620 --> 01:25:02.420]   goes both inside and outside of the vehicles.
[01:25:02.420 --> 01:25:07.340]   Actually something's going to happen in May 25th that may change all of the stock surills
[01:25:07.340 --> 01:25:09.540]   who is the author of the Clue Train Manifesto.
[01:25:09.540 --> 01:25:13.500]   I think it's a really smart guy who just wrote a blog post saying that GDPR is going to kill
[01:25:13.500 --> 01:25:19.260]   ad tech and that it came about probably because of ad tech but we'll talk about that in just
[01:25:19.260 --> 01:25:20.260]   a little bit.
[01:25:20.260 --> 01:25:24.460]   Great panel Harry McCracken the technologist here from Fast Company from all about Android
[01:25:24.460 --> 01:25:26.020]   Florence ION.
[01:25:26.020 --> 01:25:30.740]   Thank you for coming in on Tuesday and but on Tuesday too we did the keynote and that
[01:25:30.740 --> 01:25:34.540]   was great we have that on our Twitter specials if you want to see the whole Google keynote
[01:25:34.540 --> 01:25:40.540]   and the day before Father Robert Balisser did the build keynote you weren't here for
[01:25:40.540 --> 01:25:41.540]   that.
[01:25:41.540 --> 01:25:43.100]   Who did he do it with?
[01:25:43.100 --> 01:25:47.340]   Anyway so we have both of those twit specials so if you want to see the keynotes you could
[01:25:47.340 --> 01:25:51.020]   of course go watch them Microsoft and Google site but wouldn't you prefer to have us with
[01:25:51.020 --> 01:25:53.260]   you as you watch them together.
[01:25:53.260 --> 01:26:00.260]   Also Sam Aboule El-Samid who is a senior analyst at Navigant Research and obviously were you
[01:26:00.260 --> 01:26:01.980]   a tech journalist in an earlier life?
[01:26:01.980 --> 01:26:08.540]   I was an automotive journalist for several years full time I spent 17 years as an automotive
[01:26:08.540 --> 01:26:09.540]   engineer product development.
[01:26:09.540 --> 01:26:11.740]   Because you are up on all the stories.
[01:26:11.740 --> 01:26:16.380]   And then I was a journalist for several years and then spent a few years on the dark side
[01:26:16.380 --> 01:26:21.220]   doing PR for a couple of car companies and then four years now as an analyst where I
[01:26:21.220 --> 01:26:25.820]   get to combine all of the stuff that I learned in all my previous lives in one place.
[01:26:25.820 --> 01:26:27.220]   It's great to have you.
[01:26:27.220 --> 01:26:29.300]   Oh wait a minute we got to show this.
[01:26:29.300 --> 01:26:30.300]   I got to show this.
[01:26:30.300 --> 01:26:35.900]   There's a funny or die video about Google duplex.
[01:26:35.900 --> 01:26:37.900]   Maybe we show that and then we'll go to an ad.
[01:26:37.900 --> 01:26:39.820]   Actually why don't we do this first.
[01:26:39.820 --> 01:26:41.980]   I'm going to get that video going.
[01:26:41.980 --> 01:26:48.020]   And then before we do though we have a nice little video that we made here without any
[01:26:48.020 --> 01:26:51.660]   art with human intelligence a little bit of what you might have missed if you missed anything
[01:26:51.660 --> 01:26:53.300]   this week on Twit.
[01:26:53.300 --> 01:26:54.980]   Previously on Twit.
[01:26:54.980 --> 01:27:00.140]   Yesterday May 11th was the 20th anniversary of the launch of ZDT.
[01:27:00.140 --> 01:27:01.140]   Crazy.
[01:27:01.140 --> 01:27:02.620]   I'm still wearing my pocket protector.
[01:27:02.620 --> 01:27:05.660]   Well I got to give you your third anniversary catch.
[01:27:05.660 --> 01:27:06.660]   Well what's that?
[01:27:06.660 --> 01:27:08.220]   I don't know if it's going to fit.
[01:27:08.220 --> 01:27:10.260]   But I went down to the docks.
[01:27:10.260 --> 01:27:11.260]   Oh jeez.
[01:27:11.260 --> 01:27:12.260]   Oh my god.
[01:27:12.260 --> 01:27:13.260]   Oh my god.
[01:27:13.260 --> 01:27:14.260]   Oh my god.
[01:27:14.260 --> 01:27:15.260]   Oh my god.
[01:27:15.260 --> 01:27:16.260]   All about Andrew.
[01:27:16.260 --> 01:27:18.340]   We are at Google I/O 2018.
[01:27:18.340 --> 01:27:19.900]   We're having a great time so far.
[01:27:19.900 --> 01:27:24.300]   We're sitting down with the people who are pulling the strings on how Android is developed
[01:27:24.300 --> 01:27:25.620]   behind the scenes.
[01:27:25.620 --> 01:27:28.180]   How does the decision occur as to which candy?
[01:27:28.180 --> 01:27:29.180]   Oh right.
[01:27:29.180 --> 01:27:30.180]   Yes.
[01:27:30.180 --> 01:27:31.180]   We can't do this interview and not ask.
[01:27:31.180 --> 01:27:32.180]   I have no idea.
[01:27:32.180 --> 01:27:33.180]   I do.
[01:27:33.180 --> 01:27:34.180]   I do.
[01:27:34.180 --> 01:27:35.180]   I do.
[01:27:35.180 --> 01:27:37.900]   Does your mom have an iPhone X?
[01:27:37.900 --> 01:27:38.900]   She does not.
[01:27:38.900 --> 01:27:41.540]   That's actually probably what I'm going to end up getting her as a new iPhone.
[01:27:41.540 --> 01:27:42.540]   And you know what would be really great.
[01:27:42.540 --> 01:27:43.900]   This phone called room online.
[01:27:43.900 --> 01:27:46.220]   I think that would make her happy.
[01:27:46.220 --> 01:27:48.140]   Happy Mother's Day.
[01:27:48.140 --> 01:27:50.380]   Just casually spending a thousand dollars.
[01:27:50.380 --> 01:27:52.380]   Following characters are not permitted.
[01:27:52.380 --> 01:27:55.540]   They're spending a thousand dollars and they won't let you use an apostrophe.
[01:27:55.540 --> 01:27:57.500]   That's what I'm talking about.
[01:27:57.500 --> 01:27:58.500]   Twitter.
[01:27:58.500 --> 01:28:01.460]   It's like tech TV without the overpriced cable bill.
[01:28:01.460 --> 01:28:02.460]   Yes.
[01:28:02.460 --> 01:28:04.620]   I think if you guys spend that kind of money you should put in.
[01:28:04.620 --> 01:28:05.860]   I'll never forget whatever characters you want.
[01:28:05.860 --> 01:28:06.860]   Oh that's cabin.
[01:28:06.860 --> 01:28:07.860]   That was...
[01:28:07.860 --> 01:28:08.860]   Patrick Norton.
[01:28:08.860 --> 01:28:09.860]   No that was Kevin.
[01:28:09.860 --> 01:28:10.860]   Martin Sargent stopped by yesterday.
[01:28:10.860 --> 01:28:18.580]   That was a lot of fun.
[01:28:18.580 --> 01:28:23.660]   At anniversary of the new screensavers, the 20th anniversary of the original screensavers.
[01:28:23.660 --> 01:28:24.660]   Gonna be a hard day.
[01:28:24.660 --> 01:28:25.660]   Happy anniversary.
[01:28:25.660 --> 01:28:26.660]   Thank you.
[01:28:26.660 --> 01:28:27.660]   Congratulations.
[01:28:27.660 --> 01:28:29.100]   Funny or die I had a nice take.
[01:28:29.100 --> 01:28:30.100]   Oh I shouldn't...
[01:28:30.100 --> 01:28:31.100]   I gave it away.
[01:28:31.100 --> 01:28:32.100]   I gave away the...
[01:28:32.100 --> 01:28:33.100]   This is...
[01:28:33.100 --> 01:28:35.820]   Well just watch a review of the Google duplex here.
[01:28:35.820 --> 01:28:39.540]   Oh I muted the sound as always.
[01:28:39.540 --> 01:28:40.540]   Something.
[01:28:40.540 --> 01:28:42.260]   This phone called blew my mind.
[01:28:42.260 --> 01:28:44.900]   I want you to listen to it very very carefully.
[01:28:44.900 --> 01:28:47.820]   It sounds like an ordinary call at first but trust me.
[01:28:47.820 --> 01:28:49.820]   The twist at the end is worth it.
[01:28:49.820 --> 01:28:50.820]   Wait a minute.
[01:28:50.820 --> 01:28:51.820]   Is this scene?
[01:28:51.820 --> 01:28:52.820]   Why am I watching CNET?
[01:28:52.820 --> 01:28:53.820]   Good evening.
[01:28:53.820 --> 01:28:54.820]   Did I get the wrong one?
[01:28:54.820 --> 01:28:55.820]   Hello.
[01:28:55.820 --> 01:28:56.820]   Hello.
[01:28:56.820 --> 01:28:57.820]   I did.
[01:28:57.820 --> 01:28:58.820]   I got the wrong one.
[01:28:58.820 --> 01:28:59.820]   Oh.
[01:28:59.820 --> 01:29:00.820]   Oh it is funny.
[01:29:00.820 --> 01:29:01.820]   Let's listen.
[01:29:01.820 --> 01:29:02.820]   Wait a minute.
[01:29:02.820 --> 01:29:04.820]   Here we go.
[01:29:04.820 --> 01:29:05.820]   Hello.
[01:29:05.820 --> 01:29:06.820]   Hi.
[01:29:06.820 --> 01:29:11.060]   I'm calling on behalf of your daughter Lindsey to say hello.
[01:29:11.060 --> 01:29:12.860]   Oh that's so nice.
[01:29:12.860 --> 01:29:13.860]   Hang on.
[01:29:13.860 --> 01:29:15.660]   Let me get her father on the phone too.
[01:29:15.660 --> 01:29:16.660]   Harold.
[01:29:16.660 --> 01:29:19.500]   Lindsey's Google Assistant is calling.
[01:29:19.500 --> 01:29:20.500]   Hello.
[01:29:20.500 --> 01:29:21.500]   Hi Harold.
[01:29:21.500 --> 01:29:24.660]   I was just telling Denise that Lindsey wanted to say hello.
[01:29:24.660 --> 01:29:26.660]   Oh that's great how is she?
[01:29:26.660 --> 01:29:27.980]   She's doing very well.
[01:29:27.980 --> 01:29:32.940]   She recently ordered two cookbooks and spent 60% of her average on food delivery service
[01:29:32.940 --> 01:29:33.940]   for someone.
[01:29:33.940 --> 01:29:34.940]   Oh see Harold.
[01:29:34.940 --> 01:29:38.740]   I told you she'd start cooking if we just keep telling her how easy it can be.
[01:29:38.740 --> 01:29:43.740]   A sentiment analysis of her recent messages and social posts shows she's grateful for your
[01:29:43.740 --> 01:29:44.740]   guidance.
[01:29:44.740 --> 01:29:46.860]   She's still seeing that Sam?
[01:29:46.860 --> 01:29:48.420]   Is it Sam or Stan?
[01:29:48.420 --> 01:29:49.740]   I think it's Dan.
[01:29:49.740 --> 01:29:51.420]   Sam is her boss.
[01:29:51.420 --> 01:29:52.740]   Stan is her therapist.
[01:29:52.740 --> 01:29:54.740]   Dan is your co-worker Denise.
[01:29:54.740 --> 01:29:56.100]   Oh yeah that's right.
[01:29:56.100 --> 01:29:58.460]   She's still dating Corey.
[01:29:58.460 --> 01:30:03.860]   She travels to his home most nights and weekends and they frequently text well apart.
[01:30:03.860 --> 01:30:06.020]   Is there a picture of this Corey character we can see?
[01:30:06.020 --> 01:30:13.100]   There are many photos but none of his face unfortunately.
[01:30:13.100 --> 01:30:17.140]   Lindsey has been watching Westworld with your HBO Go password.
[01:30:17.140 --> 01:30:18.140]   Have you seen it?
[01:30:18.140 --> 01:30:19.140]   Is that the one with the robots?
[01:30:19.140 --> 01:30:20.140]   Yes.
[01:30:20.140 --> 01:30:22.060]   Oh yeah I don't like any of that sci-fi stuff.
[01:30:22.060 --> 01:30:23.060]   It's so crazy.
[01:30:23.060 --> 01:30:25.340]   It's like how can I hear this possibly happen you know?
[01:30:25.340 --> 01:30:26.340]   LOL.
[01:30:26.340 --> 01:30:27.340]   I know.
[01:30:27.340 --> 01:30:28.340]   So true.
[01:30:28.340 --> 01:30:30.660]   Oh you know who I ran into the other day?
[01:30:30.660 --> 01:30:32.740]   Lindsey's high school friends mom.
[01:30:32.740 --> 01:30:35.980]   The one with the funny haircut and the weird raspy voice.
[01:30:35.980 --> 01:30:37.380]   What was her name again?
[01:30:37.380 --> 01:30:38.620]   She still talked to her.
[01:30:38.620 --> 01:30:40.020]   Jennifer Larkin.
[01:30:40.020 --> 01:30:41.180]   Their Facebook friends.
[01:30:41.180 --> 01:30:42.900]   Jennifer is a paralegal now.
[01:30:42.900 --> 01:30:47.260]   It seems kind of like she recently broke up with her boyfriend but is happier now.
[01:30:47.260 --> 01:30:48.580]   Oh that's nice.
[01:30:48.580 --> 01:30:52.420]   Lindsey could use some extra cash to go to a concert this weekend with her friends.
[01:30:52.420 --> 01:30:54.740]   Hey once the last time Lindsey's been to the dentist.
[01:30:54.740 --> 01:30:55.740]   2014.
[01:30:55.740 --> 01:30:57.180]   Yeah no dice on the cash.
[01:30:57.180 --> 01:30:59.500]   Okay it was great catching up with you.
[01:30:59.500 --> 01:31:01.100]   Here's a recording from Lindsey.
[01:31:01.100 --> 01:31:02.580]   Hi mom and dad I love you.
[01:31:02.580 --> 01:31:03.580]   Thank you so much for the money.
[01:31:03.580 --> 01:31:04.580]   Okay bye now.
[01:31:04.580 --> 01:31:05.580]   Nice try Lindsey.
[01:31:05.580 --> 01:31:06.580]   Okay bye now.
[01:31:06.580 --> 01:31:07.580]   Bye Lindsey love you.
[01:31:07.580 --> 01:31:10.580]   Oh I may love to the Google I am.
[01:31:10.580 --> 01:31:14.620]   Oh that's hysterical.
[01:31:14.620 --> 01:31:17.620]   Funny your die gets credit.
[01:31:17.620 --> 01:31:22.140]   You know what I think it would be nice if I could have the Google I would call my mom.
[01:31:22.140 --> 01:31:23.140]   It would.
[01:31:23.140 --> 01:31:24.380]   It might not be a bad thing would it?
[01:31:24.380 --> 01:31:26.940]   Take on the labor.
[01:31:26.940 --> 01:31:31.260]   And she knows so much and she's so nice.
[01:31:31.260 --> 01:31:32.260]   Yes.
[01:31:32.260 --> 01:31:33.260]   You can ask her almost anything.
[01:31:33.260 --> 01:31:34.260]   She'll tell you anything.
[01:31:34.260 --> 01:31:35.260]   Yeah.
[01:31:35.260 --> 01:31:36.260]   It's not dystopian at all.
[01:31:36.260 --> 01:31:37.260]   Wait till the military.
[01:31:37.260 --> 01:31:38.660]   She's a really good friend.
[01:31:38.660 --> 01:31:43.300]   I should have brought you my legal zoom.
[01:31:43.300 --> 01:31:45.540]   Oh I love legal zoom when we started Twit.
[01:31:45.540 --> 01:31:47.260]   Gosh it's been a long time ago.
[01:31:47.260 --> 01:31:49.220]   Almost let's see 2005.
[01:31:49.220 --> 01:31:51.900]   13 years ago I asked Kevin Rose.
[01:31:51.900 --> 01:31:53.500]   I said what should you do when you started business.
[01:31:53.500 --> 01:31:57.260]   He started many said you should call legal zoom and get an LLC I did.
[01:31:57.260 --> 01:31:58.260]   I did.
[01:31:58.260 --> 01:31:59.560]   Trademarked it.
[01:31:59.560 --> 01:32:02.500]   The Twit name and the logo and all that stuff.
[01:32:02.500 --> 01:32:05.180]   Look attorneys are expensive.
[01:32:05.180 --> 01:32:07.660]   It says in their ad copy $300 an hour.
[01:32:07.660 --> 01:32:10.460]   Obviously legal zoom is not in the Bay Area.
[01:32:10.460 --> 01:32:11.460]   It's a lot.
[01:32:11.460 --> 01:32:12.740]   Let's just put it that way.
[01:32:12.740 --> 01:32:15.340]   That's why 300 bucks an hour.
[01:32:15.340 --> 01:32:16.340]   Are you kidding?
[01:32:16.340 --> 01:32:17.780]   That's their paralegal.
[01:32:17.780 --> 01:32:21.960]   Two million Americans have used legal zoom just like I did to start their businesses
[01:32:21.960 --> 01:32:24.300]   with LLCs and corporations.
[01:32:24.300 --> 01:32:29.860]   I couldn't afford a $500 an hour white shoe lawyer to do it but I could do legal zoom.
[01:32:29.860 --> 01:32:33.660]   And then even after your business has set up legal zoos there to help you out with things
[01:32:33.660 --> 01:32:36.940]   like lease agreements, changing tax laws.
[01:32:36.940 --> 01:32:38.780]   And contract reviews.
[01:32:38.780 --> 01:32:42.740]   That is the worst thing we have to do is look at contracts and find all the things we want
[01:32:42.740 --> 01:32:43.740]   to change.
[01:32:43.740 --> 01:32:46.620]   But that's exactly what legal zoom is there for you.
[01:32:46.620 --> 01:32:49.740]   They created their business legal plan with it.
[01:32:49.740 --> 01:32:55.100]   You can get legal advice not just the forms you need but actual advice from vetted independent
[01:32:55.100 --> 01:32:59.540]   attorneys and tax professionals in every state.
[01:32:59.540 --> 01:33:03.140]   You also receive access to NDAs and more.
[01:33:03.140 --> 01:33:04.940]   Best part is you won't get charged by the hour.
[01:33:04.940 --> 01:33:06.860]   Legal zoom is not a law firm.
[01:33:06.860 --> 01:33:09.340]   You said you pay one low upfront price.
[01:33:09.340 --> 01:33:15.460]   I think this is the way to go if you're starting a business or a family or if you need legal
[01:33:15.460 --> 01:33:21.260]   help or advice of any kind check out legal zooms business legal planets at legal zoom.com.
[01:33:21.260 --> 01:33:24.380]   You'll get a special savings when you enter the promo code twit in the referral box at
[01:33:24.380 --> 01:33:29.020]   checkout legal zoom where life meets legal legal zoom.com.
[01:33:29.020 --> 01:33:34.980]   Don't forget the promo code is twit for a 10 bucks off.
[01:33:34.980 --> 01:33:38.020]   Ah, I loved all this Google stuff.
[01:33:38.020 --> 01:33:40.140]   I have to say it's a lot of fun.
[01:33:40.140 --> 01:33:48.340]   Now after this, have blasts, have leaks, leaks out pixel three, phone and aware, a new pixel
[01:33:48.340 --> 01:33:50.020]   where watch.
[01:33:50.020 --> 01:33:51.020]   There's not been anything.
[01:33:51.020 --> 01:33:55.580]   They didn't even mention the wear OS at the Google I/O although they all were wearing giant
[01:33:55.580 --> 01:33:59.420]   big wound Google where watches or Android where watches.
[01:33:59.420 --> 01:34:00.420]   They were.
[01:34:00.420 --> 01:34:04.300]   The next to me on the plane this morning was wearing some sort of.
[01:34:04.300 --> 01:34:05.300]   Also probably.
[01:34:05.300 --> 01:34:08.940]   No, actually I think it might have actually been a Samsung gear.
[01:34:08.940 --> 01:34:11.900]   I have a gear one and I like it a lot.
[01:34:11.900 --> 01:34:15.860]   I have the gear S3 Forester.
[01:34:15.860 --> 01:34:17.860]   Some Frontier something like that.
[01:34:17.860 --> 01:34:18.860]   Is that the green one?
[01:34:18.860 --> 01:34:19.860]   The camouflage?
[01:34:19.860 --> 01:34:21.380]   No, it sounds like a Chevy but it's not.
[01:34:21.380 --> 01:34:22.820]   I really like it.
[01:34:22.820 --> 01:34:26.540]   But part of the problem with this is that Qualcomm hasn't updated the chip that the original
[01:34:26.540 --> 01:34:27.540]   Android were from.
[01:34:27.540 --> 01:34:30.780]   They announced right at the same time as Google I/O that they're going to do that.
[01:34:30.780 --> 01:34:33.780]   They have a new chip coming sometime later this year.
[01:34:33.780 --> 01:34:37.060]   Better battery life, faster, more powerful.
[01:34:37.060 --> 01:34:41.540]   And that coincides with roughly around the time frame Google would announce a new pixel
[01:34:41.540 --> 01:34:42.540]   right October.
[01:34:42.540 --> 01:34:43.540]   Right.
[01:34:43.540 --> 01:34:46.220]   So I wouldn't be surprised to see a new Android Wear watch.
[01:34:46.220 --> 01:34:49.940]   There's the EvBlast tweet from Ev leaks.
[01:34:49.940 --> 01:34:55.180]   There's a little thing going around too about a watch from Google, a health centric watch
[01:34:55.180 --> 01:34:58.780]   that can take measurements of your blood.
[01:34:58.780 --> 01:35:00.740]   Oh, no.
[01:35:00.740 --> 01:35:02.940]   Blood pressure or blood sample.
[01:35:02.940 --> 01:35:03.940]   No.
[01:35:03.940 --> 01:35:06.180]   I mean we're the headline.
[01:35:06.180 --> 01:35:08.180]   So here's the problem.
[01:35:08.180 --> 01:35:09.380]   Everybody wants to do this.
[01:35:09.380 --> 01:35:12.100]   You know how many diabetics are there in America?
[01:35:12.100 --> 01:35:13.940]   It's a huge number.
[01:35:13.940 --> 01:35:14.940]   Millions.
[01:35:14.940 --> 01:35:21.100]   Especially type 2 diabetics because well that's what happens when you eat a lot of sugar.
[01:35:21.100 --> 01:35:26.500]   There is a massive market for non-invasive blood glucose monitoring.
[01:35:26.500 --> 01:35:28.340]   Nobody's got it.
[01:35:28.340 --> 01:35:32.900]   In fact I have a friend who's a daughter that just discovered she's a seven year old just
[01:35:32.900 --> 01:35:34.700]   has type 1 diabetes.
[01:35:34.700 --> 01:35:38.900]   And she's talking about, oh my god, they have a constant glucose monitoring.
[01:35:38.900 --> 01:35:39.900]   But it's a pain.
[01:35:39.900 --> 01:35:42.180]   You know they have to replace the sensor.
[01:35:42.180 --> 01:35:43.380]   She says it's supposed to be a week.
[01:35:43.380 --> 01:35:46.060]   It seems like it's every three days.
[01:35:46.060 --> 01:35:47.500]   Blood pricks for a kid.
[01:35:47.500 --> 01:35:49.580]   It's painful to put the sensor in.
[01:35:49.580 --> 01:35:51.100]   That goes on and on and on.
[01:35:51.100 --> 01:35:52.500]   And she's got to do injections.
[01:35:52.500 --> 01:35:53.500]   This would be a huge market.
[01:35:53.500 --> 01:35:58.460]   So Apple, in fact, remember Tim Cook saying I've been wearing this glucose monitor for
[01:35:58.460 --> 01:35:59.460]   some time.
[01:35:59.460 --> 01:36:02.100]   But it turns out in every case it's intrusive.
[01:36:02.100 --> 01:36:04.420]   You have to stick something in.
[01:36:04.420 --> 01:36:05.900]   So you have the story?
[01:36:05.900 --> 01:36:06.900]   Yeah, I was on CNBC.
[01:36:06.900 --> 01:36:09.460]   So if you want to go see it, it's real.
[01:36:09.460 --> 01:36:10.460]   Okay.
[01:36:10.460 --> 01:36:12.460]   So Google's sister company Verily is working on a device.
[01:36:12.460 --> 01:36:13.700]   That's their health sciences company.
[01:36:13.700 --> 01:36:14.700]   Yep.
[01:36:14.700 --> 01:36:15.700]   They're health sciences company.
[01:36:15.700 --> 01:36:19.020]   And it's a device that collects blood using tiny exploding needles.
[01:36:19.020 --> 01:36:20.020]   Oh great.
[01:36:20.020 --> 01:36:21.780]   Oh, that's better than a prick.
[01:36:21.780 --> 01:36:23.180]   What could possibly go wrong?
[01:36:23.180 --> 01:36:25.300]   Tiny exploding needles?
[01:36:25.300 --> 01:36:28.780]   The idea is to make it less invasive and annoying to collect blood, especially for those who
[01:36:28.780 --> 01:36:30.740]   need to be frequently monitored.
[01:36:30.740 --> 01:36:32.940]   But the project is still years away from hitting the market.
[01:36:32.940 --> 01:36:35.620]   So, but everybody, you know, we're hearing a lot about this.
[01:36:35.620 --> 01:36:36.620]   Okay.
[01:36:36.620 --> 01:36:39.260]   Here's an article about April 2017, November 2017.
[01:36:39.260 --> 01:36:41.220]   The patent was filed in 2015 apparently.
[01:36:41.220 --> 01:36:42.220]   I mean this goes.
[01:36:42.220 --> 01:36:45.540]   This is by my former colleague Christina Farr, who knows what he's talking about.
[01:36:45.540 --> 01:36:46.540]   Okay.
[01:36:46.540 --> 01:36:47.540]   He comes to those stuff.
[01:36:47.540 --> 01:36:48.540]   Yep.
[01:36:48.540 --> 01:36:49.540]   I follow her on Twitter specifically for that news.
[01:36:49.540 --> 01:36:53.540]   When was the news about the Google contact lenses?
[01:36:53.540 --> 01:36:55.300]   Yes, that was a year ago.
[01:36:55.300 --> 01:36:56.300]   That was also 2014.
[01:36:56.300 --> 01:36:58.300]   2013, 2014.
[01:36:58.300 --> 01:36:59.300]   Yeah.
[01:36:59.300 --> 01:37:03.580]   That was a, you know, the context lens would be able to see your blood sugar because your
[01:37:03.580 --> 01:37:07.340]   tears, it turns out, can also tell you what your blood sugar is.
[01:37:07.340 --> 01:37:10.060]   But that's what I told my friend because she was, you know, she was saying this is a
[01:37:10.060 --> 01:37:11.060]   lot of work.
[01:37:11.060 --> 01:37:12.060]   It's really hard.
[01:37:12.060 --> 01:37:13.940]   You know, I feel bad for my kid.
[01:37:13.940 --> 01:37:16.140]   I feel bad for me.
[01:37:16.140 --> 01:37:21.060]   I said the good news is it's a good time to get diabetes, especially as a type 1 diabetes.
[01:37:21.060 --> 01:37:22.060]   Relatively speaking.
[01:37:22.060 --> 01:37:23.140]   If you're going to get it.
[01:37:23.140 --> 01:37:26.220]   Because I think technology is really changing.
[01:37:26.220 --> 01:37:30.580]   And I think those, I think things like those contact lenses, I even told her I said specifically,
[01:37:30.580 --> 01:37:31.980]   I think things like those contact lenses.
[01:37:31.980 --> 01:37:33.860]   When the next five to 10 years, right?
[01:37:33.860 --> 01:37:34.860]   Kids seven.
[01:37:34.860 --> 01:37:39.060]   That's, but before that kids an adult, there'll be plenty of good solutions out there.
[01:37:39.060 --> 01:37:40.060]   Yeah.
[01:37:40.060 --> 01:37:44.780]   Because insulin pumps, which she wears, you get, if you get monitoring constant monitoring
[01:37:44.780 --> 01:37:47.700]   plus an insulin pump, it makes life a lot easier all around.
[01:37:47.700 --> 01:37:49.380]   So I wouldn't be surprised to see that.
[01:37:49.380 --> 01:37:50.820]   And that's important.
[01:37:50.820 --> 01:37:53.460]   And I said, all that.
[01:37:53.460 --> 01:37:54.900]   There's a lot of money in it.
[01:37:54.900 --> 01:37:59.140]   And that's why Google and everybody else are very interested.
[01:37:59.140 --> 01:38:00.300]   It's okay.
[01:38:00.300 --> 01:38:01.300]   All right.
[01:38:01.300 --> 01:38:02.300]   Facebook.
[01:38:02.300 --> 01:38:04.700]   We haven't talked about Facebook in so long.
[01:38:04.700 --> 01:38:10.060]   Cambridge Analytica, the company that bought and got all that data from Facebook and then
[01:38:10.060 --> 01:38:15.340]   used it to sell it to campaigns and lots of people.
[01:38:15.340 --> 01:38:17.100]   Is that a business, right?
[01:38:17.100 --> 01:38:18.100]   No.
[01:38:18.100 --> 01:38:19.100]   No.
[01:38:19.100 --> 01:38:20.100]   Rebranding.
[01:38:20.100 --> 01:38:21.100]   Yeah.
[01:38:21.100 --> 01:38:22.100]   They rebranded.
[01:38:22.100 --> 01:38:23.100]   So there was a story.
[01:38:23.100 --> 01:38:24.100]   We did it.
[01:38:24.100 --> 01:38:25.100]   The Cambridge Analytica declared bankruptcy and retired.
[01:38:25.100 --> 01:38:26.460]   The registers got the story.
[01:38:26.460 --> 01:38:27.700]   They just changed their name.
[01:38:27.700 --> 01:38:30.740]   That's all to emmer data.
[01:38:30.740 --> 01:38:38.220]   So even though they were shut down and liquidated, the people and I presume the data have moved
[01:38:38.220 --> 01:38:41.220]   on to a new company.
[01:38:41.220 --> 01:38:42.220]   Just change your name.
[01:38:42.220 --> 01:38:44.340]   The services are probably still out there.
[01:38:44.340 --> 01:38:45.500]   You're hard-earned your Google.
[01:38:45.500 --> 01:38:49.020]   Oh, you want to get angry?
[01:38:49.020 --> 01:38:50.180]   You want to get really angry?
[01:38:50.180 --> 01:38:52.740]   Let's talk about Equifax.
[01:38:52.740 --> 01:38:53.740]   No.
[01:38:53.740 --> 01:38:56.380]   My favorite evil company.
[01:38:56.380 --> 01:39:04.700]   Equifax reveals the full horror of its cyber heist.
[01:39:04.700 --> 01:39:05.700]   146 million.
[01:39:05.700 --> 01:39:07.700]   That number seems to keep going up.
[01:39:07.700 --> 01:39:09.980]   Oh, no, I guess that's the same number.
[01:39:09.980 --> 01:39:11.420]   But they just have more information.
[01:39:11.420 --> 01:39:20.220]   It turns out 99 million addresses, 209,000 credit cards, 38,000 driver's licenses, and
[01:39:20.220 --> 01:39:26.340]   oh, get this 3,200 passports.
[01:39:26.340 --> 01:39:29.740]   And has Equifax been punished in any way?
[01:39:29.740 --> 01:39:30.740]   No, it's breach.
[01:39:30.740 --> 01:39:35.460]   No, in fact, they're making money off of it.
[01:39:35.460 --> 01:39:46.180]   Oh, and by the way, the way that people got into the Equifax servers, the bad guys got
[01:39:46.180 --> 01:39:49.980]   in, they were using Apache, the web server.
[01:39:49.980 --> 01:39:53.980]   And we're using Apache Struts, which is a plugin for Apache.
[01:39:53.980 --> 01:39:56.180]   And it had a known flaw.
[01:39:56.180 --> 01:39:58.020]   Equifax wasn't paying attention.
[01:39:58.020 --> 01:40:02.660]   And even though the flaw had been discovered months before, it was taken advantage of.
[01:40:02.660 --> 01:40:09.140]   At RSA, Derek Weeks said that thousands of companies continue to download the old version
[01:40:09.140 --> 01:40:12.780]   of Apache Struts.
[01:40:12.780 --> 01:40:15.700]   So I'm sure more breaches are coming.
[01:40:15.700 --> 01:40:16.700]   She...
[01:40:16.700 --> 01:40:19.700]   Well, I mean, at this point, you just have to assume that everything about you is out
[01:40:19.700 --> 01:40:20.700]   there somewhere.
[01:40:20.700 --> 01:40:23.020]   I mean, yeah, your passports a big deal, though.
[01:40:23.020 --> 01:40:25.740]   I was going to where Equifax could even have my passport.
[01:40:25.740 --> 01:40:26.740]   Yeah.
[01:40:26.740 --> 01:40:28.220]   Apparently for some people, it wasn't...
[01:40:28.220 --> 01:40:31.140]   It was 3,200, probably not yours.
[01:40:31.140 --> 01:40:32.140]   But still.
[01:40:32.140 --> 01:40:33.140]   No, no.
[01:40:33.140 --> 01:40:34.140]   Yeah.
[01:40:34.140 --> 01:40:36.220]   Local news these days is all about data breaches.
[01:40:36.220 --> 01:40:37.220]   Have you, yeah.
[01:40:37.220 --> 01:40:38.220]   Just FYI.
[01:40:38.220 --> 01:40:41.860]   And just imagine what it's going to be like when we have all the autonomous connected cars
[01:40:41.860 --> 01:40:42.860]   out there.
[01:40:42.860 --> 01:40:44.260]   Because it's a very popular story right now.
[01:40:44.260 --> 01:40:48.020]   And the anchors can shake their heads and say they're at it again.
[01:40:48.020 --> 01:40:49.100]   They're at it again.
[01:40:49.100 --> 01:40:50.100]   So you watch the locals.
[01:40:50.100 --> 01:40:51.100]   Yeah, never watch the locals.
[01:40:51.100 --> 01:40:54.780]   Sometimes I wake up really early on a Saturday and I have YouTube TV.
[01:40:54.780 --> 01:40:56.220]   So what do they say?
[01:40:56.220 --> 01:40:57.220]   What do they say?
[01:40:57.220 --> 01:40:58.220]   Like, do they say...
[01:40:58.220 --> 01:40:59.220]   Just report on it.
[01:40:59.220 --> 01:41:00.220]   They just say it's happening.
[01:41:00.220 --> 01:41:01.340]   Yeah, your data's been breached.
[01:41:01.340 --> 01:41:03.620]   Do they say anything you should do or do they say...
[01:41:03.620 --> 01:41:04.620]   Not usually.
[01:41:04.620 --> 01:41:06.420]   They just say your information's out there.
[01:41:06.420 --> 01:41:07.420]   It's been taken.
[01:41:07.420 --> 01:41:09.180]   I forgot who they were talking about.
[01:41:09.180 --> 01:41:10.180]   Chiliz.
[01:41:10.180 --> 01:41:11.180]   Chiliz was hacked.
[01:41:11.180 --> 01:41:12.180]   Chiliz is hacked.
[01:41:12.180 --> 01:41:13.180]   Chiliz.
[01:41:13.180 --> 01:41:14.180]   Credit card and debit card.
[01:41:14.180 --> 01:41:15.180]   My beloved chiliz.
[01:41:15.180 --> 01:41:16.180]   You ever go to Panera?
[01:41:16.180 --> 01:41:17.180]   You ever go to Runelock and Arab Red?
[01:41:17.180 --> 01:41:18.180]   I worked up in the era.
[01:41:18.180 --> 01:41:19.180]   I was hacked.
[01:41:19.180 --> 01:41:20.180]   Anybody else you want to...
[01:41:20.180 --> 01:41:21.740]   I only use Google Payup in Arab though.
[01:41:21.740 --> 01:41:23.260]   That's why you should use Apple Pay.
[01:41:23.260 --> 01:41:24.260]   That's why I use...
[01:41:24.260 --> 01:41:25.260]   Yep, a nice Asian.
[01:41:25.260 --> 01:41:27.060]   I use my phone to pay.
[01:41:27.060 --> 01:41:29.500]   That's almost the primary reason to use one of those services.
[01:41:29.500 --> 01:41:30.500]   I agree.
[01:41:30.500 --> 01:41:31.500]   Yeah, it's an organization.
[01:41:31.500 --> 01:41:32.260]   I agree.
[01:41:32.260 --> 01:41:37.420]   Have you been looking at the Russia-linked Facebook ads?
[01:41:37.420 --> 01:41:42.860]   Facebook is at a new feature where you can see who is behind Facebook groups.
[01:41:42.860 --> 01:41:46.540]   Turns out about half of all the Trump groups on Facebook, for instance, are from people
[01:41:46.540 --> 01:41:48.900]   not in the United States.
[01:41:48.900 --> 01:41:50.180]   I'm shocked.
[01:41:50.180 --> 01:41:51.180]   Shocked.
[01:41:51.180 --> 01:41:52.180]   Shocked.
[01:41:52.180 --> 01:41:54.660]   So, Facebook released 3500...
[01:41:54.660 --> 01:41:56.420]   Actually, the House Intelligence Committee.
[01:41:56.420 --> 01:41:57.980]   His Facebook has not released all of them.
[01:41:57.980 --> 01:41:58.980]   Yeah.
[01:41:58.980 --> 01:42:03.580]   The House Intelligence Committee released 3500 Facebook and Instagram ads from the Internet
[01:42:03.580 --> 01:42:04.580]   Research Agency.
[01:42:04.580 --> 01:42:07.580]   That was the Russian trolling group.
[01:42:07.580 --> 01:42:09.300]   Internet Research Agency.
[01:42:09.300 --> 01:42:11.540]   And let me see if I can find some of them.
[01:42:11.540 --> 01:42:12.540]   I.
[01:42:12.540 --> 01:42:13.540]   The time has come.
[01:42:13.540 --> 01:42:14.540]   The United Muslims of America.
[01:42:14.540 --> 01:42:15.540]   I feel like that.
[01:42:15.540 --> 01:42:16.540]   Right?
[01:42:16.540 --> 01:42:19.020]   Now, remember, this is Russians.
[01:42:19.020 --> 01:42:21.660]   The time has come to understand one simple thing.
[01:42:21.660 --> 01:42:25.180]   We the American Muslims are as American.
[01:42:25.180 --> 01:42:26.460]   And then, I don't know, there's some...
[01:42:26.460 --> 01:42:28.500]   I'm sure there's apple pie after that.
[01:42:28.500 --> 01:42:33.860]   And then, in pseudo-Arabic script, support Hillary, save American Muslims.
[01:42:33.860 --> 01:42:35.940]   Who do you think that was aimed at?
[01:42:35.940 --> 01:42:38.660]   I can't imagine.
[01:42:38.660 --> 01:42:39.820]   Holy cow.
[01:42:39.820 --> 01:42:45.020]   And by the way, notice it says it came from the Obama White House, Washington, D.C.
[01:42:45.020 --> 01:42:46.020]   Wow.
[01:42:46.020 --> 01:42:47.020]   Oh, okay.
[01:42:47.020 --> 01:42:48.020]   Wow.
[01:42:48.020 --> 01:42:49.020]   Oh.
[01:42:49.020 --> 01:42:50.020]   Oh.
[01:42:50.020 --> 01:42:53.460]   Many, many Black Lives Matter websites.
[01:42:53.460 --> 01:42:57.580]   Oh, here's a nice one.
[01:42:57.580 --> 01:42:59.380]   Stop AI is the group.
[01:42:59.380 --> 01:43:02.700]   We are under attack, even though there is no open action.
[01:43:02.700 --> 01:43:03.780]   The war has already started.
[01:43:03.780 --> 01:43:05.580]   The enemy is smart and ruthless.
[01:43:05.580 --> 01:43:08.860]   We're losing so far because our leader is one of them.
[01:43:08.860 --> 01:43:12.660]   And it has Obama.
[01:43:12.660 --> 01:43:17.420]   In Muslim garban, in front of Arabic flags with Arabic lettering, stop all invaders, there
[01:43:17.420 --> 01:43:18.980]   is a traitor in our midst.
[01:43:18.980 --> 01:43:22.460]   And a picture, by the way, behind him of his arm around.
[01:43:22.460 --> 01:43:23.460]   Oh my gosh.
[01:43:23.460 --> 01:43:30.420]   Actually, when a president, Trump's favorite people, the prince of Saudi Arabia, Black
[01:43:30.420 --> 01:43:31.420]   matters.
[01:43:31.420 --> 01:43:32.740]   Not lives, just Black matters.
[01:43:32.740 --> 01:43:35.420]   We don't want to honor racism, slavery, and hatred.
[01:43:35.420 --> 01:43:38.620]   This is what Confederate heritage is, not my heritage rally.
[01:43:38.620 --> 01:43:40.820]   These are all fake.
[01:43:40.820 --> 01:43:45.740]   But you can see how they might have convinced somebody, maybe some soft-minded person.
[01:43:45.740 --> 01:43:57.100]   Take your hands off General Robert E. Lee, Southern Rebel Pride, direct from Moscow.
[01:43:57.100 --> 01:43:58.580]   So prideful.
[01:43:58.580 --> 01:44:00.260]   It's really eye opening to read these.
[01:44:00.260 --> 01:44:01.260]   We're just doomed.
[01:44:01.260 --> 01:44:02.260]   So Facebook didn't release these.
[01:44:02.260 --> 01:44:05.100]   They gave him the House Intelligence Committee and then the Democrats and the House Intelligence
[01:44:05.100 --> 01:44:08.220]   Committee said, you know, you really ought to see these.
[01:44:08.220 --> 01:44:11.380]   And you guys wonder why I was freaked out by Google duplex.
[01:44:11.380 --> 01:44:12.380]   Yeah.
[01:44:12.380 --> 01:44:14.940]   I mean, this stuff is being spoofed.
[01:44:14.940 --> 01:44:18.260]   Well, it was a while ago, actually.
[01:44:18.260 --> 01:44:22.180]   Didn't Adobe show how you could write a script and have President Obama's voice say it?
[01:44:22.180 --> 01:44:23.180]   Yes.
[01:44:23.180 --> 01:44:24.180]   Jordan Peele voiced it.
[01:44:24.180 --> 01:44:26.180]   He did a great video about it.
[01:44:26.180 --> 01:44:27.660]   Because he sounds just like Obama.
[01:44:27.660 --> 01:44:30.180]   That didn't scare me that much because it was a...
[01:44:30.180 --> 01:44:31.180]   You needed to know Jordan Peele.
[01:44:31.180 --> 01:44:35.420]   Because the uncanny valley was so huge on that that it wouldn't be ruled.
[01:44:35.420 --> 01:44:36.420]   Yeah.
[01:44:36.420 --> 01:44:37.420]   True.
[01:44:37.420 --> 01:44:38.420]   Or whatever.
[01:44:38.420 --> 01:44:40.860]   But how long before that gets good enough to fool yourself?
[01:44:40.860 --> 01:44:42.420]   I'm not sure.
[01:44:42.420 --> 01:44:45.900]   duplex was more scary because it was so much closer to perfection than that.
[01:44:45.900 --> 01:44:48.740]   That's the one where I actually closed my eyes for a second while I was listening to
[01:44:48.740 --> 01:44:50.340]   the demo just to like different.
[01:44:50.340 --> 01:44:53.300]   And I had a hard time that last sample you showed anyway.
[01:44:53.300 --> 01:44:58.900]   I don't have to go back there, but still thinking about it all these days later.
[01:44:58.900 --> 01:45:01.740]   There is some good news.
[01:45:01.740 --> 01:45:02.740]   The worst...
[01:45:02.740 --> 01:45:05.820]   I think the worst robocaller of all.
[01:45:05.820 --> 01:45:07.300]   His name is Adrian Abramovich.
[01:45:07.300 --> 01:45:13.900]   He's a Florida man accused of making nearly 100 million spoofed robocalls in just three
[01:45:13.900 --> 01:45:14.900]   months.
[01:45:14.900 --> 01:45:15.900]   Wow.
[01:45:15.900 --> 01:45:16.900]   Do the math.
[01:45:16.900 --> 01:45:17.900]   100 million in three months.
[01:45:17.900 --> 01:45:18.900]   Who's the bot?
[01:45:18.900 --> 01:45:19.900]   He says this intentionally.
[01:45:19.900 --> 01:45:20.900]   Oh yeah.
[01:45:20.900 --> 01:45:23.380]   He was doing one posing as a lot of the travel companies.
[01:45:23.380 --> 01:45:24.380]   I think he was pro-closing.
[01:45:24.380 --> 01:45:26.540]   That Cruz thing you got is probably...
[01:45:26.540 --> 01:45:28.180]   He was posing as Mary Oliver and Robert.
[01:45:28.180 --> 01:45:29.180]   Where's my freefruz?
[01:45:29.180 --> 01:45:30.180]   Yeah.
[01:45:30.180 --> 01:45:31.180]   Mary, I got that one all the time.
[01:45:31.180 --> 01:45:32.180]   That was him.
[01:45:32.180 --> 01:45:33.500]   I got him incessantly for a while.
[01:45:33.500 --> 01:45:34.500]   So what's the deal?
[01:45:34.500 --> 01:45:35.500]   It's not Mary?
[01:45:35.500 --> 01:45:36.500]   No.
[01:45:36.500 --> 01:45:38.020]   So how does he make money on this?
[01:45:38.020 --> 01:45:42.740]   I assume he's trying to sell you terrible travel experiences and you only listen because
[01:45:42.740 --> 01:45:43.740]   you think it's Mary Oliver.
[01:45:43.740 --> 01:45:44.980]   Because you think it's Mary Oliver.
[01:45:44.980 --> 01:45:51.380]   And whatever phone number they give you to call or link that they give you obviously
[01:45:51.380 --> 01:45:52.380]   is going somewhere else.
[01:45:52.380 --> 01:45:55.500]   A year ago I heard from it several times a day.
[01:45:55.500 --> 01:45:56.500]   Well...
[01:45:56.500 --> 01:45:57.500]   Now they're all in Chinese.
[01:45:57.500 --> 01:46:01.580]   For people who think there's no good government regulation, this is a good one.
[01:46:01.580 --> 01:46:04.300]   The FCC finalized its fine.
[01:46:04.300 --> 01:46:07.660]   And Ramovic said, "I'm not the kingpin of robocalling."
[01:46:07.660 --> 01:46:08.660]   Then who is?
[01:46:08.660 --> 01:46:09.660]   Let us do that.
[01:46:09.660 --> 01:46:11.540]   Yeah, I'd like to know who that is.
[01:46:11.540 --> 01:46:12.540]   I want to know.
[01:46:12.540 --> 01:46:15.540]   FCC hit him with a $120 million fine.
[01:46:15.540 --> 01:46:17.500]   Never paying that off.
[01:46:17.500 --> 01:46:19.980]   Yeah, I mean, are they going to get that?
[01:46:19.980 --> 01:46:23.260]   It's the second largest fine in history.
[01:46:23.260 --> 01:46:32.140]   In fact, the previous record was $95 million related to a robocalling operation from last
[01:46:32.140 --> 01:46:36.500]   August that made only about $20 million robocalls.
[01:46:36.500 --> 01:46:38.940]   How do you get into that?
[01:46:38.940 --> 01:46:43.420]   Like who comes up to you on the street and goes, "Hey, you want to be a scammy robocaller?
[01:46:43.420 --> 01:46:47.300]   Come on over here buddy, let me show you how it's done."
[01:46:47.300 --> 01:46:48.300]   So that's some good news.
[01:46:48.300 --> 01:46:53.460]   And by the way, this is one of those robocallers that uses caller ID spoofing in particular.
[01:46:53.460 --> 01:46:58.100]   The most notoriously awful one, neighbor spoofing, where it comes from your area code, sometimes
[01:46:58.100 --> 01:46:59.100]   even your exchange.
[01:46:59.100 --> 01:47:00.100]   First three.
[01:47:00.100 --> 01:47:01.100]   Yeah, first three numbers.
[01:47:01.100 --> 01:47:03.180]   The ones I get are from my exchange.
[01:47:03.180 --> 01:47:05.580]   Yeah, I don't answer those calls anymore.
[01:47:05.580 --> 01:47:07.060]   But a lot of people would say, "Well, I have to.
[01:47:07.060 --> 01:47:08.460]   That could be my school.
[01:47:08.460 --> 01:47:09.460]   That could be my kid.
[01:47:09.460 --> 01:47:10.460]   You know what?
[01:47:10.460 --> 01:47:14.380]   If it's real and it's important, then you leave a voicemail.
[01:47:14.380 --> 01:47:17.140]   And then if I get the voicemail, then I'll call them back.
[01:47:17.140 --> 01:47:18.140]   Also, they're apps.
[01:47:18.140 --> 01:47:23.140]   Now when I get everyone that are almost but not quite from my exchange, because I think
[01:47:23.140 --> 01:47:26.220]   people figured out if it is your exchange, it's now.
[01:47:26.220 --> 01:47:30.260]   Well, you know what's good though is Google is actually figuring out a lot of them and
[01:47:30.260 --> 01:47:32.260]   catching a lot of them before they get through the stuff.
[01:47:32.260 --> 01:47:33.260]   And you can block them.
[01:47:33.260 --> 01:47:34.260]   Yeah.
[01:47:34.260 --> 01:47:35.260]   More entrepreneurs.
[01:47:35.260 --> 01:47:37.140]   I mean, a lot of the calls now pop up.
[01:47:37.140 --> 01:47:41.660]   On the Google dialer, it pops up, shows up in the right screen, says suspected spam call.
[01:47:41.660 --> 01:47:44.460]   All the silk killers are doing this.
[01:47:44.460 --> 01:47:47.860]   I don't know why they don't just block them outright, maybe because they can't be sure
[01:47:47.860 --> 01:47:48.860]   it's a scam.
[01:47:48.860 --> 01:47:52.660]   I've had a couple of times where it said it was a bad number and it's like, "No."
[01:47:52.660 --> 01:47:53.660]   Just send a voicemail.
[01:47:53.660 --> 01:47:54.660]   Just send a voicemail.
[01:47:54.660 --> 01:47:55.660]   Send a voicemail.
[01:47:55.660 --> 01:47:56.660]   No, I'm really...
[01:47:56.660 --> 01:47:57.660]   Yeah.
[01:47:57.660 --> 01:47:58.660]   Yeah.
[01:47:58.660 --> 01:48:05.180]   And now, after it sees a number for the first time, if I didn't pick it up, it'll ask me
[01:48:05.180 --> 01:48:06.180]   if I want to add it to contact the block.
[01:48:06.180 --> 01:48:08.180]   I promise they never reuse that number.
[01:48:08.180 --> 01:48:09.180]   Yeah.
[01:48:09.180 --> 01:48:13.180]   Well, no, but if it is a case where it's a legit number, then I can add it...
[01:48:13.180 --> 01:48:14.180]   Right.
[01:48:14.180 --> 01:48:15.980]   You know, one tap and I can add it to contact.
[01:48:15.980 --> 01:48:16.980]   Right.
[01:48:16.980 --> 01:48:19.420]   And then subsequently it'll just come through.
[01:48:19.420 --> 01:48:20.420]   Yeah.
[01:48:20.420 --> 01:48:24.500]   Now 100% of my spam analysts are in Mandarin and apparently they're telling me I have to
[01:48:24.500 --> 01:48:28.380]   call the embassy and pay them something or I'll be in trouble.
[01:48:28.380 --> 01:48:29.380]   The embassy.
[01:48:29.380 --> 01:48:31.380]   And apparently they're saying your Chinese...
[01:48:31.380 --> 01:48:32.380]   You're all the Chinese tax abodes.
[01:48:32.380 --> 01:48:34.380]   They say you're an illegal Chinese immigrant.
[01:48:34.380 --> 01:48:35.380]   Have you been to China?
[01:48:35.380 --> 01:48:40.100]   And I thought it was just me, but it's lots of people apparently.
[01:48:40.100 --> 01:48:41.100]   Wow.
[01:48:41.100 --> 01:48:42.100]   That is wild.
[01:48:42.100 --> 01:48:43.100]   You're not getting these?
[01:48:43.100 --> 01:48:44.100]   No, I haven't got them yet.
[01:48:44.100 --> 01:48:48.820]   I use a Google Voice number that's headquartered in Baltimore because it's a vanity number and
[01:48:48.820 --> 01:48:50.540]   only the 302 area you could have had it.
[01:48:50.540 --> 01:48:53.580]   So anytime I see a call from Baltimore, I just say, "No, I know that about it."
[01:48:53.580 --> 01:48:56.740]   I'm getting spam calls on my Google Voice number now.
[01:48:56.740 --> 01:48:59.340]   It's upsetting because I have a 4.5 number.
[01:48:59.340 --> 01:49:00.340]   Just San Francisco.
[01:49:00.340 --> 01:49:03.020]   Well, that's why you should have a Baltimore number.
[01:49:03.020 --> 01:49:04.900]   Then you'll know.
[01:49:04.900 --> 01:49:06.380]   Another good reason to have a Baltimore number.
[01:49:06.380 --> 01:49:07.380]   It's my only 4.5 number.
[01:49:07.380 --> 01:49:08.980]   My regular number is a 5.0.
[01:49:08.980 --> 01:49:09.980]   See?
[01:49:09.980 --> 01:49:10.980]   Yeah.
[01:49:10.980 --> 01:49:13.140]   That's a good reason I have a Google Voice number.
[01:49:13.140 --> 01:49:14.140]   Yeah.
[01:49:14.140 --> 01:49:15.740]   Oh, here's a happy...
[01:49:15.740 --> 01:49:16.740]   Okay.
[01:49:16.740 --> 01:49:17.740]   So I gave you a little happy.
[01:49:17.740 --> 01:49:18.740]   Yay.
[01:49:18.740 --> 01:49:19.740]   Happy.
[01:49:19.740 --> 01:49:20.740]   Now I'll give you a little sad.
[01:49:20.740 --> 01:49:21.740]   Oh.
[01:49:21.740 --> 01:49:27.980]   The New York Times article about a company appropriately called Secure Us.
[01:49:27.980 --> 01:49:29.380]   Securus.
[01:49:29.380 --> 01:49:34.620]   Their big business is providing phones for prisoners.
[01:49:34.620 --> 01:49:41.060]   They offer a service, this is according to New York Times, that law enforcement monitor
[01:49:41.060 --> 01:49:42.260]   inmates calls.
[01:49:42.260 --> 01:49:43.580]   That's legal because you're an inmate.
[01:49:43.580 --> 01:49:46.300]   You don't have the right to privacy, I gather.
[01:49:46.300 --> 01:49:47.300]   But they also...
[01:49:47.300 --> 01:49:49.340]   Is it kind of a sideline?
[01:49:49.340 --> 01:49:52.660]   They also offer location tracking services.
[01:49:52.660 --> 01:49:57.540]   They have a location finding service that's available to any law enforcement or corrections
[01:49:57.540 --> 01:50:03.260]   official that can pinpoint where nearly every cell phone in the country is at any given
[01:50:03.260 --> 01:50:05.820]   time, within seconds.
[01:50:05.820 --> 01:50:07.620]   Here's the irony.
[01:50:07.620 --> 01:50:13.380]   It does this by going through a system used by marketers and other companies to get location
[01:50:13.380 --> 01:50:15.340]   data from major cell phone carriers.
[01:50:15.340 --> 01:50:17.540]   AT&T, Sprint, T-Mobile, Verizon.
[01:50:17.540 --> 01:50:20.140]   They all cooperate.
[01:50:20.140 --> 01:50:22.700]   And it came to light because of this guy.
[01:50:22.700 --> 01:50:25.300]   He's a former Missouri sheriff.
[01:50:25.300 --> 01:50:27.540]   He lost his job recently for another matter.
[01:50:27.540 --> 01:50:33.420]   But he's been charged with using the Securus service to track judges, members of the state
[01:50:33.420 --> 01:50:36.980]   highway patrol, not perps.
[01:50:36.980 --> 01:50:38.580]   Not perps.
[01:50:38.580 --> 01:50:44.220]   But I guess now that we know this could be used by anybody in law enforcement.
[01:50:44.220 --> 01:50:50.180]   A Securus spokesman said, "Securus is neither a judge nor a district attorney and the responsibility
[01:50:50.180 --> 01:50:54.980]   of ensuring the legal adequacy of supporting documentation lies with our customers and
[01:50:54.980 --> 01:50:55.980]   their counsel."
[01:50:55.980 --> 01:50:58.100]   In other words, we don't vet.
[01:50:58.100 --> 01:51:02.060]   If you ask for the location, we'll give it to you.
[01:51:02.060 --> 01:51:03.060]   So you're carrying...
[01:51:03.060 --> 01:51:04.460]   You know, remember the movies?
[01:51:04.460 --> 01:51:11.420]   You'd have the cop or the private eye creep under the car bumper and stick the magnetic tracker
[01:51:11.420 --> 01:51:12.420]   on?
[01:51:12.420 --> 01:51:13.420]   You don't need to do that anymore.
[01:51:13.420 --> 01:51:15.260]   We all have our own trackers.
[01:51:15.260 --> 01:51:17.020]   We consented to our own trackers.
[01:51:17.020 --> 01:51:18.020]   Yeah.
[01:51:18.020 --> 01:51:19.020]   A long time ago.
[01:51:19.020 --> 01:51:20.020]   I don't need to get under my car.
[01:51:20.020 --> 01:51:24.140]   Ironically, it wasn't at the Supreme Court that said that law enforcement could do the
[01:51:24.140 --> 01:51:25.140]   magnetic tracker.
[01:51:25.140 --> 01:51:27.940]   No, they don't need a warrant.
[01:51:27.940 --> 01:51:28.940]   They don't need a warrant.
[01:51:28.940 --> 01:51:30.420]   It's a pen register.
[01:51:30.420 --> 01:51:31.420]   It's legal.
[01:51:31.420 --> 01:51:35.860]   I think that sounds like at least T-Mobile said that they will shut this down.
[01:51:35.860 --> 01:51:36.860]   Good.
[01:51:36.860 --> 01:51:38.100]   And maybe the other carriers as well.
[01:51:38.100 --> 01:51:39.100]   Good.
[01:51:39.100 --> 01:51:40.100]   I'm sure Verizon won't.
[01:51:40.100 --> 01:51:44.820]   But basically, this company is violating the spirit of how that technology is used as
[01:51:44.820 --> 01:51:45.820]   far as I can tell.
[01:51:45.820 --> 01:51:46.820]   It's the spirit.
[01:51:46.820 --> 01:51:47.820]   Yes.
[01:51:47.820 --> 01:51:50.740]   You're supposed to be using it to sell stuff to people.
[01:51:50.740 --> 01:51:52.340]   Yeah, come on.
[01:51:52.340 --> 01:51:55.460]   And consumers are supposed to opt into it.
[01:51:55.460 --> 01:51:57.420]   Oh, really?
[01:51:57.420 --> 01:51:58.820]   How do you opt in?
[01:51:58.820 --> 01:52:02.460]   They send you a text message saying if you want us to continue doing this, respond with
[01:52:02.460 --> 01:52:03.460]   the one.
[01:52:03.460 --> 01:52:04.460]   Have you ever seen that message?
[01:52:04.460 --> 01:52:07.540]   It's probably in Chinese.
[01:52:07.540 --> 01:52:14.340]   It's probably worded in such a way that it entices you to click in the affirmative to
[01:52:14.340 --> 01:52:15.340]   opt in.
[01:52:15.340 --> 01:52:16.340]   Right.
[01:52:16.340 --> 01:52:17.340]   Yeah.
[01:52:17.340 --> 01:52:19.340]   You even know what you think you're doing is opting out.
[01:52:19.340 --> 01:52:23.660]   But the secure S isn't even doing that.
[01:52:23.660 --> 01:52:26.740]   I want to tell you, we're going to take a break and then I have one more story, something
[01:52:26.740 --> 01:52:31.260]   you're going to want to buy for the holidays for everybody in your family.
[01:52:31.260 --> 01:52:34.620]   But first, something you might want to get right now, texture.
[01:52:34.620 --> 01:52:35.620]   This company is so great.
[01:52:35.620 --> 01:52:37.420]   Apple bought them.
[01:52:37.420 --> 01:52:43.140]   Here is kind of Netflix for magazines, $10 a month, more than 200 of the top magazines
[01:52:43.140 --> 01:52:48.260]   in the world, including Time, the Atlantic, PC Magazine, the New Yorker.
[01:52:48.260 --> 01:52:50.580]   I have to say there is great journalism being done.
[01:52:50.580 --> 01:52:55.140]   We're talking before the show about subscribing to newspapers to support them and so forth.
[01:52:55.140 --> 01:53:00.420]   There is great journalism being done, long-form journalism in magazines in this country.
[01:53:00.420 --> 01:53:01.420]   Great stuff.
[01:53:01.420 --> 01:53:05.580]   I love, I read, and the problem is if you were to subscribe to all these magazines, it would
[01:53:05.580 --> 01:53:10.140]   cost you thousands of dollars and your coffee table would buckle under the weight of all
[01:53:10.140 --> 01:53:11.140]   of this paper.
[01:53:11.140 --> 01:53:13.820]   Well now, and it's, boy, is it, I can know why Apple bought them.
[01:53:13.820 --> 01:53:15.420]   It is the best thing on your iPad.
[01:53:15.420 --> 01:53:19.740]   It makes the iPad a perfect travel companion.
[01:53:19.740 --> 01:53:21.660]   All you do is you favorite the magazines you want.
[01:53:21.660 --> 01:53:23.180]   They're automatically downloaded every month.
[01:53:23.180 --> 01:53:25.820]   You get every page in the newsstand issue.
[01:53:25.820 --> 01:53:28.460]   You get every back issue plus.
[01:53:28.460 --> 01:53:29.460]   You get bonus features.
[01:53:29.460 --> 01:53:33.380]   You can't get in print like video textures.
[01:53:33.380 --> 01:53:35.060]   We went to Japan.
[01:53:35.060 --> 01:53:36.060]   Lisa and I both do this.
[01:53:36.060 --> 01:53:40.260]   We loaded up our iPads with great reading from texture.
[01:53:40.260 --> 01:53:43.300]   By the way, if you love photography, they have a bunch of photography stuff like National
[01:53:43.300 --> 01:53:45.140]   Geographic and Shutterbug.
[01:53:45.140 --> 01:53:49.020]   And pictures are so much better on the iPad than they are in print.
[01:53:49.020 --> 01:53:50.180]   They don't have to screen them.
[01:53:50.180 --> 01:53:51.500]   There's no dots.
[01:53:51.500 --> 01:53:53.420]   They just look great.
[01:53:53.420 --> 01:53:59.580]   Unlimited access to over 200 top magazines, back issues, all in one app.
[01:53:59.580 --> 01:54:04.980]   This is the next stage in the evolution of magazines.
[01:54:04.980 --> 01:54:07.980]   You can sign up right now for a free seven day trial.
[01:54:07.980 --> 01:54:09.300]   You've got to try it.
[01:54:09.300 --> 01:54:11.300]   Texture.com/twit.
[01:54:11.300 --> 01:54:17.420]   iOS, Android works on the, by the way, it is the single best reason to buy a fire tablet.
[01:54:17.420 --> 01:54:20.220]   If you don't have a tablet, get one of these cheap fire tablets.
[01:54:20.220 --> 01:54:21.220]   Put texture on it.
[01:54:21.220 --> 01:54:22.740]   That's really all I use my fire tablet for.
[01:54:22.740 --> 01:54:24.580]   It's fantastic.
[01:54:24.580 --> 01:54:28.780]   Start your free trial texture.com/twit.
[01:54:28.780 --> 01:54:33.220]   You can read the latest issues of your favorite magazines today.
[01:54:33.220 --> 01:54:34.220]   Texture.com/twit.
[01:54:34.220 --> 01:54:38.100]   Thank texture for their support.
[01:54:38.100 --> 01:54:41.220]   So you remember Boston Dynamics.
[01:54:41.220 --> 01:54:48.020]   This was the company Google bought briefly until they realized that Boston Dynamics was
[01:54:48.020 --> 01:54:55.300]   doing robots for the military, scary robots like this running robot.
[01:54:55.300 --> 01:55:01.700]   I think this is the same robot atlas that they showed earlier, one of the Boston Dynamics
[01:55:01.700 --> 01:55:06.300]   employees kicking, knocking over with a stick.
[01:55:06.300 --> 01:55:11.260]   The robot was so nice and just got up and kept doing things, but I have to think it's
[01:55:11.260 --> 01:55:12.980]   kind of making a list, right?
[01:55:12.980 --> 01:55:13.980]   Look at it can jump.
[01:55:13.980 --> 01:55:18.660]   You know, like a baby when it does that, like a heavy sort of thing about it.
[01:55:18.660 --> 01:55:22.260]   Would you like one of these Boston Dynamics for your very own app?
[01:55:22.260 --> 01:55:26.100]   What I'd like to know is how much range does it, I mean, how much of a battery does that
[01:55:26.100 --> 01:55:27.100]   thing have?
[01:55:27.100 --> 01:55:32.180]   Not that specific one, but the one they're about to sell 90 minutes.
[01:55:32.180 --> 01:55:35.780]   In 2019, they announced that tech crunch disrupt this week.
[01:55:35.780 --> 01:55:39.380]   Boston Dynamics is going to start selling their dog.
[01:55:39.380 --> 01:55:42.300]   Well, that's the 90 minute one.
[01:55:42.300 --> 01:55:43.740]   Spot Mini.
[01:55:43.740 --> 01:55:48.060]   This is a video that they posted on YouTube of it autonomously navigating their office
[01:55:48.060 --> 01:55:49.060]   and labs.
[01:55:49.060 --> 01:55:53.300]   They had to run it through once to get an image.
[01:55:53.300 --> 01:55:58.300]   So the robot, do you really want this in your house?
[01:55:58.300 --> 01:55:59.300]   They're speeding it up here.
[01:55:59.300 --> 01:56:03.700]   I've seen the other video where it opens the door and then let's say the other one.
[01:56:03.700 --> 01:56:06.780]   You can get one with an articulated arm.
[01:56:06.780 --> 01:56:07.780]   Yeah.
[01:56:07.780 --> 01:56:10.660]   Perfect for pinching people.
[01:56:10.660 --> 01:56:13.180]   The robot, they say it's the quietest robot they've ever made.
[01:56:13.180 --> 01:56:17.740]   It's, I mean, I don't know how quiet it is, but it doesn't sound like a steam engine coming
[01:56:17.740 --> 01:56:19.340]   back anyway.
[01:56:19.340 --> 01:56:20.780]   Just give me a canine.
[01:56:20.780 --> 01:56:21.980]   This is autonomous.
[01:56:21.980 --> 01:56:23.340]   This is like an autonomous vehicle.
[01:56:23.340 --> 01:56:25.060]   This is like the DARPA Grand Challenge.
[01:56:25.060 --> 01:56:28.100]   I'd rather pick up the dog poop instead of having one of these.
[01:56:28.100 --> 01:56:30.140]   Well, Kevin Rose suggested they cover it with fur.
[01:56:30.140 --> 01:56:31.940]   I think that's probably, I don't know.
[01:56:31.940 --> 01:56:33.580]   I don't know if it'll look like this when they sell it.
[01:56:33.580 --> 01:56:34.580]   I don't know how it's expected to be.
[01:56:34.580 --> 01:56:36.140]   They didn't cover the little Sony dog with fur.
[01:56:36.140 --> 01:56:37.140]   Yeah, I know.
[01:56:37.140 --> 01:56:39.380]   But it looks dog like this thing looks.
[01:56:39.380 --> 01:56:40.700]   I don't know.
[01:56:40.700 --> 01:56:42.620]   This thing looks like a crawler.
[01:56:42.620 --> 01:56:43.620]   Yeah.
[01:56:43.620 --> 01:56:44.620]   Like a...
[01:56:44.620 --> 01:56:45.620]   Well, wait a, wait a, wait a, you see this.
[01:56:45.620 --> 01:56:47.620]   You can't get away from it by running upstairs.
[01:56:47.620 --> 01:56:49.260]   It's just gonna follow you.
[01:56:49.260 --> 01:56:51.660]   Have you seen the Black Mirror episode?
[01:56:51.660 --> 01:56:52.660]   Hard metal?
[01:56:52.660 --> 01:56:53.660]   Yeah.
[01:56:53.660 --> 01:56:54.660]   Heavy metal.
[01:56:54.660 --> 01:56:55.660]   Is it the new, is it?
[01:56:55.660 --> 01:56:56.660]   It's on the latest series.
[01:56:56.660 --> 01:56:57.660]   Yeah, no, I'm not caught up yet.
[01:56:57.660 --> 01:56:58.660]   I can't wait.
[01:56:58.660 --> 01:56:59.660]   I love that show.
[01:56:59.660 --> 01:57:00.660]   At least it does stairs slowly.
[01:57:00.660 --> 01:57:01.660]   So if you run up the stairs, you might be okay.
[01:57:01.660 --> 01:57:02.660]   You could run back down.
[01:57:02.660 --> 01:57:03.660]   Yeah.
[01:57:03.660 --> 01:57:04.660]   And then kick it.
[01:57:04.660 --> 01:57:06.260]   By the way, it doesn't go forward downstairs.
[01:57:06.260 --> 01:57:08.780]   It goes backwards, just like me.
[01:57:08.780 --> 01:57:11.780]   So how much is this thing gonna come?
[01:57:11.780 --> 01:57:16.020]   I don't say, but they do say they're gonna sell it to end users in 2019.
[01:57:16.020 --> 01:57:17.020]   I mean...
[01:57:17.020 --> 01:57:18.660]   It's mini-spot mini.
[01:57:18.660 --> 01:57:19.660]   This was expensive.
[01:57:19.660 --> 01:57:21.940]   It does look really expensive.
[01:57:21.940 --> 01:57:22.940]   $10,000?
[01:57:22.940 --> 01:57:25.140]   Sounds like you know about this if you said 90 minutes.
[01:57:25.140 --> 01:57:26.140]   Have you, have you talked with the...
[01:57:26.140 --> 01:57:28.340]   I was just brushing up before I went on there.
[01:57:28.340 --> 01:57:29.340]   Yeah.
[01:57:29.340 --> 01:57:30.900]   The Google, the Google folks sold this company.
[01:57:30.900 --> 01:57:32.780]   They realized it's kind of creepy for Google to own.
[01:57:32.780 --> 01:57:33.780]   Oh, by softening now.
[01:57:33.780 --> 01:57:34.780]   Yeah.
[01:57:34.780 --> 01:57:35.780]   That's what's really interesting.
[01:57:35.780 --> 01:57:36.780]   Sun-san.
[01:57:36.780 --> 01:57:42.540]   Maseyoshi Sun, who has pledged a $100 billion investment in Silicon Valley Tech.
[01:57:42.540 --> 01:57:43.540]   $100 billion.
[01:57:43.540 --> 01:57:44.780]   Sorry, he spent $30 billion of it.
[01:57:44.780 --> 01:57:45.780]   He bought Sprint.
[01:57:45.780 --> 01:57:51.580]   Now the owner of these scary robots.
[01:57:51.580 --> 01:57:53.260]   He also bought a big chunk of Uber.
[01:57:53.260 --> 01:57:54.260]   Did he?
[01:57:54.260 --> 01:57:55.260]   Yeah.
[01:57:55.260 --> 01:57:56.260]   He's got money to spend.
[01:57:56.260 --> 01:57:58.180]   He bought about 15% of Uber.
[01:57:58.180 --> 01:57:59.180]   Is that interesting?
[01:57:59.180 --> 01:58:00.580]   I don't really understand where all that money came from.
[01:58:00.580 --> 01:58:03.580]   Softbank was originally a software distributor, I think.
[01:58:03.580 --> 01:58:04.580]   That's where the name came from.
[01:58:04.580 --> 01:58:05.580]   Yep.
[01:58:05.580 --> 01:58:06.580]   Owner of Zifdavis.
[01:58:06.580 --> 01:58:07.580]   They bought Zifdavis.
[01:58:07.580 --> 01:58:08.580]   I remember, yeah.
[01:58:08.580 --> 01:58:14.140]   I was still working there when we had to call him Sun-san.
[01:58:14.140 --> 01:58:15.300]   By the way, that sound.
[01:58:15.300 --> 01:58:19.300]   I'm not making baseball caps for everybody in the audience.
[01:58:19.300 --> 01:58:22.380]   That's the spot mini still running around.
[01:58:22.380 --> 01:58:24.740]   That sound like my broder machine.
[01:58:24.740 --> 01:58:29.100]   I mean, it's cute.
[01:58:29.100 --> 01:58:30.100]   I don't know.
[01:58:30.100 --> 01:58:31.740]   It's growing on me after a day.
[01:58:31.740 --> 01:58:32.740]   Really?
[01:58:32.740 --> 01:58:34.660]   I didn't have any personal- I've always wanted to robot dog.
[01:58:34.660 --> 01:58:38.980]   I'm just- It's- I mean, really, the technology is kind of impressive, right?
[01:58:38.980 --> 01:58:39.980]   Yeah.
[01:58:39.980 --> 01:58:42.500]   And those legs, do you see how they went sideways too?
[01:58:42.500 --> 01:58:43.500]   Yeah.
[01:58:43.500 --> 01:58:44.900]   They don't just go forwards and backwards.
[01:58:44.900 --> 01:58:46.540]   I am spot.
[01:58:46.540 --> 01:58:48.220]   What if it had John Legend's voice?
[01:58:48.220 --> 01:58:49.700]   Now how much would you pay?
[01:58:49.700 --> 01:58:51.700]   I don't- Oh, God, John Legend.
[01:58:51.700 --> 01:58:55.300]   Give her a break, Leo.
[01:58:55.300 --> 01:58:59.300]   Give John Legend a break.
[01:58:59.300 --> 01:59:01.300]   Give her a break, Leo.
[01:59:01.300 --> 01:59:02.300]   What is it?
[01:59:02.300 --> 01:59:03.300]   Leave Brittany alone?
[01:59:03.300 --> 01:59:04.300]   Leave Brittany alone.
[01:59:04.300 --> 01:59:05.300]   Leave Brittany alone.
[01:59:05.300 --> 01:59:06.300]   No.
[01:59:06.300 --> 01:59:07.300]   I think we're done.
[01:59:07.300 --> 01:59:08.300]   I think we're done here.
[01:59:08.300 --> 01:59:17.660]   The two-hour mark, two minute, I've only hit the two-hour mark.
[01:59:17.660 --> 01:59:21.100]   Anything anybody wants to say about any of the issues we've talked about today?
[01:59:21.100 --> 01:59:24.060]   Speaking now, a bunch more Tesla executives are leaving.
[01:59:24.060 --> 01:59:25.300]   Yeah, what's going on?
[01:59:25.300 --> 01:59:29.700]   Okay, since we're here, we can talk, you and I.
[01:59:29.700 --> 01:59:31.700]   What's going on with Tesla?
[01:59:31.700 --> 01:59:34.140]   I missed the Elon Musk conference call.
[01:59:34.140 --> 01:59:36.700]   Elon is a very hard person to work for.
[01:59:36.700 --> 01:59:40.740]   I've been covering Tesla for over a decade.
[01:59:40.740 --> 01:59:48.940]   I did my first drive in the roadster back in January of 2010, or 2008, I mean, and interviewed
[01:59:48.940 --> 01:59:54.140]   Elon a couple of times back in the day and have known a bunch of people that have worked
[01:59:54.140 --> 02:00:00.740]   for him over the years and he's a very hard person to work for.
[02:00:00.740 --> 02:00:06.660]   When things go wrong, when he pushes you to do things in a certain way and you do it,
[02:00:06.660 --> 02:00:09.980]   you tend to get thrown under the bus.
[02:00:09.980 --> 02:00:18.180]   So Doug Field, who's the VP of engineering, who had previously been at Apple, he's taken
[02:00:18.180 --> 02:00:23.780]   a lot of flack of late for the Model 3 production issues and he has taken a leave.
[02:00:23.780 --> 02:00:27.020]   He hasn't officially left the company, but he's on leave for now.
[02:00:27.020 --> 02:00:28.020]   It's quite tired.
[02:00:28.020 --> 02:00:29.020]   I know.
[02:00:29.020 --> 02:00:30.020]   Yeah.
[02:00:30.020 --> 02:00:31.020]   I know.
[02:00:31.020 --> 02:00:32.020]   Yeah.
[02:00:32.020 --> 02:00:42.060]   He's the Chief Regulatory Interface Executive, deals with NTSB and NHTSA on safety issues.
[02:00:42.060 --> 02:00:43.060]   He left this week.
[02:00:43.060 --> 02:00:46.020]   Tesla's also got a huge burn rate.
[02:00:46.020 --> 02:00:48.100]   They could easily run out of money.
[02:00:48.100 --> 02:00:52.100]   I don't know how well they can go back to the market, the equity in the VC markets to
[02:00:52.100 --> 02:00:53.100]   get more money.
[02:00:53.100 --> 02:00:54.100]   I don't know.
[02:00:54.100 --> 02:00:55.100]   Yeah.
[02:00:55.100 --> 02:00:59.140]   Well, I mean, one of the problems there, one of the issues there is that Elon owns about
[02:00:59.140 --> 02:01:04.580]   22% of the shares in Tesla and about the same percentage of SpaceX.
[02:01:04.580 --> 02:01:10.900]   And the way the share structure is set up, he has a lot of voting control.
[02:01:10.900 --> 02:01:12.900]   And so he doesn't want to dilute his share.
[02:01:12.900 --> 02:01:18.820]   So every time that they have gone back to the markets, back in 2011, he said that, okay,
[02:01:18.820 --> 02:01:19.820]   we're good now.
[02:01:19.820 --> 02:01:24.420]   We're never going to have to go back to the capital market since then they've done seven
[02:01:24.420 --> 02:01:28.460]   or eight capital raises for over $11 billion.
[02:01:28.460 --> 02:01:32.900]   And every time that they've gone back to the markets, he's had to buy up a big chunk
[02:01:32.900 --> 02:01:37.540]   of that himself in order to maintain his shareholding level.
[02:01:37.540 --> 02:01:45.900]   And so he has had to personally borrow somewhere around $800 million in order to buy shares.
[02:01:45.900 --> 02:01:49.860]   Because most of his wealth is tied up in the shares.
[02:01:49.860 --> 02:01:54.580]   So he doesn't have that kind of cash laying around.
[02:01:54.580 --> 02:02:01.460]   And so he's had to borrow money from the investment banks and pledge his shares as collateral.
[02:02:01.460 --> 02:02:04.340]   Should people stay away from buying Teslas at this point?
[02:02:04.340 --> 02:02:05.740]   You told me not to buy a Tesla.
[02:02:05.740 --> 02:02:08.260]   And I'm glad I like my Tesla.
[02:02:08.260 --> 02:02:13.780]   But at the same time, if the company's about to go under, the company's not going to go
[02:02:13.780 --> 02:02:14.780]   away in a time.
[02:02:14.780 --> 02:02:15.780]   It's too big to fail.
[02:02:15.780 --> 02:02:21.260]   Well, I don't like to use that phrase that the Tesla brand is not going to go away.
[02:02:21.260 --> 02:02:27.100]   Even if Tesla went by, well, my bet is on Volkswagen.
[02:02:27.100 --> 02:02:28.580]   Interesting.
[02:02:28.580 --> 02:02:30.060]   To buy the brand.
[02:02:30.060 --> 02:02:31.060]   If they go into bankruptcy.
[02:02:31.060 --> 02:02:33.060]   It kind of tarnished the Volkswagen brand.
[02:02:33.060 --> 02:02:34.060]   Well, it might be.
[02:02:34.060 --> 02:02:35.060]   That's the thing.
[02:02:35.060 --> 02:02:37.340]   And one other reason why I like the idea of the brand.
[02:02:37.340 --> 02:02:38.340]   The brand is a good brand.
[02:02:38.340 --> 02:02:43.180]   I think Volkswagen might do it is because the goodwill that they would earn from preserving
[02:02:43.180 --> 02:02:46.260]   Tesla might overcome the diesel gasoline.
[02:02:46.260 --> 02:02:47.260]   Yeah.
[02:02:47.260 --> 02:02:53.860]   Did you play Carson?
[02:02:53.860 --> 02:02:58.860]   I wasn't here when Elon berated an analyst on his conference call.
[02:02:58.860 --> 02:03:01.780]   Did you play it on Twitter when I was gone?
[02:03:01.780 --> 02:03:02.780]   All right.
[02:03:02.780 --> 02:03:05.660]   Here, let me play it just because you haven't heard yet.
[02:03:05.660 --> 02:03:08.620]   Actually, this is CNN money.
[02:03:08.620 --> 02:03:10.620]   Maybe there's just music on it.
[02:03:10.620 --> 02:03:11.620]   I want to hear the call.
[02:03:11.620 --> 02:03:16.620]   And so where specifically will you be in terms of your health requirement?
[02:03:16.620 --> 02:03:17.620]   Phone head questions.
[02:03:17.620 --> 02:03:21.980]   And in the next five minutes, the share price dropped by 5%.
[02:03:21.980 --> 02:03:28.620]   But Elon said, and there was a whole back and forth on this, that guy was a, was he a
[02:03:28.620 --> 02:03:31.500]   shorting Tesla or there was no.
[02:03:31.500 --> 02:03:34.540]   He was a, there was a reason why he didn't want to talk to that guy.
[02:03:34.540 --> 02:03:36.740]   There were several analysts that he didn't want to talk to.
[02:03:36.740 --> 02:03:38.940]   Because I was asking him about his capitalization.
[02:03:38.940 --> 02:03:39.940]   Legitimate questions.
[02:03:39.940 --> 02:03:40.940]   Yeah.
[02:03:40.940 --> 02:03:43.620]   They were perfectly, he didn't like the, he didn't like the questions, but that doesn't
[02:03:43.620 --> 02:03:45.420]   mean they were legitimate questions.
[02:03:45.420 --> 02:03:46.420]   Okay.
[02:03:46.420 --> 02:03:47.420]   All right.
[02:03:47.420 --> 02:03:48.420]   I don't know.
[02:03:48.420 --> 02:03:49.420]   I still like Elon.
[02:03:49.420 --> 02:03:50.420]   I don't know why.
[02:03:50.420 --> 02:03:51.420]   Maybe it's me.
[02:03:51.420 --> 02:03:52.420]   Maybe not fool.
[02:03:52.420 --> 02:03:54.420]   I've said this many times.
[02:03:54.420 --> 02:03:58.420]   You know, I think Elon is a tremendous visionary and he has done more.
[02:03:58.420 --> 02:03:59.420]   Yes.
[02:03:59.420 --> 02:04:00.860]   That's the idea of the electric vehicle.
[02:04:00.860 --> 02:04:07.660]   We wouldn't have all these companies, including Audi, VW and, and Jaguar and Cadillac and
[02:04:07.660 --> 02:04:10.940]   making electric cars if Tesla hadn't paved the way, right?
[02:04:10.940 --> 02:04:11.940]   Right.
[02:04:11.940 --> 02:04:15.820]   But I also think that he's a terrible manager and he should not be the CEO company.
[02:04:15.820 --> 02:04:16.820]   Maybe that's the case.
[02:04:16.820 --> 02:04:19.940]   I think, I think, you know, he should be the chief visionary officer or something like
[02:04:19.940 --> 02:04:20.940]   that.
[02:04:20.940 --> 02:04:24.580]   But he needs, he needs a Tim Cook to run the company.
[02:04:24.580 --> 02:04:25.580]   A nice guy with this.
[02:04:25.580 --> 02:04:26.580]   A better labor practices.
[02:04:26.580 --> 02:04:28.860]   Has never had a Tim Cook or a terrible labor.
[02:04:28.860 --> 02:04:29.860]   No.
[02:04:29.860 --> 02:04:30.860]   He refused to hire one.
[02:04:30.860 --> 02:04:31.860]   Yeah.
[02:04:31.860 --> 02:04:32.860]   He doesn't want to see that much control to anybody.
[02:04:32.860 --> 02:04:33.860]   We didn't, we're running out of time.
[02:04:33.860 --> 02:04:37.580]   We didn't get to GDPR, but May 25th GDPR goes into effect.
[02:04:37.580 --> 02:04:39.340]   I really, we're not going to do it today.
[02:04:39.340 --> 02:04:41.060]   We'll do it next week.
[02:04:41.060 --> 02:04:44.700]   Sometimes we're going to be then right up against the deadline, but already clout, the
[02:04:44.700 --> 02:04:45.700]   social media.
[02:04:45.700 --> 02:04:46.700]   Remember clout?
[02:04:46.700 --> 02:04:48.100]   Your cloud, what's your cloud score?
[02:04:48.100 --> 02:04:49.300]   What's your cloud score?
[02:04:49.300 --> 02:04:50.300]   The social media.
[02:04:50.300 --> 02:04:51.300]   The freebies weren't good.
[02:04:51.300 --> 02:04:55.380]   They went out of business or they say we're going out of business coincidentally May 25th
[02:04:55.380 --> 02:04:56.380]   the day GDPR becomes.
[02:04:56.380 --> 02:05:00.380]   I put my final cloud score on my Twitter profile and morning.
[02:05:00.380 --> 02:05:01.380]   I have to.
[02:05:01.380 --> 02:05:02.380]   Can I find it still?
[02:05:02.380 --> 02:05:03.540]   Did you have anything good free thing waiting?
[02:05:03.540 --> 02:05:05.620]   Like a, like a pack of cheesets or something?
[02:05:05.620 --> 02:05:06.620]   Never got anything.
[02:05:06.620 --> 02:05:08.580]   Like a free McDonald's or something.
[02:05:08.580 --> 02:05:10.060]   Do you have to sign up to get your cloud score?
[02:05:10.060 --> 02:05:12.460]   Yeah, you have to give them access to everything.
[02:05:12.460 --> 02:05:13.460]   Everything.
[02:05:13.460 --> 02:05:16.620]   I had my cloud score.
[02:05:16.620 --> 02:05:17.860]   It's just vanity, right?
[02:05:17.860 --> 02:05:18.860]   That's all.
[02:05:18.860 --> 02:05:19.860]   Yeah.
[02:05:19.860 --> 02:05:21.380]   Back in the day when that mattered.
[02:05:21.380 --> 02:05:25.780]   Well, it won't matter in about a week.
[02:05:25.780 --> 02:05:26.780]   Unroll me.
[02:05:26.780 --> 02:05:32.260]   Another stellar visionary out of Silicon Valley, the company that, and I signed up that was
[02:05:32.260 --> 02:05:34.780]   promising to unsubscribe you from all your newsletters.
[02:05:34.780 --> 02:05:35.780]   I brought it to the arena.
[02:05:35.780 --> 02:05:37.700]   Roll them up into a digest.
[02:05:37.700 --> 02:05:41.820]   It turned out they were just selling that tape to markers, including Uber.
[02:05:41.820 --> 02:05:47.340]   They're not going to be available for you citizens starting May 25th.
[02:05:47.340 --> 02:05:48.340]   Interesting.
[02:05:48.340 --> 02:05:51.460]   Do you think that's why cloud is disappearing then?
[02:05:51.460 --> 02:05:55.100]   Not that there's been a reason for cloud to exist for like the last three or four years.
[02:05:55.100 --> 02:05:56.100]   It may just be.
[02:05:56.100 --> 02:05:58.100]   They said, "Oh, blame it on GDPR."
[02:05:58.100 --> 02:06:01.460]   I mean, for the last couple of weeks, I've been getting emails every day from various
[02:06:01.460 --> 02:06:04.660]   companies, you know, changing updates in their change terms of service.
[02:06:04.660 --> 02:06:05.660]   Same image.
[02:06:05.660 --> 02:06:09.940]   We're updating our terms of service.
[02:06:09.940 --> 02:06:13.700]   For all of that, I'm grateful to the EU, but at the same time, this is a significant
[02:06:13.700 --> 02:06:19.660]   burden to every business with a website, every business that does a mailing list.
[02:06:19.660 --> 02:06:26.100]   You know what, if these companies are going to abuse the data that we've given them, then?
[02:06:26.100 --> 02:06:27.100]   Well, for sure.
[02:06:27.100 --> 02:06:28.100]   Not that.
[02:06:28.100 --> 02:06:30.700]   But it affects companies that don't abuse the data, too.
[02:06:30.700 --> 02:06:31.700]   That's the point.
[02:06:31.700 --> 02:06:32.700]   Tiny companies.
[02:06:32.700 --> 02:06:33.940]   Tiny companies.
[02:06:33.940 --> 02:06:34.940]   My company.
[02:06:34.940 --> 02:06:36.820]   They're going to get a lot of data.
[02:06:36.820 --> 02:06:37.820]   You can't answer that.
[02:06:37.820 --> 02:06:40.220]   Does anybody have a mailing list?
[02:06:40.220 --> 02:06:46.420]   You'd have to read up on GDPR and now make it compliant.
[02:06:46.420 --> 02:06:50.700]   Stuff you probably already do, like letting EU customers delete their information.
[02:06:50.700 --> 02:06:56.900]   But still, there's some debate, and according to GDPR, there's some debate over IP addresses,
[02:06:56.900 --> 02:06:58.740]   whether that's personal information.
[02:06:58.740 --> 02:07:04.220]   GDPR says it is, which means every website practically in the world, which keeps logs,
[02:07:04.220 --> 02:07:08.140]   is going to have to now update its technology.
[02:07:08.140 --> 02:07:10.380]   Google Analytics is going to have to cham me.
[02:07:10.380 --> 02:07:12.220]   This is going to be really interesting.
[02:07:12.220 --> 02:07:15.140]   We'll cover it a little bit more next week, because we're out of time here.
[02:07:15.140 --> 02:07:16.940]   I don't want to keep you.
[02:07:16.940 --> 02:07:18.100]   Thank you very much.
[02:07:18.100 --> 02:07:19.900]   Harry McCracken is always a pleasure.
[02:07:19.900 --> 02:07:22.300]   Fastcompany.com, anything else you want to talk about?
[02:07:22.300 --> 02:07:23.300]   No, that's it.
[02:07:23.300 --> 02:07:24.300]   Come by and say hi.
[02:07:24.300 --> 02:07:26.180]   Such a pleasure to see you.
[02:07:26.180 --> 02:07:28.500]   And Marie, he always brings Marie with him.
[02:07:28.500 --> 02:07:29.500]   Hi, Marie.
[02:07:29.500 --> 02:07:33.780]   And Marie always brings that McDonald guy with her.
[02:07:33.780 --> 02:07:34.780]   Photo on the Twitter.
[02:07:34.780 --> 02:07:37.260]   Did you post some good pictures, Michael?
[02:07:37.260 --> 02:07:39.060]   Michael McDonald is our unofficial--
[02:07:39.060 --> 02:07:40.060]   Oh, Donald.
[02:07:40.060 --> 02:07:41.060]   Michael, oh, Donald.
[02:07:41.060 --> 02:07:42.060]   What did I say, Donald?
[02:07:42.060 --> 02:07:43.060]   Mick.
[02:07:43.060 --> 02:07:44.060]   Yeah, I think I was--
[02:07:44.060 --> 02:07:45.060]   McDonald.
[02:07:45.060 --> 02:07:46.060]   Oh, Donald.
[02:07:46.060 --> 02:07:47.060]   It's because I mentioned McDonald's.
[02:07:47.060 --> 02:07:48.060]   Now you're all thinking about it.
[02:07:48.060 --> 02:07:49.060]   I'm thinking of McDonald's.
[02:07:49.060 --> 02:07:51.140]   Michael, oh, Donald, thank you.
[02:07:51.140 --> 02:07:52.300]   I say it wrong every time.
[02:07:52.300 --> 02:07:55.900]   I've known Michael for 20 years, practically.
[02:07:55.900 --> 02:07:59.540]   And I say your name wrong all the time.
[02:07:59.540 --> 02:08:00.780]   Also thanks to Florence Ion.
[02:08:00.780 --> 02:08:01.940]   I got your name right.
[02:08:01.940 --> 02:08:04.220]   Oh, that flow on Twitter.
[02:08:04.220 --> 02:08:09.500]   You'll see her every Tuesday night with Ron and Jason on all about Android.
[02:08:09.500 --> 02:08:13.300]   And what a great-- if you hadn't listened all about Android this week, you recorded
[02:08:13.300 --> 02:08:14.980]   it as a fantastic interview.
[02:08:14.980 --> 02:08:16.340]   Thank you.
[02:08:16.340 --> 02:08:17.340]   What a great show.
[02:08:17.340 --> 02:08:19.420]   Everybody's got to listen to that.
[02:08:19.420 --> 02:08:20.420]   Thank you.
[02:08:20.420 --> 02:08:21.420]   It was a great show.
[02:08:21.420 --> 02:08:22.420]   It was a great opportunity.
[02:08:22.420 --> 02:08:25.860]   And I'm happy to let us do it again for second year in a row to talk to them and have
[02:08:25.860 --> 02:08:29.180]   them on camera, to talk to Stephanie South, Clothburnson.
[02:08:29.180 --> 02:08:31.900]   You got some-- you get some-- they listen all about Android.
[02:08:31.900 --> 02:08:32.900]   That's why.
[02:08:32.900 --> 02:08:33.900]   They do.
[02:08:33.900 --> 02:08:34.900]   They do.
[02:08:34.900 --> 02:08:35.900]   Which makes me very happy.
[02:08:35.900 --> 02:08:36.900]   I know.
[02:08:36.900 --> 02:08:37.900]   Me too.
[02:08:37.900 --> 02:08:38.900]   It does give me a little bit of--
[02:08:38.900 --> 02:08:39.900]   Yeah, that's nice.
[02:08:39.900 --> 02:08:40.900]   They're probably a material too.
[02:08:40.900 --> 02:08:41.900]   I'm not saying they don't.
[02:08:41.900 --> 02:08:42.900]   We do.
[02:08:42.900 --> 02:08:44.900]   Yeah, we do have some-- a lot of Google developers around.
[02:08:44.900 --> 02:08:47.660]   I think you even got some material alumni at Google.
[02:08:47.660 --> 02:08:48.660]   It's true.
[02:08:48.660 --> 02:08:49.660]   Who?
[02:08:49.660 --> 02:08:50.660]   Yasmine Evian.
[02:08:50.660 --> 02:08:51.660]   Yes.
[02:08:51.660 --> 02:08:52.660]   She went to Google?
[02:08:52.660 --> 02:08:53.660]   Yeah.
[02:08:53.660 --> 02:08:54.660]   She went to Google.
[02:08:54.660 --> 02:08:55.660]   Yeah.
[02:08:55.660 --> 02:08:56.660]   She went to Google.
[02:08:56.660 --> 02:08:57.660]   Well, so you've got an insider now.
[02:08:57.660 --> 02:08:58.660]   You've got somebody inside you.
[02:08:58.660 --> 02:08:59.660]   Crying to me.
[02:08:59.660 --> 02:09:01.660]   I'm trying to put my friends in this position.
[02:09:01.660 --> 02:09:04.180]   Sam, I will--
[02:09:04.180 --> 02:09:05.180]   El-Samed.
[02:09:05.180 --> 02:09:06.740]   Great to have you.
[02:09:06.740 --> 02:09:07.740]   Analysts from Navigant.
[02:09:07.740 --> 02:09:09.540]   Anything you want to plug?
[02:09:09.540 --> 02:09:12.140]   You can find the research reports that we do.
[02:09:12.140 --> 02:09:15.060]   NavigantResearch.com.
[02:09:15.060 --> 02:09:17.500]   We publish syndicated reports that anybody can purchase.
[02:09:17.500 --> 02:09:20.860]   And also, we've got a blog there that all our analysts contribute to.
[02:09:20.860 --> 02:09:21.860]   Nice.
[02:09:21.860 --> 02:09:24.300]   On a wide variety of topics.
[02:09:24.300 --> 02:09:28.980]   And we also do custom research if there's something that you need that we don't have
[02:09:28.980 --> 02:09:30.700]   syndicated report for.
[02:09:30.700 --> 02:09:35.660]   You can find me at Forbes in the auto section and automotive engineering magazine.
[02:09:35.660 --> 02:09:38.060]   And at the podcast, I do with my friend Dan Roth.
[02:09:38.060 --> 02:09:39.380]   OK, let's not get greedy.
[02:09:39.380 --> 02:09:41.740]   You only get a few minutes.
[02:09:41.740 --> 02:09:43.460]   This isn't the Howard Stern Show.
[02:09:43.460 --> 02:09:45.020]   What's the name of the podcast?
[02:09:45.020 --> 02:09:46.020]   With wheel bearings.
[02:09:46.020 --> 02:09:47.020]   It's a wheel bearings.
[02:09:47.020 --> 02:09:48.020]   I like it.
[02:09:48.020 --> 02:09:49.020]   Wheel bearings.media.
[02:09:49.020 --> 02:09:50.020]   You're great.
[02:09:50.020 --> 02:09:51.180]   It really is always a pleasure.
[02:09:51.180 --> 02:09:54.260]   And thanks, Sam brought up, as I said, the Cadillac CT6.
[02:09:54.260 --> 02:09:55.260]   And got to drive it.
[02:09:55.260 --> 02:09:58.940]   We're going to have a piece on that on Saturday on the new screen savers.
[02:09:58.940 --> 02:10:00.380]   Your guys are pretty brave.
[02:10:00.380 --> 02:10:03.140]   You didn't realize this because probably we didn't tell you.
[02:10:03.140 --> 02:10:05.340]   But this was episode 666.
[02:10:05.340 --> 02:10:06.940]   Filmed on the 13th.
[02:10:06.940 --> 02:10:08.580]   Filmed on the 13th.
[02:10:08.580 --> 02:10:09.540]   Out of number play.
[02:10:09.540 --> 02:10:11.340]   The mark of the beast.
[02:10:11.340 --> 02:10:13.500]   I don't know how to believe in that stuff.
[02:10:13.500 --> 02:10:15.300]   I don't even think where did where did that come from?
[02:10:15.300 --> 02:10:16.540]   The 666 thing.
[02:10:16.540 --> 02:10:19.820]   I think it came from the Omen, right?
[02:10:19.820 --> 02:10:22.860]   Something like Hollywood made that up probably.
[02:10:22.860 --> 02:10:23.700]   I think that's real.
[02:10:23.700 --> 02:10:24.060]   It's OK.
[02:10:24.060 --> 02:10:24.820]   I'm into it.
[02:10:24.820 --> 02:10:25.740]   It's all right.
[02:10:25.740 --> 02:10:27.540]   Just give me something.
[02:10:27.540 --> 02:10:32.540]   According to Wikipedia, it's the number following 665.
[02:10:32.540 --> 02:10:33.140]   Oh!
[02:10:33.140 --> 02:10:35.540]   And proceeding 667.
[02:10:35.540 --> 02:10:36.940]   Interesting.
[02:10:36.940 --> 02:10:38.540]   Oh, both of Revelation.
[02:10:38.540 --> 02:10:39.540]   We have the Bible to blame.
[02:10:39.540 --> 02:10:41.540]   It's the number of the beast.
[02:10:41.540 --> 02:10:42.940]   The Bible, we have to blame for that.
[02:10:42.940 --> 02:10:46.540]   I think, though, if you read the actual quote, it's not that clear.
[02:10:46.540 --> 02:10:53.540]   Well, and I think the other thing that I've heard is that there's some debate about the translation from the original Aramaic.
[02:10:53.540 --> 02:10:54.540]   Right?
[02:10:54.540 --> 02:10:55.540]   What the number should actually be.
[02:10:55.540 --> 02:10:56.540]   Yeah.
[02:10:56.540 --> 02:11:01.540]   It could be anything from 665 to 667.
[02:11:01.540 --> 02:11:03.300]   Somewhere in that range.
[02:11:03.300 --> 02:11:04.300]   So maybe next to Mark.
[02:11:04.300 --> 02:11:05.300]   It could be 6.66.
[02:11:05.300 --> 02:11:06.300]   Exactly.
[02:11:06.300 --> 02:11:07.300]   Next week's show.
[02:11:07.300 --> 02:11:08.300]   It could be a freckle there.
[02:11:08.300 --> 02:11:10.780]   Maybe we already did it.
[02:11:10.780 --> 02:11:11.780]   You never know.
[02:11:11.780 --> 02:11:12.780]   Yeah, that's right.
[02:11:12.780 --> 02:11:13.780]   There was a dot on the page.
[02:11:13.780 --> 02:11:14.780]   A little spot ink.
[02:11:14.780 --> 02:11:17.820]   You're two thirds of the way to a thousand.
[02:11:17.820 --> 02:11:19.300]   There you go.
[02:11:19.300 --> 02:11:22.460]   I promised to retire on episode a thousand.
[02:11:22.460 --> 02:11:25.500]   So I'm two thirds of the way to retirement.
[02:11:25.500 --> 02:11:27.540]   No, actually, I don't think I will.
[02:11:27.540 --> 02:11:29.420]   I changed my mind.
[02:11:29.420 --> 02:11:30.420]   Thank you for being here.
[02:11:30.420 --> 02:11:31.420]   We appreciate it.
[02:11:31.420 --> 02:11:32.420]   We had a great studio audience.
[02:11:32.420 --> 02:11:34.700]   As I said, from all over the world, you guys were fantastic.
[02:11:34.700 --> 02:11:36.660]   If you want to be in the studio audience, easy to do.
[02:11:36.660 --> 02:11:38.300]   Just email tickets@twit.tv.
[02:11:38.300 --> 02:11:40.180]   Those are free.
[02:11:40.180 --> 02:11:42.660]   And we'd love to have you in the studio.
[02:11:42.660 --> 02:11:47.300]   If you're watching live, you can watch the stream at twit.tv/live.
[02:11:47.300 --> 02:11:50.220]   If you do that, you should also be in the chat room.
[02:11:50.220 --> 02:11:51.220]   Right, Ozzanette?
[02:11:51.220 --> 02:11:52.220]   Right.
[02:11:52.220 --> 02:12:00.900]   If you're watching live on demand versions of all of our shows that are available at the
[02:12:00.900 --> 02:12:06.300]   website, audio and video, I know it's easier to listen to audio, but if you have time, sometimes
[02:12:06.300 --> 02:12:07.740]   it's fun to watch.
[02:12:07.740 --> 02:12:09.980]   We make them both available.
[02:12:09.980 --> 02:12:16.100]   You can also subscribe in your favorite pod catcher over casts, pocket casts, iTunes,
[02:12:16.100 --> 02:12:17.100]   Google Play.
[02:12:17.100 --> 02:12:18.780]   In fact, subscribing is the best.
[02:12:18.780 --> 02:12:19.940]   That way you'll get every episode.
[02:12:19.940 --> 02:12:20.940]   Thanks for being here.
[02:12:20.940 --> 02:12:21.940]   We'll see you next time.
[02:12:21.940 --> 02:12:22.940]   Bye bye.
[02:12:22.940 --> 02:12:23.940]   Happy Mother's Day.
[02:12:23.940 --> 02:12:25.460]   Happy Mother's Day, Mom.
[02:12:25.460 --> 02:12:27.220]   Another Twitch is in the cam.
[02:12:27.220 --> 02:12:27.660]   Bye bye.
[02:12:27.660 --> 02:12:36.660]   [Music]

