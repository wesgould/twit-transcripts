;FFMETADATA1
title=The Mafia is Zooming
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=774
genre=Podcast
comment=https://twit.tv/twit
copyright=These podcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.440]   It's time for Twit this week in Tech. We have a wonderful panel for you. Dylan Twini
[00:00:04.440 --> 00:00:10.500]   joins us from Vala Mail. We've got our friend Wesley Faulkner from Austin, Texas and Futurist
[00:00:10.500 --> 00:00:16.280]   Amy Webb. We'll be talking of course about this very difficult week in Tech. The response
[00:00:16.280 --> 00:00:22.720]   from Tim Cook and Sacha Nadella and why Wesley thinks Sacha got it right. We'll talk about
[00:00:22.720 --> 00:00:28.960]   improving policing and the surprising choice just made by the Minneapolis City Council,
[00:00:28.960 --> 00:00:35.720]   social and organizational barriers to change and fishing scams attempted to take down both
[00:00:35.720 --> 00:00:41.240]   the Trump and the Biden campaign. It's all coming up next on Twit. This week in Tech
[00:00:41.240 --> 00:00:45.600]   comes to you from Twit's LastPass Studios. Stay in control when it comes to your company's
[00:00:45.600 --> 00:00:50.840]   access points and authentication. LastPass makes enterprise level security simple for
[00:00:50.840 --> 00:00:57.480]   your remote workforce. Check out lastpass.com/twit to learn more.
[00:00:57.480 --> 00:01:19.360]   This is Twit. This is Twit. This week in Tech, episode 774, recorded Sunday, June 7th, 2020.
[00:01:19.360 --> 00:01:26.320]   The mafia is zooming. This episode of This Week in Tech is brought to you by Mint Mobile.
[00:01:26.320 --> 00:01:30.400]   They provide the same premium network coverage you're used to, but at a fraction of the
[00:01:30.400 --> 00:01:34.600]   costs because everything is online. Mint Mobile makes it easy to cut your wireless
[00:01:34.600 --> 00:01:39.600]   build down to just $15 a month with their three-month introductory plan and get the
[00:01:39.600 --> 00:01:45.520]   plan shipped to your door free at mintmobile.com/twit.
[00:01:45.520 --> 00:01:52.800]   And by ExtraHop. ExtraHop helps you keep your business secure and available with SaaS-based
[00:01:52.800 --> 00:01:57.600]   cloud native network detection and response. Get tips on securing and supporting remote
[00:01:57.600 --> 00:02:04.400]   access or check out the full product demo at extrahop.com/twit.
[00:02:04.400 --> 00:02:11.720]   And by Barracuda. Did you know that 91% of all cyber attacks start with an email? To
[00:02:11.720 --> 00:02:16.680]   uncover the threats hiding in your Office 365 account, get a secure and free email
[00:02:16.680 --> 00:02:27.640]   threat scan at barracuda.com/twit.
[00:02:27.640 --> 00:02:33.560]   It's time for Twit. This week in Tech, to show we cover the week's tech news. We have
[00:02:33.560 --> 00:02:39.440]   assembled a fine panel today starting with one of my favorite people. Actually, I love
[00:02:39.440 --> 00:02:44.560]   all three. I love Amy Webb though. She's so great talking about the future from the Future
[00:02:44.560 --> 00:02:50.160]   today Institute. Amy Webb.io, you're working on a new book. I know I see the signals are
[00:02:50.160 --> 00:02:59.440]   talking behind you right there. And your most recent one about the, I always want to call
[00:02:59.440 --> 00:03:05.000]   them the batfangs. I don't know why. They could be. They could be about the-
[00:03:05.000 --> 00:03:11.960]   The GMafia and the bat. The GMafia. Yeah, the big tech companies. Thanks for being here,
[00:03:11.960 --> 00:03:17.640]   Amy. Great to have you. Thank you. Also with us, Wesley Faulkner. It's great to see you,
[00:03:17.640 --> 00:03:24.840]   Wesley, Amy and I both hung with you in Austin at Southbike. And are you keeping a large
[00:03:24.840 --> 00:03:31.600]   marsupial in a cage behind you? What is- That is a bearded dragon named Pancake. Pancake
[00:03:31.600 --> 00:03:38.680]   the bearded dragon. Yes. Well, I will see how big is your bearded dragon? She's fairly
[00:03:38.680 --> 00:03:43.880]   large. I would say, not that big. Well, she make an appearance, you think? Or is she hiding?
[00:03:43.880 --> 00:03:48.680]   Oh, no. I mean, I could pull her out. No, no, no. I can do that later. Hopefully, I'm
[00:03:48.680 --> 00:03:58.480]   scared. Just keep her in the cage, please. What a bearded dragon's eat. Crickets. Oh.
[00:03:58.480 --> 00:04:05.560]   Worms, millworms. The actual eight vegetables and fruit too. Okay. But we also feed them
[00:04:05.560 --> 00:04:13.880]   do via roaches too. Not kings landing or anything like that. Not the no, not the entire ice wall,
[00:04:13.880 --> 00:04:17.720]   nothing like that. Okay, good. No. All right. It's great to have you, Wesley. Thank you for
[00:04:17.720 --> 00:04:21.600]   being here. Thanks for having me. We also welcome and it's been a while, I think, since
[00:04:21.600 --> 00:04:27.880]   we've seen Dylan Tweeny. Dylan, of course, VP communications at Valamail, longtime tech
[00:04:27.880 --> 00:04:34.360]   journalist. It's good to see you. How's it going, Leo? Dylan, you're in the Bay Area, right?
[00:04:34.360 --> 00:04:39.680]   I am. Yeah. Just south of San Francisco. Okay. San Mateo. Bye. Wesley's in Austin and Amy
[00:04:39.680 --> 00:04:45.320]   today is in Baltimore, although she has homes all over the country, apparently. No, just
[00:04:45.320 --> 00:04:51.040]   New York and Baltimore. Hey, I would love to be able to say I'm in my New York home this
[00:04:51.040 --> 00:04:59.720]   week. We, of course, come to you as always. It seems this year and a difficult week. And
[00:04:59.720 --> 00:05:06.240]   just, you know, it's hard. It's hard to know what to say. I think I really liked what Tim
[00:05:06.240 --> 00:05:14.920]   Cook wrote. Tim, Tim's been really pretty good about the crisis this nation's been going
[00:05:14.920 --> 00:05:20.920]   through starting with the pandemic. Then, of course, the horrific death of George Floyd.
[00:05:20.920 --> 00:05:26.680]   And he most recently put a note and actually put it on Apple's webpage. The last thing
[00:05:26.680 --> 00:05:32.160]   we saw from him was in house and it leaked out, but I thought it was beautifully written.
[00:05:32.160 --> 00:05:40.800]   This one is posted at apple.com/speakinguponracism. And I think, I don't know if you guys have
[00:05:40.800 --> 00:05:47.960]   read it, but I think it speaks from the heart and it speaks beautifully about the situation
[00:05:47.960 --> 00:05:54.720]   we're in today. It's a tough thing for a CEO because I think some people would say, you
[00:05:54.720 --> 00:06:01.880]   know, hey, Tim, worry about your business. Don't worry about the nation. But I think Apple
[00:06:01.880 --> 00:06:07.280]   is a big company employing a lot of people, including many black people. And I think
[00:06:07.280 --> 00:06:11.600]   it's important for him to step up. He's always done that. It's really remarkable. I can't
[00:06:11.600 --> 00:06:16.920]   imagine Steve Jobs running a note like this. Steve Jobs was glad to do open letters against
[00:06:16.920 --> 00:06:25.960]   flash, but not against racism. As difficult as it may be to admit, this is a moment when
[00:06:25.960 --> 00:06:30.120]   many people may want nothing more than a return to normalcy or to a status quo that is only
[00:06:30.120 --> 00:06:34.800]   comfortable if we avert our gaze from injustice. He's right. It's difficult that it may be
[00:06:34.800 --> 00:06:40.640]   to admit that desire is itself a sign of privilege. George Floyd's death is shocking and tragic
[00:06:40.640 --> 00:06:45.600]   proof. We must aim far higher than a normal future and build one that lives up to the
[00:06:45.600 --> 00:06:51.920]   highest ideals of equality and justice. With every breath we take, we must commit to being
[00:06:51.920 --> 00:06:57.160]   that change and to create a better, more just world for everyone. I asked you, Wesley,
[00:06:57.160 --> 00:07:01.960]   when we first started before the show began, how you're feeling, if you were at all hopeful
[00:07:01.960 --> 00:07:09.720]   or optimistic, and you said not really. The letter is nice. I think that points to it.
[00:07:09.720 --> 00:07:19.200]   The A+ letter goes for Satya Nadella from Microsoft. I think that one is the best because
[00:07:19.200 --> 00:07:25.920]   it's not put on the individual saying, "We must do better," saying, "You do better."
[00:07:25.920 --> 00:07:31.200]   It's specifically Microsoft needs to do better and we will do better internally and then shine
[00:07:31.200 --> 00:07:36.080]   as an example externally. I think that is more of a powerful statement than I've heard from
[00:07:36.080 --> 00:07:44.080]   anybody because if you feel that your organization, that racism is not a problem where you work,
[00:07:44.080 --> 00:07:50.200]   then I think you're not really focused. If you say, "I need to do better," without actually
[00:07:50.200 --> 00:07:54.600]   putting forth efforts to change and be more concrete about what you're going to change
[00:07:54.600 --> 00:07:59.680]   as a company, that is another thing. If you're saying, "I'm going to give $10 million to
[00:07:59.680 --> 00:08:03.560]   this organization," what you should do is just set aside that money in your budget and just
[00:08:03.560 --> 00:08:07.240]   saying, "We are going to go through some pain. We're going to go through some anguish and
[00:08:07.240 --> 00:08:12.520]   we're going to have to restructure and that's going to cost money." We're going to do that.
[00:08:12.520 --> 00:08:20.440]   I think some of it is like, "Well, we could hire from this place or that place, but they
[00:08:20.440 --> 00:08:27.280]   don't get up the speed soon enough or something like that." Put some more money in the training.
[00:08:27.280 --> 00:08:34.360]   Put some more money into your internal efforts to make sure that if you don't see that racism
[00:08:34.360 --> 00:08:39.440]   in your organization, find it by putting money in your organization to find it. Listen to
[00:08:39.440 --> 00:08:45.440]   complaints. Let people speak. Let people say what's wrong. Having a listen session is great,
[00:08:45.440 --> 00:08:49.880]   but turning that around and just saying, "Okay, this is what we heard. These are the recommendations
[00:08:49.880 --> 00:08:56.320]   and this is the plan for implementing them." That's what you need to do if you're really
[00:08:56.320 --> 00:09:01.080]   interested in standing with people who are oppressed by the system.
[00:09:01.080 --> 00:09:09.200]   I would argue there's one more thing to do and that is to hire people of color and women.
[00:09:09.200 --> 00:09:15.400]   Five years ago, I think in October, Google, Facebook, Apple, Microsoft all released their
[00:09:15.400 --> 00:09:26.040]   diversity reports. They made promises that things would change. Basically, since the numbers
[00:09:26.040 --> 00:09:32.920]   were public and people started tracking, there's been less than a percentage point bump for
[00:09:32.920 --> 00:09:39.400]   black and Latinx engineers and workers at a lot of these companies. While I appreciate
[00:09:39.400 --> 00:09:46.320]   the letters that have gone out this week, there has been no substantive change whatsoever
[00:09:46.320 --> 00:09:55.480]   in five years at any of these companies and their hiring practices. In addition to the
[00:09:55.480 --> 00:10:03.960]   fact that we know that there is bias and there are problems endemic within all of these organizations,
[00:10:03.960 --> 00:10:12.000]   but if you don't change the fabric, then it's really hard to change outcomes. If you're
[00:10:12.000 --> 00:10:17.000]   not being the change that you want to see within your organization, I think it's a little
[00:10:17.000 --> 00:10:27.360]   challenging to read these letters and to assume that they really do mean something beyond PR.
[00:10:27.360 --> 00:10:31.680]   I look at Microsoft's letter. I look at the letter from Tim and Sakiya and I contrast
[00:10:31.680 --> 00:10:35.320]   that with what I saw out of Ben and Jerry's, which to be fair, Ben and Jerry's has a long
[00:10:35.320 --> 00:10:41.720]   history of taking stands. Ben and Jerry's is now owned by Unilever. What they wrote that
[00:10:41.720 --> 00:10:49.880]   had specific actions and ways to hold Ben and Jerry's accountable is a different ball game.
[00:10:49.880 --> 00:11:01.720]   It's quite different. They made an ice cream. There's some real commitment. Actually, there's
[00:11:01.720 --> 00:11:07.800]   more than just hiring though because a number of companies have taken some hits because they
[00:11:07.800 --> 00:11:13.440]   talk, well, Amazon's a good example where they stand in solid area against police racism,
[00:11:13.440 --> 00:11:17.560]   but then they're selling face recognition technology to the police that is blatantly
[00:11:17.560 --> 00:11:27.560]   racist. That's because they're not listening to the people. The people who are focused on
[00:11:27.560 --> 00:11:31.920]   equity in that company do not have enough teeth, do not have enough power to actually
[00:11:31.920 --> 00:11:36.840]   affect change. A lot of people say they value diversity of opinion and thought, but when
[00:11:36.840 --> 00:11:40.760]   it comes to, well, that's going to cost us a lot of money. Like I said, that kind of wins
[00:11:40.760 --> 00:11:47.960]   out and prevents companies from taking that action to say no to that money, say no to
[00:11:47.960 --> 00:11:54.440]   that contract, to divest in the places, even though it might be more lucrative, as a whole
[00:11:54.440 --> 00:12:00.760]   for society, it could be very detrimental. It's easy to use Amazon for this because they
[00:12:00.760 --> 00:12:06.360]   constantly pay lip service to the right thing while doing the wrong thing. It seems to me.
[00:12:06.360 --> 00:12:11.800]   They fire people for trying to organize or make these things. Of course, they don't have anybody
[00:12:11.800 --> 00:12:14.840]   who has a voice in their company because they force them all out.
[00:12:14.840 --> 00:12:23.160]   They just took away the $2 hazard pay for their warehouse. They made T-shirts. They made T-shirts
[00:12:23.160 --> 00:12:30.280]   that said, thank you for risking your life to deliver Chachiki's to Americans. There's that.
[00:12:30.280 --> 00:12:36.200]   Yeah, it's very tough to turn those T-shirts in afterwards. It's over. It's challenging because
[00:12:36.200 --> 00:12:44.840]   I think it started with COVID-19 and then is accelerated by George Floyd's murder.
[00:12:44.840 --> 00:12:51.560]   There have been fractures in the American society, very deep fractures in income,
[00:12:51.560 --> 00:13:00.040]   inequality and racism, systemic bias, and runaway capitalism for decades.
[00:13:00.040 --> 00:13:08.200]   But it seems to have all come to a head. The pandemic started. The racist murders really
[00:13:08.200 --> 00:13:14.600]   put a cap on it. Now Americans are taking to the street. It feels like we're at an inflection point
[00:13:14.600 --> 00:13:17.960]   that we have a choice now. We can go one way or the other.
[00:13:19.880 --> 00:13:27.320]   It feels like we're at that moment in Civ or SimCity. I made so many bad decisions.
[00:13:27.320 --> 00:13:33.480]   There's no way not taking all of this. It's time to find the cheat codes or I just need to reboot.
[00:13:33.480 --> 00:13:37.480]   I hope that's not the case because there is no reset button.
[00:13:37.480 --> 00:13:43.400]   Well, the calls for defunding cops and police organizations is kind of like a reset. They're
[00:13:43.400 --> 00:13:49.000]   saying, let's just get rid of them and just start over. Please explain that to me.
[00:13:49.000 --> 00:13:54.040]   Now this isn't a technical subject, so we'll do it quickly. But don't we need some policing?
[00:13:54.040 --> 00:14:05.640]   I think it's extremely confusing. I'll just say that. My take on it is that there should be one
[00:14:05.640 --> 00:14:10.680]   department that says, I need someone here with a gun that can kill someone. If there's a police
[00:14:10.680 --> 00:14:15.800]   accident, you don't need to call that group. You call another civil group. If there's a report
[00:14:15.800 --> 00:14:20.040]   that needs to be made, you don't call the people with the guns that can kill people.
[00:14:20.040 --> 00:14:24.920]   There should be a call it something else. There's a lot of police work that can be done without
[00:14:24.920 --> 00:14:32.520]   life threatening force. Right. We call it police work now because the police is the default.
[00:14:32.520 --> 00:14:37.800]   Just make new organizations and new groups that can tackle those other things that don't require
[00:14:37.800 --> 00:14:43.960]   a person with a gun to kill someone that can kill someone. And that's what's needed. Just
[00:14:43.960 --> 00:14:48.760]   make it up. And so you don't say call the police. You say call and then fill in the blank of whatever
[00:14:48.760 --> 00:14:52.840]   that's called. That is kind of hitting the reset button because it's a whole different way of
[00:14:52.840 --> 00:14:59.560]   thinking about government interaction with problems. It's a different idea. But you're right, lethal
[00:14:59.560 --> 00:15:06.760]   force is not a necessity in probably 99%, maybe 99.9% of what law enforcement does.
[00:15:06.760 --> 00:15:11.640]   And you hear these cops who retire, who have never discharged their weapons.
[00:15:11.640 --> 00:15:18.360]   So even those who are trained don't necessarily need to use that because of that. I mean,
[00:15:18.360 --> 00:15:23.080]   if you look at police departments, look how they are trained, how they are indoctrinated,
[00:15:23.080 --> 00:15:29.480]   they are only really trained the most on the situations that is the rarest and the most.
[00:15:29.480 --> 00:15:34.280]   But those are life threatening. Those are the dangerous situations.
[00:15:34.280 --> 00:15:40.920]   You do want somebody to handle those. But do you want someone who's trained to fear for their
[00:15:40.920 --> 00:15:45.480]   life to act first there for all situations? I don't think so.
[00:15:45.480 --> 00:15:51.400]   My friend, I have a friend who's in code enforcement, but he carries a weapon.
[00:15:51.400 --> 00:15:56.920]   And I guess it's good that you can shoot a hoarder if you run into him, but probably doesn't need
[00:15:56.920 --> 00:16:02.440]   it most of the time. I don't know. I'll have to ask JT. All right. That's not a tech subject,
[00:16:02.440 --> 00:16:07.000]   but since you mentioned it, I was just curious because that is one of the calls to defund the
[00:16:07.000 --> 00:16:12.040]   police, but it's not necessarily eliminate all policing, but just to rethink what policing is
[00:16:12.040 --> 00:16:15.480]   and what it needs to be. Yeah, and replace them with something that's a special.
[00:16:15.480 --> 00:16:19.240]   How is that tech subject? I'm just going to say, you know,
[00:16:19.240 --> 00:16:25.640]   so I want to say a couple of things. And I'm also sort of, I always am on the IRC channel
[00:16:25.640 --> 00:16:29.800]   when we do this. So I think it's important to point out
[00:16:32.280 --> 00:16:38.200]   that all of the people on the show are not super, super left-leaning liberals. That is not me.
[00:16:38.200 --> 00:16:44.280]   So I am politically independent. If anything, I feel disenfranchised and like I don't have a party
[00:16:44.280 --> 00:16:51.000]   that represents what I believe. But this is important because, you know, I think that the problems that
[00:16:51.000 --> 00:16:56.840]   we're in today are because sometimes when we plan for the future, we get stuck with only the signals
[00:16:56.840 --> 00:17:04.200]   from that we have from the present. And so, you know, it's the year 2020. I wonder why
[00:17:04.200 --> 00:17:11.240]   our police departments don't have better technology. The weapons that they use are in a lot of ways
[00:17:11.240 --> 00:17:21.320]   very primitive, you know, very brute force. Very, yeah, you know, and I know that we have access to
[00:17:21.320 --> 00:17:27.240]   smarter, better technologies. I know that we have better access to smarter, better
[00:17:27.240 --> 00:17:33.800]   tools. They're just not being deployed. You can't defund all of the police departments
[00:17:33.800 --> 00:17:39.400]   because there are negative next-order consequences to that, just as there are
[00:17:39.400 --> 00:17:45.880]   negative next-order consequences to the situation that we're in right now. It seems to me like
[00:17:46.840 --> 00:17:52.840]   if we're not going to reboot, then now is a really good time to stop and sort of think through
[00:17:52.840 --> 00:17:58.840]   what are all of the alternative futures. The challenge that we're in is that the conversations
[00:17:58.840 --> 00:18:05.160]   become really binary and it could become technical. We could have far-ranging conversations about
[00:18:05.160 --> 00:18:10.680]   all types of alternative futures in which we are using and deploying technologies that are
[00:18:10.680 --> 00:18:19.960]   still respectful of privacy and, you know, people's human rights while also deterring
[00:18:19.960 --> 00:18:24.680]   theft and crime and everything else as it's happening in a more reasonable way.
[00:18:24.680 --> 00:18:31.880]   But now is a good time to have that conversation. The challenge is that as with every other industry,
[00:18:31.880 --> 00:18:38.680]   there's a ton of sort of embedded cultural norms and cherished beliefs that people are
[00:18:38.680 --> 00:18:42.440]   not willing to confront. If they were willing to confront their cherished beliefs,
[00:18:42.440 --> 00:18:49.400]   then we would not be on week two of protests. We would have seen some immediate actions.
[00:18:49.400 --> 00:18:57.800]   We would have seen changes versus promises. So I think the opportunity here-
[00:18:57.800 --> 00:19:05.160]   As a futurist, why is it so hard for us to think out of the box? We always, when we try to fix this
[00:19:05.160 --> 00:19:13.160]   stuff, do it in incremental ways that are clearly insufficient. Oh, let's not use face recognition
[00:19:13.160 --> 00:19:20.440]   because that's racist. Why is it so hard for us to be innovative? Is it government moves too slowly?
[00:19:20.440 --> 00:19:28.360]   You've obviously studied this. Yeah, I mean, this is what I do. So there's a biological reason,
[00:19:28.360 --> 00:19:33.240]   and then there's an organizational reason. The biological reason is that our brains are wired
[00:19:33.240 --> 00:19:37.800]   against change. It takes a tremendous amount of energy to recognize a new pattern,
[00:19:37.800 --> 00:19:43.160]   and there's a fear associated with that. Therefore, we do not confront uncertainty unless
[00:19:43.160 --> 00:19:48.520]   absolutely forced to. And once that happens, it's usually under duress. So the better thing to do
[00:19:48.520 --> 00:19:55.240]   is to be willing to always consider alternate futures that may or may not align with what you
[00:19:55.240 --> 00:20:01.240]   believe and what you're observing in the present. But that takes everybody being willing to do that,
[00:20:01.240 --> 00:20:08.120]   and by necessity, in the process of doing that, you are going to have to probably give up something
[00:20:08.120 --> 00:20:13.000]   in the short term if you want to affect change. So that's part of it. But the other part of it
[00:20:13.000 --> 00:20:23.800]   is structural and organizational. We have the guy that was that knelt on George Floyd's neck
[00:20:23.800 --> 00:20:30.600]   had been how many times disciplined? 19 disciplinary actions. Right. So we have a
[00:20:30.600 --> 00:20:40.440]   organizational problem where change is not... Yeah, but in the news, the reaction you get to that
[00:20:40.440 --> 00:20:44.280]   as well, the problem is the union, the police unions, we got to do something about that.
[00:20:44.280 --> 00:20:49.960]   And that seems to me to be that kind of incremental change instead of rethinking what you're just
[00:20:49.960 --> 00:20:55.000]   talking about, Wesley, the idea of maybe policing isn't the right answer for most of these situations.
[00:20:55.000 --> 00:21:04.680]   It's not merely changing the police union. So can we raise kids that are more flexible in this way?
[00:21:04.680 --> 00:21:13.160]   Is it possible to teach people to be more flexible? I think there's a technical solution to some of
[00:21:13.160 --> 00:21:19.720]   this. But you have a structural problem and you have a tactical problem. The tactical problem is
[00:21:19.720 --> 00:21:25.800]   solved with better technology than guns. But there's an emotional attachment. Or rubber bullets,
[00:21:25.800 --> 00:21:30.520]   by the way, which aren't so non-lethal-easier. I mean, there's like a thousand different
[00:21:30.520 --> 00:21:34.600]   ways. There's a thousand different alternative futures in which we're able to deter people in
[00:21:34.600 --> 00:21:39.400]   a meaningful way without hurting, without substantively hurting them. We're just chosen not to do that.
[00:21:39.400 --> 00:21:42.760]   And if you pitch that idea to any law enforcement agency, it's going to be a no.
[00:21:42.760 --> 00:21:50.040]   Come up with a thousand different reasons. But it's always going to be no. And the truth is that
[00:21:50.040 --> 00:21:57.320]   there's always plausible, better ways of doing anything that you're doing at any time. The other
[00:21:57.320 --> 00:22:01.240]   issue is structural. And that's a much longer, bigger conversation.
[00:22:01.240 --> 00:22:05.880]   Yeah. To get a product release every so often, just like pet companies.
[00:22:06.600 --> 00:22:10.520]   This is how we're chasing the police? Government's 2.0. Yeah. And just
[00:22:10.520 --> 00:22:16.360]   Dylan, you were going to say something. Yeah, yeah. I really like the idea that we need to have
[00:22:16.360 --> 00:22:21.880]   conversations that are more meaningful than just binary. But where do we have those conversations?
[00:22:21.880 --> 00:22:26.520]   Like it sure isn't happening on Twitter and Facebook. Well, let's talk about Twitter and
[00:22:26.520 --> 00:22:31.880]   Facebook because that's also a big part of the news this week, particularly Facebook. We'll take
[00:22:31.880 --> 00:22:36.520]   a little break and we will get into that. And I apologize. I know a lot of people tune in
[00:22:36.520 --> 00:22:44.840]   to our shows on the network to escape from reality. But sometimes reality is so vicious,
[00:22:44.840 --> 00:22:50.680]   so present, so important that we can't come. We can't really hide our heads.
[00:22:50.680 --> 00:22:54.760]   So there are times on the show when we're going to just have to talk about difficult things in life
[00:22:54.760 --> 00:23:00.760]   that are all around us. And I apologize if you've been looking for escape.
[00:23:01.640 --> 00:23:08.520]   There is no escape. Sorry. You're in it now. But fortunately, we have really great voices
[00:23:08.520 --> 00:23:14.440]   to help us through this, including Wesley Faulkner. Who, by the way, besides being black,
[00:23:14.440 --> 00:23:20.840]   is looking for work developer relations? If anybody, you know, a company like Microsoft wants to really
[00:23:20.840 --> 00:23:27.240]   stand up and say, yeah, okay, fine. Wesley's really good at what he does. And he's a good
[00:23:27.240 --> 00:23:34.440]   man. And if you're looking to a Swayze your guilt, then there you go. There you go. Help.
[00:23:34.440 --> 00:23:40.360]   You know, I have to say, this was some years ago, I met the woman that Microsoft had hired
[00:23:40.360 --> 00:23:48.360]   for accessibility. And they had given her a very wide ranging portfolio. She was able to go into
[00:23:48.360 --> 00:23:53.000]   every single department and say, you need to fix this for blind people. You need to fix this for
[00:23:53.000 --> 00:23:58.360]   you need to help people. And that's the kind of thing you need to do. You need to say, this is so
[00:23:58.360 --> 00:24:03.640]   important. We're going to have, we're going to make systemic change in our company. And I think
[00:24:03.640 --> 00:24:08.520]   you're right, Wesley, when you said the best thing that such Nadella said is, this is something
[00:24:08.520 --> 00:24:15.400]   we're going to do, we have to do as a company. It's not on you. It's not on the world. We're
[00:24:15.400 --> 00:24:19.240]   going to fix this at Microsoft. And then everybody else should do it at their companies. I think
[00:24:19.240 --> 00:24:24.440]   that's a very good point of view. And you can start by giving this. And last week, they had
[00:24:24.440 --> 00:24:30.520]   accessibly summit 2020. Yeah. And the same thing, like I was saying, they have a release cycle. This
[00:24:30.520 --> 00:24:35.320]   is what we're doing in our products to affect change and make it more accessible. We need to see
[00:24:35.320 --> 00:24:41.880]   that every, we need to see that every year for for a regulations for for sexism. I think that
[00:24:41.880 --> 00:24:46.280]   that's something every company could stand up for. What have we done this year? That's really good.
[00:24:46.280 --> 00:24:54.040]   And what are we going to do in 2021? Dylan Twini is also here. Vala mail is a great solution for
[00:24:54.040 --> 00:25:01.560]   a broken system called email to actually say that the mail came from who you said it's coming from.
[00:25:01.560 --> 00:25:05.400]   I love that. It's great to have you Dylan. And of course, Amy Webb, our futurist,
[00:25:05.400 --> 00:25:13.880]   Amy Webb.io, the future today Institute. We'll talk about Facebook when we come back. There's a
[00:25:13.880 --> 00:25:19.160]   lot to say. And I'm really, this is another talk about intractable problems. This is another one.
[00:25:19.160 --> 00:25:24.360]   Our show today brought to you by Mint Mobile. I love my Mint Mobile phone. They sent me a
[00:25:24.360 --> 00:25:31.400]   iPhone SE and I put the Mint Mobile Sim in it. It's a perfect combination. My iPhone 10 had been
[00:25:31.400 --> 00:25:35.480]   broken. And when I got it back, I said, this thing weighs 80 pounds. I'm going back to it.
[00:25:35.480 --> 00:25:39.960]   And it costs a hundred bucks a month. I'm going back to Mint Mobile. Mint Mobile will cut
[00:25:40.840 --> 00:25:49.800]   your cell phone bill significantly by a huge amount with the same great service
[00:25:49.800 --> 00:25:55.800]   at a fraction of the cost. I got my first cell phone with one of the big wireless providers back
[00:25:55.800 --> 00:26:00.920]   when I was, you know, I don't know, maybe 20 years ago. Those high monthly bills, though,
[00:26:00.920 --> 00:26:06.280]   180 bucks a month, and they keep going up, by the way, 90 bucks a month, 100 bucks a month.
[00:26:06.280 --> 00:26:12.040]   Mint Mobile has cut my wireless bill down. Well, I have the, I have a plan that's
[00:26:12.040 --> 00:26:16.600]   gives me, it's for 25 bucks a month. You can go as low as 15 bucks a month, save
[00:26:16.600 --> 00:26:23.320]   like hundreds of dollars every year. And they do it without sacrificing service.
[00:26:23.320 --> 00:26:28.680]   They're using T-Mobile. They're an MVNO. So you get the same great service, unlimited,
[00:26:28.680 --> 00:26:34.520]   text nationwide, calling, and then you buy the amount of data you're going to need. The $15
[00:26:34.520 --> 00:26:39.000]   plan is their three month introductory plan. You should absolutely start with that.
[00:26:39.000 --> 00:26:45.240]   Unlimited nationwide talking text, crazy fast 4G LTE. I started with that. I liked it so much.
[00:26:45.240 --> 00:26:52.440]   I went for the 25 buck plan, 12 gigabytes of data a month. I never use it up. And that's
[00:26:52.440 --> 00:27:00.120]   literally less than a third what I was paying for my T-Mobile service. You can bring your own phone,
[00:27:00.120 --> 00:27:03.800]   port your number over if you want, along with all your existing contacts. They have phones too.
[00:27:03.800 --> 00:27:08.680]   In fact, they have a great deal on the iPhone SE. Premium wireless, 15 bucks a month. And of
[00:27:08.680 --> 00:27:13.320]   course, if you're not 100% satisfied, Mint Mobile has you covered with their seven day money back
[00:27:13.320 --> 00:27:17.960]   guarantee. You'll be cutting your wireless bill to $15 a month with their three month introductory
[00:27:17.960 --> 00:27:23.400]   plan. It's such a great deal. Have them send you the SIM. There's no charge, $15 a month,
[00:27:23.400 --> 00:27:29.880]   three month introductory plan. You get three gigabytes a month of data. Mintmobile.com/twit.
[00:27:29.880 --> 00:27:35.880]   I liked it so much. I went for the 25 buck a month. I paid $300 for a year. I just love it. I haven't
[00:27:35.880 --> 00:27:43.480]   had a bill from them. I won't get a bill from them until next year. I love it. Mintmobile.com/twit.
[00:27:43.480 --> 00:27:49.320]   You got to love the Fox. And don't you love Ryan Reynolds ads for Mint Mobile on TV if you've seen
[00:27:49.320 --> 00:27:57.240]   them. They're hysterical. Mintmobile.com/twit. We thank them for their support of this week in
[00:27:58.360 --> 00:28:06.360]   tech. Facebook. I tell you what, we talk about this a lot. It's kind of coming to a head.
[00:28:06.360 --> 00:28:14.840]   Twitter decided that enough is enough. They labeled a couple of the president's tweets
[00:28:14.840 --> 00:28:19.640]   as misleading. And then they labeled another tweet as glorifying violence.
[00:28:19.640 --> 00:28:25.480]   The tweet about when the looting starts, the shooting starts. Yick. Mark Zuckerberg decided,
[00:28:25.480 --> 00:28:33.080]   no, not going to do it. And boy, did that cause a lot of problems. Facebook employees
[00:28:33.080 --> 00:28:39.240]   staged a virtual walkout. Although I don't know a virtual. I mean, what about a real walkout?
[00:28:39.240 --> 00:28:46.120]   Virtual walkout. They did lose some employees. But for last Monday,
[00:28:46.120 --> 00:28:49.800]   hundreds of Facebook employees refused to work. So we're not going to do it.
[00:28:51.000 --> 00:28:56.760]   And it did get Mark Zuckerberg's attention. He did say, okay, okay, we're going to do something
[00:28:56.760 --> 00:29:05.320]   about this. For a long time, he's defended free speech on the platform. I have to say,
[00:29:05.320 --> 00:29:13.720]   I was very intrigued by Ben Thompson's post on Stretecory. Because he said, I believe Facebook
[00:29:13.720 --> 00:29:20.120]   is doing the right thing by leaving Trump's post ups, post ups, and Twitter is doing the wrong
[00:29:20.120 --> 00:29:26.680]   thing by attaching facts, checked labels or obscuring them behind a warning label, both of which Twitter
[00:29:26.680 --> 00:29:33.640]   has done. He says, interfering with the public utterances of the president is bad for the tech
[00:29:33.640 --> 00:29:40.120]   industry broadly, bad for social networks specifically. But I don't really, that does not resume it
[00:29:40.120 --> 00:29:46.280]   with me. But the second point kind of does deciding what should or should not be fact checked is a
[00:29:46.280 --> 00:29:55.720]   logistical nightmare. And I would say also philosophically very challenging. He quoted Jack Dorsey's
[00:29:55.720 --> 00:30:00.920]   tweet, this does not make us an arbiter of truth. Our intention is to connect the dots of
[00:30:00.920 --> 00:30:05.080]   conflicting statements and show the information and disputes so people can judge for themselves.
[00:30:05.080 --> 00:30:13.000]   He does not buy that. He says, that's the problem in general, as you put these companies in the
[00:30:13.000 --> 00:30:19.480]   position of being arbiters of truth. So I'm going to guess that nobody on this panel likes the idea
[00:30:19.480 --> 00:30:28.280]   that Facebook should just do best speeches, more speech, just not block anything. But Ben does say,
[00:30:28.280 --> 00:30:32.440]   I believe in the more speech approach. And Ben's pretty smart.
[00:30:32.440 --> 00:30:39.880]   I feel like there's, he's kind of confusing a couple of things here. By calling it a free
[00:30:39.880 --> 00:30:44.760]   speech issue, he's making it sound like Facebook is making a decision about whether what content
[00:30:44.760 --> 00:30:52.040]   gets to stand and what content doesn't get to stand. And he even admits or says that it's not just
[00:30:52.040 --> 00:31:01.800]   they're not removing Facebook or Twitter didn't remove the tweet. They just put a warning on top of
[00:31:01.800 --> 00:31:09.800]   it. And when the New York Times covers the President's tweet, they reproduce the entire
[00:31:09.800 --> 00:31:15.800]   tweet, which seems entirely comparable. I mean, you can write about something or you can attach a
[00:31:15.800 --> 00:31:23.560]   statement saying, this is inflammatory and, you know, doesn't accord with the facts or this doesn't
[00:31:23.560 --> 00:31:28.920]   fit our terms of service, but we're not removing it because it's the President of the United States
[00:31:28.920 --> 00:31:35.240]   saying it. Like just adding a warning flag like that does not seem to me to be in any way a suppression
[00:31:35.240 --> 00:31:41.560]   of speech. It's just Facebook or Twitter, which actually did it saying, you know, this is our
[00:31:41.560 --> 00:31:52.200]   platform. And we have a responsibility to, you know, to our audience and to the people of the world to
[00:31:52.200 --> 00:31:55.720]   present information responsibly. And when it comes from a figure like the President,
[00:31:55.720 --> 00:32:01.880]   it's, you know, it's not like opening the floodgates to fact checking every single post on the
[00:32:01.880 --> 00:32:06.200]   platform. It's like saying this is actually a significant concern and we're in a position to do
[00:32:06.200 --> 00:32:11.880]   something about it. Does it matter that they didn't do it to other tweets that were equally
[00:32:11.880 --> 00:32:17.480]   incorrect? Or in fact, I think you maybe could make an argument. The President wasn't completely
[00:32:17.480 --> 00:32:24.040]   wrong about mail-in ballots having some authentication issues. That is a problem.
[00:32:24.040 --> 00:32:29.400]   Twitter was just sloppy. Here's the thing. But that's the problem. If there's sloppy,
[00:32:30.440 --> 00:32:32.840]   they shouldn't be doing it if they can't do it well. Yeah.
[00:32:32.840 --> 00:32:35.480]   Well, that's like saying the New York Times shouldn't publish.
[00:32:35.480 --> 00:32:41.240]   Well, maybe they should go. Right. I mean, I'll stop now.
[00:32:41.240 --> 00:32:47.720]   Okay. Go ahead, Demi. Well, Zuckerberg. So Facebook's official line now for years has been that it is
[00:32:47.720 --> 00:32:53.240]   a platform not making editorial judgments. Journalism schools and newsrooms across the
[00:32:53.240 --> 00:32:59.240]   country for the past decade have been arguing and debating whether or not Facebook should,
[00:32:59.240 --> 00:33:05.560]   as a platform, have some type of editorial function or whether or not they already serve an editorial
[00:33:05.560 --> 00:33:10.920]   function. And the thing is that Facebook is making editorial decisions all the time.
[00:33:10.920 --> 00:33:18.040]   And it has been doing this for years. It's algorithmic. They can argue while they want that
[00:33:18.040 --> 00:33:23.560]   they are just a platform. But a weighted algorithm makes determinations. Otherwise,
[00:33:23.560 --> 00:33:29.880]   you would have so much content that is either nonsensical or not interesting to you in your feed,
[00:33:29.880 --> 00:33:34.920]   you just wouldn't be using Facebook. But you could control that both Facebook and Twitter.
[00:33:34.920 --> 00:33:44.440]   You cannot use it to follow. No, again, Facebook, if you look at this from a not human, but rather
[00:33:44.440 --> 00:33:53.080]   algorithmic sort of point of view, Twitter has done no editorial content direction whatsoever
[00:33:53.080 --> 00:33:56.440]   until very, very recently. It just started practically. Yeah.
[00:33:56.440 --> 00:34:01.480]   Right. But Facebook has been doing it all along. Facebook absolutely suppresses content. If they
[00:34:01.480 --> 00:34:06.600]   were not suppressing content, you would literally see every single thing that every single person
[00:34:06.600 --> 00:34:10.280]   that you're connected to posts and you don't. They're making editorial choices
[00:34:10.280 --> 00:34:15.240]   algorithmically in the newsfeed is what you're so so again, I dropped out of law school before
[00:34:15.240 --> 00:34:21.160]   like right before I started. So I do not have a degree that qualifies me to say this. But if I was
[00:34:21.160 --> 00:34:27.480]   a person arguing this, I would try to make the case. I would make the case that Facebook has
[00:34:27.480 --> 00:34:33.320]   been suppressing speech for years. They're algorithmically suppressing it. They are algorithmically
[00:34:33.320 --> 00:34:38.120]   making determinations about who sees what. And therefore, they're absolutely an editorial operation.
[00:34:38.120 --> 00:34:45.160]   It's just not in the way that we think of a news organization because so much more is done
[00:34:45.160 --> 00:34:50.040]   without a human in the loop. But it's nonsense to say that they're not doing that. And as such,
[00:34:50.680 --> 00:34:57.320]   they should I am not suggesting that we clamp down on speech free speech is really important.
[00:34:57.320 --> 00:35:07.000]   But we have to we have to force the tech platforms and the companies to have a conversation where
[00:35:07.000 --> 00:35:11.640]   we're all talking about the same thing. We are talking about apples to apples, not apples to
[00:35:11.640 --> 00:35:18.280]   baseball bats or something else. It makes no sense. The point about the algorithm is really true.
[00:35:18.280 --> 00:35:23.640]   And it's clear that the algorithm favors certain kind of speech. So speech that tends to be
[00:35:23.640 --> 00:35:31.320]   emotionally engaging or inflammatory or which provokes strong emotional reactions will get
[00:35:31.320 --> 00:35:37.800]   shared more and then therefore amplified. And I think that the Facebook platform for sure is
[00:35:37.800 --> 00:35:43.560]   designed to amplify those kinds of strong emotional reactions and kind of strong emotional content
[00:35:43.560 --> 00:35:47.080]   because that's what keeps people engaged. And that's an editorial decision. I think you're right
[00:35:47.080 --> 00:35:55.080]   about that, Amy. I would pile on this too to Amy's point is that if these messages from the president
[00:35:55.080 --> 00:36:01.160]   contained a nipple, you definitely would have been censored in some way. And if it had pornography,
[00:36:01.160 --> 00:36:07.160]   it would have been taken down. And both are free speech. And so if they're for free speech,
[00:36:07.160 --> 00:36:12.600]   but they take that stuff down, then it sounds like that is a choice that they are making.
[00:36:12.600 --> 00:36:15.400]   And it's a choice that they're choosing not to make with the same content.
[00:36:16.120 --> 00:36:25.160]   I mean, yeah, I mean, that's is the problem. They've put themselves in the position of a publisher.
[00:36:25.160 --> 00:36:30.920]   And then they say, well, we don't want to be a publisher. But there's value. I mean,
[00:36:30.920 --> 00:36:36.760]   we wouldn't have seen. Here's the argument Jeff Jarvis gives. I'll repeat it. We wouldn't have seen
[00:36:36.760 --> 00:36:43.800]   the horrific footage of George Floyd's murder, except that it was posted on Facebook and it went
[00:36:43.800 --> 00:36:51.560]   viral there. Right? There's value to what Facebook prints with great power comes great
[00:36:51.560 --> 00:36:56.840]   responsibility. And that's for both Zuckerberg and Zuckerberg and Facebook, but also for the
[00:36:56.840 --> 00:37:02.760]   president of the United States. Yeah, well, so we can't control him, obviously. Yes, you can.
[00:37:02.760 --> 00:37:06.200]   At the ballot, you can control what he posts. You can control what he posts on Facebook if
[00:37:06.200 --> 00:37:12.200]   you're Mark Zuckerberg. But actually you can't because what's going to happen is a private
[00:37:12.200 --> 00:37:18.040]   institution. So yeah, they can. No, they can. But then but he's got the bully pulpit. I mean,
[00:37:18.040 --> 00:37:21.720]   he whatever he wants to say, he's going to be able to say, and it's going to get printed in
[00:37:21.720 --> 00:37:25.800]   newspapers and it's going to be put on TV. It doesn't matter if he does it on Facebook or Twitter or
[00:37:25.800 --> 00:37:30.280]   somewhere else, right? I think that just proves the point that how Facebook can do it because
[00:37:30.280 --> 00:37:33.720]   they're not taking a while really taking a while to talk. Yeah, his ability to talk.
[00:37:35.880 --> 00:37:43.080]   By the way, even though I'm I read out loud Ben's post and I think Ben's a smart guy,
[00:37:43.080 --> 00:37:48.840]   I don't have I don't know what the answer is to this. This seems to be a very difficult,
[00:37:48.840 --> 00:37:58.360]   intractable problem. And I'm not on Facebook for that for the reason that I just don't like to see
[00:37:58.360 --> 00:38:05.240]   the stuff that goes through that feed. It's it was bringing me down and I rejoined Facebook during
[00:38:05.240 --> 00:38:10.440]   the pandemic because I thought, Oh, be good to be able to stay in touch with people. This is a useful
[00:38:10.440 --> 00:38:14.360]   tool for that. And that we're calling Amy back. I think this is what we're going to do.
[00:38:14.360 --> 00:38:19.960]   And then and then it got me so depressed. I immediately killed it. Now I haven't killed my
[00:38:19.960 --> 00:38:26.520]   Twitter feed, but I have to be very judicious about reading it. I'm with you, Leo. I found that face
[00:38:26.520 --> 00:38:31.480]   I find that Facebook is not good for my mental health. I spent too much time on it. I really use
[00:38:31.480 --> 00:38:36.360]   it very judiciously. But we have to recognize a huge percentage of Americans get their news.
[00:38:36.360 --> 00:38:42.120]   Not all of it, but get a lot of it from Facebook. Well, and if you I think it was,
[00:38:42.120 --> 00:38:49.800]   I don't remember who posted this, but the top 10 stories on on Facebook, in the news feed in the
[00:38:49.800 --> 00:38:56.120]   last week or so were were all like Fox News and really, really, really conservative outlets. So
[00:38:56.600 --> 00:39:04.520]   it seems clear which way that the algorithm skews. Yeah. He seems to have been somewhat
[00:39:04.520 --> 00:39:12.760]   influenced. You never know with Mark, but someone influenced by the walk out in the protests for
[00:39:12.760 --> 00:39:21.480]   his engineers. On Friday, he said, we're going to review our Facebook policies on posts.
[00:39:22.360 --> 00:39:27.080]   The promoter threatened state use of force or voter suppression techniques. They're going to look
[00:39:27.080 --> 00:39:32.040]   into options for flagging or labeling posts that are violation, but shouldn't necessarily be
[00:39:32.040 --> 00:39:37.720]   removed entirely. This is Mark's post on Friday on Facebook. And he also said we're going to study
[00:39:37.720 --> 00:39:42.840]   Facebook's review structure to make sure the right groups and voices are at the table. Wesley,
[00:39:42.840 --> 00:39:47.880]   does that feel like lip service or do you think he's genuinely planning to do something about it?
[00:39:50.600 --> 00:39:57.160]   I'm going to say this. And this is just from friends who work at Facebook that I know,
[00:39:57.160 --> 00:40:03.880]   that Mark is as genuine as he thinks he is. So,
[00:40:03.880 --> 00:40:11.160]   that means that means like we're hearing a lot of people like the walk out and people talking
[00:40:11.160 --> 00:40:16.440]   about how they disagree because he doesn't stifle his employees. He's opening that. Yeah.
[00:40:16.440 --> 00:40:20.200]   So, that's a really good thing. And so, when he says that he's going to try to do this,
[00:40:20.200 --> 00:40:29.800]   it's credit. I think he's trying in his own words. But that being said, it's like when you're trying
[00:40:29.800 --> 00:40:34.200]   to work out a solution to a problem, but you're only thinking about it in your head without
[00:40:34.200 --> 00:40:39.960]   consulting other people, is that I feel that he rules that way. So, he makes things and makes
[00:40:39.960 --> 00:40:46.280]   judgments that make sense to him. But he's using his own thought process. And if he comes to
[00:40:46.280 --> 00:40:54.440]   what he thinks is a solution, that's the right solution. And I don't think he allows other people
[00:40:54.440 --> 00:41:01.400]   to go with a solution that he would disagree with. And I maybe nine times out of 10, he's right.
[00:41:01.400 --> 00:41:08.600]   But that means one in 10 is someone else has a better solution that doesn't get it active.
[00:41:08.600 --> 00:41:11.160]   And I think he controls Facebook situation.
[00:41:11.160 --> 00:41:16.440]   He controls Facebook with an iron grip. He has 60% voting, percent of the voting shares.
[00:41:16.440 --> 00:41:23.960]   No one can say no to him. And that's a very Silicon Valley approach, isn't it? It's,
[00:41:23.960 --> 00:41:31.080]   I'm going to think my every coder knows this approach. I'm going to think my way to the correct
[00:41:31.080 --> 00:41:38.920]   solution. And that'll be that. It's a logic problem. I'm not surprised to think that way.
[00:41:39.480 --> 00:41:46.680]   Here's the baseline of the issue. And whatever he decides, if it's negative,
[00:41:46.680 --> 00:41:51.640]   he himself and probably the people around him will not be negatively affected.
[00:41:51.640 --> 00:41:58.040]   So, if it's voter suppression or whatever, none of that really affects him, probably won't
[00:41:58.040 --> 00:42:01.400]   affect his family, won't affect his friends. It's the ultimate privilege.
[00:42:02.120 --> 00:42:11.320]   Yes. And so these decisions, I don't feel that he understands the weightiness of it and the impact
[00:42:11.320 --> 00:42:19.240]   of it from an actual well-rounded perspective. And that's not a knock on him. That's not a knock
[00:42:19.240 --> 00:42:28.040]   on his understanding. It's just that it's a fact. He doesn't, will not feel the impact of this.
[00:42:28.040 --> 00:42:32.440]   I mean, he's been rich for how long ever in his life and it's a whole adult life he's been rich.
[00:42:32.440 --> 00:42:39.400]   The people around him have been shielded by saying, hey, my friend, my cousin, my buddy is Mark
[00:42:39.400 --> 00:42:46.440]   Zuckerberg. And then they get some of that privilege too. If he was genuinely friends and
[00:42:46.440 --> 00:42:53.560]   generally connected to people who he respected that have a totally different perspective in a way of
[00:42:54.840 --> 00:43:00.120]   looking at the world, I think he would have a little bit more influence from that direction. But
[00:43:00.120 --> 00:43:06.920]   sometimes I feel that if he says, I won't do this, but instead I'm going to hand this over because
[00:43:06.920 --> 00:43:13.240]   it's that important, which I don't think he has the, the willingness to do. He doesn't have the
[00:43:13.240 --> 00:43:18.360]   willingness to say, I'm letting this go. I'm greenlining every single last thing you're doing.
[00:43:18.360 --> 00:43:23.640]   And that won't happen. That's the only thing he gives me pause. And so, but I think he thinks
[00:43:23.640 --> 00:43:29.320]   he's Jim, let's go go Dylan. Yeah, I will just add that as a communicator,
[00:43:29.320 --> 00:43:34.360]   when I see lines like we will review the policies and procedures or there will be a review,
[00:43:34.360 --> 00:43:37.800]   that feels like lip service to me. That feels like something that somebody in the communications
[00:43:37.800 --> 00:43:42.040]   department put together. That's like, there's nothing concrete, no specific, no, like we're
[00:43:42.040 --> 00:43:45.080]   going to hire. That's like Congress in asking, we're going to form a committee.
[00:43:45.080 --> 00:43:51.320]   Right. It does sound like lifsters. I'm just coming at, I'm adding the, like I said,
[00:43:51.320 --> 00:43:56.840]   I have a little bit of bias from my friends who do work there. And it is, like I said,
[00:43:56.840 --> 00:44:03.960]   he's trying to be genuine about the change. But like I said, I don't think he is going to keep
[00:44:03.960 --> 00:44:09.480]   himself in the decision matrix. And even he puts it to the committee together and has really
[00:44:09.480 --> 00:44:14.360]   good recommendations, those recommendations will be put against his thought process.
[00:44:14.360 --> 00:44:21.400]   And then if there is a tie, he wins. So the barrier of those changes, they have to be so
[00:44:21.400 --> 00:44:25.880]   much better than what he wants or thinks of for them to actually come before us.
[00:44:25.880 --> 00:44:34.840]   So let's talk about the problem as he sees it. Can we at least set the parameters of the problem?
[00:44:36.120 --> 00:44:44.040]   Is philosophically, I think, he and many of us think speech is a good thing and more speech is
[00:44:44.040 --> 00:44:51.080]   a better thing. And even if you find speech horrific and reprehensible, the free flow of speech
[00:44:51.080 --> 00:44:59.720]   is the only viable solution. On the other hand, Facebook has, and well, I'm sure continue to,
[00:45:00.600 --> 00:45:09.240]   bar terrorist groups, a variety of kinds of speech. Free speech doesn't mean anything goes. So
[00:45:09.240 --> 00:45:14.600]   he must have some rules for what kind of speech goes and what kind of speech doesn't go.
[00:45:14.600 --> 00:45:20.760]   And then of course, and I don't know if he considers this, he's certainly made enough money,
[00:45:20.760 --> 00:45:28.040]   doesn't need to, but he has a responsibility of the stakeholders in Facebook to do something that is
[00:45:28.840 --> 00:45:35.000]   good for them financially as well. Are there any other parameters that he's juggling?
[00:45:35.000 --> 00:45:41.000]   He should be juggling impact. And that seems like society, the impact on society.
[00:45:41.000 --> 00:45:45.800]   Yeah, he doesn't seem to put that in the factor because because he feels that,
[00:45:45.800 --> 00:45:50.120]   okay, let's say someone gets radicalized on Facebook and then kill someone.
[00:45:50.120 --> 00:45:57.160]   I think the onus is on that person that he would put it on. That person chose to kill someone.
[00:45:57.160 --> 00:46:03.000]   But that but it does block stuff that would radicalize people, right? I mean, that's because the law
[00:46:03.000 --> 00:46:10.200]   makes them so they have the penalty. The penalty, the penalty on them is great. So the law does not,
[00:46:10.200 --> 00:46:17.160]   he doesn't care about consequences. There's others. Exactly. Yes. Unless it's a consequence to Facebook.
[00:46:17.160 --> 00:46:23.080]   Got it. And Facebook has been used not just to radicalize individuals, but whole societies.
[00:46:23.080 --> 00:46:30.040]   You know, well, you could point to Duterte in the Philippines who used Facebook to get elected
[00:46:30.040 --> 00:46:36.600]   to support his fascist regime. That's not the first time Facebook's been used as a platform to do
[00:46:36.600 --> 00:46:41.880]   that kind of thing. And it doesn't mean more exactly. It doesn't seem like they've done much to stop that.
[00:46:41.880 --> 00:46:49.080]   No, in fact, I think that, I mean, they'll admit after the fact that they recognize that
[00:46:49.720 --> 00:46:54.840]   they were used that way and they're sorry, but you don't. Facebook is good at apologizing.
[00:46:54.840 --> 00:46:59.960]   They proactive. Yeah. Yeah. They don't want to be seen as being picking winners and losers.
[00:46:59.960 --> 00:47:04.120]   Yeah. And I understand that's that best kind of speech is more speech.
[00:47:04.120 --> 00:47:10.360]   Basically, if Hitler was in charge of Germany today, Hitler would be able to use Facebook
[00:47:10.360 --> 00:47:17.240]   to say whatever Hitler wants. And what you should do is says, you know, I do have an opinion.
[00:47:17.240 --> 00:47:22.280]   This is important. Right. And make the right choice. And I think that's you've seen the same thing
[00:47:22.280 --> 00:47:27.080]   in New York Times that was brought up today. And I don't know if you saw it right before we hit the
[00:47:27.080 --> 00:47:35.560]   air. James Bennett resigned. He did. Yes. Holy cow. So let's recap that story because that's a
[00:47:35.560 --> 00:47:44.280]   fascinating story. The New York Times editorial board, their op ed section is different from the
[00:47:44.280 --> 00:47:50.040]   news section. It's separate from the news section. And they were attempting to
[00:47:50.040 --> 00:47:58.600]   show both sides, I guess. So they ran an editorial by an Arkansas Senator Tom Cotton
[00:47:58.600 --> 00:48:04.840]   that said, no, no, we got to use military force against the protesters because
[00:48:04.840 --> 00:48:10.760]   Antifa, basically. There were factual mistakes in the editorial.
[00:48:12.440 --> 00:48:20.040]   There was a lot of criticism that that they ran that editorial. And Bennett
[00:48:20.040 --> 00:48:27.240]   took some of the heat and then said, well, I didn't read it before we published it.
[00:48:27.240 --> 00:48:35.560]   What editor hasn't published an inflammatory op ed without reading it? I mean, really, let's be
[00:48:35.560 --> 00:48:46.920]   fair. That's a bizarre defense. That's a bizarre defense. Now he's out. Was he resigned, but I
[00:48:46.920 --> 00:48:51.080]   imagine he resigned under intense pressure from the Salzburgers and the rest of the
[00:48:51.080 --> 00:48:54.200]   Times Organization because it was a real black mark on the New York Times.
[00:48:54.200 --> 00:49:01.800]   The Times over the past, I don't know, three years has made some significantly different
[00:49:03.720 --> 00:49:08.040]   changes in how it runs its opinion section. And obviously,
[00:49:08.040 --> 00:49:16.040]   you know, I think that there's been a rush to capitalize on clicks in the digital economy,
[00:49:16.040 --> 00:49:19.160]   attention is currency. And if you've got salacious
[00:49:19.160 --> 00:49:27.400]   columns, you know, there is a desire to get them out for business. I will say, by the time that
[00:49:27.400 --> 00:49:34.120]   that piece was posted, that argument was already two, two, three days old. So the
[00:49:34.120 --> 00:49:43.080]   I think that like a lot of organizations right now, the Times has made some poor choices
[00:49:43.080 --> 00:49:50.040]   and what it's publishing and for what reasons. They've also had a really good year in terms of
[00:49:50.040 --> 00:49:57.640]   attention and clicks and money. And, you know, can I just quickly say something? So I think
[00:49:57.640 --> 00:50:02.920]   it's important. Yes, we go back because you were silenced for a huge chunk of that. So I mean,
[00:50:02.920 --> 00:50:07.720]   anything from the past 10 minutes you want to talk about is very. Yeah, I just wanted to highlight.
[00:50:07.720 --> 00:50:14.040]   I think it is wrong that when we that at the moment, as we're talking about the future of content
[00:50:14.040 --> 00:50:19.640]   and who's saying what, where, whether it's a newspaper, whether it's the editorial page of a
[00:50:19.640 --> 00:50:24.760]   newspaper or Twitter, and therefore we're talking about Jack or Facebook and therefore we're talking
[00:50:24.760 --> 00:50:30.680]   about Mark, YouTube always gets left out of this conversation. And, you know, there's the initial
[00:50:30.680 --> 00:50:39.080]   launch of the inflammatory content that gets put out in some way. The important thing that happens
[00:50:39.080 --> 00:50:43.720]   though, because this is an ecosystem is all of the downstream content that then gets produced.
[00:50:43.720 --> 00:50:49.720]   And all of that is happening elsewhere. Like a lot of that's happening on YouTube. And if you
[00:50:49.720 --> 00:50:53.720]   look at everybody's personalization algorithms, you know, Facebook has done a pretty good job of
[00:50:53.720 --> 00:50:58.360]   capturing attention, but they are nowhere close to being as powerful and as strong as YouTube.
[00:50:58.360 --> 00:51:05.160]   Yeah. And the same exact criticism of the YouTube algorithm. It's a recommendation engine that
[00:51:05.160 --> 00:51:11.880]   promotes engagement and stickiness by becoming extreme, essentially. That's right. So I just,
[00:51:13.000 --> 00:51:18.120]   you know, I'm kind of like less interested in what Zuckerberg has to say and far more
[00:51:18.120 --> 00:51:25.400]   interested in what we're going to do about YouTube. Okay. Fair point. I think there are two sides
[00:51:25.400 --> 00:51:30.920]   of the same problem, obviously. The nice thing about Facebook is you can point to one person,
[00:51:30.920 --> 00:51:36.040]   Zuckerberg, because he controls it. I don't know who you can't point to Susan Wojcicki, CEO of
[00:51:36.040 --> 00:51:39.800]   YouTube, but maybe you can. You can't really point to Sundar Chai.
[00:51:39.800 --> 00:51:45.320]   A little bit in a direction. Absolutely. Oh, yeah. There's no obvious poster boy for
[00:51:45.320 --> 00:51:50.840]   YouTube's failures. Like there is for Facebook's failures. So maybe that's what I mean. But YouTube,
[00:51:50.840 --> 00:51:57.960]   it is because of YouTube that we have a world. YouTube is sort of like the kindling.
[00:51:57.960 --> 00:52:03.480]   You know, Facebook may be the giant logs that everybody sort of sees, but there is no fire
[00:52:03.480 --> 00:52:08.440]   without that foundation. And I think a lot of the foundation is the interplay between the stuff
[00:52:08.440 --> 00:52:15.880]   people hear about on Twitter and Facebook, and then get sucked into because the algos are so good.
[00:52:15.880 --> 00:52:20.440]   They are so good on YouTube that even if you are logged in with a singular account, you've got
[00:52:20.440 --> 00:52:26.600]   multiple people watching, it still knows the difference as it starts delivering you down
[00:52:26.600 --> 00:52:30.520]   content to go down the rabbit hole. And there's something different about YouTube too,
[00:52:30.520 --> 00:52:36.360]   because you can just sit back and have it autoplay. It's passive. You could start out
[00:52:36.360 --> 00:52:40.520]   watching videos about Magic the Gathering and like two hours later, while you're vegging out on
[00:52:40.520 --> 00:52:46.280]   the couch, it's like jihadist training videos or something. And it has been demonstrated to create
[00:52:46.280 --> 00:52:51.240]   to radicalize people because of that. That's right. And it doesn't need, it is a truly passive
[00:52:51.240 --> 00:52:58.280]   network. So simply by continuing to watch and have YouTube is a part of Google. So there's just a
[00:52:58.280 --> 00:53:03.800]   tremendous pipeline of data that are there to guide and understand what the system is doing.
[00:53:03.800 --> 00:53:10.520]   It's, I think in so many ways, just a different, just like significantly different than Facebook.
[00:53:10.520 --> 00:53:17.400]   So I totally agree that something more substantive should at least be discussed,
[00:53:17.400 --> 00:53:21.560]   if not done. But YouTube always gets left out of this conversation.
[00:53:21.560 --> 00:53:28.680]   No, no. And that by all means belongs in it. So let's include, in fact, YouTube is much more
[00:53:28.680 --> 00:53:32.440]   important than Twitter. And you're right, you can make the arguments much more important than
[00:53:32.440 --> 00:53:35.960]   Facebook could. But let's talk about Facebook and YouTube, because these are the, you know,
[00:53:35.960 --> 00:53:42.920]   clearly the worst and most influential. What's interesting, what seems to me very difficult is
[00:53:42.920 --> 00:53:48.520]   what to do. So I'd love to hear from all of you, all three of you, what, why don't you start,
[00:53:48.520 --> 00:53:50.440]   Amy, what should YouTube do?
[00:53:50.440 --> 00:53:54.040]   To deal with which one of these problems?
[00:53:54.040 --> 00:53:58.600]   How do you fix it? Well, how do you fix it? Do you start, do you just turn off the recommendation
[00:53:58.600 --> 00:54:06.280]   engine? The brain responds more to threat than opportunity, and our limbic system loves YouTube.
[00:54:06.280 --> 00:54:12.360]   So the challenge has always been the ecosystem, the business model, the business model requires
[00:54:12.360 --> 00:54:19.240]   our attention, even the paid version. And part of the, what is incentivizing the algos to get as
[00:54:19.240 --> 00:54:25.960]   good as they've gotten is to, is to have us sit and watch for hours. I mean, why is it so hard for
[00:54:25.960 --> 00:54:30.280]   us in this country to say a company should make less money? Why is it so hard for us to say,
[00:54:30.280 --> 00:54:37.640]   Facebook, turn off the algorithmic feed on the news feed, YouTube turn off recommendation engine.
[00:54:37.640 --> 00:54:43.320]   Yes, you'll make less money. You'll still do fine. Why is it hard for us to, can't we just say,
[00:54:43.320 --> 00:54:47.720]   turn off the recommendation engine? Leo, we are in a free market economy.
[00:54:47.720 --> 00:54:52.520]   These are publicly traded companies. You can't. You can't. The other thing I would say is,
[00:54:52.520 --> 00:54:58.840]   let's not forget that about 18 months ago, there was a tremendous amount of talk that looked finally
[00:54:58.840 --> 00:55:05.320]   pretty serious about antitrust. And all of these companies, Google, Apple, Amazon, they were all
[00:55:05.320 --> 00:55:14.760]   facing some form of antitrust enforcement from attorneys general to the various different federal
[00:55:14.760 --> 00:55:22.600]   departments. And COVID has really played into the hands of the tech companies in this case.
[00:55:22.600 --> 00:55:29.240]   And I would say even going through what not just the United States is going through,
[00:55:29.240 --> 00:55:32.280]   but there are protests everywhere because this affects everybody.
[00:55:32.280 --> 00:55:35.000]   I love it all over the world. It's amazing. Yeah.
[00:55:35.000 --> 00:55:39.720]   Leo, you and I have both spent time in Japan. Japan is not a place where there are a lot of protests.
[00:55:39.720 --> 00:55:41.560]   No, when we were in.
[00:55:41.560 --> 00:55:47.960]   Now when we were in Tokyo, our guide, our Japanese guide apologized to us because there was a protest
[00:55:47.960 --> 00:55:53.960]   and she was visibly embarrassed. She was mortified that that protest was going on. I'm so sorry you
[00:55:53.960 --> 00:55:59.240]   should see that. Right. So the fact that there are tens of thousands of people in Japan,
[00:55:59.240 --> 00:56:05.400]   protesting tells you something. Now here's the issue. As the, you know, we are, everybody is under
[00:56:05.400 --> 00:56:11.480]   intense pressure to get the economy reopened and to continue to do something about this pandemic.
[00:56:12.440 --> 00:56:19.320]   Any, any calls to have these companies make less money, any calls to reignite that anti
[00:56:19.320 --> 00:56:25.080]   force that antitrust enforcement is going to find itself. You know, anybody who's trying to do
[00:56:25.080 --> 00:56:30.920]   this is going to find themselves under pressure, not to take, not to undertake any actions that
[00:56:30.920 --> 00:56:39.320]   would cause the further loss of jobs. So I mean, I just, we, so, so I am always of the, you know,
[00:56:39.320 --> 00:56:46.040]   I always think that the way forward has to be an economic way forward. And maybe that's just my,
[00:56:46.040 --> 00:56:50.760]   my academic background is economics. So maybe that's part of where this. So you have to make it so
[00:56:50.760 --> 00:56:57.480]   that they cost the money to do this. We either have to do it so that it costs the money, but punitive
[00:56:57.480 --> 00:57:03.320]   measures usually wind up in court. Right. And when things wind up in court, they drag on.
[00:57:03.320 --> 00:57:09.640]   We've seen we saw what happened with Microsoft DOJ took so long by the time they, by the time they,
[00:57:09.640 --> 00:57:16.520]   they finished the issue was moot. That's right. So, so I don't think that punitive economic measures
[00:57:16.520 --> 00:57:21.640]   work. I think that coming up with economic incentives, which sounds, you know, crazy.
[00:57:21.640 --> 00:57:27.080]   Because what I'm suggesting is that we find out, we figure out a better way for them to win.
[00:57:27.080 --> 00:57:32.680]   But I think that given the situation that we're in, that's the most probable way forward. We have
[00:57:32.680 --> 00:57:38.440]   to figure out a way for Zuckerberg, for Dorsey, for everybody at YouTube, we have to figure out a
[00:57:38.440 --> 00:57:44.120]   way for them to be a hero to what's best for the public, right, what's in the public interest,
[00:57:44.120 --> 00:57:49.640]   while also being a hero to their shareholders. That is a tricky thing to do. And you have to be
[00:57:49.640 --> 00:57:55.080]   absolutely willing to make short term sacrifices so that you can be a hero to both and treat the
[00:57:55.080 --> 00:58:01.480]   villain as these systematic problems that we're all dealing with. But if you, if you choose to only
[00:58:01.480 --> 00:58:05.720]   be a hero to your shareholders, then we wind up in the problem that we're in. If you kind of half
[00:58:05.720 --> 00:58:11.560]   asset, then the problem still gets worse. So this requires a totally different way of thinking.
[00:58:11.560 --> 00:58:17.480]   And I think courageous leadership under extraordinary circumstances. But until somebody is willing,
[00:58:17.480 --> 00:58:23.160]   you know, so fine, Jack, if you're listening, well, after you've meditated and had your liquid
[00:58:23.160 --> 00:58:29.000]   crazy drinks all day long, you know, this is your call to action, like figure out a way to be a
[00:58:29.000 --> 00:58:35.240]   share a hero to your shareholders. While you are creating a Twitter that's, you know,
[00:58:35.240 --> 00:58:39.400]   that serves the actual public interest, it just requires creativity.
[00:58:39.400 --> 00:58:43.800]   Zuck, if you're listening to this, you know, this is your call to arms, do the same thing.
[00:58:43.800 --> 00:58:51.320]   But you people who made our future, who we all look up to also have cherished beliefs that you
[00:58:51.320 --> 00:58:56.440]   must confront. And if you are not willing to confront your cherished beliefs, then we don't move
[00:58:56.440 --> 00:59:02.680]   anywhere forward. Then we continue to spiral downward. And I just think that for the people that built
[00:59:02.680 --> 00:59:10.360]   the people that built the future, you know, what a devastating, what a horrible legacy to leave
[00:59:10.360 --> 00:59:15.720]   behind. You know, you have an opportunity now to take these platforms and to do something great,
[00:59:15.720 --> 00:59:22.200]   to lead us into a better place. But you have to choose to do that through creativity
[00:59:22.200 --> 00:59:28.600]   and new kinds of risk taking, you know, and being willing to make short-term sacrifices for the
[00:59:28.600 --> 00:59:35.080]   purpose of the long term. Wesley, what do you think? What would you do if you could make wave
[00:59:35.080 --> 00:59:44.440]   a magic wand? I think it's incredibly restrictive to think that there's one answer. For instance,
[00:59:44.440 --> 00:59:48.680]   when I hear about self-driving cars, they talk about the trolley car problem. Yeah.
[00:59:49.640 --> 00:59:56.680]   Do you kill one person or five people? But something I haven't heard yet is what if you
[00:59:56.680 --> 01:00:00.840]   turn on the car for the first time saying, hey, do you want to kill the one person or five people
[01:00:00.840 --> 01:00:07.000]   and then have the person who drives chooses? The same with the Facebook and algorithms have
[01:00:07.000 --> 01:00:13.880]   make it so you do all of them. Do every single algorithm? And then when you come into Facebook
[01:00:13.880 --> 01:00:19.400]   says, hey, do you want to see racist? Do you want to see misinformation from a politician? Do you
[01:00:19.400 --> 01:00:26.200]   want to see like all this stuff or yes or no and have people do its questionnaire and know what
[01:00:26.200 --> 01:00:31.160]   they're going to get? Well, doesn't the algorithm do that? Doesn't the algorithm automatically assess
[01:00:31.160 --> 01:00:36.200]   their interest for you? Yeah, big to what you look at. And now you can't complain. Now you can't
[01:00:36.200 --> 01:00:42.360]   say, we want President Trump, right? Trump's post taken down, right? You can choose, I want
[01:00:42.360 --> 01:00:49.640]   the Facebook that gets away from people who I or bad. I'm going to guess we're all not following
[01:00:49.640 --> 01:00:58.280]   Trump on Twitter. Possibly, but like make it so like dictators politicians, whatever choose
[01:00:58.280 --> 01:01:02.840]   choose broader strokes. People put in the marketplace, build your own algorithm,
[01:01:02.840 --> 01:01:08.600]   which Facebook do you want to the race? Facebook. Yeah, I like, you know, who who is a great human
[01:01:08.600 --> 01:01:14.280]   invasion Tom Hanks. I want the Tom Hanks face book on Facebook. Yeah. And then I will do that. And
[01:01:14.280 --> 01:01:18.680]   then just let people, I mean, if you can choose what kind of world you want to live in, what kind
[01:01:18.680 --> 01:01:26.440]   of algorithm that you want to support and then figure out what marketers, what companies, where
[01:01:26.440 --> 01:01:29.960]   they're putting their ads and you're like, Hey, I heard you're putting on the one that explicitly
[01:01:29.960 --> 01:01:35.160]   says this is a racist Facebook. It's a problem. You have a choice and they're choosing to do that.
[01:01:35.160 --> 01:01:40.920]   That's interesting. That would be my magic. I like this. Well, that's so that's really what
[01:01:40.920 --> 01:01:46.280]   that's the second part of Amy's thought, which is be creative and come up with a solution that
[01:01:46.280 --> 01:01:51.720]   serves the stakeholders that makes you more money, but also serve society. I think that's an interesting
[01:01:51.720 --> 01:01:58.520]   solution. How about you, Dylan? You got a you got a good creative thing to send Mark Zuckerberg's way.
[01:01:59.560 --> 01:02:06.200]   Yeah, let me just build on on both of those suggestions about creativity and choose your own algorithm.
[01:02:06.200 --> 01:02:12.200]   I think it'd be useful if there was more algorithmic transparency. Yes.
[01:02:12.200 --> 01:02:16.680]   So Facebook's never said how it works. No, that's right. Google knows you too.
[01:02:16.680 --> 01:02:21.880]   So you could do this on a really granular level, like on Pandora, does anybody still use Pandora?
[01:02:21.880 --> 01:02:28.200]   Probably not, but I do. But it tells you like, were you recommended this song because it's got
[01:02:28.200 --> 01:02:34.920]   heavy bass line and a female vocalist and whatever. And you can see that. What if they did that with
[01:02:34.920 --> 01:02:38.920]   YouTube videos and with the things they're recommending in your Facebook feed or with tweets? There was a
[01:02:38.920 --> 01:02:44.360]   little option to say, hey, why are you recommending this to me? Well, we're recommending it to you
[01:02:44.360 --> 01:02:49.400]   because it matches up with the other race of stuff that you like or because it has to do with like,
[01:02:49.400 --> 01:02:55.160]   I don't know, Zen or something and you've shown an interest in Zen. So here's the thing is I think
[01:02:55.160 --> 01:03:00.280]   you have to not only expose that to users, but you need to make it programmatically accessible
[01:03:00.280 --> 01:03:06.600]   so that somebody, a grad student in computer science or a journalist could go through
[01:03:06.600 --> 01:03:13.480]   and pull data out of the system. And obviously, they're going to have some concerns about not
[01:03:13.480 --> 01:03:20.040]   making it possible to reverse engineer their magic algorithms. But pull enough data out that
[01:03:20.040 --> 01:03:27.160]   you can start to say like, hey, we've noticed like there's a population of people over here to whom
[01:03:27.160 --> 01:03:34.840]   Facebook is consistently pushing, you know, Breitbart and Fox News and you know, whatever else,
[01:03:34.840 --> 01:03:41.480]   Alex Jones or whatever. And there's a population on YouTube over here that's consistently getting like,
[01:03:41.480 --> 01:03:44.040]   you know, videos about furries or something.
[01:03:45.320 --> 01:03:52.440]   I love it. You could have a third party ecosystem of tools that would help you customize to your
[01:03:52.440 --> 01:03:57.800]   liking your Facebook feed or your YouTube feed. And it would provide accountability because then
[01:03:57.800 --> 01:04:01.560]   these people doing this kind of research could publish their research and they could say like,
[01:04:01.560 --> 01:04:06.600]   look, we've noticed the algorithm works in this way or hey, we've noticed like a potential problem
[01:04:06.600 --> 01:04:11.240]   over here. And then the companies have to respond to that. I love that. Amy is at the kind of out
[01:04:11.240 --> 01:04:18.520]   of the box creativity you're talking about. I think I think we need to go right up to the edge of
[01:04:18.520 --> 01:04:23.800]   what's plausible. So I would like to say yes. And what else can we do? I mean, I think everybody's
[01:04:23.800 --> 01:04:27.800]   been sitting around waiting for a business case for self sovereign identity forever.
[01:04:27.800 --> 01:04:34.040]   To think we have a plausible business case. It also has to work globally because it's not just
[01:04:34.040 --> 01:04:38.680]   an American problem. That's right. These are American companies, but this is a global problem now.
[01:04:39.720 --> 01:04:44.200]   That's right. And so this is the other sort of thorny piece of this, which is that
[01:04:44.200 --> 01:04:52.360]   from a, if I was a developer, the question that I'm sure I would be asking myself is what's the
[01:04:52.360 --> 01:04:56.040]   taxonomy that's getting used? What's the machine learning model? How do I sort out?
[01:04:56.040 --> 01:05:05.880]   Different cultures, different people use words differently. In some cases, they're
[01:05:06.920 --> 01:05:15.080]   acceptable in other cases. They mean horrible things. Sarcasm within communities doesn't always
[01:05:15.080 --> 01:05:20.920]   play well online. So the question becomes, this is not about thought, please, or anything else.
[01:05:20.920 --> 01:05:25.240]   It's a technical question, which I don't have an answer to. And that is, how do you build that
[01:05:25.240 --> 01:05:30.360]   taxonomy? How do you build the machine learning model? And then how do you deploy it to account for
[01:05:31.080 --> 01:05:38.200]   the unknowable number of unique ways in which everybody uses language?
[01:05:38.200 --> 01:05:45.560]   And I suspect that when Facebook says publicly, we haven't been able to do X,
[01:05:45.560 --> 01:05:51.320]   that some of what they're talking about is this. So again, we isn't there another
[01:05:51.320 --> 01:05:57.720]   very creative way to approach the problem. And I think, again, because these are
[01:05:58.360 --> 01:06:03.000]   companies in a free market economy that are publicly traded that require, that they have
[01:06:03.000 --> 01:06:10.840]   fiduciary responsibilities, that we have to figure out a way to incent them to build things
[01:06:10.840 --> 01:06:17.000]   that are potentially going to be lost leaders and to give them some ability to build out risk.
[01:06:17.000 --> 01:06:22.840]   So again, here's another way to think about this. We've got OAuth as a single sign-in.
[01:06:22.840 --> 01:06:29.560]   Is there some OAuth something that would cut across all of our different communications channels
[01:06:29.560 --> 01:06:34.520]   that would at least verify? This is where I come back to self-soupt in our identity.
[01:06:34.520 --> 01:06:41.320]   And is there a way to at least verify that the person is a real person versus a bot just
[01:06:41.320 --> 01:06:47.480]   meant to stir up trouble? So that would be one layer. And maybe the next layer is, well,
[01:06:47.480 --> 01:06:55.160]   if it's a real person, then, you know, because you wind up with problems, like if you're posting
[01:06:55.160 --> 01:07:04.120]   photos of people on Facebook, posting photos of people at the lure or something, where they're
[01:07:04.120 --> 01:07:09.240]   standing in front of modern art that happens to show a nipple, how do you train a machine learning
[01:07:09.240 --> 01:07:15.160]   model to recognize that that nipple is okay, but a more salacious pose from a person who's
[01:07:15.160 --> 01:07:20.520]   got whatever. Revenge porn is bad. This is where it gets challenging. It's not something that we
[01:07:20.520 --> 01:07:25.240]   can't address, but the market doesn't reward that type of work. And my point is we have to
[01:07:25.240 --> 01:07:31.160]   figure out a way to reward financially or otherwise somebody doing the research within these organizations
[01:07:31.160 --> 01:07:36.760]   and producing some kind of result that's actionable, right? Because otherwise, we're just spinning our
[01:07:36.760 --> 01:07:42.040]   wheels. I do have to say that this, what's going on and what's gone over the last two weeks is
[01:07:42.840 --> 01:07:51.560]   society's way of saying, you got to make a change. That's one way to accomplish this. It is if, I mean,
[01:07:51.560 --> 01:07:56.920]   if you want an economic consequence, if people are marching in the street saying Facebook has to
[01:07:56.920 --> 01:08:03.960]   change, the world has to change, society has to change. That's a very strong signal coming to you
[01:08:03.960 --> 01:08:08.040]   that maybe you should be thinking about making a change. And I think that's one of the reasons
[01:08:08.040 --> 01:08:14.280]   you're seeing people like Tim Cook and Satya Nadella post big public apologies and statements about
[01:08:14.280 --> 01:08:19.160]   what's going to happen. So I mean, in a way, that's what's happening right now is people are
[01:08:19.160 --> 01:08:25.400]   saying they're standing up, they're taking the streets saying no more this must end. Isn't this
[01:08:25.400 --> 01:08:31.640]   the most healthy way for that to happen? I think that it's worth remembering that the
[01:08:31.640 --> 01:08:38.840]   obligation to maximize profit or I'm sorry to maximize shareholder return is not the only
[01:08:38.840 --> 01:08:45.880]   obligation that a publicly traded corporation has. That's a belief that or a mentality about
[01:08:45.880 --> 01:08:51.560]   thinking about public corporations that has become increasingly dominant over the last three decades
[01:08:51.560 --> 01:08:58.440]   but was did not used to be the case. So this argument that all they have to be concerned about
[01:08:58.440 --> 01:09:03.720]   as a corporation is maximizing profit, I think is shortsighted. The pendulum could go the other
[01:09:03.720 --> 01:09:08.840]   way and was what is their what is their obligation as a publicly held company.
[01:09:08.840 --> 01:09:15.720]   It's not merely to prop up your stock price, right? But what is it? Well, that's correct. That is the
[01:09:15.720 --> 01:09:20.760]   way they that is the way they are managed. That's their grade. Yeah. Yeah, that's their grade.
[01:09:23.400 --> 01:09:29.480]   But ultimately, you can affect their grade. People have an obligation not to pollute the
[01:09:29.480 --> 01:09:33.320]   environment. They have an obligation to follow the laws. And if people stand up and say,
[01:09:33.320 --> 01:09:38.040]   we're not going to buy your product, we're not going to participate in your site because we don't
[01:09:38.040 --> 01:09:42.840]   like the way you treat people. That affects your grade, doesn't it? In the long run?
[01:09:42.840 --> 01:09:48.360]   That's why I'm saying that one way people can do this is by standing up.
[01:09:48.920 --> 01:09:56.120]   Yes, it should. Or nearly. So again, there's the regulatory constraint.
[01:09:56.120 --> 01:10:03.400]   There's the business ecosystem constraint. And then there's the tacit relationship between
[01:10:03.400 --> 01:10:09.400]   and business. So I professor at business school. In business school, we talk about creating a value
[01:10:09.400 --> 01:10:15.000]   network. So this is the all the services and the products and everything related to an organization.
[01:10:15.000 --> 01:10:22.600]   And then as they gain value, the values created for customers, the value is created back
[01:10:22.600 --> 01:10:29.000]   to the organization. But it requires an omnidirectional relationship. The issue is
[01:10:29.000 --> 01:10:39.240]   we have reached a sort of fever pitch right now. But I live in the city where Freddie Gray was
[01:10:39.240 --> 01:10:45.240]   murdered. And I remember five years ago, everybody talking about getting off of Facebook is a big
[01:10:45.240 --> 01:10:53.080]   part of the problem. A lot of these same conversations we're hearing today. And nobody left Facebook.
[01:10:53.080 --> 01:10:59.000]   Right. Everybody seems to hate the fact that Trump has this Twitter megaphone that is
[01:10:59.000 --> 01:11:06.280]   at the beginning was entertaining and is now starting to fundamentally shift geoeconomics and
[01:11:06.280 --> 01:11:12.920]   the geopolitical world order. And it's not like their DA user and my user going down. But I know
[01:11:12.920 --> 01:11:19.960]   of. No, they're going up if anything, I'm sure. So so so again, just like cable news network loves
[01:11:19.960 --> 01:11:26.840]   Donald Trump. So these are publicly traded companies. And for better or worse, you know,
[01:11:26.840 --> 01:11:34.920]   they may be graded by the market, but they have a, you know, if they don't continue to grow,
[01:11:35.480 --> 01:11:42.280]   then they don't continue to, you know, do what it is that they're doing. And as long as they don't
[01:11:42.280 --> 01:11:49.640]   run a foul of actual statute or regulation, then most often they are going to do the thing that
[01:11:49.640 --> 01:11:57.080]   enables them to grow. Is action better than legislation and regulation is it depends on the
[01:11:57.080 --> 01:12:04.200]   situation? If people just get fed up and say, that's it, I quit in enough numbers. But Leo,
[01:12:04.200 --> 01:12:09.080]   these are we're talking about communication systems that are predicated that that I think in some
[01:12:09.080 --> 01:12:14.280]   ways understand us better than we understand ourselves. Right. So nobody's quitting Facebook.
[01:12:14.280 --> 01:12:20.840]   I mean, people are, but not enough, obviously. No. So what does Lee what Wesley and Dylan had
[01:12:20.840 --> 01:12:27.400]   suggested, which is a skinned version of Facebook where I want the Tom Hanks version of Facebook
[01:12:27.400 --> 01:12:33.240]   versus the existing one. You know, what would it take for that to be true? It would take some form
[01:12:33.240 --> 01:12:39.400]   of self realization. We would have to confront the people that we are, you know, and
[01:12:39.400 --> 01:12:47.400]   the way that it exists right now, we enjoy getting our limbic systems kind of like getting fired up.
[01:12:47.400 --> 01:12:53.640]   We get little hits of dopamine. So we have to figure out, because Facebook doesn't have to exist,
[01:12:53.640 --> 01:12:58.920]   you know, like Microsoft has to exist. The lights in my home don't work without Microsoft. But
[01:12:58.920 --> 01:13:03.480]   Facebook, if it went away tomorrow, I think people would be annoyed. But Facebook doesn't
[01:13:03.480 --> 01:13:09.480]   isn't required to make the world run. There is nothing forcing us to use it. There is nothing
[01:13:09.480 --> 01:13:18.120]   forcing news organizations who have been repeatedly screwed over. You know, so as much as newsrooms
[01:13:18.120 --> 01:13:23.800]   complain about Facebook, what then let's all stop? Like, what if what about a Facebook blackout
[01:13:23.800 --> 01:13:28.200]   week? What if everybody was like, we're done. So, you know, instead of just posting a black
[01:13:28.200 --> 01:13:34.840]   square on Instagram, let's actually leave the platform, even the creators who rely on that,
[01:13:34.840 --> 01:13:39.160]   like then you get their attention. But, you know, it's not going to happen.
[01:13:39.160 --> 01:13:44.360]   It's not going to happen. You may say they're not as important as the power in the lights,
[01:13:44.360 --> 01:13:48.440]   but I think there are probably a lot of people who feel like Facebook is just as vital to their
[01:13:48.440 --> 01:13:53.080]   well-being. They may be mistaken, but I think they feel that way. I know every time I say leave
[01:13:53.080 --> 01:13:57.400]   Facebook, they go, I can't do that. I can't do that. Maybe they're just addicted.
[01:13:57.400 --> 01:14:03.960]   Let's take a little break. Wesley Faulkner's here. It's great to have you from Austin, Texas.
[01:14:03.960 --> 01:14:09.320]   The king of developer relations, you would be so lucky to have this guy on your team.
[01:14:09.320 --> 01:14:14.840]   Such a great guy. He'd even bring his, what is it? A paper dragon, a Komodo dragon?
[01:14:14.840 --> 01:14:18.680]   Bearded dragon. Bearded dragon with him from time to time. You can all
[01:14:19.560 --> 01:14:23.000]   see the giant animal. Is it your kids? Bearded dragon?
[01:14:23.000 --> 01:14:30.680]   It's the family. Bearded dragon. Sadly, our Bearded dragon died.
[01:14:30.680 --> 01:14:37.560]   Oh, no. It's three days ago. Oh, my God. Pancake is as of two days ago.
[01:14:37.560 --> 01:14:43.400]   He's a replacement Bearded dragon. Yeah. She was. She's a rescue. There was her previous owners
[01:14:43.400 --> 01:14:46.760]   didn't want her. Oh, she's so how long do Bearded Dragons live?
[01:14:48.040 --> 01:14:56.280]   Gosh, a long time. 20 years. Hundreds of years. So you're going to be handing this down
[01:14:56.280 --> 01:15:01.960]   to your great grandchildren. No, I'm making that. What was the previous? Maybe Shilak.
[01:15:01.960 --> 01:15:09.160]   What was the previous Bearded Dragons name? Luna. Luna. I like Pancake.
[01:15:09.160 --> 01:15:13.800]   Except Pancake sounds like something bad has happened to her. I don't know. Is she flat?
[01:15:15.240 --> 01:15:18.680]   Yes. She's got flat. I should bring it out. You know, I'm going to. I'm going to.
[01:15:18.680 --> 01:15:22.760]   Okay. I'll do a commercial. I'll bring it back. And you bring pancake out.
[01:15:22.760 --> 01:15:29.080]   So the world can meet pancake. Dylan Twenties is also here from Valavel. It's great to see you,
[01:15:29.080 --> 01:15:36.360]   Dylan. Is this your office? Where are we here? This is a good. This is my office. Good. Yes.
[01:15:36.360 --> 01:15:40.600]   Man. Well, yeah, I'm not. I'm not trying to flex or anything like, you know,
[01:15:41.720 --> 01:15:47.720]   zoom, zoom call experts around the world. But yes, those are, those are my books.
[01:15:47.720 --> 01:15:53.480]   This is like a little garden office in the back of my garage. So I like it. I do want to point out
[01:15:53.480 --> 01:16:01.400]   that our very own Jeff Jarvis got, I think the first ever 10 out of 10 on rate my Skype room.
[01:16:01.400 --> 01:16:08.200]   And, and I think, I think honestly, it was because of the bookshelves. They're always a good choice.
[01:16:08.840 --> 01:16:14.280]   Always a good choice. I feel that the books make me look smarter. So that's why I point the camera
[01:16:14.280 --> 01:16:20.120]   in that direction. But yeah, yeah, Jeff's an actual professor. So as is Amy Webb, I'm trying
[01:16:20.120 --> 01:16:25.880]   to find his the post here, I'll find it. Amy Webb has a few books, including her own.
[01:16:25.880 --> 01:16:31.400]   A couple, she's got three great books that I could see the signals are talking. I recognize
[01:16:31.400 --> 01:16:35.720]   that right in front of it. The big nine. That's her latest book. And you're writing a new one.
[01:16:35.720 --> 01:16:39.720]   I'm excited about your new one. I can't wait to do it. I am actually. It's all about synthetic
[01:16:39.720 --> 01:16:46.920]   biology. And I have a thought on how to apply synthetic biology to some of the current social
[01:16:46.920 --> 01:16:52.520]   issues we're having. But it is not fully formed yet. But the book is getting written.
[01:16:52.520 --> 01:16:57.160]   Could we replace ourselves with synthetic people? That might be the solution.
[01:16:57.160 --> 01:17:03.960]   Well, there's synthetic media, which is if you're concerned about deepfakes. I don't know how dark
[01:17:03.960 --> 01:17:08.440]   we want to get, but I can keep you up for many nights. Great.
[01:17:08.440 --> 01:17:14.280]   The first synthetic marketplace. Fantastic. Synthetic marketplace.
[01:17:14.280 --> 01:17:27.880]   Yeah. So how's this? You can license yourself out. You can license your voice, your body,
[01:17:28.440 --> 01:17:35.560]   your face, your gestures out for other people to then use and sort of create synthetic versions of
[01:17:35.560 --> 01:17:42.840]   you to use ostensibly in product demos or in games or possibly commercials. But it's hard to.
[01:17:42.840 --> 01:17:48.600]   From a technological point of view, you can't really prevent those synths from doing other things.
[01:17:48.600 --> 01:17:53.400]   Yeah. Right. So expect some Matthew McConaughey porn soon.
[01:17:54.520 --> 01:18:01.080]   Expect expect expect the best. Expect the absolute worst.
[01:18:01.080 --> 01:18:08.680]   I would actually I would actually gladly rent my likeness out for any purpose at all. But nobody
[01:18:08.680 --> 01:18:14.040]   wants it. So can I put a plug in for since since we're paused? Please put a plug in for
[01:18:14.040 --> 01:18:21.480]   Wesley. I met him at South by last year and was so damn impressed with his really great
[01:18:21.480 --> 01:18:28.040]   understanding of tech and of what it's like to be inside of an organization that is trying to go
[01:18:28.040 --> 01:18:33.960]   through a change and anybody would be smart anywhere in the greater Austin area to scoop him up.
[01:18:33.960 --> 01:18:38.120]   He won't be looking for work for long. I'm sure he's pretty amazing. He's fantastic.
[01:18:38.120 --> 01:18:45.080]   I agree with you 100%. 100%. Here's Jeff Jarvis's 10 out of 10.
[01:18:46.200 --> 01:18:50.760]   Skype room. I think they really I think what really did it was these little chairs here and here.
[01:18:50.760 --> 01:18:58.600]   Yeah. And of course bookshelves. Our show today brought to you by our friends at ExtraHop. Let me
[01:18:58.600 --> 01:19:05.400]   talk a little bit about what ExtraHop does. As you probably have gathered, the new IT reality is
[01:19:05.400 --> 01:19:13.880]   remote access on a massive scale, right? Rapid cloud, multi cloud adoption, a steady increase in
[01:19:13.880 --> 01:19:20.120]   internet of things, devices, and it add on to that a nice layer of cyber crime. And you've
[01:19:20.120 --> 01:19:25.560]   you're baking a cake full of trouble. It's more important than ever that organizations can see
[01:19:25.560 --> 01:19:31.320]   everything going on in their environment. And I don't just mean, you know, your land.
[01:19:31.320 --> 01:19:38.520]   I'm talking about from the cloud to the data center all the way to the customer. In order to
[01:19:38.520 --> 01:19:43.560]   protect and scale your business, you need more than unified visibility. You need context for
[01:19:43.560 --> 01:19:49.800]   what you're seeing for detections. And you need intelligence response workflows so teams can easily
[01:19:49.800 --> 01:19:55.000]   collaborate act fast on that information they're getting. I am talking about the most amazing
[01:19:55.000 --> 01:20:02.520]   platform. It's called ExtraHop. ExtraHop helps you detect threats and performance issues 95%
[01:20:02.520 --> 01:20:08.920]   faster and respond 60% more efficiently. And if you don't believe me, just check out the dashboard
[01:20:08.920 --> 01:20:15.960]   demo. If you go to extrahop.com/twit, it's kind of mind boggling. It's it's eye opening. ExtraHop
[01:20:15.960 --> 01:20:21.160]   helps you keep your business secure, helps you keep it available with SAS based cloud native
[01:20:21.160 --> 01:20:26.680]   network detection and response. And you'll see on the website, you'll see testimonials,
[01:20:26.680 --> 01:20:33.480]   Wizards of the Coast uses extra hop. They secure and support their AWS cloud with extra hop,
[01:20:33.480 --> 01:20:37.880]   chief architect and information security officer at Wizards of the Coast Dam McDaniel. I got the
[01:20:37.880 --> 01:20:43.800]   quote said, there's no other company that aligns to supporting the DevOps model, the speed,
[01:20:43.800 --> 01:20:50.440]   the lack of friction than ExtraHop. The Ulta Beauty, you know, Ulta Beauty, they use extrahop
[01:20:50.440 --> 01:20:55.800]   to secure their Google cloud. And to keep networking and security teams closely aligned,
[01:20:55.800 --> 01:21:01.560]   that means the engineers have more time to focus on innovation. John Crease, who's the senior IT
[01:21:01.560 --> 01:21:06.440]   engineer at Ulta Beauty said before ExtraHop, we had limited visibility into what was going on in
[01:21:06.440 --> 01:21:11.400]   our cloud. But now we can quickly identify vulnerabilities and exploits and understand how
[01:21:11.400 --> 01:21:17.880]   our applications are performing up there in the sky. Take control of your cloud security of your
[01:21:17.880 --> 01:21:24.280]   performance with ExtraHop. Just the information you need presented in an amazingly useful way,
[01:21:24.280 --> 01:21:31.720]   it really is great. You can go to extrahop.com/twitsy, a live product demo, but you'll also find a lot
[01:21:31.720 --> 01:21:37.240]   of information on their tips on securing and supporting remote access. A lot of companies doing that now.
[01:21:37.240 --> 01:21:43.480]   And it's really, this is something you need, extrahop.com/twit.
[01:21:43.480 --> 01:21:51.720]   extrahop.com/twit. We thank them so much for their support of this week in tech. I do want,
[01:21:51.720 --> 01:21:55.720]   I don't want people to think I'm slamming Ben, because I think Ben Thompson's brilliant,
[01:21:55.720 --> 01:22:01.480]   his trajectory is an amazing website. We try to get him on the show as often as possible,
[01:22:01.480 --> 01:22:07.240]   because he's in Taiwan. The hour of this show is very difficult for him. And Monday morning,
[01:22:07.240 --> 01:22:12.600]   he brings the kids to school and stuff. But to read his post from last Tuesday,
[01:22:12.600 --> 01:22:16.840]   I don't think this is premium. It might be. I'm a member. No, I guess not, because I'm not
[01:22:16.840 --> 01:22:26.040]   signed in. Dustin the Light. This is really a fascinating look at the crescent in Madison, Wisconsin.
[01:22:26.680 --> 01:22:31.320]   He's referring, of course, to Dustin the Light is, I think it was Magic Johnson's term,
[01:22:31.320 --> 01:22:38.120]   right, about how racism is just. Karima Jill DeBarra. Karima Jill DeBarrara. Our racism is the dust
[01:22:38.120 --> 01:22:43.000]   that you don't see, that it's all around us until you shine a light on it, and then you can see it.
[01:22:43.000 --> 01:22:51.080]   And he's talking about the kind of inbuilt racism for Black and Hispanic neighborhoods in Madison,
[01:22:51.080 --> 01:23:00.680]   Wisconsin. And as a result, it bends a big data, big data guy as a result, education, poverty,
[01:23:00.680 --> 01:23:07.080]   crime, and on and on and on. And it's, he's casting a little light on the dust in there.
[01:23:07.080 --> 01:23:13.480]   Really, really good stuff. Highly recommend it, Stretakery.com. I just want to give
[01:23:13.480 --> 01:23:17.880]   bit of plug. We'll get him, we'll get him back on. We'll make him wake up the early.
[01:23:18.840 --> 01:23:25.800]   Some other big changes, Alexis O'Hanean, the creator of Reddit, resigning from the board at Reddit.
[01:23:25.800 --> 01:23:33.880]   And he has asked that he to be replaced by a Black board member. Reddit has said, "Yes, we will do
[01:23:33.880 --> 01:23:40.360]   that." He's also using future gains on the company's stock to make a difference in the world, to
[01:23:40.360 --> 01:23:47.480]   serve the Black community. He's, he's tweeted that he's pledging a million dollars to Colin Kaepernick's
[01:23:48.520 --> 01:23:55.960]   your rights camp. You know, it's kind of sad after Kaepernick got so much heat and basically
[01:23:55.960 --> 01:24:03.720]   hounded out of the NFL that Roger Goodell just did. That was not that whole video. That whole
[01:24:03.720 --> 01:24:07.720]   thing was just that talk about so I would actually, can we just pause because I would
[01:24:07.720 --> 01:24:16.280]   actually love to hear Dylan's thoughts on on that if you like the corporate backtrack on that?
[01:24:16.280 --> 01:24:19.480]   Yeah, because that's a, because it's a corporate messaging issue, right?
[01:24:19.480 --> 01:24:22.920]   Yeah. Yeah. Well. Yeah. And of course.
[01:24:22.920 --> 01:24:29.640]   Also, Kaepernick kneeling on the field. I mean, talk about a elegant, beautiful protest
[01:24:29.640 --> 01:24:39.240]   to protest racism and and police, bad policing, racist policing ended up costing him his job
[01:24:39.240 --> 01:24:45.720]   and his career. And ironically, here we are with the police officer kneeling on a Black man and
[01:24:45.720 --> 01:24:51.720]   killing him. And now protesters, I think it's really quite beautiful to see it kneeling and
[01:24:51.720 --> 01:24:57.560]   police sometimes on kneeling with them in support. Go ahead, Dylan.
[01:24:57.560 --> 01:25:01.880]   Yeah, Roger Goodell, just he just saw the winds of change blowing. He's like, crap,
[01:25:01.880 --> 01:25:06.440]   we better. I'm sorry. I shouldn't say like that. Holy cow, we better get on the right side of
[01:25:06.440 --> 01:25:10.360]   public and in here. It's kind of not credible, honestly.
[01:25:10.360 --> 01:25:18.120]   It does not seem credible. Now, he may be limited by legal restrictions because there was some kind
[01:25:18.120 --> 01:25:22.360]   of agreement with Kaepernick. And I don't know what the details are of that. They're probably secret.
[01:25:22.360 --> 01:25:26.840]   And so he may actually have his hands somewhat tied in terms of what he can say.
[01:25:26.840 --> 01:25:32.360]   But now says they were wrong. And if it was wrong and how they handled the players kneeling.
[01:25:32.360 --> 01:25:39.880]   Just it. Yeah. I'm not going to play the video, but look, Kaepernick actually talked to
[01:25:39.880 --> 01:25:46.040]   police before he started kneeling and people in the military to say, what would be a respectful
[01:25:46.040 --> 01:25:54.360]   way for me to show that I protest this? And he got people, actual veterans, to say,
[01:25:54.360 --> 01:25:58.520]   if you take a knee during the anthem, that'll show that you're still being respectful to the
[01:25:58.520 --> 01:26:03.560]   anthem, but just that you object. And then once it started getting popular, then people blew up.
[01:26:05.080 --> 01:26:09.240]   He really got badly, badly mistreated, I think. Yeah.
[01:26:09.240 --> 01:26:18.520]   By the way, we now see Pancake in all her glory. She respond to her name. Does she know her name?
[01:26:18.520 --> 01:26:23.240]   Oh, no, she isn't. Not yet. I bet she responds to pancakes.
[01:26:23.240 --> 01:26:32.200]   She does look flat. Yeah, she's had a lot of babies. That's why. Oh, oh, she's quite beautiful.
[01:26:32.200 --> 01:26:37.400]   I love the skin. The pattern on her back is beautiful.
[01:26:37.400 --> 01:26:43.560]   Yeah. That's nice. So I'm sorry that you lost. I'm really sorry that you lost Luna. That's
[01:26:43.560 --> 01:26:49.400]   terrible. But yeah, that was that was hard on everybody. Yeah. It was a tragic accident.
[01:26:49.400 --> 01:26:52.440]   It was an accident. Oh, I don't want to. Yeah. Don't tell me. Yeah.
[01:26:52.440 --> 01:26:57.480]   Can I say two things really quickly? We can get back to the story. No, do whatever you want.
[01:26:57.480 --> 01:27:04.440]   Say whatever you want. What Amy said about me was extremely touching and I really, really appreciate
[01:27:04.440 --> 01:27:10.920]   it. Thank you so much. I really love you. You just like really impressing the hell out of me.
[01:27:10.920 --> 01:27:15.400]   Oh, yeah. Yeah. I mean, I won't say where you were working at the time, but I was,
[01:27:15.400 --> 01:27:20.840]   I went back and then talked to people that I knew at that place about you and how impressed I was.
[01:27:20.840 --> 01:27:29.800]   So yeah. Oh, thank you. Wow. And thank you so much. I really, I mean, it means a lot to me,
[01:27:29.800 --> 01:27:34.280]   especially like while I'm looking for work, it's extremely, extremely hard,
[01:27:34.280 --> 01:27:41.240]   not just finding a job, but just even being mentally available and present in order to just
[01:27:41.240 --> 01:27:46.760]   do interviews. It's extremely, extremely hard. The second thing I wanted to say really quickly is
[01:27:46.760 --> 01:27:52.440]   that I'm just going to read this. Minneapolis City Council announces intent to dismantle
[01:27:52.440 --> 01:27:57.000]   police department. Wow. Commitment is to end our city's toxic relationship with the
[01:27:57.000 --> 01:28:01.640]   Minneapolis Police Department, City Council President Lisa Bender said on Sunday.
[01:28:01.640 --> 01:28:08.840]   A veto proof majority of Minneapolis City Council members announced during a rally at
[01:28:09.720 --> 01:28:16.200]   Ponder Horn Sunday that they're planning to dismantle the police department. Holy. Wow.
[01:28:16.200 --> 01:28:21.880]   Wow. What, what does that mean? I don't know. We'll see. I guess.
[01:28:21.880 --> 01:28:28.680]   It's called Defunding the Police. Yeah, that's, that's it. I think honestly,
[01:28:28.680 --> 01:28:33.880]   it's so clear the systemic racism, the mess that is in the Minneapolis Police Department
[01:28:34.680 --> 01:28:42.440]   needs a pretty dramatic cure. And I think the head of the police, like,
[01:28:42.440 --> 01:28:47.240]   association says, I've been involved in several officers involved shootings and it hasn't bothered
[01:28:47.240 --> 01:28:52.440]   me. Yeah. No, the union leader is horrific. Yeah. He's horrific.
[01:28:52.440 --> 01:28:56.680]   You know, that's, that's, I got to say Minneapolis,
[01:29:00.680 --> 01:29:09.720]   Minneapolis is a pretty cool city. It's awful. It's, it's sad. That's bad. It's really sad.
[01:29:09.720 --> 01:29:17.160]   But you know, I look at Madison, Wisconsin too. Both considered bastions of, of conscious
[01:29:17.160 --> 01:29:22.760]   liberalism, of, you know, people who really care, progressivism even. And yet you see the
[01:29:22.760 --> 01:29:28.600]   systemic racism in integrated, woven into the fabric of these communities. And it just reminds
[01:29:28.600 --> 01:29:34.760]   you it's everywhere. And, and, and the thing is, it's so woven in, it is the dust in the air.
[01:29:34.760 --> 01:29:40.200]   And it's time to clear that out. Wow. That is, they rebooted. That's dramatic.
[01:29:40.200 --> 01:29:44.840]   Rebooting is sometimes the only thing that is dramatic.
[01:29:44.840 --> 01:29:50.200]   Holy smokes. I don't know. Yeah. It's hard to imagine what this means.
[01:29:50.200 --> 01:29:58.120]   Wow. The, I think what it means is that every other police
[01:29:58.600 --> 01:30:04.040]  , you know, this is one way to, you don't want to be in a situation where you are forced
[01:30:04.040 --> 01:30:08.840]   to confront your cherished beliefs and you have, you're going to be on the losing side. So if I was
[01:30:08.840 --> 01:30:14.440]   every other police department in the country, I would confront the cherished beliefs right now
[01:30:14.440 --> 01:30:18.680]   and like figure out how to make changes and then make good on those changes publicly.
[01:30:18.680 --> 01:30:24.040]   Minneapolis City Council President Lisa Bender said, our efforts at incremental reform have failed.
[01:30:24.600 --> 01:30:31.640]   Period. Minneapolis Public Schools stopped using police, University of Minnesota stopped using
[01:30:31.640 --> 01:30:38.280]   MPD, Minneapolis Parks and Rec, no longer using MPD for security. So there is really a lack of
[01:30:38.280 --> 01:30:44.600]   faith and, and, and, you know, belief in the integrity of the Minneapolis Police Department.
[01:30:44.600 --> 01:30:50.680]   That is, that is just wild. Wow.
[01:30:53.560 --> 01:30:59.240]   That's, that's what I mean by I think the voice of the people is starting to be heard.
[01:30:59.240 --> 01:31:05.240]   And that's why it's important to stand up and speak and to protest and to get out in the streets
[01:31:05.240 --> 01:31:11.560]   and to let people know you can't, you have to, you have to speak up. You can't just sit in silence
[01:31:11.560 --> 01:31:18.840]   and say, yeah, I agree with that. You got to speak up. You got to do something. Wow. Back to,
[01:31:18.840 --> 01:31:23.080]   back to our schedule program. Sorry for interrupting. No, that's a huge, huge, huge, huge, huge,
[01:31:23.080 --> 01:31:26.600]   you learn on story. Oh my God. Yeah, it's worth it.
[01:31:26.600 --> 01:31:34.040]   It's a, so this is an opportunity to think about next order implications. So this is again,
[01:31:34.040 --> 01:31:38.760]   another quick inflection point. So again, everybody now can stop and think through many
[01:31:38.760 --> 01:31:45.240]   different alternative futures. Because the, the, you know, what we are habituated to doing is to
[01:31:45.240 --> 01:31:49.000]   take signals from the present and extrapolate them out into the future, which would mean what we
[01:31:49.000 --> 01:31:53.720]   just need to do next is like fire all these police officers and have a brand new total,
[01:31:53.720 --> 01:31:57.880]   you know, new to police department with all different people, which may not solve the problem.
[01:31:57.880 --> 01:32:02.760]   So you'd have to work fast, but there's an opportunity here to get creative and think about
[01:32:02.760 --> 01:32:07.960]   many different plausible future, future states, you know, and there's a huge
[01:32:07.960 --> 01:32:12.600]   upper catastrophe. I always say this catastrophe can sometimes be a catalyst for great change.
[01:32:12.600 --> 01:32:18.520]   And I think now is that time. I'd be very curious to see how they, how they're thinking
[01:32:18.520 --> 01:32:22.600]   through the next order impacts. Wow.
[01:32:22.600 --> 01:32:31.400]   It seems trivial compared to this, but I'm going to mention Zoom has said that we will have end-to-end
[01:32:31.400 --> 01:32:38.280]   encryption, but not for the free tier. Because then this was kind of a tone deaf
[01:32:39.720 --> 01:32:48.840]   statement from the CEO, Eric Yuan, the CEO of Zoom. We don't want to give free users
[01:32:48.840 --> 01:32:56.120]   encryption because we also want to work together with the FBI and local law enforcement in case
[01:32:56.120 --> 01:33:04.040]   some people use Zoom for a bad purpose. Of course, paid customers would never use Zoom for a bad
[01:33:04.040 --> 01:33:09.000]   purpose. Only those cheapskates using the free tier.
[01:33:09.000 --> 01:33:18.920]   I love that we live in a time when headline editors on CNET can, what was the headline? It was like
[01:33:18.920 --> 01:33:23.400]   Zoom will not offer free encryption because they want to help the cops. Yeah, there you go.
[01:33:23.400 --> 01:33:32.440]   There you go. Zoom one in and in. So Alex Stamos, who of course is the former security guy at Yahoo
[01:33:32.440 --> 01:33:36.360]   and Facebook, who was hired as a consultant by Zoom,
[01:33:36.360 --> 01:33:45.320]   said, "You know, this makes sense." He said, "Will this eliminate all abuse?" No, but since the vast
[01:33:45.320 --> 01:33:51.640]   majority of harm comes from self-service users with fake identities, this will create friction
[01:33:51.640 --> 01:33:57.720]   and reduce harm. But police are going to come after some idiot Zoom bomber. That doesn't make
[01:33:57.720 --> 01:34:05.720]   any sense. I think their concern is people using free accounts to, I don't know,
[01:34:05.720 --> 01:34:09.480]   coordinate terrorist cells or trade child porn or something like that. But the good news is you
[01:34:09.480 --> 01:34:19.000]   can only do that for 40 minutes at a time. So I guess the idea is if you pay for an account,
[01:34:19.000 --> 01:34:24.920]   we have your information. I think that that kind of minimizes how easy it is to anonymize
[01:34:24.920 --> 01:34:32.520]   this stuff. But okay. Well, meantime, Mozilla is working on a new video conferencing platform
[01:34:32.520 --> 01:34:38.280]   that's totally decentralized called Me-thing, I think. I haven't tried it yet. We use JITC,
[01:34:38.280 --> 01:34:44.680]   or actually we don't. I have a JITC server. I set up JITC. It's running. It even has a cute
[01:34:44.680 --> 01:34:52.440]   URL, twit.team. I can't get anybody here to use it. We keep using Zoom for some reason in Google
[01:34:52.440 --> 01:34:58.280]   Meet. They want to make sure that it can be seen by the police. Yeah, maybe that's it. I don't know.
[01:34:58.280 --> 01:35:02.120]   Yeah, because we run our own server, it's end-end encrypted and the only decryption happens at
[01:35:02.120 --> 01:35:08.920]   server side, which is local and not. So it is end-end encrypted. I don't know. You can't,
[01:35:08.920 --> 01:35:17.560]   long ago, it's just, you know, people use what's easiest. But I think a company who's been like
[01:35:17.560 --> 01:35:24.760]   lambasted for security is intentionally choosing not to encrypt. That sounds weird.
[01:35:24.760 --> 01:35:31.480]   Yeah. Sounds like a bad business. And I have to say, you know, I was a little, it looked a little
[01:35:31.480 --> 01:35:37.320]   bit like showboating when Zoom went out and they hired Stemos and they hired Katie Missouri's
[01:35:37.320 --> 01:35:41.560]   company and they bought Keybase. They went out and they spent, they have a lot of money. They've
[01:35:41.560 --> 01:35:45.960]   been doing very well. They spent a lot of money in all the trappings of security. But this just
[01:35:45.960 --> 01:35:51.080]   shows you when you buy somebody like Alex Stemos as a consultant, what's he going to do? Say, yeah,
[01:35:51.080 --> 01:35:59.480]   that was a bad idea. I quit. That's why it's the trappings of security as opposed to maybe,
[01:35:59.480 --> 01:36:04.200]   now they put out a white paper, Steve Gibson, our security guy went through it a couple of weeks
[01:36:04.200 --> 01:36:08.520]   ago. He said, Zoom's doing, you know, looks like what Zoom's doing for end-end encryption is really
[01:36:08.520 --> 01:36:13.320]   good. But of course, if they don't turn it on for everybody, I don't know, I don't know how useful
[01:36:13.320 --> 01:36:18.520]   it is. I guess that's a reasonable thing to say is that we don't want, we don't want bad actors
[01:36:18.520 --> 01:36:24.760]   just to use free accounts. They didn't really say, but, and we don't really have access to this data
[01:36:24.760 --> 01:36:29.880]   as far as I know, but it's possible that they are seeing substantial, you know, questionable or
[01:36:29.880 --> 01:36:34.360]   criminal activity utilizing free accounts and they want to make sure that stuff doesn't go
[01:36:34.360 --> 01:36:38.760]   underground. They may know more. That's a good point. I can't, I can't imagine like what is the
[01:36:38.760 --> 01:36:43.000]   mafia having a Zoom meeting because they can't get together because they're COVID and they're all,
[01:36:43.000 --> 01:36:47.960]   you know, doing their mafia stuff with the gallery view, it makes no sense.
[01:36:47.960 --> 01:36:54.760]   Okay, that has to be a Saturday Night Live segment. Okay, just saying.
[01:36:54.760 --> 01:36:59.720]   Like, you know, one of them is like too old and kind, like doesn't quite understand how to connect
[01:36:59.720 --> 01:37:02.920]   things and, you know, like, what the hell is going on in Zoom?
[01:37:02.920 --> 01:37:10.760]   Hey, I can't hear you. Oh my God.
[01:37:10.760 --> 01:37:16.520]   Who's doing what, what the hell is happening in 40 minutes on Zoom that would have
[01:37:16.520 --> 01:37:20.840]   wired this like totally tone deaf public service announcement.
[01:37:20.840 --> 01:37:25.000]   All right, I have a theory. I'm going to pass this, I'm going to pass this by because my theory is,
[01:37:26.760 --> 01:37:36.360]   this is, this is, he's terrified of the FBI and he's terrified of Attorney General Barr
[01:37:36.360 --> 01:37:42.280]   and he doesn't want anybody in law and federal law enforcement to say, and by the way, we got
[01:37:42.280 --> 01:37:48.760]   an end in encryption for Zoom. So this is a bone. He's throwing him a bone. It even put it in the
[01:37:48.760 --> 01:37:56.680]   headline because he doesn't want William Barr to say along with, we got to stop encryption on Facebook
[01:37:56.680 --> 01:38:02.200]   messenger and WhatsApp and we got to stop encryption here and there. He doesn't want him to add that
[01:38:02.200 --> 01:38:10.200]   Zoom to that list, right? I think that's money. I will pay real money to watch a term. I would
[01:38:10.200 --> 01:38:14.840]   love to see Barr organize himself, get onto a Zoom and make it work.
[01:38:14.840 --> 01:38:21.080]   That is, that'll be my contribution of somebody.
[01:38:24.920 --> 01:38:30.520]   Yeah, I mean, it just, that don't be ages. Just because, you know, he's an old guy.
[01:38:30.520 --> 01:38:34.360]   Just have you heard the, you're right.
[01:38:34.360 --> 01:38:40.760]   I listen, I'm having problems with it. I'm orange and I've gone through three different
[01:38:40.760 --> 01:38:44.440]   head microphones, but it's working, but it's working. We got it working. That's the key. That's
[01:38:44.440 --> 01:38:48.760]   the video. But I mean, it's like, you know, let's take a little break. That's Amy Webb. We've got
[01:38:48.760 --> 01:38:53.800]   Wesley Faulkner with us. We've got Dylan Twentie. It's great to have all three of you. This is,
[01:38:53.800 --> 01:38:57.880]   this is a great panel. It couldn't be a better time to have all three of you here.
[01:38:57.880 --> 01:39:05.400]   Let's actually, what we want to do before I do the ad, we have a promo, right? Let's take a look
[01:39:05.400 --> 01:39:11.720]   at what happened. There was no, we decided it would be a bad idea to do our usual,
[01:39:11.720 --> 01:39:17.240]   Twitter after hours on Friday. It's just too difficult a time. But we'd still had some pretty
[01:39:17.240 --> 01:39:19.880]   good shows this week and I think you'll agree when you watch this.
[01:39:19.880 --> 01:39:25.640]   Previously on Twitch, I thought you were going to celebrate Jeff Jarvis, the virtual Chipotle
[01:39:25.640 --> 01:39:31.320]   Carsten. I, I Chipotle, what's your mouth out? Thanks to Carsten. You see that bottle of official
[01:39:31.320 --> 01:39:35.320]   Taco Bell hot sauce. I'll think of you every time I burp.
[01:39:35.320 --> 01:39:40.520]   Mac break weekly. If you're going to loot an Apple store,
[01:39:40.520 --> 01:39:47.000]   you might think twice. Apparently, none of that stuff is going to be usable. It says,
[01:39:47.000 --> 01:39:51.720]   please return to Apple Walnut Street. This device has been disabled and is being tracked.
[01:39:51.720 --> 01:39:58.040]   Tech news weekly. Many people are, obviously, they're taking to the streets. They're making
[01:39:58.040 --> 01:40:03.240]   their voices heard right now. It's important to be sure that they can do this safely.
[01:40:03.240 --> 01:40:06.920]   One of the key missions for electronic frontier foundation is making sure that people have,
[01:40:06.920 --> 01:40:12.200]   you know, the, the freedom to assemble and to share their, share their voice with folks without
[01:40:12.200 --> 01:40:15.960]   concerned about government repression or warrantless surveillance. So it's important to make sure that
[01:40:15.960 --> 01:40:20.600]   we're, that we're also providing resources where legal protections are not sufficient.
[01:40:20.600 --> 01:40:28.360]   Smart tech today. Matthew, the king of shortcuts, Cass and Ellie has a shortcut for iOS. Should you
[01:40:28.360 --> 01:40:35.080]   be pulled over by police officers or are in a situation where you feel unsafe and want to record
[01:40:35.080 --> 01:40:39.880]   the situation? Since this, this actually went viral. I think like 5,000 people have downloaded
[01:40:39.880 --> 01:40:45.000]   their shortcuts so far or one of the versions I made, which at least makes me feel better that
[01:40:45.000 --> 01:40:48.920]   maybe some people who are out there protesting were able to stay in contact with their family.
[01:40:48.920 --> 01:40:56.280]   Twit.tv. Tell your friends. That's awesome. I wish you remember, this is the less quoted part
[01:40:56.280 --> 01:41:04.200]   of the First Amendment, but the right of people to peacefully assemble to protest is protected
[01:41:04.200 --> 01:41:08.200]   speech. According to the First Amendment, it's right in there. It's written right in there.
[01:41:09.080 --> 01:41:15.400]   I'll show today brought to you by Barracuda. That's a name everybody who knows security and
[01:41:15.400 --> 01:41:21.400]   technology and and the internet knows Barracuda, the provider of cloud enabled enterprise grade
[01:41:21.400 --> 01:41:28.440]   security solutions to protect you in email and networks and data in applications. Barracuda is
[01:41:28.440 --> 01:41:33.160]   the best. I mean, just the best in the business and they have an email solution that's really
[01:41:33.160 --> 01:41:38.920]   important these days with your employees going home, working from home, getting email. It gives me
[01:41:38.920 --> 01:41:46.920]   it wakes me up at night because I think about my employees. They're at home. Here in the studio
[01:41:46.920 --> 01:41:53.960]   when email comes in, we protect them. But now they're at home and the email is coming in.
[01:41:53.960 --> 01:42:01.880]   Did you know that 91% of all cyber attacks start in an email spearfishing, ransomware account take
[01:42:01.880 --> 01:42:07.160]   over conversation hijacking? You've got dozens, hundreds, maybe thousands of employees working
[01:42:07.160 --> 01:42:13.880]   remotely. Each of them getting tons of email every day. Any one of those emails can put you out of
[01:42:13.880 --> 01:42:18.920]   business. One click on the wrong email, it costs you money, costs you customers, costs you your
[01:42:18.920 --> 01:42:26.200]   reputation. Barracuda has a great blog, which I highly recommend at the website. You can read
[01:42:26.200 --> 01:42:31.320]   about their they've been analyzing emails. They do it anyway. As part of their business,
[01:42:31.320 --> 01:42:37.080]   they have been the steady increase in coronavirus related spearfishing attacks since January
[01:42:37.080 --> 01:42:45.720]   since the end of February, 667% more spearfishing attacks. Of course, the bad guys use current
[01:42:45.720 --> 01:42:51.960]   events to get to you. And you might receive, for instance, an email from somebody impersonating
[01:42:51.960 --> 01:42:59.560]   the World Health Organization saying open these tips for staying safe. It can come from anybody.
[01:42:59.560 --> 01:43:05.400]   And that's why you need Barracuda total email protection. It includes all in one email security
[01:43:05.400 --> 01:43:12.520]   for your Microsoft 365 account. You get backup, you get archiving, probably a legal requirement.
[01:43:12.520 --> 01:43:17.800]   It is in many businesses. You get AI based protection. That's important because
[01:43:17.800 --> 01:43:22.680]   spearfishing attacks are constantly mutating, changing to take advantage of the times. You need
[01:43:22.680 --> 01:43:29.720]   protection that changes faster. They've got AI based protection from spearfishing account takeover,
[01:43:29.720 --> 01:43:34.600]   business email, compromise. You get an automated incident response that gives you options to
[01:43:34.600 --> 01:43:39.960]   quickly and efficiently address attacks. As you probably know, when an attack happens,
[01:43:39.960 --> 01:43:45.640]   the speed with which you respond to it is directly related to how easy it is to fix the problem.
[01:43:45.640 --> 01:43:51.560]   Take your time. You're going to have a much bigger problem. Barracuda total email protection
[01:43:51.560 --> 01:43:55.960]   helps you get it done fast. They even have security awareness training. It'll help you
[01:43:55.960 --> 01:44:00.440]   educate your workforce. So employees could be the first line of defense against attacks.
[01:44:01.240 --> 01:44:06.760]   Barracuda. Right now there are new attacks out there taking advantage of the situation we're
[01:44:06.760 --> 01:44:11.240]   going through right now. I guarantee you there's spearfishing email that says Black Lives Matter.
[01:44:11.240 --> 01:44:17.720]   I guarantee you and you got to stop that before it gets. And this is my nightmare employee.
[01:44:17.720 --> 01:44:22.520]   Gets a spearfishing attack. They open it up. Maybe it's an email from me saying,
[01:44:22.520 --> 01:44:28.840]   got to I need you to look at this, review this spreadsheet today. They open it up. Nothing happens.
[01:44:29.480 --> 01:44:34.760]   It sits there quietly in wait on their laptop till they come to work. They join the company
[01:44:34.760 --> 01:44:39.800]   network and then it goes, I'm free and it spreads throughout the network. That's my nightmare.
[01:44:39.800 --> 01:44:44.600]   That's why you need total email protection from Barracuda. Ensure the safety and security of
[01:44:44.600 --> 01:44:49.400]   your business with Barracuda. Uncover the threats hiding in your inbox. Get a free,
[01:44:49.400 --> 01:44:54.920]   I like this. You can get this for free. Secure email threat scan of your Office 365 account.
[01:44:54.920 --> 01:45:04.200]   It's risk free. Just go to barracuda.com/twit. Barracuda.com/twit. Barracuda. You need now more
[01:45:04.200 --> 01:45:10.680]   than ever. You need Barracuda. Your journey secured. We thank you for their support of our show.
[01:45:10.680 --> 01:45:21.160]   And actually tying right into this, XSim, which is a widely used, I don't know if it's the most
[01:45:21.160 --> 01:45:28.360]   used mail transfer agent, but it is fairly widely used by a lot of mail servers and a lot of individuals
[01:45:28.360 --> 01:45:35.880]   at home. It has a well-known bug. It comes pre-installed at some Linux distros. So you may not,
[01:45:35.880 --> 01:45:43.080]   you may have it, may not even know it. CVE-2019-10149 allows a remote attacker to exit commands and
[01:45:43.080 --> 01:45:48.920]   code of their choosing. And apparently it's actively being exploited right now by the GRU.
[01:45:48.920 --> 01:45:56.840]   The Russian general staff, main intelligent director, main center for special technologies.
[01:45:56.840 --> 01:46:02.280]   Russian spies and hackers have been using this to add privileged users,
[01:46:02.280 --> 01:46:07.800]   disable network security settings, execute additional scripts for network exploitation.
[01:46:07.800 --> 01:46:15.880]   And they're going after both the Biden campaign and the Trump campaign. Everybody, it's vulnerable.
[01:46:15.880 --> 01:46:24.360]   Advanced persistent threat phishing emails, Google's tag researchers are warning that
[01:46:24.360 --> 01:46:29.400]   APTs are targeting staffers for both the Trump and the Biden campaign.
[01:46:29.400 --> 01:46:35.320]   Shane Huntley with Google's threat analysis group said on Twitter on Thursday, two separate
[01:46:35.320 --> 01:46:40.760]   phishing campaigns recently detected. One from China, another from Iran.
[01:46:40.760 --> 01:46:49.800]   The tax from Iran are on the rise. In the very recent few weeks.
[01:46:49.800 --> 01:46:53.640]   They've got good hackers. They have in the past.
[01:46:53.640 --> 01:47:00.920]   Good hackers bad name though. When I think of the GRU, I think of the group from all I can hear.
[01:47:00.920 --> 01:47:06.520]   I think of a bunch of minions poking away and keep girls funny voice.
[01:47:06.520 --> 01:47:15.000]   I think one thing that we need as a society or like as a company or organization is a way to
[01:47:15.000 --> 01:47:22.200]   like be notified and be able to put these patches in quickly. Because this is a really pet bug.
[01:47:22.200 --> 01:47:27.160]   It allows you to just take root control and push any software you want.
[01:47:27.720 --> 01:47:32.920]   It was fixed last year, but now it's being used. Yeah, it's CV 2019. It's from last year.
[01:47:32.920 --> 01:47:39.720]   It's just people like, "Oh, I didn't know. Do I ask you on their email list?" There's not an easy
[01:47:39.720 --> 01:47:45.480]   way to maintaining all of these systems. These are dusty systems in the closet.
[01:47:45.480 --> 01:47:49.160]   They've been running for years, mailing lists and stuff.
[01:47:49.160 --> 01:47:52.040]   Or in your ISP's closet.
[01:47:52.920 --> 01:47:58.040]   Yeah, that's right. That is terrifying. Can you imagine that kind of a deployment?
[01:47:58.040 --> 01:48:02.280]   That's the scariest to me.
[01:48:02.280 --> 01:48:06.200]   Thanks, Dylan.
[01:48:06.200 --> 01:48:11.240]   Yeah, you just terrified us, Dylan. Nice job.
[01:48:11.240 --> 01:48:16.760]   Well, you work with mail, right? Vala mail is about authentication, but you're probably very
[01:48:16.760 --> 01:48:22.200]   aware of all the threats that come into it. Yeah, we've actually seen a spike in coronavirus
[01:48:22.200 --> 01:48:28.600]   related fishing in the last couple of months as well. We focus on, in particular,
[01:48:28.600 --> 01:48:37.000]   as center identity. So we see a lot of impersonations, bogus domains like cdc.agency,
[01:48:37.000 --> 01:48:43.240]   which is not really a thing. The WHO is now protected with this technology called DMARC,
[01:48:43.240 --> 01:48:49.240]   but for a long time it was very easy to just send email from WHO.int, which is actually their
[01:48:49.240 --> 01:48:55.720]   domain because there was no DMARC protection. So we see a lot of that. And haven't actually
[01:48:55.720 --> 01:49:02.200]   looked or noticed any Black Lives Matter related fishing, but because of the way our network works,
[01:49:02.200 --> 01:49:05.960]   it might take a little while before we see that kind of thing.
[01:49:05.960 --> 01:49:11.000]   Yeah, I'm speculating. I haven't seen any, but you know, hackers have no
[01:49:11.000 --> 01:49:15.960]   no compunction. They have, they just don't do whatever they, whatever works, right?
[01:49:15.960 --> 01:49:20.280]   They'll jump right on whatever works. I want you to tell me if this is true,
[01:49:20.280 --> 01:49:24.840]   and I read this the other day. I wish I can remember the source. You know, all those Nigerian
[01:49:24.840 --> 01:49:31.800]   prints, you know, fishing scams that you see the scam emails, they're almost always detectable
[01:49:31.800 --> 01:49:37.080]   because the English is bad or the spelling is bad. I just read that that's actually intentional.
[01:49:37.080 --> 01:49:42.360]   Because they're English speakers. They don't. Yes. It's not that they don't understand the
[01:49:42.360 --> 01:49:47.560]   language. They're just much more interested in people too stupid to notice because they're much
[01:49:47.560 --> 01:49:53.480]   easier marks. You know, that might be it might also have to do with getting through
[01:49:53.480 --> 01:50:00.760]   spam algorithm. You misspell something and then it can't get caught. Yeah, that's right.
[01:50:00.760 --> 01:50:04.920]   It doesn't look like it's that concerted though. They're not trying to hide certain terms.
[01:50:04.920 --> 01:50:09.080]   It makes perfect sense to me that they don't want somebody who's smart enough to get,
[01:50:09.080 --> 01:50:13.160]   well, that's ungrammatical. That must be wrong. They want somebody who goes,
[01:50:13.160 --> 01:50:22.600]   Nigerian prints left me money. Oh, I love that theory, but I would be cautious of like putting
[01:50:22.600 --> 01:50:28.760]   too much stock in it. I think that smart people click on fishing email to Leo. I'm sorry to say.
[01:50:28.760 --> 01:50:37.080]   What? What? I did. You know where I get I almost have gotten bit twice is not email,
[01:50:37.080 --> 01:50:45.320]   but in text messages. I got one because my son lost his iPhone and about three weeks later,
[01:50:45.320 --> 01:50:50.440]   it wasn't probably was just coincidence. I got an email from a text from Apple saying,
[01:50:50.440 --> 01:50:56.520]   we think we found your lost iPhone. Click this link except I looked and it wasn't apple.com.
[01:50:56.520 --> 01:51:02.520]   Some bizarre link and I almost clicked on it. Now, who knows if that would have been sufficient,
[01:51:03.080 --> 01:51:08.040]   although there are plenty of exploits out there that somebody called the radio show today. I
[01:51:08.040 --> 01:51:16.040]   thought this was interesting. Sure, he'd been hacked because he his wife wanted a Facebook
[01:51:16.040 --> 01:51:23.640]   messenger on her Kindle fire tablet. Instead of going to the Kindle store to get messenger,
[01:51:23.640 --> 01:51:29.560]   he Googled it and clicked a link. Then all of a sudden, all sorts of weird things were happening.
[01:51:29.560 --> 01:51:33.800]   I thought, well, I don't first of all, he didn't put he didn't install anything on the Kindle
[01:51:33.800 --> 01:51:39.240]   fire and probably that wouldn't do it. But then I realized that could very well have been an exploit
[01:51:39.240 --> 01:51:46.520]   aimed at his router. That's a big problem too, because routers never get patched. They're in
[01:51:46.520 --> 01:51:50.920]   service for years. There are many. Nobody knows how to configure them. Nobody knows how to
[01:51:50.920 --> 01:51:57.240]   average people don't know. There are many widely distributed router hacks. Remember in the FBI,
[01:51:57.240 --> 01:52:01.160]   two years ago, said, "Everybody turn off your router and then turn it back on. I can't reboot
[01:52:01.160 --> 01:52:08.200]   your router." Remember that? After talking to the guy for a little while, I said, "Yeah,
[01:52:08.200 --> 01:52:13.960]   actually technically, I guess you could have been a router drive by." The other one that I
[01:52:13.960 --> 01:52:19.480]   almost clicked on was when I got something from, looked like Twitter, but it actually was Tvitter.
[01:52:19.480 --> 01:52:28.280]   It was Tvv it. That's awesome. Log in the Twitter. What is that called?
[01:52:28.280 --> 01:52:33.400]   Domain spoofing domain. That's lookalike domain. That's not that bad. Lookalikes.
[01:52:33.400 --> 01:52:42.040]   Yeah. I now have, I used the next DNS.io to filter. It's kind of like a pie hole in the cloud.
[01:52:42.040 --> 01:52:47.800]   And that's one of the things it looks out for is close, but not exactly the same.
[01:52:48.600 --> 01:52:54.600]   It gets tricky too, because sometimes you can put Unicode characters into domains now.
[01:52:54.600 --> 01:52:58.760]   Right. You could have that little Turkish I that doesn't have a dot or there's different kinds of
[01:52:58.760 --> 01:53:02.360]   As that look exactly like the A and there's no visual difference.
[01:53:02.360 --> 01:53:04.280]   How do you tell?
[01:53:04.280 --> 01:53:05.880]   Right. Sirelik letters.
[01:53:05.880 --> 01:53:13.560]   I was going to say, I lived in Hong Kong when the huge debate for the TDLs was raging over whether
[01:53:13.560 --> 01:53:19.560]   or not to allow non English letters in the domains. And so like, they were going to allow Chinese
[01:53:19.560 --> 01:53:24.760]   characters for the first time ever, but it was like this huge fight and huge security threat and
[01:53:24.760 --> 01:53:29.480]   all of these things. And like here we are 20 years later and Leo is on to Vitter.
[01:53:29.480 --> 01:53:33.960]   To Vitter. I loved it. It's wonderful. I love it.
[01:53:36.120 --> 01:53:44.920]   One thing that's going to happen, I feel like is for corporate employees falling for domains
[01:53:44.920 --> 01:53:49.960]   that they aren't looking for. For instance, there's a lot of, hey, take this corporate training and
[01:53:49.960 --> 01:53:54.200]   it's not on your company's domain. It's on some other company, but it's legit. But how do you know?
[01:53:54.200 --> 01:53:59.480]   Because you're not sure. There's a lot of that going on, especially is like, hey, take this
[01:53:59.480 --> 01:54:05.800]   unconscious bias training or watch this video. There's a lot of directives coming from domains.
[01:54:06.120 --> 01:54:10.440]   That aren't in the company you belong to. That people who are in the company think,
[01:54:10.440 --> 01:54:13.800]   well, it sounds official and it's hard to tell whether or not it is or not.
[01:54:13.800 --> 01:54:14.200]   Yeah.
[01:54:14.200 --> 01:54:17.560]   Isn't there some kind of like, is this spoofed? You know what I mean? Like,
[01:54:17.560 --> 01:54:25.640]   it wasn't there a website where you could put a URL in and see if it was spoofed.
[01:54:25.640 --> 01:54:26.520]   Oh, that would be good.
[01:54:26.520 --> 01:54:29.480]   Well, I'm just saying it for these, they don't need to be spoofed.
[01:54:29.480 --> 01:54:30.280]   It could be called.
[01:54:30.280 --> 01:54:30.680]   It's a real site.
[01:54:31.480 --> 01:54:35.560]   CorporateTraining.com. You can register and then send it to all the employees and saying,
[01:54:35.560 --> 01:54:36.680]   you have to do this corporate training.
[01:54:36.680 --> 01:54:39.880]   I get that all the time because I still work for somebody. I work for
[01:54:39.880 --> 01:54:46.520]   iHeart Media. My radio show is syndicated by Premiere. And we get a lot of trainings.
[01:54:46.520 --> 01:54:52.040]   I have a training I'm supposed to take. I can't get in. They have a lot of security on the getting
[01:54:52.040 --> 01:54:53.960]   in part and then the training's in flash.
[01:54:57.800 --> 01:55:03.320]   You have real clearance. Yeah, practically. But then the training is something like,
[01:55:03.320 --> 01:55:07.560]   how to meet with people safely. I think it's going to be like a COVID.
[01:55:07.560 --> 01:55:09.480]   Flash is a good intermediary for that.
[01:55:09.480 --> 01:55:12.040]   Yeah. Yeah, use flash. That's safe.
[01:55:12.040 --> 01:55:16.920]   Every time I have to take these trainings, I have to call the IT department and say,
[01:55:16.920 --> 01:55:22.600]   okay, I need my Okta reset and my password because they're one of those companies,
[01:55:22.600 --> 01:55:25.640]   it drives me crazy. It rotates its passwords every three months.
[01:55:26.200 --> 01:55:29.560]   That's going to make things more secure, making Leo have to call you
[01:55:29.560 --> 01:55:34.120]   because I have to use my password in three months. It's just nuts.
[01:55:34.120 --> 01:55:38.280]   Then they use flash. I have to install flash.
[01:55:38.280 --> 01:55:47.160]   Italian public prosecutor says, Project Gutenberg is piracy.
[01:55:47.160 --> 01:55:51.160]   Apparently, I didn't know this, but in Italy,
[01:55:54.360 --> 01:55:58.520]   the Italian communications watchdog, I guess their FCC, the Agcom,
[01:55:58.520 --> 01:56:04.280]   set up new administrative copyright enforcement powers that allows them to just
[01:56:04.280 --> 01:56:11.240]   declare sites are infringing at which point all the ISPs in Italy are required to block the site.
[01:56:11.240 --> 01:56:13.960]   That's it. No repeal, no trial, nothing.
[01:56:13.960 --> 01:56:19.960]   And Italy not have other things to be worrying about at this particular moment.
[01:56:21.240 --> 01:56:25.880]   I mean, the mafia right now is zooming. Yeah, they're zooming. They're zooming.
[01:56:25.880 --> 01:56:30.520]   It would be nice if you would be a terrible thing.
[01:56:30.520 --> 01:56:41.000]   So apparently, there was a post somewhere that listed a bunch of sites
[01:56:41.000 --> 01:56:48.600]   for piracy and Project Gutenberg, which as we all know is not a pirate site, but in fact,
[01:56:48.600 --> 01:56:56.200]   a wonderful useful tool full of public domain, text, books. It's really great.
[01:56:56.200 --> 01:57:00.680]   They were monitoring a telegram private channel with links to pirate sites.
[01:57:00.680 --> 01:57:03.560]   They blocked them all, including Project Gutenberg.
[01:57:03.560 --> 01:57:10.760]   Anyway, if you can't get to Project Gutenberg in Italy,
[01:57:10.760 --> 01:57:16.040]   I don't know who you call. Who are you going to call? Express VPN.
[01:57:16.040 --> 01:57:21.160]   There you go. That's what VPNs were made for. You nailed it. Perfect. Perfect answer.
[01:57:21.160 --> 01:57:25.640]   I think it's funny that even the pirates are padding their libraries of free content.
[01:57:25.640 --> 01:57:35.880]   We need more. HBO Max, perfect example of net neutrality and why you need it.
[01:57:35.880 --> 01:57:45.080]   AT&T, which owns HBO Max, has announced that you can watch all you want
[01:57:45.080 --> 01:57:50.840]   on the AT&T wireless network because it's not going to hit your data caps.
[01:57:50.840 --> 01:57:53.640]   If you can figure out how to find it.
[01:57:53.640 --> 01:57:57.480]   Yeah, good luck getting it. You're actually eligible to sign on.
[01:57:57.480 --> 01:58:02.440]   Netflix will, Disney plus will, everybody else will, but not the ones we own.
[01:58:02.440 --> 01:58:10.680]   Turns out, and thank you, Neil I Patel for focusing on this on the Verge, turns out AT&T
[01:58:11.320 --> 01:58:20.440]   has a way for anybody to get, they call it sponsored data. You pay and your service will no longer
[01:58:20.440 --> 01:58:28.360]   count against data caps to ATTS mobile users. It's actually AT&T's done it three times.
[01:58:28.360 --> 01:58:32.440]   All three services were owned by AT&T.
[01:58:36.520 --> 01:58:43.800]   I continually get people saying, "See? We didn't need net neutrality. Nothing bad has happened.
[01:58:43.800 --> 01:58:47.880]   So anytime anything..." Put the word sponsored in front of anything.
[01:58:47.880 --> 01:58:55.640]   That's a bad start. Anyway, good news. If you could figure out how to get an AT-HBO Max,
[01:58:55.640 --> 01:59:01.080]   it won't go against your data caps. But the point being, in case I need to explain it,
[01:59:01.080 --> 01:59:07.400]   I don't think I do to you guys are smart. Anybody else who knows a little bit about net neutrality,
[01:59:07.400 --> 01:59:14.040]   this is AT&T putting its thumb on the scale in favor of its own services over competing services.
[01:59:14.040 --> 01:59:21.080]   That's why you need net neutrality. But HPO Max needs all the help they can get in this case.
[01:59:21.080 --> 01:59:28.120]   All right, go ahead. Has anybody here subscribed to it? Probably we all have free subscriptions,
[01:59:28.120 --> 01:59:33.240]   we just don't know. Because there's lots of ways to get it for free, but not until you get me
[01:59:33.240 --> 01:59:38.120]   your password. Oh, okay. I'll be glad to. No, I haven't paid for it yet. I haven't yet.
[01:59:38.120 --> 01:59:45.400]   If you're a Comcast subscriber and you, it's like the world's worst decision tree. So if you're
[01:59:45.400 --> 01:59:50.040]   a Comcast and you have HPO, then yes, you can if you've already got that service.
[01:59:50.040 --> 01:59:54.440]   Which I am, but I still can't figure out how to get it for free. So there.
[01:59:54.440 --> 01:59:59.320]   Then you can't get it on your Roku. Yeah, or yeah, it's not on Amazon or Roku.
[01:59:59.320 --> 02:00:05.560]   It's only on Apple TV and you're. No, in fact, they've actively does not work on Amazon for a
[02:00:05.560 --> 02:00:13.640]   reason. So that's another any competitive measure. Yeah. Or is it Amazon any competitive against HBO
[02:00:13.640 --> 02:00:20.360]   or HPO any competitive against Amazon? So there's a whole fight amongst the OTAs and the OTTs around
[02:00:20.360 --> 02:00:25.480]   who and the contract that the contracts tend to be pretty long. So like if one of the big networks
[02:00:25.480 --> 02:00:30.680]   signs on with Hulu, that's usually that's like a five year contract. And so the problem is that
[02:00:30.680 --> 02:00:34.360]   the way contracts work in the entertainment industry,
[02:00:34.360 --> 02:00:43.400]   are very similar to how regulation works. When technology comes, technology moves much faster
[02:00:43.400 --> 02:00:48.200]   than regulation or contractual agreements. And so they're kind of stuck.
[02:00:48.760 --> 02:00:55.720]   I know because when I worked at Tech TV, of course, the first thing you want to do is put your
[02:00:55.720 --> 02:01:00.200]   stuff on the internet. But in order to get on cable companies, you had to promise of whatever we
[02:01:00.200 --> 02:01:07.720]   do, we'll never put our stuff on the internet. It's a what a tattered web we weave when we first
[02:01:07.720 --> 02:01:14.600]   we practice to sign contracts. Amy Webb, thank you so much for being here. You're just great. Even
[02:01:14.600 --> 02:01:21.240]   if you are, well, you know, this is Amy Webb night mode. It's that we've blocked all the blue light
[02:01:21.240 --> 02:01:28.840]   because Amy shines so bright. It's night mode. It's night mode. A B web.io, the future today,
[02:01:28.840 --> 02:01:38.200]   Institute is at dot com. Yeah, it's dot com. The real one and I guess dot, you know, vv, xi.
[02:01:38.200 --> 02:01:43.640]   Yeah, I like that one. I registered at one point. I registered my real name is
[02:01:44.280 --> 02:01:49.960]   has a cute accent over the LAO because my parents, I don't know what they were thinking. Anyway,
[02:01:49.960 --> 02:01:55.720]   just to make it hard. So I registered L E ox on thank you. Oh, dot is.
[02:01:55.720 --> 02:02:04.440]   Leo is the problem is nothing renders it. So when you look at it, it's some bizarre Unicode.
[02:02:04.440 --> 02:02:10.520]   It's useless. I thought, Oh, this would be great because I can't get Leo because Leo is long gone.
[02:02:10.520 --> 02:02:14.840]   So I thought, Oh, this will be great. I could be Leo and everybody will be able to see it,
[02:02:14.840 --> 02:02:21.160]   except I can actually show you what it renders out as because I have it on my fast mail domains.
[02:02:21.160 --> 02:02:33.160]   It is X and dash dash L O dash B J A dot I is not so useful. That's puny code.
[02:02:33.160 --> 02:02:38.040]   That's how you render Unicode using the alphabet. Is that true?
[02:02:38.040 --> 02:02:41.240]   They call it puny code. They call it puny code.
[02:02:41.240 --> 02:02:45.560]   You and why or like something else, like like, like, like small.
[02:02:45.560 --> 02:02:50.760]   Pute. Yeah, I'm not sure if it's a wire and I, but yeah, I don't get how that renders to Leo.
[02:02:50.760 --> 02:02:53.960]   Uh, I don't really know how to translate you.
[02:02:53.960 --> 02:02:59.560]   Myself. And if I don't know if I type this into a browser, I don't think that actually goes
[02:02:59.560 --> 02:03:04.200]   anywhere. But anyway, Dylan, wait a minute, wait a minute. We've just solved a giant mystery.
[02:03:04.200 --> 02:03:12.360]   Dylan, what is Elon Musk's new child? It's puny code. It's puny code.
[02:03:12.360 --> 02:03:17.800]   Code translator really is a Leo. Yes, that's it. They named their child after me.
[02:03:17.800 --> 02:03:25.240]   There is. If you go to puny coder.com, a puny code converter. We really should enter in.
[02:03:25.240 --> 02:03:30.360]   Put in your your XN code from your domain. Oh, I'm sure I'm sure we'll do it.
[02:03:33.480 --> 02:03:37.160]   What was it again? X and dash dash l O.
[02:03:37.160 --> 02:03:41.640]   Dash. Oh, I don't remember it anyway. Who cares?
[02:03:41.640 --> 02:03:48.360]   That's Dylan Twainy. He is the VP communications for Valamale VA.
[02:03:48.360 --> 02:03:51.240]   L I know that's not him. The other guy,
[02:03:51.240 --> 02:03:55.160]   VA L I M A I L dot com, right?
[02:03:55.160 --> 02:03:59.480]   Yes, correct. What do you do? Give yourself a plug. What is Valamale?
[02:04:00.920 --> 02:04:08.360]   Valamale authenticates the identity of email senders using D mark SPF and D Kim for
[02:04:08.360 --> 02:04:14.120]   standard space stuff. And then for inbound email, we prevent unknown senders like lookalike domains
[02:04:14.120 --> 02:04:19.800]   and cousin domain from getting into corporate inboxes. Very important. Actually, I use fast mail,
[02:04:19.800 --> 02:04:29.320]   which does D Kim SPF and D mark on that's why fast mail is X and dash dash l O dash p and J
[02:04:29.320 --> 02:04:33.560]   dot I S. Maybe I should have gone to Valamale.
[02:04:33.560 --> 02:04:40.600]   Thank you for being here, Dylan. If I can plug something about the website.
[02:04:40.600 --> 02:04:45.720]   Yes. Yes. There is a domain checker there. If you ever want to know whether an email that
[02:04:45.720 --> 02:04:50.040]   you're getting could potentially be sent by somebody pretending to be that domain,
[02:04:50.040 --> 02:04:56.040]   you can enter a domain into the domain checker on our homepage at Valamale dot com.
[02:04:56.040 --> 02:05:01.560]   And it'll tell you if it's protected by D mark or not. If it's protected, it should
[02:05:01.560 --> 02:05:07.720]   be much harder, much, much harder to be spoofed. If it's not protected, then it could easily be
[02:05:07.720 --> 02:05:12.680]   somebody could easily pretend to be that domain in an email that they send you.
[02:05:12.680 --> 02:05:17.400]   Yeah, for years, I've signed my emails with PGP. Like that's going to do anything.
[02:05:17.400 --> 02:05:23.880]   So maybe I should probably be more nerdy. It makes me. Yeah, this is big long block of the
[02:05:23.880 --> 02:05:28.840]   user. And I say, if you want to validate the email came from me, just run it through GPG.
[02:05:28.840 --> 02:05:29.800]   And it should tell you.
[02:05:29.800 --> 02:05:38.360]   We need we need Valamale. Clearly, we need Valamale. Thank you, Dylan. It's great to have
[02:05:38.360 --> 02:05:47.480]   you. Wesley Faulkner and pancake. Yes, I put pancake up. I love pancake. Why
[02:05:47.480 --> 02:05:53.000]   though, out of curiosity, would you prefer that to say a kitty cat?
[02:05:53.000 --> 02:06:00.360]   Oh, we have, I mean, we have a menagerie. Oh, okay. Cats, we have dogs, we have chickens,
[02:06:00.360 --> 02:06:06.680]   we have fish, we have, oh, you don't feed the chickens to the dragon, do you? No, no, we try to
[02:06:06.680 --> 02:06:11.640]   cue them separate. Okay, yeah, we have, we have everything. We have, as you can tell from the set
[02:06:11.640 --> 02:06:18.040]   up here, we homeschool our kids. And so we try to provide a rich environment for them to learn.
[02:06:18.040 --> 02:06:21.080]   Are those monosory works behind you, that kind of thing?
[02:06:21.080 --> 02:06:27.000]   They can't look like that. Yes, there's a difference. There's a different style that we use
[02:06:27.000 --> 02:06:35.240]   for depending on the lesson, but a lot of it is experiential. And so the learning is a thing that
[02:06:35.240 --> 02:06:40.360]   is part of our life and not just restricted to a certain outside of the place.
[02:06:40.360 --> 02:06:46.680]   Really awesome. And I know as a result, your kids will be superlative achievers in the world
[02:06:46.680 --> 02:06:53.880]   and the people we need most. So thank you for doing that. But well, part of that reason that we do
[02:06:53.880 --> 02:07:03.560]   that is because of racism. And so there's, it's pervasive. And one thing I did want to say is that
[02:07:03.560 --> 02:07:10.360]   if you in some time during this pandemic tweeted or sent a post about COVID-19, but you're not
[02:07:10.360 --> 02:07:15.960]   putting something about the kind of the racial collision that's going on right now, and because
[02:07:15.960 --> 02:07:21.000]   you think it doesn't affect you, then I think you're sadly mistaken. And same goes because it's
[02:07:21.000 --> 02:07:29.080]   pride month and for LGBTQIA rights and everything that this is the time to speak up and say that
[02:07:29.080 --> 02:07:35.160]   you want to be on the right side of history. Yeah. Yeah. And we're all humans. And we got to
[02:07:35.160 --> 02:07:40.680]   start treating each other like brothers and sisters because we are. And we all came from Africa,
[02:07:40.680 --> 02:07:46.280]   right? Unless we came from Mars. Or maybe we came from Mars and then came from Africa. Like
[02:07:46.280 --> 02:07:52.120]   was on how far back you want to go. That's right. You know, it doesn't matter where we came from.
[02:07:52.120 --> 02:07:55.960]   It's where we are. We're all here. We're all here. We're all in this little blue marble.
[02:07:55.960 --> 02:07:59.640]   We all got to carry it through together. That's right. God bless you. Yeah.
[02:07:59.640 --> 02:08:05.080]   And I don't know if you saw the videos of people roaming the Vegas casinos, but I don't think
[02:08:05.080 --> 02:08:11.640]   that's that's going to really be a good plan in the long run for keeping the planet intact.
[02:08:11.640 --> 02:08:19.960]   One last thing I want to plug. Yes. This Tuesday, I am going to speaking at a meetup.
[02:08:19.960 --> 02:08:26.520]   It's in Berlin, but you can attend it virtually. I have a if you go to my Twitter account,
[02:08:26.520 --> 02:08:32.920]   Wesley 83, and it's pinned. I'm requesting that as many people sign up for that as possible.
[02:08:32.920 --> 02:08:37.800]   I'll be speaking about my experience with neuro diversity. And so there will be other speakers
[02:08:37.800 --> 02:08:45.560]   talking about that about their experience in tech and how companies can better adjust to
[02:08:45.560 --> 02:08:50.360]   their neuro divergent employee base. That's awesome. I'll be talking about my personal experience.
[02:08:50.360 --> 02:08:57.960]   So I hope people attend that. That is so cool. It's an ally meetup. Why do they spell ally with
[02:08:57.960 --> 02:09:04.360]   ones instead of else? It's similar to very familiar with Kubernetes. Yeah. And they call it K8s.
[02:09:04.360 --> 02:09:09.800]   Yeah. So the eight is the letters in between the K and the S. And that's why they call it K8s.
[02:09:09.800 --> 02:09:16.920]   K8s. So ally is accessibility. And so it's a between that before you hit the Y.
[02:09:16.920 --> 02:09:21.720]   So you're an accessibility ally. I love it. Yeah. And they'll be representatives there
[02:09:21.720 --> 02:09:28.360]   talking about living with Asperger's all kinds of neuro diversity. Are you neuro diverse or you
[02:09:28.360 --> 02:09:35.560]   have a neuro diverse child or? I have dyslexia and I also have ADHD. Well, welcome to the club,
[02:09:35.560 --> 02:09:45.800]   dude. This is the ADHD corner over here. We yeah, no kidding. And not a few dyslexics working at
[02:09:45.800 --> 02:09:53.000]   Twitch. So yeah, it's nice to nice to if you sign everybody sign up because you can attend this.
[02:09:53.000 --> 02:09:58.520]   Are you using zoom? I believe it is using zoom, but it's free.
[02:10:01.880 --> 02:10:06.440]   You want the information to be public. So on a crypto, who cares? Nice. That's right.
[02:10:06.440 --> 02:10:13.560]   This is fantastic. Ian McDonald will be there. This is great. I think this is wonderful.
[02:10:13.560 --> 02:10:21.480]   Really cool. Thank you, Wesley. I really appreciate it. And everybody is the best thing to do to go
[02:10:21.480 --> 02:10:29.160]   to meetup.com or your website probably, right? It's I have the direct meetup link right on my
[02:10:29.160 --> 02:10:34.760]   Twitter account on my Twitter account. So you can or you can do a search for the Berlin ally meetup
[02:10:34.760 --> 02:10:43.560]   and you can find it there on meetup.com Twitter.com slash Wesley 83. Is that your weight or your
[02:10:43.560 --> 02:10:54.920]   birth date? I don't disclose that. Okay. It adds up to 11, I noticed. Just saying, just saying,
[02:10:55.560 --> 02:11:00.840]   just saying, it's great to great to have you on. Great to have all three of you on. What a great
[02:11:00.840 --> 02:11:10.680]   conversation. It's really for me. These get togethers are the most important part of my week because I
[02:11:10.680 --> 02:11:16.920]   get to talk to smart people, sensitive people, intelligent people, people who care about this
[02:11:16.920 --> 02:11:21.800]   very difficult world we're living in. And so I'm very grateful to have all three of you here.
[02:11:22.600 --> 02:11:34.360]   It made me feel better. And you can find me at xbj-dash l o underscore B N on the internet.
[02:11:34.360 --> 02:11:42.360]   We do it every Sunday afternoon, 130 Pacific. I'm sorry, 230 Pacific. That's 530 Eastern. That's
[02:11:42.360 --> 02:11:48.360]   2130 UTC. You can watch us do it live if you want to put that TV slash live. There's usually stuff
[02:11:48.360 --> 02:11:53.080]   beginning and after that we don't put into the main feed. If you're doing that, join us in the
[02:11:53.080 --> 02:11:59.480]   chatroom, I R C dot twit dot TV. Those are all people watching live. We also have on-demand
[02:11:59.480 --> 02:12:07.800]   versions of everything we do at our website, twit.tv. In fact, there's audio and video there.
[02:12:07.800 --> 02:12:12.920]   It's on YouTube as well. You can ask your Amazon Echo or your Google Assistant to play it.
[02:12:12.920 --> 02:12:19.720]   But the best thing to do is find your favorite podcast. They call them podcasts, I understand.
[02:12:19.720 --> 02:12:24.200]   The kids call them podcasts, podcast application, and subscribe to this week and
[02:12:24.200 --> 02:12:28.680]   tech. And that way you'll get it automatically. The minute's available on Sunday. If you do listen
[02:12:28.680 --> 02:12:33.000]   asynchronously or watch asynchronously, we also have a forum. It's kind of like the chatroom,
[02:12:33.000 --> 02:12:38.040]   but it's not in real time. So it's easy for everybody to participate. That's at twit.community.
[02:12:38.040 --> 02:12:42.680]   I'd love to see you there. Every time a new show goes up from any of the shows on the network,
[02:12:42.680 --> 02:12:47.640]   we put a post up there so you can comment there. If you have an opinion, I'm sure a few of you will
[02:12:47.640 --> 02:12:53.800]   have some things to say about today's show. Go ahead, vent@twit.community. There it is.
[02:12:53.800 --> 02:13:00.040]   Yes, we have night mode. Thank you all for being here. Have a great, safe week. Take care of
[02:13:00.040 --> 02:13:03.400]   yourselves. Take care of each other. We'll see you next time. Another twit.
[02:13:03.400 --> 02:13:15.880]   It's easy.

