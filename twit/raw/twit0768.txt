;FFMETADATA1
title=Steer Into the Skid
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=768
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2020
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.760]   It's time for Twit We Have the Best Panel.
[00:00:04.760 --> 00:00:06.960]   Ian Thompson for the Register is here.
[00:00:06.960 --> 00:00:09.240]   Futurist Amy Webb is here.
[00:00:09.240 --> 00:00:11.960]   We're going to find out when the Mick Ribble come back.
[00:00:11.960 --> 00:00:13.760]   No, she does some other stuff too.
[00:00:13.760 --> 00:00:16.560]   We'll also talk about the Apple and Google Plan
[00:00:16.560 --> 00:00:20.240]   versus the one proposed by Peter Thiel's Palantir.
[00:00:20.240 --> 00:00:24.320]   And why are people tearing down 5G towers
[00:00:24.320 --> 00:00:25.880]   to fight coronavirus?
[00:00:25.880 --> 00:00:27.320]   It's all coming up next.
[00:00:27.320 --> 00:00:29.360]   I'm very well-dressed with it.
[00:00:29.360 --> 00:00:32.800]   This week, a tech comes to you from Twit's LastPass Studios.
[00:00:32.800 --> 00:00:36.200]   You're focused on security, but our employees, LastPass,
[00:00:36.200 --> 00:00:39.240]   can ensure they are by making access and authentication
[00:00:39.240 --> 00:00:42.520]   seamless, whether they're working in the office or remotely.
[00:00:42.520 --> 00:00:48.360]   Visit lastpass.com/twit to learn more.
[00:00:48.360 --> 00:00:50.600]   Podcasts you love.
[00:00:50.600 --> 00:00:53.120]   From people you trust.
[00:00:53.120 --> 00:00:54.520]   This is Twit.
[00:00:54.520 --> 00:01:02.520]   This is Twit, this weekend tech.
[00:01:02.520 --> 00:01:07.520]   Episode 768 recorded Sunday, April 26, 2020.
[00:01:07.520 --> 00:01:09.520]   Steer into the skid.
[00:01:09.520 --> 00:01:13.520]   This episode of This Week in Tech is brought to you by ExtraHop,
[00:01:13.520 --> 00:01:17.520]   the Cybersecurity Company, that helps modern enterprises
[00:01:17.520 --> 00:01:20.520]   protect their business and keep critical systems available
[00:01:20.520 --> 00:01:23.520]   with cloud native threat detection and the threat
[00:01:23.520 --> 00:01:25.520]   detection and response.
[00:01:25.520 --> 00:01:30.520]   Check out the full product demo and more at extrahop.com/twit.
[00:01:30.520 --> 00:01:32.520]   And by LastPass.
[00:01:32.520 --> 00:01:35.520]   LastPass manages every entry point to your business,
[00:01:35.520 --> 00:01:38.520]   so you can mitigate risk, whether you're working in the office
[00:01:38.520 --> 00:01:39.520]   or remote.
[00:01:39.520 --> 00:01:42.520]   No matter where your employees are, LastPass helps you keep peace
[00:01:42.520 --> 00:01:43.520]   of mind.
[00:01:43.520 --> 00:01:47.520]   Visit lastpass.com/twit to find out how they can help you.
[00:01:47.520 --> 00:01:49.520]   And by CashFly.
[00:01:49.520 --> 00:01:53.520]   Give your users this seamless online experience they want.
[00:01:53.520 --> 00:01:57.520]   Power your site or app with CashFly's CDN and be 30% faster
[00:01:57.520 --> 00:01:59.520]   than the competition.
[00:01:59.520 --> 00:02:02.520]   Learn more at twit.cashfly.com.
[00:02:02.520 --> 00:02:05.520]   And by Stamps.com.
[00:02:05.520 --> 00:02:08.520]   Anything you can do at the post office, you can do at Stamps.com.
[00:02:08.520 --> 00:02:11.520]   Their on-demand postage means you can skip that trip.
[00:02:11.520 --> 00:02:15.520]   For a four-week trial plus free postage and a digital scale
[00:02:15.520 --> 00:02:18.520]   without any long-term commitment, go to Stamps.com.
[00:02:18.520 --> 00:02:22.520]   Click on the microphone at the top of the homepage and enter the offer code,
[00:02:22.520 --> 00:02:23.520]   Twit.
[00:02:23.520 --> 00:02:33.520]   It's time for Twit this week in Tech.
[00:02:33.520 --> 00:02:37.520]   With two of my favorite well-dressed people today,
[00:02:37.520 --> 00:02:41.520]   Amy Webb from the Future Today Institute,
[00:02:41.520 --> 00:02:42.520]   Amy Webb.io.
[00:02:42.520 --> 00:02:44.520]   So lovely to see you, darling.
[00:02:44.520 --> 00:02:46.520]   And lovely to see you as well.
[00:02:46.520 --> 00:02:47.520]   Cheers to you, Leo.
[00:02:47.520 --> 00:02:49.520]   Cheers, cheers.
[00:02:49.520 --> 00:02:50.520]   Oh, she's drinking bubbly.
[00:02:50.520 --> 00:02:53.520]   I'm drinking a little Louis XIII's cognac.
[00:02:53.520 --> 00:02:58.520]   Also, Ian Thompson joining us from the register.co.uk.
[00:02:58.520 --> 00:03:00.520]   And what are you tippling today?
[00:03:00.520 --> 00:03:05.520]   I'm having an Aaron single malt, which is thankfully being so
[00:03:05.520 --> 00:03:08.520]   thankfully being done over here now for a change.
[00:03:08.520 --> 00:03:11.520]   So this is all the reason we are so posh.
[00:03:11.520 --> 00:03:16.520]   It's all inspired, I think, by Ian's James Bond mustache.
[00:03:16.520 --> 00:03:19.520]   That's a fun mustache.
[00:03:19.520 --> 00:03:22.520]   Certainly in Fleming here.
[00:03:22.520 --> 00:03:24.520]   That's exactly right.
[00:03:24.520 --> 00:03:29.520]   It's because of Ian's mustache that I spent two hours getting into a ball gown.
[00:03:29.520 --> 00:03:32.520]   Yeah, I spent a little time without the valet here.
[00:03:32.520 --> 00:03:35.520]   It's kind of hard to get the cufflinks in.
[00:03:35.520 --> 00:03:37.520]   Well, I've got to say you look very glam, maybe.
[00:03:37.520 --> 00:03:38.520]   Have you dued all my self?
[00:03:38.520 --> 00:03:39.520]   Thank you.
[00:03:39.520 --> 00:03:40.520]   Thank you indeed.
[00:03:40.520 --> 00:03:42.520]   But in fact, quick quiz.
[00:03:42.520 --> 00:03:45.520]   There is only one James Bond who has a mustache.
[00:03:45.520 --> 00:03:47.520]   Who is it?
[00:03:47.520 --> 00:03:48.520]   It's not Sean Connery.
[00:03:48.520 --> 00:03:49.520]   It was.
[00:03:49.520 --> 00:03:50.520]   It's not Roger Moore.
[00:03:50.520 --> 00:03:51.520]   It was the middle guy.
[00:03:51.520 --> 00:03:53.520]   The skiing movie one.
[00:03:53.520 --> 00:03:54.520]   Not Pierce Brosnan.
[00:03:54.520 --> 00:03:55.520]   Did he have a mustache?
[00:03:55.520 --> 00:03:56.520]   No, no, no, no.
[00:03:56.520 --> 00:03:57.520]   It was the guy who was only in one.
[00:03:57.520 --> 00:03:58.520]   He was only in one.
[00:03:58.520 --> 00:04:00.520]   Yeah, it's the one everyone forgets.
[00:04:00.520 --> 00:04:01.520]   The first Bond.
[00:04:01.520 --> 00:04:02.520]   Dalton?
[00:04:02.520 --> 00:04:03.520]   Was that his name?
[00:04:03.520 --> 00:04:04.520]   No.
[00:04:04.520 --> 00:04:05.520]   The first Bond.
[00:04:05.520 --> 00:04:06.520]   Yeah.
[00:04:06.520 --> 00:04:07.520]   There was David.
[00:04:07.520 --> 00:04:08.520]   David Niven?
[00:04:08.520 --> 00:04:09.520]   David Niven?
[00:04:09.520 --> 00:04:11.520]   No, you're talking about Casino Royale.
[00:04:11.520 --> 00:04:12.520]   Yeah.
[00:04:12.520 --> 00:04:14.520]   That was not the first Bond.
[00:04:14.520 --> 00:04:15.520]   That was a middle.
[00:04:15.520 --> 00:04:18.520]   That was a later Bond with Woody Allen.
[00:04:18.520 --> 00:04:23.520]   And that came after Dr. No and Goldfinger and for Rachael.
[00:04:23.520 --> 00:04:24.520]   There were more Bond's.
[00:04:24.520 --> 00:04:26.520]   There was George Lazenby, right?
[00:04:26.520 --> 00:04:27.520]   George Lazenby.
[00:04:27.520 --> 00:04:28.520]   That's it.
[00:04:28.520 --> 00:04:29.520]   Yeah.
[00:04:29.520 --> 00:04:30.520]   The big skiing.
[00:04:30.520 --> 00:04:31.520]   Yeah.
[00:04:31.520 --> 00:04:32.520]   George Lazenby.
[00:04:32.520 --> 00:04:33.520]   Yeah.
[00:04:33.520 --> 00:04:34.520]   Yeah.
[00:04:34.520 --> 00:04:36.520]   He only lasted the one series.
[00:04:36.520 --> 00:04:37.520]   Sorry, the one film.
[00:04:37.520 --> 00:04:38.520]   But it's.
[00:04:38.520 --> 00:04:41.520]   Casino Royale was a jokey Bond.
[00:04:41.520 --> 00:04:43.520]   That was the one with Peter Sellers.
[00:04:43.520 --> 00:04:44.520]   Right.
[00:04:44.520 --> 00:04:45.520]   It wasn't.
[00:04:45.520 --> 00:04:46.520]   It was 67.
[00:04:46.520 --> 00:04:47.520]   Yeah.
[00:04:47.520 --> 00:04:48.520]   Yeah.
[00:04:48.520 --> 00:04:49.520]   Yeah.
[00:04:49.520 --> 00:04:50.520]   Okay.
[00:04:50.520 --> 00:04:51.520]   You're right.
[00:04:51.520 --> 00:04:52.520]   I stand corrected.
[00:04:52.520 --> 00:04:53.520]   So David Niven is definitely a tough trivia question.
[00:04:53.520 --> 00:04:54.520]   That is.
[00:04:54.520 --> 00:04:55.520]   Oh, yeah.
[00:04:55.520 --> 00:04:56.520]   And he was not the other thing I remember.
[00:04:56.520 --> 00:04:58.520]   He wasn't the only James Bond in that movie.
[00:04:58.520 --> 00:04:59.520]   No.
[00:04:59.520 --> 00:05:00.520]   No.
[00:05:00.520 --> 00:05:03.520]   It was a very strange film.
[00:05:03.520 --> 00:05:07.520]   It couldn't decide whether it wanted to be a comedy or a drama.
[00:05:07.520 --> 00:05:10.520]   I think it wanted to be a comedy but failed.
[00:05:10.520 --> 00:05:11.520]   Yes.
[00:05:11.520 --> 00:05:16.760]   So Ian was so dapper because he's been dressing up for his daily editorial meetings on the
[00:05:16.760 --> 00:05:23.680]   register and posting these crazy pictures on Facebook and Instagram.
[00:05:23.680 --> 00:05:26.680]   And this is the mustache, I guess, is part of that.
[00:05:26.680 --> 00:05:30.120]   And then you were formal and that you said we should and I think Amy, you said we should
[00:05:30.120 --> 00:05:31.120]   all be formal.
[00:05:31.120 --> 00:05:32.120]   Yep.
[00:05:32.120 --> 00:05:37.160]   Well, as long as Ian's got a mustache, I thought we should all join him.
[00:05:37.160 --> 00:05:38.160]   What's nice is it.
[00:05:38.160 --> 00:05:39.160]   Somewhere.
[00:05:39.160 --> 00:05:41.760]   Do you happen to have a ball gown lying around?
[00:05:41.760 --> 00:05:43.560]   I know it's shocking, right?
[00:05:43.560 --> 00:05:46.240]   But I have Nike's on.
[00:05:46.240 --> 00:05:47.480]   So I'm mostly dressed up.
[00:05:47.480 --> 00:05:49.600]   Yeah, I have slippers on, but that's because.
[00:05:49.600 --> 00:05:50.920]   Yeah, I'm wearing crocs.
[00:05:50.920 --> 00:05:53.280]   I will admit.
[00:05:53.280 --> 00:05:55.920]   Don't show the footwear, whatever you do.
[00:05:55.920 --> 00:06:01.400]   Well, I mean, the whole thing kicked off because after about day four or five, my standards
[00:06:01.400 --> 00:06:03.280]   really slipped.
[00:06:03.280 --> 00:06:07.120]   I started off the video news meeting in the morning and I was wearing a sweatshirt.
[00:06:07.120 --> 00:06:09.320]   I hadn't washed or washed my hair at all.
[00:06:09.320 --> 00:06:11.240]   It wasn't even combed.
[00:06:11.240 --> 00:06:14.440]   And there was some justifiable comments that I was letting the side down.
[00:06:14.440 --> 00:06:20.880]   So then I wore this and then I started dressing up and it got it got slightly weird last week
[00:06:20.880 --> 00:06:24.040]   with a girl with a pearl earring stuff, but it was great.
[00:06:24.040 --> 00:06:26.360]   I love the girl with a pearl earring.
[00:06:26.360 --> 00:06:31.400]   I found the Photoshop slightly disturbing.
[00:06:31.400 --> 00:06:33.320]   Is that did you post it on Instagram too?
[00:06:33.320 --> 00:06:34.320]   Let me see if I can pull.
[00:06:34.320 --> 00:06:35.640]   No, just on Twitter.
[00:06:35.640 --> 00:06:37.680]   I don't have an Instagram account.
[00:06:37.680 --> 00:06:44.040]   But yeah, that was a friend of mine who was just a whiz with Photoshop or gimple, whatever
[00:06:44.040 --> 00:06:48.960]   it was he was using and changed it round and it was disturbing.
[00:06:48.960 --> 00:06:52.720]   Yeah, even my mother was slightly freaked out.
[00:06:52.720 --> 00:06:54.640]   Let me see if I can find it here.
[00:06:54.640 --> 00:06:57.320]   You need friends who can deep fake you into that.
[00:06:57.320 --> 00:07:05.600]   Oh, this is the Photoshop one, not the.
[00:07:05.600 --> 00:07:07.200]   Although yours is pretty close.
[00:07:07.200 --> 00:07:13.360]   Well, I got that and I used to have a piercing in that here, but I tried to get the earring
[00:07:13.360 --> 00:07:14.360]   through and it wasn't going to.
[00:07:14.360 --> 00:07:17.320]   Oh, no, you didn't try to repook the redo the hole, did you?
[00:07:17.320 --> 00:07:20.520]   I didn't really fancy going to the ER with blood poisoning going.
[00:07:20.520 --> 00:07:23.920]   Yeah, listen, it's a long story, but.
[00:07:23.920 --> 00:07:26.280]   Wow, wow.
[00:07:26.280 --> 00:07:30.000]   Oh, it's funny what people do.
[00:07:30.000 --> 00:07:33.280]   Oh, yes, shelter in place.
[00:07:33.280 --> 00:07:36.600]   Wearing people with a baked be knife from train spotting was also fun, but yes.
[00:07:36.600 --> 00:07:38.360]   Oh, yeah, I loved your train spotting.
[00:07:38.360 --> 00:07:39.600]   That was actually terrifying.
[00:07:39.600 --> 00:07:41.680]   Yeah, that was really scary.
[00:07:41.680 --> 00:07:45.400]   And me, I'm such an even handed chap.
[00:07:45.400 --> 00:07:47.400]   Wow.
[00:07:47.400 --> 00:07:57.600]   Yeah, unfortunately I got out knifed by a member of staff who just happened to have a Japanese
[00:07:57.600 --> 00:07:59.880]   tree cutter by just like this.
[00:07:59.880 --> 00:08:03.720]   And it was OK, right, fine, back off.
[00:08:03.720 --> 00:08:05.200]   It is a great mustache, man.
[00:08:05.200 --> 00:08:07.400]   If I could grow a mustache like that, I would.
[00:08:07.400 --> 00:08:10.080]   This is really eight weeks growth though.
[00:08:10.080 --> 00:08:12.760]   And I just, I think my lips too thin.
[00:08:12.760 --> 00:08:16.960]   It looks like I should have a top hat on and be tying young women to railway tracks going,
[00:08:16.960 --> 00:08:17.960]   "Ah, am I pretty?
[00:08:17.960 --> 00:08:18.960]   I have you now."
[00:08:18.960 --> 00:08:22.320]   Yeah, no, it's very, no, it's kind of niv-n-esque.
[00:08:22.320 --> 00:08:23.320]   I think it's good.
[00:08:23.320 --> 00:08:25.160]   I think it's really good.
[00:08:25.160 --> 00:08:28.120]   Yeah, I don't know how long.
[00:08:28.120 --> 00:08:31.400]   How is your side doing, Amy, during the shelter in place?
[00:08:31.400 --> 00:08:33.480]   Where are you these days?
[00:08:33.480 --> 00:08:37.400]   So I left, so we've got a place in New York and a place in Baltimore.
[00:08:37.400 --> 00:08:41.360]   New York, I did not want to ride this through there.
[00:08:41.360 --> 00:08:43.880]   So we're in Baltimore.
[00:08:43.880 --> 00:08:46.400]   And my team is all working remote.
[00:08:46.400 --> 00:08:53.120]   And we've actually been, everybody wants to know the future and what to do.
[00:08:53.120 --> 00:08:54.120]   Yeah.
[00:08:54.120 --> 00:08:56.920]   It's been a very, very, very busy couple of weeks.
[00:08:56.920 --> 00:08:58.720]   I've been working 18-hour days.
[00:08:58.720 --> 00:08:59.720]   Wow.
[00:08:59.720 --> 00:09:01.360]   You're kind of happy to do it.
[00:09:01.360 --> 00:09:02.360]   No.
[00:09:02.360 --> 00:09:04.040]   You're working harder because of this.
[00:09:04.040 --> 00:09:06.040]   Well, yeah.
[00:09:06.040 --> 00:09:11.640]   I mean, most, so contrary to what a lot of people understand about what futurists do,
[00:09:11.640 --> 00:09:13.680]   we don't just look out into the far future.
[00:09:13.680 --> 00:09:17.400]   We also look, I mean, the future is sort of always, right?
[00:09:17.400 --> 00:09:24.080]   So we're trying to figure out now what do global society do?
[00:09:24.080 --> 00:09:27.320]   What do the supply chains look like in three months?
[00:09:27.320 --> 00:09:34.360]   What are the next-order impacts of a federated approach to people going back to school and
[00:09:34.360 --> 00:09:38.720]   to work and what liability challenges does that trigger?
[00:09:38.720 --> 00:09:46.120]   And all the stuff that's happening with privacy and tech has massive, huge implications for
[00:09:46.120 --> 00:09:49.440]   regulation and consumer sector and all this stuff.
[00:09:49.440 --> 00:09:52.440]   So we're just building model after model after model.
[00:09:52.440 --> 00:09:55.120]   It's only really one thing I'd like to know.
[00:09:55.120 --> 00:09:59.760]   And that's when will the McRib be back?
[00:09:59.760 --> 00:10:00.760]   Is that an annual thing?
[00:10:00.760 --> 00:10:02.160]   I don't know.
[00:10:02.160 --> 00:10:03.320]   It's unpredictable.
[00:10:03.320 --> 00:10:04.360]   That's the beauty of it.
[00:10:04.360 --> 00:10:05.360]   You never, never know.
[00:10:05.360 --> 00:10:11.120]   I think the president, if anybody's going to know, or your people would know this, right?
[00:10:11.120 --> 00:10:12.680]   You're not an epidemiologist.
[00:10:12.680 --> 00:10:16.880]   So I'm not an epidemiologist, but-
[00:10:16.880 --> 00:10:18.600]   How do you, well, that's the question.
[00:10:18.600 --> 00:10:25.400]   So, one question everybody says is, well, what's the medical prognosis?
[00:10:25.400 --> 00:10:27.400]   And even the epidemiologists don't know.
[00:10:27.400 --> 00:10:32.000]   In fact, the who just put out a thing saying, everybody's counting on the golden ticket,
[00:10:32.000 --> 00:10:37.280]   the certification that you've had it and you're immune, but you shouldn't count on that,
[00:10:37.280 --> 00:10:39.880]   folks, because we don't know yet if you're immune.
[00:10:39.880 --> 00:10:43.600]   So this whole line you're having antibodies may not even work.
[00:10:43.600 --> 00:10:44.600]   Right.
[00:10:44.600 --> 00:10:48.040]   So something I guess I will just announce that I haven't told anybody yet.
[00:10:48.040 --> 00:10:49.760]   I think it's probably okay.
[00:10:49.760 --> 00:10:56.600]   My next book, so the book that I wrote that came out last year was on AI.
[00:10:56.600 --> 00:11:00.840]   The next book that I started writing before that other book came out is about the future
[00:11:00.840 --> 00:11:05.840]   of synthetic biology and a lot of it has two viruses.
[00:11:05.840 --> 00:11:12.920]   Yeah, so my publisher is like, hey, could we get that manuscript now?
[00:11:12.920 --> 00:11:18.960]   So viruses are, a virus is kind of like a USB stick.
[00:11:18.960 --> 00:11:23.840]   And like if you shove a USB stick into your computer, stuff comes out.
[00:11:23.840 --> 00:11:31.880]   Maybe good, maybe bad, maybe malware, maybe a benevolent system that can fix broken code.
[00:11:31.880 --> 00:11:34.400]   It's a way of transmitting code.
[00:11:34.400 --> 00:11:36.160]   And really that's what a virus is.
[00:11:36.160 --> 00:11:37.160]   It's just code.
[00:11:37.160 --> 00:11:40.200]   It's a way of transmitting code.
[00:11:40.200 --> 00:11:46.840]   And of course the problem is we don't know, Linux is complicated.
[00:11:46.840 --> 00:11:53.560]   Like computer systems are like easy, but in some ways super, super complicated.
[00:11:53.560 --> 00:11:59.960]   And you don't always know how if you install something new, how that might have next order
[00:11:59.960 --> 00:12:03.600]   impacts on all the other parts of your machine.
[00:12:03.600 --> 00:12:09.680]   So we don't know what the bugs look like yet with this particular virus, I think is
[00:12:09.680 --> 00:12:11.960]   the problem.
[00:12:11.960 --> 00:12:15.920]   And we're not going to know because, and this is kind of interesting.
[00:12:15.920 --> 00:12:22.240]   This is why super computers have been in the news a lot because they're trying to crunch
[00:12:22.240 --> 00:12:28.480]   numbers on what are all the different ways that this virus could impact given all of
[00:12:28.480 --> 00:12:32.640]   these different variables and if we introduce these types of medications.
[00:12:32.640 --> 00:12:37.280]   It's just like, it's a compute problem at this point more than anything else.
[00:12:37.280 --> 00:12:38.280]   So interesting.
[00:12:38.280 --> 00:12:42.920]   >> That's your particular slant on it.
[00:12:42.920 --> 00:12:44.160]   >> Yeah, yeah, yeah.
[00:12:44.160 --> 00:12:48.720]   I mean, yeah, I don't have a, my background's game theory, not biology.
[00:12:48.720 --> 00:12:50.160]   So I don't know.
[00:12:50.160 --> 00:12:56.320]   And I think part of the problem is there's a lot of the other problem that we're dealing
[00:12:56.320 --> 00:13:01.920]   with this is like infodemic where there's all this information coming at us from every
[00:13:01.920 --> 00:13:02.920]   different direction.
[00:13:02.920 --> 00:13:07.040]   It's hard to tell what's believable and what's credible.
[00:13:07.040 --> 00:13:09.040]   And it's just really challenging.
[00:13:09.040 --> 00:13:10.040]   >> Yeah.
[00:13:10.040 --> 00:13:13.080]   >> Never had to have bullshit filters up quite so high.
[00:13:13.080 --> 00:13:17.800]   I mean, it's the amount of conflicting evidence that come with first off, if you got it, you're
[00:13:17.800 --> 00:13:18.800]   immune.
[00:13:18.800 --> 00:13:20.600]   Now if you've got it, you're probably not immune.
[00:13:20.600 --> 00:13:24.560]   You might have three to five percent of people might have enough antibodies to give them limited
[00:13:24.560 --> 00:13:25.560]   resistance.
[00:13:25.560 --> 00:13:29.240]   They're now apparently spinning off Tom Hanks's blood for antibodies.
[00:13:29.240 --> 00:13:30.800]   You knew that was coming.
[00:13:30.800 --> 00:13:36.840]   >> And everybody wants to know, does that, will we also be better people after the
[00:13:36.840 --> 00:13:37.840]   injection?
[00:13:37.840 --> 00:13:38.840]   That would be nice.
[00:13:38.840 --> 00:13:40.840]   >> But in interest in typewriters.
[00:13:40.840 --> 00:13:47.000]   >> Obviously, you know, I mean, I think the thing is because this is such a new virus and
[00:13:47.000 --> 00:13:51.480]   we know so little about it, we can't say it behaves the same way as the flu or a cold
[00:13:51.480 --> 00:13:52.880]   or other coronaviruses.
[00:13:52.880 --> 00:13:59.040]   So, and I guess people like me think, well, golly, we ought to be able to find out pretty
[00:13:59.040 --> 00:14:00.520]   quickly, right?
[00:14:00.520 --> 00:14:04.040]   And so we're all a little frustrated by the time of this.
[00:14:04.040 --> 00:14:07.280]   But you don't know right away.
[00:14:07.280 --> 00:14:13.600]   So what the move said, I should clarify it, is that certainly no evidence that confers
[00:14:13.600 --> 00:14:16.960]   a lifetime immunity, but there may be some limited immunity.
[00:14:16.960 --> 00:14:22.280]   I guess they were really concerned that some governments were poised to start offering,
[00:14:22.280 --> 00:14:27.720]   get out of jail free cards if you had, you know, the antibodies and they were concerned
[00:14:27.720 --> 00:14:30.080]   that that wouldn't be a very good idea.
[00:14:30.080 --> 00:14:33.360]   >> It also kind of blows a hole in this whole herd immunity rubbish.
[00:14:33.360 --> 00:14:36.680]   You know, it's just like, oh, well, if we all just get it and we'll get it out the way
[00:14:36.680 --> 00:14:38.440]   and everything will be fine.
[00:14:38.440 --> 00:14:40.200]   That's not how it's going to work, it seems.
[00:14:40.200 --> 00:14:43.360]   You need to test, isolate and kill it, just like other countries.
[00:14:43.360 --> 00:14:46.240]   >> Do you consider what the human reaction to this will be?
[00:14:46.240 --> 00:14:50.200]   Because this is the thing that seems most unpredictable to me.
[00:14:50.200 --> 00:14:53.560]   Humans are infinitely chaotic.
[00:14:53.560 --> 00:14:57.920]   And so for instance, as I'm walking around Petaluma in the last couple of days, I noticed
[00:14:57.920 --> 00:15:01.880]   fewer people wearing masks, more people kind of hanging out at bars.
[00:15:01.880 --> 00:15:06.920]   I feel like we're, and boy, you look at the beach pictures from Southern California from
[00:15:06.920 --> 00:15:07.920]   Huntington Beach.
[00:15:07.920 --> 00:15:13.160]   I feel like we're rapidly, all of us approaching that point where we throw up our hands and
[00:15:13.160 --> 00:15:14.760]   say, I can't take it.
[00:15:14.760 --> 00:15:17.520]   I don't care what happens, I'm going out.
[00:15:17.520 --> 00:15:18.520]   >> Great.
[00:15:18.520 --> 00:15:19.960]   >> Can you can't predict that though?
[00:15:19.960 --> 00:15:20.960]   We don't know, right?
[00:15:20.960 --> 00:15:27.360]   >> No, so part of the problem is that everybody is looking for two answers.
[00:15:27.360 --> 00:15:32.840]   One is, what is the new normal, and two is when is the new normal?
[00:15:32.840 --> 00:15:35.880]   And as a futurist, I always say, well, what would it take for X to be Y?
[00:15:35.880 --> 00:15:41.040]   What would it take for us to get from now to the new normal?
[00:15:41.040 --> 00:15:42.520]   And how long would that take?
[00:15:42.520 --> 00:15:46.720]   So I don't actually think people are asking about what is the new normal.
[00:15:46.720 --> 00:15:51.760]   I think people are really saying, when is my life going to have less change coming at
[00:15:51.760 --> 00:15:55.080]   it from every direction all day long?
[00:15:55.080 --> 00:15:59.600]   And for that reason, we have to remember that people are capricious.
[00:15:59.600 --> 00:16:04.720]   So you may see people obeying rules one day and not another day.
[00:16:04.720 --> 00:16:07.400]   And what drives a break in that behavior?
[00:16:07.400 --> 00:16:10.960]   You can't, like, behavioral predictions are really, really hard.
[00:16:10.960 --> 00:16:14.600]   And you can have all the social data that you want, and you can have all the behavioral
[00:16:14.600 --> 00:16:16.800]   biometric data that you want.
[00:16:16.800 --> 00:16:21.160]   Ultimately, what causes us to continue to change our minds is pretty unknown.
[00:16:21.160 --> 00:16:29.120]   So in a place like China, where you've got a top down regime, very authoritarian, a digital
[00:16:29.120 --> 00:16:32.720]   health passport works, people sort of stay in the lines.
[00:16:32.720 --> 00:16:34.200]   They color in the lines.
[00:16:34.200 --> 00:16:37.920]   That is totally not going to work in most places around the world.
[00:16:37.920 --> 00:16:44.920]   Yeah, there's a FOMO element among Westerners anyway, where we see somebody walking around
[00:16:44.920 --> 00:16:46.760]   doesn't matter if they've got a golden ticket.
[00:16:46.760 --> 00:16:48.520]   I want to walk around too.
[00:16:48.520 --> 00:16:49.520]   Yeah.
[00:16:49.520 --> 00:16:51.920]   I mean, it's so, yeah, Ian.
[00:16:51.920 --> 00:16:56.000]   I'm sorry, I'm just saying, I've been doing some reading up over the last couple of weeks.
[00:16:56.000 --> 00:17:00.480]   There are some fascinating historical parallels with where we were with the influenza epidemic
[00:17:00.480 --> 00:17:02.200]   100 years ago.
[00:17:02.200 --> 00:17:06.320]   In 1919 in San Francisco, there was an anti-mask league.
[00:17:06.320 --> 00:17:09.080]   We haven't really advanced that much in the last century, funny.
[00:17:09.080 --> 00:17:10.080]   I read that.
[00:17:10.080 --> 00:17:11.080]   Yeah.
[00:17:11.080 --> 00:17:16.920]   And in one case, a San Francisco policeman shot and killed someone who was obviously
[00:17:16.920 --> 00:17:19.720]   sick and not wearing a mask, wasn't prosecuted.
[00:17:19.720 --> 00:17:22.200]   I don't think we're going to get to that level more, should we?
[00:17:22.200 --> 00:17:29.320]   But there are some fascinating parallels in that we're still quite dumb about these things.
[00:17:29.320 --> 00:17:34.040]   And I don't think the lockdown is going to work in itself unless we test and isolate,
[00:17:34.040 --> 00:17:36.640]   but it's certainly we're getting there.
[00:17:36.640 --> 00:17:37.640]   And I agree with Amy.
[00:17:37.640 --> 00:17:42.040]   I mean, a China situation just isn't going to cut it in the US.
[00:17:42.040 --> 00:17:44.720]   I mean, we are at least lucky.
[00:17:44.720 --> 00:17:47.600]   The three of us can work from home.
[00:17:47.600 --> 00:17:53.960]   If you work in a hospital or a grocery store or you're an Uber driver, you are out of luck
[00:17:53.960 --> 00:17:55.120]   at the moment.
[00:17:55.120 --> 00:18:00.640]   So I'm trying to keep down on the, oh, it's so terrible because at least, you know,
[00:18:00.640 --> 00:18:01.640]   we're not--
[00:18:01.640 --> 00:18:02.640]   Oh, absolutely.
[00:18:02.640 --> 00:18:07.080]   Here we are sitting in evening clothes.
[00:18:07.080 --> 00:18:10.720]   You know, that seems unseemly, but we're just trying to cheer each other up.
[00:18:10.720 --> 00:18:11.720]   Exactly.
[00:18:11.720 --> 00:18:12.720]   Yeah.
[00:18:12.720 --> 00:18:13.720]   That's right.
[00:18:13.720 --> 00:18:18.760]   I don't think any of us is insensate to the horrible suffering that is occurring, not
[00:18:18.760 --> 00:18:20.960]   just in the US, but all over the world because of this.
[00:18:20.960 --> 00:18:27.320]   I got to tell you, when we, however this whole thing happened over Twitter, I've really been,
[00:18:27.320 --> 00:18:32.560]   I haven't, I've really looked forward to this whole, you know, and I look forward to seeing
[00:18:32.560 --> 00:18:33.960]   you guys.
[00:18:33.960 --> 00:18:39.000]   And I think that's, to me, that's sort of analogous to what a lot of people are going
[00:18:39.000 --> 00:18:45.000]   through, life is slower and smaller than it was before.
[00:18:45.000 --> 00:18:49.520]   And so I think if we can keep finding moments, small moments of joy, I mean, that would be
[00:18:49.520 --> 00:18:55.440]   a nice after-effect of this is chasing incremental moments of joy.
[00:18:55.440 --> 00:19:01.880]   You know, that would probably put us all in a better place going forward.
[00:19:01.880 --> 00:19:06.880]   And we've all become misinformation fake news machines because we all have our opinions
[00:19:06.880 --> 00:19:10.320]   and we all have, we all pick and choose our stories.
[00:19:10.320 --> 00:19:13.040]   And we all are trying to figure out what's going on.
[00:19:13.040 --> 00:19:18.160]   So I'm going to leave it to the expert, Amy, and let you do all the prognostic.
[00:19:18.160 --> 00:19:21.680]   Jesus, don't you shouldn't do that.
[00:19:21.680 --> 00:19:23.240]   We're all dudes.
[00:19:23.240 --> 00:19:30.360]   But it's hard because, well Ian, you looked back to the last time this happened.
[00:19:30.360 --> 00:19:36.360]   And certainly there are some analogies, but also was a very different time in 1918.
[00:19:36.360 --> 00:19:38.600]   I mean, the world was a very different place.
[00:19:38.600 --> 00:19:43.680]   Well, I mean, can you imagine if this had hit 25 years ago before we had the internet?
[00:19:43.680 --> 00:19:44.680]   Right.
[00:19:44.680 --> 00:19:48.480]   You know, I mean, it would have been catastrophically worse.
[00:19:48.480 --> 00:19:51.040]   Because at least there are some people that can work from home.
[00:19:51.040 --> 00:19:55.360]   We've got decent communications with our friends and family through, you know, Skype and on
[00:19:55.360 --> 00:19:57.600]   social media and the rest of it.
[00:19:57.600 --> 00:20:02.320]   I don't know about YouTube, but I found myself spending way more time on social media than
[00:20:02.320 --> 00:20:06.200]   I would normally just to keep up and contact with people.
[00:20:06.200 --> 00:20:12.240]   That's why I kind of swallowed my pride and rejoined Instagram and Facebook.
[00:20:12.240 --> 00:20:13.240]   That was you.
[00:20:13.240 --> 00:20:15.480]   I wasn't sure whether to pick a nest or not.
[00:20:15.480 --> 00:20:17.480]   I should have worn my tuxedo.
[00:20:17.480 --> 00:20:19.880]   It's a spammer or what?
[00:20:19.880 --> 00:20:20.880]   No, it is me.
[00:20:20.880 --> 00:20:24.680]   Because, and I thought for a couple of reasons, one, a journalistically, I kind of am curious
[00:20:24.680 --> 00:20:27.520]   what's going on there during this.
[00:20:27.520 --> 00:20:31.840]   But also it's one way that I could stay in touch with people like you and Amy.
[00:20:31.840 --> 00:20:32.840]   Well, not Amy.
[00:20:32.840 --> 00:20:34.920]   You're not on Facebook, but with the end anyway.
[00:20:34.920 --> 00:20:36.440]   No, you're not on Facebook.
[00:20:36.440 --> 00:20:37.440]   So I don't see any of them.
[00:20:37.440 --> 00:20:40.840]   I have a public, I have a public profile, but I don't actually use Facebook.
[00:20:40.840 --> 00:20:44.360]   Some of my family and friends and even colleagues are posting on Facebook.
[00:20:44.360 --> 00:20:50.360]   One of the things that was interesting, Larry Magad posted, he was at CES, came back CES.
[00:20:50.360 --> 00:20:52.000]   This is January came back.
[00:20:52.000 --> 00:20:53.440]   Deathly ill.
[00:20:53.440 --> 00:21:00.160]   And now we're learning that antibody tests have shown that there were people at CES who
[00:21:00.160 --> 00:21:02.960]   had COVID-19.
[00:21:02.960 --> 00:21:07.360]   It's very common to come back from the consumer electronics show, ill, with the flu or with
[00:21:07.360 --> 00:21:08.360]   the cold.
[00:21:08.360 --> 00:21:10.160]   I did not.
[00:21:10.160 --> 00:21:13.040]   But Larry tagged me because we had met up.
[00:21:13.040 --> 00:21:16.920]   He tagged all the people he met that he could remember that he met at CES and asked them.
[00:21:16.920 --> 00:21:21.120]   And a great many of them suffered very similar symptoms to COVID-19.
[00:21:21.120 --> 00:21:23.040]   The worst flu I've ever had.
[00:21:23.040 --> 00:21:26.040]   I could barely move, that kind of thing.
[00:21:26.040 --> 00:21:27.600]   And so there's some question now.
[00:21:27.600 --> 00:21:29.400]   I thought we dodged a bullet.
[00:21:29.400 --> 00:21:34.440]   I thought, boy, if this had emerged a couple of months earlier out of China, CES would have
[00:21:34.440 --> 00:21:36.720]   been a disaster movie.
[00:21:36.720 --> 00:21:40.080]   But maybe it was just a slow moving disaster movie.
[00:21:40.080 --> 00:21:44.560]   I got the sickest I've ever gotten at CES a couple of years ago.
[00:21:44.560 --> 00:21:45.560]   Yeah.
[00:21:45.560 --> 00:21:48.720]   That's why I didn't take it seriously because everybody always did, even in the old days.
[00:21:48.720 --> 00:21:51.920]   No, I mean, conference call is a way of life at CES.
[00:21:51.920 --> 00:21:54.720]   But they had RSA in February in San Francisco.
[00:21:54.720 --> 00:21:56.840]   And I went along to some of the sessions on that.
[00:21:56.840 --> 00:22:00.920]   And again, I had the week after I had the worst flu of my life.
[00:22:00.920 --> 00:22:07.520]   I had to turn down a twig because I'd coughed myself so hoarse I could barely speak.
[00:22:07.520 --> 00:22:09.440]   And I haven't been that sick in a decade.
[00:22:09.440 --> 00:22:11.960]   Two people tested positive at RSA.
[00:22:11.960 --> 00:22:13.640]   So you may have gotten it.
[00:22:13.640 --> 00:22:18.560]   And I think everybody's very interested in this antibody story because a antibody test
[00:22:18.560 --> 00:22:23.840]   seemed to be coming along a little faster than tests for COVID-19 seemed to be.
[00:22:23.840 --> 00:22:25.840]   So again, but like, what does that?
[00:22:25.840 --> 00:22:26.840]   What does it mean?
[00:22:26.840 --> 00:22:27.840]   What are people right?
[00:22:27.840 --> 00:22:33.120]   So I think the challenge is, so I'm actually going for a test tomorrow for an antibody
[00:22:33.120 --> 00:22:34.120]   test.
[00:22:34.120 --> 00:22:35.120]   Oh, wow.
[00:22:35.120 --> 00:22:39.720]   I think I was probably an early community spread case in New York.
[00:22:39.720 --> 00:22:43.120]   And I never, the problem is nobody would give me a test.
[00:22:43.120 --> 00:22:49.560]   I was sick at CES years and years ago.
[00:22:49.560 --> 00:22:57.200]   And then I got horribly ill at the end of February and I couldn't, I had asthma as a
[00:22:57.200 --> 00:23:02.080]   kid and I still have it, got exercise induced asthma now.
[00:23:02.080 --> 00:23:03.080]   I couldn't breathe.
[00:23:03.080 --> 00:23:06.480]   Anyhow, so I'm going in to get a test tomorrow.
[00:23:06.480 --> 00:23:11.000]   And it's a little, like I'm kind of in a like Schrodinger situation because I'm working
[00:23:11.000 --> 00:23:15.520]   with the assumption now that I've had it and I'm feeling great.
[00:23:15.520 --> 00:23:16.520]   You know, like the-
[00:23:16.520 --> 00:23:18.520]   You have the golden ticket.
[00:23:18.520 --> 00:23:24.920]   Somebody that I know did test positive and we were in close contact and since I haven't
[00:23:24.920 --> 00:23:28.040]   been sick since I just did the math.
[00:23:28.040 --> 00:23:34.800]   And in a way, knowing that he tested positive and then me doing the math, I felt the sense
[00:23:34.800 --> 00:23:36.760]   of relief that I cannot describe.
[00:23:36.760 --> 00:23:39.560]   I've been living with that relief.
[00:23:39.560 --> 00:23:44.560]   I've still been taking all the precautions, but just a sense of calm, I'm going to go
[00:23:44.560 --> 00:23:46.760]   in for this test tomorrow.
[00:23:46.760 --> 00:23:48.080]   And then what's on the other side of it?
[00:23:48.080 --> 00:23:50.760]   Will you be horrifically disappointed?
[00:23:50.760 --> 00:23:52.840]   That's what I'm well.
[00:23:52.840 --> 00:24:02.280]   You know, the false negative rate, the false positive rate on the COVID test is low.
[00:24:02.280 --> 00:24:06.960]   The false positive rate on the antibody test is high.
[00:24:06.960 --> 00:24:07.960]   Yeah.
[00:24:07.960 --> 00:24:08.960]   Yeah.
[00:24:08.960 --> 00:24:09.960]   So I don't know what I-
[00:24:09.960 --> 00:24:10.960]   There are both as I've-
[00:24:10.960 --> 00:24:11.960]   As I've-
[00:24:11.960 --> 00:24:16.920]   I've read that there are some number of COVID tests that received that emergency
[00:24:16.920 --> 00:24:22.360]   dispensation the FDA has been offering, which means I don't- I guess I gathered that they're
[00:24:22.360 --> 00:24:24.840]   not as thoroughly tested as one would hope.
[00:24:24.840 --> 00:24:30.200]   And there have been in all of the tests some number of false positives and false negatives.
[00:24:30.200 --> 00:24:33.320]   So how reliable is the test is one question?
[00:24:33.320 --> 00:24:37.200]   What does it even mean if you have the antibodies is another question?
[00:24:37.200 --> 00:24:38.200]   And what is it-
[00:24:38.200 --> 00:24:39.200]   Right.
[00:24:39.200 --> 00:24:43.880]   In regarding, you know, I told my son who came back from Valley and seven days later got
[00:24:43.880 --> 00:24:45.320]   pretty sick.
[00:24:45.320 --> 00:24:46.320]   You probably had it.
[00:24:46.320 --> 00:24:49.920]   If you test positive for the antibodies, you can get a job.
[00:24:49.920 --> 00:24:51.400]   Congratulations.
[00:24:51.400 --> 00:24:55.560]   You can work anywhere in the world because you'll be magic, right?
[00:24:55.560 --> 00:24:57.280]   Or will he?
[00:24:57.280 --> 00:24:58.280]   That's a concern.
[00:24:58.280 --> 00:25:05.920]   I worry a lot that we are entering this strange new stratification between those who have
[00:25:05.920 --> 00:25:12.080]   tested and can prove that they've got antibodies and those that don't.
[00:25:12.080 --> 00:25:18.200]   And if this winds up protracted over the next, you know, 12 to 24 months, which it very likely
[00:25:18.200 --> 00:25:21.600]   could, I'm going to have privileges.
[00:25:21.600 --> 00:25:28.560]   I already have privileges, but I'm going to have the people who have the antibodies are
[00:25:28.560 --> 00:25:34.080]   going to be a privileged class in whatever country, wherever they happen to be living.
[00:25:34.080 --> 00:25:39.200]   And I think that that's going to cause a lot of psychological problems.
[00:25:39.200 --> 00:25:44.160]   I think that's going to cause it's going to help potentially restart our economy, but
[00:25:44.160 --> 00:25:46.640]   it's also going to cause a lot of social unrest.
[00:25:46.640 --> 00:25:48.960]   I mean, I don't know.
[00:25:48.960 --> 00:25:55.120]   I'm concerned about the, I think in the one hand, like getting the antibody test good
[00:25:55.120 --> 00:25:58.720]   and giving it to everybody and focusing a lot on that, we're as much on that as we are
[00:25:58.720 --> 00:26:00.800]   a vaccine is important.
[00:26:00.800 --> 00:26:06.120]   But I don't know what to do about this like two classes of society, those who have been
[00:26:06.120 --> 00:26:07.360]   infected and those who haven't.
[00:26:07.360 --> 00:26:12.640]   Well, that's one thing this has definitely brought to light is the vast inequalities in
[00:26:12.640 --> 00:26:15.320]   American society at least.
[00:26:15.320 --> 00:26:17.200]   And that's just going to be one more.
[00:26:17.200 --> 00:26:21.280]   And I think that's one of the reasons the who, when I say the who I think of Roger
[00:26:21.280 --> 00:26:29.120]   Saltry, the World Health Organization put out that letter because they want on Friday
[00:26:29.120 --> 00:26:33.200]   or Saturday because they really didn't want countries to go around giving people these,
[00:26:33.200 --> 00:26:35.760]   you know, gold stars that were on their forehead.
[00:26:35.760 --> 00:26:36.760]   Yeah.
[00:26:36.760 --> 00:26:39.320]   It's a really interesting time.
[00:26:39.320 --> 00:26:44.240]   And one of the reasons I wanted to have you on Amy is besides I missed you, but is because
[00:26:44.240 --> 00:26:49.080]   this is kind of the meat that you deal with all the time, but you're in a unique position
[00:26:49.080 --> 00:26:54.280]   now during a massive crisis where everybody's trying to predict the future.
[00:26:54.280 --> 00:26:57.880]   But how accurate can your crystal ball be?
[00:26:57.880 --> 00:27:03.560]   I mean, what kind of, what's the level of information you give people?
[00:27:03.560 --> 00:27:04.760]   What do you say to them?
[00:27:04.760 --> 00:27:10.120]   Is Ford, for instance, is trying to figure out, are we going to have cars to sell in the
[00:27:10.120 --> 00:27:11.120]   fall?
[00:27:11.120 --> 00:27:12.320]   Will there be customers at all?
[00:27:12.320 --> 00:27:13.420]   Should we make them?
[00:27:13.420 --> 00:27:15.320]   Should we, what should we do?
[00:27:15.320 --> 00:27:18.720]   Yes, yes, I'm familiar with those questions at Ford.
[00:27:18.720 --> 00:27:21.480]   I bet you are.
[00:27:21.480 --> 00:27:25.200]   So here's what you do.
[00:27:25.200 --> 00:27:31.200]   We have this like little diagram, a post later somewhere, but it's a cone, a futurist.
[00:27:31.200 --> 00:27:35.960]   So I, my job is not about predictions, it's about preparation.
[00:27:35.960 --> 00:27:41.400]   And that's not a hedge, that's just acknowledging the complications of multivariate equations,
[00:27:41.400 --> 00:27:42.400]   right?
[00:27:42.400 --> 00:27:44.320]   We're like, we're dealing with too many variables.
[00:27:44.320 --> 00:27:50.240]   So instead of using a timeline to mark between now and the future, we use a cone and the
[00:27:50.240 --> 00:27:53.240]   further out in time you go, the more uncertainty there is.
[00:27:53.240 --> 00:27:56.640]   Therefore we have a wider spectrum of possibilities.
[00:27:56.640 --> 00:28:01.560]   But whatever we do in the near term, we have to do thinking about downstream implications.
[00:28:01.560 --> 00:28:06.840]   And we also have to imagine a plausible future state and reverse engineer that back today.
[00:28:06.840 --> 00:28:11.880]   And the analogy that I've been using a lot is one that maybe you, the two of you don't
[00:28:11.880 --> 00:28:14.360]   have to deal with where you live, but I do.
[00:28:14.360 --> 00:28:16.920]   And that's driving on a slippery ice-covered road.
[00:28:16.920 --> 00:28:20.400]   So anybody who's been behind the wheel, then I grew up in the Midwest.
[00:28:20.400 --> 00:28:23.840]   So like part of learning how to drive is you have to prove you can drive on ice.
[00:28:23.840 --> 00:28:31.960]   If you're driving along and you hit a patch of ice, the instinct that we have is to slam
[00:28:31.960 --> 00:28:32.960]   on the brakes.
[00:28:32.960 --> 00:28:37.320]   And of course, that's the wrong thing to do.
[00:28:37.320 --> 00:28:40.480]   But I think everybody sort of understands that.
[00:28:40.480 --> 00:28:42.200]   But it's good to ask why.
[00:28:42.200 --> 00:28:48.800]   And the reason is because if we slam on the brakes, you could theoretically steer the
[00:28:48.800 --> 00:28:53.640]   car to safety if you were in charge of every single variable.
[00:28:53.640 --> 00:28:57.800]   If you knew exactly what the PSI of all four of those tires were, you knew exactly how
[00:28:57.800 --> 00:29:02.080]   tight the transmission and the gear shift and everything else was, you knew exactly what
[00:29:02.080 --> 00:29:03.560]   the incline was.
[00:29:03.560 --> 00:29:07.280]   Basically, you had to be like omniscient, but we're not.
[00:29:07.280 --> 00:29:13.280]   So instead, when you hit an icy patch, you're supposed to steer into the slide, which feels
[00:29:13.280 --> 00:29:15.720]   wrong.
[00:29:15.720 --> 00:29:19.840]   But you do that while looking at the road ahead.
[00:29:19.840 --> 00:29:24.240]   And that sort of slows down what's happening.
[00:29:24.240 --> 00:29:28.360]   To me, that situation is analogous to the one that we're in right now.
[00:29:28.360 --> 00:29:32.120]   So there's all this, like all these data coming in from everywhere, right?
[00:29:32.120 --> 00:29:35.560]   Which is kind of like being on this road where you're sliding out of control.
[00:29:35.560 --> 00:29:39.720]   And the issue is everybody wants to slam on the brakes and go back to normal, right?
[00:29:39.720 --> 00:29:41.240]   Which is where we get all of these.
[00:29:41.240 --> 00:29:42.920]   How soon does the economy reopen?
[00:29:42.920 --> 00:29:45.400]   How soon do we go back to school?
[00:29:45.400 --> 00:29:50.040]   And instead, what we have to do is to kind of like lean into uncertainty.
[00:29:50.040 --> 00:29:56.400]   We have to be totally open to everything being different at all times right now because we're
[00:29:56.400 --> 00:30:02.680]   in the middle of it while keeping an eye on the road ahead and steering into that direction.
[00:30:02.680 --> 00:30:07.000]   What I'm noticing is we're very consumed with what's happening right now because it's
[00:30:07.000 --> 00:30:09.560]   a crisis, which I totally understand.
[00:30:09.560 --> 00:30:15.480]   But I don't see enough people with one eye on the road ahead steering us into a direction.
[00:30:15.480 --> 00:30:19.400]   And I think that's the biggest problem that we're facing right now, even though I know
[00:30:19.400 --> 00:30:22.480]   the immediate problems feel much more acute.
[00:30:22.480 --> 00:30:26.960]   And that's when you look at what Google and Apple are doing with, they're not calling
[00:30:26.960 --> 00:30:30.960]   it contact tracing, they're now calling it something else with Bluetooth.
[00:30:30.960 --> 00:30:35.920]   And the fact that all of that came out of the private sector and we don't know what
[00:30:35.920 --> 00:30:42.680]   the road ahead looks like or even if having all of those data, what does that really
[00:30:42.680 --> 00:30:44.080]   do?
[00:30:44.080 --> 00:30:48.800]   But it's sensible to do it just in case we can use it, right?
[00:30:48.800 --> 00:30:50.320]   Well, I don't know.
[00:30:50.320 --> 00:30:51.560]   I mean, that's, yeah.
[00:30:51.560 --> 00:30:54.720]   All right, let's take a break because this is of course one of the big topics we've been
[00:30:54.720 --> 00:30:59.040]   discussing all week, which is the Google Apple API.
[00:30:59.040 --> 00:31:02.160]   That's going to be pushed out and eventually an app perhaps.
[00:31:02.160 --> 00:31:07.440]   And in fact, in Europe, there's been some pushback, France said, we want more data.
[00:31:07.440 --> 00:31:12.360]   Apple said, you can't use Bluetooth if you exfiltrate the data, you can't keep the data.
[00:31:12.360 --> 00:31:13.560]   France is upset about that.
[00:31:13.560 --> 00:31:16.920]   Germany at first said we're not going to do it either for the same reason.
[00:31:16.920 --> 00:31:21.440]   Now they've come along and said, okay, as of today, Germany said, yeah, we'll use this
[00:31:21.440 --> 00:31:22.440]   API.
[00:31:22.440 --> 00:31:28.320]   A lot of privacy advocates are very concerned that this could be used against us down the
[00:31:28.320 --> 00:31:29.320]   road.
[00:31:29.320 --> 00:31:33.520]   Google and Apple have released a number of follow-up white papers.
[00:31:33.520 --> 00:31:37.760]   In fact, they've even changed the algorithms a little bit to say it's, you know, the data
[00:31:37.760 --> 00:31:39.680]   will be destroyed at the end of this.
[00:31:39.680 --> 00:31:42.480]   So this is a big battle going on.
[00:31:42.480 --> 00:31:47.680]   And honestly, I think that the Google Apple solution is better than the one proposed by
[00:31:47.680 --> 00:31:53.880]   the Pan-European Commission, which is much more data intrusive, ironically, given that
[00:31:53.880 --> 00:31:56.160]   GDPR came out of the EU.
[00:31:56.160 --> 00:31:57.920]   So there's a lot to talk about here.
[00:31:57.920 --> 00:32:00.040]   And I think we have two very good people to do it.
[00:32:00.040 --> 00:32:03.040]   Ian Thompson from the register.co.uk.
[00:32:03.040 --> 00:32:04.840]   Thank you for being here today.
[00:32:04.840 --> 00:32:06.080]   We really appreciate it.
[00:32:06.080 --> 00:32:07.400]   It was a pleasure.
[00:32:07.400 --> 00:32:09.880]   Mr. Bond.
[00:32:09.880 --> 00:32:16.240]   And there's actually no bond woman name that I could give you without insulting you.
[00:32:16.240 --> 00:32:23.000]   So I'm just going to stop with the bond analogy, the wonderful Amy Webb looking fantastic in
[00:32:23.000 --> 00:32:24.800]   black velvet.
[00:32:24.800 --> 00:32:25.960]   I would take you.
[00:32:25.960 --> 00:32:27.160]   You could call me Q.
[00:32:27.160 --> 00:32:29.400]   Q. Q is a woman in the later movies.
[00:32:29.400 --> 00:32:30.400]   Yeah.
[00:32:30.400 --> 00:32:31.400]   Yeah, I like Q.
[00:32:31.400 --> 00:32:32.400]   Q over him.
[00:32:32.400 --> 00:32:36.840]   Lisa last week said, let's watch a Sean Connery movie, any Sean Connery movie.
[00:32:36.840 --> 00:32:40.560]   And I popped up, never say never again, which was the kind of the weird.
[00:32:40.560 --> 00:32:43.440]   Oh, no, no, no, no, no, no, it's the worst.
[00:32:43.440 --> 00:32:45.320]   Well, you know why it's the worst in a way?
[00:32:45.320 --> 00:32:47.040]   It's the best because he's an older gentleman.
[00:32:47.040 --> 00:32:49.000]   I liked it.
[00:32:49.000 --> 00:32:50.880]   I mean, they're all terrible.
[00:32:50.880 --> 00:32:51.880]   Oh, come on.
[00:32:51.880 --> 00:32:53.080]   Goldfinger is a classic.
[00:32:53.080 --> 00:32:54.160]   Goldfinger might be the best.
[00:32:54.160 --> 00:32:55.480]   I was a doctor, no fan.
[00:32:55.480 --> 00:33:00.560]   But anyway, well, that's for we could talk about that too.
[00:33:00.560 --> 00:33:01.760]   But let's take a little break.
[00:33:01.760 --> 00:33:06.040]   I showed today brought to you by extra hop.
[00:33:06.040 --> 00:33:10.440]   They originally started out as a way to monitor your network for performance.
[00:33:10.440 --> 00:33:15.280]   It turns out when monitoring for performance, and this is not just your local network, but
[00:33:15.280 --> 00:33:20.280]   the cloud everywhere all the way to your end user, you also can learn a lot about threats.
[00:33:20.280 --> 00:33:23.960]   But more and more companies going fully remote.
[00:33:23.960 --> 00:33:28.080]   The bad guys are out there ready to take advantage of this.
[00:33:28.080 --> 00:33:34.000]   So it's more important than ever for organizations to really see everything active in their enterprise.
[00:33:34.000 --> 00:33:37.760]   From the cloud all the way down to the IoT devices employees are using to do their jobs
[00:33:37.760 --> 00:33:38.760]   from home.
[00:33:38.760 --> 00:33:46.040]   Remember the NASA employee that brought a Raspberry Pi into the NASA network and caused
[00:33:46.040 --> 00:33:49.960]   a data breach because he said, well, I just want to bring my Raspberry Pi in.
[00:33:49.960 --> 00:33:52.760]   These are the kinds of things you need to know.
[00:33:52.760 --> 00:33:55.560]   And since many of your employees are working at home, you need to know about everything
[00:33:55.560 --> 00:33:57.360]   in their environment too.
[00:33:57.360 --> 00:34:00.680]   In order to protect and scale your business, you need more than just visibility.
[00:34:00.680 --> 00:34:07.200]   You need fast, reliable, actionable information about threats and remote access problems you
[00:34:07.200 --> 00:34:10.840]   need in context so that you can do something about it.
[00:34:10.840 --> 00:34:12.360]   That's what extra hop does.
[00:34:12.360 --> 00:34:19.320]   It helps you detect threats and performance issues 95% faster and respond 60% more efficiently.
[00:34:19.320 --> 00:34:23.400]   You'll get complete visibility on every asset in real time and at scale.
[00:34:23.400 --> 00:34:27.000]   In fact, I want to invite you to go to extrahop.com/twit.
[00:34:27.000 --> 00:34:31.480]   They've got a demo dashboard so you can see what that data in real time looks like and
[00:34:31.480 --> 00:34:35.360]   how you're going to be able to see it and act on it.
[00:34:35.360 --> 00:34:36.840]   You can secure your enterprise.
[00:34:36.840 --> 00:34:39.040]   Everybody's enterprise is now hybrid.
[00:34:39.040 --> 00:34:43.720]   Wherever it is today, wherever it goes tomorrow, automatically detect new rogue and unmanaged
[00:34:43.720 --> 00:34:45.640]   IoT devices.
[00:34:45.640 --> 00:34:49.520]   See everything in your environment from the cloud to the data center all the way to the
[00:34:49.520 --> 00:34:50.520]   customer.
[00:34:50.520 --> 00:34:54.080]   And there are a lot of people, a lot of people using extra hop now.
[00:34:54.080 --> 00:34:55.840]   In fact, they're getting some real love.
[00:34:55.840 --> 00:34:57.760]   I'll give you an example, Wizards of the Coast.
[00:34:57.760 --> 00:35:00.880]   They're a chief architect, Dan McDaniel.
[00:35:00.880 --> 00:35:04.560]   They're using it for their AWS cloud.
[00:35:04.560 --> 00:35:06.200]   McDaniel said there is no other company.
[00:35:06.200 --> 00:35:11.040]   This is a direct quote that aligns to supporting the DevOps model, the speed and the lack of
[00:35:11.040 --> 00:35:12.800]   friction than extra hop.
[00:35:12.800 --> 00:35:18.720]   He loves it, the Home Depot is using extra hop to manage and secure 2,300 remote sites
[00:35:18.720 --> 00:35:22.760]   because they say there is no data set more complete and accurate than the data set we
[00:35:22.760 --> 00:35:26.040]   get from extra hop.
[00:35:26.040 --> 00:35:31.200]   These are real users getting real benefit in a very challenging environment.
[00:35:31.200 --> 00:35:35.080]   Now is the time to take control of your cloud security.
[00:35:35.080 --> 00:35:39.000]   Extra hop, EXTRA, HOP.
[00:35:39.000 --> 00:35:40.960]   They've got a lot of resources at the website.
[00:35:40.960 --> 00:35:43.880]   You can get tips on securing and supporting remote access.
[00:35:43.880 --> 00:35:48.320]   And as I said, that incredible full product demo, which is just fun to watch.
[00:35:48.320 --> 00:35:49.880]   It's really cool.
[00:35:49.880 --> 00:35:51.360]   Extrahop.com/twit.
[00:35:51.360 --> 00:35:54.520]   Take a look.
[00:35:54.520 --> 00:35:58.320]   I think you'll find this is exactly what you've been looking for.
[00:35:58.320 --> 00:35:59.320]   Extrahop.com/twit.
[00:35:59.320 --> 00:36:03.320]   We thank you for their support.
[00:36:03.320 --> 00:36:08.440]   In this difficult time, all of our sponsors are extra special to us right now.
[00:36:08.440 --> 00:36:10.720]   And we thank you for supporting us by going there.
[00:36:10.720 --> 00:36:16.320]   Extrahop.com/twit.
[00:36:16.320 --> 00:36:24.200]   So the Google Apple proposal is, I think in many ways superior to and more trustworthy
[00:36:24.200 --> 00:36:26.200]   than something a government would propose.
[00:36:26.200 --> 00:36:29.160]   And this is everything flipped on its head these days.
[00:36:29.160 --> 00:36:33.640]   But it's pretty clear governments, or at least it's certainly a reasonable fear that
[00:36:33.640 --> 00:36:39.160]   governments might use this kind of data gathering just as they use the Patriot Act long after
[00:36:39.160 --> 00:36:45.360]   9/11 to invade our personal privacy and security.
[00:36:45.360 --> 00:36:48.640]   You know, it's a great opportunity.
[00:36:48.640 --> 00:36:53.880]   And I think with companies like Google and Apple, whom I think you can reasonably say
[00:36:53.880 --> 00:36:59.440]   not only have great technical expertise, but also have a great interest in security
[00:36:59.440 --> 00:37:00.440]   and privacy.
[00:37:00.440 --> 00:37:03.800]   And certainly Apple does, you might debate about Google's interest in privacy.
[00:37:03.800 --> 00:37:05.400]   But I think they do.
[00:37:05.400 --> 00:37:08.240]   And it's tip-top security experts.
[00:37:08.240 --> 00:37:13.320]   I trust something from Google and Apple more than I would trust anything offered by France
[00:37:13.320 --> 00:37:15.400]   or Germany or the US government.
[00:37:15.400 --> 00:37:17.840]   Ian, you disagree?
[00:37:17.840 --> 00:37:21.400]   Actually kind of agree in a disagree.
[00:37:21.400 --> 00:37:26.360]   Apple and Google, I actually trust to be reasonably safe with their data because that is, with
[00:37:26.360 --> 00:37:28.120]   Apple's case, that's their selling point.
[00:37:28.120 --> 00:37:31.200]   Google's place, that's kind of a commitment they've made.
[00:37:31.200 --> 00:37:34.680]   The one company named that we're missing out of this is Palantir.
[00:37:34.680 --> 00:37:40.560]   Palantir has been given contracts by the US government and by EU states as well to do
[00:37:40.560 --> 00:37:41.880]   roughly the same sort of thing.
[00:37:41.880 --> 00:37:43.360]   My point exactly.
[00:37:43.360 --> 00:37:47.920]   I would much rather have Apple and Google do it than Palantir do it.
[00:37:47.920 --> 00:37:48.920]   Well, exactly.
[00:37:48.920 --> 00:37:54.200]   Palantir have got all the morality and self-restraint of a Labrador left with an unguarded roast chicken.
[00:37:54.200 --> 00:37:58.720]   I mean, they will take all that stuff and pile it into whatever they can, keep it, copy
[00:37:58.720 --> 00:37:59.720]   it.
[00:37:59.720 --> 00:38:01.160]   I don't trust them further than I could throw him.
[00:38:01.160 --> 00:38:02.760]   And suddenly, where is Peter Teal, by the way?
[00:38:02.760 --> 00:38:04.320]   He's been very quiet this last month.
[00:38:04.320 --> 00:38:07.160]   I'm sure he's seen the missile silo or somewhere.
[00:38:07.160 --> 00:38:11.880]   Yeah, a lot of, you know, it's funny, New Zealand has done a very good job thanks to their stellar
[00:38:11.880 --> 00:38:13.960]   PM.
[00:38:13.960 --> 00:38:18.920]   And even before this, last year we were talking about the fact, I think I was told by one
[00:38:18.920 --> 00:38:27.000]   of our wealthy community members that his peers had already all purchased places in New
[00:38:27.000 --> 00:38:29.880]   Zealand for Heidi Holes.
[00:38:29.880 --> 00:38:36.280]   I think they were more worried about world unrest and nuclear demolish demolition and
[00:38:36.280 --> 00:38:37.520]   climate change.
[00:38:37.520 --> 00:38:41.360]   But now maybe it turns out not a bad place to be if you have a virus going after you
[00:38:41.360 --> 00:38:42.360]   too.
[00:38:42.360 --> 00:38:45.640]   Well, did you hear the story about the VC who actually purchased one of these places in
[00:38:45.640 --> 00:38:46.640]   New Zealand?
[00:38:46.640 --> 00:38:47.640]   No.
[00:38:47.640 --> 00:38:52.920]   And just before the ban got to the bunker and couldn't remember what the entry code was
[00:38:52.920 --> 00:38:55.360]   and couldn't get tech support to let him in.
[00:38:55.360 --> 00:38:57.560]   He was hanging around outside.
[00:38:57.560 --> 00:39:02.760]   I mean, it's the, I mean, I've got a New Zealand tech, tech journal friend who's just saying
[00:39:02.760 --> 00:39:07.120]   awful lot of private flights came into New Zealand just before the ban came in.
[00:39:07.120 --> 00:39:10.880]   And, you know, there's the little island down on the South Island where all the millionaires
[00:39:10.880 --> 00:39:11.960]   hang out.
[00:39:11.960 --> 00:39:14.680]   And all of a sudden there's a lot of American accents being heard.
[00:39:14.680 --> 00:39:16.680]   So, but I mean, coming back to you.
[00:39:16.680 --> 00:39:18.680]   The billionaires are on boats.
[00:39:18.680 --> 00:39:19.680]   SEGA.
[00:39:19.680 --> 00:39:20.680]   Oh, yeah.
[00:39:20.680 --> 00:39:21.840]   The billionaires are in New Zealand.
[00:39:21.840 --> 00:39:23.840]   The billionaires are on their bunker.
[00:39:23.840 --> 00:39:24.840]   They're on their yachts.
[00:39:24.840 --> 00:39:25.840]   Yeah, barely.
[00:39:25.840 --> 00:39:30.520]   If you don't have a yacht, you can rent one and just ride around in style because nobody
[00:39:30.520 --> 00:39:31.520]   can get to you.
[00:39:31.520 --> 00:39:35.640]   Did you see David Gefen's enormously tone deaf post?
[00:39:35.640 --> 00:39:37.960]   I mean, it was just buttock-clenchingly bad.
[00:39:37.960 --> 00:39:40.520]   It was just like, yeah, I'm writing out isolation in my yacht.
[00:39:40.520 --> 00:39:42.800]   And it's like a 70, 80 foot yacht.
[00:39:42.800 --> 00:39:43.800]   You're like, really?
[00:39:43.800 --> 00:39:45.080]   You're looking for sympathy?
[00:39:45.080 --> 00:39:46.640]   It's like Ellen saying, oh, I mean, just like, you know, I mean, just like, you know,
[00:39:46.640 --> 00:39:48.400]   I'm a jail in my mansion.
[00:39:48.400 --> 00:39:49.600]   Life is so tough, you know.
[00:39:49.600 --> 00:39:51.600]   It's a nice yacht, I admit.
[00:39:51.600 --> 00:39:52.600]   But...
[00:39:52.600 --> 00:39:53.600]   Oh, God.
[00:39:53.600 --> 00:39:54.600]   Yeah.
[00:39:54.600 --> 00:39:57.520]   It's a little tone deaf to complain about that.
[00:39:57.520 --> 00:39:59.560]   There's a lot of that.
[00:39:59.560 --> 00:40:05.000]   There's been some backlash against celebrities too, who are, you know, whiny.
[00:40:05.000 --> 00:40:11.400]   Sam, what's his name, sitting in his mansion pretending to be sad in his, you know, 20,000
[00:40:11.400 --> 00:40:14.160]   square foot mansion?
[00:40:14.160 --> 00:40:18.960]   Yeah, I mean, but coming back to what you were saying about sort of the data handling,
[00:40:18.960 --> 00:40:23.600]   and on one level, having a COVID tracking app like that could be incredibly useful.
[00:40:23.600 --> 00:40:26.920]   But in and of itself, it's not going to do that much.
[00:40:26.920 --> 00:40:32.280]   You've got to, if you tie it down with isolation and with dealing with infection in, you know,
[00:40:32.280 --> 00:40:34.760]   in isolated units, then it could be useful.
[00:40:34.760 --> 00:40:38.560]   But just having a tracking app on your phone the whole time that tells you you may or may
[00:40:38.560 --> 00:40:39.560]   not...
[00:40:39.560 --> 00:40:43.240]   It comes back to the false positive thing that you were talking about, Amy.
[00:40:43.240 --> 00:40:47.040]   I mean, the amount of potential false positives and panic this could cause.
[00:40:47.040 --> 00:40:51.000]   I'm not sure if it's worth, you know, because we can't enforce it.
[00:40:51.000 --> 00:40:53.440]   I'm not sure if it's actually worth going ahead.
[00:40:53.440 --> 00:40:54.440]   I agree.
[00:40:54.440 --> 00:40:55.440]   And so this is the...
[00:40:55.440 --> 00:40:56.440]   So here's my...
[00:40:56.440 --> 00:40:57.440]   What I'm seeing happening.
[00:40:57.440 --> 00:41:00.360]   And I think Leo made an excellent point, which is...
[00:41:00.360 --> 00:41:07.160]   I mean, really, we're faced with a fundamental question of our time, which is, who do we trust?
[00:41:07.160 --> 00:41:11.760]   Do we trust our government structures, which have given us a lot of reason to mistrust
[00:41:11.760 --> 00:41:12.760]   them?
[00:41:12.760 --> 00:41:17.040]   Or do we trust our technology companies, which we...
[00:41:17.040 --> 00:41:23.400]   Which Joan always offered the level of transparency and traceability and explainability that we
[00:41:23.400 --> 00:41:24.400]   may ask of them.
[00:41:24.400 --> 00:41:31.000]   Now, there's no question that Apple's differential privacy work is above and beyond everybody
[00:41:31.000 --> 00:41:31.920]   else's.
[00:41:31.920 --> 00:41:36.400]   You know, they made some, I think, early...
[00:41:36.400 --> 00:41:41.280]   There was a little bit of an early mistake, at least on the Apple/Google partnership.
[00:41:41.280 --> 00:41:45.800]   With how they were doing encryption, which I think has kind of been short up.
[00:41:45.800 --> 00:41:48.360]   But the real question here is, what is the point?
[00:41:48.360 --> 00:41:53.720]   So in China, you know, they have...
[00:41:53.720 --> 00:41:57.440]   You can move about, but you are scanned.
[00:41:57.440 --> 00:42:02.600]   There is a code, and you have to show that code, which theoretically could be spoofed,
[00:42:02.600 --> 00:42:04.880]   maybe, to move around.
[00:42:04.880 --> 00:42:08.080]   This is an opt-in system in the United States.
[00:42:08.080 --> 00:42:14.800]   We don't know exactly what happens with the metadata from Bluetooth transmissions.
[00:42:14.800 --> 00:42:18.720]   We don't exactly know where the oversight is going to lie.
[00:42:18.720 --> 00:42:23.360]   I mean, if it's something that's being built by Palantir for government, there's a government
[00:42:23.360 --> 00:42:30.000]   structure in the GAO to do accountability and auditing after the fact.
[00:42:30.000 --> 00:42:31.840]   But in the private sector, we don't have that.
[00:42:31.840 --> 00:42:33.360]   We just have the market.
[00:42:33.360 --> 00:42:37.440]   And so in a free market economy, the market's going to tell us whether or not this was
[00:42:37.440 --> 00:42:39.520]   a success or a failure.
[00:42:39.520 --> 00:42:46.240]   But in the middle of all this, who owns the database of people who have tested positive?
[00:42:46.240 --> 00:42:49.280]   And that really begs the question, who owns our genetic data?
[00:42:49.280 --> 00:42:50.280]   Is it us?
[00:42:50.280 --> 00:42:55.280]   I mean, we don't really have legal answers to those questions in the US or the EU at
[00:42:55.280 --> 00:42:56.440]   the moment.
[00:42:56.440 --> 00:43:00.800]   And so to Ian's excellent point, then what does all of it matter?
[00:43:00.800 --> 00:43:07.400]   So like, fine, you get a beep and somebody tells you that you've maybe come into contact.
[00:43:07.400 --> 00:43:08.400]   Now what?
[00:43:08.400 --> 00:43:09.760]   There's no vaccine.
[00:43:09.760 --> 00:43:13.000]   You may or may not be able to get an antibody test.
[00:43:13.000 --> 00:43:17.360]   You may have found out that information because it's opt in at any point.
[00:43:17.360 --> 00:43:21.720]   So you don't know if the person that you've come into contact with tested positive two
[00:43:21.720 --> 00:43:25.520]   months ago, and you maybe just saw them yesterday.
[00:43:25.520 --> 00:43:29.560]   There's a temporal element here that's super, super important.
[00:43:29.560 --> 00:43:33.720]   Otherwise, the data are just like sometimes more data is not good.
[00:43:33.720 --> 00:43:35.040]   It's just more.
[00:43:35.040 --> 00:43:39.480]   And it could seed misunderstanding and confusion and fear.
[00:43:39.480 --> 00:43:43.320]   And at the moment, I think what we could use is a shot of confidence.
[00:43:43.320 --> 00:43:47.560]   And maybe not the bleach, but definitely a shot of confidence.
[00:43:47.560 --> 00:43:52.120]   One thing that we've got to keep in mind is that, and again, I'm not an epidemiologist.
[00:43:52.120 --> 00:43:56.360]   I don't even have to say that out loud, but I'll say it anyway, that we don't have to
[00:43:56.360 --> 00:43:57.360]   be perfect.
[00:43:57.360 --> 00:44:01.920]   We just have to reduce the infection rate, the R0 of the virus.
[00:44:01.920 --> 00:44:07.720]   If we can get it below one, then it will slow down and die out.
[00:44:07.720 --> 00:44:11.560]   And so any solution doesn't have to be universally adopted.
[00:44:11.560 --> 00:44:13.200]   It doesn't have to be perfect.
[00:44:13.200 --> 00:44:18.040]   It just has to be effective enough that you reduce the communicability.
[00:44:18.040 --> 00:44:19.040]   Yeah.
[00:44:19.040 --> 00:44:23.240]   So I'm not an epidemiologist, but I am married to a doctor.
[00:44:23.240 --> 00:44:30.520]   And what I believe is happening is we just have to wait for the virus in the virus running
[00:44:30.520 --> 00:44:31.840]   its course.
[00:44:31.840 --> 00:44:33.040]   Everybody is going to get infected.
[00:44:33.040 --> 00:44:37.680]   So we just have to sort of slow this all down so that everybody can get infected who's going
[00:44:37.680 --> 00:44:38.680]   to get infected.
[00:44:38.680 --> 00:44:43.920]   I don't want to get infected because the more now maybe this is propaganda, but there's
[00:44:43.920 --> 00:44:47.280]   been a lot of stories lately about how devastating this is.
[00:44:47.280 --> 00:44:52.760]   People getting young people having strokes, damage to organs like the liver and the heart,
[00:44:52.760 --> 00:44:57.680]   besides the lungs, long term consequences, even if you survive.
[00:44:57.680 --> 00:45:00.480]   It doesn't sound like a good illness to get.
[00:45:00.480 --> 00:45:02.120]   No, it does.
[00:45:02.120 --> 00:45:05.680]   Again, I would agree.
[00:45:05.680 --> 00:45:07.320]   So I don't want to get it.
[00:45:07.320 --> 00:45:08.320]   Right.
[00:45:08.320 --> 00:45:10.000]   Does that mean that I have to stay inside?
[00:45:10.000 --> 00:45:11.000]   I mean, I don't mind.
[00:45:11.000 --> 00:45:14.320]   Frankly, I mean, I don't care what government says.
[00:45:14.320 --> 00:45:17.880]   I don't care what skip tracing contact tracing we've got.
[00:45:17.880 --> 00:45:21.880]   I'm staying indoors and wearing a mask for the foreseeable future.
[00:45:21.880 --> 00:45:26.680]   I'm not suggesting that we go the route of the anti-vaxxers and have everybody get together
[00:45:26.680 --> 00:45:29.760]   and give each other measles or smallpox or whatever.
[00:45:29.760 --> 00:45:32.840]   That is not what I'm saying.
[00:45:32.840 --> 00:45:38.160]   I think the issue is we can't effectively stop it.
[00:45:38.160 --> 00:45:41.600]   And so in the interim, they're just trying to slow it as it spreads.
[00:45:41.600 --> 00:45:47.000]   Yeah, I mean, the analogy, I heard of the old flat center of saying, yeah, go ahead.
[00:45:47.000 --> 00:45:51.520]   Well, I mean, the problem is unless you're willing to take the kind of steps that China
[00:45:51.520 --> 00:45:56.080]   has, which I don't think America is, then you're going to have to accept there is going
[00:45:56.080 --> 00:46:01.200]   to be a pool of people out there who are constantly spreading on and spreading on.
[00:46:01.200 --> 00:46:02.200]   It's kind of like a swimming pool.
[00:46:02.200 --> 00:46:06.600]   You can't have a no peeing section of the swimming pool and expect that to stay pure.
[00:46:06.600 --> 00:46:09.960]   The water is going backwards and forwards and we're all going to get it.
[00:46:09.960 --> 00:46:16.000]   So I mean, unless you're actually willing to take a medical approach, lock down, isolate,
[00:46:16.000 --> 00:46:20.640]   backtrace the people, which would at this point would just be a colossal task, which I don't
[00:46:20.640 --> 00:46:22.800]   think Americans are going to stand for.
[00:46:22.800 --> 00:46:28.040]   And honestly, I don't see an awful lot of, until we get a vaccine, then that works, then
[00:46:28.040 --> 00:46:31.680]   we're all going to be at risk for the next year or so, as far as I can see.
[00:46:31.680 --> 00:46:33.040]   Well, unless, right.
[00:46:33.040 --> 00:46:37.120]   And so I don't think we have to think about this in fatalistic terms.
[00:46:37.120 --> 00:46:41.680]   I mean, there's, there are, and this is the part that really irritates me.
[00:46:41.680 --> 00:46:48.440]   We can't we muster the full might of human ingenuity to think our way through this problem
[00:46:48.440 --> 00:46:50.120]   in terms that aren't binary.
[00:46:50.120 --> 00:46:55.880]   Even there are a future that involves vaccines and antibody tests and a bunch of other things,
[00:46:55.880 --> 00:46:56.880]   right?
[00:46:56.880 --> 00:47:00.200]   Couldn't, I mean, couldn't somebody design a better mask?
[00:47:00.200 --> 00:47:05.920]   Couldn't we figure out some other way to do meaningful information spread and data collection
[00:47:05.920 --> 00:47:08.440]   in a way that's not half asked?
[00:47:08.440 --> 00:47:12.600]   Well, it seems like that is happening though, isn't that what Google and Apple are trying
[00:47:12.600 --> 00:47:13.600]   to do?
[00:47:13.600 --> 00:47:15.240]   I mean, maybe it's not perfect, but that's the goal.
[00:47:15.240 --> 00:47:16.240]   Yeah.
[00:47:16.240 --> 00:47:17.240]   So that's, it's interesting.
[00:47:17.240 --> 00:47:19.760]   But again, I haven't seen them answer.
[00:47:19.760 --> 00:47:29.280]   I guess I've heard now that there's the data are just being transmitted and are de-identified,
[00:47:29.280 --> 00:47:31.000]   but doesn't there have to be a database some?
[00:47:31.000 --> 00:47:33.760]   I mean, are they using distributed ledger?
[00:47:33.760 --> 00:47:35.400]   Is this a blockchain project?
[00:47:35.400 --> 00:47:37.640]   Like where, how would this?
[00:47:37.640 --> 00:47:39.280]   It is distributed.
[00:47:39.280 --> 00:47:43.480]   So you keep track of the contacts you've made on your phone.
[00:47:43.480 --> 00:47:50.440]   And if you test positive, that data is sufficient to reverse to tell those people without telling
[00:47:50.440 --> 00:47:52.960]   them about you, but you're, you're maintaining it.
[00:47:52.960 --> 00:47:58.000]   There's no, as far as I could tell, there's no central database of who met, who when.
[00:47:58.000 --> 00:48:02.200]   And Apple by the way, and Google have in an addendum said, we will destroy this data
[00:48:02.200 --> 00:48:06.760]   as soon as it's over, this data is not going to be preserved because it could be reverse
[00:48:06.760 --> 00:48:09.760]   engineered at some point if you get the whole database.
[00:48:09.760 --> 00:48:14.400]   But I think the whole, I think this is quite clever, in other words, I think they've very
[00:48:14.400 --> 00:48:20.440]   cleverly thought of a way to do this minimizing the privacy.
[00:48:20.440 --> 00:48:28.920]   I worry though a little bit, it sounds like, I sounds like people are saying, look, we're
[00:48:28.920 --> 00:48:30.600]   all going to get it.
[00:48:30.600 --> 00:48:32.800]   We're just trying to slow it down.
[00:48:32.800 --> 00:48:34.440]   We're all going to get it.
[00:48:34.440 --> 00:48:38.800]   And what they're leaving out is because at some point, it's got to be back to business
[00:48:38.800 --> 00:48:39.800]   as usual.
[00:48:39.800 --> 00:48:44.640]   Does it have to be, does it have to be?
[00:48:44.640 --> 00:48:47.440]   I mean, I don't know.
[00:48:47.440 --> 00:48:52.440]   I think that one of the things that this virus has illustrated is that at least in the United
[00:48:52.440 --> 00:48:56.680]   States, we are all living very different types of lives.
[00:48:56.680 --> 00:49:01.240]   And the virus has disrupted everybody in fundamentally different ways.
[00:49:01.240 --> 00:49:04.640]   I mean, it's a shared experience, but it's also not a shared experience.
[00:49:04.640 --> 00:49:09.760]   It very much depends on income inequality, where you stand, what your job is.
[00:49:09.760 --> 00:49:13.800]   If you're a waiter, it's a lot harder.
[00:49:13.800 --> 00:49:19.360]   And I know I'm dressed as a waiter, but fortunately I don't have to make a living as one.
[00:49:19.360 --> 00:49:26.640]   No, so I think the challenge here is, I think that it's too reductive to say, we're just
[00:49:26.640 --> 00:49:29.000]   trying to get the economy moving again.
[00:49:29.000 --> 00:49:32.360]   I don't think it's just that.
[00:49:32.360 --> 00:49:38.280]   But we don't have any, listen, I'm politically independent staunchly so.
[00:49:38.280 --> 00:49:42.320]   However, I am always in favor of smart people in charge of things.
[00:49:42.320 --> 00:49:47.000]   And we have a situation where we just don't have the right people.
[00:49:47.000 --> 00:49:49.800]   We don't have decisive leadership.
[00:49:49.800 --> 00:49:55.160]   I think it's super interesting that in the United States, we're reverting back to a 300
[00:49:55.160 --> 00:50:01.040]   year old model where we've got regional authorities versus a strong national authority.
[00:50:01.040 --> 00:50:06.640]   And it's really, this is kind of a similar model to what's happening in Europe, which
[00:50:06.640 --> 00:50:18.280]   sort of neuters the promise and position of the executive branch in the United States.
[00:50:18.280 --> 00:50:21.040]   I don't know.
[00:50:21.040 --> 00:50:24.960]   I have to say, I'm shocked and horrified that you don't feel that a labradoodle breeder
[00:50:24.960 --> 00:50:28.160]   is the kind of person that we need to charge on this operation.
[00:50:28.160 --> 00:50:30.480]   I mean, he's clearly reliable.
[00:50:30.480 --> 00:50:35.200]   >> We're talking about- >> So the first time I heard that
[00:50:35.200 --> 00:50:38.480]   I was like, okay, so he's like retired.
[00:50:38.480 --> 00:50:40.480]   >> No, that was his business.
[00:50:40.480 --> 00:50:41.480]   >> He's a capable person.
[00:50:41.480 --> 00:50:42.480]   He must be retired.
[00:50:42.480 --> 00:50:44.120]   He's super into dogs.
[00:50:44.120 --> 00:50:46.480]   He's coming out of retirement.
[00:50:46.480 --> 00:50:50.480]   The person who invented the N95 is coming out of retirement.
[00:50:50.480 --> 00:50:52.480]   That must be what's happening.
[00:50:52.480 --> 00:50:53.480]   Right?
[00:50:53.480 --> 00:50:54.480]   Somebody?
[00:50:54.480 --> 00:50:55.480]   >> Yeah, no.
[00:50:55.480 --> 00:50:59.280]   >> Unfortunately, we're back to Katrina and a heck of a jaw brownie territory.
[00:50:59.280 --> 00:51:02.000]   I mean, it seems at this point.
[00:51:02.000 --> 00:51:04.960]   It just seems, and you've seen it- >> We're talking about Brian Harrison, who
[00:51:04.960 --> 00:51:12.760]   Alex Azar picked as the guy to lead the health and human-seamuses coronavirus response.
[00:51:12.760 --> 00:51:17.680]   >> Is the onion rising on use right now because this is just bizarro world.
[00:51:17.680 --> 00:51:25.000]   >> You saw the onion on a month ago wrote a story saying predicting, I guess I should
[00:51:25.000 --> 00:51:26.000]   just retire, right?
[00:51:26.000 --> 00:51:27.680]   The onion is now doing my job.
[00:51:27.680 --> 00:51:29.680]   >> We give up, yeah.
[00:51:29.680 --> 00:51:31.680]   >> I know.
[00:51:31.680 --> 00:51:33.440]   It's a bizark.
[00:51:33.440 --> 00:51:37.360]   And you just look at the faces of the doctors in these press briefings and they're just
[00:51:37.360 --> 00:51:38.360]   dying inside.
[00:51:38.360 --> 00:51:41.840]   I mean, for goodness, I get what you're saying, Amy.
[00:51:41.840 --> 00:51:45.920]   There is this breakdown now, and particularly in the last couple of weeks between state
[00:51:45.920 --> 00:51:51.920]   and federal governments, where you've had positions where states have ordered medical
[00:51:51.920 --> 00:51:54.880]   supplies, PPE masks, the rest of it.
[00:51:54.880 --> 00:51:57.920]   And when they've ordered them paid for them, they've turned up the federal government
[00:51:57.920 --> 00:52:01.800]   and seized them and rooted them, Lord knows where.
[00:52:01.800 --> 00:52:07.000]   There is some really, really weird stuff going on in American politics at the moment.
[00:52:07.000 --> 00:52:11.800]   And as a foreigner, I have to say I'm finding it slightly trying to keep up with where this
[00:52:11.800 --> 00:52:12.800]   is going.
[00:52:12.800 --> 00:52:17.800]   You've always had this, it seems, this contrast between the state, the friction between the
[00:52:17.800 --> 00:52:19.440]   states and the federal government.
[00:52:19.440 --> 00:52:22.080]   But it's really coming to the fore right now.
[00:52:22.080 --> 00:52:27.520]   Yeah, I mean, I haven't seen it in my lifetime.
[00:52:27.520 --> 00:52:30.200]   I haven't seen anything like this.
[00:52:30.200 --> 00:52:34.080]   And there's always politicking happening.
[00:52:34.080 --> 00:52:37.720]   So you've got a Republican governor and Maryland, Larry Hogan, who actually think is doing a
[00:52:37.720 --> 00:52:40.240]   pretty good job.
[00:52:40.240 --> 00:52:44.600]   You've got Cuomo and New York way, did not like before this, but now think he's doing
[00:52:44.600 --> 00:52:45.600]   him.
[00:52:45.600 --> 00:52:48.000]   >> Yeah, it's something everybody likes Mar- >> Like Andrew.
[00:52:48.000 --> 00:52:49.000]   >> Right.
[00:52:49.000 --> 00:52:54.440]   >> But, okay, so again, this is kind of analogous to sort of other themes that we all deal
[00:52:54.440 --> 00:52:58.440]   with in our work and our life.
[00:52:58.440 --> 00:53:09.600]   And that is, when we are all sharing a common problem that's acute and needs attention,
[00:53:09.600 --> 00:53:17.480]   you either need a very strong logistics network running the show or you need a strong leader
[00:53:17.480 --> 00:53:18.480]   running the show.
[00:53:18.480 --> 00:53:23.920]   The problem in the United States is that we neither have decisive top down leadership
[00:53:23.920 --> 00:53:28.960]   nor a central authority that's running like point on all of this.
[00:53:28.960 --> 00:53:34.680]   There was a sort of awful story that I read, I think in The New Yorker, it might have been
[00:53:34.680 --> 00:53:35.680]   the Times.
[00:53:35.680 --> 00:53:37.480]   No, it wasn't.
[00:53:37.480 --> 00:53:42.960]   It was an opinion piece that Vince Serf shared with me on Medium that I will try to find
[00:53:42.960 --> 00:53:50.600]   because it was great that explained the problem that we're in is not because we weren't prepared.
[00:53:50.600 --> 00:53:56.240]   It was because there were too many preparation plans and nobody knew which one to use.
[00:53:56.240 --> 00:53:58.400]   Let me see if I can find it.
[00:53:58.400 --> 00:53:59.400]   >> That's interesting.
[00:53:59.400 --> 00:54:02.320]   That's an interesting point of view.
[00:54:02.320 --> 00:54:08.040]   >> I mean, my understanding is that we actually had preparation plans in place five years ago,
[00:54:08.040 --> 00:54:14.600]   we had CDC teams in China, we had a pandemic plan and that appears to have been abandoned
[00:54:14.600 --> 00:54:15.600]   for some reason.
[00:54:15.600 --> 00:54:20.200]   I don't know enough about the ins and outs of it, but certainly the reports, I mean,
[00:54:20.200 --> 00:54:23.800]   it seems like- >> Honestly, though, this is an interesting
[00:54:23.800 --> 00:54:30.000]   conversation and everybody's having it, but I don't know how useful it is to say what
[00:54:30.000 --> 00:54:32.920]   might have been or what we could have done better or-
[00:54:32.920 --> 00:54:33.920]   >> Totally.
[00:54:33.920 --> 00:54:36.360]   No, no, we're debriefing at the beginning of a project.
[00:54:36.360 --> 00:54:37.360]   >> We're debriefing.
[00:54:37.360 --> 00:54:39.120]   >> We're debrief at the end, right?
[00:54:39.120 --> 00:54:40.800]   Now is not the time for debriefing.
[00:54:40.800 --> 00:54:44.160]   Now we've got to go, we're on a project sprint, we've got to get to the next part.
[00:54:44.160 --> 00:54:48.760]   >> It seems to me, but I may be wrong, that the instinct, the basic instinct of the average
[00:54:48.760 --> 00:54:52.520]   American is to do the right thing.
[00:54:52.520 --> 00:54:57.200]   I think that it was everybody understood the idea, the notion of flattening the curve,
[00:54:57.200 --> 00:54:59.560]   not everybody, but enough people did.
[00:54:59.560 --> 00:55:01.360]   People are trying to self-isolate.
[00:55:01.360 --> 00:55:06.040]   In fact, by the way, not just America, all over the world, it's kind of inspiring in some
[00:55:06.040 --> 00:55:11.880]   ways to see the entire world say, "Oh, I get it, we've got to stay home, I can do that,"
[00:55:11.880 --> 00:55:17.000]   and do it, even though for many it's a huge economic nightmare.
[00:55:17.000 --> 00:55:18.760]   I think that's kind of inspiring.
[00:55:18.760 --> 00:55:25.720]   Honestly, you can have leadership at the top, but it takes good follow ship to do it.
[00:55:25.720 --> 00:55:27.320]   >> Yeah, I do trust.
[00:55:27.320 --> 00:55:30.240]   >> Trust, but you know what we're trusting.
[00:55:30.240 --> 00:55:31.240]   So what are we trusting in?
[00:55:31.240 --> 00:55:33.640]   We're not trusting in our leaders.
[00:55:33.640 --> 00:55:36.680]   We're trusting in medical authorities.
[00:55:36.680 --> 00:55:40.520]   We're listening and we're acting, and I think most people are doing the right thing.
[00:55:40.520 --> 00:55:44.040]   >> Yeah, I mean, I'd say I was never so proud of my countryman.
[00:55:44.040 --> 00:55:48.360]   There was some lovely video last week of, you know, there's all this footage of the Italians
[00:55:48.360 --> 00:55:51.920]   singing to each other in their apartments, and there was something else in Germany and
[00:55:51.920 --> 00:55:54.000]   something else in Spain.
[00:55:54.000 --> 00:55:59.640]   Somebody posted this thing online and someone started to sing somewhere over the rainbow
[00:55:59.640 --> 00:56:02.960]   in the middle of a crowded apartment block in London.
[00:56:02.960 --> 00:56:06.480]   And about five people just shouted, "Shut up, we're trying to sleep."
[00:56:06.480 --> 00:56:12.000]   That's my country.
[00:56:12.000 --> 00:56:17.320]   >> I kind of think it's interesting to watch what's happening.
[00:56:17.320 --> 00:56:24.360]   But also, I don't care what Governor Newsom says in our state or President Trump says
[00:56:24.360 --> 00:56:31.440]   or even what the WHO says, I'm going to wear my mask and I'm going to stay home as long
[00:56:31.440 --> 00:56:32.440]   as I can.
[00:56:32.440 --> 00:56:35.120]   I'm fortunate because I can work that way.
[00:56:35.120 --> 00:56:41.640]   But I see no problem and we're not going to reconvene Twit.
[00:56:41.640 --> 00:56:44.280]   We found a way for everybody to work at home.
[00:56:44.280 --> 00:56:48.440]   And I think, no, admittedly, there are a lot of people who are not working.
[00:56:48.440 --> 00:56:57.560]   And I think I also see a lot of people helping out, trying to raise money and buy food and
[00:56:57.560 --> 00:57:01.440]   help people who aren't making a living during this time.
[00:57:01.440 --> 00:57:02.440]   >> Yeah.
[00:57:02.440 --> 00:57:06.120]   And I hate to give anybody any ideas.
[00:57:06.120 --> 00:57:10.880]   Nobody, all awesome people listen to this show.
[00:57:10.880 --> 00:57:16.000]   I'm sort of heartened by the fact that we haven't seen significant upticks and ransomware
[00:57:16.000 --> 00:57:20.960]   attacks on critical infrastructure and our healthcare.
[00:57:20.960 --> 00:57:22.800]   >> It was nipping tuck for a while.
[00:57:22.800 --> 00:57:25.440]   Remember the HSS attacks initially.
[00:57:25.440 --> 00:57:28.800]   But I think, yeah, maybe even the hackers thought better of it.
[00:57:28.800 --> 00:57:29.800]   I hope so.
[00:57:29.800 --> 00:57:33.840]   >> Well, I think they knew that if they did it, then the authorities were going to come
[00:57:33.840 --> 00:57:37.600]   down on a ton of bricks because it's the perfect opportunity.
[00:57:37.600 --> 00:57:39.080]   It's just fine.
[00:57:39.080 --> 00:57:40.280]   Let's identify these people.
[00:57:40.280 --> 00:57:43.560]   Let's hang them up in the public square as they deserve to be done.
[00:57:43.560 --> 00:57:47.360]   And yeah, there does seem to be a drop off in ransomware attacks against hospitals and
[00:57:47.360 --> 00:57:48.960]   healthcare facilities.
[00:57:48.960 --> 00:57:54.360]   Unfortunately, that has also led to a stepping up of attacks against financial institutions
[00:57:54.360 --> 00:57:56.080]   and some other types of business.
[00:57:56.080 --> 00:58:00.840]   But yeah, I mean, honestly, this could be the chance to get a really good community spirit
[00:58:00.840 --> 00:58:01.840]   going.
[00:58:01.840 --> 00:58:02.840]   And it's been amazing.
[00:58:02.840 --> 00:58:06.160]   In our neighborhood, people are actually talking to each other on a regular boat.
[00:58:06.160 --> 00:58:08.360]   Okay, shouting at each other from six feet away.
[00:58:08.360 --> 00:58:12.320]   But, you know, I mean, we got together on the everyone's lawns and we all had their sat
[00:58:12.320 --> 00:58:17.000]   in the glass door and we talked things through what could we do if somebody was having problems
[00:58:17.000 --> 00:58:20.480]   getting groceries, someone else agreed to go and pick it up for them.
[00:58:20.480 --> 00:58:24.000]   I think this could be the beginning of something really quite good in terms of rebuilding
[00:58:24.000 --> 00:58:28.560]   actual social networks within those that we live with and care about.
[00:58:28.560 --> 00:58:31.760]   I just was saying this to my husband today.
[00:58:31.760 --> 00:58:39.480]   It feel America in some ways feels more like Southern Europe, which is my way of saying
[00:58:39.480 --> 00:58:40.480]   slower.
[00:58:40.480 --> 00:58:47.680]   Like I'm very, very, very busy at work, but I feel less stressed out for some reason.
[00:58:47.680 --> 00:58:55.560]   I'm stressed in other ways now, but I feel like I'm having smarter, better conversations
[00:58:55.560 --> 00:58:57.520]   with people, more thoughtful conversations.
[00:58:57.520 --> 00:59:02.240]   I'm certainly asking if I can help people out more.
[00:59:02.240 --> 00:59:04.160]   I don't know.
[00:59:04.160 --> 00:59:11.040]   I generally feel better toward the people around me than either indifferent or like,
[00:59:11.040 --> 00:59:15.920]   I got to be somewhere, get out of my way, sort of, you know, feeling that you get New
[00:59:15.920 --> 00:59:16.920]   York.
[00:59:16.920 --> 00:59:25.880]   In France, the courts have ruled that Amazon may not sell nonessential items.
[00:59:25.880 --> 00:59:30.880]   They would be fined 100,000 euros for any delivery that violates those conditions.
[00:59:30.880 --> 00:59:38.200]   Amazon had appealed this French court ruling, but lost in the Versailles Court of Appeals.
[00:59:38.200 --> 00:59:40.160]   We've had this.
[00:59:40.160 --> 00:59:43.960]   Here in the United States, they're still delivering, I know, because I'm getting things
[00:59:43.960 --> 00:59:44.960]   like pretzels.
[00:59:44.960 --> 00:59:48.280]   They're still delivering nonessential items.
[00:59:48.280 --> 00:59:53.640]   We see we've had this problem in the United Kingdom where a British policeman went on
[00:59:53.640 --> 00:59:58.720]   air and said, look, we will be checking supermarket trolleys to make sure that you're getting
[00:59:58.720 --> 01:00:01.760]   essential items only, which led to a massive thing.
[01:00:01.760 --> 01:00:05.800]   So we'll hang on with British is gin and essential item.
[01:00:05.800 --> 01:00:07.600]   And you know, wars, wars, and people.
[01:00:07.600 --> 01:00:10.800]   I'm willing to stipulate anything you could buy in a grocery store is essential.
[01:00:10.800 --> 01:00:11.800]   Yeah.
[01:00:11.800 --> 01:00:13.600]   And the police did back down on that one.
[01:00:13.600 --> 01:00:16.680]   I mean, it all comes down to what you think is essential.
[01:00:16.680 --> 01:00:19.120]   Now, you know, our hair clip is essential.
[01:00:19.120 --> 01:00:20.360]   Well, technically not.
[01:00:20.360 --> 01:00:21.360]   You can live without them.
[01:00:21.360 --> 01:00:24.960]   But a lot of people are trying to buy them at the moment because, you know, we don't
[01:00:24.960 --> 01:00:25.960]   all want to go.
[01:00:25.960 --> 01:00:28.520]   Well, actually, I'm quite looking forward to going shaggy to still.
[01:00:28.520 --> 01:00:32.080]   Do you think we should have restrictions in the US and what Amazon can sell?
[01:00:32.080 --> 01:00:40.280]   Remember that Amazon is putting workers at risk by selling anything essential or nonessential.
[01:00:40.280 --> 01:00:45.440]   The more stuff they sell, inessential stuff, the more warehouse workers are put at risk,
[01:00:45.440 --> 01:00:50.880]   the more delivery people are put at risk, is that too heavy a price to pay?
[01:00:50.880 --> 01:00:55.320]   Should Amazon not be making money or trying to make more money at this time, but instead
[01:00:55.320 --> 01:00:58.360]   limited to just stuff that people absolutely need?
[01:00:58.360 --> 01:01:02.320]   Admittedly, Amazon provides a great service because it is one of the few ways people can
[01:01:02.320 --> 01:01:03.720]   get essentials.
[01:01:03.720 --> 01:01:06.440]   So here's what I've been wondering.
[01:01:06.440 --> 01:01:12.520]   Amazon has something called AWS RoboMaker, which was built, and I don't know if it's
[01:01:12.520 --> 01:01:16.040]   launched at scale yet, but the whole point of it was to help companies test and deploy
[01:01:16.040 --> 01:01:23.680]   intelligent robotics applications at scale in the cloud, which means you have a physical
[01:01:23.680 --> 01:01:28.880]   group of robots that can work collaboratively, share the data, improve it.
[01:01:28.880 --> 01:01:32.680]   And if you push a change to one, it sort of pushes to all.
[01:01:32.680 --> 01:01:35.760]   You know, and those are not intended as toys.
[01:01:35.760 --> 01:01:40.640]   I believe those are intended in places where you would have a lot of repetitive tasks and
[01:01:40.640 --> 01:01:43.240]   motions like a warehouse.
[01:01:43.240 --> 01:01:48.800]   And Amazon's also got Scout, which it had been testing outside of Seattle, which looks
[01:01:48.800 --> 01:01:53.760]   like a it's a land-based drone similar to Narrow, but smaller.
[01:01:53.760 --> 01:02:02.040]   And, you know, all I can wonder is at what point has Amazon had the capability to push
[01:02:02.040 --> 01:02:09.040]   some of these services out at scale and chosen not to to reduce the spotlight, the federal
[01:02:09.040 --> 01:02:12.440]   oversight spotlight in from different countries, right?
[01:02:12.440 --> 01:02:17.720]   But boy, it feels like if Amazon wanted to automate its workforce, now would be the
[01:02:17.720 --> 01:02:19.440]   way the time to do it.
[01:02:19.440 --> 01:02:23.400]   I've been sort of wondering if we're going to start seeing that at some point, especially
[01:02:23.400 --> 01:02:26.040]   because there's so much.
[01:02:26.040 --> 01:02:31.160]   There's suddenly over the past like month, there's been an increase in investment increase
[01:02:31.160 --> 01:02:37.120]   and work flow around collaborative robotics and drones, land-based drones, air-based
[01:02:37.120 --> 01:02:38.120]   drones.
[01:02:38.120 --> 01:02:43.360]   And there's never been a better time to do geospatial mapping because you got nobody
[01:02:43.360 --> 01:02:46.520]   outside and nobody had, right?
[01:02:46.520 --> 01:02:49.760]   But no, I think this is exactly the point.
[01:02:49.760 --> 01:02:55.840]   If Amazon is, as you say, they've been putting money into automating the warehouse jobs and
[01:02:55.840 --> 01:02:59.040]   getting rid of the fleshy component on that for quite some time.
[01:02:59.040 --> 01:03:00.840]   Perfect time to introduce it.
[01:03:00.840 --> 01:03:05.520]   But I do feel that it's slightly disingenuous amongst a lot of tech companies because one
[01:03:05.520 --> 01:03:10.440]   of the first things Amazon did, I think it was a week or 10 days ago, was cut commissions
[01:03:10.440 --> 01:03:12.480]   to third-party suppliers.
[01:03:12.480 --> 01:03:17.480]   This is a great time to lock down your supply chain, get rid of all these tiny third-party
[01:03:17.480 --> 01:03:18.480]   people.
[01:03:18.480 --> 01:03:19.480]   He don't really make you that much money.
[01:03:19.480 --> 01:03:24.440]   It's much easier to deal with just 10,000 big suppliers than a million small ones.
[01:03:24.440 --> 01:03:30.040]   Good time to get the workforce replaced by machines because you can say it's being done
[01:03:30.040 --> 01:03:31.960]   by the public health thing.
[01:03:31.960 --> 01:03:35.960]   Google and Facebook are dropping advertising rates left, right and center to try and squeeze
[01:03:35.960 --> 01:03:38.280]   out third-party platforms as well.
[01:03:38.280 --> 01:03:43.520]   There's a lot of quite nasty stuff going on in all this, but it's the way the technology
[01:03:43.520 --> 01:03:44.520]   progresses.
[01:03:44.520 --> 01:03:45.920]   Yeah, that's why we're dropping our ad rates.
[01:03:45.920 --> 01:03:49.640]   We want to squeeze out all those other podcast networks.
[01:03:49.640 --> 01:03:50.640]   Tominate.
[01:03:50.640 --> 01:03:58.440]   Google may be reducing its ad rates because they're just desperate to get ads.
[01:03:58.440 --> 01:04:01.920]   Well, they're not exactly sure of cash though either, although.
[01:04:01.920 --> 01:04:06.280]   You could ride this one out, but it does seem like there's a little bit of vulture
[01:04:06.280 --> 01:04:07.960]   capitalism going on here.
[01:04:07.960 --> 01:04:09.600]   I like vulture capitalism.
[01:04:09.600 --> 01:04:12.560]   I haven't heard that before.
[01:04:12.560 --> 01:04:15.560]   You obviously don't read the register.
[01:04:15.560 --> 01:04:18.240]   I do, but I don't remember seeing that.
[01:04:18.240 --> 01:04:20.760]   I think that's all they call it.
[01:04:20.760 --> 01:04:24.880]   This just seems like an accelerating factor, right?
[01:04:24.880 --> 01:04:34.360]   If there was ever a time to push forward something that's going to result in technological unemployment
[01:04:34.360 --> 01:04:40.680]   and do it in a way that would escape the immediate response by regulators, now would
[01:04:40.680 --> 01:04:41.680]   be the time to do it.
[01:04:41.680 --> 01:04:46.080]   On the other hand, I think every brand has to be extra cautious because you don't want
[01:04:46.080 --> 01:04:50.600]   to be perceived as taking advantage of this situation.
[01:04:50.600 --> 01:04:53.880]   That would, I think, give you an everlasting black mark.
[01:04:53.880 --> 01:05:00.320]   Right, and again, I don't think, I think given where we are and the high degree of plausibility
[01:05:00.320 --> 01:05:05.040]   that we are going nowhere over the summer and that school, a lot of schools could be
[01:05:05.040 --> 01:05:09.160]   delayed this fall, and for all good reasons.
[01:05:09.160 --> 01:05:17.440]   Again, at what point do they start slowly rolling out some of these systems and programs
[01:05:17.440 --> 01:05:19.280]   in a way that help us?
[01:05:19.280 --> 01:05:22.720]   The first rollouts will be for medicine, I'm sure.
[01:05:22.720 --> 01:05:28.480]   And certain types of groceries to people who are older and can't get out.
[01:05:28.480 --> 01:05:33.360]   And that will help make it easier to start delivering all kinds of other things without
[01:05:33.360 --> 01:05:34.360]   people.
[01:05:34.360 --> 01:05:40.640]   Yeah, I mean, I'm torn because both my parents are in their 80s, my dad's got a lung condition,
[01:05:40.640 --> 01:05:42.360]   so I don't want him going out to the shops.
[01:05:42.360 --> 01:05:45.360]   I'm really quite happy that Amazon can actually deliver this stuff.
[01:05:45.360 --> 01:05:49.200]   But I don't know how you guys are dealing with it, but when we get an Amazon package,
[01:05:49.200 --> 01:05:53.920]   it goes onto a sideboard area, which is sort of taped out, and it's left there for two
[01:05:53.920 --> 01:05:55.760]   days just to sort of-
[01:05:55.760 --> 01:05:56.760]   I can't do that.
[01:05:56.760 --> 01:05:57.760]   I try to do it.
[01:05:57.760 --> 01:05:59.800]   But it's like Christmas day, I can't.
[01:05:59.800 --> 01:06:02.360]   Give me the package now, now, now.
[01:06:02.360 --> 01:06:08.080]   Well, so that's one of those, that's one of the issues with information, right?
[01:06:08.080 --> 01:06:12.760]   We don't know if setting it aside, there haven't been consistent studies on how long
[01:06:12.760 --> 01:06:15.920]   virus stays on cardboard.
[01:06:15.920 --> 01:06:17.920]   So at any rate-
[01:06:17.920 --> 01:06:20.960]   We just wash your hands after you open it, right?
[01:06:20.960 --> 01:06:22.480]   We have to get stuff.
[01:06:22.480 --> 01:06:25.320]   My dad's on a slew of medications.
[01:06:25.320 --> 01:06:26.480]   He's afraid to go out.
[01:06:26.480 --> 01:06:28.680]   I'm afraid to go out.
[01:06:28.680 --> 01:06:35.240]   So I think that even with all of the precautions, and again, this is what a futurist does, right?
[01:06:35.240 --> 01:06:36.880]   We're not trying to predict what happens.
[01:06:36.880 --> 01:06:44.280]   We're trying to sort of figure out what signals are there, where do we have data, and what
[01:06:44.280 --> 01:06:47.760]   are the plausible implications if we start connecting some of these things?
[01:06:47.760 --> 01:06:50.120]   So we can steer into this kid.
[01:06:50.120 --> 01:06:51.360]   So we can steer into this kid.
[01:06:51.360 --> 01:07:01.360]   And listen, if we are all much more flexible and okay with ambiguity, which is hard, and
[01:07:01.360 --> 01:07:05.200]   if we're willing to lean into uncertainty, all of this becomes a lot easier.
[01:07:05.200 --> 01:07:12.760]   The challenge is that everybody wants certainty, and they don't want any answer to change once
[01:07:12.760 --> 01:07:14.040]   it's been given to them.
[01:07:14.040 --> 01:07:16.000]   But the math doesn't work out on this one.
[01:07:16.000 --> 01:07:23.440]   There's no real way to game out every plausible future state, and then calculate which one
[01:07:23.440 --> 01:07:24.920]   we're headed to all the time.
[01:07:24.920 --> 01:07:26.120]   No, you can't.
[01:07:26.120 --> 01:07:31.200]   I can, just as in my own head, and I don't have any training, I could think of 100 scenarios.
[01:07:31.200 --> 01:07:32.200]   It's just unknown.
[01:07:32.200 --> 01:07:34.440]   I feel a little bit like the captain of the Titanic.
[01:07:34.440 --> 01:07:39.640]   Let's pour out the last vestiges of this.
[01:07:39.640 --> 01:07:43.080]   We don't know, and we'll see another vintage like it.
[01:07:43.080 --> 01:07:46.520]   And take a break, cheers all.
[01:07:46.520 --> 01:07:49.520]   This is the formal version of Twit.
[01:07:49.520 --> 01:07:51.800]   We're just trying to cheer ourselves up.
[01:07:51.800 --> 01:07:56.240]   Because Ian does it on all of his Zoom calls.
[01:07:56.240 --> 01:07:58.840]   What does software does the register use for their conference call?
[01:07:58.840 --> 01:08:00.680]   Actually, we use Google Hangouts.
[01:08:00.680 --> 01:08:02.240]   Yes, it's good.
[01:08:02.240 --> 01:08:07.080]   Because we're a G Suite company, but I've got to say Hangouts has held up pretty well.
[01:08:07.080 --> 01:08:11.400]   It's only had two days where it's already dropped the ball big time.
[01:08:11.400 --> 01:08:16.000]   And I'm not that keen on Zoom, because it's a great platform, but it's been forced to
[01:08:16.000 --> 01:08:18.200]   go to scale really, really quickly.
[01:08:18.200 --> 01:08:19.200]   Yes.
[01:08:19.200 --> 01:08:23.600]   And it's such a huge amount of intense hacking has been going on to try and find out where
[01:08:23.600 --> 01:08:24.600]   the vulnerabilities are.
[01:08:24.600 --> 01:08:27.680]   And it's no worse than any of the video conferencing platform.
[01:08:27.680 --> 01:08:29.880]   It's just been under scrutiny much more.
[01:08:29.880 --> 01:08:35.800]   So I figured Google Hangouts, but there was a lovely story that I saw at the weekend.
[01:08:35.800 --> 01:08:39.560]   Google's head of business development was apparently having one of the Google video
[01:08:39.560 --> 01:08:42.720]   meetings and was asked by one of his staff.
[01:08:42.720 --> 01:08:44.200]   Oh, that's so fire.
[01:08:44.200 --> 01:08:45.200]   You saw this one.
[01:08:45.200 --> 01:08:46.200]   Yes.
[01:08:46.200 --> 01:08:47.200]   Oh, just tell us.
[01:08:47.200 --> 01:08:48.200]   Tell us.
[01:08:48.200 --> 01:08:49.200]   Oh, OK.
[01:08:49.200 --> 01:08:51.960]   It's just like he's asked by Google staffer.
[01:08:51.960 --> 01:08:54.200]   Why is it that Zoom is eating our lunch?
[01:08:54.200 --> 01:08:56.440]   You know, it's like we've been doing this for years.
[01:08:56.440 --> 01:08:57.600]   Why are there such a...
[01:08:57.600 --> 01:09:01.800]   And then this guy's child walks in and goes, "Oh, Dad, are you on Zoom?
[01:09:01.800 --> 01:09:03.400]   All my friends really like Zoom.
[01:09:03.400 --> 01:09:04.400]   It's so easy to use."
[01:09:04.400 --> 01:09:05.400]   Easy to use.
[01:09:05.400 --> 01:09:06.400]   Oh, it's so funny.
[01:09:06.400 --> 01:09:09.240]   We don't have video of it.
[01:09:09.240 --> 01:09:10.240]   I wish we did.
[01:09:10.240 --> 01:09:16.000]   We only have third party retellings, but I just wonder whether the guy just went, "Ah,
[01:09:16.000 --> 01:09:18.000]   or if he was blushing."
[01:09:18.000 --> 01:09:21.600]   I want to buy that kid a chocolate bar because that was just so good.
[01:09:21.600 --> 01:09:25.480]   It's using Zoom now.
[01:09:25.480 --> 01:09:26.480]   We've been...
[01:09:26.480 --> 01:09:28.720]   I've been trying to get people to use Jitzy, which is an open source.
[01:09:28.720 --> 01:09:34.040]   And I even said it, Jitzy's server out of the house and I like it.
[01:09:34.040 --> 01:09:35.880]   But this is Jitzy right now.
[01:09:35.880 --> 01:09:37.360]   Hello, everybody.
[01:09:37.360 --> 01:09:45.560]   But for some reason, actually anybody wants to go in, twit.teams/twit-768.
[01:09:45.560 --> 01:09:47.600]   We can see how many people we can get in here.
[01:09:47.600 --> 01:09:56.640]   But for some reason, even the less sophisticated technology users in my family can't figure
[01:09:56.640 --> 01:09:57.640]   this out.
[01:09:57.640 --> 01:09:58.640]   They know Zoom.
[01:09:58.640 --> 01:09:59.640]   They go, "Ah, well, let's just use Zoom.
[01:09:59.640 --> 01:10:00.640]   We know how to use Zoom.
[01:10:00.640 --> 01:10:01.880]   Everybody likes Zoom."
[01:10:01.880 --> 01:10:05.480]   So that's what we end up using just because everybody can figure it out.
[01:10:05.480 --> 01:10:09.880]   I know a lot of people who cannot figure it out.
[01:10:09.880 --> 01:10:10.880]   Oh, good.
[01:10:10.880 --> 01:10:11.880]   All right.
[01:10:11.880 --> 01:10:15.840]   What's the easiest to use?
[01:10:15.840 --> 01:10:18.960]   I think for most people, it depends on how many people you're talking to.
[01:10:18.960 --> 01:10:22.600]   FaceTime seems to be the easiest thing for most people to use.
[01:10:22.600 --> 01:10:25.560]   But if you need a group, we use Hangouts at work.
[01:10:25.560 --> 01:10:28.120]   We use Zoom for some things.
[01:10:28.120 --> 01:10:29.680]   I mean, we sort of switch around.
[01:10:29.680 --> 01:10:32.680]   Blue jeans, we've used Microsoft Teams.
[01:10:32.680 --> 01:10:33.680]   You know.
[01:10:33.680 --> 01:10:34.680]   Really?
[01:10:34.680 --> 01:10:35.680]   Wow.
[01:10:35.680 --> 01:10:36.680]   Okay.
[01:10:36.680 --> 01:10:37.680]   There's someone there then.
[01:10:37.680 --> 01:10:38.680]   I don't know.
[01:10:38.680 --> 01:10:41.800]   We use some of our clients who are Microsoft shops.
[01:10:41.800 --> 01:10:43.520]   You know, we use it for them.
[01:10:43.520 --> 01:10:44.520]   Right.
[01:10:44.520 --> 01:10:45.520]   Yeah.
[01:10:45.520 --> 01:10:49.320]   I mean, we tried Zoom and a couple of the people in the office really like it.
[01:10:49.320 --> 01:10:52.160]   Personally, I'm not that enamored with it myself.
[01:10:52.160 --> 01:10:54.080]   Hangouts seems to work.
[01:10:54.080 --> 01:10:55.440]   But I don't know about you.
[01:10:55.440 --> 01:10:57.360]   And there was an article about this this week.
[01:10:57.360 --> 01:11:03.000]   But I am finding the amount of video conferencing really more wearing than I thought it would.
[01:11:03.000 --> 01:11:04.000]   You know.
[01:11:04.000 --> 01:11:05.000]   Sorry, Matt.
[01:11:05.000 --> 01:11:07.960]   I don't know what part of it is wearing.
[01:11:07.960 --> 01:11:16.280]   Although I've kind of, you know, we've worked asynchronously or synchronously as a team
[01:11:16.280 --> 01:11:18.280]   remotely for 15 years.
[01:11:18.280 --> 01:11:19.440]   So this is not new for me.
[01:11:19.440 --> 01:11:21.920]   There are a number of articles about Zoom fatigue.
[01:11:21.920 --> 01:11:28.080]   One of them talked about how people aren't used to seeing themselves on camera all day.
[01:11:28.080 --> 01:11:33.200]   So suddenly all of this, you know, kind of insecurity comes up and people are.
[01:11:33.200 --> 01:11:34.200]   I block my own video.
[01:11:34.200 --> 01:11:35.200]   I don't want to ask myself.
[01:11:35.200 --> 01:11:36.200]   Yeah, just don't look at yourself.
[01:11:36.200 --> 01:11:38.040]   Yeah, just don't look at yourself.
[01:11:38.040 --> 01:11:43.080]   Yeah, you see, I mean, I have to have the video going if I'm going to sort of do the costume
[01:11:43.080 --> 01:11:45.240]   thing, but it would be nice to block it.
[01:11:45.240 --> 01:11:47.080]   And also you've got it.
[01:11:47.080 --> 01:11:53.400]   I mean, OK, 1994, one of my first writing jobs was doing some freelance work for a PR
[01:11:53.400 --> 01:11:54.400]   on this company.
[01:11:54.400 --> 01:11:56.600]   And this will be the year of video conferencing.
[01:11:56.600 --> 01:11:58.720]   Now I've been hearing that deadline ever since.
[01:11:58.720 --> 01:12:01.960]   This actually is the year of video conferencing.
[01:12:01.960 --> 01:12:04.640]   And it's been very interesting seeing how people have adapted to it.
[01:12:04.640 --> 01:12:08.720]   Yes, it's kind of wearing and but it's the first time my family have actually done video
[01:12:08.720 --> 01:12:13.080]   conferencing is the first time we've done it as a team.
[01:12:13.080 --> 01:12:14.800]   I also you have to enforce some rules.
[01:12:14.800 --> 01:12:17.320]   For example, I am wearing trousers.
[01:12:17.320 --> 01:12:19.560]   Oh, good man.
[01:12:19.560 --> 01:12:20.560]   Good man.
[01:12:20.560 --> 01:12:22.480]   I'm not only worried trousers, I'm wearing braces.
[01:12:22.480 --> 01:12:23.480]   So there's no accidents.
[01:12:23.480 --> 01:12:25.400]   Oh, no accidents allowed.
[01:12:25.400 --> 01:12:26.400]   Yeah, but you can.
[01:12:26.400 --> 01:12:29.160]   You're wearing what?
[01:12:29.160 --> 01:12:30.160]   I'm wearing a train.
[01:12:30.160 --> 01:12:31.160]   This dress has a train.
[01:12:31.160 --> 01:12:33.520]   Where did you get this dress?
[01:12:33.520 --> 01:12:35.200]   Was this when you won the Oscar?
[01:12:35.200 --> 01:12:40.800]   Yes, that's when I was when I won the Oscar for best human.
[01:12:40.800 --> 01:12:47.000]   I mean, you can spot in video conferencing etiquette is really quite important because
[01:12:47.000 --> 01:12:48.160]   you can spot that.
[01:12:48.160 --> 01:12:51.480]   Oh, I'll just go and get this and doing these sideways.
[01:12:51.480 --> 01:12:52.480]   No chance.
[01:12:52.480 --> 01:12:54.120]   No, I can tell.
[01:12:54.120 --> 01:12:56.160]   Everyone knows.
[01:12:56.160 --> 01:13:00.000]   We've got nine people now in our Twit team conference.
[01:13:00.000 --> 01:13:03.840]   Did you guys you have to see how many people haven't turned on their microphones?
[01:13:03.840 --> 01:13:04.840]   Some I don't know.
[01:13:04.840 --> 01:13:05.840]   Beef team.
[01:13:05.840 --> 01:13:06.840]   I'm sorry.
[01:13:06.840 --> 01:13:07.840]   I don't know if this is a per.
[01:13:07.840 --> 01:13:09.080]   Do you call me to a pharmacy?
[01:13:09.080 --> 01:13:10.080]   Yeah.
[01:13:10.080 --> 01:13:11.080]   Yeah.
[01:13:11.080 --> 01:13:13.280]   Oh, great mask, by the way, mate.
[01:13:13.280 --> 01:13:14.880]   That's that's John.
[01:13:14.880 --> 01:13:16.360]   That's our board up.
[01:13:16.360 --> 01:13:19.320]   We require him to wear a mask in the studio at all times.
[01:13:19.320 --> 01:13:20.320]   Yeah.
[01:13:20.320 --> 01:13:21.320]   Oh, yeah.
[01:13:21.320 --> 01:13:23.000]   Have you seen the face of the alien face hugger masks?
[01:13:23.000 --> 01:13:24.720]   I so want one of them.
[01:13:24.720 --> 01:13:25.720]   Oh, like from.
[01:13:25.720 --> 01:13:26.720]   Yeah, German guys.
[01:13:26.720 --> 01:13:33.680]   The alien face hugger thing with like a strap going around the neck.
[01:13:33.680 --> 01:13:36.360]   I want one of those so badly.
[01:13:36.360 --> 01:13:38.080]   You could freak people out walking down the street.
[01:13:38.080 --> 01:13:39.520]   I actually got a sewing me.
[01:13:39.520 --> 01:13:40.520]   Oh, there's a kitty cat.
[01:13:40.520 --> 01:13:45.040]   I got a sewing machine and I bought a bunch of fabric and I've been making masks and I've
[01:13:45.040 --> 01:13:48.880]   been looking at different patterns, trying to find the most comfortable, most effective
[01:13:48.880 --> 01:13:49.880]   mask.
[01:13:49.880 --> 01:13:50.880]   It's kind of fun.
[01:13:50.880 --> 01:13:54.560]   You know, you can you can print your own fabric, you know, we've been doing that.
[01:13:54.560 --> 01:13:55.560]   So we've.
[01:13:55.560 --> 01:13:58.640]   No, I mean, we also have a 3D printer now.
[01:13:58.640 --> 01:14:01.920]   We're sending this to somewhere else, but you can send them.
[01:14:01.920 --> 01:14:03.120]   You can design whatever you want.
[01:14:03.120 --> 01:14:04.920]   Oh, I want to do that.
[01:14:04.920 --> 01:14:07.440]   Oh, then because I want Twit masks.
[01:14:07.440 --> 01:14:08.440]   Yeah.
[01:14:08.440 --> 01:14:13.000]   So I just made my secret I made for my husband's practice.
[01:14:13.000 --> 01:14:16.040]   I got his company's logo.
[01:14:16.040 --> 01:14:18.160]   Is he still working?
[01:14:18.160 --> 01:14:21.200]   So he's there are yes and no.
[01:14:21.200 --> 01:14:23.560]   Because you have ophthalmologic emergencies sometimes.
[01:14:23.560 --> 01:14:30.240]   Yeah, like some idiot got metal in his eye like three days ago, was like, oh, it's fine.
[01:14:30.240 --> 01:14:34.200]   And now his eyes all inflamed and it's gooey and gross.
[01:14:34.200 --> 01:14:38.560]   So that everybody in the area, a lot of the specialists said they don't want people clogging
[01:14:38.560 --> 01:14:39.560]   the ER.
[01:14:39.560 --> 01:14:40.840]   Somebody's got to treat it.
[01:14:40.840 --> 01:14:44.880]   They don't want to clog the ER with people who really aren't that kind of emergency.
[01:14:44.880 --> 01:14:50.680]   So there's a lot of the doctors in the area who are specialists are sort of on call or
[01:14:50.680 --> 01:14:54.120]   staying open for emergency cases, which is what he's doing.
[01:14:54.120 --> 01:14:55.920]   He's got to be, you know, masked up.
[01:14:55.920 --> 01:14:56.920]   Right.
[01:14:56.920 --> 01:14:57.920]   He has to have PPE, right?
[01:14:57.920 --> 01:15:04.520]   Is he more worried about giving it to somebody or getting it from someone?
[01:15:04.520 --> 01:15:06.200]   I think probably both.
[01:15:06.200 --> 01:15:07.200]   He sees a lot.
[01:15:07.200 --> 01:15:11.760]   He's does a lot of very complicated diagnoses.
[01:15:11.760 --> 01:15:15.080]   And so he he is a lot of elderly people coming in and out.
[01:15:15.080 --> 01:15:18.520]   He doesn't want to get anybody sick and he doesn't want them to get sick because they
[01:15:18.520 --> 01:15:22.560]   came to the office and he doesn't want to get sick and nobody wants to get sick, you
[01:15:22.560 --> 01:15:23.560]   know.
[01:15:23.560 --> 01:15:24.560]   Yeah.
[01:15:24.560 --> 01:15:35.880]   Let's take a little break with Lady Amy Webb joining us from the future today Institute
[01:15:35.880 --> 01:15:38.520]   for the future today.
[01:15:38.520 --> 01:15:42.440]   And from the register, it's Lord Ian Thompson.
[01:15:42.440 --> 01:15:47.280]   Actually, I can't think of two people I'd rather be quarantined with.
[01:15:47.280 --> 01:15:49.880]   I just think the conversations would be great.
[01:15:49.880 --> 01:15:50.880]   Ah, yeah.
[01:15:50.880 --> 01:15:51.880]   That's very nice.
[01:15:51.880 --> 01:15:59.240]   Yeah, we are now up to 13 unlucky 13 people in the Jitzy server there.
[01:15:59.240 --> 01:16:02.160]   Everyone's being very coy about their cameras as well.
[01:16:02.160 --> 01:16:05.880]   Well, you know, looks like somebody's got us on the big screen.
[01:16:05.880 --> 01:16:06.880]   That's turbo.
[01:16:06.880 --> 01:16:07.880]   That's nice.
[01:16:07.880 --> 01:16:08.880]   You can add your name.
[01:16:08.880 --> 01:16:09.880]   Oh, there's Dr. Mom.
[01:16:09.880 --> 01:16:10.880]   Hi, Lil.
[01:16:10.880 --> 01:16:12.480]   I recognize Lil.
[01:16:12.480 --> 01:16:14.040]   So what is it's called?
[01:16:14.040 --> 01:16:17.040]   It's Jitzy, which is an open source JITSI.
[01:16:17.040 --> 01:16:18.040]   That's not cool.
[01:16:18.040 --> 01:16:19.040]   I want to try it.
[01:16:19.040 --> 01:16:20.040]   Yeah.
[01:16:20.040 --> 01:16:21.120]   And what's nice to use is WebRTC.
[01:16:21.120 --> 01:16:22.520]   So you just send out a URL.
[01:16:22.520 --> 01:16:24.240]   You don't have to install anything.
[01:16:24.240 --> 01:16:25.520]   It's the only thing I don't like about Zoom.
[01:16:25.520 --> 01:16:28.000]   I don't like to install the Zoom software.
[01:16:28.000 --> 01:16:31.480]   I'm kind of surprised WebRTC didn't take off faster.
[01:16:31.480 --> 01:16:35.680]   I guess with you guys remember Project Morpheus from a billion years ago that was created
[01:16:35.680 --> 01:16:37.120]   to create a decentralized internet.
[01:16:37.120 --> 01:16:42.480]   I kind of thought that WebRTC, I think that that dissolved because of political infighting,
[01:16:42.480 --> 01:16:48.120]   but I really did think that WebRTC was going to sort of take off and hit the masses much
[01:16:48.120 --> 01:16:50.120]   faster than it has.
[01:16:50.120 --> 01:16:51.120]   Yeah.
[01:16:51.120 --> 01:16:52.120]   Yeah.
[01:16:52.120 --> 01:16:56.240]   I mean, coming back to what we were saying also, one of the things I'd really, this is
[01:16:56.240 --> 01:17:00.680]   that we have to call this out because this is just socially inappropriate.
[01:17:00.680 --> 01:17:05.480]   When you're in a massive Zoom call, there's always one bugger who's just like, oh, check
[01:17:05.480 --> 01:17:07.000]   out this new background and check out.
[01:17:07.000 --> 01:17:10.200]   And now I'm a potato and now I'm cubist.
[01:17:10.200 --> 01:17:12.720]   It's the worst kind of show stuff.
[01:17:12.720 --> 01:17:17.960]   We had a little disease with the snap camera.
[01:17:17.960 --> 01:17:22.760]   And there was a disease that kind of spreads throughout the company.
[01:17:22.760 --> 01:17:24.080]   I fell for it.
[01:17:24.080 --> 01:17:28.320]   Everybody fell for it a little bit because you get bored after a while and it's fun to
[01:17:28.320 --> 01:17:29.880]   be a potato.
[01:17:29.880 --> 01:17:33.560]   Well, yeah, okay.
[01:17:33.560 --> 01:17:36.560]   That's why.
[01:17:36.560 --> 01:17:37.560]   Let's take a little break.
[01:17:37.560 --> 01:17:43.840]   And then I think we can do, I have some thoughts about, and I really want to quiz you guys
[01:17:43.840 --> 01:17:50.600]   about how the world might change in some interesting ways, like education, for instance,
[01:17:50.600 --> 01:17:55.600]   how this might set the tone for a whole new way of being self-driving automobiles or in
[01:17:55.600 --> 01:17:58.760]   particular self-driving delivery vehicles.
[01:17:58.760 --> 01:18:02.800]   There's some interesting technologies, it might be forwarded by this.
[01:18:02.800 --> 01:18:08.640]   But I'm curious what you think, and then maybe some technologies like in fact autonomous
[01:18:08.640 --> 01:18:13.840]   vehicles for end users might be held back a little bit by this.
[01:18:13.840 --> 01:18:20.160]   I'd like to talk a little with our guests, the futurist Amy Webb and a man who lives
[01:18:20.160 --> 01:18:21.640]   in the future.
[01:18:21.640 --> 01:18:26.640]   Our show today brought to you by LastPass because of their sponsorship.
[01:18:26.640 --> 01:18:27.880]   We can bring you that breaking news.
[01:18:27.880 --> 01:18:29.800]   We can bring you our shows.
[01:18:29.800 --> 01:18:32.080]   They have been very good to us this year.
[01:18:32.080 --> 01:18:33.080]   Boy, you know what?
[01:18:33.080 --> 01:18:37.720]   I couldn't be happier because I've been a LastPass user, a happy LastPass user for more
[01:18:37.720 --> 01:18:39.600]   than a decade.
[01:18:39.600 --> 01:18:44.880]   LastPass is of course the world's best password manager, but it's so much more.
[01:18:44.880 --> 01:18:52.320]   And as you send your workforce home, you got to remember they now are a potential security
[01:18:52.320 --> 01:18:58.200]   risk, but LastPass can make that transition from in office to in home a lot easier.
[01:18:58.200 --> 01:19:00.720]   You still want the IT team to be the gatekeeper, right?
[01:19:00.720 --> 01:19:07.880]   You don't want to give them give up their control because after all these resources like
[01:19:07.880 --> 01:19:13.760]   your banking, like your databases, like your website, these are the keys to the kingdom.
[01:19:13.760 --> 01:19:18.840]   LastPass helps IT stay in control over which employees are accessing which resources.
[01:19:18.840 --> 01:19:22.640]   It doesn't matter if they're working from home or working from the office.
[01:19:22.640 --> 01:19:27.520]   You get unified visibility over access and authentication everywhere.
[01:19:27.520 --> 01:19:31.560]   LastPass also makes sure that your employees have secure access to their work applications
[01:19:31.560 --> 01:19:35.680]   with tools like single sign-on which employees love because it's so easy.
[01:19:35.680 --> 01:19:40.680]   They just tap yes on their phone and yet it's more secure than passwords.
[01:19:40.680 --> 01:19:43.840]   And of course, LastPass is the king of password management.
[01:19:43.840 --> 01:19:49.720]   They generate long, strong passwords and remember them for you in an encrypted password vault
[01:19:49.720 --> 01:19:52.800]   that can only be decrypted on your devices.
[01:19:52.800 --> 01:19:56.160]   LastPass never has access to it, never has the password.
[01:19:56.160 --> 01:19:58.440]   It works everywhere you do.
[01:19:58.440 --> 01:20:04.640]   Windows, Mac, Linux, iOS, Android, every time I install a new browser or set up a new system,
[01:20:04.640 --> 01:20:06.600]   it's the first thing I install.
[01:20:06.600 --> 01:20:10.920]   LastPass makes your team more productive too as well as more secure because with LastPass
[01:20:10.920 --> 01:20:17.120]   folders in the LastPass Enterprise product, you just create job assignments, workflows.
[01:20:17.120 --> 01:20:20.680]   For instance, we have an ops folder, we have a business office folder, we have folders
[01:20:20.680 --> 01:20:22.240]   for the studio.
[01:20:22.240 --> 01:20:26.000]   These different folders make it very easy to onboard an employee and say, "Look, you have
[01:20:26.000 --> 01:20:29.920]   access to these resources because we're going to add you to this folder."
[01:20:29.920 --> 01:20:31.080]   That's huge.
[01:20:31.080 --> 01:20:35.520]   LastPass lets remote employees securely share passwords across teams.
[01:20:35.520 --> 01:20:36.760]   So there's no friction.
[01:20:36.760 --> 01:20:42.600]   You know, the average business has 185 shared folders.
[01:20:42.600 --> 01:20:47.240]   It's critical for businesses to consider implementing policies and tracking reports on shared password
[01:20:47.240 --> 01:20:51.960]   activity to make sure employees are operating securely and not sharing passwords with people
[01:20:51.960 --> 01:20:53.160]   they shouldn't.
[01:20:53.160 --> 01:20:59.880]   And that's what LastPass does, seamless collaboration with coworkers without reducing your security.
[01:20:59.880 --> 01:21:05.560]   And of course, there are more phishing attacks, spear phishing particularly.
[01:21:05.560 --> 01:21:09.320]   LastPass reduces the risk of phishing schemes because it will never auto-fill passwords
[01:21:09.320 --> 01:21:11.160]   on suspicious websites.
[01:21:11.160 --> 01:21:15.480]   And for remote employees who are logging in through the VPN, LastPass adds an additional
[01:21:15.480 --> 01:21:20.960]   layer of security to your VPN through biometric multi-factor authentication.
[01:21:20.960 --> 01:21:22.320]   We couldn't live with that LastPass.
[01:21:22.320 --> 01:21:24.160]   That was even before this happened.
[01:21:24.160 --> 01:21:25.880]   And now we're so glad we have it.
[01:21:25.880 --> 01:21:28.560]   We're going to do an event with LastPass in a couple of weeks.
[01:21:28.560 --> 01:21:29.760]   I'm very excited.
[01:21:29.760 --> 01:21:30.760]   It's going to be a virtual event.
[01:21:30.760 --> 01:21:34.800]   It was going to be an in-person event, but of course, as with everything, it's virtual.
[01:21:34.800 --> 01:21:38.240]   Cyber security and identity trends unlocked.
[01:21:38.240 --> 01:21:41.240]   Our guests will be myself, Steve Gibson.
[01:21:41.240 --> 01:21:42.640]   Actually, I'll be hosting the panel.
[01:21:42.640 --> 01:21:45.920]   I'm no expert, but Steve Gibson is Andrew Keene.
[01:21:45.920 --> 01:21:47.600]   He'll be a lot of fun.
[01:21:47.600 --> 01:21:51.240]   But of course, LastPass's CISO, Gerald Bookelp.
[01:21:51.240 --> 01:21:55.040]   It's Thursday, May 14th, 1 o'clock Pacific time.
[01:21:55.040 --> 01:21:58.080]   It'll be a live panel, stream it at twit.tv/live.
[01:21:58.080 --> 01:22:01.040]   And afterwards, of course, it'll appear on the Twit events feed.
[01:22:01.040 --> 01:22:06.080]   We're going to discuss what the new decade will bring, how it's going to impact digital
[01:22:06.080 --> 01:22:10.520]   identity and authentication, both for businesses and individuals.
[01:22:10.520 --> 01:22:13.240]   We had a great event in Boston a couple of months ago.
[01:22:13.240 --> 01:22:14.520]   I'm very excited about this one.
[01:22:14.520 --> 01:22:18.120]   This will be virtual coming out of our LastPass studios.
[01:22:18.120 --> 01:22:21.200]   May 14th, just put a pin in that.
[01:22:21.200 --> 01:22:25.360]   Save the date, 1 p.m. Pacific, twit.tv/live.
[01:22:25.360 --> 01:22:31.880]   LastPass gets security easy, but effective security across every access point with LastPass.
[01:22:31.880 --> 01:22:34.640]   LastPass.com/twit to find out more.
[01:22:34.640 --> 01:22:36.640]   LastPass.com/twit.
[01:22:36.640 --> 01:22:37.640]   Thank you, LastPass.
[01:22:37.640 --> 01:22:43.040]   I'm going to tip a little Louis XIII in your honor because, man, we couldn't do it without
[01:22:43.040 --> 01:22:44.040]   you.
[01:22:44.040 --> 01:22:45.040]   LastPass.
[01:22:45.040 --> 01:22:46.040]   All right.
[01:22:46.040 --> 01:22:52.040]   Now that I've went my whistle, let me just see how many people are in the jitzy.
[01:22:52.040 --> 01:22:54.720]   Holy cow.
[01:22:54.720 --> 01:22:56.960]   Nobody's turning on cameras, but there are a lot of people in there.
[01:22:56.960 --> 01:23:03.120]   If I turn on the sound, it'd probably go crazy.
[01:23:03.120 --> 01:23:04.120]   So this is kind of fun.
[01:23:04.120 --> 01:23:05.120]   It's open source.
[01:23:05.120 --> 01:23:06.120]   I got a server.
[01:23:06.120 --> 01:23:08.400]   You know, John, I think I'm going to bring the server in here because I think this is
[01:23:08.400 --> 01:23:10.600]   a better place for it than my house.
[01:23:10.600 --> 01:23:14.280]   We have that demo network open to the outside world.
[01:23:14.280 --> 01:23:15.440]   I'll put it on that.
[01:23:15.440 --> 01:23:17.240]   I'll bring it in the next couple of days.
[01:23:17.240 --> 01:23:18.520]   It's also my Minecraft server.
[01:23:18.520 --> 01:23:21.320]   You don't mind, do you?
[01:23:21.320 --> 01:23:22.960]   No.
[01:23:22.960 --> 01:23:24.160]   Schools.
[01:23:24.160 --> 01:23:29.280]   A lot of kids, sad to say, a lot of seniors, not walking.
[01:23:29.280 --> 01:23:35.200]   Of course, West Point reuniting just for the benefit of the president, but most schools
[01:23:35.200 --> 01:23:40.120]   are having either virtual graduations, virtual proms.
[01:23:40.120 --> 01:23:41.400]   They're not getting to walk.
[01:23:41.400 --> 01:23:43.120]   And it probably is the case.
[01:23:43.120 --> 01:23:49.360]   I don't know, but this fall, we may well be going back to class online again.
[01:23:49.360 --> 01:23:56.120]   My son, our stepson, my stepson, Elisa Sun, Michael, 17, high school junior, he's been
[01:23:56.120 --> 01:24:00.880]   going to online school now for a year and a half long before this.
[01:24:00.880 --> 01:24:04.920]   And he's really comfortable with it, but I kind of wonder, you know, we've heard numbers
[01:24:04.920 --> 01:24:08.880]   like 50% of students not showing up for online classes.
[01:24:08.880 --> 01:24:13.600]   And yet at the same time, there's a lot to be said for online classes.
[01:24:13.600 --> 01:24:18.240]   It can bring education into more homes.
[01:24:18.240 --> 01:24:21.280]   It can give you access to better teachers.
[01:24:21.280 --> 01:24:24.120]   Maybe you have to, you know, it also leaves some people out if you don't have internet
[01:24:24.120 --> 01:24:27.800]   access and laptops.
[01:24:27.800 --> 01:24:28.800]   What do you think, Amy?
[01:24:28.800 --> 01:24:30.320]   What is the future of schooling?
[01:24:30.320 --> 01:24:34.720]   Is this going to give us a new kind of way of thinking about education?
[01:24:34.720 --> 01:24:42.000]   >> Yeah, so before I do that, can I just make a quick toast, since we were toasting?
[01:24:42.000 --> 01:24:43.800]   >> Please.
[01:24:43.800 --> 01:24:50.520]   >> So all of the healthcare workers are obviously, you know, putting their lives at
[01:24:50.520 --> 01:24:53.520]   risk to help us all.
[01:24:53.520 --> 01:24:58.320]   But there's a whole other group of unsung heroes that get no credit, and that's everybody
[01:24:58.320 --> 01:24:59.920]   working in IT.
[01:24:59.920 --> 01:25:05.640]   All the sys admins, all the network engineers, everybody that nobody knows who are making
[01:25:05.640 --> 01:25:12.600]   this possible, who are stressed out of their gourds right now, trying to make sure that
[01:25:12.600 --> 01:25:17.240]   all of these systems for school, for work, for everything stay active.
[01:25:17.240 --> 01:25:19.160]   So just want to say thank you.
[01:25:19.160 --> 01:25:21.360]   I've got an empty glass, but thank you.
[01:25:21.360 --> 01:25:22.800]   >> Got to say, yeah.
[01:25:22.800 --> 01:25:23.800]   Same with us.
[01:25:23.800 --> 01:25:29.360]   I mean, our IT support guy at the Reg is based in Italy.
[01:25:29.360 --> 01:25:34.080]   So he's been under lockdown for about two and a bit months now.
[01:25:34.080 --> 01:25:37.440]   And as it turns out, he doesn't really like people, so he's quite happy with it, but his
[01:25:37.440 --> 01:25:39.440]   wife isn't.
[01:25:39.440 --> 01:25:43.800]   But yeah, these are the people who are keeping us going, who are keeping the entire infrastructure
[01:25:43.800 --> 01:25:46.280]   of the nation going, and they deserve our thanks.
[01:25:46.280 --> 01:25:48.840]   >> Yeah, we have our own IT people.
[01:25:48.840 --> 01:25:51.720]   And of course, our studio engineers, John's here.
[01:25:51.720 --> 01:25:56.560]   We have somebody has to be here every single day, and Russell is staying in touch and working
[01:25:56.560 --> 01:25:57.560]   with us too.
[01:25:57.560 --> 01:26:01.920]   >> Absolutely, that is a tough job, but even tougher to do remotely.
[01:26:01.920 --> 01:26:07.480]   And when you have to come in and splice cable or reset a machine, it's even more dangerous.
[01:26:07.480 --> 01:26:10.320]   We have a lot of IT professionals in our audience.
[01:26:10.320 --> 01:26:12.760]   I'd say that's probably the vast majority of our audience.
[01:26:12.760 --> 01:26:13.760]   So I'll lift you up.
[01:26:13.760 --> 01:26:14.760]   >> So good health tool.
[01:26:14.760 --> 01:26:15.760]   Cheers.
[01:26:15.760 --> 01:26:16.760]   >> Cheers.
[01:26:16.760 --> 01:26:19.520]   >> And nobody acknowledges the stress that you're under.
[01:26:19.520 --> 01:26:21.120]   I just want to say thank you.
[01:26:21.120 --> 01:26:23.120]   So all right.
[01:26:23.120 --> 01:26:24.120]   School.
[01:26:24.120 --> 01:26:27.120]   So what would it take?
[01:26:27.120 --> 01:26:32.600]   What would it take for school to have been changed?
[01:26:32.600 --> 01:26:37.280]   Most, not, so I've talked about this on the show before.
[01:26:37.280 --> 01:26:38.720]   We have kind of a unique setup.
[01:26:38.720 --> 01:26:42.480]   We built my daughter's internet, which is her own.
[01:26:42.480 --> 01:26:44.560]   And as far as she's concerned, she's concerned.
[01:26:44.560 --> 01:26:46.400]   She has access to everything.
[01:26:46.400 --> 01:26:47.400]   She doesn't.
[01:26:47.400 --> 01:26:48.480]   It's very, very locked down.
[01:26:48.480 --> 01:26:51.440]   And we've got these separate provisions for her.
[01:26:51.440 --> 01:26:53.880]   But we have a very, very strong connection.
[01:26:53.880 --> 01:26:58.560]   So again, I'm very acknowledging my privilege here.
[01:26:58.560 --> 01:26:59.840]   She's got no problem.
[01:26:59.840 --> 01:27:03.840]   She lives with two highly technical parents.
[01:27:03.840 --> 01:27:06.360]   And so for us, this hasn't been a challenge.
[01:27:06.360 --> 01:27:12.520]   Now, I know that every day her parents, the parents that are in this, her class that are
[01:27:12.520 --> 01:27:16.960]   trying to connect remotely are challenged.
[01:27:16.960 --> 01:27:20.200]   Not everybody has stable connections.
[01:27:20.200 --> 01:27:24.240]   Some people for God's sake are still using like saddle dish, satellite.
[01:27:24.240 --> 01:27:25.240]   A lot of people.
[01:27:25.240 --> 01:27:26.480]   They're the real area.
[01:27:26.480 --> 01:27:28.400]   That may be your only choice.
[01:27:28.400 --> 01:27:29.400]   That's right.
[01:27:29.400 --> 01:27:30.560]   Not everybody's on a T3.
[01:27:30.560 --> 01:27:33.280]   And everybody's got fiber.
[01:27:33.280 --> 01:27:38.200]   There may be situations where you've got one device for a family.
[01:27:38.200 --> 01:27:45.120]   Printers are super useful when you're doing math and learning how to do equations and
[01:27:45.120 --> 01:27:46.440]   write things down.
[01:27:46.440 --> 01:27:56.720]   So I don't think it's realistic that we revert to some kind of online only system going forward.
[01:27:56.720 --> 01:28:03.120]   That being said, there are changes coming to education that I think we ought to consider.
[01:28:03.120 --> 01:28:07.520]   So one of those is that other countries were actually already set up to deal with distance
[01:28:07.520 --> 01:28:10.000]   learning in a way that we aren't.
[01:28:10.000 --> 01:28:14.160]   And that's going to give, and China is one of them, it's going to give them a competitive
[01:28:14.160 --> 01:28:15.240]   advantage.
[01:28:15.240 --> 01:28:23.080]   The other thing is that in the United States, our education systems are funded not an insignificant
[01:28:23.080 --> 01:28:24.920]   amount through the lottery.
[01:28:24.920 --> 01:28:27.480]   And people are not out buying lottery tickets.
[01:28:27.480 --> 01:28:28.480]   Let's take that was.
[01:28:28.480 --> 01:28:29.480]   Oh my God.
[01:28:29.480 --> 01:28:30.480]   Right.
[01:28:30.480 --> 01:28:31.480]   So a lot of slaves.
[01:28:31.480 --> 01:28:34.480]   You get to take your funding for the lottery.
[01:28:34.480 --> 01:28:38.360]   I wish I was making this up, but in a lot of states it's different.
[01:28:38.360 --> 01:28:43.880]   When we voted for this in California, Ian, the slogan was the kids win too.
[01:28:43.880 --> 01:28:50.280]   The sad thing is that the people who run the lottery are basically Vegas back at house
[01:28:50.280 --> 01:28:56.920]   gambling ventures who have very much made sure that the profit mostly goes to them.
[01:28:56.920 --> 01:28:57.920]   Yeah.
[01:28:57.920 --> 01:29:01.360]   So the issue is the revenue that was designated.
[01:29:01.360 --> 01:29:05.840]   So like that line and schools are already public schools for the most part are running
[01:29:05.840 --> 01:29:08.760]   super, super razor thin margins.
[01:29:08.760 --> 01:29:11.880]   So you have the lack of anyhow.
[01:29:11.880 --> 01:29:15.120]   So this is super bad.
[01:29:15.120 --> 01:29:19.960]   And the question that I keep coming back to is I'm sort of less interested in the technology
[01:29:19.960 --> 01:29:25.920]   at the moment and more interested in what 20 years from now does our workforce look
[01:29:25.920 --> 01:29:28.760]   like because we will have had a.
[01:29:28.760 --> 01:29:35.240]   There's all this longitudinal data on why early childhood education matters a lot and
[01:29:35.240 --> 01:29:43.600]   why words like volume of words over just sort of screen time that's passive.
[01:29:43.600 --> 01:29:48.480]   And we know that screen passive screen time is going way up versus active learning, which
[01:29:48.480 --> 01:29:51.000]   is going down in a lot of cases.
[01:29:51.000 --> 01:29:58.840]   So 20 years from now is also about the same time that projections are assuming that things
[01:29:58.840 --> 01:30:04.280]   stay constant, but that's when some of our AI starts to make leaps and we start to see
[01:30:04.280 --> 01:30:07.760]   more technological unemployment.
[01:30:07.760 --> 01:30:10.320]   I'm not worried about the implications for right now.
[01:30:10.320 --> 01:30:15.360]   I'm actually worried about education, the negative impacts of education 20 years from
[01:30:15.360 --> 01:30:16.360]   now.
[01:30:16.360 --> 01:30:17.360]   Yeah.
[01:30:17.360 --> 01:30:25.920]   You see, I mean, I have the next door neighbors on both sides of me are teachers.
[01:30:25.920 --> 01:30:30.000]   Holly teaches kindergarten and Jennifer teaches music.
[01:30:30.000 --> 01:30:34.080]   Now the music teaching profession has had an absolute.
[01:30:34.080 --> 01:30:38.880]   I can't say it, but a bit of a bit of a bit of a bit of a about this, but what's really
[01:30:38.880 --> 01:30:43.200]   shocked me about this is the lack of support given to teachers because they were basically
[01:30:43.200 --> 01:30:47.680]   given, you know, like a three hour course in how to prepare video calls.
[01:30:47.680 --> 01:30:48.680]   Yeah.
[01:30:48.680 --> 01:30:51.000]   Talk about heroes getting a holy.
[01:30:51.000 --> 01:30:52.000]   Yeah.
[01:30:52.000 --> 01:30:53.000]   Yeah.
[01:30:53.000 --> 01:30:54.000]   Although it's not the technology.
[01:30:54.000 --> 01:30:56.000]   Well, the syllabus has to change too.
[01:30:56.000 --> 01:30:59.360]   You can't like not stand up there in lecture.
[01:30:59.360 --> 01:31:02.360]   You can't say, Jamie, what do you, what's the answer?
[01:31:02.360 --> 01:31:04.240]   It doesn't work.
[01:31:04.240 --> 01:31:05.440]   Well, it's right.
[01:31:05.440 --> 01:31:09.120]   So I also, I'm a professor at NYU Stern School Business and Mike.
[01:31:09.120 --> 01:31:13.160]   So we went to online, the whole school went to distance learning like at the very beginning
[01:31:13.160 --> 01:31:14.240]   of March.
[01:31:14.240 --> 01:31:18.040]   We were kind of set up to do that because I was already, I teach a very different, I
[01:31:18.040 --> 01:31:22.000]   teach this foresight, but I do it using a lot of different tools.
[01:31:22.000 --> 01:31:28.680]   So we were kind of okay, but a lot of classes, you can't, you know, you can't just take the
[01:31:28.680 --> 01:31:31.880]   existing syllabus and make it work online.
[01:31:31.880 --> 01:31:35.560]   It doesn't work that way.
[01:31:35.560 --> 01:31:36.560]   So yeah.
[01:31:36.560 --> 01:31:44.960]   No, I mean, it's although there was, I think it was, it Ben adidas, who's saying he basically
[01:31:44.960 --> 01:31:48.480]   came downstairs to find his kid playing with a dog and he's like, are you supposed to be
[01:31:48.480 --> 01:31:49.480]   in class right now?
[01:31:49.480 --> 01:31:53.680]   I said, well, actually, I got a 22nd loop of video loop.
[01:31:53.680 --> 01:31:54.680]   Oh, intensive.
[01:31:54.680 --> 01:31:58.480]   Oh, run it in the video stream and I can get on with other things.
[01:31:58.480 --> 01:32:02.080]   And he goes, I was half outrage and half so impressed.
[01:32:02.080 --> 01:32:03.080]   Yeah.
[01:32:03.080 --> 01:32:07.120]   Yeah, he's learning skills, maybe not the skills you want them to learn, but still you can't
[01:32:07.120 --> 01:32:08.120]   tonight.
[01:32:08.120 --> 01:32:09.800]   Hey, useful in the real world.
[01:32:09.800 --> 01:32:10.800]   Yeah.
[01:32:10.800 --> 01:32:12.360]   That's like clever.
[01:32:12.360 --> 01:32:16.640]   Yeah, I mean, I think higher education is one thing, but I think when you're talking about
[01:32:16.640 --> 01:32:24.720]   K through six or even K through 12, where you're talking about no socialization, no social
[01:32:24.720 --> 01:32:31.040]   life, our poor son, who's been doing online school, he at least had a social life.
[01:32:31.040 --> 01:32:34.680]   We would make sure that there were kids around the rest of the time.
[01:32:34.680 --> 01:32:35.760]   Now he's going nuts.
[01:32:35.760 --> 01:32:37.520]   He's so sick of us.
[01:32:37.520 --> 01:32:40.240]   The land, you know, it's just really a challenge.
[01:32:40.240 --> 01:32:44.640]   So I don't mean to say that everybody's going to say, oh, this is the greatest thing ever
[01:32:44.640 --> 01:32:45.760]   and we're going to do more of it.
[01:32:45.760 --> 01:32:49.920]   But I do think it might impact how we think about teaching.
[01:32:49.920 --> 01:32:55.640]   And certainly going to impact how we think about the inequality in access to the internet,
[01:32:55.640 --> 01:32:57.440]   to technology in general.
[01:32:57.440 --> 01:33:02.440]   Well, I mean, we forget 5% of Americans don't have internet access.
[01:33:02.440 --> 01:33:06.480]   I mean, and frankly, for the country that invented the internet, that's pretty shameful.
[01:33:06.480 --> 01:33:07.480]   Yeah.
[01:33:07.480 --> 01:33:09.080]   But I mean, there's still people on dial up.
[01:33:09.080 --> 01:33:13.720]   There's a lot of kids who with the schools being closed don't have access to laptops anymore.
[01:33:13.720 --> 01:33:15.560]   What's how are they supposed to learn?
[01:33:15.560 --> 01:33:19.960]   I feel like, and I don't know where the money comes from for UBI, but it feels so I don't
[01:33:19.960 --> 01:33:23.240]   really, the economics never kind of clicked for me.
[01:33:23.240 --> 01:33:24.640]   I read as much Android.
[01:33:24.640 --> 01:33:26.480]   We just had UBI.
[01:33:26.480 --> 01:33:27.720]   Everybody get their $1,200.
[01:33:27.720 --> 01:33:28.720]   That's right.
[01:33:28.720 --> 01:33:34.360]   I wonder if that's not something in the future as well as some sort of universal basic income.
[01:33:34.360 --> 01:33:38.040]   There certainly needs to be universal internet access.
[01:33:38.040 --> 01:33:40.160]   I wonder if that's not in our future a little bit.
[01:33:40.160 --> 01:33:45.000]   Well, I mean, if there is universal basic income based on what they're saying, since
[01:33:45.000 --> 01:33:48.600]   Steve Munchen saying, "Oh, $1,200, that's good for everyone for 10 weeks."
[01:33:48.600 --> 01:33:49.600]   Yeah, no.
[01:33:49.600 --> 01:33:50.600]   Really?
[01:33:50.600 --> 01:33:51.600]   10 weeks.
[01:33:51.600 --> 01:33:52.600]   No.
[01:33:52.600 --> 01:33:54.920]   Well, even Yang only proposed $1,000 a month.
[01:33:54.920 --> 01:33:58.360]   I think it's got to be $2,000 a month easily.
[01:33:58.360 --> 01:33:59.760]   We don't.
[01:33:59.760 --> 01:34:04.320]   Our economic structure, we are too big of a country and there are too many people.
[01:34:04.320 --> 01:34:07.960]   So it doesn't work at this kind of scale is the problem.
[01:34:07.960 --> 01:34:08.960]   Yeah.
[01:34:08.960 --> 01:34:09.960]   Okay.
[01:34:09.960 --> 01:34:13.320]   In the UK, aren't they doing something like that?
[01:34:13.320 --> 01:34:20.200]   In the UK, basically, they're doing what's called the government will cover 80% of someone's
[01:34:20.200 --> 01:34:24.400]   salary for the next three months up to $2,400 quid.
[01:34:24.400 --> 01:34:27.240]   So it's about $3,000.
[01:34:27.240 --> 01:34:28.400]   That's pretty good.
[01:34:28.400 --> 01:34:29.400]   That's pretty amazing.
[01:34:29.400 --> 01:34:30.400]   Well, I mean, it's...
[01:34:30.400 --> 01:34:33.320]   Plus, you have universal healthcare.
[01:34:33.320 --> 01:34:39.000]   So really, you've got a life net, a safety net for anybody.
[01:34:39.000 --> 01:34:41.800]   They're not going to starve and they're going to have medical care.
[01:34:41.800 --> 01:34:42.800]   Yeah.
[01:34:42.800 --> 01:34:50.640]   I mean, it's the basic social contract in that government isn't just there to facilitate
[01:34:50.640 --> 01:34:51.640]   business.
[01:34:51.640 --> 01:34:53.200]   It's there to actually make life better for its people.
[01:34:53.200 --> 01:35:01.120]   Now, okay, in Britain, 80%, I think Australia has done 75, Netherlands has done 90.
[01:35:01.120 --> 01:35:05.360]   There's also signs of some firming up in terms of European countries not giving bailouts
[01:35:05.360 --> 01:35:09.280]   to tax exempt to people of offshore companies of offshore their taxes.
[01:35:09.280 --> 01:35:15.520]   I love it that the cruise lines, which of all the flags in Panama and the Caribbean are
[01:35:15.520 --> 01:35:17.520]   saying, "But we're American companies.
[01:35:17.520 --> 01:35:18.520]   Help us out."
[01:35:18.520 --> 01:35:20.320]   No, I don't think so.
[01:35:20.320 --> 01:35:22.040]   Well, not just the cruise liners.
[01:35:22.040 --> 01:35:24.480]   I mean, look at Richard Branson, for goodness' sake.
[01:35:24.480 --> 01:35:30.120]   Virgin Atlantic is asking the UK government for a $500 million bailout.
[01:35:30.120 --> 01:35:32.240]   They're based in the British field of Virgin Islands.
[01:35:32.240 --> 01:35:34.560]   Ask the BVI for your bailout, please.
[01:35:34.560 --> 01:35:35.560]   Don't come into...
[01:35:35.560 --> 01:35:39.800]   Didn't they also declare whatever the UK version of bankruptcy is?
[01:35:39.800 --> 01:35:41.800]   I don't think he's done the bankruptcy.
[01:35:41.800 --> 01:35:47.280]   He did sue the National Health Service because they refused to give him a contract.
[01:35:47.280 --> 01:35:53.520]   But if anyone in the UK's public perception has gone down, Richard Branson has gone from
[01:35:53.520 --> 01:36:00.240]   being that cuddly, left, sort of alternative billionaires gone to utter scumbag if he sets
[01:36:00.240 --> 01:36:02.520]   foot on the country again, we will hunt him down.
[01:36:02.520 --> 01:36:05.720]   Didn't he offer to mortgage his necker island?
[01:36:05.720 --> 01:36:07.440]   Oh, yeah, it's private island.
[01:36:07.440 --> 01:36:08.760]   That's so good of him.
[01:36:08.760 --> 01:36:09.760]   Thank you very much.
[01:36:09.760 --> 01:36:11.160]   I'll give you my private island.
[01:36:11.160 --> 01:36:13.120]   I'm only going to give you the left private island.
[01:36:13.120 --> 01:36:15.160]   I'm keeping the right private island for myself.
[01:36:15.160 --> 01:36:18.800]   Yeah, you know, and also, this private island has been burnt down once and has had two
[01:36:18.800 --> 01:36:19.800]   hurricanes.
[01:36:19.800 --> 01:36:21.800]   It's not exactly a great investment.
[01:36:21.800 --> 01:36:23.960]   You know, a lot of tax purposes.
[01:36:23.960 --> 01:36:26.920]   So, you know, but I mean, you see the same in the US.
[01:36:26.920 --> 01:36:32.280]   There's this sort of huge bailout fund and it's going out to hotel chains and restaurant
[01:36:32.280 --> 01:36:33.920]   chains and my local...
[01:36:33.920 --> 01:36:34.920]   Yeah, absolutely.
[01:36:34.920 --> 01:36:36.320]   Chris Steakhouse is going to survive.
[01:36:36.320 --> 01:36:41.200]   Yeah, but at the same time, you know, my local grocery store is down on its upwards.
[01:36:41.200 --> 01:36:42.400]   They're keeping themselves going.
[01:36:42.400 --> 01:36:46.000]   But then there's a lot of anger about the fact that big stores like Walmart can stay
[01:36:46.000 --> 01:36:48.000]   open when smaller stores cannot.
[01:36:48.000 --> 01:36:50.080]   A lot of anger about that.
[01:36:50.080 --> 01:36:51.080]   And I understand that.
[01:36:51.080 --> 01:36:52.600]   Well, I think again, that's a state by state.
[01:36:52.600 --> 01:36:55.520]   Yeah, here in California, smaller stores are open.
[01:36:55.520 --> 01:36:58.040]   But in some states, they're not.
[01:36:58.040 --> 01:37:00.160]   And in some states, everything is open.
[01:37:00.160 --> 01:37:01.160]   This is the problem.
[01:37:01.160 --> 01:37:09.480]   If we were serious in this country about stopping this, you can't half-ass it.
[01:37:09.480 --> 01:37:14.320]   And I think the problem is, so if you want every state, if you want to go back to a state-led
[01:37:14.320 --> 01:37:15.840]   model, fine.
[01:37:15.840 --> 01:37:20.280]   Then every single state has to deploy its national guard and nobody can move in between
[01:37:20.280 --> 01:37:21.520]   the states.
[01:37:21.520 --> 01:37:27.640]   And then once you have proven a certain level of, you know, once you are able to show that
[01:37:27.640 --> 01:37:32.680]   your infection rate has significantly deteriorated, right, is significantly down.
[01:37:32.680 --> 01:37:35.960]   And there's some kind of standard.
[01:37:35.960 --> 01:37:41.160]   And also you can prove that people aren't going to infect others nearby, then you can
[01:37:41.160 --> 01:37:42.680]   reopen them.
[01:37:42.680 --> 01:37:50.880]   What we are doing in the United States is not coordinated enough, not good enough.
[01:37:50.880 --> 01:37:52.640]   And unfortunately, we're not New Zealand.
[01:37:52.640 --> 01:37:53.640]   We're connected.
[01:37:53.640 --> 01:37:56.080]   You know, every state is connected to other states.
[01:37:56.080 --> 01:38:00.160]   No, I mean, it seems like we're crashing the economy for no good reason because we're
[01:38:00.160 --> 01:38:03.920]   not actually doing the follow-up work that needs to be done.
[01:38:03.920 --> 01:38:05.680]   You know, lockdown is all well and good.
[01:38:05.680 --> 01:38:10.080]   And, you know, I don't know how it is on the East Coast, but in, you know, in the Bay
[01:38:10.080 --> 01:38:12.640]   area, they've actually been pretty good about the lockdown.
[01:38:12.640 --> 01:38:13.840]   But that doesn't really help us.
[01:38:13.840 --> 01:38:18.920]   And thus we actually track infections, isolate them and lock them down.
[01:38:18.920 --> 01:38:23.240]   And I've got to say also, there's been some pretty shameless show-boating by tech firms
[01:38:23.240 --> 01:38:28.640]   that, you know, Uber Eats going out or, you know, the food delivery places, like just
[01:38:28.640 --> 01:38:30.200]   remember your food delivery driver.
[01:38:30.200 --> 01:38:34.960]   It's like, no, in this kind of situation, you buy direct from the restaurant and you
[01:38:34.960 --> 01:38:36.800]   go and pick it up yourself.
[01:38:36.800 --> 01:38:38.400]   Why pay the middleman?
[01:38:38.400 --> 01:38:40.880]   You want the producer to say alive, not the middleman?
[01:38:40.880 --> 01:38:41.880]   I don't know.
[01:38:41.880 --> 01:38:42.880]   I think you should pay them both.
[01:38:42.880 --> 01:38:47.160]   Those people are probably a lot of, I would imagine, a lot of the people doing deliveries
[01:38:47.160 --> 01:38:51.240]   and Instacart shopping right now or had other jobs until recently.
[01:38:51.240 --> 01:38:52.640]   Oh, yes, certainly.
[01:38:52.640 --> 01:38:56.120]   But I mean, if it's a choice between keeping the middleman going and keeping the actual
[01:38:56.120 --> 01:38:58.480]   producer going, I pick the producer.
[01:38:58.480 --> 01:38:59.480]   Yeah.
[01:38:59.480 --> 01:39:00.480]   Yeah.
[01:39:00.480 --> 01:39:01.480]   Yeah.
[01:39:01.480 --> 01:39:05.960]   What, are there any industries that are going to see a benefit from this?
[01:39:05.960 --> 01:39:06.960]   Yes.
[01:39:06.960 --> 01:39:07.960]   Yes.
[01:39:07.960 --> 01:39:08.960]   Huge.
[01:39:08.960 --> 01:39:09.960]   Yeah.
[01:39:09.960 --> 01:39:10.960]   We've already started.
[01:39:10.960 --> 01:39:17.560]   So those working in the area of, so like within segments of AI, like computer vision automation,
[01:39:17.560 --> 01:39:21.040]   as it relates to automated small delivery.
[01:39:21.040 --> 01:39:29.520]   So neuro, Uber had a fairly successful pilot running of autonomous delivery vehicles in
[01:39:29.520 --> 01:39:36.040]   Pittsburgh until at one point it was, they were trapping people in wheelchairs out in
[01:39:36.040 --> 01:39:40.960]   the street because nobody had trained the system to recognize wheelchair as an object
[01:39:40.960 --> 01:39:41.960]   with human in it.
[01:39:41.960 --> 01:39:44.920]   Oh, I was going to say for fun or what?
[01:39:44.920 --> 01:39:48.680]   No, but you don't have people outside.
[01:39:48.680 --> 01:39:50.280]   So again, what would it take for X to be?
[01:39:50.280 --> 01:39:51.280]   Why?
[01:39:51.280 --> 01:39:53.160]   What would it take to see a faster acceleration?
[01:39:53.160 --> 01:39:55.920]   We've got the conditions right now for that to happen.
[01:39:55.920 --> 01:39:57.960]   I'm also seeing an influx.
[01:39:57.960 --> 01:40:03.800]   I was already, but I'm certainly now seeing an influx in synthetic biology investment.
[01:40:03.800 --> 01:40:13.320]   And if it's the case, if we can redefine biology as technology, which is a little controversial,
[01:40:13.320 --> 01:40:21.480]   but if we can think of ourselves as code that can be programmable, which we can now.
[01:40:21.480 --> 01:40:22.480]   So synthetic biology.
[01:40:22.480 --> 01:40:28.040]   Do you think Elizabeth Holmes is thinking, if I could only have held that?
[01:40:28.040 --> 01:40:31.640]   Well, I mean, so it's kind of no joke though.
[01:40:31.640 --> 01:40:36.840]   I mean, Amazon was already working at home based diagnostics on all kinds of systems and
[01:40:36.840 --> 01:40:37.840]   services.
[01:40:37.840 --> 01:40:43.080]   Google and Apple are also working on diagnostic tests outside of a lab.
[01:40:43.080 --> 01:40:47.720]   We are seeing the problem that occurs when you've got too much consolidation with basically
[01:40:47.720 --> 01:40:51.320]   two labs, Quest and LabCorps in the United States.
[01:40:51.320 --> 01:40:53.040]   The rest of the system becomes brittle.
[01:40:53.040 --> 01:41:00.200]   Again, these become accelerating factors for greater investment in home based diagnostics,
[01:41:00.200 --> 01:41:07.240]   home automation, single data records linking all of this together, all of the machine learning
[01:41:07.240 --> 01:41:12.680]   and the deep learning ecosystem and the AI used for scientific discovery.
[01:41:12.680 --> 01:41:14.880]   Things are all interlinked.
[01:41:14.880 --> 01:41:18.600]   So all of that is going to see an acceleration going forward.
[01:41:18.600 --> 01:41:23.440]   So if you're somebody who's like, hey, I'm an engineer and I kind of have some interest
[01:41:23.440 --> 01:41:27.240]   in these areas, those are good areas to be looking at right now.
[01:41:27.240 --> 01:41:32.640]   I am curious though, on the automated car front and on the automated drone front, the
[01:41:32.640 --> 01:41:35.400]   big players, yes, they're pushing forward on this.
[01:41:35.400 --> 01:41:42.640]   But they just seem to be sort of drying up VC funding for the younger players who are
[01:41:42.640 --> 01:41:45.520]   trying to get to scale first.
[01:41:45.520 --> 01:41:51.880]   So it seems as though a lot of startups are now really struggling for money.
[01:41:51.880 --> 01:41:54.440]   The VC funding seems to have cropped up.
[01:41:54.440 --> 01:42:00.920]   Do you think we're going to see a sort of consolidation around the first leaders or are
[01:42:00.920 --> 01:42:05.680]   we going to see some kind of spring of new ideas?
[01:42:05.680 --> 01:42:06.680]   Right.
[01:42:06.680 --> 01:42:10.880]   So nothing that I was just talking about is new.
[01:42:10.880 --> 01:42:15.960]   That's what I'm describing as an accelerating factor that wasn't present before.
[01:42:15.960 --> 01:42:17.520]   And with the, yeah.
[01:42:17.520 --> 01:42:22.040]   So synthetic biology has its roots back in 1992.
[01:42:22.040 --> 01:42:28.080]   And a lot of the Uber in Pittsburgh using little delivery drones is like, that's been
[01:42:28.080 --> 01:42:31.000]   around for a while.
[01:42:31.000 --> 01:42:38.960]   I think the problem is you've got, the VC community and the private equity community tend to be
[01:42:38.960 --> 01:42:43.040]   chasing the shiny a lot.
[01:42:43.040 --> 01:42:54.080]   And I just envision them chasing the new shiny, which is technology related to automating parts
[01:42:54.080 --> 01:42:56.200]   of logistics in the supply chain.
[01:42:56.200 --> 01:43:03.720]   So things have to do with like private 5G infrastructure within manufacturing plants and
[01:43:03.720 --> 01:43:09.360]   factories, scraping and using metadata that comes from the collaborative robotics and
[01:43:09.360 --> 01:43:10.960]   the 5G systems.
[01:43:10.960 --> 01:43:15.120]   I see money like potentially going in that direction, which has a feeder effect back
[01:43:15.120 --> 01:43:18.040]   to automated delivery vehicles.
[01:43:18.040 --> 01:43:24.600]   So maybe we don't see like a ton of new neuro competitors or whatever.
[01:43:24.600 --> 01:43:29.160]   And maybe some of that funding dries up, but the rest of the factors that lead into
[01:43:29.160 --> 01:43:31.840]   that ecosystem are moving forward.
[01:43:31.840 --> 01:43:33.680]   You wrote a book called The Big Nine.
[01:43:33.680 --> 01:43:39.800]   It's all about the big nine IBM, Alibaba, Google, Microsoft, Amazon, Facebook, Tencent,
[01:43:39.800 --> 01:43:41.560]   Baidu and Apple.
[01:43:41.560 --> 01:43:47.080]   Which of these companies are going to see huge growth because of COVID-19?
[01:43:47.080 --> 01:43:48.080]   Yeah.
[01:43:48.080 --> 01:43:53.560]   So this is one of those cases where I really wish.
[01:43:53.560 --> 01:43:58.880]   So that, that pragmatic scenario, the, the, the, the middle of the book has three scenarios,
[01:43:58.880 --> 01:44:02.560]   optimistic, neutral and catastrophic.
[01:44:02.560 --> 01:44:10.800]   That neutral scenario is starting to take shape, which is a concern for me because it
[01:44:10.800 --> 01:44:15.480]   leads down a dystopian superhighway.
[01:44:15.480 --> 01:44:16.480]   What is the neutral?
[01:44:16.480 --> 01:44:20.800]   What is the, what are the signature features of the neutral?
[01:44:20.800 --> 01:44:23.960]   That's much just sort of more pragmatic and pre-virus.
[01:44:23.960 --> 01:44:32.200]   I was studying, again, like tech companies intersecting with health, which has been going
[01:44:32.200 --> 01:44:34.200]   on for a while.
[01:44:34.200 --> 01:44:37.040]   Tech companies intersecting with farming and agriculture.
[01:44:37.040 --> 01:44:42.760]   In a lot of countries around the world, the government just stopped funding basic research,
[01:44:42.760 --> 01:44:44.360]   so somebody else had to step in.
[01:44:44.360 --> 01:44:50.000]   So I'm not against Google or Apple or Amazon necessarily building out the future infrastructures
[01:44:50.000 --> 01:44:51.680]   of healthcare.
[01:44:51.680 --> 01:44:55.440]   But I think if they're going to, if we're going to rely on them to do that, and now we see
[01:44:55.440 --> 01:45:00.760]   an accelerating factor in the virus, we have to demand traceability, accountability,
[01:45:00.760 --> 01:45:04.920]   explainability, transparency, you know, like transparency.
[01:45:04.920 --> 01:45:11.720]   And that needs to be in place, if not first, then while this was all happening, you know?
[01:45:11.720 --> 01:45:14.760]   What happens to the tech lash?
[01:45:14.760 --> 01:45:15.920]   I don't know that there is tech lash.
[01:45:15.920 --> 01:45:19.960]   I mean, I think that there is among my friends and probably your friends, but I think if
[01:45:19.960 --> 01:45:25.040]   you ask the average person, the average person doesn't actually know to the average person,
[01:45:25.040 --> 01:45:28.760]   Amazon is just the place where you get stuff from on the web.
[01:45:28.760 --> 01:45:36.040]   Even if there's a low hum of, oh, a privacy Google, I know a lot of people don't have
[01:45:36.040 --> 01:45:39.160]   Amazon echoes because they don't want Amazon to listen.
[01:45:39.160 --> 01:45:42.920]   But that low hum doesn't stop them from shopping at Amazon.
[01:45:42.920 --> 01:45:47.920]   No, and you know, Walmart is a technology company.
[01:45:47.920 --> 01:45:48.920]   Right.
[01:45:48.920 --> 01:45:50.840]   I don't talk about it that way.
[01:45:50.840 --> 01:45:54.600]   And I just, I don't see, I see people responding.
[01:45:54.600 --> 01:45:57.320]   So again, like, what are people really upset about?
[01:45:57.320 --> 01:45:59.560]   I don't think they're really upset about their privacy.
[01:45:59.560 --> 01:46:02.520]   I think they're upset about other things.
[01:46:02.520 --> 01:46:05.960]   And it manifests in like, let's go after Amazon.
[01:46:05.960 --> 01:46:07.600]   Let's go get Amazon, the bad guy.
[01:46:07.600 --> 01:46:10.320]   Let's go get Google, the bad guy.
[01:46:10.320 --> 01:46:16.000]   I think that people are actually angry about other things and they have absolutely no idea
[01:46:16.000 --> 01:46:19.280]   what these companies actually do.
[01:46:19.280 --> 01:46:22.280]   Yeah, that's kind of the thing that drives me nuts about this.
[01:46:22.280 --> 01:46:26.880]   It's just like, well, you know, it's just like, I just order this stuff and the rest
[01:46:26.880 --> 01:46:27.880]   of it.
[01:46:27.880 --> 01:46:31.080]   And it's just like, yeah, but if you actually look at the kind of data they're collecting
[01:46:31.080 --> 01:46:35.280]   and in many ways useful, but in many ways, deeply disturbing.
[01:46:35.280 --> 01:46:40.240]   And I think one of the things this has done is widened out an awful lot of technology.
[01:46:40.240 --> 01:46:44.920]   I mean, people used to order from, in my experience, people used to order from Amazon, but they
[01:46:44.920 --> 01:46:46.680]   still went down to the shops.
[01:46:46.680 --> 01:46:48.080]   Now they can't go down to the shops.
[01:46:48.080 --> 01:46:49.720]   It's up to Amazon to provide that.
[01:46:49.720 --> 01:46:53.560]   And I think that will lead to a bit more focus as to what these companies collecting and
[01:46:53.560 --> 01:46:55.520]   what they're using this data for.
[01:46:55.520 --> 01:46:58.880]   And particularly to come back to our earlier points about the healthcare apps, we're about
[01:46:58.880 --> 01:47:05.080]   to give out a huge amount of healthcare information to companies who we have no real guarantee
[01:47:05.080 --> 01:47:06.080]   they will use sensibly.
[01:47:06.080 --> 01:47:07.080]   Right.
[01:47:07.080 --> 01:47:10.160]   So again, I'm not necessarily against it.
[01:47:10.160 --> 01:47:12.400]   I just want to know what the plan is.
[01:47:12.400 --> 01:47:16.720]   And I want that plan to be articulated in a way that everybody can understand.
[01:47:16.720 --> 01:47:20.640]   I do think though that a lot of people are just going to say, we don't care.
[01:47:20.640 --> 01:47:22.120]   Just fix it.
[01:47:22.120 --> 01:47:23.680]   Just get rid of the virus.
[01:47:23.680 --> 01:47:25.160]   We're done with it.
[01:47:25.160 --> 01:47:26.720]   Whatever works.
[01:47:26.720 --> 01:47:28.720]   Don't you?
[01:47:28.720 --> 01:47:32.520]   Yeah, unfortunately, it's, but the problem is it's not going to happen that way.
[01:47:32.520 --> 01:47:37.400]   Boring a super perfect vaccine, which comes out amazingly in the next nine months, we're
[01:47:37.400 --> 01:47:40.120]   going to put up with a year or two of this, as I said, I understand that.
[01:47:40.120 --> 01:47:44.280]   I'm just saying, I think that the backlash is tick lashes turned the other way.
[01:47:44.280 --> 01:47:48.920]   And now people are mad at privacy advocates like you Ian for slowing it down.
[01:47:48.920 --> 01:47:49.920]   Stop.
[01:47:49.920 --> 01:47:50.920]   We want this.
[01:47:50.920 --> 01:47:52.840]   Let's let's bring it on.
[01:47:52.840 --> 01:47:55.600]   You will think of.
[01:47:55.600 --> 01:47:59.000]   I think the backlash is going against the privacy advocates.
[01:47:59.000 --> 01:48:01.280]   They're saying, ah, we've heard it all before.
[01:48:01.280 --> 01:48:06.800]   Look, we, we're going to give up some privacy, but we need to because we got to get better.
[01:48:06.800 --> 01:48:08.000]   We need our jobs back.
[01:48:08.000 --> 01:48:09.800]   We need to be able to go to the mall.
[01:48:09.800 --> 01:48:12.480]   It's going to happen to the football season in September.
[01:48:12.480 --> 01:48:14.000]   All of that stuff.
[01:48:14.000 --> 01:48:19.680]   There, no person should have any, any belief that they are private.
[01:48:19.680 --> 01:48:22.640]   There is no more such, there's no such thing as privacy.
[01:48:22.640 --> 01:48:24.240]   Privacy has been dead.
[01:48:24.240 --> 01:48:26.760]   I mean, dead for a while.
[01:48:26.760 --> 01:48:33.640]   So the challenge going forward is to be, maybe to have sort of a better sense of understand,
[01:48:33.640 --> 01:48:35.320]   like a better digital street smarts, right?
[01:48:35.320 --> 01:48:39.840]   A better understanding of what data are being scraped under what circumstances.
[01:48:39.840 --> 01:48:43.400]   And there has to be some accountability.
[01:48:43.400 --> 01:48:48.880]   These, you know, the companies have to explain which data brokers are, who they're using,
[01:48:48.880 --> 01:48:50.760]   how the data are being transmitted.
[01:48:50.760 --> 01:48:53.080]   Because it's, you know, it's the obvious stuff.
[01:48:53.080 --> 01:48:57.600]   Like people sort of get like that their faces are being recognized.
[01:48:57.600 --> 01:49:03.920]   They have absolutely no conception of what ambient data and ambient computing are.
[01:49:03.920 --> 01:49:06.160]   And metadata, right?
[01:49:06.160 --> 01:49:12.160]   All of the other sort of digital emissions that are coming from us and from our homes
[01:49:12.160 --> 01:49:15.600]   and from our offices and all of it.
[01:49:15.600 --> 01:49:20.360]   I mean, you reminded me of a very, the famous phrase from "Sunscot Manelia", which was,
[01:49:20.360 --> 01:49:22.680]   you know, privacy is dead, get used to it.
[01:49:22.680 --> 01:49:26.800]   And then when the Edward Snowden revelations came out, he tweeted out, it's just like,
[01:49:26.800 --> 01:49:32.080]   wow, I had no idea quite how bad, how correct I was.
[01:49:32.080 --> 01:49:37.080]   Yeah, I mean, it's kind of like, okay, we always face, okay, some of us use Facebook
[01:49:37.080 --> 01:49:41.880]   and you don't actually put your personal, I mean, you put personal feelings on Facebook,
[01:49:41.880 --> 01:49:45.560]   but you don't, you've got to be careful about what you put on there because otherwise that
[01:49:45.560 --> 01:49:49.920]   data is going to be sold off and it's the same with Twitter and the same with any form
[01:49:49.920 --> 01:49:51.280]   of social media.
[01:49:51.280 --> 01:49:53.720]   You have to be careful about what data you're putting on there.
[01:49:53.720 --> 01:49:54.720]   I agree with you.
[01:49:54.720 --> 01:49:57.000]   Total privacy is dead and gone.
[01:49:57.000 --> 01:49:59.320]   But I'm willing to fight the regard action.
[01:49:59.320 --> 01:50:02.880]   You know, it's just, I'm not willing to handle this stuff over.
[01:50:02.880 --> 01:50:06.400]   It's like all these Facebook polls for, oh, what are your 10 favorite albums?
[01:50:06.400 --> 01:50:10.040]   Yeah, I'm really going to tell Zuckerberg what my 10 favorite albums are.
[01:50:10.040 --> 01:50:12.200]   Some of them are bammed out with, you know.
[01:50:12.200 --> 01:50:18.320]   Well, but so that's the text based, that's the obvious information I'm talking about.
[01:50:18.320 --> 01:50:26.000]   So snap in 2015, 16 or so filed a patent on, it was a computer vision patent that would
[01:50:26.000 --> 01:50:31.760]   allow it to recognize any object in the frame of the photo that somebody was sharing.
[01:50:31.760 --> 01:50:37.760]   And then they were developing an auction based ad server to sell you ads against whatever
[01:50:37.760 --> 01:50:39.800]   was in that frame.
[01:50:39.800 --> 01:50:47.760]   And this is, so like, I understand that anytime I'm transmitting data of any kind that there's,
[01:50:47.760 --> 01:50:50.440]   you know, it's multifaceted.
[01:50:50.440 --> 01:50:53.240]   My point is that the average person doesn't recognize that.
[01:50:53.240 --> 01:50:57.160]   And so it's not just your face that's being scraped.
[01:50:57.160 --> 01:51:01.760]   It's all of the ambient data around you that others can use, right?
[01:51:01.760 --> 01:51:07.160]   Well, no, I mean, I had to ask my mother-in-law to stop doing these bloody Facebook quizzes
[01:51:07.160 --> 01:51:12.600]   because basically she's handing over my entire account to a third party and we need to crack
[01:51:12.600 --> 01:51:13.800]   down on this.
[01:51:13.800 --> 01:51:19.280]   But at the same time, yeah, they're the ambient data that there's just so much that can be
[01:51:19.280 --> 01:51:24.480]   logged out, but I'm still not quite convinced about the advertising argument because to
[01:51:24.480 --> 01:51:27.840]   use your example, yes, you can see these things in the background.
[01:51:27.840 --> 01:51:29.400]   You've already got them.
[01:51:29.400 --> 01:51:31.720]   You know, I'm so sick of order.
[01:51:31.720 --> 01:51:32.720]   I find you.
[01:51:32.720 --> 01:51:34.720]   And they're getting spammed out by.
[01:51:34.720 --> 01:51:37.200]   That's why the behavioral metrics.
[01:51:37.200 --> 01:51:39.640]   So this is where the other piece of this comes in.
[01:51:39.640 --> 01:51:43.640]   So if somebody has a profile of you and there's enough P.I.
[01:51:43.640 --> 01:51:48.480]   You're personally identifiable information that the breadcrumbs that follow you around,
[01:51:48.480 --> 01:51:54.600]   those old books sitting on your shelf could potentially tell an algorithm that you then
[01:51:54.600 --> 01:51:57.280]   would be interested in Glen Live It something.
[01:51:57.280 --> 01:51:58.280]   I don't know.
[01:51:58.280 --> 01:51:59.280]   Right?
[01:51:59.280 --> 01:52:00.280]   And so...
[01:52:00.280 --> 01:52:03.120]   We have zoomed in on the books to read the titles.
[01:52:03.120 --> 01:52:04.120]   So that we...
[01:52:04.120 --> 01:52:05.120]   But you know what?
[01:52:05.120 --> 01:52:06.120]   They're not looking at the books.
[01:52:06.120 --> 01:52:13.720]   You're going to get a slew of ads for Martini Shakers, for Jin, for, you know, fancy trips
[01:52:13.720 --> 01:52:15.440]   to Caribbean islands.
[01:52:15.440 --> 01:52:19.040]   You're clearly telling the world something very different with you.
[01:52:19.040 --> 01:52:21.480]   Well, but it's not just the average...
[01:52:21.480 --> 01:52:24.080]   Let's think about content bigger and broader.
[01:52:24.080 --> 01:52:25.480]   So it's not just advertisements.
[01:52:25.480 --> 01:52:27.720]   It's also what shows up in your news feed.
[01:52:27.720 --> 01:52:33.800]   We and the amazing audience listening to us are all discerning consumers of news.
[01:52:33.800 --> 01:52:35.400]   But that is not true of everybody.
[01:52:35.400 --> 01:52:40.840]   And in this age of algorithmic determinism, where we don't have somebody human thinking
[01:52:40.840 --> 01:52:46.880]   about the factors that go into making these assessments, you start to get led down digital
[01:52:46.880 --> 01:52:47.880]   rabbit holes.
[01:52:47.880 --> 01:52:48.880]   You should never...
[01:52:48.880 --> 01:52:49.880]   You should never...
[01:52:49.880 --> 01:52:50.880]   You should never...
[01:52:50.880 --> 01:52:54.840]   Never follow the podcast recommendations that you are offered.
[01:52:54.840 --> 01:52:56.880]   You should always just listen to us.
[01:52:56.880 --> 01:53:00.240]   Because there's no algorithms on this end.
[01:53:00.240 --> 01:53:03.160]   We're just throwing stuff out.
[01:53:03.160 --> 01:53:05.240]   Home and it sticks.
[01:53:05.240 --> 01:53:06.680]   There's no news feed here.
[01:53:06.680 --> 01:53:09.640]   There's no clever algorithm behind this one.
[01:53:09.640 --> 01:53:12.040]   You talk about the books behind me.
[01:53:12.040 --> 01:53:13.640]   Yes, I'm reasonably aware.
[01:53:13.640 --> 01:53:18.120]   So I went through that bookcase and I took out all the embarrassing the John Grisham's
[01:53:18.120 --> 01:53:19.120]   and the Wilbur Smith.
[01:53:19.120 --> 01:53:24.120]   Oh, you mean reputation polishing, Mr. Ian Thompson.
[01:53:24.120 --> 01:53:25.680]   Oh, come on.
[01:53:25.680 --> 01:53:26.680]   Right.
[01:53:26.680 --> 01:53:28.560]   No one else does that on the planet.
[01:53:28.560 --> 01:53:29.560]   I will say the...
[01:53:29.560 --> 01:53:31.000]   Hang on the other side.
[01:53:31.000 --> 01:53:32.320]   The complete whole analysis.
[01:53:32.320 --> 01:53:34.760]   I highly recommend that he died now.
[01:53:34.760 --> 01:53:35.760]   Yeah, very good.
[01:53:35.760 --> 01:53:36.760]   And the guy was a genius.
[01:53:36.760 --> 01:53:39.440]   But seriously, you've got to be aware of your surroundings.
[01:53:39.440 --> 01:53:42.040]   You've got to be aware of what day you're giving.
[01:53:42.040 --> 01:53:45.720]   But the fact is that most people are.
[01:53:45.720 --> 01:53:49.080]   And I don't think they need to be.
[01:53:49.080 --> 01:53:50.640]   I don't think they want to be.
[01:53:50.640 --> 01:53:54.680]   I think when they hear people talking about it, they go, "Oh, you're just a bunch of cranks."
[01:53:54.680 --> 01:53:58.240]   It's the other side of 5G's causing COVID.
[01:53:58.240 --> 01:53:59.240]   It's just more...
[01:53:59.240 --> 01:54:00.240]   Oh, don't even get...
[01:54:00.240 --> 01:54:01.240]   It's just more noise.
[01:54:01.240 --> 01:54:02.240]   I think it's just more noise.
[01:54:02.240 --> 01:54:03.240]   Even...
[01:54:03.240 --> 01:54:09.400]   Like of all the crazy nonsense that I've heard, linking 5G to COVID is probably...
[01:54:09.400 --> 01:54:12.080]   The strangest and like the dumst.
[01:54:12.080 --> 01:54:18.840]   I think what you have to do is you have to think, "What is the psychological reason for
[01:54:18.840 --> 01:54:19.840]   that?"
[01:54:19.840 --> 01:54:21.200]   And it's like Ned Ludd.
[01:54:21.200 --> 01:54:22.200]   It's...
[01:54:22.200 --> 01:54:25.560]   You know, the modern times are causing the COVID.
[01:54:25.560 --> 01:54:28.000]   What's the most modern thing in my neighborhood?
[01:54:28.000 --> 01:54:29.800]   Oh, it's that 5G tower.
[01:54:29.800 --> 01:54:35.240]   Yeah, but I mean, people in the UK, we've had 25G towers burned down in the UK at the
[01:54:35.240 --> 01:54:36.240]   moment.
[01:54:36.240 --> 01:54:38.240]   I mean, it's just the right...
[01:54:38.240 --> 01:54:40.240]   But there's a psychological thing.
[01:54:40.240 --> 01:54:41.240]   It's not...
[01:54:41.240 --> 01:54:42.240]   You know what?
[01:54:42.240 --> 01:54:43.240]   I don't feel like...
[01:54:43.240 --> 01:54:44.240]   People are...
[01:54:44.240 --> 01:54:45.240]   You think people are rational.
[01:54:45.240 --> 01:54:46.240]   People are not rational.
[01:54:46.240 --> 01:54:51.840]   They act unemotionally and then use reason to justify the emotional thing that they chose.
[01:54:51.840 --> 01:54:55.640]   I fear your voice ahead may be right on that one, but I've got to say, when you've
[01:54:55.640 --> 01:54:59.960]   got a bunch of de-list celebrities going, "Oh, well, 5G might be the cause."
[01:54:59.960 --> 01:55:01.240]   No, it's bloody...
[01:55:01.240 --> 01:55:02.240]   It's just like...
[01:55:02.240 --> 01:55:05.960]   You know, it's like saying can opens and a cause of botulism.
[01:55:05.960 --> 01:55:06.960]   It's...
[01:55:06.960 --> 01:55:08.360]   Obviously nutty.
[01:55:08.360 --> 01:55:12.640]   I wonder even if the people who are destroying the towers feel the same way, but it's just
[01:55:12.640 --> 01:55:14.280]   so satisfying.
[01:55:14.280 --> 01:55:20.240]   We see it at first, I thought it was like kids under lockdown go out, do a bit of vandalism,
[01:55:20.240 --> 01:55:21.240]   make yourself feel better.
[01:55:21.240 --> 01:55:23.840]   Fair enough, it's a British tradition.
[01:55:23.840 --> 01:55:24.840]   When you actually get...
[01:55:24.840 --> 01:55:27.720]   Because not happening in other countries as much, is it?
[01:55:27.720 --> 01:55:30.320]   I don't think we've had any 5G vandalism here.
[01:55:30.320 --> 01:55:33.600]   A few cases in Europe, but other than that, fine.
[01:55:33.600 --> 01:55:40.160]   But if 5G really was the cause of this, you'd expect the highest outbreaks of coronavirus
[01:55:40.160 --> 01:55:43.720]   in 5G areas like Finland where they've had one day.
[01:55:43.720 --> 01:55:45.320]   It's been nonsense.
[01:55:45.320 --> 01:55:47.320]   It's obviously nonsense.
[01:55:47.320 --> 01:55:50.080]   So I always like to figure out why.
[01:55:50.080 --> 01:55:55.680]   So what is it that people are seeking in this?
[01:55:55.680 --> 01:55:58.480]   And I think it comes back to certainty.
[01:55:58.480 --> 01:56:04.000]   So there's a problem with having too much data.
[01:56:04.000 --> 01:56:07.600]   Sometimes too much data is bad and sometimes not enough data is bad.
[01:56:07.600 --> 01:56:11.800]   Too much information is sometimes bad, not enough information is sometimes bad.
[01:56:11.800 --> 01:56:14.520]   That is the current situation that we are living in.
[01:56:14.520 --> 01:56:21.600]   And I think with 5G, which is kind of like difficult to actually understand.
[01:56:21.600 --> 01:56:25.280]   I think the problem is you have some people with too much of the wrong information and
[01:56:25.280 --> 01:56:28.680]   a whole bunch of people with no information.
[01:56:28.680 --> 01:56:32.480]   There's no center ground where cooler heads prevail.
[01:56:32.480 --> 01:56:33.480]   Yeah.
[01:56:33.480 --> 01:56:36.280]   And also cooler heads don't make great headlines.
[01:56:36.280 --> 01:56:43.040]   I mean, there's a British paper called the National Paper called the Daily Express, which
[01:56:43.040 --> 01:56:48.480]   used to be a very good paper, but it's now all about asteroids and conspiracy theories
[01:56:48.480 --> 01:56:49.920]   and the rest of it.
[01:56:49.920 --> 01:56:51.680]   My grand used to read that.
[01:56:51.680 --> 01:56:54.400]   I took it as a gospel.
[01:56:54.400 --> 01:56:57.800]   We do have this information overload that we're, you know, and people who decide, well,
[01:56:57.800 --> 01:57:04.360]   I read it on the internet and we need to teach media savvy to people just to with anything
[01:57:04.360 --> 01:57:06.320]   that you read, who wrote it?
[01:57:06.320 --> 01:57:08.600]   Why did they wrote it and who paid for it to be written?
[01:57:08.600 --> 01:57:10.800]   Those are the three key questions you have to ask.
[01:57:10.800 --> 01:57:11.800]   All right.
[01:57:11.800 --> 01:57:12.800]   I want to make two columns.
[01:57:12.800 --> 01:57:14.840]   It's like tired and wired, but it's not.
[01:57:14.840 --> 01:57:20.360]   It's like pre-COVID post-COVID or no.
[01:57:20.360 --> 01:57:23.240]   So for instance, malls, malls, pre-COVID.
[01:57:23.240 --> 01:57:25.840]   They were malls post-COVID.
[01:57:25.840 --> 01:57:26.840]   No malls.
[01:57:26.840 --> 01:57:27.840]   Amazon.
[01:57:27.840 --> 01:57:28.840]   Amazon, right?
[01:57:28.840 --> 01:57:29.840]   Yeah.
[01:57:29.840 --> 01:57:30.840]   Yeah.
[01:57:30.840 --> 01:57:34.560]   There's like a hundred shopping malls that are due to close over the next month or so.
[01:57:34.560 --> 01:57:35.560]   The numbers are stacked.
[01:57:35.560 --> 01:57:37.120]   And they're not coming back.
[01:57:37.120 --> 01:57:38.120]   No, no, no.
[01:57:38.120 --> 01:57:39.120]   Like permanently.
[01:57:39.120 --> 01:57:40.120]   No.
[01:57:40.120 --> 01:57:41.120]   Yeah.
[01:57:41.120 --> 01:57:44.040]   Sad to say a lot of restaurants are gone.
[01:57:44.040 --> 01:57:45.520]   Won't come back, right?
[01:57:45.520 --> 01:57:49.320]   Because they're out of business.
[01:57:49.320 --> 01:57:54.560]   Something that was hanging by a fragile thread, that thread's being cut right now.
[01:57:54.560 --> 01:57:55.560]   Right?
[01:57:55.560 --> 01:57:56.560]   And that's the stuff.
[01:57:56.560 --> 01:57:57.560]   Yeah.
[01:57:57.560 --> 01:57:58.560]   Go ahead.
[01:57:58.560 --> 01:58:04.800]   No, I was just going to say, I mean, our local Indian place is, they've seen their, you know,
[01:58:04.800 --> 01:58:06.280]   their techings dropped by 80%.
[01:58:06.280 --> 01:58:08.440]   It's the same with every other restaurant.
[01:58:08.440 --> 01:58:09.440]   I have to.
[01:58:09.440 --> 01:58:14.400]   An apology for British listeners at the moment, but I went to get my, this may cause amusement.
[01:58:14.400 --> 01:58:17.640]   I meant to get my growler filled at the local pub.
[01:58:17.640 --> 01:58:22.520]   Now for American listeners growler, it's the bottle that you fill your beer up with.
[01:58:22.520 --> 01:58:23.520]   Yeah.
[01:58:23.520 --> 01:58:26.120]   British listeners, it's something rather rude.
[01:58:26.120 --> 01:58:34.360]   So, you took his growler to a recently did, there's no way they should put that.
[01:58:34.360 --> 01:58:35.360]   Yeah.
[01:58:35.360 --> 01:58:36.840]   No, the, I'm doing his bit.
[01:58:36.840 --> 01:58:38.920]   A lot of years going stale in the barrels.
[01:58:38.920 --> 01:58:43.800]   We need you to drink more beer in a local brew pub sells brands sells growlers for two
[01:58:43.800 --> 01:58:45.320]   hours a day.
[01:58:45.320 --> 01:58:50.440]   So, to this may cause amusement for British friends, but a nice young man filled my growler
[01:58:50.440 --> 01:58:52.560]   with his fat type.
[01:58:52.560 --> 01:58:57.040]   Drive in movies, making a comeback, right?
[01:58:57.040 --> 01:58:58.520]   We can also know cars.
[01:58:58.520 --> 01:58:59.520]   Yeah.
[01:58:59.520 --> 01:59:00.520]   Rolled up the windows.
[01:59:00.520 --> 01:59:02.000]   Oh, that's kind of cool.
[01:59:02.000 --> 01:59:03.000]   Yeah.
[01:59:03.000 --> 01:59:07.920]   Remember in 1965, I'm old enough to remember going to the US, the New York World's Fair
[01:59:07.920 --> 01:59:10.120]   and AT&T had a video phone.
[01:59:10.120 --> 01:59:15.160]   I said, you know, by 1970, we'll all be using video phones.
[01:59:15.160 --> 01:59:21.080]   And I remember in 2001, in 1977, the movie 2001, he's on the moon.
[01:59:21.080 --> 01:59:26.080]   He's talking to his daughter and his wife on earth via a video phone and a video phone
[01:59:26.080 --> 01:59:27.080]   booth.
[01:59:27.080 --> 01:59:28.080]   And it never took off.
[01:59:28.080 --> 01:59:31.520]   All of a sudden, we're all making video calls.
[01:59:31.520 --> 01:59:36.480]   I think this might have been the impetus to finally get us over the hump because we didn't
[01:59:36.480 --> 01:59:37.480]   want to.
[01:59:37.480 --> 01:59:39.320]   I think people didn't want to, but now they're making video calls.
[01:59:39.320 --> 01:59:40.320]   And I don't think we're going to stop.
[01:59:40.320 --> 01:59:44.080]   I think that this is, that's something that's changed dramatically.
[01:59:44.080 --> 01:59:48.520]   I mean, I think within the IT industry as well, one of the big, big changes is going
[01:59:48.520 --> 01:59:51.320]   to be conferences because conferences in this year.
[01:59:51.320 --> 01:59:52.320]   It's over.
[01:59:52.320 --> 01:59:53.320]   It's over.
[01:59:53.320 --> 01:59:57.320]   I'm not going to CES ever, ever, ever, ever again.
[01:59:57.320 --> 02:00:00.120]   I mean, CES at the end of the year.
[02:00:00.120 --> 02:00:01.120]   Okay.
[02:00:01.120 --> 02:00:03.120]   Sorry, Amy, please.
[02:00:03.120 --> 02:00:10.960]   So conferences are not over for a couple of very practical reasons.
[02:00:10.960 --> 02:00:14.600]   The first of which is that people are, humans are social.
[02:00:14.600 --> 02:00:20.240]   So there is no way people are going to opt for no gathering or a virtual gathering over
[02:00:20.240 --> 02:00:22.120]   an in person gathering.
[02:00:22.120 --> 02:00:26.680]   It might take a year for us to get back into the swing of things.
[02:00:26.680 --> 02:00:31.360]   But to me, this is a little analogous to after 9/11 when everybody said nobody's going
[02:00:31.360 --> 02:00:32.880]   to fly again.
[02:00:32.880 --> 02:00:36.440]   You know, we just, we have to sort of wait it out to have the new protocols and for people
[02:00:36.440 --> 02:00:37.440]   to feel confident.
[02:00:37.440 --> 02:00:39.480]   And then everybody's going to be back on planes.
[02:00:39.480 --> 02:00:43.240]   Is anybody going to go on a cruise ship ever again?
[02:00:43.240 --> 02:00:44.240]   That's you.
[02:00:44.240 --> 02:00:45.240]   You're the one who cruises.
[02:00:45.240 --> 02:00:46.240]   Yes.
[02:00:46.240 --> 02:00:48.040]   You're the last.
[02:00:48.040 --> 02:00:51.800]   I was talking yesterday with Johnny Jedd, our travel guy.
[02:00:51.800 --> 02:00:57.640]   And his thesis was you're not going to see widespread air travel or cruise travel or
[02:00:57.640 --> 02:01:02.400]   maybe even conferences until we have fast universal testing.
[02:01:02.400 --> 02:01:10.440]   So that you can go to the airport five minutes early, get a swab or spit in a vial, get tested.
[02:01:10.440 --> 02:01:11.960]   If you pass, you can board the plane.
[02:01:11.960 --> 02:01:12.960]   If not, you can't.
[02:01:12.960 --> 02:01:18.160]   Yeah, you see, at the moment, it's looking like countries are going down the route with,
[02:01:18.160 --> 02:01:22.360]   if you come into our country, you spend 14 days in quarantine before you're allowed out
[02:01:22.360 --> 02:01:23.360]   into the general population.
[02:01:23.360 --> 02:01:25.760]   That's not going to work for a touristic travel.
[02:01:25.760 --> 02:01:26.920]   Well, no, exactly.
[02:01:26.920 --> 02:01:30.760]   Even Britain's just introduced this, which they held off on it for a long while.
[02:01:30.760 --> 02:01:35.000]   Israel was straight in there with a bullet, but that is going to cripple the air industry
[02:01:35.000 --> 02:01:36.000]   for a while.
[02:01:36.000 --> 02:01:39.160]   But yeah, I think for this year, conferences are over.
[02:01:39.160 --> 02:01:45.800]   It may come back, but until we get away of either accurately testing or giving the golden
[02:01:45.800 --> 02:01:50.000]   passport to people, then mass conferences like CSA.
[02:01:50.000 --> 02:01:51.920]   CES is over.
[02:01:51.920 --> 02:01:52.920]   I'm sorry.
[02:01:52.920 --> 02:01:53.920]   There will never be another CES.
[02:01:53.920 --> 02:01:54.920]   I'm going on record.
[02:01:54.920 --> 02:01:59.400]   But I mean, I'm a security journalist and I'm, you know, definitely.
[02:01:59.400 --> 02:02:00.400]   We've got a black hat.
[02:02:00.400 --> 02:02:01.400]   Absolutely.
[02:02:01.400 --> 02:02:02.400]   That'll be back.
[02:02:02.400 --> 02:02:03.400]   Yeah.
[02:02:03.400 --> 02:02:05.240]   South by Southwest, Amy, was going to speak there this year.
[02:02:05.240 --> 02:02:06.240]   That'll be back.
[02:02:06.240 --> 02:02:07.240]   Just really.
[02:02:07.240 --> 02:02:08.240]   Just really.
[02:02:08.240 --> 02:02:09.240]   Sorry, go.
[02:02:09.240 --> 02:02:10.240]   Amy.
[02:02:10.240 --> 02:02:11.240]   Yeah.
[02:02:11.240 --> 02:02:13.440]   So we have a habit.
[02:02:13.440 --> 02:02:15.040]   We have a cognitive bias.
[02:02:15.040 --> 02:02:19.640]   When we think about the future, we tend to take signals in the present and amplify them.
[02:02:19.640 --> 02:02:24.320]   So the everywhere in the world, there's a 14 day quarantine period.
[02:02:24.320 --> 02:02:29.360]   How would this ever work out for conferences assumes that, you know, no other.
[02:02:29.360 --> 02:02:33.920]   Variable, external or internal will change between now and some point in the future.
[02:02:33.920 --> 02:02:40.120]   So I just, I would caution against, you know, the inclination is to say the future will
[02:02:40.120 --> 02:02:42.120]   be like today, but much more.
[02:02:42.120 --> 02:02:43.120]   I understand.
[02:02:43.120 --> 02:02:44.120]   That's why.
[02:02:44.120 --> 02:02:47.600]   The difference between this and 9/11 is that was an incident, a point in time.
[02:02:47.600 --> 02:02:53.400]   This is a long term and I think we're all going to have some significant PTSD from this.
[02:02:53.400 --> 02:02:55.680]   And I think people are going to become germophobic.
[02:02:55.680 --> 02:03:02.800]   At least my generation, the generation C will be germophobic to the degree that there are
[02:03:02.800 --> 02:03:07.240]   certain activities as much as we might want to do them that will be reluctant to do.
[02:03:07.240 --> 02:03:10.680]   I wonder if movie theaters are going to survive.
[02:03:10.680 --> 02:03:11.840]   Well, yeah.
[02:03:11.840 --> 02:03:13.600]   There are some.
[02:03:13.600 --> 02:03:16.120]   So what would it take for us to answer that question?
[02:03:16.120 --> 02:03:18.400]   What's the revenue?
[02:03:18.400 --> 02:03:21.520]   And AMC is about to go out of business period.
[02:03:21.520 --> 02:03:22.520]   Right.
[02:03:22.520 --> 02:03:27.000]   So how can they sustain in the short term?
[02:03:27.000 --> 02:03:31.400]   You know, my job really is about how do you sustain your operations in the short term while
[02:03:31.400 --> 02:03:34.440]   still planning on longer term growth.
[02:03:34.440 --> 02:03:40.000]   And the honest answer is there are a lot of companies, some industries that are going
[02:03:40.000 --> 02:03:42.760]   to sort of have to go on hospice.
[02:03:42.760 --> 02:03:44.800]   There's a such thing as a hospice for business.
[02:03:44.800 --> 02:03:45.800]   Exactly.
[02:03:45.800 --> 02:03:46.800]   Right.
[02:03:46.800 --> 02:03:47.800]   They're on hospice.
[02:03:47.800 --> 02:03:51.040]   I wish you had like a hospice for business because that would allow people to say, listen,
[02:03:51.040 --> 02:03:56.200]   we're in a situation where three months from now we're probably not going to exist.
[02:03:56.200 --> 02:03:57.600]   We acknowledge this.
[02:03:57.600 --> 02:04:00.360]   So it's not like it going out of business sale.
[02:04:00.360 --> 02:04:02.880]   It's a smart way to wind down a business.
[02:04:02.880 --> 02:04:07.720]   And I think that would probably be, you know, there are ways to do that and provide tax incentives
[02:04:07.720 --> 02:04:09.240]   and to do everything above board.
[02:04:09.240 --> 02:04:14.560]   And that would be a smart thing for a lot of companies, countries to start doing right
[02:04:14.560 --> 02:04:18.880]   now is to figure out what does a hospice environment.
[02:04:18.880 --> 02:04:22.120]   What would a hospice plan for business like?
[02:04:22.120 --> 02:04:24.480]   Some of it also is how vital that business is.
[02:04:24.480 --> 02:04:28.480]   I think sports will come back because we just really love sports.
[02:04:28.480 --> 02:04:30.680]   But I think we can live without movie theaters.
[02:04:30.680 --> 02:04:32.080]   We can live without malls.
[02:04:32.080 --> 02:04:33.800]   They're less essential.
[02:04:33.800 --> 02:04:39.120]   You know, I mean, honestly, I think we've got to come to a decision about what we are
[02:04:39.120 --> 02:04:41.320]   prepared to look after.
[02:04:41.320 --> 02:04:44.560]   And it's like, I love my local cinema.
[02:04:44.560 --> 02:04:45.720]   You know, love going there.
[02:04:45.720 --> 02:04:46.720]   The rest of it.
[02:04:46.720 --> 02:04:48.240]   That's fine.
[02:04:48.240 --> 02:04:49.240]   We're a sports team.
[02:04:49.240 --> 02:04:50.240]   Yep.
[02:04:50.240 --> 02:04:51.240]   Love that.
[02:04:51.240 --> 02:04:54.680]   But why on earth we're bailing out companies which do very little for us and not keeping
[02:04:54.680 --> 02:04:58.720]   the local companies which adds so much to local life.
[02:04:58.720 --> 02:05:06.720]   I mean, Petaluma, if it was just a chain place, would be a terrible place to live.
[02:05:06.720 --> 02:05:07.720]   Let's take a break.
[02:05:07.720 --> 02:05:12.080]   And I'm going to, this might be a little whistling past the graveyard, but I think we should talk
[02:05:12.080 --> 02:05:16.320]   about Apple selling Max with arm chips next year.
[02:05:16.320 --> 02:05:18.680]   But there's no conference.
[02:05:18.680 --> 02:05:22.120]   It feels so inconsequential, doesn't it?
[02:05:22.120 --> 02:05:27.080]   Well, you know, honestly, it's going to be a big change and Google's plans as well.
[02:05:27.080 --> 02:05:29.760]   So you know, you never know how this is going to work out.
[02:05:29.760 --> 02:05:30.760]   Yeah.
[02:05:30.760 --> 02:05:31.720]   Now, and your point is well taken.
[02:05:31.720 --> 02:05:37.280]   Obviously, Amy, it's very easy to have that, you know, that bubble be in that temporary
[02:05:37.280 --> 02:05:41.560]   present tense bubble and say, oh, can never be any different than this.
[02:05:41.560 --> 02:05:45.520]   But there are some things I feel like maybe we're better off without.
[02:05:45.520 --> 02:05:51.560]   I agree, but I've spent a lot of time in the past couple of weeks on the Internet Archives
[02:05:51.560 --> 02:05:56.560]   magazine section of the site where you can go through and they've got this amazing archive
[02:05:56.560 --> 02:05:59.720]   of old computing magazines like Bite.
[02:05:59.720 --> 02:06:02.200]   Oh, I used to write for Bite.
[02:06:02.200 --> 02:06:07.000]   So the whole archive is there and it's really interesting.
[02:06:07.000 --> 02:06:11.080]   I mean, the most recent, it was reading a popular mechanics from the 50s.
[02:06:11.080 --> 02:06:16.280]   And one of the big stories was about, from 1954, was about how there weren't enough computer
[02:06:16.280 --> 02:06:17.680]   engineers.
[02:06:17.680 --> 02:06:21.800]   Like that story has always been true in every single decade.
[02:06:21.800 --> 02:06:23.240]   It's true right now.
[02:06:23.240 --> 02:06:25.200]   So what does that tell us?
[02:06:25.200 --> 02:06:32.240]   That tells us that even when there are, and that was cold war, we have to make a decision
[02:06:32.240 --> 02:06:35.800]   ultimately to create the future that we want to live in.
[02:06:35.800 --> 02:06:40.880]   If we don't actively make a decision, then things kind of just happen.
[02:06:40.880 --> 02:06:44.960]   So yeah, there's a ton of optimistic framings out there.
[02:06:44.960 --> 02:06:49.920]   I mean, I really believe that we could wind up on the other side of this five years from
[02:06:49.920 --> 02:06:55.680]   now in many ways better than when we started, but we have to decide that that's what we
[02:06:55.680 --> 02:06:56.760]   want to do.
[02:06:56.760 --> 02:07:05.200]   And then we have to have leadership and we all have to march together in the same direction.
[02:07:05.200 --> 02:07:08.480]   What's the future of leadership in the new world?
[02:07:08.480 --> 02:07:10.800]   Yeah, well, I don't know.
[02:07:10.800 --> 02:07:16.520]   I mean, speaking from the UK perspective and possibly also from the US, then we definitely
[02:07:16.520 --> 02:07:21.560]   need some kind of adult leadership because we're not getting it at the moment.
[02:07:21.560 --> 02:07:25.920]   I wonder if we're going to embrace authoritarian leadership because they are, or can they
[02:07:25.920 --> 02:07:30.640]   can pretend or hope, you know, offer us more effective leadership?
[02:07:30.640 --> 02:07:35.480]   Well, I mean, Amy's more of an expert on this than I am in terms of sort of how societies
[02:07:35.480 --> 02:07:40.240]   react, but it's always struck me on a very limited sociology degree.
[02:07:40.240 --> 02:07:43.680]   Oh, M.I.C., K.E.Y., M.I.U.S.C.
[02:07:43.680 --> 02:07:49.800]   But I mean, people naturally defer to authoritarian leaders when they're scared and frightened.
[02:07:49.800 --> 02:07:57.320]   Why are Germany the hyperinflation, the threat of communism?
[02:07:57.320 --> 02:08:01.720]   Well, I mean, seriously, we're into global depression territory, but look at unemployment
[02:08:01.720 --> 02:08:03.480]   statistics at the moment.
[02:08:03.480 --> 02:08:08.880]   We are into 1930s territory at the moment, unless something really changes.
[02:08:08.880 --> 02:08:16.000]   And I do feel that's going to lead to a more authoritarian and demagogic kind of politics.
[02:08:16.000 --> 02:08:25.120]   I don't, I'm not somebody who feels the sensation of being surprised very often.
[02:08:25.120 --> 02:08:35.400]   I found myself surprised in the wake of this virus because if you think of, like, so I
[02:08:35.400 --> 02:08:42.360]   read a lot of sci-fi, obviously, in a lot of sci-fi books in the Watchmen, right, in
[02:08:42.360 --> 02:08:48.000]   movies, what's that like when things get really bad, what's the thing that saves us?
[02:08:48.000 --> 02:08:49.000]   An alien invasion.
[02:08:49.000 --> 02:08:53.880]   And when there's an alien invasion, like that's the thing that brings everybody together
[02:08:53.880 --> 02:08:58.600]   and suddenly we're all working it out and humanity is one.
[02:08:58.600 --> 02:09:01.200]   This is our alien invasion.
[02:09:01.200 --> 02:09:06.240]   And when it occurred to me that that's what was happening and the outcome is not as I
[02:09:06.240 --> 02:09:10.480]   was promised, which is like everybody steps up.
[02:09:10.480 --> 02:09:14.800]   There's a level of maturity, kumbaya.
[02:09:14.800 --> 02:09:18.040]   And the opposite seems to be what's happening, which is what Ian described.
[02:09:18.040 --> 02:09:26.080]   I had a moment of like surprise, bad, bad surprise, surprise followed by like stomach
[02:09:26.080 --> 02:09:27.080]   aches.
[02:09:27.080 --> 02:09:33.080]   I had the same thought that the COVID-19 was as amandias as alien octopus.
[02:09:33.080 --> 02:09:41.240]   But it, well, we're, I don't know, maybe we're, you know, I mean, is Iran more or less
[02:09:41.240 --> 02:09:42.440]   of a nuclear threat?
[02:09:42.440 --> 02:09:45.080]   Is North Korea more or less than that?
[02:09:45.080 --> 02:09:47.560]   The factor is going into this.
[02:09:47.560 --> 02:09:52.160]   We were already angling for years toward nationalism anyways.
[02:09:52.160 --> 02:09:53.800]   So this becomes an acceleration.
[02:09:53.800 --> 02:09:56.840]   What's ironic is nationalism does not solve this problem.
[02:09:56.840 --> 02:09:58.360]   This is a global problem.
[02:09:58.360 --> 02:10:02.880]   Nationalism is exactly the wrong point of view to solve this problem.
[02:10:02.880 --> 02:10:07.200]   But it makes people feel better, which is the eternal appeal of the whole thing.
[02:10:07.200 --> 02:10:12.120]   I mean, the whole, the only way we defeat this virus is by acting internationally and
[02:10:12.120 --> 02:10:13.120]   coordinated.
[02:10:13.120 --> 02:10:18.000]   By the way, coming right down next after the virus, climate change also requires a global
[02:10:18.000 --> 02:10:19.000]   solution.
[02:10:19.000 --> 02:10:20.000]   Exactly.
[02:10:20.000 --> 02:10:21.000]   Right.
[02:10:21.000 --> 02:10:22.160]   None of the other problems are going away.
[02:10:22.160 --> 02:10:24.440]   We were already dealing with.
[02:10:24.440 --> 02:10:25.440]   Right.
[02:10:25.440 --> 02:10:26.440]   Yeah.
[02:10:26.440 --> 02:10:30.640]   It is interesting though, there have been a rash of stories this week about, oh, well,
[02:10:30.640 --> 02:10:35.440]   you know, one good side of the lockdown is that low, low and no dioxide levels in cities.
[02:10:35.440 --> 02:10:36.440]   And that sort of thing.
[02:10:36.440 --> 02:10:41.040]   It's like, yeah, but the fundamental problems are still there.
[02:10:41.040 --> 02:10:42.320]   Let's not forget about that.
[02:10:42.320 --> 02:10:46.080]   I mean, California hasn't had an eight as hasn't had more than what?
[02:10:46.080 --> 02:10:49.960]   Two or three weeks rain this entire winter.
[02:10:49.960 --> 02:10:51.320]   And that seems to be forgotten.
[02:10:51.320 --> 02:10:53.160]   And now we're all in a drought.
[02:10:53.160 --> 02:10:54.160]   I hope and having.
[02:10:54.160 --> 02:10:55.160]   Yeah.
[02:10:55.160 --> 02:10:56.160]   Yeah.
[02:10:56.160 --> 02:10:57.480]   Where, you know, this stuff is still going on.
[02:10:57.480 --> 02:11:03.680]   Let's not get overly locked down on coronavirus because there's a host of other stuff to be
[02:11:03.680 --> 02:11:04.680]   dealt to be dealt with.
[02:11:04.680 --> 02:11:06.320]   And we really should be getting on with it.
[02:11:06.320 --> 02:11:07.320]   Yeah.
[02:11:07.320 --> 02:11:15.560]   Our show today brought to you by cash fly literally quite literally cash fly is our CDN, our content
[02:11:15.560 --> 02:11:17.800]   delivery network.
[02:11:17.800 --> 02:11:20.880]   And it's been doing this since 1999.
[02:11:20.880 --> 02:11:25.640]   We haven't been using it since 1999, but we've been using it for almost 15 years.
[02:11:25.640 --> 02:11:34.000]   In the early days of Twitter, I was puzzled, stymied by the challenge of delivering terabytes
[02:11:34.000 --> 02:11:37.320]   of data to listeners all over the world.
[02:11:37.320 --> 02:11:38.840]   We tried BitTorrent.
[02:11:38.840 --> 02:11:42.600]   We certainly web downloads that weren't going to work then along came Matt Levine from cash
[02:11:42.600 --> 02:11:46.720]   fly said, Leo, let's talk and cash fly to the rescue.
[02:11:46.720 --> 02:11:49.480]   We've been on cash fly ever since.
[02:11:49.480 --> 02:11:55.720]   And now we do petabytes of months of data, audio and video delivered to you seamlessly,
[02:11:55.720 --> 02:12:01.040]   quickly and at a predictable cost, thanks to cash fly.
[02:12:01.040 --> 02:12:06.080]   One of the things cash fly does for us and we'll do for you is smooth out the spikes.
[02:12:06.080 --> 02:12:08.200]   Our delivery is very spiky.
[02:12:08.200 --> 02:12:13.680]   In fact, I would guess for a lot of companies, it's not a smooth curve.
[02:12:13.680 --> 02:12:16.080]   There are some days where you just get a lot of downloads.
[02:12:16.080 --> 02:12:19.640]   And if you have a normal delivery system, you might be worried about exceeding your cap
[02:12:19.640 --> 02:12:21.080]   at any given time.
[02:12:21.080 --> 02:12:25.240]   You might be worried about the bill that's suddenly going to come due with cash fly.
[02:12:25.240 --> 02:12:29.280]   They take your usage, average it out over a year so you get consistent billing and you
[02:12:29.280 --> 02:12:35.240]   never have to worry about those late nights where you were suddenly a lot of people download
[02:12:35.240 --> 02:12:40.240]   the show and you go, oh no, cash fly is doing even more to help you with something they
[02:12:40.240 --> 02:12:43.760]   call 100% cash shield.
[02:12:43.760 --> 02:12:51.420]   One of the unknown costs of a CDN is your cash hit ratio.
[02:12:51.420 --> 02:12:54.720]   If you can get it to 100%, you're going to save a lot of money.
[02:12:54.720 --> 02:12:58.840]   But if you if you have a lot of cash is aging out, suddenly you're downloading a lot of
[02:12:58.840 --> 02:13:05.480]   content from your source and you may have Amazon expenses, big expenses with these data
[02:13:05.480 --> 02:13:07.680]   transfer fees.
[02:13:07.680 --> 02:13:09.960]   Cash hit ratio is awesome.
[02:13:09.960 --> 02:13:14.200]   You're basically buying some storage on cash fly and that becomes your origin.
[02:13:14.200 --> 02:13:19.480]   It brings your cash hit ratio to 100% no more surprises.
[02:13:19.480 --> 02:13:26.000]   Plus your customers are happy because 100% cash shield means no buffering.
[02:13:26.000 --> 02:13:31.360]   Because they're every time, whether it's a webpage loading, a video buffering, your
[02:13:31.360 --> 02:13:36.680]   games downloading, your podcast arriving on time, everybody wants to be faster.
[02:13:36.680 --> 02:13:39.120]   Your customers expect it.
[02:13:39.120 --> 02:13:44.360]   But when your cash items are being evicted for large one off requests costing you a fortune
[02:13:44.360 --> 02:13:48.200]   and data transfer out fees, that doesn't feel so good.
[02:13:48.200 --> 02:13:54.080]   With 100% cash shield and cash flies guaranteed availability, there's a simple way to avoid
[02:13:54.080 --> 02:13:55.080]   it.
[02:13:55.080 --> 02:14:05.040]   A dedicated storage space just for you, keeping your content closer to your customers, without
[02:14:05.040 --> 02:14:07.360]   the traffic of other companies.
[02:14:07.360 --> 02:14:09.540]   You can purchase as much space as you want.
[02:14:09.540 --> 02:14:13.920]   Your data will always be fetched from 100% cash shield rather than your origin.
[02:14:13.920 --> 02:14:19.080]   Shielding reduces origin spend by thousands a month and guarantees no cash misses.
[02:14:19.080 --> 02:14:24.800]   It's also going to dramatically improve download speeds, eliminate buffering, guaranteed availability,
[02:14:24.800 --> 02:14:29.720]   the highest quality of service, scale your video streaming to reach global audiences
[02:14:29.720 --> 02:14:31.240]   of any size.
[02:14:31.240 --> 02:14:33.840]   It's a great scalable solution for gaming.
[02:14:33.840 --> 02:14:36.520]   Now we're all doing that right, gaming online.
[02:14:36.520 --> 02:14:40.800]   Your podcast will be delivered quickly and reliably without delay or timeouts.
[02:14:40.800 --> 02:14:42.440]   And by the way, I could testify.
[02:14:42.440 --> 02:14:43.960]   It works.
[02:14:43.960 --> 02:14:44.960]   Cash flies awesome.
[02:14:44.960 --> 02:14:49.640]   You get lightning fast digital download speeds for large files, no matter where your customers
[02:14:49.640 --> 02:14:50.640]   are.
[02:14:50.640 --> 02:14:56.880]   And because cash fly servers are all over the world, they can guarantee 100% availability.
[02:14:56.880 --> 02:14:58.880]   100%.
[02:14:58.880 --> 02:15:04.160]   Cash flies global throughput performance dominance ensures any cash misses are delivered five
[02:15:04.160 --> 02:15:06.800]   times faster than from your origins.
[02:15:06.800 --> 02:15:12.720]   So even if you do have a cash hit, it's coming from cash fly and it's fast.
[02:15:12.720 --> 02:15:16.840]   Cash fly right now will give you a detailed analysis of your current CDN bill and usage
[02:15:16.840 --> 02:15:19.640]   trends and show you how much you can save.
[02:15:19.640 --> 02:15:21.760]   This is a little bit of a complicated calculation.
[02:15:21.760 --> 02:15:23.400]   How much storage should I buy?
[02:15:23.400 --> 02:15:25.440]   You know, how much should I pay for cash shield?
[02:15:25.440 --> 02:15:26.880]   How much is it going to save me?
[02:15:26.880 --> 02:15:27.880]   They can help you.
[02:15:27.880 --> 02:15:30.400]   They can get this all clear so you know what you're getting.
[02:15:30.400 --> 02:15:33.720]   See if you're ever paying 20% or more for your CDN.
[02:15:33.720 --> 02:15:37.240]   And if you're not on a CDN, why not?
[02:15:37.240 --> 02:15:39.240]   Twit.cashfly.com.
[02:15:39.240 --> 02:15:42.200]   Learn more twit.cashfly.com.
[02:15:42.200 --> 02:15:43.440]   Thank you, cash fly.
[02:15:43.440 --> 02:15:49.400]   Thank you, Matt and team, because you have saved us year after year after year.
[02:15:49.400 --> 02:15:54.560]   This is a little movie we made highlighting some of the fun things and I mean some fun
[02:15:54.560 --> 02:15:57.360]   things that happened this week on TwitWatch.
[02:15:57.360 --> 02:15:58.560]   Previously on Twit.
[02:15:58.560 --> 02:16:01.760]   Hey, I wanted to wish a happy birthday to Joe.
[02:16:01.760 --> 02:16:02.760]   Happy birthday, Joe.
[02:16:02.760 --> 02:16:04.440]   He says it's his 40th.
[02:16:04.440 --> 02:16:06.000]   We closed the studio.
[02:16:06.000 --> 02:16:07.480]   Why don't we stop having people.
[02:16:07.480 --> 02:16:09.720]   He can't be here.
[02:16:09.720 --> 02:16:10.920]   I'm sorry, Joe.
[02:16:10.920 --> 02:16:13.280]   You know, maybe on your 41st.
[02:16:13.280 --> 02:16:15.280]   Actually at this rate, maybe your 45th.
[02:16:15.280 --> 02:16:16.280]   Maybe your 50th.
[02:16:16.280 --> 02:16:17.280]   Hands on iOS.
[02:16:17.280 --> 02:16:18.280]   Hands on iOS.
[02:16:18.280 --> 02:16:24.520]   I am going to tell you about family sharing, an incredibly powerful feature that frankly,
[02:16:24.520 --> 02:16:27.280]   most people should probably use.
[02:16:27.280 --> 02:16:28.760]   Hands on photography.
[02:16:28.760 --> 02:16:33.040]   So you got your camera and you're thinking about upgrading to a new camera.
[02:16:33.040 --> 02:16:35.440]   Should you?
[02:16:35.440 --> 02:16:36.440]   Hands on Android.
[02:16:36.440 --> 02:16:37.440]   Yes, always.
[02:16:37.440 --> 02:16:40.960]   Coming up on Hands on Android, I heard from an emailer who has a question about removing
[02:16:40.960 --> 02:16:44.320]   bloatware from his Note 10 Plus.
[02:16:44.320 --> 02:16:45.420]   iOS today.
[02:16:45.420 --> 02:16:50.680]   For a lot of people, the camera on their laptop is really pretty poor.
[02:16:50.680 --> 02:16:55.260]   And if you're doing Zoom meetings or Skype, it must be a little frustrating because you
[02:16:55.260 --> 02:17:00.840]   own this amazing camera in your iPhone or even your iPad.
[02:17:00.840 --> 02:17:01.840]   Why can't I use that?
[02:17:01.840 --> 02:17:02.840]   Well, now you can.
[02:17:02.840 --> 02:17:05.880]   It's a product called Epic Cam.
[02:17:05.880 --> 02:17:06.880]   Twit.
[02:17:06.880 --> 02:17:10.240]   Still essential.
[02:17:10.240 --> 02:17:12.320]   Thanks to all of our, you made it point.
[02:17:12.320 --> 02:17:13.560]   You made such a good point.
[02:17:13.560 --> 02:17:18.620]   Thanks to all of our Twit team who work so hard at home, editors working out of their
[02:17:18.620 --> 02:17:26.020]   house, engineers who have to come in, risk their lives to watch me spew.
[02:17:26.020 --> 02:17:27.260]   It's a really great thing.
[02:17:27.260 --> 02:17:28.820]   Thank you, everybody, for working so hard.
[02:17:28.820 --> 02:17:29.820]   We have a wonderful team.
[02:17:29.820 --> 02:17:31.180]   And thank you for supporting us.
[02:17:31.180 --> 02:17:32.180]   We really appreciate it.
[02:17:32.180 --> 02:17:34.400]   All of our Twit family members.
[02:17:34.400 --> 02:17:35.900]   We have Twit 15th anniversary.
[02:17:35.900 --> 02:17:37.900]   I didn't forgot to mention this last week.
[02:17:37.900 --> 02:17:41.540]   Last week was our 15th anniversary of the original first Twit episode.
[02:17:41.540 --> 02:17:48.900]   And there is 15th anniversary merchandise available at the Twit store, twit.tv/store.
[02:17:48.900 --> 02:17:55.380]   We're going to get you a test again because I think you're going down, twit.tv/store.
[02:17:55.380 --> 02:17:57.020]   Get those 15th anniversary.
[02:17:57.020 --> 02:17:58.020]   We got it.
[02:17:58.020 --> 02:17:59.020]   What?
[02:17:59.020 --> 02:18:03.900]   You know, I asked our marketing guy, don't they have Twit 15th anniversary masks?
[02:18:03.900 --> 02:18:06.140]   We've got onesies.
[02:18:06.140 --> 02:18:07.140]   We got more.
[02:18:07.140 --> 02:18:08.740]   You got baby onesie there.
[02:18:08.740 --> 02:18:10.020]   Yeah, why don't we have masks?
[02:18:10.020 --> 02:18:11.020]   I don't want masks.
[02:18:11.020 --> 02:18:12.020]   I want Twit 15th.
[02:18:12.020 --> 02:18:14.260]   I might have to-- I like those hoodies.
[02:18:14.260 --> 02:18:15.260]   I might buy my tickets.
[02:18:15.260 --> 02:18:16.260]   Oh, they're great.
[02:18:16.260 --> 02:18:17.740]   No, we'll send it to you.
[02:18:17.740 --> 02:18:18.740]   Don't worry.
[02:18:18.740 --> 02:18:19.740]   It's on its way.
[02:18:19.740 --> 02:18:20.740]   You want some merch?
[02:18:20.740 --> 02:18:21.740]   You got some merch.
[02:18:21.740 --> 02:18:26.100]   Anybody-- Honestly, I do love the onesies because I went to-- what's this called?
[02:18:26.100 --> 02:18:32.060]   This is going right back versus-- Server 2008 went up to Redmond for a talk there.
[02:18:32.060 --> 02:18:33.380]   And they gave you a onesie?
[02:18:33.380 --> 02:18:36.140]   No, went into the Microsoft shop.
[02:18:36.140 --> 02:18:38.140]   And they had a clippy onesie.
[02:18:38.140 --> 02:18:40.340]   Oh, she's like, "Baby onesie."
[02:18:40.340 --> 02:18:43.580]   It almost looks like it's-- It makes you want to get pregnant just to get the onesie.
[02:18:43.580 --> 02:18:44.580]   It's like, "Well, no."
[02:18:44.580 --> 02:18:46.780]   It's just like, "Looks like you're trying to raise a baby."
[02:18:46.780 --> 02:18:47.780]   Would you like something?
[02:18:47.780 --> 02:18:50.780]   And I was just like, "I have to get this."
[02:18:50.780 --> 02:18:52.540]   He was having a kid.
[02:18:52.540 --> 02:18:54.180]   Is that what you said on it?
[02:18:54.180 --> 02:18:55.180]   That's awesome.
[02:18:55.180 --> 02:18:56.180]   Yeah.
[02:18:56.180 --> 02:18:57.180]   That's hilarious.
[02:18:57.180 --> 02:18:58.180]   Yeah.
[02:18:58.180 --> 02:19:02.380]   So that-- and I know for a fact that onesie has now been passed down to the third baby.
[02:19:02.380 --> 02:19:05.500]   So it's just like, you know, that was a good purchase.
[02:19:05.500 --> 02:19:06.500]   But it's--
[02:19:06.500 --> 02:19:07.500]   Looks like you--
[02:19:07.500 --> 02:19:09.740]   I remember when they tried to re-release Clippy.
[02:19:09.740 --> 02:19:10.740]   Oh, God.
[02:19:10.740 --> 02:19:11.740]   Like, Flash--
[02:19:11.740 --> 02:19:12.740]   Clippy will never die.
[02:19:12.740 --> 02:19:15.580]   Clippy, it's going to come back.
[02:19:15.580 --> 02:19:18.540]   This was like Clippy with AI, supposedly.
[02:19:18.540 --> 02:19:19.540]   Yeah.
[02:19:19.540 --> 02:19:20.540]   Yeah.
[02:19:20.540 --> 02:19:23.540]   I mean, they're trying to do it with Cortana, but it just--
[02:19:23.540 --> 02:19:28.540]   I swear, I was at the-- I was at the 2003 office launch when they announced that they
[02:19:28.540 --> 02:19:30.700]   were turning Clippy off by default.
[02:19:30.700 --> 02:19:33.700]   And the entire room of journalists was just like, "Yes!"
[02:19:33.700 --> 02:19:38.900]   That glass, that ever-citing little thing is gone.
[02:19:38.900 --> 02:19:44.820]   You know, nobody talks about this, but Microsoft was super early to voice and to digital assistance.
[02:19:44.820 --> 02:19:49.060]   I mean, Cortana, they had the infrastructure way before everybody else did.
[02:19:49.060 --> 02:19:50.060]   It's just--
[02:19:50.060 --> 02:19:51.060]   So how did they fumble it?
[02:19:51.060 --> 02:19:52.060]   Yeah, that's OK.
[02:19:52.060 --> 02:19:58.340]   Well, yeah, I mean, the Windows phone, which I thought was actually a pretty decent little
[02:19:58.340 --> 02:20:02.620]   phone just didn't have-- this is where the ecosystem comes into play.
[02:20:02.620 --> 02:20:06.340]   So it's not enough to have a great-- and this Ian goes right back to what you were saying
[02:20:06.340 --> 02:20:08.300]   about startups and investment.
[02:20:08.300 --> 02:20:10.380]   It's not enough to have a great technology.
[02:20:10.380 --> 02:20:14.980]   It's not enough to have a great technology with demonstratable use cases.
[02:20:14.980 --> 02:20:17.420]   You have to have the ecosystem around it.
[02:20:17.420 --> 02:20:20.260]   And I think the problem with that Windows phone was no ecosystem.
[02:20:20.260 --> 02:20:23.660]   The problem with Cortana is no ecosystem.
[02:20:23.660 --> 02:20:28.340]   And for a time, no clear direction on what they were doing internally with Cortana and
[02:20:28.340 --> 02:20:32.100]   what AI was going to be for Microsoft and with Microsoft.
[02:20:32.100 --> 02:20:36.340]   But I think that-- I read all the academic research.
[02:20:36.340 --> 02:20:45.300]   And their team consistently produces very strong research on machine reading comprehension
[02:20:45.300 --> 02:20:51.580]   and natural language, like a lot of the core pieces of spoken interfaces.
[02:20:51.580 --> 02:20:55.060]   But you got to get it somewhere being used.
[02:20:55.060 --> 02:20:56.060]   Yeah.
[02:20:56.060 --> 02:21:00.260]   It's your analysis for stock market investments.
[02:21:00.260 --> 02:21:04.140]   So I have to be very clear that I am not--
[02:21:04.140 --> 02:21:05.140]   You're not allowed to.
[02:21:05.140 --> 02:21:07.100]   --I'm not making best management advice.
[02:21:07.100 --> 02:21:10.180]   I mean, I advise the Federal Reserve and--
[02:21:10.180 --> 02:21:11.180]   Wow.
[02:21:11.180 --> 02:21:11.660]   --worried with-- yeah.
[02:21:11.660 --> 02:21:13.900]   So I-- you know.
[02:21:13.900 --> 02:21:15.020]   That's pretty cool.
[02:21:15.020 --> 02:21:16.140]   Yeah.
[02:21:16.140 --> 02:21:20.780]   Yeah, I mean, I do feel that Microsoft has suffered a sort of Xerox park situation about
[02:21:20.780 --> 02:21:24.820]   this, where they've developed all these great technologies and then just let them life
[02:21:24.820 --> 02:21:31.100]   out and then just-- and trust me, if you get a few beers down me and get me talking about
[02:21:31.100 --> 02:21:36.500]   Microsoft and Nokia, I will swear to the rooftops because they took-- they got this
[02:21:36.500 --> 02:21:38.580]   idea, oh, we'll buy Nokia.
[02:21:38.580 --> 02:21:43.100]   We'll substitute Windows Phone for the Symbian operating system, which Nokia was stupidly
[02:21:43.100 --> 02:21:44.580]   holding onto.
[02:21:44.580 --> 02:21:46.180]   And everyone will flock to that.
[02:21:46.180 --> 02:21:47.580]   It didn't work.
[02:21:47.580 --> 02:21:51.700]   They wrote the entire investment off against tax when we paid for it.
[02:21:51.700 --> 02:21:52.940]   And Nokia died.
[02:21:52.940 --> 02:21:58.860]   And I just-- Microsoft has this real problem between developing these great technologies
[02:21:58.860 --> 02:22:03.180]   and just complete inability to bring them to market in any way that anyone wants.
[02:22:03.180 --> 02:22:04.180]   Yeah.
[02:22:04.180 --> 02:22:05.180]   I don't understand it.
[02:22:05.180 --> 02:22:09.900]   I could not agree-- like, I could not agree more with every word that you just said.
[02:22:09.900 --> 02:22:10.900]   And it's a shame.
[02:22:10.900 --> 02:22:17.020]   It's a shame because in some-- like I said, some of the technology that I've seen and
[02:22:17.020 --> 02:22:21.580]   read about is far superior to a lot of what we have access to.
[02:22:21.580 --> 02:22:27.660]   Oh, it makes Siri the market complete-- yeah, Cortana in its early thing made Siri look
[02:22:27.660 --> 02:22:29.100]   like a complete joke.
[02:22:29.100 --> 02:22:31.100]   But they just drop the ball.
[02:22:31.100 --> 02:22:37.220]   So again, I've always thought Microsoft is invisible infrastructure.
[02:22:37.220 --> 02:22:40.020]   And they should accept that and embrace it.
[02:22:40.020 --> 02:22:42.940]   The world does not work without Microsoft.
[02:22:42.940 --> 02:22:47.620]   So become the invisible infrastructure for the enterprise and maybe don't worry so much
[02:22:47.620 --> 02:22:49.300]   about the consumer space.
[02:22:49.300 --> 02:22:52.780]   But go all in and build amazing products.
[02:22:52.780 --> 02:22:56.260]   I kind of think that's what Sacha Nadella is doing, to be honest with you.
[02:22:56.260 --> 02:22:57.260]   You don't think it is?
[02:22:57.260 --> 02:22:58.660]   I do think it's what he's doing.
[02:22:58.660 --> 02:22:59.660]   No.
[02:22:59.660 --> 02:23:02.740]   I think he's an asher guy.
[02:23:02.740 --> 02:23:08.140]   And I think it's been very interesting to watch windows going from being the crown jewel
[02:23:08.140 --> 02:23:10.580]   to something a little less.
[02:23:10.580 --> 02:23:11.580]   Yeah.
[02:23:11.580 --> 02:23:12.580]   Yeah.
[02:23:12.580 --> 02:23:16.780]   It was when you got server coming out in Linux.
[02:23:16.780 --> 02:23:21.820]   It was just like, wow, this is something Steve-- Steve Bummer would never have considered
[02:23:21.820 --> 02:23:22.820]   that.
[02:23:22.820 --> 02:23:24.940]   They're putting a Linux kernel in windows.
[02:23:24.940 --> 02:23:25.940]   I know.
[02:23:25.940 --> 02:23:26.940]   It's great.
[02:23:26.940 --> 02:23:27.940]   I love it.
[02:23:27.940 --> 02:23:32.660]   I mean, when you think back to the bomber years where he was throwing chances around--
[02:23:32.660 --> 02:23:33.660]   Oh, he's so little.
[02:23:33.660 --> 02:23:36.140]   He said Linux is a cancer.
[02:23:36.140 --> 02:23:40.420]   Well, I mean, he said the iPhone would never take off because you could have a phone without
[02:23:40.420 --> 02:23:41.420]   a keyboard.
[02:23:41.420 --> 02:23:42.420]   All of this stuff.
[02:23:42.420 --> 02:23:45.580]   I mean, OK, bomber was a great sales guy.
[02:23:45.580 --> 02:23:46.580]   Don't get me wrong.
[02:23:46.580 --> 02:23:48.140]   lousy CEO.
[02:23:48.140 --> 02:23:52.220]   And sat-in had has really turned the company around.
[02:23:52.220 --> 02:23:54.660]   And yeah, I think Microsoft's on a good course.
[02:23:54.660 --> 02:24:00.700]   I just wish they could actually take the technologies they use and implement them rather than, you
[02:24:00.700 --> 02:24:06.180]   know, and or just concentrate on being the back end and let the flashy companies do the
[02:24:06.180 --> 02:24:07.180]   rest.
[02:24:07.180 --> 02:24:13.100]   But it weren't for COVID the absolute huge story this week in technology would be this scoop
[02:24:13.100 --> 02:24:15.180]   from Mark Gurman at Bloomberg.
[02:24:15.180 --> 02:24:17.980]   And because it's Mark, I think it's credible.
[02:24:17.980 --> 02:24:23.500]   Apple aims to sell Max with its own chips starting next year.
[02:24:23.500 --> 02:24:29.620]   Apple has had some remarkable chip technology, but it's only put it in iPads and iPhones.
[02:24:29.620 --> 02:24:34.340]   But in fact, one of the other headlines that was interesting this week was one of the Android
[02:24:34.340 --> 02:24:39.260]   blogs, Android police, bemoaning the fact that Apple's cheapest iPhone, the iPhone SE,
[02:24:39.260 --> 02:24:46.260]   which just came out, has a faster processor than any Android phone, any Android phone.
[02:24:46.260 --> 02:24:51.460]   And that's because Apple's A13 and its chip technology is superb.
[02:24:51.460 --> 02:24:54.700]   But is it ready for desktop?
[02:24:54.700 --> 02:24:57.380]   And that's a very interesting question.
[02:24:57.380 --> 02:25:01.060]   Microsoft has famously put out some arm windows.
[02:25:01.060 --> 02:25:05.540]   They have windows on arm and they put out some arm devices that have been less than
[02:25:05.540 --> 02:25:06.540]   stellar.
[02:25:06.540 --> 02:25:09.100]   You're being very polite.
[02:25:09.100 --> 02:25:10.100]   Come on.
[02:25:10.100 --> 02:25:13.300]   Actually, you know, the surface X is not awful.
[02:25:13.300 --> 02:25:19.780]   Not yet, but not awful is not really a great commitment for a product.
[02:25:19.780 --> 02:25:24.820]   Look, windows on arm has always been a complete loser.
[02:25:24.820 --> 02:25:27.260]   And it's great that Apple's doing their own chips.
[02:25:27.260 --> 02:25:31.820]   Google has reportedly, actually was saying also doing their own chips.
[02:25:31.820 --> 02:25:39.540]   But I mean, you know, tell by the way, a number of people said COVID is going to kill Intel.
[02:25:39.540 --> 02:25:42.580]   Well, the financial results this week didn't suggest that.
[02:25:42.580 --> 02:25:48.220]   We've seen double digit growth in the PC market this quarter because everyone is stocking
[02:25:48.220 --> 02:25:50.220]   up on new computers to work from home with.
[02:25:50.220 --> 02:25:55.420]   Yes, but TSMC is going to they have five nanometers are going to three nanometers.
[02:25:55.420 --> 02:25:58.100]   You've got AMD coming on strong.
[02:25:58.100 --> 02:26:02.740]   It does feel like Intel might be facing some pretty strong headwinds going for.
[02:26:02.740 --> 02:26:06.940]   I think I think you're right on one level in that they haven't managed to crack the
[02:26:06.940 --> 02:26:07.940]   mobile market.
[02:26:07.940 --> 02:26:11.060]   At some is just a complete dog.
[02:26:11.060 --> 02:26:16.660]   We've got, you know, they're they're still focused on the eight eighties on the X86 architecture.
[02:26:16.660 --> 02:26:20.540]   And yeah, they're going to get their lunch eaten for them by, you know, the rest of them.
[02:26:20.540 --> 02:26:25.100]   But Intel has kind of dropped the ball for the last five years now.
[02:26:25.100 --> 02:26:28.020]   You know, I couldn't happen to they deserve it.
[02:26:28.020 --> 02:26:29.820]   I'm not saying they don't deserve it.
[02:26:29.820 --> 02:26:32.380]   They absolutely deserve it.
[02:26:32.380 --> 02:26:38.260]   I don't think Apple's such a big customer of Intel that them going to arm chips or their
[02:26:38.260 --> 02:26:42.220]   own design chips is going to be a huge hit for Intel.
[02:26:42.220 --> 02:26:50.220]   But what is interesting is if if you can get arm chips in the network centers in servers,
[02:26:50.220 --> 02:26:52.460]   that's going to be a big problem for Intel.
[02:26:52.460 --> 02:26:56.340]   But you see, they've been trying to do that for a couple of a couple of years now.
[02:26:56.340 --> 02:27:00.260]   And it doesn't seem to scale as well as it might.
[02:27:00.260 --> 02:27:08.380]   I think that if they can lick that and get the power chips in very power heavy environments
[02:27:08.380 --> 02:27:12.380]   like network network centers, that's going to be amazing.
[02:27:12.380 --> 02:27:15.380]   That's going to be.
[02:27:15.380 --> 02:27:20.300]   There's been a greater move to making chipsets in part because of AI, right?
[02:27:20.300 --> 02:27:23.380]   So you've got custom chipsets, right?
[02:27:23.380 --> 02:27:27.900]   Google announced AI can design a computer chip in under six hours this week.
[02:27:27.900 --> 02:27:28.900]   Six hours.
[02:27:28.900 --> 02:27:30.900]   Good one there.
[02:27:30.900 --> 02:27:32.900]   Well, the design is one thing.
[02:27:32.900 --> 02:27:34.420]   You've got to get it manufactured though.
[02:27:34.420 --> 02:27:38.860]   And so again, this is where it's useful to think through, are we going to suddenly bring
[02:27:38.860 --> 02:27:42.820]   the manufacturing sector like, are we going to move it out of Taiwan and back into the
[02:27:42.820 --> 02:27:43.820]   United States?
[02:27:43.820 --> 02:27:44.820]   We're going to build fabs.
[02:27:44.820 --> 02:27:45.820]   Yeah.
[02:27:45.820 --> 02:27:48.620]   And fabs have gone from a billion dollars to $10 billion.
[02:27:48.620 --> 02:27:52.420]   These are very, very advanced technology.
[02:27:52.420 --> 02:27:55.180]   But there's still a fab base in Germany.
[02:27:55.180 --> 02:27:56.580]   There's a fab base in Israel.
[02:27:56.580 --> 02:27:57.580]   Are there no fabs?
[02:27:57.580 --> 02:27:58.580]   Do we not?
[02:27:58.580 --> 02:28:00.460]   It doesn't until make chips in Oregon.
[02:28:00.460 --> 02:28:02.460]   Are there no fabs in the US?
[02:28:02.460 --> 02:28:04.460]   A few, but they're scating them down.
[02:28:04.460 --> 02:28:07.020]   I mean, it's a lunacy.
[02:28:07.020 --> 02:28:08.700]   Taiwan, yeah.
[02:28:08.700 --> 02:28:09.700]   Yeah.
[02:28:09.700 --> 02:28:15.060]   And also when we lose Taiwan to China, then that's going to be a major thing to deal with
[02:28:15.060 --> 02:28:17.140]   because that's coming down the line.
[02:28:17.140 --> 02:28:18.140]   Is it?
[02:28:18.140 --> 02:28:19.140]   Oh, yeah.
[02:28:19.140 --> 02:28:20.140]   I mean, come on.
[02:28:20.140 --> 02:28:21.140]   Yeah.
[02:28:21.140 --> 02:28:23.140]   I mean, China's move it.
[02:28:23.140 --> 02:28:24.140]   Yeah, totally.
[02:28:24.140 --> 02:28:25.140]   What?
[02:28:25.140 --> 02:28:27.820]   Just just that's World War three, isn't it?
[02:28:27.820 --> 02:28:29.380]   No, because we do.
[02:28:29.380 --> 02:28:31.340]   Who's going to come in and get it?
[02:28:31.340 --> 02:28:33.580]   Yeah, I mean, Mike, I'll do it.
[02:28:33.580 --> 02:28:35.380]   Yeah, I mean, I'm sorry.
[02:28:35.380 --> 02:28:37.620]   You're actually going to try and fight a war against China.
[02:28:37.620 --> 02:28:38.620]   Yeah, right.
[02:28:38.620 --> 02:28:40.860]   The American get is ass kicked.
[02:28:40.860 --> 02:28:41.860]   Yeah.
[02:28:41.860 --> 02:28:45.100]   I mean, it's just, it's just not going to happen.
[02:28:45.100 --> 02:28:53.860]   I mean, China, even if China will, China will absorb Taiwan over a course of years.
[02:28:53.860 --> 02:28:58.060]   They're hopefully going to do it peacefully, but it's going to happen.
[02:28:58.060 --> 02:29:00.060]   Kind of like they've absorbed Hong Kong.
[02:29:00.060 --> 02:29:01.500]   Yeah, exactly.
[02:29:01.500 --> 02:29:07.380]   I mean, China is very good at not starting wars, but still getting what it wants.
[02:29:07.380 --> 02:29:12.300]   And, you know, all this stuff about, well, Taiwan is an independent nation, please, you
[02:29:12.300 --> 02:29:14.380]   know, we couldn't defend Taiwan if we tried.
[02:29:14.380 --> 02:29:19.060]   It's really interesting to watch the difference that the World Health Organization and others
[02:29:19.060 --> 02:29:25.740]   have given to China, because in the same way that Trump's minions give deference to him,
[02:29:25.740 --> 02:29:30.260]   when somebody has a lot of power, you don't, it's hard to speak truth to power right to
[02:29:30.260 --> 02:29:33.020]   their face, isn't it?
[02:29:33.020 --> 02:29:35.020]   Well, okay, sorry, Emmy.
[02:29:35.020 --> 02:29:41.140]   No, no, he and you go and then I'll say, okay, I will say this, Taiwan dealt with coronavirus
[02:29:41.140 --> 02:29:43.860]   better than pretty much anyone else on the planet.
[02:29:43.860 --> 02:29:45.740]   They locked down the state quickly.
[02:29:45.740 --> 02:29:46.740]   They isolated.
[02:29:46.740 --> 02:29:49.660]   They backtracked the infection rates they got.
[02:29:49.660 --> 02:29:52.660]   And as a result, they've virtually been untouched by this.
[02:29:52.660 --> 02:29:56.540]   But if you honestly think that America is going to be able to win a war against China
[02:29:56.540 --> 02:30:00.260]   to over the control of Taiwan, it's not going to happen.
[02:30:00.260 --> 02:30:05.980]   China's going to walk in there in five, ten, maybe 20 years time, and they're going to
[02:30:05.980 --> 02:30:08.420]   own the process of technology that's there.
[02:30:08.420 --> 02:30:10.460]   So you know, we're just going to have to deal with that.
[02:30:10.460 --> 02:30:12.780]   Well, we're already pretty reliant on China.
[02:30:12.780 --> 02:30:15.580]   So I guess it would be that much different.
[02:30:15.580 --> 02:30:18.620]   But we'll come back and wrap this up in a second.
[02:30:18.620 --> 02:30:24.540]   Our show today brought to you by Stamps.com, which is absolutely vital.
[02:30:24.540 --> 02:30:30.360]   We know the US Postal Service is absolutely vital to small businesses.
[02:30:30.360 --> 02:30:33.980]   But these days, you probably don't want to go into the post office.
[02:30:33.980 --> 02:30:38.460]   But here's some good news with Stamps.com, you don't have to.
[02:30:38.460 --> 02:30:40.580]   Everything you would do at a post office.
[02:30:40.580 --> 02:30:46.500]   You can do right from your desk with your own computer, your own printer, and Stamps.com.
[02:30:46.500 --> 02:30:50.900]   You don't need a postage meter, you don't need a special link, you just need Stamps.com.
[02:30:50.900 --> 02:30:54.300]   And by the way, not just the postal service.
[02:30:54.300 --> 02:30:57.500]   Stamps.com also offers UPS services.
[02:30:57.500 --> 02:31:01.100]   And in both cases, you're going to save a ton of money.
[02:31:01.100 --> 02:31:02.700]   You'll print postage on demand.
[02:31:02.700 --> 02:31:05.780]   You can save money with discounts you can't get at the post office.
[02:31:05.780 --> 02:31:11.900]   A nickel off every first stamp, up to 40% off USPS shipping rates.
[02:31:11.900 --> 02:31:17.100]   And now with UPS services, discounts of up to 62%.
[02:31:17.100 --> 02:31:18.220]   And I don't know if you knew this.
[02:31:18.220 --> 02:31:22.100]   I found this out when I was talking to Stamps.com.
[02:31:22.100 --> 02:31:27.420]   If you're shipping UPS to a residence, you pay a residential surcharge unless you use
[02:31:27.420 --> 02:31:29.060]   Stamps.com.
[02:31:29.060 --> 02:31:32.220]   They got UPS to waive the residential surcharge.
[02:31:32.220 --> 02:31:33.500]   That's going to save you.
[02:31:33.500 --> 02:31:38.220]   Stamps.com brings all the services of the US Postal Service right to the computer in
[02:31:38.220 --> 02:31:41.260]   the safety and comfort of your home, your office.
[02:31:41.260 --> 02:31:42.820]   Anywhere else you're hunkering down.
[02:31:42.820 --> 02:31:44.220]   Now we use it here at Twit.
[02:31:44.220 --> 02:31:49.300]   And the nice thing is I can bring it home because Stamps.com works wherever you are.
[02:31:49.300 --> 02:31:55.260]   If you've got a printer and you've got a computer, you can send packages, you can print stamps,
[02:31:55.260 --> 02:32:00.220]   you can send any package, any letter, any class of mail, anywhere you want it to send.
[02:32:00.220 --> 02:32:03.980]   Put it out on your doorstep, the Postal Service picks it up.
[02:32:03.980 --> 02:32:10.420]   You can schedule a free package pickup or just leave it outside for their normal appearance.
[02:32:10.420 --> 02:32:11.940]   No human contact required.
[02:32:11.940 --> 02:32:13.820]   I mean, this is what you're looking for.
[02:32:13.820 --> 02:32:18.260]   And if you're an Etsy seller, an eBay seller, an Amazon seller, we go to some Mass from Etsy.
[02:32:18.260 --> 02:32:21.260]   They came $8.50 postage due.
[02:32:21.260 --> 02:32:25.820]   That just does not send a good message to your customers.
[02:32:25.820 --> 02:32:26.900]   That's a bad idea.
[02:32:26.900 --> 02:32:28.700]   You want to look professional?
[02:32:28.700 --> 02:32:29.700]   Stamps.com.
[02:32:29.700 --> 02:32:33.100]   And there's no equipment to lease, no long-term commitments.
[02:32:33.100 --> 02:32:36.700]   And this trial we're offering you, this is the best ever.
[02:32:36.700 --> 02:32:40.260]   We've been using Stamps.com for years.
[02:32:40.260 --> 02:32:42.620]   And I highly recommend it.
[02:32:42.620 --> 02:32:47.340]   Right now you're going to get a special offer that includes four weeks free trial plus
[02:32:47.340 --> 02:32:54.860]   free postage, a lot of free postage, a digital scale, no long-term commitment.
[02:32:54.860 --> 02:32:59.340]   Just go to Stamps.com, click the microphone at the top of the home page and type in Twit.
[02:32:59.340 --> 02:33:04.220]   Now more than ever, Stamps.com is going to save you money.
[02:33:04.220 --> 02:33:08.180]   Stamps.com and save you time.
[02:33:08.180 --> 02:33:09.740]   And it's safe.
[02:33:09.740 --> 02:33:11.340]   No contact email.
[02:33:11.340 --> 02:33:14.860]   I'm a US mail, no contact UPS.
[02:33:14.860 --> 02:33:16.540]   This is what you need in this time.
[02:33:16.540 --> 02:33:20.940]   Stamps.com enter the offer code Twit for this amazing special offer.
[02:33:20.940 --> 02:33:22.700]   I hope you take advantage of it.
[02:33:22.700 --> 02:33:24.100]   This is the time to do it.
[02:33:24.100 --> 02:33:27.700]   Stamps.com, we thank them so much for their support.
[02:33:27.700 --> 02:33:41.420]   So from a game theory point of view, what would it take for us to increase our military?
[02:33:41.420 --> 02:33:48.540]   What would it take to have an engagement that involves force?
[02:33:48.540 --> 02:33:52.920]   I don't think that it would, I think it would be a losing game for either side to escalate
[02:33:52.920 --> 02:33:53.920]   to that point.
[02:33:53.920 --> 02:34:01.880]   What's more likely is that China reclaims Taiwan and we don't stand in the way.
[02:34:01.880 --> 02:34:03.360]   But we, I mean don't for a second think.
[02:34:03.360 --> 02:34:05.920]   Is that because of the nuclear deterrence?
[02:34:05.920 --> 02:34:06.920]   No, no.
[02:34:06.920 --> 02:34:08.240]   It's a nuclear issue.
[02:34:08.240 --> 02:34:09.880]   I think this is an economic issue.
[02:34:09.880 --> 02:34:18.120]   And I've been advocating in the DOD that we think about, we have to redefine warfare.
[02:34:18.120 --> 02:34:26.400]   And this is a tough one for all of the labs around the country where they do advanced
[02:34:26.400 --> 02:34:30.760]   physics research, nuclear research and all the rest of it.
[02:34:30.760 --> 02:34:34.600]   We have to start thinking about war and economic terms.
[02:34:34.600 --> 02:34:40.800]   What would it mean for a war to be fought in code versus combat or using economic measures
[02:34:40.800 --> 02:34:42.360]   in ways we haven't seen before?
[02:34:42.360 --> 02:34:46.200]   It's conventional warfare between superpowers a thing of the past.
[02:34:46.200 --> 02:34:49.740]   Well, I don't know about superpowers, but I can say it's conventional warfare going
[02:34:49.740 --> 02:34:52.320]   on right now in many, many areas of the world.
[02:34:52.320 --> 02:34:59.160]   But no, I mean if the US decided to defend Taiwan, let's look at it logically.
[02:34:59.160 --> 02:35:05.260]   You've got a 4,000 mile supply gap, which you're going to get Harris by Chinese submarines
[02:35:05.260 --> 02:35:07.080]   the entire way there.
[02:35:07.080 --> 02:35:11.800]   China has spent the last 30 years developing anti-aircraft carrier technology, which is
[02:35:11.800 --> 02:35:19.480]   going to make even the best US aircraft carriers huge targets, which can't be defended.
[02:35:19.480 --> 02:35:24.920]   If the US goes to actual physical war with China, that's my biggest fear because it might
[02:35:24.920 --> 02:35:28.280]   end up nuclear in which case we all do.
[02:35:28.280 --> 02:35:32.360]   But what I suspect is going to happen is that the US will make some noises.
[02:35:32.360 --> 02:35:37.920]   China will slowly absorb Taiwan in the way that it has with Hong Kong and Taipei.
[02:35:37.920 --> 02:35:39.720]   And that's the way they work.
[02:35:39.720 --> 02:35:40.880]   They don't start wars.
[02:35:40.880 --> 02:35:41.880]   They just win them.
[02:35:41.880 --> 02:35:42.880]   Yeah.
[02:35:42.880 --> 02:35:47.440]   Amy's been talking about this on our shows for a long time because she's an expert on
[02:35:47.440 --> 02:35:52.320]   this and you've been talking about the Belt and Road Initiative, which is China's economic
[02:35:52.320 --> 02:35:57.840]   hegemony plan to spread itself throughout the world.
[02:35:57.840 --> 02:36:00.280]   And it seems very difficult to counter.
[02:36:00.280 --> 02:36:03.240]   Do you see us going to a cyber war?
[02:36:03.240 --> 02:36:08.480]   As you see that as being a credible deterrent of some kind?
[02:36:08.480 --> 02:36:14.080]   Here's what I think has been happening.
[02:36:14.080 --> 02:36:18.360]   Parts of our military have been reorganizing.
[02:36:18.360 --> 02:36:25.440]   And I think that there has been a greater focus placed on preparedness as it relates
[02:36:25.440 --> 02:36:27.320]   to code.
[02:36:27.320 --> 02:36:31.940]   The challenge is that we have a very long history of fighting war in a very different
[02:36:31.940 --> 02:36:40.580]   way, a way that is tactical and requires building things and blowing things up.
[02:36:40.580 --> 02:36:42.220]   So that's fine.
[02:36:42.220 --> 02:36:45.980]   The whole point of our military is preparedness.
[02:36:45.980 --> 02:36:51.780]   So the question is, are we preparing the right way?
[02:36:51.780 --> 02:36:58.620]   I wonder if nuclear is really the big challenge going forward or if it's attacks against our
[02:36:58.620 --> 02:37:03.900]   critical infrastructure and security flaws.
[02:37:03.900 --> 02:37:13.780]   And to me, cyber security feels more imminent and problematic than the threat of nuclear
[02:37:13.780 --> 02:37:15.300]   escalation.
[02:37:15.300 --> 02:37:20.020]   So when I say, how can we think about this in terms of economics?
[02:37:20.020 --> 02:37:25.220]   When you look at the Belt and Road Initiative, which is one of many, many policies.
[02:37:25.220 --> 02:37:32.900]   The China's been building physical infrastructure along the old Silk Road route in part.
[02:37:32.900 --> 02:37:36.540]   Mostly it's in emerging economies and emerging markets.
[02:37:36.540 --> 02:37:39.900]   But they've also been laying fiber and digital infrastructure.
[02:37:39.900 --> 02:37:49.420]   And so what happens if there's a non-contiguous but like huge swath of the planet in Africa,
[02:37:49.420 --> 02:37:55.180]   Latin America, parts of Europe, that are aligned with China and we're locked out?
[02:37:55.180 --> 02:38:03.220]   You could cripple our economy slowly by compromising our supply chains, our travel routes, our
[02:38:03.220 --> 02:38:04.220]   trade routes.
[02:38:04.220 --> 02:38:08.620]   I mean, there are a lot of different ways to wage war against another country that don't
[02:38:08.620 --> 02:38:10.300]   require guns and ships.
[02:38:10.300 --> 02:38:12.260]   It's almost like a siege.
[02:38:12.260 --> 02:38:13.740]   It's almost like a siege.
[02:38:13.740 --> 02:38:14.740]   Yeah.
[02:38:14.740 --> 02:38:15.740]   Yeah.
[02:38:15.740 --> 02:38:23.300]   But that's just what the US did to the Soviet Union in the 70s and 60s and 70s where we
[02:38:23.300 --> 02:38:24.940]   went into developing countries.
[02:38:24.940 --> 02:38:27.540]   We said, yeah, that's built this great infrastructure.
[02:38:27.540 --> 02:38:30.460]   Yeah, you owe us a certain amount of money, but you can pay that.
[02:38:30.460 --> 02:38:36.580]   And China seems to be copying exactly the same playbook, that kind of soft warfare.
[02:38:36.580 --> 02:38:37.580]   Yeah.
[02:38:37.580 --> 02:38:44.460]   And I do want to just caution against continuing to think of China as a copy paste culture.
[02:38:44.460 --> 02:38:47.540]   They were totally scraping everybody's IP and copying it.
[02:38:47.540 --> 02:38:48.540]   Like everybody knows that.
[02:38:48.540 --> 02:38:52.340]   But I think things have changed over the past decade or so.
[02:38:52.340 --> 02:38:54.420]   And they're innovating in their own right.
[02:38:54.420 --> 02:38:57.220]   And certainly when it comes to policy.
[02:38:57.220 --> 02:39:02.260]   So the bottom line is they've got people on the ground.
[02:39:02.260 --> 02:39:09.380]   So like our state departments, our companies have started, for a long time, we haven't
[02:39:09.380 --> 02:39:12.740]   had people on the ground in all of these places.
[02:39:12.740 --> 02:39:13.980]   China has.
[02:39:13.980 --> 02:39:16.380]   So they have relationships.
[02:39:16.380 --> 02:39:18.140]   They understand each other.
[02:39:18.140 --> 02:39:21.540]   They've got a cultural sensitivity and awareness.
[02:39:21.540 --> 02:39:25.220]   And our sort of, gosh, I don't want to say.
[02:39:25.220 --> 02:39:27.060]   We're number one.
[02:39:27.060 --> 02:39:28.060]   We're number one.
[02:39:28.060 --> 02:39:32.220]   I'm just going to say, like America's like balls out.
[02:39:32.220 --> 02:39:36.540]   Like here we are, you know, like, but you know what I mean?
[02:39:36.540 --> 02:39:40.180]   Like that might have worked in the 90s.
[02:39:40.180 --> 02:39:42.860]   That doesn't work in the 2020s.
[02:39:42.860 --> 02:39:49.100]   Like there's a different way of doing things and somehow, you know, in this predates Trump,
[02:39:49.100 --> 02:39:52.140]   this goes also back to this has been sort of a break.
[02:39:52.140 --> 02:39:53.380]   It's been going on for years.
[02:39:53.380 --> 02:39:54.380]   Yeah.
[02:39:54.380 --> 02:39:56.020]   So we kind of like missed the boat.
[02:39:56.020 --> 02:39:57.180]   And it was not just the US.
[02:39:57.180 --> 02:39:58.900]   I don't know if there's anything we could have done.
[02:39:58.900 --> 02:40:03.820]   I mean, I mean, certainly economic dependence on China wasn't a good idea.
[02:40:03.820 --> 02:40:06.260]   Well, no, but I mean, we've come back.
[02:40:06.260 --> 02:40:09.980]   It comes back to the central idea of hard power and soft power.
[02:40:09.980 --> 02:40:12.620]   America's always had this, you know, we are the hard power.
[02:40:12.620 --> 02:40:14.100]   We have the military.
[02:40:14.100 --> 02:40:18.260]   We have our nuclear arsenal and we've got the soft power, the cultural.
[02:40:18.260 --> 02:40:20.340]   It wants to be like America, the rest of it.
[02:40:20.340 --> 02:40:21.820]   Well, the hard power is gone.
[02:40:21.820 --> 02:40:27.260]   Basically, America hasn't won a wall for 30 years and its military is a complete mess at
[02:40:27.260 --> 02:40:28.260]   the moment.
[02:40:28.260 --> 02:40:33.620]   Soft power, that's dissolved over the last 10 to 15 years as well.
[02:40:33.620 --> 02:40:37.140]   China is perfectly willing to step into that as far as I can see.
[02:40:37.140 --> 02:40:42.820]   I'm open to suggestions, but it just seems like we're kind of painting ourselves into
[02:40:42.820 --> 02:40:44.580]   a corner on this one.
[02:40:44.580 --> 02:40:50.820]   I agree with all that, but I would offer a little more credit to the US military.
[02:40:50.820 --> 02:40:54.140]   You know, it could.
[02:40:54.140 --> 02:40:57.100]   There's been a rotating cast of characters and certainly there's been a whole bunch of
[02:40:57.100 --> 02:40:59.100]   bad headlines over the past couple of months.
[02:40:59.100 --> 02:41:02.500]   But as soon as you got nukes, I don't know if it really matters if you have a strong
[02:41:02.500 --> 02:41:07.020]   conventional force, but you can't use nukes without killing everyone else.
[02:41:07.020 --> 02:41:10.100]   I know, but it's still a credible threat.
[02:41:10.100 --> 02:41:13.500]   No, but China wins a nuclear war.
[02:41:13.500 --> 02:41:14.900]   Mao was perfectly clear.
[02:41:14.900 --> 02:41:19.460]   As long as 100,000 Chinese survive, we win.
[02:41:19.460 --> 02:41:22.020]   So in my office, I have a picture of this guy.
[02:41:22.020 --> 02:41:23.500]   So this is Herman Kahn.
[02:41:23.500 --> 02:41:25.980]   Wow, it's a great picture.
[02:41:25.980 --> 02:41:27.820]   I don't know what he has.
[02:41:27.820 --> 02:41:31.300]   I trace my lineage as a futurist back to him.
[02:41:31.300 --> 02:41:34.180]   So he was on the thermonuclear war, right?
[02:41:34.180 --> 02:41:41.740]   And was at the height of the post-military industrial complex thinking about nuclear
[02:41:41.740 --> 02:41:43.220]   deterrence.
[02:41:43.220 --> 02:41:49.020]   And I guess my thought is nuclear weapons are bad.
[02:41:49.020 --> 02:41:51.460]   I think that there are things that are worse.
[02:41:51.460 --> 02:41:52.700]   I just do.
[02:41:52.700 --> 02:41:55.180]   I think the threat of biohazard is worse.
[02:41:55.180 --> 02:42:03.220]   The threat of algorithmic like cyber sovereignty is worse in some ways.
[02:42:03.220 --> 02:42:07.340]   So I think that the war games of the 80s don't.
[02:42:07.340 --> 02:42:10.700]   Obviously, we still have nuclear weapons, so it's still an issue.
[02:42:10.700 --> 02:42:16.380]   But I don't think it's like the issue going forward with apologies for the current.
[02:42:16.380 --> 02:42:19.940]   I know, Mr. Herman Kahn, the guy who said a nuclear war is survivable.
[02:42:19.940 --> 02:42:22.700]   Yep, but he did that for a particular reason.
[02:42:22.700 --> 02:42:31.060]   So he had access to, so he was also a quantitative modeler and had access to classified air force
[02:42:31.060 --> 02:42:32.060]   data.
[02:42:32.060 --> 02:42:36.300]   And it's from him actually that we have the word scenario because he was trying to figure
[02:42:36.300 --> 02:42:41.980]   out a way to get people to.
[02:42:41.980 --> 02:42:46.500]   The whole reason that he explained that it was survivable was because he wanted to extend
[02:42:46.500 --> 02:42:52.140]   the scenario of dropping bombs out to the nth degree for the purpose of people not doing
[02:42:52.140 --> 02:42:53.140]   it.
[02:42:53.140 --> 02:42:54.140]   Right.
[02:42:54.140 --> 02:42:56.220]   So in a way saying like everybody will be dead.
[02:42:56.220 --> 02:42:57.460]   I think about this.
[02:42:57.460 --> 02:42:58.460]   Yeah.
[02:42:58.460 --> 02:42:59.460]   Right.
[02:42:59.460 --> 02:43:06.020]   And it was saying we must imagine the unimaginable if we are going to confront deep uncertainty
[02:43:06.020 --> 02:43:09.220]   over very difficult questions.
[02:43:09.220 --> 02:43:12.620]   And so that's kind of the situation that we're in right now.
[02:43:12.620 --> 02:43:13.620]   Right.
[02:43:13.620 --> 02:43:16.180]   And so I think it's good for us to think through.
[02:43:16.180 --> 02:43:21.060]   Because if we're really focused on like what's the future of nuclear deterrence, then we're
[02:43:21.060 --> 02:43:26.060]   not also paying attention to the future of bio weapons and cognitive enhancement and
[02:43:26.060 --> 02:43:35.300]   like many, many other technologies that meld with our soft technologies or like market
[02:43:35.300 --> 02:43:38.900]   manipulation or all kinds of zero days.
[02:43:38.900 --> 02:43:42.460]   Maybe there's like an archive of zero days that nobody knows yet that I'll get released
[02:43:42.460 --> 02:43:46.360]   at once and cripple our financial systems for X amount of time.
[02:43:46.360 --> 02:43:51.180]   We see now what happens when we cripple our financial systems for even a week.
[02:43:51.180 --> 02:43:54.420]   Look at what's happened.
[02:43:54.420 --> 02:44:02.340]   The deal was my electromagnetic magnetic pulse look, you know, almost trivial because the
[02:44:02.340 --> 02:44:07.300]   idea of a new trial in bomb that would only destroy people not buildings.
[02:44:07.300 --> 02:44:11.260]   Well, I mean, the idea would destroy.
[02:44:11.260 --> 02:44:16.700]   What I mean, the idea in the 80s was that EMP would just destroy the electrical grid with
[02:44:16.700 --> 02:44:22.860]   better mednet, medieval levels, but bioengineering, that kind of stuff really worries me.
[02:44:22.860 --> 02:44:25.820]   Because of EMP, we're screwed.
[02:44:25.820 --> 02:44:32.500]   But bioengineering and the tailoring of viruses, that's a really nasty area to be in.
[02:44:32.500 --> 02:44:39.740]   This is where so similarly to AI, this is where this is a really thorny issue because
[02:44:39.740 --> 02:44:42.700]   viruses are not bad necessarily.
[02:44:42.700 --> 02:44:45.540]   Just like with AI, like tech isn't bad on its own.
[02:44:45.540 --> 02:44:48.220]   Viruses aren't necessarily bad.
[02:44:48.220 --> 02:44:50.940]   They potentially do bad things.
[02:44:50.940 --> 02:44:57.180]   You could also theoretically create viruses to fix code, right, to do good things.
[02:44:57.180 --> 02:45:03.300]   So anyhow, you could create a virus to fight a cancer.
[02:45:03.300 --> 02:45:04.940]   You could create a virus to fight another virus.
[02:45:04.940 --> 02:45:07.700]   There's all kinds of opportunities here.
[02:45:07.700 --> 02:45:12.940]   However, obviously you can also use all of this to humanity's detriment just as you
[02:45:12.940 --> 02:45:16.020]   could use AI and its evolution to humanity's detriment.
[02:45:16.020 --> 02:45:24.140]   So we have nuclear bombs or sort of like the Flintstones Bam Bam version of mayhem.
[02:45:24.140 --> 02:45:25.140]   Yeah.
[02:45:25.140 --> 02:45:30.500]   We're orders of magnitude more sophisticated now in the year 2020.
[02:45:30.500 --> 02:45:35.980]   And I'm much more concerned about all this other stuff than I am about how nuclear deterrence
[02:45:35.980 --> 02:45:41.500]   goes, even given what stupid things the Trump administration did with regard to Iran.
[02:45:41.500 --> 02:45:44.380]   What a smart conversation.
[02:45:44.380 --> 02:45:47.020]   Goodness I had the brandy because I just had nothing to say.
[02:45:47.020 --> 02:45:50.180]   I could just sit here and sit back and enjoy.
[02:45:50.180 --> 02:45:52.180]   Still finishing the whiskey.
[02:45:52.180 --> 02:45:57.780]   But I have to say when the idea first came up, Amy was like, I'm so into the air.
[02:45:57.780 --> 02:46:01.700]   I was just like, yeah, OK, I'll follow that lead because we do.
[02:46:01.700 --> 02:46:05.780]   We are the best looking best dressed podcast ever.
[02:46:05.780 --> 02:46:08.180]   I just want to say.
[02:46:08.180 --> 02:46:11.780]   Thank you for giving us the opportunity.
[02:46:11.780 --> 02:46:16.500]   It was honestly just the like I've been looking forward to this and the act of getting dressed
[02:46:16.500 --> 02:46:17.500]   up.
[02:46:17.500 --> 02:46:19.260]   I kind of feel like I should do this once a week.
[02:46:19.260 --> 02:46:21.700]   OK, even if I'm OK, I'm just aware.
[02:46:21.700 --> 02:46:25.420]   But let's just let's forget anybody else on this show, Karsten.
[02:46:25.420 --> 02:46:27.980]   It's Amy and Ian from now.
[02:46:27.980 --> 02:46:32.260]   Actually, sometimes we have to talk about shiny objects every once in a while.
[02:46:32.260 --> 02:46:33.420]   You guys are too smart.
[02:46:33.420 --> 02:46:34.420]   That's the problem.
[02:46:34.420 --> 02:46:39.580]   And so we ended up talking a lot about some deep philosophy philosophical questions.
[02:46:39.580 --> 02:46:41.460]   But we're still geek.
[02:46:41.460 --> 02:46:42.460]   So give us a shiny.
[02:46:42.460 --> 02:46:43.460]   Give us a shiny.
[02:46:43.460 --> 02:46:44.460]   More shiny.
[02:46:44.460 --> 02:46:45.460]   No, I'm done with the shiny.
[02:46:45.460 --> 02:46:46.460]   We're done.
[02:46:46.460 --> 02:46:50.220]   But they did want me to ask you guys and they think Amy has some special insight of this.
[02:46:50.220 --> 02:46:52.940]   I don't know.
[02:46:52.940 --> 02:46:55.100]   Kim Jong Un, dead or alive.
[02:46:55.100 --> 02:46:57.740]   What do you think?
[02:46:57.740 --> 02:46:58.740]   What do you know?
[02:46:58.740 --> 02:46:59.740]   What do you hear?
[02:46:59.740 --> 02:47:00.740]   What's the over under on this one?
[02:47:00.740 --> 02:47:01.900]   That's the over under.
[02:47:01.900 --> 02:47:03.420]   I think he's dead.
[02:47:03.420 --> 02:47:04.820]   I think he is too.
[02:47:04.820 --> 02:47:05.820]   I do.
[02:47:05.820 --> 02:47:06.820]   Now, there's a.
[02:47:06.820 --> 02:47:07.820]   I don't know what it means.
[02:47:07.820 --> 02:47:10.220]   It's mostly destabilizing, right?
[02:47:10.220 --> 02:47:12.220]   Well, I mean, Occam's.
[02:47:12.220 --> 02:47:18.780]   Okay, Occam's razor was just his dead in the Chinese, Hong Kong and Korean sources are
[02:47:18.780 --> 02:47:21.860]   reporting things are slightly up.
[02:47:21.860 --> 02:47:26.020]   But let's not forget that we it's enormously difficult to get information out of North
[02:47:26.020 --> 02:47:27.020]   Korea.
[02:47:27.020 --> 02:47:31.300]   And we've had that people are dead before that showed up a lot.
[02:47:31.300 --> 02:47:36.020]   What I mean, the state department, the US state department, admitted that with his dad,
[02:47:36.020 --> 02:47:40.260]   they based an awful lot of their calculations on how much Hennessy vodka be delivered to
[02:47:40.260 --> 02:47:42.260]   North Korea that month.
[02:47:42.260 --> 02:47:43.740]   You know what I mean?
[02:47:43.740 --> 02:47:47.140]   Now it's all about the train, right?
[02:47:47.140 --> 02:47:51.780]   It's his train has been parked for three whole days at the Wu San.
[02:47:51.780 --> 02:47:52.780]   What's going on?
[02:47:52.780 --> 02:47:54.260]   It's based on that.
[02:47:54.260 --> 02:47:55.260]   So satellite imaging.
[02:47:55.260 --> 02:48:00.180]   I think the general is so like there was a Chinese doctor that had apparently treated
[02:48:00.180 --> 02:48:01.180]   him.
[02:48:01.180 --> 02:48:02.180]   I don't know.
[02:48:02.180 --> 02:48:07.980]   I lived in Japan for a long time and friends with a lot of Japanese reporters, they don't
[02:48:07.980 --> 02:48:11.860]   tend to report things unless they're verified.
[02:48:11.860 --> 02:48:13.700]   Japanese press doesn't really operate that way.
[02:48:13.700 --> 02:48:18.820]   So the fact that it came out of a reputable news source in Japan also makes me feel like
[02:48:18.820 --> 02:48:20.500]   it's a little bit more credible.
[02:48:20.500 --> 02:48:25.740]   Yeah, but that means we've got a nuclear power with no clear leader.
[02:48:25.740 --> 02:48:27.700]   I mean, the leader.
[02:48:27.700 --> 02:48:29.540]   Well, his leader, right?
[02:48:29.540 --> 02:48:30.540]   Well, yeah.
[02:48:30.540 --> 02:48:35.180]   Well, I mean, technically there's also other family members around.
[02:48:35.180 --> 02:48:38.580]   But you see, technically his father is still ahead of state.
[02:48:38.580 --> 02:48:39.980]   It's a necrocracy.
[02:48:39.980 --> 02:48:44.380]   His father is still technically the head of the North North Korean state.
[02:48:44.380 --> 02:48:45.900]   But yeah, he's got a sister.
[02:48:45.900 --> 02:48:46.900]   He's got cousins.
[02:48:46.900 --> 02:48:49.420]   They're the military is going in there.
[02:48:49.420 --> 02:48:55.660]   I do wonder what's going to happen though with North Korea when they lose the nuclear
[02:48:55.660 --> 02:48:56.660]   focus.
[02:48:56.660 --> 02:48:58.260]   You know, it's like, what else are they going to do?
[02:48:58.260 --> 02:49:01.660]   It's like, well, where the internet's scamming the nation of the world.
[02:49:01.660 --> 02:49:03.780]   I could tell you exactly what they could do.
[02:49:03.780 --> 02:49:08.860]   You've got a population of people who could overnight become the manufacturing capital
[02:49:08.860 --> 02:49:09.860]   of the world.
[02:49:09.860 --> 02:49:11.060]   I mean, easily.
[02:49:11.060 --> 02:49:14.220]   And these are people who could absolutely.
[02:49:14.220 --> 02:49:15.540]   So here's their opportunity.
[02:49:15.540 --> 02:49:18.180]   Let's get rid of the Kim family.
[02:49:18.180 --> 02:49:19.460]   Let's become a republic.
[02:49:19.460 --> 02:49:22.300]   Would China let them do that though?
[02:49:22.300 --> 02:49:23.300]   I don't know.
[02:49:23.300 --> 02:49:24.780]   And we didn't talk about Russia.
[02:49:24.780 --> 02:49:25.780]   Yeah.
[02:49:25.780 --> 02:49:26.780]   Yeah.
[02:49:26.780 --> 02:49:31.940]   China might be willing to let them do that if just to avoid floods of starving North Koreans
[02:49:31.940 --> 02:49:33.420]   heading over their border.
[02:49:33.420 --> 02:49:34.420]   Yeah.
[02:49:34.420 --> 02:49:38.460]   But at the same time, yeah, they could become the manufacturing capital of the world.
[02:49:38.460 --> 02:49:39.780]   But in what conditions?
[02:49:39.780 --> 02:49:40.780]   I would like.
[02:49:40.780 --> 02:49:44.180]   Oh, no, the human rights abuses would be horrible.
[02:49:44.180 --> 02:49:45.660]   But yeah, I know.
[02:49:45.660 --> 02:49:48.900]   I mean, the amount of human rights you can't do anything.
[02:49:48.900 --> 02:49:51.900]   So, okay, at least we'll get the new phone.
[02:49:51.900 --> 02:49:54.860]   Well, you can refuse to buy from them.
[02:49:54.860 --> 02:50:01.900]   But yeah, I mean, it's just like, look, North Korea is, it's the, yeah, I'm trying to think
[02:50:01.900 --> 02:50:02.900]   of the equivalent.
[02:50:02.900 --> 02:50:06.540]   Maybe Yugoslavia in the late 90s.
[02:50:06.540 --> 02:50:09.180]   I mean, it's going to fall apart.
[02:50:09.180 --> 02:50:14.580]   And it's the question is whether it falls apart in a way that it benefits the rest of
[02:50:14.580 --> 02:50:18.140]   the world or whether we pay the price.
[02:50:18.140 --> 02:50:19.780]   And it's, they've got nukes.
[02:50:19.780 --> 02:50:22.380]   We've got to be careful on this one.
[02:50:22.380 --> 02:50:23.380]   Yeah.
[02:50:23.380 --> 02:50:24.820]   Well, I'm glad.
[02:50:24.820 --> 02:50:30.500]   Well, that's a really, that's a really such an upbeat show the whole time, folks.
[02:50:30.500 --> 02:50:35.180]   I would happen to say, however, that if you're looking to adopt a cat, this is a really good
[02:50:35.180 --> 02:50:38.180]   time to do it because there's a lot of cats out there.
[02:50:38.180 --> 02:50:40.700]   They will bring you joy and we can move on from this.
[02:50:40.700 --> 02:50:45.700]   I have some red foxes, some brown or red foxes that I have available.
[02:50:45.700 --> 02:50:48.940]   If anybody would like to pick them up in our backyard, there's a whole family is moved
[02:50:48.940 --> 02:50:49.940]   in.
[02:50:49.940 --> 02:50:53.060]   We've got an opossums in our backyard and they're great.
[02:50:53.060 --> 02:50:58.100]   The opossums, I never encountered them before I came here, but an opossum looks like a rat
[02:50:58.100 --> 02:51:00.820]   in a bowl had a really bad one night stand and this was the
[02:51:00.820 --> 02:51:02.620]   strangest animals.
[02:51:02.620 --> 02:51:04.180]   Yeah, but they're really great.
[02:51:04.180 --> 02:51:06.020]   I found out they eat takes.
[02:51:06.020 --> 02:51:07.020]   They're not rabbit.
[02:51:07.020 --> 02:51:08.420]   You know, it's not great.
[02:51:08.420 --> 02:51:11.580]   The cry of the red fox, John, you have my audio.
[02:51:11.580 --> 02:51:13.620]   This is what they sound like.
[02:51:13.620 --> 02:51:16.420]   You do not want to hear this in your backyard.
[02:51:16.420 --> 02:51:17.420]   Famine.
[02:51:17.420 --> 02:51:18.420]   Yeah.
[02:51:18.420 --> 02:51:19.420]   Yeah.
[02:51:19.420 --> 02:51:20.420]   Yeah.
[02:51:20.420 --> 02:51:21.420]   Yeah.
[02:51:21.420 --> 02:51:22.420]   Yeah.
[02:51:22.420 --> 02:51:26.940]   So, so we kept hearing that and in fact it was like in stereo, there was somebody in
[02:51:26.940 --> 02:51:27.940]   the front and some in the back.
[02:51:27.940 --> 02:51:28.940]   I don't know if they're mating.
[02:51:28.940 --> 02:51:30.540]   I don't know what we kept hearing that.
[02:51:30.540 --> 02:51:35.740]   And I said, I think it's those foxes I saw the other day and thank God for YouTube because
[02:51:35.740 --> 02:51:41.220]   I immediately I searched for the scream of the red fox and I found it immediately.
[02:51:41.220 --> 02:51:43.540]   And so now we know what the red fox.
[02:51:43.540 --> 02:51:46.540]   That's exactly what I was hearing in my backyard the other day.
[02:51:46.540 --> 02:51:51.700]   And you see, I grew up in London where we now have more foxes in urban centers than we
[02:51:51.700 --> 02:51:52.900]   do in the countryside.
[02:51:52.900 --> 02:51:56.860]   There was a jellyfish spotted in the canals of Venice, ladies and gentlemen.
[02:51:56.860 --> 02:52:01.660]   Well, I mean, but when you when you hear foxes getting it on, it sounds like someone's
[02:52:01.660 --> 02:52:02.660]   being murdered.
[02:52:02.660 --> 02:52:03.660]   Yeah.
[02:52:03.660 --> 02:52:04.660]   Yeah.
[02:52:04.660 --> 02:52:05.660]   And it's just terrifying.
[02:52:05.660 --> 02:52:06.660]   And they're totally fearless.
[02:52:06.660 --> 02:52:07.660]   Yeah.
[02:52:07.660 --> 02:52:08.660]   Hey, more urban wildlife.
[02:52:08.660 --> 02:52:09.660]   I'm all for it.
[02:52:09.660 --> 02:52:10.660]   Yeah.
[02:52:10.660 --> 02:52:11.660]   Yeah.
[02:52:11.660 --> 02:52:14.220]   In Thompson, you look good, Mr. Bond.
[02:52:14.220 --> 02:52:15.420]   Very good.
[02:52:15.420 --> 02:52:19.340]   I had to take the jacket off when I didn't tie my bow tie myself like yourself.
[02:52:19.340 --> 02:52:21.940]   So respect to you, Leo, for actually doing it.
[02:52:21.940 --> 02:52:24.980]   Mine's kind of lopsided because I realize it's too.
[02:52:24.980 --> 02:52:26.540]   I have to set adjust the way.
[02:52:26.540 --> 02:52:30.100]   Yes, but you can pull it off in a dramatic fashion and have the two side.
[02:52:30.100 --> 02:52:37.740]   I mean, this is exactly this is the piece of me of cool fly to the moon.
[02:52:37.740 --> 02:52:40.700]   Let me play among the stars.
[02:52:40.700 --> 02:52:42.380]   Thank you, Amy Webb.
[02:52:42.380 --> 02:52:44.260]   Everybody should go to Amy Web.io.
[02:52:44.260 --> 02:52:45.420]   You should buy all our books.
[02:52:45.420 --> 02:52:48.300]   I am so pleased to hear that the new book is coming.
[02:52:48.300 --> 02:52:49.740]   That's very exciting.
[02:52:49.740 --> 02:52:50.740]   I will.
[02:52:50.740 --> 02:52:51.740]   Thank you.
[02:52:51.740 --> 02:52:52.740]   I can't wait to read that.
[02:52:52.740 --> 02:52:59.940]   And don't forget the future today Institute where Amy hangs her hat 18 hours a day.
[02:52:59.940 --> 02:53:02.140]   Not normally just the past couple of weeks.
[02:53:02.140 --> 02:53:05.540]   Oh, are you still offering your book?
[02:53:05.540 --> 02:53:06.540]   Yep.
[02:53:06.540 --> 02:53:09.380]   So all of our research is free.
[02:53:09.380 --> 02:53:12.700]   All of our frameworks and tools are open source and free.
[02:53:12.700 --> 02:53:18.180]   And as of tomorrow, we have an interactive version of the trend report, which has 406
[02:53:18.180 --> 02:53:20.620]   trends this year.
[02:53:20.620 --> 02:53:23.700]   We made the whole thing interactive, so it'll be live tomorrow.
[02:53:23.700 --> 02:53:27.180]   And we also have a database that's going to launch in a week or so.
[02:53:27.180 --> 02:53:30.700]   So you can search through all the different trends and see how they interact with different
[02:53:30.700 --> 02:53:31.700]   industries and.
[02:53:31.700 --> 02:53:32.700]   Yep.
[02:53:32.700 --> 02:53:37.340]   And if you want, if you're in the Department of Defense and you're looking for a game theory
[02:53:37.340 --> 02:53:44.500]   strategy, they already know where to find you.
[02:53:44.500 --> 02:53:45.500]   I bet they do.
[02:53:45.500 --> 02:53:47.380]   Could I leave with one piece of it?
[02:53:47.380 --> 02:53:49.300]   One last piece of advice.
[02:53:49.300 --> 02:53:50.500]   Yes, please.
[02:53:50.500 --> 02:53:55.380]   So if you are sitting at home with your nine year old and your husband and nobody wants
[02:53:55.380 --> 02:54:00.900]   to do bedtime and you happen to be flipping channels to get somebody to try to fall asleep.
[02:54:00.900 --> 02:54:04.340]   And Tommy comes on PBS.
[02:54:04.340 --> 02:54:06.580]   Don't watch it with your nine year old as we did.
[02:54:06.580 --> 02:54:12.340]   Tommy, the who that one with the with the with the Gypsy Queen played by Tina Turner, that
[02:54:12.340 --> 02:54:13.340]   Tommy.
[02:54:13.340 --> 02:54:15.420]   Luckily, we had never seen it.
[02:54:15.420 --> 02:54:19.380]   So luckily it came on after the acid song situation.
[02:54:19.380 --> 02:54:20.380]   Oh, she's.
[02:54:20.380 --> 02:54:21.380]   Yeah.
[02:54:21.380 --> 02:54:23.380]   Did you give your daughter nightmares?
[02:54:23.380 --> 02:54:26.940]   She's had a lot of questions about things.
[02:54:26.940 --> 02:54:29.780]   I'm the Gypsy, the acid.
[02:54:29.780 --> 02:54:31.280]   I'm hearing.
[02:54:31.280 --> 02:54:33.460]   I'm hearing love, laughter in the background.
[02:54:33.460 --> 02:54:36.700]   This was the pinball wizard thing or the.
[02:54:36.700 --> 02:54:40.300]   So we turned it on right when Elton John was singing pinball wizard.
[02:54:40.300 --> 02:54:42.020]   So we came in at that.
[02:54:42.020 --> 02:54:44.780]   And then we stayed till the end.
[02:54:44.780 --> 02:54:47.940]   So my my parting advice is don't do that.
[02:54:47.940 --> 02:54:48.940]   Moments in parenting.
[02:54:48.940 --> 02:54:49.940]   Okay.
[02:54:49.940 --> 02:54:51.780]   What are you watching?
[02:54:51.780 --> 02:54:53.340]   What is a good show?
[02:54:53.340 --> 02:54:54.780]   Dave on Hulu.
[02:54:54.780 --> 02:55:01.540]   I know like if you have access to Hulu, this is a Kirby or enthusiasm.
[02:55:01.540 --> 02:55:02.540]   Younger.
[02:55:02.540 --> 02:55:03.540]   I'm sorry.
[02:55:03.540 --> 02:55:04.540]   You made.
[02:55:04.540 --> 02:55:06.460]   Oh my God, it is great.
[02:55:06.460 --> 02:55:07.460]   Okay.
[02:55:07.460 --> 02:55:11.700]   I'll give you I'll trade you because have you watched devs yet on Hulu?
[02:55:11.700 --> 02:55:13.420]   I'm waiting for there to be more.
[02:55:13.420 --> 02:55:15.340]   I like to bank them and then watch them.
[02:55:15.340 --> 02:55:16.340]   They're all done.
[02:55:16.340 --> 02:55:17.340]   They're all done.
[02:55:17.340 --> 02:55:18.340]   They're all done.
[02:55:18.340 --> 02:55:19.340]   That'll be next.
[02:55:19.340 --> 02:55:20.340]   Was a great.
[02:55:20.340 --> 02:55:21.340]   I've been waiting.
[02:55:21.340 --> 02:55:22.340]   Yeah.
[02:55:22.340 --> 02:55:23.340]   Nick.
[02:55:23.340 --> 02:55:24.340]   First of all, Nick Offerman shows he's got some real chops.
[02:55:24.340 --> 02:55:25.340]   I love the plot.
[02:55:25.340 --> 02:55:27.340]   I think you'll be very interested in it.
[02:55:27.340 --> 02:55:28.340]   Okay.
[02:55:28.340 --> 02:55:31.620]   And it has, you know, it's a challenging show.
[02:55:31.620 --> 02:55:33.380]   And it has a perfect ending.
[02:55:33.380 --> 02:55:38.180]   I mean, I'm surprised because so many shows these days are great right to the very last
[02:55:38.180 --> 02:55:40.820]   episode and then you go, well, that's annoying.
[02:55:40.820 --> 02:55:42.940]   This one I thought had a very nice ending.
[02:55:42.940 --> 02:55:43.940]   Highly recommended.
[02:55:43.940 --> 02:55:45.740]   And it's right up your alley.
[02:55:45.740 --> 02:55:46.740]   Okay.
[02:55:46.740 --> 02:55:47.740]   Sorry.
[02:55:47.740 --> 02:55:48.740]   Yeah, I've been waiting.
[02:55:48.740 --> 02:55:49.740]   I'll watch it for sure.
[02:55:49.740 --> 02:55:54.740]   They've they built a quantum computer that has so much power.
[02:55:54.740 --> 02:55:56.900]   It can predict the future.
[02:55:56.900 --> 02:55:57.900]   See?
[02:55:57.900 --> 02:55:58.900]   Okay.
[02:55:58.900 --> 02:55:59.900]   Would you like that, Amy?
[02:55:59.900 --> 02:56:01.540]   Well, when did you like one of them?
[02:56:01.540 --> 02:56:02.540]   Okay.
[02:56:02.540 --> 02:56:03.540]   Yeah.
[02:56:03.540 --> 02:56:04.540]   Okay.
[02:56:04.540 --> 02:56:09.260]   I would say from a British perspective, two series to watch Derry Girls on Netflix.
[02:56:09.260 --> 02:56:10.260]   Derry Girls.
[02:56:10.260 --> 02:56:11.260]   I love Derry.
[02:56:11.260 --> 02:56:12.260]   D-E-R-Y, not D-A-I-R-Y.
[02:56:12.260 --> 02:56:14.580]   Yeah, about life in Northern Ireland.
[02:56:14.580 --> 02:56:15.580]   Okay.
[02:56:15.580 --> 02:56:20.100]   And also, if you haven't seen Tim, Tim Pegg in Spaced, get on there.
[02:56:20.100 --> 02:56:23.260]   Spaced is the ultimate 90s British TV show.
[02:56:23.260 --> 02:56:25.980]   So that's what I'd be mostly stocking up on this week.
[02:56:25.980 --> 02:56:28.980]   I ended up subscribing to BritBox just because of you.
[02:56:28.980 --> 02:56:29.980]   Oh, yeah.
[02:56:29.980 --> 02:56:31.500]   It's a great channel.
[02:56:31.500 --> 02:56:32.500]   I mean, okay.
[02:56:32.500 --> 02:56:38.540]   I mean, I can't obviously endorse it, but it keeps me in my British TV.
[02:56:38.540 --> 02:56:40.900]   What do you can't endorse it?
[02:56:40.900 --> 02:56:44.180]   Well, I mean, I can, but this is kind of all-
[02:56:44.180 --> 02:56:46.460]   I think you have endorsed it.
[02:56:46.460 --> 02:56:49.500]   Given the platform, okay, they're terrible.
[02:56:49.500 --> 02:56:50.500]   Don't watch them.
[02:56:50.500 --> 02:56:51.500]   They're really bad.
[02:56:51.500 --> 02:56:56.020]   Thank you, everybody.
[02:56:56.020 --> 02:56:57.940]   This was a great time.
[02:56:57.940 --> 02:57:04.700]   I'm so glad that we were able to dress up and pretend to be grownups on the Internet.
[02:57:04.700 --> 02:57:06.060]   That's what it's for.
[02:57:06.060 --> 02:57:07.060]   Thank you, Amy Webb.
[02:57:07.060 --> 02:57:08.060]   Thank you, Ian Thompson.
[02:57:08.060 --> 02:57:10.380]   Thank you for everybody who watched.
[02:57:10.380 --> 02:57:12.020]   We almost made three hours.
[02:57:12.020 --> 02:57:15.780]   If another three and a half minutes, this will be a three-hour show.
[02:57:15.780 --> 02:57:16.780]   Should we just sing a song?
[02:57:16.780 --> 02:57:17.780]   No, no.
[02:57:17.780 --> 02:57:19.660]   Let's see if we can get out of here before three hours.
[02:57:19.660 --> 02:57:21.420]   We do Twitter every Sunday afternoon.
[02:57:21.420 --> 02:57:23.860]   Full world.
[02:57:23.860 --> 02:57:28.380]   Every Sunday afternoon, about 230 Pacific, that's 530 Eastern, 2130 UTC.
[02:57:28.380 --> 02:57:29.940]   You can watch it live.
[02:57:29.940 --> 02:57:34.860]   Audio and video streaming at twit.tv/live or just ask your Amazon Echo or your Google
[02:57:34.860 --> 02:57:41.060]   voice say, "Play Twit Live," and you will get it somehow, some magical way.
[02:57:41.060 --> 02:57:42.060]   It will play for you.
[02:57:42.060 --> 02:57:45.980]   You can also ask for individual episodes if you say, "Play this week in tech."
[02:57:45.980 --> 02:57:50.260]   You'll get the most recent episode, which is not this one yet because we're still working
[02:57:50.260 --> 02:57:51.260]   on it.
[02:57:51.260 --> 02:57:52.420]   It hasn't ended yet.
[02:57:52.420 --> 02:57:56.060]   You can also get on-demand versions of this show after the fact.
[02:57:56.060 --> 02:58:01.180]   All of our shows at our website, twit.tv, subscribe in your favorite podcast client.
[02:58:01.180 --> 02:58:03.780]   That way you'll get it automatically.
[02:58:03.780 --> 02:58:07.180]   I used to be back in the dim dark past.
[02:58:07.180 --> 02:58:11.900]   I used to say, "So you'd have it for your Monday morning commute, but now you'll have
[02:58:11.900 --> 02:58:16.220]   it for your Monday morning sitting around in your jammies eating pizza."
[02:58:16.220 --> 02:58:18.300]   So that's even better, right?
[02:58:18.300 --> 02:58:19.500]   Thank you for being here.
[02:58:19.500 --> 02:58:20.500]   Stay safe.
[02:58:20.500 --> 02:58:21.500]   Stay healthy.
[02:58:21.500 --> 02:58:22.500]   Twits.
[02:58:22.500 --> 02:58:23.500]   This is amazing.
[02:58:23.500 --> 02:58:23.500]   Bye-bye.
[02:58:23.500 --> 02:58:33.380]   See you next time.
[02:58:33.380 --> 02:58:43.380]   [BLANK_AUDIO]

