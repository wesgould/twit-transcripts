;FFMETADATA1
title=SCOTUS Didn't Read My Briefs
artist=Leo Laporte, Cathy Gellis, Harry McCracken, Amanda Silberling
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-05-22
track=928
language=English
genre=Podcast
comment=<p>TikTok Montana ban, Instagram&\#039\;s Twitter clone, Tears of the Kingdom</p>\

encoded_by=Uniblab 5.3
date=2023
encoder=Lavf59.27.100

[00:00:00.000 --> 00:00:05.360]   It's time for Twit this week in Tech. Two people in studio Harry McCracken, the technologizers here.
[00:00:05.360 --> 00:00:11.040]   Kathy Galis from Tech Dirt is here. Also Amanda Silverlink joins us for the first time ever from Tech Crunch.
[00:00:11.040 --> 00:00:16.080]   We will talk about the three big Supreme Court decisions that came down this week.
[00:00:16.080 --> 00:00:21.120]   Turns out Kathy wrote Amicus briefs for two of them. She says it's one win, one loss.
[00:00:21.120 --> 00:00:27.600]   We'll find out what that means. We'll also talk about the change in streaming TV and the failure
[00:00:27.600 --> 00:00:33.520]   of pay TV services and a whole lot more. It's all coming up next. We might even play a little D&D
[00:00:33.520 --> 00:00:35.280]   next on Twit.
[00:00:35.280 --> 00:00:43.760]   Podcasts you love from people you trust. This is Twit.
[00:00:43.760 --> 00:00:55.920]   This is Twit. This week in Tech, episode 928 recorded Sunday, May 21st,
[00:00:55.920 --> 00:00:59.760]   2023. Skodas didn't read my briefs.
[00:00:59.760 --> 00:01:07.440]   This week in Tech is brought to you by GrammarlyGo. You'll be amazed at what you can do with GrammarlyGo.
[00:01:07.440 --> 00:01:13.680]   Go to Grammarly.com/go to download and learn more about GrammarlyGo.
[00:01:13.680 --> 00:01:20.960]   And by Noom. Stop chasing health trends and build sustainable, healthy habits with Noom's
[00:01:20.960 --> 00:01:26.480]   psychology-based approach. Check out Noom's first-ever book, The Noom Mindset, a deep dive into the
[00:01:26.480 --> 00:01:30.960]   psychology of behavior change available to buy now wherever books are sold. And don't forget to
[00:01:30.960 --> 00:01:39.520]   sign up for your trial at noom.com/twits. And by Duo. Duo protects against breaches with a
[00:01:39.520 --> 00:01:45.600]   leading access management suite providing strong, multi-layered defenses to only allow legitimate
[00:01:45.600 --> 00:01:51.520]   users in. For any organization concerned about being breached that needs a solution fast,
[00:01:51.520 --> 00:01:59.680]   Duo quickly enables strong security and improves user productivity. Visit signup.duo.com today
[00:01:59.680 --> 00:02:06.160]   for a free trial. And by Mint Mobile. Inflation is everywhere. Whether it's gas, utilities,
[00:02:06.160 --> 00:02:11.200]   or your favorite streaming services, thankfully Mint Mobile can give you a much needed break.
[00:02:11.200 --> 00:02:16.400]   Get your new wireless plan for just 15 bucks a month and get the plan shipped to your door for free
[00:02:16.400 --> 00:02:19.760]   when you go to mintmumble.com/twit.
[00:02:19.760 --> 00:02:31.840]   It's time for Twit this week at Tech, the show we cover the week's tech news. I'm a little scared.
[00:02:31.840 --> 00:02:37.760]   I have people in the studio today. This is Scary Harry McCracken. Hello, good to see you.
[00:02:37.760 --> 00:02:44.560]   Hey, how? Fast company, the technologizer. Your name you might have heard your ears burning
[00:02:44.560 --> 00:02:49.280]   earlier this week because people are saying, well, no one uses an iPad for day to day work.
[00:02:49.280 --> 00:02:55.120]   And I said, well, Harry does. Absolutely. I have for like 12 years. 11 inch approach now.
[00:02:55.120 --> 00:02:59.920]   And you're feeling pretty good because now you get final cut and logic on there.
[00:02:59.920 --> 00:03:05.760]   Okay, well, soon our final cut at least is exciting. You're ready. Also in studio with us from Tech
[00:03:05.760 --> 00:03:16.400]   Dirt, our favorite intellectual person. She's an attorney. Kathy Gallis. Thanks for having me.
[00:03:16.400 --> 00:03:22.640]   Hi, Kathy. CGcancel.com here on Maston on that Kathy Gallis, but more importantly, Kathy wrote
[00:03:22.640 --> 00:03:28.960]   not one but two Amicus briefs for big, big Supreme Court decisions that came down this week.
[00:03:28.960 --> 00:03:34.720]   We were going to talk about both of them. One had a lot to do with fair use. The other turns out
[00:03:34.720 --> 00:03:38.640]   didn't have anything to do with Section 230. It could have though. It could have, it will
[00:03:38.640 --> 00:03:46.480]   reverberate in the same space. It will reverberate in the same space. Wow. Okay, I'm gonna think about
[00:03:46.480 --> 00:03:50.640]   that for a little while while I introduce Amanda Silverling, who I've been wanting to get on this
[00:03:50.640 --> 00:03:56.000]   show forever and ever. She writes about culture. She's the senior culture writer at Tech Crunch.
[00:03:56.000 --> 00:04:00.560]   And it's great to see you. You've been on Tech News Weekly before, but we never got you on this
[00:04:00.560 --> 00:04:05.520]   show. So welcome. Yeah, it's exciting to be on the show. I don't want to say anything, but it looks
[00:04:05.520 --> 00:04:12.560]   like somebody stole your guitar. Oh, so yeah, it's, man, I should have that in the background.
[00:04:12.560 --> 00:04:17.840]   It's like a neon pink base and it makes me look really cool, but it's in the other room right now
[00:04:17.840 --> 00:04:22.880]   because I had a base lesson the other day. Because you were playing it. That's a good reason.
[00:04:23.920 --> 00:04:30.800]   Yeah, excellent. So it's in the other room inside of a black case, but it shouldn't be there looking
[00:04:30.800 --> 00:04:35.840]   cool. I'm playing the wrong way. So before we get to the Supreme Court, because I do want to
[00:04:35.840 --> 00:04:41.280]   get to the Supreme Court, but I also, Amanda had a scoop that I thought was really interesting.
[00:04:41.280 --> 00:04:50.160]   I kind of want to mention this. It looks like you saw an email or a from Instagram
[00:04:50.160 --> 00:04:56.880]   that Instagram is going to launch a Twitter clone. See, I love that you think it's a scoop because
[00:04:56.880 --> 00:05:03.440]   I think Leah Haberman and Matt Navarro posted this before me. So thank you for giving me the credit.
[00:05:03.440 --> 00:05:08.000]   Forget Leah. Tell me. I thought it was a scoop. Leah tweeted it. That doesn't count. You wrote
[00:05:08.000 --> 00:05:13.120]   the article. That's what matters. Yeah, I mean, also actually before we got on, apparently,
[00:05:14.480 --> 00:05:24.160]   the incoming CEO of Twitter, Linda, Yasino, or Yacarena, sorry, she just retweeted the tech
[00:05:24.160 --> 00:05:27.760]   crunch tweet of that article. So thank you for the traffic, Linda.
[00:05:27.760 --> 00:05:34.560]   That's interesting. I wonder why the new CEO of Twitter would be promoting a major competitor
[00:05:34.560 --> 00:05:39.760]   from Meta. Well, she said, I think she said like game on or something.
[00:05:39.760 --> 00:05:47.040]   It's like the welcome. Seriously. Instagram is validating the market by entering up.
[00:05:47.040 --> 00:05:52.880]   Yeah. I think our belief in rational behavior by anybody running the company that is Twitter
[00:05:52.880 --> 00:06:00.160]   these days might be overestimating. I wonder what Elon will say. So Instagram is, and this is the
[00:06:00.160 --> 00:06:07.040]   slide, I guess, that Leah Haberman got. Yeah. Instagram is going to launch up. It looks a lot
[00:06:07.040 --> 00:06:10.160]   like Twitter. It's text based, right? Not photo based.
[00:06:10.160 --> 00:06:17.920]   It says, quickly build your audience in one tap. Anyone can follow the accounts they follow on
[00:06:17.920 --> 00:06:23.920]   Instagram. So it's kind of just Instagram comments in a kind of a separate space. Yes.
[00:06:23.920 --> 00:06:31.200]   That's what it feels like a little bit. Yeah. It's, it seems like it's going to be very similar
[00:06:31.200 --> 00:06:39.280]   to Twitter. But basically, if you have an Instagram account, then you can just carry over your handle
[00:06:39.280 --> 00:06:45.680]   and your verification status if you're verified. And then it'll give a little notification like
[00:06:45.680 --> 00:06:55.280]   Amanda is now on whatever they name the Twitter clone. But yeah, I mean, this is all very speculative
[00:06:55.280 --> 00:07:01.920]   Instagram wouldn't give me a comment on the record about it. But there was another blog.
[00:07:01.920 --> 00:07:07.920]   I'm, it's in the article, but I'm forgetting the name now, but they did give them a comment and
[00:07:07.920 --> 00:07:13.680]   they were like, this is in development, essentially. But it is interesting that they are being hush,
[00:07:13.680 --> 00:07:21.680]   hush. However, I did see an email that they sent to a creator where they're basically trying to get
[00:07:21.680 --> 00:07:27.920]   like big name creators, actors, athletes to just get on the platform from the get go because
[00:07:27.920 --> 00:07:37.520]   that's kind of what drives growth. If people are like, oh man, LeBron is over here. Sure.
[00:07:37.520 --> 00:07:43.680]   That's totally what's what made Blue Sky kind of explode was suddenly some big names like AOC.
[00:07:43.680 --> 00:07:50.560]   She follows me, which, wow, it's an honor. She's never engaged with any of my Blue Sky skates.
[00:07:50.560 --> 00:07:55.280]   That's what they're called officially. But she does follow me on there. And it's like,
[00:07:55.280 --> 00:08:00.560]   it's an honor and a privilege that I am afraid she's going to unfollow me and then I'm going to
[00:08:00.560 --> 00:08:05.840]   feel bad. Just say nothing. Say nothing. So the new blog you were talking about is Money Sent,
[00:08:05.840 --> 00:08:10.960]   Money Control, rather. Yes. And they say it's going to be called either it's code name P92 or
[00:08:10.960 --> 00:08:15.920]   Barcelona, neither of which will be the name. I'm sure. Yeah.
[00:08:17.840 --> 00:08:22.560]   Meta said in a statement to Money Control, we're exploring a standalone decentralized social
[00:08:22.560 --> 00:08:27.120]   network for sharing text updates, we believe there's an opportunity for a separate space where
[00:08:27.120 --> 00:08:31.280]   creators and public figures can share timely updates about their interests. Yeah, we call it
[00:08:31.280 --> 00:08:36.320]   Twitter. We believe there is an opportunity and has been. It's interesting, though, they say
[00:08:36.320 --> 00:08:40.000]   decentralized because you don't think of Instagram is decentralized in any way.
[00:08:40.720 --> 00:08:49.920]   Nope. Your article says it will actually, this is from the slide, it will be compatible with
[00:08:49.920 --> 00:08:56.240]   other apps like Mastodon. That doesn't mean we'll be on the Fediverse necessarily.
[00:08:56.240 --> 00:09:01.280]   I don't know what that means. Yeah, it's really confusing wording because
[00:09:01.280 --> 00:09:08.000]   like we were talking about this before we went live that it's just confusing because you would
[00:09:08.000 --> 00:09:12.720]   think that if it's compatible with Mastodon, then it would be built on activity pub.
[00:09:12.720 --> 00:09:21.760]   But maybe it's not. Maybe like, are we going to have like Amanda at Instagram.com? Is that how
[00:09:21.760 --> 00:09:27.600]   it's going to be interoperable? It's like the tech behind it feels very vague, but I guess
[00:09:27.600 --> 00:09:34.880]   there's some they could be cool if it's decentralized. Yeah, there's some like little bits of I when
[00:09:34.880 --> 00:09:41.200]   I saw this get tweeted and social media, I saw a lot of cynicism behind it. But as we're talking
[00:09:41.200 --> 00:09:46.800]   about it, I actually think there's some little sparks of a good idea here, but it really feels like
[00:09:46.800 --> 00:09:51.600]   not enough meetings have actually happened in the company to work this all out because
[00:09:51.600 --> 00:09:57.920]   I think it's actually really interesting if Instagram has something that is their own community that
[00:09:57.920 --> 00:10:03.200]   they foster and they moderate and somehow monetize, but that they're using a technology that's less
[00:10:03.200 --> 00:10:08.880]   proprietary. Like that's really interesting, but that is a fundamental shift in the way that a
[00:10:08.880 --> 00:10:14.720]   Facebook property has operated. So it could be a really big idea, but that's got to kind of happen
[00:10:14.720 --> 00:10:18.960]   from sea level suite type things because it fundamentally re architects the company. Well,
[00:10:18.960 --> 00:10:23.200]   we don't know where this slide is from and it may be that the people who are doing it know more than
[00:10:23.200 --> 00:10:29.120]   the people who made this silly slide. But I completely agree. I think the opportunity is very
[00:10:29.120 --> 00:10:35.280]   interesting. If anybody were open to new ideas at this point, it would be meta, right? Because
[00:10:35.280 --> 00:10:42.160]   they're bad. They're bad on the meta versus not been a hit. I don't mean to be dual employees at
[00:10:42.160 --> 00:10:47.760]   meta, but they're bright. No, I'm not. I mean, the mind that thought that that was a good idea
[00:10:47.760 --> 00:10:54.640]   makes me wonder. But he's chasing now. Oh, not. Isn't Mark saying, Oh, whoops, I should have done AI.
[00:10:55.920 --> 00:11:03.440]   I think I don't think they're like walking it back. But I think in the investor calls, like with
[00:11:03.440 --> 00:11:09.040]   so many companies right now, it's like everyone just wants to say AI as much as possible when
[00:11:09.040 --> 00:11:14.240]   they are reporting their earnings because they're trying to make the investors happy.
[00:11:14.240 --> 00:11:23.120]   But yeah, I think I actually, I wrote this at some point. Who knows where it is on techcrunch.com
[00:11:23.120 --> 00:11:29.520]   somewhere. But like at the last quarterly earnings report, Mark was like, we're not backing away
[00:11:29.520 --> 00:11:35.840]   from the metaverse. We're just also focusing on AI. But I don't know. I said to that, Harry.
[00:11:35.840 --> 00:11:40.320]   Well, I know you've written a lot about that. They're not abandoning the metaverse. Like
[00:11:40.320 --> 00:11:45.440]   abandoning the metaverse would involve things like shutting down meta quest and so forth,
[00:11:45.440 --> 00:11:51.200]   which I do not expect to happen. It must be less of a priority, particularly when they have fewer
[00:11:51.200 --> 00:11:57.840]   people on staff and are trying to be efficient. I think they fired what 25,000 or so.
[00:11:57.840 --> 00:12:02.720]   It seems like it would be a terrible mistake to downplay AI in order to put all your resources
[00:12:02.720 --> 00:12:08.480]   into the metaverse. And in fact, meta actually does have a lot of great AI people who have
[00:12:08.480 --> 00:12:12.960]   done. John Lecun. They have done a lot of great research. And the next step is to actually figure
[00:12:12.960 --> 00:12:16.880]   out how to make that into a better and more successful meta, which has not quite happened
[00:12:16.880 --> 00:12:21.760]   yet, but could so. And they're not mutually exclusive. No, right. You could have VR with AI.
[00:12:21.760 --> 00:12:27.280]   Then you have rye. I mean, I think what people kind of feel, but people all feel is not true,
[00:12:27.280 --> 00:12:32.560]   I might point out. So it's not a bad slogan and not a bad nickname. Like you can use it if you want.
[00:12:32.560 --> 00:12:38.640]   I think what people feel is for them. I think what maybe almost everybody agrees,
[00:12:38.640 --> 00:12:43.040]   whether they've acknowledged or not, is that the metaverse is not like two years away because the
[00:12:43.040 --> 00:12:47.120]   hardware we really want for the metaverse is not two years away. And in fact, there are just
[00:12:47.120 --> 00:12:51.680]   some fundamental things about creating great metaverse hardware that we don't know how to do
[00:12:51.680 --> 00:12:56.320]   yet. I foolishly spent 16. We don't know when we will know 1600 bucks on the Oculus Pro,
[00:12:56.320 --> 00:13:03.040]   which is now 1000 very shortly thereafter. And it's not 1600 bucks worth anything. It's not much
[00:13:03.040 --> 00:13:08.880]   better than the HTC 5 or even the original Oculus Rift, which I bought through a Kickstarter sale.
[00:13:09.440 --> 00:13:12.720]   Apple is now two weeks away. Three weeks away, 3000.
[00:13:12.720 --> 00:13:20.400]   That a week from two weeks away. Do we know that though? I mean, and my concern with Apple
[00:13:20.400 --> 00:13:27.680]   launching a VR headset is that maybe this is a category that's already dead. Is that a possibility?
[00:13:27.680 --> 00:13:32.640]   Yeah, I never thought it was worth spending any attention on. Like I've just kind of sat out this
[00:13:32.640 --> 00:13:39.680]   hole. I mean, I remember Vermal. I've been in tech long enough that I've seen this before,
[00:13:39.680 --> 00:13:44.400]   and I've seen the same level of enthusiasm, and I've seen the same detachment of understanding
[00:13:44.400 --> 00:13:48.240]   what the technology could possibly solve as an identified problem.
[00:13:48.240 --> 00:13:52.320]   By the way, everything you just said, if you took out VR and put AI in instead.
[00:13:52.320 --> 00:13:58.320]   Exactly. I'm kind of sitting this one out too. I mean, you talked about, is Zuck chasoned, but
[00:13:58.320 --> 00:14:03.360]   I mean, he might say whoops, but if you don't have any sort of reckoning about what your error
[00:14:03.360 --> 00:14:06.880]   was and how you ended up coming to make it, he's just going to repeat them.
[00:14:06.880 --> 00:14:11.440]   When you put $25 billion into something, it's hard to say whoops.
[00:14:11.440 --> 00:14:17.520]   Or $44 billion. Is it anymore? Well, no, but he's been out wasted buddy. I mean,
[00:14:17.520 --> 00:14:23.600]   so Zuck kind of has, he's kind of out of the limelight. He only wasted $25 billion. Other
[00:14:23.600 --> 00:14:27.920]   people have wasted a lot more. We don't know because Apple's very secretive how much they might
[00:14:27.920 --> 00:14:34.000]   have spent on this. Your headline last month, Harry, was how Apple's headset could be a $3,000
[00:14:34.000 --> 00:14:44.160]   flop and a success. Yeah, I mean, I thought way, way back to the early 1980s when Apple released
[00:14:44.160 --> 00:14:49.920]   the Lee Suf, which was a $10,000 computer. Very expensive flop. Which only lasted for a few years
[00:14:49.920 --> 00:14:54.880]   and is remembered as a flop, but it was also a milestone along the way to the Mac.
[00:14:54.880 --> 00:14:59.360]   And even the Mac when it came out did not sell anywhere near as well as they expected.
[00:14:59.360 --> 00:15:04.640]   But on the other hand, the Mac is still around and is an enormous business. And I feel like we need
[00:15:04.640 --> 00:15:08.560]   to give the Lee Suf a little bit of credit for helping to jumpstart that process.
[00:15:08.560 --> 00:15:11.600]   That's a really good point. And therefore recently, you could even say the Apple
[00:15:11.600 --> 00:15:16.560]   watch in his first few iterations was not a compelling product. It took a while. I mean, even
[00:15:16.560 --> 00:15:19.920]   the iPhone, if you look at the earliest coverage of the iPhone after it came out,
[00:15:19.920 --> 00:15:24.480]   people were not entirely sure whether the iPhone as innovative as it was was going to be a success.
[00:15:24.480 --> 00:15:28.880]   So I think it's quite possible. We'll just take years before we know whether this Apple had
[00:15:28.880 --> 00:15:32.960]   sat was worth it or not. That may be an important thing. You can have sleeper hits. And the problem
[00:15:32.960 --> 00:15:37.920]   is, is in our culture, we don't have good ways of appreciating and measuring sleeper hits.
[00:15:37.920 --> 00:15:42.960]   But you can have things like I'm thinking the movie Clue, which came out in the theaters and
[00:15:42.960 --> 00:15:47.920]   failed. It's still sleeping as far as I can tell. Is it? No, the movie Clue, people love it.
[00:15:47.920 --> 00:15:52.000]   Oh, really? Yeah. People watch it because they glued to get, they split the endings and that
[00:15:52.000 --> 00:15:55.520]   just ended up a mess. But they glued the endings together. They watch it because it's bad.
[00:15:55.520 --> 00:16:00.880]   No, Clue is good. Oh, it's good. Yeah. It's got some fantastic performances and I highly recommend
[00:16:00.880 --> 00:16:04.160]   it. I'm going to watch it tonight. I had no idea. I turned it into a play anything.
[00:16:04.160 --> 00:16:08.880]   I mean, it has legs, but it failed at the box office, but it was a quality product. So I think
[00:16:08.880 --> 00:16:13.200]   there's certain things where they come and they end up making a difference. And sometimes they
[00:16:13.200 --> 00:16:17.440]   come, they make a difference with a dye, but something about their legacy lives on. Like the
[00:16:17.440 --> 00:16:23.200]   Lisa. Yeah. But sometimes the thing actually comes out and holds on and it doesn't quite hit the,
[00:16:23.200 --> 00:16:26.720]   you're not going to measure it as a success because it's not an astronomical success,
[00:16:26.720 --> 00:16:31.600]   but it can hold on just enough that it can live on for years and decades. And this is critical.
[00:16:31.600 --> 00:16:37.360]   Amy Webb said this last week on the show, Magic Leap had a vision for something that would take a
[00:16:37.360 --> 00:16:43.840]   decade to bring to fruition ahead of their time as a phrase that we use. Yeah. But the
[00:16:43.840 --> 00:16:48.400]   venture cap, the model of venture capital that they were using would never give them that much
[00:16:48.400 --> 00:16:55.520]   time. So they couldn't make, make it happen. Apple on the other hand has, you know, is it
[00:16:55.520 --> 00:16:59.680]   almost $3 trillion company with billions in the bank, many billions in the bank,
[00:16:59.680 --> 00:17:04.480]   probably could if it took a decade, if they were really committed, could make something of this.
[00:17:04.480 --> 00:17:10.080]   I guess my next question though is what would, what, what makes this a success? I don't want
[00:17:10.080 --> 00:17:16.400]   whatever it is this, even if we're 1500 bucks, I already have one. What would it, what would it
[00:17:16.400 --> 00:17:20.400]   take for Apple's headset to be a success? Amanda, do you ever do VR gaming?
[00:17:20.400 --> 00:17:30.880]   Yeah. I have a quest to because META was just like trying to like get them into journalists hands
[00:17:30.880 --> 00:17:36.320]   at the time, which is it is ethical because you need to have one in order to report on it.
[00:17:36.320 --> 00:17:42.560]   Do you want to view anything without it? Yeah. Yeah. Yeah. Yeah. I don't know. I, so great news
[00:17:42.560 --> 00:17:51.920]   for Mark Zuckerberg if Mark is out there. Hey, what's up? Hey, I play rec softball and today I
[00:17:51.920 --> 00:17:56.640]   overheard people while I was like running onto the field and as I'm like running, I heard someone
[00:17:56.640 --> 00:18:02.240]   talking about their Oculus and I was like, what? And then I was like, no, don't talk to them.
[00:18:02.240 --> 00:18:08.320]   You are playing softball right now. Not at work. But I have one.
[00:18:08.320 --> 00:18:12.960]   I have one. But what, but so you don't spend a lot of time in it. I'm guessing.
[00:18:12.960 --> 00:18:21.040]   I know. I mean, I did for a little bit because when I, it's one of those, when I initially got it,
[00:18:21.040 --> 00:18:28.800]   like, I like to do physical activity in places that are not my apartment, but in case you forgot,
[00:18:28.800 --> 00:18:35.280]   there was a major international event that made doing things outside of your apartment temporarily
[00:18:35.280 --> 00:18:44.480]   less possible. And the supernatural app, which the meta acquisition of within, which makes
[00:18:44.480 --> 00:18:50.320]   supernatural, which is like an exercise app that I mean, that was a whole legal situation,
[00:18:50.320 --> 00:18:57.440]   but eventually the acquisition got closed. But yeah, supernatural was like great. And I really
[00:18:57.440 --> 00:19:02.240]   enjoyed why did you stop? I thought it was really fun. Why did you stop? But oh, I stopped because
[00:19:02.240 --> 00:19:07.360]   then the yoga studio near me open. I was like, cool, I'm going to go to yoga now and like not be
[00:19:07.360 --> 00:19:13.520]   looking at a screen. And I think that's like one of my big problems with it is after work,
[00:19:13.520 --> 00:19:18.080]   I don't want to be looking at screens. And there's a really big difference between like playing your
[00:19:18.080 --> 00:19:24.320]   switch or having a thing on your head. But I don't know. I mean, my whole take on all this,
[00:19:24.320 --> 00:19:29.520]   which this is like not a particularly intellectual take. I think this is incredibly obvious. But
[00:19:29.520 --> 00:19:37.200]   the idea of playing a video game in VR is really cool. The idea of taking a work meeting in VR is
[00:19:37.200 --> 00:19:42.560]   not. And all of the marketing around how you use these headsets has been really focused on like
[00:19:42.560 --> 00:19:50.480]   going to work and like doing meetings in VR. And I just don't really think we need to be in a VR
[00:19:50.480 --> 00:19:55.360]   meeting. Like, you just told us why? It's fine. You just told us why? Because you don't want to,
[00:19:55.360 --> 00:19:59.600]   you've been looking at screens all day, you don't want to use it after work. So meta probably
[00:19:59.600 --> 00:20:02.880]   observed that and said, well, we'll use it forward. But nobody wants to use it for work either.
[00:20:02.880 --> 00:20:07.360]   So why is this better than anything else? Like, I mean, to some extent, we're talking
[00:20:07.360 --> 00:20:12.880]   computer mediated communication. But that could mean anything. Like right now we're having
[00:20:12.880 --> 00:20:17.360]   computer mediated communication. And this is fine. I don't need to have a headset and some
[00:20:18.080 --> 00:20:23.360]   3D graphic rendering of a physical environment. I mean, if we're not actually in the physical
[00:20:23.360 --> 00:20:27.280]   environment, that doesn't help me. It doesn't enhance the experience. I need to be able to hear
[00:20:27.280 --> 00:20:33.440]   you, maybe be able to see facial recognition and just have a conversation and have the connection.
[00:20:33.440 --> 00:20:39.840]   And I don't see why adding this third dimensional immersive thing, even if it was rendered well
[00:20:39.840 --> 00:20:44.800]   helps like why that's the thing that we're just all desperate for. Because we can kind of connect
[00:20:44.800 --> 00:20:50.000]   via computers via flatter technology. And let's not forget that a certain percentage of the
[00:20:50.000 --> 00:20:54.320]   population around 11% gets nauseated by the way. I think I would be in that I haven't been trying
[00:20:54.320 --> 00:20:59.760]   it, but I know I do. And so that's another reason to take it off after a little bit. Yeah, I have
[00:20:59.760 --> 00:21:04.240]   a course too, but I can't get it over my head and over my glasses. So Harry,
[00:21:04.240 --> 00:21:14.080]   assuming that this is the Lisa, when will it and what will it look like when it becomes a success?
[00:21:14.640 --> 00:21:20.960]   Well, at $3,000, I assume that Apple is not expecting this to be like an enormous consumer product.
[00:21:20.960 --> 00:21:26.480]   It might be something that enough people buy that they learn. They talked about selling a million,
[00:21:26.480 --> 00:21:30.640]   which would be a $3 billion mark. Now they're saying half a million, still a big market. So
[00:21:30.640 --> 00:21:33.680]   this seems like the price has to come way, way down. And there's certainly a lot of reporting,
[00:21:33.680 --> 00:21:38.480]   they're working on cheaper versions. I think they couldn't make it cheaper. I think that's the
[00:21:38.480 --> 00:21:46.720]   problem. It also seems like, well, smartphones got more economical once the necessary technologies
[00:21:46.720 --> 00:21:50.560]   scaled up. Even in the first year, the iPhones price was cut significantly. So that could happen.
[00:21:50.560 --> 00:21:57.200]   Probably the one thing we will learn when they have this WWDC keynote is whether they have
[00:21:57.200 --> 00:22:03.600]   any more of a better vision than Mark Zuckerberg or anybody else for killer apps that do indeed
[00:22:03.600 --> 00:22:08.960]   feel like they could be killer apps. Because other than supernatural, this whole world is very short
[00:22:08.960 --> 00:22:16.320]   on those things. And if nothing else, Apple is good at figuring out how to not just put technology
[00:22:16.320 --> 00:22:22.480]   out in the world, but figure out the stack of hardware and software and services that adds up
[00:22:22.480 --> 00:22:28.160]   to something compelling. Are we going to be blown away by it in the way that people were blown away
[00:22:28.160 --> 00:22:34.400]   by the iPhone on day one? That seems unlikely, but it's at least conceivable that some of the ideas
[00:22:34.400 --> 00:22:39.840]   out there will have more clarity after Apple shows them off. I think Apple's getting a pass
[00:22:39.840 --> 00:22:44.800]   from a lot of us because they have in the past done that. I'm not convinced that's going to happen
[00:22:44.800 --> 00:22:47.920]   this time. It's really tough. I mean, I think the bar is higher than it was for the iPhone or the
[00:22:47.920 --> 00:22:53.440]   Apple watch in a lot of ways. I mean, I think Apple's got the money that it can spend it,
[00:22:53.440 --> 00:22:57.120]   including on some things that don't work. And quite frankly, that's the point of if you've got all
[00:22:57.120 --> 00:23:01.440]   that cash, kind of what you should do. I mean, you don't want to be completely reckless,
[00:23:01.440 --> 00:23:05.040]   but on the other hand, they can afford to take chances and make longer term investments,
[00:23:05.040 --> 00:23:13.600]   so more power to them. Although if we're going to talk money for things and even like smartphones
[00:23:13.600 --> 00:23:19.680]   coming down, I don't understand 1,800 for a photable pixel. Like, I mean, maybe that's the cost of
[00:23:19.680 --> 00:23:24.480]   production, but I don't understand because even the Samsung can flip and it's expensive at $1,000.
[00:23:25.120 --> 00:23:30.480]   Like finally, there's a pixel that's photable and it folds on the axis that doesn't actually
[00:23:30.480 --> 00:23:35.920]   fit in your pocket, but that's unreachable. And who wants to carry around that much cash value
[00:23:35.920 --> 00:23:40.720]   in your pocket? And it has the equivalent of three very large screens because it has a normal
[00:23:40.720 --> 00:23:46.080]   large screen and then something double the size of a large screen. Yeah, the cheaper Samsung
[00:23:46.080 --> 00:23:51.600]   is a flip. So it's a regular size phone when you open it up. This is more like the Samsung fold.
[00:23:51.600 --> 00:23:57.680]   Same price as a Samsung fold. I think Samsung, I'm sure Google looked at Samsung said,
[00:23:57.680 --> 00:24:02.400]   well, how much can we charge? Oh, $1,700. Okay. But I don't think anybody's going to buy. I don't
[00:24:02.400 --> 00:24:06.160]   know why Google's releasing that. That doesn't make any sense. I mean, they needed to produce
[00:24:06.160 --> 00:24:11.440]   something that flipped. I'm waiting for something. So much better. Like something because they got
[00:24:11.440 --> 00:24:14.800]   bigger and bigger and bigger. And I was waiting for it to shrink and the really good way to shrink
[00:24:14.800 --> 00:24:19.600]   it is folded in half. And I'm looking at this. I'm like, oh, they have a full to full phone. I look
[00:24:19.600 --> 00:24:25.760]   at it. I'm like, you're folding it on the wrong axis. You want the Galaxy flip, which I think rumor
[00:24:25.760 --> 00:24:30.480]   has it. Samsung won't want to announce the flip for is it next in a couple of months this summer.
[00:24:30.480 --> 00:24:35.120]   I guess it's the five because the four is out. This is what you want. It's cute. See,
[00:24:35.120 --> 00:24:41.360]   it folds in half. It's the size of a pocket square. Yeah, I mean, I covered it. I would rather have
[00:24:41.360 --> 00:24:45.440]   pixel for various reasons. And I still don't want to spend $1,000 on my cell phone.
[00:24:46.240 --> 00:24:50.160]   Like I thought a thousand was user-y. And the only thing that'll happen is new generations will
[00:24:50.160 --> 00:24:53.920]   drive down the price of the old ones. But I also don't want an outdated phone because like
[00:24:53.920 --> 00:24:57.840]   some of the other considerations are like the security of the phones and you kind of want something
[00:24:57.840 --> 00:25:04.800]   that's, you know, you must be you must be a con head. I'm talking about the TV show succession.
[00:25:04.800 --> 00:25:10.400]   Oh, I haven't. So Connor Roy says running for president. We've seen one season. Well,
[00:25:10.400 --> 00:25:14.960]   you know, as Connor Roy, who had a interest in politics at a very young age is running for
[00:25:14.960 --> 00:25:21.600]   president, his slogan is he's fighting against usury and onanism. So he's at least 50% in your in
[00:25:21.600 --> 00:25:26.400]   your pocket. Anyway, I'm sorry, I'm bringing up 1980s movies as my cultural. Yeah, you brought
[00:25:26.400 --> 00:25:32.240]   flu at least I have a more modern show. All right. Moving on. Actually, Amanda, what is the
[00:25:32.240 --> 00:25:40.640]   timeframe for Instagram's Twitter clone? They said that they're aiming for release this summer. So
[00:25:41.440 --> 00:25:47.360]   we'll see. But that'll be really interesting to keep an eye on. And I will definitely be
[00:25:47.360 --> 00:25:52.000]   writing about it if that actually comes out this summer. You may think I would be against it because
[00:25:52.000 --> 00:25:57.040]   I'm not a fan of centralized networks. But if it does in fact support activity pub or the Fediverse,
[00:25:57.040 --> 00:26:02.320]   I would be I would I think that's kind of the kind of thing that would be very helpful for
[00:26:02.320 --> 00:26:09.520]   mastodon and other Fediverse applications. If Firefox is doing a mastodon, they haven't yet
[00:26:09.520 --> 00:26:14.640]   released it, but they're they plan to I think this is good. If they did a Microsoft old school,
[00:26:14.640 --> 00:26:20.960]   Microsoft have embraced and extend problems. But on the but you know, given the you know,
[00:26:20.960 --> 00:26:25.920]   little bit of details we have now, I think there's a surprising amount of potential to
[00:26:25.920 --> 00:26:31.600]   actually produce something good and useful for the ecosystem. But I will be maybe surprised if
[00:26:31.600 --> 00:26:35.680]   they do manage to produce something good and healthy for the ecosystem and not accidentally
[00:26:35.680 --> 00:26:41.600]   screw everything up for themselves or anybody else. Well, there we have it. Ladies and gentlemen,
[00:26:41.600 --> 00:26:50.240]   VR headsets, AI and Twitter, all in one segment. This is a first we got it all out of the way.
[00:26:50.240 --> 00:26:55.920]   The Supreme Court is. Show ever. Yeah, we're done. Anyone has a bingo card out there. Yeah,
[00:26:55.920 --> 00:27:00.560]   we covered it all in one segment. We are going to talk more about the Supreme Court decision.
[00:27:00.560 --> 00:27:05.040]   There's a lot more to talk about in just a bit. I really like having all three of you here,
[00:27:05.040 --> 00:27:08.240]   though. This is great. Thank you, Amanda, for being on the show for the first time ever.
[00:27:08.240 --> 00:27:14.160]   Yeah, thanks for having me on a little screen, which makes me feel like a cyborg, but kind of in
[00:27:14.160 --> 00:27:20.320]   a cool way. Well, to be fair, you have the biggest head on the set right now. So that's okay. Yeah,
[00:27:20.320 --> 00:27:26.080]   that's okay. You win. Well, yeah, she does. Just keep paying her compliments.
[00:27:26.080 --> 00:27:31.680]   I'll get bigger. No way. I'm going the other way. Techcrunch.com. She's a culture writer for
[00:27:31.680 --> 00:27:37.840]   Techcrunch. Of course, Kathy Gellis is here expert on many things, but we're going to talk
[00:27:37.840 --> 00:27:43.360]   about the Supreme Court decisions, the Andy Warhol decision, and then the Tamna, which is the
[00:27:43.360 --> 00:27:48.160]   Twitter decision and the Google decision, all of which came down. Actually, there was another
[00:27:48.160 --> 00:27:51.920]   Supreme Court decision that came down this week. Nine, nothing. We could talk about that.
[00:27:51.920 --> 00:27:56.720]   This was a good week for you, busy week. It was a weird week. It was a weird couple of days.
[00:27:56.720 --> 00:28:03.120]   There was a conference where me and some other people who are in the space were at to even talk
[00:28:03.120 --> 00:28:07.600]   about these cases. There was a whole panel. What is going on at the Supreme Court and basically
[00:28:07.600 --> 00:28:14.000]   who found out 15 minutes before the panel began. It was actually really interesting because nobody
[00:28:14.000 --> 00:28:19.440]   had to feel everyone felt bad because nobody had had time to read anything. I think the general
[00:28:19.440 --> 00:28:23.520]   thinking was Supreme Court's going to defer this decision to the very end. We were expecting this
[00:28:23.520 --> 00:28:29.440]   to be a late one, but also with a very different result potentially. But Warhol was a very late
[00:28:29.440 --> 00:28:33.520]   decision. That must have been a gut punch for you. Oh, it was terrible. Yes, I will explain why it
[00:28:33.520 --> 00:28:38.080]   was terrible. Bad for fair use. We'll talk about that in just a little bit. Also, Remit Krakin,
[00:28:38.080 --> 00:28:43.120]   the technologizer. It's great to have you. A number of people in the chatroom said, "Twits over? No,
[00:28:43.120 --> 00:28:49.280]   just be kidding. Just don't get your own. Just beginning. Our show today brought to you by
[00:28:49.280 --> 00:28:56.480]   Grammarly and something new from Grammarly. Grammarly, go." First of all, props to Grammarly. They are
[00:28:56.480 --> 00:29:02.800]   a Ukraine company. It's been a very tough time for them, but they make one of the best product
[00:29:02.800 --> 00:29:07.920]   products out there. I love Grammarly for that. They're from Ukraine, but also I love them because
[00:29:07.920 --> 00:29:14.160]   they do their core product in LISP. I love them for that. Now there's Grammarly, go. Today, we're
[00:29:14.160 --> 00:29:19.040]   working and communicating faster than ever. A lot of times when you're sitting down to that
[00:29:19.040 --> 00:29:22.480]   blank page, it's tough, right? You feel a little stuck? If you've ever had that happen, whether you're
[00:29:22.480 --> 00:29:27.760]   writing a thank you note or a business plan, then you're going to like this new Grammarly, go. I've
[00:29:27.760 --> 00:29:34.640]   been trying it in beta. This is Grammarly's new communication assistant, powered by, yes, the
[00:29:34.640 --> 00:29:41.200]   hot topic, generative AI. You'll compose intelligently. You could type a prompt into Grammarly, go.
[00:29:41.200 --> 00:29:45.680]   It will generate high quality writing. You can spend less time on wordsmithing.
[00:29:47.120 --> 00:29:52.320]   You could personalize it into your voice. Grammarly has always been good at this.
[00:29:52.320 --> 00:29:58.880]   Choosing the right tone, right? There is a set voice or tone indicator at the top of the Grammarly,
[00:29:58.880 --> 00:30:03.360]   go window. You could personalize your tone, so you could say, "I want this to be casual," or
[00:30:03.360 --> 00:30:10.400]   neutral, or formal. Or you can say, "I want this to be personal, confident, empathetic, engaging,
[00:30:10.400 --> 00:30:16.160]   witty," or direct. Grammarly, go. We'll understand your unique context, your preferred voice,
[00:30:16.880 --> 00:30:22.720]   and your goals, and we'll generate high quality writing quickly. It just takes a few clicks to
[00:30:22.720 --> 00:30:29.120]   ideate, compose, rewrite, and thoughtfully reply. You can rewrite for tone, clarity, and length,
[00:30:29.120 --> 00:30:33.920]   give Grammarly a paragraph. We've always been, I love this feature of Grammarly,
[00:30:33.920 --> 00:30:37.680]   and it will help you fine-tune your writing to reflect your personal style.
[00:30:37.680 --> 00:30:43.360]   And the tone that you're trying to attain, be effective in any situation. You can select the
[00:30:43.360 --> 00:30:48.400]   text you want to rewrite, activate Grammarly, go. Let it do it. It'll rewrite your paragraph
[00:30:48.400 --> 00:30:53.760]   to sound more formal if you want, or more exciting, or more professional, or more inspirational.
[00:30:53.760 --> 00:30:58.560]   And you know, you've got to try this because it really, really works. It's super cool.
[00:30:58.560 --> 00:31:04.400]   You'll be amazed at what you could do with Grammarly, go. Go to grammarly.com/go.
[00:31:04.400 --> 00:31:12.640]   You can download it, learn more about Grammarly, go g-r-a-m-m-a-r-l-y, grammarly.com/
[00:31:13.600 --> 00:31:20.480]   Go. For me, because I think I'm a good writer, I love it because I apply it to something I've
[00:31:20.480 --> 00:31:26.480]   written already, and it cleans up any little, you know, confusing, not just Grammar anymore.
[00:31:26.480 --> 00:31:30.320]   Now it's like, well, that was a little confusing. Let's break that into two senses, makes it clear.
[00:31:30.320 --> 00:31:36.160]   That's really good at kind of refining what you write, or if you're still looking at that blank
[00:31:36.160 --> 00:31:40.480]   page going, eh, I don't know where to start. It's a great way to get started, and then the
[00:31:40.480 --> 00:31:46.160]   juices flow, and you can write. Grammarly, I love it.com/go. Give it a try today.
[00:31:46.160 --> 00:31:53.440]   All right. I'm putting it off. Should I put off a little more and talk about Montana? No, no.
[00:31:53.440 --> 00:31:58.240]   Actually, I do want to talk about Montana because Montana banned TikTok this week.
[00:31:58.240 --> 00:32:03.120]   It thinks it did. I mean, first of all, there's a thing called the First Amendment, right?
[00:32:03.120 --> 00:32:05.360]   There's also the practical implementation of-
[00:32:05.360 --> 00:32:06.480]   How do you even do it?
[00:32:06.480 --> 00:32:14.400]   Right. So many states have already said, if you're a state employee, you can't use TikTok
[00:32:14.400 --> 00:32:19.280]   on your state-provided phone. I don't have a problem with that. Military do it, too. That's fine.
[00:32:19.280 --> 00:32:23.680]   You shouldn't be TikTok-ing when you're working anyway.
[00:32:23.680 --> 00:32:28.320]   I think that's harder to say because you get, you know, should AOC not be TikTok-ing?
[00:32:28.320 --> 00:32:29.920]   She TikTok is part of her work.
[00:32:29.920 --> 00:32:30.960]   No, it's communication.
[00:32:30.960 --> 00:32:31.600]   Yeah. So-
[00:32:32.400 --> 00:32:37.920]   That's a good point. I think it's less legally fraught to have such a rule, but it may not actually be
[00:32:37.920 --> 00:32:42.560]   good internal policy to have such a rule. And possibly also, therefore, run into some
[00:32:42.560 --> 00:32:47.600]   constitutional problems as well. But in theory, state to state, like we're governing our own
[00:32:47.600 --> 00:32:50.000]   property makes a little bit more sense legally.
[00:32:50.000 --> 00:32:57.120]   Very hard for one state to say, okay, nobody in Montana can use TikTok. I don't even know how-
[00:32:57.120 --> 00:33:02.960]   where would you begin? And by the way, the fine is significant. It's $10,000 per incident.
[00:33:02.960 --> 00:33:07.680]   So the first people would act, I guess, would be Apple and Google, who would then say,
[00:33:07.680 --> 00:33:11.600]   are you in Montana? Well, you can't get these at the TikTok app on our phone.
[00:33:11.600 --> 00:33:15.280]   One thing that has not happened, although I think there might be some runway before this
[00:33:15.280 --> 00:33:16.800]   actually kicks in. It is-
[00:33:16.800 --> 00:33:17.520]   January 1st.
[00:33:17.520 --> 00:33:23.440]   So, you know, like how many people are there actually in Montana if you, you know,
[00:33:23.440 --> 00:33:28.640]   Google has done some geo blocking before with stupid local jurisdictional rules that have tried to
[00:33:28.640 --> 00:33:31.280]   govern it in stupid ways. Spain.
[00:33:31.280 --> 00:33:37.200]   Spain with turning off the news. So they might do that. But in the meantime, there's at least
[00:33:37.200 --> 00:33:39.680]   one legal challenge has been filed so far.
[00:33:39.680 --> 00:33:46.080]   This is interesting. TikTok decided not to sue on its own behalf. But it's very clear what they
[00:33:46.080 --> 00:33:53.200]   probably did is they went to five Montana TikTok creators and said, you know, this is going to
[00:33:53.200 --> 00:33:59.760]   be a problem, isn't it? So they have TikTok users and creators challenging it.
[00:33:59.760 --> 00:34:03.440]   That's a little from a First Amendment point of view. That's probably more sensible, yeah?
[00:34:03.440 --> 00:34:07.840]   Well, I don't know if TikTok is officially sitting it out, but it's- it's- it's- it's-
[00:34:07.840 --> 00:34:13.360]   steer a corporate ship is hard. And there's a lot of considerations that you have to do.
[00:34:13.360 --> 00:34:18.640]   I'd like to see companies be a lot more aggressive and challenge a lot of these state laws, but there's
[00:34:18.640 --> 00:34:24.240]   reasons why and some not completely unreasonable reasons why they're more cautious than what they
[00:34:24.240 --> 00:34:29.280]   do. I don't know if they necessarily had any conversation with the TikTokers themselves.
[00:34:29.280 --> 00:34:34.480]   Maybe they did. Maybe they didn't. But this- I looked at it briefly. I mean, one of the things
[00:34:34.480 --> 00:34:39.120]   you're always going to have is, you know, standing questions of are these people affected in the way
[00:34:39.120 --> 00:34:42.480]   that they would even be able to challenge this. Although I think there's certainly a
[00:34:42.480 --> 00:34:49.040]   colorable argument they- Five Montana residents who quote, create, publish, view, interact with,
[00:34:49.040 --> 00:34:51.600]   and share videos and TikTok. Yeah, I mean, that sounds like standing.
[00:34:51.600 --> 00:34:56.080]   I think it gets in the door. And First Amendment standing is supposed to be more expansive
[00:34:56.080 --> 00:35:00.960]   anyway. So that does look like an injury that's likely to be accrued. And that's interesting that
[00:35:00.960 --> 00:35:06.000]   they're Montana residents. So I don't know what will happen to it, but it certainly looks colorable.
[00:35:06.000 --> 00:35:11.200]   Yeah, my feeling is TikTok is watching. And if they don't have to sue, they won't.
[00:35:12.080 --> 00:35:17.280]   It's interesting. NetChoice, who has sued in other First Amendment cases, I think in Texas and Florida,
[00:35:17.280 --> 00:35:20.880]   is saying right now, we don't have plans to sue. I think everybody's hoping the creators will
[00:35:20.880 --> 00:35:26.880]   step forward. Well, NetChoice is doing a California lawsuit. NetChoice has already filed a lawsuit
[00:35:26.880 --> 00:35:31.600]   challenging the age-appropriate design law that California had. Because that's a terrible law.
[00:35:31.600 --> 00:35:37.360]   And I mean, I think that's a thing of netChoice as an entity is showing up kind of
[00:35:37.360 --> 00:35:41.360]   their trade association, but they're showing up and they're actually bringing some of these
[00:35:41.360 --> 00:35:47.040]   challenges. And the question is, will the rest of the industry back them up on this? Because I think
[00:35:47.040 --> 00:35:52.320]   it's absolutely the right move. I think companies end up thinking, well, can I survive this? But
[00:35:52.320 --> 00:35:56.560]   that's too short term in their thinking. Because maybe you can survive this particular piece of
[00:35:56.560 --> 00:36:02.480]   crap, but you can't survive the whole onslaught. So the state of Montana, through a spokesperson
[00:36:02.480 --> 00:36:11.360]   for the AG said that they want to use geo-fencing to prevent the use of TikTok in Montana.
[00:36:11.360 --> 00:36:17.200]   Bill the wall. That sounds like the world of the Great Firewall of Montana.
[00:36:17.200 --> 00:36:26.000]   I mean, what's so interesting to, I mean, like, Kathy, I would be interesting what you would have
[00:36:26.000 --> 00:36:34.000]   to say about this. But another state specific legal issue I've been following is certain states
[00:36:34.000 --> 00:36:41.360]   passing legislation that in order to access certain adult websites, you need to verify your age.
[00:36:41.360 --> 00:36:48.320]   But you're verifying your age through a government app. And I think there's pretty-
[00:36:48.320 --> 00:36:51.440]   California started Utah and doubled down on it.
[00:36:51.440 --> 00:36:54.160]   Right. And I think Louisiana, but-
[00:36:54.160 --> 00:37:01.040]   U.K. is about to do. It seems like there's just a lot of legislation right now that's like
[00:37:01.040 --> 00:37:07.120]   very state specific regulating how people are using the internet. And I don't know, I just find
[00:37:07.120 --> 00:37:12.160]   that really interesting and kind of scary. But yeah, I feel like-
[00:37:12.160 --> 00:37:13.760]   Is it not trying to regulate-
[00:37:13.760 --> 00:37:15.040]   Internet legislation-
[00:37:15.040 --> 00:37:18.560]   Is it trying to regulate interstate commerce or no?
[00:37:18.560 --> 00:37:22.800]   There are so many problems with this. I mean, there's the purest amendment problem there.
[00:37:22.800 --> 00:37:27.280]   When you start to use the- Well, actually the first amendment problem also shows up in a couple
[00:37:27.280 --> 00:37:33.440]   of ways, one of which is, well, the imposition on the platforms and who the platforms can associate
[00:37:33.440 --> 00:37:37.600]   with and who they can speak to and what they're using and who they can have their user-
[00:37:37.600 --> 00:37:38.480]   So they're first amendment, right?
[00:37:38.480 --> 00:37:39.600]   So they're first amendments.
[00:37:39.600 --> 00:37:44.560]   It's impinging on speakers because it's going to essentially keep them off the internet.
[00:37:44.560 --> 00:37:48.560]   And it's impinging particularly on anonymous speakers because you don't get to speak anonymously
[00:37:48.560 --> 00:37:53.600]   if you have to prove yourself to people. It starts to create privacy problems because now you've
[00:37:53.600 --> 00:37:56.400]   collecting all sorts of really private sensitive data.
[00:37:56.400 --> 00:37:59.040]   You might be- It's like you have to go in and get your porn license.
[00:37:59.040 --> 00:38:01.840]   Yeah, and now you've got some fourth amendment problems.
[00:38:01.840 --> 00:38:05.360]   You might have some fifth amendment problems. I mean, this is disastrous in every way you can
[00:38:05.360 --> 00:38:09.680]   look at it. And it's also like there's no way to run an inter-commerce railroad like this because
[00:38:09.680 --> 00:38:14.960]   you can't- This is why we have a preemption clause in section 230.
[00:38:14.960 --> 00:38:20.320]   It also tends to run straight into section 230, but that was where section 230 was starting to
[00:38:20.320 --> 00:38:24.960]   get weakened by courts who tended to see a lot more things percolating out of the states as being
[00:38:24.960 --> 00:38:29.440]   like, "Oh no, this isn't preempted. This isn't preempted." And I think they're wrong.
[00:38:29.440 --> 00:38:34.960]   And I think the effect of that is going to be you can see the harm of each state doing things
[00:38:34.960 --> 00:38:40.560]   because how do you deal with what Texas wants of internet platforms to do and what California
[00:38:40.560 --> 00:38:44.480]   wants, although California is busy picking the same things that Utah wants, so it's all very
[00:38:44.480 --> 00:38:50.480]   confusing. Apple and Google have said through a trade group before this ban was approved that
[00:38:50.480 --> 00:38:56.240]   they did not think it possible to restrict access to an app to a single state. Like they didn't think
[00:38:56.240 --> 00:39:00.960]   there was a technical way to do that. It seems tough. I mean, would we see less of the stuff on
[00:39:00.960 --> 00:39:06.720]   the state level if at the federal level people spent less time yapping and more time actually
[00:39:06.720 --> 00:39:13.520]   coming up with laws if any laws make sense? Well, the problem is Congress is busy trying to
[00:39:13.520 --> 00:39:17.520]   also wreck the internet, but better that they wreck it at the federal level than they wreck it
[00:39:17.520 --> 00:39:22.320]   at the state level? It's wrecked for all of us. Instead of just Montana. I mean, because otherwise
[00:39:22.320 --> 00:39:27.200]   there's always going to be a compliance issue. And what tech companies can't do, what platforms
[00:39:27.200 --> 00:39:33.280]   can't do is respond to one state and also all 50 states because it's even not only just the 50
[00:39:33.280 --> 00:39:40.000]   states. If you start to allow state jurisdiction over the internet, then it's state and local
[00:39:40.000 --> 00:39:45.360]   immunosuppality. And it's like, you can't you can't cabin this in because like Texas could have one
[00:39:45.360 --> 00:39:49.920]   rule in Austin could have another role in this is not terrible. Joe and our discord makes it a
[00:39:49.920 --> 00:39:54.320]   good plan. I don't know historically how long this has been going on, but it seems like lately anyway,
[00:39:55.040 --> 00:40:03.040]   a lot more laws are created as theater, not with any intent of passing any constitutional review
[00:40:03.040 --> 00:40:08.640]   or getting past courts. But just because well, we said it. So you know, you you can trust us. We
[00:40:08.640 --> 00:40:12.800]   did the right thing. It feels like a lot of theatrics. It feels very performative and
[00:40:12.800 --> 00:40:18.400]   performance. Yeah, I am not sure that sounds almost two benign. I mean, I think there's
[00:40:18.400 --> 00:40:23.280]   an aspect. I think it's worse than that. But what's a little bit confusing is that. So I think
[00:40:23.280 --> 00:40:26.800]   from the red states, I mean, if they're busy banning books, then they're going to have to
[00:40:26.800 --> 00:40:31.840]   ban the internet too, because you haven't effectively like shut down kids minds unless you take away
[00:40:31.840 --> 00:40:36.960]   all the stuff that they're reading. What it boggles the mind though, is what's coming out of blue
[00:40:36.960 --> 00:40:41.920]   states. I don't understand what governor Newsom is thinking in that like, he basically produced
[00:40:41.920 --> 00:40:47.280]   just a slightly lighter version of what Utah ended up doing. What does this ended up doing? And
[00:40:47.280 --> 00:40:52.240]   no one said Republicans had a monopoly on foolishness. It'd be nice if there was a political
[00:40:52.240 --> 00:40:56.960]   opposition that actually wanted to sort of say, wait, the attacks on the constitutional order
[00:40:56.960 --> 00:41:01.360]   is a problem. So our political platform will be to do this. This is why I give money to the EFF.
[00:41:01.360 --> 00:41:08.320]   And you know, there are a lot of organized trade organizations that are, you know, trying to do
[00:41:08.320 --> 00:41:14.800]   with the right thing. Maybe not our member representative democracy. I think also they don't
[00:41:14.800 --> 00:41:19.600]   necessarily understand exactly what will break. I think that's also the I think that's fundamentally
[00:41:19.600 --> 00:41:23.520]   true of every regulator that wants to speak to it, including members of Congress who are kind of
[00:41:23.520 --> 00:41:29.600]   like something must be done and just have no idea that, no, you're going to just make the problem
[00:41:29.600 --> 00:41:34.000]   you think you're fixing worse and they don't know. And what really bothers me is the ones who
[00:41:34.000 --> 00:41:39.600]   refuse to learn. Like, I mean, I listened to it's, but you said an interesting phrase,
[00:41:39.600 --> 00:41:43.920]   something must be done. That's what's really going on. We don't know how to do it. We don't
[00:41:43.920 --> 00:41:50.240]   know if we can do it. But something must be done. I understand that we see this happening with
[00:41:50.240 --> 00:41:57.040]   the Kids Online Safety Act, where that was pretty much a direct response to the Facebook
[00:41:57.040 --> 00:42:03.840]   whistleblowers and the situation with like all these bombshell articles about Instagram being
[00:42:03.840 --> 00:42:12.240]   agent of depression and eating disorders and teenage girls. But then like, it's just like this
[00:42:12.240 --> 00:42:16.720]   same to the point of like theatricality. It's like, yeah, if you say Kids Online Safety Act,
[00:42:16.720 --> 00:42:23.120]   no one's going to be like, that's bad. But then when you think about it, and what this would do is
[00:42:23.120 --> 00:42:32.240]   like, any situation where people are putting age checks and checking people's actual like
[00:42:32.240 --> 00:42:37.120]   government identification in order to do anything on the internet, there are so many
[00:42:37.840 --> 00:42:45.360]   really scary implications of that. And no one's arguing against Kids Safety, but the way that
[00:42:45.360 --> 00:42:52.880]   these laws play out in practice, you can really tell that the people that are writing them
[00:42:52.880 --> 00:42:57.360]   don't know how the internet works and to have these point. Yeah.
[00:42:57.360 --> 00:43:04.400]   There have been so many, like, I've watched so many hearings over the last two or so years about
[00:43:04.400 --> 00:43:08.480]   how social media is making everyone depressed. And I think that is like,
[00:43:08.480 --> 00:43:15.680]   something that's worth talking about, but nothing has even come of these hearings yet other than
[00:43:15.680 --> 00:43:22.800]   like some proposed legislations that won't really fix much hearings are so much time to learn
[00:43:22.800 --> 00:43:26.560]   about the internet. But they're not learning. I mean, talk about the theatrics. Yeah,
[00:43:26.560 --> 00:43:32.960]   hearings are performative where people bring in speakers who can speak to their pre-ordained
[00:43:32.960 --> 00:43:37.600]   biases. Or worse, they bring in somebody and then grandstand and don't let them speak.
[00:43:37.600 --> 00:43:41.360]   It's just an opportunity for a member of Congress to go on for his full 10 minutes.
[00:43:41.360 --> 00:43:45.120]   There's not a whole lot of rigorous inquiry going on here, and there's not a whole lot of
[00:43:45.120 --> 00:43:48.720]   interest in rigorous inquiry. And then we get things that I'm, you know,
[00:43:48.720 --> 00:43:52.560]   to be politically listened to, like, Senator Bloom and Fal talking about like,
[00:43:52.560 --> 00:43:57.520]   I've been trying for years to make changes to 230 and that will finally be the time. It's like,
[00:43:57.520 --> 00:44:02.160]   no, the reason you've not been able to pull that off is because smart people have managed to stop
[00:44:02.160 --> 00:44:06.960]   you because it was a bad idea. It doesn't mean that the time will come where your good idea will
[00:44:06.960 --> 00:44:11.760]   survive. It will still not be a good idea. It's a post sponsor with Marsha Blackburn, Dick Blumenthaw
[00:44:11.760 --> 00:44:17.520]   of the kids in my city. I mean, it's a, yeah, when you see Marco Rubio's name and right next to
[00:44:17.520 --> 00:44:23.200]   Amy Klobuchard or Hickenlooper is it's like, well, at least there's bipartisan support, I guess.
[00:44:23.200 --> 00:44:30.240]   I have a tech dirt post from January 2001, I think, where it's a dear section 230 critics when Josh
[00:44:30.240 --> 00:44:35.920]   Haulian and Ted Cruz are your allies. And this is time to think again, because they know exactly
[00:44:35.920 --> 00:44:39.920]   why they want to get rid of this law and exactly what a little advance. That's why it's bipartisan.
[00:44:39.920 --> 00:44:44.080]   They have different agendas, but the agendas happen to me in the middle. And I listen to that
[00:44:44.080 --> 00:44:49.520]   hearing where Josh Hawley is smart and he knows that he's getting political support from Bloomin
[00:44:49.520 --> 00:44:54.480]   Fal for what he wants. And I can listen to him kind of like, oh, yes, I'm totally doing it to
[00:44:54.480 --> 00:44:58.800]   advance that woman's rights issue that you were just, we're speaking of. And Bloomin Fal doesn't
[00:44:58.800 --> 00:45:04.880]   seem to understand that he's getting played. Sorry. It's all just a series of tubes. You always have
[00:45:04.880 --> 00:45:09.760]   yeah, remember that? This is why we're all playing tears of the kingdom right now, right?
[00:45:09.760 --> 00:45:16.480]   Amanda, you're playing tears of the kingdom, I bet. Oh my God. So this is, you have to for your work,
[00:45:16.480 --> 00:45:21.040]   horrible confession. Mom, I have to play Zelda for my work. Wait, what's her confession?
[00:45:21.760 --> 00:45:30.560]   Tell Matthew Panzorino that right now. But no, I mean, like, so I have not finished Breath of
[00:45:30.560 --> 00:45:34.480]   the Wild. That is my big confession. The predecessor started tears of the kingdom.
[00:45:34.480 --> 00:45:38.640]   Because it's the sequel. It's the same game. Yeah, I mean,
[00:45:38.640 --> 00:45:43.920]   tell you it's the same game. Same place. You're still in high. You're still wandering around.
[00:45:43.920 --> 00:45:48.560]   You could play both at the same time. I really should be playing it. I'm not done.
[00:45:48.560 --> 00:45:53.760]   I don't know. Breath of the Wild. Just I'm not done with animal crossing.
[00:45:53.760 --> 00:45:59.120]   Getting to your glider. I cannot play anything till I finish my animal crossings down. And then
[00:45:59.120 --> 00:46:04.960]   well, you can't ever finish it. Oh, shocks. I never finished games. So I didn't finish
[00:46:04.960 --> 00:46:12.000]   Breath of the Wild and I am planters of the kingdom despite that. Yeah, well, I win because I'm not
[00:46:12.000 --> 00:46:17.280]   even I don't even start here. I'm talking here. Yeah. 10 million copies sold in three days.
[00:46:18.080 --> 00:46:25.600]   The switch itself is practically at the end of life. Yeah. And it's sales of new switches have
[00:46:25.600 --> 00:46:31.040]   been disappointing lately. But games for the switches people already have are still doing well.
[00:46:31.040 --> 00:46:37.280]   And many are saying this is the best game ever made 10 out of 10 in many cases.
[00:46:37.280 --> 00:46:44.960]   And this was also the standard price for switch games that are like Pokemon or Zelda or like big
[00:46:44.960 --> 00:46:50.880]   franchises like that is $60. But this was the first time that they sold a game for $70.
[00:46:50.880 --> 00:46:57.040]   And it's still I mean, three financial decision for Nintendo. They still sold in saying about
[00:46:57.040 --> 00:47:01.600]   a copies, but to put this in perspective, Breath of the Wild, a little concerning the Breath of the
[00:47:01.600 --> 00:47:08.240]   Wild only sold 30 million copies in its entire lifetime, you know, four year lifetime. Three days,
[00:47:08.240 --> 00:47:14.080]   10 million copies on tears of the kingdom. I imagine they'll be people going back and playing
[00:47:14.080 --> 00:47:19.760]   Breath of the Wild to I just like to know for the record, I had an Atari 800. I played video games
[00:47:19.760 --> 00:47:26.480]   then I had Pac-Man. I did Caverns of Mars. We had Breakout. I mean, I've got some crayons. But
[00:47:26.480 --> 00:47:30.000]   like after that, like what did it, you know, got out of my system? What do I need this for now?
[00:47:30.000 --> 00:47:35.120]   All right. Let's take a break. Those games too, by the way, on my Atari 800, which I have.
[00:47:35.120 --> 00:47:38.640]   Oh, three Atari people. That's great. That was my first computer was a 400.
[00:47:38.640 --> 00:47:46.880]   My first console was Nintendo 64. Yeah, you're you're an N64. You're a Mario kid. I bet you're
[00:47:46.880 --> 00:47:50.400]   gonna. I don't. Did you go see the Mario? Did you go see the Mario? The Atari 800 came out.
[00:47:50.400 --> 00:47:55.120]   Oh, you don't even want to know. I have not, but I do want to see it because I have heard that
[00:47:55.120 --> 00:48:00.800]   it is good. I also think it's very funny that Mario and Luigi are apparently
[00:48:00.800 --> 00:48:05.920]   arcanonically from Flatbush, Brooklyn, which I'm just very happy to know what neighborhood of
[00:48:05.920 --> 00:48:09.840]   Brooklyn they live in. I don't it just I really wanted to know what I found out.
[00:48:09.840 --> 00:48:14.560]   And we play Donkey Kong where it came from like and like the whole like.
[00:48:14.560 --> 00:48:19.760]   I mean, in an arcade in an arcade game with with sticks. I think we had one for the
[00:48:19.760 --> 00:48:23.920]   Atari, but it wasn't as old as Pac-Man. It wasn't as good as the arcade version.
[00:48:23.920 --> 00:48:28.080]   Amanda, this came out before you were born. Sorry to share this. This is before you were born.
[00:48:28.080 --> 00:48:34.080]   Oh, yeah. No, I I literally just fact check this. It was discontinued before I was born.
[00:48:34.080 --> 00:48:38.640]   Discontinued before you were born. That's like a lyric to a song.
[00:48:38.640 --> 00:48:45.760]   The Atari 800 was discontinued before I was born. I love it. I'm going to write a poem right?
[00:48:45.760 --> 00:48:50.320]   Yeah. Yeah. The Atari 400 was the first computer I bought with my own money by the way. Me too.
[00:48:50.320 --> 00:48:54.640]   Me too. Because I was playing a lot of games in the arcade and I realized all these quarters
[00:48:54.640 --> 00:48:58.480]   I've just dropped. If I bought an Atari 400, I could play Battlezone to my heart's content.
[00:48:58.480 --> 00:49:02.720]   And I still have my 400 and it's still my my friend Ben Jedward to works for ours
[00:49:02.720 --> 00:49:07.520]   Technic. I recently repaired it for me. I'm so jealous. The crummy keyboard stopped working.
[00:49:07.520 --> 00:49:10.000]   Yeah, that's the worst thing. He swapped in a replacement keyboard.
[00:49:10.000 --> 00:49:15.040]   I think we have an 800, but it had a sticky select key. So I'm wondering if I can get fixed.
[00:49:15.040 --> 00:49:20.000]   Because I think we still have it. I bet you can. Oh, I'm sad. I long ago got rid of all of those.
[00:49:20.000 --> 00:49:25.360]   Yeah. I'm sorry, Amanda. You have to be with all these old farts, but that's okay. That's life.
[00:49:25.360 --> 00:49:31.200]   No, I mean, I just had a birthday and I'm like, I don't know. I know I'm young, but it's like every
[00:49:31.200 --> 00:49:37.600]   year closer I get to 30. I'm like, I know. What am I doing? I know. What am I doing? So I someday,
[00:49:37.600 --> 00:49:41.200]   YouTube can be on a show waxing poetic about video games you played in your youth.
[00:49:41.200 --> 00:49:49.200]   But I'll be doing it. I just exploit people to make me feel like I have an age. It's good.
[00:49:49.200 --> 00:49:54.000]   You feel young. That's all that matters. Yes. Thank you. I'm very happy to be in this company.
[00:49:54.000 --> 00:49:59.840]   What position do you play in your softball league? I play second base. Nice. Oh, that's a good
[00:49:59.840 --> 00:50:05.040]   position. You range the field. Do they still allow the shift in softball?
[00:50:05.040 --> 00:50:13.200]   A little bit. I do tend to shift more closer to up the middle when there's a righty batter,
[00:50:13.200 --> 00:50:20.080]   which is often good job. Yeah. You want to get that? But then the lefties, they get me. But
[00:50:20.080 --> 00:50:26.720]   every time. Yeah, my team is called the hobantossian, which for those who don't know is a Jewish cookie.
[00:50:26.720 --> 00:50:32.640]   And I am Jewish, but a prune, only three Jewish people on the team. Well,
[00:50:32.640 --> 00:50:37.520]   even now that I don't have section two, 30 to worry about, I can complain about the new roles in
[00:50:37.520 --> 00:50:42.400]   Major League Baseball, which just exercises me completely. When you play in the hamantaschen,
[00:50:42.400 --> 00:50:51.440]   do you wear tricorn hats to match your? No, but I. Yes, I did just get a hat that like a Jewish
[00:50:51.440 --> 00:50:56.400]   artist made. That's like, it's just a hat that is embroidered and says, "Homantossian."
[00:50:56.400 --> 00:51:03.840]   And but unfortunately, I debuted it on the field today. And I realized that when I run,
[00:51:03.840 --> 00:51:09.600]   it would fall off. So I need to do a little bit of like testing and like, hat tightening before
[00:51:09.600 --> 00:51:14.640]   our next game. So that's what I'm doing outside of work. The aerodynamics of a regular baseball
[00:51:14.640 --> 00:51:18.800]   cap are not great. So I think they're supposed to fall off before your time, both of you probably,
[00:51:18.800 --> 00:51:23.680]   but the great Willie Mays used to wear a cap that was one size too big. So the win he made a running
[00:51:23.680 --> 00:51:29.920]   catch out in the outfield and his cap flew off. He looked much more dynamic. He was quite famous
[00:51:29.920 --> 00:51:33.600]   for that, actually. And then he did the basket. Maybe that's what I'm trying to do.
[00:51:33.600 --> 00:51:37.440]   He did the best of the famous basket catch. Let's take a little break. We'll come back with more
[00:51:37.440 --> 00:51:43.600]   with this sports-addled group. But first, we're not playing the video games anymore. We're getting
[00:51:43.600 --> 00:51:48.720]   out and getting fresh air. First a word. I'm doing both. Just open the window. That's
[00:51:48.720 --> 00:51:55.760]   fresh air. First a word from our sponsor, Noom. I'm a big Noom fan. My wife, Lisa, is a big Noom
[00:51:55.760 --> 00:52:00.960]   fan. I started it. I saw all those TV ads a couple of years ago. I said, you know, that looks like a
[00:52:00.960 --> 00:52:04.880]   good thing. Actually, it took about 20 of them before I said, that looks like a good thing. I
[00:52:04.880 --> 00:52:10.880]   should try it. Signed up. It's not a diet. I said, I'm going to be doing Noom, Lisa. And she said,
[00:52:10.880 --> 00:52:14.080]   you know, I'm going to be a good wife. I don't need to lose anyway, but I'm going to do it with you.
[00:52:14.080 --> 00:52:21.040]   She has now lost and kept off about 12 pounds. I lost about 16 and kept it off. Actually,
[00:52:21.040 --> 00:52:25.920]   I think it was closer to 20. I gained back a few of those, but I'm still 16 pounds down.
[00:52:25.920 --> 00:52:33.200]   But it's not from dieting. Noom, you know, these fad diets and health trends come and go.
[00:52:33.200 --> 00:52:40.560]   But Noom is not a fad. It's not a diet. It's psychology. And that's why it works. Psychology
[00:52:40.560 --> 00:52:46.400]   that helps you make intentional, sustainable choices aligned with your values and your weight loss
[00:52:46.400 --> 00:52:53.280]   goals. So Noom uses well known, established psychological techniques to help you understand
[00:52:53.280 --> 00:52:59.760]   what your eating choices are and why you have cravings. I, for instance, I'm a fog eater. I'll
[00:52:59.760 --> 00:53:03.920]   sit down. I'll, you know, I come home from work and I'll just start stuffing my face. I don't even
[00:53:03.920 --> 00:53:09.040]   know I ate it. And afterwards, I don't even know I ate it. It's a complete fog. It's unconscious
[00:53:09.040 --> 00:53:14.240]   eating. One of the things Lisa and I do now is we put the fork down in between bites. We chew.
[00:53:14.240 --> 00:53:19.200]   We don't have the TV on. We don't have the phone in front of us. We just take clothes. Sometimes we
[00:53:19.200 --> 00:53:23.280]   close our eyes and taste the food. It makes such a difference. It's little things. That's,
[00:53:23.280 --> 00:53:28.560]   now that's mine. Everybody's journey is different. Noom will give you daily lessons personalized to
[00:53:28.560 --> 00:53:34.240]   your goals, your needs. They use scientific principles like cognitive behavioral therapy.
[00:53:34.240 --> 00:53:38.800]   So you understand your relationship with food, why you have cravings, why, why you can't seem to
[00:53:38.800 --> 00:53:42.800]   change what you're doing. But there's not a diet. It's nourishing, not restrictive.
[00:53:42.800 --> 00:53:49.840]   It focuses on progress, not perfection. You can have days off. You can eat anything you want.
[00:53:49.840 --> 00:53:55.280]   I remember early on in new, I had a hot dog and I felt so guilty out of my, I messaged my coach,
[00:53:55.280 --> 00:53:58.880]   my new coach. And I said, I had a hot dog. I'm sorry. She's, what are you talking about? There's
[00:53:58.880 --> 00:54:06.320]   no forbidden food. That's okay. That's okay. You choose your level of support, by the way. You can
[00:54:06.320 --> 00:54:10.400]   have that coach five minute daily check-ins. You can have personal coaching. You can have group.
[00:54:10.400 --> 00:54:14.400]   There's a group you can, you can get as much or as little of the lessons as you want.
[00:54:14.400 --> 00:54:20.320]   Days off, totally okay. Cause progress is in a straight line. You just want to do it day, you
[00:54:20.320 --> 00:54:24.560]   know, as you do it consistently, it's been a couple of years now for Lisa and me. It's really amazing.
[00:54:24.560 --> 00:54:29.200]   Maintenance is even better because I've learned I've internalized all the lessons, right?
[00:54:29.200 --> 00:54:34.640]   And if you get off track, Noom gets you back on track. First time Noomers lose an average of 15
[00:54:34.640 --> 00:54:40.960]   pounds in 16 weeks. 95% of customers say Noom is a good long-term solution. Of course,
[00:54:40.960 --> 00:54:47.200]   your mileage may vary, but they are that's based in science. They've published 50 scientific peer
[00:54:47.200 --> 00:54:53.840]   reviewed articles describing their methods and their effectiveness. So stop chasing health trends,
[00:54:53.840 --> 00:54:59.280]   start building sustainable, healthy habits with Noom's psychology based approach. I can tell you
[00:54:59.280 --> 00:55:07.280]   it works. Sign up for your trial today, Noom and OOM.com/twit. Noom.com/twit. Check out Noom's
[00:55:07.280 --> 00:55:11.680]   first ever book. It's out now. The Noom mindset might be a good way to start. Get the book and read
[00:55:11.680 --> 00:55:17.360]   about their psychology of behavior change. It explains how it all works. Whoever books are sold.
[00:55:17.360 --> 00:55:27.280]   And don't forget to sign up for your trial at Noom.com/twit. It really, really works.
[00:55:27.280 --> 00:55:32.640]   And I love them. Thank you, Noom, for supporting the show. All right, we've got to talk about
[00:55:32.640 --> 00:55:36.160]   the Supreme Court. I've been putting it off. It's like dessert. I've been putting it off.
[00:55:36.160 --> 00:55:42.960]   That's dessert. So there were a number of, the Supreme Court was a second Tuesday in October.
[00:55:43.760 --> 00:55:51.760]   They meet in October, right? They do the arguments and then they think and then the opinions start
[00:55:51.760 --> 00:55:57.760]   coming out. I mean, in theory, they trail at some reasonable amount of time after you hear the
[00:55:57.760 --> 00:56:03.120]   oral argument. But this one, they heard in October and there had been no opinion until May and that
[00:56:03.120 --> 00:56:08.480]   was an unusually long delay. I think, although everything about how this particular iteration of
[00:56:08.480 --> 00:56:12.720]   the court likes to do things is there's past practice and then there's this court. Yeah, we won't
[00:56:12.720 --> 00:56:18.800]   talk about the shadow docket. That's a whole matter entirely. Do you want to start with,
[00:56:18.800 --> 00:56:22.880]   we should, with the Andy Warhol case? Well, you want to do the good news or the bad news first?
[00:56:22.880 --> 00:56:28.240]   Well, let me let's state the facts of the case. Okay, well, the Warhol is the all bad news case.
[00:56:28.240 --> 00:56:33.520]   The other stuff on the 230 front was the surprisingly good news case. Yeah, well, we'll start with
[00:56:33.520 --> 00:56:38.960]   the bad news first. All right. Oh, gosh. So photographer, Ling Goldsmith in 1981, took a picture of Prince.
[00:56:38.960 --> 00:56:45.360]   Yes. That's what's on the left if you're watching the video. Andy Warhol on a commission from
[00:56:45.360 --> 00:56:52.720]   Variety, I think. It was a Conde Nast publication. Conde Nast ended up later and I don't know later.
[00:56:52.720 --> 00:56:58.560]   Okay. But I don't know if there's any fair later. Anyway, they commissioned these 16
[00:56:58.560 --> 00:57:05.680]   silkscreen images clearly taken from the original Lynn Goldsmith photograph. And the
[00:57:05.680 --> 00:57:10.880]   position of Andy Warhol's estate was, and your position to write, you wrote it on Mika's brief
[00:57:10.880 --> 00:57:19.600]   for this one. So they got light. So Andy Warhol made those silkscreen adaptations of the Goldsmith
[00:57:19.600 --> 00:57:23.920]   photo while he was alive. At some point, while he was alive, and I think he died in 1987. It's hard
[00:57:23.920 --> 00:57:28.960]   to make him when you're not alive. Let's just. And that is the fact that is largely overlooked
[00:57:28.960 --> 00:57:34.240]   in this decision, and it sort of should matter. And I don't mean sort of, it should have mattered.
[00:57:34.240 --> 00:57:40.800]   So essentially, they describe briefly the successor and interest to him, which was he had a
[00:57:40.800 --> 00:57:46.320]   foundation, the Andrew Warhol Foundation. I guess basically owns the copyrights in his works.
[00:57:46.320 --> 00:57:51.120]   And he was approached by Conde Nast because Prince now had died and they wanted to do an
[00:57:51.120 --> 00:57:56.480]   article about him. And they had a choice of like, well, when you said he was alive, you didn't mean
[00:57:56.480 --> 00:58:00.240]   any Warhol, you meant Prince. Sorry, no, any Warhol. Okay.
[00:58:00.240 --> 00:58:04.640]   I was confused. I thought she said, Andy Warhol made these prints while he was alive. You met
[00:58:04.640 --> 00:58:09.760]   while Prince was alive. No, Andy Warhol when he was alive made his own work. He did not make
[00:58:09.760 --> 00:58:13.920]   them after he died. And that makes sense. Okay. That's how it's usually done.
[00:58:13.920 --> 00:58:20.160]   It was alive because he made the Prince. It's also confusing because there's Prince Prince.
[00:58:20.160 --> 00:58:24.960]   He made the silkscreen. We'll go with that. He made the silkscreen. The images. He made the images.
[00:58:24.960 --> 00:58:29.520]   And while he was alive, and they got published somewhere, but then everything was okay then.
[00:58:29.520 --> 00:58:33.600]   And everything apparently was okay. Nobody complained. Nobody complained.
[00:58:33.600 --> 00:58:42.960]   Right. Then 20 years after Warhol died, Prince now died. And so Conde Nast wanted to illustrate
[00:58:42.960 --> 00:58:49.280]   the article they had on Prince. And they went to the Andy Warhol Foundation, which now was the
[00:58:49.280 --> 00:58:54.240]   successor in interest for the rights and Andy Warhol's works. And they said, can we license that?
[00:58:54.240 --> 00:59:02.560]   And the foundation said, great, give us $10,000 I think. And then Goldsmith found out about this
[00:59:02.560 --> 00:59:09.200]   and started to threaten them in some way. And so the foundation sued for declaratory judgment
[00:59:09.200 --> 00:59:13.760]   to say, well, this should be fair use. So you shouldn't have a leg to stand on and let's just
[00:59:13.760 --> 00:59:18.480]   have the court spool on this now. And the district court said, you're right, it's fair use. Nothing
[00:59:18.480 --> 00:59:25.840]   to see here. And then the second circuit undid that in a decision that really abandoned what the
[00:59:25.840 --> 00:59:30.480]   Supreme Court had said in the Campbell versus A. Cough Rose case, which was the one where
[00:59:30.480 --> 00:59:37.920]   two live crew had. Sample the music. Saturized the Oh, pretty woman Roy Orbison's.
[00:59:37.920 --> 00:59:42.080]   By the way, that word satirized. Yeah, I may actually be saying it wrong whether it's
[00:59:42.080 --> 00:59:45.680]   satirized or parody, but I think it's all dumb either way you slice it. So I hate to give it
[00:59:45.680 --> 00:59:51.280]   that dignity. If they just used it in the production, that wouldn't be fair use. But if they were
[00:59:51.280 --> 00:59:57.040]   satirizing or parodying it like weird Al does, that would be fair use. But only
[00:59:57.040 --> 01:00:02.960]   because I, the only reason I say this is because we are always thinking about fair use here.
[01:00:02.960 --> 01:00:09.120]   Only if it parodies the original creator of it. So it can't use
[01:00:11.280 --> 01:00:16.480]   you can't use it to make another a separate. Well, well, no, I mean, it's also too hard to
[01:00:16.480 --> 01:00:20.960]   figure out what the court just said. But in theory, the idea is in the fair use in the statute,
[01:00:20.960 --> 01:00:25.120]   it talks about one of the things that makes things more likely to be fair use is if it's
[01:00:25.120 --> 01:00:30.080]   commentary. And so the interpretation is, okay, if you're commenting on the original,
[01:00:30.080 --> 01:00:35.680]   you got to be able to use the original. So that's probably it is less clear whether commenting on
[01:00:35.680 --> 01:00:40.880]   anything else would be fair use. And this decision seems to drive a stake through the heart of the
[01:00:40.880 --> 01:00:46.400]   idea that you could comment on anything else using a previously existing work. But that's going to
[01:00:46.400 --> 01:00:52.560]   fall apart in practice and be awfully chilling for anybody who the expression is. Let me just
[01:00:52.560 --> 01:00:57.840]   just reason this phrase. I want to explain what it is. So people understand what the phrase fair use
[01:00:57.840 --> 01:01:07.600]   is. It is not a law, but it's a, it's a doctrine, right? It's a, it's a, it's a point of view that says
[01:01:08.480 --> 01:01:13.760]   you can, in a limited fashion, use somebody else's copyright and material without permission,
[01:01:13.760 --> 01:01:21.840]   providing it satisfies one of what is it? It is part of the law. It is in the law. It is in the law.
[01:01:21.840 --> 01:01:23.520]   So before the 1970s,
[01:01:23.520 --> 01:01:30.400]   U.S.C. 107. Before the 1976 act, it was interpreted to be part of the law.
[01:01:30.400 --> 01:01:34.960]   It is now in '76. But they codified it. They used a lot of what the judges were,
[01:01:34.960 --> 01:01:38.960]   the factors the judges were using to consider whether something was fair use or not. And then
[01:01:38.960 --> 01:01:45.040]   they baked them into, they baked them into the statute with the 1976 act, which is basically what
[01:01:45.040 --> 01:01:51.120]   we're using now. And so fair use is there. It is part of the law and it inherently, so what's in the
[01:01:51.120 --> 01:01:56.640]   law is if you have a copyright, there's some exclusive rights you get as part of a copyright
[01:01:56.640 --> 01:02:03.440]   subject to other limitations, including the ones articulated in, in 107 and 107 talks about fair use.
[01:02:03.440 --> 01:02:08.800]   So fair use and courts go back and forth with good language or bad language, but that though
[01:02:08.800 --> 01:02:14.000]   there is a take on this, that your copyright is inherently limited by
[01:02:14.000 --> 01:02:19.920]   things that the public can still do. And there's reasons for that because it's in
[01:02:19.920 --> 01:02:25.440]   society's interest. There's two interests, conflicting interests. The creator of the original work
[01:02:25.440 --> 01:02:30.960]   has an interest in protecting their rights so they can sell it and, you know, they've made it so
[01:02:30.960 --> 01:02:35.520]   they own it. But society also has a right that it may be reused in other fashion. I mean,
[01:02:35.520 --> 01:02:43.440]   you know, we wouldn't have Cinderella if Grimm's fairy tales didn't create that notion.
[01:02:43.440 --> 01:02:49.520]   Well, the whole point to have copyright at all is we want more stuff. And so we think
[01:02:49.520 --> 01:02:50.960]   we have to protect the, give the,
[01:02:50.960 --> 01:02:52.000]   Protect creators a little bit.
[01:02:52.000 --> 01:02:54.880]   Protect creators a little bit so that they will produce the stuff that we need.
[01:02:54.880 --> 01:02:58.400]   But then, but now we've taken the stuff away, like what's the point? You're not,
[01:02:58.400 --> 01:03:02.160]   so the question is what's the tipping point? What does the law need to do so we do get more stuff?
[01:03:02.160 --> 01:03:07.040]   Society's interest is that well, we use all of this stuff to create new stuff.
[01:03:07.040 --> 01:03:13.360]   Well, we want to become important in artificial intelligence later. But anyway,
[01:03:13.360 --> 01:03:19.120]   this is why it's a big deal. So, so Goldsmith's able to take this photo of Prince and have some
[01:03:19.120 --> 01:03:24.720]   rights. In fact, she did license it for 400 bucks to a vanity fair originally. But then they reused
[01:03:24.720 --> 01:03:29.600]   it 20 years later when Prince died, they reused the anti warhol things and, and
[01:03:29.600 --> 01:03:36.080]   Goldsmith said, you didn't pay for that. I still have the rights to it. So the real question was,
[01:03:36.080 --> 01:03:43.120]   is this anti warhol creation protected by fair use?
[01:03:43.120 --> 01:03:49.200]   And the court really botched that. And I think the reason that they botched it, well, there are a
[01:03:49.200 --> 01:03:53.840]   couple, that's a weird thing, why they botched it. But the big fundamental thing that I think
[01:03:53.840 --> 01:04:00.000]   they got wrong, and I've not seen this discussed in many other quarters. So other people may disagree,
[01:04:00.000 --> 01:04:08.080]   but they ended up focusing on the later license to Conde Nast as the use. But it wasn't the use
[01:04:08.080 --> 01:04:15.360]   of the original fair use applies to the use of the original, which is what Andy Warhol himself did,
[01:04:15.360 --> 01:04:21.760]   I guess in the 1980s when he made the adaptation of the Goldsmith picture. He used it in a way to
[01:04:21.760 --> 01:04:26.960]   say something new about Prince. And then he goes and he dies and he's dead for 20 years. And then
[01:04:26.960 --> 01:04:35.520]   later the party that ends up owning the copyrights and his new works start to use his works.
[01:04:35.520 --> 01:04:42.320]   And they judged fair use based on a use of his work and construed it to be a use of the original.
[01:04:42.320 --> 01:04:47.440]   And I think that burned down an awful lot. And I think that's a really big deal. And I think
[01:04:47.440 --> 01:04:51.920]   it's a really big deal that will blow up on copyright owners, because they are a copyright
[01:04:51.920 --> 01:04:56.960]   owner and his subsequent work. And sleeping on something that they own they think is fine and
[01:04:56.960 --> 01:05:02.320]   lately and later they go on to actually exploit the copyright in something they think they own.
[01:05:02.320 --> 01:05:05.680]   And then they're going to be called on the carpet to say that the thing you thought you owned
[01:05:05.680 --> 01:05:10.880]   actually turns out I guess to be infringing. And now I think it leaves the question of, well,
[01:05:11.840 --> 01:05:17.600]   are they liable for infringement to Goldsmith? Does Goldsmith inherently own the Warhol work?
[01:05:17.600 --> 01:05:22.720]   Because even the court leaves the door open to the fact that Warhol himself, when he made that
[01:05:22.720 --> 01:05:28.400]   adaptation may have that may have actually been fair use for whatever was filling his mind about
[01:05:28.400 --> 01:05:33.280]   what his motivation was like, this is not a sustainable rubric for understanding. So I'm confused.
[01:05:33.280 --> 01:05:40.080]   So is the court. Yeah. So there's a picture. Goldsmith owns the rights that picture.
[01:05:40.880 --> 01:05:44.000]   Andy Warhol makes those pick those. He adapts the picture.
[01:05:44.000 --> 01:05:48.560]   Adapts it. That's OK. According to the court. It could be according to the court.
[01:05:48.560 --> 01:05:53.280]   Because that's transformative. As they ran their four factor analysis test,
[01:05:53.280 --> 01:05:56.720]   which is the thing that court used to use that's now in the statute,
[01:05:56.720 --> 01:06:01.360]   according as the court did it, it said, well, based on facts that are not present here.
[01:06:01.360 --> 01:06:04.640]   Yeah, he still could be considered fair use. It's fair use because it's transformative.
[01:06:04.640 --> 01:06:10.320]   It could be. It was the fall. So when so Goldsmith sued the Andy Warhol foundation.
[01:06:10.560 --> 01:06:18.160]   Mm hmm. Over the publishing of this, these pictures in Vanity Fair, the licensing of the Warhol
[01:06:18.160 --> 01:06:23.520]   version for Vanity Fair. So her contention was, well, Andy could make these pictures,
[01:06:23.520 --> 01:06:29.040]   but the Andy Warhol foundation may not license them. Apparently somehow that was different.
[01:06:29.040 --> 01:06:34.320]   OK. And the court agreed. And the court agreed. So the court isn't saying you can't make
[01:06:34.320 --> 01:06:39.840]   transformative works of a somebody's photograph, but you can't license it. I mean, it's very
[01:06:39.840 --> 01:06:44.320]   strange what the court is doing. I mean, let's say I think there's a huge fundamental problem
[01:06:44.320 --> 01:06:49.600]   with the way the court ends up viewing the term use and where the fair use is, because it's really
[01:06:49.600 --> 01:06:54.160]   about the copier's use. And this idea that the Warhol foundation was somehow the copier when
[01:06:54.160 --> 01:06:58.400]   they didn't make the work, they just ended up owning the rights of the work that Warhol made.
[01:06:58.400 --> 01:07:04.400]   So Warhol is the person who made the use of the original and arguably fairly because he
[01:07:04.400 --> 01:07:10.240]   used it to say something else. So I think by breaking it down, you break the sustainability of copyright
[01:07:10.240 --> 01:07:16.080]   law, including for copyright holders. But it was also done. And the other really toxic thing about
[01:07:16.080 --> 01:07:20.800]   this decision is this idea of, well, how dare you make money from it? That when they ran the
[01:07:20.800 --> 01:07:28.400]   four factor test about commerciality, it really just basically meant that it resulted in, if you
[01:07:28.400 --> 01:07:35.520]   make a fair use for free, you're fine. But if you want to make money from it, you're not fine.
[01:07:35.520 --> 01:07:39.920]   But that matters. This is what Soto Myar wrote. She said, Goldsmith's original photograph of
[01:07:39.920 --> 01:07:47.200]   Prince and the anti Warhol foundations copying use of that photograph in an image, shares substantially
[01:07:47.200 --> 01:07:52.800]   the same purpose and the use of is of a commercial nature. Because it was like, oh, you're just making
[01:07:52.800 --> 01:07:56.880]   pictures of Prince to license to a magazine as if these are both two same things. But Warhol did
[01:07:56.880 --> 01:08:02.000]   not make that picture to license to a magazine. No. So somehow his in 10, 20 years later,
[01:08:02.000 --> 01:08:06.800]   was not commercial. It's construed to now be the Warhol foundations. None of this makes sense
[01:08:06.800 --> 01:08:11.840]   temporally. But the other thing, and actually, I need to criticize Kagan who wrote a really scathing
[01:08:11.840 --> 01:08:17.920]   and correct assent. I there's 87 pages of this PDF that got produced by the court. I did a search
[01:08:17.920 --> 01:08:22.000]   for First Amendment. And I've also read it. I don't think the term shows up at all. And the
[01:08:22.000 --> 01:08:25.520]   really important thing about like, what is the goals and purpose of copyright? We're trying to
[01:08:25.520 --> 01:08:30.720]   stimulate the creative, the creation of more of this stuff. But part of the reason we have fair use
[01:08:30.720 --> 01:08:34.960]   is because it is the codification of the First Amendment interest. Because what are we doing
[01:08:34.960 --> 01:08:40.160]   with exclusive rights taking away what the public can do, but essentially impinging on expression.
[01:08:40.160 --> 01:08:44.560]   And that's got a, the First Amendment still exists even in the wake of copyright. And there's
[01:08:44.560 --> 01:08:49.760]   other Supreme Court precedent that talks about how fair use is of indication of the free expression
[01:08:49.760 --> 01:08:53.280]   values that the First Amendment is making sure that Congress doesn't get to step on.
[01:08:53.280 --> 01:08:57.920]   And they're stepping on it because now all of a sudden, if you want to make subsequent expression
[01:08:57.920 --> 01:09:02.240]   that uses something else that came before, you're going to be in trouble, especially if you want
[01:09:02.240 --> 01:09:05.600]   to monetize it. And there's nothing in the First Amendment that says you don't get to make money
[01:09:05.600 --> 01:09:08.640]   off of your expression, because I'm something you have to, because it's the only way you're
[01:09:08.640 --> 01:09:10.560]   going to keep the lights on. This is a.
[01:09:10.560 --> 01:09:15.760]   So Justice Kagan and Chief Justice John Roberts with the two dissenters.
[01:09:15.760 --> 01:09:22.880]   She said it will stifle creativity of every sort. It will impede new art and music and literature.
[01:09:22.880 --> 01:09:27.360]   It will thwart the expression of new ideas and the attainment of new knowledge it will make our
[01:09:27.360 --> 01:09:34.320]   world poorer. That's absolutely right. And she, and I don't understand why Justice Sotomayor,
[01:09:34.320 --> 01:09:39.360]   who normally is one of my favorite justices, who normally sees things really clearly in all
[01:09:39.360 --> 01:09:45.360]   their implications is so blind to this. And then it seems to be the differences this commercial use.
[01:09:45.360 --> 01:09:50.320]   I mean, that seems to be the thing that the, the photographers, the photographers, like I made a
[01:09:50.320 --> 01:09:55.120]   picture and I didn't get to license it the second time around because it was a second picture that
[01:09:55.120 --> 01:10:00.320]   used mine. So I lost a market opportunity and that was not fair. And that's basically what the
[01:10:00.320 --> 01:10:04.240]   court is like. You're right. That's one of the four tests, right? Is it doesn't that the
[01:10:04.240 --> 01:10:10.000]   derivative work doesn't impinge on your market opportunity? It's not the way it's phrased. And so,
[01:10:10.000 --> 01:10:13.760]   but it's part of the problem. This case was like, what are the four factors say and what do they
[01:10:13.760 --> 01:10:22.480]   end up meaning? Because the effect on the commercial market of the, of the original work matters for
[01:10:22.480 --> 01:10:28.960]   the inquiry, but the degree it's supposed to matter is, as Kagan pointed out and the foundation would
[01:10:28.960 --> 01:10:33.600]   point out, it's not supposed to eat the whole. Because the first factor is about did it trans,
[01:10:33.600 --> 01:10:40.480]   was a transformative? Did it bring something new to the table, a new message? And what Kagan is
[01:10:40.480 --> 01:10:44.800]   screaming at Sotomayor about is like, of course, apart something new. So when Kanda
[01:10:44.800 --> 01:10:48.400]   asked how to choice about what image to run to illustrate the story, they wanted the new one,
[01:10:48.400 --> 01:10:52.720]   not because they want the photo, because they say different things. They stay illustrate
[01:10:52.720 --> 01:10:57.520]   prints. The artist in a very different way. It is the case that the first test,
[01:10:57.520 --> 01:11:04.320]   and this is from 17 USC 107 in determining whether the use made of a work in any particular
[01:11:04.320 --> 01:11:10.000]   case is fair use. The factors to be considered shall include, doesn't mean limited to, but shall
[01:11:10.000 --> 01:11:15.200]   include the purpose and character of the use, including whether such uses of commercial nature.
[01:11:15.200 --> 01:11:21.440]   I mean, it does say that. It does say that. And what this court has done is interpreted in a way
[01:11:21.440 --> 01:11:26.320]   where it just eats the whole and shuts everything down. It also makes me wonder what happens to the
[01:11:26.320 --> 01:11:31.520]   author of an original work who made it with no intention to modify it is all of a sudden their
[01:11:31.520 --> 01:11:36.560]   copyright weaker than somebody who produced a work to modify it. Court doesn't answer that question.
[01:11:36.560 --> 01:11:42.720]   There's a whole lot of hand waving over some really significant issues that are not addressed
[01:11:42.720 --> 01:11:47.440]   by this court. I don't know. How sweeping was this decision? So we're not entirely sure.
[01:11:47.440 --> 01:11:54.720]   This may be a decision that is so bad. It might end up being a nothing further because it is a
[01:11:54.720 --> 01:11:59.440]   precedent, but it may end up being so weird that basically everybody just ignores it and we carry
[01:11:59.440 --> 01:12:05.440]   on with stuff as we did. So that's one Pollyanna take. I've seen some of my friends who,
[01:12:05.440 --> 01:12:10.160]   you know, try to advocate for fair uses and things like that, talk about how well it's kind of
[01:12:10.160 --> 01:12:14.800]   rested on its facts and it's more narrow that, you know, commerciality is an important thing.
[01:12:14.800 --> 01:12:20.640]   And so that will in theory help a lot of the more nonprofit uses that don't want to get obliterated
[01:12:20.640 --> 01:12:25.120]   by litigation that if you take the money out of the equation in theory, this court, this decision
[01:12:25.120 --> 01:12:29.920]   doesn't really hurt them. I just think it makes such a mess out of everything doctrinally that
[01:12:29.920 --> 01:12:35.200]   we are all in trouble, including the copyright holders. So this is kind of like, okay, let's say
[01:12:35.840 --> 01:12:42.960]   so Michael Jackson did a song called Bad. Weird Al Yankovich did a derivative work of it called Fat,
[01:12:42.960 --> 01:12:48.800]   which is fair use. Actually, he got permission from Michael Jackson. He tends to get permission. But
[01:12:48.800 --> 01:12:54.720]   that he didn't have to. That's probably fair use. Probably he's never litigated it. He doesn't
[01:12:54.720 --> 01:12:58.560]   litigated it. He had one song, I think, where he didn't get the permission. But let's say,
[01:12:58.560 --> 01:13:05.360]   I'm mad at him. So he felt bad. Let's say then I played bad on the show to rather fat on the show
[01:13:05.360 --> 01:13:10.640]   to talk about weight loss. Let's say I played it in the new mad. Now it would make sense if
[01:13:10.640 --> 01:13:15.520]   Weird Al sued me because that was his song I was using. But if the Michael Jackson estate sued me
[01:13:15.520 --> 01:13:23.920]   saying, well, that derivative work was based on bad. So we you screwed us, but I wasn't using it
[01:13:23.920 --> 01:13:29.680]   to illustrate bad. I was using to illustrate fat. Is this kind of analogous to what we just saw?
[01:13:29.680 --> 01:13:33.440]   And by the way, I'm doing it commercially because I sell ads. It's for an ad.
[01:13:33.440 --> 01:13:39.520]   I think the upshot to this decision is that could be a problem. And I don't think that's the
[01:13:39.520 --> 01:13:43.280]   way something is weird problem. It's a weird problem. I don't think the court addresses it. I think
[01:13:43.280 --> 01:13:48.960]   the court creates that problem. And I don't think that's a vector of litigation liability that's
[01:13:48.960 --> 01:13:53.440]   necessarily existed before where we've really had to contend with it. I mean, we're always in a
[01:13:53.440 --> 01:13:57.280]   problem of did you play the song and did that infringe on the copyright based in the song.
[01:13:57.280 --> 01:14:01.680]   But I think what they're basically saying is, yeah, I think that's actually a really good analogy of
[01:14:01.680 --> 01:14:06.720]   what they've just set up. Even the original work has a surprise. The Michael Jackson
[01:14:06.720 --> 01:14:10.800]   copyright holder could come after you for doing it for having played a derivative work.
[01:14:10.800 --> 01:14:14.000]   That's a derivative work. Because that may have been fair use in that context,
[01:14:14.000 --> 01:14:16.800]   but it's not fair use in the context. Yes, because I'm doing it commercially now.
[01:14:16.800 --> 01:14:21.520]   And that's going to not help your four factor analysis, but that because you use the derivative
[01:14:21.520 --> 01:14:26.480]   that the copyright holder in the original has some cause of action against you.
[01:14:26.480 --> 01:14:29.680]   That's the same thing that happened just now. I think that's and I don't think it's supposed
[01:14:29.680 --> 01:14:35.040]   to work that way. That's very interesting. I've heard a theory that with this case,
[01:14:35.040 --> 01:14:39.840]   there might have been a factor of kind of like modern art. My kid could have done that,
[01:14:39.840 --> 01:14:47.280]   which kind of well, that's the question is that that was a derivative work that Andy Warhol did.
[01:14:48.800 --> 01:14:54.560]   They did accept that Eddie Warhol's work was fair use, right? That his work itself was fair use.
[01:14:54.560 --> 01:14:59.520]   They I think they said it could be. I mean, in theory, that wasn't necessarily the facts brought
[01:14:59.520 --> 01:15:03.680]   to the case. They weren't talking about what Warhol did to the original because that wasn't
[01:15:03.680 --> 01:15:09.200]   what the litigation. Well, it ended up strange. Somehow it should have been maybe what the
[01:15:09.200 --> 01:15:13.120]   litigation was about, but somehow it ended up not being what the litigation was out about by the
[01:15:13.120 --> 01:15:17.920]   time it ended up at the Supreme Court. I'm wondering how this happened. And it seems like the court
[01:15:17.920 --> 01:15:23.520]   got so fixated on the four factors and how it wanted to interpret the commerciality aspect of it
[01:15:23.520 --> 01:15:27.840]   that they just got so lost in the forest factors that they lost the forest for the trees in terms of
[01:15:27.840 --> 01:15:31.600]   what is actually going on here and what is the implication. So unfortunately, what they've done
[01:15:31.600 --> 01:15:34.720]   is they've really muddied the whole concept of figures. I think they've muddy things in a really
[01:15:34.720 --> 01:15:38.320]   disastrous way because somebody owns those weird owl copyrights. And I don't think they're going to
[01:15:38.320 --> 01:15:44.720]   be happy to know that they might be in trouble based on issues raised by whoever he parodied in
[01:15:44.720 --> 01:15:49.120]   the first place. And I think he I think that's a vector of attack that this decision sets up.
[01:15:49.120 --> 01:15:54.960]   That's really I also think it's really interesting to think about how this is going to have implications
[01:15:54.960 --> 01:16:03.520]   in tech because I mean, what's going to happen with all of these like AI, Drake voice situations or
[01:16:03.520 --> 01:16:10.160]   I've written about fair use and copyright law a little bit in the context of fan communities.
[01:16:10.160 --> 01:16:18.640]   Like there was a Bridgerton musical and that's right. They got shut down. That was on TikTok. Yeah.
[01:16:18.640 --> 01:16:25.280]   Yeah, but that actually so I had written about the case when Netflix had sued the people that
[01:16:25.280 --> 01:16:31.440]   created the Bridgerton musical, which was basically just like these two songwriters on TikTok writing
[01:16:31.440 --> 01:16:36.720]   songs like in real time on the app. And then they made an album and then the album. So cool.
[01:16:36.720 --> 01:16:43.680]   Yeah, I mean, I believe. Yes. But I think they settled the case because I mean, I might be wrong,
[01:16:43.680 --> 01:16:49.440]   but the last time I looked at it, I think they settled the case. But there are a lot of precedence
[01:16:49.440 --> 01:16:55.920]   or there's a lot of like conversation about this in the context of like fan fiction where
[01:16:55.920 --> 01:17:02.160]   archive of our own, which is like a really big fan fiction website. They literally were like
[01:17:02.160 --> 01:17:09.600]   created in tandem with copyright lawyers to figure out like, how can we protect 16 year olds writing
[01:17:09.600 --> 01:17:16.640]   about like whatever book they're interested in when it's like, like you can't even link to a
[01:17:16.640 --> 01:17:24.480]   Venmo or like a coffee on a03 because they are so afraid of getting sued. Netflix settled.
[01:17:24.480 --> 01:17:30.480]   Which so it really, we don't know what happened and the court never decided it got to decide on
[01:17:30.480 --> 01:17:35.040]   this Netflix settled. But that's a really interesting case.
[01:17:35.040 --> 01:17:44.960]   And I think similarly with what we were talking about with government officials making legislation
[01:17:44.960 --> 01:17:50.400]   about the internet when they don't really know how the internet works. I also feel like that happens
[01:17:50.400 --> 01:17:59.520]   with art to an extent where like you could write a dissertation on whether or not this Andy Warhol
[01:17:59.520 --> 01:18:06.400]   work is like just copying or if it's an original work. And I mean,
[01:18:06.400 --> 01:18:12.160]   But the court wasn't really thinking about that. That wasn't really the issue was whether Andy's
[01:18:12.160 --> 01:18:17.280]   work was derivative or not. They were thinking about it's so focused on the photographer that she
[01:18:17.280 --> 01:18:23.120]   took a picture to be licensed in a magazine. She was putting her product in the to be licensed to a
[01:18:23.120 --> 01:18:28.720]   magazine market space and a magazine comes along wants a picture of exactly the same thing that
[01:18:28.720 --> 01:18:34.960]   she had taken a picture of. And the court was deeply offended that the subsequent work could
[01:18:34.960 --> 01:18:39.360]   somehow take that market opportunity from her because the court just treated them as equivalents
[01:18:39.360 --> 01:18:43.440]   and Kagan and in the dissent started screaming at the majority of like they're not equivalent.
[01:18:43.440 --> 01:18:49.360]   And similarly Netflix said different things. Netflix loved that the the Bridgerton parody
[01:18:49.360 --> 01:18:57.760]   when it was on TikTok. But it wasn't until they went to the Kennedy Center and had a paying a ticket
[01:18:58.720 --> 01:19:05.200]   a price for a paying concert that Netflix said they said they crossed the line
[01:19:05.200 --> 01:19:10.320]   that they had tried to establish with Barlow and Bear the the creators of the
[01:19:10.320 --> 01:19:18.960]   because Netflix has his own Bridgerton experience. And so in the minds of Bridgerton fans, there was
[01:19:18.960 --> 01:19:25.040]   a confusion between the derivative work and the original work. So one other thing that ends up
[01:19:25.040 --> 01:19:31.040]   happening in the space is there's other things that can go wrong like passing off or right to
[01:19:31.040 --> 01:19:35.200]   publicity issues. Like there's other things that sort of offend us is like, wait, maybe that isn't
[01:19:35.200 --> 01:19:39.520]   a thing that should happen. But it isn't copyright that solves that problem. It is other legal
[01:19:39.520 --> 01:19:45.040]   doctrines and other legal rights that might solve the problem. And we tend to just throw it all
[01:19:45.040 --> 01:19:50.560]   under copyright and let copyright fix it. But it ends up contorting copyright. So it stops doing
[01:19:50.560 --> 01:19:55.840]   what it's supposed to do. We need a public space. We need a public domain at town square where
[01:19:55.840 --> 01:20:02.240]   all ideas go and can be reworked because that's how creativity happens. Kagan is pointing out in
[01:20:02.240 --> 01:20:06.640]   the dissent that we've really impinged on that though. Like everybody, we already had to look
[01:20:06.640 --> 01:20:10.880]   over our shoulder where the joke was fair use is the right to hire a lawyer. Like there was already
[01:20:10.880 --> 01:20:16.320]   a lot of chilling. There is this decision would appear to produce more chilling. It kills the
[01:20:16.320 --> 01:20:20.640]   commons. And I'm thinking about, you know, I haven't seen a whole lot of people make the point that
[01:20:20.640 --> 01:20:25.040]   I just made. So stay tuned till next week where we can see if people take issue with it. But I
[01:20:25.040 --> 01:20:30.080]   think it's actually the upshot and that hurts copyright holders. So even if you buy the idea that
[01:20:30.080 --> 01:20:33.520]   copyright holders should have more and stronger rights, I think it just weakens them.
[01:20:33.520 --> 01:20:40.080]   That's really an interesting side effect. It makes me wonder I must be wrong because I can't
[01:20:40.080 --> 01:20:45.280]   believe nobody noticed. Well, it's bad enough on the face of it. They don't even need to dig
[01:20:45.280 --> 01:20:53.280]   any deeper. Yeah. You know, there is an absolute chilling effect. YouTube is very much a part of
[01:20:53.280 --> 01:20:58.480]   that. We are very, we can't do a lot of the things that I think are as a news organization are
[01:20:58.480 --> 01:21:03.920]   clearly protected by fair use because we'll get rightly or wrongly, we'll get a DMC take down on
[01:21:03.920 --> 01:21:08.000]   YouTube and then we lose those views. And there's First Amendment implications of it. You're trying
[01:21:08.000 --> 01:21:12.240]   to do expression that normally the First Amendment should protect. So why does copyright get to
[01:21:12.240 --> 01:21:17.360]   say no to it? And you have to be really careful about what copyright says no to because of the
[01:21:17.360 --> 01:21:21.120]   First Amendment. And that's what fair use is supposed to be there to make sure it doesn't say
[01:21:21.120 --> 01:21:25.920]   no to too much. I guess my point is maybe we would win in court. We could probably defend it.
[01:21:25.920 --> 01:21:30.240]   Right. The right tie or a lawyer. Yeah. It doesn't matter because it's too late.
[01:21:30.240 --> 01:21:36.800]   By then we've already lost any commercial value to the show. So the chilling effect is just taking
[01:21:36.800 --> 01:21:41.680]   us down for any length of time. So we don't even dare risk that. Not because we're worried about
[01:21:41.680 --> 01:21:47.120]   court. We might win in court. You don't want to pay those lawyers. But we can't. Yeah. Well,
[01:21:47.120 --> 01:21:52.720]   even if even if I wanted to pay the lawyers, the value of the show is gone after a few days. So
[01:21:52.720 --> 01:21:57.040]   by simply by virtue, yeah, you have the right to appeal, but by virtue of taking it down. And
[01:21:57.040 --> 01:22:03.120]   they do, by the way, you know, content ID takes stuff down immediately because it's automated.
[01:22:03.120 --> 01:22:08.480]   And yeah, we can appeal. But if it's gone for five days, we've lost the value anyway. So this
[01:22:08.480 --> 01:22:13.520]   is the problem we're in right now. And I, you know, I don't know what the answer is to it, except.
[01:22:13.520 --> 01:22:17.360]   Well, not this, but the problem is there's a whole lot of people on team,
[01:22:17.360 --> 01:22:19.920]   give more to the copyright holders, give more of the copyright holders.
[01:22:19.920 --> 01:22:24.000]   It's not good for them. I think it's not good for them. I think this case has just created a very
[01:22:24.000 --> 01:22:30.720]   real problem for them. But, you know, the reason that there's people like me and EFF and public
[01:22:30.720 --> 01:22:36.320]   knowledge that have been pointing out like you have to protect fair use is because ultimately,
[01:22:36.320 --> 01:22:40.400]   you know, there's other people expressing stuff and they need to be able to. And that we've
[01:22:40.400 --> 01:22:45.200]   accidentally untuned copyright law that it's saying no to expression, but the whole point of
[01:22:45.200 --> 01:22:49.360]   copyright law is to incentivize expression. How are you incentivizing something that you're
[01:22:49.360 --> 01:22:53.440]   chilling with the exact same rules that you're doing? You've got to tune it better. There's just
[01:22:53.440 --> 01:22:58.400]   no point in having it. So that was the bad news. That was the bad news. Also, I feel like nobody
[01:22:58.400 --> 01:23:02.880]   on the Supreme Court actually read my amicus brief because I made these points very clearly,
[01:23:02.880 --> 01:23:07.200]   very capably, and they didn't show up anywhere. And I'm really alarmed that the words First
[01:23:07.200 --> 01:23:10.880]   Amendment did not show up in the A/7 statement. Even Justice Kagan and Roberts didn't.
[01:23:10.880 --> 01:23:16.320]   Even they did. And I think it would have helped them possibly win their argument because ultimately,
[01:23:16.320 --> 01:23:20.240]   they were kind of talking about the effects, like the First Amendment value in terms of the
[01:23:20.240 --> 01:23:24.400]   expression was going to foster. But I think they really needed to doctrinally say this.
[01:23:24.400 --> 01:23:29.280]   And we miss Justice Breyer on the court because he actually understood these issues. And he had
[01:23:29.280 --> 01:23:37.040]   written really important dissent some earlier copyright decisions at the court,
[01:23:37.040 --> 01:23:40.880]   where he talked about the dance that the copyright law needs to do with the First Amendment,
[01:23:40.880 --> 01:23:44.800]   which interestingly, Justice Alito signed on to one of them. And now he signed on to the
[01:23:44.800 --> 01:23:47.520]   Sotomayor majority. So I don't know what he's thinking this is.
[01:23:47.520 --> 01:23:51.920]   There were two other cases, kind of interlinked cases that the Supreme Court took.
[01:23:51.920 --> 01:23:57.440]   We talked a lot about it after oral arguments. I think you were on with us after oral arguments,
[01:23:57.440 --> 01:24:01.520]   because you were in the court during oral arguments, which must have been an interesting thing.
[01:24:01.520 --> 01:24:06.640]   There were two days in a row. They really were the similar case that for some reason,
[01:24:06.640 --> 01:24:10.640]   they separated into two different cases. Similar fact patterns. They were different cases brought
[01:24:10.640 --> 01:24:14.320]   by different plaintiffs that had different cases that came out of the Ninth Circuit.
[01:24:14.320 --> 01:24:19.440]   So they had to separate them? Well, they were two separate things. And the Ninth Circuit kind
[01:24:19.440 --> 01:24:24.880]   of rolled on them together. And then when the cert petitions were filed, Gonzalez, I think,
[01:24:24.880 --> 01:24:29.200]   was the one that had the original cert petition. And then Twitter said,
[01:24:29.200 --> 01:24:32.960]   we're going to do a conditional one where if you grant Gonzalez, please do our case too.
[01:24:32.960 --> 01:24:38.800]   So Gonzalez versus Google and Twitter versus Tambana, in both cases, a terrorist act,
[01:24:38.800 --> 01:24:47.440]   the family of a victim sued the social media, trying to hold them liable for the attacks because
[01:24:47.440 --> 01:24:53.760]   social media allowed terrorist organizations to have accounts on either in that case of YouTube.
[01:24:53.760 --> 01:25:00.480]   In case of Google was YouTube. In case of Twitter was Twitter. And that somehow that meant
[01:25:00.480 --> 01:25:05.280]   they were liable for it. Yeah, it was all a little bigger than that. And it depended also on which
[01:25:05.280 --> 01:25:10.960]   case. But the general thing is terrorists do bad things and terrorists are also using the internet.
[01:25:10.960 --> 01:25:17.040]   So both these were both on the theories of internet is being used by terrorists. So therefore,
[01:25:17.040 --> 01:25:20.720]   internet must be liable in some way for what the terrorists are doing. Historically, and we've
[01:25:20.720 --> 01:25:28.960]   celebrated this many times, the internet is protected by section 230 of the of the DMCA or the
[01:25:28.960 --> 01:25:30.560]   communications.
[01:25:30.560 --> 01:25:37.040]   Who's wrong with the CDA saying that we're not you know, you were just can't be held liable for that,
[01:25:37.040 --> 01:25:44.000]   right? Is this a so now the Supreme Court, by the way, took the side of section 230.
[01:25:45.040 --> 01:25:51.200]   They avoided it, but they avoided it in a way that may actually be better. So they punted.
[01:25:51.200 --> 01:25:55.520]   Well, they punted, but they did better than punting. If this was actually sort of,
[01:25:55.520 --> 01:26:02.640]   I don't know if I want to say this is absolutely, but best case scenario is actually a reasonable
[01:26:02.640 --> 01:26:06.960]   term that some observers are talking about for those of us who wanted to maintain things.
[01:26:06.960 --> 01:26:12.800]   So Tamina was nine and nothing in favor of Twitter. And Google versus Gonzalez,
[01:26:12.800 --> 01:26:17.520]   or Gonzales v. Google, I don't remember how it's postured. Basically that one was punted.
[01:26:17.520 --> 01:26:21.120]   So that was the one that when I came on and talked about, because this is when I did the brief,
[01:26:21.120 --> 01:26:26.320]   and I was in the courtroom for was not well litigated that the plaintiff.
[01:26:26.320 --> 01:26:30.000]   It's funny. I thought it was the Google versus Gonzalez that would end up being the.
[01:26:30.000 --> 01:26:30.800]   Well, everybody did.
[01:26:30.800 --> 01:26:31.600]   It's always for school.
[01:26:31.600 --> 01:26:35.200]   That's the one where the we listened to the arguments, you were in the room. That was the one
[01:26:35.200 --> 01:26:40.960]   where they had some point said, well, the problem is thumbnails, which was not the thing that they
[01:26:40.960 --> 01:26:47.680]   had argued. They changed it. So they did rule on Tamina, the second case, the Twitter case,
[01:26:47.680 --> 01:26:50.000]   the one that was more conditional, that only went along for the ride.
[01:26:50.000 --> 01:26:54.080]   But then said, but based on our ruling and Tamina, there's no underlying cause of action in
[01:26:54.080 --> 01:26:55.680]   Gonzalez. So we don't have to think about it.
[01:26:55.680 --> 01:26:59.840]   So a couple of things are happening here. Leading up to this, everyone was really alarmed that
[01:26:59.840 --> 01:27:04.080]   either of these cases were taken because. And it looked like the Supreme Court took them both.
[01:27:04.080 --> 01:27:07.680]   Well, and one was. We then eye towards undermining 230.
[01:27:07.680 --> 01:27:13.600]   Right, because in earlier cases, Justice Thomas and eventually joined by probably Alito, I forget,
[01:27:13.600 --> 01:27:17.920]   it was started to get joined by people who were like, we need to take a look at section 230.
[01:27:17.920 --> 01:27:22.240]   We need to take a look at section 230. And in these companion concurrences, they were writing,
[01:27:22.240 --> 01:27:26.400]   they were articulating their understanding of section 230. That was alarmingly wrong.
[01:27:26.400 --> 01:27:31.120]   So it looked like they were that they plucked these cases out and decided to take them because
[01:27:31.120 --> 01:27:35.600]   they really wanted to make a massive section 230. So the internet community was alarmed deeply
[01:27:36.320 --> 01:27:41.040]   and did major, major brief work on both these cases as they went to the Supreme Court.
[01:27:41.040 --> 01:27:46.560]   Then what something weird happened. So we went into the courtroom for Gonzalez v. Google.
[01:27:46.560 --> 01:27:52.800]   Notice that there were some significant issues in how it was being argued by the plaintiffs and
[01:27:52.800 --> 01:27:56.960]   that the court was noticing and that they kept saying, like, let me make sure I understand your
[01:27:56.960 --> 01:28:00.960]   argument correctly, which is not a good thing you want to hear from any number of justices.
[01:28:00.960 --> 01:28:04.880]   And it looked like, and I think I came on the show and said it didn't look like they had the appetite
[01:28:04.880 --> 01:28:10.480]   to undermine section 230 on these on these claims.
[01:28:10.480 --> 01:28:17.040]   One of the things that the plaintiffs were arguing against Google was they use algorithms.
[01:28:17.040 --> 01:28:22.640]   Right. The idea was well to the extent that we think this is what they were litigating.
[01:28:22.640 --> 01:28:26.960]   Terrorist content through algorithms and that made them publishers. That made them
[01:28:26.960 --> 01:28:29.360]   complicit in the appearance of these videos on these.
[01:28:29.360 --> 01:28:36.400]   So section 230 protects a platform for content created by others, briefly paraphrase it,
[01:28:36.400 --> 01:28:38.640]   but it doesn't protect you from the stuff you create yourself.
[01:28:38.640 --> 01:28:43.200]   So if you're a publisher, if I publish, you know, if Elon Musk publishes a tweet, that's
[01:28:43.200 --> 01:28:46.000]   a different than if I publish a tweet because he's Twitter.
[01:28:46.000 --> 01:28:50.640]   Right. He's liable for his own stuff that he publishes, but he's not right for the third party
[01:28:50.640 --> 01:28:55.200]   who used his platform to publish it. So I mean, then you can litigate over, well,
[01:28:55.200 --> 01:28:58.640]   and what this case was litigating over, well, technically who created it.
[01:28:58.640 --> 01:29:04.160]   And so the plaintiff's argument, basically, although it was not well-pled, was if you
[01:29:04.160 --> 01:29:11.280]   algorithmically promote something that somebody else created, now you count as the creator.
[01:29:11.280 --> 01:29:12.480]   Or section 230 doesn't.
[01:29:12.480 --> 01:29:13.920]   If you create a thumbnail.
[01:29:13.920 --> 01:29:19.280]   Well, and this was now, this was if they wanted to got 230, this was
[01:29:19.280 --> 01:29:20.320]   I was listening.
[01:29:20.320 --> 01:29:22.800]   I was listening to the oral arguments and poor Lisa, I was going,
[01:29:23.600 --> 01:29:25.280]   they don't create the thumbnail.
[01:29:25.280 --> 01:29:30.320]   I was sitting next to the press pool and a woman was sitting in the press pool who was
[01:29:30.320 --> 01:29:33.600]   like new to these issues and I could hear her under her breath being like, but that's the
[01:29:33.600 --> 01:29:36.880]   whole argument and stuff like that. Like everybody was screaming at it.
[01:29:36.880 --> 01:29:41.920]   So what's interesting is, and it was just as Thomas who wrote this opinion,
[01:29:41.920 --> 01:29:46.880]   he actually said, no, no, no, algorithmic algorithm is everywhere.
[01:29:46.880 --> 01:29:48.480]   It's not publishing.
[01:29:48.480 --> 01:29:50.560]   So that was in the Tamna case.
[01:29:50.560 --> 01:29:55.120]   So with the Google versus Garcia, they basically punted it because they basically looked at it.
[01:29:55.120 --> 01:29:56.320]   And said, we did it in Tamna.
[01:29:56.320 --> 01:29:58.320]   But they hung their hat on Tamna.
[01:29:58.320 --> 01:30:01.840]   So Tamna wasn't really dealing with section 230 altogether.
[01:30:01.840 --> 01:30:02.800]   Oh, this is interesting.
[01:30:02.800 --> 01:30:07.120]   Do you think they did that so they wouldn't, that this wouldn't apply to 230?
[01:30:07.120 --> 01:30:11.120]   We are really baffled because this is why we think it's the best case scenario.
[01:30:11.120 --> 01:30:16.880]   Because it's getting really hard to litigate 230 since so many people don't like it or misunderstand it.
[01:30:16.880 --> 01:30:21.920]   What he ended up saying was basically like, look, if you had no 230, would there be a claim
[01:30:21.920 --> 01:30:25.280]   anyway? Because essentially it's a concept of secondary liability.
[01:30:25.280 --> 01:30:26.400]   It's aiding in a betting.
[01:30:26.400 --> 01:30:27.280]   Is it aiding in a betting?
[01:30:27.280 --> 01:30:28.240]   Is it aiding in betting?
[01:30:28.240 --> 01:30:32.560]   And the secondary liability is basically like, you know, if you do something you're responsible
[01:30:32.560 --> 01:30:35.040]   for it, but am I somehow responsible for the thing you did?
[01:30:35.040 --> 01:30:35.920]   Right.
[01:30:35.920 --> 01:30:36.640]   When you're the one who didn't-
[01:30:36.640 --> 01:30:37.920]   Do I even bet you somehow?
[01:30:37.920 --> 01:30:39.680]   Yeah, or this is your show.
[01:30:39.680 --> 01:30:40.880]   So it'll be the other way around.
[01:30:40.880 --> 01:30:41.200]   Right.
[01:30:41.200 --> 01:30:45.280]   So like, are you liable for something I say right now because it's your show?
[01:30:45.280 --> 01:30:48.720]   Mike Masnick says that actually in his opinion,
[01:30:48.720 --> 01:30:53.520]   Justice Thomas gave the best argument in favor of 230 without mentioning.
[01:30:53.520 --> 01:30:58.400]   Right. Because basically 230 basically doesn't really change that.
[01:30:58.400 --> 01:31:02.080]   But if there's no secondary liability, then there'd be no liability anyway.
[01:31:02.080 --> 01:31:04.720]   Maybe that didn't come out clear.
[01:31:04.720 --> 01:31:07.040]   But basically there would be no cause of action.
[01:31:07.040 --> 01:31:10.960]   Because you couldn't have been liable in the first place because you weren't the actor.
[01:31:10.960 --> 01:31:13.120]   It's not being pled that you were the actor.
[01:31:13.120 --> 01:31:16.320]   It's being pled that somehow there's a cause of action against you
[01:31:16.320 --> 01:31:18.960]   for secondary liability for something somebody else did.
[01:31:18.960 --> 01:31:23.040]   And he said, this statute doesn't reach that because that's not what aiding and betting
[01:31:23.040 --> 01:31:24.320]   could possibly mean.
[01:31:24.320 --> 01:31:29.360]   In which case, you still need Section 230 because it helps dispense with the litigation early.
[01:31:29.360 --> 01:31:29.600]   Yeah.
[01:31:29.600 --> 01:31:33.680]   But actually it kind of looks like with this, on a regular motion to dismiss,
[01:31:33.680 --> 01:31:37.440]   you could actually get rid of it because there's no cause of action for aiding and
[01:31:37.440 --> 01:31:41.440]   abetting for something that ISIS happens to use the platform for.
[01:31:41.440 --> 01:31:46.000]   And the language that Thomas used to talk about the algorithms and as infrastructure is that
[01:31:46.000 --> 01:31:53.280]   without something a lot more than what just a platform being a platform is not enough to
[01:31:53.280 --> 01:31:55.200]   create second to create second to failure.
[01:31:55.200 --> 01:31:59.840]   The fact that Twitter exists and that these tweets were on Twitter, it does not mean Twitter
[01:31:59.840 --> 01:32:01.200]   aided and abetted these terms.
[01:32:01.200 --> 01:32:04.400]   Yeah. He's kind of like, maybe there's some hypothetical things out there, but you got to
[01:32:04.400 --> 01:32:10.080]   plead an awful lot more before you can start construing that the way tort liability works,
[01:32:10.080 --> 01:32:13.200]   that you could have culpability on somebody who was not the direct actor.
[01:32:13.200 --> 01:32:14.800]   So this was the correct decision.
[01:32:14.800 --> 01:32:16.080]   It was a very good decision.
[01:32:16.080 --> 01:32:16.720]   We all agree about it.
[01:32:16.720 --> 01:32:17.680]   And nine to nothing.
[01:32:17.680 --> 01:32:19.040]   So we don't have to worry about that.
[01:32:19.040 --> 01:32:20.160]   It was a Thomas decision.
[01:32:20.160 --> 01:32:20.880]   It was very clear.
[01:32:20.880 --> 01:32:29.840]   But because it doesn't really mention 230, does it help strengthen 230?
[01:32:29.840 --> 01:32:31.280]   It doesn't weigh.
[01:32:31.280 --> 01:32:33.920]   And I think this is something that Masna is getting to.
[01:32:33.920 --> 01:32:35.600]   It doesn't weaken it.
[01:32:35.600 --> 01:32:41.440]   And what it kind of says is, well, all right, so we talked about Senator Blumenthal before
[01:32:41.440 --> 01:32:44.960]   he's since tweeted of, well, since the courts aren't going to give us a relief, we have to go to
[01:32:44.960 --> 01:32:45.440]   Congress.
[01:32:45.440 --> 01:32:51.440]   But basically, so in some sense, it pokes the hive of people who hate Section 230 because
[01:32:51.440 --> 01:32:53.680]   they didn't, the court didn't blow it up for them.
[01:32:53.680 --> 01:32:59.440]   But what it basically did is it kind of painted the picture of like, why we don't have the liability.
[01:32:59.440 --> 01:33:04.320]   Not as a Section 230 doing the internet as a favor, but just as a matter of law as it's been
[01:33:04.320 --> 01:33:08.240]   for hundreds of years, we don't have liability that works like this.
[01:33:08.240 --> 01:33:09.600]   And I was reading it today.
[01:33:09.600 --> 01:33:13.360]   And I almost think that Mike's piece is where he summarizes is good.
[01:33:13.360 --> 01:33:17.680]   I think it's even better than Mike says because Mike's thinking about it in the Section 230 context.
[01:33:17.680 --> 01:33:21.520]   I'm thinking about it as American law does not bend like this.
[01:33:21.520 --> 01:33:25.360]   And it shouldn't bend like this because this whole idea of what we've been developing to of
[01:33:25.360 --> 01:33:30.560]   my brother's keeper, that like third parties are responsible for something somebody else does,
[01:33:30.560 --> 01:33:35.760]   we traditionally have allowed it, but only in narrow circumstances because it doesn't feel fair
[01:33:35.760 --> 01:33:38.560]   to be responsible for something somebody else did.
[01:33:38.560 --> 01:33:43.840]   And this was a big pushback on that trend in the law to say, you know, the direct actor,
[01:33:43.840 --> 01:33:47.840]   totally liable, but non-direct actors, you've got to be really careful.
[01:33:47.840 --> 01:33:49.840]   And that's the way the law has always been.
[01:33:49.840 --> 01:33:55.120]   And this was a somewhat scholarly, I got the impression, somewhat scholarly opinion.
[01:33:55.120 --> 01:33:56.080]   It was nice.
[01:33:56.080 --> 01:34:00.880]   I mean, Thomas like looked at what abating and abetting means in common law and so forth and
[01:34:00.880 --> 01:34:04.320]   really said the right things and did the right thing.
[01:34:04.320 --> 01:34:07.840]   There might be some credible criticism, but a first-blanche reading it was,
[01:34:07.840 --> 01:34:11.760]   I am going to enjoy using that decision in Miami, you see.
[01:34:11.760 --> 01:34:12.720]   So good news.
[01:34:12.720 --> 01:34:13.360]   Good news.
[01:34:13.360 --> 01:34:14.960]   Surprisingly good news.
[01:34:14.960 --> 01:34:17.360]   And it doesn't matter that they punted Google.
[01:34:17.360 --> 01:34:19.840]   No, I mean, I think it had to happen.
[01:34:19.840 --> 01:34:21.680]   And it was probably just as well for everybody.
[01:34:21.680 --> 01:34:23.440]   It was badly argued, the plaintiff's argued.
[01:34:23.440 --> 01:34:26.880]   It was a, the court couldn't blow up to 30 on that particular case,
[01:34:26.880 --> 01:34:32.160]   but I think what Mike's point is is that the Tamna case, basically,
[01:34:32.160 --> 01:34:36.400]   because then they used the Tamna case to say that, that's what they used to get rid of the
[01:34:36.400 --> 01:34:40.880]   Google case to say that, you know, even if they had played correctly, you still wouldn't have had
[01:34:40.880 --> 01:34:42.400]   any underlying liability here.
[01:34:42.400 --> 01:34:45.280]   So the internet survived.
[01:34:45.280 --> 01:34:47.840]   And that's why we can have all of our nice social media platforms now.
[01:34:47.840 --> 01:34:50.080]   All right.
[01:34:50.080 --> 01:34:50.480]   Thank you.
[01:34:50.480 --> 01:34:55.440]   This is Kathy, you wrote the brief for Google versus Gollandzollis versus Google.
[01:34:55.440 --> 01:34:56.560]   I wrote one there.
[01:34:56.560 --> 01:35:02.560]   And I also wrote an amicus brief, as I said, in the Warhol case, which I don't, they didn't read that one.
[01:35:02.560 --> 01:35:02.880]   Yeah.
[01:35:02.880 --> 01:35:11.200]   But I imagine you made somewhat of the same 230 arguments in Gondzollis versus Google, yes?
[01:35:11.200 --> 01:35:12.640]   Oh, my typical ones of it.
[01:35:12.640 --> 01:35:13.040]   It's good.
[01:35:13.040 --> 01:35:18.640]   And one of the, the signatories on it was an individual who hosts his own mastodon server,
[01:35:18.640 --> 01:35:23.360]   because if he's going to do that for his friends, where you can see the expressive rights and stuff,
[01:35:23.360 --> 01:35:25.520]   230 works for him.
[01:35:25.520 --> 01:35:30.560]   And I really want to make sure that people understood that 230 is not just for big companies, that
[01:35:30.560 --> 01:35:34.960]   it's the boots on the ground to make sure, especially if you don't like the big companies in that
[01:35:34.960 --> 01:35:39.600]   you have choices that you need 230 to make sure other people out there can create those choices.
[01:35:39.600 --> 01:35:45.360]   It was my point exactly, because as somebody else who runs a mastodon server and an IRC and
[01:35:45.360 --> 01:35:50.160]   a discord chat, you know, it's the little people who could really, I mean, I can't afford to defend
[01:35:50.160 --> 01:35:50.640]   myself.
[01:35:50.640 --> 01:35:52.240]   230 protects us.
[01:35:52.240 --> 01:35:53.200]   Yeah, it does.
[01:35:53.200 --> 01:35:54.560]   Google can afford to defend itself.
[01:35:54.560 --> 01:35:56.240]   It did all the way to the Supreme Court.
[01:35:56.240 --> 01:35:57.840]   Twitter all the way to the Supreme Court.
[01:35:57.840 --> 01:35:59.680]   I'm not quite sure they paid those lawyers at the end.
[01:35:59.680 --> 01:36:00.720]   I'm really curious to know.
[01:36:00.720 --> 01:36:03.120]   But I believe me, I couldn't.
[01:36:03.120 --> 01:36:05.280]   And so that is really important.
[01:36:05.280 --> 01:36:07.760]   Otherwise, we wouldn't have a forum.
[01:36:07.760 --> 01:36:09.120]   We wouldn't have an IRC.
[01:36:09.120 --> 01:36:12.400]   We wouldn't have a mastodon because it would be too risky.
[01:36:12.400 --> 01:36:13.680]   So it's a good thing.
[01:36:14.880 --> 01:36:15.440]   Yep.
[01:36:15.440 --> 01:36:16.080]   That was close.
[01:36:16.080 --> 01:36:17.680]   Oh, it totally was.
[01:36:17.680 --> 01:36:24.400]   I'm having a weird weekend because I'm very happy and very unhappy, but the very happy part is
[01:36:24.400 --> 01:36:26.480]   we can breathe a little bit.
[01:36:26.480 --> 01:36:27.200]   Yeah.
[01:36:27.200 --> 01:36:28.080]   Fair use lost.
[01:36:28.080 --> 01:36:29.840]   Section 231.
[01:36:29.840 --> 01:36:29.920]   Yeah.
[01:36:29.920 --> 01:36:30.720]   That's this.
[01:36:30.720 --> 01:36:32.800]   Just skip the whole last half hour.
[01:36:32.800 --> 01:36:33.920]   That's all you need to know.
[01:36:33.920 --> 01:36:34.160]   Right.
[01:36:34.160 --> 01:36:35.840]   Now you know why.
[01:36:35.840 --> 01:36:37.680]   Fair use lost, the internet won.
[01:36:37.680 --> 01:36:37.840]   Yeah.
[01:36:37.840 --> 01:36:39.680]   To the extent that it doesn't rely on fair use.
[01:36:39.680 --> 01:36:40.720]   Good luck with that.
[01:36:40.720 --> 01:36:41.440]   Oh, God.
[01:36:41.440 --> 01:36:42.880]   Don't, no, Kathy.
[01:36:42.880 --> 01:36:44.640]   [LAUGHTER]
[01:36:44.640 --> 01:36:45.600]   We're going to take a little break.
[01:36:45.600 --> 01:36:49.520]   Well, we have more to talk about our show today brought to you by Duo.
[01:36:49.520 --> 01:36:50.560]   I know.
[01:36:50.560 --> 01:36:51.760]   You must know the name Duo.
[01:36:51.760 --> 01:36:53.840]   I know the name Duo for a long time.
[01:36:53.840 --> 01:36:55.920]   We've used Duo to protect against breaches
[01:36:55.920 --> 01:37:00.240]   with a leading access management suite.
[01:37:00.240 --> 01:37:02.080]   Strong, multi-layer defenses.
[01:37:02.080 --> 01:37:06.320]   Innovative capabilities only allow legitimate users in
[01:37:06.320 --> 01:37:12.480]   to your network or your site or your app and keep bad actors out.
[01:37:13.120 --> 01:37:16.000]   For any organization concerned about being breached
[01:37:16.000 --> 01:37:17.760]   that needs protection fast,
[01:37:17.760 --> 01:37:20.160]   Duo quickly enables strong security
[01:37:20.160 --> 01:37:22.880]   while also improving user productivity.
[01:37:22.880 --> 01:37:24.320]   So many companies use Duo.
[01:37:24.320 --> 01:37:28.880]   For that very reason, it's kind of a no-brainer.
[01:37:28.880 --> 01:37:30.320]   You just turn it on and you're safe.
[01:37:30.320 --> 01:37:34.880]   Duo prevents unauthorized access with multi-layer defenses
[01:37:34.880 --> 01:37:39.200]   and modern capabilities that thwart sophisticated malicious access attempts.
[01:37:39.760 --> 01:37:42.880]   We did a panel with Wendy Nader who was a CISO at Duo.
[01:37:42.880 --> 01:37:47.120]   And I was so impressed by her and the Duo team.
[01:37:47.120 --> 01:37:48.000]   They're so smart.
[01:37:48.000 --> 01:37:50.480]   They know what they're doing.
[01:37:50.480 --> 01:37:55.440]   This is a rock solid solution that increases authentication requirements
[01:37:55.440 --> 01:37:57.600]   in real time as the risk rises.
[01:37:57.600 --> 01:37:58.240]   Isn't that awesome?
[01:37:58.240 --> 01:38:02.400]   So if things are getting problematic,
[01:38:02.400 --> 01:38:05.600]   Duo turns it up to high.
[01:38:05.600 --> 01:38:08.640]   Duo enables high productivity by only requiring authentication.
[01:38:08.640 --> 01:38:13.040]   When needed, say it's just in time authentication,
[01:38:13.040 --> 01:38:16.560]   which means you get swift, easy, and secure access when it's safe
[01:38:16.560 --> 01:38:18.560]   and they step up the requirements when it's not.
[01:38:18.560 --> 01:38:22.400]   Duo provides an all-in-one solution for strong MFA,
[01:38:22.400 --> 01:38:27.360]   passwordless, single sign-on, and trusted endpoint verification.
[01:38:27.360 --> 01:38:29.840]   I think we all know this is very important now.
[01:38:29.840 --> 01:38:33.760]   Duo helps you implement zero trust principles by verifying users
[01:38:33.760 --> 01:38:37.200]   and their devices before they can touch your assets.
[01:38:37.200 --> 01:38:38.720]   Start your free trial.
[01:38:38.720 --> 01:38:40.000]   Sign up today.
[01:38:40.000 --> 01:38:42.000]   Signup.duo.com.
[01:38:42.000 --> 01:38:43.760]   We're really, this is, I think their first ad,
[01:38:43.760 --> 01:38:45.440]   we're really thrilled to have one to it.
[01:38:45.440 --> 01:38:47.040]   I've been a Duo fan for years.
[01:38:47.040 --> 01:38:50.080]   Signup.duo.com.
[01:38:50.080 --> 01:38:51.440]   Duo.duo.
[01:38:51.440 --> 01:38:53.760]   Signup.duo.com.
[01:38:53.760 --> 01:38:56.320]   Thank you, Duo, for all you do.
[01:38:56.320 --> 01:38:57.200]   We really appreciate it.
[01:38:57.200 --> 01:38:59.360]   And thank you,
[01:38:59.360 --> 01:39:01.760]   Twitter listeners, for using that special address
[01:39:01.760 --> 01:39:03.600]   so they know you saw it here, right?
[01:39:03.600 --> 01:39:03.920]   Right.
[01:39:05.200 --> 01:39:08.240]   Google did reach a privacy settlement with Washington State.
[01:39:08.240 --> 01:39:13.440]   This is, you know, to me, this is just the tip of the iceberg.
[01:39:13.440 --> 01:39:19.760]   You may remember Google had settings to turn off their location tracking.
[01:39:19.760 --> 01:39:23.200]   Oh, but look, it didn't do anything.
[01:39:23.200 --> 01:39:24.960]   They still track you.
[01:39:24.960 --> 01:39:30.560]   And the state of Washington sued, saying alphabet,
[01:39:31.840 --> 01:39:36.960]   well, Google's division of alphabet misled customers about its location tracking.
[01:39:36.960 --> 01:39:42.880]   The settlement resolves claims that Google deceived people into believing.
[01:39:42.880 --> 01:39:48.560]   You see, you foolish person, you believed you were controlling how Google
[01:39:48.560 --> 01:39:50.080]   collects and uses your personal data.
[01:39:50.080 --> 01:39:53.360]   They even gave you a switch, but it didn't do anything.
[01:39:53.360 --> 01:39:58.160]   A consent decree file on Wednesday requires Google to be more transparent
[01:39:58.160 --> 01:40:02.960]   about its tracking practices and provide a more detailed location technology's web page,
[01:40:02.960 --> 01:40:11.040]   describing them and finds Google $39.9 million, which is like the interest on one day revenue.
[01:40:11.040 --> 01:40:11.680]   I mean, it's not.
[01:40:11.680 --> 01:40:14.080]   Is this enough?
[01:40:14.080 --> 01:40:15.360]   Is it how important is this?
[01:40:15.360 --> 01:40:17.360]   I wonder why it's only Washington State.
[01:40:17.360 --> 01:40:22.240]   Arizona got an $85 million settlement with Google also.
[01:40:22.240 --> 01:40:24.480]   Feels like this should be something the feds should be.
[01:40:24.480 --> 01:40:28.000]   Get another place where the states are doing stuff that the hard work.
[01:40:28.000 --> 01:40:29.520]   Better down on the federal level.
[01:40:29.520 --> 01:40:33.280]   I guess Lena Kahn's worried about other stuff,
[01:40:33.280 --> 01:40:36.960]   but I'm surprised the FTC didn't weigh in on this one as well.
[01:40:36.960 --> 01:40:38.400]   But that's a good thing.
[01:40:38.400 --> 01:40:39.840]   Yeah, I'm $39.9 million.
[01:40:39.840 --> 01:40:44.160]   It's a cost to doing business for Google.
[01:40:44.160 --> 01:40:48.240]   Interesting article in TechCrunch.
[01:40:48.240 --> 01:40:49.280]   I've heard of them, Amanda.
[01:40:49.280 --> 01:40:53.280]   Your colleague--
[01:40:53.280 --> 01:40:54.560]   Hey, I think I know them, right?
[01:40:54.560 --> 01:40:55.840]   Yeah, I think I know who they are.
[01:40:55.840 --> 01:40:57.280]   It's on your check, I think.
[01:40:57.280 --> 01:40:58.560]   Your colleague, Adam--
[01:40:58.560 --> 01:41:01.280]   Actually, it actually all my checks is Yahoo.
[01:41:01.280 --> 01:41:02.160]   Oh, God.
[01:41:02.160 --> 01:41:04.320]   That's right, I forgot.
[01:41:04.320 --> 01:41:05.600]   Yeah, Yahoo Company.
[01:41:05.600 --> 01:41:08.240]   Adam Kovecovic.
[01:41:08.240 --> 01:41:09.600]   I hope I'm saying his name right.
[01:41:09.600 --> 01:41:10.240]   Kovecovic.
[01:41:10.240 --> 01:41:13.120]   Adam K, we call him.
[01:41:13.120 --> 01:41:16.080]   The government can't seize your data, but it can buy it.
[01:41:16.080 --> 01:41:22.320]   Similar story, the Biden administration proposed new protections earlier this month
[01:41:22.320 --> 01:41:26.480]   to prevent law enforcement from demanding reproductive health care data
[01:41:26.480 --> 01:41:28.640]   from companies like Twitter and Google.
[01:41:28.640 --> 01:41:34.880]   But they don't really need to because data brokers have that information,
[01:41:34.880 --> 01:41:40.000]   and the US government has been buying that data from private data brokers for years.
[01:41:40.000 --> 01:41:48.400]   In fact, we reported, I think, last week about one data broker that won its case.
[01:41:48.400 --> 01:41:51.200]   What was the name of it?
[01:41:51.200 --> 01:41:53.200]   Kovecovic or something like that?
[01:41:55.040 --> 01:42:00.720]   The appeals court held that the government hadn't really shown harm.
[01:42:00.720 --> 01:42:06.400]   Oh, well, there's no harm in selling your location data.
[01:42:06.400 --> 01:42:13.040]   But the court, I guess, implied that, well, if you come back FTC with some people who've been
[01:42:13.040 --> 01:42:14.960]   harmed by this, well, then we can talk.
[01:42:14.960 --> 01:42:17.840]   Certainly a big issue in privacy.
[01:42:17.840 --> 01:42:21.040]   I think we need a law regulating data brokers, frankly.
[01:42:21.040 --> 01:42:23.600]   It's such a shadowy business.
[01:42:23.600 --> 01:42:23.840]   Yeah.
[01:42:24.640 --> 01:42:25.840]   They're out there.
[01:42:25.840 --> 01:42:29.840]   They're collecting that data and then they just sell it to whoever wants it.
[01:42:29.840 --> 01:42:35.520]   That's why a ban against TikTok seems so silly because TikTok isn't the problem.
[01:42:35.520 --> 01:42:39.120]   The Chinese government can just go to these data brokers and get anything they want about
[01:42:39.120 --> 01:42:40.720]   anybody they want.
[01:42:40.720 --> 01:42:45.280]   It's not defining problems that you're throwing legislation at, which really needs to stop,
[01:42:45.280 --> 01:42:48.160]   because it's going to break more things and this isn't going to fix anything.
[01:42:48.160 --> 01:42:49.200]   Thank you.
[01:42:49.200 --> 01:42:55.920]   Good article.
[01:42:55.920 --> 01:43:00.560]   You wrote, I thought, Harry, I don't know if you heard it yet or not about Apple's
[01:43:00.560 --> 01:43:03.200]   announced new voice feature.
[01:43:03.200 --> 01:43:06.320]   This will be in the fall, right with iOS 17.
[01:43:06.320 --> 01:43:07.200]   Later this year.
[01:43:07.200 --> 01:43:07.680]   Later this year.
[01:43:07.680 --> 01:43:07.760]   Later this year.
[01:43:07.760 --> 01:43:10.160]   The new OS updates come out, maybe.
[01:43:10.160 --> 01:43:11.760]   But really cool.
[01:43:12.640 --> 01:43:18.560]   If you have ALS and as it progresses, it gets harder and harder to talk.
[01:43:18.560 --> 01:43:23.520]   People like Stephen Hawking start using machines that talk for them,
[01:43:23.520 --> 01:43:25.920]   Hawking quite famously.
[01:43:25.920 --> 01:43:30.240]   Now Apple is going to allow you to bank your voice.
[01:43:30.240 --> 01:43:36.000]   So if you have early stages ALS, you could bank your voice for the time when you'd have to use
[01:43:36.000 --> 01:43:36.960]   a speech machine.
[01:43:36.960 --> 01:43:37.920]   It would sound like you.
[01:43:37.920 --> 01:43:40.720]   It comes pretty close from the samples I heard.
[01:43:40.720 --> 01:43:41.280]   Have you heard it?
[01:43:41.280 --> 01:43:41.760]   Yes.
[01:43:41.760 --> 01:43:42.720]   I've heard a few samples.
[01:43:42.720 --> 01:43:46.320]   They don't sound 100% like that person might.
[01:43:46.320 --> 01:43:51.360]   They have a little bit of the typical stuff you associate with a synthesized voice,
[01:43:51.360 --> 01:43:55.360]   but they don't sound like Siri or Alexa or the Google Assistant.
[01:43:55.360 --> 01:44:00.480]   They sound like a specific human being, which is to a lot of these people who might face this
[01:44:00.480 --> 01:44:01.360]   really important.
[01:44:01.360 --> 01:44:04.560]   They would much rather express themselves with their own voice.
[01:44:04.560 --> 01:44:06.800]   And that was the goal here.
[01:44:06.800 --> 01:44:13.120]   And there have been ways to do this for a long time, but historically it could involve a lot
[01:44:13.120 --> 01:44:16.880]   of training, which if you have ALS, it becomes even more of a challenge.
[01:44:16.880 --> 01:44:19.440]   It used to be quite expensive.
[01:44:19.440 --> 01:44:20.320]   It's gotten cheaper.
[01:44:20.320 --> 01:44:24.800]   But what Apple is doing is the next step in terms of making it completely accessible,
[01:44:24.800 --> 01:44:28.720]   because it will be available on this device, which a lot of people have.
[01:44:28.720 --> 01:44:34.240]   People will be able to do the voice banking on an iPhone, an iPad or a Mac.
[01:44:34.240 --> 01:44:36.880]   I presume Apple is doing this internally.
[01:44:36.880 --> 01:44:38.640]   There are companies that do this already.
[01:44:38.640 --> 01:44:45.920]   I played with something from a company called Acapella, and I had to read 215 phrases.
[01:44:45.920 --> 01:44:47.680]   It took me about half an hour.
[01:44:47.680 --> 01:44:49.440]   And then it made my voice.
[01:44:49.440 --> 01:44:55.680]   I don't have ALS, but if I ever need it, but it's kind of cool anyway.
[01:44:55.680 --> 01:44:58.880]   This is an example of my voice.
[01:44:58.880 --> 01:45:01.280]   Let's see, make sure you have my sound, right?
[01:45:01.280 --> 01:45:02.880]   This is my voice.
[01:45:03.600 --> 01:45:06.960]   Oh, it's not coming out of the doohickey.
[01:45:06.960 --> 01:45:07.600]   Only play it again.
[01:45:07.600 --> 01:45:09.520]   Harry, can you get me a can of soda?
[01:45:09.520 --> 01:45:11.200]   This kind of sounds like me.
[01:45:11.200 --> 01:45:14.640]   From what I've heard of Apple's...
[01:45:14.640 --> 01:45:15.520]   It's better?
[01:45:15.520 --> 01:45:18.160]   If you did this with Apple's version, it would sound quite a bit more like you.
[01:45:18.160 --> 01:45:22.560]   And Apple's takes 15 minutes of training, by the way.
[01:45:22.560 --> 01:45:23.600]   A lot faster.
[01:45:23.600 --> 01:45:25.840]   I am not surprised.
[01:45:25.840 --> 01:45:31.280]   So you need to package that up so you can be the voice that people can put on their car directions.
[01:45:31.280 --> 01:45:33.600]   Yeah, actually I did that back in the day.
[01:45:33.600 --> 01:45:35.120]   Was it TomTom?
[01:45:35.120 --> 01:45:38.640]   I think TomTom allowed you to record your voice.
[01:45:38.640 --> 01:45:41.040]   And now actually I have a ways voice that's me.
[01:45:41.040 --> 01:45:43.200]   In fact, I hadn't used ways in a while.
[01:45:43.200 --> 01:45:46.560]   I think this needs to be a Twit TV subscription.
[01:45:46.560 --> 01:45:48.880]   I could put it in the club Twit.
[01:45:48.880 --> 01:45:51.520]   I hadn't used it ways in a while.
[01:45:51.520 --> 01:45:53.360]   I recorded this many five or six years ago.
[01:45:53.360 --> 01:45:57.120]   And we were worried about traffic, so I turned on ways.
[01:45:57.120 --> 01:45:58.800]   And it said, "Turn left at the next..."
[01:45:58.800 --> 01:45:59.840]   And it was me.
[01:45:59.840 --> 01:46:02.080]   And I said, "That sounds like me Lisa."
[01:46:02.080 --> 01:46:03.200]   And she said, "Who is you?"
[01:46:03.200 --> 01:46:03.840]   And that went...
[01:46:03.840 --> 01:46:08.640]   Speaking of that sort of thing, I've heard some people who have been concerned that
[01:46:08.640 --> 01:46:13.040]   Apple's technology could be used for nefarious purposes.
[01:46:13.040 --> 01:46:13.040]   Could it?
[01:46:13.040 --> 01:46:14.560]   Such as stealing somebody else's voice.
[01:46:14.560 --> 01:46:14.880]   Yes.
[01:46:14.880 --> 01:46:16.960]   I think they have done a lot of thinking about that.
[01:46:16.960 --> 01:46:20.880]   And there are a bunch of existing services, which even though Apple's is not available yet now,
[01:46:20.880 --> 01:46:25.440]   people are already using for questionable purposes.
[01:46:25.440 --> 01:46:31.600]   With Apple's... The person who wants to do this has to do the training using these random...
[01:46:31.600 --> 01:46:33.040]   Is it on the phone?
[01:46:33.040 --> 01:46:34.080]   Does it stay on the phone?
[01:46:34.080 --> 01:46:37.040]   In fact, you can just leave it on the phone if you want.
[01:46:37.040 --> 01:46:38.720]   And it never gets uploaded to the cloud at all.
[01:46:38.720 --> 01:46:42.240]   Probably the single most interesting thing Apple has done technologically is
[01:46:42.240 --> 01:46:49.120]   they've crunched us down into a form where the crunching of your voice can be done on the device.
[01:46:49.120 --> 01:46:53.360]   You can upload it to iCloud, which you probably would want to do,
[01:46:53.360 --> 01:46:57.120]   because when you're doing this, it would be for later use,
[01:46:57.120 --> 01:46:59.120]   possibly using devices you don't own yet.
[01:46:59.120 --> 01:47:01.840]   But even on iCloud, it's encrypted.
[01:47:01.840 --> 01:47:05.840]   I think it does seem like it would be tough for somebody to
[01:47:05.840 --> 01:47:09.280]   essentially steal your voice using this technology.
[01:47:09.280 --> 01:47:14.000]   It really does seem like it's under the control of the person who is banking their voice.
[01:47:14.000 --> 01:47:17.920]   Using Apple's stuff, because you have to read specific phrases, they're random.
[01:47:17.920 --> 01:47:21.600]   So you couldn't take all the recordings of me, for instance, and do this.
[01:47:21.600 --> 01:47:26.560]   You couldn't do that. It seems like it would be extremely difficult to impossible to
[01:47:26.560 --> 01:47:34.240]   record Leo's voice and then train it and create a Leo voice without Leo being aware you were doing
[01:47:34.240 --> 01:47:38.560]   that, which is certainly a reasonable thing to be concerned about with some of these technologies.
[01:47:38.560 --> 01:47:44.400]   There are AI companies, 11 labs, is one that will do this from arbitrary samples.
[01:47:44.400 --> 01:47:48.880]   But in order for us to do this, Anthony had to come in and have me read his statement,
[01:47:48.880 --> 01:47:53.120]   and said, "I am Leo Laporte and I give permission for this to be done."
[01:47:53.120 --> 01:47:57.120]   But now he's got my voice and he's doing all sorts of stuff with our promos.
[01:47:57.120 --> 01:47:59.200]   I synthesized voice.
[01:47:59.200 --> 01:48:00.880]   That was D-Scripts.
[01:48:00.880 --> 01:48:03.040]   Oh, that wasn't... That was who?
[01:48:03.040 --> 01:48:04.800]   D-Scripts.
[01:48:04.800 --> 01:48:06.400]   Does 11 AI ask?
[01:48:06.400 --> 01:48:08.560]   No, 11 AI says, "Just give me some samples."
[01:48:08.560 --> 01:48:12.880]   Yeah, I mean they say, "You assume responsibility for like..."
[01:48:12.880 --> 01:48:15.280]   It's all on you if something goes wrong.
[01:48:15.280 --> 01:48:18.320]   But any bad actor could then make my voice, right?
[01:48:18.320 --> 01:48:18.640]   Yeah.
[01:48:18.640 --> 01:48:21.920]   See, I worry. I guess I shouldn't.
[01:48:21.920 --> 01:48:29.520]   I've told though, as a result, all my family and friends, we need a secret word.
[01:48:29.520 --> 01:48:34.240]   When if I call you saying, "I've been kidnapped, please send a million dollars ransom to this address,"
[01:48:34.240 --> 01:48:35.680]   I need to have the secret word.
[01:48:35.680 --> 01:48:38.480]   That's stuff like that is already happening.
[01:48:38.480 --> 01:48:39.600]   Yeah.
[01:48:39.600 --> 01:48:41.120]   Based on technologies.
[01:48:41.120 --> 01:48:46.880]   Yeah, Larry Magad got almost got suckered by somebody pretending to be his wife.
[01:48:47.520 --> 01:48:51.040]   He wrote the story on his website.
[01:48:51.040 --> 01:48:54.800]   Yeah, I feel very nervous.
[01:48:54.800 --> 01:48:57.840]   And anybody who's in the public domain where there...
[01:48:57.840 --> 01:49:01.760]   Or for some reason or other, even you guys, just because you're on this show,
[01:49:01.760 --> 01:49:03.920]   there's enough samples of your voices now.
[01:49:03.920 --> 01:49:04.320]   Sorry.
[01:49:04.320 --> 01:49:05.840]   Tell your loved ones.
[01:49:05.840 --> 01:49:06.320]   Yeah.
[01:49:06.320 --> 01:49:09.040]   I should have a lot more fame and fortune as a consequence of this,
[01:49:09.040 --> 01:49:10.240]   for the amount of risk it's about.
[01:49:10.240 --> 01:49:11.280]   I agree.
[01:49:11.280 --> 01:49:14.080]   We're gonna have to start paying people a lot more to come on the show.
[01:49:15.200 --> 01:49:17.760]   Yeah. You know, the problem with journalism is...
[01:49:17.760 --> 01:49:19.760]   You're in the public eye, but you make no money.
[01:49:19.760 --> 01:49:19.920]   Yeah.
[01:49:19.920 --> 01:49:20.400]   Yeah.
[01:49:20.400 --> 01:49:20.640]   Yeah.
[01:49:20.640 --> 01:49:23.680]   I've said that for a long time.
[01:49:23.680 --> 01:49:24.320]   Yeah.
[01:49:24.320 --> 01:49:24.960]   It's not good.
[01:49:24.960 --> 01:49:33.040]   So CNET is unionizing, by the way, because of
[01:49:33.040 --> 01:49:37.600]   Red Ventures using AI to write their articles.
[01:49:37.600 --> 01:49:41.040]   Is this been an issue at TechCrunch at all, Amanda?
[01:49:42.320 --> 01:49:49.200]   No. We're not using generative AI in our articles, but yeah,
[01:49:49.200 --> 01:49:51.600]   I think this was like a really interesting story.
[01:49:51.600 --> 01:49:56.880]   I actually think that CNET was starting to unionize before all the AI stuff happened,
[01:49:56.880 --> 01:50:02.480]   but I think there's a really clear connection between this and
[01:50:02.480 --> 01:50:09.440]   and then like the WGA writer's strike in Hollywood, where
[01:50:11.200 --> 01:50:16.960]   in that strike, basically, AI wasn't really at the forefront,
[01:50:16.960 --> 01:50:19.680]   but they just kind of threw in something being like,
[01:50:19.680 --> 01:50:24.320]   hey, by the way, we don't want you to like replace us with AI
[01:50:24.320 --> 01:50:29.280]   or ask us to edit scripts that were written with AI and whatnot,
[01:50:29.280 --> 01:50:33.120]   which is also a legal issue, which I want to get into that.
[01:50:33.120 --> 01:50:33.200]   That was completely true.
[01:50:33.200 --> 01:50:34.960]   There was concerns, for instance, that...
[01:50:34.960 --> 01:50:35.760]   Well, how many years?
[01:50:35.760 --> 01:50:40.080]   Just 30 years of Simpson's episodes that if you fed all of those scripts
[01:50:40.080 --> 01:50:42.880]   into an AI, could it write a credible Simpson's episode?
[01:50:42.880 --> 01:50:45.520]   Writers were very concerned about that.
[01:50:45.520 --> 01:50:50.480]   I think it can write something that looks like a Simpson's episode,
[01:50:50.480 --> 01:50:52.480]   but I don't think it's going to be good.
[01:50:52.480 --> 01:50:54.400]   Yeah, we saw the Seinfeld that was horrible.
[01:50:54.400 --> 01:50:55.280]   I wrote about the...
[01:50:55.280 --> 01:50:58.320]   Nothing forever.
[01:50:58.320 --> 01:51:00.560]   That was what it was called.
[01:51:00.560 --> 01:51:01.440]   Nothing forever.
[01:51:01.440 --> 01:51:05.120]   Yeah, I mean, it was literally just like Jerry Seinfeld being like...
[01:51:05.120 --> 01:51:05.600]   It's random.
[01:51:05.600 --> 01:51:08.320]   There's a new restaurant and then a lane being like,
[01:51:09.040 --> 01:51:10.880]   "What kind of restaurant is it?"
[01:51:10.880 --> 01:51:15.280]   Which it is the show about nothing, but this was really the show about nothing.
[01:51:15.280 --> 01:51:19.200]   For those who don't know, we're talking about there was a Twitch phenomenon
[01:51:19.200 --> 01:51:25.840]   where it was a generative AI, Seinfeld animated copy that just went on forever.
[01:51:25.840 --> 01:51:28.080]   Because shut down because it eventually...
[01:51:28.080 --> 01:51:30.640]   Because all AI does became racist, right?
[01:51:30.640 --> 01:51:33.440]   That's what always happens.
[01:51:33.440 --> 01:51:34.960]   Twitch ended up taking it down.
[01:51:36.160 --> 01:51:38.960]   What happened with them was...
[01:51:38.960 --> 01:51:46.080]   I think they were using a version of OpenAI.
[01:51:46.080 --> 01:51:55.520]   Yeah, they were using a version of the OpenAI software that had gates to prevent it from saying
[01:51:55.520 --> 01:51:57.600]   things that could be offensive.
[01:51:58.160 --> 01:52:06.880]   But then I think that broke and their fail safe was to use a non-gated version of that.
[01:52:06.880 --> 01:52:14.240]   And then Jerry and his stand-up set just went and had to say a transphobic thing.
[01:52:14.240 --> 01:52:15.360]   This is nothing forever.
[01:52:15.360 --> 01:52:16.480]   I've started the ordinary.
[01:52:16.480 --> 01:52:17.280]   And infinitely generating it.
[01:52:17.280 --> 01:52:18.800]   Sorry, is somebody else talking over it?
[01:52:18.800 --> 01:52:22.880]   I wanted to play a little bit of it, but Motherboard's got a narration on top of it.
[01:52:22.880 --> 01:52:24.000]   Yeah, we can't play it anymore.
[01:52:24.000 --> 01:52:24.560]   They're gone.
[01:52:24.560 --> 01:52:25.040]   Yeah.
[01:52:25.040 --> 01:52:25.520]   Wisely.
[01:52:25.520 --> 01:52:26.560]   Wisely, so...
[01:52:27.200 --> 01:52:29.600]   I think they're actually back on Twitch now.
[01:52:29.600 --> 01:52:30.320]   Oh, are they?
[01:52:30.320 --> 01:52:31.120]   Oh boy.
[01:52:31.120 --> 01:52:33.200]   Yeah, because they were only banned for two weeks.
[01:52:33.200 --> 01:52:38.560]   But then I thought they got the good comments and sense that this isn't worth fighting over.
[01:52:38.560 --> 01:52:38.880]   I don't know.
[01:52:38.880 --> 01:52:43.360]   I was subscribed to them when I was writing about them,
[01:52:43.360 --> 01:52:46.080]   but I haven't looked at it in a while.
[01:52:46.080 --> 01:52:47.520]   But I'm still getting the emails.
[01:52:47.520 --> 01:52:50.960]   They're live on Twitch and then I just haven't checked in.
[01:52:50.960 --> 01:52:52.960]   But maybe I need to see what they're up to.
[01:52:52.960 --> 01:52:53.760]   Yeah, let's see.
[01:52:53.760 --> 01:52:55.120]   Do they have any shows scheduled?
[01:52:55.120 --> 01:52:55.920]   No, no shows.
[01:52:56.640 --> 01:52:57.680]   It's not the right channel.
[01:52:57.680 --> 01:52:58.160]   Oh, okay.
[01:52:58.160 --> 01:52:59.520]   Well...
[01:52:59.520 --> 01:53:01.920]   It's like, "Watch me forever, is there you are?"
[01:53:01.920 --> 01:53:02.880]   Watch me forever.
[01:53:02.880 --> 01:53:03.920]   Not nothing forever.
[01:53:03.920 --> 01:53:04.640]   Okay.
[01:53:04.640 --> 01:53:05.120]   They're called...
[01:53:05.120 --> 01:53:07.360]   Well, nothing forever is the name of the show,
[01:53:07.360 --> 01:53:09.120]   but watch me forever is the channel.
[01:53:09.120 --> 01:53:10.000]   It's very confusing.
[01:53:10.000 --> 01:53:10.960]   Inconsistent branding.
[01:53:10.960 --> 01:53:13.120]   That's a problem.
[01:53:13.120 --> 01:53:14.560]   Oh yeah, I've been to watch me forever,
[01:53:14.560 --> 01:53:15.200]   because it immediately...
[01:53:15.200 --> 01:53:16.880]   Oh, it looks like it's still doing it.
[01:53:16.880 --> 01:53:18.240]   There's an ad on it right now.
[01:53:18.240 --> 01:53:18.640]   Yeah.
[01:53:18.640 --> 01:53:22.160]   Well, very interesting.
[01:53:22.160 --> 01:53:24.240]   I am a contrarian on AI, by the way.
[01:53:24.240 --> 01:53:27.040]   I don't know. I'll let you guys yell at me for this.
[01:53:27.040 --> 01:53:29.120]   But I say...
[01:53:29.120 --> 01:53:30.560]   My thought is...
[01:53:30.560 --> 01:53:34.560]   Computers are not intelligent.
[01:53:34.560 --> 01:53:36.480]   They do not think they never have,
[01:53:36.480 --> 01:53:37.600]   and they never will.
[01:53:37.600 --> 01:53:41.200]   They're just a box of rocks that calculate so fast.
[01:53:41.200 --> 01:53:43.840]   It appears to be thinking.
[01:53:43.840 --> 01:53:48.960]   We don't have to worry about AI becoming dangerous,
[01:53:48.960 --> 01:53:50.480]   because it's not intelligent.
[01:53:50.480 --> 01:53:53.760]   It's just stupid words that it's stringing together.
[01:53:53.760 --> 01:53:55.760]   It doesn't even know what it's saying,
[01:53:55.760 --> 01:53:56.800]   and it is harmless.
[01:53:56.800 --> 01:54:00.320]   And people, especially the media,
[01:54:00.320 --> 01:54:03.920]   are playing it up as if it's going to be the next...
[01:54:03.920 --> 01:54:05.600]   Oh god, it's a Terminator.
[01:54:05.600 --> 01:54:07.280]   That's Skynet.
[01:54:07.280 --> 01:54:07.840]   And it's not.
[01:54:07.840 --> 01:54:09.840]   It's never going to be, because it doesn't think.
[01:54:09.840 --> 01:54:10.720]   I agree.
[01:54:10.720 --> 01:54:13.040]   The problem is that people deploying it
[01:54:13.040 --> 01:54:14.320]   are also kind of stupid,
[01:54:14.320 --> 01:54:16.320]   and they're going to deploy it in some really dumb ways,
[01:54:16.320 --> 01:54:18.400]   because they think it does more than it actually does.
[01:54:18.400 --> 01:54:20.800]   Well, that is a risk, because it's often wrong.
[01:54:20.800 --> 01:54:21.360]   Yeah.
[01:54:21.360 --> 01:54:21.680]   Right.
[01:54:21.680 --> 01:54:22.480]   Well, I think that's...
[01:54:22.480 --> 01:54:22.960]   Yeah.
[01:54:22.960 --> 01:54:24.400]   I think you're absolutely right on what it is,
[01:54:24.400 --> 01:54:25.760]   and what its limitations are.
[01:54:25.760 --> 01:54:29.040]   It doesn't mean that there's not some externalities
[01:54:29.040 --> 01:54:30.400]   we shouldn't be alarmed at,
[01:54:30.400 --> 01:54:33.600]   but I think, again, it's like there's the policy panic
[01:54:33.600 --> 01:54:36.320]   of people are getting alarmed at the wrong things,
[01:54:36.320 --> 01:54:37.440]   because they don't understand it.
[01:54:37.440 --> 01:54:39.760]   They're gunning after it is, if it's this magic,
[01:54:39.760 --> 01:54:41.520]   that needs to be boxed up.
[01:54:41.520 --> 01:54:42.880]   But that's not the problem.
[01:54:42.880 --> 01:54:44.400]   I'll go another step,
[01:54:44.400 --> 01:54:48.160]   because of course Sam Altman, the CEO of OpenAI,
[01:54:48.160 --> 01:54:49.680]   testified for the Congress,
[01:54:49.680 --> 01:54:51.600]   and said, "Yes, you should regulate us."
[01:54:51.600 --> 01:54:54.560]   And it's my opinion that all of these people
[01:54:54.560 --> 01:54:56.400]   who are building up the danger,
[01:54:56.400 --> 01:54:58.240]   including Sam Altman of AI,
[01:54:58.240 --> 01:55:00.160]   yes, we need to be regulated,
[01:55:00.160 --> 01:55:03.760]   this is them pretending that what they're doing
[01:55:03.760 --> 01:55:06.240]   is somehow intelligent,
[01:55:06.240 --> 01:55:07.760]   and oh, we better watch out,
[01:55:07.760 --> 01:55:08.960]   it's self-serving.
[01:55:08.960 --> 01:55:12.080]   They want you to think they're doing something
[01:55:12.080 --> 01:55:13.040]   more than they're doing.
[01:55:13.040 --> 01:55:14.480]   It's just fancy software.
[01:55:14.480 --> 01:55:15.840]   Altman knows very well.
[01:55:15.840 --> 01:55:16.000]   Yeah.
[01:55:16.000 --> 01:55:17.040]   It's not dangerous.
[01:55:17.040 --> 01:55:18.160]   It's not going to do anything,
[01:55:18.160 --> 01:55:19.920]   and it doesn't need to be regulated.
[01:55:19.920 --> 01:55:22.080]   Any more than a lines are going to be regulated.
[01:55:22.080 --> 01:55:26.000]   To bring it back to the CNET staff
[01:55:26.000 --> 01:55:27.120]   trying to unionize,
[01:55:27.120 --> 01:55:29.840]   I think that's an example of Kathy's point
[01:55:29.840 --> 01:55:32.240]   about how it's not necessarily the technology.
[01:55:32.240 --> 01:55:34.400]   It's the people using it,
[01:55:34.400 --> 01:55:38.000]   that are using it in questionable ways.
[01:55:38.000 --> 01:55:41.440]   But CNET was trying to publish
[01:55:41.440 --> 01:55:43.760]   news articles written with chat GPT,
[01:55:43.760 --> 01:55:46.080]   and then of course there were a bunch of accuracies.
[01:55:46.080 --> 01:55:47.040]   They were crap.
[01:55:47.040 --> 01:55:48.000]   They got that.
[01:55:48.000 --> 01:55:48.960]   They were like that, right?
[01:55:48.960 --> 01:55:50.160]   Yeah, they were bad.
[01:55:50.160 --> 01:55:51.120]   They weren't true.
[01:55:51.120 --> 01:55:55.600]   But the fact that CNET even was like,
[01:55:55.600 --> 01:55:56.640]   this is a good idea,
[01:55:56.640 --> 01:55:57.520]   we're going to do this.
[01:55:57.520 --> 01:56:00.720]   It's like, people are using this stuff in ways
[01:56:00.720 --> 01:56:02.000]   that are so like,
[01:56:02.000 --> 01:56:05.360]   I'm more afraid of the people that are using AI.
[01:56:05.360 --> 01:56:06.160]   Well, it's always the people.
[01:56:06.160 --> 01:56:06.880]   Yeah.
[01:56:06.880 --> 01:56:09.120]   And then, I mean, the same thing is also
[01:56:09.120 --> 01:56:12.640]   happening with the WGA strike,
[01:56:12.640 --> 01:56:14.800]   where like, yes,
[01:56:14.800 --> 01:56:17.840]   you could feasibly put in 30 years of Simpsons episodes
[01:56:17.840 --> 01:56:19.280]   and make a Simpsons episode.
[01:56:19.280 --> 01:56:22.000]   But it's just going to be like,
[01:56:22.000 --> 01:56:24.320]   Lisa, Claire, and Ed.
[01:56:24.320 --> 01:56:27.280]   It's going to be like that sign felt.
[01:56:27.280 --> 01:56:28.240]   It's not going to be good.
[01:56:28.240 --> 01:56:28.240]   Yeah.
[01:56:28.240 --> 01:56:31.120]   So are the writers--
[01:56:31.120 --> 01:56:32.560]   But that doesn't mean that,
[01:56:32.560 --> 01:56:35.280]   like, but yet still,
[01:56:35.280 --> 01:56:40.080]   that was in the contract that the screenwriters were trying
[01:56:40.080 --> 01:56:42.400]   to protect themselves with.
[01:56:42.400 --> 01:56:44.240]   And then the studios were like,
[01:56:44.240 --> 01:56:47.440]   no, we're not going to commit to not using generative AI.
[01:56:47.440 --> 01:56:48.480]   Well, and to make your point,
[01:56:48.480 --> 01:56:50.720]   CNET fired a bunch of writers,
[01:56:50.720 --> 01:56:53.280]   because presumably, Red Ventures thought,
[01:56:53.280 --> 01:56:55.120]   why are we paying humans to write this crap?
[01:56:55.120 --> 01:56:56.160]   I think Buzzfeed did too.
[01:56:56.160 --> 01:56:58.160]   Yeah. Buzzfeed news went out of business.
[01:56:58.160 --> 01:56:59.360]   Oh my god, yeah.
[01:56:59.360 --> 01:57:01.840]   And Vice has declared bankruptcy.
[01:57:01.840 --> 01:57:05.520]   So I understand they might want to use AI,
[01:57:05.520 --> 01:57:07.760]   but this isn't a path to success.
[01:57:07.760 --> 01:57:10.160]   You need people like Gary to write this stuff.
[01:57:10.160 --> 01:57:11.200]   On the other hand, Leo,
[01:57:11.200 --> 01:57:13.120]   I'm not going to tell you you're wrong about this,
[01:57:13.120 --> 01:57:16.320]   but I sometimes lately wondered if I'm just a box of rocks.
[01:57:16.320 --> 01:57:17.840]   That's good at just looking to get the words.
[01:57:17.840 --> 01:57:20.080]   And maybe that's true of all of us.
[01:57:20.080 --> 01:57:21.520]   That's an interesting question.
[01:57:21.520 --> 01:57:23.520]   Certainly, this technology doesn't get at least
[01:57:23.520 --> 01:57:25.760]   somewhat better in the years to come.
[01:57:25.760 --> 01:57:28.080]   It continues at the same pace it has
[01:57:28.080 --> 01:57:29.120]   over the last six months.
[01:57:29.120 --> 01:57:32.400]   It may get much better, much more quickly.
[01:57:32.400 --> 01:57:33.360]   And as with--
[01:57:33.360 --> 01:57:35.680]   I mean, there--
[01:57:35.680 --> 01:57:36.640]   Here's my thing.
[01:57:36.640 --> 01:57:37.200]   I have a--
[01:57:37.200 --> 01:57:38.720]   Maybe this is what I'm really saying.
[01:57:38.720 --> 01:57:41.280]   I have a belief that no matter how fast it gets,
[01:57:41.280 --> 01:57:42.400]   how much better it gets,
[01:57:42.400 --> 01:57:43.920]   it will never equal you.
[01:57:43.920 --> 01:57:46.960]   Because I think there is something different about you
[01:57:46.960 --> 01:57:49.600]   than a box of rocks doing very fast calculations.
[01:57:49.600 --> 01:57:51.600]   I mean, the best journalism, that's certainly true.
[01:57:51.600 --> 01:57:52.720]   I can do things like--
[01:57:52.720 --> 01:57:53.520]   Well, look at it.
[01:57:53.520 --> 01:57:55.040]   I can look at self-driving cars.
[01:57:55.040 --> 01:57:56.240]   They got better and better and better,
[01:57:56.240 --> 01:57:57.680]   but they still run over kids.
[01:57:57.680 --> 01:58:00.480]   And they will never not run over kids.
[01:58:00.480 --> 01:58:01.920]   But so do humans, unfortunately.
[01:58:01.920 --> 01:58:02.720]   Yeah.
[01:58:02.720 --> 01:58:05.280]   I can score interviews with interesting people.
[01:58:05.280 --> 01:58:08.480]   I think humans still have interesting take on--
[01:58:08.480 --> 01:58:11.920]   take some things that a computer can't replicate.
[01:58:13.360 --> 01:58:16.880]   But I think that it just raises the bar for journalists.
[01:58:16.880 --> 01:58:21.200]   There's an enormous amount of content out there.
[01:58:21.200 --> 01:58:21.920]   That's crap.
[01:58:21.920 --> 01:58:23.520]   Drowned by humans, it's terrible.
[01:58:23.520 --> 01:58:23.840]   Yeah.
[01:58:23.840 --> 01:58:25.200]   And that stuff is going to be in trouble.
[01:58:25.200 --> 01:58:26.080]   That's what's in trouble.
[01:58:26.080 --> 01:58:27.440]   I think the same thing about artists.
[01:58:27.440 --> 01:58:28.320]   The very best--
[01:58:28.320 --> 01:58:28.560]   Yeah.
[01:58:28.560 --> 01:58:30.240]   The very best stuff is generated by humans.
[01:58:30.240 --> 01:58:31.920]   I think there's still going to be a market for it.
[01:58:31.920 --> 01:58:33.680]   But it scares me a little bit,
[01:58:33.680 --> 01:58:37.040]   because if all you're doing is putting up a post about what time
[01:58:37.040 --> 01:58:38.560]   the Super Bowl is on,
[01:58:38.560 --> 01:58:40.000]   a computer can do that.
[01:58:40.000 --> 01:58:40.800]   We should be doing that.
[01:58:40.800 --> 01:58:41.280]   And you took in 15 seconds.
[01:58:41.280 --> 01:58:42.400]   And humans should not be doing that.
[01:58:42.400 --> 01:58:44.080]   At least as well as a human can or better.
[01:58:44.080 --> 01:58:47.120]   So maybe in that way it's encouraging that the people
[01:58:47.120 --> 01:58:49.200]   are actually doing good stuff of a future.
[01:58:49.200 --> 01:58:50.960]   And that the people who are completely cynical
[01:58:50.960 --> 01:58:53.200]   and just trying to monetize stuff through clicks
[01:58:53.200 --> 01:58:54.080]   might be in trouble.
[01:58:54.080 --> 01:58:56.240]   I saw a skeet, I think the other day,
[01:58:56.240 --> 01:58:57.760]   who said something like,
[01:58:57.760 --> 01:59:01.520]   "I didn't think that the robots would be doing all the art
[01:59:01.520 --> 01:59:04.640]   and the humans doing the low wage terrible work instead."
[01:59:04.640 --> 01:59:07.040]   Or something like, "This was not the future I was hoping for."
[01:59:07.040 --> 01:59:08.640]   But I think that's upside down.
[01:59:08.640 --> 01:59:11.040]   I think that robots will be doing all the low wage stupid.
[01:59:11.840 --> 01:59:13.440]   It makes more sense ultimately,
[01:59:13.440 --> 01:59:16.000]   because I think your point is hopefully correct,
[01:59:16.000 --> 01:59:19.600]   which is that humans bring a certain Genesee Qua
[01:59:19.600 --> 01:59:23.360]   that really fancy good software is just not going to be able to replicate.
[01:59:23.360 --> 01:59:27.920]   And at some point the human beings will come to appreciate that appropriately,
[01:59:27.920 --> 01:59:29.920]   designate the grunt work to the robots
[01:59:29.920 --> 01:59:32.640]   and keep the cool human stuff for the humans.
[01:59:32.640 --> 01:59:33.920]   I actually found a piece of grunt work,
[01:59:33.920 --> 01:59:37.840]   which I'm going to happily turn over to chat GPT,
[01:59:37.840 --> 01:59:40.800]   which is when we do a post on Fest Company,
[01:59:40.800 --> 01:59:45.120]   there's this SEO field where you have to write a short summary.
[01:59:45.120 --> 01:59:46.080]   - Should be great at that.
[01:59:46.080 --> 01:59:50.320]   - And actually the goal of the short summary is to be as boring as possible,
[01:59:50.320 --> 01:59:53.040]   but to be stuffed with the relevant key work.
[01:59:53.040 --> 01:59:54.320]   - That's a machine talking to machines.
[01:59:54.320 --> 01:59:56.400]   - It's actually P.T. can do that super quickly.
[01:59:56.400 --> 01:59:58.880]   If I don't have to, it can do it better than I can.
[01:59:58.880 --> 02:00:03.120]   And if I'm not doing it, I can spend more time actually doing stuff that human beings will see,
[02:00:03.120 --> 02:00:04.560]   and hopefully appreciate it.
[02:00:04.560 --> 02:00:05.680]   - Machine, it makes sense.
[02:00:05.680 --> 02:00:08.320]   Machines should do the writing aimed at machines.
[02:00:08.320 --> 02:00:10.320]   And in fact, probably at some point that should just be like,
[02:00:10.320 --> 02:00:15.680]   not even a field, but something which our publishing system does automatically.
[02:00:15.680 --> 02:00:16.480]   - We're looking at that.
[02:00:16.480 --> 02:00:17.600]   We want to use it.
[02:00:17.600 --> 02:00:18.160]   - Machines on the way down.
[02:00:18.160 --> 02:00:22.640]   - We want to use AI to do transcripts, do summaries,
[02:00:22.640 --> 02:00:24.240]   do maybe trap-
[02:00:24.240 --> 02:00:25.040]   - That's a pre-court analysis.
[02:00:25.040 --> 02:00:27.440]   That's a pre-court analysis.
[02:00:27.440 --> 02:00:28.880]   - What's that mean?
[02:00:28.880 --> 02:00:32.880]   - It'll be like Leo and three bots at some point during the show.
[02:00:32.880 --> 02:00:34.080]   - Well, that wouldn't be so bad.
[02:00:34.080 --> 02:00:34.960]   Oh, never mind.
[02:00:34.960 --> 02:00:36.320]   Sorry I said that, Benito.
[02:00:36.320 --> 02:00:39.840]   - But you can still with transcripts,
[02:00:39.840 --> 02:00:46.560]   like I often am using autorei, this is not sponsored to like record interviews.
[02:00:46.560 --> 02:00:52.160]   And it's really, really helpful and it is a lot faster than if I were hand transcribing,
[02:00:52.160 --> 02:00:53.840]   but it's still not perfect.
[02:00:53.840 --> 02:00:54.800]   - Right.
[02:00:54.800 --> 02:01:00.400]   My contention is that you can get 90% of the way it's that last 10% that is impossible.
[02:01:01.840 --> 02:01:07.440]   We've seen this again and again in technology and that's what humans can do.
[02:01:07.440 --> 02:01:08.960]   And I don't, I'm not mystical.
[02:01:08.960 --> 02:01:11.920]   I don't think that, you know, oh, they don't let ever have a soul.
[02:01:11.920 --> 02:01:18.640]   - I think people are alarmed that the acceleration of the technology is happening
[02:01:18.640 --> 02:01:20.240]   quickly in terms of improvements.
[02:01:20.240 --> 02:01:24.640]   So people are sort of panicking at the upshot, both the potential and the upshots.
[02:01:24.640 --> 02:01:25.120]   - Yes.
[02:01:25.120 --> 02:01:30.800]   - And that panic is we have more time to sort of figure out how to deal with this,
[02:01:30.800 --> 02:01:34.560]   but we don't necessarily have as much time as we thought like five years ago.
[02:01:34.560 --> 02:01:38.400]   And given that there's a number of people who are like,
[02:01:38.400 --> 02:01:41.920]   this cool thing I'm going to deploy in a really sort of socially destructive way
[02:01:41.920 --> 02:01:44.560]   is not helping dial down the panic.
[02:01:44.560 --> 02:01:46.320]   - I think it's a mistake though to extrapolate.
[02:01:46.320 --> 02:01:47.840]   Okay, look how fast it's growing.
[02:01:47.840 --> 02:01:52.240]   Just guess where it doesn't mean it's going to suddenly be Einstein.
[02:01:52.240 --> 02:01:54.880]   - No, but I think we're having a visceral emotional reaction to it
[02:01:54.880 --> 02:01:59.040]   as opposed to a clear headed cogent assessment of risk and balance and rewards and stuff like that,
[02:01:59.040 --> 02:02:00.640]   which welcome to tech policy.
[02:02:00.640 --> 02:02:01.440]   That's the way it always is.
[02:02:01.440 --> 02:02:04.720]   - And there is legitimate concern in the area of disinformation
[02:02:04.720 --> 02:02:07.200]   because you can create disinformation at speed now.
[02:02:07.200 --> 02:02:11.040]   And so we got an election coming up in 2024.
[02:02:11.040 --> 02:02:14.720]   And the zone is going to be flooded with crap.
[02:02:14.720 --> 02:02:16.320]   It's really good at creating bad crap.
[02:02:16.320 --> 02:02:20.080]   You don't need a bunch of Russians in a Russian troll farm to do that anymore.
[02:02:20.080 --> 02:02:21.200]   You can just do it with machines.
[02:02:21.200 --> 02:02:25.920]   - And chat GPT can write English a lot more clearly than your Russian troll can say.
[02:02:25.920 --> 02:02:26.320]   - Exactly.
[02:02:26.320 --> 02:02:28.080]   - It'll be grammatical.
[02:02:28.080 --> 02:02:28.960]   - Grammatical crap.
[02:02:28.960 --> 02:02:33.440]   - Yeah, I feel like this is one of those issues in tech where
[02:02:33.440 --> 02:02:37.680]   I feel like the most clear answer I can think of is that I just think
[02:02:37.680 --> 02:02:42.800]   I want people to be really conscious consumers and think about
[02:02:42.800 --> 02:02:47.840]   where what their reading is coming from and what the motivation is.
[02:02:47.840 --> 02:02:47.840]   - Exactly.
[02:02:47.840 --> 02:02:47.840]   - Yes.
[02:02:47.840 --> 02:02:52.560]   - But like, I really want to say that we should be able to trust humanity,
[02:02:52.560 --> 02:02:54.560]   but it's hard to know if that's true.
[02:02:54.560 --> 02:02:56.880]   - Well, let's teach all the kids critical thinking skills.
[02:02:56.880 --> 02:03:00.880]   Oh, but not without a box or social media or take away the internet.
[02:03:00.880 --> 02:03:03.200]   But we'll totally have the next generation ready to like,
[02:03:03.200 --> 02:03:04.560]   take these things through carefully.
[02:03:04.560 --> 02:03:07.760]   - Well, then it makes you wonder why do they want to take that stuff away, right?
[02:03:07.760 --> 02:03:09.840]   - Because they might actually have critical thinking skills
[02:03:09.840 --> 02:03:13.680]   and have some dubious responses to some of the policy getting rem down the throats.
[02:03:13.680 --> 02:03:14.640]   - It's-
[02:03:14.640 --> 02:03:15.760]   - I'm getting very depressed.
[02:03:15.760 --> 02:03:16.560]   - Yeah.
[02:03:16.560 --> 02:03:20.960]   Well, no, I mean, the good news is we don't have to take it.
[02:03:20.960 --> 02:03:25.840]   I guess I just, it makes me angry because I'll go, especially,
[02:03:25.840 --> 02:03:26.960]   it's really bad on Twitter.
[02:03:26.960 --> 02:03:32.400]   All the Bitcoin bros have been replaced now by AI bros.
[02:03:32.400 --> 02:03:37.040]   15 great things you can do with chat GPT.
[02:03:37.040 --> 02:03:39.440]   12 ways to make money with chat GPT.
[02:03:39.440 --> 02:03:43.760]   And I think the only people making money are the people who are selling these
[02:03:43.760 --> 02:03:45.600]   great ideas to suckers.
[02:03:45.600 --> 02:03:50.240]   I don't, I think maybe we've oversold the potential of AI.
[02:03:50.240 --> 02:03:51.440]   Yeah, the risk is-
[02:03:51.440 --> 02:03:53.600]   - I think what's also fascinating is
[02:03:55.360 --> 02:03:58.480]   just in terms of like the cyclicalness of trends in tech,
[02:03:58.480 --> 02:04:02.400]   I do think that there is much, much more to AI than crypto.
[02:04:02.400 --> 02:04:09.520]   However, it is really interesting that two years ago, I'm sure that this, like,
[02:04:09.520 --> 02:04:14.080]   the podcast was all about like what's happening with ETH.
[02:04:14.080 --> 02:04:20.000]   And then now in our little document here, I don't think there's a single crypto story.
[02:04:20.000 --> 02:04:21.040]   - No, no ETH stories.
[02:04:21.040 --> 02:04:22.080]   - No.
[02:04:22.080 --> 02:04:22.080]   - Nope.
[02:04:22.080 --> 02:04:24.080]   - And who knows-
[02:04:24.080 --> 02:04:24.880]   - Who was more than-
[02:04:24.880 --> 02:04:30.080]   - Will be in two years, but also AI has been powering so much tech for so long.
[02:04:30.080 --> 02:04:35.680]   It's just that now with generative AI in particular, it's much more obvious
[02:04:35.680 --> 02:04:40.000]   and people are using chat GPT just like to play around with it
[02:04:40.000 --> 02:04:41.920]   and then seeing what this tech is capable of.
[02:04:41.920 --> 02:04:44.320]   - I mean, there's certain topics that I just tune out.
[02:04:44.320 --> 02:04:45.760]   So I skip the metaverse.
[02:04:45.760 --> 02:04:50.240]   I skip the Bitcoin stuff and I'm generally skipping the AI,
[02:04:50.240 --> 02:04:52.000]   but I'm going to get sucked back into it.
[02:04:52.000 --> 02:04:54.400]   Because AI is raising a couple of issues.
[02:04:54.400 --> 02:04:59.440]   Some of which is the application of it, but the other is the inputting and the training
[02:04:59.440 --> 02:05:02.720]   and that keeps ending up hitting the copyright space hard.
[02:05:02.720 --> 02:05:07.120]   Partly because there's an intersection between the two,
[02:05:07.120 --> 02:05:09.760]   but I'm a little bit more alarmed because the people who say,
[02:05:09.760 --> 02:05:13.360]   "No, you shouldn't be able to train your AI on a population of words."
[02:05:13.360 --> 02:05:18.560]   I think impacts the right to read and what does get me worried is everybody else who's
[02:05:18.560 --> 02:05:20.880]   impacting the right to read that we're going to take away the books.
[02:05:20.880 --> 02:05:23.840]   We're going to take away the social media and I think we need to have a bigger
[02:05:23.840 --> 02:05:28.080]   reckoning of that is an important first amendment right and it's an important right for a reason.
[02:05:28.080 --> 02:05:33.680]   And I think we lock things up away from, it's not good to lock the words up from people,
[02:05:33.680 --> 02:05:38.560]   but I also think it's a problem if you lock things up so people can't send their assistant
[02:05:38.560 --> 02:05:42.640]   to go read the books for them or their robotic assistant to go read the books for them.
[02:05:42.640 --> 02:05:45.840]   And I think people need to think through what they're asking for,
[02:05:45.840 --> 02:05:48.320]   especially as a solution for, I think we have general,
[02:05:48.320 --> 02:05:53.520]   genuine problems in how we apply it, but that doesn't mean we should forbid the training because
[02:05:53.520 --> 02:05:56.640]   the training implicates completely different policy issues.
[02:05:56.640 --> 02:06:01.360]   Once in a while, I will intentionally use these chatbots to try to get them to talk about stuff
[02:06:01.360 --> 02:06:05.200]   that I've written about to see if I can see what I've written in them.
[02:06:05.200 --> 02:06:11.120]   And I'd say most of the time, even if they are kind of reflecting something I did,
[02:06:11.120 --> 02:06:14.800]   it's not in a way that I find disturbing or think should be illegal.
[02:06:14.800 --> 02:06:20.560]   I did see one of them, I think, barred the other day, which they did the kind of thing that,
[02:06:20.560 --> 02:06:25.840]   if it was a human doing it, you would say that they were either a plagiarist or flirting with
[02:06:25.840 --> 02:06:30.400]   plagiarism because there really were entire blocks of my wording in there with no attribution.
[02:06:30.400 --> 02:06:34.880]   And so it'll be interesting to see. I mean, if that kind of stuff I'm not crazy about,
[02:06:34.880 --> 02:06:39.760]   but it's quite different if me and a dozen other people who wrote about something all get
[02:06:39.760 --> 02:06:46.160]   mishmash together and something gets spaspat out on the other side that might not exist in exactly
[02:06:46.160 --> 02:06:49.840]   the same way if I hadn't written my piece. That's a creative, but it's not entirely dependent on
[02:06:49.840 --> 02:06:54.560]   my own personal wording, at least to a bad distribution if you're going to use whole blocks of what I wrote.
[02:06:54.560 --> 02:07:00.160]   I'm worried that we're accidentally banning web crawlers. Yeah, totally. I mean, this is
[02:07:00.160 --> 02:07:05.120]   the new web crawling. And people that used to think web crawling was sketchy.
[02:07:07.440 --> 02:07:14.720]   I think it's also really interesting, the difference between how we see generative AI with text versus
[02:07:14.720 --> 02:07:23.520]   with images, because I think it's a lot less obvious in generating a text block to see where
[02:07:23.520 --> 02:07:31.280]   it is drawing very specific phrases and ideas directly from writers, whereas with our,
[02:07:32.240 --> 02:07:37.920]   you can say, do this in the style of Van Gogh. And it's very clearly like, yep, those are those
[02:07:37.920 --> 02:07:45.920]   very particular kinds of brush strokes and whatnot. Or like, it feels a lot more obvious with visual
[02:07:45.920 --> 02:07:52.240]   art. And I don't know what that really, what are the implications of that? But I find it interesting.
[02:07:52.240 --> 02:07:59.520]   You see it all the time on Twitter among these new AI bros, the notion that in five years films
[02:07:59.520 --> 02:08:03.680]   will be made by AI, not filmmakers and not actors and not writers.
[02:08:03.680 --> 02:08:08.160]   I just don't think people want that though. No, I don't want that. That's for sure. I don't like
[02:08:08.160 --> 02:08:15.040]   that. I mean, I'm worried about people like for people making illustrations for like book covers
[02:08:15.040 --> 02:08:21.440]   or something. It's like, yeah, you can probably ask an AI to make like crappy or a picture of a
[02:08:21.440 --> 02:08:26.240]   bat that's turning into a woman. I don't know what book that is, but there's an idea. But
[02:08:27.600 --> 02:08:34.000]   but like for actual creative works, like, I don't want to see a film that an AI made. And I think
[02:08:34.000 --> 02:08:41.200]   there's a gimmicky interest in it, but historically, in our these gimmicky interests are only temporary.
[02:08:41.200 --> 02:08:47.120]   And I think right now we are in the moment where people think it's fun. If maybe there was a film
[02:08:47.120 --> 02:08:51.600]   made with AI, it would be like, Oh, this is interesting. But then when there's a second
[02:08:51.600 --> 02:08:55.200]   movie made using AI, no one will think it's interesting anymore.
[02:08:55.840 --> 02:09:02.480]   I'm not even crazy about movies that are mostly CGI built. And I think with one of the Star Wars,
[02:09:02.480 --> 02:09:07.680]   I think the last Jedi, they made a point to say we did our special effects by going back to modeling.
[02:09:07.680 --> 02:09:11.920]   Oh, interesting. Yeah, there's something about that where they were like, we didn't want it to be
[02:09:11.920 --> 02:09:16.400]   just so computer-y. We wanted it to actually be the old school things where there was some
[02:09:16.400 --> 02:09:20.960]   physical tangibility that still showed through onto the print that people were watching.
[02:09:20.960 --> 02:09:24.640]   How long have we been trying to fight the uncanny valley with computer animation?
[02:09:25.200 --> 02:09:28.400]   And yet I don't think we've yet done computer animation that really
[02:09:28.400 --> 02:09:35.440]   is like human, right? Or have we? Maybe I don't maybe we haven't. I don't know yet.
[02:09:35.440 --> 02:09:38.080]   It's shrunk, but I mean, you used to rise in worlds.
[02:09:38.080 --> 02:09:45.920]   But no legs is the giveaway, right? It's that's the key. All right. So I have asked mid-journey Amanda
[02:09:45.920 --> 02:09:51.840]   to give me an image of a bat that is turning into a woman. And actually, he's doing a pretty good
[02:09:51.840 --> 02:09:58.160]   job here. I, which one do you like? One, two, three, or four?
[02:09:58.160 --> 02:10:03.040]   I think four is interesting, but I'm also seeing it like this big on my screen.
[02:10:03.040 --> 02:10:07.600]   Yeah. Well, I'm going to I'm going to upscale it. I'm going to make it look a little bit bigger so
[02:10:07.600 --> 02:10:12.880]   you can you can see it. There it is. Oh, how do you like that?
[02:10:12.880 --> 02:10:13.920]   See, that's pretty cool.
[02:10:13.920 --> 02:10:18.400]   Hey, bad kind of Elizabethan. It's an Elizabethan bat turning into a woman.
[02:10:18.400 --> 02:10:25.360]   Victorian. Oh, yeah. We just have to use chat GPT to write a story about a bat turning into a woman.
[02:10:25.360 --> 02:10:31.200]   And I can't do it. You can't chat GPT. I could make it up. No, exactly.
[02:10:31.200 --> 02:10:36.160]   And my bother was that she could do it. I like this one myself. I think this is like a wood
[02:10:36.160 --> 02:10:43.520]   chat GPT. It'll just be like there once was a bat. And then suddenly it gained consciousness.
[02:10:43.520 --> 02:10:47.760]   And now it's a woman and it has to figure out how to be a person instead of a bat.
[02:10:47.760 --> 02:10:52.880]   You I'm going to this is your assignment, Harry. You do it with bar. I'll do it with chat GPT.
[02:10:52.880 --> 02:11:00.720]   All right. What will the prompt be? Let's see. Tell me a story in what's in what style should
[02:11:00.720 --> 02:11:05.440]   the story be? Do we want to shake spear in? Do we want it to Kenzian?
[02:11:05.440 --> 02:11:09.600]   I'm going to try to think of one with my brain. That isn't the obvious.
[02:11:09.600 --> 02:11:14.400]   I'm going to try to think of a brain. Okay, you two do humans. I'm going to use chat GPT for
[02:11:14.400 --> 02:11:28.640]   tell that turning into a story in the style of Dickens about a bat turning into a woman, right?
[02:11:28.640 --> 02:11:28.960]   Okay.
[02:11:28.960 --> 02:11:38.800]   Now I have to point out that I'm using chat GPT for you're using a bar. And then we have two humans.
[02:11:38.800 --> 02:11:45.760]   Here's what chat GPT for said in the crowded bustling heart of London far removed for the
[02:11:45.760 --> 02:11:52.160]   Gentile squares and manicured parks of its aristocratic corners. They existed a unique feature,
[02:11:52.160 --> 02:11:58.800]   a sight unseen in such a urban sprawl. It was an old gnarled oak. We didn't even mention an oak
[02:11:58.800 --> 02:12:03.840]   relic of ancient woods long since raised to the ground within the hollow of this time
[02:12:03.840 --> 02:12:10.000]   ravaged tree nestled an inky creature of the night, a bat named Myrtle.
[02:12:10.000 --> 02:12:17.120]   Myrtle was no ordinary bad. Her eyes held a human like lint. Her movements were purposeful
[02:12:17.120 --> 02:12:22.080]   and intelligent and she was inexplicably drawn to the city's glow, especially to its vibrant
[02:12:22.080 --> 02:12:27.120]   pulse of humanity. Her fascination for the world of men was so intense that it boarded on obsession
[02:12:27.120 --> 02:12:32.240]   obsession that would trigger a metamorphosis of a most fantastical sort. One faithful night
[02:12:32.240 --> 02:12:38.000]   a storm of biblical proportion raged. This is actually not bad. Go at what do you got, Harry?
[02:12:38.000 --> 02:12:42.320]   Oh, maybe you should read that just so we have the same flavor. All right. I'll read over it.
[02:12:42.320 --> 02:12:47.920]   Give it to me here. It starts with the big scliche in the book. Sure. Here's a story in the
[02:12:47.920 --> 02:12:55.200]   style of Dickens about a bat turning into a woman. It was a stormy night. So that's it.
[02:12:55.200 --> 02:13:00.640]   A wind held in the rain beat against the windows of the old house inside a young woman set by
[02:13:00.640 --> 02:13:07.520]   the fire reading a book. She was alone and she was scared. Suddenly there was a loud crash.
[02:13:07.520 --> 02:13:12.400]   The young woman jumped up and ran to the window. She saw a bat flying through the storm. The bat
[02:13:12.400 --> 02:13:17.520]   flew into the house and landed on the mantelpiece. The young woman was terrified. She'd never seen
[02:13:17.520 --> 02:13:22.240]   a bat before. She didn't know what to do. The bat looked at the young woman with its big dark
[02:13:22.240 --> 02:13:27.920]   eyes. The young woman felt a strange sensation. She felt like she was being drawn to the bat.
[02:13:27.920 --> 02:13:33.120]   Oh, it goes on. I'm actually more engaged by this story. It's not as well written, though,
[02:13:33.120 --> 02:13:36.560]   especially the darkest. And eventually you learned that this bat was also a woman.
[02:13:36.560 --> 02:13:42.320]   Oh, but that's kind of the kicker rather than the thing. By the way, my story ends.
[02:13:42.320 --> 02:13:44.720]   Over many months, Myrtle, now known as Miriam,
[02:13:44.720 --> 02:13:52.480]   listened to Miss Havisham's stories, her tales of lost love and long dead dreams.
[02:13:52.480 --> 02:13:57.360]   Wow. Even as Miriam, she retained the curious observant nature of her bat self.
[02:13:57.840 --> 02:14:01.600]   But now had the power to engage to influence, to help she understood the loneliness of human
[02:14:01.600 --> 02:14:05.840]   hearts, the shared plight of all beings, whether they soared in the sky or walked.
[02:14:05.840 --> 02:14:10.640]   I think chat GPD for wins. This says a similar conclusion in general with these things.
[02:14:10.640 --> 02:14:13.360]   Happy ending. They often end by talking about how great something is.
[02:14:13.360 --> 02:14:18.480]   If you ask them about a tech journalist, any tech journalist, they will say this person is
[02:14:18.480 --> 02:14:23.200]   incredible and well known and beloved. Shoot, I thought that was only me.
[02:14:23.920 --> 02:14:27.280]   Sorry, Leon. I'm so disappointed.
[02:14:27.280 --> 02:14:32.000]   It is interesting that you chose Charles Dickens for this experiment because,
[02:14:32.000 --> 02:14:40.000]   according to my Google search, is Dickens public domain? Apparently, a lot of the works of
[02:14:40.000 --> 02:14:45.520]   Dickens are in the public domain. So it does beg the question of how much of this is coming from
[02:14:45.520 --> 02:14:50.720]   the fact that chat GPT probably is trained on like entire Dickens novels.
[02:14:51.760 --> 02:14:55.120]   Oh, good point. All of that would be in there, wouldn't it?
[02:14:55.120 --> 02:14:57.840]   Yeah. By the way, it loves you, Harry.
[02:14:57.840 --> 02:14:59.840]   By the way, I mean, even just like the street,
[02:14:59.840 --> 02:15:02.160]   that probably is giving me products for things I didn't write.
[02:15:02.160 --> 02:15:07.760]   Please note this information may be outdated as my training only includes data
[02:15:07.760 --> 02:15:09.280]   after September 2021.
[02:15:09.280 --> 02:15:15.600]   You can if you have chat GPT plus, you can turn on the browsing feature in which case it adds
[02:15:15.600 --> 02:15:17.840]   more to it. Yeah, I have plugins yet.
[02:15:17.840 --> 02:15:20.160]   All right. I wrote four paragraphs of fiction while you were.
[02:15:20.160 --> 02:15:22.880]   Let's hear what happens to Myrtle the Bat.
[02:15:22.880 --> 02:15:24.000]   Oh, it's Myrtle the Bat.
[02:15:24.000 --> 02:15:24.960]   No, it does not be Myrtle.
[02:15:24.960 --> 02:15:26.960]   Okay. Actually, that would make sense, but I didn't name the Bat.
[02:15:26.960 --> 02:15:31.600]   It had been such a nice castle, abandoned, spooky, and devoid of any humans whatsoever.
[02:15:31.600 --> 02:15:34.400]   Until one day, suddenly a ruckus came from down below,
[02:15:34.400 --> 02:15:39.120]   peering down the old stone staircase, the bat saw boxes and suitcases being piled inside the door.
[02:15:39.120 --> 02:15:44.560]   It had at last happened. People had moved in and the bat quickly realized they had moved
[02:15:44.560 --> 02:15:47.520]   into its house. What was it to do with its home? So I made it.
[02:15:48.160 --> 02:15:50.960]   The bat sighed and reached its reluctant conclusion.
[02:15:50.960 --> 02:15:53.520]   If you can't beat them, he said, join them.
[02:15:53.520 --> 02:15:56.560]   And so it flew off to the attic where the old potion jars were kept,
[02:15:56.560 --> 02:16:00.720]   knocked off the lid of one holding a bubbling blue solution and dove in head first.
[02:16:00.720 --> 02:16:04.480]   As a gas-dewing pain as the burn it felt like with boring into each of its cells,
[02:16:04.480 --> 02:16:06.640]   it knew that this was still the solution.
[02:16:06.640 --> 02:16:09.120]   This time tomorrow, I shall be a woman.
[02:16:09.120 --> 02:16:10.880]   Dr. Jekyll and Ms. Bat.
[02:16:10.880 --> 02:16:13.120]   I like that. I like that.
[02:16:13.120 --> 02:16:15.280]   Amanda, you want to give it a shot or?
[02:16:16.160 --> 02:16:21.520]   Yeah, so I was thinking more about like conceptually.
[02:16:21.520 --> 02:16:26.880]   I did not write anything down. I was riveted by your experiments.
[02:16:26.880 --> 02:16:28.880]   I think you saw. We're very good. Yeah.
[02:16:28.880 --> 02:16:39.040]   Yeah, but my idea is turning this into a non-fiction essay project in which we explore
[02:16:39.600 --> 02:16:46.320]   tales from around the world, from folklore in which women and animals are connected and women
[02:16:46.320 --> 02:16:52.720]   are turning into animals or vice versa. Or for example, in the story of Cersei from Greek mythology,
[02:16:52.720 --> 02:16:57.760]   she's turning men into animals, but she herself is not being turned into an animal.
[02:16:57.760 --> 02:17:01.840]   So yeah, so I'm taking this and doing a non-fiction essay collection about
[02:17:01.840 --> 02:17:06.640]   your trials of women and animals. You can sue all these AIs because you were the one who
[02:17:06.640 --> 02:17:08.880]   came up with a woman and turning into a bat.
[02:17:08.880 --> 02:17:11.200]   Oh, gosh, no.
[02:17:11.200 --> 02:17:21.120]   And this is what Sam Altman thinks about all of this CEO of chat or the folks who do chat GBT
[02:17:21.120 --> 02:17:30.000]   open AI. I don't know why Reuters decided to use this picture of Sam testifying.
[02:17:30.000 --> 02:17:32.000]   We don't have enough moral panicking.
[02:17:32.000 --> 02:17:38.960]   You are so scared. I'm scared. He actually is the pull quote is when they asked him if
[02:17:38.960 --> 02:17:46.480]   the artificial intelligence could interfere with election integrity, he says, I'm nervous about it.
[02:17:46.480 --> 02:17:52.320]   So there's a picture of Sam being nervous about it. Please,
[02:17:52.320 --> 02:17:55.600]   you too, Sam regulated. It's too good. It's scary.
[02:17:55.600 --> 02:17:59.120]   Let's take a little break. When we come back, final thoughts.
[02:18:00.160 --> 02:18:04.320]   You guys are, gosh, I don't know. I have to give a little credit to both
[02:18:04.320 --> 02:18:06.560]   Bard and chat GBT for coming up with some interesting.
[02:18:06.560 --> 02:18:10.640]   Well, the human who wrote four paragraphs of original vision in five minutes.
[02:18:10.640 --> 02:18:14.080]   Was amazing. Gosh, I don't know. They were all quite good.
[02:18:14.080 --> 02:18:16.960]   Oh, good. I mean, compared. Oh, compared favorably to chat.
[02:18:16.960 --> 02:18:22.320]   You did a great job, Kathy, but we're going to go with the computer because it's cheaper.
[02:18:22.320 --> 02:18:24.480]   I stand with WGA.
[02:18:27.840 --> 02:18:32.160]   But you have no chance of plagiarizing Dickens.
[02:18:32.160 --> 02:18:34.800]   I would have if I had more time.
[02:18:34.800 --> 02:18:38.560]   And he's in the public domain. So I totally do that.
[02:18:38.560 --> 02:18:41.040]   Totally clear. It just took a while to physically write the program.
[02:18:41.040 --> 02:18:43.040]   I actually was impressed with what you did.
[02:18:43.040 --> 02:18:44.400]   Okay. Thank you, too. It was quite good.
[02:18:44.400 --> 02:18:50.160]   We should have a vote. Chatroom, which story would you want to finish?
[02:18:50.160 --> 02:18:52.240]   Kathy, Bard, or chat GPT?
[02:18:52.240 --> 02:18:54.000]   Which one? I'm with Kathy.
[02:18:54.000 --> 02:18:55.040]   Thank you, gosh.
[02:18:55.040 --> 02:18:58.720]   I'm glad to have the moment the human the human got it right.
[02:18:58.720 --> 02:19:00.080]   Read my nonfiction book.
[02:19:00.080 --> 02:19:05.360]   I showed they brought to you by Mint Mobile. We love Mint Mobile.
[02:19:05.360 --> 02:19:08.400]   Been using Mint Mobile for two, three years now. I can't remember.
[02:19:08.400 --> 02:19:11.200]   I got from Mint Mobile. I bought an iPhone SE.
[02:19:11.200 --> 02:19:15.840]   That was 15 bucks a month. And then I got the 15 buck a month plan.
[02:19:15.840 --> 02:19:20.240]   Unlimited talk and text plus five gigabytes a month.
[02:19:20.240 --> 02:19:22.640]   It's actually been going up. It was three when I got it.
[02:19:22.640 --> 02:19:26.800]   It's now five. They keep adding more data on the nation's number one,
[02:19:26.800 --> 02:19:30.880]   five G network, the largest five G network in America.
[02:19:30.880 --> 02:19:33.360]   This is a great deal.
[02:19:33.360 --> 02:19:38.480]   Inflation's everywhere. Prices are going up, except at Mint Mobile,
[02:19:38.480 --> 02:19:41.600]   the only thing getting inflated at Mint Mobile is a number of gigabytes you can download.
[02:19:41.600 --> 02:19:44.400]   $15 a month. What's the secret?
[02:19:44.400 --> 02:19:50.080]   They don't have stores. They're the first company to sell premium wireless service online only.
[02:19:50.080 --> 02:19:55.280]   You save a ton. Plans start at $15 a month. You can bring your family too.
[02:19:55.280 --> 02:19:57.440]   And by the way, families start at two people.
[02:19:57.440 --> 02:20:03.200]   So the family plan is a great deal and they don't have to actually just don't tell
[02:20:03.200 --> 02:20:05.200]   Ryan Reynolds, but they don't even have to be relative.
[02:20:05.200 --> 02:20:07.600]   Your family could be your friends, right?
[02:20:07.600 --> 02:20:11.680]   By going online only eliminating the traditional cost of retail, Mint Mobile
[02:20:11.680 --> 02:20:16.400]   passes significant savings on to you. They don't even charge you for the sim.
[02:20:16.400 --> 02:20:19.120]   And now they support e-sim. So if you have a phone that supports e-sims,
[02:20:19.120 --> 02:20:23.600]   it's even easier to switch to Mint Mobile. You could port your number over.
[02:20:23.600 --> 02:20:28.080]   Happens very quickly. You could bring all your contacts and apps with you or get a phone from
[02:20:28.080 --> 02:20:32.160]   them. All plans again come with unlimited talking text and high speed data delivered
[02:20:32.160 --> 02:20:37.920]   in the nation's largest 5G network switch to Mint Mobile. Get premium wireless service
[02:20:37.920 --> 02:20:43.280]   starting at just $15 a month. And I know when I say that number, you probably gloss it over
[02:20:43.280 --> 02:20:48.320]   because you think, "Oh, there's going to be added fees and services. It's going to cost me $50,
[02:20:48.320 --> 02:20:54.160]   $60, $70." No, that's the other guys. Mint Mobile, $15 a month. That's it.
[02:20:54.160 --> 02:21:00.880]   $15, $15. How much are you paying for your wireless service? Maybe you ought to think about
[02:21:00.880 --> 02:21:05.840]   switching. Get your new wireless plan for just $15 a month and get the plan shipped to your door
[02:21:05.840 --> 02:21:12.720]   for free. Go to mintmobile.com/twit, mintmobile.com/twit. They'll ship it right to your door.
[02:21:13.840 --> 02:21:19.920]   Cut your wireless bill to $15 a month. That's for 5GB of data. That's all most people need,
[02:21:19.920 --> 02:21:23.040]   but you want more. They've got more. They even have an unlimited plan. $30.
[02:21:23.040 --> 02:21:31.760]   Mint Mobile. How much are you paying right now? Three times that at least, right? Mintmobile.com/twit.
[02:21:31.760 --> 02:21:39.360]   We thank them so much for supporting our show. And you support it when you go to that address,
[02:21:39.360 --> 02:21:47.520]   mintmobile.com/twit. We've got a little mini movie we made narrated by the way by my AI voice
[02:21:47.520 --> 02:21:53.440]   on everything that happened this week on Twitch Watch. Pixel phones are gaining a new ringtone
[02:21:53.440 --> 02:22:02.080]   inspired by Kenny G. Oh my God. I swear you always know when it's Kenny G on the phone.
[02:22:02.080 --> 02:22:06.320]   Get ready for the name of this. This is the Kenny Jingtone.
[02:22:08.880 --> 02:22:16.400]   Previously on Twitch, Tech News Weekly. Late yesterday, the news broke that Montana became
[02:22:16.400 --> 02:22:21.600]   the first state to officially ban TikTok. One expert I talked to, I loved this analogy,
[02:22:21.600 --> 02:22:29.760]   compared it to legalizing flying. You can say now people can fly, but people aren't really built
[02:22:29.760 --> 02:22:37.040]   to fly. This week in Google. We know. Don't ever buy their stuff, period. If I've ever shown you
[02:22:37.040 --> 02:22:43.200]   them, just say Stacy, you are wrong. Here's the issue. One, it's actually not that there was a
[02:22:43.200 --> 02:22:46.960]   serious vulnerability, although it is very serious. And the researchers was like, this is terrible.
[02:22:46.960 --> 02:22:52.320]   We should tell Belkin Weima. So they did. And Belkin Weima said, eh, they're not going to fix it.
[02:22:52.320 --> 02:22:59.680]   Security now. If an air tag, set of air pods, or find my network accessory, is discovered to be
[02:22:59.680 --> 02:23:07.120]   unlawfully tracking a person. Law enforcement can request any available information from Apple
[02:23:07.120 --> 02:23:12.560]   to support their investigation. No one appears to care about tag ownership tracking.
[02:23:12.560 --> 02:23:18.640]   But I suspect that not many people know or have stopped to wonder. I got a hammer.
[02:23:18.640 --> 02:23:28.080]   And I got an air tech screw that. I don't want anybody to eat. You know that we know where your
[02:23:28.080 --> 02:23:34.560]   phones are. Right? It's a good point, actually. You know, they could track you about 100 ways to
[02:23:34.560 --> 02:23:40.480]   Sunday. The air tag is not the problem. Hope you enjoyed this week. And by the way, I did kill my
[02:23:40.480 --> 02:23:44.080]   air tag. And that was that. I don't miss it.
[02:23:44.080 --> 02:23:53.760]   Nast, huh? Carl up with your metaverse headset. Yeah. Yeah, I could do that. Everything I'm
[02:23:53.760 --> 02:23:58.080]   wearing tracks me. That's not I realize there's no point in worrying about your air tag.
[02:23:58.080 --> 02:24:04.880]   Everyone knows where you are at all times. NASA has made good a boo boo, at least a boo boo
[02:24:04.880 --> 02:24:12.640]   according to Jeff Bezos. His space company Blue Origin is now going to be able to make a second
[02:24:12.640 --> 02:24:20.400]   human crude lunar lander. NASA chose SpaceX to make the first one to which Jeff Bezos
[02:24:22.080 --> 02:24:24.800]   yelled and screamed and said it was a terrible thing.
[02:24:24.800 --> 02:24:33.440]   I think he might have even sued. But now, now the agency on Friday announced that there's a
[02:24:33.440 --> 02:24:39.360]   $3.4 billion contract coming your way, Jeff, to make a lunar lander for the upcoming Artemis
[02:24:39.360 --> 02:24:46.320]   mission to the moon. The Blue Origin lander will have one uncrewed demonstration mission
[02:24:46.320 --> 02:24:49.280]   ahead of a human crude demo that's set to take place in.
[02:24:50.800 --> 02:24:56.240]   I thought this was going to be like next year, 2029, John. That's like forever.
[02:24:56.240 --> 02:25:00.960]   This is currently the plan is for four astronauts to fly to the
[02:25:00.960 --> 02:25:06.320]   this is a it is a Rube Goldberg device. They're going to fly to the Gateway space station
[02:25:06.320 --> 02:25:13.520]   on NASA's Space Launch System rocket and the Orion spacecraft. Then they're going to get out,
[02:25:13.520 --> 02:25:19.760]   get into the Blue Origin and they're going to go on the Blue Origin for a week long trip to the
[02:25:19.760 --> 02:25:29.920]   moon South Pole. So complicated. SpaceX won a sole contract in 2021. Blue Origin sued and lost
[02:25:29.920 --> 02:25:35.920]   over the decision in 2021. NASA announced in 2022. Never mind. We're going to do a second one.
[02:25:35.920 --> 02:25:41.040]   So now they got to the 60th anniversary in 2029 of the original landing. Wow.
[02:25:41.040 --> 02:25:45.120]   I'm that means I'm the only person in this room who remembers it.
[02:25:45.840 --> 02:25:51.360]   I thought I remembered that watching it in kindergarten in 1969 until somebody pointed out it didn't
[02:25:51.360 --> 02:25:56.960]   happen during the day and I would not have been at school in July. But you're not old enough to
[02:25:56.960 --> 02:26:03.120]   remember it. Are you? I theoretically maybe you were a little kid. I paired your boot have
[02:26:03.120 --> 02:26:08.240]   been remembering a later landing. Yes. Yeah. Well, that counts. I remember getting up middle of
[02:26:08.240 --> 02:26:12.320]   the night. What wasn't quite the middle night? I was like four a.m. or something in Rhode Island.
[02:26:12.320 --> 02:26:20.000]   I had the Columbia launch originally in kindergarten. That was mine. That's the one that well, well,
[02:26:20.000 --> 02:26:24.880]   it fell apart later. I yeah, when I was in kindergarten, I remember 9/11.
[02:26:24.880 --> 02:26:39.280]   Let's see. A D&D streaming TV channel is coming your way. Excited.
[02:26:41.040 --> 02:26:47.360]   I love D&D and I am so skeptical of what is going on with like Hasbro who owns Wizards of the Coast,
[02:26:47.360 --> 02:26:56.000]   which owns D&D trying to make D&D like a Lord of the Rings or a Harry Potter type franchise,
[02:26:56.000 --> 02:27:01.600]   which this is literally like what Hasbro has said in communications with investors that they're
[02:27:01.600 --> 02:27:11.600]   trying to make D&D quote unquote a four quadrant streaming or a four quadrant media property or
[02:27:11.600 --> 02:27:20.080]   something. But it's just so weird because there's really not IP with D&D like what makes D&D fun
[02:27:20.080 --> 02:27:24.560]   is what you're doing with your friends, right? You're creating stories collaboratively with
[02:27:24.560 --> 02:27:31.120]   other people like that aren't property of D&D. And I don't know. I mean, I'm curious what's going
[02:27:31.120 --> 02:27:35.280]   to happen with this, but I'm also like, damn, they're just really trying to monetize this right now.
[02:27:35.280 --> 02:27:39.280]   Well, you can't really blame them. Dungeons and Dragons. I think the part of it is because of
[02:27:39.280 --> 02:27:48.000]   the successive encounter party, right? They are huge. One of the top channels on YouTube and Twitch.
[02:27:48.000 --> 02:27:58.640]   I mean, there's like, there's actual play shows like Critical Role and Dimension 20 that are
[02:27:58.640 --> 02:28:02.480]   hugely critical role. That's who I was thinking of. They're really big, aren't they?
[02:28:02.480 --> 02:28:11.920]   They're on Twitch. Yeah, yeah. When I think in like fall 2021, there was a Twitch data breach
[02:28:11.920 --> 02:28:18.960]   where someone published the money thing like in order, the most profitable channels and critical
[02:28:18.960 --> 02:28:23.520]   role was making something like $90 million a year from Twitch alone.
[02:28:23.520 --> 02:28:28.800]   $90 million. Check me on that because this is like something I wrote two years ago that I'm
[02:28:28.800 --> 02:28:32.640]   remembering. You could laugh off a lot as zeros and it would still be an issue.
[02:28:32.640 --> 02:28:36.880]   Well, I think one of the reasons Critical Role succeeds is because they're people.
[02:28:36.880 --> 02:28:42.320]   And you get into the people, right? You care about the people. They're professional voice actors
[02:28:42.320 --> 02:28:47.360]   and improvisers. And they're funny. What is D&D if not voice acting and improvising?
[02:28:48.080 --> 02:28:55.600]   And I don't know. You have activated my trap card, but I don't know. I mean, speaking of like
[02:28:55.600 --> 02:29:03.600]   the theme of like the legal issues of it all, like there was a whole controversy with G&D earlier
[02:29:03.600 --> 02:29:12.080]   this year where there's like a license that allows fans to make content like Critical Role
[02:29:12.080 --> 02:29:20.400]   or content even like, hey, I'm going to write a little module where you can have like a pirate
[02:29:20.400 --> 02:29:25.360]   class or something instead of the existing fantasy classes. So what's going on here?
[02:29:25.360 --> 02:29:31.120]   This is the dungeon master, this guy here. And then are there teams? Like there's two teams?
[02:29:31.120 --> 02:29:38.080]   I think those are just all the players and they have a stack or like that for screen size.
[02:29:38.080 --> 02:29:45.200]   Hollywood squares. Yeah. Yeah. I can see why you'd watch this. Looks more like the match.
[02:29:45.200 --> 02:29:49.120]   No, yeah. I mean, it's but you're not watching the game. You're just watching these people talk
[02:29:49.120 --> 02:29:53.280]   about it, right? There's no, it's not like a League of Legends where there's something to watch.
[02:29:53.280 --> 02:29:58.720]   No, I mean, like the way that you play is like you're quite literally just like using the structure
[02:29:58.720 --> 02:30:06.800]   of these. It's creative. That are now open source that like you're just using these rules as a way
[02:30:06.800 --> 02:30:11.760]   of telling stories with your friends and you're like improvising like at a table with people.
[02:30:11.760 --> 02:30:18.080]   Like when when I go to play D&D, like I don't know what story I'm going to tell that depends on
[02:30:18.080 --> 02:30:24.160]   like what the other people around me are doing. And so it's like compelling in the same way
[02:30:24.160 --> 02:30:30.720]   that improv is compelling. Right. Right. Well, who knows? Maybe Hasbro has the secret sauce and
[02:30:30.720 --> 02:30:36.720]   they're going to make this something very exciting. I have to say there's real trouble ahead in the
[02:30:36.720 --> 02:30:43.360]   in the streaming world, but more importantly, in the cable world, court cutting has hit an all-time
[02:30:43.360 --> 02:30:51.840]   high, according to variety. Pay TV subscriptions. Their lowest levels since 1992. You look like a
[02:30:51.840 --> 02:30:57.120]   court cutter, Kathy Gellis. I got court cut it when I moved because I couldn't have it where I moved
[02:30:57.120 --> 02:31:02.160]   to, but now I'm going to move to someplace where I could have it. But the pricing is like insane.
[02:31:02.160 --> 02:31:08.160]   At this point, I've sort of gotten used to not having the court, but I mean, I think just
[02:31:08.160 --> 02:31:14.640]   retransmission fees are such an error. Local stations. Well, of everything. And then don't get me
[02:31:14.640 --> 02:31:18.720]   started on the copyright lawsuit that obliterated low cast where now you can't rent an antenna.
[02:31:18.720 --> 02:31:26.320]   I ripped that to shreds in the tech dirt post and I will start snarling again if I start talking
[02:31:26.320 --> 02:31:30.080]   about that case, but copyright law shouldn't do it. And it was bad for everybody because
[02:31:30.080 --> 02:31:35.360]   the public has access. These are public airwaves and the public now doesn't have a reliable way of
[02:31:35.360 --> 02:31:40.080]   getting access to what is put on their own public spectrum. And this is the problem was when we
[02:31:40.080 --> 02:31:47.520]   allowed cable company or local TV channels to both charge you and run ads. They didn't want to
[02:31:47.520 --> 02:31:52.400]   give up either revenue streams. So the idea of have had supported free over the air TV.
[02:31:53.360 --> 02:31:58.400]   Nope, they didn't want to do that anymore. They wanted to make sure that you paid fees to them.
[02:31:58.400 --> 02:32:05.600]   I still miss low cast. I still miss. I can't participate in cultural events like a Super Bowl.
[02:32:05.600 --> 02:32:10.720]   I mean, I can't even watch the ads that advertisers are still paying money to have broadcast audiences
[02:32:10.720 --> 02:32:15.440]   because now the audience has to pay for the privilege of watching the ad. Pay TV penetration
[02:32:15.440 --> 02:32:26.080]   like cable or direct TV is now down to 58% of US households. Big big declines for Comcast for
[02:32:26.080 --> 02:32:33.520]   direct TV for dish Comcast dropped 614,000 customers in the first quarter alone.
[02:32:33.520 --> 02:32:38.320]   I wouldn't to your assumption, I wouldn't be a court cutter except circumstances cut it.
[02:32:38.320 --> 02:32:43.440]   And now it just may be you're having you take it in. No, I'd like to come back, but it's to
[02:32:44.080 --> 02:32:50.080]   for Heather, even YouTube TV is like 73 bucks for the base services. And that is because of the
[02:32:50.080 --> 02:32:56.960]   locals, by the way, I think that's. Yeah, I just, I don't know. I guess in my like adult life, I think
[02:32:56.960 --> 02:33:03.920]   at first I was like, I'm not paying for cable. I'm like, you know, like low paid writing job.
[02:33:03.920 --> 02:33:08.640]   And then now I'm like, I could probably afford it, but I'm just you're not having it. And I'm like,
[02:33:08.640 --> 02:33:13.440]   do I really want to pay an extra $70 a month? Your life is better without it.
[02:33:13.440 --> 02:33:19.520]   Well, I don't pay a lot of streamers either. So like, I miss when I could sort of like have one
[02:33:19.520 --> 02:33:24.240]   easy, budgetable price and get most of what I wanted. And now that reality doesn't really exist.
[02:33:24.240 --> 02:33:29.600]   But I don't really want to pay. I mean, I think I'd rather pay cable than a whole bunch of streamers,
[02:33:29.600 --> 02:33:34.720]   but at this point, the streaming market is economically nonsensical anyway. So I just pay for like a
[02:33:34.720 --> 02:33:40.880]   couple things that like happen to mean more and it's not worth dumping a whole lot of money into.
[02:33:40.880 --> 02:33:46.160]   Have you seen Harry McCracken? Have you seen speaking of streaming the Blackberry movie?
[02:33:46.160 --> 02:33:52.000]   Saw it and used it as an excuse to write about how great Blackberry was last week.
[02:33:52.000 --> 02:33:56.960]   We were so addicted to it. This is Harry's article and fast company, the Triumph. Yes, Triumph
[02:33:56.960 --> 02:34:02.560]   of Blackberry. Blackberry is no longer doing a phone, but they do do the operating system for many
[02:34:02.560 --> 02:34:06.960]   cars. Yeah. Q and X is actually pretty cool. They just acquired it too late in the game for it to
[02:34:06.960 --> 02:34:13.920]   help them make a great phone. Yeah. So the movie, which is based on the book losing the signal,
[02:34:13.920 --> 02:34:20.480]   it's out now or is it about to be out? It's not for a few weeks. It's not a documentary, right?
[02:34:20.480 --> 02:34:25.520]   No, it's in fact that the first thing you see is a card explaining that it's a fictionalized
[02:34:25.520 --> 02:34:31.360]   version of what happened and they really did not try super hard to be all that accurate.
[02:34:31.360 --> 02:34:36.480]   And it's in fact kind of cartoony. The trailer is great. I really enjoyed the trailer. Oh, good. So
[02:34:36.480 --> 02:34:41.920]   this is one to see. Well, I had some issues with that. I mean, I don't know where it is. I'm a tech
[02:34:41.920 --> 02:34:47.200]   journalist who writes about tech history. So I personally wanted something that was a little bit
[02:34:47.200 --> 02:34:52.640]   more accurate. Right. That's sad. I'd say it seems like most of the reviews have been positive and
[02:34:52.640 --> 02:34:57.760]   film critics are not going to judge it on whether they detailed exactly what happened in the 1980s
[02:34:57.760 --> 02:35:03.040]   when research and motion was getting started. So it seems like generally speaking, it has been
[02:35:03.040 --> 02:35:07.920]   a crowd, please, sir. I don't think people would want the truly geeky real story as much. That
[02:35:07.920 --> 02:35:11.360]   would have been tough. And in fact, when I wrote about it, my point was kind of that the things
[02:35:11.360 --> 02:35:15.680]   that made Blackberry great would not make for a great movie because it was stuff like the fact
[02:35:15.680 --> 02:35:19.760]   that they created. Tactile keyboards? Tactile keyboard, the fact that they had a server that
[02:35:19.760 --> 02:35:24.480]   prevented the network from coming down, the fact they cared about security. In fact,
[02:35:24.480 --> 02:35:29.120]   the movie has a few moments where there are like meetings where they tried to have people
[02:35:29.120 --> 02:35:34.160]   discussing that. And those are the least riveting parts of the movie because they just don't make
[02:35:34.160 --> 02:35:38.080]   for a great drama, even though that they do help explain like this thing once upon a time really
[02:35:38.080 --> 02:35:43.600]   mattered. It's been in theaters for two weeks, hit 1.7 million last weekend. We'll see what it does
[02:35:43.600 --> 02:35:49.360]   this weekend. That's a second weekend. This will be its third. Yeah, the trailer was great. And
[02:35:49.360 --> 02:35:54.960]   anybody who lives, I had a we all had Blackberries, right? I had a palm. I did. I was more of a trio
[02:35:54.960 --> 02:36:00.880]   person for the most part personally. Yeah, I had a trio. But right before the iPhone came out,
[02:36:00.880 --> 02:36:06.400]   my last phone before the smartphones were was a Blackberry Pearl. When did you have a Blackberry
[02:36:06.400 --> 02:36:10.960]   Amanda? You must have been in eighth grade. No, like literally, I think I was an eighth grade. I
[02:36:10.960 --> 02:36:15.280]   think it was because like my dad had them for work. And then when he got a new one, then that was
[02:36:15.280 --> 02:36:21.760]   just like the phone that I got. So we both were very into brick breaker, the Blackberry game.
[02:36:21.760 --> 02:36:27.360]   Yep. I love the Blackberry. I remember my kids saying, Dad, put down the Blackberry, pay attention
[02:36:27.360 --> 02:36:34.640]   to us. So that's that's how I know. Hey, what fun it's been to get together with all you.
[02:36:34.640 --> 02:36:39.040]   Thank you, Kathy Gellis for your analysis of the Supreme Court stuff. Very complicated,
[02:36:39.040 --> 02:36:43.520]   but I think you cut through it and explain why it was important. It clearly was. I'm glad you
[02:36:43.520 --> 02:36:49.680]   wrote those amicus briefs. I'm sorry they didn't read both of them thoroughly. But hey, you know,
[02:36:49.680 --> 02:36:54.240]   you're one for two. That's not so bad. I'll claim credit for the Google versus
[02:36:54.240 --> 02:36:58.320]   Gonzalez one. Sure. Say two for three. Okay, sure. Sure. Two for three. Yeah.
[02:36:58.320 --> 02:37:02.400]   Kathy is, of course, an author at TechDirt. If you want to hire her,
[02:37:02.400 --> 02:37:10.000]   obviously specializes in an IP loss CG Council, C O U N S E L dot com and on the mast done cloud
[02:37:10.000 --> 02:37:15.680]   at Kathy Gellis. Great to see you. First time in studio. Yes. Thank you for coming up here.
[02:37:15.680 --> 02:37:20.880]   Yes. I hope you don't catch COVID. Is it my deep fervent wish for you? Oh, thank you.
[02:37:20.880 --> 02:37:26.960]   Amanda, you're safe, I think, in your in your apartment. She's got a very softball.
[02:37:26.960 --> 02:37:31.120]   Philadelphia. Yes, softball. Well, that could be dangerous. Amanda Silverling,
[02:37:31.120 --> 02:37:36.400]   senior culture writer TechCrunch. First time on so great to have you. We will have you back
[02:37:36.400 --> 02:37:41.360]   again soon. I really appreciate you. Yeah. Thanks for having me. And I'll continue to just
[02:37:41.360 --> 02:37:46.080]   make jokes about being younger than you. Everyone feel bad. That's watching,
[02:37:46.080 --> 02:37:49.520]   which is what you want to be on the podcast. You won't be the only one. Believe me.
[02:37:49.520 --> 02:37:56.320]   This will be me in like however many years I'm going to be like, oh God, like your parents
[02:37:56.320 --> 02:38:01.920]   weren't alive for 9/11. That's right. That's exactly right. Get ready. It happens sooner than
[02:38:01.920 --> 02:38:07.280]   you think. No, it's nice to have somebody in the 30 young show. I really, I really think we need
[02:38:07.280 --> 02:38:12.000]   that perspective. I appreciate it, Amanda. It's great to see you. Harry, my buddy, my friend,
[02:38:12.000 --> 02:38:16.400]   the technologizer, find him on Fast Company. Every time he writes an article,
[02:38:16.400 --> 02:38:22.880]   it's something great, something fantastic. Always, always. No, I'm serious. You, you are no chat
[02:38:22.880 --> 02:38:30.560]   GPT. You, you really find great topics. You, you know your history. And it's always, I mean,
[02:38:30.560 --> 02:38:34.560]   I just, I read every one of them always. I know it because I lived that, dammit. You lived it. You
[02:38:34.560 --> 02:38:40.800]   were there. You were there. I want to read this Blackberry article now. Yeah. I want to see the
[02:38:40.800 --> 02:38:47.680]   movie. I love, I love the trailer. You're watching Clue tonight though. Oh yeah, that's right. Clue
[02:38:47.680 --> 02:38:54.000]   or maybe succession, the penultimate episode. You, I envy you because someday you're going to get a
[02:38:54.000 --> 02:39:00.240]   DVD box set of all four seasons. You're going to say, how did I not know this was the best show on
[02:39:00.240 --> 02:39:07.440]   television? No, I mean, I, I've been walked, like I, I've been seeing people talking about it
[02:39:07.440 --> 02:39:13.040]   on Twitter relentlessly for years. And what's funny is actually before I worked at TechCrunch,
[02:39:13.040 --> 02:39:18.160]   I was like, I don't get it. Like they're just in business meetings. And then now that I like,
[02:39:18.160 --> 02:39:25.120]   now you understand. Yes. I'm like, the stock price dropped below 120. And then they tried
[02:39:25.120 --> 02:39:32.480]   doing the hostile take over 192 per share. What is he thinking? No, it's no, it's a great show.
[02:39:32.480 --> 02:39:37.760]   I'm just very behind some. Oh, well, it's getting very exciting. We just last week, it was the
[02:39:37.760 --> 02:39:42.720]   presidential election. And it was a little too close to all that always hurts. I think we need
[02:39:42.720 --> 02:39:47.920]   more historical distance and certainty in our world before this will be called. Oh, it was very
[02:39:47.920 --> 02:39:55.920]   painful. Harry, Kathy, Amanda, thank you so much for being here. Thanks to all of you for being here.
[02:39:55.920 --> 02:40:00.240]   Thanks, especially to our club twit members who make this show and all of our shows possible.
[02:40:00.240 --> 02:40:04.480]   If you're not yet a member, I want to invite you to join seven bucks a month. You get so much,
[02:40:04.480 --> 02:40:08.240]   you get ad free versions of all of our shows shows we don't put out in public,
[02:40:08.240 --> 02:40:14.880]   like hands on Macintosh, hands on Windows, home theater, Peter Keek, the Untitled Linux show.
[02:40:14.880 --> 02:40:21.760]   You get so much for your seven bucks a month. And I can't tell you how much that seven dollars
[02:40:21.760 --> 02:40:26.640]   means to us. Only about 1% of our audience has joined the club. I'd like to get that number up.
[02:40:26.640 --> 02:40:31.920]   Would you, would you just check it out, please? Twit.tv/club twit. We'd be very,
[02:40:31.920 --> 02:40:37.760]   very grateful to have you. And if you, and if you can, no problem, it's okay. What?
[02:40:37.760 --> 02:40:42.640]   I'm sure you've made this joke, but it's cheaper than Twitter blue. It's a dollar less than
[02:40:42.640 --> 02:40:47.920]   twiddling blue and you get so much more. Actually, I will send all three of you. I will send all
[02:40:47.920 --> 02:40:53.120]   three of you a coupon because we'd like to have you in there on us because that's one of the things
[02:40:53.120 --> 02:40:57.360]   that makes club twit really fun that Discord has a lot of our hosts in there talking as well.
[02:40:57.360 --> 02:41:06.160]   We do twit every Sunday afternoon right after ask the tech guys, 2 p.m. Pacific, 5 p.m. Eastern
[02:41:06.160 --> 02:41:11.920]   time, 2,100 UTC. I say that only because if you want, you can watch us do it live. If you want the
[02:41:11.920 --> 02:41:17.120]   freshest possible version is it live.twit.tv there's streaming audio and video.
[02:41:17.120 --> 02:41:22.880]   Plus, if you are watching live, you can chat with us live in our open to all IRC chatroom,
[02:41:22.880 --> 02:41:29.760]   IRC.twit.tv. Of course, club twit is also talking about it during the show. After the fact,
[02:41:29.760 --> 02:41:34.320]   add supported versions of the show available to our website, twit.tv/thisweekintech.
[02:41:34.320 --> 02:41:38.080]   If, by the way, when you get to that page, you'll see all linked to the YouTube channel.
[02:41:38.080 --> 02:41:42.800]   There's a dedicated YouTube channel. You'll also see a link to various podcast players and the RSS
[02:41:42.800 --> 02:41:47.920]   feed. Best thing to do, subscribe in the podcast player of your choice and that way you'll get
[02:41:47.920 --> 02:41:53.360]   automatically the minutes available. That's our IRC channel there and this is the web page
[02:41:53.360 --> 02:41:58.800]   for this week in tech. Thank you everybody for being here. I'm going to go home and watch
[02:41:58.800 --> 02:42:04.160]   succession and I will see you remember we are our schedules changed a little bit this week.
[02:42:04.800 --> 02:42:12.480]   Tomorrow, Monday iOS today, what time John? 9am. So, the same time, but they're going to move
[02:42:12.480 --> 02:42:17.840]   forward today because we are going to do live coverage of Microsoft's build developers conference.
[02:42:17.840 --> 02:42:21.520]   We're going to cover, we think it's going to be important to AI announcements and maybe some
[02:42:21.520 --> 02:42:27.760]   hardware announcements from Microsoft. So, we are going to take a chance and cover both keynotes.
[02:42:27.760 --> 02:42:35.600]   9am, Pacific, 12 noon Eastern on Tuesday and Wednesday. I'll be joined by Richard Campbell
[02:42:35.600 --> 02:42:39.840]   on the first. Richard Campbell and Paul Ther out on the second. So, I'll be back Tuesday
[02:42:39.840 --> 02:42:46.560]   bright and early, 9am for the build keynote. Thanks everybody for being here. We'll see you next
[02:42:46.560 --> 02:42:52.480]   time. Another Twit is in the can. Bye bye. Bye everybody. Bye bye.
[02:42:52.480 --> 02:43:01.360]   [Music]

