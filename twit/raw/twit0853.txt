;FFMETADATA1
title=Make It Cozy
artist=Leo Laporte, Shira Lazar, Alex Kantrowitz, Carolina Milanesi
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2021-12-13
track=853
language=English
genre=Podcast
comment=Worst 0-day in a decade, Amazon outage, Assange extradition, Birds Aren't Real
encoded_by=Uniblab 5.3
date=2021
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.560]   It's time for Twit this week in Tech.
[00:00:02.560 --> 00:00:07.680]   From the big technology podcast, Alex Cantorwits joins Carolina Milanesi from the heart of
[00:00:07.680 --> 00:00:11.320]   Tech and Chirilla's are COVID survivor.
[00:00:11.320 --> 00:00:12.800]   It's going to be a great show.
[00:00:12.800 --> 00:00:18.400]   We will talk about what some are calling the worst zero day in a decade.
[00:00:18.400 --> 00:00:22.400]   Amazon out for 12 hours.
[00:00:22.400 --> 00:00:27.080]   Those 23andMe spit tests and where they're what they're really going to be.
[00:00:27.080 --> 00:00:31.880]   And what do you do with Tesla's comically large windshield wiper?
[00:00:31.880 --> 00:00:37.080]   It's all coming up next on Twit.
[00:00:37.080 --> 00:00:41.160]   Podcasts you love from people you trust.
[00:00:41.160 --> 00:00:50.480]   This is Twit.
[00:00:50.480 --> 00:00:53.440]   This is Twit this week in Tech.
[00:00:53.440 --> 00:00:59.640]   It's about 853 recorded Sunday, December 12, 2021.
[00:00:59.640 --> 00:01:01.680]   Make it cozy.
[00:01:01.680 --> 00:01:04.880]   This week in Tech is brought to you by podium.
[00:01:04.880 --> 00:01:11.640]   Join more than 100,000 businesses already using podium to streamline their customer interactions.
[00:01:11.640 --> 00:01:17.000]   Get started for free at podium.com/twit or sign up for a paid podium account and get
[00:01:17.000 --> 00:01:20.080]   a free credit card reader restriction supply.
[00:01:20.080 --> 00:01:22.320]   And by our crowd.
[00:01:22.320 --> 00:01:29.000]   Our crowd helps accredited investors invest early in pre IPO companies alongside professional
[00:01:29.000 --> 00:01:30.760]   venture capitalists.
[00:01:30.760 --> 00:01:39.280]   Join the fastest growing venture capital investment community at our crowd.com/twits.
[00:01:39.280 --> 00:01:41.520]   And by Udacity.
[00:01:41.520 --> 00:01:46.880]   Udacity offers online education that's geared toward people looking to take their technology
[00:01:46.880 --> 00:01:48.160]   to the next level.
[00:01:48.160 --> 00:01:52.240]   Get the in demand tech skills you need to advance your career.
[00:01:52.240 --> 00:01:57.120]   Visit Udacity.com/twit to learn more.
[00:01:57.120 --> 00:02:00.400]   And by UserWay.org.
[00:02:00.400 --> 00:02:05.680]   UserWay ensures your website is accessible, ADA compliant, and helps your business avoid
[00:02:05.680 --> 00:02:07.880]   accessibility related lawsuits.
[00:02:07.880 --> 00:02:13.160]   The perfect way to showcase your brand's commitment to millions of people with disabilities.
[00:02:13.160 --> 00:02:16.440]   It's not only the right thing to do, it's also the law.
[00:02:16.440 --> 00:02:29.000]   Go to UserWay.org/twit for 30% off UserWay's AI powered accessibility solution.
[00:02:29.000 --> 00:02:30.000]   It's time for Twit.
[00:02:30.000 --> 00:02:34.960]   This week at Tech the show we cover the week's tech news with some of the most interesting
[00:02:34.960 --> 00:02:35.960]   people in tech.
[00:02:35.960 --> 00:02:37.480]   Carolina MilanesÃ© is back.
[00:02:37.480 --> 00:02:39.240]   We're great to see you.
[00:02:39.240 --> 00:02:41.280]   Founder of the heart of Tech.
[00:02:41.280 --> 00:02:43.160]   She's an analyst.
[00:02:43.160 --> 00:02:45.840]   What do you principally cover?
[00:02:45.840 --> 00:02:51.360]   I cover everything that is consumer tech and now it applies yes to the consumer market
[00:02:51.360 --> 00:02:56.440]   but also to enterprise so future work and how technology is shaping business.
[00:02:56.440 --> 00:02:58.520]   That's what we're talking about, boy.
[00:02:58.520 --> 00:03:01.920]   The future of work, whatever that might be.
[00:03:01.920 --> 00:03:06.880]   Also with this expert on big tech, he has the big technology podcast and newsletter.
[00:03:06.880 --> 00:03:08.720]   Alex Cantor with it's great to have you back.
[00:03:08.720 --> 00:03:09.720]   Thanks for joining us, Alex.
[00:03:09.720 --> 00:03:11.360]   It's great to be here.
[00:03:11.360 --> 00:03:12.360]   Always nice to chat with you.
[00:03:12.360 --> 00:03:13.360]   Yes.
[00:03:13.360 --> 00:03:15.960]   You're always welcome.
[00:03:15.960 --> 00:03:19.400]   You've picked a good topic to be an expert in big tech.
[00:03:19.400 --> 00:03:22.720]   I have a big topic these days.
[00:03:22.720 --> 00:03:29.600]   Do you think Jeff Bezos will ever invite you up in the new shepherd to see the earth from
[00:03:29.600 --> 00:03:31.200]   a distance?
[00:03:31.200 --> 00:03:33.720]   Only if he's planning to not return that one.
[00:03:33.720 --> 00:03:36.520]   That will be the one.
[00:03:36.520 --> 00:03:38.400]   Michael Strand went up yesterday.
[00:03:38.400 --> 00:03:39.720]   The GMA star.
[00:03:39.720 --> 00:03:41.720]   We'll talk about that later.
[00:03:41.720 --> 00:03:43.600]   Also with us, oh, I'm so glad she's here.
[00:03:43.600 --> 00:03:46.160]   We were worried, scheduled for last week.
[00:03:46.160 --> 00:03:47.880]   Sure, Lazar said, I can't.
[00:03:47.880 --> 00:03:50.840]   I've got COVID-19, but you're feeling better.
[00:03:50.840 --> 00:03:52.640]   Welcome, sure Lazar.
[00:03:52.640 --> 00:03:53.640]   Yes.
[00:03:53.640 --> 00:03:54.640]   I'm here.
[00:03:54.640 --> 00:03:55.640]   I have your members.
[00:03:55.640 --> 00:03:56.640]   Public appearance.
[00:03:56.640 --> 00:03:57.640]   This is it, huh?
[00:03:57.640 --> 00:03:58.640]   This is it.
[00:03:58.640 --> 00:03:59.640]   Wow.
[00:03:59.640 --> 00:04:00.640]   You got it, Leo, the exclusive.
[00:04:00.640 --> 00:04:02.920]   You and your SO, your boyfriend got it.
[00:04:02.920 --> 00:04:06.240]   To each side, boyfriend still, or you have to be your partner.
[00:04:06.240 --> 00:04:08.440]   I like it.
[00:04:08.440 --> 00:04:11.240]   You and your partner both got it at Thanksgiving.
[00:04:11.240 --> 00:04:14.320]   I don't say that too loud.
[00:04:14.320 --> 00:04:19.080]   My family was mad that I shared that on social media.
[00:04:19.080 --> 00:04:25.320]   One of the things Shira is known for is openness on social media.
[00:04:25.320 --> 00:04:27.040]   But I love that about you.
[00:04:27.040 --> 00:04:28.440]   I appreciate it.
[00:04:28.440 --> 00:04:30.240]   My family did not appreciate it.
[00:04:30.240 --> 00:04:33.520]   My family's always saying, why do you talk about us so much?
[00:04:33.520 --> 00:04:35.240]   But what else we got, right?
[00:04:35.240 --> 00:04:36.240]   Exactly.
[00:04:36.240 --> 00:04:37.240]   Sorry.
[00:04:37.240 --> 00:04:38.520]   We're all born with this.
[00:04:38.520 --> 00:04:41.240]   We were born together, so it's the way it is.
[00:04:41.240 --> 00:04:43.000]   It's like being married to a comic.
[00:04:43.000 --> 00:04:45.000]   You know, you're going to be in some jokes.
[00:04:45.000 --> 00:04:46.000]   Sorry.
[00:04:46.000 --> 00:04:47.000]   That's just the way it is.
[00:04:47.000 --> 00:04:48.000]   The way the cookie crumbles.
[00:04:48.000 --> 00:04:49.160]   It's the way the cookie crumbles.
[00:04:49.160 --> 00:04:50.160]   Anyway, it's great to see you.
[00:04:50.160 --> 00:04:51.160]   I'm glad you're feeling better.
[00:04:51.160 --> 00:04:54.520]   I was worried about you, but I'm glad to hear you.
[00:04:54.520 --> 00:04:55.520]   Yeah, I appreciate it.
[00:04:55.520 --> 00:04:57.520]   What's your guys' name?
[00:04:57.520 --> 00:04:58.520]   Chris.
[00:04:58.520 --> 00:04:59.520]   You and Chris are doing it.
[00:04:59.520 --> 00:05:02.120]   He also oddly enough, he doesn't like being on camera also.
[00:05:02.120 --> 00:05:03.120]   I know, you know.
[00:05:03.120 --> 00:05:04.120]   And he's around people that hate it.
[00:05:04.120 --> 00:05:06.240]   I noticed that because I'm watching your reels.
[00:05:06.240 --> 00:05:09.040]   Chris used to be a big part of them and then he disappeared and I asked Lisa.
[00:05:09.040 --> 00:05:10.480]   I said, are they still together?
[00:05:10.480 --> 00:05:13.360]   She said, yes, Chris just doesn't like to be in the...
[00:05:13.360 --> 00:05:14.360]   Oh, she knows.
[00:05:14.360 --> 00:05:16.160]   She came and you just talked to him.
[00:05:16.160 --> 00:05:17.400]   Yeah, she could tell.
[00:05:17.400 --> 00:05:18.400]   It's obvious.
[00:05:18.400 --> 00:05:22.280]   The last one we saw, he snuck up behind you and then it was never again.
[00:05:22.280 --> 00:05:23.280]   Exactly.
[00:05:23.280 --> 00:05:27.680]   Anyway, I'm glad you're both feeling a little bit better and you get to take some.
[00:05:27.680 --> 00:05:30.800]   Now so much needed time off for the holidays.
[00:05:30.800 --> 00:05:31.800]   Right.
[00:05:31.800 --> 00:05:32.800]   Yeah.
[00:05:32.800 --> 00:05:33.800]   Exactly.
[00:05:33.800 --> 00:05:41.920]   So first of all, thoughts and prayers out to the many people dislocated by horrible tornadoes
[00:05:41.920 --> 00:05:44.920]   in the southeast.
[00:05:44.920 --> 00:05:53.400]   Just it seems every year now we're reporting that and tragedy, including how many workers
[00:05:53.400 --> 00:05:57.080]   in the Amazon warehouse in Illinois, Alex, it was 10?
[00:05:57.080 --> 00:06:02.280]   It was a few dozen were injured and I think we're still counting how many have died.
[00:06:02.280 --> 00:06:04.560]   So so so horrible.
[00:06:04.560 --> 00:06:10.640]   Some criticism of Amazon there apparently because one of the things that warehouse workers cannot
[00:06:10.640 --> 00:06:12.840]   do is bring their phones inside the warehouse.
[00:06:12.840 --> 00:06:15.560]   They leave them behind.
[00:06:15.560 --> 00:06:18.720]   And why would that may have made a difference?
[00:06:18.720 --> 00:06:22.080]   Well I think that if you're allowing workers to have cell phones, if you're treating them
[00:06:22.080 --> 00:06:25.600]   like human beings and you trust them to do their job, even if they have a phone in their
[00:06:25.600 --> 00:06:30.400]   pocket, then there's a chance they might have been alerted to the extreme weather heading
[00:06:30.400 --> 00:06:31.400]   their way.
[00:06:31.400 --> 00:06:36.560]   And all indications are was that this caught them by surprise and may have even increased
[00:06:36.560 --> 00:06:38.440]   the death and injury toll.
[00:06:38.440 --> 00:06:42.880]   Now, Amazon, I understand the policy and principle, right?
[00:06:42.880 --> 00:06:46.320]   You want to have people focusing on the task.
[00:06:46.320 --> 00:06:50.640]   And but they monitor everybody so closely anyway.
[00:06:50.640 --> 00:06:55.160]   And so you're not going to have a tornado every day, but in the off chance that you
[00:06:55.160 --> 00:06:59.120]   do, you should have people be prepared for the extreme weather heading in their direction
[00:06:59.120 --> 00:07:00.400]   or whatever else might become.
[00:07:00.400 --> 00:07:03.480]   That's a really an excellent point.
[00:07:03.480 --> 00:07:07.640]   These days not having a phone in your pocket kind of disconnects you from the world oddly
[00:07:07.640 --> 00:07:08.640]   enough.
[00:07:08.640 --> 00:07:12.680]   And it is a little paternalistic of Amazon to say, well, we don't trust you.
[00:07:12.680 --> 00:07:16.280]   You're going to be playing words with friends while you're supposed to be picking and pulling.
[00:07:16.280 --> 00:07:19.440]   That's what people say who aren't to be trusted.
[00:07:19.440 --> 00:07:22.400]   They don't trust you because you should trust them.
[00:07:22.400 --> 00:07:23.400]   Interesting point.
[00:07:23.400 --> 00:07:25.480]   There's a little projection going on.
[00:07:25.480 --> 00:07:30.120]   You know, if I were working in the warehouse and I had a phone, I'd be what's happened.
[00:07:30.120 --> 00:07:31.240]   That's probably the case, sure.
[00:07:31.240 --> 00:07:33.720]   I think you nailed it.
[00:07:33.720 --> 00:07:34.720]   Yeah.
[00:07:34.720 --> 00:07:40.640]   Somebody, some of the chat was saying this has triangle shirt waste factory vibes.
[00:07:40.640 --> 00:07:45.520]   Here's the bottom line is if they're not going to let the phones in, they need to be double
[00:07:45.520 --> 00:07:49.440]   triple prepared for the case of a natural disaster, you know, potentially heading the
[00:07:49.440 --> 00:07:50.880]   way of the fulfillment centers.
[00:07:50.880 --> 00:07:51.880]   Yeah.
[00:07:51.880 --> 00:07:55.440]   And you know, Amazon is like so up on the data, they know every little movement that's going
[00:07:55.440 --> 00:07:56.840]   on in the factory.
[00:07:56.840 --> 00:08:00.520]   So if they don't have anything built in to prepare them for this, you know, they need
[00:08:00.520 --> 00:08:05.200]   to at least revisit those processes because, you know, we're not going to say there's
[00:08:05.200 --> 00:08:08.200]   butt in their hand on their hands, but we are going to say that, you know, they need
[00:08:08.200 --> 00:08:09.880]   to do better in situations like this.
[00:08:09.880 --> 00:08:10.880]   And that's obvious.
[00:08:10.880 --> 00:08:11.880]   Yeah.
[00:08:11.880 --> 00:08:18.400]   A number of GoFundMe's have been set up for the families of the six who have passed in
[00:08:18.400 --> 00:08:21.800]   that Amazon warehouse collapse in Edwardsville, Illinois.
[00:08:21.800 --> 00:08:25.160]   So very sad, very, very tragic.
[00:08:25.160 --> 00:08:28.400]   We're sorry to have to report that.
[00:08:28.400 --> 00:08:33.320]   There's also very bad news, security news going around.
[00:08:33.320 --> 00:08:38.440]   Some are calling this the worst security flaw in a decade, which in a decade has been full
[00:08:38.440 --> 00:08:41.280]   of nasty security fault laws.
[00:08:41.280 --> 00:08:44.720]   That seems like a bad thing.
[00:08:44.720 --> 00:08:55.720]   It's a zero day in a Java logging library, a log for J that allows about the worst possible
[00:08:55.720 --> 00:09:02.600]   scenario remote code execution by logging us carefully formed string.
[00:09:02.600 --> 00:09:10.920]   Now this log for J is very widely used.
[00:09:10.920 --> 00:09:17.920]   One of the things that springs up and it's kind of an interesting side note is how much
[00:09:17.920 --> 00:09:38.000]   big business relies on open source applications maintained by volunteers, which this is.
[00:09:38.000 --> 00:09:44.360]   They are working night and day, these volunteers, unpaid volunteers to fix this.
[00:09:44.360 --> 00:09:50.880]   It affects any version of Apache log for J from 2.0 to 2.141.
[00:09:50.880 --> 00:09:55.760]   2.15 has been released is a permanent fix.
[00:09:55.760 --> 00:09:57.160]   You can get it.
[00:09:57.160 --> 00:10:04.520]   And if you are running, I would hope if you're running Apache, Apache struts, you are not
[00:10:04.520 --> 00:10:07.000]   only aware of this, but you fixed it.
[00:10:07.000 --> 00:10:13.200]   And as far as I know, I don't know of any serious impact from this.
[00:10:13.200 --> 00:10:16.160]   It was caught and fixed very quickly.
[00:10:16.160 --> 00:10:27.120]   But a lot of infrastructure relies on a small number of unpaid volunteers in this world.
[00:10:27.120 --> 00:10:29.480]   And is this something we should fix?
[00:10:29.480 --> 00:10:31.880]   Carolina, what do you think?
[00:10:31.880 --> 00:10:37.000]   That's why shocking, I didn't know that, but given that somebody else had an outage this
[00:10:37.000 --> 00:10:43.920]   week and it took a little bit longer to fix, maybe Amazon should rely on unpaid volunteers
[00:10:43.920 --> 00:10:44.920]   too.
[00:10:44.920 --> 00:10:45.920]   I don't know.
[00:10:45.920 --> 00:10:52.640]   AWS also down and much longer than the vulnerability in 4.0 JS.
[00:10:52.640 --> 00:11:00.120]   This is the famous XKCD dependency cartoon, which shows all modern digital infrastructure
[00:11:00.120 --> 00:11:12.200]   and relying on a tiny little block, a project, some random person in Nebraska has been thanklessly
[00:11:12.200 --> 00:11:14.800]   maintaining since 2003.
[00:11:14.800 --> 00:11:21.000]   Unfortunately, we know now this is really more true than not.
[00:11:21.000 --> 00:11:23.760]   What about that Amazon outage?
[00:11:23.760 --> 00:11:26.240]   How long was AWS out?
[00:11:26.240 --> 00:11:30.160]   I think it was a couple of days, wasn't it?
[00:11:30.160 --> 00:11:34.360]   So it seemed like it was a lot longer than it should be.
[00:11:34.360 --> 00:11:40.320]   And everybody relies on AWS.
[00:11:40.320 --> 00:11:41.320]   Including Amazon.
[00:11:41.320 --> 00:11:46.120]   So a lot of business where we're struggling.
[00:11:46.120 --> 00:11:53.120]   The outage was explained on Friday hours long outage earlier this week that disrupted
[00:11:53.120 --> 00:11:57.080]   its retail business and third party online services.
[00:11:57.080 --> 00:12:00.640]   One of the complaints people had is that the status page is useless.
[00:12:00.640 --> 00:12:04.480]   The problems being so they're going to fix that.
[00:12:04.480 --> 00:12:07.000]   So at least if they're down, you'll know they're down.
[00:12:07.000 --> 00:12:13.040]   Problems began in US East 1 10 30 a.m. Eastern on Tuesday.
[00:12:13.040 --> 00:12:18.600]   Amazon said an automated activity to scale capacity of one of AWS's services hosted in
[00:12:18.600 --> 00:12:24.480]   the main AWS network triggered an unexpected behavior from a large number of clients inside
[00:12:24.480 --> 00:12:26.480]   the internal network.
[00:12:26.480 --> 00:12:30.520]   This reminds me of the Facebook flaw a few months ago.
[00:12:30.520 --> 00:12:38.000]   Several AWS tools suffered EC2, which provides virtual server capacity.
[00:12:38.000 --> 00:12:42.520]   Amazon engineers work to resolve issues, bring back services over the next several hours.
[00:12:42.520 --> 00:12:44.000]   It didn't bounce back fully.
[00:12:44.000 --> 00:12:48.440]   It went down to 10 30 a.m. didn't bounce back fully.
[00:12:48.440 --> 00:12:49.520]   It's 9 40 p.m.
[00:12:49.520 --> 00:12:51.240]   So a 12 hour and 10 minute out.
[00:12:51.240 --> 00:12:55.040]   You get like a gift certificate from them or like you have to pay less money that month.
[00:12:55.040 --> 00:12:56.040]   What happens?
[00:12:56.040 --> 00:13:00.400]   I get an Amazon gift card for $13.23.
[00:13:00.400 --> 00:13:05.720]   I mean, I'm excited you didn't even apologize, honestly.
[00:13:05.720 --> 00:13:10.400]   When these things happen, you really learn who is relying on AWS, which is basically
[00:13:10.400 --> 00:13:11.400]   everybody.
[00:13:11.400 --> 00:13:12.400]   I don't know if it affected us.
[00:13:12.400 --> 00:13:13.400]   We go through AWS.
[00:13:13.400 --> 00:13:15.000]   Disney Plus was down.
[00:13:15.000 --> 00:13:16.480]   Netflix was down.
[00:13:16.480 --> 00:13:22.000]   Ticketmaster, Roomba vacuum cleaners were offline.
[00:13:22.000 --> 00:13:24.160]   Amazon's ring security cameras were offline.
[00:13:24.160 --> 00:13:29.080]   There were smart cat litter boxes, app connected ceiling fans.
[00:13:29.080 --> 00:13:30.080]   The world fell apart.
[00:13:30.080 --> 00:13:32.080]   That's like what's worrisome.
[00:13:32.080 --> 00:13:35.560]   What if there were drones and everything and if they went offline, which was like getting
[00:13:35.560 --> 00:13:38.160]   into each other and yeah.
[00:13:38.160 --> 00:13:42.000]   Amazon's own retail operations as Carolina was saying, brought to a standstill in some
[00:13:42.000 --> 00:13:44.200]   parts of the US.
[00:13:44.200 --> 00:13:48.280]   Internal apps used by Amazon's warehouse and delivery force rely on AWS.
[00:13:48.280 --> 00:13:51.800]   So most of Tuesday employees were unable to scan packages.
[00:13:51.800 --> 00:13:56.720]   It's a bad time of year to not be able to access delivery routes and scan packages.
[00:13:56.720 --> 00:14:02.880]   Third party sellers couldn't manage customer orders.
[00:14:02.880 --> 00:14:07.120]   And yes, the status page was one of the things impacted.
[00:14:07.120 --> 00:14:15.960]   Oh, well, maybe we shouldn't attach the status page to AWS because of AWS is down.
[00:14:15.960 --> 00:14:17.560]   You don't know.
[00:14:17.560 --> 00:14:22.280]   So what they did is they put a global banner on the service health dashboard, which Amazon
[00:14:22.280 --> 00:14:27.920]   says we have since learned makes it difficult for some customers to find information.
[00:14:27.920 --> 00:14:32.080]   Customers couldn't create support cases for seven hours because yeah, the support tool
[00:14:32.080 --> 00:14:34.480]   also runs an AWS.
[00:14:34.480 --> 00:14:38.600]   Who's that little block again in that giant tower of blocks this time not maintained by
[00:14:38.600 --> 00:14:44.320]   some kid in Nebraska, but still kind of a little bit of a, you know, choke point, a
[00:14:44.320 --> 00:14:46.920]   weak link in the whole thing.
[00:14:46.920 --> 00:14:48.640]   There's nothing much to report.
[00:14:48.640 --> 00:14:52.480]   I mean, this is that's Amazon's explanation.
[00:14:52.480 --> 00:14:57.840]   It's fascinating to see how much this is though the Wall Street Journal headline Amazon outage
[00:14:57.840 --> 00:15:03.840]   disrupts lives, surprising people about their cloud dependency.
[00:15:03.840 --> 00:15:08.800]   And your light switches stop working because AWS is down.
[00:15:08.800 --> 00:15:11.440]   Oops.
[00:15:11.440 --> 00:15:15.560]   The best one is, and this is the lead for the Wall Street Journal.
[00:15:15.560 --> 00:15:20.760]   Kyle Lerner and his girlfriend since something was amiss when they came home Tuesday and
[00:15:20.760 --> 00:15:26.160]   found their two Persian Himalayan cats meowing nonstop.
[00:15:26.160 --> 00:15:32.800]   They were using an internet connected feeding machine to dispense kibble, but it was, it
[00:15:32.800 --> 00:15:35.480]   hadn't been out since the morning.
[00:15:35.480 --> 00:15:36.880]   This is the best quote.
[00:15:36.880 --> 00:15:41.440]   We had to manually give them food like in ancient times.
[00:15:41.440 --> 00:15:43.880]   That is amazing.
[00:15:43.880 --> 00:15:45.720]   What are we?
[00:15:45.720 --> 00:15:49.520]   Of course they're from California from Marina Del Rey.
[00:15:49.520 --> 00:15:52.120]   Marina Del Rey was like ancient times.
[00:15:52.120 --> 00:15:54.880]   I can't Steve Peters of Los Angeles.
[00:15:54.880 --> 00:16:00.520]   Also one of your neighbors couldn't tell his Roomba vacuum to clean up the blueberry muffin
[00:16:00.520 --> 00:16:02.840]   crumbs during breakfast.
[00:16:02.840 --> 00:16:05.440]   Oh my God.
[00:16:05.440 --> 00:16:06.720]   This is like an onion.
[00:16:06.720 --> 00:16:09.400]   This is a Wall Street Journal.
[00:16:09.400 --> 00:16:13.960]   He relies on an app on his phone to beckon the machine.
[00:16:13.960 --> 00:16:18.360]   I had to resort to getting a broom and dustpan.
[00:16:18.360 --> 00:16:20.120]   It was crazy.
[00:16:20.120 --> 00:16:21.200]   Said Mr. Peters.
[00:16:21.200 --> 00:16:25.640]   I'm reading from the Wall Street Journal.
[00:16:25.640 --> 00:16:30.960]   In St. Louis, losing access to Amazon's Alexa service made Mark Edelstein feel lonely and
[00:16:30.960 --> 00:16:32.040]   helpless.
[00:16:32.040 --> 00:16:33.040]   This is weird.
[00:16:33.040 --> 00:16:36.840]   People in like from generations come will look back at this either be like what was wrong
[00:16:36.840 --> 00:16:38.040]   or like I get it.
[00:16:38.040 --> 00:16:40.240]   This is like the beginning of the end.
[00:16:40.240 --> 00:16:46.080]   Here's a picture of Steve, the man with the crumbs, unable to activate his Roomba.
[00:16:46.080 --> 00:16:48.080]   Does he look?
[00:16:48.080 --> 00:16:53.720]   Think he has something a bit more wrong in his life than a Roomba.
[00:16:53.720 --> 00:16:57.000]   This is worse than what is worse than first world problems?
[00:16:57.000 --> 00:16:58.520]   Marina Del Rey problems.
[00:16:58.520 --> 00:17:02.920]   It is like Roomba problems.
[00:17:02.920 --> 00:17:08.160]   In fairness though, it does make you think about the reliance on cloud overall.
[00:17:08.160 --> 00:17:13.480]   If you're thinking about driverless cars and smart cities or drones falling out of
[00:17:13.480 --> 00:17:14.480]   this guy.
[00:17:14.480 --> 00:17:18.120]   You're that we might have going forward.
[00:17:18.120 --> 00:17:25.320]   If you think about we were out of power, somebody unfortunately lost control and took out one
[00:17:25.320 --> 00:17:28.440]   of the posts outside our house.
[00:17:28.440 --> 00:17:31.160]   We're out of power for four hours.
[00:17:31.160 --> 00:17:33.520]   Nothing in the house was working.
[00:17:33.520 --> 00:17:38.560]   No lights because everything is smart but also because there was no Wi-Fi.
[00:17:38.560 --> 00:17:43.120]   Yes, like the ancient times I was just using my phone.
[00:17:43.120 --> 00:17:44.520]   Like an animal.
[00:17:44.520 --> 00:17:45.520]   You have drills.
[00:17:45.520 --> 00:17:47.480]   Should we have drills around this?
[00:17:47.480 --> 00:17:52.040]   Okay, for a few hours everyone, everything is going to shut down.
[00:17:52.040 --> 00:17:53.040]   Just prepare yourself.
[00:17:53.040 --> 00:17:54.040]   See what it's like.
[00:17:54.040 --> 00:17:56.760]   When it happens, we're like, okay, we got this.
[00:17:56.760 --> 00:17:57.760]   We have a plan.
[00:17:57.760 --> 00:18:03.520]   I'm surprised you didn't hear my teenager scream because it's about obviously impactive
[00:18:03.520 --> 00:18:04.520]   our gaming.
[00:18:04.520 --> 00:18:05.520]   Yeah.
[00:18:05.520 --> 00:18:06.520]   In the afternoon.
[00:18:06.520 --> 00:18:08.320]   Yeah.
[00:18:08.320 --> 00:18:13.600]   The Wall Street Journal I think is taking a little too much glee in this because of course
[00:18:13.600 --> 00:18:15.280]   is a mainstream media company.
[00:18:15.280 --> 00:18:19.000]   They're probably not thrilled with the internet in general anyway.
[00:18:19.000 --> 00:18:25.240]   The outage for Samantha Sherhogg to open blinds in her home in the Tampa Bay area of Florida,
[00:18:25.240 --> 00:18:28.560]   she couldn't instruct Alexa to turn on the lights.
[00:18:28.560 --> 00:18:34.160]   She otherwise has to move furniture to reach the main lights which are living.
[00:18:34.160 --> 00:18:35.680]   I can't stop.
[00:18:35.680 --> 00:18:39.440]   So like an animal, she had to open the blinds.
[00:18:39.440 --> 00:18:41.400]   Life is difficult.
[00:18:41.400 --> 00:18:46.400]   Life is difficult.
[00:18:46.400 --> 00:18:47.400]   That's true.
[00:18:47.400 --> 00:18:48.400]   Yeah.
[00:18:48.400 --> 00:18:54.200]   Herline voice from an accessibility perspective and that can be distressing.
[00:18:54.200 --> 00:18:55.200]   Yes.
[00:18:55.200 --> 00:19:02.160]   Especially if you're a Persian Himalayan cat who cannot find food in your cat feeder.
[00:19:02.160 --> 00:19:03.640]   I love the journal.
[00:19:03.640 --> 00:19:07.520]   I mean really way too much glee in this posting these pictures.
[00:19:07.520 --> 00:19:11.000]   They're just really enjoying this.
[00:19:11.000 --> 00:19:12.000]   I disagree.
[00:19:12.000 --> 00:19:14.440]   That's why we feed our map.
[00:19:14.440 --> 00:19:15.440]   Yeah.
[00:19:15.440 --> 00:19:16.440]   Go ahead.
[00:19:16.440 --> 00:19:19.640]   That's why you feed your animals with food?
[00:19:19.640 --> 00:19:20.640]   No, by hand.
[00:19:20.640 --> 00:19:22.480]   Because we have to bounce steps.
[00:19:22.480 --> 00:19:28.200]   I wouldn't rely on something that might miss work and there'll be dinner.
[00:19:28.200 --> 00:19:32.640]   Here's a typical of our crowd and their response.
[00:19:32.640 --> 00:19:36.600]   When the Amazon powered lights in his home wouldn't work, he checked the website down
[00:19:36.600 --> 00:19:37.600]   detector.
[00:19:37.600 --> 00:19:41.280]   Of course the lights won't work.
[00:19:41.280 --> 00:19:44.960]   First thing you do go online and see if something's down.
[00:19:44.960 --> 00:19:46.960]   It was a sigh of relief, he said.
[00:19:46.960 --> 00:19:50.400]   But the experience also made him realize how much he relies on AWS.
[00:19:50.400 --> 00:19:52.320]   What were you saying Alex?
[00:19:52.320 --> 00:19:56.240]   Is this something to worry about or is this just, you know?
[00:19:56.240 --> 00:20:01.000]   Well, first of all, I'll say that it's somewhat miraculous that it wasn't worse.
[00:20:01.000 --> 00:20:07.280]   So this was actually better failure than that's 2018 AWS failure.
[00:20:07.280 --> 00:20:08.280]   It was not as bad.
[00:20:08.280 --> 00:20:15.160]   And this was a hit to US East One, which is part of the oldest region of AWS.
[00:20:15.160 --> 00:20:18.880]   So if you think about the way that these things are built on top of each other, you
[00:20:18.880 --> 00:20:25.440]   would imagine that if you're, a lot of your foundation is in this East One, the first
[00:20:25.440 --> 00:20:29.960]   region that AWS built in and it goes down, you'd be in trouble.
[00:20:29.960 --> 00:20:35.400]   And like actually, maybe it's a testament to AWS that most of this stuff actually stayed
[00:20:35.400 --> 00:20:37.600]   up and it was only down for a few hours.
[00:20:37.600 --> 00:20:39.640]   It could have been much more disastrous.
[00:20:39.640 --> 00:20:42.560]   Now, speaking about this Wall Street Journal story, I think it's a lovely story.
[00:20:42.560 --> 00:20:43.800]   I'm going to disagree with you on...
[00:20:43.800 --> 00:20:45.680]   It's pretty funny.
[00:20:45.680 --> 00:20:46.680]   I think it's great.
[00:20:46.680 --> 00:20:51.040]   I think they really illustrate how much, you know, how do you tell a story like a server
[00:20:51.040 --> 00:20:55.320]   farm, you know, going down and away to human and accessible?
[00:20:55.320 --> 00:21:00.160]   And I happen to think, you know, getting people on record talking about how like they
[00:21:00.160 --> 00:21:02.360]   had to feed their pets like it's ancient times.
[00:21:02.360 --> 00:21:06.400]   I give them the poll, that's a beautiful story.
[00:21:06.400 --> 00:21:08.040]   But let's be honest, you know what happened.
[00:21:08.040 --> 00:21:11.720]   The reporter just went to Instagram, searched for people having problems and took their
[00:21:11.720 --> 00:21:12.720]   pictures.
[00:21:12.720 --> 00:21:16.680]   I mean, it wasn't exactly, you know, you didn't have to hit the street, you know, in
[00:21:16.680 --> 00:21:21.440]   your fedora and with your new, new, new, previous notebook and interview people to find this.
[00:21:21.440 --> 00:21:22.440]   I think they probably...
[00:21:22.440 --> 00:21:23.440]   Instagram is the street today.
[00:21:23.440 --> 00:21:24.440]   It is the street.
[00:21:24.440 --> 00:21:25.440]   It's the new street.
[00:21:25.440 --> 00:21:26.440]   Yeah.
[00:21:26.440 --> 00:21:31.120]   I'll put a few other down on the, what was remaining up on the internet and made a go of
[00:21:31.120 --> 00:21:32.120]   it.
[00:21:32.120 --> 00:21:40.720]   Instagram, Adam Moserri, the CEO, testified in front of a Senate panel this week, examining
[00:21:40.720 --> 00:21:44.280]   social media's negative effects on young people.
[00:21:44.280 --> 00:21:50.320]   Of course, Francis Hogg and the whistleblower at Facebook, among all the things she revealed,
[00:21:50.320 --> 00:21:55.560]   revealed that Instagram had internal studies that showed that young women had a lower
[00:21:55.560 --> 00:21:59.960]   self opinion as a result of using Instagram, that it made them feel bad, in some cases,
[00:21:59.960 --> 00:22:01.520]   even suicidal.
[00:22:01.520 --> 00:22:10.080]   Senators, of course, took this opportunity to show anger and ask sharp questions.
[00:22:10.080 --> 00:22:12.680]   Moserri defended the company's conduct.
[00:22:12.680 --> 00:22:14.840]   He said, "We've got safety measures in place."
[00:22:14.840 --> 00:22:23.120]   And in fact, we don't think Instagram is addictive to young people and most people are
[00:22:23.120 --> 00:22:26.080]   not harmed by Instagram, I guess.
[00:22:26.080 --> 00:22:31.480]   There's so many twisted parts of this one that the government cares about, female rights.
[00:22:31.480 --> 00:22:33.000]   Yeah, isn't that a joke?
[00:22:33.000 --> 00:22:34.000]   Yeah.
[00:22:34.000 --> 00:22:36.000]   Think of the children.
[00:22:36.000 --> 00:22:37.640]   All right.
[00:22:37.640 --> 00:22:42.720]   Instagram is also like, they're like the ultimate gas lighters in this situation.
[00:22:42.720 --> 00:22:44.160]   It's like, what?
[00:22:44.160 --> 00:22:45.680]   You're feeling crappy about yourself?
[00:22:45.680 --> 00:22:47.640]   No, that's just in your head.
[00:22:47.640 --> 00:22:48.640]   Yeah.
[00:22:48.640 --> 00:22:49.640]   Come on.
[00:22:49.640 --> 00:22:50.640]   Yeah.
[00:22:50.640 --> 00:22:51.640]   Yeah.
[00:22:51.640 --> 00:22:52.640]   Good point.
[00:22:52.640 --> 00:22:54.640]   Dick Blumenthal and others created Finstas.
[00:22:54.640 --> 00:23:00.360]   Remember, a couple of months ago, fake Instagram accounts posing as 13 year old girls, which
[00:23:00.360 --> 00:23:05.520]   on the surface sounds kind of creepy, but they were, to be honest, not what you really
[00:23:05.520 --> 00:23:10.000]   want your senator to be doing, but the idea was to see what kinds of things they saw.
[00:23:10.000 --> 00:23:19.280]   And they certainly were fed a diet, if you will, of bad food suggestions, bad ideas.
[00:23:19.280 --> 00:23:22.360]   I just wish they did this for every other thing we're fighting for.
[00:23:22.360 --> 00:23:27.560]   Like, let's put ourselves in the shoes of people asking for the change.
[00:23:27.560 --> 00:23:28.560]   Yeah.
[00:23:28.560 --> 00:23:29.560]   Like, what does that take?
[00:23:29.560 --> 00:23:32.040]   No, I mean, there's a legitimate issue.
[00:23:32.040 --> 00:23:36.280]   It just sucks at Instagram until they accept this somehow.
[00:23:36.280 --> 00:23:37.280]   Then they'll make a difference.
[00:23:37.280 --> 00:23:42.800]   If they're going to be in denial of it, because obviously they're liable and there's legal
[00:23:42.800 --> 00:23:46.160]   issues around this, they're going to still fight this, but then we're not going to see
[00:23:46.160 --> 00:23:47.160]   anything change.
[00:23:47.160 --> 00:23:51.200]   Instagram also says, well, look, we adhere to COPPA.
[00:23:51.200 --> 00:23:55.640]   We don't let people in a 13 create, we create accounts course.
[00:23:55.640 --> 00:23:57.760]   Good luck doing that.
[00:23:57.760 --> 00:24:04.840]   We ban specific features for kids under 16, but parents let kids on.
[00:24:04.840 --> 00:24:06.840]   You're an avid Instagrammer, sure.
[00:24:06.840 --> 00:24:12.000]   That's how I keep up with you.
[00:24:12.000 --> 00:24:13.640]   On balance is Instagram a positive?
[00:24:13.640 --> 00:24:15.640]   It is in your life, I would say, yes.
[00:24:15.640 --> 00:24:17.120]   Two sides of the coin.
[00:24:17.120 --> 00:24:24.120]   I think that a lot of my anxiety, there is self-worth wrapped in it, including when your
[00:24:24.120 --> 00:24:28.880]   work is focused around a platform.
[00:24:28.880 --> 00:24:31.040]   You couldn't not be an Instagram user.
[00:24:31.040 --> 00:24:32.040]   You have to.
[00:24:32.040 --> 00:24:33.040]   Yeah.
[00:24:33.040 --> 00:24:36.320]   Well, and I feel like there's a lot of freedom, there's a lot of creativity.
[00:24:36.320 --> 00:24:38.400]   I connect to a lot of people and meet a lot of people.
[00:24:38.400 --> 00:24:43.200]   There's empowerment, but there are two sides to that coin, including younger and younger
[00:24:43.200 --> 00:24:48.520]   when people are wrapping their women or wrapping their self-worth around their profiles, or
[00:24:48.520 --> 00:24:52.240]   even if they're not building their profiles, but they're looking at Instagram and the feed
[00:24:52.240 --> 00:24:56.400]   and comparing themselves, yeah, that can be very harmful.
[00:24:56.400 --> 00:24:59.040]   I don't know what the alternative is, though.
[00:24:59.040 --> 00:25:05.360]   One, make sure younger and younger people are not on the platform, and that I don't
[00:25:05.360 --> 00:25:09.360]   know how you create a feed and filter when that is the world around you.
[00:25:09.360 --> 00:25:11.600]   The world around you has this.
[00:25:11.600 --> 00:25:14.080]   It's just now an amplified version of that.
[00:25:14.080 --> 00:25:15.080]   Right.
[00:25:15.080 --> 00:25:16.080]   Well, exactly.
[00:25:16.080 --> 00:25:20.760]   In fact, Golly in our chatroom says teenagers have always had depression and body image
[00:25:20.760 --> 00:25:24.960]   issues that didn't start with Instagram.
[00:25:24.960 --> 00:25:29.240]   I think really the big problem with Instagram is it allows other people, really, and this
[00:25:29.240 --> 00:25:30.240]   is the problem.
[00:25:30.240 --> 00:25:31.240]   I don't think it's ads.
[00:25:31.240 --> 00:25:36.080]   I don't think it's that other people curate the pictures and the life that they project
[00:25:36.080 --> 00:25:39.560]   on Instagram, and there's no way you can live up to that.
[00:25:39.560 --> 00:25:45.360]   I always tell my daughter, don't judge your insides by other people's outsides because
[00:25:45.360 --> 00:25:47.320]   what you're seeing is presentational.
[00:25:47.320 --> 00:25:50.200]   It's not what they, you don't know how they feel.
[00:25:50.200 --> 00:25:52.160]   They probably feel exactly like you do.
[00:25:52.160 --> 00:25:53.160]   Yes.
[00:25:53.160 --> 00:25:57.440]   Most of the times they do, but you can't force people to share those sides of them.
[00:25:57.440 --> 00:25:59.040]   They'll share what they want to share.
[00:25:59.040 --> 00:26:00.040]   Yeah.
[00:26:00.040 --> 00:26:01.040]   Right.
[00:26:01.040 --> 00:26:03.320]   And so it's like how much is Instagram part of that versus just this is where our society
[00:26:03.320 --> 00:26:04.320]   is moving.
[00:26:04.320 --> 00:26:05.320]   It is.
[00:26:05.320 --> 00:26:06.320]   I don't think it's Instagram's fault.
[00:26:06.320 --> 00:26:07.320]   Exactly.
[00:26:07.320 --> 00:26:08.320]   It's going to happen.
[00:26:08.320 --> 00:26:13.840]   But I do think that the more that they're there for creators and for users and create
[00:26:13.840 --> 00:26:20.160]   initiatives and they do acknowledge that there is a problem, then at least that helps.
[00:26:20.160 --> 00:26:26.560]   Like other platforms have acknowledged like YouTube, I mean, you could say TikTok a bit.
[00:26:26.560 --> 00:26:30.920]   I mean, they're all part of the problem, but the more Instagram fights against this, the
[00:26:30.920 --> 00:26:36.560]   more they just seem delusional and continuing to propagate it.
[00:26:36.560 --> 00:26:37.560]   Right.
[00:26:37.560 --> 00:26:42.320]   And I remember, I'm sure you do too, Carolina, the days of Twiggy and Kate Moss, these super
[00:26:42.320 --> 00:26:46.880]   thin supermodels, which caused eating, you know, eating disorders to ripple throughout
[00:26:46.880 --> 00:26:47.880]   the world.
[00:26:47.880 --> 00:26:52.280]   And I thought, Oh, well, that's the standard for beauty is, you know, concentration camp
[00:26:52.280 --> 00:26:53.280]   thin.
[00:26:53.280 --> 00:26:55.880]   You have teenage daughters, right?
[00:26:55.880 --> 00:26:58.280]   I have a child.
[00:26:58.280 --> 00:27:06.360]   And they were born a girl and now they're gender fluid and that brings a whole different
[00:27:06.360 --> 00:27:16.080]   layer of, I guess, complication in as much as where, you know, they see themselves who
[00:27:16.080 --> 00:27:18.000]   they're safe, themselves with.
[00:27:18.000 --> 00:27:20.400]   Do you need to be in Instagram?
[00:27:20.400 --> 00:27:29.600]   So they had an account that had was a private account when they turned 13.
[00:27:29.600 --> 00:27:33.040]   They could not be least interesting now.
[00:27:33.040 --> 00:27:35.440]   Interested and they actually want.
[00:27:35.440 --> 00:27:43.480]   Well, but what is fascinating to me is why and I do think that she is is 100% spot on
[00:27:43.480 --> 00:27:49.240]   when she says that the Instagram should start with acknowledging there is a problem because
[00:27:49.240 --> 00:27:54.120]   they're spending more time and resources fighting that versus acknowledging and then
[00:27:54.120 --> 00:27:56.640]   trying to solve it.
[00:27:56.640 --> 00:28:04.240]   The reason why my kid is no longer interested is because it feels like TV.
[00:28:04.240 --> 00:28:06.720]   It feels unreal.
[00:28:06.720 --> 00:28:12.480]   And what they actually want is a subscription to discord because they want a community.
[00:28:12.480 --> 00:28:19.880]   They want people that they want to communicate with that are similarly money did have share
[00:28:19.880 --> 00:28:21.960]   interest and all of that.
[00:28:21.960 --> 00:28:29.840]   And it is fascinating to me how I think the younger generation in a ways is more savvy
[00:28:29.840 --> 00:28:32.560]   to understand what's real and what isn't.
[00:28:32.560 --> 00:28:34.680]   But at the same time, it's true.
[00:28:34.680 --> 00:28:41.480]   Depression and problem with self-image and weight and so forth has always been there.
[00:28:41.480 --> 00:28:47.880]   There's one thing to see in your school, your class, your friends is one other thing to see
[00:28:47.880 --> 00:28:52.680]   models and actresses because that's the idea of beauty and that is still there.
[00:28:52.680 --> 00:28:56.000]   All of that is still there plus Instagram, right?
[00:28:56.000 --> 00:29:03.000]   Because the idea of beauty is still the fin and the perfect and it's still mostly white
[00:29:03.000 --> 00:29:06.480]   and blonde and all of that, right?
[00:29:06.480 --> 00:29:11.000]   Versus representing the diverse world that we live in.
[00:29:11.000 --> 00:29:18.600]   So that's the part where I think my child being mixed race and gender fluid could not
[00:29:18.600 --> 00:29:24.200]   actually find what they were looking for from an Instagram perspective.
[00:29:24.200 --> 00:29:25.920]   I do think the world might be changing.
[00:29:25.920 --> 00:29:30.720]   You remember in the spring Unilever, they make Dove and a lot of brands of soap and
[00:29:30.720 --> 00:29:36.400]   so forth said we're not going to use the word normal anymore in any of our products
[00:29:36.400 --> 00:29:37.400]   or advertising.
[00:29:37.400 --> 00:29:39.600]   Boy, it's normal.
[00:29:39.600 --> 00:29:45.720]   Because there is no, to say normal is that this is normative, this is what everybody
[00:29:45.720 --> 00:29:47.200]   should be like.
[00:29:47.200 --> 00:29:51.600]   And I think that's recognizing the truth.
[00:29:51.600 --> 00:29:54.680]   And I think that's a, so I think there is more awareness of that.
[00:29:54.680 --> 00:29:56.440]   I hope there is.
[00:29:56.440 --> 00:29:57.840]   And I completely agree with you.
[00:29:57.840 --> 00:30:00.240]   Instagram needn't fight it.
[00:30:00.240 --> 00:30:05.520]   This is an opportunity to make a difference, to help fix it.
[00:30:05.520 --> 00:30:06.520]   So yeah, I don't like it.
[00:30:06.520 --> 00:30:11.520]   But certainly it's not, oh, we're going to give advice as to many hours you should spend
[00:30:11.520 --> 00:30:13.680]   and we encourage you to take a break.
[00:30:13.680 --> 00:30:14.680]   Really?
[00:30:14.680 --> 00:30:15.760]   TikTok does that too.
[00:30:15.760 --> 00:30:18.080]   I hit that every time on TikTok.
[00:30:18.080 --> 00:30:24.040]   There's a every time three in the morning, I'm scrolling, doom scrolling forever.
[00:30:24.040 --> 00:30:27.000]   And finally guy comes on says, you know, you really, I swear to God, I don't know.
[00:30:27.000 --> 00:30:28.000]   Have you ever seen this?
[00:30:28.000 --> 00:30:29.000]   Sure.
[00:30:29.000 --> 00:30:30.000]   You must have.
[00:30:30.000 --> 00:30:31.000]   Guy comes on.
[00:30:31.000 --> 00:30:32.000]   I have never seen this.
[00:30:32.000 --> 00:30:33.000]   Oh yeah.
[00:30:33.000 --> 00:30:34.080]   One of the tiktoks comes up and it's a guy saying, you know, you really ought to get something
[00:30:34.080 --> 00:30:35.080]   to drink.
[00:30:35.080 --> 00:30:36.080]   Take a break.
[00:30:36.080 --> 00:30:38.480]   Go walk, move your body, move your body.
[00:30:38.480 --> 00:30:41.040]   Walk away from the tiktok buddy.
[00:30:41.040 --> 00:30:47.560]   But by then I've already gone 300 miles down the well and there's still coming out Instagram.
[00:30:47.560 --> 00:30:48.560]   Go ahead, Alex.
[00:30:48.560 --> 00:30:51.800]   I would just say that there's an important difference between the apps that we haven't
[00:30:51.800 --> 00:30:56.560]   covered in this discussion, which is something that came out in Facebook's own research where
[00:30:56.560 --> 00:31:00.880]   Facebook is not like tiktok and that people come to Facebook and Instagram in particular
[00:31:00.880 --> 00:31:02.640]   to compare themselves against others.
[00:31:02.640 --> 00:31:05.560]   Oh, where's the tiktok for entertainment?
[00:31:05.560 --> 00:31:06.560]   Yeah.
[00:31:06.560 --> 00:31:08.120]   Twitter for news.
[00:31:08.120 --> 00:31:12.440]   And Facebook knows that this comparison part of the app is actually a real problem.
[00:31:12.440 --> 00:31:13.800]   It's what they say in their own documents.
[00:31:13.800 --> 00:31:16.760]   And I think it should have been a focus in the hearing.
[00:31:16.760 --> 00:31:20.640]   But of course it wasn't because, you know, when it comes to legislators, I don't think
[00:31:20.640 --> 00:31:23.760]   there's, you know, anyone less effective than Richard Blumenthal.
[00:31:23.760 --> 00:31:24.760]   Right.
[00:31:24.760 --> 00:31:26.600]   Doesn't really be very interested in doing anything.
[00:31:26.600 --> 00:31:27.600]   Well, nor am I.
[00:31:27.600 --> 00:31:32.720]   I mean, I'm, I've been down this road long enough to not compare myself to people on Facebook
[00:31:32.720 --> 00:31:35.840]   or Instagram or tiktok.
[00:31:35.840 --> 00:31:41.480]   But yeah, if you're 13 or 14, you're trying to figure out what life is all about.
[00:31:41.480 --> 00:31:43.880]   I can't imagine growing up with that.
[00:31:43.880 --> 00:31:46.200]   Those signals constantly flooding in on you.
[00:31:46.200 --> 00:31:47.200]   Absolutely.
[00:31:47.200 --> 00:31:55.440]   But in fairness, there is a societal part to this because you are at that age starting
[00:31:55.440 --> 00:31:59.720]   from school, you're encouraged to compare yourself to others.
[00:31:59.720 --> 00:32:00.720]   That's right.
[00:32:00.720 --> 00:32:04.720]   You know, I'm not obviously comparing schools with Instagram, but I'm saying from really
[00:32:04.720 --> 00:32:10.240]   from a broader societal perspective, I think it's not it's not over.
[00:32:10.240 --> 00:32:11.800]   It's the peer group.
[00:32:11.800 --> 00:32:13.600]   It's peer pressure.
[00:32:13.600 --> 00:32:14.600]   You should be like us.
[00:32:14.600 --> 00:32:15.640]   You should be like the cool.
[00:32:15.640 --> 00:32:16.640]   Those are the cool girls.
[00:32:16.640 --> 00:32:19.280]   Why don't you look like them?
[00:32:19.280 --> 00:32:20.280]   Schools try sometimes.
[00:32:20.280 --> 00:32:21.880]   Some schools, good schools try to fight that.
[00:32:21.880 --> 00:32:23.880]   But I think that that's you're right, Carolyn.
[00:32:23.880 --> 00:32:24.880]   I don't know.
[00:32:24.880 --> 00:32:30.160]   I do think that is the education system too, where, you know, you have a score and you
[00:32:30.160 --> 00:32:32.360]   need to fit into some parameters.
[00:32:32.360 --> 00:32:33.360]   Sure.
[00:32:33.360 --> 00:32:39.240]   And you know, they're outliers either because you're behind or you're, you know, you're
[00:32:39.240 --> 00:32:47.160]   ahead of others and you're getting into that still, you know, what is considered normal
[00:32:47.160 --> 00:32:50.400]   or average versus not.
[00:32:50.400 --> 00:32:54.120]   So I, you know, I has been fascinating.
[00:32:54.120 --> 00:32:59.080]   One of the things that we did as well over the past few years was to switch from school
[00:32:59.080 --> 00:33:07.240]   to homeschooling, spending a year almost de-schooling and in terms of, you know, asking questions
[00:33:07.240 --> 00:33:12.640]   and feeling that you can fail and you don't have to get it right that the process that
[00:33:12.640 --> 00:33:19.320]   you go through in working something out, whether it's math or science or whatever, is as important
[00:33:19.320 --> 00:33:23.520]   as getting the final and right answer, right?
[00:33:23.520 --> 00:33:27.280]   You know, it's quite fascinating.
[00:33:27.280 --> 00:33:33.480]   So actually this is something that COVID has in a way brought up.
[00:33:33.480 --> 00:33:38.320]   Certainly in the workplace, and I think our schools are very much like workplaces in the
[00:33:38.320 --> 00:33:45.320]   sense that they have an industrial era mentality of you come in, we get you to fit into the
[00:33:45.320 --> 00:33:49.680]   mole and we crank out people that fit into society.
[00:33:49.680 --> 00:33:56.920]   And it's all about getting you, you know, mushing down the sharp bits so that you fit.
[00:33:56.920 --> 00:34:02.680]   And that industrial era mentality, whether it's in work or in schooling, was a little bit challenged,
[00:34:02.680 --> 00:34:07.800]   I think, during the pandemic, when kids had to go home, they had to do Zoom, they weren't
[00:34:07.800 --> 00:34:14.360]   surrounded by their peers and the teachers were no longer really able to squish the sharp
[00:34:14.360 --> 00:34:17.200]   points out quite as much.
[00:34:17.200 --> 00:34:20.040]   Offices had to let workers go home and work from home.
[00:34:20.040 --> 00:34:24.320]   And now as we start to emerge from COVID, and I'm sure you might say we are not emerging
[00:34:24.320 --> 00:34:28.480]   at, but as we hope we start to emerge from COVID, I think the world is going to be a very
[00:34:28.480 --> 00:34:29.480]   different place.
[00:34:29.480 --> 00:34:33.480]   And then maybe schooling, they'll take an opportunity for schooling to rethink how it
[00:34:33.480 --> 00:34:34.480]   works.
[00:34:34.480 --> 00:34:42.360]   I know a lot of kids are still zooming to school.
[00:34:42.360 --> 00:34:44.000]   So here's an opportunity.
[00:34:44.000 --> 00:34:46.760]   Alex, do you see that in...
[00:34:46.760 --> 00:34:51.280]   You see, you must see that in tech companies kind of reassessing post COVID.
[00:34:51.280 --> 00:34:56.320]   Yeah, well, I think that we all know that the hybrid workplace is going to be the way that
[00:34:56.320 --> 00:35:00.240]   we move forward from this, and we're never going to go back to five days in the office
[00:35:00.240 --> 00:35:03.040]   mandatory for everyone inside every company.
[00:35:03.040 --> 00:35:04.600]   I think that's over.
[00:35:04.600 --> 00:35:07.280]   I think it's pretty good, honestly.
[00:35:07.280 --> 00:35:10.960]   We've now realized that there's a balance where we can look at what's happened with
[00:35:10.960 --> 00:35:14.440]   the economy since we went into lockdown.
[00:35:14.440 --> 00:35:16.320]   It's been an amazing run.
[00:35:16.320 --> 00:35:19.800]   And of course, some of that has to do with federal stimulus, but you can't tell me that
[00:35:19.800 --> 00:35:23.640]   productivity in the economy is slowed down in a noticeable way.
[00:35:23.640 --> 00:35:25.520]   If anything, it seems like it's the opposite.
[00:35:25.520 --> 00:35:30.360]   So we should give workers the opportunity to spend a couple of days at home and a couple
[00:35:30.360 --> 00:35:34.320]   of days in the office, if that's their decision, or maybe you trust people to spend every day
[00:35:34.320 --> 00:35:35.320]   at home.
[00:35:35.320 --> 00:35:36.320]   Yeah.
[00:35:36.320 --> 00:35:39.600]   And we're starting to figure out the ways that we're going to make this work.
[00:35:39.600 --> 00:35:43.880]   For instance, I was just on a Zoom meeting where we had me and then somebody else remote
[00:35:43.880 --> 00:35:46.280]   and then two people in the office together.
[00:35:46.280 --> 00:35:51.120]   And they kind of sat side by side on their own laptop, so they each get their own Zoom
[00:35:51.120 --> 00:35:55.000]   bubble and then me and your little guy had our own Zoom bubble.
[00:35:55.000 --> 00:35:56.000]   And was it weird?
[00:35:56.000 --> 00:35:59.080]   Yeah, it was a little weird, especially for them, but it worked.
[00:35:59.080 --> 00:36:00.080]   We're doing it too.
[00:36:00.080 --> 00:36:01.080]   It's hybrid.
[00:36:01.080 --> 00:36:02.080]   Yeah, a lot of it.
[00:36:02.080 --> 00:36:03.080]   We're doing it too.
[00:36:03.080 --> 00:36:09.120]   Instagram does say they're going to bring back a chronological feed, not right away.
[00:36:09.120 --> 00:36:14.920]   Now I don't understand what's so hard about a chronological feed.
[00:36:14.920 --> 00:36:19.480]   Adam said, the company has been working on a version of the feature for months.
[00:36:19.480 --> 00:36:24.960]   See to me, it's like the hard thing is a non chronological feed.
[00:36:24.960 --> 00:36:31.800]   Chronological means in order, but not only that, we go ahead, Alex, and then I started
[00:36:31.800 --> 00:36:33.320]   with the chronological feed.
[00:36:33.320 --> 00:36:34.320]   Yeah, exactly.
[00:36:34.320 --> 00:36:37.960]   It's like, what exactly are you developing that's going to take?
[00:36:37.960 --> 00:36:39.920]   Were you spending months on this?
[00:36:39.920 --> 00:36:41.320]   You already had it.
[00:36:41.320 --> 00:36:46.480]   I really, here's what my Siri said in a blog post in June.
[00:36:46.480 --> 00:36:51.520]   The problem with a chronological feed, the problem is it's impossible for most people
[00:36:51.520 --> 00:36:55.880]   to see everything, let alone the posts they cared about.
[00:36:55.880 --> 00:37:00.200]   So we in our infinite wisdom are going to tell you what posts you care about.
[00:37:00.200 --> 00:37:02.480]   Make sure you see those first.
[00:37:02.480 --> 00:37:03.760]   Shira, you had something to...
[00:37:03.760 --> 00:37:07.040]   Yeah, I was just going to say, why don't they just have a choice?
[00:37:07.040 --> 00:37:08.040]   Well, that's...
[00:37:08.040 --> 00:37:09.040]   You have the choice of seeing chronological.
[00:37:09.040 --> 00:37:10.040]   I think that's the idea.
[00:37:10.040 --> 00:37:11.040]   That's the idea.
[00:37:11.040 --> 00:37:12.040]   Yeah.
[00:37:12.040 --> 00:37:13.040]   Curated.
[00:37:13.040 --> 00:37:18.040]   I mean, the issue with the chronological too, there will always be a reason to complain
[00:37:18.040 --> 00:37:22.400]   because it is cool that if you're posting in your creator, people could discover it
[00:37:22.400 --> 00:37:23.640]   a few days later.
[00:37:23.640 --> 00:37:27.960]   There is a feeling like people are just going to start hyper posting, right?
[00:37:27.960 --> 00:37:28.960]   Just to be in front of people.
[00:37:28.960 --> 00:37:29.960]   Oh, that's not true.
[00:37:29.960 --> 00:37:33.480]   So it'll be interesting to see how our behavior changes in that way.
[00:37:33.480 --> 00:37:35.400]   But then you just unfollow.
[00:37:35.400 --> 00:37:37.400]   True, true.
[00:37:37.400 --> 00:37:39.200]   Yeah, I mean, that is true.
[00:37:39.200 --> 00:37:45.080]   So I just think it makes it easier for people to discover stuff from other people.
[00:37:45.080 --> 00:37:48.200]   I feel like Twitter does a decent job at that.
[00:37:48.200 --> 00:37:50.640]   They're not chronological either.
[00:37:50.640 --> 00:37:51.640]   Are they not?
[00:37:51.640 --> 00:37:53.400]   I feel like I go on Twitter really?
[00:37:53.400 --> 00:37:57.480]   Yeah, well, that's a kind of a matter for debate.
[00:37:57.480 --> 00:37:59.160]   I think there's semi-cous though.
[00:37:59.160 --> 00:38:00.160]   There's semi-cous though.
[00:38:00.160 --> 00:38:02.160]   There are algorithmic.
[00:38:02.160 --> 00:38:06.280]   They definitely built through that feed with an algorithm and you can choose, but I bet
[00:38:06.280 --> 00:38:08.880]   most users don't even know that that option to choose exists.
[00:38:08.880 --> 00:38:09.880]   Yeah, that's true.
[00:38:09.880 --> 00:38:11.480]   You will still actually use it.
[00:38:11.480 --> 00:38:16.800]   So there was actually a big uproar about this in 2015, I believe, when they said they
[00:38:16.800 --> 00:38:21.080]   would introduce it because they started off like Instagram, pure reverse crown.
[00:38:21.080 --> 00:38:24.880]   And people like, you know, said that Twitter's live and there shouldn't be an algorithm.
[00:38:24.880 --> 00:38:28.280]   And I actually broke the story that they were going to put an algorithm within the week.
[00:38:28.280 --> 00:38:29.280]   Yeah.
[00:38:29.280 --> 00:38:35.880]   And Jack denied it and about a million people over that weekend tweeted hashtag RIP Twitter.
[00:38:35.880 --> 00:38:39.760]   But I think that if you're user of the platform, you know, now that it's way better with an
[00:38:39.760 --> 00:38:41.120]   algorithm than without.
[00:38:41.120 --> 00:38:42.360]   So I give them credit.
[00:38:42.360 --> 00:38:43.720]   It's made the service better.
[00:38:43.720 --> 00:38:44.720]   Same with Instagram.
[00:38:44.720 --> 00:38:45.720]   I'm pro algorithm.
[00:38:45.720 --> 00:38:46.720]   You're pro.
[00:38:46.720 --> 00:38:50.120]   I'm anti algorithm because I think algorithm is what gets us in all this trouble.
[00:38:50.120 --> 00:38:54.880]   That when people join Facebook or Instagram, they follow the people they want to follow,
[00:38:54.880 --> 00:38:59.240]   mostly family and friends, and they want to see everything that person posts.
[00:38:59.240 --> 00:39:01.600]   But Facebook doesn't show you that.
[00:39:01.600 --> 00:39:04.200]   My mom could, my mom doesn't post anymore.
[00:39:04.200 --> 00:39:12.320]   But let's say some relative, my sister, I maybe see half of what she sees because, you know,
[00:39:12.320 --> 00:39:14.400]   oh, it's too much.
[00:39:14.400 --> 00:39:15.800]   You don't want to see all of your sisters.
[00:39:15.800 --> 00:39:16.800]   Yes, I do.
[00:39:16.800 --> 00:39:22.160]   That's why I joined algorithms are what gets you in trouble with extremism.
[00:39:22.160 --> 00:39:26.960]   Algorithms tells YouTube to take you down the rabbit hole of killing on.
[00:39:26.960 --> 00:39:29.280]   I think the algorithms are the problems.
[00:39:29.280 --> 00:39:30.280]   Talk.
[00:39:30.280 --> 00:39:31.280]   But I've been saying that for a while.
[00:39:31.280 --> 00:39:33.080]   Talk me out of that because you're pro algorithm.
[00:39:33.080 --> 00:39:35.480]   Yeah, it's the way you design the algorithm.
[00:39:35.480 --> 00:39:39.880]   So I would definitely want something showing me the stuff that I think is most relevant.
[00:39:39.880 --> 00:39:44.480]   I've tried both the Facebook and the Twitter feed on chronological reverse cron and then
[00:39:44.480 --> 00:39:46.760]   algorithm and I prefer the algorithm.
[00:39:46.760 --> 00:39:50.360]   But it does get into trouble sometimes.
[00:39:50.360 --> 00:39:52.360]   I don't deny that.
[00:39:52.360 --> 00:39:55.400]   Well, the problem is an algorithm which works completely automatically, right?
[00:39:55.400 --> 00:39:57.240]   I mean, this is not a human thinking.
[00:39:57.240 --> 00:40:03.040]   And by the way, generally, I think an algorithm is going to optimize for engagement.
[00:40:03.040 --> 00:40:04.840]   Well, you spent more time on that post.
[00:40:04.840 --> 00:40:06.680]   You want to see more of that.
[00:40:06.680 --> 00:40:09.280]   And while that's good for business.
[00:40:09.280 --> 00:40:15.680]   So here's it's why I get a lot of bikini videos on TV.
[00:40:15.680 --> 00:40:16.680]   OMG.
[00:40:16.680 --> 00:40:18.600]   So here's what we're getting.
[00:40:18.600 --> 00:40:19.600]   We're getting back to you.
[00:40:19.600 --> 00:40:20.600]   I thought that I want to.
[00:40:20.600 --> 00:40:22.400]   Shira, I want to explain that.
[00:40:22.400 --> 00:40:24.000]   It's not that I want to.
[00:40:24.000 --> 00:40:26.000]   Right now you're just getting involved.
[00:40:26.000 --> 00:40:27.480]   You told me this yourself.
[00:40:27.480 --> 00:40:30.880]   If there is a bikini video, I'm a guy.
[00:40:30.880 --> 00:40:31.880]   I'm going to slow down.
[00:40:31.880 --> 00:40:32.880]   I don't want to.
[00:40:32.880 --> 00:40:33.880]   And it's not what I want to see more of.
[00:40:33.880 --> 00:40:35.720]   But I go, oh, I don't know.
[00:40:35.720 --> 00:40:36.880]   And then I move on.
[00:40:36.880 --> 00:40:39.640]   You have to actively on TikTok say, no, no, no.
[00:40:39.640 --> 00:40:43.240]   Because if I look at that, no, I'll see more of it.
[00:40:43.240 --> 00:40:45.640]   You have to in effect manipulate the algorithm.
[00:40:45.640 --> 00:40:46.640]   Yeah.
[00:40:46.640 --> 00:40:48.960]   Yeah, that's why I get all these cards.
[00:40:48.960 --> 00:40:52.840]   You know, I get these tarot card, like new moon things.
[00:40:52.840 --> 00:40:54.040]   I'm like, holy crap.
[00:40:54.040 --> 00:40:55.520]   Because they know you're a hippie.
[00:40:55.520 --> 00:40:58.360]   Every psychic is speaking to me for something.
[00:40:58.360 --> 00:41:00.080]   Anyway, Alex, sorry.
[00:41:00.080 --> 00:41:01.080]   Yeah.
[00:41:01.080 --> 00:41:03.720]   Well, we've only done a couple of years of social media algorithm.
[00:41:03.720 --> 00:41:07.720]   Like, if you think about where we are, where Twitter did it and Instagram did it, this
[00:41:07.720 --> 00:41:10.640]   all started in 2015, 2016.
[00:41:10.640 --> 00:41:12.320]   So we're only six years in.
[00:41:12.320 --> 00:41:17.680]   So I think that we went from these pure reverse chronological feeds to algorithmic feeds.
[00:41:17.680 --> 00:41:21.040]   Now what we're going to go to is a future where you have the opportunity to toggle back
[00:41:21.040 --> 00:41:26.120]   and forth, but also pretty importantly, the opportunity for you to tell them what type
[00:41:26.120 --> 00:41:27.120]   of feed you want.
[00:41:27.120 --> 00:41:28.120]   Yeah.
[00:41:28.120 --> 00:41:30.920]   Facebook's even working on something where you can tell it what mood you want it to put
[00:41:30.920 --> 00:41:31.920]   you in.
[00:41:31.920 --> 00:41:33.800]   You know, you want it more light or do you want it reflected?
[00:41:33.800 --> 00:41:37.720]   That's an acknowledgement that Facebook manipulates your mood.
[00:41:37.720 --> 00:41:38.720]   Absolutely.
[00:41:38.720 --> 00:41:39.880]   It definitely can manipulate your mood.
[00:41:39.880 --> 00:41:43.080]   But the better thing is that it just gives you an opportunity to tune it yourself or
[00:41:43.080 --> 00:41:44.080]   spring and database.
[00:41:44.080 --> 00:41:45.080]   Also, that's dangerous.
[00:41:45.080 --> 00:41:46.080]   Yeah.
[00:41:46.080 --> 00:41:47.080]   That's dangerous.
[00:41:47.080 --> 00:41:54.760]   Yeah, because then it's what the algorithm assumes is happy with the algorithm is just
[00:41:54.760 --> 00:42:04.000]   sad. And then so you're typically cutting out a lot of content from LGBTQ or BIPOC communities
[00:42:04.000 --> 00:42:07.240]   or stuff that you need to see to be informed.
[00:42:07.240 --> 00:42:11.440]   It's like a lot of times the algorithm puts content like that or individuals in those
[00:42:11.440 --> 00:42:12.440]   communities.
[00:42:12.440 --> 00:42:13.440]   It enforces a filter bubble.
[00:42:13.440 --> 00:42:15.440]   It puts you in a box.
[00:42:15.440 --> 00:42:16.440]   Exactly.
[00:42:16.440 --> 00:42:17.440]   Exactly.
[00:42:17.440 --> 00:42:21.080]   That's like the danger of this, like, which is what they really need to figure out.
[00:42:21.080 --> 00:42:24.120]   Like, how can you be customized?
[00:42:24.120 --> 00:42:28.400]   How can you give people what they want while not filtering out things that people need
[00:42:28.400 --> 00:42:30.520]   to see and get to see, right?
[00:42:30.520 --> 00:42:34.120]   And the voices and communities that matter as well in all of this.
[00:42:34.120 --> 00:42:35.120]   Yeah.
[00:42:35.120 --> 00:42:39.560]   And I'll grant you, Alex, if algorithms worked perfectly, I'd be pro algorithm too.
[00:42:39.560 --> 00:42:40.560]   Yeah.
[00:42:40.560 --> 00:42:42.560]   The notion of an algorithm is good.
[00:42:42.560 --> 00:42:43.560]   Okay.
[00:42:43.560 --> 00:42:44.560]   Yeah.
[00:42:44.560 --> 00:42:48.240]   I think it's a work in progress, but I really do like the fact that they're going to let
[00:42:48.240 --> 00:42:52.360]   you tune these feeds to the type of experience that you want.
[00:42:52.360 --> 00:42:56.360]   And I do think it's a good thing to give people who use these platforms agency over
[00:42:56.360 --> 00:42:57.360]   their experience.
[00:42:57.360 --> 00:42:58.680]   And I just found the post.
[00:42:58.680 --> 00:43:02.560]   So what Facebook is working on and this is still in development, but it came out in
[00:43:02.560 --> 00:43:03.560]   the Hagen leaks.
[00:43:03.560 --> 00:43:05.800]   So there's four moods that you could pick.
[00:43:05.800 --> 00:43:09.160]   Recharge, feel good, throwback, and cozy.
[00:43:09.160 --> 00:43:10.160]   I don't know.
[00:43:10.160 --> 00:43:12.160]   I mean, like, you know, these things are...
[00:43:12.160 --> 00:43:13.160]   Why is cozy?
[00:43:13.160 --> 00:43:14.160]   Cozy.
[00:43:14.160 --> 00:43:15.160]   I feel cozy.
[00:43:15.160 --> 00:43:16.160]   Yeah.
[00:43:16.160 --> 00:43:17.160]   I'm into it.
[00:43:17.160 --> 00:43:18.160]   Let's try it out.
[00:43:18.160 --> 00:43:19.160]   It's going to happen.
[00:43:19.160 --> 00:43:21.960]   So like, there's going to be something that comes out like a...
[00:43:21.960 --> 00:43:27.400]   Like, okay, what's the genre that everyone's looking at on all these platforms and everyone's
[00:43:27.400 --> 00:43:29.000]   going to start playing that genre?
[00:43:29.000 --> 00:43:33.320]   I'm going to start making my Instagram feed a throwback feed because I see that throwback
[00:43:33.320 --> 00:43:35.320]   content does really well.
[00:43:35.320 --> 00:43:38.880]   You know, I think that it's really fascinating.
[00:43:38.880 --> 00:43:44.240]   Like how do you stop people from then feeding into that type of content that they know works,
[00:43:44.240 --> 00:43:45.240]   right?
[00:43:45.240 --> 00:43:49.520]   Like, and then we're back into the same scenario as we were before.
[00:43:49.520 --> 00:43:54.520]   Yeah, I'm just glad that those options aren't like outrage, you know, fake nude.
[00:43:54.520 --> 00:43:56.280]   Because that's honestly really...
[00:43:56.280 --> 00:43:58.280]   Can I pick the cancel experience?
[00:43:58.280 --> 00:44:00.880]   That could be a throwback, you know.
[00:44:00.880 --> 00:44:03.000]   But that's actually what's happening, right?
[00:44:03.000 --> 00:44:05.600]   That's the real thing that's happening.
[00:44:05.600 --> 00:44:08.480]   So maybe it is better if you say, look, not outrage this week.
[00:44:08.480 --> 00:44:09.480]   Just cozy.
[00:44:09.480 --> 00:44:10.480]   Cozy.
[00:44:10.480 --> 00:44:13.640]   I mean, if you follow a lot of cat videos, that's what you're going to get.
[00:44:13.640 --> 00:44:17.960]   Sorry, this should be the new political slogan for anyone trying to change the internet.
[00:44:17.960 --> 00:44:18.960]   Make it cozy.
[00:44:18.960 --> 00:44:20.520]   Make it cozy.
[00:44:20.520 --> 00:44:22.600]   Cut me some slides.
[00:44:22.600 --> 00:44:24.240]   I'm glad you're feeling well sure.
[00:44:24.240 --> 00:44:25.240]   It's great to have you.
[00:44:25.240 --> 00:44:26.480]   Shira Lazar is here.
[00:44:26.480 --> 00:44:28.680]   Peace inside.live.
[00:44:28.680 --> 00:44:33.800]   If you want to find some peace, and of course, the founder of What's Trending still going
[00:44:33.800 --> 00:44:34.800]   strong, whatstrending.com.
[00:44:34.800 --> 00:44:35.800]   Yep.
[00:44:35.800 --> 00:44:36.800]   It's great to have you.
[00:44:36.800 --> 00:44:37.800]   There you go.
[00:44:37.800 --> 00:44:38.800]   Thank you.
[00:44:38.800 --> 00:44:41.440]   She's been a maven since the beginning of the internet.
[00:44:41.440 --> 00:44:43.200]   That's been a while.
[00:44:43.200 --> 00:44:48.080]   Yeah, you're still quite young, but yeah, you're kind of a founder.
[00:44:48.080 --> 00:44:50.880]   I think you're of internet phenomenon.
[00:44:50.880 --> 00:44:51.880]   So so nice.
[00:44:51.880 --> 00:44:52.880]   I appreciate that.
[00:44:52.880 --> 00:44:56.760]   I'm glad you survived the big sea, the new big sea.
[00:44:56.760 --> 00:44:58.280]   Alex Cantrowitz also here.
[00:44:58.280 --> 00:45:00.040]   He is all about big technology.
[00:45:00.040 --> 00:45:06.120]   His book always, Day One, is it all about Amazon or is it about more than just Amazon?
[00:45:06.120 --> 00:45:10.880]   It's a chapter each on Amazon, Apple, Facebook, Google, Microsoft.
[00:45:10.880 --> 00:45:16.560]   But I do look at the culture of Amazon in particular as a foundation for what companies
[00:45:16.560 --> 00:45:19.560]   can do well when they're trying to reinvent themselves.
[00:45:19.560 --> 00:45:20.560]   Yeah.
[00:45:20.560 --> 00:45:26.200]   How the tech titans plan to stay on top from Penguin Random House, of course, available
[00:45:26.200 --> 00:45:27.200]   on Amazon.
[00:45:27.200 --> 00:45:29.080]   Is there an audiobook version of that?
[00:45:29.080 --> 00:45:30.760]   Yeah, I read it.
[00:45:30.760 --> 00:45:33.840]   It was the last thing I did before going into lockdown.
[00:45:33.840 --> 00:45:34.840]   Nice.
[00:45:34.840 --> 00:45:39.440]   So I was able to lose my voice for a couple of days and it was fine.
[00:45:39.440 --> 00:45:41.040]   We'll get it on Amazon then.
[00:45:41.040 --> 00:45:42.840]   I always I like to listen.
[00:45:42.840 --> 00:45:44.680]   I like to listen.
[00:45:44.680 --> 00:45:46.080]   Also with us Carolina Milanese.
[00:45:46.080 --> 00:45:48.520]   Am I saying it?
[00:45:48.520 --> 00:45:49.520]   Is it Milanesee?
[00:45:49.520 --> 00:45:50.520]   Do you like that?
[00:45:50.520 --> 00:45:51.520]   Milanesee.
[00:45:51.520 --> 00:45:52.520]   Yes.
[00:45:52.520 --> 00:45:53.520]   Milanesee.
[00:45:53.520 --> 00:45:54.880]   A woman of Milan.
[00:45:54.880 --> 00:45:56.680]   That's right.
[00:45:56.680 --> 00:45:57.680]   Is that right?
[00:45:57.680 --> 00:45:58.880]   Am I making that up?
[00:45:58.880 --> 00:45:59.880]   Close.
[00:45:59.880 --> 00:46:00.880]   No, you're not.
[00:46:00.880 --> 00:46:01.880]   It's very close to Milan.
[00:46:01.880 --> 00:46:02.880]   Yes.
[00:46:02.880 --> 00:46:03.880]   Mir Milan.
[00:46:03.880 --> 00:46:04.880]   Close to Milan.
[00:46:04.880 --> 00:46:08.640]   It's a matter of the heart of tech and an analyst and it's always great to have you
[00:46:08.640 --> 00:46:10.480]   on as well.
[00:46:10.480 --> 00:46:12.640]   Our show today brought to you by podium.
[00:46:12.640 --> 00:46:15.840]   Talk about the new world order.
[00:46:15.840 --> 00:46:19.040]   It's funny in the, I don't know, maybe the 80s or the 90s.
[00:46:19.040 --> 00:46:23.320]   If you were a business, I guess it was started really in the late 90s.
[00:46:23.320 --> 00:46:24.560]   You had to have a website, right?
[00:46:24.560 --> 00:46:27.120]   Before that, you had to have an answering machine.
[00:46:27.120 --> 00:46:28.800]   Then you had to have a website.
[00:46:28.800 --> 00:46:31.320]   Then you had to have an email address.
[00:46:31.320 --> 00:46:34.080]   You know, slowly you had to have an app.
[00:46:34.080 --> 00:46:38.360]   Nowadays, thanks to, I think probably thanks to the pandemic, we've kind of got used to
[00:46:38.360 --> 00:46:43.240]   this idea of frictionless interaction with businesses over text messages.
[00:46:43.240 --> 00:46:46.640]   You probably noticed this.
[00:46:46.640 --> 00:46:51.920]   Maybe you went to a dentist and as you left, they sent you your appointment for the next
[00:46:51.920 --> 00:46:53.440]   time you'd be there.
[00:46:53.440 --> 00:46:57.960]   Or even sometimes they said, "Leave a review on Yelp or Google.
[00:46:57.960 --> 00:46:59.800]   Let people know.
[00:46:59.800 --> 00:47:02.160]   Maybe you got a bill from a company.
[00:47:02.160 --> 00:47:07.920]   Maybe you've used texting to try to reach a company so you don't have to play phone tag.
[00:47:07.920 --> 00:47:11.320]   This is the way business gets done these days.
[00:47:11.320 --> 00:47:16.280]   More often than not, when you're experiencing that, you're experiencing podium.
[00:47:16.280 --> 00:47:19.720]   Potium is the way to stay in touch with customers.
[00:47:19.720 --> 00:47:23.760]   Local businesses everywhere, all over the country, are turning to podium because they
[00:47:23.760 --> 00:47:27.480]   make every interaction with a customer as easy as sending a text.
[00:47:27.480 --> 00:47:31.240]   Your employees love it because one inbox means they don't have to go all over the place
[00:47:31.240 --> 00:47:32.720]   to help customers.
[00:47:32.720 --> 00:47:33.720]   It makes it faster.
[00:47:33.720 --> 00:47:35.440]   It makes it better.
[00:47:35.440 --> 00:47:38.360]   Everything that makes your business great can be done faster.
[00:47:38.360 --> 00:47:40.000]   It's not just really a better way to communicate.
[00:47:40.000 --> 00:47:41.000]   It's more.
[00:47:41.000 --> 00:47:42.120]   It's a better way to do everything.
[00:47:42.120 --> 00:47:43.560]   You can get reviews from podium.
[00:47:43.560 --> 00:47:45.640]   You can collect payments from podium.
[00:47:45.640 --> 00:47:51.240]   I get a, and this is very effective, I'm sad to say, there's a local ice cream store
[00:47:51.240 --> 00:47:54.720]   that uses podium and every once in a while they say, "We haven't seen you in a while.
[00:47:54.720 --> 00:47:55.800]   Here's a coupon.
[00:47:55.800 --> 00:47:57.160]   Come on in and get some ice cream."
[00:47:57.160 --> 00:47:58.280]   It works.
[00:47:58.280 --> 00:47:59.280]   It works.
[00:47:59.280 --> 00:48:02.880]   podium makes it all as easy as pressing send.
[00:48:02.880 --> 00:48:04.000]   You're going to free up more time.
[00:48:04.000 --> 00:48:06.040]   You're going to grow your business or you get more done.
[00:48:06.040 --> 00:48:11.640]   But more importantly, most importantly, your customers are going to love it.
[00:48:11.640 --> 00:48:16.960]   With podium, you'll also close more deals with customers before the competition even
[00:48:16.960 --> 00:48:18.360]   has a chance to call them back.
[00:48:18.360 --> 00:48:19.760]   It's just frictionless.
[00:48:19.760 --> 00:48:21.800]   It's what we've come to expect.
[00:48:21.800 --> 00:48:23.560]   Join more than 100,000 businesses.
[00:48:23.560 --> 00:48:28.560]   They already use podium to streamline their customer interactions.
[00:48:28.560 --> 00:48:35.040]   You can try it for free at podimpotium.com/twit.
[00:48:35.040 --> 00:48:38.840]   If you sign up for a paid podium account, you'll get a free credit card reader.
[00:48:38.840 --> 00:48:40.800]   Restrictions apply podium.com/twit.
[00:48:40.800 --> 00:48:45.800]   There's a great demo on there that explains how it all works.
[00:48:45.800 --> 00:48:48.000]   It is easy to get started.
[00:48:48.000 --> 00:48:51.840]   You could have it done by the end of the show and your customers will just love it.
[00:48:51.840 --> 00:48:53.840]   I know I do.
[00:48:53.840 --> 00:49:03.280]   Join us on extradition in his future.
[00:49:03.280 --> 00:49:08.040]   Apparently, he had just had a stroke in jail because of the tension of this.
[00:49:08.040 --> 00:49:14.960]   The U.S. appealed the U.K. court's decision not to extradite Assange from the U.K.
[00:49:14.960 --> 00:49:15.960]   The U.K.
[00:49:15.960 --> 00:49:20.880]   High Court has now granted that appeal against an earlier refusal by a U.K.
[00:49:20.880 --> 00:49:27.360]   judge who said we're not going to extradite him on mental health grounds.
[00:49:27.360 --> 00:49:28.360]   The U.K.
[00:49:28.360 --> 00:49:31.680]   Secretary of State has the final decision.
[00:49:31.680 --> 00:49:37.000]   I am really curious what you all think of this.
[00:49:37.000 --> 00:49:39.320]   At first, Julian Assange was a hero.
[00:49:39.320 --> 00:49:46.480]   WikiLeaks exposed the U.S. using drone strikes to kill journalists and civilians.
[00:49:46.480 --> 00:49:50.280]   That was in WikiLeaks.
[00:49:50.280 --> 00:49:56.120]   But as time went by, WikiLeaks started to look more and more like an arm of Russian foreign
[00:49:56.120 --> 00:49:58.080]   policy.
[00:49:58.080 --> 00:50:02.800]   On the other hand, Assange always said, "They're going to come after me because governments
[00:50:02.800 --> 00:50:07.680]   can't tolerate the information that we post on WikiLeaks and they're going to find me and
[00:50:07.680 --> 00:50:10.320]   they're going to get me on some trumped up charge."
[00:50:10.320 --> 00:50:16.240]   The U.S. does want to put them on trial for conspiracy to hack and computer misuse, which
[00:50:16.240 --> 00:50:21.960]   for whatever you think about Assange and WikiLeaks is in my opinion exactly that trumped up charge.
[00:50:21.960 --> 00:50:27.200]   They're saying Assange encouraged and taught a hacker how to hack over chat.
[00:50:27.200 --> 00:50:29.640]   I don't think that's demonstrable.
[00:50:29.640 --> 00:50:34.440]   He'll face 18 counts connected with obtaining and disclosing defense and national security
[00:50:34.440 --> 00:50:41.600]   material primarily in 2009 and 2010.
[00:50:41.600 --> 00:50:43.720]   I'm curious what you all think of this.
[00:50:43.720 --> 00:50:49.840]   It's funny, as time goes by and Edward Snowden is in Russia and I'm starting to think maybe
[00:50:49.840 --> 00:50:56.440]   Snowden isn't the golden haired boy we thought he was, Assange is now going to be sent to
[00:50:56.440 --> 00:51:00.640]   trial in the U.S. and I'm thinking maybe he's not quite the bad guy we thought he was.
[00:51:00.640 --> 00:51:02.920]   Alex, what do you think?
[00:51:02.920 --> 00:51:05.920]   I think this is one of the cases where you have to wait for the trial.
[00:51:05.920 --> 00:51:09.040]   I mean, there's a lot of evidence that the U.S. government is going to have that we haven't
[00:51:09.040 --> 00:51:10.800]   seen yet.
[00:51:10.800 --> 00:51:15.400]   Is it possible that Julian Assange did everything above board?
[00:51:15.400 --> 00:51:21.200]   Yes, and if so, then he shouldn't be put in jail, obviously not.
[00:51:21.200 --> 00:51:27.880]   But if there are demonstrable moments where he did break U.S. law and end up hacking stuff,
[00:51:27.880 --> 00:51:29.480]   I think you have to enforce those rules.
[00:51:29.480 --> 00:51:31.240]   Now, what am I rooting for?
[00:51:31.240 --> 00:51:34.400]   I'm rooting for Julian Assange to have not done any of that stuff because I do think
[00:51:34.400 --> 00:51:37.680]   that what he did was important and in public interest.
[00:51:37.680 --> 00:51:41.560]   He's being charged because of the Chelsea Manning leaks.
[00:51:41.560 --> 00:51:48.160]   He's being accused, at least in part for kind of encouraging hacking and stuff like that
[00:51:48.160 --> 00:51:50.720]   in these chat conversations.
[00:51:50.720 --> 00:51:52.960]   But these were the airstrikes that I was talking about.
[00:51:52.960 --> 00:51:56.680]   I think the information, you know, when you speak truth to power, power doesn't like it
[00:51:56.680 --> 00:51:57.680]   so much.
[00:51:57.680 --> 00:51:58.680]   So, yeah, I think-
[00:51:58.680 --> 00:51:59.680]   Generally, we don't want to-
[00:51:59.680 --> 00:52:00.680]   Yeah, we don't want to-
[00:52:00.680 --> 00:52:01.680]   Can he get a fair trial in the U.S.?
[00:52:01.680 --> 00:52:03.520]   I guess that's the real question.
[00:52:03.520 --> 00:52:05.920]   Well, can anyone get a fair trial in the U.S.?
[00:52:05.920 --> 00:52:06.920]   No.
[00:52:06.920 --> 00:52:07.920]   That was an important part.
[00:52:07.920 --> 00:52:10.960]   But I'd look, I generally- I don't know.
[00:52:10.960 --> 00:52:13.640]   I was going to say I trust our judicial system.
[00:52:13.640 --> 00:52:16.360]   There are parts of it that aren't trustworthy, but it's a good system.
[00:52:16.360 --> 00:52:21.320]   I mean, some of the tenants and it's until proven guilty, people need to be proving stuff
[00:52:21.320 --> 00:52:22.720]   beyond a reasonable doubt.
[00:52:22.720 --> 00:52:25.400]   Like, let's see what it looks like in court.
[00:52:25.400 --> 00:52:29.360]   But again, like the activities that Julian Assange did, I'd be rooting for it all to
[00:52:29.360 --> 00:52:33.720]   be above board and for him to be able to walk because ultimately we need people like that
[00:52:33.720 --> 00:52:38.800]   that are going to be able to stick their neck out when it comes to publishing damning information
[00:52:38.800 --> 00:52:42.200]   that governments don't want out there, but is in the public interest, which is exactly
[00:52:42.200 --> 00:52:43.200]   what he did.
[00:52:43.200 --> 00:52:49.520]   I feel like we just can't- we don't know and may never know what really transpired.
[00:52:49.520 --> 00:52:51.720]   And I don't know if a trial- I hope- I agree with you.
[00:52:51.720 --> 00:52:59.080]   I would love it if a trial would reveal that and give a jury the chance to decide that
[00:52:59.080 --> 00:53:01.320]   based on real facts, not manipulated facts.
[00:53:01.320 --> 00:53:04.120]   I'm going to cross my fingers on that as well.
[00:53:04.120 --> 00:53:09.640]   If the US tries him, which they are, it's going to have the US interests at stake, right?
[00:53:09.640 --> 00:53:17.840]   It's like, I don't know if I trust the judicial system in that way because they know the repercussions
[00:53:17.840 --> 00:53:22.520]   of then proving that it was okay and he's in the right.
[00:53:22.520 --> 00:53:26.840]   I feel like there's a lot of fear of what that means in the future.
[00:53:26.840 --> 00:53:27.840]   Yeah.
[00:53:27.840 --> 00:53:32.480]   There's lots of evidence that Julian Assange is not a good person.
[00:53:32.480 --> 00:53:38.200]   Although he was accused of sexual assault in Sweden, those charges have been dropped.
[00:53:38.200 --> 00:53:39.200]   I just-
[00:53:39.200 --> 00:53:43.840]   He could have done something that was ethical in certain ways, and then there's other parts
[00:53:43.840 --> 00:53:45.160]   of him that are unethical.
[00:53:45.160 --> 00:53:48.680]   I think that two truths in that way can exist.
[00:53:48.680 --> 00:53:51.840]   I think that there's a difference between being a whistleblower and helping the common
[00:53:51.840 --> 00:53:58.160]   good and hacking just for hacking sake to create chaos.
[00:53:58.160 --> 00:53:59.160]   Right?
[00:53:59.160 --> 00:54:02.560]   I also don't see how it was a hack.
[00:54:02.560 --> 00:54:06.960]   I mean, Chelsea Manning had access to this do her job.
[00:54:06.960 --> 00:54:10.640]   If that's the case, then it's not a hack.
[00:54:10.640 --> 00:54:11.640]   Yeah.
[00:54:11.640 --> 00:54:16.960]   The US Espionage Act is notorious for being broad enough to prosecute people the US doesn't
[00:54:16.960 --> 00:54:17.960]   like.
[00:54:17.960 --> 00:54:18.960]   Exactly.
[00:54:18.960 --> 00:54:22.120]   They're going to use him to make a lesson, right?
[00:54:22.120 --> 00:54:23.120]   Yeah.
[00:54:23.120 --> 00:54:24.120]   That's basically it.
[00:54:24.120 --> 00:54:25.120]   I don't think they're going to.
[00:54:25.120 --> 00:54:31.040]   And even Chelsea Manning is out of prison, thanks to Clemency from the president.
[00:54:31.040 --> 00:54:33.520]   So I just don't know.
[00:54:33.520 --> 00:54:34.520]   I just don't know.
[00:54:34.520 --> 00:54:38.480]   And I wish we could say with some certainty, "Well, this is good.
[00:54:38.480 --> 00:54:40.200]   Justice will be done and we'll find out.
[00:54:40.200 --> 00:54:42.720]   And if he's a bad guy, he'll go to jail."
[00:54:42.720 --> 00:54:46.480]   By the way, part of the extradition agreement is that he could serve his time in Australia.
[00:54:46.480 --> 00:54:48.720]   He's an Australian citizen.
[00:54:48.720 --> 00:54:52.800]   And apparently he trusts the Australian jails, more than the US jails.
[00:54:52.800 --> 00:55:00.040]   I think it's an interesting conversation to have at the end of a week where two journalists
[00:55:00.040 --> 00:55:09.760]   receive a Nobel Prize for journalism and had really harsh words about what it takes to
[00:55:09.760 --> 00:55:11.840]   be a reporter today.
[00:55:11.840 --> 00:55:20.680]   And speaking truth to power and walking in a thin line to get to the truth.
[00:55:20.680 --> 00:55:30.720]   And I think it's like you all, I hope it did everything according to the books and in
[00:55:30.720 --> 00:55:39.000]   the name of revealing some of the things that the government has done.
[00:55:39.000 --> 00:55:40.800]   Because we need more people like that.
[00:55:40.800 --> 00:55:46.000]   We need, I think I'm a new American.
[00:55:46.000 --> 00:55:50.480]   I became American three years ago.
[00:55:50.480 --> 00:55:56.360]   And I have to say that I have my doubts about the check and balances of the system.
[00:55:56.360 --> 00:56:04.320]   And so I hope that he did everything right and like you are saying.
[00:56:04.320 --> 00:56:14.400]   And the question I think that has come up a few times is whether it was to his benefit
[00:56:14.400 --> 00:56:17.920]   of the people, so to speak.
[00:56:17.920 --> 00:56:24.280]   And surely we have come to understand some of the steps that the government is taking
[00:56:24.280 --> 00:56:28.200]   from an air rates perspective that we weren't aware of.
[00:56:28.200 --> 00:56:32.000]   And by the way, thank you for bringing up Maria Ressa.
[00:56:32.000 --> 00:56:36.720]   She's a journalist in the Philippines, Dimitri Muratov, a journalist in Russia.
[00:56:36.720 --> 00:56:43.960]   Both won the Nobel Peace Prize for their fight for freedom of expression in very difficult
[00:56:43.960 --> 00:56:47.360]   circumstances in Russia and the Philippines.
[00:56:47.360 --> 00:56:57.520]   And there was a great quote from Rappler about how difficult it is to be a journalist today.
[00:56:57.520 --> 00:57:00.280]   I wish I could find it.
[00:57:00.280 --> 00:57:03.360]   But anyway, yeah, you're absolutely right.
[00:57:03.360 --> 00:57:06.680]   The fight against the media is not a fight against the media, said Mr. Muratov.
[00:57:06.680 --> 00:57:09.560]   It is a fight against the people.
[00:57:09.560 --> 00:57:11.520]   Excellent.
[00:57:11.520 --> 00:57:15.920]   Without freedom of expression and freedom of press, this is part of the award, it will
[00:57:15.920 --> 00:57:19.760]   be difficult to successfully promote fraternity between nations, disarm them and a better
[00:57:19.760 --> 00:57:21.760]   world order to succeed in our times.
[00:57:21.760 --> 00:57:25.720]   Nice to see journalists win the Nobel Peace Prize.
[00:57:25.720 --> 00:57:31.680]   And it was interesting how they brought up social media as well and how in a way journalists
[00:57:31.680 --> 00:57:38.600]   are the ones who are seeking the truth and sharing the truth and they do that despite
[00:57:38.600 --> 00:57:40.000]   social media, right?
[00:57:40.000 --> 00:57:43.240]   Because social media is not set up in that way.
[00:57:43.240 --> 00:57:49.200]   Yeah, the Nobel Committee said Ms. Ressa and Rappler have also documented how social
[00:57:49.200 --> 00:57:56.960]   media is being used to spread fake news, harass opponents and manipulate public discourse.
[00:57:56.960 --> 00:58:02.560]   Only the 18th woman to win the Nobel Peace Prize in its 120 year history.
[00:58:02.560 --> 00:58:05.360]   So yeah.
[00:58:05.360 --> 00:58:06.840]   Congratulations.
[00:58:06.840 --> 00:58:10.400]   It's nice to see journalists get recognition for what is actually getting to be a much
[00:58:10.400 --> 00:58:12.480]   more dangerous profession.
[00:58:12.480 --> 00:58:15.680]   Fortunately, not for tech journalists so far.
[00:58:15.680 --> 00:58:18.680]   Anyway, we're all going to be.
[00:58:18.680 --> 00:58:20.600]   Okay.
[00:58:20.600 --> 00:58:25.800]   Big revelation from the information.
[00:58:25.800 --> 00:58:35.040]   Speaking of great journalism about a Tim Cook deal with the Chinese worth $275 billion.
[00:58:35.040 --> 00:58:38.000]   Now this goes back in time to 2015.
[00:58:38.000 --> 00:58:46.560]   You may remember around 2016, it was getting harder and harder for Apple to work in China.
[00:58:46.560 --> 00:58:49.880]   Apple's iPhone sales were plummeting.
[00:58:49.880 --> 00:58:54.640]   Apple apparently Cook decided to go to the country and meet with senior leadership, made
[00:58:54.640 --> 00:58:57.440]   a five year agreement in secret.
[00:58:57.440 --> 00:59:02.800]   This is the first time that's been reported in 2016 to quash a sudden burst of regulatory
[00:59:02.800 --> 00:59:04.480]   actions against Apple's businesses.
[00:59:04.480 --> 00:59:08.600]   This is Wayne Ma writing for the information.
[00:59:08.600 --> 00:59:12.640]   Before the meetings, Apple executives are scrambling to salvage the company's relationship
[00:59:12.640 --> 00:59:18.760]   with Chinese officials who believe the company wasn't contributing enough to the local economy.
[00:59:18.760 --> 00:59:25.800]   So Cook very effectively took charge here.
[00:59:25.800 --> 00:59:30.640]   Now the real question is now does that mean Apple is kind of in the back pocket of the
[00:59:30.640 --> 00:59:35.280]   Chinese Communist Party?
[00:59:35.280 --> 00:59:38.560]   They kept this agreement secret.
[00:59:38.560 --> 00:59:42.880]   The information quotes a political economist at the University of California, San Diego,
[00:59:42.880 --> 00:59:48.040]   Victor Shee, who says Apple as a global company needs to appease the Chinese government because
[00:59:48.040 --> 00:59:52.680]   China is a large market and a large production base for Apple.
[00:59:52.680 --> 00:59:55.960]   But at the same time, the vast majority of its consumers are still located outside of
[00:59:55.960 --> 00:59:57.440]   China.
[00:59:57.440 --> 01:00:01.520]   And as their opinion has turned increasingly negative toward China, Apple likely wanted
[01:00:01.520 --> 01:00:08.400]   to avoid the optics of groveling to the Chinese government.
[01:00:08.400 --> 01:00:10.520]   I don't know if it's fair to say grovel.
[01:00:10.520 --> 01:00:14.720]   They announced an investment in DD Global, a billion dollar investment.
[01:00:14.720 --> 01:00:17.320]   That's the Uber of China.
[01:00:17.320 --> 01:00:23.120]   They went to China, Jeff Williams, Lisa Jackson and CEO Cook went to China to meet with the
[01:00:23.120 --> 01:00:29.080]   Chinese government leaders at their secretive leadership compound, Zhongnhan Hai.
[01:00:29.080 --> 01:00:35.160]   Neither side disclosed details of the visit, but they signed an economic deal with China
[01:00:35.160 --> 01:00:39.600]   worth according to the information, $275 billion.
[01:00:39.600 --> 01:00:45.480]   Microsoft did similar things in years earlier, somewhat smaller commitment.
[01:00:45.480 --> 01:00:50.240]   Cisco did the same thing.
[01:00:50.240 --> 01:00:54.640]   These deals are technically non-binding, but Chinese officials take them more seriously
[01:00:54.640 --> 01:00:56.480]   than officials in other countries do.
[01:00:56.480 --> 01:01:00.120]   They are more binding than you would think.
[01:01:00.120 --> 01:01:05.200]   International attorney who writes about Chinese law Dan Harris from Harris-Bricken.
[01:01:05.200 --> 01:01:08.760]   What do you think?
[01:01:08.760 --> 01:01:15.080]   You can then trace this to China moving its servers, its iCloud servers, to the Chinese
[01:01:15.080 --> 01:01:24.160]   mainland last year, perhaps bowing to Chinese pressure to remove VPN apps from the App Store
[01:01:24.160 --> 01:01:29.400]   in China, even to go so far as to change maps.
[01:01:29.400 --> 01:01:32.240]   So it's not to offend the Chinese government.
[01:01:32.240 --> 01:01:34.080]   It's made a difference.
[01:01:34.080 --> 01:01:39.560]   iPhone sales have suddenly gone up in China considerably.
[01:01:39.560 --> 01:01:44.160]   China's state bureau of surveying and mapping told members of the Apple Maps team sometime
[01:01:44.160 --> 01:01:55.760]   in 2014 or 2015 to make the Diyao-u islands appear large even when users zoomed out.
[01:01:55.760 --> 01:02:02.000]   This has been a long-running dispute between China and Japan over who owns those islands.
[01:02:02.000 --> 01:02:06.240]   Chinese regulators threatened to withhold approval of the first Apple Watch if they
[01:02:06.240 --> 01:02:11.760]   didn't change the maps.
[01:02:11.760 --> 01:02:17.320]   So apparently when you view the Diyao-u islands in Apple Maps, they appear on a larger scale
[01:02:17.320 --> 01:02:20.560]   they still do than surrounding territories.
[01:02:20.560 --> 01:02:25.600]   It's interesting the things governments want you to do.
[01:02:25.600 --> 01:02:30.520]   Chinese regulators ordered Apple to shut down its iTunes store for China six months after
[01:02:30.520 --> 01:02:36.960]   it launched in 2016 because it didn't have the necessary "content licenses" but they fixed
[01:02:36.960 --> 01:02:39.960]   that.
[01:02:39.960 --> 01:02:48.640]   So it's not surprising to hear that China and Apple have some sort of back-channel agreement.
[01:02:48.640 --> 01:02:51.160]   Back-door agreements.
[01:02:51.160 --> 01:02:54.600]   How do we feel about that?
[01:02:54.600 --> 01:02:58.360]   What's your opinion on the entire world?
[01:02:58.360 --> 01:03:08.440]   I feel like it continues to feed into their BS and their continuation of just putting a
[01:03:08.440 --> 01:03:12.560]   chokehold on the entire world.
[01:03:12.560 --> 01:03:18.640]   It's wrong and dangerous but then it's like what are companies supposed to do unless they
[01:03:18.640 --> 01:03:20.320]   decide not to be in bed with them.
[01:03:20.320 --> 01:03:26.240]   Some companies like Google famously said, "Goodbye China."
[01:03:26.240 --> 01:03:28.520]   At a certain point someone needs to do that.
[01:03:28.520 --> 01:03:29.800]   You can continue doing these deals.
[01:03:29.800 --> 01:03:33.440]   Now that deal is over possibly they need to pay them now even more.
[01:03:33.440 --> 01:03:38.300]   Until companies take a stand and say, "We're going to lose this money but then gain more
[01:03:38.300 --> 01:03:39.640]   on the other side of it."
[01:03:39.640 --> 01:03:42.920]   Take a stand against China, nothing is going to change.
[01:03:42.920 --> 01:03:46.560]   I think it's really dangerous and it's getting worse.
[01:03:46.560 --> 01:03:52.800]   Revenue from the greater China which includes Taiwan, 68 billion, China now represents 19,
[01:03:52.800 --> 01:03:53.800]   that's to Apple.
[01:03:53.800 --> 01:03:55.800]   19% of Apple's total sales.
[01:03:55.800 --> 01:04:00.360]   That's a big chunk of your market.
[01:04:00.360 --> 01:04:02.320]   This is the question.
[01:04:02.320 --> 01:04:05.760]   How far do you go to appease a totalitarian regime?
[01:04:05.760 --> 01:04:12.600]   When it's a fifth of your market, will you just do whatever they say?
[01:04:12.600 --> 01:04:16.320]   They withhold currently privacy features for Chinese users.
[01:04:16.320 --> 01:04:20.120]   They comply with China's data security law requiring foreign companies to store more
[01:04:20.120 --> 01:04:24.880]   data about Chinese users in the country.
[01:04:24.880 --> 01:04:29.360]   At that point- Profiting against human rights violations.
[01:04:29.360 --> 01:04:34.560]   At that point, by the way, Microsoft's LinkedIn and Yahoo discontinued their Chinese operations
[01:04:34.560 --> 01:04:37.920]   rather than exceed to those demands.
[01:04:37.920 --> 01:04:40.960]   They censor the App Store in news.
[01:04:40.960 --> 01:04:43.480]   Of course, things like Tiananmen Square.
[01:04:43.480 --> 01:04:50.280]   VPNs, LGBTQ related apps, blocked.
[01:04:50.280 --> 01:04:53.680]   I think that- Go ahead, Carolina.
[01:04:53.680 --> 01:04:59.160]   I was going to say, is the price of doing business in China and you need to make a decision
[01:04:59.160 --> 01:05:06.880]   whether or not from a company perspective, it is worth you doing so not just from a business
[01:05:06.880 --> 01:05:11.400]   perspective but from an ethical perspective and who you want to be as a brand.
[01:05:11.400 --> 01:05:17.440]   I'm not surprised that Cook took care of it himself because obviously his pedigree has
[01:05:17.440 --> 01:05:20.240]   been a supply chain.
[01:05:20.240 --> 01:05:25.320]   It's been a lot of time in China working with government from a supply chain perspective
[01:05:25.320 --> 01:05:32.200]   so it's very well connected and he knows how to talk to people.
[01:05:32.200 --> 01:05:39.960]   I do think that the reason why maybe they don't get this scrutiny that they should is because
[01:05:39.960 --> 01:05:47.000]   the impact is on Chinese people, not necessarily on people internationally.
[01:05:47.000 --> 01:05:53.600]   People are quite happy to turn the other way and not be concerned about it.
[01:05:53.600 --> 01:06:00.520]   The question is how far they go to fit into what the Chinese government is requesting
[01:06:00.520 --> 01:06:01.800]   for their own user.
[01:06:01.800 --> 01:06:09.800]   I don't really think the increase in smartphone sales is necessarily linked to the deal that
[01:06:09.800 --> 01:06:11.240]   they made.
[01:06:11.240 --> 01:06:18.400]   There was a boycott of Apple for a certain period of time because it's the US brand with
[01:06:18.400 --> 01:06:26.720]   the biggest presence in China that came as a result of a ban on Huawei here in the US.
[01:06:26.720 --> 01:06:34.100]   It was very sure lived and people didn't really seem to care enough, I suppose, about
[01:06:34.100 --> 01:06:37.120]   Huawei not to buy Apple in China.
[01:06:37.120 --> 01:06:45.200]   But generally, consumers have stayed with the brand and the increase in sales has way
[01:06:45.200 --> 01:06:49.640]   more to do with the products that they're selling and the design of the iPhone and so
[01:06:49.640 --> 01:06:54.960]   forth that came with the recent models of a non-necessarily video.
[01:06:54.960 --> 01:07:00.200]   What is interesting though is that the companies that you're talking about Google LinkedIn,
[01:07:00.200 --> 01:07:04.480]   all of those companies is more on a services side than not the hardware side.
[01:07:04.480 --> 01:07:06.800]   Apple is depending on hardware.
[01:07:06.800 --> 01:07:11.920]   They're not really making a great deal of revenue from a services perspective.
[01:07:11.920 --> 01:07:14.440]   It's really all on hardware sales.
[01:07:14.440 --> 01:07:17.840]   Well, that's changing though rapidly with Apple.
[01:07:17.840 --> 01:07:18.840]   Services are becoming...
[01:07:18.840 --> 01:07:20.360]   Outside of China for sure.
[01:07:20.360 --> 01:07:21.360]   But not in China.
[01:07:21.360 --> 01:07:22.360]   But not in China.
[01:07:22.360 --> 01:07:24.440]   They're kind of foregoing that, aren't they?
[01:07:24.440 --> 01:07:25.440]   Yeah.
[01:07:25.440 --> 01:07:29.560]   Alex, this is something Big Tech in general has to face.
[01:07:29.560 --> 01:07:34.320]   If you want to be a global company, you're going to deal with less-safery characters,
[01:07:34.320 --> 01:07:39.200]   shall we say, and you have to make this decision.
[01:07:39.200 --> 01:07:40.880]   How do they deal with this?
[01:07:40.880 --> 01:07:46.040]   Well, I think that the other Big Tech companies are handling it better.
[01:07:46.040 --> 01:07:52.160]   Find me a company that's more smug, more self-satisfied, more realistic, more preachy
[01:07:52.160 --> 01:07:54.440]   in its marketing than Apple.
[01:07:54.440 --> 01:07:58.040]   Tucking about what happens on your iPhone stays on your iPhone and how you could really
[01:07:58.040 --> 01:07:59.400]   trust it.
[01:07:59.400 --> 01:08:00.840]   And then to see what it's doing.
[01:08:00.840 --> 01:08:09.920]   I mean, getting in bed with the CCP and they have their data storage in servers that are
[01:08:09.920 --> 01:08:13.560]   owned by Chinese Communist Party subsidiaries.
[01:08:13.560 --> 01:08:19.440]   So I think that every company needs to make its own determination about whether what
[01:08:19.440 --> 01:08:24.160]   it will compromise to operate in China because it's always going to be a compromise.
[01:08:24.160 --> 01:08:28.960]   But I think that for Apple to do this and then persist with its marketing messages to
[01:08:28.960 --> 01:08:31.280]   me is extremely hypocritical.
[01:08:31.280 --> 01:08:33.280]   And I think eventually it catches up with the company.
[01:08:33.280 --> 01:08:37.800]   I mean, people will eventually start looking at this stuff weird if they continue to see
[01:08:37.800 --> 01:08:40.040]   this news about Apple.
[01:08:40.040 --> 01:08:41.440]   That's a really good point.
[01:08:41.440 --> 01:08:45.840]   I don't know if there would be so much criticism if there weren't so sanctimonious about it.
[01:08:45.840 --> 01:08:48.840]   They've kind of brought it on themselves, in other words.
[01:08:48.840 --> 01:08:49.840]   No doubt.
[01:08:49.840 --> 01:08:53.320]   I think the sanctimonious, it works for them.
[01:08:53.320 --> 01:08:55.080]   They're viewed as a luxury brand.
[01:08:55.080 --> 01:09:00.240]   But Steve Jobs, but when he talked about marketing, he said it really is.
[01:09:00.240 --> 01:09:04.720]   You got a short amount of time to tell people who you are and what you stand for.
[01:09:04.720 --> 01:09:09.600]   And if Apple's telling us it's about privacy and safety and security on one hand and then
[01:09:09.600 --> 01:09:14.280]   doing these things that we're seeing in China on the other, that marketing will no longer
[01:09:14.280 --> 01:09:17.600]   align with what it actually stands for and who it is.
[01:09:17.600 --> 01:09:19.680]   And I think we'll fall apart.
[01:09:19.680 --> 01:09:21.080]   Actually that's one of the big stories this week.
[01:09:21.080 --> 01:09:23.840]   We're going to take a break when we come back.
[01:09:23.840 --> 01:09:28.840]   Does the Apple do not track button actually do anything increasing?
[01:09:28.840 --> 01:09:30.920]   Research shows no.
[01:09:30.920 --> 01:09:34.880]   It's, as Carl Bodie says in the Tech Dirt Privacy Theater.
[01:09:34.880 --> 01:09:37.920]   We'll talk about that in just a little bit.
[01:09:37.920 --> 01:09:39.680]   This is a twit.
[01:09:39.680 --> 01:09:41.560]   Carolina Milanese is here.
[01:09:41.560 --> 01:09:42.560]   Alex Cantrowitz.
[01:09:42.560 --> 01:09:43.560]   Sure, it's a little bizarre.
[01:09:43.560 --> 01:09:45.240]   It's great to have all three of you.
[01:09:45.240 --> 01:09:48.120]   Our show brought to you by Our Crowd.
[01:09:48.120 --> 01:09:52.040]   I love this idea, Our Crowd.
[01:09:52.040 --> 01:09:56.800]   My whole life covering technology I've been watching is people I know, people like Kevin
[01:09:56.800 --> 01:10:01.360]   Rose get in on these deals at the ground floor in these tech companies that they go public.
[01:10:01.360 --> 01:10:05.400]   They have an exit and suddenly these guys, they're buying fancy watches driving fancy
[01:10:05.400 --> 01:10:10.760]   cars and I go, "How do you get in on that deal flow?"
[01:10:10.760 --> 01:10:11.760]   question.
[01:10:11.760 --> 01:10:13.920]   Well, Our Crowd is the answer.
[01:10:13.920 --> 01:10:16.000]   Our Crowd is actually very cool.
[01:10:16.000 --> 01:10:22.640]   This is a venture capital firm who helps accredited investors get in on these early
[01:10:22.640 --> 01:10:29.120]   stage startups so they can be there for the big exits.
[01:10:29.120 --> 01:10:32.120]   They analyze companies around the global private market.
[01:10:32.120 --> 01:10:34.960]   They look at those with the private notice market.
[01:10:34.960 --> 01:10:36.760]   These are not companies of gone public.
[01:10:36.760 --> 01:10:39.040]   These are companies that are under the radar in many cases.
[01:10:39.040 --> 01:10:43.720]   They pick those with the greatest growth potential and then they present them to you.
[01:10:43.720 --> 01:10:51.000]   In areas like personalized medicine, cybersecurity, robotics, quantum computing and more.
[01:10:51.000 --> 01:10:56.600]   In state of the art labs, in startup garages and anywhere in between, Our Crowd is seeking
[01:10:56.600 --> 01:11:02.880]   out and identifying innovators so you can invest when growth potential is greatest early
[01:11:02.880 --> 01:11:03.880]   early on.
[01:11:03.880 --> 01:11:06.880]   Now, you have to be an accredited investor to do this.
[01:11:06.880 --> 01:11:11.760]   So when you go to our crowd.com/twit, it'll ask what country you're in and then you'll
[01:11:11.760 --> 01:11:16.440]   see the requirements because every nation has different requirements for net worth and
[01:11:16.440 --> 01:11:17.440]   so forth.
[01:11:17.440 --> 01:11:21.200]   But if you're an accredited investor, you're sophisticated enough to say, "I want to go
[01:11:21.200 --> 01:11:27.640]   beyond investing in the stock market or buying mutual funds or index funds.
[01:11:27.640 --> 01:11:32.360]   I want to get a little taste of the exciting stuff that's happening that's so far up to
[01:11:32.360 --> 01:11:37.120]   now has only been available to angel investors and VCs."
[01:11:37.120 --> 01:11:38.320]   This is a great way to do it.
[01:11:38.320 --> 01:11:44.880]   Our Crowds accredited investors have already invested $1 billion in growing tech companies.
[01:11:44.880 --> 01:11:50.920]   There have been 46 exits, 46 IPOs or sales of their investments.
[01:11:50.920 --> 01:11:55.040]   Many of their members have benefited big time from these exits and now you can.
[01:11:55.040 --> 01:12:00.160]   This is again something that I would look at only to diversify your portfolio only if
[01:12:00.160 --> 01:12:01.720]   you're an accredited investor.
[01:12:01.720 --> 01:12:06.120]   But it's a great way to get in early on innovative private market companies.
[01:12:06.120 --> 01:12:07.640]   Our Crowd does the legwork.
[01:12:07.640 --> 01:12:10.400]   Of course, at no time are you obligated.
[01:12:10.400 --> 01:12:13.800]   There's no cost to you to join our Crowd.
[01:12:13.800 --> 01:12:17.280]   It's only an opportunity that you can look at.
[01:12:17.280 --> 01:12:18.760]   Accredited investors can participate.
[01:12:18.760 --> 01:12:20.560]   There's a couple of ways you can do this.
[01:12:20.560 --> 01:12:25.280]   In single company deals, for as little as $10,000, you don't have to have millions to
[01:12:25.280 --> 01:12:27.000]   invest.
[01:12:27.000 --> 01:12:28.200]   Or you can get into their funds.
[01:12:28.200 --> 01:12:32.520]   They actually do an our Crowd fund, a number of funds.
[01:12:32.520 --> 01:12:36.000]   Those entry in that is $50,000.
[01:12:36.000 --> 01:12:38.560]   But you do have to have at least $10,000 to invest.
[01:12:38.560 --> 01:12:43.720]   Investment terms are going to vary, as I mentioned, where depending on what country you're in,
[01:12:43.720 --> 01:12:45.400]   what you should do is find out more information.
[01:12:45.400 --> 01:12:47.560]   Just go to the site, input the country you're investing from.
[01:12:47.560 --> 01:12:49.000]   You'll see what the requirements are.
[01:12:49.000 --> 01:12:54.000]   But if you are an accredited investor, I think this is a really interesting way to kind of
[01:12:54.000 --> 01:12:55.440]   get in early.
[01:12:55.440 --> 01:12:58.000]   Let the our Crowd folks, they've got the deal flow.
[01:12:58.000 --> 01:13:00.840]   They're willing to let you in on these deals.
[01:13:00.840 --> 01:13:03.320]   And of course, these are deals they're already in on as well.
[01:13:03.320 --> 01:13:07.120]   Join the fastest growing venture capital investment community right now.
[01:13:07.120 --> 01:13:12.160]   No cost to join our crowd.com/twit.
[01:13:12.160 --> 01:13:17.720]   Even for just informational purposes, I think this would be fascinating.
[01:13:17.720 --> 01:13:23.560]   Our crowd.com/twit, I should mention, because I cover these companies.
[01:13:23.560 --> 01:13:24.560]   I don't invest in them.
[01:13:24.560 --> 01:13:30.000]   That's one of the reasons I've been sitting on the sidelines watching Kevin Rose get rich.
[01:13:30.000 --> 01:13:31.000]   But that's fine.
[01:13:31.000 --> 01:13:33.040]   That's the choice I paid.
[01:13:33.040 --> 01:13:38.080]   And I'm very happy about it because I probably lose my shirt.
[01:13:38.080 --> 01:13:43.040]   If you are somebody who wants to invest in these companies, it's a great way to do it.
[01:13:43.040 --> 01:13:46.440]   Our Crowd.com/twit.
[01:13:46.440 --> 01:13:50.080]   Really interesting idea, I think.
[01:13:50.080 --> 01:13:51.080]   All right.
[01:13:51.080 --> 01:13:52.080]   Let's talk about this.
[01:13:52.080 --> 01:13:55.120]   Carl Bodie, Apple, do not track button his privacy theater.
[01:13:55.120 --> 01:13:57.320]   We're starting to see research.
[01:13:57.320 --> 01:14:00.400]   Financial Times had a big article.
[01:14:00.400 --> 01:14:05.600]   It all started, of course, seven months ago when iOS 14.5 came out.
[01:14:05.600 --> 01:14:09.760]   And Apple started surfacing a setting it had always had, deep in the settings.
[01:14:09.760 --> 01:14:15.000]   Every time you install a new app saying this app wants permission to track you across apps,
[01:14:15.000 --> 01:14:16.000]   yes or no.
[01:14:16.000 --> 01:14:21.560]   And of course, when you say it like that, people are invariably clicking no.
[01:14:21.560 --> 01:14:23.280]   In fact, about 80%.
[01:14:23.280 --> 01:14:28.240]   That's estimated if users say, no, don't, don't track me.
[01:14:28.240 --> 01:14:31.960]   And Facebook took out ads.
[01:14:31.960 --> 01:14:36.440]   Carl says Facebook cried like a disappointed toddler at Christmas.
[01:14:36.440 --> 01:14:39.640]   Except we're going to lose millions of dollars.
[01:14:39.640 --> 01:14:40.960]   In fact, Facebook was smart.
[01:14:40.960 --> 01:14:42.940]   They didn't say we're going to lose money.
[01:14:42.940 --> 01:14:44.880]   This is going to hurt small businesses.
[01:14:44.880 --> 01:14:47.840]   They took a full page ads saying this is terrible.
[01:14:47.840 --> 01:14:51.000]   But really, maybe it wasn't that bad.
[01:14:51.000 --> 01:14:56.680]   All it did was turn off the ID for advertising that Apple has the IDFA.
[01:14:56.680 --> 01:15:01.000]   And what's happened over the last seven months is companies like Facebook and Snap have figured
[01:15:01.000 --> 01:15:02.680]   out how to track you without it.
[01:15:02.680 --> 01:15:05.840]   It wasn't that useful.
[01:15:05.840 --> 01:15:10.160]   Financial Times says seven months later, companies, including Snap and Facebook have been allowed
[01:15:10.160 --> 01:15:13.080]   to keep sharing user level signals from iPhones.
[01:15:13.080 --> 01:15:16.200]   Apple didn't block that as long as that data is.
[01:15:16.200 --> 01:15:21.360]   But again, I'm going to put this in quotes, anonymized and aggregated rather than tied
[01:15:21.360 --> 01:15:22.520]   to you specifically.
[01:15:22.520 --> 01:15:27.440]   So all of the information that was tied to your IDFA is now still coming to them, just
[01:15:27.440 --> 01:15:29.360]   not tied to your IDFA.
[01:15:29.360 --> 01:15:36.840]   But as we know, many, many studies have reported anonymization doesn't work.
[01:15:36.840 --> 01:15:37.840]   Doesn't work.
[01:15:37.840 --> 01:15:41.360]   It's fairly easy to disanonymize that information.
[01:15:41.360 --> 01:15:44.520]   And in fact, there's lots of ways to do it.
[01:15:44.520 --> 01:15:50.440]   Companies still can exfiltrate information about things like how much storage you have
[01:15:50.440 --> 01:15:54.960]   free, what your IP address is, how much RAM you have, enough information that they can
[01:15:54.960 --> 01:15:56.280]   fingerprint you.
[01:15:56.280 --> 01:15:59.800]   So they may not know your name, but they know you.
[01:15:59.800 --> 01:16:02.400]   So this is the question.
[01:16:02.400 --> 01:16:07.720]   Carl writes, "Apple's opt-out button is largely decorative, helping the company brand itself
[01:16:07.720 --> 01:16:15.880]   as hyper-privacy conscious without doing the heavy lifting required of such a shift."
[01:16:15.880 --> 01:16:23.840]   The Financial Times quoted lockdown privacy, which is an app that blocks ad trackers.
[01:16:23.840 --> 01:16:28.600]   They say Apple's policy is functionally useless in stopping third-party, tracking the performative
[01:16:28.600 --> 01:16:33.880]   variety of tests on top apps and observe that personal data and device information is still
[01:16:33.880 --> 01:16:36.200]   being sent to trackers in almost all cases.
[01:16:36.200 --> 01:16:38.280]   In other words, you check that box.
[01:16:38.280 --> 01:16:40.600]   All that blocks is the IDFA.
[01:16:40.600 --> 01:16:42.120]   Everything else is still going on.
[01:16:42.120 --> 01:16:43.480]   Now I have to say I use an iPhone.
[01:16:43.480 --> 01:16:51.680]   I don't care because to me, advertisers knowing more about my preferences is not a harm, but
[01:16:51.680 --> 01:16:56.280]   I'm not a privacy, you know, strong privacy advocate.
[01:16:56.280 --> 01:16:59.560]   Shira, you use an iPhone I bet.
[01:16:59.560 --> 01:17:00.560]   I do.
[01:17:00.560 --> 01:17:03.360]   Are you worried?
[01:17:03.360 --> 01:17:05.800]   It is worrisome, of course.
[01:17:05.800 --> 01:17:10.400]   At the same time, I feel like they're, unfortunately, I'm at the mindset, like they're going to
[01:17:10.400 --> 01:17:12.200]   find another way.
[01:17:12.200 --> 01:17:13.200]   Yes.
[01:17:13.200 --> 01:17:14.200]   And it is annoying.
[01:17:14.200 --> 01:17:19.120]   Like it is annoying to feel like you're being, and I don't want to feel like I'm being listened
[01:17:19.120 --> 01:17:20.960]   to in that way.
[01:17:20.960 --> 01:17:29.040]   I feel like if there's a way to serve me up ads that are more relevant for me, then I'm
[01:17:29.040 --> 01:17:30.040]   open to that.
[01:17:30.040 --> 01:17:33.840]   I'm open to supporting small business and all that, but who am I really supporting?
[01:17:33.840 --> 01:17:36.200]   My supporting small business or am I supporting Facebook?
[01:17:36.200 --> 01:17:39.960]   Oh, because small businesses buy ads from Facebook.
[01:17:39.960 --> 01:17:44.720]   I mean, yeah, so an Instagram and what's that of a nature of the game right now?
[01:17:44.720 --> 01:17:50.920]   But I do think that like, let's be real about it, like putting this lip service up.
[01:17:50.920 --> 01:17:52.120]   It doesn't help.
[01:17:52.120 --> 01:17:55.160]   And so how do we be more transparent?
[01:17:55.160 --> 01:17:59.360]   How do we feel like we're, yeah, opting in and like having the choice?
[01:17:59.360 --> 01:18:01.480]   Is it possible to actually have that choice?
[01:18:01.480 --> 01:18:06.960]   It just increasingly feels like this is marketing lip service from Apple instead of a genuine
[01:18:06.960 --> 01:18:08.720]   commitment to privacy.
[01:18:08.720 --> 01:18:10.520]   And that's disheartening.
[01:18:10.520 --> 01:18:12.720]   I mean, I know Google's snooping on me.
[01:18:12.720 --> 01:18:14.960]   I know Facebook's snooping on me.
[01:18:14.960 --> 01:18:17.880]   That's their business model, but Apple sells hardware.
[01:18:17.880 --> 01:18:20.800]   They don't need to enable these people.
[01:18:20.800 --> 01:18:24.440]   Carolina, what's your take?
[01:18:24.440 --> 01:18:29.840]   I think we have a similar conversation as to what we were saying about Instagram.
[01:18:29.840 --> 01:18:34.840]   It's fascinating that companies spend these seven months figuring out how to work around
[01:18:34.840 --> 01:18:39.040]   it versus spending seven months figuring out how you can better serve me.
[01:18:39.040 --> 01:18:42.640]   Because I don't have a problem being tracked.
[01:18:42.640 --> 01:18:47.440]   If you're using the information in a smart way, that serves me.
[01:18:47.440 --> 01:18:51.200]   And unfortunately, there's a lot of companies that still don't do that very well from an
[01:18:51.200 --> 01:18:53.760]   ad perspective.
[01:18:53.760 --> 01:19:00.120]   And I think that's for me, I'm also not a very strong privacy pro person because I live
[01:19:00.120 --> 01:19:01.120]   on the internet.
[01:19:01.120 --> 01:19:06.520]   And yeah, I share too much about my family to share as so.
[01:19:06.520 --> 01:19:09.360]   But it's all about what's in it for me.
[01:19:09.360 --> 01:19:10.680]   I'm being very selfish.
[01:19:10.680 --> 01:19:17.000]   Like, yeah, track me, but show me that you're using the data to my benefit, not yours.
[01:19:17.000 --> 01:19:23.240]   And so I think that that to me is where the conversation should be.
[01:19:23.240 --> 01:19:28.880]   And as we talk more and more about AI and how AI is going to be part of the way that
[01:19:28.880 --> 01:19:37.320]   we serve better ads and information in general by using that even anonymized data points,
[01:19:37.320 --> 01:19:40.680]   then do a better job at it.
[01:19:40.680 --> 01:19:43.440]   And I do draw the line at Facebook.
[01:19:43.440 --> 01:19:49.160]   I'm not like, yes, I want to support small businesses, but I go and support them somewhere
[01:19:49.160 --> 01:19:52.400]   else versus over Facebook.
[01:19:52.400 --> 01:19:57.000]   I just don't think that it's like the algorithm discussion earlier.
[01:19:57.000 --> 01:20:00.360]   Is that algorithm serving me or serving the platform?
[01:20:00.360 --> 01:20:03.360]   And I do think that it's serving the platform.
[01:20:03.360 --> 01:20:05.080]   Now from an Apple perspective, I agree.
[01:20:05.080 --> 01:20:11.800]   I think that there needs to be, there could be more to be done than what they're doing.
[01:20:11.800 --> 01:20:16.120]   And the conversation is also, okay, you're so against what Facebook is doing.
[01:20:16.120 --> 01:20:19.160]   And yet, we can all use Facebook, right?
[01:20:19.160 --> 01:20:22.600]   You're so against Google and yet you are working with Google.
[01:20:22.600 --> 01:20:29.600]   So I think there are different reasons why people might be kind of skeptical about the
[01:20:29.600 --> 01:20:37.520]   privacy pro push that Apple has been serving over the past two to three years.
[01:20:37.520 --> 01:20:43.040]   I'm probably, and I get heat for this all the time, not the guy to talk to about this
[01:20:43.040 --> 01:20:46.360]   because I think the whole privacy thing is overblown.
[01:20:46.360 --> 01:20:48.240]   You're on the internet.
[01:20:48.240 --> 01:20:50.640]   My take is nothing you do is private.
[01:20:50.640 --> 01:20:51.720]   It's all out there.
[01:20:51.720 --> 01:20:56.560]   It is chiefly used for advertising to make advertising targeted.
[01:20:56.560 --> 01:20:59.000]   I have no problem with that.
[01:20:59.000 --> 01:21:03.440]   In fact, the reason I stopped using Instagram is because I kept buying every third thing
[01:21:03.440 --> 01:21:04.480]   that came up.
[01:21:04.480 --> 01:21:07.080]   It was so effective.
[01:21:07.080 --> 01:21:08.920]   It was so much targeted to me.
[01:21:08.920 --> 01:21:13.880]   I have so much stuff that I don't want that I bought on Instagram that I stopped using
[01:21:13.880 --> 01:21:17.200]   the platform as a result because it was effective.
[01:21:17.200 --> 01:21:19.000]   That's the only negative.
[01:21:19.000 --> 01:21:20.000]   It works.
[01:21:20.000 --> 01:21:22.320]   Advertising works and it's targeted and works.
[01:21:22.320 --> 01:21:24.800]   I guess you could say, well, what if government gets that?
[01:21:24.800 --> 01:21:29.000]   And we certainly know the law enforcement has access to a lot of information from your
[01:21:29.000 --> 01:21:31.240]   smartphone and from carriers.
[01:21:31.240 --> 01:21:35.560]   And, you know, again, it doesn't bother me.
[01:21:35.560 --> 01:21:41.680]   Now, if you were an activist, a dissident, if you were a gender fluid, you know, maybe
[01:21:41.680 --> 01:21:43.080]   you might be more concerned.
[01:21:43.080 --> 01:21:45.280]   I'm a white, old white man.
[01:21:45.280 --> 01:21:47.440]   I got nothing to lose.
[01:21:47.440 --> 01:21:49.600]   So I understand that other people may be more concerned.
[01:21:49.600 --> 01:21:53.800]   But on the balance, I don't know if this is...
[01:21:53.800 --> 01:21:58.040]   I think you're asking a lot to say, I want all these free services and I don't want them
[01:21:58.040 --> 01:21:59.040]   to know anything about me.
[01:21:59.040 --> 01:22:01.080]   Alex, are my nuts?
[01:22:01.080 --> 01:22:02.080]   No.
[01:22:02.080 --> 01:22:04.840]   I think you should have the option.
[01:22:04.840 --> 01:22:08.600]   And that's what I like about this Apple update.
[01:22:08.600 --> 01:22:09.600]   And I'm going to...
[01:22:09.600 --> 01:22:11.880]   If it did something...
[01:22:11.880 --> 01:22:14.600]   I can't believe I'm doing this, but I'm about to take Apple's side.
[01:22:14.600 --> 01:22:15.600]   Good.
[01:22:15.600 --> 01:22:16.600]   Taking Apple's side.
[01:22:16.600 --> 01:22:17.600]   Good.
[01:22:17.600 --> 01:22:18.600]   Somebody must do.
[01:22:18.600 --> 01:22:19.600]   Good.
[01:22:19.600 --> 01:22:23.360]   I think that what they do is block the individual tracking.
[01:22:23.360 --> 01:22:28.800]   If they have broader anonymous and we know anonymous system issues, but largely anonymized,
[01:22:28.800 --> 01:22:35.040]   aggregated information so that advertisers are now advertising to cohorts of people,
[01:22:35.040 --> 01:22:36.480]   not individuals.
[01:22:36.480 --> 01:22:41.240]   So advertisers are tracking groups of people, not individuals, and are able to optimize
[01:22:41.240 --> 01:22:42.240]   basis of that.
[01:22:42.240 --> 01:22:46.160]   I think that's actually where we want to go on the web, where it's not one-to-one marketing
[01:22:46.160 --> 01:22:49.920]   that a lot of advertisers like to talk about, but it's creepy to users.
[01:22:49.920 --> 01:22:53.960]   But if it's a group, you know, if it's targeted, but not that targeted that you kind of feel
[01:22:53.960 --> 01:22:57.040]   like they're listening to your conversations, I think that's good.
[01:22:57.040 --> 01:23:01.480]   So like the Google cohorts thing, flock.
[01:23:01.480 --> 01:23:03.800]   I think I happen to think that that's a good solution.
[01:23:03.800 --> 01:23:07.240]   And I think that I feel a lot better, you know, having companies track me in a group
[01:23:07.240 --> 01:23:08.240]   of 20 people.
[01:23:08.240 --> 01:23:09.240]   Right.
[01:23:09.240 --> 01:23:13.600]   And they do individually because ultimately, like let's say you take a worst-case scenario,
[01:23:13.600 --> 01:23:17.120]   where an engineer wanted to know everything they had they could about me and violated
[01:23:17.120 --> 01:23:20.720]   all rules to be able to access that information.
[01:23:20.720 --> 01:23:24.000]   If they find me as part of a cohort, that's very different than if they find me as an
[01:23:24.000 --> 01:23:25.160]   individual.
[01:23:25.160 --> 01:23:30.000]   But if companies said, "Okay, here's your choice."
[01:23:30.000 --> 01:23:35.240]   No privacy and you get the service for free or let's say three bucks a month and you got
[01:23:35.240 --> 01:23:37.240]   privacy.
[01:23:37.240 --> 01:23:39.480]   Would that be acceptable?
[01:23:39.480 --> 01:23:40.480]   In some cases, yes.
[01:23:40.480 --> 01:23:42.480]   I mean, I'm paying for Twitter blue, which allows...
[01:23:42.480 --> 01:23:45.240]   Yeah, these two have free access to a lot of different websites.
[01:23:45.240 --> 01:23:49.320]   Ironically, they promise no privacy from Twitter blue, but you get others.
[01:23:49.320 --> 01:23:50.320]   You have more.
[01:23:50.320 --> 01:23:51.320]   Right.
[01:23:51.320 --> 01:23:53.360]   So I have no problem, you know, paying for something premium.
[01:23:53.360 --> 01:23:57.520]   I mean, I'm paying for it twice actually, one for me, one for big technology, so six
[01:23:57.520 --> 01:23:58.520]   bucks a month.
[01:23:58.520 --> 01:24:02.720]   So Twitter, I know Facebook's considered making a pay product as well.
[01:24:02.720 --> 01:24:06.960]   I think it gets kind of difficult when you talk about going to other markets and, you
[01:24:06.960 --> 01:24:10.040]   know, when you think about their average revenue per user in the US.
[01:24:10.040 --> 01:24:14.600]   Well, also we know really what these guys will do is charge three bucks and still invade
[01:24:14.600 --> 01:24:15.600]   your privacy.
[01:24:15.600 --> 01:24:16.600]   Exactly.
[01:24:16.600 --> 01:24:17.600]   Yeah.
[01:24:17.600 --> 01:24:21.240]   So Leo, but I'm totally sympathetic that when someone builds something, you should be
[01:24:21.240 --> 01:24:25.080]   able to, you know, give them something of value in exchange for that.
[01:24:25.080 --> 01:24:27.440]   And if it's not going to be money, it can be data.
[01:24:27.440 --> 01:24:30.800]   And that's why I think that since we're going to make this compromise on the internet as
[01:24:30.800 --> 01:24:37.720]   it is, let's move to cohorts away from individualized data that can still work for advertisers.
[01:24:37.720 --> 01:24:41.080]   And there's this storyline, you know, it's very interesting, the storyline that like,
[01:24:41.080 --> 01:24:44.720]   you know, they're still collecting all the data and it's working just as well as it had
[01:24:44.720 --> 01:24:45.720]   been in the past.
[01:24:45.720 --> 01:24:49.640]   I mean, I've spoken with advertisers who've seen the impact of these changes that Apple
[01:24:49.640 --> 01:24:51.400]   has made firsthand.
[01:24:51.400 --> 01:24:54.280]   And I can tell you they're moving their money away from Facebook because it doesn't work
[01:24:54.280 --> 01:24:55.280]   as well anymore.
[01:24:55.280 --> 01:24:59.920]   And Facebook has, you know, advised investors that its stock is going to take a hit because
[01:24:59.920 --> 01:25:00.920]   of it.
[01:25:00.920 --> 01:25:05.080]   So I do think that it's actually real, but not real enough that Facebook's, you know,
[01:25:05.080 --> 01:25:07.360]   advertising business has gone off the cliff.
[01:25:07.360 --> 01:25:09.680]   And I think that's a pretty fair compromise for everyone.
[01:25:09.680 --> 01:25:10.680]   Yeah.
[01:25:10.680 --> 01:25:17.160]   But I think Alex, this point about, you know, if you cannot afford to pay for a service,
[01:25:17.160 --> 01:25:21.760]   for a compromise should not be data is really important, right?
[01:25:21.760 --> 01:25:27.080]   Because that's a little bit the argument that Facebook has been using saying, well,
[01:25:27.080 --> 01:25:28.960]   we don't make you pay for the service.
[01:25:28.960 --> 01:25:31.920]   So we have to do it this way.
[01:25:31.920 --> 01:25:37.080]   And we did actually a research when the first time that the idea of paying for a service
[01:25:37.080 --> 01:25:40.720]   for Facebook circulated, how do you want it to pay?
[01:25:40.720 --> 01:25:46.080]   You know, we served about 1500 people here in the US and nobody wanted to pay, which
[01:25:46.080 --> 01:25:52.760]   to me goes back to when the service is, you know, is not critical to people because otherwise
[01:25:52.760 --> 01:25:55.200]   you would be prepared to pay.
[01:25:55.200 --> 01:26:01.200]   But what is fascinating is that we talk a lot about privacy and data being used online,
[01:26:01.200 --> 01:26:11.120]   but I can tell you that in my move from California to Georgia, I have no idea how, what I checked
[01:26:11.120 --> 01:26:16.840]   to have every mortgage company, every moving company, everything.
[01:26:16.840 --> 01:26:18.840]   So how do they know?
[01:26:18.840 --> 01:26:20.480]   Yes, they do.
[01:26:20.480 --> 01:26:26.040]   And clearly I've not done it online because everything was coming from my mailbox.
[01:26:26.040 --> 01:26:31.360]   So, you know, it's not just online, it's real life too.
[01:26:31.360 --> 01:26:36.640]   And that continues, you know, even now with services being pitched and whatnot.
[01:26:36.640 --> 01:26:41.520]   So there are old-fashioned distribution lists still exist.
[01:26:41.520 --> 01:26:43.760]   Yeah, oh gosh, yes.
[01:26:43.760 --> 01:26:46.400]   You know, I'm a little sympathetic.
[01:26:46.400 --> 01:26:48.080]   Advertisers want to reach people who are moving.
[01:26:48.080 --> 01:26:52.320]   So they figure out, they buy mailing lists, they figure out.
[01:26:52.320 --> 01:26:55.440]   Do you bother you or getting mailing about that?
[01:26:55.440 --> 01:27:01.040]   It doesn't bother me, but it really made me realize that we are getting so hung up on
[01:27:01.040 --> 01:27:07.280]   the online part of it and basically is digitizing something that we've been facing for years.
[01:27:07.280 --> 01:27:08.280]   Right.
[01:27:08.280 --> 01:27:09.280]   It's always gone.
[01:27:09.280 --> 01:27:10.280]   It's just a different medium.
[01:27:10.280 --> 01:27:17.040]   I know, I mean, I can't, I know this is completely counterculture, but so what?
[01:27:17.040 --> 01:27:19.320]   You know, what's the harm?
[01:27:19.320 --> 01:27:21.160]   You know, look, you're going to get advertising.
[01:27:21.160 --> 01:27:24.720]   It's not like the choices, do I have ads or no ads?
[01:27:24.720 --> 01:27:30.400]   It's a choice between ads targeted at something you might be up to and ads that are just random.
[01:27:30.400 --> 01:27:34.040]   Well, there's a difference between that, like in spam, being spammed.
[01:27:34.040 --> 01:27:36.800]   Well, nothing's going to stop spam.
[01:27:36.800 --> 01:27:39.960]   I mean, I mean, like that's the thing, but you're getting all this mail, like we're
[01:27:39.960 --> 01:27:41.480]   talking about trying to save the environment.
[01:27:41.480 --> 01:27:43.760]   Why do they think I want so much Viagra?
[01:27:43.760 --> 01:27:46.440]   I mean, what?
[01:27:46.440 --> 01:27:48.200]   The Bikini's and the Viagra.
[01:27:48.200 --> 01:27:49.520]   What is it they know about me?
[01:27:49.520 --> 01:27:50.520]   You know what I'm saying?
[01:27:50.520 --> 01:27:54.560]   They know your TikTok feed, Leo.
[01:27:54.560 --> 01:27:59.880]   So, I don't, I don't particularly want to ask for Viagra and my spam, but spammers don't
[01:27:59.880 --> 01:28:00.880]   care.
[01:28:00.880 --> 01:28:03.680]   Spam is so cheap, that doesn't matter what they're sending a spam out.
[01:28:03.680 --> 01:28:08.960]   Most spam is not something you'd ever want to buy, but spam is free and cheap.
[01:28:08.960 --> 01:28:13.040]   Those mailers in your mailbox, that's not, unfortunately, the US Postal Service does
[01:28:13.040 --> 01:28:15.320]   subsidize it, so it's practically free.
[01:28:15.320 --> 01:28:17.240]   It's very cheap.
[01:28:17.240 --> 01:28:22.320]   And those ads, those Instagram ads, those are expensive relatively.
[01:28:22.320 --> 01:28:25.480]   Don't you think, if I'm going to buy an ad, I want to kind of make sure that the people
[01:28:25.480 --> 01:28:29.680]   who want my product will see it, not the ones who don't.
[01:28:29.680 --> 01:28:32.880]   I just, I don't, I think people are knee jerk about this.
[01:28:32.880 --> 01:28:37.960]   They feel like, I don't want anybody knowing anything about me.
[01:28:37.960 --> 01:28:38.960]   That's not realistic.
[01:28:38.960 --> 01:28:40.000]   We live in the world.
[01:28:40.000 --> 01:28:42.880]   People know a lot about you, even if you're not online.
[01:28:42.880 --> 01:28:44.840]   And I don't know.
[01:28:44.840 --> 01:28:47.680]   I think it's, I don't know that people are unrealistic.
[01:28:47.680 --> 01:28:54.960]   I think that people want to know who knows about them and what they're going to do about
[01:28:54.960 --> 01:28:55.960]   it.
[01:28:55.960 --> 01:28:56.960]   Do you know what I mean?
[01:28:56.960 --> 01:29:01.080]   I'm not online, so clearly I want people to know something about me, because otherwise,
[01:29:01.080 --> 01:29:03.920]   you know, part of my business model wouldn't work.
[01:29:03.920 --> 01:29:11.440]   But, but the, the, that is my conscious decision of sharing data with people versus you're
[01:29:11.440 --> 01:29:12.720]   taking something from me.
[01:29:12.720 --> 01:29:14.680]   I'm not quite sure what you're taking.
[01:29:14.680 --> 01:29:18.000]   And then I'm not sure how, you know, companies are using that.
[01:29:18.000 --> 01:29:23.440]   And I go back to my point, if you surface data, you know, ads that were actually relevant
[01:29:23.440 --> 01:29:26.680]   for me, I wouldn't mind it as much.
[01:29:26.680 --> 01:29:31.240]   It's just when you get bombarded by stuff that really you don't care about.
[01:29:31.240 --> 01:29:36.560]   Well, that's mostly just a complaint of how bad ad, ad recommendation services are.
[01:29:36.560 --> 01:29:39.480]   That's more a software problem than a privacy problem.
[01:29:39.480 --> 01:29:41.880]   There needs to be regulation.
[01:29:41.880 --> 01:29:44.880]   I feel to me, I feel to me like people are childish.
[01:29:44.880 --> 01:29:45.880]   Okay.
[01:29:45.880 --> 01:29:51.040]   I'm not, not, not you guys, but people are childish, not, not you, but other people are
[01:29:51.040 --> 01:29:53.400]   childish saying, I want my free services.
[01:29:53.400 --> 01:29:57.760]   I don't want to pay for them and I don't want you to spy on me and I don't want any ads.
[01:29:57.760 --> 01:29:59.960]   I don't, you think there needs to be just some regulation?
[01:29:59.960 --> 01:30:06.280]   I think that all this stuff has advanced so quickly that there hasn't been any regulation.
[01:30:06.280 --> 01:30:07.920]   And so it's like a free for all.
[01:30:07.920 --> 01:30:12.400]   And I think that's what consumers are saying, like we need to start sending boundaries or
[01:30:12.400 --> 01:30:17.040]   else then anything can be used and taken advantage of and used for the wrong reason.
[01:30:17.040 --> 01:30:18.840]   And I mean, like that's the issue.
[01:30:18.840 --> 01:30:20.560]   Well, of course there's always that.
[01:30:20.560 --> 01:30:23.720]   And this is somebody in the chat, I'm saying the same thing, faux poscing.
[01:30:23.720 --> 01:30:27.920]   The thing is you think they're only using your info to advertise to you.
[01:30:27.920 --> 01:30:30.240]   But what might they be using it for?
[01:30:30.240 --> 01:30:32.920]   But as yet, nobody has shown me that's happening.
[01:30:32.920 --> 01:30:36.280]   Well, yeah, Facebook, what about the election?
[01:30:36.280 --> 01:30:38.600]   Manipulate, but I knew who I was going to vote for.
[01:30:38.600 --> 01:30:39.600]   I don't care.
[01:30:39.600 --> 01:30:45.320]   Facebook shows me ads for somebody I'm not going to vote for.
[01:30:45.320 --> 01:30:47.280]   The question is where you draw the line, right?
[01:30:47.280 --> 01:30:49.280]   It's always, it's always when people say that.
[01:30:49.280 --> 01:30:53.000]   They're always saying, well, it's not going to affect me.
[01:30:53.000 --> 01:30:55.880]   But think of all the dumb people it's going to affect.
[01:30:55.880 --> 01:30:58.360]   It's like, how patronizing can you be?
[01:30:58.360 --> 01:31:01.640]   Well, Leo, like advertising's always been targeted.
[01:31:01.640 --> 01:31:06.280]   So you started by targeting base off of like, you know, the content, right?
[01:31:06.280 --> 01:31:10.360]   And so if people were watching, you know, people are watching your show, right?
[01:31:10.360 --> 01:31:12.240]   Your show is targeted based off of content.
[01:31:12.240 --> 01:31:13.240]   Yeah.
[01:31:13.240 --> 01:31:14.240]   Yeah, we do it.
[01:31:14.240 --> 01:31:15.240]   We do it the old fashioned way.
[01:31:15.240 --> 01:31:18.480]   We figure if you're a geek enough to listen to this show, you probably want to hear some
[01:31:18.480 --> 01:31:19.480]   of the things that you're doing.
[01:31:19.480 --> 01:31:20.480]   Exactly.
[01:31:20.480 --> 01:31:22.920]   And so you can run targeted advertising without knowing everything.
[01:31:22.920 --> 01:31:24.600]   No, I don't know anything about our audience.
[01:31:24.600 --> 01:31:25.600]   That's right.
[01:31:25.600 --> 01:31:28.000]   And I bet the advertising works really well for your advertising.
[01:31:28.000 --> 01:31:29.000]   I hope so.
[01:31:29.000 --> 01:31:32.360]   And you go down the line a little bit and there's, you know, on the internet, we started breaking
[01:31:32.360 --> 01:31:36.080]   things down by location, by age and gender.
[01:31:36.080 --> 01:31:37.080]   And that was fine too.
[01:31:37.080 --> 01:31:40.960]   I don't think anyone has a problem with that when you do it, you know, broadly segmented.
[01:31:40.960 --> 01:31:42.360]   And then we went another step.
[01:31:42.360 --> 01:31:46.600]   We started targeting, you know, by browsing behavior and interests.
[01:31:46.600 --> 01:31:50.920]   And then, you know, eventually you could basically pick out the thousand people you need to reach
[01:31:50.920 --> 01:31:52.480]   and find them some way.
[01:31:52.480 --> 01:31:56.320]   You can target based off of credit card information and all that type of stuff.
[01:31:56.320 --> 01:32:01.560]   And so the question is like when we think about advertising online, we're going to make
[01:32:01.560 --> 01:32:09.320]   that compromise between getting show advertisements and using free products everywhere we go.
[01:32:09.320 --> 01:32:15.600]   And the question is, you know, what is fair for a person to give up in order to be able
[01:32:15.600 --> 01:32:17.000]   to use these products?
[01:32:17.000 --> 01:32:21.480]   Because, you know, we were able to fund through advertising lots of great stuff, including
[01:32:21.480 --> 01:32:22.720]   the show.
[01:32:22.720 --> 01:32:28.480]   And we don't need necessarily to go so granular to the point that people feel creeped out.
[01:32:28.480 --> 01:32:30.320]   And I think that's the discussion here.
[01:32:30.320 --> 01:32:35.160]   And I think it's a reasonable question in terms of where you actually want to draw that line.
[01:32:35.160 --> 01:32:38.600]   I'm just saying you shouldn't be creeped out.
[01:32:38.600 --> 01:32:40.440]   Why are you being creeped out?
[01:32:40.440 --> 01:32:45.640]   If you want, if you creeped out, go move in a cabin in the woods because it's happening.
[01:32:45.640 --> 01:32:46.640]   Look at it happened to Carolina.
[01:32:46.640 --> 01:32:48.240]   You didn't have to announce she was moving.
[01:32:48.240 --> 01:32:49.240]   They just knew.
[01:32:49.240 --> 01:32:52.240]   I mean, but it's not Carolina.
[01:32:52.240 --> 01:32:53.360]   Did that feel creepy?
[01:32:53.360 --> 01:32:54.360]   I guess it might have.
[01:32:54.360 --> 01:32:56.760]   It didn't feel creepy.
[01:32:56.760 --> 01:33:02.440]   It was just annoying because I didn't need what I, you know, while they were advertising,
[01:33:02.440 --> 01:33:07.840]   it would have been much more useful if they told it how much of a nightmare moving Comcast
[01:33:07.840 --> 01:33:08.840]   would be.
[01:33:08.840 --> 01:33:13.040]   I think it's honest data.
[01:33:13.040 --> 01:33:15.160]   The data shows that people are creeped out by this.
[01:33:15.160 --> 01:33:16.160]   If you feel--
[01:33:16.160 --> 01:33:17.160]   Oh, I know they are.
[01:33:17.160 --> 01:33:18.160]   They say it all the time.
[01:33:18.160 --> 01:33:19.160]   Actually, I know.
[01:33:19.160 --> 01:33:22.560]   And not only that, they're voting with the way that they're using this Apple opt out.
[01:33:22.560 --> 01:33:25.640]   And most people are opting out of Facebook targeting.
[01:33:25.640 --> 01:33:27.520]   So yeah, I think that like largely people are.
[01:33:27.520 --> 01:33:29.800]   And I can have a discussion about whether they shouldn't be or not.
[01:33:29.800 --> 01:33:31.640]   They shouldn't be creeped out.
[01:33:31.640 --> 01:33:33.640]   That's just, people get creeped out in the same.
[01:33:33.640 --> 01:33:37.920]   They thought somebody was reading their mail when Gmail was advertising based on keyword
[01:33:37.920 --> 01:33:42.000]   engineering, and the thing that was creepy was they were anthropomorphizing it like,
[01:33:42.000 --> 01:33:43.000]   "Well, there's somebody reading--"
[01:33:43.000 --> 01:33:44.000]   Nobody's reading your mail.
[01:33:44.000 --> 01:33:45.000]   Sure.
[01:33:45.000 --> 01:33:48.480]   Well, okay, Leo, is there eventually a place where like online advertising tracks to the
[01:33:48.480 --> 01:33:51.880]   extent that you start to say, "Hey, maybe that's too much," or is every single thing?
[01:33:51.880 --> 01:33:53.200]   No, because it's just advertising.
[01:33:53.200 --> 01:33:54.680]   If you don't like it, ignore it.
[01:33:54.680 --> 01:33:58.840]   People, the real problem, I think, people don't like ads.
[01:33:58.840 --> 01:34:00.680]   They would just admit this.
[01:34:00.680 --> 01:34:03.520]   You guys, you don't want any ads.
[01:34:03.520 --> 01:34:05.000]   You don't want any ads.
[01:34:05.000 --> 01:34:07.200]   And people use ad blockers and all sorts of things.
[01:34:07.200 --> 01:34:08.200]   They don't want any ads.
[01:34:08.200 --> 01:34:09.200]   I don't want any ads.
[01:34:09.200 --> 01:34:13.400]   But it doesn't work that way because you're getting a free service that's ad supported
[01:34:13.400 --> 01:34:14.800]   like ours.
[01:34:14.800 --> 01:34:15.800]   We give people a choice.
[01:34:15.800 --> 01:34:18.680]   I mean, they can pay seven bucks a month and not get any ads.
[01:34:18.680 --> 01:34:21.840]   A tiny fraction of our audience does that.
[01:34:21.840 --> 01:34:26.080]   Far more probably just skip the ads or block the ads.
[01:34:26.080 --> 01:34:27.520]   Because I don't want any ads.
[01:34:27.520 --> 01:34:30.800]   I don't want to pay for anything, and I don't want any ads.
[01:34:30.800 --> 01:34:33.000]   I think that's a childish point of view.
[01:34:33.000 --> 01:34:37.000]   I think that's a straw man, Leo.
[01:34:37.000 --> 01:34:41.600]   Because that's not people, people are saying, they're not saying they don't want ads.
[01:34:41.600 --> 01:34:45.160]   The real question is, again, like how much tracking should there be?
[01:34:45.160 --> 01:34:50.960]   If someone is, if an advertiser is tracking my locations, you know, and I just walked into
[01:34:50.960 --> 01:34:56.000]   Shake Shack and Chick-fil-A sends me a message and says, "Get out of there.
[01:34:56.000 --> 01:34:57.000]   We're going to give you something."
[01:34:57.000 --> 01:34:58.000]   Is that creepy to you?
[01:34:58.000 --> 01:34:59.000]   No, it's for us.
[01:34:59.000 --> 01:35:01.920]   I think it's, you know, it's going to be a benefit.
[01:35:01.920 --> 01:35:02.920]   Maybe you go, "Oh, great.
[01:35:02.920 --> 01:35:03.920]   It's like that dollar at Chick-fil-A."
[01:35:03.920 --> 01:35:04.920]   I know.
[01:35:04.920 --> 01:35:09.440]   I think that specific instance is fine, but there are going to be weird situations.
[01:35:09.440 --> 01:35:10.440]   It's invasive.
[01:35:10.440 --> 01:35:11.440]   It's intrusive.
[01:35:11.440 --> 01:35:12.440]   I think that's...
[01:35:12.440 --> 01:35:13.440]   I don't know if it's fine.
[01:35:13.440 --> 01:35:16.920]   I mean, if you're a woman and who's seen, who knows that?
[01:35:16.920 --> 01:35:17.920]   I mean, Shake Shack.
[01:35:17.920 --> 01:35:18.920]   Yeah.
[01:35:18.920 --> 01:35:19.920]   Being normalized.
[01:35:19.920 --> 01:35:20.920]   It's weird.
[01:35:20.920 --> 01:35:21.920]   Right.
[01:35:21.920 --> 01:35:22.920]   Yeah.
[01:35:22.920 --> 01:35:24.680]   So for me, that's specific examples, okay?
[01:35:24.680 --> 01:35:28.480]   But like, yeah, let's say like, you know, you could see it would be creepy if you got an
[01:35:28.480 --> 01:35:31.920]   ad said, "Hey, I know you guys are trying to get pregnant right now.
[01:35:31.920 --> 01:35:32.920]   Maybe how about this?"
[01:35:32.920 --> 01:35:33.920]   There is that.
[01:35:33.920 --> 01:35:35.480]   That does come up.
[01:35:35.480 --> 01:35:36.480]   I know.
[01:35:36.480 --> 01:35:37.880]   I guess that is creepy.
[01:35:37.880 --> 01:35:41.600]   What if you walked into a Planned Parenthood and you get an ad trying to explain to you
[01:35:41.600 --> 01:35:43.120]   the harms of abortion?
[01:35:43.120 --> 01:35:44.120]   I mean, like...
[01:35:44.120 --> 01:35:47.480]   But I think all of this is really that you just don't like ads.
[01:35:47.480 --> 01:35:48.480]   I disagree.
[01:35:48.480 --> 01:35:50.560]   What do you want?
[01:35:50.560 --> 01:35:51.560]   What do you want ads?
[01:35:51.560 --> 01:35:58.720]   You want like, the ads should just be for barbed wire and concrete and chicken houses
[01:35:58.720 --> 01:35:59.720]   and I don't care.
[01:35:59.720 --> 01:36:00.720]   All right.
[01:36:00.720 --> 01:36:01.720]   I go to the map for this.
[01:36:01.720 --> 01:36:05.960]   I mean, Leo, think about what ads look like before all this targeting, right?
[01:36:05.960 --> 01:36:09.720]   Like, Madman, the whole show is them trying to make beautiful ads.
[01:36:09.720 --> 01:36:10.720]   It was all about...
[01:36:10.720 --> 01:36:13.360]   No, the whole show was trying to get you to smoke cigarettes.
[01:36:13.360 --> 01:36:14.880]   I mean, yes, yes.
[01:36:14.880 --> 01:36:17.160]   But like, okay, bad example again.
[01:36:17.160 --> 01:36:18.320]   I'm throwing out the worst one.
[01:36:18.320 --> 01:36:19.320]   And drink a lot of alcohol.
[01:36:19.320 --> 01:36:20.320]   Drink a lot of alcohol.
[01:36:20.320 --> 01:36:21.320]   I'll tell you what.
[01:36:21.320 --> 01:36:26.560]   And feel bad about your bad breath because your coworkers just didn't want to tell you.
[01:36:26.560 --> 01:36:31.680]   Advertising used to be nice to look at because they knew that the image and the story was
[01:36:31.680 --> 01:36:35.640]   going to be what was going to sell something, not the fact that they were targeting someone
[01:36:35.640 --> 01:36:37.200]   so granularly on the internet.
[01:36:37.200 --> 01:36:41.280]   And I think that we could get to a better era of advertising with a little less tracking,
[01:36:41.280 --> 01:36:45.160]   a little more focus on the creative and maybe we'll have less people hate ads because they'll
[01:36:45.160 --> 01:36:51.000]   actually be nice to look at and pleasant to consume unlike what we have today.
[01:36:51.000 --> 01:36:52.000]   I think...
[01:36:52.000 --> 01:36:53.560]   I do think is more than the ads.
[01:36:53.560 --> 01:36:58.560]   It is really about what else are you using that data for and who's buying it beyond
[01:36:58.560 --> 01:36:59.560]   advertisers?
[01:36:59.560 --> 01:37:00.560]   Well, that's my question.
[01:37:00.560 --> 01:37:01.560]   And nobody's ever...
[01:37:01.560 --> 01:37:03.200]   Yeah, I mean, so there's...
[01:37:03.200 --> 01:37:08.560]   Like people will say, "Well, what if insurers bought that and didn't insure you because you
[01:37:08.560 --> 01:37:10.480]   eat a lot of Dunkin' Donuts?"
[01:37:10.480 --> 01:37:11.480]   Right.
[01:37:11.480 --> 01:37:14.480]   Yeah, but that's not going to happen because the first thing insurers do when they check
[01:37:14.480 --> 01:37:17.640]   you out before they give you insurance is say, "Do you eat donuts?"
[01:37:17.640 --> 01:37:18.640]   They don't...
[01:37:18.640 --> 01:37:20.760]   And if you lie about it, then they don't pay off.
[01:37:20.760 --> 01:37:23.000]   So they don't need...
[01:37:23.000 --> 01:37:25.640]   I haven't seen that that's ever happened.
[01:37:25.640 --> 01:37:27.440]   There's some sort of insurance redlining because...
[01:37:27.440 --> 01:37:28.680]   It's about consent.
[01:37:28.680 --> 01:37:33.400]   It's about how much will they know about you without you actually saying it's okay to disclose
[01:37:33.400 --> 01:37:34.920]   that, right?
[01:37:34.920 --> 01:37:35.920]   And that's the problem.
[01:37:35.920 --> 01:37:39.040]   And the more we move that needle, then it's like...
[01:37:39.040 --> 01:37:45.320]   I think it's the feeling of companies and ads controlling you versus you controlling it.
[01:37:45.320 --> 01:37:47.800]   And that's the biggest fear is humans control.
[01:37:47.800 --> 01:37:54.440]   There is this skisie business of data brokers who are getting all this information, selling
[01:37:54.440 --> 01:38:00.440]   it to anybody who wants it, often law enforcement, collating information from a variety of sources
[01:38:00.440 --> 01:38:02.360]   to de-anonymize it.
[01:38:02.360 --> 01:38:08.360]   So there's this whole subcurrent of people doing this.
[01:38:08.360 --> 01:38:09.360]   And yet...
[01:38:09.360 --> 01:38:10.840]   I don't know.
[01:38:10.840 --> 01:38:13.440]   Maybe, again, I'm a cis white male in my 60s.
[01:38:13.440 --> 01:38:17.280]   So probably I'm never going to be harmed by this.
[01:38:17.280 --> 01:38:21.720]   Well, no, everyone can be harmed because if your data is being sold and used in another
[01:38:21.720 --> 01:38:24.320]   way, then what it was being...
[01:38:24.320 --> 01:38:25.760]   It's there, they're intended for.
[01:38:25.760 --> 01:38:26.760]   I'll tell you.
[01:38:26.760 --> 01:38:27.760]   That could be harmed by it.
[01:38:27.760 --> 01:38:31.880]   I sure got a lot of Medicare solicitations in the mail online everywhere else because
[01:38:31.880 --> 01:38:34.200]   they knew when my birthday was.
[01:38:34.200 --> 01:38:37.240]   And I ignored it because I didn't need to know that information.
[01:38:37.240 --> 01:38:39.120]   But that's how the world works.
[01:38:39.120 --> 01:38:41.640]   That didn't offend me.
[01:38:41.640 --> 01:38:44.720]   And it didn't feel creepy that people know how old I am.
[01:38:44.720 --> 01:38:50.800]   In fact, somebody took my birthday out of Wikipedia and I told the chatroom, "Put that
[01:38:50.800 --> 01:38:51.800]   back."
[01:38:51.800 --> 01:38:55.800]   Because it said, "He's 64 or 65.
[01:38:55.800 --> 01:38:56.800]   We don't know.
[01:38:56.800 --> 01:38:58.160]   Put my birthday in there.
[01:38:58.160 --> 01:39:00.360]   Who cares?"
[01:39:00.360 --> 01:39:01.560]   I'm being a little bit...
[01:39:01.560 --> 01:39:02.560]   I understand.
[01:39:02.560 --> 01:39:05.360]   I'm being a little bit provocative here.
[01:39:05.360 --> 01:39:08.320]   And maybe speaking a little more strongly than the other.
[01:39:08.320 --> 01:39:12.080]   But I often do wonder, "What is the real harm?
[01:39:12.080 --> 01:39:13.080]   What is the real...
[01:39:13.080 --> 01:39:17.640]   Get Carolina, give me the worst case scenario of people knowing what else..."
[01:39:17.640 --> 01:39:19.840]   I really don't care about D.A.D.'s per se.
[01:39:19.840 --> 01:39:25.000]   I do care about the amount of information that companies are going to have.
[01:39:25.000 --> 01:39:26.000]   I don't know.
[01:39:26.000 --> 01:39:32.320]   I don't know if it's about what is going to end up into a machine learning system that
[01:39:32.320 --> 01:39:37.600]   will grant mortgages to people.
[01:39:37.600 --> 01:39:38.600]   I don't know.
[01:39:38.600 --> 01:39:48.240]   It's the idea of generating so much data that is going to be used in a way that I don't
[01:39:48.240 --> 01:39:49.640]   know.
[01:39:49.640 --> 01:39:57.240]   How are you going to use all of that about me to make decisions about how you survey,
[01:39:57.240 --> 01:40:02.680]   not from an ad perspective, but actually about me as a person.
[01:40:02.680 --> 01:40:06.520]   That I think is the part that concerns me.
[01:40:06.520 --> 01:40:15.360]   And yet, I don't decide to be offline and I don't decide to not share some of the information.
[01:40:15.360 --> 01:40:18.640]   It does, it does concern me.
[01:40:18.640 --> 01:40:26.640]   I do draw a line on some of the public information that I put out there.
[01:40:26.640 --> 01:40:28.480]   I don't know.
[01:40:28.480 --> 01:40:37.080]   It's knowing if I had a COVID test and a vaccine and how you use that information for insurance
[01:40:37.080 --> 01:40:38.080]   purposes or...
[01:40:38.080 --> 01:40:39.080]   Well, here's an example.
[01:40:39.080 --> 01:40:41.280]   Well, that's for public health and policy.
[01:40:41.280 --> 01:40:47.760]   Here's an example, 23andMe, which has my genetic material and a lot of your genetic material,
[01:40:47.760 --> 01:40:52.720]   that we were an advertiser back in the day, has said that they are sharing information
[01:40:52.720 --> 01:41:00.160]   with cancer researchers now because it might be helpful to have all that information so
[01:41:00.160 --> 01:41:04.440]   that they can come up with a cure.
[01:41:04.440 --> 01:41:05.440]   Right?
[01:41:05.440 --> 01:41:09.000]   See, I rather an opt-in, like once you give your information...
[01:41:09.000 --> 01:41:10.000]   I agree.
[01:41:10.000 --> 01:41:14.920]   And then you have an alert that says, "Hey, because you've used the service, you can opt-in
[01:41:14.920 --> 01:41:16.160]   for it to be used for this.
[01:41:16.160 --> 01:41:17.160]   Are you okay with that?"
[01:41:17.160 --> 01:41:20.960]   And being very specific and you get to research and they're transparent.
[01:41:20.960 --> 01:41:24.320]   And then every time it's being used for a different thing, you get to opt-in.
[01:41:24.320 --> 01:41:27.720]   It's not like you're selling yourself and now they have just...
[01:41:27.720 --> 01:41:33.560]   Yeah, that's slightly different because I paid them a lot of money for my kind of spurious
[01:41:33.560 --> 01:41:37.480]   DNA results.
[01:41:37.480 --> 01:41:39.880]   And they kept the spit.
[01:41:39.880 --> 01:41:45.720]   On the other hand, there's a real benefit to working with AstraZeneca or whoever to create
[01:41:45.720 --> 01:41:47.360]   new medicines and so forth.
[01:41:47.360 --> 01:41:50.440]   But you should be able to choose that because that's you.
[01:41:50.440 --> 01:41:51.440]   That's your information.
[01:41:51.440 --> 01:41:52.440]   You know what?
[01:41:52.440 --> 01:41:54.280]   I can't say that they didn't offer me that choice.
[01:41:54.280 --> 01:41:57.680]   I probably in the fine print agreed to that.
[01:41:57.680 --> 01:41:58.680]   Probably.
[01:41:58.680 --> 01:42:01.520]   But that's the thing is it's fine print and it's even more obvious, right?
[01:42:01.520 --> 01:42:02.520]   Right.
[01:42:02.520 --> 01:42:03.520]   Yeah.
[01:42:03.520 --> 01:42:08.160]   And even though there's a great societal benefit, there may be a cure for cancer that comes
[01:42:08.160 --> 01:42:09.160]   out of this.
[01:42:09.160 --> 01:42:10.160]   You still say it's...
[01:42:10.160 --> 01:42:12.600]   Yeah, they should ask.
[01:42:12.600 --> 01:42:21.280]   Yeah, because it's your choice of how you want to be involved in these types of things.
[01:42:21.280 --> 01:42:22.280]   Yeah.
[01:42:22.280 --> 01:42:23.280]   Actually...
[01:42:23.280 --> 01:42:25.560]   Because just because they're using...
[01:42:25.560 --> 01:42:29.960]   If some people are using it for good, how many other people could be using it not for
[01:42:29.960 --> 01:42:33.120]   good and have these companies built trust?
[01:42:33.120 --> 01:42:34.120]   Yeah.
[01:42:34.120 --> 01:42:35.120]   Do we...
[01:42:35.120 --> 01:42:37.560]   Have they always been amazing and like in good faith?
[01:42:37.560 --> 01:42:38.560]   Can we say that?
[01:42:38.560 --> 01:42:39.560]   Yeah.
[01:42:39.560 --> 01:42:40.560]   No.
[01:42:40.560 --> 01:42:44.320]   Well, and DNA information is pretty...
[01:42:44.320 --> 01:42:45.400]   Is different in one respect.
[01:42:45.400 --> 01:42:47.800]   I can't change my DNA.
[01:42:47.800 --> 01:42:49.440]   And my kids share it.
[01:42:49.440 --> 01:42:50.440]   So I'm sharing information.
[01:42:50.440 --> 01:42:52.120]   But what if they could like then replicate you?
[01:42:52.120 --> 01:42:53.120]   You know, it's like...
[01:42:53.120 --> 01:42:55.360]   I would love it if they could replicate me.
[01:42:55.360 --> 01:42:56.360]   I hope they can.
[01:42:56.360 --> 01:42:59.280]   I was like, what do they do with that after you're gone, right?
[01:42:59.280 --> 01:43:03.040]   I want to keep the Twit Empire going forever.
[01:43:03.040 --> 01:43:04.760]   Anyway, interesting questions.
[01:43:04.760 --> 01:43:07.480]   We're going to take a little break.
[01:43:07.480 --> 01:43:09.680]   What if they could replicate you?
[01:43:09.680 --> 01:43:11.400]   Sure, if you could put your...
[01:43:11.400 --> 01:43:12.400]   You don't know what they could do.
[01:43:12.400 --> 01:43:14.480]   If you could pour your mind...
[01:43:14.480 --> 01:43:15.480]   You're too young yet.
[01:43:15.480 --> 01:43:16.680]   You're not worried about this.
[01:43:16.680 --> 01:43:20.360]   If you could pour your mind into a jar...
[01:43:20.360 --> 01:43:21.360]   Carolina goes...
[01:43:21.360 --> 01:43:22.360]   No.
[01:43:22.360 --> 01:43:23.360]   No.
[01:43:23.360 --> 01:43:24.360]   No.
[01:43:24.360 --> 01:43:26.920]   My husband would tell you that the world cannot take two of me.
[01:43:26.920 --> 01:43:27.920]   So...
[01:43:27.920 --> 01:43:31.000]   Live forever.
[01:43:31.000 --> 01:43:32.000]   Would it be you?
[01:43:32.000 --> 01:43:37.440]   No, it might just be something that acts like you would have acted, but it's not really.
[01:43:37.440 --> 01:43:38.600]   I don't know.
[01:43:38.600 --> 01:43:39.600]   That's really weird.
[01:43:39.600 --> 01:43:41.040]   You've never thought about this?
[01:43:41.040 --> 01:43:42.040]   I have.
[01:43:42.040 --> 01:43:43.520]   I mean, I watch The Good Place, you know.
[01:43:43.520 --> 01:43:45.680]   Yeah, I think about it all the time.
[01:43:45.680 --> 01:43:47.640]   Like, when is that going to happen?
[01:43:47.640 --> 01:43:49.000]   I hope it's soon.
[01:43:49.000 --> 01:43:53.120]   I mean, Elon Musk probably has the answer.
[01:43:53.120 --> 01:44:00.960]   I should mention, if you hate tracking and you don't want to be tracked, we are an ad-supported
[01:44:00.960 --> 01:44:01.960]   network.
[01:44:01.960 --> 01:44:02.960]   We're free.
[01:44:02.960 --> 01:44:03.960]   I'm very proud of that.
[01:44:03.960 --> 01:44:04.960]   It means it's democratic.
[01:44:04.960 --> 01:44:08.720]   Anybody can listen to our shows or participate with the network in any way they want.
[01:44:08.720 --> 01:44:10.240]   We have an IRC that's open to all.
[01:44:10.240 --> 01:44:11.440]   We have a Twit forums.
[01:44:11.440 --> 01:44:12.880]   We have a Mastodon instance.
[01:44:12.880 --> 01:44:15.160]   All that's free.
[01:44:15.160 --> 01:44:21.600]   But if you, A, would like to support us a little bit, or B, don't want to have any tracking,
[01:44:21.600 --> 01:44:23.440]   and C, don't want to hear any ads.
[01:44:23.440 --> 01:44:25.280]   There is an option for you.
[01:44:25.280 --> 01:44:27.320]   I do wish more people would join Club Twit.
[01:44:27.320 --> 01:44:29.320]   I love this idea.
[01:44:29.320 --> 01:44:34.120]   In fact, if we could, I would just say, let's just be an ad-free network, and you pay for
[01:44:34.120 --> 01:44:35.120]   it.
[01:44:35.120 --> 01:44:37.040]   But that hasn't worked out so far.
[01:44:37.040 --> 01:44:38.040]   But join Club Twit.
[01:44:38.040 --> 01:44:41.520]   Seven bucks a month gets you all of our shows ad-free.
[01:44:41.520 --> 01:44:44.320]   It gets you access to our Discord, which is incredible.
[01:44:44.320 --> 01:44:45.560]   I mean, really a lot of fun.
[01:44:45.560 --> 01:44:46.560]   It's a great community.
[01:44:46.560 --> 01:44:48.600]   I'm a big fan of that.
[01:44:48.600 --> 01:44:54.080]   And you get the Twit Plus feed, which has stuff like, well, Burke, spraying Christmas all
[01:44:54.080 --> 01:44:57.480]   over the studio on Tuesday, things like that.
[01:44:57.480 --> 01:45:04.600]   All you have to do is go to twit.tv/clubtwit, and you can find out more.
[01:45:04.600 --> 01:45:11.520]   And by the way, we also have corporate memberships, and we got our first corporate membership.
[01:45:11.520 --> 01:45:14.640]   I'm so thrilled.
[01:45:14.640 --> 01:45:22.080]   If you want to know more about corporate memberships, when you go to twit.tv/clubtwit, there's
[01:45:22.080 --> 01:45:23.920]   at the bottom there, there's more.
[01:45:23.920 --> 01:45:29.360]   But I want to really thank David Hickman and the folks at Resource Management Concepts,
[01:45:29.360 --> 01:45:31.120]   RMCweb.com.
[01:45:31.120 --> 01:45:36.360]   They bought 300 corporate memberships for their IT and security team, because they love security
[01:45:36.360 --> 01:45:37.360]   now.
[01:45:37.360 --> 01:45:38.360]   They can listen ad-free.
[01:45:38.360 --> 01:45:39.360]   Thank you.
[01:45:39.360 --> 01:45:41.040]   That's a great way to support what we do.
[01:45:41.040 --> 01:45:44.200]   And avoid ads and tracking, see?
[01:45:44.200 --> 01:45:49.160]   It seems like a fair, you know, less than a super duper frappuccino a month, and you
[01:45:49.160 --> 01:45:51.360]   get all of that.
[01:45:51.360 --> 01:45:56.280]   Twit.tv/clubtwit.
[01:45:56.280 --> 01:45:58.840]   Now if you are a member, you didn't hear that ad, and you're not going to hear this
[01:45:58.840 --> 01:45:59.840]   ad.
[01:45:59.840 --> 01:46:00.840]   And you know what?
[01:46:00.840 --> 01:46:02.000]   You should be sad, because this is a good product.
[01:46:02.000 --> 01:46:04.000]   It's called Udacity.
[01:46:04.000 --> 01:46:10.520]   Udacity is actually changing the world, started some years ago, about a decade ago, by a Googler
[01:46:10.520 --> 01:46:14.080]   who realized that a lot of the people coming and applying for jobs at Google, even though
[01:46:14.080 --> 01:46:19.400]   they had computer science degrees and master's degrees, didn't actually have the raw skills
[01:46:19.400 --> 01:46:21.040]   Google needed.
[01:46:21.040 --> 01:46:27.760]   They founded Udacity with one goal, to give people an online education geared toward taking
[01:46:27.760 --> 01:46:32.320]   your technology to the next level, to getting the information about the career you want,
[01:46:32.320 --> 01:46:35.560]   giving you the skills for that career, and then getting the job.
[01:46:35.560 --> 01:46:36.720]   And Udacity does it.
[01:46:36.720 --> 01:46:41.600]   They're here to encourage your career growth with their cutting-edge nanodegree programs.
[01:46:41.600 --> 01:46:45.680]   You could see the references on the website like the Udacity graduate who was working
[01:46:45.680 --> 01:46:51.920]   at a local Nigerian airline, lost his job because of COVID, decided to turn things around.
[01:46:51.920 --> 01:46:57.120]   He applied for the Machine Learning Scholarship Program.
[01:46:57.120 --> 01:47:01.440]   And for Microsoft Azure, within six months, he transitioned into a career as a data analyst.
[01:47:01.440 --> 01:47:06.040]   I know so many people who are working now, Microsoft, Google, Amazon, and elsewhere,
[01:47:06.040 --> 01:47:07.800]   thanks to Udacity.
[01:47:07.800 --> 01:47:11.280]   Check out some of the Udacity nanodegree programs.
[01:47:11.280 --> 01:47:14.280]   Data engineering, very hot right now.
[01:47:14.280 --> 01:47:16.360]   You could learn how to be a product manager.
[01:47:16.360 --> 01:47:19.120]   Every, you know, companies are desperate for this.
[01:47:19.120 --> 01:47:25.240]   Data analyst, data scientist, how about programming for data science with Python?
[01:47:25.240 --> 01:47:27.640]   Or things that are even more cutting edge.
[01:47:27.640 --> 01:47:35.600]   Perhaps you'd like a nanodegree program in AI, or flying cars, an autonomous flight engineer,
[01:47:35.600 --> 01:47:37.240]   Machine Learning Engineer.
[01:47:37.240 --> 01:47:41.120]   One of the things that's great about Udacity is they work on these programs with industry
[01:47:41.120 --> 01:47:44.800]   leaders like Microsoft and Google and IBM and Amazon.
[01:47:44.800 --> 01:47:49.320]   So in fact, the teachers you're going to get are team leads at these companies.
[01:47:49.320 --> 01:47:53.520]   So you're going to get the kind of knowledge they know you need from experts who are working
[01:47:53.520 --> 01:47:55.720]   in that field every day.
[01:47:55.720 --> 01:47:59.280]   I also think very important that their courses are project based.
[01:47:59.280 --> 01:48:00.280]   This is active learning.
[01:48:00.280 --> 01:48:01.280]   You don't just learn.
[01:48:01.280 --> 01:48:03.600]   You actually have to build projects.
[01:48:03.600 --> 01:48:04.880]   You get graded on projects.
[01:48:04.880 --> 01:48:10.200]   You get homework and projects will be reviewed by qualified professionals in the field.
[01:48:10.200 --> 01:48:12.880]   It doesn't just test your knowledge and solidify your knowledge.
[01:48:12.880 --> 01:48:18.800]   It actually helps you build a portfolio, which makes it easier to get that job because you've
[01:48:18.800 --> 01:48:21.040]   got on GitHub, you've got, oh, I built this, I built that.
[01:48:21.040 --> 01:48:22.280]   And that really makes a difference.
[01:48:22.280 --> 01:48:23.280]   It really does.
[01:48:23.280 --> 01:48:25.440]   They're going to help you do that too.
[01:48:25.440 --> 01:48:26.440]   But you're not alone.
[01:48:26.440 --> 01:48:30.040]   They've got a great mentorship program, 24/7 mentors.
[01:48:30.040 --> 01:48:34.840]   By the way, 24/7 because Udacity is truly global all over the world.
[01:48:34.840 --> 01:48:35.840]   People are doing this.
[01:48:35.840 --> 01:48:36.840]   They have day jobs.
[01:48:36.840 --> 01:48:38.240]   They want to get a better job.
[01:48:38.240 --> 01:48:42.000]   You could go five to 10 hours a week, learn at your own pace, graduate, as well as three
[01:48:42.000 --> 01:48:44.560]   months get that job right away.
[01:48:44.560 --> 01:48:46.360]   I'll give you, and they have some great scholarships too.
[01:48:46.360 --> 01:48:49.000]   You should check those out right at the bottom of the page.
[01:48:49.000 --> 01:48:51.640]   Under resources, you'll see scholarships.
[01:48:51.640 --> 01:48:56.400]   There's one from an organization, great organization, 110 Blacks in Technology.
[01:48:56.400 --> 01:49:01.920]   They teamed up with Udacity to offer part-time online tech scholarships for Black Americans
[01:49:01.920 --> 01:49:04.520]   who don't have a four-year degree.
[01:49:04.520 --> 01:49:09.680]   Up to 2,000 full scholarships for qualified applicants, they're available.
[01:49:09.680 --> 01:49:11.320]   Apply today, Udacity.com.
[01:49:11.320 --> 01:49:16.280]   Just again, just go to those scholarships under resources at the bottom of the page.
[01:49:16.280 --> 01:49:19.240]   If you're a business, Udacity has an enterprise section.
[01:49:19.240 --> 01:49:23.440]   You can upskill your entire workforce with Udacity.
[01:49:23.440 --> 01:49:26.200]   Very popular for teams.
[01:49:26.200 --> 01:49:29.920]   Learn more at the enterprise section of Udacity's website today.
[01:49:29.920 --> 01:49:35.360]   So bottom line, get the in-demand tech skills you need to advance your career, Udacity.
[01:49:35.360 --> 01:49:41.000]   UDA, CITYUDacity.com/twitudacity.com/twit.
[01:49:41.000 --> 01:49:47.760]   We thank you so much for their support of our show.
[01:49:47.760 --> 01:49:48.760]   Let's see here.
[01:49:48.760 --> 01:49:49.760]   Back to work.
[01:49:49.760 --> 01:49:53.240]   The advertising segment is over.
[01:49:53.240 --> 01:50:00.280]   If you start to see more and more reports of a Tesla's full self-driving vehicles going
[01:50:00.280 --> 01:50:07.320]   off the road, hitting things, crashing, you got to wonder, autonomy is still quite a ways
[01:50:07.320 --> 01:50:08.320]   off.
[01:50:08.320 --> 01:50:14.160]   Mercedes-Benz has received the world's first approval for their automated driving system.
[01:50:14.160 --> 01:50:15.760]   Don't get too excited.
[01:50:15.760 --> 01:50:22.440]   It's only for certain areas of the Autobahn in the new S-Class and EQS sedans.
[01:50:22.440 --> 01:50:27.400]   You have to go in these geofence stretches of highway in the Autobahn.
[01:50:27.400 --> 01:50:28.920]   This is really the deal killer.
[01:50:28.920 --> 01:50:34.320]   As far as I'm concerned, you can only go 37 miles an hour.
[01:50:34.320 --> 01:50:35.840]   That's confidence, right?
[01:50:35.840 --> 01:50:39.680]   I think you're actually more of a hazard on the Autobahn if you're going 37 miles an
[01:50:39.680 --> 01:50:42.680]   hour than not.
[01:50:42.680 --> 01:50:47.400]   Really these are designed for stop-and-go traffic, heavy traffic in geofence areas of
[01:50:47.400 --> 01:50:48.400]   the highway.
[01:50:48.400 --> 01:50:52.320]   But they got government approval.
[01:50:52.320 --> 01:51:00.400]   This is the first level three self-driving system that actually satisfies the Germany's
[01:51:00.400 --> 01:51:04.200]   federal motor transport authority that the system is safe.
[01:51:04.200 --> 01:51:07.040]   I don't know if they'd approve Tesla's full self-driving.
[01:51:07.040 --> 01:51:10.160]   I have a feeling not.
[01:51:10.160 --> 01:51:15.480]   I don't need some confident D-bag on the road thinking, "I'm in a self-driving car."
[01:51:15.480 --> 01:51:21.720]   I'm the person in front of them or her or them.
[01:51:21.720 --> 01:51:22.720]   That's the thing.
[01:51:22.720 --> 01:51:27.200]   You feel so confident because of this technology and then lo and behold.
[01:51:27.200 --> 01:51:28.200]   Guess what?
[01:51:28.200 --> 01:51:32.560]   You're in the beta program too, Shira.
[01:51:32.560 --> 01:51:34.800]   That guy behind you, he's not steering.
[01:51:34.800 --> 01:51:35.800]   Good luck.
[01:51:35.800 --> 01:51:36.800]   Right?
[01:51:36.800 --> 01:51:41.160]   And then like that one time they fall asleep, they're on call.
[01:51:41.160 --> 01:51:45.440]   Maybe they're, I don't know, making out because they're in a self-driving car.
[01:51:45.440 --> 01:51:50.360]   Or here's another nice feature of the Tesla playing a video game.
[01:51:50.360 --> 01:51:51.440]   Yeah, no.
[01:51:51.440 --> 01:51:52.840]   We don't need, we do enough of that.
[01:51:52.840 --> 01:51:54.840]   You don't need to do that while you're driving.
[01:51:54.840 --> 01:51:57.240]   Like that does not help anyone.
[01:51:57.240 --> 01:51:59.840]   Drivers can play video games while the car is moving.
[01:51:59.840 --> 01:52:06.240]   Most of the time when you have these kind of big displays, you have a cutoff that says
[01:52:06.240 --> 01:52:07.240]   what you're driving.
[01:52:07.240 --> 01:52:08.880]   You can't be doing that.
[01:52:08.880 --> 01:52:11.280]   Tesla says this is in Model 3's.
[01:52:11.280 --> 01:52:12.760]   Oh, that's not for the driver.
[01:52:12.760 --> 01:52:13.760]   That's for the passenger.
[01:52:13.760 --> 01:52:19.240]   You know, they only have one big screen in the middle.
[01:52:19.240 --> 01:52:24.400]   The automaker added the games in an over the air software update this summer.
[01:52:24.400 --> 01:52:32.000]   But they can be played by a passenger in fuel view of the driver or by the driver.
[01:52:32.000 --> 01:52:34.680]   So either way, it's kind of distracting.
[01:52:34.680 --> 01:52:36.480]   You know, I feel like Elon just says.
[01:52:36.480 --> 01:52:38.880]   The passenger doesn't have a phone where they can't.
[01:52:38.880 --> 01:52:39.880]   Yeah, come on.
[01:52:39.880 --> 01:52:41.880]   Yeah, that's true.
[01:52:41.880 --> 01:52:42.880]   Yeah.
[01:52:42.880 --> 01:52:48.760]   Tesla and its chief executive, Elon Musk did not respond to several queries from the New
[01:52:48.760 --> 01:52:52.680]   York Times about the video games and whether they could jeopardize safety.
[01:52:52.680 --> 01:52:55.280]   Elon's very available if you want to talk about Dogecoin.
[01:52:55.280 --> 01:52:57.040]   But don't ask him about safety.
[01:52:57.040 --> 01:53:01.520]   It reminds me of the press conference he did where he throws the rock at the car.
[01:53:01.520 --> 01:53:02.520]   The cyber truck.
[01:53:02.520 --> 01:53:04.600]   Smashes the window.
[01:53:04.600 --> 01:53:11.640]   We were talking about that today actually with Samable Sam at the cyber truck.
[01:53:11.640 --> 01:53:15.320]   Yeah, was supposed to have bulletproof windows.
[01:53:15.320 --> 01:53:20.600]   And so the one of Elon's employees comes out with a heavy metal kit.
[01:53:20.600 --> 01:53:21.800]   Elon did it too.
[01:53:21.800 --> 01:53:23.440]   He was like, are you sure you want to do that?
[01:53:23.440 --> 01:53:25.480]   He's like, yeah, let's do this.
[01:53:25.480 --> 01:53:26.560]   He throws it against the window.
[01:53:26.560 --> 01:53:28.040]   It smashes.
[01:53:28.040 --> 01:53:33.560]   And then Elon has to continue his presentation for another hour in front of a smashed window.
[01:53:33.560 --> 01:53:35.400]   Wasn't the best look.
[01:53:35.400 --> 01:53:38.760]   The cyber truck we actually just we were to the reason we're talking about this is the
[01:53:38.760 --> 01:53:47.080]   there was a tweet somebody was flying a drone over the Tesla factory and saw a new model
[01:53:47.080 --> 01:53:56.680]   of the cyber truck in the parking lot that originally the the cyber truck didn't have
[01:53:56.680 --> 01:53:57.680]   side mirrors.
[01:53:57.680 --> 01:54:02.560]   It turns out it's a minor thing, but the National Highway Transportation Safety Administration
[01:54:02.560 --> 01:54:05.960]   requires that you have side mirrors.
[01:54:05.960 --> 01:54:07.320]   So they put those on there.
[01:54:07.320 --> 01:54:13.760]   And also oddly was lacking in its original design a windshield wiper.
[01:54:13.760 --> 01:54:19.440]   Now at least according to the drone photo, it has a like a three foot long, one three
[01:54:19.440 --> 01:54:23.280]   foot long windshield wiper that covers the whole thing.
[01:54:23.280 --> 01:54:24.440]   Is are there two?
[01:54:24.440 --> 01:54:26.040]   I thought it was just one.
[01:54:26.040 --> 01:54:31.240]   Oh, there's two blades because nobody sells windshield wiper blades that are three feet
[01:54:31.240 --> 01:54:32.240]   long.
[01:54:32.240 --> 01:54:34.680]   So you have two long blades.
[01:54:34.680 --> 01:54:36.600]   All right.
[01:54:36.600 --> 01:54:37.600]   I'm sorry.
[01:54:37.600 --> 01:54:39.200]   I shouldn't mock Tesla.
[01:54:39.200 --> 01:54:42.040]   But no, it's okay.
[01:54:42.040 --> 01:54:43.040]   It's easy to mock.
[01:54:43.040 --> 01:54:44.880]   Here's the yeah, it is.
[01:54:44.880 --> 01:54:48.960]   Well, when you look at this, this is not nearly as aggressive and ugly as the original
[01:54:48.960 --> 01:54:49.960]   cyber truck.
[01:54:49.960 --> 01:54:55.400]   Although I imagine there's a lot of people who will want to buy this the same kind of
[01:54:55.400 --> 01:54:59.800]   yeah, didn't it people buy hummers and things like that.
[01:54:59.800 --> 01:55:00.800]   That's a beautiful truck.
[01:55:00.800 --> 01:55:01.800]   You like that?
[01:55:01.800 --> 01:55:02.800]   Are you going to get one Alex?
[01:55:02.800 --> 01:55:03.800]   It's interesting.
[01:55:03.800 --> 01:55:06.960]   I didn't get one, but I would get one.
[01:55:06.960 --> 01:55:07.960]   Yeah.
[01:55:07.960 --> 01:55:08.960]   If I live.
[01:55:08.960 --> 01:55:09.960]   Look at the size of that windshield wiper.
[01:55:09.960 --> 01:55:12.160]   Oh, it's glorious.
[01:55:12.160 --> 01:55:16.120]   Part of the problem is unlike most cars, whichever rectangular screens, this has a giant square
[01:55:16.120 --> 01:55:18.080]   screen, a wind screen.
[01:55:18.080 --> 01:55:21.080]   So you know, you need a big windshield wiper.
[01:55:21.080 --> 01:55:22.080]   I don't know.
[01:55:22.080 --> 01:55:25.960]   I just think the angles, the metal, the windows.
[01:55:25.960 --> 01:55:26.960]   It's kind of cool.
[01:55:26.960 --> 01:55:27.960]   Yeah.
[01:55:27.960 --> 01:55:29.760]   Or it's a nice looking car.
[01:55:29.760 --> 01:55:30.760]   Nice.
[01:55:30.760 --> 01:55:31.760]   Look at it.
[01:55:31.760 --> 01:55:33.520]   A little trying window type of thing.
[01:55:33.520 --> 01:55:34.520]   Yeah.
[01:55:34.520 --> 01:55:35.520]   Don't throw it.
[01:55:35.520 --> 01:55:36.520]   It's like Tesla porn.
[01:55:36.520 --> 01:55:37.520]   It's Tesla porn.
[01:55:37.520 --> 01:55:38.520]   Don't throw a car in a ball at it.
[01:55:38.520 --> 01:55:39.520]   Yeah.
[01:55:39.520 --> 01:55:40.520]   Yeah.
[01:55:40.520 --> 01:55:41.520]   Look, I also.
[01:55:41.520 --> 01:55:42.520]   Yeah.
[01:55:42.520 --> 01:55:43.520]   In fairness.
[01:55:43.520 --> 01:55:47.160]   Do a roving the do kind of ruin the look though.
[01:55:47.160 --> 01:55:49.600]   Well, they say you can remove them.
[01:55:49.600 --> 01:55:51.520]   They're magnetically attached.
[01:55:51.520 --> 01:55:53.520]   Oh, and I'll grow.
[01:55:53.520 --> 01:55:59.520]   Also, you would just look kind of cool driving one of those things around people will look.
[01:55:59.520 --> 01:56:00.520]   Sure.
[01:56:00.520 --> 01:56:01.520]   Yeah.
[01:56:01.520 --> 01:56:02.520]   It's fun.
[01:56:02.520 --> 01:56:05.120]   I'll bounce off of it except for one of those cannonballs.
[01:56:05.120 --> 01:56:07.120]   Don't throw cannonballs at it.
[01:56:07.120 --> 01:56:08.120]   Nope.
[01:56:08.120 --> 01:56:12.880]   Otherwise, you got, I mean, it would be great for driving in New York actually because
[01:56:12.880 --> 01:56:15.720]   like everyone has to get out of the way.
[01:56:15.720 --> 01:56:16.720]   Yeah.
[01:56:16.720 --> 01:56:20.320]   York, a lot of mountain ranges in New York for the big truck.
[01:56:20.320 --> 01:56:22.320]   No, but okay.
[01:56:22.320 --> 01:56:23.800]   This is everybody knows this.
[01:56:23.800 --> 01:56:27.280]   People who buy those off-road vehicles, they're driving down the street.
[01:56:27.280 --> 01:56:28.280]   They're not going.
[01:56:28.280 --> 01:56:30.320]   The best is the pickup trucks.
[01:56:30.320 --> 01:56:31.320]   Yeah.
[01:56:31.320 --> 01:56:32.320]   It's true.
[01:56:32.320 --> 01:56:37.520]   They're not using it to like put stuff into like like renovations or put stuff into the
[01:56:37.520 --> 01:56:38.520]   bag.
[01:56:38.520 --> 01:56:40.320]   Other people around here do that.
[01:56:40.320 --> 01:56:41.320]   Kickstarter.
[01:56:41.320 --> 01:56:47.320]   I don't know what could possibly grow wrong is moving its crowdfunding platform to blockchain.
[01:56:47.320 --> 01:56:51.240]   It's creating a new company develop and distribute the technology.
[01:56:51.240 --> 01:56:56.320]   They will reveal this on Wednesday.
[01:56:56.320 --> 01:56:59.920]   A Kickstarter will unveil.
[01:56:59.920 --> 01:57:01.520]   This is from Bloomberg.
[01:57:01.520 --> 01:57:02.520]   It will unveil.
[01:57:02.520 --> 01:57:07.440]   A project will merge the two worlds, hatching a standalone company to build a crowdfunding
[01:57:07.440 --> 01:57:12.040]   system much like Kickstarter's, but based on blockchain technology.
[01:57:12.040 --> 01:57:15.400]   You know, you just throw the word in blockchain and it suddenly it's fresh and new.
[01:57:15.400 --> 01:57:17.720]   I don't know how that changes things.
[01:57:17.720 --> 01:57:20.720]   New company doesn't have a name as far as we know.
[01:57:20.720 --> 01:57:25.680]   They're going to transition its current site to the new protocol sometime next year.
[01:57:25.680 --> 01:57:29.440]   Behind the scenes, it shouldn't affect that people use the site.
[01:57:29.440 --> 01:57:34.560]   You don't have to suddenly use Bitcoin or anything.
[01:57:34.560 --> 01:57:42.840]   I'm banned from Kickstarter because I invested far too many projects that have never.
[01:57:42.840 --> 01:57:48.040]   Sometimes I'll get, sometimes I'll get like a thing in the mail.
[01:57:48.040 --> 01:57:52.680]   I don't remember buying like I got this backpack for photographers or something.
[01:57:52.680 --> 01:57:57.600]   It turns out I'd give it was a Kickstarter project from years ago that they finally made
[01:57:57.600 --> 01:57:58.600]   something.
[01:57:58.600 --> 01:57:59.600]   It was awful.
[01:57:59.600 --> 01:58:01.240]   It was hilarious.
[01:58:01.240 --> 01:58:03.440]   I never got toots the unicorn.
[01:58:03.440 --> 01:58:08.400]   Somebody in the chat room mashed potato reminding me of that.
[01:58:08.400 --> 01:58:13.320]   That was a rainbow farting unicorn.
[01:58:13.320 --> 01:58:16.600]   No, never got that.
[01:58:16.600 --> 01:58:19.400]   You know, I kind of wish I had the idea.
[01:58:19.400 --> 01:58:22.600]   I should log on and see what's going on with the toots project.
[01:58:22.600 --> 01:58:27.400]   I wonder that it didn't get it's fundraising goal.
[01:58:27.400 --> 01:58:29.800]   No, I think it did, but I think it did get it's funded.
[01:58:29.800 --> 01:58:30.800]   That's the thing.
[01:58:30.800 --> 01:58:31.800]   It got its goal.
[01:58:31.800 --> 01:58:32.800]   But they still didn't make it.
[01:58:32.800 --> 01:58:35.920]   There's no requirement that you make the thing.
[01:58:35.920 --> 01:58:37.480]   Let me see.
[01:58:37.480 --> 01:58:45.920]   All of the things I've purchased on Kickstarter, the solo V2 safety net against phishing, the
[01:58:45.920 --> 01:58:49.160]   phone case that does more with zero bulk.
[01:58:49.160 --> 01:58:50.960]   The UV, we did get that, John.
[01:58:50.960 --> 01:58:51.960]   Oh my God.
[01:58:51.960 --> 01:58:53.120]   The UV mask.
[01:58:53.120 --> 01:58:54.360]   You never got yours, John?
[01:58:54.360 --> 01:58:58.000]   All day active UV, air purification, face mask.
[01:58:58.000 --> 01:58:59.000]   You can have mine.
[01:58:59.000 --> 01:59:02.520]   It's so heavy, like your head baths down.
[01:59:02.520 --> 01:59:07.480]   I did get the hygiene hand anti-microbial brass EDC door opener.
[01:59:07.480 --> 01:59:12.040]   But the only problem with that is it took them so long to make it that long before I
[01:59:12.040 --> 01:59:13.440]   got it, I saw people using this.
[01:59:13.440 --> 01:59:18.280]   Oh yeah, it was on eBay for a buck because they were copied immediately.
[01:59:18.280 --> 01:59:23.880]   Where is toots the unicorn?
[01:59:23.880 --> 01:59:28.280]   I did get my F lashes, fun interactive LED eyelashes.
[01:59:28.280 --> 01:59:31.480]   I haven't had the nerve to wear those yet.
[01:59:31.480 --> 01:59:35.880]   Maybe they took toots the unicorn off my list because I don't think it's ever going
[01:59:35.880 --> 01:59:36.880]   to happen.
[01:59:36.880 --> 01:59:43.280]   The idea was if you could tie it into Twitter, oh yeah, here was a good investment.
[01:59:43.280 --> 01:59:49.840]   My very first Kickstarter decentralized the web with diaspora, fully funded.
[01:59:49.840 --> 01:59:51.320]   This was going to replace Facebook.
[01:59:51.320 --> 01:59:53.360]   Remember that?
[01:59:53.360 --> 01:59:54.360]   I do remember that.
[01:59:54.360 --> 02:00:00.400]   2013, $200,000 they raised.
[02:00:00.400 --> 02:00:02.520]   By the way, you can go there.
[02:00:02.520 --> 02:00:03.520]   It's there.
[02:00:03.520 --> 02:00:05.120]   It's a website.
[02:00:05.120 --> 02:00:10.960]   Anyway, maybe blockchain will make it better.
[02:00:10.960 --> 02:00:12.760]   Let's see.
[02:00:12.760 --> 02:00:16.240]   It's such an optimist.
[02:00:16.240 --> 02:00:18.560]   Michael Strahan wants to go back into space.
[02:00:18.560 --> 02:00:20.480]   He says, "I want to go back.
[02:00:20.480 --> 02:00:22.320]   I can't get enough of space."
[02:00:22.320 --> 02:00:27.320]   Yeah, because he only had like three minutes.
[02:00:27.320 --> 02:00:34.040]   Michael Strahan, the host of Good Morning America, former NFL player, yesterday went up with
[02:00:34.040 --> 02:00:36.000]   the new Shepard aircraft.
[02:00:36.000 --> 02:00:37.560]   There were actually six people aboard.
[02:00:37.560 --> 02:00:41.240]   It's the largest group of people on New Shepard.
[02:00:41.240 --> 02:00:46.560]   They experienced a few minutes of weightlessness and breathtaking reviews of the earth before
[02:00:46.560 --> 02:00:47.800]   descending back.
[02:00:47.800 --> 02:00:51.080]   This is the new addiction, Leo.
[02:00:51.080 --> 02:00:53.080]   The new addiction, space.
[02:00:53.080 --> 02:00:54.080]   You got a quarter of a million.
[02:00:54.080 --> 02:00:56.560]   Once you get it, you can't get it out of it.
[02:00:56.560 --> 02:00:58.040]   Not the final frontier.
[02:00:58.040 --> 02:01:02.400]   Also on the flight, Laura Shepard, Churchley, the oldest daughter of Allen Shepard, the rocket
[02:01:02.400 --> 02:01:09.240]   is called a new Shepard named after Allen Shepard's first American in space.
[02:01:09.240 --> 02:01:10.640]   How many years ago was that?
[02:01:10.640 --> 02:01:17.520]   1960 something, so like 50 years ago.
[02:01:17.520 --> 02:01:20.800]   Strahan and Churchley both flew for free.
[02:01:20.800 --> 02:01:22.760]   The other four paid.
[02:01:22.760 --> 02:01:25.440]   We don't know how much, but it's estimated around a quarter of a million dollars.
[02:01:25.440 --> 02:01:29.680]   It's expensive to do this.
[02:01:29.680 --> 02:01:32.640]   Sure you want to go into space?
[02:01:32.640 --> 02:01:33.800]   I wouldn't do it.
[02:01:33.800 --> 02:01:34.800]   Would you have to go?
[02:01:34.800 --> 02:01:35.800]   I would do it too.
[02:01:35.800 --> 02:01:40.480]   It kind of freaks me out though, but it seems like it's pretty safe.
[02:01:40.480 --> 02:01:41.480]   Yeah.
[02:01:41.480 --> 02:01:42.480]   Right?
[02:01:42.480 --> 02:01:43.480]   Well, so far.
[02:01:43.480 --> 02:01:47.720]   Nothing's blown up yet.
[02:01:47.720 --> 02:01:52.360]   But yeah, I mean, it seems like pretty amazing to have the opportunity to do that.
[02:01:52.360 --> 02:01:53.720]   Yeah, pretty amazing.
[02:01:53.720 --> 02:01:56.520]   What are you going to do though?
[02:01:56.520 --> 02:01:58.920]   What are you going to like actually do while you're there?
[02:01:58.920 --> 02:01:59.920]   It's nothing.
[02:01:59.920 --> 02:02:00.920]   Just take it in for yourself.
[02:02:00.920 --> 02:02:04.520]   Just kind of memorize it because it's going to be over so soon, so quickly.
[02:02:04.520 --> 02:02:06.640]   You got to wear some snap glasses.
[02:02:06.640 --> 02:02:10.800]   There you go.
[02:02:10.800 --> 02:02:15.200]   Let's see, what else?
[02:02:15.200 --> 02:02:20.240]   Suddenly TCL has stopped selling their Google TV.
[02:02:20.240 --> 02:02:26.080]   Apparently, if you bought it, I'm sorry, software problems with it.
[02:02:26.080 --> 02:02:28.920]   Best Buy was the only retailer to sell these Google TVs.
[02:02:28.920 --> 02:02:29.920]   They've pulled them all.
[02:02:29.920 --> 02:02:34.840]   The entire line mysteriously disappeared according to 9 to 5 Google due to performance
[02:02:34.840 --> 02:02:39.600]   complaints, both the 5 and 6 series TCLs.
[02:02:39.600 --> 02:02:41.800]   Other TCLs have Roku built in, which works great.
[02:02:41.800 --> 02:02:45.640]   In fact, my mom, I got her one.
[02:02:45.640 --> 02:02:49.240]   But I guess the Google TV not working so well in the TCLs.
[02:02:49.240 --> 02:02:52.400]   I guess a word of warning.
[02:02:52.400 --> 02:02:55.320]   Well, is it though?
[02:02:55.320 --> 02:02:58.240]   Because some people saying that there was no problem with it.
[02:02:58.240 --> 02:02:59.920]   Oh, not that I want to.
[02:02:59.920 --> 02:03:00.920]   There's something else?
[02:03:00.920 --> 02:03:01.920]   I don't know.
[02:03:01.920 --> 02:03:07.760]   There's speculate too much, but somebody else is making TVs now and they haven't actually
[02:03:07.760 --> 02:03:14.360]   revealed who the manufacturer is, but they look a lot like TCL TV.
[02:03:14.360 --> 02:03:19.000]   Oh, those Amazon TVs you're talking about?
[02:03:19.000 --> 02:03:22.640]   Amazon with Amazon's Fire built in?
[02:03:22.640 --> 02:03:26.480]   Oh, that's a better theory.
[02:03:26.480 --> 02:03:28.600]   I like that.
[02:03:28.600 --> 02:03:30.720]   Let's start that rumor.
[02:03:30.720 --> 02:03:37.040]   So TCL, which no one knows, but apparently he's done a deal to make TVs for Amazon.
[02:03:37.040 --> 02:03:40.280]   Well, it hasn't been confirmed, so I'm totally specky.
[02:03:40.280 --> 02:03:45.760]   No, we're admitting this is a complete rumor.
[02:03:45.760 --> 02:03:48.160]   That's I like it.
[02:03:48.160 --> 02:03:54.520]   Now, there was a great story about how nobody's naming kids anymore.
[02:03:54.520 --> 02:03:55.520]   Duh.
[02:03:55.520 --> 02:04:00.120]   I just feel bad for the people who are already named.
[02:04:00.120 --> 02:04:05.400]   But you may remember when Amazon picked that there was a very well-known website, the dot
[02:04:05.400 --> 02:04:06.400]   com.
[02:04:06.400 --> 02:04:07.400]   Remember that?
[02:04:07.400 --> 02:04:08.400]   Anybody?
[02:04:08.400 --> 02:04:09.400]   Analytics.
[02:04:09.400 --> 02:04:10.400]   Yep.
[02:04:10.400 --> 02:04:11.400]   Analytics.
[02:04:11.400 --> 02:04:14.720]   Yeah, you could figure out how many people are going to a website, but the nice thing
[02:04:14.720 --> 02:04:17.440]   about it was that it was always inaccurate.
[02:04:17.440 --> 02:04:20.520]   Yeah, because in order for it to work, I remember this.
[02:04:20.520 --> 02:04:21.680]   People would tell it.
[02:04:21.680 --> 02:04:23.720]   Number one on the left.
[02:04:23.720 --> 02:04:28.160]   In order for it to work, the people visiting the site had to be running the plugin on their
[02:04:28.160 --> 02:04:29.960]   browser.
[02:04:29.960 --> 02:04:33.800]   So it was just, you know, self-selected group, and then they would statistically multiply
[02:04:33.800 --> 02:04:37.560]   it by a big number to get the actual number.
[02:04:37.560 --> 02:04:46.560]   So apparently, in order to use the name of Amazon bought dot com, and they are now shutting
[02:04:46.560 --> 02:04:48.560]   it down.
[02:04:48.560 --> 02:04:49.560]   No.
[02:04:49.560 --> 02:04:50.560]   RIP.
[02:04:50.560 --> 02:04:51.560]   RIP.
[02:04:51.560 --> 02:04:52.920]   Whoa, it went out.
[02:04:52.920 --> 02:04:53.920]   We hardly knew you.
[02:04:53.920 --> 02:04:54.920]   Let's analyze it.
[02:04:54.920 --> 02:05:01.440]   May 2022, no new monthly stats will be released going forward.
[02:05:01.440 --> 02:05:08.960]   The company said 25 years ago, wow, 25 years ago, we founded internet after two decades
[02:05:08.960 --> 02:05:13.000]   of helping you, yes, you, find, reach, and convert your digital audience.
[02:05:13.000 --> 02:05:19.880]   We made the difficult decision, at least Jeff Bezos did, to retire a common May 1st.
[02:05:19.880 --> 02:05:24.000]   Thank you for making us your go-to resource for content research, competitive analysis,
[02:05:24.000 --> 02:05:26.080]   keyword research, and so much.
[02:05:26.080 --> 02:05:29.560]   Can they just change the name?
[02:05:29.560 --> 02:05:32.840]   They're going to actually literally shut down the API.
[02:05:32.840 --> 02:05:35.760]   So it's pretty- There was enough money there at this point.
[02:05:35.760 --> 02:05:37.320]   They're like, let's just cash out.
[02:05:37.320 --> 02:05:39.080]   No, Google has analytics that everybody uses.
[02:05:39.080 --> 02:05:41.080]   I wonder if Amazon has an analytics platform.
[02:05:41.080 --> 02:05:42.080]   I wouldn't be surprised.
[02:05:42.080 --> 02:05:44.960]   I'm sure they have for themselves.
[02:05:44.960 --> 02:05:46.920]   Are you excited about meta?
[02:05:46.920 --> 02:05:47.920]   Sure.
[02:05:47.920 --> 02:05:52.120]   Are you going to get all in on the metaverse and wear a visor around?
[02:05:52.120 --> 02:05:53.960]   And is that going to be you?
[02:05:53.960 --> 02:05:56.920]   Are you- I feel like the- Hit person I know.
[02:05:56.920 --> 02:05:58.680]   So- Oh, I really appreciate it.
[02:05:58.680 --> 02:06:00.680]   I don't know what that says about me or you.
[02:06:00.680 --> 02:06:03.160]   She says more about me than anything, I think.
[02:06:03.160 --> 02:06:04.800]   But I think it's interesting.
[02:06:04.800 --> 02:06:10.040]   I definitely think the idea of doing things in the metaverse is going to be big.
[02:06:10.040 --> 02:06:12.400]   I'm getting more into NFTs.
[02:06:12.400 --> 02:06:13.400]   Are you?
[02:06:13.400 --> 02:06:14.400]   See?
[02:06:14.400 --> 02:06:15.400]   Yeah.
[02:06:15.400 --> 02:06:16.400]   She's hip.
[02:06:16.400 --> 02:06:18.840]   She's with us.
[02:06:18.840 --> 02:06:26.160]   And then different even coins are then going to NFTs in the metaverse and different utility
[02:06:26.160 --> 02:06:28.760]   coins or companies on the blockchain.
[02:06:28.760 --> 02:06:33.360]   I do think it's like it's buzzy, but then also it will be functional.
[02:06:33.360 --> 02:06:36.680]   I mean, I did a keynote in the metaverse the other week.
[02:06:36.680 --> 02:06:38.800]   I went to a conference in the metaverse.
[02:06:38.800 --> 02:06:39.800]   Wow.
[02:06:39.800 --> 02:06:40.800]   And did a keynote.
[02:06:40.800 --> 02:06:41.800]   Did you have legs or not?
[02:06:41.800 --> 02:06:42.800]   Which kind of metaverse was it?
[02:06:42.800 --> 02:06:43.800]   Was it one of those?
[02:06:43.800 --> 02:06:44.800]   But it was two.
[02:06:44.800 --> 02:06:45.800]   I did it.
[02:06:45.800 --> 02:06:46.800]   I was on my desktop.
[02:06:46.800 --> 02:06:48.520]   So I wasn't wearing anything.
[02:06:48.520 --> 02:06:49.520]   Okay.
[02:06:49.520 --> 02:06:50.520]   I didn't have to.
[02:06:50.520 --> 02:06:51.520]   Yeah.
[02:06:51.520 --> 02:06:52.520]   Okay.
[02:06:52.520 --> 02:06:54.480]   But- But the people who are attending your virtual-
[02:06:54.480 --> 02:06:55.480]   Yeah.
[02:06:55.480 --> 02:06:56.880]   We were all virtual.
[02:06:56.880 --> 02:06:58.440]   We had our avatars and everything.
[02:06:58.440 --> 02:07:03.440]   It's just basically like similar to the gaming communities, right?
[02:07:03.440 --> 02:07:07.120]   But it's just like other things that live in the virtual universe.
[02:07:07.120 --> 02:07:09.520]   I don't know how much we're going to use it for like work.
[02:07:09.520 --> 02:07:11.920]   I mean, like will every meeting be in the metaverse?
[02:07:11.920 --> 02:07:12.920]   Oh, God.
[02:07:12.920 --> 02:07:13.920]   I think that'll just get like annoying.
[02:07:13.920 --> 02:07:14.920]   Oh, I hope not.
[02:07:14.920 --> 02:07:15.920]   I don't know.
[02:07:15.920 --> 02:07:21.280]   So I do think though, community is like where it's really going to shine is within like niche
[02:07:21.280 --> 02:07:26.360]   communities where you can like connect with each other, whether yeah, you're part you've
[02:07:26.360 --> 02:07:27.360]   bought NFTs.
[02:07:27.360 --> 02:07:28.360]   You're part of the discord.
[02:07:28.360 --> 02:07:30.360]   You have access to the metaverse.
[02:07:30.360 --> 02:07:32.200]   And you're in clubhouse.
[02:07:32.200 --> 02:07:34.560]   And that's the trifecta ladies and gentlemen.
[02:07:34.560 --> 02:07:36.720]   Patrick, you did it all.
[02:07:36.720 --> 02:07:37.720]   Blockchain NFT.
[02:07:37.720 --> 02:07:38.720]   Okay.
[02:07:38.720 --> 02:07:39.720]   Metaverse clubhouse.
[02:07:39.720 --> 02:07:40.720]   You got it all.
[02:07:40.720 --> 02:07:41.720]   Yeah.
[02:07:41.720 --> 02:07:46.520]   It's going to be a part of it or, you know, like like dating or events like anything, anything
[02:07:46.520 --> 02:07:51.960]   you would want to do virtually that you can't necessarily do in person to have access to
[02:07:51.960 --> 02:07:54.640]   the metaverse experience of that.
[02:07:54.640 --> 02:07:59.840]   I can definitely see that happening and then doing transactions in the metaverse.
[02:07:59.840 --> 02:08:02.880]   I mean, have you seen the real estate stuff in the metaverse?
[02:08:02.880 --> 02:08:03.880]   That's crazy.
[02:08:03.880 --> 02:08:04.880]   Wait a minute.
[02:08:04.880 --> 02:08:07.320]   Are you buying a house in the real world or in the metaverse?
[02:08:07.320 --> 02:08:09.560]   People are buying real estate in the metaverse.
[02:08:09.560 --> 02:08:11.520]   That's just a second life.
[02:08:11.520 --> 02:08:13.120]   Yeah, it's second life.
[02:08:13.120 --> 02:08:14.960]   It's like everything's old is new again.
[02:08:14.960 --> 02:08:15.960]   Yeah.
[02:08:15.960 --> 02:08:16.960]   We're going to be able to get to the real invention.
[02:08:16.960 --> 02:08:21.360]   Are you old enough to remember second that you play in second life?
[02:08:21.360 --> 02:08:22.360]   Me?
[02:08:22.360 --> 02:08:23.360]   No, I was never into that.
[02:08:23.360 --> 02:08:27.000]   But I was also, I, to be honest, I've not never been into gaming that much.
[02:08:27.000 --> 02:08:31.800]   So when I was in the metaverse and I was moving around someone who was directing me where
[02:08:31.800 --> 02:08:35.640]   to go to get to my keynote, he kept seeing me going in circles.
[02:08:35.640 --> 02:08:39.640]   Because I didn't figure out how to move my body.
[02:08:39.640 --> 02:08:43.640]   We're like, that's a great image.
[02:08:43.640 --> 02:08:48.560]   Literally, I was like moving up and down, seeing every perspective.
[02:08:48.560 --> 02:08:51.800]   I was like, how do I just move forward and just see in front of me?
[02:08:51.800 --> 02:08:53.040]   I have no idea.
[02:08:53.040 --> 02:08:55.640]   I'm so new, but I'm such a new.
[02:08:55.640 --> 02:08:59.720]   I think I saw Alex Cantor was in a bare suit in second life once.
[02:08:59.720 --> 02:09:00.720]   Was that you?
[02:09:00.720 --> 02:09:04.280]   I wasn't planning on announcing that news.
[02:09:04.280 --> 02:09:05.280]   This show.
[02:09:05.280 --> 02:09:08.000]   As long as we're here, you're free.
[02:09:08.000 --> 02:09:10.000]   I know you are.
[02:09:10.000 --> 02:09:13.880]   After denying this for close to 15 years, I was.
[02:09:13.880 --> 02:09:15.800]   You exclude the drop suit.
[02:09:15.800 --> 02:09:19.080]   Did you ever play Grand Theft Auto?
[02:09:19.080 --> 02:09:20.080]   Of course.
[02:09:20.080 --> 02:09:22.000]   Yeah, who didn't?
[02:09:22.000 --> 02:09:23.000]   Mm-hmm.
[02:09:23.000 --> 02:09:27.720]   Did it, did you suddenly start carjacking cars in the real life?
[02:09:27.720 --> 02:09:30.880]   Well, Leo, I wasn't planning on addressing this on this.
[02:09:30.880 --> 02:09:31.880]   Wow.
[02:09:31.880 --> 02:09:32.880]   Putting you on the circle on this one.
[02:09:32.880 --> 02:09:34.480]   I was not speaking about this.
[02:09:34.480 --> 02:09:35.480]   I will say yes.
[02:09:35.480 --> 02:09:39.640]   I did do many carjacking and Grand Theft Auto and I have no regrets.
[02:09:39.640 --> 02:09:45.640]   In Illinois state, according to eyewitness to Chicago, an Illinois state lawmaker is proposing
[02:09:45.640 --> 02:09:55.560]   a ban on Grand Theft Auto because of a recent surge in carjacking around the Chicago area.
[02:09:55.560 --> 02:10:00.680]   He says it's all because of Grand Theft Auto and he's going to plan to ban sales of the
[02:10:00.680 --> 02:10:02.080]   game in Illinois.
[02:10:02.080 --> 02:10:03.080]   Carjacking?
[02:10:03.080 --> 02:10:04.080]   Yeah.
[02:10:04.080 --> 02:10:05.080]   Go ahead.
[02:10:05.080 --> 02:10:06.080]   Go ahead.
[02:10:06.080 --> 02:10:12.880]   He says carjacking is not normal and carjacking must stop.
[02:10:12.880 --> 02:10:15.960]   I don't know.
[02:10:15.960 --> 02:10:23.400]   If you live in Illinois and you voted this clown in, you have my deepest sympathy.
[02:10:23.400 --> 02:10:28.520]   He says he, this is, I'll give you his name, Marcus Evans.
[02:10:28.520 --> 02:10:31.520]   He's a Democratic state representative.
[02:10:31.520 --> 02:10:33.720]   Grand Theft Auto, by the way, has been around since 1997.
[02:10:33.720 --> 02:10:39.080]   So if it's suddenly causing a surge in carjacking, I don't know.
[02:10:39.080 --> 02:10:46.480]   He also thanked a community activist early Walker for starting Operation Safe Pump.
[02:10:46.480 --> 02:10:48.680]   Not what it sounds like.
[02:10:48.680 --> 02:10:52.200]   Security teams are patrolling gas stations to protect people while they're pumping gas,
[02:10:52.200 --> 02:10:56.040]   which apparently is where a lot of carjacking happen.
[02:10:56.040 --> 02:11:01.440]   Walker says, "Representative Evans and I have researched and concluded these very young offenders
[02:11:01.440 --> 02:11:08.600]   of carjacking are greatly influenced by the Grand Theft audio video game."
[02:11:08.600 --> 02:11:13.320]   I believe there is bipartisan support to ban this game from being sold in Illinois.
[02:11:13.320 --> 02:11:14.320]   I shouldn't laugh.
[02:11:14.320 --> 02:11:16.320]   Carjacking's nothing to laugh at.
[02:11:16.320 --> 02:11:20.160]   Grand Theft Auto, on the other hand.
[02:11:20.160 --> 02:11:28.840]   Well, it's, you know, we've suffered too long from politicians that have been corrupt
[02:11:28.840 --> 02:11:35.160]   and largely unwilling to take on the real evils in society.
[02:11:35.160 --> 02:11:36.160]   Finally.
[02:11:36.160 --> 02:11:41.080]   And so I have all represented Evans for going there and taking on with really yelling us,
[02:11:41.080 --> 02:11:42.680]   Grand Theft Auto.
[02:11:42.680 --> 02:11:46.480]   And I do think that it's time for us to take on some other serious asylum problems.
[02:11:46.480 --> 02:11:52.200]   For instance, let's start to take on the serious epidemic of thievery.
[02:11:52.200 --> 02:11:54.360]   And I think we should ban Oceans 11, 12.
[02:11:54.360 --> 02:12:00.160]   All these casino robberies, one after the other, it's because of those movies.
[02:12:00.160 --> 02:12:01.160]   It's horrible.
[02:12:01.160 --> 02:12:04.760]   And you know, while we're at it, I think we need to start to talk about the Scrooge of
[02:12:04.760 --> 02:12:05.760]   prison breaks.
[02:12:05.760 --> 02:12:10.280]   And therefore I'm calling on representatives across the United States to ban the Shawshank
[02:12:10.280 --> 02:12:11.280]   Redemption.
[02:12:11.280 --> 02:12:17.240]   Have you noticed more and more young people are trying to dig holes to the center of
[02:12:17.240 --> 02:12:19.240]   the earth and I blame Minecraft.
[02:12:19.240 --> 02:12:20.240]   Got to ban it.
[02:12:20.240 --> 02:12:21.240]   Got to ban it.
[02:12:21.240 --> 02:12:22.240]   Got to ban it.
[02:12:22.240 --> 02:12:24.920]   They're just digging down.
[02:12:24.920 --> 02:12:29.520]   Here's a story that unites Carolina and Alex.
[02:12:29.520 --> 02:12:34.960]   Italy finds Amazon 1.13 billion euros.
[02:12:34.960 --> 02:12:39.040]   Now we're talking because a lot of these find 70 million, 12 million.
[02:12:39.040 --> 02:12:40.040]   Big deal.
[02:12:40.040 --> 02:12:44.880]   But you know, we're talking billions now for abusing market dominance.
[02:12:44.880 --> 02:12:47.960]   This is the Italian competition authority.
[02:12:47.960 --> 02:12:53.680]   Although I as I know Carolina, correct me, but I like to call it the authority to get
[02:12:53.680 --> 02:12:58.200]   a car and a car and a car.
[02:12:58.200 --> 02:12:59.680]   That's wrong.
[02:12:59.680 --> 02:13:00.680]   Concoranza.
[02:13:00.680 --> 02:13:01.680]   Concoranza.
[02:13:01.680 --> 02:13:03.680]   I love Italian.
[02:13:03.680 --> 02:13:11.200]   The authority car and a car and a del marcato.
[02:13:11.200 --> 02:13:12.200]   Is that right?
[02:13:12.200 --> 02:13:13.200]   That's right.
[02:13:13.200 --> 02:13:14.200]   It's close.
[02:13:14.200 --> 02:13:15.200]   Yeah, that's pretty good.
[02:13:15.200 --> 02:13:21.200]   Anyhow, Milan, if I said it that way, the AGCM, they say Amazon's giving sellers using
[02:13:21.200 --> 02:13:28.320]   its Amazon logistics fulfillment by Amazon advantages in terms of visibility and sales,
[02:13:28.320 --> 02:13:30.320]   including access to its prime label.
[02:13:30.320 --> 02:13:38.640]   Amazon says we will appeal, but it is a big fine 1.13 billion euros.
[02:13:38.640 --> 02:13:40.400]   Good for the good for the.
[02:13:40.400 --> 02:13:41.400]   That's right.
[02:13:41.400 --> 02:13:42.400]   One more time.
[02:13:42.400 --> 02:13:48.200]   Atorita, guarante de la concorrenza, del marcato.
[02:13:48.200 --> 02:13:49.440]   Good for them.
[02:13:49.440 --> 02:13:51.680]   Taking a stand.
[02:13:51.680 --> 02:13:53.520]   Big news in the US.
[02:13:53.520 --> 02:13:58.240]   Jessica Warsen-Wossel is now the chairman or has been confirmed by the Senate to lead
[02:13:58.240 --> 02:14:01.920]   the FCC, the first female chair in FCC history.
[02:14:01.920 --> 02:14:05.880]   She is one of the best as far as I'm concerned personally.
[02:14:05.880 --> 02:14:07.480]   One of the best FCC commissioners.
[02:14:07.480 --> 02:14:14.920]   She really understands things like net neutrality will be an excellent FCC chair person.
[02:14:14.920 --> 02:14:20.360]   And amazingly, I don't know how you get this kind of a vote in the Senate as it's constituted
[02:14:20.360 --> 02:14:21.360]   today.
[02:14:21.360 --> 02:14:23.520]   68 to 31.
[02:14:23.520 --> 02:14:25.480]   So wow.
[02:14:25.480 --> 02:14:26.480]   Nice.
[02:14:26.480 --> 02:14:27.480]   She will.
[02:14:27.480 --> 02:14:34.440]   It is thought place particular emphasis on broadband access, increasing high speed internet
[02:14:34.440 --> 02:14:37.000]   coverage in underserved areas.
[02:14:37.000 --> 02:14:38.000]   Amen.
[02:14:38.000 --> 02:14:39.000]   Amen.
[02:14:39.000 --> 02:14:40.000]   How LaLuya.
[02:14:40.000 --> 02:14:41.000]   Congrats.
[02:14:41.000 --> 02:14:45.360]   On the other hand, I don't think Gigi Stone is going to get in there.
[02:14:45.360 --> 02:14:52.120]   She's been kind of she won't get a full Senate vote till after the new year.
[02:14:52.120 --> 02:14:59.560]   And she is the fifth and currently open seat for the FCC nominee from President Biden.
[02:14:59.560 --> 02:15:00.560]   Let's take a little break.
[02:15:00.560 --> 02:15:02.200]   We've got to wrap things up.
[02:15:02.200 --> 02:15:03.200]   We're going way long.
[02:15:03.200 --> 02:15:04.200]   Thank you.
[02:15:04.200 --> 02:15:05.200]   You guys are very patient.
[02:15:05.200 --> 02:15:06.760]   Let's let me quick break.
[02:15:06.760 --> 02:15:08.400]   Then a final story.
[02:15:08.400 --> 02:15:09.400]   Birds are not real.
[02:15:09.400 --> 02:15:11.200]   And I'm going to prove it to you.
[02:15:11.200 --> 02:15:14.200]   Our show today brought to you by userway.org.
[02:15:14.200 --> 02:15:16.200]   Now this is real.
[02:15:16.200 --> 02:15:20.240]   There is a real problem for 60 million Americans with disabilities.
[02:15:20.240 --> 02:15:23.440]   When they come to your website, they can't use it.
[02:15:23.440 --> 02:15:25.120]   They can't navigate it.
[02:15:25.120 --> 02:15:27.240]   The images don't have all tags.
[02:15:27.240 --> 02:15:30.760]   And the worst for you, they can't use your shopping cart.
[02:15:30.760 --> 02:15:31.760]   They can't fill in forms.
[02:15:31.760 --> 02:15:33.400]   They can't use it.
[02:15:33.400 --> 02:15:36.480]   You need to make your website accessible.
[02:15:36.480 --> 02:15:38.400]   You need to make it ADA compliant.
[02:15:38.400 --> 02:15:44.080]   It's a federal law that says websites are public entities and have to provide equal
[02:15:44.080 --> 02:15:46.000]   access to its services for all Americans.
[02:15:46.000 --> 02:15:48.360]   Domino's found that out the hard way.
[02:15:48.360 --> 02:15:50.600]   The pizza, big national pizza chain, they said, "Well, wait a minute.
[02:15:50.600 --> 02:15:51.800]   Our website has a phone number."
[02:15:51.800 --> 02:15:54.160]   They went all the way to the Supreme Court on this one.
[02:15:54.160 --> 02:15:56.880]   Supreme Court said, "Nope, that's separate but equal."
[02:15:56.880 --> 02:15:59.840]   The ADA requires accessibility.
[02:15:59.840 --> 02:16:03.720]   So the good news is you can do it with userway.
[02:16:03.720 --> 02:16:10.400]   Userway has an incredible AI powered solution that tirelessly enforces all of those complicated
[02:16:10.400 --> 02:16:14.040]   web content accessibility guidelines.
[02:16:14.040 --> 02:16:16.440]   And they do it with one line of JavaScript.
[02:16:16.440 --> 02:16:19.600]   It can do more than a tire team of developers.
[02:16:19.600 --> 02:16:24.960]   Userway is AI and machine learning solutions power accessibility for over 1 million websites,
[02:16:24.960 --> 02:16:27.400]   including some of the biggest websites in the world.
[02:16:27.400 --> 02:16:34.320]   The COLA uses Disney, eBay, FedEx, Walmart, all use userway.
[02:16:34.320 --> 02:16:39.800]   And now this top of the line, best in class enterprise level accessibility tools available
[02:16:39.800 --> 02:16:46.120]   to you, small and medium business owners, I was shocked at how affordable it is.
[02:16:46.120 --> 02:16:51.160]   It's less than the cost of the fonts on our front page, the Google fonts that we pay for.
[02:16:51.160 --> 02:16:54.880]   When you scale, you need userway because it's going to grow with you.
[02:16:54.880 --> 02:17:00.560]   If it can handle Disney, it can handle you and accessible and compliant websites.
[02:17:00.560 --> 02:17:02.120]   Not just the right thing to do.
[02:17:02.120 --> 02:17:03.280]   It's good for business.
[02:17:03.280 --> 02:17:08.280]   Then a Killies Heal for Website registration forms nav menus that leave out millions of
[02:17:08.280 --> 02:17:09.280]   people.
[02:17:09.280 --> 02:17:11.920]   Just ask the voice of Siri Susan Bennett.
[02:17:11.920 --> 02:17:13.400]   She's a big fan.
[02:17:13.400 --> 02:17:20.360]   Userway is trusted by more than 1 million websites and 60 million users with disabilities.
[02:17:20.360 --> 02:17:26.880]   Userway.org to learn how one line of code can make your website accessible.
[02:17:26.880 --> 02:17:32.960]   Whether you're using WordPress Shopify, Wix, it's easy to add, same for AEM site core,
[02:17:32.960 --> 02:17:36.720]   SharePoint, even any site that's hand coded, any site at all.
[02:17:36.720 --> 02:17:40.840]   Because if you can add a line of JavaScript that pulls in all the userway, AI, machine
[02:17:40.840 --> 02:17:42.840]   learning, computer vision, it's going to go in there.
[02:17:42.840 --> 02:17:46.360]   It's automatically generating alt tags for your images.
[02:17:46.360 --> 02:17:49.160]   It uses its computer vision to generate image descriptions.
[02:17:49.160 --> 02:17:50.640]   You can add to it.
[02:17:50.640 --> 02:17:53.680]   You can add to it, but it gets you started.
[02:17:53.680 --> 02:17:59.120]   It will remediate complex nav menus, make sure all your pop-ups are accessible.
[02:17:59.120 --> 02:18:03.920]   Even make your colors accessible while remaining true to your brand.
[02:18:03.920 --> 02:18:06.480]   You'll get a detailed report of all the violations that were fixed.
[02:18:06.480 --> 02:18:07.480]   Look, check it out.
[02:18:07.480 --> 02:18:08.480]   Go to userway.org/twit.
[02:18:08.480 --> 02:18:11.160]   They've got a scanning tool.
[02:18:11.160 --> 02:18:15.160]   Just see if your site's ADA compliant.
[02:18:15.160 --> 02:18:17.880]   You at least do that.
[02:18:17.880 --> 02:18:21.240]   And if it's not, I've got to tell you, this is the easiest way to do it.
[02:18:21.240 --> 02:18:27.200]   Userway can make any website fully accessible, ADA compliant, avoid lawsuits.
[02:18:27.200 --> 02:18:31.840]   Most importantly, serve a community that needs your help, just a little bit, to make your
[02:18:31.840 --> 02:18:33.320]   website accessible.
[02:18:33.320 --> 02:18:37.200]   With userway, everyone who visits your site can browse seamlessly, can customize it to
[02:18:37.200 --> 02:18:40.040]   fit their needs.
[02:18:40.040 --> 02:18:44.040]   This is a way to show your commitment to millions of people with disabilities.
[02:18:44.040 --> 02:18:46.440]   Open up a whole new market and just do the right thing.
[02:18:46.440 --> 02:18:48.720]   Go to userway.org/twit.
[02:18:48.720 --> 02:18:49.720]   I'm a big supporter.
[02:18:49.720 --> 02:18:50.720]   Get 30% off.
[02:18:50.720 --> 02:18:54.640]   I hear from blind users all the time because of the radio show.
[02:18:54.640 --> 02:18:55.880]   Our stuff is audio, right?
[02:18:55.880 --> 02:18:58.480]   So we have a lot of blind listeners.
[02:18:58.480 --> 02:19:00.960]   And it's frustrating to go out on the web.
[02:19:00.960 --> 02:19:03.720]   A lot of sites need userway.
[02:19:03.720 --> 02:19:09.400]   Userway's AI-powered accessibility solution makes the internet accessible for everyone.
[02:19:09.400 --> 02:19:12.160]   Visit userway.org/twit.
[02:19:12.160 --> 02:19:15.640]   They're making a big difference.
[02:19:15.640 --> 02:19:17.960]   And thank you, Userway, for your support.
[02:19:17.960 --> 02:19:19.400]   We had a great week this week on Twitch.
[02:19:19.400 --> 02:19:23.040]   You know, I was going to -- John, can you -- do you have that video?
[02:19:23.040 --> 02:19:24.040]   Let's play the video.
[02:19:24.040 --> 02:19:25.560]   You could see this for yourself.
[02:19:25.560 --> 02:19:28.200]   Burke, it's time to deck the halls.
[02:19:28.200 --> 02:19:32.920]   Everybody who's watching this on the screen, this is the power of Leo the Port.
[02:19:32.920 --> 02:19:33.920]   I declared Christmas.
[02:19:33.920 --> 02:19:36.360]   It's "Gramble, Scrubble, Scrubble, Scrubble, Scrubble, Quick, Quick, Quick, Quick, Quick!"
[02:19:36.360 --> 02:19:37.360]   Wait a minute.
[02:19:37.360 --> 02:19:38.360]   I think it's unbalanced.
[02:19:38.360 --> 02:19:40.360]   There's more on the left than there are on the right.
[02:19:40.360 --> 02:19:41.360]   Oh!
[02:19:41.360 --> 02:19:43.800]   Burke, are you the Grinch this year?
[02:19:43.800 --> 02:19:44.800]   He was not happy.
[02:19:44.800 --> 02:19:46.800]   On to it.
[02:19:46.800 --> 02:19:47.800]   Security now.
[02:19:47.800 --> 02:19:55.240]   A botnet has infected more than one million Windows machines globally and continues to
[02:19:55.240 --> 02:20:00.640]   infect new machines at a rate of thousands more per day.
[02:20:00.640 --> 02:20:08.500]   It's using the public Bitcoin blockchain technology to protect itself from disruption.
[02:20:08.500 --> 02:20:10.040]   This week in Enterprise Tech.
[02:20:10.040 --> 02:20:14.960]   There's services like Ring, Disney Plus, Roku, Coinbase, Venmo, and I've got to laundry
[02:20:14.960 --> 02:20:17.120]   lists more of these things that are out there.
[02:20:17.120 --> 02:20:20.200]   They got a lesson in redundancy and fail over this past week.
[02:20:20.200 --> 02:20:24.800]   That's right, AWS went down, maybe shocked to hear it was human error.
[02:20:24.800 --> 02:20:26.200]   Mac break weekly.
[02:20:26.200 --> 02:20:32.000]   Bad guys are putting air tags on high-end vehicles according to the York Regional Police.
[02:20:32.000 --> 02:20:34.200]   Those are abusing their air tags.
[02:20:34.200 --> 02:20:38.200]   They're putting air tags in your gas or your trailer.
[02:20:38.200 --> 02:20:42.840]   I just didn't expect this from Canada.
[02:20:42.840 --> 02:20:44.840]   According to the York Regional Police, come back.
[02:20:44.840 --> 02:20:48.520]   Twit, making the world safe for technology.
[02:20:48.520 --> 02:20:53.880]   Burke did such a good job decorating the studio that we haven't touched a thing, which
[02:20:53.880 --> 02:20:58.920]   might explain why I have these strange antlers in front of me.
[02:20:58.920 --> 02:21:00.080]   There's a metal deer.
[02:21:00.080 --> 02:21:01.080]   I don't know.
[02:21:01.080 --> 02:21:02.080]   Is that a reindeer?
[02:21:02.080 --> 02:21:03.080]   Is that a reindeer?
[02:21:03.080 --> 02:21:04.840]   Is that Dasher or a dancer?
[02:21:04.840 --> 02:21:05.840]   Make sure it doesn't hurt you.
[02:21:05.840 --> 02:21:06.840]   Can you?
[02:21:06.840 --> 02:21:07.840]   Yeah.
[02:21:07.840 --> 02:21:09.840]   I'm not sure it's just doesn't hurt you.
[02:21:09.840 --> 02:21:11.360]   It was so good I couldn't touch it.
[02:21:11.360 --> 02:21:13.360]   He did such a good job.
[02:21:13.360 --> 02:21:19.600]   Thursday, this is, sure you don't have this yet, but any minute now, you're going to
[02:21:19.600 --> 02:21:22.200]   say, "It happens to everybody.
[02:21:22.200 --> 02:21:23.720]   Why is it so dark in this restaurant?"
[02:21:23.720 --> 02:21:26.160]   I can hardly read the menu.
[02:21:26.160 --> 02:21:31.400]   You're going to say, "Are they making the print on websites smaller because you are
[02:21:31.400 --> 02:21:36.800]   going to, as you cross," and it's, I know, it's a years away, "cross the age of 40,
[02:21:36.800 --> 02:21:43.040]   suddenly suffer what we all suffer," which is far-sightedness, the ability to read,
[02:21:43.040 --> 02:21:45.400]   because your eyes get older.
[02:21:45.400 --> 02:21:46.720]   Thanks for the explanation.
[02:21:46.720 --> 02:21:48.480]   Just ahead of time.
[02:21:48.480 --> 02:21:49.800]   There is a new drop.
[02:21:49.800 --> 02:21:51.080]   The FDA has approved it.
[02:21:51.080 --> 02:21:57.800]   It will be hitting the market on Thursday, and I drop from a company called Vueity takes
[02:21:57.800 --> 02:21:59.280]   effective 15 minutes.
[02:21:59.280 --> 02:22:04.600]   One drop in each eye provides sharper vision for six to 10 hours, and I drop.
[02:22:04.600 --> 02:22:05.680]   You don't need readers.
[02:22:05.680 --> 02:22:10.200]   You don't have to go to the drug store and go to the Dr. Dean Adele.
[02:22:10.200 --> 02:22:11.640]   None of you.
[02:22:11.640 --> 02:22:12.640]   You don't know.
[02:22:12.640 --> 02:22:16.640]   Carolina, you might remember that, but I, yeah.
[02:22:16.640 --> 02:22:20.600]   Would you use an eye drop instead of wearing readers?
[02:22:20.600 --> 02:22:24.680]   I'm as blind as a bat, so I'm minus 11 on both eyes.
[02:22:24.680 --> 02:22:25.680]   Oh my God.
[02:22:25.680 --> 02:22:26.680]   You really are.
[02:22:26.680 --> 02:22:27.680]   Holy cow.
[02:22:27.680 --> 02:22:30.040]   I thought I was blind as a bat.
[02:22:30.040 --> 02:22:32.040]   You're worse than me.
[02:22:32.040 --> 02:22:33.040]   Yeah.
[02:22:33.040 --> 02:22:39.120]   So, yeah, it is annoying to wear lenses and now at the age of 52, having to wear readers
[02:22:39.120 --> 02:22:40.120]   as well.
[02:22:40.120 --> 02:22:41.120]   It's the worst.
[02:22:41.120 --> 02:22:46.320]   You know, it's why I didn't get everybody said you should get that surgery that corrects
[02:22:46.320 --> 02:22:48.360]   your nearsightedness.
[02:22:48.360 --> 02:22:49.360]   And then I did.
[02:22:49.360 --> 02:22:50.360]   I talked to my doctor about it.
[02:22:50.360 --> 02:22:51.360]   He said, but it is, yeah.
[02:22:51.360 --> 02:22:55.120]   As soon as you hit 40, you're going to have to wear glasses anyway, because you can't read.
[02:22:55.120 --> 02:22:57.120]   I thought, well, what's the point?
[02:22:57.120 --> 02:23:00.640]   If I want to have to wear glasses anyway.
[02:23:00.640 --> 02:23:06.440]   Prezbioppia is the condition that older folks suffer from.
[02:23:06.440 --> 02:23:09.400]   Unfortunately, now, so that's the upside.
[02:23:09.400 --> 02:23:11.680]   Medical science, amazing.
[02:23:11.680 --> 02:23:15.440]   It's going to cost $80 for a 30-day supply.
[02:23:15.440 --> 02:23:16.440]   Wow.
[02:23:16.440 --> 02:23:17.440]   Yeah.
[02:23:17.440 --> 02:23:20.320]   And how much do the Dr. Dean of the L readers cost?
[02:23:20.320 --> 02:23:21.880]   Four bucks and 50 cents.
[02:23:21.880 --> 02:23:25.080]   And it works best in people 40 to 55 after that.
[02:23:25.080 --> 02:23:26.580]   Forget it.
[02:23:26.580 --> 02:23:30.440]   Side effects include headaches and red eyes.
[02:23:30.440 --> 02:23:34.400]   But that's a small percentage of people.
[02:23:34.400 --> 02:23:40.160]   Just in the annals of medical science, a big breakthrough.
[02:23:40.160 --> 02:23:48.640]   And finally, I love this, and I actually want to give credit to the New York Times and the
[02:23:48.640 --> 02:23:50.240]   wonderful Taylor Lawrence.
[02:23:50.240 --> 02:23:55.720]   She is really good on following cultural trends, almost as good as you are sure, Liz
[02:23:55.720 --> 02:23:57.640]   are, with what's trending.
[02:23:57.640 --> 02:23:59.840]   I was the old, you know, I'm the elder.
[02:23:59.840 --> 02:24:00.840]   Yeah.
[02:24:00.840 --> 02:24:02.840]   You're the Taylor Lorenz of the internet.
[02:24:02.840 --> 02:24:05.280]   We heard the before before.
[02:24:05.280 --> 02:24:06.720]   No, I love her actually.
[02:24:06.720 --> 02:24:07.720]   I've known her for years.
[02:24:07.720 --> 02:24:09.520]   She does really, really good.
[02:24:09.520 --> 02:24:10.520]   Yeah.
[02:24:10.520 --> 02:24:16.640]   And you might have seen this now that you're in Atlanta because this van has popped up in
[02:24:16.640 --> 02:24:18.360]   the Southeast.
[02:24:18.360 --> 02:24:23.080]   Have you seen this guy driving around in the van that says, "Birds aren't real.
[02:24:23.080 --> 02:24:25.040]   Wake up.
[02:24:25.040 --> 02:24:28.240]   Wake up birds are government surveillance drones.
[02:24:28.240 --> 02:24:31.680]   There are no birds."
[02:24:31.680 --> 02:24:32.680]   And you know, it's kind of credible.
[02:24:32.680 --> 02:24:34.960]   He says, "The birds, they charge on power lines."
[02:24:34.960 --> 02:24:37.120]   That explains a lot.
[02:24:37.120 --> 02:24:42.120]   That's why they're sitting there.
[02:24:42.120 --> 02:24:47.160]   Also on his van, his birds aren't real.
[02:24:47.160 --> 02:24:51.040]   Van has a picture of a bird that shows with a microphone, the camera, the antenna, the
[02:24:51.040 --> 02:24:58.280]   speaker, the battery, and the inductive charging coil are on the pigeon.
[02:24:58.280 --> 02:25:02.560]   It also says pigeons are liars, liars.
[02:25:02.560 --> 02:25:07.200]   Last month they protested outside Twitter headquarters in San Francisco to demand the
[02:25:07.200 --> 02:25:09.600]   company change its bird logo.
[02:25:09.600 --> 02:25:10.600]   Okay.
[02:25:10.600 --> 02:25:15.760]   But Taylor, thank you for telling us because I thought it was real.
[02:25:15.760 --> 02:25:17.640]   It's all tongue in cheek.
[02:25:17.640 --> 02:25:18.640]   Birds.
[02:25:18.640 --> 02:25:20.960]   The founders of birds aren't real.
[02:25:20.960 --> 02:25:25.240]   Know that birds are in fact real.
[02:25:25.240 --> 02:25:33.240]   It's a parody social movement to show how people are falling into the rabbit hole of
[02:25:33.240 --> 02:25:35.760]   conspiracy.
[02:25:35.760 --> 02:25:41.720]   And in fact, one of the founders said, "If you actually believe that birds aren't real,
[02:25:41.720 --> 02:25:45.120]   you got a bigger problem than this."
[02:25:45.120 --> 02:25:46.320]   A spontaneous joke.
[02:25:46.320 --> 02:25:47.680]   They're actually buying billboards.
[02:25:47.680 --> 02:25:50.880]   This is the billboard in Memphis, Tennessee.
[02:25:50.880 --> 02:25:58.160]   Note by the way, the birds perching on the billboard proving it's true.
[02:25:58.160 --> 02:26:02.720]   They're probably trying to mess it up.
[02:26:02.720 --> 02:26:07.520]   And as our chatter was saying, evil IRC is saying, if birds were real, wouldn't they
[02:26:07.520 --> 02:26:09.040]   just fly off the edge of the earth?
[02:26:09.040 --> 02:26:10.680]   What's keeping them on the planet?
[02:26:10.680 --> 02:26:11.680]   They know better.
[02:26:11.680 --> 02:26:13.680]   Yes, since the earth is flat.
[02:26:13.680 --> 02:26:14.680]   Right.
[02:26:14.680 --> 02:26:15.680]   Know that.
[02:26:15.680 --> 02:26:16.680]   Yeah.
[02:26:16.680 --> 02:26:25.120]   Here is in St. Louis, the founder of Birds Aren't Real Burning, a St. Louis Cardinals
[02:26:25.120 --> 02:26:27.840]   flag during a protest.
[02:26:27.840 --> 02:26:34.240]   I'm not sure if this is Birds Aren't Real Vann.
[02:26:34.240 --> 02:26:35.240]   This is good.
[02:26:35.240 --> 02:26:39.120]   They actually sell Birds Aren't Real merchandise, which I would like to have.
[02:26:39.120 --> 02:26:44.600]   The money helps them cover their living expenses several thousand dollars a month.
[02:26:44.600 --> 02:26:48.720]   All the money from our merch lineup goes into making sure me and Connor can do this full
[02:26:48.720 --> 02:26:50.360]   time.
[02:26:50.360 --> 02:26:58.640]   None of the proceeds go to anything harmful.
[02:26:58.640 --> 02:26:59.720]   Birds Aren't Real.
[02:26:59.720 --> 02:27:01.720]   I think it's true.
[02:27:01.720 --> 02:27:02.720]   It's brilliant.
[02:27:02.720 --> 02:27:03.720]   It's performance art.
[02:27:03.720 --> 02:27:04.720]   It's great.
[02:27:04.720 --> 02:27:05.720]   It is brilliant.
[02:27:05.720 --> 02:27:06.720]   Yeah.
[02:27:06.720 --> 02:27:08.280]   I would like them to be at least giving the money.
[02:27:08.280 --> 02:27:11.160]   It'd be fun to have them going to them.
[02:27:11.160 --> 02:27:17.600]   Yeah, or some sort of nonprofit or like misinformation, news, journalism.
[02:27:17.600 --> 02:27:19.600]   I don't think they make so much money.
[02:27:19.600 --> 02:27:21.760]   I think if they made a lot more, they might.
[02:27:21.760 --> 02:27:22.760]   True.
[02:27:22.760 --> 02:27:25.840]   But at this point, I think they're barely, I think they're just barely paying for the
[02:27:25.840 --> 02:27:27.000]   Vann and the billboards.
[02:27:27.000 --> 02:27:28.000]   But you're right.
[02:27:28.000 --> 02:27:29.760]   It'd be nice if they make any extra.
[02:27:29.760 --> 02:27:31.480]   Let's just, let's just suggest that.
[02:27:31.480 --> 02:27:32.560]   Yeah, but it is.
[02:27:32.560 --> 02:27:33.560]   It's very smart.
[02:27:33.560 --> 02:27:38.200]   I mean, I wouldn't be surprised if like these folks are at get the next Comedy Central show.
[02:27:38.200 --> 02:27:39.200]   Right.
[02:27:39.200 --> 02:27:41.840]   And it's like what's the next thing they're going to do, right?
[02:27:41.840 --> 02:27:42.840]   Yeah.
[02:27:42.840 --> 02:27:43.840]   Sure.
[02:27:43.840 --> 02:27:44.840]   I'm glad you're feeling better.
[02:27:44.840 --> 02:27:45.840]   I'm sorry you got COVID.
[02:27:45.840 --> 02:27:47.720]   Thank you.
[02:27:47.720 --> 02:27:49.360]   But now you're super immune.
[02:27:49.360 --> 02:27:51.480]   Like you could just go out in the world and enjoy life.
[02:27:51.480 --> 02:27:53.040]   You know, that's exactly what I'm going to do.
[02:27:53.040 --> 02:27:54.360]   I'm going to just jump right in.
[02:27:54.360 --> 02:27:56.160]   Yeah, cough on people, all of that.
[02:27:56.160 --> 02:27:57.640]   Say, don't worry.
[02:27:57.640 --> 02:27:58.640]   I've had COVID.
[02:27:58.640 --> 02:28:01.040]   I'm good.
[02:28:01.040 --> 02:28:04.320]   You will be back at work as soon as your tests negative.
[02:28:04.320 --> 02:28:05.320]   Yes.
[02:28:05.320 --> 02:28:07.040]   Yeah, I'm going to be working from home until then.
[02:28:07.040 --> 02:28:08.040]   Okay.
[02:28:08.040 --> 02:28:10.360]   Where can we find your work?
[02:28:10.360 --> 02:28:13.000]   At Shearel Lazar, follow me everywhere.
[02:28:13.000 --> 02:28:16.640]   And if you're wondering what Peace Inside Live is, we create and produce mindfulness
[02:28:16.640 --> 02:28:19.480]   programs virtually and in person.
[02:28:19.480 --> 02:28:23.680]   We have a 21 day meditation challenge, which people are buying for the holidays, for their
[02:28:23.680 --> 02:28:25.520]   family, friends and loved ones.
[02:28:25.520 --> 02:28:27.400]   I like it.
[02:28:27.400 --> 02:28:28.560]   You put in your information there.
[02:28:28.560 --> 02:28:33.160]   We don't sell it off to any sketchy people.
[02:28:33.160 --> 02:28:36.280]   We also have a Thailand retreat coming up in March.
[02:28:36.280 --> 02:28:37.760]   Oh, nice.
[02:28:37.760 --> 02:28:39.640]   So I'll have to send that to you.
[02:28:39.640 --> 02:28:44.680]   Lisa and I did your Valentine's Day event and loved it.
[02:28:44.680 --> 02:28:47.680]   And that was led by that woman who was holding the thing.
[02:28:47.680 --> 02:28:50.480]   Jordana, my co-founder who's in Thailand.
[02:28:50.480 --> 02:28:53.400]   So that's coming up and a lot more.
[02:28:53.400 --> 02:28:54.400]   Yeah.
[02:28:54.400 --> 02:28:57.400]   So go to the site to find out, to join our newsletters.
[02:28:57.400 --> 02:28:59.200]   We can bring you Peace Inside.
[02:28:59.200 --> 02:29:00.200]   Nice.
[02:29:00.200 --> 02:29:01.200]   Look at that.
[02:29:01.200 --> 02:29:02.200]   Come on.
[02:29:02.200 --> 02:29:03.200]   Look at that.
[02:29:03.200 --> 02:29:05.360]   Best beaches in the world, I'm told.
[02:29:05.360 --> 02:29:06.960]   The islands of Thailand.
[02:29:06.960 --> 02:29:07.960]   I've never been.
[02:29:07.960 --> 02:29:08.960]   I'm looking forward to going.
[02:29:08.960 --> 02:29:09.960]   Oh, man.
[02:29:09.960 --> 02:29:11.360]   Can you talk, Lisa, into this?
[02:29:11.360 --> 02:29:12.760]   She'll do it if you say.
[02:29:12.760 --> 02:29:15.040]   Yeah, I'll send her a link.
[02:29:15.040 --> 02:29:16.160]   Yeah, I want that.
[02:29:16.160 --> 02:29:19.080]   That sounds great.
[02:29:19.080 --> 02:29:22.960]   And Jordana will be leading it, but you're going to be there too.
[02:29:22.960 --> 02:29:27.040]   I'm going to be there helping, but also just attending, to be honest.
[02:29:27.040 --> 02:29:29.120]   The itinerary is pretty amazing.
[02:29:29.120 --> 02:29:30.120]   Oh, this is incredible.
[02:29:30.120 --> 02:29:34.320]   And then she has other facilitators and other areas that we're visiting who will be.
[02:29:34.320 --> 02:29:36.560]   You're going to go to the elephant, even.
[02:29:36.560 --> 02:29:39.120]   I want to go to the elephant haven.
[02:29:39.120 --> 02:29:40.600]   That sounds cool.
[02:29:40.600 --> 02:29:42.360]   Oh, please talk, Lisa.
[02:29:42.360 --> 02:29:43.360]   And it is.
[02:29:43.360 --> 02:29:44.360]   She listens to you.
[02:29:44.360 --> 02:29:45.360]   Thank you.
[02:29:45.360 --> 02:29:46.360]   Thank you, share.
[02:29:46.360 --> 02:29:48.360]   I'm glad you're feeling better.
[02:29:48.360 --> 02:29:49.360]   It's great to have you.
[02:29:49.360 --> 02:29:50.360]   I appreciate it.
[02:29:50.360 --> 02:29:51.360]   It takes you.
[02:29:51.360 --> 02:29:57.200]   Alik's Cantrowitz, Big Tech, trembles shivers in its boots when the name Alex Cantrowitz
[02:29:57.200 --> 02:29:58.200]   comes up.
[02:29:58.200 --> 02:30:02.800]   Big technology podcast, bigtechnology.substack.com for the newsletter.
[02:30:02.800 --> 02:30:05.400]   The book always day one.
[02:30:05.400 --> 02:30:06.400]   Plug something else.
[02:30:06.400 --> 02:30:09.440]   Oh, you're muted.
[02:30:09.440 --> 02:30:11.800]   Is this us or is this you?
[02:30:11.800 --> 02:30:12.960]   Did you know I was muted.
[02:30:12.960 --> 02:30:16.160]   I'll let people know what I have coming up this week on the podcast.
[02:30:16.160 --> 02:30:17.720]   Benedict Evans is going to come.
[02:30:17.720 --> 02:30:18.720]   Oh, I love him.
[02:30:18.720 --> 02:30:19.720]   Oh, he's great.
[02:30:19.720 --> 02:30:21.040]   Oh, he's so brilliant.
[02:30:21.040 --> 02:30:23.360]   We're going to talk about meta.
[02:30:23.360 --> 02:30:26.440]   We're going to talk about whether web three is real or not.
[02:30:26.440 --> 02:30:29.280]   We'll talk about Andres and Horitz and what they're doing.
[02:30:29.280 --> 02:30:32.960]   And then last week, if you go to the feed now, you can see a discussion between Daniel
[02:30:32.960 --> 02:30:34.800]   Kahneman and Jan Lecoot.
[02:30:34.800 --> 02:30:36.360]   I love Daniel Kahneman.
[02:30:36.360 --> 02:30:38.000]   I love the thinking.
[02:30:38.000 --> 02:30:39.000]   What is it?
[02:30:39.000 --> 02:30:40.000]   Thinking quickly, breathing slowly.
[02:30:40.000 --> 02:30:41.840]   You can fast and slow.
[02:30:41.840 --> 02:30:42.840]   Something like that.
[02:30:42.840 --> 02:30:44.560]   Which is all about how humans make decisions.
[02:30:44.560 --> 02:30:48.840]   And then Jan Lecooten, who builds AI for meta.
[02:30:48.840 --> 02:30:52.360]   And they had a discussion that's kind of academic.
[02:30:52.360 --> 02:30:56.560]   But we talked a little bit about like how the human mind thinks and how you can transpose
[02:30:56.560 --> 02:30:57.560]   that into AI.
[02:30:57.560 --> 02:31:01.880]   And it was interesting because we started, you know, my thought was we would have a conversation
[02:31:01.880 --> 02:31:04.920]   focused on trying to learn about where AI was going.
[02:31:04.920 --> 02:31:09.000]   But actually, like, there was a debate about how the human brain works.
[02:31:09.000 --> 02:31:11.840]   And then, you know, I found that kind of interesting.
[02:31:11.840 --> 02:31:16.240]   Like you can learn a little bit about how we as humans think.
[02:31:16.240 --> 02:31:21.000]   If you think about what it's like to build a brain quote, unquote, from the ground up
[02:31:21.000 --> 02:31:22.320]   when you're building AI.
[02:31:22.320 --> 02:31:24.440]   Anyway, it was pretty interesting.
[02:31:24.440 --> 02:31:26.360]   So I would recommend people go check it out.
[02:31:26.360 --> 02:31:27.360]   Oh my God.
[02:31:27.360 --> 02:31:29.400]   This sounds like a must listen.
[02:31:29.400 --> 02:31:31.480]   That's some great people.
[02:31:31.480 --> 02:31:33.320]   Yeah, it was fun.
[02:31:33.320 --> 02:31:35.520]   Yeah, we quote Benedict Evans all the time.
[02:31:35.520 --> 02:31:36.920]   He's really small.
[02:31:36.920 --> 02:31:37.920]   He's great.
[02:31:37.920 --> 02:31:39.120]   Oh, that's neat.
[02:31:39.120 --> 02:31:43.720]   Is that can I get it from bigtechnology.substack.com or is there a special place for the podcast?
[02:31:43.720 --> 02:31:44.720]   Yeah.
[02:31:44.720 --> 02:31:50.200]   If you go to big technology podcast and your podcast app of choice, Apple, Spotify, Overcast,
[02:31:50.200 --> 02:31:54.160]   pocketcast, whatever cast you use, it will be there.
[02:31:54.160 --> 02:31:56.760]   You know, this is the one thing we have to fight as podcasters.
[02:31:56.760 --> 02:32:00.920]   We've got to find a better way to spread the word.
[02:32:00.920 --> 02:32:02.080]   That's right.
[02:32:02.080 --> 02:32:06.320]   That dialogue that you just did every podcast in the world, that's how we all end with it.
[02:32:06.320 --> 02:32:07.320]   Right.
[02:32:07.320 --> 02:32:09.320]   Wherever you find your podcasts.
[02:32:09.320 --> 02:32:10.760]   Wherever you find your podcasts.
[02:32:10.760 --> 02:32:12.600]   I want to listen to Daniel Kahneman.
[02:32:12.600 --> 02:32:13.600]   He's amazing.
[02:32:13.600 --> 02:32:15.600]   Love his book.
[02:32:15.600 --> 02:32:16.600]   Yeah, Yann Lee Koon.
[02:32:16.600 --> 02:32:17.960]   I've heard about for years.
[02:32:17.960 --> 02:32:21.040]   I would be very curious what he's what he has to say.
[02:32:21.040 --> 02:32:22.040]   So that's great.
[02:32:22.040 --> 02:32:23.200]   Good stuff.
[02:32:23.200 --> 02:32:24.200]   Thank you, Alex.
[02:32:24.200 --> 02:32:25.200]   Great to have you here.
[02:32:25.200 --> 02:32:26.200]   Thank you.
[02:32:26.200 --> 02:32:27.200]   Always great to be on the show.
[02:32:27.200 --> 02:32:30.640]   Your Alina Milanese has given me Italian lessons.
[02:32:30.640 --> 02:32:34.000]   She's the founder of the heart of tech.
[02:32:34.000 --> 02:32:37.720]   I think this whole show we should just do in Italian is such a good language.
[02:32:37.720 --> 02:32:44.560]   That is hot.
[02:32:44.560 --> 02:32:46.600]   How do you say so?
[02:32:46.600 --> 02:32:51.640]   How do you say so in French, they say a computer is an ordinateur, which is a terrible name.
[02:32:51.640 --> 02:32:54.000]   What is the name for computer in Italian?
[02:32:54.000 --> 02:32:55.000]   Computer.
[02:32:55.000 --> 02:32:57.000]   That's a good name.
[02:32:57.000 --> 02:33:00.760]   What is the name for the French call it a computer?
[02:33:00.760 --> 02:33:02.280]   Okay, computer.
[02:33:02.280 --> 02:33:03.280]   But you say computer.
[02:33:03.280 --> 02:33:04.280]   You say it like that.
[02:33:04.280 --> 02:33:08.040]   You sort of give it a little bit of a little romance.
[02:33:08.040 --> 02:33:12.760]   Well, the actual word is system informatico, which is similar to the French.
[02:33:12.760 --> 02:33:14.680]   But nobody says it anymore.
[02:33:14.680 --> 02:33:17.960]   Probably nobody says it or any more as well.
[02:33:17.960 --> 02:33:20.440]   Tell us about the heart of tech.
[02:33:20.440 --> 02:33:26.360]   The heart of tech is a company that helps tech company really get serious about diversity
[02:33:26.360 --> 02:33:33.400]   and inclusion and CSR and not using it as a marketing tool, but really putting their
[02:33:33.400 --> 02:33:39.240]   money where their mouth is and change the way they operate from a product perspective
[02:33:39.240 --> 02:33:42.920]   as well as towards their employees as well.
[02:33:42.920 --> 02:33:44.840]   So important.
[02:33:44.840 --> 02:33:53.280]   My other work is creating strategies, which is a tech company, consultancy company.
[02:33:53.280 --> 02:33:56.800]   You can find me on Twitter and Instagram.
[02:33:56.800 --> 02:34:01.800]   You cannot find me on, well, you can, but I'm never on Facebook and LinkedIn at Caromilla
[02:34:01.800 --> 02:34:02.800]   Nese.
[02:34:02.800 --> 02:34:09.360]   And I have a column on Forbes where I talk about technology and the impact on society.
[02:34:09.360 --> 02:34:10.360]   I should have.
[02:34:10.360 --> 02:34:11.600]   And we didn't get around to it.
[02:34:11.600 --> 02:34:17.440]   Talked about Apple and the changes at Apple and now pretty much everybody who was involved
[02:34:17.440 --> 02:34:24.120]   in getting Apple to pay attention to equity is gone.
[02:34:24.120 --> 02:34:28.120]   And I don't know if that's because Apple's pushed them out or they've, I don't know, thought
[02:34:28.120 --> 02:34:30.080]   there's better places for them to be.
[02:34:30.080 --> 02:34:37.560]   But I feel like that movement has ended sort of, which is too bad.
[02:34:37.560 --> 02:34:38.560]   Share Scarlet.
[02:34:38.560 --> 02:34:41.320]   Oh, wait a minute.
[02:34:41.320 --> 02:34:44.320]   She was going to withdraw her complaint and now she's not.
[02:34:44.320 --> 02:34:45.680]   Oh, that's interesting.
[02:34:45.680 --> 02:34:46.680]   She's not.
[02:34:46.680 --> 02:34:49.000]   Oh, she backed down.
[02:34:49.000 --> 02:34:51.480]   See, I thought maybe they paid her off.
[02:34:51.480 --> 02:34:54.840]   She left with drew a complaint with the NLRB.
[02:34:54.840 --> 02:34:57.400]   No, she's not withdrawing it.
[02:34:57.400 --> 02:34:58.400]   Okay.
[02:34:58.400 --> 02:35:00.040]   Well, I guess we'll be talking about that on Tuesday.
[02:35:00.040 --> 02:35:05.080]   Do you have any thoughts you want to throw in then this is this?
[02:35:05.080 --> 02:35:12.600]   They did come to an agreement and as part of that agreement, she was going to withdraw
[02:35:12.600 --> 02:35:13.840]   the complaint.
[02:35:13.840 --> 02:35:15.560]   Now I don't know what is going on.
[02:35:15.560 --> 02:35:19.240]   I don't know because she'll have to give up.
[02:35:19.240 --> 02:35:24.240]   You know, usually the way that works is they say, we're going to give you $30,000 in severance,
[02:35:24.240 --> 02:35:30.160]   but you have to sign this that says you will drop the complaint and not say anything more.
[02:35:30.160 --> 02:35:33.680]   And perhaps what happened is she said, well, take your money and stick it because I'm going
[02:35:33.680 --> 02:35:37.200]   to keep the complaint going.
[02:35:37.200 --> 02:35:39.280]   Very interesting.
[02:35:39.280 --> 02:35:40.280]   Very interesting.
[02:35:40.280 --> 02:35:41.440]   I think good for her.
[02:35:41.440 --> 02:35:44.200]   That's a brave stand to take and it costly.
[02:35:44.200 --> 02:35:45.200]   No doubt.
[02:35:45.200 --> 02:35:49.840]   I was a little disappointed when I heard that she came to an agreement because it seemed
[02:35:49.840 --> 02:35:52.560]   that she fought for a long time.
[02:35:52.560 --> 02:35:54.320]   I don't know.
[02:35:54.320 --> 02:36:00.840]   I'm not sure I have all the information to make a call of whether or not what happened
[02:36:00.840 --> 02:36:02.080]   is what happened.
[02:36:02.080 --> 02:36:08.880]   But it was always better not to settle and see this thing to the end.
[02:36:08.880 --> 02:36:11.600]   Looks like actually the NLRB had something to do with it.
[02:36:11.600 --> 02:36:18.560]   They rejected her withdrawal requests because Apple added a clause saying that she would
[02:36:18.560 --> 02:36:23.640]   not solicit and encourage her in sight anyone to file any charge or complaint with any administrative
[02:36:23.640 --> 02:36:26.840]   agency or court against Apple for one year.
[02:36:26.840 --> 02:36:29.600]   The NLRB said, you got to leave that out.
[02:36:29.600 --> 02:36:32.520]   That's not even legal.
[02:36:32.520 --> 02:36:36.720]   They requested Apple strike, encourage or incite.
[02:36:36.720 --> 02:36:39.160]   Apple said no.
[02:36:39.160 --> 02:36:41.560]   And the NLRB as a result said, but you don't get too much.
[02:36:41.560 --> 02:36:45.720]   You don't get to withdraw it because we're going to pursue this one.
[02:36:45.720 --> 02:36:50.040]   That's just more evidence that Apple is trying to suppress all of this rather than let it
[02:36:50.040 --> 02:36:51.440]   resolve itself.
[02:36:51.440 --> 02:36:53.200]   Carolina, thank you.
[02:36:53.200 --> 02:36:56.960]   The heart of tech does important work and we appreciate you being here.
[02:36:56.960 --> 02:36:58.720]   It's always great to have you.
[02:36:58.720 --> 02:37:00.000]   We thank you all for joining us.
[02:37:00.000 --> 02:37:06.120]   We do the show this week in tech every Sunday afternoon about 230 Pacific, 530 Eastern,
[02:37:06.120 --> 02:37:13.120]   230 UTC.
[02:37:13.120 --> 02:37:18.240]   I mentioned that because you can watch us do it live, kind of a behind the scenes feed,
[02:37:18.240 --> 02:37:21.160]   audio or video at live.twit.tv.
[02:37:21.160 --> 02:37:24.160]   Watching live, chat live at IRC.twit.tv.
[02:37:24.160 --> 02:37:29.280]   And of course, club members, as always, can chat with us live with all of the animated
[02:37:29.280 --> 02:37:31.280]   gifts and all in Discord.
[02:37:31.280 --> 02:37:32.280]   And they do.
[02:37:32.280 --> 02:37:33.280]   They definitely do.
[02:37:33.280 --> 02:37:36.280]   And I'm going to do it live.
[02:37:36.280 --> 02:37:38.280]   And I'm going to do it live.
[02:37:38.280 --> 02:37:40.280]   And I'm going to do it live.
[02:37:40.280 --> 02:37:42.280]   And I'm going to do it live.
[02:37:42.280 --> 02:37:44.280]   And I'm going to do it live.
[02:37:44.280 --> 02:37:45.280]   And I'm going to do it live.
[02:37:45.280 --> 02:37:47.280]   And I'm going to do it live.
[02:37:47.280 --> 02:37:48.280]   And I'm going to do it live.
[02:37:48.280 --> 02:37:49.280]   And I'm going to do it live.
[02:37:49.280 --> 02:37:50.280]   And I'm going to do it live.
[02:37:50.280 --> 02:37:51.280]   And I'm going to do it live.
[02:37:51.280 --> 02:37:52.280]   And I'm going to do it live.
[02:37:52.280 --> 02:37:53.280]   And I'm going to do it live.
[02:37:53.280 --> 02:37:54.280]   And I'm going to do it live.
[02:37:54.280 --> 02:37:55.280]   And I'm going to do it live.
[02:37:55.280 --> 02:37:56.280]   And I'm going to do it live.
[02:37:56.280 --> 02:37:57.280]   And I'm going to do it live.
[02:37:57.280 --> 02:37:58.280]   And I'm going to do it live.
[02:37:58.280 --> 02:37:59.280]   And I'm going to do it live.
[02:37:59.280 --> 02:38:00.280]   And I'm going to do it live.
[02:38:00.280 --> 02:38:01.280]   And I'm going to do it live.
[02:38:01.280 --> 02:38:02.280]   And I'm going to do it live.
[02:38:02.280 --> 02:38:03.280]   And I'm going to do it live.
[02:38:03.280 --> 02:38:04.280]   And I'm going to do it live.
[02:38:04.280 --> 02:38:05.280]   And I'm going to do it live.
[02:38:05.280 --> 02:38:06.280]   And I'm going to do it live.
[02:38:06.280 --> 02:38:07.280]   And I'm going to do it live.
[02:38:07.280 --> 02:38:08.280]   And I'm going to do it live.
[02:38:08.280 --> 02:38:09.280]   And I'm going to do it live.
[02:38:09.280 --> 02:38:10.280]   And I'm going to do it live.
[02:38:10.280 --> 02:38:11.280]   And I'm going to do it live.
[02:38:11.280 --> 02:38:12.280]   And I'm going to do it live.
[02:38:12.280 --> 02:38:13.280]   And I'm going to do it live.
[02:38:13.280 --> 02:38:14.280]   And I'm going to do it live.
[02:38:14.280 --> 02:38:15.280]   And I'm going to do it live.
[02:38:15.280 --> 02:38:16.280]   And I'm going to do it live.
[02:38:16.280 --> 02:38:17.280]   And I'm going to do it live.
[02:38:17.280 --> 02:38:18.280]   And I'm going to do it live.
[02:38:18.280 --> 02:38:19.280]   And I'm going to do it live.
[02:38:19.280 --> 02:38:20.280]   And I'm going to do it live.
[02:38:20.280 --> 02:38:21.280]   And I'm going to do it live.
[02:38:21.280 --> 02:38:22.280]   And I'm going to do it live.
[02:38:22.280 --> 02:38:23.280]   And I'm going to do it live.
[02:38:23.280 --> 02:38:24.280]   And I'm going to do it live.
[02:38:24.280 --> 02:38:25.280]   And I'm going to do it live.
[02:38:25.280 --> 02:38:26.280]   And I'm going to do it live.
[02:38:26.280 --> 02:38:27.280]   And I'm going to do it live.
[02:38:27.280 --> 02:38:28.280]   And I'm going to do it live.
[02:38:28.280 --> 02:38:29.280]   And I'm going to do it live.
[02:38:29.280 --> 02:38:30.280]   And I'm going to do it live.
[02:38:30.280 --> 02:38:31.280]   And I'm going to do it live.
[02:38:31.280 --> 02:38:32.280]   And I'm going to do it live.
[02:38:32.280 --> 02:38:33.280]   And I'm going to do it live.
[02:38:33.280 --> 02:38:34.280]   And I'm going to do it live.
[02:38:34.280 --> 02:38:35.280]   And I'm going to do it live.
[02:38:35.280 --> 02:38:36.280]   And I'm going to do it live.
[02:38:36.280 --> 02:38:37.280]   And I'm going to do it live.
[02:38:37.280 --> 02:38:38.280]   And I'm going to do it live.
[02:38:38.280 --> 02:38:39.280]   And I'm going to do it live.

