;FFMETADATA1
title=Outrage Moms
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=708
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.960]   It's time for Twit, this week in Tech. What a great show we have for you.
[00:00:02.960 --> 00:00:07.040]   Rihanna Wu, Canada for Congress, is back along with futurist Amy Webb.
[00:00:07.040 --> 00:00:09.520]   There is so much to talk about.
[00:00:09.520 --> 00:00:12.080]   Elon Musk, he's in trouble again.
[00:00:12.080 --> 00:00:14.400]   Those folding phones, does anybody want them?
[00:00:14.400 --> 00:00:17.440]   And the weirdest cats you've ever seen. It's all coming up next.
[00:00:17.440 --> 00:00:18.000]   On Twit.
[00:00:18.000 --> 00:00:21.760]   Netcasts you love.
[00:00:21.760 --> 00:00:23.760]   From people you trust.
[00:00:23.760 --> 00:00:29.200]   This is Twit.
[00:00:29.200 --> 00:00:45.360]   This is Twit, this week in Tech. Episode 708 recorded Sunday, March 3rd, 2019.
[00:00:45.360 --> 00:00:46.640]   Outrage moms.
[00:00:46.640 --> 00:00:52.000]   This week in Tech is brought to you by LastPass, ensures safer workplace
[00:00:52.000 --> 00:00:57.120]   passwords sharing with LastPass. The number one most preferred password manager is at
[00:00:57.120 --> 00:00:59.840]   LastPass.com/Twit to learn more.
[00:00:59.840 --> 00:01:05.600]   And by 1000Is. Companies that run in the cloud rely on 1000Is.
[00:01:05.600 --> 00:01:10.960]   It's the place they go first to see, understand, and improve the digital experience of their
[00:01:10.960 --> 00:01:15.600]   cloud-based applications and services. Do the cloud write and improve services for
[00:01:15.600 --> 00:01:23.280]   your customers and employees today. Visit 1000Is.com/Twit and buy Stamps.com.
[00:01:23.280 --> 00:01:27.040]   Buy and print real U.S. postage the instant you need it right from your desk.
[00:01:27.040 --> 00:01:31.280]   For a special offer go to Stamps.com, click the microphone and enter Twit.
[00:01:31.280 --> 00:01:39.040]   And by Sophos Cybersecurity. In an age of evolving cyber threats, you need evolved cyber security.
[00:01:39.040 --> 00:01:43.760]   Powered by artificial intelligence, Sophos can detect threats before they strike.
[00:01:43.760 --> 00:01:48.000]   Killing ransomware, viruses, and other cyber threats dead in their tracks.
[00:01:48.000 --> 00:01:53.200]   Get a free security scan and/or a free trial today at Sophos.com.
[00:01:53.200 --> 00:02:02.080]   It's time for Twit this week at Tech. The week's Tech News has dissected and analyzed
[00:02:02.080 --> 00:02:06.240]   with precision instruments by some of the best tech journalists in the world.
[00:02:06.240 --> 00:02:08.560]   I am just going to sit back and enjoy this show.
[00:02:08.560 --> 00:02:15.760]   Brianna Wu is here. She is a candidate for Congress once again in Massachusetts 8th District.
[00:02:15.760 --> 00:02:21.200]   And of course, a long time geek, game developer builds her own car engines, takes them apart,
[00:02:21.200 --> 00:02:24.640]   siphons off her own oil. Hello Brianna Wu. How are you doing?
[00:02:24.640 --> 00:02:28.080]   It's good to be here. And we're going to talk about cars a little later.
[00:02:28.080 --> 00:02:34.720]   She's a long time internal combustion engine enthusiast.
[00:02:34.720 --> 00:02:38.400]   I want to be clear. I want to get to like EVs just like you.
[00:02:38.400 --> 00:02:40.560]   Working your way out. I'm working on it. Okay.
[00:02:42.720 --> 00:02:46.480]   We'll talk about that. And I'm thrilled to have back because she was just here on Friday,
[00:02:46.480 --> 00:02:51.600]   but she's back Amy Webb, our futurist. We love Amy, Amy Webb.io.
[00:02:51.600 --> 00:02:59.200]   She is the founder of the Future Today Institute, the author of a number of books.
[00:02:59.200 --> 00:03:04.960]   Her first, The Signals are Talking, which we had her on for, where she teaches us all to be futurists.
[00:03:04.960 --> 00:03:09.040]   Her brand new book is The Big Nine. And we'll talk a little bit about that as we did on Friday.
[00:03:09.040 --> 00:03:13.280]   How the tech titans and their thinking machines could warp humanity. It's all about
[00:03:13.280 --> 00:03:17.600]   AI. Amy, great to have you back. Thank you. I always love being here.
[00:03:17.600 --> 00:03:24.000]   Oh, it's rare that we get a professor of strategic foresight from the NYU Stern School of Business
[00:03:24.000 --> 00:03:29.520]   on our airway. It is because I am the only one. So it's not actually that rare.
[00:03:29.520 --> 00:03:36.240]   So we've been a few times. I was saying before the show, I just wanted to thank you.
[00:03:36.240 --> 00:03:41.840]   We had talked the last time you were here some time ago about how you consulted a Hulu show called
[00:03:41.840 --> 00:03:49.040]   The First, which is about the first man mission to Mars. It's fiction. Sean Penn is the astronaut,
[00:03:49.040 --> 00:03:53.760]   and it's got a lot of drama in it. And you did all the kind of the future prediction for it.
[00:03:53.760 --> 00:03:57.680]   And I just wanted to say how much I loved the show. At the time, I said, "Sean Penn,"
[00:03:57.680 --> 00:03:59.840]   but he was great. I really thought he was great.
[00:03:59.840 --> 00:04:03.120]   Yeah, thanks for watching it. We had a small, but mighty audience.
[00:04:04.320 --> 00:04:11.200]   It's worth watching. Yeah. And I think what was so great about the show was that the acting was
[00:04:11.200 --> 00:04:18.000]   amazing. But this was all about being as true to what's plausible for the future as we possibly
[00:04:18.000 --> 00:04:23.920]   could be. So it's set in the near future. And the world looks different, but eerily familiar,
[00:04:23.920 --> 00:04:28.400]   which I think is great. Yeah, like they're all using, and this is one of the things I think we
[00:04:28.400 --> 00:04:33.440]   all agree is probably coming, but they're all using augmented reality spectacles that look like
[00:04:33.440 --> 00:04:41.120]   regular glasses. But it's really fun because at one point, the Tashal Macalon, somebody will come
[00:04:41.120 --> 00:04:45.920]   under her office and she said, "Let me share this. You could see what's going on." She swipes the
[00:04:45.920 --> 00:04:52.000]   video over to their glasses. It's so cool. And nobody had a cell phone. And oh, there weren't
[00:04:52.000 --> 00:04:59.440]   anywhere there. That's because one of the things that we've modeled, we being in my shop, is that
[00:05:00.560 --> 00:05:04.880]   this is the beginning of the end of smartphones. And I know that's a hard pill to swallow,
[00:05:04.880 --> 00:05:09.280]   given what just debuted at Mobile World Congress. But all of the data point to
[00:05:09.280 --> 00:05:14.400]   less and less mobile phones, more and more peripherals over the next decade.
[00:05:14.400 --> 00:05:18.400]   I bet there's-- I think that sounds great. Yeah. Yeah. I bet there's a precedent
[00:05:18.400 --> 00:05:24.160]   for what's happening today because it's the silly season for phones. We got folder phones,
[00:05:24.160 --> 00:05:30.880]   we got snap, bracelet phones from TCL. We got the weird form factors. I bet there's a president
[00:05:30.880 --> 00:05:38.640]   technology that when sales flatten, people start going weird. That's the sure sign that you're at
[00:05:38.640 --> 00:05:43.760]   the end of the line for a product. There totally is. I can tell you exactly. So if you go back to
[00:05:43.760 --> 00:05:50.720]   1998, '99, I'm sure that all three of us were in the same boat. I was carrying around-- and that
[00:05:50.720 --> 00:05:56.560]   would be fair, I lived in Japan at the time. I had a digital camera that doubled as an MP3 player.
[00:05:56.560 --> 00:06:04.000]   Oh, yeah. Like an old-school digital camera. I had a mini-disc player. I had a separate Wi-Fi
[00:06:04.000 --> 00:06:11.120]   sniffer made by Canary that I carry around. I remember those. Yeah. Yeah. Oh, yeah. I had a GPS
[00:06:11.120 --> 00:06:17.040]   device that connected in a weird way to my Blackberry once I bought a Pearl. And that's
[00:06:17.040 --> 00:06:24.960]   kind of sort of generated GPS using the waypoints from the phone. So I had all of these devices
[00:06:24.960 --> 00:06:29.520]   started getting weird, right? And there was this convergence that happened over the next couple of
[00:06:29.520 --> 00:06:34.240]   years. And the end of that convergence was the beginning of the iPhone. So the iPhone comes out
[00:06:34.240 --> 00:06:38.800]   and suddenly we have all this compute in just one device and all these features. Well, now two
[00:06:38.800 --> 00:06:44.720]   decades later, we're starting to see a divergence back in the other direction as we have things like
[00:06:44.720 --> 00:06:51.520]   smart earbuds, which collect our bio data and also let us play our music, smart yoga pants,
[00:06:51.520 --> 00:06:58.240]   dog better. There's all these other things and the phone is once again, retreating into the
[00:06:58.240 --> 00:07:05.120]   background. But it's something I think about a lot is I don't think it's just me. I have started
[00:07:05.120 --> 00:07:10.480]   to get the thing where my neck is just killing me from looking down at my phone at the time. Do
[00:07:10.480 --> 00:07:15.920]   you know what I mean? And like, just I don't know being in this position all day. I don't think
[00:07:15.920 --> 00:07:20.960]   humans are meant to bend like that. And I can't wait for it to get to the point where it's in your
[00:07:20.960 --> 00:07:26.160]   glasses and you just don't have to deal with all of that. And it's not just bending down. So there's
[00:07:26.160 --> 00:07:32.880]   some really interesting work by a couple of op them, op themological and optometric associations.
[00:07:32.880 --> 00:07:39.680]   So not lobbyists, like credible researchers. Within the next couple of decades, the overwhelming
[00:07:39.680 --> 00:07:44.560]   majority of people in Western countries are going to be nearsighted. And the reason for that is,
[00:07:44.560 --> 00:07:51.120]   and there is a reason. The reason is that our biology is not evolving as fast as the technology.
[00:07:51.120 --> 00:07:59.040]   And as humans, our eyes have been engineered to see far, not near. And most of us are increasingly
[00:07:59.040 --> 00:08:04.720]   I happen to know all this because my husband's an eye doctor. So there is a whole new
[00:08:05.680 --> 00:08:11.200]   disease called computer vision syndrome, which causes it's a real thing. And it causes dry eye
[00:08:11.200 --> 00:08:15.920]   and neck cramping, some of the stuff you're describing. And it is all because our eyes were
[00:08:15.920 --> 00:08:21.760]   never meant to stare at a screen that like very close to our faces all day long. And certainly not
[00:08:21.760 --> 00:08:27.440]   to go between near and far continuously, which most of us are now doing. So that's going to make
[00:08:27.440 --> 00:08:32.880]   this transition into wearing smart glasses and possibly contact lenses, though that'll be much
[00:08:32.880 --> 00:08:36.720]   further out in the distance. That will help accelerate that process.
[00:08:36.720 --> 00:08:43.600]   We're not meant to sit either, I suppose. I mean, what's that doing to us sitting in a chair all day?
[00:08:43.600 --> 00:08:49.920]   Yeah, so we're really in an unnatural kind of situation. So you're saying that I wouldn't be
[00:08:49.920 --> 00:08:56.320]   nearsighted today. If I, what did I do when I was a kid read a lot of books? I always thought
[00:08:56.320 --> 00:09:03.200]   that was a kind of not true. No, it is true. So if you're somebody, I mean, it's a it's
[00:09:03.200 --> 00:09:08.960]   heritable. So if you have nearsightedness or extreme farsightedness or whatever else,
[00:09:08.960 --> 00:09:13.280]   it's probably because your parents did. But for kids who spent a lot of time reading,
[00:09:13.280 --> 00:09:18.160]   or for kids who are now spending a lot of time looking at screens, they have a much higher
[00:09:18.160 --> 00:09:24.560]   probability of having site visions much earlier in life. Holy cow. Well, that's cheerful.
[00:09:25.520 --> 00:09:27.120]   This is a good lead into hollow lenses.
[00:09:27.120 --> 00:09:33.600]   Yeah, what's going to happen when we get augmented reality? I mean, I honestly, once they're like,
[00:09:33.600 --> 00:09:40.080]   they were in your show, that seems like the glasses that both of you are wearing,
[00:09:40.080 --> 00:09:44.400]   I would be wearing if I weren't wearing contact lenses, and maybe they'll even be contact lenses,
[00:09:44.400 --> 00:09:47.040]   and you maybe you'll have something in your ear so that you can hear.
[00:09:47.040 --> 00:09:51.360]   That's right. That seems like, so do you think that's what's going to replace the smartphone?
[00:09:52.000 --> 00:09:56.720]   So again, this is the models that I've been working on for a while, the data driven models.
[00:09:56.720 --> 00:10:02.960]   But right. So what we are probably looking at is a slow transition away from our smart, like,
[00:10:02.960 --> 00:10:08.720]   in a decade, the smartphone functions that we currently have in our androids and iPhones are
[00:10:08.720 --> 00:10:13.360]   going to look a lot like the flip phone of yesterday year. And there will be holdouts,
[00:10:13.360 --> 00:10:18.480]   but more and more people will transition into wearing smart glasses, which for a time will
[00:10:18.480 --> 00:10:24.000]   still require a secondary device, like a ring or a wristband. And in the early days of this,
[00:10:24.000 --> 00:10:28.560]   they'll use our phones as connective tissue because the compute will still need to happen
[00:10:28.560 --> 00:10:33.280]   somewhere near us. But increasingly, as we've got AI in the cloud and serverless computing and
[00:10:33.280 --> 00:10:39.680]   all these other things happening, and we can once actual 5G networks, not what people are calling
[00:10:39.680 --> 00:10:46.320]   5G right now, are deployed. We won't have issues with latency, all of the other challenges that
[00:10:46.320 --> 00:10:52.160]   we have today. And so that transition from thing that we hold in front of our face to thing that
[00:10:52.160 --> 00:10:58.400]   we wear on our face will become much more seamless and natural. And in the interim, we're doing,
[00:10:58.400 --> 00:11:06.560]   you made me think of my Palm 7, which I had. Remember that crazy? I loved it. It was a palm,
[00:11:06.560 --> 00:11:11.040]   but in order to give it wireless connectivity in the days before we had smartphones that had a
[00:11:11.040 --> 00:11:16.800]   crazy antenna. And in order to make it useful, maybe you had to get a folding keyboard.
[00:11:16.800 --> 00:11:25.680]   And I think the folding phones are kind of in that same goofy ballpark. This is TCL's
[00:11:25.680 --> 00:11:31.520]   foldable phone, which turns into a bracelet. Is that why you're wearing the flip bracelet?
[00:11:31.520 --> 00:11:37.120]   This is yeah, because it kind of is like a snap bracelet. I don't think that the way that you
[00:11:37.120 --> 00:11:41.200]   put it on is like that. I don't see I'm terrible at this. You've already cracked the screen.
[00:11:41.200 --> 00:11:46.160]   But here's a question I have about screens. So Samsung has a folding
[00:11:46.160 --> 00:11:52.480]   folding phone. I swear to God, you look at the S9 the wrong way and that thing cracks.
[00:11:52.480 --> 00:11:56.880]   How are they going to keep these from falling apart? I don't know. No, the better question is,
[00:11:56.880 --> 00:12:05.280]   what's how do you have a phone case for a folding phone? Well, in a way, I think the galaxy fold
[00:12:05.280 --> 00:12:10.080]   because it has a screen on the front, small screen, and then you open it up for the big screen.
[00:12:10.080 --> 00:12:13.840]   That's a little easier to put in a case because closed, it's kind of like a case.
[00:12:13.840 --> 00:12:17.120]   You'd have to have a case that opened up. It's kind of like a traditional phone.
[00:12:17.120 --> 00:12:20.960]   To like those leather cases that my parents had with their notebook. Yeah.
[00:12:20.960 --> 00:12:28.560]   Yeah. I don't know. I definitely think there's a there's a use case for these foldable phones.
[00:12:28.560 --> 00:12:34.160]   I get that a lot of people it seems gimmicky. I really agree with the lea when you're saying like
[00:12:34.160 --> 00:12:38.400]   when they start rolling out the weird phones goofy. That's when it's really.
[00:12:38.400 --> 00:12:40.400]   That's not this is a daytime. Yeah. That's
[00:12:40.400 --> 00:12:46.720]   and and it's a little bigger than that. The weird one though, I don't I don't think what makes any
[00:12:46.720 --> 00:12:54.400]   sense is the phone from Huawei, which folds out. So that I don't know how you put that in a case
[00:12:54.400 --> 00:13:02.560]   because that has its screen on the outside. And but apparently there are cases people are making
[00:13:02.560 --> 00:13:06.400]   cases or planning to make cases. The other issue with these is they're ridiculous.
[00:13:06.400 --> 00:13:14.560]   The $3,000 for the Huawei $2,000 for the fold. We should also the the idea of a dual screen phone
[00:13:14.560 --> 00:13:18.720]   has actually been around for a while. And there's been a bunch of Chinese companies making
[00:13:18.720 --> 00:13:23.920]   literal flip phones. Right. Screen on either side. No, it's got a new one.
[00:13:23.920 --> 00:13:31.760]   There was a mazoo phone out for a while. These were crazy looking dual screen phones that very
[00:13:31.760 --> 00:13:38.000]   few I never saw anybody using. But the difference now is the sort of folding without losing the
[00:13:38.000 --> 00:13:45.840]   pixels. I mean, I'm really into the idea of a foldable phone everywhere. I carry my iPhone 10
[00:13:45.840 --> 00:13:52.560]   with me and I carry my 11 inch iPad Pro just because you know, if I have 10 minutes like read
[00:13:52.560 --> 00:13:58.800]   the New York Times, I just like the bigger screen is easier to type on there. And I just think about
[00:13:59.600 --> 00:14:05.280]   an iPad. How can they really start innovating on the iPad? It felt like more than the iPhone,
[00:14:05.280 --> 00:14:11.360]   they hit a wall with what was bringing consumer value. And I think something you could fold out
[00:14:11.360 --> 00:14:16.640]   more easily to put into a purse. I just that's a gadget I would very seriously look at.
[00:14:16.640 --> 00:14:23.040]   There is a question about though, as you pointed out, this this one screen that bends in the middle.
[00:14:23.040 --> 00:14:28.240]   What's this bed and gonna end up? How many times can you close that before it starts getting a
[00:14:28.240 --> 00:14:32.640]   crease? Yeah. This I kind of like this model. This is more I think what you're talking about.
[00:14:32.640 --> 00:14:37.840]   This is from TCL. They're not making it. It's a concept, but it's two screens with a hinge in
[00:14:37.840 --> 00:14:42.800]   them in the middle. Yeah, you know, you want to make screen. You want to screen all the way across.
[00:14:42.800 --> 00:14:50.080]   Yeah, I don't want to research a lot of this going into the show. And ultimately,
[00:14:50.080 --> 00:14:54.640]   I kept coming back to folding is interesting. Retracting is better. Oh, right.
[00:14:57.120 --> 00:15:04.800]   I could see a device that we retract or it's almost like tiled and more modular. So at one
[00:15:04.800 --> 00:15:09.920]   point, Google had something called the R phone, which never actually went right. They don't testing
[00:15:09.920 --> 00:15:15.200]   that in in the. They inherited this from Motorola when they bought them. It was a modular build
[00:15:15.200 --> 00:15:20.560]   your own phone kit. The series there was not everybody wants the same capabilities. So you could
[00:15:20.560 --> 00:15:26.560]   buy the modules you like and snap them together into a phone, which sounds like a Lego phone, but.
[00:15:26.560 --> 00:15:30.160]   A little bit, but it kind of made sense. And Sony has been.
[00:15:30.160 --> 00:15:36.160]   Sony now has these panels, these sort of Sony tiles. And you can fit together any number of
[00:15:36.160 --> 00:15:43.200]   them to sort of build your own display. So I could see in a handful of years, a tile concept phone
[00:15:43.200 --> 00:15:47.920]   that that, you know, again, allowed you to build some more functionality. But Leo, I have to agree
[00:15:47.920 --> 00:15:52.400]   with you. I mean, once technology starts getting silly, that's usually an indication that we're
[00:15:52.400 --> 00:15:58.240]   on to something else. And, you know, we how many people use a physical typewriter? How many people
[00:15:58.240 --> 00:16:03.360]   use a I had a brother word processor, you know, just sometimes. That was an interim device, right?
[00:16:03.360 --> 00:16:07.200]   A little bit of computer, a little bit of typewriter. It had a little tiny screen.
[00:16:07.200 --> 00:16:12.400]   Yeah. I think we're in the interim device space. That could go on for a while, though,
[00:16:12.400 --> 00:16:18.000]   because 5G is at least two or three years off. Definitely. In fact, I saw one analyst say,
[00:16:18.560 --> 00:16:25.600]   Oh, good news. 5G will be available in 2025 to a full 32% of the nation. It's like,
[00:16:25.600 --> 00:16:30.480]   well, that's a third of the nation in three years. That's not where the small cell is coming from,
[00:16:30.480 --> 00:16:34.480]   where we've got an antagonistic relationship with right now. Who's going to build these?
[00:16:34.480 --> 00:16:38.080]   Yeah. I mean, there's there's a lot to be figured out. So I and I agree with you,
[00:16:38.080 --> 00:16:44.960]   that's a prerequisite. You have to have fast, easily available, low latency is important, too.
[00:16:44.960 --> 00:16:49.280]   And that's one of the features of 5G networks. We got to solve the battery issue.
[00:16:49.280 --> 00:16:54.560]   The hollow lens is a fairly hefty device with only three hours of battery life, right?
[00:16:54.560 --> 00:17:02.240]   And if you start reading about how they actually do the lenses inside to go into your eyeball,
[00:17:02.240 --> 00:17:08.240]   like this is a very, very, very complicated gadget. Like if you start reading about how hollow
[00:17:08.240 --> 00:17:14.560]   lens and the pupillary distance that they calculate every time you put it on, I just, it's like an
[00:17:14.560 --> 00:17:20.080]   iPhone. I could always see how that would get to the point of being a mainstream consumer thing
[00:17:20.080 --> 00:17:26.000]   that anyone could afford. But I'm looking at the technology for these glasses. And you start thinking
[00:17:26.000 --> 00:17:32.000]   about miniaturizing the battery or producing these very, very complicated lenses that really need to
[00:17:32.000 --> 00:17:38.720]   be made individually for the person. And I start wondering if it's going to be something like
[00:17:38.720 --> 00:17:44.480]   hollow lens or if like that's just a stop off place before we get to neural interfaces.
[00:17:44.480 --> 00:17:49.600]   Like, so it's going to be easier, like replicating that through the eyeball or just telling your
[00:17:49.600 --> 00:17:58.160]   brain what you want to see. So I think the other important thing is, we've gotten used to a mobile
[00:17:58.160 --> 00:18:02.960]   phone as a singular device that does a lot of things, whether it's for enterprise or for
[00:18:02.960 --> 00:18:09.200]   individuals or for government. As we diverge back out, for a time, we're going to have different
[00:18:09.200 --> 00:18:14.400]   headsets that serve different purposes. So hollow lens, this new version of hollow lens is just for
[00:18:14.400 --> 00:18:19.280]   enterprise. It's not for individuals. And it has a limited number of applications. The applications
[00:18:19.280 --> 00:18:24.160]   that it does have are pretty awesome. I saw there's a really cool application where you can see
[00:18:24.160 --> 00:18:30.800]   through walls. So if you work in contracting and construction, they can overlay the blueprints,
[00:18:30.800 --> 00:18:36.000]   which interestingly is where all this AR started, right? An industrial assembly lines.
[00:18:36.000 --> 00:18:40.960]   Yeah, that's where it came from. So that's kind of interesting. There are some really interesting
[00:18:40.960 --> 00:18:45.760]   applications for training, for like surgeons and other things like that.
[00:18:45.760 --> 00:18:53.200]   Then there's the magic leap side, which at the moment, the battery pack, the compute sort of
[00:18:53.200 --> 00:18:59.280]   happens on your body and the glasses look silly. But the idea there is that those would become a
[00:18:59.280 --> 00:19:06.160]   more easy to access consumer device. And the glasses will look more like what I've got on,
[00:19:06.160 --> 00:19:11.760]   which is normal looking glasses. That can also provide that refraction.
[00:19:11.760 --> 00:19:17.840]   But then there's this whole other school where maybe the future is more about retinal projection
[00:19:19.040 --> 00:19:24.160]   in some cases. So again, I think this is one of those things where we're going to see wacky,
[00:19:24.160 --> 00:19:29.760]   weird technology for the next probably 12 to 15 years as all of this gets sorted out. And there's
[00:19:29.760 --> 00:19:35.200]   experimentation. And there's convergence and divergence over and over and over again. So
[00:19:35.200 --> 00:19:40.800]   if you're into gadgets, we should be in the golden era of wacky gadgets for the next decade or so.
[00:19:40.800 --> 00:19:46.640]   I think what I don't know if either of you read Blake J. Harris's new book, The History of
[00:19:46.640 --> 00:19:52.320]   the Future, he basically had a ton of access to Facebook and Oculus. And he was there
[00:19:52.320 --> 00:19:59.280]   basically chronicling the downfall of Oculus and following Palmer Lucky around for the entire time.
[00:19:59.280 --> 00:20:05.680]   If you read why Oculus ultimately failed, I think it's because they have like this,
[00:20:05.680 --> 00:20:11.920]   they were over promising, right? They have this big grand vision of us basically living in Ready
[00:20:11.920 --> 00:20:18.640]   Player One. What I like about HoloLens and what they're offering this time is it's such a narrow
[00:20:18.640 --> 00:20:24.480]   vision of what you need. A really good example. I ride a motorcycle and often I like to ride
[00:20:24.480 --> 00:20:31.840]   motorcycles to like events when I can. You can't touch a smart screen when you're on a motorcycle
[00:20:31.840 --> 00:20:39.760]   because you have on gloves, right? Like capacitive touchscreen. It can't access that. So a motorcycle
[00:20:39.760 --> 00:20:47.040]   helmet with AR built in to show me like map, to show me information that I can't access on a phone.
[00:20:47.040 --> 00:20:54.320]   That is something I would very happily pay $3,000 for because it's about my safety. So I think like,
[00:20:54.320 --> 00:21:00.960]   I think almost the reason all of these ideas failed is we tried to promise everything. And I
[00:21:00.960 --> 00:21:07.200]   think Microsoft is doing the right thing by focusing on just a few things and doing them very well.
[00:21:07.200 --> 00:21:12.720]   Contractor schematics like doctors in operating rooms, all those things.
[00:21:12.720 --> 00:21:14.960]   It's also good for Microsoft's business because it sells
[00:21:14.960 --> 00:21:21.440]   Azure, which is really the really all Microsoft cares about. I wonder and Amy, you could probably
[00:21:21.440 --> 00:21:26.880]   correct me on this, but I wonder if we're not making the Malthusian mistake.
[00:21:26.880 --> 00:21:34.000]   So Malthus predicted that we the world population, if it continues to grow at the rate it's growing,
[00:21:34.000 --> 00:21:42.720]   will run out of food and everybody will die in like the year 1910. It didn't happen.
[00:21:42.720 --> 00:21:48.000]   And I think we may be making that mistake with technology because we've been living in this
[00:21:48.000 --> 00:21:53.440]   era of Moore's law, where just things got cheaper and faster at doubling every 18 months.
[00:21:53.440 --> 00:21:58.000]   And we kind of expect that to continue, except that I think we're kind of,
[00:21:58.640 --> 00:22:04.880]   I believe we're kind of hitting the edge here, the wall battery technology were stuck.
[00:22:04.880 --> 00:22:08.720]   Somebody's going to have to make an amazing breakthrough. There's some huge paradigm shift.
[00:22:08.720 --> 00:22:13.120]   That's it because we've been trying to make this better for a long time with little success.
[00:22:13.120 --> 00:22:16.400]   5G I think is a lot harder to implement than anybody admits.
[00:22:16.400 --> 00:22:22.400]   And there's a lot of issues that are going to get in between us and 5G, including the Huawei
[00:22:22.400 --> 00:22:26.880]   issue, which we'll talk about. Computing technology is going to get smaller,
[00:22:27.840 --> 00:22:33.600]   but we don't, but interface design has not advanced at the rate it needs to for us.
[00:22:33.600 --> 00:22:39.440]   Look at the Apple Watch, which is smaller and clever and has the worst interface ever designed.
[00:22:39.440 --> 00:22:43.760]   It sells, but despite the interface.
[00:22:43.760 --> 00:22:51.760]   Do you see what I'm sure as a futurist, you think about this. We can't just assume it's straight line.
[00:22:52.480 --> 00:22:59.760]   Sure. So as a futurist, my job is not to make predictions. It's to gather data, run models,
[00:22:59.760 --> 00:23:02.880]   using that data and make connections to try to figure out where we're headed.
[00:23:02.880 --> 00:23:10.800]   There are some linear throughputs that sometimes make sense, but we've reached this point where
[00:23:10.800 --> 00:23:18.160]   breakthroughs in one technology tend to cause an acceleration in the research and the breakthroughs
[00:23:18.160 --> 00:23:25.760]   in other technologies. The problem is when our patients level and our enthusiasm for technology
[00:23:25.760 --> 00:23:34.800]   butts up against reality. And that happens a lot. It happens in the 1980s when after lots and lots
[00:23:34.800 --> 00:23:41.760]   of promises about artificial intelligence that failed to materialize, people started investors,
[00:23:41.760 --> 00:23:48.240]   the government stripped away funding because the practical realities and the commercialization
[00:23:48.240 --> 00:23:53.120]   of technology didn't pan out the way that they sort of, that the hype had promised.
[00:23:53.120 --> 00:23:59.600]   So we do run into a problem over and over again and it's cyclical. When we get these like
[00:23:59.600 --> 00:24:04.320]   inflections across lots of different technologies that cause this burst of activity.
[00:24:04.320 --> 00:24:10.320]   And then all of a sudden everybody expects sweeping change quickly, which of course never comes.
[00:24:10.320 --> 00:24:18.560]   On top of all of that, in order for us to have, for example, 5G, like who's going to build that?
[00:24:18.560 --> 00:24:24.640]   It's not part of our national infrastructure. We have four major wireless carriers and at the moment
[00:24:24.640 --> 00:24:30.320]   it would cost a lot of money for them to build out all of those cells. And there's a lot there.
[00:24:30.320 --> 00:24:34.000]   >> 5G might be the same position. Fiber is.
[00:24:34.000 --> 00:24:36.880]   >> Sure. >> Now, didn't we think fiber was going to change the world?
[00:24:36.880 --> 00:24:39.360]   >> Yeah, I have to say, the interesting thing is,
[00:24:39.360 --> 00:24:42.080]   >> Yeah, the first step in the situation just came out this week and they're talking at
[00:24:42.080 --> 00:24:45.520]   nationalizing 5G and having- >> Yeah, that is interesting.
[00:24:45.520 --> 00:24:49.120]   >> Basically, that's a good idea. >> I'm not automatically against that.
[00:24:49.120 --> 00:24:54.400]   >> No, their position is reasonable, which is it's a critical national infrastructure.
[00:24:54.400 --> 00:24:55.040]   >> Yeah.
[00:24:55.040 --> 00:25:00.720]   >> The problem is when I hear the Trump administration say that, you have to read between the lines.
[00:25:00.720 --> 00:25:06.000]   It's not really nationalized. They're against nationalization of anything. That's socialism.
[00:25:06.000 --> 00:25:11.040]   What they really are saying is we're going to have a consortium of corporate interests that will
[00:25:11.040 --> 00:25:16.560]   determine- >> What I'm guessing it's going to be is the American people footing the bill to-
[00:25:16.560 --> 00:25:21.440]   >> So that AT&T can make- >> More infrastructure for AT&T and Verizon to
[00:25:21.440 --> 00:25:23.600]   put this infrastructure in place. >> Hey, we got-
[00:25:23.600 --> 00:25:28.560]   >> Sounds like a bad idea. >> We got fiber and I've heard it said that we got fiber because of the
[00:25:28.560 --> 00:25:34.640]   dot com bust that there was so much money flowing into the markets at the end of the century.
[00:25:35.200 --> 00:25:42.000]   People were over building fiber, much of it's still dark, and then it crashed. But just the same way
[00:25:42.000 --> 00:25:48.880]   we got the railroads in the 19th century and then all the railroad companies went bust,
[00:25:48.880 --> 00:25:52.800]   but we still got an intercontinental railroad. We have intercontinental fiber, but it's all
[00:25:52.800 --> 00:25:55.680]   dark, right? >> So a couple of quick things.
[00:25:55.680 --> 00:25:58.000]   >> Yeah, correct all my historical mistakes.
[00:25:58.000 --> 00:26:04.640]   >> No, no, no, no. The first thing I'll say is we live in a different era. And I think we are
[00:26:04.640 --> 00:26:10.240]   less likely to go along with eminent domain for the good of everybody.
[00:26:10.240 --> 00:26:12.240]   >> That's right. >> Our culture has evolved.
[00:26:12.240 --> 00:26:15.680]   >> That's right. >> And so this is why you don't see a lot of transportation changes.
[00:26:15.680 --> 00:26:22.240]   To lay fiber means to temporarily disrupt or permanently disrupt some of our streets and some
[00:26:22.240 --> 00:26:27.120]   other things. >> We have enough transcontinental fiber. We don't have it to the curb.
[00:26:27.120 --> 00:26:29.920]   >> Right, now. >> We don't have it- >> The last mile problem, right?
[00:26:29.920 --> 00:26:37.120]   The last mile problem, I think again, a lot of times the big companies get a bad rap.
[00:26:37.120 --> 00:26:44.320]   I can tell you because I know some folks who worked on part of this project that in Philadelphia
[00:26:44.320 --> 00:26:52.800]   were Comcast is headquartered several times in Philly, in Baltimore, in some East Coast cities.
[00:26:53.520 --> 00:27:00.160]   Comcast was working with the local city government to bring fiber.
[00:27:00.160 --> 00:27:06.960]   And the city governments, the individual city governments were expecting all of these big
[00:27:06.960 --> 00:27:13.520]   concessions and big payouts. And they thought that they were playing hardball.
[00:27:13.520 --> 00:27:19.680]   And in reality, what they were doing was making it so extraordinarily expensive to lay fiber per foot.
[00:27:20.240 --> 00:27:28.320]   That it was easier for Comcast to say, you know what? Why would we bother? Why are we doing this?
[00:27:28.320 --> 00:27:36.640]   So I think that this is another key piece of this, whether we're talking about 5G or we're
[00:27:36.640 --> 00:27:44.320]   talking about AI or whatever technology. There's always multiple sides to what's happening and
[00:27:44.320 --> 00:27:47.040]   it's easy to blame the big tech companies. >> It's very complicated.
[00:27:47.040 --> 00:27:52.800]   >> It's complicated. It really is. >> And Brianna, I'm sorry because she didn't want to talk about
[00:27:52.800 --> 00:27:57.680]   politics. She wanted to talk about gadgets. But you know, you can't get very far in gadgets
[00:27:57.680 --> 00:28:01.040]   without talking about politics, especially infrastructure politics.
[00:28:01.040 --> 00:28:02.000]   >> Yeah, definitely.
[00:28:02.000 --> 00:28:07.680]   >> And yeah, I just so sometimes I feel like we have the potential and this great vision for
[00:28:07.680 --> 00:28:13.600]   the future, but we lack the wherewithal to make it happen for a variety of reasons. And I won't
[00:28:13.600 --> 00:28:16.720]   blame. I won't ascribe blame to any one entity.
[00:28:16.720 --> 00:28:23.120]   You know, because it does feel a little science fiction to me that we were someday, in my lifetime,
[00:28:23.120 --> 00:28:26.080]   have spectacles that we could send videos to each other like that.
[00:28:26.080 --> 00:28:31.040]   >> I mean, it kind of does. But again, if it's the case that we're already wearing
[00:28:31.040 --> 00:28:34.160]   glasses and increasingly we're going to have to be wearing glasses.
[00:28:34.160 --> 00:28:36.560]   >> We're all going to have to because we're blind for our phones.
[00:28:36.560 --> 00:28:43.280]   >> And if it's the case that technology tends to be, innovation tends to be cyclical, not singular.
[00:28:43.280 --> 00:28:49.760]   So, you know, just as it took a couple of times to get to social media that we all accepted,
[00:28:49.760 --> 00:28:54.640]   because remember there was a friendster and then MySpace. And then Facebook is when
[00:28:54.640 --> 00:29:00.320]   that concept finally hit. There have been AR glasses now in existence since the 90s.
[00:29:00.320 --> 00:29:06.000]   And they were around Google Glass. I think what we're going to start seeing is a rapid
[00:29:06.000 --> 00:29:10.800]   acceleration as all of these other pieces fall into place. So it feels very sci-fi,
[00:29:10.800 --> 00:29:15.680]   but I think if you had gone back, Leo, to 1998 and told yourself, guess what?
[00:29:15.680 --> 00:29:19.840]   All of these crazy things you're using, your Canary wireless sniffer, your
[00:29:19.840 --> 00:29:23.360]   Toshiba satellite pro that's heavy enough to double as a self-defense weapon.
[00:29:23.360 --> 00:29:24.400]   >> Yeah.
[00:29:24.400 --> 00:29:26.400]   >> You know, I do have to say this.
[00:29:26.400 --> 00:29:31.920]   >> They're going to be the iPhone. Actually, I have videotape of me in 1995 with a Palm
[00:29:31.920 --> 00:29:39.600]   Pilot. Or no, I think it was actually an Apple Newton saying someday these will be small enough
[00:29:39.600 --> 00:29:42.800]   they'll be ubiquitous internet and you'll be able to carry it in your pocket. You'll have always
[00:29:42.800 --> 00:29:47.600]   on connection to the internet. You'll have a pocket computer. That wasn't that far-fetched
[00:29:47.600 --> 00:29:49.680]   even in 1995. >> Right.
[00:29:49.680 --> 00:29:52.000]   >> The technology was there. >> Maybe not.
[00:29:52.000 --> 00:29:54.000]   >> That's why the Palm suffering came along.
[00:29:54.000 --> 00:29:54.320]   >> Right.
[00:29:54.320 --> 00:30:01.200]   >> But at the same time, we have to be realistic. I mean, I think when I was a kid, we all expect
[00:30:01.200 --> 00:30:03.440]   we'd have flying cars by now. >> Yeah, where are those?
[00:30:03.440 --> 00:30:09.200]   >> And there's a realistic risk to the public if you're giving them control of a
[00:30:09.200 --> 00:30:15.040]   device that can fly. I think in the same way, one of the things I thought was so interesting
[00:30:15.040 --> 00:30:20.880]   about this book about Oculus is a lot of their research that they did leading into it was about
[00:30:20.880 --> 00:30:25.920]   are people going to basically wear this goofy gadget on their head?
[00:30:25.920 --> 00:30:26.400]   >> Right.
[00:30:26.400 --> 00:30:30.320]   >> And they decide because it's virtual reality and you're in your home.
[00:30:30.320 --> 00:30:31.360]   >> And no, I can see you.
[00:30:31.360 --> 00:30:37.360]   >> Like, you're not worried about the social cost. But when I think about glasses and how
[00:30:37.360 --> 00:30:41.440]   you might eventually miniaturize that, you've got to get around the computing power.
[00:30:41.440 --> 00:30:46.560]   You've got to get around the batteries the really big, like 3D graphics.
[00:30:46.560 --> 00:30:52.880]   I can tell you firsthand how computationally expensive it is to like positionally track.
[00:30:52.880 --> 00:30:58.160]   So I think sometimes you hit a hard limit and I just-
[00:30:58.160 --> 00:31:00.560]   >> Yeah, that was my question. >> I have a hard time imagining that.
[00:31:00.560 --> 00:31:05.280]   >> Yeah. It is completely conceivable that you hit a wall on some technology.
[00:31:05.280 --> 00:31:10.720]   Of course, what happens, you end around and you find other ways to get there.
[00:31:10.720 --> 00:31:17.440]   >> I don't know. What I've seen and I was down visiting the Magic Leap guys not too long ago,
[00:31:17.440 --> 00:31:26.000]   what I've seen out of that shop, my experience using the headset, to me, this is not a big leap
[00:31:26.000 --> 00:31:30.080]   at all forward to think that we will have that compute.
[00:31:31.280 --> 00:31:38.560]   All of the- now, not tomorrow. I think a decade from now, this is entirely within the realm of
[00:31:38.560 --> 00:31:44.320]   plausibility. If I can just add one last thing about our expectations versus reality,
[00:31:44.320 --> 00:31:49.120]   when we think about the future of transportation, and for some reason when we think about the future,
[00:31:49.120 --> 00:31:55.280]   we always come back to the idea of flying cars. We think that oftentimes we're the first ones
[00:31:55.280 --> 00:32:00.880]   to be disappointed. The very first patent for a flying car, it's called the
[00:32:00.880 --> 00:32:07.440]   Curtis Autoplane. It was filed in 1913 and a concept was built the following year.
[00:32:07.440 --> 00:32:18.480]   It was 1917. For every decade since 1917, there has been a brand new set of prototypes of flying
[00:32:18.480 --> 00:32:24.880]   cars that have actually taken off. Those early models are hilarious. That's the Curtis Autoplane.
[00:32:24.880 --> 00:32:29.600]   Some of those early models are really funny because they were just whatever the existing car of the
[00:32:29.600 --> 00:32:40.800]   time with some wings stuck on it in a stronger engine. You have to stop and part of our difficulty
[00:32:40.800 --> 00:32:46.160]   in approaching the future in a way that doesn't fetishize the future, but instead take a really
[00:32:46.160 --> 00:32:52.480]   pragmatic look is pop culture. We've lived with the idea of flying cars for so long that it's really
[00:32:52.480 --> 00:32:58.560]   hard for us to conceptualize a future mode of transportation that doesn't involve a flying car.
[00:32:58.560 --> 00:33:04.640]   For God's sake, two weeks ago Boeing released its latest version of a flying car, which is like
[00:33:04.640 --> 00:33:13.040]   a quadcopter sort of thing with a helicopter cockpit. Instead, there's lots of other ways to
[00:33:13.040 --> 00:33:21.360]   get around. There's tunneling underground. There are personal low energy vehicles like scooters and
[00:33:21.360 --> 00:33:26.640]   golf carts. They're all different types of futuristic modes of transportation.
[00:33:28.080 --> 00:33:33.120]   We don't give them as much credence or credit because they're newer to us and we have a cognitive
[00:33:33.120 --> 00:33:37.840]   bias against accepting things that are new as plausible for the longer term.
[00:33:37.840 --> 00:33:42.640]   That's really a good point. We're comfortable with the notion of a notion of flying car.
[00:33:42.640 --> 00:33:47.760]   We are going to continue this conversation. I don't want to interrupt it. We have to,
[00:33:47.760 --> 00:33:52.160]   but you probably all could take a little break. Let your minds cool off for a moment.
[00:33:52.160 --> 00:33:57.520]   Amy Webb is here. Her new book is great. The big nine. We talked about it on triangulation this week.
[00:33:57.520 --> 00:34:02.880]   How the tech tightens their thinking machines could warp humanity. She has some great acronyms
[00:34:02.880 --> 00:34:09.680]   in here for the Chinese AI giants. Baidu, Alibaba, Tencent. Those are the bats.
[00:34:09.680 --> 00:34:20.480]   For the American AI giants, she's got the GMafia, Google, Microsoft, Apple, Facebook, IBM,
[00:34:20.480 --> 00:34:27.840]   and Amazon, the GMafia. Never the twain shall meet. We'll get to that in a little bit.
[00:34:27.840 --> 00:34:32.720]   Lots of good stuff. Thank you for being here, Amy. I really appreciate it. The book is fascinating.
[00:34:32.720 --> 00:34:37.360]   And as always, very well written. Brianna Wu is here. You're going to run again, Brianna.
[00:34:37.360 --> 00:34:42.400]   You're crazy. I'm already running again. This is all we're doing. This is an
[00:34:42.400 --> 00:34:49.840]   80-hour job. 2020? Your primary bid. That's our old site. We've got completely new one. It's
[00:34:49.840 --> 00:34:55.840]   Brianna Wu for Congress. Oh, good. All right. We'll find the new one. That's the 2018 site. Oh, my gosh.
[00:34:55.840 --> 00:35:01.600]   Brianna ran in the Massachusetts 8th. Did not make it in the primary. You have an incumbent
[00:35:01.600 --> 00:35:08.080]   who's been there for years in the Democratic Party. But AOC has showed us that the incumbency
[00:35:08.080 --> 00:35:14.000]   is no guarantee of re-election. It's true. So maybe there's a new breed coming to Congress.
[00:35:14.000 --> 00:35:21.040]   And if there is, Brianna's got to be a part of it. I got over 17,000 votes. We got almost half the
[00:35:21.040 --> 00:35:25.920]   number that we needed to win. And this is the first time I'd ever run for anything. So we're
[00:35:25.920 --> 00:35:30.720]   coming back. I know what I'm doing now. That's always a plus when you're trying to do something.
[00:35:30.720 --> 00:35:36.080]   And it's not like the people who voted for me last time are going to go away. We're going to
[00:35:36.080 --> 00:35:41.600]   build on that success. Right. Well, awesome. And where can people go if they want to find out more
[00:35:41.600 --> 00:35:47.200]   and contribute? So we actually, we've got a new site coming up this week. That's Brianna Wu
[00:35:47.200 --> 00:35:53.600]   for Congress. And you can support my campaign and technologically literate leadership in Washington
[00:35:53.600 --> 00:35:59.840]   at supportbriana.com. Nice. It's an exciting time to be in politics.
[00:36:01.040 --> 00:36:07.280]   I would say exciting. That's a very double method. May you live in interesting times.
[00:36:07.280 --> 00:36:12.320]   Definitely. Our show today brought to you by LastPass. We know that you know
[00:36:12.320 --> 00:36:19.280]   that the current password system is broken. But no one's come up with a replacement quite yet. So
[00:36:19.280 --> 00:36:27.440]   until they do, may I suggest a little password hygiene. LastPass is a password vault, a piece of
[00:36:27.440 --> 00:36:35.040]   software that works on every platform you use Mac, Windows, Linux, iOS, Android works with every
[00:36:35.040 --> 00:36:40.960]   browser to generate long, strong passwords. You couldn't remember in a million years. But
[00:36:40.960 --> 00:36:47.120]   don't worry. LastPass remembers them, records them, stores them securely. They're so encrypted
[00:36:47.120 --> 00:36:51.680]   that only you can read them. Not even the folks at LastPass can read them. They're only decrypted
[00:36:51.680 --> 00:36:57.680]   at device level and makes it easy to do makes it convenient. So for so long, we've been talking
[00:36:57.680 --> 00:37:02.320]   about the tradeoff between convenience and security. Well, this is one security tradeoff
[00:37:02.320 --> 00:37:07.520]   that actually is more convenient because LastPass will fill in the passwords in your browser,
[00:37:07.520 --> 00:37:14.880]   on your mobile phone iOS 12, Android. Now you just get to an app that you want to log into. LastPass
[00:37:14.880 --> 00:37:20.000]   fills in the password. It's actually fast and fun. You won't worry about reusing passwords
[00:37:20.000 --> 00:37:24.640]   because LastPass will generate new passwords for you. In fact, they even have a great security check.
[00:37:24.640 --> 00:37:29.920]   And a new feature in LastPass that I noticed just the other day will warn you when you log into a
[00:37:29.920 --> 00:37:36.560]   site, you've used that password elsewhere and give you the chance to clean that up. LastPass
[00:37:36.560 --> 00:37:41.520]   is used. It's the number one password manager out there. It's been used, it's used by 13 and
[00:37:41.520 --> 00:37:46.480]   a half million people worldwide. But it's also important for business. And that's really what
[00:37:46.480 --> 00:37:52.880]   I'd like to pitch you today. Your business, and if you own a business, you know, that you have to
[00:37:52.880 --> 00:37:56.960]   have employees and you know that most of your employees don't know as much as you do about
[00:37:56.960 --> 00:38:03.200]   security, about passwords. They're still using monkey 123, their dog's name and their birth date,
[00:38:03.200 --> 00:38:08.560]   or worse, they're posting it on the post it note on the front of the monitor. Or even worse than
[00:38:08.560 --> 00:38:12.320]   that, we know more than half employees share passwords, not only with other employees, but
[00:38:12.320 --> 00:38:18.400]   with friends and family. You need LastPass at work. It'll keep your passwords safe. You can actually
[00:38:18.400 --> 00:38:23.760]   give employees access to vital assets without giving them passwords. You let you have complete
[00:38:23.760 --> 00:38:28.320]   administrative oversight. You can set master password requirements, enable password resets,
[00:38:28.320 --> 00:38:34.640]   restrict access when needed over 100 policies. You get actionable security reports, shared folders,
[00:38:34.640 --> 00:38:39.360]   which I love. That means everybody in our ops team gets a full set of passwords in the shared
[00:38:39.360 --> 00:38:44.800]   folder. You just add them to that group. Same thing with our business department. Protect yourself
[00:38:44.800 --> 00:38:50.560]   in your business with LastPass, the number one most preferred password manager over 43,000 businesses,
[00:38:50.560 --> 00:38:56.320]   including to it. And we've used LastPass enterprise for many years ever since one of our employees
[00:38:56.320 --> 00:39:02.400]   posted all his passwords on a public website. Oh, God. Well, no, seriously, yeah, he's a smart guy.
[00:39:02.400 --> 00:39:07.760]   He made the website. It wasn't like, you know, Facebook, but it was also a public. What's he?
[00:39:07.760 --> 00:39:11.440]   No, yeah, we took it down real quick and installed LastPass.
[00:39:11.440 --> 00:39:18.560]   Leo, I had a nightmare this week because I had someone on my team. They were working on like
[00:39:18.560 --> 00:39:25.360]   our financial credentials. And I found out I was glancing at our team email and I found out
[00:39:25.360 --> 00:39:30.480]   that they were just emailing the passwords, like all of our financial credentials to each other.
[00:39:30.480 --> 00:39:37.280]   And I hit the roof. People in the, you go into politics. You don't necessarily think about
[00:39:37.280 --> 00:39:42.800]   information security. And this is why the DNC was hacked. It's really 16 because people don't think
[00:39:42.800 --> 00:39:48.000]   about this stuff. So you better believe like we are looking at solutions like LastPass.
[00:39:48.000 --> 00:39:52.880]   Well, you need seriously. We use it and we started using it ever since that happened.
[00:39:52.880 --> 00:39:58.000]   You can also say, for instance, and we do this, you have to have two factor authentication.
[00:39:58.000 --> 00:40:04.400]   LastPass works with all the devices. Duo security, Yubiki. They have their own authenticator. Of course,
[00:40:04.400 --> 00:40:08.720]   it works with Google authenticator. The LastPass authenticator is pretty cool because it's not a
[00:40:08.720 --> 00:40:15.040]   six digit code. It's a push pop pop up that says approver deny, which is very easy for employees,
[00:40:15.040 --> 00:40:20.080]   but also more secure than text messaging. I just, everything they've done at all,
[00:40:20.080 --> 00:40:24.320]   they've thought about it all. We interviewed Joe Seagrest, the founder of LastPass,
[00:40:24.320 --> 00:40:28.640]   Steve Gibson looked at the code and gave it his thumbs up. He uses LastPass. That ought to tell
[00:40:28.640 --> 00:40:34.080]   you something. In fact, it's everybody ought to be using it. There's LastPass for families.
[00:40:34.400 --> 00:40:39.040]   LastPass premium for personal use is LastPass teams. If you have 50 or fewer employees,
[00:40:39.040 --> 00:40:44.400]   but we use the big boy LastPass, LastPass enterprise, and we would never go back.
[00:40:44.400 --> 00:40:49.920]   And it's just awesome. You need LastPass. Lots of people use LastPass, but really,
[00:40:49.920 --> 00:40:53.760]   all that matters is you and your security. LastPass.com/twit.
[00:40:53.760 --> 00:40:58.160]   And if you are a LastPass fan and you're going to the security conference this week in San Francisco,
[00:40:58.160 --> 00:41:04.800]   the RSA conference, I'm going to be hosting the cocktail party after the show at Bourbon and Branch,
[00:41:04.800 --> 00:41:08.640]   along with Jason Hall and Megan Maroney. We're all going to be there. I love the LastPass team.
[00:41:08.640 --> 00:41:14.320]   So we just volunteered, hey, if you'll let us in, we'll go. It's a speak easy. So you have to have
[00:41:14.320 --> 00:41:19.760]   a password to go to the party. But if you're at RSA, just go to the LastPass booth and say Leo said,
[00:41:19.760 --> 00:41:25.440]   what's the password? And they'll tell you what the password is so you can get in.
[00:41:26.480 --> 00:41:30.560]   We'll see on Wednesday if you come to that LastPass.com/twit.
[00:41:30.560 --> 00:41:35.440]   What is the password in? What's our LastPass password?
[00:41:35.440 --> 00:41:42.080]   Is it shark? What is it? I can't remember for swordfish. That's it.
[00:41:42.080 --> 00:41:50.480]   I knew it was a fish of some kind. Swordfish. Let's see here. You wanted to talk Amy about
[00:41:50.480 --> 00:41:55.280]   this site. This person does not exist. I think it was you, Amy, right? This person does not exist.
[00:41:55.280 --> 00:42:03.200]   This is a really creepy site. That person doesn't exist. That's an AI generated using a technique
[00:42:03.200 --> 00:42:09.440]   called GAN. I've heard of adversarial networks, right? That was built by a guy named Philip,
[00:42:09.440 --> 00:42:16.080]   who's a former, I think he was a software engineer at Google, and he just took a publicly available
[00:42:19.440 --> 00:42:25.680]   GAN and system built by Nividia. Do you have to have a bunch of faces in a database?
[00:42:25.680 --> 00:42:30.800]   There are plenty of corpora that are available. Nividia actually built something like this
[00:42:30.800 --> 00:42:35.840]   somewhat similarly not too long ago. And if you go on the site toward the bottom,
[00:42:35.840 --> 00:42:40.000]   it was there the other day. I don't know if he took it off. There's a little explainer,
[00:42:40.000 --> 00:42:44.960]   a couple of links that shows the research and where all of this came from. There you go.
[00:42:44.960 --> 00:42:51.680]   And how he did it. The short answer is this is actually not all that complicated to do.
[00:42:51.680 --> 00:43:01.360]   And that is why after this came out, there were many other copycat sites making automatically
[00:43:01.360 --> 00:43:06.960]   generated images of other things. Oh, really? Oh, is there a cat's version of this?
[00:43:06.960 --> 00:43:14.000]   There is. So the really quickly generative adversarial networks are kind of like two systems
[00:43:14.000 --> 00:43:18.320]   playing the Turing test against each other, trying to fool each other until they come up with something
[00:43:18.320 --> 00:43:21.440]   that's believable. And so Nividia has been left behind. So that's the adversarial part.
[00:43:21.440 --> 00:43:26.160]   They're fighting each other. And this is a relatively new area, like a new branch of
[00:43:26.160 --> 00:43:33.440]   artificial intelligence research. So that's interesting. However, this cat does not exist,
[00:43:33.440 --> 00:43:40.240]   will give you nightmares because some of the cats that are automatically generated look kind
[00:43:40.240 --> 00:43:50.640]   of normal. I have seen things. I have seen things. I cannot unsee that that's not that bad. I should
[00:43:50.640 --> 00:43:59.920]   send you guys some photos. That one's pretty bad. I saw one where a cat's got. Oh, Lordy.
[00:43:59.920 --> 00:44:06.880]   Bad. They're really bad. Wow. So is this just a poorly designed algorithm?
[00:44:07.840 --> 00:44:13.680]   Or is it harder with cats? Yeah, you think there'd be enough cat pictures that you could find a good
[00:44:13.680 --> 00:44:18.160]   like set of information interpolate. There's a hole in that leopard. Oh, wow.
[00:44:18.160 --> 00:44:25.440]   I think I live with these cats. These look like my cats. Some of them are not so bad.
[00:44:25.440 --> 00:44:33.440]   Yeah. I don't know if this is really doing. Did you see that? I did show it again. I missed it.
[00:44:34.640 --> 00:44:39.840]   Oh my God. So that's a catman. Cat him. Hugh cat. It's like a cat.
[00:44:39.840 --> 00:44:46.320]   A cat. It's a cat. That's like the fresco that they destroyed. That's like the one who painted the
[00:44:46.320 --> 00:44:53.200]   face. That's what that looks like. Wow. Okay. So this is probably not the best use of
[00:44:53.200 --> 00:45:00.720]   GAN. I've ever seen. No, but but it is important because this same idea. So if you think about
[00:45:00.720 --> 00:45:07.920]   artificial intelligence from the very beginning, we've sort of built AI with the understanding
[00:45:07.920 --> 00:45:14.720]   that it would either beat us or replicate us. So that is what we're starting. That's what this
[00:45:14.720 --> 00:45:21.280]   is. So this person does not exist as a generative adversarial network. This is a hard thing to do
[00:45:21.280 --> 00:45:29.360]   because we are very subtly tuned as humans to recognize and flaws, errors. And this looks like
[00:45:29.360 --> 00:45:34.080]   real people. These look like people you know. That's right. I mean, there's some it's a little
[00:45:34.080 --> 00:45:41.040]   janky. So some of the you can see some, you know, it's bad. No, it's not bad at all. It's not bad.
[00:45:41.040 --> 00:45:46.320]   I have some fears about this for a lot of different reasons. I don't want to. I don't want to make
[00:45:46.320 --> 00:45:54.880]   this all political, but you know, they're one of the ways you find out if someone like harassing
[00:45:54.880 --> 00:46:00.960]   you online is a bot is you look at the picture that they're using. And generally it comes from
[00:46:00.960 --> 00:46:08.080]   another person's like photo online. There was a piece this week, basically by women of color,
[00:46:08.080 --> 00:46:12.480]   talking about what they were talking about is digital blackface, like basically people
[00:46:12.480 --> 00:46:19.120]   using avatars of people of color online to kind of represent themselves. We've had that happen.
[00:46:19.120 --> 00:46:26.640]   Real right to derail conversations. It's easy for me to imagine this technology being used as another
[00:46:26.640 --> 00:46:35.760]   component in information warfare to basically give bot armies that much more believability
[00:46:35.760 --> 00:46:42.480]   might be the word. So I realize like this kind of you've got two AIs battling each other. It's
[00:46:42.480 --> 00:46:48.640]   not hard to do this kind of technology is inevitable. But I do think it's worth thinking about how
[00:46:48.640 --> 00:46:54.480]   this is going to affect information warfare. I'm curious what you think about the open AI
[00:46:54.480 --> 00:47:00.240]   text generator, because that was the reason they said, I think this was just a publicity
[00:47:00.240 --> 00:47:04.560]   stuff. But I'd like to hear what you guys think. I know that I know some of the guys that worked on
[00:47:04.560 --> 00:47:09.920]   that. Okay. That project. So I don't think it was publicity. I think unfortunate. So quick
[00:47:09.920 --> 00:47:16.080]   background, open AI is a group that was has had some personal changes, but partially was
[00:47:16.800 --> 00:47:25.440]   founded partially by Elon Musk and some others. And it's intended to be a consortium where
[00:47:25.440 --> 00:47:32.240]   research is done for everybody, not just for singular companies. What they've done is built a
[00:47:32.240 --> 00:47:38.960]   random text generator, but it's very good and very convincing. So it basically you start with a
[00:47:38.960 --> 00:47:44.320]   human prompt. One example is like a teacher asking a student to write an essay on the Civil War,
[00:47:44.320 --> 00:47:52.720]   and the resulting text is really, really convincing. And what's different between that and, for example,
[00:47:52.720 --> 00:47:59.280]   narrative insights, which is one of the companies that's taking structured data like stocks or
[00:47:59.280 --> 00:48:04.640]   sport scores and generating automatically content that runs in news organizations,
[00:48:04.640 --> 00:48:10.160]   the difference that this is more open ended and obviously far more complicated.
[00:48:10.160 --> 00:48:15.680]   Yeah, because we've had who was it? The New York Post or the Daily News has had
[00:48:15.680 --> 00:48:23.840]   auto generated sports stories for eight years. But sports is such a narrow, constrained field.
[00:48:23.840 --> 00:48:29.360]   You can pretty much do it automatically. Now, I would argue that when it comes to college essay
[00:48:29.360 --> 00:48:34.480]   writing, that may be the same. All of our communications when done well are the result of formulas.
[00:48:34.480 --> 00:48:39.120]   So this is essentially building an algorithm that represents the formula that takes the
[00:48:39.120 --> 00:48:42.960]   information and spits it out in a way that we might. But here's what I think is,
[00:48:42.960 --> 00:48:50.720]   well, the thing that went wrong is that the press release and the way that it was discussed
[00:48:50.720 --> 00:48:58.800]   made it sound so scandalous and sort of made a big deal out of something that I think is remarkable,
[00:48:58.800 --> 00:49:06.960]   not like the sign of the apocalypse or something. The paper is very measured.
[00:49:07.760 --> 00:49:13.600]   In the press release, they said, "We're not going to release this technology because it's too dangerous."
[00:49:13.600 --> 00:49:18.800]   But this is a problem. When it comes to artificial intelligence,
[00:49:18.800 --> 00:49:23.840]   there is an extraordinary amount of misplaced optimism and fear. And I would put that
[00:49:23.840 --> 00:49:28.800]   description because the paper was fairly measured and interesting. And honestly, it wasn't that
[00:49:28.800 --> 00:49:36.720]   remarkable. However, the announcement was would be in the misplaced fear
[00:49:37.360 --> 00:49:45.040]   and intentionally sort of scandalizing something, which is not good for the AI community at all.
[00:49:45.040 --> 00:49:51.200]   Again, it changes expectations. And the last thing we need is to change expectations and
[00:49:51.200 --> 00:49:58.560]   get people either fired up about investing a ton more money and expecting products really soon
[00:49:58.560 --> 00:50:03.440]   or taking money out. We have to go forward with clear minds.
[00:50:04.080 --> 00:50:08.560]   But I do bring it up because it's directly in response to what Brianna was saying.
[00:50:08.560 --> 00:50:13.040]   And in fact, they say this in the summary of the paper. We can imagine the application of these
[00:50:13.040 --> 00:50:19.680]   models for malicious purposes, information jamming, generating misleading news articles
[00:50:19.680 --> 00:50:24.480]   and personating others online. I think the biggest issue would be perhaps comment jamming
[00:50:24.480 --> 00:50:33.040]   with seemingly real comments that are so quickly generated in such volume that
[00:50:33.760 --> 00:50:35.520]   overwhelms the normal, the humans.
[00:50:35.520 --> 00:50:40.640]   And this is just automating the humans. Yeah, because humans are already malicious.
[00:50:40.640 --> 00:50:43.520]   There are plenty of malicious actors who could write better than this.
[00:50:43.520 --> 00:50:49.680]   But there's a few. This is automating malicious propaganda. So instead of having,
[00:50:49.680 --> 00:50:56.080]   instead of the United States employees, people to do the same thing that the paper is predicting
[00:50:56.080 --> 00:51:01.760]   that the GAN could be used for it. So what I would say is we should always, technology always
[00:51:01.760 --> 00:51:06.320]   evolves in ways that we don't think through in advance. Something always goes wrong.
[00:51:06.320 --> 00:51:11.680]   So it's incumbent upon us to, again, lay out foreign advance. What are the scenarios for
[00:51:11.680 --> 00:51:16.160]   the catastrophic scenarios, the pragmatic scenarios, the optimistic scenarios, and think
[00:51:16.160 --> 00:51:19.280]   all of this through, which we should already be doing.
[00:51:19.280 --> 00:51:20.720]   >> There's a lot of things.
[00:51:20.720 --> 00:51:26.160]   >> Yeah, I would want to add to that. We don't have a choice whether this is going to happen.
[00:51:26.160 --> 00:51:31.440]   You could have magically every tech company in the United States and all of our startups tomorrow
[00:51:31.440 --> 00:51:35.600]   decide, "Oh, you know what? We're going to set this one out. It's too dangerous. We don't like it."
[00:51:35.600 --> 00:51:42.080]   There was a story that came out of the Wall Street Journal just a few months ago talking about how
[00:51:42.080 --> 00:51:50.000]   China in using AI for hacking foreign governments, this is something they're really,
[00:51:50.000 --> 00:51:54.880]   really getting serious about. So when it comes to propaganda, even if we put the brakes on this,
[00:51:54.880 --> 00:52:00.640]   someone else is going to develop it in another country. So Amy, I would really agree with you.
[00:52:00.640 --> 00:52:06.560]   I think the solution to this isn't to ban technology. I think it's to get a lot more
[00:52:06.560 --> 00:52:14.080]   cynical in our models for social media. Something, I think all of us, we've got
[00:52:14.080 --> 00:52:21.200]   pretty large social media presences. I think we're all used to the anger moms that can come
[00:52:21.200 --> 00:52:29.040]   towards you. And for me, I know I take someone that uses their real name a lot more seriously
[00:52:29.040 --> 00:52:33.200]   than someone with two random words and eight digits behind their Twitter handle.
[00:52:33.200 --> 00:52:38.960]   I think the answer for this particular model is going to be, we need to put more emphasis on
[00:52:38.960 --> 00:52:45.360]   identity. We need to put more emphasis on verifying that people are who they say they are.
[00:52:45.360 --> 00:52:52.160]   And I realized that has very serious implications for say, you know, LGBT people that may be coming
[00:52:52.160 --> 00:52:56.720]   out of the closet. They're trying to find authentic self. They're experimenting with their identity
[00:52:56.720 --> 00:53:02.320]   online. I don't want to erase people like that. But at the same time, we've got to be a lot more
[00:53:02.320 --> 00:53:08.800]   suspicious that just because a Twitter account exists, I've begun to increasingly doubt if there's
[00:53:08.800 --> 00:53:16.080]   a real person behind it. If that makes sense. Yes. And in fact, I think the actual paper
[00:53:16.080 --> 00:53:22.720]   in its summary from OpenAI was much more measured and said exactly that. This is the paragraph I
[00:53:22.720 --> 00:53:28.560]   found most compelling. These findings combined with earlier results on synthetic imagery like
[00:53:28.560 --> 00:53:36.080]   deep fakes, audio and video imply that technologies are reducing the cost of generating fake content
[00:53:36.080 --> 00:53:40.720]   and waging disinformation campaigns. So the public at large, you'll need to become more
[00:53:40.720 --> 00:53:46.960]   skeptical of texts they find online. That's all. Just or there are other ways around this too. I
[00:53:46.960 --> 00:53:52.480]   mean, you know, there's ways there are ways to digitally watermark content. There are plenty of
[00:53:52.480 --> 00:53:59.920]   ways to authenticate. I think that the key here is most of the content that's distributed online and
[00:53:59.920 --> 00:54:07.120]   certainly news organizations have not been fast enough to advance. You know, they're only recently
[00:54:07.120 --> 00:54:17.360]   and I think in a large part due to Craig Newmark of Craigslist, who's really been focused a lot on
[00:54:17.360 --> 00:54:23.520]   trust. Only recently, news organizations started to think through the technology side of this. So,
[00:54:23.520 --> 00:54:31.440]   you know, we can't say like, there are plenty of ways to create fake content. It's much easier now.
[00:54:31.440 --> 00:54:37.280]   It's easier to distribute it. So there's no way to put a lid on it. So the best way around it is to
[00:54:37.280 --> 00:54:44.240]   figure out how to authenticate real content. To make it difficult though, there's also, you know,
[00:54:44.240 --> 00:54:48.960]   sometimes people say, well, Twitter and Facebook and all these sites would be fixed if everybody
[00:54:48.960 --> 00:54:53.360]   just had to be their real person. They couldn't create fake accounts. But there's also a genuine
[00:54:53.360 --> 00:54:58.240]   need, I think, for anonymity online. Sure. So that's the challenge is, yeah, if everybody were
[00:54:58.240 --> 00:55:01.760]   forced to use their real names and you had to prove your identity before you could post,
[00:55:01.760 --> 00:55:05.920]   I think a lot of this would go away because most of these trolls don't want anybody to know it's them.
[00:55:05.920 --> 00:55:11.920]   But at the same time, it would eliminate some very valuable conversations that occur in anonymity.
[00:55:11.920 --> 00:55:18.000]   Yeah. So how do you do that? I think a lot of these issues you're trying to compete to
[00:55:18.000 --> 00:55:23.840]   fundamentally different principles here because I think all of us are here. We value in anonymity,
[00:55:23.840 --> 00:55:29.840]   right? Like we value privacy online. So if you're talking about free speech and free speech,
[00:55:29.840 --> 00:55:34.720]   if you're talking about authenticating every single person, I would point out that as a policy
[00:55:34.720 --> 00:55:41.120]   China employees. And I don't think many people would praise their internet as one of the more open
[00:55:41.120 --> 00:55:44.560]   world. It's probably antithetical to free speech, isn't it?
[00:55:44.560 --> 00:55:47.760]   It's very complicated. These are difficult issues to solve.
[00:55:47.760 --> 00:55:54.880]   The founders, I don't, the founders could not possibly have imagined the world in which we live,
[00:55:54.880 --> 00:56:00.160]   but also the ways that our behaviors have changed in response to all of this technology.
[00:56:00.160 --> 00:56:06.320]   There's a statesmanship that doesn't exist anymore in Washington, DC. If you think about FDR,
[00:56:07.680 --> 00:56:14.640]   who suffered from polio, who couldn't walk, who had serious physical issues, and it was never
[00:56:14.640 --> 00:56:19.600]   brought up. And maybe there were reasons to have brought it up, but it wasn't brought up. And I
[00:56:19.600 --> 00:56:27.440]   think he did very well by Americans. The problem is that some of the laws that we have, like free
[00:56:27.440 --> 00:56:34.800]   speech, don't entirely make sense in the era of technology and certainly not necessarily in an
[00:56:34.800 --> 00:56:43.600]   era of automated content generation. So we have to, I think, be more flexible in how we think about
[00:56:43.600 --> 00:56:53.280]   what the founders intended. And we have to approach a lot of our information laws with more
[00:56:53.280 --> 00:56:59.440]   sophistication than we have in the past. And I know that's an unpopular thing to say,
[00:56:59.440 --> 00:57:03.520]   but this is only going to get worse if we don't shift our thinking going forward.
[00:57:04.480 --> 00:57:12.000]   I have to say, I don't think it would be the worst thing in the world if outrage mobs lost power,
[00:57:12.000 --> 00:57:18.000]   because you couldn't always make sure that it wasn't an AI thing running it. I mean,
[00:57:18.000 --> 00:57:23.840]   if I critique someone online, my name is right next to that. I have sure had bad tweets like
[00:57:23.840 --> 00:57:32.960]   held against me in my professional career. I don't know. I think outrage as a political force online,
[00:57:32.960 --> 00:57:39.360]   I don't think if that questions were brought into the mix about the legitimacy of that,
[00:57:39.360 --> 00:57:41.920]   I don't think that would be the worst thing in the entire world.
[00:57:41.920 --> 00:57:44.960]   But that would be the end of Twitter. I mean, if you didn't have outrage, you have no Twitter.
[00:57:44.960 --> 00:57:48.400]   That's not how I use Twitter. I have to say.
[00:57:48.400 --> 00:57:53.360]   Oh, every time I say this, there's people like you say, oh, no, I find great value in it. I
[00:57:53.360 --> 00:57:58.000]   this is how, and I say, well, yeah, okay, I don't understand you of all people, Brandon,
[00:57:58.000 --> 00:58:02.800]   we should hate Twitter. It was used as a weapon against you.
[00:58:02.800 --> 00:58:08.320]   It is. I upset the NRA this week. It has been a tough week on Twitter.
[00:58:08.320 --> 00:58:11.120]   I think if I were a politician, I would never use Twitter.
[00:58:11.120 --> 00:58:17.200]   Ever. I know the idea, the theory is great. You could communicate directly with your
[00:58:17.200 --> 00:58:22.400]   constituency without intermediation, no press in between. But it's also hugely risky as
[00:58:22.960 --> 00:58:30.000]   many politicians will attest to. So Amy asked a question. What is what are outrage moms?
[00:58:30.000 --> 00:58:33.200]   I don't know what that is. I don't either, but I can imagine.
[00:58:33.200 --> 00:58:41.680]   You an outraged mother? I mean, I'm a mom, but like, mom, not mom.
[00:58:41.680 --> 00:58:46.320]   Mom, mom, not mom. Oh, good. Okay, let's clarify that because I thought you said mom as well.
[00:58:46.320 --> 00:58:51.680]   All right, that's good. I was thinking mothers against Trump driving. I don't know what. Yes.
[00:58:52.640 --> 00:58:59.040]   Mobs. About like say, so here's an example. By the way, that's a great show title.
[00:58:59.040 --> 00:59:01.520]   I had somebody write that down. Outraged moms. Okay. I just,
[00:59:01.520 --> 00:59:08.640]   you know, like think about Battlefield, Battlefield two. It came out. They had really
[00:59:08.640 --> 00:59:16.000]   exploitive micro transactions in that. And a lot of gamers got together and like basically
[00:59:16.000 --> 00:59:21.600]   screamed until EA changed that model. That's a good thing. But on the other hand of that,
[00:59:21.600 --> 00:59:29.280]   you have like game developers regularly like harassed online. If like a call of duty patch
[00:59:29.280 --> 00:59:36.480]   changes the strength of a gun and people don't like it. So like it's a tool. It's like a chainsaw.
[00:59:36.480 --> 00:59:40.880]   It could be used as a. I got to ask you a personal question because you were the target.
[00:59:40.880 --> 00:59:44.320]   One of the most famous targets of game or gate you were forced to move.
[00:59:45.040 --> 00:59:52.480]   You've experienced violence. People throw bricks through your windows. And I would say, I mean,
[00:59:52.480 --> 00:59:56.880]   maybe a lot of that happened. I don't know on 4chan and Reddit or somewhere. But I think Twitter
[00:59:56.880 --> 01:00:02.000]   was probably the primary vehicle of all that. Yeah. And you still support Twitter?
[01:00:02.000 --> 01:00:11.440]   I really mean this, Leo. I have had dozens upon dozens of talks with Twitter in back channel
[01:00:11.440 --> 01:00:17.920]   over the years talking about the policies that failed me and other women. And I've seen them
[01:00:17.920 --> 01:00:25.200]   really work their butt off to address those issues. And I could point to at least 10 things
[01:00:25.200 --> 01:00:30.880]   that happened to me that Twitter worked to change so it doesn't happen to other women.
[01:00:30.880 --> 01:00:37.280]   So I don't think Twitter is perfect. But I do think they have worked to address these issues
[01:00:37.280 --> 01:00:42.880]   in good faith. And I respect them for that. Wow. You're a bigger person than I am,
[01:00:42.880 --> 01:00:45.280]   or a sadist. What are the.
[01:00:45.280 --> 01:00:48.000]   A masochist, I think is the word you're looking for.
[01:00:48.000 --> 01:00:54.880]   Yeah. I mean, I killed my Twitter account with half a million user Twitter follower,
[01:00:54.880 --> 01:00:59.040]   Twitter account because you're still on, aren't you? Yeah. I mean, I see.
[01:00:59.040 --> 01:01:01.680]   I killed it and it's still there. So I don't know what's going on.
[01:01:01.680 --> 01:01:07.040]   I don't want to kill it. I want to deactivate it because I don't want somebody to be at Leo
[01:01:07.040 --> 01:01:13.920]   Laporte. But to me, it seems like no good has ever come out of Twitter.
[01:01:13.920 --> 01:01:18.320]   I don't know. And the early early, I hate to be one of those people back in my day.
[01:01:18.320 --> 01:01:19.440]   Yeah. Well, I know.
[01:01:19.440 --> 01:01:21.120]   When no one was on, it was great.
[01:01:21.120 --> 01:01:27.120]   I'll tell you how I use Twitter. I found it difficult to have a conversation.
[01:01:27.120 --> 01:01:33.120]   So I use it as a listening post mostly. I agree.
[01:01:33.120 --> 01:01:38.880]   And I use it in conjunction with Nuzzle, which is this really amazing app started by the guy
[01:01:38.880 --> 01:01:45.760]   who founded Friendster. And it's recently been bought by Chartbeat, not Chartbeat, by Scroll in,
[01:01:45.760 --> 01:01:50.560]   I think is the name of the company, Tony Hale, who was at Chartbeat, started his startup.
[01:01:50.560 --> 01:01:59.600]   Anyhow, what's cool about it is I have dozens and dozens of Twitter lists that I keep private,
[01:01:59.600 --> 01:02:02.720]   and they're all super granular on very, very granular topics.
[01:02:02.720 --> 01:02:08.720]   And I spent over a year putting all those lists together, and they're really representative of
[01:02:08.720 --> 01:02:16.800]   multiple viewpoints. And I update them all the time. And I use Nuzzle to surface when enough
[01:02:16.800 --> 01:02:24.240]   people have shared that those lists follow have shared or posted content, similar content,
[01:02:24.240 --> 01:02:27.040]   it will get aggravated by Nuzzle.
[01:02:27.040 --> 01:02:31.120]   I use Nuzzle. I love Nuzzle. Jonathan Abrams has been on before to talk about it.
[01:02:31.120 --> 01:02:33.600]   That's a sewage treatment plant.
[01:02:33.600 --> 01:02:40.880]   That's the equivalent of taking the Twitter fire hose and making it drinkable at the other end.
[01:02:40.880 --> 01:02:46.480]   You're not seeing Twitter. It's extracting the signals from Twitter.
[01:02:46.480 --> 01:02:51.280]   No, every now and then I type in terms that I know are flashpoints.
[01:02:52.560 --> 01:02:58.560]   And it's a horrible, disgusting, awful, bad, really, really bad. And when the whole deep fix
[01:02:58.560 --> 01:03:03.280]   thing was first happening, I guess last December, so earlier than that,
[01:03:03.280 --> 01:03:08.960]   a lot of that content was being shared out on Twitter, and I was shocked that it-
[01:03:08.960 --> 01:03:14.160]   On the other hand, you got to see what people were talking about, what was going on. It is a
[01:03:14.160 --> 01:03:16.880]   good way to put your finger on the pulse of what's happening.
[01:03:17.600 --> 01:03:23.360]   Yeah, so I use it for that. I speak at conferences or I do book signings or whatever.
[01:03:23.360 --> 01:03:25.680]   I can engage with your Twitter and a lot.
[01:03:25.680 --> 01:03:30.880]   Yeah, actually, I have a little script that runs. So for those of you who are listening in,
[01:03:30.880 --> 01:03:34.800]   who speak at conferences, here's a very clever way to use Twitter.
[01:03:34.800 --> 01:03:41.680]   So there's a little script. It requires a little bit of know-how and breaking keynote if you use it.
[01:03:41.680 --> 01:03:45.440]   But you can put in the presenter notes a little-
[01:03:45.440 --> 01:03:48.320]   you have the script running, you've got a little bit of code in the presenter notes,
[01:03:48.320 --> 01:03:53.760]   and then you can live tweet while you present. So I'll be at South by this weekend.
[01:03:53.760 --> 01:03:54.960]   Oh, that's cool.
[01:03:54.960 --> 01:03:58.240]   I'm one of the big featured room. There's be thousands of people there.
[01:03:58.240 --> 01:04:02.160]   I'm going to go through a lot of information. I always show my tech trends,
[01:04:02.160 --> 01:04:06.720]   because this is when the report comes out. So what I will be doing is I will be sharing
[01:04:06.720 --> 01:04:10.880]   out and live tweeting my own talk, sharing all of the data, all of the links.
[01:04:10.880 --> 01:04:13.440]   Wow, that's a really great way.
[01:04:14.240 --> 01:04:18.320]   For people, again, so like if you think of Twitter more as a utility than as a way to
[01:04:18.320 --> 01:04:24.240]   communicate with other humans, I think of it as a utility. And I find when I'm specifically
[01:04:24.240 --> 01:04:27.200]   using it as a utility, it to be quite useful.
[01:04:27.200 --> 01:04:32.400]   I think about when Cara Swisher, Jack Dorsey said, "Well, let's do a conversation on Twitter
[01:04:32.400 --> 01:04:33.440]   together." Oh my God, that was crazy.
[01:04:33.440 --> 01:04:38.720]   And it was a complete fan. I mean, this is the founder and CEO of Twitter,
[01:04:38.720 --> 01:04:44.080]   couldn't make the conversation work. That told me this is not a conversation,
[01:04:44.080 --> 01:04:48.160]   platform. No, it's not anymore. It's but it is a good utility. And I think if we re and,
[01:04:48.160 --> 01:04:59.360]   years ago, many years ago, I told Jack that they ought to, this was the beginning of some
[01:04:59.360 --> 01:05:02.080]   of the fake issues that we're dealing with now in the harassment and everything else.
[01:05:02.080 --> 01:05:08.000]   If you look at Twitter's numbers, they're stock, like their valuation.
[01:05:08.000 --> 01:05:11.680]   I could buy it. I could buy it. It's not running up. It's running down.
[01:05:11.680 --> 01:05:14.400]   Maybe that's what I'll do. I'll just buy Twitter.
[01:05:14.400 --> 01:05:19.360]   Well, but let's think about it. So this should be Twitter, like Trump getting elected should be
[01:05:19.360 --> 01:05:22.640]   the greatest thing that ever happened to Twitter. Yeah, the president uses it.
[01:05:22.640 --> 01:05:27.120]   Because not much that man tweets. And yet, they're not like they should be swimming in money and
[01:05:27.120 --> 01:05:33.280]   they're not. So if their investors were smart, if everybody was smart, they would turn this thing
[01:05:33.280 --> 01:05:38.080]   over to a foundation, let a foundation buy it out. And it should be turned into a newswire for
[01:05:38.080 --> 01:05:44.640]   the 21st century. We've talked a lot about that. Twitter is a utility, Twitter owned by some
[01:05:44.640 --> 01:05:53.600]   agency. And still have functionality still, but enable it. And there are ways to build in
[01:05:53.600 --> 01:05:57.360]   different layers of verification and authenticity and all of the other things. But I mean,
[01:05:57.360 --> 01:06:02.800]   it is not rocket science. If they wanted to weed all the trolls out and there are ways to make
[01:06:02.800 --> 01:06:07.040]   sure that it's humans who are actually sending out content, that's not complicated.
[01:06:07.040 --> 01:06:12.240]   Well, let's see. Okay. So you just said, Brianna, that you feel like they've made some positive
[01:06:12.240 --> 01:06:17.440]   changes, but not enough. So I'll give you an example of one of addresses, what you were just
[01:06:17.440 --> 01:06:23.600]   saying, Amy, it used to be where Twitter, if someone sent me a death threat, and I reported
[01:06:23.600 --> 01:06:30.160]   they got suspended, they could take 10 minutes, make another account, and just get back on there.
[01:06:31.200 --> 01:06:37.760]   Right now, what happens if you get suspended from that, and your IP address looks suspiciously,
[01:06:37.760 --> 01:06:41.920]   like one that just got suspended, it's going to ask you to have to give a phone number
[01:06:41.920 --> 01:06:49.840]   along with that. I do want to point out, though, that that solution is a very western solution,
[01:06:49.840 --> 01:06:55.360]   because if you start thinking about, you know, around the world, like a lot of people don't have
[01:06:55.360 --> 01:07:01.280]   a dedicated internet connection, they use their phone as their only internet device. So like,
[01:07:01.280 --> 01:07:05.360]   these problems, like one of the biggest things I've learned in talking to Twitter about these
[01:07:05.360 --> 01:07:10.880]   issues is just because something is a solution in America, that doesn't mean it doesn't have
[01:07:10.880 --> 01:07:17.600]   consequences to other societies. Like, look at, look at, you know, what's app and how that's being
[01:07:17.600 --> 01:07:23.040]   used to spread misinformation in India. Like, it's really terrifying. Like, we think, oh, this is
[01:07:23.040 --> 01:07:28.560]   great private conversation. And then it's kind of being used as a vector to spread
[01:07:28.560 --> 01:07:33.360]   things that are not true, and there's no way anybody can stop it. So, you know, there's a
[01:07:33.360 --> 01:07:37.840]   there's a more global view here that does make what Twitter is trying to do difficult.
[01:07:37.840 --> 01:07:43.520]   And I agree. And I would just say, I would just say, it's not as though all of these problems
[01:07:43.520 --> 01:07:48.480]   with Twitter popped up overnight. And if we're talking about, you know, and that's sort of one
[01:07:48.480 --> 01:07:53.040]   technology in one space, you know, we start thinking about other technologies like
[01:07:53.040 --> 01:07:58.640]   all of the things happening within an artificial intelligence, genomic editing in CRISPR,
[01:07:58.640 --> 01:08:04.480]   autonomous transport, and there's all these different areas of technology. It is incumbent
[01:08:04.480 --> 01:08:08.880]   upon the people, people, I think, building the technology to model out risk in advance. And I've
[01:08:08.880 --> 01:08:12.960]   had conversations with Twitter multiple times. And the thing that I've always heard is, well,
[01:08:12.960 --> 01:08:16.320]   we just didn't we were just working on the product. We're just focused on the product. We weren't
[01:08:16.320 --> 01:08:20.720]   thinking about making money. If I hear one more person say they're focused on the product,
[01:08:20.720 --> 01:08:24.400]   you know, it's possible to focus on the product while thinking about the ramifications and
[01:08:24.400 --> 01:08:29.360]   implications of that product. And quite honestly, that's what the management team should be doing.
[01:08:29.360 --> 01:08:38.160]   Risk profiling is should be part and parcel of what every tech company is working on. And, you know,
[01:08:38.160 --> 01:08:42.960]   that that's how we prevent that's how we, you know, that's how we reverse engineer preferred
[01:08:42.960 --> 01:08:49.120]   futures to now so that Twitter isn't the, you know, that it doesn't have the kind of problems,
[01:08:49.120 --> 01:08:54.800]   the world changing problems that we're now starting to see. It didn't happen overnight, you know.
[01:08:54.800 --> 01:09:00.400]   So there's something I have to say about this. I watched Terminator 2 for the first time
[01:09:00.400 --> 01:09:06.160]   a couple of weeks ago. And I've not seen it for a long time. I really mean this. I've been very
[01:09:06.160 --> 01:09:09.200]   serious. Oh, not for the first time ever, but just for the first time in a while.
[01:09:09.200 --> 01:09:14.720]   For the first time in a long time. And there was a character in there that I was not able to
[01:09:14.720 --> 01:09:22.480]   appreciate as a young person. But as someone 41 in 2019, I really saw in a different way.
[01:09:22.480 --> 01:09:28.160]   And that was Miles Dyson. So do you all remember the scene in Terminator 2 where he's got the model
[01:09:28.160 --> 01:09:33.440]   of the Skynet thing in his office and he's ignoring his family and always talking about
[01:09:33.440 --> 01:09:38.240]   all the glorious things Skynet is going to be able to do when it's built. It's going to be
[01:09:38.240 --> 01:09:43.040]   a jet pilot that can fly and we'll never get tired. It's never going to make mistakes.
[01:09:43.040 --> 01:09:49.760]   She can put you in charge of weapon systems. And I saw that and I had not seen this movie since I was
[01:09:49.760 --> 01:09:58.640]   like a young 20 something really getting interested in engineering. And I thought about my perspective
[01:09:58.640 --> 01:10:04.160]   back then where I could only see the glory and the promise about all the things we were building,
[01:10:04.160 --> 01:10:11.520]   like connecting everyone online. Information online, like all this stuff, music online.
[01:10:11.520 --> 01:10:17.440]   And I hadn't really had the capacity in my 20s to think about, well, what's going to happen to
[01:10:17.440 --> 01:10:25.280]   music artists when the only way they can make a living is to go on tour forever. Or what's going
[01:10:25.280 --> 01:10:32.640]   to happen to our democracy when there are no editors deciding what stories are or not important.
[01:10:32.640 --> 01:10:38.560]   And most of what people are watching is cat videos. There's something fundamentally
[01:10:38.560 --> 01:10:48.320]   naively optimistic about engineers. And I count myself among that where we don't think about the
[01:10:48.320 --> 01:10:55.440]   ways the things we build could be misused. And I think that's something we need to start having
[01:10:55.440 --> 01:11:02.000]   very serious conversations about in school. Because like you said, Amy, this is over and over and over
[01:11:02.000 --> 01:11:08.720]   and we just seem to not have the cognitive capacity to think about this stuff.
[01:11:08.720 --> 01:11:13.920]   Well, there's two things I agree with both of you they ought to. So that's the first thing is we've
[01:11:13.920 --> 01:11:18.640]   got to stop being naive and start kind of really thinking of the consequences. But there's also
[01:11:18.640 --> 01:11:24.240]   the issue. I don't know how doable that is. How I mean, you're a futurist. Did the tools exist,
[01:11:24.240 --> 01:11:31.360]   Amy? Did the tools exist in 2006? Yes. For Jack and Eve to say, well, this is really cool, but.
[01:11:31.360 --> 01:11:40.400]   Yeah. So the role of again, like the role of a futurist is not to predict the future. It's to
[01:11:40.400 --> 01:11:44.720]   model out risk and opportunity. And as a discipline, this has been around for 100 years.
[01:11:44.720 --> 01:11:48.320]   And it would work. You would have worked in 2006. You could have foreseen this.
[01:11:49.120 --> 01:11:56.720]   You could have foreseen you on musk. We're seeing exactly how we. Yeah. I mean,
[01:11:56.720 --> 01:12:01.200]   I guess so. So I advise parts of the government. So I'm trying to figure out what I can say and
[01:12:01.200 --> 01:12:04.880]   what I can't. I've decided I'm not going to say what I was going to say. But let me
[01:12:04.880 --> 01:12:10.960]   do this. Let me say this. Come on. They're not listening. It is totally possible
[01:12:12.480 --> 01:12:19.920]   with the right tools. And this is not difficult stuff. It takes a willingness to confront deep
[01:12:19.920 --> 01:12:27.120]   uncertainty with facts, not cognitive biases. And that requires data evidence. So there are
[01:12:27.120 --> 01:12:33.200]   plenty of tools. In fact, all of mine, I've open sourced all of my research. So every all of our
[01:12:33.200 --> 01:12:37.760]   research, all of our tools, everything is open source. It's all free. You can take it, use it,
[01:12:37.760 --> 01:12:44.240]   I hope remix it, do whatever you want with it. And it is totally possible to model out what are
[01:12:44.240 --> 01:12:49.440]   all of the plausible ways in which things can go wrong now, just because and that is what I do
[01:12:49.440 --> 01:12:56.160]   for a living. So my job is to work with lots of different organizations and all around the world,
[01:12:56.160 --> 01:13:02.640]   mostly on risk, sometimes on growth and opportunity to, but I'm good at risk.
[01:13:02.640 --> 01:13:06.320]   That's interesting. So that's really interesting. So people really are saying,
[01:13:06.320 --> 01:13:09.600]   okay, what are the risks? That's right. Now it's really interesting.
[01:13:09.600 --> 01:13:14.800]   But then once we have the risk profile set up and all of the possible ways in which things can go
[01:13:14.800 --> 01:13:19.920]   wrong, first of all, that's not static because there are brand new variables that get introduced
[01:13:19.920 --> 01:13:23.840]   all the time and things are constantly changing. Second of all, somebody's got to be willing to
[01:13:23.840 --> 01:13:27.920]   take action. So the other piece of this is because Brian, I totally agree with you. Most of the
[01:13:27.920 --> 01:13:33.040]   engineers that I know are pretty happy people. And they're pretty excited about changing the world
[01:13:33.040 --> 01:13:38.240]   in positive ways. They don't love it when I come in and tell them all the ways in which
[01:13:38.240 --> 01:13:46.080]   all of their wonderful work could go horribly horribly wrong. But better that we acknowledge it
[01:13:46.080 --> 01:13:52.560]   so that we can say, well, if this is the case, now we build in this extra feature or now we erase
[01:13:52.560 --> 01:13:56.400]   the corpus that we're using to do this training and we start with something totally fresh that
[01:13:56.400 --> 01:14:02.400]   doesn't have all the bias, whatever it might be. Or maybe we slow down development a little bit,
[01:14:02.400 --> 01:14:06.480]   or maybe we sandbox part of what we're doing and test it under these different circumstances.
[01:14:06.480 --> 01:14:12.240]   What winds up happening is it takes a little longer and it may cost a little bit more money.
[01:14:12.240 --> 01:14:18.160]   And in this day and age, unfortunately, the investors have very little patience. And so
[01:14:18.160 --> 01:14:25.200]   there's this rush to productize and commercialize everything fast, which unfortunately butts up
[01:14:25.200 --> 01:14:30.400]   against a lot of risk modeling. But, Leo, to answer your question, of course it was possible.
[01:14:31.440 --> 01:14:36.800]   You just have to have the fortitude to do it. And then once you see what the risk profile is,
[01:14:36.800 --> 01:14:39.760]   to take some kind of action, it doesn't mean that the product has to die.
[01:14:39.760 --> 01:14:40.880]   Is it too late now?
[01:14:40.880 --> 01:14:50.960]   Yeah. I mean, yes. We've seen the risk profile. I created some of the risk profiles for the state
[01:14:50.960 --> 01:14:55.040]   of this. This would be much more credible if you could bring me a paper you wrote.
[01:14:55.760 --> 01:15:01.280]   A note you wrote to F Williams in 2007 saying, "But, dude, watch out for the president."
[01:15:01.280 --> 01:15:06.080]   Well, I didn't model that part out, but I didn't.
[01:15:06.080 --> 01:15:10.160]   Actually, though, in hindsight, and of course everything's easier in hindsight,
[01:15:10.160 --> 01:15:15.280]   it would make sense that that's exactly because of the direct communication to
[01:15:15.280 --> 01:15:21.920]   that a demagogue would love Twitter. In fact, demagogues all over the world love Twitter and
[01:15:21.920 --> 01:15:28.160]   Facebook. There was plenty of evidence to model in the year 2014 and 2015
[01:15:28.160 --> 01:15:31.520]   foreign influence wreaking havoc on Twitter.
[01:15:31.520 --> 01:15:34.080]   Well, but I mean, even just like Hitler would have loved Twitter.
[01:15:34.080 --> 01:15:35.040]   Sure.
[01:15:35.040 --> 01:15:36.320]   Gubbles would have eaten it up.
[01:15:36.320 --> 01:15:37.280]   Yep.
[01:15:37.280 --> 01:15:39.120]   And they would have loved AI.
[01:15:39.120 --> 01:15:40.960]   Oh, great.
[01:15:40.960 --> 01:15:42.400]   So...
[01:15:42.400 --> 01:15:49.520]   I think on a happier note, I would say this when the things that make me...
[01:15:49.520 --> 01:15:49.760]   I'm Jewish.
[01:15:49.760 --> 01:15:50.080]   What do you...
[01:15:50.080 --> 01:15:54.560]   I would have been bad situation for me.
[01:15:54.560 --> 01:15:58.480]   It seems highly likely that there is somewhere out there a baby Hitler.
[01:15:58.480 --> 01:16:04.480]   And all these tools are just waiting for her or him to come along.
[01:16:04.480 --> 01:16:13.200]   We also assume that the next tragedy or the next villain will look like the ones that we've
[01:16:13.200 --> 01:16:19.360]   already always seen. I would argue that we don't need another Hitler to wreak that kind of horrific
[01:16:19.360 --> 01:16:22.960]   damage. And in fact, I would say that there are plenty of...
[01:16:22.960 --> 01:16:24.240]   No, we have to.
[01:16:24.240 --> 01:16:24.720]   Terta.
[01:16:24.720 --> 01:16:33.280]   We have people who are happy to use social media to wreak havoc.
[01:16:33.280 --> 01:16:40.720]   I do think computer science is getting to a point where we've kind of tackled the low hanging
[01:16:40.720 --> 01:16:45.600]   fruit. I don't think there's going to be another breakthrough that's happening by
[01:16:45.600 --> 01:16:49.200]   the equivalent of Mark Zuckerberg in his dorm room.
[01:16:49.200 --> 01:16:50.160]   Really? Really?
[01:16:50.160 --> 01:16:50.720]   Really?
[01:16:50.720 --> 01:16:51.920]   Well, yeah, I do.
[01:16:51.920 --> 01:16:54.880]   Because I think if you think everything that's going to be invented has been invented?
[01:16:54.880 --> 01:16:57.120]   No, no. That's not my point at all.
[01:16:57.120 --> 01:16:58.640]   I think that there is...
[01:16:58.640 --> 01:16:59.440]   How can I put this?
[01:16:59.440 --> 01:17:03.920]   If you read the Elizabeth Holmes book, one of the things they talk about is how
[01:17:03.920 --> 01:17:10.160]   it takes so long to get next leap breakthroughs in medical science.
[01:17:10.160 --> 01:17:14.480]   And there's a reason why people to get their Nobel Prizes, they're in their 70s.
[01:17:14.480 --> 01:17:19.440]   Because we've kind of taken the lower hanging fruit there.
[01:17:19.440 --> 01:17:24.240]   And now it takes years of research and real understanding to hit breakthroughs.
[01:17:24.240 --> 01:17:32.480]   I think computer science is getting to a point where there certainly work smaller teams can do.
[01:17:32.480 --> 01:17:39.520]   But I think increasingly as we're moving forward, it's going to be like teams of 20, 30 people and
[01:17:39.520 --> 01:17:43.360]   millions of dollars to build things that are truly useful.
[01:17:43.360 --> 01:17:48.800]   And one of the reasons that makes me feel a little hope for the future is I think that
[01:17:48.800 --> 01:17:54.640]   as we think about what the structure of engineering teams needs to be moving forward,
[01:17:54.640 --> 01:18:02.640]   I think there is a spot there to have like ethicists on hand to start thinking about these models.
[01:18:02.640 --> 01:18:04.320]   And how those need to be...
[01:18:04.320 --> 01:18:07.920]   The things we build could be misused.
[01:18:07.920 --> 01:18:14.080]   I think there is room for a professional standard of ethics for engineers the same way we have those
[01:18:14.080 --> 01:18:21.280]   for lawyers. So I think moving forward, I do think we're going to see a more of a professionalization
[01:18:21.280 --> 01:18:27.760]   of software development that I think could help us kind of keep some of these issues in check.
[01:18:27.760 --> 01:18:31.760]   I don't want to interrupt, but I have to. We're going to stop down for a moment for a word from our
[01:18:31.760 --> 01:18:37.840]   sponsors. I will keep the thread going. It feels like sometimes that we relitigate Twitter on
[01:18:37.840 --> 01:18:44.160]   every episode. But it is kind of an intractable, difficult problem. But let's move on because
[01:18:44.160 --> 01:18:49.520]   you said something, Amy, that kind of scared me a little bit, which is Hitler would love AI.
[01:18:49.520 --> 01:18:51.760]   And I want to talk about that when we come back.
[01:18:51.760 --> 01:18:56.320]   I always like, I'm sure that's going to be the name of the episode.
[01:18:56.320 --> 01:19:00.240]   No, no, I can't put Hitler in the name of the episode. It's too horrible.
[01:19:00.240 --> 01:19:04.960]   But your point is well taken. Let's put it this way, a demagogue.
[01:19:05.520 --> 01:19:10.880]   That's right. Would love AI. And what does that mean? And I have an example, which is going to
[01:19:10.880 --> 01:19:18.000]   come up in just a moment. But first, a word from this is this is the good AI, the nice AI,
[01:19:18.000 --> 01:19:25.440]   the AI that aims to please 1000 eyes, which really, it was it's kind of a cool story. It was founded
[01:19:25.440 --> 01:19:34.640]   about 10 years ago at UCLA with really a breakthrough research in networks and how they work.
[01:19:35.600 --> 01:19:40.080]   And they got funding from the National Science Foundation. They created a great company. And now
[01:19:40.080 --> 01:19:44.560]   their tools are available to you. And I think you should really call it million eyes. Because what
[01:19:44.560 --> 01:19:53.840]   it is, is it sensors in every part of the internet, 1000 eyes, unique path visualization technology
[01:19:53.840 --> 01:20:00.240]   extends beyond boundaries. You can see, understand and improve the experience for all your app services
[01:20:00.240 --> 01:20:06.640]   and websites, right down to the coffee shop from all the way from the cloud to the coffee shop.
[01:20:06.640 --> 01:20:12.800]   There are, you know, other typical IT monitoring is kind of passive. It operates in the silo. It
[01:20:12.800 --> 01:20:16.880]   could see your data center. But the world is moving to the cloud. It is a very different
[01:20:16.880 --> 01:20:24.080]   experience. And if you offer software or tools or cloud services, you know, if you're in this
[01:20:24.080 --> 01:20:29.520]   business, the things happen and you just go, we can never know. That's why they draw the cloud
[01:20:29.520 --> 01:20:37.200]   as a puffy little opaque cloud. You, we can never know what happened there in the cloud, except now
[01:20:37.200 --> 01:20:44.000]   you can't. It's like you've been looking through a mirrored smudged window and somebody comes along
[01:20:44.000 --> 01:20:50.160]   with a squeegee 1000 eyes is the squeegee that gives you a perfect, immediate unmatched view of
[01:20:50.160 --> 01:20:54.800]   all the networks, all the dependencies that impact your users digital experience.
[01:20:56.400 --> 01:21:01.440]   This you are, as soon as you try this tool, you're going to say, how did we live without it?
[01:21:01.440 --> 01:21:07.120]   And we can never live without it again. You know how it is. You've got a cloud app or a service.
[01:21:07.120 --> 01:21:14.000]   It goes down or worse. It's worse. It's not even down. It's like weirdly slow in certain regions
[01:21:14.000 --> 01:21:21.200]   or just something strange is happening. And you just, you have no idea with 1000 eyes, you can see
[01:21:21.200 --> 01:21:26.240]   exactly what's happening. You don't see people when they're moving to the cloud, one of the things
[01:21:26.240 --> 01:21:30.320]   business loves the cloud. There's a lot of, but one of the things that scares people is as you gain
[01:21:30.320 --> 01:21:35.200]   agility with the cloud, you increase risk. And this is what really scares people, you lose control.
[01:21:35.200 --> 01:21:41.440]   So suddenly something's going on. You just can't tell what you're scrambling to find the cause,
[01:21:41.440 --> 01:21:46.480]   you're losing revenue, employee productivity is declining. Your brand image is going down.
[01:21:46.480 --> 01:21:53.440]   Your users are clamoring. They're at the door with pikes and torches going, we fix it.
[01:21:54.400 --> 01:21:58.800]   What if all of a sudden you had instant visibility into your entire service delivery path from the
[01:21:58.800 --> 01:22:02.960]   cloud to your end user, even all the portions you don't own, you don't control, because a lot of
[01:22:02.960 --> 01:22:08.960]   it you don't. 1000 eyes is cloud based software. It's unlike anything you've seen before, sensors
[01:22:08.960 --> 01:22:16.080]   all over, built to help organizations do the cloud right. A massive array of vantage points,
[01:22:16.080 --> 01:22:21.200]   they span the global internet, private clouds, even the Wi-Fi in your local coffee shop.
[01:22:22.400 --> 01:22:27.440]   It is amazing. Top banks use it. Some of the biggest companies in the world enterprises,
[01:22:27.440 --> 01:22:32.960]   SaaS companies, the world's largest and fastest growing brands all use 1000 eyes software to do
[01:22:32.960 --> 01:22:38.800]   the cloud and do it right. All I could say is if you're in this business and you do not know
[01:22:38.800 --> 01:22:44.720]   what's going on with your cloud, you need to know you need 1000 eyes go to 1000 eyes.com/twits.
[01:22:44.720 --> 01:22:49.840]   See what you've been missing. They have a great book you can give to the boss. It's an ebook called
[01:22:49.840 --> 01:22:55.840]   five cloud migration challenges you shouldn't ignore. But really if the cloud is important to you,
[01:22:55.840 --> 01:22:59.840]   or it's going to be important to your future, you've got to find out about this. It is an amazing
[01:22:59.840 --> 01:23:08.720]   tool. They just blow me away. 1000 eyes.com/twits. 1000 eyes.com/twits thrive in a connected world.
[01:23:08.720 --> 01:23:16.160]   Have 1000 eyes. So we've been talking about AI. We've been talking about social. We've been
[01:23:16.160 --> 01:23:24.400]   talking about how repressive governments might use it. Now comes the statistic we've talked
[01:23:24.400 --> 01:23:33.840]   before about China and their social credit scores. We're now learning that it would be air travelers
[01:23:33.840 --> 01:23:39.280]   in China last year were blocked from buying tickets 17 and a half million times for social
[01:23:39.280 --> 01:23:45.440]   credit offenses, including unpaid taxes and fines. Five and a half million times from buying
[01:23:45.440 --> 01:23:51.360]   train tickets. This is Chinese information from the National Public Credit Information Center.
[01:23:51.360 --> 01:23:55.600]   In addition, 128 people were blocked from leaving China.
[01:23:55.600 --> 01:24:06.320]   This is social credit. We talked to Amy on Friday about China's BRI, their foreign
[01:24:06.320 --> 01:24:12.320]   investment and outreach and how you think they're going to export the social contract.
[01:24:13.280 --> 01:24:20.240]   So that's part of what my new book, The Big Nine, is about. It's about the nine big companies
[01:24:20.240 --> 01:24:25.200]   that control the future of artificial intelligence because they have the lion's share of patents.
[01:24:25.200 --> 01:24:31.760]   They are building the frameworks, the chipsets, the custom silicon. They are funding and partnering
[01:24:31.760 --> 01:24:37.840]   with the schools where in the universities where courses are being taught. Three of those
[01:24:38.240 --> 01:24:43.840]   companies are based in China. Those are Baidu, Alibaba, and Tencent. While they're independent
[01:24:43.840 --> 01:24:48.320]   companies, they very much work under the thumb of Beijing. There have been a couple of
[01:24:48.320 --> 01:24:53.600]   important things that have happened in China over the past couple of years. For one,
[01:24:53.600 --> 01:24:58.880]   China's president is brilliant. His name is Xi Jinping. He's a president for life.
[01:24:58.880 --> 01:25:04.640]   They've recently changed how things work. He's effectively president for life,
[01:25:04.640 --> 01:25:12.320]   which is a very easy way to consolidate power and to push new regulations and big strategies
[01:25:12.320 --> 01:25:18.880]   through. One of those big strategies is something called the BRI or the Belt and Road Initiative.
[01:25:18.880 --> 01:25:26.960]   Initially, people thought that this was about China rebuilding its old silk trading route for
[01:25:26.960 --> 01:25:32.400]   the purpose of economic development and trade. However, it's important to look at
[01:25:33.920 --> 01:25:40.240]   all of these things with a bigger, broader lens. China currently has 58 pilot countries
[01:25:40.240 --> 01:25:45.600]   that are not just getting physical infrastructure built, but also digital infrastructure,
[01:25:45.600 --> 01:25:50.480]   which is why there is 5G rolling out. It's just not rolling out in the US. It's rolling out
[01:25:50.480 --> 01:25:59.280]   in part in China, but also among these 58 countries. They're laying fiber. They are helping these
[01:25:59.280 --> 01:26:05.920]   countries, many of which have autocratic leaders like the Philippines. They're helping them
[01:26:05.920 --> 01:26:13.200]   start to collect data on all of their citizens using many of the same tools and AI and recognition
[01:26:13.200 --> 01:26:23.920]   systems and all the rest that are currently in place in vast parts of China. The parts of the
[01:26:24.560 --> 01:26:31.680]   social credit score is now being deployed to other countries around the world. You may say to yourself,
[01:26:31.680 --> 01:26:38.320]   "So what? I'm an American or I'm wherever you have a Canadian, I'm German, I don't live there,
[01:26:38.320 --> 01:26:43.760]   what does that matter?" I think the reason that this matters is, again, if you model out what this
[01:26:43.760 --> 01:26:50.480]   looks like over many years to come, to me it seems entirely plausible that China is rebuilding
[01:26:51.040 --> 01:26:56.080]   the world and subdividing it in a new world order with China at the helm.
[01:26:56.080 --> 01:27:01.840]   Do they have ambitions of world domination or is it an economic?
[01:27:01.840 --> 01:27:06.080]   I think it's both. When we talked a little bit about this on triangulation,
[01:27:06.080 --> 01:27:12.720]   China's approaching 1.4 billion people. It's a lot of people spread out over a very big
[01:27:12.720 --> 01:27:19.040]   swath of land. Part of this is social control, in part because while China's economy may have
[01:27:19.040 --> 01:27:23.520]   slowed down a little bit, it's up-rally trending.
[01:27:23.520 --> 01:27:30.240]   And all of those people are about, China's about to see upward social mobility at a scale
[01:27:30.240 --> 01:27:33.760]   we've never seen before in human history. These people are going to want to buy stuff.
[01:27:33.760 --> 01:27:37.040]   This is good, right? They already want buy stuff.
[01:27:37.040 --> 01:27:41.440]   That's good. It was a feudal economy 100 years ago.
[01:27:41.440 --> 01:27:46.720]   Sure. The problem is that China can't build and buy, manufacture all the stuff that all of these
[01:27:46.720 --> 01:27:53.200]   people need. Part of this is renegotiating trade agreements and building new trade partners.
[01:27:53.200 --> 01:27:58.480]   That's part of it. I will be perfectly blunt. The other part of this is that China really doesn't
[01:27:58.480 --> 01:28:06.560]   love our Western Democratic ideals. They would much prefer to see a world reshaped where the
[01:28:06.560 --> 01:28:12.560]   United States and our allies and our Western Democratic ideals are not the de facto global
[01:28:12.560 --> 01:28:23.200]   standard. They would much rather see their market communism. They're a special brand of communism
[01:28:23.200 --> 01:28:29.280]   as a predominant ideology all over the world.
[01:28:29.280 --> 01:28:33.600]   And if you're not worried about this, take a look. This is the Belt and Road Initiative map
[01:28:33.600 --> 01:28:40.080]   from Wikipedia. The red is China. The orange is members of the Asian infrastructure investment
[01:28:40.080 --> 01:28:45.760]   bank. These are people who China is sending money to. The black is the Belt and Road corridors.
[01:28:45.760 --> 01:28:55.200]   That's a scary map. So this is like there's a lot here, but part of what's happening is technology.
[01:28:55.200 --> 01:28:59.760]   So we may not have Huawei phones in the US, but the phones are pretty awesome, actually.
[01:28:59.760 --> 01:29:06.640]   They're great. They're great. They're being deployed. I love my P20. In fact, Paul Farrah from
[01:29:06.640 --> 01:29:12.000]   Windows Weekly shot all these great pictures at Mobile World Congress with his Huawei P20 phone.
[01:29:12.000 --> 01:29:17.520]   It's an amazing phone. That's right. I do want to say about this. I'm not trying to,
[01:29:17.520 --> 01:29:22.240]   I would be really clear because I'm running for Congress that I'm not trying to defend China here.
[01:29:22.240 --> 01:29:30.560]   But I would say I have a father-in-law who's Chinese and immigrated here, I think it was 1963.
[01:29:31.200 --> 01:29:38.640]   And I have a lot of relatives through marriage, they're native from China. And I want to say
[01:29:38.640 --> 01:29:45.760]   those relationships give me a different perspective on everything that we're talking about here.
[01:29:45.760 --> 01:29:54.000]   I think you're right on Amy when you say the structure of China is not really well attuned to
[01:29:55.280 --> 01:30:02.160]   American-style democracy. But I also want to say it, when I've told Chinese family members the
[01:30:02.160 --> 01:30:09.040]   story about gamergate or they see me in the news, it seems insane to them. It seems completely
[01:30:09.040 --> 01:30:16.800]   insane to me and I live in America. But the idea of everyone being able to harass someone online
[01:30:16.800 --> 01:30:21.920]   and you can't track people down and they're threatening people with death threats and that
[01:30:21.920 --> 01:30:27.840]   there's no way to check that. That seems insane to someone from China. I'm not saying-
[01:30:27.840 --> 01:30:31.200]   As long as you're from the Eastern part and you're not a wager.
[01:30:31.200 --> 01:30:34.400]   I mean, there's some- Yeah, if you're a Muslim in China, it's a different matter.
[01:30:34.400 --> 01:30:39.920]   I'm trying to be very clear here. So I'm not defending that. What I'm saying is-
[01:30:39.920 --> 01:30:41.200]   It's complicated. I understand.
[01:30:41.200 --> 01:30:47.200]   There are paradigms in China that took me a while to understand being a Western white woman
[01:30:47.200 --> 01:30:53.600]   that married into that family. Some of them are very, very sexist. I will never forget when
[01:30:53.600 --> 01:30:59.520]   I was at one of my first Christmas dinners with my family of in-laws and all the women were
[01:30:59.520 --> 01:31:04.560]   dismissed to the kitchen to go wash dishes while the men sat around and talk politics.
[01:31:04.560 --> 01:31:13.760]   I didn't like that very much. But there were also paradigms there. Like the head of the house,
[01:31:13.760 --> 01:31:20.880]   the father ultimately makes the decision. But every single child has a right that is really
[01:31:20.880 --> 01:31:28.000]   recognized in the culture to come forward and plead their case. So I just want to say like
[01:31:28.000 --> 01:31:34.560]   what my experience has been spending 10 years in a marriage just with the family from China
[01:31:34.560 --> 01:31:42.640]   has been some of my Western assumptions about them socially. I can see upon reflection how they were
[01:31:42.640 --> 01:31:51.520]   a little bit maybe condescending might be the word. Vice did a really interesting story about social,
[01:31:51.520 --> 01:31:57.520]   the Chinese social media score system. It's exactly as you said, people are prevented from
[01:31:57.520 --> 01:32:04.400]   buying tickets to go on high speed trains and they definitely pay a penalty. But the other side of
[01:32:04.400 --> 01:32:12.240]   that I had not seen in Western media was like elderly people paid to go around their town
[01:32:12.240 --> 01:32:19.600]   and find people doing good deeds and reward them with social credit for doing that. So
[01:32:19.600 --> 01:32:25.040]   I want to be clear, I don't think this is something that we should implement in America.
[01:32:25.040 --> 01:32:32.640]   I think it's contrary to our views here. I'm just saying that particular society values the whole
[01:32:32.640 --> 01:32:38.320]   and the health of society as a whole more than ours does. We really praise individuality.
[01:32:38.320 --> 01:32:38.640]   Sure.
[01:32:38.640 --> 01:32:45.120]   And the last I want to say on this is David Brooks, I saw a video from him this week where he was
[01:32:45.120 --> 01:32:51.360]   talking about what ails America. And this guy, he's conservative, he's nuts when I agree with,
[01:32:51.360 --> 01:32:58.800]   but he was talking about how in America we don't know our neighbors. We're suspicious of our neighbors.
[01:32:58.800 --> 01:33:05.760]   We have half as many events at our house today as families did 30 years ago and we're more and
[01:33:05.760 --> 01:33:14.480]   more distrustful people. So I'm just saying they are making a societal choice to use technology in a
[01:33:14.480 --> 01:33:23.840]   way that values the whole harmony more than ours does. And I think ours is better, but I don't want
[01:33:23.840 --> 01:33:28.480]   to just dismiss that as evil because I think that's not really the whole vision.
[01:33:28.480 --> 01:33:34.160]   Sure. So there's been quite a bit of fear mongering when it comes to China and a lot of
[01:33:34.160 --> 01:33:42.560]   vilification. So I lived in China and I lived in Japan. And part of the social credit score system,
[01:33:42.560 --> 01:33:48.960]   if you talk to a lot of Chinese people who are not Uyghurs or ethnic minorities of other kinds,
[01:33:51.520 --> 01:33:55.920]   they don't, a lot of people don't see this as that big of a deal. Partially, this is because in Chinese
[01:33:55.920 --> 01:34:02.720]   culture, taddling on somebody else as a way to create social harmony is a long standing tradition
[01:34:02.720 --> 01:34:07.520]   that goes back to feudal times. So this is a way of automating that. So for us, this seems very
[01:34:07.520 --> 01:34:13.680]   strange, the social credit score, but for others, until they run until they find that they are being
[01:34:13.680 --> 01:34:20.480]   restricted, it's not, it's not, doesn't seem like something that is all that horrible now.
[01:34:21.360 --> 01:34:28.080]   The point is when this gets exported into other countries where those traditions are not native,
[01:34:28.080 --> 01:34:38.080]   and the purpose of this is to create control. And let's again think this through over many years,
[01:34:38.080 --> 01:34:44.160]   if you have a social credit score and somebody else is determining whether or not you can move
[01:34:44.160 --> 01:34:48.800]   freely, whether or not your kids can go to some good schools or bad schools, whether or not you
[01:34:48.800 --> 01:34:55.440]   can apply to get a car or a home loan or any other number of things, it's likely that your travel
[01:34:55.440 --> 01:35:01.840]   to other countries will wind up being restricted to. From my vantage point, given the data that we
[01:35:01.840 --> 01:35:07.360]   have access to right now, it's entirely plausible that the countries that are aligned and using the
[01:35:07.360 --> 01:35:14.560]   social credit score with individuals having social credit scores and perhaps companies having their
[01:35:14.560 --> 01:35:21.760]   own version of that kind of a score becomes the new de facto way of making decisions.
[01:35:21.760 --> 01:35:30.000]   And those decisions are being made autonomously by AI systems, which means that Americans could
[01:35:30.000 --> 01:35:36.400]   wind up not able to travel into countries because we don't have a social credit score, or maybe our
[01:35:36.400 --> 01:35:40.560]   companies can't do business in certain places because they don't have a social credit score.
[01:35:41.120 --> 01:35:46.720]   So I think we need to take a lot of this much more seriously than I think we have been,
[01:35:46.720 --> 01:35:53.040]   not in a sort of way that not like looking at this in a fascinating way or fear-mongering,
[01:35:53.040 --> 01:35:59.280]   but taking a very pragmatic, practical look at how like what the implications, downstream implications
[01:35:59.280 --> 01:36:05.280]   of all of this is into the future. We're not thinking about it that way, and we're certainly
[01:36:05.280 --> 01:36:11.040]   not looking at how autonomous decision-making systems built by a relative few number of people
[01:36:11.040 --> 01:36:17.440]   intended to optimize everybody's life intersects with other areas of life like governing and
[01:36:17.440 --> 01:36:25.680]   travel and health and on and on and on. I agree with every word you just said strongly, and I think
[01:36:25.680 --> 01:36:32.240]   this really backs up one of my main critiques of the Trump era of American government, where we
[01:36:32.240 --> 01:36:37.360]   are withdrawing from the world. If we're not out there bringing our paradigms forward,
[01:36:37.360 --> 01:36:42.800]   if we're not talking to other countries about how this needs to affect travel,
[01:36:42.800 --> 01:36:48.320]   how this needs to affect the things we adopt with loans, if we're not engaging other countries,
[01:36:48.320 --> 01:36:54.080]   someone else will. So I very strongly agree with everything you just said, and I think it's really
[01:36:54.080 --> 01:36:58.560]   on us to go out and to argue the other side of this.
[01:36:58.560 --> 01:37:02.240]   It's difficult, of course, cultural differences are very difficult to navigate.
[01:37:03.200 --> 01:37:07.520]   You mentioned cognitive bias a little while ago, Amy, cognitive bias are us.
[01:37:07.520 --> 01:37:14.800]   For instance, I've always thought that the collectivist style of life in China, we had a
[01:37:14.800 --> 01:37:19.840]   lot to be admired, and that here in the United States where we celebrate individualism, often
[01:37:19.840 --> 01:37:25.920]   that's pathological in this country, and it can be pathological collectivism as well.
[01:37:25.920 --> 01:37:31.040]   I think it is important to say maybe we need to understand other cultures,
[01:37:32.160 --> 01:37:35.200]   and maybe understand them a little bit better before we criticize them.
[01:37:35.200 --> 01:37:38.960]   I'm going to take a break and we come back. We're going to highlight this
[01:37:38.960 --> 01:37:44.320]   because there's a big fjure. Talk about outrage, moms. There's a big fjure
[01:37:44.320 --> 01:37:52.720]   because Google has refused to pull an app from Saudi Arabia that tracks women,
[01:37:52.720 --> 01:37:58.800]   which sounds like a bad thing, but it's complicated as everything is. And we'll talk about that when
[01:37:58.800 --> 01:38:05.520]   we come back. It's great to have two brilliant minds here, and I could just sit back and be the
[01:38:05.520 --> 01:38:10.640]   jello boy that I am. Brianna Wu, running for Congress in the Massachusetts 8th,
[01:38:10.640 --> 01:38:14.880]   brand a woo for Congress. Are we right on that Twitter or do you have a new Twitter?
[01:38:14.880 --> 01:38:18.160]   Oh gosh, yes, that is correct. Brianna Wu.
[01:38:18.160 --> 01:38:25.200]   Okay. It was Space Cat Gal. We're eliminating that. No Congress members should be called Space Cat Gal.
[01:38:25.200 --> 01:38:30.720]   My team talked to me. They had a fit for like, look, you cannot. And I came up with that when I
[01:38:30.720 --> 01:38:35.440]   was my god, I was working at GameStop when I came up with that handle.
[01:38:35.440 --> 01:38:40.240]   And this is the problem with handles. And the chat rooms full of people with handles that
[01:38:40.240 --> 01:38:46.320]   they thought up when they were 12, and they thought were pretty cool. And you know, you can grow out
[01:38:46.320 --> 01:38:56.560]   of them. I understand. Brianna Wu. Much more grown up. Also, Amy Webb, her book, The Big Nine,
[01:38:56.560 --> 01:39:03.280]   talks about these issues in a very trenchant and accessible and yet really provocative way. I
[01:39:03.280 --> 01:39:08.000]   think it's well worth reading how the tech titans and their thinking machines could warp
[01:39:08.000 --> 01:39:11.520]   humanity. Well, I want to talk more about AI too, because that's really the meat of this
[01:39:12.400 --> 01:39:17.120]   subject. And it is really where we're kind of going to be coming up against some that and
[01:39:17.120 --> 01:39:23.280]   crisper, I think, in the next few years, are going to change everything. And I wonder how well we can
[01:39:23.280 --> 01:39:28.480]   predict what that's going to be like. But I'd like to hear Amy's suggestions for that. Meanwhile,
[01:39:28.480 --> 01:39:34.480]   let's take a break. I think we all need to just kind of simmer and think about what we've just
[01:39:34.480 --> 01:39:39.840]   heard. So I'm going to play you something completely mindless. A summary of the week we just had
[01:39:40.480 --> 01:39:47.680]   on to it. To it. Well, today, we are going to talk about frustration.
[01:39:47.680 --> 01:39:56.400]   We have some games that will leave you in a puddle on the floor. Crying.
[01:39:56.400 --> 01:40:03.600]   Triangulation. Amy Webb talks about her new book, The Big Nine, how the tech titans and their thinking
[01:40:03.600 --> 01:40:10.000]   machines could warp humanity. A lot of decisions are being made quickly or under duress. When it
[01:40:10.000 --> 01:40:15.840]   comes to something like AI, which is not a single technology, but many technologies predicated on
[01:40:15.840 --> 01:40:23.280]   making decisions on our behalf, I think it's a good idea for us to stop for a few minutes and ask
[01:40:23.280 --> 01:40:28.800]   about, you know, what does it all mean? This week in computer hardware, I have not felt
[01:40:28.800 --> 01:40:33.760]   like I meant something when I was out in public the way that I did when I had the first generation
[01:40:33.760 --> 01:40:39.760]   iPhone. And now I'm just a regular person and nobody cares. But what if I got the folding phone?
[01:40:40.320 --> 01:40:45.200]   I could be a big deal again. It's a status. You could also start wearing stripper glitter all
[01:40:45.200 --> 01:40:50.720]   over your face or dye your hair purple or wearing nothing but, you know, assless chaps and probably
[01:40:50.720 --> 01:40:56.240]   get just as much expensive. But none of those things cost $2,000, Patrick. I think you're missing
[01:40:56.240 --> 01:40:59.600]   the point. Depends on the glitter depends on the chaps, buddy.
[01:40:59.600 --> 01:41:08.960]   Pat's Jack. Mary Jo, did you ever pick a laptop? I have not yet. I'm looking, looking around.
[01:41:08.960 --> 01:41:14.800]   I mean, I'd be narrowed it down to which Mac model you want or how. I think she's going Linux.
[01:41:14.800 --> 01:41:22.720]   You know, a good pair of assless chaps can cost upwards of $3,000. I just want to point that
[01:41:22.720 --> 01:41:31.280]   out extra with glitter. Yeah. Are you Wow? Wow. Are you sure? What the glitter is? What made you go
[01:41:31.280 --> 01:41:38.720]   Wow? I was just I was wondering what Leo was like in college. That's that's all I was
[01:41:38.720 --> 01:41:46.400]   like, that handle was in fact, assless chaps. I'm just saying. Our show. Hey, Brian, boy,
[01:41:46.400 --> 01:41:52.880]   are they sorry brought to you by stamps.com. Thank you, stamps for being very understanding
[01:41:52.880 --> 01:41:59.280]   and patient. But honestly, honestly, if you are doing mailing of any kind, maybe you have a
[01:41:59.280 --> 01:42:04.240]   glitter factory and you send litter out to people, if you're sending out stuff through the US mail
[01:42:04.240 --> 01:42:08.560]   and you're hand addressing it or typing, what are you going to select? All right. And looking
[01:42:08.560 --> 01:42:14.000]   and putting stamps on your mail, that's not professional. You need stamps.com. The best way to
[01:42:14.000 --> 01:42:20.080]   buy and print really US postage, the instant you need it right from your desk. You don't even have
[01:42:20.080 --> 01:42:23.440]   to go to the post office. Like everything you do at the post office, you could do at your desk.
[01:42:25.280 --> 01:42:30.000]   Stamps.com means your ink. You don't need special ink. You don't need a
[01:42:30.000 --> 01:42:33.920]   postage meter. You don't need anything. You just need your computer, your printer, your ink.
[01:42:33.920 --> 01:42:37.920]   You could print postage right on an envelope, for instance, including your logo. The return
[01:42:37.920 --> 01:42:42.480]   address is automatically filled in. They can even pull the sending the recipient's address
[01:42:42.480 --> 01:42:48.240]   from the web. If you're an Etsy seller, Amazon, eBay, it just pulls it right out of there. And
[01:42:48.240 --> 01:42:52.320]   then you don't even have to do anything. It'll fill out the customs forms for you. If you're
[01:42:52.320 --> 01:42:56.080]   sending internationally, it'll fill out the certified mail forms. You even get discounts
[01:42:56.080 --> 01:43:00.960]   you can't get at the post office. All of the amazing services of the US Postal Service,
[01:43:00.960 --> 01:43:08.960]   Ben Franklin's brilliant invention brought right to your desk. So you want it? I got a great deal
[01:43:08.960 --> 01:43:15.120]   for you and I got a great deal for you. You can enjoy the stamps.com service with our special offer.
[01:43:15.120 --> 01:43:20.720]   You're going to go right there to the top there. See that radio microphone? On the top there. Click
[01:43:20.720 --> 01:43:26.080]   that and then enter the offer code TWIT. And you're going to get an amazing four-week trial.
[01:43:26.080 --> 01:43:31.520]   Includes a big chunk of free postage, a digital scale that you can use so you always have
[01:43:31.520 --> 01:43:36.080]   exactly the right postage. You've got to do this because it's crazy to do anything else.
[01:43:36.080 --> 01:43:40.320]   Whether you're a small office sending invoices and an online selling shipping out products,
[01:43:40.320 --> 01:43:45.120]   even if you're a warehouse sending out thousands of packages a day, we send everything with stamps.com.
[01:43:45.120 --> 01:43:49.200]   In fact, if I need stamps, I go to Debbie and I said, print me some stamps. She gave me some
[01:43:49.200 --> 01:43:55.520]   stamps. I should get those that have my picture on them. My picture on them was awesome from stamps.com.
[01:43:55.520 --> 01:44:00.000]   I'm no Abraham Lincoln, but it's kind of cool to have my own picture on my stamps.
[01:44:00.000 --> 01:44:04.640]   Simply use your computer to print official US postage 24/7 whenever you need it for any letter,
[01:44:04.640 --> 01:44:08.960]   any package, any class of mail, anywhere you want to send it. And by the way, they'll even recommend
[01:44:08.960 --> 01:44:12.960]   you know, ways to save money. I say, oh, you should send this media mail.
[01:44:12.960 --> 01:44:18.640]   Once your mail is ready, you hand it to the mail carrier or you drop in the mailbox.
[01:44:18.640 --> 01:44:22.640]   And you know, there are restrictions to the size and weighted packages you can mail in a mailbox,
[01:44:22.640 --> 01:44:27.040]   but not with stamps.com. No, because you're using stamps.com. They're in DCA system,
[01:44:27.040 --> 01:44:31.200]   works so well. The post office says, no, that's fine. If you stamps.com, just put it in the mailbox.
[01:44:31.200 --> 01:44:35.200]   That's fine. I love it. With stamps.com, you get, I've said this before, you get discounts,
[01:44:35.200 --> 01:44:39.200]   you can't get to the post office. You get five cents off every first class stamp.
[01:44:39.200 --> 01:44:44.320]   You get 40% off priority mail. This thing pays for itself and then some. You got to do it.
[01:44:44.320 --> 01:44:48.400]   Go to stamps.com, click the microphone at the top of the homepage and enter to it. If you're not,
[01:44:48.400 --> 01:44:52.800]   using stamps.com, you mustn't be using the mail. That's all I can think of. Stamps.com
[01:44:52.800 --> 01:45:01.280]   and enter the offer code, Twitch. Let's just forget all that other stuff happened.
[01:45:01.280 --> 01:45:05.600]   I was just thinking, no, I know you were thinking. You see this layout said,
[01:45:05.600 --> 01:45:09.920]   this is a picture I just had made of my husband. That's your husband. I love that.
[01:45:09.920 --> 01:45:16.240]   I think that was stamp and sending it out to people. I know it would be awesome.
[01:45:16.240 --> 01:45:19.600]   So it's a separate process. They have a thing where you can get stamps printed up
[01:45:19.600 --> 01:45:24.480]   with pictures or whatever you want. They're at a self-adheasing stamps and you get a sheet or two
[01:45:24.480 --> 01:45:28.400]   or whatever you want of them. Then you run it through the printer and the stamps.com puts this
[01:45:28.400 --> 01:45:34.320]   postage on it. It's so cool. Oh, it'll print directly on the envelope.
[01:45:34.320 --> 01:45:38.560]   Or on the envelope. You guys, you should do more commercials with me. You're great.
[01:45:38.560 --> 01:45:43.920]   I mean, Leo, it'll print right on the envelope. Yes, Amy. It will print right on the envelope.
[01:45:43.920 --> 01:45:50.320]   No, listen, I'll tell you something. I bought stamps off of stamps.com.
[01:45:50.320 --> 01:45:55.280]   And the reason was because I couldn't easily buy stamps from the USPS website.
[01:45:55.280 --> 01:46:00.000]   Yes. It was impossible. Right. You know why? The Postal Service loves stamps.com.
[01:46:00.000 --> 01:46:04.400]   Of course, you can't do this without the cooperation of the Postal Service. They love stamps.com.
[01:46:04.400 --> 01:46:07.760]   They say, please, everybody's use stamps. If you want to come up and pull off this website is
[01:46:07.760 --> 01:46:12.400]   like ridiculous. We're getting that cuts on that. The little post office never has stamps.
[01:46:12.400 --> 01:46:16.160]   Literally, they're always out. Or there's somebody who doesn't have
[01:46:16.160 --> 01:46:20.400]   a last time I was in post office. I love the post office. I have a soft spot
[01:46:20.400 --> 01:46:24.800]   for the use Postal Service. And I realized why because as a kid, all the good stuff that
[01:46:24.800 --> 01:46:28.800]   ever happened to me came through the mail. You know, my Captain Crutch Dakota ring,
[01:46:28.800 --> 01:46:33.760]   my Mr. Potato Head doll that I ordered from the... It all came through. See, I wasn't a grown-up,
[01:46:33.760 --> 01:46:38.080]   so I didn't get bills. I didn't get any of the negative stuff. I only got the good stuff.
[01:46:38.080 --> 01:46:41.760]   If mail came from me, when I was a kid, it was good. So I loved the postman.
[01:46:41.760 --> 01:46:47.760]   Yeah, that's not our situation. I get everybody else's mail. Everybody else gets our mail.
[01:46:47.760 --> 01:46:51.840]   And I know that because I subscribe to the USPS, they'll scan all of these.
[01:46:51.840 --> 01:46:53.520]   Oh, yeah, I love that. I do that.
[01:46:53.520 --> 01:46:55.360]   Yeah, we'll scan and tell you what's coming today.
[01:46:55.360 --> 01:46:59.600]   I do that. And like most days of the week, half of that stuff is missing and we get other people's
[01:46:59.600 --> 01:47:03.520]   stuff. Oh, you got to get a better mail carrier. I know my mail carrier, where personal friends
[01:47:03.520 --> 01:47:06.560]   she'll come at, she'll say stuff. She'll say, oh, you're getting that tea again? I said, yeah,
[01:47:06.560 --> 01:47:08.640]   you want some? Oh, yeah, it takes a... It's great.
[01:47:08.640 --> 01:47:09.920]   Will she deliver to these coasts?
[01:47:09.920 --> 01:47:14.160]   I wish. No, she's great. Yeah, I get that postal thing where they send the...
[01:47:14.160 --> 01:47:17.200]   It's coming. It's coming. I love that.
[01:47:17.200 --> 01:47:23.120]   So this is controversial. But I think if you dig deeper and I want to thank Hacker News,
[01:47:23.120 --> 01:47:28.560]   which is a great service, a new service provided by Y Combinator, because I, like everybody else,
[01:47:28.560 --> 01:47:35.120]   read the Business Insider column or article. Even if you just read the headline, it's in Cindy.
[01:47:36.000 --> 01:47:42.880]   Bring out the outrage, moms. Google, siding with Saudi Arabia, refuses to remove widely
[01:47:42.880 --> 01:47:50.080]   criticized government app, which get this. Let's men track women and control their travel.
[01:47:50.080 --> 01:47:57.280]   Google has rejected calls to remove a Saudi government app. They called Jackie Spear called
[01:47:57.280 --> 01:48:04.160]   them. Your future colleague, Brianna, from the House of Representatives. And she said, they said,
[01:48:04.160 --> 01:48:08.560]   "The app doesn't violate our terms of service." Now, you just read this news, you're going to say,
[01:48:08.560 --> 01:48:14.160]   "Oh, but see, I think there's another example of maybe some cultural misunderstanding and
[01:48:14.160 --> 01:48:19.520]   an un-misunderstanding of what this app really does." So it is a governmental app
[01:48:19.520 --> 01:48:27.760]   that provides official services among them a digitized version of the yellow sheet.
[01:48:27.760 --> 01:48:32.240]   So this is the law in Saudi Arabia. I mean, this is really the problem is this,
[01:48:32.240 --> 01:48:37.200]   the law in Saudi Arabia for a woman to leave the country, her legal guardian, her husband,
[01:48:37.200 --> 01:48:45.200]   if not her family, has to fill out a form, not her. She can't do it. So she has to say,
[01:48:45.200 --> 01:48:51.760]   "Well, husband, I'd like to leave the country. Could you sign this?" And by the way, to prevent
[01:48:51.760 --> 01:48:55.520]   forgery, the legal guardian, her husband has to show up at the office with the form.
[01:48:57.280 --> 01:49:02.480]   So the point being made by a lot of women in Saudi Arabia and elsewhere is,
[01:49:02.480 --> 01:49:08.320]   "No, this app is great because first of all, you don't have to go to the office. It facilitates
[01:49:08.320 --> 01:49:13.040]   getting this permission, which we have to get anyway. The app doesn't change the rule."
[01:49:13.040 --> 01:49:23.040]   I'll quote a tweet posted by Mona El-Tahawi, who had a female relative and Saudi feminist
[01:49:23.920 --> 01:49:27.120]   send this. Mona, I hope you're having a great day. Just wanted to bring your attention aside.
[01:49:27.120 --> 01:49:30.880]   Most feminist activists are forgetting right now. Absure, that's the name of the app,
[01:49:30.880 --> 01:49:35.840]   is a symptom of a problem, not the problem itself. In fact, if the public pressure currently
[01:49:35.840 --> 01:49:39.840]   circulating on social media against Google and Apple continues, this could be troubling
[01:49:39.840 --> 01:49:46.320]   to Saudi women. The app is an abomination, of course, but it has helped women rather than the
[01:49:46.320 --> 01:49:51.920]   opposite. Those who want to flee can do so with app access, but never could be for with actual
[01:49:51.920 --> 01:49:57.120]   paperwork in the previous bureaucratic system. Guardians would use it as an excuse because it's
[01:49:57.120 --> 01:50:03.280]   so hard, and they would use it as an excuse. My father used to have to physically file paperwork
[01:50:03.280 --> 01:50:12.320]   in order for us to travel. So this is actually in a very perverse way, an enabling technology
[01:50:12.320 --> 01:50:18.240]   for Saudi women. This, to me, is similar to this problem of understanding what the Chinese are doing.
[01:50:19.520 --> 01:50:22.800]   I'm going to give Google a pass on this one. What do you guys think?
[01:50:22.800 --> 01:50:28.640]   I think based on the information you've just given, it seems like that to me as well.
[01:50:28.640 --> 01:50:34.080]   There are a lot of people also on Hacker News who say, "It's facilitating an immoral action."
[01:50:34.080 --> 01:50:39.120]   Which it is. It's facilitating a Saudi government.
[01:50:39.120 --> 01:50:48.000]   I think it speaks to the need for women to get involved with the political process,
[01:50:48.000 --> 01:50:54.160]   and I would encourage people in that country to change that law because it sounds deeply sexist
[01:50:54.160 --> 01:51:03.600]   to me. Somebody in the Hacker News comments said, "What if Saudi Arabia only freed their
[01:51:03.600 --> 01:51:08.720]   African slaves in 1962? What if a new king re-enslaved them? I guess Google would be fine with hosting
[01:51:08.720 --> 01:51:17.360]   slave auction apps." Whether it's Google or Facebook, which is in trouble yet again,
[01:51:17.920 --> 01:51:27.680]   in the EU as of yesterday, or you name it, the platforms have to either make a decision that
[01:51:27.680 --> 01:51:33.280]   they are nothing more than an intermediary between people and businesses, and that's it.
[01:51:33.280 --> 01:51:44.080]   Which would mean that their app stores and all of the content is not aggregated or...
[01:51:44.080 --> 01:51:51.280]   It's sort of... There's no weight to how any of it's aggregated. There's no recommended lists.
[01:51:51.280 --> 01:51:56.640]   There's nothing like that. Or they have to say, "You know what? We're a platform and we are making
[01:51:56.640 --> 01:52:00.320]   decisions." And then when they make those decisions, I think they have to be extremely
[01:52:00.320 --> 01:52:05.120]   transparent and consistent in how those decisions are being made. And the problem is that at the
[01:52:05.120 --> 01:52:10.640]   moment, they are not doing that. And so... This is the hard one.
[01:52:10.640 --> 01:52:14.880]   Because if they didn't make Absure that wouldn't change the law,
[01:52:14.880 --> 01:52:17.520]   wouldn't change the requirements, they'd just make it more onerous, in fact.
[01:52:17.520 --> 01:52:28.400]   Yeah. So my point is, get out ahead of it. The big tech companies are eroding our trust
[01:52:28.400 --> 01:52:36.400]   every day. And it's going to be hard to regain it. However, we can't divorce ourselves from the
[01:52:36.400 --> 01:52:40.560]   technology companies either. So this is creating an acrimonious relationship that I can guarantee
[01:52:40.560 --> 01:52:43.840]   you ends in regulation. Yeah, we are legal. Right? I mean...
[01:52:43.840 --> 01:52:49.920]   That's right. And regulation is not good at all because there's not going to be a singular
[01:52:49.920 --> 01:52:54.560]   regulation all around the world. There's going to be lots of different types of regulations.
[01:52:54.560 --> 01:52:58.560]   And ultimately, regulation is going to stifle innovation. So guardrails are good,
[01:52:58.560 --> 01:53:04.480]   regulation is bad. But the only way to avoid that situation is to be very upfront before people
[01:53:04.480 --> 01:53:10.400]   get upset, before it's discovered that there's a microphone in your thermostat that nobody knew
[01:53:10.400 --> 01:53:15.600]   was there. Right? Before you put an app in the app store that you know is probably going to be
[01:53:15.600 --> 01:53:21.200]   controversial, be transparent in how and why those decisions are being made. But that sort of thing
[01:53:21.200 --> 01:53:25.600]   takes courageous leadership, which we need to start seeing demonstrated now.
[01:53:25.600 --> 01:53:29.200]   All right. I got a minefield for Brianna Wu. And if you want to...
[01:53:29.200 --> 01:53:30.720]   Let's do it.
[01:53:30.720 --> 01:53:37.120]   You want to recuse yourself on this one, do. But I think a brave and savvy political
[01:53:37.120 --> 01:53:40.400]   leader like you will take this on. Let's do it.
[01:53:40.400 --> 01:53:46.480]   And this to me is this challenging. So you know the Jedi contract, which is an artificial
[01:53:46.480 --> 01:53:53.520]   intelligence face recognition contract that Google was a... It's a $10 billion contract.
[01:53:53.520 --> 01:53:59.920]   Google was going to try to get in the running for. And thanks to a petition of thousands of
[01:53:59.920 --> 01:54:04.800]   Google employees decided, "Yeah, maybe we better stay away from that." Meanwhile, Jeff Bezos said,
[01:54:04.800 --> 01:54:12.000]   "Well, what the hell's wrong with United States technology companies helping the legally
[01:54:12.000 --> 01:54:20.080]   authorized military forces do their job to defend us? We've got a military. What's wrong
[01:54:20.080 --> 01:54:25.520]   with us creating products for the military?" And took the contract or said, "I don't remember if
[01:54:25.520 --> 01:54:29.440]   they got it or they said they'd take it. I think they got it." $10 billion contract.
[01:54:29.440 --> 01:54:36.880]   Microsoft, same issue. Microsoft has faced a petition by a smaller number of employees, 50
[01:54:36.880 --> 01:54:44.080]   employees, saying, "We don't want you to take this $479 million contract to supply hollow lens to
[01:54:44.080 --> 01:54:54.080]   the military." Satya Nadella kind of tipped the Jeff Bezos thing saying, "Microsoft would not
[01:54:54.080 --> 01:54:59.360]   withhold technology from democratic governments. We made a principal decision that we're
[01:54:59.360 --> 01:55:04.080]   not going to withhold technologies from institutions that we have elected in democracies to protect
[01:55:04.080 --> 01:55:10.480]   the freedoms we enjoy." I have to say, it's very interesting in this country that
[01:55:10.480 --> 01:55:17.760]   it used to be World War II. Everybody pitched in. Rosie the Riveter built fighter planes.
[01:55:17.760 --> 01:55:25.040]   It used to be patriotic. Now, in some quarters, quite the opposite. Is it because it's AI,
[01:55:25.040 --> 01:55:28.640]   because it's such double-edged technologies, because it's augmented reality?
[01:55:28.640 --> 01:55:34.320]   Well, I think if Amy, you were talking at people at these tech companies, speaking up,
[01:55:34.320 --> 01:55:39.360]   having more of a voice doing the right thing. I personally think that's a little bit of an
[01:55:39.360 --> 01:55:45.200]   optimistic vision in the future, but one component of that would certainly be people at these companies
[01:55:45.200 --> 01:55:51.760]   having conversations internally about what they're comfortable building. And I think those are
[01:55:51.760 --> 01:55:58.240]   great conversations to have. I have friends on the Microsoft Azure team that had conversations
[01:55:58.240 --> 01:56:06.640]   about their databases being used by ICE. So I think that's really good. For me personally,
[01:56:06.640 --> 01:56:13.120]   I am running for United States Congress. I believe in the United States. I want the United States
[01:56:13.120 --> 01:56:21.120]   to be safe and secure. I'm not an anti-military pacifist. My father served in the Navy. My grandfather
[01:56:21.120 --> 01:56:26.480]   served in World War II. So personally, I think when it comes to AI technology,
[01:56:26.480 --> 01:56:32.880]   we need to realize that China and other countries are employing this to
[01:56:32.880 --> 01:56:40.880]   basically use an information warfare and cyber warfare. So I think we need to reach out to
[01:56:40.880 --> 01:56:45.920]   companies like Google, see if there's work we can do together that everybody can feel good about.
[01:56:45.920 --> 01:56:52.640]   If there's not, I think there's a place for the DoD to fund that research. But the bottom line is
[01:56:52.640 --> 01:56:59.520]   other countries are going to research these things. So it's like if you've got the nation next door
[01:56:59.520 --> 01:57:05.600]   that's inventing bows and arrows, we've got to be looking into those things as well.
[01:57:05.600 --> 01:57:10.080]   Yeah, I understand why engineers would say, well, we want to say in what we develop in
[01:57:10.080 --> 01:57:14.800]   some pacifists might say, well, we don't want to develop military technology. But ultimately,
[01:57:14.800 --> 01:57:18.080]   the company that you're working for is going to make that decision. And Nadella says,
[01:57:18.080 --> 01:57:22.720]   we'll have conversations with all our employees. We want to hear them. But at this point, we think
[01:57:22.720 --> 01:57:27.360]   it's right now. Amy Webb, I guess I'm going to guess that you're no stranger to the halls of the Pentagon.
[01:57:27.360 --> 01:57:34.560]   No, I have spent time in the Pentagon. So you've made that decision personally.
[01:57:34.560 --> 01:57:38.000]   Sure. Listen, I
[01:57:40.800 --> 01:57:46.480]   the future of the future of our future wars are going to be fought in code, not combat. They're
[01:57:46.480 --> 01:57:53.840]   not going to look anything like we've seen before. And the reality is that our government
[01:57:53.840 --> 01:57:58.560]   outside of the military, but I would argue also inside of the military for many years, wasn't
[01:57:58.560 --> 01:58:08.480]   developing and funding our readiness. And as a result of that, there are other countries around
[01:58:08.480 --> 01:58:16.560]   the world that have become a pacing threat. So we need to consider that as well as the role that
[01:58:16.560 --> 01:58:24.560]   large technology companies have always played in shoring up our national defense and security.
[01:58:24.560 --> 01:58:31.760]   We've always had that relationship. I think what's different now, again, is that society has changed
[01:58:31.760 --> 01:58:41.600]   and employees of companies feel that their employers should be responsible and tell them and be very
[01:58:41.600 --> 01:58:46.800]   transparent about all of the projects that they're working on and why. It was not like that in the
[01:58:46.800 --> 01:58:53.840]   40s, 50s, and 60s. So that has changed. And there's a pretty big difference between
[01:58:54.800 --> 01:59:02.160]   systems that don't require booths on the ground to blow things up and autonomous systems that
[01:59:02.160 --> 01:59:09.200]   in vehicles and everything else that are doing that in communities where there are likely to
[01:59:09.200 --> 01:59:18.400]   be other kinds of casualties. But what I would say is that most of us are protected every day from
[01:59:18.400 --> 01:59:24.400]   all of the evils that happen outside of our homes and offices. We don't have to deal with
[01:59:25.200 --> 01:59:30.960]   really, even when things are really bad, we don't have to think about shoring up our nation's
[01:59:30.960 --> 01:59:35.600]   security. Somebody's got to do that. And not just in our country and every country. So
[01:59:35.600 --> 01:59:41.520]   this is challenging. This is really difficult stuff going forward. And
[01:59:41.520 --> 01:59:51.600]   it would be terrific if we could be in a position where there was more collaboration
[01:59:51.600 --> 01:59:58.480]   between Silicon Valley and Washington DC in a way that wasn't purely transactional because right now
[01:59:58.480 --> 02:00:05.360]   it is a transactional relationship and oftentimes antagonistic. So it would be great if the engineers
[02:00:05.360 --> 02:00:10.480]   got to go to the Pentagon and the people in the Pentagon spent significant time out in Silicon
[02:00:10.480 --> 02:00:17.440]   Valley. And there was more of a revolving door between the two so that we weren't just talking
[02:00:17.440 --> 02:00:22.640]   about Bezos and Amazon negotiating whatever, how many billion dollar deal to build,
[02:00:22.640 --> 02:00:26.640]   to use facial recognition technology in a way that makes us all feel bad.
[02:00:26.640 --> 02:00:34.160]   We're not in that situation culturally right now and it's going to take us a while to steer
[02:00:34.160 --> 02:00:38.240]   the ship in a slightly different direction. There are people that are working on this,
[02:00:38.240 --> 02:00:42.240]   but it would be great if there was more interplay between the Valley and DC.
[02:00:43.040 --> 02:00:48.240]   Amy, can I add something on to what you just said about future wars being fought in code? I think
[02:00:48.240 --> 02:00:56.160]   that is so dead on. And if you're someone like me that thinks of violence and war as a last resort,
[02:00:56.160 --> 02:01:03.040]   I would really encourage you to go read a fascinating book by Kim Zetter is called Countdown to Zero
[02:01:03.040 --> 02:01:09.360]   Day. It's about the development of Stuxnet Source. And this is one of the most fascinating cyber
[02:01:09.360 --> 02:01:15.600]   weapons has ever been developed. So we have two choices here. Iran is developing nuclear weapons
[02:01:15.600 --> 02:01:23.840]   and we can either go in and target them with missiles or send in special forces or have a war
[02:01:23.840 --> 02:01:31.200]   that's option A. What they did was option B. It's absolutely fascinating what this cyber weapon did.
[02:01:31.200 --> 02:01:37.360]   It was propagated to different versions of, I forget which operating system, I think it was Windows.
[02:01:37.360 --> 02:01:44.480]   It was this one version of this and it ended up being targeted towards their center futures.
[02:01:44.480 --> 02:01:47.280]   Yeah, the scatter controllers in the center futures.
[02:01:47.280 --> 02:01:53.200]   Yeah, exactly. And it got there and started looping the video while controlling the motors
[02:01:53.200 --> 02:01:59.520]   inside the center futures to make the malfunction and to slowly run until they just ended up like
[02:01:59.520 --> 02:02:07.040]   basically breaking down. And what I appreciate about this is it was a solution to a problem
[02:02:07.040 --> 02:02:14.080]   where no one was hurt, like no bullets were fired. And I think this has a lot of things we've got
[02:02:14.080 --> 02:02:20.240]   to think about defensively because right now America is on the defense. But like offensively,
[02:02:20.240 --> 02:02:26.560]   I think you would have to be a really extreme kind of pacifist to not think we need
[02:02:27.920 --> 02:02:34.720]   that to feel suspicious about employing say facial recognition air borders to see if like
[02:02:34.720 --> 02:02:41.280]   members of ISIS are trying to get into the United States. Like there are there are solutions here
[02:02:41.280 --> 02:02:47.440]   that save lives. And I think we should be clear eyed about the abuses that are possible. And I
[02:02:47.440 --> 02:02:52.960]   think that's why technologists need to be in those decisions. But I think there's a way to really
[02:02:54.000 --> 02:03:01.600]   get around some of the horrors of war by engaging in this. But even Stuxnet isn't all positive. By
[02:03:01.600 --> 02:03:06.400]   the way, does that ever answer the question? Who developed it? Was it the Israelis? Was it the US?
[02:03:06.400 --> 02:03:15.200]   I heard Israelis. But yeah, yeah. So it's not a complete positive because a Stuxnet escaped.
[02:03:15.200 --> 02:03:22.160]   Weaponized against all of us. And there's some argument that even though it might have initially
[02:03:22.160 --> 02:03:27.360]   slowed down the Iranians, all they did was secure their systems better and
[02:03:27.360 --> 02:03:34.240]   step up their nuclear development program. So maybe it works. Maybe it didn't work.
[02:03:34.240 --> 02:03:40.560]   You know what? Here's the deal in everything we've just talked about. It's complicated. It's
[02:03:40.560 --> 02:03:47.680]   really, really complicated. And I want to say, Brianna, I think it's a temptation for politicians
[02:03:48.480 --> 02:03:55.520]   to avoid these very difficult moral quandaries because there isn't a black and white right answer.
[02:03:55.520 --> 02:04:01.920]   And I think also, I want to pat ourselves on the back because I think generally in news,
[02:04:01.920 --> 02:04:09.040]   people go for the hot take instead of the nuanced difficult conversations because there's no obvious
[02:04:09.040 --> 02:04:14.720]   answer. It's difficult. And we have to, it's thorny and we have to really deal with it. And I want to
[02:04:14.720 --> 02:04:20.240]   admire you, Brianna, for being willing to say that, to talk about these complicated situations
[02:04:20.240 --> 02:04:25.360]   without the obvious hot take that is the best way to earn votes, unfortunately.
[02:04:25.360 --> 02:04:30.720]   And I want to commend our audience because you have the patience for this. This is
[02:04:30.720 --> 02:04:35.520]   be a lot easier. And certainly a lot of news organizations just go for that hot take.
[02:04:35.520 --> 02:04:41.680]   There's certainly a better way to get an audience. But I think we need to talk about this.
[02:04:43.040 --> 02:04:48.960]   And it's challenging. It really is. We need to have adult conversations and all of these things.
[02:04:48.960 --> 02:04:53.600]   And we need to be willing to do research. We need to be willing to have our minds changed.
[02:04:53.600 --> 02:04:57.840]   And it seems to be the last thing that political, at least political conversation in this country.
[02:04:57.840 --> 02:05:03.760]   There have been, I mean, I will say that within DOD, there have been some changes. There's been
[02:05:03.760 --> 02:05:08.240]   more funding. There's, there are some joint operations now on AI.
[02:05:10.720 --> 02:05:19.360]   You know, so again, I would urge us to consider that keeping the world, keeping the world
[02:05:19.360 --> 02:05:27.920]   peaceful requires a lot of military might that's and readiness. So I mean, that is the, and my
[02:05:27.920 --> 02:05:32.720]   academic background is in game theory, so, or partially in game theory. So I mean, this is not,
[02:05:32.720 --> 02:05:38.320]   and it's not us. It's not our generation. So you're saying that the only way to win is not to play,
[02:05:38.320 --> 02:05:48.560]   that's not true. I would say one way to win is to demonstrate readiness and then hopefully not
[02:05:48.560 --> 02:05:56.560]   ever have to engage. Yeah. Well, since, you know, I guess World War II, we, you know, since the first
[02:05:56.560 --> 02:06:02.800]   and only use of atomic weapons, we've had skirmishes, we've had small wars, but we haven't had a world
[02:06:02.800 --> 02:06:08.560]   war. And I think there's a number of reasons. My two favorite reasons are global trade,
[02:06:08.560 --> 02:06:12.800]   the more dependent we are on each other economically, the less likely we are to burn each other down.
[02:06:12.800 --> 02:06:21.040]   And the, and look what's happening. There's another issue, but that's, and the threat of a
[02:06:21.040 --> 02:06:27.840]   devastating war. And when, when war, the cost of war outweighs the benefits of war, people aren't
[02:06:27.840 --> 02:06:35.680]   going to wage it. And so peace through strength is, is absolutely part of that. And I'm, honestly,
[02:06:35.680 --> 02:06:41.840]   if I had to choose, I'd be a pacifist, but I realized that that's probably not a viable strategy
[02:06:41.840 --> 02:06:47.840]   for nation states. Could I make a quick recommendation to everybody? So if you're into what we're
[02:06:47.840 --> 02:06:52.000]   talking about, we got a lot of homework from this show, by the way, I already got a six book list
[02:06:52.000 --> 02:06:58.160]   here. So I'll go the less reading route. Dr. Strangelove still holds up.
[02:06:58.160 --> 02:06:58.720]   Awesome.
[02:06:58.720 --> 02:06:59.680]   And I'll give you all time.
[02:06:59.680 --> 02:07:00.160]   Awesome.
[02:07:00.160 --> 02:07:03.360]   I watch it every couple months. I always find something new.
[02:07:03.360 --> 02:07:08.240]   And you should rewatch that. And I'll tell you something, war games, all these years later,
[02:07:08.240 --> 02:07:11.120]   is both nostalgic and totally spot on.
[02:07:11.120 --> 02:07:14.720]   And so my callback was not inappropriate.
[02:07:14.720 --> 02:07:15.440]   Yeah, totally.
[02:07:15.440 --> 02:07:18.160]   So that's what I mean to people. Jonathan.
[02:07:18.160 --> 02:07:20.320]   You know what? People got it, which is kind of cool.
[02:07:20.400 --> 02:07:21.440]   Yeah.
[02:07:21.440 --> 02:07:25.600]   I thought maybe that's too obscure, but no, no, we're talking to geeks here.
[02:07:25.600 --> 02:07:30.240]   All right. Coming up, our hot take outrage session.
[02:07:30.240 --> 02:07:35.840]   All it takes is one word for me, Facebook, and they're in and again, ladies and gentlemen,
[02:07:35.840 --> 02:07:40.080]   they are in and again. But first a word from our sponsor.
[02:07:40.080 --> 02:07:44.560]   And I love this sponsor, Sophos Cybersecurity. We use Sophos.
[02:07:44.560 --> 02:07:51.200]   That is our protection at Twit from the outside world. We use Sophos hardware and Sophos software
[02:07:51.200 --> 02:07:56.000]   to protect ourselves from all the threats that can bring any business to its knees.
[02:07:56.000 --> 02:08:01.360]   Sophos is really sophisticated. They are now using artificial intelligence. They're using
[02:08:01.360 --> 02:08:06.080]   deep learning. Here's the problem. Hackers can come up with threats faster than the
[02:08:06.080 --> 02:08:11.280]   security companies can come up with defenses. Well, at least it used to be that way.
[02:08:11.280 --> 02:08:17.200]   Now, thanks to deep learning, which uses neural networks to interpret data and respond to threats,
[02:08:17.200 --> 02:08:22.800]   it doesn't even know about you're now safer. It works instantly.
[02:08:22.800 --> 02:08:29.040]   And this is exactly the kind of protection you need. No wonder Sophos recently ranked number one.
[02:08:29.040 --> 02:08:34.240]   S.E. Labs did an independent security test. The best protection ratings across the barbed for
[02:08:34.240 --> 02:08:38.720]   both large enterprises and small businesses. And now they've, I think they've done something
[02:08:38.720 --> 02:08:42.640]   kind of interesting. They've made, they've taken this advanced technology that businesses use
[02:08:42.640 --> 02:08:47.200]   and made a premium version for the home, for your Mac and for your PC. It's called Sophos Home.
[02:08:47.200 --> 02:08:52.720]   Real-time protection from the latest ransomware attacks, malicious software, hacking attempts,
[02:08:52.720 --> 02:08:57.520]   it can detect hacking attempts because it knows what they look like, right?
[02:08:57.520 --> 02:09:01.680]   It's incredibly easy to use whether you're simply securing your own laptop or managing the security
[02:09:01.680 --> 02:09:08.000]   of multiple devices in your own home or around the world because it is a cloud-based system
[02:09:08.560 --> 02:09:13.440]   with an online console. That means you can sign up for one account and protect all the
[02:09:13.440 --> 02:09:19.280]   Macs and PCs in your home and even in your relatives' homes. You can keep friends and family
[02:09:19.280 --> 02:09:23.920]   secure even if they're thousands of miles away. You can remotely manage security, clean up threats,
[02:09:23.920 --> 02:09:29.120]   keep systems safe. I think a lot of us are the IT department for our family members.
[02:09:29.120 --> 02:09:34.400]   And this is a great tool. You may know Sophos' tagline is security made simple. The whole thing
[02:09:34.400 --> 02:09:39.040]   is very easy. You log in from your browser and you secure your systems today and everything that's
[02:09:39.040 --> 02:09:45.120]   going on. I really like this method of keeping track of what's going on. So, bottom line, home
[02:09:45.120 --> 02:09:49.200]   user or enterprise, Sophos has you covered some of the largest businesses in the world
[02:09:49.200 --> 02:09:54.800]   you use Sophos to stay protected from those ransomware attacks. Third-party reviewers consistently rank
[02:09:54.800 --> 02:09:59.680]   Sophos among the best cybersecurity providers. And I love that synchronized security that can
[02:09:59.680 --> 02:10:02.800]   manage all your products from a single cloud-based console. You're going to love it too.
[02:10:02.800 --> 02:10:06.640]   And it's free. You can try it for free. It's not free forever, but it's free to try.
[02:10:06.640 --> 02:10:11.360]   And they always have offered. And we've mentioned this many times a really good free
[02:10:11.360 --> 02:10:20.160]   security scan that you can run at Sophos. S-O-P-H-O-S Sophos.com. That's it. Just Sophos.com.
[02:10:20.160 --> 02:10:27.360]   We thank them so much for their support of Twit. Facebook. Oh, don't you love Facebook.
[02:10:29.360 --> 02:10:34.480]   Let me just ask before I delve into this. Do we even need Facebook?
[02:10:34.480 --> 02:10:40.400]   A lot of businesses do. Their data are tied up in their apps and
[02:10:40.400 --> 02:10:47.360]   for a lot of sign-ons. Oh, yeah. Like that flow app that monitors your menstrual cycles.
[02:10:47.360 --> 02:10:52.480]   You're the one that has Facebook code. The one that has Facebook code that every time you use it,
[02:10:52.480 --> 02:10:56.960]   whether you're a Facebook user or not, it sends it off to Facebook. That app? Yeah, they need it.
[02:10:58.800 --> 02:11:04.880]   My campaign team, part of hiring hardened professionals this time around for 2020,
[02:11:04.880 --> 02:11:10.800]   is you're talking to people that are very pragmatic and have won elections. And they sat down with
[02:11:10.800 --> 02:11:15.200]   me at this cycle and they're like, "Bree, we know you hate Facebook. We know you're
[02:11:15.200 --> 02:11:22.400]   from the publicly objectionable, but the cold, hard fact of it is we ran the numbers. And 20%
[02:11:22.400 --> 02:11:28.400]   of the people in the district that you need to vote for you are on Twitter and 83% of them are
[02:11:28.400 --> 02:11:33.680]   on Facebook." That's the people's social network, not Twitter. That's right. That's right.
[02:11:33.680 --> 02:11:38.480]   And it makes me want to throw up because I think they are the worst company in tech today,
[02:11:38.480 --> 02:11:46.000]   like bad for democracy, bad for the tech industry, bad for virtual reality. But I think you don't
[02:11:46.000 --> 02:11:51.840]   have a choice. This is a byproduct of a monopoly. So Facebook is one of the GMafia in my book.
[02:11:52.720 --> 02:11:59.440]   And in life. And so the center part of the book, I go into an optimistic, pragmatic
[02:11:59.440 --> 02:12:04.800]   and scenarios for the future. And I guess I will spoil this a little bit, but Facebook doesn't
[02:12:04.800 --> 02:12:09.760]   exist in the future in two of those sets of scenarios. What's going to take them down?
[02:12:09.760 --> 02:12:15.680]   I don't think it's a matter of what takes them down as much as how does the business model survive
[02:12:15.680 --> 02:12:23.040]   once there are two things happening. One, again, if regulation becomes unavoidable,
[02:12:23.040 --> 02:12:28.080]   that is a catastrophic blow to Facebook. And two, Zuckerberg said as much to Congress.
[02:12:28.080 --> 02:12:30.240]   He said, "This is our business model. We can't exist without this."
[02:12:30.240 --> 02:12:36.240]   The second thing is what I like to call the diet coke test. So have you ever noticed that
[02:12:36.240 --> 02:12:42.320]   you'll notice if you start looking around that people who are drinking diet coke tend to be of a
[02:12:42.320 --> 02:12:49.120]   certain age and younger people, and by that, like very older millennials, young gen X and above drink
[02:12:49.120 --> 02:12:54.640]   diet coke's kids don't because they have good taste. Well, because they didn't grow up with it
[02:12:54.640 --> 02:12:58.800]   because it wasn't a social touch point. And there was a and people say, "Are they the Pepsi
[02:12:58.800 --> 02:13:03.840]   generation? Is that why?" No, they're the bottled water generation. That's an orange
[02:13:03.840 --> 02:13:11.760]   generation. Right. So anyhow, so Facebook has its own diet coke test coming up.
[02:13:12.480 --> 02:13:17.440]   There's an entire generation of people who not only didn't grow up with Facebook as their primary
[02:13:17.440 --> 02:13:21.680]   social network, but have been told over and over again that Facebook is evil. So I think Facebook,
[02:13:21.680 --> 02:13:28.560]   I would put the odds against Facebook surviving in the long term, but it exists for right now.
[02:13:28.560 --> 02:13:32.480]   This explains actually something because, and this is one of the stories, I won't go into all
[02:13:32.480 --> 02:13:36.080]   those things that Facebook did wrong this week. There's not enough time I want to talk about
[02:13:36.080 --> 02:13:42.960]   Elon Musk. But remember when we were talking about that research program that Facebook had put on
[02:13:42.960 --> 02:13:49.200]   Apple after Apple banned the onavo program because it was spying on users. And so Facebook took the
[02:13:49.200 --> 02:13:53.120]   onavo code, put it in the program with a different name and then put it out. And then Apple said,
[02:13:53.120 --> 02:13:58.560]   "No, no, you can't do that." Facebook said, "No, no, no, less than 5% of the people who,
[02:13:58.560 --> 02:14:02.560]   by the way, chose to participate in this market research program were teens."
[02:14:04.160 --> 02:14:08.480]   Isn't that illegal? Well, they got, oh, and we had it and we got written permission from their
[02:14:08.480 --> 02:14:18.560]   parents, they said. Well, it turns out tech crunches obtained Facebook's unpublished February 21st
[02:14:18.560 --> 02:14:23.760]   response to questions about the research program to Mark Warner, said it to Mark Warner. In the
[02:14:23.760 --> 02:14:32.000]   letter to Mark Warner, they admitted, well, it wasn't 5%, it was 18%. And of course, as you might
[02:14:32.000 --> 02:14:37.760]   imagine, the real reason that onavo existed and the Facebook research app existed is because they
[02:14:37.760 --> 02:14:42.400]   were trying for the life of them to figure out what the hell kids are doing. What are they doing?
[02:14:42.400 --> 02:14:49.920]   We got to fuck, what program should we buy next? 18%. So they lied.
[02:14:49.920 --> 02:14:58.720]   Fine. But that's become the, it's a pathology now.
[02:14:58.720 --> 02:15:02.960]   That's the problem. Lie and then apologize then lie again. And then apologize later,
[02:15:02.960 --> 02:15:09.360]   and there's never any consequences. I mean, and a fine, even if a fine gets levied,
[02:15:09.360 --> 02:15:12.960]   maybe that irritates investors for five seconds and then we all go back to business as usual.
[02:15:12.960 --> 02:15:17.280]   Okay, enough Facebook.
[02:15:17.280 --> 02:15:22.640]   Because I promise to- I don't have anything new to say. No, we've said it. They need to face
[02:15:22.640 --> 02:15:27.360]   more fun. They need to face consequences. I love Jeremy Burge, who we love. He's on the
[02:15:27.360 --> 02:15:32.240]   Unicode, a comedian, the founder of Mojipedia, a great tweet. For years, Facebook claimed that
[02:15:32.240 --> 02:15:38.080]   adding a phone number was for two-factor authentication, right? You secured your Facebook account.
[02:15:38.080 --> 02:15:43.840]   You had to give them a phone number so they could text you, right? Well, now it can be searched,
[02:15:43.840 --> 02:15:50.000]   and there's no way to disable it. There's no way. The original Facebook number prompt never
[02:15:50.000 --> 02:15:54.560]   mentioned that it would be used for anything but two-factor. It does now. They added that months
[02:15:54.560 --> 02:16:03.600]   afterwards because they got caught. They're going ahead. We knew this because of excellent research.
[02:16:03.600 --> 02:16:10.160]   I think it was from pro-publica, but I can't remember where they bought ads based on people's
[02:16:10.160 --> 02:16:14.320]   phone numbers, people who had never given their phone number to Facebook except for
[02:16:14.320 --> 02:16:21.840]   two-factor. I could go on, but I won't. I did because we wanted to talk about Elon. I promised
[02:16:21.840 --> 02:16:28.960]   that we would talk about Elon. Yes. Elon in the something again,
[02:16:28.960 --> 02:16:36.400]   he's promising. He's not using Twitter. He's the poster boy. I know.
[02:16:36.400 --> 02:16:42.400]   Yup. One tweet. By the way, a two-word tweet. It wasn't even a 280 character tweet.
[02:16:42.400 --> 02:16:47.760]   No, I think the other-- Are you talking about the SEC violation? Yeah, $20 million for
[02:16:47.760 --> 02:16:55.040]   saying funding acquired. Yeah. No. I mean, he got in trouble this time. No, he's in trouble again.
[02:16:55.040 --> 02:17:03.600]   Right. Right. SEC has asked a judge to hold Elon Musk in contempt because after he got fined for
[02:17:03.600 --> 02:17:11.760]   $20 million, he agreed. It made a deal that he would not-- he would get approval from the SEC
[02:17:11.760 --> 02:17:17.280]   before he made any material tweets about Tesla's business. He just tweeted that Tesla would make
[02:17:17.920 --> 02:17:21.040]   500,000 vehicles this year and then four hours later saying, "I mean,
[02:17:21.040 --> 02:17:27.680]   the company's annualized production rate at the end of 2019 could be around 500,000 vehicles."
[02:17:27.680 --> 02:17:33.760]   The SEC went to the court saying, "Must it not seek or receive pre-approval?" Prior to publishing
[02:17:33.760 --> 02:17:40.240]   this tweet, which was inaccurate and disseminated to over 24 million people Musk has until March 11th
[02:17:40.240 --> 02:17:46.080]   to explain to the court why he should not be held in contempt. So I have a lot to say about--
[02:17:46.080 --> 02:17:47.440]   Ms. Stockston, 3%
[02:17:47.440 --> 02:17:52.160]   Leo, you and I both worked as journalists. And I think--
[02:17:52.160 --> 02:17:56.960]   I think could you use the present tense for me? Because honestly, I feel like I'm still a journalist.
[02:17:56.960 --> 02:18:01.120]   Okay. Right. Right. But I mean, like hard, hard, like covering--
[02:18:01.120 --> 02:18:05.120]   I never went to courtrooms. I never went to city hall meetings. I was never a reporter. I was
[02:18:05.120 --> 02:18:13.600]   a husband. So I've sat there and covered rape cases, murder cases, really, really serious things.
[02:18:14.400 --> 02:18:23.920]   And one of the things that experience taught me is you've got to treat judges with an immense
[02:18:23.920 --> 02:18:29.040]   amount of respect and take that process unbelievably seriously.
[02:18:29.040 --> 02:18:29.760]   They're cranky.
[02:18:29.760 --> 02:18:40.160]   What really bothers me about Elon is he had the SEC thing before. He got his deal,
[02:18:40.160 --> 02:18:46.720]   and then he goes on CBS and is just blasting everyone involved in the most conceited interview
[02:18:46.720 --> 02:18:51.760]   I've ever seen. And then he comes back and he does this. And I just--
[02:18:51.760 --> 02:19:01.120]   I have to say, it's so unbelievably arrogant. I cannot process it. I understand there's an
[02:19:01.120 --> 02:19:07.120]   argument that, well, if you look at what he says, it could be seen as an honest mistake.
[02:19:07.120 --> 02:19:11.280]   And let's give him the benefit of the doll. But he dug it deeper, of course,
[02:19:11.280 --> 02:19:15.120]   he did. He did. He did. And he was dissing the SEC,
[02:19:15.120 --> 02:19:22.000]   something's broken with the SEC. So he-- It reminds me of Roger Stone.
[02:19:22.000 --> 02:19:29.040]   And I was, again, in trouble with the court after being told, "Stop it." He used--
[02:19:29.040 --> 02:19:31.360]   It was a great documentary. It was a monologue.
[02:19:31.360 --> 02:19:33.920]   Get me Roger Stone. Yeah, and Netflix.
[02:19:33.920 --> 02:19:41.040]   Love him. He used Instagram, not Twitter. But I would submit that there are just some people who
[02:19:41.040 --> 02:19:49.440]   should really not be using social media. And I'm one of them. I don't, because I'm afraid of the
[02:19:49.440 --> 02:19:54.400]   backlash. I mean, I don't know. What are you going to do?
[02:19:54.400 --> 02:19:59.200]   Well, I would posit something maybe totally different, which is--
[02:20:00.480 --> 02:20:06.080]   I don't know Elon at all. Nor do I. I drive a Tesla. And I am, I will save
[02:20:06.080 --> 02:20:10.480]   grateful to Elon for what he has done. Very grateful.
[02:20:10.480 --> 02:20:15.760]   I also drive a Tesla. And I'm looking forward to the summon feature, which I guess is going to
[02:20:15.760 --> 02:20:19.840]   get rolled out sometime soon so that my car can come and pick me up in the parking lot.
[02:20:19.840 --> 02:20:26.000]   But here's what I would say. We don't know him, but I have to think that as you ascend
[02:20:27.440 --> 02:20:31.920]   throughout the various social strata and economic strata, I think the closer that you get to the
[02:20:31.920 --> 02:20:38.960]   top, the more lonely it is and the harder it is to get probably affirmation that feels genuine
[02:20:38.960 --> 02:20:44.320]   in any way. And I just wonder if he tweets stuff out like this, because he needs somebody to say
[02:20:44.320 --> 02:20:51.840]   you're doing a great job. And he doesn't-- I don't know. Maybe he's something totally different,
[02:20:51.840 --> 02:20:56.320]   but maybe he's just very lonely and needs somebody to tell him he's doing a good job.
[02:20:56.960 --> 02:21:03.440]   The great Gatsby. F's got Fitzgerald. Let me tell you about the very rich. They are different
[02:21:03.440 --> 02:21:08.000]   from you and me. They possess and enjoy early, and it does something to them.
[02:21:08.000 --> 02:21:14.480]   Makes them soft where you're hard and cynical, where we are trustful in a way that unless you
[02:21:14.480 --> 02:21:20.000]   were born rich, it is very difficult to understand. They think deep in their hearts that they are
[02:21:20.000 --> 02:21:25.840]   better than we are because we had to discover the compensations and refuges of life for ourselves,
[02:21:26.400 --> 02:21:32.080]   even when they enter deep into our world or sink below us, they still think they are better than
[02:21:32.080 --> 02:21:38.320]   we are. They are different. He wrote that 90 years ago. I think it's still terrific.
[02:21:38.320 --> 02:21:46.400]   Oh my gosh. Yeah. I was saying, like, you've reading this book about Palmer Lucky recently.
[02:21:46.400 --> 02:21:52.800]   He has someone who got a lot of blowback and ultimately was pressured and forced to leave
[02:21:52.800 --> 02:22:02.880]   Facebook because of some memes he allegedly funded that were very anti-Hillary and allegedly terrible.
[02:22:02.880 --> 02:22:08.480]   Something I thought that was very interesting about this book was how much it humanized him.
[02:22:08.480 --> 02:22:16.480]   I walked away from this book really feeling like there was never a story told about this kind of
[02:22:17.120 --> 02:22:24.480]   brilliant engineer that was 19 years old and trying to revive this dead technology and trying
[02:22:24.480 --> 02:22:28.960]   to figure out positional tracking when no one else could. It just gave me a very different
[02:22:28.960 --> 02:22:34.000]   perspective on that. And Amy just bouncing off what you were saying, I have no doubt there is a
[02:22:34.000 --> 02:22:46.000]   flawed and very likable person there in Elon. But at the same time, the SEC is not a show.
[02:22:46.000 --> 02:22:52.800]   We have people's live savings tied up in these companies. And after Theranos, if you listen to
[02:22:52.800 --> 02:22:59.200]   the podcast, the dropout, you get to listen to investors that put their life savings in this
[02:22:59.200 --> 02:23:05.920]   company and were destroyed over it. So I would say it's not personal in feeling like he needs to be
[02:23:05.920 --> 02:23:11.760]   held to standards. This is just the law. And if he's not going to take it seriously,
[02:23:11.760 --> 02:23:16.960]   I think he should meet his consequences in front of a judge and with due process.
[02:23:16.960 --> 02:23:21.120]   I'm just saying he should get off Twitter. That's all. I second that.
[02:23:21.120 --> 02:23:28.400]   TikTok find $5.7 million. This is the number one app, especially with young people.
[02:23:28.400 --> 02:23:33.120]   If Facebook were wanted another app, this would be the one to buy, except it's already been bought
[02:23:33.120 --> 02:23:40.000]   by ByteDance, a giant in China. TikTok, which combined musically with TikTok and is the biggest
[02:23:40.000 --> 02:23:46.240]   social media network for the under 20s, has been apparently collecting information and illegally
[02:23:46.240 --> 02:23:57.280]   collecting information from children under 13 FTC sued and 15, sorry, a $5.7 million fine.
[02:23:57.280 --> 02:24:04.800]   Actually, this goes back to the musically days. So TikTok kind of gotten the heat by buying them.
[02:24:06.960 --> 02:24:12.720]   Okay, I'm going to end with one last story that's a, I think, an upbeat story to cheer you all up.
[02:24:12.720 --> 02:24:24.000]   Santiago Lopez from Argentina. He was a young kid, didn't have a lot of money,
[02:24:24.000 --> 02:24:30.480]   was inspired by the movie Hackers, taught himself how to hack watching. And I love this free online
[02:24:30.480 --> 02:24:37.920]   tutorials and reading popular blogs. At the age of 16, he earned his first bug bounty, $50.
[02:24:37.920 --> 02:24:47.040]   He's continued to hack after school. He is now 19 years old, hacks full time. He's the first
[02:24:47.040 --> 02:24:54.240]   teenage hacker to earn $1 million in bug bounties. He's reported more than 1600 security
[02:24:54.240 --> 02:25:00.240]   flossed organizations, Twitter and Verizon, among others, private enterprise, as well as government
[02:25:00.240 --> 02:25:09.120]   entities. I love this self taught hacker from Buenos Aires from Argentina, Santiago Lopez.
[02:25:09.120 --> 02:25:15.600]   And he learned online. I think that's a really great story. The bug bounties work as far as being
[02:25:15.600 --> 02:25:20.640]   not the only component of cybersecurity, but certainly a component. And I think for me,
[02:25:20.640 --> 02:25:27.280]   one of the plans I would have for United States and cyber warfare is I think we should radically
[02:25:27.280 --> 02:25:33.760]   expand our bug bounty system. I would love to have people penetration testing, like schools,
[02:25:33.760 --> 02:25:39.360]   hospitals, other non military infrastructure and figuring out how to make it more secure.
[02:25:39.360 --> 02:25:47.520]   I think about the number of women I know personally who have amazing engineering skills and have left
[02:25:47.520 --> 02:25:52.720]   the field because they feel like the culture isn't worth it. How awesome that would be if they could
[02:25:52.720 --> 02:25:58.400]   do this kind of work and pursue these bounties in making your schools and hospitals safer.
[02:25:58.400 --> 02:26:00.160]   I think it's a great idea. Love it.
[02:26:00.160 --> 02:26:07.440]   Man, I really like having you guys on. Thank you so much for being here.
[02:26:07.440 --> 02:26:11.440]   Amy Webb's new book is the big nine, How the Tech Titans and their thinking machines could warp
[02:26:11.440 --> 02:26:15.200]   humanity. You're doing a book tour right now. Are you going to be anywhere we should look for?
[02:26:16.320 --> 02:26:20.480]   If you go to Amy Webb.io, the book tour schedule is there. I'm in
[02:26:20.480 --> 02:26:24.000]   one Matt South by this coming weekend. So that's right. Of course.
[02:26:24.000 --> 02:26:32.240]   Doing book signing on Saturday at 1230. I'm going to be in. I'll be at politics and pros.
[02:26:32.240 --> 02:26:38.160]   If you're in Seattle, I'm doing a town hall debate. I'm doing, I'll be in Chicago. I'll be in New York.
[02:26:38.160 --> 02:26:43.520]   I'll be in San Francisco, Boston, Minneapolis, Baltimore. That list that you're showing is actually
[02:26:43.520 --> 02:26:50.160]   not even complete. So I'll be all over the place. And I, you know, I would love to say hi in person.
[02:26:50.160 --> 02:26:54.320]   I'm actually really impressed because I thought book tours were a thing of the past.
[02:26:54.320 --> 02:27:01.360]   Your publisher must love you. I will. I will say this. The people who got early copies of the book
[02:27:01.360 --> 02:27:08.960]   who read it, it really scared the hell out of them. You know, it is not, it is not a field.
[02:27:08.960 --> 02:27:14.800]   It's a fast read. It is not a feel good book. The last part of it is solutions and tactical
[02:27:14.800 --> 02:27:22.000]   recommendations for everybody, for you, for me, for government, for universities. So, but it scared
[02:27:22.000 --> 02:27:26.960]   the hell out of them. So I think it's gotten some buzz. And I just found out yesterday
[02:27:26.960 --> 02:27:34.640]   that it made two lists. So ironically on Amazon, it's not even out yet, right? It's not out for a
[02:27:34.640 --> 02:27:40.000]   couple of days. No, it doesn't come out until Tuesday. Amazon named it the best. It got listed
[02:27:40.000 --> 02:27:46.320]   for both best history book and best nonfiction book for the month. Congratulations. It's not even out.
[02:27:46.320 --> 02:27:51.680]   Yeah. So I'm pretty, I'm pretty jazzed. And I wrote this book because we need to take action
[02:27:51.680 --> 02:27:56.160]   and we need to change the developmental track of AI. And I think we all have a part in doing that.
[02:27:56.160 --> 02:28:00.240]   So I really hope people will read it and take it seriously. I think, though,
[02:28:00.240 --> 02:28:06.160]   yeah, just bought your book on audible and I will be, I will read that. And I would love to,
[02:28:06.160 --> 02:28:08.880]   if I'm fortunate enough to win my election, talk about how to get here.
[02:28:08.880 --> 02:28:12.560]   Absolutely. And I, you don't have to listen to my voice. That's the best part of the
[02:28:12.560 --> 02:28:18.000]   audible book. It's not me. There's a person who has a pleasant voice who you will enjoy listening
[02:28:18.000 --> 02:28:22.560]   to. I enjoy listening to you, Amy. But I do have a marketing idea. I think you need slap
[02:28:22.560 --> 02:28:26.800]   bracelets to say the big nine. And then you could just hand those out at all your.
[02:28:26.800 --> 02:28:30.480]   Yeah, with no weird pixel line breaking up the middle. Yeah.
[02:28:30.480 --> 02:28:35.120]   Thank you, Amy. Thank you, Brianna Wu, Brianna Wu for Congress,
[02:28:35.120 --> 02:28:41.760]   the Massachusetts 8th. We're going to get her in this time. Go to, you know,
[02:28:41.760 --> 02:28:45.600]   follower at Brianna Wu. The website is what Brianna Wu.
[02:28:45.600 --> 02:28:50.720]   It's Brianna Wu for Congress. And if I can tell you about something, Leo, we're,
[02:28:50.720 --> 02:28:56.000]   we're about to have a really, I'm seeing here as the old, like temporary website, we're having a
[02:28:56.000 --> 02:29:00.720]   really big release this week. It's a completely redone site. But one of the things that
[02:29:00.720 --> 02:29:07.840]   it was really hard for me to do as a candidate is we have a bunch of videos telling people about
[02:29:07.840 --> 02:29:14.560]   my life. I can tell people about game or gate, but one of the things that is really hard for me to
[02:29:14.560 --> 02:29:20.640]   talk about is someone running for public office is, you know, I was adopted, which meant my first
[02:29:20.640 --> 02:29:26.080]   family just threw me away when I was born. And then when I came out to my parents in my 20s,
[02:29:26.080 --> 02:29:33.280]   they disowned me instantly. I lost my second family as well. And I talk a lot about how
[02:29:33.280 --> 02:29:40.960]   that struggle to find myself and to come back really kind of defined my politics and my commitment
[02:29:40.960 --> 02:29:47.600]   to my neighbors and the people around me. So that is coming out this week. If Twitter listeners want
[02:29:47.600 --> 02:29:50.560]   to see that or share that, I would personally appreciate it.
[02:29:50.560 --> 02:29:55.280]   I will and Brianna, I just donated a hundred bucks because I want you to win.
[02:29:55.280 --> 02:29:57.680]   I appreciate that. Thank you very much.
[02:29:57.680 --> 02:30:02.480]   So I just, you're the shopping cart system on your website works. And if I was in your state,
[02:30:02.480 --> 02:30:03.920]   I would vote for you.
[02:30:03.920 --> 02:30:08.960]   But if you can't contribute, I did that last time, I'll do it again. And I'm really, I have to say,
[02:30:08.960 --> 02:30:14.000]   I'm really enjoying contributing to all the female candidates for 2020. I think that's my no men.
[02:30:15.040 --> 02:30:19.440]   No man. I like you, Joe Biden. I'm sorry. No men. No men allowed. No.
[02:30:19.440 --> 02:30:25.920]   Thank you, Brianna. Brianna. And don't forget Brianna hosts a fabulous podcast with Simone
[02:30:25.920 --> 02:30:33.120]   de Roche for and our friend, Christina Warren. It's called rocket because it's a rocket to the moon.
[02:30:33.120 --> 02:30:38.960]   And you must listen to that on relay FM. Thank you everybody for being here. What a fun show.
[02:30:38.960 --> 02:30:44.080]   This was I appreciate it. We do Twitter every Sunday afternoon, three p.m. Pacific, six p.m.
[02:30:44.080 --> 02:30:50.560]   Eastern time. That's 2300 UTC. Just a program note. I realized I'm sitting around for an hour
[02:30:50.560 --> 02:30:55.360]   waiting for the show to start. So we might try to start it earlier from now on. What are we thinking
[02:30:55.360 --> 02:31:00.800]   Carson? Maybe two 15 Pacific five 15 Eastern two 15 sounds about right? Yeah, it may be a little
[02:31:00.800 --> 02:31:05.440]   later because I'm getting off the radio show it too. And we'll come over and it's a long commute
[02:31:05.440 --> 02:31:12.880]   across the hall, but I'll try to make it in 15 minutes. So if you want to watch the show live,
[02:31:12.880 --> 02:31:19.440]   start thinking about doing it at two 15 Pacific five 15 Eastern that's 22 15 UTC. You don't have
[02:31:19.440 --> 02:31:24.080]   to watch live. It's fun to if you do go in the chatroom at IRC.twit.tv, but you know everything we
[02:31:24.080 --> 02:31:28.880]   do really is on demand. That's the whole point of it. And you could find those shows on demand at
[02:31:28.880 --> 02:31:36.000]   our website, twit.tv. You know, just subscribe if you can find your favorite podcast application
[02:31:36.000 --> 02:31:39.920]   and say, I want to listen to this week in tech every week and now we'll have it for your Monday
[02:31:39.920 --> 02:31:44.080]   morning commute. If you want to be here in studio at a great studio audience, all you have to do is
[02:31:44.080 --> 02:31:49.920]   email tickets at twit.tv. We'll make sure there's a a chair out for you. Thanks for being here. We'll
[02:31:49.920 --> 02:31:52.640]   see you next time. Another twit is in the can.
[02:31:52.640 --> 02:32:02.960]   Do the twit. All right. Do the twit, baby. Do the twit. All right. Do the twit.

