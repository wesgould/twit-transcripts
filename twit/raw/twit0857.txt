;FFMETADATA1
title=New World Disorder
artist=Leo Laporte, Amy Webb, Fr. Robert Ballecer, SJ
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-01-10
track=857
language=English
genre=Podcast
comment=The future of finance, Web3, NASA Webb Telescope, synthetic biology, CES 2022
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:07.360]   It's time for Twit this week in Tech. A different show this week. We're kicking off the New Year with a show about the future
[00:00:07.360 --> 00:00:10.560]   Father Robert Ballis here the digital Jesuit joins us
[00:00:10.560 --> 00:00:16.520]   He's actually firmly mired in the Middle Ages, but with an eye toward the future and futurist Amy Webb
[00:00:16.520 --> 00:00:24.920]   She's just written a book about synthetic biology a look at the future plus a little news from CES coming up next on Twit
[00:00:27.840 --> 00:00:30.680]   Podcasts you love from people you trust
[00:00:30.680 --> 00:00:33.840]   This is Twit
[00:00:33.840 --> 00:00:45.080]   This is Twit this week in Tech episode
[00:00:45.080 --> 00:00:49.480]   857 recorded Sunday January 9th, 2022
[00:00:49.480 --> 00:00:52.120]   New World Disorder
[00:00:52.560 --> 00:01:01.400]   This episode of this week in Tech is brought to you by our crowd our crowd it helps accredited investors invest early in pre IPO companies
[00:01:01.400 --> 00:01:08.320]   alongside professional venture capitalists join the fastest growing venture capital investment community at our crowd
[00:01:08.320 --> 00:01:11.240]   com/twit and by
[00:01:11.240 --> 00:01:20.120]   Linode developed deploy and scale your modern applications faster and easier get $100 in credit when you visit linode
[00:01:20.120 --> 00:01:22.400]   com/twit
[00:01:22.480 --> 00:01:23.840]   and by
[00:01:23.840 --> 00:01:30.440]   Wealthfront to start building your wealth and get your first five thousand dollars managed for free for life
[00:01:30.440 --> 00:01:32.560]   Go to wealthfront.com
[00:01:32.560 --> 00:01:34.200]   /twit
[00:01:34.200 --> 00:01:35.440]   and by
[00:01:35.440 --> 00:01:41.080]   Better help join over one million people who take in charge of their mental health as a listener
[00:01:41.080 --> 00:01:45.080]   You'll get 10% off your first month by visiting better help.com
[00:01:45.080 --> 00:01:47.400]   /twit
[00:01:47.720 --> 00:01:57.920]   It's time for Twit this week in Tech the show we cover the weeks Tech News although this week
[00:01:57.920 --> 00:02:01.240]   We're gonna cover next year's tech news
[00:02:01.240 --> 00:02:08.480]   Joining us today our very own futurist. We always love having her on Amy Webb from the future today Institute
[00:02:08.480 --> 00:02:13.820]   CEO there and also a genius in every way. Hi Amy
[00:02:14.400 --> 00:02:19.720]   Hey Leo good to see you your new book is just weeks away. Are you excited?
[00:02:19.720 --> 00:02:25.440]   Very I cannot wait for people to read this not just because I'm I'm it I spent
[00:02:25.440 --> 00:02:28.780]   Years working on this and I think it's the most
[00:02:28.780 --> 00:02:32.960]   Import this is the most the most important technologies biology
[00:02:32.960 --> 00:02:40.880]   And I can't wait for people to read what we found your timing is actually really good because we are very much aware of now
[00:02:40.960 --> 00:02:46.320]   How important biology is to life on earth and so I think it couldn't be better timing
[00:02:46.320 --> 00:02:52.680]   I can't wait to read about this and you do cover MRM RNA and synthetic biology and all sorts of stuff. I can't wait
[00:02:52.680 --> 00:02:56.320]   It's gonna come just in time for Valentine's Day. So
[00:02:56.320 --> 00:03:03.600]   It is it's my my weird present to the world Lisa knows now which is gonna get for Valentine's day
[00:03:03.600 --> 00:03:10.920]   Also with us and I'm really pleased because he took his life into his own hands to go
[00:03:10.920 --> 00:03:17.080]   To CES this week father Robert ballis air the digital Jesuit hey Robert
[00:03:17.080 --> 00:03:20.520]   If she's a futurist, I guess that I'm a pastist
[00:03:20.520 --> 00:03:23.880]   Yeah, it's actually kind of appropriate
[00:03:23.880 --> 00:03:25.880]   We had planned a much larger show
[00:03:25.880 --> 00:03:32.800]   But the way the world is the number of people have canceled on us and so it is actually much more intimate show
[00:03:32.800 --> 00:03:34.800]   Which actually I don't mind in fact
[00:03:34.800 --> 00:03:39.240]   Honestly if I if I had to do the just the two of you for the rest of my life
[00:03:39.240 --> 00:03:42.320]   I would be very very happy with it you two of my favorite people
[00:03:42.320 --> 00:03:46.840]   But it is in a way kind of appropriate Amy is a futurist. She's looking ahead
[00:03:46.840 --> 00:03:49.520]   You are a priest in
[00:03:49.520 --> 00:03:53.280]   In the oldest church in the West
[00:03:53.280 --> 00:03:56.400]   Some would say a medieval
[00:03:56.400 --> 00:04:02.840]   Theology although as we know father Robert is far from medievalist
[00:04:02.840 --> 00:04:08.360]   But I think yeah, you're your perspective perhaps actually Amy you wanted to kick this off
[00:04:08.360 --> 00:04:11.960]   So I'm gonna let you do this it is gonna be a show about the future
[00:04:11.960 --> 00:04:19.000]   We had hoped to have a sci-fi author and some other people on to kind of give us some extra kind of insight of that
[00:04:19.000 --> 00:04:22.720]   So we had some cancellations, but I think we could still talk about this
[00:04:22.720 --> 00:04:25.360]   We'll also talk about the week's tech news and see yes, too
[00:04:25.360 --> 00:04:26.800]   There's plenty to talk about this week
[00:04:26.800 --> 00:04:32.120]   But why don't you kick things off because you was your idea. I blame you. It was your idea
[00:04:32.120 --> 00:04:37.680]   You thought it'd be fun and I thought so too to do a show about not just today's technology
[00:04:37.680 --> 00:04:39.000]   But tomorrow's
[00:04:39.000 --> 00:04:40.120]   Yeah
[00:04:40.120 --> 00:04:45.440]   So maybe just by way of a little bit of level setting so I'm my so I'm a futurist
[00:04:45.440 --> 00:04:48.440]   I'm a quantitative futurist my my goal
[00:04:48.440 --> 00:04:54.240]   Professionally is to make my job super boring so that nobody lights up when I say what I do anyway for real
[00:04:54.240 --> 00:04:56.760]   you have failed
[00:04:56.760 --> 00:04:58.760]   I failed so far
[00:04:58.760 --> 00:05:02.600]   But I want to not be the most like the most interesting person at the cocktail party
[00:05:02.600 --> 00:05:06.400]   I want people to look at me and be like yeah, you're like an accountant just in a different way
[00:05:06.480 --> 00:05:12.520]   So I thought it would be useful just really quickly to have everybody sort of for us to sort of just level set
[00:05:12.520 --> 00:05:19.480]   People think of futures as like palm readers or as gypsies, you know telling them. Yeah, you're not there's a reason
[00:05:19.480 --> 00:05:21.640]   No, no, there's a reason for that, but
[00:05:21.640 --> 00:05:26.760]   But there's a cognitive reason why we care about the future if you if you stop and think about it
[00:05:26.760 --> 00:05:28.560]   You know, we live in the moment
[00:05:28.560 --> 00:05:34.600]   So why should we think about the future and the answer has to do with what's called the zagarnic effect
[00:05:35.520 --> 00:05:37.520]   So there's a Soviet psychologist named
[00:05:37.520 --> 00:05:44.200]   Blooma's like arnic who he was doing a bunch of research and found that you know people don't really like it when we don't
[00:05:44.200 --> 00:05:47.960]   Right finish
[00:05:47.960 --> 00:05:49.960]   Well, we don't get to completion
[00:05:49.960 --> 00:05:56.960]   And we hate we hate that lack of completion so much that we invent what if scenarios
[00:05:56.960 --> 00:06:03.920]   But it's kind of it's kind of part of life. You don't make it to the end. You're gonna die in the middle of the story no matter
[00:06:03.920 --> 00:06:04.920]   What?
[00:06:04.920 --> 00:06:10.480]   Right, but the yes, but in the interim our brain is just continuously making up
[00:06:10.480 --> 00:06:15.240]   It's try its puzzles. We're just constantly problem-solving and yeah solving puzzles
[00:06:15.240 --> 00:06:21.480]   So the oracle of Delphi for those of you who remember your Greek mythology and high school and college
[00:06:21.480 --> 00:06:23.960]   To that point Delphi's at the center of the world
[00:06:23.960 --> 00:06:30.400]   There was a temple there and basically there was a priestess an oracle who would answer questions
[00:06:30.400 --> 00:06:37.920]   and some of this happened through divination, which is this ancient practice of just like you know looking at different lots and
[00:06:37.920 --> 00:06:40.720]   It's sort of the idea of chance, especially
[00:06:40.720 --> 00:06:48.800]   statistically is there is a very recent invention because the absence of chance was always intervention by God so
[00:06:48.800 --> 00:06:55.360]   and basically I think that divination was always a way of avoiding critical thinking about the future and and
[00:06:55.800 --> 00:06:59.800]   Sitting with that uncomfort like it's very uncomfortable to sit with the lack of completion
[00:06:59.800 --> 00:07:04.520]   So now as you go on through the years what starts off as divination
[00:07:04.520 --> 00:07:07.440]   ends up in the fourth century with St. Augustine
[00:07:07.440 --> 00:07:10.040]   Questioning whether or not there is such a thing as free will
[00:07:10.040 --> 00:07:19.200]   Or God's pre-determination and then Descartes who's looking at free will again, you know and think is like forms the foundation of AI
[00:07:20.080 --> 00:07:27.280]   All the way up to Jules Verne who's sort of writing pre-science fiction to HG Wells who I know everybody knows
[00:07:27.280 --> 00:07:28.480]   but
[00:07:28.480 --> 00:07:32.200]   Probably his most important work was not the time machine. It was this
[00:07:32.200 --> 00:07:40.620]   Paper that he wrote called the anticipations of the reaction of mechanical scientific progress upon human life and thought
[00:07:40.620 --> 00:07:42.620]   Oh, who would ever forget that?
[00:07:46.280 --> 00:07:52.040]   Well, basically what he was saying was because he was a journalist. He did something he called predictive writing
[00:07:52.040 --> 00:07:56.240]   But he was trying to say that these the sort of what if scenario
[00:07:56.240 --> 00:07:58.200]   He the word scenario didn't exist yet
[00:07:58.200 --> 00:08:03.020]   But what if these are important questions to ask and we can't just assume that fate is
[00:08:03.020 --> 00:08:08.500]   In charge of everything if we want to make our own destinies we have to do something about it
[00:08:08.500 --> 00:08:13.760]   So he he did and through this process which he was trying to get into universities
[00:08:13.760 --> 00:08:18.400]   He wanted people to study it the way they studied math. He imagined a national highway system
[00:08:18.400 --> 00:08:23.720]   Automated machines that he thought at that point would replace what was the servant class
[00:08:23.720 --> 00:08:30.520]   He thought of prefab houses. He thought at some point the collapse of capitalism would come and later on
[00:08:30.520 --> 00:08:37.800]   Unfortunately as tends to happen he got into eugenics. So his utopian future was um, yeah
[00:08:37.800 --> 00:08:42.240]   So that that happens but anyhow then you get to World War two and you get
[00:08:42.720 --> 00:08:47.280]   Nicholas Ressler and and Olaf Helmer who used something called the Delphi method
[00:08:47.280 --> 00:08:52.520]   Which is a structured way of having conversations to collect data again thinking about what if
[00:08:52.520 --> 00:08:57.760]   Herman Kahn who I trace my origin to he took a turn and
[00:08:57.760 --> 00:09:01.120]   Built on what they did but this time used game theory
[00:09:01.120 --> 00:09:03.440]   which is what I did academically and
[00:09:03.440 --> 00:09:09.240]   Developed probabilistic outcomes and so it was Kahn who first used the word scenario
[00:09:09.240 --> 00:09:11.760]   He was having lunch with a friend of his who worked in Hollywood
[00:09:12.280 --> 00:09:16.440]   It's World War two. He's he's trying to get military officials to
[00:09:16.440 --> 00:09:19.200]   think very
[00:09:19.200 --> 00:09:23.960]   very critically about what would be the aftermath of dropping a bomb on Japan and
[00:09:23.960 --> 00:09:30.120]   He couldn't get them to pay attention and and he kept telling people like just sending like numbers
[00:09:30.120 --> 00:09:36.000]   Aren't like it doesn't mean anything and he was actually trying to prevent this from happening
[00:09:36.000 --> 00:09:39.480]   So he wrote what became on thermo nuclear war
[00:09:41.080 --> 00:09:44.320]   And anyhow that takes us up to the modern era
[00:09:44.320 --> 00:09:50.040]   So you've got Pierre Vak who's this crazy Frenchman super into Eastern religions becomes a pivotal
[00:09:50.040 --> 00:09:55.960]   person executive at Royal Dutch shell because he figures out how to predict oil shocks
[00:09:55.960 --> 00:10:01.480]   And makes the business case for foresight Peter Schwartz
[00:10:01.480 --> 00:10:07.440]   You know and a bunch of people after them sort of become this diaspora and now
[00:10:07.440 --> 00:10:09.440]   Today
[00:10:09.440 --> 00:10:11.440]   We've got people like me
[00:10:11.440 --> 00:10:17.040]   so I'm trained and you know, it's it's not divination anymore or speculation
[00:10:17.040 --> 00:10:21.760]   but it is said it you know, it's still sitting with uncertainty and
[00:10:21.760 --> 00:10:25.600]   I would say now that there's a there's a little bit of a gap
[00:10:25.600 --> 00:10:34.360]   There's theorists. These are academics who are pretty closed off and they don't really care about practical applications and then there's enthusiasts
[00:10:34.360 --> 00:10:39.280]   They are super interested in the future, but don't necessarily have training
[00:10:39.520 --> 00:10:44.080]   And I think there's a in-between space that's useful. That's necessary for
[00:10:44.080 --> 00:10:49.040]   For society because our future depends on this type of work. I think
[00:10:49.040 --> 00:10:53.360]   And in a way, it's good to precondition people to say look, we're not
[00:10:53.360 --> 00:10:58.400]   Delphic oracles we're not going to predict what's going to happen because that's almost impossible
[00:10:58.400 --> 00:11:04.680]   This was going making the rounds on reddit last week from the New York Herald magazine of
[00:11:04.680 --> 00:11:09.240]   1922 what the world will be like in a hundred years
[00:11:09.240 --> 00:11:10.840]   and and
[00:11:10.840 --> 00:11:13.120]   Well, there's a lot of interesting stuff
[00:11:13.120 --> 00:11:21.120]   In here. It's of course a lot of mistakes as well, including they said we won't be cooking anymore in fact private dwellings
[00:11:21.120 --> 00:11:26.200]   We're gonna disappear because we're all gonna live in cooperative hassings with dining halls
[00:11:26.200 --> 00:11:29.120]   But some of it was actually pretty accurate
[00:11:29.120 --> 00:11:34.020]   That's not what you do. You're not trying to say what what the world's gonna look like in
[00:11:35.200 --> 00:11:39.120]   1922 no now now that being said
[00:11:39.120 --> 00:11:45.440]   My job is to explore alternative futures. So it's not about prediction
[00:11:45.440 --> 00:11:52.560]   It's about preparation. So a lot of times what I'm what I have a big team that I work with a lot of times
[00:11:52.560 --> 00:11:56.320]   We're going to companies the executives at companies and saying what if
[00:11:56.320 --> 00:12:02.120]   Your entire supply chain is not based anymore in China
[00:12:02.120 --> 00:12:06.640]   Yeah, what if it's based somewhere else what would be the potential outcomes or?
[00:12:06.640 --> 00:12:10.960]   Hey, we've there's this synthetic biology thing on the very near term horizon
[00:12:10.960 --> 00:12:18.800]   What if the outcome of this is that humans live ultra long lives, you know, that's good and on the one hand
[00:12:18.800 --> 00:12:23.040]   It's actually really bad for sports for perspective professional sports leagues. What why?
[00:12:23.040 --> 00:12:25.240]   Why
[00:12:25.240 --> 00:12:30.000]   Because you wind up with games. I mean, I'm not like super super into sports, but I'm super into baseball
[00:12:30.720 --> 00:12:32.720]   and you know
[00:12:32.720 --> 00:12:38.360]   You could make the argument that the star players who are also the most expensive never retire
[00:12:38.360 --> 00:12:45.980]   It would be bad it would make the game super boring, but it would also break the financial models. Yeah, right?
[00:12:45.980 --> 00:12:49.280]   So anyhow
[00:12:49.280 --> 00:12:55.680]   Professional sports to survive that's and you also need turnover just in general in government life
[00:12:55.680 --> 00:13:02.760]   I mean, yes, you can have supreme art current. So like the current supreme court gets to be you know on the court for life
[00:13:02.760 --> 00:13:05.200]   If if life now means a hundred and twenty years
[00:13:05.200 --> 00:13:12.880]   You know instead of eighty years. Yeah, does that become a problem? I think it probably does. Yeah, it's already too old as is the Congress
[00:13:12.880 --> 00:13:13.720]   Yeah
[00:13:13.720 --> 00:13:15.720]   So anyhow, we don't make predictions
[00:13:15.720 --> 00:13:17.840]   but we do
[00:13:17.840 --> 00:13:23.840]   Look at you again using not speculation, but like data. What are the possible outcomes?
[00:13:23.840 --> 00:13:31.640]   What are the next order impacts and the point of this sometimes is to inspire and delight, but what I'm hoping is that
[00:13:31.640 --> 00:13:40.400]   It enables it reduces uncertainty so we can make smarter decisions and also so that it inserts us into that future
[00:13:40.400 --> 00:13:47.920]   My concern is that with politics and COVID and everything else people are sort of nowists and
[00:13:48.960 --> 00:13:55.080]   And also absolutists and I think our future demands a sense of awe and wonder, you know
[00:13:55.080 --> 00:13:57.960]   Not complacency or disillusionment
[00:13:57.960 --> 00:14:03.600]   But leaning into that uncertainty with the hope that we can learn and we don't have all the answers
[00:14:03.600 --> 00:14:06.160]   And we're willing to make better decisions
[00:14:06.160 --> 00:14:12.560]   By the way her company's Amy's company the future today Institute does make available
[00:14:12.560 --> 00:14:16.600]   It's a tech trends report. You've been doing this for 14 years. I have a rare
[00:14:17.600 --> 00:14:23.200]   Print version of this 12 actually 13 because you started zero volumes
[00:14:23.200 --> 00:14:30.080]   But this is also online for everybody to see a future today Institute comm slash trans just add a curiosity
[00:14:30.080 --> 00:14:32.920]   You ever go back to a year one and see how you did
[00:14:32.920 --> 00:14:40.120]   Yeah, yeah, so after that preamble about this not being predictive and I can't possibly know the exact future
[00:14:40.120 --> 00:14:42.960]   We have a 97% accuracy, okay, nice
[00:14:44.160 --> 00:14:47.640]   But that's partly because you're not trying to say this will happen
[00:14:47.640 --> 00:14:53.680]   But truck talking about trends and things like that right now you can use data to calculate trajectories
[00:14:53.680 --> 00:14:56.520]   So we can determine for example
[00:14:56.520 --> 00:15:03.280]   You know machine learning will be on this trajectory and we think that the time horizons look like X right?
[00:15:03.280 --> 00:15:09.600]   The exact year doesn't matter the exact inflection point doesn't matter as much. It's it's sort of
[00:15:10.160 --> 00:15:14.680]   backwards engineering to the present what that could mean at different points, right?
[00:15:14.680 --> 00:15:19.960]   So I got I've been wrong on a few things NFC was one of them
[00:15:19.960 --> 00:15:27.760]   I I misread some patents and the near field communications chips in the back of phones that you tap against things
[00:15:27.760 --> 00:15:30.880]   Yeah, right would you say they were gonna be big or not?
[00:15:30.880 --> 00:15:36.320]   Big in iPhone specifically and I got that we just got the you know what we thought so too
[00:15:36.320 --> 00:15:41.680]   We thought now that iPhone has finally adopted NFC technology because they were really slow
[00:15:41.680 --> 00:15:45.360]   This is gonna make it take off and it's quite the opposite. It has not yeah
[00:15:45.360 --> 00:15:50.800]   Yeah, it's also took a lot longer for QR codes, which we all you know it took COVID for well
[00:15:50.800 --> 00:15:52.960]   We was interested in QR codes have taken off
[00:15:52.960 --> 00:15:58.360]   Maybe because of the pandemic, but yes, I'm using them much more than ever before yeah
[00:15:58.360 --> 00:16:01.600]   There's no way that's gonna take off that's nuts
[00:16:01.600 --> 00:16:04.560]   Well, I had been in Japan for anyhow. It doesn't matter
[00:16:04.560 --> 00:16:09.480]   I thought 10 years ago that that was gonna start to create sort of a
[00:16:09.480 --> 00:16:16.920]   Spatial hyper linking sort of being able to in the real world. Yeah hyperlink the real world
[00:16:16.920 --> 00:16:21.080]   Yeah, and it did that did happen in China and in Asia
[00:16:21.080 --> 00:16:24.280]   It just didn't take off in the US and that wasn't a huge deal
[00:16:24.280 --> 00:16:29.800]   But a lot of the times that the the work that we're doing leads to multi billion dollar investments
[00:16:30.640 --> 00:16:36.560]   Like big decisions, so we need to be we need to do this at you know with some some thought
[00:16:36.560 --> 00:16:44.240]   You know we do something in in my neck of the words that we call chaining and there's two sides of it
[00:16:44.240 --> 00:16:49.520]   The chain of decisions is what you would typically think of when you are looking at the future
[00:16:49.520 --> 00:16:54.240]   These are the steps that I want to take in order to put my organization where I think it needs to be
[00:16:54.240 --> 00:16:57.480]   But then along with any chain of decisions
[00:16:57.480 --> 00:17:03.800]   We also look at chain of consequences and chain of consequences is more like what Amy is talking about where we actually start asking
[00:17:03.800 --> 00:17:06.160]   the questions of the my new show well if
[00:17:06.160 --> 00:17:08.920]   exit Y and Z happens
[00:17:08.920 --> 00:17:15.320]   What are the other possible consequences from XY and Z happening rather than just looking at I want XY and Z to happen because that
[00:17:15.320 --> 00:17:23.640]   Allows me to move to step two or step three. I think the Jesuits are unusual in fact in their devotion to these kinds of methodologies and rigorous
[00:17:23.640 --> 00:17:26.120]   thinking I think it's more because
[00:17:26.920 --> 00:17:32.080]   You guys are trying to make thought rigorous and have always been it seems to me always been
[00:17:32.080 --> 00:17:34.840]   inclined that way
[00:17:34.840 --> 00:17:36.120]   That's it's part of our
[00:17:36.120 --> 00:17:37.000]   organizational DNA
[00:17:37.000 --> 00:17:42.280]   I mean that's one of the things that set us aside where we were willing to make something a
[00:17:42.280 --> 00:17:46.920]   Pronouncement that seemed completely out of place brash
[00:17:46.920 --> 00:17:48.680]   heretical to some
[00:17:48.680 --> 00:17:52.920]   But then it turned out to be true and the reason why it turned out to be true is not because well
[00:17:52.920 --> 00:17:57.880]   our guys had a great finger on the pulse of the future it was because we actually sat in a room and
[00:17:57.880 --> 00:18:03.800]   Went back and forth with a lot of screaming and someone finally said look if this happens
[00:18:03.800 --> 00:18:08.120]   Then there are four other possibilities that we need to at least prepare for
[00:18:08.120 --> 00:18:10.680]   It's like the Ben a Jesuits
[00:18:10.680 --> 00:18:16.440]   Yes, and which by the way, they were based on Jesuits. I think they were
[00:18:18.440 --> 00:18:24.120]   Well, and this is interesting because you work for an organization that is among the oldest organizations
[00:18:24.120 --> 00:18:26.120]   But probably is the oldest organization
[00:18:26.120 --> 00:18:30.120]   Uh still next I mean almost 2000 years of continuous history
[00:18:30.120 --> 00:18:33.720]   So that gives you a certain a certain standing in all of this
[00:18:33.720 --> 00:18:39.480]   That is a different than the rest of us who at most are looking in a century ahead
[00:18:39.480 --> 00:18:46.120]   Uh, but let's just say some of those decisions have not been great. They never all worked out. I understand
[00:18:47.640 --> 00:18:49.640]   That's just I'll leave it there
[00:18:49.640 --> 00:18:56.200]   And the Jesuits themselves are really only at 500 years old. So you really can't yeah a young we're young ins. Yeah
[00:18:56.200 --> 00:19:00.360]   Franciscans look at us and just go like who are you again? Sorry
[00:19:00.360 --> 00:19:05.960]   Well, okay, that's we're gonna actually get into this in a little bit more detail
[00:19:05.960 --> 00:19:09.800]   There is we'll talk about CES too because you were there and I do want to absolutely
[00:19:09.800 --> 00:19:12.120]   Talk about it
[00:19:12.120 --> 00:19:17.480]   And actually the kind of predictions of the future we do a twit is more like will there be a CES next
[00:19:17.560 --> 00:19:23.080]   Year than anything much more significant than that and by the way, we still get those wrong almost all the time
[00:19:23.080 --> 00:19:25.080]   I thought CES was long gone
[00:19:25.080 --> 00:19:30.520]   Uh, it seems seems not to have disappeared entirely anyway, even though it's it's dwindling
[00:19:30.520 --> 00:19:36.920]   We've got a just a wonderful panel of people today. I'm so glad to have Amy Webb here from Futures today institute
[00:19:36.920 --> 00:19:39.880]   Father Robert ballis air you're in vegas right now
[00:19:39.880 --> 00:19:41.960]   I am in vegas
[00:19:41.960 --> 00:19:46.840]   I'm here for my last pcr to uh to finish up and then i'm heading back to seperate sisco
[00:19:47.080 --> 00:19:52.520]   Then back to rum. Oh, okay. Did you come to the us just for this or you were visiting and other things?
[00:19:52.520 --> 00:19:58.440]   Okay, uh, so about once a year I get about a month off and I use it to visit family
[00:19:58.440 --> 00:20:03.960]   And it's two weeks with my sister two weeks with my parents and CES just happened to fall in that and your parents
[00:20:03.960 --> 00:20:09.320]   Happened to live in Las Vegas, which makes that even more convenient. Yeah much easier. I that's an amazing coincidence
[00:20:09.320 --> 00:20:11.800]   I just do i want to thought that
[00:20:11.800 --> 00:20:14.360]   Okay, it's not a coincidence
[00:20:15.480 --> 00:20:21.400]   Way back in the day, uh, when I was doing all these conferences. I may have purchased a house
[00:20:21.400 --> 00:20:27.080]   That they then moved into when they retired. That's okay a little plan behind that
[00:20:27.080 --> 00:20:34.360]   Uh, we're gonna take a little break in that and I want to talk about one of our sponsors right now that actually is exactly around this topic
[00:20:34.360 --> 00:20:36.920]   Planning for your future
[00:20:36.920 --> 00:20:41.320]   Uh, I know that many of the people who watch this show uh, are investors
[00:20:41.880 --> 00:20:48.680]   Uh, if you are somebody who is looking to find the next big thing to invest in you should know about our crowd
[00:20:48.680 --> 00:20:55.400]   Our crowd is a venture capital firm that analyzes companies across the entire global
[00:20:55.400 --> 00:21:02.440]   Private market now. These are not companies that have yet been discovered. They're looking for the next big
[00:21:02.440 --> 00:21:10.040]   Thing and because they are a large vc firm. They have really access to great what they called deal flow
[00:21:10.040 --> 00:21:16.840]   They really can see what's going on in areas that are about to break out like personalized medicine
[00:21:16.840 --> 00:21:20.760]   cyber security robotics quantum computing
[00:21:20.760 --> 00:21:25.160]   You know in state of the art labs and startup garages and anywhere in between
[00:21:25.160 --> 00:21:30.280]   Our crowd is identifying innovators. They make it possible for you to invest
[00:21:30.280 --> 00:21:34.520]   When the growth potential is the greatest early on
[00:21:34.520 --> 00:21:38.440]   Our crowd is the fastest growing venture capital investment community
[00:21:38.920 --> 00:21:42.360]   Uh, in order to participate now, I should say first of all it costs you nothing
[00:21:42.360 --> 00:21:46.120]   To go to our crowd and and join up that's free
[00:21:46.120 --> 00:21:53.320]   Uh, but at that point you might have the opportunity to invest but I have to explain first you have to be an accredited investor
[00:21:53.320 --> 00:21:58.280]   That's a legal term. Every country has different standards for what an accredited investor
[00:21:58.280 --> 00:22:05.880]   Is it usually has to do with net income and expertise and so forth because you know, this is not something you would do with your first dollar
[00:22:05.880 --> 00:22:10.200]   This is something you would do if you've you know been wise you've put a side money
[00:22:10.200 --> 00:22:13.320]   You've you've got your savings plan your retirement plan
[00:22:13.320 --> 00:22:20.040]   But you're looking for another area to invest in that is exciting that is interesting and has a lot of growth potential
[00:22:20.040 --> 00:22:26.680]   21 of their portfolio companies are unicorns, but this is what's cool. They weren't unicorns when they got in
[00:22:26.680 --> 00:22:32.360]   That's what's so important many of our crowds members have benefited from the over 40
[00:22:32.920 --> 00:22:39.160]   IPOs and sale exits a portfolio companies again another reason you want to get in early
[00:22:39.160 --> 00:22:45.720]   If you're an accredited investor, you could participate in single company deals with our crowd for as little as $10,000
[00:22:45.720 --> 00:22:47.720]   You don't have to have millions to invest
[00:22:47.720 --> 00:22:52.760]   Uh, you can get in the funds they have some funds as well. You can get in for those as little as $50,000
[00:22:52.760 --> 00:22:58.760]   But I should tell you a minute you have to be an accredited investor and a minimum of $10,000 is required to invest
[00:22:58.760 --> 00:23:02.680]   Investment terms are going to vary depending on what you invest in
[00:23:03.160 --> 00:23:08.040]   You can get more information when you go to the site the key is you're going to put the country that you live in
[00:23:08.040 --> 00:23:11.160]   Into the site and they will tell you what the rules are
[00:23:11.160 --> 00:23:18.440]   I'll give you an example though of the kind of interesting startups our crowd already can give you access to for instance
[00:23:18.440 --> 00:23:20.440]   You can invest in blue tree
[00:23:20.440 --> 00:23:25.240]   Blue tree is in the to in in the food tech market
[00:23:25.240 --> 00:23:31.400]   And they they have technology that our crowds investors believe could revolutionize this billion dollar plus
[00:23:31.880 --> 00:23:35.080]   Total addressable food tech market. They've developed a process
[00:23:35.080 --> 00:23:41.720]   And I think this is really cool to significantly reduce the sugar in any natural liquid
[00:23:41.720 --> 00:23:47.960]   Which of course makes it healthier lowers health risks, but without changing the taste
[00:23:47.960 --> 00:23:53.720]   Blue tree has already signed a five-year 100 million liter contract with an industry leader
[00:23:53.720 --> 00:23:56.440]   They're on their way. This is the kind of interesting
[00:23:57.080 --> 00:24:00.840]   New stuff you can get involved in investing today at our crowd
[00:24:00.840 --> 00:24:05.640]   So if you're interested if maybe in blue tree or one of their other portfolio companies
[00:24:05.640 --> 00:24:09.560]   Go to our cr o wd
[00:24:09.560 --> 00:24:12.120]   our crowd calm
[00:24:12.120 --> 00:24:17.720]   Slash twit costs you nothing to join that'll give you access to the research the information
[00:24:17.720 --> 00:24:21.800]   Then you can make a decision about whether you want to get involved our crowd
[00:24:23.480 --> 00:24:29.800]   Cr o wd.com/twit join the fastest growing venture capital investment community
[00:24:29.800 --> 00:24:33.000]   I love this because this is traditionally not been something
[00:24:33.000 --> 00:24:35.400]   We normal people can get involved in
[00:24:35.400 --> 00:24:40.280]   You know, this is something you gotta you gotta know people in sandhill road to get involved in now
[00:24:40.280 --> 00:24:47.240]   Anybody can our crowd.com/twit. Let me thank them so much for their support of this week in tech
[00:24:47.240 --> 00:24:50.120]   We had we'd wanted to have Kevin rose on the show to talk about
[00:24:50.120 --> 00:24:52.760]   Finance the world is changing
[00:24:53.080 --> 00:24:55.080]   Absolutely. Thanks to crypto
[00:24:55.080 --> 00:24:59.400]   Thanks to nft's thanks to defi decentralized
[00:24:59.400 --> 00:25:06.520]   Finance, I read a really interesting article today this morning by moxie marlin spike who I have a lot of respect for
[00:25:06.520 --> 00:25:08.680]   he is
[00:25:08.680 --> 00:25:16.120]   The guy who came up with signal. He's a brilliant technologist, but he also had some very interesting points to make about web three
[00:25:16.120 --> 00:25:18.520]   So let's start off
[00:25:18.520 --> 00:25:20.520]   talking about
[00:25:20.520 --> 00:25:26.280]   Crypto the web and in particularly web three the idea at least the proponents of web three
[00:25:26.280 --> 00:25:29.240]   Uh, say the idea is a decentralized
[00:25:29.240 --> 00:25:34.360]   Worldwide web that is run on blockchain not by a big platform company
[00:25:34.360 --> 00:25:38.120]   Uh, i've seen people kind of disagree with that
[00:25:38.120 --> 00:25:43.880]   Uh, including moxie who says essentially, you know, there's a fundamental problem
[00:25:43.880 --> 00:25:49.000]   With a decentralized platform. It just will not innovate as fast as a centralized platform
[00:25:49.560 --> 00:25:53.640]   So all the all the innovation happens with the platforms
[00:25:53.640 --> 00:25:56.520]   He says you have the same problem with web three
[00:25:56.520 --> 00:25:59.160]   Amy, what do what do you and your?
[00:25:59.160 --> 00:26:05.080]   People think about uh about the future of web three. Did you spend any time talking about that yet?
[00:26:05.080 --> 00:26:08.360]   Yeah, um, so heading into
[00:26:08.360 --> 00:26:17.000]   Uh, this year so our our big annual report comes out of south by um, south by southwest in march every year and one of the key themes is decentralization
[00:26:18.760 --> 00:26:20.360]   You know, I
[00:26:20.360 --> 00:26:25.800]   I have a real problem with the way that I hear it being described not from a technical or a protocol
[00:26:25.800 --> 00:26:31.160]   point of view but from sort of the point of view of what what it means. So if you if you go back
[00:26:31.160 --> 00:26:33.960]   You know, they're the web one oh world
[00:26:33.960 --> 00:26:41.240]   Um post academia as it was becoming commercialized to scribe the the standards right in the protocols to enable
[00:26:41.240 --> 00:26:45.320]   What was essentially one to many distributions? So you've got a centralized
[00:26:46.040 --> 00:26:49.960]   Hub for content or date or whatever it might be and it and it sort of goes out to many people
[00:26:49.960 --> 00:26:52.760]   You know web two oh
[00:26:52.760 --> 00:26:55.720]   Um was about interoperability
[00:26:55.720 --> 00:27:04.520]   Um openness and I think if you go back, you know decentralization was probably at the top of that list too because um
[00:27:04.520 --> 00:27:08.120]   Uh bit torrent was a part of that
[00:27:08.120 --> 00:27:10.280]   Um
[00:27:10.280 --> 00:27:15.160]   You know a lot of places where I used to scrape music was was part of that ecosystem
[00:27:15.800 --> 00:27:17.800]   Jason was a part of that, you know
[00:27:17.800 --> 00:27:22.520]   J store like there are all of these things that were part of that ecosystem prior to facebook
[00:27:22.520 --> 00:27:27.640]   You know that the types of companies today that we argue are too consolidated and have too much power
[00:27:27.640 --> 00:27:33.800]   Um and even the people who built a lot of the web 2o. I used to go to web 2o summit
[00:27:33.800 --> 00:27:35.960]   uh billion and a half years ago
[00:27:35.960 --> 00:27:38.520]   um, you know and
[00:27:38.520 --> 00:27:41.240]   and uh web 2o at the time was
[00:27:42.440 --> 00:27:47.320]   The precursor to twitter the precursor to facebook and some of these other things so you had odeo
[00:27:47.320 --> 00:27:50.360]   You had flicker you had wordpress you had six apart right
[00:27:50.360 --> 00:27:52.280]   um
[00:27:52.280 --> 00:27:57.160]   But over time you wind up with consolidation. So it's kind of not surprising that yet again
[00:27:57.160 --> 00:28:03.320]   We're talking about decentralization and the technology is certainly more sophisticated
[00:28:03.320 --> 00:28:04.840]   but
[00:28:04.840 --> 00:28:11.960]   In almost every other circumstance any time you look at an innovation cycle and companies developing new product services
[00:28:12.040 --> 00:28:14.360]   Protocols standards whatever it might be
[00:28:14.360 --> 00:28:24.920]   You wind up with consolidation. So I I know everybody's very excited about their prospect of web 3 which I think generally speaking stands for
[00:28:24.920 --> 00:28:31.240]   New sets of protocols. Um, you know using ethereum or or using
[00:28:31.240 --> 00:28:39.000]   New types of smart contracts tokens right a lot of this runs by tokens and and decentralized autonomous organizations and things like that
[00:28:40.200 --> 00:28:44.440]   But at some point you still have to have a group of people who are building out these systems
[00:28:44.440 --> 00:28:47.880]   Who are making decisions and at the moment it tends to be the same group of people
[00:28:47.880 --> 00:28:50.440]   It's pretty homogenous. So
[00:28:50.440 --> 00:28:54.840]   I i'm excited about some of the technology
[00:28:54.840 --> 00:29:02.280]   Certainly and I think that there are practical applications like it's going to give artists more control to to earn revenue
[00:29:02.280 --> 00:29:07.000]   You know in a web 3o world. I don't know if spotify has to exist anymore stuff like that
[00:29:07.880 --> 00:29:12.360]   But if what everybody's super excited about is decentralization, I think it's worthwhile to go back in time
[00:29:12.360 --> 00:29:19.240]   And look at the like go back to 2004 and some of the public conversations are on decentralization
[00:29:19.240 --> 00:29:22.360]   um and at the very earliest days of of
[00:29:22.360 --> 00:29:26.760]   The commercial web. I mean these these conversations pop up over and over again
[00:29:26.760 --> 00:29:32.040]   And I just worry that it's the same basic group of people making decisions for everybody but
[00:29:32.040 --> 00:29:34.600]   Without the world view of everybody
[00:29:34.600 --> 00:29:38.840]   I think you nailed it. I often say just like you the technologies are interesting
[00:29:38.840 --> 00:29:43.800]   But the actual implementations seem like more of the same jack dorsey
[00:29:43.800 --> 00:29:46.360]   tweeted a couple of weeks ago
[00:29:46.360 --> 00:29:52.760]   You don't own web 3 the vc's and their lp's their limited partners do it will never escape their incentives
[00:29:52.760 --> 00:29:57.400]   It's ultimately a centralized entity with a different label
[00:29:57.400 --> 00:30:01.480]   And in fact, that's exactly what marcy marlin spike was pointing out not out of
[00:30:01.960 --> 00:30:07.640]   Jack's implying some sort of kind of maliciousness, but moxie saying it's just the way it's the nature of things
[00:30:07.640 --> 00:30:12.680]   That these things end up becoming centralized because that's where all the innovation happens
[00:30:12.680 --> 00:30:17.800]   And it ends up being the platforms control it. He made an nft thought this would be kind of interesting
[00:30:17.800 --> 00:30:21.560]   You want to learn more about nft? So moxie is a very talented coder
[00:30:21.560 --> 00:30:24.920]   created an nft that changes
[00:30:24.920 --> 00:30:31.880]   Based on who's looking at it, you know nft's don't don't in any way guarantee that that
[00:30:31.960 --> 00:30:33.960]   Image is going to be any
[00:30:33.960 --> 00:30:36.680]   Particular way you might buy an nft of the Mona Lisa
[00:30:36.680 --> 00:30:42.280]   But since you don't actually have the Mona Lisa in your hand, it's just a website that can change so he created it
[00:30:42.280 --> 00:30:47.160]   Uh, if you look at his nft on open c it looks one way
[00:30:47.160 --> 00:30:52.680]   This nft on the left if you looked on an ararable it looks this way and if you looked at it in your wallet
[00:30:52.680 --> 00:30:54.760]   It would look like a poop emoji
[00:30:56.840 --> 00:31:02.840]   Uh, and his point is that many of the highest price nft's could turn into poop emojis at any time
[00:31:02.840 --> 00:31:06.280]   He just made it explicit, but what was even more interesting is
[00:31:06.280 --> 00:31:13.160]   After a few days of doing this his nft was removed from open c which is the nft marketplace
[00:31:13.160 --> 00:31:14.920]   uh
[00:31:14.920 --> 00:31:17.640]   Now it wasn't living on open c, but because
[00:31:17.640 --> 00:31:22.360]   Open c took it down it no longer appeared anywhere including
[00:31:23.240 --> 00:31:27.480]   His crypto wallet because it isn't decentralized
[00:31:27.480 --> 00:31:35.080]   It turns out everybody's doing a jason query to open c to see what that nft is and if open c doesn't list it
[00:31:35.080 --> 00:31:39.800]   It doesn't exist. So he really demonstrated a very clear way
[00:31:39.800 --> 00:31:45.480]   He says if it's it means if your nft is removed from open c it disappears from your wallet
[00:31:45.480 --> 00:31:50.440]   It doesn't functionally matter that my nft is indelibly on the blockchain somewhere
[00:31:50.920 --> 00:31:57.480]   Because the wallet and increasingly everything else in the ecosystem is just using the open c api to display it
[00:31:57.480 --> 00:32:02.600]   So, uh, it's a myth in other words that it's a decentralized
[00:32:02.600 --> 00:32:07.720]   Thing platforms inevitably just the nature of the beast and i think he's not saying this is a bad
[00:32:07.720 --> 00:32:10.280]   Thing it's just something you should be aware of
[00:32:10.280 --> 00:32:14.920]   And and you know it's it's it isn't what it promises to be
[00:32:14.920 --> 00:32:17.400]   Yeah, there's look
[00:32:17.400 --> 00:32:21.560]   When you're talking about web three when you're talking about the blockchain cryptocurrency nft's
[00:32:21.560 --> 00:32:27.800]   You're essentially running into the same problem. The idea the idea of decentralization is fantastic
[00:32:27.800 --> 00:32:31.400]   Wonderful promise is there it looks like it would work except for one
[00:32:31.400 --> 00:32:35.800]   Thing there's one component that kills everything and that is the reward component
[00:32:35.800 --> 00:32:43.480]   The reward for running a node the road the reward for allowing your resources to be used by a web application
[00:32:43.480 --> 00:32:48.360]   The reward for allowing your work to be represented in this ecosystem
[00:32:48.360 --> 00:32:54.840]   Has to be present and when you start talking about reward now you have to get into either proof of stake or proof of work
[00:32:54.840 --> 00:33:00.760]   And now we got the inefficiencies and the bureaucracies that we were trying to get rid of in the first place
[00:33:00.760 --> 00:33:06.680]   In other words, the idea of blockchain the idea of web three is great
[00:33:06.680 --> 00:33:10.920]   But the implementation destroys what the original idea was
[00:33:11.320 --> 00:33:16.360]   And now you've got crypto bros who are trying to shine over that last part they're saying well
[00:33:16.360 --> 00:33:21.080]   Don't worry about it. Don't don't it's the next technology is going to fix for that
[00:33:21.080 --> 00:33:26.600]   But how do you fix for fundamental greed and the need to have some sort of
[00:33:26.600 --> 00:33:33.560]   Authoritative source that tell you what is and is not legitimate and unfortunately that is not in the protocol
[00:33:33.560 --> 00:33:38.360]   That's not in the technology. That's a it's a fundamental flaw of human nature. Yeah, I agree
[00:33:38.760 --> 00:33:42.520]   So right so there's an economic argument picking up what pardry just said
[00:33:42.520 --> 00:33:44.200]   um
[00:33:44.200 --> 00:33:52.520]   You know adam a lot of the people who are fierce proponents of web three using decentralization as the best
[00:33:52.520 --> 00:33:56.200]   Possible way forward for humanity through protocols and tech and everything else
[00:33:56.200 --> 00:34:02.520]   They um will in the same breath often quote adam smith or they'll allude to to arguments by smith
[00:34:02.520 --> 00:34:05.800]   You know and sort of the invisible hand of the free market system improves
[00:34:06.520 --> 00:34:10.200]   Improves things for everybody except that there are always people in charge
[00:34:10.200 --> 00:34:14.920]   So there is no like there's no such thing anywhere as a truly democratized
[00:34:14.920 --> 00:34:17.400]   um flat system
[00:34:17.400 --> 00:34:23.800]   Without any centralization whatsoever and we actually already saw the problems play out in real time
[00:34:23.800 --> 00:34:28.280]   So do you guys remember constitution dao is happening?
[00:34:28.280 --> 00:34:31.960]   Yeah, they they raised 40 million dollars to buy a copy
[00:34:32.440 --> 00:34:37.800]   44 to buy a copy of the u.s. Constitution. They lost out to a hedge fund
[00:34:37.800 --> 00:34:45.160]   Magnet the guy by the way the guy interestingly who saved uh robin hood after the uh, the stonk
[00:34:45.160 --> 00:34:48.840]   Scandal of game stop, but anyway, yes
[00:34:48.840 --> 00:34:51.480]   Well, but here's why this matters um
[00:34:51.480 --> 00:34:58.200]   So, I mean it's a ridiculous story a bunch of people met up on twitter and they form a dao a decentralized wash on miss organization
[00:34:58.200 --> 00:35:01.080]   They use the very tools that you just heard pardry talk about
[00:35:01.720 --> 00:35:05.160]   Um, and they they remarkably raise an enormous
[00:35:05.160 --> 00:35:10.600]   Money yeah huge amount of money and for a very long time. It was looking as though this group of people
[00:35:10.600 --> 00:35:17.480]   Um was probably going to win and if they won the the if they won that constitution at auction
[00:35:17.480 --> 00:35:19.720]   It was one of the original 13 copies
[00:35:19.720 --> 00:35:23.000]   Um only one of two that's in private circulation
[00:35:23.000 --> 00:35:25.480]   They had a right to do anything they wanted with it
[00:35:25.480 --> 00:35:30.360]   They could donate it to a museum or they could have folded it into a paper airplane and flown it into a bonfire
[00:35:30.360 --> 00:35:35.800]   Tony tour into as many pieces as there were doubt contributors and given each one of them a tiny little bit
[00:35:35.800 --> 00:35:39.880]   Right now here's the problem. None of which they planned to do we should point out. No, okay
[00:35:39.880 --> 00:35:44.440]   So the issue is they lost they lost thank mercifully right they they lost
[00:35:44.440 --> 00:35:50.040]   Um the auction but now they're sitting on top of 44 million dollars and a whole bunch of people want that money back
[00:35:50.040 --> 00:35:53.400]   So now you've got so this sort of illustrates the central problem
[00:35:53.400 --> 00:35:57.080]   The problem is even more than that because the cost the gas fees
[00:35:57.560 --> 00:36:03.800]   Of refunding it end up being you get nothing back in fact it could cost you money to get quote get your money back
[00:36:03.800 --> 00:36:10.440]   Right, but somebody so it this has to do with accountability and one of the arguments that I hear made all the time is but
[00:36:10.440 --> 00:36:17.480]   There's the immutable ledger right and that assures accountability and the answer is yes in certain circumstances
[00:36:17.480 --> 00:36:22.760]   But the practical realities of how we're really to each other is humans and what our expectations are
[00:36:23.240 --> 00:36:28.280]   But up against the the technology itself. So well and as moxie pointed out
[00:36:28.280 --> 00:36:30.520]   Yeah, it may exist on the blockchain
[00:36:30.520 --> 00:36:36.280]   But if you need to go through an api with a centralized platform to seal it's on the blockchain in mox nix
[00:36:36.280 --> 00:36:38.200]   You can't see it
[00:36:38.200 --> 00:36:44.200]   And by the way nft's are too big to live on the blockchain all that lives there is a reference to another place
[00:36:44.200 --> 00:36:46.760]   Which also may not exist or turn it right and even
[00:36:46.760 --> 00:36:52.520]   Yeah, and even ethereum, you know, um and the and the token
[00:36:53.000 --> 00:36:55.000]   um system, I mean
[00:36:55.000 --> 00:37:00.360]   These are still open protocols in as much as tcp i's p and htps and you know
[00:37:00.360 --> 00:37:08.040]   We have series of protocols, but things have to get built on top of them and those things typically require some type of leadership structure
[00:37:08.040 --> 00:37:11.880]   So I just that's moxie's point platforms will always
[00:37:11.880 --> 00:37:17.080]   Went out in this now I had invited kevin rose because I knew this was a part of the conversation
[00:37:17.080 --> 00:37:21.640]   We were gonna have i'm sorry that he couldn't make it today because he is very bullish on all of this
[00:37:22.120 --> 00:37:24.120]   And i'm sure that he would have a defense of it
[00:37:24.120 --> 00:37:29.800]   But I have to say and I keep looking into this and I look to people like moxie jack dorsey
[00:37:29.800 --> 00:37:32.280]   there did seem to be a
[00:37:32.280 --> 00:37:35.160]   Common idea that perhaps this isn't
[00:37:35.160 --> 00:37:42.760]   Uh the miracle that we were hoping it would be but then that leaves the question is the internet always going to be just
[00:37:42.760 --> 00:37:44.920]   dominated by big platforms, Amy
[00:37:44.920 --> 00:37:51.720]   I think the internet's always going to be dominated by a handful of people who are hyper connected and make a lot of decisions that
[00:37:51.800 --> 00:37:53.800]   That have that's just the way it is
[00:37:53.800 --> 00:38:00.120]   Um, I heard chris dixon and uh navel on tim ferris podcast that most have aired a couple days ago
[00:38:00.120 --> 00:38:04.120]   They were talking about you know chris dixon is majored in philosophy. I think he's got it
[00:38:04.120 --> 00:38:09.000]   He was on a phd track for for philosophy and he does he's a wonderful writer. He's a great communicator
[00:38:09.000 --> 00:38:14.120]   Listening to the two of them talk, you know, yeah, it's dazzling. It sounds. Well. I should also point out
[00:38:14.120 --> 00:38:21.000]   He's also a partner at a 16 z he's right right which is the primary promoter of web three
[00:38:21.960 --> 00:38:26.120]   Right and that's my point, you know another there is another way to look at this and again I
[00:38:26.120 --> 00:38:33.320]   I believe that our future demands a sense of awe and wonder and I don't mean to sound super jaded
[00:38:33.320 --> 00:38:34.280]   but
[00:38:34.280 --> 00:38:41.480]   Is isn't it possible that this is just an evolution of technology an important one, but it isn't as fundamental right and
[00:38:41.480 --> 00:38:43.320]   and um
[00:38:43.320 --> 00:38:48.040]   And maybe we have an opportunity here to fix some of what wrong the last time to make this
[00:38:48.760 --> 00:38:50.760]   To make things more inclusive
[00:38:50.760 --> 00:38:55.960]   Padre mentioned finance bros, you know, he's right there are women who are in this space, but you never hear from them
[00:38:55.960 --> 00:38:58.440]   There are people of color who are doing great things. You never hear from them
[00:38:58.440 --> 00:39:04.920]   So if the whole point here is you know inclusivity and and decentralization and for the people then it's got to be more than a handful of people
[00:39:04.920 --> 00:39:08.040]   Making all these decisions. What about things like micro lending?
[00:39:08.040 --> 00:39:13.160]   Uh, there are other movements in finance to make it more equitable
[00:39:13.160 --> 00:39:18.280]   Uh, do you see those in as being important in the future?
[00:39:19.240 --> 00:39:25.240]   Well, there's a lot of that happening especially in africa right um a lot of where you don't have a big
[00:39:25.240 --> 00:39:31.640]   entrenched incumbent banking system, so you can have things like empesa
[00:39:31.640 --> 00:39:35.320]   Online payment systems stuff that is very different and new
[00:39:35.320 --> 00:39:37.880]   Um, is that the future?
[00:39:37.880 --> 00:39:41.480]   Well, I think the place where this plays out better. It's regionally first of all
[00:39:41.480 --> 00:39:45.720]   Um, there's a reason that emp has had took off in africa and it probably wouldn't take off in chicago
[00:39:46.280 --> 00:39:50.760]   Um, but de centralization so defi decentralized finance
[00:39:50.760 --> 00:39:56.040]   Countries like china issuing the yuan, which is the digital version of their currency
[00:39:56.040 --> 00:39:57.480]   um
[00:39:57.480 --> 00:40:05.960]   Changing you know banking we do a ton of work and in the banking and private equity and just financial sector and it's so antiquated
[00:40:05.960 --> 00:40:13.720]   Um, there's been no real push to innovate in any significant way. So this definitely changes things and potentially
[00:40:14.280 --> 00:40:20.360]   Levels the playing field a little bit especially when you talk about digital identity and just making it easier for people who are unbanked
[00:40:20.360 --> 00:40:23.880]   To participate in in the economy on the one hand
[00:40:23.880 --> 00:40:30.920]   Um, insurance is another place where we do a lot of work and like the amount of paper forms that most people still have to fill out
[00:40:30.920 --> 00:40:38.120]   It's 2022, you know, like you shouldn't have to write stuff on paper. Um, so or or mortgages. So I think
[00:40:38.120 --> 00:40:39.880]   um
[00:40:39.880 --> 00:40:41.880]   Making things just easier
[00:40:42.440 --> 00:40:46.600]   Giving people better access reducing a lot of middleware if middleware are humans
[00:40:46.600 --> 00:40:50.520]   You shouldn't have to like it's insane to buy a house in this country
[00:40:50.520 --> 00:40:55.320]   It's crazy. Absolutely insane the a number of people that are involved and i'm not trying to put people out of work
[00:40:55.320 --> 00:40:57.400]   But i'm also trying to be just practical
[00:40:57.400 --> 00:40:59.800]   um, so yeah
[00:40:59.800 --> 00:41:01.880]   there in the smart contract space and
[00:41:01.880 --> 00:41:05.480]   Uh finance space and decentralizing space
[00:41:05.480 --> 00:41:12.200]   Um, yeah, it and it also provides opportunities to countries like venezuela whose uh currencies become
[00:41:12.920 --> 00:41:13.400]   Um
[00:41:13.400 --> 00:41:17.240]   super devalued but it's also a potential problem for the united states because
[00:41:17.240 --> 00:41:21.480]   If countries start moving into stablecoins, you know, the dollar doesn't matter as much anymore
[00:41:21.480 --> 00:41:25.000]   So this is there my point is like this is this is more than nft's
[00:41:25.000 --> 00:41:27.240]   And um, hugely more
[00:41:27.240 --> 00:41:33.880]   Yeah, and it's like it's it's important, but it's also like i don't know i get real irritated slash nervous when i hear people
[00:41:33.880 --> 00:41:37.960]   Talking about this new tech utopia of web 3
[00:41:37.960 --> 00:41:41.080]   Um, which is i think misguided
[00:41:41.480 --> 00:41:46.040]   utopia turns into dystopia with the blink of an eye unfortunately sure can
[00:41:46.040 --> 00:41:48.200]   Uh
[00:41:48.200 --> 00:41:52.360]   In 19 the best place to see that yeah is the is the rise of all the altcoins
[00:41:52.360 --> 00:41:56.520]   Every single altcoin says the guy who invested heavily in doge
[00:41:56.520 --> 00:41:59.320]   But i mean that was for fun. I did it out of
[00:41:59.320 --> 00:42:01.720]   I didn't think people were actually going to do it seriously
[00:42:01.720 --> 00:42:03.800]   But i mean
[00:42:03.800 --> 00:42:06.360]   Everything you didn't have to invest you just set up a minor
[00:42:06.840 --> 00:42:13.240]   Yeah, i just said about minor but all of them have come about because people realized there was something wrong with the previous
[00:42:13.240 --> 00:42:19.080]   Cryptocurrency that they were they were bullish on and then they moved to the next one and it turned out. Oh, it's exactly the same
[00:42:19.080 --> 00:42:25.000]   Well, let's move to the next one. It's almost the definition of insanity if we move to enough cryptocurrencies enough
[00:42:25.000 --> 00:42:29.560]   Blockchains that are done with doge is a last one. How about she knew you boo right exactly
[00:42:29.560 --> 00:42:33.720]   Yeah, how about uh if anybody pays attention to soccer, which i don't but um
[00:42:34.440 --> 00:42:37.480]   Lionel messy who's arguably messy coin right?
[00:42:37.480 --> 00:42:43.080]   So he left fc barcelona, which was a huge big deal for parisane germane
[00:42:43.080 --> 00:42:46.840]   Which apparently is not as good of a team again. I totally don't pay attention to soccer
[00:42:46.840 --> 00:42:50.360]   But part of his comp package was in tokens. Yeah
[00:42:50.360 --> 00:42:55.400]   Um, and there are a handful of soccer clubs that are making hundreds of millions of dollars by issuing
[00:42:55.400 --> 00:43:02.280]   fan tokens which are kind of like minority shares of uh of the team but it has real world financial
[00:43:02.600 --> 00:43:06.200]   consequences for the players so it's i mean it's it's dogecoin is stupid
[00:43:06.200 --> 00:43:12.120]   However, all of these altcoins and tokens are having real world financial repercussions
[00:43:12.120 --> 00:43:16.280]   The new mayor of new york city is taking his first three page hikes in bitcoin
[00:43:16.280 --> 00:43:18.920]   Let's not talk about you know
[00:43:18.920 --> 00:43:24.680]   At least once a month at least once a month i get an email a phone call or a visit from someone on the other side of the wall
[00:43:24.680 --> 00:43:29.080]   Who has been pitched something a new company has come to them and said well
[00:43:29.080 --> 00:43:33.000]   We want to create a vatican coin or frances coin. Oh my god
[00:43:33.000 --> 00:43:35.400]   I have to explain it to them
[00:43:35.400 --> 00:43:40.280]   Well, because look if if they can get the catholic church to say this is our coin
[00:43:40.280 --> 00:43:46.440]   It automatically has a pomp and dump potential that's beyond anything you've ever seen. Yeah
[00:43:46.440 --> 00:43:51.080]   And that's what i explained to them. I say look. Yes, it would make a ridiculous amount of money
[00:43:51.080 --> 00:43:53.960]   And it would completely destroy your reputation. So
[00:43:54.760 --> 00:44:02.520]   And the reputation of the church i might have in 1922 100 years ago banks built giant marble edifices
[00:44:02.520 --> 00:44:10.680]   To show that they were financially stable that you could trust them you could put your money in their big old safe in the basement
[00:44:10.680 --> 00:44:17.560]   Here in petaluma that we have three old bank buildings that are vacant or occupied by antique stores
[00:44:17.560 --> 00:44:22.840]   Because that doesn't exist anymore. What do what do financial institutions do today?
[00:44:23.960 --> 00:44:25.960]   To let us know it's safe
[00:44:25.960 --> 00:44:29.000]   I just the other day I had to buy
[00:44:29.000 --> 00:44:33.160]   I helped my daughter buy renter's insurance the apartment building she moved into required it
[00:44:33.160 --> 00:44:36.440]   I went to lemonade, which is an online
[00:44:36.440 --> 00:44:42.840]   Insurance broker in online. It took me three minutes to sign her up for renter's insurance
[00:44:42.840 --> 00:44:45.960]   It scared me
[00:44:45.960 --> 00:44:49.880]   I wanted to go to a big bank with lots of forms
[00:44:49.880 --> 00:44:53.240]   Sometimes it's too easy, right?
[00:44:54.200 --> 00:44:56.200]   again, I think it's the the
[00:44:56.200 --> 00:45:00.040]   expectation versus reality and so a lot of these is
[00:45:00.040 --> 00:45:06.440]   I think the most the the most interesting story in decentralization is it has to do with
[00:45:06.440 --> 00:45:13.400]   a sort of emerging new world disorder things are being disordered, right? And so
[00:45:13.400 --> 00:45:19.000]   That's the scariest time of all the chinese curse may you live in interesting time? Yeah, well
[00:45:19.400 --> 00:45:29.400]   so again this has cognitively we are challenged by this and so trust is absolutely paramount, especially in financial services, however
[00:45:29.400 --> 00:45:37.000]   These institutions are vulnerable to disruption because they've been refusing to change for so long good point
[00:45:37.000 --> 00:45:39.560]   right from
[00:45:39.560 --> 00:45:46.280]   banking to insurance to forgive me father for my sin in saying this too, but religion
[00:45:46.280 --> 00:45:48.600]   you know
[00:45:48.600 --> 00:45:51.720]   You know, this is so I don't know when I go to a church
[00:45:51.720 --> 00:45:56.600]   I like a big steeple a nice big bell and organ. I it's the same thing by the way
[00:45:56.600 --> 00:46:03.880]   It's exactly what churches have done for centuries the bigger the edifice the more you figure. Um, God must be here
[00:46:03.880 --> 00:46:06.280]   Yeah, yeah, but yeah, but
[00:46:06.280 --> 00:46:10.040]   I if I don't move my hand exactly in a certain way
[00:46:10.040 --> 00:46:17.160]   I will get letters of condemnation saying that I've destroyed during the mass, right? It's it's just that's what happens. Yep. Wow
[00:46:17.800 --> 00:46:23.720]   Yeah, I mean you you what you talk about people who want things a certain way and the same way that it has always been quote
[00:46:23.720 --> 00:46:28.440]   Unquote always been be at my institutions. There's a church in san francisco
[00:46:28.440 --> 00:46:32.920]   That I used to go to on christmas eve because they still did the latin mass
[00:46:32.920 --> 00:46:36.680]   And I happen to like it and I don't like this vernacular
[00:46:36.680 --> 00:46:40.120]   This is what is this vernacular that everybody uses?
[00:46:40.120 --> 00:46:45.960]   Uh, I asked uh when when I started to it. I told patrick norton my old friend from the screen savers
[00:46:46.360 --> 00:46:50.360]   Uh that I I didn't want to work for the man anymore. And he said leo let me tell you something
[00:46:50.360 --> 00:46:52.760]   There's always a man
[00:46:52.760 --> 00:46:57.400]   And I think that that was a very valuable lesson. He was absolutely
[00:46:57.400 --> 00:47:03.240]   Right, uh, let's take uh actually before we before we I did want to talk a little bit more about
[00:47:03.240 --> 00:47:04.680]   uh
[00:47:04.680 --> 00:47:06.680]   finance in the in the realm of
[00:47:06.680 --> 00:47:12.280]   Income inequality because one of the things covid has really brought forward. It's a as a problem in this country
[00:47:12.280 --> 00:47:14.440]   I think it's somewhat of a problem globally
[00:47:14.840 --> 00:47:16.840]   Uh a problem of income inequality
[00:47:16.840 --> 00:47:21.160]   Uh the fact that the rich are absolutely getting richer
[00:47:21.160 --> 00:47:28.280]   And you're starting to see people get nervous about companies like amazon and apple reaching three trillion dollars in market value
[00:47:28.280 --> 00:47:32.200]   And and and sensing that there's even though that's been our
[00:47:32.200 --> 00:47:37.160]   You know, uh capitalistic goal that there's something a little bit
[00:47:37.160 --> 00:47:40.120]   Obscene about people becoming that wealthy
[00:47:40.760 --> 00:47:43.880]   Uh, and of course when people are that wealthy and then there are people
[00:47:43.880 --> 00:47:48.360]   As there are in san francisco who are living on the streets because they can't afford housing
[00:47:48.360 --> 00:47:55.640]   You really start to see that inequality. What do what do what does future today institute have to say about that? Is that something you you think about?
[00:47:55.640 --> 00:48:03.640]   We do so, um, well, we look at uh what we would call 11 sources of macro change and wealth distribution is is a big one
[00:48:03.640 --> 00:48:08.280]   So anytime we're looking at the future of any science or tech. We're always looking at it through these different lenses
[00:48:09.000 --> 00:48:15.240]   Um, and yeah, listen, we're we got a problem. Um, it's too big of a stretch between
[00:48:15.240 --> 00:48:20.360]   Um, the the highest and the lowest brackets and it probably didn't help a lot uh that that is getting worse
[00:48:20.360 --> 00:48:25.960]   It's getting worse and it didn't help that jeff bezos was uh, I don't know what he was doing over the holidays
[00:48:25.960 --> 00:48:29.800]   But those those photos if you haven't seen them that he was playing pit pole
[00:48:29.800 --> 00:48:33.160]   He was he was
[00:48:33.160 --> 00:48:36.520]   Uh, i don't know if any of you have watched succession
[00:48:36.600 --> 00:48:42.600]   Um, I don't know if this is willing anything, but it feels very much like bezos is in the realm of uh, kendall right now
[00:48:42.600 --> 00:48:44.200]   Absolutely
[00:48:44.200 --> 00:48:45.480]   Absolutely
[00:48:45.480 --> 00:48:46.520]   So
[00:48:46.520 --> 00:48:53.160]   Listen, the characteristics that tended to build and drive civilizations over long periods of time, um tend to kill us
[00:48:53.160 --> 00:48:55.320]   Uh, you can
[00:48:55.320 --> 00:49:00.280]   There's lots of examples of this and um in part that's because there's uh, the the
[00:49:01.160 --> 00:49:10.040]   You know, we've we create these incentives for what's working to to sort of preserve that with the promise that everybody will will benefit at some point
[00:49:10.040 --> 00:49:10.680]   And
[00:49:10.680 --> 00:49:17.480]   You know, it's it's probably time that we reevaluate whether or not our current systems make sense because
[00:49:17.480 --> 00:49:18.840]   Ultimately
[00:49:18.840 --> 00:49:23.560]   If you wind up with too big of a gap, you don't just have moral and ethical problems
[00:49:23.560 --> 00:49:27.080]   Um, you're we're gonna wind up with with lots of you could
[00:49:27.880 --> 00:49:31.480]   It's unsustainable, which means that democracy is unsist unsustainable
[00:49:31.480 --> 00:49:37.320]   You know, which means that your future happiness if you happen to live in the united states is unsustainable
[00:49:37.320 --> 00:49:38.040]   So
[00:49:38.040 --> 00:49:41.480]   And it's accelerating as of as of the start of october
[00:49:41.480 --> 00:49:43.480]   That's if I can remember this correctly
[00:49:43.480 --> 00:49:49.880]   Those who make over 500 000 a year would be considered in the in the top 1 and the one the top 1 of the united states
[00:49:49.880 --> 00:49:55.960]   Now own 26 27 of all the assets of the united states
[00:49:56.520 --> 00:49:59.880]   The next 60 percent so the middle class own 26.6
[00:49:59.880 --> 00:50:04.440]   So the top 1 percent now own more than the next 60
[00:50:04.440 --> 00:50:11.240]   Percent that is completely completely unsustainable and if you look at the chart, it's actually accelerating
[00:50:11.240 --> 00:50:13.880]   So it feeds on itself as those
[00:50:13.880 --> 00:50:19.480]   Uh those that that wealth is consolidated means that when you go through times of economic depression like we are right now
[00:50:19.480 --> 00:50:26.040]   The only ones who are able to take advantage of it are the ones who have liquid wealth those who can buy distressed assets
[00:50:26.200 --> 00:50:29.640]   So they're going to buy more and more and more and the gap will get more pronounced
[00:50:29.640 --> 00:50:31.560]   eventually
[00:50:31.560 --> 00:50:39.560]   It's societal collapse. I mean if if all of the wealth is concentrated in the top 1 to 3 percent there is nothing left
[00:50:39.560 --> 00:50:46.520]   Yeah, and if you look again like if you extrapolate that out in what other circumstance would we be okay with that concentration?
[00:50:46.520 --> 00:50:51.560]   Like if you're managing your own financial portfolio, you're not going to head like you're not going to just put everything into two stocks
[00:50:52.120 --> 00:50:57.320]   Like the math doesn't work out on that or if you're if you're although if I could buy stock in elon musk and jeff bezos
[00:50:57.320 --> 00:50:58.680]   I might
[00:50:58.680 --> 00:51:02.920]   Yeah, do you hear what elon musk did and at cvs pardrey were you in the tunnel by any chance?
[00:51:02.920 --> 00:51:06.600]   Oh my god, I actively avoided the tunnel
[00:51:06.600 --> 00:51:12.040]   So boring company bored a tunnel under las vegas they put 90 tesla's down there
[00:51:12.040 --> 00:51:15.320]   And apparently there was a massive traffic jam
[00:51:16.440 --> 00:51:22.280]   The tunnel wasn't even big enough for people. I don't I'm not sure how permits got pulled because you couldn't even there wasn't enough egress
[00:51:22.280 --> 00:51:27.480]   Like if if somebody had to get out. Yeah, it would have been a nightmare
[00:51:27.480 --> 00:51:30.440]   Right
[00:51:30.440 --> 00:51:33.000]   There there was somebody put a video up. I can't remember who
[00:51:33.000 --> 00:51:38.280]   Of them. Let me see if this is the this is miklo's tweet
[00:51:38.280 --> 00:51:40.840]   You can see what it's like
[00:51:40.840 --> 00:51:44.440]   This is this should scare the hell out of you
[00:51:44.920 --> 00:51:46.440]   Oh
[00:51:46.440 --> 00:51:51.320]   There's no exits. There's no there's no second tunnel that you could escape to
[00:51:51.320 --> 00:51:55.320]   If anything goes wrong, uh, you are under las vegas
[00:51:55.320 --> 00:51:57.720]   in a
[00:51:57.720 --> 00:51:58.840]   in a
[00:51:58.840 --> 00:52:00.280]   Tesla
[00:52:00.280 --> 00:52:04.840]   And there was a big pile up right there was like some kind of crazy well it was it was just a traffic jam
[00:52:04.840 --> 00:52:06.920]   It wasn't it was there were no accidents
[00:52:06.920 --> 00:52:12.920]   But it sounds I mean I've driven through a tunnel before I that was not really a big draw from me. No
[00:52:13.320 --> 00:52:17.400]   I'm sorry. Yeah, but it does beg the question. Why do we make fun of?
[00:52:17.400 --> 00:52:22.040]   People make fun of the hyperleap loop. They make fun of warring company
[00:52:22.040 --> 00:52:27.240]   Why because it just challenges an existing mental model that we're all really comfortable with just like
[00:52:27.240 --> 00:52:37.480]   Hey, what happens if we distributed wealth slightly differently? I've been working on a digital dividend project that actually relies on web 3 tech
[00:52:37.480 --> 00:52:41.480]   Um, it's just a different model, but but people are very uncomfortable
[00:52:41.960 --> 00:52:47.560]   Having to break their mental models to imagine an alternative or many alternative futures
[00:52:47.560 --> 00:52:51.560]   And we would be in such a better place if we were willing to explore
[00:52:51.560 --> 00:52:56.920]   Something that is slightly outside of what we're used to. I mean as you know humanity
[00:52:56.920 --> 00:52:58.840]   Yeah
[00:52:58.840 --> 00:53:02.200]   um, I do I do wonder if we're we're we're buying into
[00:53:02.200 --> 00:53:07.240]   a vision, uh that we don't necessarily
[00:53:08.760 --> 00:53:11.480]   Share because it looks so I don't know
[00:53:11.480 --> 00:53:18.120]   Sexy you mean the tunnel or the hyperleap. No, I'm talking about jeff bezos is yacht actually here's the
[00:53:18.120 --> 00:53:20.760]   Here's the picture
[00:53:20.760 --> 00:53:22.760]   jeff bezos is pit bull
[00:53:22.760 --> 00:53:28.120]   In the middle of the ocean the the heart the heart the mirror heart sunglasses are wet really
[00:53:28.120 --> 00:53:35.080]   Uh over the like made it over the top. I appreciate that somehow he got super buff and maybe he's using synthetic biology
[00:53:35.080 --> 00:53:37.320]   I don't know so like the pants the shirt
[00:53:37.800 --> 00:53:43.160]   He actually has a uh trainer who lives with him. That's that's how he looks so good
[00:53:43.160 --> 00:53:51.320]   Uh, I mean these people realize that we're getting very close to the let them eat cake part. We are there. I would submit. Yes
[00:53:51.320 --> 00:53:54.280]   This is very much mariente toinette
[00:53:54.280 --> 00:53:57.400]   Celebrating 2022
[00:53:57.400 --> 00:54:02.920]   Yeah, oh wait. Wait. Okay. So leo wait a minute. Wait a minute. This is super important. Can you zoom in on her hand?
[00:54:02.920 --> 00:54:07.320]   Uh, there's some photoshop going on here, isn't there? There's two things
[00:54:07.480 --> 00:54:13.400]   There's photoshop and for eagle eyed people who are used to looking at a lot of celebrity photos like I am that woman's wearing a ring
[00:54:13.400 --> 00:54:19.080]   She's got a ring on that finger. That's what left. Oh, you think they're engaged now. I don't know, but she's got weird
[00:54:19.080 --> 00:54:23.400]   Uh weird tiny baby fingers on top of a giant parent
[00:54:23.400 --> 00:54:28.680]   With a ring there is some photoshop activity of some kind
[00:54:28.680 --> 00:54:36.200]   In fact, it looks like there's too many hands on her butt. So I know there's that's not that's uncanny valley here. I don't know
[00:54:36.840 --> 00:54:40.600]   Gatica that is the piano player in gatica is what that is
[00:54:40.600 --> 00:54:43.880]   Yeah, okay, baby hands
[00:54:43.880 --> 00:54:50.760]   Uh, there's definitely you right now. I'm really perturbed plus you can't unsee that ball is in the pool
[00:54:50.760 --> 00:54:53.160]   I know it doesn't belong in the pool
[00:54:53.160 --> 00:54:56.520]   There's something going on here. All right. Let's take a little break
[00:54:56.520 --> 00:55:02.200]   We were talking about the dystopian future. We didn't even get to the issue the environmental issues
[00:55:02.200 --> 00:55:05.640]   of uh of crypto and uh defy
[00:55:06.520 --> 00:55:12.600]   Although I've seen charts that say well, you should see the environmental cost of the traditional banking system as well
[00:55:12.600 --> 00:55:15.320]   um
[00:55:15.320 --> 00:55:20.920]   I don't I get we'll get to the environment next. It's not even close. We'll get to the environment next
[00:55:20.920 --> 00:55:22.280]   um
[00:55:22.280 --> 00:55:28.200]   And and a lot more deep fakes artificial neurons ai optimized manufacturing
[00:55:28.200 --> 00:55:31.800]   bioprinted food. This is Amy's list
[00:55:34.120 --> 00:55:39.480]   There's also really good news. There's the james web telescope. What a miracle of a technology
[00:55:39.480 --> 00:55:45.640]   I mean that's kind of mind boggling and that in a way in a sense really is is the amazing
[00:55:45.640 --> 00:55:47.720]   disconnect
[00:55:47.720 --> 00:55:48.680]   uh
[00:55:48.680 --> 00:55:54.840]   About living in the future. We are living in an amazing future in so many ways and a dystopian future in so many other ways
[00:55:54.840 --> 00:55:57.320]   And it's it's a little it's difficult
[00:55:57.320 --> 00:56:02.280]   To say the least let's take a little bit of a break. You know what I like about the future
[00:56:03.000 --> 00:56:08.440]   Linux, that's what I like about the future and if you want to run a server don't run it yourself
[00:56:08.440 --> 00:56:15.000]   Don't run that's what moxie said. Nobody wants to run their own server. You need linode whether you're developing a personal project
[00:56:15.000 --> 00:56:18.440]   Managing larger workloads you deserve simple
[00:56:18.440 --> 00:56:21.480]   affordable accessible cloud computing
[00:56:21.480 --> 00:56:29.320]   Solutions and they are on point they're amazing with linode you can build applications using a simple cloud manager
[00:56:29.720 --> 00:56:33.400]   The api you can even do command line if you're totally tough
[00:56:33.400 --> 00:56:40.680]   Totally a boss quickly scale up or down with standard vms dedicated cpus and enterprise grade gpu's
[00:56:40.680 --> 00:56:45.640]   Linodes come a long way. Baby. I started using linode years ago when they first started
[00:56:45.640 --> 00:56:50.920]   Still love it. Still have my linode account. You'll get a support experience way above the rest
[00:56:50.920 --> 00:56:54.200]   People choose linode. Yeah, by the way, I'm saying it night
[00:56:55.160 --> 00:56:59.800]   L-i-n-o-d-e you know what i'm talking about right l-i-n-o-d-e
[00:56:59.800 --> 00:57:06.280]   They love it because of course the customer support experience is great by geeks for geeks
[00:57:06.280 --> 00:57:13.800]   Linodes independence and mission drive them to a different standard where the customers the driving force behind everything they do
[00:57:13.800 --> 00:57:20.040]   You're gonna love the pricing pay as you go. It's predictable. It's transparent. There's never any surprises
[00:57:20.440 --> 00:57:25.480]   I think they pioneered this actually the predictable flat pricing model for cloud computing
[00:57:25.480 --> 00:57:31.880]   And you go and you look at some of these other guys and you don't know what you're going to be paying because there's so many different tiers and
[00:57:31.880 --> 00:57:34.840]   Not at linode. No more anxiety over hidden costs
[00:57:34.840 --> 00:57:38.920]   They make it as simple as it can be to launch and scale in the cloud
[00:57:38.920 --> 00:57:44.680]   With linode you'll get flat pricing across every global data center. Yeah, by the way, you know
[00:57:44.680 --> 00:57:49.800]   They are fully global. They are fully you know
[00:57:50.440 --> 00:57:51.960]   State of the art
[00:57:51.960 --> 00:57:58.200]   I just I just I am so thrilled with how well linode has done. They've got a great intuitive cloud manager
[00:57:58.200 --> 00:58:00.280]   They've got the best api in the business
[00:58:00.280 --> 00:58:03.240]   Documentation is incredible
[00:58:03.240 --> 00:58:07.080]   All you have to do if you want to know how to do something in linode just google
[00:58:07.080 --> 00:58:12.040]   How do you do that in linode and there's pages and pages of there's youtube videos
[00:58:12.040 --> 00:58:15.320]   And of course, there's always that award-winning support to back it up
[00:58:15.320 --> 00:58:18.760]   Linode makes it easy to manage your applications in the cloud
[00:58:19.240 --> 00:58:22.280]   And their infrastructure has just gone
[00:58:22.280 --> 00:58:24.920]   full bore they are
[00:58:24.920 --> 00:58:28.280]   reliable enterprise grade infrastructure 11 data centers
[00:58:28.280 --> 00:58:36.120]   Worldwide they've got all the peering relationships you'd expect they've been around forever their next generation network linode
[00:58:36.120 --> 00:58:42.440]   Uh delivers the most modern infrastructure and performance you need any can innovate at scale and at the price you'll love
[00:58:42.440 --> 00:58:48.840]   Whatever you want to do host a website build an app store or backup media launch and enrich your developer applications
[00:58:48.840 --> 00:58:54.200]   Your hosted services your websites ai machine learning gaming services
[00:58:54.200 --> 00:59:00.440]   You need a custom cicd environment put it on linode launch and scale in the cloud where they're virtual machines
[00:59:00.440 --> 00:59:02.920]   You could choose shared and dedicated compute instances
[00:59:02.920 --> 00:59:11.000]   And we're going to even give you a hundred dollars in credit you could use on s3 compatible object storage or manage kubernetes or more
[00:59:11.000 --> 00:59:16.360]   g2 crowd in 2021 rated linode the easiest to use
[00:59:17.880 --> 00:59:21.720]   cloud developers love linode they choose it because they can make managing
[00:59:21.720 --> 00:59:28.520]   complex cloud infrastructure easy with simple bundled pricing a full featured api and a hundred percent human
[00:59:28.520 --> 00:59:30.600]   support
[00:59:30.600 --> 00:59:35.000]   Linode pioneered cloud computing back in 2003. That's when I first signed up with them
[00:59:35.000 --> 00:59:41.080]   That was three years before aws, but I got to point out. They're not the old guys
[00:59:41.080 --> 00:59:47.800]   They have really kept up there are state of the art develop deploy and scale your modern applications faster and easier
[00:59:47.800 --> 00:59:51.480]   They're the best and we're going to get you started with a hundred dollar credit
[00:59:51.480 --> 00:59:56.120]   When you go to linode.com/twit l-i-n-o-d
[00:59:56.120 --> 01:00:00.440]   No gotchas no surprises. They're the best linode
[01:00:00.440 --> 01:00:03.320]   dot com slash
[01:00:03.320 --> 01:00:07.080]   Twit highly highly recommended really glad to have them on the
[01:00:07.080 --> 01:00:09.640]   show
[01:00:09.640 --> 01:00:13.400]   Amy you made a great list of the future what else should we uh
[01:00:13.400 --> 01:00:16.120]   Talk about in this new world disorder
[01:00:17.320 --> 01:00:22.520]   We you talked about space and I think space is excited. Let's do a let's do a good thing
[01:00:22.520 --> 01:00:24.680]   Yeah, let's do something we could be positive about
[01:00:24.680 --> 01:00:30.760]   Right, so space web the the the no relation James web telescope. Yes
[01:00:30.760 --> 01:00:34.760]   Um, so yes, he's not uncle James to you
[01:00:34.760 --> 01:00:37.800]   jimmy telescope. No
[01:00:37.800 --> 01:00:43.080]   No, um, I actually remember seeing a replica of it at at south by
[01:00:43.080 --> 01:00:46.200]   Probably 10 years ago or something. They've been working on this
[01:00:47.160 --> 01:00:49.160]   uh, john showed me
[01:00:49.160 --> 01:00:56.040]   A picture. Yeah, he that what I was 20 years ago. John that picture of them recruiting at cal state
[01:00:56.040 --> 01:00:58.280]   la
[01:00:58.280 --> 01:01:05.400]   Big sign come join us so we you know nasa join nasa you can help us build the james web space telescope
[01:01:05.400 --> 01:01:13.480]   So this is 25 years. They've been working on this 25 years. They were part of the problem was that they were kind of inventing the technology as they were going
[01:01:15.320 --> 01:01:20.280]   It is difficult to underscore why this is such a big deal and um, you know
[01:01:20.280 --> 01:01:27.640]   Today cosmology I took two cosmology classes in college and I got so overwhelmed
[01:01:27.640 --> 01:01:31.240]   I actually had a panic attack. It was one of the first panic attacks. I had I you know
[01:01:31.240 --> 01:01:40.280]   It's space is big. It's really I passed out. Yep, and and I had an amazing professor who I loved but he was going through all the math and it
[01:01:42.600 --> 01:01:49.720]   Realizing how big the known universe was and where our place was in it. Uh, I got so overwhelmed that I I completely blacked out
[01:01:49.720 --> 01:01:55.160]   And it turns out I was not the first person to pass out in one of his how interesting wow
[01:01:55.160 --> 01:02:03.560]   But um, so cosmologists think that the universe is about 18 13.8 billion years old and what the james web telescope
[01:02:03.560 --> 01:02:05.880]   Um is going to help us
[01:02:05.880 --> 01:02:11.320]   Understand is what happened? I think what happened or about a hundred million years
[01:02:12.040 --> 01:02:16.120]   Basically after the big bang. Yeah, just just a couple hundred million years afterwards
[01:02:16.120 --> 01:02:21.000]   So, I mean all these where did light come from? Yeah, where did energy come from?
[01:02:21.000 --> 01:02:26.360]   Um, my husband is obsessed my husband brian is obsessed with this and every night
[01:02:26.360 --> 01:02:32.120]   He checks that he's like obsessively checking. There's a great website that they made so you can track exactly. This is it
[01:02:32.120 --> 01:02:36.520]   This is where is web. Yeah. Yeah. Yeah, that's a great website
[01:02:37.080 --> 01:02:42.680]   Every night. He like gives me a download before we're like we're laying in bed and we're falling asleep and I start getting all
[01:02:42.680 --> 01:02:46.200]   Like sick to my stomach. I'm thinking about the possibilities
[01:02:46.200 --> 01:02:53.000]   Okay, we are now on reaction here now 75 to Lagrange to
[01:02:53.000 --> 01:02:56.520]   Uh, we have but so the good news is fully deployed
[01:02:56.520 --> 01:03:02.760]   Uh, there are still I think 47 checkpoints left but hundreds have been completed without error
[01:03:02.760 --> 01:03:06.440]   Which is really was the real concern. I mean this this deployment
[01:03:06.920 --> 01:03:14.600]   Was incredibly complex. It was very incredibly complex, but they've got the sun shield open fully in fully tension
[01:03:14.600 --> 01:03:17.000]   Yeah, that's open
[01:03:17.000 --> 01:03:22.840]   So one of the cool tech parts of this technology if you look again, it'll tell you what temperature it is on different sides
[01:03:22.840 --> 01:03:27.640]   So on the side that's facing the sun. It's like it's like
[01:03:27.640 --> 01:03:33.560]   Very very very hot. Yeah, it shows it right there. It's 131 degrees and on the side that shield did
[01:03:34.120 --> 01:03:37.000]   It's like it's like calvin like to like
[01:03:37.000 --> 01:03:46.120]   It's like super super cold and so the physics of getting those two temperature states to exist on the same device at the same time without it cracking
[01:03:46.120 --> 01:03:50.200]   I mean it could just completely mind blowing the technology that went into building this thing
[01:03:50.200 --> 01:03:53.400]   Yeah, it bends my mind. I can't think about it. I don't even want to talk about it
[01:03:53.400 --> 01:03:57.400]   I mean I want it and what I think is also mind-bending is the information we're going to gain
[01:03:57.400 --> 01:04:02.120]   Not just looking back into the distant past of the galaxy or the universe
[01:04:02.600 --> 01:04:04.920]   but also looking for exoplanets
[01:04:04.920 --> 01:04:08.280]   uh, you know, it was only about two 20 years ago that we've
[01:04:08.280 --> 01:04:16.040]   Finally proved that there were planets outside of our solar system now. We're able to look for rocky planets a planet
[01:04:16.040 --> 01:04:18.040]   You could stand on that might have air
[01:04:18.040 --> 01:04:23.400]   Uh, this is right. That's why that's why the the near and mid infrared
[01:04:23.400 --> 01:04:30.760]   Instruments are so important on the james web because the really really old objects that are moving away at incredible speeds
[01:04:31.080 --> 01:04:34.200]   They're all redshifted to an incredible degree. You cannot see them
[01:04:34.200 --> 01:04:39.560]   This thing is what a hundred times more sensitive than any instrument that we could we've ever had in that range
[01:04:39.560 --> 01:04:43.800]   Which means that you can you can detect rocky planets. You can detect dust rings
[01:04:43.800 --> 01:04:49.160]   You can detect things that you know, we don't even know what's going to be there yet
[01:04:49.160 --> 01:04:55.960]   That's the part for me. That's so exciting. Yes. Hubble was able to to catalog an amazing amount
[01:04:56.440 --> 01:05:04.120]   of of of objects in the cosmos what web can give us is web can give us a different understanding of what actually can exist
[01:05:04.120 --> 01:05:05.960]   in the cosmos
[01:05:05.960 --> 01:05:07.960]   um, which I mean
[01:05:07.960 --> 01:05:10.920]   Again, I'm with Amy on this it really boggles the mind
[01:05:10.920 --> 01:05:16.360]   When you start to think of of the types of knowledge that this can open to humankind
[01:05:16.360 --> 01:05:19.080]   I love it that your cosmology class made you faint
[01:05:19.080 --> 01:05:22.280]   I did
[01:05:22.280 --> 01:05:26.280]   That's another small p. I had a huge crush on my professor
[01:05:26.920 --> 01:05:28.920]   Okay, and uh
[01:05:28.920 --> 01:05:31.480]   Already elevated
[01:05:31.480 --> 01:05:36.760]   I was like I was like super humiliated that I I mean I blacked out drooled fell off of my
[01:05:36.760 --> 01:05:43.080]   Are you kidding? He loved it. Are you kidding? That is such an affirmation of his life choices
[01:05:43.080 --> 01:05:45.160]   I
[01:05:45.160 --> 01:05:50.040]   I did talk to him afterwards and I was like he was like listen if cosmology is like something you were thinking about
[01:05:50.040 --> 01:05:50.600]   I
[01:05:50.600 --> 01:05:53.720]   Yeah, I don't think it's gonna be for you. It's like if you faint when you see blood
[01:05:53.720 --> 01:05:58.360]   You don't want to be a surgeon if you faint when you see the vastness of the cosmos
[01:05:58.360 --> 01:06:02.280]   You probably don't want to be a cosmology. I I will tell you it um
[01:06:02.280 --> 01:06:08.680]   This is part of the reason why I love the expanse so much so the the book series
[01:06:08.680 --> 01:06:11.000]   Um, and also the show is
[01:06:11.000 --> 01:06:16.600]   Is so the physics are right like the science is very good, but there's something profound about
[01:06:16.600 --> 01:06:21.240]   You know where where we are and what are places?
[01:06:21.800 --> 01:06:27.000]   And I know people are making fun of Bezos for his phallic shaped ridiculously looking like well
[01:06:27.000 --> 01:06:32.120]   That's really I mean honestly the race you know like from the from the of the billionaires to get to
[01:06:32.120 --> 01:06:35.160]   Uh, you know, right so I I get that I totally get it
[01:06:35.160 --> 01:06:41.080]   I know that there's like a billionaire cowboy race right now to send it's like it's like that most hyper masculine thing that can happen
[01:06:41.080 --> 01:06:46.840]   Right, I'm gonna send I'm gonna send my giant penis shaped thing up into space faster than your giant penis shaped thing
[01:06:46.840 --> 01:06:50.760]   um, but here's why I think all of this matters I
[01:06:51.720 --> 01:06:55.080]   I am privileged we are privileged to be
[01:06:55.080 --> 01:06:59.880]   That at the origin of humanity's next story
[01:06:59.880 --> 01:07:05.320]   I mean 500 years from now people are going to look back at this moment in time as the as the time that this started
[01:07:05.320 --> 01:07:09.000]   Six months from now as my husband says, you know
[01:07:09.000 --> 01:07:16.920]   Our reality is going to be different because the the James web that that telescope is going to start sending back data that is going to literally
[01:07:16.920 --> 01:07:19.080]   change our understanding of
[01:07:19.320 --> 01:07:25.240]   Reality as we start spending you know, there are multiple projects going up to space
[01:07:25.240 --> 01:07:28.440]   The ISS only has whatever a couple years of funding left on it
[01:07:28.440 --> 01:07:30.040]   So there's gonna have to be
[01:07:30.040 --> 01:07:34.920]   Either more funding or something else in geopolitically. I think more funding is going to be rough going forward
[01:07:34.920 --> 01:07:37.640]   So amazon among many other
[01:07:37.640 --> 01:07:41.320]   Not amazon blue origin among many other companies are building private
[01:07:41.320 --> 01:07:44.360]   um space stations and we are
[01:07:44.360 --> 01:07:47.800]   Making it easier to get back and forth. I mean this is
[01:07:48.200 --> 01:07:50.200]   If you've watched the expanse
[01:07:50.200 --> 01:07:56.200]   We're this is the beginning. It's us. We're the ones alive today and if you you know to wrap your head around that
[01:07:56.200 --> 01:08:02.440]   Um, you know, it's so completely profound. I think it makes me again
[01:08:02.440 --> 01:08:05.960]   I feel the sense of great sense of awe and nausea
[01:08:05.960 --> 01:08:11.080]   You know at the same time. Can I bring you down to earth so in the watchmen and speaking of
[01:08:11.080 --> 01:08:13.720]   Yeah, comic books and movies. One of my favorites
[01:08:14.440 --> 01:08:20.360]   No, you consider it one of the great works of fiction. I know you've told you've told me that and it's why I read it because of you
[01:08:20.360 --> 01:08:26.840]   Ozmandy is in order to bring the world together because it's about to blow itself up with a nuclear war
[01:08:26.840 --> 01:08:32.440]   fakes an invasion of uh alien octopus on New York city
[01:08:32.440 --> 01:08:38.200]   And it works the world gets together. I feel like we're though we are at that
[01:08:38.760 --> 01:08:44.760]   Critical junction where we are about to destroy our world and about to destroy ourselves. Do you think?
[01:08:44.760 --> 01:08:49.640]   I know there's no Ozmandy as to launch a mechanical octopus at us
[01:08:49.640 --> 01:08:51.880]   But do you think maybe james web?
[01:08:51.880 --> 01:08:57.960]   Will give us some perspective enough perspective to start thinking more because we can't just say oh, you know
[01:08:57.960 --> 01:09:01.640]   We're gonna go to mars is fine. Let destroy the planet. We can't do that
[01:09:01.640 --> 01:09:04.360]   Yeah, so
[01:09:04.360 --> 01:09:06.120]   and also
[01:09:06.120 --> 01:09:12.040]   My thinking on getting to mars is like we could just change our biochemistry through synthetic biology and create
[01:09:12.040 --> 01:09:14.520]   Oh, you are an optimist. This is scary
[01:09:14.520 --> 01:09:17.160]   So anyhow, there's that um
[01:09:17.160 --> 01:09:19.560]   Listen, I love the watchmen
[01:09:19.560 --> 01:09:23.960]   If they don't resolve this proton molecule
[01:09:23.960 --> 01:09:27.800]   Situation in season six, I might be real pissed off
[01:09:27.800 --> 01:09:30.280]   Uh, okay
[01:09:30.280 --> 01:09:34.840]   I'm probably not going to because they're gonna be going to move. I know because it's not right right. I know I know
[01:09:35.240 --> 01:09:39.080]   Um, it would be nice if they just flicked at it. I more than the stupid began
[01:09:39.080 --> 01:09:41.080]   We're gonna spoil it. So I always stop talking right right?
[01:09:41.080 --> 01:09:47.240]   I actually do like that though because that is an example of yeah, it's the best of times. It's the worst of times
[01:09:47.240 --> 01:09:50.600]   Uh, it's it is what really what life is
[01:09:50.600 --> 01:09:53.480]   Right, but you were asking about
[01:09:53.480 --> 01:09:58.520]   What is it gonna take for us to to come together and fix to come together? So
[01:09:58.520 --> 01:10:04.040]   You know, I really I don't the reason why I love the watchmen is because I there's a part of me that identifies with dr
[01:10:04.040 --> 01:10:07.320]   Manhattan is also a part of me that identifies with dr
[01:10:07.320 --> 01:10:11.240]   Strange, which I guess makes sense if you're like in the marvel universe because of uh foresight
[01:10:11.240 --> 01:10:17.720]   But dr Manhattan is um, you know, he almost gives up at some point, right? Because yeah, he does give up
[01:10:17.720 --> 01:10:21.400]   He moves to the moon or something. Yeah, right? This could because you you know
[01:10:21.400 --> 01:10:25.160]   We we we have multiple times dealt with
[01:10:25.160 --> 01:10:31.640]   Um, like we're we're living through a catastrophic scenario right now. COVID is a catastrophic scenario that you would have thought
[01:10:32.440 --> 01:10:34.440]   Would have we should have known better
[01:10:34.440 --> 01:10:37.160]   We should be better prepared
[01:10:37.160 --> 01:10:42.120]   Right. The outcome has not been kumbaya. We're all gonna get together and help each other out. The outcome has been
[01:10:42.120 --> 01:10:47.960]   Um politics and school boards like we're inventing new ways to classify ourselves up. Yep
[01:10:47.960 --> 01:10:52.680]   Right. So I would actually be interested to hear the pajres thoughts on
[01:10:52.680 --> 01:10:56.600]   When we start getting data back from from the telescope
[01:10:56.600 --> 01:11:00.040]   Do you think that's gonna shift people's perspectives?
[01:11:00.600 --> 01:11:05.400]   You know, just do they do you think that they will feel a greater sense of something anything?
[01:11:05.400 --> 01:11:10.920]   I would like them to I think there's going to be people like yourself and like myself and like leo who
[01:11:10.920 --> 01:11:14.520]   Will invite people to see the beauty in the data that's coming back
[01:11:14.520 --> 01:11:18.680]   In the knowledge that we now have of the cosmos that we did not have before
[01:11:18.680 --> 01:11:22.600]   Um, I think there's going to be some phenomenal changes to our understanding of
[01:11:22.600 --> 01:11:26.760]   What we can and cannot reach there's going to be some theorems about
[01:11:27.240 --> 01:11:32.360]   Uh, you know, how how the how the universe is expanding that is just going it's going to blow our minds
[01:11:32.360 --> 01:11:38.760]   And it could actually make us appreciate more the little ball of dirt and water that we're living on right now
[01:11:38.760 --> 01:11:41.320]   But unfortunately, um
[01:11:41.320 --> 01:11:45.320]   We have an unresolved scenario in mankind not just the united states
[01:11:45.320 --> 01:11:49.000]   But on around the planet of a fundamental distrust
[01:11:49.000 --> 01:11:51.720]   of basically everything
[01:11:51.720 --> 01:11:55.560]   Um, and I don't see a way to fix that yet
[01:11:55.720 --> 01:11:58.280]   And it's not going to be fixed with the james web telescope
[01:11:58.280 --> 01:12:00.440]   It it can inspire
[01:12:00.440 --> 01:12:04.040]   But as many people as are going to be inspired by the data coming back
[01:12:04.040 --> 01:12:08.760]   They're going to be those who think that either it's of no consequence or
[01:12:08.760 --> 01:12:13.400]   And I I would not have said this five years ago. We're going to believe that all of this is just fake
[01:12:13.400 --> 01:12:16.200]   I used to laugh at that thinking
[01:12:16.200 --> 01:12:22.600]   Yeah, but but that's real now. I mean we actually have to live with a large percentage of the population of the planet
[01:12:23.080 --> 01:12:26.200]   Who will not believe anything that they cannot touch
[01:12:26.200 --> 01:12:33.400]   And for them space is the ultimate thing that you cannot touch. This is no longer not believing in the moon landing
[01:12:33.400 --> 01:12:38.520]   This is not believing in data that could if you actually looked at it changed how you
[01:12:38.520 --> 01:12:40.280]   believe
[01:12:40.280 --> 01:12:42.280]   your place in the cosmos is
[01:12:42.280 --> 01:12:43.720]   uh, and
[01:12:43.720 --> 01:12:49.320]   I mean as as as much as an optimist as I am by profession and by nature
[01:12:50.120 --> 01:12:53.160]   I am not optimistic about those people changing their minds
[01:12:53.160 --> 01:13:00.600]   Yeah, that's that's why I get irritated when I listen to people like chrystixen or I listen to
[01:13:00.600 --> 01:13:04.120]   You know, basically in every conversation. We're talking about
[01:13:04.120 --> 01:13:10.040]   decentralization d phi AI, you know pick your favorite acronym chrisper
[01:13:10.040 --> 01:13:18.920]   Um, you know, we we wind up in these binaries, right? So you've got utopias and you got dystopias and by and large
[01:13:19.560 --> 01:13:20.520]   um
[01:13:20.520 --> 01:13:22.280]   In the in the 18th and 19th
[01:13:22.280 --> 01:13:29.240]   18th century all of those proto futurists and sci-fi writers. It was very everything was about utopian societies
[01:13:29.240 --> 01:13:35.640]   Um, even eugenics, which is something they wrote quite a bit about was all was all always about utopias
[01:13:35.640 --> 01:13:40.440]   And then we flipped in the 21st century 20th century. It was dystopia
[01:13:40.440 --> 01:13:45.240]   So AI equals robot overlords taking our jobs and then murdering us in our sleep
[01:13:45.240 --> 01:13:48.040]   You know, um environmental collapse
[01:13:48.760 --> 01:13:53.640]   And we've got a shift away from binaries and that shift cannot be into disbelief, right?
[01:13:53.640 --> 01:13:58.920]   Which is exactly what you're talking about. It's like it's like we either have utopia or dystopia or
[01:13:58.920 --> 01:14:04.760]   the middle ground is misinformation and and like that's an incredibly short-sighted
[01:14:04.760 --> 01:14:07.240]   Not to mention dangerous viewpoint
[01:14:07.240 --> 01:14:11.400]   And and I think again, we're just we're shutting ourselves off to uncertainty
[01:14:11.400 --> 01:14:16.760]   And discovery and we have to make some room for that. It's but but I think life is
[01:14:17.160 --> 01:14:20.280]   Gray life is not black or white. Yeah, and I think this is
[01:14:20.280 --> 01:14:26.440]   Some like an american characteristics. I don't see this happening in other countries, especially not in places like japan
[01:14:26.440 --> 01:14:29.800]   You know, we need to have certainty. Otherwise, somehow we're threatened
[01:14:29.800 --> 01:14:35.560]   um and and the people that i've always found to be the most generative and
[01:14:35.560 --> 01:14:42.760]   Humble but also productive and the people who matter most in society are those who are are okay with uncertainty
[01:14:42.760 --> 01:14:45.320]   Um, I agree. You know, and they agree
[01:14:45.320 --> 01:14:49.480]   And they're they're what willing to investigate and change their their thinking over time
[01:14:49.480 --> 01:14:57.000]   I'm going to bring up a very sensitive and difficult subject and robert. This is in no way an attack on you or your faith
[01:14:57.000 --> 01:15:00.760]   But I do think that
[01:15:00.760 --> 01:15:02.200]   Hyper
[01:15:02.200 --> 01:15:08.360]   extreme religiosity in this world not just this country but in this world has become an issue
[01:15:08.360 --> 01:15:10.360]   and
[01:15:10.360 --> 01:15:14.440]   Yes, and uh, and that in fact that kind of reactionary
[01:15:15.160 --> 01:15:18.200]   Thinking is problematic, but i'll give you a chance to
[01:15:18.200 --> 01:15:23.000]   Convince me. I'm glad you brought that up because there is
[01:15:23.000 --> 01:15:27.400]   There is something that we talk about a lot in in spirituality
[01:15:27.400 --> 01:15:31.160]   Um that I think actually can help the conversation
[01:15:31.160 --> 01:15:35.160]   When you actually have a a spirituality that is healthy
[01:15:35.160 --> 01:15:43.240]   When you have a spirituality that leads you to be a better person rather than a more cynical or just mad person
[01:15:44.040 --> 01:15:48.920]   You have a spirituality that opens you up to the potential of being fooled now now
[01:15:48.920 --> 01:15:52.680]   Let me explain what that means. It doesn't mean. Oh, yes. I'm going to believe everything
[01:15:52.680 --> 01:15:56.120]   It means that you are willing to make leaps of faith
[01:15:56.120 --> 01:16:00.520]   That may prove to be wrong and you have to accept that you may be wrong
[01:16:00.520 --> 01:16:03.160]   aren't we taught though that that that faith is what
[01:16:03.160 --> 01:16:07.480]   Makes us all possible that you have to make a leap of faith
[01:16:07.480 --> 01:16:12.680]   No, no, that's that's that's the other side the other side of of the faith cookie
[01:16:13.240 --> 01:16:17.640]   Is those who see faith as something that says I will never be fooled
[01:16:17.640 --> 01:16:20.200]   You will never pull the wool over my eyes
[01:16:20.200 --> 01:16:24.280]   Therefore, I will never have to make a leap of faith because I already know everything
[01:16:24.280 --> 01:16:28.920]   That's the side of faith that is so dark and damaging and unfortunately
[01:16:28.920 --> 01:16:33.000]   That's that's what they knew was talking about that's that hyper partisiveness of
[01:16:33.000 --> 01:16:35.800]   I'm not going to believe you because I will never be fooled
[01:16:35.800 --> 01:16:43.080]   If you are not willing to make that faith leap if you're not willing to be fooled some of the time and to later on admit
[01:16:43.080 --> 01:16:44.680]   When you've been fooled
[01:16:44.680 --> 01:16:48.200]   Then you will never expand your understanding of the world in which you live
[01:16:48.200 --> 01:16:51.720]   You will always be your nice cozy little cover. Yeah
[01:16:51.720 --> 01:16:55.400]   This was my big big problem with the reboot of the cosmos
[01:16:55.400 --> 01:16:57.080]   um
[01:16:57.080 --> 01:16:59.400]   with Neil deGrasse Tyson, but was started by
[01:16:59.400 --> 01:17:07.720]   Help me out the family guy creator seff seff McFarland. Yeah, not farland. Right. My my big problem
[01:17:07.720 --> 01:17:12.440]   Wait a minute. Seth McFarland did the reboot. Oh, yeah. I didn't know that
[01:17:12.680 --> 01:17:16.280]   He like single-handedly funded it really. Oh, yeah. Yeah. Wow
[01:17:16.280 --> 01:17:20.600]   And my big problem with that was that it wasn't I mean
[01:17:20.600 --> 01:17:26.680]   And let's you know this this came out at the height of the sort of Trump era when when there was a lot of disparagement of science
[01:17:26.680 --> 01:17:31.800]   Um, but the problem was that there was so much so much overt
[01:17:31.800 --> 01:17:33.560]   um
[01:17:33.560 --> 01:17:34.920]   atheism
[01:17:34.920 --> 01:17:39.320]   Almost sort of as its own form of religion like a religious
[01:17:40.040 --> 01:17:46.200]   Adherence to atheism that that also closes off the door to curiosity and exploration
[01:17:46.200 --> 01:17:49.560]   It makes no sense. Right and and it
[01:17:49.560 --> 01:17:55.640]   In some ways diminishes the whole point of the point of the you know exploring the cosmos is to
[01:17:55.640 --> 01:18:02.600]   to gain a better understanding and and to expand how we think then shutting you know sort of
[01:18:03.640 --> 01:18:10.440]   forcing everybody to shut off their their curiosity to me is just as bad as a hyper religious person
[01:18:10.440 --> 01:18:16.760]   Um, you know telling everybody that they're wrong if they're not true believers. Yeah, none of that makes any sense
[01:18:16.760 --> 01:18:22.040]   It's never made any sense. But dogmatism of any kind is a dead end. Yeah. Yeah, let's let's say that
[01:18:22.040 --> 01:18:24.840]   um
[01:18:24.840 --> 01:18:26.840]   Although I'll stand up for atheism
[01:18:26.840 --> 01:18:32.440]   Atheist but listen, I'm uh, I mean I think people who have been listening to the show for a long time
[01:18:32.440 --> 01:18:35.640]   I'm you know, I'm jewish right I was raised um
[01:18:35.640 --> 01:18:38.200]   Jewish I it's uh
[01:18:38.200 --> 01:18:40.200]   There's a horrible thing I could say about
[01:18:40.200 --> 01:18:44.360]   How other people would qualify me as being jewish? I'm but I'm not gonna do that because that's very not pc
[01:18:44.360 --> 01:18:50.920]   Um, but I I don't I've never really felt a belief, but it doesn't mean that i'm closed off to be
[01:18:50.920 --> 01:18:56.920]   exploring you know what I mean? Um, and I don't I don't know but like how how
[01:18:58.280 --> 01:19:04.600]   It's the certainty that bothers me the certainty that it's it's you either are a true believer or you are a true denier
[01:19:04.600 --> 01:19:12.920]   And I feel like especially because of what I do for a living. We should always be leaning into alternative possibilities. Yeah, well, I I'll accept fully that
[01:19:12.920 --> 01:19:14.920]   I don't know
[01:19:14.920 --> 01:19:20.120]   Uh, you know, that's that's kind of absolute. I mean the more you know the more you realize the less you know
[01:19:20.120 --> 01:19:23.640]   but um at the same time I do feel like
[01:19:23.640 --> 01:19:27.000]   uh
[01:19:27.000 --> 01:19:30.520]   Religious faith has started to become a problem in this world
[01:19:30.520 --> 01:19:34.840]   And I I think actually the way you describe and I don't mean merely a christian faith
[01:19:34.840 --> 01:19:41.560]   uh religious extremism of all kinds is a problem and religious extremism is a problem in Judaism. I mean there's
[01:19:41.560 --> 01:19:44.360]   That's why i'm saying it's not
[01:19:44.360 --> 01:19:53.160]   Exclusion. Yeah, no, I mean there's super orthodox super ultra orthodox jews that don't believe in vaccinations and have been attending like 10 000 person funerals
[01:19:53.160 --> 01:19:55.160]   I mean like how irresponsible could you be?
[01:19:55.720 --> 01:19:59.560]   You know, so it's it's a problem. It's a problem because
[01:19:59.560 --> 01:20:06.360]   When you don't leave when when you adhere to certainty and you leave no room for uncertainty
[01:20:06.360 --> 01:20:10.280]   Then you are going to have problems because what would it take for?
[01:20:10.280 --> 01:20:14.680]   You'd your certainty to be absolute you would literally have to be dr
[01:20:14.680 --> 01:20:15.400]   Manhattan
[01:20:15.400 --> 01:20:18.120]   You would have to have unlit and you would have to be a supercomputer
[01:20:18.120 --> 01:20:23.800]   You'd have to have complete comprehensive knowledge of all data all the time as they are unfolding in real time
[01:20:24.120 --> 01:20:30.040]   And you would have to have a superhuman capability of of computing all of those data all of the time in order to be certain
[01:20:30.040 --> 01:20:31.880]   you know, we
[01:20:31.880 --> 01:20:37.480]   The humble thing to do is to recognize that we ourselves can never have access to all data all of the time
[01:20:37.480 --> 01:20:41.960]   And there's no way to compute all of those data anyways because things are continuously changing
[01:20:41.960 --> 01:20:44.760]   Right math is the logic of certainty
[01:20:44.760 --> 01:20:47.160]   Statistics is the logic of uncertainty
[01:20:47.160 --> 01:20:53.880]   And if and if you think about life in terms of statistical probabilities rather than mathematical absolutes you you get through life
[01:20:53.880 --> 01:20:57.000]   A lot easier. Yeah, I think humility is a good word
[01:20:57.000 --> 01:21:00.600]   I think in this
[01:21:00.600 --> 01:21:07.480]   Uh, let's take a little break you guys are so good and uh, we didn't even get to CES. Can we do one segment on CES?
[01:21:07.480 --> 01:21:13.800]   Sure, that sounds fun. We know we can cover like the two or three things that were actually fun from CES
[01:21:13.800 --> 01:21:19.640]   Uh, but I still want to I still I love these deep conversations and I think the future is a great topic
[01:21:19.640 --> 01:21:24.920]   So we'll get more and we have two excellent people to do this father Robert ballastair the digital Jesuit
[01:21:24.920 --> 01:21:28.840]   Who is uh, on leave from the vatican gets to be with us a little bit
[01:21:28.840 --> 01:21:33.880]   I think it's interesting because I think the Jesuits are the lea are in many ways the least dogmatic
[01:21:33.880 --> 01:21:37.320]   Uh of the cat members of the catholic church
[01:21:37.320 --> 01:21:42.840]   We've got a range we've got some dogmatic Jesuits. Yeah, but but yeah, you know our history
[01:21:42.840 --> 01:21:48.200]   Is to question. Yeah, that's kind of what we do and as far as I know
[01:21:48.680 --> 01:21:53.400]   They're you the order is unique in that it embraces, uh evolution. Yes
[01:21:53.400 --> 01:22:01.080]   Oh, that actually the church does the church does now the church does the catholic church. Wow. Wow. Yeah, most people don't realize that
[01:22:01.080 --> 01:22:03.160]   No, I did not know that
[01:22:03.160 --> 01:22:08.760]   Uh, and the earth is round as it turns out. So you know, it's it's round and it goes around the sun
[01:22:08.760 --> 01:22:10.760]   Wow
[01:22:10.760 --> 01:22:13.560]   Wow. Yeah, there have been people burned at the stake over less
[01:22:13.560 --> 01:22:15.240]   All right
[01:22:15.240 --> 01:22:21.400]   Also with us uh, Amy web whose eyes are uh cast into the future but is firmly grounded right here with us
[01:22:21.400 --> 01:22:23.080]   And i'm so glad to have her
[01:22:23.080 --> 01:22:25.880]   Uh, and by the way, uh, yes a professor
[01:22:25.880 --> 01:22:29.960]   Muff'son was very very cute. It was a cute. My god if he's
[01:22:29.960 --> 01:22:33.880]   He was he was like he's kind of like my type. I'm into like older baldness
[01:22:33.880 --> 01:22:37.560]   I
[01:22:37.560 --> 01:22:40.440]   I did briefly shave my head, but i'll never be bald. I'll tell you that right now
[01:22:40.440 --> 01:22:42.840]   Uh
[01:22:44.680 --> 01:22:49.240]   It's great to have you both i showed today brought to you speaking of money by wealth front now
[01:22:49.240 --> 01:22:54.440]   We talked we've been talking it's interesting. We talked earlier about how once you've get your
[01:22:54.440 --> 01:23:02.120]   You know your your uh, uh, financial security settled. It's time to start looking to intramore interesting investments
[01:23:02.120 --> 01:23:04.360]   But I want you I really want you to focus
[01:23:04.360 --> 01:23:07.480]   on building your wealth
[01:23:07.480 --> 01:23:11.480]   Us in a sane way first and that's where wealth front is so
[01:23:12.280 --> 01:23:18.520]   Great. It's not day trading. It's not watching the stock market day and day and now it's not buying game stop or amc
[01:23:18.520 --> 01:23:21.240]   Because somebody on reddit told you is the right thing to do
[01:23:21.240 --> 01:23:27.720]   There I see a lot of ads these days for investment apps that make oh, it's easy to start trading. There's one that says oh
[01:23:27.720 --> 01:23:31.560]   I take my advice from social media. Yeah, i'm not sure that's the
[01:23:31.560 --> 01:23:34.680]   Just because it's easy doesn't mean
[01:23:34.680 --> 01:23:37.240]   You know what you're doing
[01:23:37.240 --> 01:23:40.920]   Wealth front makes it easy to invest and easy to grow your savings
[01:23:41.320 --> 01:23:45.800]   But it does it with a diversified portfolio that balances your other
[01:23:45.800 --> 01:23:50.680]   Riskier bets you can start investing in no time with wealth front's classic portfolio
[01:23:50.680 --> 01:23:57.000]   Or make it your own with things that you care about you still can buy things like socially responsible funds. They get out
[01:23:57.000 --> 01:24:00.760]   They've got an amazing by the way socially responsible portfolio
[01:24:00.760 --> 01:24:06.360]   Technology do you want to invest in crypto trusts? You can do that as well. There's hundreds of other investments
[01:24:07.000 --> 01:24:11.800]   But the point is wealth front was designed by financial experts to help you build your wealth
[01:24:11.800 --> 01:24:18.120]   In a solid sane way to turn your good ideas into great investments without having to do it all yourself
[01:24:18.120 --> 01:24:22.520]   Rebalancing your portfolio experts always say you got to rebalance. You got to rebalance
[01:24:22.520 --> 01:24:27.720]   You got a lot of equities. You're over heavy inequities. You got to look at other investments wealth front does that?
[01:24:27.720 --> 01:24:31.400]   Automatically you don't even have to know what rebalancing is you're doing it
[01:24:31.400 --> 01:24:36.760]   Wealth front is so successful. They're trusted with over 28 billion dollars in assets
[01:24:37.240 --> 01:24:43.320]   It's easy to get started $500, but it's all about building your savings for your future whether it's retirement
[01:24:43.320 --> 01:24:50.440]   Buying that house college education for your kids grow your wealth the easy way let wealth front do the work for you
[01:24:50.440 --> 01:24:56.360]   So simple so powerful 4.9 out of 5 stars in the apple app store. That's a good sign, right?
[01:24:56.360 --> 01:25:03.000]   Yeah, it's easy to use. Yeah, it's effective start building your wealth get your first $5,000 managed free for life
[01:25:03.560 --> 01:25:09.720]   That is excellent. Go to wealthfront.com/twit wealthfront w e a l t h
[01:25:09.720 --> 01:25:12.440]   Fr o n t wealthfront
[01:25:12.440 --> 01:25:14.760]   Dot com slash twit start building your wealth
[01:25:14.760 --> 01:25:18.200]   Go to wealthfront.com a slash twit to get started
[01:25:18.200 --> 01:25:25.560]   Today, we thank wealth front for supporting this week in tech and thank you for supporting us by using that address by the way wealthfront.com
[01:25:25.560 --> 01:25:28.040]   slash
[01:25:28.040 --> 01:25:30.040]   twit
[01:25:30.440 --> 01:25:36.280]   Uh, CES determined determined not to shut down one more time
[01:25:36.280 --> 01:25:45.080]   Uh, but only what did you say father? Robert a quarter of the normal people about 40,000 instead of the usual 170,000
[01:25:45.080 --> 01:25:49.960]   showed up because uh fear of covid a lot of companies
[01:25:49.960 --> 01:25:56.600]   Uh, some of the biggest companies said yeah, we're not gonna go there were still plenty of keynotes some of them done by zoom
[01:25:57.400 --> 01:26:02.520]   Um, there was there was stuff to see were were all the halls full of exhibitors
[01:26:02.520 --> 01:26:09.640]   They were not they were most definitely not my favorite is the central hall so that if if you're not familiar with CES
[01:26:09.640 --> 01:26:15.160]   The central halls were all the big players go. That's your sony. That's your microsoft. They're all the TVs are
[01:26:15.160 --> 01:26:21.560]   Exactly all the all the blinky shiny ones. Uh, I think the best representation of CES
[01:26:21.800 --> 01:26:27.720]   2022 was where the lg booth should have been it is a premium spot as you walk into the hall
[01:26:27.720 --> 01:26:33.320]   You have to pass by lg. Yes, they had that video waterfall last year or two years ago. Right, right?
[01:26:33.320 --> 01:26:35.720]   And the tunnel they always do something tunnel. They had nothing
[01:26:35.720 --> 01:26:40.360]   Nothing it was literally the space was empty. It had some signage
[01:26:40.360 --> 01:26:46.040]   And it had QR code so you could go there and scan it and it would show you what would be there. Oh my gosh
[01:26:46.600 --> 01:26:52.280]   So they made for these decisions like last minute and so they didn't have a way to reconfigure the space or what happened
[01:26:52.280 --> 01:26:56.920]   Absolutely. Yeah, so uh what they see us put out a press release. They said look we've lost
[01:26:56.920 --> 01:27:02.280]   40 or 50 vendors, but we brought on 200. Well, the 200 they were basically saying
[01:27:02.280 --> 01:27:06.520]   Uh, we'll give you a booth just come just come and do something. We'll give you a booth
[01:27:06.520 --> 01:27:12.840]   There were a lot of booths that actually still had like the the opening desk and the starter paper there the entire show
[01:27:12.840 --> 01:27:15.320]   So they gave it to someone and they didn't show up
[01:27:15.320 --> 01:27:19.720]   That happened quite a bit now. That's not to say that it was it was a ghost town
[01:27:19.720 --> 01:27:24.120]   I actually had some of the best conversations that's at this CES and i've ever had
[01:27:24.120 --> 01:27:30.520]   Because there wasn't that constant crush of people there wasn't that need to move huge amounts of people through your booth
[01:27:30.520 --> 01:27:36.600]   So I had some great technical discussions with the people at bmw and with some of the people from the greater uk
[01:27:36.600 --> 01:27:38.440]   UK technical group
[01:27:38.440 --> 01:27:44.440]   Uh down in the startup pavilion. It it was it was actually nice. I don't think we're ever gonna have a ces like this again
[01:27:44.440 --> 01:27:46.440]   So i'm kind of glad I went. Yeah
[01:27:46.440 --> 01:27:53.240]   And and what the one reason to go to ces especially is for tv's to look at things that you wouldn't otherwise
[01:27:53.240 --> 01:27:58.600]   Be able to to understand and oppress release you can get all the tech specs and so forth
[01:27:58.600 --> 01:28:07.480]   But for instance this lg oled throne you never if you don't get to sit in it. He probably don't appreciate the
[01:28:07.480 --> 01:28:10.520]   Did you get to sit in it?
[01:28:10.520 --> 01:28:11.560]   Uh, no, no
[01:28:11.560 --> 01:28:16.200]   I actually there's there were two things that I think were the standout visual wise
[01:28:16.200 --> 01:28:19.480]   Yeah, the standouts of ces one of them was outside
[01:28:19.480 --> 01:28:24.280]   Uh, it was bmw bmw had their color changing vehicle
[01:28:24.280 --> 01:28:27.000]   Yeah, it was an e-book
[01:28:27.000 --> 01:28:34.520]   Interesting it was fun. I love gonzo technology. I sat there and I took five minutes of footage because it is so compelling
[01:28:34.520 --> 01:28:37.160]   Uh very impractical, but then again
[01:28:37.720 --> 01:28:41.800]   That's one of the things that I enjoy about ces I want in practical tech
[01:28:41.800 --> 01:28:47.400]   I want I remember seeing demonstrations of the self-driving cars 15 years ago
[01:28:47.400 --> 01:28:51.720]   Yeah, uh, you know, this is the sort of stuff that it doesn't look practical right now
[01:28:51.720 --> 01:28:57.080]   But you will find a practical way to put it kind of what bothers me about ces is there's a lot of non
[01:28:57.080 --> 01:29:00.040]   starters. There's a lot of you know
[01:29:00.040 --> 01:29:04.280]   Kind of sci-fi that it is. I get it. Yeah
[01:29:04.280 --> 01:29:07.080]   But that's what I mean
[01:29:07.640 --> 01:29:13.480]   That's what the world's fairer used to be or the the great expert that they so maybe I should think of it as a world's fair and not a
[01:29:13.480 --> 01:29:17.080]   Trage well, no, not anymore. I mean I the last time I went to ces
[01:29:17.080 --> 01:29:23.240]   I've been a couple times in the past 15 years, but in the early days. It was what mattered was the meetings in the rooms
[01:29:23.240 --> 01:29:27.320]   Yeah, they still had booth bait. So yeah, there've been some ces has changed
[01:29:27.320 --> 01:29:29.640]   It's gone through several every rate. So when it started
[01:29:29.640 --> 01:29:34.280]   It was just a place for retailers to find out what they were going to stock for the holiday season
[01:29:34.600 --> 01:29:39.320]   That was it. Yeah, that was the whole reason for the the conference to exist then it became
[01:29:39.320 --> 01:29:44.360]   Well, let's show you some of the crazy stuff. We're going to show you a prototype for a flying car
[01:29:44.360 --> 01:29:49.960]   We're going to show you the prototype for a nuclear powered washing machine completely impractical would never ever make it out there
[01:29:49.960 --> 01:29:57.480]   But it was it was nice for a young geek to say wow, I wonder if I could actually make that could I make that in a practical way
[01:29:57.480 --> 01:30:00.440]   then the iPhone came and I I will say
[01:30:01.480 --> 01:30:08.840]   For three years there ces became iPhone accessories. Yeah. Yeah, and I just it was the worst the worst years ever
[01:30:08.840 --> 01:30:14.200]   And now it's kind of a mix. Yes. You've got a lot of those meetings where people are deciding what they're going to stock
[01:30:14.200 --> 01:30:17.800]   But you're also getting a little bit more of the hey
[01:30:17.800 --> 01:30:22.200]   We pushed the tech to where we think it could be in 10 years
[01:30:22.200 --> 01:30:29.560]   We have no plans to manufacture this in the near future, but do you like it? And I kind of like that mix. Yeah
[01:30:30.680 --> 01:30:36.440]   So tell us some of the things you besides the color shifting car. All right, they're not going to make that as a real product
[01:30:36.440 --> 01:30:37.800]   Are they?
[01:30:37.800 --> 01:30:44.280]   No, I mean they're probably going to include little bits and pieces you can have it on the phone. Yeah, an entire car
[01:30:44.280 --> 01:30:47.400]   No, it's it would be too expensive and too easy to break
[01:30:47.400 --> 01:30:52.680]   There was something down in the startup pavilion. It was from the greater uk tech
[01:30:52.680 --> 01:30:55.400]   Foundation
[01:30:55.400 --> 01:30:59.240]   Basically, they did a pavilion for the uk and the spokesperson
[01:30:59.880 --> 01:31:01.640]   For this group
[01:31:01.640 --> 01:31:04.840]   Was this robot that they had created that was using machine learning
[01:31:04.840 --> 01:31:11.560]   To answer people now it sounds cheesy and when I first heard about it sounds so cheesy. It sounds like exact so cheesy
[01:31:11.560 --> 01:31:14.360]   Exactly the stuff that I hated
[01:31:14.360 --> 01:31:19.800]   See when I was filming it. I was like, yeah, okay. So this is basically, uh, this is an Alexa
[01:31:19.800 --> 01:31:25.320]   With a robot phase. Yeah, but then then I went behind the robot and I filmed it
[01:31:25.720 --> 01:31:31.560]   And I saw people interacting with the robot and I realized oh my god. There's something actually here people were enjoying
[01:31:31.560 --> 01:31:35.880]   Having a calm actual conversations with something that was not alive
[01:31:35.880 --> 01:31:38.680]   And yes
[01:31:38.680 --> 01:31:42.920]   What did it look like there was an actual robot with a button? It was very west. I have a picture right here
[01:31:42.920 --> 01:31:45.160]   It's a west world. Basically, right? It's
[01:31:45.160 --> 01:31:49.080]   But it it actually engaged people people were actually
[01:31:49.080 --> 01:31:54.360]   Enthusiastic about talking to the robot asking it questions seeing what kind of responses would come back with
[01:31:54.840 --> 01:32:00.840]   That that was probably the most successful demo at CES. I think it was way beyond what they thought they were going to get
[01:32:00.840 --> 01:32:05.560]   I think the problem with that for me is that in 1938
[01:32:05.560 --> 01:32:08.280]   Um, the western electric corporation
[01:32:08.280 --> 01:32:12.600]   And moto man that would be fair moto man was was not an AI
[01:32:12.600 --> 01:32:17.400]   It had it was using telephone relays, but it did it smoked it even smoked
[01:32:17.400 --> 01:32:23.240]   So so it's been, you know a really really long time. This is just creepy
[01:32:24.440 --> 01:32:30.280]   And it's not really I mean this isn't going to be doing my dishes anytime soon, right? I mean, right right?
[01:32:30.280 --> 01:32:36.200]   Yeah, this is more of a but that's why it's also not uncanny valley. Yeah, because they didn't try to make it human
[01:32:36.200 --> 01:32:38.440]   It's obviously synthetic
[01:32:38.440 --> 01:32:44.440]   But they were able to sort of they were it doesn't make you think about what is human and what is it that?
[01:32:44.440 --> 01:32:49.960]   You know, we look at this and it and you really very quickly anthropomorphize it and say oh yeah, I see feelings
[01:32:49.960 --> 01:32:54.040]   And it is anthropomorphic. We don't have to anthropomorphize it
[01:32:54.040 --> 01:32:56.040]   It looks like a
[01:32:56.040 --> 01:32:58.040]   Purpose yeah, yeah
[01:32:58.040 --> 01:33:05.640]   But I think it's worth noting that you know almost what 80 years ago a similar robot existed at a similar conference
[01:33:05.640 --> 01:33:12.360]   Yeah, yeah, but now if you want if you want what was actually useful from ces
[01:33:12.360 --> 01:33:16.680]   By and not not even a question. It was all the biotech
[01:33:16.680 --> 01:33:19.720]   And abbot abbot was the standout
[01:33:19.720 --> 01:33:21.400]   100%
[01:33:21.400 --> 01:33:28.040]   So they're the ones who make the the rats the rapid antigen tests and they were handing those things up. Oh my god. They don't make rats
[01:33:28.040 --> 01:33:30.280]   Okay
[01:33:30.280 --> 01:33:33.640]   But but their other tech they they they created a device
[01:33:33.640 --> 01:33:39.400]   That can it looks for markers in the blood that are released when there is a concussive brain injury
[01:33:39.400 --> 01:33:47.400]   And now they've created a handheld device that you put a drop of blood into a cartridge and it can detect if a person has had a concussive brain injury
[01:33:47.400 --> 01:33:49.400]   before
[01:33:49.400 --> 01:33:52.920]   Well, I know I know and that's what I said. I said you understand there's shades of Theranos here
[01:33:52.920 --> 01:33:59.320]   They said yeah, but this is actually approved. Okay. This actually works. Okay. They also have a deep brain stimulation device
[01:33:59.320 --> 01:34:03.800]   It's one like a pacemaker and anyone who's suffering from Parkinson's can now have relief
[01:34:03.800 --> 01:34:12.760]   They've also made a patch that does real-time monitoring of everything from glucose levels to blood chemistry keto
[01:34:12.760 --> 01:34:15.160]   Ketosis, yeah
[01:34:15.160 --> 01:34:20.840]   Exactly. So I mean this this is actually technology that I was like if you gave this gave this to me. I would actually use it
[01:34:20.840 --> 01:34:26.440]   I would find that use has this is not future tech clearly seen this as one of the
[01:34:26.440 --> 01:34:36.440]   Apple looks like they're going they're looking really hard at AR at cars and at health and they've already got a product that is you know kind of
[01:34:36.440 --> 01:34:41.240]   Their entree into the health world, which is the apple watch and it's very clear
[01:34:41.800 --> 01:34:47.640]   They're looking to this as one of the next big ways for them to make money a bit multi-billion dollar
[01:34:47.640 --> 01:34:51.880]   Industry of course number one would be blood sugar non-invasive
[01:34:51.880 --> 01:34:59.960]   Blood sugar measurements. There's 14 million diabetics in the United States who would all immediately buy an apple watch if it could give them their blood sugar
[01:34:59.960 --> 01:35:02.520]   Without a prick
[01:35:02.520 --> 01:35:06.600]   So amie let's talk about that
[01:35:06.600 --> 01:35:09.800]   Health is that the next big thing
[01:35:10.760 --> 01:35:17.640]   Yeah, well, there's a couple of drivers for for why health tech is accelerating one is there's a big component
[01:35:17.640 --> 01:35:22.040]   with AI so AI for predictive analytics and
[01:35:22.040 --> 01:35:26.280]   You kind of need ai to make this work because the signals are so weak
[01:35:26.280 --> 01:35:30.200]   You need something that can be trained so that it will recognize those signals
[01:35:30.200 --> 01:35:36.200]   Right. So that's happening the other thing that's happening is the quantified self-movement that
[01:35:37.160 --> 01:35:42.280]   You know was sort of coming and going and fits and starts and the people like that though every athlete
[01:35:42.280 --> 01:35:44.840]   I know wears a Fitbit or a they do now
[01:35:44.840 --> 01:35:48.520]   Yeah, they do now, but I think if you were to go back 15 years at CES when
[01:35:48.520 --> 01:35:56.040]   Quantified self was kind of everybody was talking about it. It never happened, but the technology and the price points of you know sort of
[01:35:56.040 --> 01:35:58.120]   aligned
[01:35:58.120 --> 01:36:04.360]   So you've got younger people interested in optimizing everything from sleep to you know endurance and everything else
[01:36:04.760 --> 01:36:09.160]   and then you've got an aging population that is affluent enough
[01:36:09.160 --> 01:36:13.880]   and tech literate enough to to want to invest in devices
[01:36:13.880 --> 01:36:18.360]   Yeah, we knew the boomers would kind of move the needle as we have our entire lives
[01:36:18.360 --> 01:36:21.080]   We were a giant market and now we're aging
[01:36:21.080 --> 01:36:26.200]   And you've got covid which has shifted everybody into thinking more about health and diagnostics
[01:36:26.200 --> 01:36:31.480]   So it's kind of a perfect storm last year. I think it was or two years ago at CES the smart toilet launched
[01:36:32.040 --> 01:36:36.040]   Which was research that Stanford started in whatever 2010
[01:36:36.040 --> 01:36:39.960]   I mean, it's like putting a bunch putting much of sensors in the one place that you really
[01:36:39.960 --> 01:36:41.880]   Analytics are
[01:36:41.880 --> 01:36:48.040]   As disgusting as that sounds clearly could be a very big health benefit
[01:36:48.040 --> 01:36:50.600]   Right
[01:36:50.600 --> 01:36:55.560]   They they use it for dorms so they could detect when they where there was covid 19
[01:36:55.560 --> 01:36:58.520]   Well, yeah, in fact, we know they're an analyzing sewage
[01:36:59.800 --> 01:37:02.520]   And uh effluent to to find covid
[01:37:02.520 --> 01:37:07.320]   Viruses in populations, and it's been a very good indicator
[01:37:07.320 --> 01:37:11.160]   And there's startups that are there's a there's a company in
[01:37:11.160 --> 01:37:16.440]   England that will that for a while was before we were constantly getting our noses swabbed
[01:37:16.440 --> 01:37:19.320]   This was a voluntary spit into a tube at a grocery store
[01:37:19.320 --> 01:37:26.440]   Uh where you could get your dna tested and then they would give you a wristband that would glow different colors and and help you shop
[01:37:26.440 --> 01:37:29.160]   Basically, it was like dna base that is weird
[01:37:29.800 --> 01:37:31.800]   So that is weird and it's not a one-off
[01:37:31.800 --> 01:37:36.760]   So I my point is well, I know a lot of people who have done these tests that say this is how you should eat
[01:37:36.760 --> 01:37:42.520]   Uh 23 and me was even offering that kind of information right and it's what you should be eating
[01:37:42.520 --> 01:37:47.640]   I don't know if it's snake oil or real it's right it's if you talk to any geneticist
[01:37:47.640 --> 01:37:50.280]   They'll they'll tell you this is a little bit like a horoscope
[01:37:50.280 --> 01:37:55.960]   You know, you can use it for entertainment purposes, but like don't base your horoscope any perfect decisions, you know
[01:37:56.360 --> 01:37:59.880]   There was an episode of farscape where they used a
[01:37:59.880 --> 01:38:05.320]   Sort of a dna test to determine who you would be compatible with in relationships
[01:38:05.320 --> 01:38:07.880]   Yeah, there's actually a dna base
[01:38:07.880 --> 01:38:14.840]   We are just needing by the way. We already have that system. It's in the nose and it's very effective
[01:38:14.840 --> 01:38:20.280]   There's a lot of you're laughing, but there's a lot of evidence
[01:38:21.240 --> 01:38:28.280]   That genetically your best match batteries exhausted by the way your batteries exhausted father. I have exhausted my best
[01:38:28.280 --> 01:38:30.760]   I wasn't a pleasure battery to bed
[01:38:30.760 --> 01:38:33.640]   That's a problem
[01:38:33.640 --> 01:38:37.160]   But there's a lot of evidence there's a lot of evidence that we are that
[01:38:37.160 --> 01:38:40.120]   You your genetic
[01:38:40.120 --> 01:38:45.960]   Best genetic match is somebody who smells good to you that your nose is already doing that and it wouldn't be surprising
[01:38:45.960 --> 01:38:48.600]   It would make sense from an evolutionary point of view
[01:38:48.600 --> 01:38:55.800]   Right. I mean I think the thing is is how do you monetize all that and so part of isn't that a shame that that's always the metric?
[01:38:55.800 --> 01:38:58.120]   Well
[01:38:58.120 --> 01:39:04.520]   Again, look at you know, where do you grow apple apples making phones and computers and there's a lot of our you know
[01:39:04.520 --> 01:39:09.240]   A lot of our models show that that you know, the phone is gonna be dead in a couple of years
[01:39:09.240 --> 01:39:13.240]   And it's hard to wrap your head around just like father robert. It's uh exhausted. Yeah
[01:39:14.120 --> 01:39:16.120]   Exhausted
[01:39:16.120 --> 01:39:23.640]   Health is so big and so broken that I think companies are
[01:39:23.640 --> 01:39:29.800]   Like seeing a viable opportunity to disrupt and the sort of Clayton christian senators the lemon
[01:39:29.800 --> 01:39:31.720]   That's I think part of the problem
[01:39:31.720 --> 01:39:37.240]   That's part of the problem in this country is we don't do it unless you can make money at it
[01:39:37.480 --> 01:39:44.200]   It's one of the reasons we have very expensive pharmaceuticals in place of simple solutions that no one's
[01:39:44.200 --> 01:39:49.800]   Compatant that we don't we do high tech medicine, but we don't do low tech medicine
[01:39:49.800 --> 01:39:57.240]   And I think there's a lot of evidence that outcomes could be better if uh, there were you know, there were perhaps you know
[01:39:57.240 --> 01:40:03.240]   Better relationships between doctors and patients instead of better machinery between them
[01:40:04.280 --> 01:40:09.560]   In a way doesn't doesn't that desire to make it profitable mislead us somewhat
[01:40:09.560 --> 01:40:16.600]   Right. So this as far as I'm concerned is the tragedy of of the the free market. Yes the free market
[01:40:16.600 --> 01:40:20.760]   You know the free market is like algorithmic determinism, right?
[01:40:20.760 --> 01:40:25.080]   If if left to its own devices, it will continue to soldier on perfect analogy
[01:40:25.080 --> 01:40:27.640]   Perfect. That's good for everybody. Yes
[01:40:27.640 --> 01:40:32.120]   So there is a balance but again it forces a different mental model and at the moment
[01:40:32.120 --> 01:40:35.240]   You're either libertarian or you're republican or you're a you know
[01:40:35.240 --> 01:40:38.440]   A strict interpretationist everybody needs a label
[01:40:38.440 --> 01:40:41.880]   um if we could dispense with the labels and get on with the business of
[01:40:41.880 --> 01:40:45.240]   Creating better equity for everybody. I think we'd be in a better spot
[01:40:45.240 --> 01:40:49.800]   Yeah, and I understand you need a capital to do some of these more expensive things right
[01:40:49.800 --> 01:40:52.200]   So the the antidote to that is
[01:40:52.200 --> 01:40:56.280]   Direct was like just like throwing a shit ton of money at basic research
[01:40:56.280 --> 01:41:01.240]   We see other governments doing this we see china that's who needs to do it government is in theory
[01:41:01.240 --> 01:41:05.880]   Not motivated by profit government is in theory motivated by societal outcome
[01:41:05.880 --> 01:41:09.800]   Right and I would argue that that is a very and i'm i'm in a very high
[01:41:09.800 --> 01:41:12.840]   You know, i'm in a very privileged place which means i'm
[01:41:12.840 --> 01:41:16.840]   But i'm not wealthy enough to not pay taxes. So i'm just in that bracket. That's
[01:41:16.840 --> 01:41:18.760]   Isn't that the worst?
[01:41:18.760 --> 01:41:23.800]   You make a lot of money, but you're not wealthy enough not to pay taxes. Yeah, just exactly
[01:41:23.800 --> 01:41:24.440]   Yeah, I agree
[01:41:24.440 --> 01:41:29.720]   So like take my money and please throw it at science and it doesn't mean that the government does everything
[01:41:29.720 --> 01:41:34.360]   But it means we need to have better public private partnerships. It also means that government has to be held
[01:41:34.360 --> 01:41:42.040]   I'm with you. I'm horrible part of the reason why this james web telescope took so long to launch funding was because it was working funding
[01:41:42.040 --> 01:41:48.040]   And it was working at the typical pace of government, which is probably not right for everybody going forward, right?
[01:41:48.040 --> 01:41:53.320]   Right and yet. I don't want mark zuckerberg to be in charge of our space exploration either
[01:41:53.560 --> 01:41:57.560]   I don't want zuck or elon musk or bezos to be in charge
[01:41:57.560 --> 01:42:03.160]   But I want some of the freedom and the speed that comes with the free you know
[01:42:03.160 --> 01:42:08.440]   Elon has done an awful lot to forward our space program all by himself
[01:42:08.440 --> 01:42:14.440]   That's right. Yeah. Well, not all my other way to he had a few thousand engineers and scientists working with him
[01:42:14.440 --> 01:42:17.480]   But he's been right and i think bezos
[01:42:17.480 --> 01:42:22.440]   I think bezos gets a lot of shit for many reasons some of which are very legitimate
[01:42:22.440 --> 01:42:24.440]   But but he has also
[01:42:24.440 --> 01:42:30.600]   You know, it's not just musk building the future with space x there's a lot of other things that are happening
[01:42:30.600 --> 01:42:36.600]   But but but the bottom line is we can't outsource the future of space
[01:42:36.600 --> 01:42:43.480]   to the commercial sector because the incentives there are tied to profit
[01:42:43.480 --> 01:42:46.360]   but do you think we have a pretty maybe this is a
[01:42:47.960 --> 01:42:54.360]   Roadmap for the way we should handle this it seems like at least in space. We've had a pretty good balance between
[01:42:54.360 --> 01:43:00.280]   Commercial space and government and it's done. I think a pretty good job
[01:43:00.280 --> 01:43:04.040]   Is this now a new model for medicine and other areas?
[01:43:04.040 --> 01:43:08.680]   Well, it certainly did when we were still in the business of defining and acting on moonshots
[01:43:08.680 --> 01:43:14.120]   You know, there's no Kennedy and i'm not like a like a only government could do that. I mean it yeah, but
[01:43:14.760 --> 01:43:18.520]   We don't have we we don't have a I mean this is part of the problem
[01:43:18.520 --> 01:43:25.160]   There's so much policy uncertainty. We don't have the ability to plan for the next 10 years in the United States because because we don't have a
[01:43:25.160 --> 01:43:27.480]   Department of foresight
[01:43:27.480 --> 01:43:32.520]   Um, so what happens is a lot of this becomes politicized or just left to the private sector
[01:43:32.520 --> 01:43:37.080]   And then they build and build and then a bunch of people get upset, you know that there is an alternative
[01:43:37.080 --> 01:43:42.680]   Let me give you one quick example Dubai. So I did not go to ces, but I did go to the world's fair
[01:43:43.160 --> 01:43:45.480]   Did you really expose 2020?
[01:43:45.480 --> 01:43:50.680]   It was uh, 2021. Yeah, yes, it was absolutely mind-blowing to me
[01:43:50.680 --> 01:43:57.400]   Um, and there's nothing there science-wise. That's very interesting and the usa pavilion for many reasons was
[01:43:57.400 --> 01:43:59.160]   um
[01:43:59.160 --> 01:44:01.000]   I which I helped advise on I think
[01:44:01.000 --> 01:44:06.520]   For what it was was okay, but our lack of planning meant that it was not as good as it could have been
[01:44:06.520 --> 01:44:11.800]   Isn't that ironic? Oh my god. It's not it's like totally on point. Yep for the United States
[01:44:11.800 --> 01:44:18.040]   Yeah, we would have had a better pavilion, but the dog ate our homework and uh, and many many many things
[01:44:18.040 --> 01:44:20.040]   um, but
[01:44:20.040 --> 01:44:25.320]   The they built a city in the middle of a desert. I mean it is it is just absolutely
[01:44:25.320 --> 01:44:27.640]   I mean, that's what's amazing about Dubai
[01:44:27.640 --> 01:44:33.400]   Now one of the things that they are doing that we are not doing is that they're trying to figure out
[01:44:33.400 --> 01:44:39.480]   What is the future of drone? So the ua in Dubai specifically like what is the future of drones for delivery for people?
[01:44:39.960 --> 01:44:42.280]   Transportation for pets and objects. What does that look like?
[01:44:42.280 --> 01:44:49.400]   But while the technology is developing they are also they also have like a beta lab for policy. So they've got people
[01:44:49.400 --> 01:44:57.080]   Working alongside the technologists to figure out okay. Well, what should policy look like in response or what does urban planning look like?
[01:44:57.080 --> 01:44:59.560]   While the technology is developing
[01:44:59.560 --> 01:45:01.480]   And it's a really interesting synergy
[01:45:01.480 --> 01:45:09.400]   It's like a totally unique and different approach that we might adopt here if there wasn't such a wedge between the valley and and uh
[01:45:09.880 --> 01:45:11.880]   You know to see
[01:45:11.880 --> 01:45:17.960]   Uh, all right. I want to take a little break and I do want to go right into the heart of your expertise
[01:45:17.960 --> 01:45:23.400]   Amy's next book is all about the biotech revolution
[01:45:23.400 --> 01:45:29.960]   I've pre-ordered it. I'm very excited. Uh, I can't wait to get it. It comes out february 15th
[01:45:29.960 --> 01:45:33.000]   Uh, let's talk about biotech
[01:45:33.000 --> 01:45:37.560]   A lot of science is being done in space. That's great
[01:45:37.880 --> 01:45:40.840]   But we kind of need to solve some problems here on planet earth
[01:45:40.840 --> 01:45:46.440]   So let's see what that future holds in just a bit. Amy web is here from the future today institute
[01:45:46.440 --> 01:45:49.800]   Father robert balisair from the middle ages
[01:45:49.800 --> 01:45:57.880]   Do you not have another battery? Uh, father robert? Uh, well, unfortunately no, but I do
[01:45:57.880 --> 01:46:05.320]   I do have this this camera from like 1940. Uh, I think that'll work that'll work
[01:46:07.640 --> 01:46:13.080]   That's hysterical. Yeah, you forgot to plug it in. I guess I did not I forgot the plug in the power
[01:46:13.080 --> 01:46:18.440]   I had the hd and light plugged in but not the power. That's so funny. It's so funny
[01:46:18.440 --> 01:46:23.480]   All right, but before we uh, let's let's pause for a minute lots more future to talk about before we do that though
[01:46:23.480 --> 01:46:26.840]   Let's talk about the past what happened this week on twit
[01:46:26.840 --> 01:46:32.520]   There's this guy named jeff javis. He thinks this emoji is a whistling emoji
[01:46:32.520 --> 01:46:35.320]   Well, everybody knows it's a kissing emoji
[01:46:36.120 --> 01:46:41.560]   Only yesterday. I learned I've been using this incredibly inappropriately as it turns out. Wow
[01:46:41.560 --> 01:46:44.760]   Yes, you've been sending people kisses
[01:46:44.760 --> 01:46:51.560]   Previously on twit hands on photography. I am going to answer a question that I get
[01:46:51.560 --> 01:46:56.200]   Just about every single day. Which camera should I buy?
[01:46:56.200 --> 01:46:59.160]   the tech guy
[01:46:59.160 --> 01:47:00.600]   qd olet
[01:47:00.600 --> 01:47:01.480]   Ah
[01:47:01.480 --> 01:47:08.040]   Quantum dot olet the benefits are many including greater brightness much greater brightness because always been
[01:47:08.040 --> 01:47:10.440]   an area olet's have lagged behind
[01:47:10.440 --> 01:47:16.520]   Lcd's and lcd's is that they're just not as bright does this song? Yes, it does
[01:47:16.520 --> 01:47:22.520]   This week in google regular people who don't spend their days thinking about law thinking about tech
[01:47:22.520 --> 01:47:27.720]   You know they have political will this is a democracy they can choose the laws that regulate them
[01:47:27.720 --> 01:47:30.760]   So how do you educate them so they can ask for good things?
[01:47:30.760 --> 01:47:34.840]   Then when they call their members of congress, they will ask for things that are actually more useful
[01:47:34.840 --> 01:47:37.240]   Then just do something burn it all down. What do I care?
[01:47:37.240 --> 01:47:43.160]   I hear section 230 is bad. You might as well get rid of it and how dare you not twit making the world safe for technology
[01:47:43.160 --> 01:47:48.520]   Well, let's take a little break father robert ballis here from the middle ages
[01:47:48.520 --> 01:47:54.120]   Amy web from the future. I'm stuck right here in the present our show today brought to you by
[01:47:54.120 --> 01:47:55.880]   better
[01:47:55.880 --> 01:48:01.240]   Help after listening to the show you might need some better help better help
[01:48:01.240 --> 01:48:09.400]   Is a licensed therapist you can consult with online without any of the stigma without any of the embarrassment
[01:48:09.400 --> 01:48:15.320]   For a better cost and it really works and I can tell you that from personal experience
[01:48:15.320 --> 01:48:20.760]   If something's preventing you from achieving your goals if something's interfering with your happiness
[01:48:20.760 --> 01:48:24.040]   If you've just got and I think we all do these days
[01:48:24.920 --> 01:48:26.120]   uh
[01:48:26.120 --> 01:48:28.120]   Letting feeling in the pit of your stomach
[01:48:28.120 --> 01:48:35.800]   Better help is here better help calm slash they'll match you with your own licensed professional therapist
[01:48:35.800 --> 01:48:37.800]   One of the things I really liked about it
[01:48:37.800 --> 01:48:45.000]   I've always been a believer that you've got to find the right therapist better help makes it very easy to try therapists and to move on
[01:48:45.000 --> 01:48:48.440]   Uh with no downside to find the person you want
[01:48:48.440 --> 01:48:52.200]   Uh good friend of mine went through I think three or four therapists
[01:48:52.760 --> 01:48:56.760]   Before this person found exactly the right person and better help made that easy
[01:48:56.760 --> 01:49:01.480]   You'll connect in a safe private online environment if you don't want to use video
[01:49:01.480 --> 01:49:08.440]   You don't have to you can have a weekly phone session. You can it's not a crisis line. I want to make that clear
[01:49:08.440 --> 01:49:12.280]   Nor is it self help it's professional counseling
[01:49:12.280 --> 01:49:17.160]   With licensed therapists done securely online
[01:49:17.160 --> 01:49:19.960]   You can send a message to your counselor anytime
[01:49:20.520 --> 01:49:25.400]   You'll get timely thoughtful responses. You can schedule weekly video or phone sessions again
[01:49:25.400 --> 01:49:29.800]   You don't have to be on camera. You don't have to sit in an uncomfortable waiting room
[01:49:29.800 --> 01:49:35.880]   You don't have to have nobody need know that you're doing this and I know for some people that's you know, that's they don't want they're embarrassed
[01:49:35.880 --> 01:49:38.600]   You I should say you shouldn't be embarrassed
[01:49:38.600 --> 01:49:42.600]   This is really I'm such a supporter of this better help
[01:49:42.600 --> 01:49:48.920]   Is committed to facilitating great therapeutic matches. They make it easy and free to change counselors if you need to
[01:49:49.400 --> 01:49:55.160]   It's more affordable than traditional offline counseling. They even have financial aid
[01:49:55.160 --> 01:49:59.400]   It is available worldwide. I love that too
[01:49:59.400 --> 01:50:03.000]   And you can find the particular expertise you need online
[01:50:03.000 --> 01:50:09.640]   Because they've got counselors in all areas. You don't have to limit yourself to what's available in your location
[01:50:09.640 --> 01:50:16.600]   You can in other words, you can get a licensed professional who's an expert in depression or stress or anxiety
[01:50:17.160 --> 01:50:19.960]   Relationships family conflicts
[01:50:19.960 --> 01:50:24.120]   Well, I needed that last month sleeping trauma
[01:50:24.120 --> 01:50:31.320]   Anger I have a good personal friend whose sleep has been terrible
[01:50:31.320 --> 01:50:33.800]   Counseling has made such a difference
[01:50:33.800 --> 01:50:37.000]   LGBTQ matters
[01:50:37.000 --> 01:50:38.200]   grief
[01:50:38.200 --> 01:50:40.920]   Don't suffer alone if you're in grief right now
[01:50:40.920 --> 01:50:45.240]   Don't suffer alone. There's somebody there who can really help
[01:50:45.880 --> 01:50:47.640]   self-esteem
[01:50:47.640 --> 01:50:54.760]   And of course anything you share confidential. It's very convenient. It's very professional and it's really affordable
[01:50:54.760 --> 01:50:59.720]   There's plenty of testimonials on the website. You can look there. I will give you a testimonial
[01:50:59.720 --> 01:51:05.240]   It really works. In fact, so many people now are using better help. They're actually recruiting additional counselors
[01:51:05.240 --> 01:51:09.000]   This is an idea whose time is varied. It's very timely
[01:51:09.000 --> 01:51:15.240]   All 50 states they're looking for therapists. So if you're a licensed counselor, uh, contact them as well
[01:51:15.880 --> 01:51:20.280]   Uh, but look I just really I don't I don't know how I can emphasize this
[01:51:20.280 --> 01:51:25.000]   This is something that really works that will make a difference in your life
[01:51:25.000 --> 01:51:30.520]   If you've resisted talking to a professional about these issues ever before
[01:51:30.520 --> 01:51:33.640]   Now's the time to do it. It's easy
[01:51:33.640 --> 01:51:40.520]   Join over 1 million people who've taken charge of their mental health and as a listener, you're going to get 10 percent off your first month
[01:51:40.520 --> 01:51:43.640]   Just by going to better help dot com slash twit
[01:51:44.680 --> 01:51:48.200]   Better help dot com slash twit time to take charge of your mental health
[01:51:48.200 --> 01:51:50.920]   And the thing is this works
[01:51:50.920 --> 01:51:56.760]   It really works. It can really help you will feel better better help be et t t er
[01:51:56.760 --> 01:51:59.080]   H e l p
[01:51:59.080 --> 01:52:03.000]   Dot com slash twit help is on the way better help dot com
[01:52:03.000 --> 01:52:09.480]   Slash twit. These are really really good people that do a very good job. It's very it's very important job
[01:52:10.760 --> 01:52:17.080]   Uh, all right. Amy web your new book coming out in uh, just a few weeks. Are you excited?
[01:52:17.080 --> 01:52:21.480]   I am please pre-order it. I did
[01:52:21.480 --> 01:52:25.880]   For two reasons. There's a paper shortage and so oh
[01:52:25.880 --> 01:52:28.920]   Good lord even paper
[01:52:28.920 --> 01:52:33.720]   Yeah, there's there's supply chain is all screwed up paper right now. So I my publisher has ordered
[01:52:35.000 --> 01:52:41.400]   They've assured me enough books, but when my last book came out the big nine, um, they ran out a lot of book stories ran out of books
[01:52:41.400 --> 01:52:46.360]   Um, so anyway, and this is really such a hot topic. This is going to be a bestseller
[01:52:46.360 --> 01:52:50.920]   I think people really want to know it's not just crisper. It's not just mr
[01:52:50.920 --> 01:52:53.880]   Any vaccines there is stuff going on in biotech
[01:52:53.880 --> 01:52:58.360]   That I think is going to be as significant as james web if not more so
[01:52:58.360 --> 01:53:03.480]   Yeah, um, I I agree with you. It's it's I actually think it's the most important
[01:53:03.960 --> 01:53:10.920]   Technology of our lifetimes and and I was so intrigued by it that I wrote a book on it. Um, you know what? You know who else?
[01:53:10.920 --> 01:53:15.240]   Uh, thinks this is going to be that important bill gates
[01:53:15.240 --> 01:53:18.840]   I remember even this was 20 years ago
[01:53:18.840 --> 01:53:24.440]   Bill would always take these vacations these long vacations where he would read up on something
[01:53:24.440 --> 01:53:28.040]   And even then he said the next big thing is biotech
[01:53:28.040 --> 01:53:32.840]   And I need to be an expert in this and he's very few people know this but sorry
[01:53:32.840 --> 01:53:34.840]   I didn't mean to interrupt please go ahead
[01:53:34.840 --> 01:53:38.840]   Well, very few people know this but microsoft is actually a big player. Oh, yeah space
[01:53:38.840 --> 01:53:43.080]   Microsoft is is trying to figure out how to store data inside of dna
[01:53:43.080 --> 01:53:46.680]   I knew this because esther dyson would go with him on these trips and she told me yeah
[01:53:46.680 --> 01:53:53.720]   We brought all of these books. We did all of this reading and yeah bill gates has been personally an investment microsoft also
[01:53:53.720 --> 01:53:56.920]   Um, it's a very exciting time
[01:53:56.920 --> 01:54:00.360]   What do you see is the big?
[01:54:00.520 --> 01:54:04.280]   Developments that are going to happen and make a difference in our lives in the next decade or so
[01:54:04.280 --> 01:54:07.560]   Yeah, um, so first of all, it's kind of a weird term
[01:54:07.560 --> 01:54:12.680]   So synthetic biology is a relatively new interdisciplinary field of science
[01:54:12.680 --> 01:54:17.400]   So if you've got an engineering background, you're going to recognize parts of what you do in
[01:54:17.400 --> 01:54:25.080]   This form of wetware if you're if you've worked in a eye in design in circuit board creation like
[01:54:25.080 --> 01:54:28.040]   Um, there's a huge overlap
[01:54:28.360 --> 01:54:35.720]   Um, so basically researchers design or they redesign organisms at a molecular level to give them new purposes
[01:54:35.720 --> 01:54:38.280]   or to enhance them and
[01:54:38.280 --> 01:54:46.680]   This this really does change everything because in a way to make a analogy to like coding it gives us right access to life. So
[01:54:46.680 --> 01:54:54.600]   Um, so why is it synthetic as a poet? What does that mean synthetic biology? So it's a good question. Um, so
[01:54:55.560 --> 01:54:58.520]   We're obviously using a ctg so
[01:54:58.520 --> 01:55:06.280]   Atomic organic anybody who loves the movie gataka knows that those are the key amino acids. Yeah. Yeah, okay
[01:55:06.280 --> 01:55:15.960]   Um, so yes, uh, so so that part's not synthetic that the part is um, the reason we call it synthetic biology is because it's synthesizing
[01:55:15.960 --> 01:55:20.200]   Um code if we're for new purposes. So it's
[01:55:20.200 --> 01:55:23.240]   It's sort of sequencing and synthesizing
[01:55:23.880 --> 01:55:25.880]   So so when watson and krick
[01:55:25.880 --> 01:55:32.120]   Uh who totally stole and used the work of roslin franklin without ever giving her any credit
[01:55:32.120 --> 01:55:36.680]   And then watson who who was like awful um disparaged her publicly
[01:55:36.680 --> 01:55:42.200]   Such as such a tragedy. I it really is he's um, hara horrible
[01:55:42.200 --> 01:55:44.680]   Horrible, uh, so
[01:55:44.680 --> 01:55:48.920]   You know, they sort of built the first model revealed the first model of dna
[01:55:48.920 --> 01:55:52.440]   And that gave others the ability to read our biological source code
[01:55:52.440 --> 01:55:56.840]   So that's like read permissions edit permissions would be crisper
[01:55:56.840 --> 01:56:00.600]   So this is limited ability to splice and move around a ctg
[01:56:00.600 --> 01:56:04.760]   Um that that gives us sort of a edit level permission
[01:56:04.760 --> 01:56:09.880]   You can hook up a special protein to a guide rna and it gets you to the right part of the cell and
[01:56:09.880 --> 01:56:17.720]   moves along the dna strand until it finds the right sequence it sounds like it's very much influenced by uh, computer technology by digital
[01:56:17.720 --> 01:56:20.280]   It totally is yeah, it totally is and so
[01:56:20.840 --> 01:56:26.760]   Synthetic biology literally is like it gives us the ability to write new code. So if you're a developer, you know
[01:56:26.760 --> 01:56:30.680]   You might invent new machine learning algos or a new protocol
[01:56:30.680 --> 01:56:33.640]   Um, but you're you're in a way like still
[01:56:33.640 --> 01:56:39.400]   Sort of fundamentally tethered to the architecture of whatever, you know the the platform and the systems logic
[01:56:39.400 --> 01:56:43.960]   Um, which means that the end you're still working in ones and zeros. So it's kind of the same in biology
[01:56:43.960 --> 01:56:50.440]   You're still in the end using a ctg code, but if biology is the fundamental technology platform
[01:56:50.920 --> 01:56:56.200]   Then this gives us the ability to to write almost like you would write in a word document or or like
[01:56:56.200 --> 01:57:00.120]   You know terminal or says like like pick your favorite program, right?
[01:57:00.120 --> 01:57:03.080]   Um, and you would be able to write what you want
[01:57:03.080 --> 01:57:09.320]   Um literally send it to a printer. It sounds like science fiction, but we actually uh
[01:57:09.320 --> 01:57:13.720]   M rna vaccines are one of the outcomes of this
[01:57:13.720 --> 01:57:19.400]   That's absolutely right and this research has been you know 10 years. It's going on 10 years
[01:57:20.040 --> 01:57:22.040]   um, but it was a way of
[01:57:22.040 --> 01:57:24.020]   applying
[01:57:24.020 --> 01:57:30.920]   Uh, you know, they they sequenced the the genetic code of the virus in a very short amount of time over a weekend
[01:57:30.920 --> 01:57:33.560]   over a weekend
[01:57:33.560 --> 01:57:36.120]   So
[01:57:36.120 --> 01:57:41.320]   So it it's really remarkable, but it's also not at all remarkable that we got these vaccines so fast
[01:57:41.320 --> 01:57:43.400]   And they would have been in the market faster except for
[01:57:43.400 --> 01:57:45.000]   Um
[01:57:45.000 --> 01:57:47.880]   We had to test them and you know supply chain. Yeah
[01:57:48.840 --> 01:57:55.720]   But basically this means it's an ask a question. Yeah, yeah, yeah, so so I know that synthetic biology basically allows you to
[01:57:55.720 --> 01:57:57.800]   repurpose organisms
[01:57:57.800 --> 01:58:04.680]   Uh by editing long stretches of dna versus say genetic editing which would be you know much smaller sequences
[01:58:04.680 --> 01:58:10.840]   How close is this to being able to not repurpose an organism but to create an organism?
[01:58:10.840 --> 01:58:16.360]   Yeah, well, that's already that's already happened. So a guy named Craig manner. This is scary
[01:58:17.320 --> 01:58:24.360]   So there there's some sort of figureheads in this field that are heroes of mine and craig ventners one of them and then there was uh the guy who sequenced the genome
[01:58:24.360 --> 01:58:31.160]   He sure he well so watson was leading a program at the government doing it old school and
[01:58:31.160 --> 01:58:37.880]   Basically completely unwilling to look at any new techniques or tools then there's this like surfer guy
[01:58:37.880 --> 01:58:39.480]   Uh
[01:58:39.480 --> 01:58:44.360]   Ready to like thumb his nose at any, you know, like higher order government thing
[01:58:44.360 --> 01:58:46.760]   Comes up with a totally different way to do it
[01:58:47.240 --> 01:58:53.640]   And then um, there's a great movie to be made about the race to compete to to uncover the human genome and at any rate
[01:58:53.640 --> 01:58:55.240]   um
[01:58:55.240 --> 01:58:57.880]   They've already created a synthetic organism from scratch
[01:58:57.880 --> 01:59:02.920]   And the first ever there's a bacterium and it's the first ever
[01:59:02.920 --> 01:59:09.560]   Organism that has ever existed. It's a living thing that technically had computers as parents
[01:59:09.560 --> 01:59:13.960]   And it is self-replicating. Oh now see this place
[01:59:14.600 --> 01:59:17.320]   Everybody's science fiction terror
[01:59:17.320 --> 01:59:21.480]   Um, this is why i'm obsessed with the proto molecule
[01:59:21.480 --> 01:59:24.120]   Right
[01:59:24.120 --> 01:59:26.120]   It is a
[01:59:26.120 --> 01:59:29.960]   Totally frickin is and and if you've read the the books
[01:59:29.960 --> 01:59:35.640]   He's talking about the expanse and it's not a spoiler because it's really at the very beginning of the of the expanse
[01:59:35.640 --> 01:59:42.760]   It is and i'm sadly like it's the disease that that is the problem, but okay go ahead and it's it's a self-propagating
[01:59:43.320 --> 01:59:46.360]   Organism that you know the challenge is that biology
[01:59:46.360 --> 01:59:53.320]   The the the interesting thing is that pretty soon actually now we're going to be able to we can program
[01:59:53.320 --> 01:59:59.720]   Biological biological structures as though though they were tiny computers right that that's and this this concept
[01:59:59.720 --> 02:00:03.800]   Made me go down all these rabbit holes, which is how i wound up writing the book
[02:00:03.800 --> 02:00:06.360]   So read the book, but i have to say that we have
[02:00:06.360 --> 02:00:11.480]   governmentally in the past and and and as a society globally
[02:00:12.360 --> 02:00:15.000]   deprecated certain kinds of research because we were
[02:00:15.000 --> 02:00:17.960]   for instance cloning
[02:00:17.960 --> 02:00:25.240]   Remember the scientist in china who said i edited a gene and was able to eliminate
[02:00:25.240 --> 02:00:31.160]   I can't remember which disease yeah, it was aids and i will tell you that in retrospect
[02:00:31.160 --> 02:00:34.360]   He was basing a lot of his research on genetic enhancement
[02:00:34.360 --> 02:00:38.840]   So one of the offshoots of that like the not that's one of the things we've decided globally
[02:00:39.640 --> 02:00:45.000]   Not to do well you can proclaim something globally, but then practice very different things
[02:00:45.000 --> 02:00:47.800]   We got disappeared by the chinese government after doing it
[02:00:47.800 --> 02:00:54.680]   Yeah, when it becomes so easy to edit a genetic sequence as it currently is you can't keep a little
[02:00:54.680 --> 02:00:59.160]   So that's why i'm afraid no longer need state resources to be able to do something like this
[02:00:59.160 --> 02:01:05.720]   Right so here's so so one of the things that we do in the book is there's nine nine significant risks
[02:01:07.240 --> 02:01:13.000]   And one of the things that i so we we wrote these so i keep saying we i wrote this with a um
[02:01:13.000 --> 02:01:18.360]   A friend who's a microbiologist who's got a has done a ton of work in the space and andrew hass
[02:01:18.360 --> 02:01:24.520]   Andrew hassle um so we develop some scenarios again. What if right we're trying to figure out
[02:01:24.520 --> 02:01:28.120]   You know what what are possible outcomes?
[02:01:28.120 --> 02:01:29.320]   um
[02:01:29.320 --> 02:01:32.120]   One of those outcomes i i was trying to figure out
[02:01:32.680 --> 02:01:37.720]   So obviously there's like the lab leak theory and messenger rna at around the same time
[02:01:37.720 --> 02:01:42.760]   That was starting to bubble up on social media. There's academic paper published showing
[02:01:42.760 --> 02:01:48.680]   A lot of the sequencing um so the sequencing happens in the you in your local country
[02:01:48.680 --> 02:01:54.360]   But the synthesis which is to say the printing out of stuff and sending it back a lot of that's happening in china
[02:01:54.360 --> 02:01:56.040]   um
[02:01:56.040 --> 02:01:59.880]   And the there was a team of israeli researchers that were like, huh?
[02:02:00.280 --> 02:02:03.960]   I wonder if there's a way for us to inject malware into genetic code
[02:02:03.960 --> 02:02:06.600]   um and to obscure it so that
[02:02:06.600 --> 02:02:11.560]   You can't detect in the transmission that anything has happened, but you would send totally an
[02:02:11.560 --> 02:02:18.120]   Inoculous code, you know to to somebody in china and and get back not an innocuous sample
[02:02:18.120 --> 02:02:22.120]   But something that's potentially virulent and they were able to do that
[02:02:22.120 --> 02:02:28.200]   And so what that made me think of was well, you know what happens if like that actually happens in a lab
[02:02:29.160 --> 02:02:31.000]   Who do you go like who's in charge?
[02:02:31.000 --> 02:02:36.120]   So I started calling my friends at the state department and d o d and homeland security i'm calling everybody i'm like, hey
[02:02:36.120 --> 02:02:40.600]   If this actually like I wrote this crazy scenario if this actually happened
[02:02:40.600 --> 02:02:47.640]   What then and the answer is nobody knows because we are totally unprepared for a cyber
[02:02:47.640 --> 02:02:53.640]   Biological attack and it is totally plausible that something like that could happen and by the way
[02:02:54.360 --> 02:02:59.400]   You don't have to have a virus that kills millions of people for it to be horrific
[02:02:59.400 --> 02:03:01.720]   You you could invent a virus for one person
[02:03:01.720 --> 02:03:03.720]   Donald trump
[02:03:03.720 --> 02:03:08.360]   Had a terrible security team he left they left behind all kinds of garbage in fact
[02:03:08.360 --> 02:03:12.120]   There was an artist collective that collected a bunch of his stuff
[02:03:12.120 --> 02:03:14.360]   Including his dna
[02:03:14.360 --> 02:03:18.360]   Presumably yes, right at the at and others who are at weft one year
[02:03:18.360 --> 02:03:23.480]   I think and and put it up on on a website like hey if you want to buy these people like here's a fork
[02:03:23.560 --> 02:03:25.560]   You can take Donald trump's, you know, nay
[02:03:25.560 --> 02:03:29.960]   Right, so if I had that I but wait a minute the very fact so
[02:03:29.960 --> 02:03:36.040]   The very fact that the secret service should be sequestering anything with the president's dna on it
[02:03:36.040 --> 02:03:38.760]   Is a terrifying idea?
[02:03:38.760 --> 02:03:41.400]   Sure. Well, but it's
[02:03:41.400 --> 02:03:48.040]   Again, we need to we need to change our mental models because this legitimately now a possible
[02:03:48.040 --> 02:03:50.600]   This is right now. Oh, listen. I could sequence
[02:03:50.920 --> 02:03:56.920]   I could make I could potentially engineer something that doesn't have to kill somebody but it could debilitate them enough
[02:03:56.920 --> 02:04:01.320]   Maybe they've got chronic diarrhea if you've got a ceo with chronic diarrhea
[02:04:01.320 --> 02:04:05.160]   Suddenly you have fiduciary responsibility to tear your tell your shareholders
[02:04:05.160 --> 02:04:09.400]   And if not that potentially runs afoul of you know like the new
[02:04:09.400 --> 02:04:12.200]   totally
[02:04:12.200 --> 02:04:15.800]   Ransomware of your gut that's absolutely right. That's what i'm talking about
[02:04:15.800 --> 02:04:21.720]   So there's a ton of opportunity on the horizon, but but as with every technology. There's dual risk
[02:04:21.720 --> 02:04:25.160]   It doesn't mean we don't use it. I mean there's a great quote in the abin
[02:04:25.160 --> 02:04:31.320]   Yeah, we've crip blocked your hearing if you'd like it back. Please send 50 bitcoin to the following address
[02:04:31.320 --> 02:04:34.280]   Oh my goodness. This is yeah, I mean
[02:04:34.280 --> 02:04:36.520]   Tell us what the quote is
[02:04:36.520 --> 02:04:42.520]   So so it's it's a james has say quarry who's like the pen name of daniel abraham and ty frank who wrote the series
[02:04:42.520 --> 02:04:45.480]   So i'm going to read it. We're back to the humans. Yeah
[02:04:45.800 --> 02:04:51.480]   So yes, it's my obsession it killed humans. Therefore it was a weapon, but radiation killed humans
[02:04:51.480 --> 02:04:56.040]   And a medical x-ray machine wasn't intended as a weapon right? So I mean
[02:04:56.040 --> 02:04:59.480]   This is biology. We are manipulating it
[02:04:59.480 --> 02:05:04.120]   We've got right access to humanity's sort source code and we're going to do great things
[02:05:04.120 --> 02:05:10.360]   You won't need e ink in the future to change the color of your car. We'll be able to create a new change the color of your skin
[02:05:10.360 --> 02:05:13.240]   Sure to the diabetes comment earlier
[02:05:13.240 --> 02:05:16.920]   There's actually work underway to turn your skin into its own pharmacy
[02:05:16.920 --> 02:05:25.080]   So you'll be able to detect when you're having a sugar episode and automatically solve it. We may not need insulin who in the future. Yeah
[02:05:25.080 --> 02:05:31.800]   Okay, wow, but but there's all kinds of horrific risk on the horizon and at the moment
[02:05:31.800 --> 02:05:38.520]   We don't really have a plan as we never have a plan. Well, what would be the plan? It doesn't sound like there is any
[02:05:38.520 --> 02:05:41.640]   I mean government could try and make that would out lot
[02:05:41.960 --> 02:05:46.600]   Certainly people were fighting against gmo's probably incorrectly, but we're fighting against gmo's
[02:05:46.600 --> 02:05:51.160]   Uh, we have a if they've hated gmo's you're gonna really hate this
[02:05:51.160 --> 02:05:56.440]   Right. So one of the big risks and by the way, there is a whole chapter just on
[02:05:56.440 --> 02:06:00.920]   The plan and the plan is is gonna require licensing and a bunch of other things
[02:06:00.920 --> 02:06:03.240]   You know, you have to have a driver's license to drive a car
[02:06:03.240 --> 02:06:08.200]   You don't have to have a drive you don't have to have a license to operate synthetic biology is probably a mistake
[02:06:08.200 --> 02:06:11.160]   I feel like this is going to be very hard to control though because there is
[02:06:11.960 --> 02:06:15.080]   So much, I mean, but it's already out there
[02:06:15.080 --> 02:06:20.760]   For instance, we were not able to control the manufacturer atomic bomb, which was a difficult thing to do
[02:06:20.760 --> 02:06:22.680]   We did our best
[02:06:22.680 --> 02:06:28.120]   But there was so much espionage going on and there was secrets being sold that eventually the secret leaked out
[02:06:28.120 --> 02:06:33.640]   Uh and proliferation of atomic weapons has historically been a concern
[02:06:33.640 --> 02:06:37.960]   This is far worse. Is it as hard as making an atomic bomb?
[02:06:39.080 --> 02:06:42.760]   No, because one of the first experiments was no not at all
[02:06:42.760 --> 02:06:48.440]   Uh was sequencing the polio virus. I mean somebody created smallpox in a lab just to show that they could create smallpox
[02:06:48.440 --> 02:06:50.840]   I listen, I'm in a
[02:06:50.840 --> 02:06:52.200]   I I
[02:06:52.200 --> 02:06:56.920]   I don't think that there was some kind of intentional lab leak. Okay with this virus. I don't think that
[02:06:56.920 --> 02:07:02.440]   I do however think there's a strong possibility that this is the result of something called gain of function research
[02:07:02.440 --> 02:07:07.880]   Which is when scientists sort of mutate the hell out of something in order to see how bad it could get
[02:07:07.960 --> 02:07:15.080]   This was a this was a way of preparing for worse viruses, but we don't need to do that. I agree
[02:07:15.080 --> 02:07:17.240]   That's probably a bad idea. No like 20 years ago
[02:07:17.240 --> 02:07:23.640]   Maybe but but deep mind has cracked one of biology when one of science's most thorny questions like like
[02:07:23.640 --> 02:07:29.960]   Being able to figure out computationally what proteins look like we we have powerful AI tools that
[02:07:29.960 --> 02:07:35.480]   You know, we're never going to be absolute, but we can run simulations to see what probable
[02:07:35.960 --> 02:07:41.080]   Mutations so father rober. I've decided I am no longer an atheist. I would like to come to you
[02:07:41.080 --> 02:07:42.440]   and
[02:07:42.440 --> 02:07:48.840]   Request absolution. I think we're in a world of hurt. Is there this is the future of the catholic church
[02:07:48.840 --> 02:07:54.120]   We are going to store original genomes so that when people screw up their own our DNA with editing
[02:07:54.120 --> 02:07:59.640]   I want to start over you back. Yeah, we're basically cloud backups for genetics. Yeah, I want to reset, please
[02:07:59.640 --> 02:08:03.960]   Uh, this is terrifying a future you're just describing Amy
[02:08:04.600 --> 02:08:06.600]   But it's also never buying but it's
[02:08:06.600 --> 02:08:10.360]   That's inevitable. I think it's inevitable. I think it's almost impossible to control
[02:08:10.360 --> 02:08:15.160]   I think there will be great benefits from it, but I do fear that they will be outweighed
[02:08:15.160 --> 02:08:22.120]   By the diarrhea. It doesn't it doesn't have to be we're facing existential threats
[02:08:22.120 --> 02:08:27.320]   We've got a climate emergency cop 26 ended without alignment, you know, again
[02:08:27.320 --> 02:08:32.840]   Is there something we can do besides reducing carbon emissions? Absolutely you could create artificial leaves
[02:08:32.840 --> 02:08:36.120]   You could re-engineer strains of yeast you do all kinds of things
[02:08:36.120 --> 02:08:41.640]   Um, is there you know if we're gonna live on Mars right now given our current genetic makeup
[02:08:41.640 --> 02:08:48.360]   It's improbable that we're gonna survive so can we re-engineer ourselves for off planet living the answer is obviously yes
[02:08:48.360 --> 02:08:55.400]   We have fertility issues. We've got emerging health issues. I mean we have a we have a global
[02:08:55.400 --> 02:09:02.440]   Food insecurity crisis so this technology this biotechnology synthetic biology helps with all of that
[02:09:02.600 --> 02:09:05.320]   But we have once again
[02:09:05.320 --> 02:09:08.920]   Funding that is starting to spiral out of control and people
[02:09:08.920 --> 02:09:14.600]   You know once you start throwing once vc start throwing money around, you know, the the end result is usually not good
[02:09:14.600 --> 02:09:17.560]   We've got ip problems. We have people who believe in
[02:09:17.560 --> 02:09:21.080]   Scientists, but they do not believe in science
[02:09:21.080 --> 02:09:25.720]   Which is a problem. We've got misinformation and we have the threat of bio
[02:09:25.720 --> 02:09:31.160]   You know the escalation of bio warfare. So but we have some time, you know, like we just
[02:09:31.160 --> 02:09:33.160]   Doesn't sound like we have a lot of time
[02:09:33.160 --> 02:09:35.720]   And I think honestly
[02:09:35.720 --> 02:09:41.560]   COVID has been a a dress rehearsal for our complete ineptitude
[02:09:41.560 --> 02:09:45.080]   I don't think I
[02:09:45.080 --> 02:09:48.040]   Are you a pessimist or are you an optimist in this?
[02:09:48.040 --> 02:09:53.400]   I'm a futurist, uh, which means that I'm emotionally detached most of the time
[02:09:53.400 --> 02:09:56.440]   Um, I think I'm gonna go watch the expanse
[02:09:57.880 --> 02:10:05.880]   Pretend you promised me you were gonna watch it last time Leo. It's so good watch it was subtitles that first series
[02:10:05.880 --> 02:10:09.560]   I will tell you of all the books that I've written this people are like, oh my gosh
[02:10:09.560 --> 02:10:14.840]   You have you become an optimist overnight? You seem like this seems so uplifting this doesn't seem uplifting at all to me
[02:10:14.840 --> 02:10:16.840]   This seems like a terrifying
[02:10:16.840 --> 02:10:19.000]   dystopia that we're heading
[02:10:19.000 --> 02:10:21.400]   palmel into
[02:10:21.400 --> 02:10:24.360]   Well
[02:10:24.360 --> 02:10:26.360]   It's here so
[02:10:27.320 --> 02:10:33.080]   Now is the time to get educated all we've done is demonstrate the complete utter incompetence of government
[02:10:33.080 --> 02:10:35.640]   To control anything like this
[02:10:35.640 --> 02:10:39.240]   Yeah, um, that's been clear. We can't even get
[02:10:39.240 --> 02:10:43.320]   COVID under control, uh, that's nothing
[02:10:43.320 --> 02:10:46.920]   Compared to the things you described. There's a different
[02:10:46.920 --> 02:10:52.360]   What's the difference? There's an economic factor to this. This this is an arms war
[02:10:53.000 --> 02:11:00.120]   If you start having large organizations that are doing wholesale genetic manipulation and genetic sequence editing
[02:11:00.120 --> 02:11:02.600]   um, it becomes a
[02:11:02.600 --> 02:11:06.280]   As huge advantage as far as economies are concerned
[02:11:06.280 --> 02:11:13.000]   Uh, and and once you have that you do get the power of the state behind it is china again. That's where is china with this?
[02:11:13.000 --> 02:11:17.720]   Amy. I know you follow this closely. Uh, I presume they're all over this
[02:11:18.360 --> 02:11:25.080]   They very much are china has put part of its sovereign wealth fund towards synthetic biology. It sees, um, and it's no, you know
[02:11:25.080 --> 02:11:30.520]   I started really paying attention to this while I was working on my last book about the futures of ai
[02:11:30.520 --> 02:11:36.920]   Um, so so it's part of their 14th five-year plan and in their long-term strategic roadmap
[02:11:36.920 --> 02:11:41.960]   I will say this because I know this sounds and and I can I can get pretty dark pretty fast
[02:11:41.960 --> 02:11:44.200]   part part of
[02:11:44.200 --> 02:11:48.840]   Why I'm very hopeful about this and I open the book with this is that I had a whole bunch of miscarriages
[02:11:48.840 --> 02:11:51.000]   I tried and failed
[02:11:51.000 --> 02:11:57.240]   Um to to start a family and it was it was anybody who's any man woman
[02:11:57.240 --> 02:12:00.120]   Anybody who's been through this
[02:12:00.120 --> 02:12:04.520]   Uh knows how horrific it is or if you've got somebody in your life who's been through it
[02:12:04.520 --> 02:12:08.040]   You know, we've left our we've left
[02:12:08.040 --> 02:12:10.920]   Uh creating new life
[02:12:11.480 --> 02:12:16.360]   Up to complete chance and and science gives us an out it gives us optionality
[02:12:16.360 --> 02:12:21.880]   And rather than wondering what was wrong with me because because medically there was nothing wrong with me
[02:12:21.880 --> 02:12:29.080]   Um, this would have given me an an alternative my mother died fat of a a horrible death
[02:12:29.080 --> 02:12:33.720]   It was a super rare form of cancer that they couldn't figure out
[02:12:33.720 --> 02:12:38.520]   I've got another friend whose whose had childhood diabetes his entire life
[02:12:38.600 --> 02:12:45.720]   It's managed well, but it's you know, he's he's walking a different path than everybody else's and my my point is that is scary as some of the sounds
[02:12:45.720 --> 02:12:49.960]   There's enormous hope for all of us on horizon. It gives us
[02:12:49.960 --> 02:12:58.040]   Optionality to improve our everyday lives and not just us. I mean this makes medicine cheaper if if insulin goes away
[02:12:58.040 --> 02:13:04.040]   Think of how many people you know people are are there's there's a gray market for insulin on facebook
[02:13:04.520 --> 02:13:11.080]   Because it's so expensive, which is ridiculous ridiculous. So it takes we started today talking about decentralization
[02:13:11.080 --> 02:13:16.680]   For me the great promise of synthetic biology is that it it it decentralizes in some way
[02:13:16.680 --> 02:13:18.680]   I mean in the truest sense of the of the term
[02:13:18.680 --> 02:13:22.760]   You know access to some of these tools, which is a good thing
[02:13:22.760 --> 02:13:29.400]   But we have to go forward with a sort of measured and very you know, sort of pragmatic outlook on all of this
[02:13:30.440 --> 02:13:33.800]   It is at the level where somebody could be doing this in a garage though, right?
[02:13:33.800 --> 02:13:38.360]   It absolutely is there's something called iGEM if you've got kids in high school or college
[02:13:38.360 --> 02:13:42.920]   They can actually start doing this. There's elementary school kids can get involved
[02:13:42.920 --> 02:13:48.280]   Which is not the worst thing right if we think of this being the next evolution of engineering
[02:13:48.280 --> 02:13:55.720]   Okay, but why do we we are not science literate? This is a huge problem. No, it is
[02:13:55.720 --> 02:13:58.440]   right, so so I am all for
[02:13:59.080 --> 02:14:04.120]   Helping people get more literate, but again, there's always going to be fringe cases. There's a crazy guy
[02:14:04.120 --> 02:14:10.600]   Who's bio hacking himself at every turn and yeah, we've interviewed him. I know the guy. Yeah, yeah, so
[02:14:10.600 --> 02:14:18.760]   The book genesis machine our quest to rewrite life in the age of synthetic biology now I have to read it
[02:14:18.760 --> 02:14:24.280]   Uh, and then father robert i'll be over for confession later today
[02:14:25.000 --> 02:14:29.960]   Holy well welcome you with open arms and uh, we've got a guest room for you. So yeah
[02:14:29.960 --> 02:14:32.840]   Um
[02:14:32.840 --> 02:14:34.840]   It's enough to make someone turn to religion
[02:14:34.840 --> 02:14:38.440]   Well 50 years from now
[02:14:38.440 --> 02:14:43.080]   They're going to look back at this time in our history and they're going to say can you believe that?
[02:14:43.080 --> 02:14:50.040]   People used to leave their progeny to chance. Yeah, they used to leave the next generation to a genetic
[02:14:50.120 --> 02:14:54.680]   I'd feel better about that if I trusted people's choices more
[02:14:54.680 --> 02:14:59.480]   You know, uh, honestly
[02:14:59.480 --> 02:15:04.840]   I think maybe nature had a better way of doing it, uh, but you know what
[02:15:04.840 --> 02:15:10.200]   We're headed had long into the future and I can't imagine two people
[02:15:10.200 --> 02:15:18.520]   I'd rather go down that road with and these two right here. Amy you've given us so much food for thought future today institute.com
[02:15:19.480 --> 02:15:22.520]   If you are a business person a ceo
[02:15:22.520 --> 02:15:28.200]   Or if you work for the government and you're trying to figure out what the hell is going on
[02:15:28.200 --> 02:15:30.120]   This is the this is the person to talk to
[02:15:30.120 --> 02:15:36.920]   Future today institute. We do need a little more strategic planning in our life. I think we do. Let's do it right this time
[02:15:36.920 --> 02:15:43.720]   Uh, father robert ballis air. Thank you for joining us. Thank you for going to ce s so we didn't have to
[02:15:43.720 --> 02:15:47.240]   I appreciate it. Did you get your battery?
[02:15:47.240 --> 02:15:51.960]   Looks like the camera got better again. It uh, it's slightly it's slightly back
[02:15:51.960 --> 02:15:59.800]   I basically had to plug in a portable battery bank, which is actually not enough, but hopefully it'll last long. Let you go before you get exhausted again. Yeah
[02:15:59.800 --> 02:16:06.760]   What a great conversation. Um, I thank you both so much for being here just fascinating stuff
[02:16:06.760 --> 02:16:09.400]   I think so much that we need to do more of this
[02:16:09.400 --> 02:16:16.280]   Uh, we had originally scheduled some others who were not able to make it this week. Maybe we'll do this again
[02:16:16.840 --> 02:16:22.280]   Uh, with the people who we also wanted, uh, as well as you guys because I think there's so much more to talk about
[02:16:22.280 --> 02:16:24.360]   We didn't get to all of it by any means
[02:16:24.360 --> 02:16:30.200]   Uh, Amy, thank you for setting the table though and uh, what an amazing amazing conversation
[02:16:30.200 --> 02:16:32.680]   Thank you. Amy web
[02:16:32.680 --> 02:16:36.520]   Thank you for letting me go completely off the rails as usual. I appreciate it
[02:16:36.520 --> 02:16:39.880]   Awesome and and yes, I'll watch the expanse. Okay
[02:16:39.880 --> 02:16:42.280]   Uh, thank you
[02:16:42.280 --> 02:16:43.560]   I keep trying
[02:16:43.720 --> 02:16:45.560]   I lose track. I don't
[02:16:45.560 --> 02:16:47.560]   I mean, I'll read the books instead
[02:16:47.560 --> 02:16:51.160]   Maybe I'll also read the books. The books are tracking almost exactly
[02:16:51.160 --> 02:16:54.760]   The books are actually fantastic. Yeah, I think I'll do the books instead. I just just something about it
[02:16:54.760 --> 02:16:59.320]   I just have a hard time, uh engaging with it on tv. So maybe it's the books
[02:16:59.320 --> 02:17:05.480]   Um, thank you, robert come visit us as soon as it's safe. We'd love to see you again
[02:17:05.480 --> 02:17:08.360]   I'll be back in six months or so. Yeah
[02:17:09.160 --> 02:17:15.240]   Thanks so much to our fabulous audience for being here and listening to all this. It's been a wild
[02:17:15.240 --> 02:17:17.080]   wild
[02:17:17.080 --> 02:17:19.320]   Show we do to it every sunday afternoon
[02:17:19.320 --> 02:17:24.200]   Uh, it's always different. It's always interesting. It's one of the reasons I like it. We have a rotating
[02:17:24.200 --> 02:17:29.880]   A group of uh panelists and so it's always a different subject, but I think this was really great
[02:17:29.880 --> 02:17:35.080]   Uh, if you want to watch it live around 230 pacific 530 eastern
[02:17:35.560 --> 02:17:38.760]   2230 utc just go to live.twit.tv
[02:17:38.760 --> 02:17:44.360]   If you're watching live, it'd be great to chat live our chat room is uh wide open. Amy was in there, which was great
[02:17:44.360 --> 02:17:47.480]   Uh, and I know father robert frequently goes in there
[02:17:47.480 --> 02:17:50.600]   irc.twit.tv
[02:17:50.600 --> 02:17:55.800]   And I will turn on the subtitles. Amy says turn on the subtitles if you watch the expanse
[02:17:55.800 --> 02:18:02.040]   I that's my new go-to for everything. Thanks also to all the folks in our discord server
[02:18:02.120 --> 02:18:06.040]   If you're not member of club twit, you're really ought to join it seven bucks a month
[02:18:06.040 --> 02:18:10.680]   What do you get? You get add free versions of all the shows you get access to the discord where
[02:18:10.680 --> 02:18:13.240]   Discord where conversations like this happen all the time
[02:18:13.240 --> 02:18:19.320]   You also get the twit plus feed and there's content that we do that does not make it out into the podcast
[02:18:19.320 --> 02:18:24.360]   But exists only for club twit members for instance stacey higginbotham's book club
[02:18:24.360 --> 02:18:30.200]   She uh, they're reading right now autonomous by annalene newitz which covers almost all of these subjects
[02:18:30.600 --> 02:18:33.560]   It's a sci-fi work, but it's very much in this arena
[02:18:33.560 --> 02:18:38.440]   Uh, that's a great book. It's a really good book and she I love I analyze great. We interviewed her
[02:18:38.440 --> 02:18:40.440]   She's a terrific writer. Yeah, yeah
[02:18:40.440 --> 02:18:44.200]   Uh all about robots and all kinds of stuff autonomous ai
[02:18:44.200 --> 02:18:52.920]   Uh, if you uh, if you are interested just go to twit.tv/club twit is the untitled linux show. There's the giz fizz
[02:18:52.920 --> 02:19:00.280]   Um, there is the book club. We've got interviews. I any anaco will be the ask me anything subject
[02:19:00.280 --> 02:19:04.040]   This coming week and all of that's yours for a mere seven bucks a month
[02:19:04.040 --> 02:19:11.560]   Um go to twit.tv/club twit to find out more after the fact everything we do is always available for free
[02:19:11.560 --> 02:19:17.400]   No membership required. It's ad supported of course. You can find the show at twit.tv our website
[02:19:17.400 --> 02:19:21.240]   There's a youtube channel every show has its own youtube channel
[02:19:21.240 --> 02:19:25.560]   And of course, uh the best way to get it probably subscribe in your favorite podcast player
[02:19:25.560 --> 02:19:28.600]   And that way you'll get it automatically you don't even have to think about it just downloads and
[02:19:29.240 --> 02:19:32.680]   You'll have it for a monday morning commute if you so desire
[02:19:32.680 --> 02:19:35.960]   If your podcast player allows for reviews do me a favor
[02:19:35.960 --> 02:19:38.920]   You know, this is the longest running tech show
[02:19:38.920 --> 02:19:42.520]   Uh podcast out there 15 years now
[02:19:42.520 --> 02:19:44.920]   And more
[02:19:44.920 --> 02:19:50.680]   Uh, and so as a result, we don't get on the charts anymore or anything like that because you know, we're we're just old timers
[02:19:50.680 --> 02:19:57.320]   But a review would help a five star review would really help us get the word out hard to believe there people who still don't know how the twit exists
[02:19:57.560 --> 02:20:03.160]   But there are people I firmly believe who would enjoy this show who don't know about it spread the word that helps us out a lot
[02:20:03.160 --> 02:20:05.720]   Thank you all for being here
[02:20:05.720 --> 02:20:09.400]   Stay safe. We'll be back next week and other twits
[02:20:09.400 --> 02:20:12.360]   Okay
[02:20:12.360 --> 02:20:16.200]   Do
[02:20:16.200 --> 02:20:18.200]   The twit, baby
[02:20:18.200 --> 02:20:19.900]    Doin' the twit, alright 
[02:20:19.900 --> 02:20:21.740]    Doin' the twit, baby 

