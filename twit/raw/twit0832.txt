;FFMETADATA1
title=Lorem Ipsum Dolor Sit Amet
artist=Jason Snell, Harry McCracken, Rosemary Orchard, Andy Ihnatko
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2021-07-18
track=832
language=English
genre=Podcast
comment=Windows 365, Windows on iPad
encoded_by=Uniblab 5.3
date=2021
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.400]   It's time for "Twit." I'm Jason Snell, sitting in for Leo, who's floating somewhere near Hawaii, I think.
[00:00:04.400 --> 00:00:08.400]   We've got a great show, Harry McCracken, Andy and Ico, Rosemary Orchard.
[00:00:08.400 --> 00:00:12.300]   We're gonna make you laugh, we're gonna make you cry, we'll make you laugh again, I promise.
[00:00:12.300 --> 00:00:14.100]   It's all next on "Twit."
[00:00:14.100 --> 00:00:18.100]   Podcasts You Love
[00:00:18.100 --> 00:00:19.900]   From People You Trust
[00:00:19.900 --> 00:00:22.500]   This is "Twit."
[00:00:28.900 --> 00:00:34.200]   This is "Twit." Episode #832, recorded July 18, 2021.
[00:00:34.200 --> 00:00:37.200]   Laura Mipsom, Dole or Sit Amit.
[00:00:38.100 --> 00:00:42.600]   This episode of This Week at Tech is brought to you by CheckOut.com.
[00:00:42.600 --> 00:00:49.600]   Modern businesses need flexible payment systems that can help them adapt to change, grow and scale fast.
[00:00:49.600 --> 00:00:54.800]   CheckOut.com is a leading cloud-based global payment solutions provider.
[00:00:54.800 --> 00:01:00.100]   Request a free no commitment demo at checkout.com/twits.
[00:01:00.100 --> 00:01:03.600]   And by ITProTV.
[00:01:03.600 --> 00:01:07.000]   CompTIA A+ is the key to your IT career.
[00:01:07.000 --> 00:01:13.000]   And as the official video training partner for CompTIA, ITProTV will help you get certified.
[00:01:13.000 --> 00:01:18.800]   Visit itpro.tv/twit for an additional 30% off all consumer subscriptions
[00:01:18.800 --> 00:01:21.200]   for the lifetime of your active subscription.
[00:01:21.200 --> 00:01:24.200]   When you use the code "Twit30" at checkout.
[00:01:24.200 --> 00:01:26.600]   And by Mint Mobile.
[00:01:26.600 --> 00:01:32.200]   Mint Mobile's secret sauce is they're the first company to sell wireless service online only.
[00:01:32.200 --> 00:01:36.200]   To get your new wireless plan for just 15 bucks a month
[00:01:36.200 --> 00:01:41.600]   and get the plan shipped to your door free, go to mintmobile.com/twit.
[00:01:41.600 --> 00:01:44.000]   And by Endava.
[00:01:44.000 --> 00:01:49.200]   Subscribe and listen to Tech Reimagined, the podcast from Endava.
[00:01:49.200 --> 00:01:51.200]   From wherever you get your podcast.
[00:01:51.200 --> 00:01:59.200]   Welcome back to "Twit" this week in tech.
[00:01:59.200 --> 00:02:01.000]   I am not Leo Laport.
[00:02:01.000 --> 00:02:04.800]   I'm Jason Snell sitting in for Leo Laport who is on assignment.
[00:02:04.800 --> 00:02:06.200]   I think we crossed planes.
[00:02:06.200 --> 00:02:08.800]   I just got back from Hawaii and he has gone to Hawaii.
[00:02:08.800 --> 00:02:11.000]   But I'm going to take care of you for this episode.
[00:02:11.000 --> 00:02:12.200]   It's all going to be good.
[00:02:12.200 --> 00:02:13.400]   And the reason it's going to be good.
[00:02:13.400 --> 00:02:14.200]   It's not about me.
[00:02:14.200 --> 00:02:19.000]   It's about these three wonderful people who are joining me on this week's episode.
[00:02:19.000 --> 00:02:20.400]   To my left.
[00:02:20.400 --> 00:02:23.600]   He was my counterpart for many years at PC World.
[00:02:23.600 --> 00:02:26.800]   When I worked at Mac World, it's Harry McCracken from Fast Company.
[00:02:26.800 --> 00:02:28.600]   Harry, thank you for being here.
[00:02:28.600 --> 00:02:29.200]   Thank you, Sam.
[00:02:29.200 --> 00:02:30.400]   Welcome back to the mainland.
[00:02:30.400 --> 00:02:31.400]   Thank you very much.
[00:02:31.400 --> 00:02:32.400]   It's good to be back.
[00:02:32.400 --> 00:02:33.000]   I can say that.
[00:02:33.000 --> 00:02:37.400]   I was there long enough that I had a good vacation and I still felt like it was fine to return
[00:02:37.400 --> 00:02:40.800]   to the mainland from the land of the trade winds.
[00:02:40.800 --> 00:02:41.600]   Oh, OK.
[00:02:41.600 --> 00:02:42.400]   I take it back.
[00:02:42.400 --> 00:02:43.600]   More pineapple and pizza.
[00:02:43.600 --> 00:02:44.200]   Maybe.
[00:02:44.200 --> 00:02:45.200]   Also here.
[00:02:45.200 --> 00:02:52.000]   Rosemary Orchard who is the host of among other things, the Automators podcast on Relay FM.
[00:02:52.000 --> 00:02:53.000]   One of my favorites.
[00:02:53.000 --> 00:02:54.400]   Rose, welcome.
[00:02:54.400 --> 00:02:55.200]   Hello.
[00:02:55.200 --> 00:02:56.200]   Thank you for having me.
[00:02:56.200 --> 00:03:00.200]   I also host iOS today here on "Twit" for anybody who's not checked that out in the world.
[00:03:00.200 --> 00:03:04.600]   I guess if you care about the Twit Network, that's fine too.
[00:03:04.600 --> 00:03:06.400]   I know everybody's got a show on Twit.
[00:03:06.400 --> 00:03:08.000]   Everybody's got a show on Relay.
[00:03:08.000 --> 00:03:09.400]   It's all going on.
[00:03:09.400 --> 00:03:11.000]   OK, I'll do better.
[00:03:11.000 --> 00:03:11.800]   I'll do better.
[00:03:11.800 --> 00:03:16.200]   He has a show on Relay FM about Android, but I'm not going to introduce him with that.
[00:03:16.200 --> 00:03:18.800]   I'm going to say you know him every week from Mac Break Weekly.
[00:03:18.800 --> 00:03:20.200]   It's Andy and not go.
[00:03:20.200 --> 00:03:20.600]   Hello.
[00:03:20.600 --> 00:03:21.400]   How'd I do?
[00:03:21.400 --> 00:03:22.000]   How'd I do?
[00:03:22.000 --> 00:03:24.000]   I think you did just great.
[00:03:24.000 --> 00:03:28.600]   And also, I think you're really throwing off anybody who was, you know how like there's
[00:03:28.600 --> 00:03:33.000]   always those times where there's one person in the comic book who wonders, gee, isn't
[00:03:33.000 --> 00:03:38.800]   it weird that both Batman and Bruce Wayne are both in central city at the same time?
[00:03:38.800 --> 00:03:44.400]   So the fact that you and Leo the Port happened to be in Hawaii at the last time that could
[00:03:44.400 --> 00:03:48.800]   have that could have provoked some people to wonder if actually we've blown someone's
[00:03:48.800 --> 00:03:49.400]   secret identity.
[00:03:49.400 --> 00:03:55.200]   Don't know which one of us, but by putting your your LMD double back in Hawaii and coming
[00:03:55.200 --> 00:03:58.400]   back here, I think that that's going to really extend your story.
[00:03:58.400 --> 00:04:00.000]   Your secret identity another 20 years.
[00:04:00.000 --> 00:04:00.500]   Hold on.
[00:04:00.500 --> 00:04:01.400]   It's a beautiful place.
[00:04:01.400 --> 00:04:03.800]   Hawaii and Tahiti are both beautiful places.
[00:04:03.800 --> 00:04:06.600]   Anyway, that we passed the tech podcasting baton temporarily.
[00:04:06.600 --> 00:04:09.200]   And so thank you to Leo for inviting me back on.
[00:04:09.200 --> 00:04:13.100]   Now, when we do these tech podcasts, I got to be honest, I have two impulses.
[00:04:13.100 --> 00:04:16.600]   One of them is to talk about the stories that suggest that technology is terrible and
[00:04:16.600 --> 00:04:17.600]   ruining the world.
[00:04:17.600 --> 00:04:19.600]   And the other one is like, Oh, fun gadgets.
[00:04:19.600 --> 00:04:20.600]   It's wacky.
[00:04:20.600 --> 00:04:23.100]   We're going to have a little bit of both, but I want to start with a story that broke
[00:04:23.100 --> 00:04:23.600]   today.
[00:04:23.600 --> 00:04:28.200]   I just want to talk about it a little bit broken the Washington Post, a bunch of different
[00:04:28.200 --> 00:04:30.000]   news entities behind this.
[00:04:30.000 --> 00:04:36.400]   And it is the revelation that the targets of a bunch of really nasty spyware in smartphones
[00:04:36.400 --> 00:04:38.260]   have been unveiled.
[00:04:38.260 --> 00:04:43.640]   It includes people from this basically a long list of phone numbers of people who has have
[00:04:43.640 --> 00:04:46.440]   had their phones hacked supposedly.
[00:04:46.440 --> 00:04:51.560]   This report suggests that the largest number of phones being hacked was in Mexico.
[00:04:51.560 --> 00:04:58.520]   Also in the Middle East, including Qatar and Bahrain and Yemen and the UAE and Saudi Arabia,
[00:04:58.520 --> 00:05:07.800]   but also in India and in Hungary where the regime is fairly authoritarian.
[00:05:07.800 --> 00:05:12.080]   Apparently, journalists who were questioning the Hungarian authorities had their names
[00:05:12.080 --> 00:05:13.360]   added to this list.
[00:05:13.360 --> 00:05:16.840]   The idea here is that there's this company.
[00:05:16.840 --> 00:05:17.840]   They're Israeli.
[00:05:17.840 --> 00:05:18.840]   They're called the NSO group.
[00:05:18.840 --> 00:05:23.960]   They've got a bunch of hacks that they have said I think publicly are only supposed to
[00:05:23.960 --> 00:05:28.440]   be used for good, which is an interesting like, "Okay, no, we won't sell it to anyone
[00:05:28.440 --> 00:05:29.440]   bad.
[00:05:29.440 --> 00:05:30.840]   We'll only sell it to people who are good."
[00:05:30.840 --> 00:05:34.440]   But now these journalists have come up with a very long list of the people who are apparently
[00:05:34.440 --> 00:05:39.840]   on this firm's log that have potentially been hacked and have their smartphone hacked.
[00:05:39.840 --> 00:05:44.920]   So this is an interesting story because it isn't about the details of the hack as much
[00:05:44.920 --> 00:05:50.880]   as it is about the targets and who apparently mostly nations have targeted.
[00:05:50.880 --> 00:05:56.480]   And surprisingly, it is politicians, government officials, human rights activists, business
[00:05:56.480 --> 00:05:58.640]   executives and journalists.
[00:05:58.640 --> 00:06:00.800]   So this is awful.
[00:06:00.800 --> 00:06:05.040]   Hey panel, what do you think?
[00:06:05.040 --> 00:06:06.360]   Welcome to 2021.
[00:06:06.360 --> 00:06:13.040]   We got rid of COVID with abounding 49% of the population and we feel as though we're
[00:06:13.040 --> 00:06:15.120]   ready to move on to the next catastrophe.
[00:06:15.120 --> 00:06:21.000]   Yeah, it's just another indication of exactly how bad and prevalent the spyware is because
[00:06:21.000 --> 00:06:22.240]   it's no...
[00:06:22.240 --> 00:06:28.600]   When we were young, Jason, spyware was like a Matthew Broderick type per person who's
[00:06:28.600 --> 00:06:33.360]   trying to steal $1,000, $2,000, but now it is an operation.
[00:06:33.360 --> 00:06:35.720]   It is a business and it's an enterprise.
[00:06:35.720 --> 00:06:41.360]   And it is worse than that, it's a weapon that can be purchased and targeted by a lot
[00:06:41.360 --> 00:06:45.680]   of very, very bad people operating in a lot of very repressive governments.
[00:06:45.680 --> 00:06:52.720]   I mean, just last month or last week, two executives at a French spyware company were
[00:06:52.720 --> 00:06:59.240]   actually indicted by the French courts on crimes against humanity because they had sold
[00:06:59.240 --> 00:07:07.600]   spyware tools to Egypt and Libya and knowing full well that these tools could be used to
[00:07:07.600 --> 00:07:15.440]   round up people and torture them and make them lead to a very, very terrible, terrible end.
[00:07:15.440 --> 00:07:19.160]   So given that they knew what these tools are going to be used for, now we're sort of...
[00:07:19.160 --> 00:07:23.680]   The fact that we are elevating spyware to the level of you can be accused and tried
[00:07:23.680 --> 00:07:28.080]   for crimes against humanity by creating and providing these tools to someone who then
[00:07:28.080 --> 00:07:34.800]   uses it for murder and torture, that is exactly how weird the 21st century has become.
[00:07:34.800 --> 00:07:36.920]   Harry, what do you think about this?
[00:07:36.920 --> 00:07:42.680]   One of the notes that I noticed is the Jamal Khashoggi's wife and fiance were both on the
[00:07:42.680 --> 00:07:43.680]   list.
[00:07:43.680 --> 00:07:47.680]   It's unclear whether he was and of course he was then assassinated in the Saudi embassy
[00:07:47.680 --> 00:07:49.280]   in Turkey.
[00:07:49.280 --> 00:07:50.760]   There are some real...
[00:07:50.760 --> 00:07:54.240]   If you connect the dots to what has already been disclosed, it's a pretty serious list
[00:07:54.240 --> 00:07:57.760]   of people who are being surveilled apparently.
[00:07:57.760 --> 00:08:03.800]   Yes, and I think it's more evidence that well, being randomly hacked is scary.
[00:08:03.800 --> 00:08:09.520]   A lot of the really scary stuff involves very precision hacking and people are really going
[00:08:09.520 --> 00:08:11.440]   after somebody in particular.
[00:08:11.440 --> 00:08:14.640]   Unfortunately journalists sometimes end up on those lists.
[00:08:14.640 --> 00:08:22.760]   There are a lot of people who want to keep tabs on them, including authoritarian governments.
[00:08:22.760 --> 00:08:26.280]   The company behind this, I mean, it's not a hacking group.
[00:08:26.280 --> 00:08:32.320]   It's a company that sells products allegedly for use and things that might be reasonable.
[00:08:32.320 --> 00:08:38.120]   They have guidelines about how they want their software used, but it's really hard to ensure
[00:08:38.120 --> 00:08:43.160]   that when you come up with tools like these, that they will only be used for applications
[00:08:43.160 --> 00:08:48.880]   that everybody can agree might be a reasonable thing to do.
[00:08:48.880 --> 00:08:54.080]   One of the things I hear is you don't want to necessarily close exploits in smartphone
[00:08:54.080 --> 00:08:59.520]   software because it's used by intelligence agencies to gather information about the bad
[00:08:59.520 --> 00:09:03.960]   guys, but this is an interesting case where this is a company they're known.
[00:09:03.960 --> 00:09:08.120]   We've known that they've had these tools for a while now for several years, but when
[00:09:08.120 --> 00:09:13.760]   you see how it's used, it does make you realize that a lot of these arguments about, oh,
[00:09:13.760 --> 00:09:17.600]   well, these security holes, we can use them for good.
[00:09:17.600 --> 00:09:19.680]   That's a questionable argument.
[00:09:19.680 --> 00:09:21.600]   You've got to define what good is.
[00:09:21.600 --> 00:09:24.840]   There are a lot of people out there whose definition of good is radically different than
[00:09:24.840 --> 00:09:26.680]   yours or mine.
[00:09:26.680 --> 00:09:28.400]   So Rosemary, what do you think?
[00:09:28.400 --> 00:09:34.120]   Is there anything that we could call a good security hole or is that one of the lessons
[00:09:34.120 --> 00:09:35.120]   here?
[00:09:35.120 --> 00:09:39.000]   Is that, look, any hack is a bad hack?
[00:09:39.000 --> 00:09:43.800]   We've said this before whenever governments have said, we want a backdoor, but it's only
[00:09:43.800 --> 00:09:44.800]   for us.
[00:09:44.800 --> 00:09:50.040]   The any backdoor that's only for the good guys is also going to be used by the bad guys.
[00:09:50.040 --> 00:09:54.600]   Any software that's released of only use this against bad people, good and white, tend to
[00:09:54.600 --> 00:09:56.480]   be shades of gray.
[00:09:56.480 --> 00:09:58.480]   There's way more than 50 out there.
[00:09:58.480 --> 00:10:03.520]   But you can never say that every time that this particular incident happens, it's always
[00:10:03.520 --> 00:10:07.200]   for bad reasons or for good reasons.
[00:10:07.200 --> 00:10:14.360]   I think at the end of the day, this is more terrifying than random hacks because people
[00:10:14.360 --> 00:10:19.960]   are very much targeted and you can see now an entire list of people who may have known
[00:10:19.960 --> 00:10:23.640]   that they were targeted or maybe this has just come out of nowhere for them and they're
[00:10:23.640 --> 00:10:28.040]   now questioning a whole bunch of other things that have happened because they've realized
[00:10:28.040 --> 00:10:30.440]   that they were being targeted for this.
[00:10:30.440 --> 00:10:34.280]   It's like when scammers get a hold of your phone number and you fall for one scam, every
[00:10:34.280 --> 00:10:38.440]   other phone you get from then on is going to be a scam caller.
[00:10:38.440 --> 00:10:43.720]   So yeah, I think there's never a good hack for things like this.
[00:10:43.720 --> 00:10:50.600]   Unfortunately, there's always things that can be used against people in ways that they
[00:10:50.600 --> 00:10:55.400]   did not expect and that is a shame when it's the technology that they trust.
[00:10:55.400 --> 00:11:01.040]   Yeah, this is why Apple and Google always say, "Well, look, we're going to close any
[00:11:01.040 --> 00:11:05.120]   hole we can find because we can't control who uses this."
[00:11:05.120 --> 00:11:10.360]   And they may have some awareness of what nations are acting and intelligence services
[00:11:10.360 --> 00:11:12.120]   are acting in certain ways.
[00:11:12.120 --> 00:11:15.080]   But their goal as the platform owners is to close it up.
[00:11:15.080 --> 00:11:16.080]   And these are scary too.
[00:11:16.080 --> 00:11:20.120]   Again, you've probably heard about these hacks before because this has been reported
[00:11:20.120 --> 00:11:23.560]   before it's just who was on this list that is new.
[00:11:23.560 --> 00:11:27.560]   And the Guardian in the UK reported that they'll be rolling out names.
[00:11:27.560 --> 00:11:31.240]   All of these journalistic organizations that are part of this story are going to be rolling
[00:11:31.240 --> 00:11:35.120]   out details of who was targeted over this next week.
[00:11:35.120 --> 00:11:36.520]   So you'll hear more about it.
[00:11:36.520 --> 00:11:37.800]   But we have heard some of this before.
[00:11:37.800 --> 00:11:43.320]   The idea here is a text message or an iMessage in the case of iPhones is sent.
[00:11:43.320 --> 00:11:44.480]   Sometimes you have to click on a link.
[00:11:44.480 --> 00:11:48.200]   There are some exploits that actually don't even require you to click on a link.
[00:11:48.200 --> 00:11:49.600]   Just receive the message.
[00:11:49.600 --> 00:11:51.480]   Just receive silently.
[00:11:51.480 --> 00:11:54.800]   And that's all that's required, which is terrifying.
[00:11:54.800 --> 00:11:57.120]   And many of these have been plugged.
[00:11:57.120 --> 00:12:01.840]   But the idea is there are always more holes out there.
[00:12:01.840 --> 00:12:04.320]   And yeah, it's pretty terrifying.
[00:12:04.320 --> 00:12:08.720]   Especially if it's completely silent because I feel like I'm smart enough.
[00:12:08.720 --> 00:12:13.560]   I'm not going to click on any dangerous links, which is probably not true in itself.
[00:12:13.560 --> 00:12:16.360]   But at least I have some ability to protect myself.
[00:12:16.360 --> 00:12:22.440]   And if all that has to happen is that I've received a message, but I'm defenseless.
[00:12:22.440 --> 00:12:23.920]   Yeah.
[00:12:23.920 --> 00:12:28.600]   Also when you consider that oftentimes the purpose of this is to identify people that
[00:12:28.600 --> 00:12:32.400]   are next to be shot, repatriated and disappeared.
[00:12:32.400 --> 00:12:34.160]   And that's absolutely no joke.
[00:12:34.160 --> 00:12:36.480]   It's not just targeting activists.
[00:12:36.480 --> 00:12:40.280]   It's targeting activists to find out who they're messaging with, who they're making contact
[00:12:40.280 --> 00:12:41.280]   with.
[00:12:41.280 --> 00:12:42.280]   It's not just journalists.
[00:12:42.280 --> 00:12:44.160]   It's who are in their circle.
[00:12:44.160 --> 00:12:46.960]   Who can we bring in on any sort of pretext?
[00:12:46.960 --> 00:12:52.200]   It's simple things like who is actually in our country right now that we can nab just
[00:12:52.200 --> 00:12:55.000]   to simply send a message and simply act tough.
[00:12:55.000 --> 00:12:57.000]   And again, this is not a case of harassment.
[00:12:57.000 --> 00:13:02.480]   This is not a case of, oh, well, you know, thousands of send a point, oh, one Bitcoin
[00:13:02.480 --> 00:13:05.440]   to this address or else you get your files back.
[00:13:05.440 --> 00:13:09.760]   It is people just simply disappear because of intelligence like this.
[00:13:09.760 --> 00:13:14.120]   And this is why this is again, you started off with a real doozy.
[00:13:14.120 --> 00:13:18.600]   This is why this is such a damn scary piece of news.
[00:13:18.600 --> 00:13:19.600]   Yeah.
[00:13:19.600 --> 00:13:23.400]   And you can think about journalists also having being threatened into silence is another thing,
[00:13:23.400 --> 00:13:25.000]   even if they're not going to be assassinated.
[00:13:25.000 --> 00:13:28.240]   And some of some of these countries, the journalists have been killed.
[00:13:28.240 --> 00:13:31.320]   But in other cases, you may have a journalist who is silenced because they're blackmailed
[00:13:31.320 --> 00:13:32.320]   or something like that.
[00:13:32.320 --> 00:13:34.120]   Yeah, it's a fun story.
[00:13:34.120 --> 00:13:35.800]   Yeah, a lot of fun.
[00:13:35.800 --> 00:13:40.920]   My understanding is that Apple and Google have one of the shocker, one of the recent
[00:13:40.920 --> 00:13:47.320]   frontiers of security, operating system security has been how incoming text messages are processed.
[00:13:47.320 --> 00:13:48.320]   Right?
[00:13:48.320 --> 00:13:49.920]   Obviously they look at this.
[00:13:49.920 --> 00:13:55.240]   Both of these operating systems, both Android and iOS hit by similar kinds of bugs.
[00:13:55.240 --> 00:13:59.880]   And I'm sure that over the last few years, Google and Apple have put a lot of time and
[00:13:59.880 --> 00:14:05.120]   effort into kind of sandboxing as much as possible the input from text messages.
[00:14:05.120 --> 00:14:08.280]   Of course, that is the last war.
[00:14:08.280 --> 00:14:09.400]   And who knows?
[00:14:09.400 --> 00:14:13.040]   If Apple and Google know they're probably not talking about it because they don't want
[00:14:13.040 --> 00:14:14.280]   them to know that they know.
[00:14:14.280 --> 00:14:19.880]   But like, there will be a next war, another corner, a soft corner, a soft underbelly of
[00:14:19.880 --> 00:14:24.320]   a mobile operating system that will be the locus for even more attacks.
[00:14:24.320 --> 00:14:25.560]   And that's going to keep happening.
[00:14:25.560 --> 00:14:28.680]   So I guess throw your phone in the water and walk away?
[00:14:28.680 --> 00:14:29.680]   I don't know.
[00:14:29.680 --> 00:14:34.840]   It's a difficult thing because we all rely on our phones and on our communication networks.
[00:14:34.840 --> 00:14:38.400]   But scary to see this and how it's being put into use.
[00:14:38.400 --> 00:14:41.680]   Right, that's the real scary thing this time.
[00:14:41.680 --> 00:14:43.960]   I want to shift gears slightly, but only slightly.
[00:14:43.960 --> 00:14:46.640]   I swear I have some fun stories to talk about too.
[00:14:46.640 --> 00:14:51.200]   But I want to talk about something that Andy sent to me earlier this week.
[00:14:51.200 --> 00:14:52.760]   So maybe Andy, you can get us started.
[00:14:52.760 --> 00:14:59.880]   The story here, this in particular is from The Verge, is a retail stores packed with unchecked
[00:14:59.880 --> 00:15:01.600]   facial recognition.
[00:15:01.600 --> 00:15:06.400]   And the idea here is there's a campaign called Ban Facial Recognitions and Stores that have
[00:15:06.400 --> 00:15:11.600]   identified a whole bunch of stores that are using facial recognition technology on their
[00:15:11.600 --> 00:15:16.320]   customers when they're in the stores, including Apple.
[00:15:16.320 --> 00:15:18.920]   And yes, you go into that Apple store.
[00:15:18.920 --> 00:15:20.840]   And your face is getting scanned.
[00:15:20.840 --> 00:15:29.040]   And using AI to scan faces is, you know, it's been a trend, but it also makes you think,
[00:15:29.040 --> 00:15:31.040]   makes you have some privacy concerns.
[00:15:31.040 --> 00:15:32.320]   Andy, you sent this article to me.
[00:15:32.320 --> 00:15:34.400]   What do you think about it?
[00:15:34.400 --> 00:15:41.440]   It's just an indication of exactly how dangerous this technology is, how easy it is to use this
[00:15:41.440 --> 00:15:48.120]   technology to abuse the power of people who already have a little bit too much power over
[00:15:48.120 --> 00:15:52.880]   people who can be victimized without ever finding out about it.
[00:15:52.880 --> 00:15:59.040]   And finally, it's part of, do we really want to develop into a society where we assume
[00:15:59.040 --> 00:16:03.320]   that any time that we leave the house, and even sometimes when we are just walking through
[00:16:03.320 --> 00:16:09.760]   a store, walking through a bank, walking through, going through our lives, that we are being,
[00:16:09.760 --> 00:16:11.800]   we're taking part in some sort of a perp walk.
[00:16:11.800 --> 00:16:18.520]   I mean, I can't, it's, the thing is facial recognition, it's one of those really, really
[00:16:18.520 --> 00:16:23.280]   delicious sounding technologies where, hey, a camera can simply identify people, which
[00:16:23.280 --> 00:16:28.040]   means that if there's someone that has been causing problems at your bar to identify them
[00:16:28.040 --> 00:16:33.160]   as soon as he steps in, or even before they step in, or we'll be able to catch that person
[00:16:33.160 --> 00:16:37.320]   who committed that crime because of security camera, we'll be able to provide a match from
[00:16:37.320 --> 00:16:40.040]   that face to a database that we have somewhere.
[00:16:40.040 --> 00:16:45.160]   And again, that is enticing, it is very, very seductive.
[00:16:45.160 --> 00:16:52.320]   But this is, it's actually probably no accident that this story dropped this week because on
[00:16:52.320 --> 00:17:00.000]   Tuesday, the house committee had a three and a half hour long meeting with testimony about
[00:17:00.000 --> 00:17:03.440]   the use of facial recognition in policing.
[00:17:03.440 --> 00:17:08.880]   If the, the, the committee asked the general accounting office to prepare a report on, hey,
[00:17:08.880 --> 00:17:14.400]   look, we have the 42 different federal agencies that have police forces.
[00:17:14.400 --> 00:17:18.400]   How are they, how are they using facial recognition technology?
[00:17:18.400 --> 00:17:22.680]   And the answer came back, well, because there's absolutely no regulation on how they can, how
[00:17:22.680 --> 00:17:26.120]   they're supposed to use this, they're using it for just about anything they want to use
[00:17:26.120 --> 00:17:27.440]   it for.
[00:17:27.440 --> 00:17:33.760]   They are drawing upon databases, including driver's license photos, passport photos, security
[00:17:33.760 --> 00:17:38.160]   cam footage, pictures that they're getting off of social media, mugshot files.
[00:17:38.160 --> 00:17:45.040]   So there's a one part of the reports claimed that there is half of every, half of the entire
[00:17:45.040 --> 00:17:49.640]   United States population is in some sort of a facial recognition database.
[00:17:49.640 --> 00:17:51.880]   And again, no controls on it whatsoever.
[00:17:51.880 --> 00:17:57.040]   And if you think that if you, if even if we don't start discussing the racial bias that
[00:17:57.040 --> 00:18:03.120]   has been proven to, to befall all of these facial recognition technologies, the fact that
[00:18:03.120 --> 00:18:11.840]   it is a very, very prevalent tool to use to keep harassing parts of society that keep
[00:18:11.840 --> 00:18:18.320]   getting harassed, people color, Muslims, any sort of community that get harassed by a,
[00:18:18.320 --> 00:18:20.600]   by a power structure, get harassed even worse.
[00:18:20.600 --> 00:18:26.840]   This story in the Verge mentioned out that it's not even that I think it was which drug
[00:18:26.840 --> 00:18:32.920]   store, Rite Aid, I think it was, that they're using facial recognition technology, but they're
[00:18:32.920 --> 00:18:34.360]   not using it in every store.
[00:18:34.360 --> 00:18:37.840]   They're using it in stores that are in predominantly black areas.
[00:18:37.840 --> 00:18:40.800]   Wow, we could not have predicted that.
[00:18:40.800 --> 00:18:46.640]   And so this is why it kind of disturbed me that the part of the, the, the House Committee
[00:18:46.640 --> 00:18:51.520]   meeting on Tuesday was about let's discuss, like what, let's react to this report, let's
[00:18:51.520 --> 00:18:55.400]   get testimony from a whole bunch of different people and let's at least start to have discussions
[00:18:55.400 --> 00:18:57.560]   on what should be done about this.
[00:18:57.560 --> 00:19:02.560]   And there are a lot of really good arguments about here is what, here's the, we absolutely
[00:19:02.560 --> 00:19:05.200]   need laws and that was agreed upon by everybody.
[00:19:05.200 --> 00:19:08.040]   Here is the sort of regulations that we need to regulate this tightly and here's the sort
[00:19:08.040 --> 00:19:09.560]   of regulations we need.
[00:19:09.560 --> 00:19:14.960]   But nowhere was it seriously argued that this technology needs to needs a moratorium.
[00:19:14.960 --> 00:19:20.200]   There's actually a law being proposed that's, that's, that's in committee right now that
[00:19:20.200 --> 00:19:21.360]   proposes such a thing.
[00:19:21.360 --> 00:19:23.560]   It doesn't simply make it, it doesn't make it illegal.
[00:19:23.560 --> 00:19:29.320]   It just simply says that there is a moratorium at least on federal use of facial facial recognition
[00:19:29.320 --> 00:19:32.480]   technology until an act of Congress reverses it.
[00:19:32.480 --> 00:19:37.360]   And so it really disappoints me that in all of this conversation about this technology
[00:19:37.360 --> 00:19:41.920]   that so many huge corporations are prepared to make huge amounts of money on, there is
[00:19:41.920 --> 00:19:48.200]   no discussion about maybe this is just too damaging a technology for our society and
[00:19:48.200 --> 00:19:52.920]   maybe we should just keep our hands off of it until we can use it responsibly.
[00:19:52.920 --> 00:19:57.960]   Rosemary, you live in the UK where CCTV cameras are on every corner.
[00:19:57.960 --> 00:20:03.040]   What is our reasonable expectation of privacy in public?
[00:20:03.040 --> 00:20:08.920]   Well, generally the rule is that in public there is, you know, you don't expect things
[00:20:08.920 --> 00:20:11.320]   that happen to actually be private.
[00:20:11.320 --> 00:20:14.560]   But the thing is, is stores are public, they're private property.
[00:20:14.560 --> 00:20:18.960]   They're owned by, well, whoever owns a store and then whoever rents it to, you know, Walgreens
[00:20:18.960 --> 00:20:22.560]   or whoever's in it, if the company doesn't own it themselves.
[00:20:22.560 --> 00:20:26.200]   And I think it's one thing knowing that actions are on camera.
[00:20:26.200 --> 00:20:30.160]   So if there's a crime that can wind back to that time stamp, they can see it, they can
[00:20:30.160 --> 00:20:34.240]   find out, you know, okay, this is actually what happened there, where we have this and
[00:20:34.240 --> 00:20:36.320]   we can prove that this person's guilty.
[00:20:36.320 --> 00:20:40.720]   But then they have to go about identifying the people who are in that who could then potentially
[00:20:40.720 --> 00:20:42.840]   also make statements, right?
[00:20:42.840 --> 00:20:47.520]   And this is where the facial recognition gets tricky because on the one hand that, you know,
[00:20:47.520 --> 00:20:51.160]   say, you know, somebody pushes somebody else out in front of a car and you want to identify
[00:20:51.160 --> 00:20:54.160]   the other people around so that they can also make a statement and then the person who did
[00:20:54.160 --> 00:20:56.120]   the pushing gets prosecuted.
[00:20:56.120 --> 00:20:57.840]   Seems like a clear case for good.
[00:20:57.840 --> 00:21:02.160]   But then you start getting into, you know, less clear things and it's exactly the same
[00:21:02.160 --> 00:21:06.040]   as before with this technology is supposed to be used for good versus, you know, don't
[00:21:06.040 --> 00:21:08.200]   use it for bad things.
[00:21:08.200 --> 00:21:09.800]   It's always going to end up being used.
[00:21:09.800 --> 00:21:13.000]   And we've seen this here with Rite Aid, they're discriminating, they're not running it in
[00:21:13.000 --> 00:21:16.600]   every single store to use consistently across all of their stores.
[00:21:16.600 --> 00:21:20.040]   No, they've started with the low income areas.
[00:21:20.040 --> 00:21:26.120]   And I don't know about you, but my experience with people is there are good people everywhere.
[00:21:26.120 --> 00:21:29.560]   Regardless of what area they live in, regardless of what walk of life they come from, regardless
[00:21:29.560 --> 00:21:36.840]   of the color of their skin, their gender orientation, et cetera, most people are good people.
[00:21:36.840 --> 00:21:43.360]   So discriminating against certain areas just feels worse in this case because, yeah, I
[00:21:43.360 --> 00:21:44.800]   mean, it's just terrible.
[00:21:44.800 --> 00:21:51.440]   But I don't see what good facial recognition really can do in a store because what have
[00:21:51.440 --> 00:21:54.600]   stores have done before and what big companies have done before if there was a problem with
[00:21:54.600 --> 00:21:58.800]   a person is that they have photographs of that person in the security area and for the
[00:21:58.800 --> 00:22:00.720]   staff, security check that.
[00:22:00.720 --> 00:22:05.920]   And if they see that person, then, you know, they say, sorry, you need to leave.
[00:22:05.920 --> 00:22:07.800]   And what are they going to do with facial recognition?
[00:22:07.800 --> 00:22:10.920]   It's going to ping up a thing that says, hey, you need to tell this person they need to
[00:22:10.920 --> 00:22:11.920]   leave.
[00:22:11.920 --> 00:22:13.800]   You still need the security there to do the actual work.
[00:22:13.800 --> 00:22:16.920]   So why not get them to do the recognition?
[00:22:16.920 --> 00:22:22.760]   Because people tend to be better at recognizing other people versus a computer who can, which
[00:22:22.760 --> 00:22:25.440]   can make more mistakes at these things.
[00:22:25.440 --> 00:22:28.040]   I have seen, you know, my mom, I'll look my iPhone.
[00:22:28.040 --> 00:22:32.640]   My mom doesn't look that similar to me, but we now have a slightly similar haircut.
[00:22:32.640 --> 00:22:37.880]   And Siri has learned as my hair has got shorter, what I look like and my mom's got shorter
[00:22:37.880 --> 00:22:39.040]   hair as well now.
[00:22:39.040 --> 00:22:42.680]   And oh, wait, she managed to unlock my phone the other day and no, my watch wasn't on my
[00:22:42.680 --> 00:22:43.680]   wrist.
[00:22:43.680 --> 00:22:46.640]   So that is a case of facial recognition gone wrong.
[00:22:46.640 --> 00:22:49.280]   And you know, that's something, you know, innocent.
[00:22:49.280 --> 00:22:50.280]   It doesn't matter.
[00:22:50.280 --> 00:22:53.560]   But in the grand scheme of things, you know, we know that these things are not flawless.
[00:22:53.560 --> 00:22:55.600]   We know they are not perfect.
[00:22:55.600 --> 00:22:59.120]   Yeah, I'm worried about where this may end up going.
[00:22:59.120 --> 00:23:03.360]   And I'm glad that some companies have said that they will not be using it.
[00:23:03.360 --> 00:23:08.080]   But I also think that maybe state laws here to blame because I believe CCTV in the US is
[00:23:08.080 --> 00:23:13.080]   the same subject to the same restrictions as audio recordings with microphones.
[00:23:13.080 --> 00:23:16.760]   You have one party and two party states where, and that then dictates whether or not they
[00:23:16.760 --> 00:23:19.760]   have to put up signs and say whether or not there's CCTV.
[00:23:19.760 --> 00:23:23.200]   I think that those laws perhaps need to be reviewed.
[00:23:23.200 --> 00:23:30.160]   Recognition needs to have separate, you know, clauses for that and so that people can then
[00:23:30.160 --> 00:23:34.920]   perhaps choose to just do their shopping online so that they don't have to be watched.
[00:23:34.920 --> 00:23:40.440]   The idea that this is I hate to make a slippery slope argument, but the idea that, okay, well,
[00:23:40.440 --> 00:23:44.640]   we just really want to, we have a known shop lifter and we can identify who they are and
[00:23:44.640 --> 00:23:46.680]   when they come back, we don't even need to know who they are.
[00:23:46.680 --> 00:23:50.480]   But if we see them again, the security people can be alerted.
[00:23:50.480 --> 00:23:55.200]   But, you know, or you're showing your face in public, it's public and therefore what's
[00:23:55.200 --> 00:23:56.480]   the difference.
[00:23:56.480 --> 00:24:02.400]   But once you start building a facial recognition database and now it knows every path you take,
[00:24:02.400 --> 00:24:08.520]   if you merge those databases to get together, you have created a panopticon essentially.
[00:24:08.520 --> 00:24:11.360]   Everywhere you go in public is logged.
[00:24:11.360 --> 00:24:16.720]   Every store you go, every aisle you linger on and, you know, the more you talk about
[00:24:16.720 --> 00:24:20.000]   putting these things in computers and databases, again, you can say, oh, well, we're not going
[00:24:20.000 --> 00:24:21.400]   to do anything bad with it.
[00:24:21.400 --> 00:24:23.400]   Take us back to our previous story.
[00:24:23.400 --> 00:24:28.240]   The truth is most of this technology will be used in a bad way by somebody at some point
[00:24:28.240 --> 00:24:30.560]   because it's just sort of inevitable.
[00:24:30.560 --> 00:24:32.600]   And that's completely up to the whim of the retailer.
[00:24:32.600 --> 00:24:38.600]   By the way, I think Apple says that they don't use facial recognition, which is not the same
[00:24:38.600 --> 00:24:40.800]   thing as saying we've never used facial recognition.
[00:24:40.800 --> 00:24:46.800]   So it's possible that both the claim against Apple and what they're saying are correct.
[00:24:46.800 --> 00:24:51.760]   Walmart went from using Clearview AI, which is kind of a worst case scenario, in terms
[00:24:51.760 --> 00:24:57.080]   of being abusive and inaccurate, to now saying that they won't use facial recognition.
[00:24:57.080 --> 00:24:59.800]   But they could change their mind tomorrow and there's nothing to prevent them from doing
[00:24:59.800 --> 00:25:05.200]   it, which I think is one of the reasons why there's such a strong argument for legislation
[00:25:05.200 --> 00:25:06.200]   playing a role there.
[00:25:06.200 --> 00:25:12.400]   Because there are a bunch of major retailers who say they're not going to do it right now.
[00:25:12.400 --> 00:25:16.520]   But we just have to rely on their goodwill and what their feelings are right now.
[00:25:16.520 --> 00:25:22.320]   And certainly, even if you're really opposed to this technology, it's possible to be somewhat
[00:25:22.320 --> 00:25:27.120]   sympathetic to the challenges of running a large retail chain having to deal with shop
[00:25:27.120 --> 00:25:28.120]   left-aims.
[00:25:28.120 --> 00:25:33.560]   So I feel like that their concern over all the money they lose will ultimately tug them
[00:25:33.560 --> 00:25:35.960]   back to some of these technologies.
[00:25:35.960 --> 00:25:43.200]   And by the way, I am worried about the idea that going online and shopping there is a
[00:25:43.200 --> 00:25:47.240]   way to avoid being spiked on because it's just a different set of tools that they use to
[00:25:47.240 --> 00:25:48.240]   spy us online.
[00:25:48.240 --> 00:25:49.760]   But there are lots of those as well.
[00:25:49.760 --> 00:25:55.560]   So in the old days, in-person retail was kind of appealing because it felt like it was
[00:25:55.560 --> 00:25:59.720]   a little bit possible to be anonymous if you went in and used cash.
[00:25:59.720 --> 00:26:00.720]   Okay, with cash.
[00:26:00.720 --> 00:26:04.680]   But even if you use cash right now, that doesn't mean they're not figuring out who you are.
[00:26:04.680 --> 00:26:05.680]   Yeah.
[00:26:05.680 --> 00:26:11.320]   Andy, you were the one who said I was on the beach in Hawaii when that email came in.
[00:26:11.320 --> 00:26:15.960]   And I was like, this is a good story for Twit, I guess, when I get back to the mainland.
[00:26:15.960 --> 00:26:17.720]   I'll let you have the last word.
[00:26:17.720 --> 00:26:23.040]   What do you think about where we're going with this?
[00:26:23.040 --> 00:26:27.440]   This is not the future that most of us signed up for.
[00:26:27.440 --> 00:26:31.760]   We want controls over the things that are most dangerous that could be used to hurt
[00:26:31.760 --> 00:26:32.760]   us.
[00:26:32.760 --> 00:26:35.040]   I don't think it's possible to put the genie back in the bottle.
[00:26:35.040 --> 00:26:38.600]   I don't think it's possible to get an outright ban on this technology.
[00:26:38.600 --> 00:26:44.280]   Probably not possible to make it into something that cannot be privately sold to a private
[00:26:44.280 --> 00:26:49.080]   company for whatever use that they want to make of it.
[00:26:49.080 --> 00:26:54.680]   If what I'm hoping is that we don't collectively simply decide that, oh, this is a great idea.
[00:26:54.680 --> 00:26:56.080]   We should have more cameras.
[00:26:56.080 --> 00:26:57.080]   We should have...
[00:26:57.080 --> 00:27:00.240]   Why are we limiting this to just investigations?
[00:27:00.240 --> 00:27:04.840]   We should make sure that every single driver's license photo is part of this big database
[00:27:04.840 --> 00:27:07.480]   of facial recognition.
[00:27:07.480 --> 00:27:12.480]   The thing is we understand that when we lift this gun and we pull this trigger, big bang
[00:27:12.480 --> 00:27:15.000]   happens and lots of distractions happen.
[00:27:15.000 --> 00:27:18.960]   We don't understand exactly what is happening and what the damage is going to be done with
[00:27:18.960 --> 00:27:19.960]   this.
[00:27:19.960 --> 00:27:22.600]   That's the real scary thing about any new piece of technology.
[00:27:22.600 --> 00:27:27.600]   We get focused on the bang and the flash and not on well.
[00:27:27.600 --> 00:27:33.120]   This machine is designed to punch an explosive hole into whatever is aimed at.
[00:27:33.120 --> 00:27:34.520]   You can say it's for law enforcement.
[00:27:34.520 --> 00:27:36.080]   You can say it's for target shooting.
[00:27:36.080 --> 00:27:39.160]   You can say it's for game hunting.
[00:27:39.160 --> 00:27:40.600]   This is what the machine does.
[00:27:40.600 --> 00:27:44.880]   Do not ignore that this is what the machine does and what it will always do.
[00:27:44.880 --> 00:27:51.360]   I do think that in the end laws will need to be written in order to clarify what can
[00:27:51.360 --> 00:27:56.640]   and can't be done because it's one thing when facial recognition is Johnny, the security
[00:27:56.640 --> 00:28:00.160]   guard saying, "Hey, I remember that guy."
[00:28:00.160 --> 00:28:03.880]   It's another thing when there's a database and there are multiple databases and what's
[00:28:03.880 --> 00:28:08.400]   their data policy and are they sharing the data and that's when you need really somebody
[00:28:08.400 --> 00:28:12.400]   to say, "We need some rules here about how you do and don't use that."
[00:28:12.400 --> 00:28:15.720]   Hopefully we will get some at some point.
[00:28:15.720 --> 00:28:21.200]   Very quickly, one of the most important pieces of testimony was a gentleman in Michigan who
[00:28:21.200 --> 00:28:28.520]   was arrested for shoplifting based solely on a security camera that supposedly matched
[00:28:28.520 --> 00:28:34.480]   his face to the actual perpetrator's face and the investigator just said, "Okay, good
[00:28:34.480 --> 00:28:36.480]   enough for me.
[00:28:36.480 --> 00:28:41.560]   Arrested this man, African American man, in his home in front of his wife and his kids."
[00:28:41.560 --> 00:28:45.240]   As soon as they got him to the station, everybody looked at him in the picture and said, "No,
[00:28:45.240 --> 00:28:46.240]   no way.
[00:28:46.240 --> 00:28:51.320]   This is clearly not the same person, but he took them all day to be at the police station."
[00:28:51.320 --> 00:28:56.280]   Yes, it's true that the investigation, I believe the investigator actually got beaten
[00:28:56.280 --> 00:29:01.200]   down a rank and punished because the entire department was saying, "This is a disgrace.
[00:29:01.200 --> 00:29:05.200]   This is not a proper investigation," but it just goes to show with the lack of any
[00:29:05.200 --> 00:29:06.200]   regulations whatsoever.
[00:29:06.200 --> 00:29:10.760]   It is possible for someone to simply say, "Well, the computer says there's an 82% chance
[00:29:10.760 --> 00:29:12.440]   that this is these two of the same people.
[00:29:12.440 --> 00:29:14.480]   I'm going to shoot an arrest warrant."
[00:29:14.480 --> 00:29:20.800]   All right, well, I promise I'll come up with something a little more happy in the next
[00:29:20.800 --> 00:29:21.800]   segment.
[00:29:21.800 --> 00:29:26.760]   But before that, what would make you happier than hearing the beautiful voice of Leo Laporte
[00:29:26.760 --> 00:29:29.360]   telling you about something that you should be interested in?
[00:29:29.360 --> 00:29:30.360]   Leo?
[00:29:30.360 --> 00:29:31.360]   Hey, kids.
[00:29:31.360 --> 00:29:32.360]   Leo Laporte here.
[00:29:32.360 --> 00:29:33.760]   Yes, I know him in Hawaii.
[00:29:33.760 --> 00:29:36.000]   Thank you, Jason Snell, for filling in.
[00:29:36.000 --> 00:29:38.000]   It's been a great show so far.
[00:29:38.000 --> 00:29:39.080]   I hope you don't mind.
[00:29:39.080 --> 00:29:43.560]   I just want to interrupt to introduce a brand new sponsor to you all.
[00:29:43.560 --> 00:29:45.680]   Check out.com.
[00:29:45.680 --> 00:29:54.800]   As much as we love technology, it shouldn't get in the way whether it's an e-commerce
[00:29:54.800 --> 00:29:58.320]   platform or just interrupting innovation.
[00:29:58.320 --> 00:30:04.560]   I have to say that we are currently burdened with some obsolete payment systems.
[00:30:04.560 --> 00:30:09.880]   They're heavily layered, they're disconnected, they're a cost center to the business.
[00:30:09.880 --> 00:30:10.800]   It's not what you need.
[00:30:10.800 --> 00:30:15.640]   If you're a modern business, you need a flexible payment system that can help you adapt, change,
[00:30:15.640 --> 00:30:18.440]   grow, scale fast.
[00:30:18.440 --> 00:30:23.040]   I want to tell you about a company with tech that approaches payments through a radical
[00:30:23.040 --> 00:30:24.400]   new lens.
[00:30:24.400 --> 00:30:25.560]   It's got a great name.
[00:30:25.560 --> 00:30:29.160]   Easy to remember, check out.com.
[00:30:29.160 --> 00:30:31.720]   Check out did a lot of research on this space.
[00:30:31.720 --> 00:30:36.800]   They partnered with Oxford Economics, did an end-to-end analysis of the payments value
[00:30:36.800 --> 00:30:39.240]   chain for merchants.
[00:30:39.240 --> 00:30:44.280]   One big problem, and I think everybody who does this knows, is to climb credit cards
[00:30:44.280 --> 00:30:46.160]   for a variety of reasons.
[00:30:46.160 --> 00:30:47.160]   They cost a lot.
[00:30:47.160 --> 00:30:54.960]   They cost the UK, US, French and German markets a total of $20.3 billion last year alone.
[00:30:54.960 --> 00:30:58.800]   $12.7 billion went to competitors.
[00:30:58.800 --> 00:31:02.120]   $7.6 billion completely written off.
[00:31:02.120 --> 00:31:04.040]   That's shocking by itself, but there's more.
[00:31:04.040 --> 00:31:08.480]   The study found merchants are not currently optimizing the consumers' significant willingness
[00:31:08.480 --> 00:31:12.800]   to pay for speed, convenience, and security online.
[00:31:12.800 --> 00:31:18.160]   Most merchants do not feel their payments data is informing their business strategy
[00:31:18.160 --> 00:31:19.680]   or their innovation.
[00:31:19.680 --> 00:31:26.040]   In fact, 56% of the customers surveyed said that they won't come back to a site because
[00:31:26.040 --> 00:31:28.880]   the site doesn't offer their preferred payment method.
[00:31:28.880 --> 00:31:30.280]   They don't make it easy.
[00:31:30.280 --> 00:31:32.000]   Here's another shocking statistic.
[00:31:32.000 --> 00:31:37.960]   Most merchants spend more than 10% of their payments budget, fixing disputes, fraud, outages.
[00:31:37.960 --> 00:31:39.560]   This is costing you.
[00:31:39.560 --> 00:31:44.840]   CEOs are more likely to overestimate the quality of data returned to them by payment providers.
[00:31:44.840 --> 00:31:50.760]   CEOs tend to underestimate the extent to which disconnected payments are hindering growth.
[00:31:50.760 --> 00:31:56.200]   Most merchants focus on the per transaction costs of their payments, ignoring the back-end
[00:31:56.200 --> 00:31:57.200]   costs.
[00:31:57.200 --> 00:32:03.240]   This research really helped check out to find clearly what their model was going to be.
[00:32:03.240 --> 00:32:08.640]   They are a leading, cloud-based global payment solution provider.
[00:32:08.640 --> 00:32:15.160]   Their payment platform is purpose-built with some very important priorities, simplicity,
[00:32:15.160 --> 00:32:17.280]   scalability, and speed.
[00:32:17.280 --> 00:32:20.880]   They're perfect for any merchant wants to seamlessly integrate better payment solutions
[00:32:20.880 --> 00:32:21.880]   globally.
[00:32:21.880 --> 00:32:27.680]   If you get improved acceptance globally, better and more actionable granular data, you get
[00:32:27.680 --> 00:32:31.920]   a flexible product structure that merchants can adapt to their needs, and it's combined
[00:32:31.920 --> 00:32:35.800]   with really personal white glove service.
[00:32:35.800 --> 00:32:41.880]   Check out the brands they're using, checkout.com across the globe, Pizza Hut, TransferWise,
[00:32:41.880 --> 00:32:47.200]   Klarna, Revolut, Samsung, all use checkout.com.
[00:32:47.200 --> 00:32:54.280]   I know when I use a payment system, if it's not fast, if it's not easy, if I don't feel
[00:32:54.280 --> 00:33:00.880]   like I can trust it, I am either going to end that transaction right there before I push
[00:33:00.880 --> 00:33:04.200]   the buy button or I'm not coming back.
[00:33:04.200 --> 00:33:06.800]   That's why checkout.com is so important.
[00:33:06.800 --> 00:33:08.360]   It builds trust with your customers.
[00:33:08.360 --> 00:33:09.440]   It's speedy.
[00:33:09.440 --> 00:33:11.000]   It's simple.
[00:33:11.000 --> 00:33:12.200]   Customers love it.
[00:33:12.200 --> 00:33:13.200]   Check out.com.
[00:33:13.200 --> 00:33:17.000]   It's the dominant choice for organizations that are looking for the fastest, most innovative
[00:33:17.000 --> 00:33:19.880]   and reliable global payment solution provider.
[00:33:19.880 --> 00:33:26.480]   Look, I want you to request a free, there's no commitment demo at checkout.com/twit.
[00:33:26.480 --> 00:33:30.000]   Check out.com/twit.
[00:33:30.000 --> 00:33:31.040]   Get your free demo.
[00:33:31.040 --> 00:33:33.560]   Check out.com/twit.
[00:33:33.560 --> 00:33:36.520]   We thank them so much for supporting this week in tech.
[00:33:36.520 --> 00:33:37.520]   Appreciate it.
[00:33:37.520 --> 00:33:38.520]   Thank you, checkout.
[00:33:38.520 --> 00:33:42.360]   And please, if you want to check out, check out.
[00:33:42.360 --> 00:33:47.440]   Use that address so they know you saw it here at checkout.com/twit.
[00:33:47.440 --> 00:33:49.080]   Now back to the show.
[00:33:49.080 --> 00:33:50.080]   Thank you, Leo.
[00:33:50.080 --> 00:33:51.080]   Okay.
[00:33:51.080 --> 00:33:52.080]   What can we do?
[00:33:52.080 --> 00:33:53.240]   What can we do next?
[00:33:53.240 --> 00:33:57.360]   Well, sad news of a sort, but it's not going to bring you down like some other sad news.
[00:33:57.360 --> 00:33:59.760]   I've got some Twitter news for you.
[00:33:59.760 --> 00:34:02.440]   It's no more.
[00:34:02.440 --> 00:34:04.320]   Twitter is shutting down fleets.
[00:34:04.320 --> 00:34:08.520]   If you thought that every single site in existence was going to have its own version
[00:34:08.520 --> 00:34:15.320]   of Instagram stories, Twitter won't anymore because that's what fleets was.
[00:34:15.320 --> 00:34:17.520]   I'm going to take this as good news.
[00:34:17.520 --> 00:34:18.520]   I don't know.
[00:34:18.520 --> 00:34:21.880]   I know all of my panelists are on Twitter just like I am.
[00:34:21.880 --> 00:34:28.400]   I love the idea that Twitter rolled out a feature, looked at it, said, "Nah, you know
[00:34:28.400 --> 00:34:30.640]   what, it doesn't really work.
[00:34:30.640 --> 00:34:34.280]   We're going to get rid of it and move on and do something different."
[00:34:34.280 --> 00:34:39.640]   So even though I'm sure that the five people who really liked fleets are going to be sad,
[00:34:39.640 --> 00:34:42.240]   I'm choosing this as a positive.
[00:34:42.240 --> 00:34:45.920]   I want to view this as a positive panelists.
[00:34:45.920 --> 00:34:47.080]   You're going to miss fleets.
[00:34:47.080 --> 00:34:48.800]   Is this okay?
[00:34:48.800 --> 00:34:51.800]   You've morning sackcloth ashes weeping?
[00:34:51.800 --> 00:34:57.520]   How can I miss fleets when I never actually fleet it or check them out?
[00:34:57.520 --> 00:35:03.560]   It turns out that Twitter got excited about fleets, which are really a knockoff of stories,
[00:35:03.560 --> 00:35:08.120]   which were invented by Snapchat and then became even more popular on Instagram and then we're
[00:35:08.120 --> 00:35:09.120]   copied by everybody.
[00:35:09.120 --> 00:35:10.120]   Literally everybody.
[00:35:10.120 --> 00:35:13.640]   Twitter thought it might be a way to get people who lurk in Twitter and don't actually
[00:35:13.640 --> 00:35:17.120]   create content to do stuff.
[00:35:17.120 --> 00:35:20.120]   That's the eternal challenge for Twitter is that most of the tweets come from a very
[00:35:20.120 --> 00:35:21.480]   small group of people.
[00:35:21.480 --> 00:35:23.280]   Yes, who used fleets?
[00:35:23.280 --> 00:35:28.040]   So they launched fleets and it turned out that people who already loved doing stuff on Twitter
[00:35:28.040 --> 00:35:34.600]   and tweeting also liked fleeting, but these kind of casual lurkers did not use them.
[00:35:34.600 --> 00:35:39.120]   So they did not actually really fulfill the goal that Twitter had for itself.
[00:35:39.120 --> 00:35:40.280]   So it makes perfect sense.
[00:35:40.280 --> 00:35:46.840]   I suspect you will see some of the other many, many apps and services that knocked off stories
[00:35:46.840 --> 00:35:54.720]   also rethinking them because they make a lot of sense in a certain number of instances,
[00:35:54.720 --> 00:36:03.480]   but it's not like LinkedIn needs stories or Slack or Twitter turned out.
[00:36:03.480 --> 00:36:09.400]   Andy, are you mourning fleets or is this good that Twitter is mature enough to give up?
[00:36:09.400 --> 00:36:12.320]   You know, I thought there were more people at mass this morning.
[00:36:12.320 --> 00:36:15.120]   I thought that the prayers were a little bit more solemn.
[00:36:15.120 --> 00:36:17.400]   I didn't know why and now I guess I know why.
[00:36:17.400 --> 00:36:19.640]   I mean, it hits the Catholics harder.
[00:36:19.640 --> 00:36:24.840]   So really when Twitter makes a makes a makes a feature to leech on like that, sick, no
[00:36:24.840 --> 00:36:25.840]   fleet.
[00:36:25.840 --> 00:36:27.480]   Um, exactly.
[00:36:27.480 --> 00:36:28.480]   Something.
[00:36:28.480 --> 00:36:33.920]   No, no, no going forth to favor it and like the Lord, but there is the thing.
[00:36:33.920 --> 00:36:41.360]   I mean, I the biggest problem of any service or any software really anything is when they
[00:36:41.360 --> 00:36:45.880]   flew sight of who they are, what they're about and why people like them.
[00:36:45.880 --> 00:36:51.040]   Instagram could integrate something like this very, very well because it really is about
[00:36:51.040 --> 00:36:56.040]   I want to I want to see a little piece of media from somebody that doesn't take that
[00:36:56.040 --> 00:36:58.320]   long to really access or digest.
[00:36:58.320 --> 00:36:59.320]   Perfect fit.
[00:36:59.320 --> 00:37:03.720]   Twitter people every time Twitter tries to be something other than Twitter people reminded
[00:37:03.720 --> 00:37:05.560]   that we like you because you're Twitter.
[00:37:05.560 --> 00:37:08.760]   You don't you don't have to don't you don't depart your hair on a different side.
[00:37:08.760 --> 00:37:14.240]   You don't have to start liking, you know, albums from Bowie's Berlin days.
[00:37:14.240 --> 00:37:19.480]   It just be you and because this is this is why we have given you this tentacle like grip
[00:37:19.480 --> 00:37:25.120]   of control and power over our society because the thing that the the the the mechanism by
[00:37:25.120 --> 00:37:28.480]   which you make a suffer is very, very effective, very, very addictive.
[00:37:28.480 --> 00:37:31.640]   And we we underscribe it very, very strongly.
[00:37:31.640 --> 00:37:34.560]   Don't try to make yourself into a multimedia platform.
[00:37:34.560 --> 00:37:35.560]   You're not one.
[00:37:35.560 --> 00:37:36.720]   And we have plenty of options.
[00:37:36.720 --> 00:37:39.800]   Rosemary, do you miss fleets?
[00:37:39.800 --> 00:37:41.200]   Are you going to already miss them?
[00:37:41.200 --> 00:37:43.280]   How can you miss them if they're already gone?
[00:37:43.280 --> 00:37:44.280]   I don't know.
[00:37:44.280 --> 00:37:45.880]   I mean, stop trying to make fleets happen.
[00:37:45.880 --> 00:37:47.760]   It's just not going to happen.
[00:37:47.760 --> 00:37:50.840]   So I thought fleets were coming soon.
[00:37:50.840 --> 00:37:54.280]   And then I saw this news and went, Oh, they already happened.
[00:37:54.280 --> 00:37:58.680]   So yeah, I never fleet it.
[00:37:58.680 --> 00:37:59.680]   That's probably a good thing.
[00:37:59.680 --> 00:38:05.000]   I rarely use Instagram, but I wonder if part of the problem with this is Twitter didn't
[00:38:05.000 --> 00:38:09.960]   make this available to everybody else through their API as far as I could tell.
[00:38:09.960 --> 00:38:11.920]   To this way, tweet bot never implemented it.
[00:38:11.920 --> 00:38:14.080]   I use tweet bot to access Twitter.
[00:38:14.080 --> 00:38:17.680]   Therefore, I did not see fleets full stop.
[00:38:17.680 --> 00:38:22.880]   And I feel like maybe this is where apps like Instagram and so on can do much better because
[00:38:22.880 --> 00:38:26.400]   if you want to access Instagram, you have to use the Instagram app and therefore they
[00:38:26.400 --> 00:38:32.600]   can roll features out using their A, B, Z testing or, you know, just to everybody, whatever
[00:38:32.600 --> 00:38:34.680]   they want to do.
[00:38:34.680 --> 00:38:38.680]   But yeah, Twitter is not Instagram.
[00:38:38.680 --> 00:38:41.880]   And I think they should probably stop trying to make fleets happen.
[00:38:41.880 --> 00:38:45.200]   So it's yeah, I never quite got the idea of it.
[00:38:45.200 --> 00:38:47.600]   Like why would I want to do this on Twitter?
[00:38:47.600 --> 00:38:50.120]   I have Instagram for that.
[00:38:50.120 --> 00:38:51.120]   Yeah.
[00:38:51.120 --> 00:38:52.120]   Yeah.
[00:38:52.120 --> 00:38:53.440]   I think there was a mismatch there for sure.
[00:38:53.440 --> 00:38:56.720]   I do have a twist to this story, by the way, the positive twist here is that Twitter is
[00:38:56.720 --> 00:39:02.240]   also making some other things happen that might actually be worth making happen.
[00:39:02.240 --> 00:39:06.320]   But I do want to mention Twitter bot a few weeks ago, a great website.
[00:39:06.320 --> 00:39:10.560]   They bought a company that happened to have already bought this website called Nuzzle.
[00:39:10.560 --> 00:39:15.200]   And if you don't know about Nuzzle, Nuzzle was a news aggregator.
[00:39:15.200 --> 00:39:19.200]   It was kind of a news reader except it used your Twitter feeds and other people's Twitter
[00:39:19.200 --> 00:39:25.560]   feeds or Twitter lists as the raw material and would parse those links and see like if
[00:39:25.560 --> 00:39:30.360]   a lot of your friends linked to a story, that would be the top story on your Nuzzle page.
[00:39:30.360 --> 00:39:37.320]   And it was a really nice way to turn a Twitter feed into a news feed instead of a discussion
[00:39:37.320 --> 00:39:40.640]   feed because sometimes you want to see the discussion and sometimes you just follow a
[00:39:40.640 --> 00:39:44.000]   bunch of smart people who have got a lot of good links.
[00:39:44.000 --> 00:39:46.760]   And Nuzzle is now shut down, which makes me sad.
[00:39:46.760 --> 00:39:51.880]   But in announcing the acquisition, Twitter said they were going to implement, they were
[00:39:51.880 --> 00:39:54.040]   looking toward implementing a version of Nuzzle on Twitter.
[00:39:54.040 --> 00:39:55.440]   And I thought, well, there you go.
[00:39:55.440 --> 00:39:57.640]   That's a feature that probably should be part of Twitter.
[00:39:57.640 --> 00:39:59.520]   It probably worked better as part of Twitter.
[00:39:59.520 --> 00:40:01.920]   The idea that you could sort of have a news view into Twitter.
[00:40:01.920 --> 00:40:03.160]   So I'm hopeful there.
[00:40:03.160 --> 00:40:07.040]   And what gives me more hope that maybe Twitter's product group has turned it around a little
[00:40:07.040 --> 00:40:13.780]   bit is another announcement this week, which is that Twitter on iOS now lets you edit who
[00:40:13.780 --> 00:40:17.240]   can reply to your old tweets.
[00:40:17.240 --> 00:40:21.400]   Now this is a feature that they rolled out for new tweets where you could say only my
[00:40:21.400 --> 00:40:26.360]   followers or only the people mentioned here or nobody can reply to this tweet.
[00:40:26.360 --> 00:40:29.800]   And that was a little bit interesting but a little bit weird.
[00:40:29.800 --> 00:40:34.000]   This new feature though I really like because it suggests that somebody at Twitter knows
[00:40:34.000 --> 00:40:35.680]   about dog piling.
[00:40:35.680 --> 00:40:41.960]   Finally, the idea that you do a perfectly innocuous tweet and then somewhere, somehow
[00:40:41.960 --> 00:40:46.760]   a large group of people come and start replying to it and harassing you.
[00:40:46.760 --> 00:40:50.640]   And with this new feature, Twitter's showing, hey, wouldn't it be nice if you could take
[00:40:50.640 --> 00:40:53.000]   that tweet that you regret?
[00:40:53.000 --> 00:40:56.040]   You don't regret the text, you regret that it's being engaged with in this way and sort
[00:40:56.040 --> 00:41:00.920]   of say, yeah, I don't want anybody replying to that anymore or only the people I was talking
[00:41:00.920 --> 00:41:03.280]   to and do it after the fact.
[00:41:03.280 --> 00:41:07.080]   I think it's so smart and it's one of the first examples I've seen in a long time of
[00:41:07.080 --> 00:41:11.600]   Twitter actually getting how its service is not just used but misused.
[00:41:11.600 --> 00:41:15.920]   Twitter has always had this reputation for kind of overthinking everything and then being
[00:41:15.920 --> 00:41:20.520]   paralyzed by the idea of changing anything and thinking it over endlessly.
[00:41:20.520 --> 00:41:26.040]   So I'd say that both Twitter getting rid of things and Twitter adding things, they're
[00:41:26.040 --> 00:41:27.800]   both promising signs.
[00:41:27.800 --> 00:41:33.640]   Yeah, I don't know if Andy and Rose, I don't know if you've been dog piled before but like
[00:41:33.640 --> 00:41:38.120]   I've had those innocuous tweets go where it goes into some other subculture and they
[00:41:38.120 --> 00:41:43.600]   have interpreted it in a totally weird way that was never intended and I would love to
[00:41:43.600 --> 00:41:48.680]   shut it down and it's too late and now with this feature you could shut it down.
[00:41:48.680 --> 00:41:54.360]   Yeah, but the big problem is when it's something you said like four or five years ago that's
[00:41:54.360 --> 00:42:01.880]   now stripped of all context and I feel as though if you said it maybe you should keep
[00:42:01.880 --> 00:42:10.560]   it up there but there's something doubly stinging about if when it becomes reactivated and fresh
[00:42:10.560 --> 00:42:15.440]   batteries and fresh charge are put into it and now people are getting social media cred
[00:42:15.440 --> 00:42:22.400]   over replying and retweeting it and making their replies get lots of lights and lots
[00:42:22.400 --> 00:42:27.440]   of retweets and then somebody who is working for a website who has to have three things
[00:42:27.440 --> 00:42:31.600]   up over the course of the afternoon wants to, oh suddenly this is suddenly this is a trending
[00:42:31.600 --> 00:42:34.720]   topic and there's nothing here's something else to really talk about.
[00:42:34.720 --> 00:42:36.240]   So yeah, you're absolutely right.
[00:42:36.240 --> 00:42:45.400]   I think that Twitter is becoming a lot more conscious of how its service can be both direct
[00:42:45.400 --> 00:42:51.520]   and directly weaponized by bad people and also just incidentally just cause the storm
[00:42:51.520 --> 00:42:57.400]   cloud to hover over someone who really doesn't deserve to be reigned on and they're trying
[00:42:57.400 --> 00:43:00.600]   to address this step by step by step wherever they can.
[00:43:00.600 --> 00:43:03.960]   So yeah, I hope this is part of a growing trend.
[00:43:03.960 --> 00:43:09.120]   I had a tweet in the early days when I was at Macworld, early days of the iPhone, early
[00:43:09.120 --> 00:43:12.760]   days of Twitter and it was one of those days where like there were lines around the block
[00:43:12.760 --> 00:43:16.800]   for the iPhone 3GS or something and I tweeted and it was back in the day when Twitter was
[00:43:16.800 --> 00:43:20.360]   super casual like literally you wouldn't, there weren't retweets, you couldn't really
[00:43:20.360 --> 00:43:26.480]   put quote tweets, there was no context and I did a tweet that was like yeah, sarcastically,
[00:43:26.480 --> 00:43:32.240]   yeah I guess this, this iPhone thing is not going to work out and on an iPhone anniversary
[00:43:32.240 --> 00:43:35.960]   like two years ago I suddenly got dunked on by all these people who were like get a load
[00:43:35.960 --> 00:43:39.880]   of this guy who didn't think the iPhone would be successful and it was like that's not what
[00:43:39.880 --> 00:43:43.960]   that tweet was at all but again it was just raw material for somebody to kind of stoke
[00:43:43.960 --> 00:43:49.280]   the hate machine on Twitter and if I had this feature what I turn off all replies to that
[00:43:49.280 --> 00:43:53.260]   tweet and just quiet it down, yeah I probably would instead I just deleted it because I
[00:43:53.260 --> 00:43:59.160]   stand by my sarcastic comment about the iPhone 3GS line I guess but it's not worth it so
[00:43:59.160 --> 00:44:01.160]   this is why people delete old tweets I guess.
[00:44:01.160 --> 00:44:04.920]   It would be nice to auto-freeze comments on old tweets that would make them.
[00:44:04.920 --> 00:44:10.040]   Yeah and again I feel like this move makes me think Twitter actually has some people
[00:44:10.040 --> 00:44:14.040]   getting features implemented that understand all the ways that Twitter is used kind of in
[00:44:14.040 --> 00:44:19.240]   an unpleasant way that maybe they can shut down and make it a better thing.
[00:44:19.240 --> 00:44:24.240]   Yeah, yeah I think we all know people who have a service go through and delete old tweets
[00:44:24.240 --> 00:44:28.040]   some people have it very aggressively delete anything other than 24 hours other people
[00:44:28.040 --> 00:44:32.680]   leave it a couple of weeks or a month or so and you know that's just proof that something
[00:44:32.680 --> 00:44:35.160]   like this is needed and it should be automated.
[00:44:35.160 --> 00:44:40.200]   I'm not saying that because I love automation I'm saying that because realistically after
[00:44:40.200 --> 00:44:45.320]   a month you probably don't want people replying to your tweets after a month has passed you
[00:44:45.320 --> 00:44:50.760]   know Twitter is what 280 characters that is not a lot of text things get taken out of context
[00:44:50.760 --> 00:44:56.200]   incredibly easily even from a full length novel so it's no surprise that that happens on Twitter
[00:44:56.200 --> 00:44:57.200]   all the time.
[00:44:57.200 --> 00:45:01.160]   Right I like having my old tweets because I can there's sometimes there's gold back there
[00:45:01.160 --> 00:45:05.200]   where you quote something you're like oh my god can you believe this happened five years
[00:45:05.200 --> 00:45:10.040]   ago or whatever I hate to delete them all but I understand why people do and yeah I like
[00:45:10.040 --> 00:45:14.080]   that idea that's you know you see that in forums all the time too the idea that forum
[00:45:14.080 --> 00:45:19.280]   threads get locked after a while because you know why continue reviving this thing after
[00:45:19.280 --> 00:45:21.240]   the fact all the context has been lost.
[00:45:21.240 --> 00:45:25.400]   Well anyway good job whoever out there did that for Twitter thumbs up to you.
[00:45:25.400 --> 00:45:30.640]   But Rose had a really good point that that would be a lovely feature to simply say every
[00:45:30.640 --> 00:45:35.120]   all my tweets should expire after 30 days just delete them after 30 days.
[00:45:35.120 --> 00:45:36.120]   Or lock them.
[00:45:36.120 --> 00:45:38.000]   Or lock them.
[00:45:38.000 --> 00:45:43.460]   But the good thing about deleting them is that if your if your personal philosophy and
[00:45:43.460 --> 00:45:47.800]   policy is that none of the things that I put on this Twitter account are going to be really
[00:45:47.800 --> 00:45:52.640]   relevant a month from now two months from now three months from now so I may as well get
[00:45:52.640 --> 00:45:57.640]   rid of them so that people don't people people will understand that I said this at a time
[00:45:57.640 --> 00:46:03.080]   when the Red Sox were in game six and looks like they're going to be thrown out of the
[00:46:03.080 --> 00:46:07.880]   postseason again and I was really really on edge and so I said some angry things about
[00:46:07.880 --> 00:46:11.760]   the pitching staff that maybe I regret saying right now.
[00:46:11.760 --> 00:46:13.200]   I'm very excited right now.
[00:46:13.200 --> 00:46:19.160]   But the thing is if I had gone back and selectively deleted stuff that I thought was going to
[00:46:19.160 --> 00:46:24.080]   get me in trouble that's kind of suspicious in and of itself people should ask gee why
[00:46:24.080 --> 00:46:30.200]   is it that people seem to be mentioning something related to a tweet that doesn't exist anymore.
[00:46:30.200 --> 00:46:35.080]   Whereas if you simply say well look of course there's if you want to find out my background
[00:46:35.080 --> 00:46:40.080]   by checking my Twitter feed I mean just as a matter of policy nothing over 30 days old
[00:46:40.080 --> 00:46:44.920]   is still on my feed I just have it automatically deleted there's no malice about this I don't
[00:46:44.920 --> 00:46:49.920]   pick and choose to make myself look smarter or more more enlightened I just simply have
[00:46:49.920 --> 00:46:56.000]   these have this policy where 30 days 30 days is the freshen state just like a Twinkie.
[00:46:56.000 --> 00:47:01.920]   I think the idea here is that you're you're giving Twitter users choices that are features
[00:47:01.920 --> 00:47:06.880]   that really need to be in Twitter and there are these weird things like Rose mentioned
[00:47:06.880 --> 00:47:12.880]   you can wire up a weird API kind of app that will go and auto delete your tweets but really
[00:47:12.880 --> 00:47:16.880]   that should be a feature of Twitter you know the it shouldn't be a third party opportunity
[00:47:16.880 --> 00:47:22.640]   and likewise locking tweets should be a feature of Twitter and and I'm not maybe I'm reading
[00:47:22.640 --> 00:47:27.040]   too much into the tea leaves here but I look at this feature and the ability to lock these
[00:47:27.040 --> 00:47:34.200]   tweets manually after a while and I think oh a good sign that maybe they get it so more
[00:47:34.200 --> 00:47:40.840]   please I guess I would say to Twitter more please I want to pivot here to something that
[00:47:40.840 --> 00:47:46.600]   I found really really fun which is what can't you do on an iPad now I'm a big iPad fan
[00:47:46.600 --> 00:47:52.120]   I didn't bring my iPad today but I'm a huge iPad fan and I've been watching Harry McCracken's
[00:47:52.120 --> 00:47:57.840]   Twitter feed about all the things he's been doing with a particular app called iDOS
[00:47:57.840 --> 00:48:01.520]   include which led this week to bench Edwards writing a whole story about how you can actually
[00:48:01.520 --> 00:48:08.440]   run Windows 3.1 entirely finally Windows on the iPad like window wing except no it's
[00:48:08.440 --> 00:48:13.960]   that not that Windows it's actual Windows 3.1 or if you really want you can also run
[00:48:13.960 --> 00:48:19.200]   a TRS 80 and again these are things that computers can emulate stuff but to see it on
[00:48:19.200 --> 00:48:24.320]   an iPad where we've been trained sort of no there's you don't do things like that on
[00:48:24.320 --> 00:48:31.320]   an iPad has been a lot of fun so Harry how did you get into this and what have you done
[00:48:31.320 --> 00:48:37.080]   what oh my god what have you done well you know emulators for old computers have been
[00:48:37.080 --> 00:48:41.520]   around for a long time like almost as soon as the old computers that started being old
[00:48:41.520 --> 00:48:46.620]   computers right people were emulating them and I'd say for a while I've sort of idly
[00:48:46.620 --> 00:48:54.360]   wondered whether it was possible to run TRS 80 software on an iPad and not surprisingly
[00:48:54.360 --> 00:48:59.600]   it's not obvious how you do that because Apple probably is not going to allow a TRS
[00:48:59.600 --> 00:49:04.680]   80 emulator under the app store even if somebody builds one but it then it dawned on me that
[00:49:04.680 --> 00:49:09.920]   there is actually an emulator which is available on the app store and that's iDOS 2 iDOS
[00:49:09.920 --> 00:49:13.840]   has been around for I think about a decade and it was on the store then it got kicked
[00:49:13.840 --> 00:49:20.280]   off and it came back and last year Apple allowed iDOS to have access to files and to
[00:49:20.280 --> 00:49:26.920]   treat a folder like a hard drive which makes it very easy to drag old software into a
[00:49:26.920 --> 00:49:35.720]   folder and have access to it and it's suddenly dawned on me that there are DOS TRS 80 emulators
[00:49:35.720 --> 00:49:42.520]   and I have access to DOS on my iPad so maybe I will be able to emulate my TRS 80 on my
[00:49:42.520 --> 00:49:48.440]   iPad and I discovered it works really well and then I was continuing to fool around that
[00:49:48.440 --> 00:49:55.400]   I thought I'm not really sure what versions of Windows are compatible with iDOS but maybe
[00:49:55.400 --> 00:50:03.960]   win 3.1 being old and relatively simple is and I installed it which took like 30 seconds
[00:50:03.960 --> 00:50:09.960]   and it actually works really well too and because of Fast Company writing how to's about
[00:50:09.960 --> 00:50:14.960]   emulating old computers is not really our thing it dawned on me that Ben Jed words at
[00:50:14.960 --> 00:50:22.120]   how to geek his thing is A. writing how to's and B. writing about old computers so I suggested
[00:50:22.120 --> 00:50:27.800]   the Ben's that he write about this and he was nice enough to do so and to give me credit
[00:50:27.800 --> 00:50:33.240]   so I got to bask in the glory of the cool story he wrote about this topic so how do you feel
[00:50:33.240 --> 00:50:39.760]   about the it does it feel weird or does it feel strangely normal to run a TRS 80 application
[00:50:39.760 --> 00:50:44.960]   or to run Windows 3.1 on the iPad do you just sort of once once you're in there does the
[00:50:44.960 --> 00:50:50.240]   screen fall away and you're just back at your desk in 1985 it works remarkably well
[00:50:50.240 --> 00:50:58.080]   I have to say I kind of assume that it might be quirky and it might not work and running
[00:50:58.080 --> 00:51:04.160]   a beta of iPad OS 15 and there are a few issues with the keyboard occasionally freezing when
[00:51:04.160 --> 00:51:08.920]   you first go in but and you also have to figure out stuff like how the keyboards map because
[00:51:08.920 --> 00:51:17.000]   with the TRS 80 the TRS 80 keys are being mapped to I does which then has to be mapped
[00:51:17.000 --> 00:51:23.680]   to the iPad's keyboard so the sort of I sort of randomly hit keys until I realize how to
[00:51:23.680 --> 00:51:29.720]   do it but but it works surprisingly well with Windows 3.1 the main thing I had to do is
[00:51:29.720 --> 00:51:38.600]   a you have to install sound drivers but DOSBox which is the technology that that I DOS uses
[00:51:38.600 --> 00:51:44.240]   supports sound blaster and you can upgrade to a better graphics driver and I found that
[00:51:44.240 --> 00:51:50.480]   slowing down the the Windows mouse tracking helped by the way the other thing about Windows
[00:51:50.480 --> 00:51:56.240]   in particular is that it only works at all because Apple introduced crucial support relatively
[00:51:56.240 --> 00:52:01.400]   recently without that Windows would not really make sense and now it does now those who are
[00:52:01.400 --> 00:52:05.480]   not watching the video version of this or not in the studio with us I'm staring at Harry's
[00:52:05.480 --> 00:52:10.320]   screen which is sitting at a DOS prompt right now which is just freaking me out because
[00:52:10.320 --> 00:52:16.240]   it's in the smart keyboard here and again emulation is one thing but the idea of it
[00:52:16.240 --> 00:52:20.360]   having on an iPad where we there were the rules used to be you couldn't emulate anything on
[00:52:20.360 --> 00:52:27.600]   on iOS but that's no longer the case and so now we have this this app and it's not the
[00:52:27.600 --> 00:52:32.520]   only one I wanted to mention a couple of apps that are really great if right like iOS
[00:52:32.520 --> 00:52:37.840]   doesn't have a terminal either like the Mac does or Windows does but there is an app called
[00:52:37.840 --> 00:52:45.640]   A shell and another app called ISH ish that is those are both believe it or not I believe
[00:52:45.640 --> 00:52:53.520]   ISH at least is emulating an Intel PC running Linux but it works and it means you can run
[00:52:53.520 --> 00:53:00.880]   Python scripts or Pearl scripts on an iPad just like you can play a solitaire in Windows
[00:53:00.880 --> 00:53:08.800]   3.1 boy that takes me back to the 90s there Harry and I think maybe Apple knows what it's
[00:53:08.800 --> 00:53:13.160]   doing and this is not something that's randomly squeaked by the guidelines it's possible that
[00:53:13.160 --> 00:53:18.800]   Apple says this is not really a security concern you know Apple also gets concerned about somebody
[00:53:18.800 --> 00:53:25.120]   essentially building their own app ecosystem on top of theirs there's pop imagine the TRS
[00:53:25.120 --> 00:53:31.680]   80 software in-app purchase possibilities right right and yeah 30 year old Windows apps or
[00:53:31.680 --> 00:53:37.640]   40 year old TRS 80 apps are probably not going to kill Apple's business so I'm hoping that
[00:53:37.640 --> 00:53:42.080]   they kind of are doing this because maybe they don't think it's cool but maybe they also
[00:53:42.080 --> 00:53:46.600]   at least don't object to it so what was your favorite rediscovery on the TRS 80 side I didn't
[00:53:46.600 --> 00:53:51.640]   have a TRS 80 I this is going to shock people I had an Apple 2 but but I had a friend with
[00:53:51.640 --> 00:53:57.480]   a TRS 80 and so like what what jumped out at you is like oh I can't believe this this
[00:53:57.480 --> 00:54:01.360]   app that either you never saw or that you remember fondly well one interesting discovery
[00:54:01.360 --> 00:54:08.040]   is that there were a couple of guys who wrote a lot of the best arcade games for the TRS
[00:54:08.040 --> 00:54:12.160]   80 which which is a real challenge because the TRS 80 did not officially support sound
[00:54:12.160 --> 00:54:17.120]   at all they did extremely blocky graphics but they managed to write some fun arcade games
[00:54:17.120 --> 00:54:22.920]   so there's a Donkey Kong and and they thought this was when Donkey Kong was new and hot
[00:54:22.920 --> 00:54:29.040]   and they thought maybe if we write Donkey Kong we can convince Nintendo to let us license
[00:54:29.040 --> 00:54:32.560]   the rights to it so they wrote a really good Donkey Kong and they took up to Nintendo
[00:54:32.560 --> 00:54:38.560]   who knowing Nintendo it's not a great shock that Nintendo was not interested in this but
[00:54:38.560 --> 00:54:46.480]   about 10 years after that in the 90s they released their version of Donkey Kong which
[00:54:46.480 --> 00:54:52.800]   we are now going to attempt to run yes let me see if I can I love this I love emulation
[00:54:52.800 --> 00:55:00.280]   in general I actually copied all my Apple 2 discs over a couple of years ago from high
[00:55:00.280 --> 00:55:05.360]   school so that I could have access to turns out the terrible things these stupid high
[00:55:05.360 --> 00:55:10.640]   school things that I wrote as a as a teenager but I've got them now forever so that's great
[00:55:10.640 --> 00:55:19.840]   I guess so this was written in 82 released in the 90s and again this is not none of the
[00:55:19.840 --> 00:55:27.120]   graphical charm of the original Donkey Kong black and white graphics but it's recognizably
[00:55:27.120 --> 00:55:30.840]   Donkey Kong that's the thing that gets me about it is the gameplay is really surprisingly
[00:55:30.840 --> 00:55:36.240]   good there were other back in those days there was a fake everything for the TRS 80 so there
[00:55:36.240 --> 00:55:42.240]   were other fake Donkey Kongs and Pac-Man and Space Invaders that does some of the TRS
[00:55:42.240 --> 00:55:46.760]   80 experience I think as my recollection is that there were a lot of sort of alternatives
[00:55:46.760 --> 00:55:52.320]   to what was available elsewhere of the TRS 80 just spunky in that way right it was sort
[00:55:52.320 --> 00:55:59.280]   of unloved by a lot of people and and so spunky I've enjoyed me acquainting myself with text-based
[00:55:59.280 --> 00:56:04.520]   adventures a guy named Scott Adams not the Dilbert guy right the original those were
[00:56:04.520 --> 00:56:10.440]   my first adventures got Adam did some great games which are still fun Zork which was the
[00:56:10.440 --> 00:56:14.520]   most sophisticated in terms of letting you talk and complete sentences and having a
[00:56:14.520 --> 00:56:19.200]   lot of humor and atmosphere duet is still fun and you can also play more modern versions
[00:56:19.200 --> 00:56:25.000]   of Zork but it's kind of fun to get the actual experience I had when I was in high school
[00:56:25.000 --> 00:56:32.560]   with the very plain white text on black background interface so if you if you like more you should
[00:56:32.560 --> 00:56:37.240]   definitely check out Ben Jenward's story about running Windows 3.1 on an iPad it's kind of
[00:56:37.240 --> 00:56:44.800]   mind-blowing it can be done and follow Harry on Twitter because you will get not only TRS
[00:56:44.800 --> 00:56:50.520]   80 information but other Radio Shack related information it's very it's my favorite Radio
[00:56:50.520 --> 00:56:55.680]   Shack fan site is Harry's Twitter page and also all of Harry's great stories at Fast
[00:56:55.680 --> 00:57:00.440]   Company but but you know I'm I'm there for the Radio Shack I'm gonna be honest I'm my
[00:57:00.440 --> 00:57:05.080]   way here today I visited the Radio Shack in Santa Rosa which is abandoned but like in
[00:57:05.080 --> 00:57:09.320]   its original form they still have the Radio Shack sign outside it's almost as if they
[00:57:09.320 --> 00:57:15.160]   decided to build a shrine we went to a target when we were on Maui and there was a Radio
[00:57:15.160 --> 00:57:19.760]   Shack right down the way I'm sure closed but I saw the sign I thought of you actually
[00:57:19.760 --> 00:57:23.480]   I thought oh Harry should I go over there take a picture of this Hawaii Radio Shack just
[00:57:23.480 --> 00:57:26.920]   for Harry and I'm kind of kicking myself now people pretty much do think of me now when
[00:57:26.920 --> 00:57:30.880]   they think of Radio Shack it's a personal brand it's not a bad personal brand to have if you're
[00:57:30.880 --> 00:57:34.040]   gonna have a personal brand all right we're gonna be back with more probably some downers
[00:57:34.040 --> 00:57:39.000]   again maybe some other fun stuff too yeah you know but first let's listen to Leo again
[00:57:39.000 --> 00:57:43.000]   I think he's got something important to tell us this episode of Twitter has brought to
[00:57:43.000 --> 00:57:49.200]   you as it has been for many years by IT Pro TV I know anybody listening to our shows loves
[00:57:49.200 --> 00:57:55.680]   technology right maybe you're thinking I want to build a career in IT well of course if you
[00:57:55.680 --> 00:58:01.240]   ask around the first thing people are gonna say is CompTIA A+ that's the cert more companies
[00:58:01.240 --> 00:58:08.480]   want most people get it's your beginning of your IT career it is the cert the A+ cert
[00:58:08.480 --> 00:58:13.880]   from CompTIA it'll expand your horizons whether it's location pay or employer get you a better
[00:58:13.880 --> 00:58:18.800]   job because it shows you got the stuff you know your material now here's the good news
[00:58:18.800 --> 00:58:24.560]   IT Pro TV is an official the official video training partner for CompTIA so not only are
[00:58:24.560 --> 00:58:29.680]   you gonna get the search you need you're gonna get training you'll enjoy along the way IT Pro
[00:58:29.680 --> 00:58:36.240]   TV is famous for their edutainers people who are pros in the field who know their stuff
[00:58:36.240 --> 00:58:41.720]   but are also really great at communicating and even entertaining as you learn they make
[00:58:41.720 --> 00:58:48.200]   IT learning fun and interesting IT Pro TV is the best online source for IT education and
[00:58:48.200 --> 00:58:53.600]   not just for individuals for teams too for organizations as well IT Pro TV gives you the
[00:58:53.600 --> 00:58:58.480]   most up-to-date information while preparing you for the exams that give you the certs
[00:58:58.480 --> 00:59:03.160]   employers are truly looking for in their future IT employees if you're already in IT
[00:59:03.160 --> 00:59:08.560]   it helps you recertify it helps you keep your skills up helps you get a better job sorry
[00:59:08.560 --> 00:59:12.560]   I won't tell the boss helps you get a better job this is a great month to find out about
[00:59:12.560 --> 00:59:17.720]   IT Pro TV because it's CompTIA month so what does that mean well IT Pro TV will be featuring
[00:59:17.720 --> 00:59:24.080]   two CompTIA themed webinars Don Pezzette's Techno podcast will have two CompTIA guests
[00:59:24.080 --> 00:59:28.600]   so you get to hear from the people behind CompTIA all through the month they'll be giving
[00:59:28.600 --> 00:59:35.120]   away CompTIA exam vouchers that's nice and then at the end of the month Friday July 30th
[00:59:35.120 --> 00:59:42.480]   2 p.m. it's CompTIA Jeopardy Live IT Pro TV you're not gonna want to miss this all of
[00:59:42.480 --> 00:59:51.200]   that's available to all of you IT Pro TV's 7 Studios over 58 hours of on-demand IT training
[00:59:51.200 --> 00:59:55.720]   make it the perfect environment for you to not only learn IT but to polish your skills
[00:59:55.720 --> 01:00:01.600]   to get new certs and because IT Pro TV keeps those 7 Studios hot all day Monday through
[01:00:01.600 --> 01:00:06.000]   Friday they're always updating the content they've always got you know they're teaching
[01:00:06.000 --> 01:00:10.680]   to the latest tests they're they're they're working with the latest versions of software
[01:00:10.680 --> 01:00:15.920]   when new stuff comes out new certs new software they've got the information you need that's
[01:00:15.920 --> 01:00:21.560]   really a defining difference for IT Pro TV and it's not just CompTIA it's Apple it's
[01:00:21.560 --> 01:00:27.800]   Microsoft it's Cisco it's security skills it's Python and on and on and on.
[01:00:27.800 --> 01:00:32.920]   IT Pro TV makes it so easy to get a good IT education because you can do it anytime you
[01:00:32.920 --> 01:00:37.480]   want from the comfort and convenience of your own home on your own schedule they know a
[01:00:37.480 --> 01:00:42.000]   lot of you have jobs maybe jobs you don't love you want to get into IT they're gonna
[01:00:42.000 --> 01:00:47.720]   make it easy for you they're live or on-demand worldwide via Chromecast they have a Roku app
[01:00:47.720 --> 01:00:52.880]   and Apple TV app you can watch on your PC you can watch on iOS or Android device they
[01:00:52.880 --> 01:00:57.600]   make it very easy to get this content there's lots of reviews on the site but here's one
[01:00:57.600 --> 01:01:03.160]   from an IT Pro guy working in IT now this site has helped me with two certifications but
[01:01:03.160 --> 01:01:08.920]   also as the supplemental material for my grad school classes give it a try you won't be
[01:01:08.920 --> 01:01:15.400]   disappointed he says go to it pro dot TV slash twit the offer code twit 30 gets a 30%
[01:01:15.400 --> 01:01:23.600]   off all consumer subscriptions it pro dot TV slash twit again the offer code twit 30
[01:01:23.600 --> 01:01:29.640]   for 30% off the lifetime of your active subscription as long as you stay active you save 30% that's
[01:01:29.640 --> 01:01:37.020]   fantastic IT Pro TV build or expand your IT career and enjoy the journey with IT Pro TV
[01:01:37.020 --> 01:01:43.600]   IT Pro TV is at it pro dot TV slash twit again that offer code twit 30 thank you so much
[01:01:43.600 --> 01:01:49.440]   IT Pro TV Tim and Don for supporting our shows we're really happy to be partnered with you
[01:01:49.440 --> 01:01:58.040]   IT Pro TV slash twit now back to Jason and the gang thank you Leo it's weird he's sitting
[01:01:58.040 --> 01:02:02.160]   where I'm sitting it's like there's a little strain we should do a more for something there
[01:02:02.160 --> 01:02:07.520]   would be very strange I want to talk about Facebook for a moment and no wait wait where
[01:02:07.520 --> 01:02:13.560]   are you going come back come back I used to do a podcast with Stephen Hackett over at
[01:02:13.560 --> 01:02:19.440]   relay FM where we did news of the week kind of like twit and we ended up essentially
[01:02:19.440 --> 01:02:25.960]   having to do a Facebook is terrible segment every week it was kind of still crushing after
[01:02:25.960 --> 01:02:30.520]   a while but there's some Facebook news I thought we would at least mention it one is
[01:02:30.520 --> 01:02:37.080]   a story that came out this week about how Facebook has fired dozens of people over abusing
[01:02:37.080 --> 01:02:44.000]   access to user data this is from a book it came out this week it's actually kind of old
[01:02:44.000 --> 01:02:49.960]   news in that these were things that happened five six years ago but the story is chilling
[01:02:49.960 --> 01:02:56.280]   because this is one of those examples where somebody with access at Facebook basically
[01:02:56.280 --> 01:03:00.000]   wanted like one of them was gotten an argument with his girlfriend and she left their hotel
[01:03:00.000 --> 01:03:05.960]   room and he used the Facebook back end to find where her location was it's stuff like
[01:03:05.960 --> 01:03:11.280]   that like really creepy weird stuff that at the very least if it doesn't say that Facebook
[01:03:11.280 --> 01:03:15.640]   was bad because they did fire these people apparently but also that Facebook's policies
[01:03:15.640 --> 01:03:21.560]   regarding access to information are really weak and questionable and allowed these people
[01:03:21.560 --> 01:03:26.120]   to do these things so that's not great and then the other Facebook news I'm just going
[01:03:26.120 --> 01:03:31.360]   to roll them together here and then let all of you react as you choose was there was
[01:03:31.360 --> 01:03:38.360]   literally a surgeon general's warning against Facebook this week Vivek Murthy the surgeon
[01:03:38.360 --> 01:03:46.720]   general released a statement warning people about COVID vaccine misinformation on Facebook
[01:03:46.720 --> 01:03:52.720]   and Facebook responded and said no no no we're actually great in a way that was not convincing
[01:03:52.720 --> 01:04:00.800]   I would say to most anybody but what a situation to be in when the surgeon general is saying
[01:04:00.800 --> 01:04:06.760]   that you are a your website is a health danger to the people of the United States so it's
[01:04:06.760 --> 01:04:14.200]   going great everybody going great and you can somebody flash the this is fine up on the
[01:04:14.200 --> 01:04:22.960]   screen yeah yeah yeah you can buy the kind of publicity you know you know who else the
[01:04:22.960 --> 01:04:27.600]   surgeon general has come up against ladies and gentlemen cigarettes tobacco the most
[01:04:27.600 --> 01:04:34.200]   important and successful industry in the world oh we're going places we're going places free
[01:04:34.200 --> 01:04:40.200]   to foodie to foodie scoops for everybody this week yeah that was that was not a not a proud
[01:04:40.200 --> 01:04:46.040]   Dave and not a proud week and not a proud month we're not especially proud company the
[01:04:46.040 --> 01:04:52.560]   book that has the head those those stories an ugly truth inside Facebook's battle for
[01:04:52.560 --> 01:04:58.800]   domination by Shira Frankel and Cecilia Kang very well researched very well reported I
[01:04:58.800 --> 01:05:06.480]   got my copy like late last week I'm not done reading it yet but yeah it makes a as you
[01:05:06.480 --> 01:05:12.120]   open the book knowing that this is the this the tech company that I trust the least that
[01:05:12.120 --> 01:05:17.800]   I think is doing the most damage and is the most reckless of all the ones that if there
[01:05:17.800 --> 01:05:23.040]   are five five to ten companies that most people could name of those companies absolutely the
[01:05:23.040 --> 01:05:28.280]   worst of the lot and I open the open the the cover and I hear the critical of the spine
[01:05:28.280 --> 01:05:34.280]   and yet I'm not prepared for the enormity of the case that these two reporters make it's
[01:05:34.280 --> 01:05:39.800]   just they're making it's not just a series of anecdotes they really do forensically make
[01:05:39.800 --> 01:05:45.960]   the case that this is not a company that particularly cares about its impact on people in society
[01:05:45.960 --> 01:05:53.720]   at large that they care about bad press they and that's it and they have a very dysfunctional
[01:05:53.720 --> 01:05:58.280]   co-heads of the very very top that basically if they don't want something done it doesn't
[01:05:58.280 --> 01:06:03.200]   get done which means that the people who are who report to them are like filter every single
[01:06:03.200 --> 01:06:09.720]   decision through is is Zuckerberg likely to care about this no then it's not a problem
[01:06:09.720 --> 01:06:13.760]   I got the impression that the question that they ask is how can we fix this so that nobody
[01:06:13.760 --> 01:06:20.760]   mentions it again which is not the same thing as how can we fix this full stop it's a PR
[01:06:20.760 --> 01:06:25.400]   problem it's not a problem where they are again weaponizing social media against the
[01:06:25.400 --> 01:06:29.280]   political the democratic political process so long as they're so long as someone wipes
[01:06:29.280 --> 01:06:33.440]   their fingerprints off the knife they feel they feel as though they're still doing their
[01:06:33.440 --> 01:06:40.320]   responsibility for their fiduciaries responsibility shareholders yeah yeah it's somebody said
[01:06:40.320 --> 01:06:44.720]   in the chat where the Facebook apology department must be on vacation I want to know why these
[01:06:44.720 --> 01:06:50.400]   people have got access to personal data in general because clearly they shouldn't be
[01:06:50.400 --> 01:06:54.440]   trusted with it and secondly why they're allowed access to the personal data of people they
[01:06:54.440 --> 01:07:00.440]   know because that I mean I get that it's more complicated to say you're allowed access
[01:07:00.440 --> 01:07:05.320]   to some personal data but not of anybody that you actually know I'm very well aware of that
[01:07:05.320 --> 01:07:10.680]   but you know people will abuse these things we have to remember human suck there are great
[01:07:10.680 --> 01:07:14.200]   people out there there are bad people out there we have to expect that at some point somebody's
[01:07:14.200 --> 01:07:20.320]   going to do something stupid and yeah people share stupid information we're all well aware
[01:07:20.320 --> 01:07:27.400]   of that so yeah I wonder why I've worked up plenty of jobs where data security was a thing
[01:07:27.400 --> 01:07:31.600]   and you know what they did they didn't give me access to data unless I absolutely had
[01:07:31.600 --> 01:07:35.880]   to have it and then I got access to the absolute minimum amount of data for the absolute minimum
[01:07:35.880 --> 01:07:39.920]   amount of time and then my access was yanked as I requested it because I don't want to be
[01:07:39.920 --> 01:07:44.520]   responsible for anything that goes horribly horribly wrong and I kind of feel that that's
[01:07:44.520 --> 01:07:50.760]   what Facebook should be doing but I guess maybe they they didn't figure that out yet yeah
[01:07:50.760 --> 01:07:54.760]   and this other story the surgeon general's warning this is a similar sort of thing which
[01:07:54.760 --> 01:07:59.480]   is there basically saying there's a very small number of people who are responsible for
[01:07:59.480 --> 01:08:04.360]   most of the vaccine misinformation that's in social media it's a very small number it's
[01:08:04.360 --> 01:08:09.520]   not actually hard to find out who these people are and the challenge is what they say is that
[01:08:09.520 --> 01:08:14.520]   people's Facebook social circles are amplifying this information and it's leading to all sorts
[01:08:14.520 --> 01:08:18.800]   of misinformation that of course leads to people not getting vaccinated which is the safest
[01:08:18.800 --> 01:08:24.960]   course of action to not dying of COVID-19 and Facebook's response again seems to be damage
[01:08:24.960 --> 01:08:29.840]   control and it's from somebody called their director of integrity which is funny because
[01:08:29.840 --> 01:08:34.240]   it's such a misleading statement it sort of refers to the general Facebook population
[01:08:34.240 --> 01:08:41.880]   and how they're trending the same as the general public in terms of vaccine opinions but of
[01:08:41.880 --> 01:08:46.080]   course it's not about the general population it's actually about a very specific hesitant
[01:08:46.080 --> 01:08:52.480]   population and of course Facebook also has all the data and when we do have stories about
[01:08:52.480 --> 01:08:57.280]   Facebook data becoming public as we did a few weeks ago Facebook's reaction tends to be
[01:08:57.280 --> 01:09:03.680]   oh we're gonna shut that down we don't want actually any of that data to be public and I mean I know
[01:09:03.680 --> 01:09:09.280]   look we beat up Facebook a lot I'm sure that it happens all the time here it's happened on
[01:09:09.280 --> 01:09:17.280]   other podcasts I've done they're not always the worst possible case but they are enough times that
[01:09:17.280 --> 01:09:21.600]   I don't think they've got any benefit of the doubt from most of us about how they do their
[01:09:21.600 --> 01:09:27.600]   business. You may answer to Rose's question about why these people had access to this data it's
[01:09:27.600 --> 01:09:33.680]   apparently because when you're writing code it actually is useful to have access to the data
[01:09:33.680 --> 01:09:39.280]   that your code is about and that's trumped privacy concerns to a great degree it sounds it sounds
[01:09:39.280 --> 01:09:44.320]   like just a vast number of engineers at Facebook. It would be less efficient. It had access to the
[01:09:44.320 --> 01:09:53.920]   data with very little oversight and the book I think essentially puts that on Mark Zuckerberg.
[01:09:53.920 --> 01:10:01.520]   He had the option of putting privacy first and making it way harder for these people to get access
[01:10:01.520 --> 01:10:08.240]   to the data and having a far smaller number of people who had access to it and instead Facebook
[01:10:08.240 --> 01:10:15.520]   opted for the approach that was good in terms of moving fast and building stuff which is always
[01:10:15.520 --> 01:10:20.640]   your top of mind for that company. Yeah I just want to jump as in as a software developer and say
[01:10:20.640 --> 01:10:27.040]   this is what mock data is for fake data and then you have testers who test your code with real data
[01:10:27.040 --> 01:10:32.560]   on a completely separate server that that's how that really should work. All other companies can do
[01:10:32.560 --> 01:10:38.160]   this Facebook has the money they need to do that. One of the big trends these days with
[01:10:38.160 --> 01:10:44.320]   AI is synthetic data where you take the real data and you're able to essentially convert it
[01:10:44.320 --> 01:10:50.640]   into stuff based on the actual data but with no privacy concerns because it's no longer that data
[01:10:50.640 --> 01:10:56.960]   and it seems logical and maybe Facebook is doing this in some cases I don't know that they're not.
[01:10:56.960 --> 01:11:00.240]   Right this was several years ago when these incidents happened. It seems like they have the
[01:11:00.240 --> 01:11:05.120]   opportunity to get what they need from the data with way fewer privacy concerns.
[01:11:06.720 --> 01:11:12.480]   That's great stuff. I want to I'm going to do a little plug here for a book by Tech Journalist
[01:11:12.480 --> 01:11:16.560]   Charles Arthur. It just came out last week I think two weeks ago called social warming
[01:11:16.560 --> 01:11:22.960]   the dangerous and polarizing effects of social media. Really thoughtful not a Facebook is bad
[01:11:22.960 --> 01:11:29.520]   because Facebook kind of book but like how the motivations of social media apps and how they're
[01:11:29.520 --> 01:11:37.360]   constructed leads to rewarding behavior human behavior and human weakness that leads us down
[01:11:37.360 --> 01:11:44.400]   some very dark alleys as the I believe the back cover says nobody meant for Facebook to facilitate
[01:11:44.400 --> 01:11:50.160]   a genocide or for Twitter to be used to harass women or for YouTube to radicalize young men
[01:11:50.160 --> 01:11:56.800]   but all those things happen nobody meant for it to happen and why and what might be a path out of
[01:11:56.800 --> 01:12:01.600]   that it's a it's a good book by a really good writer so I'll just throw a plug in book plug there.
[01:12:01.600 --> 01:12:07.760]   I just it's a good book social warming check it out if you want to read more about why we are
[01:12:07.760 --> 01:12:13.360]   where we are. I want to talk about something a little more fun maybe I mean I thought it was fun
[01:12:13.360 --> 01:12:20.640]   which was this announcement that Microsoft is going to take its PCs and put them in the cloud
[01:12:20.640 --> 01:12:28.880]   and call it Windows 365 and let anybody on an iPad like Harry's iPad or or on a Mac on like my M1
[01:12:28.880 --> 01:12:35.680]   Mac that probably is never going to emulate Windows particularly well and Microsoft just said you know
[01:12:35.680 --> 01:12:43.440]   what starting soon for businesses they'll be able to pay for Windows 365 and you just get a window
[01:12:43.440 --> 01:12:49.040]   that has a PC in it and you don't have the PC the PC is a magic PC that lives in a cloud
[01:12:49.040 --> 01:12:54.800]   somewhere as as they do that's where PCs come from and I just I got a kick out of this because
[01:12:54.800 --> 01:13:00.640]   this feels so new Microsoft to me where they're like yeah you want a window that is a PC we got
[01:13:00.640 --> 01:13:05.360]   that for you whatever put it on your iPad I you know Harry you spend a lot of time in PC world
[01:13:05.360 --> 01:13:11.120]   what was your reaction to this story I mean it sounds really cool it's not a technological
[01:13:11.120 --> 01:13:15.840]   breakthrough as far as I know this it's using their old game technology game streaming technology
[01:13:15.840 --> 01:13:20.480]   some of us remember something called online which was a game streaming service from more than 10
[01:13:20.480 --> 01:13:25.280]   years ago which briefly had something called online desktop which was very similar to this it was
[01:13:25.280 --> 01:13:31.120]   a way to run Windows on mobile devices in the cloud and even before that a company called Citrix
[01:13:31.120 --> 01:13:37.840]   which has been around for decades was about you know putting computers on the server and letting
[01:13:37.840 --> 01:13:43.360]   people get access to them on demand so it's not a technological breakthrough but the fact Microsoft
[01:13:43.360 --> 01:13:48.880]   is doing this is great I'm probably in the same situation as a lot of people and that I still have
[01:13:48.880 --> 01:13:56.320]   an Intel based Mac and I virtualize Windows occasionally and I'm a little intimidated by the idea of
[01:13:56.320 --> 01:14:03.600]   M1 just because right now there's not a clear route to running Windows and it sounds like what
[01:14:03.600 --> 01:14:07.760]   Microsoft is doing might be all the Windows I need right and it would work on your iPad too it would
[01:14:07.760 --> 01:14:13.840]   work well and the other cool thing is it's a Windows machine that spans all your devices so you can
[01:14:13.840 --> 01:14:18.560]   run it from a Windows PC you can run it from a Mac you can run it from an iPad it's the same machine
[01:14:18.560 --> 01:14:24.320]   no matter where you use it it's a little unclear what the pricing is really like and this is for
[01:14:24.320 --> 01:14:29.760]   for businesses but they say it's for businesses as small as a one person business so I would
[01:14:29.760 --> 01:14:34.800]   suggest I resemble that remark yeah it suggests it might be fairly accessible I think some consumers
[01:14:34.800 --> 01:14:40.480]   would like to do this too and I'm hoping that the next step after this initial version is something
[01:14:40.480 --> 01:14:47.680]   that is at a price point and a level of ease of use that makes sense for almost anybody I'd
[01:14:47.680 --> 01:14:51.760]   certainly be a customer for this for sure I just like the idea of Microsoft saying hey you don't
[01:14:51.760 --> 01:14:56.560]   want to buy a PC it's fine you can still just run Windows in your web browser it's fine like just
[01:14:56.560 --> 01:15:01.840]   for Microsoft to say that that's something and it is kind of meaningful that they're doing this now
[01:15:02.960 --> 01:15:06.640]   they could have done it before and they did not and I'm hoping it's because they're comfortable that
[01:15:06.640 --> 01:15:14.640]   it works well enough if you have a really long memory which Jason does you'll remember virtual PC
[01:15:14.640 --> 01:15:20.320]   which was a Microsoft product for running Windows on a Mac before that before the days of Intel
[01:15:20.320 --> 01:15:25.760]   Macs so Microsoft does have a long-standing interest in helping people on different devices
[01:15:25.760 --> 01:15:31.120]   use Windows and this is sort of the ultimate expression of that idea because if this works then
[01:15:31.760 --> 01:15:36.000]   you don't have to worry about emulation or virtualization anymore this should work
[01:15:36.000 --> 01:15:42.800]   in any device forever which is not true of things like virtualization and Intel-based Macs
[01:15:42.800 --> 01:15:46.720]   that that was sort of ultimately a dead end and I think people have been aware it would probably
[01:15:46.720 --> 01:15:54.000]   go away at some point yeah I just having a Windows PC around in this way instead of having to worry
[01:15:54.000 --> 01:16:01.360]   about some of these other methods of doing it even a hard fast Mac user like me I was really
[01:16:01.360 --> 01:16:05.920]   enthusiastic about this because it's just like oh so a Windows PC can just be another tool in the
[01:16:05.920 --> 01:16:10.400]   toolbox I think that's pretty great Andy Rose any thoughts about that you're excited about it
[01:16:10.400 --> 01:16:15.840]   I mean I looked at this and my immediate thought was wait didn't Amazon already do this
[01:16:15.840 --> 01:16:23.280]   and they did Amazon Workspaces $25 a month for a one virtual CPU to give you buy some memory
[01:16:23.280 --> 01:16:28.320]   Windows machine in north Virginia by the looks about $33 a month in the US so
[01:16:28.320 --> 01:16:32.560]   why could software have to at least be equivalent to or cheaper than that pricing
[01:16:32.560 --> 01:16:38.080]   for to be you know better for people this also has an educational discount
[01:16:38.080 --> 01:16:45.280]   but yeah I mean it's interesting I had a Windows machine at my last day job it was a virtual
[01:16:45.280 --> 01:16:48.960]   Windows server which I just used as a Windows computer for three three things that I needed
[01:16:48.960 --> 01:16:53.600]   a Windows computer for and that was it because I could get into it from anywhere and it was also
[01:16:53.600 --> 01:16:59.360]   my backup machine if and when things went horribly horribly wrong I can see a lot of people having
[01:16:59.360 --> 01:17:05.600]   use case for this I personally don't think I would need it I have a Windows machine running
[01:17:05.600 --> 01:17:10.160]   down there is it's got some hard drives plugged into it and is doing some stuff and is a gaming
[01:17:10.160 --> 01:17:14.720]   machine if such when I want to play games I don't think virtual Windows is going to be great for
[01:17:14.720 --> 01:17:19.920]   games but yeah I'm sure a lot of people will want this and it'll also be great for those people
[01:17:19.920 --> 01:17:24.320]   maybe on Chromebooks who need a more powerful machine and didn't realize it when they bought it and
[01:17:24.320 --> 01:17:30.720]   now they can rent one the Amazon implementation is pretty cumbersome and not a lot of fun to set
[01:17:30.720 --> 01:17:36.320]   up I can say after having tried to set it up and so I think Microsoft does have lots of opportunity
[01:17:36.320 --> 01:17:42.000]   to make this sort of like you know click click click put in a credit card information and bang
[01:17:42.000 --> 01:17:49.760]   you have a Windows computer yeah new platform owner Andy you excited you very like oh boy
[01:17:50.320 --> 01:17:57.840]   Windows yay well no it's it's a great idea and I have a I have an account on a similar service
[01:17:57.840 --> 01:18:06.080]   called shadow that it's a it's it's basic purpose is to allow you to essentially have a Windows
[01:18:06.080 --> 01:18:11.360]   gaming PC remotely and yes it's great because you can you can play it on you can use it on your
[01:18:11.360 --> 01:18:17.440]   on your iPad I mean I had such a smile on my face Harry when you were demonstrating Windows 3.1
[01:18:17.440 --> 01:18:22.960]   because I had like Windows 10 just full screen on my iPad you know with the keyboard with the
[01:18:22.960 --> 01:18:28.160]   trackpad and being able to like install like the Windows version of Scrivener like the full desktop
[01:18:28.160 --> 01:18:33.520]   version of Chrome because you don't need a floppy you don't need a disk or anything you install
[01:18:33.520 --> 01:18:38.320]   everything over the over the network anyway and it's it's sprightly it is fast enough to run games
[01:18:38.320 --> 01:18:44.080]   if you want to do that and it is a way to to say that look I don't have a I don't have enough of a
[01:18:44.080 --> 01:18:50.480]   need for Windows PC to spend a thousand dollars even $800 and have this extra piece of kit I just have
[01:18:50.480 --> 01:18:55.600]   this I have this project that I'm working with these other people on this window shop so I need
[01:18:55.600 --> 01:18:59.760]   to have access to Windows for two or three months to work on this and that's actually one of the
[01:18:59.760 --> 01:19:04.720]   use cases big use cases that Microsoft was mentioning about this that if you've got a temp worker in
[01:19:04.720 --> 01:19:10.240]   or if you got you have to add more people in on a project you don't have to add nor hardware to
[01:19:10.240 --> 01:19:16.000]   your loadout nor do you have to like ship laptops and ship desktops out to people you simply give
[01:19:16.000 --> 01:19:21.760]   them a link that they now they done then click on which logs them into a fully configured PC that
[01:19:21.760 --> 01:19:27.120]   has all the software they need all the files they need all the access to other servers that they
[01:19:27.120 --> 01:19:32.080]   servers that they need and when they're done you can just simply take them off that server and
[01:19:32.080 --> 01:19:39.440]   they're and they're good to go I I do hope that it's something that becomes more like shadow PC where
[01:19:39.440 --> 01:19:44.160]   basically anybody can get in on this and do this if they decide that they again want access to
[01:19:44.160 --> 01:19:50.080]   Windows I could I don't I'm not sure if it makes sense to spend for me to spend $360 a year
[01:19:50.080 --> 01:19:57.760]   for access to actually a pretty good middle of the road PC in terms of RAM in terms of processors
[01:19:57.760 --> 01:20:03.280]   in terms of graphics graphics card and whatever but nonetheless it's so it really does patch a
[01:20:03.280 --> 01:20:09.760]   lot of holes for a lot of people I did see that some other people were reporting that that Microsoft
[01:20:09.760 --> 01:20:14.720]   product was going to be about $32 per seat so it's not as though people are going to be saving
[01:20:14.720 --> 01:20:20.080]   any money with this but the problem with the services that you have to make sure that it's
[01:20:20.080 --> 01:20:25.600]   you can't make it into a Ponzi scheme you have to have the the processing power for every single
[01:20:25.600 --> 01:20:30.400]   user who signs up for this so if this becomes popular and now you've got like 8 million users
[01:20:30.400 --> 01:20:35.680]   suddenly you've got to have enough CPU power to to serve them all which is why on a lot of these
[01:20:35.680 --> 01:20:40.160]   services like shadow sometimes it will take you you sign up and it'll take you like two or three
[01:20:40.160 --> 01:20:45.280]   months before they will give you an account because they just need to add capacity or wait for somebody
[01:20:45.280 --> 01:20:50.080]   to cancel their account before they can let you in but it is it really is a terrific idea and
[01:20:50.080 --> 01:20:56.880]   really does underscore that the the really complicated relationship that Microsoft keeps having with
[01:20:56.880 --> 01:21:01.520]   their with the PC hardware community where if now if they're the point where look we don't care
[01:21:01.520 --> 01:21:07.520]   we don't care if you're running Windows on air so long as you're running it on running Windows
[01:21:07.520 --> 01:21:12.240]   yes to hell with Dell to hell with HP to hell with Lenovo or whoever they are these days
[01:21:12.240 --> 01:21:18.640]   just give us 30 bucks a month we're good and long we can flex our cloud performance
[01:21:18.640 --> 01:21:23.360]   to other contractors that just helps us out quite a bit thank you very much it feels like the ultimate
[01:21:24.480 --> 01:21:32.240]   example of Sacha Nadella's Microsoft versus Steve Ballmer's Microsoft is Sacha Nadella
[01:21:32.240 --> 01:21:37.440]   he's a cloud guy yep and he's like you know Windows is important but Windows can be in the
[01:21:37.440 --> 01:21:43.280]   cloud and that's just fine it's it's it just and again as as Harry pointed out technically
[01:21:43.280 --> 01:21:49.280]   this is not like it's not something that Amazon and all sorts of other companies have been able to
[01:21:49.280 --> 01:21:55.440]   do before putting a PC making an available provision to you doing screen share like there's nothing
[01:21:55.440 --> 01:22:02.000]   groundbreaking here except that it's literally from Microsoft and we know the power like how many
[01:22:02.000 --> 01:22:05.360]   times have we heard that story which is like well yeah Microsoft's got a version of this but you
[01:22:05.360 --> 01:22:11.280]   know it's not slack it's Microsoft Teams and everybody who's got a Microsoft relationship in a company
[01:22:11.280 --> 01:22:15.920]   somewhere is like what what Microsoft did it well it's from Microsoft we're just gonna do like the
[01:22:15.920 --> 01:22:22.560]   weight of Microsoft as Microsoft as being in businesses as being the platform owner saying yeah this is
[01:22:22.560 --> 01:22:29.600]   totally legit and in fact we will sell you one of those rent you one of those PCs too just it's a
[01:22:29.600 --> 01:22:34.480]   very different Microsoft than the one that maybe we all grew up with and I think it's really cool
[01:22:34.480 --> 01:22:38.720]   because I think it I love it when Microsoft says you know when you want to use Windows use Windows
[01:22:38.720 --> 01:22:44.400]   we don't care where you are just use Windows we are happy to charge you to use Windows wherever you
[01:22:44.400 --> 01:22:53.680]   are love it I have a really one more story before we take a break which is about Steam and the new
[01:22:53.680 --> 01:22:58.880]   Steam Deck I don't know if you've seen this or not a lot of people tried to make pre-orders some people
[01:22:58.880 --> 01:23:05.200]   succeeded some people failed those of you who are video streamers or or rosemary it's not the new
[01:23:05.200 --> 01:23:13.520]   stream deck from Elgato although there is one that was announced the same day it's the Steam Deck
[01:23:13.520 --> 01:23:20.080]   which looks kind of like a really big Nintendo switch but it's actually a Linux PC and it's got
[01:23:20.080 --> 01:23:27.920]   Steam OS on it with Steam's store and it will play games it's got a couple different Windows API
[01:23:27.920 --> 01:23:33.600]   emulation layers so that you can take games that are built for Windows that are on the Steam store
[01:23:33.600 --> 01:23:38.880]   and they will just work now you know this probably hasn't shipped how how just will they work it's
[01:23:38.880 --> 01:23:44.800]   always the question with pre-announced products but people are in my timeline on Twitter we're
[01:23:44.800 --> 01:23:50.160]   super excited about this because the idea that you can take Steam games PC games and get something
[01:23:50.160 --> 01:23:56.080]   that you know if you squint a little bit is kind of like a Nintendo switch and you and Steam the
[01:23:56.080 --> 01:24:00.560]   Steam Deck is supposed to be what four hours three hours depends on what you're playing I think
[01:24:00.560 --> 01:24:07.040]   they said you play Portal 2 you can play it for like six hours on on the Steam Deck so on one level
[01:24:07.040 --> 01:24:15.680]   again is this new technology but it's an it's a really interesting package coming from essentially
[01:24:15.680 --> 01:24:23.520]   a platform owner in a way because the Steam store is so popular so I I was very confused that it
[01:24:23.520 --> 01:24:29.360]   wasn't a stream deck but also very impressed with this announcement and well as I was laying on the
[01:24:29.360 --> 01:24:35.840]   beach because that's where I was when this happened again just mentioning that for no reason so
[01:24:35.840 --> 01:24:41.680]   anybody have some thoughts about about the the not a switch it's fun to see this renaissance
[01:24:41.680 --> 01:24:48.000]   and mobile gaming hardware because some years ago a lot of people were very quick to declare it dead
[01:24:48.000 --> 01:24:54.560]   based on the fact that smartphones and tablets were really catching on for gaming and they told
[01:24:54.560 --> 01:25:00.960]   companies like Nintendo don't even bother to build new gaming hardware mobile hardware it's
[01:25:00.960 --> 01:25:06.880]   pointless and thank heavens Nintendo ignored that advice and came out with the switch which is
[01:25:06.880 --> 01:25:13.760]   one of their biggest phenomenon ever and it's fun to see both both things like the Steam Deck
[01:25:13.760 --> 01:25:19.920]   but also kind of the opposite of the spectrum there's Playdate which is also a new mobile gaming
[01:25:19.920 --> 01:25:26.560]   platform which you are not going to run any of your AAA fresh prison shooters on it's something
[01:25:26.560 --> 01:25:33.440]   radically different but I feel like it's a fun era when these companies you would never would
[01:25:33.440 --> 01:25:37.840]   have expected either gaming hardware are doing that and at least have a fighting chance at succeeding
[01:25:37.840 --> 01:25:45.040]   at it and I think we have Nintendo to thank for showing that that mobile gaming hardware can thrive
[01:25:45.040 --> 01:25:51.600]   in an era when people are carrying smartphones around in their pockets and steam really I mean
[01:25:51.600 --> 01:25:56.240]   steam feels like a platform I know it's it's running on Windows PCs and it's running on Macs and it's
[01:25:56.240 --> 01:26:01.200]   running on Linux but when you see it like this this is a this is a platform owner move this is
[01:26:01.200 --> 01:26:06.320]   like really what it's about is steam that's the important part here they have this built-in
[01:26:06.320 --> 01:26:12.400]   extremely large audience and even if a small percentage of those people are excited about this
[01:26:12.400 --> 01:26:19.360]   assuming it's good it has a chance of being really successful yeah I kind of have to wonder if they're
[01:26:19.360 --> 01:26:25.280]   how much money they're making on these game decks or if they're actually selling them at break even
[01:26:25.280 --> 01:26:29.120]   because so long as it gets people in the case keep some of the casino keeps them
[01:26:29.120 --> 01:26:34.000]   keeps them playing keeps them paying that doesn't necessarily have to be a big earner for them
[01:26:34.000 --> 01:26:39.280]   and the fact that I mean when I when I when this news started started to spread I was shocked to
[01:26:39.280 --> 01:26:45.040]   find out that it was 399 dollars at the entry level and even the really good ones aren't that much
[01:26:45.040 --> 01:26:50.960]   more expensive in a world where similar gaming PCs handheld gaming PCs and the same form factor
[01:26:50.960 --> 01:26:57.360]   they are priced almost at the level of a gaming laptop so they made a super attractive product
[01:26:57.360 --> 01:27:02.560]   it's a product that the gaming community thanks to Nintendo is has already been very very well
[01:27:02.560 --> 01:27:07.840]   trained to use and embrace they didn't appear to screw anything up in the design of the thing
[01:27:07.840 --> 01:27:14.480]   because it's essentially they said let's write let's make a pc that run that runs steam really
[01:27:14.480 --> 01:27:20.160]   really well but we're not gonna just out of spite make it impossible for you to connect a keyboard
[01:27:20.160 --> 01:27:24.880]   to it connected to a screen connect other controllers to it we're not going to limit what it can do
[01:27:24.880 --> 01:27:28.640]   we just want to make this the most attractive product as possible and that really does make
[01:27:28.640 --> 01:27:33.680]   you think that there there their game here is to make sure that take take people who are already
[01:27:33.680 --> 01:27:40.720]   very very happy steam customers and make them like slobberingly devoted steam customers or
[01:27:40.720 --> 01:27:45.040]   steam customers that are that are now more likely to buy games because they're not
[01:27:45.040 --> 01:27:49.840]   locked down as they as they might have been and they get to be in this more much more immersive
[01:27:49.840 --> 01:27:57.200]   sort of sort of area i mean i have to say that even i i i i i i regain control my senses within
[01:27:57.200 --> 01:28:01.760]   seconds but i it really did trigger my three hundred ninety nine dollars i gotta buy it gotta
[01:28:01.760 --> 01:28:07.280]   buy it i know i thought i have to get that i want to get that it's like i can i can write about it and
[01:28:07.280 --> 01:28:11.520]   i can it's almost like it's i'm getting it for free aren't i they said oh yeah i'll put on the
[01:28:11.520 --> 01:28:16.560]   business card it'll be fine you don't play you don't play any games whatsoever i i already let you
[01:28:16.560 --> 01:28:20.960]   talk me into ordering a play date when that comes with that goes on pre-order don't push your luck son
[01:28:20.960 --> 01:28:29.760]   rose what do you think you're gonna get one no i'm considering a retrograde pocket too which is
[01:28:29.760 --> 01:28:36.160]   eighty dollars um and um won't lock me into the steam system i don't have enough steam games for
[01:28:36.160 --> 01:28:42.640]   to be worth it i i buy games on good old games and stuff like that so um yeah i'm not sure it's i'm
[01:28:42.640 --> 01:28:47.440]   not the target market i'm the kind of person who owns the nintendo switch and plays it really
[01:28:47.440 --> 01:28:52.880]   intensely for a week and then ignores it for two and a half months and then comes back to it so i
[01:28:52.880 --> 01:28:58.160]   don't think the steam deck is is what i was looking for and um a stream deck is what you were looking
[01:28:58.160 --> 01:29:03.680]   stream deck yes uh though that said i i don't need the new one my my 32 button one is perfect
[01:29:03.680 --> 01:29:07.920]   i think somebody could make a really great game for that stream deck it's got so many buttons you
[01:29:07.920 --> 01:29:13.360]   could do like i mean james tomsen maybe i don't know dream or or lights off or i mean there's
[01:29:13.360 --> 01:29:19.120]   there you could do a game for that many buttons probably yes james tomsen will probably do it
[01:29:19.120 --> 01:29:24.320]   because why why not right after he builds a calculator for it that that's where it all starts okay
[01:29:24.320 --> 01:29:28.640]   uh we got more to talk about including some uh some fun and interesting stuff yet to come believe
[01:29:28.640 --> 01:29:35.360]   it or not but first we've got to go back to the disembodied pre-taped voice of hawaii vacationer
[01:29:35.360 --> 01:29:40.560]   leo le port can i just interrupt for one moment i'm sorry we'll get right back to the show but
[01:29:40.560 --> 01:29:46.320]   thank you jason for filling in for me uh when i you know i'm in hawaii and i'm using mint mobile
[01:29:46.320 --> 01:29:53.280]   i have to say it is the mobile wireless service you want and i know you know when i tell you it's
[01:29:53.280 --> 01:29:58.880]   15 bucks a month you're going to say i don't believe it i mean we've been for years
[01:29:59.600 --> 01:30:07.600]   big wireless providers have been um lying to us right fine print contracts there's always a catch
[01:30:07.600 --> 01:30:12.400]   you know so when i heard about mint mobile's premium wireless service starting at 15 bucks a
[01:30:12.400 --> 01:30:16.800]   month i thought you know oh there's a catch here right but i've been using their service now for
[01:30:16.800 --> 01:30:22.160]   more than a year and i gotta tell you it not only is it real it makes sense there's secret
[01:30:22.160 --> 01:30:27.760]   sauces that the first companies sell wireless service online only no retail stores no crazy
[01:30:27.760 --> 01:30:32.160]   overhead costs the savings are get passed down to you and there's no mystery fees there's nothing
[01:30:32.160 --> 01:30:39.280]   it's just sweet savings direct to you now there are all sorts of different plans including a uh no
[01:30:39.280 --> 01:30:43.680]   data cap plan but i you should check it out i think the best thing to do is start off with
[01:30:43.680 --> 01:30:48.240]   their introductory offer 15 dollars a month that includes as do all their plans by the way
[01:30:48.240 --> 01:30:56.720]   unlimited talk and text plus high-speed data on the nation's largest 5g network you can bring
[01:30:56.720 --> 01:31:01.040]   your own phone if you want they'll send you a sim no cost if you go to the mint mobile site you can
[01:31:01.040 --> 01:31:05.040]   enter in the i me i number for the phone and i'll tell you whether it's works basically it'll work
[01:31:05.040 --> 01:31:11.920]   with any phone that works with t-mobile or gsm networks um you know their lte uh it runs on t
[01:31:11.920 --> 01:31:16.640]   mobile so if your phone works with t-mobile it'll work just fine with mint mobile but you save so
[01:31:16.640 --> 01:31:20.640]   much money and of course if you're not a hundred percent satisfied then mobile has you cover with
[01:31:20.640 --> 01:31:28.880]   their seven-day money back guarantee so again just to recap mint mobile mint mobile.com/tuit
[01:31:28.880 --> 01:31:34.880]   you get premium wireless service that's unlimited nationwide talk and text plus high-speed data just
[01:31:34.880 --> 01:31:42.400]   15 bucks a month and they all ship you the plan to your door at no charge most other companies
[01:31:42.400 --> 01:31:46.960]   charge you for the sims they don't even charge you for the sim go to mint mobile.com/tuit bring
[01:31:46.960 --> 01:31:52.400]   your own phone buy one for mint mobile i got an iphone sc from them very affordable the whole
[01:31:52.400 --> 01:31:58.240]   package total thirty dollars a month phone plus data plus phone unlimited text and talk i mean it
[01:31:58.240 --> 01:32:05.520]   it's such a good deal cut your wireless bill down to 15 bucks a month mint mobile.com/tuit
[01:32:05.520 --> 01:32:12.960]   15 bucks a month it's not made up there's no catch that's it mint mobile.com/tuit thank you so much
[01:32:12.960 --> 01:32:21.440]   for supporting this week in tech and now back to the foxy Jason Snell. Jason. Oh Leo thank you so
[01:32:21.440 --> 01:32:27.600]   much boy that's this bump that we did in Hawaii now has extra meaning. I am Jason Snell I'm here
[01:32:27.600 --> 01:32:32.800]   with Harry McCrack and Andy Anaco and Rosemary Orchard still talking about tech lots of stuff going
[01:32:32.800 --> 01:32:37.680]   on my old pal Heather Kelly who used to work with me at Macworld it's now a way tech reporter for
[01:32:37.680 --> 01:32:42.240]   the Washington Post and she had a story this week that I thought was really good this could be
[01:32:42.240 --> 01:32:48.160]   filed away as the one of those bummer stories about how everybody's got your information and
[01:32:48.160 --> 01:32:52.720]   everybody all your privacy is is vanquished but I feel like there's something inspirational about
[01:32:52.720 --> 01:32:57.360]   this as well that once you're armed with this knowledge you can do the right thing the story is
[01:32:57.360 --> 01:33:03.040]   called lots of apps use your personal contacts few will tell you what they do with them and this is
[01:33:03.040 --> 01:33:08.800]   just it's a little quirk if you've ever been in your in your smartphone and you launched an app and
[01:33:08.800 --> 01:33:15.840]   it said I would like to look at your contacts you might be like oh sure but what can happen in those
[01:33:15.840 --> 01:33:21.840]   scenarios is that the apps then have access to your entire contact library there's a philosophical
[01:33:21.840 --> 01:33:27.360]   discussion to be had to be had by the way about whether you really should even be allowed to grant
[01:33:27.360 --> 01:33:35.040]   access to contacts do your friends phone numbers belong to you do you have the right to actually
[01:33:35.040 --> 01:33:39.760]   give that information to whoever wants it you sign up for clubhouse and clubhouse is great send me
[01:33:39.760 --> 01:33:47.440]   all your contacts do you want to do that but also what happens then they get siphoned off they get
[01:33:47.440 --> 01:33:53.440]   taken somewhere else do they get compared to other people are there are social maps being built behind
[01:33:53.440 --> 01:33:59.040]   the scenes against your knowledge about who knows who some horror stories in here about therapists
[01:33:59.040 --> 01:34:04.000]   who keep their patience information and their phones and then they're uploaded to a site like
[01:34:04.000 --> 01:34:10.800]   Venmo where they can be where it's a public viewing of contacts and all of a sudden they've disclosed
[01:34:10.800 --> 01:34:17.920]   their patients names in public which is a no no and and again we could make this about oh they're
[01:34:17.920 --> 01:34:23.120]   out to get your information because maybe they are some of them probably are but what I like about
[01:34:23.120 --> 01:34:31.600]   the story is I think four warned is four armed uh android and iOS I believe give you some features
[01:34:31.600 --> 01:34:38.800]   some power to say no I don't want to share my contacts with this app I refuse and then you can
[01:34:38.800 --> 01:34:44.640]   decide do you want to do that or do you not want to do that and uh I just I really like that idea
[01:34:44.640 --> 01:34:50.160]   because I think the problem here is in part that we don't think about it when we say yes we're not
[01:34:50.160 --> 01:34:56.800]   thinking I'm disclosing everybody I know and without them saying it's okay I've just handed their phone
[01:34:56.800 --> 01:35:03.280]   number and their address and who knows what else in the notes field to who knows who so I thought
[01:35:03.280 --> 01:35:09.280]   this story was really really great uh and and uh you know I open it up for discussion that's a
[01:35:09.280 --> 01:35:14.480]   terrible host thing to do Leo is gonna revoke this but like I don't know I mean I when I signed up
[01:35:14.480 --> 01:35:22.240]   for clubhouse I didn't I mean I didn't want to share my contacts and it was really kind of creepy
[01:35:22.240 --> 01:35:28.480]   to say like I don't even think I have the right to share other people's phone numbers with any
[01:35:28.480 --> 01:35:33.280]   service that I sign into it was a great story and one of the troubling things about it was that Heather
[01:35:33.280 --> 01:35:38.640]   asked a bunch of big companies what they're doing with the contacts they got from users and in some
[01:35:38.640 --> 01:35:44.080]   cases some of those companies wouldn't tell her uh they were relatively few it seemed that that
[01:35:44.080 --> 01:35:48.800]   were really specific about how they're using contacts um I think one thing that is a little
[01:35:48.800 --> 01:35:54.560]   encouraging is that as this becomes more public both apple and google really do have a lot of
[01:35:54.560 --> 01:36:01.680]   opportunity to give us even more control um right apple recently did a new photo feature where you
[01:36:01.680 --> 01:36:07.920]   can instead of sharing your entire photo library you can pick specific photos to share right and
[01:36:07.920 --> 01:36:12.240]   uh I look at that and I think well yeah you should do that for every single bit of data sharing on
[01:36:12.240 --> 01:36:19.040]   that device like with location on apple um you can give um an app just a vague idea of where you are
[01:36:19.040 --> 01:36:24.960]   because most most apps don't need to know precisely where you are and it really feels like um it would
[01:36:24.960 --> 01:36:30.880]   not be surprising if part of the follow up to Heather's story is the the companies that control
[01:36:30.880 --> 01:36:36.640]   the platforms giving us a lot more power and unlike some stuff out there this does seem to be an
[01:36:36.640 --> 01:36:42.880]   instance where um there's stuff that can be done about it uh one of the challenges which is pointed
[01:36:42.880 --> 01:36:49.360]   out in the story is once a company has access to your contacts you can't yank them back right
[01:36:49.360 --> 01:36:54.080]   they they have them pretty much forever and uh so there may not be that much you can do about what
[01:36:54.080 --> 01:36:59.120]   you showed in the past but moving forward I think there is good opportunity to uh to give us a lot
[01:36:59.120 --> 01:37:08.640]   more control yeah and it's and it's not like it's about uh people wind up on a mailing list it really
[01:37:08.640 --> 01:37:15.360]   is about the part of the job of of these marketers is to solve for x that if you have uh if you have
[01:37:15.360 --> 01:37:20.800]   uh if the information that they're collecting is anonymized that's great but if you have enough
[01:37:20.800 --> 01:37:26.400]   samples of anonymized data uh and enough you can calculate with a bit for the missing variables
[01:37:26.400 --> 01:37:32.080]   and a list like this is a very very powerful tool it also brings to mind that one of the most valuable
[01:37:32.080 --> 01:37:38.400]   single pieces of information that the data collectors can get is your cell phone number
[01:37:38.400 --> 01:37:43.280]   because it uniquely identifies you it follows you from one place to another so you might have
[01:37:43.280 --> 01:37:47.120]   moved three times in the past five or six years but you probably still have the same cell number
[01:37:47.120 --> 01:37:54.880]   and this is if they can attach uh unknown anonymized subject a 4411444b
[01:37:55.440 --> 01:38:03.600]   with oh well I know that his cell phone number is 8184412211 wow now you've pretty much that's
[01:38:03.600 --> 01:38:08.640]   pretty much the master key that unlocks so much other stuff so yeah it's it's it's it's terrible
[01:38:08.640 --> 01:38:16.800]   that we that we sometimes don't pause as we're so eager to get to the part of the app that lets us
[01:38:16.800 --> 01:38:22.480]   give ourselves big eyes and a propeller on our nose that we just blow past the oh yeah by all
[01:38:22.480 --> 01:38:26.480]   by all means uh take a look at all my contacts i'm sure i'm gonna want to share some of these new
[01:38:26.480 --> 01:38:30.320]   animations with my friends of mine well no it's not about sharing things with friends of yours
[01:38:30.320 --> 01:38:35.600]   it really is collecting information and trying to make life a little bit more tragic for all
[01:38:35.600 --> 01:38:42.880]   for all of us everywhere this is the story about Venmo by the way uh disclosing Joe Biden's Venmo
[01:38:42.880 --> 01:38:48.560]   account because he sent Venmo uh he's used Venmo and has said said money to his grandkids and they
[01:38:48.560 --> 01:38:52.880]   were able to backtrack from there and find Joe Biden's entire Venmo friend list which oh boy
[01:38:52.880 --> 01:38:58.400]   grandpa Joe sent me some money for for ice cream thanks grandpa Joe but still the point is they
[01:38:58.400 --> 01:39:04.160]   added an opt out but if you don't know that your data is there you don't know to opt out so it's not
[01:39:04.160 --> 01:39:08.720]   great it's not great it's not the end of the world but it's not great you should be aware that you're
[01:39:08.720 --> 01:39:14.320]   giving this stuff away these things should be opt in not opt out to be very clear they always
[01:39:14.320 --> 01:39:18.880]   have to be opt out rather than opt in and secondly what the heck are they gonna do with something
[01:39:18.880 --> 01:39:23.280]   like GDPR which i'm aware it doesn't affect them because they don't operate in the european
[01:39:23.280 --> 01:39:27.840]   economic area but the california data law is definitely affect Venmo if i tell them i need you
[01:39:27.840 --> 01:39:34.720]   to delete all of my personal identifying information are they gonna actually do that because it seems
[01:39:34.720 --> 01:39:40.480]   like they probably wouldn't because oh but that's not related to your account that's related to
[01:39:40.480 --> 01:39:45.200]   Jason smells account or Joe Biden's account or whatever no it's my personal information i don't
[01:39:45.200 --> 01:39:52.160]   want it out there um yeah i i personally think this is a big issue and they should never be
[01:39:52.160 --> 01:39:57.360]   collecting this information um and yeah it's it's one of those things i do wish apple was a
[01:39:57.360 --> 01:40:02.480]   little bit better specifically about the photo sharing um i i had an issue when it first came
[01:40:02.480 --> 01:40:07.360]   out where i said oh no this happens and allowed access to any photos but then it couldn't access
[01:40:07.360 --> 01:40:11.520]   the camera now i'm pretty certain that was because i was on a beta at the time and we all know
[01:40:11.520 --> 01:40:17.840]   betas break things um but yeah it would be good if this was a little more easily tweakable um so
[01:40:17.840 --> 01:40:23.120]   you could say hey maybe i can access this album of photos or it can only access photos that i take
[01:40:23.120 --> 01:40:27.840]   within the app but why does my toothbrush app need access to my location they say it so that they
[01:40:27.840 --> 01:40:32.560]   can only sync my toothbrush but when i'm at home well guess what i haven't been anywhere in 18 months
[01:40:32.560 --> 01:40:38.000]   so i'm always at home um so uh yeah apparently they want to know that well phillips thank you
[01:40:38.000 --> 01:40:43.840]   very much but you're not getting my location yeah and and we've had a lot of conversations about
[01:40:43.840 --> 01:40:48.960]   location data right like location data and and photos a little bit um but that's what struck me
[01:40:48.960 --> 01:40:54.240]   about this story is is contacts maybe people don't think about them so just i am saying to everybody
[01:40:54.240 --> 01:40:59.280]   who is listening now think about it the next time or go into your privacy settings and see
[01:40:59.920 --> 01:41:03.360]   what services you've granted access to your contacts because when you're doing that
[01:41:03.360 --> 01:41:09.200]   it's not just your like your basic information it can potentially be your entire contacts database
[01:41:09.200 --> 01:41:17.680]   and that's probably i mean having restrictions is good but you do sort of feel like they're patching
[01:41:17.680 --> 01:41:23.040]   an initial whole instead of revisiting that feature and saying actually by default everything
[01:41:23.040 --> 01:41:28.640]   should be completely locked down and then we should give data very in very limited ways beyond
[01:41:28.640 --> 01:41:34.160]   that and i'm actually surprised that um that they haven't done more in that way because
[01:41:34.160 --> 01:41:39.440]   quite honestly you shouldn't be able to do a couple of taps and send all of the phone numbers
[01:41:39.440 --> 01:41:44.800]   of everybody you know to anyone that's uh that's that's a bad idea it's a bad idea and the odd
[01:41:44.800 --> 01:41:50.960]   thing is it's not like this is really that much of a revelation because path many years ago got in
[01:41:50.960 --> 01:41:57.600]   trouble for what they did with address books and uh they built the whole social graph right so there
[01:41:57.600 --> 01:42:03.440]   is an alternate universe where apple and google really screwed um put the screws on some of the
[01:42:03.440 --> 01:42:09.440]   stuff a lot earlier than than they have and i'm not sure why they didn't because it seems fairly
[01:42:09.440 --> 01:42:16.560]   obvious in a lot of ways it's just that people don't think about it well um we've got more to talk
[01:42:16.560 --> 01:42:23.280]   about but uh this is a whole network with shows that people like rosemary orchard and andian naco
[01:42:23.280 --> 01:42:28.160]   we're on every week and great stuff happens here every week magical stuff even when leo's in
[01:42:28.160 --> 01:42:34.320]   hawaii quite frankly so we should take a look at what great stuff happened this week at twit
[01:42:34.320 --> 01:42:39.520]   because i don't really like eating or food in general and so it almost becomes
[01:42:39.520 --> 01:42:44.640]   wait wait wait i'm sorry so let's step back for a second i'm a weirdo food i yeah if i could go
[01:42:44.640 --> 01:42:49.040]   eating yeah if i could go without eating i would i gotta tell you so that's actually pretty impressive
[01:42:49.040 --> 01:42:56.000]   and you could save a lot of money previously on twit this week in enterprise tech now we've
[01:42:56.000 --> 01:43:03.440]   been talking about how the scc speed standards are just too darn so agit pied did absolutely freaking
[01:43:03.440 --> 01:43:11.600]   nothing other than mess up the industry slow things down and make america wholly unprepared
[01:43:12.160 --> 01:43:20.240]   for pandemic ios today coming up on ios today rosemary orchard and i cover some apps that will
[01:43:20.240 --> 01:43:27.360]   leave you puzzled yes that's right we're doing puzzle games all about android and that the flagship
[01:43:27.360 --> 01:43:32.720]   killer so-called has been caught cheating not a good book my friend not he's not he one plus included
[01:43:32.720 --> 01:43:38.240]   a blacklist of popular apps from the play store all of which are prevented from taking full advantage
[01:43:38.240 --> 01:43:44.160]   of the phone's power it ends up just becoming a really unfortunate headline tech news weekly
[01:43:44.160 --> 01:43:50.720]   twitter has made the decision to shut down fleets i'm not surprised based on what i've seen in terms
[01:43:50.720 --> 01:43:57.040]   of usage of the platform i did give fleets a try it just it just didn't quite feel right send out
[01:43:57.040 --> 01:44:04.000]   a fleet if you want to it is definitely going away to it subscribe download tell a friend while you're
[01:44:04.000 --> 01:44:16.640]   at it man that that shot of mica sergeant with mary joe fully and paul thorat surrounding him
[01:44:16.640 --> 01:44:21.920]   i feel really bad for the guy who's like you're hosting windows weekly this week go don't whatever
[01:44:21.920 --> 01:44:28.320]   you do don't tell him that you don't like food ah there's a lot of good stuff in there and we got
[01:44:28.320 --> 01:44:33.360]   we got rosemary orchard in there being puzzled with with mica that was that's really great lots of
[01:44:33.360 --> 01:44:40.640]   good stuff here at twitt um all right if you love okay hi tesla fans hi i like tesla's they're great
[01:44:40.640 --> 01:44:46.560]   full self-driving beta v nine came out um
[01:44:46.560 --> 01:44:52.800]   wasn't that great like there are some great youtube videos showing that it still has problems with
[01:44:52.800 --> 01:45:00.240]   on making unprotected left turns and driving in san francisco and things like that uh but it's out
[01:45:00.240 --> 01:45:06.960]   elan musk uh said it was going to be out for a very long time and then it did come out um i have to be
[01:45:06.960 --> 01:45:13.920]   honest um at some point i lost faith in the entire full self-driving thing there was um there was a
[01:45:13.920 --> 01:45:19.360]   point several years ago where i thought oh yeah these computers are smart these these computer
[01:45:19.360 --> 01:45:24.000]   wizards they're they're brilliant they're gonna figure it out and now when i see tesla which is a
[01:45:24.000 --> 01:45:30.960]   company that has a pretty impressive track record in a lot of areas um struggle mightily to release
[01:45:30.960 --> 01:45:38.560]   new versions of this thing that they insist on calling full self-driving um and i don't know i've
[01:45:38.560 --> 01:45:44.000]   lost faith at this point i i feel like every uh there's another story this week we heard that maybe
[01:45:44.000 --> 01:45:50.800]   the guy who helped squire the apple watch to completion kevin lynch has been moved from that to apples
[01:45:51.680 --> 01:45:57.120]   car project which i'm like oh apple car that's interesting and then the recap is apples full
[01:45:57.120 --> 01:46:05.440]   self-driving car and i thought oh no no no that's that's never gonna happen so friends around me
[01:46:05.440 --> 01:46:12.800]   am i wrong have you can you reinstill the faith in in self-driving cars or are you all just like
[01:46:12.800 --> 01:46:18.480]   finally jason figured it out it's not gonna happen yeah i mean it was it was so much fun three or four
[01:46:18.480 --> 01:46:24.560]   years ago when everybody was starting up a self-driving car company and like it was it was like the
[01:46:24.560 --> 01:46:29.680]   bitcoin of the early 2010s and we all thought that we're going to be at self-driving cars and
[01:46:29.680 --> 01:46:36.080]   hover cars by now uh and it's see it but it's it's this is the kind of kind of technology where
[01:46:36.080 --> 01:46:45.680]   every getting halfway there from where you are is always easy but it gets harder every time you do
[01:46:45.680 --> 01:46:51.520]   it so if we go from zero it's easy to get like it get get self-driving software halfway there
[01:46:51.520 --> 01:46:56.560]   the next time you want to get it to 75% there and it's twice as hard the next time you want to get
[01:46:56.560 --> 01:47:02.000]   it have that again to the to the finish line and it's twice as hard and you will because
[01:47:02.000 --> 01:47:07.840]   the zenos paradox you will kind of never get there and meanwhile you get these the expectations
[01:47:07.840 --> 01:47:16.640]   being raised by uh by tesla by calling it autopilot when i mean secret if you haven't figured this out
[01:47:16.640 --> 01:47:23.760]   yet uh there's a standard for uh for self-driving car technology one through five one being the lowest
[01:47:23.760 --> 01:47:29.200]   five being what we all imagine to be self-driving car it's as defined as you don't need to have
[01:47:29.200 --> 01:47:34.160]   someone in the driver's seat you don't even have to have a steering wheel pedals and you can use
[01:47:34.160 --> 01:47:42.480]   it in every situation no matter what uh tesla the uh uh waymo's cars are at level four i think right
[01:47:42.480 --> 01:47:48.640]   now meaning that there are situations they're running in test test situations and kind of a 50
[01:47:48.640 --> 01:47:56.240]   square mile area in suburban phoenix arizona plus mountain view plus uh san francisco and even
[01:47:56.240 --> 01:48:00.800]   there even there in a situation where if it's raining that's the situation where no they're not
[01:48:00.800 --> 01:48:06.720]   comfortable uh having self-driving cars if you can ask for a waymo taxi and in phoenix and it's raining
[01:48:06.720 --> 01:48:11.280]   you will your your waymo taxi will have someone behind the wheel to make sure the things are working
[01:48:11.280 --> 01:48:16.480]   but they're calling it autopilot and again they're at level two meaning that you have to have a human
[01:48:16.480 --> 01:48:22.160]   being not only sitting there behind the wheel with the feet on the pedals but also paying as close
[01:48:22.160 --> 01:48:27.680]   attention to the driving process as they would have they if they were actually driving this car
[01:48:28.240 --> 01:48:32.800]   and they want to call it autopilot and they want to start charging two hundred dollars a month
[01:48:32.800 --> 01:48:38.320]   for this malarkey they're nowhere near ready and we're not even we're not even at a place where we
[01:48:38.320 --> 01:48:43.680]   can start to regulate uh self-driving cars yet we're not the technology is not mature enough that we
[01:48:43.680 --> 01:48:50.560]   can start asking governments okay well we want to start putting autonomous vehicles on public roads
[01:48:50.560 --> 01:48:55.760]   how what what do what rules are we going to have to comply with in order to make that happen we've
[01:48:55.760 --> 01:49:01.200]   not we just don't have the data yet to make that happen we most most successful by far and but
[01:49:01.200 --> 01:49:06.560]   they're still doing most of their miles in simulation rather than on the road so yeah i mean i i think
[01:49:06.560 --> 01:49:11.360]   it's gonna i think it's gonna happen but it's gonna have to be that thing where it's just slow
[01:49:11.360 --> 01:49:17.920]   incremental changes and suddenly we find that we might be at an airport and the instead of having a
[01:49:17.920 --> 01:49:24.240]   like a monorail sort of thing that takes you from the terminals to the parking area maybe we will
[01:49:24.240 --> 01:49:30.640]   have like autonomous vehicles that we can hail that will take us to where our rental car is maybe we
[01:49:30.640 --> 01:49:35.200]   will be in a situation where for on a college campus we can have a hail riding service where on the
[01:49:35.200 --> 01:49:39.200]   campus the the car is going to drive automatically but it's going to be a long long time before we
[01:49:39.200 --> 01:49:46.560]   get into that sort of uh dreamscape where we get to be calling uber and it's a self-driving uber and
[01:49:46.560 --> 01:49:54.560]   it takes me from the suburbs all the way into a meeting in the city without my screaming for my
[01:49:54.560 --> 01:50:00.400]   life and texting last messages to loved ones now i they've probably already sent the emails but i'm
[01:50:00.400 --> 01:50:04.400]   just going to clarify something for all of those uh the the tesla fans out there there are actually
[01:50:04.400 --> 01:50:10.320]   two levels of driving here which is probably part of the the problem with the marketing is there's
[01:50:10.320 --> 01:50:16.320]   autopilot and there's full self-driving and autopilot is actually the smart cruise control feature
[01:50:16.320 --> 01:50:20.400]   and i've used that and i think it's actually pretty nice and that they pretty did a pretty good job
[01:50:20.400 --> 01:50:24.560]   but it's literally just a smart cruise control when you're on the freeway and then full self
[01:50:24.560 --> 01:50:28.800]   driving is when you like start in your driveway and say take me somewhere and it stops at the stop
[01:50:28.800 --> 01:50:34.720]   signs and it makes all the turns and that's the stuff that is uh guess what a lot harder to do and
[01:50:34.720 --> 01:50:43.600]   and again i feel like uh tesla is a really nice product and there's yet there's something about
[01:50:43.600 --> 01:50:50.320]   the and it comes from elon musk it is the over promise under deliver ethos of the whole thing
[01:50:50.320 --> 01:50:55.520]   that drives me crazy because like they have done some really great engineering but when everything
[01:50:55.520 --> 01:50:59.840]   is late and everything is over promised and everything is misnamed to make it seem more impressive
[01:50:59.840 --> 01:51:05.520]   than it is you start to get kind of down on what the company is doing even though it's doing and
[01:51:05.520 --> 01:51:12.000]   this goes honestly this goes for space x2 it is endemic to elon musk companies that they do amazing
[01:51:12.000 --> 01:51:19.120]   things but only in the context of having over promised so yeah yeah and then and the fact that
[01:51:19.120 --> 01:51:24.160]   the apple car who knows for sure but that the apple car project is still being described similarly
[01:51:24.160 --> 01:51:31.520]   as an autonomous vehicle um i can see apple doing a smart car of some sort but it's that last
[01:51:31.520 --> 01:51:38.240]   um i can't believe i just walked into that at that last mile that i don't believe i just i just
[01:51:38.240 --> 01:51:42.640]   that's that's the really hard part is can you really just not have anybody paying attention
[01:51:42.640 --> 01:51:48.640]   or behind the wheel and that that we have not had enough evidence of i think yeah the videos on
[01:51:48.640 --> 01:51:55.280]   youtube of people using the beta of uh the full self-driving are really a little scary actually
[01:51:55.280 --> 01:52:00.000]   they do that people are running beta software so it tries to do this at all on real roads uh-huh
[01:52:00.000 --> 01:52:08.080]   there's a little unnerving but i don't know i'm going to be near near oceans and
[01:52:08.080 --> 01:52:13.520]   lakes and and cliffs and be running beta software in my car the hell is wrong with people apparently
[01:52:13.520 --> 01:52:18.160]   most of the people have access to this beta art tesla employees at this point so it's not vast numbers
[01:52:18.160 --> 01:52:23.360]   of people but things like left hand turns that are really hard even if you're just a human being
[01:52:23.360 --> 01:52:30.640]   uh or even harder for a car and it is a little creepy to see people uh try to entrust their
[01:52:30.640 --> 01:52:35.440]   their tesla to make a left hand turn and it just utterly fail to do so in a safe fashion
[01:52:36.800 --> 01:52:43.120]   yeah it's just it just isn't there and and i think it's okay that it's not there i think the
[01:52:43.120 --> 01:52:49.840]   problem is that you have uh the head of the company talking about ho oh yo no like any day now full
[01:52:49.840 --> 01:52:56.080]   self-driving is coming and you know just just be a little more humble about it that's probably
[01:52:56.080 --> 01:53:02.080]   impossible for elan to do that but like tesla's got some great stuff it is building smart software
[01:53:02.080 --> 01:53:07.440]   it has a lot of interesting sensors but to go all the way to full self-driving you are put you
[01:53:07.440 --> 01:53:15.040]   are putting the bar so high and yes you watch these youtube videos and uh they can't clear that bar
[01:53:15.040 --> 01:53:20.400]   and and that's because set the bar lower set the bar lower it's the antithesis of the apple way of
[01:53:20.400 --> 01:53:26.240]   doing that which is apple and we may be years and years away from apple even acknowledging they're
[01:53:26.240 --> 01:53:32.800]   working on this problem and if they do ever shut up um it's likely to be in better shape than uh
[01:53:32.800 --> 01:53:37.680]   tesla has done where they're they're experimenting or they'll only announce smart cruise control
[01:53:37.680 --> 01:53:41.440]   essentially autopilot and not full self-driving because they're like yeah it's not good enough
[01:53:41.440 --> 01:53:45.040]   seems more likely and people's lives are in the balance that's the other thing is it's very unlikely
[01:53:45.040 --> 01:53:50.160]   that all of us running the ios 15 beta are going to accidentally kill ourselves or other people
[01:53:50.160 --> 01:53:55.760]   while we do it i mean it's possible it's anything is possible but it's it's a lot less likely
[01:53:55.760 --> 01:54:00.480]   than if that beta software is running on a car maybe that's why the apple watch has so much like
[01:54:00.480 --> 01:54:05.360]   fall detection stuff in it so that the people that they do run over will be able to get medical
[01:54:05.360 --> 01:54:10.560]   attention really really quickly which is very thoughtful of them it's very very helpful yeah
[01:54:10.560 --> 01:54:16.160]   tesla just starts giving an apple watch to everybody just in case one of their cars hits them yeah
[01:54:16.160 --> 01:54:20.880]   no i recently got a new car and it's got smart stuff in it so it can detect like road signs to
[01:54:20.880 --> 01:54:25.280]   tell you what speed you should be driving and things like that and it can also detect the
[01:54:25.280 --> 01:54:30.240]   white lines on the road and the edge of the road if you're on a motorway or freeway for
[01:54:30.240 --> 01:54:36.880]   americans um it can't do it on any normal road as far as i can tell because it literally just
[01:54:36.880 --> 01:54:41.680]   doesn't recognize you know the faded white lines in the middle of the road because guess what
[01:54:41.680 --> 01:54:47.040]   companies don't pay enough money to actually maintain roads around here and i'm sure everywhere
[01:54:47.040 --> 01:54:51.920]   has that kind of problem it's like okay my car's a Renault it doesn't have necessarily all the
[01:54:51.920 --> 01:54:57.680]   money behind it that tesla's got but dyson port what was it 600 million into building an electric
[01:54:57.680 --> 01:55:03.360]   vehicle and that didn't pan out so i'm wondering how much is this uh you know autonomous electric
[01:55:03.360 --> 01:55:09.920]   self-driving car uh costing apple and uh you know is it ever going to see the light of bay well
[01:55:09.920 --> 01:55:14.960]   they're good for it they've got they've got money enough money under their pillow to pay for it a
[01:55:14.960 --> 01:55:22.080]   couple times over probably but yeah i do wonder sometimes having kevin lynch move into apple car
[01:55:22.080 --> 01:55:28.080]   territory is a real interesting sign though because he he is perceived at least to be the guy who
[01:55:28.080 --> 01:55:34.400]   got the apple watch over the finish line and if he's the closer um what does that mean does that
[01:55:34.400 --> 01:55:38.240]   mean it's in disarray and their desperate need of help or does it mean that they're kind of closing
[01:55:38.240 --> 01:55:43.040]   on on having a product or does it mean that kevin lynch is really bored of apple watch and
[01:55:43.040 --> 01:55:46.640]   wants to do something different i don't know what we're all reading the apple
[01:55:46.640 --> 01:55:51.600]   tea leaves the kremlinology that goes on but it is an interesting move because his last one was
[01:55:51.600 --> 01:55:57.280]   apple's last major product new product which is the apple watch it is it is really weird because i
[01:55:57.280 --> 01:56:05.360]   i have a completely honest i have no clue what apple is up to with the with self-driving cars i i
[01:56:05.360 --> 01:56:10.720]   have no idea i can't i mean if you could give me a list of a hundred possibilities and i would
[01:56:10.720 --> 01:56:15.680]   nod at each one of them saying i guess sure that makes sense uh but the thing that keeps
[01:56:15.680 --> 01:56:22.560]   that that keeps me interested is knowing the level of talent they keep bringing into this project
[01:56:22.560 --> 01:56:28.640]   that that's the slide deck they keep showing people say okay well i know that you are incredibly
[01:56:28.640 --> 01:56:34.560]   successful in the field of auto design and you have one of the most prestigious positions at one of
[01:56:34.560 --> 01:56:39.600]   the most prestigious auto companies in the world but i'm going to show you this little i'm going to
[01:56:39.600 --> 01:56:44.320]   show you this little keynote deck in 22 minutes of your time and at the end of that time they're
[01:56:44.320 --> 01:56:48.960]   saying you know what i'm going to quit my job and start working for apple that is one hell of a
[01:56:48.960 --> 01:56:55.680]   pitch deck and so whatever it is that there is showing them whatever plans or or whatever uh you
[01:56:55.680 --> 01:56:59.360]   know do you want to keep so you want to keep selling sugar water to kids or do you want to help change
[01:56:59.360 --> 01:57:08.400]   the world it's definitely convincing people to again leave leave positions of great success and
[01:57:08.400 --> 01:57:14.960]   great security and great renown and join this the this this project that i keep comparing it to
[01:57:14.960 --> 01:57:21.280]   like the soviet moonlander project where at some point the soviet said okay uh we're canceling the
[01:57:21.280 --> 01:57:26.640]   moonlander project i know that you you you you you you and you have been doing nothing but training
[01:57:26.640 --> 01:57:31.360]   to be the first to be to walk on the moon but not only we're canceling this project we will never
[01:57:31.360 --> 01:57:37.440]   acknowledge this project actually existed so good luck with the good good thanks for your past five
[01:57:37.440 --> 01:57:42.320]   or six years of servicing good luck that's what apple could absolutely do if they simply decide that
[01:57:42.320 --> 01:57:47.920]   nah this was yeah actually we were going to go ahead with the with the apple car project but
[01:57:47.920 --> 01:57:53.040]   tony has an idea for a really cool new watch band that really is going to scratch that innovation
[01:57:53.040 --> 01:57:56.640]   itch for us for this year so so what you're saying it's probably not the dental plan
[01:57:56.640 --> 01:58:02.000]   probably not the dental plan i understand they have a really good health club on the apple campus
[01:58:02.640 --> 01:58:08.960]   yeah maybe maybe um okay we've got some more fun stories to talk about before we wrap up uh but
[01:58:08.960 --> 01:58:15.360]   first one last visit to the time delayed vacationer himself mr lea laport
[01:58:15.360 --> 01:58:22.640]   one more one more commercial let me just briefly mention and dava and d_a_v_a_
[01:58:22.640 --> 01:58:28.720]   they have a great podcast i know you're gonna like it's called tech reimagined they just put out
[01:58:28.720 --> 01:58:33.680]   season two of tech reimagined here's how it works it brings together leading tech
[01:58:33.680 --> 01:58:39.520]   personalities industry experts i mean names you know gai-kai wasaki mary williams alex hunter
[01:58:39.520 --> 01:58:46.640]   brian mcbride tom groover dave cobblin uh email martinez viola llewellin and they talk about
[01:58:46.640 --> 01:58:52.720]   big technology and how it's changing our world guests and hosts talk about how
[01:58:52.720 --> 01:58:58.000]   technology and its industries are impacting our everyday lives how our relationship with
[01:58:58.000 --> 01:59:04.800]   technology is constantly being reimagined hence the name tech reimagined uh they they kind of
[01:59:04.800 --> 01:59:10.320]   focus on different areas for instance there's two episodes in the series about insurance insurance
[01:59:10.320 --> 01:59:15.520]   reimagined the guests are and norcolate and kevin crawford they talk about how you know what
[01:59:15.520 --> 01:59:20.320]   it and means to the insurance industry it's actually fascinating a lot of inside information
[01:59:20.320 --> 01:59:27.360]   there's two episodes about ai the role of ai reimagined with boris sergal and radu orgadon
[01:59:28.080 --> 01:59:32.640]   uh and they talk about the regulations the accountability expectations that arise when you
[01:59:32.640 --> 01:59:37.680]   use ai to solve complex problems there's a lot of interesting philosophical issues they also
[01:59:37.680 --> 01:59:42.400]   talk about and i always find this fascinating what the future holds for ai and people using it on
[01:59:42.400 --> 01:59:48.880]   a daily basis if you if you're interested in shopping who isn't parts one and two of our
[01:59:48.880 --> 01:59:56.080]   shopping experience reimagined really goes in depth into how tech is changing commerce
[01:59:56.080 --> 02:00:01.360]   the guests tomah speechin and jeremy mays dive into some of the most significant shifts they've
[02:00:01.360 --> 02:00:06.800]   seen in consumer behavior over the last year covid changed a lot it accelerated a lot of the change
[02:00:06.800 --> 02:00:14.720]   uh direct to consumer much more popular uh buy online pickup in store again took off
[02:00:14.720 --> 02:00:19.360]   and how the shift to digital is pushing people in companies to reimagine the way
[02:00:19.360 --> 02:00:25.360]   we shop what is the future of shopping find out with tech reimagined and dava has been doing
[02:00:25.360 --> 02:00:29.680]   this of course that's their business reimagining the relationship between technology and people
[02:00:29.680 --> 02:00:34.880]   for years their podcast tech reimagined explores this relationship on a deeper level
[02:00:34.880 --> 02:00:39.840]   with a look at the most recent experiences with technology and its experts learn more
[02:00:39.840 --> 02:00:46.240]   about how tech is becoming so much more in this world it's constantly growing and changing it's a
[02:00:46.240 --> 02:00:52.320]   podcast you will not want to miss subscribe and listen to tech reimagined just search for that
[02:00:52.320 --> 02:00:57.920]   the podcast from endava from wherever you get your podcast we thank you and dava for supporting
[02:00:57.920 --> 02:01:05.120]   our podcast and we invite everybody to try out there's tech reimagined now back to Jason
[02:01:05.120 --> 02:01:09.360]   Snell and this weekend tech and i'll be back next week thanks Jason for filling in
[02:01:09.360 --> 02:01:15.760]   hello ha means hello and goodbye i don't know which one i'm saying to Leo there both maybe both
[02:01:15.760 --> 02:01:21.920]   maybe both why can't it be both a few other fun stories for you before we wrap up today
[02:01:22.800 --> 02:01:28.720]   this one little philosophical another uh Andy and not co original well not the story but the
[02:01:28.720 --> 02:01:34.560]   link that he sent to me and it's such a good fun weird topic there's a documentary called Roadrunner
[02:01:34.560 --> 02:01:44.240]   which is about the celebrity chef Anthony Bourdain um and he he committed suicide and the the it's
[02:01:44.240 --> 02:01:50.000]   sort of the people involved in his show and people who knew him uh writing uh or doing a documentary
[02:01:50.000 --> 02:01:55.280]   about him including dealing with the fact that he committed suicide and it uses clips he was in
[02:01:55.280 --> 02:02:00.880]   so many shows that it uses a lot of clips of his voice to tell the story however there were things
[02:02:00.880 --> 02:02:08.240]   that he had written that he never said that anyone can recall on tape and so what the documentarians
[02:02:08.240 --> 02:02:17.360]   decided to do was use an ai voice training system trained on his previous recorded words
[02:02:18.160 --> 02:02:24.160]   to generate Anthony Bourdain's voice narrating portions of the documentary with words that he
[02:02:24.160 --> 02:02:29.920]   wrote but never said out loud and this now i've read some unflattering stories about this
[02:02:29.920 --> 02:02:34.320]   documentary saying that it's actually kind of weird and and uh it lied some of his personal
[02:02:34.320 --> 02:02:39.040]   history and that the more you see it the more kind of questionable it is that it was even made but
[02:02:39.040 --> 02:02:43.280]   i haven't seen it i don't even know i don't really want to talk about the documentary itself as much
[02:02:43.280 --> 02:02:50.000]   as this idea that Andy sent to me about is it like it's his words right but he never said them out
[02:02:50.000 --> 02:02:57.600]   loud so what is and i'll grant you three decades ago we had video of uh Fred Astaire dancing with
[02:02:57.600 --> 02:03:02.080]   vacuum cleaner on a Super Bowl ad so it's not as if this sort of thing hasn't happened before but
[02:03:02.080 --> 02:03:08.000]   at this point the technology exists uh with ai there's a company called dscript that makes a uh
[02:03:08.000 --> 02:03:14.240]   an audio editing tool that if you feed it enough audio of a human being it will let you type anything
[02:03:14.240 --> 02:03:19.520]   you want and you can get that person's voice to say it and we've seen this before Roger Ebert had a
[02:03:19.520 --> 02:03:25.760]   Ebert ask voice built for him that he decided he didn't want to use and here we have a case where
[02:03:25.760 --> 02:03:31.760]   Anthony Bourdain cannot speak for himself he's no longer with us it sounds like perhaps some of
[02:03:31.760 --> 02:03:36.640]   the people closest to him were not consulted on this and yet here's his voice that's being made
[02:03:36.640 --> 02:03:44.400]   to say things that he did right but then he never actually said it is a real weird 21st century ethical
[02:03:44.400 --> 02:03:50.320]   conundrum so and Andy thank you for recommending this to me um i feel like you have strong feelings
[02:03:50.320 --> 02:03:56.160]   about this that's just my guess yeah it's it just has so many different layers to it but first of
[02:03:56.160 --> 02:04:03.440]   all in the context of a documentary it's unnecessary and it's really damaging because you yourself
[02:04:03.440 --> 02:04:08.720]   brought up that uh val kill the val killman has it has a documentary because of his throat cancer
[02:04:08.720 --> 02:04:14.560]   he can't he his speech is very much compromised so for a lot of the narration that's in quote his
[02:04:14.560 --> 02:04:20.800]   voice he the he and the filmmakers cast someone to represent his voice uh Roger Ebert sounds like
[02:04:20.800 --> 02:04:26.560]   him but it's not him exactly exactly yeah Roger Ebert did the exact same thing uh when his memoir
[02:04:26.560 --> 02:04:32.160]   while he was alive was produced into a movie he chose a voice artist uh he's actually he's actually
[02:04:32.160 --> 02:04:37.920]   also a voice artist who uh does a lot of the uh empire uh empire villains in the uh in a lot of the
[02:04:37.920 --> 02:04:43.680]   star star wars games but he got to choose who's going to represent his voice uh Anthony Bourdain
[02:04:43.680 --> 02:04:50.320]   did not have a say in this as you say but in the context of a documentary um when you find that
[02:04:50.320 --> 02:04:58.080]   one thing has been faked it's ruins your it ruins your perceptions of everything else this is my
[02:04:58.080 --> 02:05:02.400]   problem with uh with Michael Moore documentaries that you'll you'll you'll watch it and then you'll
[02:05:02.400 --> 02:05:07.600]   say wait a minute that's totally not true I know for a fact that's not true and then no matter how
[02:05:07.600 --> 02:05:14.480]   positive or or uh uh humanist this whole this whole story is I can't trust anything that's being
[02:05:14.480 --> 02:05:19.200]   said in this documentary so if they chew if the filmmaker chose not to simply have somebody
[02:05:19.200 --> 02:05:24.720]   us not even necessarily sound alike but someone who has a compatible voice read this email I think
[02:05:24.720 --> 02:05:30.880]   it's a 42nd email to a to a friend it's it really interferes with how you start to go with this
[02:05:30.880 --> 02:05:36.160]   but now but outside of a documentary it really is very very interesting um I'm sure that the
[02:05:36.160 --> 02:05:43.200]   reason why Fred Astaire was able to be uh likeness was able to have a vacuum cleaner in his hand
[02:05:43.200 --> 02:05:47.040]   dancing up and down the steps from uh one of his famous clips from I think it was from the park
[02:05:47.040 --> 02:05:52.960]   was a broad way but I could be wrong with that is because he was a big celebrity he had really good
[02:05:52.960 --> 02:05:59.120]   lawyers and part of his estate was he he made the decisions that well after I die is it okay to
[02:05:59.120 --> 02:06:03.680]   use my likeness and my and films of my movies in advertising and he signed off on that maybe he
[02:06:03.680 --> 02:06:08.480]   didn't anticipate how it could be used but he did think about it he could have said no he decided
[02:06:08.480 --> 02:06:12.560]   to say yes so I think that now we're going to have this extra element where people are going to
[02:06:12.560 --> 02:06:17.680]   where uh celebrities are going to have to I were going to say I do not give my if I don't want my
[02:06:17.680 --> 02:06:22.880]   voice to be recreated if I don't want my likeness to be recreated then that then then that's going
[02:06:22.880 --> 02:06:27.120]   to be in their wills but let's go into something a little bit more personal this is what I'm going
[02:06:27.120 --> 02:06:32.400]   to cut I swear to god I've got a shot clock in my head because this could be like two hours of
[02:06:32.400 --> 02:06:38.640]   conversation for the three of us but for the past couple of years I've been thinking about how technology
[02:06:38.640 --> 02:06:46.640]   um how technology plays a role in the grieving process when we lose a loved one
[02:06:47.280 --> 02:06:55.280]   that uh we've all been to we've all now been to funerals in which there is always that that
[02:06:55.280 --> 02:07:01.680]   little hdtv on the table that has a slideshow of all kinds of different pieces of video and and
[02:07:01.680 --> 02:07:06.640]   and photos that were collected over the course of this person's life and helps us to remember
[02:07:06.640 --> 02:07:12.400]   who they were but imagine a little step in the future as you said we do have this technology
[02:07:12.400 --> 02:07:17.280]   where if you have enough samples of someone's voice you can't recreate that voice what if
[02:07:17.280 --> 02:07:23.360]   for certain people they decided that would help them with their grieving process if they could have
[02:07:23.360 --> 02:07:30.720]   the emails that that your your mom or your dad sent to you read in their voice with a synthetic
[02:07:30.720 --> 02:07:35.200]   voice that they create or even this is going to sound really creepy but again people grieve in
[02:07:35.200 --> 02:07:41.680]   their own ways what if you decided that I want to have I I don't want to have my my smart speaker
[02:07:41.680 --> 02:07:47.120]   speak and my my dad mother's voice but I want a voice that reminds me of my mother that maybe a
[02:07:47.120 --> 02:07:52.640]   voice that that maybe has grew up in the same neighborhood as my mother what does does that
[02:07:52.640 --> 02:07:58.400]   become the sort of technology where it is helpful and healing or is that just so creepy we want no
[02:07:58.400 --> 02:08:03.760]   part of that uh forever and ever these are questions that we're going to have to start asking ourselves
[02:08:03.760 --> 02:08:09.280]   I mean we we are in a we are in a society where it is part of our grieving tradition to have
[02:08:10.080 --> 02:08:17.280]   the body of a loved one on display for a couple of days in a funeral home that in itself could
[02:08:17.280 --> 02:08:23.920]   seem pretty weird it might seem less weird than you know my my favorite but my my my dad read the
[02:08:23.920 --> 02:08:29.200]   Hobbit to me like as a as a little kid when I used to go to bed I want to hear my my father's voice
[02:08:29.200 --> 02:08:33.760]   reading the Hobbit to me from start to finish that if technology like this could be used for
[02:08:33.760 --> 02:08:39.120]   something like that maybe doesn't matter that the deceased person didn't get a choice in the matter
[02:08:39.120 --> 02:08:44.160]   because on some level they're no longer here and because these products are not being used in a
[02:08:44.160 --> 02:08:49.680]   public fashion maybe that's a situation in which we can sort of overlook the fact that we didn't
[02:08:49.680 --> 02:08:55.120]   get sent from the living I will commend to everybody by the way the black mirror episode be right back
[02:08:55.120 --> 02:09:00.800]   which is about a version of this what Andy just described and is again philosophically there are
[02:09:00.800 --> 02:09:05.360]   lots of questions here about uh what do you have the right to do I think it's interesting like if
[02:09:05.360 --> 02:09:11.280]   Al Kilmer wanted an AI reconstruction of his voice to narrate his movie because he's alive he could
[02:09:11.280 --> 02:09:17.200]   choose that and he chose not to Anthony Bourdain doesn't get that choice but you're this is of
[02:09:17.200 --> 02:09:22.640]   21st century choice that didn't exist before that that for all that Roger Ebert went through
[02:09:22.640 --> 02:09:28.160]   trying to find a synthetic voice based on all of his hours of audio he wasn't satisfied with it I
[02:09:28.160 --> 02:09:34.560]   suspect that today they could make a pretty immaculate Roger Ebert voice for him but he isn't here
[02:09:34.560 --> 02:09:40.000]   anymore and so we probably shouldn't do that even though it would be great to hear him reading his
[02:09:40.000 --> 02:09:46.720]   essays because he's no longer here I should say I believe that the Bourdain estate did sign off on
[02:09:46.720 --> 02:09:54.320]   this which well that doesn't mean I like it that does mean something but an even more important
[02:09:54.320 --> 02:09:58.880]   fact is that the only reason we're discussing this at all is because a writer for the New Yorker
[02:09:58.880 --> 02:10:05.200]   named Helen Rosner saw they was writing about the documentary and saw it and wondered to herself
[02:10:05.200 --> 02:10:10.480]   why do they have audio of Anthony Bourdain reading his email it's it's really weird they would have
[02:10:10.480 --> 02:10:16.720]   that so she she asked them and Morgan Neville the uh who's a really good documentary on him who
[02:10:16.720 --> 02:10:23.120]   made this uh acknowledge that they did that in several instances um but he did not disclose it
[02:10:23.120 --> 02:10:28.320]   and he seemed I have to say a little bit flippant in this response to her he kind of said there
[02:10:28.320 --> 02:10:31.760]   are other places where we did this but you're not gonna be able to tell what they are and maybe
[02:10:31.760 --> 02:10:36.400]   someday we can have this debate but I'm not gonna have a debate about the ethics while I'm promoting
[02:10:36.400 --> 02:10:41.760]   my movie right now yeah and his ex his Bourdain's ex-wife said that she didn't give permission but
[02:10:41.760 --> 02:10:46.160]   she's probably not the one who's involved with the estate who's giving permission and that really
[02:10:46.160 --> 02:10:53.040]   bothers me if if it had started with you know a title saying some of some of Anthony Bourdain's
[02:10:53.040 --> 02:10:58.000]   dialogue in this was recreated I think it would be a radically different situation than than what
[02:10:58.000 --> 02:11:02.480]   they painted themselves into the disclosure by not being upfront about it I think it's to Andy's
[02:11:02.480 --> 02:11:08.400]   point about um trust of the information that you're seeing in a documentary even if you just put
[02:11:08.400 --> 02:11:16.400]   up a super title that said um this is uh a reconstruction yes of his voice um that you're hearing reading
[02:11:16.400 --> 02:11:21.600]   this uh you might have a reaction of this is amazing we get to hear his voice reading his words
[02:11:21.600 --> 02:11:25.680]   but instead it's just a little bit weird I'm I'm not saying this is a scandal either I just am
[02:11:25.680 --> 02:11:33.040]   saying that it is it gives one pause about the capabilities that we have to do things like this
[02:11:33.040 --> 02:11:38.240]   and I think that's interesting I mentioned D-script earlier the idea with D-script is if you're
[02:11:38.240 --> 02:11:45.760]   working for a very popular famous podcast let's say this week in tech with Leo you could get Leo
[02:11:45.760 --> 02:11:51.840]   to speak all the words into D-script that it wants and then when Leo goes on vacation he doesn't even
[02:11:51.840 --> 02:11:58.800]   have to pre-tape the ads or what if one ad changed after he already left uh then somebody here could
[02:11:58.800 --> 02:12:03.760]   re-type that part and output it as audio and it would sound like Leo said it and that's why that
[02:12:03.760 --> 02:12:08.560]   product exists so it's generally the idea is that it's with the approval of the person it's like a
[02:12:08.560 --> 02:12:15.120]   very busy correspondent is not around and you need one slight wording change in the piece before you
[02:12:15.120 --> 02:12:22.480]   lock it but still when I first saw that feature I could not believe it and since you brought it up
[02:12:22.480 --> 02:12:28.560]   Andrew Mason actually put a blog up a blog post on Friday addressing this exact thing after a couple
[02:12:28.560 --> 02:12:33.360]   paragraphs said for what it's worth here's our take unapproved voice cloning is a slippery slope
[02:12:33.360 --> 02:12:37.120]   as soon as you get into a world where you're making subjective judgment calls about whether
[02:12:37.120 --> 02:12:42.640]   specific cases can be ethical it won't be long before anything goes even if you think that the
[02:12:42.640 --> 02:12:48.480]   use in the Bardane documentary is ethical is there enough upside to open this pandara's box we think
[02:12:48.480 --> 02:12:53.200]   the answer is no whatever the path may be to universal enlightenment we can get there without
[02:12:53.200 --> 02:13:02.880]   reanimating the debt I told you this would be a fun segment right I have to wonder did it not
[02:13:02.880 --> 02:13:09.120]   occur to them to have the people that received the emails read this and say and he sent me this
[02:13:09.120 --> 02:13:15.040]   and then just read it because I mean that just feels like they could have avoided an entire ethical
[02:13:15.040 --> 02:13:23.280]   minefield yeah you're so right yeah I think this is the thing is they didn't think it would be a
[02:13:23.280 --> 02:13:28.160]   big deal and Helen Rosser noticed it and it became more of a thing and I think the answer is that
[02:13:28.160 --> 02:13:33.280]   this is the kind of thing we have discussions like this about and the next documentary says you know
[02:13:33.280 --> 02:13:38.720]   what let's disclose this we'll still use it but we're going to disclose it or let's not use it and
[02:13:38.720 --> 02:13:43.520]   use somebody else instead whatever but they'll they'll probably think about it a little bit more that's
[02:13:43.520 --> 02:13:48.720]   all I'm saying just maybe think about it a little more um a couple stories before we go and thank
[02:13:48.720 --> 02:13:54.160]   you all for being with us so and thank you panelists for being with us um one story before we go
[02:13:54.160 --> 02:13:58.160]   Harry and I are residents of the Bay Area I just wanted to point out this story in the New York
[02:13:58.160 --> 02:14:05.840]   Times this week headlined Tech Workers who swore off the Bay Area are coming back anybody who has
[02:14:05.840 --> 02:14:11.520]   been in the Bay Area they were San Francisco or anywhere in the surrounding for any amount of time
[02:14:11.520 --> 02:14:18.160]   remembers how many they actually they remember when they lose track of how many times the Bay Area
[02:14:18.160 --> 02:14:24.560]   in San Francisco is declared dead I have lost track it happened when I moved here in the early 90s
[02:14:24.560 --> 02:14:28.960]   it happened in the late 90s during the dot com crash it happened in the late 2000s during the
[02:14:28.960 --> 02:14:34.240]   financial crisis it happened again during covid the idea that everybody's leaving the Bay Area and
[02:14:34.240 --> 02:14:37.840]   everybody's leaving San Francisco and they'll never come back and everybody who's here goes
[02:14:37.840 --> 02:14:45.200]   probably not and then wait you know set your watch wait for it wait for it oh here's the piece
[02:14:45.200 --> 02:14:49.920]   about how they're not actually leaving or if they did leave they're coming back I you know I don't
[02:14:49.920 --> 02:14:55.280]   really have a lot to say about this other than it always happens like this this is just how it is
[02:14:55.280 --> 02:15:00.880]   this is San Francisco is a city and this is a region founded on the gold rush people rush in
[02:15:00.880 --> 02:15:07.840]   to get rich and some of them stay and some of them go and it's kind of a weird place but there's
[02:15:07.840 --> 02:15:13.600]   value to be here and so every time I'm just saying if you see San Francisco declared dead again
[02:15:13.600 --> 02:15:20.320]   take the under it's probably not going to happen and some of the people who did move like move to
[02:15:20.320 --> 02:15:25.440]   San Jose yeah if your new home is 45 minutes away from your old one you really have not made a big
[02:15:25.440 --> 02:15:33.680]   life change a lot of people move to to Reno or or Truckee which is right on the Nevada border
[02:15:33.680 --> 02:15:40.000]   which is not quite the same or having a vacation house up there and relocating temporarily like it's
[02:15:40.000 --> 02:15:45.280]   not California's a big place too so people can relocate out of the city center to a place where
[02:15:45.280 --> 02:15:50.880]   maybe they can afford a house which they've been doing for many years since time immemorial
[02:15:50.880 --> 02:15:56.080]   tons of people who have moved up to Truckee or Tahoe that's that's not a new trend yeah yeah so
[02:15:56.080 --> 02:16:03.200]   I'm just saying San Francisco's never over it's just never over everybody will always think it's
[02:16:03.200 --> 02:16:08.640]   over it's never over it always comes back it always will it changes all the time and people do leave
[02:16:08.640 --> 02:16:15.520]   but new people come and that's just how it is so anyway really a dog bites man kind of story the
[02:16:15.520 --> 02:16:22.800]   tech workers are back and finally last story my friend Glenn Fleischman is really interested in
[02:16:22.800 --> 02:16:28.080]   lots of quirky things including old type he recently bought a replica of a Gutenberg bible
[02:16:28.080 --> 02:16:32.880]   it's Glenn you if you know Glenn you know you get it but this story is so good and I wanted to
[02:16:32.880 --> 02:16:39.760]   point it out to people it's a story from Antigone Journal.com it is called Laura Ipsom filler fail killer
[02:16:39.760 --> 02:16:45.840]   tale if you've ever done any desktop publishing or seen weird things posted on the internet that
[02:16:45.840 --> 02:16:53.360]   don't make any sense you might wonder what is Laura Ipsom Dolar sit on it this strange latin-ish
[02:16:53.360 --> 02:17:01.040]   text that isn't quite right for latin where did it come from why is it here well this story tells
[02:17:01.040 --> 02:17:05.680]   all and the story is really pretty great because it turns out there are also great pictures of like
[02:17:06.720 --> 02:17:12.000]   bags of food packaging that never got the actual text that's supposed to go that's what Laura
[02:17:12.000 --> 02:17:17.360]   Ipsom is it's filler text when the regular text isn't ready so you have a bag that says food and
[02:17:17.360 --> 02:17:24.960]   drink Laura Ipsom Dolar sit on it sounds great family special Laura Ipsom Dolar sit on it the idea
[02:17:24.960 --> 02:17:29.600]   there is you replace that with a real text later sometimes people forget but it's a great way for
[02:17:29.600 --> 02:17:33.760]   designers to mock up a page and see what it looks like with type on it this story though is
[02:17:33.760 --> 02:17:39.760]   amazing because it finds that this particular order of latin occurred in a very particular book
[02:17:39.760 --> 02:17:46.160]   that was in addition of Cicero that was published in the early 20th century that that was widely
[02:17:46.160 --> 02:17:52.960]   distributed and probably somebody somewhere in the late 60s because that's what it originated it is
[02:17:52.960 --> 02:17:57.200]   not from time immemorial it is not from the time of the Romans it is not from the time of Gutenberg
[02:17:57.200 --> 02:18:01.760]   it's from the late 60s somebody was like oh we should probably have some fake text to put in
[02:18:01.760 --> 02:18:09.360]   there just in case and they picked up a used copy of this latin by Cicero and they dumped it in there
[02:18:09.360 --> 02:18:16.000]   and then changed it the other great part of the story changed it to add some more English isish
[02:18:16.000 --> 02:18:22.800]   punctuation and word endings so that anybody who knows latin looks at and goes this isn't latin
[02:18:22.800 --> 02:18:28.880]   this doesn't make any sense and they found the actual literal page number in the Cicero book
[02:18:29.760 --> 02:18:33.760]   in this particular edition that contains the source text and there's actually a sort of a second
[02:18:33.760 --> 02:18:40.320]   page a little later that they kind of glom together I love this stuff like like the fact that somebody
[02:18:40.320 --> 02:18:46.320]   on the internet went to the trouble to find out the origin of loram ipsum text boy it gives talk
[02:18:46.320 --> 02:18:51.200]   about our first segment this is what gives me hope for humanity that this kind of stuff is going on
[02:18:51.200 --> 02:18:56.880]   it's great yeah the company that did it was letrasette which some of us get nostalgic about because
[02:18:56.880 --> 02:19:02.080]   they made these sheets of rub on letters which before desktop publishing was super useful
[02:19:02.080 --> 02:19:07.360]   yeah and so they needed some some Greek text we call it Greek text even though it looks latin
[02:19:07.360 --> 02:19:14.000]   the idea of filler text now modern modern journalism students in the like know that we generally use
[02:19:14.000 --> 02:19:19.200]   things that are misspelled words so that our spelling checkers catch them things like tk you see
[02:19:19.200 --> 02:19:26.240]   that a lot or lead spelled led or deck spelled d e k or other words that are should be real words
[02:19:26.240 --> 02:19:30.720]   but they're misspelled so that everybody knows that's not right you shouldn't put that in the
[02:19:30.720 --> 02:19:36.160]   newspaper tomorrow you shouldn't post that on the website but back in the day letrasette needed some
[02:19:36.160 --> 02:19:40.800]   sort of generic text and anybody who knows what loram ipsum is it's just like I feel like this
[02:19:40.800 --> 02:19:45.760]   is such a great origin story I I assume that this was an ancient printing practice you too that's the
[02:19:45.760 --> 02:19:53.680]   late 60s in london yep nope I remember I took I took latin as my as my language in high school
[02:19:53.680 --> 02:19:59.680]   and after my second year I came across I came across loram ipsum and I decided to like try to
[02:19:59.680 --> 02:20:05.040]   translate it I got to loram ipsum to loram it I got as far as the whip is itself both pleasure
[02:20:05.040 --> 02:20:09.680]   and pain before realizing okay I'm in way over my head this was fun but I'm not going to go any
[02:20:09.680 --> 02:20:14.320]   farther than this one of the mind-blowing things is that it's not loram it's dilloram right and
[02:20:14.320 --> 02:20:19.760]   the de is broken across the page break and the person who was putting it in at letrasette just
[02:20:19.760 --> 02:20:24.640]   didn't care didn't care you want to what since we're talking about letrasette wanted to know
[02:20:24.640 --> 02:20:29.040]   something else it's really really fun if you look at like if you look at the actual like movie
[02:20:29.040 --> 02:20:35.520]   props that that are made for like you know military backpacks and high tech like futuristic
[02:20:35.520 --> 02:20:39.120]   brie breathers and stuff like that and there's like that you know how like a lot of stuff will
[02:20:39.120 --> 02:20:43.600]   have like if you buy chainsaw have like a block of safety information like somewhere it's like oh
[02:20:43.600 --> 02:20:48.000]   by the way make sure that you bleed off the gases from this before you remove this housing
[02:20:48.000 --> 02:20:53.360]   there are a bunch of the letrasette rub on lettering packages they have like a block
[02:20:53.360 --> 02:20:58.480]   of tech that simply said oh this is a copyright letrasette you apply these by simply putting the
[02:20:58.480 --> 02:21:04.720]   layer on the spoon and so you'll have like boba Fett's rocket pack on the back has the instructions
[02:21:04.720 --> 02:21:12.480]   from letrasette on how to apply press-off lettering it's like somebody it's like giving a cake
[02:21:12.480 --> 02:21:16.240]   or decoration order and saying okay put this word underneath that put this other word are they
[02:21:16.240 --> 02:21:21.600]   right underneath that put this other word like no no don't don't print the instructions like
[02:21:21.600 --> 02:21:26.480]   a like boat like bojack horse horse mr that was like that was a running gag like all season
[02:21:26.480 --> 02:21:30.640]   where like at the like the final episode of the season there's a banner after like this person
[02:21:30.640 --> 02:21:35.680]   always gets this screwed up there's a banner that simply says come on this should not be this hard
[02:21:35.680 --> 02:21:42.800]   instead of happy birthday well this has been irrelevant to our conversation but this has been
[02:21:42.800 --> 02:21:48.480]   a technical identity everybody who stuck with us for two hours we turned it around didn't it we
[02:21:48.480 --> 02:21:54.720]   saved it there you go we did it we made it fun eventually uh and that's in in large part due to
[02:21:54.720 --> 02:22:00.480]   my wonderful panel thank you all rosemary orchard where can people find you and the stuff that you
[02:22:00.480 --> 02:22:06.800]   do if you go to rosemaryorchard.com then there's links to all of the things social media podcasts
[02:22:06.800 --> 02:22:12.560]   books etc so it's all there so easy rosemaryorchard.com it's just your name that's that's uh thank you
[02:22:12.560 --> 02:22:16.640]   for uh making it so easy to find your stuff harry macraken how about you?
[02:22:16.640 --> 02:22:22.720]   festcompany.com for most of it um twitter i'm harry macraken that's where you get your radio
[02:22:22.720 --> 02:22:27.440]   shack content those are probably the two go to places once they know great while i write about
[02:22:27.440 --> 02:22:32.240]   radio shack even for fest company and they haven't fired me up okay good very good and Andy and i
[02:22:32.240 --> 02:22:37.680]   go how about you uh well harry i'm owed like 10 years worth of battery of the month club nine
[02:22:37.680 --> 02:22:42.480]   volt batteries i don't know if you're the person i need to talk to not my problem okay you have to
[02:22:42.480 --> 02:22:48.240]   talk to me after i've heard that a lot but i spelled my last name which is no easy feat i'm not
[02:22:48.240 --> 02:22:54.400]   going twitter i'm not going on instagram uh you can also listen to me i'm on most fridays sometimes
[02:22:54.400 --> 02:23:01.200]   thursday's on wgbh boston's npr station just go to wgbhnews.org if you search right in there you'll
[02:23:01.200 --> 02:23:07.280]   see all of my weekly tech roundups news roundups and i want to thank leo one last time with a
[02:23:07.280 --> 02:23:13.280]   little bit of a story i was in mauie last week my family and i did a an adventure on what's called
[02:23:13.280 --> 02:23:18.880]   the mauie sailing canoe mauie sailing canoe.com we're out there we're snorkeling we're having a
[02:23:18.880 --> 02:23:25.600]   good time and as we're sailing along the guy sage who is piloting this boat with his son
[02:23:25.600 --> 02:23:30.880]   uh named crew he was his name is crew he was also the crew which i thought it was pretty clever
[02:23:30.880 --> 02:23:36.000]   he says what do you do and you know i do that ah boy right on the internet and i use podcast
[02:23:36.000 --> 02:23:41.600]   and i have to explain podcast i'm like tech podcasting and he goes oh leo loport
[02:23:41.600 --> 02:23:49.200]   he said the magic words that was a tech podcasting oh leo loport so thank you leo
[02:23:49.200 --> 02:23:54.880]   they're listening to you in huai where you are right now and leo will be back next week thank you
[02:23:54.880 --> 02:24:00.080]   all for letting me sub in thanks to my guests again thanks to ant for sitting in the producers
[02:24:00.080 --> 02:24:06.160]   chair this week and guess what folks another twit it barely fits but it's in the can
[02:24:06.160 --> 02:24:18.000]   do the twist all right do the twist baby do the twist all right do the twist baby do the

