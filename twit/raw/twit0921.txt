
[00:00:00.000 --> 00:00:05.340]   Coming up on this week in Tech, I am Micah Sargent subbing in for Leo LaPorbele. He's on vacation.
[00:00:05.340 --> 00:00:09.840]   And I have a great show planned for you. We've got the Swiftie panel.
[00:00:09.840 --> 00:00:15.340]   It's Christina Warren, senior developer advocate at GitHub. Alex Wilhelm, reporter at TechCrunch,
[00:00:15.340 --> 00:00:19.140]   and a bra al-Hiti video host and producer for CNET.
[00:00:19.140 --> 00:00:22.740]   And we've got a lot to talk about. We start off the show,
[00:00:22.740 --> 00:00:28.080]   running down some April Fool's gags that kind of turns into just a desire
[00:00:28.080 --> 00:00:33.800]   to have fun back on the internet. Then we hit AI. There's a lot to talk about,
[00:00:33.800 --> 00:00:38.380]   including that open letter regarding AI and how it's dangerous.
[00:00:38.380 --> 00:00:42.900]   Hmm, was what they said true and were the people who actually signed the letters,
[00:00:42.900 --> 00:00:47.780]   truly the people who signed the letters. We also talk about Twitter.
[00:00:47.780 --> 00:00:52.480]   Some certain celebrities getting boosted, where we might go outside of Twitter,
[00:00:52.480 --> 00:00:57.080]   how we wish we could return to how Twitter used to make us feel.
[00:00:57.080 --> 00:01:04.380]   And we complain a little about GM deciding that CarPlay and Android Auto
[00:01:04.380 --> 00:01:10.380]   are not worth keeping around in future electronic vehicles.
[00:01:10.380 --> 00:01:13.880]   It's all that plus so much more coming up on This Week in Tech.
[00:01:13.880 --> 00:01:16.280]   [MUSIC]
[00:01:16.280 --> 00:01:18.080]   Podcasts you love.
[00:01:18.080 --> 00:01:19.080]   >> From people you trust.
[00:01:19.080 --> 00:01:20.580]   [MUSIC]
[00:01:20.580 --> 00:01:22.280]   >> This is Twitter.
[00:01:22.280 --> 00:01:27.980]   [MUSIC]
[00:01:27.980 --> 00:01:29.280]   >> This is Twitter.
[00:01:29.280 --> 00:01:34.180]   This Week in Tech hosted this time by Mike Asargent, episode 921,
[00:01:34.180 --> 00:01:39.980]   recorded Sunday, April 2, 2023, the Swiftie Panel.
[00:01:39.980 --> 00:01:43.480]   This episode of This Week in Tech is brought to you by Lookout,
[00:01:43.480 --> 00:01:48.080]   whether on a device or in the cloud, your business data is always on the move.
[00:01:48.080 --> 00:01:53.580]   Minimize risk, increase visibility, and ensure compliance with Lookout's unified platform.
[00:01:53.580 --> 00:01:56.080]   Visit Lookout.com today.
[00:01:56.080 --> 00:01:58.980]   And by ExpressVPN.
[00:01:58.980 --> 00:02:02.480]   ExpressVPN lets you choose from over 90 different countries.
[00:02:02.480 --> 00:02:05.880]   So every time you run out of stuff to watch, you can fire up the app,
[00:02:05.880 --> 00:02:09.380]   switch your country, and hit connect. Protect your data.
[00:02:09.380 --> 00:02:15.880]   For three extra months free with a one-year package, go to ExpressVPN.com/twit.
[00:02:15.880 --> 00:02:19.580]   >> Thanks for listening to this show as an ad supported network.
[00:02:19.580 --> 00:02:26.380]   We are always looking for new partners with products and services that will benefit our qualified audience.
[00:02:26.380 --> 00:02:27.980]   Are you ready to grow your business?
[00:02:27.980 --> 00:02:32.180]   Reach out to advertise at twit.tv and launch your campaign now.
[00:02:32.180 --> 00:02:38.180]   [MUSIC]
[00:02:38.180 --> 00:02:43.980]   It's time for Twit this week in Tech and I am your host this week, Micah Sargent.
[00:02:43.980 --> 00:02:49.680]   That's right, as many of you probably already know, because it seems like you've got his calendar in your own phones.
[00:02:49.680 --> 00:02:56.580]   Leo Laport is out as he is on vacation and will be for most of this month.
[00:02:56.580 --> 00:03:01.780]   We will have several guest hosts of Twit while he is away.
[00:03:01.780 --> 00:03:03.480]   And today it just so happens to be me.
[00:03:03.480 --> 00:03:04.180]   And I'm so excited.
[00:03:04.180 --> 00:03:11.580]   This is my first time hosting this week in Tech and I've got a great panel prepared for you.
[00:03:11.580 --> 00:03:18.780]   First off, video host and producer for CNET and my fellow Midwesterner, at least original Midwesterner.
[00:03:18.780 --> 00:03:21.780]   It's a bra or al-hiti. Welcome.
[00:03:21.780 --> 00:03:24.180]   >> Thank you so much for having me. I'm so excited to be on.
[00:03:24.180 --> 00:03:29.180]   >> Yeah, it's great to get you back on the show and I'm looking forward to chatting with you.
[00:03:29.180 --> 00:03:35.980]   We are also joined across some level of distance by senior developer advocate at GitHub.
[00:03:35.980 --> 00:03:39.180]   It's Christina Warren. Hello.
[00:03:39.180 --> 00:03:41.680]   >> Hello. Hello. It's so good to be back.
[00:03:41.680 --> 00:03:44.580]   It's so good to see you in the big hosting chair. I love this.
[00:03:44.580 --> 00:03:45.380]   >> You are.
[00:03:45.380 --> 00:03:46.780]   >> I love this so much.
[00:03:46.780 --> 00:03:50.780]   You and I have known each other for such a long time and I feel like this is like,
[00:03:50.780 --> 00:03:53.380]   I love this and I just love seeing you in the big hosting chair.
[00:03:53.380 --> 00:03:56.780]   >> It is a milestone. I'm like, I'm living it up right now.
[00:03:56.780 --> 00:03:57.780]   I got to tell you.
[00:03:57.780 --> 00:04:01.980]   I'm also excited to be joined by a reporter at TechCrunch,
[00:04:01.980 --> 00:04:05.580]   but honestly, just an all-around swell guy.
[00:04:05.580 --> 00:04:08.380]   It's Alex Wilhelm. Welcome back to the show, Alex.
[00:04:08.380 --> 00:04:13.780]   >> Thank you for having me. I just want to say, Mike, you look amazing.
[00:04:13.780 --> 00:04:16.780]   The haircut fit overall. You just look like you're glowing.
[00:04:16.780 --> 00:04:18.180]   >> Oh, my goodness.
[00:04:18.180 --> 00:04:21.580]   Thank you so much. Wow. It's such a kind panel today.
[00:04:21.580 --> 00:04:24.780]   I think it's a kind panel a lot of time.
[00:04:24.780 --> 00:04:29.780]   I wanted to start the show off by, I guess it's going to be a bit of a throwback
[00:04:29.780 --> 00:04:37.380]   because I remember a day where I would have to remind myself every night
[00:04:37.380 --> 00:04:43.580]   before April 1st rolled around to be very, very careful when I logged on to the internet
[00:04:43.580 --> 00:04:46.580]   the next day that I didn't believe anything that was happening
[00:04:46.580 --> 00:04:51.180]   because there were April Fool's jokes aplenty and lots of different online sites
[00:04:51.180 --> 00:04:56.380]   had lots that they wanted to share. And this year, in fact, this morning,
[00:04:56.380 --> 00:05:00.380]   I thought, "Oh, my goodness, did anything happen on April Fool's Day?"
[00:05:00.380 --> 00:05:06.380]   So I thought I'd ask the panel if any April Fool's pranks from Tech Companies
[00:05:06.380 --> 00:05:11.180]   stuck out to you. And I think, Alex, you said there was one that caught your eye.
[00:05:11.180 --> 00:05:16.180]   >> Yeah, it was actually kind of a failed April 1st prank
[00:05:16.180 --> 00:05:19.580]   because Elon Musk promised that everyone with a legacy Twitter blue
[00:05:19.580 --> 00:05:24.180]   or Trekmark was going to lose it. And I woke up yesterday morning waiting to see the demise
[00:05:24.180 --> 00:05:29.180]   of my verified status only to find out that apparently not technically possible.
[00:05:29.180 --> 00:05:31.780]   So as of before the show, still verified.
[00:05:31.780 --> 00:05:34.180]   So I guess Elon Musk pranked himself.
[00:05:34.180 --> 00:05:38.180]   That's a good one. That's a good one.
[00:05:38.180 --> 00:05:41.180]   Yeah, did anyone else see anything? A bra.
[00:05:41.180 --> 00:05:46.580]   I was wondering if you knew if TikTok did anything for April Fool's Day.
[00:05:47.580 --> 00:05:52.780]   That's a great question. I didn't see anything I would have because I spent on Godly hours on that app.
[00:05:52.780 --> 00:05:56.980]   But also, Twitter was the main thing for me too.
[00:05:56.980 --> 00:05:59.580]   That was the first thing I did when I woke up also where I was like,
[00:05:59.580 --> 00:06:02.780]   "Is that blue check still there?" And it is. And then I was thinking,
[00:06:02.780 --> 00:06:05.780]   "Okay, I guess they said they're going to start taking away the birth."
[00:06:05.780 --> 00:06:10.580]   It would start on April 1st. So I'm kind of still just watching.
[00:06:10.580 --> 00:06:15.780]   But that was really where my attention was. But TikTok, I didn't kind of disappointed, honestly.
[00:06:15.780 --> 00:06:19.180]   I don't know if we're all just, are we all over being pranked?
[00:06:19.180 --> 00:06:21.180]   I don't know. I don't know. I even kind of...
[00:06:21.180 --> 00:06:22.980]   That's... I don't know what's going on.
[00:06:22.980 --> 00:06:26.380]   I thought I was, but its absence makes the heart grow fonder.
[00:06:26.380 --> 00:06:27.580]   Now I'm feeling about it.
[00:06:27.580 --> 00:06:31.180]   It's all gone away. And I'm going, "Where's the life on the internet?"
[00:06:31.180 --> 00:06:35.180]   Christina, you've... I mean, especially working at one point in Mashable,
[00:06:35.180 --> 00:06:38.180]   I'm sure covered lots of April Fool's Day pranks.
[00:06:38.180 --> 00:06:39.580]   Where did they all go?
[00:06:39.580 --> 00:06:42.980]   Yeah, I didn't see many either.
[00:06:42.980 --> 00:06:49.180]   And I wonder, I feel like part of it is that there is this part where it had been this fun sort of activity
[00:06:49.180 --> 00:06:51.180]   and you would see companies do really creative things.
[00:06:51.180 --> 00:06:55.580]   And then as what almost always happens to brands, like ruin it, they just go too far.
[00:06:55.580 --> 00:07:00.380]   And it just gets the point where it crosses over from being kind of cute to being cringe.
[00:07:00.380 --> 00:07:04.380]   And that happened a number of years ago. But I'm with you.
[00:07:04.380 --> 00:07:11.580]   Now I am disappointed that TikTok didn't do something or that TikTokers didn't try to do something.
[00:07:11.580 --> 00:07:15.580]   I did see one video, Linus Tech Tips, that YouTube channel,
[00:07:15.580 --> 00:07:23.380]   they made a funny video about Linus basically starting a farm of some sort.
[00:07:23.380 --> 00:07:29.580]   And I watched part of the video, I'll be honest, I was not completely paying attention.
[00:07:29.580 --> 00:07:32.380]   But it seemed funny, but it was obviously obvious.
[00:07:32.380 --> 00:07:34.980]   It was also obvious that it was a joke.
[00:07:34.980 --> 00:07:39.780]   Yeah, maybe that's where it requires is whenever it exists in that middle space
[00:07:39.780 --> 00:07:44.180]   where those of us who know sort of are steeped in technology
[00:07:44.180 --> 00:07:49.180]   where we are going, oh no, the people are going to be fooled by this
[00:07:49.180 --> 00:07:52.380]   and it's going to result in us having to sort of explain.
[00:07:52.380 --> 00:07:55.980]   And remember Volkswagen's April Fool's Day joke?
[00:07:55.980 --> 00:07:57.580]   Where they lied to journalists?
[00:07:57.580 --> 00:07:58.780]   I was thinking about that.
[00:07:58.780 --> 00:07:59.780]   Yeah.
[00:07:59.780 --> 00:08:03.780]   Oh, that was bad. That was bad. It was Volkswagen, right?
[00:08:03.780 --> 00:08:04.780]   Oh, yeah.
[00:08:04.780 --> 00:08:05.780]   Okay, but hear me out though.
[00:08:05.780 --> 00:08:09.580]   I think the internet back in like 2012, 2013 was fun.
[00:08:09.580 --> 00:08:13.580]   Because it was less high stakes because these companies weren't trillion-dollar businesses.
[00:08:13.580 --> 00:08:15.580]   Many of them were kind of on the come up.
[00:08:15.580 --> 00:08:17.580]   There were fewer total people in tech, it felt like.
[00:08:17.580 --> 00:08:19.580]   So a joke had more resonance.
[00:08:19.580 --> 00:08:23.580]   And then everything got a little bit brandified, as you said.
[00:08:23.580 --> 00:08:26.580]   But I kind of missed the, and I know this is not the right phrasing,
[00:08:26.580 --> 00:08:28.580]   but the more innocent era of fun on the internet.
[00:08:28.580 --> 00:08:32.580]   When it was more your friends hanging out, and now Twitter is just like a fire.
[00:08:32.580 --> 00:08:33.580]   Completely.
[00:08:33.580 --> 00:08:38.580]   Well, also, but I was with you, Alex, when Elon said that you're getting rid of it,
[00:08:38.580 --> 00:08:42.580]   you know, you're getting rid of all the checkmarks and it would start on April 1st.
[00:08:42.580 --> 00:08:45.580]   I was like, have you ever been online ever?
[00:08:45.580 --> 00:08:50.580]   Because this is, okay, so the day that you're going to take away all the verification to say,
[00:08:50.580 --> 00:08:53.580]   who is, you know, who, like whatever you want to say about verification,
[00:08:53.580 --> 00:08:57.580]   is it a, you know, obviously it's been a status symbol and it has been, you know,
[00:08:57.580 --> 00:08:58.580]   meaningless for a long time.
[00:08:58.580 --> 00:09:00.580]   But it also has to a certain extent.
[00:09:00.580 --> 00:09:02.580]   And like, oh, yeah, this person is this person.
[00:09:02.580 --> 00:09:07.580]   Okay. So you're going to get rid of that on like the one day of the year where the internet goes crazy anyway.
[00:09:07.580 --> 00:09:09.580]   Like, have you ever been online?
[00:09:09.580 --> 00:09:18.580]   This seems like this is just, you know, was it meta or was it intentional or was it just completely
[00:09:18.580 --> 00:09:21.580]   detached and misunderstanding entirely?
[00:09:21.580 --> 00:09:23.580]   Yeah, I was confusing no matter what.
[00:09:23.580 --> 00:09:28.580]   Yeah, I, again, I do sort of, I'm a little sad.
[00:09:28.580 --> 00:09:35.580]   I will say a network that I'm a part of, the Relay FM Network, where I have a podcast called Clockwise.
[00:09:35.580 --> 00:09:41.580]   They did as they do every year, a little April Fool's Day joke where they pretend to launch a couple of shows.
[00:09:41.580 --> 00:09:46.580]   And so this year there were a couple of shows that were not actual shows that were going to be launched,
[00:09:46.580 --> 00:09:52.580]   including one that was like a nature podcast where you just heard the sounds of nature,
[00:09:52.580 --> 00:09:53.580]   which I was like, you know what?
[00:09:53.580 --> 00:09:54.580]   I don't think I would hate that.
[00:09:54.580 --> 00:09:55.580]   That might be nice.
[00:09:55.580 --> 00:09:56.580]   Right.
[00:09:56.580 --> 00:09:59.580]   And then just go drive somewhere, like literally go to a field.
[00:09:59.580 --> 00:10:00.580]   Yeah, that's true.
[00:10:00.580 --> 00:10:01.580]   They're, that's the podcast.
[00:10:01.580 --> 00:10:03.580]   I do a pro work though.
[00:10:03.580 --> 00:10:04.580]   True.
[00:10:04.580 --> 00:10:05.580]   But I do appreciate that.
[00:10:05.580 --> 00:10:07.580]   Really, FM apparently did an inverse NPR.
[00:10:07.580 --> 00:10:10.580]   They launched fake shows and be our cancel real ones.
[00:10:10.580 --> 00:10:12.580]   Oh, rest in peace.
[00:10:12.580 --> 00:10:15.580]   Oh, now how many shows did they.
[00:10:15.580 --> 00:10:16.580]   I didn't have a follow up there.
[00:10:16.580 --> 00:10:17.580]   That was just me being sad.
[00:10:17.580 --> 00:10:18.580]   Yeah.
[00:10:18.580 --> 00:10:19.580]   I mean, we got a poor one out.
[00:10:19.580 --> 00:10:23.580]   Well, no, I won't pour this out, but I'll pretend to pour it out for the shows.
[00:10:23.580 --> 00:10:30.580]   Speaking of stuff that I think was a little bizarre, a little hard to understand.
[00:10:30.580 --> 00:10:36.580]   In fact, there was one site that said we thought it was an April Fool's Day joke.
[00:10:36.580 --> 00:10:42.580]   Did y'all hear the one about GM deciding that they weren't going to be offering car play
[00:10:42.580 --> 00:10:45.580]   and Android audience in future course?
[00:10:45.580 --> 00:10:46.580]   Yes.
[00:10:46.580 --> 00:10:47.580]   What?
[00:10:47.580 --> 00:10:48.580]   What?
[00:10:48.580 --> 00:10:57.580]   So quickly, GM, which of course, Chevrolet is one of kind of their main car lines, they
[00:10:57.580 --> 00:11:05.580]   seem to decide that in the future, their electric vehicles are not going to support third party
[00:11:05.580 --> 00:11:09.580]   infotainment systems like car play and like Android Auto.
[00:11:09.580 --> 00:11:14.580]   And they are going with sort of a more in-house, although it's through Google infotainment
[00:11:14.580 --> 00:11:18.580]   system that is built into the car as opposed to being a projection system where when you
[00:11:18.580 --> 00:11:24.580]   plug in your phone, it sends all of that information to your car's system and displays that information.
[00:11:24.580 --> 00:11:29.580]   And I have to tell you, when I first heard this, I talked a little bit about it this morning
[00:11:29.580 --> 00:11:37.580]   on Ask the Tech guys, I was a little bit beside myself because it seemed like, I remember
[00:11:37.580 --> 00:11:43.580]   seeing a survey about people who, when they were making a car decision, along with of course,
[00:11:43.580 --> 00:11:47.580]   electric vehicle being part of what they were thinking about, it was the car's infotainment
[00:11:47.580 --> 00:11:49.580]   system that was super important to them.
[00:11:49.580 --> 00:11:50.580]   I'm curious.
[00:11:50.580 --> 00:11:55.580]   Have any of you thought about buying a car or thinking about buying a car and how important
[00:11:55.580 --> 00:12:00.580]   is even in your current car, the infotainment system for you?
[00:12:00.580 --> 00:12:05.580]   I mean, so we had a dumb car, like a equivalent of a dumb phone for a long time, and then
[00:12:05.580 --> 00:12:10.580]   Ladybug, our beloved Volvo S40 started to shake at like 50 miles an hour so she had to go,
[00:12:10.580 --> 00:12:15.580]   and we bought Bob the Subaru, we name our cars, I don't know why, and it has Apple CarPlay,
[00:12:15.580 --> 00:12:16.580]   and it's like magic.
[00:12:16.580 --> 00:12:17.580]   It's the best thing ever.
[00:12:17.580 --> 00:12:21.580]   I can buy Taylor Swift, like Max Volume in the car and looks amazing.
[00:12:21.580 --> 00:12:24.580]   And then I read this and I was like, the thing I just discovered is already being phased
[00:12:24.580 --> 00:12:25.580]   out.
[00:12:25.580 --> 00:12:27.580]   Is this what middle age feels like?
[00:12:27.580 --> 00:12:34.580]   No, I mean, so I don't drive, but if I were to buy a car for someone else to drive me around
[00:12:34.580 --> 00:12:39.580]   in it, it would be an absolute requirement for it to have, you know, CarPlay.
[00:12:39.580 --> 00:12:44.580]   And it's interesting because you mentioned that, Micah, about surveys showing that that's
[00:12:44.580 --> 00:12:46.580]   what people look for.
[00:12:46.580 --> 00:12:48.580]   This has actually been the case for more than a decade.
[00:12:48.580 --> 00:12:56.180]   I remember being on a panel with some people from Ford more than 10 years ago, and back
[00:12:56.180 --> 00:12:59.780]   then they had a bunch of futures that they, you know, employed to kind of look at next
[00:12:59.780 --> 00:13:03.260]   trends, and they were already seeing the fact that the average age of people getting their
[00:13:03.260 --> 00:13:06.580]   driver's license was increasing, which is obviously bad for car companies.
[00:13:06.580 --> 00:13:11.380]   But they were looking at like back then, one of the big, you know, pressing things, this
[00:13:11.380 --> 00:13:16.220]   was even before Android Auto and CarPlay was the infotainment system and how connected
[00:13:16.220 --> 00:13:17.380]   that can be.
[00:13:17.380 --> 00:13:22.860]   And there was this big movement, you know, but amongst the car companies to actually,
[00:13:22.860 --> 00:13:28.380]   many of them had their own systems that were at various levels of smart.
[00:13:28.380 --> 00:13:32.380]   And they wound up seeding, most of them wound up seeding it to Android Auto or CarPlay.
[00:13:32.380 --> 00:13:36.140]   Frankly, A, because it was cheaper than B because the interface was better and people
[00:13:36.140 --> 00:13:37.380]   wanted it enough.
[00:13:37.380 --> 00:13:44.100]   But I mean, like my mom, she has a fairly recent Mercedes, but it does not have, for whatever
[00:13:44.100 --> 00:13:47.820]   reason, she didn't, when she got it, like didn't have the CarPlay option.
[00:13:47.820 --> 00:13:49.620]   My dad's new car does.
[00:13:49.620 --> 00:13:50.900]   And she hates it so much.
[00:13:50.900 --> 00:13:54.300]   So when she gets a new car, like that is, she's like made it very clear.
[00:13:54.300 --> 00:13:57.220]   Like it has to have CarPlay.
[00:13:57.220 --> 00:14:02.340]   So it is so odd to me that they're talking about dropping this for these new EVs, like
[00:14:02.340 --> 00:14:05.820]   models that already have them will keep them, but the new ones won't.
[00:14:05.820 --> 00:14:07.580]   I'm like, what are you doing?
[00:14:07.580 --> 00:14:11.980]   Because even if you make this relatively easy for someone to connect their phone to, if it
[00:14:11.980 --> 00:14:17.540]   has a worse user experience, and let's be honest, it will, like how long is this going
[00:14:17.540 --> 00:14:18.540]   to go on?
[00:14:18.540 --> 00:14:22.180]   I just, this feels just like such a dumb move.
[00:14:22.180 --> 00:14:23.180]   Yeah.
[00:14:23.180 --> 00:14:24.180]   And I think that.
[00:14:24.180 --> 00:14:25.180]   Good.
[00:14:25.180 --> 00:14:26.180]   Please.
[00:14:26.180 --> 00:14:27.180]   Yeah.
[00:14:27.180 --> 00:14:28.180]   Oh, I was just going to say, I think that's the thing.
[00:14:28.180 --> 00:14:32.140]   I think it's like the fact that this is something that is kind of slowly picking up steam.
[00:14:32.140 --> 00:14:36.500]   A lot more people are becoming a lot more attached to things like CarPlay and Android
[00:14:36.500 --> 00:14:40.460]   Auto and then to not allow that to really stick and already be like, oh, we're doing
[00:14:40.460 --> 00:14:41.940]   something different now.
[00:14:41.940 --> 00:14:44.620]   For me, the last car I had was a 2012 Corolla.
[00:14:44.620 --> 00:14:47.100]   So it's been a minute and I sold that when I moved out to the Bay Area.
[00:14:47.100 --> 00:14:50.660]   So I didn't, I did not have any of this, but every time you get in somebody's car, they
[00:14:50.660 --> 00:14:55.100]   have one of these systems and it's integrated with their phones and it makes the process
[00:14:55.100 --> 00:14:56.100]   a lot more seamless.
[00:14:56.100 --> 00:14:59.420]   They're less likely to kind of pick up their phone and check it if they can kind of see
[00:14:59.420 --> 00:15:00.620]   everything that's going on.
[00:15:00.620 --> 00:15:03.260]   So I don't know, it's kind of a strange move.
[00:15:03.260 --> 00:15:08.780]   Well, and as Christina pointed out, by doing that, they're letting the other company, the
[00:15:08.780 --> 00:15:12.780]   car manufacturers, letting the other companies worry about all that stuff.
[00:15:12.780 --> 00:15:17.300]   So if up to this point, you've made so many cars that have this, this functionality built
[00:15:17.300 --> 00:15:23.460]   in, which is essentially just a video connection between point A and point B and a little bit
[00:15:23.460 --> 00:15:26.340]   of data being sent back and forth.
[00:15:26.340 --> 00:15:27.340]   It's so easy to do that.
[00:15:27.340 --> 00:15:31.860]   So to actually now invest the time and money into having a system that's built into the
[00:15:31.860 --> 00:15:36.700]   car, as opposed to just being projected from the phone, is so much more work.
[00:15:36.700 --> 00:15:41.340]   But the Verge article points out the thing that really drives this, drives this home
[00:15:41.340 --> 00:15:50.620]   upon is that this is all about the company's GM specifically getting so much more data
[00:15:50.620 --> 00:15:52.460]   on the drivers.
[00:15:52.460 --> 00:16:00.740]   Especially if you offer an infotainment system that on top of doing music and mapping and
[00:16:00.740 --> 00:16:05.300]   everything else, it also is looking at your fuel economy, it's looking at how fast you're
[00:16:05.300 --> 00:16:06.300]   going.
[00:16:06.300 --> 00:16:11.300]   All of that stuff, they suddenly have this data that they can use and make all sorts
[00:16:11.300 --> 00:16:14.500]   of opportunities for marketing and everything else.
[00:16:14.500 --> 00:16:25.180]   And I think in that way, it makes sense.
[00:16:25.180 --> 00:16:27.940]   This is the worst thing for consumers I can possibly imagine.
[00:16:27.940 --> 00:16:33.100]   It's going to be an operating system by GM with Google Parts glued together by people
[00:16:33.100 --> 00:16:39.100]   who've never built an OS before, designed to upsell you and sell your data while not
[00:16:39.100 --> 00:16:40.700]   connecting to your iPhone.
[00:16:40.700 --> 00:16:41.700]   Fantastic.
[00:16:41.700 --> 00:16:42.700]   Really an idea.
[00:16:42.700 --> 00:16:48.220]   This is literally driven by people in accounting who want to squeeze margin out of cars and
[00:16:48.220 --> 00:16:50.220]   it makes me want to vomit it into my hands.
[00:16:50.220 --> 00:16:53.660]   But before we get on to more serious things, Christina doesn't drive.
[00:16:53.660 --> 00:16:54.660]   I don't drive.
[00:16:54.660 --> 00:16:55.860]   A bar doesn't have a car.
[00:16:55.860 --> 00:16:58.900]   This is the least mobile panel I've ever been.
[00:16:58.900 --> 00:17:03.260]   My spouse goes, I've driven our car once across a parking lot, literally one time.
[00:17:03.260 --> 00:17:04.260]   And it scared me.
[00:17:04.260 --> 00:17:06.260]   So go team.
[00:17:06.260 --> 00:17:07.260]   Interesting.
[00:17:07.260 --> 00:17:08.260]   Oh man.
[00:17:08.260 --> 00:17:14.500]   So you don't drive now, bro, but being from the Midwest, you drove more than that.
[00:17:14.500 --> 00:17:15.500]   Oh gosh.
[00:17:15.500 --> 00:17:16.500]   Yeah.
[00:17:16.500 --> 00:17:19.140]   It was the only way to get around, especially in like a smaller town.
[00:17:19.140 --> 00:17:20.140]   Yeah.
[00:17:20.140 --> 00:17:23.580]   I feel like moving out to the bay was kind of a no-brainer to not have a car, but I feel
[00:17:23.580 --> 00:17:25.660]   like everyone has a car in the bay now.
[00:17:25.660 --> 00:17:26.660]   I don't know.
[00:17:26.660 --> 00:17:28.140]   It does seem like that, doesn't it?
[00:17:28.140 --> 00:17:29.140]   Right.
[00:17:29.140 --> 00:17:34.340]   Post-pandemic, I think something changed, but I'm still holding on to the no-car lifestyle
[00:17:34.340 --> 00:17:37.660]   because I don't want to deal with the traffic and the parking and the break-ins.
[00:17:37.660 --> 00:17:38.660]   Yeah.
[00:17:38.660 --> 00:17:42.660]   Putting signs in that say, please don't take any, there's nothing in here of value.
[00:17:42.660 --> 00:17:44.740]   And they, at least in Seattle, they don't follow that.
[00:17:44.740 --> 00:17:45.740]   They don't care.
[00:17:45.740 --> 00:17:47.180]   They still, they still on it.
[00:17:47.180 --> 00:17:48.180]   Oh no, no, no.
[00:17:48.180 --> 00:17:54.220]   And in our covered like, you know, like building that I live in that is like locked
[00:17:54.220 --> 00:17:57.940]   and whatnot, like my husband's car has been broken into multiple times and he doesn't
[00:17:57.940 --> 00:17:59.940]   even have a nice car.
[00:17:59.940 --> 00:18:06.300]   They leave the Teslas alone, but they go straight for, you know, the 2004 Volvo.
[00:18:06.300 --> 00:18:07.300]   Too bad.
[00:18:07.300 --> 00:18:08.300]   Yeah.
[00:18:08.300 --> 00:18:09.300]   It's too bad.
[00:18:09.300 --> 00:18:10.300]   But no, it is.
[00:18:10.300 --> 00:18:11.300]   Yeah, even with the signs.
[00:18:11.300 --> 00:18:12.300]   Please don't do this.
[00:18:12.300 --> 00:18:16.820]   But I was going to say what you're exactly right, Alex, like this is a subscription play,
[00:18:16.820 --> 00:18:21.700]   but what is so dumb about this and I get the money to go all in on subscriptions, there's
[00:18:21.700 --> 00:18:25.420]   an argument you could make where you could like lock the access to the car player, the
[00:18:25.420 --> 00:18:27.820]   Android Auto behind some sort of GM subscription.
[00:18:27.820 --> 00:18:29.700]   It would still be gross, but you could do that.
[00:18:29.700 --> 00:18:34.060]   But I would honestly prefer that what this is going to be, which is as you said, their
[00:18:34.060 --> 00:18:37.580]   own like kind of hobbled together operating system from people who've never built an
[00:18:37.580 --> 00:18:38.580]   OS.
[00:18:38.580 --> 00:18:42.060]   And because they're probably going to want to have like their own like system on a ship
[00:18:42.060 --> 00:18:47.340]   system here because it's not a projection, it's not going to get updates and it's going
[00:18:47.340 --> 00:18:51.620]   to become obsolete and people keep their cars for longer than you keep your phone.
[00:18:51.620 --> 00:18:55.500]   So the nice thing about car play and Android Auto and whatnot is yes, at a certain point
[00:18:55.500 --> 00:18:59.840]   maybe they will become a little bit too decrepit to like work all the way, but you'll still
[00:18:59.840 --> 00:19:04.020]   be able to have basic things where parts of the screen will be able to display, you know,
[00:19:04.020 --> 00:19:07.620]   this is the song that's playing or this is the maps thing because it is just a display
[00:19:07.620 --> 00:19:08.620]   thing.
[00:19:08.620 --> 00:19:10.340]   And that's going to go away here.
[00:19:10.340 --> 00:19:18.900]   And so I'm guessing that in another decade we will see all these GM original, you know,
[00:19:18.900 --> 00:19:20.740]   OS things that didn't work out.
[00:19:20.740 --> 00:19:24.780]   It will just another being counter will come and go, oh, well, it's cost us too much money
[00:19:24.780 --> 00:19:26.180]   to try to maintain all these things.
[00:19:26.180 --> 00:19:30.460]   So now we're just going to partner up with with the operating system, manufacturers,
[00:19:30.460 --> 00:19:33.020]   the phones and go back to the old way again.
[00:19:33.020 --> 00:19:35.620]   Yeah, so social excited.
[00:19:35.620 --> 00:19:44.420]   I just it's it is kind of frustrating to me thinking about what the next five, ten years
[00:19:44.420 --> 00:19:49.540]   looks like in terms of how many subscription services are going to be hitting my account
[00:19:49.540 --> 00:19:55.660]   every month, but it is the way that the companies are doing things and even companies that are
[00:19:55.660 --> 00:20:01.220]   at one point you the agreement was you buy a thing and you spend a good amount of money
[00:20:01.220 --> 00:20:02.420]   on that thing.
[00:20:02.420 --> 00:20:08.300]   And then it is your thing and you have it and you can use it to now every little piece
[00:20:08.300 --> 00:20:12.220]   and part of it has a subscription that's built into it.
[00:20:12.220 --> 00:20:18.700]   This is a little silly, but I was really excited about they had these little sort of robot
[00:20:18.700 --> 00:20:24.060]   friends and the little robot friend you could get and it would it had like a little face
[00:20:24.060 --> 00:20:29.100]   and it would respond to you and it had a little home where it would go in charge itself and
[00:20:29.100 --> 00:20:33.980]   it could like show you the weather and it was a little goofy, but it was kind of a fun
[00:20:33.980 --> 00:20:37.540]   little toy and I thought, oh, I might have to get one of those.
[00:20:37.540 --> 00:20:41.460]   You know, it's a bit of an investment, but it'll be fun to have and it drives around
[00:20:41.460 --> 00:20:43.260]   your house and is cute.
[00:20:43.260 --> 00:20:48.620]   And then I saw, oh, on top of buying the device itself, which is already pretty pricey, you
[00:20:48.620 --> 00:20:53.740]   have to pay $10 a month to be able to continue to use it and use all the features that it
[00:20:53.740 --> 00:20:54.740]   has.
[00:20:54.740 --> 00:20:57.700]   It's like is anything not subscription based?
[00:20:57.700 --> 00:21:03.220]   Okay, can I throw in one car based subscription counter argument to our consensus?
[00:21:03.220 --> 00:21:04.220]   Yeah.
[00:21:04.220 --> 00:21:08.980]   Okay, I have to buy a car eventually and actually relearn how to drive at some point in time
[00:21:08.980 --> 00:21:14.020]   because being a one car family with a little human is stretching our ability to not lose
[00:21:14.020 --> 00:21:16.340]   our minds.
[00:21:16.340 --> 00:21:19.860]   There's this like three wheel electric car clean out that's like a motorcycle in a case
[00:21:19.860 --> 00:21:23.900]   essentially and it's tiny and it's like a good little one person city car and that's
[00:21:23.900 --> 00:21:26.860]   all I need around Providence where the streets are really narrow.
[00:21:26.860 --> 00:21:31.140]   And you can buy it or you can rent it for like 200 bucks a month later this year.
[00:21:31.140 --> 00:21:32.900]   And I'm thinking, why not?
[00:21:32.900 --> 00:21:36.660]   Like I don't need and I don't want to buy a depreciating asset anyways, really.
[00:21:36.660 --> 00:21:42.860]   So why not just essentially lease this e-car and I can be the biggest dork in town and no
[00:21:42.860 --> 00:21:44.460]   one will like pay for that privilege.
[00:21:44.460 --> 00:21:45.460]   It's fine.
[00:21:45.460 --> 00:21:48.540]   Yeah, I mean, I don't leave the house much anyways, so I don't really care.
[00:21:48.540 --> 00:21:52.980]   But like to me, that's kind of cool because I don't want to go out and spend 25 or 50
[00:21:52.980 --> 00:21:57.500]   k on a car because apparently they got really expensive since I bought my last one.
[00:21:57.500 --> 00:21:59.620]   Yeah, so I think maybe for some things they're okay.
[00:21:59.620 --> 00:22:02.380]   But if you buy a car, it should be yours.
[00:22:02.380 --> 00:22:05.700]   I hate turning everything into an asset that you don't actually own.
[00:22:05.700 --> 00:22:09.300]   Even though I get the corporate American perspective, I just think it sucks.
[00:22:09.300 --> 00:22:12.860]   You've you yes, copy and paste exactly that.
[00:22:12.860 --> 00:22:14.700]   I get it, but it's it.
[00:22:14.700 --> 00:22:17.660]   I don't like it.
[00:22:17.660 --> 00:22:21.900]   So one other thing that kind of exists in this bizarre space where you're kicking the
[00:22:21.900 --> 00:22:22.900]   show off with.
[00:22:22.900 --> 00:22:30.020]   I don't know if you all heard about I'm sure you did that open letter about AI where the
[00:22:30.020 --> 00:22:37.620]   several signatories were saying, Hey, look, we think that any AI technology that is more
[00:22:37.620 --> 00:22:43.420]   advanced than GPT four, we should put a pause on that.
[00:22:43.420 --> 00:22:50.340]   We should put a pause at least six months on investigating and trying to create AI that's
[00:22:50.340 --> 00:22:52.740]   any better than this.
[00:22:52.740 --> 00:22:58.820]   And we need to just be more mindful of the fact that there are dangers to this AI.
[00:22:58.820 --> 00:23:03.900]   And then of course, as it got shared around, look at the people that are on this list.
[00:23:03.900 --> 00:23:04.900]   It's Steve Wozniak.
[00:23:04.900 --> 00:23:05.900]   It's Elon Musk.
[00:23:05.900 --> 00:23:08.300]   It's all of these different AI researchers.
[00:23:08.300 --> 00:23:18.140]   It's the the co creator or the the CEO of FTX, Sam, Bankman, Freed, it is all these different
[00:23:18.140 --> 00:23:20.060]   people.
[00:23:20.060 --> 00:23:26.980]   It turns out, according to vice that several of the people on the list who were quote unquote
[00:23:26.980 --> 00:23:34.300]   confirmed were not actually the people who were on the list, but instead were fake people
[00:23:34.300 --> 00:23:37.740]   who said that they were the people who were on the list and confirmed that they wanted
[00:23:37.740 --> 00:23:39.060]   their name signed.
[00:23:39.060 --> 00:23:42.860]   And then on top of that, after the letter was released, there were several people who
[00:23:42.860 --> 00:23:48.780]   did sign the list or the letter who said, Hey, actually, we don't agree with many of
[00:23:48.780 --> 00:23:52.220]   the points in the letter.
[00:23:52.220 --> 00:23:53.980]   So I don't know if they didn't have time to read it.
[00:23:53.980 --> 00:23:57.100]   They just decided to sign it or what was going on there.
[00:23:57.100 --> 00:23:59.380]   But this whole thing is a mess.
[00:23:59.380 --> 00:24:04.660]   And this this vice piece kind of goes into depth about the research that vice did itself.
[00:24:04.660 --> 00:24:11.220]   And then I think overall about a conversation surrounding the potential dangers of AI, but
[00:24:11.220 --> 00:24:15.860]   also the conversation around AI.
[00:24:15.860 --> 00:24:19.580]   And that I think is the bigger deal here.
[00:24:19.580 --> 00:24:24.980]   Last Thursday, I had, I believe it's from the Washington Post.
[00:24:24.980 --> 00:24:31.100]   We had an incredible guest on who talked about, who talked about talking about AI and the
[00:24:31.100 --> 00:24:39.220]   pitfalls that we often fall into and sort of creating a, we know about news literacy,
[00:24:39.220 --> 00:24:45.300]   but this was about AI literacy and in helping people to understand what AI can and can't
[00:24:45.300 --> 00:24:51.900]   do in its current state, what it could possibly do and helping to deal with the fears surrounding
[00:24:51.900 --> 00:24:55.140]   AI and sort of what's manageable and what's not.
[00:24:55.140 --> 00:25:02.180]   But I wanted to get the panels take on this open letter, what we're all feeling about
[00:25:02.180 --> 00:25:08.020]   generative AI, particularly for many of us, for many of us as journalists.
[00:25:08.020 --> 00:25:14.180]   And then even as developers and developer advocates and maybe even Christina, what you've heard
[00:25:14.180 --> 00:25:20.860]   from developers, because I know Microsoft itself is working on even making further improvements
[00:25:20.860 --> 00:25:30.700]   to, for example, co-pilot and what the sort of sentiment is about where AI is right now
[00:25:30.700 --> 00:25:32.300]   and where it's going.
[00:25:32.300 --> 00:25:36.420]   So why don't we start with Christina and then I'd love to hear from the both of you as well.
[00:25:36.420 --> 00:25:37.420]   Yeah.
[00:25:37.420 --> 00:25:41.380]   So the, the one, I guess kind of disclosure that I will make is that I work at GitHub who
[00:25:41.380 --> 00:25:48.900]   has a relationship with, with OpenAI to, to make a GitHub co-pilot and obviously a Microsoft
[00:25:48.900 --> 00:25:53.740]   was our parent company and they are very involved in this stuff too.
[00:25:53.740 --> 00:25:58.140]   I think that there are definitely reasons to be cautious and I can understand the point
[00:25:58.140 --> 00:26:02.780]   that the letter putting aside all the mess about who really was signing it and, and, and,
[00:26:02.780 --> 00:26:11.100]   and who wasn't aside, I can understand some of the, I guess, fear or at least the, the,
[00:26:11.100 --> 00:26:14.860]   the need to want to maybe take a pause on things.
[00:26:14.860 --> 00:26:20.180]   But I, it, it, the letter as I read it seems to me is, is completely misguided in some
[00:26:20.180 --> 00:26:21.180]   ways.
[00:26:21.180 --> 00:26:25.900]   I think in some ways it over hypes and over buys into some of the realities of where AI
[00:26:25.900 --> 00:26:31.780]   is now, versus like where it might be in five years and in five years, maybe these, these
[00:26:31.780 --> 00:26:34.820]   fears would, would be more applicable.
[00:26:34.820 --> 00:26:39.700]   But I think right now this is buying into a reality that we're just not seeing.
[00:26:39.700 --> 00:26:42.780]   And I'm going to be honest, I think some of the people who've signed the letter who have,
[00:26:42.780 --> 00:26:47.740]   you know, maintained that they've signed it are coming at this from like, it strikes
[00:26:47.740 --> 00:26:51.300]   me as a little bit hypocritical because these are people who have already invested their
[00:26:51.300 --> 00:26:55.380]   own time and money into AI models that maybe aren't as advanced as some of these things
[00:26:55.380 --> 00:26:56.380]   are.
[00:26:56.380 --> 00:26:57.380]   And so I have to start a question.
[00:26:57.380 --> 00:27:00.940]   Are you actually worried about humanity or are you just worried that you are going
[00:27:00.940 --> 00:27:04.940]   to be caught flat footed and that someone else will be able to make more money than you
[00:27:04.940 --> 00:27:06.220]   will in the short term?
[00:27:06.220 --> 00:27:08.860]   Like that, that's, that's where the cynic in me is going to go off.
[00:27:08.860 --> 00:27:12.940]   I think that we need to have conversations about AI literacy and what it can and can't
[00:27:12.940 --> 00:27:13.940]   do.
[00:27:13.940 --> 00:27:17.980]   And I think that we need to have conversations about, you know, how we want these models
[00:27:17.980 --> 00:27:18.980]   to evolve.
[00:27:18.980 --> 00:27:23.820]   But I don't know of this letter, it was A in good faith or B is actually representing
[00:27:23.820 --> 00:27:28.300]   the future that we're in right now and not, you know, frankly, a little bit of hyphen
[00:27:28.300 --> 00:27:29.700]   science fiction of where we might be.
[00:27:29.700 --> 00:27:32.940]   Well put, Alex.
[00:27:32.940 --> 00:27:37.700]   I mean, how can I follow that?
[00:27:37.700 --> 00:27:40.780]   Not trying to get Christina in trouble, but one of the things I'm most excited about in
[00:27:40.780 --> 00:27:45.820]   the world is actually what's going on with co-pilot and co-generation in general with
[00:27:45.820 --> 00:27:48.580]   these large language models for LLMs.
[00:27:48.580 --> 00:27:52.060]   And the reason why is I haven't really written code since high school when I was learning
[00:27:52.060 --> 00:27:56.860]   C++ out of a book that was like this thick and I didn't make it that far because it was
[00:27:56.860 --> 00:27:57.860]   boring.
[00:27:57.860 --> 00:27:59.940]   And what I would love to do is be able to build things.
[00:27:59.940 --> 00:28:05.340]   And I think that once we get generative code development from these AI tools into a more
[00:28:05.340 --> 00:28:08.980]   workable state, I, Alex, I'm going to be able to build cool things.
[00:28:08.980 --> 00:28:09.980]   Yes.
[00:28:09.980 --> 00:28:11.300]   And that doesn't seem to be that far away.
[00:28:11.300 --> 00:28:12.300]   No.
[00:28:12.300 --> 00:28:13.300]   Is that dangerous?
[00:28:13.300 --> 00:28:14.900]   No, it's an extension I feel Christina.
[00:28:14.900 --> 00:28:16.700]   I've kind of like what folks can already do.
[00:28:16.700 --> 00:28:17.700]   Totally.
[00:28:17.700 --> 00:28:18.700]   No, I totally agree with you.
[00:28:18.700 --> 00:28:19.700]   And we hear a lot of that.
[00:28:19.700 --> 00:28:22.020]   I mean, where the co-generation is right now, is it's not bad?
[00:28:22.020 --> 00:28:26.620]   And for some things that would usually come from a book, like a, you know, right, right,
[00:28:26.620 --> 00:28:31.020]   like you can ask a GitHub co-pilot right now to write you an application that will play
[00:28:31.020 --> 00:28:33.260]   rock, paper, scissors in Python.
[00:28:33.260 --> 00:28:34.260]   And it will do that.
[00:28:34.260 --> 00:28:37.380]   But that is something that would be in a very basic kind of like introduction to programming
[00:28:37.380 --> 00:28:38.380]   book, right?
[00:28:38.380 --> 00:28:40.340]   Like this is a very simple exercise.
[00:28:40.340 --> 00:28:44.820]   And you can ask, you know, chat, GPT and co-pilot, some other things that will help you maybe
[00:28:44.820 --> 00:28:46.060]   generate some code blocks.
[00:28:46.060 --> 00:28:48.260]   And right now it's still a little bit more piecemeal.
[00:28:48.260 --> 00:28:51.100]   You still do need to know what you're doing to a certain extent.
[00:28:51.100 --> 00:28:52.860]   So that's why we call it a co-pilot.
[00:28:52.860 --> 00:28:54.260]   It's not doing it for you.
[00:28:54.260 --> 00:28:55.260]   It's assisting you.
[00:28:55.260 --> 00:29:00.020]   But I do think that we will reach a point where you can have someone who has an idea
[00:29:00.020 --> 00:29:03.500]   of this is what I want to do and I can put together like a minimal viable product that
[00:29:03.500 --> 00:29:05.380]   will do this.
[00:29:05.380 --> 00:29:08.580]   And some people see this as a threat to their jobs and to the future.
[00:29:08.580 --> 00:29:14.460]   I don't because I think that you've always, what we've seen with, when coding languages
[00:29:14.460 --> 00:29:18.220]   have become easier to use and people have been able to build more things, we've just
[00:29:18.220 --> 00:29:23.300]   seen the explosion and more people wanting to build applications, not fewer.
[00:29:23.300 --> 00:29:28.100]   So does that mean that maybe some low level jobs will go away potentially?
[00:29:28.100 --> 00:29:31.900]   But I would also question whether people would have been hired to build those apps to
[00:29:31.900 --> 00:29:32.900]   begin with.
[00:29:32.900 --> 00:29:37.540]   I think that this could just be an opportunity for people who would have never paid for it
[00:29:37.540 --> 00:29:41.140]   and would have otherwise never been able to have the skills to build an application might
[00:29:41.140 --> 00:29:42.140]   actually have that opportunity.
[00:29:42.140 --> 00:29:43.140]   And that's exciting.
[00:29:43.140 --> 00:29:44.140]   Sorry, going on Alex.
[00:29:44.140 --> 00:29:50.100]   And I think that when you think about co-pilot, it's just one layer higher of abstraction in
[00:29:50.100 --> 00:29:54.020]   the code language stack from assembly up through other languages.
[00:29:54.020 --> 00:29:59.700]   And then the next step I think is kind of no code systems paired with prompts that I
[00:29:59.700 --> 00:30:03.420]   say, please connect this button to that, have it turn purple when I click it.
[00:30:03.420 --> 00:30:04.660]   And then the code's done in the background.
[00:30:04.660 --> 00:30:06.300]   I don't have to deal with it.
[00:30:06.300 --> 00:30:08.260]   And then suddenly me Alex can do so much more.
[00:30:08.260 --> 00:30:10.620]   And that's very exciting as a step forward.
[00:30:10.620 --> 00:30:15.940]   I think the letter going back to Mike's point is do-marism from fake optimists who are
[00:30:15.940 --> 00:30:20.260]   bummed out that they're behind aka Elon, so whatever.
[00:30:20.260 --> 00:30:23.100]   But past that, I'm still very excited about all of this.
[00:30:23.100 --> 00:30:27.260]   And I'm going to show up now on the De Broglie, but I just can't believe that the people
[00:30:27.260 --> 00:30:31.020]   who claim that everyone else is a bummer about tech are not writing bummer letters about
[00:30:31.020 --> 00:30:32.020]   their competitors.
[00:30:32.020 --> 00:30:33.020]   Like lame.
[00:30:33.020 --> 00:30:34.020]   (laughs)
[00:30:34.020 --> 00:30:36.180]   A broir, what's your, what are your thoughts?
[00:30:36.180 --> 00:30:40.620]   I just want to say I think it's kind of like sadly poetic that even the people who
[00:30:40.620 --> 00:30:43.220]   it appears signed the letter didn't, like what is real?
[00:30:43.220 --> 00:30:44.220]   Like none of it's real.
[00:30:44.220 --> 00:30:45.220]   (laughs)
[00:30:45.220 --> 00:30:49.940]   It was kind of like who knows.
[00:30:49.940 --> 00:30:52.220]   But I think I agree with both of your points.
[00:30:52.220 --> 00:30:56.660]   And I think the thing that I'm thinking about is that for a lot of us who are in the tech
[00:30:56.660 --> 00:30:59.420]   space, we see a lot of the potential here.
[00:30:59.420 --> 00:31:00.740]   We see a lot of the dangers here.
[00:31:00.740 --> 00:31:04.980]   But I think this scary thing and the point of concern is that this has all just kind
[00:31:04.980 --> 00:31:12.500]   of exploded so quickly and from so many angles and from so many companies that I think the
[00:31:12.500 --> 00:31:15.060]   fear of misinformation is a real one.
[00:31:15.060 --> 00:31:20.420]   And I think it becomes a matter of making sure there's this like, there's this tendency
[00:31:20.420 --> 00:31:22.660]   in tech to just do things because you can.
[00:31:22.660 --> 00:31:28.100]   And it makes me wonder in what capacity should we be using these technologies?
[00:31:28.100 --> 00:31:34.900]   And in what ways can we improve society and different assets of our lives?
[00:31:34.900 --> 00:31:39.300]   But yeah, I think it's also something to think about when you look at the names on here.
[00:31:39.300 --> 00:31:41.580]   Okay, why are these people signing this letter?
[00:31:41.580 --> 00:31:45.980]   I think it's very valid to kind of be a little skeptical of that.
[00:31:45.980 --> 00:31:49.780]   But I think the biggest thing for me is just thinking about how quickly this is all come
[00:31:49.780 --> 00:31:53.940]   together and how the everyday person is going to perceive this and if they're going to be
[00:31:53.940 --> 00:31:57.340]   able to tell what's real and what's not because it's getting a lot harder.
[00:31:57.340 --> 00:31:58.340]   Yes.
[00:31:58.340 --> 00:31:59.340]   It's true.
[00:31:59.340 --> 00:32:02.820]   The Washington Post piece that I want to point everyone to was by Tatum Hunter who joined
[00:32:02.820 --> 00:32:04.620]   me on Techno's Weekly to talk about it.
[00:32:04.620 --> 00:32:06.980]   And then the piece is called three things.
[00:32:06.980 --> 00:32:09.460]   Everyone's getting wrong about AI.
[00:32:09.460 --> 00:32:14.020]   And although that does have sort of a BuzzFeed style headline that may make you go, I don't
[00:32:14.020 --> 00:32:18.940]   want to know, you should absolutely check this piece out because one of the things that
[00:32:18.940 --> 00:32:23.820]   I felt a little attacked by on this piece because it was like, oh, this is me.
[00:32:23.820 --> 00:32:26.580]   I wasn't even thinking about this.
[00:32:26.580 --> 00:32:34.460]   It was how when I would go and I'd talk to chat GPT or type to chat GPT and it would
[00:32:34.460 --> 00:32:39.300]   provide an answer that I could confirm was correct because it was something that I already
[00:32:39.300 --> 00:32:42.980]   knew or I was actively kind of looking at.
[00:32:42.980 --> 00:32:44.340]   It started to lead me.
[00:32:44.340 --> 00:32:50.500]   If I'm seeing it, do these things and get it right, then it's starting to put in me this
[00:32:50.500 --> 00:32:57.900]   belief that it's going to continue to be correct about things and sort of have this false belief
[00:32:57.900 --> 00:33:02.940]   in the system that it's not going to have those issues where then the next time I ask
[00:33:02.940 --> 00:33:07.700]   it something, it's hallucinating an answer as opposed to giving me one.
[00:33:07.700 --> 00:33:12.780]   And I think that was one aspect of it in particular that really stuck out to me because
[00:33:12.780 --> 00:33:19.620]   while I feel like from the get go, because I have some level of understanding of how a
[00:33:19.620 --> 00:33:25.540]   large language model works and because I tend to be skeptical of technology in general,
[00:33:25.540 --> 00:33:29.860]   at least in the sense of just inquiring about it, I thought, oh, I'm going to be okay with
[00:33:29.860 --> 00:33:30.860]   this.
[00:33:30.860 --> 00:33:36.820]   But then to have that moment and go, oh, I need to maintain a certain skepticism that
[00:33:36.820 --> 00:33:41.620]   just because it's giving me this answer and it's gotten so many different things correct,
[00:33:41.620 --> 00:33:47.260]   I still should follow through and have some due diligence because if we don't have some
[00:33:47.260 --> 00:33:52.700]   level of due diligence, then that is where, and that's the thing, it's misinformation,
[00:33:52.700 --> 00:33:59.660]   not disinformation because it's not that the AI is actively trying to lie to me, but it
[00:33:59.660 --> 00:34:03.700]   is just by its nature potentially saying something that is untrue.
[00:34:03.700 --> 00:34:08.180]   So yeah, just us being mindful of that, even though we, because it's kind of like, you
[00:34:08.180 --> 00:34:16.020]   see, when you see a child start walking for the first time and you're like, oh my goodness,
[00:34:16.020 --> 00:34:19.900]   I'm so proud of you, like you've done this, you don't suddenly think that they can build
[00:34:19.900 --> 00:34:27.620]   a car or they can suddenly like change the way that nuclear mechanics works or whatever.
[00:34:27.620 --> 00:34:33.540]   But when I saw the AI get this small thing right, then suddenly I'm going, oh my goodness,
[00:34:33.540 --> 00:34:37.700]   now I can do all of these things, if it's going to be great at it, and I, you know, potentially
[00:34:37.700 --> 00:34:40.220]   you could find yourself putting too much faith in it.
[00:34:40.220 --> 00:34:46.900]   And as in terms of being an assistant that helps me complete something that I want to
[00:34:46.900 --> 00:34:53.860]   do, what you were talking about, Alex, I have used this a number of times, I know just a
[00:34:53.860 --> 00:34:58.700]   tiny bit of Python, and it's not enough to do some of the things that I wanted to do.
[00:34:58.700 --> 00:35:05.980]   And so being able to go into chat GPT and say, hey, this is what I want to do to add
[00:35:05.980 --> 00:35:11.660]   onto this, or I couldn't figure out what regular expression I needed to create to be able to
[00:35:11.660 --> 00:35:15.940]   pick out some specific text from a document using it to do that.
[00:35:15.940 --> 00:35:18.860]   That stuff is fantastic, it's really good at that.
[00:35:18.860 --> 00:35:24.940]   And that is where it is such a help, I think, to humanity and can potentially like just make
[00:35:24.940 --> 00:35:27.060]   so much of a difference for people.
[00:35:27.060 --> 00:35:30.820]   And that's also a sticking point, because I read an interesting African, it was a Twitter
[00:35:30.820 --> 00:35:35.420]   thread or a post from someone who contributed a lot to Stack Overflow, which is a place
[00:35:35.420 --> 00:35:39.780]   where developers come together often to ask questions and solve things with one another.
[00:35:39.780 --> 00:35:44.060]   And they were saying effectively, I don't want all of my contributions to be hovered
[00:35:44.060 --> 00:35:48.460]   up into the great AI in the sky, and then used in ways that I may not approve or that
[00:35:48.460 --> 00:35:50.140]   may not be fair to me.
[00:35:50.140 --> 00:35:56.060]   And reasonable, it smacks a little bit to me of people trying to de-index from Google,
[00:35:56.060 --> 00:36:00.460]   if you're a silo internet, so I'm kind of like, is this Ludditism or is this a reasonable
[00:36:00.460 --> 00:36:03.540]   complaint from a person who contributes to an open forum?
[00:36:03.540 --> 00:36:06.220]   I have yet to kind of figure out where I stand.
[00:36:06.220 --> 00:36:09.980]   But at the same time, Mike, if you were like, here's the thing, you can use it, I'm going
[00:36:09.980 --> 00:36:14.620]   to click the big green button because I personally want to do more stuff.
[00:36:14.620 --> 00:36:17.980]   But I am glad we're at least talking about the ethics behind the source materials that
[00:36:17.980 --> 00:36:18.980]   are going into these systems.
[00:36:18.980 --> 00:36:23.380]   And I think that's where Italy ended up getting kind of peevish about the whole thing.
[00:36:23.380 --> 00:36:24.380]   Absolutely.
[00:36:24.380 --> 00:36:28.620]   One last example that I have to give, because I think that this is, again, one of the rare
[00:36:28.620 --> 00:36:32.420]   instances, well, not rare instances, because it's becoming less rare, but one of the very
[00:36:32.420 --> 00:36:37.580]   specific instances where something like a large language model can work, I remembered
[00:36:37.580 --> 00:36:42.100]   that there was an episode of either, there's either an episode of television or there was
[00:36:42.100 --> 00:36:47.700]   some movie where in that episode of television or in that movie, there was a person who was
[00:36:47.700 --> 00:36:53.700]   taking antidepressant medications and they were living their life, everything was fine.
[00:36:53.700 --> 00:36:59.620]   But at some point, some naturalist had prescribed St. John's Wort.
[00:36:59.620 --> 00:37:05.820]   And what many of you may know is that if you take St. John's Wort, it is one of those rare
[00:37:05.820 --> 00:37:11.780]   substances that can actually make your body absorb more of different types of medication.
[00:37:11.780 --> 00:37:15.140]   It makes your body better at absorbing other types of medication.
[00:37:15.140 --> 00:37:21.700]   So if you take St. John's Wort and an anti-depressant at the same time, then you can become at risk
[00:37:21.700 --> 00:37:27.620]   for serotonin syndrome where you end up having too much serotonin in your brain and it causes
[00:37:27.620 --> 00:37:28.620]   all sorts of issues.
[00:37:28.620 --> 00:37:35.380]   In the show or the movie, the person started to lose their mind and no one knew why it was
[00:37:35.380 --> 00:37:39.580]   happening and it, lo and behold, it was because they were taking St. John's Wort.
[00:37:39.580 --> 00:37:42.340]   So I typed into Chad GPT.
[00:37:42.340 --> 00:37:46.220]   I know there's a movie or a TV show out there where this happens and then it was able to
[00:37:46.220 --> 00:37:49.300]   respond and be like, yeah, that was this episode of Doctor House.
[00:37:49.300 --> 00:37:52.540]   Oh, did it also happen in Veep?
[00:37:52.540 --> 00:37:54.340]   Oh, see, so I've got Chad GPT.
[00:37:54.340 --> 00:37:57.140]   No, it was Ant-Pro, and actually it was like, this happened in Veep.
[00:37:57.140 --> 00:38:01.060]   But there was an episode of Doctor House where that had happened and yeah, the whole time
[00:38:01.060 --> 00:38:06.300]   it was because of the St. John's Wort.
[00:38:06.300 --> 00:38:10.260]   Anyway, that is just one of those unique situations where because it's hoovered up all of that
[00:38:10.260 --> 00:38:14.500]   information, I don't have all of that here and it was kind of a hard thing to Google.
[00:38:14.500 --> 00:38:18.940]   I tried that first and it was just more all of these warnings about taking St. John's
[00:38:18.940 --> 00:38:20.420]   Wort with the medication.
[00:38:20.420 --> 00:38:22.140]   So it's like, oh, okay.
[00:38:22.140 --> 00:38:23.140]   That was a great thing.
[00:38:23.140 --> 00:38:24.140]   Congratulations.
[00:38:24.140 --> 00:38:26.140]   You know, invented Google back when it was good.
[00:38:26.140 --> 00:38:27.140]   Yeah.
[00:38:27.140 --> 00:38:28.140]   Yeah.
[00:38:28.140 --> 00:38:30.540]   I mean, remember when you could type in Google, like, it's that song that goes bum, bum,
[00:38:30.540 --> 00:38:32.940]   bum, bum, bum, bum, bum, and you will be like, oh, yes, of course.
[00:38:32.940 --> 00:38:34.540]   This is 1983 rendition of Toto.
[00:38:34.540 --> 00:38:36.100]   You're like, oh, fantastic.
[00:38:36.100 --> 00:38:39.100]   So if Chad GPT is going to make Google good again, cool.
[00:38:39.100 --> 00:38:41.140]   I've heard for God's, Google's trash now.
[00:38:41.140 --> 00:38:43.060]   Are we going to get hats?
[00:38:43.060 --> 00:38:44.060]   Let's say that.
[00:38:44.060 --> 00:38:45.060]   Make Google great again.
[00:38:45.060 --> 00:38:46.060]   Oh, God.
[00:38:46.060 --> 00:38:50.100]   I just want to start not red with white font on the front because even as a joke, I think
[00:38:50.100 --> 00:38:52.260]   I'll pass on the modern KKK helmet.
[00:38:52.260 --> 00:38:55.660]   You have Chad GPT design them.
[00:38:55.660 --> 00:39:00.300]   Make these the least offensive thing they can possibly be with this tagline.
[00:39:00.300 --> 00:39:01.300]   We'll see what it comes up.
[00:39:01.300 --> 00:39:04.220]   It's like a Pikachu hat or something that everyone loves.
[00:39:04.220 --> 00:39:05.220]   All right.
[00:39:05.220 --> 00:39:08.020]   I want to take a quick break before we come back with lots more.
[00:39:08.020 --> 00:39:09.580]   I've got a great panel.
[00:39:09.580 --> 00:39:15.900]   Christina Warren, Alex Wilham and Abraar Al-Hiti are all here today for this episode
[00:39:15.900 --> 00:39:23.300]   of This Week in Tech, which is brought to you today by Lookout as we've been talking about
[00:39:23.300 --> 00:39:24.300]   on the show.
[00:39:24.300 --> 00:39:30.460]   And I think as many of you know, business has changed forever because we've seen a change
[00:39:30.460 --> 00:39:35.380]   in the boundaries of where we work or even how we work.
[00:39:35.380 --> 00:39:37.020]   All those boundaries have kind of disappeared.
[00:39:37.020 --> 00:39:42.380]   That means your data, it's always on the move, whether on a device or in the cloud, across
[00:39:42.380 --> 00:39:44.820]   networks at the local coffee shop.
[00:39:44.820 --> 00:39:48.500]   And that's great for your workforce because it means you can have people who are working
[00:39:48.500 --> 00:39:53.860]   from different places and are able to get their work done no matter what is going on.
[00:39:53.860 --> 00:39:56.660]   It's a challenge for IT security.
[00:39:56.660 --> 00:40:01.260]   Those folks who are trying to make sure that all of that data doesn't make it to the wrong
[00:40:01.260 --> 00:40:02.260]   hands.
[00:40:02.260 --> 00:40:04.660]   So Lookout is there to help you out with that.
[00:40:04.660 --> 00:40:07.420]   It helps you control your data and free your workforce.
[00:40:07.420 --> 00:40:11.540]   With Lookout, you're going to gain complete visibility into all your data so you can minimize
[00:40:11.540 --> 00:40:17.020]   risk from external and internal threats plus ensure compliance.
[00:40:17.020 --> 00:40:21.660]   By seamlessly securing hybrid work, your organization doesn't have to sacrifice productivity for
[00:40:21.660 --> 00:40:23.620]   security because that's a big thing.
[00:40:23.620 --> 00:40:28.900]   If we've got all of these folks who are trying to get their work done, but every time they
[00:40:28.900 --> 00:40:33.740]   try to log on because they're doing it from a coffee shop or their home network isn't
[00:40:33.740 --> 00:40:37.140]   exactly what it's supposed to be, they're hitting all of these roadblocks of, "Oh, you've
[00:40:37.140 --> 00:40:40.940]   got to log into this and you've got to type this in and you've got to do this in order
[00:40:40.940 --> 00:40:43.260]   that just gets in the way of getting work done."
[00:40:43.260 --> 00:40:48.420]   So Lookout makes that simpler, it makes IT security so much easier.
[00:40:48.420 --> 00:40:53.180]   Working with multiple point solutions and legacy tools in today's environment, it's too
[00:40:53.180 --> 00:40:54.420]   complex.
[00:40:54.420 --> 00:40:59.500]   So with its single unified platform, Lookout reduces IT complexity giving you more time
[00:40:59.500 --> 00:41:02.900]   to focus on whatever else comes your way.
[00:41:02.900 --> 00:41:04.220]   Good, I love this.
[00:41:04.220 --> 00:41:09.980]   Good data protection isn't a cage, it's a springboard letting you and your organization
[00:41:09.980 --> 00:41:12.140]   bound toward a future of your making.
[00:41:12.140 --> 00:41:15.380]   So visit Lookout.com today.
[00:41:15.380 --> 00:41:20.100]   Super easy to remember, lookout.com today to learn more on how to safeguard your data,
[00:41:20.100 --> 00:41:23.100]   how to secure your hybrid work and how to reduce IT complexity.
[00:41:23.100 --> 00:41:25.140]   That's Lookout.com.
[00:41:25.140 --> 00:41:29.820]   Thank you Lookout for sponsoring this week's episode of This Week in Tech.
[00:41:29.820 --> 00:41:35.980]   All right, back from the break and I think it's time to talk about a big tech company
[00:41:35.980 --> 00:41:40.820]   with a big old conference coming up in June.
[00:41:40.820 --> 00:41:46.540]   Apple has announced its worldwide developer conference for 2023, a mostly online event
[00:41:46.540 --> 00:41:53.060]   with a small sort of special event that's taking place in person.
[00:41:53.060 --> 00:41:58.740]   But along with that announcement of course comes the slate of rumors that goes along
[00:41:58.740 --> 00:42:02.100]   with any Apple announcement.
[00:42:02.100 --> 00:42:06.180]   And I think with this one of course the way that the worldwide developers conference
[00:42:06.180 --> 00:42:12.340]   happens every year is it kicks off with a keynote and at that keynote Apple announces
[00:42:12.340 --> 00:42:19.060]   the new features, updates, changes that are taking place for all of its software platforms.
[00:42:19.060 --> 00:42:25.620]   Mac OS, TVOS, WatchOS, HomePod OS, iPadOS, iOS, etc.
[00:42:25.620 --> 00:42:31.460]   And so we will always see software announcements at the event and then developers spend the
[00:42:31.460 --> 00:42:36.540]   rest of the week learning how to make use of these new tools or how to make use of the
[00:42:36.540 --> 00:42:39.220]   updates to the existing tools.
[00:42:39.220 --> 00:42:49.180]   But there are some rumors, Christina, that Apple is slated to announce maybe its ARVR
[00:42:49.180 --> 00:42:50.180]   headset.
[00:42:50.180 --> 00:42:55.060]   Is it time for Apple to dip its toe into ARVR?
[00:42:55.060 --> 00:43:04.060]   I mean, I think it would have been better if they did it a couple years ago before the
[00:43:04.060 --> 00:43:05.860]   tie has kind of shifted.
[00:43:05.860 --> 00:43:09.500]   But hey, when it's ready, it's ready.
[00:43:09.500 --> 00:43:15.500]   I mean, I definitely want to see what this very long rumored headset looks like.
[00:43:15.500 --> 00:43:22.660]   And it certainly seems like from the design of the invitation that we might finally be
[00:43:22.660 --> 00:43:25.740]   looking at this.
[00:43:25.740 --> 00:43:26.820]   I guess it's time?
[00:43:26.820 --> 00:43:27.820]   I don't know.
[00:43:27.820 --> 00:43:35.820]   This feels like other than the long rumored Apple car, this seems like the least slam dunk
[00:43:35.820 --> 00:43:42.800]   least makes sense, kind of like big swing, new category Apple thing that I can recall
[00:43:42.800 --> 00:43:44.900]   in a really long time.
[00:43:44.900 --> 00:43:50.300]   The watch was kind of weird, especially when it was first positioned to be this fashion
[00:43:50.300 --> 00:43:53.300]   object, they very quickly pivoted that to fitness.
[00:43:53.300 --> 00:43:56.900]   And it completely was a massive success.
[00:43:56.900 --> 00:44:03.700]   Obviously, phones and tablets make sense.
[00:44:03.700 --> 00:44:07.660]   I feel like we're still waiting for whatever the killer app will be.
[00:44:07.660 --> 00:44:12.380]   If any company is going to give us a killer app for ARVR, it will be Apple.
[00:44:12.380 --> 00:44:14.860]   But yeah, maybe now it's finally time.
[00:44:14.860 --> 00:44:19.620]   And it would be sort of ironic if they were showing it off in this sort of weird hybrid
[00:44:19.620 --> 00:44:22.780]   virtual, but also in person worlds.
[00:44:22.780 --> 00:44:23.780]   Right?
[00:44:23.780 --> 00:44:25.580]   Yeah, because how do you do that?
[00:44:25.580 --> 00:44:26.580]   Right?
[00:44:26.580 --> 00:44:33.700]   And especially with a product that is so focused on the visual aspect of it, do you
[00:44:33.700 --> 00:44:38.100]   have people on stage who are very good at describing what they're seeing?
[00:44:38.100 --> 00:44:39.100]   Right.
[00:44:39.100 --> 00:44:44.420]   Because you could never trust the virtual demonstrations of these products because it's
[00:44:44.420 --> 00:44:48.460]   not the same as being able to see it quite literally with your eyes.
[00:44:48.460 --> 00:44:49.460]   Right?
[00:44:49.460 --> 00:44:53.300]   So this is going to be, is this completely computer generated?
[00:44:53.300 --> 00:44:55.140]   Is this what I'm actually seeing?
[00:44:55.140 --> 00:44:59.500]   This is a hard product, I think just in general to launch and to have people trusted enough
[00:44:59.500 --> 00:45:02.660]   to where they're willing to spend the money because that's what we're hearing too, is
[00:45:02.660 --> 00:45:04.900]   that this is going to be a pretty pricey object.
[00:45:04.900 --> 00:45:07.740]   And Alex, I saw you really rolling that arm in there.
[00:45:07.740 --> 00:45:08.740]   So you must have an agree.
[00:45:08.740 --> 00:45:09.740]   Sorry.
[00:45:09.740 --> 00:45:13.340]   I would have learned to do in my maturity is not interrupt people, but then I have to
[00:45:13.340 --> 00:45:15.140]   not talk.
[00:45:15.140 --> 00:45:18.300]   And it's very hard when I have many things that I want to say that I need to be polite
[00:45:18.300 --> 00:45:19.500]   and leave room for others.
[00:45:19.500 --> 00:45:23.180]   So anyways, I just want to say that Christina said something very interesting that there's
[00:45:23.180 --> 00:45:25.540]   going to be a keynote, I believe, or maybe that is you might get.
[00:45:25.540 --> 00:45:27.140]   But anyways, they're not keynotes to your point.
[00:45:27.140 --> 00:45:28.140]   They're actually just infomercials.
[00:45:28.140 --> 00:45:32.860]   In the old days, you had to have a device on stage, use it and have it work.
[00:45:32.860 --> 00:45:37.060]   Now everything's so pre-produced, it's like listening to like a really bad pop single
[00:45:37.060 --> 00:45:40.860]   when all the personalities have been stripped out of it by producers and mixers and masters.
[00:45:40.860 --> 00:45:43.540]   The points of which is not really much soul left.
[00:45:43.540 --> 00:45:49.580]   And so if Apple pulls this off, I'll buy it because I buy everything Apple makes eventually.
[00:45:49.580 --> 00:45:53.420]   I am skeptical, but I do help that if they do have something, it's not just what they
[00:45:53.420 --> 00:45:58.940]   usually do, which is a lot of glitzy stats on a screen, some dude in a laboratory with
[00:45:58.940 --> 00:45:59.940]   a coat on.
[00:45:59.940 --> 00:46:01.700]   I hope it's a bit more touch and feel.
[00:46:01.700 --> 00:46:07.020]   And to that point, when Microsoft first shot off the HoloLens nerd helmet, I got to use
[00:46:07.020 --> 00:46:08.020]   it that day.
[00:46:08.020 --> 00:46:11.620]   They put me in a room with it, I got to put it on, I got to play with it.
[00:46:11.620 --> 00:46:12.620]   Awesome.
[00:46:12.620 --> 00:46:15.340]   So I didn't work out commercially, but the technology was freaking fantastic.
[00:46:15.340 --> 00:46:20.340]   So I think you need to have that tactile, real, no BS, no pre-chewed, let me touch it,
[00:46:20.340 --> 00:46:21.340]   let me turn it on.
[00:46:21.340 --> 00:46:22.700]   Otherwise, I just don't care.
[00:46:22.700 --> 00:46:23.700]   Yeah.
[00:46:23.700 --> 00:46:25.420]   Yeah, because again, what is real?
[00:46:25.420 --> 00:46:33.580]   As I was saying earlier, I'm curious for you, have you used any VR devices and what's
[00:46:33.580 --> 00:46:38.780]   your kind of take on VR, as it currently stands and where it could go?
[00:46:38.780 --> 00:46:46.220]   It's been years since I last put on what was innocuous.
[00:46:46.220 --> 00:46:52.860]   I think this is really the ultimate test to see the power of Apple because so many companies
[00:46:52.860 --> 00:46:59.020]   have been trying so hard to make us want to put a headset on for VR, AR.
[00:46:59.020 --> 00:47:02.700]   And this is going to be the ultimate test not just because of the technology, but also
[00:47:02.700 --> 00:47:07.060]   because of the price, as you'd mentioned, their rumors would cost somewhere around $3,000.
[00:47:07.060 --> 00:47:08.980]   That's a lot of money.
[00:47:08.980 --> 00:47:12.340]   And so it's an Apple product, so you're going to pay for it.
[00:47:12.340 --> 00:47:17.820]   And I think it's just going to be really interesting to see what the tech looks like
[00:47:17.820 --> 00:47:19.820]   and what the adoption looks like.
[00:47:19.820 --> 00:47:22.500]   And you think about other things too, like foldable phones.
[00:47:22.500 --> 00:47:25.620]   That's kind of been a slow take off if Apple rolls a lot of foldable phone.
[00:47:25.620 --> 00:47:26.620]   Will that happen?
[00:47:26.620 --> 00:47:30.700]   So there are a couple areas here where there are rumors that Apple is working to create
[00:47:30.700 --> 00:47:31.700]   a product.
[00:47:31.700 --> 00:47:37.300]   And I am just really curious to see if people are going to be willing to do it.
[00:47:37.300 --> 00:47:38.300]   And I think they are.
[00:47:38.300 --> 00:47:42.780]   I think they're going to be more likely to put on an Apple headset versus a Quest Pro.
[00:47:42.780 --> 00:47:47.540]   I think there's a lot of skepticism and scrutiny around companies like Meta.
[00:47:47.540 --> 00:47:53.140]   And I think also, Meta hasn't necessarily sold it very well.
[00:47:53.140 --> 00:47:58.540]   I still don't understand why you would need or want that headset, but Apple will find
[00:47:58.540 --> 00:48:00.700]   a way to try to convince you to want it.
[00:48:00.700 --> 00:48:01.700]   Absolutely.
[00:48:01.700 --> 00:48:06.900]   And to your point about that, if you go back in time before iPhones got larger screens,
[00:48:06.900 --> 00:48:09.660]   iPhone users mocked Android users for having tablets.
[00:48:09.660 --> 00:48:12.380]   Oh my gosh, your screen is so big.
[00:48:12.380 --> 00:48:13.380]   What is that?
[00:48:13.380 --> 00:48:14.380]   And that fake iPad.
[00:48:14.380 --> 00:48:17.060]   And then Apple's screen got bigger and everyone was like, this is the best.
[00:48:17.060 --> 00:48:19.700]   And you kind of sit back and you're like, how did they do that?
[00:48:19.700 --> 00:48:20.700]   That's magic.
[00:48:20.700 --> 00:48:24.660]   So I'm with you, even if it's 3K, I'm pretty sure I'll find some way to talk myself into
[00:48:24.660 --> 00:48:28.180]   buying one, just like I bought an iPad Pro that I don't use.
[00:48:28.180 --> 00:48:31.940]   I got on Apple's gross margins, I bought an Apple pencil that I lost.
[00:48:31.940 --> 00:48:32.940]   Shout out Apple's gross margins.
[00:48:32.940 --> 00:48:36.140]   But you were the reason they have value.
[00:48:36.140 --> 00:48:37.140]   It's a highly, highly.
[00:48:37.140 --> 00:48:38.140]   Exactly.
[00:48:38.140 --> 00:48:39.140]   Personally.
[00:48:39.140 --> 00:48:41.780]   I mean, I'm an American with a credit card and I work near technology.
[00:48:41.780 --> 00:48:43.820]   It's going to happen.
[00:48:43.820 --> 00:48:47.180]   My question is, what do we want to use these for?
[00:48:47.180 --> 00:48:51.820]   Because I struggle a little bit and I, you know, in my little office pod here, I've got
[00:48:51.820 --> 00:48:52.820]   three different computers.
[00:48:52.820 --> 00:48:55.780]   I've got work Mac, personal iMac and a gaming PC.
[00:48:55.780 --> 00:49:00.340]   I've even got like a gaming sim driving rig in the closet over there.
[00:49:00.340 --> 00:49:05.460]   I have a reasonable gaming budget and I just don't have VR yet because I just don't, you
[00:49:05.460 --> 00:49:09.020]   know, so my question is, if it borrows right, that they will have the killer app.
[00:49:09.020 --> 00:49:10.020]   Cool.
[00:49:10.020 --> 00:49:11.020]   What is it?
[00:49:11.020 --> 00:49:12.020]   That's what I can't put my finger on.
[00:49:12.020 --> 00:49:13.020]   I can either.
[00:49:13.020 --> 00:49:19.260]   That's the thing is, so that I get story-eyed sometimes about VR.
[00:49:19.260 --> 00:49:21.060]   I've got the Oculus Quest 2.
[00:49:21.060 --> 00:49:23.860]   I bought it before they use the price.
[00:49:23.860 --> 00:49:28.260]   And I used it for a while and thought, oh yeah, this is so great.
[00:49:28.260 --> 00:49:34.460]   I'm going to especially do the virtual workstation where I can have a big screen here and two
[00:49:34.460 --> 00:49:36.740]   little ones here and have my virtual.
[00:49:36.740 --> 00:49:39.900]   And I just, I don't, I mean, I don't want to mess up my hair.
[00:49:39.900 --> 00:49:44.340]   I don't want to be sitting in this space where it's like hot and there's just this device
[00:49:44.340 --> 00:49:45.340]   on my face.
[00:49:45.340 --> 00:49:48.740]   Like, there are so many reasons why it's uncomfortable.
[00:49:48.740 --> 00:49:50.420]   It's unwieldy.
[00:49:50.420 --> 00:49:52.380]   It's awkward.
[00:49:52.380 --> 00:49:57.300]   It's all of these things that I don't know how they get past that.
[00:49:57.300 --> 00:50:02.700]   And I really like, I have to say, I feel like I have said this for years and I still believe
[00:50:02.700 --> 00:50:03.700]   it.
[00:50:03.700 --> 00:50:10.820]   I am just, I feel like all of this is just stopgap to the eventual AR future where I
[00:50:10.820 --> 00:50:13.820]   can hold out my hand and it is a phone screen.
[00:50:13.820 --> 00:50:14.820]   You know what I mean?
[00:50:14.820 --> 00:50:22.020]   Or just in front of me, I can have my notifications pop up or any time I look at a wall and there's
[00:50:22.020 --> 00:50:26.180]   a rectangle that's available that can suddenly become a screen or the, I mean, we've already
[00:50:26.180 --> 00:50:30.900]   got huds in cars in some cases and I know none of you are mobile.
[00:50:30.900 --> 00:50:31.900]   Sorry.
[00:50:31.900 --> 00:50:37.440]   But it's that that is what I think we're waiting on because that is the time where it
[00:50:37.440 --> 00:50:41.100]   is not this unwieldy ski goggles.
[00:50:41.100 --> 00:50:42.820]   We're strapping to our face.
[00:50:42.820 --> 00:50:43.820]   Right.
[00:50:43.820 --> 00:50:47.220]   No, the ski goggles were strapping to our face that, look, I know that apples will, I'm
[00:50:47.220 --> 00:50:49.380]   sure be the best design of all of them.
[00:50:49.380 --> 00:50:55.340]   Like you, I also got a quest to right before the price went up because I was like, well,
[00:50:55.340 --> 00:50:58.020]   I'm not going to pay more for this old thing.
[00:50:58.020 --> 00:50:59.020]   Exactly.
[00:50:59.020 --> 00:51:01.540]   You know, so that was my way of getting on.
[00:51:01.540 --> 00:51:02.540]   And it's fun.
[00:51:02.540 --> 00:51:05.260]   I think there's some decent kind of gaming things like you, Alex.
[00:51:05.260 --> 00:51:10.060]   I also have a gaming PC and I have all the consoles and I don't play games all that much,
[00:51:10.060 --> 00:51:11.620]   but I have all of the gaming things.
[00:51:11.620 --> 00:51:14.900]   And so I'm still trying to figure out like, what is the experience would bring me into
[00:51:14.900 --> 00:51:15.900]   this?
[00:51:15.900 --> 00:51:19.940]   I do think that there are some gaming scenarios where I could be like, oh, if I could have
[00:51:19.940 --> 00:51:25.220]   like a like like star fields, that actually strikes me as a game that if that was able
[00:51:25.220 --> 00:51:29.300]   to be like a like a VR sort of experience, and I know it's not going to be, but that
[00:51:29.300 --> 00:51:33.340]   stuff seems like the sort of thing that I could be like, okay, this, this could be incredible.
[00:51:33.340 --> 00:51:34.340]   Right.
[00:51:34.340 --> 00:51:38.620]   And Starfield is the new space based RPG that's coming from the famous company that's done
[00:51:38.620 --> 00:51:39.620]   the stuff.
[00:51:39.620 --> 00:51:40.620]   Bethesda.
[00:51:40.620 --> 00:51:41.620]   Yes.
[00:51:41.620 --> 00:51:42.620]   But that's supposed to say it.
[00:51:42.620 --> 00:51:43.620]   Okay.
[00:51:43.620 --> 00:51:44.620]   It's like, no, no, I forgot.
[00:51:44.620 --> 00:51:45.620]   It's Skyrim, right?
[00:51:45.620 --> 00:51:47.820]   Um, no, they make a fallout.
[00:51:47.820 --> 00:51:50.860]   Oh, because Skyrim's terrible, but I haven't played Fallout yet.
[00:51:50.860 --> 00:51:51.860]   I'll give that a try.
[00:51:51.860 --> 00:51:52.860]   Okay.
[00:51:52.860 --> 00:51:53.860]   Fallout's amazing.
[00:51:53.860 --> 00:51:56.860]   And so that would be like, I could see that kind of being cool.
[00:51:56.860 --> 00:52:00.420]   But yeah, but you're kind of look, I think the world all sort of looking for like, what
[00:52:00.420 --> 00:52:01.420]   is the experience?
[00:52:01.420 --> 00:52:04.020]   What is the thing that would take the next level?
[00:52:04.020 --> 00:52:07.620]   Because even something like like saying like, okay, I can now have great FaceTime conversations
[00:52:07.620 --> 00:52:09.500]   where I can see the person I'm talking to.
[00:52:09.500 --> 00:52:10.500]   Okay.
[00:52:10.500 --> 00:52:13.140]   But am I going to see them like with their Memoji?
[00:52:13.140 --> 00:52:14.140]   Right.
[00:52:14.140 --> 00:52:20.380]   And I'm going to see actual them, in which case, are they going to see me and my stupid goggles?
[00:52:20.380 --> 00:52:26.180]   There's also this whole thing that apparently the rumors are that to get the battery thing
[00:52:26.180 --> 00:52:30.980]   working, it'll be a clip on battery pack, which on the one hand, I'm like, okay, that's
[00:52:30.980 --> 00:52:33.580]   better than having the headset be really heavy.
[00:52:33.580 --> 00:52:36.780]   But on the other hand, I'm like, you're going to make me clip on a battery pack.
[00:52:36.780 --> 00:52:38.580]   Haven't had to do that since what?
[00:52:38.580 --> 00:52:39.580]   The early.
[00:52:39.580 --> 00:52:40.580]   Exactly.
[00:52:40.580 --> 00:52:43.060]   It's going to remind me of like, I was going to say, yeah, that's going to remind me of
[00:52:43.060 --> 00:52:49.900]   like when I used to have my Game Boy, like battery pack thing, you know, as a kid or,
[00:52:49.900 --> 00:52:52.500]   you know, fortunately we don't have to do this as frequently.
[00:52:52.500 --> 00:52:56.300]   But when everybody used to have to carry around like a battery pack for their phone, right?
[00:52:56.300 --> 00:53:00.420]   Like everybody would just carry like a huge, this was like five years ago now because we
[00:53:00.420 --> 00:53:01.420]   all have tablets.
[00:53:01.420 --> 00:53:03.580]   There's plenty of battery built in.
[00:53:03.580 --> 00:53:04.580]   Yeah, exactly.
[00:53:04.580 --> 00:53:05.740]   Plenty of battery built in.
[00:53:05.740 --> 00:53:09.980]   But like that's the sort of thing is like, okay, so what is the experience going to be?
[00:53:09.980 --> 00:53:13.140]   And how is it going to be better than what we've already got?
[00:53:13.140 --> 00:53:17.180]   And on top of that, how is it going to be worth $3,000?
[00:53:17.180 --> 00:53:21.940]   Which if that is true, I'm still skeptical about that because even for Apple, that seems
[00:53:21.940 --> 00:53:24.620]   an absurd price point to come in at.
[00:53:24.620 --> 00:53:25.620]   Yeah.
[00:53:25.620 --> 00:53:29.100]   And I have a larger technology budget than I probably should because I'm a little bit
[00:53:29.100 --> 00:53:31.540]   obsessed and still having my entire life.
[00:53:31.540 --> 00:53:36.100]   And three K is I have to go talk to my class and be like, I would like to waste $3,000
[00:53:36.100 --> 00:53:38.980]   and then she'll give me a look like, or you could not.
[00:53:38.980 --> 00:53:39.980]   And it was 500.
[00:53:39.980 --> 00:53:45.780]   I could be like, honey, by the way, I wasted five hours and then I'll get my earloped
[00:53:45.780 --> 00:53:46.780]   and it's fine.
[00:53:46.780 --> 00:53:52.020]   500, 1000, 1500 for me, like my budget, like it's not even going to be a conversation.
[00:53:52.020 --> 00:53:54.460]   3000, I have to think about it.
[00:53:54.460 --> 00:53:58.500]   And especially, and I'm thinking, like 3000 was like, that's three, that's three MacBook
[00:53:58.500 --> 00:53:59.500]   errors.
[00:53:59.500 --> 00:54:00.500]   Really?
[00:54:00.500 --> 00:54:02.260]   That's the beauty of the technology price now?
[00:54:02.260 --> 00:54:04.900]   I mean, that's what it should be, right?
[00:54:04.900 --> 00:54:06.700]   But like that's three MacBook errors.
[00:54:06.700 --> 00:54:09.700]   How many MacBook errors was your couch back there?
[00:54:09.700 --> 00:54:12.140]   So I'm getting a nice, it is a really nice couch.
[00:54:12.140 --> 00:54:13.140]   Yeah.
[00:54:13.140 --> 00:54:14.140]   It is.
[00:54:14.140 --> 00:54:15.140]   Thank you so much.
[00:54:15.140 --> 00:54:16.140]   It's a great backdrop.
[00:54:16.140 --> 00:54:17.140]   Yeah.
[00:54:17.140 --> 00:54:21.300]   It is actually, it's funny how the three of us on the panel have such different vibes
[00:54:21.300 --> 00:54:24.940]   in our backgrounds.
[00:54:24.940 --> 00:54:26.020]   Mine is extra.
[00:54:26.020 --> 00:54:29.660]   So I hope you really look like put together.
[00:54:29.660 --> 00:54:32.220]   Like I want to have like, I would have like a dinner party at your house because I know
[00:54:32.220 --> 00:54:35.340]   the foods could be like on point and well point out.
[00:54:35.340 --> 00:54:36.340]   I have tea parties.
[00:54:36.340 --> 00:54:39.220]   If anyone's interested, it's like my thing.
[00:54:39.220 --> 00:54:44.140]   Wait, once we all get these Apple headsets, we can virtually have it in your house.
[00:54:44.140 --> 00:54:45.140]   I want to go.
[00:54:45.140 --> 00:54:46.980]   That's what I want to headset for.
[00:54:46.980 --> 00:54:49.060]   So everyone can do tea party together.
[00:54:49.060 --> 00:54:50.060]   Yeah.
[00:54:50.060 --> 00:54:52.020]   And that I would love, but I also want to like be there in person.
[00:54:52.020 --> 00:54:54.860]   See, let's do both.
[00:54:54.860 --> 00:54:57.580]   Wait, because then, but then also heads up.
[00:54:57.580 --> 00:55:00.340]   Are less money than the headset will be.
[00:55:00.340 --> 00:55:01.340]   So, oh man.
[00:55:01.340 --> 00:55:02.340]   Oh no.
[00:55:02.340 --> 00:55:04.780]   So I have to ask Apple errors.
[00:55:04.780 --> 00:55:09.460]   So let's think then, let's go back to when the Apple Watch was first announced.
[00:55:09.460 --> 00:55:14.860]   Because I think when before we knew exactly what that was going to be, and even after Apple
[00:55:14.860 --> 00:55:20.740]   first announced it as the most personal device you'll ever own, it was not something that
[00:55:20.740 --> 00:55:25.420]   we thought was going to be a thing that we needed.
[00:55:25.420 --> 00:55:31.660]   And now this is an incredibly popular wearable device that a lot of people use for health
[00:55:31.660 --> 00:55:36.100]   and fitness and notifications on their wrist.
[00:55:36.100 --> 00:55:42.220]   And it's almost become a proprietary eponym where it's Kleenex and you've got an Apple
[00:55:42.220 --> 00:55:44.260]   Watch on your wrist, even if it's a Fitbit.
[00:55:44.260 --> 00:55:47.500]   But what, I mean, do we think of this?
[00:55:47.500 --> 00:55:51.620]   What is it that makes us feel, because it does sound like we're saying that this is
[00:55:51.620 --> 00:55:55.780]   different from something like the Apple Watch where it doesn't seem like it's going to
[00:55:55.780 --> 00:56:00.700]   fall into that boat of, like it makes sense for them to have and for us to have and for
[00:56:00.700 --> 00:56:01.700]   us to want?
[00:56:01.700 --> 00:56:05.300]   Well, let me take one point about the Apple Watch because you're right that it has become
[00:56:05.300 --> 00:56:07.420]   ubiquitous and it is a bona fide hit.
[00:56:07.420 --> 00:56:13.140]   But as I mentioned, it was originally sold as this weird fashion thing.
[00:56:13.140 --> 00:56:16.620]   That was where all the marketing and all the conversations and interviews that I did with
[00:56:16.620 --> 00:56:22.700]   people at Apple was really focused on the design and talking about it as a fashion accessory.
[00:56:22.700 --> 00:56:25.580]   But obviously, we then have, and part of that was because they didn't have a lot of
[00:56:25.580 --> 00:56:31.140]   apps at launch, it was kind of a glorified notification screen.
[00:56:31.140 --> 00:56:36.940]   It took some time for them to have the actual Watch OS SDK to really let people build interesting
[00:56:36.940 --> 00:56:39.580]   things that could run standalone.
[00:56:39.580 --> 00:56:41.580]   But they dropped the price.
[00:56:41.580 --> 00:56:42.580]   And they dropped the price a lot.
[00:56:42.580 --> 00:56:46.220]   They got rid of the gold edition, the $16,000 watch.
[00:56:46.220 --> 00:56:49.460]   They thought that China was going to go crazy for.
[00:56:49.460 --> 00:56:51.860]   China didn't go crazy for.
[00:56:51.860 --> 00:56:56.580]   And it became this thing that was like, I think I always get the stainless steel model in part
[00:56:56.580 --> 00:57:02.020]   because I've used my company's fitness budget as they would let me buy an Apple Watch, but
[00:57:02.020 --> 00:57:04.020]   then I'm like, well, that's what I'll buy.
[00:57:04.020 --> 00:57:08.780]   Most people get the aluminum sport edition and you buy it every couple of years and it's
[00:57:08.780 --> 00:57:09.780]   great.
[00:57:09.780 --> 00:57:12.620]   It's just kind of a general, it's an upgrade from a Fitbit, right?
[00:57:12.620 --> 00:57:14.220]   But it is sort of this thing.
[00:57:14.220 --> 00:57:18.260]   So the price dropped and then they focused in on the health benefits.
[00:57:18.260 --> 00:57:21.780]   This is the sort of thing where for me, it's hard to see how does this become that if
[00:57:21.780 --> 00:57:23.780]   you're starting at $3,000?
[00:57:23.780 --> 00:57:24.780]   Yeah.
[00:57:24.780 --> 00:57:25.780]   I mean, it's good.
[00:57:25.780 --> 00:57:30.780]   No, because we're trying to think about it, right?
[00:57:30.780 --> 00:57:31.780]   But go ahead, Abra.
[00:57:31.780 --> 00:57:36.140]   Oh, no, I was just going to say, I think, you know, especially with if we're talking
[00:57:36.140 --> 00:57:39.940]   about health features on the Apple Watch, they really, really focused on giving you something
[00:57:39.940 --> 00:57:42.260]   with the watch that you're not going to get with your phone because it's like, okay,
[00:57:42.260 --> 00:57:45.660]   I can count my steps on the watch, okay, but I can also count my steps on the phone.
[00:57:45.660 --> 00:57:46.660]   Right.
[00:57:46.660 --> 00:57:49.940]   And then they added more health features to the watch that would, you know, entice you
[00:57:49.940 --> 00:57:52.300]   to buy that so that you're adding to what you can already do.
[00:57:52.300 --> 00:57:55.620]   So what can they add to the headset that you can already do on your phone?
[00:57:55.620 --> 00:57:56.620]   Okay.
[00:57:56.620 --> 00:58:00.180]   So maybe you're thinking, you know, getting that heads up display as you're navigating
[00:58:00.180 --> 00:58:03.020]   or as you're walking around, is that something that you really need?
[00:58:03.020 --> 00:58:06.420]   Can they convince you that's something that you really need, maybe using maps or something?
[00:58:06.420 --> 00:58:07.420]   I don't know.
[00:58:07.420 --> 00:58:09.980]   But I think, yeah, I think answering that question, what can I get through that that
[00:58:09.980 --> 00:58:13.660]   will allow me to step outside of my home, maybe even leave my phone behind?
[00:58:13.660 --> 00:58:14.660]   I don't know.
[00:58:14.660 --> 00:58:17.780]   Something along those lines just to really convince you that this is going to be worth
[00:58:17.780 --> 00:58:18.780]   my money.
[00:58:18.780 --> 00:58:19.780]   Do you think?
[00:58:19.780 --> 00:58:20.780]   Go ahead.
[00:58:20.780 --> 00:58:21.780]   Go ahead, Alex.
[00:58:21.780 --> 00:58:26.020]   Oh, I was just going to say, if we combine the HUD idea of every rectangle being a screen
[00:58:26.020 --> 00:58:29.420]   and a browser's point about, you know, having to be something that you can kind of use
[00:58:29.420 --> 00:58:34.220]   today, what if instead of having a nerd helmet that encases your face, we had a small screen
[00:58:34.220 --> 00:58:37.020]   that was attached to the side of your glasses in front of your glasses?
[00:58:37.020 --> 00:58:39.420]   I don't think about it in this small time.
[00:58:39.420 --> 00:58:42.780]   And then what you could do is you could see and also get notifications.
[00:58:42.780 --> 00:58:47.580]   And I just want to say that Google Glass, though we all giggle at who used it and who
[00:58:47.580 --> 00:58:48.580]   got beaten up wearing it.
[00:58:48.580 --> 00:58:49.580]   Yes.
[00:58:49.580 --> 00:58:52.540]   The one time I got to wear a Google Glass, I saw the translation app where it would take
[00:58:52.540 --> 00:58:56.780]   a sign in language and replace it with your language.
[00:58:56.780 --> 00:58:57.780]   Brilliant.
[00:58:57.780 --> 00:59:01.580]   One of the few times in tech when I was like, hot dang, that's really different.
[00:59:01.580 --> 00:59:02.580]   That is amazing.
[00:59:02.580 --> 00:59:03.580]   I totally agree.
[00:59:03.580 --> 00:59:07.260]   I think that like there was a, there was a concept video that they made for Google Glass,
[00:59:07.260 --> 00:59:10.580]   which was not what the actual experience was to go back to what you were saying earlier,
[00:59:10.580 --> 00:59:14.060]   Mike, about how, how are they going to show this off at WWDC?
[00:59:14.060 --> 00:59:18.340]   The concept video for Google Glass, a decade plus later, I still think is one of the most
[00:59:18.340 --> 00:59:26.660]   a I will never forget Google Glass being at IO and seeing a Sergei, like a jump out of
[00:59:26.660 --> 00:59:30.420]   the plane and then walk up on stage.
[00:59:30.420 --> 00:59:34.260]   That was, that's, that to this day is still like one of the most impressive keynote feeds
[00:59:34.260 --> 00:59:36.580]   that I don't think we'll ever be beaten.
[00:59:36.580 --> 00:59:40.580]   But that concept video, which, which was not what the actual experience was where you could
[00:59:40.580 --> 00:59:45.220]   be like, oh, this will be a sort of map and this is how you can walk or the translating,
[00:59:45.220 --> 00:59:48.140]   you know, the words, which was a real thing.
[00:59:48.140 --> 00:59:51.860]   And the, even the idea of like, oh, you could take photos, you know, with, with kind of,
[00:59:51.860 --> 00:59:55.140]   with your eyes really compelling.
[00:59:55.140 --> 00:59:58.220]   Google Glass didn't work for a lot of reasons.
[00:59:58.220 --> 01:00:02.300]   The earliest to doctors were definitely hindrance to that.
[01:00:02.300 --> 01:00:03.300]   But I think you're right.
[01:00:03.300 --> 01:00:06.860]   Like that is the sort of thing maybe, and maybe that is, I think, that gets to my guess
[01:00:06.860 --> 01:00:07.860]   point.
[01:00:07.860 --> 01:00:08.860]   Maybe that is the problem.
[01:00:08.860 --> 01:00:12.660]   Like we're all kind of waiting for that next level where we can have these things merged,
[01:00:12.660 --> 01:00:15.820]   you know, and kind of be a little bit like minority reports.
[01:00:15.820 --> 01:00:20.220]   But do we have to have this first step first?
[01:00:20.220 --> 01:00:23.340]   And I guess, and I guess the question is, because a lot of people have been trying these
[01:00:23.340 --> 01:00:27.340]   things with these headsets for, you know, eight years now, I guess going back to the
[01:00:27.340 --> 01:00:32.700]   first Oculus is, are we willing to wait through kind of the drudgery and like kind of the
[01:00:32.700 --> 01:00:39.900]   acceptance that this is the, um, rear projection TV era before we get to like our, you know,
[01:00:39.900 --> 01:00:43.500]   our, our LED and LCD, you know, flat panel TVs.
[01:00:43.500 --> 01:00:44.500]   Yeah.
[01:00:44.500 --> 01:00:49.300]   Do you think that the biggest problem with Google Glass was that it wasn't Apple Glass?
[01:00:49.300 --> 01:00:50.300]   Yeah.
[01:00:50.300 --> 01:00:52.540]   So I just changed the branding.
[01:00:52.540 --> 01:00:55.580]   Had Apple sell it, I bet it would have sold what a hundred times as many units.
[01:00:55.580 --> 01:00:56.580]   I think it was totally.
[01:00:56.580 --> 01:00:59.940]   I think it was absolutely that was one of the main issues.
[01:00:59.940 --> 01:01:05.900]   But I think that unfortunately the group of people who first got their hands on Google
[01:01:05.900 --> 01:01:10.620]   Glass made it incredibly unfashionable and what sort of gosh, is that the word?
[01:01:10.620 --> 01:01:11.620]   Oh, yeah.
[01:01:11.620 --> 01:01:13.380]   Well, their glass holes was the thing.
[01:01:13.380 --> 01:01:14.380]   Yeah.
[01:01:14.380 --> 01:01:15.860]   And they really like they embodied it.
[01:01:15.860 --> 01:01:16.860]   So they did.
[01:01:16.860 --> 01:01:21.820]   No, and I, but I think that goes along with like if it had been an Apple versus Google,
[01:01:21.820 --> 01:01:27.180]   I think that Apple would have been much better about seeding that early adopter thing.
[01:01:27.180 --> 01:01:28.180]   Yeah, the group.
[01:01:28.180 --> 01:01:29.180]   Yeah.
[01:01:29.180 --> 01:01:30.180]   Yeah.
[01:01:30.180 --> 01:01:34.060]   But, but as you wonder though, like a bit, a bit, a bit, you know, also I will say to
[01:01:34.060 --> 01:01:36.500]   Google Glass was just a little too far ahead of its time.
[01:01:36.500 --> 01:01:38.700]   The vision of where it was, it wasn't able to do it.
[01:01:38.700 --> 01:01:42.180]   Like it could translate those words for you and it was magical, but it also took a long
[01:01:42.180 --> 01:01:43.180]   time.
[01:01:43.180 --> 01:01:46.900]   You need it to be instantaneous for that sort of thing to really work, which is ironically
[01:01:46.900 --> 01:01:49.660]   sort of what we're seeing with, with generative AI now.
[01:01:49.660 --> 01:01:55.380]   Whereas when you look at AI assistance from, you know, a decade ago, it promises this stuff
[01:01:55.380 --> 01:01:56.380]   that it wasn't there.
[01:01:56.380 --> 01:01:59.780]   Now we can add, I think the reason why people get so excited about chat GPT, even when it
[01:01:59.780 --> 01:02:03.820]   hallucinates, even when it's wrong, is that you can ask these questions and it will tell
[01:02:03.820 --> 01:02:07.300]   you the name of the TV show and the episodes that you were talking about.
[01:02:07.300 --> 01:02:11.620]   And it will, it will give you this information like immediately and it's not taking forever.
[01:02:11.620 --> 01:02:13.660]   It's giving it to you instantly.
[01:02:13.660 --> 01:02:16.700]   And that's, that's what we need for, for these things, I think to work.
[01:02:16.700 --> 01:02:17.700]   I agree.
[01:02:17.700 --> 01:02:18.700]   Absolutely.
[01:02:18.700 --> 01:02:23.380]   And the fact that it, because this is what I think is going to be so magnificent about
[01:02:23.380 --> 01:02:30.020]   what Microsoft is doing with office, when you can let that assistant because, so it's,
[01:02:30.020 --> 01:02:31.620]   it's multiple points here.
[01:02:31.620 --> 01:02:37.300]   One, I don't have to speak to it in its specific syntax and diction.
[01:02:37.300 --> 01:02:39.220]   I can say what I want.
[01:02:39.220 --> 01:02:43.300]   And it is already using that large language model processing to figure out what I'm actually
[01:02:43.300 --> 01:02:44.300]   asking.
[01:02:44.300 --> 01:02:50.500]   And then what it can do from there is go out to different aspects of, in the case of office,
[01:02:50.500 --> 01:02:53.460]   these different tools and pull all of that together.
[01:02:53.460 --> 01:02:59.860]   So when I say, take this presentation and turn it into a document for a summary, I didn't
[01:02:59.860 --> 01:03:03.860]   have to say, hey, boo, boo, please do this and this and this.
[01:03:03.860 --> 01:03:05.180]   And then I have to wait for it to do that.
[01:03:05.180 --> 01:03:08.020]   And then I have to ask the next thing and then the next thing and the next thing, it
[01:03:08.020 --> 01:03:10.180]   just can take all of what I'm saying.
[01:03:10.180 --> 01:03:14.860]   And then it properly translates it into the language of the computer system and then goes
[01:03:14.860 --> 01:03:17.300]   out to those three or four different aspects.
[01:03:17.300 --> 01:03:23.740]   And the fact that it can do all of that together because it has the ability to translate what
[01:03:23.740 --> 01:03:28.860]   we as humans are actually asking into something that the computer can understand, I think
[01:03:28.860 --> 01:03:30.380]   that's what makes it next level.
[01:03:30.380 --> 01:03:36.660]   I do want to ask though too, because I do think that, you know, the AR thing is a more
[01:03:36.660 --> 01:03:37.660]   long term.
[01:03:37.660 --> 01:03:45.700]   We've heard that this could be a mixed reality headset, but a lot of times with VR, the supposedly
[01:03:45.700 --> 01:03:49.900]   compelling and killer apps are games.
[01:03:49.900 --> 01:03:54.620]   Does Apple's lack of chops in gaming?
[01:03:54.620 --> 01:04:01.180]   Is that going to be harmful to the company's ability to push a product that very likely
[01:04:01.180 --> 01:04:07.060]   is going to be heavily focused, at least in its first iterations on that gaming ecosystem?
[01:04:07.060 --> 01:04:12.540]   And then furthermore, given what we've seen about Apple inviting special press members
[01:04:12.540 --> 01:04:16.340]   to the game to talk about the gaming stuff that they're doing, and I'm talking a lot
[01:04:16.340 --> 01:04:17.980]   about gaming on Mac OS.
[01:04:17.980 --> 01:04:25.260]   And I think there was a game that launched on Mac OS not too terribly long ago, Resident
[01:04:25.260 --> 01:04:31.900]   Evil or something like that, I don't recall, is this going to be trouble for Apple to be
[01:04:31.900 --> 01:04:37.300]   able to say, hey, we've got a device and you can game on it and you want to game with us
[01:04:37.300 --> 01:04:44.740]   as opposed to Microsoft's Xbox or the PlayStation?
[01:04:44.740 --> 01:04:45.900]   So Christian, you don't game much.
[01:04:45.900 --> 01:04:47.140]   A RAR, do you game?
[01:04:47.140 --> 01:04:48.140]   No.
[01:04:48.140 --> 01:04:54.900]   I mean, I game, but I just, I don't, I game a smaller proportion of what I spend on gaming.
[01:04:54.900 --> 01:04:58.980]   Okay, no, I have the next box sitting on my desk over there that I've used twice.
[01:04:58.980 --> 01:05:00.660]   Oh, so none of us are gamers.
[01:05:00.660 --> 01:05:02.300]   No, I'm a PC gamer.
[01:05:02.300 --> 01:05:03.660]   Oh, so you do PC game?
[01:05:03.660 --> 01:05:04.820]   I'm an actual gamer.
[01:05:04.820 --> 01:05:05.820]   I'm not.
[01:05:05.820 --> 01:05:07.060]   Oh, no, get him off.
[01:05:07.060 --> 01:05:08.060]   Get him off of here.
[01:05:08.060 --> 01:05:09.060]   Bye.
[01:05:09.060 --> 01:05:10.060]   This is one of my longest.
[01:05:10.060 --> 01:05:11.220]   Okay, okay, calm down.
[01:05:11.220 --> 01:05:16.540]   It's a really, it's a really good point about Apple and gaming because I love my Macs.
[01:05:16.540 --> 01:05:17.540]   I do.
[01:05:17.540 --> 01:05:18.540]   They're great.
[01:05:18.540 --> 01:05:22.940]   But if I ever want to play anything that's interactive, if I are PC, if I are up steam,
[01:05:22.940 --> 01:05:24.860]   it is, it is easier, it is faster.
[01:05:24.860 --> 01:05:25.860]   Things are better.
[01:05:25.860 --> 01:05:27.580]   They're more optimized.
[01:05:27.580 --> 01:05:34.020]   And frankly, I'm trying to think of a single game I've seen on iOS app store that I've
[01:05:34.020 --> 01:05:38.980]   looked at on iPads and iPhones for years that I would be excited about playing in a VR context.
[01:05:38.980 --> 01:05:40.620]   And the answer is none.
[01:05:40.620 --> 01:05:44.780]   I think so given that either they're going to have no games that make it exciting and
[01:05:44.780 --> 01:05:49.380]   therefore it's kind of to put out the gate or they're working with major game developers
[01:05:49.380 --> 01:05:52.300]   already on this, but I feel like we've heard nothing about that.
[01:05:52.300 --> 01:05:53.460]   I don't think we would.
[01:05:53.460 --> 01:05:56.540]   So it Apple can't keep its secrets anymore.
[01:05:56.540 --> 01:05:57.540]   It's too big.
[01:05:57.540 --> 01:05:59.620]   It almost is a detriment to the company.
[01:05:59.620 --> 01:06:04.820]   I think that Minecraft is owned by a certain big company because I think that's a killer
[01:06:04.820 --> 01:06:05.820]   AR experience.
[01:06:05.820 --> 01:06:07.820]   Oh, you're dead on.
[01:06:07.820 --> 01:06:09.220]   I've had the, it's so funny.
[01:06:09.220 --> 01:06:13.420]   I had the same conversation with somebody the other day where I said exactly that where
[01:06:13.420 --> 01:06:14.860]   I was like minecraft.
[01:06:14.860 --> 01:06:16.500]   It was actually in my work slack.
[01:06:16.500 --> 01:06:20.380]   I was like Minecraft ironically would be like the perfect killer thing for this.
[01:06:20.380 --> 01:06:24.780]   And I know they tried to show that off in one of the early HoloLens demos, which was
[01:06:24.780 --> 01:06:29.220]   amazing, but like you're like, okay, but now you could actually bring it there.
[01:06:29.220 --> 01:06:34.660]   And, and not that I think that, you know, I think that they would be very happy to have
[01:06:34.660 --> 01:06:40.300]   Minecraft on stage instead of Roblox, you know, showing that off, right?
[01:06:40.300 --> 01:06:43.380]   But those are the two things where you think about you're like, okay, that, but Minecraft
[01:06:43.380 --> 01:06:45.820]   really would be like a perfect like.
[01:06:45.820 --> 01:06:52.780]   I would love to build like it is to turn my living to turn a bras sofa into a Minecraft
[01:06:52.780 --> 01:06:58.100]   mine, dig down inside and find like gold or I don't even know how mine works.
[01:06:58.100 --> 01:07:00.860]   It would be so fun.
[01:07:00.860 --> 01:07:04.540]   But Christina's point about the early HoloLens stuff is dead on because that was the demo
[01:07:04.540 --> 01:07:05.620]   that really made me feel amazing.
[01:07:05.620 --> 01:07:09.100]   I pushed a block off a thing and it was like, yeah, through a hole at a table, but the table
[01:07:09.100 --> 01:07:11.820]   was real there, but I couldn't see the table because I was wearing HoloLens and it was
[01:07:11.820 --> 01:07:16.660]   like it did feel like a step forward turned out to nowhere, but maybe someone will get
[01:07:16.660 --> 01:07:17.660]   it right down the road.
[01:07:17.660 --> 01:07:21.980]   And if someone is going to get it right late, a bit like smartphones and the iPhone, perhaps
[01:07:21.980 --> 01:07:22.980]   it's going to be Apple.
[01:07:22.980 --> 01:07:26.700]   So I'm like skeptical, but always willing to give them benefit of the doubt because I've
[01:07:26.700 --> 01:07:27.700]   been wrong in the past.
[01:07:27.700 --> 01:07:32.060]   So I don't want to say no, I'm going to say, yeah, probably not, but.
[01:07:32.060 --> 01:07:33.700]   Yeah, that's how I feel.
[01:07:33.700 --> 01:07:37.980]   I'm like, I will never bet against Apple, but I'm also a skeptical, but some of the rumors
[01:07:37.980 --> 01:07:42.260]   about the price and some of those things was I'm like, oh, that just doesn't feel right
[01:07:42.260 --> 01:07:43.260]   at all.
[01:07:43.260 --> 01:07:46.020]   And I tried to remind myself, oh, well, remember how much they said the iPad was going to cost
[01:07:46.020 --> 01:07:47.620]   and the iPad was only $500.
[01:07:47.620 --> 01:07:48.620]   That's true.
[01:07:48.620 --> 01:07:53.420]   And, you know, so maybe the wrong on that, but you do wonder you're like, you need that
[01:07:53.420 --> 01:08:00.260]   killer app and to reverse for like, I think that for the Apple Watch, it really was like
[01:08:00.260 --> 01:08:03.860]   the health stuff, like heart rate, like when you could start doing the heart monitoring
[01:08:03.860 --> 01:08:08.180]   and starting like the fall detection and things like that, that's when it went from being
[01:08:08.180 --> 01:08:12.780]   like, okay, well, this is a glorified fit that to, well, oh, this is also this thing
[01:08:12.780 --> 01:08:18.380]   I can get my parents or, you know, that, that, you know, somebody else, you know, might be
[01:08:18.380 --> 01:08:19.380]   useful for me.
[01:08:19.380 --> 01:08:24.780]   I mean, like the Apple Watch, I had some sort of weird heart thing where my, my, uh,
[01:08:24.780 --> 01:08:27.940]   hooray was like, I was just standing.
[01:08:27.940 --> 01:08:29.700]   It was like 140.
[01:08:29.700 --> 01:08:34.380]   And for just resting heart rate and it notified me of that, right?
[01:08:34.380 --> 01:08:36.620]   Like, but there have been other eighth of situations.
[01:08:36.620 --> 01:08:40.860]   I wasn't an a thip, fortunately, where that's a real killer feature.
[01:08:40.860 --> 01:08:47.220]   And so I don't know what the corollary there is in this world, but if anybody can get it,
[01:08:47.220 --> 01:08:48.700]   it's probably them.
[01:08:48.700 --> 01:08:49.700]   Yeah.
[01:08:49.700 --> 01:08:54.780]   The watch though, like sits to for me, like, very much on top of my iPhone and how I think
[01:08:54.780 --> 01:08:56.500]   about their usefulness, you know?
[01:08:56.500 --> 01:08:57.500]   Yes.
[01:08:57.500 --> 01:09:02.660]   I'm very curious, does the AR VR thing from Apple, does it sit on top of my Mac or is
[01:09:02.660 --> 01:09:04.140]   it in Canada?
[01:09:04.140 --> 01:09:07.180]   What does it kind of lean on for leverage to get a toehold into the market?
[01:09:07.180 --> 01:09:12.940]   If it's not existing applications or an obvious, easy price point, you're running
[01:09:12.940 --> 01:09:13.940]   the lower options.
[01:09:13.940 --> 01:09:14.940]   Yeah.
[01:09:14.940 --> 01:09:15.940]   Good point.
[01:09:15.940 --> 01:09:16.940]   No, that's a good point.
[01:09:16.940 --> 01:09:20.180]   I mean, it would be interesting if they had it sit on top of the Mac is in that case,
[01:09:20.180 --> 01:09:23.580]   I'd be like, well, okay, maybe instead of buying two Apple, you know, studio displays,
[01:09:23.580 --> 01:09:29.100]   I'd just buy one of these things and then I have, you know, like all the screens.
[01:09:29.100 --> 01:09:31.980]   That would be a way for me to talk myself into buying one.
[01:09:31.980 --> 01:09:33.420]   I'm not going to lie.
[01:09:33.420 --> 01:09:39.340]   But it probably will be building off of the iPhone, which is, you know, has had AR kit
[01:09:39.340 --> 01:09:42.460]   for a long time and that they will be building those experiences on.
[01:09:42.460 --> 01:09:47.140]   But then yeah, you have to wonder, but can this can this operate independently?
[01:09:47.140 --> 01:09:48.700]   That's the question.
[01:09:48.700 --> 01:09:49.700]   That's the question.
[01:09:49.700 --> 01:09:52.020]   Does anyone here use AR on their iPhone?
[01:09:52.020 --> 01:09:59.900]   So the thing is many of us who are using our iPhones for cameras are using AR kit, whether
[01:09:59.900 --> 01:10:06.500]   we know it or not, because there are there's some of the camera functionality that makes
[01:10:06.500 --> 01:10:12.660]   use of some of the tools in AR kit to do proper focusing and proper depth of field effects.
[01:10:12.660 --> 01:10:15.820]   If you're using it, I don't really use the portrait effects very often.
[01:10:15.820 --> 01:10:20.220]   But in terms of sort of apps that make use of AR, I can't remember the last time I did
[01:10:20.220 --> 01:10:26.780]   it outside of just purposely doing a gimmick where I was kind of like, I designed.
[01:10:26.780 --> 01:10:28.260]   The measurement app.
[01:10:28.260 --> 01:10:29.580]   Yeah, yeah, the measurement app.
[01:10:29.580 --> 01:10:30.580]   That's a good one.
[01:10:30.580 --> 01:10:35.860]   That's actually a good recommendation for people to get a chance to see AR kit in action
[01:10:35.860 --> 01:10:41.340]   in a way that's somewhat beneficial because you can measure like a whole wall using AR,
[01:10:41.340 --> 01:10:42.340]   which is kind of nice.
[01:10:42.340 --> 01:10:44.020]   It's actually very useful.
[01:10:44.020 --> 01:10:45.020]   Yeah.
[01:10:45.020 --> 01:10:47.180]   What about any other uses for that?
[01:10:47.180 --> 01:10:50.420]   Is anyone playing any games with AR?
[01:10:50.420 --> 01:10:52.660]   No, no commute.
[01:10:52.660 --> 01:10:55.740]   No, no, no, no, no, I need an iPhone game.
[01:10:55.740 --> 01:10:57.460]   Okay, fair enough.
[01:10:57.460 --> 01:11:04.500]   I do want to talk about something that is okay.
[01:11:04.500 --> 01:11:10.180]   So Apple just recently announced a new service called Apple Pay Later.
[01:11:10.180 --> 01:11:17.820]   And this is a way for people to make a purchase that is split up over payments.
[01:11:17.820 --> 01:11:22.820]   And so in some cases can be allowed to or can help you make a bigger purchase.
[01:11:22.820 --> 01:11:28.940]   So you, you know, $300 split into four payments, no fees, no interest.
[01:11:28.940 --> 01:11:33.620]   And many people I think are familiar with the different services that are out there that
[01:11:33.620 --> 01:11:36.100]   offer similar features.
[01:11:36.100 --> 01:11:40.740]   So you can, there's some where they do charge a fee, but they spread it out over a longer
[01:11:40.740 --> 01:11:42.060]   period of time.
[01:11:42.060 --> 01:11:44.340]   There are some that work just like this.
[01:11:44.340 --> 01:11:49.540]   There are some that are built into kind of like those payment apps that already exist.
[01:11:49.540 --> 01:11:52.420]   And then some companies I've actually made use of Amazon.
[01:11:52.420 --> 01:11:55.420]   I don't know if they still do this because I haven't seen it in a long time.
[01:11:55.420 --> 01:12:00.100]   But when I first moved here and bought some furniture for my living room, I actually made
[01:12:00.100 --> 01:12:06.060]   use of Amazon's payment system where it split up the sofa that is not as cool as a broad
[01:12:06.060 --> 01:12:07.780]   hours into four payments.
[01:12:07.780 --> 01:12:09.980]   And I was able to get that furniture.
[01:12:09.980 --> 01:12:14.940]   But I want to talk first about the service in general, but then I want to get into a
[01:12:14.940 --> 01:12:20.700]   little bit of a discussion about where people are in terms of the responsibility of this
[01:12:20.700 --> 01:12:26.300]   technology because I, what I find interesting about this is how different people from different
[01:12:26.300 --> 01:12:30.300]   financial backgrounds in particular have different thoughts about this.
[01:12:30.300 --> 01:12:35.940]   So first and foremost, if it's something that you care to share, have you ever made use
[01:12:35.940 --> 01:12:40.820]   of a sort of buy now pay later system and has it been beneficial to you?
[01:12:40.820 --> 01:12:48.940]   And then if you have used Apple Pay, are you looking forward to Apple adding this functionality?
[01:12:48.940 --> 01:12:54.940]   And it's up to you who wants to start.
[01:12:54.940 --> 01:12:56.940]   We'll go with a bra.
[01:12:56.940 --> 01:12:57.940]   Okay.
[01:12:57.940 --> 01:12:58.940]   Amazing.
[01:12:58.940 --> 01:13:04.460]   I have not used a buy now pay later system before.
[01:13:04.460 --> 01:13:09.300]   I think one of the things that some financial experts have been cautioning with this and
[01:13:09.300 --> 01:13:14.220]   with other systems is just not putting too much on your plate, right?
[01:13:14.220 --> 01:13:17.420]   So making sure that you're, it's great that these platforms exist.
[01:13:17.420 --> 01:13:19.380]   And I think there are lots of good uses for them.
[01:13:19.380 --> 01:13:24.260]   But I think, you know, one of the points is to just remember that, you know, just to
[01:13:24.260 --> 01:13:29.820]   keep in mind your financial limits and, you know, maybe not necessarily be enticed by
[01:13:29.820 --> 01:13:31.300]   how convenient this may seem.
[01:13:31.300 --> 01:13:35.820]   But this, but I do think that this is a really great convenient feature to just be baked into
[01:13:35.820 --> 01:13:40.020]   Apple Pay because I use Apple Pay as if it's Apple paying for my stuff.
[01:13:40.020 --> 01:13:42.460]   Like it's just like Apple Pay's.
[01:13:42.460 --> 01:13:43.460]   Yeah.
[01:13:43.460 --> 01:13:44.460]   Yeah.
[01:13:44.460 --> 01:13:49.540]   So I think it's, it's super interesting to see this kind of stuff just become a lot more
[01:13:49.540 --> 01:13:52.580]   seamless and built into our devices.
[01:13:52.580 --> 01:13:57.700]   But, but yeah, I think just, I'm always overly cautious about these things.
[01:13:57.700 --> 01:14:02.500]   And I just try to hold myself back if it's not something that I think is, is right for
[01:14:02.500 --> 01:14:05.820]   me to be purchasing at, at a specific point in time.
[01:14:05.820 --> 01:14:06.820]   Nice.
[01:14:06.820 --> 01:14:07.820]   Alex.
[01:14:07.820 --> 01:14:08.980]   So I'm kind of, I'm kind of similar.
[01:14:08.980 --> 01:14:12.340]   I don't, I haven't used any B and Pills services that I'm aware of.
[01:14:12.340 --> 01:14:14.580]   I see them online wherever I go.
[01:14:14.580 --> 01:14:16.780]   They're like, do you want to split your sandwich into four payments?
[01:14:16.780 --> 01:14:17.780]   Right.
[01:14:17.780 --> 01:14:20.620]   And I have to admit that I don't because that's ridiculous.
[01:14:20.620 --> 01:14:22.700]   I also don't really use Apple Pay.
[01:14:22.700 --> 01:14:26.700]   I just use, we just have one credit card, we just have an auto pay just to get miles
[01:14:26.700 --> 01:14:27.700]   for Southwest.
[01:14:27.700 --> 01:14:29.140]   So I just use that.
[01:14:29.140 --> 01:14:31.740]   I feel kind of like a lot right now because I feel like I should be using Apple Pay more
[01:14:31.740 --> 01:14:32.740]   often.
[01:14:32.740 --> 01:14:39.260]   But to me, Apple's move into FinTech in general seems like a search for growth and a bit of
[01:14:39.260 --> 01:14:41.940]   a strange place for the company to be in.
[01:14:41.940 --> 01:14:44.140]   So I'm a little bit skeptical of it.
[01:14:44.140 --> 01:14:48.500]   I didn't like the Apple Card, the Goldman Sachs partnership because it didn't seem to
[01:14:48.500 --> 01:14:52.980]   be at all useful as you bought tons of Apple products, which seems to be just self-serving
[01:14:52.980 --> 01:14:54.820]   the way that was kind of gross.
[01:14:54.820 --> 01:14:57.780]   So to me, when they announced this, the reason why I cared about the B and Pills announcement
[01:14:57.780 --> 01:15:01.940]   from Apple was that the stocks of public B and Pills companies got whacked.
[01:15:01.940 --> 01:15:06.100]   Because everyone was thinking that Apple was going to come in like a 10 ton gorilla or whatever
[01:15:06.100 --> 01:15:08.420]   the phrase is and smash everybody.
[01:15:08.420 --> 01:15:12.260]   And then it just didn't come out until, you know, now we're talking about it again.
[01:15:12.260 --> 01:15:15.500]   And I feel like the moment since then has passed, like the days when Klarna was worth
[01:15:15.500 --> 01:15:20.580]   like $60 billion and square bought afterpay for like $25 billion and all of this, the
[01:15:20.580 --> 01:15:24.820]   valuation of FinTech companies has gone down by like an enormous factor because this stuff
[01:15:24.820 --> 01:15:28.020]   isn't as valuable as a business as people hoped it was.
[01:15:28.020 --> 01:15:30.700]   So Apple doing this feels a little bit late, it feels a little bit modest.
[01:15:30.700 --> 01:15:35.820]   And frankly, I don't think I care very well, Christina.
[01:15:35.820 --> 01:15:36.820]   Yeah.
[01:15:36.820 --> 01:15:42.260]   So I have, it's weird because I do feel like exactly like Alex said, I think he's dead
[01:15:42.260 --> 01:15:47.340]   on that the moment when Klarna and afterpay and the firm and all these companies were
[01:15:47.340 --> 01:15:49.700]   these mass evaluations, that's over.
[01:15:49.700 --> 01:15:53.020]   And part of that is strictly like financial stuff, right?
[01:15:53.020 --> 01:15:58.420]   Like in an era when interest rates were zero, then that those services made complete sense
[01:15:58.420 --> 01:16:02.380]   because you could take on some of the risk, even if you know that a certain portion of
[01:16:02.380 --> 01:16:07.700]   people are going to wind up not paying, you can take on something of that and you can
[01:16:07.700 --> 01:16:13.260]   have lower or even zero interest in some of those cases, offers.
[01:16:13.260 --> 01:16:15.140]   But interest rates are not zero anymore.
[01:16:15.140 --> 01:16:20.580]   And so, and you know, lending has become a lot more stringent.
[01:16:20.580 --> 01:16:25.580]   So the timing does feel off, but it also feels like maybe they have to launch it just
[01:16:25.580 --> 01:16:30.460]   because it's done finally and they have obligations to their friends at Goldman Sachs and whoever
[01:16:30.460 --> 01:16:33.300]   they partnered with on this to do it or not.
[01:16:33.300 --> 01:16:37.220]   I tend to not use by now, Paylater services.
[01:16:37.220 --> 01:16:41.860]   I have a few times, it was literally a zero percent interest and they were like, for six
[01:16:41.860 --> 01:16:42.860]   months, you can do this.
[01:16:42.860 --> 01:16:47.980]   And I'm like, well, if you're not going to charge me, I can make more money, having
[01:16:47.980 --> 01:16:50.860]   money in other places than paying it all at once, right?
[01:16:50.860 --> 01:16:52.180]   So I'm not opposed to that.
[01:16:52.180 --> 01:16:54.380]   But in general, I don't like them.
[01:16:54.380 --> 01:16:57.820]   I find them predatory if I'm being completely honest.
[01:16:57.820 --> 01:17:01.540]   There's very much of a payday loan type of smell with this stuff.
[01:17:01.540 --> 01:17:07.940]   I don't like it when Microsoft, my former employer is putting these things into Microsoft Edge.
[01:17:07.940 --> 01:17:10.660]   I don't like it when Apple is getting into this area.
[01:17:10.660 --> 01:17:13.900]   It feels, it feels gross.
[01:17:13.900 --> 01:17:19.100]   That said, you know, if people can genuinely find benefit out of it and people who might
[01:17:19.100 --> 01:17:26.500]   have otherwise difficult time getting access to credit and some other things maybe, but
[01:17:26.500 --> 01:17:29.900]   in general, I'm not a huge fan of these services.
[01:17:29.900 --> 01:17:33.540]   And I tend to think that they can get people in a lot of trouble really quickly because
[01:17:33.540 --> 01:17:36.740]   people don't understand how compound interest works and they understand what the terms that
[01:17:36.740 --> 01:17:38.980]   they're necessarily signing up for are.
[01:17:38.980 --> 01:17:41.620]   And yeah.
[01:17:41.620 --> 01:17:47.140]   I think that's the way that you put that is you put it very well.
[01:17:47.140 --> 01:17:53.020]   I think with the zero percent interest and with the no fees and as long as that remains
[01:17:53.020 --> 01:17:57.980]   true, I feel a little bit better about those systems because.
[01:17:57.980 --> 01:18:09.420]   So I come from a pretty poor growing up family and even to this day, a lot of financial struggles
[01:18:09.420 --> 01:18:11.500]   in my family.
[01:18:11.500 --> 01:18:19.860]   And so I have seen for a few family members who were in situations where they had someone
[01:18:19.860 --> 01:18:25.580]   pass away or something like that and they needed to go to a funeral that was in a few
[01:18:25.580 --> 01:18:27.500]   states over.
[01:18:27.500 --> 01:18:30.940]   They didn't have the money on hand to be able to afford the plane ticket.
[01:18:30.940 --> 01:18:35.460]   And so being able to use one of these kinds of services was the difference between being
[01:18:35.460 --> 01:18:36.900]   able to go there or not.
[01:18:36.900 --> 01:18:41.060]   And because it was zero interest and yada yada, then it was something that was doable
[01:18:41.060 --> 01:18:44.740]   for them where they were basically just paying back the plane ticket over time where they
[01:18:44.740 --> 01:18:46.660]   could afford that more.
[01:18:46.660 --> 01:18:52.980]   And in those specific instances or like when I first moved here and was adjusting to California
[01:18:52.980 --> 01:18:57.420]   rent versus Missouri rent and everything was involved with that and having to just spend
[01:18:57.420 --> 01:19:03.620]   money to move halfway across the country, it was beneficial to me as a zero percent interest
[01:19:03.620 --> 01:19:07.820]   and no fees situation to be able to spread that out.
[01:19:07.820 --> 01:19:12.620]   And so I'm always mindful of that in the back of my head in the same place where I've
[01:19:12.620 --> 01:19:17.020]   got the other side that's going also don't let this become predatory because I don't ever
[01:19:17.020 --> 01:19:21.660]   want it to be the sort of a leadest perspective because there are sometimes where it almost
[01:19:21.660 --> 01:19:28.500]   feels like when someone gets upset that like a person who doesn't make very much money
[01:19:28.500 --> 01:19:32.380]   who maybe needs financial help from the government.
[01:19:32.380 --> 01:19:35.660]   So food stamps, for example, or EBT, I guess they call it.
[01:19:35.660 --> 01:19:41.220]   They have EBT and then you learn, oh, but you like pay a monthly subscription for Netflix.
[01:19:41.220 --> 01:19:44.060]   If you are on EBT, then you shouldn't have Netflix.
[01:19:44.060 --> 01:19:46.580]   Like, you know, I don't ever want it to feel like that.
[01:19:46.580 --> 01:19:50.700]   That sort of elitist person who doesn't make enough money can't ever have anything that
[01:19:50.700 --> 01:19:55.060]   is a comfort item or can't, you know, make them happier, something that they need.
[01:19:55.060 --> 01:20:00.300]   It is as long as it remains in that place, then I think that it is a benefit overall,
[01:20:00.300 --> 01:20:04.260]   especially as things keep getting more and more expensive.
[01:20:04.260 --> 01:20:08.980]   But as everyone's saying here, I think the word that everybody's using that the term
[01:20:08.980 --> 01:20:14.860]   is when it becomes predatory and, you know, I have seen a family member who went down the
[01:20:14.860 --> 01:20:15.860]   route.
[01:20:15.860 --> 01:20:22.980]   I couldn't even, I didn't even know this existed, but they had found an app that was practically,
[01:20:22.980 --> 01:20:27.580]   like you were saying, Christina, a payday loan app and it had just ridiculous interest
[01:20:27.580 --> 01:20:31.500]   rates, but they weren't thinking about that when they did it and it, you know, ended up
[01:20:31.500 --> 01:20:33.740]   costing them so much money.
[01:20:33.740 --> 01:20:38.340]   So when it's that, that is very, very, very, very, very bad.
[01:20:38.340 --> 01:20:45.180]   As a means of maybe democratizing access in a non-predatory way, that is where I feel
[01:20:45.180 --> 01:20:49.780]   like I'm trying to be more open-minded to it.
[01:20:49.780 --> 01:20:50.780]   So yeah, I don't know.
[01:20:50.780 --> 01:20:54.820]   I'm keeping an eye on things though, because I don't know what it looks like if a person
[01:20:54.820 --> 01:21:00.140]   doesn't end up paying for one month, then does interest then kick in and fees kick in?
[01:21:00.140 --> 01:21:03.980]   Is it only fee-free and interest-free as long as you're making the payments?
[01:21:03.980 --> 01:21:06.340]   I mean, those different services all have different terms.
[01:21:06.340 --> 01:21:08.860]   So yeah, something to keep an eye on.
[01:21:08.860 --> 01:21:09.860]   Go ahead.
[01:21:09.860 --> 01:21:11.700]   I think all that was incredibly well put.
[01:21:11.700 --> 01:21:15.140]   And when I said that I don't care about it, I was more trying to reference the passing moment
[01:21:15.140 --> 01:21:17.340]   versus it not being an important part of the conversation.
[01:21:17.340 --> 01:21:21.540]   So I just want to clear up that I was being dismissive of the timing versus the conversation
[01:21:21.540 --> 01:21:24.020]   that we're having about keeping things on the up and up.
[01:21:24.020 --> 01:21:26.780]   But to put it in perspective about how much time has fastened, this was, I think, kind
[01:21:26.780 --> 01:21:31.100]   of on the center, on the come-up, if you will.
[01:21:31.100 --> 01:21:35.580]   I bought a peloton during the pandemic because apparently that's how life goes.
[01:21:35.580 --> 01:21:37.900]   And I paid for it when I bought it.
[01:21:37.900 --> 01:21:40.580]   And everyone kind of was made fun of me on Twitter for this.
[01:21:40.580 --> 01:21:42.820]   And they were like, "Oh, you should have just put it on a firm because then you could
[01:21:42.820 --> 01:21:45.060]   get zero percent interest for X number of months."
[01:21:45.060 --> 01:21:47.580]   And I was just like, "I don't want to have an affirmative account.
[01:21:47.580 --> 01:21:49.020]   I don't want to deal with that.
[01:21:49.020 --> 01:21:50.980]   I just wanted to buy a peloton."
[01:21:50.980 --> 01:21:51.980]   So I did.
[01:21:51.980 --> 01:21:52.980]   It was weird.
[01:21:52.980 --> 01:21:58.020]   Everyone was almost like money shaming me about how I didn't collect no interest on
[01:21:58.020 --> 01:21:59.020]   that money.
[01:21:59.020 --> 01:22:00.020]   Weird.
[01:22:00.020 --> 01:22:02.500]   What's so funny is the whole reason I have an affirmative account, which I didn't use,
[01:22:02.500 --> 01:22:05.460]   is I was going to get my parents a peloton.
[01:22:05.460 --> 01:22:08.300]   And I signed up just to see, "Well, what would they cover it?"
[01:22:08.300 --> 01:22:10.700]   And they were like, "Yeah, we'll cover the whole thing, interest free."
[01:22:10.700 --> 01:22:16.020]   And then my mom told me under no expression, she was like, "We do not want a peloton."
[01:22:16.020 --> 01:22:17.980]   Please don't send that to our house, please.
[01:22:17.980 --> 01:22:20.100]   She was like, "We do not want a peloton."
[01:22:20.100 --> 01:22:22.820]   She was like, "I have to have a certain type of bike for my knees and the peloton is
[01:22:22.820 --> 01:22:23.820]   not the type of bike.
[01:22:23.820 --> 01:22:26.500]   And so do not buy us a peloton."
[01:22:26.500 --> 01:22:28.220]   So I was like, "All right, well, I'm not getting a peloton.
[01:22:28.220 --> 01:22:29.660]   I don't have room for a peloton.
[01:22:29.660 --> 01:22:30.660]   And I won't use it.
[01:22:30.660 --> 01:22:34.260]   So I have an affirmative account that I've never used anything on, but I do still get
[01:22:34.260 --> 01:22:38.500]   their emails from that.
[01:22:38.500 --> 01:22:39.500]   That's funny.
[01:22:39.500 --> 01:22:40.500]   That is interesting.
[01:22:40.500 --> 01:22:45.060]   But I will say that the era in which a firm was growing before a wind public, when peloton
[01:22:45.060 --> 01:22:49.380]   was blowing up and they were the hottest thing in the world, that now feels like 74 years
[01:22:49.380 --> 01:22:50.380]   ago.
[01:22:50.380 --> 01:22:51.380]   It really does.
[01:22:51.380 --> 01:22:52.380]   It really does.
[01:22:52.380 --> 01:22:53.380]   And it was, right?
[01:22:53.380 --> 01:22:54.380]   Because it was pre-pandemic.
[01:22:54.380 --> 01:22:57.460]   It was, you know, and then pandemic a little bit too.
[01:22:57.460 --> 01:23:02.860]   But the interest rates were zero and money was just free-flying.
[01:23:02.860 --> 01:23:04.300]   And that is not the case anymore.
[01:23:04.300 --> 01:23:09.220]   And so, yeah, to your point, I don't know what the rules are if you don't make a payment.
[01:23:09.220 --> 01:23:12.460]   I mean, I think usually the principal is due, but I don't know if interest starts or what
[01:23:12.460 --> 01:23:13.740]   the circumstances are.
[01:23:13.740 --> 01:23:18.940]   And I don't know the size of the loans, but I do become concerned because I want these
[01:23:18.940 --> 01:23:22.940]   options to be available to people because like you were saying, "Michael, if it's the
[01:23:22.940 --> 01:23:26.700]   difference between someone being able to go to a funeral or not, then that should exist."
[01:23:26.700 --> 01:23:31.100]   And I definitely don't want to shame anybody for, "I don't care what your circumstances
[01:23:31.100 --> 01:23:32.100]   are.
[01:23:32.100 --> 01:23:34.100]   You still deserve to have Netflix or anything else."
[01:23:34.100 --> 01:23:39.100]   But I do worry, it's like these things can become incredibly predatory.
[01:23:39.100 --> 01:23:42.660]   And in the way that they make it look, you know, we see it the same way that you see
[01:23:42.660 --> 01:23:45.980]   with in-app purchase stuff where they will show you like the, "Oh, it's only going to
[01:23:45.980 --> 01:23:46.980]   cost this much a week."
[01:23:46.980 --> 01:23:47.980]   And you don't see the weak part.
[01:23:47.980 --> 01:23:50.500]   And you make it look like dark patterns.
[01:23:50.500 --> 01:23:51.500]   It's totally dark patterns.
[01:23:51.500 --> 01:23:53.940]   And so I become really concerned with this.
[01:23:53.940 --> 01:23:57.980]   And I become concerned with this from any company that is looking at this as a revenue stream
[01:23:57.980 --> 01:23:59.780]   and as a growth engine.
[01:23:59.780 --> 01:24:06.100]   And I express this concerns to the integration of Microsoft Edge when I was at Microsoft.
[01:24:06.100 --> 01:24:10.340]   And I feel the same way about this stuff at Apple.
[01:24:10.340 --> 01:24:16.460]   I'm not, in general, I'm not a big fan, but I also do appreciate why these systems exist
[01:24:16.460 --> 01:24:20.740]   because not everyone wants a credit card or can't have a credit card or whatever their
[01:24:20.740 --> 01:24:21.740]   circumstances are.
[01:24:21.740 --> 01:24:22.740]   Yeah.
[01:24:22.740 --> 01:24:23.740]   Yes.
[01:24:23.740 --> 01:24:28.380]   But if you are annoyed at the cost of Apple's AR/VR/Doohickey costing $3,000, and Apple has
[01:24:28.380 --> 01:24:33.380]   a brand new BNPL system, one plus one equals three.
[01:24:33.380 --> 01:24:34.380]   Oh my God.
[01:24:34.380 --> 01:24:38.620]   No, you're exactly right though, because I do use as the one thing I use my Apple card
[01:24:38.620 --> 01:24:42.900]   for is primarily just for Apple purchases because I can get the interest free.
[01:24:42.900 --> 01:24:46.580]   And in my mind, I'm like, "Well, yeah, I could pay for this immediately, but why not
[01:24:46.580 --> 01:24:48.540]   put it over 24 months?
[01:24:48.540 --> 01:24:49.540]   Why not?"
[01:24:49.540 --> 01:24:50.540]   Yeah.
[01:24:50.540 --> 01:24:51.860]   Now the money has an actual value to it.
[01:24:51.860 --> 01:24:53.540]   Put that in your money market account at 4%.
[01:24:53.540 --> 01:24:54.980]   And you'll make 73 cents.
[01:24:54.980 --> 01:24:55.980]   Good job.
[01:24:55.980 --> 01:24:59.580]   That's 73 cents I can spend on a gumball.
[01:24:59.580 --> 01:25:00.580]   Maybe.
[01:25:00.580 --> 01:25:02.380]   Yeah, exactly.
[01:25:02.380 --> 01:25:03.380]   Who knows?
[01:25:03.380 --> 01:25:07.460]   All right, let's take a quick break before we come back with loads more.
[01:25:07.460 --> 01:25:09.340]   I've got a great panel today.
[01:25:09.340 --> 01:25:11.380]   Abraar Al-Hiti from CNET.
[01:25:11.380 --> 01:25:18.860]   I've got Alex Wilhelm from TechCrunch and Christina Warren, aka FilmGirl, senior developer advocate
[01:25:18.860 --> 01:25:20.660]   at GitHub.
[01:25:20.660 --> 01:25:29.780]   This week in Tech is brought to you by ExpressVPN, which is genuinely the VPN I use and have
[01:25:29.780 --> 01:25:33.020]   used for years.
[01:25:33.020 --> 01:25:37.260]   You probably already know about ExpressVPN because we've talked a lot about it on this
[01:25:37.260 --> 01:25:39.060]   show.
[01:25:39.060 --> 01:25:44.660]   And because many of you are probably have used it or are using it, and that's because
[01:25:44.660 --> 01:25:46.500]   it's so great.
[01:25:46.500 --> 01:25:52.340]   It's so important that you are protecting yourself online, and there are a number of
[01:25:52.340 --> 01:25:53.340]   ways to do so.
[01:25:53.340 --> 01:25:58.940]   But I think sometimes we might forget about the ways we need to protect ourselves in terms
[01:25:58.940 --> 01:26:06.420]   of our browsing online and the protection that we need to have from our ISPs, in some
[01:26:06.420 --> 01:26:13.580]   cases, your internet service provider, and even companies like big tech companies.
[01:26:13.580 --> 01:26:23.260]   But there are other reasons why you and I like using ExpressVPN, and that's whenever
[01:26:23.260 --> 01:26:28.260]   we're streaming content, like content from Netflix, because there are thousands of shows
[01:26:28.260 --> 01:26:32.980]   on Netflix that are only available outside of the US.
[01:26:32.980 --> 01:26:37.780]   So you can use a VPN to change your country if you'd like to access them.
[01:26:37.780 --> 01:26:39.540]   You are appearing from somewhere else.
[01:26:39.540 --> 01:26:41.820]   You're traveling without having to travel.
[01:26:41.820 --> 01:26:43.780]   ExpressVPN lets you do that.
[01:26:43.780 --> 01:26:47.860]   What the ExpressVPN app does is it essentially encrypts your data and reroutes it through
[01:26:47.860 --> 01:26:51.500]   a server location in your place of choice.
[01:26:51.500 --> 01:26:55.300]   Not only is this going to be protecting the browsing that you're doing online, but it
[01:26:55.300 --> 01:27:01.740]   also lets you control which country you want a service like Netflix to think you're in.
[01:27:01.740 --> 01:27:04.860]   ExpressVPN lets you choose from more than 90 countries.
[01:27:04.860 --> 01:27:09.580]   So every time you run out of stuff to watch, just fire up the app on your laptop, your
[01:27:09.580 --> 01:27:12.340]   smart TV, switch your country and hit connect.
[01:27:12.340 --> 01:27:15.420]   Once you refresh the page, you're going to brand new selection of shows.
[01:27:15.420 --> 01:27:16.900]   It's that simple.
[01:27:16.900 --> 01:27:20.620]   And this is honestly the best part, and I will be honest.
[01:27:20.620 --> 01:27:27.100]   It's one of the means of my enjoyment online is it's not just for Netflix.
[01:27:27.100 --> 01:27:32.860]   You can use ExpressVPN to unlock shows or sports on other streaming services too.
[01:27:32.860 --> 01:27:40.860]   One example is that there is an Australian based streaming service called 10 play and
[01:27:40.860 --> 01:27:47.140]   10 play has the most incredible reality TV show on it called Australian survivor.
[01:27:47.140 --> 01:27:51.060]   Yes, there's American survivor, but forget about American survivor.
[01:27:51.060 --> 01:27:54.620]   Australian survivor is a thousand times better.
[01:27:54.620 --> 01:28:00.420]   Unfortunately, it used to be that you could watch it in the United States via I don't
[01:28:00.420 --> 01:28:01.540]   remember what service.
[01:28:01.540 --> 01:28:03.140]   But at one point it was there.
[01:28:03.140 --> 01:28:09.020]   And then I kid you not on February 14th on Valentine's Day at 5 p.m.
[01:28:09.020 --> 01:28:13.100]   Their license to I'm going to get mad their license to stream it ran out.
[01:28:13.100 --> 01:28:19.260]   And instead of letting me literally finish the finale episode of the show, they just
[01:28:19.260 --> 01:28:24.220]   stopped letting me watch it three quarters of the way through the episode.
[01:28:24.220 --> 01:28:26.100]   So I fired up ExpressVPN.
[01:28:26.100 --> 01:28:31.380]   I hopped on over to 10 play and suddenly I was able to finish the episode and continue
[01:28:31.380 --> 01:28:34.020]   the wonderful experience that is Australian survivor.
[01:28:34.020 --> 01:28:35.020]   I'm not kidding.
[01:28:35.020 --> 01:28:37.740]   If any of you like survivor, you've got to see Australian survivor.
[01:28:37.740 --> 01:28:39.460]   It's so much better.
[01:28:39.460 --> 01:28:42.140]   ExpressVPN is rated number one by CNET.
[01:28:42.140 --> 01:28:46.500]   You may have heard of them wired tech radar and countless other services.
[01:28:46.500 --> 01:28:47.820]   And you know what?
[01:28:47.820 --> 01:28:49.180]   It's rated number one by me.
[01:28:49.180 --> 01:28:52.300]   I use ExpressVPN on all of my devices.
[01:28:52.300 --> 01:28:58.020]   And as much as they are a sponsor of the network, but the thing that I feel sets ExpressVPN
[01:28:58.020 --> 01:29:02.020]   apart is sometimes people tell you they went through the privacy policies.
[01:29:02.020 --> 01:29:05.260]   They went through the terms of service and maybe they skimmed it.
[01:29:05.260 --> 01:29:10.660]   No, I wanted to be sure that what I once believed about ExpressVPN to be true was still
[01:29:10.660 --> 01:29:11.660]   true.
[01:29:11.660 --> 01:29:17.980]   And so I did so much research making sure that you know, because ExpressVPN was acquired
[01:29:17.980 --> 01:29:21.100]   by a company and there was some like, oh, you know what's going on?
[01:29:21.100 --> 01:29:23.020]   I looked through privacy policies.
[01:29:23.020 --> 01:29:25.620]   I read independent reports.
[01:29:25.620 --> 01:29:31.220]   In fact, one of the cool things about ExpressVPN, there was one period of time where there
[01:29:31.220 --> 01:29:36.460]   was a government who went and tried to seize ExpressVPN's assets and their servers and
[01:29:36.460 --> 01:29:37.460]   everything.
[01:29:37.460 --> 01:29:45.300]   But because ExpressVPN runs all of these servers on flash memory instead of running it on hard
[01:29:45.300 --> 01:29:47.660]   drives, then it's volatile memory.
[01:29:47.660 --> 01:29:51.340]   So that means that whenever it powers off, all that stuff is gone.
[01:29:51.340 --> 01:29:52.420]   And that is how they do it.
[01:29:52.420 --> 01:29:58.140]   They make sure that after it's done, after, you know, it's time to sort of refresh, everything
[01:29:58.140 --> 01:29:59.140]   goes away.
[01:29:59.140 --> 01:30:02.340]   So the government tried to seize their assets and they're like, we don't have anything
[01:30:02.340 --> 01:30:03.340]   for you.
[01:30:03.340 --> 01:30:04.340]   I mean, you can come in and look.
[01:30:04.340 --> 01:30:05.700]   There wasn't anything for them to have.
[01:30:05.700 --> 01:30:06.700]   So they're not collecting.
[01:30:06.700 --> 01:30:07.700]   They're not logging.
[01:30:07.700 --> 01:30:08.700]   They're not doing that.
[01:30:08.700 --> 01:30:11.940]   And when ExpressVPN says that, they actually do mean it.
[01:30:11.940 --> 01:30:17.640]   And it holds up because despite just independent auditors who have confirmed it, I think the
[01:30:17.640 --> 01:30:22.380]   best way to find out if a company is truly not logging your data is to have the government
[01:30:22.380 --> 01:30:25.940]   of the country suddenly come knocking when they weren't planning on it.
[01:30:25.940 --> 01:30:28.100]   And then they look and they can't get anything either.
[01:30:28.100 --> 01:30:34.340]   So if you're looking for a VPN to trust, look no further, expressvpn.com/twit.
[01:30:34.340 --> 01:30:38.540]   Head there right now and get three extra months free of their service.
[01:30:38.540 --> 01:30:44.140]   That's eXPRESVPN.com/twit.
[01:30:44.140 --> 01:30:45.140]   TWIT.
[01:30:45.140 --> 01:30:47.900]   ExpressVPN.com/twit.
[01:30:47.900 --> 01:30:52.340]   To learn more and sign up, thank you ExpressVPN for sponsoring this.
[01:30:52.340 --> 01:30:57.460]   Weeks episode of this week in tech.
[01:30:57.460 --> 01:30:59.500]   All right, back from the break.
[01:30:59.500 --> 01:31:05.260]   And now it is time to move along home.
[01:31:05.260 --> 01:31:10.180]   Let's talk about e3.
[01:31:10.180 --> 01:31:13.940]   So I know, I know it's sad.
[01:31:13.940 --> 01:31:16.220]   e3 is canceled.
[01:31:16.220 --> 01:31:17.220]   Why?
[01:31:17.220 --> 01:31:19.220]   What's going on?
[01:31:19.220 --> 01:31:21.420]   All right, better question.
[01:31:21.420 --> 01:31:23.860]   e3 is canceled, sad.
[01:31:23.860 --> 01:31:26.340]   And somehow CES is still not.
[01:31:26.340 --> 01:31:28.340]   What's up with that?
[01:31:28.340 --> 01:31:32.260]   Well, one of them is way better at selling booths than the other.
[01:31:32.260 --> 01:31:37.740]   Oh, so you think this is like a marketing sort of business development issue rather
[01:31:37.740 --> 01:31:42.500]   than just the big companies not wanting to participate?
[01:31:42.500 --> 01:31:44.780]   Well, that's the same thing.
[01:31:44.780 --> 01:31:45.780]   Fair enough.
[01:31:45.780 --> 01:31:46.780]   Fair enough.
[01:31:46.780 --> 01:31:47.780]   Yeah, that's true.
[01:31:47.780 --> 01:31:48.780]   Right, right.
[01:31:48.780 --> 01:31:51.700]   You can see, yes, it's seen the same thing where a lot of the big companies have pulled
[01:31:51.700 --> 01:31:52.700]   out, right?
[01:31:52.700 --> 01:31:56.460]   Like Microsoft used to always have a big booth there and an Intel and some of the other
[01:31:56.460 --> 01:31:58.540]   things that have their evening keynotes.
[01:31:58.540 --> 01:32:03.180]   And you still have some of that, but most of the biggest tech companies don't have the
[01:32:03.180 --> 01:32:05.060]   big keynotes they used to.
[01:32:05.060 --> 01:32:07.820]   But they were being able to effectively bend.
[01:32:07.820 --> 01:32:11.220]   They went to car companies because car tech is a big part of it.
[01:32:11.220 --> 01:32:14.940]   And so they were able to get all the car manufacturers there, turn it into a car show.
[01:32:14.940 --> 01:32:18.740]   They're able to go to a lot of the Chinese manufacturers and other parts of Asia.
[01:32:18.740 --> 01:32:20.700]   And bring them in.
[01:32:20.700 --> 01:32:25.460]   So I think they were still able to sell booth sales, whereas E3, if you don't have Microsoft,
[01:32:25.460 --> 01:32:29.620]   you don't have Sony, you don't have Nintendo.
[01:32:29.620 --> 01:32:35.420]   There's very little reason for a lot of the studios and publishers to show up, especially
[01:32:35.420 --> 01:32:40.620]   when you also have the fan conferences that have been incredibly successful, where they
[01:32:40.620 --> 01:32:47.500]   can get more hands on, you know, do more of that, you know, that level of marketing push
[01:32:47.500 --> 01:32:51.740]   than what they used to get from E3.
[01:32:51.740 --> 01:32:52.740]   That starts to be correct.
[01:32:52.740 --> 01:32:56.180]   But also, you know, when they think about game distribution and where you get your gaming
[01:32:56.180 --> 01:33:01.540]   information, I mean, Steam recently had an event called City Builder Faster, something
[01:33:01.540 --> 01:33:02.540]   like that.
[01:33:02.540 --> 01:33:04.500]   And I love city building video games.
[01:33:04.500 --> 01:33:08.500]   And I was like, Oh my gosh, there's literally a tailored event just for me.
[01:33:08.500 --> 01:33:12.620]   And they worked with any developers, put out demos, early access for certain games.
[01:33:12.620 --> 01:33:16.700]   And even some really small devs got to partake in this thing that at E3, they would have
[01:33:16.700 --> 01:33:20.540]   been absolutely marginalized because it was tailored and online.
[01:33:20.540 --> 01:33:21.540]   It was possible.
[01:33:21.540 --> 01:33:26.820]   Another example, Paradox, which makes Crusader Kings and EU and all those kind of grand strategy
[01:33:26.820 --> 01:33:29.660]   games, they just have Paradox Con or something like that.
[01:33:29.660 --> 01:33:30.660]   It's their own thing.
[01:33:30.660 --> 01:33:31.660]   They don't need to come to E3.
[01:33:31.660 --> 01:33:34.500]   It's a bit like how the Grammys don't matter anymore.
[01:33:34.500 --> 01:33:35.500]   Sorry.
[01:33:35.500 --> 01:33:36.500]   Kind of.
[01:33:36.500 --> 01:33:37.500]   Yeah.
[01:33:37.500 --> 01:33:38.500]   No, you're dead on it.
[01:33:38.500 --> 01:33:39.500]   But then you have things like packs, right, which can be more generalist.
[01:33:39.500 --> 01:33:42.820]   And you have things like TwitchCon, which frankly is a little bit like E3 in some
[01:33:42.820 --> 01:33:46.100]   instances because you will have publishers there and you have personalities who are
[01:33:46.100 --> 01:33:49.340]   behind the stuff that people want to do.
[01:33:49.340 --> 01:33:53.740]   And but I mean, I think it started with the big tech companies.
[01:33:53.740 --> 01:33:58.620]   Apple's never done CES, but they would have their own direct events.
[01:33:58.620 --> 01:34:02.020]   But when they left Mackerels, right, like they used to have their keynote as part of
[01:34:02.020 --> 01:34:03.020]   Mackerels.
[01:34:03.020 --> 01:34:04.940]   And when Apple was like, we're not going to be part of Mackerels anymore, we're just
[01:34:04.940 --> 01:34:06.420]   going to go direct.
[01:34:06.420 --> 01:34:09.060]   And then the Nintendo Direct is a thing.
[01:34:09.060 --> 01:34:14.780]   And Sony and Microsoft, Xbox have their things.
[01:34:14.780 --> 01:34:18.860]   There's not a reason to have everybody at this one industry conference, especially
[01:34:18.860 --> 01:34:24.260]   if you can have the either online or in person indie ones like you're talking about around
[01:34:24.260 --> 01:34:28.740]   a certain genre or people find other ways to get their information.
[01:34:28.740 --> 01:34:31.860]   It's sad, but I'm also it's kind of one of those things.
[01:34:31.860 --> 01:34:35.500]   I'm like, well, this has been coming for a really long time.
[01:34:35.500 --> 01:34:42.100]   And what do we think is sort of the is it that there are fewer people going?
[01:34:42.100 --> 01:34:44.700]   Is it that you get more control if you do it yourself?
[01:34:44.700 --> 01:34:46.620]   Is it that it costs less money?
[01:34:46.620 --> 01:34:52.540]   If you I mean, I'm sure it's an amalgamation, but what sort of started the trend of just
[01:34:52.540 --> 01:34:56.940]   getting companies do we think of just getting companies to sort of go, we'll just do that.
[01:34:56.940 --> 01:34:58.780]   We don't I don't need you three.
[01:34:58.780 --> 01:35:01.660]   Yeah, I think I think it was the control aspect.
[01:35:01.660 --> 01:35:04.820]   I think it went from like, okay, why do I have to deal with this third party to have
[01:35:04.820 --> 01:35:11.780]   to talk about what does like what is my time slots and what is, you know, how big is my
[01:35:11.780 --> 01:35:15.340]   you know, booth or other areas going to be or what can I show or what can I not and who
[01:35:15.340 --> 01:35:16.340]   can I allow here?
[01:35:16.340 --> 01:35:22.140]   I have to think that they just kind of realize, well, we have, you know, trust that we have
[01:35:22.140 --> 01:35:23.940]   the direct to audience experience.
[01:35:23.940 --> 01:35:27.340]   I think social media was honestly also changed it too, right?
[01:35:27.340 --> 01:35:32.180]   Because it was one thing when like when I was growing up in the 90s and reading about
[01:35:32.180 --> 01:35:37.060]   E3 and like dreaming of going, you would read about it like in the magazines like a month
[01:35:37.060 --> 01:35:39.060]   after it happened.
[01:35:39.060 --> 01:35:43.820]   And then you know, you had websites like IGN and some others that started up and they
[01:35:43.820 --> 01:35:48.300]   would maybe give you information like the next day or the following week.
[01:35:48.300 --> 01:35:53.420]   But now it's at the point, you know, where you can literally watch the presentations.
[01:35:53.420 --> 01:35:54.660]   You don't want to miss happening in E3.
[01:35:54.660 --> 01:35:56.020]   You could watch it as it happened.
[01:35:56.020 --> 01:35:57.020]   It's happening on Twitter.
[01:35:57.020 --> 01:35:59.420]   It's happening on other forms of social media.
[01:35:59.420 --> 01:36:02.220]   So you no longer have that weight.
[01:36:02.220 --> 01:36:07.500]   And so at that point, I think the companies realize, oh, we actually have these audiences
[01:36:07.500 --> 01:36:09.140]   that are just tuned into us.
[01:36:09.140 --> 01:36:12.860]   We don't have to have the benefit of the trade organization.
[01:36:12.860 --> 01:36:15.860]   And then the press that would all come to that thing to write about it.
[01:36:15.860 --> 01:36:21.140]   Like we can literally just go directly to our gamers.
[01:36:21.140 --> 01:36:22.900]   And so I think that's a social media thing.
[01:36:22.900 --> 01:36:23.900]   It used to.
[01:36:23.900 --> 01:36:24.900]   Yeah, democratization, right?
[01:36:24.900 --> 01:36:29.340]   I used to hide copies of PC gamer in my bedroom because I was banned from playing video
[01:36:29.340 --> 01:36:34.860]   games as a kid because my parents were more conservative religious perhaps than I've ended
[01:36:34.860 --> 01:36:35.860]   up.
[01:36:35.860 --> 01:36:39.980]   And I would read about Doom three and these games that I couldn't wait to play later
[01:36:39.980 --> 01:36:42.700]   on when I was, you know, on my own.
[01:36:42.700 --> 01:36:46.300]   And I cared about their view because they were the industry source.
[01:36:46.300 --> 01:36:48.260]   Now what do I care about?
[01:36:48.260 --> 01:36:49.260]   Not reviews, really.
[01:36:49.260 --> 01:36:51.780]   I just go to steam and look at the recent review rating.
[01:36:51.780 --> 01:36:52.780]   Is it overwhelmingly positive?
[01:36:52.780 --> 01:36:53.780]   Is it very positive?
[01:36:53.780 --> 01:36:54.780]   Is it mixed?
[01:36:54.780 --> 01:36:58.740]   And frankly, I know I can go read those reviews, figure out the apply to me or not.
[01:36:58.740 --> 01:37:01.180]   And very quickly, I don't need the gatekeepers.
[01:37:01.180 --> 01:37:04.500]   And this is why the Grammys, the Emmys, the blah, blah, blah, and the other two HIKI's
[01:37:04.500 --> 01:37:08.060]   and the awards stuff that people will still pretend to care about are losing their relevance
[01:37:08.060 --> 01:37:11.740]   because they just don't have the grip on the culture that they used to.
[01:37:11.740 --> 01:37:16.100]   And this is why I think Taylor Swift can put out her own music and have a good number
[01:37:16.100 --> 01:37:17.180]   one around the world overnight.
[01:37:17.180 --> 01:37:23.380]   She doesn't need a gatekeeper and E3, CES and et cetera are essentially byproducts of a
[01:37:23.380 --> 01:37:26.020]   bygone, more centralized information era.
[01:37:26.020 --> 01:37:27.420]   The internet killed them slowly.
[01:37:27.420 --> 01:37:28.420]   Wow.
[01:37:28.420 --> 01:37:30.420]   I absolutely love that you mentioned Taylor Swift twice.
[01:37:30.420 --> 01:37:31.420]   I was going to say.
[01:37:31.420 --> 01:37:32.420]   Me too.
[01:37:32.420 --> 01:37:33.420]   Me too.
[01:37:33.420 --> 01:37:34.420]   I was like, hell yeah.
[01:37:34.420 --> 01:37:35.420]   This is a panel.
[01:37:35.420 --> 01:37:38.020]   I don't quite include myself in that.
[01:37:38.020 --> 01:37:39.460]   So I'm just going to pull back.
[01:37:39.460 --> 01:37:43.140]   This is the Swiftie panel and I'm realizing now I put together the Swiftie panel and I
[01:37:43.140 --> 01:37:44.140]   didn't realize that's what happened.
[01:37:44.140 --> 01:37:46.620]   I'm so proud of all of us.
[01:37:46.620 --> 01:37:51.180]   I've been on with Christina before a couple of times and I just know Christina for a long
[01:37:51.180 --> 01:37:52.180]   time.
[01:37:52.180 --> 01:37:56.340]   But, Abraar, I went to your Twitter profile before and I saw that you had just mentioned
[01:37:56.340 --> 01:37:57.340]   there.
[01:37:57.340 --> 01:38:02.100]   I'm so vigilant because I think antihero acoustic version is a high piece of modern
[01:38:02.100 --> 01:38:03.100]   art.
[01:38:03.100 --> 01:38:04.100]   Yes.
[01:38:04.100 --> 01:38:07.740]   And my baby loves it because often she won't sleep.
[01:38:07.740 --> 01:38:11.420]   And so what she gets is that on repeat with me trying to harmonize.
[01:38:11.420 --> 01:38:13.780]   So it's been a smash hit at our house.
[01:38:13.780 --> 01:38:14.780]   Aww.
[01:38:14.780 --> 01:38:15.780]   I love that.
[01:38:15.780 --> 01:38:16.780]   I'm so happy to be on this panel with you guys.
[01:38:16.780 --> 01:38:17.780]   Good choice, Michael.
[01:38:17.780 --> 01:38:18.780]   You'd be great.
[01:38:18.780 --> 01:38:21.260]   I got to have to say.
[01:38:21.260 --> 01:38:22.980]   Oh my God.
[01:38:22.980 --> 01:38:26.860]   John in our studio just held up a sign that said, they're the problem.
[01:38:26.860 --> 01:38:28.860]   He loves us.
[01:38:28.860 --> 01:38:30.460]   Oh, hi John.
[01:38:30.460 --> 01:38:31.900]   It was very good.
[01:38:31.900 --> 01:38:34.100]   Very culturally mindful.
[01:38:34.100 --> 01:38:35.860]   I did have a question though.
[01:38:35.860 --> 01:38:42.940]   It used to be that because I've gone to CES like three times over the, I don't know
[01:38:42.940 --> 01:38:43.940]   the past.
[01:38:43.940 --> 01:38:44.940]   Oh boy.
[01:38:44.940 --> 01:38:46.860]   A lot of years.
[01:38:46.860 --> 01:38:49.500]   And I forgot that I am also getting older.
[01:38:49.500 --> 01:38:54.300]   Anyway, and in going to CES those times, hate it.
[01:38:54.300 --> 01:38:57.180]   Obviously don't have any interest in going.
[01:38:57.180 --> 01:39:00.460]   But one of the things that you hear whenever people talk about going to that or going to
[01:39:00.460 --> 01:39:07.020]   any of these sort of big in-person events is like, yeah, you know, it doesn't seem super
[01:39:07.020 --> 01:39:13.180]   useful except for the opportunity to socialize, except for the opportunity to get together
[01:39:13.180 --> 01:39:15.980]   with people in the industry you haven't seen before.
[01:39:15.980 --> 01:39:19.300]   Are we getting to a place where that doesn't even matter anymore?
[01:39:19.300 --> 01:39:23.820]   Well, again, though, I think it depends because it's an industry conference.
[01:39:23.820 --> 01:39:28.620]   So there are other ones that are, you know, you have things like GAC, which are more focused,
[01:39:28.620 --> 01:39:33.940]   you know, on the game developers and you have some other maybe more focused conferences
[01:39:33.940 --> 01:39:35.420]   that are around that.
[01:39:35.420 --> 01:39:39.460]   For things like CES, you know, there might be other, you know, industry symposiums people
[01:39:39.460 --> 01:39:42.580]   could go to to kind of meet up with one another.
[01:39:42.580 --> 01:39:48.060]   But for many, many years, you know, like the normal person going to the trade show aspect
[01:39:48.060 --> 01:39:49.860]   has been gone.
[01:39:49.860 --> 01:39:56.860]   Like I remember with E3, because it was in Atlanta in the 90s, a number of times.
[01:39:56.860 --> 01:40:01.620]   I remember like taking like a business card from like the local electronics boutique and
[01:40:01.620 --> 01:40:02.940]   like passing it off me.
[01:40:02.940 --> 01:40:08.700]   Like, even though I'm not old enough to work at EV games, like I have a business card and
[01:40:08.700 --> 01:40:14.020]   this is me and let me have access to go, you know, through the trade show floor and see
[01:40:14.020 --> 01:40:15.660]   all the stuff.
[01:40:15.660 --> 01:40:19.460]   And that's most of the best stuff is not even on the floor now.
[01:40:19.460 --> 01:40:23.820]   So again, it's those things like packs and you know, Blizzcon and things like that that
[01:40:23.820 --> 01:40:30.260]   I think have more of the social aspect where people want to meet up and do that.
[01:40:30.260 --> 01:40:31.900]   Like I think that it's weird.
[01:40:31.900 --> 01:40:36.460]   I almost feel like there is still that aspect, but those things have happened with maybe
[01:40:36.460 --> 01:40:40.940]   smaller or more focused conferences rather than like the big omni conference.
[01:40:40.940 --> 01:40:45.580]   I do feel like for some companies, the reason they go to CES to your point is probably we
[01:40:45.580 --> 01:40:48.980]   know that there will be a ton of manufacturers and a ton of partners and a ton of potentially
[01:40:48.980 --> 01:40:51.340]   money people that we can get time with.
[01:40:51.340 --> 01:40:53.300]   But they're doing those meetings while they're in Las Vegas.
[01:40:53.300 --> 01:40:56.900]   It's not necessarily like happening on the floor of CES.
[01:40:56.900 --> 01:40:57.900]   Yeah.
[01:40:57.900 --> 01:41:04.380]   So I ran into the CEO of Microsoft in the bowels of some Vegas hotel at one CES after
[01:41:04.380 --> 01:41:06.140]   I was told that he was not going.
[01:41:06.140 --> 01:41:11.740]   And what they meant by that was he's not going to talk to you, but he was going to do
[01:41:11.740 --> 01:41:14.020]   and this is to be clear, I'm not annoyed.
[01:41:14.020 --> 01:41:19.700]   He was going there to talk to partners, vendors who were all in town and reasonable enough.
[01:41:19.700 --> 01:41:23.660]   But that's behind closed doors out of the public eye.
[01:41:23.660 --> 01:41:27.220]   And that does not make the actual event any more particularly relevant to us.
[01:41:27.220 --> 01:41:28.340]   And I just one more thing.
[01:41:28.340 --> 01:41:32.780]   I think as the gaming companies have become increasingly blobs, you might say, like one
[01:41:32.780 --> 01:41:36.180]   or two major blobs, I think that they just don't want to share.
[01:41:36.180 --> 01:41:39.980]   Whereas in the old days, if there's 15 leading companies, then you might all want to get
[01:41:39.980 --> 01:41:44.540]   together, collect viewership and try to outshine.
[01:41:44.540 --> 01:41:46.860]   But if there's only three companies, you're going to end up with them all going their
[01:41:46.860 --> 01:41:48.180]   own way.
[01:41:48.180 --> 01:41:50.380]   So Microsoft shouldn't be allowed to buy Activision.
[01:41:50.380 --> 01:41:52.380]   That's what I'm saying.
[01:41:52.380 --> 01:41:53.380]   Wow.
[01:41:53.380 --> 01:41:54.380]   There it is.
[01:41:54.380 --> 01:41:58.300]   Really, really want to make our favorite marine angry don't you?
[01:41:58.300 --> 01:41:59.300]   I'm sorry.
[01:41:59.300 --> 01:42:00.300]   It's anti-competitive.
[01:42:00.300 --> 01:42:03.020]   Like, I, okay, like, of course it is.
[01:42:03.020 --> 01:42:09.500]   Of course Adobe buying Figma is anti-competitive businesses don't buy stuff to increase competition
[01:42:09.500 --> 01:42:15.540]   So all acquisitions are anti-competitive is what you're saying.
[01:42:15.540 --> 01:42:17.100]   I'm not going to go that far.
[01:42:17.100 --> 01:42:22.060]   I think the ones that we're discussing, let's see, Microsoft owns a gaming platform that
[01:42:22.060 --> 01:42:24.580]   it wants to compete against PlayStation with.
[01:42:24.580 --> 01:42:26.180]   What do they use to compete?
[01:42:26.180 --> 01:42:27.180]   Gaming exclusives.
[01:42:27.180 --> 01:42:30.780]   And sure, they're going to pledge 10 years of play nice.
[01:42:30.780 --> 01:42:31.780]   Great.
[01:42:31.780 --> 01:42:35.420]   I'll be 43 then when they stop doing that.
[01:42:35.420 --> 01:42:37.580]   How about you like to win?
[01:42:37.580 --> 01:42:38.580]   This capital.
[01:42:38.580 --> 01:42:43.500]   I just don't believe they're going to suddenly put on kid gloves and become all hunky-dory.
[01:42:43.500 --> 01:42:45.660]   Well I don't know if anyone on the-
[01:42:45.660 --> 01:42:47.660]   Sorry, I didn't need to go.
[01:42:47.660 --> 01:42:49.140]   Just, that's a thing.
[01:42:49.140 --> 01:42:50.420]   That's all right.
[01:42:50.420 --> 01:42:57.140]   I think now is actually a good time to take a quick break for a message from Leo.
[01:42:57.140 --> 01:42:58.140]   Hey, everybody.
[01:42:58.140 --> 01:42:59.140]   Leo LePort here.
[01:42:59.140 --> 01:43:03.780]   I am the founder and one of the hosts at the Twit podcast network.
[01:43:03.780 --> 01:43:06.820]   I want to talk to you a little bit about what we do here at Twit.
[01:43:06.820 --> 01:43:14.900]   Because I think it's unique and I think for anybody who is bringing a product or a service
[01:43:14.900 --> 01:43:19.620]   to a tech audience, you need to know about what we do here at Twit.
[01:43:19.620 --> 01:43:25.940]   We've built an amazing audience of engaged, intelligent, affluent listeners who listen
[01:43:25.940 --> 01:43:29.740]   to us and trust us when we recommend a product.
[01:43:29.740 --> 01:43:34.700]   Our mission statement is Twit is to build a highly engaged community of tech enthusiasts.
[01:43:34.700 --> 01:43:41.460]   Already, your ears should be perking up at that because highly engaged is good for you.
[01:43:41.460 --> 01:43:44.820]   Tech enthusiasts, if that's who you're looking for, this is the place.
[01:43:44.820 --> 01:43:49.540]   We do it by offering them the knowledge they need to understand and use technology in today's
[01:43:49.540 --> 01:43:50.540]   world.
[01:43:50.540 --> 01:43:55.620]   I hear from our audience all the time, part of that knowledge comes from our advertisers.
[01:43:55.620 --> 01:43:56.620]   We are very careful.
[01:43:56.620 --> 01:44:03.740]   We pick advertisers with great products, great services with integrity and introduce them
[01:44:03.740 --> 01:44:09.020]   to our audience with authenticity and genuine enthusiasm.
[01:44:09.020 --> 01:44:12.980]   That makes our host red ads different from anything else you can buy.
[01:44:12.980 --> 01:44:21.140]   We are literally bringing you to the attention of our audience and giving you a big fat endorsement.
[01:44:21.140 --> 01:44:24.340]   We like to create partnerships with trusted brands.
[01:44:24.340 --> 01:44:29.940]   Brands who are in it for the long run, long term partners that want to grow with us.
[01:44:29.940 --> 01:44:32.380]   We have so many great success stories.
[01:44:32.380 --> 01:44:38.700]   Tim Broome, who founded ITProTV in 2013, started advertising with us on day one, has been with
[01:44:38.700 --> 01:44:40.380]   us ever since.
[01:44:40.380 --> 01:44:45.260]   He said, "We would not be where we are today without the Twit network."
[01:44:45.260 --> 01:44:47.660]   I think the proof is in the pudding.
[01:44:47.660 --> 01:44:52.380]   Advertisers like ITProTV and Audible, they have been with us for more than 10 years.
[01:44:52.380 --> 01:44:56.020]   They stick around because their ads work.
[01:44:56.020 --> 01:44:59.060]   Honestly, isn't that why you're buying advertising?
[01:44:59.060 --> 01:45:00.260]   You get a lot with Twit.
[01:45:00.260 --> 01:45:02.660]   We have a very full service attitude.
[01:45:02.660 --> 01:45:08.020]   We almost think of it as artisanal advertising, boutique advertising.
[01:45:08.020 --> 01:45:11.900]   You'll get a full service continuity team.
[01:45:11.900 --> 01:45:16.140]   People who are on the phone with you, who are in touch with you, who support you with
[01:45:16.140 --> 01:45:19.780]   everything from copywriting to graphic design.
[01:45:19.780 --> 01:45:22.080]   You are not alone in this.
[01:45:22.080 --> 01:45:24.820]   We embed our ads into the shows.
[01:45:24.820 --> 01:45:26.860]   They're not added later.
[01:45:26.860 --> 01:45:28.340]   They're part of the shows.
[01:45:28.340 --> 01:45:33.300]   But often, they're such a part of our shows that our other hosts will chime in on the ad,
[01:45:33.300 --> 01:45:35.220]   saying, "Yeah, I love that."
[01:45:35.220 --> 01:45:39.980]   Just the other day, one of our hosts said, "Man, I really got to buy that.
[01:45:39.980 --> 01:45:44.940]   That's an additional benefit to you because you're hearing people, our audience trusts
[01:45:44.940 --> 01:45:47.860]   saying, "Yeah, that sounds great."
[01:45:47.860 --> 01:45:50.860]   We deliver, always over-deliver on impressions.
[01:45:50.860 --> 01:45:54.220]   You know you're going to get the impressions you expect.
[01:45:54.220 --> 01:45:56.180]   The ads are unique every time.
[01:45:56.180 --> 01:45:58.020]   We don't pre-record them and roll them in.
[01:45:58.020 --> 01:46:02.020]   We are genuinely doing those ads in the middle of the show.
[01:46:02.020 --> 01:46:03.980]   We'll give you great onboarding services.
[01:46:03.980 --> 01:46:08.020]   Adtech with pod sites that's free for direct clients.
[01:46:08.020 --> 01:46:11.820]   Gives you a lot of reporting, gives you a great idea of how well your ads are working.
[01:46:11.820 --> 01:46:13.140]   You'll get courtesy commercials.
[01:46:13.140 --> 01:46:17.540]   You actually can take our ads and share them across social media and landing pages.
[01:46:17.540 --> 01:46:19.540]   That really extends the reach.
[01:46:19.540 --> 01:46:23.220]   There are other free goodies too, including mentions in our weekly newsletter that sent
[01:46:23.220 --> 01:46:27.700]   to thousands of fans, engaged fans who really want to see this stuff.
[01:46:27.700 --> 01:46:31.820]   We give you bonus ads and social media promotion too.
[01:46:31.820 --> 01:46:38.580]   If you want to be a long-term partner, introduce your product to a savvy, engaged tech audience.
[01:46:38.580 --> 01:46:41.740]   Visit twit.tv/advertise.
[01:46:41.740 --> 01:46:43.100]   Check out those testimonials.
[01:46:43.100 --> 01:46:45.300]   Mark McCrary is the CEO of Authentic.
[01:46:45.300 --> 01:46:50.140]   You probably know him, one of the biggest original podcast advertising companies.
[01:46:50.140 --> 01:46:53.420]   We've been with him for 16 years.
[01:46:53.420 --> 01:46:59.460]   Mark said the feedback from many advertisers over 16 years across a range of product categories.
[01:46:59.460 --> 01:47:04.180]   Everything from razors to computers is that if ads and podcasts are going to work for
[01:47:04.180 --> 01:47:07.020]   a brand, they're going to work on Twitch shows.
[01:47:07.020 --> 01:47:12.740]   I'm very proud of what we do because it's honest, it's got integrity, it's authentic,
[01:47:12.740 --> 01:47:18.700]   and it really is a great introduction to our audience of your brand.
[01:47:18.700 --> 01:47:20.500]   Our listeners are smart.
[01:47:20.500 --> 01:47:25.100]   They're engaged, they're tech savvy, they're dedicated to our network.
[01:47:25.100 --> 01:47:29.340]   That's one of the reasons we only work with high-integrity partners that we've personally
[01:47:29.340 --> 01:47:30.660]   and thoroughly vetted.
[01:47:30.660 --> 01:47:33.500]   I have absolute approval on everybody.
[01:47:33.500 --> 01:47:36.420]   If you've got a great product, I want to hear from you.
[01:47:36.420 --> 01:47:40.900]   Elevate your brand by reaching out today at advertise@twit.tv.
[01:47:40.900 --> 01:47:42.420]   Break out of the advertising norm.
[01:47:42.420 --> 01:47:46.020]   Grow your brand with host red ads on twit.tv.
[01:47:46.020 --> 01:47:53.420]   Visit twit.tv/advertise for more details or you can email us at advertise@twit.tv if
[01:47:53.420 --> 01:47:55.180]   you're ready to launch your campaign now.
[01:47:55.180 --> 01:47:56.580]   I can't wait to see your product.
[01:47:56.580 --> 01:47:57.580]   So give us a ring.
[01:47:57.580 --> 01:47:58.580]   All right.
[01:47:58.580 --> 01:48:00.140]   Thanks, Leo, for that short little message.
[01:48:00.140 --> 01:48:03.380]   We are back and we have a great panel.
[01:48:03.380 --> 01:48:08.980]   Christina Warren, senior developer advocate at GitHub, Alex Wilhelm of TechCrunch, and
[01:48:08.980 --> 01:48:15.260]   Abrar Al-Hiti of CNET, video host and producer, all here with us.
[01:48:15.260 --> 01:48:18.460]   I'm just scrolling through my list here.
[01:48:18.460 --> 01:48:20.700]   I did want to talk about something.
[01:48:20.700 --> 01:48:27.740]   Christina, I don't know if you feel like this is something that you aren't able to discuss
[01:48:27.740 --> 01:48:28.740]   or you are.
[01:48:28.740 --> 01:48:33.700]   Of course, I totally understand because this is about security co-pilot.
[01:48:33.700 --> 01:48:40.420]   Microsoft is working on a GPT-4 powered assistant for cybersecurity.
[01:48:40.420 --> 01:48:43.500]   I thought this was cool given.
[01:48:43.500 --> 01:48:48.580]   Microsoft seems to be, in the same way, I think, of course, first and foremost of Apple because
[01:48:48.580 --> 01:48:52.660]   I cover that company more than any other in a given week.
[01:48:52.660 --> 01:48:56.220]   One of the things that I've seen Apple do, and Amazon has done this in a big way too,
[01:48:56.220 --> 01:49:03.260]   is brand their artificial intelligence stuff.
[01:49:03.260 --> 01:49:07.220]   Siri is not just the virtual assistant at Apple.
[01:49:07.220 --> 01:49:15.740]   It is also the AI mind that is paying attention to your behavior and then serving up different
[01:49:15.740 --> 01:49:16.740]   suggestions.
[01:49:16.740 --> 01:49:18.980]   They call those Siri suggestions.
[01:49:18.980 --> 01:49:25.140]   We've seen now Microsoft where at one point, I think Cortana was seeming to be the big overall
[01:49:25.140 --> 01:49:30.700]   umbrella brand for all the different kinds of AI active suggestions it was making.
[01:49:30.700 --> 01:49:33.940]   Now, it seems like co-pilot is the brand.
[01:49:33.940 --> 01:49:37.460]   We've seen co-pilot in GitHub.
[01:49:37.460 --> 01:49:40.660]   We've seen co-pilot making its way into office.
[01:49:40.660 --> 01:49:43.580]   Now, there's a security co-pilot.
[01:49:43.580 --> 01:49:49.500]   This is specifically for security professionals.
[01:49:49.500 --> 01:49:55.940]   I think, once again, this is a fascinating concept, this idea that you will have a little
[01:49:55.940 --> 01:50:03.380]   angel sitting on your shoulder that is helping you as you're moving along and you are working
[01:50:03.380 --> 01:50:12.580]   on things to provide some assistance, to provide even because of the fact that this virtual
[01:50:12.580 --> 01:50:21.140]   assistant or this virtual experience can look at all aspects of your security posture.
[01:50:21.140 --> 01:50:24.380]   Be able to look at all of those signals and pull that together and say, "Hey, it's looking
[01:50:24.380 --> 01:50:26.220]   like someone's trying to breach your system.
[01:50:26.220 --> 01:50:29.300]   Here's what you need to know about this."
[01:50:29.300 --> 01:50:35.500]   Once again, Alex, this is that augmentation of a human's ability that I think is a very
[01:50:35.500 --> 01:50:40.340]   valuable, a potentially very valuable tool where the human ultimately has to make the
[01:50:40.340 --> 01:50:48.180]   choice of how to approach the issue, but to be able to have something that can take all
[01:50:48.180 --> 01:50:54.220]   of that data and put it down into something that makes sense and look at potential trends
[01:50:54.220 --> 01:50:58.180]   for that data, I think is a really cool idea.
[01:50:58.180 --> 01:51:06.300]   I like, Christina, that Microsoft does seem to be going, "We have this partnership in
[01:51:06.300 --> 01:51:13.020]   place and we have the technology and then they're going, "Where all can we stick it?
[01:51:13.020 --> 01:51:18.020]   It's kind of spaghetti project, but it seems to be working in a very positive way so far."
[01:51:18.020 --> 01:51:20.620]   Yeah, no, I think you're right.
[01:51:20.620 --> 01:51:26.860]   It definitely does seem to be like, I think that the brand that we're going for a lot
[01:51:26.860 --> 01:51:31.380]   of things is definitely co-pilot.
[01:51:31.380 --> 01:51:32.860]   I work at GitHub.
[01:51:32.860 --> 01:51:38.180]   We were the ones who, how that was chosen, but the Microsoft started to use it.
[01:51:38.180 --> 01:51:45.740]   Microsoft 365 co-pilot and security co-pilot and Dynamics co-pilot.
[01:51:45.740 --> 01:51:54.460]   It follows the Microsoft tradition of taking a common word like word and turning it into
[01:51:54.460 --> 01:51:56.260]   a product name.
[01:51:56.260 --> 01:52:03.260]   I do like that, but no, but I think that this is the sort of area where when you have something
[01:52:03.260 --> 01:52:07.460]   that could be really complicated to explain things like security topics or to synthesize
[01:52:07.460 --> 01:52:11.180]   a lot of different information, I think this could be a really good use of AI.
[01:52:11.180 --> 01:52:16.740]   One thing I would say is as with anything else, don't become just reliant on this particular
[01:52:16.740 --> 01:52:17.900]   thing.
[01:52:17.900 --> 01:52:22.940]   I would never say to anybody, "Make this be the only thing you rely on for your security
[01:52:22.940 --> 01:52:23.940]   stuff."
[01:52:23.940 --> 01:52:27.740]   I do feel like this could be a good way of helping synthesize a lot of information and
[01:52:27.740 --> 01:52:30.540]   point things out that people might otherwise miss.
[01:52:30.540 --> 01:52:35.060]   Yeah, I think the branding points is very good because Microsoft has taken common words
[01:52:35.060 --> 01:52:36.460]   and turned them into brand names.
[01:52:36.460 --> 01:52:40.060]   It's also taken people's names and used them quite sensitively as well.
[01:52:40.060 --> 01:52:46.060]   For example, the last few of Microsoft's big friends fan, Chandler Bing, Bing Search.
[01:52:46.060 --> 01:52:47.060]   Well known fact.
[01:52:47.060 --> 01:52:48.060]   By the way, I'm kidding.
[01:52:48.060 --> 01:52:49.060]   Just be clear.
[01:52:49.060 --> 01:52:52.900]   I think the branding points are actually good here because if you go back in time to Microsoft
[01:52:52.900 --> 01:52:54.460]   consumer products, remember SkyDrive?
[01:52:54.460 --> 01:52:55.460]   Yeah.
[01:52:55.460 --> 01:52:58.460]   Before it was OneDrive because of a copyright issue?
[01:52:58.460 --> 01:52:59.460]   Before it was LiveDrive.
[01:52:59.460 --> 01:53:01.460]   Oh, LiveDrive didn't last.
[01:53:01.460 --> 01:53:02.460]   That was terrible.
[01:53:02.460 --> 01:53:04.540]   It was SkyDrive LiveDrive OneDrive.
[01:53:04.540 --> 01:53:05.540]   Yeah.
[01:53:05.540 --> 01:53:07.900]   Then everyone forgot because OneDrive was bleh.
[01:53:07.900 --> 01:53:12.860]   I thought the Cortana name was super hype because it was a cool use of IP from the gaming world
[01:53:12.860 --> 01:53:15.860]   and bringing it into a consumer productivity perspective.
[01:53:15.860 --> 01:53:23.020]   However, co-pilot to the point about it being AI helping you as your buddy actually, which
[01:53:23.020 --> 01:53:26.900]   is what Cortana was doing but no one knew that in the video game franchise world.
[01:53:26.900 --> 01:53:28.380]   I think copout's frickin' brilliant.
[01:53:28.380 --> 01:53:29.540]   I think it's really good.
[01:53:29.540 --> 01:53:34.180]   And this particular example, I think, is how AI is going to show up everywhere relatively
[01:53:34.180 --> 01:53:35.180]   quickly.
[01:53:35.180 --> 01:53:36.580]   And I think it's super cool.
[01:53:36.580 --> 01:53:38.620]   I obviously am not a cybersecurity professional.
[01:53:38.620 --> 01:53:40.820]   I don't have any perspective on using this.
[01:53:40.820 --> 01:53:44.300]   But in terms of where I think we're going to see this stuff show up here, it's a great
[01:53:44.300 --> 01:53:48.260]   example of it and may make us all more secure because God knows we could use it.
[01:53:48.260 --> 01:53:53.340]   I am going to ask, oh, no, please, Abra, and then I have a question for all of you.
[01:53:53.340 --> 01:53:57.980]   I was going to say, all I want is for outlook to be more usable.
[01:53:57.980 --> 01:54:01.180]   And so can we add, can I just search my email and find it?
[01:54:01.180 --> 01:54:02.180]   Like that's all I want.
[01:54:02.180 --> 01:54:03.180]   For real, right?
[01:54:03.180 --> 01:54:04.180]   That's my only suggestion.
[01:54:04.180 --> 01:54:05.180]   Yeah.
[01:54:05.180 --> 01:54:06.180]   Yeah.
[01:54:06.180 --> 01:54:14.180]   So the question that I have for everyone and prepare yourselves, everybody, but I'll start
[01:54:14.180 --> 01:54:16.260]   with Abra.
[01:54:16.260 --> 01:54:19.580]   What copilot do you wish you had right now?
[01:54:19.580 --> 01:54:27.540]   So in a world where you get to have someone make you a tech copilot, where -- and it can
[01:54:27.540 --> 01:54:33.380]   be for anything, where do you wish you had this AI copilot helping you out day to day?
[01:54:33.380 --> 01:54:34.380]   Oh, wow.
[01:54:34.380 --> 01:54:37.620]   Oh, that's a really good question.
[01:54:37.620 --> 01:54:38.620]   Day to day.
[01:54:38.620 --> 01:54:43.460]   Well, I'll give a -- well, that's kind of boring.
[01:54:43.460 --> 01:54:46.620]   It's going to say for like work.
[01:54:46.620 --> 01:54:51.820]   Like, if it was for work, I would love to just like -- like I would love to brainstorm
[01:54:51.820 --> 01:54:53.380]   story ideas with --
[01:54:53.380 --> 01:54:54.380]   Yeah.
[01:54:54.380 --> 01:54:55.380]   Like, wouldn't that be fun?
[01:54:55.380 --> 01:54:56.380]   Just kind of like --
[01:54:56.380 --> 01:54:57.380]   Just to have like --
[01:54:57.380 --> 01:54:58.380]   And just like angles.
[01:54:58.380 --> 01:54:59.380]   Yeah, a little rubber ducky.
[01:54:59.380 --> 01:55:00.380]   What is that?
[01:55:00.380 --> 01:55:01.380]   That's not the word.
[01:55:01.380 --> 01:55:02.380]   There's a developer thing.
[01:55:02.380 --> 01:55:03.380]   A sounding board.
[01:55:03.380 --> 01:55:05.980]   That's a thing to determine looking for.
[01:55:05.980 --> 01:55:06.980]   Yeah.
[01:55:06.980 --> 01:55:08.220]   Hey, this is an idea that I have.
[01:55:08.220 --> 01:55:10.180]   And then it's like, oh, that's a great idea.
[01:55:10.180 --> 01:55:11.900]   But have you thought about this?
[01:55:11.900 --> 01:55:13.780]   And it just sort of tweaks the idea a little bit.
[01:55:13.780 --> 01:55:14.780]   Yeah.
[01:55:14.780 --> 01:55:15.780]   That's a good copilot.
[01:55:15.780 --> 01:55:16.780]   Exactly.
[01:55:16.780 --> 01:55:17.780]   That would be helpful.
[01:55:17.780 --> 01:55:18.780]   Story copilot.
[01:55:18.780 --> 01:55:19.980]   Christina, what's your copilot?
[01:55:19.980 --> 01:55:20.980]   Okay.
[01:55:20.980 --> 01:55:24.580]   So, I mean, I do actually use GitHub copilot all the time.
[01:55:24.580 --> 01:55:27.860]   That is a plug, but it's also -- it's really good.
[01:55:27.860 --> 01:55:29.380]   And I do use that with work a lot.
[01:55:29.380 --> 01:55:32.460]   And it's -- and we don't advertise that it works for pros, but it actually does work
[01:55:32.460 --> 01:55:37.420]   pretty well for pros, although I mostly use it for creating demos.
[01:55:37.420 --> 01:55:39.260]   I would love a cleaning copilot.
[01:55:39.260 --> 01:55:48.660]   Oh, so is it like, oh, no, I just got this -- I just -- I squeezed the turnip and the
[01:55:48.660 --> 01:55:50.780]   juice landed on my pants.
[01:55:50.780 --> 01:55:52.700]   And I do know how to get the stamina.
[01:55:52.700 --> 01:55:57.220]   Well, it would be like, okay, I'm going to see the state of what rooms are right now.
[01:55:57.220 --> 01:56:02.420]   And I'm going to help you prioritize an actual plan to get the most out of doing like a
[01:56:02.420 --> 01:56:04.820]   cleaning and like your laundry and other stuff.
[01:56:04.820 --> 01:56:10.620]   Honestly, I need like a, you know, a Marie Kondo, like sort of like copilot.
[01:56:10.620 --> 01:56:11.620]   Oh, my God.
[01:56:11.620 --> 01:56:12.620]   Yes.
[01:56:12.620 --> 01:56:15.460]   And organizational cleaning space.
[01:56:15.460 --> 01:56:16.460]   Oh, my God.
[01:56:16.460 --> 01:56:17.460]   Yes.
[01:56:17.460 --> 01:56:18.460]   Let's get on that for sure.
[01:56:18.460 --> 01:56:20.460]   And Alex, what's your copilot?
[01:56:20.460 --> 01:56:23.660]   I'm just blown away by how good the first two are because I would absolutely use both
[01:56:23.660 --> 01:56:25.140]   of them.
[01:56:25.140 --> 01:56:29.260]   For me, though, the thing that I've really long wanted and I'm hoping is going to be
[01:56:29.260 --> 01:56:33.220]   now possible is an OS level buddy.
[01:56:33.220 --> 01:56:39.620]   I want to be writing and then look at 18 tabs that I pulled up in my extra, extra
[01:56:39.620 --> 01:56:44.180]   Chrome window to read later and just be like, oh, yes, can you just put those away into
[01:56:44.180 --> 01:56:45.180]   a file for me?
[01:56:45.180 --> 01:56:48.180]   I'm going to ask them for them back a little bit later on, but they're killing my ram.
[01:56:48.180 --> 01:56:49.980]   And it would go, sure, Alex.
[01:56:49.980 --> 01:56:55.580]   And here's why I think this is not going to be possible because chat GPT can ingest natural
[01:56:55.580 --> 01:57:00.340]   language and it can and do stuff and it can make code.
[01:57:00.340 --> 01:57:05.500]   So why can't it take my words and then use Zappier or whatever and plug into applications,
[01:57:05.500 --> 01:57:06.500]   right?
[01:57:06.500 --> 01:57:10.580]   And then just write code or send little magic bits through the tubes and then do stuff for
[01:57:10.580 --> 01:57:11.580]   me.
[01:57:11.580 --> 01:57:14.140]   And then it's smart enough that it should work most of the time because my stuff that
[01:57:14.140 --> 01:57:15.620]   I need isn't complex.
[01:57:15.620 --> 01:57:18.940]   It's mostly just very basic stuff, but I'm very busy and I'm going to hurry.
[01:57:18.940 --> 01:57:19.940]   So that's what I want.
[01:57:19.940 --> 01:57:25.060]   But if I can't have that, I will take cleaning bot or story bot to be clear because those
[01:57:25.060 --> 01:57:26.060]   are awesome.
[01:57:26.060 --> 01:57:27.060]   Those are both.
[01:57:27.060 --> 01:57:28.900]   No, I know I love that, Alex.
[01:57:28.900 --> 01:57:32.740]   And honestly, like you could also plug that into like obsidian or another one of those
[01:57:32.740 --> 01:57:38.340]   like second brain apps and be like, okay, put all this stuff here and bring up that document
[01:57:38.340 --> 01:57:41.180]   that I was looking at last week that was talking about this.
[01:57:41.180 --> 01:57:42.180]   Yeah.
[01:57:42.180 --> 01:57:43.180]   I love that.
[01:57:43.180 --> 01:57:44.180]   And it could do it.
[01:57:44.180 --> 01:57:45.180]   You're exactly right.
[01:57:45.180 --> 01:57:46.180]   The technology is there.
[01:57:46.180 --> 01:57:49.820]   We just need to be able to have like maybe a localized trained models like just train it
[01:57:49.820 --> 01:57:51.460]   on everything that's on my computer.
[01:57:51.460 --> 01:57:52.460]   Yes.
[01:57:52.460 --> 01:57:55.380]   And I just worked sometimes, but it didn't work this time.
[01:57:55.380 --> 01:57:59.820]   I looked at my phone and I hit the little power button to get Siri to pull up.
[01:57:59.820 --> 01:58:03.380]   And I was like, Siri, please play X by artist Y on Spotify.
[01:58:03.380 --> 01:58:06.500]   And it went working on it, working on it, working on it and failed.
[01:58:06.500 --> 01:58:11.180]   And I'm like, Lord above, I remember when you bought Siri like 15 years ago and it still
[01:58:11.180 --> 01:58:12.780]   can't play a song.
[01:58:12.780 --> 01:58:13.780]   Right.
[01:58:13.780 --> 01:58:14.780]   So all that technology is trash.
[01:58:14.780 --> 01:58:15.780]   Just throw it away.
[01:58:15.780 --> 01:58:16.780]   Yep.
[01:58:16.780 --> 01:58:21.380]   And then just use GPT's ability to ingest and kick out text and have that double as code
[01:58:21.380 --> 01:58:22.380]   and then do stuff.
[01:58:22.380 --> 01:58:23.380]   Why not?
[01:58:23.380 --> 01:58:24.380]   Yeah.
[01:58:24.380 --> 01:58:25.380]   Why not work now?
[01:58:25.380 --> 01:58:30.900]   And also be able to play our Taylor Tunes playlist on Apple Music, which is my Apple
[01:58:30.900 --> 01:58:34.940]   Music playlist of every single Taylor Swift song and single from everything.
[01:58:34.940 --> 01:58:35.940]   So yeah.
[01:58:35.940 --> 01:58:40.500]   Can I just say to the Swifties here, I love it when I pull up Spotify now and I'm playing
[01:58:40.500 --> 01:58:47.220]   the new album and it says at the bottom, copyright, not Republic, not RCA, not Capitol
[01:58:47.220 --> 01:58:48.220]   Records.
[01:58:48.220 --> 01:58:49.220]   It says copyright.
[01:58:49.220 --> 01:58:50.220]   Taylor Swift.
[01:58:50.220 --> 01:58:51.220]   Our queen.
[01:58:51.220 --> 01:58:54.420]   I am so happy for all of you.
[01:58:54.420 --> 01:58:57.500]   And I'm so happy for these ideas.
[01:58:57.500 --> 01:59:02.420]   My bot would be basically the wire cutter, but as an AI chat bot.
[01:59:02.420 --> 01:59:07.460]   So anytime I have a problem and I'm thinking about buying something, I ask it and it's
[01:59:07.460 --> 01:59:13.700]   like, here is the one that you should probably get because it's good for your budget, but
[01:59:13.700 --> 01:59:16.980]   is also good enough to do this and also has these two extra.
[01:59:16.980 --> 01:59:21.340]   It will take what the wire cutter knows, but also all the thousands of other sites out
[01:59:21.340 --> 01:59:26.220]   there and use all of that together to recommend me the best thing for me specifically.
[01:59:26.220 --> 01:59:27.220]   Oh, yeah.
[01:59:27.220 --> 01:59:32.140]   What I would say is the best thing for your needs is X, but I checked your city bank account
[01:59:32.140 --> 01:59:33.500]   and you can have a lot.
[01:59:33.500 --> 01:59:34.500]   Yes, exactly.
[01:59:34.500 --> 01:59:35.500]   Yeah.
[01:59:35.500 --> 01:59:37.340]   It's like, you wish you could have this.
[01:59:37.340 --> 01:59:39.140]   You got the Apple Play leader.
[01:59:39.140 --> 01:59:42.780]   I'm just going to shame you a lot and be like, if you considered increasing your income to
[01:59:42.780 --> 01:59:46.940]   afford better shoes, if it literally go, what?
[01:59:46.940 --> 01:59:47.940]   What are those?
[01:59:47.940 --> 01:59:51.940]   Thank you for those of you got that.
[01:59:51.940 --> 01:59:52.940]   I do.
[01:59:52.940 --> 02:00:01.300]   I do wish that I could increase or decrease the personality of my AI in the same way that
[02:00:01.300 --> 02:00:03.420]   I can with carrot weather.
[02:00:03.420 --> 02:00:07.740]   So for those of you who have not used the carrot weather app, it's this weather app
[02:00:07.740 --> 02:00:11.300]   for the, for iPhone and other devices.
[02:00:11.300 --> 02:00:14.980]   And you can kind of, it has these little responses that it gives and you can make it more or
[02:00:14.980 --> 02:00:16.820]   less snarky.
[02:00:16.820 --> 02:00:19.340]   And I do like when it sort of says mean things.
[02:00:19.340 --> 02:00:23.300]   Okay, this is getting weird, but I like what it says mean things to me.
[02:00:23.300 --> 02:00:27.900]   And I kind of wish that I could get my AI bot to have a little bit more of a personality
[02:00:27.900 --> 02:00:28.900]   as well.
[02:00:28.900 --> 02:00:31.580]   So then in that case, I would say don't buy those shoes.
[02:00:31.580 --> 02:00:32.580]   You don't need them.
[02:00:32.580 --> 02:00:36.820]   You ever accidentally tell more about your personality than you've been to them podcast?
[02:00:36.820 --> 02:00:37.820]   Yeah.
[02:00:37.820 --> 02:00:41.340]   Just then I wish I had an AI about that was like, whoa, slow down, Mike.
[02:00:41.340 --> 02:00:44.340]   You know, what you're about to say is not for anyone to hear.
[02:00:44.340 --> 02:00:49.060]   I think we try to send a tweet with profanity and Twitter is like, people don't like these
[02:00:49.060 --> 02:00:51.420]   tweets that use these words.
[02:00:51.420 --> 02:00:52.580]   Please don't say this.
[02:00:52.580 --> 02:00:56.100]   It's just going to embarrass everyone, but especially you.
[02:00:56.100 --> 02:01:00.580]   I just say to them, all of this that we're talking about, I don't see why it isn't possible
[02:01:00.580 --> 02:01:01.580]   the next five years.
[02:01:01.580 --> 02:01:07.380]   Christina would need cameras in her rooms to have them ingest images and stuff, but the
[02:01:07.380 --> 02:01:11.260]   story bot would just want to read your past covers and say, well, you all talked about
[02:01:11.260 --> 02:01:13.140]   this six months ago.
[02:01:13.140 --> 02:01:16.020]   Does that have any pertinence?
[02:01:16.020 --> 02:01:18.620]   And I'll just call Mike and tell him no.
[02:01:18.620 --> 02:01:25.060]   So I would be possible with I think the only thing that it need to have is just more a
[02:01:25.060 --> 02:01:27.900]   more recent database than September 2021.
[02:01:27.900 --> 02:01:33.780]   So if it had a more recent database in September 2021, I've pretty much already got mine other
[02:01:33.780 --> 02:01:37.980]   than it doesn't have access to my banking information, but I honestly feel okay with
[02:01:37.980 --> 02:01:38.980]   that.
[02:01:38.980 --> 02:01:39.980]   Right.
[02:01:39.980 --> 02:01:44.020]   I wanted to do that based on everything, but financial means because I want to keep that
[02:01:44.020 --> 02:01:45.020]   to myself.
[02:01:45.020 --> 02:01:49.180]   So I think mine's possible in its current state.
[02:01:49.180 --> 02:01:52.660]   Alex, is yours possible?
[02:01:52.660 --> 02:01:57.980]   So I don't know why it isn't because if it was, people would build it, I think, right?
[02:01:57.980 --> 02:02:01.020]   No, I mean, I think the good idea is I think you probably have to do it at the OS level,
[02:02:01.020 --> 02:02:02.020]   right?
[02:02:02.020 --> 02:02:03.020]   But I definitely do.
[02:02:03.020 --> 02:02:08.540]   I definitely think there's, I mean, presumably somebody could build like an application that
[02:02:08.540 --> 02:02:11.020]   had full access to everything to do it.
[02:02:11.020 --> 02:02:14.820]   But I think you could, but I think it would be one of those things like, because basically
[02:02:14.820 --> 02:02:18.460]   we're just talking about like on the Mac, for instance, we would just be kind of talking
[02:02:18.460 --> 02:02:20.700]   about like, Automator voice, right?
[02:02:20.700 --> 02:02:25.740]   Like, like something that they could, you know, use natural language to pull up, you know,
[02:02:25.740 --> 02:02:32.140]   the sorts of search things or to, you know, run macros, you know, that you're trying to
[02:02:32.140 --> 02:02:33.640]   accomplish.
[02:02:33.640 --> 02:02:35.140]   It definitely seems like it should be possible.
[02:02:35.140 --> 02:02:36.140]   Yeah.
[02:02:36.140 --> 02:02:37.140]   But just add a little hooks in every app.
[02:02:37.140 --> 02:02:38.140]   Yeah.
[02:02:38.140 --> 02:02:40.340]   I mean, to talk and pull and put things back into it.
[02:02:40.340 --> 02:02:42.340]   Which AppleScript basically already has.
[02:02:42.340 --> 02:02:43.340]   It's on a Mac.
[02:02:43.340 --> 02:02:44.340]   It's, that's what I'm saying.
[02:02:44.340 --> 02:02:48.340]   So basically it would just be kind of a way of like, you know, making like Apple scripts,
[02:02:48.340 --> 02:02:52.460]   you know, on the fly with your voice.
[02:02:52.460 --> 02:02:56.100]   And that's what I mean when I'm talking about those large language models is it's not just
[02:02:56.100 --> 02:02:57.100]   ingest.
[02:02:57.100 --> 02:03:01.620]   It's, well, I don't want to talk about what that out just will go with that.
[02:03:01.620 --> 02:03:02.620]   It's a dress.
[02:03:02.620 --> 02:03:03.620]   Thank you.
[02:03:03.620 --> 02:03:10.100]   Yes, it is, it is taking the translation of what you're saying in and then it's also
[02:03:10.100 --> 02:03:13.740]   able to then go from that and also tell the other thing.
[02:03:13.740 --> 02:03:15.300]   It talks in their languages too.
[02:03:15.300 --> 02:03:21.060]   That's what I think makes this so incredible and why what you're talking about seems entirely
[02:03:21.060 --> 02:03:26.500]   possible because as long as it has the context of this is how I have to talk to the machine.
[02:03:26.500 --> 02:03:32.260]   Luckily, with, with Mac OS, we do have a lot of those hooks already built in.
[02:03:32.260 --> 02:03:39.220]   It's just no one is able to write every single kind of program they need to do to use the
[02:03:39.220 --> 02:03:40.380]   system.
[02:03:40.380 --> 02:03:45.300]   But if the, if the, the AI could do that on the fly, then that would work.
[02:03:45.300 --> 02:03:46.300]   Yeah.
[02:03:46.300 --> 02:03:49.620]   Did you guys read the, the iOS 16.4 story?
[02:03:49.620 --> 02:03:52.220]   That wasn't the prep, the prep sheet for today's show.
[02:03:52.220 --> 02:03:55.780]   And did you fall asleep reading it because it was so fantastically boring?
[02:03:55.780 --> 02:03:56.780]   Oh my gosh.
[02:03:56.780 --> 02:03:57.780]   New emojis.
[02:03:57.780 --> 02:03:58.780]   Blah.
[02:03:58.780 --> 02:04:02.660]   Oh, well, mobile S7 figured out to the current level of technology.
[02:04:02.660 --> 02:04:07.460]   And frankly, every time I update my Mac to a new, whatever, it feels the exact same.
[02:04:07.460 --> 02:04:11.140]   And Windows 11 is Windows 10 with the button move to the middle.
[02:04:11.140 --> 02:04:12.140]   Okay.
[02:04:12.140 --> 02:04:13.140]   Whatever.
[02:04:13.140 --> 02:04:19.140]   The first OS that gets what we're describing, correct and functional is going to win.
[02:04:19.140 --> 02:04:20.140]   And we are.
[02:04:20.140 --> 02:04:21.140]   I think you're dead on.
[02:04:21.140 --> 02:04:22.140]   I mean, for something new and this is it.
[02:04:22.140 --> 02:04:26.540]   And I just, I'm curious to know when it comes and how good is it in the first iteration.
[02:04:26.540 --> 02:04:29.340]   And does it steal Michael's credit card information?
[02:04:29.340 --> 02:04:31.260]   I just, I think we're going there.
[02:04:31.260 --> 02:04:34.860]   And I think it's going to be awesome, especially for people who have low visibility, you know,
[02:04:34.860 --> 02:04:39.900]   limited hearing or speak a language that most computer programs are written in.
[02:04:39.900 --> 02:04:41.420]   It's going to be huge for accessibility.
[02:04:41.420 --> 02:04:43.940]   And I think it's going to open up computing to a lot more folks and it's going to make
[02:04:43.940 --> 02:04:44.940]   our lives better.
[02:04:44.940 --> 02:04:46.740]   So I'm pretty hype.
[02:04:46.740 --> 02:04:47.980]   Hype as can be.
[02:04:47.980 --> 02:04:51.260]   You know what else I'm hype about?
[02:04:51.260 --> 02:04:53.660]   It's club twit.
[02:04:53.660 --> 02:04:58.900]   You out there can join the club at twit.tv/clubtwit.
[02:04:58.900 --> 02:05:02.860]   And when you do, you are going to get a lot of great things.
[02:05:02.860 --> 02:05:10.500]   Our club is available for starting at $7 a month or $84 a year and joining the club
[02:05:10.500 --> 02:05:17.220]   get to access to ad free episodes of every single Twitch show, which is awesome.
[02:05:17.220 --> 02:05:23.100]   It's just all the content, none of the extra stuff because you in effect are the sponsor.
[02:05:23.100 --> 02:05:24.620]   You are supporting the show.
[02:05:24.620 --> 02:05:29.940]   And so in doing so, we say, hey, these are the ad free episodes of all of the Twitch shows.
[02:05:29.940 --> 02:05:33.780]   You also get access to the Twit Plus bonus feed that has extra content.
[02:05:33.780 --> 02:05:38.900]   You won't find it anywhere else behind the scenes, before the show, after the show.
[02:05:38.900 --> 02:05:43.620]   The events we hold in the club are often published there.
[02:05:43.620 --> 02:05:48.620]   It's a great feed of so much extra stuff that, you know, when you join, you get to look back
[02:05:48.620 --> 02:05:50.940]   through and find the things that you want.
[02:05:50.940 --> 02:05:54.220]   You also get access to the members only Discord server.
[02:05:54.220 --> 02:05:57.180]   That is a place where you can go to chat with your fellow club Twit members, but also
[02:05:57.180 --> 02:05:58.980]   those of us here at Twit.
[02:05:58.980 --> 02:06:03.340]   It is constantly popping, as they say.
[02:06:03.340 --> 02:06:04.780]   There's a lot going on in there.
[02:06:04.780 --> 02:06:08.500]   There's so many different channels where you can chat about different subjects.
[02:06:08.500 --> 02:06:12.180]   Many of us here, many of the hosts here are regularly there.
[02:06:12.180 --> 02:06:17.500]   My cohost for iOS today, her name is Rosemary Orchard, and she is regularly answering questions
[02:06:17.500 --> 02:06:19.020]   in the club as well.
[02:06:19.020 --> 02:06:23.660]   So that's a great place if you're wanting to develop a shortcut and you are trying to
[02:06:23.660 --> 02:06:25.580]   figure out how to make it work.
[02:06:25.580 --> 02:06:29.900]   Rosemary Orchard is often there answering those questions.
[02:06:29.900 --> 02:06:32.420]   You may have heard me say starting at $7 a month.
[02:06:32.420 --> 02:06:37.940]   That's because some folks, they were like, "Hey, honestly, you keep adding more stuff
[02:06:37.940 --> 02:06:44.180]   to the club and we feel like we would like to pay more than $7 a month because there's
[02:06:44.180 --> 02:06:45.460]   more value here than that."
[02:06:45.460 --> 02:06:51.980]   So we flipped the right switch and you can choose starting at $7 to sort of make your
[02:06:51.980 --> 02:06:53.140]   subscription what you want.
[02:06:53.140 --> 02:06:58.980]   So if you want to give $8 a month or $9 or whatever, you're able to do that.
[02:06:58.980 --> 02:07:02.860]   And I think part of the reason why that's the case is because as we've continued with
[02:07:02.860 --> 02:07:06.860]   the club, we've added more and more to it and we'll continue to do so.
[02:07:06.860 --> 02:07:12.580]   So there's the untitled Linux show that's club Twit exclusive is a show all about Linux.
[02:07:12.580 --> 02:07:17.860]   We've also got Hands on Windows, which is a great program from Paul Therat that is tips
[02:07:17.860 --> 02:07:20.260]   and tricks all about windows.
[02:07:20.260 --> 02:07:23.700]   You've got my show Hands on Mac and other clubs Twit exclusive.
[02:07:23.700 --> 02:07:29.180]   A short format show, meaning 10, 15, 20 minutes episodes that are covering tips and tricks
[02:07:29.180 --> 02:07:33.900]   for not just the Mac, but also iOS and iPadOS.
[02:07:33.900 --> 02:07:40.380]   As well as Scott Wilkinson's Home Theater Geeks, which is relaunched by way of the club.
[02:07:40.380 --> 02:07:46.460]   So if you have home theater stuff that you are curious about or you're thinking about
[02:07:46.460 --> 02:07:52.540]   your next purchase there, then Home Theater Geeks is a great place to go and learn more
[02:07:52.540 --> 02:07:53.620]   about that.
[02:07:53.620 --> 02:08:02.060]   So if that sounds good to you, please head to twit.tv/clubtwit and join the club.
[02:08:02.060 --> 02:08:07.100]   And thank you, thank you, thank you from the bottom of my heart and all of us here at Twit
[02:08:07.100 --> 02:08:10.140]   for choosing to be subscribers.
[02:08:10.140 --> 02:08:12.900]   You help keep this thing rolling along.
[02:08:12.900 --> 02:08:17.100]   You can keep the AC running so I don't die of heat stroke here in the studio.
[02:08:17.100 --> 02:08:19.860]   So thank you for that, all of you.
[02:08:19.860 --> 02:08:28.620]   And yeah, after you join the club, please let us know how you like it and Ant Pruitt, the
[02:08:28.620 --> 02:08:32.940]   community manager is always on the lookout making sure that we're providing the best
[02:08:32.940 --> 02:08:35.620]   experience possible.
[02:08:35.620 --> 02:08:40.700]   And with that, I think it's time to see what took place on Twit this week.
[02:08:40.700 --> 02:08:42.020]   This is a good answer.
[02:08:42.020 --> 02:08:45.060]   I asked who said mistakes were made.
[02:08:45.060 --> 02:08:48.140]   The phrase mistakes were made is often used in politics and business to acknowledge that
[02:08:48.140 --> 02:08:50.900]   something went wrong without taking responsibility for it.
[02:08:50.900 --> 02:08:51.900]   Honest to God.
[02:08:51.900 --> 02:08:52.900]   It's like what it says here.
[02:08:52.900 --> 02:08:53.900]   Are you open yet?
[02:08:53.900 --> 02:08:57.740]   It's like, well, we usually open at 930, but listen, it was a yes or no question, honey.
[02:08:57.740 --> 02:08:58.740]   Are you open?
[02:08:58.740 --> 02:08:59.740]   Just ask it.
[02:08:59.740 --> 02:09:02.020]   We just want to get to the point.
[02:09:02.020 --> 02:09:04.620]   Why is Paul's throat so angry?
[02:09:04.620 --> 02:09:11.580]   I do not have enough information about that person to help with your request previously
[02:09:11.580 --> 02:09:13.420]   on Twit.
[02:09:13.420 --> 02:09:14.420]   All about Android.
[02:09:14.420 --> 02:09:21.860]   The headline of my latest Samsung Galaxy S23 review is Samsung's Galaxy S23 is one of
[02:09:21.860 --> 02:09:26.580]   the best small Android phones you can buy right now.
[02:09:26.580 --> 02:09:33.780]   The camera algorithms on the S23 are so much better than they were on the last generation
[02:09:33.780 --> 02:09:35.060]   of the S22.
[02:09:35.060 --> 02:09:39.460]   This week in space, we're joined by Colonel Eileen Collins of the US Air Force, the first
[02:09:39.460 --> 02:09:42.540]   woman to pilot and command a space shuttle.
[02:09:42.540 --> 02:09:44.180]   Think about the stars.
[02:09:44.180 --> 02:09:51.900]   Set yourself like exciting goals, something that is possible, but set high goals and try
[02:09:51.900 --> 02:09:54.980]   to make a path to get there.
[02:09:54.980 --> 02:09:56.180]   Security now.
[02:09:56.180 --> 02:10:03.740]   Microsoft has started testing a cryptocurrency wallet, which they're planning to build into
[02:10:03.740 --> 02:10:07.220]   their edge browser.
[02:10:07.220 --> 02:10:15.060]   And an assets tab will let you stare lovingly at your NFTs Twit, which is all you can do
[02:10:15.060 --> 02:10:16.060]   with them.
[02:10:16.060 --> 02:10:17.060]   So good.
[02:10:17.060 --> 02:10:18.060]   Good.
[02:10:18.060 --> 02:10:19.060]   Exactly.
[02:10:19.060 --> 02:10:20.060]   Yeah.
[02:10:20.060 --> 02:10:24.780]   You know, I keep like five to 10 NFTs here in my jacket.
[02:10:24.780 --> 02:10:28.700]   If anybody would like one, I can pass those out after the show.
[02:10:28.700 --> 02:10:30.300]   They'll disappear before they make it to you.
[02:10:30.300 --> 02:10:32.100]   They're actually self-destructing NFTs.
[02:10:32.100 --> 02:10:35.460]   But, you know, that's kind.
[02:10:35.460 --> 02:10:37.060]   By their very nature.
[02:10:37.060 --> 02:10:41.860]   I thought we could end the show by talking about everybody's least favorite social media
[02:10:41.860 --> 02:10:43.260]   platform.
[02:10:43.260 --> 02:10:46.620]   It's Twitter.
[02:10:46.620 --> 02:10:54.460]   So there's been a lot kind of coming out since the open sourcing of Twitter's code,
[02:10:54.460 --> 02:10:57.300]   or at least parts of its code.
[02:10:57.300 --> 02:11:03.220]   And also the sort of changes that have, and in some cases have not taken place when it
[02:11:03.220 --> 02:11:11.620]   comes to Twitter blue, there were some specific Twitter accounts that had kind of their own
[02:11:11.620 --> 02:11:14.660]   little portion in Twitter code.
[02:11:14.660 --> 02:11:22.460]   So depending on which author it was, it could mean that their tweets were getting boosted
[02:11:22.460 --> 02:11:26.260]   and shown to more people than others.
[02:11:26.260 --> 02:11:32.100]   A platformer has a list of Twitter VIPs that are getting boosted over everyone else, including
[02:11:32.100 --> 02:11:46.140]   NBA All-Star, LeBron James, Ben Shapiro, Alexandria Ocasio-Cortez, or AOC, and President Joe Biden,
[02:11:46.140 --> 02:11:54.060]   as well as conservative commentator, Cat-Turd 2.
[02:11:54.060 --> 02:11:55.060]   Am I out of the loop?
[02:11:55.060 --> 02:11:59.420]   I didn't know there was someone named Cat-Turd 2 who was needing to be boosted to lots of
[02:11:59.420 --> 02:12:00.420]   people.
[02:12:00.420 --> 02:12:02.420]   Well, then you don't follow Donald Trump's tweets, truths.
[02:12:02.420 --> 02:12:05.780]   Sorry, I'm from Truth Social, where he's been citing a recent, and I hate to say this
[02:12:05.780 --> 02:12:11.340]   out a lot of their kids in the room, Cat-Turd Paul, in which he won over Ron DeSantis, and
[02:12:11.340 --> 02:12:14.260]   he's been quoting it, and it is surreal.
[02:12:14.260 --> 02:12:19.180]   This is where we really are.
[02:12:19.180 --> 02:12:25.340]   That Cat-Turd 2's polling is being cited by presidential candidates on their Twitter
[02:12:25.340 --> 02:12:29.260]   alternative social networks because they were banned from Twitter for two years.
[02:12:29.260 --> 02:12:31.260]   They're a lot of back now, but they don't want to be back.
[02:12:31.260 --> 02:12:36.340]   Right, because Truth is part of Truth Social, which is part of the Trump Media and Technology
[02:12:36.340 --> 02:12:39.940]   group, which was going to merge with another company in a spat combination, but now that's
[02:12:39.940 --> 02:12:42.580]   under investigation for shenanigans.
[02:12:42.580 --> 02:12:43.580]   Right.
[02:12:43.580 --> 02:12:45.900]   2023, I hope your bingo cards fall.
[02:12:45.900 --> 02:12:52.180]   I feel like someone just took a printed on a web page from TechCrunch, cut it up into
[02:12:52.180 --> 02:12:57.420]   little pieces, and then added in like a mad libs, and we're just pulling these little
[02:12:57.420 --> 02:13:01.380]   words out of a hat and just putting them together because I'm so lost here.
[02:13:01.380 --> 02:13:03.220]   Don't mock my writing process.
[02:13:03.220 --> 02:13:07.140]   Well, also, I was going to say, and added some chat GPT to it.
[02:13:07.140 --> 02:13:08.140]   Yeah.
[02:13:08.140 --> 02:13:10.100]   So that's going on too.
[02:13:10.100 --> 02:13:17.940]   I mean, so I don't know, when I saw this and I saw a similar little, I remember seeing
[02:13:17.940 --> 02:13:21.900]   something specifically where it was showing parts of the code and it had author in.
[02:13:21.900 --> 02:13:28.100]   Yeah, well, no, because the recommendation algorithm or whatever is now on GitHub and
[02:13:28.100 --> 02:13:30.100]   I'm not talking about the leaked one.
[02:13:30.100 --> 02:13:31.940]   I'm talking about the other one.
[02:13:31.940 --> 02:13:36.220]   That was the thing from last week where there was a DMCA takedown notice.
[02:13:36.220 --> 02:13:37.220]   I don't know.
[02:13:37.220 --> 02:13:40.380]   I'm talking about the actual thing, Twitter has uploaded stuff.
[02:13:40.380 --> 02:13:44.860]   It's on GitHub, and then people were going through the code in real time, or like, "Hey,
[02:13:44.860 --> 02:13:46.940]   Elan, how come you're getting boosted so much more?
[02:13:46.940 --> 02:13:47.940]   Why are you here?"
[02:13:47.940 --> 02:13:49.340]   And he was like, "I don't know.
[02:13:49.340 --> 02:13:52.340]   This is a complete shock to me."
[02:13:52.340 --> 02:13:53.340]   Cut to stories.
[02:13:53.340 --> 02:13:59.420]   I think also from a platformer several weeks ago where engineers were basically instructed,
[02:13:59.420 --> 02:14:01.900]   make sure Elan is coming.
[02:14:01.900 --> 02:14:05.860]   His engagement is high because he's looking at his numbers very closely, and if they
[02:14:05.860 --> 02:14:07.700]   fall, he doesn't want to hear that.
[02:14:07.700 --> 02:14:11.300]   So they're like, "All right, well, I guess we better put him on everybody's 4U page because
[02:14:11.300 --> 02:14:15.260]   we got to give him all the feels."
[02:14:15.260 --> 02:14:19.140]   What I hate is the tick-tockification of every single service.
[02:14:19.140 --> 02:14:20.540]   I don't want a 4U page.
[02:14:20.540 --> 02:14:22.620]   My Twitter feed is selected by me.
[02:14:22.620 --> 02:14:25.140]   I don't want Spotify to become tick-tock.
[02:14:25.140 --> 02:14:27.020]   But we need to go back.
[02:14:27.020 --> 02:14:30.300]   Abra, can you say the name of Twitter's owner out loud for us?
[02:14:30.300 --> 02:14:31.300]   Yes, thank you.
[02:14:31.300 --> 02:14:33.220]   Let's go down the list.
[02:14:33.220 --> 02:14:34.220]   Elan, ask.
[02:14:34.220 --> 02:14:38.260]   Okay, so I go with the Elon, and then Christina, can you say it again?
[02:14:38.260 --> 02:14:40.540]   Elon, I say those the wrong way.
[02:14:40.540 --> 02:14:41.540]   Yeah.
[02:14:41.540 --> 02:14:42.540]   But that's a French word.
[02:14:42.540 --> 02:14:44.940]   Elan with a little kick over the A. I mean, it's entirely different.
[02:14:44.940 --> 02:14:48.500]   This is like I call my company Yahoo, and no one appreciates it.
[02:14:48.500 --> 02:14:49.500]   Interesting.
[02:14:49.500 --> 02:14:50.980]   I've never heard that.
[02:14:50.980 --> 02:14:52.700]   I've never heard that.
[02:14:52.700 --> 02:14:58.140]   Once again, now we've switched to some sort of blog about languages, and we're cutting
[02:14:58.140 --> 02:15:00.140]   out words for man.
[02:15:00.140 --> 02:15:05.420]   I did want to, but to go back to this, were any of you surprised by this?
[02:15:05.420 --> 02:15:10.660]   I don't think this just seemed obvious to me that some Twitter accounts were in the
[02:15:10.660 --> 02:15:12.460]   algorithm part of things.
[02:15:12.460 --> 02:15:20.420]   Of course, if I had learned that somehow in my timeline that was just reverse chronological,
[02:15:20.420 --> 02:15:22.180]   they were somehow boosting things.
[02:15:22.180 --> 02:15:23.180]   That's a different story.
[02:15:23.180 --> 02:15:31.580]   The algorithm, did anyone believe that it was anything other than it's most popular and
[02:15:31.580 --> 02:15:34.500]   also just those that they choose to be most popular?
[02:15:34.500 --> 02:15:38.820]   Or were we really hoping and believing and thinking that this was just completely?
[02:15:38.820 --> 02:15:42.500]   These are the most engaged tweets on the platform.
[02:15:42.500 --> 02:15:44.500]   Was it a surprise to anybody?
[02:15:44.500 --> 02:15:51.980]   No, but I think that the, like the reasonness in which this was, like, and to be clear, I'm
[02:15:51.980 --> 02:15:54.780]   not at all suggesting that this started with Elon.
[02:15:54.780 --> 02:15:57.780]   See, I'll say it the way that everyone else has that way.
[02:15:57.780 --> 02:16:02.180]   I'm not at all trying to suggest that it started with Elon, but I, Elon, whatever.
[02:16:02.180 --> 02:16:03.180]   I love it.
[02:16:03.180 --> 02:16:04.180]   No, say it your way.
[02:16:04.180 --> 02:16:05.180]   Say it your way.
[02:16:05.180 --> 02:16:06.180]   I like it.
[02:16:06.180 --> 02:16:07.180]   All right.
[02:16:07.180 --> 02:16:08.180]   All right.
[02:16:08.180 --> 02:16:09.180]   All right.
[02:16:09.180 --> 02:16:10.180]   All on.
[02:16:10.180 --> 02:16:12.380]   I'm not saying that it started with him, but I am, they're clearly have been changes,
[02:16:12.380 --> 02:16:13.380]   and you can see it.
[02:16:13.380 --> 02:16:19.500]   It's funny in what they literally have, that the project title is the algorithm.
[02:16:19.500 --> 02:16:24.660]   That's what it's called in the Twitter organization on GitHub.
[02:16:24.660 --> 02:16:31.860]   But just to see the brazenness about some of this is just see, like, yes, this, this,
[02:16:31.860 --> 02:16:35.860]   these people and especially this small man does have to be boosted.
[02:16:35.860 --> 02:16:37.620]   That's just really funny to me.
[02:16:37.620 --> 02:16:41.340]   It feels different than like TikTok's algorithm, right?
[02:16:41.340 --> 02:16:44.580]   So it's like, TikTok's algorithm, you see something and you're like, this was meant
[02:16:44.580 --> 02:16:45.580]   to reach me.
[02:16:45.580 --> 02:16:46.580]   Like, they're not.
[02:16:46.580 --> 02:16:47.580]   Correct.
[02:16:47.580 --> 02:16:51.260]   You feel like, you know, all the stars have aligned.
[02:16:51.260 --> 02:16:55.940]   And with Twitter, I scroll through the for you page and I'm like, why?
[02:16:55.940 --> 02:16:58.820]   Like sometimes I'm like, okay, great.
[02:16:58.820 --> 02:17:02.660]   But I feel like it feels a lot more forced and especially reading this, seeing how forced
[02:17:02.660 --> 02:17:08.740]   it really is, it feels inauthentic and it just makes being on Twitter that much more
[02:17:08.740 --> 02:17:09.740]   miserable.
[02:17:09.740 --> 02:17:10.740]   I feel like, no, look at it.
[02:17:10.740 --> 02:17:13.420]   And it's like, you don't understand me.
[02:17:13.420 --> 02:17:17.460]   And if you're trying to say you do, then you're making me feel bad about myself.
[02:17:17.460 --> 02:17:23.300]   If this is the person that I am coming across as, whereas TikTok, it's like, oh man, you
[02:17:23.300 --> 02:17:24.500]   just get me, don't you?
[02:17:24.500 --> 02:17:25.500]   I just feel some seeds.
[02:17:25.500 --> 02:17:26.500]   Right.
[02:17:26.500 --> 02:17:27.500]   Right.
[02:17:27.500 --> 02:17:28.500]   I was gonna say, I don't, I'm with Alex.
[02:17:28.500 --> 02:17:31.540]   I don't like the TikTok vacation or everything, but that's mostly because these other places
[02:17:31.540 --> 02:17:33.300]   can't do it as well as TikTok does.
[02:17:33.300 --> 02:17:34.300]   Yeah.
[02:17:34.300 --> 02:17:35.300]   Yeah.
[02:17:35.300 --> 02:17:36.300]   So it was just like a, it's just a bad version, right?
[02:17:36.300 --> 02:17:39.220]   So I'm like, well, no, TikTok actually does get me and it freaks me out sometimes, but
[02:17:39.220 --> 02:17:45.260]   I love it, whereas like this, no, this is your idea of what you think based on something.
[02:17:45.260 --> 02:17:47.740]   Even the YouTube algorithm gets me pretty well.
[02:17:47.740 --> 02:17:51.620]   Like, and, and I like it because it changes a lot because I'll get into other things and
[02:17:51.620 --> 02:17:54.380]   it'll be like, oh, remember this thing from six months ago?
[02:17:54.380 --> 02:17:56.100]   But it'll be like a one off.
[02:17:56.100 --> 02:18:00.620]   It'll know not to show me stuff that I was really into two years ago.
[02:18:00.620 --> 02:18:02.980]   But yeah, you're exactly right.
[02:18:02.980 --> 02:18:06.740]   The Twitter thing, it's just, yeah, it just doesn't work.
[02:18:06.740 --> 02:18:11.380]   Oh, they've done, they've done a bad job and they should, they should feel bad.
[02:18:11.380 --> 02:18:12.380]   Exactly.
[02:18:12.380 --> 02:18:17.580]   Speaking of that, the another thing I think they've done a bad job with is the sort of
[02:18:17.580 --> 02:18:19.820]   changes in Twitter blue.
[02:18:19.820 --> 02:18:27.940]   And we have seen not only several news organizations and other publications try to, or not try
[02:18:27.940 --> 02:18:31.860]   to, but actually say there's no way we're going to be paying for this special type of
[02:18:31.860 --> 02:18:33.180]   check mark on the platform.
[02:18:33.180 --> 02:18:36.720]   But we've also seen several celebrities who are saying to, no, we won't be able to
[02:18:36.720 --> 02:18:38.260]   be doing that.
[02:18:38.260 --> 02:18:45.020]   And I wonder because we talked, you know, when we, we talk about sort of the, the moves that
[02:18:45.020 --> 02:18:51.980]   a company is making a lot of the time you see a company, once they've announced that
[02:18:51.980 --> 02:18:54.460]   decision, they really do move forward with it.
[02:18:54.460 --> 02:19:03.780]   But because of the fragility of the, of Elmo, then you have a situation where it's incredibly
[02:19:03.780 --> 02:19:07.080]   mercurial, incredibly volatile, and who knows what will happen.
[02:19:07.080 --> 02:19:11.800]   So I'm curious, do you think there's going to be another, another change for Twitter
[02:19:11.800 --> 02:19:12.800]   blue?
[02:19:12.800 --> 02:19:18.680]   If all of the people that they would hope would be part of this thousand dollar, or however
[02:19:18.680 --> 02:19:21.960]   much it is, it might be more than that, a subscription service.
[02:19:21.960 --> 02:19:24.520]   How are they going to, this doesn't seem to be a way that they're going to make a lot
[02:19:24.520 --> 02:19:28.400]   of money if all of the news organizations are going, nah, not for us.
[02:19:28.400 --> 02:19:33.640]   Are you trying to say that someone who's very good at one thing may not be good at another
[02:19:33.640 --> 02:19:34.640]   thing?
[02:19:34.640 --> 02:19:35.640]   I may be saying that.
[02:19:35.640 --> 02:19:36.640]   Yeah.
[02:19:36.640 --> 02:19:38.640]   If I put that together, yeah, one plus one equals three.
[02:19:38.640 --> 02:19:39.640]   Yeah.
[02:19:39.640 --> 02:19:43.220]   There's a, there's a, I think it's a, it's either a Charlie Munger or Warren Buffett quote,
[02:19:43.220 --> 02:19:46.980]   but they said, and I'm going to paraphrase here, when someone who is a reputation for
[02:19:46.980 --> 02:19:50.780]   being good at business goes into an industry that has a reputation for being a bad place
[02:19:50.780 --> 02:19:56.520]   to do business, the reputation of the business itself usually is the one that wins out.
[02:19:56.520 --> 02:19:58.120]   And social media is a hard place to be.
[02:19:58.120 --> 02:20:02.680]   Twitter's always been the kind of like sickly child of the major social media platforms.
[02:20:02.680 --> 02:20:06.320]   It was doing medium before he bought it and loaded it up with debt.
[02:20:06.320 --> 02:20:07.320]   And now it's struggling.
[02:20:07.320 --> 02:20:11.440]   And I think this is just, this is death flails from someone trying to figure out how to pay
[02:20:11.440 --> 02:20:14.480]   off 14 billion dollars, that's really not for Tesla stock.
[02:20:14.480 --> 02:20:20.080]   But I mean, I paid for Twitter blue originally because I wanted to support my home on that.
[02:20:20.080 --> 02:20:21.080]   Same.
[02:20:21.080 --> 02:20:25.120]   Like, you know, when Reddit launched gold, I contributed when it came out because I used
[02:20:25.120 --> 02:20:27.520]   Reddit, you know, and so I was here for it.
[02:20:27.520 --> 02:20:33.840]   But when Elon made it a referendum on your approval of his personality, I was out.
[02:20:33.840 --> 02:20:34.840]   Yeah.
[02:20:34.840 --> 02:20:38.920]   And I think that a lot of folks, much like how I probably wouldn't buy a Tesla now because
[02:20:38.920 --> 02:20:43.200]   I don't want to be conflated with an Elon stand.
[02:20:43.200 --> 02:20:46.960]   You know, I think he's seen the other side of the goal of personality here.
[02:20:46.960 --> 02:20:49.160]   Can I tell you, I quite literally had that.
[02:20:49.160 --> 02:20:53.440]   So this is, this was how much it was affecting me.
[02:20:53.440 --> 02:20:57.120]   In that moment, what you're talking about not being conflated with an Elon stand.
[02:20:57.120 --> 02:21:07.240]   I had met some new friends and a lot of the people that I spend time around don't have
[02:21:07.240 --> 02:21:08.240]   Twitter.
[02:21:08.240 --> 02:21:11.440]   And so on the rare occasion when there is someone who does have Twitter, I'm always
[02:21:11.440 --> 02:21:16.320]   very excited and I want to follow them and, you know, make that.
[02:21:16.320 --> 02:21:22.120]   And so I found out that my friend had Twitter and I was about to follow him on Twitter.
[02:21:22.120 --> 02:21:25.120]   And then I had this realization, they're going to see this blue check and they're going to
[02:21:25.120 --> 02:21:31.480]   think I'm an, an Elon supporter and that I'm like paying for this service.
[02:21:31.480 --> 02:21:34.760]   And so I was like, you know what, I'm just going to write.
[02:21:34.760 --> 02:21:39.000]   I'm going to feel like an idiot doing so, but I'm going to write like a paragraph going.
[02:21:39.000 --> 02:21:40.560]   So listen, I really want to follow you.
[02:21:40.560 --> 02:21:43.520]   But then I had this moment where I thought, if I follow you, you're going to do it.
[02:21:43.520 --> 02:21:44.520]   And so I did.
[02:21:44.520 --> 02:21:47.240]   And I sent it to them and they thought it was hilarious and were like, Oh no, you know,
[02:21:47.240 --> 02:21:48.720]   I wasn't worried about that or whatever.
[02:21:48.720 --> 02:21:49.720]   And it's fine.
[02:21:49.720 --> 02:21:52.760]   But yeah, I didn't want to be conflated with that.
[02:21:52.760 --> 02:21:58.160]   And so that was kind of the anti branding that was in place there.
[02:21:58.160 --> 02:22:04.480]   And now it's all up in the air anyway, in terms of will this, will these blue check marks
[02:22:04.480 --> 02:22:06.560]   stay or will they go?
[02:22:06.560 --> 02:22:11.360]   And will any of it, will any of it matter going forth?
[02:22:11.360 --> 02:22:15.640]   Because it sounds like every time we're all logging on to the platform, a lot of times
[02:22:15.640 --> 02:22:17.600]   it just makes us not feel very good.
[02:22:17.600 --> 02:22:22.520]   And I don't find myself, I still have it and I still occasionally open the app, but I don't
[02:22:22.520 --> 02:22:26.080]   find myself tweeting all that often and I don't find myself checking into the app all that
[02:22:26.080 --> 02:22:27.440]   often.
[02:22:27.440 --> 02:22:34.200]   And I don't know have any of you, because I think, Christina, I have not seen you do
[02:22:34.200 --> 02:22:36.000]   much on like Mastodon.
[02:22:36.000 --> 02:22:38.200]   Are you using any of their platforms?
[02:22:38.200 --> 02:22:40.040]   No, I'm on Mastodon.
[02:22:40.040 --> 02:22:43.480]   And I'm on Mastodon fairly frequently, but it's a weird thing.
[02:22:43.480 --> 02:22:47.760]   It's like when I had to step off from Twitter a little bit, just because frankly, a the
[02:22:47.760 --> 02:22:49.520]   algorithm got worse.
[02:22:49.520 --> 02:22:54.080]   So even the suggested things, the stuff that I would see got demonstrably worse.
[02:22:54.080 --> 02:22:57.720]   So it's not bad enough that they're trying to copy TikTok, but they're doing a worse
[02:22:57.720 --> 02:23:02.000]   job than they even used to do when they were copying Instagram.
[02:23:02.000 --> 02:23:07.000]   So an engagement and other stuff is messed up and then a certain chunk of your social
[02:23:07.000 --> 02:23:10.920]   graph goes away and you're like, where are they now?
[02:23:10.920 --> 02:23:16.240]   And so it's a consequence to all of that is that I'm not as engaged with all those things.
[02:23:16.240 --> 02:23:19.680]   It's weird because I can still be on Mastodon for all of my tech stuff.
[02:23:19.680 --> 02:23:20.680]   And that's great.
[02:23:20.680 --> 02:23:21.680]   And there's a great community there.
[02:23:21.680 --> 02:23:22.880]   But none of my memes are there.
[02:23:22.880 --> 02:23:23.880]   None of my health cultures are.
[02:23:23.880 --> 02:23:25.840]   None of my Swifties are there.
[02:23:25.840 --> 02:23:31.320]   And so I can't get into the ridiculous stuff that happens.
[02:23:31.320 --> 02:23:36.360]   And so you still kind of have to be there, but it does feel like my home is died or at
[02:23:36.360 --> 02:23:43.080]   least it's been blown up and we're now rebuilding, but we don't really like the new dictator.
[02:23:43.080 --> 02:23:48.760]   And like you, like Alex, I used to pay for Twitter blue because I wanted to support my
[02:23:48.760 --> 02:23:52.600]   home and I've been verified for a very long time and it had nothing to do with that.
[02:23:52.600 --> 02:23:55.680]   I was like, no, I actually, I had a lot of friends who worked at Twitter and I knew people
[02:23:55.680 --> 02:23:56.680]   who were working on things.
[02:23:56.680 --> 02:23:58.040]   It was like, I want to support.
[02:23:58.040 --> 02:24:01.120]   Like people have always said, you know, if you like something, pay for it, I want to
[02:24:01.120 --> 02:24:04.920]   actually put my money around my mouth is and I did it happily.
[02:24:04.920 --> 02:24:10.520]   And now he's even ruined that because you don't want to, you know, this thing that was
[02:24:10.520 --> 02:24:14.800]   so funny, this thing that was a status symbol for a lot of people is no longer a status symbol.
[02:24:14.800 --> 02:24:16.440]   It's an anti-satisf symbol.
[02:24:16.440 --> 02:24:21.760]   And then this plan to get people to pay for accounts, you know, you have people who are
[02:24:21.760 --> 02:24:25.240]   some of your biggest users who are like, absolutely not.
[02:24:25.240 --> 02:24:29.200]   Although what is interesting and some of my own reporting had had borne this out too was
[02:24:29.200 --> 02:24:33.760]   that there were a number of publications who were getting and other big companies who were
[02:24:33.760 --> 02:24:38.080]   getting, they got emails, you know, like last month or something, basically being like,
[02:24:38.080 --> 02:24:44.000]   hey, we will give you your free gold verified handle and you can have up to five affiliate
[02:24:44.000 --> 02:24:45.000]   accounts.
[02:24:45.000 --> 02:24:47.760]   But if you want to have more than that, then you're going to have to pay $1,000 a month
[02:24:47.760 --> 02:24:49.480]   and $50 per affiliate.
[02:24:49.480 --> 02:24:52.800]   So they were already kind of seeding to, I'm assuming they're biggest advertisers to
[02:24:52.800 --> 02:24:54.600]   try to keep them involved.
[02:24:54.600 --> 02:24:59.600]   But it is very interesting to see all these celebrities and like big publishers being
[02:24:59.600 --> 02:25:01.240]   like, no, we're not going to pay.
[02:25:01.240 --> 02:25:05.760]   Now a number of them, I'm sure, were already getting them for free because they were advertisers.
[02:25:05.760 --> 02:25:11.880]   But it is seeing how like you're really are cutting off your nose despite your face to
[02:25:11.880 --> 02:25:14.080]   get that $96 a year.
[02:25:14.080 --> 02:25:16.320]   It's going great.
[02:25:16.320 --> 02:25:20.680]   On top of the verified checkmarks, kind of losing all meaning, I remember when we were
[02:25:20.680 --> 02:25:24.080]   all notified that we could only have two FA if we were Twitter blue subscribers.
[02:25:24.080 --> 02:25:30.120]   I was like, okay, so I'm risking my online security because I'm not paying $8 a month
[02:25:30.120 --> 02:25:31.120]   for Twitter blue.
[02:25:31.120 --> 02:25:34.040]   It's just like, it's been going downhill for a while.
[02:25:34.040 --> 02:25:36.680]   And I don't know how sustainable at all is really.
[02:25:36.680 --> 02:25:43.440]   Yeah, we had someone call earlier into ask the tech guys and this person had a Twitter
[02:25:43.440 --> 02:25:45.800]   handle that was a pretty common one.
[02:25:45.800 --> 02:25:51.160]   And so she would regularly get the emails that said, you know, your Twitter account is
[02:25:51.160 --> 02:25:54.480]   trying is, you're trying to reset your Twitter account.
[02:25:54.480 --> 02:25:57.640]   Here's the, you know, use this to change your password or whatever.
[02:25:57.640 --> 02:26:03.200]   And so because of that, she had concerns that someone would attempt to hack her Twitter
[02:26:03.200 --> 02:26:05.440]   account just in general.
[02:26:05.440 --> 02:26:08.080]   So she had two factor authentication turned on.
[02:26:08.080 --> 02:26:13.320]   So then when Twitter made this change, she was getting a notification that was saying,
[02:26:13.320 --> 02:26:16.560]   hey, we're sending you your two factor authentication code.
[02:26:16.560 --> 02:26:17.720]   She would type it in.
[02:26:17.720 --> 02:26:20.880]   But then as soon as she went in, it would not let her use her account until she turned
[02:26:20.880 --> 02:26:22.760]   off two factor authentication.
[02:26:22.760 --> 02:26:26.560]   But because she had a very common Twitter handle, she was worried that in that period
[02:26:26.560 --> 02:26:30.760]   of time where she went to make the change from using a phone number to using the code
[02:26:30.760 --> 02:26:36.680]   that someone would somehow gain access to her account and be, you know, take control
[02:26:36.680 --> 02:26:37.680]   of it.
[02:26:37.680 --> 02:26:44.440]   And so I like had this moment of just, you know, sadness thinking about someone who is,
[02:26:44.440 --> 02:26:50.080]   you know, so concerned and wants to be as protected as they possibly can be online, who
[02:26:50.080 --> 02:26:54.080]   then has this, this moment of going, what do I do in this situation?
[02:26:54.080 --> 02:27:00.280]   And it just, it adds unnecessary fear and it's fun as it were.
[02:27:00.280 --> 02:27:02.760]   But I don't think needs to be there.
[02:27:02.760 --> 02:27:10.120]   And all because of, you know, looking for more ways to bring in money that hasn't happened
[02:27:10.120 --> 02:27:11.840]   for the company, frankly, yet.
[02:27:11.840 --> 02:27:12.840]   Yeah.
[02:27:12.840 --> 02:27:13.840]   Go ahead.
[02:27:13.840 --> 02:27:17.080]   I think it'll be, sorry, I also just think it'll be interesting to compare this to Meta
[02:27:17.080 --> 02:27:18.080]   Verified.
[02:27:18.080 --> 02:27:22.120]   And I'm surprised that another company kind of borrowed that model, but like, you know,
[02:27:22.120 --> 02:27:24.560]   they're not taking away people's legacy checkmarks.
[02:27:24.560 --> 02:27:29.280]   But I think it'll be really interesting to see how they kind of navigate that and how
[02:27:29.280 --> 02:27:33.920]   they take that business model and compare that to what ends up happening with Twitter.
[02:27:33.920 --> 02:27:34.920]   Yeah.
[02:27:34.920 --> 02:27:35.920]   No, I think that's a great point.
[02:27:35.920 --> 02:27:39.960]   And the interesting thing about Meta Verified, A, I think that in the one hand, like, it's
[02:27:39.960 --> 02:27:44.200]   just an easier way for people who really want to have that verified Instagram or Facebook
[02:27:44.200 --> 02:27:45.200]   thing to do it.
[02:27:45.200 --> 02:27:49.480]   But B, they are essentially offering a little bit of like a value add by bait, basically
[02:27:49.480 --> 02:27:54.840]   saying we will have, you will have better customer service and will respond to you more.
[02:27:54.840 --> 02:27:58.680]   And so if you were a business or someone, you could kind of see that as being worth paying
[02:27:58.680 --> 02:27:59.680]   for.
[02:27:59.680 --> 02:28:03.040]   I worry that it feels a little bit kind of like a bait and switch thing where they're
[02:28:03.040 --> 02:28:05.960]   going to be like, Hey, verified people will show up more in your feed.
[02:28:05.960 --> 02:28:09.880]   And so isn't this a great way for your business to be more visible on Facebook or whatever?
[02:28:09.880 --> 02:28:13.400]   But I mean, whatever, that is what it is.
[02:28:13.400 --> 02:28:19.240]   But it, it, the way they're doing it seems to be make much more sense than whatever this,
[02:28:19.240 --> 02:28:20.240]   this Twitter thing is.
[02:28:20.240 --> 02:28:23.280]   And the latest rumor, and I don't know how true any of this is.
[02:28:23.280 --> 02:28:26.800]   I've asked some of my friends who used to work in Twitter engineering and have not been
[02:28:26.800 --> 02:28:28.200]   able to confirm this.
[02:28:28.200 --> 02:28:32.680]   But the rumor is that the reason that they can't just like run like a mask, like script
[02:28:32.680 --> 02:28:37.720]   to debarify everyone is that that would break a bunch of other things.
[02:28:37.720 --> 02:28:40.960]   So it has to be a manual process.
[02:28:40.960 --> 02:28:46.400]   And so you can imagine like the pain and who knows how many places it has to be changed.
[02:28:46.400 --> 02:28:50.360]   So you can imagine the amount of work that that would entail that is probably going to
[02:28:50.360 --> 02:28:56.840]   be left to one engineer who has 16 other jobs because most of the engineering staff is gone.
[02:28:56.840 --> 02:29:01.160]   So who even knows how long it will be before we lose our blue checks?
[02:29:01.160 --> 02:29:03.320]   You know, if that even happens at all.
[02:29:03.320 --> 02:29:04.320]   What a night.
[02:29:04.320 --> 02:29:05.320]   And I thought about this too.
[02:29:05.320 --> 02:29:09.960]   The other thing is like, I hope that somewhere there are backups of all of this because what
[02:29:09.960 --> 02:29:15.960]   happens in the future when the, if the company ends up surviving its current state and is
[02:29:15.960 --> 02:29:21.200]   in the hands of someone who's trying to keep it going and bring it to a place that it once
[02:29:21.200 --> 02:29:23.120]   was at its heyday or what have you.
[02:29:23.120 --> 02:29:28.480]   And then all of that information that came along with those verifications and everything
[02:29:28.480 --> 02:29:33.920]   that was involved with that, that legacy data I guess is just gone because there's so much
[02:29:33.920 --> 02:29:36.920]   that's been messed about as things have changed.
[02:29:36.920 --> 02:29:44.080]   Just in general, that sort of loss of a timeline of information is kind of sad to me.
[02:29:44.080 --> 02:29:51.600]   I understand the internet is volatile, but it's in some ways it is a history of humanity
[02:29:51.600 --> 02:29:53.160]   from the point of the internet on.
[02:29:53.160 --> 02:29:55.800]   And so yeah, that kind of is upsetting.
[02:29:55.800 --> 02:29:56.800]   Yeah.
[02:29:56.800 --> 02:30:05.520]   I mean, I've been on Twitter since 2008, I think, you know, that was when my, my A star
[02:30:05.520 --> 02:30:09.560]   with a one, which was now pretty far, you know, in the past.
[02:30:09.560 --> 02:30:12.320]   So I, my whole adult life, you know, I've been on Twitter.
[02:30:12.320 --> 02:30:13.320]   Same.
[02:30:13.320 --> 02:30:14.880]   I've been on hundreds of thousands of tweets.
[02:30:14.880 --> 02:30:18.080]   I deleted my tweets a couple of times and they were always in the six figures, which
[02:30:18.080 --> 02:30:19.520]   is embarrassing.
[02:30:19.520 --> 02:30:22.040]   But there's nothing else still.
[02:30:22.040 --> 02:30:24.040]   I mean, I mean, I love y'all.
[02:30:24.040 --> 02:30:25.040]   Massedons trash.
[02:30:25.040 --> 02:30:26.360]   I don't want to use it.
[02:30:26.360 --> 02:30:27.360]   It's too hard.
[02:30:27.360 --> 02:30:28.360]   I couldn't figure it out.
[02:30:28.360 --> 02:30:29.360]   It was too different.
[02:30:29.360 --> 02:30:30.360]   I just want tweets.
[02:30:30.360 --> 02:30:31.360]   I just want Twitter.
[02:30:31.360 --> 02:30:32.360]   That's all I want.
[02:30:32.360 --> 02:30:33.360]   It works for me.
[02:30:33.360 --> 02:30:34.360]   It works for my brain, my workflow.
[02:30:34.360 --> 02:30:35.360]   I don't read email.
[02:30:35.360 --> 02:30:36.360]   I read tweets.
[02:30:36.360 --> 02:30:40.920]   Like I, that's what I have up on my screen and just losing like recently my Twitter client
[02:30:40.920 --> 02:30:42.000]   tweet and stopped working.
[02:30:42.000 --> 02:30:45.320]   I'd not have to use this crappy web tweet deck thing.
[02:30:45.320 --> 02:30:47.840]   It's just getting worse and worse and worse.
[02:30:47.840 --> 02:30:52.640]   And there's going to come a day when I realize that it is no longer for me and my brain worth
[02:30:52.640 --> 02:30:53.640]   it.
[02:30:53.640 --> 02:30:56.520]   And then I'm just going to have a sub stack, which is fine.
[02:30:56.520 --> 02:30:58.680]   But like, but it's, we've lost our home.
[02:30:58.680 --> 02:31:00.760]   No, I mean, it's, it's, it's a similar thing.
[02:31:00.760 --> 02:31:07.880]   Like I, I, I've been on Twitter for 16 years and, you know, like, I'm, there are some of
[02:31:07.880 --> 02:31:12.440]   the Maston apps are better, like a elk.zone is a very, very good web interface.
[02:31:12.440 --> 02:31:13.440]   Elk.zone.
[02:31:13.440 --> 02:31:14.440]   What am I going?
[02:31:14.440 --> 02:31:15.440]   Buck hunting?
[02:31:15.440 --> 02:31:19.440]   Basically, but, but, but what's great about it is basically they've recreated the Twitter
[02:31:19.440 --> 02:31:21.600]   interface back before the Twitter interface became bad.
[02:31:21.600 --> 02:31:23.160]   So that's really good.
[02:31:23.160 --> 02:31:28.160]   There's a, there's ivory, which is tweet bot, but in a for, for Maston.
[02:31:28.160 --> 02:31:32.040]   So there are some things that are better, but I'm not going to at all try to, yeah,
[02:31:32.040 --> 02:31:33.280]   argue with you that it is better.
[02:31:33.280 --> 02:31:37.560]   I mean, and you're, you're Alex, you're part of like the, the first name club, you know,
[02:31:37.560 --> 02:31:43.640]   so your account, I'm sure you've been trying to get like hacked, you know, many, many times
[02:31:43.640 --> 02:31:44.800]   over the years.
[02:31:44.800 --> 02:31:46.400]   All those emails go to a Gmail account.
[02:31:46.400 --> 02:31:47.400]   I don't check.
[02:31:47.400 --> 02:31:48.400]   Nice.
[02:31:48.400 --> 02:31:49.400]   Exactly.
[02:31:49.400 --> 02:31:53.840]   I, I had the opportunity to be at Christina, but I didn't want to like pull clout and,
[02:31:53.840 --> 02:31:55.160]   and make them give it to me.
[02:31:55.160 --> 02:31:56.160]   And I should have.
[02:31:56.160 --> 02:31:59.680]   And I still regret this, like, like 14 years later.
[02:31:59.680 --> 02:32:04.000]   But, um, yeah, uh, it, there is nothing else.
[02:32:04.000 --> 02:32:07.040]   We're just all kind of going to all these other diasporas.
[02:32:07.040 --> 02:32:08.040]   Yeah.
[02:32:08.040 --> 02:32:09.040]   And trying them.
[02:32:09.040 --> 02:32:12.760]   I, I, that's what I keep doing is trying to see if anything else matches what my experience
[02:32:12.760 --> 02:32:14.880]   has been on, on Twitter.
[02:32:14.880 --> 02:32:18.760]   Uh, I think I joined in November of 2008.
[02:32:18.760 --> 02:32:21.080]   Um, and yeah, it has been my home.
[02:32:21.080 --> 02:32:24.800]   It's where I've had, you know, the, it's my biggest following.
[02:32:24.800 --> 02:32:32.640]   It is where I have, um, made a lot of the contacts that resulted in my current, you know,
[02:32:32.640 --> 02:32:34.200]   what I do now.
[02:32:34.200 --> 02:32:36.720]   And it, I like, I love, I love.
[02:32:36.720 --> 02:32:38.560]   I love Twitter and what is provided.
[02:32:38.560 --> 02:32:43.280]   I think I've just kind of gone, I've crawled away from the internet in general in terms
[02:32:43.280 --> 02:32:47.040]   of sort of sharing and, uh, consuming.
[02:32:47.040 --> 02:32:50.560]   And so that's why I don't find myself on it or other platforms.
[02:32:50.560 --> 02:32:54.320]   And I just, I, I'm in all the different places, but I'm just not using any of them.
[02:32:54.320 --> 02:32:59.480]   And I want, I wish that I had that, uh, original experience that I had with Twitter back.
[02:32:59.480 --> 02:33:06.680]   Um, I did want to ask if anyone has joined or attempted to join, uh, T2, which is made
[02:33:06.680 --> 02:33:09.280]   by some, uh, former Twitter engineers.
[02:33:09.280 --> 02:33:10.280]   Yeah.
[02:33:10.280 --> 02:33:13.640]   I signed up yesterday because I heard they were getting free blue checks.
[02:33:13.640 --> 02:33:14.640]   Yeah.
[02:33:14.640 --> 02:33:15.640]   All of the legacy blue checks.
[02:33:15.640 --> 02:33:16.640]   That's what I did too.
[02:33:16.640 --> 02:33:17.640]   Just for that reason.
[02:33:17.640 --> 02:33:20.600]   So I, so, so I, I did send off a thing to that, but I don't know.
[02:33:20.600 --> 02:33:25.960]   I haven't checked my email to see if, uh, I got anything, uh, T2.social for folks who
[02:33:25.960 --> 02:33:28.440]   are, uh, listening is the name of it.
[02:33:28.440 --> 02:33:35.920]   And yes, if you have a, uh, legacy verified account, then you basically, um, can have
[02:33:35.920 --> 02:33:41.280]   that like confirmed, I think you send them a direct message with a specific word or whatever.
[02:33:41.280 --> 02:33:43.800]   And then it'll kind of bring it over.
[02:33:43.800 --> 02:33:50.760]   So I joined the platform because I saw that and it, they have basically, uh, in the interviews
[02:33:50.760 --> 02:33:56.360]   that they've taken, they've said, kind of like what, um, Instagram did whenever they
[02:33:56.360 --> 02:33:59.800]   were talking about how they copied Snapchat stories where they're just like, yeah, we're,
[02:33:59.800 --> 02:34:00.800]   we're copying Snapchat.
[02:34:00.800 --> 02:34:02.560]   Uh, T2 has kind of said the same thing.
[02:34:02.560 --> 02:34:06.880]   Yeah, it's pretty much just a wholesale copy of Twitter, uh, in its original state.
[02:34:06.880 --> 02:34:09.360]   So that part sounded promising.
[02:34:09.360 --> 02:34:11.200]   Um, but I don't know.
[02:34:11.200 --> 02:34:16.720]   I just, it doesn't seem like any of these are catching on as a whole.
[02:34:16.720 --> 02:34:19.520]   And none of it feels like Twitter once did ultimately.
[02:34:19.520 --> 02:34:20.520]   All right.
[02:34:20.520 --> 02:34:21.520]   I joined the waitlist.
[02:34:21.520 --> 02:34:22.520]   Amen.
[02:34:22.520 --> 02:34:23.520]   Nice.
[02:34:23.520 --> 02:34:27.560]   I'm not in yet, but I'll be there and maybe this will be the one maybe, maybe who knows?
[02:34:27.560 --> 02:34:29.000]   I remember friend feed.
[02:34:29.000 --> 02:34:30.000]   That was fun.
[02:34:30.000 --> 02:34:31.000]   A friend feed was great.
[02:34:31.000 --> 02:34:35.440]   And I mean, hey, look, that's what got Brett Taylor to Facebook.
[02:34:35.440 --> 02:34:38.840]   I know, I know, you know, I was going to say if everybody anybody on this thing, you know,
[02:34:38.840 --> 02:34:43.480]   I was going to say if anybody does, but if friend feed was great, I shouldn't be dismissive
[02:34:43.480 --> 02:34:48.520]   about arcane facts that normal people don't have in their brain because they weren't covering
[02:34:48.520 --> 02:34:50.480]   social media back in the dark ages.
[02:34:50.480 --> 02:34:51.960]   I think it's a very good point.
[02:34:51.960 --> 02:34:52.960]   Yes, it is.
[02:34:52.960 --> 02:34:54.680]   Uh, it is a very good point.
[02:34:54.680 --> 02:34:58.120]   Um, I think, uh, I think we've done it.
[02:34:58.120 --> 02:35:04.360]   Um, we have given Alex enough time to not have to, uh, let the baby go to sleep.
[02:35:04.360 --> 02:35:06.640]   Hopefully there's no way she's down.
[02:35:06.640 --> 02:35:08.280]   She won't be down until nine 30.
[02:35:08.280 --> 02:35:10.840]   I mean, I'm just joining the war on the second act.
[02:35:10.840 --> 02:35:14.520]   Well, that means there'll be lots of music to listen to, right?
[02:35:14.520 --> 02:35:16.080]   Uh, lots of Taylor Swift.
[02:35:16.080 --> 02:35:17.680]   So that's good at least.
[02:35:17.680 --> 02:35:19.720]   There's a new acoustic version of lavender haze.
[02:35:19.720 --> 02:35:22.720]   So I heard I haven't had to hear it yet, but I'm so excited.
[02:35:22.720 --> 02:35:24.320]   Well, there you go.
[02:35:24.320 --> 02:35:25.800]   That's something you could look forward to.
[02:35:25.800 --> 02:35:29.800]   Ah, well, Christina Warren, senior developer advocate at GitHub.
[02:35:29.800 --> 02:35:32.080]   Thank you so much for your time.
[02:35:32.080 --> 02:35:33.080]   Thank you for having me.
[02:35:33.080 --> 02:35:34.080]   It's so good to be on.
[02:35:34.080 --> 02:35:35.080]   So good to be with you on your first show.
[02:35:35.080 --> 02:35:37.120]   You're hosting big, big, big share.
[02:35:37.120 --> 02:35:38.120]   I love it.
[02:35:38.120 --> 02:35:39.120]   Thank you so much.
[02:35:39.120 --> 02:35:40.120]   Thank you.
[02:35:40.120 --> 02:35:45.320]   And Alex Wilhelm, reporter at TechCrunch, who has a much longer title that he didn't
[02:35:45.320 --> 02:35:46.960]   want to share with us today.
[02:35:46.960 --> 02:35:54.680]   Um, have her of a great bookcase and a listener of many a swift, uh, song.
[02:35:54.680 --> 02:35:55.680]   Thank you for your time.
[02:35:55.680 --> 02:35:57.400]   Uh, all that's lovely.
[02:35:57.400 --> 02:35:59.800]   I just want to say doubly what Christina just said.
[02:35:59.800 --> 02:36:01.000]   You did a lovely job.
[02:36:01.000 --> 02:36:02.440]   It's a big chair to sit in.
[02:36:02.440 --> 02:36:05.680]   We all know exactly how Leo sounds and how he goes through things.
[02:36:05.680 --> 02:36:09.320]   So just, I mean, I've been on your other show on, on the network, but I think you did
[02:36:09.320 --> 02:36:10.320]   a fantastic job today.
[02:36:10.320 --> 02:36:11.880]   So thanks for having me.
[02:36:11.880 --> 02:36:13.040]   And uh, let's do it again.
[02:36:13.040 --> 02:36:14.040]   Awesome.
[02:36:14.040 --> 02:36:15.200]   Thanks so much, Alex.
[02:36:15.200 --> 02:36:19.520]   And last, but certainly not least, video host and producer for CNET.
[02:36:19.520 --> 02:36:20.960]   I, I just think you're great.
[02:36:20.960 --> 02:36:23.960]   Abrail Heade, thank you so much for joining me.
[02:36:23.960 --> 02:36:24.960]   Thank you so much.
[02:36:24.960 --> 02:36:26.240]   I want to echo what they both said.
[02:36:26.240 --> 02:36:29.960]   You are such an incredible host and it's such an honor to be on your first show doing
[02:36:29.960 --> 02:36:30.960]   this.
[02:36:30.960 --> 02:36:31.960]   So thank you for having us.
[02:36:31.960 --> 02:36:32.960]   Thanks everybody.
[02:36:32.960 --> 02:36:35.320]   You're going to have me all tearing up here at the end.
[02:36:35.320 --> 02:36:38.160]   I appreciate it and sincerely appreciate your time.
[02:36:38.160 --> 02:36:40.960]   I know this can be a real long show.
[02:36:40.960 --> 02:36:45.960]   And so, uh, yeah, uh, on a Sunday, it, it means a lot that you're here to join me to
[02:36:45.960 --> 02:36:47.960]   talk tech.
[02:36:47.960 --> 02:36:51.040]   This has been this week in tech.
[02:36:51.040 --> 02:36:56.280]   I will have another, of course, this week of tech next week with another host while Leo
[02:36:56.280 --> 02:36:57.280]   is out.
[02:36:57.280 --> 02:37:01.200]   But thank you all out there for joining us for this episode.
[02:37:01.200 --> 02:37:04.480]   And I hope you have a great geek week.
[02:37:04.480 --> 02:37:08.200]   Yeah, I'm saying the thing that Leo usually says at the end of ask the tech guys at the
[02:37:08.200 --> 02:37:10.800]   end of this week in tech because I didn't get to say it earlier.
[02:37:10.800 --> 02:37:12.960]   So goodbye everybody.
[02:37:12.960 --> 02:37:14.960]   Thank you for joining us.
[02:37:14.960 --> 02:37:17.640]   You didn't say and that's another twin.
[02:37:17.640 --> 02:37:19.640]   Oh, another kid is in the camp.
[02:37:19.640 --> 02:37:20.640]   There you go.
[02:37:20.640 --> 02:37:21.640]   This is amazing.
[02:37:21.640 --> 02:37:22.640]   Do the twin.
[02:37:22.640 --> 02:37:23.640]   Do the twin.
[02:37:23.640 --> 02:37:24.640]   All right.
[02:37:24.640 --> 02:37:25.640]   Do the twin, baby.
[02:37:25.640 --> 02:37:26.640]   Do the twin.
[02:37:26.640 --> 02:37:27.640]   All right.
[02:37:27.640 --> 02:37:28.640]   Do the twin.
[02:37:28.640 --> 02:37:29.640]   All right.
[02:37:29.640 --> 02:37:30.640]   Do the twin.
[02:37:30.640 --> 02:37:33.220]   (upbeat music)

