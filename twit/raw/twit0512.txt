;FFMETADATA1
title=The Wombat Test
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=512
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons Attribution Non-Commercial Share-Alike license. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2015
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.260]   It's time for Twit this week in Tech. Great panel for you Ian Thompson from the
[00:00:04.260 --> 00:00:08.760]   Register. Serenity Caldwell from I'm or Matthew Ingram from Fortune. We're gonna
[00:00:08.760 --> 00:00:12.800]   talk about all the announcements Google made at I/O. What Apple might talk about
[00:00:12.800 --> 00:00:18.780]   at WWDC and the lady who brought in an Apple One computer for recycling. It's
[00:00:18.780 --> 00:00:22.480]   all coming up next on Twit.
[00:00:22.480 --> 00:00:33.900]   NetCasts you love from people you trust. This is Twit. Bandwidth for this
[00:00:33.900 --> 00:00:45.800]   weekend tech is provided by CashFly at C A C A C A F L Y dot com. This is Twit
[00:00:45.800 --> 00:00:54.440]   this weekend tech episode 512 recorded Sunday May 31st 2015 the wombat test.
[00:00:54.440 --> 00:00:59.240]   This weekend tech is brought to you by Personal Capital with Personal Capital
[00:00:59.240 --> 00:01:03.880]   you get award-winning financial tools unbiased advice and a transparent view
[00:01:03.880 --> 00:01:08.380]   of all your investments best of all it's free to sign up go to personal
[00:01:08.380 --> 00:01:13.960]   capital dot com slash Twit and buy Gazelle the fast and simple way to sell
[00:01:13.960 --> 00:01:18.520]   your used gadgets find out what your used iPhone iPad or other Apple product is
[00:01:18.520 --> 00:01:25.000]   worth at Gazelle dot com and buy Dropbox for business Dropbox for
[00:01:25.000 --> 00:01:29.240]   business lets your team sync and share files just like Dropbox and you can
[00:01:29.240 --> 00:01:33.900]   connect Dropbox for business with over 300,000 apps for project management visit
[00:01:33.900 --> 00:01:39.480]   Dropbox dot com slash Twit for a free 14-day trial that's Dropbox dot com
[00:01:39.480 --> 00:01:46.200]   slash Twit and buy little bits the easy way to build electronics with modular
[00:01:46.200 --> 00:01:50.640]   building blocks go to little bits dot com slash Twit and you'll receive $20
[00:01:50.640 --> 00:01:57.720]   off your first kit plus free shipping in the US it's time for Twit this week in
[00:01:57.720 --> 00:02:04.040]   tech show we cover the week's tech news proudly live and in living color Ian
[00:02:04.040 --> 00:02:08.560]   Thompson is here from the register good to see you see you see you see that I should
[00:02:08.560 --> 00:02:12.640]   warn you that he just tweeted on another with this week in tech show we'll try to
[00:02:12.640 --> 00:02:18.560]   keep it clean but after a tough week dot dot dot well has it been a tough week
[00:02:18.560 --> 00:02:22.600]   for you you weren't at Google our Google I owe no it was a tough week all right
[00:02:22.600 --> 00:02:27.280]   really was a tough week oh good a bunch of other stuff and it was yeah would
[00:02:27.280 --> 00:02:31.880]   sashday morning just like right turned all the alarms off I'm gonna sleep in
[00:02:31.880 --> 00:02:35.400]   to 11 o'clock and then at 830 rolled over into a puddle of cat vomit so oh
[00:02:35.400 --> 00:02:41.200]   that is a tough week I still have a scar we have kittens I was scar on my back you
[00:02:41.200 --> 00:02:44.960]   can't get sick from a kitty scratch can you wouldn't have thought like something
[00:02:44.960 --> 00:02:50.160]   called cat scratch fever or anything is there there is okay if you stop
[00:02:50.160 --> 00:02:56.600]   biting your shirt also with us from fortune I'm happy to say
[00:02:56.600 --> 00:03:01.880]   formerly of giga on the famous Matthew Ingram from you know during the week you
[00:03:01.880 --> 00:03:06.600]   join us in your office but today you're a lot yeah I'm out of friends I was
[00:03:06.600 --> 00:03:11.440]   actually very nice that looks very comfy it is well I want to thank you both
[00:03:11.440 --> 00:03:15.280]   for being here I really appreciate that serenity Caldwell is stuck in traffic
[00:03:15.280 --> 00:03:20.200]   but she'll be with us she's in Boston so stuck in traffic in Boston means she
[00:03:20.200 --> 00:03:25.040]   could be with us sometime in the next few weeks I'll let you know I'll keep
[00:03:25.040 --> 00:03:28.720]   eyes posted we'll get her here yeah in a bit you know if you think you had a
[00:03:28.720 --> 00:03:33.440]   bad week I gotta think the product manager for apples photos app had a bad
[00:03:33.440 --> 00:03:39.480]   week well I never want to flicker as well yeah yeah it's yeah they I kind of
[00:03:39.480 --> 00:03:42.280]   got the fitting Google oversold the photo thing but it's certainly really
[00:03:42.280 --> 00:03:45.280]   interesting what they're doing with it I don't know if they oversold it if you
[00:03:45.280 --> 00:03:48.960]   played with it I have played with it and then I took it immediately off my phone
[00:03:48.960 --> 00:03:54.080]   once I read the terms and conditions oh what do they say they own all our
[00:03:54.080 --> 00:03:58.100]   pictures yep oh that's pretty standard yeah yeah I know it's
[00:03:58.100 --> 00:04:01.720]   covering but at the same time it's just like right I'm gonna test this out in
[00:04:01.720 --> 00:04:05.000]   the office privately with a couple of photos I really couldn't give a monkeys
[00:04:05.000 --> 00:04:09.740]   about so it's but yeah I mean it looks pretty good it looks better than what
[00:04:09.740 --> 00:04:13.820]   apples got at the moment certainly it's ironic it's the same name as well yeah
[00:04:13.820 --> 00:04:17.240]   well there's only so many things you can call pictures of people so you know I
[00:04:17.240 --> 00:04:22.340]   guess they're all gonna use it can't trade my word Apple only has
[00:04:22.340 --> 00:04:26.740]   themselves to blame let's face it I mean this is a company with eight hundred
[00:04:26.740 --> 00:04:29.520]   billion dollars or something and they haven't been able to figure out how to
[00:04:29.520 --> 00:04:34.360]   get a cloud service or properly their photo their photo thing has gone through
[00:04:34.360 --> 00:04:38.080]   multiple revisions and it's like punching yourself in the face repeatedly
[00:04:38.080 --> 00:04:41.620]   every time you try to use it it's just makes me want to throw my computer out
[00:04:41.620 --> 00:04:47.640]   the window I unlike you Ian uploaded every photo I've ever taken to this
[00:04:47.640 --> 00:04:55.060]   me too I have well over 39,000 photos on here now and the categorization alone
[00:04:55.060 --> 00:05:03.100]   makes that worthwhile so it did this all here's my son Henry it did I didn't
[00:05:03.100 --> 00:05:07.260]   I didn't maybe I'll choose me instead that's probably we'll give kids thank you
[00:05:07.260 --> 00:05:11.240]   track them from childhood to adulthood with any with any mistake that's an
[00:05:11.240 --> 00:05:15.620]   interesting question cuz I know as Henry's here both as a child and as a 20
[00:05:15.620 --> 00:05:19.700]   year old but Abby is not here both as a child 20 year old and have plenty of
[00:05:19.700 --> 00:05:27.100]   pictures of both yeah so I and then it's places and then it's things including
[00:05:27.100 --> 00:05:35.540]   cars skylines stadiums let's just see cats how how many cat pictures lots of
[00:05:35.540 --> 00:05:40.020]   cat you do like your cats well they're new and of course you know the new the
[00:05:40.020 --> 00:05:45.220]   new thing gets all the pictures dogs there were a few errors it categorized
[00:05:45.220 --> 00:05:52.100]   some cats as dogs they categorized some stone lions as dogs but it's pretty I
[00:05:52.100 --> 00:05:57.100]   mean this I didn't do anything this I uploaded these pictures don't there's
[00:05:57.100 --> 00:06:02.340]   there's a stone lion it thinks is a cat a dog I think it's not stone stone
[00:06:02.340 --> 00:06:06.100]   doing to these creatures but and there's three cat pictures that it thought were
[00:06:06.100 --> 00:06:11.100]   dogs but you know flickers doing this to right and obviously there were some
[00:06:11.100 --> 00:06:15.020]   massive breakthrough in artificial intelligence and image recognition in
[00:06:15.020 --> 00:06:18.340]   the last few years because it seems like everybody does this now but it's still
[00:06:18.340 --> 00:06:24.980]   incredibly useful that is not a that is not a dog that is a lion shaped cat
[00:06:24.980 --> 00:06:32.660]   but yeah but still it's it is kind of amazing that they could go through
[00:06:32.660 --> 00:06:38.380]   almost 40,000 pictures of mine in less than a day fully categorized them
[00:06:38.380 --> 00:06:42.020]   without any intervention on my part and they've actually been doing this for a
[00:06:42.020 --> 00:06:47.660]   while I know flicker just came out with it recently but I've had 50,000 60,000
[00:06:47.660 --> 00:06:52.500]   photos at Google+ photos which is basically the same thing and once I
[00:06:52.500 --> 00:06:56.900]   turned on Google's personal search I could go to a Google search box and type
[00:06:56.900 --> 00:07:01.740]   in my photos of Caitlin our oldest daughter and it would pull up every
[00:07:01.740 --> 00:07:05.980]   photo I've ever taken it's really amazing I just typed in a search for
[00:07:05.980 --> 00:07:11.660]   Paris in the snow and let's see if it finds anything I know I have pictures of
[00:07:11.660 --> 00:07:17.100]   Paris and yes it's snowy you don't find that mind boggling no no I think it's
[00:07:17.100 --> 00:07:21.020]   great I just don't have that many photos that I want to put in the cloud
[00:07:21.020 --> 00:07:23.820]   call me Mr. Karen Bolder
[00:07:23.820 --> 00:07:29.380]   to me what is Google gonna do sell my I mean what do you what do you think
[00:07:29.380 --> 00:07:32.780]   they're gonna do sell no I don't see any particular point to it I back all my
[00:07:32.780 --> 00:07:36.980]   photos up onto a hard drive a month put them onto another hard drive and take
[00:07:36.980 --> 00:07:40.460]   that into work so I've got dual backups on two in two locations I leave it at
[00:07:40.460 --> 00:07:44.300]   that I don't really access that many photos unless I know I'm gonna look
[00:07:44.300 --> 00:07:48.620]   go and do need something specific so as long as I've got copies of home and
[00:07:48.620 --> 00:07:53.300]   work then you know I'm not really sold on sticking on the cloud I'm glad I'm
[00:07:53.300 --> 00:08:00.020]   really glad that I had a Google+ auto photo backup turned on so every photo I
[00:08:00.020 --> 00:08:05.140]   take on my phone automatically gets uploaded because my backpack got stolen
[00:08:05.140 --> 00:08:12.340]   when we were in Italy laptop iPad and an external hard drive with every photo
[00:08:12.340 --> 00:08:16.580]   I've ever taken so someone in Italy has all my photos I'd much rather Google
[00:08:16.580 --> 00:08:21.340]   had them but so now I didn't have to worry about not having a laptop because
[00:08:21.340 --> 00:08:26.900]   all the photos that I own exists you know on Google servers I can reach them
[00:08:26.900 --> 00:08:30.940]   wherever as long as I have an internet connection yeah I think that theft is
[00:08:30.940 --> 00:08:33.860]   where I'm gonna come a cropper with my approach because it you know that it's
[00:08:33.860 --> 00:08:36.940]   all well and good but if somebody does half inch your backpack and runs off
[00:08:36.940 --> 00:08:40.500]   with it you are a little bit stuffed that's a good that's a good example of
[00:08:40.500 --> 00:08:45.260]   how it works well but yeah initially as well it's happens an awful lot we just
[00:08:45.260 --> 00:08:47.900]   came back from there last summer and awful lot of people lost their bags
[00:08:47.900 --> 00:08:51.500]   retargeted a trey rectal if yesterday he had exactly the same thing happen
[00:08:51.500 --> 00:09:00.580]   but you know I trade is like a Buddhist or something because he just said well
[00:09:00.580 --> 00:09:06.940]   it just happens and that's life it's the photo tax I just bought new gear and
[00:09:06.940 --> 00:09:11.420]   he just I would have been I was gonna say I want that head on the spike out
[00:09:11.420 --> 00:09:12.420]   out
[00:09:12.420 --> 00:09:17.420]   you know the cloud certainly made a big difference for us because like five
[00:09:17.420 --> 00:09:22.180]   six years ago everything I owned would have been on that computer including all
[00:09:22.180 --> 00:09:25.580]   my photos that's right and I would have been heartbroken you know all these
[00:09:25.580 --> 00:09:30.500]   pictures of my kids and stuff but as it was now I just get to upgrade my
[00:09:30.500 --> 00:09:35.900]   laptop I couldn't care less is there a what is going on with cloud storage
[00:09:35.900 --> 00:09:42.380]   prices is I you know for Google to offer this base I mean okay it's probably
[00:09:42.380 --> 00:09:46.700]   less than a tear unlimited is probably less than a terabyte in 99% of the cases
[00:09:46.700 --> 00:09:51.860]   but still offering millions of people a terabyte of storage for free is not
[00:09:51.860 --> 00:09:56.340]   insignificant how can they do this but I think they must know that most people
[00:09:56.340 --> 00:10:00.660]   aren't going to use that much right most people are going to put up a bunch of
[00:10:00.660 --> 00:10:06.140]   recent photos maybe some old you know keep six but they're not going to be like
[00:10:06.140 --> 00:10:10.860]   us they're not going to upload 75 gigabytes of photos I don't think yeah I
[00:10:10.860 --> 00:10:14.740]   mean there are limits as well in that you know maximum 16 megapixel for images
[00:10:14.740 --> 00:10:20.100]   right maximum 1080p for videos and then when they put them on there they can
[00:10:20.100 --> 00:10:23.340]   press them that they can press them down and muck about with it so you can't
[00:10:23.340 --> 00:10:26.260]   stick raw files up on there you can't stick you can but you'll pay for it
[00:10:26.260 --> 00:10:29.460]   well if you pay yeah I mean once you Google Drive allowance has gone up then
[00:10:29.460 --> 00:10:33.420]   yeah it's ten dollars I have a terabyte on Google Drive so because I because you
[00:10:33.420 --> 00:10:38.420]   do too yeah because of the pixels yeah so I get like one three I'm I'm kind of a
[00:10:38.420 --> 00:10:45.100]   cloud backup I'm sort of platform agnostic go ahead you use the word
[00:10:45.100 --> 00:10:50.060]   slut you know you want it yeah or or or I'm obsessed with redundancy so I
[00:10:50.060 --> 00:10:56.540]   actually pay Amazon yeah and Dropbox and Google me too have a terabyte yeah
[00:10:56.540 --> 00:11:02.180]   Microsoft offers unlimited storage if you're a office 365 subscribers so I have
[00:11:02.180 --> 00:11:07.820]   that Facebook will take every picture admittedly crushed yeah you know into
[00:11:07.820 --> 00:11:12.620]   nothingness but they'll do it there's no use there's no reason to lose a picture
[00:11:12.620 --> 00:11:17.620]   ever again let's put it no let's say that thing that the thing that's great
[00:11:17.620 --> 00:11:22.780]   about Google photos is so I've got like you I've got you know 50,000 60,000
[00:11:22.780 --> 00:11:27.740]   photos but I don't have time to tag on the photos yeah and you can't find the
[00:11:27.740 --> 00:11:31.980]   one you want or you can't you remember oh yeah it was a photo of so and so or
[00:11:31.980 --> 00:11:36.260]   maybe it was in Paris or maybe like at least this gives you the ability to kind
[00:11:36.260 --> 00:11:41.300]   of go through them without taking hours to try and find a single photo or tag a
[00:11:41.300 --> 00:11:45.580]   photo I just typed Australia now normally I would go in the lightroom I
[00:11:45.580 --> 00:11:50.340]   would go through all the photos I would tag them right a collection I didn't
[00:11:50.340 --> 00:11:54.580]   have to do any of that here's all the photos I took in Australia you know by
[00:11:54.580 --> 00:12:00.300]   the way what kind what is that animal what do you call that is it a wombat is
[00:12:00.300 --> 00:12:06.700]   it a wombat it's a wall I think it's a wombat alright here's a test wombat we
[00:12:06.700 --> 00:12:13.860]   know there's a wombat picture in here somewhere it passed the wombat test
[00:12:13.860 --> 00:12:21.460]   no I did not expect that away there's the name of the show right there one
[00:12:21.460 --> 00:12:25.540]   bat test spelt on this is pretty now there's more than one wombat picture it
[00:12:25.540 --> 00:12:30.260]   didn't get them all so it's only a you know kind of a moderate wombat test
[00:12:30.260 --> 00:12:34.060]   pass you know it's because there's another one but that's a wombat from
[00:12:34.060 --> 00:12:38.340]   behind yeah so you can't expect it to really that's why that's behind I could
[00:12:38.340 --> 00:12:43.540]   just be yeah they could just be a ball of fur but it got the big one wow you
[00:12:43.540 --> 00:12:49.140]   didn't say wombat behind you just said wombat yeah wait a minute you really
[00:12:49.140 --> 00:12:52.620]   okay okay wait a minute now I'm not gonna say wombat behind but let's try
[00:12:52.620 --> 00:12:59.220]   Tasmanian devil you think it'll recognize that oh yeah yeah well if it did wombat
[00:12:59.220 --> 00:13:05.980]   Tasmanian devil from behind oh oh did I spell Tasmanian wrong I did got a
[00:13:05.980 --> 00:13:12.640]   spell it right Google's it passed the wombat test but it failed the
[00:13:12.640 --> 00:13:15.900]   Tasmanian devil test you went all the way to Australia and you didn't get a
[00:13:15.900 --> 00:13:19.380]   picture of a drop bear what is going on what is a drop bear drop bears that
[00:13:19.380 --> 00:13:22.700]   terrible beast they drop out the trees and attack you you can only deal with
[00:13:22.700 --> 00:13:26.380]   them by smearing mama it's on the top of your head you are such a liar no no
[00:13:26.380 --> 00:13:30.300]   it's an old Australian thing the truth the drop there drop there and it's the
[00:13:30.300 --> 00:13:34.900]   drop there you know the Tasmanian devil scary enough I gotta tell you I don't
[00:13:34.900 --> 00:13:38.740]   know why Google didn't know that that was a Tasmanian devil maybe you thought it
[00:13:38.740 --> 00:13:42.820]   was a cat I should look through the cat picture and see just a very angry cat
[00:13:42.820 --> 00:13:46.620]   I've pretty much been looking animal pretty much everything in Australia is
[00:13:46.620 --> 00:13:53.020]   deadly for you even the beer yeah yeah spiders that'll paralyze you instantly
[00:13:53.020 --> 00:13:57.660]   oh nice yeah that's great my favorites the tiny little shellfish that's got a
[00:13:57.660 --> 00:14:00.900]   stinger in it yeah hits you yeah some people have actually tried to kill
[00:14:00.900 --> 00:14:04.940]   themselves from the pain it's just oh that's nice right something look forward
[00:14:04.940 --> 00:14:13.180]   to future anyway that was I think from a consumer point of view that was
[00:14:13.180 --> 00:14:17.700]   probably the big announcement of Google yeah yeah and I want to you know
[00:14:17.700 --> 00:14:21.460]   apples got WWDC coming up you think that they they're kind of scrambling to try
[00:14:21.460 --> 00:14:25.180]   to respond to that no they don't care I don't honestly think Apple gives a
[00:14:25.180 --> 00:14:29.580]   monkeys about Google about Google I mean the thing was also I mean photo is
[00:14:29.580 --> 00:14:35.580]   great but they also announced pay and the fingerprint sensing thing which Apple
[00:14:35.580 --> 00:14:38.660]   was doing right here we go they're playing catch up in other regards yeah
[00:14:38.660 --> 00:14:42.700]   he swings around about some that one so Google interesting though Ben Thompson
[00:14:42.700 --> 00:14:47.940]   and I were talking on Twitter about how Apple you know devices are what they're
[00:14:47.940 --> 00:14:52.500]   all about right so services in a way are just a way to try and get people to
[00:14:52.500 --> 00:14:57.060]   buy and use their devices and Google's the exact opposite so it doesn't actually
[00:14:57.060 --> 00:15:01.620]   care about devices devices are just a way to get people to use Google services
[00:15:01.620 --> 00:15:06.580]   which helps explain why Apple devices are so great and their services are
[00:15:06.580 --> 00:15:11.940]   terrible and Google's services are great but their devices aren't so good it
[00:15:11.940 --> 00:15:17.340]   also means it's tough to compete with Google if for instance Apple it's not an
[00:15:17.340 --> 00:15:20.380]   Apple's interest to give away cloud storage I mean that's not a profit
[00:15:20.380 --> 00:15:24.340]   center they need to they need to they need to charge for that yeah but Google
[00:15:24.340 --> 00:15:28.500]   because they make money elsewhere otherwise kind of right in this kind of
[00:15:28.500 --> 00:15:33.540]   nebulous use the net more we'll make more money kind of thing they can and
[00:15:33.540 --> 00:15:37.300]   they could put a lot of resources into it that makes it very hard to compete
[00:15:37.300 --> 00:15:40.980]   it's although if you think about it Apple has a hundred and forty billion
[00:15:40.980 --> 00:15:46.500]   dollars in cash cash and near cash that's a mind boggling amount of money if if
[00:15:46.500 --> 00:15:51.580]   you wanted to get smart about a cloud service could you could just hire people
[00:15:51.580 --> 00:15:56.060]   and buy things and build things I mean if they really really wanted to it's not
[00:15:56.060 --> 00:15:59.100]   as though their resource constraint well yeah but it's not even it's not on
[00:15:59.100 --> 00:16:01.500]   their business model because Apple customers of people who will pay a
[00:16:01.500 --> 00:16:06.300]   price premium for what they consider to be good and be it even if it isn't very
[00:16:06.300 --> 00:16:10.740]   good so I mean it's they're really quite happy to keep prices high and let
[00:16:10.740 --> 00:16:14.260]   Google race for the bottom on that one there's an advantage though that Google
[00:16:14.260 --> 00:16:18.500]   has and this is where Apple might come up against come a cropper to use a phrase
[00:16:18.500 --> 00:16:23.980]   you might have used Ian if I'd let you is that Google because they give away
[00:16:23.980 --> 00:16:27.860]   all these services because they have all these photos are going to be able to do
[00:16:27.860 --> 00:16:32.020]   deep data mining in ways that Apple because they are yeah you know exactly
[00:16:32.020 --> 00:16:36.820]   promoting privacy can't you already see this with Google now but I wonder I
[00:16:36.820 --> 00:16:41.420]   mean I wonder how soon before I get a photographic analytics from Google you
[00:16:41.420 --> 00:16:46.260]   know 32% of your photos or a food the caloric content of most of the pictures
[00:16:46.260 --> 00:16:53.700]   is high hello serenity Caldwell welcome rain hi great to have you a
[00:16:53.700 --> 00:16:57.660]   traffic in Boston being what it is I'm amazed you're here at all but thank you
[00:16:57.660 --> 00:17:03.940]   yes no I am glad to be there no the I think it's really interesting Google's
[00:17:03.940 --> 00:17:08.740]   sort of yeah so of course there was the interview with Bradley Horowitz that
[00:17:08.740 --> 00:17:13.260]   Stephen Levy did over on medium I think the day that this was announced at the
[00:17:13.260 --> 00:17:18.020]   keynote and in that that in that interview has a couple of very interesting
[00:17:18.020 --> 00:17:22.900]   sound bites including the fun fact where they're like oh well we have no current
[00:17:22.900 --> 00:17:26.980]   plans to use all of our analytical data that we're mining from your photos for
[00:17:26.980 --> 00:17:33.060]   advertising but we're not ruling it out if say and he used something really funny
[00:17:33.060 --> 00:17:38.420]   like oh if say Tesla had a recall and we noticed that you had a Tesla in the
[00:17:38.420 --> 00:17:44.340]   photos we'd be able to tell you about that and wouldn't that be good isn't that
[00:17:44.340 --> 00:17:48.540]   good or does that was that not good well it's potential to be good yeah well
[00:17:48.540 --> 00:17:53.300]   here's the thing I don't know people people tend to take the the argument of
[00:17:53.300 --> 00:17:57.100]   like Google is evil because they're mining your data or Google is wonderful
[00:17:57.100 --> 00:18:01.260]   because they're trying to make your life easier and I don't think it's it's a
[00:18:01.260 --> 00:18:05.420]   company it's not a person it doesn't it doesn't be like let's let's yeah
[00:18:05.420 --> 00:18:12.580]   exactly it just depends on what you value from your services it might even be
[00:18:12.580 --> 00:18:17.780]   evil accidentally right yeah actually set out to be evil exactly like I think
[00:18:17.780 --> 00:18:21.540]   they actually have you know I wrote a pretty a pretty not necessarily critical
[00:18:21.540 --> 00:18:27.140]   piece earlier this week but just kind of a like just be aware of the
[00:18:27.140 --> 00:18:30.940]   potential of what Google could do for your photos which is for one one hand
[00:18:30.940 --> 00:18:34.060]   it's going to make it much easier for you to search your photos because you
[00:18:34.060 --> 00:18:38.780]   will be able to get that quick real time oh auto-faced detection and all of
[00:18:38.780 --> 00:18:43.620]   that which is good yeah exactly and it's I saw the tail end of that right there
[00:18:43.620 --> 00:18:48.180]   um and I you know I think that that's really cool and I think that that's
[00:18:48.180 --> 00:18:52.900]   amazing technology but then you look at the flip side where currently Google
[00:18:52.900 --> 00:18:57.260]   photos doesn't analyze who's in the photos but it could at one point you know
[00:18:57.260 --> 00:19:00.940]   at down the line it could analyze who's in the photos it could analyze what
[00:19:00.940 --> 00:19:04.940]   place you've been at and it's like oh this person you know you're you're
[00:19:04.940 --> 00:19:09.220]   intimate with this person in this photo and you're at this restaurant and you
[00:19:09.220 --> 00:19:15.340]   seem to go there you know quite often so I'm going to show you ads in Gmail or
[00:19:15.340 --> 00:19:19.060]   something for a coupon for this restaurant and on one hand that's really
[00:19:19.060 --> 00:19:23.620]   awesome because yay we go to this restaurant a lot and coupons are always
[00:19:23.620 --> 00:19:27.460]   nice but on the other hand you think about that kind of data and you think
[00:19:27.460 --> 00:19:33.180]   about their ability to collect that kind of data that you know they could
[00:19:33.180 --> 00:19:36.940]   either do really wonderful things with it like offer you coupons and offer you
[00:19:36.940 --> 00:19:41.260]   relevant information or that kind of data could be misused and I don't
[00:19:41.260 --> 00:19:45.100]   necessarily think even Google is the person is the company or the person that
[00:19:45.100 --> 00:19:49.220]   would misuse that data it's just if that data is being collected and if that
[00:19:49.220 --> 00:19:53.660]   data is being stored somewhere heaven for friends that you know either that
[00:19:53.660 --> 00:19:58.300]   data is compromised or that you know or someone decides to take a vested
[00:19:58.300 --> 00:20:03.020]   interest in like hmm this you know this data could potentially get me the
[00:20:03.020 --> 00:20:08.300]   information of millions of customers and I was to you know I was reading them
[00:20:08.300 --> 00:20:12.060]   that old article to do you guys remember a couple years back when the New York
[00:20:12.060 --> 00:20:16.300]   Times interview or was interviewing I think the former sales director of
[00:20:16.300 --> 00:20:20.740]   Target who was talking about how they pre-planned circulars to figure out how
[00:20:20.740 --> 00:20:23.940]   people are pregnant before they even know or you can tell anybody else that
[00:20:23.940 --> 00:20:29.060]   was a great article I started a story it was new somebody was pregnant before
[00:20:29.060 --> 00:20:32.780]   her parents did yeah because of what she bought because how she wandered the
[00:20:32.780 --> 00:20:36.500]   story yeah exactly because of the things that she was putting in her cart and
[00:20:36.500 --> 00:20:41.500]   those cut and it had the telling line where it's like well we found that when
[00:20:41.500 --> 00:20:45.460]   we we send people circulars that have all baby items in them and they haven't
[00:20:45.460 --> 00:20:50.940]   told anybody they're pregnant they find that really really creepy but say if you
[00:20:50.940 --> 00:20:55.700]   know we send them this circular and it has random things peppered in it like a
[00:20:55.700 --> 00:21:01.300]   lawn mower or like a beer that they would never buy they just think oh this is
[00:21:01.300 --> 00:21:06.780]   random yeah no oh everybody gets this everybody gets this circular and hey this
[00:21:06.780 --> 00:21:10.340]   is this is a great coupon for baby items and I'm gonna need this in two
[00:21:10.340 --> 00:21:15.140]   months but it's that kind of a thought right where you know there's the there's
[00:21:15.140 --> 00:21:20.100]   the level of like how much data can we can we use that we've collected about
[00:21:20.100 --> 00:21:25.740]   you without you being like okay that's weird yeah I think Google is doing face
[00:21:25.740 --> 00:21:30.740]   recognition because I just searched for my son's name and I was I mean I know
[00:21:30.740 --> 00:21:34.180]   it does face recognition but I think it matches it to a name because then I
[00:21:34.180 --> 00:21:40.180]   found all these pictures of Henry is both an adult and a kid and so it and I
[00:21:40.180 --> 00:21:44.180]   don't think I tagged them in any particular way I've done the same thing
[00:21:44.180 --> 00:21:48.900]   yeah so I think it's just saying this is doing face matching I mean it's
[00:21:48.900 --> 00:21:53.220]   that's not just photos I mean Google now the whole the whole the whole sort of
[00:21:53.220 --> 00:21:57.660]   promise of Google now is that they'll know so much about you that they can
[00:21:57.660 --> 00:22:02.420]   suggest helpful things but knowing that much about you means they know a lot
[00:22:02.420 --> 00:22:06.940]   about you and they can also probably if they wanted to they could suggest not
[00:22:06.940 --> 00:22:11.980]   so nice things or they could someone seeing that data could suggest not so
[00:22:11.980 --> 00:22:16.300]   nice things so it's I mean I think you're right serenity it's not may not be
[00:22:16.300 --> 00:22:22.300]   actively evil or good but it definitely has good and evil application
[00:22:22.300 --> 00:22:25.860]   there is a burden the by the way is that Ian why you didn't give them all your
[00:22:25.860 --> 00:22:29.620]   photos I don't like giving for giving that much information out for free I
[00:22:29.620 --> 00:22:33.620]   mean the Senate's currently debating the Patriots Act this very day they've come
[00:22:33.620 --> 00:22:37.060]   into recess to do it and I'm not that worried about Google to be honest I'm
[00:22:37.060 --> 00:22:41.380]   more worried about a lawful request from a new law that's been passed that says
[00:22:41.380 --> 00:22:45.260]   right we have the right to scoop all that data and you know you why make it
[00:22:45.260 --> 00:22:51.140]   easy for them that's all I feel I think though that there's a risk that I don't
[00:22:51.140 --> 00:22:56.780]   know I see I really like this and I really want this and I want Google now
[00:22:56.780 --> 00:23:01.300]   and I would be thrilled if I had a Google pair of Google augmented reality
[00:23:01.300 --> 00:23:05.820]   glasses that would pop up people's names and reputation scores as I walked
[00:23:05.820 --> 00:23:10.860]   around this would be valuable to me and I think there's a risk that by
[00:23:10.860 --> 00:23:17.700]   overstating the risks we could eliminate a really useful thing I mean we you know
[00:23:17.700 --> 00:23:21.140]   I guess there's always been a debate about new technologies you know Jack
[00:23:21.140 --> 00:23:25.900]   Hard looms oh god let's destroy those because he put us right that debates
[00:23:25.900 --> 00:23:30.100]   always happened but I just I fear that we won't get some innovations it could be
[00:23:30.100 --> 00:23:35.660]   very valuable on balance because of of kind of a tenuous or a nebulous fear
[00:23:35.660 --> 00:23:42.220]   of something that isn't real a tech a techno a techno panic sorry go on
[00:23:42.220 --> 00:23:46.060]   pace go ahead oh yeah no I mean that that is the worry that is the it's the
[00:23:46.060 --> 00:23:50.900]   frustrating balance between how much how much privacy can we potentially
[00:23:50.900 --> 00:23:55.900]   compromise at the at the extent of innovation but also there's a burden if
[00:23:55.900 --> 00:23:59.820]   you're gonna say what if there's a burden on you to say how great that
[00:23:59.820 --> 00:24:03.980]   possibility is I mean it's very easy to say what if yeah I was gonna I'm sorry I
[00:24:03.980 --> 00:24:10.300]   deal with the security beat an awful lot on the Reg yeah and okay two three
[00:24:10.300 --> 00:24:13.980]   years ago I was being called paranoid for suggesting some of the things I'd
[00:24:13.980 --> 00:24:17.660]   suggested were going on and then when the Snowden leaks came out we realized
[00:24:17.660 --> 00:24:22.220]   quite how deep down that hole we were I think that led a lot of people to say
[00:24:22.220 --> 00:24:25.620]   actually no I'm pulling back from that and that's why Google and Apple and others
[00:24:25.620 --> 00:24:30.260]   are really having problems for the end of the day troopers aren't knocking on our
[00:24:30.260 --> 00:24:35.540]   doors and taking us away in the dead of night to a cell I mean that hasn't
[00:24:35.540 --> 00:24:39.300]   happened yet hasn't yet being the crucial word on that well you bet that's
[00:24:39.300 --> 00:24:45.260]   the problem is I can presume all sorts of moral injury I take the risk yeah because
[00:24:45.260 --> 00:24:49.500]   I get great stuff out of it like I could type you know marmot and get one
[00:24:49.500 --> 00:24:54.740]   bats or whatever it is yeah but Leo it's it's more a it's more a perfect like a
[00:24:54.740 --> 00:24:58.380]   protection of your personal privacy where you're you know you can be willing to
[00:24:58.380 --> 00:25:01.940]   give as much as you want to a certain service and just be like you know what
[00:25:01.940 --> 00:25:06.540]   I've accepted that assert I mean anyone who's on the internet and who is active
[00:25:06.540 --> 00:25:10.100]   on the internet has to basically is signs a contract basically like I am
[00:25:10.100 --> 00:25:14.500]   giving a certain amount of myself away I'm on Twitter people might divulge where
[00:25:14.500 --> 00:25:18.700]   what cafes I'm at if they follow me intently and they really want to find
[00:25:18.700 --> 00:25:22.020]   out like someone wants to find out where you live they really they really will
[00:25:22.020 --> 00:25:27.060]   find out where you live it's it's not that hard ultimately but it's a question
[00:25:27.060 --> 00:25:31.420]   of like how much of this stuff am I voluntarily giving over at four for
[00:25:31.420 --> 00:25:35.780]   great things well it's how much you know how much you know about what Google's
[00:25:35.780 --> 00:25:40.140]   gonna do with it right yes like it's how much is Google disclosing about what
[00:25:40.140 --> 00:25:43.540]   they do with the data what they could do with the data is it being anonymized
[00:25:43.540 --> 00:25:47.900]   is it you know there has to be a certain amount of information that they give you
[00:25:47.900 --> 00:25:51.900]   so that you can make an informed choice about whether to do it or not yeah and I
[00:25:51.900 --> 00:25:56.500]   did find it really interesting you know at something that Apple I think does
[00:25:56.500 --> 00:25:59.940]   really well in this sphere is that they make it very clear all right this is
[00:25:59.940 --> 00:26:03.700]   exactly what we're doing with your data when it gets uploaded to iCloud and all
[00:26:03.700 --> 00:26:07.860]   of that and this is what we will never do with your data and that does come at
[00:26:07.860 --> 00:26:11.220]   a bit of a cost premium because they do charge for it rather than just making
[00:26:11.220 --> 00:26:15.860]   the services free with Google but cool I feel like obf skates that a lot they've
[00:26:15.860 --> 00:26:21.420]   gotten better about it but this still you know when Google photos came out and I
[00:26:21.420 --> 00:26:24.420]   was looking at the splash page you know it's like this beautiful thing here are
[00:26:24.420 --> 00:26:29.940]   all of the magical things that it can do and you cannot like that the the terms
[00:26:29.940 --> 00:26:33.940]   and conditions the privacy policy you don't even have to like check a box to
[00:26:33.940 --> 00:26:37.860]   sign up for it if you have a Google account you are opted in you already
[00:26:37.860 --> 00:26:43.060]   checked that box baby long time ago that's already yours and if you've
[00:26:43.060 --> 00:26:47.620]   uploaded any photos those are our to Google+ photos thing those are already
[00:26:47.620 --> 00:26:52.020]   in Google photos are you there's no there's no there's just no obvious like
[00:26:52.020 --> 00:26:55.300]   here's what we do with this here's what we don't and effective it wasn't for
[00:26:55.300 --> 00:26:59.940]   that Bradley Horowitz interview I would still be combing through Google's privacy
[00:26:59.940 --> 00:27:03.940]   policy trying to figure out what specifically applied to photos versus
[00:27:03.940 --> 00:27:08.340]   what was applied to say Gmail and I think it's really telling that he's that he's
[00:27:08.340 --> 00:27:12.460]   saying like oh this is going to be Google photos is going to be Gmail for your
[00:27:12.460 --> 00:27:16.580]   photos and I think Gmail did a lot of wonderful things I do use Gmail but
[00:27:16.580 --> 00:27:21.060]   there are also downsides of Gmail you know we've we've discovered just how
[00:27:21.060 --> 00:27:24.660]   deep that rabbit that advertising rabbit hole goes when the feds come are they
[00:27:24.660 --> 00:27:27.540]   gonna haul away everybody who's Google but everybody from Apple's gonna be
[00:27:27.540 --> 00:27:33.460]   safe no no not at all I mean but it's it's a question of I think just taking a
[00:27:33.460 --> 00:27:37.700]   fundamental view about your privacy and yes someone with a fairly poor memory
[00:27:37.700 --> 00:27:41.380]   I want those glasses that will whisper in your ear who it is you're talking to
[00:27:41.380 --> 00:27:46.260]   me I want all that stuff that's fine but I'm not prepared to give up a certain
[00:27:46.260 --> 00:27:50.500]   amount to do that I use Gmail every day is it for work and I use a personal
[00:27:50.500 --> 00:27:54.500]   personal business you know I share stuff on Twitter I share stuff on Facebook
[00:27:54.500 --> 00:27:58.100]   but you've got to be aware of what you're sharing and why you're sharing it
[00:27:58.100 --> 00:28:02.980]   it's just being aware I really think is that awareness is the key factor like if
[00:28:02.980 --> 00:28:07.020]   you don't want Alex Jones to determine what kind of features I get in the
[00:28:07.020 --> 00:28:12.420]   internet because you know there are none of us want to leave well no some of us
[00:28:12.420 --> 00:28:19.580]   do want that they're in the chatroom right now so so biscuit dev says Leo you
[00:28:19.580 --> 00:28:23.580]   sound like a smoker with a craving for his siggies yeah I'm hooked that's a
[00:28:23.580 --> 00:28:28.980]   British poo that's a pretty good I'm ready I'm hooked I acknowledge it well
[00:28:28.980 --> 00:28:33.180]   you do have a choice I mean it's so I mean and you even have the choice not to
[00:28:33.180 --> 00:28:37.620]   use the internet at all if you really yeah you really want to be paranoid
[00:28:37.620 --> 00:28:41.020]   absolutely well yeah I'm gonna go to live on a farm somewhere you know it's
[00:28:41.020 --> 00:28:45.220]   just actually there's a life if you don't want modern conveniences you have to
[00:28:45.220 --> 00:28:49.460]   live on a farm or somewhere yeah disconnected this is that this is a fancy
[00:28:49.460 --> 00:28:53.980]   won't you won't upload photos of their kids to Facebook or to Google because
[00:28:53.980 --> 00:28:58.540]   they just don't want to take the risk of those photos you know going somewhere
[00:28:58.540 --> 00:29:02.020]   they don't want me certainly don't begrudge them that that's their absolute
[00:29:02.020 --> 00:29:06.060]   right I wouldn't want to I mean there's nobody suggesting you have to do this
[00:29:06.060 --> 00:29:12.340]   no no of course and I mean go ahead oh no I'm just saying if you do it you've
[00:29:12.340 --> 00:29:16.460]   got to be aware that yes of what you are doing I agree you're sharing and if
[00:29:16.460 --> 00:29:19.660]   you're actually gonna stick photos up on I mean I stick photos up on Facebook on
[00:29:19.660 --> 00:29:22.540]   on Twitter and that sort of thing yeah but you've always got to have in the
[00:29:22.540 --> 00:29:26.980]   back of your mind you're sharing this with everyone think about what you're
[00:29:26.980 --> 00:29:31.300]   doing before you do it you know it's it's just some people don't feel strongly
[00:29:31.300 --> 00:29:35.380]   about it and I respect that decision totally but for me personally just the
[00:29:35.380 --> 00:29:39.940]   way I roll yeah yeah I think it's just about especially the folks you know we
[00:29:39.940 --> 00:29:44.060]   talk about our tech savvy people we also talk you know my parents who who are
[00:29:44.060 --> 00:29:48.700]   aware of tech and who use tech every day but may not realize oh if I upload
[00:29:48.700 --> 00:29:53.620]   this photo this photo to Google photos even if I set it to private and I never
[00:29:53.620 --> 00:29:57.700]   share it with anyone there is a small possibility that someone else may see
[00:29:57.700 --> 00:30:02.180]   that data or that data may get used for advertising and it's just about people
[00:30:02.180 --> 00:30:07.060]   being aware like yeah being able to tell the public hey this is what's you know
[00:30:07.060 --> 00:30:10.300]   this is what's going on not fear mongering not saying this is the worst
[00:30:10.300 --> 00:30:14.500]   thing ever and it's all going to you know be terrible but just knowing this is a
[00:30:14.500 --> 00:30:18.620]   you know this is this is a thing this is a thing I want that choice to I don't
[00:30:18.620 --> 00:30:23.060]   want the lowest common denominator privacy or the highest common denominator
[00:30:23.060 --> 00:30:27.420]   privacy to determine what I can do and that's what I fear I think if you if you
[00:30:27.420 --> 00:30:31.020]   if you have if everybody's rattling the cage oh my god we got to protect our
[00:30:31.020 --> 00:30:35.980]   privacy and so there's well Google never even though Google I'm sure has face
[00:30:35.980 --> 00:30:40.180]   recognition they're never going to implement it because of people like you
[00:30:40.180 --> 00:30:44.940]   damn you they will make it a little bit I don't get a choice right they will make
[00:30:44.940 --> 00:30:48.340]   it an option the number of people like me compared to the total no they can't
[00:30:48.340 --> 00:30:51.940]   make it off a 90% because then the people who will opt in will be all the
[00:30:51.940 --> 00:30:56.420]   stalkers and creeps and so they said they said they deliberately didn't do it
[00:30:56.420 --> 00:30:59.420]   with Google class because they were afraid of the repercussions right yeah
[00:30:59.420 --> 00:31:03.660]   so that's that's kind of what I'm arguing is gosh you guys are you know the
[00:31:03.660 --> 00:31:08.620]   techno Amish out here are keeping us from living in the future because you guys
[00:31:08.620 --> 00:31:12.900]   I think it is fair I think it is fair to point out though that Google isn't
[00:31:12.900 --> 00:31:17.900]   just giving billions of people photo storage with terabytes of data because
[00:31:17.900 --> 00:31:24.900]   of its heart yeah what is it they are bringing in that stuff just like
[00:31:24.900 --> 00:31:29.220]   they're giving you voice recognition and and automatically recognize you
[00:31:29.220 --> 00:31:32.820]   your voicemails and so on because they want the data and so that they can learn
[00:31:32.820 --> 00:31:38.220]   more about you and people like you and and learn how to do things with that
[00:31:38.220 --> 00:31:42.900]   data that you are providing them yeah I'll not content just so that they can use
[00:31:42.900 --> 00:31:48.020]   it not for bad reasons but they're for their own reasons well they I presume it
[00:31:48.020 --> 00:31:52.300]   has to do with advertising advertising I do so I certainly think is part of it
[00:31:52.300 --> 00:31:57.700]   in terms of Google yeah well in terms of Google as a company as a whole I mean
[00:31:57.700 --> 00:32:02.060]   if you look at all of Google's goals going back to the very origin of Google
[00:32:02.060 --> 00:32:06.620]   I feel like their their goal from day one has been we want to not only understand
[00:32:06.620 --> 00:32:10.140]   humanity but we want to build the biggest database about humanity possible
[00:32:10.140 --> 00:32:13.940]   right because then we can learn things from it and then we can build really
[00:32:13.940 --> 00:32:18.020]   smart computers and that's where the super you know paranoid scary people go
[00:32:18.020 --> 00:32:21.700]   off and skynet yeah exactly where it's like we are teaching computers so many
[00:32:21.700 --> 00:32:26.380]   things that you know if AGI or ASI ever gets flipped on this will be a very
[00:32:26.380 --> 00:32:31.620]   interesting interesting progression but in general I don't think that they're
[00:32:31.620 --> 00:32:34.580]   collecting the data solely for advertising I think they have a genuine
[00:32:34.580 --> 00:32:37.940]   interest in just we want to build a really cool database I mean they're
[00:32:37.940 --> 00:32:41.380]   nerds at heart right they want to they want to build a big picture and I
[00:32:41.380 --> 00:32:44.380]   know they said something I haven't heard him say in a while which is the original
[00:32:44.380 --> 00:32:47.820]   mission of Google which is to organize the world's information they use that
[00:32:47.820 --> 00:32:50.980]   phrase it I haven't heard that in a while from Google they used it in the in
[00:32:50.980 --> 00:32:54.140]   the press conference over the over the photo app they were saying basically we
[00:32:54.140 --> 00:32:57.660]   want to organize your memories and it does do a very good job of it I think
[00:32:57.660 --> 00:33:01.780]   as to enter you right in the terms of this database has obviously given them a
[00:33:01.780 --> 00:33:07.260]   big advantage in machine learning because photo depends on this the whole
[00:33:07.260 --> 00:33:11.140]   Gmail advertising system depends on this and they've obviously got way ahead of
[00:33:11.140 --> 00:33:13.820]   the pack when it comes to deep machine learning and that's gonna be really
[00:33:13.820 --> 00:33:19.060]   interesting and potentially useful you may yet especially things like Google
[00:33:19.060 --> 00:33:22.980]   now they use that data to be able to recommend things to you to be able to
[00:33:22.980 --> 00:33:27.700]   understand your behavior to be able to say to Leo you know there's some there's a
[00:33:27.700 --> 00:33:31.700]   new wombat calendar out you might want to get it because we know you like
[00:33:31.700 --> 00:33:37.300]   wombat's you know their their ability to offer other services that are useful
[00:33:37.300 --> 00:33:41.540]   like that is based on the data that they've built up over the last 10 years
[00:33:41.540 --> 00:33:48.380]   about you I just I am glad to give you and everybody else the choice to not
[00:33:48.380 --> 00:33:54.020]   participate that's fine just please don't let your terror of technology keep me
[00:33:54.020 --> 00:33:58.100]   from getting some of the cool stuff I want it's not terror and you know just
[00:33:58.100 --> 00:34:01.220]   as you are happy for me to do this I'm happy for you to do your stuff as well
[00:34:01.220 --> 00:34:04.260]   I'm giving them everything and the number of people out there who are
[00:34:04.260 --> 00:34:07.460]   actually withholding information in this way it's minimal so small it's not
[00:34:07.460 --> 00:34:11.180]   gonna hold up technology development no I don't think so but actually you made a
[00:34:11.180 --> 00:34:14.420]   point that that I thought was interesting it's not that Google might do
[00:34:14.420 --> 00:34:20.540]   something bad itself deliberately it's that the data that it has accumulated
[00:34:20.540 --> 00:34:24.900]   could be used by someone else say the government to do something bad and
[00:34:24.900 --> 00:34:29.460]   Google would have no choice but to give up that information exactly so much
[00:34:29.460 --> 00:34:33.780]   that they're gonna do it yeah I mean if a law comes in saying you have to hand
[00:34:33.780 --> 00:34:38.500]   this over right they have to comply with lawful requests so toast you know
[00:34:38.500 --> 00:34:43.420]   anybody live blogging the Senate's session right now I don't know I would
[00:34:43.420 --> 00:34:48.580]   be good to have it to keep an eye on it because as you know the sunset June
[00:34:48.580 --> 00:34:53.020]   first midnight is sunset for section 215 I think the whole Patriot Act is
[00:34:53.020 --> 00:34:57.820]   sunsetted in just a few hours that's three minutes it's three key sections of
[00:34:57.820 --> 00:35:04.700]   incoming section 215 five hours 11 minutes and 40 seconds so they're meeting
[00:35:04.700 --> 00:35:09.380]   right now Mitch McConnell's going you gotta pass this Patriot Act Mitch McConnell
[00:35:09.380 --> 00:35:13.420]   wants the Patriot Act permanent he wants a permanent Patriot Act him I'm afraid
[00:35:13.420 --> 00:35:18.820]   of I don't want Google to give him and my information well also wouldn't I
[00:35:18.820 --> 00:35:21.580]   wouldn't trust him to find his own backside with both hands yeah torch let alone
[00:35:21.580 --> 00:35:25.900]   get that access to my full data this is the EFF article why Mitch McConnell
[00:35:25.900 --> 00:35:30.700]   cannot be allowed to decide the fate of the Patriot Act you know I'm I'm no no
[00:35:30.700 --> 00:35:34.700]   fan of the Paul family and Rand Paul but boy he's fighting a good fight
[00:35:34.700 --> 00:35:38.740]   there yeah and I have to support him for that so we'll we'll try to keep an eye
[00:35:38.740 --> 00:35:43.820]   on that if you're watching live because there's five hours left for the Senate
[00:35:43.820 --> 00:35:47.900]   to do something to keep it will sunset if they do nothing well yeah I mean it
[00:35:47.900 --> 00:35:51.220]   might sunset for a day or two but at the end of the day that's good to mean that
[00:35:51.220 --> 00:35:55.660]   I swarming into the US borders with eight in the heart yeah and as a better
[00:35:55.660 --> 00:35:59.980]   piece of legislation will be fine and I think Rand Paul is mad as a sack of
[00:35:59.980 --> 00:36:05.220]   badges most of the time but on this one he's actually right yeah we'll watch the
[00:36:05.220 --> 00:36:09.840]   sunset the sunset on a section 215 it's gonna be a beautiful image that's
[00:36:09.840 --> 00:36:15.140]   sunset he but then I'm a Google making us like Twitter has a live feed going
[00:36:15.140 --> 00:36:20.820]   of course Twitter does hashtag peter the world happens on Twitter these days
[00:36:20.820 --> 00:36:25.300]   well I'll load that up and we'll keep an eye on it meanwhile we got a great
[00:36:25.300 --> 00:36:30.220]   panel this is gonna this is already fun and it's gonna get funner that's not a
[00:36:30.220 --> 00:36:32.860]   word is it ah well what the hell you make them up it is
[00:36:32.860 --> 00:36:39.820]   you could say siggies I could say funner no indeed
[00:36:39.820 --> 00:36:45.420]   Matthew Ingram is here for Fortune magazine Ian Thompson from the
[00:36:45.420 --> 00:36:49.180]   register from serenity Caldwell I didn't give you credit so I'll say serenity
[00:36:49.180 --> 00:36:53.780]   call maufer I'm more calm wonderful panel of people talking about the
[00:36:53.780 --> 00:36:56.980]   week's news will continue on in the just a moment I showed they brought to you
[00:36:56.980 --> 00:37:02.100]   by personal capital I love personal capital you know if it's important to
[00:37:02.100 --> 00:37:07.740]   plan for your future it's important to save it's important to invest but I think
[00:37:07.740 --> 00:37:11.860]   so many people just kind of stick that money somewhere and forget about it or
[00:37:11.860 --> 00:37:15.580]   worse they're using a broker but a broker who doesn't have their best
[00:37:15.580 --> 00:37:20.820]   interests at heart a broker who's working on commission who's pushing
[00:37:20.820 --> 00:37:24.860]   products on you because he'll make more money or encouraging you to trade more
[00:37:24.860 --> 00:37:29.140]   because he makes money every time you trade that's gonna shave years off of
[00:37:29.140 --> 00:37:33.100]   your retirement I don't want you to do that I want you to try personal capital
[00:37:33.100 --> 00:37:38.460]   right now personal capital is free it's absolutely secure they have mobile
[00:37:38.460 --> 00:37:44.260]   apps as you could see for iOS for Android for the Amazon Kindle store they also
[00:37:44.260 --> 00:37:48.500]   have a watch apps they've a watch for an Android where and now for Apple Watch
[00:37:48.500 --> 00:37:53.700]   it'll keep you up to date on what's going on with your investments and their
[00:37:53.700 --> 00:37:59.460]   tools will help you plan for the future this is all completely free open your
[00:37:59.460 --> 00:38:03.500]   free account link your investment in bank accounts in seconds a dashboard of
[00:38:03.500 --> 00:38:08.780]   your complete net worth will be there updated in real time in one secure place
[00:38:08.780 --> 00:38:13.700]   now that's part one part two is per and you don't have to use this I want to
[00:38:13.700 --> 00:38:18.340]   point out it's a freemium model and most people don't most people just take the
[00:38:18.340 --> 00:38:22.900]   free stuff but they are also an SEC registered financial advisors and not
[00:38:22.900 --> 00:38:26.940]   brokers they have a fiduciary responsibility to look out for your best
[00:38:26.940 --> 00:38:31.420]   interest they don't work on commission but they can give you very very good
[00:38:31.420 --> 00:38:36.700]   advice they give you a sense of what they can do if you put one hundred thousand
[00:38:36.700 --> 00:38:39.940]   dollars or more and assets into the personal capital dashboard you'll get a
[00:38:39.940 --> 00:38:45.460]   free 30-minute review that I did it's really straightforward very valuable you
[00:38:45.460 --> 00:38:49.300]   talk about your goals your risk tolerance when you plan to retire you know your
[00:38:49.300 --> 00:38:53.380]   time horizon and what you've got your for investments now and they're really
[00:38:53.380 --> 00:38:57.620]   good I was very impressed take control of your financial future personal
[00:38:57.620 --> 00:39:01.660]   capital to give you total clarity and transparency to make better investment
[00:39:01.660 --> 00:39:06.460]   decisions right away and it's free you even get a free no obligation portfolio
[00:39:06.460 --> 00:39:09.820]   consultation if you link more than a hundred thousand dollars in assets in the
[00:39:09.820 --> 00:39:14.540]   dashboard personal capital dot com slash twit to try it today I've been using it
[00:39:14.540 --> 00:39:18.500]   for it's almost three years now personal capital dot com slash twit I think I'm
[00:39:18.500 --> 00:39:23.700]   this for their support of this week in tech what's the hashtag for as should I
[00:39:23.700 --> 00:39:27.980]   just section 215 or I'm doing Patriot Act to the moment that's a good one I
[00:39:27.980 --> 00:39:34.220]   should do that what is the opposite of patriotic Patriot Act yeah okay funny
[00:39:34.220 --> 00:39:40.780]   it's fine you know this is a likely discussion we just had there are two
[00:39:40.780 --> 00:39:44.460]   sides to this discussion to president Obama says I you know I don't want to be
[00:39:44.460 --> 00:39:48.140]   the and I don't want to my watch that there is a terrorist act that we could
[00:39:48.140 --> 00:39:53.460]   have found out about but we didn't because we didn't let the NSA do its job is not
[00:39:53.460 --> 00:39:57.100]   an unreasonable point of view well if you look at the evidence there's
[00:39:57.100 --> 00:40:01.940]   absolutely no it helps it's been able to actually stop anything despite all of
[00:40:01.940 --> 00:40:05.860]   this bulk collection yeah if they had even one example not even a very good
[00:40:05.860 --> 00:40:09.900]   one that would be great but there's zero yeah yeah I mean we we already in the
[00:40:09.900 --> 00:40:16.300]   911 report they said explicitly all the legal process was there that was needed
[00:40:16.300 --> 00:40:19.940]   to get the information which could have stopped the attacks but it wasn't gelled
[00:40:19.940 --> 00:40:23.540]   together and examined in the way which which people teased out these little
[00:40:23.540 --> 00:40:28.060]   things from there and as you say Matthew I mean there they haven't been able to
[00:40:28.060 --> 00:40:33.140]   give us a single example beyond one Somali taxi driver who gave three thousand
[00:40:33.140 --> 00:40:37.660]   two hundred dollars to a terrorist group right from this whole thing so why are
[00:40:37.660 --> 00:40:42.300]   we paying for it why are we doing it what but they're the ex okay I'm gonna
[00:40:42.300 --> 00:40:48.700]   play the devil's Africa they're the experts if they want these tools do you
[00:40:48.700 --> 00:40:51.900]   think that the NSA they're evil or I think they're patriots I think they're
[00:40:51.900 --> 00:40:54.580]   trying to put they're doing their best to protect us I think a lot of people at
[00:40:54.580 --> 00:40:57.900]   the NSA are patriots but there's a certain kind of mindset which means if
[00:40:57.900 --> 00:41:02.900]   we can grab this data why not why not yeah in the state it was all out there so
[00:41:02.900 --> 00:41:06.540]   they could ask the telephone companies for it they could ask Google for it but
[00:41:06.540 --> 00:41:10.660]   they want it themselves and the point is it's not targeted in any way you know
[00:41:10.660 --> 00:41:14.700]   there there are ways to get to collect data that is relevant there are
[00:41:14.700 --> 00:41:18.940]   processes to go through this is not that this is collecting everything you say
[00:41:18.940 --> 00:41:25.180]   and do and click on and look at just in case maybe randomly you might be
[00:41:25.180 --> 00:41:29.620]   involved in something bad and they can figure that out now Matthew you don't
[00:41:29.620 --> 00:41:33.420]   care I mean you just are you watching this like a sporting event like you know
[00:41:33.420 --> 00:41:40.500]   or do you care I do care I mean I obviously it doesn't it affects me
[00:41:40.500 --> 00:41:45.340]   secondarily you know it affects me when I use services like Google or when I
[00:41:45.340 --> 00:41:49.860]   enter the US or when I deal with with companies in the US but I mean I'm
[00:41:49.860 --> 00:41:55.980]   positive they're grabbing my clicks and my stream and my everything either
[00:41:55.980 --> 00:42:00.020]   they're getting it directly or they've got to deal with the Canadian version of
[00:42:00.020 --> 00:42:04.580]   the NSA to trade data so I know it's happening and I think the it's the
[00:42:04.580 --> 00:42:08.940]   indiscriminate nature of it that that bothers me it's not it's not that they're
[00:42:08.940 --> 00:42:13.660]   doing it it's that they're doing it with everyone and all of their data
[00:42:13.660 --> 00:42:17.780]   without without any kind of targeting that's what bothers me so there you go
[00:42:17.780 --> 00:42:22.660]   we have it on the screen this is C-SPAN 2 they're voting now whether to limit
[00:42:22.660 --> 00:42:28.980]   debate so that's a cloture vote right yeah 70 76 to 17 it looks like that has
[00:42:28.980 --> 00:42:35.020]   passed so they've got cloture they're going to debate on it that means they
[00:42:35.020 --> 00:42:38.900]   will get at least to vote on well presumably they're going to be voting on
[00:42:38.900 --> 00:42:42.580]   USA freedom I mean Mitch McCollan's got his own belief straight up reauthorize
[00:42:42.580 --> 00:42:47.220]   yeah but USA freedom is up for the vote that's the one that was passed in the
[00:42:47.220 --> 00:42:50.860]   House of Representatives limiting the mass collection of phone data just a
[00:42:50.860 --> 00:42:54.220]   little bit the Senate could not get a cloture they couldn't get the super
[00:42:54.220 --> 00:42:58.580]   majority needed to vote on it last time they as they reconvene maybe that's
[00:42:58.580 --> 00:43:01.540]   what's going on right now it's hard to hard to follow they just look like
[00:43:01.540 --> 00:43:05.460]   they're standing there which is mostly what I yeah they just kind of stand
[00:43:05.460 --> 00:43:13.020]   there a lot and look around governments yeah money we've got this problem in
[00:43:13.020 --> 00:43:16.060]   the UK at the moment they just have had to put a snoopers to a snoopers charter
[00:43:16.060 --> 00:43:19.220]   through and our new Prime Minister wants to ban all applications with
[00:43:19.220 --> 00:43:23.300]   encryption that can't be unlocked by the government he also wants to to ban all
[00:43:23.300 --> 00:43:27.620]   what is he what was what Cameron say that was we talked about last time he
[00:43:27.620 --> 00:43:34.340]   were here was so wild yeah he's oh he he wants basically you cannot use an
[00:43:34.340 --> 00:43:39.620]   encryption scheme in the UK that cannot be broken by law enforcement we also
[00:43:39.620 --> 00:43:45.060]   said we should no longer tolerate free speech because yes yes what a delightful
[00:43:45.060 --> 00:43:49.140]   quote that was going from an oldie Tony and like himself yes we've perhaps
[00:43:49.140 --> 00:43:54.580]   been too open-minded yeah let's not be so open-minded okay let's say yeah that's
[00:43:54.580 --> 00:43:57.780]   very easy for you to say but you know if you're part of the people who come to
[00:43:57.780 --> 00:44:02.660]   the UK from outside and who make up the UK mostly these days that's a little
[00:44:02.660 --> 00:44:08.700]   insulting just a little bit and I hope he's made to eat those words you got C51
[00:44:08.700 --> 00:44:15.700]   up north we sure do yeah tell us about that well it's trying to do something
[00:44:15.700 --> 00:44:22.460]   similar I mean C51 is is I'm trying to remember where it is exactly in the but
[00:44:22.460 --> 00:44:29.700]   it the fundamentally the the intent is the same to to do as much sort of
[00:44:29.700 --> 00:44:35.820]   monitoring as possible yeah it's you know anti-terrorism so the the rationale is
[00:44:35.820 --> 00:44:43.620]   that we need to do these things in order to stop terrorists but I think the you
[00:44:43.620 --> 00:44:49.220]   know the the risk in my mind is that you it's a little like the asking people
[00:44:49.220 --> 00:44:54.500]   to give the place a key in case they want to come in and look around your house
[00:44:54.500 --> 00:44:59.740]   just in case you're doing something bad will they do it maybe not but in in a
[00:44:59.740 --> 00:45:04.300]   way you give up a little bit of freedom each time you do that in return for what
[00:45:04.300 --> 00:45:09.580]   in return for some theoretical potential benefit that you might get at some
[00:45:09.580 --> 00:45:13.500]   future point yeah it feels to me a little bit like say you're in a
[00:45:13.500 --> 00:45:18.020]   relationship with the guy or with a girl who's like you know I love you let's
[00:45:18.020 --> 00:45:22.860]   you know we're we're such wonderful couple we should share each other's
[00:45:22.860 --> 00:45:26.580]   location data you know we should share each other's passwords you know just
[00:45:26.580 --> 00:45:33.140]   yeah for openness sake for just just in case and then is not a good thing I
[00:45:33.140 --> 00:45:37.620]   shouldn't do that you saying you should know well I mean I think I think it's
[00:45:37.620 --> 00:45:41.340]   different I think that I think in some in some cases it's fine but it's one it's
[00:45:41.340 --> 00:45:46.340]   one the person like intentionally is like so we should share these things and
[00:45:46.340 --> 00:45:49.900]   then two months later you get a day like why were you at the grocery store
[00:45:49.900 --> 00:45:53.060]   we'll be doing at the grocery store at three in the morning and that's kind of
[00:45:53.060 --> 00:45:57.740]   what I feel like the NSA is sometimes where it's like it collects all of the
[00:45:57.740 --> 00:46:03.740]   same the NSA is a bad girlfriend an obsessive and obsessive boyfriend or
[00:46:03.740 --> 00:46:11.260]   girlfriend yeah exactly it's absolutely absolutely well it's it's like what you
[00:46:11.260 --> 00:46:17.100]   are saying you well exactly exactly well I'm just looking out for just looking
[00:46:17.100 --> 00:46:21.780]   up for our relationship I want everything to be you know to be kosher
[00:46:21.780 --> 00:46:27.740]   and wonderful no I it frustrates me because yes I do think that there are
[00:46:27.740 --> 00:46:31.340]   potentially valid uses for this data but it's like he was you was saying it was
[00:46:31.340 --> 00:46:36.580]   just it's so much data it's so much data they have you know they have a
[00:46:36.580 --> 00:46:41.860]   giant data farms that they are putting all this data into it would take months
[00:46:41.860 --> 00:46:47.180]   for them to sort through it even with computers they can do the things that
[00:46:47.180 --> 00:46:50.860]   they say they want to do they want to convince that they can actually find
[00:46:50.860 --> 00:46:57.740]   the things they say they want to find I mean it's actually funny is that if they
[00:46:57.740 --> 00:47:02.780]   really wanted to develop a smart system for this where they just collected the
[00:47:02.780 --> 00:47:07.420]   data that was potentially important Google would actually be the company to
[00:47:07.420 --> 00:47:14.820]   work with I was thinking yeah but if the if the headline US government and
[00:47:14.820 --> 00:47:18.780]   Google working to or NSA and Google working together to create smart
[00:47:18.780 --> 00:47:23.820]   algorithm the world would explode but but you know they are the people with
[00:47:23.820 --> 00:47:28.220]   with the knowledge in this area they are the they are the company that actually
[00:47:28.220 --> 00:47:32.580]   knows okay we we did collect a bunch of data and now we know how to create
[00:47:32.580 --> 00:47:36.900]   smart algorithms that you know target very specific sets of data on the other
[00:47:36.900 --> 00:47:40.100]   hand Google are ultra pissed at the NSA at the moment when they found out they
[00:47:40.100 --> 00:47:43.420]   were tapping in its connects between their data space and I feel like you
[00:47:43.420 --> 00:47:46.980]   what I'm never sure they're not protesting too much you know all of these
[00:47:46.980 --> 00:47:51.580]   companies oh don't do that not every single Google engineer I've spoken to
[00:47:51.580 --> 00:47:55.980]   about this is furious about that yeah they use words we can't use on the
[00:47:55.980 --> 00:48:04.220]   well it is interesting though it is interesting since you mentioned C51 one
[00:48:04.220 --> 00:48:09.980]   of the one of the sort of frightening aspects of that bill it's very similar
[00:48:09.980 --> 00:48:14.580]   to what Cameron was talking about its restrictions on speech so it's not just
[00:48:14.580 --> 00:48:18.820]   trying to track people who might do something bad it's actually trying to
[00:48:18.820 --> 00:48:24.740]   restrict people's ability to say things that could be interpreted in bad ways
[00:48:24.740 --> 00:48:30.340]   and I think that's a you know that's a depressing sort of evolution of
[00:48:30.340 --> 00:48:34.140]   protection I'm resisting being just kind of a paranoid crazy
[00:48:34.140 --> 00:48:41.980]   paranoid any government crazy and going down that road because I don't I
[00:48:41.980 --> 00:48:49.540]   I cut so is are we in actually perilous times like more so than always or is
[00:48:49.540 --> 00:48:53.900]   this just kind of the normal isn't it's always been this way or is there
[00:48:53.900 --> 00:49:01.780]   something really is this are we like Germans in Germany in 1938 are we I mean
[00:49:01.780 --> 00:49:07.500]   what is what what is giving some historical context the rationale you mean
[00:49:07.500 --> 00:49:11.660]   well I just I wonder is how serious how much danger are we actually how much
[00:49:11.660 --> 00:49:17.180]   danger are we in well I can say as a foreign who's come to this country seven
[00:49:17.180 --> 00:49:22.340]   years ago things have moved in a more scary way yeah than they agree with us
[00:49:22.340 --> 00:49:26.300]   but the key concern is not you know what the government is now and what they're
[00:49:26.300 --> 00:49:31.020]   doing now the fact is storage is cheap and storage is permanent so yeah the
[00:49:31.020 --> 00:49:36.340]   government now may be fine 30 years down the line if things get really weird and
[00:49:36.340 --> 00:49:40.780]   they've got all this material to go back to then it's probably not going to be
[00:49:40.780 --> 00:49:43.580]   fine but you're taking a gamble as to whether things are going to get better
[00:49:43.580 --> 00:49:47.380]   or worse politically and got to say over the last seven years a little bit worse
[00:49:47.380 --> 00:49:50.900]   it's probably the way I think if you look at if you're trying to compare it to
[00:49:50.900 --> 00:49:56.580]   something historically I think you would find a lot of NSA advocates and a lot
[00:49:56.580 --> 00:50:00.780]   of you know government security types would argue that the kind of situation
[00:50:00.780 --> 00:50:05.740]   that the environment we're in now is not even remotely comparable to past wars
[00:50:05.740 --> 00:50:11.340]   because we're talking about agencies and entities that that you can't even
[00:50:11.340 --> 00:50:16.660]   really put your finger on they don't have they're not armies they're not typical
[00:50:16.660 --> 00:50:21.780]   sort of you know combatants in a war you have people who become radicalized and
[00:50:21.780 --> 00:50:25.340]   then join a shadowy group that's connected to some other shadowy group
[00:50:25.340 --> 00:50:29.180]   and then explosions happened and people are killed and that's a very very
[00:50:29.180 --> 00:50:35.020]   difficult thing militarily to to protect against or to fight and so I
[00:50:35.020 --> 00:50:40.340]   think that's if there's one rationale that's used it's that the threat is
[00:50:40.340 --> 00:50:44.620]   everywhere or the potential threat is everywhere and so we need heirs ears
[00:50:44.620 --> 00:50:48.620]   everywhere and eyes everywhere we need them Batman thing where we're listening
[00:50:48.620 --> 00:50:52.260]   to every phone conversation that anyone is having because in order to protect
[00:50:52.260 --> 00:50:57.740]   people you know that's true no I don't think it's true at all I was very nearly
[00:50:57.740 --> 00:51:02.060]   killed by an IRA bomb I was fired standing standing in London in London
[00:51:02.060 --> 00:51:08.260]   in now 91 Victoria coach station there was a about a two pound bomb in a
[00:51:08.260 --> 00:51:12.100]   lister bin which I was standing next to having a cigarette and reading my paper
[00:51:12.100 --> 00:51:15.580]   before getting on the train and literally five minutes after I left the station
[00:51:15.580 --> 00:51:21.500]   it exploded one dead 39 injured now as it turns out it's possible that fun that
[00:51:21.500 --> 00:51:25.220]   bomb was partially funded by people in the US who were giving money to the IRA
[00:51:25.220 --> 00:51:29.860]   but even so I would love to know who planted that bomb I would love to actually
[00:51:29.860 --> 00:51:34.340]   get to a room with him with a crowbar but I'm not prepared to give up everyone's
[00:51:34.340 --> 00:51:38.220]   privacy just for that right and I think you've got to have a so we have a few
[00:51:38.220 --> 00:51:41.420]   casualties we just have to take I'm sorry you really do I mean
[00:51:41.420 --> 00:51:44.580]   everywhere else around the world has lived with terrorism for a very long
[00:51:44.580 --> 00:51:49.220]   time and the minute the USA got hit then it seemed to lose its mind over this I
[00:51:49.220 --> 00:51:53.340]   expected better it's true I agree with you I think we did lose our we really
[00:51:53.340 --> 00:52:00.420]   reacted in a knee jerk fashion in a in a reactionary way it's we're very
[00:52:00.420 --> 00:52:04.180]   protective over our citizens in the US we're very protective over our
[00:52:04.180 --> 00:52:07.740]   patriotic yeah we're all we're protective over our right to freedom we're
[00:52:07.740 --> 00:52:11.660]   protective over our country we fought hard for this country and I mean that
[00:52:11.660 --> 00:52:15.060]   may not you know most of the most of the people who have been an active duty
[00:52:15.060 --> 00:52:20.580]   may be retired or you know may have just had recent influence in Iraq but we
[00:52:20.580 --> 00:52:25.180]   still have that you know that undercurrent running through the government and that
[00:52:25.180 --> 00:52:31.260]   leads again I feel like the NSA all of this I do believe was started with
[00:52:31.260 --> 00:52:35.260]   somewhat noble intentions in the well we can't ever let this happen again how
[00:52:35.260 --> 00:52:40.700]   are we blindsided how you know how how how did we you know lose how did we lose
[00:52:40.700 --> 00:52:45.100]   how did we how did we let this you know this giant horrible disaster happen
[00:52:45.100 --> 00:52:52.900]   but I feel like over you become like a this is a weird a weird thing but there
[00:52:52.900 --> 00:52:56.340]   is a there was a movie called like bright eyes or blue eyes or steel or
[00:52:56.340 --> 00:53:00.260]   something like that it's about a woman whose husband or fiance gets murdered in
[00:53:00.260 --> 00:53:04.540]   Central Park and then she goes basically she she gets a gun and then she starts
[00:53:04.540 --> 00:53:09.140]   going vigilante over anybody being being crazy or you take that every Charles
[00:53:09.140 --> 00:53:13.260]   Bronson movie ever pretty much pretty much no but it's but it's that kind of
[00:53:13.260 --> 00:53:16.980]   the thing where it starts small it starts like well I can't let ever let
[00:53:16.980 --> 00:53:20.380]   myself be vulnerable again I can't ever let myself be hurting again and then the
[00:53:20.380 --> 00:53:25.180]   next thing that you know you know you're you're going around doing vigilante
[00:53:25.180 --> 00:53:28.580]   work or you're collecting so much data that you've turned into somebody that you
[00:53:28.580 --> 00:53:33.500]   don't recognize anymore and that's what I feel about the NSA I think that's the
[00:53:33.500 --> 00:53:37.300]   risk the risk is that you do a whole bunch of things that seem rational and
[00:53:37.300 --> 00:53:43.540]   you give up tiny tiny bits of your freedom or tiny tiny bits of your privacy or
[00:53:43.540 --> 00:53:48.980]   whatever and but before you know it you're you slid down this slippery slope and
[00:53:48.980 --> 00:53:53.140]   you can't get back and you are in a place that you don't like well this is it
[00:53:53.140 --> 00:53:56.540]   it never comes back I mean that we're seeing the USA Freedom Act being debated
[00:53:56.540 --> 00:54:00.300]   at the moment you remember this is the very first piece of legislation which
[00:54:00.300 --> 00:54:04.380]   is sought to loosen the amount of surveillance that's going on since 1978
[00:54:04.380 --> 00:54:08.220]   now the world has changed a lot since then we've had an awful lot of stuff
[00:54:08.220 --> 00:54:12.900]   coming up but giving this stuff away is easy getting it back is really
[00:54:12.900 --> 00:54:16.940]   impossible incredibly difficult and when you're giving you know if you're
[00:54:16.940 --> 00:54:20.380]   giving a couple yards here and a couple yards there and a couple yards here and
[00:54:20.380 --> 00:54:24.780]   then sooner or later you realize that you've given you know a couple thousand
[00:54:24.780 --> 00:54:30.380]   meters and you're like well now that you know that starting line is way over
[00:54:30.380 --> 00:54:37.100]   there and trying to inch back that way is a very very difficult I realize I'm
[00:54:37.100 --> 00:54:42.420]   mixing metric and yet that may be exactly what's happening right now on the floor
[00:54:42.420 --> 00:54:47.740]   of the United States Senate that the senators having realized that we over
[00:54:47.740 --> 00:54:52.420]   reacted that we went too far are actually considering letting it expire
[00:54:52.420 --> 00:54:57.460]   isn't that a positive sign it is it definitely is yeah it's just taking a
[00:54:57.460 --> 00:55:00.940]   bloody long time to get it done and they're doing it a very half-assed way if
[00:55:00.940 --> 00:55:04.220]   you don't want to be saying so well and would they have with the have done that
[00:55:04.220 --> 00:55:08.460]   at all if it hadn't been without a note exactly yeah thank God for snowing I
[00:55:08.460 --> 00:55:13.940]   think it's fair to say that he made a huge difference in this discussion
[00:55:13.940 --> 00:55:17.340]   absolutely I mean I don't know if this will be going on right now a black hat
[00:55:17.340 --> 00:55:21.700]   four years ago I suggested to a former member of the FBI that maybe echelon
[00:55:21.700 --> 00:55:25.780]   was being used to slice by on European and US businesses and yeah I'd absolutely
[00:55:25.780 --> 00:55:31.180]   fit me yeah and then Snowden comes out and it's just like ah well we were doing
[00:55:31.180 --> 00:55:35.340]   a little bit of that but our reasons were good and you're like I'm sorry you
[00:55:35.340 --> 00:55:37.860]   know you've got to have a tight rein on these things otherwise they can get out
[00:55:37.860 --> 00:55:43.900]   of control so if 9 out of 10 people can be good honest patriots or people who
[00:55:43.900 --> 00:55:47.820]   really want to upload the law if one person out of those those 10 those
[00:55:47.820 --> 00:55:52.740]   hundred with those thousand who has access to that data is not honest or not
[00:55:52.740 --> 00:55:57.420]   trustworthy or you know has bad intentions that makes that that destroys
[00:55:57.420 --> 00:56:04.860]   the entire concept meanwhile let's look at some shiny new gadgets brought to us
[00:56:04.860 --> 00:56:10.100]   by the lightning the mood well I do feel like we're a little bit distracted
[00:56:10.100 --> 00:56:14.220]   the death aren't we I mean there's no accident that we have these incredible
[00:56:14.220 --> 00:56:21.180]   spruettin circuses these spectacles these gadgets these toys it's very tempting
[00:56:21.180 --> 00:56:28.340]   just to relax and say it's okay because I have the newest galaxy and what could
[00:56:28.340 --> 00:56:34.220]   possibly go wrong here that can search all my photos yeah yeah so we'll talk a
[00:56:34.220 --> 00:56:40.020]   little bit more about Google I owe there's some Apple news to WWDC is coming up
[00:56:40.020 --> 00:56:46.340]   June 8th is a little more than a week away that should be a lot of fun we'll
[00:56:46.340 --> 00:56:51.580]   be covering that live next week we could talk a little bit about what's going on
[00:56:51.580 --> 00:56:56.540]   in media we talked before the show Matthew Ingram knows very well how
[00:56:56.540 --> 00:57:00.980]   difficult it is for tech blogs to succeed these days is it was there another
[00:57:00.980 --> 00:57:08.820]   casualty this week or is it all good is it all wonderful and a life sentence for
[00:57:08.820 --> 00:57:14.460]   Ross Obricht is it that's I don't know what to say about that but that's
[00:57:14.460 --> 00:57:17.460]   fascinating we're gonna take a break come back and talk more about that we had a
[00:57:17.460 --> 00:57:21.460]   good week this week on Twitter do you have a promo ready because I would just
[00:57:21.460 --> 00:57:26.820]   like you to watch this and see what you missed previously on Twitter that doesn't
[00:57:26.820 --> 00:57:31.220]   look at all dorks yeah you look like Bender
[00:57:31.220 --> 00:57:36.220]   Twitter live specials that's jump open camera design with a fully integrated
[00:57:36.220 --> 00:57:40.640]   version GoPro and similar that turns raw footage into VR video with it looks
[00:57:40.640 --> 00:57:47.420]   seamless that's amazing tech news today the IRS got hacked America's tax
[00:57:47.420 --> 00:57:51.060]   collection agency the internal revenue service disclosed yesterday that hackers
[00:57:51.060 --> 00:57:56.500]   gain access to the personal information of more than 100,000 US taxpayers the
[00:57:56.500 --> 00:57:59.740]   main thing that that people suspect that these hackers are going to do with this
[00:57:59.740 --> 00:58:04.420]   information is use it to file fraudulent tax returns the new screen
[00:58:04.420 --> 00:58:09.580]   savers Jim Cutler our great voice over who I'm the guy that though is Netcast you
[00:58:09.580 --> 00:58:14.500]   love the chat room wants to know are you a Fremen do you use the spice I love
[00:58:14.500 --> 00:58:22.420]   Fremen before you buy light as it had it's the new Apple MacBook I don't need
[00:58:22.420 --> 00:58:26.560]   another MacBook but gosh for traveling for getting around this thing is
[00:58:26.560 --> 00:58:31.340]   gorgeous and I love the gold to it now also available in several colors of
[00:58:31.340 --> 00:58:37.460]   unapologetic plastic we like we prefer to call it floor last summer thank you
[00:58:37.460 --> 00:58:44.220]   as Jim Cutler yes you don't can you do an intervention on Renee Richie how
[00:58:44.220 --> 00:58:49.660]   many bands has Renee bought now oh my gosh I think he has one of every kind of
[00:58:49.660 --> 00:58:53.620]   the 42 millimeter although I to be fair I have one of every color of the sport
[00:58:53.620 --> 00:58:58.220]   band but it was for research and dual matching because that wait a minute you
[00:58:58.220 --> 00:59:03.660]   wait a minute you're not those are those are heterogeneous I think that's a
[00:59:03.660 --> 00:59:10.540]   violation of something you're wearing brown and green green and pink you can't
[00:59:10.540 --> 00:59:16.580]   oh that's pink yeah yeah well I never thought about your fashion rebel I'm
[00:59:16.580 --> 00:59:20.300]   more that was mixed and matching I stole it from swatch really are you a
[00:59:20.300 --> 00:59:24.500]   lot yeah I don't think so I think Tim Cook's gonna you're gonna hear from Tim
[00:59:24.500 --> 00:59:31.980]   Cook probably Johnny we didn't design the most beautiful watch we've ever
[00:59:31.980 --> 00:59:38.420]   made so you could ruin it by mixing bands so starting this week we're
[00:59:38.420 --> 00:59:44.820]   implementing band copy protection prevent people from doing that you know
[00:59:44.820 --> 00:59:51.380]   the tactic engine it can secretly give you a shock is there's a needle buried
[00:59:51.380 --> 00:59:56.980]   deep within the watch the tiny drop of strict nine our show today brought to you
[00:59:56.980 --> 01:00:01.940]   by Gazelle if you are hankering for something new and shiny the right thing
[01:00:01.940 --> 01:00:06.180]   to do would be to take the old shiny and sell it on gazelle don't put it in the
[01:00:06.180 --> 01:00:09.180]   drawer where it's gonna gather dust don't throw it into the landfill there
[01:00:09.180 --> 01:00:14.580]   plenty of people would like your your old iPhone your old iPad your Samsung
[01:00:14.580 --> 01:00:20.100]   or Galaxy device your blackberry your tablets gazelle paste top dollar and
[01:00:20.100 --> 01:00:23.060]   you know what's great you can go to gazelle right now get a quote and that
[01:00:23.060 --> 01:00:26.220]   quotes locked in for 30 days you have plenty of time to think about it to
[01:00:26.220 --> 01:00:30.500]   ponder to get the new device to copy the data over if you forget to wipe the
[01:00:30.500 --> 01:00:34.180]   data on your old device they'll do it for you in fact they even buy broken
[01:00:34.180 --> 01:00:36.940]   iPhones and iPads of course you wouldn't be able to wipe the data on that but
[01:00:36.940 --> 01:00:40.140]   they'll take care of that for you they're great then you might say well what
[01:00:40.140 --> 01:00:43.500]   happens to these fabulous oh and by the way when you do sell them they'll send
[01:00:43.500 --> 01:00:46.980]   you a box postage paid on anything worth more than a buck so you don't even pay
[01:00:46.980 --> 01:00:52.020]   postage on that and they turn it around fast they'll give you cash check or pay
[01:00:52.020 --> 01:00:56.460]   pal credit or Amazon gift card that is a nice thing because they bump the
[01:00:56.460 --> 01:01:01.140]   Amazon gift card up 5% but you may ask as I am often asked well what happens to
[01:01:01.140 --> 01:01:05.940]   those gadgets well the very best gadgets that gazelle gets they sell back in
[01:01:05.940 --> 01:01:12.460]   their certified pre-owned program from gazelle devices are available you can
[01:01:12.460 --> 01:01:17.860]   get the I think iPhones iPads and Galaxy devices they're in two conditions
[01:01:17.860 --> 01:01:23.220]   certified like new like new certified good show some gentle signs of wear but
[01:01:23.220 --> 01:01:27.060]   you know they work a hundred percent and you save of course some extra money
[01:01:27.060 --> 01:01:30.900]   you get a great device at a great price gazelles put everything through their
[01:01:30.900 --> 01:01:34.260]   30-point inspection process it's absolutely rigorous everything's fully
[01:01:34.260 --> 01:01:37.820]   functional and of course certified pre-owned devices are backed by a 30-day
[01:01:37.820 --> 01:01:44.060]   risk free return policy if you want a new gadget do the right thing bring it to
[01:01:44.060 --> 01:01:53.180]   gazelle or buy it from gazelle you can do both G A Z E L L E gazelle dot com
[01:01:53.180 --> 01:02:00.180]   great panel here serenity Caldwell and Thompson Matthew Ingram some you know what
[01:02:00.180 --> 01:02:05.780]   I like about all of you all have distinct voices and I mean speaking
[01:02:05.780 --> 01:02:09.900]   voices I mean writing what your your your your positions are clear and
[01:02:09.900 --> 01:02:15.900]   distinct I love that in fact that's we kind of live in the age of a personality
[01:02:15.900 --> 01:02:19.620]   media more than ever before even anchors you know you're a little bit more
[01:02:19.620 --> 01:02:22.820]   distinct but boy you get on the web and everybody has a distinct unique style
[01:02:22.820 --> 01:02:28.660]   and I love that but I have to I'm a little worried first was giga almost even
[01:02:28.660 --> 01:02:32.660]   the first to go under but giga home suddenly pulled the plug happy to say
[01:02:32.660 --> 01:02:36.580]   Matthew a german five of his colleagues went to fortune the rest seemed to all
[01:02:36.580 --> 01:02:43.580]   have found jobs elsewhere as well now this week recode which if anything you
[01:02:43.580 --> 01:02:47.660]   would think with Walt Mossberg and Cara Swisher Liz Gaines I mean what a
[01:02:47.660 --> 01:02:51.940]   great team John Chamellesky somebody so many great people there that was a
[01:02:51.940 --> 01:02:57.380]   great blog and and Matthew maybe you know better than I but the fact that they
[01:02:57.380 --> 01:03:04.060]   were sold to Vox Media for stock indicates does it not that that they were in
[01:03:04.060 --> 01:03:08.780]   that this was a fire sale that they needed to sell quick I don't know if
[01:03:08.780 --> 01:03:13.380]   that's if we can jump to that conclusion okay I don't know that it's fair to say
[01:03:13.380 --> 01:03:18.900]   it was a fire sale I have heard reports from people close to the company that
[01:03:18.900 --> 01:03:24.780]   they had lots of money left okay I think I think they I don't I don't think they
[01:03:24.780 --> 01:03:30.100]   had to sell or certainly had to sell right now but it feels to me like they
[01:03:30.100 --> 01:03:34.220]   felt eventually they were gonna have to do something right they were gonna have
[01:03:34.220 --> 01:03:37.460]   to sell or they were gonna have to partner up with somebody because it's
[01:03:37.460 --> 01:03:45.540]   just it's harder and harder to make a go of it as a as a small entity so and my
[01:03:45.540 --> 01:03:50.540]   analogy is the kind of the barbell effect I mean you either have to be super small
[01:03:50.540 --> 01:03:54.780]   and sort of hyper targeted and focused and I think you're a good example of
[01:03:54.780 --> 01:03:59.420]   doing that and how I can work or you have to be huge and massive and have
[01:03:59.420 --> 01:04:03.900]   immense reach and billions of paid views I mean that's the kind of game we're
[01:04:03.900 --> 01:04:09.300]   talking about they were not either of those things so they were sort of focused
[01:04:09.300 --> 01:04:14.300]   but still you know 45 people that's a fair number gig oh was arguably in the
[01:04:14.300 --> 01:04:18.860]   same kind of category in that kind of Valley of death in the middle where you're
[01:04:18.860 --> 01:04:24.620]   not big enough to have scale but you're too big to kind of be hyper targeted and
[01:04:24.620 --> 01:04:27.980]   focused that's what it feels like to me and so they saw the writing on the wall
[01:04:27.980 --> 01:04:33.300]   and decided you know Vox is great you know Jim Bank office a smart guy they
[01:04:33.300 --> 01:04:39.180]   they're sort of some patico on a lot in a lot of ways and they've got a bunch of
[01:04:39.180 --> 01:04:43.820]   money because they just did a huge round. Kara said and she's very honest that's
[01:04:43.820 --> 01:04:46.540]   one of the things we really like about Kara but we've got to get her back on
[01:04:46.540 --> 01:04:51.820]   soon. Sometimes too honest. Yeah yeah she she said to them told the New York
[01:04:51.820 --> 01:04:55.900]   Times everybody's bigger than us it's not a secret that being us by the way
[01:04:55.900 --> 01:04:59.740]   no because we're smaller than you Kara it's not a secret that being a smaller
[01:04:59.740 --> 01:05:03.660]   fish is really hard kind of implying there were some difficulties at recode.
[01:05:03.660 --> 01:05:09.220]   Well and I think my sense was that one of those difficulties was if you're an
[01:05:09.220 --> 01:05:13.780]   independent entity like them you have to have your own back end you have to
[01:05:13.780 --> 01:05:18.580]   build your own stuff you have to do all your own at sales you have to have all
[01:05:18.580 --> 01:05:23.900]   that stuff yourself that adds a huge amount of you know just cost and it's
[01:05:23.900 --> 01:05:29.900]   time-consuming and it's much easier to use the resources of a larger entity
[01:05:29.900 --> 01:05:33.340]   and I think that was an appeal for them. I've got some.
[01:05:33.340 --> 01:05:37.260]   Can I impress? Oh go ahead Eddie. Oh no sir I just couldn't say I mean when when
[01:05:37.260 --> 01:05:40.780]   the news came out I was a bit shot because I mean Kara in particular we
[01:05:40.780 --> 01:05:46.420]   were on a free what was a great journal is all but they got 44 employees on a
[01:05:46.420 --> 01:05:51.460]   hit count of was it 1.5 million unique some month that's an awful lot of stuff
[01:05:51.460 --> 01:05:55.260]   and the key to any kind of web or any kind of publishing operation is you've
[01:05:55.260 --> 01:05:59.140]   got to keep the cost low so you can stay profitable. I mean they had more stuff
[01:05:59.140 --> 01:06:03.140]   than we do and you know we still managed to turn a profit on this one so there
[01:06:03.140 --> 01:06:06.540]   was a lot of spend going on and not a lot of return coming in I don't understand
[01:06:06.540 --> 01:06:11.700]   why they screwed it up. What about the the kind of some people were saying well
[01:06:11.700 --> 01:06:16.460]   it shows you that it's not the brand the journalist brand by itself is not that
[01:06:16.460 --> 01:06:20.940]   valuable that Mossberg minus the Wall Street Journal is not as valuable that
[01:06:20.940 --> 01:06:25.980]   David Pogue minus the New York Times is not as valuable is that? Certainly Bob
[01:06:25.980 --> 01:06:31.020]   Bluff sets you know had a newsletter posting which he talked about that Dan
[01:06:31.020 --> 01:06:36.940]   Lyons. My answer is the way I said that. Yeah Facebook yeah. They were both
[01:06:36.940 --> 01:06:41.940]   basically making the same point which is it's not enough to just be a star you
[01:06:41.940 --> 01:06:47.900]   have to be associated with a big entity in some way you know Bob compared it to
[01:06:47.900 --> 01:06:52.900]   musicians try and go out on their own and do their own albums and it's just a
[01:06:52.900 --> 01:06:58.180]   lot of work and so I think you can do it I mean there's certainly examples of
[01:06:58.180 --> 01:07:01.740]   people who have done it John Gruber is a great example of someone who's been
[01:07:01.740 --> 01:07:08.020]   able to but he's one guy. Well that's the example. That's the problem. Yeah.
[01:07:08.020 --> 01:07:13.380]   A single guy even Andrew Sullivan you could argue he didn't make a piece shut up.
[01:07:13.380 --> 01:07:20.340]   He shut down but but he also he had eight or nine or ten people yeah that's a
[01:07:20.340 --> 01:07:26.540]   large amount of costs if it had been him and you know one other person Ben
[01:07:26.540 --> 01:07:32.540]   Thompson who does TraTakari runs the whole thing from his house in Taipei and
[01:07:32.540 --> 01:07:37.340]   so for him getting three thousand people to pay a hundred dollars a year boom
[01:07:37.340 --> 01:07:42.420]   he's you know that's a great business for him but that wasn't what Walt and
[01:07:42.420 --> 01:07:46.260]   Cara were trying to do. They were trying to build and it isn't what Gigome was
[01:07:46.260 --> 01:07:50.660]   trying to do rightly or wrongly. They were trying to build something large and
[01:07:50.660 --> 01:07:55.820]   in order to build something large that game has changed a lot like you have to
[01:07:55.820 --> 01:08:01.020]   be larger than large. You know Buzzfeed's getting 200 million page views or
[01:08:01.020 --> 01:08:04.900]   unique visitors or something that's not a game that you can play without a lot
[01:08:04.900 --> 01:08:10.420]   of money. No I really feel it falls it start falling into two camps where
[01:08:10.420 --> 01:08:14.700]   previously if you wanted to have a big website and you wanted to have a lot of
[01:08:14.700 --> 01:08:19.540]   a lot of following and a lot of page views you would throw a little bit of
[01:08:19.540 --> 01:08:23.060]   money at it you would get a couple of you know crack talent and then just keep
[01:08:23.060 --> 01:08:27.180]   on shoving people into the machine and being like all right the more content we
[01:08:27.180 --> 01:08:31.820]   turn out the more page views were gonna get right right and it used to be that
[01:08:31.820 --> 01:08:35.140]   that web companies didn't feel like they had to pay that much money for it and
[01:08:35.140 --> 01:08:38.700]   now that the web has become more competitive they see that and they're
[01:08:38.700 --> 01:08:42.820]   like oh well if we're going to launch a new website we need to pour in millions
[01:08:42.820 --> 01:08:48.020]   and millions of dollars to make sure that we launch. Think about the daily is a
[01:08:48.020 --> 01:08:51.940]   great example of this. The daily launched with a you know with two offices and a
[01:08:51.940 --> 01:08:57.460]   bunch of staff and a huge video team and they launched huge at scale. And Rupert
[01:08:57.460 --> 01:09:02.380]   Murdoch's money. Yeah and well but exactly they they launched with they they
[01:09:02.380 --> 01:09:06.860]   launched with Murdoch's backing. They hired a hundred and one year later.
[01:09:06.860 --> 01:09:12.420]   Yeah. Oh and touch. Well because ultimately ultimately I feel like the model of
[01:09:12.420 --> 01:09:16.980]   throwing a bunch of money at the wall and then just being like with a hundred
[01:09:16.980 --> 01:09:20.980]   monkeys typing and a hundred typewriters we will have internet gold. It doesn't
[01:09:20.980 --> 01:09:24.820]   quite work like that anymore. It really doesn't. I started this discussion with
[01:09:24.820 --> 01:09:30.260]   the word voices. Unique per strong personal voices seem very important.
[01:09:30.260 --> 01:09:35.020]   But the structure of journalism doesn't really doesn't really benefit
[01:09:35.020 --> 01:09:39.500]   that as much as it used to. Back in the old days of print if you got an exclusive
[01:09:39.500 --> 01:09:44.540]   the only place you could read it was on your news site the next day. Right now
[01:09:44.540 --> 01:09:49.340]   when recode or when Giggorome or anyone else breaks a big story. Yeah you'll get
[01:09:49.340 --> 01:09:53.100]   the highly cited tag that no one reads on the Google news rankings that brings
[01:09:53.100 --> 01:09:56.660]   in your quarter of your readership. But at the same time everyone and his dog
[01:09:56.660 --> 01:10:00.660]   will write a quick pracy if you're lucky they'll link into you. So there's no real
[01:10:00.660 --> 01:10:05.140]   benefit. You know and it's just the journalism which gets in the hits and the
[01:10:05.140 --> 01:10:09.740]   actual. I think that's I think that's why the hardest model of all is to do
[01:10:09.740 --> 01:10:14.820]   news. Even if it is tech news or even if it's something else news it's going to
[01:10:14.820 --> 01:10:19.020]   be out there. Is your version of that news going to be really dramatic. It's a
[01:10:19.020 --> 01:10:23.380]   commodity is what you say. Right. Come on and sell it. What does seem to work and
[01:10:23.380 --> 01:10:26.620]   certainly what worked for Andrew Sullivan raised you know almost a million
[01:10:26.620 --> 01:10:31.700]   dollars. Ben Thompson and Sir Techery can seem to be able to make this work. It's
[01:10:31.700 --> 01:10:36.780]   analysis. Ben is not doing news right he's telling people what to think about
[01:10:36.780 --> 01:10:41.500]   all the news that has been flowing over them for the past whatever 24 48 hours.
[01:10:41.500 --> 01:10:46.020]   What is important what matters. This is an analogous actually to a
[01:10:46.020 --> 01:10:50.660]   transition that happened when the internet happened you know 20 years ago
[01:10:50.660 --> 01:10:56.580]   the fact became devalued in pre-internet going to the library and
[01:10:56.580 --> 01:11:01.500]   ascertaining a fact took some labor and that gave that fact some value. What
[01:11:01.500 --> 01:11:04.940]   do we teach kids in school to memorize facts. There was some value it's
[01:11:04.940 --> 01:11:09.260]   completely devalued now. If you want to know when somebody was born it's three
[01:11:09.260 --> 01:11:13.340]   seconds away thanks to Google. Right. But what is not what has not become
[01:11:13.340 --> 01:11:16.900]   devalued. In fact what is grain value and the devaluation the fact is curation
[01:11:16.900 --> 01:11:23.700]   analysis context all of the things that a human brain can add to fact and
[01:11:23.700 --> 01:11:28.260]   that's just a special example of that. Right and I think it's also it's not
[01:11:28.260 --> 01:11:33.340]   just like you said the voices it's not just it's it's trust. I think that's
[01:11:33.340 --> 01:11:37.980]   Craig Newmark says trust is the new black and I think in a lot of ways he's
[01:11:37.980 --> 01:11:42.780]   right the it's not you can get any amount of information you want about
[01:11:42.780 --> 01:11:47.580]   anything more than you could ever want so quickly. Yeah you want someone that you
[01:11:47.580 --> 01:11:52.460]   trust to say don't pay attention to any of that pay attention to this. That's
[01:11:52.460 --> 01:11:56.620]   kind of the important thing. That was the insight that I created
[01:11:56.620 --> 01:12:01.260]   Twit 10 years ago was well I'm not going to get in the business of reporting
[01:12:01.260 --> 01:12:04.420]   because that's commoditized and what am I going to do that's going to be any
[01:12:04.420 --> 01:12:08.100]   different than than a hundred better reporters on the ground in Silicon
[01:12:08.100 --> 01:12:12.740]   Valley but what we can add is trust where the analysis and understanding and
[01:12:12.740 --> 01:12:16.940]   context and and I have to say personality is a very big and part of that too.
[01:12:16.940 --> 01:12:20.820]   Sure sureness. I mean publications can differentiate themselves on a news
[01:12:20.820 --> 01:12:24.580]   perspective in the way they approach the news certainly I mean if you want sort
[01:12:24.580 --> 01:12:28.700]   of a straight read go to AP if you want something with a bit of character go
[01:12:28.700 --> 01:12:32.380]   somewhere else as long as the basic facts are right and the bet you know and
[01:12:32.380 --> 01:12:36.300]   the approach is right that's fine. I mean but what worried me I mean less
[01:12:36.300 --> 01:12:38.860]   worried about Rico and I was more worried about Gigaron because you had a
[01:12:38.860 --> 01:12:42.900]   bunch of really good journalists there you guys are too smart and yeah and it
[01:12:42.900 --> 01:12:45.980]   just for some reason it didn't work and they pulled the funding out of it.
[01:12:45.980 --> 01:12:50.660]   Well and I would say and I would say if there was a mistake and I don't you know
[01:12:50.660 --> 01:12:55.620]   I wasn't involved in the business side I didn't I don't have the details about
[01:12:55.620 --> 01:13:01.820]   those aspects. What it feels like to me is that Gigome tried to get too big too
[01:13:01.820 --> 01:13:08.980]   quickly and that certainly that opened up a gap obviously a financial gap and I
[01:13:08.980 --> 01:13:12.900]   I don't feel like recode was there but I think they certainly could have gotten
[01:13:12.900 --> 01:13:17.100]   there if you want to be a certain size or you're trying to get to a certain size
[01:13:17.100 --> 01:13:21.540]   but that doesn't match up with what you're bringing in eventually you're just
[01:13:21.540 --> 01:13:27.380]   gonna you're gonna run out of rope. You're gonna burn yeah yeah. I think that's kind of
[01:13:27.380 --> 01:13:32.220]   sorry go on. No I was just gonna say I mean we ran into the same thing at IDG
[01:13:32.220 --> 01:13:37.300]   with Mackerel and Tech Hive and PC World where it's just at a certain point
[01:13:37.300 --> 01:13:42.700]   you can't you you can't run old media business on new media you can't say I
[01:13:42.700 --> 01:13:46.980]   want a staff of 40 to 100 reporters who are full-time with benefits and
[01:13:46.980 --> 01:13:51.460]   everything else which is not to say that people shouldn't be full-time but that
[01:13:51.460 --> 01:13:56.460]   you figure out okay how many full-time people can we make can we reasonably
[01:13:56.460 --> 01:14:01.780]   pay and do excellent reporting who are all smart wonderful you know talented
[01:14:01.780 --> 01:14:06.540]   people now let's let's run the site with the bare minimum of that running
[01:14:06.540 --> 01:14:11.100]   reasonable hours see what we can do and then when there is improvement build
[01:14:11.100 --> 01:14:14.380]   upon that the problem is is that the big companies or the companies that are
[01:14:14.380 --> 01:14:19.620]   used to you know are used to old media are you you know companies like IDG for
[01:14:19.620 --> 01:14:23.900]   example they look at oh well you know the news like in order for us to get page
[01:14:23.900 --> 01:14:28.100]   use quickly so that we can pay off our advertising and all of that we need all
[01:14:28.100 --> 01:14:31.660]   of the content in order to have all the content we need writers and emerge and
[01:14:31.660 --> 01:14:34.620]   have writers we have to have a huge staff and we need to make sure that
[01:14:34.620 --> 01:14:37.340]   everybody's talking to each other so we're going to get a big office and it
[01:14:37.340 --> 01:14:40.380]   has to be in New York or San Francisco because that's where all the news is
[01:14:40.380 --> 01:14:44.180]   and it's like this is the internet this is the age of instant communication you
[01:14:44.180 --> 01:14:47.420]   don't need to be in San Francisco or New York anymore it helps that's a great
[01:14:47.420 --> 01:14:50.900]   point and I think yeah and I think for entities like the New York Times or
[01:14:50.900 --> 01:14:56.300]   the Washington Post or or any sort of large magazine or I mean Jesus 14
[01:14:56.300 --> 01:15:00.980]   fortune and time-maker facing the same problem you there's this feeling that
[01:15:00.980 --> 01:15:06.580]   you have to figure out how to support the cost base that you have so we have
[01:15:06.580 --> 01:15:10.180]   all these reporters and we have all these offices and we have all this stuff we
[01:15:10.180 --> 01:15:14.060]   need to find out a way to pay for that digitally but that's the wrong way of
[01:15:14.060 --> 01:15:20.540]   looking at the problem you you you you can't find a way to support that old
[01:15:20.540 --> 01:15:25.220]   cost structure because of the model you use print advertising is going away and
[01:15:25.220 --> 01:15:30.460]   that and what's replacing it is is ten times smaller you know it's an order of
[01:15:30.460 --> 01:15:34.660]   magnitude smaller so you can't just say well we need to find a way to replace
[01:15:34.660 --> 01:15:40.540]   you have to rethink the whole way you do what you do and the whole structure
[01:15:40.540 --> 01:15:44.500]   that you've built up over time because it's not it doesn't make financial sense
[01:15:44.500 --> 01:15:48.340]   anymore yeah makes me feel damn lucky that's all I can say I don't know how
[01:15:48.340 --> 01:15:53.020]   we're surviving well I think with a digital publication everyone has to do
[01:15:53.020 --> 01:15:56.660]   everyone has to work you can't afford to have large accounts departments or
[01:15:56.660 --> 01:16:01.740]   human resources departments everyone's got to either write sell manage or edit
[01:16:01.740 --> 01:16:06.380]   and it's just to be you have to be very focused about what you want to do right
[01:16:06.380 --> 01:16:11.780]   you can't just say well we just we want to be a really great site with lots of
[01:16:11.780 --> 01:16:15.460]   news about stuff like I think you just wanted to everything yeah you have to be
[01:16:15.460 --> 01:16:20.260]   very very focused because the days of sort of a mass media entity where and you
[01:16:20.260 --> 01:16:25.660]   went to that entity to get everything from sports to business to to gardening
[01:16:25.660 --> 01:16:30.140]   columns to comics to whatever that's just gone and it's not going to come back
[01:16:30.140 --> 01:16:34.180]   it doesn't matter how good you are and so
[01:16:34.180 --> 01:16:40.780]   ruins everything that's the damn it that's the plot line of this show no no it's
[01:16:40.780 --> 01:16:44.420]   entirely true though you know what my math is saying is there a hundred
[01:16:44.420 --> 01:16:46.860]   you know what if there weren't Google somebody else would have come along
[01:16:46.860 --> 01:16:52.140]   what was the internet to ruin everything right and the internet removes in
[01:16:52.140 --> 01:16:57.180]   efficiency and you know what human beings are damn it is hugely inefficient so
[01:16:57.180 --> 01:17:01.580]   how do you make things more efficient you remove the human beings and so if you
[01:17:01.580 --> 01:17:06.700]   can remove human beings from your by the way that's what's happened to giga
[01:17:06.700 --> 01:17:11.060]   home don't you think they've removed the humans and now the brand was bought
[01:17:11.060 --> 01:17:14.620]   my knowingly which is it what they call a demand media company which basically
[01:17:14.620 --> 01:17:19.980]   is a light term for it machine generated stories that are clickbait that's kind
[01:17:19.980 --> 01:17:23.620]   of removing the humans isn't it it is a little bit and I think there's you know
[01:17:23.620 --> 01:17:29.620]   what I have when even when I wrote about demand media I thought it made a lot of
[01:17:29.620 --> 01:17:33.500]   sense their model I don't want to do it they're gaming the internet they're
[01:17:33.500 --> 01:17:37.820]   doing exactly sure they are so what you do with demand media is you look and see
[01:17:37.820 --> 01:17:41.580]   what search terms get a lot of results you could do this automatically then
[01:17:41.580 --> 01:17:47.300]   you'd buy or generate cheap 500 word articles let's say people are searching
[01:17:47.300 --> 01:17:52.020]   for Elvis belt buckles well you just make damn sure you have an article about
[01:17:52.020 --> 01:17:56.620]   Elvis belt buckles however stupid taudrey poorly written doesn't matter
[01:17:56.620 --> 01:18:01.340]   because you put ads on that page and you will get links that's demand media and
[01:18:01.340 --> 01:18:04.940]   you know what I would say that that model that model is not that different
[01:18:04.940 --> 01:18:09.380]   from what some websites are doing going to look different than BuzzFeed vice and
[01:18:09.380 --> 01:18:13.100]   all the other really male they look at it huffing and post invented that
[01:18:13.100 --> 01:18:19.180]   yeah so and that's one model right if you if you want to try and make the
[01:18:19.180 --> 01:18:22.900]   internet work for you that is one model it's not for everyone that's
[01:18:22.900 --> 01:18:26.900]   certainly not for me but that is one way to approach it the other way is to just
[01:18:26.900 --> 01:18:30.740]   go to the complete opposite end of the spectrum yeah which is to be as as
[01:18:30.740 --> 01:18:36.180]   targeted as you can on a specific user base or community that you connect
[01:18:36.180 --> 01:18:39.300]   with and that's one thing that's what it does we're made with humans yeah yeah
[01:18:39.300 --> 01:18:42.940]   and I don't want to I don't want to you know flatter you unnecessarily but I
[01:18:42.940 --> 01:18:47.460]   think Twitter does a great job of doing that connecting with readers and users
[01:18:47.460 --> 01:18:52.420]   and viewers who care about specific things and telling them the things they
[01:18:52.420 --> 01:18:56.740]   want to know and and having a back and forth with them yeah that is very very
[01:18:56.740 --> 01:19:00.580]   difficult to replace yeah and you can't do that with an algorithm it just
[01:19:00.580 --> 01:19:05.740]   doesn't work it doesn't work without humans algorithms are also dependent when
[01:19:05.740 --> 01:19:09.220]   we talk about the age of sort of the disconnected web where everybody's going
[01:19:09.220 --> 01:19:14.620]   to to places for different things Google and Duck.Go and Yahoo and any any
[01:19:14.620 --> 01:19:19.700]   search engine is a major component of that and I mean you saw demands we saw
[01:19:19.700 --> 01:19:24.260]   demands take a huge dump a couple years back when Google basically decided oh
[01:19:24.260 --> 01:19:27.620]   we're going to yeah we're gonna change our algorithm and we're going to
[01:19:27.620 --> 01:19:32.500]   completely mess up everything and that's the Facebook risk too right you
[01:19:32.500 --> 01:19:37.500]   hook your wagon to Facebook and they're your best pal and then they decide they
[01:19:37.500 --> 01:19:40.700]   actually want to target some other kind of content and then your content
[01:19:40.700 --> 01:19:43.420]   disappears yeah when you're left holding the bat seems like this is
[01:19:43.420 --> 01:19:47.180]   historical bug and I think it's historically inevitable humans are great
[01:19:47.180 --> 01:19:51.900]   at there's always going to be a sec 10% of humans who just who's only real
[01:19:51.900 --> 01:19:55.660]   interest is making money who figure out how I can game a system of whether it's
[01:19:55.660 --> 01:20:01.620]   tulip bulbs or the internet or whatever how can I game that system make a ton
[01:20:01.620 --> 01:20:07.260]   of money I don't really care you know about the product I'm making and and then
[01:20:07.260 --> 01:20:10.180]   there's always the other than the other 10 mostly the people in the middle but
[01:20:10.180 --> 01:20:13.620]   there's not these the other 10% that is doing this kind of handcrafted or
[01:20:13.620 --> 01:20:18.020]   tissing on media where they we don't really care about the money we care about
[01:20:18.020 --> 01:20:22.620]   creating the best possible product and I think that was one thing I'm actually
[01:20:22.620 --> 01:20:28.780]   kind of sad that Andrew Sullivan quit because I think he did show that you can
[01:20:28.780 --> 01:20:33.140]   do it it was working you know the daily dish was making a million dollars that's
[01:20:33.140 --> 01:20:36.340]   not an insignificant amount for one person that's rare two people that's
[01:20:36.340 --> 01:20:41.020]   and I mean he felt burnt out and so on but I still think he was doing exactly
[01:20:41.020 --> 01:20:45.500]   the same thing that I wrote a column about him and Amanda Palmer who was
[01:20:45.500 --> 01:20:49.740]   kick-startering yeah you know a record and I was saying fundamentally they're
[01:20:49.740 --> 01:20:53.660]   doing the same thing one is doing it with music and one is doing it with you
[01:20:53.660 --> 01:20:59.020]   know what Andrew was doing opinion but most of them were connecting with a
[01:20:59.020 --> 01:21:04.260]   community of readers and listeners and and that's all they cared about right
[01:21:04.260 --> 01:21:08.860]   probably I don't know but I would guess that Andrew made enough money that he
[01:21:08.860 --> 01:21:13.580]   could stop you know yeah and then he said good I got a pile of money here I'm
[01:21:13.580 --> 01:21:16.900]   tired I want to do something else and he just stopped and that's one thing that
[01:21:16.900 --> 01:21:19.820]   does happen that's very different if you're not a big business and you're not
[01:21:19.820 --> 01:21:28.060]   building you know a legacy you just stop I am done and I will say I don't know
[01:21:28.060 --> 01:21:33.260]   what care what's what their motivations were but it is a hard work doing what
[01:21:33.260 --> 01:21:37.700]   they've tried to do filing and things de-started yeah that stuff is really
[01:21:37.700 --> 01:21:40.780]   really hard especially when you're a small company and I certainly don't
[01:21:40.780 --> 01:21:45.060]   blame them for deciding oh not like someone else to do all of that hard work
[01:21:45.060 --> 01:21:49.900]   and they can just go back to doing what they do do you think now the rumor was
[01:21:49.900 --> 01:21:54.580]   that Comcast which is an investor in both vox the purchaser and recode the
[01:21:54.580 --> 01:22:02.180]   seller pushed them together in order to later to vower them both well it certainly
[01:22:02.180 --> 01:22:08.740]   seems plausible to me I mean Comcast has talked about at least reportedly has
[01:22:08.740 --> 01:22:13.380]   talked to vox about acquiring the whole thing is this what happens is is there's
[01:22:13.380 --> 01:22:17.300]   just consolidation of the media it seems to happen in every business at
[01:22:17.300 --> 01:22:20.940]   every time it's that you get the you get something new an innovation like the
[01:22:20.940 --> 01:22:25.980]   internet which creates an environment in which many minnows can blossom and
[01:22:25.980 --> 01:22:29.620]   then a bigger fish came and ate all the minnows and a bigger fish ate that one
[01:22:29.620 --> 01:22:32.220]   and a bigger fish ate that one and ultimately consolidates back to the way
[01:22:32.220 --> 01:22:35.740]   it was and occasionally one of the big fish chokes on its own food and diets
[01:22:35.740 --> 01:22:39.620]   and if you want eats the remains and the whole thing cycles around again yes but
[01:22:39.620 --> 01:22:49.980]   I mean yeah try and disnify that one I'm worried by the thought of Comcast
[01:22:49.980 --> 01:22:54.300]   taking that over I'm equally worried about Verizon's investments in the
[01:22:54.300 --> 01:22:57.540]   media field particularly as Verizon apparently tried to set up a magazine
[01:22:57.540 --> 01:23:01.500]   last year which said you can write about anything apart from net neutrality and
[01:23:01.500 --> 01:23:06.900]   government's looping so you know it's tricky you know I don't trust
[01:23:06.900 --> 01:23:10.740]   Comcast but I can throw them but I say give them the benefit that out so far
[01:23:10.740 --> 01:23:14.220]   this and we'll have to ask our historian of journalism Jeff Jarvis at
[01:23:14.220 --> 01:23:21.060]   some point but this sounds like the history of media for 200 years I mean I
[01:23:21.060 --> 01:23:26.860]   don't think that the Hearst Empire was in any way pure or in total
[01:23:26.860 --> 01:23:32.020]   the only part is in it not at all danger you know so this is not nothing new
[01:23:32.020 --> 01:23:36.620]   and it's and it's interesting to think about you know I work for a unit of
[01:23:36.620 --> 01:23:42.060]   timing now timing was based on a magazine that Henry Booth right Lou
[01:23:42.060 --> 01:23:46.500]   started when he was in his 20s I'm pretty sure he started it with much the
[01:23:46.500 --> 01:23:50.300]   same attitude that BuzzFeed started with he aggregated the heck out of
[01:23:50.300 --> 01:23:54.460]   everything he took stories from other people and rewrote them so that they
[01:23:54.460 --> 01:23:58.500]   were funny or shorter or whatever and then he built this giant media empire
[01:23:58.500 --> 01:24:02.500]   that now is the thing that everyone else is trying to destabilize I mean
[01:24:02.500 --> 01:24:07.220]   there are patterns that repeat themselves isn't that interesting how that
[01:24:07.220 --> 01:24:10.580]   happens we're gonna take a break we're gonna pack well I do want to talk a
[01:24:10.580 --> 01:24:16.660]   little bit and we wish you shot a picture of the the Google giant 16
[01:24:16.660 --> 01:24:21.140]   camera virtual reality array we heard a lot about cardboard I want to talk a
[01:24:21.140 --> 01:24:26.420]   little bit about get back to the shiny stuff in just a second but you know I
[01:24:26.420 --> 01:24:30.300]   what I love is having a panel of smart people and we could talk and this is
[01:24:30.300 --> 01:24:34.940]   one thing you can't do on mainstream media no for 15 20 minutes on a
[01:24:34.940 --> 01:24:42.060]   completely philosophical subject that no one cares about that's Twitter
[01:24:42.060 --> 01:24:47.340]   you got enough people watching our mission statement we talk about
[01:24:47.340 --> 01:24:51.340]   obsessively about stuff you couldn't care less about our show today brought to
[01:24:51.340 --> 01:24:54.500]   you by Dropbox for business if you're in business you know your employees are
[01:24:54.500 --> 01:24:58.780]   using Dropbox we did a little survey it was so funny everybody in the office
[01:24:58.780 --> 01:25:02.020]   is using their personal Dropbox account to share stuff to work together to
[01:25:02.020 --> 01:25:06.060]   collaborate which is great I'm not knocking it they love Dropbox it works
[01:25:06.060 --> 01:25:09.700]   great for them but I was thrilled when we could sign up for Dropbox for business
[01:25:09.700 --> 01:25:13.460]   and kind of get that all consolidated it's the same experience the same UI our
[01:25:13.460 --> 01:25:17.420]   employees already love we don't have to train them we don't have to persuade
[01:25:17.420 --> 01:25:21.300]   them to use it they say oh yeah yeah great and they each get a terabyte of
[01:25:21.300 --> 01:25:26.780]   data so they go oh you know I don't have to pay for it yeah but as the employee
[01:25:26.780 --> 01:25:31.140]   error you control it the IT professionals have admin controls like
[01:25:31.140 --> 01:25:34.780]   remote wipe somebody leaves the company they don't leave with all your goodies
[01:25:34.780 --> 01:25:47.420]   I saw that was it was suing was it it was a jawbone suing fit bit yeah fit bit
[01:25:47.420 --> 01:25:50.580]   fit bit was hiring away employees and they'd say hey before you leave could you
[01:25:50.580 --> 01:25:53.300]   just put a thumb drive in the computer and download all the data bring along
[01:25:53.300 --> 01:25:59.140]   with you oh sure no problem wearing orange if they're doing that yes so
[01:25:59.140 --> 01:26:03.220]   this is beauty you have control remote wipe you'd get great sharing and
[01:26:03.220 --> 01:26:07.780]   permission controls complete audit logs you know that the people only the right
[01:26:07.780 --> 01:26:10.780]   people are getting that sensitive company information so your employees
[01:26:10.780 --> 01:26:14.380]   are happy you're happy and of course it integrates with third-party security
[01:26:14.380 --> 01:26:19.700]   and admin solutions SIEM DLP e-discovery and they know that you really want
[01:26:19.700 --> 01:26:24.020]   stuff to be private so of course Dropbox for business uses encryption for file
[01:26:24.020 --> 01:26:29.980]   data in transit and at rest plus segmentation and hashing to anonymize files
[01:26:29.980 --> 01:26:33.740]   and there are additional security features like single sign-on and two-step
[01:26:33.740 --> 01:26:38.820]   verification all of which mean this is a solution you will love four million
[01:26:38.820 --> 01:26:42.500]   businesses are now converting from those personal Dropbox accounts to
[01:26:42.500 --> 01:26:47.460]   Dropbox for business we did and I highly recommend you give it a try take
[01:26:47.460 --> 01:26:51.100]   advantage of what your employees already know and love they love Dropbox
[01:26:51.100 --> 01:26:56.820]   who doesn't sign up for Dropbox for business Dropbox dot com slash to it for
[01:26:56.820 --> 01:27:02.980]   free 14-day trial Dropbox for business drop box dot com slash to it we thank
[01:27:02.980 --> 01:27:10.020]   them so much for their support so what is the name of this I want this camera
[01:27:10.020 --> 01:27:15.860]   jump jump the jump yeah you just got a jump they should have David Lee Roth
[01:27:15.860 --> 01:27:21.900]   come out I'll go I'll just say at the afterparty you know a loincloth
[01:27:21.900 --> 01:27:30.820]   jump itself anymore for this thing jump jump everybody so this is 16 GoPro well
[01:27:30.820 --> 01:27:34.100]   they're gonna first of all they're gonna publish a model for this so that
[01:27:34.100 --> 01:27:39.180]   anybody can open source plans right so anybody can build this there are
[01:27:39.180 --> 01:27:43.060]   Dropbox says we'll sell one which I figured if you got 16 drop boxes we're
[01:27:43.060 --> 01:27:47.660]   talking six or seven grand I mean what I said Dropbox GoPro obviously six or
[01:27:47.660 --> 01:27:52.940]   seven grand but you know that's not a lot well and they said you can actually
[01:27:52.940 --> 01:27:57.140]   build the array for it but I'm gonna build one you know I was talking
[01:27:57.140 --> 01:28:01.260]   obsessed but they know they do love it it's awesome and YouTube supports it
[01:28:01.260 --> 01:28:06.060]   right so you'll be able to record these videos you'll even be able to do it live
[01:28:06.060 --> 01:28:11.140]   so here's my pledge to you as soon as we can we're gonna build one of these
[01:28:11.140 --> 01:28:15.860]   suckers and we're gonna put it in the studio so that you can wear your gear VR
[01:28:15.860 --> 01:28:20.060]   your cardboard I don't think it's how most people will watch the show but you
[01:28:20.060 --> 01:28:24.540]   might watch live that way if you can look around it'll be like being in the
[01:28:24.540 --> 01:28:31.260]   studio mm-hmm except without the great free prizes so but other well also means
[01:28:31.260 --> 01:28:37.740]   the bottle of whiskey I've got stashed down there show up I really this is
[01:28:37.740 --> 01:28:41.700]   something this is just coolness funness that I don't see any value to Google
[01:28:41.700 --> 01:28:44.860]   for but maybe this is the next big thing on YouTube I think there is a
[01:28:44.860 --> 01:28:49.300]   considerable amount of value for it because they can use the stuff like this for
[01:28:49.300 --> 01:28:52.740]   both virtual reality and augmented reality systems they don't have to tie
[01:28:52.740 --> 01:28:57.340]   themselves to one platform and with the VR goggles coming out so it's not it
[01:28:57.340 --> 01:29:02.140]   Oculus Rift doesn't own the market anymore no I mean well that Google don't
[01:29:02.140 --> 01:29:05.020]   think is interested isn't interested in owning the market what it is in
[01:29:05.020 --> 01:29:08.380]   streets providing the content for it making it other people and hosting that
[01:29:08.380 --> 01:29:12.380]   content so that YouTube gets even more hits and more and more more business I've
[01:29:12.380 --> 01:29:17.260]   been skeptical on VR but I have to say the idea that you could especially for
[01:29:17.260 --> 01:29:21.180]   live that you could be say at the Academy Awards and look around and see
[01:29:21.180 --> 01:29:24.700]   all that's George I'm sitting next to George Clooney and that would be pretty cool
[01:29:24.700 --> 01:29:31.220]   when I was in Italy there was a guy who had an Oculus and they had a film of
[01:29:31.220 --> 01:29:35.420]   Cirque du Soleil and it was as though you were sitting on the front of the
[01:29:35.420 --> 01:29:39.980]   stage so you could turn to your left and right and see other performers in the
[01:29:39.980 --> 01:29:43.780]   show who were talking to you were gesturing towards you while you were
[01:29:43.780 --> 01:29:47.580]   watching the show it was incredibly powerful I mean people would pay
[01:29:47.580 --> 01:29:50.380]   money to do that we were talking I think we'll probably partner with our
[01:29:50.380 --> 01:29:53.620]   neighbors here Pixelcore Alex Lindsey's group but that was talking with the
[01:29:53.620 --> 01:29:57.900]   Pixelcore folks and they said here's what we want to do plays fictional plays
[01:29:57.900 --> 01:30:02.100]   so this is actually a big trend in Broadway like Tina and Tony's wedding
[01:30:02.100 --> 01:30:04.300]   where you're in an immersive environment or you're going through a
[01:30:04.300 --> 01:30:07.780]   house and it's all around you it's all around you you could do this with VR and
[01:30:07.780 --> 01:30:11.340]   they have to fix it so that you can tune the audio so that when you look at
[01:30:11.340 --> 01:30:15.580]   somebody you hear them louder than those people but you could be immersed
[01:30:15.580 --> 01:30:20.020]   surrounded by actors improv or doing a script and you could look around it be
[01:30:20.020 --> 01:30:23.740]   like being the middle of mass or something it'd be so cool yeah
[01:30:23.740 --> 01:30:28.980]   again yeah it's it's a Neil Stevens since the diamond age where yeah where
[01:30:28.980 --> 01:30:33.660]   there's with our actors yes yeah yeah yeah that's that's my first thought
[01:30:33.660 --> 01:30:38.220]   yeah anything that moves us closer to Neil Stevens Stevenson's universe I'm
[01:30:38.220 --> 01:30:41.940]   all in it's always good honestly yeah I'm a cynic about this I think the first
[01:30:41.940 --> 01:30:46.460]   two applications to drive it can be computer gaming and porn gaming for
[01:30:46.460 --> 01:30:49.140]   people will spend that amount and then when the cost level comes down the
[01:30:49.140 --> 01:30:52.140]   poor industry's gonna get into it Oculus said it's gonna be fifteen hundred
[01:30:52.140 --> 01:30:56.380]   dollars it for the gaming computer you'd have to have plus the rift it's
[01:30:56.380 --> 01:31:00.420]   available next to you that's not bad if you're a serious gamer nothing I'm
[01:31:00.420 --> 01:31:04.140]   nobody spends money like the gamers yeah I mean they are hardcore about having
[01:31:04.140 --> 01:31:07.700]   the best rigs the best yeah yeah I see there's a gamer in the back
[01:31:07.700 --> 01:31:10.980]   Chris would you you would want yeah Chris has my Oculus Rift I gave him the
[01:31:10.980 --> 01:31:16.980]   developer edition so that's I think that I'm a believer now in that I was big I
[01:31:16.980 --> 01:31:20.580]   was saying no augmented realities where it's at but I think there are immersive
[01:31:20.580 --> 01:31:23.660]   there are places where an immersive experience would be great I do think I
[01:31:23.660 --> 01:31:29.380]   think augmented is going to be much more broadly yeah sort of desirable I
[01:31:29.380 --> 01:31:34.380]   think there's going to be there may be people like me who I can't watch a lot
[01:31:34.380 --> 01:31:39.300]   of Oculus Rift without feeling me too ocean sickness particularly things that
[01:31:39.300 --> 01:31:45.260]   are moving quickly so that's gonna reduce probably my desire to do it but but
[01:31:45.260 --> 01:31:49.660]   AR for sure like I would buy Google glasses that would tell me all the
[01:31:49.660 --> 01:31:53.020]   things that are around me as I'm wandering around a new city for sure
[01:31:53.020 --> 01:31:56.700]   that's the thing you can wonder around and I pay an AR rake where she can't
[01:31:56.700 --> 01:32:00.540]   wander around in a virgin no it's very it's stationary and I think they're
[01:32:00.540 --> 01:32:04.780]   and let's face it you have a ski mask on your face yes right yeah you're not
[01:32:04.780 --> 01:32:08.660]   gonna get anywhere with a ski mask absolutely ridiculous but again there
[01:32:08.660 --> 01:32:12.540]   are there are useful applications I think actually a big sort of unexplored
[01:32:12.540 --> 01:32:17.220]   one is teaching and remote remote learning where I mean you look at how
[01:32:17.220 --> 01:32:20.220]   colleges are getting more and more expensive and you look at the sheer
[01:32:20.220 --> 01:32:24.020]   number of online classes being available just imagine someone putting
[01:32:24.020 --> 01:32:27.020]   one of these rings you know in the back of their classroom or even in the
[01:32:27.020 --> 01:32:32.220]   middle of their classroom and all the sudden yeah you know training oh yeah
[01:32:32.220 --> 01:32:37.500]   I just read an article it was on softwares one of the security blogs about
[01:32:37.500 --> 01:32:41.340]   apparently the telesurgery software they're using is hackable a man in the
[01:32:41.340 --> 01:32:44.420]   middle you could do a man in middle attack and take over a surgery this
[01:32:44.420 --> 01:32:50.680]   scares me a little bit should scare you a lot I don't know how widespread teles
[01:32:50.680 --> 01:32:54.180]   surgery is but let's lock that down shall we yeah particular of the site to
[01:32:54.180 --> 01:33:03.060]   me is involved yeah Google also has projects solely this was interesting this
[01:33:03.060 --> 01:33:08.040]   was the it was this was like radar or something that sees so they showed as an
[01:33:08.040 --> 01:33:11.920]   example you can turn your fingers like you're twisting a knob and an actual
[01:33:11.920 --> 01:33:16.400]   virtual novel twist did you play with this Jason how was it Google I owe yeah
[01:33:16.400 --> 01:33:21.180]   I did it do what did it kind of live up to the promise I would say I mean you
[01:33:21.180 --> 01:33:26.480]   know based on the examples that they had set up on the floor it was interesting
[01:33:26.480 --> 01:33:30.980]   it was hard to get a sense of how accurate things were when I was playing
[01:33:30.980 --> 01:33:34.500]   around with it but I mean it was obvious that it was doing what it what it
[01:33:34.500 --> 01:33:37.740]   promised in the sense that you know things moved when you were moving your
[01:33:37.740 --> 01:33:41.180]   fingers and well this would go and this would go different motions and create a
[01:33:41.180 --> 01:33:45.580]   wave and stuff so yeah even for scrolling yeah you know even for swiping like
[01:33:45.580 --> 01:33:49.200]   minority report style right you know swiping through pages that you're
[01:33:49.200 --> 01:33:52.480]   looking at on a heads-up display yeah I mean the demo was really really
[01:33:52.480 --> 01:33:56.880]   impressive but Google's really good at doing impressive demos particularly the
[01:33:56.880 --> 01:34:01.640]   atom area yeah no question yeah I mean it's just they were talking about the you
[01:34:01.640 --> 01:34:05.720]   know their their tango mobile phone and it's been going they've been talking
[01:34:05.720 --> 01:34:09.120]   about it for two years now and show me the damn hardware I mean this looked
[01:34:09.120 --> 01:34:13.560]   great but show me working in a consumer sense that my mother can use I'm sold
[01:34:13.560 --> 01:34:18.280]   until then we'll see yeah actually saw an article about the guy who developed
[01:34:18.280 --> 01:34:26.340]   the the UI in minority report and also did some consulting for Ironman the sort
[01:34:26.340 --> 01:34:31.420]   of you know heads-up display screens moving around he's designing real
[01:34:31.420 --> 01:34:36.220]   interfaces now so presumably we'll see lots of that type of stuff come to
[01:34:36.220 --> 01:34:42.620]   mark exhausting I like this all day I want to do that yeah that's why I like
[01:34:42.620 --> 01:34:47.900]   that I like Google because you just do a little thing like well I wonder I I wonder
[01:34:47.900 --> 01:34:52.860]   how much or how little gestures will especially frontwards gestures I have to
[01:34:52.860 --> 01:34:56.540]   wonder if it's not necessarily maybe something that like a glove or something
[01:34:56.540 --> 01:35:00.460]   where you put on with your hands where you can have them in your lap and you
[01:35:00.460 --> 01:35:05.660]   just you know it's almost like you're typing I know you're a bit wished I want
[01:35:05.660 --> 01:35:11.580]   to go just with your nose I could use that for the Apple Watch honestly
[01:35:11.580 --> 01:35:16.580]   yeah there have been times where I'm like I want to scroll this but I do not want
[01:35:16.580 --> 01:35:22.500]   I don't want to use a secondary hand yeah and I can't just say Siri scroll
[01:35:22.500 --> 01:35:26.500]   well Google showed off the with the Android Wear thing they showed a wrist
[01:35:26.500 --> 01:35:31.460]   movement where you go where you feel like that yeah one of our readers
[01:35:31.460 --> 01:35:34.700]   came in said I've been trying this and my wrist has hasn't been the source since
[01:35:34.700 --> 01:35:39.460]   I was a teen well go do it a lot boy it's not that hard you can just go like
[01:35:39.460 --> 01:35:44.100]   that it's not you don't have to like make a big jerk out of yourself this is
[01:35:44.100 --> 01:35:49.020]   the urbane all the LG watches got 5.1 even the original LG watch which they
[01:35:49.020 --> 01:35:55.540]   shipped at Google I owe and last year and you know I have to say I have an Apple
[01:35:55.540 --> 01:36:00.940]   watch but I partly because I don't want to have to use an iPhone I really I think
[01:36:00.940 --> 01:36:05.220]   Android Wear is pretty nice everybody mocks me for this watch saying it's
[01:36:05.220 --> 01:36:11.100]   I really like Android where I look there's yeah that unfortunately I have a I
[01:36:11.100 --> 01:36:14.820]   tried on the urbane a couple weeks back when I was hanging out with the the my
[01:36:14.820 --> 01:36:20.060]   Android you probably couldn't wear it no yeah every single Android Wear watch
[01:36:20.060 --> 01:36:23.420]   is like this around me I wrote an article about this where I'm like
[01:36:23.420 --> 01:36:27.100]   unfortunately I actually like a lot of things about Android where I think it's
[01:36:27.100 --> 01:36:31.780]   actually doing some really smart things and wearables and I'm I'm excited for
[01:36:31.780 --> 01:36:35.060]   the way that the Apple Watch and Android Wear are gonna kind of push each other
[01:36:35.060 --> 01:36:40.020]   up forward but I'm waiting for Android Wear to actually have sizes that are
[01:36:40.020 --> 01:36:44.220]   wearable by women or by men with small wrists because right now even the
[01:36:44.220 --> 01:36:49.340]   smallest available does not even remotely fit my wrist like this is the
[01:36:49.340 --> 01:36:54.300]   38 millimeter Apple watch and even that one is just you know just barely the
[01:36:54.300 --> 01:36:59.380]   right size like there's there's my wrist and it's like that's 3842 the
[01:36:59.380 --> 01:37:02.980]   Moto 360 and the urbane and that's going up my wrist but if you scroll down
[01:37:02.980 --> 01:37:07.300]   there's actually there's the urbane on my wrist did you say my wrist hasn't been
[01:37:07.300 --> 01:37:12.460]   that's a teenager one of our readers it just suck in okay
[01:37:12.460 --> 01:37:17.340]   I'm very slow too much of a clean mind like eight minutes to get that joke but I
[01:37:17.340 --> 01:37:20.420]   can sympathize with you so I'm see I've got fairly slim wrists as well and I
[01:37:20.420 --> 01:37:24.220]   tried that original LG and it was just like it was like having a brick on your
[01:37:24.220 --> 01:37:28.380]   wrist I just think it's about time computer industry recognized that most of
[01:37:28.380 --> 01:37:33.660]   its users are overweight and it's you know screw you thin people I'm still not
[01:37:33.660 --> 01:37:36.780]   sold on the smartwatch is a mask consumer item but they get there they
[01:37:36.780 --> 01:37:39.500]   are no they're not a mask consumer item although Apple seems to be selling
[01:37:39.500 --> 01:37:44.420]   30,000 a week where makes it a pretty good consumer item yeah but once the
[01:37:44.420 --> 01:37:47.780]   fanboys run out of cash and the people take a people who taking a slightly
[01:37:47.780 --> 01:37:51.540]   more reasonable approach and look at it and go solve that for a game of soldiers
[01:37:51.540 --> 01:37:56.700]   then you can how many Apple watches in the audience - okay how many just other
[01:37:56.700 --> 01:38:01.260]   small watch how many other smart watches - how many people are not wearing a
[01:38:01.260 --> 01:38:07.100]   watch at all most and then one guy's got a time X or is that a fancy watch
[01:38:07.100 --> 01:38:11.340]   there it's pretty fit it's a fancy watch like so how we look that he doesn't want
[01:38:11.340 --> 01:38:19.020]   to get ripped robbed by the guy behind him I feel bad for the lady who dropped
[01:38:19.020 --> 01:38:25.940]   off and Apple wanted the recycling center a $200,000 Apple won at the
[01:38:25.940 --> 01:38:30.300]   recycling center they're looking for you
[01:38:30.300 --> 01:38:37.380]   that article makes me really sad though because it's from the from the sounds of
[01:38:37.380 --> 01:38:41.740]   it she dropped it off because her husband died prematurely and she just
[01:38:41.740 --> 01:38:46.820]   wanted to she wanted to ex-evane I mean I can totally see you know I could
[01:38:46.820 --> 01:38:49.900]   see like if God forbid my father died I could see my mother just being like
[01:38:49.900 --> 01:38:52.500]   nope it's all going away because I don't want to look at it because it's
[01:38:52.500 --> 01:38:55.780]   gonna hurt too much and I don't think she actually you know she probably didn't
[01:38:55.780 --> 01:38:59.020]   even think about it it was just like oh these were his things these are the
[01:38:59.020 --> 01:39:03.740]   things that you know he loved and I just can't look at it right now well a
[01:39:03.740 --> 01:39:09.660]   serious kudos to clean Bay Area because they found it and sold it for $200,000
[01:39:09.660 --> 01:39:13.660]   it's not some made-up number they actually sold it to a collector for $200,000
[01:39:13.660 --> 01:39:18.060]   they want to give her a check for half which is actually I think very generous
[01:39:18.060 --> 01:39:21.060]   I think it's incredibly just very much yeah I mean you think all that's it
[01:39:21.060 --> 01:39:25.500]   one input one rogue employee to say right okay no that's obviously useless I'll
[01:39:25.500 --> 01:39:28.580]   take that away and oh I'm off to the Bahamas for the next two weeks yeah
[01:39:28.580 --> 01:39:33.700]   exactly very honest of them more power to pat on the back to clean Bay Area and
[01:39:33.700 --> 01:39:37.220]   if you ever want to give away you know something that's really really valuable
[01:39:37.220 --> 01:39:42.420]   just give it to them and they'll sell it and send you half I'm sure we have an
[01:39:42.420 --> 01:39:48.140]   Apple one in the basement somewhere we have everything else doing John come on
[01:39:48.140 --> 01:39:55.100]   don't give up we can make it already we got some we have so many old junkie
[01:39:55.100 --> 01:39:59.460]   things there I still can't better throw a male ZX81 even though it's that's a
[01:39:59.460 --> 01:40:04.420]   great computer did you build it yourself no no I'm ashamed to say that we
[01:40:04.420 --> 01:40:08.820]   bought the we bought the pre-made version of the ZX81 was so many people's
[01:40:08.820 --> 01:40:12.460]   introduction it was a kit right originally I could buy it over here as a
[01:40:12.460 --> 01:40:16.100]   kid in the UK they sold it as a base unit because time makes sold it over
[01:40:16.100 --> 01:40:20.180]   time it's your soul here was a corn was it or I know that was the BBC model
[01:40:20.180 --> 01:40:23.680]   my yeah now this was Sinclair Sinclair that's right and it happens
[01:40:23.680 --> 01:40:27.940]   clear yeah he had this idea that oh you don't need anything like an instruction
[01:40:27.940 --> 01:40:30.900]   manual just give him a book of basic people want to learn how to program in
[01:40:30.900 --> 01:40:36.500]   basic you were forced to so you know so that was that was a great computer I'm
[01:40:36.500 --> 01:40:40.160]   jealous that you had one and I'd love to have one if you ever it's in the
[01:40:40.160 --> 01:40:44.180]   storage unit back at home if you die tell your wife bring it here and I'll
[01:40:44.180 --> 01:40:49.340]   take it okay yeah we have some boxes yeah look at that
[01:40:49.340 --> 01:40:53.940]   manufactured in Scotland yeah works it really was keyboard I've ever used
[01:40:53.940 --> 01:40:57.780]   well it was just a membrane it was yeah you know and without the 16k battery
[01:40:57.780 --> 01:41:03.900]   pack a ramp pack a very limited use but it was yeah 50 sold a million and a half
[01:41:03.900 --> 01:41:08.060]   it was 50 quid you know that's how many I watches are sold every month yeah I
[01:41:08.060 --> 01:41:12.220]   mean it was 50 50 pounds you couldn't buy a Chromebook for that these days but
[01:41:12.300 --> 01:41:16.740]   as I say you do have to learn basic to use it so mixed blessing you stored
[01:41:16.740 --> 01:41:20.860]   stuff by an external tape cassette tape ah yeah you try and tell young people
[01:41:20.860 --> 01:41:24.460]   today that used to do it all around with its own control but try to get the
[01:41:24.460 --> 01:41:30.500]   program on board that's what it sounded like made piracy really easy
[01:41:30.500 --> 01:41:37.980]   apparently I'm supposed to never do it herself yes would know 1981 so yeah
[01:41:37.980 --> 01:41:41.980]   that's I got my first computer a couple years before that but I remember when
[01:41:41.980 --> 01:41:48.260]   these came out these were really us and an Atari 1040 oh good bit yeah I had a
[01:41:48.260 --> 01:41:52.660]   Atari 400 is my first computer membrane keyboard also I got tired of banging on
[01:41:52.660 --> 01:41:57.220]   that so I got the 800 which had a real keyboard you know what kids to so okay
[01:41:57.220 --> 01:42:04.620]   that's what kids today have little bits I am so jealous kids today you got it
[01:42:04.620 --> 01:42:10.180]   made little bits makes these incredibly easy to use electronics kits with
[01:42:10.180 --> 01:42:16.340]   modular building blocks you can get your dog to text or make a robotic snack
[01:42:16.340 --> 01:42:21.700]   server modules range from very simple like power sensors and LEDs and what's
[01:42:21.700 --> 01:42:26.100]   great about this the very complex programmable units and so far there are
[01:42:26.100 --> 01:42:30.940]   now already over 16 modules and what's great there's no soldering they just
[01:42:30.940 --> 01:42:36.260]   snap together like magnets and they do all sorts of cool stuff this is the
[01:42:36.260 --> 01:42:41.540]   kid I have here is the deluxe kit that's 18 modules 5 million circuit
[01:42:41.540 --> 01:42:46.180]   combinations there's 15 projects in the box if really good documentation help
[01:42:46.180 --> 01:42:51.060]   you put that all together this is the new one though I really oh man if you're
[01:42:51.060 --> 01:42:55.180]   you know a parent you're trying to get your kid interested in a technology but
[01:42:55.180 --> 01:42:58.620]   you want them to understand how it works this is so great the base and the
[01:42:58.620 --> 01:43:03.100]   deluxe kits great way for getting kids started this is the space kit developed
[01:43:03.100 --> 01:43:10.620]   yes developed in partnership with NASA you can do earth and space science I you
[01:43:10.620 --> 01:43:15.460]   know this I mean those kids today are so like well for a while I felt back because
[01:43:15.460 --> 01:43:19.420]   we did they we had chemistry sets yeah so we could blow stuff up but this is the
[01:43:19.420 --> 01:43:24.020]   new chemistry set they have an Arduino coding kit introduces kids to programming
[01:43:24.020 --> 01:43:29.740]   there's a synth kit for musicians which includes a modular analog synthesizer
[01:43:29.740 --> 01:43:34.660]   every little bits kit comes with these this great manual which really makes it
[01:43:34.660 --> 01:43:38.380]   fun and easy for kids to do it you know because the truth is you know if you get
[01:43:38.380 --> 01:43:42.980]   this as a parent you're gonna have to figure it out yeah so believe me you
[01:43:42.980 --> 01:43:46.820]   will not be frustrated I actually built what I built a little tickler a little
[01:43:46.820 --> 01:43:51.260]   kitty a little kitty tickler oh okay kitty for my kitty cat a little bit had a
[01:43:51.260 --> 01:43:55.260]   feather that whenever what were you thinking nothing nothing nothing at all
[01:43:55.260 --> 01:44:00.020]   something different in Great Britain I'm sure it does so you get a buzzer a
[01:44:00.020 --> 01:44:04.980]   light wire RGB servo motors that's what I had the kitty tickler on a buzzer
[01:44:04.980 --> 01:44:10.380]   inverter oh you learn logic to write logic gates here's a latch will inverter
[01:44:10.380 --> 01:44:16.860]   a fork oh man this is so cool there's a tickle machine see you didn't believe
[01:44:16.860 --> 01:44:20.860]   me oh look remember when you went in the back of the magazine you get a buzzer
[01:44:20.860 --> 01:44:24.260]   for a handshake well now you can make an actual buzzer with real electricity
[01:44:24.260 --> 01:44:28.740]   oh great because that's something we really want to make here's an auto
[01:44:28.740 --> 01:44:35.180]   greeter it waves why why spend energy waving your hand as you pass through
[01:44:35.180 --> 01:44:40.140]   the rabble you could use the auto trunk crane oh I love little bits they just
[01:44:40.140 --> 01:44:43.060]   getting better and better and better we want you to give it a try they're
[01:44:43.060 --> 01:44:46.620]   offering new customers twenty dollars off their first kit when you go to little
[01:44:46.620 --> 01:44:52.540]   bits ally not little bits I know I say it that way with little bits L I T T L E
[01:44:52.540 --> 01:44:56.820]   bits dot com slash twit and free shipping in the United States little bits I love
[01:44:56.820 --> 01:45:04.900]   these guys little bits dot com slash twit making kids lives a little bit better
[01:45:04.900 --> 01:45:12.900]   with little so delightful bits I was able to go ahead I was gonna say I don't
[01:45:12.900 --> 01:45:16.860]   know if anyone else saw that Marvel's doing a contest for for girls I think
[01:45:16.860 --> 01:45:23.220]   14 to 17 who are making micro like make a micro architecture project and then you
[01:45:23.220 --> 01:45:27.980]   submit it in for Ant-Man and like they you have the potential to go meet
[01:45:27.980 --> 01:45:32.580]   Imagineers and then you also get to teach people your age how to build that
[01:45:32.580 --> 01:45:36.420]   project in a local area school and I was just thinking about that I'm like that
[01:45:36.420 --> 01:45:39.500]   would actually be a really good pairing it's like get get your kid a little bit
[01:45:39.500 --> 01:45:43.700]   set and then you know see if they want to do this contest it's just it's I don't
[01:45:43.700 --> 01:45:47.660]   know it I think that is really really cool that we're encouraging kids and
[01:45:47.660 --> 01:45:52.380]   young girls to do I grow a little micro building I'm starting to get bullish on
[01:45:52.380 --> 01:45:55.740]   this idea that maybe they will actually be women involved in technology in the
[01:45:55.740 --> 01:46:00.980]   in some day in some distant future they'll actually be women involved in this and
[01:46:00.980 --> 01:46:04.140]   you know we're making such an effort at this point they might even be in the
[01:46:04.140 --> 01:46:09.380]   majority and is is there any person at all who thinks that's a bad idea that's
[01:46:09.380 --> 01:46:13.940]   a great idea sold on not the first off that program was a woman Grace Hopper
[01:46:13.940 --> 01:46:19.100]   yeah that the history was littered with great Susan care we love Susan she's
[01:46:19.100 --> 01:46:22.820]   great to the icons for the original Mac I know Matthew Ingram you got to get out
[01:46:22.820 --> 01:46:27.220]   of here it is so great to have you always on the show any thought I'd
[01:46:27.220 --> 01:46:32.140]   mentioned that the Rossville brick life without parole for creating the silk
[01:46:32.140 --> 01:46:38.860]   road that seems like it just seems outrageous to me I mean I know he did
[01:46:38.860 --> 01:46:44.260]   lots of bad things I read a horrifying discussion he had with someone about
[01:46:44.260 --> 01:46:48.660]   you know putting a hit on a couple of people that's he wanted to pay in
[01:46:48.660 --> 01:46:52.660]   Bitcoin so how serious could he have been so obviously that's not the kind of
[01:46:52.660 --> 01:46:57.460]   thing we want to encourage but it feels to me as though he's being asked to pay
[01:46:57.460 --> 01:47:01.340]   the price for a whole bunch of other people who did things he just did the
[01:47:01.340 --> 01:47:06.220]   back end right right he basically built a platform so it's a little like trying
[01:47:06.220 --> 01:47:09.620]   to go after Craig Newmark because somebody hired a hit man on Craigslist
[01:47:09.620 --> 01:47:13.900]   that's what it feels like to me an example is being made of him certainly I
[01:47:13.900 --> 01:47:17.940]   mean life without parole is just it's ridiculous I mean if you look at the
[01:47:17.940 --> 01:47:22.220]   same citizenship carried out handed out for fraud or a thing which hurt many
[01:47:22.220 --> 01:47:26.380]   more people but of course so many of the people who run our banks Goldman Sachs
[01:47:26.380 --> 01:47:29.500]   Lehman Brothers those guys who have been in jail and will be in jail forever
[01:47:29.500 --> 01:47:35.300]   right oh no no they're still running those banks oh yeah never mind slap on
[01:47:35.300 --> 01:47:39.860]   the wrist time never mind if you read the judges summing up it was utterly
[01:47:39.860 --> 01:47:44.500]   partisan the minimum he could have given him was 20 years there were a hundred
[01:47:44.500 --> 01:47:49.260]   and ninety seven letters in his behalf from friends and family saying give him
[01:47:49.260 --> 01:47:52.740]   we understand he's gonna serve time he's gonna do 20 years give him the minimum
[01:47:52.740 --> 01:47:58.900]   they gave him instead life without parole and a hundred eighty four million
[01:47:58.900 --> 01:48:03.060]   dollar fine I think you know working for cigarettes it's gonna be a little hard
[01:48:03.060 --> 01:48:07.220]   for him to pay that off it's gonna be very hard and he's gonna be a very very
[01:48:07.220 --> 01:48:11.820]   old man when he gets out I think for the actual crime that he committed I said
[01:48:11.820 --> 01:48:15.420]   yes a crime but way out proportion of proportion
[01:48:15.420 --> 01:48:19.300]   Matthew thank you so much for being here great thanks for having you on
[01:48:19.300 --> 01:48:25.660]   fortunes.com he's Matthew I on the Twitter and he is he is always just a
[01:48:25.660 --> 01:48:30.580]   beacon of sensible common sense of intelligence it's always nice to have
[01:48:30.580 --> 01:48:34.820]   you and we'll see you soon. Great thanks to Ian Thompson who is actually quite
[01:48:34.820 --> 01:48:42.020]   the opposite he's acidic acid-tongued no I love Ian too you fund a read always
[01:48:42.020 --> 01:48:47.020]   smart like you the find them at register dot co dot uk and I'm glad you've been
[01:48:47.020 --> 01:48:50.380]   coming up here a lot I appreciate it. No no it's always good fun. Love having you
[01:48:50.380 --> 01:48:54.580]   heard so far enough. Love having you here and when you that when that when you
[01:48:54.580 --> 01:48:57.700]   pass away in that Sinclair you could always you know we'll give you a yeah
[01:48:57.700 --> 01:49:00.500]   there's a couple of people you might have to fight for it but hey we could put it on
[01:49:00.500 --> 01:49:08.100]   you know live fight is the prize. Do I have to say ZX though do I have to I'm
[01:49:08.100 --> 01:49:11.620]   sorry if you can win a British computer you got so British it's the way it is
[01:49:11.620 --> 01:49:16.100]   it's the rules I can talk British governor thank you also too sorry about
[01:49:16.100 --> 01:49:22.860]   that thank you also to the great serenity Caldwell yes yes I know she does do
[01:49:22.860 --> 01:49:27.540]   roller derby but she's also a brilliant writer and you can catch her work at imore.com
[01:49:27.540 --> 01:49:32.340]   she's the one who discovered the Apple Watch doesn't work if you have a sleeve
[01:49:32.340 --> 01:49:37.260]   with a tattoo so it works it just doesn't work for very specific tattoos you can't
[01:49:37.260 --> 01:49:41.540]   do the heart rate. Yeah well I my boyfriend has a has a sleeve and it works
[01:49:41.540 --> 01:49:46.300]   fine it's just it's it's dark solid colors so if you have if you have black
[01:49:46.300 --> 01:49:50.780]   right here it's not gonna work when you're next when you when you next Derby
[01:49:50.780 --> 01:49:56.540]   match. Next one is in July I just got back from a practice what's the name of
[01:49:56.540 --> 01:50:01.500]   the what's the name of the of the team. I skate for the cosmonauties as part of
[01:50:01.500 --> 01:50:07.340]   the Boston Derby names and also the the Boston massacre and the Boston B party
[01:50:07.340 --> 01:50:11.980]   are our travel teams and they have a game next weekend. Nice thank you serenity
[01:50:11.980 --> 01:50:16.020]   great to have you here thank you to to everyone joining us as you probably
[01:50:16.020 --> 01:50:19.980]   noticed we're still doing live behind the scenes broadcasts and I think we
[01:50:19.980 --> 01:50:22.860]   probably will continue that we what we've decided to do is give every every
[01:50:22.860 --> 01:50:27.980]   host the choice of whether they want to record their shows live on the stream
[01:50:27.980 --> 01:50:31.500]   or record them off the stream and have the produced versions on the stream most
[01:50:31.500 --> 01:50:34.980]   of the hosts want to do that I'm gonna continue to do that we are working on
[01:50:34.980 --> 01:50:39.180]   solutions to our chat room many of the mods want to move on I don't blame
[01:50:39.180 --> 01:50:42.660]   them it's really been kind of a battle zone but we are not planning on killing
[01:50:42.660 --> 01:50:47.140]   chat we're just gonna find a way to make it a little bit easier safer and a
[01:50:47.140 --> 01:50:51.420]   better experience for those people who have to keep an eye on things we know
[01:50:51.420 --> 01:50:55.900]   how important our community is and I I I'm not willing to give it up not for
[01:50:55.900 --> 01:50:59.700]   nothing if you want to be so we will be live again next week if you want to
[01:50:59.700 --> 01:51:05.180]   watch you can 3 p.m. Pacific 6 p.m. Eastern time that's 2200 UTC live dot
[01:51:05.180 --> 01:51:08.700]   twit dot TV if you'd like to be in studio we love having you at a great
[01:51:08.700 --> 01:51:12.580]   audience of people here this week we really appreciate your being here just
[01:51:12.580 --> 01:51:16.820]   email us if you can tickets at twit dot TV we'll put you on the list we'll
[01:51:16.820 --> 01:51:19.980]   make sure we have a chair for you and of course lovely parting gifts for each
[01:51:19.980 --> 01:51:24.620]   and every one of you just for coming to the show tickets at twit well it's it's
[01:51:24.620 --> 01:51:29.540]   I know it's a rubber band but it's it's imprinted right it says to it on it you
[01:51:29.540 --> 01:51:34.980]   know tickets at twit dot TV if you want to watch our on-demand video or audio
[01:51:34.980 --> 01:51:39.060]   always available wherever you get your podcast on our website twit dot TV the
[01:51:39.060 --> 01:51:43.140]   new website will launch between I hope between now and next show sometime this
[01:51:43.140 --> 01:51:49.220]   week so if you see a sudden change in how the website looks thanks to four
[01:51:49.220 --> 01:51:52.460]   kitchens a great designers and programmers there and we'll kind of give
[01:51:52.460 --> 01:51:57.460]   you a tour at some point how it works and all the new features there what we're
[01:51:57.460 --> 01:52:01.780]   really excited about sharing with you is the new website comes with a public
[01:52:01.780 --> 01:52:08.020]   API which we will publish and let you write your own stuff if you wish so that
[01:52:08.020 --> 01:52:13.540]   you can scrape our content the API is very rich lots of detail it's what the
[01:52:13.540 --> 01:52:19.100]   website uses the website is a consumer of the API but it's also what
[01:52:19.100 --> 01:52:23.140]   our new apps will use and we're talking to app developers about creating apps
[01:52:23.140 --> 01:52:29.420]   for iOS Android Windows that means Windows 10 Xbox One and Windows Phone as
[01:52:29.420 --> 01:52:33.100]   well as all the other platforms and that API is gonna make that I think a lot
[01:52:33.100 --> 01:52:37.220]   not only a lot easier but a lot better the apps should be really really sing so
[01:52:37.220 --> 01:52:40.700]   we're excited about that thanks for being here we'll see you next time another
[01:52:40.700 --> 01:52:54.940]   Twitch is in the game

