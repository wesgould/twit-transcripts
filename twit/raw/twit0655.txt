;FFMETADATA1
title=Banana Is Phone
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=655
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:06.020]   It's time for Twit this week in tech. Great panel for you. Amy Webb, Brianna Wu, Michael
[00:00:06.020 --> 00:00:11.920]   Nunez. We're going to talk about the Galaxy S9 just announced from Samsung. Privacy regulations
[00:00:11.920 --> 00:00:20.560]   in the US and China and their differences. An Apple repair services calling 911. Thousands
[00:00:20.560 --> 00:00:36.120]   of times it's all coming up next on Twit.
[00:00:36.120 --> 00:00:48.040]   Bandwidth for this week in tech is provided by cash fly at C A C H E F L Y dot com.
[00:00:48.040 --> 00:00:56.400]   This is Twit this week in tech. Episode 655 recorded Sunday, February 25th, 2018. Banana
[00:00:56.400 --> 00:00:59.160]   is phoned.
[00:00:59.160 --> 00:01:03.880]   This week in tech is brought to you by Audible. Audible makes getting more books in your life
[00:01:03.880 --> 00:01:08.880]   easy. Sign up for the Gold Plus One plan to get two free books and a 30 day free trial
[00:01:08.880 --> 00:01:16.160]   at audible.com/twit2. And by legal zoom get your dream business up and running or take
[00:01:16.160 --> 00:01:21.600]   control of your family's future with legal zoom. For special savings visit legal zoom.com
[00:01:21.600 --> 00:01:28.600]   and enter Twit at checkout. And by zip recruiter hiring zip recruiter has revolutionized how
[00:01:28.600 --> 00:01:32.840]   you'll do it. Their technology identifies people with the right experience and invites
[00:01:32.840 --> 00:01:38.960]   them to apply to your job. Try it free today at zip recruiter.com/twit.
[00:01:38.960 --> 00:01:44.160]   And by cloud Spanner from Google cloud platform. Cloud Spanner is the only horizontally scalable
[00:01:44.160 --> 00:01:50.960]   and strongly consistent relational database service. To learn more visit g.co/getspanner
[00:01:50.960 --> 00:01:54.880]   today.
[00:01:54.880 --> 00:01:59.840]   It's time for Twit this week in tech to show we cover the week's tech news. Very good
[00:01:59.840 --> 00:02:03.720]   panel this week. I say that every week but I meet it every week. Let's start with Amy
[00:02:03.720 --> 00:02:09.160]   Webb. She's a futurist. And the author of a book this is actually how I met her called
[00:02:09.160 --> 00:02:16.840]   The Signals are Talking. Why today's fringe is tomorrow's mainstream. She is joining us
[00:02:16.840 --> 00:02:20.800]   from where are you somewhere different? You're a Nashville.
[00:02:20.800 --> 00:02:25.680]   I know this week I'm in Nashville. I'm speaking at a big conference tomorrow on the future
[00:02:25.680 --> 00:02:31.600]   of energy. Nice. Founder of the future today Institute your new FTI trend report. You just
[00:02:31.600 --> 00:02:33.600]   held it up. It's going to be out soon, right?
[00:02:33.600 --> 00:02:41.200]   So exciting. It launches at South by Southwest on March 11th. We have 225 emerging tech trends
[00:02:41.200 --> 00:02:46.480]   this year. This is the 11th annual edition of the report. It's our largest yet. And it
[00:02:46.480 --> 00:02:51.960]   has had 6 million cumulative views. So this thing is I can't wait. That's fantastic.
[00:02:51.960 --> 00:02:57.680]   It's digital also but we've made some printed versions. So it goes online on March 10th.
[00:02:57.680 --> 00:03:01.560]   It's actually you can sign up to receive an advance copy now on our website. But on March
[00:03:01.560 --> 00:03:03.880]   11th it'll be everywhere so you can download it.
[00:03:03.880 --> 00:03:07.400]   We'll be in Austin on March 9th doing a panel down there. If you're in the area come by.
[00:03:07.400 --> 00:03:13.640]   All right. We're at the Capital One House in Anton's but unfortunately our panel's at
[00:03:13.640 --> 00:03:20.840]   8.30 in the morning. We're going to do a panel on actually I'm excited about it on consumer
[00:03:20.840 --> 00:03:27.640]   security. You know consumers hear all the you know meltdown, specter, Equifax, all the
[00:03:27.640 --> 00:03:33.640]   issues. But what is the what is really at stake for them and what should what are the
[00:03:33.640 --> 00:03:36.880]   real practical things we should do? We have some really good security experts on. So that's
[00:03:36.880 --> 00:03:42.640]   a lot of fun. Yeah. Anyway, thank you Amy for being here all the way from Nashville.
[00:03:42.640 --> 00:03:49.120]   Also joining us from the Massachusetts 8th District where she is a candidate for US Congress.
[00:03:49.120 --> 00:03:54.520]   Brianna Wu, Space Cat Gal is back in the studio. Hey, how you doing Brianna?
[00:03:54.520 --> 00:03:59.720]   I'm feeling really good. I have to tell you I've had so many conversations with elected
[00:03:59.720 --> 00:04:05.120]   officials as I'm running for Congress here about cybersecurity. I said to press it because
[00:04:05.120 --> 00:04:09.680]   you will talk to people and they're in charge of like a massive computer system here in the
[00:04:09.680 --> 00:04:15.680]   state and you'll ask them specific technical questions and all they ever say is like we
[00:04:15.680 --> 00:04:21.560]   have an IT department like no one is thinking about it. They're not. I was trying to talk
[00:04:21.560 --> 00:04:25.880]   to someone the other day about like look if you've got these machines, these voting machines,
[00:04:25.880 --> 00:04:30.000]   you've got to check and make sure the version of software that's running on it is the same
[00:04:30.000 --> 00:04:34.920]   software you think it is. And it's just it's a level too high for them to even talk about
[00:04:34.920 --> 00:04:40.360]   it. We've got to have technologically literate people actually making decisions. Highly problematic.
[00:04:40.360 --> 00:04:44.240]   Yeah, I find it also very frustrating that as far as I can tell Equifax is gotten off
[00:04:44.240 --> 00:04:52.000]   scot-free. No punishment. None at all. No consequences whatsoever. In fact, they made
[00:04:52.000 --> 00:04:57.200]   money on the breach. What really gets me about that Leo is if you actually go out and talk
[00:04:57.200 --> 00:05:03.000]   to people there is angry about Equifax is you could possibly imagine. You talk to them.
[00:05:03.000 --> 00:05:07.040]   It doesn't matter if they're a Republican, a Democrat, not politically active. You just
[00:05:07.040 --> 00:05:12.200]   see their blood pressure rising and you would think that just out of pure self interest,
[00:05:12.200 --> 00:05:16.280]   you would have politicians talking about it. And they just I don't know if they're just
[00:05:16.280 --> 00:05:21.760]   so caught up in the system or they just don't care. But for whatever reason, it's like you
[00:05:21.760 --> 00:05:27.320]   said they got off scot-free. Yeah. Hey, Branna, are you doing anything for candidates in the
[00:05:27.320 --> 00:05:32.720]   state of Massachusetts around election fraud and is there any like what's happening? I
[00:05:32.720 --> 00:05:37.520]   can't say that I've had really big talks with the secretary of state's office about that.
[00:05:37.520 --> 00:05:44.680]   We are currently having there are concerns. But when you ask questions, I hope not burning
[00:05:44.680 --> 00:05:49.480]   any bridges when I say I haven't liked the answers I've gotten to that question. So,
[00:05:49.480 --> 00:05:53.360]   you know, we need to take it more seriously. Also in the studio, well, actually not in
[00:05:53.360 --> 00:05:57.560]   the studio in his studio. Is that Mashable Michael Nunez? You remember him from popular
[00:05:57.560 --> 00:06:04.000]   science? He's now over at the Mashable headquarters where actually he's at work on a Sunday evening.
[00:06:04.000 --> 00:06:08.480]   I'm sorry, deputy tech editor over there. Hey, Michael. Hey, how are you? Great to have
[00:06:08.480 --> 00:06:13.720]   you as always. You're not you're you're everybody at Xemty over there. That's because
[00:06:13.720 --> 00:06:18.000]   everybody's in Barcelona probably, right? That's exactly right. Yeah. So I've been here for
[00:06:18.000 --> 00:06:23.440]   a little while monitoring our mobile world Congress coverage. Obviously the Samsung Galaxy
[00:06:23.440 --> 00:06:28.720]   S9 release and a couple of other of the Android phones that came out today. So it's been a
[00:06:28.720 --> 00:06:34.040]   busy Sunday. But you know, it's it's what I love to do. So I'm happy to be here. We
[00:06:34.040 --> 00:06:40.640]   were actually streamed along with Samsung the event 9am our time, noon your time, and
[00:06:40.640 --> 00:06:45.080]   about five in the evening in Barcelona, where they announced the S9 looks like it went very
[00:06:45.080 --> 00:06:51.800]   well. Very pleased to see Samsung did not follow Apple in almost any respect. And they
[00:06:51.800 --> 00:06:57.120]   were very proud to say no notch. We still have a headphone jack. Yes, there's a fingerprint
[00:06:57.120 --> 00:07:02.320]   reader. But most importantly to me, they didn't raise the hike the price an awful lot. There's
[00:07:02.320 --> 00:07:13.280]   this the S9 is $720 the S9 plus the 6.2 inch model with dual cameras, $120 more, $840.
[00:07:13.280 --> 00:07:19.000]   That's not exactly holding the line, but it's still not a $1000 phone. Well, yeah, kind
[00:07:19.000 --> 00:07:26.600]   of is though, is it? It is Amy says it is. Well, so I have an essay and I'm a pretty careful
[00:07:26.600 --> 00:07:31.680]   person and I've now broken two screens because the thing is slippery is butter and it's delicate.
[00:07:31.680 --> 00:07:37.240]   So I've not put it in a case or that's even with a case. Oh, no, no, this is with a case.
[00:07:37.240 --> 00:07:42.680]   And yeah, so in the butter fingers, it's not just me though, these these screens are constantly
[00:07:42.680 --> 00:07:45.560]   getting broken. And if you choose to repair, it's, you know, a couple hundred bucks.
[00:07:45.560 --> 00:07:49.240]   Now, do you think you'd have the same problem than iPhone 10, which is also an edge to edge
[00:07:49.240 --> 00:07:56.200]   slippery? I would have so many more problems with an iPhone. I love it when you said earlier
[00:07:56.200 --> 00:08:03.000]   that Samsung isn't copying Apple. I mean, Apple, you know, well, okay, but there's no notch.
[00:08:03.000 --> 00:08:11.400]   They're doing dual cameras, but they're doing F1.5 and F2.4, which Apple does not do. It looks like
[00:08:11.400 --> 00:08:16.680]   they're closer to Google's Pixel because they have a dual pixel camera. I'm very excited about
[00:08:16.680 --> 00:08:22.040]   the low light and they're doing something Apple can't do on the new S9, which is 960 frames per
[00:08:22.040 --> 00:08:26.920]   second super slow mo. And that's attractive. What is the reaction you're hearing from Barcelona, Michael?
[00:08:26.920 --> 00:08:32.120]   Well, I think most people are excited about oddly enough, the Animoji rip off. So you
[00:08:32.120 --> 00:08:37.160]   have not come out. Oh, come on. But they are. That's what they're copying Apple is the
[00:08:37.160 --> 00:08:42.920]   Animoji. The one. Yeah, people love people love these things. And frankly, I like them. They're
[00:08:42.920 --> 00:08:48.280]   a lot of fun. So I'm excited to actually play with that when the phone gets back to New York.
[00:08:48.280 --> 00:08:54.200]   I think generally, you know, the consensus is that this isn't a leap ahead of the Galaxy S8. So if
[00:08:54.200 --> 00:08:58.920]   you have last year's model, there's not enough reason to go out to run out and get the S9. But
[00:08:58.920 --> 00:09:03.320]   they're still building on a very solid foundation. You know, Samsung has nearly perfected this
[00:09:03.320 --> 00:09:09.160]   Galaxy series with the edge to edge display. I think a lot of people like the performance,
[00:09:09.160 --> 00:09:14.280]   obviously on the cameras and low light settings. You know, the display itself is really nice. The
[00:09:14.280 --> 00:09:20.680]   phone looks very premium. So people still consider this one of the best Android phones available.
[00:09:20.680 --> 00:09:27.160]   It's just, you know, it's not really pushing the boundary in the same way that the iPhone 10 is
[00:09:27.160 --> 00:09:32.760]   trying to, you know, guess where where phones are headed in the future. So this doesn't quite set
[00:09:32.760 --> 00:09:38.120]   the same benchmark, but it's still a very good phone. The Animoji feature that they demonstrated
[00:09:38.120 --> 00:09:46.760]   on stage did not feature aliens, monkeys, lions. It just took your picture and turned it into
[00:09:46.760 --> 00:09:49.960]   something that vaguely looked like you. But is there more to it than that?
[00:09:49.960 --> 00:09:55.240]   No, that's it. That's it. But people love doing that. I mean, like people do that on the Nintendo
[00:09:55.240 --> 00:09:59.880]   Wii. They do that through Bitmoji. I mean, and people are obsessed with themselves.
[00:09:59.880 --> 00:10:04.760]   And I guess that's the point is I could do this with Bitmoji on a Snapchat on any phone, right?
[00:10:05.480 --> 00:10:10.840]   Well, you could, but you can't do this actually animates. So the emoji right now,
[00:10:10.840 --> 00:10:15.960]   it doesn't you can't talk and have its lips move. Yeah, exactly. And so I think, you know,
[00:10:15.960 --> 00:10:20.520]   I'm sure Snapchat has the capability to do this. Obviously they have like 3D renders of your
[00:10:20.520 --> 00:10:30.280]   Bitmoji in Snapchat. But this is one of the only apps or features that lets you control a 3D avatar
[00:10:30.280 --> 00:10:34.600]   much like you would with a puppet or something like that. So it's just it's a fun, stupid feature.
[00:10:34.600 --> 00:10:39.240]   But and so it's like not you shouldn't go buy the phone just because of this one thing.
[00:10:39.240 --> 00:10:43.320]   However, it's a nice addition. I think if you decide to buy it,
[00:10:43.320 --> 00:10:48.600]   I could tell you I have the Animoji on the iPhone 10 and I used it a few times at the beginning
[00:10:48.600 --> 00:10:52.440]   when I first got it. I've never even thought of using it again. Maybe that's just because I'm an
[00:10:52.440 --> 00:10:58.280]   old person. It's the same old. I used it once. Yeah, it's gimmicky. I don't what do I want to
[00:10:58.280 --> 00:11:04.920]   send an animated tiger for. So I want to know like explode gate like last year they kind of
[00:11:04.920 --> 00:11:10.120]   brought their the milliamps of the battery down a little bit. Did they amp it up this year?
[00:11:10.120 --> 00:11:13.960]   Did they are they betting big on the battery again or are they worried it's going to explode?
[00:11:13.960 --> 00:11:20.680]   So I can't say off the top of my head what the size of the I know I memorized all the stats
[00:11:20.680 --> 00:11:27.720]   3000 milliamp hours for the S9 and for the S9 plus 3500. That's still a lot below. I think the Note
[00:11:27.720 --> 00:11:34.520]   7 was 4000. Yeah, I was going to say I think they haven't gone quite up to the same capacity as
[00:11:34.520 --> 00:11:41.720]   the Note 7 that was explode. Do you find it amazing? I thought after the Note gate after the Note 7
[00:11:41.720 --> 00:11:50.040]   Samsung would really hurt that this would be a big deal. It did not seem to impact them at all.
[00:11:50.040 --> 00:11:55.000]   They immediately the very next quarter they had a massive quarter with the S8 and the Note 8.
[00:11:55.000 --> 00:12:02.680]   Nobody held back. I guess people believed as I think is probably the case that Samsung
[00:12:02.680 --> 00:12:05.640]   will be extra. This is the least likely phone to explode now.
[00:12:05.640 --> 00:12:09.480]   Well, there's not a lot of competition in the marketplace though, right? So you know,
[00:12:09.480 --> 00:12:13.800]   it's Samsung and Apple and that's it, right? Right. So I mean if you're not, you know,
[00:12:13.800 --> 00:12:19.720]   if you're interested in Android, there just there are other models but I prefer the Google pixels,
[00:12:19.720 --> 00:12:24.680]   but you're right. Most people are doing poorly though. Are they doing that badly?
[00:12:25.160 --> 00:12:31.000]   Oh, the Pixel 2 is, yeah. Because an abomination, I think it was only like three million units sold
[00:12:31.000 --> 00:12:35.000]   worldwide or something. It's so sad. It's easily the best. It's having a hard time with hardware.
[00:12:35.000 --> 00:12:40.520]   I just I think they just don't have it together. They've tried to be a hardware company a few times
[00:12:40.520 --> 00:12:46.920]   now and it's just not, you know. Samsung's work bans, unlike most other companies,
[00:12:46.920 --> 00:12:53.240]   its marketing dollar is tied to its revenue. So as they can, as they roll in the dough and they
[00:12:53.240 --> 00:12:57.960]   really are rolling in the dough, they buy more ads and you'll see more Samsung ads
[00:12:57.960 --> 00:13:04.040]   everywhere than anything. So I mean, even Apple, which buys a lot of ads, you'll see Samsung's
[00:13:04.040 --> 00:13:09.480]   marketing everywhere and obviously that's overcome many concerns people have. And yeah,
[00:13:09.480 --> 00:13:13.720]   maybe it's there's just no competition, although there is competition is just people aren't choosing
[00:13:13.720 --> 00:13:22.840]   it. Well, I think the creation of OLED panels and I think the production process for a long time
[00:13:22.840 --> 00:13:26.840]   has been something that Samsung has really cornered the market on. So they're one of the
[00:13:26.840 --> 00:13:31.480]   biggest phone manufacturers. And so the success of the phone. And they make the screens. They
[00:13:31.480 --> 00:13:38.360]   even make the screens for Apple. That's right. Here's. Samsung has sort of greater brand penetration
[00:13:38.360 --> 00:13:42.600]   because they're not just making phone like HTC has a problem has many problems, but HTC has a
[00:13:42.600 --> 00:13:47.800]   problem because you don't buy other HTC products. People have Samsung refrigerators and washing
[00:13:47.800 --> 00:13:54.600]   machines. So it's a brand with much higher capacity and greater penetration. So I think even if they
[00:13:54.600 --> 00:13:59.160]   falter, there's so many other products in the marketplace that people do like. And they're so
[00:13:59.160 --> 00:14:04.840]   far ahead in other areas like IoT. I just, you know, you know, Brianna, you started to say,
[00:14:04.840 --> 00:14:09.800]   show me you make some good phones, right? Yeah, I mean, I think I don't know if you guys watch the
[00:14:09.800 --> 00:14:15.480]   YouTube channel unbox therapy, but they bring out a always amazingly talented, but they bring out a
[00:14:15.480 --> 00:14:20.200]   lot of these phones that, you know, we just don't hear about if you watch, you know, this week in
[00:14:20.200 --> 00:14:26.360]   tech or the verge or other things like that every week. And you know, I've been really impressed
[00:14:26.360 --> 00:14:31.480]   with the quality of them when I've seen them in person, like some of the Chinese people I know
[00:14:31.480 --> 00:14:36.680]   hear embossed it. Like it's, I think it's a bigger brand in China than in the United States.
[00:14:36.680 --> 00:14:41.560]   Yeah. Yeah. What about the US intelligence agencies have advised against purchasing
[00:14:41.560 --> 00:14:46.440]   phones from Huawei and Huawei and they could potentially be spying on you. So
[00:14:46.440 --> 00:14:54.200]   Huawei had a deal. It's very sad. Huawei had a deal for their new phone with both AT&T and Verizon.
[00:14:54.200 --> 00:15:01.000]   And after the US government said, no, no, no, both you, ATD Verizon pulled out. And Huawei was,
[00:15:01.000 --> 00:15:05.960]   you know, devastated. They said, if you don't have a carrier deal in the US, you don't sell phones.
[00:15:08.040 --> 00:15:13.160]   Is it do you actually know this is an interesting question? All, all, I mean, all the iPhones are
[00:15:13.160 --> 00:15:21.160]   made in China. Is there something about a Huawei or a Xiaomi or that makes it more inherently
[00:15:21.160 --> 00:15:26.760]   more risky than a phone that's from an American company made in China? What do you think, Michael?
[00:15:26.760 --> 00:15:33.320]   I think it comes down to the software that's packaged with the phone. So I, it's a phenomenal
[00:15:33.320 --> 00:15:39.800]   question. I mean, you know, I think theoretically, you know, all phones that, that begin from the
[00:15:39.800 --> 00:15:47.800]   same origin would be, would have the same risk. What's the stop? People to go from going into Fox
[00:15:47.800 --> 00:15:53.080]   Con unbeknownst to Apple and saying, I mean, after all, this company is a Chinese company and
[00:15:53.080 --> 00:15:57.080]   saying to them, here's a little something extra you'd be putting in the phone. Thank you very
[00:15:57.080 --> 00:16:02.760]   much. Don't tell Apple. Yeah. I think it's, I think it's a great point. I, I, I imagine that
[00:16:02.760 --> 00:16:07.480]   they're pretty strict quality assurance. You know, Apple is probably has people they're watching
[00:16:07.480 --> 00:16:12.760]   for exactly that. Yeah. And also testing device, the devices in the US, you know, once they reach
[00:16:12.760 --> 00:16:16.920]   the US, I'm sure that they have a team of engineers that are trying these things out. So, uh,
[00:16:16.920 --> 00:16:20.280]   And we should point out that Samsung's phones are not made in China. They're made in Korea.
[00:16:20.280 --> 00:16:26.120]   I feel like I have to add to that. Oh, sorry. I was just going to say, I feel like I have to add
[00:16:26.120 --> 00:16:31.080]   to that. Yeah, I did a, um, a piece on the Fysabille, which is an absolute disaster for
[00:16:31.080 --> 00:16:36.040]   a half Poe a few weeks ago. Now, speaking with the F South about like, um, you know, does,
[00:16:36.040 --> 00:16:42.200]   is it directly possible for Congress to pass laws in that channel saying Apple has to do A, B,
[00:16:42.200 --> 00:16:47.880]   or C to their phone? Um, you know, could be having that kind of policy here? The answer is, we don't
[00:16:47.880 --> 00:16:52.120]   know. The answer is flat out. We don't know if they did have something like that. We wouldn't be
[00:16:52.120 --> 00:16:57.720]   able to know. So, um, you know, like if I can elect it, I will certainly say what I can. But
[00:16:57.720 --> 00:17:02.440]   I think it's worth saying, like people could have that same legitimate fear against states,
[00:17:02.440 --> 00:17:07.480]   especially because we literally just have this big Fysabille, uh, renewed, which gives us
[00:17:07.480 --> 00:17:12.920]   broad powers to spy on the entire planet in secret. And that's the, that's the important part is that
[00:17:12.920 --> 00:17:18.680]   a national security letter often says, and you can't tell anybody we told you exactly. Yep. You have
[00:17:18.680 --> 00:17:24.040]   three judges on the Fysacort that theoretically can give some pushback and we can read some of
[00:17:24.040 --> 00:17:29.400]   their rulings. But as far as it being transparent for the public, no, um, it's really scary the more
[00:17:29.400 --> 00:17:34.280]   you learn about it. There is a certain hypocrisy in us complaining, for instance, that the Russians
[00:17:34.280 --> 00:17:39.400]   impacted our elections when we have been for a hundred years impacting elections all over the
[00:17:39.400 --> 00:17:44.680]   world. It's just, you know, we, we didn't think of using Facebook to do it. Here's the story that
[00:17:44.680 --> 00:17:51.480]   was big in Reuters this week. Apple moves to store iCloud keys in China raising human rights
[00:17:51.480 --> 00:17:59.480]   fears. But the, but I have to say this is not, I mean, Apple is a company doing business in China.
[00:17:59.480 --> 00:18:05.240]   Any company doing business in China is required to obey the laws of China, just as any company
[00:18:05.240 --> 00:18:10.040]   doing business in the US is required to obey our laws. And incidentally, guess where the iCloud
[00:18:10.040 --> 00:18:17.000]   keys are stored in the US, they're stored at Apple and they will hand them over to law enforcement.
[00:18:17.000 --> 00:18:23.560]   They given a FISA request, an NSL letter or a simple subpoena or more.
[00:18:23.560 --> 00:18:31.000]   So this is, it's no different in other words. Well, I think there's, there's, there's a key
[00:18:31.000 --> 00:18:36.040]   difference. You know, I, so I lived in China for a while and in China, you don't have a reasonable
[00:18:36.040 --> 00:18:43.720]   expectation of privacy, even if you're, um, pretty digital, digitally savvy and sort of, uh,
[00:18:44.280 --> 00:18:48.440]   on the higher end of the income spectrum. That is, you know, in the United States,
[00:18:48.440 --> 00:18:52.680]   people are sort of blindly using the packaged software that, and the, the tools and the systems
[00:18:52.680 --> 00:18:57.880]   that come with their computers and their, you know, cell phones. You know, and I don't think the
[00:18:57.880 --> 00:19:05.320]   average person who uses keychain understands what that is and, uh, where the data is and who has
[00:19:05.320 --> 00:19:11.800]   access to the data. Um, you know, and so we sort of blindly click buttons and follow answers
[00:19:11.800 --> 00:19:17.080]   here and we lack a certain amount of skepticism that in, so you know, sort of seems a little
[00:19:17.080 --> 00:19:22.520]   counterintuitive, but, um, I think people who were, who would be using the service in China are
[00:19:22.520 --> 00:19:26.600]   a little bit more street savvy. Um, you know, they're safer is what you're saying.
[00:19:26.600 --> 00:19:33.320]   The Americans who trust Apple. I'm saying they're more aware and I would, I would be surprised if
[00:19:33.320 --> 00:19:39.400]   you know, it's been my experience that, um, in the digital realm, at least people are a lot less
[00:19:39.400 --> 00:19:46.280]   naive in China. And China's a huge com countries is not everybody, but it seems to be a, a greater
[00:19:46.280 --> 00:19:51.000]   sophistication there among sort of the average user in my experience than I, than I observe
[00:19:51.000 --> 00:19:56.280]   here in the United States. Well, I think that the average person's, uh, Michael and then Brianna,
[00:19:56.280 --> 00:20:00.760]   go ahead. Yeah. Sorry. The average person's relationship with the government and how they use
[00:20:00.760 --> 00:20:05.160]   technology and what they're able to see and not see, I think, is much different in China,
[00:20:05.160 --> 00:20:08.920]   uh, when you compare it to the average US citizen. So of course the government is spying on,
[00:20:08.920 --> 00:20:15.640]   on American citizens, but not in the same capacity that China is spying on and clamping down on.
[00:20:15.640 --> 00:20:21.320]   Do we know that behind the layers? So do we know that though? We don't know it because of
[00:20:21.320 --> 00:20:26.280]   the Faisal bill. We don't know what the American government is doing. We don't have a culture of,
[00:20:26.280 --> 00:20:31.400]   um, sort of layers upon layers upon layers of a sort of information network where there are
[00:20:31.400 --> 00:20:38.040]   tendrils that may span up through prefectural, um, governors. Yeah, it's a, it's different here.
[00:20:38.040 --> 00:20:42.840]   Let me, let me ask you this, since you've lived there, Amy, uh, this month, the Wall Street Journal
[00:20:42.840 --> 00:20:48.200]   had this article about the Chinese police adding facial recognition glasses that they can walk
[00:20:48.200 --> 00:20:53.560]   around and they can see people and they get a media facial recognition. This is the kind of thing
[00:20:53.560 --> 00:20:59.720]   Google Glass, uh, people were worried about Google Glass because now do they average Chinese
[00:20:59.720 --> 00:21:03.960]   citizen? I mean, you did this in America. Well, that'd be a big deal. Is the average Chinese
[00:21:03.960 --> 00:21:08.200]   citizen look at this and say, Oh, that's good. We're going to be safer or do they say, Oh, that's
[00:21:08.200 --> 00:21:12.680]   terrible. A couple of things. First of all, this, this is, this has been reported as though this is
[00:21:12.680 --> 00:21:19.880]   some kind of groundbreaking new technology in the 2008, I think it was World Cup. Um,
[00:21:19.880 --> 00:21:25.640]   this exact technology was, and a, and a much slower version of it, of course, and, and,
[00:21:25.640 --> 00:21:30.600]   and less powerful, but it was already in use and it was being tested to sort of predetermine who
[00:21:30.600 --> 00:21:36.120]   were the rappled rosers are, I use likely to cause problems. China has a very different
[00:21:36.120 --> 00:21:41.000]   attitude towards privacy than a lot of other places do around the world. So there are already
[00:21:41.000 --> 00:21:45.800]   systems where if you J walk, you know, you're, there's, there's facial recognition everywhere.
[00:21:45.800 --> 00:21:51.080]   So if you, if you J walk, your face is shown on a, on a sort of digital billboard with your name
[00:21:51.080 --> 00:21:56.120]   and your employer and you know, it's a purpose of it. It is to publicly shame you. Is it,
[00:21:56.120 --> 00:22:01.400]   um, is that true? It's considered a good thing or a bad thing. It's my sense that at least in some
[00:22:01.400 --> 00:22:05.720]   parts of China, that's considered a good thing. That's like, yes, that's keeping order.
[00:22:05.720 --> 00:22:09.960]   That's right. Think about from the other end though, like you would never have something like
[00:22:09.960 --> 00:22:13.880]   gamergate in China with people sending hundreds and hundreds and hundreds of
[00:22:13.880 --> 00:22:18.680]   dough and they take a lot of pride in that. Yeah, I've talked to my father in law about this.
[00:22:18.680 --> 00:22:24.040]   It's just, it's a different culture with different priorities there. I, I did want to say about the
[00:22:24.040 --> 00:22:29.960]   Apple, you said Apple keeps the encrypted keys stored here in the United States. Are you talking
[00:22:29.960 --> 00:22:36.360]   about the, we're talking about the file that your iCloud with all of your passwords,
[00:22:36.360 --> 00:22:40.760]   we're not talking at your public and private key to actually decrypt everything in iCloud.
[00:22:40.760 --> 00:22:45.800]   So remember during San Bernardino, Apple said to the FBI, no, we won't decrypt the phone.
[00:22:45.800 --> 00:22:49.560]   However, if you had just thought to bring the phone to the guy's house,
[00:22:49.560 --> 00:22:54.680]   because it's probably sinking to iCloud, most iPhones do, we'd be glad to hand over his iCloud
[00:22:54.680 --> 00:23:00.680]   data. Oh, Apple, Apple has never, they've, they don't like to talk about it, but they've never
[00:23:00.680 --> 00:23:04.600]   hidden the fact that they've always had access to your iCloud data. Dropbox has access to your
[00:23:04.600 --> 00:23:10.520]   Dropbox data. It's not that Google has access to Google drive data. That's typical in these
[00:23:10.520 --> 00:23:16.360]   situations. Your key does not protect you. It's, it's, right. I don't know why this is a mystery to
[00:23:16.360 --> 00:23:23.400]   everybody. It seems to be, but it shouldn't be. But that's why this headline is interesting to me.
[00:23:23.400 --> 00:23:28.360]   Apple moves to store iCloud keys in China. Well, of course, the Chinese say if you're going to store
[00:23:28.360 --> 00:23:33.000]   data of Chinese citizens, it better be stored in China, including the key. Just as here in the
[00:23:33.000 --> 00:23:38.360]   United States, Apple stores iCloud keys in the US and it's available to law enforcement as it would
[00:23:38.360 --> 00:23:43.880]   be in China. I guess I incorrectly assumed that that would be encrypted because she kind of trust
[00:23:43.880 --> 00:23:48.520]   out who did the right thing. Okay. So it is encrypted, so they have to break the encrypted child.
[00:23:48.520 --> 00:23:53.800]   No, Apple has the key. Okay. Well, isn't, isn't the point that if a Chinese government official
[00:23:53.800 --> 00:23:59.960]   wanted to look at citizens iCloud account, they, it would be easier for them to do that now that
[00:23:59.960 --> 00:24:04.600]   the keys are based in China rather than having to deal with like the US legal system and
[00:24:06.040 --> 00:24:09.480]   so that was my takeaway. It was just that it's much easier for human rights.
[00:24:09.480 --> 00:24:14.440]   Your US company doing business in China would just as we would demand that
[00:24:14.440 --> 00:24:21.640]   right. But you've got to follow the rules of the right local place. I mean, again, like I just,
[00:24:21.640 --> 00:24:26.600]   there's a level of sophisticated, I think there's a
[00:24:26.600 --> 00:24:35.960]   I think I would be surprised if people are storing copious amounts of very sensitive private data
[00:24:35.960 --> 00:24:39.640]   on, you know, in China, they would know better. Yeah. That's what I'm saying.
[00:24:39.640 --> 00:24:42.840]   That's fair. I mean, not everybody, but yes, that's that would be my way.
[00:24:42.840 --> 00:24:46.760]   We're more vulnerable in the US because the general impression in the US is, oh, no,
[00:24:46.760 --> 00:24:52.040]   it's an iCloud Apple protects my privacy. It's safe. Life is a lot easier here. And it gets,
[00:24:52.040 --> 00:24:58.920]   you know, and we just lack, we lack the skepticism that I see in other places. And we sort of blindly
[00:24:58.920 --> 00:25:05.000]   follow the, the edicts that we see on our screens without stopping to think for a few minutes about
[00:25:05.000 --> 00:25:09.960]   the consequences. So, and it is the case that China's process for looking at that data is
[00:25:09.960 --> 00:25:15.160]   different here in the United States. You need a legal entity with a warrant or a subpoena or an
[00:25:15.160 --> 00:25:22.440]   NSL in China. It's not quite as difficult for authorities to demand those keys. Court approval
[00:25:22.440 --> 00:25:27.080]   isn't required under Chinese law. So the police can just do it. Now, here's something interesting
[00:25:27.080 --> 00:25:32.600]   that GDPR goes into play in Europe in a couple of months. This is a huge sweeping privacy regulation.
[00:25:32.600 --> 00:25:39.640]   Yeah. And so I'm sort of now curious to find out how, how key chain,
[00:25:39.640 --> 00:25:45.080]   how does that work going forward? I mean, how many different, you know what I mean? So just
[00:25:45.080 --> 00:25:48.840]   from a practical point of view, like how many times do you have to authenticate and how do they
[00:25:48.840 --> 00:25:52.680]   ensure the encryption and so that it still meets the standards of all that, that regulation?
[00:25:52.680 --> 00:25:57.080]   Yeah, because businesses are required to protect that personal data.
[00:25:58.840 --> 00:26:05.560]   That's very interesting, isn't it? I imagine the EU will require that iCloud data be stored in
[00:26:05.560 --> 00:26:11.160]   or maybe that's why they're doing it in China. I don't know. Or maybe that's partly why it's,
[00:26:11.160 --> 00:26:16.840]   you know, maybe everybody's in Europe will have their data stored in China to get around some of
[00:26:16.840 --> 00:26:24.040]   just the complicated issues. I don't know. You know, I, there are a lot of, there's a cloud service,
[00:26:24.040 --> 00:26:29.880]   Dropbox-like service called Trasor at T-R-E-S-O-R-I-T and they trumpet, "We're in Switzerland."
[00:26:29.880 --> 00:26:35.480]   So you're safe. I worked out really well for everybody's bank.
[00:26:35.480 --> 00:26:46.680]   I think in general, in the long run, the trend is away from GDPR, but I'm actually, at first,
[00:26:46.680 --> 00:26:51.640]   I was a little skeptical of EU regulation of American companies, but now I'm starting to think
[00:26:51.640 --> 00:26:56.360]   they're the last, that's the last stand. That's our last chance to protect our privacy.
[00:26:56.360 --> 00:27:01.400]   I mean, I've had a lot of talks about how we would pass something like an omnibus privacy bill
[00:27:01.400 --> 00:27:05.720]   here in the United States. Is there interest in that? Is there, I'm sorry, I missed that.
[00:27:05.720 --> 00:27:07.480]   Is there an interest in that? Is that going to happen?
[00:27:07.480 --> 00:27:12.520]   I think there is with consumers, especially the people that donate to my campaign. The question is,
[00:27:12.520 --> 00:27:18.280]   could we actually get something like that through Congress? Because it's not, I don't think,
[00:27:18.280 --> 00:27:23.640]   from the lawyers I've talked to, the tech subcommittee would have the authority to do that.
[00:27:23.640 --> 00:27:31.640]   I think we need the United States is, I think we need to realize that most of our laws about privacy
[00:27:31.640 --> 00:27:38.120]   and online behavior were passed in the 90s. And I think we need a fresh look at that.
[00:27:38.120 --> 00:27:42.280]   I just don't know if this Congress will ever have the will to do something serious.
[00:27:42.280 --> 00:27:47.800]   So I think when you're looking to Europe, if they're taking step forwards, I'll take that.
[00:27:48.520 --> 00:27:53.480]   Jeanie PR goes into effect in May, and I would bet they'll be quite a reckoning.
[00:27:53.480 --> 00:28:01.960]   This is not, so I've been talking about this for a while. We're going to wind up with a
[00:28:01.960 --> 00:28:05.000]   series, we're going to wind up with splintered internet, where we have, we have
[00:28:05.000 --> 00:28:09.480]   internets that are defined, different versions of the internet with different versions of content
[00:28:09.480 --> 00:28:13.720]   that's defined by geographic borders, which is going to make our current fake news nightmare,
[00:28:14.600 --> 00:28:19.880]   a insurmountable headache. But it also makes it the cost of doing business,
[00:28:19.880 --> 00:28:23.960]   the practicalities of doing business. I mean, it's just, it could exponentially change
[00:28:23.960 --> 00:28:29.560]   in a very negative way how all of us access information and each other.
[00:28:29.560 --> 00:28:42.360]   I don't know if even more regulation, regulation in absence of helping people gain digital literacy.
[00:28:42.360 --> 00:28:47.160]   Right. So the laws haven't changed in the 90s, but I don't think a lot of people have changed
[00:28:47.160 --> 00:28:53.080]   either in their approach to what they're sharing and how they're sharing information and what their
[00:28:53.080 --> 00:28:59.160]   expectations for their privacy is. And some of the onus has got to be on us too, which you can't
[00:28:59.160 --> 00:29:05.320]   just all be about regulation. I understand that point and I really understand where you're coming
[00:29:05.320 --> 00:29:09.800]   from. But I think when you look at something like Facebook, I don't think Facebook is ever going
[00:29:09.800 --> 00:29:14.440]   to change. I think we can write all the headlines in the world shaming them about their business
[00:29:14.440 --> 00:29:19.160]   practices. I don't think they're ever going to do the right thing. I think Facebook has grown so
[00:29:19.160 --> 00:29:25.080]   large and I think their leadership is really divorced from the reality of the octopus that
[00:29:25.080 --> 00:29:30.760]   they've created. Talk to people that own media companies. Like Facebook has really, really,
[00:29:30.760 --> 00:29:36.120]   really been bad for most media companies. And I think you look at their role in the election
[00:29:36.120 --> 00:29:41.480]   in a real failure to have accountability about Russians buying ads on their platform.
[00:29:41.480 --> 00:29:46.760]   You know, here in Boston, if a person bought an ad for a certain political candidate,
[00:29:46.760 --> 00:29:52.600]   that television station would be in jail. And the fact that we don't have those same laws online,
[00:29:52.600 --> 00:29:58.760]   I think we've got to take a fresh look at this. And I tell you, I really get it. I understand it.
[00:29:58.760 --> 00:30:04.120]   As someone that's run a business, it's really bizarre to be running for office and to start
[00:30:04.120 --> 00:30:09.800]   being the one talking about regulation. I just don't see a lot of these companies ever doing the
[00:30:09.800 --> 00:30:15.560]   right thing. It's very challenging. I agree with you, Amy, that individuals are responsible.
[00:30:15.560 --> 00:30:20.680]   But at the same time, companies like Facebook should, I don't think should be allowed to run
[00:30:20.680 --> 00:30:27.160]   rough shot over people's privacy. The problem is that here's the problem with regulation.
[00:30:27.160 --> 00:30:33.960]   Regulation and policy are written by people in such a way so that it passes.
[00:30:33.960 --> 00:30:40.520]   What you wind up with is highly specific limiting language, because anything that's too broad
[00:30:40.520 --> 00:30:45.880]   can't make its way through. And so we just constantly have regulation that's out.
[00:30:45.880 --> 00:30:47.800]   Like the moment that it gets passed, it's outdated.
[00:30:47.800 --> 00:30:51.880]   So is the political process you're saying is inadequate to do this?
[00:30:52.440 --> 00:30:57.720]   I'm saying we need to work. I mean, listen, Brianna, I totally, I support you and I,
[00:30:57.720 --> 00:31:03.480]   it would be awesome if you got to Congress and made some positive changes. That'd be amazing.
[00:31:03.480 --> 00:31:11.160]   But we also just as individuals need to come up with some workarounds.
[00:31:11.160 --> 00:31:17.640]   Yeah, it may end up being, I mean, ultimately, that's also the solution of fake news that people
[00:31:17.640 --> 00:31:22.040]   say, well, it's really going to be up to individuals. You can't stop fake news. It's going to be up
[00:31:22.040 --> 00:31:29.240]   to each and every one of us to be more critical. But at the same time, I think the reason the
[00:31:29.240 --> 00:31:34.840]   GDPR, the general data protection regulation in Europe exists is because the United States
[00:31:34.840 --> 00:31:38.680]   has abrogated its duty. GDPR is aimed at American companies.
[00:31:38.680 --> 00:31:45.000]   Sure. That's right. I feel like I did just want to add one point of fact here. I think I would
[00:31:45.000 --> 00:31:50.360]   encourage people, I understand the fear that like if you got all of Congress to vote on a
[00:31:50.360 --> 00:31:54.600]   technology bill, my God, those people have no idea what they're talking about.
[00:31:54.600 --> 00:31:59.400]   I understand that. But the way that Congress works and the way that the Senate works is when
[00:31:59.400 --> 00:32:04.520]   you're elected, you get assigned to certain subcommittees like Al Franken was assigned to the
[00:32:04.520 --> 00:32:09.240]   judiciary. I hope to serve on the technology subcommittee. And then you have a group of
[00:32:09.240 --> 00:32:14.680]   interested people, hopefully with some career experience in that field to kind of read about
[00:32:14.680 --> 00:32:21.800]   an issue and understand it. So it's not like this hodgepodge of useless stuff. So I do think that
[00:32:21.800 --> 00:32:28.680]   when I do think that government at its best can write good law that is well-informed by the facts,
[00:32:28.680 --> 00:32:33.240]   I just think currently the people serving on that all too often serve powerful
[00:32:33.240 --> 00:32:39.400]   as we have seen with our discussion on net neutrality. Verizon and AT&T sure gave those
[00:32:39.400 --> 00:32:41.320]   people a lot of money for their elections.
[00:32:43.320 --> 00:32:48.760]   I don't want to throw out the political process because if you say, well, the political process
[00:32:48.760 --> 00:32:54.120]   is inadequate to it, then you have zero regulation. Amy, is that what you're proposing?
[00:32:54.120 --> 00:33:02.040]   Now the challenge is, so my job is to think very, very far into the future. So the problem is that
[00:33:02.040 --> 00:33:07.960]   right now, so deep fakes, we all remember what started as something.
[00:33:07.960 --> 00:33:13.080]   The point of fakes with the new face is, yeah. So here's the problem. This raised all these
[00:33:13.080 --> 00:33:20.840]   thorny issues that I haven't heard anybody discuss. And as a result, so first of all,
[00:33:20.840 --> 00:33:26.360]   who owns your face? Well, that's not clear. I went through with a fine-toothed comb,
[00:33:26.360 --> 00:33:33.640]   all of Facebook's terms of service in every part of the website that addresses nudity and people
[00:33:34.200 --> 00:33:37.560]   that would have addressed the issue of deep fakes. Same thing for Twitter.
[00:33:37.560 --> 00:33:45.800]   You know, so basically nobody broke the terms of service on Twitter. And for Facebook, did you
[00:33:45.800 --> 00:33:51.720]   guys see that if you feel like you've been a victim of revenge porn or of deep fakes, their
[00:33:51.720 --> 00:33:58.200]   solution at the moment, the last that I read was that you have to use Facebook messenger to send
[00:33:58.200 --> 00:34:02.440]   in a completely nude photo of yourself. That was their proposal. That was a nutty idea.
[00:34:02.440 --> 00:34:09.720]   Yeah. Well, that's being piloted in Australia, as far as I know. So, I mean, here's the problem.
[00:34:09.720 --> 00:34:14.360]   The problem is that the platforms are constantly absolving themselves of any blame.
[00:34:14.360 --> 00:34:18.440]   They're saying we're platforms. We're not responsible. That's right. But although,
[00:34:18.440 --> 00:34:26.280]   so here's the problem. Who owns your face? Who owns what gets to be done with your face?
[00:34:26.280 --> 00:34:32.520]   If your face is you, how do you get your face back? Right? And I could list like 20 legal
[00:34:32.520 --> 00:34:37.880]   questions for which we have no answers and there's no debate and there's no policy and, you know,
[00:34:37.880 --> 00:34:43.480]   nobody's discussing that. And I could do the same thing for, you know, various aspects of
[00:34:43.480 --> 00:34:49.640]   machine learning and who owns these different parts of your data and the decisions that are made.
[00:34:49.640 --> 00:34:57.800]   Get ready for HR 1865, allowing states and victims to fight online sex trafficking act of 2017,
[00:34:57.800 --> 00:35:05.160]   which in effect makes websites responsible for the content posted on the website. It is a complete
[00:35:05.160 --> 00:35:10.600]   overreaction, but it means that every website were responsible for anything that is posted on that
[00:35:10.600 --> 00:35:16.840]   website. Which will become a First Amendment fight because somebody will make the case that
[00:35:17.880 --> 00:35:23.240]   it's limiting, you know, it's hindering and unnecessarily limiting the expression of free speech in the
[00:35:23.240 --> 00:35:27.240]   comment section. A provider of an interactive computer service that publishes information
[00:35:27.240 --> 00:35:31.720]   I'm reading from it provided by an information content provider with reckless disregard that
[00:35:31.720 --> 00:35:35.880]   the information is in furtherance of a sex traffic offense to be subject to a criminal
[00:35:35.880 --> 00:35:42.200]   fine or imprisonment for not more than 20 years. And, and for, I mean, good luck.
[00:35:42.200 --> 00:35:47.480]   Here's the problem. Here's the problem. The deep fake sites. So it takes like three, you know,
[00:35:47.480 --> 00:35:53.080]   motherboards did really great reporting on this. So if this is something listeners are interested in,
[00:35:53.080 --> 00:35:57.560]   I would, I would strongly suggest you take a look at their reporting, you know, so, so here's the
[00:35:57.560 --> 00:36:04.920]   thing. Finally, Pornhub and finally Reddit take down the thread where this is all being discussed
[00:36:04.920 --> 00:36:10.360]   and how two guides are being shown. But there are clones everywhere, everywhere. You can,
[00:36:10.360 --> 00:36:15.880]   you can throw a rock on the internet and hit a website where you can find somebody who's willing
[00:36:15.880 --> 00:36:20.920]   to make a deep fake for you, you know, because out of the goodness of their heart, or for, you
[00:36:20.920 --> 00:36:26.120]   know, a fraction of Bitcoin. So the problem is you can't, you can't regulate an idea. And that's
[00:36:26.120 --> 00:36:29.400]   the problem that we keep coming back to in the United States over and over again. That is our
[00:36:29.400 --> 00:36:33.800]   current problem with the Second Amendment that's going to become a problem with our Fifth Amendment
[00:36:33.800 --> 00:36:39.560]   and various different kinds of technologies. It's the problem with deep fakes. Once the idea is out
[00:36:39.560 --> 00:36:45.480]   there, it's very, very difficult to draw it back in. And all of the technology that makes us so
[00:36:45.480 --> 00:36:50.920]   productive and enhances our lives in so many ways, you know, can fork and take us all in a
[00:36:50.920 --> 00:36:57.400]   different direction. That's the problem. I need to say, but I sort of disagree with that. The
[00:36:57.400 --> 00:37:03.560]   notion that you can't regulate or stop an idea, you know, like, I think you can look at even just
[00:37:03.560 --> 00:37:08.840]   like copyrighted MP3 sharing. You know, that was something that was a problem. I think people
[00:37:08.840 --> 00:37:12.920]   identified it. Some people made the case that you're making now, which is like, well, cats out of the
[00:37:12.920 --> 00:37:18.040]   bag, people know how to share peer to peer. And the internet's only getting faster. The demand for
[00:37:18.040 --> 00:37:24.200]   audio is only getting, is only growing. And so the second Napster came on the scene, you couldn't
[00:37:24.200 --> 00:37:30.760]   put all of the stuff back into, into the box. The reality is that we've kind of solved that
[00:37:30.760 --> 00:37:36.120]   issue now. You know, most people use streaming services rather than, rather than downloading
[00:37:36.120 --> 00:37:42.280]   illegal MP3s as they once did. I think in the case of deep fakes, you know, you can't,
[00:37:42.280 --> 00:37:48.440]   you can't make an idea illegal, but you can make the act of creating fake pornography illegal. I
[00:37:48.440 --> 00:37:53.800]   don't think that's out of line or even other. Sure. You could totally do that. My point is that
[00:37:53.800 --> 00:37:59.320]   there will, that's like fighting last yesterday's war, right? So sure, we, religious leaders could
[00:37:59.320 --> 00:38:04.440]   get together tomorrow and come up with a, you know, the house could pass a anti deep fake bill.
[00:38:04.440 --> 00:38:09.240]   That the, the problem is that in order to get that legislation through, you're using language
[00:38:09.240 --> 00:38:13.560]   for something that's already happened, not for the next, you know, third, fourth, and fifth,
[00:38:13.560 --> 00:38:18.120]   and sixth things that'll happen. And just going back to the MP3 share a minute.
[00:38:18.120 --> 00:38:24.280]   That's a piracy process. Well, but, you know, so, so Canada has legislation on the books that
[00:38:24.280 --> 00:38:30.120]   makes it illegal and finable for anybody to post anything that's pirated on Google. So if I search
[00:38:30.120 --> 00:38:35.400]   in Canada on something and I see a pirated version of it come up, that's illegal. And that's in direct
[00:38:35.400 --> 00:38:40.520]   response to the fact that they are, they, you know, you may not, you know, so, so peer to peer
[00:38:40.520 --> 00:38:45.080]   file sharing may not have, may not be super popular in the United States anymore because people
[00:38:45.080 --> 00:38:49.560]   can stream, but that's not the case elsewhere in the world. Right. And that's that we keep losing
[00:38:49.560 --> 00:38:54.760]   perspective on, you know, just because something no longer is attractive or popular here in the
[00:38:54.760 --> 00:38:58.360]   United States, you know, we have the sort of recency phenomenon where we forget that
[00:38:58.920 --> 00:39:01.320]   maybe things are not, you know, maybe things are a little different elsewhere.
[00:39:01.320 --> 00:39:07.800]   Of course, but we're talking about US policy and lawmakers, you know, taking responsibility
[00:39:07.800 --> 00:39:13.640]   for the things that are happening online. I think, you know, as you said, like, sure,
[00:39:13.640 --> 00:39:18.040]   they're solving a problem that is sort of, that's already happened in there and, and they're sort
[00:39:18.040 --> 00:39:24.200]   of losing ground on the next big, the next big battle here. But, but I think that's kind of how,
[00:39:25.160 --> 00:39:30.440]   that's kind of how law, the law system works. So like the, the political system works, you know,
[00:39:30.440 --> 00:39:36.840]   we can't, the lawmakers job is not to predict, you know, future crimes. I mean, to some extent,
[00:39:36.840 --> 00:39:42.040]   totally, totally agree. I totally agree. We, we had, we've had offices,
[00:39:42.040 --> 00:39:48.760]   whose express job it was to, and that is what strategic foresight is, to map out,
[00:39:48.760 --> 00:39:53.800]   given what we know to be true today and all of the data that we have, what's likely coming and to
[00:39:54.360 --> 00:40:00.680]   monitor that and then write policy so that we can keep track. I mean, technology is evolving so
[00:40:00.680 --> 00:40:05.240]   quickly that it can no longer keep up with, I'm sorry, that the policy can no longer keep up with
[00:40:05.240 --> 00:40:11.320]   technology the way that it has in the past. And just really quickly on the internet. So we don't
[00:40:11.320 --> 00:40:19.640]   see the internet as sort of a, as the commons, the same way that we do the, the air overhead, right?
[00:40:19.640 --> 00:40:26.680]   So, so with different ecological and environmental issues, we have multinational stakeholder groups
[00:40:26.680 --> 00:40:34.600]   where there are global scale regulations and, you know, not entirely enforceable in the way that
[00:40:34.600 --> 00:40:39.640]   a traditional law might be, you know, if you, if you break that, that the penalties may be enforceable,
[00:40:39.640 --> 00:40:44.120]   but we, we absolved ourselves of that with the internet. So there is no, you know, there,
[00:40:44.120 --> 00:40:49.160]   there are different groups here and there, but we don't have global treaties. We don't have
[00:40:49.160 --> 00:40:54.920]   global regulatory policies. So even if we were to pass a law in the United States and it became
[00:40:54.920 --> 00:41:00.600]   illegal to create a deep fake, you know, here, there's nothing preventing somebody from doing
[00:41:00.600 --> 00:41:06.120]   that exact same thing and having it show up in the exact same way, you know, as long as they were
[00:41:06.120 --> 00:41:11.160]   outside of the boundaries of the United States. So I feel like I need to add something here because
[00:41:11.160 --> 00:41:17.000]   this isn't hypothetical for me. I actually, I know the people at the very forefront of working on this
[00:41:17.000 --> 00:41:21.560]   area of legislation. Her name is Maryam Frank. She's one of the most brilliant lawyers in the United
[00:41:21.560 --> 00:41:27.720]   States. I've worked with her on this at Harvard and she herself, like the person that's on the forefront
[00:41:27.720 --> 00:41:33.160]   in trying to figure out the legal apparatus for fighting this, she's not 100% on the side of
[00:41:33.160 --> 00:41:38.680]   government regulation. Like she talks to Google all the time about updating their policies. She
[00:41:38.680 --> 00:41:44.360]   talks to Reddit. She talks to Facebook. Then she goes and tries to work with local law like here in
[00:41:44.360 --> 00:41:50.680]   Massachusetts. She wants to give prosecutors more power. So I think with all respect to your point,
[00:41:50.680 --> 00:41:56.920]   I understand like globally, there's not one single solution you can point to. What something like
[00:41:56.920 --> 00:42:02.520]   this is going to require is exactly what Maryam is doing. She's looking at the problem for a large
[00:42:02.520 --> 00:42:08.520]   legislative point of view and she's walking working with all the involved actors. So like Google,
[00:42:08.520 --> 00:42:14.840]   if Google will remove certain things from the search results because they agree it's an invasion
[00:42:14.840 --> 00:42:22.440]   of privacy, then that reduces some of the risks. So it's a very difficult problem to solve. Just
[00:42:22.440 --> 00:42:28.040]   talk to her. She spends all of her time doing this. But I certainly think you're seeing her move
[00:42:28.040 --> 00:42:33.880]   forward on that. And I think the situation with revenge porn is much better than it was five years
[00:42:33.880 --> 00:42:38.920]   ago because she didn't shrug her shoulders. She got in there and worked with the law and
[00:42:38.920 --> 00:42:49.080]   started making a difference here. It's a challenge. It's a big challenge. And we don't want,
[00:42:49.080 --> 00:42:58.360]   I frankly think it's virtually intractable. And yet you don't want to give up. You need to try to
[00:42:58.360 --> 00:43:03.560]   make some steps. You need to try to do something because the alternative is to give up. You're not
[00:43:03.560 --> 00:43:08.120]   saying we should give up, Amy. You're saying we should be more forward thinking.
[00:43:08.120 --> 00:43:12.680]   That's absolutely right. I am not saying to give up and I don't mean to sound like a fatalist.
[00:43:12.680 --> 00:43:18.760]   What I am saying is the problems that we are going to face over the next 10 to 20 years are
[00:43:18.760 --> 00:43:25.240]   nothing. Our orders of magnitude more complicated than we've seen thus far. And it's going to require
[00:43:25.240 --> 00:43:33.320]   very smart people like Brianna. And I think that there's a new crop of people who are running
[00:43:33.320 --> 00:43:37.320]   for Congress. There are people who are in Congress who are very smart. There's people who are writing
[00:43:37.320 --> 00:43:43.000]   policy. It's just going to require a different approach. And there is an inherent tension between
[00:43:43.000 --> 00:43:52.520]   the way that we have always created policy and law and the current ways in which we share
[00:43:52.520 --> 00:43:58.440]   information and use technology. I would submit that you are a stakeholder in this, Amy.
[00:43:59.000 --> 00:44:04.440]   Your focus is in the future. That absolutely has to be considered. The women who are today
[00:44:04.440 --> 00:44:10.920]   victims of repent, revenge porn and fakes also are a stakeholder and need to be considered.
[00:44:10.920 --> 00:44:16.440]   The challenge Brianna that your friend faces is you also have to consider Google. You have to consider
[00:44:16.440 --> 00:44:23.160]   kind of bad ideas like FOSTA, which is going to be by the way, HR 1865 is going to be voted on
[00:44:23.160 --> 00:44:30.440]   Tuesday. This is not hypothetical. It's not in committee. This is about to vote. As a representative
[00:44:30.440 --> 00:44:34.760]   about to vote on this, this is a bad idea. And there are plenty of those as well.
[00:44:34.760 --> 00:44:41.320]   But I think we need to find a way forward. And the sad fact, maybe that's a hopeful fact. I don't
[00:44:41.320 --> 00:44:46.600]   know. The fact of the matter is we are faced right now because of the speed the technology is changing
[00:44:46.600 --> 00:44:53.000]   our lives in so many ways with artificial intelligence, self-driving cars,
[00:44:53.000 --> 00:44:58.920]   free speech. We are faced with very difficult challenges. That's why by the way, Brianna,
[00:44:58.920 --> 00:45:04.680]   I hope you win. And I hope you understand what you're getting yourself into because this is a mess.
[00:45:04.680 --> 00:45:10.200]   Look at that bill. You just mentioned, Leah, I would vote against that bill. I would have no
[00:45:10.200 --> 00:45:15.560]   problem with that. I know it takes a lot of heat for that. It's a terrible solution. But I would
[00:45:15.560 --> 00:45:22.840]   say this. All respect to the generation that's older than me. I do think that new generations
[00:45:22.840 --> 00:45:28.600]   come up and take the helm. And it's my hope. Maybe this is idealistic. But it's my hope that
[00:45:28.600 --> 00:45:34.920]   the next generation is going to have less of a ruthless approach to problem-solving. And maybe
[00:45:34.920 --> 00:45:39.880]   we can change the culture in Congress. It's certainly taking a worse approach.
[00:45:39.880 --> 00:45:47.640]   And that's a scorched earth approach anyway. We're so unfortunately polarized over this.
[00:45:47.640 --> 00:45:51.880]   It's impossible to make a headway in any direction. We had a very good interview on
[00:45:51.880 --> 00:45:56.760]   Friday on triangulation with Andrew Keene who's for a long time said the internet is broken
[00:45:56.760 --> 00:46:01.640]   and has written a new book on what we needed to fix it. Essentially, that's his position.
[00:46:01.640 --> 00:46:05.160]   He says, this is very similar to the Industrial Revolution where you had
[00:46:06.280 --> 00:46:10.600]   technologists and you have them today who are very hopeful that technology will just solve all of
[00:46:10.600 --> 00:46:15.400]   this. There's no need for regulation. It'll all be fixed. They'll be universal income. Everything
[00:46:15.400 --> 00:46:22.120]   will be super cheap. It's all going to be fine to worry. There are the naysayers who say this is
[00:46:22.120 --> 00:46:29.000]   all a mess. Let's destroy the looms. Let's go back to the past. Let's get kerosene and light the
[00:46:29.000 --> 00:46:33.000]   homes that way. And then there's the, he calls them the maybes that the people in the middle.
[00:46:33.000 --> 00:46:36.840]   And this is what you need to be. This is what we all need to be. People who understand this is a
[00:46:36.840 --> 00:46:41.080]   very difficult problem, but not throw up their hands, not assume that it's going to happen by
[00:46:41.080 --> 00:46:45.960]   itself, but actually dig in and have these discussions and see what we can do to fix it.
[00:46:45.960 --> 00:46:52.440]   And you can't expect agreement. I guess one of the things that worried me that Amy that you said
[00:46:52.440 --> 00:46:58.520]   was that the legislative process is essentially political. It is about compromise, but that's a
[00:46:58.520 --> 00:47:05.240]   necessity. I don't think you can write the perfect law. Do you think that that's a failing?
[00:47:05.240 --> 00:47:11.000]   That means it won't work? I was at the state of the net, which so there's this
[00:47:11.000 --> 00:47:17.320]   conference that happens in this gathering, annual gathering in DC. And I don't know,
[00:47:17.320 --> 00:47:24.200]   over the first two hours, I must have heard the word bot, like uttered by senators and
[00:47:25.960 --> 00:47:30.200]   like top level people. And I got to the point where it was like, I don't think everybody
[00:47:30.200 --> 00:47:34.200]   understands what their, that word means. I don't think it means. That's exactly right. That's
[00:47:34.200 --> 00:47:40.760]   exactly right. And here's why I found that so concerning, because if our, and I don't expect
[00:47:40.760 --> 00:47:46.040]   every lawmaker to have a very high level understanding, deep level understanding of all
[00:47:46.040 --> 00:47:52.680]   different technologies that they could possibly regulate. However, our legislative process is
[00:47:52.680 --> 00:47:58.600]   necessarily slow. And that's a good thing. We don't want our laws changing every five seconds.
[00:47:58.600 --> 00:48:06.280]   The challenge, however, is that technology is becoming complex, and it is presenting us with
[00:48:06.280 --> 00:48:13.000]   questions that we have never had to ask or an answer before. We talk about artificial
[00:48:13.000 --> 00:48:19.880]   intelligence. It's not just automation, but for me, this is the area that I research. The thing for
[00:48:19.880 --> 00:48:26.040]   me that's most concerning has to do with autonomous decision making by machines that don't share
[00:48:26.040 --> 00:48:31.080]   conscious, don't have consciousness the way that we do. So machines are making sort of
[00:48:31.080 --> 00:48:37.400]   non-conscious decisions, which will be the first time in humanity that's ever happened.
[00:48:37.400 --> 00:48:44.200]   And the problem is that questions like, who owns your face, right? We'll seem childish today,
[00:48:45.560 --> 00:48:51.480]   compared with the direction that we're headed in. And if I have a lawmaker who five years after
[00:48:51.480 --> 00:48:56.280]   the fact is having a hard time explaining to a group of technologists what a bot is,
[00:48:56.280 --> 00:49:01.400]   that signifies to me that we need to come up with, you know, we need for sort of the
[00:49:01.400 --> 00:49:09.480]   grand scale, you know, issues in society, our current system works, you know, works.
[00:49:09.480 --> 00:49:14.440]   But when it comes to technology, we need to think of, we need to think of alternate methods
[00:49:15.080 --> 00:49:21.080]   to have faster, smarter conversations, stakeholders who are not just tied to the commercial sector,
[00:49:21.080 --> 00:49:24.120]   and who are not totally politicized, right? It's a tall order.
[00:49:24.120 --> 00:49:30.600]   It's a very tall order. And my fear is that if you get too negative about it, then what happens is
[00:49:30.600 --> 00:49:39.400]   you leave the democratic process behind, because an authoritarian process is very good at handling
[00:49:39.400 --> 00:49:46.200]   this and can move very quickly. Maybe not. Yeah. And maybe that's not the right answer either.
[00:49:46.200 --> 00:49:51.560]   Yeah. Michael Nunez is here from Mashable. I'm going to give you the final word, Michael,
[00:49:51.560 --> 00:49:57.160]   because we've been hogging the conversation before we take a break. Isn't it fascinating?
[00:49:57.160 --> 00:50:02.040]   And I I'm trying not to get depressed by it. Yeah, I also I kind of forgot I was on the show
[00:50:02.040 --> 00:50:06.360]   for a second. Yeah, give me two. I'm listening. Yeah, two smart people here. Yeah.
[00:50:06.360 --> 00:50:10.360]   Yeah. I think just to clarify my point, you know, it's not that lawmakers shouldn't be thinking
[00:50:10.360 --> 00:50:13.720]   about the future. Of course, they should be doing that, but laws are often written based on precedent,
[00:50:13.720 --> 00:50:19.400]   right? So it's things happen. And then lawmakers react. And I think that's just the sort of realistic
[00:50:19.400 --> 00:50:24.040]   expectation that I've come to learn. I think, you know, to Amy's point about what to expect in the
[00:50:24.040 --> 00:50:28.600]   future, I think it's the P P tape. I think it's the Donald Trump P tape. I'm surprised that that
[00:50:28.600 --> 00:50:33.720]   hasn't surfaced already. What does this have to do with the P tape? Well, it doesn't matter. Let's
[00:50:33.720 --> 00:50:40.440]   talk about it, which may or may not exist. Yeah. Well, it probably who knows whether it exists
[00:50:40.440 --> 00:50:47.320]   in reality, but but things like deep fakes and and and AI generated. Do you think you think we're
[00:50:47.320 --> 00:50:52.040]   being set up so that they could say that's a fake if it should emerge? Is that what you're saying?
[00:50:52.040 --> 00:51:00.360]   Yeah, I mean, I'm not I'm not not in the conspiracy in the theory. Yeah, but but I think that it would
[00:51:00.360 --> 00:51:06.920]   be I think it's realistic to expect a video like that to serve in a larger scale. Here's what I think
[00:51:06.920 --> 00:51:12.520]   you're saying that is I think really important. And it's the sad state of the affairs is that we've
[00:51:12.520 --> 00:51:18.520]   set up all of these things that if people are good and benevolent and wonderful, are good and
[00:51:18.520 --> 00:51:23.080]   benevolent and wonderful, YouTube, Facebook, Twitter, on and on many of the great technologies
[00:51:23.080 --> 00:51:29.960]   we've created. But we didn't countenance and in plan for is people who aren't good, people who are
[00:51:29.960 --> 00:51:36.840]   motivated in a variety of different ways. And they have discovered technology and have discovered
[00:51:36.840 --> 00:51:44.680]   ways to bend it that are problematic. But that's always happened with every technology.
[00:51:44.680 --> 00:51:51.240]   You know, it every technology that has always been created with benevolent purposes in mind
[00:51:51.240 --> 00:51:56.040]   has taken a turn for the human suck. So what do you do about that?
[00:51:56.040 --> 00:52:03.480]   Preparing for the suck in a tooth. Yeah, no, but this is this is my whole point. We have people
[00:52:03.480 --> 00:52:10.200]   in Washington, DC who are not thinking not assuming that at some point the catastrophic scenarios
[00:52:10.200 --> 00:52:15.160]   are plausible. And if you're not in that headspace, I'm not saying to be negative all the time,
[00:52:15.160 --> 00:52:20.840]   but you have to acknowledge the possibility that these things could happen, right? Or something bad.
[00:52:20.840 --> 00:52:23.400]   Well, and then acknowledge that you can't stop it.
[00:52:23.400 --> 00:52:30.360]   No, so then let's come up with 100% promise you, Amy, is as someone who's doing this every single day,
[00:52:30.360 --> 00:52:36.920]   I have to promise you they have no idea. They have no idea what the fake says. They have no plan for
[00:52:36.920 --> 00:52:42.440]   this. And they just this is the fundamental problem is they see it as.
[00:52:42.440 --> 00:52:45.080]   Be careful. Be careful.
[00:52:45.080 --> 00:52:52.360]   Try to knowledge as someone with it's like a backup singer that they can hire and bring in to solve
[00:52:52.360 --> 00:52:58.600]   the problem. They have no stake in this problem by themselves. And it is, I'm telling you this
[00:52:58.600 --> 00:53:03.560]   is someone who's a software engineer, someone who like stopped to get involved and do this.
[00:53:03.560 --> 00:53:10.120]   And I am so depressed by the level of technological literacy that I see in government.
[00:53:10.120 --> 00:53:15.960]   But that's why I need everyone on Twitch that's listening to this to realize the process needs
[00:53:15.960 --> 00:53:22.200]   you. Get involved, go run for your town committee, go sit on your local political party council,
[00:53:22.200 --> 00:53:26.920]   especially if you're a Republican. Republicans, we need Republicans that care about these issues.
[00:53:26.920 --> 00:53:34.520]   The thing I've found is technologists generally agree on certain core of things,
[00:53:34.520 --> 00:53:39.880]   no matter what political party we belong to. So Amy, I really agree with you is worse than I
[00:53:39.880 --> 00:53:44.680]   possibly could have imagined. But that's why we have to not get cynical and get involved.
[00:53:44.680 --> 00:53:49.880]   All right. That's a good way to put it. And we're going to take a break at that on that note.
[00:53:49.880 --> 00:53:53.160]   This is a conversation. It's interesting that having more and more on this network, I think
[00:53:53.160 --> 00:53:58.440]   that we are all kind of aware of this now. And we're wondering what to do.
[00:53:58.440 --> 00:54:05.480]   My fear is that we let some point abandon a democratic policies and become a technocratic
[00:54:05.480 --> 00:54:11.640]   state because I think there's going to be a feeling at some point that only the technocrats can solve
[00:54:11.640 --> 00:54:15.240]   this. And the Mark Zuckerbergs of the world are going to step up and say, I'll take care of this.
[00:54:15.240 --> 00:54:18.600]   The Jeff Bezos of the world said, you know, you know what, I understand how this works.
[00:54:18.600 --> 00:54:25.480]   Just let me take care of this. And I do see that as being one of the possible scenarios
[00:54:25.480 --> 00:54:30.040]   that respond to all of these problems that we're talking about. And it's one that I don't and
[00:54:30.040 --> 00:54:36.120]   think would be a very good pass to go down. But there we go. Let's take a break. We got a great
[00:54:36.120 --> 00:54:39.800]   panel. As I said, Michael Nunez for Mashable. Great to have you, Michael Brianna Wu,
[00:54:39.800 --> 00:54:46.520]   Congress running for Congress. The primary has been set September 2018. If you're in the
[00:54:46.520 --> 00:54:53.400]   Massachusetts 8th, you know what I think. You know what I'm up for. And Amy Webb, the author of
[00:54:53.400 --> 00:54:59.880]   the Signals are talking why today's fringe is tomorrow's mainstream. And we'll be very
[00:54:59.880 --> 00:55:04.120]   interested to see the FTI trend report when it comes out. Is there a place people can go to order
[00:55:04.120 --> 00:55:09.720]   it now to get on the list now? Yeah, well, we give it away. So it's free. So if you go to
[00:55:09.720 --> 00:55:15.080]   futuretodayinstitute.com, there's a link to it from the front page. Fantastic. Thank you.
[00:55:15.080 --> 00:55:21.560]   Our show today brought to you by Audible. If some of this gets you down, here's some good news.
[00:55:23.240 --> 00:55:29.000]   You can listen to a great book, a work of art that can take over your life and your mind and give
[00:55:29.000 --> 00:55:35.080]   you respite. And believe me, I do it every single day. Thanks to Audible. There's plenty of time.
[00:55:35.080 --> 00:55:40.280]   Reading is great. And but my problem is, as I think probably a lot of you, life took over. And I
[00:55:40.280 --> 00:55:45.080]   didn't have enough time to read. But Audible fills those holes in your day where you can't hold a
[00:55:45.080 --> 00:55:49.960]   book, you can't read, but you could listen to a book when you're driving to work, when you're
[00:55:49.960 --> 00:55:54.680]   walking the dog, when you're doing the dishes of the yard work. And Audible is such a great
[00:55:54.680 --> 00:56:00.120]   solution for that. I'm very excited about there's a couple of books I'm looking forward to. I
[00:56:00.120 --> 00:56:04.840]   already got on the waitlist for this one called Tangerine. It's a novel that sounds fantastic.
[00:56:04.840 --> 00:56:12.120]   It's a pre-order. But then I do listen to a lot of nonfiction as well. I mentioned that I'm
[00:56:12.120 --> 00:56:16.760]   watching, rewatching HBO's Band of Brothers. And I thought, you know, I really ought to
[00:56:17.800 --> 00:56:22.920]   listen to the original Stephen Ambrose book. In fact, there's a ton of Stephen Ambrose is one of my
[00:56:22.920 --> 00:56:29.320]   favorite historians. Isn't he great? And there's a ton of Ambrose works, including Band of Brothers
[00:56:29.320 --> 00:56:36.040]   that you can listen to. Audible always has the best narrators. That's one of the things you might
[00:56:36.040 --> 00:56:41.480]   say, Oh, I don't know. I don't want somebody droning into my ear. I can't listen to a book. Yes,
[00:56:41.480 --> 00:56:49.320]   you believe me. In some ways, listening to a book is the best way to absorb a book. I just love it.
[00:56:49.320 --> 00:56:52.760]   And so many great books. Here's what we're going to do to get you started. If you're a little,
[00:56:52.760 --> 00:56:58.600]   I don't know, is this for me? We've got a great offer for you. It's called the gold plus one plan.
[00:56:58.600 --> 00:57:04.520]   You're going to sign up for the gold plus one plan at audible.com/twit2. Here's how it works.
[00:57:04.520 --> 00:57:10.440]   You're going to get two books to start and then a 30 day free trial and a book a month after that.
[00:57:10.440 --> 00:57:15.880]   Two books to start means you don't have to do the tough thing of picking just one. And I can tell
[00:57:15.880 --> 00:57:20.280]   you that's always tough for me. I have a huge library of audible books. I've been an audible
[00:57:20.280 --> 00:57:27.000]   subscriber since the year 2000. So I have over 500 books. I've been one since 1998.
[00:57:27.000 --> 00:57:33.880]   You beat me. What are you listening to, Brianna? I've got my phone right here. Lost Camp of Girls
[00:57:33.880 --> 00:57:40.120]   Forever. That's really good. Lost Camp of Girls Forever. That's scary. It's really, it's really scary.
[00:57:40.120 --> 00:57:46.520]   In Percephalus Rising, the same James S.A. Corey novel. The Expanse is on sci-fi channel.
[00:57:46.520 --> 00:57:50.680]   Yes, the Expanse is so good. Oh, I love it. But the books are so much better.
[00:57:50.680 --> 00:57:56.200]   I agree. The biathan wakes is probably one of the very best books on audible and so is
[00:57:56.200 --> 00:57:59.560]   Snow Crash. You should sign up today and just listen to those.
[00:57:59.560 --> 00:58:07.080]   Oh, I love both those books. And in fact, I have both those books on audible. So I am with you 100%
[00:58:07.080 --> 00:58:13.960]   on that. And if you want something a little cheerier to improve your outlook, I like to listen to
[00:58:13.960 --> 00:58:19.480]   nonfiction to the Stephen Pinker book, The Better Angels of Our Nature really will make you feel
[00:58:19.480 --> 00:58:24.840]   better about today. It'll really cheer you up. Bill Gates recently picked Stephen Pinker's new book
[00:58:24.840 --> 00:58:28.600]   as his favorite book of all time in Light and Men Now. That's on there too. Here's the deal. Go
[00:58:28.600 --> 00:58:35.560]   to audible.com/twit2. Get yourself two free books. I think we've talked about eight now. But anyway,
[00:58:35.560 --> 00:58:40.920]   pick two and then one book credit per month. It is a great deal, 30 day free. As with all the
[00:58:40.920 --> 00:58:45.320]   free subscriptions, you get the Wall Street Journal of the New York Times Daily audio programs. So
[00:58:45.320 --> 00:58:49.480]   you have more to listen to. The offer is good in the US and Canada, but Audible is all over the world.
[00:58:49.480 --> 00:58:54.280]   So if you're in another country, don't hesitate. Try it anyway. To get two free audiobooks and a
[00:58:54.280 --> 00:59:02.120]   free 30 day trial, audible.com/twit2. Audible.com/twit2. You've been a member since 1998,
[00:59:02.120 --> 00:59:06.520]   Brianna. You beat me. I thought I was the king. No. Do you remember when you had to get the
[00:59:06.520 --> 00:59:13.320]   Rio 500? That's what I had to do. Before that, they had a proprietary thing. They called the auto.
[00:59:13.320 --> 00:59:18.520]   Do you remember that? I do. Do you remember burning your own CDs? Yes. Yes. It would fail
[00:59:18.520 --> 00:59:22.680]   three fourths of the time. Oh my gosh. I love Audible. I started listening to audiobooks when
[00:59:22.680 --> 00:59:26.600]   you had to get cassettes. And that was really a lousy experience because you only had a month
[00:59:26.600 --> 00:59:30.440]   for one thing and you had to mail them back. The cassettes, one would break, right? And they say,
[00:59:30.440 --> 00:59:34.040]   "Oh, no problem if one breaks. Just mail it in and we'll send you a new one. But I'm stuck in
[00:59:34.040 --> 00:59:38.440]   the middle of the book now. I can't move on." It's frustrating. Audio is so much better when you
[00:59:38.440 --> 00:59:42.920]   can download it. One of the nice things about Audible, you get the book right away. If you say,
[00:59:42.920 --> 00:59:46.760]   "Oh, I got to read that," you're reading it five seconds later. So that's really nice.
[00:59:46.760 --> 00:59:52.920]   Audible.com/twit2. Thank them for their support of Twit. They make it all possible.
[00:59:53.480 --> 01:00:01.000]   We should pick something more cheerful. We kind of went down a bad road there. But I think
[01:00:01.000 --> 01:00:07.560]   we do that more and more lately because I think that these things are coming up more and more
[01:00:07.560 --> 01:00:14.840]   often, frankly. This has become one of the issues in our lives. I think people are feeling
[01:00:14.840 --> 01:00:22.680]   anxious. Yes. Wonder why. Because we're well. But I mean, we're in this sort of, we sort of
[01:00:22.680 --> 01:00:26.440]   crossed over into this weird territory. And I think regardless of where you are on the political
[01:00:26.440 --> 01:00:32.600]   spectrum, after the last election, I think things have, we're sort of in unfamiliar territory.
[01:00:32.600 --> 01:00:35.880]   And I think people are just feeling more anxious in general.
[01:00:35.880 --> 01:00:42.440]   One technology in the environment. Yeah. Comedy and politics. Yeah. I think though,
[01:00:42.440 --> 01:00:48.200]   even if that hadn't happened, I think people would be feeling badly right now about social media.
[01:00:49.240 --> 01:00:54.680]   That certainly accelerated it. But I think we're starting to see this. Andrew Keeney I was talking
[01:00:54.680 --> 01:01:00.680]   about on Friday calls it surveillance capitalism. He says Google and Facebook who are getting
[01:01:00.680 --> 01:01:08.200]   massively, there was a good article in the Sunday Times by Charles Duhigg, the case against Google.
[01:01:08.200 --> 01:01:15.560]   It's so big now that it needs to be regulated. That you just can't let a company become that
[01:01:15.560 --> 01:01:21.160]   dominant. I think more. I'm more worried about Amazon. I was just going to say, I'm surprised that
[01:01:21.160 --> 01:01:26.360]   Amazon is more clever. That's why I'm more worried. Jeff's smart.
[01:01:26.360 --> 01:01:33.880]   He's smart. Oh, yeah. And if you stop and think about, they don't currently butt up against any
[01:01:33.880 --> 01:01:41.800]   antitrust legislation or laws because they're totally diversified. But Amazon is a health
[01:01:41.800 --> 01:01:47.640]   care company. Amazon is a credit card company. Although interesting, somebody in the chatroom
[01:01:47.640 --> 01:01:51.160]   we were talking about, the Samsung at the beginning of the show said, if you go to Korea,
[01:01:51.160 --> 01:01:58.520]   Samsung is everywhere. The 30% of the GDP, he said, my friends in Korea have a Samsung health plan.
[01:01:58.520 --> 01:02:03.720]   They go to a hospital run by Samsung. The school is sponsored by Samsung.
[01:02:03.720 --> 01:02:12.600]   They're a refrigerator, a washer, and dryer Samsung. This is certainly the case in other
[01:02:12.600 --> 01:02:17.480]   countries that these corporations are getting more and more dominant. In fact, remember,
[01:02:17.480 --> 01:02:23.080]   Korea prosecuted Samsung's CEO for bribery. Put them in jail. They just let them out.
[01:02:23.080 --> 01:02:33.320]   You know, I worry that we're headed in that direction. We're headed towards a
[01:02:33.320 --> 01:02:38.680]   technocracy. I really do. Here is something cheerful. See, we got in the depressing thing again.
[01:02:38.680 --> 01:02:47.400]   Let's talk about gadgets. 150 new emojis. Yay. This is from emoji. There's a party face.
[01:02:47.400 --> 01:02:54.680]   A woozy face. Wait a minute. Wow. There's a lot. There's a lotion bottle.
[01:02:54.680 --> 01:02:59.880]   A lotion bottle. I don't know what that's for. I think that goes with the old cosmetics kit.
[01:02:59.880 --> 01:03:08.280]   Cold face and hot face. Ginger's. You finally have an emoji for you as do curly-haired people.
[01:03:08.280 --> 01:03:17.080]   And old folks like me have white-haired people. And bald people. There's a picture for an emoji
[01:03:17.080 --> 01:03:22.920]   for you. There's a super villain. I don't know. Is that fair to say the super villain,
[01:03:22.920 --> 01:03:27.800]   the superhero look a lot alike. I don't really know. There is a completely weird disembodied leg.
[01:03:28.600 --> 01:03:34.280]   Everybody needs a leg. When do you need to send somebody a leg?
[01:03:34.280 --> 01:03:40.600]   What are you shortening in your sense? It's a major award.
[01:03:40.600 --> 01:03:46.840]   I run a lot. I could see myself texting my husband. This is what I need to.
[01:03:46.840 --> 01:03:54.360]   This is why I wonder if this is what I need to do. There's also feet disembodied.
[01:03:54.360 --> 01:04:03.960]   Now, there's a weird one. Tooth and bone. Bone. Okay. These, by the way, this is an arduous
[01:04:03.960 --> 01:04:09.640]   process they have to go through. The emojis are determined by the Unicode Committee. In fact,
[01:04:09.640 --> 01:04:15.720]   Jerry McGrewsh, who founded Wikipedia, this video is from Emojipedia, has been on our show many times.
[01:04:15.720 --> 01:04:22.200]   He is part of the emoji committee. They receive proposals for emoji from all over the world. They
[01:04:22.200 --> 01:04:27.160]   have to then narrow them down and approve them. They're looking for emojis that are of universal
[01:04:27.160 --> 01:04:34.680]   international import and lab coat and goggles. Sure. Hiking boot and a flat.
[01:04:34.680 --> 01:04:45.960]   That is a paramecium. There's a mosquito and a parrot, a badger and a peacock, a swan,
[01:04:45.960 --> 01:04:50.840]   and a strange black and white raccoon. Is that a Japanese raccoon, a Chinese raccoon?
[01:04:51.720 --> 01:04:56.360]   I've never seen a raccoon look like that. By the way, these renderings are not official.
[01:04:56.360 --> 01:05:02.040]   This is just what the Unicode Committee proposes as a reference, but everybody,
[01:05:02.040 --> 01:05:06.680]   Twitter, Google, Facebook, they all do Apple all design their own and they won't look just like
[01:05:06.680 --> 01:05:12.200]   this. Kangaroo. Okay. Now, Andy Inaco on our Mac Break Weekly show is it. He's in New England.
[01:05:12.200 --> 01:05:18.680]   He had to point out that lobster has been cooked. It's not that is not a healthy lobster. That is
[01:05:18.680 --> 01:05:24.600]   a cooked lobster because it's bright red. Maybe we need a green lobster. Let's see. We'll see if
[01:05:24.600 --> 01:05:31.720]   Apple does one that's not cooked. I have a mouth. The leg makes it strange reappearance.
[01:05:31.720 --> 01:05:38.200]   Now it's a hip. I guess a lot of us have got long luxurious legs. It's a very elegant hippo.
[01:05:38.200 --> 01:05:46.680]   As is the llama. There's food, leafy greens, cupcakes, and bagels. There's a moon cake.
[01:05:47.800 --> 01:05:54.680]   What is that? I don't know. I couldn't tell you that. Yeah, that's a there's a holiday where you
[01:05:54.680 --> 01:06:02.200]   eat them. It could just be a pie. But I think that it's an egg in the middle or something. There's
[01:06:02.200 --> 01:06:06.600]   something there going on there. It's like Mardi Gras King cake. Yeah, it could be like a king cake.
[01:06:06.600 --> 01:06:16.200]   Salt. Everybody needs salt, a compass and luggage, bricks, an escape board, lacrosse and a flying disc,
[01:06:16.200 --> 01:06:24.520]   aka we should point out Frisbee. There's a Jigsaw puzzle chest pawn softball magnet toolbox.
[01:06:24.520 --> 01:06:29.800]   Did they post a list of the rejects? Yeah, sometimes Jeremy will talk about them.
[01:06:29.800 --> 01:06:34.040]   There have been some great. I'm so curious. Yeah. Let me see if they if there's there's your
[01:06:34.040 --> 01:06:38.680]   lotion bottle by the way. Yeah. Yeah. Yeah. Abacus and fire extinguishers. Let's see what the rejects see.
[01:06:39.960 --> 01:06:46.760]   This is these are these are really what matter is these textual descriptions because that so in
[01:06:46.760 --> 01:06:52.040]   fact, they do call it a super villain and a superhero. So maybe they'll be a better distinguish
[01:06:52.040 --> 01:06:57.960]   distinguishing characteristics. Safety pins, sponge, infinity, pirate flag.
[01:06:57.960 --> 01:07:04.440]   Let me see if he mentions the rejects when when Jeremy comes on, he always talks about the
[01:07:04.440 --> 01:07:11.480]   rejects. But I don't see that on the on the article in a MoGP. Yeah. But yes, just to talk about
[01:07:11.480 --> 01:07:17.080]   something positive that's working. I think our standards committees are a pretty good example of
[01:07:17.080 --> 01:07:22.200]   something that works in technology, like the USB committee and the emoji committee, Unicode
[01:07:22.200 --> 01:07:27.320]   committee. Yeah, I think it's because you stack it with people. They're vested experts in a field
[01:07:27.320 --> 01:07:32.920]   that are not politicians. I think that's why we have such a result with that. If you talk to the
[01:07:32.920 --> 01:07:37.160]   people who serve on those committees, they will say, Oh my God, they're broken. It's a fight.
[01:07:37.160 --> 01:07:42.360]   There's big companies come in. But I think you're right. I think that while there have been examples
[01:07:42.360 --> 01:07:49.160]   of standards not being influenced by the big companies, I think in general, it kind of does
[01:07:49.160 --> 01:07:52.840]   work. It certainly works a lot better than government. So maybe that is a model for the future.
[01:07:52.840 --> 01:08:02.200]   I don't know. Here's a little word of warning for people who sell their Apple equipment.
[01:08:03.000 --> 01:08:10.200]   Brendan Mulligan, who was an entrepreneur and a designer, he says, I sold an old iMac to somebody
[01:08:10.200 --> 01:08:19.480]   and I had access to his location for over three years. This doesn't happen if the new owner signs
[01:08:19.480 --> 01:08:26.680]   into their iCloud account, but apparently the new owner did not. Now he had erased the computer.
[01:08:26.680 --> 01:08:33.000]   He'd installed a fresh copy of OS 10. He says, the mistake I made was I didn't sign out of iCloud
[01:08:33.000 --> 01:08:38.760]   of iMac before erasing. I thought erasing it would do it, but it doesn't. So you have to remove.
[01:08:38.760 --> 01:08:45.080]   Oh, wow. So if you've erased it. So are you sort of been like purgatory? If you know,
[01:08:45.080 --> 01:08:51.160]   it just means that that Mac will show up. And the worst thing is maybe this is maybe we're
[01:08:51.160 --> 01:08:55.960]   telling people something they shouldn't know, but you could, he said points that out with two clicks
[01:08:55.960 --> 01:09:00.520]   at any point I could have shut down the user's computer, wiped it clean. They couldn't stop it.
[01:09:00.520 --> 01:09:05.480]   They'd have no control. So there's a message to both sides. If you're selling a Mac
[01:09:05.480 --> 01:09:12.840]   log, not don't merely erase it. Remove it from your Find My iPhone from your iCloud account,
[01:09:12.840 --> 01:09:18.920]   which you can do before you erase it. And then if you buy a Mac or an iPhone for somebody,
[01:09:18.920 --> 01:09:23.560]   make sure you log it into your iCloud account so that the previous owner can't lock you out of
[01:09:23.560 --> 01:09:31.800]   your own stuff. Wow. So I guess my question is if you erase everything on your computer,
[01:09:31.800 --> 01:09:36.200]   right? And you don't sign out of iCloud. Is there no way? You could go to another device,
[01:09:36.200 --> 01:09:43.320]   I think, and remove it. I think. Yeah. It's the way that they've updated this multiple device sign-in.
[01:09:43.320 --> 01:09:48.360]   It's crazy, isn't it? It's crazy. And it's also challenging. Like if you don't, you know,
[01:09:48.360 --> 01:09:52.920]   and the, I just had this problem with my iPad the other day, I was trying to sign on and
[01:09:53.240 --> 01:10:01.000]   it's really, it's really confusing. And I'm a pretty tech-savvy person. I can't imagine how
[01:10:01.000 --> 01:10:06.600]   it's, you know, confounding would be. Yeah. And what if you don't have another Apple device?
[01:10:06.600 --> 01:10:10.680]   Or what if somebody stole your backpack and they have all your devices? Like, doesn't that,
[01:10:10.680 --> 01:10:14.360]   you know, like, isn't it meant to prevent you theft of information? Good point.
[01:10:14.360 --> 01:10:19.560]   They have screens out there of what iOS was like, like for iOS one, two, and three, and like
[01:10:19.560 --> 01:10:24.680]   compare it to all the setup screens today. And it's like, you got through so many modal
[01:10:24.680 --> 01:10:30.680]   dialogues these days. And I just, you know, like my measure of what I feel like normal people can
[01:10:30.680 --> 01:10:36.360]   get is like watching my husband. He just completely tunes it out. We bought him a new machine and just,
[01:10:36.360 --> 01:10:40.840]   he just doesn't want to deal with it. So it's, I feel like it's gotten so complicated. It's very
[01:10:40.840 --> 01:10:47.240]   easy for me to understand why someone would buy a Mac and would just, it's do later, do later.
[01:10:47.240 --> 01:10:53.320]   Yeah. Great caller, a guy who was in his 80s called the radio show today. He says, I'm a
[01:10:53.320 --> 01:10:59.000]   technical writer. I've been writing about technology for 40 years. I have a Windows computer. I've
[01:10:59.000 --> 01:11:04.040]   been using Windows for 30 years. I have a Surface Pro right here. Somebody gave me an iPad. I can't
[01:11:04.040 --> 01:11:09.400]   figure out how the hell to use it. What do those pictures mean? What am I? And I said, oh, easy.
[01:11:09.400 --> 01:11:13.960]   Give it to a two year old. They'll show you. They intuitively know how to use it. The problem is,
[01:11:13.960 --> 01:11:18.680]   we have to unlearn everything we ever knew. He said, is there a manual for this thing?
[01:11:18.680 --> 01:11:22.760]   And actually I found out, thank you to the chat room, there is a manual. It doesn't come on the
[01:11:22.760 --> 01:11:28.920]   iPad, but you can go into iBooks and download Apple's 100, it's a 300 page manual on how to use
[01:11:28.920 --> 01:11:37.640]   an iPad for old people. Because kids, I swear, I've seen nonverbal kids launch Netflix and launch
[01:11:37.640 --> 01:11:47.240]   Finneas and Furb completely autonomously. Here's an Apple repair center that apparently doesn't
[01:11:47.240 --> 01:11:55.080]   really know how to use Apple technology. They've accidentally called 911 about 1600 times in
[01:11:55.080 --> 01:12:01.000]   four months. And no one knows why. This is Elk Grove, California. The Elk Grove Police Department
[01:12:01.000 --> 01:12:05.400]   said, yeah, we get about 20 calls a day from the Apple repair center. There's nobody on the other
[01:12:05.400 --> 01:12:10.920]   line. Apparently they're coming from iPhones and Apple watches. I don't know if you know,
[01:12:10.920 --> 01:12:15.320]   but there's an emergency call feature now. If you press both buttons and hold it for a few seconds,
[01:12:15.320 --> 01:12:21.640]   it'll call 911. Apple said, we take this very seriously and we're working closely with local
[01:12:21.640 --> 01:12:29.880]   law enforcement to investigate the cause. I'm sure this doesn't continue. I guess it's a little,
[01:12:29.880 --> 01:12:32.360]   I don't know, there's something they're doing in the recycling center.
[01:12:33.000 --> 01:12:36.360]   Maybe it's the way that they're trying to take the case, like, off the case.
[01:12:36.360 --> 01:12:40.120]   Yeah, it's got to be right. It's but it's an Apple facility. That's the funny thing.
[01:12:40.120 --> 01:12:45.640]   Wait, they're dealing with like broken iPhones. It's like the digitizers all randomly
[01:12:45.640 --> 01:12:48.120]   pressing. Yeah, maybe that's what it's like dragging across the screen.
[01:12:48.120 --> 01:12:54.200]   Yeah. I mean, I think really they should probably have a Faraday cage around the building or something.
[01:12:54.200 --> 01:13:01.080]   They need to do something 30. So apparently this is not unusual.
[01:13:02.760 --> 01:13:07.480]   It's a problem, of course, rail growth because their lines are constantly being peppered by
[01:13:07.480 --> 01:13:13.640]   fake 911 calls. But I didn't know this, but in 2011, a Google research study
[01:13:13.640 --> 01:13:20.520]   looked at 911 calls in San Francisco and found this is seven years ago that 30% of the 911 calls
[01:13:20.520 --> 01:13:27.720]   were accidental from wireless phones. But 37% from wired phones were accidental.
[01:13:28.520 --> 01:13:32.840]   That was the part that I thought was accidentally calling 911 for your landline.
[01:13:32.840 --> 01:13:38.200]   Same two year old is trying to launch Phineas and Fur by the landline. Come on, I know you,
[01:13:38.200 --> 01:13:49.640]   when now what's weird is they can't just hang up if nobody's in the line because it could be
[01:13:49.640 --> 01:13:55.160]   somebody who's, you know, tied up. And so when the dispatcher receives those calls, they they
[01:13:56.360 --> 01:14:01.080]   listen for a while. When they only hear an open line, they have to call back and leave a voicemail.
[01:14:01.080 --> 01:14:08.360]   I mean, it's it's it's not an easy problem to solve. All right, that was our happy segment.
[01:14:08.360 --> 01:14:18.360]   Mogees, 911 calls. I don't want to go into the case against Google, but do you have an opinion on
[01:14:18.360 --> 01:14:25.400]   that on that story? Anybody have a thought on this? We talked about it a lot on our show on
[01:14:25.400 --> 01:14:29.800]   Wednesday this weekend Google critics say the search giant is squelching competition before it
[01:14:29.800 --> 01:14:37.960]   begins. They use as an example of vertical search shopping engine called, what was it? Found me,
[01:14:37.960 --> 01:14:44.040]   something like that. I'll get the name right. And this couple, Adam and Chavon Raff, who have
[01:14:44.040 --> 01:14:49.480]   basically were put out of business by Google, they created a vertical search shopping search.
[01:14:50.280 --> 01:14:57.720]   And when they went live with it, they disappeared from the Google search page because Google
[01:14:57.720 --> 01:15:03.960]   in their algorithm said, well, that you're just generating a link farm, basically, you're
[01:15:03.960 --> 01:15:09.240]   generating links to shopping sites. And we don't think that's what users want. And essentially,
[01:15:09.240 --> 01:15:16.120]   they went out of business. They've been suing. This is the problem. Google is a commercial entity.
[01:15:16.920 --> 01:15:24.360]   It's not the de facto. They're not archivists, right? So they don't have any obligation
[01:15:24.360 --> 01:15:30.600]   to show every single page and index it so that it can be listed publicly.
[01:15:30.600 --> 01:15:37.480]   No, they're private enterprise. Now, I guess if you're, you could go after them as the EU has
[01:15:37.480 --> 01:15:42.360]   for favoring their own shopping against a competitor, but Google's explanation actually
[01:15:42.360 --> 01:15:51.640]   is kind of credible, which is, well, we don't surface sites that don't have any outbound links,
[01:15:51.640 --> 01:15:58.280]   or inbound links that just have a lot of outbound links because we consider them spam.
[01:15:58.280 --> 01:16:04.280]   Of course, as they point out in this article, that's all Google is. It's a bunch of, there's no
[01:16:04.280 --> 01:16:07.800]   any links to a Google search result. It's all outbound links.
[01:16:09.000 --> 01:16:14.520]   Do you know what gives me a lot of pause is I think if you look at the anti, the
[01:16:14.520 --> 01:16:18.920]   legislation against Microsoft in the 90s, basically ruling them in a monopoly.
[01:16:18.920 --> 01:16:25.480]   I think now many years later, if you look at that, I think it's a very mixed bag if that
[01:16:25.480 --> 01:16:29.720]   particular legislation did anything. Well, you should read this article because, again,
[01:16:29.720 --> 01:16:34.840]   they talk about this. And they say, Google wouldn't exist if Microsoft hadn't been slapped on the
[01:16:34.840 --> 01:16:41.400]   wrist by the DOJ. A process that took 10 years cost a lot of money. A lot of people said it went
[01:16:41.400 --> 01:16:45.880]   so slow that by the time the DOJ won that case, it didn't matter anymore.
[01:16:45.880 --> 01:16:53.800]   I guess it's like when I look at Facebook, I see issues that Facebook has had on media. It's like,
[01:16:53.800 --> 01:17:00.440]   I understand that argument. That is an anti competition case I would be very interested in
[01:17:00.440 --> 01:17:07.560]   looking at. For Google, I can see it. It's any of these, when you're talking about getting
[01:17:07.560 --> 01:17:13.640]   government involved in slicing up a business as important as Google, it gives me pause.
[01:17:13.640 --> 01:17:17.480]   So let's not say I don't think we should do it. I just think it's something you should think
[01:17:17.480 --> 01:17:25.000]   very, very deeply about. Yeah, I think what worries me here. So I think it's totally valid for Google
[01:17:25.000 --> 01:17:33.000]   to say that this company was just sending links outward. And therefore, maybe it wasn't the best
[01:17:33.000 --> 01:17:38.920]   website. But what's a little bit scary is that Google can, by flipping a switch, Google can
[01:17:38.920 --> 01:17:43.720]   either create a multi-million dollar business. This company had solved a problem that people
[01:17:43.720 --> 01:17:48.440]   legitimately wanted solved. And it could have grown into a very successful business. But by turning
[01:17:48.440 --> 01:17:55.320]   the switch off, Google was able to eliminate a competitor. And so it does tie into, I think,
[01:17:55.320 --> 01:18:01.560]   the legislation against Microsoft that you're talking about, because you could have made the
[01:18:01.560 --> 01:18:07.800]   same case for Internet Explorer. I think Microsoft had every right to put Internet Explorer on the
[01:18:07.800 --> 01:18:17.080]   machines that were running Windows. But luckily, we were all given better choices. We were given
[01:18:17.080 --> 01:18:22.120]   Netscape. We were given Firefox. We were given eventually Chrome. And the reason that those
[01:18:22.120 --> 01:18:27.880]   things were able to take off is because Microsoft got slapped in the wrist for this Internet Explorer
[01:18:27.880 --> 01:18:33.080]   thing. And what's interesting is, I don't know if it was in this New York Times article or a separate
[01:18:33.080 --> 01:18:38.840]   story that I read this week. But I think a lot of the employees that were working at Microsoft
[01:18:38.840 --> 01:18:45.640]   around that time said that there was, it did have a residual effect. After they were slapped on
[01:18:45.640 --> 01:18:51.400]   the wrist by the government, executives were thinking twice before making decisions about
[01:18:51.400 --> 01:18:54.040]   some of their business. That was in this article. Yep.
[01:18:54.040 --> 01:18:59.640]   Yeah, because they didn't want to go through that or deal again, because it was such a,
[01:18:59.640 --> 01:19:04.680]   it painted the company in such a negative light. And it was such a headache for the higher ups that
[01:19:04.680 --> 01:19:12.200]   that there was careful consideration around decisions that were made after the fact. And so I think
[01:19:12.200 --> 01:19:17.880]   with, you're seeing that right now with Facebook and the way that it handles news and also the way
[01:19:17.880 --> 01:19:25.080]   that it interacts with Washington, I think right now, from what I've heard from people inside Facebook,
[01:19:25.080 --> 01:19:28.600]   like the number one thing that Facebook doesn't want is to be regulated. And so,
[01:19:28.600 --> 01:19:37.800]   so there's a lot of special consideration around things like deciding whether to flag fake news
[01:19:37.800 --> 01:19:44.360]   or deciding whether to hire more curators to curb fake news. There's really, really careful
[01:19:44.360 --> 01:19:50.440]   consideration and long debates about any tiny little maneuver that has to deal with either the
[01:19:50.440 --> 01:19:59.640]   news or politics in the US and any of the sensitive content on Facebook right now because they're
[01:19:59.640 --> 01:20:05.320]   basically on high alert because the one thing that they don't want is exactly what Microsoft
[01:20:05.320 --> 01:20:11.800]   wants you in the 90s. So that's beneficial. The article says that the United States courts have
[01:20:11.800 --> 01:20:18.440]   increasingly held that the government has to show consumer harm to win in a case like this.
[01:20:18.440 --> 01:20:23.480]   And that's part of the problems consumers are not complaining. The people who complain
[01:20:23.480 --> 01:20:27.240]   against Google or other companies, that's why it's succeeded in the European Union where
[01:20:27.240 --> 01:20:33.400]   competition is considered more important than that or a more important way to protect consumers
[01:20:33.400 --> 01:20:37.480]   is to preserve innovation and competition between companies. So good articles, an interesting
[01:20:37.480 --> 01:20:42.360]   article. It's a difficult challenge. It's clear though, as Google and Facebook and Amazon get bigger
[01:20:42.360 --> 01:20:46.920]   and bigger, the challenges are going to get bigger and bigger. And this is another one of those
[01:20:46.920 --> 01:20:53.000]   difficult to legislate. Let's take a break and talk some more. Great panel here, Amy Webb.
[01:20:53.000 --> 01:21:00.280]   She's a futurist, created the Future Today Institute publisher of the annual FTI trend report
[01:21:00.280 --> 01:21:04.680]   just out in her book, which is really worth working on. I'll help you think about the
[01:21:04.680 --> 01:21:08.920]   future differently. The signals are talking why today's fringes tomorrow's mainstream,
[01:21:08.920 --> 01:21:14.600]   Brianna Wu. She's running for Congress in the Massachusetts 8th District primaries coming up
[01:21:14.600 --> 01:21:20.600]   September. Brianna's working hard. She's raising money. You can go to Brianna Wu 2018.com
[01:21:20.600 --> 01:21:23.960]   to help out if you think we need more smart people like Brianna and Congress. I do.
[01:21:25.480 --> 01:21:31.240]   And of course, from Mashable, Michael Nunez, who has not yet announced whether he's running,
[01:21:31.240 --> 01:21:38.120]   but he does believe in victory. Give it to us. I showed today brought to you by speaking of legal
[01:21:38.120 --> 01:21:43.640]   matters, legal zoom, small business hot topic this year. We talk a lot about the big businesses
[01:21:43.640 --> 01:21:48.600]   and the power, but this is National Small Business Month. Actually, as a small business,
[01:21:48.600 --> 01:21:54.280]   I kind of appreciate legal zoom. When I was starting out and I consulted my friends who had started
[01:21:54.280 --> 01:21:59.320]   companies, they said, "You need to make an LLC to protect yourself." And I went to legal zoom
[01:21:59.320 --> 01:22:03.240]   and they made it easy and affordable, even though I couldn't, I was brand new. I couldn't afford a
[01:22:03.240 --> 01:22:08.520]   law firm. I got the advice and the information and the forms I needed from legal zoom. Whether
[01:22:08.520 --> 01:22:13.560]   you're already in business or just starting out 2018 is going to be an exciting year. The new tax
[01:22:13.560 --> 01:22:17.960]   law is perhaps the most significant change for business owners in the last 30 years. A lot of us
[01:22:17.960 --> 01:22:23.560]   looking at, "Well, should I incorporate? Should I be at an LLC, a pass-through entity? Legal zoom
[01:22:23.560 --> 01:22:27.160]   can help you understand what it means for you." They're not a law firm. I want to make this clear,
[01:22:27.160 --> 01:22:32.840]   but they do hook you up with independent tax professionals and attorneys. So you can get your
[01:22:32.840 --> 01:22:38.760]   questions answered about the new tax bill, about incorporation, about business matters. Legal zoom
[01:22:38.760 --> 01:22:43.800]   understands you need to tap into the right resources to run a successful business. So they have been
[01:22:43.800 --> 01:22:48.600]   around for 16 years. They helped me a lot in the beginning and they have taken what they know
[01:22:49.560 --> 01:22:54.120]   to provide business owners with the tools they need to start and run their businesses the right
[01:22:54.120 --> 01:22:58.440]   way. They have a white glove service for business owners. Everything you need to run your business.
[01:22:58.440 --> 01:23:04.360]   I still get the email from a legal zoom reminding me it's time to do this or that, which is fantastic.
[01:23:04.360 --> 01:23:09.880]   I don't have to worry about compliance or when things are due. They let me know.
[01:23:09.880 --> 01:23:16.040]   The services include tax consultation, intellectual property, payroll, business compliance. They do
[01:23:16.040 --> 01:23:20.680]   it all. Over the next few weeks, because it's National Small Business Month, we're going to talk about
[01:23:20.680 --> 01:23:25.960]   different ways Legal Zoom can help you this month. So if you're in a small business or you want to
[01:23:25.960 --> 01:23:30.840]   start one, and who doesn't really want to have their own business, stay tuned. For now, check out
[01:23:30.840 --> 01:23:36.840]   LegalZoom.com today and get special savings when you enter the promo code TWIT in the referral box
[01:23:36.840 --> 01:23:43.880]   at checkout. LegalZoomerLife meets legal. LegalZoom.com. If you see something you want there,
[01:23:44.440 --> 01:23:47.640]   make sure you use the offer code TWIT so they know you heard it here and you get a special
[01:23:47.640 --> 01:23:55.480]   $10 off LegalZoom.com. I'm always forever grateful to LegalZoom. We trademarked the TWIT logo,
[01:23:55.480 --> 01:24:05.160]   the TWIT name. That was back in 2004, 2005. They were, it was early on and I'm very grateful to them.
[01:24:05.160 --> 01:24:10.520]   I was just having to laugh at those prices. That's what one second of calling your
[01:24:10.520 --> 01:24:18.200]   phone call. I know. I know. Now we have, of course, we have a law firm and every time I get on a phone
[01:24:18.200 --> 01:24:23.160]   with lawyers and there's three of them on the phone, I can't help it. I'm thinking that cost
[01:24:23.160 --> 01:24:29.240]   $12. That cost $12. They're saying hello and you're just like, let's move on.
[01:24:29.240 --> 01:24:32.920]   Oh, no, they do. They say, how was your day? And I said, don't I don't want to talk about that?
[01:24:32.920 --> 01:24:37.240]   And they bill in 15 minute increments. Oh, yeah. I don't want to talk about that.
[01:24:39.720 --> 01:24:46.840]   No, but we have, I really love our attorneys. They're really great. But I kind of wish I were
[01:24:46.840 --> 01:24:57.240]   back in the LegalZoom days, I got to say. Let's see. Let's talk about machine learning and ad buys.
[01:24:57.240 --> 01:25:04.520]   Google has announced something called auto ads. They're not ads for automobiles, but they're
[01:25:04.520 --> 01:25:11.960]   ads that will use artificial intelligence, they say, to help you with placement and
[01:25:11.960 --> 01:25:18.440]   make choices for monetization. I feel like this is the beginning of what we're going to see big time.
[01:25:18.440 --> 01:25:26.280]   And it's people still paying attention to ads. I'd be curious. I just be curious to find out if
[01:25:26.280 --> 01:25:32.520]   we've got, I hope so. You're not talking about my ads. You're talking about banner ads, right?
[01:25:33.720 --> 01:25:39.160]   No, because in the audio world, people just listen, right? I'm talking about the
[01:25:39.160 --> 01:25:42.840]   I don't see banner ads. And I think there's this whole problem with
[01:25:42.840 --> 01:25:47.880]   ad blockers. Everybody's running by ad blockers. Even Chrome now has ad blockers.
[01:25:47.880 --> 01:25:55.880]   Michael, I can say that like on Twitter and Facebook ads, they do work for a congressional
[01:25:55.880 --> 01:26:00.920]   campaign, at least like people do pay attention to those. I buy a lot of businesses that would
[01:26:00.920 --> 01:26:06.520]   say that they work for them as well. Like Daniel, Wellington, all birds. And there are multi-million
[01:26:06.520 --> 01:26:11.960]   dollar companies have been made in the past. I have all birds. I have all birds. Yep. They
[01:26:11.960 --> 01:26:17.480]   worked for me. I actually hate Instagram because they put an ad every what is it, eighth post.
[01:26:17.480 --> 01:26:21.080]   And I have bought so much crap. They're very effective.
[01:26:21.080 --> 01:26:27.080]   But I'm talking about Google, right? Because Google's not selling ads on Facebook.
[01:26:27.080 --> 01:26:31.480]   They're doing banner ads, right? Right. So I'm saying people like Michael, do you have,
[01:26:31.480 --> 01:26:36.120]   you probably can't tell us very much, but like from Mashable.
[01:26:36.120 --> 01:26:37.800]   I have Google ads on Mashable, I'm sure.
[01:26:37.800 --> 01:26:41.800]   Yeah, I think I mean, I don't obviously.
[01:26:41.800 --> 01:26:47.400]   You don't know because this is good because they separate that away from you as they should.
[01:26:47.400 --> 01:26:51.320]   Yeah, that's right. But I'm sure we do. I mean, I imagine that we do.
[01:26:51.320 --> 01:26:57.160]   In a sense that people like are like, there's avoidance, you know, like, I don't even,
[01:26:57.160 --> 01:27:02.200]   I know that there are ads on my screen. I have ad blockers up, but I just, I don't even,
[01:27:02.200 --> 01:27:05.320]   they don't register anymore, you know. I know.
[01:27:05.320 --> 01:27:09.800]   I know. Yeah. For me, that's very true. Except in the case of like, you know, when they're inserted
[01:27:09.800 --> 01:27:14.040]   into feeds that I'm looking at, they're very effective. I've never really clicked on Google
[01:27:14.040 --> 01:27:18.680]   AdSense ads to the end. Yeah. My suspicion is this is exactly why Google's doing this.
[01:27:19.320 --> 01:27:22.440]   Because they want to make ads more effective. They claim
[01:27:22.440 --> 01:27:28.280]   that they are seeing with this auto, you know, this auto-official intelligence,
[01:27:28.280 --> 01:27:31.640]   auto ads placement that they're seeing average revenue lifted 10%
[01:27:31.640 --> 01:27:38.360]   with revenue increases ranging from 5 to 15%. I would guess that's exactly why because they're,
[01:27:38.360 --> 01:27:43.480]   you know, they're using things like heat maps where people look and they're really starting to
[01:27:43.480 --> 01:27:48.360]   get smart about ad placement. I guess it makes sense too. Like I think about how much time for
[01:27:48.360 --> 01:27:54.440]   us on a campaign, we do a versus B testing and different keyword testing to, you know,
[01:27:54.440 --> 01:28:00.200]   with like, and we do all kinds of, you know, advertising like mailing lists or buying certain
[01:28:00.200 --> 01:28:06.920]   kinds of ads. And either as a candidate, get in and get really deep with like, what is my message
[01:28:06.920 --> 01:28:11.400]   I'm trying to do? Who, who am I trying to target? Or you have to hire someone that's very,
[01:28:11.400 --> 01:28:16.600]   very expensive to go like work that out for you and find that market segmentation. So,
[01:28:17.560 --> 01:28:22.760]   like I read this and I'm like, I would absolutely use something like this if it was effective.
[01:28:22.760 --> 01:28:29.240]   So yeah, I think it's a, it's certainly like removing the human element from that,
[01:28:29.240 --> 01:28:35.400]   which I like. So I'm actually curious. You, you, do you have consultants who help you with this?
[01:28:35.400 --> 01:28:43.400]   So to be honest, when big Democrat party consultants call me, I just, I think it's a dark road to
[01:28:43.400 --> 01:28:48.920]   go down. You know, when I run into people, they're passionate about say, cyber security and want
[01:28:48.920 --> 01:28:54.360]   to get involved with our campaign, we do hire people like that that have experience with marketing.
[01:28:54.360 --> 01:28:56.840]   But you know, as far as like, you know, like that,
[01:28:56.840 --> 01:29:02.840]   Now, if you win the primary, the DNC is going to step up and say, okay, we'll take it from here,
[01:29:02.840 --> 01:29:08.040]   aren't they? I guess I've, I've upset a lot of people at the DNC and running because, you know,
[01:29:08.040 --> 01:29:13.560]   I've, I've, I've, I've critiqued our party quite a bit, like FISA. I'm very upset about that. So
[01:29:13.560 --> 01:29:19.880]   we'll, we'll see. We'll see. They may know, like if you win the primary, they got to go with you.
[01:29:19.880 --> 01:29:27.640]   Yeah, I hope so. So do they have to or can they know they don't have to do anything? Right.
[01:29:27.640 --> 01:29:32.040]   Right. I mean, I imagine they would. It's not like I'm going to ask Bernie Sanders.
[01:29:32.040 --> 01:29:36.680]   See what, see what happened with Bernie. Yeah. I think that's really fair. Yeah.
[01:29:37.480 --> 01:29:40.920]   Fine. Today is on an on endorsed, right? Yeah.
[01:29:40.920 --> 01:29:44.600]   Did that just happen hours ago? Yeah. Yeah. So very happy.
[01:29:44.600 --> 01:29:48.200]   I'm just curious because there, of course, there was a big article in Wired this week about
[01:29:48.200 --> 01:29:56.280]   saying, in effect, forget Russian bots. That wasn't Trump, the Trump campaign used Facebook
[01:29:56.280 --> 01:30:01.400]   very effectively. They won because they used it very effectively. It's actually a fascinating
[01:30:01.400 --> 01:30:06.600]   article. And one of the things we also written by the guy who created their advertising platform.
[01:30:06.600 --> 01:30:12.520]   I think that's a really. So that story. Yeah. So he was the former head of advertising
[01:30:12.520 --> 01:30:19.160]   Facebook. He wrote the what? Chaos on on Tony. Oh, he's a jerk. Well, I mean,
[01:30:19.160 --> 01:30:25.640]   us monkeys is the worst. Oh, it's it's a fine. You know, it's a fine book, but I think it's not.
[01:30:25.640 --> 01:30:34.520]   Build the platform champion. Okay. Okay. Wired. What? This is not editorial. This is a, this is an
[01:30:34.520 --> 01:30:40.840]   opinion piece. By the way, an opinion. No, I think they call it editorial. And by the way,
[01:30:40.840 --> 01:30:47.400]   somebody pointed out Casey Newton wrote essentially the same story in the last year about how,
[01:30:47.400 --> 01:30:51.320]   you know, the Trump campaign, let me see if I can find it, the Trump campaign very
[01:30:51.320 --> 01:30:58.680]   intent, you know, cleverly. Here it is. How faced, how Trump conquered Facebook without Russian ads.
[01:30:59.400 --> 01:31:04.200]   This is this doesn't say opinion. I mean, it's, you know,
[01:31:04.200 --> 01:31:08.920]   rolled at the bottom and see if it lists his sometimes they they'll put a disclaimer at the
[01:31:08.920 --> 01:31:17.800]   in the tagline at the bottom. I don't even see a bio at the bottom. I don't even see mentioned a
[01:31:17.800 --> 01:31:24.120]   chaos monkeys which made him persona non grata in Silicon Valley for some time. Oh, oh, look,
[01:31:24.120 --> 01:31:29.880]   here. Okay. Look at this page. Can you tell anything about this page? There's a little word here.
[01:31:29.880 --> 01:31:38.040]   Ideas. That is that mean? It's an opinion piece? I don't know. I know the people who, who work on
[01:31:38.040 --> 01:31:47.720]   the editorial side. I, they're, I don't think that this, this is surprising. Well, this is the new
[01:31:47.720 --> 01:31:53.080]   behind a paywall wired, right? Yeah. And I have, I have other issues with Wired this month, but,
[01:31:53.080 --> 01:31:59.240]   but, but, you know, I'm not, I'm not saying that, that, you know, that this story isn't, I just
[01:31:59.240 --> 01:32:03.960]   think that that should be disclosed. I mean, if we're going to say now in the bio is an idea is
[01:32:03.960 --> 01:32:09.160]   contributed for wire than it talks. It does talk about chaos monkeys in the bio on the left. So,
[01:32:09.160 --> 01:32:15.560]   but I didn't, you know, now that I see it, I guess it's an opinion piece, but it sure felt like a
[01:32:15.560 --> 01:32:22.600]   heavily researched piece on how, and I think there's merit in this on how Facebook really does
[01:32:22.600 --> 01:32:29.400]   give some real tools to political campaigns. Do you advertise on Facebook, Brianna, for your
[01:32:29.400 --> 01:32:34.200]   campaign? Yeah, absolutely. In fact, the same tools that Trump used for his election nation
[01:32:34.200 --> 01:32:38.760]   builder, we use that as well. It's, it's a really amazing thing. Like, it's really good. Like,
[01:32:38.760 --> 01:32:43.560]   you type it in, you get all kinds of data about who's following you and what their interests are.
[01:32:43.560 --> 01:32:49.480]   I forget the number. I think it's some ridiculously high number of software engineers have contributed
[01:32:49.480 --> 01:32:55.640]   to my campaign. So, you know, but yeah, there's all that data out there. So you got to use it.
[01:32:55.640 --> 01:33:05.240]   We think of, so we think of right now, the Republicans being further behind and with Obama
[01:33:05.240 --> 01:33:10.680]   was the sort of advent of amazing technology and grassroots and social media and all this.
[01:33:10.680 --> 01:33:16.760]   The Republicans in the early 2000s were light years ahead of the Democrats. And for a hot minute,
[01:33:16.760 --> 01:33:22.280]   I don't know if you guys ever saw this, it was Bush and Cheney when they were running for about
[01:33:22.280 --> 01:33:26.520]   five seconds. That was more than five seconds for a couple of hours. They had this poster
[01:33:26.520 --> 01:33:31.480]   generator on their website, their campaign site, where you could type in, they learned quickly.
[01:33:31.480 --> 01:33:36.920]   Didn't they? Whatever you know, like, you don't let the internet do that. No.
[01:33:36.920 --> 01:33:44.120]   Yes, for Bush, Cheney, 2004. And anything, any poster that got created, it was intended for
[01:33:44.120 --> 01:33:49.720]   you to print out, but they created a gallery. And so they were all being funneled into this gallery.
[01:33:49.720 --> 01:33:55.000]   And it took like less than an hour for, you know, neighborhood moms for Bush, Cheney,
[01:33:55.000 --> 01:34:02.360]   2004 to turn into like the most ridiculous, crazy and the whole thing got taken down.
[01:34:02.360 --> 01:34:10.120]   But I did not know about this. And actually, I was looking, Metafilter had a link to all of them,
[01:34:10.120 --> 01:34:17.800]   but they're all 404s now. They've deleted all of them. And here's an article from Wired,
[01:34:17.800 --> 01:34:22.280]   Bush site unplugged poster tool. That was it. Yeah.
[01:34:22.280 --> 01:34:27.800]   It seemed like a good idea at the time. But they were pretty, you know, we don't,
[01:34:27.800 --> 01:34:33.000]   we don't now in the era of Trump don't think about this, but the Republicans were really far ahead
[01:34:33.000 --> 01:34:36.760]   in a lot of the technology in the early days, like really far ahead. Yeah. Yeah.
[01:34:36.760 --> 01:34:40.520]   Doing some really innovative things. All right. So treat this Wired article really
[01:34:40.520 --> 01:34:45.240]   as an inside look at how it works. Let me find it again. I put here it is.
[01:34:45.240 --> 01:34:51.480]   From the guy who was the product manager for custom audiences, I didn't, I should have read
[01:34:51.480 --> 01:34:56.440]   this more carefully. Thank you, Michael, for pointing this out. And he created this custom
[01:34:56.440 --> 01:35:01.960]   audiences is the tool, one of the tools you said, nation builder, Brianna, that must be another one.
[01:35:03.000 --> 01:35:09.960]   That allows basically allows a advertiser to understand, to buy specific groups, right?
[01:35:09.960 --> 01:35:17.320]   Yes, and target target specific groups. I think that's correct.
[01:35:17.320 --> 01:35:22.360]   There's also a tool that lets you take, I know, because Jason Calicana has told me about this,
[01:35:22.360 --> 01:35:27.880]   take your mailing list and find like-minded people, feed it to Facebook and the Facebook
[01:35:27.880 --> 01:35:32.040]   algorithm says, well, here's people who are on them or not on the mailing list, but are like-minded,
[01:35:32.040 --> 01:35:34.440]   you should be talking to. It's called look-alike audiences.
[01:35:34.440 --> 01:35:41.720]   Yep. We've got that. You got that too. Yeah, we do. It's expensive. It costs a lot of money too.
[01:35:41.720 --> 01:35:46.120]   When you've built a custom audience, can you build look-alike audiences? The most unknown and
[01:35:46.120 --> 01:35:51.080]   poorly understood yet powerful weapon in the Facebook ads arsenal. With a mere mouse click
[01:35:51.080 --> 01:35:55.480]   from our hypothetical campaign manager, Facebook now searches the friends of everyone in the
[01:35:55.480 --> 01:36:00.920]   custom audience trying to find everyone who looks like you using a witch's brew of mutual
[01:36:00.920 --> 01:36:07.160]   engagement. By the way, every day, this tool becomes better, right? Because Facebook gets more data
[01:36:07.160 --> 01:36:12.280]   all the time, and it's just self-reinforcing better and better and better.
[01:36:12.280 --> 01:36:17.560]   It's Facebook going to dominate every campaign from now on in?
[01:36:17.560 --> 01:36:24.280]   Well, I mean, think about this, Leo. I was thinking about this the other day. I was going to go drive
[01:36:24.280 --> 01:36:29.880]   to talk to a group of disabled people here in Massachusetts. This is very important. These
[01:36:29.880 --> 01:36:35.640]   are my constituents. I've got to meet them and listen to them. But it's like three hours to drive
[01:36:35.640 --> 01:36:42.120]   there and back and come to another event. If I'd spent that time on that same Facebook group
[01:36:42.120 --> 01:36:50.920]   disabled people, you get plugged in so much. It is. I think when you know somebody on Facebook
[01:36:50.920 --> 01:36:55.720]   and you're talking to them, I think that's a genuine connection where you see what's important
[01:36:55.720 --> 01:37:02.040]   to them every day. Honestly, I think in this sense, it's more of a net positive than it is a net
[01:37:02.040 --> 01:37:08.360]   negative. When people can genuinely talk to you, I think that's better than trying to go to a
[01:37:08.360 --> 01:37:11.800]   campaign with 500 other people and shake their hand for four seconds.
[01:37:11.800 --> 01:37:19.080]   We are very fortunate that we have a chatroom that is very adept at this thing called the internet.
[01:37:19.640 --> 01:37:27.640]   Thank you, Bleak has provided me with some of the... Oh, nice. Yeah. So chatroom folks. I'm sure
[01:37:27.640 --> 01:37:33.880]   somewhere there is there've got to be screen grabs of some of the other posters. I mean,
[01:37:33.880 --> 01:37:39.000]   you probably can't show. We probably can't show all of them. That one said Bush, Cheney,
[01:37:39.000 --> 01:37:44.360]   2004, stealing elections since, I don't know. Oh, this one's... They were amazing.
[01:37:44.360 --> 01:37:48.680]   This one's dead. Yeah, you don't let people write their own slogans for your campaign.
[01:37:48.680 --> 01:37:53.720]   No, but just a good one. War is Peace. Hate is Love. Bush, Cheney,
[01:37:53.720 --> 01:37:59.160]   2004. Yeah. Of course, coming from the 1984 truth speak.
[01:37:59.160 --> 01:38:04.440]   But from a technical vantage point, just on the back end, it was a pretty... Like,
[01:38:04.440 --> 01:38:09.160]   obviously the implementation didn't get thought through, but the technology was actually pretty
[01:38:09.160 --> 01:38:16.200]   good for back then. Yeah, that's clever. Yeah. Yeah. Love it.
[01:38:16.200 --> 01:38:22.600]   There were some really funny ones. There was a whole series with Jefferson Airplane,
[01:38:22.600 --> 01:38:27.400]   is that a band? Yeah. Featuring like Jefferson Airplane lyrics for some reason.
[01:38:27.400 --> 01:38:34.920]   One side makes you taller. All right. Well, we're going to take a break. This is so much fun,
[01:38:34.920 --> 01:38:38.440]   and I'm so glad we have the smartest people on the internet with us today to protect me
[01:38:39.000 --> 01:38:43.560]   from whatever I might buy next on Facebook. Michael Nunez for Mashable.
[01:38:43.560 --> 01:38:47.800]   Thank you for staying late in the office tonight. Anything more coming in from... Actually,
[01:38:47.800 --> 01:38:52.120]   I want to ask you what other... We talked all about the Samsung. I want to ask you a bit what
[01:38:52.120 --> 01:38:56.200]   other phones are new at Mobile World Congress. What other things are you looking for? Oh, there's
[01:38:56.200 --> 01:39:03.240]   some recommendations we can thank. Yeah. Good. Brianna Wu, BriannaWoo 2018.com. Donate.
[01:39:04.040 --> 01:39:09.320]   And Amy Webb, she'll take that money by great Facebook ads and you'll see a lot more of her in
[01:39:09.320 --> 01:39:14.840]   your feed. The signals are talking why today's fringes tomorrow's mainstream. We had a fun week
[01:39:14.840 --> 01:39:21.400]   on Twitch. We've even made a highlight reel for your Delectation Watch. Previously on Twitch.
[01:39:21.400 --> 01:39:26.840]   Do we switch back now? Yeah. I guess we do. Is it time? Of course, we'll have to wipe things up,
[01:39:26.840 --> 01:39:30.680]   wipe things, you know, the date and stuff. What? And my germs probably too. Probably, but...
[01:39:31.320 --> 01:39:34.920]   That's okay. They swapped iPhone and Google Pixel.
[01:39:34.920 --> 01:39:40.280]   Hario West to Jason. Can I give you a tip on Venmo that I think? Change your privacy settings
[01:39:40.280 --> 01:39:48.440]   because by default, all of your transactions are revealed. Oh my god. Google AI can predict
[01:39:48.440 --> 01:39:53.480]   heart attacks just by looking at your eyes. See, technology's not so bad.
[01:39:53.480 --> 01:39:59.720]   Until your insurance company starts taking pictures of you on the street and denies you
[01:40:00.600 --> 01:40:05.880]   saying sorry, Jeff, you can't get a insurance because we could see from your eyes, you're going to get sick.
[01:40:05.880 --> 01:40:14.200]   That is very panoptic, Leo. Hey, I'm all about the panoptic come in. A bad path on this show.
[01:40:14.200 --> 01:40:19.960]   Twitch Live Specials. It looks S9 and S9 plus.
[01:40:19.960 --> 01:40:25.000]   Gads then, it's sexy. It's got the curves. You know, you got some some nice metallic.
[01:40:25.000 --> 01:40:28.120]   And they're the ports. They're the ports, ladies and gentlemen.
[01:40:28.680 --> 01:40:31.080]   Oh, look at that. What is going on? Dandelion. Wait.
[01:40:31.080 --> 01:40:35.720]   This is AR. They're holding their phones up.
[01:40:35.720 --> 01:40:37.400]   Right. That's probably AR.
[01:40:37.400 --> 01:40:41.800]   Twitch. For help with the technology addiction problem, call 1-800-TWEAT.
[01:40:41.800 --> 01:40:43.320]   I wish that never was.
[01:40:43.320 --> 01:40:47.640]   Hey, thank you. Thank you very much.
[01:40:47.640 --> 01:40:50.360]   Bye.
[01:40:50.360 --> 01:40:53.960]   Thank you. That's it. Hit it in quickly.
[01:40:55.320 --> 01:41:01.080]   I showed you by Zipper Cruder. If you're doing some hiring, Zipper Cruder is going to be the easiest way
[01:41:01.080 --> 01:41:05.240]   to fill that job with exactly the right person. The right person is out there. But how do you reach
[01:41:05.240 --> 01:41:09.800]   them? With all those job boards, all the different places you can go, well, that's the beauty of Zipper
[01:41:09.800 --> 01:41:15.880]   Cruder. One post on Zipper Cruder posts to 100 plus job sites with one click. And even better,
[01:41:15.880 --> 01:41:20.760]   Zipper Cruder has created a smarter way to find the right people. They built a platform
[01:41:20.760 --> 01:41:26.280]   that finds the right job candidates for you. Zipper Cruder, it learns what you're looking for,
[01:41:26.280 --> 01:41:31.480]   identifies people with the right experience, and literally invites them to apply to your job.
[01:41:31.480 --> 01:41:36.920]   These invitations have revolutionized how you find your next hire. Technology, you got to use it.
[01:41:36.920 --> 01:41:42.680]   It's out there. And it can really change what you do. It turns out 80% of employers who post
[01:41:42.680 --> 01:41:49.160]   a job on Zipper Cruder, 80% get a quality candidate through the site in just one day.
[01:41:49.160 --> 01:41:53.720]   It really works. We've used it. It's amazing. As Zipper Cruder doesn't stop there, they even
[01:41:53.720 --> 01:41:57.800]   go through the resumes, the applications, and spotlight the strongest applications. So you
[01:41:57.800 --> 01:42:02.520]   don't have to miss a great match because you're overwhelmed. The right candidates are out there.
[01:42:02.520 --> 01:42:07.400]   They're waiting for you. Zipper Cruder will help you find them. Businesses of all sizes trust
[01:42:07.400 --> 01:42:13.160]   Zipper Cruder for their hiring needs from the Fortune 100 to Little Old Twit. Right now,
[01:42:13.160 --> 01:42:17.640]   you can try Zipper Cruder free. Just go to zippercruder.com/twit.
[01:42:17.640 --> 01:42:21.480]   Zipper Cruder, look at the fields. Look at all the categories. Look at all the companies.
[01:42:21.480 --> 01:42:25.400]   Facebook uses Zipper Cruder. That actually is really a good endorsement.
[01:42:25.400 --> 01:42:34.600]   Facebook uses Zipper Cruder. Zipper Cruder.com/twit. It's the smartest way to hire,
[01:42:34.600 --> 01:42:41.960]   and it's easy and free right now at zippercruder.com/twit. Michael Nunez Mashable
[01:42:42.520 --> 01:42:48.600]   is manning the MWC desk in New York City. You're the only one there.
[01:42:48.600 --> 01:42:54.600]   Yeah, yeah. But all the reports are coming into you, right?
[01:42:54.600 --> 01:42:59.560]   Yeah, I mean, they've slowed down at this point. I think everyone in Barcelona is probably sleeping
[01:42:59.560 --> 01:43:07.000]   right now, but it's been a busy day. A lot of big Android announcements. I feel like a lot of the
[01:43:07.000 --> 01:43:11.720]   news is bigger in Europe and other parts of the world, but it's still really interesting to follow
[01:43:11.720 --> 01:43:16.040]   if you're at all interested in mobile technology. I mean, it's all happening this weekend.
[01:43:16.040 --> 01:43:23.640]   It's funny because Apple, Google, Microsoft, Facebook, they all have their own events.
[01:43:23.640 --> 01:43:28.360]   They all make their announcements at those events. They don't piggyback off of other events.
[01:43:28.360 --> 01:43:31.320]   But for some reason, the rest of the companies, including Samsung,
[01:43:31.320 --> 01:43:34.840]   they like to be at Mobile World Congress. Why is that?
[01:43:34.840 --> 01:43:41.560]   Yeah, I think it's just because there are so many brands and companies in one place at one time,
[01:43:41.560 --> 01:43:47.320]   and it's also just, I think, like you said, a piggyback is a good word. In the case of Samsung,
[01:43:47.320 --> 01:43:54.520]   they're able to piggyback off of some of the excitement and some of the, I don't know, just
[01:43:54.520 --> 01:44:00.200]   the buzz surrounding that event. They've definitely tried to do their own thing in the past. There
[01:44:00.200 --> 01:44:05.720]   was one year where they rented Radio City Music Hall in New York City. Oh God, that was the worst.
[01:44:06.280 --> 01:44:11.560]   Their Galaxy phone at their own event. And so they've tried to do... That was the one where they had a
[01:44:11.560 --> 01:44:17.880]   fake Broadway play. And at one point, they were talking about how you could hover your fingers
[01:44:17.880 --> 01:44:21.640]   over the phone. You didn't have to touch it. And they had, and I'm sad to say, a woman
[01:44:21.640 --> 01:44:26.600]   said, "When my nail polish is wet, I can use my Samsung phone."
[01:44:26.600 --> 01:44:31.400]   Yeah. Oh, cringey. Yeah, really cringey.
[01:44:32.920 --> 01:44:35.640]   Yeah. So I think like following that... They've learned from that.
[01:44:35.640 --> 01:44:43.000]   Generally regarded as kind of a failure, even though it was just... Samsung trying to do what
[01:44:43.000 --> 01:44:47.560]   Apple does really well, which is host its own event and create a lot of buzz around that.
[01:44:47.560 --> 01:44:52.360]   When Samsung tried, it was off the mark. There were moments during that presentation that
[01:44:52.360 --> 01:44:57.400]   were just tone deaf. And so I think they've reverted back to what they had been doing, which was
[01:44:58.680 --> 01:45:05.480]   sort of using other industry events to make these big announcements. So in this case,
[01:45:05.480 --> 01:45:10.120]   I don't even think the Galaxy S9 event was associated directly with Mobile World Congress.
[01:45:10.120 --> 01:45:16.520]   I don't think it was an official event at the show. It just happened to be this Sunday,
[01:45:16.520 --> 01:45:20.360]   or today, I guess, which sort of predates... Which indeed actual...
[01:45:20.360 --> 01:45:22.440]   It just happened to be in Barcelona.
[01:45:22.440 --> 01:45:26.440]   Yeah, exactly. It's the weekend right before the conference case stops.
[01:45:28.280 --> 01:45:29.400]   I don't know if that...
[01:45:29.400 --> 01:45:31.560]   There were other announcements you said. There were other phones there?
[01:45:31.560 --> 01:45:36.280]   There were. Yeah. One of the more interesting things for me at least,
[01:45:36.280 --> 01:45:41.960]   because I'm just kind of weird, is the Nokia... Some of the...
[01:45:41.960 --> 01:45:42.920]   A1, A1, A1, A1.
[01:45:42.920 --> 01:45:45.880]   I'm so excited about the Banae phone.
[01:45:45.880 --> 01:45:46.280]   A1, A1, A1, A1.
[01:45:46.280 --> 01:45:46.680]   Yes.
[01:45:46.680 --> 01:45:48.520]   Matrix coming back.
[01:45:48.520 --> 01:45:49.880]   I can't wait.
[01:45:49.880 --> 01:45:52.520]   I call it the Matrix phone because I'm obsessed with the Matrix.
[01:45:52.520 --> 01:45:53.880]   Total Banae Tricks.
[01:45:53.880 --> 01:45:56.920]   Wait a minute. This is the phone that was in the Matrix?
[01:45:56.920 --> 01:45:57.720]   Yeah.
[01:45:57.720 --> 01:45:57.720]   Yeah.
[01:45:57.720 --> 01:45:59.160]   It's called the Minae phone.
[01:45:59.160 --> 01:45:59.800]   I don't know why.
[01:45:59.800 --> 01:46:01.160]   It's Banana Yellow. That's why.
[01:46:01.160 --> 01:46:01.960]   Well...
[01:46:01.960 --> 01:46:06.120]   This is JK from the Verge showing it off.
[01:46:06.120 --> 01:46:09.320]   Oh, so it slides down. It's a feature phone.
[01:46:09.320 --> 01:46:11.080]   It's not a smartphone. Or is it a smartphone?
[01:46:11.080 --> 01:46:12.600]   It's a feature phone.
[01:46:12.600 --> 01:46:14.280]   It has 4G in it.
[01:46:14.280 --> 01:46:16.840]   Okay. But it doesn't have a operating system,
[01:46:16.840 --> 01:46:17.480]   at per se.
[01:46:17.480 --> 01:46:21.880]   Yeah, and the screen is tiny and there's no quartique.
[01:46:21.880 --> 01:46:24.520]   But Kano Reeves will come to wherever you are, if you call.
[01:46:26.280 --> 01:46:28.040]   And that is why I'm buying my soft one.
[01:46:28.040 --> 01:46:30.040]   I want that.
[01:46:30.040 --> 01:46:33.640]   I remember watching the Matrix and trying to figure out how to get one,
[01:46:33.640 --> 01:46:36.600]   like a thing in Mississippi when that came out.
[01:46:36.600 --> 01:46:37.640]   And I did the math.
[01:46:37.640 --> 01:46:40.680]   I thought it was going to cost like $3,000 and I couldn't get it.
[01:46:40.680 --> 01:46:42.120]   And now I can get it.
[01:46:42.120 --> 01:46:43.080]   So I'm very interested.
[01:46:43.080 --> 01:46:44.840]   And you better get it in Banana Yellow.
[01:46:44.840 --> 01:46:46.200]   Are you really being there?
[01:46:46.200 --> 01:46:46.840]   Oh, okay.
[01:46:46.840 --> 01:46:47.240]   Okay.
[01:46:47.240 --> 01:46:47.800]   I can do that.
[01:46:47.800 --> 01:46:48.200]   That would be...
[01:46:48.200 --> 01:46:48.600]   You would do.
[01:46:48.600 --> 01:46:50.680]   Yeah, I'd do it.
[01:46:50.680 --> 01:46:50.760]   Yeah, I'd do it.
[01:46:50.760 --> 01:46:51.320]   Exactly.
[01:46:51.320 --> 01:46:55.880]   You know, this is a phone that I've wanted for more than a decade, I guess.
[01:46:55.880 --> 01:46:57.240]   So I was really excited.
[01:46:57.240 --> 01:46:58.920]   That's not the same action though, right?
[01:46:58.920 --> 01:47:00.760]   You can't like click it and have it...
[01:47:00.760 --> 01:47:02.200]   Wow.
[01:47:02.200 --> 01:47:03.240]   To the fan.
[01:47:03.240 --> 01:47:04.200]   So you're right.
[01:47:04.200 --> 01:47:04.920]   You're exactly right.
[01:47:04.920 --> 01:47:08.120]   I mean, I noticed I wasn't going to point that out because I wasn't sure
[01:47:08.120 --> 01:47:09.320]   how people would respond to that.
[01:47:09.320 --> 01:47:11.640]   But in the movie, there is...
[01:47:11.640 --> 01:47:12.920]   We were born spatly.
[01:47:12.920 --> 01:47:13.000]   That's true.
[01:47:13.000 --> 01:47:14.120]   Actuated mechanism.
[01:47:14.120 --> 01:47:17.880]   So when you push a button, the bottom shoots out.
[01:47:17.880 --> 01:47:23.320]   Because spring loaded, whereas this new one appears to just slide off,
[01:47:23.320 --> 01:47:24.440]   which is a little less expensive.
[01:47:25.320 --> 01:47:26.360]   It's so sweet.
[01:47:26.360 --> 01:47:26.920]   It's so sweet.
[01:47:26.920 --> 01:47:27.560]   It's so much fun.
[01:47:27.560 --> 01:47:32.440]   Why would they miss that critical feature?
[01:47:32.440 --> 01:47:32.680]   Yeah.
[01:47:32.680 --> 01:47:37.000]   I think the better thing to say is, wait a minute, Nokia just made another phone.
[01:47:37.000 --> 01:47:38.840]   No, it makes me close.
[01:47:38.840 --> 01:47:39.800]   Yeah.
[01:47:39.800 --> 01:47:41.800]   I know.
[01:47:41.800 --> 01:47:43.880]   Yeah, I still get excited when...
[01:47:43.880 --> 01:47:47.720]   Here's Neo on his banana phone.
[01:47:47.720 --> 01:47:48.120]   Yeah.
[01:47:48.120 --> 01:47:50.360]   There's Mr. Smith.
[01:47:50.360 --> 01:47:51.560]   And...
[01:47:51.560 --> 01:47:54.520]   I love that movie.
[01:47:54.520 --> 01:47:55.800]   The Matrix was...
[01:47:55.800 --> 01:47:57.080]   I have never felt...
[01:47:57.080 --> 01:47:58.920]   Oh, he's flipping Mr. Smith off.
[01:47:58.920 --> 01:47:59.800]   That's mean.
[01:47:59.800 --> 01:48:00.600]   That's rude.
[01:48:00.600 --> 01:48:03.080]   Do you remember different...
[01:48:03.080 --> 01:48:05.560]   I don't think that's the phone.
[01:48:05.560 --> 01:48:07.800]   Ring, ring, ring, ring, ring, ring, ring, ring...
[01:48:07.800 --> 01:48:11.960]   I have once again been deceived by YouTube.
[01:48:11.960 --> 01:48:17.240]   I wanted to find the actual video.
[01:48:17.240 --> 01:48:18.760]   Here, I think this is...
[01:48:18.760 --> 01:48:19.640]   I hope this is it.
[01:48:19.640 --> 01:48:20.200]   This is...
[01:48:20.200 --> 01:48:22.600]   I'm going to say banana is full on the rest of the day.
[01:48:22.600 --> 01:48:23.560]   Banana is full?
[01:48:23.560 --> 01:48:24.680]   Oh, I like that.
[01:48:24.680 --> 01:48:26.440]   So I think this is smart.
[01:48:26.440 --> 01:48:29.640]   Nokia announced in 33.10 last year or the year before,
[01:48:29.640 --> 01:48:31.640]   which was also one of those candy bar phones.
[01:48:31.640 --> 01:48:34.280]   And how did that sell?
[01:48:34.280 --> 01:48:34.680]   Do you know?
[01:48:34.680 --> 01:48:34.920]   Is it...
[01:48:34.920 --> 01:48:37.080]   I don't think well.
[01:48:37.080 --> 01:48:39.560]   I don't know any actual numbers,
[01:48:39.560 --> 01:48:43.320]   but my impression is that it didn't do spectacularly,
[01:48:43.320 --> 01:48:45.480]   but it's just kind of interesting to know
[01:48:45.480 --> 01:48:47.400]   that there's still a demand for these...
[01:48:47.400 --> 01:48:50.200]   I guess you can call them dumb phones.
[01:48:50.200 --> 01:48:52.040]   And I don't know if it's an astalgic...
[01:48:52.040 --> 01:48:52.840]   Ooh, ooh.
[01:48:52.840 --> 01:48:53.480]   Watch, but...
[01:48:53.480 --> 01:48:53.960]   There it is.
[01:48:53.960 --> 01:48:54.600]   There it is.
[01:48:54.600 --> 01:48:55.720]   Oh, look at that coming.
[01:48:55.720 --> 01:48:57.080]   Wait a minute, let's see that again.
[01:48:57.080 --> 01:49:01.080]   I need the Samsung super slow motion for that.
[01:49:01.080 --> 01:49:03.720]   He's opening up the FedEx package.
[01:49:03.720 --> 01:49:06.280]   Yes, they do have FedEx in the matrix.
[01:49:06.280 --> 01:49:11.240]   Oh, baby.
[01:49:11.240 --> 01:49:13.720]   I wanted that phone so bad.
[01:49:13.720 --> 01:49:14.600]   Really?
[01:49:14.600 --> 01:49:15.400]   It's funny.
[01:49:15.400 --> 01:49:16.200]   I love that movie,
[01:49:16.200 --> 01:49:18.440]   but I didn't get fetishized about the phone.
[01:49:18.440 --> 01:49:19.480]   Oh, I did.
[01:49:19.480 --> 01:49:19.880]   Oh, good.
[01:49:19.880 --> 01:49:20.840]   Yes.
[01:49:20.840 --> 01:49:21.800]   Yes.
[01:49:21.800 --> 01:49:23.720]   I've been looking for you, Neo.
[01:49:23.720 --> 01:49:25.320]   I've been looking for you, Neo.
[01:49:25.320 --> 01:49:26.680]   I know the microphone.
[01:49:26.680 --> 01:49:28.840]   But unfortunately, you and I...
[01:49:28.840 --> 01:49:30.760]   You know, what I was thinking about recently was like,
[01:49:30.760 --> 01:49:33.800]   I heard that they're going to remake the matrix.
[01:49:33.800 --> 01:49:36.040]   They're going to reboot the matrix pretty soon.
[01:49:36.040 --> 01:49:38.120]   And I wonder what the matrix would be like
[01:49:38.120 --> 01:49:39.480]   in the era of smartphones.
[01:49:39.480 --> 01:49:41.400]   You know, does the premise still hold true?
[01:49:41.400 --> 01:49:43.960]   Can you create these situations where
[01:49:43.960 --> 01:49:46.360]   they're running to a landline and that sort of thing?
[01:49:46.360 --> 01:49:49.880]   So, anyways, I'll just throw that out there for...
[01:49:49.880 --> 01:49:54.600]   Once they met the architect, the whole series...
[01:49:54.600 --> 01:49:55.880]   The whole thing fell apart.
[01:49:55.880 --> 01:49:58.840]   It's completely over now.
[01:49:58.840 --> 01:50:01.000]   See, the whole premise of the matrix
[01:50:01.000 --> 01:50:04.280]   shows the matrix except at the height of our civilization,
[01:50:04.280 --> 01:50:06.360]   which is clearly the 90s, right?
[01:50:06.360 --> 01:50:07.720]   It's not going to be today.
[01:50:07.720 --> 01:50:11.240]   So, you know, Spice Girls, MTV.
[01:50:11.240 --> 01:50:16.360]   Here's a little clip.
[01:50:16.360 --> 01:50:18.760]   Just for those of you who don't remember of Neo
[01:50:18.760 --> 01:50:21.080]   in the matrix, may turn on the...
[01:50:21.080 --> 01:50:22.520]   Oh, I love this.
[01:50:22.520 --> 01:50:24.520]   Wait a minute. Wait a minute.
[01:50:24.520 --> 01:50:27.240]   No, it's not gonna be better.
[01:50:27.240 --> 01:50:28.600]   Is that Will Ferrell as...
[01:50:28.600 --> 01:50:29.400]   Oh, I'm just...
[01:50:29.400 --> 01:50:31.720]   Because you, my friend, are completely...
[01:50:31.720 --> 01:50:32.920]   whipped.
[01:50:32.920 --> 01:50:35.880]   Those sh*t.
[01:50:35.880 --> 01:50:38.440]   Watch the sass, Captain Sassy Pants.
[01:50:38.440 --> 01:50:40.120]   Yeah, you're kind of spazzing out, dude.
[01:50:40.120 --> 01:50:41.640]   You haven't answered my question.
[01:50:41.640 --> 01:50:42.280]   Yes, I did.
[01:50:42.280 --> 01:50:44.280]   You see...
[01:50:44.280 --> 01:50:44.520]   What?
[01:50:44.520 --> 01:50:45.400]   You haven't answered my...
[01:50:45.400 --> 01:50:46.200]   I'm trying.
[01:50:46.200 --> 01:50:47.800]   You just need to let me talk.
[01:50:47.800 --> 01:50:48.600]   Why am I here?
[01:50:48.600 --> 01:50:51.080]   You shot up.
[01:50:51.080 --> 01:50:51.720]   You won't let it.
[01:50:51.720 --> 01:50:52.600]   No, you won't let it!
[01:50:52.600 --> 01:50:54.920]   I'm the one who talks!
[01:50:54.920 --> 01:50:56.520]   Hey, now shut!
[01:50:56.520 --> 01:50:57.240]   Here's open!
[01:50:57.240 --> 01:50:59.000]   All right, enough.
[01:50:59.000 --> 01:50:59.480]   I'm sorry.
[01:50:59.480 --> 01:51:01.640]   Once again, I have been fooled by YouTube.
[01:51:01.640 --> 01:51:03.480]   That was so much better than the original.
[01:51:03.480 --> 01:51:03.800]   It was.
[01:51:03.800 --> 01:51:05.480]   That was the MTV Awards.
[01:51:05.480 --> 01:51:05.800]   Yeah.
[01:51:05.800 --> 01:51:08.440]   Will Ferrell and Justin Timberlake...
[01:51:08.440 --> 01:51:12.120]   Mocking the greatest movie ever made.
[01:51:12.120 --> 01:51:14.040]   The Matrix 3.
[01:51:14.040 --> 01:51:15.320]   That first...
[01:51:15.320 --> 01:51:17.160]   The first Matrix was...
[01:51:17.640 --> 01:51:21.000]   Bring it up until the end when Love Save the Day was
[01:51:21.000 --> 01:51:24.440]   like the greatest thing I've ever seen.
[01:51:24.440 --> 01:51:24.840]   I agree.
[01:51:24.840 --> 01:51:27.000]   And the fact that somebody thinks it's a good idea
[01:51:27.000 --> 01:51:27.960]   to remake that in the year...
[01:51:27.960 --> 01:51:28.840]   Terrible.
[01:51:28.840 --> 01:51:31.480]   Well, no, are they remaking it or continuing it?
[01:51:31.480 --> 01:51:32.680]   I think they're gonna reap it.
[01:51:32.680 --> 01:51:33.480]   What?
[01:51:33.480 --> 01:51:36.840]   That shows they run out of ideas.
[01:51:36.840 --> 01:51:38.520]   I'm actually excited for the review.
[01:51:38.520 --> 01:51:39.320]   I'm not even gonna lie.
[01:51:39.320 --> 01:51:40.520]   I probably go see it.
[01:51:40.520 --> 01:51:41.640]   I liked all three.
[01:51:41.640 --> 01:51:42.120]   Yeah.
[01:51:42.120 --> 01:51:42.840]   I wanted to do it.
[01:51:42.840 --> 01:51:44.200]   Oh, well, there's something wrong with you then.
[01:51:44.200 --> 01:51:44.600]   I don't...
[01:51:44.600 --> 01:51:45.240]   I'm...
[01:51:45.240 --> 01:51:45.800]   I'm...
[01:51:45.800 --> 01:51:46.520]   I'm rare.
[01:51:46.520 --> 01:51:49.000]   One of the rare true fans of the Matrix.
[01:51:49.000 --> 01:51:49.480]   Oh.
[01:51:49.480 --> 01:51:50.040]   So...
[01:51:50.040 --> 01:51:52.280]   Yeah, I'm here for all of it.
[01:51:52.280 --> 01:51:54.040]   Did you see the...
[01:51:54.040 --> 01:51:54.840]   Did you see the...
[01:51:54.840 --> 01:51:56.840]   When did you see the Matrix?
[01:51:56.840 --> 01:51:57.080]   Were you...
[01:51:57.080 --> 01:51:58.200]   Did you see it in a movie theater?
[01:51:58.200 --> 01:51:59.720]   Or did you see it like later on?
[01:51:59.720 --> 01:52:00.760]   Later on?
[01:52:00.760 --> 01:52:01.000]   No.
[01:52:01.000 --> 01:52:02.040]   I saw it in the...
[01:52:02.040 --> 01:52:04.600]   I saw it on a plane for the very first time.
[01:52:04.600 --> 01:52:04.760]   Oh, it's true.
[01:52:04.760 --> 01:52:08.280]   And I just became obsessed and bought the DVD.
[01:52:08.280 --> 01:52:09.960]   I think probably like you, Amy.
[01:52:09.960 --> 01:52:11.960]   I didn't know what I was going to.
[01:52:11.960 --> 01:52:13.720]   I went to a matinee.
[01:52:13.720 --> 01:52:14.920]   I had not read anything.
[01:52:14.920 --> 01:52:15.560]   I know...
[01:52:15.560 --> 01:52:16.680]   I had no idea.
[01:52:16.680 --> 01:52:18.120]   And I walked out of that theater
[01:52:18.120 --> 01:52:21.240]   with my draw on the ground.
[01:52:21.240 --> 01:52:21.560]   Absolutely.
[01:52:21.560 --> 01:52:24.680]   Trying to figure out if I was in the Matrix.
[01:52:24.680 --> 01:52:25.240]   And that's why I hated it so much.
[01:52:25.240 --> 01:52:25.480]   Yeah.
[01:52:25.480 --> 01:52:26.520]   Right.
[01:52:26.520 --> 01:52:27.560]   And that's why...
[01:52:27.560 --> 01:52:28.440]   And I saw it...
[01:52:28.440 --> 01:52:30.040]   I was living in Japan at the time
[01:52:30.040 --> 01:52:32.520]   and I had just gotten out of a three-hour Ikeeto practice.
[01:52:32.520 --> 01:52:33.320]   And so...
[01:52:33.320 --> 01:52:34.680]   Like the Kung Fu scenes.
[01:52:34.680 --> 01:52:35.880]   Yeah, I suppose I'll sweaty stuff.
[01:52:35.880 --> 01:52:36.520]   All the time.
[01:52:36.520 --> 01:52:38.120]   Oh, yeah.
[01:52:38.120 --> 01:52:40.440]   And that's why I had this like...
[01:52:40.440 --> 01:52:41.880]   I had never felt that way.
[01:52:41.880 --> 01:52:42.440]   I felt...
[01:52:44.120 --> 01:52:47.560]   I was just absolutely blown away.
[01:52:47.560 --> 01:52:51.640]   And that's why the two Matrixes that came afterwards were so...
[01:52:51.640 --> 01:52:53.480]   Like, unbelievably...
[01:52:53.480 --> 01:52:54.680]   Yeah.
[01:52:54.680 --> 01:52:56.120]   Disappointing and depressing.
[01:52:56.120 --> 01:52:57.720]   'Cause I expected so much more.
[01:52:57.720 --> 01:52:58.360]   Yeah.
[01:52:58.360 --> 01:53:00.600]   I do have to say the PlayStation 2 game,
[01:53:00.600 --> 01:53:04.440]   they actually went and filmed all these like scenes.
[01:53:04.440 --> 01:53:07.400]   It works in conjunction with the second Matrix movie.
[01:53:07.400 --> 01:53:07.480]   Yeah.
[01:53:07.480 --> 01:53:10.280]   And you've got like all these awesome scenes with Ghost
[01:53:10.280 --> 01:53:12.280]   and Jada Pinkett Smith's character.
[01:53:12.280 --> 01:53:15.880]   It's like the game itself, the gameplay is very mediocre.
[01:53:15.880 --> 01:53:17.800]   But it's a really good story.
[01:53:17.800 --> 01:53:19.320]   I thought that was very successful.
[01:53:19.320 --> 01:53:23.400]   If you're a real fan, Michael, like you will actually play all the way to that game.
[01:53:23.400 --> 01:53:25.400]   Also, just to defend the second one briefly,
[01:53:25.400 --> 01:53:29.720]   I really liked the metaphors that were used across the movie.
[01:53:29.720 --> 01:53:36.520]   So like, you know, back doors were physical parts of the Matrix.
[01:53:36.520 --> 01:53:37.640]   And what else?
[01:53:37.640 --> 01:53:40.440]   Like, you know, the ghosts, I think were viruses.
[01:53:40.440 --> 01:53:44.360]   And there was a lot of computer jargon
[01:53:44.360 --> 01:53:46.280]   that was used in like really creative ways.
[01:53:46.280 --> 01:53:48.040]   In a way, I think, I mean, given rumor,
[01:53:48.040 --> 01:53:53.000]   we'd seen sneakers and hackers and all these terrible movies about technology.
[01:53:53.000 --> 01:53:55.880]   In a way, this was the first movie where it at least
[01:53:55.880 --> 01:53:58.840]   looked plausible at the technology.
[01:53:58.840 --> 01:54:00.440]   And it felt like, yeah, they're getting it right.
[01:54:00.440 --> 01:54:01.480]   Yep.
[01:54:01.480 --> 01:54:02.520]   All right.
[01:54:02.520 --> 01:54:04.680]   And then I thought it was just the story fell apart.
[01:54:04.680 --> 01:54:06.600]   I don't know.
[01:54:06.600 --> 01:54:10.680]   I, this rebooting like amazing movies always makes me really nervous.
[01:54:10.680 --> 01:54:11.240]   I know.
[01:54:11.240 --> 01:54:11.720]   I know.
[01:54:11.720 --> 01:54:12.680]   And this is one of them.
[01:54:12.680 --> 01:54:13.240]   This is one of them.
[01:54:13.240 --> 01:54:14.120]   Well, for total recall.
[01:54:14.120 --> 01:54:18.200]   Oh, Arnold is the only total recall.
[01:54:18.200 --> 01:54:18.760]   I'm sorry.
[01:54:18.760 --> 01:54:18.760]   Yeah.
[01:54:18.760 --> 01:54:19.320]   There's no reason.
[01:54:19.320 --> 01:54:20.360]   Oh, the movie is bad.
[01:54:20.360 --> 01:54:22.040]   It's a terrible movie.
[01:54:22.040 --> 01:54:23.480]   That's what makes it so good.
[01:54:23.480 --> 01:54:28.920]   If you watch it now, you realize, my God, did we have no taste back then?
[01:54:28.920 --> 01:54:29.480]   What?
[01:54:29.480 --> 01:54:30.760]   That thing is the worst.
[01:54:30.760 --> 01:54:32.200]   But it's fun.
[01:54:32.200 --> 01:54:32.840]   I'm not the reboot.
[01:54:32.840 --> 01:54:33.640]   I'm not the reboot.
[01:54:33.640 --> 01:54:35.160]   The reboot is unwatchable.
[01:54:35.160 --> 01:54:35.640]   Slick.
[01:54:35.640 --> 01:54:36.200]   Yeah, I agree.
[01:54:36.200 --> 01:54:40.520]   I mean, but I have to say, it's the same thing with Terminator.
[01:54:40.520 --> 01:54:45.320]   The special effects in those days were not, you know, even believable.
[01:54:45.320 --> 01:54:48.040]   They weren't like Sharknado level good.
[01:54:48.040 --> 01:54:50.200]   Not even Sharknado level good.
[01:54:50.200 --> 01:54:54.920]   Listen, no matter what evil is going on in the world,
[01:54:54.920 --> 01:54:55.960]   nothing's as bad.
[01:54:55.960 --> 01:54:59.480]   Thought like, like somebody green lighted Sharknado.
[01:54:59.480 --> 01:54:59.640]   Yeah.
[01:54:59.640 --> 01:55:01.160]   And that gives us hope for the future.
[01:55:01.160 --> 01:55:01.320]   Yeah.
[01:55:01.320 --> 01:55:01.880]   Yeah.
[01:55:01.880 --> 01:55:02.040]   Yeah.
[01:55:03.960 --> 01:55:06.200]   Is Twitter lockout not a thing anymore?
[01:55:06.200 --> 01:55:09.240]   Is that like, is that the fastest meme that ever ended?
[01:55:09.240 --> 01:55:16.440]   This was, of course, Twitter earlier this week decided to delete a number of bots.
[01:55:16.440 --> 01:55:18.280]   We don't know.
[01:55:18.280 --> 01:55:20.600]   I don't know if we ever got the details on how many,
[01:55:20.600 --> 01:55:26.760]   but conservatives decided that they were being targeted because it turned out oddly.
[01:55:26.760 --> 01:55:30.920]   A lot of those bots followed conservative users.
[01:55:30.920 --> 01:55:37.240]   And so this was an example of how there is a divergence in the reality
[01:55:37.240 --> 01:55:40.840]   of the people living.
[01:55:40.840 --> 01:55:47.560]   Conservatives thought this was proof positive, proof positive that Twitter was out to block them
[01:55:47.560 --> 01:55:48.280]   and hurt them.
[01:55:48.280 --> 01:55:53.240]   And others said, well, of course, you're the first people who are going to lose followers.
[01:55:53.240 --> 01:56:00.360]   Now, some people actually were real people who were locked out and they were asked
[01:56:01.000 --> 01:56:03.880]   to verify their phone number, but that's not a big deal.
[01:56:03.880 --> 01:56:09.560]   We don't know how Twitter, what its criteria were.
[01:56:09.560 --> 01:56:11.720]   We don't know how many it deleted.
[01:56:11.720 --> 01:56:15.960]   This is a real guy apparently, or maybe who knows, for all I know he's actually a
[01:56:15.960 --> 01:56:19.560]   Russian controlled account that decided to make a big deal out of it.
[01:56:19.560 --> 01:56:22.920]   Actually, that's the part of the problem of this is you don't now,
[01:56:22.920 --> 01:56:24.600]   you no longer trust anybody.
[01:56:24.600 --> 01:56:25.560]   You don't know what that it's like.
[01:56:25.560 --> 01:56:26.440]   We're in the matrix.
[01:56:27.400 --> 01:56:31.000]   5,000 of my blocked accounts went away.
[01:56:31.000 --> 01:56:35.560]   So my number went from like 15,000 blocked under like 10,000.
[01:56:35.560 --> 01:56:36.680]   Oh, that's interesting.
[01:56:36.680 --> 01:56:41.800]   I only lost a few hundred followers, but I should look at how many of the people I've blocked went away.
[01:56:41.800 --> 01:56:46.760]   You lost 5,000 accounts that you had blocked.
[01:56:46.760 --> 01:56:48.600]   Those were presumably Russian bots.
[01:56:48.600 --> 01:56:53.240]   Well, most of the accounts I blocked were during game or gate, but yeah, I mean, just
[01:56:53.240 --> 01:56:58.040]   they were game or gate. So do you think during game or gate where you were one of the chief
[01:56:58.040 --> 01:57:00.200]   targets and you had to move and everything was terrible?
[01:57:00.200 --> 01:57:04.920]   Do you think that a lot of that was internet research agency now in hindsight?
[01:57:04.920 --> 01:57:07.000]   I mean, I don't think it was bots.
[01:57:07.000 --> 01:57:10.760]   I do think that they specialized in a Twitter.
[01:57:10.760 --> 01:57:15.720]   They use bots to attack me, but I don't think it was like a Russian propaganda thing.
[01:57:15.720 --> 01:57:20.520]   I think people found out they could buy bots to have certain behavior.
[01:57:20.520 --> 01:57:23.480]   And we actually worked with Twitter quite a bit in
[01:57:23.480 --> 01:57:27.480]   Gosh, it was 2016 and 2017 to work on that.
[01:57:27.480 --> 01:57:29.960]   And they were very effective in finally solving that.
[01:57:29.960 --> 01:57:32.920]   Interesting. Very interesting.
[01:57:32.920 --> 01:57:39.640]   So you think that these were just accounts for sale that were used by,
[01:57:39.640 --> 01:57:42.600]   was it mostly you think American trolls that we're going after?
[01:57:42.600 --> 01:57:43.800]   Yeah, that's my belief with it.
[01:57:43.800 --> 01:57:49.960]   Okay. We've learned so much, it's since the indictment about this internet research agency,
[01:57:49.960 --> 01:57:54.520]   which had a significant budget and was using real accounts, fake accounts,
[01:57:54.520 --> 01:57:56.600]   sock puppet accounts to influence.
[01:57:56.600 --> 01:58:00.760]   I don't want to even say American election and influence American thought
[01:58:00.760 --> 01:58:03.400]   to disturb America.
[01:58:03.400 --> 01:58:05.800]   I wouldn't be surprised if gamer gate was part of that campaign,
[01:58:05.800 --> 01:58:08.200]   but it was before really this all became an issue.
[01:58:08.200 --> 01:58:11.560]   I mean, there are goals to divide us.
[01:58:11.560 --> 01:58:15.320]   It's not ideological.
[01:58:15.320 --> 01:58:17.080]   It doesn't have anything with bright versus left.
[01:58:17.080 --> 01:58:19.400]   Like they support people on the right end of the draft.
[01:58:19.400 --> 01:58:20.360]   So there it is.
[01:58:20.360 --> 01:58:25.080]   Intel getting a little bit of heat.
[01:58:25.080 --> 01:58:31.400]   They apparently knew about the chip flaws in the the spectrum meltdown chip flaws
[01:58:31.400 --> 01:58:36.040]   months before they told the US cyber officials in cert.
[01:58:36.040 --> 01:58:42.680]   In fact, they told Chinese companies Lenovo and others before they told the US government.
[01:58:43.880 --> 01:58:49.000]   This was kind of a known issue on their end though for like 20 years,
[01:58:49.000 --> 01:58:51.320]   for like a very, very long time.
[01:58:51.320 --> 01:58:53.160]   Well, that's also part of the story, isn't it?
[01:58:53.160 --> 01:59:00.600]   That speculative execution, which is a technique that Intel AMD and ARM all use to speed up
[01:59:00.600 --> 01:59:05.640]   processor performance, had a potential problem, a leakage of information.
[01:59:05.640 --> 01:59:12.760]   And yes, there was a paper written in 1994 saying exactly that, that you've got to watch out
[01:59:12.760 --> 01:59:17.400]   because there's this could be there's, you know, I don't know if they mentioned timing attacks,
[01:59:17.400 --> 01:59:18.840]   but that these could be problematic.
[01:59:18.840 --> 01:59:25.160]   So Intel probably did have some idea, but I don't know if they knew how serious it was.
[01:59:25.160 --> 01:59:30.280]   And really, a spectrum meltdown were discovered kind of simultaneously,
[01:59:30.280 --> 01:59:37.160]   but I'm a security team's because timing attacks have become something we were all aware of.
[01:59:37.160 --> 01:59:39.240]   And they were really looking at timing attacks.
[01:59:39.240 --> 01:59:42.680]   There were other there was rohammer and other timing attacks that had been discovered.
[01:59:42.680 --> 01:59:46.200]   And I think the security community then said, yeah, we should see what else we can do with these.
[01:59:46.200 --> 01:59:51.960]   And that's when speculative execution, you know, really speculative execution been in the
[01:59:51.960 --> 01:59:53.800]   Intel chips for that long.
[01:59:53.800 --> 02:00:01.000]   Because you know, it's really been in there that long, the ring one and ring zero executions of it.
[02:00:01.000 --> 02:00:01.960]   Yeah, all of that.
[02:00:01.960 --> 02:00:07.880]   Yeah, predictive branching and speculative execution, because Intel was running, it was hitting a wall.
[02:00:09.480 --> 02:00:13.640]   And these were ways to speed up chips. And it worked so well, by the way, everybody adopted it.
[02:00:13.640 --> 02:00:18.600]   Current and former US government officials have raised concerns that the government was
[02:00:18.600 --> 02:00:24.680]   not informed about these flaws before they became public. In fact, we had on us, the screens
[02:00:24.680 --> 02:00:29.160]   every yesterday, Ian Thompson from the register, the register revealed this. And it wasn't till
[02:00:29.160 --> 02:00:34.440]   the register published this article that Intel, and by the way, Intel's first reaction was yours.
[02:00:36.280 --> 02:00:42.520]   That Intel fell, oh, now we got to tell everybody. So this is, it was Google's Project Zero that
[02:00:42.520 --> 02:00:49.480]   had formed Intel AMD and arm holdings of the problem back in June. As with most security
[02:00:49.480 --> 02:00:53.480]   revelations, they gave the chip makers 90 days before public disclosure.
[02:00:53.480 --> 02:01:00.440]   Wait, who's the Vay? Alphabet was the Google security team that discovered.
[02:01:00.440 --> 02:01:06.200]   Yeah, Project Zero, the fabulous Tavis Ormond D and his team over there.
[02:01:06.200 --> 02:01:14.760]   Anyway, I don't know. So I was just talking to my husband, who is also a geek and all of his
[02:01:14.760 --> 02:01:21.240]   friends work at Amazon. And anyhow, it's really interesting, apparently Amazon Web Services
[02:01:21.240 --> 02:01:28.280]   to deal with the problem. The centers are configured so well that it was a down time,
[02:01:29.160 --> 02:01:35.960]   minimal down time. It wasn't that huge of a deal. And for people running on Microsoft servers,
[02:01:35.960 --> 02:01:44.200]   it was a much more difficult, but it says something about the organizational structure of the
[02:01:44.200 --> 02:01:47.240]   backbone companies that we all rely on that are sort of invisible.
[02:01:47.240 --> 02:01:52.760]   And incidentally, those are the people who really have to worry about Spectre and Meltdown,
[02:01:52.760 --> 02:01:57.000]   because there are multiple people using the same processor. So if one of them is a bad guy.
[02:01:58.840 --> 02:02:03.960]   Right. And the virtual machines, apparently, were part of a problem too, because if you had an
[02:02:03.960 --> 02:02:10.440]   apparently could leap, the problem could propagate. And so many machines are now virtualized.
[02:02:10.440 --> 02:02:14.040]   That's like another weird problem that nobody probably thought of.
[02:02:14.040 --> 02:02:18.200]   Right. This should be a good time for me to mention our sponsor. And then we'll get some
[02:02:18.200 --> 02:02:26.200]   final thoughts, including a word from Kylie Jenner. But first, first, let's talk about Google's
[02:02:26.200 --> 02:02:33.560]   cloud platform and something they called cloud Spanner. As often as the case, Google tries these
[02:02:33.560 --> 02:02:38.680]   technologies, uses them internally. Cloud Spanner is a first horizontally scalable,
[02:02:38.680 --> 02:02:46.280]   strongly consistent relational database service tested by fire in Google's own usage, and now
[02:02:46.280 --> 02:02:54.280]   available to you on the cloud platform. It's kind of common wisdom that distributed databases can't
[02:02:54.280 --> 02:02:59.640]   be both relational and scalable. But what if you didn't have to make trade-offs? What if there were
[02:02:59.640 --> 02:03:05.720]   a no compromise solution, a fully managed database service that's consistent, that scales horizontally
[02:03:05.720 --> 02:03:10.840]   across data centers. It speaks SQL, so you don't have to learn a new language. Introducing Cloud
[02:03:10.840 --> 02:03:15.800]   Spanner. It's a mission critical relational database service from Google cloud platform,
[02:03:15.800 --> 02:03:21.160]   built from the ground up and battle tested at Google for strong consistency and high availability
[02:03:21.160 --> 02:03:27.720]   at global scale. Cloud Spanner delivers scalability, high transaction performance, and strong
[02:03:27.720 --> 02:03:37.240]   consistency across rows, regions, even continents with an industry leading get this SLA 99.999%.
[02:03:37.240 --> 02:03:44.680]   No planned downtime enterprise grade security multi language support. You could choose from
[02:03:44.680 --> 02:03:53.000]   C#, Go, Java, Node.js, PHP, Python, and Ruby for the client libraries. They've got a JDBC driver
[02:03:53.000 --> 02:03:59.640]   for connectivity with all the popular third party tools. It's very affordable, pricing is very simple,
[02:03:59.640 --> 02:04:04.840]   and no surprise, it's very predictable. You can even try it free. Find out more about Cloud Spanner.
[02:04:04.840 --> 02:04:12.040]   I'm going to give you a short URL, makes it easier for you to type it in. g.co/getspanner.
[02:04:12.760 --> 02:04:22.360]   That's g.co/googlesurlshortner. g.co/getspanner. Find out more about the mission critical relational
[02:04:22.360 --> 02:04:31.960]   database service from Google cloud platform. Battle hardened in the Google servers. g.co/getspanner.
[02:04:31.960 --> 02:04:37.320]   We think I'm really thrilled to have Google cloud platform as a sponsor on our shows. It tells
[02:04:37.320 --> 02:04:44.120]   me that Google knows there's a lot of geeks listening. A lot of geeks listening. Not as many as
[02:04:44.120 --> 02:04:50.760]   listening to Kylie Jenner. Kylie Jenner, I don't know anything about the Kardashians.
[02:04:50.760 --> 02:05:00.600]   That's probably us. But I gather that she is a third generation Kardashian.
[02:05:02.760 --> 02:05:06.840]   And she is apparently very popular with the youngens. Is that true?
[02:05:06.840 --> 02:05:12.280]   Yes, I think I can confirm it. She's very popular.
[02:05:12.280 --> 02:05:16.600]   The youngest person on this show, Michael, I think you're going to tell us everything.
[02:05:16.600 --> 02:05:24.760]   So on Thursday, Kylie Jenner tweets, "So does anyone else not open Snapchat anymore?"
[02:05:24.760 --> 02:05:28.120]   I don't even know if she talks like that, but I'm going to pretend she does. Or is it just me?
[02:05:28.120 --> 02:05:35.960]   Ugg so sad. Actually, shortly after she realized, "Maybe I... Did I go too far?"
[02:05:35.960 --> 02:05:41.160]   So she immediately says, "Still love you there, Snap. My first love. Snapchat
[02:05:41.160 --> 02:05:45.560]   immediately loses $1.3 billion in value on the stock."
[02:05:45.560 --> 02:05:54.440]   Now, to be fair, it also happened that an analyst that day downgraded the stock.
[02:05:56.520 --> 02:06:01.000]   A lot of this is coming from Snapchat fans who are saying we don't like the new Snapchat.
[02:06:01.000 --> 02:06:07.560]   It's worth adding. They have no business plan. The people running Snapchat have completely
[02:06:07.560 --> 02:06:13.960]   blown their IPO. This is not the best social media company in the world.
[02:06:13.960 --> 02:06:20.280]   I feel like Kylie Jenner is just one more like problem.
[02:06:20.280 --> 02:06:22.040]   The nail in the coffin, maybe.
[02:06:24.200 --> 02:06:28.680]   We should point out that their CEO and founder, Evan Spiegel, is perhaps one of the highest paid
[02:06:28.680 --> 02:06:35.080]   executives in the US. After the IPO, he collected a $636 million stock grant.
[02:06:35.080 --> 02:06:42.920]   It won't vest fully for two more years. Don't go hit them up for a billion now.
[02:06:42.920 --> 02:06:52.040]   You could look at this story in a more optimistic framing. Content still obviously matters a lot.
[02:06:52.760 --> 02:06:56.440]   Good point. That's a good thing. She's content for what it's worth.
[02:06:56.440 --> 02:07:06.680]   She is content. She's photos and inane nonsensical videos. They drive traffic and people seem to care.
[02:07:06.680 --> 02:07:13.320]   If the content is threatened to go away, then market value is lost. I actually think that's a
[02:07:13.320 --> 02:07:18.840]   good thing for content creators. It reminds them of their value. It's not the platform. It's us.
[02:07:20.360 --> 02:07:25.880]   You actually take a little positive. That's really good, Amy. I like it.
[02:07:25.880 --> 02:07:33.240]   What I don't like is that the FCC order became official. Yesterday, Friday, the
[02:07:33.240 --> 02:07:39.400]   restoring internet freedom order, which in fact, of course, kills net neutrality, entered the federal
[02:07:39.400 --> 02:07:46.920]   registry. Now, the key here is that now all the lawsuits can begin. Now that it's the regulation
[02:07:46.920 --> 02:07:55.000]   of the land, so to speak. There are many states, attorneys general. There are many entities,
[02:07:55.000 --> 02:08:03.240]   EFF and others who are going to be suing. This begins. Meanwhile, net neutrality is dead,
[02:08:03.240 --> 02:08:09.320]   at least until some court comes along and protects it, which I don't think is going to happen.
[02:08:09.320 --> 02:08:12.920]   I was really trying to get through a show without dropping the off-ball.
[02:08:15.160 --> 02:08:21.080]   Nothing positive to say about this FCC chairman or any part of the process.
[02:08:21.080 --> 02:08:27.240]   I thought for a while, "We still don't have to worry because the big ISPs are not going to..."
[02:08:27.240 --> 02:08:32.520]   They realize people are watching them. AT&T immediately,
[02:08:32.520 --> 02:08:38.840]   AT&T, the company took out the full page ad saying, "Yeah, we love net neutrality."
[02:08:38.840 --> 02:08:44.680]   They immediately rolled out new features that are basically zero rating. They expanded their
[02:08:44.680 --> 02:08:52.200]   sponsored data program. If you're a prepaid wireless customer, guess who? Guess who? You can watch
[02:08:52.200 --> 02:08:58.600]   with no cost to you in data AT&T's own products, direct TV, U-verse and full screen.
[02:08:58.600 --> 02:09:05.240]   If you have an AT&T product, you no longer have to pay. It doesn't count against your data plan
[02:09:05.240 --> 02:09:10.600]   if you watch our stuff. I think that's exactly what we were talking about.
[02:09:11.880 --> 02:09:19.640]   Paid fast lanes. It was AT&T who said, "No, no, no. We believe in net neutrality."
[02:09:19.640 --> 02:09:26.760]   Congress is really involved. I'm a Amia Word judge. Every year at about this time,
[02:09:26.760 --> 02:09:31.320]   I start getting all these DVDs in the mail. They're screeners for your consideration
[02:09:31.320 --> 02:09:38.600]   screeners. Just as this was happening, I got this giant box of AT&T shows, which I didn't even
[02:09:38.600 --> 02:09:50.200]   know existed. The AT&T Death Star doesn't worry me nearly as much as Comcast, but we'll see what
[02:09:50.200 --> 02:09:53.480]   will happen. I think they are going to tread carefully initially. We'll see.
[02:09:53.480 --> 02:10:01.960]   Big stories of the week Dropbox on Friday went public with their plans to file on IPO.
[02:10:01.960 --> 02:10:08.120]   Talk about another company that doesn't make any money. But the good news is they're losing less
[02:10:08.120 --> 02:10:13.960]   money. Instead of losing $200 million, as they did two years ago, they only lost $110 million
[02:10:13.960 --> 02:10:19.720]   last year. Can you imagine going back in a time machine 200 years and telling everybody,
[02:10:19.720 --> 02:10:25.640]   like, "If you're going to be a success, if you're going to lose $200 million a year,
[02:10:25.640 --> 02:10:28.360]   you're going to be a success." Brilliant business plan.
[02:10:28.360 --> 02:10:37.320]   You're going to own a yacht. I don't buy stocks in tech companies, so don't listen to me,
[02:10:37.320 --> 02:10:45.480]   but I don't even understand it. Do you buy stock in a company that is not making money?
[02:10:45.480 --> 02:10:52.680]   Well, someday they will or what? Some people named Kylie, who is a bitmoji, is going to call you
[02:10:52.680 --> 02:10:54.760]   right here. Yeah. Yeah.
[02:10:54.760 --> 02:11:03.960]   Kylie, just remember this future past person. It's coming. Dropbox is valued, privately valued.
[02:11:03.960 --> 02:11:08.040]   Now at $10 billion, they plan to raise about half a billion dollars with the IPO.
[02:11:08.040 --> 02:11:12.760]   Wow. Nice work if you can get. Agering amount of money.
[02:11:12.760 --> 02:11:18.760]   Especially at the higher competition, I find myself asking all the time, like,
[02:11:18.760 --> 02:11:24.840]   "Why do I continue paying Dropbox $20 a month when I have iCloud Drive and all these other
[02:11:24.840 --> 02:11:32.680]   services?" It's a well-done service, but I find myself increasingly asked why I'm paying for it.
[02:11:32.680 --> 02:11:35.560]   It's hard to think of a differentiator, isn't it? Yeah.
[02:11:35.560 --> 02:11:41.640]   Well, the user interface is phenomenal. I mean, if you've ever tried using Box or even Google Drive
[02:11:41.640 --> 02:11:47.880]   for a lot of businesses, I think it makes more sense to use Dropbox because it's just so intuitive.
[02:11:47.880 --> 02:11:55.000]   You have a folder on your desktop that sings to the cloud, and it's a lot easier to use. You can see
[02:11:55.000 --> 02:12:02.200]   photo previews much more easily than you can in a lot of services. I don't know. I always thought
[02:12:02.200 --> 02:12:05.960]   they were all sort of the same. It's like, "Oh, I can just kind of interchange any one of these
[02:12:05.960 --> 02:12:11.560]   cloud services." What I've come to realize is that they're all vastly different from each other.
[02:12:11.560 --> 02:12:15.960]   Oh, interesting. A lot of my friends prefer Dropbox, honestly. A lot of my friends that
[02:12:15.960 --> 02:12:22.760]   own and operate businesses. I had all of them, and I still have many of them. I had iCloud,
[02:12:22.760 --> 02:12:29.800]   I had Microsofts, OneDrive, Google Drive, some oddball ones like Trezorit.
[02:12:29.800 --> 02:12:37.560]   I just recently killed them all except Dropbox because Dropbox, it's an ecosystem thing.
[02:12:37.560 --> 02:12:43.800]   All the iOS apps save to iCloud and Dropbox. Actually, a lot of them don't even save to iCloud.
[02:12:43.800 --> 02:12:50.760]   It's there. Now, I also am aware of, as we talked about earlier, that Dropbox is not private,
[02:12:51.480 --> 02:12:55.160]   that the keys are held by Dropbox. You shouldn't put anything there that you want to
[02:12:55.160 --> 02:13:00.120]   put up privately. I just use it as because services and iOS expect it for their
[02:13:00.120 --> 02:13:08.360]   settings and storing their files and things like that. Maybe they do have a reason to be.
[02:13:08.360 --> 02:13:12.040]   I think we should wrap this up. We've been going long enough. You guys are champions.
[02:13:12.040 --> 02:13:18.040]   You haven't gone to the bathroom in hours. Dinner is cold. Amy Webb, you could tell your
[02:13:18.040 --> 02:13:21.480]   husband I got the computer glasses. You recommend it. He recommended it.
[02:13:21.480 --> 02:13:25.000]   That's great. I will absolutely tell you how are they working out?
[02:13:25.000 --> 02:13:29.160]   They're great. If we talked last time, I realized that's what I need to sit at my desktop because I
[02:13:29.160 --> 02:13:35.640]   can barely see it. I took your advice and got the blue filter in it and the slightly higher
[02:13:35.640 --> 02:13:42.200]   magnification. It's great. That's awesome. That's great. Thank you for me. Her husband is an
[02:13:42.200 --> 02:13:52.040]   ophthalmologist, but he also is good for advice about that kind of thing. Amy is a futurist.
[02:13:52.040 --> 02:13:56.760]   Her book The Signals are Talking is really great. She gives away all the secrets of her trade,
[02:13:56.760 --> 02:14:01.560]   which is bizarre. But hey, if she's going to do it, you might as well read it.
[02:14:01.560 --> 02:14:06.360]   "Why today's fringe is tomorrow's mainstream, and we will be looking for the future
[02:14:06.360 --> 02:14:12.600]   today Institute trend report, which you can get from the website or go to Amy Webb.io and find
[02:14:12.600 --> 02:14:16.680]   out more about everything Amy's up to." So nice to always. Great to have you on.
[02:14:16.680 --> 02:14:24.200]   Thank you, Amy. Brianna Wu, we're rooting for you. Brianna Wu in 22. No, that's wrong. We need a slogan.
[02:14:24.200 --> 02:14:32.600]   Who are you going to call? Who are you going to call? I love it. You need a meme generator on
[02:14:32.600 --> 02:14:37.080]   your website. You can have people make posters. I thought about that. Yeah. You know, it's like,
[02:14:37.080 --> 02:14:42.680]   you know, this is 2018. Positive attention is almost as valuable as negative attention.
[02:14:42.680 --> 02:14:49.400]   That's a good point. That's a very good point. We don't forget Bush Chaney. They didn't they win
[02:14:49.400 --> 02:14:55.960]   in 2004? I believe they did. They did. Who are you going to call? Brianna Wu, that's who I like.
[02:14:55.960 --> 02:15:02.040]   Do I say if you want to donate to my campaign, it's supportbray.com. Support Bri.
[02:15:02.040 --> 02:15:11.400]   B-R-I-A-N-N-A.com. Support Brianna.com and find out more about our Canessee Brianna Wu 2018.com.
[02:15:11.400 --> 02:15:17.320]   And if you're in the Massachusetts 8th, September, market in the calendar, the first Tuesday in
[02:15:17.320 --> 02:15:23.000]   September, you have a job to do. Who are you going to call? Who are you going to call?
[02:15:23.000 --> 02:15:27.720]   So many of you, how many congressional candidates will be like, yeah, I'll come have coffee with you.
[02:15:27.720 --> 02:15:30.040]   Nice. That's nice. You know, you can do that.
[02:15:30.040 --> 02:15:35.720]   All right. So, so happy to know you, Brianna. And so proud of what you're doing. It's fantastic.
[02:15:35.720 --> 02:15:39.240]   A lot of people would just, you know, disappear, move to an island.
[02:15:39.240 --> 02:15:46.360]   You said, I'm going to take the bull by the horns. Let's do it. Yep. I'm a fighter. Michael Nunez,
[02:15:46.360 --> 02:15:51.400]   from Mashable, where he's senior tech editor there, his day has been long deputy tech editor. I
[02:15:51.400 --> 02:15:57.320]   just promoted you. Sorry. I'll take it. Get that senior guy out of there. The day has been long.
[02:15:57.320 --> 02:16:03.000]   You started early with the Mobile World Congress. I thank you for staying late with us at my club.
[02:16:03.000 --> 02:16:04.840]   Thank you. That's been awesome. Always a pleasure.
[02:16:04.840 --> 02:16:11.640]   Yeah. The panel is so fun. Aren't they great? Yeah, really awesome. I thoroughly enjoyed this.
[02:16:11.640 --> 02:16:15.880]   Like more than I should have as a participant. I know what you mean. That's why I do this show.
[02:16:16.600 --> 02:16:21.320]   I just, to me, it's all about just, I get friends in and I sit back and go,
[02:16:21.320 --> 02:16:26.600]   aren't you guys smart? Wow. You make me think. I learn. I love it. And I include you in that,
[02:16:26.600 --> 02:16:30.280]   Michael. Thank you for being here. Thank you all for watching. If you want to watch live,
[02:16:30.280 --> 02:16:36.440]   we do it 3 p.m. Sunday afternoons, Pacific Times, 6 p.m. Eastern 2300 UTC. We have a live stream
[02:16:36.440 --> 02:16:42.680]   at twit.tv/live. You can listen to a live audio stream too on any voice activated device. I was
[02:16:42.680 --> 02:16:48.520]   just playing with the echo the other day. And you actually now have to say, "Echo, listen to tune
[02:16:48.520 --> 02:16:53.480]   in Twit Live." But if you say that, your echo will play our live stream so you can see whatever is
[02:16:53.480 --> 02:16:57.880]   going on in the studio at any given time. Or you can ask for any individual podcast in here in the
[02:16:57.880 --> 02:17:02.280]   most recent version. Echo, listen to this week in tech. You'll get the most recent version.
[02:17:02.280 --> 02:17:07.320]   You can also go to our website, twit.tv/live. If you do either of the live things,
[02:17:07.320 --> 02:17:12.360]   please join the chat room because that's a great way to give us feedback. I'm watching
[02:17:12.360 --> 02:17:15.720]   the chat room as we go. I get great ideas, links, all sorts of information.
[02:17:15.720 --> 02:17:23.480]   It's really important part of our broadcast day for the live shows. Please, irc.twit.tv.
[02:17:23.480 --> 02:17:28.840]   If you want to be in the studio live, we have a great live audience today. It's fantastic.
[02:17:28.840 --> 02:17:33.880]   Email us tickets@twit.tv and we will make sure there's a chair out for you.
[02:17:33.880 --> 02:17:39.640]   If you can't watch live, we always have on-demand versions of everything we do, both audio and video
[02:17:39.640 --> 02:17:44.440]   at our website and wherever you find your favorite podcast. In fact, if you do me a favor and subscribe,
[02:17:44.440 --> 02:17:47.960]   that would be great. Don't forget, I think there's a little time left to take the survey,
[02:17:47.960 --> 02:17:51.880]   twit.tv/survey. Once a year, we try to learn a little bit more about you. Not because we're
[02:17:51.880 --> 02:17:55.320]   going to share that with any third party, but just because it helps us do a better job.
[02:17:55.320 --> 02:18:01.240]   When advertisers ask us things like, "Are you people college educated?" We could say, "Yeah,
[02:18:01.240 --> 02:18:05.000]   five percent of them are." Whatever. I don't know what the number is. I'm sure it's higher.
[02:18:05.800 --> 02:18:12.200]   Thank you. I'm not, so I don't know. Thank you all for being here. I appreciate it. We'll see you next
[02:18:12.200 --> 02:18:16.920]   time. Another twit. I was in the camp. Bye-bye.
[02:18:17.560 --> 02:18:24.360]   Do the twit. All right. Do the twit, baby. Do the twit. All right. Do the twit.

