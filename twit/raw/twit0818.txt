;FFMETADATA1
title=The Tunnel That Bored Vegas
artist=Leo Laporte, Denise Howell, Fr. Robert Ballecer, SJ, Dan Moren
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2021-04-12
track=818
language=English
genre=Podcast
comment=Google vs Oracle, Elon Musk's Boring Tunnel, Google IO 2021
encoded_by=Uniblab 5.1
date=2021
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.880]   It's time for Twit this weekend tech. We've got a doctor, a priest and a lawyer on this show.
[00:00:04.880 --> 00:00:09.440]   Well maybe not a doctor. Dan Morin is a doctor of colors, six colors.
[00:00:09.440 --> 00:00:15.440]   .com. Father Robert Ballis there, the digital Jesuit, and our attorney Denise Howell.
[00:00:15.440 --> 00:00:19.520]   We have lots to talk about including the Google Oracle SCOTUS decision.
[00:00:19.520 --> 00:00:26.240]   I think a big victory for tech. We'll also talk about Elon Musk's boring tunnel in Las Vegas.
[00:00:26.240 --> 00:00:33.920]   Turns out it actually is kind of boring. And Google I/O, can you solve the puzzle?
[00:00:33.920 --> 00:00:37.120]   Break out your punch cards. It's all coming up next on Twit.
[00:00:37.120 --> 00:00:45.440]   Podcasts you love from people you trust. This is Twit.
[00:00:54.320 --> 00:01:00.800]   This is Twit this week at Tech. Episode 818, recorded Sunday April 11th, 2021.
[00:01:00.800 --> 00:01:08.080]   The Tunnel that Bored Vegas. This week at Tech is brought to you by Zip Recruiter.
[00:01:08.080 --> 00:01:13.120]   Finding someone to wear many hats or one very specific hat is no easy task.
[00:01:13.120 --> 00:01:18.320]   Zip Recruiter finds people with a right experience for your job and then invites them to apply.
[00:01:18.880 --> 00:01:24.720]   Whether that's a civil engineer in New York or a mascot in Missouri. Try Zip Recruiter free
[00:01:24.720 --> 00:01:32.640]   at zipprecruiter.com/twit. And by Amazon Pharmacy. Amazon Prime members can save on
[00:01:32.640 --> 00:01:37.600]   prescription medication even when not using insurance and get free two-day delivery.
[00:01:37.600 --> 00:01:46.880]   Learn more at amazon.com/twitRx. And by podium. Find out how podium can help your business reach
[00:01:46.880 --> 00:01:54.640]   more customers. Get started free today at podium.com/twit. And by Barracuda.
[00:01:54.640 --> 00:01:58.800]   Hackers are always looking for the weakest link in your security configuration,
[00:01:58.800 --> 00:02:04.960]   especially in your email security. Barracuda's new threat analyzer tool helps you gain visibility
[00:02:04.960 --> 00:02:16.720]   into your particular vulnerabilities. Visit barracuda.com/twit.
[00:02:16.960 --> 00:02:22.240]   It's time for Twit this week in Techley Show. We cover the week's tech news with a lovely panel
[00:02:22.240 --> 00:02:28.800]   which is rotating constantly. And the only reason we rotate, I love every one of them and I want
[00:02:28.800 --> 00:02:35.440]   every panel we have to be the only panel. But then I wouldn't be able to get people on that I love
[00:02:35.440 --> 00:02:41.760]   as much like Denise Howell. Long time host of this week in law. But are you still a practicing lawyer,
[00:02:41.760 --> 00:02:46.160]   Denise? I am indeed a practicing lawyer, yes. Intellectual property law still?
[00:02:47.120 --> 00:02:53.200]   Yes, a bit of that. I do everything that touches online communications and the internet basically.
[00:02:53.200 --> 00:02:59.600]   Well, you know why you're on this week? Yeah. Big, scotus decision. Big, scotus decision. And
[00:02:59.600 --> 00:03:03.920]   we always have to call in legal counsel when anything like that happens. It's always a pleasure
[00:03:03.920 --> 00:03:09.280]   to see Denise. Always a pleasure to see Father Robert Ballisair, the digital Jesuit. Now at
[00:03:09.280 --> 00:03:19.360]   digitaljazuit.com. I'm also a practicing lawyer. Are you really? No. Are there are there
[00:03:19.360 --> 00:03:23.600]   priest lawyers? Is that is that a thing? Oh, yes. Oh, definitely. Most definitely. And they're all
[00:03:23.600 --> 00:03:31.600]   Jesuits, I'm sure. A lot of the, I think the majority of priests who are non-canon lawyers,
[00:03:31.600 --> 00:03:36.960]   but lawyers are Jesuits. We kind of have a thing on that. Okay, there's a lot of logic involved in
[00:03:36.960 --> 00:03:41.520]   that. And I really can't parse that statement, but I'm going to take your word for it. Okay.
[00:03:41.520 --> 00:03:46.320]   There was an or it was an and there was a not. I don't know. It's too much. This is DJ
[00:03:46.320 --> 00:03:53.120]   digitaljazuit.com. And by the way, this is where people can now go. You're going to open up to
[00:03:53.120 --> 00:03:59.360]   the public your Minecraft server. Right. This is that thing that we started a couple of years back.
[00:03:59.360 --> 00:04:03.440]   It exploded because we got trolled, but then we had the wonderful folks from CloudFlare. They
[00:04:03.440 --> 00:04:09.360]   came in and they protected us because it was in the news. Great. Blackrails came in.
[00:04:09.360 --> 00:04:14.400]   Vatican starts. Minecraft server gets hacked. It was like, it was a great headline. It was a
[00:04:14.400 --> 00:04:19.120]   great headline. It wasn't hacked. Of course, it was just trolled. It was trolled. You were doing
[00:04:19.120 --> 00:04:23.680]   Rust. Now you're doing something called factoria, which I'm going to get in on. Correct. But the
[00:04:23.680 --> 00:04:28.560]   thing that got me, you're going to do TF2, which is probably the greatest mod game of all time.
[00:04:29.520 --> 00:04:36.480]   Terraria among us, which is great fun. And my game, Valheim, the Viking building game.
[00:04:36.480 --> 00:04:42.400]   So I'm joining. I'm already a member, but I'm going to give you the five bucks a month because
[00:04:42.400 --> 00:04:49.120]   we'll make you an admin, Leo. No, that's the last thing. Digitaljazuit.com.
[00:04:49.120 --> 00:04:57.440]   And from six colors.com, home of the colorful graphs. It's Dan Moore and always a pleasure to see you
[00:04:57.440 --> 00:05:00.880]   visiting us from Boston. It appears. Is that the Charles behind you?
[00:05:00.880 --> 00:05:07.040]   I mean, virtually. Yes. I am across the river from Boston. So it's accurate in that regard,
[00:05:07.040 --> 00:05:10.800]   at least you're looking across the river to Boston. Yes. That's right. Are you in Cambridge?
[00:05:10.800 --> 00:05:14.160]   I wish I were a doctor, by the way, so that you could have a doctor, a lawyer, and a
[00:05:14.160 --> 00:05:19.520]   priest walk into a bar. It would be a joke waiting to be written. I'm going to call you Dr. Morin.
[00:05:19.520 --> 00:05:26.000]   Doesn't matter. It's an honorary doctorate. I gave it to you. Thank you. I gave it. It is an honor. Yes.
[00:05:26.800 --> 00:05:33.680]   So boy, there's a whole lot of stuff. This panel is assembled because of the whole lot of stuff we
[00:05:33.680 --> 00:05:39.600]   can talk about. Dan, one of the things that we don't know what's going to happen, there were
[00:05:39.600 --> 00:05:44.800]   many rumors that Apple was going to do an event last month. In fact, rumormonger John
[00:05:44.800 --> 00:05:49.360]   Prosser had to shave his eyebrows because he got it wrong. There was not an event. He said,
[00:05:49.360 --> 00:05:53.840]   I'll shave my eyebrows. If there's not an Apple event on March 23, he is now eyebrowless.
[00:05:56.000 --> 00:06:02.960]   Here we are, middle of April. No event on the 13th, obviously. They would have sent out invitations.
[00:06:02.960 --> 00:06:09.440]   So the next event could be the 20th or the 27th. Do you think there will be an event this month?
[00:06:09.440 --> 00:06:14.320]   I don't know. I'm kind of assulating between whether they have an event or whether they just
[00:06:14.320 --> 00:06:18.400]   decide to do some stuff via press release or just drop a video, which they've done as well.
[00:06:18.400 --> 00:06:22.080]   Last year, when they did that iPad Pro announcement with a magic keyboard,
[00:06:22.080 --> 00:06:26.160]   they just sort of like, oh, here's a video. Some members of the press got pretty briefed or
[00:06:26.160 --> 00:06:32.880]   embargoed information. But because of the pandemic, they don't have to necessarily send out invites
[00:06:32.880 --> 00:06:37.040]   a week in advance unless they really want to drum up on audience. For a lot of these things,
[00:06:37.040 --> 00:06:43.920]   I think it's much more focused at the press and the diehards. I'm not sure whether or not they'll
[00:06:43.920 --> 00:06:47.600]   hold a big event, but I would be surprised if there wasn't some sort of product announcement
[00:06:47.600 --> 00:06:52.000]   coming in the back half of April because there's so much stuff that has gotten rumored that
[00:06:52.000 --> 00:06:56.720]   just has never showed up. They're way overdue. Well, the most overdue for the air tags,
[00:06:56.720 --> 00:07:02.800]   more than a year since they announced those. We've seen all these hints in source code and
[00:07:02.800 --> 00:07:11.840]   resources and things that we know they exist. I'm thinking, or do they? Is it a conspiracy theory?
[00:07:11.840 --> 00:07:16.480]   Is it all a clever rules to make us believe? I know. No, I don't believe that either. But I
[00:07:16.480 --> 00:07:21.520]   did have a moment, and I was talking to my colleague Jason Snell about this, where it was just sort of a
[00:07:21.520 --> 00:07:26.480]   well, do they decide? Maybe this isn't something they want to do, like they were sort of playing
[00:07:26.480 --> 00:07:30.800]   with it and they were thinking about it and maybe it's just not a market they want to be in?
[00:07:30.800 --> 00:07:35.680]   I don't know. It's very strange to have something go on that long beforehand.
[00:07:35.680 --> 00:07:41.280]   It was a year ago, this month, we saw the Apple product support video that featured air tags
[00:07:41.280 --> 00:07:45.920]   by accident. It was an accident, although it kind of applied that they had planned to launch this
[00:07:45.920 --> 00:07:52.720]   a year ago. They're strong rumors. They're due for an iPad Pro that's been a year.
[00:07:52.720 --> 00:08:00.640]   And of course, we know that they're looking at these new mini LED displays that would be
[00:08:00.640 --> 00:08:09.440]   eventually go to the iPad, go to the phones, and maybe even go to the laptop. They've certainly
[00:08:09.440 --> 00:08:17.280]   ordered them. We know they'd be buying them. And then maybe an Apple TV. Could it be the chip shortage
[00:08:17.280 --> 00:08:21.200]   that's holding this back? Is that because this is getting worse and worse, is chip shortage?
[00:08:21.200 --> 00:08:26.400]   I'm strongly starting to think that that is playing a big part in this, because if you look at
[00:08:26.400 --> 00:08:32.320]   sort of Apple's history of doing events in the spring, a lot of times there's an event in March
[00:08:32.320 --> 00:08:37.520]   or early April. Too much later than that, you start getting pretty close to WWDC, and I think they
[00:08:37.520 --> 00:08:41.440]   like to space stuff out of it. We know that June 7th, that starts this year.
[00:08:41.440 --> 00:08:47.040]   Yeah. So once you get into May, you're like a month out, and it gets a little squishy in
[00:08:47.040 --> 00:08:52.320]   terms of how much stuff you want to release that much in advance. And then I think we look at the
[00:08:52.320 --> 00:08:56.480]   history of things like the iPad Pro getting updated. And the iPads in general, I think, tend to get
[00:08:56.480 --> 00:09:01.360]   some sort of bump in the spring in March or so. The last year, that's when we had the Magic
[00:09:01.360 --> 00:09:06.400]   Keyboard announcement and the revamped 2020 iPad Pros. And then the iPad Air came out last fall
[00:09:06.400 --> 00:09:10.240]   and kind of beat the iPad Pro in a bunch of different specs. And everyone's been kind of
[00:09:10.240 --> 00:09:14.480]   sitting there going like, well, what about the iPad Pro? Like, what is happening with that?
[00:09:14.480 --> 00:09:19.280]   Isn't it going to get a new processor or something? And so there's a lot of things playing parts
[00:09:19.280 --> 00:09:23.680]   here, the pandemic, obviously, but I think that chip shortage, I think that's real. I think that's
[00:09:23.680 --> 00:09:28.400]   playing a big, a big deal in this. Ford stopped making F one 50s, or they're on and off GM just
[00:09:28.400 --> 00:09:34.480]   stopped some of its assembly lines. They can't get the chips. The chip shortage has become massive.
[00:09:34.480 --> 00:09:45.600]   The president is having a summit, 20 top executives from 20 different companies,
[00:09:45.600 --> 00:09:50.560]   they'll be assembling at the White House on Monday. I don't know what you can. I mean,
[00:09:50.560 --> 00:09:56.000]   it's like, we all agree. Yeah, there's a chip shortage. There's a toilet paper shortage. Let's
[00:09:56.000 --> 00:09:58.640]   have a summit. What are you going to do? There's a shortage.
[00:10:00.240 --> 00:10:05.360]   One of the things that insulates Apple, though, is that unlike a lot of the other big manufacturers
[00:10:05.360 --> 00:10:11.760]   for GM, etc, they don't run a lot of their assembly lines in just in time format.
[00:10:11.760 --> 00:10:16.320]   So most of the factories, they assume that they're going to be able to get the parts. They build
[00:10:16.320 --> 00:10:23.520]   a lot of where they announce even exactly. Apple always has a huge lead time, unless
[00:10:24.560 --> 00:10:30.640]   the pandemic struck right before when they were getting into components.
[00:10:30.640 --> 00:10:39.120]   TSMC says they're going to come to this shortage summit on Monday. They make the Apple chips,
[00:10:39.120 --> 00:10:45.360]   and Apple, too, kind of the dismay of other companies, buys up pretty much all of the production,
[00:10:45.360 --> 00:10:52.480]   which has helped them up to now. But at some point, the pipeline runs low. I don't think
[00:10:52.480 --> 00:10:58.720]   it's merely COVID, though. I'm wondering if this also has to do with just massive increase in use.
[00:10:58.720 --> 00:11:07.760]   Internet service providers are now being told that new routers are 60 months off, more than a year
[00:11:07.760 --> 00:11:16.400]   off, because the router manufacturers, like Xilink, can't get the chips to put in the routers.
[00:11:17.280 --> 00:11:24.400]   So this seems to me, this is a kind of a multi-pronged problem. You're right in to about just in time.
[00:11:24.400 --> 00:11:29.040]   A lot of manufacturers just wait until they're going to get the orders, and they order the chips,
[00:11:29.040 --> 00:11:35.040]   but if they're not there, that's it. But I think it's also just COVID and a huge ramp up in consumption.
[00:11:35.040 --> 00:11:40.080]   More people are using computer hardware and chips than ever before.
[00:11:40.080 --> 00:11:44.320]   And everything has chips in it, right? Like every single thing that you rely on now, every day,
[00:11:44.320 --> 00:11:50.160]   being at home, routers, computers, smartphones, tablets, cars, I mean, everything has chips in it.
[00:11:50.160 --> 00:11:56.480]   And a lot of people turn to obviously consumerism in the pandemic to have a little bit of that,
[00:11:56.480 --> 00:12:00.720]   like feeling good about something, right? I'm going to buy a new device,
[00:12:00.720 --> 00:12:03.840]   going to buy a new piece of electronics, or I'm just going to upgrade my infrastructure,
[00:12:03.840 --> 00:12:09.280]   because I'm working from home all the time now. And that has cascaded into, I think, just huge
[00:12:09.280 --> 00:12:13.760]   amounts of demand. And the factories just can't keep up with that.
[00:12:13.760 --> 00:12:21.120]   Yeah. Well, we've seen at least the 25% boost year over year in the sales of desktops and laptops.
[00:12:21.120 --> 00:12:25.920]   And that's just those two segments now include all the support equipment that's sold in order
[00:12:25.920 --> 00:12:30.640]   to make those work remotely. And yeah, that's not an inconsequential boom.
[00:12:30.640 --> 00:12:36.240]   Well, you even, I mean, Denise, you have, Dan, do you have kids?
[00:12:37.040 --> 00:12:40.640]   No. No, Denise, your son is schooling at home, right?
[00:12:40.640 --> 00:12:44.320]   Yes. By choice. Oh, by choice, he could go in?
[00:12:44.320 --> 00:12:51.680]   He could. Yeah. His district instituted an all virtual school this year, which he enrolled in.
[00:12:51.680 --> 00:12:57.840]   So if he had stayed at his same high school, he would have been back and forth. What is this
[00:12:57.840 --> 00:13:02.320]   week look like? What is that? It's going to look like all year. So we've had a lot of consistency,
[00:13:02.320 --> 00:13:05.440]   and it's been a good fit for him. But you see this with a lot of families. Suddenly,
[00:13:05.440 --> 00:13:10.320]   you know, we needed a computer. Now everybody's mom and dad are working from home.
[00:13:10.320 --> 00:13:14.480]   Kids are going to school. Suddenly we need five computers. All of them running Zoom,
[00:13:14.480 --> 00:13:20.640]   and we need, oh, by the way, we need more bandwidth now. It's just a, it's just a, you know,
[00:13:20.640 --> 00:13:28.640]   cascade of needs all of a sudden, thanks to COVID. I mean, it's made these companies very,
[00:13:28.640 --> 00:13:34.960]   very, very rich. But it's also wrong. I would like to see at some point an analysis of what the
[00:13:34.960 --> 00:13:39.440]   replacement of old routers have done to the vulnerability of the edge on the internet,
[00:13:39.440 --> 00:13:42.080]   because we've always talked about how vulnerable some of those are.
[00:13:42.080 --> 00:13:44.880]   Get ready. Especially when it's the sprout or something. Yeah. Get ready.
[00:13:44.880 --> 00:13:48.880]   Now that they're getting replaced, is the attack surface decreased? I'd love to see that.
[00:13:48.880 --> 00:13:52.800]   Right. Or in the case of internet service providers, not getting replaced because
[00:13:52.800 --> 00:13:56.800]   Xilank can't make enough routers. I mean, the only thing that's going to happen tomorrow
[00:13:56.800 --> 00:14:02.560]   at the summit is, I mean, there's nothing they can do about tomorrow, or next week,
[00:14:02.560 --> 00:14:07.440]   or next year, we're talking a couple of years. TSMC has said we're going to spend $100 billion
[00:14:07.440 --> 00:14:15.200]   over the next three years to build new fabs. That, you know, that'll help in three years.
[00:14:15.200 --> 00:14:16.880]   Oh, yeah. That'll be ready next week. Yeah. Right.
[00:14:16.880 --> 00:14:22.720]   I think they're saying 2025, something like that for these new fabs.
[00:14:22.720 --> 00:14:26.800]   Does the demand stretch that long too, right? I mean, like by the time they built them and it
[00:14:26.800 --> 00:14:32.080]   goes, maybe goes back to more of a normal level. Well, I guess we got these all these extra fabs we built.
[00:14:32.080 --> 00:14:35.440]   We'll just have to find new ways to use chips. Of course, that was.
[00:14:35.440 --> 00:14:40.400]   I think we'll do that. Yeah. Maybe that's, I mean, look at what I thought that GM would have,
[00:14:40.400 --> 00:14:44.880]   or Ford would have to stop their truck production because they couldn't get enough microprocessors.
[00:14:44.880 --> 00:14:51.520]   Remember microns? Just like Henry Ford imagined. Yeah, right. Micron. Micron built up a brand
[00:14:51.520 --> 00:14:55.760]   new factory in Texas because there was a huge demand for memory products. And right about the
[00:14:55.760 --> 00:15:00.480]   time that the factory came online, there was a glut and the bottom fell out. They were forced to
[00:15:00.480 --> 00:15:05.600]   sell the factory at a loss. So it's interesting. We, I mean, we're kind of at that. Yes, there's a
[00:15:05.600 --> 00:15:09.520]   demand right now, but is that the man going to stretch out for the years it's going to take
[00:15:09.520 --> 00:15:13.040]   the whole factory? What do you do if you TSMC or Intel? Do you ignore it and just say, well,
[00:15:13.040 --> 00:15:16.400]   we'll make it or do you say, well, we should jump on this because who knows what it's going to be
[00:15:16.400 --> 00:15:21.760]   like at three Intel's building two new fabs in Arizona, which is probably smart because
[00:15:21.760 --> 00:15:27.840]   they're also betting on geopolitical issues, making it a problem getting chips from China
[00:15:28.640 --> 00:15:34.640]   and chips made the US. Hmm, that might be more appealing to a lot of American companies coming
[00:15:34.640 --> 00:15:40.960]   down the road. Maybe this is a situation where the government assists these companies to help
[00:15:40.960 --> 00:15:48.320]   them ride out those downturns in the economy. Wow, you know, government's writing a lot of
[00:15:48.320 --> 00:15:56.480]   checks these days. Yeah. I guess you have to. But wow, maybe I mean, that is a that is a national
[00:15:56.480 --> 00:16:02.000]   security issue. If you can't build your high tech industry, that's a problem. Yeah.
[00:16:02.000 --> 00:16:08.480]   Especially when China is the alternative, right? Right. When your other major superpower rival
[00:16:08.480 --> 00:16:12.800]   is the one who does control a lot of that infrastructure and is building up theirs,
[00:16:12.800 --> 00:16:17.840]   you kind of have to put the money where your mouth is. Well, you got the CEOs of Ford and GM
[00:16:17.840 --> 00:16:22.800]   going to this event. And you know, they're going to say, what are you going to do to help us?
[00:16:23.600 --> 00:16:32.480]   American jobs are at stake. Biden's proposed at least $100 billion to boost to quote boost US
[00:16:32.480 --> 00:16:40.160]   semiconductor production. But that's not going to change anything for years. It's a really it's a
[00:16:40.160 --> 00:16:47.040]   very interesting conundrum. I'm sure that we will put we will write a check. But it could be a bad
[00:16:47.040 --> 00:16:52.320]   check. A check is just a check. What do you do with it? It's not magic. It's not an automatically
[00:16:52.320 --> 00:16:57.680]   creative factory. Yeah. It's a really interesting situation. Who would have thought this? You know,
[00:16:57.680 --> 00:17:05.280]   this is one of the things I love covering tech over the last few decades is that
[00:17:05.280 --> 00:17:12.560]   you try to see the future, but you did you never do. And who would have thought the internet's
[00:17:12.560 --> 00:17:17.840]   biggest problem would have been social media gone awry or that the biggest problem we would have
[00:17:17.840 --> 00:17:22.240]   is so much demand for for computers that we could and cars and everything else that we
[00:17:22.240 --> 00:17:26.880]   couldn't make enough chips to fill them all. Who would have thought that? You know, right now,
[00:17:26.880 --> 00:17:32.000]   there is someone. So there's someone in the basement of a financial services company who's
[00:17:32.000 --> 00:17:37.680]   writing a new algorithm that is taking to account what a pandemic will do to the economy. A pandemic
[00:17:37.680 --> 00:17:46.080]   now increases the tech sector. And that's just until then. Yeah. Yeah. Like as a like, you know,
[00:17:46.080 --> 00:17:50.960]   as a sci fi writer as well, like, I feel like that's something that that often happens. Like you
[00:17:50.960 --> 00:17:56.720]   look at the sci fi of 50s, right? And it's all about moon colonies and spaceships and space exploration
[00:17:56.720 --> 00:18:02.720]   and really nobody imagining the internet and the hugest like change that was going to come to the
[00:18:02.720 --> 00:18:08.160]   all of society was basically being interconnected far less than people talking about like, Oh, well,
[00:18:08.160 --> 00:18:11.840]   clearly, because we're in the space race right now, we're looking forward and assuming the space
[00:18:11.840 --> 00:18:17.120]   race extends decades into the future. If not centuries into the future, when technology
[00:18:17.120 --> 00:18:21.280]   basically took a sharp left turn and nobody really anticipated it. Do you make notes as you're
[00:18:21.280 --> 00:18:26.080]   reading the tech news going, Oh, I got to put that in the, you know, the galactic cold war. That's
[00:18:26.080 --> 00:18:29.760]   got that's going to be. Yeah, I think about it. I think about it. But it's hard, right? Because
[00:18:29.760 --> 00:18:33.360]   you're always like, am I throwing a dart at a dartboard that's not even going to be there?
[00:18:33.360 --> 00:18:40.240]   Right. Right. I wonder if that's inspiring future sci fi writers. We by generation wrote about moon
[00:18:40.240 --> 00:18:45.520]   colonies and interstellar exploration. Is there a sci fi writer right now who's thinking about
[00:18:45.520 --> 00:18:47.920]   what the next Twitter is going to look like? Dan,
[00:18:47.920 --> 00:18:53.040]   I'm going to change the tactics. Not me, but somebody is somebody definitely looking at the
[00:18:53.040 --> 00:18:57.760]   next social media because it is interesting. Actually, you know, we've had Rob Reed on the show.
[00:18:57.760 --> 00:19:04.480]   He wrote a very prescient book about a Facebook like entity knowing everything about everything
[00:19:04.480 --> 00:19:07.920]   that's going to happen. And actually, to the point where it could predict you were going to order a
[00:19:07.920 --> 00:19:12.240]   certain beer as you walked into a bar and had it ready for you, you know, when you sat down or
[00:19:12.240 --> 00:19:17.920]   had the ad anyway ready for you when you sat down. So yeah, it's risky though. You're right,
[00:19:17.920 --> 00:19:22.720]   because because the other side of that is 10 years from now that sci fi feels really tated.
[00:19:22.720 --> 00:19:28.640]   Like, you know, that was clearly he was writing in 1992 because that, you know, nobody cares
[00:19:28.640 --> 00:19:36.720]   about that anymore. All right. Well, chip shortage story number one. I don't, we haven't solved the
[00:19:36.720 --> 00:19:40.160]   problem. Do you think it's going to get worse before it gets better?
[00:19:40.160 --> 00:19:48.080]   Very much demands go anywhere. I mean, yeah. Yeah. It's just it's backing up right now.
[00:19:48.080 --> 00:19:53.440]   I'm actually surprised demand has not plateaued. I would have thought everybody who needed a
[00:19:53.440 --> 00:19:58.640]   computer got one by now. And it would have plateaued. It's not just computers though, right? Like,
[00:19:58.640 --> 00:20:02.480]   it's everything. Cars like we're talking about are, you know, every piece of technology goes in
[00:20:02.480 --> 00:20:07.280]   your house, set top box game console, right? All of this stuff. Yeah, you still can't get a
[00:20:07.280 --> 00:20:14.160]   PlayStation fiber Xbox series X to save your life. Unless you have a teenager at home, like I do
[00:20:14.160 --> 00:20:19.520]   it. He managed to score one. But he's got lots of time on the internet to figure that out.
[00:20:19.520 --> 00:20:26.000]   All right, let's take a little break. We've got a lot of things to talk about, including Denise Howl.
[00:20:27.600 --> 00:20:33.840]   The law one of the longest running court cases, 11 years in the making Google versus Oracle,
[00:20:33.840 --> 00:20:40.640]   the Supreme Court put an end to it this week. And I just can't wait to hear what you have to say
[00:20:40.640 --> 00:20:47.120]   about it. I find this fascinating, but we need you to explain it all. All righty. Thank you.
[00:20:47.120 --> 00:20:51.600]   Our show today brought to you, by the way, it's great to have Denise here, Father Robert,
[00:20:51.600 --> 00:20:56.240]   Dan Morin. It's kind of like old home week. That's going to be a fun twit. Our show today brought to
[00:20:56.240 --> 00:21:01.040]   you by Zippa Cruder. Hiring's coming back. If you're one of those companies, it's actually
[00:21:01.040 --> 00:21:08.320]   staffing up. Hey, thank you. That's wonderful. And you want to know about Zippa Cruder, because
[00:21:08.320 --> 00:21:17.680]   it is simply put the easiest way to hire. If you're hiring for spring, you've got a position.
[00:21:17.680 --> 00:21:25.760]   You know, I mean, somewhere out there, the perfect person, right? How many times do we compromise,
[00:21:25.760 --> 00:21:28.800]   though? Well, I'm sure the perfect person is out there, but this is the best we could do.
[00:21:28.800 --> 00:21:34.320]   And maybe you'll regret it down the road. It's so important, because a company is just made up of
[00:21:34.320 --> 00:21:41.200]   people and the people you hire can make or break your company. So don't compromise. The best person
[00:21:41.200 --> 00:21:45.920]   for the job is out there. You just need a way to find that person. You're searching for a needle
[00:21:45.920 --> 00:21:51.200]   in a haystack. There's no better way to do it with Zippa Cruder. First of all, as soon as you
[00:21:51.200 --> 00:21:56.720]   post on Zippa Cruder, your post goes to 100 plus job sites with one click of the mouse.
[00:21:56.720 --> 00:22:03.520]   So you're reaching out, you're casting the broadest net possible. It goes to social networks. It goes
[00:22:03.520 --> 00:22:09.920]   everywhere. And it goes everywhere in the country with and for every industry, whether you're
[00:22:09.920 --> 00:22:16.640]   hiring a civil engineer in New York or an attorney in Colorado, even if you're hiring a mascot in
[00:22:16.640 --> 00:22:23.600]   Missouri. Yes, that is a real posting. Zippa Cruder can put it out everywhere, but then
[00:22:23.600 --> 00:22:28.320]   then it does something pretty remarkable because people come to Zippa Cruder to apply for jobs.
[00:22:28.320 --> 00:22:34.960]   They have on file hundreds of thousands of current resumes. They apply their matching technology to
[00:22:34.960 --> 00:22:40.720]   their existing resumes. They match your job to people looking for work. When they get a good match,
[00:22:40.720 --> 00:22:44.400]   they actively invite that person to apply. They say, we found a job for you. That's great for
[00:22:44.400 --> 00:22:48.960]   people applying at Zippa Cruder, but it's great for you too, because it means you're going to get
[00:22:48.960 --> 00:22:56.240]   qualified applicants fast. Four out of five employers, 80%, who post on Zippa Cruder get a
[00:22:56.240 --> 00:23:01.360]   quality candidate within the first day. Now we've used Zippa Cruder many times. In our experience,
[00:23:01.360 --> 00:23:08.080]   it's not been a day, it's been hours. I mean, they start rolling in fast. It's an amazing solution.
[00:23:08.080 --> 00:23:12.720]   And when I say rolling in, it doesn't come to your phone, your inbox, it goes into the Zippa Cruder
[00:23:12.720 --> 00:23:18.320]   interface, which makes it very easy. They recommend people who fit well. You could scan the resumes
[00:23:18.320 --> 00:23:22.240]   because they're all pre-formatted. So it's easy to read. And then you could pick the right person
[00:23:22.240 --> 00:23:29.680]   fast. Try it right now for free. Only our listeners get this link, zippracruder.com/twitzippracruder
[00:23:29.680 --> 00:23:38.560]   dot com slash twit for any industry anywhere. It's the best zippracruder.com slash T-W-I-T.
[00:23:38.560 --> 00:23:43.440]   You can try it free right now. We thank you so much, Zippa Cruder, for supporting this week in
[00:23:43.440 --> 00:23:48.240]   tech. We thank you this week in tech listeners for supporting us by using that URL so they know
[00:23:48.240 --> 00:24:00.560]   you saw it here, zippracruder.com slash twit. Google Oracle 11 years in the making. I have to say
[00:24:00.560 --> 00:24:08.400]   I did not see this one coming Denise. No, I didn't see it coming either. And yes, 11 years is a
[00:24:08.400 --> 00:24:15.760]   long time. But I just want to put it in perspective. 11 years to go up and down the appellate system
[00:24:15.760 --> 00:24:21.920]   the way this one did is really not that long a period of time in our US judicial system. I looked
[00:24:21.920 --> 00:24:27.840]   this up for you, Leo, because you were talking about how long running the case was. Apparently
[00:24:27.840 --> 00:24:34.000]   there was a case in the United States that holds the dubious distinction of being the longest courtroom
[00:24:34.000 --> 00:24:44.720]   battle. It involved an estate and it went on for 57 years. That's like a lifetime.
[00:24:44.720 --> 00:24:51.040]   The plaintiff, it's maybe several lifetimes. Myra Clark Gaines was her name. And of course,
[00:24:51.040 --> 00:24:56.160]   there's Dickens Bleak House, which revolves around Jarn Dies versus Jarn Dies famously.
[00:24:56.160 --> 00:25:01.840]   Oh yeah, yeah, yeah. Find exactly how long that famously is. It's still going 150 years later.
[00:25:01.840 --> 00:25:07.440]   Yes, exactly. Well, Microsoft DOJ went on from 98. Well, that might have been
[00:25:07.440 --> 00:25:12.800]   quite 20 years. It went on for quite a while. So I guess it isn't unheard of. What's really
[00:25:12.800 --> 00:25:19.360]   feels unusual is is the way this went because Google would win and Google would lose and Google
[00:25:19.360 --> 00:25:24.000]   would win and Google would lose the the appellate court would throw it back to the lower court
[00:25:24.000 --> 00:25:27.680]   or throw it back to the appellate court. This thing was ping ponged back and forth.
[00:25:27.680 --> 00:25:34.560]   Yeah, there were a lot of so there were a lot of issues here that required a jury to help
[00:25:34.560 --> 00:25:40.240]   work them out when you have a fair use argument like the one that Google ultimately prevailed on.
[00:25:40.240 --> 00:25:45.920]   There are a lot of facts that a jury has to determine, but then it's ultimately up to the court and
[00:25:45.920 --> 00:25:52.000]   the judicial system once they have all those factual determinations to decide if fair use
[00:25:52.000 --> 00:25:59.440]   happened. So there's that complicating factor we needed. It couldn't just happen in the absence
[00:25:59.440 --> 00:26:04.800]   of a jury. So some of these issues were jury issues that go back to the trial that started
[00:26:04.800 --> 00:26:14.640]   almost 11 years ago now. Actually, the trial started almost instantly after Oracle bought Sun.
[00:26:15.520 --> 00:26:23.120]   And Sun had invented Java, which was a right once run anywhere programming language is still
[00:26:23.120 --> 00:26:27.360]   probably to this day with the most popular programming language in the world. Certainly,
[00:26:27.360 --> 00:26:33.600]   it's right up there. I remember interviewing Jonathan Schwartz, who was the CEO of Sun at the time,
[00:26:33.600 --> 00:26:37.760]   and he couldn't say this at the time, but during the interview was on triangulation. He said,
[00:26:37.760 --> 00:26:45.680]   you know, I knew something was up because when we started meeting with Oracle about all the assets,
[00:26:45.680 --> 00:26:53.280]   and I started talking about Java, I could see the lawyers in the room lighting up. Oracle immediately
[00:26:53.280 --> 00:27:02.000]   recognized what this meant. In fact, at one point, the courts had told Google, "You owe Oracle $9
[00:27:02.000 --> 00:27:12.320]   billion for infringing on their Java API." So this goes way back to the purchase of Sun. Might even
[00:27:12.320 --> 00:27:18.480]   have been the motivating reason Oracle purchased Sun. But remember, you have to go back even before
[00:27:18.480 --> 00:27:23.200]   that because that was the start of the legal case. The start of the saga actually happened,
[00:27:23.200 --> 00:27:31.600]   I think in 2003, when Google Schmidt approached Sun and said, "We want to license Java SE.
[00:27:31.600 --> 00:27:35.680]   We want to use a brand right?" They wanted to use it for Android, but they had a stipulation.
[00:27:35.680 --> 00:27:40.480]   They said, "Look, we want to open source this." And Sun didn't want to do that because they felt
[00:27:40.480 --> 00:27:44.960]   that they were just going to fork it, and then they would lose their licensing fee. So Google
[00:27:44.960 --> 00:27:50.560]   decided to call off the deal, and instead they made a clean room version of Java. So the entire case
[00:27:50.560 --> 00:27:56.320]   hinges off was that actually a clean room. No, it doesn't. But it didn't the early days,
[00:27:56.880 --> 00:28:04.080]   but it was eventually, now, correct me if I'm wrong, Denise Howell, because you know law,
[00:28:04.080 --> 00:28:10.240]   this is complicated. But eventually that was not the issue. The issue was whether a clean room
[00:28:10.240 --> 00:28:18.000]   version of Java was somehow protected, whether the APIs that Google duplicated, they didn't use
[00:28:18.000 --> 00:28:25.280]   Oracle code, they didn't use Sun's code, they just duplicated the APIs, because otherwise a
[00:28:25.280 --> 00:28:31.200]   program expecting Java wouldn't be able to use whatever Dalvik or whatever Google ended up writing.
[00:28:31.200 --> 00:28:38.400]   And then so at one point, a jury, I think, ruled that APIs were copyrightable.
[00:28:38.400 --> 00:28:46.080]   That was the turning point. Yes, it was. And you're right. There's no question that copying happened
[00:28:46.080 --> 00:28:53.680]   here. And the opinion itself, I recommend Justice Breyer's opinion. It's 62 pages long as I have
[00:28:53.680 --> 00:28:59.440]   it open on. Did he seem to really understand it? Okay, so this is really interesting, I think.
[00:28:59.440 --> 00:29:06.000]   The very beginning of the opinion starts out with, and let me find the exact language here,
[00:29:06.000 --> 00:29:14.240]   because you'll love it. One second. I'll just mention a couple of, while you're looking that
[00:29:14.240 --> 00:29:21.760]   up, turning points on this, Judge Alsop, who was covering this case in the Northern District of
[00:29:21.760 --> 00:29:29.440]   California, the first jury trial, Oracle v. Google, very famously in 2012, learned Java,
[00:29:29.440 --> 00:29:37.040]   learned Java so that he could see if it was, because the issue at this point was
[00:29:37.040 --> 00:29:45.120]   what was copied. And Oracle came down to one routine, the range check routine,
[00:29:46.080 --> 00:29:53.920]   that Oracle felt Google literally copied. The judge learned Java and wrote his own range check
[00:29:53.920 --> 00:30:01.520]   routine, just to see if that was a copy or just the logical, likely way to write such a thing.
[00:30:01.520 --> 00:30:05.200]   The judge said, "I couldn't have told you the first thing about Java before this trial,
[00:30:05.200 --> 00:30:09.360]   but I have done and still do a lot of programming myself and other languages. I've written blocks
[00:30:09.360 --> 00:30:15.280]   of code like range check 100 times or more. I could do it. You could do it. It's so simple.
[00:30:16.240 --> 00:30:26.480]   He wrote it in Java and the judge who learned Java, there was a line from his decision that said,
[00:30:26.480 --> 00:30:32.240]   "So long as the specific code used to implement a method is different, anyone is free under the
[00:30:32.240 --> 00:30:37.040]   copyright act to write his or her own code to carry out the exact same function as long.
[00:30:37.040 --> 00:30:42.400]   It does not matter that the declaration or method header is identical." That was his final verdict.
[00:30:42.400 --> 00:30:48.480]   Yeah. But that all went out the window because it ended up being, all right, all right, you didn't
[00:30:48.480 --> 00:30:58.000]   steal the code. But we have copied the code. Nobody's ever just been copied. They got away.
[00:30:58.000 --> 00:31:04.000]   37 examples, which included documentation. So there were comments that were on the run.
[00:31:04.000 --> 00:31:07.840]   They clearly copied it. But that wasn't what they were suing over in the long run, right?
[00:31:09.200 --> 00:31:14.320]   I'm so confusing. The jury found that it was a fair use. And ultimately, that's the determination
[00:31:14.320 --> 00:31:20.400]   that the Supreme Court got behind, got behind in a big way. And this was very important because
[00:31:20.400 --> 00:31:28.560]   if you could say, "Oh no, I own an API. No one can copy it." It would break the world as we know it.
[00:31:28.560 --> 00:31:32.560]   You couldn't write code anymore. Right. And actually, I should correct myself because I just
[00:31:32.560 --> 00:31:37.520]   contradicted what I said earlier. Juries find facts that go to the fair use determination. It
[00:31:37.520 --> 00:31:45.120]   was Judge Alsop, the wonderful guy who threw himself so headlong into this so that he could
[00:31:45.120 --> 00:31:50.880]   understand it so thoroughly that made that initial fair use determination.
[00:31:50.880 --> 00:31:54.800]   So he says, "It's okay. That's fair use." That's fair use. And that's-
[00:31:54.800 --> 00:31:58.800]   And ruled in Google's favor. Yes. But then Oracle Appeals.
[00:32:00.800 --> 00:32:08.800]   And there's a whole question. The issue that you hit on earlier, and it was still very much an
[00:32:08.800 --> 00:32:14.640]   issue when it got to the Supreme Court, the copyright ability of an API, was something that
[00:32:14.640 --> 00:32:22.160]   the Federal Circuit, which was the Court of Appeal here, found yes. APIs can be copyrightable.
[00:32:22.160 --> 00:32:31.680]   The Supreme Court has has punted on that. They've decided that since the fair use argument is
[00:32:31.680 --> 00:32:38.960]   established, it doesn't matter whether APIs are copyrightable or not. They just assumed
[00:32:38.960 --> 00:32:44.240]   for purposes of writing their decision that they were and decided we don't have to decide
[00:32:44.240 --> 00:32:50.240]   that critical issue. There's a lot- this is a changing area of the law. There's this technology
[00:32:50.240 --> 00:32:54.800]   changing all the time. This may be something that we want to re-examine at some point or another.
[00:32:54.800 --> 00:33:02.960]   They did reverse that part of the Federal Circuit's decision and decided we're not going to say
[00:33:02.960 --> 00:33:07.520]   whether it's copyrightable or not at this point. Was that wise to judge that? Probably.
[00:33:07.520 --> 00:33:16.000]   Well, Justice Thomas didn't think so. He dissented. But personally, I think it probably was wise.
[00:33:16.800 --> 00:33:25.920]   I think that there's a lot to unpack in the squishy nature as far as the law is concerned of an API.
[00:33:25.920 --> 00:33:31.120]   So they darched it. It very much straddles this boundary between copyright and patent.
[00:33:31.120 --> 00:33:37.600]   So they said it might be copyrightable and might not be, but we're going to rule that it's fair
[00:33:37.600 --> 00:33:42.320]   use so it doesn't matter. That's right. So even if we say that it's copyrightable,
[00:33:42.320 --> 00:33:48.000]   which we're not saying, even if it were copyrightable, it's fair use and that's all that matters.
[00:33:48.000 --> 00:33:55.040]   But what I was going to say- Justice Breyer's opinion, yes. Yes. Praises to Judge Alsop for his
[00:33:55.040 --> 00:34:00.800]   understanding of the critical issues at hand. The court here, you can scratch your head about
[00:34:00.800 --> 00:34:06.240]   whether the actual justices of the Supreme Court have a thorough going understanding of API calls
[00:34:06.240 --> 00:34:14.160]   or not. But if you read the first part of this decision, it gives you faith in our court system
[00:34:14.160 --> 00:34:19.520]   and particularly the vehicle of the Supreme Court to really thoroughly understand issues.
[00:34:19.520 --> 00:34:25.360]   Whether the justices do or not, the clerks do, they get a lot of friends of the court briefs,
[00:34:25.360 --> 00:34:30.560]   and they can write an opinion. I really would love to hear from some of our
[00:34:30.560 --> 00:34:35.760]   programmer listeners if they read through the first part of this opinion, which is prefaced by
[00:34:35.760 --> 00:34:41.680]   through an API a programmer can draw upon a vast library of pre-written code to carry out complex
[00:34:41.680 --> 00:34:48.400]   tasks for laypersons, including judges, juries, and many others. Some elaboration of this description
[00:34:48.400 --> 00:34:55.840]   may prove useful and then it launches into pages of detail about the facts of this case.
[00:34:55.840 --> 00:35:01.280]   And as I, you know, I'm not a programmer, I'm a lawyer, but I read through this and it rings
[00:35:01.920 --> 00:35:09.360]   really accurately to me. So I'd be interested in how our geekier listeners react to it as well.
[00:35:09.360 --> 00:35:14.880]   I think the court, you know, in my humble opinion, did a good job here of trying to grasp the actual
[00:35:14.880 --> 00:35:21.280]   actual issues at hand. And the dissent from Justice's Alito and Thomas really was over this copyright
[00:35:21.280 --> 00:35:27.280]   thing. They felt the court should have ruled on whether you could copyright an API. Yes.
[00:35:27.280 --> 00:35:36.000]   I think I think Breyer's opinion was this is, as you said, in flux, we should defer this as
[00:35:36.000 --> 00:35:41.760]   long as we can because this is a complicated thing and things are changing. We can rule on this part
[00:35:41.760 --> 00:35:46.960]   of it and dispose of this matter without having making any assumption about copyright. Should it
[00:35:46.960 --> 00:35:52.880]   really be the legislative thing? Should it be Congress that decides, you think, on the right of the way?
[00:35:52.880 --> 00:35:58.640]   Very much so. I mean, that's where this confusion exists in the difference, you know, the actual
[00:35:58.640 --> 00:36:05.040]   descriptions of what's patentable and what's copyrightable that come up to us from the legislature.
[00:36:05.040 --> 00:36:05.520]   Right.
[00:36:05.520 --> 00:36:10.960]   So if we're in a gray area here, I think the court wisely decided, you know, it's not for us to
[00:36:10.960 --> 00:36:17.760]   make that call. And perhaps the legislature will want to do so, recognizing that we're in a gray
[00:36:17.760 --> 00:36:23.920]   area. I agree with you. Breyer's description of an API is actually pretty adept.
[00:36:23.920 --> 00:36:32.800]   And I think he understood the issues involved with making an API unusable. Fair use is probably a good
[00:36:32.800 --> 00:36:39.120]   way to handle this until Congress goes. Go ahead, Father. Is this legal can kicking? I mean,
[00:36:39.120 --> 00:36:43.440]   since they haven't really decided that it's going to come up again. At some point,
[00:36:43.440 --> 00:36:47.600]   someone's going to try to make a play for saying, you can't use my API without paying me.
[00:36:47.600 --> 00:36:54.400]   Well, I think the fair use determination had sat off, right? I mean, if somebody does what
[00:36:54.400 --> 00:37:00.720]   Google did here, again, they used a small part less than 1%, I think, of the overall
[00:37:00.720 --> 00:37:08.960]   code that was at issue. So they, you know, you go through the fair use factors, did you use,
[00:37:08.960 --> 00:37:14.560]   you know, the least amount that you could? And is it transformative? And really what this opinion
[00:37:14.560 --> 00:37:21.200]   gets to is, you know, what are the goals of the copyright act and what, you know, how are they
[00:37:21.200 --> 00:37:27.040]   supposed to serve innovation going forward? Are we consistent with that goal with those goals in
[00:37:27.040 --> 00:37:36.400]   this decision? And the court lays out how they feel that, you know, it much like there's copying
[00:37:36.400 --> 00:37:42.080]   involved in the existence of a search engine, but we all need search engines and they serve a very
[00:37:42.080 --> 00:37:48.480]   practical and useful and innovative purpose for us. Even though none of those search engine
[00:37:48.480 --> 00:37:55.440]   precedents were cited in this decision, it's a similar kind of reasoning where you need to
[00:37:55.440 --> 00:38:05.920]   look at all the factors and decide whether fair use is necessary here to make, you know,
[00:38:05.920 --> 00:38:12.000]   the cogs of the machine turn. And I think that's, although the court didn't put it that way,
[00:38:12.000 --> 00:38:19.040]   and I'm putting it less elegantly than the court put, I feel like it's a, there was a practical
[00:38:19.040 --> 00:38:28.240]   basis to this opinion. So what is, so in, at least in the, in the usage that I understand of fair use,
[00:38:28.240 --> 00:38:35.040]   there are some tests for fair use. Ultimately, it's up to a court to decide it's a defense, not,
[00:38:35.040 --> 00:38:38.480]   it's not a proactive thing, it's only a defense if somebody sues you, but,
[00:38:38.480 --> 00:38:41.440]   right. But there are some tests that are commonly used.
[00:38:41.440 --> 00:38:48.400]   Do those also apply to this? What did the, the justices talk about how this, why this is fair use?
[00:38:48.400 --> 00:38:52.640]   Yeah, very much so. And what, what did they say?
[00:38:52.640 --> 00:39:01.680]   Why is this fair use? The amount of copying was a tiny bit compared to the actual work. They looked
[00:39:01.680 --> 00:39:07.520]   at the nature of the work is this is not like copying music or art or, you know, this is
[00:39:07.520 --> 00:39:18.160]   copying as a means to an end, a very practical functional end, and various other factors that
[00:39:18.160 --> 00:39:24.000]   went into the analysis. Yeah, the, because fair use is a legal issue, the court here got to look at,
[00:39:24.000 --> 00:39:27.840]   you know, it didn't have to defer. It defines it. It defines. Yes, didn't have to defer to what
[00:39:27.840 --> 00:39:33.440]   had been done below and could go through and decide why it felt that the factors applied.
[00:39:33.440 --> 00:39:36.400]   Was it wasn't one of the factors I read through this very briefly, but like wasn't one of the
[00:39:36.400 --> 00:39:43.200]   factors like essentially the financial gain or profit of it? Like if you're using it for money
[00:39:43.200 --> 00:39:48.080]   making, I guess, because like Android is, is open source. They kind of sidestep that.
[00:39:48.080 --> 00:39:54.640]   Well, Google, as Oracle was quick to point out, made a lot of money on the Android.
[00:39:54.640 --> 00:40:04.400]   Sure. Yeah. That's where the nine billion came from. Right. But yes, it was, you know, there are
[00:40:04.400 --> 00:40:10.800]   a whole host of factors that go into deciding whether something is fair use. And certainly,
[00:40:10.800 --> 00:40:15.120]   whether it's commercial or non-commercial is one of them, but again, they all sort of weigh
[00:40:15.120 --> 00:40:23.200]   against each other. And what the court seemed to be most concerned with here is there was a lot
[00:40:23.200 --> 00:40:30.480]   of discussion of the investment of time and resources and brain power of the people
[00:40:30.480 --> 00:40:40.800]   working with the APIs. And was that all going to go for nothing? Shouldn't that be something
[00:40:40.800 --> 00:40:48.080]   that's meaningful? And the court certainly felt that it should. Did the court consider damage to
[00:40:48.080 --> 00:40:54.640]   Oracle? Did they consider the Google made a lot of money off Oracle's back? Was that,
[00:40:54.640 --> 00:40:56.800]   I mean, I'm sure that's how Oracle feels about it.
[00:40:56.800 --> 00:41:04.800]   I'm sure it is. No, this seemed to be more focused on the actual fair use analysis.
[00:41:04.800 --> 00:41:15.200]   And given that there was no recoverable harm because fair use applied, you wouldn't get into
[00:41:15.200 --> 00:41:22.640]   the damages phase of anything. The history of this case is hysterical because this is not the
[00:41:22.640 --> 00:41:29.920]   first time the Supreme Court has seen it. It went up to the Supreme Court over this fair use thing,
[00:41:29.920 --> 00:41:33.600]   and they sent it back to the lower court saying, no, no, you have to decide this.
[00:41:33.600 --> 00:41:40.160]   The lower court decided it was not fair use that Oracle had won, at which point Google appeals
[00:41:40.160 --> 00:41:44.800]   once again to the Supreme Court. Now, this time the Supreme Court can't send it back to the
[00:41:44.800 --> 00:41:49.200]   lower court because the lower court did in fact rule that it was not fair use, so they had to
[00:41:49.200 --> 00:41:56.400]   take this on. And this is why I was concerned. I felt like this is not looking good for Google.
[00:41:56.400 --> 00:41:59.920]   I wasn't rooting for Google because they're Google or against Oracle because they're Oracle,
[00:41:59.920 --> 00:42:05.440]   although that's tempting. But I really felt like any determination that you could close off
[00:42:05.440 --> 00:42:12.320]   APIs would be really damaging. As the justices, by the way, or as Breyer pointed out, that the
[00:42:12.320 --> 00:42:15.920]   whole industry relies on this free sharing of information.
[00:42:15.920 --> 00:42:24.800]   That's why I keep thinking, again, it's a totally different body of law, but the courts that had
[00:42:24.800 --> 00:42:32.640]   to wrestle with copyright when it came to search engines had similar conundrums in front of them.
[00:42:32.640 --> 00:42:38.640]   Yes, there's copying happening here. And there it was copying of things that were
[00:42:39.120 --> 00:42:44.880]   arguably creative, all the things that go into your search results, the art, the writing,
[00:42:44.880 --> 00:42:49.200]   everything else that you can search for on the internet. And that stuff is being copied so that
[00:42:49.200 --> 00:42:57.600]   you can find them. And yet the courts that considered whether search engines should exist
[00:42:57.600 --> 00:43:03.120]   found, yeah, they should. And I feel like that's what's there's some of that kind of
[00:43:03.120 --> 00:43:07.360]   pragmatic approach going on here. People will be reading this opinion for a long time,
[00:43:07.360 --> 00:43:13.120]   and I imagine it'll be taught in law schools. It's just, and again, I'm not a lawyer,
[00:43:13.120 --> 00:43:19.280]   but just in my reading of it, I found it fascinating. And I thought the reasoning was very intriguing.
[00:43:19.280 --> 00:43:29.360]   Breyer does take on Thomas' dissent and explains why he feels like we shouldn't necessarily rule on
[00:43:29.360 --> 00:43:38.880]   this copyright issue. It's really it. I think it's historic. Would you agree that this is historic?
[00:43:38.880 --> 00:43:44.080]   I do. And I'm, you know, I think we'd all kind of forgotten about it.
[00:43:44.080 --> 00:43:47.840]   Knew it was working. I kept getting reminded.
[00:43:47.840 --> 00:43:54.880]   Yeah, you're right. It went on for so long. Yeah, almost as long as tweets been around,
[00:43:54.880 --> 00:43:57.840]   we've been covering it from the very beginning. And then a pandemic came along and delayed it
[00:43:57.840 --> 00:44:01.680]   further. Yeah. One of the things that we talked about on
[00:44:01.680 --> 00:44:09.040]   Twiad was the fourth test for fair use. And that is, did you impinge upon the original owner's
[00:44:09.040 --> 00:44:14.720]   ability to exploit their work? And as you said in the start, Leo, Java is the most used programming
[00:44:14.720 --> 00:44:20.560]   language in the world. So obviously it hasn't hurt them. It hasn't decreased the value of the
[00:44:20.560 --> 00:44:26.240]   thing that they own. So I mean, I think that's for me, looking down the decision, that's probably
[00:44:26.240 --> 00:44:33.760]   the most compelling part, which is if if Google had come in and done this clean room copy of Java
[00:44:33.760 --> 00:44:40.880]   SE, and now no one was using Java from Oracle slash Sun, then yes, you could claim that you've
[00:44:40.880 --> 00:44:46.240]   destroyed something that was a value. And therefore it's not fair use. But as it is right now,
[00:44:46.240 --> 00:44:52.000]   it's just increased the value of that material position. In fact, Briar addresses that saying,
[00:44:52.000 --> 00:44:57.600]   this is all about a version of Android that is so old, only about 7% of devices still run it.
[00:44:57.600 --> 00:45:03.360]   It's basically obsolete. So you can't claim that there was any commercial harm at this point.
[00:45:03.360 --> 00:45:09.840]   It's really fascinating. And they did, as you said, Denise, they did say those four tests
[00:45:09.840 --> 00:45:16.720]   are not the only tests. It's not limited to those tests. And the court felt very clearly that it
[00:45:16.720 --> 00:45:23.600]   was up to it to decide it didn't have to. It was not bound by those four tests at all. So that's
[00:45:23.600 --> 00:45:30.480]   also will probably does this become? Does this decision become important for fair use down the
[00:45:30.480 --> 00:45:36.880]   road? Yeah, this is the most recent fair use decision we have from the highest court in the land.
[00:45:36.880 --> 00:45:43.200]   And it's good news for fair use. Good news for fair use. Good, because I use for use all the time.
[00:45:45.040 --> 00:45:50.560]   Well, I mean, I say that with an asterisk. Every fair use case is distinct, right? That's why you
[00:45:50.560 --> 00:45:57.040]   have to have juries figure out the facts. And then the courts figure out whether the fair use factors
[00:45:57.040 --> 00:46:04.480]   apply. It's not like, okay, we all get fair uses now, the law of the land. No, it's case by case,
[00:46:04.480 --> 00:46:10.560]   every single time. Yeah, it's if you're interested, it's worth reading the decisions online at
[00:46:10.560 --> 00:46:17.520]   supremecourt.gov. And it is what is it? What is it? 59 pages? 62 pages. The PDF. Wow.
[00:46:17.520 --> 00:46:24.080]   But it's, but it's, it's actually, I love reading this kind of stuff, because it shows that these
[00:46:24.080 --> 00:46:29.840]   actually are in these are smart people really exercising their intellect and really trying to
[00:46:29.840 --> 00:46:36.960]   size. They're not it's not like Congress. It's not it's not about compromising. It's trying to
[00:46:36.960 --> 00:46:42.320]   find the truth. And it's really great. I just it's inspiring to read it. Did Oracle has Oracle
[00:46:42.320 --> 00:46:47.760]   responded in any way to this? I'm sure they're disappointed, but it's yeah, they're they're appalled.
[00:46:47.760 --> 00:46:55.600]   They're they're calling it a travesty. A travesty. Yeah. We gave so much money to President Trump,
[00:46:55.600 --> 00:47:01.840]   we got three justices on the bench. By the way, Amy Comey Barrett was not on the bench when this
[00:47:01.840 --> 00:47:07.760]   was first argued. So she was not allowed to vote. It was only the other eight justices that voted.
[00:47:07.760 --> 00:47:14.160]   Right. Oracle is characterizing this is this is what you get when you have a monopoly. This is how
[00:47:14.160 --> 00:47:19.520]   we Google's been able to drag this out for years and years and you know, just keep that bone in
[00:47:19.520 --> 00:47:26.640]   its teeth. And so it's really kind of trying to cast this politically as Google couldn't,
[00:47:26.640 --> 00:47:29.920]   we're the little guy, Oracle saying, I hope you all appreciate that.
[00:47:29.920 --> 00:47:35.840]   Oracle saying that with the way they've conducted themselves over the last three decades is no,
[00:47:35.840 --> 00:47:42.160]   no, you don't get to do that. Sorry. They say Google stole Java. And this is why regulatory
[00:47:42.160 --> 00:47:46.400]   authorities around the world and in the United States are examining Google's business practices.
[00:47:46.400 --> 00:47:52.480]   So here's what's happening. Okay, we lost to the Supreme Court. You go get them FTC, go get them,
[00:47:52.480 --> 00:48:00.880]   go get them. Go. Right. Wow. Wow. It was it's a huge week. Supreme Court actually
[00:48:00.880 --> 00:48:04.960]   did not stop with that. There were a number of other important decisions,
[00:48:04.960 --> 00:48:10.400]   but that was clearly clearly the most important. I'm so glad you were here, Denise.
[00:48:10.400 --> 00:48:16.800]   It's still not completely clear, but I feel like the good guy is one. And I don't mean Google. I mean,
[00:48:17.440 --> 00:48:22.720]   the internet like I feel like it's a very well reasoned decision. But I tend to,
[00:48:22.720 --> 00:48:30.320]   I should put all of my content comments in the context of the fact that I am a fan of fair use
[00:48:30.320 --> 00:48:38.000]   and being applied in in proper circumstances. And I feel like it took 11 years for courts to
[00:48:38.000 --> 00:48:43.280]   you know, struggle through these complicated issues and decide these were proper circumstances.
[00:48:43.280 --> 00:48:48.320]   Yeah, fair use has always been tricky and difficult. Although I remember the EFF put out a bumper
[00:48:48.320 --> 00:48:53.200]   sticker, which I used to have. I don't know where it is. It says fair use has a posse.
[00:48:53.200 --> 00:48:59.360]   You remember. And the posse includes eight justices on the United States Supreme Court. That's
[00:48:59.360 --> 00:49:05.520]   pretty good. Actually, we'll take two off for the dissent. So six, that's that's a majority that counts.
[00:49:05.520 --> 00:49:11.360]   Yes. And it's good to see that the court can function, you know, with eight justices. Yeah.
[00:49:12.000 --> 00:49:18.160]   Yeah. Well, in this case, we'll do even better with 13 or 100. Why not why stop there? Let's have
[00:49:18.160 --> 00:49:22.640]   two justices for every state in the union. What do you think? Oh, no. What do you think?
[00:49:22.640 --> 00:49:28.640]   No, I'm kidding. I didn't say that. No, I didn't say that. Yeah. No, six to majority is a good solid
[00:49:28.640 --> 00:49:33.840]   majority. That's a good way to win. Although you win by one vote. It's just as good. Our show
[00:49:33.840 --> 00:49:41.200]   today brought to you by Amazon pharmacy. I actually very well remember pill pack because they were
[00:49:41.200 --> 00:49:46.960]   a sponsor on Twitter and I use them for a long time for meds and for vitamins and so forth.
[00:49:46.960 --> 00:49:52.560]   Amazon bought pill pack and they're using the amazing pill pack technology for Amazon pharmacy.
[00:49:52.560 --> 00:49:59.440]   And I got to say it's fantastic. Amazon pharmacy works with your doctor. The prescriptions are
[00:49:59.440 --> 00:50:05.680]   sent directly to Amazon pharmacy. They fill them and deliver them right to your door. No need to
[00:50:05.680 --> 00:50:10.880]   leave the house. That's one less errand and a, you know, a lot safer for you. It'll save you a lot
[00:50:10.880 --> 00:50:14.880]   of time by delivering the medication to your door. You don't have to wait in line in pharmacy.
[00:50:14.880 --> 00:50:19.440]   It's a lot easier because the doctor sends the prescription straight to Amazon pharmacy.
[00:50:19.440 --> 00:50:24.000]   Amazon pharmacy works with your doctor to make sure not only is the prescription accurate
[00:50:24.000 --> 00:50:27.520]   but that you're getting exactly what you need. And that's great. And of course,
[00:50:27.520 --> 00:50:32.080]   there's a pharmacist online. You can always talk to a pharmacist at Amazon pharmacy. Yes,
[00:50:32.080 --> 00:50:36.640]   you can use your insurance. It works with most insurance plans nationwide. But if you don't have
[00:50:36.640 --> 00:50:42.160]   insurance, you can also as an Amazon Prime member get to get the great discounts too. And you, of
[00:50:42.160 --> 00:50:46.960]   course, if you're a Prime member, you always get free today delivering. I have one of the things I
[00:50:46.960 --> 00:50:52.320]   really love about Amazon pharmacies. You know the price ahead of time. How many times you've gone
[00:50:52.320 --> 00:50:56.720]   to the pharmacy, you give me your prescription. They fill it. You don't find out how much it's
[00:50:56.720 --> 00:51:01.440]   going to cost till you hit to the cash register. And I got to say, that's usually a little bit of
[00:51:01.440 --> 00:51:07.680]   sticker shock. With Amazon pharmacy, you'll see the copay. You'll see the price both with insurance
[00:51:07.680 --> 00:51:13.520]   and without insurance. And sometimes you can make a better economic decision, say, you know,
[00:51:13.520 --> 00:51:19.200]   I'm not going to use insurance on this. It's really good to know ahead of time what the price is.
[00:51:19.200 --> 00:51:24.880]   Of course, your medical information is protected and safe. They never share your personal health
[00:51:24.880 --> 00:51:29.920]   data outside the pharmacy. You don't have to worry about that. You're well protected. And there's
[00:51:29.920 --> 00:51:35.840]   always a pharmacist there. And because it's Amazon, there's a pharmacist there every hour of the day
[00:51:35.840 --> 00:51:41.840]   a night 24/7. So you never, and it's kind of, you know, it's nice. You can talk to the pharmacist.
[00:51:41.840 --> 00:51:46.400]   It's, you know, someone anonymous. It's not. It's a very easy way to get the information you need
[00:51:46.400 --> 00:51:52.080]   about your medication. Don't you never have to feel a little intimidated or afraid or embarrassed to
[00:51:52.080 --> 00:51:56.800]   talk to the pharmacist and get the information in the name. Amazon Prime members, you'll save on
[00:51:56.800 --> 00:52:02.240]   prescription medication. If you're not using insurance and you'll get free two day delivery,
[00:52:02.240 --> 00:52:14.640]   find out more Amazon.com/twitRx. That's Amazon.com/twitRx. Amazon pharmacy, Amazon.com/twitRx.
[00:52:14.640 --> 00:52:20.480]   As I could, as I said, I used pill pack for years and I really think it's a great service.
[00:52:20.480 --> 00:52:26.080]   And this is going to be even better Amazon pharmacy. Goodbye lines. Hello home delivery.
[00:52:27.040 --> 00:52:30.720]   There's some things that just better since pandemic, you know, that we don't have to go back
[00:52:30.720 --> 00:52:36.640]   to the battle days. There's some benefits to the whole thing.
[00:52:36.640 --> 00:52:45.280]   Maybe not, maybe not this. I love Gizmodo's article. Elon Musk's, the boring company,
[00:52:45.280 --> 00:52:49.440]   you're in Vegas sometimes your family's in Vegas, right? Father Robert? My family's there, yeah.
[00:52:49.440 --> 00:52:56.400]   Well, I remember the last time I was there, which was CES 2020, they had a big dig with a boring
[00:52:56.400 --> 00:53:05.600]   hole to bore this tunnel under Las Vegas. Gizmodo's review is out. Media outlets in Vegas were
[00:53:05.600 --> 00:53:12.240]   invited for a sneak peek of Elon Musk's new form of public transit on Thursday. The headline says
[00:53:12.240 --> 00:53:19.680]   it all Elon Musk's public transit in Las Vegas still just humans driving cars slowly in a tunnel.
[00:53:19.680 --> 00:53:30.800]   That's pretty much it. It's brutal. At least it's a Tesla, but cost them $52 million. The loop,
[00:53:30.800 --> 00:53:36.960]   it's 40 feet underground, one and a half miles long. I saw the dig, the beginning of it, which is the
[00:53:36.960 --> 00:53:42.480]   Las Vegas Convention Center. That's Central Station. There's West Station and South Station. Those
[00:53:42.480 --> 00:53:50.000]   are above ground stations. You can see the videos. This is not like an artist's perspective or a
[00:53:50.000 --> 00:53:59.680]   cartoon. This is actually what it looks like. For those that watch expensive carnival ride,
[00:53:59.680 --> 00:54:04.000]   it's not even fun like a carnival ride. What are they going? 20 miles an hour?
[00:54:05.840 --> 00:54:13.680]   That's just like what the... I thought there'd be at least like buses. These look like model
[00:54:13.680 --> 00:54:19.280]   Xs or model Ys going under there. I mean, it is the perfect track for autopilot though.
[00:54:19.280 --> 00:54:23.600]   There's no way for it to get to an accident. No pedestrians. No firehars.
[00:54:23.600 --> 00:54:26.160]   Never say never. It was probably a way.
[00:54:26.160 --> 00:54:31.600]   Top speed. I sympathize. These are big problems they take time to solve.
[00:54:31.600 --> 00:54:38.080]   All right. Top speed. 35 miles an hour. That's faster than walking.
[00:54:38.080 --> 00:54:45.520]   When we were sold at one of things, we were going to be going in vacuum tubes at 600 miles an hour
[00:54:45.520 --> 00:54:51.680]   from San Francisco to Los Angeles in 30 minutes. They mentioned 16 passenger vehicles.
[00:54:51.680 --> 00:54:59.600]   Not yet. They're really just... I think you just invented trains again.
[00:54:59.600 --> 00:55:01.600]   [laughter]
[00:55:01.600 --> 00:55:10.480]   In full disclosure, my wife works for a public transit agency. I'm a very public transit.
[00:55:10.480 --> 00:55:11.840]   Me too. Absolutely.
[00:55:11.840 --> 00:55:19.360]   It seems like constantly a lot of these tech things where they're disrupting transit are just like,
[00:55:19.360 --> 00:55:23.920]   "Well, are you really disrupting it? Are you just essentially trying to reinvent something
[00:55:23.920 --> 00:55:28.960]   that is already there and maybe doesn't need disruption so much as it needs more funding and
[00:55:28.960 --> 00:55:35.120]   support?" At full capacity, according to Mick Acres,
[00:55:35.120 --> 00:55:44.080]   4400 people per hour can be transported because there's 62 vehicles. You're just going to get a nice
[00:55:44.080 --> 00:55:52.320]   little ride in the car. They're so slow. I'm sorry. I just...
[00:55:53.840 --> 00:55:59.120]   You know what would make this better? What? If you had some of those... What do they call those?
[00:55:59.120 --> 00:56:06.560]   The two-wheeled scooters? Oh, segues. Yeah. Just pick that segues. You get a segue loop. Same speed.
[00:56:06.560 --> 00:56:09.280]   Let you drive. There might be more collisions, but there might be more fun.
[00:56:09.280 --> 00:56:14.560]   But if you get like rollerblade through it or something, I don't know. That sounds like skateboard.
[00:56:14.560 --> 00:56:16.720]   Yeah. Especially with that lighting.
[00:56:16.720 --> 00:56:18.720]   We have... Yeah, that seems awesome.
[00:56:18.720 --> 00:56:23.440]   I think you know this, Robert, because remember we did a promo in San Francisco on segues.
[00:56:23.440 --> 00:56:26.160]   We did. Which was so much fun for the new screen savers.
[00:56:26.160 --> 00:56:28.240]   I think you knew who crashed.
[00:56:28.240 --> 00:56:35.680]   Lisa and I had so much fun. We bought two segues. Pretty much stopped using it when we found our
[00:56:35.680 --> 00:56:44.080]   teenage boy jousting on the segues with his friends. Denise is laughing because she knows.
[00:56:44.080 --> 00:56:48.640]   I think that's... I have one of those. It's not a segues, a teenage boy.
[00:56:48.640 --> 00:56:53.360]   They found a piece of lumber with... Still it nails in it, by the way. And they're holding it under
[00:56:53.360 --> 00:57:00.560]   their arm and they're jousting. Let me ask you this. Do they also have e-bikes that are not locked
[00:57:00.560 --> 00:57:06.720]   up somewhere? They also have e-bikes. So they can joust with those. Yeah. Because they... Steve
[00:57:06.720 --> 00:57:11.840]   Wozniak used to do segue polo with his four. Yeah, I played segue polo with Woz.
[00:57:13.280 --> 00:57:23.440]   He loved his segues. Wait, you got an off-road one. No, no. I did for the promo in Big Fat Tires.
[00:57:23.440 --> 00:57:27.680]   That's the same one that the former president of segue drove off a cliff to his death.
[00:57:27.680 --> 00:57:34.400]   That's right. So I decided not to buy those. Not to be tempted to go off-road. We just have
[00:57:34.400 --> 00:57:42.880]   the road segues. You know, I get cynical about this just because I used to love
[00:57:42.880 --> 00:57:47.520]   trains in the United States. I have traveled coast to coast on trains and I have a little
[00:57:47.520 --> 00:57:52.320]   fair with them. And then I got here and I rode a high speed and I was thinking,
[00:57:52.320 --> 00:57:59.920]   oh my god, no, we have no idea what trains are. Yeah. It's true. Yeah, every other country I've
[00:57:59.920 --> 00:58:05.760]   ever been to, the trains are better. It kills me. It kills me. It would be so nice to have...
[00:58:05.760 --> 00:58:11.600]   Well, I'm with your wife, Dan. Mass transit, that's what we need. It's a solution. We gotta get it.
[00:58:12.480 --> 00:58:19.920]   Dr. Mom's saying, Leo, I told you men in their 20s are idiots. Yeah, you know what? Okay. So Michael
[00:58:19.920 --> 00:58:26.960]   was the one jousting on the segues. Now I remember I gave Henry my older son, he's 25. A hoverboard.
[00:58:26.960 --> 00:58:32.080]   A hoverboard. And he got on Tosh 2.0 because they were hoverboard jousting.
[00:58:32.080 --> 00:58:40.480]   I heard about that from Tosh 2.0 before I heard about it from you. I was watching late night. I was
[00:58:40.480 --> 00:58:46.160]   like, that's Henry. What the hell? It's so embarrassing.
[00:58:46.160 --> 00:58:54.480]   You remember then in the video, there was a quick elbow shot that just took the guy out completely.
[00:58:54.480 --> 00:59:00.560]   I said, "Henry, please." He also, there's another thing, by the way, I shouldn't
[00:59:00.560 --> 00:59:04.880]   pour Henry. I shouldn't mention all this, but I can't help it. There's another thing, I guess
[00:59:04.880 --> 00:59:10.560]   his TikTok inspired, where you jump off a roof and land on your back on a table and it breaks,
[00:59:10.560 --> 00:59:17.040]   it breaks your fall. Have you ever seen that? Why? Don't do it, kids. No. Why? That's my question.
[00:59:17.040 --> 00:59:22.640]   That's worse than planking. I asked him. Why, Henry? Because he sent me the video of him. He's
[00:59:22.640 --> 00:59:27.200]   jumping off a roof. He flips around. He lands. He says, "Oh, it's safe dad." And the girls love it.
[00:59:27.200 --> 00:59:33.600]   Okay, in 20 years when he has back problems, he's laid back on that clip. No kidding.
[00:59:34.480 --> 00:59:37.040]   Anyway, I'm sorry. It's not the Henry show.
[00:59:37.040 --> 00:59:46.560]   Cara Swisher got a big interview with Tim Cook, although, and you wrote up about it, Dan,
[00:59:46.560 --> 00:59:52.400]   so that means you listened to it. I long ago learned never interview CEOs because they're not
[00:59:52.400 --> 01:00:00.160]   stupid. You hope for a stupid CEO or at least a indiscreet one, but they're always well trained.
[01:00:00.160 --> 01:00:03.200]   Tim Cook is not going to say anything. No, he's the man who's very rich.
[01:00:03.200 --> 01:00:08.640]   I was, Jason Stell and my colleague at Six Colors saying, talking about interviewing Steve Jobs
[01:00:08.640 --> 01:00:13.040]   once for five minutes. He didn't want to be there. I didn't really want to be there.
[01:00:13.040 --> 01:00:17.280]   It was kind of an unpleasant experience. You look for actually people like Woz because Woz,
[01:00:17.280 --> 01:00:23.840]   I think probably because of a brain injury, tells the truth. And so, Woz is great. He'll say,
[01:00:23.840 --> 01:00:27.920]   and then you always get juicy quotes out of Woz, but you're never going to get a juicy quote.
[01:00:27.920 --> 01:00:35.280]   No, Tim Cook is very, you can tell from his demeanor that he plays things very close to the vest.
[01:00:35.280 --> 01:00:38.800]   He's very deliberate in the way that he speaks. And he manipulations.
[01:00:38.800 --> 01:00:41.920]   I love when he took over. Yeah, there are a few things in there that I think are interesting.
[01:00:41.920 --> 01:00:48.240]   You got to kind of read between the lines here. He talked a bit about AR, which he's been talking
[01:00:48.240 --> 01:00:52.880]   about for a long time now. And he made some kind of strange comments about, like, what in this
[01:00:52.880 --> 01:00:55.440]   show? Wouldn't it be great? Or we're having this conversation, would it be better if we could
[01:00:55.440 --> 01:01:00.160]   just have like graphs? You're like, wow, Tim, I know you really love graphs, but I don't think
[01:01:00.160 --> 01:01:06.960]   that really is better for most people. But the way he talks about it is much more concrete in
[01:01:06.960 --> 01:01:11.280]   terms of the way he's thinking about it. In the past, it's always been, it's an area of interest,
[01:01:11.280 --> 01:01:16.640]   right? And we've seen the stuff they've done on iOS. This feels like, I know there is something
[01:01:16.640 --> 01:01:20.480]   coming and it's, you know, we are going to be doing something around this. And I'm trying to very
[01:01:20.480 --> 01:01:26.880]   much talk around it. And so you can kind of see in the sort of the negative space, right? Like,
[01:01:26.880 --> 01:01:31.520]   in what he's not saying that like there is something there that he is avoiding talking about. So I
[01:01:31.520 --> 01:01:35.360]   think it is interesting to see that because, you know, they'll never talk about future products.
[01:01:35.360 --> 01:01:40.320]   They were working on this or whatever. But you get a very different feeling from that versus say,
[01:01:40.320 --> 01:01:44.240]   she asked him about car stuff as well. And obviously there's been a lot written
[01:01:44.880 --> 01:01:50.400]   talking about Apple being interested in cars. And he, you know, Cook is willing to say like,
[01:01:50.400 --> 01:01:54.800]   oh, you know, there's a bunch of stuff. You know, we look at a lot of things and that feels
[01:01:54.800 --> 01:01:59.280]   right much more vague where it's something that's like, we know they're working on something. But I
[01:01:59.280 --> 01:02:05.040]   think even they don't know what that is or even if it will become a product. As opposed to the AR
[01:02:05.040 --> 01:02:09.600]   stuff, which feels much more like this is something that's coming probably sooner rather than later.
[01:02:09.600 --> 01:02:14.560]   I always liken it to Kremlinology. You can tell if you've been following it very closely,
[01:02:14.560 --> 01:02:19.840]   you can see the progress in what he said, what he's willing to say. So if you have a deep
[01:02:19.840 --> 01:02:25.120]   knowledge of it, you can kind of infer some stuff from it. So it sounds like we're getting closer to
[01:02:25.120 --> 01:02:29.760]   AR or some sort of mixed reality headset from Apple. That's what the rumors have been saying as well.
[01:02:29.760 --> 01:02:33.760]   Sure. Right. And I think it's hard for him to avoid because he knows how much attention Apple gets,
[01:02:33.760 --> 01:02:38.560]   right? Like Bloomberg and all these other places writing plus all the leakers, all this stuff,
[01:02:38.560 --> 01:02:42.960]   like it's hard to talk around that when there's so much information that's out there even if
[01:02:42.960 --> 01:02:50.160]   you're missing huge swaths. Right. Here's one thing Apple probably wishes we didn't know about,
[01:02:50.160 --> 01:02:56.080]   but this is always the risk with litigation. And Denise, you correct me if I'm wrong, but the
[01:02:56.080 --> 01:03:04.560]   opposition gets to pose your executives and it gets to do discovery and see emails. And it's
[01:03:04.560 --> 01:03:10.720]   almost inevitable when you get these big fights that stuff is revealed that maybe a company wishes
[01:03:10.720 --> 01:03:16.800]   weren't revealed very famously in the Apple Google battle. A lot of stuff came out. Well,
[01:03:16.800 --> 01:03:22.240]   Apple's in a battle with Epic right now over the App Store and over Epic Games and Fortnite.
[01:03:22.240 --> 01:03:29.600]   And a little bit of a smoking gun here, a deposition of Eddie Q, Senior Vice President of
[01:03:29.600 --> 01:03:35.920]   Internet Software and Services. Q says in 2013, there were plans to make
[01:03:35.920 --> 01:03:43.920]   eye messages for Android where it would have had quote cross compatibility with the iOS platform.
[01:03:43.920 --> 01:03:48.640]   So the users of both platforms would have been able to exchange meshes is with one another
[01:03:48.640 --> 01:03:55.280]   seamless thing. Hallelujah. Didn't say why it didn't happen. Craig Federighi though,
[01:03:55.840 --> 01:03:59.120]   during a deposition, Senior Vice President of Software Engineering,
[01:03:59.120 --> 01:04:04.480]   acknowledged that putting eye message on Android would simply serve to remove an obstacle
[01:04:04.480 --> 01:04:12.480]   to iPhone families giving their kids Android phones, cheaper Android phones. Uh oh. And then
[01:04:12.480 --> 01:04:22.400]   an email from Phil Schiller really seals it. He said the number one quoting Phil Schiller's email,
[01:04:22.400 --> 01:04:27.360]   the number one most difficult reason to leave the Apple universe is eye message. The number one
[01:04:27.360 --> 01:04:33.040]   thing we have to lock people in, in other words, eye message amounts to serious lock in. And Schiller
[01:04:33.040 --> 01:04:40.560]   said that quote, moving eye message to Android will hurt us more than help us. This email illustrates
[01:04:40.560 --> 01:04:47.680]   why. So all right, we thought it, we knew it. It's not a big surprise. But now we know there's a real
[01:04:47.680 --> 01:04:52.960]   reason. It's their product. I mean, why would they do that? Why would they make a feature that works
[01:04:52.960 --> 01:04:58.480]   interoperably with their biggest competitor? I find this story like a little bit of man windows.
[01:04:58.480 --> 01:05:03.120]   Yeah, but it's a little dog bites man. I mean, moving eye tunes to windows was nice because it
[01:05:03.120 --> 01:05:06.880]   let them expand their iPod sales. Steve Jobs fought it. They needed that. They needed that.
[01:05:06.880 --> 01:05:12.080]   Well, right. It ended up benefiting. It benefited. Yeah. And whereas this, there's no benefit for
[01:05:12.080 --> 01:05:16.720]   Apple for doing that. It's nice. Like people will like it, but it's not something that will sell
[01:05:16.720 --> 01:05:22.480]   more products. And I, you know, as a company that's looking at its bottom line, I just can't fathom
[01:05:22.480 --> 01:05:26.800]   why they would really consider doing it unless they felt like it's we're going to get more people
[01:05:26.800 --> 01:05:32.640]   switching from Android because they will be so convinced about how good eye messages rather than
[01:05:32.640 --> 01:05:35.680]   people who are going like, Oh, well, if I don't have to worry about losing eye message, I might as
[01:05:35.680 --> 01:05:40.240]   well buy a cheaper phone. Like I think the risks vastly outweigh the possible benefits for them on
[01:05:40.240 --> 01:05:45.200]   that one. Now, I agree with you in that it makes business sense for them not to do that. Any other
[01:05:45.200 --> 01:05:51.440]   company? Absolutely. But for me, the issue is that Apple has always said that these measures
[01:05:51.440 --> 01:05:57.200]   are more about create about curating the experience than anything else. And now that you've got emails
[01:05:57.200 --> 01:06:02.720]   from the executive saying basically, yeah, we could do it, but we'd rather them stay on iPhone.
[01:06:02.720 --> 01:06:08.560]   That doesn't strike the right court. That's that's not the Apple that they're trying to portray.
[01:06:08.560 --> 01:06:11.120]   That's the Apple has just another business. It's the truth.
[01:06:11.120 --> 01:06:12.800]   Right. It's the truth.
[01:06:12.800 --> 01:06:16.400]   Who are you the we're curating the experience and the experience is iOS.
[01:06:16.400 --> 01:06:20.880]   Like that's the best experience. Why? You know, Steve Jobs's famous quote about iTunes on Windows
[01:06:20.880 --> 01:06:25.600]   was it's like given a glass of ice water to someone in hell. And I think it would probably say the
[01:06:25.600 --> 01:06:29.680]   same thing about eye message on Android. Sure, you got eye message, but you're still using Android.
[01:06:29.680 --> 01:06:35.840]   It's germane though to the Epic trial though, because don't forget there is no way to use a
[01:06:35.840 --> 01:06:43.840]   different messenger on iOS as your default. You can't you're locked into eye messages on iOS.
[01:06:43.840 --> 01:06:49.840]   So if you wanted to all as a family use WhatsApp so that your poor Android cousins could still
[01:06:49.840 --> 01:06:55.600]   message with you and are operate with you, you wouldn't have a positive experience on Apple because
[01:06:55.600 --> 01:06:59.760]   you wouldn't get you can do it right. Right. It's not it's not the default, but like,
[01:06:59.760 --> 01:07:03.600]   I don't know, I've used a bunch of different messaging apps on my phone, including WhatsApp and
[01:07:03.600 --> 01:07:08.640]   Signal and Telegram and all this stuff. And yeah, it's it's not the best experience, but it's not a
[01:07:08.640 --> 01:07:13.200]   bad experience. And I don't know that it's necessarily that much worse than than using the
[01:07:13.200 --> 01:07:18.400]   same sort of thing on Android either. I mean, the real problem is frankly that SMS. Well, that's
[01:07:18.400 --> 01:07:21.840]   a nice thing that got away and it ended up being the lowest commented on. Well, that's why people
[01:07:21.840 --> 01:07:27.440]   like Apple messages, right? Because it is an SMS client that it's that is much, much better than an
[01:07:27.440 --> 01:07:33.040]   SMS client. Right. And it gives you a data experience that's 10 times better. It lets you send text
[01:07:33.040 --> 01:07:38.960]   messages if you have to, but you but you can use a positive ends. You know, on a I have to say on
[01:07:38.960 --> 01:07:44.640]   Android, you can use third party apps as your default text messenger. Google would prefer you
[01:07:44.640 --> 01:07:50.160]   used Android messages, but you can use it. This is why I think Google does messaging so much better
[01:07:50.160 --> 01:07:55.920]   because I just use Hangouts or no voice than Hangouts and Alo than Duo than Hangouts.
[01:07:58.880 --> 01:08:03.520]   I am so pissed off because we're on vacation first vacation in a year and a half. Not far away.
[01:08:03.520 --> 01:08:08.320]   We just went to Napa for a couple of days. Don't worry, socially distanced mask, everything.
[01:08:08.320 --> 01:08:13.920]   But my daughter's trying to communicate with me. She's on Android and I've been using Hangouts
[01:08:13.920 --> 01:08:19.600]   on my Pixel phone and Google picked last week to turn the flip the switch to make Hangouts just
[01:08:19.600 --> 01:08:24.400]   completely stop working. It no longer sends or receives messages. And I have no I don't have
[01:08:24.400 --> 01:08:30.000]   a Pixel phone with me. I was using Hangouts on my iPhone so that I could get messages that were
[01:08:30.000 --> 01:08:36.640]   sent to my Pixel number. That's how my daughter messages me. And so we couldn't communicate via
[01:08:36.640 --> 01:08:43.600]   text messaging. I had to call her like a savage. But it just happened to me mid-thread.
[01:08:43.600 --> 01:08:47.920]   Yes. Literally going back and forth. And then suddenly I lost the SMS option. I was like,
[01:08:47.920 --> 01:08:54.720]   wait, what's gone? And it's gone. No, and I'm a Google Fi user. So we were we were
[01:08:54.720 --> 01:09:00.320]   grandfathered in a little longer. It's gone. So I this week made a quest to find a replacement
[01:09:00.320 --> 01:09:06.400]   that had I had I messages. I would have gladly used that on Android. I looked at signal
[01:09:06.400 --> 01:09:10.640]   guys. They have signal on iOS could have signal on Android. But it's annoying because it uses
[01:09:10.640 --> 01:09:15.760]   your phone number and you can only have it on one other device at a time. It deactivates any.
[01:09:15.760 --> 01:09:21.200]   So I finally found something called pulse, which is a good tech. You can use it as your fault
[01:09:21.200 --> 01:09:26.720]   messenger on your on your Android device. You can use it on the desktop. I can use it on Linux,
[01:09:26.720 --> 01:09:34.800]   which is nice. And but there's so Luke Klinker, the author says, I have an iOS app, but Apple won't
[01:09:34.800 --> 01:09:41.360]   approve it because they say this is an app that requires an Android phone and they're not going
[01:09:41.360 --> 01:09:46.160]   to prove it. And there you go again with the lock in because that would be a perfect solution for
[01:09:46.160 --> 01:09:51.760]   me. So I have to use a progressive web app, which does not work very well on iOS to try to use
[01:09:51.760 --> 01:09:59.360]   pulse. There is really nothing to replace Hangouts, which is a little frustrating. And Apple, if you
[01:09:59.360 --> 01:10:05.360]   would make message, I guess here's the point, Dan. Yes, obviously, Apple's acting in their own
[01:10:05.360 --> 01:10:11.680]   interest. But from day one, I remember, you know, pressing my nose against the glass at the
[01:10:11.680 --> 01:10:19.120]   at the computer store looking at the Lisa saying, Oh, if only I had $10,000. And then the Mac came
[01:10:19.120 --> 01:10:24.160]   out and I was pressing my nose against the window at Macy's and fortunately, I had a Macy's card and
[01:10:24.160 --> 01:10:30.160]   I was able to charge it and paid for that for years. But I got up on iMac in 1984. And from day one,
[01:10:30.160 --> 01:10:37.440]   Apple has been for the rest of us think different. They've pushed this thing that we care about the
[01:10:37.440 --> 01:10:43.360]   users. We want to give you the best experience. If you really cared about the users, Apple,
[01:10:43.360 --> 01:10:48.000]   you would consider people who have Android phones and not make them green bubbles. But
[01:10:48.000 --> 01:10:55.440]   the truth is that the bieuze is that really all it is, a color thing? The badge of shame.
[01:10:57.360 --> 01:11:00.960]   It's part of it. I mean, it's part of it. Like there are certainly additional features that you get
[01:11:00.960 --> 01:11:06.800]   in iMessage when you talk between iOS or Apple devices. I think your point is, well, we'll take
[01:11:06.800 --> 01:11:11.280]   in Leo, but I think the challenge there is like, do they have to care about everybody who are not
[01:11:11.280 --> 01:11:16.640]   their customers? I mean, I understand the scales have fallen from my eyes. I thought Apple was,
[01:11:16.640 --> 01:11:23.120]   I thought Apple loved me as much as I loved them. And now I know they're just another gosh,
[01:11:23.120 --> 01:11:29.600]   darn for profit company. So I have to look up who Apple's lawyers are in this case with Epic.
[01:11:29.600 --> 01:11:36.880]   I haven't done that yet, but probably Dan boys. I don't know. No, that would be David boys.
[01:11:36.880 --> 01:11:41.520]   David boys. That would be like up at a Supreme Court level, right? Yeah. We're down here slogging
[01:11:41.520 --> 01:11:47.840]   away in the trial court taking depositions. And what happens when that happens is the lead lawyer
[01:11:47.840 --> 01:11:52.800]   on the case hardly ever wants to sit in on a deposition. It's just not a good use of time.
[01:11:53.280 --> 01:11:57.840]   The junior lawyers are trained. Gibson done and crutcher. It's Gibson done. Okay, so
[01:11:57.840 --> 01:12:04.400]   great big huge multi-national firm. Epic is represented by crevath,
[01:12:04.400 --> 01:12:10.480]   swain and more. The same huge multi-national law firm. So what happens is the partners at
[01:12:10.480 --> 01:12:15.200]   these law firms charge, I mean, it's upwards of a thousand dollars an hour. Oh my God.
[01:12:15.200 --> 01:12:20.080]   Two thousand dollars an hour. Oh my God. I haven't researched it lately, but it's definitely a
[01:12:20.080 --> 01:12:25.920]   thousand plus an hour at those two firms. So just from a cost efficiency standpoint,
[01:12:25.920 --> 01:12:30.480]   you don't want the lead lawyer in there. And what winds up happening is someone who's
[01:12:30.480 --> 01:12:38.240]   not necessarily that seasoned might be sitting in on a deposition and the witness has been prepped.
[01:12:38.240 --> 01:12:42.960]   And the lawyer is just there to sort of like listen for questions and object when they should
[01:12:42.960 --> 01:12:50.800]   object. But mostly it's a fishing expedition by the party taking the deposition. And this
[01:12:50.800 --> 01:12:55.120]   shouldn't have happened. He shouldn't have been able to give those answers because.
[01:12:55.120 --> 01:13:00.240]   Apples attorney should have said apples assured attorney should have been
[01:13:00.240 --> 01:13:05.120]   receiving that I'm guessing that the lead lawyer was far from that turn was in there.
[01:13:08.480 --> 01:13:14.320]   Well, they got the emails too. So is that going to hurt him though? You think is that evidence
[01:13:14.320 --> 01:13:20.640]   of a monopolistic practice? Well, I think there's much more that goes into that analysis.
[01:13:20.640 --> 01:13:27.680]   Your market share really is huge. And does apple. But see, apple says not
[01:13:27.680 --> 01:13:32.960]   huge. It's not huge. Look at you could buy an Android phone. It's not huge. We don't even have
[01:13:32.960 --> 01:13:39.200]   to half the market. Right. And messaging is not the main issue in the Epic case. It's the
[01:13:39.200 --> 01:13:44.720]   App Store. But they do want to lock you in. Yeah. They do want to lock you in. Whatever. I mean,
[01:13:44.720 --> 01:13:48.560]   who wouldn't? What company wouldn't give in the option, right? If they can keep their people,
[01:13:48.560 --> 01:13:52.240]   you know, keep their customers and prevent them growing elsewhere. I'm just a fool. I thought
[01:13:52.240 --> 01:14:00.000]   these companies liked me. Just personally anecdotally, I can tell you as a family who uses iPhones,
[01:14:00.000 --> 01:14:06.880]   the bigger, much bigger lock in for me is find my iPhone. You know, you can find everybody where
[01:14:06.880 --> 01:14:10.640]   they are. You have that on Android too though. You just don't know it. Yeah. Right.
[01:14:10.640 --> 01:14:19.520]   Actually, Dan, you had an interesting story. A fine my is, I guess, not going to be restricted
[01:14:19.520 --> 01:14:28.640]   to apple anymore. Oh, oh, did he lock up? Dan has no comment. He's been advised by his lawyer,
[01:14:28.640 --> 01:14:36.400]   not to say someone someone has used fine my Dan. Fine my Dan. Yeah. Leo, I was just wondering,
[01:14:36.400 --> 01:14:41.600]   one of the ways that apple is defending its practices right now is to say that they can charge
[01:14:41.600 --> 01:14:47.760]   developers 30% because all that money goes into maintaining the App Store and they don't get a lot
[01:14:47.760 --> 01:14:52.800]   of money from the far majority of the apps that are hosted on the App Store. Oh, just 30%. Does that
[01:14:52.800 --> 01:14:59.520]   remind you of a business model of an industry from say the 90s that exploded for doing that exact
[01:14:59.520 --> 01:15:05.920]   sort of reasoning? And that's basically the music industry. Oh, yeah. We charge so much money
[01:15:05.920 --> 01:15:09.440]   because most of the acts we take in don't make money. And so therefore, this is really the
[01:15:09.440 --> 01:15:14.800]   fair thing. Well, even worse than that, we don't want to sell singles because most albums only
[01:15:14.800 --> 01:15:21.040]   have a couple of good songs on it. So you'll never buy the album if we just sold to the good songs.
[01:15:22.160 --> 01:15:25.840]   And that really broke down. And you know who broke that down? Steve Jobs, who said, no,
[01:15:25.840 --> 01:15:30.000]   you're going to sell 99 cents singles and like it. And that destroyed the album.
[01:15:30.000 --> 01:15:35.920]   I understand the arguments that this is good business. But the problem is if you're burning
[01:15:35.920 --> 01:15:41.280]   through all that goodwill, the first time someone comes in to disrupt your market like jobs did,
[01:15:41.280 --> 01:15:46.400]   then it's gone. It gets destroyed almost immediately. And there's no way to hold on to it
[01:15:46.400 --> 01:15:51.200]   except to start suing people. Yeah. I've used fine by Dan Morin and we found him.
[01:15:52.080 --> 01:15:53.120]   Yeah.
[01:15:53.120 --> 01:16:00.320]   My gosh. I'm getting worried. So Apple is even though AirTags is not out, maybe this is their
[01:16:00.320 --> 01:16:04.960]   response. Oh, we never meant to make AirTags. We'll just let everybody else make it. They're
[01:16:04.960 --> 01:16:09.360]   allowing third parties to use the fine line network. Now that's a big deal. Right. And they've
[01:16:09.360 --> 01:16:14.240]   clearly partnered with a few of these companies to sort of get them in the front door because
[01:16:14.240 --> 01:16:18.160]   there's a handful of things that are right out the gate. I think one from Belkin, I think some of the
[01:16:18.160 --> 01:16:22.400]   eBikes was a van move, I think, which have this sort of...
[01:16:22.400 --> 01:16:24.560]   Fine, my eBikes. That would be good. Yeah.
[01:16:24.560 --> 01:16:28.800]   Yeah. I mean, and so they're building a whole framework for it. And I think this is a clever
[01:16:28.800 --> 01:16:34.480]   move because it does raise the question of, well, does Apple want to play in the AirTag space or
[01:16:34.480 --> 01:16:38.480]   do they just decide, you know what? This isn't a thing that we need to build a product for.
[01:16:38.480 --> 01:16:38.960]   Right.
[01:16:38.960 --> 01:16:44.160]   But we can leverage third parties who want to use essentially this entirely huge network of
[01:16:44.160 --> 01:16:49.360]   Apple devices that can be used to find things. And then it also doesn't hurt them in terms of
[01:16:49.360 --> 01:16:52.960]   this whole ongoing antitrust stuff. It makes them look good. Right.
[01:16:52.960 --> 01:16:53.840]   It's good. All of our.
[01:16:53.840 --> 01:16:56.000]   It's what I'm saying. We want to work with all these other people.
[01:16:56.000 --> 01:17:00.160]   One of the companies that's been bitching about Apple, especially about fine line, is Tyle.
[01:17:00.160 --> 01:17:04.480]   Because they say Apple, you know, presuming that Apple's going to release AirTags,
[01:17:04.480 --> 01:17:09.680]   Apple will have an undeniable advantage. Is Tyle part of this third party?
[01:17:09.680 --> 01:17:11.440]   Tyle's not currently part of the company.
[01:17:11.440 --> 01:17:12.640]   No, I'm not surprised.
[01:17:12.640 --> 01:17:17.920]   I know how interesting. I will be interested to see how that changes because at a certain point,
[01:17:17.920 --> 01:17:22.080]   Tyle will be shooting itself in the foot if it doesn't build in support for this system,
[01:17:22.080 --> 01:17:25.600]   right? Because if all its competitors are going to use it and can leverage this, oh,
[01:17:25.600 --> 01:17:30.560]   hey, you know, Tyle's great. You can use it to find other people's probably like who have lost
[01:17:30.560 --> 01:17:34.800]   their keys and use Tyle. But there's still a fraction of Tyle devices that there are to Apple
[01:17:34.800 --> 01:17:35.680]   devices. Right.
[01:17:35.680 --> 01:17:39.840]   So if every other competitor to Tyle is like leveraging Apple's network to be able to find stuff
[01:17:39.840 --> 01:17:43.600]   and Tyle is not. Tyle's going to be in a disadvantage.
[01:17:43.600 --> 01:17:50.240]   So that's really to your point of Apple in a defensive move against the anti-trustless
[01:17:50.240 --> 01:17:53.200]   accusations says, well, no, look how open we are.
[01:17:53.200 --> 01:17:58.080]   We're even letting our competitors make their own first party solution either, right?
[01:17:58.080 --> 01:18:01.040]   They said, we're just working with third parties. Like that looks great for them.
[01:18:01.040 --> 01:18:01.840]   So who knows?
[01:18:02.960 --> 01:18:07.760]   Apple dropped its commission fee for smaller apps, right?
[01:18:07.760 --> 01:18:13.200]   It did to 15% if you make less than a million. By the way, Google has now done the same thing.
[01:18:13.200 --> 01:18:17.760]   Also for companies that make less than a million, which tells you something,
[01:18:17.760 --> 01:18:23.680]   like the vast bulk of revenue Apple and Google make in the app stores and companies that make
[01:18:23.680 --> 01:18:24.880]   more than a million dollars.
[01:18:24.880 --> 01:18:30.560]   Right. But one of the arguments against Epic is that the vast majority of apps
[01:18:31.840 --> 01:18:33.920]   are free and thus pay Apple nothing.
[01:18:33.920 --> 01:18:38.960]   So it's a small percentage of the total ecosystem that's actually making.
[01:18:38.960 --> 01:18:47.280]   Although I would point out that in fact, Epic's Fortnite is free and Epic hates it that the
[01:18:47.280 --> 01:18:53.040]   billions of dollars in revenue they make, they have to give 30% cut to Apple on costumes and
[01:18:53.040 --> 01:18:58.160]   dance moves and all the other stuff. I mean, this is the problem. This actually is a big problem
[01:18:58.160 --> 01:19:04.240]   that Apple is facing in the game segment is free to play games or free to download,
[01:19:04.240 --> 01:19:08.320]   not free to play in the long run. You end up spending a lot of money on those games as you,
[01:19:08.320 --> 01:19:09.680]   as everybody knows.
[01:19:09.680 --> 01:19:11.600]   Apple's actually responded to that this week.
[01:19:11.600 --> 01:19:18.160]   They almost doubled the arcade games to 180 games. Many of them,
[01:19:18.160 --> 01:19:23.840]   classics that were not free to play or they were free to play, but they had a lot of additional
[01:19:24.640 --> 01:19:29.120]   downloadable content. And it really reminds you how nice it is to play a game that doesn't beg you
[01:19:29.120 --> 01:19:33.360]   for money every five minutes. There's some good stuff on the new Apple arcade.
[01:19:33.360 --> 01:19:37.760]   But how will I get the most recent dance moves or
[01:19:37.760 --> 01:19:41.280]   to invent them yourself?
[01:19:41.280 --> 01:19:43.600]   I think you just got a TikTok.
[01:19:43.600 --> 01:19:48.240]   Yes. Oh, yes. Oh, yes.
[01:19:51.040 --> 01:19:57.200]   The biggest Apple story that will probably stay the biggest story for some time to come. Well,
[01:19:57.200 --> 01:20:03.200]   Apple Epic is pretty big, but Apple's new privacy policy, 14.5 is due,
[01:20:03.200 --> 01:20:08.000]   according to Tim Cook, any day now, what do you say? He said in the next couple of weeks.
[01:20:08.000 --> 01:20:09.120]   A few weeks.
[01:20:09.120 --> 01:20:13.840]   Everyone was wondering if it was pegged to some sort of product hardware release,
[01:20:13.840 --> 01:20:16.800]   but it's unclear now. A new iPad sometime in the next,
[01:20:16.800 --> 01:20:21.920]   theological time to release 14 to what? 14.5, which will be new iPad,
[01:20:21.920 --> 01:20:30.960]   OS, new iOS and new TVOS will now officially require apps to ask the user to track them
[01:20:30.960 --> 01:20:37.040]   when you install the app for targeted advertising purposes. This is that new app tracking
[01:20:37.040 --> 01:20:45.680]   transparency. And this is Wall Street Journal is joining the Notice the Wall Street Journal,
[01:20:45.680 --> 01:20:52.400]   which hates Apple and Google because, well, they're a newspaper and Apple and Google are the future.
[01:20:52.400 --> 01:20:58.240]   But they keep ragging on Apple about this. And now there's a series of articles in the Wall
[01:20:58.240 --> 01:21:05.760]   Street Journal about how even small businesses are going to be destroyed by this, which frankly
[01:21:05.760 --> 01:21:14.560]   is bull hockey. No small business cares about tracking users so they can be more effective in
[01:21:14.560 --> 01:21:23.680]   their ad campaigns. Apple is going to turn the flip to switch. Facebook is the real
[01:21:23.680 --> 01:21:28.240]   company that's going to hate it. They're the ones who took out the full page out of the Wall Street
[01:21:28.240 --> 01:21:32.800]   Journal. Right. And they're using small businesses kind of as a stocking was to say,
[01:21:32.800 --> 01:21:37.680]   it's not us. It's not this book. It'll be fine. It's a small business.
[01:21:37.680 --> 01:21:42.880]   Well, what it is, is small businesses who buy ads on Facebook and use Facebook's
[01:21:43.440 --> 01:21:50.720]   knowledge about you to buy ads aimed at you. I honestly have a big problem with ad tracking.
[01:21:50.720 --> 01:21:57.680]   I have much more problem with the ads taking over pages and slowing down downloads and being
[01:21:57.680 --> 01:22:03.440]   security hazards. Frankly, an ad that is about something I'm interested in to me is preferable.
[01:22:03.440 --> 01:22:08.400]   But there's also the argument here. And I don't quite understand the logic in some ways of the
[01:22:08.400 --> 01:22:11.760]   pushback on the small businesses because your argument there is like, right, small businesses
[01:22:11.760 --> 01:22:16.160]   will get in trouble. So what should we do? We should leave things the way they are right now
[01:22:16.160 --> 01:22:20.480]   and your data privacy doesn't matter. Like, is that the alternative? Is your pitching is
[01:22:20.480 --> 01:22:25.360]   everything's fine? Because clearly, there is this argument for data privacy.
[01:22:25.360 --> 01:22:32.880]   I'm going to be the contrarian and say, okay, look, I understand privacy and everybody deserves to
[01:22:32.880 --> 01:22:41.040]   choose. But I mean, I'm sick and tired of clicking the button accepting cookies on every goddamn
[01:22:41.040 --> 01:22:46.880]   website I go to. That's not improving anybody's experience in any appreciable way.
[01:22:46.880 --> 01:22:54.720]   The paranoia over tracking is way out of control, I think. And I think ads that are
[01:22:54.720 --> 01:23:00.960]   tailored to your interests is not a bad thing. What? So I'll let you explain to me, Dan, how
[01:23:00.960 --> 01:23:04.320]   horrible is ad tracking? Why should I be so concerned? It's just the worst.
[01:23:05.520 --> 01:23:11.280]   No, I think the key thing to remember here is tracking and personalization isn't going away.
[01:23:11.280 --> 01:23:16.240]   What's actually changing is just how granular it gets. And if indeed, if you're, you know,
[01:23:16.240 --> 01:23:21.920]   if you want the ads that are specifically targeted to you, Leo LaPort, like, what does Leo want,
[01:23:21.920 --> 01:23:26.160]   then you might be losing out here a little bit. That said, all Apple's doing is putting up a
[01:23:26.160 --> 01:23:30.560]   box that says, are you well, you get the choice. Yeah, choice is good. You're right. You're right.
[01:23:30.560 --> 01:23:34.000]   They're not they're not banning it. They're just making you let you know it's there.
[01:23:34.000 --> 01:23:39.760]   It's estimated that about 70 to 80% of Apple users when posed with this question will say,
[01:23:39.760 --> 01:23:44.400]   oh, absolutely no tracking. Turn it off. That's what that's what everyone's afraid of.
[01:23:44.400 --> 01:23:49.040]   And there are still facilities for doing tracking and personalization. It's just not as fine
[01:23:49.040 --> 01:23:54.320]   grained, right? Like it's still about like, by the way, right? Apple has, and they're allowing
[01:23:54.320 --> 01:24:00.480]   frameworks for this. Yeah. Apple does a lot of ad tracking. Yeah, but I mean, that what they do
[01:24:00.480 --> 01:24:06.320]   is find ways to anonymize it or something very similar to Google flock, which is the segment
[01:24:06.320 --> 01:24:11.200]   I mean, they're putting you in a segment. Right. And even Google realizes this ship is sailing,
[01:24:11.200 --> 01:24:14.320]   right? That's the reason they came out with that whole announcement that we're changing the way we
[01:24:14.320 --> 01:24:19.680]   do this. This is flocking. I think there's still some concerns about it. I don't know all the fine
[01:24:19.680 --> 01:24:23.600]   details of it. And I've seen some people be critical of it. But I think it honestly, it's
[01:24:23.600 --> 01:24:28.480]   probably still a lot better than what we have now. I'm not convinced that it's, you know, fixes all
[01:24:28.480 --> 01:24:32.560]   the potential problems with data privacy. And certainly a lot of their stuff is first party,
[01:24:32.560 --> 01:24:35.600]   and nobody knows more about you than Google, I would argue even Facebook.
[01:24:35.600 --> 01:24:40.240]   So I think there's, it's an improvement, but it's not a, it's not a cure all.
[01:24:40.240 --> 01:24:46.960]   And just to complete that circle, not to make references to hypothetical works of fiction,
[01:24:46.960 --> 01:24:53.840]   but the, don't forget that Apple and Google still have the data. They're, they're governing
[01:24:53.840 --> 01:24:59.440]   what other people do with the data. It's the, we also have to wonder and worry and
[01:24:59.440 --> 01:25:04.800]   handling about how much we trust Apple and Google and the rest of these big companies who are also
[01:25:04.800 --> 01:25:11.040]   by the way working on AI. So, you know, what, what does AI eat data?
[01:25:11.040 --> 01:25:19.920]   I mean, look, look, I'm 100% against tracking 100% and I'm convinced of the, not the evils of
[01:25:19.920 --> 01:25:24.480]   tracking, but the evils of the way that tracking data has been used. How is it ultimately?
[01:25:24.480 --> 01:25:29.360]   An example of how it's been used in a, okay. So this is actually, this is a thing that I've
[01:25:29.360 --> 01:25:35.680]   actually done here in my official capacity. I've had to explain how tracking could be misused.
[01:25:35.680 --> 01:25:42.400]   Take the information from one single user. If I can track your browsing, gesture browsing,
[01:25:42.400 --> 01:25:46.160]   let's forget everything else I might be able to track on your device. I can find the sites that
[01:25:46.160 --> 01:25:51.120]   you go to. I can find the items that you've been looking at. I can find the services that you use
[01:25:51.120 --> 01:25:58.080]   from that profile. Even if I don't know who you are, I can come within a couple of points of accuracy
[01:25:58.080 --> 01:26:04.080]   of knowing what your political leanings are, what your demographic stats are, how much you earn
[01:26:04.080 --> 01:26:11.040]   per year, what your education is. And I can custom tailor a way to convince you to be angry at
[01:26:11.040 --> 01:26:16.960]   something. That was the whole Cambridge analytic thing, which by the way, as it turned out,
[01:26:16.960 --> 01:26:22.880]   was completely ineffective and was a scam, not on us, but on the people who pay Cambridge
[01:26:22.880 --> 01:26:28.960]   analytic and for the data. But the fact that they couldn't do it properly doesn't mean that
[01:26:28.960 --> 01:26:34.640]   it can't be done. I mean, we've done it done in very small batches. There is something there that
[01:26:34.640 --> 01:26:40.640]   if you don't take care can be horribly misused. Well, okay, so who has the data, not the advertiser,
[01:26:40.640 --> 01:26:45.280]   in this case, the advertiser goes to Facebook or Google and says, I want to buy a political ad
[01:26:45.280 --> 01:26:50.640]   with these demographic information. They don't get information about who that person is. They
[01:26:50.640 --> 01:26:54.480]   just know that their ad is going to go to those people. But Facebook and Google know that.
[01:26:54.480 --> 01:26:59.840]   Correct. So are you worried that Facebook and Google are going to do what are they going to do
[01:26:59.840 --> 01:27:04.240]   with that besides selling ads against it? So I can go to Facebook and I can create a custom
[01:27:04.240 --> 01:27:10.080]   campaign and say, look, I would like to approach these types of people. And I want to pitch an ad
[01:27:10.560 --> 01:27:16.080]   that specifically calls out the inefficiencies of the government and why are we wasting tax
[01:27:16.080 --> 01:27:19.200]   dollars. Right. Because I know that those people are the ones who be fertile,
[01:27:19.200 --> 01:27:25.280]   like we upset by that. And now I can direct that anger. And I can do that for 50 or 60 different
[01:27:25.280 --> 01:27:30.480]   subgroups, all directing them towards a common hate, even though they have different things that
[01:27:30.480 --> 01:27:38.080]   they hate. So is it that's democratic? It can be. But but more than maybe it's
[01:27:38.080 --> 01:27:43.440]   it's horribly manipulative. Is it? I mean, it's just it's just telling something people they want to
[01:27:43.440 --> 01:27:48.400]   hear that happens all the time. And Facebook, you got to keep in mind too, is like that's that's
[01:27:48.400 --> 01:27:52.240]   works for them, right? Because their whole thing is engagement. They want more people on their site.
[01:27:52.240 --> 01:27:56.480]   They want more eyeballs. They want more clicks that all drives stuff drives those ads, right? It's
[01:27:56.480 --> 01:28:01.440]   sort of a circular, you know, monster eating itself, like where it's like, oh, the more you come here
[01:28:01.440 --> 01:28:05.520]   and the more you get angry, which means the more you come here, the more you'll see those ads that
[01:28:05.520 --> 01:28:10.480]   will make you angry that will keep you coming back. Right. And as a democracy, do we want people
[01:28:10.480 --> 01:28:14.320]   getting their information and making their political decisions based on advertising?
[01:28:14.320 --> 01:28:18.400]   Rather than. Well, I don't want to be decided by out
[01:28:18.400 --> 01:28:22.560]   journalistic. That's what we're doing. That's up to people to make that decision. I think that
[01:28:22.560 --> 01:28:26.080]   are you would you like to ban political advertising?
[01:28:26.080 --> 01:28:32.960]   I certainly have controls on it. Yeah, controls are not a bad thing.
[01:28:33.600 --> 01:28:37.520]   Controls in what degree that you can advertise to somebody who might be interested in your message.
[01:28:37.520 --> 01:28:43.840]   If I could say, look, I want people who are in favor of gun control. I'd like to buy ads for those
[01:28:43.840 --> 01:28:50.880]   people. You would ban that? I don't know that I would ban that specifically, but I think I think
[01:28:50.880 --> 01:28:55.520]   there are have to be ways in terms of things being conducted in a fair and reasonable manner. And
[01:28:55.520 --> 01:29:00.320]   it doesn't necessarily come down to targeting just what people want to hear. But also, I think as
[01:29:00.320 --> 01:29:03.920]   the father was pointing out, what is the message that you're actually selling? It's one thing to be
[01:29:03.920 --> 01:29:06.960]   saying. I'm in gun control, why I'm in favor of the Second Amendment. But if you're twisting
[01:29:06.960 --> 01:29:11.120]   things specifically to appeal to people, which obviously is what politics does to a certain
[01:29:11.120 --> 01:29:14.720]   amount, I think there are ways to combat that, especially when it comes to disinformation.
[01:29:14.720 --> 01:29:18.800]   Right. I mean, look, I think it's exactly right that Apple can offer you a choice. I don't have
[01:29:18.800 --> 01:29:22.720]   a problem with that at all. I think we should have a choice in all of these regards. If somebody
[01:29:22.720 --> 01:29:25.840]   doesn't want to be tracked, they should be able to say, I don't want to be tracked.
[01:29:25.840 --> 01:29:31.920]   But which by the way, right now is difficult. But I also worry very much about the knee-jerk
[01:29:31.920 --> 01:29:39.760]   reaction, this pro-privacy knee-jerk reaction, like, well, it's all bad because it's all the harms
[01:29:39.760 --> 01:29:47.200]   I've been told are speculatives kind of, well, it could, it might. But none of them are actually
[01:29:47.200 --> 01:29:51.600]   active. And the thing that people bring up a lot, Cambridge Analytica, turned out they didn't have
[01:29:51.600 --> 01:29:58.160]   any of those capabilities. Do you wait until the harm has happened, though? Is that the question?
[01:29:58.160 --> 01:30:04.640]   Well, isn't that what you normally do? Isn't that, I mean, Denise, you're an attorney.
[01:30:04.640 --> 01:30:09.600]   Isn't it better to wait for the harm to happen before you patch security vulnerabilities
[01:30:09.600 --> 01:30:13.040]   until they've been exploited? I don't know. Yeah, I think the harm has happened.
[01:30:13.040 --> 01:30:19.680]   You know, we've got U.S. government agencies confirming that the United States electorate has
[01:30:19.680 --> 01:30:27.360]   been manipulated in a couple of different presidential elections now. You know, this matters. And
[01:30:27.360 --> 01:30:33.680]   do we want to wait for more harm to happen? I don't think so. I think we're dealing with something
[01:30:33.680 --> 01:30:39.680]   quantitatively different than the way political advertising was done in the past.
[01:30:39.680 --> 01:30:43.520]   Yeah, because they've gotten very good at it. Yeah. Well, let's not forget there was an
[01:30:43.520 --> 01:30:48.240]   an arm to insurrection. There was an armed insurrection a couple months ago that was a part
[01:30:48.240 --> 01:30:52.160]   stolen by these kinds of things in part can't blame it, but it didn't it didn't help. And it
[01:30:52.160 --> 01:30:57.440]   certainly was used like the indications seem to be was at track on the equals January 6th. No,
[01:30:57.440 --> 01:31:01.840]   but it's there's a there is a slippery slope there as well, too. I mean, I don't agree with you, Leo,
[01:31:01.840 --> 01:31:08.080]   that there's there is probably an over compensation. And I would say in large part, that's because
[01:31:08.080 --> 01:31:13.120]   the controls have been so lax. I think the pendulum has swung back so far in the other direction
[01:31:13.120 --> 01:31:19.680]   because there was basically nothing it was the wild west. I would I think we can agree. And any
[01:31:19.680 --> 01:31:24.800]   sensible person can agree that it should be up to any individual to the degree to which they are
[01:31:24.800 --> 01:31:30.080]   tracked. So if somebody really wants to protect their privacy, there should be means and ways to
[01:31:30.080 --> 01:31:36.960]   do that. But I also worry that privacy advocates are going and I can see it happening are preventing
[01:31:36.960 --> 01:31:41.840]   a lot of useful technologies from developing because the companies developing I'm saying,
[01:31:41.840 --> 01:31:46.080]   well, we'll get we're get hit on privacy on this one things like face recognition. And and
[01:31:46.080 --> 01:31:53.360]   I think it should be a choice. I agree. I don't think that the choice should be made for all of us
[01:31:53.360 --> 01:32:00.000]   that these technologies shall not happen because they might impinge on privacy. I don't I that bothers
[01:32:00.000 --> 01:32:04.800]   me. I feel kind of sad that that if you pop up a message to people on their iPhone, say if you
[01:32:04.800 --> 01:32:09.760]   want to do you want to be tracked, most are going to say no, mostly right now they don't have that
[01:32:09.760 --> 01:32:14.880]   kind of meaningful choice at all. But on the other hand, most say no, but not because they really
[01:32:14.880 --> 01:32:20.880]   understand what the issues are. Most people say I don't want to target an advertising.
[01:32:20.880 --> 01:32:26.960]   But honestly, I don't I think you do want targeted advertising.
[01:32:26.960 --> 01:32:32.800]   One thing when I start doing diapers ads on Twitter, it happened when we started doing
[01:32:32.800 --> 01:32:39.200]   manscaped ads, people said, I don't want to hear about below the bell grooming. Well, you want
[01:32:39.200 --> 01:32:46.160]   targeted ads. But Leo, this group, this group, this panel and this audience are not the target for
[01:32:46.160 --> 01:32:50.800]   this discussion because most of us can make an informed decision about whether or not we care
[01:32:50.800 --> 01:32:56.000]   about tracking. That's true. The vast majority of the world, all of the people that I educate here,
[01:32:56.000 --> 01:33:01.040]   they have no idea that this is even a thing. Yeah. So they don't they don't even so they can't.
[01:33:01.040 --> 01:33:05.520]   So this is what terrifies Facebook and Google is Apple's going to let them know.
[01:33:05.520 --> 01:33:09.280]   And these privacy labels, even worse, right?
[01:33:09.280 --> 01:33:18.000]   I like what Apple's doing to try and and mollify the advertisers and say, hey, we're going to give
[01:33:18.000 --> 01:33:23.280]   you some tools. You can see, you know, what sort of positive reaction you've gotten to your ad
[01:33:23.280 --> 01:33:30.000]   without having to know exactly what user did what. Right. That that seems useful. Yeah.
[01:33:30.800 --> 01:33:35.200]   Yeah. And I don't think the sky is going to fall as a result of this. No. I mean, I think there's
[01:33:35.200 --> 01:33:40.960]   been a lot of things that people sort of have, you know, treated like it's apocalyptic. And I'm
[01:33:40.960 --> 01:33:43.920]   not sure that this is going to necessarily put anybody out of business. I think it's going to
[01:33:43.920 --> 01:33:48.800]   change the way that that ads are used. But I honestly, I also wonder this sort of the flip side too is
[01:33:48.800 --> 01:33:52.400]   like, you know, I think Father made a great point about like, it's not necessarily for us.
[01:33:52.400 --> 01:33:58.480]   Because like, when was the last time you actually clicked on an ad? Like, I have a lot of cases
[01:33:58.480 --> 01:34:03.520]   where I have them off. Right. I had the last one. When's the last time you so not for us that not only do I,
[01:34:03.520 --> 01:34:07.680]   do I, you know, every time I get those cookie policy things, I don't just click. Okay, I go in
[01:34:07.680 --> 01:34:15.520]   and customize them. So I love ads. I love them because I, I, a few, a fuzz all of my, my tracking
[01:34:15.520 --> 01:34:21.360]   you get the weirdest ads ever. Yeah. So I get ads in Japanese and it's Spanish and for eateries,
[01:34:21.360 --> 01:34:25.200]   I would never have been more annoying than seeing an ad for something you'd actually be interested in
[01:34:25.200 --> 01:34:35.040]   though. No, because I'm a bit of an. And I, I enjoy. Okay. The truth comes out. All right,
[01:34:35.040 --> 01:34:39.840]   let's take a little break. Father Robert Balasair. Hey, he said it. He's a bit of a, you know,
[01:34:39.840 --> 01:34:47.680]   Dan. No, he's a great guy. Dan Morin, six colors.com science fiction author. What's your latest book?
[01:34:47.680 --> 01:34:53.840]   It's the Alif extraction, which came out last May. Exciting. Are you working on a sequel?
[01:34:54.800 --> 01:34:59.040]   I'm working on something. I can't say too much, but I'm definitely working on stuff. I, you know,
[01:34:59.040 --> 01:35:03.440]   when I will, I will shout it. But right now I can't say. Exciting. Well, come back and tell us when
[01:35:03.440 --> 01:35:09.920]   it's okay. Denise Howell, attorney at law, Denise, how about info? What are you working on these days?
[01:35:09.920 --> 01:35:17.040]   I'm representing clients in their privacy compliance. A lot of times. Oh, I bet. Yeah. Yeah.
[01:35:18.400 --> 01:35:26.560]   I'm still taking care of my kid and just trying to get life back to normal these days.
[01:35:26.560 --> 01:35:34.000]   Enjoying your, I enjoy your regular wardrobe pics on Instagram. Yes. I, on the side, I like to advocate
[01:35:34.000 --> 01:35:40.960]   for reducing the textile waste problem globally. So maybe you don't know this, but Denise is able
[01:35:40.960 --> 01:35:48.240]   to go to a good will or a thrift store and put together amazing, so cool outfits. And she posts
[01:35:48.240 --> 01:35:53.840]   them. What's it's Denise? How, what's your Instagram? It's D how? D how? Yeah. Highly recommended.
[01:35:53.840 --> 01:35:59.520]   Well, thank you. Our show today brought to you by podium. Another thing I highly recommend.
[01:35:59.520 --> 01:36:03.920]   I've experienced, you probably have experienced podium. If you leave, let's say a dentist's office
[01:36:03.920 --> 01:36:08.880]   and you get a text saying, Hey, don't forget to give us a review or how, how did you like our
[01:36:08.880 --> 01:36:15.280]   service? Or if you have any issues, here's the number to text us back at. That's podium. If you've
[01:36:15.280 --> 01:36:20.240]   ever wanted to contact a local business to find out more and wish you could have just texted them
[01:36:20.240 --> 01:36:27.440]   instead of calling them, don't you prefer to text? I do. That's podium in the 90s. You know,
[01:36:27.440 --> 01:36:32.960]   your business needed an email address by the 2000s. You had to have a website by 2010. You had to
[01:36:32.960 --> 01:36:39.520]   have that Twitter account and Facebook presence by 2021. Your business needs to be texting. And
[01:36:39.520 --> 01:36:46.080]   that's why you need podium. I'll give you a stat that'll tell you something. Average email,
[01:36:46.080 --> 01:36:54.640]   about 20% open rate, one in five, 98% of text messages will be opened. Who doesn't open text
[01:36:54.640 --> 01:37:00.320]   messages? Everybody does. It's a great way to stay in touch to serve your clients, your customers.
[01:37:00.320 --> 01:37:05.360]   podium is the messaging platform that will power your business. Very easy for you to set up.
[01:37:05.360 --> 01:37:09.920]   You can be onboarded in less than a day. And podium has a great team in place to answer your
[01:37:09.920 --> 01:37:15.760]   questions to walk you through everything. Businesses use podium for a variety of things. You can ask
[01:37:15.760 --> 01:37:21.520]   for reviews on your favorite review sites. By the way, that really works because the customer was
[01:37:21.520 --> 01:37:27.760]   just in the store or just in the practice. You can collect payments. podium allows payments.
[01:37:27.760 --> 01:37:32.240]   It's a great way. Customers love that. Especially nowadays, when they don't want to go in the
[01:37:32.240 --> 01:37:37.280]   store, give you money and come out with a product, they can do it all with podium. You can communicate
[01:37:37.280 --> 01:37:41.600]   with customers. You can capture leads. And here's the thing your staff will like it because it's
[01:37:41.600 --> 01:37:48.960]   all from a single inbox. podium helps you adapt to the changing customer expectations. And when
[01:37:48.960 --> 01:37:54.480]   you find out the business context, I could tell you you're going to use them. I know I do because
[01:37:54.480 --> 01:38:00.160]   it's easier for me. I'd far rather order a product through text. Maybe I'm just a nerd, but I don't
[01:38:00.160 --> 01:38:04.720]   think I'm alone on this one. With podium reviews, text customers, they'll leave an online review
[01:38:04.720 --> 01:38:10.560]   that improves your search rankings. podium web chat on your website. Let's website visitors text
[01:38:10.560 --> 01:38:14.480]   with their team right from your homepage. And your team can respond right from their phones,
[01:38:14.480 --> 01:38:20.160]   which is really nice for them. They don't have to change, adjust their life to respond.
[01:38:20.160 --> 01:38:24.960]   There's podium video chat. You can use that to meet with customers. You get paid fast over text
[01:38:24.960 --> 01:38:30.400]   with podium payments. And it all comes together in the podium inbox, which keeps leads warm
[01:38:30.400 --> 01:38:37.200]   and lets you respond to feedback easily all in one place. I think you're going to love podium. And
[01:38:37.200 --> 01:38:41.920]   just take a look at the testimonials on the podium website, the bridal collection. A lot of small
[01:38:41.920 --> 01:38:46.240]   businesses. I mean, this is really who this is for. A big business probably already has a solution,
[01:38:46.240 --> 01:38:51.840]   if not podium. But if you don't, if you're the bridal collection, Lynn, she's the owner there.
[01:38:51.840 --> 01:38:57.600]   She processed $200,000 in no contact payments. She says, we don't have to take credit cards
[01:38:57.600 --> 01:39:03.680]   to the store. We can do it completely remotely. podium has been a godsend for us. South Tampa
[01:39:03.680 --> 01:39:08.160]   family and cosmetic dentistry. This is my dentist uses it. It's really cool. They were able to get
[01:39:08.160 --> 01:39:14.720]   nearly 1200 reviews, by the way, 4.9 stars if you're in South Tampa. Dr. Wyatt says the number of
[01:39:14.720 --> 01:39:22.080]   walk-ins as a result of our reviews has skyrocketed. podium is a great way to stay in touch, to keep
[01:39:22.080 --> 01:39:27.040]   in touch, to do the things customers want to do on the platform your customers already know how to
[01:39:27.040 --> 01:39:32.000]   use. They don't have to surf to your website. They don't have to make a phone call. They just
[01:39:32.000 --> 01:39:38.160]   use text. Find out how podium can help your business reach more customers. You can get started for
[01:39:38.160 --> 01:39:44.960]   free today. And as I said, the onboarding is fast and easy. podium.com/twit. You can see a demo at
[01:39:44.960 --> 01:39:51.200]   their website to podium.com/twit. We thank them so much for supporting this week in tech. We thank
[01:39:51.200 --> 01:40:02.480]   you for supporting this week in tech by using that special address podium. P-o-d-i-u-m podium.com/twit.
[01:40:04.560 --> 01:40:11.280]   Did Father Robert, did you solve the Google IO puzzle? No, no. I didn't find out about that
[01:40:11.280 --> 01:40:16.800]   till today. I've been focusing on religious stuff. Oh, that darn religious stuff. I know.
[01:40:16.800 --> 01:40:24.160]   This is one of the things I just, Google, last week Mike Elgin said something I thought was actually,
[01:40:24.160 --> 01:40:30.640]   once he said it a light bulb went off, he said Sundar Pichai is the worst CEO in Silicon Valley.
[01:40:30.640 --> 01:40:37.360]   The Google has been mismanaged ever since Larry Page and Sergey Brin left ever since Eric Schmidt
[01:40:37.360 --> 01:40:42.720]   left. I find it hard to deny that. But there's some things that Google spirit still comes through.
[01:40:42.720 --> 01:40:49.200]   And I really like the puzzles that they do. Now the puzzle is over I guess because everybody
[01:40:49.200 --> 01:40:55.520]   knows Google IO is going to be online May 18th through 20th. But last week it was a fun puzzle
[01:40:55.520 --> 01:41:01.920]   using, oh here it is. You can still go to the puzzle. It uses punch cards. Oh, I love punch cards.
[01:41:01.920 --> 01:41:07.920]   I know. So the first thing is, who said this? Well, anybody can just Google that. It matters
[01:41:07.920 --> 01:41:15.760]   little first who arrives at an idea rather than what is significant is how far that idea,
[01:41:15.760 --> 01:41:21.200]   rather what is significant is how far that idea can go. So everybody knows you just put this in
[01:41:21.200 --> 01:41:29.680]   quotes. It matters little by the way. It's got a lot easier since everybody has searched for this.
[01:41:29.680 --> 01:41:39.040]   Who first arrives? Yeah, right. Who first arrives? Let's put that in quotes. See who said that.
[01:41:39.040 --> 01:41:45.040]   Sophie German first search result. All right, we're going to type that in. Now this is where
[01:41:45.040 --> 01:41:53.760]   you really want to pay attention. I J G E R M A N. I think I am. Oh, there's right. Oh, very good.
[01:41:53.760 --> 01:41:58.480]   See you win already. Leo, it works on being as well. Does it really?
[01:41:58.480 --> 01:42:09.120]   Just in case now I think my screen is. Yeah, there we go. Okay. So pay attention. Do not pass
[01:42:09.120 --> 01:42:15.920]   the next press the next button until you note where these punches are. Okay.
[01:42:15.920 --> 01:42:26.320]   As this is bringing back. Oh, yeah. Yeah. Oh, P H I E. Because if you submit that,
[01:42:26.320 --> 01:42:32.640]   your next task is to create a punch card. This is where I stopped.
[01:42:36.000 --> 01:42:41.520]   Okay, I got to do this before I sleep down. Yeah. Oh, my God. It's actually not hard, but it's fun.
[01:42:41.520 --> 01:42:46.000]   Surely you've done this before the text says this year, you'll greet the planet anew from right
[01:42:46.000 --> 01:42:53.760]   where you are. Write a well known test program composed of two worlds. Yeah, two words. Very good.
[01:42:53.760 --> 01:42:58.960]   Often the first program written when learning to code hello world. So I'm guessing you just
[01:42:58.960 --> 01:43:04.960]   write hello world by punching the proper holes in here. But you got to know what you're doing
[01:43:04.960 --> 01:43:12.880]   because I hope you paid attention to Sophie's German. I did not. So I'm punching randomly.
[01:43:12.880 --> 01:43:19.760]   The earliest programming was done this way. Just punching random holes in a car.
[01:43:19.760 --> 01:43:25.520]   Invalid invalid invalid submission. Try again. Try again. So anyway, have fun. Yeah, see,
[01:43:25.520 --> 01:43:32.160]   Robert, I thought you'd enjoy that. Yeah. Invalid. You know, it's fun to see these again, but I do
[01:43:32.160 --> 01:43:36.720]   not have fond memories of punch cards. No, of course not. But these, the good news is you cannot
[01:43:36.720 --> 01:43:43.680]   fold spindle or mutilate. So no, no, I never had a problem with that. So I had a final project
[01:43:43.680 --> 01:43:50.160]   that I was putting carefully into boxes. And as I was in, oh, no, the stack flipped out. Oh,
[01:43:50.160 --> 01:43:55.680]   I mean, that's it's basically gone. They don't number those. Yeah, you just might as well punch
[01:43:55.680 --> 01:44:03.600]   some new ones. Yeah, not great. Yeah. Why were you doing punch? You're a young man. Why were you
[01:44:03.600 --> 01:44:09.280]   doing? I didn't even have to use punch cards. Why were you doing punch cards? You have to remember,
[01:44:09.280 --> 01:44:16.720]   I did some early classes. And that's what we had. That's all you had. We weren't even to pass
[01:44:16.720 --> 01:44:25.360]   Cal yet. Oh, God. Pick you in Fortran with punch cards. Here's another. Okay. By the way, I mentioned
[01:44:25.360 --> 01:44:33.440]   that Sundar Pajai, terrible CEO, anybody want to say no? That's I'm wrong? Anybody? No, apparently
[01:44:33.440 --> 01:44:41.040]   not. Okay. Just, you know, I mean, that's kind of controversial. I mean, but and I've, I went
[01:44:41.040 --> 01:44:45.040]   Sundar was running the Chromebook division. I really had a huge amount of respect for him. I
[01:44:45.040 --> 01:44:49.120]   thought it was a great guy. He was a smart guy. I don't know if he's done the best with Google.
[01:44:49.120 --> 01:44:53.200]   Maybe it's not his fault. Maybe somebody else. Seems like a smart guy, but I think it's just
[01:44:53.200 --> 01:44:58.640]   Google seems to lack a vision. I think it's kind of what it is. They're kind of just keeping on,
[01:44:58.640 --> 01:45:04.480]   keeping on. Well, I could cite a few things that I like about what Google's doing these days,
[01:45:04.480 --> 01:45:09.280]   but it's such a black box. You know, you can't really apply on what's going on internally.
[01:45:09.280 --> 01:45:12.960]   Here's an interesting one. I think it's under Pajai. I automatically compare him to someone
[01:45:12.960 --> 01:45:18.000]   like Sacha Nadella. And Sacha Nadella has evolved Microsoft and taken them in a completely different
[01:45:18.000 --> 01:45:24.240]   direction and revitalized it. So yeah, maybe Sundar Pajai is not, he's not horrible, but he hasn't done
[01:45:24.240 --> 01:45:32.080]   that. He's basically the maintenance CEO. And the problem with Silicon Valley companies is if you
[01:45:32.080 --> 01:45:36.960]   just maintain your plane is going to slowly sink, it's not eventually you're going to hit a mountain.
[01:45:36.960 --> 01:45:41.920]   I mean, it's just not. So here's an example. When Android 11 came out
[01:45:43.520 --> 01:45:51.600]   on Pixel devices, Pixel 4 XL, Pixel 4a took a major hit in performance. We won't, we don't know if the
[01:45:51.600 --> 01:45:58.400]   5 and 4a 5g did because they were released with it 11 on it. So we never had any benchmarks prior
[01:45:58.400 --> 01:46:06.560]   to Android 11. So the latest Pixel drop, Google started rolling it out in April. All of a sudden,
[01:46:08.160 --> 01:46:15.680]   all of these phones are suddenly performing like significantly better. Apparently,
[01:46:15.680 --> 01:46:22.400]   Android 11 had some horrific bug that was causing a degradation from 30 to 50%
[01:46:22.400 --> 01:46:30.720]   in 3D Mark performance. Suddenly, the Pixel 5 score goes up like 50%.
[01:46:33.520 --> 01:46:39.120]   People who had, as I do, a Pixel 4 XL had noticed performance degradation with Android 11.
[01:46:39.120 --> 01:46:48.000]   Hey, good news. The latest Android update, April 2011, 2021 is going to improve it. It includes
[01:46:48.000 --> 01:46:54.320]   some performance optimizations for certain graphics intensive apps and games. That's just,
[01:46:54.320 --> 01:47:02.400]   to me, that's sloppy release. Look, if this was on just a standard Android phone, I would
[01:47:02.400 --> 01:47:06.960]   understand it. You have to do some testing. Maybe some testing was missed. This is the flagship.
[01:47:06.960 --> 01:47:14.240]   You did not test against the flagship? Yeah. Or you didn't, you let it slide.
[01:47:14.240 --> 01:47:21.280]   Or you didn't care. Why would you let slide 30 to 50%? It's not nothing. That's a big deal.
[01:47:21.280 --> 01:47:26.480]   Honestly, I feel like let it slide is the operative phrase at Google these days.
[01:47:26.480 --> 01:47:31.200]   It's like, and there was even a rumor, oh, they're not going to do a 5a. Google had to say, no,
[01:47:31.200 --> 01:47:36.560]   no, we're going to do a 5a, really. But, you know, it's like, everybody believed it. It's like,
[01:47:36.560 --> 01:47:42.320]   yeah, they really didn't seem that enthused about the 5, to be honest. It was a sleepy time
[01:47:42.320 --> 01:47:49.840]   performance. Here's another piece of evidence. Waymo, Google's self-driving vehicle arm,
[01:47:49.840 --> 01:47:58.880]   was on track in 2018. They announced plans to buy up to 20,000 Jaguar I-Pace electric cars,
[01:47:58.880 --> 01:48:05.920]   up to 62,000 more Chrysler Pacific events for their self-driving fleet. They announced plans to
[01:48:05.920 --> 01:48:11.920]   launch a driverless commercial taxi service before the end of 2018. So, we have a 20,000 Jaguar's
[01:48:11.920 --> 01:48:18.320]   62,000 Chrysler's, that's 82,000 cars. They now in their fleet have well over 600.
[01:48:18.320 --> 01:48:23.760]   Well over 600.
[01:48:27.760 --> 01:48:34.720]   They're all doing a loop, a slow loop. Can you blame that on the, how difficult self-driving vehicle
[01:48:34.720 --> 01:48:41.600]   is and maybe it's not their fault? Or, I mean, by the way, at the same time as this, we learned
[01:48:41.600 --> 01:48:48.160]   this, CEO John Kraftchick announced he's stepping down. Because there's nothing to do,
[01:48:48.880 --> 01:48:58.560]   frankly. He came in, he was a former auto exec who came in in 2015 to take Waymo to the moon.
[01:48:58.560 --> 01:49:00.880]   Instead, we got about 600 cars.
[01:49:00.880 --> 01:49:07.360]   Remember all of those plans were made at a time when that industry looked like it was really
[01:49:07.360 --> 01:49:09.040]   taking off. So, that's the question. That's the question.
[01:49:09.040 --> 01:49:15.360]   And then they realized, wow, this is actually a lot harder than we thought it was going to be.
[01:49:15.360 --> 01:49:18.880]   The technical difficulties. So, maybe this is a way off the scale.
[01:49:18.880 --> 01:49:21.520]   This is just the way of way it is.
[01:49:21.520 --> 01:49:25.280]   Well, I mean, there were a bunch of setbacks with some of the accidents that happened,
[01:49:25.280 --> 01:49:30.640]   with a lot of the self-driving tech. I think that has also made people more cautious about
[01:49:30.640 --> 01:49:35.040]   sort of full steamy head when there are a lot of problems that need to get out of it.
[01:49:35.040 --> 01:49:37.360]   So, they should name it Way less.
[01:49:37.360 --> 01:49:44.800]   I think the accident in Arizona brought to the front the fact that
[01:49:44.800 --> 01:49:48.240]   if you've got beta software in a self-driving car, you kill people.
[01:49:48.240 --> 01:49:52.960]   It's not like the rest of the industry where, okay, we'll fix it in the next version.
[01:49:52.960 --> 01:49:58.160]   And then they realized, yeah, this is why Detroit took so long to make cars.
[01:49:58.160 --> 01:50:00.160]   Yeah. Because you're dealing with tech that.
[01:50:00.160 --> 01:50:04.320]   But Detroit also killed 35,000 people last year in cars.
[01:50:04.320 --> 01:50:05.600]   So, well, yeah.
[01:50:05.600 --> 01:50:12.400]   Some of that is just the people being less comfortable with computers making that decisions,
[01:50:12.400 --> 01:50:14.400]   then people making that decisions because you're totally right.
[01:50:14.400 --> 01:50:17.120]   Like if I kill you, well, it's my fault.
[01:50:17.120 --> 01:50:21.440]   But if you're a computer person, you're not perfect, but a computer is supposed to be perfect.
[01:50:21.440 --> 01:50:25.120]   So, if a computer kills someone, we can't even arrest a computer, right?
[01:50:25.120 --> 01:50:26.880]   Like, what are we going to do about that?
[01:50:26.880 --> 01:50:31.280]   Actually, right. And Dan, as a science fiction writer, does your brain go down the road of,
[01:50:31.280 --> 01:50:35.680]   okay, so not only can these cars be dangerous to the people using them as they're intended,
[01:50:35.680 --> 01:50:41.600]   but then they can be co-opted and weaponized and, you know, all the terrible conclusions.
[01:50:41.600 --> 01:50:48.560]   I have this huge argument with my best friend from college is an emergency room doctor.
[01:50:48.560 --> 01:50:51.840]   And he is all aboard the self-driving car idea.
[01:50:51.840 --> 01:50:56.320]   He loves this because he sees so many people come in as like victims of accidents.
[01:50:56.320 --> 01:51:00.480]   Yeah. And on the flip side, you know, I started in my tech career as a programmer,
[01:51:00.480 --> 01:51:03.760]   and I have seen the problems, the programs I've run into.
[01:51:03.760 --> 01:51:07.040]   And I'm like, how could you ever trust a computer to do that?
[01:51:07.040 --> 01:51:10.320]   It can barely, like, do these simple algorithms, right?
[01:51:10.320 --> 01:51:13.600]   Like, I'm hardly doing a range checking Java.
[01:51:13.600 --> 01:51:16.480]   And you want to drive a car?
[01:51:16.480 --> 01:51:22.480]   We're all going to be okay with the AI in our car until that AI picks a political party.
[01:51:22.480 --> 01:51:30.080]   And then we get Dan, Dan, you're revealing a secret that programmers try to hide.
[01:51:30.080 --> 01:51:35.760]   We see the code. We know everything's a house of cards.
[01:51:35.760 --> 01:51:37.680]   It's amazing. Anything works.
[01:51:38.800 --> 01:51:41.040]   I constantly baffles me.
[01:51:41.040 --> 01:51:44.560]   I mean, and as anybody who's used modern technology and been like,
[01:51:44.560 --> 01:51:47.840]   just this afternoon, I was trying to listen to a podcast on like an AirPlay speaker,
[01:51:47.840 --> 01:51:50.480]   where I was folding laundry. And I was like, it just wouldn't do it.
[01:51:50.480 --> 01:51:53.600]   And I was like, I don't know. I don't know what's happening.
[01:51:53.600 --> 01:51:56.560]   Like, I have no idea how to fix this. I don't know what's broken.
[01:51:56.560 --> 01:51:57.040]   Oh.
[01:51:57.040 --> 01:51:59.440]   And you know, I sympathize. It's hard problems.
[01:51:59.440 --> 01:52:03.840]   But I think when those problems move over to like, well, let's let it drive our cars.
[01:52:03.840 --> 01:52:06.160]   I, it terrifies me. I don't like it.
[01:52:06.160 --> 01:52:15.280]   Actually, I quoted 35,000 auto deaths. Actually in 2020, it was up 24% to 42,000.
[01:52:15.280 --> 01:52:23.760]   For some reason, more people died in pandemic is the highest auto death rate since 1924.
[01:52:23.760 --> 01:52:26.400]   We thought there were a few.
[01:52:26.400 --> 01:52:27.520]   That's so sense. We were driving.
[01:52:27.520 --> 01:52:28.640]   It's interesting.
[01:52:28.640 --> 01:52:31.280]   There are so many people who thought like, there's no traffic.
[01:52:31.280 --> 01:52:32.880]   I can think because I definitely saw that.
[01:52:32.880 --> 01:52:34.720]   There were people like zooming around my town.
[01:52:34.720 --> 01:52:35.360]   Because, you know.
[01:52:35.360 --> 01:52:42.320]   So it's kind of a mystery. But that's what the experts think is that because there was
[01:52:42.320 --> 01:52:49.760]   less traffic, people sped. And of course, the faster you go, the higher rate of deaths by accident.
[01:52:49.760 --> 01:52:56.000]   So I suspect there are fewer miles driven, but they were driven in a much higher speed.
[01:52:56.000 --> 01:52:59.040]   And people drag raced too. We didn't hear a lot of that.
[01:52:59.040 --> 01:52:59.440]   Yeah.
[01:52:59.440 --> 01:53:00.720]   Going on.
[01:53:00.720 --> 01:53:06.560]   So in a way, when you see a number like that, how many deaths have there been from self-driving
[01:53:06.560 --> 01:53:10.320]   vehicles? It's fewer than five. It's three, I think. Something like that.
[01:53:10.320 --> 01:53:18.240]   Versus 42,000. And if you think, well, if everything were self-driving vehicle,
[01:53:18.240 --> 01:53:22.000]   would it be zero? Probably not. But it wouldn't be 42,000.
[01:53:22.000 --> 01:53:28.080]   Probably not. I mean, it's unclear. We're still in such early phases.
[01:53:28.080 --> 01:53:31.280]   And I think there is an argument for that. But I think it doesn't take into account the
[01:53:31.280 --> 01:53:37.520]   intangibles of it. Right? Like, there are all these weird questions. And maybe Denise has
[01:53:37.520 --> 01:53:42.640]   input on this. But like, what's like liability like? Right? If you get hit by a self-driving car,
[01:53:42.640 --> 01:53:47.440]   is it the person who was behind the wheel who wasn't driving? Is it the person who wrote the
[01:53:47.440 --> 01:53:49.760]   software? Is it the person who built the hardware platform?
[01:53:49.760 --> 01:53:52.720]   As you did. You wrote the crap software.
[01:53:54.080 --> 01:53:59.520]   That killed that. The algorithm failed to identify you as a person. Like, is that your fault? I don't
[01:53:59.520 --> 01:54:03.520]   know. It seems wild. I think what do they say? They say the insurance companies say,
[01:54:03.520 --> 01:54:11.280]   who's at fault if a self-driving car crashes? I remember reading something. I think the company
[01:54:11.280 --> 01:54:17.840]   that made the car, but I don't know. Not the passenger. I guarantee you, plaintiff's lawyers
[01:54:17.840 --> 01:54:23.040]   aren't going to take that for an answer. They're going to go back to their first-year-twords class.
[01:54:23.040 --> 01:54:30.240]   So, wow. Revisit all of causation, which leads to liability and apply it to this situation.
[01:54:30.240 --> 01:54:36.640]   The answer is, whoever has the biggest pockets is responsible.
[01:54:36.640 --> 01:54:45.600]   Here is an article from Hardison and Cochrane, attorneys at law in, apparently, North Carolina.
[01:54:48.080 --> 01:54:53.360]   In North Carolina, auto accident lawyers routinely help injured, blah, blah, blah.
[01:54:53.360 --> 01:54:56.720]   This is probably just an ad for their services.
[01:54:56.720 --> 01:55:01.120]   Self-driving insurance.
[01:55:01.120 --> 01:55:05.200]   It's responsible. In general, liability rests with one or more
[01:55:05.200 --> 01:55:11.040]   to your point, Denise, of three options. Human error, vehicle malfunction,
[01:55:11.040 --> 01:55:16.720]   lacks government oversight. The government's got deep pockets, and improper design and
[01:55:16.720 --> 01:55:23.760]   manufacturing. So, let's just sue them all. The only person I guess not responsible is the guy in
[01:55:23.760 --> 01:55:32.640]   the backseat. Maybe even he caused a problem. We don't know. Facebook facing a little scrutiny
[01:55:32.640 --> 01:55:41.280]   because, oh, golly, another 500 million user information leaked. This is old information,
[01:55:41.280 --> 01:55:49.200]   except as the story evolved, Facebook looked more and more culpable. The data from roughly
[01:55:49.200 --> 01:55:55.680]   533 million Facebook users included things like profile names, Facebook ID numbers, email addresses,
[01:55:55.680 --> 01:56:01.760]   and phone numbers. This is all stuff that is probably out there anyway if you're on Facebook.
[01:56:01.760 --> 01:56:10.640]   Facebook said initially, oh, this is the breach we reported in 2019. We fixed that in August of 2019.
[01:56:11.520 --> 01:56:19.760]   But now we're starting to think that it actually was scraped from Facebook
[01:56:19.760 --> 01:56:31.440]   by using a Facebook tool. So, as Facebook eventually explained, this is from a wired.
[01:56:31.440 --> 01:56:35.440]   In a background, comments to wired, in its Tuesday blog as well,
[01:56:36.640 --> 01:56:43.360]   it's not, no, that 533 million records. It's an entirely different data set.
[01:56:43.360 --> 01:56:50.640]   This one was created by abusing a flaw in the Facebook address book contacts import feature.
[01:56:50.640 --> 01:56:59.440]   It's kind of ingenious. It really is. It's basically a person created a contact list with
[01:56:59.440 --> 01:57:03.520]   every phone number in existence. Shoved it through the tool. Clever.
[01:57:03.520 --> 01:57:07.760]   And then as Facebook does, it assumed that if it was in your contacts list, you must know the
[01:57:07.760 --> 01:57:11.440]   person and it said, oh, do you want to add your friends on Facebook?
[01:57:11.440 --> 01:57:20.400]   This is why we fuzz our identity. Yeah, maybe now. Now this is the answer to that question.
[01:57:20.400 --> 01:57:25.520]   By the way, that same year, there was a vulnerability discovered in Instagram,
[01:57:25.520 --> 01:57:33.440]   same contact import, same problem. Facebook said, no, we already knew about that.
[01:57:33.440 --> 01:57:44.720]   And we've fixed it. Now, I mean, it just gets worse. It's really unclear if this is all old data,
[01:57:44.720 --> 01:57:52.400]   if there's new data, we had a bug in our system. But did we fix it? Did we fix it completely?
[01:57:52.400 --> 01:57:59.520]   It's just a mess. What do you tell people? If you put stuff on Facebook, assume,
[01:58:00.160 --> 01:58:05.040]   I've always said this, assume it's going to be public. I've just been telling people don't use
[01:58:05.040 --> 01:58:09.360]   Facebook. Oh, that's another way to do Instagram. Don't use WhatsApp. But I mean, very few are
[01:58:09.360 --> 01:58:13.360]   listening. And I've been telling people if you're going to use those services, don't use your real
[01:58:13.360 --> 01:58:18.320]   data. Dye. Obviously. Yeah.
[01:58:18.320 --> 01:58:24.480]   Facebook says we're not going to notify you just assume.
[01:58:29.520 --> 01:58:33.520]   It's too many. We're going to pre notify you. We're just going to when you sign up for the account,
[01:58:33.520 --> 01:58:36.720]   we'll know. There's too many people information that's been compromised. We can't.
[01:58:36.720 --> 01:58:40.880]   We're going to just like the plaintiff's lawyers aren't going to listen to insurance
[01:58:40.880 --> 01:58:44.400]   companies. They're not going to listen to Facebook either. Yeah. Yeah. Yeah.
[01:58:44.400 --> 01:58:55.280]   Yeah. Microsoft is apparently and talks to buy nuance. This is this for me. The illustration
[01:58:55.280 --> 01:58:59.360]   here is a little fish being eaten by a big fish by a bigger fish by a bigger fish.
[01:58:59.360 --> 01:59:07.040]   By a bigger fish. nuance bought up all of the voice to the speech to text software that was on the
[01:59:07.040 --> 01:59:12.560]   market. Right. And at the time, a lot of fax software. I don't suppose that's much of a profit
[01:59:12.560 --> 01:59:21.680]   center anymore. So Microsoft had its own. Remember, they had to learn out and house be, I think.
[01:59:21.680 --> 01:59:27.040]   And then this this collapsed thought there was dragon naturally speaking.
[01:59:27.040 --> 01:59:32.000]   nuance bought it. There was it collapsed the whole market into nuance.
[01:59:32.000 --> 01:59:42.320]   Revenue of $1.48 billion on net income of $91 million. They lost $217 million a year before.
[01:59:42.320 --> 01:59:47.680]   Microsoft apparently in time. They just want to buy everybody. Are they still
[01:59:47.680 --> 01:59:51.200]   talking about buying discord? I don't know if that's still on the table.
[01:59:52.320 --> 01:59:56.640]   nuance used to be the one that underpinned Siri in the earliest days.
[01:59:56.640 --> 02:00:00.880]   Siri used nuance. That's right. Yeah. They used their technology. It was never it was one of
[02:00:00.880 --> 02:00:04.880]   those things that like everybody kind of knew, but Apple had never specifically said, I think.
[02:00:04.880 --> 02:00:09.600]   And then Apple, you know, of course, because it wanted to control everything. It built it all
[02:00:09.600 --> 02:00:13.760]   in house. Yeah. House instead. So that makes sense. They probably cause some of their
[02:00:13.760 --> 02:00:17.200]   popularity to go off a cliff. That's where they lost $217 million. Yeah.
[02:00:17.200 --> 02:00:21.760]   Well, that's what Apple's banned. So much both between the fact that Apple did it and
[02:00:21.760 --> 02:00:25.040]   everybody else, right? Like Amazon was doing voice recognition and Microsoft's doing voice
[02:00:25.040 --> 02:00:28.320]   recognition and Google's doing voice recognition and all the big tech companies started doing it in
[02:00:28.320 --> 02:00:36.640]   house. Nobody needs nuance anymore. Right. The price is $56 a share. I don't know. I should
[02:00:36.640 --> 02:00:41.040]   I should have been prepared. I could have looked up new boxes stock price, but I'll be frank with
[02:00:41.040 --> 02:00:50.240]   you. I don't really care. I don't I this is another one. I it wasn't that I didn't care. I just
[02:00:50.240 --> 02:00:57.600]   couldn't bring myself to watch the monkey playing ping pong using Elon Musk's neural link.
[02:00:57.600 --> 02:01:03.360]   He's got to see the big wire in his head. Oh my God. Did anybody watch that? So I don't have to.
[02:01:03.360 --> 02:01:09.120]   Only watched it because it was on Fallon and Fallon's joke was then the monkey took a call from his
[02:01:09.120 --> 02:01:14.080]   buddy monkey and asked how all those shampoo trials were going.
[02:01:15.600 --> 02:01:21.600]   I'm just here playing on. I feel so bad for this poor mckack. They put a chip in its head.
[02:01:21.600 --> 02:01:34.800]   And by the way, they come to show you how little they care about the monkey. Its name is apparently
[02:01:34.800 --> 02:01:42.160]   pager. Oh my God. Come on, man. Really? You couldn't come up with better than pager.
[02:01:44.240 --> 02:01:52.640]   And my brother like her here and more recent pda. My brother pda pagers at first shown using a
[02:01:52.640 --> 02:02:00.000]   joystick than eventually using only its mind to what is he? Is he vaping there? What is that?
[02:02:00.000 --> 02:02:06.720]   What's going on? What's going on there? What the hell have you done to this? Why are we?
[02:02:06.720 --> 02:02:11.280]   Why are we just turning monkeys into the worst versions of ourselves? This is like sitting around
[02:02:11.280 --> 02:02:14.080]   all day. This is just annoying.
[02:02:14.080 --> 02:02:20.480]   Bringing them down to our level. Today we are pleased to reveal the link's capability to enable a
[02:02:20.480 --> 02:02:28.240]   mckack monkey named pager to move a cursor on a computer screen using neural activity with a
[02:02:28.240 --> 02:02:36.000]   1024 electrode fully implanted neural recording and data transmission device termed the n1 link.
[02:02:37.520 --> 02:02:43.920]   Oh, that poor monkey. I just feel bad for him. First they named me pager. Then they put a chip
[02:02:43.920 --> 02:02:48.960]   in my head. Now I have to play pong for the rest of my life. At least get him a better game.
[02:02:48.960 --> 02:02:55.600]   God pong, really. But would you want a neural link if it would make your call of duty score
[02:02:55.600 --> 02:03:00.640]   is much better? If I could play Valheim without actually getting out of bed, maybe.
[02:03:01.920 --> 02:03:07.920]   If you could play Valheim while broadcasting while I'm doing a show, maybe. Now we're talking.
[02:03:07.920 --> 02:03:16.880]   Now we're talking. I mean the applications for people who are disabled are of course,
[02:03:16.880 --> 02:03:22.400]   but there's a long way between. Well, Dan, you're the science fiction writer. How long
[02:03:22.400 --> 02:03:27.920]   before we can use this to do something really interesting? I guess if you could play ping pong,
[02:03:27.920 --> 02:03:34.480]   you're pretty close, right? I mean pong is fairly simple. I would think that there's probably
[02:03:34.480 --> 02:03:38.800]   a ways to go before you start getting more complex stuff. That's dead. I mean, I grew
[02:03:38.800 --> 02:03:45.840]   beneath the technologies for assistive stuff is huge. That's a huge opportunity there. I think
[02:03:45.840 --> 02:03:50.240]   that the opportunity before it's commercialized, I think it's I think we're a ways off. I think
[02:03:50.240 --> 02:03:56.880]   you'll be getting over the hump of would you if you are a person who doesn't need a device implanted
[02:03:56.880 --> 02:04:00.800]   in your brain? Will you electorally get something implanted in your brain?
[02:04:00.800 --> 02:04:05.280]   Denise has a big one. I think that's a big hump. If you're a paraplegic and you can't,
[02:04:05.280 --> 02:04:10.080]   you know, this is the only way. I mean, this is great better than a stick or a puff mechanism.
[02:04:10.080 --> 02:04:15.120]   But if you're just looking for it for like, oh, I'll improve my Call of Duty game. I think we
[02:04:15.120 --> 02:04:19.360]   I think we got a long way to go before willing to make that jump. This is not the metaverse yet.
[02:04:19.360 --> 02:04:25.280]   Let's take a break in this sort of technology is also being used in Parkinson's patients.
[02:04:25.280 --> 02:04:30.800]   Yes, little electrodes. They put I actually it works far more fascinating. It works. I have a friend
[02:04:30.800 --> 02:04:38.720]   good friend. It is implant. It's completely halted the tremors. It's incredible. It's a it's
[02:04:38.720 --> 02:04:44.080]   really a miracle cure. And then there's the cochlear implants, which are also fascinating.
[02:04:44.080 --> 02:04:50.160]   What microphone are you using, Robert? I just noticed that. Is that something new?
[02:04:50.160 --> 02:04:55.760]   This is a high up here. 40, but you got a good 3D printed my own isolation mount.
[02:04:55.760 --> 02:05:02.560]   Oh my God. You're such a geek. I thought you three you thought you were going to say,
[02:05:02.560 --> 02:05:07.360]   3D printed a leather driving club to wrap around it because that's kind of what it looks like.
[02:05:07.360 --> 02:05:12.320]   Let's take a little break. Final words in a moment with our fabulous panel. I showed
[02:05:12.320 --> 02:05:18.640]   today brought to you by Barracuda. Barracuda hacker. You know the name. One of the best known
[02:05:18.640 --> 02:05:25.200]   names in enterprise security. This message from Barracuda hackers. They're always looking for
[02:05:25.200 --> 02:05:31.040]   the weakest link in your security configuration. And of course, the way hackers get into your
[02:05:31.040 --> 02:05:37.440]   network so often is with phishing emails through your email security. If you can find those email
[02:05:37.440 --> 02:05:42.080]   vulnerabilities before anybody else, you can defend against cyber attacks. So Barracuda has
[02:05:42.080 --> 02:05:48.320]   created something new and very cool. It's their threat analyzer tool from traditional malware to
[02:05:48.320 --> 02:05:54.160]   the latest spear phishing account takeover conversation hijacking. Barracuda has identified
[02:05:54.160 --> 02:06:01.440]   13 types of email threats. And you can't just say, oh, block them all. You know, you have to have
[02:06:01.440 --> 02:06:06.560]   several layers of security working together to protect you effectively against all 13. And if
[02:06:06.560 --> 02:06:12.240]   any one of those is still vulnerable, you know the bad guys are going to figure out which one,
[02:06:12.240 --> 02:06:17.120]   and they're going after you. It's literally all they spend their time on. When they find that gap
[02:06:17.120 --> 02:06:21.840]   in your security, they choose the appropriate threat type. They customize it to get into your
[02:06:21.840 --> 02:06:26.880]   system. And they can end up costing you millions of dollars and your reputation in the market.
[02:06:26.880 --> 02:06:31.440]   Now here's where it gets hard with hundreds of highly targeted personalized threat variants emerging
[02:06:31.440 --> 02:06:36.480]   daily and many different kinds of on Prem and cloud based email systems. It can be challenging
[02:06:36.480 --> 02:06:44.800]   to identify your specific gaps or vulnerabilities. I invite you to take the Barracuda threat analyzer.
[02:06:44.800 --> 02:06:51.440]   It's simple and fast to use Barracuda.com/twit. You'll answer some multiple choice questions
[02:06:51.440 --> 02:06:56.080]   about your email security setup. It shouldn't take more in a couple of minutes. And then the
[02:06:56.080 --> 02:07:00.720]   Barracuda threat analyzer will provide a custom report telling you which threat types you're
[02:07:00.720 --> 02:07:06.080]   most vulnerable to. You'll get custom recommendations on how to strengthen your defenses against those
[02:07:06.080 --> 02:07:11.760]   attacks. And by the way, this is all free and easy to use. Barracuda's December spear phishing
[02:07:11.760 --> 02:07:20.080]   report found 12% 12% of all spear phishing attacks are business email compromise. And that's almost
[02:07:20.080 --> 02:07:26.240]   doubled from 2019. That's because these attacks work. According to the FBI's most recent internet
[02:07:26.240 --> 02:07:31.840]   crime report, business email, compromised attacks led to over three and a half billion dollars in
[02:07:31.840 --> 02:07:38.960]   losses last year. Government of Puerto Rico lost 2.6 million in a single attack last year. This is
[02:07:38.960 --> 02:07:46.160]   all through email. Does your email security protect against email compromise? Try the Barracuda threat
[02:07:46.160 --> 02:07:50.400]   analyzer today. If it's cost you nothing, you'll get a full report showing you exactly what you
[02:07:50.400 --> 02:07:57.440]   need to do to secure your email. Barracuda.com/twit. Find out where those hidden threats are. Barracuda's
[02:07:57.440 --> 02:08:05.280]   threat analyzer, barracuda.com/twit. Thank you, Barracuda for supporting this week in tech.
[02:08:06.000 --> 02:08:10.560]   Hey, before we go on real quick programming note, I'll be back here tomorrow morning.
[02:08:10.560 --> 02:08:17.040]   830 Pacific 1130 Eastern time with Sam Ebbles. Samad, we're going to be covering Jensen Wong's
[02:08:17.040 --> 02:08:23.280]   keynote for the NVIDIA GPU technology conference. This is normally something we don't cover,
[02:08:23.280 --> 02:08:28.240]   but I think NVIDIA is going to have a lot to say. They're very, you know, right now they're, I think,
[02:08:28.240 --> 02:08:33.920]   one of the prime movers in technology. And I'm very interested what they have to say. Sam's going
[02:08:33.920 --> 02:08:38.480]   to join us because of course they're big in self-driving vehicles, but of course gaming,
[02:08:38.480 --> 02:08:46.480]   Bitcoin mining, machine learning, NVIDIA is everywhere. So if you're around a Monday morning,
[02:08:46.480 --> 02:08:53.520]   830 Pacific 1130 Eastern, good news. Wong has said he's only going to do an hour.
[02:08:53.520 --> 02:09:01.120]   So it won't be a long morning. We'll just get to the meat of the matter quickly. Join me at 830.
[02:09:01.680 --> 02:09:07.840]   So I better affect. We better run through the rest of these stories. I'm going to be exhausted.
[02:09:07.840 --> 02:09:17.680]   E3 2021 virtual, but they have set the dates. They will go live again in LA, they hope in 2022.
[02:09:17.680 --> 02:09:25.440]   So I still waiting for Mobile World Congress to say, okay, we give, but they still want to do a
[02:09:25.440 --> 02:09:31.440]   live event next month or is it this month? There are a few. I'm waiting for DefCon. I'm waiting
[02:09:31.440 --> 02:09:38.160]   for CES. I mean, these are conferences that people are going to be spending a lot of money very
[02:09:38.160 --> 02:09:41.920]   soon to try to start setting something up if they don't announce. It has become a little bit
[02:09:41.920 --> 02:09:48.720]   of a ritual to say, oh, we're going to be live and then never mind. I don't know why they,
[02:09:48.720 --> 02:09:56.880]   you know, I mean, do what E3 did, said, look, you know, June's coming. We'll do E3 virtually next year.
[02:09:56.880 --> 02:10:02.720]   It's okay. Next year is fine. I mean, to the group 2021 is pretty much a write-off for live events,
[02:10:02.720 --> 02:10:07.440]   right? I mean, even in the States, if we get that magical number of vaccinations,
[02:10:07.440 --> 02:10:13.040]   that's not the rest of the world. Right. That's a big problem, isn't it? We're, you know,
[02:10:13.040 --> 02:10:19.840]   I'm starting to feel a little guilty. We're doing 3 million vaccinations a day in the US.
[02:10:19.840 --> 02:10:23.920]   And how many vaccinations a day are they doing in Italy right now, Father Robert?
[02:10:24.560 --> 02:10:31.280]   It's like 15 or 16 maybe. But it's not, it's not for lack of trying or wanting to do it.
[02:10:31.280 --> 02:10:34.320]   You can't get them. Right. We bought them all up.
[02:10:34.320 --> 02:10:42.320]   And apparently there is a little bit of incompetence. I mean, it's a tiny, tiny, tiny bit, a soup song.
[02:10:42.320 --> 02:10:48.080]   You could say that if, you know, USA, USA, we bought them up because we could, you know,
[02:10:48.080 --> 02:10:54.400]   we figured it out. LG's closing its mobile phone business worldwide. I think the kiss
[02:10:54.400 --> 02:10:58.720]   of death was when they announced the phone. Remember the wing they announced it earlier this year?
[02:10:58.720 --> 02:11:04.560]   Flips around and turns into a tea. I thought it was cool. I heard it was terrible, but I thought,
[02:11:04.560 --> 02:11:08.320]   I like the idea, I liked the thinking outside the box. Yeah, it's outside the box. It always seemed
[02:11:08.320 --> 02:11:13.120]   to do good. They did like weird stuff that nobody else was ever going to do. It didn't work for them,
[02:11:13.120 --> 02:11:18.080]   but I appreciated it. There were some really nice, I mean, I like outside the box, but at some point
[02:11:18.080 --> 02:11:22.160]   someone in the testing lab should have held the phone and said, there's no good way to hold this.
[02:11:23.120 --> 02:11:26.960]   What are we doing? I need to rethink why we're trying to fly.
[02:11:26.960 --> 02:11:32.640]   I think the nail was already in their coffin, unfortunately, but then the last ditch.
[02:11:32.640 --> 02:11:40.480]   Nice segue for me. Empathy is out of stealth. A digital assistant aimed at bereaved families.
[02:11:40.480 --> 02:11:46.080]   It was only a matter of time. This feels like something out of a science fiction novel.
[02:11:46.080 --> 02:11:50.240]   Yeah, that's, that's what I want. Is this like a parking page for social media accounts?
[02:11:50.240 --> 02:11:55.200]   Is that the thing we provided? We provided that companion in the form of native apps
[02:11:55.200 --> 02:12:02.560]   that are built to empower bereaved families. What? Okay, the CEO says it's like GPS for the
[02:12:02.560 --> 02:12:08.720]   recently bereaved. Now, now, do you, now do you understand? No, that's not a problem anyone has.
[02:12:08.720 --> 02:12:17.040]   My beer is on body. It's an AI based platform. You have bigger problems.
[02:12:19.840 --> 02:12:27.120]   Okay, we shouldn't laugh. But it's funny. Okay. Speaking of end the line,
[02:12:27.120 --> 02:12:34.160]   Logitech is discontinuing harmony remotes. This is criminal. These are the only good universal
[02:12:34.160 --> 02:12:40.800]   remotes. I just had to buy one of mine, a button died on it, and I couldn't find that model anymore.
[02:12:40.800 --> 02:12:44.240]   And so I bothered one of my co-hosts on one of my shows. He's like, Oh yeah, I don't use my
[02:12:44.240 --> 02:12:48.800]   morphs. Like I'll send it to me. And I'm like, this is now when this one dies, I will have no recourse.
[02:12:48.800 --> 02:12:53.680]   Like, because I love my Logitech. It's not the best. It's time to start hoarding. It's time to start.
[02:12:53.680 --> 02:12:59.200]   Yeah. It's one of those things. It wasn't that it was good, but it was so much better than
[02:12:59.200 --> 02:13:03.040]   everything else that was out there. I've used some other ones and they just all fall flat.
[02:13:03.040 --> 02:13:07.840]   The secret sauce was the database, right? Because it knew how everything worked. And you could
[02:13:07.840 --> 02:13:11.760]   just say, I have this, this and this. And the database would say, good, we've programmed your
[02:13:11.760 --> 02:13:16.080]   remote was so much easier than anybody else. It works nicely. Yeah. And it's just smart and
[02:13:16.080 --> 02:13:19.120]   it was flexible and the software was terrible, but it worked.
[02:13:19.120 --> 02:13:24.240]   Harmony says they will continue to maintain the database. We're just not going to manufacture
[02:13:24.240 --> 02:13:29.440]   anymore. And I have to think it just be they said it just the business is too small. And I have to
[02:13:29.440 --> 02:13:37.440]   think it's been shrinking because nowadays, you know, used to be you'd have 20 remotes on the sofa
[02:13:37.440 --> 02:13:44.320]   arms, right? But nowadays, really, you have a streamer. That streamer remote turns on the TV,
[02:13:44.320 --> 02:13:49.760]   turns up and down the volume and then plays the thing back. You probably don't need a harmony as much.
[02:13:49.760 --> 02:13:55.920]   Yeah. And I think most TVs come with like you said, either the TV or the stream.
[02:13:55.920 --> 02:14:00.640]   Some comes with a program with that works with everything. Yeah. Yeah. So.
[02:14:00.640 --> 02:14:07.440]   And then there's CEC. So my TV, I have Xbox on it, Apple TV and
[02:14:08.960 --> 02:14:15.360]   Google TV. Oh, and a Nintendo switch. But I turned on CEC and no matter if I just pick up the remote
[02:14:15.360 --> 02:14:20.800]   for one of those devices, it the TV turns on switches to that channel. I use it in the graph,
[02:14:20.800 --> 02:14:28.000]   the TV. So I guess it's really become easy. Easy. And most of us have TVs that have apps
[02:14:28.000 --> 02:14:32.880]   that are updated and right and perfect for five, 10 years. So it's just the, you know,
[02:14:32.880 --> 02:14:42.720]   at the times are changing. Right to repair very much in the news. Colorado has decided
[02:14:42.720 --> 02:14:51.920]   to deny the right to repair after stories of environmental disaster and wheelchairs on fire.
[02:14:51.920 --> 02:14:58.480]   The right to repair bill died in the Colorado state legislature. Three hours of testimony from
[02:14:58.480 --> 02:15:02.320]   business leaders, disabled advocates, a nine year old activist.
[02:15:02.320 --> 02:15:08.080]   Legislators said there are too many unanswered questions. The proposed law is too broad.
[02:15:08.080 --> 02:15:13.440]   It's not the only state, about half of the states now are considering right to repair laws. And I
[02:15:13.440 --> 02:15:20.480]   think there's even a right to repair federal right to repair movement. But actually Colorado's law was
[02:15:20.480 --> 02:15:28.960]   only 11 pages. It was pretty simple. If you own it, you can fix it. Period. But that's,
[02:15:28.960 --> 02:15:34.560]   there's too many unanswered questions. So John Deere will be very happy. Yeah. Is John Deere in
[02:15:34.560 --> 02:15:40.400]   Colorado? I'm sure they lobby like crazy. No, but I mean, they're their headquarters.
[02:15:42.080 --> 02:15:47.360]   Anyway, sad, but you know, not the end for right of repair. There's a good,
[02:15:47.360 --> 02:15:55.600]   who did the interview with a system 70s? Lewis Rossman did a good, on YouTube, did a good
[02:15:55.600 --> 02:15:59.920]   interview with the guy and engineer at system 76 on right to repair, highly recommended.
[02:15:59.920 --> 02:16:07.760]   And there is finally, there's nothing to watch on TV. So we're just going to keep doing this show
[02:16:07.760 --> 02:16:14.800]   for the next 18 hours because streaming services have finally run out of TV. Is that true? Bloomberg
[02:16:14.800 --> 02:16:20.160]   says it's true. We knew it would happen sometime production was halted for months because of COVID.
[02:16:20.160 --> 02:16:23.440]   It started up again, but it takes, you know, it takes a long time to put a show out.
[02:16:23.440 --> 02:16:29.040]   I don't know. I have to say there don't seem to be very many good movies
[02:16:29.040 --> 02:16:33.440]   on the number of originals on Netflix is declined 12%.
[02:16:36.160 --> 02:16:43.520]   It usually goes up every year. So a drop in 12% significant. HBO's biggest releases in March,
[02:16:43.520 --> 02:16:50.560]   just a recut Justice League, the Zack Snyder cut. Hey, that was one movie that lasts as long as
[02:16:50.560 --> 02:16:56.880]   three. So that was, you know, that was good. Wait, did you, have you watched it? Yeah. I never
[02:16:56.880 --> 02:17:01.920]   saw the original. I loved it. Yeah, I thought it was great. And I did see the original. I watched
[02:17:01.920 --> 02:17:08.880]   over three nights because it was four hours long. But, but and I'm not a comic book fan. And I didn't
[02:17:08.880 --> 02:17:15.600]   see the original because it was so badly reviewed. But I thought it was good because I as not as
[02:17:15.600 --> 02:17:20.720]   as a non comic book fan, I liked all the backstory stuff. What, why did you like it Denise? What,
[02:17:20.720 --> 02:17:24.320]   what? I thought it was a better Wonder Woman movie than the last Wonder Woman movie.
[02:17:24.320 --> 02:17:30.480]   Well, anything's anything's better than the last Wonder Woman movie. That was a long one.
[02:17:30.480 --> 02:17:35.840]   You hit it. I loved all the backstories. Yeah. That's what he had. I never watched the
[02:17:35.840 --> 02:17:43.520]   Superman Batman. No, don't. And I'm a D. You guys. I mean, if I, I mean, I am not a Marvel fan as
[02:17:43.520 --> 02:17:48.080]   much as D.C. I grew up on Batman as Superman. So for me, I liked that. I liked to see them.
[02:17:48.080 --> 02:17:55.760]   I'm teaching a communications course at the Gregorian University and one of the chapters
[02:17:55.760 --> 02:18:01.840]   we did was on editing on color correcting. And I do side by sides from the theatrical release
[02:18:01.840 --> 02:18:07.920]   and the Snyder cut to show how the exact same scenes, the exact same actors, mostly the same pacing
[02:18:07.920 --> 02:18:12.720]   feel completely different when you just change the shade. Just because it's so we changed the
[02:18:12.720 --> 02:18:18.640]   coloring. Yep. He reshaded it. I know there was a lot of new CGI done. They shot a lot of
[02:18:18.640 --> 02:18:24.960]   additional stuff, the backstory stuff. So it is a very different movie. Just the, just the shading.
[02:18:24.960 --> 02:18:29.440]   That's interesting. What do you do to brighten it up? No, he dulled it.
[02:18:29.440 --> 02:18:34.240]   Dulled it. He, he like took away the comic book colors. Yeah. Made it a more dull tone.
[02:18:34.240 --> 02:18:40.800]   Go to YouTube and look for Wonder Woman side by side and they'll play both scenes side by side.
[02:18:40.800 --> 02:18:45.520]   And it feels completely different even though it's the same scene. It's wonderful.
[02:18:45.520 --> 02:18:50.800]   Well, there you go. I have something to watch tonight.
[02:18:52.160 --> 02:18:57.360]   15 minute YouTube video. That solves that streaming problem. Father Robert Ballisair,
[02:18:57.360 --> 02:19:05.440]   digitaljazuit.com is live. You got to go there. I am going to be in instantly. I already have an
[02:19:05.440 --> 02:19:11.280]   account on the Discord server. I think it's from Minecraft. Well, and I want to play,
[02:19:11.280 --> 02:19:17.680]   you told me about this new game, Ress, factorial, almost called it Ress storio. It is kind of rusty,
[02:19:17.680 --> 02:19:22.080]   but it's, but it looks like a lot of fun and I want to play that. So if I'm already on the server,
[02:19:22.080 --> 02:19:28.880]   what do I do to get into the other games? Do I have to? Well, because once you're in the Discord
[02:19:28.880 --> 02:19:35.680]   server, it automatically grants you access to the other servers and all digitaljazuit.com,
[02:19:35.680 --> 02:19:40.480]   Minecraft@digitaljazuit.com and so on and so forth. But I'm going to send you a donation because
[02:19:40.480 --> 02:19:46.640]   you want to finance this. If you feel like it great, I mean, we've got this guy in the chat room
[02:19:46.640 --> 02:19:51.360]   right now, McLovin, who has been basically offering his services and hardware for free.
[02:19:51.360 --> 02:19:55.920]   Oh, that's great. Thank you, McLovin. And yeah, we just want to expand it because
[02:19:55.920 --> 02:20:01.200]   there is an interest. People love playing in a server that isn't completely toxic.
[02:20:01.200 --> 02:20:06.640]   And you never know when you might run into a cardinal. Actually, this is true.
[02:20:06.640 --> 02:20:10.720]   Yeah, you may not know you're running into a cardinal. Oh, he'll own you.
[02:20:10.720 --> 02:20:15.840]   Cardinal Tagli takes no prison on the platform.
[02:20:17.360 --> 02:20:22.640]   So the current games are Minecraft, Rust, and Factorial, you say planning, upcoming games,
[02:20:22.640 --> 02:20:28.080]   TF2, Terraria among us and my game, Valheim. Valheim, I'm playing solo right now,
[02:20:28.080 --> 02:20:31.280]   but I would love to get in a server with some other people.
[02:20:31.280 --> 02:20:37.360]   You know who else plays Valheim? I play, so I will be playing Valheim. You play Valheim.
[02:20:37.360 --> 02:20:40.560]   Good. Brian Burnett plays Valheim. Awesome. Alex.
[02:20:40.560 --> 02:20:43.200]   Oh, nice. Well, all the-
[02:20:43.200 --> 02:20:44.800]   McLovin is saying we've had the Pope.
[02:20:45.520 --> 02:20:49.840]   No, McLovins. What? Is he joking? Which game? What game?
[02:20:49.840 --> 02:20:57.120]   Minecraft. Did he build anything? No, he looked at it and typed a message.
[02:20:57.120 --> 02:21:00.640]   He didn't sleep in one of the beds and make it his spawn point.
[02:21:00.640 --> 02:21:04.160]   No, it was basically he was just coming through the house and we're like,
[02:21:04.160 --> 02:21:06.240]   "Hey, can you take a look at this really cool?" Oh, that's really cool.
[02:21:06.240 --> 02:21:13.760]   Yeah. Really cool. Thank you, McLovin, for helping Father Robert out. That's pretty exciting.
[02:21:14.560 --> 02:21:18.400]   Mr. Dan Morin is at sixcolors.com,
[02:21:18.400 --> 02:21:26.480]   @dmorin on the Twitter @dmorin, and his books are available at six-color. No,
[02:21:26.480 --> 02:21:28.880]   you have a- Where do I go for your books? Is it Dan Morin?
[02:21:28.880 --> 02:21:33.600]   Anywhere. Anywhere. You can go to dmorin.com, but they're on Amazon. They're in bookstores.
[02:21:33.600 --> 02:21:36.560]   I've got one right here. I'll hold it up for the camera. Hold it up. There it is,
[02:21:36.560 --> 02:21:43.200]   the Alif Extraction, the latest. But if I buy it through your website, you get a cut, right?
[02:21:43.200 --> 02:21:47.600]   It's not even for sale on my website, but I've got links to Amazon, Barnes & Noble,
[02:21:47.600 --> 02:21:53.600]   Kobo, you're basically anybody. But if I buy it through the link to Amazon, you get extra money,
[02:21:53.600 --> 02:21:58.000]   I hope. Probably. I think I've got an affiliate link on my website. Probably. He doesn't even know.
[02:21:58.000 --> 02:22:02.720]   He doesn't even know. That's my publisher does all the importance. Oh, yeah. Well, that means they
[02:22:02.720 --> 02:22:07.920]   get the cuts. Yeah. Yeah, that's right. I'm just the talent. Yeah. Well, I can't wait. That's
[02:22:07.920 --> 02:22:13.360]   exciting. Thank you, Dan. It's great to see you again. And of course, always a pleasure and a
[02:22:13.360 --> 02:22:18.720]   privilege to have Denise Howell on the show, especially when there's thorny legal questions,
[02:22:18.720 --> 02:22:23.600]   but anytime. Thank you so much for having me. Can I give two PSAs before we run?
[02:22:23.600 --> 02:22:29.840]   PSA time. All right. So the first is a more serious one. This week is the anniversary of
[02:22:29.840 --> 02:22:34.800]   Kurt Cobain's death. I don't know if anybody caught Saturday night live last night, but that dress
[02:22:34.800 --> 02:22:40.800]   kid, Kedy, was so brilliantly wearing was a tribute. Both of his ensembles were
[02:22:40.800 --> 02:22:46.960]   tribute sticker Cobain. He was only 27. Yeah. That's a bad year, generally, for everybody.
[02:22:46.960 --> 02:22:55.520]   Our national suicide prevention hotline is 800-273-8255. Good. It's getting harder and
[02:22:55.520 --> 02:23:01.680]   harder to find people who haven't been touched by this in their lives. So definitely be there
[02:23:01.680 --> 02:23:08.080]   for your friends and family and for all and sundry. And then on a lighter note,
[02:23:08.080 --> 02:23:15.440]   tomorrow is when Sam Adams starts giving away free beer. Wait a minute. What?
[02:23:15.440 --> 02:23:21.360]   They don't give it away tomorrow. It's the day that they're going to start. I guess the first 10,000
[02:23:21.360 --> 02:23:28.720]   people who post to social media, shoot, I don't know what the hashtag is, but you can look it up.
[02:23:28.720 --> 02:23:35.360]   If you post that you have been vaccinated, please don't post your Vax card. All you can do is post
[02:23:35.360 --> 02:23:41.360]   one of their little stickers or just a band-aid on your arm. That's going to be fine for their
[02:23:41.360 --> 02:23:45.520]   proof purposes. And the first 10,000 people are going to get a voucher for a free beer.
[02:23:45.520 --> 02:23:51.680]   Hashtag shot for Sam. There we go. And I can't believe the Boston guy. I didn't mention that.
[02:23:51.680 --> 02:23:56.240]   No, I didn't know. I didn't know, man. How could you not know? Everybody knows.
[02:23:56.240 --> 02:24:00.960]   Wait, why is it Laganita's doing something like this? National beer day.
[02:24:00.960 --> 02:24:07.600]   So it doesn't have to just be Sam Adams. Beer money for the first 10,000 people.
[02:24:07.600 --> 02:24:14.000]   You could post the back of your card. Would that work? I know you asked me some questions.
[02:24:14.000 --> 02:24:19.520]   I don't like the idea. I don't know all the ins and outs of posting cards and what people can do
[02:24:19.520 --> 02:24:23.920]   with your card data once they have it. But I don't know that I want the answers to those questions.
[02:24:23.920 --> 02:24:29.200]   I'd just rather you keep your cards to yourself. All right, I got to play because I love the Sam
[02:24:29.200 --> 02:24:34.720]   Adams ads with your cousin from Boston. I get to play this one just for this doesn't take you long.
[02:24:34.720 --> 02:24:43.040]   I'm double parked. That's all in his pants. From Boston.
[02:24:43.040 --> 02:24:46.800]   Sam's on me.
[02:24:46.800 --> 02:24:51.280]   In your face. In your face.
[02:24:52.560 --> 02:24:58.400]   Did I get the shot? No, you saw the needle and passed out cold. Here you go. Continue to wear a mask.
[02:24:58.400 --> 02:25:02.480]   Good luck, guys. It's a breeze.
[02:25:02.480 --> 02:25:11.520]   That's too accurate. That's in from Boston. I love this. I'm from Providence. It's practically
[02:25:11.520 --> 02:25:18.720]   Boston. Thank you, Denise. Thank you so much for being here. Thank you, Dan Morin.
[02:25:18.720 --> 02:25:23.600]   Father Robert Ballas here. Thank you all for joining us. We do tweet on a Sunday afternoon around
[02:25:23.600 --> 02:25:30.160]   230 Pacific 530 Eastern 2130 UTC. There's a live stream you could watch if you don't have to because
[02:25:30.160 --> 02:25:35.040]   it's a podcast. We make it available after the fact. But if you want to watch us do it live,
[02:25:35.040 --> 02:25:41.680]   all the stuff we cut out later, it's twit.tv/live. People who watch live really should chat live at
[02:25:41.680 --> 02:25:48.960]   irc.twit.tv, a wonderful community forum where I get all my best jokes. You can also get on-demand
[02:25:48.960 --> 02:25:55.920]   versions of our show and every show we do at our website, twit.tv. That's my Sam Adams ad.
[02:25:55.920 --> 02:26:02.720]   There we go. Twit.tv. You can also get it on YouTube. There's a YouTube channel for all of our
[02:26:02.720 --> 02:26:07.680]   shows. In fact, go to the master channel, youtube.com/twit. You get links to all the shows.
[02:26:08.720 --> 02:26:13.200]   And little twit bits and so forth. Best way to do this though, get yourself a podcast app
[02:26:13.200 --> 02:26:18.640]   and subscribe. In fact, while you're subscribing, leave a nice review for us if you would.
[02:26:18.640 --> 02:26:24.720]   We really appreciate it. Get it automatically that way every Sunday night after the show is done.
[02:26:24.720 --> 02:26:28.640]   Thanks for being here everybody. We'll see you next time. Another twit. Actually,
[02:26:28.640 --> 02:26:32.720]   peak announcement next Sunday. Make sure you tune in. Another twit.
[02:26:33.040 --> 02:26:34.720]   This is amazing.
[02:26:34.720 --> 02:26:45.340]   ♪♪♪
[02:26:45.340 --> 02:26:47.060]   ♪ Do it with a toy ♪

