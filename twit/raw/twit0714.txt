;FFMETADATA1
title=Baby's First Chinese Internet
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=714
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2019
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.000]   It's time for Twit this weekend tech. Thanks to Jason Stelfe for filling in for me last week.
[00:00:04.000 --> 00:00:08.640]   I'm back and man we have a great show for you. Amy Webb is here, Christina Warren.
[00:00:08.640 --> 00:00:12.960]   We're going to talk about the arrest of Julian Assange. Can you defend him?
[00:00:12.960 --> 00:00:20.880]   Maybe you can. We'll also talk about Amazon. They're moving into your house. And baby's first
[00:00:20.880 --> 00:00:26.080]   Chinese internet. How Amy keeps her child safe online. It's all coming up next on Twit.
[00:00:28.560 --> 00:00:31.840]   NetCasts you love. From people you trust.
[00:00:31.840 --> 00:00:37.280]   This is Twit.
[00:00:37.280 --> 00:00:52.720]   This is Twit this weekend tech episode 714 recorded Sunday April 14th 2019.
[00:00:52.720 --> 00:00:58.720]   Babies first Chinese internet. This weekend tech is brought to you by Zip Recruiter.
[00:00:58.720 --> 00:01:03.840]   Hiring is challenging but there's one place you can go where hiring is simple and smart.
[00:01:03.840 --> 00:01:08.800]   That place is Zip Recruiter where growing businesses connect to qualified candidates.
[00:01:08.800 --> 00:01:17.040]   Try it free at ziprecruiter.com/twit. And by Molecule. Molecule is reimagining the future of clean air
[00:01:17.040 --> 00:01:23.360]   starting with the air purifying. For $75 off your first order visit Molecule.com and enter the promo
[00:01:23.360 --> 00:01:31.120]   code TWIT1. And by ExpressVPN. Protect your online privacy with one click. It's that easy.
[00:01:31.120 --> 00:01:36.320]   For three extra months free with a one year package go to expressvpn.com/twit.
[00:01:36.320 --> 00:01:43.360]   And by WordPress. Turn your dreams into reality and launch your website at WordPress.com.
[00:01:43.360 --> 00:01:47.920]   Get 15% off any new plan at WordPress.com/twit.
[00:01:47.920 --> 00:01:52.880]   It's time for Twit this weekend tech the show where we cover the weeks.
[00:01:52.880 --> 00:02:01.120]   Tech news Aloha from Hawaii East. I'm Leo Laport. Thanks to Jason Snell for filling in for me last
[00:02:01.120 --> 00:02:06.560]   week. Back from vacation as you might have noticed and we thought we'd have some fun this week with
[00:02:06.560 --> 00:02:13.760]   two of my favorite people Amy Webb is here our futurist from Amy Webb.io. She is the founder of
[00:02:13.760 --> 00:02:21.200]   the the the future today Institute professor of strategic foresight at NYU Stern School and the
[00:02:21.200 --> 00:02:26.960]   author of two wonderful books the signals are talking where she explains how you can do her job.
[00:02:26.960 --> 00:02:31.600]   And then the big nine where she shows how you can't do her job only Amy Webb can do her job.
[00:02:31.600 --> 00:02:34.560]   This is the new book and really really good. Hi Amy welcome.
[00:02:35.120 --> 00:02:42.720]   Hey Leo welcome back. It's good to have you here also joining us. She's been all over the world.
[00:02:42.720 --> 00:02:47.760]   Christina Warren senior cloud developer advocate at Microsoft. She's been on the
[00:02:47.760 --> 00:02:56.240]   Ignite tour to Milan and Dubai and all over the place. Hello Christina. Hey Leo welcome back.
[00:02:56.240 --> 00:03:00.720]   Thank you. Welcome back to you too. Thank you. All right you're not done though.
[00:03:02.080 --> 00:03:07.440]   No I will be in Stockholm next week and then I will have a few weeks off but build as in between
[00:03:07.440 --> 00:03:15.040]   that time and then I will be in Mumbai at the end of May and then knock on wood. I will not be
[00:03:15.040 --> 00:03:20.800]   traveling internationally for a little bit. So what do you do on these these these Ignite tours
[00:03:20.800 --> 00:03:26.880]   for developers right. Yeah they're for developers and for IT professionals and operations people.
[00:03:26.880 --> 00:03:33.040]   And so I actually give two talks at each stop is what I've been doing. So I help out with the
[00:03:33.040 --> 00:03:37.120]   opener which is kind of like what we're doing in lieu of a keynote where we kind of show off
[00:03:37.120 --> 00:03:42.240]   some of the different things that that you can do with them you know visual studio code with live
[00:03:42.240 --> 00:03:50.640]   share and and some of our different CI/CD interfaces. And then I give a talk on introduction to Azure
[00:03:50.640 --> 00:03:56.000]   and cloud computing. So it's kind of like a high level 101 you know course for people who
[00:03:56.000 --> 00:04:00.160]   maybe aren't familiar with cloud computing and especially kind of the Azure ecosystem.
[00:04:00.160 --> 00:04:03.840]   It goes into things like you know the differences between containers and VMs and
[00:04:03.840 --> 00:04:08.480]   talks a little bit about what serverless computing is and things like that. And then I do a talk
[00:04:08.480 --> 00:04:15.200]   on introduction to Azure networking which is talking basically about how various networking
[00:04:15.200 --> 00:04:21.680]   virtual networking a load balancer and other types of services work on Azure and how people can
[00:04:21.680 --> 00:04:27.680]   you know use that to connect their on-prem stuff to what might be in the cloud or and and all that.
[00:04:27.680 --> 00:04:34.000]   Well it's great to have you both on. I followed the tech news the whole time I was in Kauai.
[00:04:34.000 --> 00:04:40.320]   I did. I know because I would know that I was coming back today and I thought I was going to
[00:04:40.320 --> 00:04:46.240]   want to know what happened. I wasn't I admit a little shocked to turn on the TV a couple of
[00:04:46.240 --> 00:04:52.640]   days ago and see Julian Assange carted away from the Ecuadorian embassy in London looking. I'm
[00:04:52.640 --> 00:04:58.800]   afraid a little like little haggard. Well I think he was going for the Jesus thing. I don't know
[00:04:58.800 --> 00:05:05.200]   what it was. He almost was like blessing people as as they took him out. There he is.
[00:05:05.200 --> 00:05:13.440]   So but there are two takes on this. The US government says oh no we're not we're not going
[00:05:13.440 --> 00:05:21.520]   after him for what he published. We're going after him because he hacked and encouraged hacking.
[00:05:21.520 --> 00:05:27.680]   He helped Chelsea Manning break in to a classified system. They have emails saying my team is working
[00:05:27.680 --> 00:05:34.080]   on it. That's the smoking gun apparently. On the other hand it's pretty obvious that the minute they
[00:05:34.080 --> 00:05:42.080]   arrested Chelsea Manning they were gunning for Julian Assange. WikiLeaks published a stunning video
[00:05:42.080 --> 00:05:50.240]   which I vividly remember called collateral murder showing US drones used on civilians
[00:05:50.240 --> 00:05:57.520]   actually was military helicopters on civilians and journalists. And this is where
[00:05:57.520 --> 00:06:04.720]   WikiLeaks really became famous. More recently of course in the election in 2016 Julian went from
[00:06:04.720 --> 00:06:12.160]   a hero of the left to enemy of the left. When he we don't know he published emails from the
[00:06:12.160 --> 00:06:16.960]   Democratic National Committee. We don't know if he was acting on Russia's behalf or not.
[00:06:16.960 --> 00:06:21.360]   But in this in any case that's not what he's been arrested for. He's been arrested for hacking.
[00:06:21.360 --> 00:06:28.880]   I'm the Obama administration I think. He was public enemy number one for the Obama administration.
[00:06:28.880 --> 00:06:35.520]   Yes. But there was I think a very salient reason that the Obama administration did not pursue charges
[00:06:35.520 --> 00:06:42.080]   against him. And that was because we don't have a clear definition of what constitutes speech
[00:06:42.080 --> 00:06:51.440]   in an era of big tech. And so if they were to pursue charges and if somebody were to claim that he
[00:06:51.440 --> 00:06:56.320]   was a publisher doing an act of journalism. And the first amendment protects him.
[00:06:57.520 --> 00:07:03.760]   Well, I mean, it may or may not have protected him, but it opened up an arena for discussion that I
[00:07:03.760 --> 00:07:09.600]   think we not me but like the people who were involved weren't yet ready to have. Look at Nixon
[00:07:09.600 --> 00:07:15.040]   didn't like didn't like Daniel Ellsberg because he leaked the Pentagon papers. But without the
[00:07:15.040 --> 00:07:19.760]   Pentagon papers we wouldn't have known the true state of the Vietnam War. That was protected. The
[00:07:19.760 --> 00:07:25.360]   New York Times and the Washington Post published it. I hope you saw the the movie about it. The post.
[00:07:26.320 --> 00:07:32.080]   It was very dramatic. They tried to break into a to Assange Ellsberg psychiatrist's office.
[00:07:32.080 --> 00:07:37.680]   They wanted him, but they never they never got him because I think this is it. This is this.
[00:07:37.680 --> 00:07:42.160]   Well, that's my question. Is this different? He has a good article by James Paul
[00:07:42.160 --> 00:07:46.560]   in the Atlantic. You don't have to like Julian Assange to defend him.
[00:07:46.560 --> 00:07:51.520]   The ask the effort to extradite and prosecute the WikiLeaks fatter threatens the free medium.
[00:07:53.040 --> 00:07:58.160]   Which is in contrast to the motherboard article we showed earlier, which says it's it's not about
[00:07:58.160 --> 00:08:05.360]   free press. It's about hacking. Look, it's trouble troubling. It's problematic. Assange is a
[00:08:05.360 --> 00:08:10.000]   problematic person. But that's the whole point of free speech you have to defend.
[00:08:10.000 --> 00:08:17.040]   But it raises a worse point, which is we don't have there are a lot of fundamental questions that
[00:08:17.040 --> 00:08:22.480]   unfortunately we don't we are not yet discussing in this country because our country in order to
[00:08:22.480 --> 00:08:27.120]   have those conversations requires a lawsuit first, right? To have the conversations where we get to
[00:08:27.120 --> 00:08:32.400]   action. So we have a lot of questions around what is speech what constitutes speech if it's been
[00:08:32.400 --> 00:08:39.920]   automated or are we you know, or the algorithms to blame is the data to blame, you know, from that to
[00:08:39.920 --> 00:08:47.520]   who owns your face, who owns your biometrics. And you think the proper place is for the courts
[00:08:47.520 --> 00:08:52.720]   to decide? No, I don't think so. But that unfortunately is the way that democracy,
[00:08:52.720 --> 00:08:58.160]   you know, that the democracy does in this country. Now in other countries,
[00:08:58.160 --> 00:09:04.240]   they've just jumped ahead to regulation. But this is a this is a problem that's that's not going to go
[00:09:04.240 --> 00:09:09.040]   away. I should point out that the reason Assange was in the Ecuadorian embassy was not because of
[00:09:09.040 --> 00:09:15.200]   US charges, but Swedish charges against him for rape. Those charges were dropped.
[00:09:15.520 --> 00:09:25.840]   But now he's arrested for on basically the London Metropolitan Police arrested him on the
[00:09:25.840 --> 00:09:30.480]   behalf of the United States. They're going to extradite him because he wasn't indicted in the US for
[00:09:30.480 --> 00:09:39.280]   this for this hacking at the at the end of his piece, ball says, Assange might be in a hole,
[00:09:39.280 --> 00:09:43.760]   scratch that Assange is in a hole, but we're going to have to stand up for him anyway. I know I'm
[00:09:43.760 --> 00:09:48.320]   I have very mixed feelings about this. I think he did a lot of good with collateral justice or
[00:09:48.320 --> 00:09:56.480]   collateral murder rather. I think he was probably a Russian cutout in the 2016 election. So there's
[00:09:56.480 --> 00:10:02.960]   that on the other hand, I think there are there are I don't know I know the minute he published
[00:10:02.960 --> 00:10:08.560]   collateral collateral murder, he would be there would be trumped up charges against him of some
[00:10:08.560 --> 00:10:15.040]   kind. I saw. I've had two jobs two careers. I've been a futurist for 15 years and the years before
[00:10:15.040 --> 00:10:19.360]   that I was a foreign correspondent. So I worked at the Wall Street Journal in it Newsweek. And here's
[00:10:19.360 --> 00:10:23.840]   what I would have to say, you know, there are plenty of cases like the Panama Papers where there's been
[00:10:23.840 --> 00:10:31.120]   widespread collaboration and journalists working for the public interests and much like the panic
[00:10:31.120 --> 00:10:38.400]   on papers, releasing information, you know, to shine the light on on corruption. I think we need
[00:10:38.400 --> 00:10:45.360]   some kind of, you know, Assange makes it and WikiLeaks make it sound like this is all open and
[00:10:45.360 --> 00:10:51.440]   through that openness and freedom comes justice, but it's not entirely open. They're still an
[00:10:51.440 --> 00:10:56.160]   arbiter. And you know what I mean? Yeah. It's it's it's and it's complicated. And this is an
[00:10:56.160 --> 00:11:00.880]   ancient arbiter who's who's co-opted by other governments in his own ways.
[00:11:05.440 --> 00:11:13.360]   I mean, Chelsea Manning, her sentence was what did Obama do? He didn't he commuted it, right?
[00:11:13.360 --> 00:11:19.200]   He didn't know he didn't pardon her. He commuted her sentence. Right. He said, you've served enough
[00:11:19.200 --> 00:11:27.760]   time. We're going to let you out. Clearly, people were very angry at WikiLeaks for releasing that.
[00:11:27.760 --> 00:11:34.080]   But I think it I think it's very analogous to the Panama Papers or the Pentagon Papers.
[00:11:34.080 --> 00:11:39.360]   These were government leaks, but they were of information that was in the public interest.
[00:11:39.360 --> 00:11:41.920]   And there's also timing. So while
[00:11:41.920 --> 00:11:50.960]   there were certainly revelations about lives that were put in danger, lives that were lost
[00:11:50.960 --> 00:12:00.720]   once that information was dumped. However, the there were instances of new people whose lives
[00:12:00.720 --> 00:12:06.640]   were put in say, you know, in danger. Again, this is these are the kinds of things that a journalist
[00:12:06.640 --> 00:12:13.680]   or a team of journalists working together would. No, Snowden did it responsibly, right? Snowden
[00:12:13.680 --> 00:12:20.240]   took the material and gave it to a number of reputable publications and said, you need to vet this.
[00:12:20.240 --> 00:12:24.480]   And you need to put it out in an appropriate way, because I'm not capable of doing that.
[00:12:24.480 --> 00:12:28.720]   Assange did the opposite. In fact, I think what you're thinking about that was really
[00:12:28.720 --> 00:12:34.720]   troubling was the American diplomatic cables that were leaked that had the names of American
[00:12:34.720 --> 00:12:40.480]   agents and put them in in grave peril. That's right. And just because the documents are leaked
[00:12:40.480 --> 00:12:44.640]   doesn't mean that they are correct. They just dumped them out. They didn't they didn't
[00:12:44.640 --> 00:12:48.240]   vet them. They didn't give them a journalistic entity. They claimed we're a journalistic entity,
[00:12:48.240 --> 00:12:53.840]   but they just dumped them. Right. Which to me, I guess is why I've always viewed Snowden and
[00:12:53.840 --> 00:12:59.200]   Assange differently. And because of exactly what you said, I've always viewed them in different ways.
[00:12:59.200 --> 00:13:04.080]   And in my thoughts on Chelsea Manning, although I think that the way that she was treated while
[00:13:04.080 --> 00:13:11.840]   she was incarcerated and a lot of those things have been terrible, because she was a soldier when she
[00:13:11.840 --> 00:13:19.440]   leaked that information. There was certain things that for me made that a little bit more,
[00:13:20.080 --> 00:13:24.720]   I guess complicated too. But I think that that gets to the key point in this, right? Is that
[00:13:24.720 --> 00:13:33.280]   what he's now being kind of accused of are things that are, I guess fundamentally comes to the question
[00:13:33.280 --> 00:13:40.720]   of whether or not what he does is journalism. And although I see- >> Is he making better comments?
[00:13:40.720 --> 00:13:44.160]   >> Is he saying I'm a journalist, I'm doing journalism?
[00:13:44.160 --> 00:13:48.640]   >> He has said that. He's not saying that right now. He has said that in the past.
[00:13:48.640 --> 00:13:52.880]   WikiLeaks has absolutely said that in the past. But WikiLeaks has changed so much in the last
[00:13:52.880 --> 00:13:57.360]   decade that it's hard to know where they stand, right? And I think it's interesting that a lot of
[00:13:57.360 --> 00:14:04.560]   the First Amendment advocates and experts have very clearly tried to narrow this and say this is
[00:14:04.560 --> 00:14:11.040]   not going under journalistic things. And again, this is about hacking, which whether you're a
[00:14:11.040 --> 00:14:17.360]   journalist or not is you're not immune to those sorts of things. If you're breaking into other
[00:14:17.360 --> 00:14:26.240]   people's databases and whatnot. But still, this question does come up, right? What was the role
[00:14:26.240 --> 00:14:31.040]   in this and how is this different? I'm with you, Leo. It's complicated.
[00:14:31.040 --> 00:14:32.560]   >> It's troubling to me.
[00:14:32.560 --> 00:14:38.000]   >> It is troubling in a lot of ways. It's interesting to think about what would have happened if he
[00:14:38.000 --> 00:14:40.480]   would have just not been such a jerk. >> Such a dick.
[00:14:40.480 --> 00:14:42.960]   >> As an embassy. >> But that's- >> Okay.
[00:14:44.000 --> 00:14:49.520]   Like honestly, because it got the point that it was Ecuador finally had enough and they're like,
[00:14:49.520 --> 00:14:56.560]   we're not dealing with you anymore. But- >> This is a clear example of
[00:14:56.560 --> 00:15:04.320]   short-term gain without thinking about the downstream implications. And I think that
[00:15:04.320 --> 00:15:13.040]   in all of my, from what I've read, it seems as though some of the people involved
[00:15:13.920 --> 00:15:18.880]   had their hearts in the right places. But again, you've got to think in a very rational,
[00:15:18.880 --> 00:15:23.360]   measured way, what are all of the implications of making this information public at this particular
[00:15:23.360 --> 00:15:28.960]   time in this particular way? >> Well, I think we can agree that it should have been done,
[00:15:28.960 --> 00:15:35.280]   at least should have been done in Snowden fashion. Something that responds away, right?
[00:15:35.280 --> 00:15:39.920]   >> Right. But he's not a US citizen.
[00:15:39.920 --> 00:15:43.600]   >> Right. >> So this is the everything. I know a lot
[00:15:43.600 --> 00:15:50.720]   about the First Amendment because I had to study it. People, a lot of Americans don't quite
[00:15:50.720 --> 00:15:54.640]   understand the First Amendment and people outside the United States aren't bound by it.
[00:15:54.640 --> 00:15:56.880]   >> Right. >> So again, I think this is where-
[00:15:56.880 --> 00:16:00.320]   >> But he's being prosecuted by the US, not by people outside the United States.
[00:16:00.320 --> 00:16:03.440]   >> Yeah, but First Amendment is not a piece of this. This is under a-
[00:16:04.640 --> 00:16:10.800]   just standard cyber crimes law. >> To reiterate your point, this is from the ball article.
[00:16:10.800 --> 00:16:15.520]   The Obama administration had an institutional policy that concluded prosecuting Assange
[00:16:15.520 --> 00:16:20.720]   for the publication of classified documents was too controversial and legally risky
[00:16:20.720 --> 00:16:26.800]   because a successful prosecution could serve as the foundation for future misuse of that
[00:16:26.800 --> 00:16:31.680]   authority to clamp down on more traditional media outlets. The risk to the First Amendment
[00:16:31.680 --> 00:16:38.160]   was significant. And then he points out perhaps the Trump administration has discovered additional
[00:16:38.160 --> 00:16:46.880]   evidence that changes that analysis. And that's what we don't know yet. But the optics of it are,
[00:16:46.880 --> 00:16:51.040]   if you're going to prosecute him for the email that says, "Our team is working on it, we're
[00:16:51.040 --> 00:16:56.080]   going to help you crack this Chelsea." If you're going to prosecute him on that, it's clear that
[00:16:56.080 --> 00:17:01.520]   what you're prosecuting him for the release of the information. You just don't want to call
[00:17:01.520 --> 00:17:07.760]   it that. It's like putting Al Capone in jail for tax evasion. It wasn't the real reason he was
[00:17:07.760 --> 00:17:12.720]   in Alcatraz, but it was a good, it was an effective surrogate. >> Well, and also,
[00:17:12.720 --> 00:17:18.880]   let's not forget that you never want to know lawyers or governments want to bring somebody to
[00:17:18.880 --> 00:17:22.720]   trial or bring charges if they're not very, very certain that they're going to win.
[00:17:26.000 --> 00:17:32.800]   And Assange complicated it by his personality disorder. I guess I could call it a personality.
[00:17:32.800 --> 00:17:38.960]   Something went on that caused him to- >> Well, I think if you're confined for seven years-
[00:17:38.960 --> 00:17:42.400]   >> That could be the reason. >> I'm confined in my house for a minute.
[00:17:42.400 --> 00:17:44.960]   >> One of the reasons the Ecuador and MSC kicked him out is he wasn't cleaning up after his cat.
[00:17:44.960 --> 00:17:50.640]   That's one of the reasons they gave. >> Yeah, well, there's other cleaning issues.
[00:17:50.640 --> 00:17:56.560]   Playing music loud at night, he was stinky and he wasn't cleaning up after his cat.
[00:17:56.560 --> 00:18:03.360]   >> There was a great article a few months ago about somebody who I guess like he was their
[00:18:03.360 --> 00:18:10.560]   house guest and he wouldn't leave who described what it was like. This is years before, he wouldn't
[00:18:10.560 --> 00:18:14.240]   the Ecuadorian embassy. Apparently he's like the house guest from hell, no matter where he is and
[00:18:14.240 --> 00:18:19.440]   no matter what his circumstances. >> But you shouldn't put somebody in jail for that.
[00:18:19.440 --> 00:18:25.440]   >> No, I agree with you. >> But the same sign I'm with you, but I also
[00:18:25.440 --> 00:18:29.520]   sort of sympathize in some ways with the Ecuadorian. >> I don't blame the Ecuadorians. In fact,
[00:18:29.520 --> 00:18:34.480]   I'm impressed. >> We didn't sign up for this. >> For nine years they put up with that, it's pretty good.
[00:18:34.480 --> 00:18:43.040]   >> The sad thing is that this is a good opportunity for us to talk about how to move forward and what
[00:18:43.040 --> 00:18:46.400]   needs to change going forward and how to- and that's not happening.
[00:18:46.400 --> 00:18:54.640]   We're like nobody's taking this as an opportunity to recalibrate how we deal with our laws and
[00:18:54.640 --> 00:18:58.720]   technology and crime and all that stuff. >> Your point being that things have changed
[00:18:58.720 --> 00:19:03.440]   thanks to technology and we need to update our thinking about what that means, what the first
[00:19:03.440 --> 00:19:08.480]   amendment means in that light. >> Yeah, and I feel like this we cycle every couple years.
[00:19:08.480 --> 00:19:12.640]   I'm looking at my bookshelf and there's a wired reporter named Andy Greenberg who wrote this
[00:19:12.640 --> 00:19:18.640]   really great book called "This Machine, Steel Secrets." Like every couple years there's this
[00:19:18.640 --> 00:19:24.320]   conversation that sort of pipes up and we all get fascinated about it and then we forget about it
[00:19:24.320 --> 00:19:28.640]   and we move on and then we're surprised when it all happens again or we're stuck in a similar
[00:19:28.640 --> 00:19:35.040]   situation where we don't know what to do and now's, you know, I just feel like we keep missing
[00:19:35.040 --> 00:19:41.040]   these opportunities to make some changes, you know? >> And let's not forget, speaking of forgetting,
[00:19:41.040 --> 00:19:47.200]   one more person I don't want to forget, who's reality winner who is in jail five years for
[00:19:47.200 --> 00:19:51.200]   leaking the hacking report that told us that Russia was hacking the elections.
[00:19:51.200 --> 00:19:58.640]   >> That one's a stranger case to me. >> Yeah, that one I feel bad for her in some ways.
[00:19:58.640 --> 00:20:05.840]   A, she tried to pass this off the right way. The intercept really messed up there.
[00:20:05.840 --> 00:20:09.200]   You know, like honestly, you look at that and you're like, if you're going to be accepting,
[00:20:09.200 --> 00:20:13.120]   if you're going to be in my opinion, if you're saying, send us secure things and we're going
[00:20:13.120 --> 00:20:18.240]   to protect you, then you need to take that of utmost importance and that they really dropped
[00:20:18.240 --> 00:20:23.760]   the ball there and that one. >> She's under the longest sentence ever imposed in federal court
[00:20:23.760 --> 00:20:29.840]   for an unauthorized release of government information to the media. So, yeah, this is more than a slide.
[00:20:29.840 --> 00:20:35.280]   >> Well, but so this is interesting, right? So right after WikiLeaks, a handful of news organizations
[00:20:35.280 --> 00:20:40.480]   got together, including the Guardian and created a secure leaking area of their website.
[00:20:40.480 --> 00:20:42.400]   >> Right. >> Right.
[00:20:42.400 --> 00:20:48.880]   >> So, yes. >> Right. So, and that had been in use and the standard, I mean, she,
[00:20:48.880 --> 00:20:53.360]   reality winner as far as I understand, followed the appropriate steps to try to
[00:20:53.360 --> 00:20:59.280]   port internally and then leak in a way that she would have been covered under whistleblower, I thought.
[00:21:01.440 --> 00:21:06.640]   This is by the way, why Snowden did what he did. He felt that he would go to jail if he'd
[00:21:06.640 --> 00:21:09.200]   dried that route. >> Yeah.
[00:21:09.200 --> 00:21:14.320]   >> The proof Snowden right in effect. >> Yeah, no, but I mean, and I don't know where the breakdown
[00:21:14.320 --> 00:21:19.200]   was at the intercept with that process. I know that when, because a gizmodo media group had
[00:21:19.200 --> 00:21:23.200]   secure drop and I know that even get access, like I had to do another like an all day training
[00:21:23.200 --> 00:21:28.400]   thing to even be able to use it and there were very specific things and our lawyers were involved
[00:21:28.400 --> 00:21:33.520]   and there was a whole, but there was a whole process. So, I don't know where the breakdown was with
[00:21:33.520 --> 00:21:38.320]   when they published things or what not, like what was visible and what wasn't, but it was
[00:21:38.320 --> 00:21:46.880]   something that clearly was not done correctly. >> I feel like she snuck printouts out and I think
[00:21:46.880 --> 00:21:53.600]   that they published those unchanged and they were able to, because of the secret dots on printouts,
[00:21:53.600 --> 00:21:59.520]   they were able to trace it back to her because they published photos of the printouts rather than
[00:21:59.520 --> 00:22:04.320]   the content itself by itself, something like that. >> So, I'm super curious, the training that you
[00:22:04.320 --> 00:22:12.320]   had to look at content that had been leaked, what was the training? What did you learn?
[00:22:12.320 --> 00:22:17.760]   >> Well, I mean, it was about how to use the system, how to access things. And then there were
[00:22:17.760 --> 00:22:26.320]   things about, yeah, in terms of when viewing data, how to handle it. >> Was there a prescriptive,
[00:22:26.320 --> 00:22:32.480]   like aside from the technical, like how to get the content? Was there a codified in this decision
[00:22:32.480 --> 00:22:37.760]   tree, in this case, tell our legal department, in this case you can report on the story?
[00:22:37.760 --> 00:22:42.880]   >> Yeah, I mean, that was definitely part of it and it wasn't maybe codified in that way,
[00:22:42.880 --> 00:22:47.200]   but it was definitely one of those things. And it was definitely, I mean, for us, it went
[00:22:47.200 --> 00:22:55.280]   in doubt, talks to the lawyers who were proportionately downstairs to get clearance and definitely
[00:22:55.280 --> 00:23:00.640]   talks to the lawyers before uploading any sort of documents you get from secure drop to document
[00:23:00.640 --> 00:23:05.840]   code or whatever. >> Although, it gives photos probably more concerned about Apple than the
[00:23:05.840 --> 00:23:09.520]   US government, right? I mean, it was- >> Not necessarily.
[00:23:09.520 --> 00:23:15.920]   >> Really? Did you expect leaks from the NSA? >> I mean, at this point, some of the best
[00:23:15.920 --> 00:23:19.200]   reporters who you have there are Dell Cameron. >> Right.
[00:23:19.200 --> 00:23:24.720]   >> And so, not necessarily, I mean, you never know when you're going to be getting changed.
[00:23:24.720 --> 00:23:27.200]   >> It's true. >> And it wasn't just Gizmodo,
[00:23:27.200 --> 00:23:32.240]   it was also the investigations team and it was lots of other parts of other sites. But no,
[00:23:32.240 --> 00:23:37.680]   I mean, you would be surprised. >> I'm showing my age and my bias because I still think of
[00:23:37.680 --> 00:23:42.960]   gadget blogs versus real journalism, and that's completely weird. I see a headline in Buzzfeed,
[00:23:42.960 --> 00:23:48.800]   I think, is this six ways to cook a steak or is this going to be an expose?
[00:23:48.800 --> 00:23:52.080]   >> Right. >> And often I'm wrong, so yeah.
[00:23:52.080 --> 00:23:57.760]   >> I mean, and the truth of the matter is, is that because we were a media entity that was known
[00:23:57.760 --> 00:24:05.200]   to publish things, people would leak things regardless. People knew that they could come to us
[00:24:05.200 --> 00:24:09.760]   because that would get published, which is not always the case. So, there were things that we
[00:24:09.760 --> 00:24:15.200]   would get that others wouldn't or that we might be hungry or for because we would publish those
[00:24:15.200 --> 00:24:18.960]   things. And that's still the case with that organization now across all of its different
[00:24:18.960 --> 00:24:23.040]   properties is they'll publish things that not everyone else will. So, you would be surprised
[00:24:23.040 --> 00:24:27.440]   the sort of stuff that would come in. And especially around any of the elections and stuff,
[00:24:27.440 --> 00:24:29.600]   especially about cyber crime stuff. >> Sure. >> Yeah.
[00:24:29.600 --> 00:24:34.160]   >> There's a lot of that stuff. >> Yeah, and there's a big overlap these days
[00:24:34.160 --> 00:24:38.000]   between cyber crime and political crime. So, absolutely.
[00:24:38.000 --> 00:24:42.720]   >> All right. Well, let's move on. Lots more to talk about.
[00:24:42.720 --> 00:24:50.000]   Google's in a little bit of trouble. I'm a little worried about Apple and Amazon's moving to Bellevue.
[00:24:50.000 --> 00:24:53.520]   Are you going to? I don't know if they just built the spheres.
[00:24:53.520 --> 00:24:56.880]   >> I know. They just built them. >> They're gorgeous.
[00:24:56.880 --> 00:25:00.000]   >> I mean, you like the spheres? >> I love the spheres.
[00:25:00.000 --> 00:25:05.360]   >> 73 degrees all year old. >> Yeah. Like Hawaii in Seattle.
[00:25:05.360 --> 00:25:11.440]   It's beautiful. There's like 40,000 species of plant life. It's perfect humidity. It smells good.
[00:25:11.440 --> 00:25:14.320]   It's like, I want to move in there. You could work in there. >> I know.
[00:25:14.320 --> 00:25:22.080]   I live just about a quarter of a mile away from them, which is why I don't mind the moving to Bellevue,
[00:25:22.080 --> 00:25:28.480]   because maybe Seattle West Side home prices will go down maybe. Maybe.
[00:25:28.480 --> 00:25:33.120]   >> I want to write a dystopian science fiction novel in which Seattle has moved out of the spheres,
[00:25:33.120 --> 00:25:39.040]   and they've been taking it over by gangs. I don't know. There's something there.
[00:25:39.040 --> 00:25:43.040]   There's something there, the future. Let's take a break. I showed you today, brought to you by
[00:25:43.040 --> 00:25:48.640]   Zip Recruiter. Hello, Zip Recruiter. Good to see you. We're big fans of Zip Recruiter.
[00:25:48.640 --> 00:25:54.240]   Hiring is such an important part of building a company. What a company. What is a company?
[00:25:54.240 --> 00:25:59.360]   A company is made of people with an aligned goal working together.
[00:25:59.360 --> 00:26:04.800]   Getting the right people can take your company to the moon and start. Getting the wrong people
[00:26:04.800 --> 00:26:10.160]   can take you down. That's why hiring is the most important thing you do in your company.
[00:26:10.160 --> 00:26:14.400]   It's also sometimes the most challenging. What's really hard if you're a small company like ours,
[00:26:14.400 --> 00:26:19.600]   or you just have a few employees, is you're doing all this when you're down a person.
[00:26:19.600 --> 00:26:24.400]   You're already in the penalty box, and now you've got to win the game. That's why you need Zip
[00:26:24.400 --> 00:26:30.720]   Recruiter on your team. Zip Recruiter is the smart way to hire. First of all,
[00:26:30.720 --> 00:26:34.480]   we've already mentioned this. When you go to ziprecruiter.com/twit and post your job,
[00:26:34.480 --> 00:26:40.400]   you're sending it to over 100 of the world's top job boards. You're spreading your net wide.
[00:26:40.400 --> 00:26:44.400]   Your message goes out. You're more likely to reach that perfect person. You just don't know.
[00:26:44.400 --> 00:26:48.000]   You know they're out there. You just don't know where they are. So you're more likely to get to
[00:26:48.000 --> 00:26:53.360]   that person, but there's more than that. This is what's so cool. They don't use the word AI,
[00:26:53.360 --> 00:26:58.640]   but I almost want to use this word AI because they use matching technology. They already have
[00:26:58.640 --> 00:27:03.600]   millions of resumes because lots of people come to Zip Recruiter. It's a job site. So they scan
[00:27:03.600 --> 00:27:08.960]   those resumes to find people with the right experience for your job. Then they say,
[00:27:08.960 --> 00:27:14.720]   "You might want to apply to this job." I've seen this work because when our bookkeeper
[00:27:14.720 --> 00:27:19.120]   a couple of months, six months ago, gave us two weeks notice, Lisa went, "Oh, no. I have to do
[00:27:19.120 --> 00:27:25.760]   the bugs. I don't want to do it. I got a heart." So this is Zip Recruiter. She posted it before
[00:27:25.760 --> 00:27:32.800]   lunch. They came in, like kept coming in, "Oh, this is good. Oh, this person's great." We got
[00:27:32.800 --> 00:27:40.000]   three before lunch, great candidates. That's that artificial intelligence. That's that matching
[00:27:40.000 --> 00:27:45.600]   technology. As the applications come in, Zip Recruiter analyzes each one, spotlights the top
[00:27:45.600 --> 00:27:50.480]   candidates. So you never miss a great match. And by the way, they don't flood your inbox. They don't
[00:27:50.480 --> 00:27:55.200]   call you. That all goes into the Zip Recruiter interface. All the resumes are formatted. So they
[00:27:55.200 --> 00:27:59.040]   look the same as easy. You can even have screening questions, essay questions, true, false, multiple
[00:27:59.040 --> 00:28:04.640]   choice. So you can eliminate people who just don't fit. Zip Recruiter is so effective
[00:28:04.640 --> 00:28:10.960]   that four out of five employers who post on Zip Recruiter get a quality candidate through the site
[00:28:11.520 --> 00:28:16.880]   within the first day. We got one within the first hour. It blew my mind. Right now, this
[00:28:16.880 --> 00:28:23.600]   week in Tech listeners can try Zip Recruiter free at our exclusive web address, ziprecruiter.com/twit.
[00:28:23.600 --> 00:28:34.160]   ziprecruiter.com/twit. Zip Recruiter is the smartest way to hire ziprecruiter.com/twit. We thank
[00:28:34.160 --> 00:28:40.800]   them so much for a service that we have used and loved. So I got a little nervous when I read
[00:28:40.800 --> 00:28:45.120]   this in the Apple developer documentation. Microsoft's moving this way too. I'm actually
[00:28:45.120 --> 00:28:50.080]   curious, Christina, what you think of this. I think every operating system looks at
[00:28:50.080 --> 00:28:57.360]   mobile operating systems, especially iOS, and says, you know, iOS is more secure because Apple
[00:28:57.360 --> 00:29:09.200]   requires developers to have a signature and they vet the apps. Microsoft released Windows S
[00:29:09.200 --> 00:29:16.400]   same idea. You got to get it through the store. I know that Apple has Gatekeeper on Mac OS.
[00:29:16.400 --> 00:29:20.560]   And now the latest thing, which scared some people, and maybe it depends how you read this,
[00:29:20.560 --> 00:29:29.600]   Mike, Apple says, before 10.14.5, the next version of Mac OS,
[00:29:29.600 --> 00:29:38.400]   if you want to distribute software on Mac OS, your software has to be notarized in order to run.
[00:29:39.360 --> 00:29:44.640]   In a future version of Mac OS, notarization will be required by default for all software.
[00:29:44.640 --> 00:29:51.120]   That really confuses me. I heard about this and I read through it because I've got a couple of
[00:29:51.120 --> 00:29:55.520]   scripts that I wrote that are going to be broken. But I don't understand.
[00:29:55.520 --> 00:30:04.320]   So code signing makes sense for security, but right now, if you download an app on Mac OS,
[00:30:04.960 --> 00:30:10.240]   and it's not signed, but you have to like buy a password. It's more like Android, you could
[00:30:10.240 --> 00:30:14.800]   check a box. Yeah, you have to right click and see. I agree to install this anyway. Yes.
[00:30:14.800 --> 00:30:22.400]   Right. So it strikes me that this is a way to lock out anybody who has not gotten the additional
[00:30:22.400 --> 00:30:27.840]   layer of which is not free, by the way, you have to have a developer ID. You have to,
[00:30:27.840 --> 00:30:34.320]   you know, there's a lot of extra like, what does this all I mean, why make the change, I guess,
[00:30:34.320 --> 00:30:37.280]   is the thing that I don't quite understand. Well, there's not really,
[00:30:37.280 --> 00:30:41.920]   there's also issues if you're like, you have your own scripts, if you're doing development at home
[00:30:41.920 --> 00:30:46.240]   and you write a program for yourself, can you run it? So there there've been, I've seen a lot
[00:30:46.240 --> 00:30:50.800]   of commentary on this. And the question is, what does Apple mean? Apple has yet to explain that
[00:30:50.800 --> 00:30:55.440]   when they say, "Notarization will be required by default for all software." Does that mean
[00:30:55.440 --> 00:30:59.760]   that the end you must mean that the end user, that that's the default, but the end user can
[00:30:59.760 --> 00:31:04.240]   override it, must mean, otherwise you couldn't write your own programs.
[00:31:04.240 --> 00:31:09.520]   Yeah, I believe that's what, I mean, that's how I'm reading it, is that it's going to be
[00:31:09.520 --> 00:31:13.840]   by default, the same way that gatekeeper is by default, it's going to say that apps have to be
[00:31:13.840 --> 00:31:17.600]   notarized. And I assume that the reason they're doing this is that what has happened a number of
[00:31:17.600 --> 00:31:24.000]   times is that there will be software that is signed by a developer certificate and then is
[00:31:24.000 --> 00:31:29.440]   something happens to it. It's bypassed in some way. And so they want to have that
[00:31:29.440 --> 00:31:34.480]   one extra level of protection to ensure that that's happening. If you're writing your own
[00:31:34.480 --> 00:31:41.360]   scripts and whatnot, and if you do have a developer ID, there is a way that you can kind of build
[00:31:41.360 --> 00:31:47.120]   this into your workflow and have the notarization process part of it. And I pay for a developer
[00:31:47.120 --> 00:31:52.240]   account, in part, just do stuff like that. And also, because I like to get the early access
[00:31:52.240 --> 00:31:57.760]   builds to things. It's 99 bucks. It's not prohibitively expensive. No, but it's also not one of those
[00:31:57.760 --> 00:32:02.960]   things that every single person who creates a few custom AppleScript things should have to do,
[00:32:02.960 --> 00:32:10.320]   you know, if they don't want to get alerted all the time. There's kind of a ridiculous process
[00:32:10.320 --> 00:32:15.200]   there. It would be nice if they had a way to get notarized without having to be part of the
[00:32:15.200 --> 00:32:19.520]   developer program, you know, like if it was something that would be free that was separate.
[00:32:19.520 --> 00:32:24.720]   It's like, okay, if you want to distribute in an app store, you have to pay. But if you're not
[00:32:24.720 --> 00:32:29.680]   distributing the app store, we'll still give you a developer account. We can notarize your stuff.
[00:32:29.680 --> 00:32:36.400]   - You can self-notarize. But you have to have a developer ID and a certificate.
[00:32:36.400 --> 00:32:40.960]   - I don't think that's just like a name, like a new name for an existing work stream that
[00:32:40.960 --> 00:32:46.720]   I essentially my understanding, the point of this would be that we know who made every app
[00:32:46.720 --> 00:32:50.240]   running on your system. You know who they not currently know that?
[00:32:50.240 --> 00:32:53.360]   - No, of course not. I can give you an app. I can send you an app.
[00:32:53.360 --> 00:32:55.600]   - Oh, I see which means somebody else's.
[00:32:55.600 --> 00:33:01.120]   - Yeah. So now you have to be verified by Apple. This is you. This is your real address.
[00:33:01.120 --> 00:33:05.680]   As you point out, Christina, this has not worked perfectly. There were Turkish certificates that
[00:33:05.680 --> 00:33:08.880]   were used by malware. They were legitimate certificates.
[00:33:08.880 --> 00:33:11.840]   - Yeah. There've been a lot of those. And there's also been the sort of thing that's
[00:33:11.840 --> 00:33:16.080]   happened is that people oftentimes were for pirated stuff. Because we were seeing this on iOS as well,
[00:33:16.080 --> 00:33:21.120]   where people would create, they would use the enterprise certificates and use it as a way to
[00:33:21.120 --> 00:33:22.880]   strip you. - Including Facebook and Google.
[00:33:22.880 --> 00:33:28.960]   - Exactly. And what they were doing was actually something that a lot of companies have used for
[00:33:28.960 --> 00:33:32.960]   years to kind of get away with some of the app store restrictions. Because for the enterprise
[00:33:32.960 --> 00:33:36.560]   things, you can bypass a lot of that stuff. So they're like, "Oh, we want to access pirated
[00:33:36.560 --> 00:33:42.560]   content. We can do that using this app that has been done by a developer certificate." And
[00:33:42.560 --> 00:33:47.760]   I guess because they have so many of those, those aren't vetted in the same way. And like you said,
[00:33:47.760 --> 00:33:52.320]   the Turkish malware instance, that's been the sort of situation. So I think that they're trying to
[00:33:52.320 --> 00:33:58.800]   stop that as a vector of spreading malware and spreading other things. Because people do
[00:33:58.800 --> 00:34:06.880]   use their developers certificates to distribute stuff. And there might not be that same level of
[00:34:06.880 --> 00:34:12.160]   scrutiny that you think that there might be. But you're not wrong in the fact that
[00:34:12.160 --> 00:34:16.320]   it's going to be an edge case, but it's going to be the edge cases where this is one of those
[00:34:16.320 --> 00:34:20.080]   instances, at least in my opinion, where the people who are impacted the most by this are some of
[00:34:20.080 --> 00:34:24.800]   the power users and the people who are really dedicated to being on Mac OS.
[00:34:24.800 --> 00:34:28.880]   - Well, people who use Microsoft's visual studio
[00:34:28.880 --> 00:34:36.240]   on Mac because it says you have to use Xcode 10 or later. The only reason,
[00:34:36.240 --> 00:34:41.040]   any, if Microsoft said this, everybody go, "Yeah, no, that's, you know, there's always going to be a
[00:34:41.040 --> 00:34:47.120]   way." But people, maybe it's just me, but I'm paranoid about Apple. The day they started the App Store
[00:34:47.120 --> 00:34:51.440]   and Gatekeeper, I thought, "It's only a matter of time before they make sure that everything goes
[00:34:51.440 --> 00:34:57.200]   to the App Store. This is their new model. We collect 30% of all revenue for the privilege of
[00:34:57.200 --> 00:35:01.680]   letting you run something on Mac OS. There is an advantage that, you know, a case you can make
[00:35:01.680 --> 00:35:08.320]   to consumers, well, it makes you safer. I guess if I, it would be really problematic to do it on a
[00:35:08.320 --> 00:35:10.320]   desktop operating system now. - Yeah.
[00:35:10.320 --> 00:35:14.480]   - So what happens then if you're... So, like, I have a couple of, like,
[00:35:14.480 --> 00:35:21.120]   there's a couple of different ways of distributing prototype software, and I'm just, now I'm trying
[00:35:21.120 --> 00:35:25.200]   to figure out how does that even... - Well, on iOS, they have that, they have a way of doing it.
[00:35:25.200 --> 00:35:28.720]   They have a test flight application. - Yeah, exactly. There's test flight.
[00:35:28.720 --> 00:35:31.200]   - And that's when you use that enterprise certificate.
[00:35:31.200 --> 00:35:36.400]   - Right. So what person is notarized is the... Do you have to, do you have to, like, go through
[00:35:36.400 --> 00:35:41.120]   this entire process and initial... - Yes. You can't, I can't just send you something to run on iOS.
[00:35:41.120 --> 00:35:45.600]   - Right. But I think that what will happen is that it's to be part of that build workflow. So
[00:35:45.600 --> 00:35:50.000]   once you build it in next code, it's going to be the same way it would be signed. Now, the way that
[00:35:50.000 --> 00:35:53.680]   you would have to still sign some sort of, you know, prototype thing to go through, you know, hockey
[00:35:53.680 --> 00:35:57.760]   or test flight or whatever, the notarization process is going to be part of that workflow.
[00:35:57.760 --> 00:36:01.760]   And I don't know definitively, I'll have to check for a provisional studio for Mac,
[00:36:01.760 --> 00:36:07.280]   but I assume that because there is like a CLI and there are different ways that you can
[00:36:07.280 --> 00:36:14.640]   incorporate Xcode stuff into other IDEs like Visual Studio for Mac, that they will just,
[00:36:14.640 --> 00:36:18.080]   you know, use that sort of thing. - That would make sense, right? You just, you use that part of
[00:36:18.080 --> 00:36:23.760]   Xcode to sign. - Exactly. And so, so I assume it's going to be a pain when you get set up,
[00:36:23.760 --> 00:36:28.640]   but I think that the idea will be once you have it done and once it's in your kind of, you know,
[00:36:28.640 --> 00:36:34.800]   keychain of things that you're doing with your tooling, theoretically, it shouldn't be that...
[00:36:34.800 --> 00:36:36.960]   - I hope so. - ...there shouldn't be any defense.
[00:36:36.960 --> 00:36:40.960]   - I'm sure overstating this. I just, you could see why people went, whoa.
[00:36:40.960 --> 00:36:45.600]   - Well, without a doubt, they haven't done a great job of explaining what this is and it's also,
[00:36:45.600 --> 00:36:50.480]   you know, when they say by default, that implies, again, like with Gatekeeper, that there's still
[00:36:50.480 --> 00:36:54.880]   going to be a way to install other things. And I'm with you, I think that if your main operating
[00:36:54.880 --> 00:37:00.160]   system, like even, you know, S mode is a mode, you know, if you need to install things outside
[00:37:00.160 --> 00:37:06.720]   of the store or things that can't be done from the store, you can turn that off. I think that,
[00:37:06.720 --> 00:37:10.480]   you know, I don't know how many people... I know I wouldn't be willing to accept an operating
[00:37:10.480 --> 00:37:14.960]   system. It's one of the reasons I don't use Chrome OS. It only lets you run certain types
[00:37:14.960 --> 00:37:18.240]   of applications. It's just... - You know where this comes from?
[00:37:18.240 --> 00:37:24.480]   - I won't put up with it on a laptop. - I just get, I'm nervous because I feel like this so-called
[00:37:24.480 --> 00:37:30.480]   post-PC era, we're heading, hurtling down this track, where everything will be like iOS or Chrome
[00:37:30.480 --> 00:37:35.440]   OS. And certainly consumers, in many respects, should be using Chrome OS and iOS. I tell people
[00:37:35.440 --> 00:37:44.160]   all the time, that's a safer alternative if your needs are simple. But as a driver of a truck,
[00:37:44.160 --> 00:37:50.400]   in effect, I want to be able to do my own thing and I want others to be able to. I want open-source
[00:37:50.400 --> 00:37:56.000]   software to exist. There's just a lot of reasons why some operating systems need to be open.
[00:37:56.000 --> 00:38:02.240]   I mean, I guess Linux will always be open, right? - Well, there's also interoperability, right? So,
[00:38:02.240 --> 00:38:07.600]   I mean, I guess we kind of been talking about that, but this creates another fissure and more
[00:38:07.600 --> 00:38:14.080]   organization which is not good because this isn't just about a singular OS, it ties into
[00:38:14.080 --> 00:38:19.440]   cloud services, right? So when cloud services have AI now as a service on the cloud and it's,
[00:38:19.440 --> 00:38:24.960]   you know, there's a longer tail here. - Is it something that we need to do to protect users?
[00:38:24.960 --> 00:38:33.760]   Is it that the security environment has become so hostile? - Yeah. I mean, nobody takes,
[00:38:33.760 --> 00:38:39.520]   nobody, if they weren't forced to, people wouldn't go through on firmware updates. They don't change
[00:38:39.520 --> 00:38:45.440]   their passwords. They're using legacy code. They're downloading what they think are games
[00:38:45.440 --> 00:38:49.360]   to their devices. - So it's your brother-in-law's fault. It's not our fault. It's your brother-in-law
[00:38:49.360 --> 00:38:52.240]   as fault. Tell him to knock it off. - I should do that.
[00:38:52.240 --> 00:38:57.280]   - Yeah. I mean, knock it off. My dad has been the victim of ransomware twice.
[00:38:57.280 --> 00:39:04.480]   - Although, so has Norris Kydrow, the second biggest aluminum producer in the world,
[00:39:04.480 --> 00:39:08.560]   Merck one of the biggest pharmaceutical companies in the world. It's not like it's restricted
[00:39:08.560 --> 00:39:11.440]   to family members. - There's another way to think about this. There's another way to think
[00:39:11.440 --> 00:39:17.440]   about this, which is if it's the case that people are very concerned about data and privacy and
[00:39:17.440 --> 00:39:22.400]   it's an election year, and Apple's already fairly far ahead with differential privacy and other
[00:39:22.400 --> 00:39:32.000]   kinds of measures, right? Then it kind of, to me, looks like a way to generate strategic advantage
[00:39:32.000 --> 00:39:38.080]   and leverage over everybody else. If their products are beloved and they've locked them down such that
[00:39:38.080 --> 00:39:41.920]   like nobody else can get inside and they're the literally gatekeepers of all that data.
[00:39:41.920 --> 00:39:44.640]   - Yeah. - Under the size of privacy.
[00:39:44.640 --> 00:39:46.720]   - And that's the funny thing. Yeah. - Yeah.
[00:39:46.720 --> 00:39:48.640]   - Regulations that are being- - You're protecting.
[00:39:48.640 --> 00:39:52.560]   You're safe in our walled garden. - That's right. That's exactly right.
[00:39:52.560 --> 00:39:57.920]   That would be probably music to the ears of a lot of legislators who are proposing all kinds of
[00:39:57.920 --> 00:40:02.880]   crazy privacy measures right now. - Well, speaking of privacy New York Times with not one but two
[00:40:02.880 --> 00:40:08.720]   articles about a Google technology that's being used by law enforcement. See, Google knows where
[00:40:08.720 --> 00:40:17.120]   you are at all times. Apparently, Google is happy to share that information with law enforcement.
[00:40:17.120 --> 00:40:27.440]   The thing that we- I'd heard about this being proposed in the past that I think was an upper
[00:40:27.440 --> 00:40:33.360]   New York state, a police department asked Google, "Well, we'd like to know everybody who is in the
[00:40:33.360 --> 00:40:40.080]   vicinity of these convenience store robberies at these times." And Google fought it. And I thought
[00:40:40.080 --> 00:40:45.920]   at the time that was kind of refused by the courts as a phishing expedition except that now,
[00:40:45.920 --> 00:40:51.520]   according to the New York Times, this happens all the time where Google will be asked to provide
[00:40:51.520 --> 00:40:58.880]   information. In this case, they started with a murder in a phoenix suburb. A search warrant
[00:40:58.880 --> 00:41:07.120]   required Google to provide information on all the devices near the killing, potentially capturing
[00:41:07.120 --> 00:41:13.440]   the whereabouts of anyone in the area. - This isn't just Google. Amazon has also
[00:41:13.440 --> 00:41:18.720]   the external cameras and Google's external house cameras.
[00:41:18.720 --> 00:41:24.960]   - Yeah, the noise. - But there's a lot of communities where police are asking the neighbors to let
[00:41:24.960 --> 00:41:29.760]   them connect. There's a community in Dallas where the, I'm sorry, Houston, where the police have asked
[00:41:29.760 --> 00:41:34.560]   neighbors to let them take over their cameras and look at footage whenever.
[00:41:34.560 --> 00:41:40.720]   - Well, but I mean, I've seen enough police procedurals on TV to know that the first thing you do is
[00:41:40.720 --> 00:41:45.360]   go to the convenience store across the street and you ask to see the videos because they have a
[00:41:45.360 --> 00:41:50.640]   camera pointing at the crime scene. That doesn't bother me. - Why does this bother you?
[00:41:52.000 --> 00:41:58.400]   Because everybody who was in that vicinity is drawn up in this law enforcement net. And in fact,
[00:41:58.400 --> 00:42:05.760]   the case that the New York Times talks about, a suspect was arrested because due to this data,
[00:42:05.760 --> 00:42:12.160]   because his phone said he was on the scene and it turned out after he spent a week in jail,
[00:42:12.160 --> 00:42:17.840]   it wasn't him. It was his mother's ex-boyfriend who borrowed his car.
[00:42:19.120 --> 00:42:25.920]   And so this guy because of this, and see, this is to me the classic fishing expedition,
[00:42:25.920 --> 00:42:32.160]   where police, I know police would love to say, well, we'd like to know everybody who was in the area.
[00:42:32.160 --> 00:42:39.440]   I thought that was illegal, but maybe I'm not a constitutional. - Yeah, I'm with you. I mean,
[00:42:39.440 --> 00:42:43.120]   I don't know. I'm not a fan of this either. The first thing I thought about when I saw this,
[00:42:43.120 --> 00:42:49.520]   I was like, okay, well, now I want to purge all of my Google history, you know, on my phone and what
[00:42:49.520 --> 00:42:56.480]   not. - By the way, on Hacker News, that was the immediate response of everybody on Hacker News was,
[00:42:56.480 --> 00:43:00.320]   well, here's how you stop that by turning off all your Google location information.
[00:43:00.320 --> 00:43:04.240]   - But of course, what's frustrating about that is that it's helpful to have--
[00:43:04.240 --> 00:43:08.240]   - And then your maps don't work. - That's right. Exactly. That's what I'm saying. And it's not like
[00:43:08.240 --> 00:43:13.920]   I'm not anticipating being in a situation where this will matter or well, I will be implicated
[00:43:13.920 --> 00:43:18.640]   in something, but it's not about that. I don't like the idea, as you say, of it's one thing, if the
[00:43:18.640 --> 00:43:24.720]   camera is captured from the convenience store, whatever, I know I'm in a public place, I know my
[00:43:24.720 --> 00:43:29.120]   image might be there. And if they can identify me and ask me questions, that's fine. - Maybe that's
[00:43:29.120 --> 00:43:35.040]   what's different, because you're in a public area, but that GPS information, and I know some courts
[00:43:35.040 --> 00:43:40.320]   have ruled this, is a more personal thing. - Right. And also, we've seen before where,
[00:43:40.320 --> 00:43:44.240]   you know, it can be far beyond just that vicinity. You know, they might be asking,
[00:43:44.240 --> 00:43:48.320]   oh, we just want this radius, but if they go a little bit further, you know, I mean, it just,
[00:43:48.320 --> 00:43:53.680]   it makes me uneasy. I'm not a fan. And I don't like that it seems like this has become commonplace
[00:43:53.680 --> 00:43:57.840]   into your point that Google is, because I remember the first time that this came up, they're like,
[00:43:57.840 --> 00:44:01.760]   oh, no, we're going to fight this. And I, as I naively assumed, oh, well, they're going to continue
[00:44:01.760 --> 00:44:06.000]   to fight this. And now it's like, eh, now we'll just hand it over. It's like, really, really,
[00:44:06.000 --> 00:44:10.320]   keep it just not being there. - So again, they're like, there are fixes here, so that the,
[00:44:10.320 --> 00:44:15.200]   the easy OS fix is just like, build them into the OS on your devices and put an off, like,
[00:44:15.200 --> 00:44:20.080]   toggle on, off button in a visible, easy to understand. - Yeah, but remember, Google getting
[00:44:20.080 --> 00:44:24.640]   in trouble, because it turns out when you did turn it off, it didn't actually turn it off.
[00:44:24.640 --> 00:44:28.080]   - Well, some of the other apps wouldn't work. I don't know. Again, like,
[00:44:28.800 --> 00:44:32.480]   we, but this is not the first time we've had this conversation. We had this conversation
[00:44:32.480 --> 00:44:36.000]   after the San Bernardino shootings. What was it now? Three years ago?
[00:44:36.000 --> 00:44:43.360]   - Yeah. - You know, I, so we're, we're going to keep cycling back to this. And what I would say is,
[00:44:43.360 --> 00:44:47.520]   you know, the New York Times is a very large publication, but most of America doesn't read it.
[00:44:47.520 --> 00:44:53.280]   So like, we're having this conversation. You know, most people aren't reading that story and have
[00:44:53.280 --> 00:45:00.000]   absolutely no idea how their data are being collected. And there's actually, has a brand name for it.
[00:45:00.000 --> 00:45:06.560]   They call it Sensor Vault. And according to the New York Times, Google's sensor vault is a boon
[00:45:06.560 --> 00:45:14.320]   for law enforcement. This is how it works. There's a database. It's connected to a Google service
[00:45:14.320 --> 00:45:20.320]   called Location History. Everybody should go to google.com/dashboard and look at your location
[00:45:20.320 --> 00:45:26.560]   used history. You'll see how granular it is. By the way, according to the Times, location history is
[00:45:26.560 --> 00:45:31.680]   not on by default. But as soon as you turn on your phone and you start to use a Google service
[00:45:31.680 --> 00:45:36.080]   like maps, it'll say, "Can I turn on location history or photos? Can I turn on location?"
[00:45:36.080 --> 00:45:41.360]   You know, and most people say, "Yes, Google says we use this to target ads and to measure how
[00:45:41.360 --> 00:45:46.240]   effective they are." So by the way, that means they're watching you go into stores.
[00:45:47.600 --> 00:45:51.760]   And then they watch the credit card transaction and they know the credit card was yours and they
[00:45:51.760 --> 00:45:57.280]   even offer this information to people who buy Google ads. It's not just a click. We can also tell
[00:45:57.280 --> 00:46:00.960]   you if the person ended up buying your product using their credit card because we're watching
[00:46:00.960 --> 00:46:08.320]   where they are. And that's actually old technology that launched a couple of years ago.
[00:46:08.320 --> 00:46:13.440]   Yeah. But the thing that's to me a little scary is, and I don't blame law enforcement,
[00:46:13.440 --> 00:46:16.640]   it's a tool. And as long as Google's willing to give this to you, why not?
[00:46:16.640 --> 00:46:22.800]   Sure. These are called so-called geo-fence requests. So they say, instead of, and see,
[00:46:22.800 --> 00:46:26.880]   this is what I thought was unconstitutional. Instead of saying, "Where was Leo on the night of
[00:46:26.880 --> 00:46:34.000]   Friday, July 17th?" They say, "Well, who was in that area on the night of July 17th? Let's see
[00:46:34.000 --> 00:46:39.680]   them all and we'll talk to each one of them or we'll investigate some other way each one of them.
[00:46:39.680 --> 00:46:44.560]   Google labels the devices with anonymous ID numbers, detectives look at locations and
[00:46:44.560 --> 00:46:48.800]   movement patterns, this is according to the New York Times, to see if any appear relevant to the
[00:46:48.800 --> 00:46:55.840]   crime. Once they narrow the field to a few devices, Google then Presto pulls off the
[00:46:55.840 --> 00:47:04.080]   the hider and reveals the information like names and email addresses. So, anyways,
[00:47:04.080 --> 00:47:09.040]   something people should be aware of, I guess. You're right. Very few people will be,
[00:47:09.040 --> 00:47:13.920]   except people who read the New York Times are listening to this show. But I'm surprised this is.
[00:47:13.920 --> 00:47:23.200]   So that, again, that begs the question, this is where we get stuck. We buy the technology and so
[00:47:23.200 --> 00:47:30.400]   we feel like we own it. And feeling like we own it, I think, makes people believe that they have
[00:47:30.400 --> 00:47:35.680]   some agency in what's being done. And in fact, we don't own any of the technology. We own the
[00:47:35.680 --> 00:47:40.960]   ability to use it for some amount of time. But we don't own it outright. Like when you buy a
[00:47:40.960 --> 00:47:46.960]   water bottle, you get to determine all the things that happen with that water bottle on your own.
[00:47:46.960 --> 00:47:52.640]   I think we've done a piss poor job of making sure that everyday people understand this.
[00:47:52.640 --> 00:47:55.920]   And to your point that this is something that probably
[00:47:55.920 --> 00:48:03.440]   governments should look into and regulations about it, Illinois attempted this. April 10th,
[00:48:03.440 --> 00:48:08.080]   the Illinois State Senate passed something called the Keep Internet Devices Safe Act.
[00:48:08.080 --> 00:48:17.840]   And in fact, it did pass, but after fierce lobbying from an industry association back by Amazon and
[00:48:17.840 --> 00:48:28.160]   Google, they defanged it a little bit by taking away any punishment. But the idea was that no private
[00:48:28.160 --> 00:48:36.960]   entity could turn on or enable a device's microphone unless you agreed. And the bill requires any
[00:48:36.960 --> 00:48:42.320]   recordings or other personal information captured by devices, protect against quote unauthorized
[00:48:42.320 --> 00:48:49.680]   access, acquisition, destruction, use, modification and disclosure of the data. Initially, the bill
[00:48:49.680 --> 00:48:55.120]   would have made this an unlawful practice under the Consumer Fraud and Deceptive Business Practices
[00:48:55.120 --> 00:49:02.560]   Act, which could result in fines about $50,000 per case. But lobbying by the Internet Association
[00:49:02.560 --> 00:49:06.400]   defanged it by saying, "We don't have to have a punishment."
[00:49:06.400 --> 00:49:12.400]   Ultimately, Illinois has actually been pretty far ahead in fighting big tech.
[00:49:12.400 --> 00:49:14.240]   Really, this is interesting.
[00:49:14.240 --> 00:49:23.520]   But here's the thing. The laws and the regulation only matters if they are enforced. And also,
[00:49:23.520 --> 00:49:29.600]   if they deter people from using things that the regulators think are bad for them.
[00:49:29.600 --> 00:49:35.600]   And the problem is that people keep buying and using the stuff, knowing that there may be
[00:49:35.600 --> 00:49:36.480]   negative outcomes.
[00:49:36.480 --> 00:49:44.400]   But I'm going to give you my reaction to this, which is kind of the opposite, which is, "Oh,
[00:49:44.400 --> 00:49:51.760]   great. Now all these privacy, nervous Nellies are going to make it illegal for my Amazon Echo to work."
[00:49:52.240 --> 00:49:55.040]   All right. I want my Echo.
[00:49:55.040 --> 00:49:58.080]   I can't think, right? That's always the story.
[00:49:58.080 --> 00:49:59.280]   I wanted to listen to me.
[00:49:59.280 --> 00:50:05.920]   The convenience store. So if somebody was robbed in a convenience store in the 80s,
[00:50:05.920 --> 00:50:09.920]   the police are still going to show up and they're going to ask a bunch of people who were there,
[00:50:09.920 --> 00:50:13.840]   and then they're going to round up the usual suspects in probably a very, very racist way.
[00:50:15.120 --> 00:50:22.720]   I would argue that the only difference between the 1980s and the late 2010s is that
[00:50:22.720 --> 00:50:27.920]   it requires less humans and more of that process is automated. But it strikes me that it's sort of
[00:50:27.920 --> 00:50:31.040]   like the same process still. No?
[00:50:31.040 --> 00:50:32.320]   I don't know.
[00:50:32.320 --> 00:50:35.520]   You're right. I mean, now we have racist face recognition.
[00:50:35.520 --> 00:50:36.720]   So that's simple.
[00:50:36.720 --> 00:50:37.520]   But it's like the same.
[00:50:37.520 --> 00:50:39.280]   That speeds up the process.
[00:50:39.280 --> 00:50:40.160]   It speeds up.
[00:50:40.160 --> 00:50:44.640]   Right. So it speeds up what we were kind of already doing. I'm not saying that's good or bad.
[00:50:44.640 --> 00:50:47.440]   It's just another tool for law enforcement.
[00:50:47.440 --> 00:50:53.280]   You said there's got to be checks and balances. Law enforcement, I understand.
[00:50:53.280 --> 00:50:58.640]   And we want them to fight crime and arrest bad guys. And we want them to do it in a
[00:50:58.640 --> 00:51:06.160]   constitutionally manner that protects us against overreaching state authorities.
[00:51:06.160 --> 00:51:12.160]   So we want both. And we need both. I'm not against law enforcement using the technology.
[00:51:12.800 --> 00:51:17.920]   I just think it needs to be done. Conforming with the Constitution,
[00:51:17.920 --> 00:51:22.480]   particularly the rules against unlawful search and seizure and
[00:51:22.480 --> 00:51:26.800]   testifying against yourself, the Fourth and Fifth Amendments.
[00:51:26.800 --> 00:51:35.360]   Who had that story recently about the people who do QA on, I think it was Amazon devices and
[00:51:35.360 --> 00:51:39.840]   what they've been listening to as part of that QA process.
[00:51:39.840 --> 00:51:40.640]   Oh, yeah.
[00:51:40.640 --> 00:51:42.240]   That just came out this week too.
[00:51:42.240 --> 00:51:46.720]   Yeah. Yeah. Yeah. Amazon said, well, but you know what? This is not a surprise to anybody who
[00:51:46.720 --> 00:51:54.080]   understands how AI works that this is called training. Yeah. So Amazon says humans
[00:51:54.080 --> 00:52:02.480]   are going to transcribe to improve the customer experience. Some of your recordings, this comes
[00:52:02.480 --> 00:52:08.080]   from a Bloomberg story. So we'll get Bloomberg the credit. Amazon workers are listening to what you
[00:52:08.080 --> 00:52:14.480]   tell. Echo. Global team. Not everybody. No, no, everybody. They're all sampling. Yeah,
[00:52:14.480 --> 00:52:19.840]   they can't. Right. They can't. And I think some of them were in Romania and some of them, but
[00:52:19.840 --> 00:52:27.360]   you know, sounded like the they question like some people thought they heard over her domestic
[00:52:27.360 --> 00:52:32.400]   violence. You know, other ones heard really. So they were PTS, they had PTS.
[00:52:32.400 --> 00:52:35.760]   Amazon employee, according to Bloomberg employees, thousands of people around the world that help
[00:52:35.760 --> 00:52:41.520]   improve Amazon's digital assistant. The team listens to voice recordings captured in Echo
[00:52:41.520 --> 00:52:46.240]   owners, homes and offices. The recordings are transcribed, annotated, and fed back into the software
[00:52:46.240 --> 00:52:50.880]   to this is to make it better, right? To eliminate gaps in echoes understanding if human speech.
[00:52:50.880 --> 00:52:54.800]   And actually, Amazon's asked us to do that too, right? If you can actually go back and look at
[00:52:54.800 --> 00:52:59.120]   all the things you said, you can hear what you said, and you could tell Echo if you got it right.
[00:53:00.480 --> 00:53:05.360]   The voice review process and Bloomberg had seven people, anonymous sources.
[00:53:05.360 --> 00:53:14.720]   It's not a surprise. Mix of contractors, full-time Amazon employees work from Boston to Costa Rica
[00:53:14.720 --> 00:53:20.240]   to India to Romania. They can't speak publicly about the program because they signed on disclosure
[00:53:20.240 --> 00:53:25.360]   agreements. They worked nine hours a day. What a nightmare job. Parsing as many as a thousand
[00:53:25.360 --> 00:53:33.920]   audio clips per shift. According to two workers, it sounds about as bad as working as a
[00:53:33.920 --> 00:53:40.080]   Facebook stacker in the Amazon house. Look, it's cleared that technology has created a lot of
[00:53:40.080 --> 00:53:47.840]   inhuman jobs, but at least there are jobs that humans can do. That's one of the saddest things
[00:53:47.840 --> 00:53:53.200]   I've ever heard. That really got me. At least we got a job, man. It's so depressing, but you're
[00:53:53.200 --> 00:53:59.280]   right. It's true until we train them well enough and then we'll be out of work. It's like Uber
[00:53:59.280 --> 00:54:04.240]   drivers, basically. You're just a stand-in for the artificial intelligence until we get that good
[00:54:04.240 --> 00:54:10.320]   enough. The worker, the worker's mostly mundane. One worker in Boston said he mined accumulated
[00:54:10.320 --> 00:54:17.840]   voice data. You like this film girl for specific utterances such as Taylor Swift. They were listening
[00:54:17.840 --> 00:54:25.440]   to your echo and annotated them to indicate the searcher meant the musical artist as opposed to,
[00:54:25.440 --> 00:54:29.600]   I don't know, Taylor Swift who wrote Gulliver's Travels. What I don't know. What other
[00:54:29.600 --> 00:54:36.320]   occasionally now on the other hand, well, I was occasionally the listeners picked up things
[00:54:36.320 --> 00:54:41.520]   echo owners likely would rather stay private. A woman singing badly off key in the shower. Well,
[00:54:41.520 --> 00:54:47.840]   who doesn't do that? Or a child screaming for help? Ooh, that would be hard. The teams use
[00:54:47.840 --> 00:54:51.840]   internal chat rooms to share files when they need help parsing a muddled word or
[00:54:51.840 --> 00:54:55.280]   come across an amusing recording.
[00:54:55.280 --> 00:55:04.720]   So Amazon's response, we take the security and privacy of our customers' personal information.
[00:55:04.720 --> 00:55:09.280]   Seriously, we only annotated an extremely small sample of echo voice recordings.
[00:55:09.840 --> 00:55:13.280]   I'm sorry, I said the A word in order to improve the customer experience.
[00:55:13.280 --> 00:55:17.840]   For example, this information helps us train our speech recognition and natural language
[00:55:17.840 --> 00:55:23.120]   understanding systems. This was exactly the same thing that we were learning about.
[00:55:23.120 --> 00:55:31.440]   Who was that that was doing that nest? I can't remember.
[00:55:31.440 --> 00:55:34.160]   With the microphone. Yeah.
[00:55:35.360 --> 00:55:41.600]   This is just what this is. I think people, technology is always assumed to be
[00:55:41.600 --> 00:55:46.000]   first and apologize later. Well, but also it's also assumed there's no human intervention.
[00:55:46.000 --> 00:55:50.480]   It's in the signy box. But somebody's got to train this stuff.
[00:55:50.480 --> 00:55:56.320]   We use multi-factor authentication to restrict access, service encryption, and
[00:55:56.320 --> 00:56:01.760]   audits of our control environment to protect it. And by the way, this article does not imply
[00:56:01.760 --> 00:56:06.640]   that there were leaks of any of this information. It's just a revelation that it happens.
[00:56:06.640 --> 00:56:10.640]   But if you'd asked me before this, I would have, and I thought about it, I was like,
[00:56:10.640 --> 00:56:12.320]   well, yeah, I'm sure they're doing that.
[00:56:12.320 --> 00:56:18.720]   And so what happens if somebody decides to do an audio file dump on the WikiLeaks using all of this?
[00:56:18.720 --> 00:56:22.080]   Or whatever? It can have me singing off, Keith.
[00:56:22.080 --> 00:56:28.960]   No, but like, and supposedly it's anonymized. But you can still associate the account number
[00:56:28.960 --> 00:56:32.000]   according to the story. Well, this account number with the...
[00:56:32.000 --> 00:56:36.880]   This validates all the people say, I will never have an Amazon Echo or Google Home or any of these
[00:56:36.880 --> 00:56:44.800]   things in my house. Yes, they will. Yeah, I was going to say, I mean, there are some people who
[00:56:44.800 --> 00:56:51.360]   won't. But once you've had the ability to say, hey, keyword, play, whatever, or read me the weather,
[00:56:51.360 --> 00:56:58.080]   or what's on my calendar, it's kind of awesome. I mean, as creepy as all of this stuff has the
[00:56:58.080 --> 00:57:03.600]   potential to be, there's this other side of it that is really compelling. But it's also, yeah,
[00:57:03.600 --> 00:57:08.160]   you know? Yeah, but it's also really hard to buy a device that doesn't have a camera or a speaker
[00:57:08.160 --> 00:57:14.880]   in it. Very true. The remote controls you get from your cable services have a microphone built
[00:57:14.880 --> 00:57:17.600]   in. And some of them you just talk to and talk to them, you have to talk to them. Why do you
[00:57:17.600 --> 00:57:22.240]   think that is? Is that because everybody's demanding the ability to ask to watch Game of Thrones?
[00:57:22.240 --> 00:57:26.960]   No, because they want the information to sell to other people. The companies want to be able to
[00:57:26.960 --> 00:57:30.640]   do that. Well, I mean, the thing is, is that with the TV capture stuff, they don't even need to have
[00:57:30.640 --> 00:57:35.280]   the camera and the TV thing. I get to literally capture what pixels are on the screen and then
[00:57:35.280 --> 00:57:40.960]   send them out to say, oh, this is what you were watching. I mean, that was what happened with the
[00:57:40.960 --> 00:57:46.160]   Visio and the Samsung cases. It's not even about a camera. It's literally taking a sampling of
[00:57:46.160 --> 00:57:50.240]   the pixels that are on the screen. And if you I've tried to do this, I tried to do this recently
[00:57:50.240 --> 00:57:55.760]   to try to find like a non smart television set, because I was just looking for like a dumb TV thing.
[00:57:55.760 --> 00:57:59.600]   I want to monitor. I'm with you. I just want to monitor. I'll add the intelligence.
[00:57:59.600 --> 00:58:04.160]   You can't do it. Nope. Can't do it. Like it's not possible unless you're getting something
[00:58:04.160 --> 00:58:08.240]   that's years old out of warranty and, you know, all kinds of other things. Like they literally
[00:58:08.240 --> 00:58:12.640]   don't exist, even at the very high end where you used to be able to go. And if you spent like
[00:58:12.640 --> 00:58:17.840]   X thousands of dollars, you know, it wouldn't have it, even those, it's like, no, it's going to have
[00:58:17.840 --> 00:58:22.640]   the stuff in it. You know, my, how my, my rule of thumb for knowing if this is problematic
[00:58:23.440 --> 00:58:29.920]   is if my reaction is, well, I have nothing to hide. I'm not doing anything wrong. So if Google knows
[00:58:29.920 --> 00:58:34.320]   where I am, big deal, I'm not saying anything bad. So if Amazon's listening to me, big deal.
[00:58:34.320 --> 00:58:40.080]   And I know as soon as I say that to myself, which I have, we got a problem. There's a big problem.
[00:58:40.080 --> 00:58:43.840]   Nobody should ever have to say, well, I've got nothing to hide. No, never.
[00:58:43.840 --> 00:58:50.400]   Did I ever tell you about the origin of the word privacy in Japanese? It sounds like a non-sequit
[00:58:50.400 --> 00:58:58.000]   or a bit, it's sequenced. So I lived in Japan the first time in the, in the 90s.
[00:58:58.000 --> 00:59:07.440]   And this was right on the cusp of the internet and being able to share internet data and buy
[00:59:07.440 --> 00:59:15.760]   stuff. And I remember in Japan that I couldn't just order stuff online yet because it was a very
[00:59:15.760 --> 00:59:20.000]   private country and people thought it was crazy that you would just type in a credit card number.
[00:59:20.000 --> 00:59:27.920]   Wow. That was insane. This bastion of all this super high tech was a place where internet
[00:59:27.920 --> 00:59:35.200]   e-commerce hadn't quite landed yet. And when it finally did, they had to invent a word to talk
[00:59:35.200 --> 00:59:40.480]   about privacy because privacy was eroding. And they had never had to discuss it before because it was
[00:59:40.480 --> 00:59:49.520]   just assumed that you had it. And the Japanese word for privacy is "pudayibashi." They borrowed it.
[00:59:49.520 --> 00:59:55.520]   Wow. Isn't that funny? That it's not because they didn't know about privacy because it was just
[00:59:55.520 --> 01:00:00.720]   you didn't need a word for it. It just is. Right. So to what you were just saying, which is,
[01:00:00.720 --> 01:00:05.120]   I don't have anything to hide. I don't like the fact that I have to have that conversation. We
[01:00:05.120 --> 01:00:11.120]   didn't used to have to have that conversation. And now here we are where either we just assume that
[01:00:11.120 --> 01:00:16.320]   we hope that we don't have anything to hide and yet we have no control over it anyways. So this
[01:00:16.320 --> 01:00:22.640]   is just the way we're appeasing ourselves, I guess. Or we try to rail against it or we just close
[01:00:22.640 --> 01:00:29.440]   our eyes and forget that everybody else is looking in. I don't know. This is a good panel for
[01:00:29.440 --> 01:00:38.480]   this topic because I think I'll leave myself out. Both of you embrace the future. I will include
[01:00:38.480 --> 01:00:43.680]   myself on that. We're technologists. We love what technology has done and can do for the future.
[01:00:44.880 --> 01:00:47.760]   But at the same time, especially you, Amy, as a futurist,
[01:00:47.760 --> 01:00:53.680]   we have to grapple with some very difficult challenges that technology brings us.
[01:00:53.680 --> 01:01:02.400]   Yeah. I think you're both very well aware of both sides of that double edged sword.
[01:01:02.400 --> 01:01:08.880]   So, Christina, you have devices to listen in and your place of residence, right?
[01:01:08.880 --> 01:01:14.560]   Yeah, I do. And I have conflicted things about it. We have
[01:01:14.560 --> 01:01:19.920]   unplugged in. But then, yeah, but I think about that. I'm like, is this good enough? Is this
[01:01:19.920 --> 01:01:23.600]   something that I really want? And then you're more likely because you've lived in public for a
[01:01:23.600 --> 01:01:30.560]   long time. This is true. This is very true. If you ever feel like you need to have a conversation,
[01:01:30.560 --> 01:01:36.080]   do you ever unplugged your device to have a private conversation? I haven't.
[01:01:36.080 --> 01:01:39.680]   Lisa won't let me have a device with a camera in the bedroom.
[01:01:39.680 --> 01:01:43.360]   Yeah, I don't do that. I don't have it in the bedroom.
[01:01:43.360 --> 01:01:46.160]   We have so many listening devices in the bedroom that if I say
[01:01:46.160 --> 01:01:50.080]   the A word, several of them will.
[01:01:50.080 --> 01:01:59.280]   By the way, it's a problem in France because the words for with her are a VEXA.
[01:01:59.280 --> 01:02:04.880]   Apparently that's a problem. And in Mexico, it's a problem because the word for make,
[01:02:04.880 --> 01:02:12.880]   itcho is also apparently that's Amazon's reason for having this facility in Romania.
[01:02:13.360 --> 01:02:20.560]   Is the darn French and Mexicans keep saying the word by accident? I know mine wakes up all the
[01:02:20.560 --> 01:02:26.400]   time for no apparent reason. As soon as my Google, yeah, yeah, for no, Cortana never wakes up though,
[01:02:26.400 --> 01:02:32.080]   oddly. What do you have Cortana on? They don't have a speaker out. Harmon Carden makes a Cortana
[01:02:32.080 --> 01:02:38.880]   speaker and I have it. Yeah, I don't even I don't even have that. I don't even have a stack in my
[01:02:38.880 --> 01:02:43.920]   office. I have a Google home max, which is the biggest on top of that, the echo show, and then
[01:02:43.920 --> 01:02:47.920]   on top of that, it's not even a rack. They're just standing on top of each other.
[01:02:47.920 --> 01:02:51.520]   But some kind of like crazy boombox I want to see glue it all together.
[01:02:51.520 --> 01:02:55.840]   If I had a social network, I would post a picture of it, but I don't.
[01:02:55.840 --> 01:03:03.360]   Yeah, no, I have a I have I have a few echo devices. I have a bunch of Sonos things that now have the
[01:03:03.360 --> 01:03:08.560]   echo things built in. I do have a home pod, but I mean, who cares about home pod?
[01:03:09.280 --> 01:03:13.680]   But Siri's listening because she does wake up sometimes, right? Oh, I know she does wake up
[01:03:13.680 --> 01:03:18.080]   sometimes. Sometimes I'll hear her babbling. She's in our kitchen and I'll just go in the kitchen.
[01:03:18.080 --> 01:03:23.200]   She's just babbling. Like she's lost her mind. Like she's talking about something. I will say,
[01:03:23.200 --> 01:03:27.280]   I will say I do love Siri and my Apple TV, but that's about the only Siri that I like.
[01:03:27.280 --> 01:03:33.120]   And then I got a Google home mini from Spotify because they had like some, oh, if you're in a
[01:03:33.120 --> 01:03:37.840]   family account, we'll send you a free one. But I'll be honest, I haven't hooked that one up.
[01:03:38.640 --> 01:03:44.880]   I don't know. I should. I should put it in. I should put it in the in one of the places. I don't
[01:03:44.880 --> 01:03:49.040]   know. I feel like I'm podcasting all the time. No matter what, somebody's listening. But it is
[01:03:49.040 --> 01:03:54.000]   your point. But I think to your point, the whole having to have sort of lived in public thing,
[01:03:54.000 --> 01:03:58.640]   that might be part of the difference. There's still lines that I will draw. Like you said,
[01:03:58.640 --> 01:04:04.320]   I don't have cameras, you know, in my bedroom or any of those places, but well, that was my
[01:04:04.320 --> 01:04:08.800]   wife. I would have done it. I don't care. I do have, I do have an echo spot in my closet.
[01:04:08.800 --> 01:04:14.560]   And what about smart mirrors? Do you do either of you have any of those? There's a whole,
[01:04:14.560 --> 01:04:19.920]   I don't. So I'm kind of into it. Home automation is one of the big trends that we're tracking
[01:04:19.920 --> 01:04:27.120]   this year. And so there's a suite of Peloton like home smart home. Oh, yeah. Yeah. I know the
[01:04:27.120 --> 01:04:33.040]   guy I know Ryan Vance, who was a longtime tech TV guy and did the Tony Gonzalez
[01:04:33.040 --> 01:04:36.560]   Fit program is working with them. I can't wait to get one. Are they out yet?
[01:04:36.560 --> 01:04:42.880]   The one is out. There's another one. The really cool looking one that I have a feeling I would
[01:04:42.880 --> 01:04:46.480]   rip right off of. I don't know how it does. Because it has stuff attached to it. Right. Right.
[01:04:46.480 --> 01:04:53.600]   But I, this is kind of the point, right? I think we that we are trading cool and convenience
[01:04:53.600 --> 01:05:02.640]   for a willing and willful ignorance about what happens on the back end. And I think we're all
[01:05:02.640 --> 01:05:08.880]   doing it. I mean, if if the three of us are doing it and we're hyper connected and we understand,
[01:05:08.880 --> 01:05:15.280]   think of the implications for people who are moving into Amazon homes. Amazon and Lanar have,
[01:05:15.280 --> 01:05:20.480]   Lanar is America's largest home builder. They've partnered and are building smart homes that
[01:05:20.480 --> 01:05:25.120]   don't just have a few Alexa device, a word, sorry, devices that have. Imagine your problem.
[01:05:25.120 --> 01:05:29.520]   Sorry. I started playing the this is the tonal device. Yeah, the phone watching. Actually,
[01:05:29.520 --> 01:05:37.840]   a friend of mine successfully led her apartment complex and they're a big kind of chain of building,
[01:05:37.840 --> 01:05:42.720]   you know, I guess management to not install the smart locks as she's a security professional.
[01:05:42.720 --> 01:05:48.080]   And she was very, very adamant. She's right. She is right. Because, and look, to me, that is one of
[01:05:48.080 --> 01:05:53.040]   those places where I would completely not be okay with that. Where I'm being told, you have to have
[01:05:53.040 --> 01:05:56.640]   this smart lock installed. You don't have any alternative. This is what we're going to do. We
[01:05:56.640 --> 01:06:02.800]   can enter your house at any time, especially when the security behind those has been so easily hacked
[01:06:02.800 --> 01:06:08.640]   and whatnot. Yeah, but come on. I mean, aren't locks just a suggestion anyway?
[01:06:08.640 --> 01:06:15.520]   It's just a social norm. It's not really. And there's windows. If somebody wants to break in,
[01:06:15.520 --> 01:06:19.280]   there's somebody who wants to get in your house. If somebody is a mind breaking the social norm,
[01:06:19.280 --> 01:06:26.160]   they can get into your house. Right. This is where it's useful to look at the bigger
[01:06:26.160 --> 01:06:31.680]   constellation of things. So Amazon has also invested in prefab home companies.
[01:06:31.680 --> 01:06:36.000]   Oh, yeah, they have a whole system. Right. The whole house is smart.
[01:06:36.000 --> 01:06:41.760]   That's right. So that's what I'm saying. We're quibbling right now over smart,
[01:06:41.760 --> 01:06:44.960]   whether or not we have a camera in our bedroom. And I guess what I'm trying to say is,
[01:06:44.960 --> 01:06:50.160]   unless we change how we think about this, it's inevitable that we will all have camera that will
[01:06:50.160 --> 01:06:55.920]   all constantly be under persistent surveillance, which in some ways, I think are great. But we
[01:06:55.920 --> 01:06:59.520]   don't have all of the back end legal infrastructure and privacy infrastructure,
[01:06:59.520 --> 01:07:04.880]   all that other stuff in place yet. The technology, as usual, is pushing far ahead of our capacity
[01:07:04.880 --> 01:07:09.760]   to think about what it all means. Right. I mean, but the one thing I would say that kind of gives
[01:07:09.760 --> 01:07:13.120]   me hope a little bit this because, of course, you're completely right. But there does seem to be
[01:07:13.120 --> 01:07:16.720]   kind of a breaking point where people are pushed too far. And I mean, Facebook is going through that
[01:07:16.720 --> 01:07:21.600]   right now where for many, many years, many of us never expected that the public, whatever,
[01:07:21.600 --> 01:07:25.600]   turn against Facebook, despite all the different things that they were doing and all the different,
[01:07:25.600 --> 01:07:30.320]   you know, breaches that were happening. And then with the combination of Cambridge Analytica
[01:07:30.320 --> 01:07:36.640]   and the involvement with Myanmar and the ridiculous amount of other atrocities,
[01:07:36.640 --> 01:07:43.120]   less people are using them. They have actually seen fewer people signing into standard Facebook.
[01:07:43.120 --> 01:07:47.600]   Now they still have their myriad of other services that they can get all kinds of information from.
[01:07:47.600 --> 01:07:54.800]   But there is this palpable kind of backlash against that. And so, it's about finding that balance.
[01:07:54.800 --> 01:08:00.720]   But I think to your point, yeah, when even those of us who know better are still willing to take
[01:08:00.720 --> 01:08:07.120]   the bad parts, even knowing what can potentially be bad about being in this always on surveilled
[01:08:07.120 --> 01:08:12.400]   society, like what chances anyone else have. I mean, I even think about myself, so I travel a lot.
[01:08:12.400 --> 01:08:17.600]   And before the show, you both were talking, you both travel a lot as well. With biometrics and what
[01:08:17.600 --> 01:08:23.040]   not, with the amount of times I go through airports, I use clear I'm in these other things. Like,
[01:08:23.040 --> 01:08:29.120]   my photo, my fingerprints, other information about me is maintained. And in
[01:08:29.120 --> 01:08:36.400]   an uncountable number of databases, you know, my passport information, my photos, all that stuff,
[01:08:36.400 --> 01:08:41.040]   as I travel through airports and it's now being held by foreign governments. And that's not even
[01:08:41.040 --> 01:08:45.600]   something that I have the opportunity to opt out of, right? Like, that's just if you want to fly
[01:08:45.600 --> 01:08:49.600]   through someplace, that's what you have to do. Aren't you just tempted to give up at some point?
[01:08:49.600 --> 01:08:54.480]   I mean, have you given up or no? Yeah, I mean, I have and I haven't. It's like, I'll have certain
[01:08:54.480 --> 01:09:00.160]   things that I will. But yeah, there's a certain amount of malaise, I think, and fatigue with all
[01:09:00.160 --> 01:09:06.880]   of it. Fighting it is so hard. And ultimately, you have to move to a cabin in the woods.
[01:09:06.880 --> 01:09:12.400]   Or you or you try to create systemic change, which is mine. That's what you do. And I like that.
[01:09:12.400 --> 01:09:21.760]   I am because I, to me, giving up is tantamount to giving up on our futures.
[01:09:21.760 --> 01:09:26.240]   The big nine. I'm not willing to do that. Amy's book is all about that.
[01:09:26.240 --> 01:09:33.040]   Well, because we can, I don't want to, like, if you like stop for a moment and just think about
[01:09:33.040 --> 01:09:38.400]   what's the worst possible feeling you could have, for me, it's regret. Like, that is the worst.
[01:09:39.840 --> 01:09:46.480]   And I don't want to be 30 years from now looking like feeling a horrific sense of regret that we
[01:09:46.480 --> 01:09:51.040]   could have taken a different path and we could have chosen, we could have chosen to slow down
[01:09:51.040 --> 01:09:58.400]   for five seconds and figure out how can everybody still make plenty of money. But how does everybody,
[01:09:58.400 --> 01:10:03.920]   how can we make this so that everybody wins and doesn't, everybody wins a little more and loses a
[01:10:03.920 --> 01:10:09.600]   little less? I don't want to be filled with regret that I didn't play my part. I didn't do something
[01:10:09.600 --> 01:10:14.480]   when I could have. Like, that's the worst, that regret, that's like the worst feeling because
[01:10:14.480 --> 01:10:19.600]   where it comes from is a place of there was something that I could have done differently or
[01:10:19.600 --> 01:10:23.520]   I didn't have to make that mistake. You know, and I could have, they could have had a better
[01:10:23.520 --> 01:10:28.880]   outcome. I don't want to, I don't want to be in that place. So that's why I, I've even given up.
[01:10:28.880 --> 01:10:31.680]   I've even given up regret. I just know I'm going to regret this.
[01:10:31.680 --> 01:10:37.360]   It's just life. Regrets, I've had a few.
[01:10:37.360 --> 01:10:40.880]   You have, but that's why we're talking about this. You could have spent the whole time.
[01:10:40.880 --> 01:10:46.160]   Yeah, I know. I think there's a good chance that will end up in the world of
[01:10:46.160 --> 01:10:52.480]   idiocracy where I think we're assuming that all this stuff is going to work and then
[01:10:52.480 --> 01:10:56.880]   that we're going to make a perfect surveillance system and it's going to know everything about it.
[01:10:56.880 --> 01:11:01.600]   There's also the option. Here's the smart home that Lenard is building with Amazon.
[01:11:02.400 --> 01:11:08.000]   This is the pantry. It's lined with something that no longer exists. Dash buttons.
[01:11:08.000 --> 01:11:14.080]   They thought, oh, this will be great. In the pantry, we'll put dash buttons next to everything.
[01:11:14.080 --> 01:11:17.520]   And then you press the button when you run out, except Amazon stopped making them.
[01:11:17.520 --> 01:11:22.800]   Yeah. And I feel like we're creating a janky future that's just going to break down.
[01:11:22.800 --> 01:11:28.000]   That's again, like, look at the bigger picture. There's sensor technology being built inside of
[01:11:28.000 --> 01:11:33.360]   materials that are in our pantries. So you don't need a dash button if you, as long as you put your
[01:11:33.360 --> 01:11:36.800]   stuff back in the street. It was just early technology that we, yeah, it's like a Las Vegas
[01:11:36.800 --> 01:11:41.520]   hotel where soon as you take it off the shelf. The minibar thing. You're paying for it.
[01:11:41.520 --> 01:11:46.560]   Yeah. Well, I mean, look, I had a bunch of dash buttons and I loved the idea of the
[01:11:46.560 --> 01:11:50.400]   theory and I did actually hack them so that they could be used for other things, which was really
[01:11:50.400 --> 01:11:55.040]   fun. But that the problem was, right? Like, okay, so you get them for toilet paper. That's a perfect
[01:11:55.040 --> 01:11:59.280]   thing to get them for. Except you don't think about the fact that you need toilet paper until
[01:11:59.280 --> 01:12:04.320]   you're on your left or stay there for a day or two and it will be delivered.
[01:12:04.320 --> 01:12:08.800]   Well, and that's the issue, right? At that point, you're like, okay, I'm going to actually have to
[01:12:08.800 --> 01:12:15.680]   run down the street and buy toilet paper rather than waiting for prime to come. Now, if they had
[01:12:15.680 --> 01:12:20.800]   integrated the dash buttons with prime now, like the same day deliver things. That's right.
[01:12:20.800 --> 01:12:26.160]   Right. Because I use prime now all the time and way too much. And in that point, I'm like,
[01:12:26.160 --> 01:12:32.960]   oh, I can get this in an hour if I pay another $5 done. That would be useful, but it wasn't.
[01:12:32.960 --> 01:12:37.680]   So, I did enjoy my Haribo one though because I had one for the gummy bears.
[01:12:37.680 --> 01:12:44.080]   Oh, that's dangerous. Can I offer a totally unrelated tangent? That's funny.
[01:12:44.080 --> 01:12:46.080]   Yes. Related to gummy bears. Oh, wait.
[01:12:46.080 --> 01:12:51.280]   Okay. So where I grew up, I grew up in Northwest Indiana, just outside of Chicago.
[01:12:51.280 --> 01:12:59.360]   And there's this gummy bear factory called Albinies. And they make both regular gummy bears and sugar
[01:12:59.360 --> 01:13:03.840]   free gummy bears. Do not do the sugar free gummy bears. Oh, but the sugar free ones are so bad.
[01:13:03.840 --> 01:13:08.640]   Oh, no, no, no, no. If you want to have a really good laugh, go on to Amazon and look at the reviews
[01:13:08.640 --> 01:13:13.520]   for the sugar free gummy bears. It's like, oh my God, it'll be like the best 10 minutes you've
[01:13:13.520 --> 01:13:16.720]   spent ever. For those who are wondering what could possibly go wrong?
[01:13:16.720 --> 01:13:24.240]   It's totally true. The fake sweetener they use has a slightly laxative effect. You can
[01:13:24.240 --> 01:13:31.040]   imagine the rest. Yeah. Well, if you only won, you're okay. You eat like three to four of those.
[01:13:31.040 --> 01:13:37.120]   It's explosive. You know, it's very explosive. We had somebody who actually
[01:13:37.120 --> 01:13:42.400]   had a five pound bag. It's the best Amazon review ever. Oh, yeah. That's what I'm saying,
[01:13:42.400 --> 01:13:46.400]   because I buy the five pound regular bags all the time and you have to make sure you don't get
[01:13:46.400 --> 01:13:52.640]   the sugar free ones because yes, there was actually we had somebody, I think it was a gizmoto who like
[01:13:52.640 --> 01:13:59.360]   knowingly did a review of them and they get, oh, it can't be that bad. And then it was worse than
[01:13:59.360 --> 01:14:04.640]   anyone could have ever imagined. So the top positive review on Herobo sugar free classic gummy bear
[01:14:04.640 --> 01:14:12.640]   one pound bag is the horror at 30,000 feet. The top critical review is one of the worst days of my
[01:14:12.640 --> 01:14:18.640]   life. But that's the Haribo. You need the albinies. Albinies is worse? No, no, no. Yeah, that's
[01:14:18.640 --> 01:14:25.920]   that's where the hilarious ones seem fairly hilarious. Okay, albinies. Yeah. So this is the
[01:14:25.920 --> 01:14:32.480]   hometown favorite. It is. The results are noxious and disgusting. Use it your own risk and be
[01:14:32.480 --> 01:14:38.880]   prepared for a fate worse than death. So all those 561 critical reviews are hilarious
[01:14:38.880 --> 01:14:44.400]   detailed accounts about partly because they sell them in five pound bags. Yeah. Yeah.
[01:14:44.400 --> 01:14:48.480]   There's a really funny one that's like, why would you sell this in a five pound bag?
[01:14:48.480 --> 01:14:57.600]   The night of 1000 waterfalls. That's a good one. Like distant thunder.
[01:15:00.400 --> 01:15:09.360]   Just don't unless it's a gift for someone you ate. These are good for losing 10 pounds.
[01:15:09.360 --> 01:15:16.000]   All right. So we won't buy these. I didn't realize that there was an alternative to Haribo.
[01:15:16.000 --> 01:15:21.520]   Well, the funny thing is even though the sugar free they have exactly the same caloric count
[01:15:21.520 --> 01:15:26.480]   as non sugar free. So what are they using for the sweetener?
[01:15:28.960 --> 01:15:32.880]   Who even knows that it's something bad something. Don't you don't want to eat?
[01:15:32.880 --> 01:15:40.480]   Regular ones are good. They're flavorful. They're flavorful. They're delicious.
[01:15:40.480 --> 01:15:48.480]   It's in the heart of everything we do. It's not just on our bag. It's at the heart of everything we do.
[01:15:48.480 --> 01:15:54.640]   They're like a family run company. I feel so bad. It's been there forever. Yeah. Yeah. They're
[01:15:54.640 --> 01:15:59.920]   using a rithrotol or some sort of sugar alcohol. I'm sure. Oh, malletol. That is actually literally
[01:15:59.920 --> 01:16:08.240]   baby laxative. Really? Yeah. Oh my God. That's good. It's the number one ingredient.
[01:16:08.240 --> 01:16:18.960]   That's so messed up. Yum. All right. Let's take a little tiny break.
[01:16:21.600 --> 01:16:26.720]   Amy, I'm looking for your big book. I want to get this. Now you sent it to me,
[01:16:26.720 --> 01:16:30.240]   but people can get it themselves right from the future today Institute report.
[01:16:30.240 --> 01:16:33.840]   Yes. This is available for free online. We give away all of our research.
[01:16:33.840 --> 01:16:39.440]   We always print up some of our clients. And so I've got like we have a handful. If you want to
[01:16:39.440 --> 01:16:43.760]   buy a hard copy, you can go to our website, but you can download it for free. So yeah,
[01:16:43.760 --> 01:16:48.960]   you can make your own print out if you if you want. Yeah. I mean, it's a dream of paper, but yeah.
[01:16:48.960 --> 01:16:55.120]   The future future today Institute with Amy is I'm even more excited about this than the five pound
[01:16:55.120 --> 01:17:00.320]   bag of gummy bears. You said you're going to send me. She said she's sending me one. I can't wait.
[01:17:00.320 --> 01:17:07.360]   And also film girl. It's suddenly got dark in. Suddenly got dark. So during the break,
[01:17:07.360 --> 01:17:12.000]   I'm going to turn on the so the lights and the building go off at a certain period of time. So
[01:17:12.000 --> 01:17:16.560]   you're going to switch them back on again. Yeah, they're trying to like, you know, save the
[01:17:16.560 --> 01:17:21.040]   environment or something, save money, but I will go reset that in just a second.
[01:17:21.040 --> 01:17:26.640]   That's good. I'm taking a break. Here's something you would love, by the way, in your team space.
[01:17:26.640 --> 01:17:31.840]   How many people are in that team space? It varies. I mean, I think we've got like six desks, but
[01:17:31.840 --> 01:17:38.080]   okay, you need a molecule air fresher for sure. In there. We have this in our house. It's funny
[01:17:38.080 --> 01:17:44.560]   because Lisa, we were in Hawaii. I don't know why. No pollen. No runny nose. No headache. The
[01:17:44.560 --> 01:17:50.720]   minute she gets home, pollen in the air, our cats come in. They're yellow. They're they're
[01:17:50.720 --> 01:17:54.800]   black cats, but they're coated with yellow pollen. You can see the brown, the football,
[01:17:54.800 --> 01:17:59.200]   all this yellow pollen is that time of year. And we've had a big rainy season. So it's an
[01:17:59.200 --> 01:18:03.920]   allergy nightmare. Thank God we have the molecule. This saved our lives. We've had the molecule
[01:18:03.920 --> 01:18:09.680]   in that for a couple of years. And it's fun because if if if somebody turns it off or we go away,
[01:18:10.320 --> 01:18:17.760]   Lisa knows immediately, but when that's running, it is a miracle molecule is not it's not a HEPA
[01:18:17.760 --> 01:18:24.640]   filter. It's reimagining the future of clear air. It uses something they call Pico technology.
[01:18:24.640 --> 01:18:29.200]   The HEPA filter has been around since World War II. It's a filter literally and it can trap big
[01:18:29.200 --> 01:18:34.880]   particles, but not the little ones that can cause some of the worst allergies and other problems,
[01:18:34.880 --> 01:18:39.840]   health problems. The Pico technology short for photo electrochemical oxidation
[01:18:39.840 --> 01:18:45.360]   goes well beyond what a HEPA filter will do it. Not only captures allergens, okay, HEPA filter
[01:18:45.360 --> 01:18:52.080]   might do that, but it eliminates them. Mold bacteria viruses viruses particles so small they go right
[01:18:52.080 --> 01:18:57.520]   through a HEPA filter trapped and eliminated even airborne chemicals, volatile organic compounds
[01:18:57.520 --> 01:19:03.360]   like formaldehyde from your carpet or fumes from paint pollutants 1000 times smaller than those
[01:19:03.360 --> 01:19:09.440]   a HEPA filter can catch molecules technology has been personally effective and verified by science
[01:19:10.320 --> 01:19:15.600]   and used by real people like us, including allergy and asthma sufferers around the country.
[01:19:15.600 --> 01:19:22.400]   We have a my son has a friend who has asthma comes with his breather is inhaler. He never
[01:19:22.400 --> 01:19:25.520]   needs to use it when he's in our house. We like it so much. We got one for Michael's bedroom.
[01:19:25.520 --> 01:19:32.320]   We now have one in the studio too. It's amazing. The fund the technology was funded by the EPA.
[01:19:32.320 --> 01:19:36.720]   It has been tested by third parties and university laboratories like the University of
[01:19:36.720 --> 01:19:42.240]   South Florida Center for Biological Defense, Minnesota University's particle calibration
[01:19:42.240 --> 01:19:47.760]   laboratory. Really beautiful. It's a solid sleek aluminum. It's kind of like the apple of air
[01:19:47.760 --> 01:19:54.400]   purifiers. Very easy to change the filters. It has two filters a pre filter and then that Pico
[01:19:54.400 --> 01:20:01.040]   filter which is it's a it's a it's almost like a catalytic converter. It captures the particles
[01:20:01.040 --> 01:20:06.560]   and then UV light burns the particles up. It's really kind of remarkable. Plus you can tie it to
[01:20:06.560 --> 01:20:10.960]   your Wi-Fi network or a Bluetooth network control it by a Bluetooth. It has controls on the top.
[01:20:10.960 --> 01:20:14.000]   But if you want to do it with your phone and the nice thing about tying it to your Wi-Fi
[01:20:14.000 --> 01:20:20.720]   it'll automatically order new filters when you need them auto refills. Molecules amazing. We're
[01:20:20.720 --> 01:20:24.720]   going to get just $75 off your first order. And if you hear heard these ads before and went and saw
[01:20:24.720 --> 01:20:29.520]   that they were sold out, they now have them in stock. They do sell out fast though. So go to molecule
[01:20:29.520 --> 01:20:36.320]   with a K-M-O-L-E-K-U-L-E dot com and use a promo code TWIT1. It's a new promo code TWIT1
[01:20:36.320 --> 01:20:43.600]   at checkout for $75 off your first order. Don't just capture allergens. Destroy them with molecule.
[01:20:43.600 --> 01:20:52.080]   You'll thank me later. Molecule dot com promo code TWIT and the number one. M-O-L-E-K-U-L-E.
[01:20:53.520 --> 01:21:01.360]   And we thank the support. Here's the Jim on the wall that you were talking about Amy. It's called
[01:21:01.360 --> 01:21:06.480]   the tunnel. So this is a smart mirror. I don't know if this is what I want. But
[01:21:06.480 --> 01:21:10.800]   I thought a smart mirror would just say, "Hey, you look good today."
[01:21:10.800 --> 01:21:16.400]   There's another one. There's a couple out there where they... This looks like it's just showing
[01:21:16.400 --> 01:21:20.800]   the ones that I've seen and I can't remember the brand. It has stuff on them. It looks at what
[01:21:20.800 --> 01:21:26.640]   you're doing and then automatically changes the tension and tells you to
[01:21:26.640 --> 01:21:30.720]   move your posture. Xbox used to do that. I like that. With the Kinect.
[01:21:30.720 --> 01:21:35.040]   Right? It would know your heart rate. It would know if you're working hard enough.
[01:21:35.040 --> 01:21:38.480]   It would adjust it based on that. That really killed me when they killed that product. I thought
[01:21:38.480 --> 01:21:43.600]   that was a great product. It was great. It had good stuff for sure. It just shows you sometimes.
[01:21:43.600 --> 01:21:47.440]   People freaked out. But people freaked out. Is that why? Like the next one came out.
[01:21:47.440 --> 01:21:52.880]   People freaked out about the Kinect stuff and the always on stuff. And a lot of things,
[01:21:52.880 --> 01:21:58.160]   people were not into it. So yeah. So Apple bought the company, was an Israeli company.
[01:21:58.160 --> 01:22:04.000]   And it's in the front of your iPhone X and XS. I know. And it's funny because Windows Hello
[01:22:04.000 --> 01:22:09.840]   has a lot of that same stuff with it too. And it's great. I'm always amazed by how...
[01:22:09.840 --> 01:22:15.760]   I thought Apple bought the... I thought Apple was a... Was it just only ever a subsidiary?
[01:22:15.760 --> 01:22:18.720]   I thought it was. They didn't buy Kinect. They bought the company that did the
[01:22:18.720 --> 01:22:25.760]   foundational technology for Kinect. Apple stole them out. They made the camera,
[01:22:25.760 --> 01:22:31.920]   the three-dimensional kind of IR cameras sort of tech for the very first Kinect. The second
[01:22:31.920 --> 01:22:36.480]   Kinect, the Kinect for the Xbox One was made, I think, in-house. The final... The Prime Sense was
[01:22:36.480 --> 01:22:45.600]   the sensor. Prime Sense. Yes. And Apple bought them, which is kind of weird.
[01:22:46.240 --> 01:22:50.560]   That Microsoft... I guess Microsoft knew. Well, I mean, although, I mean,
[01:22:50.560 --> 01:22:55.920]   like I said, I think it was done in-house because the second Kinect was made by a different...
[01:22:55.920 --> 01:23:02.320]   Made with different stuff. And Windows Hello is largely the same idea. And...
[01:23:02.320 --> 01:23:05.280]   Yeah. Once you get the idea, it's not probably too hard to...
[01:23:05.280 --> 01:23:11.760]   Yeah. Windows Hello is impresses me, but it can be at a weird angle. And it'll still,
[01:23:11.760 --> 01:23:16.480]   as long as I'm not out in bright sunlight, it'll recognize me basically anywhere.
[01:23:16.480 --> 01:23:23.840]   I wish I had Face ID on my laptop. I mean, Touch ID is fine, but there is something about
[01:23:23.840 --> 01:23:28.240]   just having to like casually glance up and automatically be logged in that's very nice.
[01:23:28.240 --> 01:23:33.520]   So I got a new Windows laptop. I got that new SIGrapad. And I have the choice between all the
[01:23:33.520 --> 01:23:40.800]   different because it has a Hello camera and has a Hello finger per meter. I could also do the...
[01:23:40.800 --> 01:23:44.880]   There's like eight ways to lock in. And I'm using the finger per meter. But you know what I think
[01:23:44.880 --> 01:23:50.000]   is Hello is secure? The Face recognition is secure as a fingerprint. Is it good?
[01:23:50.000 --> 01:23:53.440]   Yeah. It's the exact same... I mean, it's the exact same encryption. So whether you're using the
[01:23:53.440 --> 01:23:57.840]   fingerprint sensor or the camera... No, but I mean, it's the... Somebody can't... My twin can't unlock it,
[01:23:57.840 --> 01:24:01.520]   right? Actually, I don't have a twin. So that probably is true.
[01:24:01.520 --> 01:24:06.080]   Yeah. I mean, I think your twin probably could unlock it. That was a problem with Apple's iPhone
[01:24:06.080 --> 01:24:09.920]   10, right? Family numbers were able to unlock it. Well, I mean, I think it's a problem with
[01:24:09.920 --> 01:24:14.240]   anything. If you have an identical twin, like they're probably going to be able to use their
[01:24:14.240 --> 01:24:18.640]   facial thing to do it, probably, honestly. But that's... If you have like an identical twin,
[01:24:18.640 --> 01:24:22.880]   it's not enough to be... You look alike, like even for journals, it might not work. But if you have
[01:24:22.880 --> 01:24:29.200]   an identical twin, I mean... There was... DNA. So, you know... Right. When the iPhone 10 came out,
[01:24:29.200 --> 01:24:35.120]   there were a mother and son and he could unlock... In fact, wait a minute, Megan Maroney's son could
[01:24:35.120 --> 01:24:39.760]   unlock her iPhone 10 coming to think of it. Yeah. That's interesting. And their identical twins,
[01:24:39.760 --> 01:24:44.720]   so they could unlock each other and they could unlock her. So, it's... I think, as I remember,
[01:24:44.720 --> 01:24:50.080]   it does things like spacing between the eyes. It does the geometry of the face. It's doing,
[01:24:50.080 --> 01:24:55.200]   you know, all these weird things. And then it's changing things every time you're using it's
[01:24:55.200 --> 01:24:58.800]   updating. So, you know, the fingerprint sensor is obviously really fast, too, because one of my
[01:24:58.800 --> 01:25:03.360]   laptops, my Huawei doesn't have a face ID or doesn't have the Windows-hole camera. But it has a better
[01:25:03.360 --> 01:25:08.080]   fingerprint reader, though, I think. Yeah. But it does have the Windows-hole fingerprint reader.
[01:25:08.080 --> 01:25:14.560]   Right. Very good. So, that's instantly like it's super fast. But, you know, on my surface book,
[01:25:14.560 --> 01:25:20.160]   obviously, that has Windows-hole. And it's interesting how even if your hair changes or makeup or,
[01:25:20.160 --> 01:25:26.480]   you know, weight or whatever, like it continues to just be able to pick it up and work.
[01:25:26.480 --> 01:25:31.200]   It does. I know why... See, the problem... This is a desktop, the Surface Studio.
[01:25:32.400 --> 01:25:37.920]   Every time I'm going to log in, I either have to lean over it like lurch or raise it up so that
[01:25:37.920 --> 01:25:41.440]   it can see me because it has to be at the right angle. So, there is a disadvantage to having it.
[01:25:41.440 --> 01:25:47.600]   There is. Yeah. I was going to say I have a theme issue with my iPad Pro, because if I'm like
[01:25:47.600 --> 01:25:51.920]   laying in bed with it and I've got it at a weird thing, then I have to like sit up exactly. I've
[01:25:51.920 --> 01:25:59.360]   got the same sort of situation. Yep. Yep. So, you know that Walmart has a smart shopping cart.
[01:25:59.360 --> 01:26:03.120]   Did I talk to you about this last time? No. I was on. So... Oh, that's interesting.
[01:26:03.120 --> 01:26:11.920]   How smart is it? So, real smart. So, it's not in production yet. But they've built a concept for
[01:26:11.920 --> 01:26:16.560]   a shopping cart that collects basic biometrics once you get to the store. So, you would lay your
[01:26:16.560 --> 01:26:22.160]   hands on the shopping cart. And it takes... Put some hands on the shopping cart.
[01:26:22.160 --> 01:26:26.800]   The hands on the cart. It takes a baseline reading of your heart, beat, your perspiration,
[01:26:26.800 --> 01:26:31.520]   the tension that you're holding the cart with. Why? And obviously, there are also cameras all
[01:26:31.520 --> 01:26:36.000]   around the store. And I think the idea is as you move throughout the store, it looks for
[01:26:36.000 --> 01:26:41.120]   fluctuations. So, if you get to aisle seven and you're trying to find your Captain Crunch and
[01:26:41.120 --> 01:26:44.800]   you're ready to... You know, you can't find it and you're ready to blow a gasket. My heart always
[01:26:44.800 --> 01:26:50.000]   pans when I get to the Captain Crunch aisle. There is. No, that's really... That is really smart.
[01:26:50.000 --> 01:26:53.840]   You know, who comes over to help you find it. But obviously, there's data being...
[01:26:54.400 --> 01:26:59.600]   Are they pitching this to say, "Oh, and it could save your life if you have a heart attack at our store?"
[01:26:59.600 --> 01:27:06.080]   Or no? You know, I don't... I don't know what their pitch... I don't think so at the moment. But
[01:27:06.080 --> 01:27:11.200]   here's the... I mean, you guys are giving away your faces and your fingerprints and...
[01:27:11.200 --> 01:27:17.040]   Everything. We don't... You know, what happens if one of these companies gets bought or sold?
[01:27:17.040 --> 01:27:22.720]   Right? I mean, because we're sort of... We're kind of like laxed days ago about our...
[01:27:22.720 --> 01:27:25.200]   All of our biometrics as well as our DNA.
[01:27:25.200 --> 01:27:32.560]   No, that's why I haven't done any of those DNA kits, frankly, even though I would really like
[01:27:32.560 --> 01:27:35.760]   the information from them and I think it would be really good. I haven't done it. That's kind of
[01:27:35.760 --> 01:27:39.600]   where I draw the line. My fingerprints stuff, I don't really have a say in the matter in some of
[01:27:39.600 --> 01:27:44.160]   the cases, you know, especially if you're... Like I said, if you're traveling a lot or whatever, but
[01:27:44.160 --> 01:27:49.920]   for some of the... In my face, I certainly don't. But my DNA is the one thing where I'm like...
[01:27:49.920 --> 01:27:54.560]   My faces are like... They're in... Right. But like some of this other... I mean...
[01:27:54.560 --> 01:28:01.120]   Well, did you read today's... New York Times, The Privacy Project, Sarah Zhang, writing about...
[01:28:01.120 --> 01:28:08.400]   At first, I thought, you know, the title is AI is Changing Insurance. Some technologies are
[01:28:08.400 --> 01:28:14.240]   better left in the laboratory. At first, I thought, oh, I'd like to know what is... This is more a
[01:28:14.240 --> 01:28:20.960]   potential threat. But this is something... I'm just a vorac. Whenever we talk about privacy,
[01:28:20.960 --> 01:28:26.320]   this is always the example he would use. Well, you go buy donuts a lot. If the GPS says you're
[01:28:26.320 --> 01:28:32.480]   always at the Duncan store, you're going to have a hard time getting insurance. But maybe this is
[01:28:32.480 --> 01:28:37.280]   what insurance companies are planning to do. Oh, completely it is. I mean, that's always been...
[01:28:37.280 --> 01:28:41.440]   I mean, they'll give you the subsidized Apple Watch or Fitbit or Fitbit Knockoff, which is...
[01:28:41.440 --> 01:28:45.040]   Yeah, they have... That's real. They offer... If you use a Fitbit, you can get a reduction in life
[01:28:45.040 --> 01:28:49.440]   insurance, right? Right, right, right. Because my parents, for instance, like, I guess, whatever
[01:28:49.440 --> 01:28:53.280]   their thing is, like, my dad had like a knockoff Fitbit and he... It broke and he didn't like it
[01:28:53.280 --> 01:28:57.120]   over his birthday, which was like a week and a half ago, I bought him whatever the latest
[01:28:57.120 --> 01:29:01.280]   greatest Fitbit was. It was like, here you go. Because John Hancock did this last year, I remember.
[01:29:01.280 --> 01:29:07.360]   Yeah. And... But, you know, and that's a good thing. You can get reduced rates and what not.
[01:29:07.360 --> 01:29:12.400]   But then you do wonder, okay, at least for now how it's worked is that they'll just give you kind of,
[01:29:12.400 --> 01:29:19.760]   you know, like a lower premium or whatever, regardless of what your health is. But then you
[01:29:19.760 --> 01:29:24.640]   wonder, okay, are they going to start trying to gather the information from that? And at least right
[01:29:24.640 --> 01:29:31.200]   now, they're not. But that's got to be the next level, right? Which is they're saying, oh, you know,
[01:29:31.200 --> 01:29:35.760]   if we're seeing this sort of activity, then we can anticipate that you'll have... You can have
[01:29:35.760 --> 01:29:41.120]   lower premiums. But the inverse is true too. If we see, you know, this type of activity,
[01:29:41.120 --> 01:29:46.080]   then we will charge you more. That I'm not super jazzed about. And I say this as somebody who,
[01:29:46.080 --> 01:29:51.040]   by, you know, all accounts would be on the lower premium scale, at least for things like, you know,
[01:29:51.040 --> 01:29:55.920]   like heart disease and cholesterol and blood pressure and whatnot. I'm waving at you from the
[01:29:55.920 --> 01:30:02.400]   other end of that scale. Hello. Well, here's... So, you know, I have a picture of this. Right?
[01:30:02.400 --> 01:30:10.480]   Yeah. You're wearing devices. You're in... You're effectively plugging into Apple and or Apple and
[01:30:10.480 --> 01:30:18.080]   Google and Amazon or some combination. And those companies increasingly are mining and refining
[01:30:18.080 --> 01:30:22.560]   health data because they're increasingly offering their own health things or they're starting to
[01:30:22.560 --> 01:30:28.080]   build their health. Yes. My Samsung as a health app, I know it's monitoring everything because I...
[01:30:29.280 --> 01:30:33.280]   Here's the concern that I have. The concern that I have is that all this technology that we've
[01:30:33.280 --> 01:30:38.320]   been talking about today, 10 years from now or perhaps, you know, maybe five years from now,
[01:30:38.320 --> 01:30:45.760]   makes a decision that I haven't burned enough calories and it's... Somebody's decided to optimize
[01:30:45.760 --> 01:30:50.480]   and nudge me into better health and therefore I can't open my garage door and drive to work.
[01:30:50.480 --> 01:30:51.200]   [Laughter]
[01:30:51.200 --> 01:30:55.440]   I have to take my bike or walk. But you would do that because you get a 30% discount in your
[01:30:55.440 --> 01:31:00.720]   insurance. But... Again, wait a minute. Wait a minute. This is kind of a serious point
[01:31:00.720 --> 01:31:07.920]   is because the problem is that decision making technologies are by design inflexible and there's
[01:31:07.920 --> 01:31:12.560]   always context. So, sure, there are days that I'm feeling like lazy and I don't want to move
[01:31:12.560 --> 01:31:17.760]   and I don't want to exercise and I just want to eat whatever and be left alone. You know,
[01:31:17.760 --> 01:31:22.640]   there are other days when there may be very real reasons that I cannot... Like, right now,
[01:31:22.640 --> 01:31:28.160]   I've got two broken ankles, right? And... Did you jump out of an airplane without a parachute?
[01:31:28.160 --> 01:31:31.920]   It's a long story. Oh, dear. I'm sorry. No, fell off of a stage.
[01:31:31.920 --> 01:31:36.560]   Oh, gosh. But then like David Grohl, I picked up my guitar and I kept...
[01:31:36.560 --> 01:31:37.280]   I said, "I'm okay."
[01:31:37.280 --> 01:31:42.000]   Kind of. And then I, you know, have to go in for surgery in a couple of weeks. But...
[01:31:42.000 --> 01:31:48.800]   My point is, like, my foot hurts, you know, and I technically can walk but it hurts really badly
[01:31:48.800 --> 01:31:53.280]   to walk right now. So, you shouldn't? The thing is that our smart devices...
[01:31:53.280 --> 01:31:57.280]   Does it know? Again, like, you have to start thinking this through. It's not just about
[01:31:57.280 --> 01:32:05.600]   insurers charging less money or more money. We'll make it possible for you to call and ask
[01:32:05.600 --> 01:32:09.600]   for a waiver today. But think of... Your number 43...
[01:32:09.600 --> 01:32:17.280]   Your call is important to us. My concern is that there's relatively like a few number of people
[01:32:17.280 --> 01:32:22.160]   who are making these decisions about what our best lives and our most optimized lives are,
[01:32:22.160 --> 01:32:27.520]   who don't have as wide a cultural perspective as I might like, who can't possibly think through
[01:32:27.520 --> 01:32:34.000]   every possible... That's right. You know, variable and we don't have overwrite capacity. So, and
[01:32:34.000 --> 01:32:39.040]   terms of service are changing all the time and if you're not... If you don't have like an eagle eye
[01:32:39.040 --> 01:32:44.560]   paying attention to all of this, you miss it. And my concern is for... My concern is always for
[01:32:44.560 --> 01:32:49.680]   everyday people. Everyday people who are caught in the middle of all of this stuff, whose lives
[01:32:49.680 --> 01:32:56.560]   wind up unintentionally worse off because there wasn't transparency or nobody thought that,
[01:32:56.560 --> 01:33:01.920]   you know, explaining things was all that important or that they were just going to force them to
[01:33:01.920 --> 01:33:06.240]   live their best life, their best healthiest life. And therefore, we were going to use their data in
[01:33:06.240 --> 01:33:12.080]   this way. There's always external circumstances that we got to take into account.
[01:33:13.040 --> 01:33:17.120]   I 1000% agree. I mean, and you're right. And we don't do a good enough job of the way that we
[01:33:17.120 --> 01:33:21.840]   train these models and there isn't really the push to train them better. Where, like you said,
[01:33:21.840 --> 01:33:26.480]   we look at those nuances and where people think about what the potential consequences are. And
[01:33:26.480 --> 01:33:32.960]   even if, you know, my personal circumstances right now would be negatively impacted,
[01:33:32.960 --> 01:33:38.160]   doesn't mean that they couldn't be down the line or that I even want it to be part of consideration
[01:33:38.160 --> 01:33:44.320]   at all. Because a lot of times the data doesn't even show what people want to read from it.
[01:33:44.320 --> 01:33:50.560]   You know, I mean, I think you're exactly right. These things can have long-term negative impacts
[01:33:50.560 --> 01:33:57.360]   on our society that we don't take into account and that doesn't have enough scrutiny because
[01:33:57.360 --> 01:34:01.200]   people are thinking, "Oh, look at how great this is and look at how much this will be optimized
[01:34:01.200 --> 01:34:04.400]   and how much more productive will be." But what are you really... Well, and more importantly,
[01:34:04.400 --> 01:34:07.200]   I think it's clear that insurance companies need to be more profitable.
[01:34:07.200 --> 01:34:13.040]   Somebody actually just in the IRC wrote, KV just wrote, "It's a privacy tax on the poor."
[01:34:13.040 --> 01:34:16.240]   And I think that's a really... Oh, completely. That's an... It's completely accurate.
[01:34:16.240 --> 01:34:20.960]   Absolutely. Yeah. But I think it's a privacy tax on the poor and those who either...
[01:34:20.960 --> 01:34:22.960]   Why, explain that. Pardon?
[01:34:22.960 --> 01:34:28.480]   Explain how it's a privacy tax. Yeah. So I don't think it is at the moment,
[01:34:28.480 --> 01:34:34.960]   but I think it will be increasingly. If you've got all of these devices... So the way that I like
[01:34:34.960 --> 01:34:41.200]   to think about this is Apple charges a premium, but Apple's devices are... The privacy is fairly
[01:34:41.200 --> 01:34:46.480]   short up and people understand what data probably is moving around and where. But that's not the case
[01:34:46.480 --> 01:34:50.640]   with all of these other devices. And if you're somebody who wants to be connected but can't afford
[01:34:50.640 --> 01:34:58.400]   to buy into Apple's ecosystem, then your data are more likely to be extracted and mined in
[01:34:58.400 --> 01:35:04.000]   refined and given away. But it's not entirely transparent that that's what's happening.
[01:35:04.000 --> 01:35:11.200]   In the health device, if you're somebody who can't afford to pay or you need to pay less for your
[01:35:11.200 --> 01:35:18.400]   insurance for whatever reason, and you're willing to give up your data, it is. It's kind of like a
[01:35:18.400 --> 01:35:24.960]   privacy tax. In other words, you have worse insurance. It'll pay more for insurance if you can't
[01:35:24.960 --> 01:35:32.560]   afford to have the fancy monitoring stuff. Or you're trying to just pay less for whatever reason.
[01:35:32.560 --> 01:35:35.520]   Exactly. And you don't understand what you're giving up in the process.
[01:35:35.520 --> 01:35:39.840]   Right. Well, and it's not just the poor, I would also say the elderly because my parents are both
[01:35:39.840 --> 01:35:47.200]   at the age where I guess they're on Medicare or whatever. And that's what's offering. I guess my
[01:35:47.200 --> 01:35:54.320]   dad gets his free Fitbit thingy or whatever. And obviously, once you're at retirement age and
[01:35:54.320 --> 01:35:59.600]   whatnot, and you're living off of your retirement off of your savings off of whatever stipends you're
[01:35:59.600 --> 01:36:03.680]   getting, that's going to go into consideration too. So I think it's the poor and the elderly.
[01:36:03.680 --> 01:36:09.200]   And we're all going to be older at one point regardless of what we do. So, yeah.
[01:36:09.200 --> 01:36:11.280]   Well, how about this? There is an optimistic framing here, though.
[01:36:11.280 --> 01:36:16.000]   How about this? A new policy proposal by the Trump administration calls for the surveillance
[01:36:16.000 --> 01:36:22.560]   of disabled people's social media profiles to determine the necessity of their disability benefits.
[01:36:23.600 --> 01:36:28.640]   The proposal, which aims to cut down on the number of fraudulent disability claims would
[01:36:28.640 --> 01:36:33.040]   monitor the profiles of disabled people and flag content that shows them doing physical
[01:36:33.040 --> 01:36:39.760]   activities. That's horrific. Or how about this? Here's a patent application from State Farm
[01:36:39.760 --> 01:36:47.440]   for, they call it the aggregation and correlation of data for life management purposes.
[01:36:48.480 --> 01:36:55.040]   It's a plan for aggregating home data, vehicle data, and personal health data
[01:36:55.040 --> 01:37:01.360]   to help you have a better life. Of course, State Farm's interest is not having you have a better
[01:37:01.360 --> 01:37:07.520]   life. No, it's not. So I was hit by the car. Look at his diagram.
[01:37:07.520 --> 01:37:16.160]   I deal insurance stuff right now because I was some some idiot drove his car into me when I was
[01:37:16.160 --> 01:37:20.800]   crossing the street in a pedestrian crosswalk. This is separate from the bus incident?
[01:37:20.800 --> 01:37:25.600]   This was the bus incident. I was hit by the car thrown under the bus.
[01:37:25.600 --> 01:37:32.400]   I'm dealing with insurance right now with his insurance company and it's a whole thing.
[01:37:32.400 --> 01:37:37.360]   My God, and already it's dealing with their trying to be like, "Oh, it wasn't that bad.
[01:37:37.360 --> 01:37:42.720]   You went to work the next day." I was like, "Well, yeah, because I'm dumb and because I didn't,
[01:37:42.720 --> 01:37:48.240]   you know, like, there you go. They got you." I'm like, I still couldn't write. I couldn't
[01:37:48.240 --> 01:37:52.960]   you. I'm right handed. I couldn't type. I couldn't use my wrist for, you know, like sufficiently.
[01:37:52.960 --> 01:37:55.680]   Like I've got all kinds of evidence and everything. But yeah,
[01:37:55.680 --> 01:37:59.360]   Jack Lemon movie many years ago called The Fortune Cookie. Jack Lemon's an insurance
[01:37:59.360 --> 01:38:04.400]   examiner following a guy around on crutches to find out if he really is heard. Here's the,
[01:38:04.400 --> 01:38:11.200]   by the way, this is a brilliant, a brilliant patent application. Here's here's you, Christina.
[01:38:12.000 --> 01:38:17.040]   Yeah. Here's your car. Here's your house. It all goes into the cloud.
[01:38:17.040 --> 01:38:22.240]   The brain. Where a computing device, which includes a processor, memory, and user interfaces,
[01:38:22.240 --> 01:38:27.360]   processes it, puts it back down in the data processor so that we can
[01:38:27.360 --> 01:38:34.560]   I don't know. Manage your life. Yeah, judge. So that we can judge and need a picture of the judging.
[01:38:34.560 --> 01:38:40.080]   So I'm going to say this once again. Everybody loves to point the finger at China and China's
[01:38:40.080 --> 01:38:46.000]   social credit system. And, you know, wow, that's crazy. Thank God we don't live there. And, you know,
[01:38:46.000 --> 01:38:51.280]   ours is just not that coordinated. But you're crazy to think that
[01:38:51.280 --> 01:38:59.440]   and the thing that really gets me is that it's happening in slow motion right in front of our eyes.
[01:38:59.440 --> 01:39:03.680]   Yeah. And we marvel at it and we fetishize the future and we don't take any action.
[01:39:03.680 --> 01:39:08.800]   Right. And again, like, let's just, this is where things get sticky because
[01:39:09.520 --> 01:39:16.640]   we have to be able to mine and refine and automate some of these processes in order to do things like
[01:39:16.640 --> 01:39:24.800]   use machines to help us figure out cures to cancer. Like to do all of these amazingly difficult,
[01:39:24.800 --> 01:39:31.600]   wicked, you know, problem solving that we want to accomplish that's possible. That's on the horizon.
[01:39:31.600 --> 01:39:36.400]   We got to figure out a way to do this without our consumerism getting in the way and our,
[01:39:36.400 --> 01:39:40.400]   you know, strong desire to make a buck. You know, well, thank you.
[01:39:40.400 --> 01:39:43.200]   This is complicated because these companies are public and they need to
[01:39:43.200 --> 01:39:47.920]   give a fiduciary responsibility to their shareholders. We got to sort this stuff out.
[01:39:47.920 --> 01:39:51.760]   I'm just grateful that Google has decided to create an AI ethics board.
[01:39:51.760 --> 01:40:00.000]   Manage. It's going so well. Oh, wait a minute. So well, they canceled it. One week later.
[01:40:01.840 --> 01:40:07.040]   That was never mind. It was only going to meet four times a year anyway. So
[01:40:07.040 --> 01:40:12.880]   doomed from the start. Yeah. Completely. That was the strangest assemblage of people that I've,
[01:40:12.880 --> 01:40:17.200]   I mean, like that was a weird, that was weird from the beginning. Yeah.
[01:40:17.200 --> 01:40:24.800]   Let's take a, let's take a break and we'll be back with more first before we do anything we should.
[01:40:24.800 --> 01:40:31.040]   I wasn't here last week and I, and I've asked, we put cameras everywhere in the building and I've
[01:40:31.040 --> 01:40:36.800]   asked our team to put together a surveillance video of what I missed this week on Twitch.
[01:40:36.800 --> 01:40:42.320]   Let's range together on to it. This was part of my pirate costume that I wore for Halloween,
[01:40:42.320 --> 01:40:44.720]   I think a couple of years ago. Right. Your Johnny Depp costume.
[01:40:44.720 --> 01:40:49.840]   That's what everybody said. But I was like, no, I'm just a standard pirate because it's Johnny Depp.
[01:40:49.840 --> 01:40:57.120]   Yeah. No, not him. Yeah. Okay. Sorry. All about Android. Look at this gaming laptop of a phone.
[01:40:59.120 --> 01:41:06.640]   That is a gamer phone. Now, why are there connectors on the sides of this case?
[01:41:06.640 --> 01:41:14.960]   Interesting. Because intellectual property be damned. Yeah, clearly. Cool. The Black Shark 2 is the
[01:41:14.960 --> 01:41:20.960]   device that's really interesting. Tech news weekly. According to a new Gallup poll, 61% of people
[01:41:20.960 --> 01:41:26.240]   polled believe that they use their phone less than the people around them. That's 61% of people
[01:41:26.240 --> 01:41:31.600]   believe that they use their phone less. In reality, in my case, it's true. Am I part of the problem?
[01:41:31.600 --> 01:41:36.560]   I wouldn't consider you part of the problem, but I think you're lying to yourself.
[01:41:36.560 --> 01:41:42.320]   You know what you do do? We share an office and you talk to your Google Assistant a lot,
[01:41:42.320 --> 01:41:45.520]   and I think you're talking to me. And there needs to be a word. Like, you're just like,
[01:41:45.520 --> 01:41:49.440]   remind me to get the laundry. I was like, I'm not going to remind you to get the laundry.
[01:41:50.400 --> 01:41:57.280]   To it. Technology isn't always pretty, but we are. Jason, how was that phone?
[01:41:57.280 --> 01:42:01.520]   How are they not getting sued by Nintendo? Oh, yeah, because it's exactly the same as the
[01:42:01.520 --> 01:42:06.880]   switch. It's the switch. Yeah. It looks pretty cool. Nintendo is getting sued by Game Shark over...
[01:42:06.880 --> 01:42:12.320]   Really? For real? For real. Yeah. That's the shark too. So Game Shark did it first. Yes.
[01:42:12.320 --> 01:42:15.840]   Then Nintendo did it. Then Game Shark... I heard the people suing Nintendo.
[01:42:16.720 --> 01:42:22.240]   That's nuts. I didn't know that. See? It's something called...
[01:42:22.240 --> 01:42:29.520]   There's a phrase for it. Reverse confusion. When a company comes along... I don't know.
[01:42:29.520 --> 01:42:35.520]   Let's say you had a podcast network called Twit. And a company comes along and names
[01:42:35.520 --> 01:42:41.040]   itself similarly and then becomes really big and successful. Then people just assume that you,
[01:42:41.040 --> 01:42:48.080]   Little Twit, copied the big success with comedy when in fact it was Game Shark that had the idea
[01:42:48.080 --> 01:42:56.000]   all along. Our show today... You like my beads? Our show today brought you... I got this little
[01:42:56.000 --> 01:43:02.160]   loo out and I got this when we checked in. Our show today brought to you by ExpressVPN. You
[01:43:02.160 --> 01:43:05.680]   better believe when I'm on the road. If I am on an App and Wi-Fi access point, you know, if we get
[01:43:05.680 --> 01:43:10.880]   to the hotel and they say, "Oh, hey, good news. Just go ahead and use the Wi-Fi." There's no password
[01:43:10.880 --> 01:43:17.760]   there's no anything. Just go ahead and use it. That's crazy. That means I'm on the same network as
[01:43:17.760 --> 01:43:23.040]   everybody else at that hotel and anybody who's sitting anywhere nearby because everybody can use
[01:43:23.040 --> 01:43:30.800]   it. That's when you fire up the VPN and you better believe I fired up ExpressVPN. ExpressVPN is a
[01:43:30.800 --> 01:43:38.400]   virtual private network that will protect you on public Wi-Fi. If you leave that internet connection
[01:43:38.400 --> 01:43:41.680]   unencrypted, you might as well be right in your password and your credit card numbers on a
[01:43:41.680 --> 01:43:47.280]   billboard for the rest of the world to see ExpressVPN protects you by encrypting all the traffic
[01:43:47.280 --> 01:43:52.400]   from right through that hotel's Wi-Fi all the way from your computer to the ExpressVPN servers.
[01:43:52.400 --> 01:43:58.960]   And by the way, servers all over the world 160 different locations. So you could choose a server
[01:43:58.960 --> 01:44:04.960]   near you or you could choose a server that's in the country you want to emerge in. If you're in,
[01:44:04.960 --> 01:44:09.920]   you know, a foreign landing you want to use your Netflix, no problem. Just, you know, use the
[01:44:09.920 --> 01:44:15.200]   ExpressVPN server in the US and you're there. You're secured, you're anonymized, your data is
[01:44:15.200 --> 01:44:21.520]   encrypted and very important when you're looking for a VPN ExpressVPN does no logging. They are
[01:44:21.520 --> 01:44:26.560]   not tracking you. So if you're worried that you're being tracked by your ISP or your carrier,
[01:44:26.560 --> 01:44:30.800]   if you're worried that you're being snooped upon, if you're worried that you're being attacked,
[01:44:30.800 --> 01:44:37.600]   get ExpressVPN. They have simple apps that run on any iPhone, Android phone, Mac or PC,
[01:44:37.600 --> 01:44:42.800]   one button click and you're protected and safe. And it's less than $7 a month
[01:44:42.800 --> 01:44:48.240]   for the number one VPN service, according to tech radar and a 30 day money back guarantee. So
[01:44:48.240 --> 01:44:52.240]   protect your online activity today. Find out how you can get three extra months free with a one-year
[01:44:52.240 --> 01:45:02.160]   package at ExpressVPN.com/twit express e X P R E S S express VPN.com/twit. Don't ask me
[01:45:02.160 --> 01:45:07.840]   on this bell VPN ExpressVPN.com/twit three extra months with a one-year subscription.
[01:45:07.840 --> 01:45:13.760]   We thank them for their support. We thank you for supporting them and thereby supporting us.
[01:45:15.760 --> 01:45:24.240]   YouTube struggling a little bit because there's a lot of video on there that advertisers don't
[01:45:24.240 --> 01:45:31.520]   want to buy. And they're getting a lot of heat for a lot of weird things. So according to Bloomberg,
[01:45:31.520 --> 01:45:40.320]   they're now trying a new metric Bloomberg calls responsibility. Two new internal metrics at YouTube
[01:45:40.320 --> 01:45:44.400]   introduced over the last two years to figure out how well videos are performing, not how many
[01:45:44.400 --> 01:45:50.640]   subscribers, not how many views one tracks the total time people spend on YouTube,
[01:45:50.640 --> 01:45:56.560]   including comments they post and read, not just clips. The other is a measurement called quality
[01:45:56.560 --> 01:46:03.680]   watch time. The goal is anyway to spot content that achieves something more constructive than just
[01:46:03.680 --> 01:46:11.760]   keeping you watching. This is a sensible plan from YouTube's point of view. One of the problems
[01:46:11.760 --> 01:46:16.480]   people have with YouTube is the recommendation engine that slowly pushes you towards more and
[01:46:16.480 --> 01:46:21.840]   more extreme content in order to increase your watch time. This sounds like this is Google's
[01:46:21.840 --> 01:46:29.760]   attempt to get around that. Thoughts? Christina.
[01:46:29.760 --> 01:46:36.960]   It's not a bad option to go to, I guess. Did you ever consider being a YouTube star, Christina?
[01:46:37.680 --> 01:46:40.960]   If I were 10 years younger, totally. What does that have to do with it?
[01:46:40.960 --> 01:46:46.400]   I think I'm too old. I think I'm too old. But how old do you have to be? You're a kid. What do
[01:46:46.400 --> 01:46:53.120]   you have to be 12? I think you definitely do you like in your early 20s. Yeah, like I think
[01:46:53.120 --> 01:46:56.800]   you need to be in your early 20s. But should I know if I were 10 years younger, I think that
[01:46:56.800 --> 01:47:00.720]   would have been the direction I would have gone. That's why I can just never get my YouTube numbers
[01:47:00.720 --> 01:47:06.480]   up. We have a family friend that's grooming. Let me, it's not my family. It's my husband's
[01:47:06.480 --> 01:47:13.760]   family, knows people who are grooming their 11 or 12 year old to become good.
[01:47:13.760 --> 01:47:20.560]   No, no. It's like grooming your kid to be horrifically disappointed in life.
[01:47:20.560 --> 01:47:28.640]   For real. There's something analogous to what YouTube is currently doing and what the broadcast
[01:47:28.640 --> 01:47:38.080]   industry did. Just after TV started broadcasting. Again, I think this has to do with potential
[01:47:38.080 --> 01:47:44.320]   impending regulation. It's proving that there's some type of beneficial component to the content
[01:47:44.320 --> 01:47:52.320]   that's being aired. The original, when kids programming was first on, the industry went through
[01:47:52.320 --> 01:47:57.600]   that decades and decades ago. It strikes me as the same thing, though, that every tech company,
[01:47:57.600 --> 01:48:03.280]   Facebook's doing this too. Now, where they throw out ideas that they have no intention of,
[01:48:03.280 --> 01:48:06.880]   they're going to change anything, but they're so terrified that Congress is going to come down
[01:48:06.880 --> 01:48:16.240]   on them. It's basically hand waving, if you ask me. Yeah, I think so. I mean, listen, every time.
[01:48:16.240 --> 01:48:23.280]   So how are they going to, what's the work stream process? I'd love to know exactly how this is
[01:48:23.280 --> 01:48:29.120]   very vague. What is a quality? What is quality time? What does that even mean? How do you judge
[01:48:29.120 --> 01:48:34.160]   that? There's no... And who's right and who's defining it? And the problem really is, it's the
[01:48:34.160 --> 01:48:39.040]   same problem Facebook faces, which is if you... And it's algorithmically easy to do,
[01:48:39.040 --> 01:48:44.400]   you optimize for engagement. That's something a computer program can easily do. Optimize for
[01:48:44.400 --> 01:48:49.680]   engagement. You get... And Facebook experiences in the newsfeed and YouTube is experiencing it
[01:48:49.680 --> 01:48:57.280]   with a recommendation engine. And it just descends to the lowest... I guess TV went through this,
[01:48:57.280 --> 01:49:04.000]   too, didn't it? The lowest possible quality stuff. When I think of... So again, when I see,
[01:49:04.000 --> 01:49:10.560]   first of all, my daughter doesn't... How old is she? She's eight. And as far as she's concerned...
[01:49:10.560 --> 01:49:16.320]   So she's prime YouTube material. Right. Except that we built her her own network. So she thinks
[01:49:16.320 --> 01:49:20.240]   she has free and unfettered access to the internet. And we built her like a little Chinese internet
[01:49:20.240 --> 01:49:25.840]   inside of her house. Amazing. So she doesn't... Babies first Chinese internet. I like it.
[01:49:25.840 --> 01:49:31.840]   Yeah. No, she thinks she's... She's fallen. She's got it. Got all access and, you know,
[01:49:31.840 --> 01:49:36.720]   surprise. Now, how do you do this? You don't... Because clearly, like, things like parents thought,
[01:49:36.720 --> 01:49:40.560]   "Oh, YouTube kids, that'll be safe." And then they found out it's creepy as hell.
[01:49:41.280 --> 01:49:48.400]   Now, it's not... A lot of the content... My first window into what this world was like was
[01:49:48.400 --> 01:49:54.560]   were unboxing videos. And I could not for the life of me figure out why... Because I watched...
[01:49:54.560 --> 01:49:59.280]   She... That's what she wanted to watch a couple of years ago. And I was just like, "What?" I don't
[01:49:59.280 --> 01:50:03.040]   understand this. And then I don't know if it was in the... I think it was in the New Yorkers.
[01:50:03.040 --> 01:50:07.760]   There was a doctor who had written an explanation of why these unboxing videos were so attractive to
[01:50:07.760 --> 01:50:15.760]   little kids. And it's because at very early ages, their sensory systems and emotional maturity
[01:50:15.760 --> 01:50:22.240]   are just starting to tap into the element of surprise and delight. But without the negative
[01:50:22.240 --> 01:50:28.000]   repercussions. So it's like stimulating the part of the brain that's just starting to develop. And
[01:50:31.120 --> 01:50:37.840]   stupid... What's the name of that stupid doll thing? They come in these big...
[01:50:37.840 --> 01:50:41.200]   Which one? There's quite a few of them. Yeah, yeah, yeah. The Kinder Eggs?
[01:50:41.200 --> 01:50:42.960]   You speak... No, no, no, they're not Kinder Eggs.
[01:50:42.960 --> 01:50:46.400]   But there's all these YouTube unwrapping videos of these stupid...
[01:50:46.400 --> 01:50:54.160]   These toys were reverse engineered. They were a toy version of an unboxing video.
[01:50:54.160 --> 01:50:58.480]   And there's like 100 different things to the animals.
[01:50:58.480 --> 01:51:01.280]   That's not a Hatchimal. Now it's not a Hatchimal.
[01:51:01.280 --> 01:51:04.000]   The hell are these stupid things? That's a horrible name, whatever that is.
[01:51:04.000 --> 01:51:10.000]   At any rate... They're Hatchimal collectable.
[01:51:10.000 --> 01:51:12.800]   Well, if they're not Hatchimals or something else that are obviously...
[01:51:12.800 --> 01:51:21.600]   So the Great Firewall of Amy, how does this protect her against Hatchimals?
[01:51:21.600 --> 01:51:29.360]   So originally we were like, YouTube kids, I guess I was drinking the Kool-Aid along with
[01:51:29.360 --> 01:51:37.520]   everybody else. And then I realized some of it is like this kind of programming, which is
[01:51:37.520 --> 01:51:41.200]   stimulating in a way. There's also... Yeah, there's plenty of finding...
[01:51:41.200 --> 01:51:45.600]   Are you ready for a super... Sorry, I don't know why my volume is so high.
[01:51:45.600 --> 01:51:50.720]   I'm watching a little Hatchimals. Yeah, that's actually not it. This is something worse.
[01:51:50.720 --> 01:51:54.320]   Even worse? Yeah, yeah. It's the Hatchimals unboxing.
[01:51:54.320 --> 01:51:57.120]   What is it? What do they call them?
[01:51:57.120 --> 01:52:04.400]   L-O-L surprise dolls. They are expensive. They're horrible.
[01:52:04.400 --> 01:52:08.480]   But that is what was reverse engine. They're brilliant. That's what not reverse engine.
[01:52:08.480 --> 01:52:09.600]   Oh, I'm looking at these.
[01:52:09.600 --> 01:52:13.120]   Both Kristi and I are quickly googling L-O-L surprise dolls.
[01:52:13.120 --> 01:52:18.880]   So I let her watch YouTube at the beginning and then I saw what it was.
[01:52:18.880 --> 01:52:25.520]   And not all of it's bad. Some of it's bad. Some of it, I think, must have been what me watching
[01:52:25.520 --> 01:52:29.360]   Bill Nye the Science Guy was probably like to my parents. Well, that's the problem.
[01:52:29.360 --> 01:52:33.360]   How do you judge, right? That's the point. Bill Nye the Science Guy was... Obviously,
[01:52:33.360 --> 01:52:38.560]   that was educational. That was amazing. But for somebody who was older, it was a lot of jarring
[01:52:38.560 --> 01:52:45.120]   quick cuts and loud music and big graphics, which I'm sure a lot of people railed against them
[01:52:45.120 --> 01:52:49.680]   and older people and thought, well, this is horrible. Much like today, we are railing against some
[01:52:49.680 --> 01:52:53.920]   of what we're now seeing on YouTube was the point that I was trying to get to 20 minutes ago.
[01:52:53.920 --> 01:52:57.120]   Anyhow, we don't use all of it.
[01:52:57.120 --> 01:53:02.000]   Yeah, or actually, it's not Bill Nye, because that's actually educational. This would be more
[01:53:02.000 --> 01:53:05.120]   like Jim and the various...
[01:53:05.120 --> 01:53:08.080]   True their cartoon.
[01:53:08.080 --> 01:53:13.440]   Like, Gaijou, the things that they created, like the animated show Jim was literally
[01:53:13.440 --> 01:53:17.520]   created to sell a toy brand, right? Like they created these Saturday morning cartoons with
[01:53:17.520 --> 01:53:21.680]   the express purpose of selling toys. So they did the same type of thing where they're like,
[01:53:21.680 --> 01:53:27.760]   okay, we're going to create this TV show that its whole purpose is to launch a toy line,
[01:53:27.760 --> 01:53:32.640]   which I don't know if LOL, I mean, LOL surprise dolls is kind of cool, although it's clearly...
[01:53:32.640 --> 01:53:35.520]   It even says on the doll unbox me.
[01:53:35.520 --> 01:53:41.520]   So the whole point of it is, right, so if you watch anymore of it, it is an unboxing video
[01:53:41.520 --> 01:53:46.560]   that is just made out of plastic. So the problem that I have with these is that once it is an
[01:53:46.560 --> 01:53:48.480]   activity, so once it's...
[01:53:48.480 --> 01:53:52.160]   And it does encourage kids to create their own YouTube channel? I mean, is that...
[01:53:52.160 --> 01:53:58.720]   No, my daughter doesn't want to do that, but once you've unwrapped, there is no toy,
[01:53:58.720 --> 01:54:03.440]   so that the thing that you do was unbox it, you unwrap it in all these different places.
[01:54:03.440 --> 01:54:05.120]   And then you're not going to...
[01:54:05.120 --> 01:54:05.840]   That's it.
[01:54:05.840 --> 01:54:06.720]   They're a little tiny.
[01:54:06.720 --> 01:54:07.200]   You're done.
[01:54:07.200 --> 01:54:08.000]   You're not going to play with them.
[01:54:08.000 --> 01:54:12.000]   So all we're doing is generating a lot of waste that winds up getting thrown out.
[01:54:12.000 --> 01:54:17.280]   It's a ball full of plastic crap that you're going to throw out later.
[01:54:17.280 --> 01:54:20.720]   That's right. And they're selling an activity, right?
[01:54:20.720 --> 01:54:26.000]   And these, by the way, are really expensive, but this is somebody who reverse engineered a
[01:54:26.000 --> 01:54:33.200]   commercial, leave viable internet meme, which are unboxing videos and has done quite well.
[01:54:33.200 --> 01:54:35.360]   There's the gauntlets or the boots or something.
[01:54:35.360 --> 01:54:36.960]   Yeah. These are like...
[01:54:36.960 --> 01:54:40.160]   These little balls that you're seeing get unwrapped.
[01:54:40.160 --> 01:54:41.680]   Those things are like 50 bucks.
[01:54:41.680 --> 01:54:42.560]   50 bucks?
[01:54:42.560 --> 01:54:46.160]   Some of them, the larger that they are, the more expensive.
[01:54:46.160 --> 01:54:47.360]   The bigger balls cost more.
[01:54:47.360 --> 01:54:48.000]   Expensive.
[01:54:48.000 --> 01:54:48.240]   Yeah.
[01:54:48.240 --> 01:54:51.040]   Okay, but you get a doll.
[01:54:51.040 --> 01:54:52.560]   See? And then it dresses up.
[01:54:52.560 --> 01:54:54.400]   From $50.
[01:54:54.400 --> 01:54:57.360]   Like...
[01:54:57.360 --> 01:54:59.920]   Okay.
[01:54:59.920 --> 01:55:00.080]   No.
[01:55:00.080 --> 01:55:02.800]   So this is not clearly the science guy.
[01:55:03.840 --> 01:55:05.360]   No, but my point was like...
[01:55:05.360 --> 01:55:06.560]   But we can't judge.
[01:55:06.560 --> 01:55:07.760]   We're a different generation.
[01:55:07.760 --> 01:55:09.200]   I think so.
[01:55:09.200 --> 01:55:15.120]   And the problem is your daughter in 15 years is not going to be able to talk to her peers about
[01:55:15.120 --> 01:55:15.680]   LOL.
[01:55:15.680 --> 01:55:18.080]   Surprise me, dolls.
[01:55:18.080 --> 01:55:23.760]   There is plenty of, I think, content on YouTube created by younger kids that is probably
[01:55:23.760 --> 01:55:26.720]   educational, nowhere near what Bill Nye was...
[01:55:26.720 --> 01:55:28.640]   Like, they're not even...
[01:55:28.640 --> 01:55:31.040]   There is good science video on YouTube that kids watch.
[01:55:31.040 --> 01:55:32.160]   It's not a fair comparison.
[01:55:32.160 --> 01:55:32.320]   Yeah.
[01:55:32.320 --> 01:55:38.800]   Right. So the question is, isn't there a value in watching a cartoon that is about...
[01:55:38.800 --> 01:55:41.360]   Like the reboot of She-Ra is amazing.
[01:55:41.360 --> 01:55:42.400]   Now that's not on YouTube.
[01:55:42.400 --> 01:55:44.400]   That was on Netflix, but it's like...
[01:55:44.400 --> 01:55:45.120]   So do you have...
[01:55:45.120 --> 01:55:47.360]   So I still want to know about the Great Firewall of Amy.
[01:55:47.360 --> 01:55:52.880]   So do you have a whitelist of things she can look at or how does this work?
[01:55:52.880 --> 01:55:53.200]   Do you...
[01:55:53.200 --> 01:55:53.680]   I have a...
[01:55:53.680 --> 01:55:54.480]   I'm very curious about this as well.
[01:55:54.480 --> 01:55:56.560]   You have a Plex server where you stock it.
[01:55:56.560 --> 01:55:57.520]   We also have that.
[01:55:57.520 --> 01:55:59.440]   Family-approved videos?
[01:56:00.080 --> 01:56:01.840]   So we... Yes. So we have...
[01:56:01.840 --> 01:56:06.400]   How much do I invite the outside world?
[01:56:06.400 --> 01:56:09.760]   Well, just the general gist of somebody can duplicate it.
[01:56:09.760 --> 01:56:10.320]   Yes. So it's just is...
[01:56:10.320 --> 01:56:14.560]   We have multiple networks that serve different purposes that are all behind
[01:56:14.560 --> 01:56:17.120]   different types of firewalls themselves.
[01:56:17.120 --> 01:56:18.320]   You are such a geek.
[01:56:18.320 --> 01:56:20.400]   Her network...
[01:56:20.400 --> 01:56:24.000]   The joke in our family is that my husband is hardware and I'm software,
[01:56:24.000 --> 01:56:25.440]   except when...
[01:56:25.440 --> 01:56:27.600]   It's a whole other thing.
[01:56:28.480 --> 01:56:32.320]   So you have VLANs in your house and each...
[01:56:32.320 --> 01:56:33.200]   We ran...
[01:56:33.200 --> 01:56:36.160]   We bought a hundred year old house and the first thing we did was run
[01:56:36.160 --> 01:56:39.200]   conduit everywhere so that we could run our own tables.
[01:56:39.200 --> 01:56:42.880]   So we have a closet like a wiring.
[01:56:42.880 --> 01:56:45.120]   We have a beautiful rack in the basement.
[01:56:45.120 --> 01:56:46.080]   It's gorgeous.
[01:56:46.080 --> 01:56:46.480]   Amazing.
[01:56:46.480 --> 01:56:47.280]   It's very exciting.
[01:56:47.280 --> 01:56:50.800]   The Great Firewall of Amy has a beautiful rack in the basement.
[01:56:50.800 --> 01:56:54.160]   No, I'm pretty happy by this because...
[01:56:54.160 --> 01:56:54.640]   I'm making...
[01:56:54.640 --> 01:56:57.280]   The title's getting longer and longer and longer.
[01:56:57.280 --> 01:56:59.840]   No, I'm not a parent and I'm not going to be one,
[01:56:59.840 --> 01:57:02.480]   but this seems like this is the right way if you're going to do it.
[01:57:02.480 --> 01:57:04.080]   No, this is brilliant.
[01:57:04.080 --> 01:57:04.560]   I always thought of myself like...
[01:57:04.560 --> 01:57:05.200]   What's the right way?
[01:57:05.200 --> 01:57:06.080]   There is a risk.
[01:57:06.080 --> 01:57:08.160]   No, I have to point out there is a risk
[01:57:08.160 --> 01:57:10.960]   because the worldview is created by mom and dad
[01:57:10.960 --> 01:57:13.040]   and it is a limited worldview.
[01:57:13.040 --> 01:57:14.240]   Now I understand she's young enough.
[01:57:14.240 --> 01:57:16.400]   That's probably okay and it's normal.
[01:57:16.400 --> 01:57:19.600]   But I also know kids who grew up and the only thing they could watch
[01:57:19.600 --> 01:57:25.120]   was Bible verse cartoons.
[01:57:25.120 --> 01:57:26.240]   No, no, no, it's not like that.
[01:57:26.240 --> 01:57:27.120]   It's more...
[01:57:27.120 --> 01:57:28.400]   We want to give her...
[01:57:28.400 --> 01:57:32.080]   So yes, we've basically turned the internet off
[01:57:32.080 --> 01:57:35.200]   except for white labeled places where she can go.
[01:57:35.200 --> 01:57:36.160]   I think that's great.
[01:57:36.160 --> 01:57:38.400]   There is no YouTube kids.
[01:57:38.400 --> 01:57:40.160]   At what age will you stop this though?
[01:57:40.160 --> 01:57:43.040]   I don't know, is she matures?
[01:57:43.040 --> 01:57:45.200]   She's an only child.
[01:57:45.200 --> 01:57:46.000]   She's a fairly...
[01:57:46.000 --> 01:57:47.200]   We're old parents.
[01:57:47.200 --> 01:57:49.440]   She's a fairly mature eight-year-old, but we'll see.
[01:57:49.440 --> 01:57:53.680]   She reminds me a lot of what Stacey Higginbotham does with her daughter, by the way.
[01:57:53.680 --> 01:57:55.360]   Yeah, so Stacey and I know each other.
[01:57:55.360 --> 01:57:57.680]   I mean, I think that there is a...
[01:57:57.680 --> 01:57:58.960]   There are parents who are sort of...
[01:57:58.960 --> 01:58:02.800]   I don't believe in no screen time.
[01:58:02.800 --> 01:58:04.640]   I believe in smart screen time.
[01:58:04.640 --> 01:58:05.920]   Ultimate screen time.
[01:58:05.920 --> 01:58:06.400]   That's right.
[01:58:06.400 --> 01:58:09.280]   She has access to all of the robots and all of the device.
[01:58:09.280 --> 01:58:09.920]   We got all kinds of...
[01:58:09.920 --> 01:58:12.000]   We got a house full of stuff that she can access,
[01:58:12.000 --> 01:58:14.880]   but she's only accessing it on her network.
[01:58:14.880 --> 01:58:16.720]   Do you teach me coding?
[01:58:16.720 --> 01:58:19.520]   Now they do some of that at school,
[01:58:19.520 --> 01:58:20.640]   but I'll tell you what they do at school.
[01:58:20.640 --> 01:58:21.280]   That's amazing.
[01:58:21.280 --> 01:58:25.120]   They have digital literacy classes
[01:58:25.120 --> 01:58:27.920]   where once a week they have to learn...
[01:58:27.920 --> 01:58:31.920]   Bobby, put a photo without asking Jane.
[01:58:31.920 --> 01:58:33.600]   Oh, that's really good.
[01:58:33.600 --> 01:58:35.360]   That's really good.
[01:58:35.360 --> 01:58:37.360]   It's part of their values class,
[01:58:37.360 --> 01:58:41.200]   is digital values, which is amazing, but not every school does that.
[01:58:41.200 --> 01:58:43.040]   No, but see what I like about this,
[01:58:43.040 --> 01:58:46.400]   because see, my whole thing, most of the parents that I've talked to
[01:58:46.400 --> 01:58:49.040]   who try to do similar things, they don't go to the extreme you have,
[01:58:49.040 --> 01:58:53.520]   but this is smart because I always think about what if it were me at that age,
[01:58:53.520 --> 01:58:58.320]   and if it were me at that age, I would figure out the mom and dad were censoring the internet,
[01:58:58.320 --> 01:59:00.800]   and I would find out how to override the system.
[01:59:00.800 --> 01:59:01.680]   But this is pretty...
[01:59:01.680 --> 01:59:02.320]   She knows.
[01:59:02.320 --> 01:59:03.440]   This would take...
[01:59:03.440 --> 01:59:04.320]   Right, no, I know.
[01:59:04.320 --> 01:59:05.440]   We're not finding a grammar.
[01:59:05.440 --> 01:59:06.160]   She's totally knows.
[01:59:06.160 --> 01:59:07.120]   I'm not saying you are.
[01:59:07.120 --> 01:59:10.160]   What I'm saying though is that I would know,
[01:59:10.160 --> 01:59:13.760]   and then I would be like, "Okay, well, then what do I need to do to take control of the router?"
[01:59:13.760 --> 01:59:14.480]   Exactly.
[01:59:14.480 --> 01:59:15.200]   That's the whole thing.
[01:59:15.200 --> 01:59:18.640]   Which at that point, I would like to think that if I were a parent,
[01:59:18.640 --> 01:59:20.320]   and again, I'm not one, and I'm not going to be one,
[01:59:20.320 --> 01:59:23.040]   but if I were, I would like to think that if my child did
[01:59:23.040 --> 01:59:25.680]   do that successfully, it'd be like, "Okay, you know what?
[01:59:25.680 --> 01:59:27.200]   Congratulations. You deserve this."
[01:59:27.200 --> 01:59:30.560]   Yeah, you have to earn your way out of the tech prison.
[01:59:30.560 --> 01:59:31.040]   Right.
[01:59:31.040 --> 01:59:31.360]   Yeah.
[01:59:31.360 --> 01:59:35.200]   But this is a level above and beyond what the...
[01:59:35.200 --> 01:59:36.560]   Most people can't do this.
[01:59:36.560 --> 01:59:40.000]   What they do is they go out and buy the Disney Circle thing,
[01:59:40.000 --> 01:59:40.960]   and they let Disney disagree.
[01:59:40.960 --> 01:59:41.920]   Exactly.
[01:59:41.920 --> 01:59:44.080]   This is the point that I've been trying to make all night.
[01:59:44.080 --> 01:59:45.120]   It's so going to be easy to get over.
[01:59:45.120 --> 01:59:45.360]   Yeah.
[01:59:45.360 --> 01:59:46.400]   Right.
[01:59:46.400 --> 01:59:51.520]   So the problem is that my husband's best friend is a white hat.
[01:59:52.960 --> 01:59:55.520]   You know, we run in very technical circles,
[01:59:55.520 --> 01:59:59.600]   and so we're all on key base sharing ideas, parent...
[01:59:59.600 --> 02:00:05.280]   You know, like, we're in a different kind of situation than most people.
[02:00:05.280 --> 02:00:09.520]   And so, yeah, we all share that we've got a Plex server that everybody shares,
[02:00:09.520 --> 02:00:12.240]   and there is a kid approved, if you're seven years old,
[02:00:12.240 --> 02:00:13.520]   this is the stuff you get to watch.
[02:00:13.520 --> 02:00:19.920]   The problem is that I'm in a verified group of people,
[02:00:19.920 --> 02:00:22.080]   and I have all this additional knowledge.
[02:00:22.080 --> 02:00:24.160]   And my point is, like,
[02:00:24.160 --> 02:00:28.320]   I feel like everybody else should have access to at least the knowledge
[02:00:28.320 --> 02:00:31.920]   so that they can then make decisions, or they can afford...
[02:00:31.920 --> 02:00:35.280]   We can make it so that people can afford to buy in.
[02:00:35.280 --> 02:00:40.240]   Again, this goes back to the comments somebody made about privacy and taxes and wealth.
[02:00:40.240 --> 02:00:43.040]   You know, if you don't know this stuff,
[02:00:43.040 --> 02:00:46.320]   or if you can't afford to make alternate choices or build out
[02:00:46.320 --> 02:00:48.560]   and provision like six different networks in your home,
[02:00:49.200 --> 02:00:52.080]   right, because you can barely afford the one network that you've got access to
[02:00:52.080 --> 02:00:54.320]   and the standard modem and router situation,
[02:00:54.320 --> 02:01:02.720]   then you're kind of left open, and that impacts the future generation of people
[02:01:02.720 --> 02:01:03.440]   who grow up.
[02:01:03.440 --> 02:01:11.600]   This is interesting, because this is the opposite of the privacy tax for poor people.
[02:01:11.600 --> 02:01:13.520]   This is...
[02:01:13.520 --> 02:01:18.000]   There is a technologically savvy group of adults
[02:01:18.880 --> 02:01:23.360]   who are creating this safe space for a very small...
[02:01:23.360 --> 02:01:24.960]   You have to write a book, Amy.
[02:01:24.960 --> 02:01:25.840]   And I...
[02:01:25.840 --> 02:01:28.320]   Well, but this is the part of the great firewall of Amy.
[02:01:28.320 --> 02:01:29.680]   Well, but this is the part of the city is thinking this could be productized,
[02:01:29.680 --> 02:01:30.640]   like honestly.
[02:01:30.640 --> 02:01:33.120]   But this could be something that you can't care with that thing.
[02:01:33.120 --> 02:01:33.520]   The problem is productizes the problem,
[02:01:33.520 --> 02:01:36.160]   because Disney Circle is going to have a corporate bias.
[02:01:36.160 --> 02:01:39.680]   I mean, I bet you anything, the new Disney streaming service
[02:01:39.680 --> 02:01:42.240]   will be part of the Disney Circle, you know, right?
[02:01:42.240 --> 02:01:43.600]   That's what I'm supposed to be talking about.
[02:01:43.600 --> 02:01:44.480]   I'm just going to be like Disney, I mean, like,
[02:01:44.480 --> 02:01:46.080]   Synology or someone, you know what I mean?
[02:01:46.080 --> 02:01:46.560]   Somebody.
[02:01:46.560 --> 02:01:52.960]   There's a way to do this if you were like a server company or like amplify or somebody.
[02:01:52.960 --> 02:01:58.480]   It seems to me this is the growing gulf between the haves and have-nots,
[02:01:58.480 --> 02:02:00.400]   except it's the technically literate and the technically literate.
[02:02:00.400 --> 02:02:03.600]   Right. And this is the problem that I have with all the people in the valley
[02:02:03.600 --> 02:02:09.680]   who are now like the new autism, the new like I'm on the spectrum is my kids get no screens.
[02:02:09.680 --> 02:02:11.600]   And they don't use any of the products that I work on.
[02:02:12.800 --> 02:02:18.880]   And the problem that I have with that is that they're either being dumb or they're just trying to like,
[02:02:18.880 --> 02:02:21.200]   that's a stupid, that doesn't make any sense.
[02:02:21.200 --> 02:02:25.760]   You know, like, you can't live in a world that doesn't have technology.
[02:02:25.760 --> 02:02:27.520]   You're obviously tech savvy people.
[02:02:27.520 --> 02:02:30.800]   Why would you just sort of go in this weird alternate?
[02:02:30.800 --> 02:02:36.480]   It doesn't, it's totally disingenuous or it's like intentionally being kind of dumb, you know?
[02:02:36.480 --> 02:02:40.960]   Well, I think a lot of it is the same because the same people who used to like
[02:02:40.960 --> 02:02:43.840]   when, you know, I was growing up and when you're growing up with parents who would be like,
[02:02:43.840 --> 02:02:48.400]   oh, well, my child doesn't watch TV. We don't have a TV in our house, which, you know,
[02:02:48.400 --> 02:02:53.040]   it was just like, okay, good for you, I guess. But I, it seems a little-
[02:02:53.040 --> 02:02:57.200]   I can't even imagine Christina Warren not being able to watch TV as a teenager.
[02:02:57.200 --> 02:03:01.680]   No, in fact, my parents, no, it was so funny is that my parents like limited what I could
[02:03:01.680 --> 02:03:03.680]   watch to a certain age, you know, and then-
[02:03:03.680 --> 02:03:06.000]   Me too. I got half an hour at night.
[02:03:06.000 --> 02:03:08.880]   And then I would get in trouble and I would negotiate-
[02:03:08.880 --> 02:03:09.360]   Right.
[02:03:09.360 --> 02:03:15.520]   With them. And I would say, okay, so you put me on restriction, fine. Melrose Place is on.
[02:03:15.520 --> 02:03:20.320]   And I can't miss Melrose Place. So I'll take two days if I can watch this-
[02:03:20.320 --> 02:03:21.440]   It's a pretty good-
[02:03:21.440 --> 02:03:21.440]   Monday, night, and day at all.
[02:03:21.440 --> 02:03:25.040]   Gladly trade you Tuesday for a Melrose Place tonight.
[02:03:25.040 --> 02:03:26.160]   Those are still-
[02:03:26.160 --> 02:03:26.720]   Right. And it works.
[02:03:26.720 --> 02:03:28.880]   Skills that you developed in the process.
[02:03:28.880 --> 02:03:29.440]   Right.
[02:03:29.440 --> 02:03:29.600]   With other like-
[02:03:29.600 --> 02:03:30.400]   Negotiation.
[02:03:30.400 --> 02:03:36.720]   I mean, I'll tell you, when I left, so the first time I was basically gone, I missed the
[02:03:36.720 --> 02:03:41.120]   Clinton hearings. I missed like the mid-90s through the mid-2000s.
[02:03:41.120 --> 02:03:45.440]   I missed all that pop culture. And it was like an alien when I came back.
[02:03:45.440 --> 02:03:46.960]   And there's 10 years of my-
[02:03:46.960 --> 02:03:51.520]   You know, there's a big chunk of my life where I can't follow along a conversation
[02:03:51.520 --> 02:03:52.000]   once people-
[02:03:52.000 --> 02:03:55.760]   Like I'm not part of that collective culture in my own country.
[02:03:55.760 --> 02:03:56.400]   Is that to savannah?
[02:03:56.400 --> 02:03:56.880]   Is that to savannah?
[02:03:56.880 --> 02:03:59.120]   I mean, it isn't-
[02:03:59.120 --> 02:04:01.520]   Yeah, I mean, it is in some ways. I miss some-
[02:04:01.520 --> 02:04:05.680]   I miss some cues. I mean, obviously I can follow along.
[02:04:05.680 --> 02:04:07.920]   But to me, this is why you don't-
[02:04:07.920 --> 02:04:14.320]   You can rage against the machine without cutting yourself off of the lifeblood.
[02:04:14.320 --> 02:04:16.800]   That is the culture of the place where you're living.
[02:04:16.800 --> 02:04:17.840]   Right?
[02:04:17.840 --> 02:04:19.360]   That's good. I like it.
[02:04:19.360 --> 02:04:20.640]   So again, it's like this-
[02:04:20.640 --> 02:04:21.200]   But we don't-
[02:04:21.200 --> 02:04:24.560]   Somehow we've lost our ability to be flexible and how we think about stuff.
[02:04:24.560 --> 02:04:25.680]   It's either all or nothing.
[02:04:25.680 --> 02:04:28.560]   And then there's plenty of parents that-
[02:04:28.560 --> 02:04:31.360]   You know, we go out to dinner and they plop their kids down with a-
[02:04:31.360 --> 02:04:34.960]   And I'm sorry if people listening to this are a family like this,
[02:04:34.960 --> 02:04:35.760]   but-
[02:04:35.760 --> 02:04:37.280]   Almost everybody is. I should just-
[02:04:37.280 --> 02:04:39.120]   They plop their kids down with a-
[02:04:39.120 --> 02:04:41.440]   With a screen and that's how they do their dinner.
[02:04:41.440 --> 02:04:44.000]   I never judge parents because it's such a hard job.
[02:04:44.000 --> 02:04:45.280]   Yeah, no, and you shouldn't.
[02:04:45.280 --> 02:04:47.040]   And that's an excellent way to get trolled.
[02:04:47.040 --> 02:04:49.360]   So I'm not judging. I'm just stating.
[02:04:49.360 --> 02:04:51.280]   And there are parents who do completely the opposite,
[02:04:51.280 --> 02:04:53.920]   which is nobody gets any screen time ever.
[02:04:53.920 --> 02:04:58.560]   I just give my kid a five pound bag of sugar-free gummy bears.
[02:04:58.560 --> 02:05:02.160]   And I said, "Kid, enjoy."
[02:05:04.240 --> 02:05:05.920]   Watch as much TV as you can.
[02:05:05.920 --> 02:05:08.960]   And I'll see you later.
[02:05:08.960 --> 02:05:12.320]   I showed today, brought to you by Wordpress.
[02:05:12.320 --> 02:05:13.200]   I'll tell you one thing-
[02:05:13.200 --> 02:05:17.200]   And I think this is true of your daughter, not yet.
[02:05:17.200 --> 02:05:17.600]   But there-
[02:05:17.600 --> 02:05:21.520]   At some point, the other thing you have to decide with kids is-
[02:05:21.520 --> 02:05:23.200]   Is their presence on the internet?
[02:05:23.200 --> 02:05:24.960]   Because you can't keep a kid off-
[02:05:24.960 --> 02:05:28.320]   Or anybody off the internet because that vacuum will be filled.
[02:05:28.320 --> 02:05:31.600]   It'll just be filled by other people's posts about your child,
[02:05:31.600 --> 02:05:33.520]   videos about your child, name calling,
[02:05:33.520 --> 02:05:34.720]   and all sorts of stuff.
[02:05:34.720 --> 02:05:35.920]   So I always tell teenagers,
[02:05:35.920 --> 02:05:37.840]   and I think it's right about seventh or eighth grade,
[02:05:37.840 --> 02:05:39.600]   get a website,
[02:05:39.600 --> 02:05:41.360]   start putting your best stuff on it.
[02:05:41.360 --> 02:05:44.640]   This is going to be you when people Google your name.
[02:05:44.640 --> 02:05:46.640]   So this is something you need to do.
[02:05:46.640 --> 02:05:48.000]   And it's true for individuals.
[02:05:48.000 --> 02:05:49.520]   It's true for businesses.
[02:05:49.520 --> 02:05:50.880]   You need to have a website.
[02:05:50.880 --> 02:05:53.120]   And that's why I always tell people about Wordpress.com
[02:05:53.120 --> 02:05:54.080]   because it's the easiest,
[02:05:54.080 --> 02:05:57.440]   free way to create your website.
[02:05:57.440 --> 02:06:00.640]   Right now, that will become who you are.
[02:06:00.640 --> 02:06:02.720]   In fact, my suggestion is,
[02:06:02.720 --> 02:06:05.440]   if you're having a kid get their domain name now,
[02:06:05.440 --> 02:06:06.080]   register it.
[02:06:06.080 --> 02:06:07.360]   I did for my kids.
[02:06:07.360 --> 02:06:09.760]   Got a 20 year domain name registered.
[02:06:09.760 --> 02:06:11.520]   I figured by then they'd know if they wanted it.
[02:06:11.520 --> 02:06:14.480]   Get their email account.
[02:06:14.480 --> 02:06:16.560]   Start this process right now.
[02:06:16.560 --> 02:06:18.640]   The minute Matt Mullenweg announced it,
[02:06:18.640 --> 02:06:20.240]   I started using it for my blog,
[02:06:20.240 --> 02:06:21.680]   and I've used it ever since.
[02:06:21.680 --> 02:06:23.440]   I've been on Wordpress.com for 12 years
[02:06:23.440 --> 02:06:25.280]   because let them do the hard work.
[02:06:25.280 --> 02:06:27.440]   Let them do the hosting, the tech support.
[02:06:27.440 --> 02:06:31.600]   You know, you probably heard about some Wordpress security flaws.
[02:06:31.600 --> 02:06:34.800]   That somebody was just releasing into the wild this week
[02:06:34.800 --> 02:06:35.840]   without any warning.
[02:06:35.840 --> 02:06:39.040]   Hey, don't worry, Wordpress.com takes care of that stuff.
[02:06:39.040 --> 02:06:41.840]   So you don't have to constantly follow all the security bones
[02:06:41.840 --> 02:06:42.880]   and make sure you're safe.
[02:06:42.880 --> 02:06:43.680]   You are safe.
[02:06:43.680 --> 02:06:45.280]   You're on Wordpress.com.
[02:06:45.280 --> 02:06:48.160]   Wordpress.com was started by Matt many years ago
[02:06:48.160 --> 02:06:50.400]   so that anyone could publish their ideas,
[02:06:50.400 --> 02:06:52.720]   have their voice on the web,
[02:06:52.720 --> 02:06:54.720]   put a portfolio up, open a store,
[02:06:54.720 --> 02:06:56.560]   start a blog,
[02:06:56.560 --> 02:06:58.480]   let people know about your business.
[02:06:58.480 --> 02:07:00.720]   And it's a site that's free to start,
[02:07:00.720 --> 02:07:04.080]   but can grow with you all the way up to an e-commerce site
[02:07:04.080 --> 02:07:05.920]   and some of the biggest publications in the world.
[02:07:05.920 --> 02:07:08.160]   They're using Wordpress.com.
[02:07:08.160 --> 02:07:10.720]   No two-week trials, no hidden fees.
[02:07:10.720 --> 02:07:14.400]   And most importantly, you own your content forever.
[02:07:14.400 --> 02:07:16.800]   Sure, it's fine for your kid to have a Facebook page
[02:07:16.800 --> 02:07:18.800]   or an Instagram site or a Snapchat, whatever.
[02:07:18.800 --> 02:07:21.680]   But it's so important for you as an individual
[02:07:21.680 --> 02:07:24.400]   and for every business to have one place that's yours,
[02:07:24.400 --> 02:07:25.840]   that you own forever.
[02:07:25.840 --> 02:07:29.600]   Upload anything you want, text, pictures, video, audio,
[02:07:29.600 --> 02:07:31.040]   download it again and get it out.
[02:07:31.040 --> 02:07:31.920]   It's never trapped.
[02:07:31.920 --> 02:07:34.880]   And a great customer support team,
[02:07:34.880 --> 02:07:36.720]   they're not just page-turners in a notebook,
[02:07:36.720 --> 02:07:40.320]   they're actually Wordpress lovers and users and experts.
[02:07:40.320 --> 02:07:42.880]   And they're there 24 hours a day to help you, even weekends.
[02:07:42.880 --> 02:07:46.480]   The Wordpress platform is powerful and flexible.
[02:07:46.480 --> 02:07:48.880]   It can grow as big as you need it to be,
[02:07:48.880 --> 02:07:50.640]   but it can start as small as you want.
[02:07:50.640 --> 02:07:52.960]   Millions of people use Wordpress.com
[02:07:52.960 --> 02:07:54.560]   every day to turn their dreams into reality.
[02:07:54.560 --> 02:07:56.560]   In fact, the number that blows me away
[02:07:56.560 --> 02:08:02.480]   every time I read it, 33% of the entire internet is powered by Wordpress.
[02:08:02.480 --> 02:08:06.320]   One third of all the internet runs on the Wordpress software.
[02:08:06.320 --> 02:08:09.120]   The best way to do it, Wordpress.com.
[02:08:09.120 --> 02:08:10.720]   Go to Wordpress.com/twit,
[02:08:10.720 --> 02:08:13.360]   you'll get 15% off any new plan purchase right now.
[02:08:13.360 --> 02:08:18.000]   Wordpress.com/twit, 15% off any new plan purchase,
[02:08:18.000 --> 02:08:20.560]   Wordpress.com/twit.
[02:08:20.560 --> 02:08:23.600]   Please go there so that they know you heard it here.
[02:08:23.600 --> 02:08:25.440]   That helps us and we thank you, Wordpress,
[02:08:25.440 --> 02:08:29.760]   for me personally, 12 years of leoloport.com.
[02:08:29.760 --> 02:08:34.960]   I would imagine, given how sophisticated you are with your kid,
[02:08:34.960 --> 02:08:39.600]   Amy, a website would be something you would do at some point too, right?
[02:08:39.600 --> 02:08:45.360]   You agree with what I said about you've got to put yourself out there
[02:08:45.360 --> 02:08:47.120]   so that you control your reputation?
[02:08:47.120 --> 02:08:51.920]   So I guess we're all getting to know each other tonight.
[02:08:51.920 --> 02:08:52.640]   Why not, right?
[02:08:55.040 --> 02:09:00.000]   So we created a digital trust fund for her when she was born,
[02:09:00.000 --> 02:09:03.600]   which means that I registered her, you know, I've got her domain,
[02:09:03.600 --> 02:09:06.160]   I've got her social media names locked up,
[02:09:06.160 --> 02:09:11.760]   and accounts and I keep adding to it, but we don't post anything.
[02:09:11.760 --> 02:09:12.800]   But you have.
[02:09:12.800 --> 02:09:14.000]   She's got it when she's ready.
[02:09:14.000 --> 02:09:15.520]   Yeah.
[02:09:15.520 --> 02:09:16.320]   And then...
[02:09:16.320 --> 02:09:19.440]   The thing I wish I had thought of, of course, there was no Gmail at the time.
[02:09:19.440 --> 02:09:21.040]   My kids are 24 and 27.
[02:09:21.040 --> 02:09:23.280]   But I love this idea.
[02:09:23.280 --> 02:09:24.720]   Somebody told me about this years ago.
[02:09:24.720 --> 02:09:27.200]   Maybe it was one of the geek dads.
[02:09:27.200 --> 02:09:29.680]   Get a Gmail account when your baby's born,
[02:09:29.680 --> 02:09:34.240]   start sending stuff to it, and then give her the keys to the account
[02:09:34.240 --> 02:09:36.880]   when she's 18 or 21, whatever you think is appropriate.
[02:09:36.880 --> 02:09:37.520]   I think that's a...
[02:09:37.520 --> 02:09:38.480]   I love that idea.
[02:09:38.480 --> 02:09:40.080]   That's the best baby book ever, right?
[02:09:40.080 --> 02:09:40.960]   Yeah.
[02:09:40.960 --> 02:09:43.120]   Technology could be...
[02:09:43.120 --> 02:09:46.640]   If you think about it, it could be used well.
[02:09:46.640 --> 02:09:48.560]   I wish you'd write a book or something.
[02:09:48.560 --> 02:09:51.360]   On parenting, are you crazy?
[02:09:51.360 --> 02:09:53.680]   Years ago, no, no.
[02:09:53.680 --> 02:09:59.520]   Like, I would rather talk all day long about any other hot button issue you can imagine.
[02:09:59.520 --> 02:10:00.720]   I know. It's the hardest thing in the world.
[02:10:00.720 --> 02:10:05.920]   Parenting is the easiest way to invite lots of trouble into your life.
[02:10:05.920 --> 02:10:09.920]   You know another way? Talk about politics.
[02:10:09.920 --> 02:10:12.080]   But I'm going to have to do it.
[02:10:12.080 --> 02:10:15.600]   The house is voted to save net neutrality.
[02:10:15.600 --> 02:10:19.440]   And the White House is, of course, going to veto it.
[02:10:19.440 --> 02:10:21.520]   I feel like I have whiplash.
[02:10:21.520 --> 02:10:22.880]   Yeah, this goes back and forth.
[02:10:22.880 --> 02:10:24.640]   Except it really doesn't go back and forth.
[02:10:24.640 --> 02:10:27.440]   It's really, they've eliminated it.
[02:10:27.440 --> 02:10:31.280]   And now they're just fighting and fighting and fighting to get it back.
[02:10:31.280 --> 02:10:35.200]   Meanwhile, state, you know, the states are going ahead and passing their own resolutions.
[02:10:35.200 --> 02:10:39.520]   So the issue, and of course, you know, that's problematic too,
[02:10:39.520 --> 02:10:41.360]   because the internet is a global phenomenon.
[02:10:41.360 --> 02:10:42.400]   It's absolutely not wrong.
[02:10:42.400 --> 02:10:44.880]   I'm just saying I'm glad I live in the state of Washington.
[02:10:44.880 --> 02:10:45.360]   Yes.
[02:10:45.360 --> 02:10:48.560]   So at least you have the best law of all, right?
[02:10:48.560 --> 02:10:49.840]   Right.
[02:10:49.840 --> 02:10:53.120]   So at least until this is all figured out, I don't have to worry about,
[02:10:53.120 --> 02:10:56.720]   you know, my ISP being given information, um,
[02:10:56.720 --> 02:10:58.960]   regardless of what the federal statutes are.
[02:10:58.960 --> 02:11:02.160]   So you're going to hear from the White House when they veto it,
[02:11:02.160 --> 02:11:05.040]   because they will save the internet act.
[02:11:05.040 --> 02:11:07.680]   You're going to hear stats that are essentially bogus.
[02:11:07.680 --> 02:11:12.000]   Um, since the new rule was adopted in 2018,
[02:11:12.000 --> 02:11:15.920]   consumers have benefited from a greater than 35% increase in average,
[02:11:15.920 --> 02:11:18.000]   fixed broadband download speeds.
[02:11:18.000 --> 02:11:21.200]   And we went from 13th to 6th in the world.
[02:11:21.200 --> 02:11:24.480]   Uh, by the way, even that's nothing to celebrate.
[02:11:24.480 --> 02:11:26.960]   In 2018, fiber was also made available in more new homes
[02:11:26.960 --> 02:11:30.880]   in any previous year capital investment by the nation's top six internet service
[02:11:30.880 --> 02:11:32.640]   providers increased $2.3 billion.
[02:11:32.640 --> 02:11:37.600]   Um, some of this, of course, has been ongoing for years.
[02:11:37.600 --> 02:11:43.680]   Um, anyway, there's a good verge article that debunks a lot of these claims
[02:11:43.680 --> 02:11:46.320]   and it'd probably be worth reading it.
[02:11:46.320 --> 02:11:48.880]   And let's not give up the fight for net neutrality.
[02:11:48.880 --> 02:11:52.800]   Cause we know that that's something has to happen.
[02:11:52.800 --> 02:11:58.480]   23 state attorneys general are filing lawsuits, have filed lawsuits in state by state,
[02:11:58.480 --> 02:12:03.200]   of course, including Washington state, uh, states are in acting their own net neutrality.
[02:12:03.200 --> 02:12:05.280]   So that's awesome.
[02:12:05.280 --> 02:12:06.560]   Yes.
[02:12:06.560 --> 02:12:12.640]   That means we're going to wind up with, uh, one of the, one of the tech trends we talked about
[02:12:13.200 --> 02:12:17.920]   and modeled a couple of years ago was, um, splinter nets, which is not a, and I made up, but I mean,
[02:12:17.920 --> 02:12:20.960]   it's just, again, like this is so stupid.
[02:12:20.960 --> 02:12:21.600]   Like with it.
[02:12:21.600 --> 02:12:22.080]   Yep.
[02:12:22.080 --> 02:12:26.640]   The last thing you want now is to have a state vice like to, is to like have the states figure
[02:12:26.640 --> 02:12:27.600]   out how to handle this.
[02:12:27.600 --> 02:12:28.000]   Absolutely.
[02:12:28.000 --> 02:12:30.080]   How do you, like, what does that do for business?
[02:12:30.080 --> 02:12:33.200]   It's just a stupid data crosses state lines.
[02:12:33.200 --> 02:12:34.560]   What happens to it?
[02:12:34.560 --> 02:12:35.200]   Yeah, exactly.
[02:12:35.200 --> 02:12:38.640]   I mean, no, I mean, and it's happening on international levels too, right?
[02:12:38.640 --> 02:12:40.080]   Like with, with some of the different,
[02:12:40.080 --> 02:12:40.960]   Oh, look at this one.
[02:12:40.960 --> 02:12:43.520]   Countries are kind of like, I looked in Philly for a while.
[02:12:43.520 --> 02:12:44.880]   Is this going to be a thing where you like,
[02:12:44.880 --> 02:12:46.640]   I'm on the Philly internet.
[02:12:46.640 --> 02:12:48.240]   It's, you know,
[02:12:48.240 --> 02:12:48.720]   Yeah.
[02:12:48.720 --> 02:12:50.240]   The Philly internet's the best in it.
[02:12:50.240 --> 02:12:53.280]   It's over to like, like Texas to get slightly better.
[02:12:53.280 --> 02:12:56.400]   Like you're going to change your IP so that you're like 10 miles away.
[02:12:56.400 --> 02:12:58.400]   Now we have a good use for VPNs.
[02:12:58.400 --> 02:12:59.840]   No, it will.
[02:12:59.840 --> 02:13:02.000]   50 servers, one for each state.
[02:13:02.000 --> 02:13:02.880]   You choose the internet.
[02:13:02.880 --> 02:13:03.360]   You want it.
[02:13:03.360 --> 02:13:04.640]   Not lined up happening.
[02:13:04.640 --> 02:13:07.200]   No, that's totally what's going to want to happening.
[02:13:07.200 --> 02:13:10.480]   And unless we can have an administration change and then roll back,
[02:13:10.480 --> 02:13:13.920]   you know, this stuff and then the FCC can do what it should be doing,
[02:13:13.920 --> 02:13:19.680]   which is to, you know, like protect people and not just hand everything over to the ISPs.
[02:13:19.680 --> 02:13:23.600]   Although as a classic example of what could possibly go wrong,
[02:13:23.600 --> 02:13:33.840]   the French internet referral unit has reported to archive.org 550 of their archived URLs as
[02:13:33.840 --> 02:13:36.480]   terrorist contents falsely.
[02:13:37.280 --> 02:13:37.840]   My dad.
[02:13:37.840 --> 02:13:43.920]   Take down notices from the French IRU for a bunch,
[02:13:43.920 --> 02:13:46.800]   including, by the way, the Gutenberg,
[02:13:46.800 --> 02:13:48.560]   Smithsonian, the Grateful Dead.
[02:13:48.560 --> 02:13:55.200]   And actually the Grateful Dead might be considered terrorists to certain government agencies.
[02:13:55.200 --> 02:13:55.600]   I don't know.
[02:13:55.600 --> 02:13:57.200]   Crazy.
[02:13:57.200 --> 02:14:00.800]   What is possibly what was in that C-SPAN report?
[02:14:00.800 --> 02:14:05.920]   Like what was, who was holding up what post what floor chart that was deemed
[02:14:06.720 --> 02:14:15.120]   to the problem is with the used laws, you don't, you get less than 24 hours to respond.
[02:14:15.120 --> 02:14:18.960]   The archive.org is panicking.
[02:14:18.960 --> 02:14:21.760]   That's totally not the way that they intended.
[02:14:21.760 --> 02:14:23.040]   No, there's a one hour.
[02:14:23.040 --> 02:14:23.440]   I'm sorry.
[02:14:23.440 --> 02:14:24.080]   Did I get it?
[02:14:24.080 --> 02:14:25.440]   I exaggerated.
[02:14:25.440 --> 02:14:28.000]   There's a one hour requirement.
[02:14:28.000 --> 02:14:31.600]   You have to take those URL down immediately.
[02:14:31.600 --> 02:14:33.120]   You have one hour to do so.
[02:14:33.120 --> 02:14:34.240]   Or what?
[02:14:35.440 --> 02:14:39.040]   Or somebody comes and yells at you.
[02:14:39.040 --> 02:14:40.560]   Somebody comes and hits you with a baguette.
[02:14:40.560 --> 02:14:41.120]   I don't know.
[02:14:41.120 --> 02:14:42.640]   I don't know.
[02:14:42.640 --> 02:14:44.160]   I'll take that bag.
[02:14:44.160 --> 02:14:45.840]   I'll take that baguette.
[02:14:45.840 --> 02:14:50.960]   The French Internet referral unit falsely identified hundreds of URLs on archive.org
[02:14:50.960 --> 02:14:52.240]   as terrorist propaganda.
[02:14:52.240 --> 02:14:57.440]   The one hour requirement means, by the way, there was no one there when they came in.
[02:14:57.440 --> 02:14:58.480]   It was on the weekend.
[02:14:58.480 --> 02:15:03.120]   It's a mistake.
[02:15:03.840 --> 02:15:08.960]   Blocking procedures may be implemented against us.
[02:15:08.960 --> 02:15:09.920]   It says archive.org.
[02:15:09.920 --> 02:15:14.560]   If we don't remove the content, so they'll be blocked in France.
[02:15:14.560 --> 02:15:16.320]   Splinternet.
[02:15:16.320 --> 02:15:19.520]   That's the in the way.
[02:15:19.520 --> 02:15:25.920]   We do not want our, we do not want the bulkanization of our tech and our digital infrastructure.
[02:15:25.920 --> 02:15:30.640]   This gives extraordinary power to authoritarian regimes
[02:15:31.600 --> 02:15:34.800]   who would love to see us fighting these stupid dumb,
[02:15:34.800 --> 02:15:39.600]   I mean, just these, this is just a, again, this is like,
[02:15:39.600 --> 02:15:43.920]   this is why regulation often doesn't work.
[02:15:43.920 --> 02:15:49.200]   I mean, I understand why people, why we pursue it, but in practice, it doesn't work.
[02:15:49.200 --> 02:15:51.760]   But you've been arguing for it all along.
[02:15:51.760 --> 02:15:54.000]   No, I've not been arguing for regulation.
[02:15:54.000 --> 02:15:57.840]   I'm very much, I think the blunt instrument of regulation is the,
[02:15:57.840 --> 02:16:00.800]   is the, not the smart way forward.
[02:16:00.800 --> 02:16:03.760]   The smart way forward is to come up with something we don't have yet,
[02:16:03.760 --> 02:16:07.840]   which I know is much more difficult, but we've got to figure out a way to collaborate.
[02:16:07.840 --> 02:16:11.440]   We're again, you know, we have three like epicenters of power,
[02:16:11.440 --> 02:16:15.600]   at least in this country, Wall Street, DC, and our West Coast capital,
[02:16:15.600 --> 02:16:20.480]   which is the valley slash Bellevue, you know, and we've got to figure out a way to get them to
[02:16:20.480 --> 02:16:21.200]   collaborate.
[02:16:21.200 --> 02:16:23.040]   Oh my God, we're screwed.
[02:16:23.040 --> 02:16:25.520]   Well, I'm just saying, like otherwise.
[02:16:25.520 --> 02:16:27.280]   That's not going to happen.
[02:16:28.400 --> 02:16:30.240]   No, but regulation is bad.
[02:16:30.240 --> 02:16:33.200]   And this is another one of these things where like this kind of regulation,
[02:16:33.200 --> 02:16:34.240]   because it doesn't make sense.
[02:16:34.240 --> 02:16:35.840]   Look, you just,
[02:16:35.840 --> 02:16:36.080]   crazy.
[02:16:36.080 --> 02:16:36.560]   Oh, it's crazy.
[02:16:36.560 --> 02:16:37.040]   No, it's crazy.
[02:16:37.040 --> 02:16:37.680]   It's crazy.
[02:16:37.680 --> 02:16:38.160]   It's crazy.
[02:16:38.160 --> 02:16:39.680]   And France doing its own thing.
[02:16:39.680 --> 02:16:45.360]   And all of these different companies now and countries coming up with their own policies
[02:16:45.360 --> 02:16:48.640]   and value statements around all different types of technology, including AI.
[02:16:48.640 --> 02:16:55.120]   There's an evangelical group of Christians who have now come up with a Christian stance on what AI
[02:16:55.120 --> 02:16:55.840]   should be.
[02:16:55.840 --> 02:16:57.360]   Oh, God.
[02:16:58.000 --> 02:17:02.000]   So I, what the val, what, like, this is my point.
[02:17:02.000 --> 02:17:03.360]   Like we're all going about this.
[02:17:03.360 --> 02:17:06.240]   We're all, we're all doing our own thing.
[02:17:06.240 --> 02:17:11.040]   And what happens when everybody's individual interests wind up colliding,
[02:17:11.040 --> 02:17:14.480]   which they're going to, you know, then you wind up with a takedown notice for
[02:17:14.480 --> 02:17:17.120]   floor charts shown on C-SPAN.
[02:17:17.120 --> 02:17:17.920]   It makes no sense.
[02:17:17.920 --> 02:17:22.800]   I really, there's so many great stories, but we've gone way too long.
[02:17:22.800 --> 02:17:23.920]   So we're going to have to wrap it up.
[02:17:23.920 --> 02:17:26.160]   I really did want to talk about the Secret Service agent
[02:17:26.880 --> 02:17:33.120]   when they arrested that Chinese woman who had four passports, no, two, three, four phones.
[02:17:33.120 --> 02:17:33.920]   She's the malware.
[02:17:33.920 --> 02:17:39.040]   Malware in a USB stick about the Secret Service agent who plugged it into a machine to see what
[02:17:39.040 --> 02:17:39.680]   was on it.
[02:17:39.680 --> 02:17:42.080]   But we'll just save that for another day.
[02:17:42.080 --> 02:17:44.480]   I bet that was that it was that a male or a female agent?
[02:17:44.480 --> 02:17:46.960]   Does it?
[02:17:46.960 --> 02:17:47.360]   Plug it in.
[02:17:47.360 --> 02:17:48.160]   Do we need?
[02:17:48.160 --> 02:17:52.400]   Yes, it does because I bet you that person has a lot of sex without condoms.
[02:17:55.440 --> 02:18:03.360]   Samuel Ivanovich, who was the first person to interview Zhang at Mar-a-Lago, testified that,
[02:18:03.360 --> 02:18:05.520]   oh, no, he didn't.
[02:18:05.520 --> 02:18:07.120]   So he didn't do it.
[02:18:07.120 --> 02:18:10.320]   Another agent put Zhang's thumb drive into his computer.
[02:18:10.320 --> 02:18:15.760]   It immediately began to install files, a very out of the ordinary event.
[02:18:15.760 --> 02:18:17.440]   He had never seen happen before.
[02:18:17.440 --> 02:18:18.480]   What are you doing?
[02:18:18.480 --> 02:18:20.400]   Oh my God.
[02:18:20.400 --> 02:18:21.920]   This is from the Miami Herald.
[02:18:21.920 --> 02:18:23.840]   Oh my God.
[02:18:24.560 --> 02:18:26.000]   Well, maybe we don't know.
[02:18:26.000 --> 02:18:30.800]   Maybe it was a very special personal computer that wasn't connected to the internet or something.
[02:18:30.800 --> 02:18:38.080]   I mean, we can hope, but I mean, I arrest somebody for suspicion on something.
[02:18:38.080 --> 02:18:41.120]   I'm totally just going to take their thumb drive and plug it into my computer.
[02:18:41.120 --> 02:18:42.080]   Like, yeah, of course.
[02:18:42.080 --> 02:18:44.400]   That's the very first thing I'm going to do.
[02:18:44.400 --> 02:18:50.880]   It immediately began to install files, a quote, very out of the ordinary event.
[02:18:50.880 --> 02:18:53.840]   He had never seen happen before during this kind of analysis.
[02:18:54.480 --> 02:18:55.120]   Analysis.
[02:18:55.120 --> 02:18:55.600]   Oh my God.
[02:18:55.600 --> 02:18:56.560]   What is in the analysis?
[02:18:56.560 --> 02:18:57.840]   Let's open it and see what happens.
[02:18:57.840 --> 02:19:03.040]   The agent had to immediately stop the analysis to halt any further corruption of his computer.
[02:19:03.040 --> 02:19:05.920]   The analysis is still ongoing, but inconclusive.
[02:19:05.920 --> 02:19:07.440]   I don't think they know what analysis means.
[02:19:07.440 --> 02:19:08.640]   I think it's your brother-in-law.
[02:19:08.640 --> 02:19:09.520]   No, I was going to say.
[02:19:09.520 --> 02:19:10.080]   Your dad.
[02:19:10.080 --> 02:19:11.120]   All right.
[02:19:11.120 --> 02:19:13.200]   Well, my dad's totally my dad.
[02:19:13.200 --> 02:19:15.360]   Well, let's see what's on it.
[02:19:15.360 --> 02:19:18.000]   Let's see what happens.
[02:19:18.000 --> 02:19:21.120]   You know, you can turn off auto run.
[02:19:21.120 --> 02:19:21.760]   I'm just saying.
[02:19:22.720 --> 02:19:23.200]   Exactly.
[02:19:23.200 --> 02:19:24.880]   That E-X-E.
[02:19:24.880 --> 02:19:25.920]   What does that mean?
[02:19:25.920 --> 02:19:26.560]   Excellent.
[02:19:26.560 --> 02:19:27.440]   I should clear on it.
[02:19:27.440 --> 02:19:30.640]   All right.
[02:19:30.640 --> 02:19:31.120]   Enough.
[02:19:31.120 --> 02:19:31.600]   Enough.
[02:19:31.600 --> 02:19:33.920]   I am so happy to have you two on here.
[02:19:33.920 --> 02:19:36.320]   We decide normally we have a four-person panel.
[02:19:36.320 --> 02:19:36.960]   We said, no, no.
[02:19:36.960 --> 02:19:38.400]   Christina Warren, Amy Webb.
[02:19:38.400 --> 02:19:40.640]   That's all we need.
[02:19:40.640 --> 02:19:41.840]   That's a show.
[02:19:41.840 --> 02:19:44.960]   And I absolutely prove true.
[02:19:44.960 --> 02:19:46.400]   Christina is the greatest.
[02:19:46.400 --> 02:19:49.600]   We've known her since the good old days of Mashable,
[02:19:49.600 --> 02:19:51.920]   senior developer, Cloud developer,
[02:19:51.920 --> 02:19:52.800]   advocate at Microsoft.
[02:19:52.800 --> 02:19:55.440]   Now, catch her at the Ignite tour
[02:19:55.440 --> 02:19:58.080]   coming soon to a town near you.
[02:19:58.080 --> 02:19:59.120]   Yes.
[02:19:59.120 --> 02:20:02.320]   And on Channel 9, you do stuff there too, right?
[02:20:02.320 --> 02:20:03.280]   I sure do.
[02:20:03.280 --> 02:20:05.680]   YouTube.com/MicrosoftDeveloper.
[02:20:05.680 --> 02:20:05.920]   Nice.
[02:20:05.920 --> 02:20:07.600]   So you are on YouTube.
[02:20:07.600 --> 02:20:08.560]   You are a YouTube star.
[02:20:08.560 --> 02:20:09.120]   She's a YouTube star.
[02:20:09.120 --> 02:20:09.440]   Oh.
[02:20:09.440 --> 02:20:11.680]   So you're a YouTube star for, you know,
[02:20:11.680 --> 02:20:13.200]   Geeks.
[02:20:13.200 --> 02:20:15.200]   The best audience.
[02:20:15.200 --> 02:20:16.800]   Without a doubt.
[02:20:16.800 --> 02:20:18.080]   We're an influencer.
[02:20:18.080 --> 02:20:19.200]   She's an influencer.
[02:20:19.520 --> 02:20:20.400]   Of course she is.
[02:20:20.400 --> 02:20:21.600]   And always glad to have you.
[02:20:21.600 --> 02:20:23.520]   Thank you for keeping the lights on.
[02:20:23.520 --> 02:20:25.440]   Late at night in Redmond.
[02:20:25.440 --> 02:20:27.680]   Thank you for having me.
[02:20:27.680 --> 02:20:29.520]   Are you rushing off to watch Game of Thrones?
[02:20:29.520 --> 02:20:30.560]   We have how many hours?
[02:20:30.560 --> 02:20:31.120]   Yes.
[02:20:31.120 --> 02:20:31.840]   Happy been hour.
[02:20:31.840 --> 02:20:32.480]   Thrones youth.
[02:20:32.480 --> 02:20:34.240]   Now they put it out on,
[02:20:34.240 --> 02:20:37.520]   if you have HBO Go, you can watch it early, right?
[02:20:37.520 --> 02:20:38.560]   That's what I'm doing.
[02:20:38.560 --> 02:20:38.960]   Which is what I'm doing.
[02:20:38.960 --> 02:20:39.520]   Exactly.
[02:20:39.520 --> 02:20:42.640]   So it's so in one hour and you can watch it right now, Amy.
[02:20:42.640 --> 02:20:45.760]   Yeah, in fact, if I don't leave pretty soon, I'm going to get...
[02:20:45.760 --> 02:20:47.520]   Are you having a party?
[02:20:47.520 --> 02:20:47.920]   I'm getting a deal done.
[02:20:48.800 --> 02:20:50.320]   Is your party of two?
[02:20:50.320 --> 02:20:51.040]   You're eight.
[02:20:51.040 --> 02:20:52.880]   You don't let your eight-year-old watch Game of Thrones.
[02:20:52.880 --> 02:20:53.440]   It's not on the...
[02:20:53.440 --> 02:20:56.240]   It's blocked by the Great Firewall of Amy.
[02:20:56.240 --> 02:20:58.800]   No, I don't blame you.
[02:20:58.800 --> 02:21:00.960]   All right.
[02:21:00.960 --> 02:21:02.720]   Amy Webb, the author of The Big Nine.
[02:21:02.720 --> 02:21:04.240]   You got to read this great book.
[02:21:04.240 --> 02:21:06.960]   And always welcome on our air.
[02:21:06.960 --> 02:21:09.600]   You can find out more at the Future Today Institute
[02:21:09.600 --> 02:21:11.440]   or just go to ameweb.io.
[02:21:11.440 --> 02:21:12.960]   And I'm glad to know you're on Keybase.
[02:21:12.960 --> 02:21:13.760]   I did not know that.
[02:21:13.760 --> 02:21:16.160]   I will figure out your handle and follow you.
[02:21:16.160 --> 02:21:17.760]   Good luck.
[02:21:17.760 --> 02:21:19.280]   Yeah, I searched for Amy Webb.
[02:21:19.280 --> 02:21:20.080]   I didn't find it.
[02:21:20.080 --> 02:21:21.840]   You could follow me.
[02:21:21.840 --> 02:21:22.640]   I'm Leo Laport.
[02:21:22.640 --> 02:21:26.480]   Keybase, everybody who's a geek should know about keybase.io.
[02:21:26.480 --> 02:21:28.960]   It's free and it's an amazing service.
[02:21:28.960 --> 02:21:31.760]   I use their encrypted Git for all my private stuff.
[02:21:31.760 --> 02:21:34.640]   It's fantastic and great encrypted chat.
[02:21:34.640 --> 02:21:37.520]   Keeps your PGP's keys and all that stuff.
[02:21:37.520 --> 02:21:38.480]   Thank you, Amy.
[02:21:38.480 --> 02:21:39.840]   Thank you, Christina.
[02:21:39.840 --> 02:21:41.520]   What a great show to come back to.
[02:21:41.520 --> 02:21:43.840]   This is actually better than Kawhi.
[02:21:43.840 --> 02:21:46.400]   No, it's not.
[02:21:46.400 --> 02:21:46.960]   Thank you.
[02:21:46.960 --> 02:21:48.160]   Thank you for saying that.
[02:21:48.160 --> 02:21:50.000]   It's better than the Lula we saw last night.
[02:21:50.000 --> 02:21:50.560]   I'll say that.
[02:21:50.560 --> 02:21:51.200]   That may be true.
[02:21:51.200 --> 02:21:51.680]   I mean.
[02:21:51.680 --> 02:21:53.840]   Thank you, everybody.
[02:21:53.840 --> 02:21:56.640]   We do Twitter every Sunday afternoon, round about 2.15.
[02:21:56.640 --> 02:21:58.000]   Right after the radio show,
[02:21:58.000 --> 02:22:01.680]   that's 2.15 Pacific 5.15 Eastern time.
[02:22:01.680 --> 02:22:06.800]   That's about, oh, I don't know, 21.20 UTC, something like that.
[02:22:06.800 --> 02:22:09.440]   Come by and watch Twitter.tv/live.
[02:22:09.440 --> 02:22:12.240]   There's a live audio and video stream, so you can watch or listen.
[02:22:12.240 --> 02:22:15.760]   If you're doing that, chat room is a great place to hang out.
[02:22:15.760 --> 02:22:17.280]   They're watching and listening live too.
[02:22:17.280 --> 02:22:19.520]   It's irc.twit.tv.
[02:22:19.520 --> 02:22:22.640]   Everybody, even Amy's daughter, will feel safe there.
[02:22:22.640 --> 02:22:23.920]   It should be part.
[02:22:23.920 --> 02:22:26.240]   It should be inside the Great Firewall of Amy.
[02:22:26.240 --> 02:22:29.600]   If you want to watch us in studio, you're more than welcome to do so.
[02:22:29.600 --> 02:22:32.320]   We had a great live audience from all over the world of Australia.
[02:22:32.320 --> 02:22:35.360]   And let's see, Connecticut.
[02:22:35.360 --> 02:22:42.560]   Pennsylvania, Washington, D.C., and Mountain View.
[02:22:43.360 --> 02:22:46.080]   Please come and join us. Just email tickets@twit.tv.
[02:22:46.080 --> 02:22:47.600]   We'll put a chair out for you.
[02:22:47.600 --> 02:22:50.480]   You can also get on-demand versions of everything we do at our website,
[02:22:50.480 --> 02:22:55.760]   twit.tv, or best idea yet to subscribe in your favorite podcast application.
[02:22:55.760 --> 02:22:57.040]   That way you won't miss an episode.
[02:22:57.040 --> 02:22:58.080]   You'll have a time for your money.
[02:22:58.080 --> 02:23:00.560]   Can you enjoy Game of Thrones, everybody?
[02:23:00.560 --> 02:23:02.400]   We'll see you next time.
[02:23:02.400 --> 02:23:04.080]   Another Twit is in the can.
[02:23:04.080 --> 02:23:14.400]   See you next time.

