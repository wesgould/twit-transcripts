;FFMETADATA1
title=A Straw Man Without Legs
artist=Leo Laporte, Stacey Higginbotham, Doc Rock, Sam Abuelsamid
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2021-08-09
track=835
language=English
genre=Podcast
comment=Apple's proposed CSAM protections, Google Pixel 6, Firefox declines
encoded_by=Uniblab 5.3
date=2021
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:03.240]   It's time for Twit this week in Tech. Great panel for you. It's got an old home week,
[00:00:03.240 --> 00:00:09.520]   Sammabool, Sammabool joins us. My car guy. Doc Rock joins us from Hawaii and Stacey
[00:00:09.520 --> 00:00:15.600]   Higginbotham from this week in Google. We are going to talk, I think, a lot about Apple's
[00:00:15.600 --> 00:00:22.440]   new proposed child pornography protections. Some say it's a privacy invasion. Others say,
[00:00:22.440 --> 00:00:26.840]   think of the children. This is not an easy one, but we'll go deep on this. We'll talk
[00:00:26.840 --> 00:00:32.660]   about Google's reveal of the Pixel 6. It's kind of an interesting choice when they've
[00:00:32.660 --> 00:00:43.160]   got a new 5A coming along next week. And Firefox lost 50 million users last year. Almost 20%
[00:00:43.160 --> 00:00:49.360]   of its total. Is that a bad thing? I think it is. We'll talk about that a whole lot more.
[00:00:49.360 --> 00:00:53.600]   Coming up next, it's going to be a fun Twit. Stay tuned.
[00:00:53.600 --> 00:01:12.160]   This is Twit. This is Twit. This is Twit. This week in Tech. Episode 835 recorded Sunday,
[00:01:12.160 --> 00:01:20.400]   August 8, 2021. A straw man without legs. This week in Tech is brought to you by Modern
[00:01:20.400 --> 00:01:25.480]   Finance. The financial landscape is harder than ever to navigate, but you don't have
[00:01:25.480 --> 00:01:30.760]   to do it alone. Download and subscribe to Modern Finance from Kevin Rose, wherever you
[00:01:30.760 --> 00:01:35.040]   listen to podcasts. And get ahead of the future of finance.
[00:01:35.040 --> 00:01:40.800]   And by Zip Recruiter. Over two-thirds of Americans are planning to travel this summer. That means
[00:01:40.800 --> 00:01:47.160]   airlines, restaurants and more are ramping up their hiring. Who do they turn to? Zip Recruiter.
[00:01:47.160 --> 00:01:51.720]   Zip Recruiter's technology finds qualified candidates for your job and invites them to
[00:01:51.720 --> 00:01:57.840]   apply. Right now you can try Zip Recruiter free at ziprecruiter.com/twit.
[00:01:57.840 --> 00:02:03.800]   And by Udacity. Gain in-demand tech skills in as little as three months with Udacity's
[00:02:03.800 --> 00:02:12.280]   part-time online tech courses. Visit Udacity.com/twit and get 75% off any program with a code
[00:02:12.280 --> 00:02:21.440]   quit 75. And by AT&T Active Armor. We rely so much on our phones these days and are always
[00:02:21.440 --> 00:02:26.200]   on them whether it's live streaming content, catching up with family on weekly video calls
[00:02:26.200 --> 00:02:32.120]   or watching your favorite podcast. There's no room for fraud calls. Thankfully, AT&T
[00:02:32.120 --> 00:02:38.760]   makes customer security a priority, helping block those pesky calls. It's not complicated.
[00:02:38.760 --> 00:02:46.320]   AT&T Active Armor. 24/7 proactive network security and fraud call blocking to help stop threats
[00:02:46.320 --> 00:03:02.040]   at no extra charge. Capitable device and service required visit attt.com/activearmor for details.
[00:03:02.040 --> 00:03:05.800]   It's time for Twit this weekend tech. The show we cover the latest tech news with the
[00:03:05.800 --> 00:03:13.240]   dream team, people we've pulled in from our Twit, larger Twit family. And boy, we got
[00:03:13.240 --> 00:03:17.640]   a good family this week. I wish this was for Thanksgiving. Joining us from this week in
[00:03:17.640 --> 00:03:25.720]   Google, the wonderful Stacey Higginbotham Stacey on IOT.com at Gigastacey on the Twitter. She's
[00:03:25.720 --> 00:03:30.680]   does that podcast, the IoT podcast with Kevin Tofill. It's no nice to see you on a Sunday
[00:03:30.680 --> 00:03:34.040]   instead of a Wednesday. Thank you for doing this Stacey.
[00:03:34.040 --> 00:03:37.800]   Thank you for inviting me. I felt like I abandoned y'all for two weeks, so I had to come back
[00:03:37.800 --> 00:03:44.280]   early. Yeah, we did miss you on Twig. I miss Jo. Yeah, but we're glad you're here for Twit.
[00:03:44.280 --> 00:03:48.520]   Actually, I'm really glad because we're going to start off with a tough topic, but I'll get
[00:03:48.520 --> 00:03:54.680]   to that in a second. Also with his Doc Rock, he's a creator. It says so in his t-shirt,
[00:03:54.680 --> 00:04:02.440]   youtube.com/docrock from Honolulu. Hello, Doc. How you doing? Good to see you here.
[00:04:02.440 --> 00:04:07.880]   I feel so bad. I forgot you live in Oahu. And when we were there, we should have called,
[00:04:07.880 --> 00:04:11.880]   and I apologize. But it just gives me excuse to go back because we love violent.
[00:04:11.880 --> 00:04:16.440]   There you go. That's better. It is beautiful. We fell in love. I understand why you stayed.
[00:04:16.440 --> 00:04:21.720]   You fell in love too. Yeah. Yeah, it was something about no snow and, you know,
[00:04:21.720 --> 00:04:25.800]   not having to deal with buying different clothes. One set of clothes last all year.
[00:04:25.800 --> 00:04:31.800]   Doc is also the community manager at Ecamm. You might see the Ecamm hat. We'll talk about
[00:04:31.800 --> 00:04:36.520]   Ecamm in a little bit, but I know Micah still loves the Ecamm setup. Let's him work from home.
[00:04:36.520 --> 00:04:42.280]   I still use Ecamm. There you go. Oh, what? I didn't know that station. Okay, I got you.
[00:04:42.280 --> 00:04:47.400]   Anything you need, I got you. I actually had Rosemary on my show not that long ago,
[00:04:47.400 --> 00:04:51.960]   and we nerd it out over shortcuts forever. And all she could say, oh my God, I love this.
[00:04:51.960 --> 00:04:57.560]   This is so cool. So yeah. I'll have to go check that out. I had no idea. All right.
[00:04:57.560 --> 00:05:04.520]   See already that's happening. The magic. All she did. From Ipsolyady, Michigan. It is
[00:05:04.520 --> 00:05:10.680]   Sam Ebull, Sam Ed. My car guy, podcaster, wheelbarings.media, principal researcher,
[00:05:10.680 --> 00:05:15.640]   guide house insights. He's on my radio show every Sunday. Just talk to him, actually. Hi, Sam.
[00:05:16.280 --> 00:05:19.400]   Hello, Leo. Good to meet you, Doc Rock and Stacy.
[00:05:19.400 --> 00:05:23.560]   Oh, I should introduce you. I didn't know. I just assume everybody knows everybody.
[00:05:23.560 --> 00:05:28.280]   Anyway, well, we kind of introduced met each other before you came into the studio.
[00:05:28.280 --> 00:05:31.720]   Oh, good. And we've seen pictures of your wedding show and why.
[00:05:31.720 --> 00:05:38.040]   I think we're family. I think we can. So the big story of the week, and it's actually a
[00:05:38.040 --> 00:05:45.800]   developing story, Apple announced on Thursday that they are going to start implementing an iOS 15
[00:05:45.800 --> 00:05:51.560]   new child safety measures. In fact, if you go to apple.com/child.safety,
[00:05:51.560 --> 00:05:57.240]   you can read about this. And you know, on the face of it, when I first read it, I thought,
[00:05:57.240 --> 00:06:02.040]   well, this seems like a smart way to do it. I'll just, I'll explain quickly what they're doing
[00:06:02.040 --> 00:06:07.960]   so that we can then have a good conversation about why it's the worst thing ever. Because,
[00:06:07.960 --> 00:06:14.600]   apparently, privacy advocates, including the EFF, the Center for Democracy and Technology,
[00:06:14.600 --> 00:06:20.360]   a lot of cryptographers and others are very upset about this. So there's an organization
[00:06:20.360 --> 00:06:25.640]   called the National Center for Exploited and Missing Children. It's a non-governmental
[00:06:25.640 --> 00:06:32.280]   nonprofit, but it is funded by the US government, Nick, Nick, I'll call it, that has a database of
[00:06:32.280 --> 00:06:38.840]   child pornography or CSAM, child sexual abuse material. You'll hear those acronyms a lot in this
[00:06:38.840 --> 00:06:45.320]   conversation, Nick, meck and CSAM. So the neck, neck database is a fingerprint database. It's a hash,
[00:06:45.320 --> 00:06:53.080]   a cryptographic hash of a lot of awful child pornography images, actual specific images,
[00:06:53.080 --> 00:06:56.840]   each of which have been fingerprinted. And the fingerprinting they do is that in such a way that
[00:06:56.840 --> 00:07:02.360]   even if the image is cropped or later turned into a black and white image or modified some way,
[00:07:02.360 --> 00:07:08.920]   the fingerprint does not change. So that hash is specific to that image. You should also know
[00:07:08.920 --> 00:07:17.000]   that it's not specific to whatever the image is of. It is that picture. So you could take a picture
[00:07:17.000 --> 00:07:22.760]   of a car, hash it. Another picture of the same car would not have the same fingerprint. So those
[00:07:22.760 --> 00:07:31.080]   fingerprints are tied directly to a specific pornographic image, child porn image. So this database
[00:07:31.080 --> 00:07:35.160]   has been around for a while. In fact, it's my understanding companies have been using this for
[00:07:35.160 --> 00:07:40.200]   a while. Facebook, Google and others have been using this as a while for a while. Apple plans,
[00:07:40.200 --> 00:07:45.960]   starting with iOS 15, to put a copy of this database on your device.
[00:07:45.960 --> 00:07:55.240]   And then to compare to create a fingerprint of images that you send through messages,
[00:07:55.240 --> 00:08:01.720]   whether it's SMS or data only, images you upload to iCloud images you download from iCloud.
[00:08:01.720 --> 00:08:08.920]   To run that hash, you can do it locally on the iPhone and then compare the fingerprint.
[00:08:08.920 --> 00:08:15.880]   Important according to Apple's white paper, Apple does not learn about anything about images that
[00:08:15.880 --> 00:08:23.000]   don't match the database. Apple cannot access metadata or visual derivatives for matched images
[00:08:23.000 --> 00:08:28.360]   until a threshold of matches is exceeded. They do not say what that threshold is. Is it one
[00:08:28.360 --> 00:08:32.520]   picture? Is it five? Is it ten? Is it a hundred? They don't say. But there is some threshold. It's
[00:08:32.520 --> 00:08:38.120]   not just apparently not just one picture matching. You have to have more than one. The risk they say
[00:08:38.120 --> 00:08:45.480]   of the system incorrectly flagging an account is extremely low. And if there's a report going
[00:08:45.480 --> 00:08:48.680]   to be made, so the two things can happen. If there's a match and you match the threshold,
[00:08:48.680 --> 00:08:54.520]   so if there's some number of matches, if it's on a child's phone under 13, the parents will be
[00:08:54.520 --> 00:09:00.920]   notified. If it's on an adults or somebody 13 or Oliver's phone after that threshold is passed,
[00:09:00.920 --> 00:09:09.000]   image will be sent, a low quality image will be sent to Apple, which will be reviewed by a human.
[00:09:09.000 --> 00:09:13.800]   If it does in fact match that known CSAM image, then they will notify NECMEC.
[00:09:15.080 --> 00:09:19.480]   It says users can't access or view the database of known images. Users can't identify which
[00:09:19.480 --> 00:09:31.400]   images were flagged. So Apple's trying to create a system that respects privacy but still does
[00:09:31.400 --> 00:09:38.120]   something about child pornography. This is something every company is worried about because apparently
[00:09:38.120 --> 00:09:43.480]   there's a lot of child pornography. The internet's widely used to distribute it and store it. So
[00:09:43.480 --> 00:09:49.960]   Apple doesn't want to be doing that. In fact, they have up to now been among the big tech companies,
[00:09:49.960 --> 00:09:57.240]   the slowest to report CSAM images on their servers. This is an image that Apple distributed
[00:09:57.240 --> 00:10:04.680]   of how the hashes work. It will now, here's one thing that does concern people, it will also match
[00:10:04.680 --> 00:10:17.320]   against your iCloud photos. I think it's important to confirm that it doesn't match pictures. If I
[00:10:17.320 --> 00:10:24.760]   took a picture of my toddler in the bathtub, it's not looking for that. It's only looking for
[00:10:24.760 --> 00:10:31.880]   pictures that have already been identified as child porn. And that's fairly important. There
[00:10:31.880 --> 00:10:35.000]   have been a number of mainstream media outlets that have actually gotten that wrong and said
[00:10:35.000 --> 00:10:38.440]   that they're looking for a certain amount of nudity or things like that. It doesn't work that way.
[00:10:38.440 --> 00:10:42.840]   It is merely a cryptographic hash against known child porn images.
[00:10:42.840 --> 00:10:52.920]   So you mentioned that this can detect if you've cropped the image or resized it or
[00:10:52.920 --> 00:10:59.400]   done various other things to it, that it can still recognize the image. Machine vision systems are
[00:11:00.760 --> 00:11:11.400]   not that reliable in general. Do we have any information about how accurate this actually is?
[00:11:11.400 --> 00:11:18.920]   They're false positive with something like one. Hold on, hold on. They're not looking for
[00:11:18.920 --> 00:11:23.240]   this as a child. What they're looking for, they've got a set of pixels.
[00:11:23.240 --> 00:11:27.400]   It's not face recognition at all. When they're creating these hashes, what they're doing is
[00:11:27.400 --> 00:11:31.000]   they're taking a set of computers are actually really good at this because they're not turning
[00:11:31.000 --> 00:11:34.760]   this into meaning. They don't know what they're looking at. They're looking at a set of pixels,
[00:11:34.760 --> 00:11:40.280]   which is why if you crop it, they can still recognize X number of those pixels and match it back to
[00:11:40.280 --> 00:11:47.960]   the original. So that's a hash. Is it doing the hash based on initially on some subset,
[00:11:47.960 --> 00:11:52.920]   some part of maybe the middle of the image. So if you do crop it, you're still capturing
[00:11:53.560 --> 00:12:01.800]   that part of it because a hash, if you take a binary file and you hash it,
[00:12:01.800 --> 00:12:05.720]   if you change it, it doesn't work the same. It changes the hash.
[00:12:05.720 --> 00:12:10.440]   That's right. This is called... I'll read from Apple's White Paper.
[00:12:10.440 --> 00:12:17.480]   They call it a neural hash. It is, you're writing this, a perceptual hashing function
[00:12:17.480 --> 00:12:24.600]   that maps images to numbers. Perceptual hashing bases this hash, this fingerprint on features of
[00:12:24.600 --> 00:12:29.720]   the image instead of precise pixel values. So that's, you're exactly right, Sam.
[00:12:29.720 --> 00:12:32.680]   So we're not doing a machine vision system and then hashing that.
[00:12:32.680 --> 00:12:38.120]   It says the system computes these hashes by using an embedding network to produce image descriptors
[00:12:38.120 --> 00:12:43.640]   and then converting those descriptors to integers using what they call a hyperplane
[00:12:44.280 --> 00:12:49.800]   local sensitivity hashing process so that different images produce different hashes.
[00:12:49.800 --> 00:12:53.800]   And they give an example here of two different images of a palm tree
[00:12:53.800 --> 00:13:01.400]   that have the same neural hash, but then a third image that has different content and different hash.
[00:13:01.400 --> 00:13:07.800]   Doc Rock, you were about to say that the rate of false positives is very low in the order of
[00:13:07.800 --> 00:13:10.920]   what, one to a billion or something? No, it was in a trillion.
[00:13:10.920 --> 00:13:15.160]   Trillions. That was the point. And it's like, I think one thing, people get freaked out because
[00:13:15.160 --> 00:13:20.040]   they think there's going to be false positives. But I also have this conversation with my friends
[00:13:20.040 --> 00:13:24.520]   all the time because I think people hear the word billion and hear the word trillion, but they
[00:13:24.520 --> 00:13:31.480]   don't know how to put something to it. So there was a guy a long time ago that made a text file
[00:13:31.480 --> 00:13:37.960]   that was literally a billion as put out as like one and zeros or something. And the text file was
[00:13:37.960 --> 00:13:43.240]   like stupid long. Like this has a billion characters in it. Just to show somebody, you'd be scrolling
[00:13:43.240 --> 00:13:50.120]   that thing for like 40 minutes to get to the bottom of the text file. I use one that kind of
[00:13:50.120 --> 00:13:56.440]   mess. If you take the person who identifies of the Christian faith, if I gave you a dollar every
[00:13:56.440 --> 00:14:02.760]   day since the person you look up to was born, I'd be at 740 some odd million. I'm still not at a
[00:14:02.760 --> 00:14:10.040]   billion. So like billion is a very big number. Trillion is even more than that. So the false
[00:14:10.040 --> 00:14:15.080]   positives things that they've put into effect has been good. And everyone's mad because Apple
[00:14:15.080 --> 00:14:19.000]   won't talk about like how they're doing it. Well, if we tell you how we doing it, man, then
[00:14:19.000 --> 00:14:22.280]   people can figure out how to undo it. They're pretty clear. This white paper is pretty clear.
[00:14:22.280 --> 00:14:26.680]   But yeah, they left some stuff. So we should flatten. We should flatten this notion that
[00:14:27.480 --> 00:14:35.720]   A, you might be fingered incorrectly, that that's a very, very, very, very low probability. That's
[00:14:35.720 --> 00:14:40.840]   not the problem with this. But people do have not just people, some pretty serious people have
[00:14:40.840 --> 00:14:48.200]   an issue with it. This is a letter that is circulating now has last cat last count 1800 signatures
[00:14:48.200 --> 00:14:53.800]   from well known cryptographers, privacy and security experts. It's at apple privacy letter.com
[00:14:53.800 --> 00:15:00.440]   an open letter against Apple's privacy invasive content scanning technology. Now I'll point out
[00:15:00.440 --> 00:15:06.200]   my initial reaction to what Apple's doing after reading all of the material Apple put out was.
[00:15:06.200 --> 00:15:10.840]   This seems like they really they want to do something about child pornography. And they're trying to do
[00:15:10.840 --> 00:15:18.680]   it responsibly. But I have to confess I've been convinced otherwise first by this letter and
[00:15:18.680 --> 00:15:23.880]   then by a caller to the radio show, but I'll explain that in a second. This letter says,
[00:15:23.880 --> 00:15:30.600]   while child exploitation is a serious problem and while efforts to combat it are almost unquestionably
[00:15:30.600 --> 00:15:36.520]   well intentioned. And then this part's in bold. Apple's proposal introduces a backdoor
[00:15:36.520 --> 00:15:43.080]   that threatens to undermine fundamental privacy protection for all users of Apple products.
[00:15:43.960 --> 00:15:50.680]   And the reasoning the EFF gives the CDT gives and somebody I really respect Dr. Matthew Green of
[00:15:50.680 --> 00:15:58.520]   Johns Hopkins gives is that it is a slippery slope. And at first I thought, is it a slippery slope?
[00:15:58.520 --> 00:16:02.440]   I hate slippery. I hate slippery slope armaments. Well, that's the thing if you know,
[00:16:02.440 --> 00:16:08.040]   is there is there a problem with the way they're doing it right now? No. But but I and at first I
[00:16:08.040 --> 00:16:12.440]   thought slippery, I had the same reaction. Oh, a slippery slope. Everything's a slippery slope.
[00:16:12.440 --> 00:16:17.480]   Everything can get worse, but you don't act today on what's going on today on the potential that
[00:16:17.480 --> 00:16:22.440]   it could get worse. Because everything's a slippery slope. If you do that, everything can get worse.
[00:16:22.440 --> 00:16:28.760]   Matthew Green said yesterday, we were gradually headed toward a future where less and less of
[00:16:28.760 --> 00:16:33.080]   our information had to be under the control and review of anyone but ourselves. For the first
[00:16:33.080 --> 00:16:38.280]   time since the 90s, we were taking our privacy back today. We're on a different path. And this
[00:16:38.280 --> 00:16:42.920]   is the slippery slope people are talking about. The pressure is going to come from the UK, from the
[00:16:42.920 --> 00:16:50.120]   US, from India, from China. I'm terrified about what it's going to look like. Why would Apple want
[00:16:50.120 --> 00:16:56.280]   to tell the world, Hey, we've got this tool. Now let me, I want to get your opinions, all of you.
[00:16:56.280 --> 00:17:03.480]   I'm just going to say one more thing and I'll stop talking. A guy called Joe. Hello, a giant
[00:17:03.480 --> 00:17:08.520]   name Joe is in Brooklyn. That's okay, Stacey. We know your dog. That means some delivery,
[00:17:08.520 --> 00:17:13.720]   some wonderful delivery is coming, right? She's going to run off to silence the animal.
[00:17:13.720 --> 00:17:19.240]   Joe, who's a regular in our chat room and our discord server as a club,
[00:17:19.240 --> 00:17:25.240]   Twitter member from Brooklyn, very reasonable said, okay, here's why this is potentially a problem.
[00:17:25.240 --> 00:17:31.640]   Now that Apple has said we could do this. The issue is governments coming to them saying, well,
[00:17:31.640 --> 00:17:36.520]   okay, I got a new database and I want you to do the same thing for that database.
[00:17:36.520 --> 00:17:42.760]   And the problem is when you're operating in a country like China or Saudi Arabia or Russia,
[00:17:42.760 --> 00:17:50.600]   they can do that. And so the premise is, well, what if what if the Saudi Arabia where it's illegal
[00:17:50.600 --> 00:17:58.280]   to be gay said, I have a database of images of gay people. I want you to also check against that.
[00:17:59.000 --> 00:18:04.680]   Or there's actually a database, a terrorist database with so-called terrorist images,
[00:18:04.680 --> 00:18:11.160]   the global internet formed a counterterrorism has created a similar fingerprint database.
[00:18:11.160 --> 00:18:17.720]   But no one knows what's in that database because there's no external oversight.
[00:18:17.720 --> 00:18:23.960]   What's to keep this is with the EFFs talking about that was to keep the US government or China
[00:18:23.960 --> 00:18:28.360]   or Russia or any other governments are saying, Apple, if you want to do business in our country,
[00:18:28.360 --> 00:18:34.680]   in addition to that neck, neck database, we've got a database of terrorist images we want you to flag.
[00:18:34.680 --> 00:18:41.640]   Can Apple at that point say, well, okay, we'll do it for channel pornography. We're not going to
[00:18:41.640 --> 00:18:46.600]   do it for you. In which case the country can say, well, then you're not going to do business in
[00:18:46.600 --> 00:18:52.200]   our country. In fact, I would submit apples already opened this door by saying we can do it. It's too
[00:18:52.200 --> 00:18:59.800]   late. What are your thoughts, Stacey? Okay. So first, this database has existed for a while and is
[00:18:59.800 --> 00:19:06.040]   used for Facebook. So it's not that the feds are unaware that this is possible. So that's one.
[00:19:06.040 --> 00:19:08.920]   I don't think Apple has opened any door that hasn't already been there.
[00:19:08.920 --> 00:19:13.880]   Two, this is such a tough issue because we're basically putting the potential human rights
[00:19:13.880 --> 00:19:21.800]   violations of governments against the rights of children. Or rather, solving a documented
[00:19:21.800 --> 00:19:29.320]   and real problem with child pornography. And this gets to the heart of what I always
[00:19:29.320 --> 00:19:33.640]   talk about and think about as we're building out the tools for what will essentially become a
[00:19:33.640 --> 00:19:42.040]   potential surveillance society, right? And we have to grapple with these issues, not just
[00:19:42.040 --> 00:19:49.080]   by saying, I'm going to ban this tool, or I'm going to fundamentally break this tool by, you know,
[00:19:49.080 --> 00:19:57.320]   adding encryption, or maybe I'm banning the tool by saying we can't have cell phone photos. I mean,
[00:19:57.320 --> 00:20:03.240]   that horse is out of the barn. So now what we have to do is we have to figure out,
[00:20:03.240 --> 00:20:09.720]   and this is hard, does a company have a moral obligation to people in other countries,
[00:20:09.720 --> 00:20:14.200]   or do they have a moral obligation to their shareholders to make as much money as possible?
[00:20:14.920 --> 00:20:22.760]   And we can't say that Saudi Arabia is not going to ask Apple to do this, for example.
[00:20:22.760 --> 00:20:30.680]   What we can say is Apple maybe don't give in to them. And the issue here isn't,
[00:20:30.680 --> 00:20:36.360]   I know this is a fraught issue, and I understand that this is very,
[00:20:37.480 --> 00:20:45.160]   this is fundamentally, it has the potential to be a huge problem. And I'm, I am like,
[00:20:45.160 --> 00:20:50.360]   I was like, slipper slope, I see this slippery slope, I've seen it before, but we deal with this
[00:20:50.360 --> 00:20:56.680]   and regulations all the time. That's the point of making regulations is saying, this is the line
[00:20:56.680 --> 00:21:02.200]   we're going to go to today, and this is where it's going to stop. And as you see, we move back and
[00:21:02.200 --> 00:21:08.040]   forth over this line over time as MLK would say, the moral arc of the universe shifts.
[00:21:08.040 --> 00:21:12.280]   So the question then becomes,
[00:21:12.280 --> 00:21:22.760]   what do we expect from Apple? What do we expect our government to do? What kind of rules can we
[00:21:22.760 --> 00:21:28.760]   put in place? It's not to say shove this horse back into the barn, because it's already out.
[00:21:28.760 --> 00:21:33.800]   We have this capability. Everyone knows we have this capability. We're going to have more cameras
[00:21:33.800 --> 00:21:40.440]   and more places, which means other people who have less privacy, privacy focused values as Apple
[00:21:40.440 --> 00:21:46.520]   are going to be able to offer this as well. So let's have this conversation and let's see where
[00:21:46.520 --> 00:21:50.440]   we can draw these lines. Let's understand where other governments are going to draw those lines,
[00:21:50.440 --> 00:21:57.960]   and then let's work from there. That's it. The issue Joe brought up, and I think it was appropriate,
[00:21:57.960 --> 00:22:05.000]   was when Apple operates in another country, it doesn't matter what we say. It's what China says.
[00:22:05.000 --> 00:22:10.840]   So when China said to Apple, no more VPNs in the App Store, Apple didn't have a choice of
[00:22:10.840 --> 00:22:16.120]   complying. They only had a choice of leaving China. And that's a big, Apple's not going to leave
[00:22:16.120 --> 00:22:21.400]   China. It's too big for their business. So what we could say is, well, yeah, Apple should only do
[00:22:21.400 --> 00:22:28.200]   business in the United States. Yeah, well, that's that's the choice we have to make.
[00:22:28.200 --> 00:22:31.480]   Yeah, I mean, I don't think Apple's going to do that.
[00:22:31.480 --> 00:22:39.000]   You know, Google left China, but to be fair, Google didn't have a lot of business in China.
[00:22:39.000 --> 00:22:45.560]   Apple does. But everyone from Western country, if China or Saudi Arabia says, let's just go
[00:22:46.040 --> 00:22:53.800]   gay people in Saudi Arabia, or the Uighurs in China. I mean, it's documented what China is doing
[00:22:53.800 --> 00:22:59.400]   there. But we're in, and there are plenty of companies that are complicit in that that
[00:22:59.400 --> 00:23:06.680]   are work with the US. If someone wants to make that their goal, they could come up to Apple and say,
[00:23:06.680 --> 00:23:11.880]   look, you are killing gay people at this theoretical killing gay people in Saudi Arabia.
[00:23:12.360 --> 00:23:17.480]   Don't listen to their government. If everybody in the EU and everybody in the US decided to boycott
[00:23:17.480 --> 00:23:23.880]   them. I mean, part of this is like, but we don't do that. Apple, you know, Apple is doing that in
[00:23:23.880 --> 00:23:31.560]   China. And no one's saying get out of China, Apple, because it's not okay with us. And so this is the
[00:23:31.560 --> 00:23:37.640]   prop. All right, Doc, rock your turn. All right, so where do you come down on this? I'm sure you
[00:23:37.640 --> 00:23:44.440]   have an opinion. Of course I do. I think that solving this issue is bigger than the fear of a
[00:23:44.440 --> 00:23:49.400]   bunch of people in the room freaking out over what might happen later. And here's why. I know that
[00:23:49.400 --> 00:23:57.480]   sounds kind of glib, but here's why. There are people that you know right now that if the emergency
[00:23:57.480 --> 00:24:01.960]   came up, you had to go somewhere, you could take your child to and hand to them and trust them,
[00:24:01.960 --> 00:24:07.560]   just care for that kid for a week. Why? Because the reputation of the relationship you've built
[00:24:07.560 --> 00:24:13.240]   with them and the way that they behave, the way they hold their self, their core values, if you will.
[00:24:13.240 --> 00:24:20.520]   From now, backwards, there has been no other company whose core value has been more about your
[00:24:20.520 --> 00:24:25.800]   privacy than any other large tech companies. And yes, they're not perfect. Nobody is,
[00:24:25.800 --> 00:24:31.960]   but they've been the best in the business. Right. So aren't they're going to take your stuff and do
[00:24:31.960 --> 00:24:37.160]   something with it for you? You know, Joe Schmo in a personal level? No, not going to happen.
[00:24:37.160 --> 00:24:43.240]   Right. If you look at the lovely charts from Jason that you love so much,
[00:24:43.240 --> 00:24:51.560]   like Apple could probably walk away from Saudi Arabia and not care because they cover it sooner
[00:24:51.560 --> 00:24:56.920]   or later. Right. China's a little bit harder, but they also have a better pushback to China too.
[00:24:56.920 --> 00:25:00.280]   Everyone thinks, well, they won't walk away from China because the amount of business they do in
[00:25:00.280 --> 00:25:05.640]   China. Well, guess what? One of the things that they have in their strength is the same amount of
[00:25:05.640 --> 00:25:12.520]   business that they do in China. Apple pulling out of China pulls out billions of dollars from China.
[00:25:12.520 --> 00:25:17.720]   So China's not going to pull that flex any faster than Apple would pull that flex. Everyone loves
[00:25:17.720 --> 00:25:22.360]   to throw that one around. That one makes about as much sense as screen doors in the submarine.
[00:25:22.360 --> 00:25:28.680]   I've been the Fox car. I've literally walked around the campus of Foxconn. I've been in the
[00:25:28.680 --> 00:25:35.640]   building where they're freezing iPads to see if they can handle it. So yeah, no, that city,
[00:25:35.640 --> 00:25:41.080]   Foxconn City itself, where everyone is, there's over a half a million people working on all
[00:25:41.080 --> 00:25:47.800]   mirrory attack, not just Apple stuff. But trust me, no one's walking away from anything. That's like,
[00:25:47.800 --> 00:25:53.960]   when Alan Iverson was one of the biggest jerks in the planet, but the Sixers could not get rid of
[00:25:53.960 --> 00:25:59.720]   them. Who was going to be the guard? Some you have a brother-in-law right now. You can't really stand,
[00:25:59.720 --> 00:26:06.520]   but that's who your sister picked. Never mind. I got to drop that subject. Leo's going to get me
[00:26:06.520 --> 00:26:10.200]   in trouble. Sounds like there's something you want to share with us there, doctor.
[00:26:10.200 --> 00:26:15.480]   But you just know what I'm saying. I do know. Like everyone loves to throw that slippy stuff
[00:26:15.480 --> 00:26:20.520]   around. I agree. I agree. But there's a balance. Come on. No, I think Apple,
[00:26:21.480 --> 00:26:26.520]   China may not say to Apple, look, you got to do this if Apple really says, yeah, and if we have to,
[00:26:26.520 --> 00:26:29.560]   we're pulling out. Of course, China like Google pull out.
[00:26:29.560 --> 00:26:36.760]   Well, no thing to, because I've been to China many times as well, people get VPS
[00:26:36.760 --> 00:26:40.520]   before Apple had it in the stores. Like Golden Frog was my life when I went there.
[00:26:40.520 --> 00:26:46.760]   It's not that big a deal. China likes to throw that around because it looks good for the newspapers,
[00:26:46.760 --> 00:26:51.000]   but they can't stop all the people trying to listen to what they want to listen to.
[00:26:51.000 --> 00:26:56.120]   Sam, you're going to give us the final thought here.
[00:26:56.120 --> 00:27:07.960]   I agree with most of what both Stacey and Doc had to say. I totally agree that we absolutely
[00:27:07.960 --> 00:27:14.920]   need to do, I think, within reason, whatever we can to address the problem of child abuse.
[00:27:16.040 --> 00:27:27.160]   But I'm troubled by this because for a number of reasons, the things you said about any
[00:27:27.160 --> 00:27:36.600]   individual government, once you have this technology in there, can pressure force a company to
[00:27:36.600 --> 00:27:40.280]   do things with it that was not intended for it.
[00:27:43.000 --> 00:27:50.840]   I think that there's also, because of the work that I do around automated driving as well,
[00:27:50.840 --> 00:27:56.920]   I see a lot of the problems with perception systems. I brought that up originally.
[00:27:56.920 --> 00:28:06.920]   I think that this approach is also problematic. While it may not lead to that many false positives
[00:28:06.920 --> 00:28:12.840]   right now with the way it's being used now, I can definitely see where there is the
[00:28:12.840 --> 00:28:23.800]   potential for a lot of errors in this. I see this every day with perception systems that are being
[00:28:23.800 --> 00:28:31.400]   used in the technologies that I cover as an auto industry analyst. I'm troubled by the use of a
[00:28:31.400 --> 00:28:39.960]   technology that is known and understood by everybody that uses it to be problematic at best.
[00:28:41.480 --> 00:28:49.320]   It's not as reliable as you would want it to be. The potential for misuse of it,
[00:28:49.320 --> 00:28:55.800]   and not just in China or Saudi Arabia or Uganda, but even here in the United States. We have seen,
[00:28:55.800 --> 00:29:05.320]   in recent years, government officials that have wanted to use and abuse technology and
[00:29:06.200 --> 00:29:13.320]   try to force technology companies to do things. While they, to our knowledge at least, haven't
[00:29:13.320 --> 00:29:23.560]   really been successful in forcing that issue yet, I see a huge risk of if we put this technology out
[00:29:23.560 --> 00:29:31.400]   there that future administrations will use it even here in the United States in ways that
[00:29:32.120 --> 00:29:39.960]   we as a citizenry don't want, that it would be undesirable for protecting the rights of
[00:29:39.960 --> 00:29:48.680]   everyone in the population. You have to think that Apple has made this calculus that I would
[00:29:48.680 --> 00:29:53.800]   hope, they strategically have gone down the road that we've just gone down. If they haven't,
[00:29:53.800 --> 00:29:58.600]   then they're not doing their job and have made some sort of peace with themselves about
[00:29:59.160 --> 00:30:02.920]   what they would do should China ask them to do something like this or should
[00:30:02.920 --> 00:30:05.880]   Saudi Arabia. Do you think Apple has said to themselves,
[00:30:05.880 --> 00:30:13.800]   okay, well, we'll say no. Yes, they haven't said no so far to China,
[00:30:13.800 --> 00:30:19.160]   when it comes to VPNs or storing, but surely they've got a lot of data on local servers that
[00:30:19.160 --> 00:30:23.000]   the pan-tour up on has access to. They must understand that they've they've
[00:30:23.000 --> 00:30:26.520]   thou made a step that's going to, they're going to get some pressure.
[00:30:27.160 --> 00:30:33.560]   The New York Times points, let me one more thing then Stacy, the New York Times points out that
[00:30:33.560 --> 00:30:39.560]   US law requires tech companies to flag cases of child sexual abuse to the authorities.
[00:30:39.560 --> 00:30:45.880]   Apple has historically flagged fewer cases than other companies. Last year, for instance,
[00:30:45.880 --> 00:30:53.400]   Apple reported 265 cases to neck mech while Facebook reported 20.3 million
[00:30:54.680 --> 00:31:00.280]   according to the center statistics that enormous gap is in due in part to Apple's decision not to
[00:31:00.280 --> 00:31:06.440]   scan for such material citing the privacy of its users Facebook has. I have to think that maybe
[00:31:06.440 --> 00:31:11.480]   we don't see there's more that we don't see that perhaps Apple has already been under significant
[00:31:11.480 --> 00:31:18.440]   pressure from the US government to do this because well, 265 cases compared to 20.3 million.
[00:31:18.440 --> 00:31:21.320]   There's a pretty big discrepancy. I don't think all the child
[00:31:22.040 --> 00:31:27.240]   pornography is on Facebook and none of it's on Apple. I'm sure that it's comparable. Go ahead, Stacy.
[00:31:27.240 --> 00:31:31.560]   I'm sure there's actually probably more because it is historically private.
[00:31:31.560 --> 00:31:37.800]   Private. I would say I would love for this to open up a broader discussion about what rights
[00:31:37.800 --> 00:31:43.160]   people have on their phones. This has been such a gray area for the last, how long have we been
[00:31:43.160 --> 00:31:54.120]   in doing the internet? Let's say 30 or 40 years where we still haven't figured out the right rules
[00:31:54.120 --> 00:32:01.720]   for setting privacy. I don't spend a lot of time on child porn. It's obviously super upsetting for
[00:32:01.720 --> 00:32:10.440]   me. It's not horrible. Some say that that's of course, think of the children is the refuge of
[00:32:10.440 --> 00:32:21.480]   scoundrel. It is 100% except there is actually a child porn community. There are horrific images.
[00:32:21.480 --> 00:32:31.560]   There are. I would love for this to be a total straw man argument, but it's a straw man with
[00:32:31.560 --> 00:32:38.600]   actual real legs. We have to pay attention to it. We can't just be like, how many is it really?
[00:32:39.640 --> 00:32:48.440]   Or ignore it like that. If we had rules that say, hey, they should follow our constitution.
[00:32:48.440 --> 00:32:54.440]   I know this is only in the US. I'm not talking about the rest of the world. We have treaties that
[00:32:54.440 --> 00:33:00.360]   talk about that. We should build our laws first and then talk to adjust our treaties to meet those.
[00:33:00.360 --> 00:33:08.920]   We have laws for search and seizure. We haven't proactively talked about what that should look
[00:33:08.920 --> 00:33:15.160]   like as a society. We were relying on court cases and we shouldn't be doing that anymore.
[00:33:15.160 --> 00:33:21.640]   We can't afford to be doing it given the technology we're deploying. That's my biggest argument here.
[00:33:21.640 --> 00:33:24.680]   So if this gets us to actually talk about that, go right.
[00:33:24.680 --> 00:33:32.200]   I'm going to stay see. I want to see it come out. I know it's one of those things that's hard to
[00:33:32.200 --> 00:33:37.080]   say. We'll handle that bridge when we come to it. But imagine a good that could happen
[00:33:37.080 --> 00:33:42.200]   between now and say two years from now when something that becomes a hassle comes up.
[00:33:42.200 --> 00:33:46.200]   If we could take out, let's just use that Facebook number.
[00:33:46.200 --> 00:33:52.680]   So we take the Apple number and we match the Facebook number. Look at how much great we've done.
[00:33:52.680 --> 00:33:57.320]   Then if something happens three years from now, where we got to argue with some government or whatever.
[00:33:57.320 --> 00:34:01.000]   So what? If they're doing 23 million a year and then we get to add,
[00:34:01.000 --> 00:34:04.680]   well, I think that's what you said. We add another 23 million a year to that.
[00:34:04.680 --> 00:34:08.040]   So now we're at 46 million a year that we're just squashing.
[00:34:08.040 --> 00:34:12.040]   And then we double that. I'm sorry, triple that because hey, we're going to do this three years
[00:34:12.040 --> 00:34:16.760]   before we run into the first issue. I'll take that. It's just one of those things.
[00:34:16.760 --> 00:34:21.640]   It's like, Russian, sooner or later, you're going to be wrong. But let's not
[00:34:21.640 --> 00:34:27.560]   not play it because we might be wrong way down the lane. The other thing I think we glazed over
[00:34:27.560 --> 00:34:32.760]   really quick and double check my math on this. If you disable the iCloud, the In theory is not
[00:34:32.760 --> 00:34:35.080]   supposed to work, right? That's what it said in the paper.
[00:34:35.080 --> 00:34:43.400]   Just the part of the iCloud storage, the transiting using messages, I know, I thought that was odd.
[00:34:43.400 --> 00:34:45.720]   And maybe I have to become more clear on that.
[00:34:45.720 --> 00:34:51.320]   There, Apple's feature, which scans photos and text messages,
[00:34:51.320 --> 00:34:56.920]   will be available only to families with joint iCloud accounts. If parents turn it on,
[00:34:56.920 --> 00:35:01.320]   their child's iPhone will analyze every photo received or sent a text message.
[00:35:02.600 --> 00:35:07.320]   To determine if it includes nudity. Now, this is, I don't know if the time's got this wrong,
[00:35:07.320 --> 00:35:11.640]   or if this is, this is not what I read. Nude photos sent to a child will be blurred,
[00:35:11.640 --> 00:35:16.840]   and the child will have to choose whether to view it. That's a whole different thing.
[00:35:16.840 --> 00:35:21.240]   I don't know if the time's got that wrong. Did the times get that wrong?
[00:35:21.240 --> 00:35:27.080]   That's puzzling to me. I mean, that's not involving that. That's not involving the hashes
[00:35:27.080 --> 00:35:31.000]   or anything. That's not from the database. That's a whole different. That's just doing
[00:35:31.000 --> 00:35:35.000]   perception on the incoming images. I don't think that that's something to decide.
[00:35:35.000 --> 00:35:38.680]   I think the time's got that wrong. They're not alone. I think the Washington Post also
[00:35:38.680 --> 00:35:44.280]   got this wrong, but it's unclear to me. Did Apple announce two things that they were going to do?
[00:35:44.280 --> 00:35:47.320]   One on child pornography, both of which to address, like,
[00:35:47.320 --> 00:35:53.240]   saving naked pictures of over two liners? I thought it was all about
[00:35:53.240 --> 00:35:58.200]   fingerprinting. So I understand why people are confused.
[00:35:59.880 --> 00:36:03.640]   The Washington Post says Apple is prying into iPhones to find sexual predators.
[00:36:03.640 --> 00:36:09.560]   The moves aimed at preventing predators and pedophiles from using Apple's services raised
[00:36:09.560 --> 00:36:17.240]   some civil liberties concerns. See, the whole, they're prying into iPhones. That just automatically
[00:36:17.240 --> 00:36:21.320]   puts it in the negative. It applies that they're actually, that Apple's actually looking at it.
[00:36:21.320 --> 00:36:24.920]   And it's not. It's not on device. See, I can't stand when they do stuff like that,
[00:36:24.920 --> 00:36:29.000]   because that's what the regular people read, right? We get to read each other, right? We have
[00:36:29.000 --> 00:36:32.840]   each other to bounce ideas off, talk these things out, try to come up with some things.
[00:36:32.840 --> 00:36:38.120]   And then we spend years trying to un-program the one story that grandma read,
[00:36:38.120 --> 00:36:44.440]   or granddad read on the paper. And Washington Post, hey, that's the paper, right? Well,
[00:36:44.440 --> 00:36:48.840]   the second paper. So yeah, don't write headlines like that. That's irritating.
[00:36:48.840 --> 00:36:55.240]   But I have to say, John Gruber, who did a very, I think, a very good job of breaking it all down,
[00:36:55.240 --> 00:37:05.080]   concludes, will Apple actually flatly refuse any and all such demands? If they do, it's all good.
[00:37:05.080 --> 00:37:10.040]   If they don't, and these features creep into surveillance for things like political dissent,
[00:37:10.040 --> 00:37:17.400]   copyright infringement, LGBT imagery, or adult pornography, anything at all beyond the irrefutable
[00:37:17.400 --> 00:37:24.840]   sea-sam, it'll prove disastrous to Apple's reputation for privacy protection. The stakes are
[00:37:24.840 --> 00:37:29.400]   incredibly high, and Apple knows it. Whatever you think of Apple's decision to implement these
[00:37:29.400 --> 00:37:33.960]   features, they're not doing so lightly. I have to think Apple does not want to do this,
[00:37:33.960 --> 00:37:40.200]   that this is not something Apple ever wanted to do, but Apple was under intense pressure to do so
[00:37:40.200 --> 00:37:44.920]   from somebody. And finally decided, all right, we're going to do what we're going to do. We're
[00:37:44.920 --> 00:37:50.680]   going to do this now because we think we can do it in a way that protects privacy. And we will
[00:37:50.680 --> 00:37:57.160]   worry about what happens down the road, down the road, that they compromised in other words.
[00:37:57.160 --> 00:38:01.800]   But they must have seen the potential. The idea of somebody calling up Apple saying,
[00:38:01.800 --> 00:38:09.720]   please send me images of all of the alphabet folks, so that I can go do something heinous to them.
[00:38:11.240 --> 00:38:17.000]   What do you think Tim is going to say to that? Our chatroom says,
[00:38:17.000 --> 00:38:21.480]   they're searching your data. It's no different than the secret police searching all homes.
[00:38:21.480 --> 00:38:27.640]   Not even the same thing. How is it? They're not looking.
[00:38:27.640 --> 00:38:33.640]   It's not a person, one, it's not like the secret police. And if it were the secret police,
[00:38:33.640 --> 00:38:40.360]   it would be like the secret police having blindfolds on and listening for a specific tone of
[00:38:40.360 --> 00:38:44.600]   something guilty. And if they hear it, they then take their blindfold off and take it.
[00:38:44.600 --> 00:38:47.080]   That's a terrible analogy.
[00:38:47.080 --> 00:38:50.600]   It's terrible, but it makes more sense than just me.
[00:38:50.600 --> 00:38:52.520]   That's actually not searching me.
[00:38:52.520 --> 00:38:59.080]   That's actually not the worst analogy. I mean, that's not that different from the kinds of things
[00:38:59.080 --> 00:39:07.720]   that happen, for example, in East Germany, under the Stasi. They monitored people all the time
[00:39:07.720 --> 00:39:13.640]   listening for stuff. So I think it's not, this is something that has happened before.
[00:39:13.640 --> 00:39:23.880]   What if AT&T had a technology? Actually, we think this was actually being done for some years
[00:39:23.880 --> 00:39:31.400]   under the sobriquet. Was it Envoy? That the five eyes were scanning,
[00:39:31.960 --> 00:39:37.480]   kind of impersonally in an automated fashion, all phone conversations looking for words like
[00:39:37.480 --> 00:39:43.320]   bomb and C7 and various terrorist phrases. And they were collecting those phone calls in a
[00:39:43.320 --> 00:39:48.920]   database and were then later acting in some way on those. That happened.
[00:39:48.920 --> 00:39:53.480]   Yeah, that technology existed, I believe. US always denied it, but with Great Britain,
[00:39:53.480 --> 00:39:56.520]   I think it finally said, yeah, well, yeah, we've been doing that.
[00:39:56.520 --> 00:39:59.720]   AT&T admitted they were working with NSA to do that.
[00:40:01.400 --> 00:40:05.800]   So, and I think the country agreed that's a bad idea.
[00:40:05.800 --> 00:40:10.760]   Right, so going back to what Susan said, these things are already in place, people.
[00:40:10.760 --> 00:40:15.400]   Don't get it twisted. Don't make like, and even if Elba said, okay, we scrapped this idea, we're out.
[00:40:15.400 --> 00:40:21.880]   Everybody else is doing it. So at least it'd be nice to have the most ethical of them all in the
[00:40:21.880 --> 00:40:25.000]   party, because it's happening whether we like it or not.
[00:40:27.000 --> 00:40:34.760]   Here's, to me, this boils down into an interesting question, which has come up many times before.
[00:40:34.760 --> 00:40:42.200]   This is a whole new category we have, the smartphone, where we are storing our lives for the first
[00:40:42.200 --> 00:40:50.600]   time in a single electronic device that is internet connected and is effectively controlled by the
[00:40:50.600 --> 00:40:55.880]   companies that make them. Let's not deny that Apple and Google have control over these devices
[00:40:55.880 --> 00:41:02.200]   in a way that perhaps if you thought about it, you really wouldn't want your secret store of all
[00:41:02.200 --> 00:41:10.040]   your private stuff in their hands, but you do. But that's because case law has never dealt with
[00:41:10.040 --> 00:41:18.200]   that before. This is a new, this is kind of unknown territory for all of us, including the courts
[00:41:18.200 --> 00:41:22.520]   and the legal system and these companies. We are storing everything in this phone.
[00:41:22.520 --> 00:41:28.280]   And I think many privacy advocates, maybe I'll include myself, have said there needs to be a much
[00:41:28.280 --> 00:41:35.880]   higher standard of protection for these devices because we put so much in here, because we're storing
[00:41:35.880 --> 00:41:41.000]   so much in here. Can I add a nuance to your argument here? Yes. Because it's important.
[00:41:41.000 --> 00:41:45.800]   Not only are we putting everything in our phone, a lot of us are uploading it to the cloud where
[00:41:45.800 --> 00:41:51.720]   it will stay literally forever, combine that with the fact that we have enough cheap computing
[00:41:51.720 --> 00:41:56.760]   that you can analyze this data in ways that have never been analyzed and tie it together.
[00:41:56.760 --> 00:42:02.680]   That's right. In ways that implicate or so. So it's not just that we're putting more of our
[00:42:02.680 --> 00:42:09.480]   lives on our phone. It's that it's it stays there for the entirety of our life. And it's really easy
[00:42:09.480 --> 00:42:15.800]   to mine that in a way that it has never been before. Yeah. That's all. Yes. And to your point,
[00:42:15.800 --> 00:42:20.280]   the other thing I run through this and try to explain to my family members about why they need
[00:42:20.280 --> 00:42:27.640]   complicated passwords. And I showed them, you know, sort of entropy screens. And I literally
[00:42:27.640 --> 00:42:31.480]   really had this conversation with my 11 year old niece. And once I explained it to her,
[00:42:31.480 --> 00:42:36.200]   she actually understood it probably better than the people who needed the most.
[00:42:36.200 --> 00:42:41.400]   But explain it to her how I could just go to Amazon and rent a couple computers to just go
[00:42:41.400 --> 00:42:45.560]   through all of the passwords on your computer and probably get that done before we have dinner.
[00:42:46.360 --> 00:42:50.360]   It's different from where everyone thinks there's a guy in the basement sitting there just trying
[00:42:50.360 --> 00:42:55.720]   all these passwords one at a time until they get it. And I'm like, no, man, I could scan
[00:42:55.720 --> 00:43:00.680]   everything with a couple computers from Amazon, cost me about five or six bucks to rent after a
[00:43:00.680 --> 00:43:04.920]   little bit and just run through everything in your computer and know everything. So you're
[00:43:04.920 --> 00:43:10.040]   right to that point. And it's only getting better. But then it also makes me remember to what Sam was
[00:43:10.040 --> 00:43:15.960]   talking about earlier, we are at the infancy of sort of neural computer as it is and a computing,
[00:43:15.960 --> 00:43:21.800]   sorry, and it's getting better. So the stuff that we're at with the false positive noun and where
[00:43:21.800 --> 00:43:27.560]   we are a perception of imagery now, all of that stuff is going to be in a Moore's Law curve
[00:43:27.560 --> 00:43:34.680]   on the way up for the next five or six years as well. So we are sort of real time battling both
[00:43:34.680 --> 00:43:36.280]   ends, if that makes any sense.
[00:43:36.280 --> 00:43:45.640]   My I think the issue is that law enforcement and people who are sworn to protect us who really
[00:43:45.640 --> 00:43:51.640]   want to genuine and I'll say this genuinely want to stop child pornography, but also protect us
[00:43:51.640 --> 00:43:59.080]   against all sorts of harms. Look at this smart device as a treasure trove to do that. Like we've
[00:43:59.080 --> 00:44:06.040]   never had anything like this before. And we desperately want to use it because we have, by the way,
[00:44:06.040 --> 00:44:12.600]   we have lots of laws and, you know, child pornographers get arrested all the time and go to jail as they
[00:44:12.600 --> 00:44:20.680]   should using existing techniques and technologies and laws. There is this kind of, I think false,
[00:44:20.680 --> 00:44:26.520]   use the word straw man, this is false straw man, this one does not have legs that
[00:44:26.520 --> 00:44:33.560]   these modern technologies allow criminals to really act out because they can now hide what they're
[00:44:33.560 --> 00:44:37.960]   doing. I don't know if that's actually the case. In fact, I think there's a good case to be made
[00:44:37.960 --> 00:44:43.320]   that modern technologies have given police more access. I think they have I think they have,
[00:44:43.320 --> 00:44:49.400]   I think they have a going dark problem, really. One thing to think about and I'll just make this
[00:44:49.400 --> 00:44:58.440]   real personal for everyone who has children. Not, not, but think about baby monitors. And I do this
[00:44:58.440 --> 00:45:03.720]   a lot in this like my whole thing is the smart home. So we have put sensors around our house and
[00:45:03.720 --> 00:45:09.880]   as my daughter grew up, I had to really think about the type of sensors and when was the right time
[00:45:09.880 --> 00:45:17.240]   to take like a camera out of her room. And I think the government, if you think about it, I mean,
[00:45:17.240 --> 00:45:21.080]   yes, and I'm paternalistic in the sense that I have a child that I'm responsible for. The government
[00:45:21.080 --> 00:45:25.720]   is somewhat paternalistic in the sense that they're trying to look out for the good of their citizenry.
[00:45:25.720 --> 00:45:32.200]   So we have all these tools and it's really interesting the debate over when and how we choose to deploy
[00:45:32.200 --> 00:45:38.520]   it. Some parents will deploy like keystroke loggers on their kids' bone or, you know, monitor every
[00:45:38.520 --> 00:45:44.760]   single site they, I mean, the levels are different for everyone. But if you think about the way you
[00:45:44.760 --> 00:45:53.560]   treat your child's, how you safeguard your child with increasing ability, thanks to technology,
[00:45:53.560 --> 00:45:57.720]   this is the same thing that government's trying to do, which is why I think we need to have these
[00:45:57.720 --> 00:46:03.400]   like discussions about where we want to draw the lines. Yeah. Okay. And I think that it's also not
[00:46:03.400 --> 00:46:11.960]   unreasonable. I know we don't like the slippery slope argument, but historically, the fact that,
[00:46:11.960 --> 00:46:17.720]   you know, we've seen that slippery slope happen, that these phones exist with all sorts of information,
[00:46:17.720 --> 00:46:25.080]   law enforcement has given more and more tools, more and more power to investigate. And somebody,
[00:46:25.080 --> 00:46:30.280]   there's a good thread, Alex Stamos has a good thread on Twitter about this and somebody named David
[00:46:30.280 --> 00:46:36.360]   responding to him said, you could catch and prevent all sorts of crime. If you installed a machine
[00:46:36.360 --> 00:46:41.480]   learning solution and everyone's devices to monitor their activity for signs of trouble and relay
[00:46:41.480 --> 00:46:47.960]   them to law enforcement, but we consider such ideas unconscionable. And that's the problem is,
[00:46:47.960 --> 00:46:53.160]   law enforcement always going to, for now law enforcement is always going to want more access.
[00:46:53.160 --> 00:47:00.200]   This is a, this phone is a treasure chest for them. So they're always going to be pushing for more.
[00:47:00.200 --> 00:47:08.680]   And I don't think it's unreasonable for people to say, let's not give in too easily to this,
[00:47:08.680 --> 00:47:13.960]   even with this very justifiable reason for this. Yeah, I think that's it. It's not just a phone.
[00:47:13.960 --> 00:47:18.760]   It's happening everywhere. Yeah. And you know, one thing that I thought I would say,
[00:47:18.760 --> 00:47:24.280]   I would real quick, I'll make this super fast. I think we also give a little bit too much credit
[00:47:24.280 --> 00:47:30.040]   to government and law enforcement for what their capabilities are. They have the hardest time
[00:47:30.040 --> 00:47:36.200]   making the decision on what time to have lunch. Let's just get that square. If you ever want to see
[00:47:36.200 --> 00:47:42.680]   what pure law enforcement bungling was, go Netflix right now and watch the story of
[00:47:42.680 --> 00:47:49.560]   Willie and Sal from Miami. Okay. And the reason why I bring that specifically, growing up as a kid
[00:47:49.560 --> 00:47:56.440]   in that exact timeframe, all we ever heard was the fear and like there was all these cop shows about,
[00:47:56.440 --> 00:48:01.480]   you know, the narcos taking out, you know, these drug dealers and the war on drugs. What's this,
[00:48:01.480 --> 00:48:08.040]   it was this very, very talked about at our, you know, my age bracket. Those guys went basically
[00:48:08.040 --> 00:48:15.320]   undetected for like 25 years. They were the largest importers of lizard drugs in the nation. And
[00:48:15.320 --> 00:48:22.040]   nobody even knew who they were. So it's funny. But I think we might be giving our law enforcement
[00:48:22.040 --> 00:48:26.440]   a little bit too much credit and the stuff that they would be able to do to become the stopo-like,
[00:48:26.440 --> 00:48:31.480]   as everyone likes to say, we're at a different time and place. I don't think they have as much
[00:48:31.480 --> 00:48:37.960]   power as people think they do. All right. And I wish to be very clear. Nobody is that
[00:48:37.960 --> 00:48:43.800]   advocating child abuse or child pornography. And that's the problem with using this as the
[00:48:43.800 --> 00:48:49.000]   reasoning for this is if you say, no, no, no, we want to protect privacy, people assume that you're
[00:48:49.000 --> 00:48:56.600]   saying, screw the children, that's not what anybody is saying. It's just a very difficult problem,
[00:48:56.600 --> 00:49:02.840]   because, and again, it's it is a slippery slope. It is the beginning of some sort of
[00:49:02.840 --> 00:49:08.200]   surveillance on a device that contains the most private information in your life. And
[00:49:08.200 --> 00:49:13.640]   do we want to where do we where do we draw the line, I guess, is the question. Go ahead, Sam.
[00:49:13.640 --> 00:49:22.440]   You know, Doc's right. You know, I agree that we probably do give, you know, law enforcement and
[00:49:22.440 --> 00:49:30.920]   government more credit for their capabilities than is probably deserved. But on the flip side of that,
[00:49:30.920 --> 00:49:36.920]   you know, there that doesn't mean that there's no one within government, or, you know, the said
[00:49:36.920 --> 00:49:42.440]   earlier, you know, that there won't be someone within government that will have the capability or
[00:49:42.440 --> 00:49:51.000]   the desire to use it and misuse it. And having that technology available to them, I think, you know,
[00:49:51.000 --> 00:49:56.520]   does set a very dangerous precedent in terms of the kind of society we want to live in. You know, I
[00:49:56.520 --> 00:50:03.640]   think life inherently has some risk associated with it. And I'm not saying that, you know,
[00:50:03.640 --> 00:50:09.720]   that we want to put our kids at risk, absolutely not. But, you know, I don't think we can,
[00:50:09.720 --> 00:50:17.160]   I don't think we can ever, you know, get to a completely utopian society where nobody does
[00:50:17.160 --> 00:50:24.120]   anything wrong. But I think, you know, letting, letting our leaders, you know, whether, you know,
[00:50:24.120 --> 00:50:31.560]   whether they're in government or law enforcement, get the tools to, to try to achieve that,
[00:50:31.560 --> 00:50:37.080]   I think will inherently invite the potential for abuse of that, you know, whether it's at,
[00:50:37.080 --> 00:50:42.040]   you know, on a large scale or on a small scale, I don't think that that's the kind of abuse that
[00:50:42.040 --> 00:50:47.960]   we want to try to that we want to allow that we want to enable with, with those in power.
[00:50:47.960 --> 00:50:52.440]   All right, it was a great conversation. And we're obviously not going to solve this problem. I
[00:50:52.440 --> 00:50:56.840]   suspect this is going to be a conversation that will appear on every show on the network this
[00:50:56.840 --> 00:51:01.240]   week. In fact, it's all we're going to read about for some time to come. I wonder if Apple will
[00:51:01.240 --> 00:51:07.480]   respond to this pressure or just go full speed ahead, which is typically what Apple does. They
[00:51:07.480 --> 00:51:14.040]   just kind of ignore. I have to say, Nick Mick's response, which is telling Apple ignore the
[00:51:14.040 --> 00:51:21.640]   shrieking protest and go full speed ahead was a little tone deaf. You know, there, this is a
[00:51:21.640 --> 00:51:25.400]   kind of I think you're exactly right, Stacy. This is something we have to talk about. We have to
[00:51:25.400 --> 00:51:29.560]   figure out we have to think about deeply. I'm sure Apple has, but they need to talk to us a little
[00:51:29.560 --> 00:51:34.360]   bit more about their thinking. And I would like to hear assurances that we're going to draw the
[00:51:34.360 --> 00:51:42.600]   line. This is not going to this. This isn't the beginning of incremental loss of privacy across
[00:51:42.600 --> 00:51:48.760]   the board. You'll and you'll have to do the same things we did with. And these are breaking down.
[00:51:48.760 --> 00:51:53.880]   This is why we also need to address this, like the FISA courts and, you know,
[00:51:53.880 --> 00:51:59.320]   we have to set parameters in place so people don't abuse it because we know they will.
[00:51:59.320 --> 00:52:10.360]   But just to say that we're here at this juncture, it is scary. And we know that if we change things,
[00:52:10.360 --> 00:52:15.480]   things are going to change and some people will abuse it. We have to decide if we're going to move
[00:52:15.480 --> 00:52:22.520]   forward or not and fix this. Yeah. It's really challenging. I'm glad we had this talk. I'm glad
[00:52:22.520 --> 00:52:28.040]   we had this talk. We'll have more in just a little bit. Stacy is here. It's great to have her Stacy
[00:52:28.040 --> 00:52:35.320]   Yigabotham for this week in Google and Stacy on IoT, her great IoT podcast. Doc Rock, the creator,
[00:52:35.320 --> 00:52:43.080]   YouTube.com/docrock with his purple mic, purple everything. Yes. Black outfit with purple. I like
[00:52:43.080 --> 00:52:49.480]   it. It's a good black and purple. Good combination. Vibranium. Vibranium, really? That it? All right.
[00:52:49.480 --> 00:52:53.800]   No, no. I support the Alzheimer's.org. Oh, that's right. Alzheimer's is purple.
[00:52:53.800 --> 00:52:59.240]   Alzheimer's. Yeah. While I tell you about Kevin Rose's new podcast, Kevin came to us and said,
[00:52:59.240 --> 00:53:03.880]   "You got to tell him about modern finance." I said, "Kevin, I know about modern finance. I'm a huge
[00:53:03.880 --> 00:53:10.120]   fan." In fact, you'll remember this Stacy when we started talking about NFTs, which was all the
[00:53:10.120 --> 00:53:17.640]   rage a few months ago. I was confused. But the very first episode of MoFi, as we fans of the podcast
[00:53:17.640 --> 00:53:24.120]   call, was all about NFTs and it made it very clear to me. The investment world, the modern
[00:53:24.120 --> 00:53:29.560]   finance has changed dramatically. Not just NFTs, there's bitcoins, there's robo investors.
[00:53:29.560 --> 00:53:35.960]   How do you know how it works? What's right for you? What it even means, Kevin Rose is the guy to
[00:53:35.960 --> 00:53:43.320]   explain it. Modern finance helps to demystify crypto, decentralized finance, and more. Of course,
[00:53:43.320 --> 00:53:49.720]   you know Kevin Rose from his years on the screen savers on tech TV and he's been on all of our shows
[00:53:49.720 --> 00:53:55.560]   many times. We love having Kevin on whenever he can. He's a venture capitalist. In fact,
[00:53:55.560 --> 00:54:00.840]   according to Bloomberg, one of the top 25 angel investors in the world, he currently had true
[00:54:00.840 --> 00:54:05.720]   ventures. He was a Google Ventures for a while. He has his own startups, a number of them, all
[00:54:05.720 --> 00:54:11.240]   of which were very successful. Time magazine called him one of the top 25 most influential
[00:54:11.240 --> 00:54:17.880]   people on the web. I think he's, I consider him a true friend, but also I think he's very,
[00:54:17.880 --> 00:54:23.800]   very good at interviewing people and explaining things. This is a show, if you're interested in
[00:54:23.800 --> 00:54:29.480]   crypto, this is the show for novice and expert alike. Modern finance, they do two, something
[00:54:29.480 --> 00:54:36.280]   interesting to two shows on the one feed. They do a weekly consensus episode that's exploring
[00:54:36.280 --> 00:54:42.280]   weekly news, explains what it all means, makes it breaks it down for you. The other is an interview
[00:54:42.280 --> 00:54:48.200]   episode with individual crypto founders, NFT artists and others. So between the two, you're
[00:54:48.200 --> 00:54:55.320]   going to get a very good understanding of what's going on in modern finance. Modern finance will
[00:54:55.320 --> 00:55:00.040]   equip you to discuss and understand the crypto landscape and maybe more importantly,
[00:55:00.040 --> 00:55:05.960]   know whether or not to invest and how you should invest and what the risks are. Feel
[00:55:05.960 --> 00:55:10.680]   informed about what you're getting into. Don't miss out on the next big thing. Join Kevin Rose
[00:55:10.680 --> 00:55:18.280]   on modern finance every week. You don't want to miss a beat, the modern finance podcast. I,
[00:55:18.280 --> 00:55:24.520]   you know, mofai.net is the website. I call it mofai, but it probably best to search for modern
[00:55:24.520 --> 00:55:30.600]   finance on your favorite podcast application. Makes sense of all the coins and the chaos.
[00:55:30.600 --> 00:55:36.280]   The financial landscape is harder than ever to navigate, but you don't have to do it alone.
[00:55:36.280 --> 00:55:41.400]   Download and subscribe to modern finance wherever you listen to podcasts. That's modern finance
[00:55:41.400 --> 00:55:46.280]   on every podcast player known to me. Don't be the last person on the next train out.
[00:55:46.280 --> 00:55:52.520]   Listen to modern finance and get ahead of the future of finance. I'm really proud of Kevin. In
[00:55:52.520 --> 00:55:57.640]   fact, I strongly suggest not only subscribing to get the new episodes, but going back and listening
[00:55:57.640 --> 00:56:04.360]   to some of the earlier episodes, that first episode on NFTs, as I said, absolutely clarified
[00:56:04.360 --> 00:56:11.080]   this in my mind. He's very good at explaining this stuff. Modern finance mofai. I love it.
[00:56:11.080 --> 00:56:16.200]   What do you think, Stacey? Am I a optimist or a pessimist?
[00:56:16.200 --> 00:56:22.200]   I think I can't tell because you're often a devil's advocate. So I really don't know.
[00:56:23.160 --> 00:56:29.480]   Good. I truly don't. I feel like I want to believe that you think people are inherently good,
[00:56:29.480 --> 00:56:34.360]   but you're aware that they're foibles and sometimes they sometimes suck. I'm very
[00:56:34.360 --> 00:56:39.080]   I'm absolutely acutely aware of the paradox of human nature. I was going to say,
[00:56:39.080 --> 00:56:43.480]   "realist," which is kind of the same. People say, "Hey, is he half full or half empty?"
[00:56:43.480 --> 00:56:46.760]   I'm like, "It's always full." And I'm like, "What do you mean?" Because the other half is aired.
[00:56:48.520 --> 00:56:52.760]   Actually, you're kind of like the engineers. So they say, "The optimist says a glass is half full.
[00:56:52.760 --> 00:56:57.880]   The pessimist says the glass is half empty. The engineer says the glass is the wrong size for the
[00:56:57.880 --> 00:57:04.760]   water." Obviously, you got to redesign the glass. You got to redesign the glass. Retool everything.
[00:57:04.760 --> 00:57:13.400]   I like to say that I'm hopeful, but not optimistic. Oh, God. I honestly, and I know Stacey, maybe you
[00:57:13.400 --> 00:57:17.800]   missed it. No, I think you were here for it. I'm starting to get more and more dejected over the
[00:57:17.800 --> 00:57:21.720]   state of the world, but that might just be something that I'm going to say. I think that's just the
[00:57:21.720 --> 00:57:26.680]   state of the world right now. Maybe it is. I feel like 2020 really just threw us off to like,
[00:57:26.680 --> 00:57:29.960]   I used to be an optimist. Then, happened.
[00:57:29.960 --> 00:57:39.400]   It's a combination of that and getting older. I didn't want to be the old cranky old man yelling
[00:57:39.400 --> 00:57:44.440]   at the clouds. I always thought, "I can't. Me neither. I never wanted to be that back in my day
[00:57:44.440 --> 00:57:49.960]   and now I find myself saying it all the time." The other one that's really classy, I find myself
[00:57:49.960 --> 00:57:55.880]   doing this. Hey, you see my glasses? Which my dad used to do? We all thought it was her name.
[00:57:55.880 --> 00:57:59.160]   It's on your head. They're right on my head. Always. It cracks me up now.
[00:57:59.160 --> 00:58:06.680]   Do you do that? Yes. It's kind of scary. I'm just like, my dad's probably looking down going,
[00:58:07.960 --> 00:58:13.560]   would you say, "No, homie?" I visit my dad on Monday. I hadn't seen him in a while. He's 88.
[00:58:13.560 --> 00:58:18.360]   I visited him for his 80th birthday. I wrote him when I was saying, "Every day,
[00:58:18.360 --> 00:58:25.400]   I see more and more of you in me." That's really true. As I get older, I go, "I look in the mirror.
[00:58:25.400 --> 00:58:31.160]   I go, "Dad, what are you doing there?" Scary. I think someone from your audience sent me
[00:58:32.920 --> 00:58:36.840]   a mechanical guy. Oh, I'm not lighter. Somebody sent you a keyboard?
[00:58:36.840 --> 00:58:41.720]   Oh, my God. A tester. Oh, a tester. Oh, those are great.
[00:58:41.720 --> 00:58:45.000]   Oh, is that how you're using it? On the third row, all the way to the right,
[00:58:45.000 --> 00:58:49.640]   I have those Gateron Browns. They're a nice combination. I like the M-T-R-A.
[00:58:49.640 --> 00:58:52.680]   The third row, all the way to the end. Wow, you really know your head.
[00:58:52.680 --> 00:58:56.840]   This is the hard part. Which way do I do it? This way?
[00:58:56.840 --> 00:59:02.600]   I'll just look for anyone that's brown on the edge and three. I just saw it.
[00:59:02.600 --> 00:59:04.280]   On the edge of three. I don't have that option.
[00:59:04.280 --> 00:59:10.920]   So, are these Cherry MX switches? No, this is a bunch of options.
[00:59:10.920 --> 00:59:12.600]   Oh, it's not just Cherry switches.
[00:59:12.600 --> 00:59:16.040]   Cherry's and other brand. Other brand.
[00:59:16.040 --> 00:59:19.800]   There's Alps. There's Cherry. Gateron.
[00:59:19.800 --> 00:59:25.400]   Gaterons are definitely in there. People used to say Cherry was the best,
[00:59:25.400 --> 00:59:27.640]   but I think there's better ones now than Cherry, right?
[00:59:27.640 --> 00:59:33.000]   What do you like? Yeah, Cherry's kind of older, but my favorite thing about Gateron Browns
[00:59:33.000 --> 00:59:36.360]   is they have the feeling of blues, which all of us old DOS keyboard,
[00:59:36.360 --> 00:59:39.400]   Matthias keyboard, people that are coming in that blue, red game.
[00:59:39.400 --> 00:59:44.680]   It has that same feeling, but not as noisy. And it's super funny that I have this on my desk,
[00:59:44.680 --> 00:59:48.840]   because my project is to take the sucker part and lube it, which will make it quieter.
[00:59:48.840 --> 00:59:52.520]   Wait a minute. You shouldn't lube your keys. That sounds like a bad idea.
[00:59:52.520 --> 00:59:54.920]   No, it's a lot of work. You got to take them apart.
[00:59:54.920 --> 00:59:56.520]   What are you lubricating it with?
[00:59:56.520 --> 01:00:00.200]   What's the word? I can't say the word. It's a weird thing.
[01:00:00.200 --> 01:00:04.280]   So I want to say diacrylic, but that might not be the right word.
[01:00:04.280 --> 01:00:05.080]   Dymethical.
[01:00:05.080 --> 01:00:07.800]   The keyboard nerds. There you go. Dymethical.
[01:00:07.800 --> 01:00:10.040]   Wasn't that one keyboard nerd?
[01:00:10.040 --> 01:00:10.040]   Was it that one? Was it that one?
[01:00:10.040 --> 01:00:13.560]   The old contact cold remedy was dymethical? Oh, that's pymethical.
[01:00:13.560 --> 01:00:19.400]   Dymethical is an hair and makeup stuff to make it slippery on your skin and hair.
[01:00:19.400 --> 01:00:22.760]   It's possibly it. I don't want the keyboard people to come after me,
[01:00:22.760 --> 01:00:27.400]   but I watch all the keyboard people on there on YouTube now.
[01:00:27.400 --> 01:00:31.240]   And so I got the keyboard puller. I got everything I need to take apart all these switches.
[01:00:31.240 --> 01:00:34.360]   And I just bought some purple switches, but they're a little smooch.
[01:00:34.360 --> 01:00:38.680]   Doug Emmon, our chatroom says, man, hipster keyboards are as much work as a hipster beard.
[01:00:38.680 --> 01:00:42.920]   Hey, more and more expensive.
[01:00:42.920 --> 01:00:46.760]   You need the comb. You need the wax. You need the special shampoo.
[01:00:48.760 --> 01:00:54.200]   I remember I was going to grow a beard. And this is very typical of me before I do something.
[01:00:54.200 --> 01:00:58.680]   I get all the stuff so that I can do it. So I have beard comb.
[01:00:58.680 --> 01:01:02.360]   I have beard wax. I have beard, all the beard stuff.
[01:01:02.360 --> 01:01:06.520]   And then Lisa said, you're not growing a beard. What are you talking about?
[01:01:06.520 --> 01:01:10.760]   So I have the stuff. I don't have the beard.
[01:01:10.760 --> 01:01:14.200]   I had pushback for like a month. And I was like, just let it go.
[01:01:14.200 --> 01:01:18.200]   Let it go. We see what happens. I had pushback for like a month. This is like 2012.
[01:01:18.200 --> 01:01:19.960]   And now she's okay with me.
[01:01:19.960 --> 01:01:23.320]   Oh, loves it. Loves it. Now she's like, oh, this is the greatest thing.
[01:01:23.320 --> 01:01:28.520]   Yeah, it gets softer. It gets softer. It's only stubbly for like a month or so.
[01:01:28.520 --> 01:01:33.480]   And then it's softer. But the number one thing you can't wash with like regular people,
[01:01:33.480 --> 01:01:35.640]   hair shampoo, special. You have special leadership.
[01:01:35.640 --> 01:01:39.480]   I have it all because there's a big difference. Sam, I don't think it's the
[01:01:39.480 --> 01:01:44.440]   latest much of that going on. Oh, oh, no, we got we have lots of hipsters around here.
[01:01:44.440 --> 01:01:49.000]   That's we have no shortage of that. But this is also why, you know, I just take a beard
[01:01:49.000 --> 01:01:53.560]   tremor through my beard once a week or so. And I don't have to do any other maintenance.
[01:01:53.560 --> 01:01:58.840]   Does Andrew have a beard Stacy quick through? Oh, no. No, no, you wouldn't tolerate that.
[01:01:58.840 --> 01:02:05.000]   He I think he's going to get laser his neck laser. So he never has to shave his neck again.
[01:02:05.000 --> 01:02:10.040]   Yeah. Wow. I know. That's not that.
[01:02:10.040 --> 01:02:15.720]   Makes me sound. I haven't been in the 40 years. Really 40 years you haven't you've had a beard
[01:02:15.720 --> 01:02:21.160]   for 40 years? I'm almost I can't. Because no one knows what you're looking at.
[01:02:21.160 --> 01:02:24.120]   Since I was 18. Yeah, if you shaved people, who the hell is that?
[01:02:24.120 --> 01:02:27.880]   My wife, my wife wouldn't recognize you. She would not recognize you.
[01:02:27.880 --> 01:02:34.680]   She's never seen me without a beard. So my dad in a high school, your book photo.
[01:02:36.120 --> 01:02:41.400]   My dad had a mustache and a beard my entire life. And then he shaved his beard. And we were all
[01:02:41.400 --> 01:02:46.600]   like really weirded out. And then like when I went away to college, I guess at one point in time,
[01:02:46.600 --> 01:02:51.160]   he shaved his mustache. And we did recognize them. I'm pretty sure your family would recognize you,
[01:02:51.160 --> 01:02:56.120]   Sam, but they would be like, but I recognize you, but I don't like it.
[01:02:56.120 --> 01:02:59.880]   I know who you are, but I don't like you.
[01:03:01.240 --> 01:03:07.160]   How about the Pixel six? This is an interesting story because I compare. Yeah. Well, first of all,
[01:03:07.160 --> 01:03:13.400]   so do I, it makes me want one. Yeah. But I compare it to what Apple does, which is zip not nothing.
[01:03:13.400 --> 01:03:18.920]   We know pretty much they'll be an iPhone 13. It'll be next month. And you know, we'll know
[01:03:18.920 --> 01:03:22.920]   basically be thinner, faster, lighter, the best phone we've ever made. We know all that stuff.
[01:03:22.920 --> 01:03:28.920]   But Apple is, you know, they they literally there was a guy in China who was sharing
[01:03:29.480 --> 01:03:35.000]   pictures of the prototypes that Apple sends to case manufacturers. Apple sent him a cease and
[01:03:35.000 --> 01:03:37.800]   assist that we're going to turn you into the Chinese government. You got to tell us where you
[01:03:37.800 --> 01:03:42.840]   got those. I mean, they're dead serious. You don't mess with them. So Google, which has a leak
[01:03:42.840 --> 01:03:47.480]   problem as well has apparently taken the opposite point of view. If you're going to leak it, we
[01:03:47.480 --> 01:03:56.440]   better leak it first. So Google invited a number of journalists. I always call them tame journalists.
[01:03:56.440 --> 01:04:01.480]   People they know are going to be positive about it. In to sit down with Rick Osterlows,
[01:04:01.480 --> 01:04:06.440]   their hardware chief and get a demo. They weren't allowed to record it. They weren't allowed to,
[01:04:06.440 --> 01:04:11.800]   you know, do anything. They basically had to sit and listen to Rick. It was great. I like Rick.
[01:04:11.800 --> 01:04:19.400]   But it ended up being a basically a puff piece for the Pixel 6, but they showed it. We now know
[01:04:19.400 --> 01:04:22.840]   what it looks like. In fact, there's even a page in the Google store of the Pixel 6, even though
[01:04:22.840 --> 01:04:29.640]   it won't be out till October. Is this a good strategy? Who's got the better strategy, Stacey?
[01:04:29.640 --> 01:04:36.760]   Apple or Google? I feel like Google always accidentally leaks their own stuff. I would just,
[01:04:36.760 --> 01:04:40.040]   they recognize they're not good at this. Let's just let them have it.
[01:04:40.040 --> 01:04:46.280]   I think also it's a good way to gin up some excitement over a phone that has not been
[01:04:46.280 --> 01:04:51.880]   exciting for the last two years. You put a conversation space in there. I'm not a conversation space.
[01:04:51.880 --> 01:04:57.720]   You put a placeholder in someone's brain because, yes, I had the benefit of being September 12.
[01:04:57.720 --> 01:05:04.520]   So every year from a birthday, there's a day before, normally the day before or the day of,
[01:05:04.520 --> 01:05:09.560]   somewhere, it starts at about the 10th. So depending on the calendar, I'm normally within a couple
[01:05:09.560 --> 01:05:14.520]   days. And then that's exactly what I get for myself every year. It's always two weeks later,
[01:05:14.520 --> 01:05:18.840]   whatever. But what they've done here for someone who's not in a tech space is not going to buy two
[01:05:18.840 --> 01:05:26.280]   phones. They basically put a conversation holder. And it's like, I was really, really thinking
[01:05:26.280 --> 01:05:31.320]   about getting this car, but now I know this other car is coming or I was thinking about getting
[01:05:31.320 --> 01:05:35.320]   this phone. Now I know this other phone is coming. And we talked about this on Mac
[01:05:35.320 --> 01:05:40.200]   Break Weekly on Tuesday. And somebody in the chat pointed out, Apple, if they pre-enounce,
[01:05:40.200 --> 01:05:45.400]   will kill sales of their existing phone. And they're still selling the iPhone 12,
[01:05:45.400 --> 01:05:52.120]   like hotcakes. Google, Google probably decided we can't get much worse than the sales of the
[01:05:52.120 --> 01:05:57.480]   Pixel 5. They got nothing to lose. They did, by the way, apparently they're going to announce a 5A
[01:05:57.480 --> 01:06:06.520]   in the next couple of weeks, which is bizarre. Well, you said it on weekly, they don't have a
[01:06:06.520 --> 01:06:13.080]   strategy. Their strategy is what should we do? What should we do? They did announce a new chip.
[01:06:13.080 --> 01:06:17.640]   They were very cagey. They didn't really say what I think is the case, which is that Samsung is
[01:06:17.640 --> 01:06:23.320]   making this chip. It will be based on an Exynos, but have tense, they're going to call it the Google
[01:06:23.320 --> 01:06:29.720]   Tensor, because there will be machine learning capability. Of course, TPUs, the Google Tensor
[01:06:29.720 --> 01:06:34.840]   units are machine learning units that Google offers on the cloud. Hence the naming.
[01:06:34.840 --> 01:06:38.040]   Maybe they'll be. Why do you think it's an Exynos?
[01:06:38.840 --> 01:06:46.360]   That seems to be the consensus. Well, we know Samsung is going to make it. I think the consensus
[01:06:46.360 --> 01:06:51.240]   is they're just going to add some capabilities to an existing Samsung sign, but we don't know.
[01:06:51.240 --> 01:06:56.520]   It does say that they did their own SOC as opposed to their own application processor.
[01:06:56.520 --> 01:06:59.320]   Never mind. I was just curious. You know what I'm saying?
[01:06:59.320 --> 01:07:03.480]   And Samsung's making it. So who's doing the CPU?
[01:07:05.000 --> 01:07:08.360]   Samsung probably. It's going to be something that's
[01:07:08.360 --> 01:07:14.440]   arm instruction set based. I guess it could be. Would Samsung make a chip that's got the Qualcomm?
[01:07:14.440 --> 01:07:22.040]   I guess it could. Sure. Yeah. Okay. I mean, Sam, as long as Qualcomm's getting made by Google,
[01:07:22.040 --> 01:07:29.960]   for the design. Yeah. In Samsung, before their Exynos line had gotten all tweaked out, they actually
[01:07:29.960 --> 01:07:33.560]   did some of their phones with Exynos. And some they just continued to use the Qualcomm.
[01:07:33.560 --> 01:07:40.200]   US phones always have Qualcomm because Qualcomm has a patents on LTE and stuff, right? They need
[01:07:40.200 --> 01:07:45.720]   to use that in the US, CDMA. And then in the rest of the world, they usually put Exynos in their own
[01:07:45.720 --> 01:07:50.680]   processor. But yeah, they continue to buy Qualcomm's but only the US. Now that the CDMA
[01:07:50.680 --> 01:07:56.600]   never turned off. Yeah. Yeah. Do you even need that component tree in there?
[01:07:56.600 --> 01:08:03.240]   What's what's Qualcomm going to do? Anyway, they still have plenty of LTE patents.
[01:08:03.240 --> 01:08:07.400]   Yeah. They've got other patents. Plus they've brought it out into WiFi and all this other
[01:08:07.400 --> 01:08:13.960]   things. They make other stuff. What about the bump? So you all three just said,
[01:08:13.960 --> 01:08:20.520]   Oh, I can't wait to get one. But it's K. Google's doubling down on the camera bump.
[01:08:20.520 --> 01:08:24.200]   They're making it the camera bump bar. It's going all the way across the phone.
[01:08:24.200 --> 01:08:29.480]   I think it's a nice design decision around something that you have to deal with. It solves
[01:08:29.480 --> 01:08:34.200]   that problem. Yeah. You know why I like it? I love the next six P.
[01:08:34.200 --> 01:08:41.880]   Yeah. And it had a bar. Did it? You know, when we hold this, right? We have it in the hand, right?
[01:08:41.880 --> 01:08:46.120]   You could actually use that as a thing. You can rest the bar on your I'm
[01:08:46.120 --> 01:08:51.560]   impressed on the bottom of the bump as I hold it in order to get like that because then I won't
[01:08:51.560 --> 01:08:58.520]   put my I always have my finger on the lens. Why? Something about the way the phone naturally is
[01:08:58.520 --> 01:09:02.920]   hold it held is my fingers right there. So having a bump might help with that. Right. Yeah. Put the
[01:09:02.920 --> 01:09:06.440]   DJ is me and just going crazy about what I could do with the bump bar.
[01:09:06.440 --> 01:09:14.440]   It's going to have the bump will have cameras all the way along it. It looks like they're going to
[01:09:14.440 --> 01:09:21.400]   have three lenses. They said, are they all cameras or some of the next six P sensors? Yeah.
[01:09:23.000 --> 01:09:29.320]   When they did the six P, we had images that leaked out months before, as you would expect.
[01:09:29.320 --> 01:09:35.880]   And in those images, it looked like that bump actually stuck out a lot further than actually
[01:09:35.880 --> 01:09:41.960]   did in reality. Right. Yeah. I mean, in this case here, I think it is still pretty prominent,
[01:09:41.960 --> 01:09:48.680]   but it had a bar that went across the entire it was at the top of the phone, not part way down,
[01:09:48.680 --> 01:09:54.520]   but it still effectively had a bar all the way. And it was fine. I had no issues with it. And
[01:09:54.520 --> 01:09:59.560]   yeah, that was that was the first bar is better than that I had for that. I had
[01:09:59.560 --> 01:10:05.240]   other brands. Yeah. When you go to live flat on stuff, the bump just makes it rock,
[01:10:05.240 --> 01:10:09.720]   whereas the bar will not cause as much rocking. It looks like Drasmin's and you're right. People
[01:10:09.720 --> 01:10:17.640]   put Pac. I don't, but there are people, many of them. She's got a pop socket. So
[01:10:17.640 --> 01:10:24.200]   I'm just saying that it's, you know, the bump is not going to interfere with the pop.
[01:10:24.200 --> 01:10:30.920]   It's I it stops me from wireless charging. It doesn't slide into my pocket, right? I don't,
[01:10:30.920 --> 01:10:35.240]   I never understood the I think the bump is going to help you with wireless charging, Stacy,
[01:10:35.240 --> 01:10:39.880]   because again, now your index finger will have a nice little right where to go.
[01:10:39.880 --> 01:10:43.320]   Like guideway. We're going to put a bump in the socket. Yeah.
[01:10:43.320 --> 01:10:49.000]   They'll have to make a new cradle. It's a little shorter to go under the bump. Otherwise it'd be
[01:10:49.000 --> 01:10:53.880]   tilted. I don't know. So mine's a three. So I don't actually have wireless charging.
[01:10:53.880 --> 01:10:59.400]   I'm waiting for this phone. Yes. This is the phone. You all are waiting for actually. I'm like
[01:10:59.400 --> 01:11:04.360]   four or two. Yeah, I have, I bought the four XL and I didn't buy the five because it didn't seem
[01:11:04.360 --> 01:11:11.240]   like much of an upgrade from that. You know, a lot of people complain, you know, a lot of reviewers
[01:11:11.240 --> 01:11:18.920]   complained about the 765 snapdragon in the in the five. I had an issue with it. It's got
[01:11:18.920 --> 01:11:24.600]   it's got plenty of performance. Yeah. The the only issue with it is, you know, when you're when
[01:11:24.600 --> 01:11:29.400]   it's doing the image processing after you take a photo, it takes a little bit longer to process
[01:11:29.400 --> 01:11:34.440]   the HDR. But you know, I'm doing other stuff anyway. I'm not immediately looking at every
[01:11:34.440 --> 01:11:40.440]   image anyway. So I don't doesn't matter. The new 10-sore system on a chip will have
[01:11:40.440 --> 01:11:46.120]   massive improvements in AI, Rick officer load. They said, I almost hate to repeat the
[01:11:46.120 --> 01:11:55.560]   complete marketing BS. But I guess I will that 10-sore unlocks the ability to run data center level
[01:11:55.560 --> 01:12:01.560]   AI models locally on the chip without connecting to the cloud. And he did do a demo. Does that
[01:12:01.560 --> 01:12:08.120]   even mean? All right. That's BS marketing. Talk to me when you can do data.
[01:12:08.120 --> 01:12:14.120]   We're complaining on the phone. Yeah. That's what I like. I'm like data center. I mean,
[01:12:14.120 --> 01:12:19.080]   what is that? You can do it anywhere. Yeah. Your only limit is the number of models you can run.
[01:12:19.080 --> 01:12:25.480]   I mean, you can and the size of your storage because you got your battery. I guess. So
[01:12:25.480 --> 01:12:31.000]   by did demonstrate to you for what it's worth that. I was just gonna say for what it's worth,
[01:12:31.000 --> 01:12:35.880]   you know, that the TPUs have been used in data centers for a long time. And actually,
[01:12:35.880 --> 01:12:44.040]   Google's not the only one that uses TPUs. Nvidia builds TPUs into a lot of their SOCs as well.
[01:12:44.600 --> 01:12:52.440]   So they're very highly regarded for doing that type of the matrix math processing that's required
[01:12:52.440 --> 01:12:56.440]   for processing neural network. Well, that's interesting. I thought so. I think it's a good
[01:12:56.440 --> 01:13:01.880]   thing to put in there. I thought Google owns Tensor Processing Unit. It's a generic word.
[01:13:01.880 --> 01:13:08.440]   Well, I think they developed it. But they designed to license it to Nvidia and others.
[01:13:08.440 --> 01:13:15.400]   Oh, OK. It's kind of like arm. Yeah. Yeah. TensorFlow or whatever they think. TensorFlow.
[01:13:15.400 --> 01:13:20.120]   Yeah. They like to have their own little special words. TensorFlow.
[01:13:20.120 --> 01:13:24.120]   Well, no, TensorFlow is this specific. TensorFlow I think is the right word.
[01:13:24.120 --> 01:13:30.200]   So is TPU. Yeah. Well, TPU is the processing unit is the...
[01:13:30.200 --> 01:13:35.800]   Yeah. Yeah. I didn't mean to sidetrack it. I just hard remembered that Google did it.
[01:13:37.560 --> 01:13:47.640]   So yeah. It's like the neural engine and the Apple Acer or Qualcomm's got their own AI cores.
[01:13:47.640 --> 01:13:54.680]   So it's an AI optimized processing core. So one of the groups that got to go in with
[01:13:54.680 --> 01:14:00.200]   Rick Osterlo, Sam Rutherford from Gizmodo, and he did get a demo of the Tensor chip that I thought,
[01:14:01.080 --> 01:14:07.880]   OK, that's a good selling point. Rick played a video of someone giving a presentation in French.
[01:14:07.880 --> 01:14:13.480]   Now in the past, Google still has live captioning, right? So you could watch that video and it would
[01:14:13.480 --> 01:14:19.000]   live caption it, but it would live caption it in French. Now you can not only run live caption,
[01:14:19.000 --> 01:14:24.440]   but you can also put it in interpreter mode. And the thing can translate in real time while it's
[01:14:25.320 --> 01:14:31.000]   putting a caption up on a video and apparently did quite well. And that's kind of amazing.
[01:14:31.000 --> 01:14:38.440]   That's a reason to get that, I think. There's also a new voice dictation feature in the Google
[01:14:38.440 --> 01:14:43.240]   keyboard, Gboard that lets you speak instead of type while texting. Well, we've always had that,
[01:14:43.240 --> 01:14:49.800]   but the Pixel 6 can correct mistakes in real time. I think that's always been there,
[01:14:49.800 --> 01:14:53.560]   but maybe that's something they showed off. I guess maybe something they're going to do better.
[01:14:54.200 --> 01:15:00.040]   Anyway, so you think this was a good strategy on Google's part because it's got you to pause
[01:15:00.040 --> 01:15:06.760]   instead of buying. Here's the thing Google is going to sell a 5A all of a sudden. They've
[01:15:06.760 --> 01:15:18.280]   announced it. August 26 is the rumor, $450. So maybe there is a phone for them to kill.
[01:15:22.120 --> 01:15:31.800]   Apparently, the 5A is running a Snapdragon 765, which is the same thing that's in the 4A5G
[01:15:31.800 --> 01:15:37.720]   and the 5. So what is the difference between the 4A5G and the 5A?
[01:15:37.720 --> 01:15:41.720]   Always the problem I've had with, I guess the price is the biggest difference.
[01:15:45.480 --> 01:15:50.760]   The 4A5G is what, $4.99? Yeah, this is $450. 50 bucks cheaper.
[01:15:50.760 --> 01:15:58.920]   Well, I think we've come to the conclusion in this segment. Google is just confused as hell.
[01:15:58.920 --> 01:16:04.200]   I think they're phone strategies just optimized for something else that we can't afford.
[01:16:04.200 --> 01:16:08.040]   We can't afford to say that. They're doing that. What is that multi-level
[01:16:08.040 --> 01:16:15.560]   testing or they're doing twins? Boy, they just understand each other. Andy mentioned that he
[01:16:15.560 --> 01:16:19.560]   just wanted Apple to tell us because we're not children. It's not Christmas anymore.
[01:16:19.560 --> 01:16:24.920]   Andy, I love you, but I'm going to fight you on that because I actually missed the days of not
[01:16:24.920 --> 01:16:30.920]   knowing when something was coming and then waking up to this fantastic news. I bet you like
[01:16:30.920 --> 01:16:39.640]   the line for stuff too. No, I don't. I'm not allowed to pre-order. I kind of missed that.
[01:16:39.640 --> 01:16:44.440]   The Apple lines were always fun. You'd meet interesting. They're fun. Yeah, but not just standing,
[01:16:44.440 --> 01:16:48.760]   standing, not doing nothing. The Apple lines were fun because you can't do stuff. We can't doubt.
[01:16:48.760 --> 01:16:54.120]   I was working at Apple, so I got to go up and down the live and hand out water and brownies.
[01:16:54.120 --> 01:16:59.080]   Make people laugh because nobody thought they were going to get anything. And so we were always
[01:16:59.080 --> 01:17:03.960]   cool because we would order like, "Hey, the Starbucks is there. Can you guys bring over 300 ice teas?"
[01:17:03.960 --> 01:17:12.120]   Yeah, we're not joking. 300 ice teens and clean out the pastry cabinet. Just bring that in a couple
[01:17:12.120 --> 01:17:17.880]   of things of coffee. We just have Starbucks feed like the whole line. And every so
[01:17:17.880 --> 01:17:21.880]   shocking awe, right? That was our thing during the John Ronson years was shocking awe.
[01:17:21.880 --> 01:17:27.320]   Do you remember, wasn't it you, John, that we camped out at the Apple store?
[01:17:28.120 --> 01:17:33.080]   That must have been for the four or something. It was a long time ago. We went there the night
[01:17:33.080 --> 01:17:41.160]   before you'd kind of camped out in Santa Rosa on a recliner. That was the last time I think we
[01:17:41.160 --> 01:17:46.440]   waited in line. It was a recliner. Yeah, you got to be comfortable, right?
[01:17:46.440 --> 01:17:52.040]   I think the last line was 4s. If I remember correctly, the last line was 4s. Everything after
[01:17:52.040 --> 01:17:55.720]   that, we've been on like, you know, order ahead of time, get it at home kind of thing.
[01:17:55.720 --> 01:18:03.640]   It is impressive because Apple solved that problem where companies like NVIDIA and Intel and AMD
[01:18:03.640 --> 01:18:11.000]   and others are still suffering and unable to make these devices. Although Tim Cook did warn in the
[01:18:11.000 --> 01:18:18.520]   analyst call last week that the chip shortages for the legacy nodes, the non-Apple custom chips
[01:18:18.520 --> 01:18:25.160]   might in fact cause a short supply for some of these new iPhones when they come out. So that
[01:18:25.160 --> 01:18:31.160]   might happen. Sam, is the chip shortage easing up? I know you cover this now.
[01:18:31.160 --> 01:18:34.680]   Not yet. No, because it's on a motive as well.
[01:18:34.680 --> 01:18:42.120]   It's sitting a lot harder. Yeah. Yeah. Here in southeast Michigan, we've got storage lots full
[01:18:42.120 --> 01:18:47.800]   of tens of thousands of vehicles that are waiting, you know, that are basically built just waiting
[01:18:47.800 --> 01:18:53.240]   to add one more ECU and there are two more ECUs, you know, because they're missing some chips. And
[01:18:53.800 --> 01:18:59.320]   as with Apple, you know, it's oftentimes it's those legacy components. You know,
[01:18:59.320 --> 01:19:05.960]   in an ECU, you'll have a microcontroller in there and you might have three, four, five other chips
[01:19:05.960 --> 01:19:11.080]   on that board, you know, that are often, you know, older style components that could be anywhere from
[01:19:11.080 --> 01:19:18.760]   28 to 90 nanometers, you know, so they're old parts that are still being used. And though
[01:19:18.760 --> 01:19:24.840]   that's often what is missing from a lot of these vehicles. Let me take a little break. We have more
[01:19:24.840 --> 01:19:31.480]   with our fabulous panel, Sam and Bull Sam and our car guy, Stacy Higginbotham, my IOT guy,
[01:19:31.480 --> 01:19:38.520]   and Doc Rock, my rock and roll guy. I don't know. My purple guy, my Ecamm guy.
[01:19:38.520 --> 01:19:46.040]   Our show today brought to you by Zip Recruiter. There are hiring guys, according to Forbes,
[01:19:46.840 --> 01:19:54.600]   gyms, nail salons, mom and pop stores, and more set to go on epic hiring sprees
[01:19:54.600 --> 01:20:00.840]   in the coming months because there's pent up demand for all these services. I know I walk down
[01:20:00.840 --> 01:20:06.920]   the street in Petaluma every single restaurant has a sign in the window saying help wanted,
[01:20:06.920 --> 01:20:12.600]   help wanted. I'm excited about getting to go back to the movies or go back to the gym or maybe go
[01:20:12.600 --> 01:20:17.800]   out to eat. Well, there are millions of jobs that'll have to be filled first. So where do these
[01:20:17.800 --> 01:20:22.600]   businesses turn to fill these roles fast? I got to tell you, putting the sign in the window,
[01:20:22.600 --> 01:20:28.040]   probably not the best method. What about Zip Recruiter? Right now you can try Zip Recruiter
[01:20:28.040 --> 01:20:35.480]   free at ziprecruiter.com/twit. One post on Zip Recruiter casts the widest possible net,
[01:20:35.480 --> 01:20:40.760]   posts your job listing on 100 plus job sites, job boards. So you're going to get out to the
[01:20:40.760 --> 01:20:44.280]   largest number of people. And by the way, including social networks,
[01:20:44.280 --> 01:20:49.320]   the most people see it. But don't worry, you're not going to flood it with applications coming
[01:20:49.320 --> 01:20:56.760]   into your inbox or into your phone calling you at home. They all go into the Zip Recruiter interface,
[01:20:56.760 --> 01:21:02.360]   which is great because that means Zip Recruiter can do some very cool things. First of all,
[01:21:02.360 --> 01:21:08.200]   Zip Recruiter's matching technology will scan resumes across this network of millions of job
[01:21:08.200 --> 01:21:15.880]   seekers. And if there's a match, if that candidate fits your open role, they'll actually proactively
[01:21:15.880 --> 01:21:22.280]   invite them to apply for your job. That's pretty cool. And the interface means not only do you not
[01:21:22.280 --> 01:21:27.800]   get the phone calls or the emails, it's very easy to review all the applicants. They pre-format
[01:21:27.800 --> 01:21:32.280]   the resumes. I know this from firsthand experience. We use the Percruiter all the time. They pre-format
[01:21:32.280 --> 01:21:40.040]   the resumes makes it easy to read. You probably have some absolute musts. You can pre-screen
[01:21:40.040 --> 01:21:45.240]   candidates with yes, no questions, multiple choice, even essay questions. So you can eliminate people
[01:21:45.240 --> 01:21:50.440]   who just don't fit your needs exactly. You can easily review recommended candidates from Zip
[01:21:50.440 --> 01:21:56.200]   Recruiter and invite your top choices to apply for your job. That encourages them to apply faster.
[01:21:56.200 --> 01:22:00.920]   According to Zip Recruiter's internal data, jobs where employers invite candidates to apply,
[01:22:01.640 --> 01:22:07.000]   get two and a half times more candidates. And that's good for you. The more choices you have,
[01:22:07.000 --> 01:22:12.760]   the better you'll be able to pick the exact right person. Zip Recruiter's technology is
[01:22:12.760 --> 01:22:17.640]   so effective that four out of five employers who post on Zip Recruiter get a quality candidate
[01:22:17.640 --> 01:22:23.400]   within the first day. I have to say, for us, we've done this several times now. I can remember Lisa
[01:22:23.400 --> 01:22:28.840]   posting at breakfast. And within an hour or two, we get, I love it because she goes,
[01:22:28.840 --> 01:22:34.680]   oh, this person's great. Oh, this person's great. We get great candidates within the first few hours
[01:22:34.680 --> 01:22:40.520]   before lunch. Before lunch, right now you can try Zip Recruiter for free. We have a very special
[01:22:40.520 --> 01:22:45.640]   offer for you at zipprecruiter.com/to it. Please use that address. I want them to know you
[01:22:45.640 --> 01:22:57.400]   heard it here. Zip Recruiter.com/twit. Help yourself out. Make hiring a pleasure, not a pain.
[01:22:57.400 --> 01:23:05.800]   ziprecruiter.com/twit. Zip Recruiter is, and I can vouch for the smartest way to hire.
[01:23:05.800 --> 01:23:13.480]   Let's see. Well, I got some more Google stories as long as we're in the Google verse.
[01:23:13.480 --> 01:23:18.040]   All the things you miss Stacy on this weekend, Google. Actually, I take it back. We never talk
[01:23:18.040 --> 01:23:21.320]   about Google on this weekend, Google. Yeah. I was like, what are you talking about? We never
[01:23:21.320 --> 01:23:28.840]   talk about them. Did you get your $2.15 check? The Google plus class action.
[01:23:28.840 --> 01:23:38.280]   So remember there was a privacy breach. The Google exposed the private data 52 million users
[01:23:38.280 --> 01:23:46.200]   in 2018. They got sued. It was a class action suit. Actually, you were on the show at the time.
[01:23:46.200 --> 01:23:50.920]   I remember we talked about this. There was this first small breach, like a hundred thousand.
[01:23:50.920 --> 01:23:54.920]   And then all of a sudden there was this huge breach of millions.
[01:23:54.920 --> 01:24:03.320]   And since nobody's been working on Google plus anyway, Google's fix for it was just to shut the
[01:24:03.320 --> 01:24:09.640]   whole thing down. Okay, that's fine. That's it. It's over. The class action suit, Matt,
[01:24:09.640 --> 01:24:17.320]   Matt and Zach Harris versus Google filed October 2018 blamed Google's lax approach to data security.
[01:24:17.320 --> 01:24:24.200]   The case settled in June of last year. Google agreed to pay out seven and a half million
[01:24:24.200 --> 01:24:29.080]   dollars. Now it's time for math. Here's your math problem. Take out half that money.
[01:24:29.080 --> 01:24:35.320]   Three and a quarter for legal fees. You know, the lawyers, the ones who get rich on these.
[01:24:37.080 --> 01:24:45.800]   1.7 million people filled out the right forms by October 2020. So you divide the three and a
[01:24:45.800 --> 01:24:56.120]   quarter million by 1.7 million and you get a whopping $2.50 per person. Congratulations. You're a winner.
[01:24:56.120 --> 01:25:01.960]   I know we make fun of this, but you do have to have some incentive to get people to file
[01:25:01.960 --> 01:25:06.360]   class action suits, right? Because otherwise, why the heck would you do it? And they can be a
[01:25:06.360 --> 01:25:13.640]   deterrent to the type of things like arbitration or lackluster approaches towards privacy.
[01:25:13.640 --> 01:25:18.360]   All of these things like this will help deter it. I mean, I don't love them. I'm just like,
[01:25:18.360 --> 01:25:22.680]   we always mock it. I got a bag of pop chips once. That was cool.
[01:25:22.680 --> 01:25:30.360]   I, you know, honestly, I get these checks every once in a while. I don't even cash them. It's like
[01:25:31.400 --> 01:25:35.160]   two bucks, really? Okay. It's not worth the hassle. It's not worth the hassle.
[01:25:35.160 --> 01:25:39.880]   Okay. There's no hassle to cashing checks. Y'all, you just take a picture of it. You sign it,
[01:25:39.880 --> 01:25:43.880]   you take a picture and it goes to your bank account. It's not like you have to drive to the bank.
[01:25:43.880 --> 01:25:49.960]   Okay. So if you see a penny on the heart sense for doing a mobile deposit,
[01:25:49.960 --> 01:25:54.280]   oh, it's even. Well, you got to get a first one. You need to get a better bank because that sucks.
[01:25:54.280 --> 01:25:59.720]   But if you were walking down the streets, Stacy, and you saw a penny on the ground, would you
[01:25:59.720 --> 01:26:05.000]   reach down to pick it up? Not a penny, but if I saw a quarter, I have 100%
[01:26:05.000 --> 01:26:09.960]   with not a nickel. Would you reach out for a nickel? Oh, so hard to say a dime.
[01:26:09.960 --> 01:26:16.680]   Maybe you think I have been really lucky. I have found like a $100 bill in the ground.
[01:26:16.680 --> 01:26:21.000]   There you go. I reached out and picked that up. I gave it to the restaurant where I found it.
[01:26:21.000 --> 01:26:26.280]   And I was like, Hey, we found this. But I find money on the street all the time. And usually it's
[01:26:26.280 --> 01:26:29.480]   like in places where I think I know where it belongs. So I give it back to people.
[01:26:29.480 --> 01:26:34.680]   But every time then, I, Lisa and I were at the grocery store. I let she went in and I parked.
[01:26:34.680 --> 01:26:38.760]   I got out of the car and there on the ground was a packet with a bank.
[01:26:38.760 --> 01:26:43.720]   You know, those packets, they get put the money in and I opened it up and there was like $400 in
[01:26:43.720 --> 01:26:49.400]   there. And I thought I'm rich. And I ran into tell Lisa, she said, Oh, thank you for finding that.
[01:26:49.400 --> 01:26:57.720]   That's mine. And she took it. So that must have dropped out of my purse. To this day. I don't
[01:26:57.720 --> 01:27:01.000]   know if she was telling the truth. But anyway, is to prove it.
[01:27:01.000 --> 01:27:04.120]   Proof it. What are the serial numbers?
[01:27:04.120 --> 01:27:09.880]   That's what it's been ages since I found any cash because who carries cash?
[01:27:09.880 --> 01:27:15.560]   Harry's cash. I definitely got caught out the other day. And I was like, man, I just realized I
[01:27:15.560 --> 01:27:21.720]   haven't had cash in my wallet for like nine months. It was so weird. The other day, we were going to
[01:27:21.720 --> 01:27:27.560]   the farmer's market and my wife asked if I had any cash and I was looking around, you know,
[01:27:27.560 --> 01:27:29.960]   looking at managed to find like $12.
[01:27:29.960 --> 01:27:39.000]   Yeah, I have this big jar with pennies in it all. So when we were in Hawaii, actually,
[01:27:39.000 --> 01:27:45.240]   that really sealed the deal with the Apple Watch because, you know, Hawaii was under some
[01:27:45.240 --> 01:27:50.440]   pretty strict still COVID regulations when we were there. We had a show that we were vaccinated
[01:27:50.440 --> 01:27:54.440]   everything and you go in a restaurant and stuff. And whenever I paid, there were people who wouldn't
[01:27:54.440 --> 01:27:59.720]   accept cash, but everybody took Apple Pay or touchless payments. And I got paid. You got good
[01:27:59.720 --> 01:28:04.440]   here. Man, ABC stores, baby, just everything. You know, I was almost like I didn't even pay
[01:28:04.440 --> 01:28:09.640]   for anything. I just go in, take stuff, get my spam. What does it call me?
[01:28:09.640 --> 01:28:14.920]   What's it be? I get my spam. Moose will be tapped to pay. I'm out of here, man.
[01:28:16.200 --> 01:28:20.920]   Oh, man, see, you had only the ones you buy from the store. The next time you come here,
[01:28:20.920 --> 01:28:26.040]   I'll have care. Make you snow. I have care. His mom makes them for you. And I guarantee you,
[01:28:26.040 --> 01:28:31.880]   you'll be looking for a house because they're good. Even the ABC store. And the reason I bought
[01:28:31.880 --> 01:28:37.720]   it is the guy was stocking it. So it just arrived. So even the ABC, it's the perfect hand food to
[01:28:37.720 --> 01:28:42.440]   just have around. And if you're going to go somewhere and be out all day, and you don't know if you're
[01:28:42.440 --> 01:28:47.160]   going to get to eat, just keep it in your backpack. I keep them in my camera bag. And then if you get
[01:28:47.160 --> 01:28:52.440]   stuck, you're good. If not, then save it for the next day. Yeah. What is it? It's spam? It's a
[01:28:52.440 --> 01:28:58.600]   thin layer of spam. I'm a big lava rice wrapped in the same way. And then Naughty, yes, or seaweed.
[01:28:58.600 --> 01:29:02.840]   And then you can get them with if you're not a spam person, you can get it with chickens and
[01:29:02.840 --> 01:29:06.920]   some people make it with little hot dogs, you know, so there's all different types of stuff.
[01:29:06.920 --> 01:29:14.280]   And we actually stuff miso pork in the middle. So there's no actual meat on the outside. The
[01:29:14.280 --> 01:29:18.600]   miso pork is stuffed in the middle. And then she wraps it in the ball. All right. And you put the
[01:29:18.600 --> 01:29:22.760]   wrap around. That's on a giri. Yeah. On the giri. That's my favorite snack food. Yeah.
[01:29:22.760 --> 01:29:28.360]   So it should be on the giri, basically the same thing, but not really. Yeah. I mean,
[01:29:28.360 --> 01:29:34.680]   very similar. Carrots. Why just adjusted it? I'm going to I'm coming back. I missed out.
[01:29:35.400 --> 01:29:39.560]   Damn. Yes. Oning giri is life. Like when I was going to college in Japan, that was life,
[01:29:39.560 --> 01:29:43.800]   you never leave anywhere without one because you just don't know when you might get stuck and you
[01:29:43.800 --> 01:29:50.040]   can't eat and just reaching your back stack and boom, ham food. Let's go. Do you see? I used to bring
[01:29:50.040 --> 01:29:53.640]   them and they're cheap and delicious. Do you speak Japanese?
[01:29:53.640 --> 01:30:01.000]   I know who I'm going to take you must kid or and so that's your more hunting and pedipede
[01:30:01.000 --> 01:30:07.160]   and undecuement. I think he's being self-deprecating saying just a little, not that well, but you sure
[01:30:07.160 --> 01:30:11.400]   sound like he spoke it to me. That's what you call Japanese culture. You have to do that. You got
[01:30:11.400 --> 01:30:16.440]   a self-deprecate first everything. Their moonwalk right over. Yeah. I know. I don't speak Japanese
[01:30:16.440 --> 01:30:23.640]   very well, but now you got me. I only do one phrase in Japanese when I went to Japan and it was
[01:30:23.640 --> 01:30:31.560]   Sumi Maasen. Nintendo. What was it? Sumi Maasen? What is Sumi Maasen? Excuse me. Excuse me. Excuse me.
[01:30:31.560 --> 01:30:35.960]   It's so handy. Yeah. Sumi Maasen is all the time. And you did it well because most people don't
[01:30:35.960 --> 01:30:42.200]   say it correctly and you actually said it very well. So pick up to you, Stacy. Thank you. Hold on.
[01:30:42.200 --> 01:30:45.800]   Let's see. Give that woman. Oh, okay. Oh, okay. Oh, Shabas.
[01:30:45.800 --> 01:30:49.400]   Mm, one guy. One guy. It's extremely.
[01:30:49.960 --> 01:30:54.840]   And the only guy it is extremely like formal, but if you just wore an omizu,
[01:30:54.840 --> 01:30:58.520]   only guys who must. That's when you want a couple of. Obizu, anokai shabas. That's what it was.
[01:30:58.520 --> 01:31:03.240]   Yes. Yes. Because I always had to ask for more water in Japan. Tiny little cups. Yeah,
[01:31:03.240 --> 01:31:07.720]   because it comes to small. A deep bow, you know, to just to show your respect and, you know,
[01:31:07.720 --> 01:31:11.480]   we're taking the show. I was very apologetic. We're going to Japan. All right. You guys are
[01:31:11.480 --> 01:31:14.840]   going to be on it with me. Oh, it's on. It's on. Let's go. I'd love to.
[01:31:14.840 --> 01:31:19.240]   Japan is happening. I would love to go back to Japan. I've been there in 11 years.
[01:31:19.240 --> 01:31:25.320]   I've only been a couple of times, but I loved it. Yeah, your daughter would probably enjoy it.
[01:31:25.320 --> 01:31:30.440]   Would you say my friends live in Toyota town and they have a lot of people connected in the factory
[01:31:30.440 --> 01:31:35.400]   in IT. So like, yeah, if you ever go, I can introduce you to some Toyota folks.
[01:31:35.400 --> 01:31:39.880]   Yeah, I've got lots of contacts. All the automakers over there.
[01:31:40.600 --> 01:31:47.480]   And I work with them on a regular basis. So it's I love the opportunity to go over there.
[01:31:47.480 --> 01:31:51.560]   The food is so good. Oh, I love it. So the minute you hear about electric Tacoma,
[01:31:51.560 --> 01:31:53.800]   please email me so I can start eating the couch.
[01:31:53.800 --> 01:31:57.800]   That's why it was real slow on the electric vehicles. I think what happened.
[01:31:57.800 --> 01:32:02.120]   Well, they're first, but they're slow. I mean, you don't press and then that's not the same.
[01:32:02.120 --> 01:32:06.680]   That was it. Yeah. Yeah. They're big on hybrids. They saw a lot of hybrids, but,
[01:32:07.240 --> 01:32:14.440]   and they've been promoting hydrogen fuel cells, but they have never been big on battery electrics.
[01:32:14.440 --> 01:32:21.880]   They're very, in many ways, a very conservative company. And they just, they didn't trust the
[01:32:21.880 --> 01:32:27.400]   battery technology. And so they might be right after hearing about fires with the Chevy bolts and
[01:32:27.400 --> 01:32:35.400]   Teslas and, but they are launching a series of new battery EVs starting next year. Their first
[01:32:35.400 --> 01:32:41.880]   ones across over called the BZ4X, which we've all begged them to change the name before it goes on
[01:32:41.880 --> 01:32:50.920]   sale. What apparently this week, President Biden signed, I don't know what force of power it has
[01:32:50.920 --> 01:32:56.520]   an executive order saying not zero, but it was kind of saying, putting a flag in the sand saying,
[01:32:56.520 --> 01:33:03.240]   we want 50% of all vehicles sold in the US by was it 2030 to be a. Yeah, it's yeah,
[01:33:03.240 --> 01:33:10.600]   target of 50% 40 to 50% of all news vehicle sales to be plug in vehicles. So battery electric
[01:33:10.600 --> 01:33:14.760]   and plug in hybrids. And most, the reality is most of those are going to be battery electric.
[01:33:14.760 --> 01:33:20.280]   Do you think that's a reasonable goal? It seems like, I know a lot of countries are saying 100%.
[01:33:20.280 --> 01:33:28.360]   Yeah. Well, the industry is moving in the direction to be ready to do that. They're launching a whole
[01:33:28.360 --> 01:33:33.480]   bunch of new EVs over the next several years. They're converting plants that have been building
[01:33:33.480 --> 01:33:41.000]   gasoline cars for decades to building EVs and building battery plants, which is the other piece
[01:33:41.000 --> 01:33:45.000]   of this because if you're going to build EVs, you got to have batteries. And so they're investing
[01:33:45.000 --> 01:33:53.000]   billions of dollars in new plants to build giant lithium ion batteries. So their product
[01:33:53.000 --> 01:34:00.200]   planning is all on assuming that they're targeting getting to that 50% or more
[01:34:00.200 --> 01:34:10.280]   threshold by 2030. But to do that, to make that a realistic proposition, they need some help on
[01:34:10.280 --> 01:34:14.600]   charging infrastructure, for example. We need a lot more chargers if we're going to have
[01:34:14.600 --> 01:34:21.800]   that many EVs on the road. And some extra purchase incentives for consumers to make it more affordable,
[01:34:21.800 --> 01:34:32.120]   because the price gap is closing, but EVs are still more costly to purchase than gasoline vehicles
[01:34:32.120 --> 01:34:38.360]   at this point. The New York Times says this is good for Tesla, bad for Toyota and other companies
[01:34:38.360 --> 01:34:44.920]   that are not yet on the EV bandwagon. Does that sound right? Tesla's of course all electric.
[01:34:44.920 --> 01:34:51.160]   Yeah, Tesla's all EV. A lot of other companies are moving in that direction. GM has said they
[01:34:51.160 --> 01:35:00.360]   want to be all EV by the early 2030s. Ford is planning to go mostly electric by that time frame,
[01:35:00.360 --> 01:35:09.480]   Stellantis, Volkswagen. The Japanese automakers, oddly enough are the ones that are the biggest
[01:35:09.480 --> 01:35:16.680]   laggards in this particular e-toyota. One thing they say that's why Biden included plug-in
[01:35:17.240 --> 01:35:24.120]   hybrids in that goal, because as a weight of peace Toyota. Yeah, Toyota and some of the
[01:35:24.120 --> 01:35:30.440]   other automakers are selling it. But Toyota in particular, because they sell so many hybrids,
[01:35:30.440 --> 01:35:35.800]   such a big percentage of their sales now are hybrids, especially some of their most popular cars.
[01:35:35.800 --> 01:35:43.320]   Like the Toyota RAV4, a quarter of those in the US now are hybrids. And in Europe,
[01:35:43.320 --> 01:35:49.240]   over half of RAV4 sales are hybrids now. It outsells the Prius. The Prius is on its way out.
[01:35:49.240 --> 01:35:52.920]   Yeah, nobody's buying Prius anymore. People are buying RAV4 hybrids instead.
[01:35:52.920 --> 01:35:56.680]   Interesting. Because RAV4 has a little bit more style, a little bit more functionality,
[01:35:56.680 --> 01:36:01.240]   a little bit more room. Yeah, more room. It's bigger. And here's the crazy thing. Like having
[01:36:01.240 --> 01:36:07.800]   the i3 and then having an electric 5 series, I prefer the i3 better, even though the 5 is way
[01:36:07.800 --> 01:36:12.840]   more comfortable with Dober ride like the whole nine yards. It's just the idea of like, I might
[01:36:12.840 --> 01:36:17.720]   actually have to go to the gas station at some point. And that drives me crazy because I would
[01:36:17.720 --> 01:36:22.440]   rather never go. Well, and I'm sure, especially for you, you know, living in Honolulu, you know,
[01:36:22.440 --> 01:36:27.640]   the traffic is really bad. The i3, you know, it's got a nice small physical footprint. It doesn't
[01:36:27.640 --> 01:36:33.240]   take up much space on the road. It's going to be easier to park, easier to maneuver around and
[01:36:33.240 --> 01:36:38.760]   traffic. It was so epic. Yeah, it's a cool little car. It's kind of weird looking.
[01:36:38.760 --> 01:36:44.840]   It was a fun thing to drive. It is weird looking. At least it was in the market for an electric
[01:36:44.840 --> 01:36:50.360]   vehicle because Michael finally got his driver's license and is taking the bolt. And she looked
[01:36:50.360 --> 01:36:55.640]   at the i3, she said, I'm not going to buy that. It's too funny. So what she buy, she bought a mini
[01:36:55.640 --> 01:37:04.280]   Cooper, which is basically an i3 with switches. It's got the i3 hardware in there, but in a cooler
[01:37:04.280 --> 01:37:09.080]   looking body. Yeah, it's almost the same car. It just has a body kit. But when you know what a
[01:37:09.080 --> 01:37:14.760]   funny thing about it is as funny looking in as ugly as that car was, I get more when I had when
[01:37:14.760 --> 01:37:19.800]   I was driving that thing around, I got more compliments out of that car. Really get out of
[01:37:19.800 --> 01:37:26.280]   a gorgeous electric five series. Why number one, the circus bear mentality. Six foot 250
[01:37:26.280 --> 01:37:34.520]   do climbs out of that thing. You know, you fit in there. And I mean, I got I fit. I got mad
[01:37:34.520 --> 01:37:40.520]   space in here because it's extremely spacious. It's it's freakishly spacious when you figure how
[01:37:40.520 --> 01:37:47.160]   small it is. Right? So it was the yeah. So it was one of those things that everybody I think they
[01:37:47.160 --> 01:37:52.840]   were more freaked out that how the heck did this middle limb back to get out of that little car?
[01:37:55.240 --> 01:38:00.200]   I actually insist on sitting in the mini before she bought it because I don't make sure I could
[01:38:00.200 --> 01:38:03.880]   get into it. It's pretty comfortable. I'm looking forward to it. It's gonna be a lot of fun to drive.
[01:38:03.880 --> 01:38:09.640]   Not a lot of battery, but you know, we don't go anywhere. So well, I mean, when you need to go
[01:38:09.640 --> 01:38:16.280]   long distances to take the must-take-the-must-take-take-the-bolt. Yeah. I asked Sam a question. Yes.
[01:38:16.280 --> 01:38:20.360]   Since he's here and he's an expert. I'm like, that's right. That's what I was trying to sneak in my
[01:38:20.360 --> 01:38:27.000]   Toyota question. I will open it up to everyone, including the chat room. But in like six months,
[01:38:27.000 --> 01:38:31.560]   I'm gonna have to be giving up my car to my daughter, which means I get a new car. You're the
[01:38:31.560 --> 01:38:39.800]   same, but we are. Yeah. Yeah. So I have a Tesla, one of the old original, like, 13, 2012, Model
[01:38:39.800 --> 01:38:46.200]   S. I love that car and I hate that car because I love it's dry. I love driving it. I want all
[01:38:46.200 --> 01:38:51.240]   electric, but it's too freaking big for me. I hate how big it is. So I would like a small,
[01:38:51.240 --> 01:38:58.440]   fun, all electric vehicle. Go. I would definitely take a look at the Volkswagen ID4.
[01:38:58.440 --> 01:39:05.240]   You drove that last week. Yeah. Yeah. You know, it's definitely much more compact,
[01:39:05.240 --> 01:39:12.360]   but it's still roomy inside. Depending on what your timing is, early in the year,
[01:39:12.360 --> 01:39:17.560]   Nissan's going to launch the ARIA, their own compact crossover. Depending on your budget,
[01:39:17.560 --> 01:39:23.080]   you might also want to look at the Polestar 2. It's quite a bit smaller than the Model S.
[01:39:23.080 --> 01:39:31.240]   It's really good to drive. It's quick. The Volvo XC40 recharge, that's another battery EV. So
[01:39:31.240 --> 01:39:35.960]   there's a whole bunch of them that are coming to market. And of course, you know, if you decide
[01:39:35.960 --> 01:39:39.720]   that maybe go on small is not what you want to do, you can also get a Hummer EV.
[01:39:41.480 --> 01:39:45.320]   I asked Lisa to get that. In fact, I actually tried to talk her into a Porsche
[01:39:45.320 --> 01:39:49.800]   Ticon, but she was going to say Ticon. That was a good mindset. Yeah. Everybody seems to love that
[01:39:49.800 --> 01:39:59.400]   car. Here's a picture of Lisa in her new in her new Aston Super Mini. Yeah. She really rocking out
[01:39:59.400 --> 01:40:06.440]   in that mini. Yeah. The Audi Q4 e-trons, another one to take a look at. It's based on the same
[01:40:07.000 --> 01:40:11.640]   platform, same hardware as the VW ID for, but it's a little more stylish. If you want something,
[01:40:11.640 --> 01:40:19.640]   it looks a little more premium. So I hear you get out of the e-tron. I'm taking notes. I'm like,
[01:40:19.640 --> 01:40:25.400]   okay, this is good. This is exciting. I wanted that e-tron when they had the wagon version before
[01:40:25.400 --> 01:40:30.280]   they came out with the SUV version and then it disappeared. I think it was an A3 e-tron.
[01:40:30.280 --> 01:40:34.360]   It was like a blue one. Yeah, that was a plug-in hybrid. So that's based on the golf.
[01:40:36.040 --> 01:40:41.800]   Oh, I don't know how I forgot this. The Hyundai Ionic 5 is also coming out this fall, which is
[01:40:41.800 --> 01:40:49.560]   another one that's going to be really interesting because it's going to have 350 kilowatt charging
[01:40:49.560 --> 01:40:56.680]   capability. So you can charge it from 10 to 80% in about 18 minutes. So Stacey, you're on an island.
[01:40:56.680 --> 01:41:00.520]   Do you drive off the island or is this just for getting around? Oh, I take my Tesla into the
[01:41:00.520 --> 01:41:07.720]   mountains. It's the best car. Oh, okay. And you have a charger at home, obviously. I have a charger
[01:41:07.720 --> 01:41:12.280]   at home and then there's actually chargers like all along the other big peninsula. I love that car.
[01:41:12.280 --> 01:41:16.920]   Yeah. You get the supercharger network. Sam and I were talking about the radio show this week because
[01:41:16.920 --> 01:41:24.440]   he was driving a Mustang Mach-E up in the wilds of Michigan and found it a little hard to find
[01:41:24.440 --> 01:41:30.120]   those electrify America chargers. That's going to be the trick. Does the Biden executive order do
[01:41:30.120 --> 01:41:36.680]   anything about beefing up our electric charging network? Yeah. It talks about adding more
[01:41:36.680 --> 01:41:42.680]   EV chargers, building up the charging infrastructure, adding they want to add 500,000 public charging
[01:41:42.680 --> 01:41:48.600]   stations, which is great, except that you can't do that through an executive order. That's
[01:41:48.600 --> 01:41:53.320]   something a lot of what was in the executive order is basically saying,
[01:41:53.320 --> 01:41:58.040]   suggestions, Congress, Congress, you should do something about it. Help me out here. We need to do
[01:41:58.040 --> 01:42:02.920]   something. Yeah. Yeah. Because Congress has got to give some money first in order for them to do it.
[01:42:02.920 --> 01:42:08.760]   Yeah. I wish more. I mean, that's a nice way. I'm just thinking of an easy word to say. I wish
[01:42:08.760 --> 01:42:14.200]   more fossil heads would get their head around that electric vehicles will be very helpful and
[01:42:14.200 --> 01:42:18.440]   everyone goes, well, it costs more to do this. And so there's these weird, you know, mythical arguments
[01:42:18.440 --> 01:42:23.720]   of which cost more to run and which costs less to run. One of the things I know living on an island
[01:42:23.720 --> 01:42:29.640]   with the way our weather works, it's just a general wear and tear of driving from a combustion engine
[01:42:29.640 --> 01:42:34.920]   vehicle, the stuff that drips onto the ground causes the asphalt not to really stick and pop out.
[01:42:34.920 --> 01:42:41.720]   So I, Leo, you were just here. Our government spends every single day patching like 3000 potholes
[01:42:41.720 --> 01:42:46.360]   and they come back a month and a half later right back out. I saw them doing that.
[01:42:46.360 --> 01:42:52.200]   So that's one of the things that EVs will do better is because they're not dropping
[01:42:52.840 --> 01:42:58.920]   like caustic things onto the ground. And so infrastructure will actually save more.
[01:42:58.920 --> 01:43:03.720]   That's interesting. Yes. Batter batteries are environmentally thinkless. Sorry, because I
[01:43:03.720 --> 01:43:09.080]   will get upset because they don't understand that your gassing is not just the gas and whatever,
[01:43:09.080 --> 01:43:13.640]   you know, using all the batteries or worse. That stuff dripping on the ground is extremely
[01:43:13.640 --> 01:43:18.760]   caustic. It just is. Or when you pick up your kids from school and you're idling because you're
[01:43:18.760 --> 01:43:22.280]   waiting for your kids, you're not poisoning all their little lungs.
[01:43:22.280 --> 01:43:29.400]   And driving EVs, because they've got that instant torque, you know, they the way they
[01:43:29.400 --> 01:43:34.760]   take off is fantastic. You know, it feels so. Oh, yeah. Once you drive, what you go like,
[01:43:34.760 --> 01:43:39.960]   people never go back. It's really never want to go. Yeah. I do miss the noise, you know,
[01:43:39.960 --> 01:43:48.520]   of a great engine, but you know, I'm so deaf. The Mustang has a switch to play recorded
[01:43:48.520 --> 01:43:53.800]   engine noise into the cabin. And I can't hear it. I don't know what's going on it. I switch it.
[01:43:53.800 --> 01:43:59.480]   I does that too, right? It has the fake noise. And I'm like, Sam, can you hear it when you turn
[01:43:59.480 --> 01:44:04.360]   on the propulsion noise? I don't hear any noise at all. It's not, you know, it's not as loud as
[01:44:04.360 --> 01:44:08.360]   what you get with a V8 Mustang. It doesn't sound like my Mustang. It is a lot more subtle. Okay.
[01:44:08.360 --> 01:44:12.840]   But you know, but it's something that, you know, it's not just recorded engine noise. It's a
[01:44:12.840 --> 01:44:18.600]   it's a synthetic sound that is trying to evoke the feeling of driving a Mustang. I don't need
[01:44:18.600 --> 01:44:20.600]   that. I like it. I like that. Still, I like that. Still,
[01:44:20.600 --> 01:44:30.520]   the tris is right in the backseat and make the noise for you. The truth is,
[01:44:30.520 --> 01:44:36.920]   and I know this because traffic is a nightmare in a wahoo, which surprised shocked me.
[01:44:36.920 --> 01:44:42.600]   The real problem is not electric or gas. The real problem is private ownership of cars. I don't
[01:44:42.600 --> 01:44:46.680]   know if we're ever going to solve that problem. No, we're not because in a way, you know,
[01:44:46.680 --> 01:44:52.200]   and here this is exactly what it is here. We we happen to be a place, it's sort of a high
[01:44:52.200 --> 01:44:57.400]   immigrant population, right? Because of the way the island was. Yeah, controversy. It's a very
[01:44:57.400 --> 01:45:01.960]   diverse island. Yeah, right. It was controversically taken away. Many of the cultures got here in
[01:45:01.960 --> 01:45:08.360]   order to help run factories for dull and our farms for dull, plantations. When you came from
[01:45:08.360 --> 01:45:12.920]   someplace where people didn't have things, you get here, your first generation, you want to provide
[01:45:12.920 --> 01:45:17.560]   your kids things better than the next. Right. And so that everybody does, right? But it's
[01:45:17.560 --> 01:45:22.040]   amplified here because of the whole American dream stories, whatever. I come from a place where I
[01:45:22.040 --> 01:45:28.280]   was walking, catching trains or maybe even still riding a four legged vehicle. And now I get to
[01:45:28.280 --> 01:45:32.920]   own a car. When my kids are old, I'm going to get them a car right away. So you have a large family,
[01:45:32.920 --> 01:45:40.200]   you got five or six kids and every kid gets a call. So it gets it gets crazy. Keep doing that.
[01:45:40.200 --> 01:45:45.560]   You gotta get your kids scooters, but nobody wants to sacrifice. I understand. Oh my goodness.
[01:45:45.560 --> 01:45:51.800]   I'm looking at a scooter that has a dumb name. It's called scooter tron something scooter makes
[01:45:51.800 --> 01:45:59.240]   scooters in or something crazy. It's really just a sweet one. I want one. I was in Traverse City
[01:45:59.880 --> 01:46:06.760]   for a conference this week. And they an old friend of mine introduced, was working with
[01:46:06.760 --> 01:46:14.840]   this UK company called Zap Scooters, introduced their first electric scooter. It's going on sale.
[01:46:14.840 --> 01:46:21.480]   There's going to start deliveries in Paris later this month. And it's a really cool looking scooter.
[01:46:21.480 --> 01:46:26.840]   It's got about a 40 mile range. We'll go, I think it'll go up to about 50 miles an hour.
[01:46:27.480 --> 01:46:31.720]   But it's got removable batteries. So you can take the batteries out, take them inside and
[01:46:31.720 --> 01:46:36.040]   charge them up. You can get spare batteries just like you used to do with your with your Samsung
[01:46:36.040 --> 01:46:42.920]   phones. Leo, you can get spare batteries. Yeah. And it's actually in the in the rundown here,
[01:46:42.920 --> 01:46:53.400]   the Zap Scooter. And it's actually it's designed as a more premium scooter. And it's a really cool
[01:46:53.400 --> 01:47:00.040]   looking design. And I think it's definitely that that's the sort of thing that especially in cities,
[01:47:00.040 --> 01:47:06.600]   especially in crowded cities. I think we need to get more people adopting micro mobility solutions,
[01:47:06.600 --> 01:47:12.440]   whether it's a great, great, great, you know, other small vehicles like that.
[01:47:12.440 --> 01:47:17.720]   Apparently it is scooter. McScooter. I'm not joking. It was called Rollie. The one that I saw
[01:47:17.720 --> 01:47:23.240]   is called the Rollie. It stands up by itself. It's got a gigantic fat wheel. I've seen the Rollie.
[01:47:23.240 --> 01:47:29.960]   They're never cared about this until I went to San Diego the like a couple months before the
[01:47:29.960 --> 01:47:34.680]   situation as I call it. And I ripped around San Diego for like a week and a half with just
[01:47:34.680 --> 01:47:39.240]   lime scooters. And I was like, I want this life. This is so cool. This is the Rollie.
[01:47:40.120 --> 01:47:49.560]   Rollie scooter, son, the intelligent scooter. Oh, I want that. It's so cute. And get off. It
[01:47:49.560 --> 01:47:57.160]   stands up by itself. What is it? Is it like a safe way of balancing self balancing? Oh, that's
[01:47:57.160 --> 01:48:04.120]   neat. So it's like a little segue scooter. How much are those? I don't know yet too much.
[01:48:04.120 --> 01:48:07.560]   You try to find out. I love my E-bike. I stopped riding it because somebody got
[01:48:07.560 --> 01:48:12.440]   creamed on his bike on the on the bridge that I crossed would cross to get here every day.
[01:48:12.440 --> 01:48:16.280]   And I just I feel like it's not safe anymore. That's why I still drive a car.
[01:48:16.280 --> 01:48:20.600]   That's another reason why I say I was right. If more people would do the micro mobility.
[01:48:20.600 --> 01:48:24.760]   Yeah. You just safer. This is a great conversation Leo because you just been here.
[01:48:24.760 --> 01:48:29.800]   There's a lot of driving for things that are under a mile and it drives me insane because I
[01:48:29.800 --> 01:48:33.560]   lived in Japan where you don't get in a car unless he's over two months. Yeah, that's right.
[01:48:33.560 --> 01:48:39.080]   Two mile is the threshold. But there's a lot of places where you're just going to the store
[01:48:39.080 --> 01:48:43.400]   and the store is less than a mile away. But nobody will walk. Everybody will drive.
[01:48:43.400 --> 01:48:48.840]   And that's what makes it crowded. It's the short trip center obnoxious going to work and from work
[01:48:48.840 --> 01:48:55.320]   we get that but it's the everyday little short trips. And so Biki which is those little mint
[01:48:55.320 --> 01:49:00.040]   green bikes you saw has helped out a lot but mostly the tourists ride them because they come
[01:49:00.040 --> 01:49:05.640]   from places that had bikes. So we're still trying to get it out of the head that I need my own car.
[01:49:05.640 --> 01:49:11.800]   And of course you saw the gigantic 2003 Toyotas that are like lifted this high off the ground.
[01:49:11.800 --> 01:49:16.040]   Yeah. You look brand new but it's just obnoxious for an island like where are you going with that?
[01:49:16.040 --> 01:49:20.520]   So the rollies between three and thousand and thirty five hundred bucks.
[01:49:20.520 --> 01:49:29.880]   Yeah. They come in orange. Oh yeah. There you go. You like orange or you first cyan yellow
[01:49:29.880 --> 01:49:34.040]   red white or carbon fiber. I might get orange. That'd be more visible.
[01:49:34.040 --> 01:49:40.120]   You might be more visible. The zap scooters on line 60 in the rundown if you're interested.
[01:49:40.120 --> 01:49:44.280]   I'm looking at it. It's beautiful. Okay. Will it make it here Sam?
[01:49:44.280 --> 01:49:49.160]   Yeah it will be coming here next year. So they're rolling it out city by city.
[01:49:49.160 --> 01:49:55.640]   Maybe I don't need a car because we if we had two cars. There you go. My daughter.
[01:49:55.640 --> 01:50:04.280]   Got her. There you go. We have ended private ownership of vehicles on this show ladies and
[01:50:04.280 --> 01:50:10.920]   gentlemen. No I have it. We own three vehicles in this case. But one of them would be a zap urban
[01:50:10.920 --> 01:50:17.320]   mobility vehicle. That's cool. The other is electric but still that's cool. Well this is
[01:50:17.320 --> 01:50:21.720]   electric too right? Yeah yeah. Yeah. Yeah. This is the one I was talking about the batteries are
[01:50:21.720 --> 01:50:29.640]   removable. And a lot of the scooter companies especially in Asia now are doing removable
[01:50:29.640 --> 01:50:38.280]   batteries. And there's a company called Goguro that is setting up a chart battery top station.
[01:50:38.280 --> 01:50:44.520]   We're having a Stacey attack. What happened Stacey? Are you familiar with Goguro? The prices?
[01:50:44.520 --> 01:50:50.520]   Oh yeah. Oh hi. Yes. Oh hi. So hi. You can get a $2,500 tax credit on this one.
[01:50:50.520 --> 01:50:55.640]   On the zap or on the roller road go below. On the on the zap. So this this one's this is
[01:50:55.640 --> 01:51:01.400]   their $9,000. Y'all are 7,500 for the regular. But you get but how much is the
[01:51:01.400 --> 01:51:07.640]   20? 2,500 bucks off of that. But you know think about what a you know just a regular e-bike cost
[01:51:07.640 --> 01:51:13.960]   these days. You're spending 6,000. Yeah. Yeah. Yeah. The new track that I was looking at was
[01:51:13.960 --> 01:51:21.160]   6 grand. Yeah. For a track roadway roadway something like that. Okay. That's a fancy bike.
[01:51:21.160 --> 01:51:26.360]   That's a track maybe made in America. Well not really. But that's
[01:51:26.360 --> 01:51:30.520]   fans. Parts of it are made in America. Yeah. The logo.
[01:51:30.520 --> 01:51:37.640]   Let's take a little break so we can help Stacey pay for her zap i 300.
[01:51:41.880 --> 01:51:49.080]   Our show today brought to you by Udacity. I know a lot of you love technology. That's why you
[01:51:49.080 --> 01:51:55.000]   listen to our shows. I do too. That's why when Udacity was launched in 2011 10 years ago they
[01:51:55.000 --> 01:51:59.240]   just celebrating their 10th anniversary. I was really excited. It was started by a Googler
[01:51:59.240 --> 01:52:03.880]   who noted that a lot of the people were applying for jobs at Google even though they came out of
[01:52:03.880 --> 01:52:09.320]   prestigious universities and got to fancy degrees in technology and
[01:52:09.320 --> 01:52:14.120]   computer science didn't have the real skills that they needed to work at Google.
[01:52:14.120 --> 01:52:19.480]   So he started Udacity said we're going to do and the very first thing Udacity did was the
[01:52:19.480 --> 01:52:24.760]   nanodegree. We're going to go to companies and say what do the skills people need to get jobs
[01:52:24.760 --> 01:52:33.320]   at your company? Udacity is a unique part-time online education program geared for people like
[01:52:33.320 --> 01:52:37.880]   you looking to take your technology to the next level. They offer the latest cutting edge
[01:52:37.880 --> 01:52:45.800]   nanodegree programs which you can't get anywhere else things like AI, deep learning, flying car
[01:52:45.800 --> 01:52:53.160]   an autonomous flight engineer, intro to self-driving cars, machine learning engineer, robotic software
[01:52:53.160 --> 01:52:59.000]   engineer and they design these courses by partnering with industry leaders, Microsoft, Google, IBM,
[01:52:59.000 --> 01:53:05.560]   AWS and more. In fact team leads at these top companies are often the instructor. People were
[01:53:05.560 --> 01:53:12.760]   passionate to understand the skills you'll need to work with them. If you love learning and you want
[01:53:12.760 --> 01:53:17.800]   to get a job in tech Udacity is for you even if I mean honestly I take their courses just because
[01:53:17.800 --> 01:53:23.160]   I love this stuff and it's a great way to learn. Udacity can help you master the latest tech
[01:53:23.160 --> 01:53:27.560]   skills and techniques of course they've got the videos that you can watch the lectures on your
[01:53:27.560 --> 01:53:33.560]   own time at your convenience but more importantly and I really appreciate this they focus on active
[01:53:33.560 --> 01:53:40.760]   learning. Your courses are project based you actually have to implement the things you've
[01:53:40.760 --> 01:53:46.440]   learned and that makes a huge difference in your understanding. You've got to test your knowledge
[01:53:46.440 --> 01:53:50.520]   you've got to do these projects. Now you're not doing them alone homework and projects are reviewed
[01:53:50.520 --> 01:53:56.760]   by qualified professionals. You'll have real human feedback and code reviews so you really
[01:53:56.760 --> 01:54:02.280]   kind of get used to the kind of environment that you'll be working in in big tech and of course
[01:54:02.280 --> 01:54:07.400]   you have access to mentors 24/7. So yes you're going to have your I tell you it's hard you're
[01:54:07.400 --> 01:54:11.400]   going to go have points where you go I'm what do I do I don't know how this works how do I do
[01:54:11.400 --> 01:54:18.440]   you ask a mentor. It's a it's really a successful way to do this and and you can do this in your
[01:54:18.440 --> 01:54:23.000]   own time in as little as five or ten hours a week you work at your own pace anytime of the day or
[01:54:23.000 --> 01:54:28.680]   night there are mentors available all day and all night 24/7 because they're global and you can
[01:54:28.680 --> 01:54:33.000]   graduate as little as three months you might have a better job but before next year wouldn't that
[01:54:33.000 --> 01:54:39.240]   be awesome and and maybe it's a good thing because according to the world economic forum 75 million
[01:54:39.240 --> 01:54:46.200]   jobs are going to be replaced by automation in the next three years that could very well be yours
[01:54:46.200 --> 01:54:56.360]   prepare for the future with Udacity over 14 million people in over 240 countries now use Udacity
[01:54:56.360 --> 01:55:01.240]   you can see the course listings at Udacity.com my suggestion is look at them and pick something
[01:55:01.240 --> 01:55:05.800]   that gets you excited don't pick something because you say oh I have to do this or I should do this
[01:55:05.800 --> 01:55:11.240]   pick something that you say I want I want to be an autonomous flight engineer I want to learn
[01:55:11.240 --> 01:55:15.640]   this because I tell you what that passion is going to drive you through the learning you're
[01:55:15.640 --> 01:55:20.760]   going to be better at it and when you get that job you're going to be happy wouldn't you like to be
[01:55:20.760 --> 01:55:27.880]   happy in your job wouldn't that be nice once you enroll as a student in specific course offerings
[01:55:27.880 --> 01:55:31.800]   you're going to view the online course you're going to complete a series of projects and support
[01:55:31.800 --> 01:55:37.320]   courses they have free courses yes they also offer flexible payment options so you can learn at your
[01:55:37.320 --> 01:55:42.520]   own pace and schedule and by the way if you're a business you should check out Udacity for enterprise
[01:55:42.520 --> 01:55:47.240]   if you've got a team that needs to master cutting edge technologies nowadays we really want our IT
[01:55:47.240 --> 01:55:53.480]   teams to be experts in cybersecurity for sure or AI or data science with Udacity for enterprise
[01:55:53.480 --> 01:55:59.000]   you can upskill your entire workforce with real world project based learning be sure to check out
[01:55:59.000 --> 01:56:04.280]   the enterprise section of Udacity's website today get the in-demand tech skills you need
[01:56:04.280 --> 01:56:11.960]   to advance your career visit Udacity uda ci ty Udacity.com/twit
[01:56:11.960 --> 01:56:16.360]   by the way we've got a really good deal for you i'm really thrilled to be able to tell you about
[01:56:16.360 --> 01:56:27.880]   this use the offer code TWIT 75 75% off any program do not wait this is going to be a limited time
[01:56:27.880 --> 01:56:38.280]   offer i'm thrilled we can offer this to you Udacity.com/twit the offer code TWIT 75 or 75% off
[01:56:40.040 --> 01:56:49.400]   wow i have a segue actually we plugged in the segway as the other day and the batteries
[01:56:49.400 --> 01:56:55.320]   why are you laughing see this is the problem with the segue because i thought you had a segue
[01:56:55.320 --> 01:57:03.880]   to a new topic no i have a segue scooter that you're right oh not that no i love it but yeah the
[01:57:03.880 --> 01:57:07.960]   battery won't charge anymore i guess we left it sitting too long but i think it's easy to replace
[01:57:07.960 --> 01:57:13.160]   them so i'm going to find out they're also covered with mud because the kids when michael was little
[01:57:13.160 --> 01:57:18.920]   he would just they'd go off road they'd ride everywhere they'd trash the things but they are
[01:57:18.920 --> 01:57:23.800]   so much so much fun i should if you didn't look like such a dork because you're nine feet tall
[01:57:23.800 --> 01:57:30.040]   when you're riding a segue if you did they have that new c80 which is a moped segue oh the new stuff
[01:57:30.040 --> 01:57:34.200]   the nine butts because they got bought by nine but the nine butts that's pretty cool we have an
[01:57:34.200 --> 01:57:41.240]   original segue before they got bought so i thought they were pretty cool i like electric i'm all
[01:57:41.240 --> 01:57:46.440]   electric we've got electric bikes segways electric vehicles i think it's great i i just love it
[01:57:46.440 --> 01:57:55.560]   uh let's see here what else should we talk about firefox lost almost 50 million users last year
[01:57:55.560 --> 01:58:01.880]   that's depressing out of a total of 250 some million that's a big drop
[01:58:01.880 --> 01:58:10.200]   we talked about this on security now i really think it's important what do you think that there be
[01:58:10.200 --> 01:58:17.880]   a diversity of browser ecosystems firefox is kind of the last hold out everybody's gone chromium based
[01:58:17.880 --> 01:58:24.600]   not just google chrome microsoft edge i use vivaldi using vivaldi right now that's chromium based
[01:58:25.160 --> 01:58:31.080]   uh brave is chromium based uh everything is it's a duck duck oh wait no that's not that's a
[01:58:31.080 --> 01:58:36.680]   duck duck go has a browser they have a browser yeah okay they do yeah uh there it's all it's all
[01:58:36.680 --> 01:58:42.120]   chromium based which on the one hand i guess is is good because then it's maximum compatibility
[01:58:42.120 --> 01:58:48.520]   but on the other hand i think i think diversity is good yeah you know in software ecosystem you know
[01:58:48.520 --> 01:58:54.360]   because if there if there are vulnerabilities exactly that's what steve was saying you know if you if
[01:58:54.360 --> 01:58:59.560]   you've got something you know alternatives you you can switch to something else do you think that
[01:58:59.560 --> 01:59:06.680]   there um slide was due to when edge hit the mac edge for the mac was dope it was fastest for a
[01:59:06.680 --> 01:59:11.800]   minute and then it got clued you but for the minute it was like the fastest browser for the mac yeah
[01:59:11.800 --> 01:59:18.040]   and i i was like i'm i can't believe i'm saying this i'm really judging i know edge is jumping
[01:59:18.040 --> 01:59:23.000]   yeah exactly that's what i wonder i wonder if that did cause any slide though to firefox
[01:59:23.000 --> 01:59:27.880]   and i'm also makes me curious what the heck happened like ed's team because you came out of
[01:59:27.880 --> 01:59:34.120]   the gate like yeah we got this and then i don't know what a big but a huge drop over a year
[01:59:34.120 --> 01:59:39.720]   that's a huge drop um i just think chrome is more and more dominant and edge didn't help
[01:59:39.720 --> 01:59:46.280]   uh there is safari we mac people use a web kit based browser but what do you think doc rock
[01:59:46.280 --> 01:59:53.480]   is i know he's use max all the time he's so right i use safari as my primary browser but i'm finding
[01:59:53.480 --> 01:59:57.240]   more and more because i am doing a lot of stuff with content creation and doing things where i
[01:59:57.240 --> 02:00:02.200]   have to do what we're doing today call in to someplace else i always have to use chrome for that
[02:00:02.200 --> 02:00:08.360]   because the one thing that's great about safari is the security but that security comes at a cost
[02:00:08.360 --> 02:00:13.720]   when i can't do regular stuff right because you know and again i get what they're trying to do and
[02:00:13.720 --> 02:00:20.040]   i appreciate that i don't mind having two browsers most people that irritates them they so they just
[02:00:20.040 --> 02:00:26.920]   say screw it and they just use chrome so they're kind of a the most flexible but they're the ones that's
[02:00:26.920 --> 02:00:31.320]   also mucking up the security because that's where your friends breach and they get your information
[02:00:31.320 --> 02:00:37.000]   even if you are tight right so it's it's kind of for personally me it drives me nuts i wish we would
[02:00:37.000 --> 02:00:43.000]   just use the right tools for the right job or everybody i don't know i'm only thinking about the
[02:00:43.000 --> 02:00:49.480]   consensus and right now recently strangely enough i've been using firefox a lot i might go back
[02:00:49.480 --> 02:00:55.560]   firefox focus i switched to yeah firefox focus on ios is great because it's a it got ad blocker
[02:00:55.560 --> 02:00:59.800]   it's got privacy built and they're still using web kit you can't do anything about that on ios
[02:00:59.800 --> 02:01:06.920]   but you can at least use uh uh user interface on top of web kit that does more to protect your
[02:01:06.920 --> 02:01:12.360]   privacy i i agree with you i like firefox focus their new security stuff has just been really
[02:01:12.360 --> 02:01:17.640]   really nice and it hasn't seemed and again i'm only been messa-wanted for about two months now
[02:01:17.640 --> 02:01:23.640]   it hasn't seemed to cause a performance issue and that's why i've been enjoying the heck out of
[02:01:23.640 --> 02:01:29.240]   firefox right now so i'd be super upset if they win you know still no better battery life on a
[02:01:29.240 --> 02:01:35.640]   macbook than non-safari safari really is good on the battery i think a chrome is not so good
[02:01:37.080 --> 02:01:41.640]   somebody in the chair said safari is secure because apple doesn't spy on you
[02:01:41.640 --> 02:01:55.720]   here's another laugh elan musk as ceo of apple i think this book got it wrong it's by a pretty
[02:01:55.720 --> 02:02:02.520]   reputable reporter tim higgins the wall street journal i know tim and you know i i think that
[02:02:02.520 --> 02:02:06.920]   there may be certain elements of the story that might be wrong but i think that the fundamental
[02:02:06.920 --> 02:02:12.600]   is probably right because i i'm oh you're the first person i've heard me know that my team and
[02:02:12.600 --> 02:02:18.600]   tim and elan both said we've never talked to each other ever i've never met the guy and i think
[02:02:18.600 --> 02:02:23.880]   that that's bs because they were they were sitting three feet apart from each other at a meeting with
[02:02:23.880 --> 02:02:28.920]   donald trump back in 2017 they're sitting on others there's photos of them sitting on other side of
[02:02:28.920 --> 02:02:35.640]   saffra cat that's when he called him tim apple i believe right yeah um so you know they have met
[02:02:35.640 --> 02:02:42.520]   and i'm pretty sure they have spoken now the the thing that may be wrong in tim's story you know
[02:02:42.520 --> 02:02:49.800]   because what he's has his hearsay he didn't get it from either elan or or or uh or tim but i you know
[02:02:49.800 --> 02:03:00.520]   i think that um i'm i'm very very confident that um tesla did have conversations with apple about
[02:03:00.520 --> 02:03:05.320]   an acquisition during one of the numerous times elan tweeted this this is what elan tweeted
[02:03:05.320 --> 02:03:10.840]   cook and i have never spoken or written to each other ever there was a point where i requested
[02:03:10.840 --> 02:03:15.880]   to meet with cook to talk about apple buying tesla there were no conditions of acquisition
[02:03:15.880 --> 02:03:23.240]   proposed whatsoever he refused to meet and by the way musk rubs it in by saying tesla was worth
[02:03:23.240 --> 02:03:27.640]   about six percent of today's value in other words tim could have gotten to you i don't i don't believe
[02:03:27.640 --> 02:03:33.960]   that tweet i think i think elan's lying um which she's he's been known to do once or twice oh yeah
[02:03:33.960 --> 02:03:40.520]   um you know i i think he did request a meeting with tim um i wouldn't be surprised if tim
[02:03:40.520 --> 02:03:45.640]   did not want to meet directly with him to talk about an acquisition that maybe he talked to
[02:03:45.640 --> 02:03:54.040]   um somebody else what's the see what's the c oo's name nice three no no no maybe maybe my
[02:03:54.040 --> 02:04:01.720]   street i'm thinking of the other cfo's is uh jeff uh williams jeff jeff williams yeah so i
[02:04:01.720 --> 02:04:09.400]   i wouldn't be surprised if there was a conversation between uh tesla and someone at apple yeah and i
[02:04:09.400 --> 02:04:17.000]   also would not be the least bit shocked if elan insisted that he be ceo of the combined company
[02:04:17.000 --> 02:04:22.440]   that was because this was this this is this is something we've anybody's been following elan for a
[02:04:22.440 --> 02:04:29.160]   long time knows part of the reason why you know things have gone the way they have it tesla is
[02:04:29.160 --> 02:04:37.080]   because he is a micromanager he he had he owns like 22 23 percent of the stock in tesla yeah and
[02:04:37.080 --> 02:04:43.720]   the reason why he has retained that level of of control is because he after his experiences
[02:04:43.720 --> 02:04:50.360]   with paypal and zip two and his previous companies he did not want to give up control of any of his
[02:04:50.360 --> 02:04:56.840]   new companies so he's got similar stakes in space x and all his other companies and i would not be
[02:04:56.840 --> 02:05:03.000]   the least bit surprised if if a condition of any deal and this is why no other automaker would ever
[02:05:03.000 --> 02:05:10.440]   consider buying tesla unless it was in bankruptcy and the stock was wiped out is because they don't
[02:05:10.440 --> 02:05:19.480]   that nobody else wants elan to be part of the organization right so higgins tweeted that the
[02:05:19.480 --> 02:05:27.720]   anecdote comes from musk's own telling of the story according to people who heard it
[02:05:27.720 --> 02:05:33.400]   tim did not hear it but others didn't said oh yeah elan said this i could totally see elan musk
[02:05:33.400 --> 02:05:38.920]   bs because it sounds like it's a great story yeah so i was talking to him probably he probably
[02:05:38.920 --> 02:05:45.080]   told that story yeah this is how the story goes uh in 2016 i thought you know apple wants to do cars
[02:05:45.080 --> 02:05:52.760]   they should buy tesla i called elan musk and said on one condition i'm ce ceo i called cook i
[02:05:52.760 --> 02:06:00.760]   rather cook thought he meant staying ceo of tesla no no no elan says i mean ceo of apple at which point
[02:06:00.760 --> 02:06:04.680]   tim apple said f you and hung up on me i could totally see
[02:06:04.680 --> 02:06:12.120]   somebody BSing that story right and and and elan would absolutely tell that exact story exactly that
[02:06:12.120 --> 02:06:18.440]   way yeah so so maybe that didn't happen that's that story was told by tim i mean elan but that doesn't
[02:06:18.440 --> 02:06:25.080]   mean it happened but the principle is probably exact it was exaggerating all right good because
[02:06:25.080 --> 02:06:30.200]   higgins i think it's very well respected i didn't know you knew him so that okay that makes a lot that
[02:06:30.200 --> 02:06:35.640]   makes a lot more sense somebody uh told him oh yeah he was elan was going around boasting that
[02:06:35.640 --> 02:06:43.240]   tim cook hung up on him after seeing f u yeah i have a hard time believing that that
[02:06:43.240 --> 02:06:49.080]   tim cook would say that to anybody never but i totally believe that elan would insist on being
[02:06:49.080 --> 02:06:55.480]   ceo the combined company i could see tim saying elan you're high what do you been smoking but that's
[02:06:55.480 --> 02:07:04.440]   all that's all that's all i can uh hey good news apple has beaten a patent troll and the amazing
[02:07:04.440 --> 02:07:11.720]   thing about this story it was actually a federal judge in marshall texas that threw out the patent
[02:07:11.720 --> 02:07:16.440]   a three hundred eight million dollar patent infringement
[02:07:16.440 --> 02:07:27.800]   uh from a company that i think charitably we could call a trols okay patent troll personalize i
[02:07:27.800 --> 02:07:33.560]   was trying to come up with a better phrase uh what do they call them a somebody that there is a
[02:07:33.560 --> 02:07:41.160]   non-practicing it non-practin p e which is a non-practicing entity personalized media communications
[02:07:41.160 --> 02:07:49.720]   apparently the judge was a little judge ronnie gill strap of marshall texas was a little affronted
[02:07:49.720 --> 02:07:55.480]   at the whole thing he said personalized media gets nothing in fact has to cover apple's legal
[02:07:55.480 --> 02:08:02.040]   costs or at least some of them but what he was affronted by is a process i didn't know about
[02:08:02.040 --> 02:08:09.400]   that it's no longer done this way but back in the eighties when the term of a patent would last 17
[02:08:09.400 --> 02:08:17.160]   years the term began not when the application was entered but when you were granted the patent
[02:08:17.160 --> 02:08:25.640]   so personalized media applied in the eighties but slow walked it they they didn't go finish the
[02:08:25.640 --> 02:08:32.440]   paperwork waiting to see if the patent would be worth anything the company filed hundreds of
[02:08:32.440 --> 02:08:37.560]   applications in the late eighties and nineties when these rules were in effect but no patents were
[02:08:37.560 --> 02:08:45.640]   awarded until 2010 and of course at that point they've got you know 17 years from 2010 not from the
[02:08:45.640 --> 02:08:54.600]   eighties uh the judge was in this ridiculous uh the rules have changed it now since 1995
[02:08:54.600 --> 02:09:01.480]   your patent term starts with the date of application and lasts for 20 years after that so this is
[02:09:01.480 --> 02:09:07.720]   called this technique was called submarineing and uh in fact if you ask me it's a giveaway that
[02:09:07.720 --> 02:09:12.440]   you're a patent troll if you do that they apparently would wait to see if the patent was going to be
[02:09:12.440 --> 02:09:17.240]   worth anything before following through they didn't want the clock start ticking until they knew they
[02:09:17.240 --> 02:09:24.280]   could make something out of it wasn't this the same company that that had the uh so supposed patent
[02:09:24.280 --> 02:09:33.880]   on podcasts no it was a similar name yeah i think so oh wow yeah we got a demand letter from them for
[02:09:33.880 --> 02:09:39.480]   some ridiculous amount of money um they actually made a mistake with us because they asked for so
[02:09:39.480 --> 02:09:45.640]   much money that would have been cheaper to defend it in court you and so you know i almost said
[02:09:45.640 --> 02:09:50.600]   that was personal audio personal audio okay that's different yeah personal this is personalized
[02:09:50.600 --> 02:09:58.760]   media based in the fine sugarland texas hey that's right i know appeal box in sugarland
[02:09:58.760 --> 02:10:03.960]   yeah they actually filed dozens of lawsuits and they usually settle because companies go out
[02:10:03.960 --> 02:10:10.040]   it's gonna be cheaper just to give you some some money last year a jury and marshal i think marshal
[02:10:10.040 --> 02:10:15.080]   is going to get a new reputation this used to be the home of these patent lawsuits because uh the
[02:10:15.080 --> 02:10:21.640]   juries were so pro the little guy and so andy the big guy they actually cleared alphabet of claims it
[02:10:21.640 --> 02:10:28.200]   was a fringing patents from personalized media for adaptive video streaming um there's a case
[02:10:28.200 --> 02:10:33.080]   against netflix which they got moved to new york so that's not going to be as probably as successful
[02:10:33.080 --> 02:10:39.960]   as successful as successful as they'd like anyway good i always like seeing a non-practicing entity
[02:10:42.200 --> 02:10:46.680]   take it in the shorts is that okay to say that's stacy i don't even know what it means
[02:10:46.680 --> 02:10:54.440]   i i i don't know stacy's my conscience i was like i don't want that job actually please do not
[02:10:54.440 --> 02:11:00.840]   i would like none of the thoughts i need to have access to your conscience
[02:11:00.840 --> 02:11:05.240]   none of them you don't have to do that anymore unless it unless it hits you
[02:11:05.240 --> 02:11:10.840]   unless you or hits me as you know if it's something you say is like i'm like oh no
[02:11:10.840 --> 02:11:14.360]   be oh don't say that i'm happy to do that but anything else no okay
[02:11:14.360 --> 02:11:22.600]   once i get my punch leo button i can't wait uh actually i want to talk a little bit you have a
[02:11:22.600 --> 02:11:29.720]   good article about uh google's cameras the nest doorbell the nest which is called the hello right uh
[02:11:29.720 --> 02:11:39.160]   nest can this is nest in their their media kit calls it the nest doorbell parentheses battery
[02:11:39.160 --> 02:11:44.200]   but on the side on the imagery on the side and the little it's actually very nice looking it's
[02:11:44.200 --> 02:11:50.600]   called the hell it says hello on the left yeah i have a hello rim yeah anyway so i think it's a hello
[02:11:50.600 --> 02:11:57.240]   but yeah it's the new nest doorbell yeah it is pretty look at that it's very jazzy looking yeah
[02:11:57.240 --> 02:12:02.600]   but you're not but you think they could have done they could have done more with the ai involved
[02:12:02.600 --> 02:12:08.760]   yeah i thought i found this very disappointing and i know it's disappointing probably in the same way
[02:12:08.760 --> 02:12:13.640]   a tiger mom is disappointed in their children right you could be president why are you just hanging
[02:12:13.640 --> 02:12:20.920]   out with a job right so with this google has done a couple things they they've said that they can do
[02:12:20.920 --> 02:12:25.480]   machine learning on the device for nine different models which is very nice on a battery powered
[02:12:25.480 --> 02:12:33.400]   device they can direct packages people vehicles familiar faces if you pay a subscription and animals
[02:12:33.400 --> 02:12:38.040]   including dogs and cats possibly raccoons the the person i talked to is telling me a story about
[02:12:38.040 --> 02:12:43.800]   the raccoons but i'm sad because they don't do anything with this except make notifications a
[02:12:43.800 --> 02:12:50.200]   little bit better and in the smart home i feel like hey if you see a familiar face in the home or
[02:12:50.200 --> 02:12:54.600]   coming in the door maybe you wanted to serve the security system maybe you want to give me that
[02:12:54.600 --> 02:13:01.080]   option or i would love to see you know if you see my dog on my furniture play a siren you know
[02:13:02.280 --> 02:13:09.160]   these are the types of things i think in google has beat into us that it is really good at machine
[02:13:09.160 --> 02:13:14.360]   learning and it is usually really good at machine learning so i'm perplexed as to why they didn't
[02:13:14.360 --> 02:13:18.200]   do more with this i guess that's why i'm disappointed they're perfectly fine as a revamp
[02:13:18.200 --> 02:13:27.320]   i personally you know have almost no smart devices in our home in fact my wife doesn't want
[02:13:27.320 --> 02:13:32.840]   cameras inside the house at all and yeah at least in the middle of that but that's a doorbell is
[02:13:32.840 --> 02:13:39.000]   outside right yeah well even even that the only doorbell or the only camera we've had on the outside
[02:13:39.000 --> 02:13:45.240]   is one wise camera that i set up in the backyard just to watch a robin's nest that had
[02:13:45.240 --> 02:13:53.720]   some robin's nest in one of the bushes beside our garage but you know we're we just not into that
[02:13:53.720 --> 02:14:03.400]   stuff so it's but you know it's interesting that you know google has you know not done as much as
[02:14:03.400 --> 02:14:09.800]   they could you know with the ai in this thing yeah that's funny my entire house is like that
[02:14:09.800 --> 02:14:16.280]   except for i can't put a camera on my door because condo and i of course no cameras in the house
[02:14:16.280 --> 02:14:21.240]   because condo but i love having like wait wait wait wait wait wait wait wait wait wait wait you have
[02:14:21.240 --> 02:14:25.320]   cameras in the house we could we have all the doors have to be the same right and so not
[02:14:25.320 --> 02:14:30.120]   everybody's going to buy the you're going to get like all myriad versions of jazz if you said nest
[02:14:30.120 --> 02:14:36.840]   i mean start not nest alone ring alone there's like 27 different looking rings and so you know
[02:14:36.840 --> 02:14:40.840]   you got people like me who buy the high end just because and then you get other people like that
[02:14:40.840 --> 02:14:44.920]   i'm going to get the five dollar one i got from you know some bootleg place and then just would make
[02:14:44.920 --> 02:14:49.560]   the hallways look at so we can't do it but we're working on trying to get the board to come to
[02:14:49.560 --> 02:14:56.280]   some decision to allow it because it's extremely helpful for the ups man right we all get packages
[02:14:56.280 --> 02:15:04.680]   every day so i know my ups man he's been my new p_s man for 30 years so like i i'm matt knows apple
[02:15:04.680 --> 02:15:09.800]   days okay i know he's not home yet but let me call him and let him know i will be the apple day matt
[02:15:09.800 --> 02:15:16.600]   leave the phone yo he knows he he will tense me hey i'm almost there i will take off from where i am
[02:15:16.600 --> 02:15:22.360]   to get like you know you still got inside right get to know you p_s guy man it's a marvelous thing
[02:15:22.360 --> 02:15:27.880]   so yeah i i agree i saw some of the stuff you know that's not quite there yet with the smart home
[02:15:27.880 --> 02:15:33.160]   does feel like a let down when you run into that situation and so for me again a big one
[02:15:33.160 --> 02:15:39.880]   say i'm cooking shabu shabu and i get a false positive on my nest uh smoke detector i can't just
[02:15:39.880 --> 02:15:46.040]   say hey you know g people can you kill the smoke detector i'm chef boy i burn him up today let it
[02:15:46.040 --> 02:15:52.280]   happen and then they'll be like okay but you can't i like either got to disable it or you know mess
[02:15:52.280 --> 02:15:57.000]   with the don't pay any attention to this for an hour thing it's it's i'm what states it should be
[02:15:57.000 --> 02:16:02.440]   smoother by now because it's been so long and i guess my other fight is when you live in a place
[02:16:02.440 --> 02:16:07.160]   that only has air condition i can't get a smart thermostat because they all want to work off
[02:16:07.160 --> 02:16:14.280]   for h vac we don't use h we only got vac so i need i don't know what to do with this driving me insane
[02:16:15.400 --> 02:16:20.680]   so do you think stacey part of the reason they don't make these smarter is they're concerned i mean
[02:16:20.680 --> 02:16:27.320]   google port google anytime they do anything with ai people go creepy crossing the creepy line
[02:16:27.320 --> 02:16:36.040]   you think they're worried about that the creepy line i don't know i mean so i had in like 2015 i
[02:16:36.040 --> 02:16:43.720]   think 2014 even a net admo camera yeah netamo i don't know how to say it and when i had it had
[02:16:43.720 --> 02:16:50.360]   familiar faces free actually so if it recognized it recognized all my family's faces and if it
[02:16:50.360 --> 02:16:56.680]   didn't see them for a while it would turn on it would arm the security system basically and when
[02:16:56.680 --> 02:17:02.360]   it saw us come home it would disarm it and that didn't feel creepy net admo did everything locally
[02:17:02.360 --> 02:17:09.240]   on the device the object detection models are running locally on the google device which makes
[02:17:09.240 --> 02:17:17.320]   it less creepy so i don't know if it's a creepy factor or if it's just i really i just don't understand
[02:17:17.320 --> 02:17:21.720]   and it makes me sad because i really i do like people i do think what they're doing in the smart
[02:17:21.720 --> 02:17:29.720]   home is is actually pretty nice so yeah i don't know i somebody said that i could just read the
[02:17:29.720 --> 02:17:34.520]   only thing creepy about this device is the google logo they're not actually doing anything creepy
[02:17:34.520 --> 02:17:39.320]   but just because it says google people are going to think that and i forgot where i heard that it
[02:17:39.320 --> 02:17:45.160]   might have been rene but that's true is the logo yeah there's no logo on that hello doorbell thank
[02:17:45.160 --> 02:17:52.440]   goodness i do wish it would say things like joe's at the door or i mean it says there's someone at the
[02:17:52.440 --> 02:17:58.120]   front door i was hoping that these newer capabilities will be will be better yeah like oh yeah like
[02:17:58.120 --> 02:18:04.840]   like there's you give it a good example of a the swimming pool example of uh oh i forgot about that
[02:18:04.840 --> 02:18:09.960]   yeah that's a great example it is uh where you put a camera you can say you can say that there's a
[02:18:09.960 --> 02:18:13.960]   zone so you can say this is a swimming pool zone what you'd like it to be able to do is say if
[02:18:13.960 --> 02:18:18.280]   if an adult enters the zone don't worry about it but if a kid enters the zone let me know
[02:18:18.280 --> 02:18:25.720]   would be really useful uh that will be huge huge safety feature so i agree with you on that
[02:18:26.920 --> 02:18:29.560]   uh let's take a little break we can wrap it up because it's waffle time
[02:18:29.560 --> 02:18:37.240]   in on vane bridge island uh but you're not wrong leah you're not wrong
[02:18:37.240 --> 02:18:45.880]   but first a word from our sponsor this show is supported by at and t active armor we use our
[02:18:45.880 --> 02:18:50.200]   phones all the time these days i mean come on everybody does we're always on them whether it's
[02:18:50.200 --> 02:18:56.200]   live streaming content catching up with the family on weekly video calls watching your favorite
[02:18:56.200 --> 02:19:01.800]   podcast that's great the last thing you want in the middle of a show is a fraudulent call trying
[02:19:01.800 --> 02:19:06.920]   to sell your auto insurance i just you know it got to the point it was so bad for a while that i
[02:19:06.920 --> 02:19:13.240]   just stopped answering the phone then along comes at and t with this incredible solution active armor
[02:19:13.240 --> 02:19:19.400]   at and t makes customer security a priority and helps block those pesky calls it's not
[02:19:19.400 --> 02:19:28.120]   complicated at t active armor 24 7 proactive network security and fraud call blocking
[02:19:28.120 --> 02:19:33.000]   to help stop threats and you know what at no extra charge of course you do have to have a
[02:19:33.000 --> 02:19:40.440]   compatible device and service required visit at t.com/active armor for details and i can say
[02:19:40.440 --> 02:19:48.840]   this sincerely thank you at and t i can answer my phone again uh all right all right you did
[02:19:48.840 --> 02:19:53.400]   quickly tell me by swarm technologies is going to be important for space x
[02:19:53.400 --> 02:19:59.720]   actually stacey yeah you talked about this on your i just had yeah i had sarah spengalo on the show
[02:19:59.720 --> 02:20:06.760]   a couple couple months back i thought this was both a crazy thing and a really scary thing because
[02:20:06.760 --> 02:20:11.800]   our swarm is tiny one of the founders of swarm we should say so yes sarah spengalo is the CEO
[02:20:11.800 --> 02:20:20.040]   she's also a former canadian astronaut just oh nice so fancy they make these things called tiles
[02:20:20.040 --> 02:20:28.040]   and they connect to the tiny little satellites that are up there in the orthor orbit
[02:20:28.040 --> 02:20:36.280]   and the cool thing is they can deliver internet access to things low bandwidth internet access
[02:20:36.280 --> 02:20:41.640]   for five dollars a month i'm actually testing out a swarm satellite service right now or
[02:20:41.640 --> 02:20:47.000]   eval kit right now she's kind of like uh mini starlink sort of going
[02:20:47.000 --> 02:20:54.040]   is it low low bandwidth starlink basically it's it's a low power WAN so it's low power
[02:20:54.040 --> 02:20:58.120]   wide area network this is like you know how i always talk about the internet of things needs
[02:20:58.120 --> 02:21:04.600]   connectivity this cheap ubiquitous and like the sidewalk like sidewalk yeah it's like sidewalk
[02:21:04.600 --> 02:21:11.400]   except this is satellite based um and they have 121 satellites in orbit so it's it's a non-trivial
[02:21:11.400 --> 02:21:16.440]   network they've got up there yeah no they've done and they're gonna i can't remember i'm sorry i'm
[02:21:16.440 --> 02:21:21.640]   i'm writing a story at it actually right now but i haven't got all the data in there yet it's got
[02:21:21.640 --> 02:21:30.040]   tks i can have some stats uh seven hundred fifty data packets seven hundred fifty data packets a
[02:21:30.040 --> 02:21:37.480]   month but that's five dollars a month and it's global so who would pay for that the company that
[02:21:37.480 --> 02:21:43.640]   made the iot device or would the end user pay for that i would think most well it depends on the
[02:21:43.640 --> 02:21:48.440]   business model you want to have yeah but probably maybe be part of a subscription like a nest
[02:21:48.440 --> 02:21:53.720]   subscription yeah you'd make it if you were doing consumer it'd be a subscription if you were building
[02:21:53.720 --> 02:21:59.240]   a like this is their their customer base a lot of them are like in the industrials kind of world so
[02:21:59.240 --> 02:22:06.040]   they're like monitoring sensors on gas pipelines and this is great for oh yeah yeah 750 data packets
[02:22:06.040 --> 02:22:10.120]   i don't know how many bits that is but it's not a lot she says they can send email oh she wants me
[02:22:10.120 --> 02:22:15.080]   to test out sending email that's amazing do downlink but not uplink they have the
[02:22:15.080 --> 02:22:19.800]   technical capability for uplink but they don't advertise it and it's not where they want to go
[02:22:19.800 --> 02:22:25.560]   with this well so it's not gonna be up to them anymore because well yeah space is now the owner
[02:22:28.120 --> 02:22:32.520]   they get cheap connectivity for iot devices and do you think that they'll somehow
[02:22:32.520 --> 02:22:37.560]   roll this technology into starlink or is or do they just see this as an adjunct business
[02:22:37.560 --> 02:22:45.080]   i don't know enough about where starlink i'd have to research where starlink is are they in the ku band
[02:22:45.080 --> 02:22:51.080]   or are they oh gosh i don't know i don't know that's a good question i mean i used to cover it so i
[02:22:51.080 --> 02:22:57.000]   feel like star links in a separate band so you can't roll these together in the sky so it just be
[02:22:57.000 --> 02:23:01.320]   another business that's that they would do because they've already got these satellites up there
[02:23:01.320 --> 02:23:07.640]   yeah i mean and you i'm trying to think of the logical reason to have a dual-band
[02:23:07.640 --> 02:23:14.920]   receiver down that could do ku or it's right right now if i weren't talking i'd be furiously
[02:23:14.920 --> 02:23:21.560]   looking up what band starlink is too expensive uh to i mean you really it doesn't make sense to use
[02:23:21.560 --> 02:23:28.120]   a for iot the the receivers are bigger everything you know none of this makes any sense so it seems
[02:23:28.120 --> 02:23:33.400]   like an interesting expansion of starlink what do you think sam you i know you follow uh Elon quite
[02:23:33.400 --> 02:23:42.520]   closely um yeah i mean this is this i'm not really familiar with this particular one you know i think
[02:23:42.520 --> 02:23:50.040]   starlink i think has is a fascinating technology you know and the ability to provide uh connectivity
[02:23:50.040 --> 02:23:59.080]   everywhere um i i love the the concept so far what i've seen you know from early betas of starlink
[02:23:59.080 --> 02:24:04.040]   you know it's not as good as was claimed but you know maybe when there's getting better another
[02:24:04.040 --> 02:24:07.880]   ten thousand another ten thousand satellites up there i'll tell you yeah then maybe it'll be good i
[02:24:07.880 --> 02:24:13.560]   follow the starlink subreddit uh and it's expensive it's well that's a hundred bucks a month five
[02:24:13.560 --> 02:24:18.600]   hundred dollars for the gear but if you are in an area where you don't get internet yeah if there's
[02:24:18.600 --> 02:24:25.320]   no cable or fly where you want then you know it's uh it's a really for a lot of people and i have to
[02:24:25.320 --> 02:24:31.480]   say the vast majority of people on the starlink subreddit on reddit love it they're thrilled
[02:24:31.480 --> 02:24:35.000]   and there's a lot of people saying i when i when i get it when do i get it
[02:24:35.000 --> 02:24:40.920]   eilons also started is implied that it will be mobile at some point which is to me very interesting
[02:24:40.920 --> 02:24:45.320]   right now it has to be tied to an address but uh at some point he says we're going to be able to
[02:24:45.320 --> 02:24:49.560]   well i think that you can have a mobile device doing this which would be very i think i think the
[02:24:49.560 --> 02:24:55.240]   real challenge there with with the mobile part is you know that you have to keep the the antenna aimed
[02:24:55.240 --> 02:25:01.240]   at a satellite um you know this is and the battery power yeah and the battery power yeah see i want
[02:25:01.240 --> 02:25:07.320]   to have a cruise ship so i can tell his view of the starlink beta you know he he had a hard time
[02:25:07.320 --> 02:25:12.360]   just finding a place where he could get a clean view of the sky challenge you know that's even a
[02:25:12.360 --> 02:25:17.720]   tree in the way of obstruction is allowed yeah is to kill the signal we have a host who does
[02:25:17.720 --> 02:25:23.480]   uh floss weekly jonathan i know who is raving but he's in noklohoma so again yeah it really helps
[02:25:23.480 --> 02:25:29.080]   to have no such trees you fatter at the cake the twins have that thing too ordinary uh new
[02:25:29.080 --> 02:25:34.360]   england coast somewhere not that far away from the headquarters they actually got one for their beach
[02:25:34.360 --> 02:25:40.840]   house all right and yeah he said yeah like it was all good at first and then the tree you know gotta
[02:25:40.840 --> 02:25:46.520]   get these trees out of way it became not winter anymore and the tree boom is like oh i gotta move
[02:25:46.520 --> 02:25:53.000]   this thing you gotta put it on a pole taller than the trees that's the key um you said it's the k
[02:25:53.000 --> 02:26:00.040]   a k u bandstacy it is the k a so they're we're talking you know swarm satellites are down here
[02:26:00.040 --> 02:26:07.640]   elons are oh here let's there we go up here elon yeah sarahs yeah um and then their radios are
[02:26:07.640 --> 02:26:16.280]   going to be transmitting it different decibels and i don't see it does not seem reasonable or
[02:26:16.280 --> 02:26:21.800]   possible now starlink has been focused on building software that can really optimize
[02:26:21.800 --> 02:26:28.280]   um for coverage and in communication like i don't believe elon and his mobile stuff right now i'm
[02:26:28.280 --> 02:26:35.640]   just going to tell you honestly um so maybe there's some like maybe swarm has uh you have little
[02:26:35.640 --> 02:26:44.680]   faith in elon i can't magical technologies or this could be the other thing that swarm has is access
[02:26:44.680 --> 02:26:52.680]   to the same satellite airwaves so the same spectrum that orbcom is using and i can't remember the
[02:26:52.680 --> 02:26:58.760]   wavelengths the frequency range right off the top of my head but um so this could be a play to get
[02:26:58.760 --> 02:27:07.000]   access to more airwaves which would then have more could allow elon to have more capacity on
[02:27:07.000 --> 02:27:13.640]   SpaceX this is interesting this is the swarm eval kit for evaluating 500 you have one of those
[02:27:13.640 --> 02:27:20.440]   it's my oh should i tell people so this is a solar panel this is a solar panel uh that the
[02:27:20.440 --> 02:27:25.800]   transmitter is just this little antenna is all yeah it's good so there's a battery so it'll run
[02:27:25.800 --> 02:27:32.680]   yeah so you have one what do you do with it i well i want to take it out next week i'm hoping to take
[02:27:32.680 --> 02:27:37.080]   it out to the olympics to test actually where i won't have connectivity she doesn't mean it right
[02:27:37.080 --> 02:27:40.840]   now the summer olympics in tokyo she meets the amount olympics amounts in to be queer
[02:27:40.840 --> 02:27:45.400]   the mountains that are you know two hours to the west of east i'd like to take this to the olympics
[02:27:45.400 --> 02:27:52.120]   and stream the swimming event uh you couldn't stream it's one way it's uh it's one way only yeah
[02:27:52.840 --> 02:27:58.600]   uh and by the way the data packets are 192 bytes up to 192 bytes per packet so it's really a very
[02:27:58.600 --> 02:28:03.160]   low bandwidth but suitable for us yeah it's optimized for sensors yeah yeah yeah very interesting
[02:28:03.160 --> 02:28:07.640]   you know companies like Bechtel you know when they're doing these things in their bridges they put
[02:28:07.640 --> 02:28:13.160]   sensors in right when they're walking when they do the bridge walks they can walk over the bridge
[02:28:13.160 --> 02:28:17.320]   actually Bechtel is big in the bay so when they walk across the bridge they can detect if there's
[02:28:17.320 --> 02:28:22.760]   any cracks or any issues that you're having you know with the cabling or whatever so yeah this
[02:28:22.760 --> 02:28:27.560]   would be cool because i'm think i was just looking at the picture here and even with ag like you can
[02:28:27.560 --> 02:28:32.360]   tell if you're having some problem in the field you can just send a signal and then someone knows
[02:28:32.360 --> 02:28:37.320]   to get there right away and solve it so yeah it's kind of cool well wrap things up with a happy
[02:28:37.320 --> 02:28:47.240]   birthday to the world wide web launched 30 years ago august 6th the very first website ever launched
[02:28:47.240 --> 02:29:01.000]   Tim Berners-Lee august 6th 1991 it was actually it said the world wide web w3 is a wide area hyper linked
[02:29:01.000 --> 02:29:08.120]   something or other pretty cool it's hard to believe this thing that we take completely for
[02:29:08.120 --> 02:29:16.520]   granted is only 30 years old that you know is it was invented before or after a lot of us were
[02:29:16.520 --> 02:29:24.520]   born and pretty cool so there is a project to restore that page was launched in 2013
[02:29:24.520 --> 02:29:34.120]   um oh wait a minute then PR says here it is you can go there let's do it info.sern.ch browse the
[02:29:34.120 --> 02:29:42.920]   first website that's it it's pretty basic a wide area hyper media information retrieval
[02:29:42.920 --> 02:29:48.200]   initiative aiming to give universal access to a large universe of documents would you have invested
[02:29:48.200 --> 02:29:56.120]   money in this if you'd read that sentence 30 years ago i don't think so stacey find out about
[02:29:56.120 --> 02:30:01.960]   swarm we'll talk again on wednesday on this week in google we missed you terribly last two weeks i
[02:30:01.960 --> 02:30:08.200]   can't wait to get you back stacey's website stacey on iot.com subscribe to her free newsletter check
[02:30:08.200 --> 02:30:13.560]   out the events and of course you got to listen to the iot podcast stacey does with kevin toffle
[02:30:13.560 --> 02:30:21.160]   waffles in your future. something snacky because i am hungry. something snacky coming up
[02:30:21.160 --> 02:30:27.640]   it's great to see you stacey thank you so much for spending your sunday with us we really appreciate
[02:30:27.640 --> 02:30:35.000]   it thanks thanks thanks your family too for giving you up i'm coming over for karen's uh
[02:30:36.040 --> 02:30:43.160]   onigiri anytime you say i'll be there doc rock youtube.com i'm sorry karen's mom
[02:30:43.160 --> 02:30:50.920]   karen's mom yeah does karen make it too uh no i see she don't have to cook she has me and her
[02:30:50.920 --> 02:30:57.560]   mom so she has to cook and all the most loved cooking good good deal man you're a catch
[02:30:57.560 --> 02:31:05.080]   uh youtube.com/docrock of course he is uh the uh e-cam what do you call yourself with e-cam the
[02:31:05.080 --> 02:31:09.320]   e-cam out-me-a-manager community man guy who gives you all that the most
[02:31:09.320 --> 02:31:18.360]   but you can take it e-c-a-m-m.com it's a really useful tool for live streaming mica uses it to do
[02:31:18.360 --> 02:31:26.680]   ios today so does rosemary um it's really really neat and it's uh m1 compatible if you've got a
[02:31:26.680 --> 02:31:32.360]   mac that's really cool and not just compatible m1 optimized anything else you want to plug doctor
[02:31:33.320 --> 02:31:39.160]   no really um just doing my thing on my show and you know having a good time and yeah you know
[02:31:39.160 --> 02:31:43.720]   i just wish everybody would dive in and if you ever have any questions anybody you guys can reach
[02:31:43.720 --> 02:31:48.600]   me you know i'm always down to help you're the best it's always a pleasure to have you thank
[02:31:48.600 --> 02:31:53.880]   you for being here thank you uh thanks also to sam this was fun it was old home week thanks also to
[02:31:53.880 --> 02:31:59.960]   sam ables samad guy's house insight he's a principal principal researcher there he's my car guy and
[02:31:59.960 --> 02:32:07.640]   if you love autos and auto tech you'll love wheel bearings his podcast we does with my good friend
[02:32:07.640 --> 02:32:14.040]   robbie that's nice and nicole oops that's a doc rocks page you can show that and then i'll show
[02:32:14.040 --> 02:32:18.680]   wheel bearings i was like man sam looks better than that dude on that page
[02:32:18.680 --> 02:32:27.880]   wheel bearings dot media uh how's how's it going you having fun you i know you just drove the
[02:32:27.880 --> 02:32:34.120]   Mustang this week robbie was in switzerland for crying out loud yeah he got to go over there
[02:32:34.120 --> 02:32:40.200]   to drive the numerisades bends eq s uh their e v that's a good deal rberto bald one of course
[02:32:40.200 --> 02:32:48.040]   uh this week he's got the uh new Cadillac Escalade holy cow we're actually going to record a new
[02:32:48.040 --> 02:32:53.960]   episode uh this evening oh good after we're done with this so okay anything else you want to
[02:32:53.960 --> 02:32:58.680]   plug sam it's always a pleasure to have you on uh you know that my day job is an analyst at
[02:32:58.680 --> 02:33:04.680]   guide house insights you know if you're interested in anything any research related to emerging
[02:33:04.680 --> 02:33:10.280]   technologies around energy use you know check it out guidehouseinsides.com and uh check out our
[02:33:10.280 --> 02:33:16.920]   blog there where all the analysts write stuff right interesting stuff there um so uh and you know
[02:33:16.920 --> 02:33:22.360]   if you need anything uh any any market research in those areas that you don't find listed on the
[02:33:22.360 --> 02:33:27.720]   site already you know give us a shout we'll be happy to talk to you it's uh you know we don't
[02:33:27.720 --> 02:33:31.560]   plug that enough so i'm glad you mentioned that i always mentioned that you work there but we
[02:33:31.560 --> 02:33:37.400]   don't talk about what you do there so uh that's good guidehouseinsides.com we're all we're all about
[02:33:37.400 --> 02:33:45.320]   energy and sustainability nice nice that's a good area to be in these days i suspect certainly is
[02:33:45.320 --> 02:33:50.840]   yes it keeps us busy yes thank you all for being here we do this weekend oh you know what i didn't
[02:33:50.840 --> 02:33:58.520]   do yet do we have a promo let's sneak that in real quickly this is what you missed this week on twit
[02:33:58.520 --> 02:34:05.240]   maybe you didn't feel more sorry that we we have non-visual listeners for this i can't
[02:34:05.240 --> 02:34:14.680]   describe this picture cable room the aesthetic taste of the person is reflected in the chair
[02:34:14.680 --> 02:34:25.960]   that we can see because whoa yeah anyway previously on twit hands on tack today i want to talk to
[02:34:25.960 --> 02:34:31.080]   you folks about another laptop that i got the chance to play with this is a budget-friendly
[02:34:31.080 --> 02:34:39.800]   laptop 40 corporate person it's always on the go it is the ulyn novo think pad el 13 yoga
[02:34:39.800 --> 02:34:46.600]   jin to all about android we started the week off with google uh going ahead and unveiling the
[02:34:46.600 --> 02:34:52.680]   pixel six and the pixel six pro um and it was revealed that google is aiming for the top of the
[02:34:52.680 --> 02:34:59.400]   line premium level phone here windows weekly panos tweeted here's a little sneak peek of the
[02:34:59.400 --> 02:35:04.600]   new snipping tool for windows 11 this thing must be a project reunion app because what they've done
[02:35:04.600 --> 02:35:09.800]   is they figured out a way to bring modern ui to a legacy app that's what this is and that's what
[02:35:09.800 --> 02:35:12.680]   the new paint is going to be and then there was something else i don't know if there and then
[02:35:12.680 --> 02:35:20.200]   that's why i was pumped to it bring your brain will do the rest that's what years of following
[02:35:20.200 --> 02:35:26.600]   microsoft is has done for you yeah and listen it's so useful like uh cottle parties or in conversations
[02:35:26.600 --> 02:35:31.080]   with friends it's like so what do you think of the new snipping tool i am the toast baster i
[02:35:31.080 --> 02:35:36.760]   really glad you asked me that actually paul's right now he's in mexico went to mexico city
[02:35:36.760 --> 02:35:41.240]   sam Miguel de iande and wanawato i can't wait to talk to him when he gets back he's got a
[02:35:41.240 --> 02:35:47.640]   lot uh lot uh of stuff he's been uh visiting so i look forward to windows weekly uh this coming
[02:35:47.640 --> 02:35:54.120]   wednesday and of course right after that this week in google uh if you are not yet a member of club
[02:35:54.120 --> 02:35:59.560]   twit i might uh want to encourage you to help support the network it makes a big difference
[02:36:00.200 --> 02:36:06.360]   to us it has really turned out to be a real success it's a way of supporting twit by donating
[02:36:06.360 --> 02:36:11.960]   seven bucks a month we've decided to give you some benefits including ad-free versions of all
[02:36:11.960 --> 02:36:17.160]   the shows in fact if you're a club twit member you're not even hearing this because it's cut out of
[02:36:17.160 --> 02:36:23.320]   the show uh you also get and by the way audio and video you also get access to our great discord
[02:36:23.320 --> 02:36:28.280]   which is a lot of fun we we've really been enjoying the discord i think it's my new social network
[02:36:28.840 --> 02:36:33.480]   and the twit plus feed which has all the good stuff before and after the shows that
[02:36:33.480 --> 02:36:39.560]   it's the cutting room floor we just scrape those up we package them into sausage and
[02:36:39.560 --> 02:36:44.520]   send it your way via the twit plus bonus content feed all of that seven bucks a month
[02:36:44.520 --> 02:36:50.200]   but mostly you do it for the warm and fuzzy feeling that you're helping support uh what we do here
[02:36:50.200 --> 02:36:55.480]   at twit and we really are grateful you want to know more twit.tv/club twit
[02:36:56.760 --> 02:37:04.120]   we do this week in tech every sunday afternoon 230 pacific that's 530 eastern time 2130 UTC you're
[02:37:04.120 --> 02:37:10.920]   invited to watch us live if you want no charge there's a live audio and video stream at twit.tv/live
[02:37:10.920 --> 02:37:16.120]   if you're watching live chat with us live of course there's the irc which has been around now
[02:37:16.120 --> 02:37:23.480]   almost as long as the world wide web i think i started doing it in 92 at irc.twit.tv there's
[02:37:23.480 --> 02:37:28.120]   also the brand new discord channel if you're a member of club twit you could twit chat live
[02:37:28.120 --> 02:37:36.600]   and of course do the animated gifts in our discord discord channel you can get shows on
[02:37:36.600 --> 02:37:42.120]   demand after the fact on our website twit.tv there's a youtube channel devoted to all of our
[02:37:42.120 --> 02:37:46.520]   shows each channel has each show has its own channel or you could subscribe in your favorite
[02:37:46.520 --> 02:37:51.480]   podcast client do me a favor if they allow reviews you know on the apple podcaster
[02:37:52.600 --> 02:37:57.720]   pocket casts or google leave us a five star review let the world know share your love
[02:37:57.720 --> 02:38:03.960]   we appreciate it helps help spread the word about this week in tech and of course if you listen
[02:38:03.960 --> 02:38:08.840]   on demand you might want to chat on demand we have not only the discord we have a discourse our
[02:38:08.840 --> 02:38:16.280]   own forums at twit.community and our social media uh it twit.social it's uh the fediverse it's a
[02:38:16.280 --> 02:38:24.120]   mastodon i'll give you a log into that someday john twit.social thanks everybody for being here
[02:38:24.120 --> 02:38:29.400]   it's a lot of fun i'll see you next week or maybe during the week during some of our other shows
[02:38:29.400 --> 02:38:32.200]   another twit is in the cage take care
[02:38:32.200 --> 02:38:43.080]   do the twit all right do the twit baby do the twit all right do the twit baby do the

