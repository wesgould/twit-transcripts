;FFMETADATA1
title=A Fat Suit for Your Phone
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=636
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.640]   It's time for Twit this week in Tech. Great panel, Aaron Griffith from Wired,
[00:00:04.640 --> 00:00:11.600]   Granawoo, Space Cat, Gallon, Canada for Congress, and Dan Patterson from CBS Interactive.
[00:00:11.600 --> 00:00:16.320]   We're going to talk about new social media guidelines for reporters at the time. So they
[00:00:16.320 --> 00:00:20.720]   have a good idea, a bad idea. Twitter says we're going to change things.
[00:00:20.720 --> 00:00:27.920]   Honest, we'll talk about Darth Vader and his friends and the best-selling game in America.
[00:00:27.920 --> 00:00:30.960]   It's Gone Platinum. It's all coming up next on Twit.
[00:00:30.960 --> 00:00:37.520]   Netcast you love. From people you trust.
[00:00:37.520 --> 00:00:50.240]   This is Twit. Ban with for this week in Tech is provided by CashFly at cachiefly.com.
[00:00:55.360 --> 00:01:02.880]   This is Twit this week at Tech. Episode 636 recorded Sunday, October 15th, 2017.
[00:01:02.880 --> 00:01:08.720]   A fat suit for your phone. This week at Tech is brought to you by Carbonite. Keep your business
[00:01:08.720 --> 00:01:14.480]   safe this year. Protect it from ransomware and hacker attacks with automatic cloud backup from
[00:01:14.480 --> 00:01:18.960]   Carbonite. Try it free at carbonite.com. But don't forget to use the offer code TWIT
[00:01:18.960 --> 00:01:24.880]   so you get two free bonus points if you decide to buy. And buy Euro. Never think about Wi-Fi again
[00:01:24.880 --> 00:01:31.200]   with Euro's hyper-fast, super simple Wi-Fi system. And now the second generation Euro is here.
[00:01:31.200 --> 00:01:36.400]   Try banned twice as fast and you can get free overnight shipping when you visit Euro.com.
[00:01:36.400 --> 00:01:40.320]   Select overnight shipping at checkout and enter our promo code TWIT.
[00:01:40.320 --> 00:01:48.000]   And by ITProTV, a good ITPro is always learning and ITProTV is the resource to keep you and your
[00:01:48.000 --> 00:01:56.000]   team's skills up to date. Visit itpro.tv/twit and use the code TWIT30 to get a free seven-day trial
[00:01:56.000 --> 00:02:00.560]   and 30% off a monthly membership for the lifetime of your active subscription.
[00:02:00.560 --> 00:02:06.080]   And by the Ring Video Doorbell with Ring you can see and talk to anyone at your door
[00:02:06.080 --> 00:02:12.160]   or in your backyard from anywhere in the world using your smartphone. It's like Call or ID for
[00:02:12.160 --> 00:02:17.840]   your house. Go to ring.com/twit and get up to $150 off a Ring of Security Kit.
[00:02:17.840 --> 00:02:26.160]   It's time for TWIT this week in Tech to show we cover the great, late, late, great tech news.
[00:02:26.160 --> 00:02:31.840]   This week's Tech News with a panel of brilliant people. Aaron Griffith is here. Her new job,
[00:02:31.840 --> 00:02:38.720]   I'm thrilled to say senior writer at Wired. She came from Fortune and we met at the podcast
[00:02:38.720 --> 00:02:43.760]   up front a couple of years ago. How many of those have they had now? They just had another one in
[00:02:43.760 --> 00:02:50.480]   September and they're bigger and more important than ever. How big is the podcast advertising industry
[00:02:50.480 --> 00:02:54.160]   now? How big of a market is that? Not as big as I'd like it to be.
[00:02:54.160 --> 00:02:58.880]   Pretty small. Smaller than the amount of coverage and hype that it gets. I think people would be
[00:02:58.880 --> 00:03:04.560]   surprised at how small it actually is compared to... Aaron, it's a renaissance. We're having a
[00:03:04.560 --> 00:03:09.840]   renaissance, please. They're wrong to say on a podcast, right? No, it's the right thing to say.
[00:03:09.840 --> 00:03:15.520]   It's the honest thing to say. I always want you to... That's a good point. I just saw the stat
[00:03:15.520 --> 00:03:23.360]   for radio ad sales. Like 18 billion years ago. 18 billion. But we get a tiny, tiny fraction of that.
[00:03:23.360 --> 00:03:28.320]   Hundreds of millions still, right? I don't even know if it's that high, to be honest still.
[00:03:29.280 --> 00:03:38.000]   Our net... Actually, gross revenue net of commissions last year was 9.8 million.
[00:03:38.000 --> 00:03:44.400]   But that's... So we're one of the largest independent podcast networks. But if you say there's 10 of them,
[00:03:44.400 --> 00:03:48.160]   that's... I mean, I wouldn't know. I don't bet it's not hundreds.
[00:03:48.160 --> 00:03:53.200]   Unless there's a really long term. I was thinking I read 150 a couple of year, 150 that's gone.
[00:03:53.200 --> 00:03:54.880]   But that's all a guess. Nobody ever asked me.
[00:03:56.880 --> 00:04:01.840]   And I tell people... Because you say it out loud. I tell people, but I'm sure Panoply and Gimlet
[00:04:01.840 --> 00:04:08.080]   aren't saying what their revenue is, because I think it's probably a low number. It's... Yeah,
[00:04:08.080 --> 00:04:11.920]   you don't make a lot of money in podcasts. Remember that everybody and stay away.
[00:04:11.920 --> 00:04:18.400]   Dan Patterson's also here from Tech Republic at CBI. A CBS interactive, always a good friend.
[00:04:18.400 --> 00:04:24.080]   Great to see you again, Dan. Always great to be here. I remember podcasting back when you had to
[00:04:24.720 --> 00:04:30.400]   hand code that XML and put that orange button on your site. And Adam Curry's pod show was going
[00:04:30.400 --> 00:04:36.880]   to change the world. I did. I hand crafted our RSS feeds for probably a year or two.
[00:04:36.880 --> 00:04:42.720]   Yeah, I mean to... Make it sound artisanal. You know what the name of our agency, the Ad Sales
[00:04:42.720 --> 00:04:50.480]   Agency that sells our ads is called Artisanal. Because we are handmade media. We make each show
[00:04:50.480 --> 00:04:57.200]   by hand. What's the Etsy of media? Us. Us. Us.
[00:04:57.200 --> 00:05:02.640]   Hey, you know what else is here? It's really great to welcome back. Brianna Wu, we love Brianna
[00:05:02.640 --> 00:05:08.720]   Spacecat Gal. She's a game developer, giant space cat, but also running for Congress in the
[00:05:08.720 --> 00:05:15.680]   Massachusetts 8th district. And there will be an election someday. Someday next year it's going
[00:05:15.680 --> 00:05:20.640]   to be probably in September. So I've got one to do. Your opponent has run without opposition for
[00:05:20.640 --> 00:05:26.320]   some time. Isn't that right? Quite a while. You know, he had a union challenger a few years ago,
[00:05:26.320 --> 00:05:32.480]   but they didn't manage to make a significant dent. And I think people are ready for a change.
[00:05:32.480 --> 00:05:37.440]   That's the opponent in the Democratic primaries. And then it would be an election. That's correct.
[00:05:37.440 --> 00:05:44.960]   I'm trying to primary him. He's not a Democrat in my opinion. He holds... He's very weak on women's
[00:05:44.960 --> 00:05:51.760]   rights. He got into politics originally to actually fight against gay rights. You know, he voted against
[00:05:51.760 --> 00:05:56.720]   the assault weapons ban. He's not really a Democrat. He got into politics because he wanted to fight
[00:05:56.720 --> 00:06:03.600]   gay rights. Absolutely. Absolutely. It's good to have a cause. Good to have a cause. In Massachusetts.
[00:06:03.600 --> 00:06:09.360]   In Massachusetts. In Massachusetts. Yeah, one of the first bills he tried to pass when he was in
[00:06:09.360 --> 00:06:15.680]   the state house was a bill that would basically make... It would basically erase hate crimes
[00:06:15.680 --> 00:06:22.000]   against gay people and transgender people. If it was found out that the gay or transgender person
[00:06:22.000 --> 00:06:27.520]   was acting quote unquote "lessiviously," this was in the Guena Ruho and Matthew Shepard era.
[00:06:27.520 --> 00:06:30.160]   So that's our goal. Talk about blaming the victim. Oh, you were acting
[00:06:30.160 --> 00:06:35.520]   to the city of the city and say it's okay to beat you up. Right, right. Pre-shocking stuff. Wow.
[00:06:35.520 --> 00:06:39.520]   Well, we've fortunately come a long way. Maybe it's time to relect Brianna Wu in the
[00:06:39.520 --> 00:06:46.320]   Massachusetts. I'm just saying. I'm just saying. You okay? Yep. We actually, we were talking before
[00:06:46.320 --> 00:06:52.800]   the show because we... Dan's kind of jokingly said, "I don't don't, you know, I don't want to
[00:06:52.800 --> 00:06:58.480]   state anything about my politics," but you would do in fact, I think all three of you are not hiding
[00:06:58.480 --> 00:07:05.040]   your political slant one way or the other, but the New York Times published what has become a
[00:07:05.040 --> 00:07:10.720]   somewhat controversial letter. This is from Dean Baskett, their executive. Actually, I don't know
[00:07:10.720 --> 00:07:16.240]   if they published it. Well, they did. It's in the New York Times. No, they did. New social media
[00:07:16.240 --> 00:07:19.920]   guidelines. We believe that to remain the world's best news organization, we have to maintain a
[00:07:19.920 --> 00:07:25.440]   vibrant presence on social media, but we also need to make sure we're engaging responsibly in
[00:07:25.440 --> 00:07:29.760]   social media and along with the values of our newsroom. So they've updated their guidelines.
[00:07:29.760 --> 00:07:37.680]   Is this something every media outlet should adopt? For instance, our journalists must not
[00:07:37.680 --> 00:07:41.760]   express partisan opinions, promote political views, endorse candidates, make offensive comments,
[00:07:41.760 --> 00:07:45.760]   or do anything that undercuts the Giants Times journalistic reputation. I guess if you're
[00:07:45.760 --> 00:07:50.480]   tweeting as a Times reporter, that's not unreasonable. That's why they have columnist,
[00:07:50.480 --> 00:07:54.560]   though, and that's why they keep their columnist separate from their journalists. The columnist
[00:07:54.560 --> 00:07:59.920]   can express all kinds of opinions, and that's their job. And I think that line gets blurred a
[00:07:59.920 --> 00:08:05.360]   lot in people's minds because a lot of digital writers are columnists or they express opinions,
[00:08:05.360 --> 00:08:10.240]   and then they also do reporting. They're kind of like in this hybrid rule, but at old school places,
[00:08:10.240 --> 00:08:15.040]   like the Times, the line is very clear. In new school places like ESPN, but we'll get to that in a
[00:08:15.040 --> 00:08:21.760]   second. We consider this is the one that I think some people bridle that. We consider all social
[00:08:21.760 --> 00:08:25.680]   media activity by our journalists to come under this policy. You may think your Facebook page,
[00:08:25.680 --> 00:08:30.720]   Twitter feed Instagram Snapchat, or other social media accounts or private zones and separate from
[00:08:30.720 --> 00:08:39.680]   your old times, but everything we post or like is to some degree public. What do you think, Dan?
[00:08:39.680 --> 00:08:48.080]   Is that an appropriate concern? Yeah, whether, you know, whether BK needs to write this and
[00:08:48.080 --> 00:08:53.600]   articulate it or not, although maybe it is important for the reasons Aaron just mentioned that often
[00:08:53.600 --> 00:08:59.120]   particularly in the stages of social media, it's easy to conflate opinion with fact, and a place
[00:08:59.120 --> 00:09:04.880]   like the Times is often under attack for being liberal or leaning left. And for him to say,
[00:09:04.880 --> 00:09:12.000]   these are what our policies are. Maybe that helps create a little more transparency in their operations.
[00:09:12.880 --> 00:09:18.080]   I would say just as a journalist, I love CBSI and nobody tells me what to think or what to write,
[00:09:18.080 --> 00:09:24.480]   but I don't express opinions like we just talked about in the pre-show. I don't express opinions
[00:09:24.480 --> 00:09:30.000]   publicly because it undermines my work. I can't report on politics. It's like writing about the
[00:09:30.000 --> 00:09:35.200]   iPhone and Android and saying you have a preference. It undermines the faith and confidence readers
[00:09:35.200 --> 00:09:40.560]   have and whether somebody wants to believe in me. It's not about me and the story is not about me.
[00:09:40.560 --> 00:09:45.280]   The story is about the story. And if I make it about my political opinions, it undermines not
[00:09:45.280 --> 00:09:50.560]   just faith in me, but faith and confidence in the story. And we need that now more than ever.
[00:09:50.560 --> 00:09:51.920]   Boy, that is so old school.
[00:09:51.920 --> 00:09:59.440]   One thing I noticed that Mike Isaac, who I think everyone knows is Twitter,
[00:09:59.440 --> 00:10:00.400]   doesn't. Love it.
[00:10:00.400 --> 00:10:06.080]   It's very opinionated and just kind of silly and jokey and extremely active on Twitter.
[00:10:06.080 --> 00:10:11.520]   And he tweeted something that I thought was pretty smart, which is that everyone at the times was
[00:10:11.520 --> 00:10:17.280]   pretty much already acting this way. But now this is just kind of codifying it. And so people at
[00:10:17.280 --> 00:10:23.200]   the times, even the most outrageous tweeters aren't upset about it. They kind of know what Dan just
[00:10:23.200 --> 00:10:28.400]   articulated, which is that if they want to be taken seriously, they have to be careful about what
[00:10:28.400 --> 00:10:29.200]   they put out there.
[00:10:29.200 --> 00:10:37.440]   It's not at all inappropriate, but what's interesting is how important social media has become
[00:10:37.440 --> 00:10:44.240]   and how difficult this is because presumably times journalists have Facebook accounts for the same
[00:10:44.240 --> 00:10:49.600]   reason other people do to keep track of family and friends. And if you have a, you might assume
[00:10:49.600 --> 00:10:56.560]   your Snapchat posts are not part of your public persona, but that's how Dean wants to treat.
[00:10:56.560 --> 00:11:00.480]   I guess it makes sense. Mike, Mike, I don't know if this is a response to this or not, but
[00:11:00.480 --> 00:11:05.360]   he did tweet on Friday. I am going to tweet some great stuff tonight.
[00:11:05.360 --> 00:11:08.960]   Here's, can you say this? You got to see how he's spelled tweet.
[00:11:08.960 --> 00:11:14.080]   That's great. That's great. That's the most Mike Isaac tweet of all.
[00:11:14.080 --> 00:11:20.080]   Yeah, something I found a lot of people don't know this about me before I went into engineering.
[00:11:20.080 --> 00:11:25.600]   I actually majored in journalism. I worked as an investigative journalism journalist for many years.
[00:11:25.600 --> 00:11:31.360]   I think it's a great background for a politician. Something I found when I was trying to write
[00:11:31.360 --> 00:11:36.320]   stories, I'll give you an example. I was writing a story in Mississippi about a landlord that
[00:11:36.320 --> 00:11:41.280]   didn't want to pay to clean up a toxic waste spill in the middle of their apartment complex,
[00:11:41.280 --> 00:11:47.200]   and you had literal residents. They're sitting there like all camped around Green Goo,
[00:11:47.200 --> 00:11:50.560]   and like they've got hives on their bodies. Oh, Lord.
[00:11:50.560 --> 00:11:56.080]   I remember trying to write this story, and my editor going, "Well, you're not being objective
[00:11:56.080 --> 00:12:03.040]   to the landlord. You're not being objective." For me personally, I think this is why I make a
[00:12:03.040 --> 00:12:07.840]   better politician than a journalist, because I do have a point of view, and I can't hide that.
[00:12:07.840 --> 00:12:15.360]   I like that the New York Times is holding their reporters to the highest standard possible.
[00:12:16.000 --> 00:12:23.360]   I just privately, I'm not convinced that journalistic objectivity really, really exists.
[00:12:23.360 --> 00:12:30.000]   We called it the view from nowhere in school, pretending there is no objective truth here,
[00:12:30.000 --> 00:12:36.160]   and you don't have any perspective as your gathering facts. I'm just not convinced it's really there
[00:12:36.160 --> 00:12:41.280]   privately. But I don't think that that is the pretense here. I think that that's a pretense
[00:12:41.280 --> 00:12:46.400]   that's foisted on a lot of reporters. I think the pretense is exactly what the K articulated,
[00:12:46.400 --> 00:12:52.480]   which is I, as a reporter, will approach stories fairly and honestly, and I will do my best in that,
[00:12:52.480 --> 00:12:57.120]   just like anybody who goes to their job. Your job is to do the best job you possibly can.
[00:12:57.120 --> 00:13:01.600]   So I don't think it's that everything is going to be black and white objective. I think it is that
[00:13:01.600 --> 00:13:05.840]   I will do my best to be as fair as possible to the story and the subjects in the story.
[00:13:06.800 --> 00:13:12.720]   I think one thing people were criticizing was that this is going to force reporters to do this kind
[00:13:12.720 --> 00:13:19.760]   of false equivalency thing that is problematic. A lot of news stories tweet many varieties of
[00:13:19.760 --> 00:13:24.240]   and also the other thing is, I don't know if you follow many people from the Wall Street Journal,
[00:13:24.240 --> 00:13:29.920]   but they have the strictest social media policy of any place that I've heard where they can't
[00:13:29.920 --> 00:13:35.200]   even tweet stories that aren't from the Wall Street Journal. And as a result, their Twitter feeds
[00:13:35.200 --> 00:13:40.080]   are, at least, I'm not totally sure on this, but I'm pretty sure that's still the policy. And a lot
[00:13:40.080 --> 00:13:46.400]   of their feeds are pretty boring because they don't really get to comment on stuff that isn't
[00:13:46.400 --> 00:13:52.880]   that isn't in the journal. Yeah. It's a Twitter is such a challenge for everybody.
[00:13:52.880 --> 00:13:58.720]   In so many ways, I know, Brianna, you've been attacked viciously on Twitter.
[00:14:00.720 --> 00:14:08.640]   During gamergate, our president uses Twitter in all sorts of interesting and unusual ways.
[00:14:08.640 --> 00:14:18.640]   Rose McGowan, of course, this is the most recent kerfuffle on Twitter, immediately eclipsing the
[00:14:18.640 --> 00:14:26.880]   times was that Rose McGowan, the actress Rose McGowan, who was doing a tweet storm about men in
[00:14:26.880 --> 00:14:32.960]   Hollywood got kicked off Twitter. Her whole account was banned because, according to Twitter, she
[00:14:32.960 --> 00:14:37.600]   doxed somebody. She put out a private phone number on a tweet, but they don't have the capability of
[00:14:37.600 --> 00:14:43.280]   blocking a tweet or deleting a tweet. So they had the leader account. And then as a result,
[00:14:43.280 --> 00:14:50.320]   on Friday, there was a boycott women are supposed to boycott Twitter. And then
[00:14:50.320 --> 00:14:57.360]   there's it Twitter just seems to be a natural place for upset outrage. It's difficult.
[00:14:57.360 --> 00:15:02.160]   And now Jack Dorsey says we're going to have new rules on late Friday. He said we're going to have
[00:15:02.160 --> 00:15:08.400]   new rules on how many times this headline could run every six months new rules on hate harassment
[00:15:08.400 --> 00:15:14.640]   in response to the boycott, which at least he responded to. He says we're going to have a
[00:15:14.640 --> 00:15:21.040]   crackdown on unwanted sexual advances, hate symbols, non-consexual nudity, violent groups,
[00:15:21.040 --> 00:15:26.880]   tweets that glorify violence, starting in just a few weeks, which means I guess all of that stuff
[00:15:26.880 --> 00:15:32.960]   is currently unblocked and on Twitter. So let me ask Brianna, because Brianna,
[00:15:32.960 --> 00:15:38.080]   I'm sure you have a strong opinion about how Twitter is often misused to attack people.
[00:15:38.080 --> 00:15:41.120]   Is it what is Twitter to do?
[00:15:42.320 --> 00:15:47.680]   Well, I think I'd like to back up for a second here and say, you know, I've never been suspended
[00:15:47.680 --> 00:15:54.000]   from Twitter because I don't find it hard to not threaten people or docs people or use violent
[00:15:54.000 --> 00:16:01.760]   language. And you know, I am very disappointed that a lot of people on the left were kind of boxed
[00:16:01.760 --> 00:16:07.280]   into defending, you know, this private doxing. I mean, it really put me in what felt like an
[00:16:07.280 --> 00:16:08.480]   unfair situation.
[00:16:09.600 --> 00:16:12.320]   Did you boycott on, I don't think you boycott it on Friday.
[00:16:12.320 --> 00:16:17.680]   I thought about it at first, I said, no, and I had some people come to me privately and say,
[00:16:17.680 --> 00:16:22.240]   please boycott. And I said, okay, fine, I will do it. I can use 24 hours off of Twitter.
[00:16:22.240 --> 00:16:24.880]   It's actually a great feeling. I use Twitter.
[00:16:24.880 --> 00:16:28.720]   Yeah. Oh, it was great. I got a lot of stuff done professionally that day.
[00:16:28.720 --> 00:16:32.560]   I have said this a thousand times, Leo, and I want to say it again.
[00:16:32.560 --> 00:16:38.800]   Twitter has never gotten the credit it deserves for what they've done on their platform. They have
[00:16:38.800 --> 00:16:44.320]   largely gotten rid of their harassment bot problem. It's recently as a year ago,
[00:16:44.320 --> 00:16:48.480]   there were harassment bots that were targeting me and other journalists. They would just tweet
[00:16:48.480 --> 00:16:54.400]   the same doxing thing a hundred a thousand times that it would tweet it every when you interacted
[00:16:54.400 --> 00:17:01.120]   with. They finally got that problem solved. I do see the number of the responsiveness they
[00:17:01.120 --> 00:17:07.360]   have to death threats. I do see that going down. But I think overall, the trajectory of the company
[00:17:07.360 --> 00:17:12.560]   is addressing this. Look at the feature where at the bottom of a tweet, it will say,
[00:17:12.560 --> 00:17:17.840]   show additional replies, some of which may be offensive. This is a really good feature
[00:17:17.840 --> 00:17:23.840]   because it blocks most of the garbage. And I have never seen Twitter get the credit it deserves for
[00:17:23.840 --> 00:17:29.760]   that. There's a lot more work to go. But I really wish we could change the conversation a bit more
[00:17:29.760 --> 00:17:34.880]   to Facebook or Reddit, both of which docs people much more freely than Twitter does.
[00:17:36.480 --> 00:17:41.760]   Here's Jack's tweet storm. We see voices being silenced on Twitter every day. We've been working
[00:17:41.760 --> 00:17:47.440]   to counteract this for the past two years. It sounds like you agree, Brianna. Others don't. But
[00:17:47.440 --> 00:17:52.880]   we prioritize this in 2016. We update our policy has increased the size of our teams. It wasn't enough.
[00:17:52.880 --> 00:17:57.360]   Others might agree. Oh, okay. You're right. You've been doing it, but it hasn't been enough.
[00:17:57.360 --> 00:18:02.800]   In 2017, we made it our top priority and made it a lot of progress today. We saw voices silencing
[00:18:02.800 --> 00:18:07.520]   themselves and voices speaking out because we're still not doing enough. We've been working
[00:18:07.520 --> 00:18:12.240]   intensely over the past few months of focus today on making some critical decisions where
[00:18:12.240 --> 00:18:16.320]   you've decided to take a more aggressive stance on our rules and how we enforce them.
[00:18:16.320 --> 00:18:19.520]   These changes will start rolling out in the next few weeks more to share next week.
[00:18:19.520 --> 00:18:23.200]   That seems like actually the best response I've heard from Twitter in some time, at least public
[00:18:23.200 --> 00:18:28.560]   response. I mean, at least he didn't say, at least he didn't say, we have to do better.
[00:18:29.200 --> 00:18:34.800]   That's the stock phrase. He's gotten tired. Yeah. Biz used to say that all the time.
[00:18:34.800 --> 00:18:39.840]   Ev famously apologized. How long ago was that? I mean, that's exactly what he said.
[00:18:39.840 --> 00:18:44.000]   We've got to do better. But it's also a challenge. I was sympathetic because
[00:18:44.000 --> 00:18:50.800]   as Alex Stamos, the CSO of Facebook said, "You guys, this is hard stuff.
[00:18:50.800 --> 00:18:57.440]   If you want to have a free platform, people could freely express themselves.
[00:18:58.080 --> 00:19:02.240]   Somebody I read an article that said it was the Arab Spring that really spoiled Twitter because
[00:19:02.240 --> 00:19:07.520]   that's when they got in their minds. Look, we can be used to change the world if we let
[00:19:07.520 --> 00:19:12.080]   people have free speech on Twitter. There's a real social value to that.
[00:19:12.080 --> 00:19:19.120]   They were so naive that they never considered that there's also people out there who are going
[00:19:19.120 --> 00:19:25.520]   to exploit that in a negative way. It's just sort of that Silicon Valley magical thinking
[00:19:26.640 --> 00:19:32.400]   where they never even occurred to them. They just only see good coming out of their creation.
[00:19:32.400 --> 00:19:38.560]   It's very, very convenient that all of the social companies, including Google,
[00:19:38.560 --> 00:19:46.160]   figured out Russian tampering and bots after the 2016 election, not while it was going on.
[00:19:46.160 --> 00:19:50.560]   In fact, we reached out to Twitter about some of these harassment bots and about the growth of
[00:19:50.560 --> 00:19:56.560]   their bots. They said, "We're a public company. We have an incentive to kill all bots
[00:19:56.560 --> 00:20:01.360]   on the platform." In fact, at Tech Republic, we covered the rise of bots and just we didn't do
[00:20:01.360 --> 00:20:08.800]   any magic. We just covered each presidential candidate and downloaded the Twitter API and
[00:20:08.800 --> 00:20:14.080]   put it in Excel. You can see an inflation of bots through the campaign. So if
[00:20:14.080 --> 00:20:19.600]   Twitter wants us to believe this nonsense now, yeah, you guys can be sorry, but you had a huge
[00:20:19.600 --> 00:20:23.920]   profit incentive not to be sorry when it mattered. Would you include Google and Facebook in that?
[00:20:24.480 --> 00:20:32.480]   Yes, absolutely. I mean, I believe that these companies say they have a moral core, but these
[00:20:32.480 --> 00:20:37.840]   are for-profit public companies. I don't believe that they're inherently evil, but I do believe that
[00:20:37.840 --> 00:20:43.440]   they expect us that we're either stupid or naive that we didn't see this happening a long time ago
[00:20:43.440 --> 00:20:48.720]   or that they didn't see this happening a long time ago. Well, they saw it and they just didn't care.
[00:20:48.720 --> 00:20:51.920]   Yes, exactly. Well, they didn't care because they had strict,
[00:20:51.920 --> 00:20:56.000]   they had strong financial incentives not to care. Yes, precisely.
[00:20:56.000 --> 00:21:01.920]   Something we did at My Studios, I spent a lot of time picking apart these bots and doing a lot of
[00:21:01.920 --> 00:21:06.640]   independent research on Twitter and harassment bots. Please understand what I'm talking about,
[00:21:06.640 --> 00:21:13.760]   harassment bots. I am differentiating a bot that is written to target and dock somebody like me
[00:21:13.760 --> 00:21:19.520]   versus the political propaganda bot. That's a different problem. I can say from my
[00:21:20.320 --> 00:21:25.680]   looking at the data, which could be incomplete, but this is just a partial look at it.
[00:21:25.680 --> 00:21:32.400]   What I see is you say a stat like 9/10s of all political opinions came from bots and that
[00:21:32.400 --> 00:21:38.800]   sounds really scary, but then you start looking at the reach of the bots and who's following
[00:21:38.800 --> 00:21:45.600]   these bots, and it's not as great. There's a really big difference if I tweet something
[00:21:46.160 --> 00:21:52.080]   with 70,000 followers versus somebody tweets it with 10 followers who's also a robot.
[00:21:52.080 --> 00:21:59.360]   I'm not saying one way or the other. I'm saying I'm unconvinced about what the impact is of that,
[00:21:59.360 --> 00:22:04.720]   and I'd like to look into it deeper. One of the tactical things that the bots intended to do
[00:22:04.720 --> 00:22:10.240]   was knowing precisely that. Of course, on Facebook and Google, the echo is significantly larger.
[00:22:10.240 --> 00:22:15.600]   On Twitter, of course, right, who's going to follow the account with three tweets and no profile
[00:22:15.600 --> 00:22:20.640]   picture, but what they tried to do tactically was raise trends in that left-hand column.
[00:22:20.640 --> 00:22:26.240]   You can see that repeatedly. I'm not saying something that's controversial,
[00:22:26.240 --> 00:22:33.440]   but I'll let you guys yell at me and talk me out of it. Of course, I think you can acknowledge that
[00:22:33.440 --> 00:22:41.600]   the Russians directly or indirectly at the instruction of Vladimir Putin
[00:22:42.560 --> 00:22:48.480]   decided to not necessarily throw the election, perhaps throw the election, perhaps just so seeds
[00:22:48.480 --> 00:22:54.640]   of discord, but decided to really kind of royal the waters in the United States. They did that
[00:22:54.640 --> 00:22:59.440]   using social media. It's particularly easy to do that with Twitter, Facebook,
[00:22:59.440 --> 00:23:02.960]   and buying ads on Google since it's an automated process.
[00:23:02.960 --> 00:23:09.280]   I think we could stipulate that that happened. We know that that happened.
[00:23:09.840 --> 00:23:13.920]   What I'm not sure I'm willing to stipulate is its effect. It's how strong an effect it had.
[00:23:13.920 --> 00:23:21.120]   I'm not convinced that, for instance, that got Donald Trump elected. I think there were many
[00:23:21.120 --> 00:23:26.160]   factors that got him elected, and he legitimizes some of the legitimate reasons he got elected.
[00:23:26.160 --> 00:23:32.400]   It's a fun thing for people who aren't happy with the results of the election.
[00:23:32.400 --> 00:23:38.000]   By the way, I include myself in that group, but I'm going to try to be objective here.
[00:23:38.560 --> 00:23:41.600]   It's a fun thing for people who aren't happy with the results of the election to say, "Oh, well,
[00:23:41.600 --> 00:23:46.800]   it's all Russia's fault, but I'm not convinced that that's the case."
[00:23:46.800 --> 00:23:51.600]   I think it legitimizes what may have been real reasons for that to have happened.
[00:23:51.600 --> 00:24:00.560]   Talk me out of that. Tell me why. I understand it's illegal, and it's something that even the
[00:24:00.560 --> 00:24:06.080]   founding fathers were concerned about for a foreign nation to interfere with our elections.
[00:24:06.080 --> 00:24:12.880]   That's a fact. I do say this. Go ahead, Brianna, and then, Eric.
[00:24:12.880 --> 00:24:19.200]   Sorry, I didn't mean to. What would I say is somebody that's actually making ads and putting
[00:24:19.200 --> 00:24:25.360]   them out there? If you saw the list of FEC requirements I have to follow, it is bananas.
[00:24:25.360 --> 00:24:30.880]   It is down to the font you put in the bottom of the, "I'm Brianna Wu and I approve this ad."
[00:24:31.360 --> 00:24:38.000]   I do think that we need to have a serious talk about, if I put a television ad on that is regulated
[00:24:38.000 --> 00:24:44.080]   in so many different ways. Wire ads on Twitter and Facebook and Google not regulated in the same way.
[00:24:44.080 --> 00:24:50.080]   I think they should be. Facebook lobby knowing it hard for years not to be regulated and convince
[00:24:50.080 --> 00:24:55.440]   the Federal Elections Commission, the FEC, not to give them the same requirements that broadcast
[00:24:55.440 --> 00:25:03.600]   media have. That's the part of this. That's the part of this. That's the part of this.
[00:25:03.600 --> 00:25:10.640]   We're a new, we're a baby industry. Don't we, let us grow. I think it's also important to
[00:25:10.640 --> 00:25:17.360]   differentiate that, yes, we need to make sure that we don't marginalize or undermine the other
[00:25:17.360 --> 00:25:24.000]   causes of any particular election or the previous one. But that we do say, we're having a conversation
[00:25:24.000 --> 00:25:28.320]   about the role technology played and will continue to play. This is a significant one. It's really
[00:25:28.320 --> 00:25:33.680]   important to make sure we understand the mechanics of this so that we don't overshoot it with over
[00:25:33.680 --> 00:25:40.960]   regulation or undershoot it and not take it seriously. Is it solvable? Can we get another? Is social
[00:25:40.960 --> 00:25:46.560]   media solvable? Well, that's my bet. Exactly. 2020, we've got another presidential election.
[00:25:46.560 --> 00:25:50.080]   By 2020, that's a pretty long time frame. That's three years. Next year.
[00:25:50.080 --> 00:25:54.000]   And this does not. Well, next year is 2018. And Breanna's going to be in that election.
[00:25:54.000 --> 00:26:01.040]   But let's give you three years. Is it possible to say there will be no Russian interference?
[00:26:01.040 --> 00:26:08.480]   Oh, no. Well, the ad problem is easy to solve with disclosures and more transparency,
[00:26:08.480 --> 00:26:14.000]   no more dark ads. And like you said, that maybe didn't have the same size of impact as the fake
[00:26:14.000 --> 00:26:19.680]   news problem, which is much, I think, harder to solve and a lot bigger and probably had a
[00:26:19.680 --> 00:26:25.200]   outsized effect on the election. My feeling, again, talk me out of it,
[00:26:25.200 --> 00:26:30.160]   is that fake news only worked with people who had already made up their minds. People
[00:26:30.160 --> 00:26:34.880]   chose the news they wanted to believe. You can say, well, that's factually untrue. The
[00:26:34.880 --> 00:26:40.320]   pope did not endorse Donald Trump. That isn't really germane. I don't think that convinced anybody.
[00:26:40.320 --> 00:26:45.440]   It only reinforced what people already believe. Well, which could stop you from changing your
[00:26:45.440 --> 00:26:51.680]   mind when something like the Access Hollywood tape comes out. Okay. Okay. And it's pretty easy
[00:26:51.680 --> 00:26:57.520]   to correlate this. You can go to L2 political. They scrape. I've never seen a company that has
[00:26:57.520 --> 00:27:04.000]   better data than L2 does. And you can see, I don't think that 20, 16 election is in their database.
[00:27:04.000 --> 00:27:10.000]   Yeah, but you can see correlate where there were news items in Facebook and particular counties
[00:27:10.000 --> 00:27:15.680]   and how they voted and their demographics. And so it's not like it's not impossible to answer this
[00:27:15.680 --> 00:27:20.800]   question. It's also the case that due to the Electoral College and the way our president's
[00:27:20.800 --> 00:27:26.960]   selections work, you didn't have to sway many votes. You nearly had a way 77,000 votes in key
[00:27:26.960 --> 00:27:32.400]   discounts. And I agree that there are lots of questions about how that was done, how they knew
[00:27:32.400 --> 00:27:39.280]   which districts to sway. There's a lot, there's a lot more about this. But I also, I also think you
[00:27:39.280 --> 00:27:45.040]   could make the case that, I mean, it was very complex. There were many reasons why the election
[00:27:45.040 --> 00:27:49.600]   happened the way it did. And I hate for a number of reasons for Twitter, Facebook and Google to
[00:27:49.600 --> 00:27:54.720]   become the scapegoats for that. >> Here's my cynical response to that, which is that,
[00:27:54.720 --> 00:28:03.440]   because they are so aggressively arguing that it was very small, it was just a blip compared to
[00:28:03.440 --> 00:28:09.200]   the amount of content that's on Facebook. It makes me not believe them. And it makes me want to
[00:28:09.200 --> 00:28:14.640]   look into the impact a lot more. Just because that is their response makes me think,
[00:28:14.640 --> 00:28:17.840]   there's more to it. >> You sure sound guilty.
[00:28:17.840 --> 00:28:24.080]   That's what you're saying, right? >> I could say, if I could have one more thing
[00:28:24.080 --> 00:28:29.840]   onto this, something we did is we went through all my mentions that happened during the Democratic
[00:28:29.840 --> 00:28:34.640]   primary. And I don't think this is talking about it, talked about enough. So we're seeing a really
[00:28:34.640 --> 00:28:40.240]   big division, the Democratic Party between the Bernie wing and the Hillary wing. We went back
[00:28:40.240 --> 00:28:45.600]   through that and looked at this phenomena of Bernie Bros and where it came from. And when we
[00:28:45.600 --> 00:28:53.040]   really studied in my mentions, the mentions that were the most hostile or had personal attacks or
[00:28:53.040 --> 00:28:59.680]   even doxery, a lot of those were coming from bots. And I personally believe there were
[00:28:59.680 --> 00:29:06.080]   coming from Russian bots. So I think that this is a bigger problem than voting for one candidate.
[00:29:06.080 --> 00:29:10.720]   I think that Russia is very artificially trying to keep us at each other's threats.
[00:29:10.720 --> 00:29:16.320]   >> Yes. >> I mean, it's so easy to get us so angry at each other and attacking each other.
[00:29:16.320 --> 00:29:20.720]   They win when we're divided. It's not even a Republican versus Democrat thing. It's about
[00:29:20.720 --> 00:29:26.720]   keeping America angry at each other. >> And you were in Ukraine, you kind of saw firsthand,
[00:29:26.720 --> 00:29:34.000]   Ukraine is the test tube for Russian interference. And with hacking and all sorts of tricks.
[00:29:34.000 --> 00:29:42.160]   I think one case, strong case to be made is that really Russia wins if Russia's seen as a player.
[00:29:42.160 --> 00:29:46.000]   >> Yes. >> That some of this is merely, hey, we're important.
[00:29:46.000 --> 00:29:52.400]   >> That's the long game. And the long game is to undermine self-confidence. And just like
[00:29:52.400 --> 00:29:58.080]   Brana said, to keep America and their adversaries. It's not just America, it's France, it's Germany
[00:29:58.080 --> 00:30:03.760]   and others, but to keep them divided. Look, you can look at, I love the Russian people,
[00:30:03.760 --> 00:30:09.040]   but you can look at Soviet military and propaganda tactics going back a century.
[00:30:09.040 --> 00:30:16.000]   And one of the ways they messed with us at the Global Cyber Security Summit was sending thousands
[00:30:16.000 --> 00:30:22.240]   of attempts to brute force your accounts. And then they would come into our rooms,
[00:30:22.240 --> 00:30:28.720]   they would send dudes to sit next to us. What Russia wants to do is to undermine your own
[00:30:28.720 --> 00:30:34.880]   confidence and to make sure that a divided world is a world that Russia can conquer.
[00:30:34.880 --> 00:30:38.400]   >> Wait, sorry, could you go back, they were in your rooms?
[00:30:38.400 --> 00:30:42.560]   >> Yeah, yeah, I mean, this is a long story. I don't want to rat a whole year.
[00:30:42.560 --> 00:30:46.320]   We wrote about some of this, we were attacked with, so that we were at the Global Cyber
[00:30:46.320 --> 00:30:48.960]   Security Summit and we were accompanied by both companies.
[00:30:48.960 --> 00:30:50.480]   >> This was in Kiev. >> During this was in the Ukraine.
[00:30:50.480 --> 00:30:56.400]   >> Yes, but we were with the former Chief Counsel of the NSA, we were with, I mean,
[00:30:56.400 --> 00:31:01.440]   Anthony Blinken, who is a former Secretary of State. So the party we were traveling with
[00:31:01.440 --> 00:31:08.240]   were reasonably high profile. We were at the head of the bank, the heads of state.
[00:31:09.200 --> 00:31:14.720]   So the second I got up on stage, every single one of my accounts was hammered.
[00:31:14.720 --> 00:31:19.200]   When I went back to our hotel, one of my colleagues who runs an AI company said, hey,
[00:31:19.200 --> 00:31:24.400]   you know, I went into my hotel room and, you know, room service had done everything.
[00:31:24.400 --> 00:31:28.720]   You know, nobody's drinking water in a glass. And he said, room service had cleaned it up,
[00:31:28.720 --> 00:31:32.560]   but I found a half drank glass of water and a little note that says prid it,
[00:31:32.560 --> 00:31:37.920]   and which is hello in Russian. So it's like a little thing.
[00:31:37.920 --> 00:31:42.000]   Hey, we're watching. >> Yeah, so we got, I go to the front desk,
[00:31:42.000 --> 00:31:46.560]   and of course the conference was several kilometers away, but I didn't tell them that.
[00:31:46.560 --> 00:31:50.560]   I just said, hey, you know, I've been in and out all day. Could you tell me the last time I
[00:31:50.560 --> 00:31:55.040]   forgot something in my room, the last time I was here, they're like, oh, yeah, 1235.
[00:31:55.040 --> 00:31:59.280]   Like, I could have a new room. >> Wow.
[00:31:59.280 --> 00:32:04.880]   >> So a few days later, we interviewed the head of the foreign bank and I come back and I'm writing
[00:32:04.880 --> 00:32:09.840]   my notes and working on a story. And these two guys come out. We're right next to the FSB,
[00:32:09.840 --> 00:32:17.040]   the Russian, the new KGB, they were attached to our hotels. So I'm sending their drinking beer
[00:32:17.040 --> 00:32:23.040]   working in a story. And this guy, this big dude with tattoos comes out and sits about 10 yards from
[00:32:23.040 --> 00:32:28.160]   me. And then this guy in a crisp white shirt and cheesy black aviators comes over and sits
[00:32:28.160 --> 00:32:34.160]   about 10 yards from my right, like satellites. And they're just looking at me. Like, Jesus, you guys,
[00:32:34.160 --> 00:32:37.360]   freak, I- >> You know what's really interesting, a free society
[00:32:37.360 --> 00:32:45.360]   and social media, make it trivial for people to mess with you, whether it's to create bots to
[00:32:45.360 --> 00:32:53.840]   harass you or to bother you in a cafe. And if somebody, whether it's a nation state or just a
[00:32:53.840 --> 00:33:00.800]   troll wants to take advantage of these tools, it's trivially easy. And the only thing stopping them
[00:33:00.800 --> 00:33:06.720]   really is societal norms. You can't, I don't think you can stop that.
[00:33:06.720 --> 00:33:13.200]   How would you propose to stop that? Look what they did with Pokemon Go for crying out loud.
[00:33:13.200 --> 00:33:20.320]   So there's a Russian, among other things, Russians created these YouTube videos, which if you've
[00:33:20.320 --> 00:33:26.160]   seen them are completely non credible. I can't believe that these swayed anybody. They're supposed
[00:33:26.160 --> 00:33:33.200]   to be American, African Americans who are voting for Trump and hate Hillary, but they don't speak
[00:33:33.200 --> 00:33:38.080]   English. They have African accents. They're clearly not, these are videos not made in the US.
[00:33:38.080 --> 00:33:47.760]   One of them was a website called do not shoot dot US. And they created a contest on Pokemon Go.
[00:33:47.760 --> 00:33:52.720]   The people would go out, they would go to alleged places where there were incidents of police
[00:33:52.720 --> 00:34:00.320]   brutality, create Pokemon names corresponding to names of the victims, and then take over places
[00:34:00.320 --> 00:34:04.400]   and winners of the contest would get Amazon gift cards. No, not clear if anybody got any
[00:34:04.400 --> 00:34:11.120]   Amazon gift cards. But the point of this is just to upset people. Now, maybe it's Russians,
[00:34:11.120 --> 00:34:17.520]   maybe it wasn't, but it was just trollish behavior. I think we all know you can't stop trolls.
[00:34:20.160 --> 00:34:24.720]   Not in a free society. Not in a free society. So what are you going to do?
[00:34:24.720 --> 00:34:30.320]   Well, I would definitely agree with that. I think like you go after the big variables here.
[00:34:30.320 --> 00:34:34.320]   Like you assume you're not going to be able to have 100% solutions. So like,
[00:34:34.320 --> 00:34:42.400]   you know, it was mentioned the manipulation of trending topics on Twitter. I think clearly,
[00:34:42.400 --> 00:34:49.520]   they should like look at that and try to curb the the effect that bots have on it. Like look at
[00:34:49.520 --> 00:34:54.640]   high quality users and make those worth like war points, like wait the wait the
[00:34:54.640 --> 00:34:58.560]   hour plugging holes generated. Yeah, plugging your plugging holes in a
[00:34:58.560 --> 00:35:02.080]   diet that's filled with Swiss cheese. You fix the Twitter one, then they're going to use something
[00:35:02.080 --> 00:35:07.680]   else and use some mail. They're going to sit next to you in a cafe. Security is it's not a
[00:35:07.680 --> 00:35:12.880]   destination. It's a process. You're absolutely right. You're not going to be able to finish this,
[00:35:12.880 --> 00:35:18.640]   but you've got to keep working on it. Well, Congress might have a solution. This,
[00:35:18.640 --> 00:35:23.840]   okay, I'm going to try to say this with a straight face. Two members of Congress,
[00:35:23.840 --> 00:35:30.000]   one Republican and one a Democratic, opposed a new bill. They call it the ACDC Act,
[00:35:30.000 --> 00:35:36.880]   which they it's one of those retro nims where they clearly thought up the acronym first,
[00:35:37.840 --> 00:35:45.120]   and then and then decided how what it what it said. Let me see if I can I can find this, but the
[00:35:45.120 --> 00:35:51.680]   premise of the of the bill is to allow businesses that are being hacked to hack them back.
[00:35:51.680 --> 00:35:58.560]   To make it legal for businesses to respond to being hacked. That'll fix it.
[00:35:58.560 --> 00:36:02.400]   Wow. Any thoughts?
[00:36:07.120 --> 00:36:11.360]   I got nothing. It's hard not to laugh. This is I might as well name names. Let's
[00:36:11.360 --> 00:36:18.080]   give credit where credit is due. This is Tom Graves, who's a congressman from Georgia and
[00:36:18.080 --> 00:36:25.680]   Kirsten Sonema, who's from Arizona. The ACDC stands for the Active Cyber Defense
[00:36:25.680 --> 00:36:30.240]   Certainty Act, which is of course nuts. Yeah, but that's but they could.
[00:36:30.240 --> 00:36:34.080]   The word certainty there really throws me. Yeah, what's the certainty?
[00:36:35.360 --> 00:36:39.920]   Graves said, well, it doesn't solve every problem. It brings some light to the dark places where
[00:36:39.920 --> 00:36:45.760]   cyber criminals operate. The certainty the bill provides. Well, here he he explains it.
[00:36:45.760 --> 00:36:49.840]   Well, empower individuals and companies to use new defenses against cyber criminals.
[00:36:49.840 --> 00:36:56.400]   I hope it spurs a new generation of tools and methods to level the lopsided cyber battlefield,
[00:36:56.400 --> 00:37:03.760]   if not given edge to cyber defenders. That sounds like the script to a superhero movie.
[00:37:04.800 --> 00:37:08.960]   Hack them back. What could possibly go wrong?
[00:37:08.960 --> 00:37:17.840]   I would say that there is a place in regulation for Congress to legalize certain kinds of white
[00:37:17.840 --> 00:37:25.840]   hat hacking and look at the nutrition tests. Like, absolutely. Security research. We have a
[00:37:25.840 --> 00:37:30.800]   lot of work we can do there. Nobody should ever be a go-to-jail for doing real legitimate research.
[00:37:30.800 --> 00:37:35.920]   Absolutely. But this isn't it. This is like they saw an episode of Mr. Robot and wrote a bad
[00:37:35.920 --> 00:37:41.360]   baseball net. It's like, well, I presume that it's dead on arrival. But the fact that they
[00:37:41.360 --> 00:37:46.560]   even could introduce this bill blows my mind that they even could think this was a good idea.
[00:37:46.560 --> 00:37:53.120]   Yeah. Oh, let's see what kind of hacking tools, Equifax, what viruses, Equifax can write,
[00:37:53.120 --> 00:37:59.760]   and get those bad guys. Get those bad guys. Wow. Just stunning.
[00:38:00.640 --> 00:38:05.840]   All right. We're going to take a break. And we can all breathe for a moment. Dan Patterson is here
[00:38:05.840 --> 00:38:09.840]   from CBS Interactive. He's a senior writer at Tech Republic. Oh, he's a pleasure. I'd like to hear
[00:38:09.840 --> 00:38:16.240]   more about the Global Cyber Security Summit. That sounds really, really interesting. Also,
[00:38:16.240 --> 00:38:22.080]   with us Brianna Wu, she is a game developer, an entrepreneur, a founder, and oh, by the way,
[00:38:22.080 --> 00:38:29.360]   running for Congress in the Massachusetts 8th District, Brianna Wu 2018.com. She's got all of my
[00:38:29.360 --> 00:38:36.800]   votes. And Erin Griffith is also here. She's now a senior writer at Wired, her brand new gig.
[00:38:36.800 --> 00:38:42.480]   We're thrilled to hear it. And it's great to have you. And she's training for a half marathon,
[00:38:42.480 --> 00:38:49.040]   which is why there's a bicycle behind her. You're just very athletic. That's all.
[00:38:49.040 --> 00:38:55.200]   I showed today, brought to you by Carb on night. One way to keep yourself safe, not by hacking
[00:38:55.200 --> 00:39:00.960]   the other guys, but at least backing up your data. Unless at least ask businesses to do that.
[00:39:00.960 --> 00:39:06.080]   Protect yourself from acts of God, fire, flood, hurricanes. Gosh, knows we see a lot of those
[00:39:06.080 --> 00:39:13.360]   these days from human error. Your employees sometimes are your biggest enemies throwing out
[00:39:13.360 --> 00:39:19.920]   stuff by accident. And from ransomware, the best solution to all of that, having a great cloud
[00:39:19.920 --> 00:39:24.960]   backup. That's what Carbonite does. They're the data protection experts. They've got plans for
[00:39:24.960 --> 00:39:30.240]   home and office for Mac and for PC. They've got high availability. I really want to emphasize that
[00:39:30.240 --> 00:39:36.400]   they have plans now for every kind of business, the E2 Evault backup system, which gives you a
[00:39:36.400 --> 00:39:42.000]   hard drive on premises that then backs up to the cloud. So you've got the best of both worlds.
[00:39:42.000 --> 00:39:45.840]   They've got high availability solutions. They can get your business back up and running.
[00:39:45.840 --> 00:39:52.080]   And if you don't think your business is at risk, believe me, that data on the servers in your
[00:39:52.080 --> 00:39:57.040]   computer and your system, that's everything. Without that, your building could burn down,
[00:39:57.040 --> 00:40:02.080]   but lose the data. Now you're really in trouble. You've got to go to Carbonite. Find out more
[00:40:02.080 --> 00:40:07.520]   Carbonite.com for small, medium, and large businesses and homes as well. Start your free trial today.
[00:40:07.520 --> 00:40:14.400]   Just go to Carbonite.com. You can see that 30-day free trial. Put a tweet in there as the offer
[00:40:14.400 --> 00:40:19.600]   code. You'll get two bonus months if you decide to buy. These guys are the good guys in the war
[00:40:19.600 --> 00:40:24.480]   against data loss. They are the data protection experts. I've used them and known them for years,
[00:40:24.480 --> 00:40:28.720]   and I couldn't recommend them more highly. Carbonite.com, don't forget the offer code
[00:40:28.720 --> 00:40:31.440]   "Twit" for two months. Free. Who's got a dog?
[00:40:31.440 --> 00:40:37.280]   No, I don't, but that's my neighbor. Let me close the window.
[00:40:37.280 --> 00:40:43.360]   No, no, no, no, no. I like dogs. No, no, that's fine. That's fine. It's funny. We had a
[00:40:44.880 --> 00:40:50.000]   one and a half year old in here. I noticed she's left. She was reading a book though because she
[00:40:50.000 --> 00:40:53.360]   was bored. You know, one and a half year old sent to be bored by this show. The book is called
[00:40:53.360 --> 00:41:01.280]   Darth Vader and Friends. What is wrong with children today? I'm sorry.
[00:41:01.280 --> 00:41:05.840]   But isn't the idea that I haven't read the book, but isn't the idea that if Darth Vader had
[00:41:05.840 --> 00:41:09.360]   had friends, maybe he would have turned out differently? Oh, is that the idea of the book?
[00:41:09.360 --> 00:41:13.360]   That's just my guess. Let me steal it from Rose and find out here. Wait, I see.
[00:41:13.920 --> 00:41:20.960]   Yeah, Darth Vader and Friends. It's got a cute picture of Darth dancing using E3PO and R2D2.
[00:41:20.960 --> 00:41:27.440]   I like this. No spoilers. I am off. No, no, no, that's right.
[00:41:27.440 --> 00:41:33.280]   Oh, I like how it starts a long time ago in a galaxy far, far away. I can almost hear the music.
[00:41:33.280 --> 00:41:38.960]   With, while ruling the Galactic Empire, Lord Darth Vader aims to crush the Rebel Alliance
[00:41:38.960 --> 00:41:45.760]   with a little help from his friends. Wait a minute, his twin children Luke and Leia. I don't, what?
[00:41:45.760 --> 00:41:49.920]   What? Have other plans and powerful friends of their own?
[00:41:49.920 --> 00:41:55.760]   This facility is crude, but it should be adequate to freeze more ice cream.
[00:41:55.760 --> 00:42:02.720]   Leia says, "Cross." What is that? Is that the pork? We already got those in here? This is very
[00:42:02.720 --> 00:42:08.720]   ahead of this time. Maybe we saw, oh, if Jar Jar is in it, I don't want to read it. We used to be
[00:42:08.720 --> 00:42:15.840]   in Friends with Queen Amidala, not Jar Jar. Okay, kids today, I'll tell you, and it's a little job
[00:42:15.840 --> 00:42:22.160]   of the hut in the back. Oh, no, but don't make your nose on the deck. Oh, just just got.
[00:42:22.160 --> 00:42:30.320]   Which means nobody understands me like you do. Wow. Darth Vader and Friends,
[00:42:30.320 --> 00:42:35.360]   Jeffrey Brown, hardcover Barnes and Noble, $25. Go ahead, pick it up today. Well, they got a wall
[00:42:35.360 --> 00:42:40.400]   calendar too. Darth Vader and Friends wall calendar, get yours today.
[00:42:40.400 --> 00:42:50.240]   Okay, that was a little, a moose bush, a little, sure bit to loosen the pallet in between courses.
[00:42:50.240 --> 00:42:56.320]   Where do we go? Where do we go next? Oh, anybody get one of those especially configured
[00:42:57.120 --> 00:43:00.480]   Google Home Minis that Google was handing out at their event?
[00:43:00.480 --> 00:43:07.040]   This is obviously an error, but I think it's a funny error. So,
[00:43:07.040 --> 00:43:12.000]   and a little tip to tech companies in general, if you're giving out review units to journalists,
[00:43:12.000 --> 00:43:20.320]   you might want to check the Always Listening feature. Artem Risakovsky writing in Android
[00:43:20.320 --> 00:43:28.640]   Police, apparently he got one of these minis. And smart guy, he checked his Google recordings,
[00:43:28.640 --> 00:43:34.000]   you know, that Google has a dashboard, you could see what recordings. And he noticed a whole bunch
[00:43:34.000 --> 00:43:44.240]   of new recordings from a device called Mushroom. And what, wait a minute, not only a whole bunch
[00:43:44.240 --> 00:43:53.360]   of them, but like everything he said, and he put it in his bathroom, everything he said. And he says
[00:43:53.360 --> 00:43:57.280]   several days passed without me noticing anything wrong. In the meantime, the mini was behaving
[00:43:57.280 --> 00:44:01.760]   very differently for all the other Google homes and echoes in my home. It was waking up thousands
[00:44:01.760 --> 00:44:06.800]   of times a day, recording what it heard, then sending those recordings to Google.
[00:44:10.800 --> 00:44:19.200]   Good news is he caught it. One. Google. Yeah, Google sent, he sent an email to Google saying,
[00:44:19.200 --> 00:44:28.320]   first of all, is the mini called code name mushroom. And look at my activity, because all of a sudden
[00:44:28.320 --> 00:44:32.320]   it looks like it's recording. Please forward this to the Google home team for response.
[00:44:33.680 --> 00:44:43.680]   So 10 minutes later, what we're looking into it by that evening, a Google person came to his house.
[00:44:43.680 --> 00:44:50.000]   This is my favorite part of this story. They were in like 10 fire alarm mode,
[00:44:50.000 --> 00:44:55.760]   van out to Oakland, like we'll be there in a minute. By nine o'clock, he wasn't even home.
[00:44:55.760 --> 00:45:01.760]   They exchanged the Google home. He said they left him with two replacement home minis, although
[00:45:02.480 --> 00:45:06.560]   you might not want to plug those until they get back to you. The next day, they said, yes, it
[00:45:06.560 --> 00:45:11.280]   turns out there was a hardware issue. The touch, because you can touch the top of a home and it
[00:45:11.280 --> 00:45:16.640]   will trigger it. And it was the thing with sensing touches all the time. And so their fix was to
[00:45:16.640 --> 00:45:23.200]   disable the touch. And I presume before everybody else gets their mini, it will be fixed. But the
[00:45:23.200 --> 00:45:27.760]   real question is, did every journalist who got one of those minis at the Google event, and there
[00:45:27.760 --> 00:45:36.960]   were hundreds, were they all being recorded? Thanks to our Tim. Yeah, anymore.
[00:45:36.960 --> 00:45:43.840]   That's a bad, bad story, because it confirms what a lot of people think that the echoes,
[00:45:43.840 --> 00:45:50.240]   they're listening to me. I mean, it's, I trust that Google has good intentions here. And I
[00:45:50.240 --> 00:45:55.840]   think this is just a development bug. I don't, I don't think anything like nefarious is going on
[00:45:55.840 --> 00:46:00.720]   here. But the issue, like, there's another story we're going to talk about later with, you know,
[00:46:00.720 --> 00:46:06.480]   antivirus software being used to help with Russian spying. And the issue here for me,
[00:46:06.480 --> 00:46:10.960]   isn't that I don't trust Google? It's I don't trust the security and the integrity of these
[00:46:10.960 --> 00:46:17.440]   devices, particularly when they open up the API to other people. You know, I have an Android phone,
[00:46:17.440 --> 00:46:23.280]   but I would never put this or the Amazon one in my home and probably not even the Apple one.
[00:46:23.280 --> 00:46:29.520]   It's just, it's a level too far. And I think if you're, you know, a high risk target for this kind of
[00:46:29.520 --> 00:46:37.520]   activity, I just don't think it's safe. Well said, Dan, would you, would you bring a smartphone to
[00:46:37.520 --> 00:46:43.200]   something like the Global Security Summit? I know most people go to Defcon and Black Hat are very
[00:46:43.200 --> 00:46:49.440]   careful about what stuff they bring. Yeah, you know my answer. I think what Breonna just said is
[00:46:49.440 --> 00:46:55.680]   right. It's probably not Amazon or Google, although these are bad stories for IoT consumer stuff, but
[00:46:55.680 --> 00:47:04.560]   it is the security of these devices themselves. And you know, I brought an iOS device to
[00:47:04.560 --> 00:47:10.320]   key expecting some things. My device was fine. Everyone with an Android device is not me being
[00:47:10.320 --> 00:47:15.840]   political, but everyone with an Android device was hacked. No kidding. Simply because there are
[00:47:15.840 --> 00:47:21.600]   more threat vectors and you have a diversity, a larger diversity, the broader spectrum of
[00:47:21.600 --> 00:47:27.440]   operating systems, hardware, you know, it's, it's, and if you don't take best practices,
[00:47:27.440 --> 00:47:34.560]   you're going to get hacked no matter what. Do you guys agree? I hear this a lot from security
[00:47:34.560 --> 00:47:39.200]   people that if you want to be secure, don't use an Android phone, you guys agree with that?
[00:47:39.200 --> 00:47:42.320]   Yeah. You do.
[00:47:43.360 --> 00:47:48.720]   I do. You know, I'm not trying to be an Apple fan girl here, but there's a reason I stick to
[00:47:48.720 --> 00:47:53.600]   all Apple and it's, you know, it's primarily the threat factor. Just like you said, with so many
[00:47:53.600 --> 00:47:58.480]   different phones and different operating systems. There's just, there's so many ways to get through
[00:47:58.480 --> 00:48:05.040]   an Android device that, you know, Apple, it's not even that, it's not even that I think it's that
[00:48:05.040 --> 00:48:11.200]   much more secure, but it's like when it's that limited the number of ways to attack it,
[00:48:11.200 --> 00:48:15.360]   I just think it's easier for them to plug those holes. So, you know, as somebody that worries a
[00:48:15.360 --> 00:48:21.200]   lot about cybersecurity, it's why he's Apple products. I got a call on the radio shit today from somebody
[00:48:21.200 --> 00:48:27.280]   who said my wife has an iPhone seven and it keeps bugging her to upgrade to iOS 11.
[00:48:27.280 --> 00:48:34.880]   How do I turn that off? And I said, well, I understand why it's bugging you, but one of the
[00:48:34.880 --> 00:48:40.160]   reasons the majority of Apple users are running the most recent operating system is because of this,
[00:48:40.160 --> 00:48:46.480]   Apple has the ability to push the update out and then bug you until you do it. And it's very,
[00:48:46.480 --> 00:48:51.760]   it's actually really good security practice. The problem with Android phones is that they can't
[00:48:51.760 --> 00:48:58.400]   do that. They can't force an update. There's also, you know, stingrays are less common here,
[00:48:58.400 --> 00:49:03.520]   although they will become more common as hard work goes, the hardware costs go down. And, you know,
[00:49:03.520 --> 00:49:08.960]   stingrays, you don't think about this, but it's hard where they can pull encryption keys right off
[00:49:08.960 --> 00:49:17.680]   of your phone. Explain what a stingray is before you. Yeah, so a stingray is a, a, it's a device that
[00:49:17.680 --> 00:49:24.880]   transmits fake signals and you can connect. So a fake T mobile fake AT&T fake Verizon and your phone
[00:49:24.880 --> 00:49:30.000]   is configured to just connect to a network. So especially when you go overseas and you kind of
[00:49:30.000 --> 00:49:35.280]   have different or weird networks you may be getting on, a stingray is designed and they have these
[00:49:35.280 --> 00:49:39.600]   at the White House. They have these at large sporting events, but it's designed because it'll also
[00:49:39.600 --> 00:49:46.800]   fill the air with noise and make it harder to connect. But you can, when you're connected to a
[00:49:46.800 --> 00:49:53.120]   stingray, the amount of data that a hacker, that an attacker has access to is significantly more,
[00:49:53.120 --> 00:49:58.480]   zero day would be the only way they could get more data than connecting using a stingray.
[00:49:58.480 --> 00:50:04.000]   And with iOS, even with iOS, many of your encryption keys are exposed to a stingray through
[00:50:04.880 --> 00:50:09.920]   the cellular network. And with, with Android, there are simply more threat vectors. There are more
[00:50:09.920 --> 00:50:15.920]   ways that an attacker can get into your phone for all the reasons you guys in Leo just enumerated.
[00:50:15.920 --> 00:50:20.640]   There's just more threat vectors and people take security less seriously on those devices.
[00:50:20.640 --> 00:50:24.480]   So a stingray will just slurp that up. Federal agency.
[00:50:24.480 --> 00:50:32.000]   You get your public key. It's basically you join it as a trusted cell site. Yeah, your phone goes,
[00:50:32.000 --> 00:50:37.920]   I trust you. But you're talking about the encryption key. Like I can see it like giving
[00:50:37.920 --> 00:50:41.920]   your public key out there. How can't get your private key in addition to that? Like is
[00:50:41.920 --> 00:50:47.200]   like all the computation is still, yeah, it can man in the middle of an SSL.
[00:50:47.200 --> 00:50:52.640]   Yeah. Yeah. And okay. Often there are stories, I don't know if this is true or not, but there are
[00:50:52.640 --> 00:50:57.920]   stories that stingrays can see your private keys in iOS. I don't know that I buy that, but this is
[00:50:57.920 --> 00:51:02.800]   floating around in dark web forums. But what it can do, it's just like Leo said, a man in the middle.
[00:51:02.800 --> 00:51:07.840]   It's like if you had a wire shark or something pulling packets down,
[00:51:07.840 --> 00:51:14.800]   except you had access to all this, unless it was encrypted in transit, that that data is just open.
[00:51:14.800 --> 00:51:24.240]   Wow. So here, according to the ACLU, are US law enforcement agencies that you stingrays,
[00:51:24.240 --> 00:51:32.960]   including the IRS, FBI, NSA, Special Ops National Guard, US Army, and local law enforcement as well,
[00:51:32.960 --> 00:51:37.760]   in California, for instance, local and state police have cell site simulators or stingrays.
[00:51:37.760 --> 00:51:42.000]   Let's see, Massachusetts local police have cell site
[00:51:42.000 --> 00:51:47.840]   stimulators. You guys are in New York state? Yes, local and state police have cell site
[00:51:47.840 --> 00:51:54.400]   simulators. So just to be aware, now I'm less worried about frankly, about law enforcement,
[00:51:54.400 --> 00:52:01.440]   obviously, and much more worried about hackers. But the whole thing underscores the insecurity
[00:52:01.440 --> 00:52:10.880]   of cell phones. And here's a great post from Philip P. N, who's the co-founder of Shotwell Labs.
[00:52:10.880 --> 00:52:15.360]   He says, want to see something crazy? Open this link on your phone with Wi-Fi turned off.
[00:52:15.360 --> 00:52:22.400]   I'm not going to give it out. But it is essentially all the information
[00:52:22.400 --> 00:52:27.600]   that your carrier knows about you, including your home address phone numbers, cell phone,
[00:52:27.600 --> 00:52:32.960]   contract details, and latitude and longitude describing the current location of your cell phone.
[00:52:32.960 --> 00:52:41.040]   And this is information not only that the IT&T calls it mobile identity API.
[00:52:42.080 --> 00:52:46.160]   This is there is an there is on your cell phone an API
[00:52:46.160 --> 00:52:52.320]   that can be used to look up all of this information.
[00:52:52.320 --> 00:53:03.760]   Telco providers offer this. Cell phones are horrifically insecure. US telcos appear,
[00:53:03.760 --> 00:53:09.200]   according to this article appear to be selling direct non anonymized real time access
[00:53:09.920 --> 00:53:15.680]   to consumer telephone data to third party services, not just federal law enforcement officials,
[00:53:15.680 --> 00:53:20.000]   where they'll then selling access to that data. Hey, it's a good way to make money.
[00:53:20.000 --> 00:53:26.960]   So I it's a terrible, it's a terrible way to make money. If your job is to.
[00:53:26.960 --> 00:53:30.560]   If you're a T&T, it's a good way to make money.
[00:53:30.560 --> 00:53:35.120]   Central service to people. This is an application that that'll be private.
[00:53:35.600 --> 00:53:40.960]   And then of course, we know that the radio baseband software in all cell phones, and I don't know
[00:53:40.960 --> 00:53:46.720]   if Apple has mitigated this or not, but the baseband software is highly hackable.
[00:53:46.720 --> 00:53:54.080]   And we've seen that be abused. Now that takes some sophistication on the part of the attacker,
[00:53:54.080 --> 00:54:01.680]   but I don't think there's any credit any any doubt that attackers are very sophisticated these days.
[00:54:02.240 --> 00:54:06.800]   I don't know. Why did this get to be the bad news show? I apologize.
[00:54:06.800 --> 00:54:11.360]   Yeah, I would I would I would have to say about that. Like this is the first I'm hearing about this.
[00:54:11.360 --> 00:54:16.480]   So I'm talking out through the top of my head, but yeah, I think that if you're talking at law
[00:54:16.480 --> 00:54:21.120]   enforcement using these techniques, they clearly need to go to court and get a search warrant,
[00:54:21.120 --> 00:54:26.560]   right? Like worthless wiretaps. I have no issue go before a judge, probably will cause a while
[00:54:26.560 --> 00:54:31.200]   for it. I will find in Congress, I'll give you everything you need for that. But worthless
[00:54:31.200 --> 00:54:36.960]   wiretaps, like it's very clearly unconstitutional. And both parties have done a lot to a road that
[00:54:36.960 --> 00:54:41.600]   as far as the AT&T thing, you know, something I've looked into and I've talked to a lot of
[00:54:41.600 --> 00:54:48.800]   lawyers about is if we introduce liability into the equation and we say, okay, AT&T like clearly
[00:54:48.800 --> 00:54:53.520]   Congress can't regulate how you should code your software because that's not going to go well.
[00:54:53.520 --> 00:54:58.960]   But if you're reckless in how you're doing it and clearly an API that anyone can access with that
[00:54:58.960 --> 00:55:03.840]   kind of information, that strikes me as reckless. You know, you should encrypt that. You should
[00:55:03.840 --> 00:55:09.760]   have public keys, private keys, whatever you need to do to secure that. I would introduce liability
[00:55:09.760 --> 00:55:15.280]   into the equation. And if they happen to have an Equifax level event, I think they would have to pay
[00:55:15.280 --> 00:55:22.000]   for it. So I think Equifax has set a very high standard now for what it was. But they were
[00:55:22.000 --> 00:55:24.960]   reckless with it. It's reckless. And you and I and everyone on the show was.
[00:55:24.960 --> 00:55:29.120]   I think the link no longer works. So that's that's that's the good news. But Dwight Silverman tweeted
[00:55:29.120 --> 00:55:35.920]   that out. All right, here's a little something to cheer you up. Tim Cook speaking to I think he's
[00:55:35.920 --> 00:55:41.760]   speaking to students at Oxford University question and answer session has a little, I would call this
[00:55:41.760 --> 00:55:46.320]   award drove malfunction. Can you show the animated gift? There's something there's something in his
[00:55:46.320 --> 00:55:52.320]   pie. Oh, it's falling out of his. Oh, oh, oh, oh, that's an iPhone 10, by the way, ladies and gentlemen,
[00:55:52.320 --> 00:55:57.840]   you could tell by the and he hides it immediately. His iPhone 10 just so he's wearing slippery
[00:55:57.840 --> 00:56:04.000]   slacks, but I must compliment him on his choice of socks. I think those are why he's pulling down
[00:56:04.000 --> 00:56:12.000]   his pants after I think he's embarrassed that he people are I don't know. That was again, another
[00:56:12.000 --> 00:56:18.880]   ooze bush. We have to break up the bad news with occasional. Was there a point? I mean, I saw that
[00:56:18.880 --> 00:56:24.640]   being shared everywhere. And I was like, are people just just fun? You don't learn anything about it.
[00:56:24.640 --> 00:56:29.280]   It's just like the phone soap. Well, it is slippery. It's got to be slippery. It's glad.
[00:56:29.280 --> 00:56:34.880]   But I don't think that's really you could put a who who carries a man for man purses.
[00:56:34.880 --> 00:56:40.240]   Well, I think you're exactly right. And who carries around a phone with that? A case
[00:56:40.240 --> 00:56:48.480]   but anyway, right? Doesn't doesn't everybody? I don't really. No, I my phone goes naked. You
[00:56:48.480 --> 00:56:50.960]   go bare back. Why? Why don't you have a case?
[00:56:50.960 --> 00:56:57.840]   Because I think the life of the phone is to get scratched. And it's like, it's like
[00:56:57.840 --> 00:57:04.240]   destiny. It's just destiny. Apple spends all this time every year making it thinner and lighter.
[00:57:04.240 --> 00:57:10.160]   It's like, it's and more beautiful. It's like if you lost 20 pounds and then you celebrate it by
[00:57:10.160 --> 00:57:16.720]   going and putting on a fat suit, I just don't. It just feels so. What you're saying is,
[00:57:16.720 --> 00:57:19.680]   is a case for your phone is like putting a fat suit on it.
[00:57:19.680 --> 00:57:24.480]   Wow. That sounds like someone who has not cracked that many screens.
[00:57:24.480 --> 00:57:28.000]   That's you obviously don't drop your phones exactly right. Yeah.
[00:57:28.000 --> 00:57:34.880]   I have apple care. Oh, so they're hiding the real cost of your negligence.
[00:57:34.880 --> 00:57:40.080]   All right. No, I'm just joking. I'm teasing. So we're going to take a break. We will. I
[00:57:40.080 --> 00:57:43.040]   guess we should get now that we've talked about iPhones. We should probably talk a little bit
[00:57:43.040 --> 00:57:48.400]   about the rumor mill and we're all waiting 12 days and counting until you can order an iPhone 10.
[00:57:48.400 --> 00:57:59.760]   A fat suit for your phone. I'm sorry. Like, no, that's the best description I ever heard.
[00:57:59.760 --> 00:58:07.520]   That's absolutely awesome. I show today brought to you by the hero. I am in love with my hero.
[00:58:07.520 --> 00:58:12.880]   Euro is revolutionizing Wi-Fi. Obviously, Brianna has one because I could tell by her gasp.
[00:58:12.880 --> 00:58:19.440]   I love it. Isn't it awesome? How many of you show a hands out there? I know many of you
[00:58:19.440 --> 00:58:25.840]   have trouble with Wi-Fi, right? It drops. You got to buffering when you're trying to watch a movie.
[00:58:25.840 --> 00:58:33.760]   It's just annoying as heck. Euro solves all this. Euro is more than Wi-Fi. It's modern.
[00:58:33.760 --> 00:58:39.360]   And the new second generation, Euro is tri-band. So it's got a 2.4 gigahertz radio and dual
[00:58:39.360 --> 00:58:45.600]   to 5 gigahertz radios. It's twice as fast. It's the easiest way. Look at that. You plug the new
[00:58:45.600 --> 00:58:50.560]   beacons right into the wall. That's it. No wires. And you're extending your Wi-Fi throughout your
[00:58:50.560 --> 00:58:56.720]   house. Now, it's not like a traditional Wi-Fi extender. A Wi-Fi extender cuts your Wi-Fi speed in half
[00:58:56.720 --> 00:59:00.960]   because it has to communicate with the base and then with you and then with the base. The hero
[00:59:00.960 --> 00:59:07.200]   does it behind the scenes. So you get full speed. Of course, WPA2 encryption. You can create a guest
[00:59:07.200 --> 00:59:12.800]   network. There's so many nice features. I can actually, and I do this all the time, I can say to my
[00:59:12.800 --> 00:59:19.920]   echo, pause Michael's internet and our 14 year old has kicked off the internet until such time as
[00:59:19.920 --> 00:59:25.760]   I dane to unpause it. It's a great punishment. I can look at my era right now, see how everything's
[00:59:25.760 --> 00:59:30.960]   doing, what devices. Actually, this is even cooler. This is my mom's hero I'm looking at. I love the
[00:59:30.960 --> 00:59:35.280]   hero so much when I got the new one. I sent her the old one, set up her hero at home so I can
[00:59:35.280 --> 00:59:40.640]   actually check my mom's network. There's all sorts of nice features with the hero. Let me change my
[00:59:40.640 --> 00:59:44.880]   account so I can show you what I've got going on my account. Switch network. So that's my mom's
[00:59:44.880 --> 00:59:52.640]   network. Here's my home network. My mom had seven connected devices. I have 36 connected devices.
[00:59:53.440 --> 00:59:55.120]   That's why you need an era of folks.
[00:59:55.120 --> 01:00:04.080]   Iro has some really nice features. For instance, I can filter with the family profiles. I can filter.
[01:00:04.080 --> 01:00:10.080]   I've basically assigned every device in the house to me, to Lisa or to Michael, our 14 year old.
[01:00:10.080 --> 01:00:15.040]   I can control with safe filters, the kinds of things Michael can see. I can block adult content,
[01:00:15.040 --> 01:00:20.960]   illegal or criminal violent content. I've got safe search turned on. It of course finds malware,
[01:00:20.960 --> 01:00:26.160]   ransomware, ransomware and blocks it throughout the house. I've scheduled a pause for Michael every
[01:00:26.160 --> 01:00:31.760]   evening at 10 p.m. The internet goes off on his devices because it's time for bed. And I tell
[01:00:31.760 --> 01:00:36.320]   you if you've got a teenager, that is huge. You can pause and unpause the internet on devices.
[01:00:36.320 --> 01:00:44.240]   This hero fundamentally changes your relationship with your Wi-Fi. It's the greatest thing ever.
[01:00:44.240 --> 01:00:48.960]   I want you to check it out. You can build a Wi-Fi system that's more tailored to your home than
[01:00:48.960 --> 01:00:55.680]   ever before. They've been doing this now. In fact, I think this is their anniversary. They've been
[01:00:55.680 --> 01:01:03.040]   doing it since 2016. The new thread radio in the era, I haven't played with this yet. I can't wait.
[01:01:03.040 --> 01:01:07.040]   It lets you connect to low power devices like lock store bells and other sensors
[01:01:07.040 --> 01:01:11.760]   over the thread network. It's so easy. If you need more beacons, I actually have five
[01:01:11.760 --> 01:01:16.000]   euros in my mom's place. Not because she has such a big house, but she has an upstairs downstairs.
[01:01:16.000 --> 01:01:19.680]   And then she has her studio at back and I wanted to have connections everywhere.
[01:01:19.680 --> 01:01:24.320]   Free overnight shipping in the US and Canada when you go to ero.com,
[01:01:24.320 --> 01:01:29.600]   select overnight shipping and then you're twit to make it free. So you can get an
[01:01:29.600 --> 01:01:36.240]   hero tomorrow and your life will change for the better. Don't forget to use the code twit
[01:01:36.240 --> 01:01:43.360]   to get free overnight shipping in the US and Canada. Ero.com. I could go on and on. I could
[01:01:43.360 --> 01:01:48.080]   show you so many more things I can do with it is truly awesome. We're talking about.
[01:01:48.080 --> 01:01:57.040]   Yes, Brianna? No, no, no, I was because I ran tests with mine and I was paying like $119
[01:01:57.040 --> 01:02:02.080]   a month for Verizon Fios, but it wasn't going fast enough throughout my house. So
[01:02:02.080 --> 01:02:08.960]   yeah, the router was the bottleneck. Oh, it was amazing. Like it's literally three times faster
[01:02:08.960 --> 01:02:14.000]   my bedroom than it was. It's twice as fast in the front of the house. It's just a freaking amazing
[01:02:14.000 --> 01:02:20.400]   product. I got a little side business. I wired the neighbor up. People now call me
[01:02:20.400 --> 01:02:24.000]   say my Wi-Fi is bad. Can you help? And I go, yeah, I got the solution. Let me just come over with
[01:02:24.000 --> 01:02:29.360]   some heroes here. I got it. I got it all fixed up. It's kind of amazing. Aaron Griffith is here.
[01:02:29.360 --> 01:02:34.640]   She writes for Wired magazine, senior writer there. Actually, is it Wired magazine or do you just say
[01:02:34.640 --> 01:02:40.080]   Wired? It's just Wired. It's Wired. The magazine is the website. We're a multi-platform. We're on
[01:02:40.080 --> 01:02:46.880]   Snapchat, we're on Facebook. We've got it all. You know what? It's great to have you, Aaron.
[01:02:46.880 --> 01:02:52.880]   Great to have Dan Patterson from CBS Interactive, Brianna Wu, Space Cat Gal from Giant Space Cat.
[01:02:52.880 --> 01:02:59.520]   She's also running for Congress. I have to say, I thought for a while, Instagram was just going to
[01:02:59.520 --> 01:03:05.360]   knock Snapchat out. The stock market apparently thought the same thing. Snapchat, it's not over, is it?
[01:03:05.360 --> 01:03:13.520]   No. I think it's interesting that publishers are using Snapchat as a way to get to a young audience,
[01:03:13.520 --> 01:03:19.040]   right? They have a loyal audience. The problem is that they're possibly being kneecapped in their
[01:03:19.040 --> 01:03:26.720]   growth. By Instagram. Facebook. Yeah, that older audience isn't necessarily going to join now
[01:03:26.720 --> 01:03:34.560]   because they have Instagram. Oversees, Facebook owns an app. They acquired an Israeli company
[01:03:34.560 --> 01:03:40.560]   that helps people manage their data that is apparently hurting Snapchat. I've heard anecdotally
[01:03:40.560 --> 01:03:47.200]   that Snapchat's one of the biggest data hogs around. People, especially overseas who are looking to
[01:03:47.200 --> 01:03:51.200]   decrease their data usage. Snap is actually being hurt by this.
[01:03:51.760 --> 01:03:59.280]   Interesting. Snaps growth then is really in the older population because I just saw a story that
[01:03:59.280 --> 01:04:05.520]   from Piper Jaffrey, they surveyed teens in the US about media habits. They do this in spring and fall.
[01:04:05.520 --> 01:04:14.160]   47% of surveyed teens say Snapchat is their preferred social media. That's up from 39%. In
[01:04:14.160 --> 01:04:19.920]   other words, Snapchat is growing among young people. Still tops for teens.
[01:04:19.920 --> 01:04:27.840]   The one benefit they have is Gen Z coming up as the biggest, as bigger the millennials. So
[01:04:27.840 --> 01:04:36.800]   the problem is, as a public company, it's really hard to satisfy investors and deliver that kind
[01:04:36.800 --> 01:04:40.480]   of consistent growth. Twitter, obviously, has struggled with that their whole
[01:04:41.760 --> 01:04:47.600]   lifetime as a publicly traded company. Snap's going to struggle with it too. The question is,
[01:04:47.600 --> 01:04:51.120]   does Evans Beagle really care? The answer seems to be so far, not really.
[01:04:51.120 --> 01:04:57.840]   My experience in media is you always want the younger audience because the theory is they'll
[01:04:57.840 --> 01:05:03.440]   grow old with you and the older audience is going to die. Are they fickle? That's the problem.
[01:05:03.440 --> 01:05:07.760]   Kids today, I swear. Snap loyalty numbers are hurting as well.
[01:05:08.480 --> 01:05:13.280]   Interesting. What does that mean? They're growing among teens, but their teens aren't as loyal.
[01:05:13.280 --> 01:05:18.160]   Yeah, the last time we did numbers on this might have been six months ago, so it's aged a little
[01:05:18.160 --> 01:05:23.520]   bit, but that they are willing to jump platform. It's more of the features unless they're not as
[01:05:23.520 --> 01:05:28.960]   tied to the brand. Isn't that interesting? There are some businesses. A Facebook might be one where
[01:05:28.960 --> 01:05:34.320]   you get to a point, a critical mass where the network effect is so strong.
[01:05:35.680 --> 01:05:38.880]   Nobody, I don't think anybody could do to Facebook what Facebook did to MySpace.
[01:05:38.880 --> 01:05:44.320]   Right? Yeah, that's probably a fair assessment. The problem really is that
[01:05:44.320 --> 01:05:51.120]   it's almost impossible for someone to build the next Facebook now because Facebook is so
[01:05:51.120 --> 01:05:55.680]   aggressively squashing any startup that gets any kind of traction. Well, that's why they own
[01:05:55.680 --> 01:06:01.520]   Instagram, right? Correct. But even they're going even earlier, they're going even earlier with that
[01:06:01.520 --> 01:06:05.520]   where they, there was a story in I think the Wall Street Journal a month or so ago about
[01:06:05.520 --> 01:06:10.240]   this video, Asickron has video chat company called House Party. It started as Mirkat,
[01:06:10.240 --> 01:06:13.440]   whichever one remembers. They pivoted to this thing called House Party that was getting a lot of
[01:06:13.440 --> 01:06:20.400]   traction. Facebook immediately launched like four different versions of a copycat just to try to
[01:06:20.400 --> 01:06:27.920]   squash them. They'll spend any amount of money. Similar to the aggressiveness of Amazon, I guess,
[01:06:27.920 --> 01:06:35.040]   back in the day on pricing. Facebook has shown that they don't want any other social startup of
[01:06:35.040 --> 01:06:40.240]   any type, any other social product to get to scale. They'll do anything to squash it.
[01:06:40.240 --> 01:06:45.520]   It's reminiscent of tactics or reminiscent of Microsoft in the early and mid 90s.
[01:06:45.520 --> 01:06:50.720]   And Gulf and Devour. Yeah, perhaps that's a better analogy.
[01:06:50.720 --> 01:06:53.520]   Wow. But Amazon and diapers.com.
[01:06:53.520 --> 01:06:56.480]   Right. There you go. Diapers.com. No, I think you're dead on.
[01:06:56.480 --> 01:07:03.200]   Yeah. Jeff Bezos very famously was threatened by diapers.com. So he manipulated the market,
[01:07:03.200 --> 01:07:10.160]   dropped prices on Amazon for diapers, basically practically killed diapers.com, then bought them
[01:07:10.160 --> 01:07:15.680]   at a market basement price. Was there a story where they had a, they had developed an algorithm
[01:07:15.680 --> 01:07:20.080]   that would whenever diapers would lower their price, Amazon's prices would automatically go
[01:07:20.080 --> 01:07:26.480]   to the exact same level? Yeah. Yeah. That seems like that should be illegal. But is it? Maybe
[01:07:26.480 --> 01:07:31.520]   it's not. I mean, if you could say Amazon was a monopoly, then you could perhaps go after them
[01:07:31.520 --> 01:07:36.960]   with any trust. Well, there are a monopoly online. Although did you see there was a story about how
[01:07:36.960 --> 01:07:43.600]   Target and Walmart, which now employs Mark Lohr of jet.com, which has spent his entire career trying
[01:07:43.600 --> 01:07:49.600]   to get back at Amazon for the diapers.com. He started diapers.com, then started jet to try to
[01:07:49.600 --> 01:07:55.040]   get back at him. That was bought by Walmart. Yeah. And now they're teaming up with Target and
[01:07:55.040 --> 01:08:01.280]   creating this anti Amazon alliance to try to somehow get some traction on the internet. It's
[01:08:01.280 --> 01:08:08.640]   fascinating. Not just Target and Walmart, Google, because Google doesn't have the shopping
[01:08:08.640 --> 01:08:15.440]   breadth that Amazon and the Echo have. And Google has a lot of money at stake because if you
[01:08:15.440 --> 01:08:20.160]   start to shop directly through the Amazon app or through your Alexa or through whatever Amazon
[01:08:20.160 --> 01:08:24.960]   invents next and you're not Googling socks or whatever you want. And then clicking on that
[01:08:24.960 --> 01:08:32.000]   sponsored Amazon app. That's a huge chunk of money that Amazon pays for those ads that now
[01:08:32.000 --> 01:08:42.560]   is completely circumvented. It's fascinating. We talked last week about the architect who is
[01:08:42.560 --> 01:08:48.800]   suing Google because they did kind of the same thing he see claims. Engulfed and devoured, they
[01:08:49.600 --> 01:08:56.480]   signed an NDA with him. They acquired his firm or they licensed his technology and then just said,
[01:08:56.480 --> 01:09:02.240]   yeah, thanks anyway and built the same thing themselves. And he got a judge last week to agree
[01:09:02.240 --> 01:09:06.320]   that this lawsuit could now be extended to racketeering, which is of course.
[01:09:06.320 --> 01:09:13.520]   Wow. Yeah. So. Wow. That's actually that's extremely significant because what happens a lot,
[01:09:13.520 --> 01:09:18.400]   especially with companies like Google and Salesforce even, is that they have a venture capital arm
[01:09:18.400 --> 01:09:23.600]   that kind of does the scouting and maybe even invest in some of these companies and then buys a
[01:09:23.600 --> 01:09:28.640]   competitor or builds it themselves. And it's so common. You hear stories about this all the time.
[01:09:28.640 --> 01:09:33.360]   And so for someone to actually be able to get justice out of it is going to be.
[01:09:33.360 --> 01:09:37.120]   We'll see. Definitely going to change. But he's going to be able to see on those grounds that
[01:09:37.120 --> 01:09:42.960]   that essentially the assertion that Google has a pattern of stealing trade secrets from people.
[01:09:42.960 --> 01:09:47.840]   By fighting them to collaborate, which is of course what Microsoft was accused of doing in the 90s.
[01:09:48.720 --> 01:09:49.840]   And apparently a face book.
[01:09:49.840 --> 01:09:54.160]   There was a situation, a startup, a startup that had gotten investment from Amazon's Alexa fund
[01:09:54.160 --> 01:10:01.920]   had publicly accused them of basically copying whatever. Oh, God, my, my Alexa just turned on.
[01:10:01.920 --> 01:10:06.400]   Say echo, say echo. You're having the same problem. All of our listeners are,
[01:10:06.400 --> 01:10:10.960]   I caution you all. I should probably have told you this before the show. Say the word echo
[01:10:10.960 --> 01:10:12.640]   instead of anything else.
[01:10:14.560 --> 01:10:17.280]   You use Siri. It's all on you. We can't help you.
[01:10:17.280 --> 01:10:19.040]   I just happen to be sitting five feet from it.
[01:10:19.040 --> 01:10:22.000]   Yeah. No, but everybody who listens, if they're listening on an echo,
[01:10:22.000 --> 01:10:24.800]   right, as soon as you say the A word things have.
[01:10:24.800 --> 01:10:30.080]   I have yet to buy a dollhouse or anything, but I've tried. I have.
[01:10:30.080 --> 01:10:33.120]   Well, the name of the, the name of the fund actually is the,
[01:10:33.120 --> 01:10:34.160]   the way it was.
[01:10:34.160 --> 01:10:34.960]   The sale like, yeah.
[01:10:34.960 --> 01:10:36.160]   Say it.
[01:10:36.160 --> 01:10:42.960]   I'm sorry.
[01:10:42.960 --> 01:10:45.840]   Anyway, there is, there, who is the professor? We're going to interview him on
[01:10:45.840 --> 01:10:48.720]   triangulation in a couple of weeks. It's Scott.
[01:10:48.720 --> 01:10:56.080]   Carsten doesn't remember either, but he is the, the guy who's recently got a lot of press
[01:10:56.080 --> 01:11:01.760]   by saying Silicon Valley is worn out as welcome, but more importantly that it's time for government
[01:11:01.760 --> 01:11:03.040]   and regulators. Galloway.
[01:11:03.040 --> 01:11:04.000]   Galloway. Thank you.
[01:11:04.000 --> 01:11:07.200]   He's the, he's the head of that firm we were mentioning earlier, L2.
[01:11:07.200 --> 01:11:13.760]   Oh, I didn't know that. Oh, how interesting. I thought he was a professor or is he both?
[01:11:13.760 --> 01:11:15.120]   Yes, he's both.
[01:11:15.120 --> 01:11:17.360]   Man, some people have way too much energy.
[01:11:17.360 --> 01:11:21.360]   And he just wrote this book, the big four or whatever.
[01:11:21.360 --> 01:11:26.640]   It's on my list. I've got to read it. But so he's a clinical professor of marketing,
[01:11:26.640 --> 01:11:30.720]   which is weird. I don't know what that means. Clinical professor marketing at the New York
[01:11:30.720 --> 01:11:37.680]   University Stern School of Business. And he was named one of the best 50 best business school
[01:11:37.680 --> 01:11:44.800]   professors by poets and quants. So there's that. And he have founded L2, how interesting.
[01:11:44.800 --> 01:11:50.880]   But he is also asserting that it's time for government to regulate these companies that they
[01:11:50.880 --> 01:11:57.840]   have now verged into this territory where they're actually a threat. And this is a thread through
[01:11:57.840 --> 01:12:03.120]   this show because they're a, they're a, they're power. There's huge social power,
[01:12:03.120 --> 01:12:09.040]   allowed Russians to mess with us. You know, I mean, is this,
[01:12:09.040 --> 01:12:14.800]   Branna, you're going to be sitting in the halls, the hallowed halls of Congress in 2019.
[01:12:14.800 --> 01:12:17.680]   What are you going to do?
[01:12:17.680 --> 01:12:21.120]   I mean, I'm not a person.
[01:12:21.120 --> 01:12:21.680]   Whoa.
[01:12:21.680 --> 01:12:22.800]   What are you going to do?
[01:12:22.800 --> 01:12:26.720]   I am not going to make it so these companies can't innovate. You know,
[01:12:26.720 --> 01:12:32.640]   something we talked about last time I was on your show, Leo, was for whatever reason,
[01:12:32.640 --> 01:12:37.440]   government is managing to do less and less for many, many reasons.
[01:12:37.440 --> 01:12:39.600]   So maybe we need these technology companies.
[01:12:39.600 --> 01:12:45.280]   I do see like Google's alphabet arm coming forward and doing a lot of really interesting
[01:12:45.280 --> 01:12:49.600]   technology with addressing global warming. So they did in Puerto Rico. They're putting
[01:12:49.600 --> 01:12:55.280]   Project Loon balloons up over Puerto Rico. They got permission from the FCC to give Puerto Rico
[01:12:55.280 --> 01:12:56.400]   wireless connectivity.
[01:12:56.400 --> 01:13:02.240]   Yeah. So I'm very, I'm very suspicious of anything. It kind of uses them,
[01:13:02.240 --> 01:13:07.760]   you know, kind of uses tech as like something to blame for every problem we have in the United
[01:13:07.760 --> 01:13:12.960]   States right now. I don't agree with that. Now, if you're talking like regulating ads on Facebook,
[01:13:12.960 --> 01:13:18.400]   the way we regulate ads on television, that seems very reasonable to me. But, you know,
[01:13:19.360 --> 01:13:26.240]   I just, I see the forces coming against all these companies. And I do think that they need to think
[01:13:26.240 --> 01:13:31.040]   about their approach because it's clearly generating public sentiment for this. But, you know, me
[01:13:31.040 --> 01:13:36.880]   personally, would I be part of this? No. It's what my friend Jeff Jarvis would call Techno panic.
[01:13:36.880 --> 01:13:42.800]   And it's very easy to stimulate among people. And, you know, with privacy concerns, with concerns
[01:13:42.800 --> 01:13:49.280]   about big business with antitrust concerns. But it's really important to remind everybody
[01:13:49.280 --> 01:13:56.800]   about the amazing advantages, the amazing things we've been able to do thanks to these big companies.
[01:13:56.800 --> 01:14:03.200]   And so you really want to, I think, balance any regulation. I agree with you 100% with
[01:14:03.200 --> 01:14:10.240]   the with the time for serving in these companies to, I mean, what we saw with Weinstein exists
[01:14:10.240 --> 01:14:16.000]   within the technology industry and and other. Let's face it, anywhere men are in charge. It
[01:14:16.000 --> 01:14:19.840]   exists. >> Yes, and I say because of the tech industry,
[01:14:19.840 --> 01:14:24.400]   because the tech industry will face whether it's rational or not, they will face
[01:14:24.400 --> 01:14:29.680]   wings of disapproval in the coming years. And if we want reasonable regulation, and if we want
[01:14:29.680 --> 01:14:34.800]   a reasonable conversation, these companies need to stick their hubris in the backseat and say,
[01:14:34.800 --> 01:14:40.320]   there are things that we could be better actors in some ways. And here are the ways that we are
[01:14:40.320 --> 01:14:45.840]   doing those things. We care about you and we care about your privacy or whatever. The messaging is
[01:14:45.840 --> 01:14:51.600]   very bad right now. And I think that it's kind of tone deaf to the sentiment that exists through
[01:14:51.600 --> 01:14:57.840]   the rest of the country. It takes one recession for people to kind of change their attitude about
[01:14:57.840 --> 01:15:02.000]   these companies. >> I think that's actually to be fair to Professor Galloway. That's kind of
[01:15:02.000 --> 01:15:07.040]   what he's urging is that the tide is changing and it's time for companies to sit up, take notice,
[01:15:07.040 --> 01:15:12.400]   and do the right and be proactive. >> And the big question, I mean, I don't ever,
[01:15:12.400 --> 01:15:17.920]   I don't really realistically think that major antitrust action can be successful and break
[01:15:17.920 --> 01:15:22.880]   up one of these companies. But the biggest question in any antitrust case is consumer choice. And
[01:15:22.880 --> 01:15:27.280]   I think the thing that scares a lot of people is the realization that you actually don't have a lot
[01:15:27.280 --> 01:15:34.400]   of choices in what technology services that you use. I mean, you can't live your life
[01:15:34.400 --> 01:15:40.880]   carrying around a flip phone, not using Gmail if that's what your company uses. And
[01:15:40.880 --> 01:15:47.040]   buy, I mean, to go to live your life outside of the big four tech companies would be to kind of be
[01:15:47.040 --> 01:15:51.120]   in a cabin in the woods. It's almost impossible. And I think that's what's a little bit scary is
[01:15:51.120 --> 01:15:54.960]   that we don't really have as much choices that we think we do. >> Yeah.
[01:15:54.960 --> 01:16:01.840]   These are big, big issues. But since we have a future congressperson here, I think we need to
[01:16:01.840 --> 01:16:08.560]   for Brianna. >> Thank you. >> We're gonna be calling you, we're gonna be writing you.
[01:16:09.760 --> 01:16:15.120]   You're gonna be our point person. >> I will take like calls, absolutely. I would love to hear
[01:16:15.120 --> 01:16:22.080]   from your listeners. >> Good. Let's see here. Let's take a break. We have quite a bit more to
[01:16:22.080 --> 01:16:28.000]   talk about. Great panel here, Brianna Wu. Don't forget the website, Brianna Wu 2018.com.
[01:16:29.520 --> 01:16:40.640]   Aaron Griffith is here from the fabulous Wired platform. And from CBSi, CBS Interactive,
[01:16:40.640 --> 01:16:46.400]   Dan Patterson, senior writer at Tech Republic. We had a great week on Twitch. We had a fun week
[01:16:46.400 --> 01:16:52.240]   on Twitch. Yes, Karsten, prepare that. We have a set up the projector, get the AV squad in here,
[01:16:52.240 --> 01:16:56.320]   bring up, set up the screen because we have a little video to show you. Watch.
[01:16:57.120 --> 01:17:05.040]   Previously on Twitch. >> Hi, Mom. >> Hey, sweetie. How are you? >> Hi, Mom, Mary. Say hi to Megan and
[01:17:05.040 --> 01:17:12.240]   the gang on iOS today. >> Megan, happy to say hi. >> Hi, Mary. >> I know what Megan looks like.
[01:17:12.240 --> 01:17:19.040]   >> iOS today. >> If your parents are confused about tech, we have ways to help you help them.
[01:17:19.040 --> 01:17:26.800]   >> I'm a good person to interview because you have consistently given me every single device.
[01:17:26.800 --> 01:17:33.360]   >> As it comes up, triangulation. >> It makes it harder to make sci-fi when you're trying to be real,
[01:17:33.360 --> 01:17:37.920]   right? I mean, it would be so much easier if you could add one or two elements of traditional
[01:17:37.920 --> 01:17:43.200]   space sci-fi and say, yeah, we don't have to worry about taking a shuttle from the ship to the
[01:17:43.200 --> 01:17:49.520]   station because I just transport over. >> It makes it different. But part of what defines the feeling
[01:17:49.520 --> 01:17:55.680]   of any project is the choice of obstacles. By tuning a different set of obstacles than the usual,
[01:17:56.240 --> 01:18:01.680]   it does mean we have to think things through. It does mean sometimes we have to two things
[01:18:01.680 --> 01:18:06.880]   out that we wouldn't have had to do if we did something more familiar. But it also means we
[01:18:06.880 --> 01:18:12.480]   get things that nobody else has gotten to do. >> To it, it's where your brain and tech meet.
[01:18:12.480 --> 01:18:19.680]   >> I have a bunch of Luddite friends who are scared. They don't have the son I have. I'm so
[01:18:19.680 --> 01:18:26.080]   sorry for them. >> Mom, by the way, wants a show now, by the way. I'm just going to say she
[01:18:26.080 --> 01:18:31.360]   she called me back. She said, can I be on every week? I'd like a show. And I'm thinking about it.
[01:18:31.360 --> 01:18:36.720]   I'm considering it. Our show today brought to you by ITProTV. If you're looking for
[01:18:36.720 --> 01:18:41.680]   a way to get those tech skills so you can get a great job in IT, and I know great many of you
[01:18:41.680 --> 01:18:46.320]   either working IT or would like to work in IT, or maybe you do work in IT and you just want to
[01:18:46.320 --> 01:18:52.320]   keep up to date, ITProTV is such a great resource to keep you and your team if they have a great
[01:18:52.320 --> 01:18:58.960]   team platform to up to date. They kind of blown me away. They modeled what they were doing and
[01:18:58.960 --> 01:19:04.800]   what we did at Tech TV and Twit. They now have five studios. They're cranking out so much great
[01:19:04.800 --> 01:19:11.760]   content, 125 hours of new courseware every single week, 2,000 hours plus of on-demand training.
[01:19:11.760 --> 01:19:17.120]   For getting the certs in every area, certified ethical hacker, Kali Linux,
[01:19:17.680 --> 01:19:23.920]   Microsoft server, AWS sysops administrator, CompTIA security plus. How to run a Windows
[01:19:23.920 --> 01:19:28.640]   network, a Macintosh network, I go on and on and on. You can watch these anytime they stream
[01:19:28.640 --> 01:19:33.760]   them of course on your computer, but also iOS, Android. They work on the Chromecast,
[01:19:33.760 --> 01:19:38.560]   they have a Roku app, they have an Apple TV app. You can watch on your big screen TV,
[01:19:38.560 --> 01:19:42.800]   then get in the car, listen on your phone, get to work and watch on your computer and learn
[01:19:42.800 --> 01:19:49.440]   all the time. Whether you're looking for a CISSP or a CISA or CEH, you get the search you need,
[01:19:49.440 --> 01:19:54.640]   you get the skills you need. If you are managing an IT team, I want you to consider ITProTV,
[01:19:54.640 --> 01:20:00.080]   their team solution gives you group pricing, a great supervisor portal where you can control
[01:20:00.080 --> 01:20:04.560]   your team's training schedule, create custom groups, training assignments, see individual
[01:20:04.560 --> 01:20:10.160]   analytics, group analytics, viewing time, downloads, course completion, tracking,
[01:20:10.160 --> 01:20:18.320]   everything you need. They've got such a great platform. Go to the website ITProTV/Twit and find
[01:20:18.320 --> 01:20:25.120]   out how ITProTV can offer you or your team the best IT training solution. If you are an individual,
[01:20:25.120 --> 01:20:32.560]   use the offer code TWIT30 at ITProTV/Twit. You'll get a seven-day trial free, you also get 30% off
[01:20:32.560 --> 01:20:37.600]   for the lifetime of your account when you sign up for an individual monthly membership using the
[01:20:37.600 --> 01:20:44.480]   code TWIT30. If you want to know more about the team solution, you can sign up there for a free
[01:20:44.480 --> 01:20:52.160]   demo of the supervisor portal. ITProTV, I love these guys, Tim and Don, they watched what we were
[01:20:52.160 --> 01:20:56.320]   doing and they said this would be a great way to learn how to be an IT professional. They made it
[01:20:56.320 --> 01:21:02.720]   ITProTV/Twit out of Gainesville, Florida, they're great guys. Use the offer code TWIT30 to try it
[01:21:02.720 --> 01:21:12.560]   free for seven days and 30% off your account. Back to the, let me find something fun, something
[01:21:12.560 --> 01:21:18.720]   exciting. Qualcomm files a patent lawsuit against Apple. I'm trying to seeking the ban to sell a
[01:21:18.720 --> 01:21:27.440]   manufacturer of iPhones. I should say Qualcomm is a sponsor, they advertise their Snapdragon
[01:21:27.440 --> 01:21:32.640]   Gigabit radio, which is only on Android phones because Apple doesn't use it. But these two
[01:21:32.640 --> 01:21:39.600]   are in a bitter battle to the end. Apple says, "We're not going to pay our $2 billion a year license fee."
[01:21:39.600 --> 01:21:46.320]   Maybe I'm a little, I'm more disposed towards Qualcomm. They invented stuff. They are charging
[01:21:46.320 --> 01:21:51.920]   license fees. I don't think unreasonable license fees, but maybe that's what the debate is about.
[01:21:51.920 --> 01:22:01.680]   But these are technologies every phone has to have there. And so I, anyway, the battle has
[01:22:01.680 --> 01:22:07.680]   spreading. It's now World War III, really. It's just crazy.
[01:22:07.680 --> 01:22:16.960]   Yeah, my husband, he does a patent law for a biotech firm. So yeah, I asked him about this.
[01:22:16.960 --> 01:22:22.400]   And he sees it as just them playing really, really, really ultimate heart ball.
[01:22:22.400 --> 01:22:25.760]   It's very public negotiations is what it is. It's very public.
[01:22:25.760 --> 01:22:33.680]   That's exactly how he reads it. Patents, it's like, if you look and know the companies and the law
[01:22:33.680 --> 01:22:39.200]   firms that do a lot of patent legislation, it ends up being like they have a pool of money that
[01:22:39.200 --> 01:22:44.240]   just keeps going back and forth between them because it's like defensive and offensive warfare.
[01:22:44.240 --> 01:22:50.480]   Seems like there ought to be a better way. Anyway, I won't weigh in on the battle, but it's
[01:22:51.200 --> 01:22:59.040]   yeah, I don't know. Meanwhile, one plus I like the one plus one phones. I love them. One plus,
[01:22:59.040 --> 01:23:07.040]   remember, had a deal with I can't even remember anymore, but they had an Android ROM that they
[01:23:07.040 --> 01:23:12.880]   used to use. Then they lost that deal. Whatever happened to that company, by the way, that was
[01:23:12.880 --> 01:23:17.600]   making the ROM for them. They were there did a deal with Microsoft. It got weird. Anyway,
[01:23:17.600 --> 01:23:23.120]   they created the oxygen OS, which is essentially a pretty pure version of Android for their one
[01:23:23.120 --> 01:23:32.320]   plus phones, except it's been discovered. Chris Moore, a developer, looked and realized that his
[01:23:32.320 --> 01:23:37.920]   one plus was uploading a lot of not anonymous information back to the company, including the
[01:23:37.920 --> 01:23:44.080]   IEMI, the serial number, the MAC address. It was even telling the company when the MAC address,
[01:23:44.080 --> 01:23:48.960]   yeah, the MAC address, when the apps were opened, when the handset was unlocked,
[01:23:48.960 --> 01:23:55.280]   and there was no opt-in, there was no way to disable. It was a very complicated way to
[01:23:55.280 --> 01:23:59.920]   disable it, but no obvious way to disable it. Speaking of reckless.
[01:23:59.920 --> 01:24:03.920]   That's that, you know, it's one of those things companies sometimes do, and sometimes it's just
[01:24:03.920 --> 01:24:08.560]   an accident. Sometimes it's like, well, just do it until we get caught, and then we'll apologize and
[01:24:08.560 --> 01:24:14.640]   fix it. And I don't know which it is, but the CEO of OnePlus, the co-founder Carl Pace said,
[01:24:14.640 --> 01:24:18.960]   by the end of the month, by the end of this month, October, we're going to fix that.
[01:24:18.960 --> 01:24:23.840]   Every one plus phone running oxygen OS will have a prompt in the setup wizard that asks users if
[01:24:23.840 --> 01:24:30.400]   they want to join our user experience program. That's what they're calling it. And it will indicate
[01:24:30.400 --> 01:24:35.040]   that the program collects analytics, will provide terms of service agreement that explains what
[01:24:35.040 --> 01:24:40.480]   we collect, why we will stop collecting telephone numbers, MAC addresses, and Wi-Fi information.
[01:24:40.480 --> 01:24:46.080]   The company, we just, we wanted to give you better support, a better experience. And you know what,
[01:24:46.080 --> 01:24:51.360]   companies do that. I'm glad they got caught, and I hope they do the update, and it sounds like
[01:24:51.360 --> 01:25:01.840]   they will. So, is it tech behaving badly, or oops, I made a mistake. I won't do it again. I don't know.
[01:25:01.840 --> 01:25:07.200]   You just don't know. Well, there's no apology, so. They didn't apologize with that.
[01:25:07.200 --> 01:25:12.080]   And it's, and the calling it the user experience program,
[01:25:12.080 --> 01:25:18.000]   like that just feels a little bit like before. And we just want to make a better phone. That's
[01:25:18.000 --> 01:25:25.840]   all we just, just try to make a better phone. Yeah. I mean, I think the best idea I have,
[01:25:25.840 --> 01:25:31.200]   something I've thought about a lot policy wise is how we address this, you know, IoT model,
[01:25:31.200 --> 01:25:36.880]   that your rewards company is for collecting all the data that they can't, you know, because you can
[01:25:36.880 --> 01:25:41.680]   sell it and you can make money for it. You know, in a capital system, there's every incentive to
[01:25:41.680 --> 01:25:48.080]   capture and collect as much information as you can about your users. The best idea I have for that
[01:25:48.080 --> 01:25:54.320]   to kind of turn it around is right now there's not really a civil liability aspect if that
[01:25:54.320 --> 01:26:00.560]   information is hacked and it causes damage to the users actually having it. So, I think we do have
[01:26:00.560 --> 01:26:06.080]   to introduce liability into that. So, you know, if they are collecting all this information about
[01:26:06.080 --> 01:26:11.760]   you and they're not securing it correctly and somebody ends up hacking that, then that is very
[01:26:11.760 --> 01:26:16.400]   financial expensive. So, I think that that approach. So, you would punish, you would punish Equifax
[01:26:16.400 --> 01:26:23.360]   severely at this point. Oh, yeah, absolutely. Like, they had a very high up. Yeah. Oh, it's just,
[01:26:23.360 --> 01:26:28.400]   the more you understand about how they screwed up, it just, it gets you angrier and angrier.
[01:26:29.360 --> 01:26:33.920]   But I just, I think that liability is the only way I can figure out to solve this problem because
[01:26:33.920 --> 01:26:38.960]   you can't have regulations saying you can't collect A, B, or C. You could just make it expensive to,
[01:26:38.960 --> 01:26:45.040]   you know, collect and store all this information. The Equifax story keeps getting worse, by the way.
[01:26:45.040 --> 01:26:50.320]   This time, this was discovered again by a secure thank God for security researchers. And I agree
[01:26:50.320 --> 01:26:56.160]   with you, Brianna, we really have to indemnify them because so, so often now they get, they get
[01:26:56.160 --> 01:27:02.880]   prosecuted for pen testing, for finding flaws. And there's a huge disincentive for them to reveal
[01:27:02.880 --> 01:27:08.160]   these flaws at this point because you don't know how the company is going to react. The Equifax
[01:27:08.160 --> 01:27:18.400]   has a page. This is so awful. Equifax, this is recent. They fixed it, but until like a couple of days
[01:27:18.400 --> 01:27:23.280]   ago, they had a page, you know, where you go to dispute a line in your credit report. On that page,
[01:27:23.840 --> 01:27:31.280]   there was a link that would prompt you to download a phony flash player, phony flash player.
[01:27:31.280 --> 01:27:39.040]   And that put spammed you with ads. And Equifax's defense was, well, we didn't do it as a third party
[01:27:39.040 --> 01:27:48.160]   that we put on the website. And then, and then by the way, I just saw today,
[01:27:48.160 --> 01:27:53.760]   another firm's doing the same. Was it Experian TransUnion? TransUnion was doing the same thing.
[01:27:54.640 --> 01:27:56.560]   These, thank you.
[01:27:56.560 --> 01:28:04.320]   Hold up. There's no way to hold them accountable for this because it should be hard pressed to find
[01:28:04.320 --> 01:28:12.480]   anyone to disagree that they need to be. So why can't more than that, they should be at a higher
[01:28:12.480 --> 01:28:20.960]   standard. Any company that is holding really privileged information about us and Equifax,
[01:28:20.960 --> 01:28:26.160]   TransUnion, Experian, they have Novus is another one. They have all of our financial records. They
[01:28:26.160 --> 01:28:30.400]   have our salary history. They have our socials. They have our credit card numbers. They have our
[01:28:30.400 --> 01:28:35.040]   addresses. They have everything. Drivers license. They have everything. They need to be held to a
[01:28:35.040 --> 01:28:40.800]   much higher standard. It's a crap business they're in. They sell that information to other companies.
[01:28:40.800 --> 01:28:48.160]   I understand we need this stuff because you can't apply for credit if you can't somehow prove
[01:28:48.160 --> 01:28:52.880]   that you're credit worthy. I get it, but they've got to be held to a super high standard.
[01:28:52.880 --> 01:28:58.240]   Now somebody proposed, I think it was Johnny Jett on my radio show this weekend, and I thought
[01:28:58.240 --> 01:29:03.040]   this was a great idea. Brianna can do this. We're going to give you another bill you can write.
[01:29:03.040 --> 01:29:10.960]   The credit freeze, which is what your response is now to this, which is to go to each individual
[01:29:10.960 --> 01:29:16.080]   credit reporting agency, say, put a freeze on my account. Nobody can ask for my data without my
[01:29:16.080 --> 01:29:21.600]   permission. If I want to apply for credit, I'll unfreeze it for that particular use and then freeze it again.
[01:29:21.600 --> 01:29:28.880]   That, I love this, should be the default now. These companies can collect the data. I understand
[01:29:28.880 --> 01:29:36.320]   it needs to be, but it should be me that releases that data, not them. It should be frozen. The only
[01:29:36.320 --> 01:29:42.720]   person that should be able to take to get my data is me or a company I specifically allow to do that.
[01:29:42.720 --> 01:29:48.400]   What do you think of that? I think it's really funny you mentioned that. We're about to put out
[01:29:48.400 --> 01:29:55.120]   a proposal that's very similar to that. It is really time to admit that social security number
[01:29:55.120 --> 01:30:02.160]   plus birth date is a very hack basis for your identity, and we need to get a little bit past
[01:30:02.160 --> 01:30:06.560]   sets. What we're going to be proposing is the social security and privacy number.
[01:30:08.560 --> 01:30:14.160]   You're going to get people going crazy because they're going to say, "No, that's a nationwide ID
[01:30:14.160 --> 01:30:20.800]   system." That's scary. We already have that. The way we're going to do is we're going to put it
[01:30:20.800 --> 01:30:26.160]   all in your hands exactly like you said. Along with your social security card or driver's license,
[01:30:26.160 --> 01:30:34.160]   you will get embedded in that public key and a private key. Yes, Equifax can go through and they
[01:30:34.160 --> 01:30:40.800]   can collect that information about you. If I open up a bank account, I have to use my private key
[01:30:40.800 --> 01:30:46.720]   with my public key to generate something. If they lose my trust, I can revoke that certificate.
[01:30:46.720 --> 01:30:53.360]   Just like Microsoft can revoke a certificate. We've got to bring modern security standards
[01:30:53.360 --> 01:31:02.720]   to applying for credit because right now, identity theft is a cost for us because we don't choose
[01:31:02.720 --> 01:31:09.600]   to do business with Equifax. That's brilliant. This is why we need to like people like Brianna
[01:31:09.600 --> 01:31:14.400]   and more people like Brianna to Congress because there are technological solutions. I understand
[01:31:14.400 --> 01:31:20.400]   the big problem is authentication, but we have, it's well known, a very good system for
[01:31:20.400 --> 01:31:31.520]   authentication. It's the public key crypto. I love this idea. My federal ID, my ID should be my
[01:31:31.520 --> 01:31:39.520]   public key, which I generate myself. I can revoke at any time. I keep held to my chest,
[01:31:39.520 --> 01:31:46.000]   my private key. No one else can get access to that. That's a great idea, Brianna. That is
[01:31:46.000 --> 01:31:52.160]   strong authentication. It solves this whole problem. It's pretty straightforward. We've been
[01:31:52.160 --> 01:31:58.000]   doing it everywhere else for the last 15, 20 years. Let's bring this to something very,
[01:31:58.000 --> 01:32:05.440]   very important, your credit, your identity. What's the biggest barrier to stopping us from doing that?
[01:32:05.440 --> 01:32:10.560]   I don't coerce a period of time. I'm serious. Consumer adoption.
[01:32:10.560 --> 01:32:16.160]   It would be standards. It would be standards across all the different state lines. You'd have to pass
[01:32:16.160 --> 01:32:23.120]   federal standards. It's weird if you think about driver's licenses as recently as the 80s,
[01:32:23.120 --> 01:32:29.520]   you didn't even have to have your picture on it in some states. We basically need to make a
[01:32:29.520 --> 01:32:35.440]   standard that's federal and we need to mandate that you ought to bring people in to update their
[01:32:35.440 --> 01:32:40.320]   Social Security card or driver's license. It's a lot of small details.
[01:32:40.320 --> 01:32:44.640]   I'll tell you why it's a non-starter in the United States, actually, Aaron, because we're
[01:32:44.640 --> 01:32:51.360]   paranoid. A lot of the problems that we have in the United States come from our national identity.
[01:32:51.360 --> 01:32:58.240]   There's a new book which I just downloaded for Ron Audible about how since day one in the United
[01:32:58.240 --> 01:33:05.600]   States, we have always believed fake news in conspiracy theories. This is part of our national
[01:33:05.600 --> 01:33:11.840]   identity. It comes from our notion that the individual is supreme and that everybody's entitled
[01:33:11.840 --> 01:33:16.480]   to their own opinion and beliefs. It's one of the things that keeps this because what it would be
[01:33:16.480 --> 01:33:22.080]   identified immediately is a national ID card. There are many, many people in the United States
[01:33:22.080 --> 01:33:27.440]   who are basically paranoid who say, "No, not going to happen." I could tell you right now,
[01:33:27.440 --> 01:33:31.520]   it's a non-starter in Congress. All you have to do is say national ID and that's it. It's over.
[01:33:31.520 --> 01:33:37.040]   It's not going anywhere. Here's a country that has it, Estonia. In fact,
[01:33:37.040 --> 01:33:42.160]   anybody in the world can get one of these. When I was in Estonia and I'm meant to do it,
[01:33:42.160 --> 01:33:49.280]   I didn't get around to it, they have a national state issued digital ID card that has on it
[01:33:49.280 --> 01:33:58.880]   a 20-48-bit public key on a chip. You have a card. It is your national health insurance card.
[01:33:58.880 --> 01:34:02.960]   It's your proof of identification when logging into bank accounts. It's your digital signature.
[01:34:02.960 --> 01:34:07.920]   You use it to vote. Check medical records, submit tax claims, get prescriptions.
[01:34:09.680 --> 01:34:15.040]   They make one available, a similar card available to anybody in the world.
[01:34:15.040 --> 01:34:21.040]   This is brilliant. It's exactly what you just described and the reason it's a non-starter,
[01:34:21.040 --> 01:34:28.160]   I think. I'll vote for you, Brianna. Again and again, I won't stop you, but I do know that this
[01:34:28.160 --> 01:34:32.240]   is one of those hot button issues. I know that I've worked in AMTALK Radio long enough.
[01:34:32.240 --> 01:34:36.160]   This is a hot button issue for some reason.
[01:34:36.160 --> 01:34:40.240]   I spent five years in AMTALK. There's 1 million people.
[01:34:40.240 --> 01:34:43.280]   Yeah, Dan, it's like it's equivalent to take our guns away.
[01:34:43.280 --> 01:34:46.880]   It is just... Oh, yeah. It's just never going to happen.
[01:34:46.880 --> 01:34:49.280]   And federalism. I mean, there is a national identity in the national...
[01:34:49.280 --> 01:34:50.400]   Can't you bring Brandon somehow?
[01:34:50.400 --> 01:34:50.800]   I'm sorry?
[01:34:50.800 --> 01:34:52.080]   Rebranded.
[01:34:52.080 --> 01:34:52.480]   I can't bring Brandon.
[01:34:52.480 --> 01:34:54.000]   [laughter]
[01:34:54.000 --> 01:34:56.160]   Yeah, right. That's exactly what has to happen.
[01:34:56.160 --> 01:35:01.200]   Well, especially if you can generate your own public key, of course, you're not going to do that,
[01:35:01.200 --> 01:35:01.760]   right? Because...
[01:35:01.760 --> 01:35:02.880]   No.
[01:35:02.880 --> 01:35:09.120]   Mom's not going to do that. Actually, my mom could do that, but most people aren't going to do that.
[01:35:09.120 --> 01:35:15.440]   I mean, you could start even if it wasn't like a federal ID card.
[01:35:15.440 --> 01:35:16.080]   Make it grassroots.
[01:35:16.080 --> 01:35:16.480]   There you go.
[01:35:16.480 --> 01:35:24.560]   That's the place you start the discussion, but then you go, "Okay, so in exchange for not
[01:35:24.560 --> 01:35:29.200]   coming down really heavily regulating like Equifax in companies like that.
[01:35:29.200 --> 01:35:34.880]   Okay, so can you agree to use this standard? Can you agree that for you to open up new accounts,
[01:35:34.880 --> 01:35:40.240]   you need to use this standard?" I sometimes get frustrated when people say, "X can't be done."
[01:35:40.240 --> 01:35:46.320]   Because it's like the expectation that something can't be done drives the whole conversation.
[01:35:46.320 --> 01:35:51.040]   No, no, no. It's a fair point. We should be clear-eyed and realistic about this.
[01:35:51.040 --> 01:35:55.040]   But I think just because you start a discussion at a certain point doesn't mean
[01:35:56.080 --> 01:36:00.160]   look at one political party in the United States. They take the most extreme view.
[01:36:00.160 --> 01:36:06.080]   Then even when they get something in the middle, it's a win. I think as far as updating
[01:36:06.080 --> 01:36:12.320]   like cryptographic standards, there has to be someone that's out there pushing better practices.
[01:36:12.320 --> 01:36:17.200]   There is... I mean, I do this. Maybe we need to get everybody who...
[01:36:17.200 --> 01:36:19.920]   People will watch our and listen to our shows or techies.
[01:36:21.120 --> 01:36:29.840]   I publish my PGP key, my public key. Everywhere I go, I sign emails with it. You can have it. Here it is.
[01:36:29.840 --> 01:36:36.080]   It's long because I made a 4096-bit key because I don't want anybody to crack it.
[01:36:36.080 --> 01:36:39.760]   But that's my public key. If you ever doubt that I am who I say I am,
[01:36:39.760 --> 01:36:45.200]   take my public key and say, "Okay, Leo, prove your identity with your private key."
[01:36:45.200 --> 01:36:49.040]   If you get an email from me, you can validate that not only did I send it,
[01:36:49.040 --> 01:36:52.960]   and it wasn't changed since I sent it. This is a great system.
[01:36:52.960 --> 01:36:56.160]   What do we do if and when quantum happens?
[01:36:56.160 --> 01:37:00.720]   Make it 8,192-bit.
[01:37:00.720 --> 01:37:06.240]   Seriously, there's no real impediment to making these keys prohibitively long.
[01:37:06.240 --> 01:37:12.480]   I'm not convinced. I mean, I don't know. I'm not an expert on quantum computing. The theory is that
[01:37:13.280 --> 01:37:19.680]   some computing technology massively parallel perhaps will come along and eliminate public key crypto
[01:37:19.680 --> 01:37:25.680]   will have to come up with something else. But it works right now. It works really well.
[01:37:25.680 --> 01:37:28.480]   God damn it. Why do you just have to say that?
[01:37:28.480 --> 01:37:35.920]   Are we how close are we, Dan? Is quantum just around the corner?
[01:37:35.920 --> 01:37:39.760]   I don't think we're close, but I talked to some people who say five years.
[01:37:39.760 --> 01:37:44.000]   I talked to some people who say we're five years from AGI too, which is nonsense.
[01:37:44.000 --> 01:37:46.320]   What's AGI?
[01:37:46.320 --> 01:37:49.920]   40 years? Artificial general intelligence, which is,
[01:37:49.920 --> 01:37:52.960]   incidentally speaking of companies squashing competition,
[01:37:52.960 --> 01:38:00.080]   the entire academic world is pretty pissed at Google for acquiring much of the great minds
[01:38:00.080 --> 01:38:06.960]   and AI right now and putting them to use within DeepMind or on projects DeepMind is dreamed up.
[01:38:07.680 --> 01:38:12.800]   But I don't know 40 years maybe, but it will happen and that means all secrets retroactively
[01:38:12.800 --> 01:38:16.720]   will disappear. We're not enough use perfect for secrecy, right? You can use.
[01:38:16.720 --> 01:38:18.960]   Yes, right. Yeah. All right. It's possible.
[01:38:18.960 --> 01:38:23.280]   Question about that Google thing though. Like didn't those people go there voluntarily?
[01:38:23.280 --> 01:38:26.000]   It's not. Yeah, for sure. Yeah. Yeah.
[01:38:26.000 --> 01:38:31.600]   Yeah. I'm not going to say I'll give you a million dollars, Aaron, if you join our research team.
[01:38:31.600 --> 01:38:36.640]   Who's going to say no to that? Yeah, you're absolutely right. I mean, it's a nuanced conversation,
[01:38:36.640 --> 01:38:40.880]   but the conversation that people are having is that the company is coming in with a bunch of
[01:38:40.880 --> 01:38:44.880]   capital that nobody can meet. We need some of those people to bring back that.
[01:38:44.880 --> 01:38:51.280]   We made that compute engine and the special artificial intelligence machine learning
[01:38:51.280 --> 01:38:57.360]   processor array and all sorts of stuff. But it is a wide open field, right, Dan?
[01:38:57.360 --> 01:39:00.480]   Yeah. Yeah. Yeah. And I think anybody can do it right now.
[01:39:01.440 --> 01:39:08.080]   I'm not sure I'm in a hurry for AGI. That's that may that may we met this whole conversation
[01:39:08.080 --> 01:39:13.360]   could be moot if the machines get smarter than us. Here's a good one.
[01:39:13.360 --> 01:39:19.760]   37,000 Chrome users downloaded a fake adblock plus plus extension. It seems like Chrome
[01:39:19.760 --> 01:39:24.560]   extensions. There's no security. There's a Chrome extension store if you use Chrome
[01:39:24.560 --> 01:39:30.080]   and adblock plus, which is a legitimate ad blocker. Many people want to get it, but with
[01:39:30.080 --> 01:39:36.000]   the problem is they went to the store and downloaded the wrong one. It got through Google's verification
[01:39:36.000 --> 01:39:42.000]   process and lived in the official Chrome store for a while. They've taken down the listing after
[01:39:42.000 --> 01:39:47.840]   Tay Swift on security. I love this Taylor Swift on security account, by the way. I'm sure everybody
[01:39:47.840 --> 01:39:55.840]   knows about about Swift on security. Do you bet Dan knows who it is? No, no idea.
[01:39:56.960 --> 01:40:03.200]   Why are more people trying to figure out who this is? A security person who poses his Taylor
[01:40:03.200 --> 01:40:09.920]   Swift. I love it. Anyway, once swift on security revealed that Google pulled it down, but that was
[01:40:09.920 --> 01:40:18.640]   after 37,000 people downloaded it. They got to fix that process. That's a big hole.
[01:40:21.920 --> 01:40:30.320]   Oculus had their Oculus Connect event this week. They announced something interesting. A standalone
[01:40:30.320 --> 01:40:41.520]   VR headset for $200. It doesn't even require a phone. Is this what we've been waiting for?
[01:40:41.520 --> 01:40:50.160]   This is going to bring VR to the masses. I mean, does anyone else here like actually play VR games?
[01:40:50.160 --> 01:40:55.440]   Because when I'm playing raw data and I'm shooting people and like stabbing with the sword,
[01:40:55.440 --> 01:41:02.480]   but the wire gets in the way. It really does from really being immersed in the experience.
[01:41:02.480 --> 01:41:10.400]   So I'm not quite sold that it's going to work yet at that price point, but I'm definitely excited
[01:41:10.400 --> 01:41:18.960]   to see this come out. Pretty cool. Then they have a new Oculus, the Santa Cruz, the future of Rift.
[01:41:19.760 --> 01:41:24.960]   Yes. A standalone headset just like you're talking about, but the power of the Oculus Rift,
[01:41:24.960 --> 01:41:30.560]   I guess they've got a PC built-in somehow and inside out position tracking, which means
[01:41:30.560 --> 01:41:36.640]   no more cameras with the Rift and the Vive. You have to have devices, external devices that
[01:41:36.640 --> 01:41:46.960]   position you in space. You know, I have all of that stuff. I don't find myself running to play
[01:41:46.960 --> 01:41:56.560]   VR games anymore. It feels like a cool gimmick, but maybe I don't know.
[01:41:56.560 --> 01:42:03.840]   No, I think you're dead on with that. I think it's really telling that Epic spent all this money
[01:42:03.840 --> 01:42:08.160]   and produced Robo Recall, which is one of the very best Oculus Rift games.
[01:42:08.160 --> 01:42:13.680]   But no one's ever played. I've never talked to another gamer in my entire life that's played it.
[01:42:13.680 --> 01:42:20.000]   And just like you, Leo, I played my Super Nintendo Classic a lot more this month than I did in my
[01:42:20.000 --> 01:42:25.200]   Oculus in the last year. You know what people are playing? The number one game in America right now?
[01:42:25.200 --> 01:42:34.800]   Cuphead. Yes. Yes. One million copies sold. A $20 game has been in development for many, many years.
[01:42:34.800 --> 01:42:40.080]   They showed it at E3. When did they show up, Brianna? E3, like 2013, I think.
[01:42:40.640 --> 01:42:45.520]   Long time. It's been in development a while. Yeah. But you have to see the art style.
[01:42:45.520 --> 01:42:50.000]   You want to play it? I have it right here. Oh, it's so gorgeous.
[01:42:50.000 --> 01:42:59.200]   I can't stop playing Cuphead. It is the music, the art. It is a retro game that is a lot of fun
[01:42:59.200 --> 01:43:06.720]   to play. But what I love is it's two brothers, right? I feel like it's a small group.
[01:43:07.840 --> 01:43:10.720]   What I love is that they succeeded so wildly that
[01:43:10.720 --> 01:43:23.520]   So let's see. This is not me. I'm going to cop to something here.
[01:43:23.520 --> 01:43:29.360]   My 14 year olds playing under my account and he is almost done with the game.
[01:43:29.360 --> 01:43:37.520]   So this is not me. This is way too hard for me. But I can't. It's a fun game because
[01:43:38.480 --> 01:43:49.440]   you don't mind dying. You keep playing. Oh, oh, I only want more life.
[01:43:49.440 --> 01:44:00.160]   And that's the saga of my Cuphead experience. Yeah.
[01:44:03.200 --> 01:44:06.720]   I think this is a new tradition on the Twitter. We have to play it every episode.
[01:44:06.720 --> 01:44:10.800]   I'm badly, badly hooked to the Cuphead.
[01:44:10.800 --> 01:44:16.640]   Now, how do I get it off my screen? I want to go back to the news. Oh, there we go. Okay.
[01:44:16.640 --> 01:44:22.400]   One million copies of it. They went platinum in like a few days or something. It's pretty great.
[01:44:22.400 --> 01:44:27.520]   Shell says, I like this. This is very, to me, this is super good news.
[01:44:27.520 --> 01:44:30.880]   Shell is buying an electric car charging company.
[01:44:32.320 --> 01:44:35.360]   Shell, of course, the big gasoline company
[01:44:35.360 --> 01:44:42.320]   is probably rightly so concerned that people might not be buying gasoline petrol.
[01:44:42.320 --> 01:44:51.920]   Eventually. They plan to launch electric charging in the majority of their 45,000 gas stations.
[01:44:51.920 --> 01:44:58.160]   This is to me an acknowledgement and I think a very important one that we are moving rapidly
[01:44:58.160 --> 01:45:02.080]   towards a renewables future. And you know what's doing it? The economics.
[01:45:02.880 --> 01:45:07.120]   The lawmakers couldn't make it happen. The industry didn't make it happen.
[01:45:07.120 --> 01:45:09.280]   Economics are going to make it happen.
[01:45:09.280 --> 01:45:14.080]   I think it's also overseas competition because it's going to happen in China
[01:45:14.080 --> 01:45:20.480]   far before it comes to the US. And I think people and companies see that.
[01:45:20.480 --> 01:45:23.120]   And they see European adoption ramping up too.
[01:45:23.120 --> 01:45:28.800]   It's a lot of looking around and realizing we're behind.
[01:45:28.800 --> 01:45:32.960]   The good news is there's kind of an unlimited amount of sun hitting this planet.
[01:45:32.960 --> 01:45:38.160]   If we could really get efficient at turning it into energy, we wouldn't need to dig up things
[01:45:38.160 --> 01:45:42.320]   or split atoms. We could just use sun power.
[01:45:42.320 --> 01:45:48.880]   I think the I was talking to somebody about this and that it seems to be the agreement is the
[01:45:48.880 --> 01:45:53.680]   issues not collecting it. We actually are getting very good at collecting it is storing it
[01:45:53.680 --> 01:45:56.080]   because of course the sun doesn't shine all the time.
[01:45:57.200 --> 01:46:00.880]   We need better ways to store the power it's generated.
[01:46:00.880 --> 01:46:03.280]   Let's take a break. We're going to have fun. Go ahead.
[01:46:03.280 --> 01:46:04.240]   I'll let Brianna speak.
[01:46:04.240 --> 01:46:10.160]   No, no, I was just going to say I, you know, I I'm definitely all in on renewable energy.
[01:46:10.160 --> 01:46:14.800]   It's such an important policy issue. And as you said, like China is kicking our butt on it.
[01:46:14.800 --> 01:46:18.720]   It's it's so important. But at the same time, I love cars.
[01:46:18.720 --> 01:46:21.040]   I know. We're worried about.
[01:46:21.040 --> 01:46:23.280]   You want a gas powered car.
[01:46:24.240 --> 01:46:29.360]   I do. About five Porsche came in and I just I hate this going to be obsolete in 20 years.
[01:46:29.360 --> 01:46:34.480]   Yeah. But you know, like a lot of people still love records and they find places to buy them
[01:46:34.480 --> 01:46:36.960]   and they listen to them. Yeah.
[01:46:36.960 --> 01:46:41.440]   You know, they'll be, you know, it's like books. They'll be
[01:46:41.440 --> 01:46:49.920]   and vinyl records. They'll be a collectors guild or something. They'll be a way to do it.
[01:46:49.920 --> 01:46:52.240]   They'll be a way to do it. It might be very expensive.
[01:46:53.440 --> 01:46:59.280]   Oh, but I don't think car. I don't think gas engines internal combustion engines are going to go away.
[01:46:59.280 --> 01:47:04.000]   And but imagine what the world be like if 50% of them were replaced by electric,
[01:47:04.000 --> 01:47:06.400]   just 50% massive change.
[01:47:06.400 --> 01:47:15.680]   A lot of people in self driving world like to compare car ownership to horse ownership.
[01:47:15.680 --> 01:47:19.920]   Yeah, I'm not sure. I I'm not sure I buy that. And that's mostly because I think that there's a
[01:47:19.920 --> 01:47:25.200]   lot of psychological barriers to self driving. I know there's some economic reasons for it.
[01:47:25.200 --> 01:47:29.280]   And I think there's also some technical barriers. It may not. I mean, we're not.
[01:47:29.280 --> 01:47:37.360]   It's not quite there. I and I speak as a Tesla owner. I'm not sure I would take my hands off the
[01:47:37.360 --> 01:47:44.320]   wheel just yet. Thank you. Leo, I know you got to do a spot, but I got a scram here. I got an
[01:47:44.320 --> 01:47:49.840]   engagement DP. Great to have you on the show, Dan Patterson. He is senior writer at Tech
[01:47:49.840 --> 01:47:56.080]   Republic at CBS Interactive. We didn't get to talk about the Kiev Summit that you were at in the
[01:47:56.080 --> 01:48:00.720]   summer soon. Let's come. Let's get you back real soon. We'll talk about it. And thank you,
[01:48:00.720 --> 01:48:05.440]   Aaron and Brianna. It was great to speak with you. Great to have you. Likewise. Take care.
[01:48:05.440 --> 01:48:13.200]   Actually, we'll wrap this up after one word from our sponsor. It's all about my ring video doorbell,
[01:48:13.200 --> 01:48:18.000]   another product I am absolutely in love with. We've had our ring doorbell. We installed a couple
[01:48:18.000 --> 01:48:26.960]   of years ago, easy to install. And it transformed. I don't know. My sense of my home. So the ring
[01:48:26.960 --> 01:48:31.520]   video doorbell is a doorbell. You just unscrewed my old doorbell. I have a wired doorbell. They have
[01:48:31.520 --> 01:48:36.880]   a battery powered one as well. I unscrewed it, put the ring on. Took me about an hour because I'm
[01:48:36.880 --> 01:48:41.520]   not a handyman. They have videos. It's pretty easy to do. They even give you all the tools you need
[01:48:41.520 --> 01:48:46.400]   to do it. And now I can see anybody at my front door when they ring the doorbell. I can answer it.
[01:48:47.280 --> 01:48:50.560]   Even if I'm not home, I can answer it from my phone. I can say, Hey, yeah, what is it?
[01:48:50.560 --> 01:48:57.680]   Now the latest thing is they've added this new they completely expanding their offerings. They've
[01:48:57.680 --> 01:49:03.200]   got now the home security system. And they've got this the ring floodlight cam, which I have
[01:49:03.200 --> 01:49:08.480]   going to I'm putting in all around my house. I'm actually I have lights around my house,
[01:49:08.480 --> 01:49:12.560]   but I'm going to replace about four of them with this, not only because these are motion
[01:49:12.560 --> 01:49:17.120]   activated floodlights. That's that's a that's a great thing. They're led so they don't burn out.
[01:49:17.120 --> 01:49:21.840]   I love that. They're they use the same wiring that my you know, my outdoor lights do,
[01:49:21.840 --> 01:49:29.600]   but they also have this on them. It's an HD camera, a speaker, a microphone. I can see what's going
[01:49:29.600 --> 01:49:34.080]   on outside in my yard. The lights come on. Somebody comes in my backyard lights come on.
[01:49:34.080 --> 01:49:37.600]   I can see who it is. I'll get a notification on my phone, even if I'm somewhere in the world.
[01:49:37.600 --> 01:49:43.120]   We were in France looking at our doorbell. Look, it's all it's fantastic. You know who's at your
[01:49:43.120 --> 01:49:46.960]   house? You know what's going on? You know what packages have been left? I knew that my iPhone 8
[01:49:46.960 --> 01:49:52.080]   had arrived. I love it. Then this camera is fantastic as you can see intruders in your yard. You can
[01:49:52.080 --> 01:49:56.960]   speak to them, try to scare them off. And if you can't, there's a button on the app. You press a
[01:49:56.960 --> 01:50:03.920]   110 decibel alarm goes off and believe me, that's when they go running. Ring is getting better and
[01:50:03.920 --> 01:50:08.560]   better. If you haven't been to ring.com/twit in a while, check out all of the great new products
[01:50:08.560 --> 01:50:12.880]   at Ring. And we've got a deal in there. Ring of security kit you could choose from the original
[01:50:12.880 --> 01:50:20.720]   Ring or the Ring Pro and get one, two or three of the brand new floodlight cams really surround your
[01:50:20.720 --> 01:50:26.320]   home with security and get up to $150 off. Wall Street Journal called the Ring the best of CES
[01:50:26.320 --> 01:50:34.080]   2017. And I agree 100%. It's just, it's, you know, I'm not too nervous. It's not for me. It's not
[01:50:34.080 --> 01:50:38.400]   about security. It's about knowing what's going on. Knowing what's going on around the house,
[01:50:38.400 --> 01:50:47.440]   who's coming and going? It's fantastic. Ring.com/twit save up to 150 bucks off your new Ring of
[01:50:47.440 --> 01:50:54.720]   Security kit. All right, final story. And then we'll let everybody go home. You like all of our
[01:50:54.720 --> 01:50:59.120]   advertisers, Bran. I have to, I have to say, I'm going to vote for you three times now.
[01:50:59.120 --> 01:51:05.440]   I like that. You know, there's a product that Google comes out that put out that
[01:51:05.440 --> 01:51:11.440]   competes in the same space and it's terrible. Like you have to buy jackets off of Amazon for it,
[01:51:11.440 --> 01:51:16.240]   for it to be, you know, usable outside. You're talking about the nest. Yeah, it's better. Yeah,
[01:51:16.240 --> 01:51:22.000]   nest IQ cam. So, yeah. Okay. And this has nothing to do with the fact that they're not a sponsor.
[01:51:22.000 --> 01:51:27.520]   I bought nest IQ cams, bought nest outdoor cams. The IQ cam is supposed to recognize you after a
[01:51:27.520 --> 01:51:33.520]   while. It's still it's the other day, it said, there's an intruder in your house. I looked,
[01:51:33.520 --> 01:51:39.920]   it was my alarm balloons. It is, it is the dumbest artificial enough. I actually, I ended up taking
[01:51:39.920 --> 01:51:44.080]   it out and bringing it here because it's like Lisa said, no, let, no, I don't want that camera
[01:51:44.080 --> 01:51:48.880]   in the house. That's crazy. My alarm balloons. But it, well, you know, it was great. It zoomed
[01:51:48.880 --> 01:51:56.880]   right in on it. My great image of them. All right, finals, let's do our final story. The US
[01:51:56.880 --> 01:52:08.720]   and Japan in a giant robotic duel on Twitch. Wow. Seven o'clock. This Tuesday, America's
[01:52:08.720 --> 01:52:21.760]   giant fighting robot. This could end war as we know it. It's back in July 2015, the US team of
[01:52:21.760 --> 01:52:30.000]   engineers, megabots, incorporated, challenged their Japanese rivals, Suido Bashi heavy industry
[01:52:30.000 --> 01:52:36.480]   to a fight. But they never found a venue. And of course, upgrading the robots has taken some time.
[01:52:36.480 --> 01:52:43.760]   But they're finally, these, I'm not joking. There are these, these are mechs with people inside them.
[01:52:43.760 --> 01:52:51.520]   I think there's absolutely no technical story here, but I just can't wait.
[01:52:51.520 --> 01:52:53.360]   Oh my God.
[01:52:53.360 --> 01:52:54.160]   That looks so bad.
[01:52:54.160 --> 01:52:56.160]   I'm all in on this.
[01:52:56.160 --> 01:53:03.360]   And you know, finally, I can chant USA, USA and really mean it. Really, really,
[01:53:04.000 --> 01:53:08.320]   the battle of the robots. And I think it's kind of a cool look at this is a great picture of the
[01:53:08.320 --> 01:53:15.600]   megabots team next to their. That's a mech. I don't know what those weapons are, but I
[01:53:15.600 --> 01:53:24.080]   hope they're potato. Yeah. I hope it's not anything lethal. I just hope they're not destroyed after
[01:53:24.080 --> 01:53:32.800]   this so they can go on tour afterwards. There's six. I live in Boston. I need that to get around
[01:53:32.800 --> 01:53:39.200]   town. That's so grim. So the Verge is quick to point out that it's not really Pacific Rim.
[01:53:39.200 --> 01:53:45.040]   Okay, they're they're big 16 and 13 feet tall, but they're slow and have limited mobility. So
[01:53:45.040 --> 01:53:50.880]   they say it's going to be more like rock 'em, suck 'em robots. That is Pacific Rim. On the
[01:53:50.880 --> 01:53:56.560]   other hand, they're 12 tons and they have eight foot long chainsaws. So, you know, I'll take it.
[01:53:58.080 --> 01:54:04.000]   It could be fun. We'll be covering that. And also, it's a happy ending.
[01:54:04.000 --> 01:54:09.680]   The 40th anniversary of the thing that got me into computing my first personal computer,
[01:54:09.680 --> 01:54:16.560]   the Atari 2600 game machine. Remember this? Love that. Now, I know there's a whole Nintendo
[01:54:16.560 --> 01:54:21.840]   generation out there and they're probably laughing at us right now, Brianna. But this was the this,
[01:54:21.840 --> 01:54:29.600]   I was I was dropped so much money playing coin up tank that I said, I can't I can't afford to
[01:54:29.600 --> 01:54:34.640]   keep doing this. So I bought a 2600 and started playing it at home. And that kind of after that,
[01:54:34.640 --> 01:54:40.480]   I got an Atari 400, then an 800, then I got a North Star advantage and I got a then the IBM PC
[01:54:40.480 --> 01:54:44.640]   came out. Then I got that then I got a Macintosh and well, the rest is history. So I think this was
[01:54:44.640 --> 01:54:52.000]   the gateway drug. The Atari 2600 is 40 years old today, 30 million units in this lifetime.
[01:54:52.000 --> 01:54:58.400]   Apparently, it's it's iconic. It's iconic. It's a it's a classic box. I asked, what do you think
[01:54:58.400 --> 01:55:04.240]   about the Atari box is coming out? I'm very skeptical about the success of that. Is it is it like the
[01:55:04.240 --> 01:55:09.600]   Nintendo where you're playing the same old games? No, that would be great. That tells like a great
[01:55:09.600 --> 01:55:16.800]   product. They are basically putting out a it's a low powered Linux box. And I was gonna have older
[01:55:16.800 --> 01:55:22.880]   games on it, but it's gonna like split the difference between a high powered PC and like a
[01:55:22.880 --> 01:55:29.360]   mid level PC. So I just don't think people are gonna spend $400 for that. But that's just my opinion.
[01:55:29.360 --> 01:55:32.960]   I have what we're you know what we will get one and get a review unit. That's that sounds really
[01:55:32.960 --> 01:55:39.840]   interesting. I have wasn't aware of that. You're tar it's called the Atari box. That's it. You
[01:55:39.840 --> 01:55:45.120]   can't even get one with the wood panel finish like that. I'm gonna buy that. I'm gonna buy.
[01:55:45.120 --> 01:55:51.200]   Look at that. No, that's sweet. It's it is a retro gaming console, but it's not the the the
[01:55:51.200 --> 01:55:58.320]   really old games like Nintendo. It is gonna run Linux. That's interesting. 90,000 fans have
[01:55:58.320 --> 01:56:05.360]   registered to learn more about it. AMD processor Radeon graphics. Yeah, I wouldn't buy it for a PC,
[01:56:05.360 --> 01:56:12.080]   but that's a sweet looking little the original Atari probably had vinyl wood grain. So
[01:56:12.080 --> 01:56:22.320]   Aaron Griffith so much so much pleasure having you here. I know you ran like 13 miles today and I
[01:56:23.200 --> 01:56:28.640]   I'm still training. Oh, you're still training. All right. Well, I'm grateful we're here. Thank
[01:56:28.640 --> 01:56:34.800]   you for having me. Yes. Wired magazine and Wired.com. She's now senior writer over there covering
[01:56:34.800 --> 01:56:42.160]   well, well, whatever she wants business, but more and tech and anything you're interested in.
[01:56:42.160 --> 01:56:48.640]   Thank you so much for being here today, Aaron. Thanks to Brianna Wu. She fought the outright
[01:56:48.640 --> 01:56:56.080]   fought the outright and one giant space cat founder and now candidate for the
[01:56:56.080 --> 01:57:01.280]   Democratic Party. Well, soon to be candidate for the Democratic Party in Massachusetts,
[01:57:01.280 --> 01:57:06.960]   8th district, but you got to vote in the primary. So make sure you go you donate, you find out more
[01:57:06.960 --> 01:57:11.200]   if you live in Massachusetts. I was just joking about voting eight times for you.
[01:57:11.200 --> 01:57:18.560]   And that's the Russia's decide to help. We also thank Dan Patterson for being here
[01:57:18.560 --> 01:57:24.320]   senior writer with the tech Republic. Thank you all for joining us on a Sunday afternoon. We do
[01:57:24.320 --> 01:57:30.240]   the show about 3 p.m. Pacific, 6 p.m. Eastern 2300 UTC. If you'd like to stop by and watch live,
[01:57:30.240 --> 01:57:37.760]   you can at twit.tv/live. You can also join us in our fabulous chat remire, se.twit.tv.
[01:57:37.760 --> 01:57:42.480]   It's always a great bunch in there. We always have a lot of fun. The kids behind the scenes.
[01:57:43.200 --> 01:57:48.960]   We also have an open studio on on Sundays. If you want to come by and visit us as the entire
[01:57:48.960 --> 01:57:55.040]   Daugherty family did, they're here in their in town for a wedding and it and that's where Rose
[01:57:55.040 --> 01:58:00.560]   came from. And where's Rose? Oh, she's outside. She's with dad right now. Oh, good. All right.
[01:58:00.560 --> 01:58:06.480]   I see Rose's mom. I thought, Oh, and oh, dear. Rose has decided to join the Rebel Alliance and
[01:58:06.480 --> 01:58:10.400]   we're not going to see hide her hair for quite some time. Anyway, if you want to be here in
[01:58:10.400 --> 01:58:14.000]   studio, we'd love to have you just make sure you email us because we really need to know
[01:58:14.000 --> 01:58:20.400]   ahead of time. So our guard doesn't shoot you. The email is no, he was nice guy, right? Yeah,
[01:58:20.400 --> 01:58:26.160]   he's armed. The email is tickets@twit.tv. That's a statement on the modern world, isn't it? That
[01:58:26.160 --> 01:58:29.520]   we have to have an armed guard at the door, but I thought it'd be better to protect our
[01:58:29.520 --> 01:58:34.800]   staff than not. I wouldn't I would hate to feel guilty about that. We do make on-demand
[01:58:34.800 --> 01:58:40.320]   versions available of all of our shows on the website, twit.tv. Please subscribe. That way,
[01:58:40.320 --> 01:58:44.400]   you won't miss an episode. It's a great way to start your week with this week in tech. I'm
[01:58:44.400 --> 01:58:50.720]   Leo Laport. Thanks for being here. And now I have to say another twit is in the kit. Oh, bye.
[01:58:50.720 --> 01:59:00.400]   [Music]

