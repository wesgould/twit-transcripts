;FFMETADATA1
title=You Toot Too, Right?
artist=Leo Laporte, Patrick Beja, Brianna Wu, Shoshana Weissmann
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2023-01-23
track=911
language=English
genre=Podcast
comment=Twitter API fallout, Section 230, TikTok ban, tech layoffs, AmazonSmile RIP
encoded_by=Uniblab 5.3
date=2023
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:05.160]   It's time for Twit this week in Tech. Great panel for you all the way from Finland,
[00:00:05.160 --> 00:00:11.840]   Patrick Beijadjoins, Brianna Wu, and Senator Shoshana Weissman. We have lots to talk about
[00:00:11.840 --> 00:00:16.480]   what's going on at Twitter with the third party clients, the Supreme Court considers
[00:00:16.480 --> 00:00:24.120]   section 230, a TikTok ban layoffs in Big Tech, and an anniversary for the most influential
[00:00:24.120 --> 00:00:30.660]   computer in my career. It's all coming up next on Twit.
[00:00:30.660 --> 00:00:43.320]   Podcasts you love from people you trust. This is Twit.
[00:00:43.320 --> 00:00:52.520]   This is Twit. This week in Tech, episode 9/11 for Sunday, January 22nd, 2023. You too,
[00:00:52.520 --> 00:00:58.320]   right? This episode of This Week in Tech is brought to you by Shopify. Shopify makes
[00:00:58.320 --> 00:01:05.080]   it simple to sell to anyone from anywhere. This is possibility powered by Shopify.
[00:01:05.080 --> 00:01:09.640]   Sign up for $1 per month trial period. Take your business to the next level today. Visit
[00:01:09.640 --> 00:01:18.720]   Shopify.com/Twit all lowercase. And by ExpressVPN. Stop handing over your personal data to the
[00:01:18.720 --> 00:01:23.880]   Big Tech monopoly that mines your activity and sells your information. Protect yourself
[00:01:23.880 --> 00:01:30.360]   with the only VPN I trust to keep me safe online. Visit ExpressVPN.com/Twit to get three
[00:01:30.360 --> 00:01:33.680]   extra months free on a one-year package.
[00:01:33.680 --> 00:01:38.200]   Thanks for listening to this show. As an ad supported network, we are always looking for
[00:01:38.200 --> 00:01:44.280]   new partners with products and services that will benefit our qualified audience. Are you
[00:01:44.280 --> 00:01:56.440]   ready to grow your business? Reach out to Advertise@Twit.tv and launch your campaign now.
[00:01:56.440 --> 00:02:01.320]   It's time for Twit this week in Tech. Show that we cover the week's Tech news. Oh, I
[00:02:01.320 --> 00:02:06.440]   am so excited about this panel. I say that every week, but I really mean it this week.
[00:02:06.440 --> 00:02:12.240]   Brianna Wu is back, ladies and gentlemen, from Rebellion Pack, former candidate for Congress
[00:02:12.240 --> 00:02:21.080]   in Massachusetts, game developer, Porsche, rehabilitator, speed runner. Did I get a role?
[00:02:21.080 --> 00:02:26.600]   Literally, I was literally buying a Porsche part from my vintage 911 while you were introducing
[00:02:26.600 --> 00:02:29.080]   the show. So yes, that is absolutely accurate.
[00:02:29.080 --> 00:02:38.160]   And I saw you wanted to buy some old non-running. It was like an auction for a $20 call.
[00:02:38.160 --> 00:02:44.920]   Yes, no, no, no. It's a Lotus Elise. I've always wanted a Lotus Elise. I don't know
[00:02:44.920 --> 00:02:50.080]   if you know this car, but when they brought to the United States, they put Toyota engines
[00:02:50.080 --> 00:02:55.080]   in it. And later it was the model for the Roadster, the Tesla Roadster. 100%. It's
[00:02:55.080 --> 00:03:00.400]   great car. It's super light. When you're in it, there's literally nothing but you and
[00:03:00.400 --> 00:03:06.400]   a pedal and a shifter. There's nothing in the interior, but it's just magical to drive.
[00:03:06.400 --> 00:03:11.240]   There's one I'm looking at auction for. And because the car market is so soft right now,
[00:03:11.240 --> 00:03:14.400]   if I can get a good deal on it, I think I'm going to go buy it.
[00:03:14.400 --> 00:03:19.280]   You said it was like $35, but it's not. I presume not running.
[00:03:19.280 --> 00:03:23.400]   No, no, no. It's so it's an auction that keeps going up.
[00:03:23.400 --> 00:03:27.320]   Oh, it was like the Twitter furniture. I get it. Exactly. Yeah.
[00:03:27.320 --> 00:03:31.360]   Exactly. So if I can get around $40,000, I'm going to jump on it because I can make
[00:03:31.360 --> 00:03:34.560]   money fixing that up and selling it here in New England.
[00:03:34.560 --> 00:03:40.600]   I thought I was so excited because I thought I could buy one of those soundproof conference
[00:03:40.600 --> 00:03:45.600]   room booths from the Twitter auction because then we could do like I could put that in
[00:03:45.600 --> 00:03:48.000]   my living room and we could do shows in there.
[00:03:48.000 --> 00:03:51.080]   It's a great idea. Oh, yeah. But it ended up like it was like the
[00:03:51.080 --> 00:03:56.000]   was get smart cone of silence. You go in this room and it's dead silent. Anyway, it went
[00:03:56.000 --> 00:03:59.000]   for like 10,000. I could. Oh, my goodness.
[00:03:59.000 --> 00:04:05.600]   Let's go to Finland quickly and say hello to Patrick Bejaw of Laurent. Hey, so nice to
[00:04:05.600 --> 00:04:09.280]   be here. You are out of hibernation. You have two small
[00:04:09.280 --> 00:04:13.560]   children. They're now old enough for you to do a show.
[00:04:13.560 --> 00:04:20.480]   I guess almost almost I'm eliciting the help of my wonderful wife who will let me sleep
[00:04:20.480 --> 00:04:24.560]   tomorrow because it's the middle of the middle of the night here.
[00:04:24.560 --> 00:04:30.680]   But yeah, my daughter is almost too. Wow. So congratulations, dad.
[00:04:30.680 --> 00:04:34.320]   It's happening. It's happening to you. Soon life can start again.
[00:04:34.320 --> 00:04:38.800]   Yeah. And I was telling you, I have a 30 year old and she's just as much work as ever.
[00:04:38.800 --> 00:04:42.600]   So I don't have to change your diapers. So that's the good news.
[00:04:42.600 --> 00:04:46.480]   Welcome, Patrick. It's great to see you. There is a common theme. If you look at all three
[00:04:46.480 --> 00:04:49.920]   people's backgrounds, you'll see the common theme in a moment. Let's welcome Senator
[00:04:49.920 --> 00:04:55.800]   Shoshana. Shoshana Weisman had a digital media at rstreet.org.
[00:04:55.800 --> 00:05:02.160]   She is being attacked by a giant hot dog. We're friends. It's okay.
[00:05:02.160 --> 00:05:06.320]   You give full consent. Oh, yeah. Yeah. It's fine. Like it's cat and
[00:05:06.320 --> 00:05:11.640]   mouse. It's kind of cute, you know. So you have a giant hot dog behind you.
[00:05:11.640 --> 00:05:16.120]   Used to be SpongeBob. I guess you've moved out from your home under the sea.
[00:05:16.120 --> 00:05:20.760]   Yeah, I live above ground now. It's a little bit easier on the lungs, you know.
[00:05:20.760 --> 00:05:28.360]   Yeah, sloth committee chair. And by the way, professional hiker, your pictures from Chile,
[00:05:28.360 --> 00:05:33.120]   a mountaineer, I guess is what you'd call it. We're gorgeous, really beautiful stuff.
[00:05:33.120 --> 00:05:40.640]   Oh, thank you. I can't take any credit. Everything there was so beautiful. I was just like overwhelmed
[00:05:40.640 --> 00:05:46.000]   by it. And I really miss it. Really nice. And then Patrick has over his left shoulder
[00:05:46.000 --> 00:05:52.720]   a right shoulder. He's got Mario over his left shoulder. Yeah. And of course, we have
[00:05:52.720 --> 00:05:57.720]   the queen of speedrunning Mario, Brianna Wu, right? What is your current speedrunning
[00:05:57.720 --> 00:06:04.280]   record? Oh, my goodness. So I actually, one of my dreams is not to be in the Super Bowl.
[00:06:04.280 --> 00:06:08.960]   It's to be in the speedrunning equivalent of the Super Bowl, which is called Game Stung
[00:06:08.960 --> 00:06:16.960]   Quick. And I got my Princess Peach speedrun time down enough that I am actually going
[00:06:16.960 --> 00:06:23.400]   to be in GDQ, which is a huge honor. I know it sounds so stupid to normal people, but
[00:06:23.400 --> 00:06:27.960]   this is, it's, I'm trying to think of an equivalence like an Academy Award. It's like,
[00:06:27.960 --> 00:06:33.160]   you know, it's like winning the World Series. And I'm going to be in that next month. I'm
[00:06:33.160 --> 00:06:37.920]   so psyched about this. Is it a head to head speedrunning? Like you know, so you have to
[00:06:37.920 --> 00:06:44.160]   show off live and try to break a world record live. And it's a charity, which is awesome.
[00:06:44.160 --> 00:06:50.360]   100%. But you've got to understand, you've got to sit there and speedrun a game with
[00:06:50.360 --> 00:06:55.800]   no mistakes where like the difference in a successful run and a failed run, it's like
[00:06:55.800 --> 00:07:03.000]   literally one 30th of a second in parts of it and not screw up and die. So like the
[00:07:03.000 --> 00:07:07.920]   tension is very high. I can't. I don't know why you do that to yourself, but okay. I'm
[00:07:07.920 --> 00:07:14.720]   a pseudo masochist. Is the event you're going to be in the Frost Fatal? That is it. It's
[00:07:14.720 --> 00:07:21.280]   an all woman speedrunning event Frost Fatal end of next month through March. Cannot wait.
[00:07:21.280 --> 00:07:28.080]   And this will be for Malala, which is great. Supporting the Malala dot org. I love that.
[00:07:28.080 --> 00:07:34.240]   Oh, I'll be watching. Will that, will that make it more tense or less tense? No, I could
[00:07:34.240 --> 00:07:41.320]   use all the good wishes I can get for this. So please do go. So are you control me in
[00:07:41.320 --> 00:07:49.040]   the chat with people? I will. I will say I knew Brianna before she was a speedrunner.
[00:07:49.040 --> 00:07:53.680]   So you're still on Twitter, Brianna. I know you're pretty active still. Sure. I know
[00:07:53.680 --> 00:07:59.600]   you're still pretty active on Twitter. Patrick, are you still on the on the tweet or butter?
[00:07:59.600 --> 00:08:06.520]   I am. I wish there was an alternative, but and please no one say Mastodon. I mean, yes,
[00:08:06.520 --> 00:08:13.000]   I'm on Mastodon as well. It's really great. But I don't think I was going to say unless
[00:08:13.000 --> 00:08:19.400]   he messes up Colossaly, which you already have. It's kind of amazing how Elon has found ways
[00:08:19.400 --> 00:08:25.720]   to continue to mess up. Like I thought it was going to be calm for a while. And then
[00:08:25.720 --> 00:08:34.280]   then last week, a week ago Thursday, he pulled the plug on third party apps. He killed the
[00:08:34.280 --> 00:08:42.280]   API without telling anybody and without even admitting to it. And then a few days later,
[00:08:42.280 --> 00:08:46.600]   well, the information had a story saying we have the slack messages. It was intentional.
[00:08:46.600 --> 00:08:50.300]   A few days later, they said, yeah, yeah, yeah, because these guys, Twitter,
[00:08:50.300 --> 00:08:56.360]   Rifik and tweet bot and Tuss, they were breaking the rules to which, you know, Craig
[00:08:56.360 --> 00:09:02.440]   Hockenberry and Paul Hoddard said, what we've been doing this for 15 years. What rules?
[00:09:02.440 --> 00:09:06.920]   So which Twitter's even worse. Twitter's response last Thursday was to make up some
[00:09:06.920 --> 00:09:14.000]   rules, stick them in and say those rules. They didn't even specify any rules. No, they
[00:09:14.000 --> 00:09:19.840]   were like first after like, I don't know, four days or a week, they were they said, yeah,
[00:09:19.840 --> 00:09:26.040]   we made some changes so some clients might not work. And then a week after that, they
[00:09:26.040 --> 00:09:36.600]   updated the not the ULA, but the API rules, the developer, the API rules saying, yeah,
[00:09:36.600 --> 00:09:43.160]   we are upholding some longstanding, oh, sorry, it was the other way around. We were uploading
[00:09:43.160 --> 00:09:48.720]   some longstanding rules. So there you go. Without specifying anything, anything like
[00:09:48.720 --> 00:09:55.560]   they didn't imagine being a developer whose life it is to job, it is to develop your app
[00:09:55.560 --> 00:10:04.920]   for 10 years. And all of a sudden it doesn't work. And for days, no one says anything. And
[00:10:04.920 --> 00:10:09.760]   even when they end up saying something, they don't say anything. It is I've, you know,
[00:10:09.760 --> 00:10:15.280]   I've had my issues with Twitter and Elon Musk and the way he's been running it. Like, you
[00:10:15.280 --> 00:10:22.160]   know, giving Nazis back their voice was not great, for example. But I think this is even
[00:10:22.160 --> 00:10:29.040]   worse if that's possible. It got me so infuriated. Is it really I mean, all he's doing is saying
[00:10:29.040 --> 00:10:37.880]   we want you to use our client because that's where our advertising is. And so I think that's
[00:10:37.880 --> 00:10:44.760]   far as it's the blocking the third party apps is not the problem. It's the way of doing it
[00:10:44.760 --> 00:10:51.120]   that is so disrespectful. Well, we're we're have been around since the beginning, Sean
[00:10:51.120 --> 00:10:56.440]   Heber. We had Craig Hockenberg of the icon factory on Techno's weekly great interview.
[00:10:56.440 --> 00:11:00.360]   You can see that on us. The tech guys are on the Techno's weekly. But Sean Heber wrote
[00:11:00.360 --> 00:11:06.720]   the the the end of an era blog post for icon factory and pointed out it's been Twitter-ific
[00:11:06.720 --> 00:11:11.920]   has been part of Twitter since 2007. They invented the blue bird that Twitter then started
[00:11:11.920 --> 00:11:19.600]   using. They invented the word tweet. They won many Apple design awards. This was, you
[00:11:19.600 --> 00:11:28.120]   know, the icon factories bread and butter. Paul Hadad of a tweet bot wrote a very sad
[00:11:28.120 --> 00:11:31.800]   of post. Here's the graveyard. And by the way, notice the elephant. That's because they're
[00:11:31.800 --> 00:11:37.280]   writing a mastodon client called ivory to replace that. But but all of these small companies
[00:11:37.280 --> 00:11:42.160]   have said, please, we've pulled them from the abstract. Do not ask for a refund. We can't
[00:11:42.160 --> 00:11:49.520]   afford it. They could put these companies in a business. Yeah, I do just want to if
[00:11:49.520 --> 00:11:54.200]   I could just say I do just want to say I don't think this is as bad as Elon.
[00:11:54.200 --> 00:12:00.720]   Putting Nancy's on. Yeah, I'm I'm I'm I'm back. Well, in all of that back on there, I think
[00:12:00.720 --> 00:12:06.720]   we need to put that in perspective here. You know, I want to be honest and say, Leo, there's
[00:12:06.720 --> 00:12:13.000]   not many things I am more sympathetic to Elon on. These these apps have always been in kind
[00:12:13.000 --> 00:12:19.120]   of uncomfortable water with Twitter because of the reason you just outlined like Twitter
[00:12:19.120 --> 00:12:25.120]   is there ultimately to sell ads. It screws out their ability to bring very specific products
[00:12:25.120 --> 00:12:29.720]   out to the to the widest audience possible. Yeah, they have this API and they do have a
[00:12:29.720 --> 00:12:36.080]   long working relationship. But you know, Twitter long before Elon Musk bought Twitter has been
[00:12:36.080 --> 00:12:42.040]   really encouraging people to use this out every single way they can't. A lot of the newer
[00:12:42.040 --> 00:12:47.000]   features like you know, Twitter spaces, you can't access that from a browser. You need
[00:12:47.000 --> 00:12:53.560]   it from a smartphone to do that. And it's a it's a great feature. So, you know, it's
[00:12:53.560 --> 00:12:58.560]   not to say I agree with the way Elon has done this. I don't think it's terrible. I think
[00:12:58.560 --> 00:13:05.000]   it's a terrible human being. I want records saying that. But I think it's fair to say
[00:13:05.000 --> 00:13:09.000]   that these developers should have known this this writing has been on the wall for quite
[00:13:09.000 --> 00:13:14.960]   a while. It's actually not the first time it happened. Remember, Bill Gross was starting
[00:13:14.960 --> 00:13:20.080]   to buy up all the third party apps and Jack Dorsey cut off a lot of third party access
[00:13:20.080 --> 00:13:26.000]   ended up buying tweet deck to keep it out of his hands. So although Dorsey, as I seem to
[00:13:26.000 --> 00:13:31.640]   remember, has said that was a big mistake to cut off the party apps because as much as
[00:13:31.640 --> 00:13:36.840]   they ride upon Twitter, they also contribute to Twitter and they add to the Twitter user
[00:13:36.840 --> 00:13:41.240]   base. Shoshana, do you probably just use the website, I would guess, like most people?
[00:13:41.240 --> 00:13:46.680]   It's a big mix. It's a lot of tweet deck. A lot, you know, I use it in all different kinds
[00:13:46.680 --> 00:13:50.960]   of ways. But I know that just people like different interfaces and stuff. So I think
[00:13:50.960 --> 00:13:55.040]   it is kind of short sighted to I mean, you know, his behavior, the way he went about it
[00:13:55.040 --> 00:13:59.400]   is bad. I think we all kind of agree there that that's just not how you treat other people
[00:13:59.400 --> 00:14:03.480]   in business, especially after he's been throwing other people under the bus and just creating
[00:14:03.480 --> 00:14:09.680]   all these waves for no reason in certain cases. But yeah, like I plug lots of apps into other
[00:14:09.680 --> 00:14:14.680]   apps using Zapier and using a Cheggra mat. I love those. They're so good. And I've had
[00:14:14.680 --> 00:14:20.480]   some wild ones like 50 step automations, just sometimes you want to use apps differently
[00:14:20.480 --> 00:14:25.480]   and you want to find better ways to connect them or make them work for you. And when people
[00:14:25.480 --> 00:14:29.160]   are kind of getting fed up with your app generally through other decisions, whether
[00:14:29.160 --> 00:14:33.440]   or not even that's, you know, it's legitimate for them to be upset about it. You don't keep
[00:14:33.440 --> 00:14:37.240]   making waves. You know, it's one thing if you wanted them to come to the table and be
[00:14:37.240 --> 00:14:41.000]   like, listen, we have issues here, we need to work through this. Let's try to make this
[00:14:41.000 --> 00:14:45.160]   work. But just pulling the plug for no reason, while other people are upset about the way
[00:14:45.160 --> 00:14:50.240]   he's behaving and the way he's running things. I just don't think it's smart business sense,
[00:14:50.240 --> 00:14:54.480]   which is like, I think everyone thinks just every random rich guy is going to be able
[00:14:54.480 --> 00:14:59.560]   to do amazing things if he touches any kind of company. But people have different skills
[00:14:59.560 --> 00:15:04.760]   and maybe this just isn't his skill set, which is okay. But like for his own sake and all
[00:15:04.760 --> 00:15:08.760]   the money he's spent on it, you would think he would just be like, well, maybe I don't
[00:15:08.760 --> 00:15:12.920]   want to lose like $44 billion. Like let me take a step back and like figure that out.
[00:15:12.920 --> 00:15:17.400]   But it's just kind of wild to me that he hasn't done that. And he just keeps making waves
[00:15:17.400 --> 00:15:22.840]   and like, you know, going after people he's worked with or who were at the company and
[00:15:22.840 --> 00:15:28.320]   firing people and all of this. It's just kind of a big mess for no reason that doesn't
[00:15:28.320 --> 00:15:30.880]   benefit him in any way that I can see.
[00:15:30.880 --> 00:15:34.560]   I kind of, you know, it's funny, Brian, I would say I didn't agree with you except I
[00:15:34.560 --> 00:15:39.720]   kind of do agree with you. In this regard, I mean, it is a privately held, I mean, really
[00:15:39.720 --> 00:15:45.640]   privately held company. One guy owns a whole thing. He can do anything he wants with it.
[00:15:45.640 --> 00:15:51.520]   I'm a fan of sorry, Patrick Mastodon, an open platform is the Fediverse and open platforms
[00:15:51.520 --> 00:15:56.840]   for that very reason that no one owns it. And it's kind of there's an API and it's,
[00:15:56.840 --> 00:16:01.560]   you know, governed by its users. And if somebody decides to do something bad, they can be kind
[00:16:01.560 --> 00:16:08.640]   of, you know, routed around. And Twitter's not that it's a centralized proprietary system.
[00:16:08.640 --> 00:16:12.960]   So on the one hand, I agree with you, he's doing, he's doing what's best for the business
[00:16:12.960 --> 00:16:19.440]   in this, in this one respect, at least short term, because he's preserving the ad base,
[00:16:19.440 --> 00:16:24.720]   although apparently 40% of the ad revenues disappeared out the window in the last couple
[00:16:24.720 --> 00:16:31.240]   of months. There was a very good long piece by Zoe Schiff or Casey Newton and Alex Heath.
[00:16:31.240 --> 00:16:37.680]   Of course, they're the platformer, a sub stack, but they, I guess the Verge has rights to it.
[00:16:37.680 --> 00:16:44.160]   Very good kind of summary of the whole sorted story that came out a couple of days ago,
[00:16:44.160 --> 00:16:51.640]   called extremely hard core next to it, by the way, this is a little mean, a little mean.
[00:16:51.640 --> 00:16:57.880]   They have a Elon net wortho meter. And as you scroll through the story and the events starting
[00:16:57.880 --> 00:17:04.320]   from the very beginning when Elon in April acquired 9.2% stake in Twitter, his net worth
[00:17:04.320 --> 00:17:09.720]   0.0 meter thermometer goes down and down and down. There's also some great illustrations.
[00:17:09.720 --> 00:17:14.880]   The production on this is great, but it is the first place I've seen every step of the
[00:17:14.880 --> 00:17:21.480]   way, what happened. Also, a lot of contributions from people who were on staff, who were later
[00:17:21.480 --> 00:17:28.280]   fired or quit, who give us an inside look at what was going on. I love the illustrations.
[00:17:28.280 --> 00:17:35.280]   This is everybody trying to print out their 30, 30 days worth of code contributions. Apparently,
[00:17:35.280 --> 00:17:39.200]   they hadn't printed anything. It's so long. Hold on a sec. Apparently they hadn't printed
[00:17:39.200 --> 00:17:45.400]   anything in so long that none of the printers worked. So they were busily trying to figure
[00:17:45.400 --> 00:17:49.680]   out how to get the printers to work. Eventually a group of executive assistants said, we'll
[00:17:49.680 --> 00:17:55.760]   print the code, just send us a PDF because those printers of course worked. And then
[00:17:55.760 --> 00:18:00.720]   a new missive went out saying, I'll forget it. Just come with come be ready to show your
[00:18:00.720 --> 00:18:06.160]   code on the screen. And it says, if you've already printed it, please shred in the bins
[00:18:06.160 --> 00:18:10.880]   on San Francisco 10. Thank you. So we saw there was a lot of detail in this. Go ahead,
[00:18:10.880 --> 00:18:14.040]   Brianna. I know you. I know you. You want to qualify.
[00:18:14.040 --> 00:18:17.720]   I was just going to say I just don't I want to be very clear with people because I don't
[00:18:17.720 --> 00:18:22.720]   want to get clipped or taken out of context. I'm not saying what Elon did is right or
[00:18:22.720 --> 00:18:26.440]   I'm defending this at all. I'm saying you shouldn't be surprised anyway. If you look
[00:18:26.440 --> 00:18:32.000]   at X dot com, this is the guy that brought banking services online in the 90s in a way
[00:18:32.000 --> 00:18:37.640]   that allowed anyone to access any bank account and withdraw money with no safeguards whatsoever.
[00:18:37.640 --> 00:18:41.760]   He just threw it together in a very haphazard way. If you look at the way Tesla was brought
[00:18:41.760 --> 00:18:47.920]   to market, there are a thousand stories of that starship nearly crashing and exploding.
[00:18:47.920 --> 00:18:52.400]   You look at the way you treated the people that designed the model wide doors that failed
[00:18:52.400 --> 00:18:57.920]   originally. I think it was very, very poor treatment of it. I had a model X with Falcon
[00:18:57.920 --> 00:19:03.200]   wing doors that after the rain, when you open the door, a flood would come down. A sheet
[00:19:03.200 --> 00:19:08.320]   of water would come down from the inside part of the door on anybody sitting up in the seat
[00:19:08.320 --> 00:19:11.760]   because they never thought of putting, I don't know, rain gutters on them.
[00:19:11.760 --> 00:19:16.560]   There was a huge lawsuit about this with the basically the ownership of who who engineered
[00:19:16.560 --> 00:19:23.360]   it. My point is here is not then defending what he did. It should not be a surprise to
[00:19:23.360 --> 00:19:28.400]   the developers or anyone else that Elon would do this. He's not your friend. He's not some
[00:19:28.400 --> 00:19:34.160]   genius billionaire. He's out to make money. He has a political agenda here. He doesn't
[00:19:34.160 --> 00:19:38.160]   care who's going to get hurt along the way, whether it's employees or his partner.
[00:19:38.160 --> 00:19:46.000]   That's the tension. He owns it. If he wants to drive it into the ground and lose $44 billion,
[00:19:46.000 --> 00:19:53.200]   he still has $132 billion. He can lose it and not go hungry. It's his right to do that. On the
[00:19:53.200 --> 00:20:00.560]   one hand, on the other hand, Twitter did have a and maybe still has this great import for us as a
[00:20:00.560 --> 00:20:07.360]   society, for the globe, to give a voice to people who were voiceless, whether it was the Arab Spring
[00:20:07.360 --> 00:20:14.400]   or Occupy Wall Street or Black Twitter. Many people got their news from it. Many people built
[00:20:14.400 --> 00:20:19.360]   communities around it. A lot I hear from a lot of people who said, "Yeah, like Brianna, I probably
[00:20:19.360 --> 00:20:25.440]   know Brianna because of Twitter, probably many of you because of Twitter first, certainly Shoshana
[00:20:25.440 --> 00:20:32.880]   because of Twitter." It's a public thing that we've all lost. This is to me the whole problem
[00:20:32.880 --> 00:20:38.000]   with proprietary. Somebody could buy it, somebody with enough money could buy it and destroy it.
[00:20:38.000 --> 00:20:47.280]   That's his right. About the proprietary thing, I really, Macedon is great. I have an account. Follow
[00:20:47.280 --> 00:20:52.720]   me, not Patrick, at Macedon. It's not social. I love it. I think it is absolutely necessary.
[00:20:52.720 --> 00:20:57.840]   I just don't think it is going to replace Twitter at any time soon. It's a different experience.
[00:20:57.840 --> 00:21:05.440]   I agree. I agree. Well, it is. Twitter is still here. Twitter is going to stay here for a long
[00:21:05.440 --> 00:21:11.840]   time, I think. This is what I was trying to say because it provides the service it provides.
[00:21:11.840 --> 00:21:18.000]   Until it's the incumbent advantage, even with everything that's happened, we're still on it.
[00:21:18.000 --> 00:21:24.880]   We're still using it. You're implying that it's in the past, the past Leo for all the things that we
[00:21:24.880 --> 00:21:32.720]   did and how we met up with each other through Twitter is still here. I don't think it's a lot
[00:21:32.720 --> 00:21:39.120]   more is going to have to go wrong, which a lot has already gone wrong before people actually
[00:21:39.120 --> 00:21:46.720]   stop using it. It is the incumbent advantage, which is enormous for Elon Musk. That's the thing
[00:21:46.720 --> 00:21:50.800]   I find interesting. It is in the past for me. It's in the complete rearview mirror for me. I don't
[00:21:50.800 --> 00:21:56.880]   use it anymore. Same thing with Facebook. It's in the rearview mirror. Maybe that's why I am in
[00:21:56.880 --> 00:22:02.400]   that mindset already, but you're absolutely right. I've asked this for the last month of
[00:22:02.400 --> 00:22:08.320]   every panelist. They're all still like you. Very active on Twitter. They're still using it.
[00:22:08.320 --> 00:22:13.360]   And to me, I almost want to say, aren't you feel like that supporting Elon in a way?
[00:22:13.360 --> 00:22:21.200]   Of course, but we live in a society. I vote with my life. I vote with my
[00:22:21.200 --> 00:22:25.200]   links. I vote with my eyeballs. I vote with my money. I don't want to support. Maybe you can.
[00:22:25.200 --> 00:22:29.280]   I had a lot of people who can't. I had a blue check and I paid for it. Well, I didn't pay for
[00:22:29.280 --> 00:22:33.760]   the blue check, but I had Twitter blue, but I stopped immediately as soon as Elon took over.
[00:22:33.760 --> 00:22:36.240]   He would have stopped me anyway. Apparently he disconnected everybody.
[00:22:36.240 --> 00:22:42.800]   It's an important means of communication for a lot of people that can't see or that have
[00:22:42.800 --> 00:22:45.520]   integrated into the business. And this is the thing. You're thinking of it as a public wheel,
[00:22:45.520 --> 00:22:52.160]   as a public square, and it should transcend Elon. Oh, I don't think it should transcend
[00:22:52.160 --> 00:22:56.800]   anything. I'm just a realist. That's how it is. And I agree with Rihanna and you, he can do
[00:22:56.800 --> 00:23:03.360]   whatever the hell he wants with this platform. I think he should be a little bit more discerning
[00:23:03.360 --> 00:23:07.040]   and respectful of the devs that have been working with the platform for a long time.
[00:23:07.040 --> 00:23:13.120]   That's just like it's not even it's like ethics and morals, but he has the right to do anything
[00:23:13.120 --> 00:23:20.480]   he wants with it. But I think the platform is unavoidable if you're in a certain type of business.
[00:23:21.280 --> 00:23:28.240]   It's funny because I tell my marketing team and our execs, get off Twitter. I don't think we
[00:23:28.240 --> 00:23:31.680]   should support it. Twitch should not have anything to do with Twitter. I've thought that for a long
[00:23:31.680 --> 00:23:38.320]   time. And they say the same thing. No, we have to be. That's where the audience is. Brianna,
[00:23:38.320 --> 00:23:44.640]   what do you worry about supporting? You're actually giving Elon comfort aid to the enemy?
[00:23:45.520 --> 00:23:52.160]   There is an article in Forbes that came out literally about five days ago. It's literally just my
[00:23:52.160 --> 00:23:59.200]   public tweets going after Elon that have gone viral. I see myself as conducting good old school
[00:23:59.200 --> 00:24:04.640]   information. It's a good platform to attack him. Yeah. 100%. Has he banned you or has he blocked
[00:24:04.640 --> 00:24:09.360]   you or has he got anything to say? Amazingly, he has it. We've gotten in a fight or two. But
[00:24:09.360 --> 00:24:15.200]   a lot of people attribute that, "Hey, should I step down?" Tweet to something I said to him.
[00:24:15.200 --> 00:24:20.080]   So yeah, I'm there to... By the way, he had a poll, lost the poll and did nothing.
[00:24:20.080 --> 00:24:26.240]   Yeah, exactly. I'm there to undermine him. I'm going to tell you candidly, I'm looking at a
[00:24:26.240 --> 00:24:34.080]   book deal right now to write a book, basically just going after him. And that's why I'm there.
[00:24:34.960 --> 00:24:39.040]   I'm there to fight dirty after Mississippi. I got no problem getting into the book. Let me give you
[00:24:39.040 --> 00:24:46.800]   a little ammunition from this article by Casey and Zoey. On October 26th, an engineer mother of two,
[00:24:46.800 --> 00:24:53.040]   let's call her Alicia. They had more than two dozen, I think, Twitter employees sourcing in this,
[00:24:53.040 --> 00:24:56.960]   former Twitter employees sourcing this article. Sat in a glass conference room in San Francisco,
[00:24:56.960 --> 00:25:03.120]   trying to explain the details of Twitter's tech stack to Elon. This was two days before he
[00:25:03.120 --> 00:25:08.000]   actually bought the company. But Musk was sitting two seats away with his elbows propped and the
[00:25:08.000 --> 00:25:12.960]   table looked sleepy. When he did talk, he was to ask questions about cost. How much does Twitter
[00:25:12.960 --> 00:25:19.600]   spend on data centers? Why was everything so expensive? Alicia was already tired of Musk's
[00:25:19.600 --> 00:25:22.400]   antics for months. He'd gone back and forth about buying the company. You remember that?
[00:25:22.400 --> 00:25:28.240]   He decided to back out of the deal. Blah, blah, blah. So here they were trying to show Musk what he
[00:25:28.240 --> 00:25:32.640]   was about to buy and all he wanted to talk about was money. Fine, she thought. If Musk wants to know
[00:25:32.640 --> 00:25:36.960]   about money, I'll tell him she launched into a technical explanation of the company's data center
[00:25:36.960 --> 00:25:42.560]   efficiency. Curious to see if he'd following along. And here's the anecdote. Instead, he interrupted,
[00:25:42.560 --> 00:25:48.400]   "I was writing C programs in the 90s," he said dismissively. "I understand how computers work."
[00:25:48.400 --> 00:25:55.520]   So you know, I was writing C programs in the 90s too, but I don't think I'd understand Twitter's
[00:25:55.520 --> 00:26:02.640]   text tag. I can tell you right now. He was dismissive. Even worse in this article
[00:26:02.640 --> 00:26:13.280]   is his friend David Sacks. But also how Elon tweeted Sacks. Sacks was a venture capitalist,
[00:26:13.280 --> 00:26:18.320]   a fellow South African. He'd worked with Musk at PayPal. He started Yammer,
[00:26:18.880 --> 00:26:25.920]   sold that to Microsoft, so you know, successful. During this conversation with Alicia,
[00:26:25.920 --> 00:26:30.960]   Sacks walks in and Musk says, "David, this meeting's too technical for you.
[00:26:30.960 --> 00:26:35.280]   Waving his hand to dismiss Sacks, wordlessly Sacks turned and walked out,
[00:26:35.280 --> 00:26:41.920]   leaving the engineers slack-jawed. Musk's imperiousness in the middle of a session he
[00:26:41.920 --> 00:26:48.480]   appears to be botching with something to behold." There's no question he's...
[00:26:48.480 --> 00:26:53.840]   How much can we curse here? You could say he's an A-hole.
[00:26:53.840 --> 00:26:58.720]   Right. There's no question about that. And I have to say,
[00:26:58.720 --> 00:27:06.320]   a little bit irritated when people take pleasure in continuing to point it out,
[00:27:06.320 --> 00:27:09.760]   because we know that. It's no new news. You're right. You're right.
[00:27:11.120 --> 00:27:17.840]   He is a complete... He's full of himself. He's probably... He doesn't... But since we're going there,
[00:27:17.840 --> 00:27:27.680]   please, I have this image that keeps coming back to my head of other A-holes, like, for example,
[00:27:27.680 --> 00:27:32.320]   Steve Jobs, Thomas Edison, Henry Ford, for example. All notorious...
[00:27:32.320 --> 00:27:38.160]   There you go. If I'm going to play a little bit of Devil's Advocate here,
[00:27:38.160 --> 00:27:47.120]   which is a thing I do much to my own chagrin, isn't there a... SpaceX and Tesla have done things?
[00:27:47.120 --> 00:27:52.160]   Is there any chance that we're focusing on the A-hole-enex,
[00:27:52.160 --> 00:27:57.120]   of the Elon Musk? Well, it makes for good copies, so that's why. But you're right.
[00:27:57.120 --> 00:28:01.120]   You're right. I guess then that leads to the question with her Twitter.
[00:28:05.840 --> 00:28:11.280]   Anybody want to make a prediction? A year from today. Elon, by the way, has a very...
[00:28:11.280 --> 00:28:14.800]   Well, he certainly doesn't seem to be having any strokes of genius sometimes.
[00:28:14.800 --> 00:28:18.000]   He's got a big interest payment coming up this month.
[00:28:18.000 --> 00:28:20.160]   I think he's got a big interest payment coming up this month.
[00:28:20.160 --> 00:28:23.520]   I mean, that's not the end of January. Yeah.
[00:28:23.520 --> 00:28:30.240]   Maybe because I'm just such an optimist, I do think it'll make it. And I kind of do like the
[00:28:30.240 --> 00:28:35.520]   point too of how there's lots of A-holes who do cool things too. I think it's...
[00:28:35.520 --> 00:28:38.320]   I go back to... It's almost a prerequisite, isn't it?
[00:28:38.320 --> 00:28:45.120]   Yeah. I go to Jonathan Heitz's work a lot because I'm definitely guilty of being... Oh, he's great.
[00:28:45.120 --> 00:28:48.960]   I love him. And I'm very guilty of being a black and white thinker. I'm like, "Oh, I just
[00:28:48.960 --> 00:28:52.880]   have to write this person off." Or, "Oh, this person is great and there's no flaws." And
[00:28:52.880 --> 00:28:58.080]   I've gotten over that over time in part because of his work. And I think that it's possible Elon
[00:28:58.080 --> 00:29:03.360]   fits into the dynamic where he has some very, very, very toxic things but can also do some very
[00:29:03.360 --> 00:29:09.920]   incredible things. And one of my favorite examples from his book, which seems kind of obvious, I guess,
[00:29:09.920 --> 00:29:16.480]   is Bill Clinton that he would make such a silly and awful but kind of... There was no reason he had
[00:29:16.480 --> 00:29:21.760]   to do what he did with an intern, a two-in intern really. But there was no real reason for that.
[00:29:21.760 --> 00:29:27.600]   And he was so brilliant in certain other ways that it's hard to contrast but different parts
[00:29:27.600 --> 00:29:31.840]   of people's minds work very differently. And different parts of their lives work very differently.
[00:29:31.840 --> 00:29:37.520]   So it could just be that sometimes people can be awful in one regard but brilliant in others.
[00:29:37.520 --> 00:29:43.760]   And I'm hoping the brilliance comes through and he delegates more with Twitter and lets it flourish
[00:29:43.760 --> 00:29:49.040]   because I think it's something that can be really great. But if he doesn't, it'll fail. And I guess
[00:29:49.040 --> 00:29:54.400]   maybe this is the thing that'll kind of determine how we end up seeing him, what he's
[00:29:54.400 --> 00:29:59.120]   willing to make of it, what he's willing to turn it into. And if he's willing to get out of his own
[00:29:59.120 --> 00:30:02.560]   way, that'll kind of put him in perspective, I think, for a lot of people.
[00:30:02.560 --> 00:30:07.760]   The book, by the way, Jonathan. I got to do a little pushback on all of this.
[00:30:07.760 --> 00:30:11.200]   Hold on just a sec. I just want to... Because we mentioned Jonathan and I want to
[00:30:11.200 --> 00:30:16.160]   give him the full plug. The book is the righteous mind why good people are divided by politics and
[00:30:16.160 --> 00:30:20.000]   religion. That's not his latest but I did interview him on triangulation when that came out.
[00:30:20.000 --> 00:30:27.360]   His notice is the coddling of the American mind. He's an interesting thinker and often has some
[00:30:27.360 --> 00:30:30.480]   various doot stuff. Go ahead. I'm sorry, Brian. I just want to give you a little... No, no, no.
[00:30:30.480 --> 00:30:37.200]   It's well said. I just want to say, look, you can have differences of opinions. I mean, I ran for
[00:30:37.200 --> 00:30:43.040]   Congress. I work with people. I don't agree with stuff on all the time. It's just the nature of my
[00:30:43.040 --> 00:30:51.920]   job. I would posit to you, Elon Musk is a unique threat. He's uniquely terrible. Steve Jobs was a
[00:30:51.920 --> 00:30:58.800]   jerk. Steve Jobs didn't have literally lines of women that worked at his companies, suing him
[00:30:58.800 --> 00:31:06.560]   over sexual misconduct, and then mocking those women publicly. Steve Jobs did not have lawsuits
[00:31:07.680 --> 00:31:13.680]   like of things so racist on the Tesla assembly lines. I'm not even going to repeat on this show.
[00:31:13.680 --> 00:31:24.000]   Steve Jobs did not give anti-vaxxers a huge megaphone and specifically bring them back to a platform
[00:31:24.000 --> 00:31:31.680]   to broadcast those ideas and legitimize them to over 200 million people. Steve Jobs did not
[00:31:31.680 --> 00:31:37.920]   bring back someone who literally tried to incite an insurrection against the United States
[00:31:37.920 --> 00:31:47.520]   and gave them a huge megaphone. There is a pattern of egregious, toxic, extremely damaging
[00:31:47.520 --> 00:31:54.320]   conduct to our democracy. I would suggest to all of you, this is not just boys being boys or someone
[00:31:54.320 --> 00:32:00.800]   that's a flawed figure. This is someone that is a uniquely toxic person to our discourse,
[00:32:00.800 --> 00:32:06.560]   and we need to be realistic and factual about that. I agree with you, and that's why I left Twitter.
[00:32:06.560 --> 00:32:11.760]   I don't think everyone is going to leave Twitter because you're supporting this by just making
[00:32:11.760 --> 00:32:17.360]   Twitter a valuable place to be. The best thing anybody could do is to abandon it,
[00:32:17.360 --> 00:32:24.080]   abandon ship. I obviously am not leading a charge here. I did the same thing with Facebook years
[00:32:24.080 --> 00:32:31.120]   ago, and I said, "Come on, everybody, let's dump Facebook." Hello? Anybody? Go ahead, Patrick.
[00:32:31.120 --> 00:32:39.680]   No, I don't think anyone is disputing. My joke earlier about him letting natties back on the
[00:32:39.680 --> 00:32:46.160]   platform was a very serious stab about the real issue with the guy.
[00:32:48.240 --> 00:32:55.040]   We still talk about him and says something. I thought he was coming down.
[00:32:55.040 --> 00:32:59.360]   We haven't done much Elon stuff in the last couple of weeks, and then he did this thing to
[00:32:59.360 --> 00:33:06.160]   the third-party apps. We could stop for this episode. We're going to stop right now. We've
[00:33:06.160 --> 00:33:11.680]   exceeded our Elon quota for the week. Just to point out that the first installment of interest
[00:33:11.680 --> 00:33:17.920]   payments will be due at the end of the month. It's going to be about one and a half billion a year.
[00:33:17.920 --> 00:33:23.280]   What is that? At least a hundred million at the end of the month. He can afford it. He could
[00:33:23.280 --> 00:33:31.760]   write a check. He could sell a jet. I think he could keep this going for a long time. He's got a
[00:33:31.760 --> 00:33:38.160]   lot of money. If you were willing to lose money, he's got a lot of money. Now he's got a lot more
[00:33:38.160 --> 00:33:44.160]   because he sold the Twitter @Planter for more than $10,000 in the giant blue Twitter sign. How
[00:33:44.160 --> 00:33:48.960]   much that go for? $100,000. I have it in my apartment now. I was going to say she should
[00:33:48.960 --> 00:33:53.920]   have more space than you would think. Good job. You can snag in that while you go. I saved up
[00:33:53.920 --> 00:34:01.200]   for it. I won't be able to hide for a couple of years. You got it. Who bought it? I think it's
[00:34:01.200 --> 00:34:07.360]   one of the goons they call the people who are backing up. I bet Jason Gallicans has the big
[00:34:07.360 --> 00:34:12.640]   Twitter sign in his house in Hillsborough estate. We're going to take a little break and continue
[00:34:12.640 --> 00:34:17.520]   with a lot more. It's so good to have you. No more Elon. We're going with the Patrick Bejah,
[00:34:17.520 --> 00:34:26.640]   complete and utter. We've hit the quota. The ban is on. Our show today brought to you by Shopify.
[00:34:26.640 --> 00:34:33.360]   I love that sound. That's your sign. This year, forget about those run-of-the-mill resolutions
[00:34:33.360 --> 00:34:39.520]   instead. Start your own near-years resolution. That's the sound to start selling on Shopify.
[00:34:39.520 --> 00:34:45.360]   I know so many people for whom Shopify is transforming their ability to make a living. My own daughter
[00:34:45.360 --> 00:34:52.960]   uses a Shopify store because it's so easy to set up to sell her T-shirts. My son uses Shopify
[00:34:52.960 --> 00:35:00.720]   to sell his salt because it's so easy. It works so well and it's completely global. Shopify is the
[00:35:00.720 --> 00:35:05.680]   commerce platform revolutionizing millions of businesses worldwide. This is why I'm a fan of
[00:35:05.680 --> 00:35:12.400]   Shopify because it's empowering to people who want to start a business. Every minute, new sellers
[00:35:12.400 --> 00:35:19.920]   around the world are revolutionizing their businesses, making that first kaching with Shopify.
[00:35:19.920 --> 00:35:25.040]   Love that. What's incredible to me about Shopify is no matter how big you want to grow,
[00:35:25.040 --> 00:35:30.160]   Shopify is there to empower you with the confidence and control to take your business to the next level.
[00:35:30.640 --> 00:35:37.840]   Whether you're selling succulents or stilettos or flavored salts, Shopify simplifies
[00:35:37.840 --> 00:35:43.440]   selling online and in-person so you can focus on successfully growing your business. That's
[00:35:43.440 --> 00:35:49.920]   exactly what Hank's doing. It's really great to see. It made it so easy for him. He sells out
[00:35:49.920 --> 00:35:56.480]   every time he gets a new batch of salts and Shopify makes it easy so he can focus on what drives the
[00:35:56.480 --> 00:36:04.240]   sales, his TikTok and Instagram videos. That's the point. Shopify covers every sales channel.
[00:36:04.240 --> 00:36:08.320]   You can actually use it. I didn't know this in-person with a point of sales system.
[00:36:08.320 --> 00:36:12.800]   Of course, it's an all-in-one e-commerce platform as well, even lets you sell across
[00:36:12.800 --> 00:36:20.640]   social media marketplaces like TikTok, Facebook, Instagram. It is a true lever.
[00:36:21.600 --> 00:36:27.360]   It's incredible. You can move the world with this thing. Packed with industry leading tools
[00:36:27.360 --> 00:36:32.000]   ready to ignite your growth. Shopify gives you complete control over your business
[00:36:32.000 --> 00:36:36.800]   and your brand without having to learn any new skills and design or code. I was so impressed
[00:36:36.800 --> 00:36:40.320]   when I went to his website. I thought, "That looks really good. How'd you do that?" He said, "Shopify."
[00:36:40.320 --> 00:36:45.360]   Do you want marketing made simple? Shopify removes the gas work with built-in tools that help you
[00:36:45.360 --> 00:36:51.280]   create, execute and even analyze your online marketing campaigns. Henry's really good about that.
[00:36:51.920 --> 00:36:59.920]   He takes the feedback, really uses it to optimize. It works. He has no training in this, by the way,
[00:36:59.920 --> 00:37:05.840]   but it makes it easy. You can customize your online store to your style, connect with new
[00:37:05.840 --> 00:37:10.080]   customers to drive growth, even maintain the relationships that will keep them coming back no
[00:37:10.080 --> 00:37:16.400]   matter how big you want to grow. Shopify grows with your business no matter how far or big you grow.
[00:37:16.400 --> 00:37:22.160]   Thanks to 24/7 help, an extensive business course library, an endless list of integrations and
[00:37:22.160 --> 00:37:27.520]   third-party apps, anything you can think of from on-demand printing to accounting, to chat bots,
[00:37:27.520 --> 00:37:34.720]   Shopify is there to support your success every step of the way. Now, it's your turn. Get serious
[00:37:34.720 --> 00:37:42.400]   about selling. Try Shopify today. This is possibility, powered by Shopify. This is what really excites
[00:37:42.400 --> 00:37:48.000]   me about the internet. It's just incredible. You can sign up for a dollar a month trial period right
[00:37:48.000 --> 00:37:58.240]   now. Shopify.com/twit. S-H-O-P-I-F-Y.com/twit. That's all lowercase. No need to hit the shift key.
[00:37:58.240 --> 00:38:07.360]   Shopify.com/twit. Take your business to the next level today. Shopify.com/twit. Thank you so much
[00:38:07.360 --> 00:38:13.760]   for their support and on behalf of my family. Thank you for getting the kids off of my
[00:38:13.760 --> 00:38:19.760]   checkbook. Thank you, Shopify. All right, big story. Actually, we probably should have led with this.
[00:38:19.760 --> 00:38:26.720]   The Supreme Court is going to hear, I think, a very interesting case. I wish I had been trained as a
[00:38:26.720 --> 00:38:32.400]   lawyer. I know one of the things you do and a lawyer is as a lawyer in your training is you learn
[00:38:32.400 --> 00:38:40.160]   how to state the facts of a case in a coherent, succinct way. Then you can talk about the merits
[00:38:40.160 --> 00:38:46.240]   of it in the pros and cons. I'm going to do my best, but it won't be very good. The case is--
[00:38:46.240 --> 00:38:52.000]   I'm just warning you ahead of time. The case is Gonzales versus Google. The Supreme Court has
[00:38:52.000 --> 00:38:56.400]   agreed to hear it. I'm not sure. The arguments haven't happened yet, but we're starting to see
[00:38:56.400 --> 00:39:06.000]   briefs from all the big tech companies come in. At the root of the case is a family, the Gonzales
[00:39:06.000 --> 00:39:13.440]   family in France whose daughter was killed in 2015. In a Paris bistro, you probably remember this
[00:39:13.440 --> 00:39:23.360]   2015 ISIS terrorist attack. No Amy Gonzales. The reason they're suing Google in this is because
[00:39:24.080 --> 00:39:32.960]   ISIS allegedly relied on YouTube to recruit before the attack. So the family has sued to hold Google
[00:39:32.960 --> 00:39:41.840]   libel for aiding and abetting terrorists. Their contention is that the Google recommendation algorithm
[00:39:41.840 --> 00:39:48.160]   radicalized young people and turned them into terrorists with these ISIS videos.
[00:39:48.160 --> 00:39:52.320]   Google says, of course, well, we take those videos down the minute we find them.
[00:39:53.840 --> 00:40:00.160]   But they're not denying the algorithm. They are saying, we are protected under section 230.
[00:40:00.160 --> 00:40:07.040]   And really, that's why this is a very important case. Section 230 of the Digital Millennium
[00:40:07.040 --> 00:40:12.320]   Copyright Act, DMCA written by Ron Wyden, to protect the internet. It's very simple.
[00:40:12.320 --> 00:40:22.800]   It really is-- it's two rights. One, the right to-- or the not reliable as a company. For instance,
[00:40:22.800 --> 00:40:28.080]   I'll give you the immediate example to me. We have an IRC chat room. We have a Discord chat.
[00:40:28.080 --> 00:40:33.760]   We have forums at Twit. Community. We have a Mastodon, Twit.Social.
[00:40:33.760 --> 00:40:40.560]   I would be liable and I could be sued by anybody for stuff posted there except that
[00:40:40.560 --> 00:40:47.280]   section 230 makes me immune to prosecution and immediately causes the suit to be dropped.
[00:40:47.280 --> 00:40:51.600]   The judges say, no, you're protected. That's part one. Part two of my rights, it also gives
[00:40:51.600 --> 00:40:57.520]   me the right to moderate. This is very important. Without the protection, there's the risk of
[00:40:57.520 --> 00:41:05.040]   taking down, let's say, Donald Trump's account or, as I do often, taking down hate speech in our
[00:41:05.040 --> 00:41:10.560]   Mastodon instance would also open me to liability. That person could sue me for taking that down.
[00:41:10.560 --> 00:41:15.600]   And maybe I'd win in court, but I'd have to defend it in court. Section 230 means I don't have to
[00:41:15.600 --> 00:41:20.480]   defend it. So section 230 is very important to the internet. We've talked about it a lot.
[00:41:20.480 --> 00:41:28.320]   The part of the problem, I think, here is that it is being associated with the big tech giants.
[00:41:28.320 --> 00:41:35.760]   Google, Meta, Twitter, everybody is assigning onto Amicus Brief, friends of the court briefs,
[00:41:35.760 --> 00:41:41.040]   why the Supreme Court should not do this. I love this one. The Supreme Court just
[00:41:41.040 --> 00:41:48.320]   yesterday, I think, allowed, or maybe it was Friday, Reddit mods to anonymously comment on this in
[00:41:48.320 --> 00:41:55.440]   Reddit's Amicus. The reason being Reddit mods use algorithms all the time to moderate their
[00:41:55.440 --> 00:42:02.480]   individual sub-readits. But many of them are anonymous. And so normally in an Amicus Brief,
[00:42:02.480 --> 00:42:07.520]   you sign it. Reddit went to the Supreme Court saying, "Can we have anonymous, because our
[00:42:07.520 --> 00:42:13.280]   moderators want to stay anonymous for a good reason, in many cases? Can we allow their comments?"
[00:42:13.280 --> 00:42:20.720]   And the Supreme Court said, "Yes. Sophia Kope of the EFF said, 'We're happy the Supreme Court
[00:42:20.720 --> 00:42:26.400]   recognized the First Amendment rights of Reddit moderators to speak to the court about their concerns."
[00:42:26.400 --> 00:42:36.240]   So actually, I should mention that our street, which is the lobbying organization that Shoshana,
[00:42:36.240 --> 00:42:40.080]   where are you an employee or a contractor for them? I should get that.
[00:42:40.080 --> 00:42:44.480]   Oh, employee. And we're technically a think tank, because sometimes we just put out ideas,
[00:42:44.480 --> 00:42:48.480]   sometimes we're like, "Hey, please do this, or hey, please don't do this." It varies on the day.
[00:42:48.480 --> 00:42:52.320]   So it's more, and in fact, this comes from Jonathan Cannon, your policy council.
[00:42:52.320 --> 00:42:59.520]   So this is more an Amicus Brief of why Section 230 should be protected.
[00:42:59.520 --> 00:43:04.560]   But you take kind of an interesting point of view.
[00:43:09.920 --> 00:43:19.760]   So petitioners claim that a test has been used by the lower courts to resolve 230 cases,
[00:43:19.760 --> 00:43:24.400]   to say that YouTube's recommendation algorithms are not protected.
[00:43:24.400 --> 00:43:31.440]   And I have to say, "I'm not sure about this, boy. I know I need Section 230, because we would
[00:43:31.440 --> 00:43:36.880]   not have a chatroom, or forums, or mastodon if Section 230 didn't exist, because it would put
[00:43:36.880 --> 00:43:40.160]   me out of business. I'd have to defend frivolous lawsuits right and left."
[00:43:40.160 --> 00:43:47.440]   So I'm in favor 230, but I kind of understand the family's point of view that Google did more
[00:43:47.440 --> 00:43:56.240]   than just, you know, Google's out recommendation, in effect, taking an editorial position on this
[00:43:56.240 --> 00:44:05.360]   stuff, I think maybe should make them liable. And so that's why this initial test,
[00:44:06.000 --> 00:44:15.280]   it's a traditional editorial function test. Now your cannon, your council said both theories are wrong.
[00:44:15.280 --> 00:44:20.720]   Partitionals traditional editorial functions test is unsupported by the text of 230. That's true,
[00:44:20.720 --> 00:44:24.880]   230 is very clean and simple. It's almost like a constitutional amendment. It's very good.
[00:44:24.880 --> 00:44:29.920]   And it's also, he says, "Not consistent with lower court decisions that purportedly make use of it.
[00:44:30.560 --> 00:44:38.080]   The conventional three-pronged barns test, which lower courts typically use to determine whether
[00:44:38.080 --> 00:44:45.120]   Section 230 applies a much better fit." So he's kind of lobbying for this barns test. I don't,
[00:44:45.120 --> 00:44:52.880]   I should probably read up on this. He doesn't describe what the barns test. But he does say,
[00:44:52.880 --> 00:44:59.040]   "Google's algorithmic recommendations satisfy all three prongs, and this are entitled to Section 230.
[00:45:00.000 --> 00:45:04.880]   YouTube's labeling of relevant videos with the words up next does not void 230 protection,
[00:45:04.880 --> 00:45:10.480]   just as a newspaper guiding resource to the remainder of a front page story would not void
[00:45:10.480 --> 00:45:16.720]   that paper's legal protections." I don't know if up next is exactly the same as continued on page 25.
[00:45:16.720 --> 00:45:21.920]   Cannon says so. "Nor does YouTube lose Section 230 protection for arranging its site in a way that
[00:45:21.920 --> 00:45:30.800]   is navigable and relevant for users." That makes sense. A newspaper does not wave otherwise applicable
[00:45:30.800 --> 00:45:35.040]   legal protections for publishing an article when it puts it on the front page. YouTube does not
[00:45:35.040 --> 00:45:39.760]   lose protections for hosting a video when its algorithm makes that video up next. I feel a little
[00:45:39.760 --> 00:45:46.640]   funny about that one. And he says, "Even if recommendations were distinct pieces of content rather than
[00:45:46.640 --> 00:45:52.480]   necessary byproducts of organizing content, these recommendations would be generated by user inputs
[00:45:52.480 --> 00:45:57.920]   subject to neural, neutral algorithmic rules and thus not speech developed by YouTube."
[00:45:57.920 --> 00:46:03.440]   I would contend those are not neutral rules that the rules are in fact to optimize for profit
[00:46:03.440 --> 00:46:08.080]   by optimizing for engagement. And I don't think that is neutral. Finally, he says, "A ruling for
[00:46:08.080 --> 00:46:12.320]   petitioners would lead to dire consequences for online speech." This, I agree with thwarting the
[00:46:12.320 --> 00:46:21.440]   purpose of Section 230. So our street is in favor of keeping 230 intact as are, of course, all the
[00:46:21.440 --> 00:46:27.440]   big tech companies. So I think this is a fascinating subject. I've done a terrible job. I would get a
[00:46:27.440 --> 00:46:34.640]   C minus in law school. But that's my statement of the case, of the facts of the case and my thoughts
[00:46:34.640 --> 00:46:38.640]   about it. Patrick, go ahead. Shashana, you should start because this is your brief.
[00:46:38.640 --> 00:46:43.200]   Sure. So I actually didn't know we were going to talk Section 230 today, but I'm still wearing
[00:46:43.200 --> 00:46:49.600]   my Section 230 necklace. I really like the law. And especially for someone so free market is
[00:46:49.600 --> 00:46:54.960]   I don't often like laws, but I do think it was very well written. In a way, it preserves the free
[00:46:54.960 --> 00:47:04.320]   market, right? Exactly. The free market of ideas can exist. Right, exactly. And also, you had one
[00:47:04.320 --> 00:47:09.280]   error that I'm going to call you out. I used that it was part of the DMCA, and it was part of the
[00:47:09.280 --> 00:47:14.560]   Communications Decent Decent. I'm sorry, CIA. I apologize. You're fine. There's a million acronyms.
[00:47:14.560 --> 00:47:19.920]   Yes, the CIA. That's exactly right. Yeah. But we've done work on the DMCA too, which is why I'm
[00:47:19.920 --> 00:47:24.640]   like, oh, I know what that is. But I think with algorithms, there's a bunch of different things to
[00:47:24.640 --> 00:47:29.120]   consider for how Section 230 applies. And one thing that I really love that you're talking about
[00:47:29.120 --> 00:47:35.360]   is that it applies to all sizes of things that it's not just the bigger tech companies. But Facebook
[00:47:35.360 --> 00:47:40.720]   in the past, or meta technically, has actually lobbied to kind of sideline 230 in certain cases.
[00:47:40.720 --> 00:47:45.280]   So some of them, some of the larger tech companies disagree on like how 230 should work, where it
[00:47:45.280 --> 00:47:50.640]   should apply. For me, I'm genuinely forward for the smaller platforms, because we see the way Twitter
[00:47:50.640 --> 00:47:54.720]   is going like we were talking about before. I want competitors, and I want to make sure that,
[00:47:55.360 --> 00:48:00.480]   there's an incumbent advantage, but 230 makes it so that there's a little bit less of that,
[00:48:00.480 --> 00:48:05.200]   because without 230, these smaller platforms like Mastodon doesn't have a huge team of lawyers.
[00:48:05.200 --> 00:48:10.480]   Nor does Leo sort of, and I don't know if Mastodon would be liable if something bad
[00:48:10.480 --> 00:48:15.040]   were posted on Leo's instance, I think I would be liable, probably Mastodon as well.
[00:48:15.040 --> 00:48:21.280]   So that's part of it, the layers of liability there. And with algorithms in particular,
[00:48:21.280 --> 00:48:26.480]   algorithms are speech. So you can be liable for your own speech, like Twitter's liable for
[00:48:26.480 --> 00:48:33.760]   its own writing and its own content, just not for user content. But as far as how it impacts
[00:48:33.760 --> 00:48:40.720]   an event here, like how an algorithm impacts an event, the inputs weren't optimizing for the
[00:48:40.720 --> 00:48:46.160]   worst of the worst content. It wasn't trying to do that. And also part of it, I guess, is that
[00:48:47.680 --> 00:48:53.360]   people on their own will find the content they want to find. Awful stuff and conspiracy theory
[00:48:53.360 --> 00:49:00.640]   spread them on the internet long before algorithms were in use. And because of that, I think sometimes
[00:49:00.640 --> 00:49:05.520]   people tend to demonize algorithms more than they deserve, not to say that there's not problems with
[00:49:05.520 --> 00:49:10.960]   them, or that there's not ways to improve them or make them safer and make them better. But I think
[00:49:10.960 --> 00:49:15.600]   sometimes people forget that the demon in the algorithm is the person it's optimizing for,
[00:49:15.600 --> 00:49:21.520]   which it could just be a regular user. But in this case, I just don't think the other side has a
[00:49:21.520 --> 00:49:27.520]   really strong case that these items were connected, that it was the specific terrorists, that it was
[00:49:27.520 --> 00:49:34.800]   terrorists broadly, who were radicalized by YouTube. Yeah, I mean, and the court may end up saying,
[00:49:34.800 --> 00:49:38.960]   well, that you don't have standing. This isn't, you know, these aren't people who were involved,
[00:49:38.960 --> 00:49:46.560]   blah, blah, blah. But I does raise a very interesting issue. And I guess my issue is that an algorithm
[00:49:46.560 --> 00:49:54.000]   to optimize profitability through creating engaging, more engaging, or showing more engaging content
[00:49:54.000 --> 00:50:01.680]   tends to move towards extremes. That's I mean, we've seen that. And I worry that that is problematic,
[00:50:01.680 --> 00:50:10.000]   right? And I would hate to, and I would, what I don't want to do is lose 230 protections.
[00:50:10.000 --> 00:50:14.800]   I also understand that algorithms are used in a variety of ways. These Reddit,
[00:50:14.800 --> 00:50:20.160]   Reddit moderators say, look, we use algorithms all the time, not in that way, but we use them all
[00:50:20.160 --> 00:50:27.680]   the time to, to, you know, protect the sub-readits. We don't want to have that tool taken away from
[00:50:27.680 --> 00:50:34.560]   us. I understand that. But it does, I feel like Google's a little liable for surfacing content that is
[00:50:34.560 --> 00:50:38.800]   more profitable because it's more engaging and hence more extreme.
[00:50:38.800 --> 00:50:48.400]   Well, they, it does surface, but I think one key element, oh, I'm bumping into my desk, sorry,
[00:50:48.400 --> 00:50:56.880]   one key element is how good a job are they doing at policing this and removing the
[00:50:56.880 --> 00:51:05.520]   objectionable content. And I don't know how well they were doing it that in 2015 and a few years
[00:51:05.520 --> 00:51:11.600]   before. But I really think from what I know of the platform that nowadays they're doing a pretty
[00:51:11.600 --> 00:51:19.120]   decent job, partly motivated by rules in the EU that states, you know, you have to remove something
[00:51:19.120 --> 00:51:27.920]   within 24 hours of it being, you know, of you being notified. And, and I think that they're,
[00:51:27.920 --> 00:51:33.840]   you're never going to have any form of media or platform because we're talking about the internet,
[00:51:33.840 --> 00:51:41.280]   where user generated content can appear that is going to be completely seen. That is just
[00:51:41.680 --> 00:51:50.880]   impossible. Anyone saying otherwise is, is I think wrong. And if Google is doing an earnest job
[00:51:50.880 --> 00:51:54.960]   at policing the platform, maybe, you know, we could argue that maybe they're not in certain
[00:51:54.960 --> 00:52:02.080]   instances, but I think for things that are related to terrorism and child pornography,
[00:52:02.080 --> 00:52:08.000]   they're probably doing their darnest, they say, and I think they're right. They try to take those
[00:52:08.000 --> 00:52:11.520]   videos down as soon as they finally take them down. I mean, they're, they're doing everything they
[00:52:11.520 --> 00:52:16.400]   can. So I'm not, I'm not faulting for that. But I do feel like, and again, the word algorithm is
[00:52:16.400 --> 00:52:19.760]   probably a bad choice because algorithms can mean a lot of things. This means a computer program.
[00:52:19.760 --> 00:52:26.400]   But I do feel like there's a danger to algorithms to programs that surface content
[00:52:26.400 --> 00:52:33.520]   because it's more engaging, hence more profitable. I mean, that seems to me to be a little different
[00:52:33.520 --> 00:52:37.440]   from what people are doing on Reddit, but mastodons doing with the trending topics.
[00:52:37.440 --> 00:52:41.600]   That seems different, but maybe it's not. Maybe it's not in under the law different enough that we can
[00:52:41.600 --> 00:52:49.040]   allow it. The problem is that algorithms optimizing for engagement is kind of an
[00:52:49.040 --> 00:52:57.200]   emergent property of you trying to offer good user experience to your users. You know, what those
[00:52:57.200 --> 00:53:05.440]   algorithms do is show stuff that is that people like to watch. So again, it becomes an issue of,
[00:53:06.000 --> 00:53:12.000]   I think, so that's part of it. But the other question, if you'll allow me to, to give another
[00:53:12.000 --> 00:53:19.760]   angle to this, let's say, let's study for a second, what happens if you do remove 230, if you,
[00:53:19.760 --> 00:53:31.840]   because the web today is largely a user generated content powered web. And if you remove section
[00:53:31.840 --> 00:53:39.600]   230, I think you're right. In an instant, those businesses are threatened, very much threatened.
[00:53:39.600 --> 00:53:42.880]   And it's like a bunch of Coca-Cola ads and stuff. I mean, it's just not.
[00:53:42.880 --> 00:53:49.840]   Well, not just Coca-Cola ads, user generated web, I'm not sure can survive. So what you end up with
[00:53:49.840 --> 00:54:00.480]   is institutional media and no small guy, like, or gal, you only have big companies that can afford
[00:54:00.480 --> 00:54:06.560]   to produce content and to have websites. And that is, I think, the biggest, because,
[00:54:06.560 --> 00:54:14.720]   again, we don't like Nazis. And I think there are a lot of issues currently with them abusing the
[00:54:14.720 --> 00:54:22.480]   the section 230, essentially, nests of the web to be able to be there and proliferate.
[00:54:22.480 --> 00:54:29.840]   I still don't think we should remove user generated content, which I don't think it's on,
[00:54:29.840 --> 00:54:36.720]   you know, it's so big section 230. I don't think it's unreasonable to say if the Supreme Court rules
[00:54:36.720 --> 00:54:46.080]   a certain way, the user generated web is threatened. And that's a very concerning prospect for me,
[00:54:46.080 --> 00:54:50.880]   because you're left with, you know, big companies. And that's it. You don't have anything else on
[00:54:50.880 --> 00:54:55.040]   the internet. Yeah. That's what a lot of my concern revolves around as well, just because
[00:54:55.840 --> 00:55:00.640]   good ideas come up all over and, you know, Facebook's not doing so great and people have had issues
[00:55:00.640 --> 00:55:04.480]   with it. I will say there's this one really great article. And I think it was in the Wall Street
[00:55:04.480 --> 00:55:11.280]   Journal on how Facebook after January 6 after the attack on the Capitol, they tried to minimize
[00:55:11.280 --> 00:55:16.320]   political content and tried to make it just other content, whether it's fashion or whether
[00:55:16.320 --> 00:55:21.760]   it's just other friend stuff. And people hated it. They wanted more political content after saying
[00:55:21.760 --> 00:55:25.840]   they didn't want it. But it's kind of interesting to say, well, okay, well, we want this. Well, not
[00:55:25.840 --> 00:55:30.880]   really not like that. And I think there's a lot of complexity here that even if they're not optimizing
[00:55:30.880 --> 00:55:36.080]   for profit, in certain ways, it's still, you know, the users don't always like it. And it's not
[00:55:36.080 --> 00:55:41.920]   just about the profit side, but it's about the things the users like. And then on the algorithm side
[00:55:41.920 --> 00:55:47.280]   too, like I know we've been talking about it here, but just, you know, you need algorithms
[00:55:47.280 --> 00:55:52.960]   as a tool for moderation. I think it was Casey Newton too, who had made this point that mental
[00:55:52.960 --> 00:55:58.480]   illness and like real trauma can stem from moderators who are looking at just the worst of the worst
[00:55:58.480 --> 00:56:03.440]   content. And it takes real pressure off of them when you're able to use code and, you know, use
[00:56:03.440 --> 00:56:08.400]   stuff like that to be able to help in the moderation process. And I think it's important that when
[00:56:08.400 --> 00:56:14.000]   we demonize things as part of this, that it's something that's bad across the board. Like, it's
[00:56:14.000 --> 00:56:18.800]   not just bad if Google does it, but it's bad if all trails does it. And I tend to go from, you know,
[00:56:18.800 --> 00:56:24.800]   the biggest example to the littlest ones, because, you know, I've seen scams posted on all trails,
[00:56:24.800 --> 00:56:30.320]   and we know them using algorithms to maximize profit, I still don't think should mean that
[00:56:30.320 --> 00:56:35.520]   they're viable for scams that people are posting. And then, that's absolutely right. In fact,
[00:56:35.520 --> 00:56:41.680]   all trails using algorithms to pick the best trails in an area to hike is exactly what it should
[00:56:41.680 --> 00:56:47.040]   be doing. Right? Yeah. There's no problem with that. So what if it increases profit?
[00:56:47.040 --> 00:56:53.920]   I have to say that that what you said, Patrick's important because the New York Times yesterday
[00:56:53.920 --> 00:56:58.960]   had a piece, not an opinion piece, but an article Supreme Court poised to reconsider key tenants of
[00:56:58.960 --> 00:57:06.720]   online speech and focused on big companies for this is the lead for years, giant social networks
[00:57:06.720 --> 00:57:12.880]   like Facebook, Twitter, and Instagram have operated under true crucial tenants. And they talk about
[00:57:12.880 --> 00:57:17.840]   section 230. Now the Supreme Court has poised to reconsider those rules. Mrs. The Point,
[00:57:17.840 --> 00:57:27.600]   Facebook, Google, Twitter, Instagram, Snapchat, TikTok, will all survive that. But my little
[00:57:27.600 --> 00:57:33.680]   sites will not. And all the sites like all trails will not. It's not about the big tech giants here.
[00:57:33.680 --> 00:57:38.720]   And it's a mistake. And I think that's a politicization of it because there's a kind of political
[00:57:38.720 --> 00:57:46.160]   agenda against big tech. And that's the politicization of what is something that's really not political.
[00:57:46.160 --> 00:57:51.200]   I'll go ahead, Brianna. I'm sure you I bet you're a 230 absolutist, I would guess.
[00:57:51.200 --> 00:58:00.080]   No. I'm going to put this. I have been around politics enough to know when you were on a failing
[00:58:00.080 --> 00:58:08.400]   side of an issue being framed. And I've got to say, when it comes to tech companies being more
[00:58:08.400 --> 00:58:14.240]   responsible with their algorithms, they want I listened to this whole discussion. This is a
[00:58:14.240 --> 00:58:19.200]   framing that you're just going to lose. If the way they've stacked the deck is it's like,
[00:58:19.200 --> 00:58:25.920]   look, you either believe in section 230, or you believe that Google should continue to do very
[00:58:25.920 --> 00:58:33.120]   little and not enough about this. Like we barely talk about red pill tube, which is a huge
[00:58:33.120 --> 00:58:38.480]   phenomena. If you have young children out there that are their boys, you need to be aware of what
[00:58:38.480 --> 00:58:44.880]   they're looking at on YouTube. There is a very frightening sub genre of outright massage in this
[00:58:44.880 --> 00:58:51.440]   out there is not just Andrew Tate. I look at it every single day. That algorithm is dangerous.
[00:58:51.440 --> 00:58:59.600]   The the the the the radical like reactionary part of the politics that are anti-vax and
[00:58:59.600 --> 00:59:07.120]   pro violence and anti democracy, that is dangerous. And I have the opinion that Facebook and Twitter
[00:59:07.120 --> 00:59:14.400]   and YouTube and all these companies have completely failed and their civic responsibility to provide
[00:59:14.400 --> 00:59:22.480]   a functional public square. And their lawyers have come into an argument that is absolutely
[00:59:22.480 --> 00:59:30.480]   reasonable that their daughter, y'all, their daughter was killed by a terrorist group. And
[00:59:30.480 --> 00:59:39.760]   they have a very clear chain of evidence of how this rabbit hole on YouTube aided in the radicalization.
[00:59:39.760 --> 00:59:45.840]   That is a problem worth solving. And at some point, some adults are going to have to have
[00:59:45.840 --> 00:59:51.440]   a more nuanced conversation that, look, you either want to throw section 230 in the airlock
[00:59:51.440 --> 00:59:57.280]   because it is correctly going to hurt people like you, Leo. But we've still got to do something
[00:59:57.280 --> 01:00:04.720]   about this problem. Our democracy is on its last legs, y'all. These algorithms are designed to
[01:00:04.720 --> 01:00:11.040]   make us angry at each other and screaming at each other all day about the stupidest stuff
[01:00:11.040 --> 01:00:17.200]   that does not matter. And our country is not going to survive. We've got to address this at some point.
[01:00:17.200 --> 01:00:22.160]   We're seeing the consequences of that. This was this was actually my thinking initially was,
[01:00:22.160 --> 01:00:29.360]   yeah, we yeah, 230 is important. And we got to do something. And so I came from that point of view.
[01:00:29.360 --> 01:00:33.520]   But one of the problems I'm having as I talk to a lot of people about this
[01:00:34.160 --> 01:00:40.480]   is it's very hard to draw a bright, wet line about around which algorithms are okay and which
[01:00:40.480 --> 01:00:47.920]   aren't. And that's, I don't think something the courts or the legislatures are going to be able to
[01:00:47.920 --> 01:00:53.920]   do. It's too nebulous of a concept. So maybe I'm starting to come around to the section 230
[01:00:53.920 --> 01:01:00.720]   absolutist just because we can't say, well, that algorithm is radicalizing, but that one's not.
[01:01:01.280 --> 01:01:07.920]   Well, it's even it's even more than that. It's not even about, I don't think the algorithm.
[01:01:07.920 --> 01:01:14.800]   It comes back to another fundamental question about moderating the internet. Who decides what's,
[01:01:14.800 --> 01:01:21.920]   you know, what should be moderated? Obviously, Brianna has some very strong opinions about that.
[01:01:21.920 --> 01:01:28.320]   And I think for the most part, if not for everything, I probably would agree with with Brianna on what
[01:01:28.320 --> 01:01:35.680]   should be moderated. The problem is again, who do you put in charge? Are we going to say, okay,
[01:01:35.680 --> 01:01:42.800]   Brianna goes to YouTube and decides this is what video should be deleted. And, you know,
[01:01:42.800 --> 01:01:48.800]   what if she makes a mistake or what if YouTube makes a mistake and delete something that shouldn't
[01:01:48.800 --> 01:01:56.080]   be and who decides this? And I guarantee you again, coming at it from the European point of view,
[01:01:56.880 --> 01:02:07.120]   when you what YouTube would love is for the legal system to tell them what should be deleted.
[01:02:07.120 --> 01:02:12.160]   And they will gladly delete everything that the legal system tells them to far more than I would
[01:02:12.160 --> 01:02:18.720]   like, including many of our shows. But okay, go ahead. It's well, I think your shows are being
[01:02:18.720 --> 01:02:23.280]   deleted because YouTube wants to please their advertisers through content, ideas, stuff like that.
[01:02:23.280 --> 01:02:28.560]   Yeah, or our content, large content creators over small content creators. Yeah.
[01:02:28.560 --> 01:02:40.480]   Exactly. But if we found a way for a governmental or, you know, judicial authority to decide
[01:02:40.480 --> 01:02:48.000]   and have enough manpower to review stuff that should be deleted, that's what we're edging towards
[01:02:48.000 --> 01:02:55.120]   in France or in the EU, YouTube would be very happy to do it. The uncertainty is very bad for
[01:02:55.120 --> 01:03:00.720]   their business, for their image. And the problem is there's no solution to this. There's so much
[01:03:00.720 --> 01:03:05.680]   content that you can't review everything and decide individually what should or shouldn't be.
[01:03:05.680 --> 01:03:14.640]   So red peel to, which is a big problem, if they had legal, like, you know, a document saying,
[01:03:14.640 --> 01:03:20.560]   you should delete that, they would, I'm certain. Well, and they, part of it is that there's so much
[01:03:20.560 --> 01:03:25.360]   content is put up so fast, they say we delete all the ISIS videos the minute we find them,
[01:03:25.360 --> 01:03:29.200]   but clearly they can't get all of them because there's so much being put up there.
[01:03:29.200 --> 01:03:35.280]   It's almost a trap. I also have a, so it's funny. This actually relates to something,
[01:03:35.280 --> 01:03:41.680]   this relates to something I'm giving on Tuesday. I'm having a webinar through our
[01:03:41.680 --> 01:03:47.440]   street where we talk about government content moderation, just the logistics of it, kind of like
[01:03:47.440 --> 01:03:52.480]   when maybe a senator is like, hey, that person was being mean to me. And that, you know, being on
[01:03:52.480 --> 01:03:57.920]   the totally illegitimate side, all the way up to court orders that they can't reveal that,
[01:03:57.920 --> 01:04:02.960]   you know, government has said, please keep this content up or please take this content down,
[01:04:02.960 --> 01:04:09.600]   that there's a whole spectrum here of a very legitimate, very illegitimate. And some of it's
[01:04:09.600 --> 01:04:14.800]   concerning from government, not just American governments, but governments all over the world.
[01:04:14.800 --> 01:04:18.880]   And I think a lot of times we kind of trust our own to be like, all right, generally, you know,
[01:04:18.880 --> 01:04:23.680]   if we set some guards for them, they'll do it right, they'll, they'll police content, right?
[01:04:23.680 --> 01:04:29.040]   But in other countries, it definitely won't be the same. Like when, I forget which country,
[01:04:29.040 --> 01:04:34.960]   but it was one in the Middle East where it was the audio only app clubhouse. And, and they were
[01:04:34.960 --> 01:04:39.840]   saying, hey, clubhouse, you know, we might want your user info because you're not recording things.
[01:04:39.840 --> 01:04:45.040]   We want to know who's talking against us. Oh, yeah. So there's that whole spectrum there.
[01:04:45.040 --> 01:04:49.600]   Yeah. Can you tell that? That's a really interesting question. Because it's an opposite theory, right?
[01:04:49.600 --> 01:04:54.160]   Exactly. So there's that whole spectrum. And I think there's stuff to think through there,
[01:04:54.160 --> 01:04:58.960]   because even when the government flags content, it might be something dangerous and it might be
[01:04:58.960 --> 01:05:02.800]   totally legitimate, or it might not, no matter what part of the process.
[01:05:02.800 --> 01:05:08.080]   Well, look what happened with Modi's government in India. And Twitter, by the way, cooperated.
[01:05:08.080 --> 01:05:15.440]   Modi did not like a BBC documentary about him. In India, they're blocking the YouTube videos
[01:05:15.440 --> 01:05:19.840]   and Twitter posts and they've asked Twitter to take them down. And apparently Twitter has agreed
[01:05:19.840 --> 01:05:26.800]   has done it. These are by the documentary, as far as I can tell, is not libelous. It's telling
[01:05:26.800 --> 01:05:32.320]   the truth about Modi. He just doesn't want anybody in India to see it. There's a perfect example of
[01:05:32.320 --> 01:05:38.240]   government intervention gone wrong. If I may, yeah, sorry, go ahead, Shashanna.
[01:05:38.240 --> 01:05:42.560]   Oh, yeah. Just super quick. One other thing I just wanted to add too is that there's also
[01:05:42.560 --> 01:05:47.600]   different theories from the law enforcement side of whether, you know, let's say there's
[01:05:47.600 --> 01:05:53.440]   there's something illegal going on and there's certain law enforcement and it can be agency by
[01:05:53.440 --> 01:05:58.560]   agency, but they think it's worth it to keep this content up so we can continue to watch it
[01:05:58.560 --> 01:06:04.400]   out in the open and find the people who were being drawn into it versus what if we take this down
[01:06:04.400 --> 01:06:08.880]   and it's better off that way and others who are concerned, okay, what if you put it in the dark,
[01:06:08.880 --> 01:06:13.280]   then can we not understand what's going on there? Is that worse? So not to say that, you know,
[01:06:13.280 --> 01:06:17.520]   that there's certain things that don't seem obvious, but sometimes I even question myself
[01:06:17.520 --> 01:06:21.920]   when I'm thinking, man, I wish that was down. I'm kind of a little bit more glad it's in the open
[01:06:21.920 --> 01:06:26.880]   where people can see it and do something about it if it turns into something. I originally thought
[01:06:26.880 --> 01:06:32.080]   that when the internet began that, oh, this is great because all of this hate that's under a rock
[01:06:32.080 --> 01:06:39.280]   will be exposed to sunlight and it'll go away and instead just multiply it didn't quite work out
[01:06:39.280 --> 01:06:45.760]   that way. Go ahead, Brianna. I hear what all of y'all are saying. I really do. I genuinely hear you
[01:06:45.760 --> 01:06:53.440]   and I agree with you that when it comes to section 230, I think we're all in complete agreement
[01:06:53.440 --> 01:07:00.240]   that we want this to stay largely how it is. This is my question to you and I genuinely want an
[01:07:00.240 --> 01:07:05.760]   answer from any of y'all here. I hear what you don't like. I hear that loud and clear.
[01:07:05.760 --> 01:07:13.520]   What do you propose we do about this problem? Is it just acceptable losses to you? Because tech
[01:07:13.520 --> 01:07:19.600]   doesn't have a good answer for this. What I hear constantly is, well, if we do this, it's going to
[01:07:19.600 --> 01:07:26.000]   cause this. This is a problem. We don't want to do this. That's fine. What is your damn solution?
[01:07:26.000 --> 01:07:33.440]   Because we cannot keep ignoring this as a society. Leah, you asked earlier why we are quitting
[01:07:33.440 --> 01:07:38.800]   Twitter. I can give you an academic answer about that. The reality is it's a wildly addicting product
[01:07:38.800 --> 01:07:47.280]   and as far as why people didn't like Facebook lessening political content after the January 6
[01:07:47.280 --> 01:07:52.240]   insurrection. Because we like being angry. It's wildly addicting. Just being angry at each other.
[01:07:52.240 --> 01:07:55.440]   It's like asking why do people still use heroin? Don't they know how bad it is for us? Why are
[01:07:55.440 --> 01:08:02.960]   you still smoking? What do y'all want to do about this problem of radicalization? Or is it just an
[01:08:02.960 --> 01:08:09.040]   angrier, more dysfunctional country? Just an acceptable loss to continue on this path?
[01:08:10.160 --> 01:08:19.840]   I can try to answer that. I'll turn back to Leah on Torsana and say one thing we can try and we
[01:08:19.840 --> 01:08:27.520]   should do is trust our damned government, which is something that Americans seem completely
[01:08:27.520 --> 01:08:38.880]   incapable of doing, which is a kind of malady of the mind. Are you surprised observing from Finland?
[01:08:38.880 --> 01:08:41.200]   Are you surprised that we don't trust our government?
[01:08:41.200 --> 01:08:49.120]   I think that we have a politicized Supreme Court, 100% polarized Congress.
[01:08:49.120 --> 01:08:58.080]   We're banning African American studies in Florida. What do you mean? Trust our government.
[01:08:58.080 --> 01:09:04.240]   I will amend what I said. When I said trust your government, I wasn't talking about you. I
[01:09:04.240 --> 01:09:09.040]   meant in general people should trust the government. Maybe the United States is the exception.
[01:09:09.040 --> 01:09:18.880]   What I mean is I think that part of the situation, this is completely like I don't know what I'm
[01:09:18.880 --> 01:09:25.280]   talking about, but part of the situation you're in, the reason is that you initially don't trust
[01:09:25.280 --> 01:09:31.200]   anyone. It doesn't matter. But we'll vote for whoever. That's how you end up with Trump in the
[01:09:31.200 --> 01:09:34.320]   world. Why? That's right. It's more of a statement than it is.
[01:09:34.320 --> 01:09:36.720]   I'm electing somebody to run the place. Yes.
[01:09:36.720 --> 01:09:45.840]   I really think that part of the platform of the people who led your country to January 6th
[01:09:45.840 --> 01:09:53.440]   is sowing so much chaos and distrust in the government that nothing matters. So they end
[01:09:53.440 --> 01:09:58.400]   up benefiting because if you don't trust anyone, then the ones who don't have a plan, or not who
[01:09:58.400 --> 01:10:10.000]   don't have a plan, who are ultimately the chaotic people and having a chance to be elected.
[01:10:10.000 --> 01:10:14.640]   And then they thrive on that. But in general, if you look at what we're doing in the EU,
[01:10:14.640 --> 01:10:26.160]   we are shaping the way, which by the way, giving the example of India and China is something that
[01:10:26.160 --> 01:10:31.200]   I do as well. It's like, well, if you want the companies to obey the government, then what happens
[01:10:31.200 --> 01:10:36.080]   when the government says something bad? Well, yes, it is a problem, but it doesn't mean as Breanna was
[01:10:36.080 --> 01:10:42.800]   suggesting, essentially, we still need to do something. And in the EU, we're really, and I'm
[01:10:42.800 --> 01:10:51.520]   surprised that we're achieving that result, we're really getting to a place where the big tech
[01:10:52.480 --> 01:10:58.640]   has to do what we, as the will of the people expressed through the government,
[01:10:58.640 --> 01:11:08.400]   they have to do what we ask them to. They have to, or they abandon a market of 450 million people,
[01:11:08.400 --> 01:11:14.400]   which they don't want to do. So they're accepting things that they were saying, "Oh, my God,
[01:11:14.400 --> 01:11:19.600]   can't you imagine what it will do to our business? And when we'll, "Oh, we can do this." No, they're
[01:11:19.600 --> 01:11:26.080]   doing it because the government said, "You either do or you're out." It does feel like a little bit
[01:11:26.080 --> 01:11:31.040]   of an experiment. I'm glad to watch the experiment. It's a mixed bag a little bit.
[01:11:31.040 --> 01:11:38.560]   And yes, they have the absolute regulatory authority to put a business on the ropes.
[01:11:38.560 --> 01:11:43.120]   It's starting to change in new ways. I hear what you're saying. I genuinely hear what you're saying.
[01:11:43.120 --> 01:11:48.880]   There's no future in the United States where we just trust our government and start a lot of
[01:11:48.880 --> 01:11:51.520]   regulation here. It's just not who we are.
[01:11:51.520 --> 01:11:53.120]   Okay, so what's your solution?
[01:11:53.120 --> 01:11:59.520]   Hold on, just let me finish, please. There's no future where Americans just stop owning guns,
[01:11:59.520 --> 01:12:06.240]   either. What I think we need to do is exactly why I ran for Congress. I think that we should take
[01:12:06.240 --> 01:12:11.040]   a much stronger role. I think that the people that serve on the Space Science and Technology
[01:12:11.040 --> 01:12:17.360]   Subcommittee, they put Gonzalez on that committee, the one I ran for Congress wanting to serve on,
[01:12:17.360 --> 01:12:23.120]   because it's so important because it regulates all of this stuff. We've got to take a more active role
[01:12:23.120 --> 01:12:29.600]   in regulation. I think that should be the hottest place for people to serve. It seems as this,
[01:12:29.600 --> 01:12:39.440]   I don't want to say ghetto, but a shameful thing to do. I think in lieu of stronger regulation
[01:12:39.440 --> 01:12:45.760]   there for a congressional place, I think we need state houses to step up and do their job there.
[01:12:45.760 --> 01:12:51.440]   California could do immensely good work there if they were to step up and require things,
[01:12:51.440 --> 01:12:55.840]   because their economy is so great the same way they have with automotive standards.
[01:12:55.840 --> 01:13:02.320]   Beyond that, I think YouTube and Facebook should be holding themselves to far higher standards,
[01:13:02.320 --> 01:13:08.480]   and we, as technologically literate people, should be expecting more from them, rather than
[01:13:08.480 --> 01:13:14.480]   defending them on everything and expecting less. We are getting the social media platforms we are
[01:13:14.480 --> 01:13:21.920]   asking for. I think that there's a real tendency to shrug our shoulders and just ignore the body count.
[01:13:21.920 --> 01:13:29.120]   Let me get Shoshana in, because I think she might have something to say about all this.
[01:13:29.120 --> 01:13:33.600]   Sure. A couple of things. First, it makes sense that Gonzalez would serve on the committee,
[01:13:33.600 --> 01:13:39.440]   because he's had decades of experience in science. He was the chief scientist at NASA for years.
[01:13:39.440 --> 01:13:43.680]   So he just has this really long resume. I think I just think it's silly to criticize
[01:13:43.680 --> 01:13:49.360]   them for being on that. But second, that committee tends, or the subcommittee,
[01:13:49.360 --> 01:13:55.920]   tends to not do so much on that side of technology policy. It's definitely more energy in commerce.
[01:13:55.920 --> 01:14:00.480]   It's at least in the house that tends to do the legislation. I'm not exactly sure why,
[01:14:00.480 --> 01:14:03.840]   because sometimes the committees don't always align in ways you would think.
[01:14:03.840 --> 01:14:11.360]   There have been a couple, science-based technology tends to do more cybersecurity
[01:14:11.360 --> 01:14:15.440]   while energy in commerce, for whatever reason. They tend to do the tech legislation.
[01:14:15.440 --> 01:14:22.240]   But also, I don't want legislation just to do legislation I wanted to be right, which is why I take
[01:14:22.240 --> 01:14:31.440]   it seriously when legislation going after algorithms says you're only allowed to have time
[01:14:31.440 --> 01:14:37.280]   and time ordered or some basic thing like that and anything else will follow under this new
[01:14:37.280 --> 01:14:44.160]   regulation. The proposals have been really, really basic. It's not really technologically
[01:14:44.160 --> 01:14:50.160]   literate. So if there were larger issues that they wanted to tackle, the legislation out there
[01:14:50.160 --> 01:14:55.360]   did a really bad job of going after it. But I think that there's other ways to go about it,
[01:14:55.360 --> 01:15:00.080]   because I genuinely don't think 230 is the problem. One thing that I really like is funding
[01:15:00.080 --> 01:15:07.840]   Nick McMahon and also making sure that platforms can maintain child exploitation materials longer
[01:15:07.840 --> 01:15:12.560]   in order to match it and use hashes that go find more of it. I think in certain cases,
[01:15:12.560 --> 01:15:17.280]   they're limited by the, I forget exactly the number of days that are allowed to keep it for.
[01:15:17.280 --> 01:15:21.520]   It's obviously something that we wouldn't want anyone to have in any case. But if it allows them
[01:15:21.520 --> 01:15:25.520]   to find more of it and work further with Nick McMahon in order to find more of it, I think that's a
[01:15:25.520 --> 01:15:29.360]   really good way. Also, what? Center for Missing and Exploited Children.
[01:15:29.360 --> 01:15:33.040]   Just thank you. I will never remember what it all stands for. I just know that they do really
[01:15:33.040 --> 01:15:38.000]   good work. I just for people who are saying, what's that? Yeah. Oh, yeah, yeah. And also
[01:15:38.000 --> 01:15:43.600]   a federal privacy law, I think is a good way to go about things. I do worry a lot about the states
[01:15:43.600 --> 01:15:49.920]   doing things on their own, because I have nuanced issues with California's laws, but also
[01:15:49.920 --> 01:15:54.800]   with Florida and Texas, they just made these super blatantly unconstitutional laws saying,
[01:15:54.800 --> 01:16:00.240]   oh, you can't have bias in moderation. And it's super anti-first amendment, not to say that the
[01:16:00.240 --> 01:16:04.240]   same stuff couldn't happen on the federal level. The Supreme Court is also considering those two
[01:16:04.240 --> 01:16:10.480]   laws. So, yeah, that'll be, yeah, I won't sleep a lot. That'll be so much. Friday.
[01:16:10.480 --> 01:16:19.120]   The oral arguments on Friday for the Texas and Florida laws, and then next month for the
[01:16:19.120 --> 01:16:27.040]   Section 230 thing. I actually didn't realize they're coming up so fast. Yeah. Yeah. Somebody in the
[01:16:27.040 --> 01:16:31.360]   chat room says, come on, let's get real. The Supreme Court's going to overturn 230. It's obvious.
[01:16:31.360 --> 01:16:38.560]   I certainly hope not. I certainly hope not. I understand, Brianna, especially coming from
[01:16:38.560 --> 01:16:43.600]   politics, you're kind of real politic point of view, which is you've got to find a way, you can't
[01:16:43.600 --> 01:16:49.920]   be purist about this. You've got to find a way to protect Section 230. But at the same time,
[01:16:49.920 --> 01:16:57.200]   we may need to make progress with pollers. I'm saying that it's very easy to shrug your shoulders
[01:16:57.200 --> 01:17:02.640]   at the radicalization that's happening on YouTube. I mean, I've seen this firsthand. I don't want
[01:17:02.640 --> 01:17:08.640]   to go through it again on the show. Yeah. It's a real problem. It really affects people's lives.
[01:17:08.640 --> 01:17:15.120]   And how about TikTok? How do you feel about TikTok? I think it's a much more wholesome place to
[01:17:15.120 --> 01:17:21.440]   spend time than you do. I kind of agree with you. And nevertheless, what is it now up to 20 or 30
[01:17:21.440 --> 01:17:29.520]   states who banned TikTok? And now Alabama has banned TikTok. So Auburn has banned TikTok from
[01:17:29.520 --> 01:17:37.280]   its Wi-Fi network. And I think a lot of this has kind of politicized story from the Washington
[01:17:37.280 --> 01:17:42.640]   Post as state span TikTok on government devices evidence of harm is thin.
[01:17:42.640 --> 01:17:52.960]   I kind of agree with this. I hate to say, you know, Scott Galloway had some commentary on this. I
[01:17:52.960 --> 01:18:01.600]   really thought was fair. And you know, I think that you have to assume that China does not have
[01:18:01.600 --> 01:18:07.520]   these same like firewalls between their companies and their government. And I think you've got to
[01:18:07.520 --> 01:18:14.400]   assume that those companies are going to be giving all of that data like freely over to, you know,
[01:18:14.400 --> 01:18:21.440]   to the PRC basically. So I think that there are reasonable national security concerns you can have
[01:18:21.440 --> 01:18:28.640]   about a another country shaping what an entire generation of young people, what they see, how they
[01:18:28.640 --> 01:18:33.920]   feel about things. I think that's also an algorithmic bias that you should you should be considering.
[01:18:33.920 --> 01:18:39.040]   So I'm glad to see I don't know, censoring it is the idea. I think there's an adult discussion
[01:18:39.040 --> 01:18:47.440]   to be had there. Yeah. Go ahead, Shoshana. And then you are right. You want to seed your time
[01:18:47.440 --> 01:18:52.160]   to the audible podcaster from the state of Finland. Go ahead, sir.
[01:18:53.600 --> 01:19:01.360]   Yeah, I think a really difficult thing with TikTok is that no one is saying or well,
[01:19:01.360 --> 01:19:09.200]   the real issue isn't that harm is happening now. The issue is it is a potential for harm in the
[01:19:09.200 --> 01:19:19.760]   future influenced by the Communist Party in China. And we don't know that it's happening. We don't
[01:19:19.760 --> 01:19:26.720]   know that it necessarily could happen, but we'll never be sure. And what Rihanna is saying is
[01:19:26.720 --> 01:19:36.800]   completely right. The whole generation of humans is living on TikTok now. And it started with dances
[01:19:36.800 --> 01:19:42.480]   and you know, that kind of thing. It's there's a lot of content about everything on TikTok.
[01:19:42.480 --> 01:19:49.440]   And it's incredibly creative. I spend some time on TikTok. It's amazing. But it is also
[01:19:49.440 --> 01:20:00.480]   potentially maybe influenced by the Communist Party. And there was a story, I don't know if you saw it,
[01:20:00.480 --> 01:20:07.520]   but there was a story about heating on TikTok, which was controlled by people at the company
[01:20:07.520 --> 01:20:15.440]   and people at Biden's in China. Heating is essentially pushing a video to be to get more views. So
[01:20:15.440 --> 01:20:20.640]   you put it on the for you page and it substantially increases the amount of views it gets because it's
[01:20:20.640 --> 01:20:29.440]   presented to more people. The idea that the whole generation, not only could I know you talk about
[01:20:29.440 --> 01:20:35.920]   this often, Leo, you're like, why does it matter if they have this and that information about me
[01:20:35.920 --> 01:20:42.080]   on TikTok? They know what you like. They know what you watch. They know what you enjoy. And it's
[01:20:42.080 --> 01:20:48.160]   just as bad as Cambridge Analytica, I think. And just as impotent, by the way, as Cambridge
[01:20:48.160 --> 01:20:56.480]   Analytica. A lot of evidence that he achieved nothing and was overselling its capabilities.
[01:20:56.480 --> 01:21:01.440]   I don't think TikTok, yeah, TikTok knows Roman's algorithm what I want to watch, which apparently
[01:21:01.440 --> 01:21:08.880]   went in Bikini's. But it doesn't know who I am. And I don't know if it's an invasion of privacy.
[01:21:08.880 --> 01:21:12.880]   It's big interest is in giving me more of what I spend time looking at.
[01:21:12.880 --> 01:21:24.720]   No, but yeah, now. But what if, and it's maybe a huge, how do you say it? It's a huge red herring.
[01:21:24.720 --> 01:21:34.640]   But what if the Chinese government has plugs into TikTok? It's the same as Huawei. We don't know
[01:21:34.640 --> 01:21:39.360]   if they have ways of changing the software on the
[01:21:39.360 --> 01:21:46.480]   sufficient reason to ban them now. No, no, no, that's a theoretical. Of course,
[01:21:46.480 --> 01:21:52.880]   there's all sorts of theoretical hazards. Children could suddenly all eat tide pods.
[01:21:52.880 --> 01:21:58.880]   But those are all theoretical. And I don't think you can ban something based on a theory.
[01:22:00.080 --> 01:22:06.080]   I don't, Shoshana, I would suspect you and I are going to agree on this. Look, I'm a progressive.
[01:22:06.080 --> 01:22:10.880]   I'm on the left. I'm also someone who's an engineer and deeply pragmatist. I'm interested
[01:22:10.880 --> 01:22:16.080]   in building things. There is a generational disconnect that I've noticed with younger leftists
[01:22:16.080 --> 01:22:23.360]   that America is always in the wrong for everything. We are an evil imperialist power.
[01:22:23.360 --> 01:22:30.800]   Our cops are all POSs that want to do nothing other than to slaughter people of color in the streets.
[01:22:30.800 --> 01:22:38.160]   Communism is the best answer. And capitalism is the answer to every single problem that the
[01:22:38.160 --> 01:22:46.160]   United States faces. That is a swear to God honest assessment of the mindset of a lot of younger
[01:22:46.160 --> 01:22:52.000]   people. And it's not, it's not to say I don't agree with many of those issues.
[01:22:52.000 --> 01:22:57.280]   I thought that was all true. I think there's some more nuanced discussion to be had.
[01:22:57.280 --> 01:23:04.240]   And I think that TikTok, I think if you look at political TikTok, there's certainly a section
[01:23:04.240 --> 01:23:10.560]   that could amp that up and show that to a generation of people over and over and over and over and over.
[01:23:10.560 --> 01:23:15.040]   And just like you said, YouTube could do and does with white supremacy. Right.
[01:23:15.040 --> 01:23:20.560]   100% is the exact same threat. So my only argument here is that we can do that.
[01:23:20.560 --> 01:23:23.280]   Damn it. How do we say it? We got to take it more seriously.
[01:23:23.280 --> 01:23:27.600]   Regularly. I agree. It's a serious problem. So we got to solve it. You're doing something
[01:23:27.600 --> 01:23:32.800]   interesting in Finland. We should mention Patrick is French, but he lives in Finland.
[01:23:32.800 --> 01:23:37.200]   New York Times had an article how Finland is teaching a generation to spot misinformation.
[01:23:37.200 --> 01:23:44.720]   Is it possible to do something with the consumers instead of regulating the providers?
[01:23:47.120 --> 01:23:54.720]   Well, I think both need to happen. And we tend to talk a lot about the very, very necessary
[01:23:54.720 --> 01:24:03.360]   part, which is making sure the providers do their job. And by the way, the difference between
[01:24:03.360 --> 01:24:10.080]   YouTube and TikTok, the concern here with TikTok is what if by dance controlled by China has their
[01:24:10.080 --> 01:24:16.640]   shortcut to get into TikTok. That's a very different discussion from however imperialistic the US is
[01:24:16.640 --> 01:24:23.360]   and however much it controls YouTube, but setting that aside. Yes, platforms should do a better job
[01:24:23.360 --> 01:24:29.120]   and we should push them through political action to do a better job. But also, I think it's important
[01:24:29.120 --> 01:24:40.480]   to educate the populace. And that starts in school. Finland has a high media literacy rate,
[01:24:40.480 --> 01:24:50.720]   including social media and internet in general. And it comes from a very serious and real danger,
[01:24:50.720 --> 01:24:56.960]   which is I think the world has been made aware of much more clearly in the past year,
[01:24:56.960 --> 01:25:05.200]   which is the neighbor to the east. And so the whole country has always been aware and always been very
[01:25:06.160 --> 01:25:15.280]   active in fighting propaganda and fake news as it is financed by Russia.
[01:25:15.280 --> 01:25:23.920]   And that's the real world. Yeah, your neighbor to the east. Yes, absolutely. That motivates a
[01:25:23.920 --> 01:25:31.040]   huge amount as it does for all neighboring countries of Russia. But it motivates a huge amount of
[01:25:31.840 --> 01:25:41.920]   even the Finnish identity who the Finnish are. But so part of the importance of making sure you're not
[01:25:41.920 --> 01:25:48.080]   essentially overrun and conquered is making sure that people understand what fake news and
[01:25:48.080 --> 01:25:58.240]   propaganda is. And it does start in school. This is the media literacy index, which ranks a number of
[01:25:58.960 --> 01:26:10.160]   countries in Europe, primarily, but not only Europe. And Finland does rank highest in that
[01:26:10.160 --> 01:26:16.560]   index because you have a lot of teachers that are taking it, well, not upon themselves. It's
[01:26:16.560 --> 01:26:24.960]   part of the curriculum. But in junior high, you start discussing social media and how it can
[01:26:24.960 --> 01:26:30.320]   be used and what news mean, and not just social media, like actual news, how it means where it
[01:26:30.320 --> 01:26:39.680]   comes from, who's publishing it, why one might share that kind of opinion. And it works, you know,
[01:26:39.680 --> 01:26:46.640]   it's just, it's an important part of how you become a literate citizen. And it's something that
[01:26:46.640 --> 01:26:53.360]   everyone should be paying attention to. There's a lot to say there, but I do want to mention
[01:26:53.360 --> 01:27:00.640]   an interesting aspect of that study. In Europe, there are four big groups. The northern country
[01:27:00.640 --> 01:27:07.680]   mostly are well ranked. They're in the first cluster, then you have a second cluster, which is
[01:27:07.680 --> 01:27:15.360]   essentially France, Spain, Germany, etc. Third cluster, which is less well ranked, Italy, Hungary,
[01:27:15.360 --> 01:27:20.800]   a couple of those. And then you have a couple of other clusters, which are even worse ranks.
[01:27:21.760 --> 01:27:32.400]   I have a question for you. Finland is the highest ranked. It has 76 points. Usually they're around
[01:27:32.400 --> 01:27:41.280]   70 for the northern countries. France is at 58 points. And that's still in the good, you know,
[01:27:41.280 --> 01:27:46.880]   second cluster. So pretty good. People have a pretty good handle on what things mean and what
[01:27:46.880 --> 01:27:54.000]   they, you know, how to spot fake news, essentially. I wonder if you could do you know what the US is?
[01:27:54.000 --> 01:28:00.720]   Where the US? Yes, exactly. All right. I have a few countries outside the outside of Europe.
[01:28:00.720 --> 01:28:03.280]   Would you guess where the US ranks? That's an interesting question.
[01:28:03.280 --> 01:28:10.720]   It's tempting to say we're terrible. I feel like we're not so bad. I feel like we'd be in the 50s.
[01:28:12.800 --> 01:28:19.440]   Close to France. We better be as good as France. I'm going to go some the 40s maybe.
[01:28:19.440 --> 01:28:25.120]   Okay. I know it's weird that I'm being the pessimist here, but I was going to guess 30s.
[01:28:25.120 --> 01:28:33.280]   I think she's right. I think that's why something that my friend, Tom Merritt, always says,
[01:28:33.280 --> 01:28:39.920]   you need data and you need studies because our opinions and biases can be very confusing.
[01:28:41.120 --> 01:28:46.480]   The US actually ranks, I don't know how, you know, that it is a very official study. It's a very
[01:28:46.480 --> 01:28:52.080]   serious thing, but I don't know how well it measures things. But the US ranks two points above France.
[01:28:52.080 --> 01:28:56.480]   60 and France is 58. I knew we were better than France. I knew it.
[01:28:56.480 --> 01:29:04.240]   And I was I'm guilty as well. I was surprised that France was in the second cluster at 58,
[01:29:04.240 --> 01:29:10.480]   you know, better. I would have said we have so much disinformation about vaccines and even the
[01:29:10.480 --> 01:29:15.040]   war in that's what makes us good. That's what makes us good. We have lots of practice.
[01:29:15.040 --> 01:29:20.160]   I guess. And I, you know, I think this is one thing that people must understand about the United
[01:29:20.160 --> 01:29:29.440]   States. It's not a majority that is polarized, that is crazed, is QAnon or whatever. This is a
[01:29:29.440 --> 01:29:36.720]   small minority. It's a loud minority, but it's not a big majority. Even though Trump got 70 million
[01:29:36.720 --> 01:29:45.040]   votes in the last election, I think the true believers are far or half that at best.
[01:29:45.040 --> 01:29:50.880]   So I'm not surprised. I actually really, and then by the way, two points is probably well within
[01:29:50.880 --> 01:29:58.000]   the margin of error. So let's say we tied France just to be fair. We'll tie France. I'm not surprised.
[01:29:58.000 --> 01:30:02.880]   I mean, I I'm not surprised Finland's better at it because as you say, there's a more immediate
[01:30:02.880 --> 01:30:09.760]   threat. It is incredibly unifying. You know, well,
[01:30:09.760 --> 01:30:13.440]   and Nordic nations are somewhat more homogeneous, aren't they?
[01:30:13.440 --> 01:30:19.760]   Two. I mean, that's another issue. There is well. One thing I will say is that the middle class is
[01:30:19.760 --> 01:30:24.000]   much larger in at least Finland. I don't know so much about it. We used to have a good middle class
[01:30:24.000 --> 01:30:32.640]   Sweden. It's dwindling. That feels like it's something that is an issue. And that in many ways,
[01:30:32.640 --> 01:30:40.880]   it is. And well, it's not only amazingly great things. Finland is very much anti-aggression.
[01:30:40.880 --> 01:30:46.640]   Everyone's well, not everyone, but there are a lot of a lot of people, you know, where they came from
[01:30:46.640 --> 01:30:53.040]   meeting their translucent and white and they have long hair and they're from Finland. You don't see a
[01:30:53.040 --> 01:30:59.760]   lot of immigrants. Which again, you know, I was saying it's coming from issues with Russia. If
[01:30:59.760 --> 01:31:04.480]   you open the borders, it's kind of difficult to say, anyway, that's a different story.
[01:31:04.480 --> 01:31:09.280]   But so it's a very different world. And so it's hard to create is important to
[01:31:09.280 --> 01:31:14.240]   educate people. It's hard to create it. I want to take a little bit of a break. We have lots more
[01:31:14.240 --> 01:31:20.000]   to talk about, including big layoffs in big tech. And you had a story of how Apple has avoided
[01:31:20.000 --> 01:31:24.000]   those layoffs kind of interesting. But also, I think it's interesting to see where the layoffs
[01:31:24.000 --> 01:31:30.240]   are happening, especially in AR and VR. We'll talk about we had Connie Gugelmo, the editor in
[01:31:30.240 --> 01:31:36.080]   chief of CNET on last week talked a little bit about the AI powered articles at CNET.
[01:31:36.080 --> 01:31:41.520]   It's turning out to be a little bit more of a scandal than we thought. We'll talk about that.
[01:31:41.520 --> 01:31:48.080]   And it is a very big birthday for one of the most important computers of all time. That more
[01:31:48.080 --> 01:31:54.400]   coming up in just a little bit with our wonderful panel. Shoshana, it's so nice to have you. Shoshana
[01:31:54.400 --> 01:32:01.520]   Weisman from our street. She is Senator Weisman, I'm sorry, Senator Shoshana on the Twitter,
[01:32:01.520 --> 01:32:10.960]   one letter in S-O-S-H-O-S-H-A-N-A. Show, Shana, very easy. But there are two S's and two N's in
[01:32:10.960 --> 01:32:15.840]   Weisman. So that's confusing. But that's why Senator Shoshana and congratulations on your
[01:32:15.840 --> 01:32:21.200]   continued work with the Sloth community. Thank you. I've been very thankful to serve for as long as I
[01:32:21.200 --> 01:32:28.800]   have. Yes. Sloths need friends in the world. I think we can all agree on that. Also, Patrick,
[01:32:28.800 --> 01:32:31.600]   you just go to her Twitter. You'll see what I'm talking about.
[01:32:31.600 --> 01:32:42.800]   Patrick Végion, not Patrick.com. So many podcasts in so many languages. His command of English is
[01:32:42.800 --> 01:32:49.920]   amazing. He's a French native Francophone, right? But also I presume speak Swedish by now, right?
[01:32:49.920 --> 01:32:55.840]   Which is not an easy language. No, I speak Swedish. My wife is part of the Swedish-speaking
[01:32:55.840 --> 01:33:01.120]   minority of this bilingual country. So much easier to learn Swedish. I'm very thankful.
[01:33:01.120 --> 01:33:05.920]   I'm very thankful. Just a little tricky. And a little Japanese, which would help you with the
[01:33:05.920 --> 01:33:18.480]   finish, I would think. But maybe not. I don't know. What do I know? I love saying that. And all
[01:33:18.480 --> 01:33:24.400]   the rest. Great to have you, Patrick. And of course, the wonderful Brianna Wu, speedrunner.
[01:33:24.400 --> 01:33:31.040]   Wishing you all the luck in the contest next month. That's very, very exciting.
[01:33:31.840 --> 01:33:34.720]   Do you know what days you'll be competing? Like what days we should watch?
[01:33:34.720 --> 01:33:41.760]   Yeah, it's on my schedule. I think it's that Tuesday I need to. I've been so focused on just
[01:33:41.760 --> 01:33:46.480]   making sure I can get through that 11 minutes without screwing it up. So you're practicing.
[01:33:46.480 --> 01:33:51.840]   Do you do it every day now? Every day. You got the Princess Peach speedrun. Yeah.
[01:33:51.840 --> 01:33:57.280]   Princess Peach speedrun. This is such an embarrassing thing to be obsessed with. But you know,
[01:33:57.280 --> 01:34:02.080]   it's my skill. So I'm going to go with that. Which game is it though? Super Mario too.
[01:34:02.080 --> 01:34:10.240]   Okay, two. We're all good at something. And in your case, it's a little weird, but that's good.
[01:34:10.240 --> 01:34:18.240]   That's good. Our show today. Yeah, I was just showing this is old. This is from 2005,
[01:34:18.240 --> 01:34:27.360]   these Super Princess Peach scores. But which so is this an old? This is not a DS game.
[01:34:27.360 --> 01:34:31.600]   That's a different game. That's the DS version. This is an interesting one where
[01:34:31.600 --> 01:34:36.960]   Mario gets kidnapped and Princess Peach has to go save him. Oh, no wonder you like it.
[01:34:36.960 --> 01:34:44.320]   No wonder. No wonder. No, I understand. And you are. You can watch a YouTube of this, right?
[01:34:45.040 --> 01:34:51.920]   100%. Where is that? Let's see. You can, I think the easiest way is to go speedrun.com.
[01:34:51.920 --> 01:34:57.360]   Look at the records for Super Mario Advance. And I'm right there or the Super Mario All-Stars.
[01:34:57.360 --> 01:35:05.360]   I hold records on all those categories. She's amazing. Thank you very much. All three of you
[01:35:05.360 --> 01:35:10.320]   is wonderful to have you. And boy, are you thoughtful and smart. I love that. I really appreciate that.
[01:35:10.320 --> 01:35:16.400]   Our show today brought to you by Express VPN. There is a way, frankly, to protect yourself
[01:35:16.400 --> 01:35:22.880]   against overreaching governments, intrusive ISPs and big tech companies that want to sell
[01:35:22.880 --> 01:35:30.720]   your information. It's a good VPN. Is it a little weird? It is to me that the same company that
[01:35:30.720 --> 01:35:38.400]   controls half of online retail is also passively eavesdropping on your private conversations at home.
[01:35:38.400 --> 01:35:43.760]   You know what I'm talking about, right? Hello? What about the idea that a single company controls
[01:35:43.760 --> 01:35:50.640]   90% of internet searches and runs your email service and maybe tracks everything you do on
[01:35:50.640 --> 01:35:58.000]   your smartphone? This is a bigger problem than you might imagine because instead of one signal,
[01:35:58.000 --> 01:36:02.400]   they get signals from a variety of sources, which allows them to build an amazing dossier.
[01:36:02.400 --> 01:36:08.240]   All about you. Big tech is more powerful nowadays than most countries. And they profit
[01:36:08.240 --> 01:36:13.680]   by exploiting your personal data. Time to put a layer of protection between your online activity
[01:36:13.680 --> 01:36:18.640]   and those tech juggernauts. And all the other people want to spy on you, the data brokers,
[01:36:18.640 --> 01:36:23.040]   who are buying your information from your internet service provider and your cell phone carrier.
[01:36:23.040 --> 01:36:27.920]   Protect yourself with a good VPN. And when it comes to VPNs, there's only one I use. There's only
[01:36:27.920 --> 01:36:33.520]   one I trust Express VPN. And that's because I've really looked into this. The thing about a VPN
[01:36:33.520 --> 01:36:37.600]   is, yeah, you're protecting yourself against those immediate threats. But in a way, you're just
[01:36:37.600 --> 01:36:42.720]   kicking the can down the road. You've got to trust the VPN provider. And no one does more
[01:36:42.720 --> 01:36:48.960]   to protect your privacy than Express VPN. Express VPNs VPN servers are all over the world. So that
[01:36:48.960 --> 01:36:54.000]   means you can surf to any country in the world. I was able to whenever we're in Mexico, watch
[01:36:54.000 --> 01:36:59.120]   Thursday night football, because I told Amazon Prime, we were in Miami. And from their point of
[01:36:59.120 --> 01:37:04.800]   view, we were thanks to Express VPN. So that's a big benefit. But Express VPN also uses a custom
[01:37:04.800 --> 01:37:10.480]   Debian distribution on all their servers that wipes itself and the whole hard drive every single day.
[01:37:10.480 --> 01:37:17.120]   Reboot wipe start over. And as if that weren't enough, their trusted server technology, which
[01:37:17.120 --> 01:37:23.520]   they created, runs in RAM sandbox to can't write to the hard drive. So when you push that big button
[01:37:23.520 --> 01:37:30.160]   on your Express VPN app, on Windows, Mac, Linux, Android, iOS, wherever you are, you're spinning up
[01:37:30.160 --> 01:37:35.600]   a server in that memory that can't write to the drive. And as soon as you disconnect,
[01:37:35.600 --> 01:37:44.400]   goes away with no trace of your visit. Now that's privacy. Express VPN is in a free VPN, but I would
[01:37:44.400 --> 01:37:48.560]   submit that's okay. That's good. You want to support them. Because it's expensive to run those
[01:37:48.560 --> 01:37:53.920]   servers. It's expensive to rotate the IP addresses. So you always have a fresh IP address. It's
[01:37:53.920 --> 01:37:59.360]   expensive to have that custom server software. But it's only it's less than seven bucks a month. I
[01:37:59.360 --> 01:38:05.680]   think that's a fair price to pay for the best VPN service. 100% of your data, of course, is encrypted.
[01:38:05.680 --> 01:38:10.560]   That keeps you safe at open hop spots and anywhere bad guys could snoop on what you're up to.
[01:38:10.560 --> 01:38:17.200]   More than that, Express VPN does this without slowing your connection. They invest in speed.
[01:38:17.200 --> 01:38:22.160]   That's why it's rated number one by CNET and wired and tech radar and countless others is
[01:38:22.160 --> 01:38:28.400]   why I use it. You can run it anywhere, including your router, run it on your router. And you're
[01:38:28.400 --> 01:38:33.280]   not going to get any complaints from people saying, Oh, you know, it slowed down as sluggish. No,
[01:38:33.280 --> 01:38:37.440]   they won't even know. In fact, I can't tell you how many times I've expressed VPN on and
[01:38:37.440 --> 01:38:42.560]   forgotten about it for weeks. One of the best things is it's easy to use download the app on
[01:38:42.560 --> 01:38:48.000]   your phone, your computer, tap one button, that big button you're protected. Stop handing over
[01:38:48.000 --> 01:38:53.840]   personal information to those spy companies that mine your activity and sell your information.
[01:38:53.840 --> 01:38:59.600]   The big tech monopoly protect yourself with the VPN I trust to keep me safe online. Express
[01:38:59.600 --> 01:39:06.240]   VPN, go to exp R E S S VPN dot com slash twit. Sign up for a one year package. That's the best
[01:39:06.240 --> 01:39:10.800]   deal you get an extra three months, 15 months for the price of 12 brings it down well under
[01:39:10.800 --> 01:39:18.640]   seven bucks a month. It's a great deal. Express VPN dot com slash twit. Thank you, Express VPN
[01:39:18.640 --> 01:39:24.960]   for the job you do. And for supporting all of our shows. And thank you for supporting the show.
[01:39:24.960 --> 01:39:30.960]   By using that address Express VPN dot com slash twit. I have been asked Patrick,
[01:39:30.960 --> 01:39:39.840]   to have you pronounce sauna? Is there a special way Finns say sauna? Well,
[01:39:39.840 --> 01:39:47.280]   I think it would be sauna sauna is that the Swedish way or the Finnish way?
[01:39:48.240 --> 01:39:52.960]   That's kind of similar. I maybe some Finns will want to murder me if I say that. But
[01:39:52.960 --> 01:39:58.720]   yeah, it feels kind of similar. Okay, Salna. Salna with that. That's fine. No,
[01:39:58.720 --> 01:40:04.560]   Salna. Do you like do you have a sauna at home? I do not. What kind of
[01:40:04.560 --> 01:40:10.000]   is a great shame for the family? Yes. What kind of Finnish family are you? Oh, it's those
[01:40:10.000 --> 01:40:16.480]   Swedes again. We live in a very old house. Well, very old. I mean, it's 100 years old.
[01:40:17.360 --> 01:40:21.680]   And there is a sauna by the by the water over there, which you can't see.
[01:40:21.680 --> 01:40:27.360]   It's just it's not been the house was abandoned for a long time, not abandoned,
[01:40:27.360 --> 01:40:32.080]   but not really used. And the sauna stopped working. When we moved in, it was like it
[01:40:32.080 --> 01:40:39.600]   hadn't been used in 30 or 40 years. Almost Wow. Regularly. So the sauna was not our first priority
[01:40:39.600 --> 01:40:44.960]   to fix. And also I don't really like it. So you don't like sitting in a hot box and sweating.
[01:40:45.760 --> 01:40:50.320]   Okay. It's exactly how we say it in Mississippi. Salna,
[01:40:50.320 --> 01:40:51.040]   Salna,
[01:40:51.040 --> 01:40:56.880]   Go over. Yeah, go on to sit in the sauna and then do the co plunge. Now,
[01:40:56.880 --> 01:41:05.040]   got some hickory sticks to whip you with. No, not conduct. I'll give you some notes later, Leo.
[01:41:05.040 --> 01:41:10.400]   Okay, thank you very much. Close. Close. You know, one of my goals in life, maybe when I have
[01:41:10.400 --> 01:41:15.280]   more time is to learn the various Southern dial, the Southern regional visits, because
[01:41:15.280 --> 01:41:20.480]   Mississippi's one Carolina is another one. Texas is a third. Florida is another one.
[01:41:20.480 --> 01:41:23.440]   They're you're nodding, Shoshana. Where are you from? The south?
[01:41:23.440 --> 01:41:27.120]   Oh, no, but I started to pick up on the little dialects. Yeah.
[01:41:27.120 --> 01:41:30.800]   People can never figure out where I'm from until I say water. And then they're like, oh,
[01:41:30.800 --> 01:41:30.960]   you know,
[01:41:30.960 --> 01:41:41.600]   Oh, she has a water water. Microsoft. Apple actually, Apple, no layoffs at Apple. Microsoft Amazon,
[01:41:42.320 --> 01:41:50.480]   Google, Google's cutting 12,000 jobs. Microsoft 11,000 Amazon, I think 15,000 big layoffs.
[01:41:50.480 --> 01:41:57.680]   Most of these are in the face of massive hiring over the pandemic. In some cases,
[01:41:57.680 --> 01:42:02.560]   doubling the size of these companies. So they're, you know, I mean, look,
[01:42:02.560 --> 01:42:08.560]   I feel for you if you got laid off. It's, it's horrible. In fact, I'm seeing a lot of Googlers who,
[01:42:08.560 --> 01:42:13.280]   I saw one guy tweet that he's been there for 20 years and jump. Boom, you're done.
[01:42:13.280 --> 01:42:22.480]   That's horrible, but it is kind of understandable if you doubled your, your, your team in three
[01:42:22.480 --> 01:42:27.840]   years that you might want to trim it down as times get a little bit tougher. What I find interesting
[01:42:27.840 --> 01:42:36.000]   is where the layoffs are happening. So Microsoft laid off the entire team behind virtual mixed
[01:42:36.000 --> 01:42:41.600]   reality and HoloLens, according to Windows Central. That's very telling.
[01:42:41.600 --> 01:42:47.920]   Microsoft with HoloLens had kind of a lead in mixed reality.
[01:42:47.920 --> 01:42:55.280]   Alex Kippman, who led the team and kind of left in his space is gone. And it seems that with him,
[01:42:55.280 --> 01:43:05.120]   the spirit, the spunk have departed from the HoloLens team. By the way, Congress has now told
[01:43:05.120 --> 01:43:11.280]   the army, you can't buy anymore of those. You can't get it. There was somebody in the army.
[01:43:11.280 --> 01:43:15.520]   I can't remember his name who loved HoloLens and was really pushing it. Congress has now said,
[01:43:15.520 --> 01:43:17.760]   yeah, no, that's not, that's not going to happen.
[01:43:17.760 --> 01:43:25.120]   Is it crazy that all of them dumped all this money into this technology? And no one was able to
[01:43:25.120 --> 01:43:33.120]   bring a successful product to market? I mean, Oculus, HoloLens magically, magic lens magically.
[01:43:33.120 --> 01:43:38.400]   Yeah. I mean, it's not for a lack of putting money into a Facebook
[01:43:38.400 --> 01:43:42.160]   ton. Made is, that is put $10 billion a year into it.
[01:43:42.160 --> 01:43:48.240]   Why is it so hard to bring this to market? Is it hard or that nobody really wants it, really?
[01:43:48.240 --> 01:43:55.920]   I think it's that they have not, it is a technology looking for a problem to solve and they have not
[01:43:55.920 --> 01:44:01.760]   found that problem yet. But there's got to be something, there's got to be something like,
[01:44:01.760 --> 01:44:09.360]   crazy. I think there's also an issue. If we're talking about AR, I think that the technology
[01:44:09.360 --> 01:44:18.800]   wasn't ready. It's very clunky. But if you start, like VR and AR try to solve very different problems,
[01:44:19.680 --> 01:44:29.120]   and AR could be a really interesting application of the general area of technology. But the
[01:44:29.120 --> 01:44:36.480]   headsets were very clunky and still are. Where it becomes useful is when you can actually use
[01:44:36.480 --> 01:44:43.280]   it and it's wireless. Wireless enough, let's say, we're not there yet from the technology standpoint,
[01:44:43.280 --> 01:44:49.360]   which is, of course, what makes what Apple might do at some point really interesting because
[01:44:49.360 --> 01:44:55.040]   maybe they're waiting for the technology to be there. Although they have apparently canceled their
[01:44:55.040 --> 01:45:04.160]   super lightweight AR only headset, which was planned for what was it, 2025. So maybe even
[01:45:04.160 --> 01:45:09.280]   they think it's not going to go anywhere. Apple, according to Mark Gurman at Bloomberg, delays AR
[01:45:09.280 --> 01:45:15.520]   glasses, plans, cheaper mixed reality headset. The thing that would worry me if I were Apple,
[01:45:15.520 --> 01:45:21.360]   I'd be looking at Microsoft and Meta, who are the leaders in this and the struggles that they're
[01:45:21.360 --> 01:45:28.800]   having and start to wonder, am I putting too much money into this AR space? I agree.
[01:45:28.800 --> 01:45:37.040]   I actually thought 2023 was going to be the maker break here for VR specifically. AR is a slightly
[01:45:37.040 --> 01:45:43.360]   different story, but Meta, it doesn't seem as breaking through. And the bigger
[01:45:43.360 --> 01:45:51.520]   bad sign, I think, is what PlayStation is doing with PSVR 2, which is prohibitively expensive. I
[01:45:51.520 --> 01:45:58.080]   don't think you price a product that's 600 bucks. If you want to make it into a viable, you know,
[01:45:58.080 --> 01:46:02.480]   it's an add-on to the PlayStation 5, which is already 500. It's more expensive than the console
[01:46:02.480 --> 01:46:10.400]   itself. So if you want to get both, it's prohibitive. And the lineup is, I mean, it's not horrible,
[01:46:10.400 --> 01:46:18.640]   but you don't have, they didn't put a lot of work into exclusives, which would sell the platform.
[01:46:18.640 --> 01:46:24.400]   So I think they're going to be making money on selling the device, but they're not looking to
[01:46:24.400 --> 01:46:30.000]   have a strong installed base, which will be a significant market for them in the future. That's
[01:46:30.000 --> 01:46:37.200]   how I'm reading the TV use here. The product is great, but I don't know that it is the breakthrough
[01:46:37.200 --> 01:46:45.680]   VR product that I was kind of hoping would be needed to make VR break into like between
[01:46:45.680 --> 01:46:53.680]   Facebook or Oculus. And this would be, okay, now VR is here. And looking at both of those, I'm like,
[01:46:53.680 --> 01:46:58.960]   well, it's kind of like it has been for a while. It's a little bit better, but I don't think it's
[01:46:58.960 --> 01:47:03.600]   going to break the area. This is succeeded in his gaming. I mean, I could see what PlayStation
[01:47:03.600 --> 01:47:11.520]   might think that that's their best bet. Microsoft and meta of late, both have focused on productivity,
[01:47:11.520 --> 01:47:16.960]   which seems nuts to me. Why would anybody want to go put on a helmet and get into a team's meeting
[01:47:16.960 --> 01:47:23.040]   is beyond. Yeah, I remember that. And like the lack of legs, I was like, and then they put the
[01:47:23.040 --> 01:47:28.320]   fake legs on to pretend they had legs. Like that's not a great place to be. But something Brianna said,
[01:47:28.320 --> 01:47:33.440]   I think really hit me where it's just like, it's like in search of a, in search of a problem,
[01:47:33.440 --> 01:47:37.680]   it's a solution in search of a problem. And I'm sure that it'll be useful down the line.
[01:47:37.680 --> 01:47:44.160]   But I'm not sure it'll be useful down the line. I think it's a gimmick. Nobody wants it. It makes
[01:47:44.160 --> 01:47:49.520]   a certain percentage around 11% of people sick to their stomachs. That's not a good look for a
[01:47:49.520 --> 01:47:54.400]   consumer product. I don't know. You don't want to be sick. I don't know.
[01:47:55.920 --> 01:47:59.120]   All it takes is one person in the family to say, you know, this is, I'm getting,
[01:47:59.120 --> 01:48:03.440]   and that's it. You know, throw it out. I'm not going to do it.
[01:48:03.440 --> 01:48:11.440]   Then there's of course the more speculative. And I guess you could push this use of it as
[01:48:11.440 --> 01:48:16.960]   augmented reality where if only Apple could invent a way to make it look like regular spectacles,
[01:48:16.960 --> 01:48:20.720]   and you put it on and you don't look like a complete dork and you just can see what's going on,
[01:48:20.720 --> 01:48:25.120]   but you're getting a heads up display on the world around you, I could see that maybe and
[01:48:25.120 --> 01:48:31.600]   it wouldn't make you nauseous, I don't think. And you know, it'd be useful, wouldn't be specifically
[01:48:31.600 --> 01:48:38.720]   for productivity. Trying to do this to a menu book and bopping my finger in this weird gesture
[01:48:38.720 --> 01:48:44.960]   that you use is terrible. It's a terrible experience sliding windows around Tom Cruise notwithstanding.
[01:48:44.960 --> 01:48:52.160]   It's a because it doesn't work well enough. It doesn't work well enough because I think
[01:48:52.160 --> 01:49:01.360]   the science fiction ready player one version of VR still has promise, but we're definitely not
[01:49:01.360 --> 01:49:07.280]   there yet. Well, essentially, I think it has promise, but you'd have to have a special treadmill.
[01:49:07.280 --> 01:49:13.920]   That's you run in various direction. No, not going that far. That's the ready player one. Look,
[01:49:13.920 --> 01:49:19.120]   I think, yeah, no, of course. Until you can jack a cable into the back of your head,
[01:49:19.760 --> 01:49:25.520]   like in neuromancer, I don't know. And I don't see that being, by the way, this is another one of
[01:49:25.520 --> 01:49:31.600]   Elon's exciting technologies, Neuralink. I'm not going to be the first in line to get my brain
[01:49:31.600 --> 01:49:38.240]   modified by Elon Musk. Oh, God, that's a whole thing. I've been studying Neuralink a lot lately
[01:49:38.240 --> 01:49:43.520]   and all the problems it has. No, but, you know, I think the threat to Apple actually bringing this
[01:49:43.520 --> 01:49:48.720]   to market is a lot bigger than I think people are considering. Look at what's happened to
[01:49:48.720 --> 01:49:55.520]   Meta, right? Like Mark Zuckerberg comes forward puts this big future vision forward for the company,
[01:49:55.520 --> 01:50:01.120]   where it's basically betting on this technology. It doesn't seem to have come to fruition,
[01:50:01.120 --> 01:50:06.480]   and it's destroyed the stock pricing costs people their jobs. I think it's not an exaggeration to
[01:50:06.480 --> 01:50:14.000]   say it's a limiting Facebook's ability to attract engineers to their team. And it's really put the
[01:50:14.000 --> 01:50:19.680]   future of Facebook into question. You know, the storyline for many of us with Apple for a long
[01:50:19.680 --> 01:50:25.360]   time has been, look, this new AR thing is coming. It's going to be like the iPhone all over again.
[01:50:25.360 --> 01:50:29.040]   They're going to hit out the park and this is going to be the next 20 years of computing,
[01:50:29.040 --> 01:50:33.600]   just like the desktop was 20 years of computing and the mobile phone was 20 years of computing.
[01:50:33.600 --> 01:50:39.840]   Everyone is failing at this though. And gamers, we are the people that are going to adopt this
[01:50:39.840 --> 01:50:45.120]   first. I've played some PlayStation VR games. I've got every single headset that exists because
[01:50:45.120 --> 01:50:51.200]   I've done VR development. It's not a place I spend time when I game generally. It's just not.
[01:50:51.200 --> 01:50:58.160]   It's never going to be. So I think the stakes for Apple in bringing forward a product to market,
[01:50:58.160 --> 01:51:03.120]   this just another pleasant distraction. I think there are very real concerns about what they can
[01:51:03.120 --> 01:51:08.800]   do to their stock price and their ability to attract talent. Our son Michael who is 19 and loves
[01:51:09.360 --> 01:51:14.800]   VR currently is playing that I guess it's a Madden game where you can be the quarterback
[01:51:14.800 --> 01:51:21.360]   and a team. And I see him standing for the TV with his headset on going like this a lot.
[01:51:21.360 --> 01:51:28.400]   Throw it a lot of passes. But he likes it. It keeps him somewhat engaged. We got the Oculus
[01:51:28.400 --> 01:51:33.600]   Pro, the $1600 meta because I wanted to say, well, what's the best out there? And if that's
[01:51:33.600 --> 01:51:38.560]   the best we can do, it's not particularly compelling. Is it like VR?
[01:51:39.280 --> 01:51:42.640]   There are a lot of people. I don't think it's enough to make a market.
[01:51:42.640 --> 01:51:49.920]   No, no, it's not. Well, it is a market. It's just not a big market and certainly not an Apple-sized
[01:51:49.920 --> 01:51:59.280]   market. But I think it would be foolish for any of us to dismiss Apple out of hand. They have
[01:51:59.280 --> 01:52:04.640]   succeeded where many companies have failed repeatedly in the years before they brought their
[01:52:04.640 --> 01:52:12.320]   thing to market. So we'll never know before they actually put it out. They have failed at a couple
[01:52:12.320 --> 01:52:19.360]   of things. But I think the giant home pod, I thought that was hysterical. Here's a $300
[01:52:19.360 --> 01:52:26.720]   single speaker, $300 bucks, that's $600 for the speaker pair that has Siri in it that they
[01:52:26.720 --> 01:52:31.600]   discontinued two years ago saying nobody wants to pay $400 for this. And they brought it,
[01:52:31.600 --> 01:52:36.880]   they just brought it back last week. And in euros, it's more expensive than the first one.
[01:52:36.880 --> 01:52:42.320]   I don't understand that one anyway. You put in an article, which I think was an interesting
[01:52:42.320 --> 01:52:48.800]   article from the Wall Street Journal talking about how Apple is not doing layoffs. And what I think
[01:52:48.800 --> 01:52:56.320]   is most telling is the number of hires that happened between September 2019 and last September of last
[01:52:56.320 --> 01:53:08.480]   year, Facebook grew 94%, almost doubled in size. Google 57%, Microsoft 53% massive hiring.
[01:53:08.480 --> 01:53:14.880]   Meanwhile, Apple only grew 20%. And by itself, that would explain, you know,
[01:53:14.880 --> 01:53:23.040]   why it's not having to fire people. The article also says not having gourmet free lunches,
[01:53:24.640 --> 01:53:29.600]   which is by the way, the first thing Elon killed at Twitter, he sold all the kitchen gear.
[01:53:29.600 --> 01:53:39.520]   So Apple is maybe being more fiscally prudent. But we had heard rumors that they were transferring
[01:53:39.520 --> 01:53:48.160]   people from Mac and iOS divisions over to the AR division, trying to get that business off the
[01:53:48.160 --> 01:53:52.720]   ground. I guess it makes sense if you're Apple, you've got a single product that's almost half,
[01:53:52.720 --> 01:53:57.920]   I think a little more than half of your entire revenue, the iPhone. And that's not going to last
[01:53:57.920 --> 01:54:02.960]   forever. It's getting saturated. It's really important for you to figure out what's next, right?
[01:54:02.960 --> 01:54:10.080]   Yeah, I think it's a smart thing to do. You know, you see companies just kind of rise and die.
[01:54:10.080 --> 01:54:15.520]   And I think that they're actively trying not to do that is really smart related. Also, I'm not
[01:54:15.520 --> 01:54:19.760]   sure if you guys knew, but they have a $100 thermometer. I was buying a new one.
[01:54:21.360 --> 01:54:25.840]   Yeah, I was, I have an Apple gift card. I'm like, my friend told me they had a thermometer and I'm
[01:54:25.840 --> 01:54:30.320]   like, this is five times the price of like a normal thermometer. But it's got an Apple logo on it.
[01:54:30.320 --> 01:54:35.600]   I've been like, I'm trying to understand like, it's this special thermometer. Like, can I do
[01:54:35.600 --> 01:54:42.560]   anything that I can't do with it? To be fair, to be fair, it is made by why things not Apple,
[01:54:42.560 --> 01:54:50.160]   they sell it. Oh, okay. It's a it's a temporal thermometer. Let's you take the temperature of
[01:54:50.160 --> 01:54:55.440]   your current time state or something. I don't know where you are, which dimension.
[01:54:55.440 --> 01:55:01.040]   This is one of those things you put to your forehead and that gives you color coded fever
[01:55:01.040 --> 01:55:04.400]   indicators. These are the I just look at these and I go, that's gonna break.
[01:55:04.400 --> 01:55:08.320]   Oh, that's gonna break. I'm gonna live it. I'm gonna live it.
[01:55:08.320 --> 01:55:13.360]   The blood pressure when they sell is pretty good. Frankly, I actually have a lot of why things
[01:55:13.360 --> 01:55:19.280]   I have a white thing scale, white things, blood pressure cuff, why things other stuff. I can't
[01:55:19.280 --> 01:55:25.200]   remember what it is. Why things showed at CES, a little egg you put in your toilet to test your
[01:55:25.200 --> 01:55:31.120]   pee. So they're really expanding, I think, into new exciting new Apple.
[01:55:31.120 --> 01:55:37.600]   You are questions about that egg just super quick. Like, do you have to use the same one? Is it a
[01:55:37.600 --> 01:55:42.320]   set of eggs or is and you like, you push that egg or no, first of all, don't flush it. I think
[01:55:42.320 --> 01:55:46.960]   it's attached. I asked the same questions. I don't blame you, Shoshana. It can distinguish
[01:55:46.960 --> 01:55:55.920]   different people's pee. And it can distinguish between pee and toilet water. So you don't have
[01:55:55.920 --> 01:56:02.240]   to worry about that. And it can do a lot of different things like vitamin deficiencies.
[01:56:02.240 --> 01:56:07.600]   I imagine the biggest market for this will be companies that want to p test their plays at all
[01:56:07.600 --> 01:56:14.240]   times. But maybe they'll be a home market. Certainly, the quantified self is a very big
[01:56:15.520 --> 01:56:20.320]   business. It could be medically useless. I think you're right.
[01:56:20.320 --> 01:56:29.760]   There's probably a reason why Apple is looking into AR and VR and not that area of tech.
[01:56:29.760 --> 01:56:36.160]   Size of market might be one. But I think it's smart that Apple is looking into,
[01:56:36.160 --> 01:56:44.080]   obviously, it's smart. They understand they have disrupted many markets and they understand
[01:56:44.080 --> 01:56:50.960]   what they're not above being disrupted themselves. I think what Facebook is doing is smart.
[01:56:50.960 --> 01:56:56.800]   The way they're going about it is not a smart. Staking the whole company on this and being so
[01:56:56.800 --> 01:57:04.240]   public and doing it research, at least somewhat secret or mentioning, "Okay, we're looking into this."
[01:57:04.240 --> 01:57:13.200]   That's the way Apple and most companies usually do it. And Microsoft was trying it out. They didn't
[01:57:13.200 --> 01:57:18.400]   say all of a sudden, "Well, goodbye everything else we were doing. We're now the whole lens company."
[01:57:18.400 --> 01:57:28.240]   Let's be very, very clear on this. If Apple never brings this to market, we are all going to
[01:57:28.240 --> 01:57:33.360]   want something very, very important. They're a host of Apple technologies that they've developed
[01:57:33.360 --> 01:57:38.720]   and released at every single state of the union. They're tremendously, tremendously helpful.
[01:57:38.720 --> 01:57:46.080]   Ten years ago, Apple did not even have a real 3D building API in Xcode, something that was a
[01:57:46.080 --> 01:57:51.200]   professional tool game developers could use. That exists today. It's good. There's all kinds of
[01:57:51.200 --> 01:57:57.360]   spatial recognition stuff that developers can use. The entire unified development stack that
[01:57:57.360 --> 01:58:04.080]   Apple has is much better off today for all this R&D. Apple is poured into this project. And that's
[01:58:04.080 --> 01:58:09.680]   a win. I just think if we're looking at, "You asked me the day you put a gun to my head and said,
[01:58:09.680 --> 01:58:15.200]   "Brian, what do you think is the next 20 years of computing? What technology are you going to bet on?"
[01:58:15.200 --> 01:58:23.920]   It's not AR/VR. I'm going to bet on chat GPT and get that kind of AI. That stuff I could see
[01:58:23.920 --> 01:58:30.400]   using every single day. My husband, he has four Hugo Awards for science fiction. He was working on a
[01:58:30.400 --> 01:58:37.520]   new writing project. Frank's problem is it's so hard for him to write that first draft.
[01:58:37.520 --> 01:58:44.640]   He entered it into chat GPT, had it generate something. It was terrible. It was generic,
[01:58:44.640 --> 01:58:51.520]   but it was a place for him to go through and then start writing. It was an outline for him
[01:58:51.520 --> 01:58:57.840]   to then start shaping into something that was actually very good. That is a useful tool that
[01:58:57.840 --> 01:59:01.920]   you could build into Word or Pages. They're a million applications.
[01:59:01.920 --> 01:59:06.720]   Apparently he's doing that. They were big investor in OpenAI. The creators of
[01:59:06.720 --> 01:59:11.920]   Jet GPT and the rumors are going to put $10 billion into it and are going to incorporate it into
[01:59:11.920 --> 01:59:17.920]   office. But that's just one of many. It's really interesting how quickly people are coming up with
[01:59:17.920 --> 01:59:26.960]   ways to use this. One of many, many uses, including in search, document summaries, language translation,
[01:59:26.960 --> 01:59:35.120]   computer programming, it's a very interesting time. That's a good example of how it's so hard
[01:59:35.120 --> 01:59:40.160]   to predict what's going to change the world. If Meta had put $10 billion a year into that,
[01:59:40.160 --> 01:59:46.560]   they might have a lot more to show for it. But you just sometimes pick the wrong horse.
[01:59:49.680 --> 01:59:58.560]   I think when we look back, 2022 is the year that we will have talked about Elon Musk way too much,
[01:59:58.560 --> 02:00:02.640]   but really the important part of it, the important part of that here, tech-wise,
[02:00:02.640 --> 02:00:09.200]   is certainly generative AI. We started talking about it with, it was Lambda, interestingly,
[02:00:09.200 --> 02:00:14.400]   with that engineer that thought it had become conscious. It's a Google point.
[02:00:16.240 --> 02:00:22.560]   They got scooped by them. You had the deli to that arrived and then mid-journey.
[02:00:22.560 --> 02:00:27.600]   So it was images first and now Chad GPT. It's funny because it has been bubbling. It's one of
[02:00:27.600 --> 02:00:35.360]   those things AI in general, which is a big umbrella word. But it's almost like a cold fusion.
[02:00:35.360 --> 02:00:42.720]   It's not a cold fusion, sorry, fusion in general, which is like it's always five years away or 10
[02:00:42.720 --> 02:00:48.000]   years away. We're like, "Okay, you've been saying it's like better batteries." You've been saying
[02:00:48.000 --> 02:00:55.520]   that for 20 years. We don't believe in it. And AI has been in development. It's been in development
[02:00:55.520 --> 02:01:05.440]   at least 15 years with those GAN and those kinds of networks. Now, all of a sudden, 2022,
[02:01:05.440 --> 02:01:12.320]   it started working, not just as proof of concept or on tiny images, thumbnail images.
[02:01:12.320 --> 02:01:16.960]   No, it's amazing. It started really working and it's absolutely going to change everything. And
[02:01:16.960 --> 02:01:24.320]   we're only at Chad GPT, like GPT 3 or 3.5. Imagine GPT 20. It's all of a sudden,
[02:01:24.320 --> 02:01:29.760]   even if there are huge issues with it, with accuracy and
[02:01:33.280 --> 02:01:40.320]   copyright and a lot of problems that have to be figured out, it will be incredibly useful for
[02:01:40.320 --> 02:01:48.160]   so many things. And it made Siri obsolete and Google Assistant obsolete in three weeks. You're
[02:01:48.160 --> 02:01:53.680]   like, "Okay, I want this to be my virtual assistant." Not the dumb one that doesn't know what I'm
[02:01:53.680 --> 02:02:00.720]   asking for. It's definitely the next one. I got to ask you as a comic guy, one of the big
[02:02:00.720 --> 02:02:08.800]   departments that got asked at Amazon is Comixology. That's not good.
[02:02:08.800 --> 02:02:15.040]   Well, I mean, Comixology has been transformed, the purchase by Amazon.
[02:02:15.040 --> 02:02:25.360]   I guess many people would tell you, I think it's doing okay. It's a viewer now, more than anything
[02:02:25.360 --> 02:02:32.480]   else. You can't repurchase anything on Comixology. I think Comixology was already toast, kind of,
[02:02:32.480 --> 02:02:40.640]   for most people who liked it. Is there another choice? Yeah. Not really. You can go to Marvel
[02:02:40.640 --> 02:02:48.560]   directly or DC, which were, I guess, Comixology licensed their app tech to them because it was
[02:02:48.560 --> 02:02:54.160]   really the same app with the skin. I don't know if Amazon put a stop to that, but no, there isn't
[02:02:54.160 --> 02:02:58.480]   really an alternative. It's Comixology or nothing. They have the whole market.
[02:02:58.480 --> 02:03:04.240]   I guess with Marvel having its own platform, that's got to have eaten a little bit away
[02:03:04.240 --> 02:03:10.720]   from Comixology future. Maybe Comixology, as in Amazon envisioned, it was pretty much done.
[02:03:10.720 --> 02:03:16.560]   It's just a viewer of comic books. Yeah, they sell everything on Amazon itself.
[02:03:16.560 --> 02:03:22.160]   But if you want to buy stuff, because everything Marvel is available in Comixology,
[02:03:22.160 --> 02:03:25.920]   so if you're going to buy it, I don't think you want to buy it on Marvel's platform.
[02:03:25.920 --> 02:03:30.400]   You want to buy it on Comixology, because then you also have on the same platform,
[02:03:30.400 --> 02:03:37.360]   DC, and a lot of manga as well, which for many people is very important. I bought a bunch of
[02:03:37.360 --> 02:03:43.280]   manga on Comixology. It works really well through Amazon directly. So you don't want to go to Marvel
[02:03:43.280 --> 02:03:48.080]   because you're restricting yourself to buying this stuff on the platform that doesn't have everything.
[02:03:48.240 --> 02:03:58.480]   So we talked to Gugelma editor and chief last week. She was on the panel about these AI written
[02:03:58.480 --> 02:04:09.040]   articles. 75 of them written in mostly CNET's family finance articles that were kind of...
[02:04:09.040 --> 02:04:15.680]   She said it's the articles that no reporter wants to write. And CNET has said the AI generated it
[02:04:15.680 --> 02:04:25.360]   and then humans reviewed it. The Verge has been a little bit more critical of all this. I thought,
[02:04:25.360 --> 02:04:29.840]   well, that's reasonable. And there is a technology which people have been critical of, and I don't
[02:04:29.840 --> 02:04:35.680]   think for any good reason where financial numbers are inserted into financial articles automatically.
[02:04:35.680 --> 02:04:43.440]   That's no big deal. But Miyasato and James Vincent writing at the Verge say that something
[02:04:43.440 --> 02:04:49.200]   maybe a little bit more malign is going on because remember CNET, which was bought by
[02:04:49.200 --> 02:05:00.880]   CBS for some billions of dollars, and then they sold it to Red Ventures, an equity capital firm for
[02:05:00.880 --> 02:05:08.320]   a quarter of what they paid for it. And as often happens when these equity capital firms come along,
[02:05:08.320 --> 02:05:13.520]   they start to focus on profits. They're usually purchases that are driven heavily by debt and
[02:05:13.520 --> 02:05:20.000]   they need money. And it looks like maybe some of this automated content generation
[02:05:20.000 --> 02:05:28.560]   is related to the old SEO farm, the content farms. This was big, I don't know, 10 years ago,
[02:05:28.560 --> 02:05:33.520]   when people noticed, well, see, you go to Google and you search for belt buckles.
[02:05:35.920 --> 02:05:40.160]   If you have an article about belt buckles, which no one in the world wants to write,
[02:05:40.160 --> 02:05:45.360]   you're going to get those hits. And then you can have affiliate links on there selling belt buckles
[02:05:45.360 --> 02:05:55.760]   and make a lot of money. I remember when Google kind of ended up clobbering these content forms,
[02:05:55.760 --> 02:06:01.760]   but maybe the way to do it is to buy an established brand like CNET and start creating a lot of stories
[02:06:02.880 --> 02:06:07.120]   using AI, quickly, cheaply generated stories to do this.
[02:06:07.120 --> 02:06:11.920]   Don't you think there's kind of a long term cost to that, though, Lea?
[02:06:11.920 --> 02:06:18.800]   Well, it's debases the brand. But remember, that's the problem with these equity capital
[02:06:18.800 --> 02:06:24.560]   companies. They don't care. Right. It seems like it's something that could work in the short term.
[02:06:24.560 --> 02:06:30.880]   Like if I click on a podcast on your network, I know every single time, like the host is going
[02:06:30.880 --> 02:06:35.120]   to be very knowledgeable. I know there's going to be a certain journalistic standard.
[02:06:35.120 --> 02:06:41.920]   I know there's going to be a certain level decorum and, you know, like journalistic standards to it,
[02:06:41.920 --> 02:06:46.080]   right? You can switch over tomorrow to having chat GPT.
[02:06:46.080 --> 02:06:50.800]   Yeah, we'd make a lot of money for a year until everybody figured that and then we'd have 100%.
[02:06:50.800 --> 02:06:56.640]   100%. But that's what equity capital companies do. They squeeze the value out of a company
[02:06:56.640 --> 02:07:01.920]   till the company is a husk of itself and then they throw it away. That would be very sad if
[02:07:01.920 --> 02:07:07.440]   that's what happens to CNET. This is from the Verge article, Red Ventures, the current owners'
[02:07:07.440 --> 02:07:13.040]   business model is straightforward and explicit. It publishes content designed to rank highly
[02:07:13.040 --> 02:07:18.720]   in Google search for, quote, "high intent queries" and then monetizes that traffic with lucrative
[02:07:18.720 --> 02:07:26.160]   affiliate links. In addition to CNET, Red Ventures bought the PointSky, bank rate, and creditcards.com,
[02:07:26.160 --> 02:07:32.880]   all of which monetize through credit card affiliate fees. In fact, the CNET AI stories at the center
[02:07:32.880 --> 02:07:38.560]   of the controversy are exactly this strategy. Can you buy a gift card with a credit card?
[02:07:38.560 --> 02:07:46.880]   Is one. What is Zell and how does it work? Is another. Now, it's funny. Lindsay Tarantine,
[02:07:46.880 --> 02:07:51.840]   who was a regular on this show until she became a higher up in the hierarchy and
[02:07:51.840 --> 02:07:57.040]   Kade Guljelmo, who has been on the show many times. I have huge respect for many of my friends'
[02:07:57.040 --> 02:08:02.480]   work at CNET, but I don't know these Red Venture guys. I'm really starting to think that this is
[02:08:02.480 --> 02:08:10.160]   really what's going on at CNET. A lot of the CNET is now directing questions to Lance Davis,
[02:08:10.160 --> 02:08:17.440]   Vice President of Content at Red Ventures. It's my guess that this is who's driving
[02:08:18.160 --> 02:08:25.280]   this SEO farming. That might in fact be the end game for CNET. That would be very sad.
[02:08:25.280 --> 02:08:32.960]   I mean, those articles that no one wants to write, that's what you train junior reporters on.
[02:08:32.960 --> 02:08:40.960]   That's how they become good reporters. I don't have a problem with AI writing stories, but if
[02:08:41.520 --> 02:08:51.600]   the entire goal of the site is to generate affiliate links via SEO, you nailed it. It's over for that
[02:08:51.600 --> 02:08:58.960]   site. I hope that's not the case at CNET right now. One funny thing a couple of weeks ago as chat
[02:08:58.960 --> 02:09:05.920]   TPT was getting better, I saw it integrated with Zapier. I messaged my team. I'm like, "Oh my gosh,
[02:09:05.920 --> 02:09:13.360]   guys, we can automate some stuff." I was saying, "Why don't we try and disclose it, but why don't we
[02:09:13.360 --> 02:09:19.760]   try one of these AI sites and have it write some of our press releases or some content that's not
[02:09:19.760 --> 02:09:26.080]   scholar written on our site?" I freaked out my colleague so much. She's not very tech savvy,
[02:09:26.080 --> 02:09:30.240]   which is fine. She's a normal human being, but she's like, "Shawn, you're freaking me out. I don't
[02:09:30.240 --> 02:09:36.000]   want AI taking my job. This is too fast. I like this job, but it was so funny to watch everyone's
[02:09:36.000 --> 02:09:41.440]   reactions." Then also think through, maybe there's some stuff that we could use to expand. Maybe we
[02:09:41.440 --> 02:09:47.600]   try it on newsletters and stuff. It's sad to see people use it for bad, but of course, people
[02:09:47.600 --> 02:09:53.760]   will always with tech. It's interesting. I'm curious what good it can do if it can take pressure off
[02:09:53.760 --> 02:10:00.160]   of reporters who have to write seven stories. Maybe they get a first draft from it. Then they
[02:10:00.160 --> 02:10:04.400]   go and put in their style and edit it something really good, but maybe parts of it are done for
[02:10:04.400 --> 02:10:09.360]   them. Not that we should have reporters have to write 10 stories a day. That's not sustainable either,
[02:10:09.360 --> 02:10:13.520]   but maybe they get something out of it. Maybe it makes some of the models.
[02:10:13.520 --> 02:10:20.080]   And look at how your husband's using it, Brianna, Frank's using it as just a starting point.
[02:10:20.080 --> 02:10:25.760]   I'm not against the AI. In fact, to CNET's credit, they say now they've paused using AI written
[02:10:25.760 --> 02:10:36.800]   stories entirely. I think it's been... I just wanted to say what I'm concerned about it,
[02:10:36.800 --> 02:10:43.680]   but what I'm thinking about is what happens to the... I guess we're circling back to the user-generated
[02:10:43.680 --> 02:10:51.040]   content version of the web, which is the one we live in now. What happens to it when content
[02:10:51.040 --> 02:10:57.920]   generation costs a hundredth... I don't want to say it's got to be free, but a hundredth
[02:10:57.920 --> 02:11:07.760]   of what it is. Now, if you want to generate an article about belt buckles, you can generate
[02:11:07.760 --> 02:11:15.120]   an article about every single piece of clothing or apparel in the world, in all languages,
[02:11:15.680 --> 02:11:23.120]   with three clicks, or not three clicks, but in three days. And you can turn them into videos
[02:11:23.120 --> 02:11:31.840]   to put on YouTube and on TikTok and images on Instagram, and content becomes essentially
[02:11:31.840 --> 02:11:40.080]   non-issue to create. I don't know, especially in a world where money on the web is
[02:11:41.200 --> 02:11:48.240]   happens through display ads and ads in general. I don't know what that does to
[02:11:48.240 --> 02:11:56.240]   our current version of the internet. It might not change it much. I think it
[02:11:56.240 --> 02:12:04.400]   does have a huge impact. I don't know that it will be good or bad, but generating content
[02:12:04.400 --> 02:12:10.400]   that much more easily is almost for free. Kind of freaks me out a little bit, because that's not
[02:12:10.400 --> 02:12:16.640]   step one. It's not even step two. It's maybe step five in five or ten years. Maybe I think less than
[02:12:16.640 --> 02:12:26.080]   that, but it changes the nature of our user experience on the web.
[02:12:26.080 --> 02:12:28.480]   One thing I can guarantee... I'm concerned about that.
[02:12:28.480 --> 02:12:30.960]   This is a story we'll be talking a lot about in the next few years.
[02:12:30.960 --> 02:12:35.680]   AI... Oh, yeah. Everywhere. I want to take a little break, and then we have some final
[02:12:36.320 --> 02:12:41.920]   stories coming up in just a little bit. Our show today brought to you by viewers like you, our
[02:12:41.920 --> 02:12:46.800]   wonderful club Twit members. I just want to put in a plug for club Twit. It is a great place
[02:12:46.800 --> 02:12:53.360]   to hang, but it's also a great way to support the Twit network. Club Twit is $7 a month,
[02:12:53.360 --> 02:12:58.320]   or you can get a yearly $84 a year package. There's also corporate memberships. It gives you
[02:12:58.320 --> 02:13:02.960]   ad-free versions of all of our shows. It gives you the Twit Plus feed, which includes shows we
[02:13:02.960 --> 02:13:07.200]   don't put out anywhere else like hands-on, Mac-a-tosh with Micah, Paul Therat's hands-on
[02:13:07.200 --> 02:13:13.200]   Windows, the untitled Linux show that gives Fizz and more. Plus, you get the fantastic Discord,
[02:13:13.200 --> 02:13:20.160]   which is a community of like-minded geeks that is so much fun. You've been in there
[02:13:20.160 --> 02:13:24.000]   while we're doing the show, Breanne has been in there talking about restoring pinball machines,
[02:13:24.000 --> 02:13:32.640]   which is great. I hang out talking about coding in there. We have sections in the Discord
[02:13:32.640 --> 02:13:38.560]   to talk about pretty much anything geeks are interested in, including books and booze and all
[02:13:38.560 --> 02:13:43.200]   sorts of things. We also have, thanks to our community manager, Aunt Pruitt does a great job.
[02:13:43.200 --> 02:13:49.920]   A lot of AMAs and fireside chats. When two Dows coming up, host of all about Android. That'll
[02:13:49.920 --> 02:13:55.200]   be February 9th. We're going to do a very special triangulation episode. We'll start it in the club,
[02:13:55.200 --> 02:14:00.160]   so club members can ask questions of Daniel Suarez. His new book comes out this month,
[02:14:00.160 --> 02:14:06.960]   actually early next month. It's going to be called Critical Mass, a sequel to Delta V. I cannot wait.
[02:14:06.960 --> 02:14:12.320]   I got my advance copy. Daniel will be our guest on February 10th. It'll end up being a triangulation,
[02:14:12.320 --> 02:14:18.560]   but club members will get early access to that. We also have Samable Samed, our car guy,
[02:14:18.560 --> 02:14:22.960]   coming up March 2nd. But that's just a sample. If you're not a member of the club, it supports
[02:14:22.960 --> 02:14:28.080]   us. It keeps the lights on, keeps our staff employed, helps us make new content. Please
[02:14:28.640 --> 02:14:33.760]   go to twit.tv/clubtwit and join the fun. I think it's worth it.
[02:14:33.760 --> 02:14:38.320]   Hey, everybody. Leo LePorte here. I'm the founder and one of the hosts at the Twit
[02:14:38.320 --> 02:14:42.960]   podcast network. I want to talk to you a little bit about what we do here at Twit,
[02:14:42.960 --> 02:14:51.680]   because I think it's unique. I think for anybody who is bringing a product or a service to a tech
[02:14:51.680 --> 02:14:57.680]   audience, you need to know about what we do here at Twit. We've built an amazing audience of
[02:14:57.680 --> 02:15:05.760]   engaged, intelligent, affluent listeners who listen to us and trust us when we recommend a product.
[02:15:05.760 --> 02:15:10.480]   Our mission statement is to build a highly engaged community of tech enthusiasts.
[02:15:10.480 --> 02:15:17.440]   Already, your ears should be perking up at that because highly engaged is good for you.
[02:15:17.440 --> 02:15:21.840]   Tech enthusiasts, if that's who you're looking for, this is the place. We do it by offering
[02:15:21.840 --> 02:15:27.440]   them the knowledge they need to understand and use technology in today's world. I hear from our
[02:15:27.440 --> 02:15:32.720]   audience all the time. Part of that knowledge comes from our advertisers. We are very careful.
[02:15:32.720 --> 02:15:39.920]   We pick advertisers with great products, great services with integrity and introduce them
[02:15:39.920 --> 02:15:46.960]   to our audience with authenticity and genuine enthusiasm. That makes our host red ads different
[02:15:46.960 --> 02:15:53.840]   from anything else you can buy. We are literally bringing you to the attention of our audience
[02:15:53.840 --> 02:16:00.160]   and giving you a big fat endorsement. We like to create partnerships with trusted brands,
[02:16:00.160 --> 02:16:05.920]   brands who are in it for the long run, long term partners that want to grow with us.
[02:16:05.920 --> 02:16:12.320]   And we have so many great success stories. Tim Broome, who founded ITProTV in 2013,
[02:16:12.320 --> 02:16:17.600]   started advertising with us on day one has been with us ever since. He said, quote,
[02:16:17.600 --> 02:16:23.200]   "We would not be where we are today without the Twit network." I think the proof is in the pudding.
[02:16:23.200 --> 02:16:28.480]   Advertisers like ITProTV and Audible that have been with us for more than 10 years,
[02:16:28.480 --> 02:16:35.120]   they stick around because their ads work. And honestly, isn't that why you're buying advertising?
[02:16:35.120 --> 02:16:40.000]   You get a lot with Twit. We have a very full service attitude. We almost think of it as
[02:16:40.000 --> 02:16:47.120]   kind of artisanal advertising, boutique advertising. You'll get a full service continuity team.
[02:16:47.120 --> 02:16:52.720]   People who are on the phone with you, who are in touch with you, who support you with everything
[02:16:52.720 --> 02:17:00.480]   from copywriting to graphic design. So you are not alone in this. We embed our ads into the
[02:17:00.480 --> 02:17:05.360]   shows. They're not added later. They're part of the shows. In fact, often,
[02:17:05.360 --> 02:17:09.680]   they're such a part of our shows that our other hosts will chime in on the ads saying,
[02:17:09.680 --> 02:17:15.200]   "Yeah, I love that." Or just the other day, one of our hosts said, "Man, I really got to buy that."
[02:17:15.200 --> 02:17:21.680]   That's an additional benefit to you because you're hearing people, our audience trusts saying,
[02:17:21.680 --> 02:17:27.680]   "Yeah, that sounds great." We deliver, always over deliver on impressions. So you know you're
[02:17:27.680 --> 02:17:33.200]   going to get the impressions you expect. The ads are unique every time. We don't pre-record
[02:17:33.200 --> 02:17:38.400]   them and roll them in. We are genuinely doing those ads in the middle of the show. We'll give
[02:17:38.400 --> 02:17:43.200]   you great onboarding services, ad tech with pod sites that's free for direct clients.
[02:17:43.200 --> 02:17:47.840]   Gives you a lot of reporting, gives you a great idea of how well your ads are working.
[02:17:47.840 --> 02:17:51.760]   You'll get courtesy commercials. You actually can take our ads and share them across
[02:17:51.760 --> 02:17:56.640]   social media and landing pages. That really extends the reach. There are other free goodies,
[02:17:56.640 --> 02:18:02.320]   too, including mentions in our weekly newsletter that sent to thousands of fans, engaged fans who
[02:18:02.320 --> 02:18:07.040]   really want to see this stuff. We give you bonus ads and social media promotion, too.
[02:18:07.040 --> 02:18:14.480]   So if you want to be a long-term partner, introduce your product to a savvy, engaged tech audience.
[02:18:14.480 --> 02:18:20.880]   Visit twit.tv/advertise. Check out those testimonials. Mark McCrary is the CEO of
[02:18:20.880 --> 02:18:26.160]   Authentic. You probably know him, one of the biggest original podcast advertising companies.
[02:18:26.160 --> 02:18:32.720]   We've been with him for 16 years. Mark said the feedback from many advertisers over 16 years across
[02:18:32.720 --> 02:18:39.680]   a range of product categories, everything from razors to computers, is that if ads and podcasts
[02:18:39.680 --> 02:18:44.880]   are going to work for a brand, they're going to work on Twitch shows. I'm very proud of what we do
[02:18:44.880 --> 02:18:51.760]   because it's honest. It's got integrity. It's authentic. And it really is a great introduction
[02:18:51.760 --> 02:18:59.040]   to our audience of your brand. Our listeners are smart. They're engaged. They're tech savvy.
[02:18:59.040 --> 02:19:04.080]   They're dedicated to our network. And that's one of the reasons we only work with high-integrity
[02:19:04.080 --> 02:19:09.040]   partners that we've personally and thoroughly vetted. I have absolute approval on everybody.
[02:19:09.600 --> 02:19:14.640]   If you've got a great product, I want to hear from you. Elevate your brand by reaching out today at
[02:19:14.640 --> 02:19:22.000]   advertise at twit.tv. Break out of the advertising norm. Grow your brand with host red ads on twit.tv.
[02:19:22.000 --> 02:19:29.760]   Visit twit.tv/advertise for more details, or you can email us advertise at twit.tv if you're
[02:19:29.760 --> 02:19:34.000]   ready to launch your campaign now. I can't wait to see your product. So give us a ring.
[02:19:34.000 --> 02:19:38.560]   We had a great week this week on Twit. We got a little mini movie to show you of some of the
[02:19:38.560 --> 02:19:45.120]   things you might have missed. Watch. Now I'm in fact ready because JAMAB we really actually have
[02:19:45.120 --> 02:19:56.960]   another two weeks in a row. Another chat room. Celebrity. Obda. Week. Maybe Patrick.
[02:19:56.960 --> 02:20:03.200]   Hill ahead. Oh, years ago I was on gizfiz just before I started working for Twit. And so I've
[02:20:03.200 --> 02:20:09.520]   been working for Twit almost 10 years now. Previously on Twit. Mac Break Weekly.
[02:20:09.520 --> 02:20:17.600]   Lo and behold just hours before airtime. Apple ship new Macs. What? We have well announced them
[02:20:17.600 --> 02:20:22.080]   anyway. They're shipping them next week. We will have review units next week. New Mac books.
[02:20:22.080 --> 02:20:30.480]   A new Mac mini. A new Mac M2 Pro and Macs. Tech news weekly. First we talk to Craig
[02:20:30.480 --> 02:20:36.640]   Hockenberry of the icon factory and also the creator of the first Twitter app.
[02:20:36.640 --> 02:20:44.160]   Yeah, he's here to talk about the fact that Twitter has closed off API access for some third
[02:20:44.160 --> 02:20:53.280]   party clients. The suspension is basically beneath campus and there's nothing we can do about it.
[02:20:53.280 --> 02:21:00.080]   Hands on photography. And yes, macro photography can be quite expensive. It adds up. Well,
[02:21:00.080 --> 02:21:04.560]   that's not the case today. I'm telling you right now we're going to save you some money in the
[02:21:04.560 --> 02:21:08.560]   world of macro photography. Put that lens on backwards. Right here and it's just going to work
[02:21:08.560 --> 02:21:14.800]   perfectly for you. Twit. Unbelievable as always. I can just see the Twitter comments.
[02:21:14.800 --> 02:21:22.480]   Hey, put the lens on wrong. That's how you do it. That's the magic. If you haven't caught up on
[02:21:22.480 --> 02:21:27.440]   all of our shows this week, it was a good week to catch up on and Chris Dicty Bartolo made his first
[02:21:27.440 --> 02:21:33.200]   appearance on Ask the Tech guys to her. Bring in, we're getting the band back together.
[02:21:33.200 --> 02:21:39.200]   This is the 40th anniversary of a computer that changed my life. It didn't exactly sell very well,
[02:21:39.200 --> 02:21:50.640]   but it kind of changed the world. The Lisa, the Lisa came out January 19th, 1983, one year before
[02:21:50.640 --> 02:21:56.560]   the Macintosh. And I remember pressing my nose against the computer store window.
[02:21:56.560 --> 02:22:02.160]   Looking at the Lisa, looking at the price tag. It wasn't as expensive as a 007 pinball machine,
[02:22:02.160 --> 02:22:09.760]   but it was $10,000. And I just thought, oh, I wish I could get that. Apple surprised us all by
[02:22:09.760 --> 02:22:17.440]   releasing a much, well, yeah, much less expensive. I guess it was 2500 bucks Macintosh with a very
[02:22:17.440 --> 02:22:23.680]   similar operating system the very next year. But here's the really interesting news. The
[02:22:23.680 --> 02:22:31.520]   computer history museum in conjunction with the 40th anniversary has posted the source code
[02:22:31.520 --> 02:22:40.400]   to the Lisa software, including system and applications software. How do you like your Pascal,
[02:22:40.400 --> 02:22:46.400]   Brianna? I, I met Pascal. That was the, the computers in Mississippi where I learned computer
[02:22:46.400 --> 02:22:53.120]   science were so old that a turbo Pascal was my first language in 1994. So I can do that.
[02:22:53.120 --> 02:22:58.960]   Yeah, I, I originally wrote a lot of Mac software in Pascal for the first Mac and Tashes.
[02:22:58.960 --> 02:23:07.040]   So you can download the zip file, which I just have done. I wish they'd put it on GitHub, but okay,
[02:23:07.040 --> 02:23:15.840]   I've just downloaded 20 megabytes of Lisa source code. That's all. That's all. You couldn't get
[02:23:15.840 --> 02:23:19.520]   anything done in 20 megabytes. But you could have a text editor in 20 megabytes these days.
[02:23:20.800 --> 02:23:28.080]   But here are all the files. If you, I think for somebody like you, Brianna, this, this is,
[02:23:28.080 --> 02:23:34.160]   and somebody like me, this is heaven. This is, I love it. Do you ever look on eBay at the old
[02:23:34.160 --> 02:23:40.000]   leases and try to get one? Because I've been looking for years for one. They're still like,
[02:23:40.000 --> 02:23:47.040]   I was just looking $5,700 for a wing, your kid version of one. They're so valuable.
[02:23:47.040 --> 02:23:52.400]   They're not any cheaper. No. Oh my gosh. That's a little disappointing.
[02:23:52.400 --> 02:23:57.040]   All right. Shall we look at some source code? What do you want to look at drivers?
[02:23:57.040 --> 02:24:04.320]   I want to find like Bill Atkinson's quick draw, early quick draw code or something like that.
[02:24:04.320 --> 02:24:11.280]   I call it make fun of me for being a nerd, but like man, like this is this is very nerdy.
[02:24:12.080 --> 02:24:18.320]   This is very nerdy. Here's the assembly language. This was 68,000. I think it was.
[02:24:18.320 --> 02:24:23.920]   Looks like it's well-commoded, which is nice. Yeah, this is that'll make it a little easier to
[02:24:23.920 --> 02:24:30.560]   discern. Anyway, what would you run this on like today? You know, I'm sure you could have an
[02:24:30.560 --> 02:24:36.400]   emulator. Yeah. Actually, it might be harder to find a Pascal compiler that could
[02:24:36.400 --> 02:24:42.480]   compile it to be honest with you. I think I've got my old turbo Pascal four disks. I don't know.
[02:24:42.480 --> 02:24:47.280]   I don't know. I don't know. I don't know. I don't know. It's you got a compile. I don't know.
[02:24:47.280 --> 02:24:52.400]   That's a good question. It looks like it has a lot of 68,000 assembler. So that's going to be a
[02:24:52.400 --> 02:24:57.600]   potential issue. Anyway, happy. Somewhere there's a school in Mississippi. I've got to go get a
[02:24:57.600 --> 02:25:02.800]   computer from there still using it. There you go. There you go. T-Mobile announced a hacker
[02:25:02.800 --> 02:25:09.680]   once again has broken into their private records, which aren't so private, and stole 37 million
[02:25:09.680 --> 02:25:15.680]   customer records name, address, and account number. They say, although I'm starting to not
[02:25:15.680 --> 02:25:23.440]   believe companies anymore, credit card data was not released. They'll probably offer you a years
[02:25:23.440 --> 02:25:28.720]   worth of life lock or something in response. But credit card's password, social security
[02:25:28.720 --> 02:25:34.320]   numbers not accessed. But be aware that they've got your phone number, they've got your email.
[02:25:34.320 --> 02:25:41.600]   This is not the first time it's happened at T-Mobile. In 2021, a hacker stole personal
[02:25:41.600 --> 02:25:47.360]   information, including social security numbers and driver's license information of 13 million active
[02:25:47.360 --> 02:25:52.720]   and 40 million perspective T-Mobile customers. They settled a class action related to that
[02:25:52.720 --> 02:25:58.000]   breach for half a billion dollars and promised it would never happen again.
[02:25:58.000 --> 02:26:07.600]   Lord, it's just one of the things our street does want to do on the federal level as a privacy
[02:26:07.600 --> 02:26:13.920]   law. We feel like that might help here, also making sure reporting requirements can come together
[02:26:13.920 --> 02:26:17.840]   for government agencies too for when they're hacked. When I see stuff like that, I'm like,
[02:26:17.840 --> 02:26:28.880]   oh, we do something here. And the final story, Amazon Smile is frowning. No more. Smile was a
[02:26:28.880 --> 02:26:36.320]   charitable version of Amazon. If you went to smile.amazon.com, you could designate a small amount of
[02:26:36.320 --> 02:26:41.360]   percentage of your purchases to the charity of your choice over a million charities participated.
[02:26:41.360 --> 02:26:47.760]   Amazon has not said how much money was given out, but I imagine over the 10 years of the program
[02:26:48.720 --> 02:26:55.760]   it was quite a bit. Why would Amazon? Why would Amazon discontinue such a great program?
[02:26:55.760 --> 02:27:02.640]   They said after almost a decade the program has not grown to create the impact we had hoped
[02:27:02.640 --> 02:27:08.400]   originally with so many eligible organizations more than a million globally. Our ability to have
[02:27:08.400 --> 02:27:14.080]   an impact was often spread too thin. They didn't, they didn't announce like any replacement.
[02:27:14.960 --> 02:27:22.640]   And I have to point you to a Reddit post from a guy who used to work at Amazon corporate
[02:27:22.640 --> 02:27:29.280]   and say, who says, let me tell you how the entire program Amazon Smile got created.
[02:27:29.280 --> 02:27:38.560]   The problem was that a large number of customers would start their shopping at Google.com. I bet
[02:27:38.560 --> 02:27:45.440]   you still do this. Search for a product, click the link, and then buy it on Amazon. When that type
[02:27:45.440 --> 02:27:51.920]   of search happens, Amazon pays Google an affiliate fee. Internally Amazon thought if we could just
[02:27:51.920 --> 02:27:58.640]   force users to go to Amazon, maybe even offer a small but lesser amount than we pay to Google,
[02:27:58.640 --> 02:28:08.080]   for charity, we could, you know, keep the difference. Smile required you start your shopping on smile.amazon.com.
[02:28:08.080 --> 02:28:15.600]   How many of us did this? I did it. It helps kill customers going to Google, saves money over paying
[02:28:15.600 --> 02:28:21.840]   Google, and you know, it makes us look good. That's why for the program to work, you have to
[02:28:21.840 --> 02:28:29.440]   start shopping at smile.amazon.com. Literally everything the company does says this former
[02:28:29.440 --> 02:28:35.360]   executive is about profits and extended customer lifetime value. Everything, even the charity
[02:28:35.360 --> 02:28:43.200]   programs are just designed to save Amazon money to which another Reddit user said, "Yeah, I was a
[02:28:43.200 --> 02:28:47.840]   founding member of the Smile program, part of the charity support team working with nonprofits
[02:28:47.840 --> 02:28:53.440]   to help them receive the funds. Left in 2016, three years after fully fleshing out the program,
[02:28:53.440 --> 02:29:00.000]   you are completely correct. The intent of the program was to be cost neutral the amount Amazon
[02:29:00.000 --> 02:29:04.240]   donated to charities was about equal to the cost it saved by not having to pay Google.
[02:29:04.960 --> 02:29:10.320]   The tax write off wasn't the real reason Goodwill is just marketing fodder. He says he left because
[02:29:10.320 --> 02:29:17.440]   I wanted to work for charities and he's not working for a nonprofit. So that's another reason to be
[02:29:17.440 --> 02:29:23.520]   cynical. Anytime. I kind of love it. It kind of makes me happy. I'm like, that's really, really smart.
[02:29:23.520 --> 02:29:28.240]   Like, man, like, you kind of wonder what happened to the person who came up with the idea, you know?
[02:29:28.240 --> 02:29:36.800]   Yeah, hope you got a big promotion and a nice bunch of stock. Folks, I think this is a good time to say
[02:29:36.800 --> 02:29:44.800]   thank you to our wonderful panel and say say goodbye. Smile, by the way, raised $400 million
[02:29:44.800 --> 02:29:52.080]   according to Amazon for charities. So that sounds like an impact. Yeah, I'm just saying. Sounds like
[02:29:52.080 --> 02:29:57.440]   it was accomplishing something. It's kind of weird that they stopped. Shoshana, Senator Shoshana,
[02:29:58.240 --> 02:30:02.400]   thank you so much for being here. Head of digital media at rstreet.org.
[02:30:02.400 --> 02:30:06.880]   Follow her on the Twitter at Senator Shoshana. Anything you want to plug?
[02:30:06.880 --> 02:30:13.360]   Just rstreet.org and thank you for having me. You're always so kind. I always love the other
[02:30:13.360 --> 02:30:18.160]   panelists you bring on. I learned so much and I just appreciate you having me. Well, you're so
[02:30:18.160 --> 02:30:27.600]   great, of course. You know, and I hope that the sloths survived this attack on, I don't know what,
[02:30:27.600 --> 02:30:33.360]   what are what are what are what is the sloth committee up to at the sloth? I mean, we're
[02:30:33.360 --> 02:30:38.800]   investigating January 6 because what you think it was really about it was the sloths. It was all
[02:30:38.800 --> 02:30:43.200]   the floths all the time. It was all about the sloths. So I'm here trying to solve it.
[02:30:44.080 --> 02:30:50.880]   And I do recommend Shoshana is it quite a hiker. Mountaineer, I guess would be the proper term.
[02:30:50.880 --> 02:30:56.880]   And there are lots of wonderful pictures of her recent visit to Chile and others.
[02:30:56.880 --> 02:31:04.720]   Where are you going next? Oh, hopefully if it happens, the California super bloom. I'm so excited.
[02:31:04.720 --> 02:31:11.680]   The super bloom. Is that an algae thing? It's like it has to have the right weather conditions.
[02:31:11.680 --> 02:31:18.400]   And in the Carizzo area, I think it is. It's like just hills and hills of wildflowers. I love wildflowers.
[02:31:18.400 --> 02:31:23.280]   I probably won't be at hiking as much as I will just be like obsessing over the
[02:31:23.280 --> 02:31:28.560]   flowers. You'll be sneezing. He'll be so because he said such a wet winter so far.
[02:31:28.560 --> 02:31:33.520]   We probably will have a super bloom this year. We haven't had one in a while because we've been
[02:31:33.520 --> 02:31:39.200]   in a serious drought and you'll see all those beautiful wildflowers. It only happens every
[02:31:39.200 --> 02:31:42.560]   decade or so. I want to see pictures. That's great.
[02:31:42.560 --> 02:31:49.360]   Thank you. Have a wonderful trip. That's wonderful. And you like, tell me, because I download, I
[02:31:49.360 --> 02:31:56.080]   think maybe because of you, that trails program. You think that's a good program for non-mountaineers
[02:31:56.080 --> 02:32:01.680]   like me? Oh, yeah. It's so great because you get information on the safety on anything you need
[02:32:01.680 --> 02:32:07.760]   to know. There was one time I was hiking in northern Utah and the reviewer before me had said, hey,
[02:32:08.800 --> 02:32:14.640]   some bears chased us just FYI. I was a little more mentally prepared for bears to chase me.
[02:32:14.640 --> 02:32:19.920]   When there's rinsleys, they'll tell you about it. How do you mentally prepare for bears to chase you?
[02:32:19.920 --> 02:32:25.680]   You just know. You just kind of like, okay, I just got to look out. Be more careful here.
[02:32:25.680 --> 02:32:31.520]   Okay. All you have to do is run faster than your friend. You know that, right? You don't have to
[02:32:31.520 --> 02:32:37.680]   be. Oh, yeah, yeah. And also people flip out over black bears. You should take them seriously,
[02:32:37.680 --> 02:32:42.000]   but they're not grizzly bears. They're probably not going to kill you. Really,
[02:32:42.000 --> 02:32:46.800]   bears are different. And then you worry, but you just have to keep and you carry some bear spray
[02:32:46.800 --> 02:32:56.640]   and you're good. I just read an article about cave bears and our Neanderthal ancestors. There
[02:32:56.640 --> 02:33:04.400]   are marks on the bones that tell archaeologists that they were making bearskin clothes to survive
[02:33:04.400 --> 02:33:10.880]   the cold winters in the. We can take them. Yeah. Yeah. So there. There you go. Shoshanna,
[02:33:10.880 --> 02:33:15.600]   you're great. Thank you so much for you being here and your giant hot dog. I am.
[02:33:15.600 --> 02:33:22.560]   I'm pretty grateful. Grateful to the giant hot dog. Patrick stayed up till God knows. What is it
[02:33:22.560 --> 02:33:33.120]   now? Three in the morning? Three in the morning. You know, I was glad to do it because not so much
[02:33:33.120 --> 02:33:38.880]   you, Leo, but Shoshanna and Brianna were great. Are they great? Yeah. Yeah, they really are. I
[02:33:38.880 --> 02:33:44.640]   feel blessed by people like you and Shoshanna and Brianna that we can get on our shows. It's
[02:33:44.640 --> 02:33:51.680]   I don't understand why they come on, but I'm glad you do. And stay up till three in the morning.
[02:33:51.680 --> 02:33:57.360]   I am very grateful. Thank you. Of course, not Patrick.com is a place to go to find out about
[02:33:57.360 --> 02:34:01.840]   all of the wonderful shows, both in English and in French that Patrick does. Are you going to do
[02:34:01.840 --> 02:34:09.680]   a Swedish podcast? I don't think so. I don't, as I mentioned, I have two small children. So maybe
[02:34:09.680 --> 02:34:14.320]   one day. I know. They feel like what? I know the feeling I've done too much. I must stop.
[02:34:14.320 --> 02:34:23.760]   But I will, I will mention, I will plug my my mastodon again. Yay. Not Patrick at Mastodon.
[02:34:23.760 --> 02:34:31.360]   That's social just to make it again very clear that I hope that that becomes a viable alternative
[02:34:31.360 --> 02:34:37.360]   to to the other. I am now following you, Patrick. And of course, we have our
[02:34:37.360 --> 02:34:43.760]   Oh, I have to follow you. Why do I have to? You don't have to, but that's okay. I am a big fan of
[02:34:43.760 --> 02:34:49.120]   our of Mastodon. As you know, we have our own Mastodon instance, Twitter, social. And it's wonderful.
[02:34:49.120 --> 02:34:55.200]   Oh, you even you've a toot in French. Wait, I don't know what to do. It's I guess a toot suite.
[02:34:55.200 --> 02:35:00.240]   So thank you. Thank you for being here, Patrick. I appreciate it. Brandon.
[02:35:00.240 --> 02:35:07.440]   I said breaking news, Leo. A study just came out. Shoshana discussed what you were talking about.
[02:35:07.440 --> 02:35:14.240]   Six percent of Americans think they could destroy a grizzly bear in a hand to hand fight.
[02:35:15.280 --> 02:35:22.080]   Only two percent of British people think that. So you know, that's just the that's the good
[02:35:22.080 --> 02:35:27.040]   old American confidence that works out so well for us. You know what I love is this comes from
[02:35:27.040 --> 02:35:33.920]   New Zealand, where I think this article is intended to laugh at Americans. I don't know. I'm just
[02:35:33.920 --> 02:35:40.400]   thinking perhaps six percent of Americans think not just any bear. They could tackle a grizzly bear.
[02:35:41.280 --> 02:35:45.840]   Yeah, I just recommend the excellent Leonardo DiCaprio film, The Revenant.
[02:35:45.840 --> 02:35:51.920]   Watch it before you watch it before you attempt to wrestle a grizzly bear. I'm just saying. Okay.
[02:35:51.920 --> 02:35:57.920]   Oh my God. He makes maybe it's a spoiler, but he's alive. So you know, he survives. Yeah, never
[02:35:57.920 --> 02:36:03.600]   know. Never know. I have something to plug if I can, but please please please. So this is not
[02:36:03.600 --> 02:36:10.000]   about me. I could plug my pack. I could plug my work. I don't need that today. My dear, sweet husband,
[02:36:10.000 --> 02:36:18.640]   Frank Wu. You all know him. You love him. He's crazy. So he's been working extremely hard on his
[02:36:18.640 --> 02:36:25.360]   science fiction writing career. And he is up for an ant lab award, which is one of the most prestigious
[02:36:25.360 --> 02:36:32.320]   awards for for analog magazine, which is one of the oldest institutions in all the science fiction.
[02:36:32.320 --> 02:36:41.600]   So I'm asking you, if you have a chance, go to frank Wu writes.com. You can, it's the very first
[02:36:41.600 --> 02:36:49.680]   blog post. You can read his story. And if you think it's up to the quality that analog deserves,
[02:36:49.680 --> 02:36:55.760]   he would be honored to have your vote. These kinds of awards literally make or break careers.
[02:36:55.760 --> 02:37:02.480]   They lead to book deals and Frank works so hard at this. So I'm out. If you enjoy anything I've
[02:37:02.480 --> 02:37:07.360]   ever done on Twitch, please go read my husband's work and consider supporting him. And yes,
[02:37:07.360 --> 02:37:14.480]   especially since he puts it online. He does fantastic. And God, I loved analog when I was a kid. I
[02:37:14.480 --> 02:37:19.280]   read it religiously. It was one of the original pulps, one of the greats in science fiction.
[02:37:20.240 --> 02:37:26.560]   It's hard science fiction. So the thing analog does is it it hires writers with a science background,
[02:37:26.560 --> 02:37:33.120]   Frank has a PhD in bacterial genetics, which is why he developed the COVID vaccine. And he brings
[02:37:33.120 --> 02:37:40.080]   that knowledge to his science fiction. In this case, it's a novella. So I will absolutely read it
[02:37:40.080 --> 02:37:48.160]   and vote. I will vote often. Thank you. Just one. Okay. Whatever. However,
[02:37:48.160 --> 02:37:54.160]   ever meant they let me rebellion pack.com. If you want to participate in Brianna's good
[02:37:54.160 --> 02:37:59.760]   political work. And of course, follow her on Twitter at Brianna. Who I suppose you don't have
[02:37:59.760 --> 02:38:07.120]   to talk about or anything like that. You're not a what you're not a tutor. You're not.
[02:38:07.120 --> 02:38:16.720]   No, I do not. No, absolutely not. Okay. That's that's all I need to know. Hey, thank you so much,
[02:38:16.720 --> 02:38:22.640]   Brianna, Patrick Shoshana. Thanks to all of you for joining us. We do Twitch every Sunday afternoon,
[02:38:22.640 --> 02:38:29.520]   2 p.m. Pacific 5 p.m. Eastern 2200 UTC. I say that so you can watch live if you want. Get the first
[02:38:29.520 --> 02:38:37.040]   edition, the first draft of Twitch, live streams, audio and video available at live.tuit.tv. If
[02:38:37.040 --> 02:38:44.080]   you're watching live chat live in our IRC open to all IRC.tuit.tv or if you're a member of club
[02:38:44.080 --> 02:38:50.000]   to it in our club to it discord, we'd love to have you in either place. After the fact you can get
[02:38:50.000 --> 02:38:56.080]   the show at our website, twit.tv. When you're at the website, you will see links to other podcast
[02:38:56.080 --> 02:38:59.920]   clients. You can subscribe there. In fact, that's the best way to get it. So you get it as soon as
[02:38:59.920 --> 02:39:04.400]   it's available on a Sunday evening for your Monday morning commute. There's also a dedicated
[02:39:04.400 --> 02:39:10.240]   YouTube channel. Any way you consume it is good for us. We appreciate it. Thank you for watching.
[02:39:10.240 --> 02:39:15.920]   Thank you for listening and join us next week. Another Twitch is in the can. Thank you everybody.
[02:39:15.920 --> 02:39:20.440]   And this is amazing.
[02:39:20.440 --> 02:39:22.440]   Do on the Twit, right.
[02:39:22.440 --> 02:39:26.480]   Do on the Twit, baby.

