;FFMETADATA1
title=Weaponized FOMO
artist=Leo Laporte, David Spark, Fr. Robert Ballecer, SJ, Amy Webb
album_artist=TWiT
publisher=TWiT
album=This Week in Tech
TRDA=2022-11-14
track=901
language=English
genre=Podcast
comment=Twitter brand chaos, Meta job cuts, FTX fiasco, Apple sued, Kevin Conroy RIP
encoded_by=Uniblab 5.3
date=2022
encoder=Lavf58.76.100

[00:00:00.000 --> 00:00:02.480]   [MUSIC PLAYING]
[00:00:02.480 --> 00:00:04.320]   Podcasts you love.
[00:00:04.320 --> 00:00:06.840]   From people you trust.
[00:00:06.840 --> 00:00:08.360]   This is Twit.
[00:00:08.360 --> 00:00:10.440]   [MUSIC PLAYING]
[00:00:10.440 --> 00:00:15.600]   This is Twit.
[00:00:15.600 --> 00:00:19.400]   This week in tech, episode 901 recorded Sunday,
[00:00:19.400 --> 00:00:25.200]   November 13, 2022, weaponized FOMO.
[00:00:25.200 --> 00:00:26.360]   This week in tech is brought to you
[00:00:26.360 --> 00:00:30.200]   by Worldwide Technology at HPE.
[00:00:30.200 --> 00:00:33.200]   With an innovative culture, thousands of IT engineers,
[00:00:33.200 --> 00:00:35.520]   application developers, unmatched labs,
[00:00:35.520 --> 00:00:38.880]   and integration centers for testing and deploying technology
[00:00:38.880 --> 00:00:43.120]   at scale, WWT helps customers bridge the gap
[00:00:43.120 --> 00:00:45.600]   between strategy and execution.
[00:00:45.600 --> 00:00:51.360]   To learn more about WWT, visit www.wt.com/twit.
[00:00:51.360 --> 00:00:53.800]   And by OnLogic.
[00:00:53.800 --> 00:00:56.600]   OnLogic is helping innovators around the world
[00:00:56.600 --> 00:00:59.560]   solve their most complex technology challenges
[00:00:59.560 --> 00:01:02.280]   using OnLogic industrial computers,
[00:01:02.280 --> 00:01:05.000]   engineered for reliability, even in environments
[00:01:05.000 --> 00:01:07.360]   that would challenge or destroy traditional computer
[00:01:07.360 --> 00:01:08.160]   hardware.
[00:01:08.160 --> 00:01:11.760]   Learn more and find out about OnLogic's 30-day risk-free hardware
[00:01:11.760 --> 00:01:16.280]   trial by visiting onlogic.com/twit.
[00:01:16.280 --> 00:01:18.640]   And by Mint Mobile.
[00:01:18.640 --> 00:01:22.120]   Get premium wireless service from just $15 a month
[00:01:22.120 --> 00:01:24.400]   with no unexpected plot twists.
[00:01:24.400 --> 00:01:30.240]   You'll make your wallet very happy by going to mintmobile.com/twit.
[00:01:30.240 --> 00:01:32.440]   And by Noom.
[00:01:32.440 --> 00:01:35.080]   With their psychology-first approach,
[00:01:35.080 --> 00:01:39.160]   Noom Weight empowers you to build more sustainable habits
[00:01:39.160 --> 00:01:40.240]   and behaviors.
[00:01:40.240 --> 00:01:44.240]   Sign up for your trial at noom.com/twit.
[00:01:44.240 --> 00:01:46.840]   [MUSIC PLAYING]
[00:01:50.320 --> 00:01:52.720]   It's time for "Twit," this week in Tech, the show
[00:01:52.720 --> 00:01:54.080]   we cover the weeks.
[00:01:54.080 --> 00:01:55.960]   Tech News.
[00:01:55.960 --> 00:01:58.240]   This is going to be a fun-filled episode.
[00:01:58.240 --> 00:02:02.200]   David Spark is here, host and producer of CSO series.
[00:02:02.200 --> 00:02:06.960]   He's all about the security known David since Tech TV days.
[00:02:06.960 --> 00:02:08.400]   Hello, David.
[00:02:08.400 --> 00:02:09.400]   Good to see you.
[00:02:09.400 --> 00:02:11.400]   1998.
[00:02:11.400 --> 00:02:12.400]   [WHISTLE]
[00:02:12.400 --> 00:02:13.880]   Wow.
[00:02:13.880 --> 00:02:14.800]   Somebody--
[00:02:14.800 --> 00:02:16.200]   Four years.
[00:02:16.200 --> 00:02:22.200]   Somebody on the YouTube buys old crap.
[00:02:22.200 --> 00:02:23.680]   And I was watching his YouTube--
[00:02:23.680 --> 00:02:24.600]   actually, somebody else was in.
[00:02:24.600 --> 00:02:27.120]   They sent me a link watching his YouTube channel.
[00:02:27.120 --> 00:02:31.320]   And he found a DVC recorder.
[00:02:31.320 --> 00:02:35.880]   And with it, a handful of DVC pro tapes
[00:02:35.880 --> 00:02:38.320]   of the Call for Help show.
[00:02:38.320 --> 00:02:41.920]   And I sent him a comment on his YouTube.
[00:02:41.920 --> 00:02:44.280]   I said, dude, that's how we recorded the show.
[00:02:44.280 --> 00:02:45.840]   Those could be masters.
[00:02:45.840 --> 00:02:46.960]   That's probably not a dub.
[00:02:46.960 --> 00:02:48.640]   That's probably a master.
[00:02:48.640 --> 00:02:49.440]   Probably, yeah.
[00:02:49.440 --> 00:02:50.600]   Yeah.
[00:02:50.600 --> 00:02:51.720]   Anyway, great to see you, David.
[00:02:51.720 --> 00:02:52.640]   Thank you for joining us.
[00:02:52.640 --> 00:02:54.120]   I don't know why I brought that up.
[00:02:54.120 --> 00:02:56.680]   Also, here is other things to talk about.
[00:02:56.680 --> 00:02:57.600]   Believe me.
[00:02:57.600 --> 00:03:00.600]   Amy Webb, CEO of the Future Today Institute,
[00:03:00.600 --> 00:03:02.840]   our favorite future is-- hi, Amy.
[00:03:02.840 --> 00:03:04.120]   Hey, Leo.
[00:03:04.120 --> 00:03:08.480]   She's just back from a 10-mile hike with the Girl Scouts.
[00:03:08.480 --> 00:03:09.280]   Or what is--
[00:03:09.280 --> 00:03:11.760]   Actually, with the Scouts.
[00:03:11.760 --> 00:03:16.920]   So my daughter is in one of the first and only all girls
[00:03:16.920 --> 00:03:17.920]   boy scout troops.
[00:03:17.920 --> 00:03:19.560]   Isn't that cool?
[00:03:19.560 --> 00:03:22.080]   And they're all working towards their Eagle Scout
[00:03:22.080 --> 00:03:22.560]   badges.
[00:03:22.560 --> 00:03:24.360]   Oh, I love it.
[00:03:24.360 --> 00:03:25.880]   Awesome.
[00:03:25.880 --> 00:03:27.680]   And we just did a--
[00:03:27.680 --> 00:03:33.120]   I mean, it wasn't brutal, but it was a pretty steep hike.
[00:03:33.120 --> 00:03:36.120]   It was backpacking, and then it was backcountry camping.
[00:03:36.120 --> 00:03:38.320]   So everything you carry in and everything you carry out.
[00:03:38.320 --> 00:03:39.760]   And it was snowing and cold.
[00:03:39.760 --> 00:03:42.240]   Is this for a badge or just for fun?
[00:03:42.240 --> 00:03:46.240]   This was just for grit and experience.
[00:03:46.240 --> 00:03:47.320]   And you have it.
[00:03:47.320 --> 00:03:48.320]   You have grit.
[00:03:48.320 --> 00:03:49.880]   I have some ice to do a lot of that
[00:03:49.880 --> 00:03:52.280]   when I was younger and my hips didn't hurt like they did
[00:03:52.280 --> 00:03:55.360]   today, but I took some anti-inflammatory medication
[00:03:55.360 --> 00:03:56.040]   with me.
[00:03:56.040 --> 00:03:57.520]   Managed to get through it.
[00:03:57.520 --> 00:03:59.160]   We-- you know, I was at the Veterans Day
[00:03:59.160 --> 00:04:01.480]   parade on Saturday or Friday.
[00:04:01.480 --> 00:04:05.720]   And the Scout troop came, and it was half girls.
[00:04:05.720 --> 00:04:07.600]   And it reminded me, yeah, it's scouting now.
[00:04:07.600 --> 00:04:08.280]   It's just scouting.
[00:04:08.280 --> 00:04:09.280]   Yeah, it's scouting.
[00:04:09.280 --> 00:04:12.200]   It's-- the Girl Scouts kind of tap out at some point.
[00:04:12.200 --> 00:04:15.560]   And this is really about-- this troop is amazing.
[00:04:15.560 --> 00:04:21.040]   And it's all about leadership and sort of being independent
[00:04:21.040 --> 00:04:23.000]   and building confidence.
[00:04:23.000 --> 00:04:24.280]   And it's really incredible.
[00:04:24.280 --> 00:04:26.160]   The troop leaders are amazing.
[00:04:26.160 --> 00:04:30.280]   There are-- there was the first female Eagle Scout.
[00:04:30.280 --> 00:04:31.120]   Was Isabelle a tiny.
[00:04:31.120 --> 00:04:31.600]   Ours was one of them.
[00:04:31.600 --> 00:04:34.880]   That is not her, but we have one of the first.
[00:04:34.880 --> 00:04:36.560]   Look at all those merit badges.
[00:04:36.560 --> 00:04:38.560]   137 of them.
[00:04:38.560 --> 00:04:40.640]   It's-- I can tell you, it is no joke.
[00:04:40.640 --> 00:04:42.720]   And these kids are-- these kids are tough.
[00:04:42.720 --> 00:04:44.280]   They are resilient.
[00:04:44.280 --> 00:04:47.480]   If you've got a daughter or a son or a person,
[00:04:47.480 --> 00:04:51.480]   a non-binary person, I'll put a good word in for Scouts.
[00:04:51.480 --> 00:04:52.600]   It really is pretty great.
[00:04:52.600 --> 00:04:56.840]   That's a, by the way, transformation for BSA.
[00:04:56.840 --> 00:04:58.920]   So I'm very, very glad to hear that.
[00:04:58.920 --> 00:05:02.000]   And I love it that there are women Eagle Scouts now.
[00:05:02.000 --> 00:05:03.760]   That's fantastic.
[00:05:03.760 --> 00:05:04.360]   Welcome, Amy.
[00:05:04.360 --> 00:05:05.360]   Good to have you.
[00:05:05.360 --> 00:05:06.520]   Thanks.
[00:05:06.520 --> 00:05:11.600]   And the digital Jesuit-- because David said, who are you?
[00:05:11.600 --> 00:05:13.040]   What is this DJ?
[00:05:13.040 --> 00:05:16.400]   He is the digital Jesuit father, Robert Palisare.
[00:05:16.400 --> 00:05:18.240]   Hello, Robert.
[00:05:18.240 --> 00:05:19.000]   Oh, I'm sorry, Leo.
[00:05:19.000 --> 00:05:21.600]   I didn't realize you would come in.
[00:05:21.600 --> 00:05:24.800]   You're way too cold for school.
[00:05:24.800 --> 00:05:28.480]   Great to see you, Padre, in the Vatican.
[00:05:28.480 --> 00:05:30.960]   When we go out there to visit you in April,
[00:05:30.960 --> 00:05:33.080]   Robert says, let's do the show.
[00:05:33.080 --> 00:05:41.320]   We'll do it from this roof overlooking the Vatican.
[00:05:41.320 --> 00:05:42.000]   Can we do it?
[00:05:42.000 --> 00:05:43.240]   This one.
[00:05:43.240 --> 00:05:45.040]   Can we do that?
[00:05:45.040 --> 00:05:47.880]   We absolutely-- I mean, this view right here
[00:05:47.880 --> 00:05:50.920]   is about 10 steps from my office on the same floor.
[00:05:50.920 --> 00:05:53.800]   So I just move a few monitors and some cables,
[00:05:53.800 --> 00:05:54.400]   and we're good to go.
[00:05:54.400 --> 00:05:55.080]   We're going to do it.
[00:05:55.080 --> 00:05:55.680]   It's a deal.
[00:05:55.680 --> 00:05:57.160]   April.
[00:05:57.160 --> 00:05:57.680]   Yeah.
[00:05:57.680 --> 00:05:59.360]   Benito says, you're going to need you an engineer for that.
[00:05:59.360 --> 00:05:59.840]   Yeah, Benito.
[00:05:59.840 --> 00:06:02.400]   We'll take you.
[00:06:02.400 --> 00:06:03.560]   We will take you.
[00:06:03.560 --> 00:06:07.760]   So what a week we have had.
[00:06:07.760 --> 00:06:10.640]   It's not normal that I would have two stories
[00:06:10.640 --> 00:06:15.040]   vying for our lead, but there are two massive stories,
[00:06:15.040 --> 00:06:17.440]   and they are somewhat interlocking, I think.
[00:06:17.440 --> 00:06:22.280]   There's, of course, the collapse of FTX and of crypto in general.
[00:06:22.280 --> 00:06:27.880]   And then the ongoing pyre at Twitter.
[00:06:27.880 --> 00:06:29.280]   Let's start with Twitter, because we've
[00:06:29.280 --> 00:06:30.440]   been talking before the show, and we all
[00:06:30.440 --> 00:06:32.640]   have strong opinions about this.
[00:06:32.640 --> 00:06:36.560]   The latest Twitter news is coming out.
[00:06:36.560 --> 00:06:37.880]   And again, a lot of this news--
[00:06:37.880 --> 00:06:42.280]   who was it?
[00:06:42.280 --> 00:06:42.800]   The Verge.
[00:06:42.800 --> 00:06:46.760]   Somebody tried to contact Elon Twitter for a comment,
[00:06:46.760 --> 00:06:49.400]   and the response was, Twitter has no communications
[00:06:49.400 --> 00:06:50.680]   department anymore.
[00:06:50.680 --> 00:06:56.800]   So there's no way to really know if any of this is true or not.
[00:06:56.800 --> 00:07:00.000]   But the latest seems to be true, which
[00:07:00.000 --> 00:07:08.520]   is that 4,400 contract employees were summarily terminated
[00:07:08.520 --> 00:07:09.520]   this weekend.
[00:07:09.520 --> 00:07:12.680]   That's on top of the 50% that they dumped.
[00:07:12.680 --> 00:07:14.240]   That's on top of the employees.
[00:07:14.240 --> 00:07:20.120]   So they dumped somewhere around 3,000 to 4,000 employees,
[00:07:20.120 --> 00:07:23.520]   many of whom have asked, but that's to come back.
[00:07:23.520 --> 00:07:25.920]   But then now the latest is these contract employees,
[00:07:25.920 --> 00:07:28.920]   and it seems to be they didn't tell them ahead of time.
[00:07:28.920 --> 00:07:31.080]   They just disconnected their slack.
[00:07:31.080 --> 00:07:33.800]   They disconnected their tools and disconnected
[00:07:33.800 --> 00:07:36.120]   their company email, because you know what?
[00:07:36.120 --> 00:07:37.960]   They're just contractors.
[00:07:37.960 --> 00:07:39.240]   And of course, they get no--
[00:07:39.240 --> 00:07:39.840]   Fortunately--
[00:07:39.840 --> 00:07:41.560]   That's a security issue, that I mean--
[00:07:41.560 --> 00:07:42.640]   Half of those.
[00:07:42.640 --> 00:07:45.160]   Well, they could send an email at the same time saying,
[00:07:45.160 --> 00:07:47.320]   we're terminating your contract.
[00:07:47.320 --> 00:07:50.040]   They're apparently neglected to do that.
[00:07:50.040 --> 00:07:50.680]   Which is weird.
[00:07:50.680 --> 00:07:51.180]   I could listen.
[00:07:51.180 --> 00:07:54.120]   With the amount of animosity right now, I can absolutely--
[00:07:54.120 --> 00:07:56.400]   If it was me, I don't know.
[00:07:56.400 --> 00:07:57.280]   David should speak to this.
[00:07:57.280 --> 00:07:59.320]   But if I was to see so, or if I was the--
[00:07:59.320 --> 00:07:59.680]   Oh, yeah.
[00:07:59.680 --> 00:07:59.680]   --the situation--
[00:07:59.680 --> 00:07:59.680]   Oh, yeah.
[00:07:59.680 --> 00:07:59.680]   --the situation--
[00:07:59.680 --> 00:08:01.600]   --by the way quit.
[00:08:01.600 --> 00:08:02.100]   Yeah.
[00:08:02.100 --> 00:08:03.240]   They don't have a see so.
[00:08:03.240 --> 00:08:06.080]   Well, I would have shut everything down first also.
[00:08:06.080 --> 00:08:08.120]   I mean, it's not the humane nice thing to do,
[00:08:08.120 --> 00:08:09.600]   but it's the safe thing to do.
[00:08:09.600 --> 00:08:11.040]   That is, by the way, the process.
[00:08:11.040 --> 00:08:14.600]   When people leave, the security department
[00:08:14.600 --> 00:08:17.600]   does a process of closing accounts,
[00:08:17.600 --> 00:08:19.680]   removing access, things like that.
[00:08:19.680 --> 00:08:24.120]   So identity management.
[00:08:24.120 --> 00:08:25.720]   And if you're doing things that quickly,
[00:08:25.720 --> 00:08:29.360]   I don't see how you can physically shut that many people
[00:08:29.360 --> 00:08:30.480]   down that quickly.
[00:08:30.480 --> 00:08:31.120]   I don't know.
[00:08:31.120 --> 00:08:32.320]   I don't see how that's possible.
[00:08:32.320 --> 00:08:33.920]   Yeah.
[00:08:33.920 --> 00:08:36.160]   Yeah, I mean, and I feel for anybody laid off,
[00:08:36.160 --> 00:08:38.920]   there were 11,000 layoffs at Meta this week also.
[00:08:38.920 --> 00:08:40.840]   I mean, this is a tough time.
[00:08:40.840 --> 00:08:45.960]   A lot of those employees met a handle that kind of dramatically
[00:08:45.960 --> 00:08:49.240]   differently than Twitter handled it.
[00:08:49.240 --> 00:08:52.040]   But it's hard to lose a job whether or not you get severance,
[00:08:52.040 --> 00:08:55.440]   whether you get health care and so forth.
[00:08:55.440 --> 00:08:56.520]   You're still out of work.
[00:08:56.520 --> 00:09:01.320]   So according to Casey Newton on the platformer newsletter,
[00:09:01.320 --> 00:09:03.880]   contractors aren't being notified at all.
[00:09:03.880 --> 00:09:06.880]   They're just losing access to Slack and email.
[00:09:06.880 --> 00:09:08.840]   Managers figured it out when their workers just
[00:09:08.840 --> 00:09:11.360]   disappeared from their system.
[00:09:11.360 --> 00:09:14.560]   So there is normally some notification,
[00:09:14.560 --> 00:09:17.360]   even if it happens after the fact.
[00:09:17.360 --> 00:09:18.920]   Stripe is the good example, right?
[00:09:18.920 --> 00:09:21.280]   So the Stripe also has some layoffs.
[00:09:21.280 --> 00:09:23.480]   And in the middle of every, the catastrophe that was happening
[00:09:23.480 --> 00:09:27.840]   at Twitter and Meta, I think that the letter that was public,
[00:09:27.840 --> 00:09:30.360]   you know, they didn't sign it by the way, Stripe.
[00:09:30.360 --> 00:09:33.120]   They signed the names of them, like the CEO.
[00:09:33.120 --> 00:09:34.880]   You know, their actual names were signed.
[00:09:34.880 --> 00:09:37.080]   And it was humane and honest.
[00:09:37.080 --> 00:09:39.200]   And I think if it's got to be done,
[00:09:39.200 --> 00:09:42.320]   that's a much nicer way to do it.
[00:09:42.320 --> 00:09:46.240]   I do wonder though, the big issue is the fact that they've,
[00:09:46.240 --> 00:09:49.720]   they let go a huge chunk of infrastructure workers.
[00:09:49.720 --> 00:09:52.560]   And according to some reports, they let go of some infrastructure workers
[00:09:52.560 --> 00:09:54.280]   in the middle of changes.
[00:09:54.280 --> 00:09:58.320]   That's a recipe for a disaster for a company that has already let go
[00:09:58.320 --> 00:09:59.720]   so many of their engineers.
[00:09:59.720 --> 00:10:03.600]   They don't know what most of the moving parts do.
[00:10:03.600 --> 00:10:05.400]   So right now it's coasting on inertia.
[00:10:05.400 --> 00:10:09.120]   You are actually watching the disaster unfold is what we're actually buying.
[00:10:09.120 --> 00:10:11.120]   Can I add a disaster to this disaster?
[00:10:11.120 --> 00:10:12.680]   Because nobody's really talking about it yet.
[00:10:12.680 --> 00:10:16.040]   So because of the inflation rate,
[00:10:16.040 --> 00:10:20.240]   there are a lot of companies that still have pensions
[00:10:20.240 --> 00:10:22.680]   where people who work anywhere close to be like,
[00:10:22.680 --> 00:10:26.800]   if you're close to retirement and you do the math and it works out that you're going to,
[00:10:26.800 --> 00:10:31.400]   if you put in those last two years of work, you're effectively working for free,
[00:10:31.400 --> 00:10:33.760]   those people are taking early retirement.
[00:10:33.760 --> 00:10:36.520]   And that's all happening between now and the end of the month.
[00:10:36.520 --> 00:10:42.520]   We are going to see tens of, potentially tens of thousands of people
[00:10:42.520 --> 00:10:45.160]   who are retiring all at once.
[00:10:45.160 --> 00:10:47.720]   On top of all of these people who are now out of work,
[00:10:47.720 --> 00:10:51.960]   there's going to be disastrous economic consequences that I don't think we're prepared for.
[00:10:51.960 --> 00:10:57.320]   Let me ask David because you do cover CISOs.
[00:10:57.320 --> 00:11:00.360]   How the CISO of Twitter departed?
[00:11:00.360 --> 00:11:03.560]   He left with the head of Trusty O'Roth,
[00:11:03.560 --> 00:11:07.960]   who had been kind of tweeting and trying to calm people down
[00:11:07.960 --> 00:11:12.840]   and the head of advertising and a number of executives.
[00:11:12.840 --> 00:11:17.960]   And they all left why at the same time?
[00:11:17.960 --> 00:11:21.240]   Well, some were let go, some chose to go,
[00:11:21.240 --> 00:11:25.000]   but I actually don't know specifically the reason why the CISO left, but I mean...
[00:11:25.000 --> 00:11:28.040]   I do. You want to know? You want to know?
[00:11:28.040 --> 00:11:29.400]   You want to know?
[00:11:29.400 --> 00:11:36.120]   So there was an internal Slack message from an unnamed lawyer
[00:11:36.120 --> 00:11:42.200]   saying, internal in Twitter saying, look,
[00:11:42.200 --> 00:11:47.080]   we've lost all the compliance people.
[00:11:47.080 --> 00:11:51.160]   We are under an FTC consent decree,
[00:11:51.160 --> 00:11:55.560]   which we've already paid more than a hundred million dollars for.
[00:11:55.560 --> 00:11:58.520]   The consent decree says before, get listen to this,
[00:11:58.520 --> 00:12:02.760]   before Twitter rolls out any new feature, it has to be vetted for security
[00:12:02.760 --> 00:12:08.280]   and you have to tell us the FTC what you're doing and how secure it is.
[00:12:08.280 --> 00:12:11.560]   This message went out saying, we're very concerned because
[00:12:11.560 --> 00:12:15.720]   this compliance is now on the head of engineering.
[00:12:15.720 --> 00:12:20.280]   You are responsible for certifying the compliance to the consent decree
[00:12:20.280 --> 00:12:22.360]   of any new feature you roll out.
[00:12:22.360 --> 00:12:24.120]   There's no one above you to do it.
[00:12:24.120 --> 00:12:29.240]   At which point there was a mass head for the exit
[00:12:29.240 --> 00:12:33.080]   because people don't want to be responsible for that.
[00:12:33.080 --> 00:12:36.920]   He quoted the, I guess, the de facto chief counsel,
[00:12:36.920 --> 00:12:41.160]   which is Elon Musk's personal lawyer saying, this is a man who launches rockets.
[00:12:41.160 --> 00:12:46.440]   He's not scared of the FTC to which some reported...
[00:12:46.440 --> 00:12:47.560]   That's apples and oranges.
[00:12:47.560 --> 00:12:52.680]   To which some reported, look, you've already been fine, 114 million dollars.
[00:12:52.680 --> 00:12:58.600]   The next one's going to be another comma over, that this is really serious.
[00:12:58.600 --> 00:13:02.360]   Of course, the same day, President Biden when asked about this,
[00:13:02.360 --> 00:13:04.040]   said, yeah, we're going to keep an eye on this.
[00:13:04.040 --> 00:13:09.640]   I think that part of the reason the CISO left
[00:13:09.640 --> 00:13:15.240]   is that there's some serious concern about liability landing on his shoulders.
[00:13:15.240 --> 00:13:21.160]   That is, by the way, the big discussion that's been happening among CISO.
[00:13:21.160 --> 00:13:25.560]   I'll rope in the Joe Sullivan Uber story as well.
[00:13:25.560 --> 00:13:28.120]   When that broke and he was found guilty,
[00:13:28.120 --> 00:13:32.200]   that was... All CISOs started to worry for themselves,
[00:13:32.200 --> 00:13:37.000]   and this became a big issue about getting an appointment agreement
[00:13:37.000 --> 00:13:38.680]   when you take on the role of a CISO.
[00:13:38.680 --> 00:13:47.560]   There is no similarity, I believe, between Joe Sullivan and the CISO of Twitter who just left.
[00:13:47.560 --> 00:13:50.120]   There is none whatsoever here.
[00:13:50.120 --> 00:13:56.280]   And CISOs, we were talking about this on our show recently,
[00:13:56.280 --> 00:13:59.400]   about burnout, about cybersecurity professionals.
[00:13:59.400 --> 00:14:04.600]   If you've got bad culture and unreasonable expectations,
[00:14:04.600 --> 00:14:08.680]   looks like the formula we got here, that is cybersecurity burnout,
[00:14:08.680 --> 00:14:09.720]   and that's why people leave.
[00:14:09.720 --> 00:14:13.560]   So it is like the magical formula that's happened to Twitter.
[00:14:13.560 --> 00:14:17.880]   So that's why you are getting people burnt out in a mass exodus,
[00:14:17.880 --> 00:14:21.640]   not just of Elon's doing, but others choosing to go.
[00:14:21.640 --> 00:14:24.040]   Because there's no question to have Twitter on your resume,
[00:14:24.600 --> 00:14:27.480]   and to be available in the cybersecurity space right now,
[00:14:27.480 --> 00:14:29.000]   you will be snatched up immediately.
[00:14:29.000 --> 00:14:32.440]   I mean, every CISO we have on our shows, well, not every.
[00:14:32.440 --> 00:14:35.000]   I was saying almost every is looking to hire.
[00:14:35.000 --> 00:14:38.840]   So if you worked in cybersecurity and you were in Twitter,
[00:14:38.840 --> 00:14:40.440]   you will have no problem finding another job.
[00:14:40.440 --> 00:14:44.200]   So first of all, correct myself.
[00:14:44.200 --> 00:14:45.160]   The CISO was a woman.
[00:14:45.160 --> 00:14:46.440]   Leah Kistner.
[00:14:46.440 --> 00:14:48.360]   Yes, we apologize, not he, she.
[00:14:48.360 --> 00:14:50.680]   Leah Kistner.
[00:14:50.680 --> 00:14:52.280]   No, I'm sorry.
[00:14:52.280 --> 00:14:54.520]   I think the fact is that I think Leah is a male.
[00:14:54.520 --> 00:14:54.920]   No, no.
[00:14:54.920 --> 00:14:55.560]   He is a woman.
[00:14:55.560 --> 00:14:55.960]   Am I wrong?
[00:14:55.960 --> 00:14:56.520]   I checked.
[00:14:56.520 --> 00:14:57.720]   Oh, excuse my ignorance.
[00:14:57.720 --> 00:14:59.640]   It could be, it could be Leah, could be Leah.
[00:14:59.640 --> 00:15:00.680]   I checked.
[00:15:00.680 --> 00:15:03.800]   Chief privacy officer Damon Kieran, Damien Kieran,
[00:15:03.800 --> 00:15:07.320]   chief compliance officer Marianne Fogarty all left at the same time.
[00:15:07.320 --> 00:15:12.760]   Now, I've got a story from Bloomberg from a few days ago
[00:15:12.760 --> 00:15:15.800]   in which Spiro, who's Elon's lawyer, said,
[00:15:15.800 --> 00:15:17.480]   "Twitter has spoken to the FTC.
[00:15:17.480 --> 00:15:20.520]   Its first compliance check is upcoming."
[00:15:21.240 --> 00:15:23.800]   But he said, "Don't worry, the legal department is handling it."
[00:15:23.800 --> 00:15:30.200]   Didn't somebody, didn't one of the GCs send a slack or something to the remaining Twitter
[00:15:30.200 --> 00:15:30.200]   employees?
[00:15:30.200 --> 00:15:31.800]   Yeah, that's the one I was talking about.
[00:15:31.800 --> 00:15:32.200]   Yeah.
[00:15:32.200 --> 00:15:33.560]   About the, the whistle blowing.
[00:15:33.560 --> 00:15:33.880]   Yeah.
[00:15:33.880 --> 00:15:35.080]   And that's, that's pretty significant.
[00:15:35.080 --> 00:15:37.800]   He said you should all become whistle blowers.
[00:15:37.800 --> 00:15:38.680]   This is internal.
[00:15:38.680 --> 00:15:41.960]   Well, it wasn't, I mean, wasn't it more like,
[00:15:41.960 --> 00:15:45.000]   if you choose to disclose you are, you know,
[00:15:45.000 --> 00:15:46.520]   you should know that you are covered by,
[00:15:46.520 --> 00:15:50.680]   I mean, by the, you know, whistle blowing law in the United States, wasn't it?
[00:15:50.680 --> 00:15:51.000]   Yeah.
[00:15:51.000 --> 00:15:51.560]   Verte, I mean.
[00:15:51.560 --> 00:15:56.360]   It was just reaching out and saying you could have some personal liability here.
[00:15:56.360 --> 00:15:59.240]   So don't do anything that I can sketch you about.
[00:15:59.240 --> 00:15:59.480]   Okay.
[00:15:59.480 --> 00:16:02.600]   So that's what question I've got for all of you, maybe David mostly.
[00:16:02.600 --> 00:16:08.520]   So if you were, you know, if you were chief privacy officer, compliance officer, CISO,
[00:16:08.520 --> 00:16:12.120]   do you assume personal liability if something goes wrong or is that?
[00:16:12.120 --> 00:16:16.760]   No, that's the, that's the employment agreement that I was referencing earlier is,
[00:16:16.760 --> 00:16:17.000]   yeah.
[00:16:17.000 --> 00:16:21.960]   Or, you know, they were, they were, they were agreeing to that are, are making a huge mistake.
[00:16:21.960 --> 00:16:22.200]   Right.
[00:16:22.200 --> 00:16:25.000]   So they would not have been, I mean, they were leaving because they don't want to either be
[00:16:25.000 --> 00:16:27.560]   a part of what's happening or they don't want to deal with the aftermath of a time.
[00:16:27.560 --> 00:16:33.960]   And that's the other thing is that CISOs do not, and you know, security professor do not own risk,
[00:16:33.960 --> 00:16:36.200]   don't own cyber security risk of the company.
[00:16:36.200 --> 00:16:39.880]   Their job is to explain risk to the business leaders.
[00:16:39.880 --> 00:16:42.520]   And it's for the business leader to take on the risks who's saying,
[00:16:42.520 --> 00:16:45.800]   if you do this and compliance is a perfect example of it, because
[00:16:45.800 --> 00:16:52.360]   that is a very known risk where there's a very clear financial loss that can be had if you are
[00:16:52.360 --> 00:16:55.560]   not compliant. I mean, it's, it's, it's clear as day right there.
[00:16:55.560 --> 00:16:57.800]   Is there like a medical, you know, how there's malpractice?
[00:16:57.800 --> 00:17:02.520]   Is there such a thing as malpractice in risk and compliance and security?
[00:17:02.520 --> 00:17:05.560]   I mean, there's always some, I don't know.
[00:17:05.560 --> 00:17:09.720]   There's always something that's analogous to malpractice, right?
[00:17:09.720 --> 00:17:13.400]   The company, the company could say in the contract, we indemnify you
[00:17:13.400 --> 00:17:18.520]   against any judgments from the FTC. But there's always a risk if the FTC
[00:17:18.520 --> 00:17:25.080]   sees that this employee knew that something bad was happening and didn't do anything about it.
[00:17:25.080 --> 00:17:29.320]   I don't care if the company identifies, indemnifies you.
[00:17:29.320 --> 00:17:33.800]   I feel like that employee is at somewhat at risk and has some responsibility.
[00:17:33.800 --> 00:17:38.280]   Am I wrong? David, you can't, you can't say, you know, it's not my fault.
[00:17:38.280 --> 00:17:39.560]   Look at my contract.
[00:17:39.560 --> 00:17:46.600]   No, yeah. So the job of the CISO is to communicate the risk situation. Like,
[00:17:46.600 --> 00:17:53.080]   okay, if we don't do this, if we don't meet this compliance regulation or this, this regulation,
[00:17:53.080 --> 00:17:58.440]   which is a compliance requirement, then you will be fined this. It is very clear.
[00:17:58.440 --> 00:18:04.280]   If we don't do this, you know, we don't have endpoint protection out here, then there's
[00:18:04.280 --> 00:18:07.640]   possibility someone's going to break in our network, that's going to increase our risk level
[00:18:07.640 --> 00:18:12.920]   to here. Are you aware of that? And one of the big things CISOs do when they're explaining risk
[00:18:12.920 --> 00:18:18.120]   to the business is they say, here's what the risk situation is for not doing this,
[00:18:18.120 --> 00:18:24.920]   we can spend X dollars to reduce this risk. Are you aware of that? Do you want to spend that?
[00:18:24.920 --> 00:18:28.360]   As long as you want to spend this money, then they have to sign off on the risk.
[00:18:28.360 --> 00:18:32.760]   That's a key thing. They have to actually sign. I acknowledge this is a risk.
[00:18:32.760 --> 00:18:35.800]   Right. But what's the report? Okay. So fine. What does that mean though?
[00:18:36.360 --> 00:18:39.960]   If it turns out that it was communicated to them what the situation was,
[00:18:39.960 --> 00:18:43.400]   right? But what's the repercussion as well? Let me read this thing. By the way,
[00:18:43.400 --> 00:18:45.720]   this comes just really a paper trail is what we're looking at.
[00:18:45.720 --> 00:18:51.400]   This comes from Alex Heath at the Verge who says, we know who this person inside Twitter is.
[00:18:51.400 --> 00:18:57.560]   Obviously, we're not going to reveal that. They looked at the Slack message and wrote this in a
[00:18:57.560 --> 00:19:03.640]   note posted to Twitter's Slack and viewable to all staff. An attorney on the company's privacy team
[00:19:03.640 --> 00:19:08.760]   wrote, quote, Elon has shown this is somebody inside. Elon has shown his only priority with
[00:19:08.760 --> 00:19:13.880]   Twitter users is how to monetize them. I do not believe he cares about the human rights activists,
[00:19:13.880 --> 00:19:18.600]   the dissidents, our users in unmonetizable regions and all the other users who've made
[00:19:18.600 --> 00:19:23.880]   Twitter the global town square you've all spent so long building. And we all love.
[00:19:23.880 --> 00:19:29.880]   It says he has heard that Alex Sparrow, the current head of legal, says that Elon's willing
[00:19:29.880 --> 00:19:35.480]   to take on a huge amount of risk in relation to this company, Musk's new legal department is
[00:19:35.480 --> 00:19:42.520]   asking engineers, and this is a good question for you, David, to quote, self-certify compliance with
[00:19:42.520 --> 00:19:49.880]   FTC rules and other privacy laws. This is in that lawyer's note. And the Verge checked another
[00:19:49.880 --> 00:19:56.280]   employee familiar with the matter said the same. And so at that point, the lawyer said,
[00:19:56.280 --> 00:20:00.600]   if you're being asked to do anything you're uncomfortable with, become a whistleblower.
[00:20:00.600 --> 00:20:06.840]   If you were uncomfortable, you knew there was something bad. Nobody upstairs was willing to hear
[00:20:06.840 --> 00:20:11.320]   it or sign off on it. Do that process, that orderly process you're talking about, David,
[00:20:11.320 --> 00:20:17.640]   I think you have to go to the FTC at that point, don't you? You can't just say that.
[00:20:17.640 --> 00:20:24.520]   Well, I mean, you know, if you see someone being negligent, but also you may have
[00:20:25.160 --> 00:20:29.480]   NDAs that you've signed as a employee. But that's why you go whistleblower because that
[00:20:29.480 --> 00:20:35.560]   obviates the NDA. Yeah, I guess. But it sounds like there's no obligation other than maybe a
[00:20:35.560 --> 00:20:40.040]   moral obligation. Certainly a moral obligation. Yeah. That's the thing is like, for example,
[00:20:40.040 --> 00:20:46.040]   one of the things we hear a lot from CISOs is, you know, I'm not running the business. It's my job to
[00:20:46.040 --> 00:20:53.720]   reduce the risk, communicate the risk, and do my best on that. If someone chooses to do something
[00:20:53.720 --> 00:20:58.040]   that I don't agree with, it's not my responsibility because I'm not running the business.
[00:20:58.040 --> 00:21:02.680]   So I'm just communicating to that. But it's very, very important that they acknowledge they
[00:21:02.680 --> 00:21:07.160]   understand it and they sign off on it, which I'm sorry, let's just get into the grander discussion
[00:21:07.160 --> 00:21:13.960]   of business. Businesses are in the business. All businesses take on risk, not just cybersecurity
[00:21:13.960 --> 00:21:20.440]   risk. There's all kinds of risks. So they have to acknowledge it. Some people are riskier investors
[00:21:20.440 --> 00:21:25.800]   than others. You can't bring cyber risk down to zero. So you're willing to take certain risks
[00:21:25.800 --> 00:21:31.240]   in certain areas to maybe open up the business to do other things. And that is pretty clear.
[00:21:31.240 --> 00:21:37.320]   That's what Elon is doing here. He's taking a lot of risks. I actually think Elon is doing
[00:21:37.320 --> 00:21:41.240]   something different. Oh, good. I think he is. Hold on. Hold the thought because we're going to get
[00:21:41.240 --> 00:21:45.320]   to it. I really want to know because that's the next step. We're just talking about all the
[00:21:45.320 --> 00:21:51.720]   horrible things that have happened. And I want to know why. The final paragraphs of this
[00:21:51.720 --> 00:21:57.160]   lawyer's Slack message is all of this is extremely dangerous for our users.
[00:21:57.160 --> 00:22:03.880]   Also, given that the FTC can and will find Twitter billions of dollars pursuant to the FTC
[00:22:03.880 --> 00:22:08.680]   consent order, extremely determined by Twitter's longevity as a platformer, users deserve better
[00:22:08.680 --> 00:22:13.000]   than this. I would guess this person's gone, by the way, by now. If you feel uncomfortable about
[00:22:13.000 --> 00:22:18.040]   anything you're being asked to do, call Twitter ethics hotline or submit a report. Please note
[00:22:18.040 --> 00:22:24.760]   the FTC's number is FTC help. You may also remember that much reached out to whistlebloweraid.org.
[00:22:24.760 --> 00:22:29.640]   I wish you all luck. It's been an honor to work with all of you. That is, that is a,
[00:22:29.640 --> 00:22:36.360]   you know, career suicide note, if I ever, ever heard one. There have been, I have seen a lot of
[00:22:36.360 --> 00:22:42.680]   people warning it. The security status of Twitter is now unknown that if you have,
[00:22:42.680 --> 00:22:46.520]   you should never have had anything private in your DM. But if you do get, you know,
[00:22:46.520 --> 00:22:53.640]   erase your DMS, download your data. Is there, do you think there's cause for concern about
[00:22:53.640 --> 00:22:59.720]   the security of Twitter? Robert? Yes. Absolutely.
[00:22:59.720 --> 00:23:05.160]   And after this, back to the FTC agreement, I remember in the 2011 FTC agreement, in part B,
[00:23:05.160 --> 00:23:09.240]   this was the, this was the nuts and bolts part of the, of the 2011 agreement.
[00:23:09.240 --> 00:23:15.800]   It says that there has to be someone or some ones who are specifically designated by name to be
[00:23:15.800 --> 00:23:21.560]   accountable for an accountable is a very important legal word for the information security program.
[00:23:21.560 --> 00:23:26.920]   There is no one like that at Twitter. They have all quit. So that means that the company is now
[00:23:26.920 --> 00:23:33.240]   in material breach of the f t of the 2011 FTC agreement. If they are in breach of the 2011 FTC
[00:23:33.240 --> 00:23:37.640]   agreement, that means that you cannot count on them to secure any of the data that they currently
[00:23:37.640 --> 00:23:41.400]   have. And if there's any question about how the way they can name, by the way, they can name,
[00:23:41.400 --> 00:23:46.840]   quote, any individual to quote be in charge, even though they don't technically have a CISO anymore.
[00:23:46.840 --> 00:23:51.960]   And by the way, that's interesting that you brought that up, Padre, because you will see
[00:23:51.960 --> 00:23:56.600]   there's a lot of new comfort, a lot of companies that have been around for a long time that are
[00:23:56.600 --> 00:24:05.000]   all of a sudden getting their very first CISOs now. Yes, yes. They're growing up. Yes.
[00:24:05.000 --> 00:24:08.920]   Senator Ed Markey. So now let's get to the impersonation problem, because that's another
[00:24:08.920 --> 00:24:12.680]   thing that happened this week. Remember the humerus, I must say, it was very funny.
[00:24:12.680 --> 00:24:17.720]   Not too. Actually, some of it might have been funny. Some of it was really catastrophic,
[00:24:17.720 --> 00:24:24.440]   especially if you were really not really like Lily situation. Well, yes, that also. And, but
[00:24:24.440 --> 00:24:28.200]   like Nintendo, which was- Mario slipping off the customer's side.
[00:24:28.200 --> 00:24:32.440]   Sorry, but there were a lot of horrific images and that happened in the middle of the night for
[00:24:32.440 --> 00:24:38.120]   Japan. I actually talked to some of my friends there. That was nothing that anybody was preparing
[00:24:38.120 --> 00:24:43.400]   for at like two o'clock in the morning. So, as we know, Elon says, for eight bucks,
[00:24:43.400 --> 00:24:50.120]   you can buy a blue check, which prior to this men authentication, there was a lot of back and forth.
[00:24:52.600 --> 00:24:56.440]   For a while, for like five minutes, Marquez Brownlee tweeted, look, not only do I have a blue check,
[00:24:56.440 --> 00:25:02.520]   but now it says this is an official account. I have a great check. And then Elon replied,
[00:25:02.520 --> 00:25:08.760]   no, not anymore. That went away. Apparently, they put a hold on this. You can still buy Twitter
[00:25:08.760 --> 00:25:13.000]   blue for eight, ninety nine, but you won't get a blue check to put a hold on that. But in between
[00:25:13.000 --> 00:25:18.200]   those times, there were people impersonating authenticated impersonating Elon, Kathy Griffin,
[00:25:18.200 --> 00:25:23.800]   got booted off the platform for saying she was Elon. Elon said, you can do this, but you have to say
[00:25:23.800 --> 00:25:29.880]   it's a parody account. Did Facebook do that for a while? The sort of labeling things parody
[00:25:29.880 --> 00:25:37.400]   and that seemed to go away. Yeah. I mean, the problem with parody, and this is the famous
[00:25:37.400 --> 00:25:41.400]   onion definition of parody. Right. Well, and this is also the famous onion pleading
[00:25:41.400 --> 00:25:46.200]   in the in the Supreme Court cases. If it has to be labeled parody, that kind of takes some of the
[00:25:47.080 --> 00:25:51.640]   fun. Even Colbert had a joke about this the other day where he told the chicken cross the road joke,
[00:25:51.640 --> 00:25:56.040]   but set it up with explaining that he was about to tell him about the tell a joke kills the joke,
[00:25:56.040 --> 00:26:02.440]   right? Yes. Ed Markey tweeting Senator Markey, the Senator Markey, a Washington Post reporter
[00:26:02.440 --> 00:26:08.440]   was able to create a verified account impersonating me. I'm asking for answers from Elon Musk,
[00:26:08.440 --> 00:26:14.680]   who's putting profits over people and his debt over stopping disinformation. He sent a letter to
[00:26:14.680 --> 00:26:21.640]   a Musk, of course, as senators like to do. And then he tweeted, one of your companies,
[00:26:21.640 --> 00:26:27.720]   Elon is under an FTC consent decree, auto safety watch dog, NHTSA is investigating you for another
[00:26:27.720 --> 00:26:32.360]   company of yours for killing people. And you're spending your time picking fights online,
[00:26:32.360 --> 00:26:39.240]   fix your companies or Congress will to which Elon Musk replies to Senator Markey and the
[00:26:39.240 --> 00:26:45.160]   Washington Post, perhaps it's because your real account sounds like parody. That's not a smart
[00:26:45.160 --> 00:26:51.400]   way to respond to a Senator. No, it really isn't. That's not a good idea. But I understand, you know
[00:26:51.400 --> 00:26:57.560]   what? I kind of understand from that point of view, Elon's basically a 13 year old. And
[00:26:57.560 --> 00:27:01.560]   you know, I'm fighting words. I'm not going to take it.
[00:27:04.600 --> 00:27:09.560]   So what's if we covered everything should we do more on the impersonation things? Because
[00:27:09.560 --> 00:27:18.200]   this is crazy. Well, it's just the whole, it violated the whole principle of the blue check.
[00:27:18.200 --> 00:27:25.320]   Right. And by the way, Twitter has this long history of when people come up with a really bad
[00:27:25.320 --> 00:27:32.200]   idea that everybody knows is a bad idea, they exploited and it is exposed instantaneously.
[00:27:32.200 --> 00:27:38.440]   Like I think about the Microsoft AI thing that did a while ago. That was more of a Microsoft
[00:27:38.440 --> 00:27:42.600]   problem to be fair than a Twitter. Oh, yeah, it was a Microsoft. But I'm just talking about
[00:27:42.600 --> 00:27:50.120]   problems in general that the community realizes are stupid. So it always amuses me. They're like,
[00:27:50.120 --> 00:27:56.360]   Oh, I think we'll just do this. And people will go along. But if people don't like it and realize
[00:27:56.360 --> 00:28:03.240]   the fallacy in it, they'll prove very quickly the fallacy like they did with what Elon did,
[00:28:03.240 --> 00:28:09.560]   like they did with the Microsoft AI situation. So one of the fake accounts was Eli Lilly.
[00:28:09.560 --> 00:28:16.280]   And wish was a bad one. Yeah, which the fake account with an authorized blue check. I think by
[00:28:16.280 --> 00:28:20.760]   now everybody knows the blue check's meaningless. So this could only have. I actually, Leo, I don't
[00:28:20.760 --> 00:28:24.760]   actually know that that's true. I think that was part of the problem. I think we're more in the
[00:28:24.760 --> 00:28:30.280]   know, not everybody as in the know as we are here. I think the challenge is that not everybody
[00:28:30.280 --> 00:28:36.440]   knew that. And for one tweet that I think probably those of us listening to this recognized was
[00:28:36.440 --> 00:28:43.800]   not true. You know, it moved the market and it didn't just explain what happened.
[00:28:43.800 --> 00:28:47.400]   So right. So there was a show it. Yeah, I'll show it.
[00:28:47.400 --> 00:28:53.800]   My jumping ahead. So there was a tweet from what looked like if an authentic Eli Lilly account.
[00:28:54.680 --> 00:28:59.560]   Yeah. That said, insulin is now. It's gone now. So that's why it's not, I think it's not showing.
[00:28:59.560 --> 00:29:07.160]   Yeah. So basically it was like, insulin is now free. Yeah. It was not taken down. And it didn't
[00:29:07.160 --> 00:29:11.800]   sound official. There was no like, if that was true, you would usually see a press release and
[00:29:11.800 --> 00:29:19.560]   some more. But again, we're talking about a wide sort of birth of people who may not be as savvy.
[00:29:20.440 --> 00:29:25.880]   But it had a market impact. So it was what 3.4% or I forget what the actual one was.
[00:29:25.880 --> 00:29:32.120]   It was really in the market. Well, but then it had. It also had an impact on its competitors.
[00:29:32.120 --> 00:29:36.440]   So that was so no fee, no, no, no risk. So again, from my point of view,
[00:29:36.440 --> 00:29:44.040]   it seems that there is liability here because this was an avoidable. This was a situation that
[00:29:44.040 --> 00:29:49.480]   Twitter caused. I think the FTC is well within its rights to investigate.
[00:29:49.480 --> 00:29:55.720]   Eli Lilly on their on their actual official account, which now has both a blue check and an official
[00:29:55.720 --> 00:30:01.080]   check. We apologize who those who have been served a misleading message from a fake Lilly account.
[00:30:01.080 --> 00:30:04.200]   Our official Twitter account is Lily pad, which perhaps is not the best
[00:30:04.200 --> 00:30:09.960]   Twitter handle for an official account. The fake one sounded more official. It was Eli Lilly
[00:30:09.960 --> 00:30:15.880]   and company, right? That was the hand. Now, here's the thing. I think the stock will come back.
[00:30:15.880 --> 00:30:23.320]   Obviously, it's bogus. So that's not the issue. The issue is it raised awareness that Eli Lilly
[00:30:23.320 --> 00:30:29.320]   is, in fact, got price gouging on insulin. And so there is. That's not brand new news.
[00:30:29.320 --> 00:30:33.480]   I mean, anything is on it. No, but it brought it up, right? It caused some temperature.
[00:30:33.480 --> 00:30:37.800]   It was it was a horrible thing that happened to a company that we would all agree probably
[00:30:37.800 --> 00:30:42.440]   needed something horrible to happen to it. But that doesn't change the fact that this was a massive,
[00:30:42.440 --> 00:30:48.280]   massive drop. And yes, the stock is recovering, but it's still down 1.26% from where it was before
[00:30:48.280 --> 00:30:51.000]   this happened. The market's out. You have to look beyond the market, though. There's a long
[00:30:51.000 --> 00:30:54.200]   deal here. Yes. And I don't think the market's done. I don't think the market's done. I think
[00:30:54.200 --> 00:30:57.240]   we're going to have their shorts called. They're going to have their leverage positions called
[00:30:57.240 --> 00:31:03.000]   because of this. They can all go for remediation. I don't think the market's done. I think that
[00:31:03.000 --> 00:31:07.880]   they're pretty quickly. Any investor, serious investor knew that Eli Lilly was not announcing
[00:31:07.880 --> 00:31:12.840]   that insulin is free now. And if they did, they were quickly disabused of that.
[00:31:12.840 --> 00:31:20.040]   But it got something to the public for that. Then sort of overshadowed by other things. And so
[00:31:20.040 --> 00:31:24.760]   now there's a public conversation yet again, which there should be, by the way, it's embarrassing.
[00:31:24.760 --> 00:31:30.760]   Yes. But this goes into a greater discussion. And we talk about this a lot in security of,
[00:31:31.320 --> 00:31:37.960]   you don't have to make a permanent, if you want to cause damage, it doesn't need to be permanent.
[00:31:37.960 --> 00:31:43.560]   It can be temporary. And temporary is enough to cause enough damage. And this is exactly what
[00:31:43.560 --> 00:31:50.440]   happened. It's weaponized information. We call it, we call it mental malware.
[00:31:50.440 --> 00:31:56.840]   Are we lucky? Remember this, this all happened right before the midterm elections. Are we lucky
[00:31:57.640 --> 00:32:02.920]   that in fact, it didn't disrupt the elections? I mean, it could have, right?
[00:32:02.920 --> 00:32:07.560]   Oh, yeah. There's a lot of damage that could have done. It was 24 hours beforehand.
[00:32:07.560 --> 00:32:10.440]   It's not enough time for the Russian bot farms to...
[00:32:10.440 --> 00:32:16.680]   Yeah. I mean, again, I think the story about the fact that Twitter was imploding sort of
[00:32:16.680 --> 00:32:21.720]   overshadowed the potential misinformation that might have been sent.
[00:32:22.520 --> 00:32:31.800]   So big layoffs, lots of brain drain going on. Elon not looking too serious, although I note,
[00:32:31.800 --> 00:32:35.640]   you know, in the first few days after he bought Twitter, he tweeted three or four times at the
[00:32:35.640 --> 00:32:39.960]   normal rate. He's really dropped off a lot. And most of his tweets have nothing to do with
[00:32:39.960 --> 00:32:45.160]   Twitter all of a sudden. Although he did tweet on Friday that all of this excitement has really
[00:32:45.160 --> 00:32:52.360]   increased engagement on Twitter. He said, "Well, it's really improved." The number of posts on
[00:32:52.360 --> 00:32:59.080]   Twitter, which may be true, I don't know, sounds a little like he protests too much.
[00:32:59.080 --> 00:33:05.880]   Yeah, they're doing so well that he sold another $4 billion of Tesla stock to cover the losses.
[00:33:05.880 --> 00:33:09.320]   Yeah. So yeah, that sounds like a... I mean, the real problem for Twitter,
[00:33:09.320 --> 00:33:16.280]   let's say they get past the FTC and a pissed off Senator, the real problem is that advertisers
[00:33:16.280 --> 00:33:20.920]   don't want to be anywhere near this, right? That's the... Did you see he threatened the advertisers?
[00:33:20.920 --> 00:33:27.560]   Yeah. He's going to charge them more for big cowards. Oh my gosh, that's an episode from that.
[00:33:27.560 --> 00:33:32.600]   He was going to go sunset strip show. He was going to say he was going to go
[00:33:32.600 --> 00:33:38.840]   thermonuclear name and shame if they cut off their advertising. That's a guarantee
[00:33:38.840 --> 00:33:44.280]   that advertisers are going to... What are you trying to do? Who's talks to their customers that way?
[00:33:44.280 --> 00:33:49.480]   He is in effect telling advertisers, "I am a loose candidate and I can hurt your brand."
[00:33:49.480 --> 00:33:55.800]   I don't think that's what he's doing. I honestly don't think... I think what he's doing is creating
[00:33:55.800 --> 00:34:01.400]   a situation. So if any of you out there have employees, think about the process that you would
[00:34:01.400 --> 00:34:06.040]   use to get rid of an employee who's underperforming, you can't just fire them.
[00:34:06.040 --> 00:34:10.520]   You know, I guess unless you're putting them on, you create a paper. You have to go through a process
[00:34:10.520 --> 00:34:15.880]   where you either... I mean, some companies sort of make the circumstances such that they want to quit,
[00:34:15.880 --> 00:34:23.080]   right? Or you have a whole long process of write-ups and things like that. Is it plausible that
[00:34:23.080 --> 00:34:29.880]   given the weight of the investment and the total improbability that any current financial model
[00:34:29.880 --> 00:34:34.600]   works out to pay that back, that he's creating the circumstance where there is no alternative,
[00:34:34.600 --> 00:34:38.440]   but bankruptcy so that he gets out of this whole thing and Twitter dies permanently?
[00:34:38.440 --> 00:34:44.520]   That would be my take, my hot take. There was talk that he wasn't... That
[00:34:44.520 --> 00:34:50.280]   bankruptcy was, I think, a possible option. He said he told... I think it is the only option.
[00:34:50.280 --> 00:34:54.120]   It's inevitable. But he said this is the only one who has a payment platform.
[00:34:54.120 --> 00:34:57.160]   It doesn't make sense when there's plenty of... It's just a big sense. You buy something and
[00:34:57.160 --> 00:35:01.400]   then you file for bankruptcy? No, I don't think that's why he did it. I think he was... Listen,
[00:35:01.400 --> 00:35:09.080]   I mean, again, let's go back in time. I think this started out as a probably overconfident ruse
[00:35:09.080 --> 00:35:13.240]   that turned into what felt like a real thing. I mean, we're kind of seeing this pattern over
[00:35:13.240 --> 00:35:18.920]   and over again with powerful men, right? And then it kind of became real and there was no way to
[00:35:18.920 --> 00:35:23.800]   walk things back. And this seems to be the only way for him to get out of it at this point.
[00:35:25.000 --> 00:35:29.080]   It's not a really good way to get out of it, though, because I mean, he's burning a lot of his personal
[00:35:29.080 --> 00:35:33.080]   love. Yeah, but he didn't want to be in it. The problem was there was no way to get out without
[00:35:33.080 --> 00:35:37.640]   a lawsuit. Yeah, he was... Apparently, I agree with you, Amy, that the original
[00:35:37.640 --> 00:35:45.800]   decision to buy Twitter was either a joke or... I don't think it was a joke. I mean, I think that
[00:35:45.800 --> 00:35:49.560]   there's a certain amount of high-falutian loathing that's happening right now. Yeah,
[00:35:49.560 --> 00:35:54.840]   blovey-atings. Among our tech Oleia garks who are running the show all over the place, and this was
[00:35:54.840 --> 00:36:00.840]   kind of like... He was shown his tail feathers. A little bit, yeah. And then it kind of got real.
[00:36:00.840 --> 00:36:06.200]   And there's no way to get out of it without a lawsuit. Which was the whole like,
[00:36:06.200 --> 00:36:10.520]   "All of Twitter is bots proved me they're not. This is a reason for me. I've got grounds to
[00:36:10.520 --> 00:36:13.960]   not go through at the sale." Do you think he always thought, "Well, I can always get out of it."
[00:36:13.960 --> 00:36:20.280]   Yeah, I think that, again, I would call them the sort of tech Oleia garks think that they can
[00:36:20.280 --> 00:36:25.480]   get in or out of anything. I think that's part of how we got to know. Yeah. And then for some reason,
[00:36:25.480 --> 00:36:30.680]   really feared what was going to happen in the Delaware Court of Chancery. He was about to be
[00:36:30.680 --> 00:36:37.320]   deposed. They had already revealed some embarrassing direct messages. Probably he knew of others
[00:36:37.320 --> 00:36:42.360]   that were to come. I'm wondering if he had some conversations with Mudge, Peter Zatko,
[00:36:42.360 --> 00:36:47.720]   the Twitter whistleblower at some point that might have been incriminating. So for some reason,
[00:36:47.720 --> 00:36:53.480]   which we don't fully know, he decided, "I guess I have to buy it." Do you think at that point,
[00:36:53.480 --> 00:36:59.320]   he starts squirming and looking for ways. So he put in of his own money,
[00:36:59.320 --> 00:37:04.680]   I don't know, somewhere around 30 billion. He borrowed 30 billion. Was it? I thought it was like,
[00:37:04.680 --> 00:37:09.160]   I thought he put up the money, but I thought the real amount was closer to a billion. Was it that
[00:37:09.160 --> 00:37:14.760]   much? Oh, you know, it was a lot. It was way way. It was more than that. Okay. Yeah. So he got 30,
[00:37:14.760 --> 00:37:20.280]   borrowed 13, I think, billion from banks. Those guys cannot be very happy at this point.
[00:37:20.280 --> 00:37:25.560]   They're really exposed. They were really foolish for going in on us.
[00:37:25.560 --> 00:37:30.360]   No, it's a mistake, right? But he also got money from Larry Ellison, kicked in a couple of bill.
[00:37:30.360 --> 00:37:36.120]   The Saudi Sovereign Fund, we don't know how much, but a lot. So you're right, Elon might not have
[00:37:36.120 --> 00:37:43.960]   been in for more than 13 or 14 billion. If he declares bankruptcy, would it be?
[00:37:43.960 --> 00:37:49.320]   Chapter 11 reorganization, is that what you think? Does he then get to put off the creditors?
[00:37:49.320 --> 00:37:53.560]   He's got more than a billion dollar yearly annual interest.
[00:37:53.560 --> 00:37:57.720]   So again, I think this is one of these things where if everybody, if the Saudi
[00:37:57.720 --> 00:38:02.840]   royal, you know, the Saudi Sovereign Wolf Fund still wants to go out and have, I guess, not a beer,
[00:38:02.840 --> 00:38:07.000]   but some type of non-alcoholic drink, etc. on a bar, because Elon's cool, then everything is
[00:38:07.000 --> 00:38:12.280]   copacetic. I just don't think that's going to happen. I think he is actually stuck. I think he's
[00:38:12.280 --> 00:38:17.240]   actually stuck. And the worse that the situation gets, the worse this becomes for his other holdings
[00:38:17.240 --> 00:38:21.720]   from the other companies and their market cap. This is a calculating-
[00:38:21.720 --> 00:38:23.720]   Tesla cycle of apocalyptic health.
[00:38:23.720 --> 00:38:29.560]   This is a question, I mean. Do you think that these, because I was just thinking about the sheer
[00:38:29.560 --> 00:38:35.560]   number of people that were let go, what other company could function if they just one day
[00:38:35.560 --> 00:38:40.600]   got rid of half of their employees? And I'm talking any company over a thousand employees.
[00:38:40.600 --> 00:38:45.480]   How do you function if half in your employees are gone? And Samaritan, right? Without a lot.
[00:38:45.480 --> 00:38:47.320]   There weren't that many to start. They only had what?
[00:38:47.320 --> 00:38:49.720]   6, 6,000. 7,500. Yeah.
[00:38:49.720 --> 00:38:56.120]   And so could this been all a plan like, well, I'm going to have to like he was thinking
[00:38:56.120 --> 00:39:00.600]   dangerous before this? That's exactly what I'm saying. I think Elon is quiet quitting Twitter.
[00:39:00.600 --> 00:39:04.600]   He's quiet quitting Twitter. I mean, I think that's what's happening.
[00:39:04.600 --> 00:39:05.880]   What happens to this one?
[00:39:05.880 --> 00:39:11.080]   How does he get out of this? What happens to his finance?
[00:39:11.080 --> 00:39:12.520]   We're watching. What do you think?
[00:39:12.520 --> 00:39:17.160]   What are the assets? Listen, Twitter, so again, let's stop for a moment and like,
[00:39:17.160 --> 00:39:21.160]   forget about all the insanity. It's happened. If Twitter were to go away tomorrow,
[00:39:21.160 --> 00:39:26.520]   Twitter is not general purpose technology, right? Like electricity. The only way to calculate-
[00:39:26.520 --> 00:39:31.720]   what's the total valuation of the internet? There's no way to calculate it but for in the reverse.
[00:39:32.520 --> 00:39:37.160]   Right? That's a general purpose technology. That is not Twitter. Twitter goes away tomorrow.
[00:39:37.160 --> 00:39:41.400]   There's no meaningful economic impact. There might be on the companies that are
[00:39:41.400 --> 00:39:46.520]   solely set up to serve Twitter and anybody who's using OAuth, you know, and using Twitter to sign
[00:39:46.520 --> 00:39:52.280]   on, there might be hiccups. Twitter goes away tomorrow. There's no real problem. There's no real
[00:39:52.280 --> 00:39:58.440]   infrastructure. The user base isn't all that valuable because it's relatively small, right?
[00:39:59.560 --> 00:40:05.080]   So again, who would buy it? How do you monetize it? There's no business.
[00:40:05.080 --> 00:40:09.240]   That's a good point. There's a lot of other services that would cause major disruption.
[00:40:09.240 --> 00:40:12.040]   That's a good point. Yeah, it's not the power grid for crying out loud.
[00:40:12.040 --> 00:40:17.800]   No, but do you think of things like sales force or Oracle if they went down?
[00:40:17.800 --> 00:40:20.040]   Yeah. Well, that would cause some serious disruption.
[00:40:20.040 --> 00:40:24.760]   He can't sell it. Nobody's going to buy it even for a billion dollars at this point, right?
[00:40:24.760 --> 00:40:30.680]   He's really turning it down. Now, years and years ago, I was talking to, I was wasting my breath on
[00:40:30.680 --> 00:40:35.960]   some news organizations and saying that there's no way that this is going to monetize in the
[00:40:35.960 --> 00:40:39.800]   long run because the average, like there's no model that I saw really working. So,
[00:40:39.800 --> 00:40:43.880]   news work should buy it, turn it into a 21st century wire service.
[00:40:43.880 --> 00:40:44.680]   I love it.
[00:40:44.680 --> 00:40:49.720]   Allow people to use it as we currently do, but police some of the bots and other things and
[00:40:50.440 --> 00:40:56.840]   control more of the advertising destiny. Nobody would collaborate to do that.
[00:40:56.840 --> 00:40:57.880]   So, I don't know.
[00:40:57.880 --> 00:41:01.640]   That's, by the way, the people who are not moving off of Twitter
[00:41:01.640 --> 00:41:08.680]   include all the news services. Although Reuters now has a mastodon feed,
[00:41:08.680 --> 00:41:13.800]   but I think the news services don't have anything anywhere they can go like Twitter.
[00:41:13.800 --> 00:41:17.960]   Oh, well, they'll always be somewhere where people want to spout something off and
[00:41:17.960 --> 00:41:19.560]   release information.
[00:41:19.560 --> 00:41:23.320]   No, but I'll tell you, because we work with some of them and some others.
[00:41:23.320 --> 00:41:29.560]   And I know that there was a meeting among the largest foundations, whether or not they should
[00:41:29.560 --> 00:41:31.720]   all leave Twitter or stay on.
[00:41:31.720 --> 00:41:41.160]   Well, you got to stay where the news is breaking. You just have to. That's your job as being news.
[00:41:41.160 --> 00:41:46.360]   Like when people complained about reporting on our president, I'm like, well, he's the president.
[00:41:46.360 --> 00:41:52.760]   You got to kind of report on that. So it's kind of the job of news to where the news is breaking,
[00:41:52.760 --> 00:41:54.840]   or where you can get that information. You have to gather it.
[00:41:54.840 --> 00:41:57.080]   So a few organizations.
[00:41:57.080 --> 00:42:01.800]   It's something super unique. They didn't really own any IP that can't be replicated.
[00:42:01.800 --> 00:42:05.560]   They had a couple of patents for machine learning. They had a couple of patents for interconnected
[00:42:05.560 --> 00:42:10.600]   networks, but those were just utility patents. They weren't something that you could like.
[00:42:10.600 --> 00:42:15.240]   They're not all members. That is their asset.
[00:42:15.240 --> 00:42:17.640]   The user base was the members and he drove them away.
[00:42:17.640 --> 00:42:21.560]   I mean, remember, they're not all gone yet. Some have left.
[00:42:21.560 --> 00:42:23.080]   There's still plenty of people. But the important ones.
[00:42:23.080 --> 00:42:27.400]   You have to remember, everyone looks at the 1.3 billion users of Twitter and they say,
[00:42:27.400 --> 00:42:31.240]   that's the big number. So even if a million left, that's not really that big of a problem.
[00:42:31.240 --> 00:42:36.600]   But you look at the revenue of Twitter, 92% came from advertisers. Of that, 92% that came from
[00:42:36.600 --> 00:42:43.240]   advertisers, 50 to 60% came from the United States. Content developed in the United States.
[00:42:43.240 --> 00:42:49.480]   So in the United States, you had about 38 million active daily users. 25% of those,
[00:42:49.480 --> 00:42:57.320]   or about 9 million, generated the content of about 97% of that. So 9 million users in the United
[00:42:57.320 --> 00:43:04.360]   States are responsible for roughly three quarters of Twitter's revenue.
[00:43:05.080 --> 00:43:08.120]   So if he's driven away a million of those, that is a huge hit.
[00:43:08.120 --> 00:43:12.600]   That is unlabelable. The latest Mastodon number is the last week they've grown to one and a half
[00:43:12.600 --> 00:43:19.080]   million users. So I presume most of them came from Twitter. Well, they're definitely getting
[00:43:19.080 --> 00:43:24.360]   plenty of butts. Yeah. Mastodon is not a Twitter replacement.
[00:43:24.360 --> 00:43:30.920]   It's not. And it shouldn't be. It's designed to kind of avoid some of the pitfalls Twitter
[00:43:31.560 --> 00:43:38.520]   offers, especially to disadvantaged people, people in the LGBTQ+
[00:43:38.520 --> 00:43:50.280]   community. They left Twitter a long time ago, frankly, because Twitter has always been kind of,
[00:43:50.280 --> 00:43:58.600]   last five years, been a toxic place. It's really, so are we going to mourn the loss of Twitter,
[00:43:59.160 --> 00:44:07.400]   Father? Yeah. Okay. My experience of Twitter was a lot different than some others because
[00:44:07.400 --> 00:44:13.640]   very early on I wrote a bot that got rid of the worst signals from Twitter. And it was very good
[00:44:13.640 --> 00:44:19.960]   at it. It made Twitter a very useful place for me. And it still is. And yes, I'm on Mastodon.
[00:44:19.960 --> 00:44:25.480]   Yes, I'm trying out a couple of other social media services. But Twitter was actually useful
[00:44:25.480 --> 00:44:30.440]   for me. It wasn't just a place to broadcast thoughts. I got some very interesting ideas
[00:44:30.440 --> 00:44:34.680]   from the people that I met in the communities that I formed within Twitter. So that is a unique
[00:44:34.680 --> 00:44:40.360]   forum that I do not think is duplicatable anywhere else on an existing internet property.
[00:44:40.360 --> 00:44:51.000]   I agree. And I've used Twitter again, since like 2006 and along the way. So early on,
[00:44:51.000 --> 00:44:56.360]   that's where my friends were. And it was fun to have conversations on public, I guess. But
[00:44:56.360 --> 00:45:04.120]   I was there for the links people were sharing pre political, you know, Meyer. And Jonathan Abrams,
[00:45:04.120 --> 00:45:11.240]   who I really respect is always slightly ahead of the curve. He founded Friendster.
[00:45:11.240 --> 00:45:17.400]   He also started something called Nuzzle and use easy EL, which was this awesome app that you
[00:45:17.400 --> 00:45:22.680]   could set up. If you had Twitter lists, once a certain threshold of people posted accounts
[00:45:22.680 --> 00:45:27.400]   posted the same link, you would get a notification. So this is a really, really great way. I made
[00:45:27.400 --> 00:45:32.280]   like 200 private Twitter lists that were each about a specific topic. It was a really great way
[00:45:32.280 --> 00:45:37.720]   to surface totally original signals that I just would not have picked up on my own. Actually,
[00:45:37.720 --> 00:45:41.400]   then Twitter bought it and turned it into blue and dumped it down and made it terrible. But
[00:45:42.840 --> 00:45:50.360]   and I use Twitter to present. I used to have the script running in keynote. And I would, anytime I
[00:45:50.360 --> 00:45:56.440]   gave a keynote presentation somewhere, I would sort of have a back channel of me talking while I
[00:45:56.440 --> 00:46:01.080]   was talking on stage. And it would tweet out the links. You know, I just I would like try to think
[00:46:01.080 --> 00:46:04.840]   ahead of what somebody might stop when I was talking to look up on their own. And I would
[00:46:04.840 --> 00:46:12.280]   tweet it while I was saying it. You know, it's the it's like those other utilities that I really
[00:46:13.080 --> 00:46:20.440]   really miss. And I miss. Yeah, it it it's a different place than it was. And I'm sort of ready to let it go.
[00:46:20.440 --> 00:46:28.680]   Um, Black Twitter. A perfect example of a community that
[00:46:28.680 --> 00:46:34.200]   formed on Twitter that gave people voices that didn't have voices before the Arab Spring.
[00:46:34.200 --> 00:46:39.080]   There are lots of historic things that happened on Twitter.
[00:46:41.000 --> 00:46:44.680]   So I mean, I think it's appropriate. Are we premature in a morning?
[00:46:44.680 --> 00:46:51.080]   I think there's a giant dump fest going on right now on Twitter. That's what I'm hearing.
[00:46:51.080 --> 00:46:57.080]   What do you mean by dump fest? There's a I'm watching the feed here and I'm listening to you. I think
[00:46:57.080 --> 00:47:01.400]   there's a lot of people trash again. A lot of people spend many, many years building up their
[00:47:01.400 --> 00:47:06.920]   community in their audience. And right now there's one guy who's acting very aggressively and he owns
[00:47:06.920 --> 00:47:13.640]   it. I say we should all chill out. Just stick around and see what happens. That was my original
[00:47:13.640 --> 00:47:18.760]   plan until I was kind of force fed Elon's tweets. And I thought, you can always can
[00:47:18.760 --> 00:47:23.080]   not stop following. I could I'm I could. But so what happened? You just you had not you didn't
[00:47:23.080 --> 00:47:27.800]   have the chronological view on. Yeah, I had the home feed on and I had like eight Elon Musk tweets
[00:47:27.800 --> 00:47:34.520]   in a row, all of which were worse and worse and worse. He's by the way, stop doing that.
[00:47:34.520 --> 00:47:39.160]   I think he probably got the message that probably is counterproductive. Let's see what is I mean,
[00:47:39.160 --> 00:47:46.040]   these things. Listen, I think all of this thing, I'm a futurist, but we look backwards twice as
[00:47:46.040 --> 00:47:51.400]   much as we look forwards and I, you know, we sort of go in cycles of things. So, you know,
[00:47:51.400 --> 00:47:55.800]   we've got social media that are kind of imploding. I think I think we're in a period of social
[00:47:55.800 --> 00:48:01.080]   distortion where we're sort of spreading out again and going into different directions. But at some
[00:48:01.080 --> 00:48:05.720]   point, years down the line, there will be consolidation again. So the Mastodon may be
[00:48:05.720 --> 00:48:10.360]   decentralized, but there will still be a consolidation of players in the market as there is with every
[00:48:10.360 --> 00:48:16.040]   single industry over and over again. So it's interesting. So I said in the pre show, I said in
[00:48:16.040 --> 00:48:19.560]   the pre show that I'm going to be slim pickens right in the spa and into the ground. I mean, I,
[00:48:19.560 --> 00:48:24.920]   I like David said, I spent years building up a community that I find very valuable on Twitter.
[00:48:24.920 --> 00:48:28.600]   And so I'm going to stay there if for nothing else, but for them, was that a strange
[00:48:28.600 --> 00:48:33.160]   love reference time I am moving to Vasadon? I'm sorry. Was that a strange love reference?
[00:48:33.160 --> 00:48:37.720]   Yeah, of course. Yeah. I got to throw one of the slim pickens right in the
[00:48:37.720 --> 00:48:43.880]   at Tom. Yeah, it's my favorite movie. I just I love tearing that pot. So much that I
[00:48:43.880 --> 00:48:49.880]   interrupt to do. I apologize. Just I mean, there is there is an interesting tangent to this. You
[00:48:49.880 --> 00:48:55.720]   heard about the the senior director of engineering got caught sending out a slack that is just tone
[00:48:55.720 --> 00:49:03.480]   deaf now do tell so Luke, a simon. Look, Evan Simmons. He is the senior director of engineering
[00:49:03.480 --> 00:49:08.600]   at Twitter sent out a slack saying, I'm going to I'm going to quote this. This is going to be the
[00:49:08.600 --> 00:49:14.040]   challenge. The engineers I am bringing back the ones he fired and then rehired are weak, lazy,
[00:49:14.040 --> 00:49:19.000]   unmotivated, and they may even be against an e on Twitter. They were cut for a reason. So we
[00:49:19.000 --> 00:49:22.680]   need to think of these people as just needing to be around until the knowledge transition is
[00:49:22.680 --> 00:49:27.880]   completed. Imagine sending that out on a company slack to the people who are going to read and say,
[00:49:27.880 --> 00:49:32.840]   yeah, I I'm going to be as slow as possible. I'm not motivated for you. So wait a second. Let's
[00:49:32.840 --> 00:49:36.760]   let's pause for a moment on that. So that really went out. Yes, that really went out. That really
[00:49:36.760 --> 00:49:42.760]   went out. Yes. So you bring those to my boys, but you called them lazy. Yep. Was that posturing
[00:49:42.760 --> 00:49:47.800]   though? Just because half of those employees are going to design back doors into their software.
[00:49:47.800 --> 00:49:51.880]   So they could hack it later when they're dumped again. Well, if I was
[00:49:51.880 --> 00:49:57.800]   employed Pandora's box of problems. If I was brought back, I am writing 10 lines of code where
[00:49:57.800 --> 00:50:03.960]   one will do. I mean, come on, I will go as slow as possible. I will milk it for every bit of salary
[00:50:03.960 --> 00:50:08.440]   and and concessions that I can get more of like, I'm going to build a back door so I can get into
[00:50:08.440 --> 00:50:19.640]   this thing when I want to. Yeah. So it's they some of them, I understand have to come back because they
[00:50:19.640 --> 00:50:24.840]   weren't they were in order to avoid the war and act. Elon who originally wanted just to fire him
[00:50:24.840 --> 00:50:31.880]   without severance. No notice, realized he had to give him 90 days of continued salary. Those people
[00:50:31.880 --> 00:50:39.240]   are still getting paid. And so it's easy to call them back because they never really left. And
[00:50:39.240 --> 00:50:43.160]   if they don't come back, they say, well, then you're not getting your 90 days. Now you've quit.
[00:50:43.160 --> 00:50:49.240]   So you might wonder, well, why would anybody come back after being fired? Well, that's why.
[00:50:49.240 --> 00:50:56.760]   They may not go to my point. Who could dump half of their staff and still be operation? Yeah. And then
[00:50:56.760 --> 00:51:00.680]   he goes, wait a second. I can't still be a dump. A dump of five days after you walk in the door.
[00:51:00.680 --> 00:51:05.720]   How do you even know? And certainly, as you point out, count the number of pages of code
[00:51:05.720 --> 00:51:12.520]   committed is a terrible way to do it. Can I ask you guys a question? Yeah. Have any of you
[00:51:12.520 --> 00:51:21.000]   had conversations about Elon with others and gotten into had bad react? Let me put it this way.
[00:51:21.000 --> 00:51:27.400]   I feel like Elon's almost religion on his own. And there's no room. I mean, two things get people
[00:51:27.400 --> 00:51:32.680]   more upset than anything else. And that is there like crypto and Elon.
[00:51:32.680 --> 00:51:39.400]   These are our two big stories. Yeah, I know. But like the reaction that people have to any
[00:51:39.400 --> 00:51:46.360]   critical, any critical, like logical criticism of what's happening right now is grounds for,
[00:51:46.360 --> 00:51:55.000]   you know, verbal assault. I don't know. There's a very diagram. Sorry, Padra.
[00:51:55.000 --> 00:51:59.000]   I was just there's a Venn diagram here of the block bot. You have the Elon stands,
[00:51:59.000 --> 00:52:02.680]   you have the crypto stands, and then you have the Joe Rogan stands. And where do they meet?
[00:52:02.680 --> 00:52:08.120]   That's right in the middle. That's an instant block. That's just no, I'm sorry. I don't want
[00:52:08.120 --> 00:52:14.040]   your bot. Anything is your bot works. It's a Venn diagram. I need to take a break. We'll have more.
[00:52:14.040 --> 00:52:20.360]   It's an amazing time we're living in. And then there's FTX. We're going to talk about that.
[00:52:21.480 --> 00:52:28.360]   Our show today brought to you by worldwide technology and HPE Hewlett Packard Enterprise.
[00:52:28.360 --> 00:52:33.400]   WWT is at the forefront of innovation working with clients all over the world to transform their
[00:52:33.400 --> 00:52:39.960]   businesses. And I have to tell you, because we went out and saw it right before COVID in March of
[00:52:39.960 --> 00:52:44.840]   2020, Lisa and I went out to St. Louis and saw their Advanced Technology Center. And this thing
[00:52:44.840 --> 00:52:52.760]   is a mind blowing amazing thing for anybody to have. Anybody. The ATC is a research and testing lab
[00:52:52.760 --> 00:52:58.760]   with more than half a billion dollars of equipment from all the OEMs that you might deal with.
[00:52:58.760 --> 00:53:03.400]   Why did they build this? I mean, they've been building it for a decade. This is in many buildings,
[00:53:03.400 --> 00:53:09.160]   rack after rack of incredible gear. Well, they built it at first for their own engineers. So they
[00:53:09.160 --> 00:53:18.280]   could test products, integrate solutions, rollout solutions. It would reduce their evaluation time
[00:53:18.280 --> 00:53:24.920]   for new technologies. For months to weeks, they could spin up proofs of concept and pilots. It was
[00:53:24.920 --> 00:53:33.400]   a great way for WWT engineers, for the WWT team to understand the technologies that they're working
[00:53:33.400 --> 00:53:42.680]   with. But they've done something that is really incredible. They have opened it up to all of you,
[00:53:42.680 --> 00:53:49.560]   anybody on the WWT platform can access the ATC. You don't even have to go to St. Louis to do it.
[00:53:49.560 --> 00:53:55.880]   They offer hundreds of on demand and schedulable labs you can do from anywhere in the world.
[00:53:55.880 --> 00:54:01.080]   At any time of the day or night, like HPE's Data Services Cloud Console Lab,
[00:54:01.960 --> 00:54:06.280]   these labs represent the newest advances in every area of enterprise technology,
[00:54:06.280 --> 00:54:11.080]   multi-cloud architecture, security, networking, primary and secondary storage,
[00:54:11.080 --> 00:54:16.600]   data analytics, AI, even things like DevOps process, and so much more.
[00:54:16.600 --> 00:54:24.600]   This has been a great boon for WWT internally, but now it's available to everybody. With the ATC,
[00:54:24.600 --> 00:54:29.960]   you can test out products and solutions before you go to market. You can access technical articles,
[00:54:29.960 --> 00:54:36.280]   expert insights, demonstration videos, white papers, hands-on labs, all the tools you need to stay
[00:54:36.280 --> 00:54:42.680]   up to date with latest technology. Here's the key. WWT is a partner. They're a partner and they
[00:54:42.680 --> 00:54:49.640]   want their partners to succeed. That's why they offer you these great tools. It's not just a physical
[00:54:49.640 --> 00:54:56.200]   lab space in St. Louis. It's a completely virtual lab that you can access anywhere in the world 365
[00:54:56.200 --> 00:55:01.240]   days a year and it's free. All you have to do is be a member of the ATC platform. That's not the only
[00:55:01.240 --> 00:55:05.880]   thing WWT does. We went out there in March and we did an event with so much fun. Mary Jo Foley
[00:55:05.880 --> 00:55:13.560]   came out with us. Make sure you check out WWT's events and communities, lots of stuff going on there.
[00:55:13.560 --> 00:55:19.240]   Learn together about the technology trends, hear the latest research and insights
[00:55:19.240 --> 00:55:25.720]   from the WWT experts because they are your partners in this. Whatever your business need,
[00:55:26.120 --> 00:55:33.240]   WWT can deliver scalable, tried and tested tailored solutions. WWT brings strategy and
[00:55:33.240 --> 00:55:39.720]   execution together to make a new world happen. Learn more about WWT, the Advanced Technology Center,
[00:55:39.720 --> 00:55:46.040]   to gain access to all their free resources. It's very simple. Just go to www.wt.com/twit,
[00:55:46.040 --> 00:55:56.040]   create a free account on the ATC platform. www.wt.com/twit. It is such a benefit to every company that
[00:55:56.040 --> 00:56:04.600]   uses enterprise technology. You'd be crazy not to join it. www.wt.com/twit. David Spark is here from
[00:56:04.600 --> 00:56:11.880]   well, I've known him for years, but his current effort is CSOS series at CSOS series.com. You
[00:56:11.880 --> 00:56:16.680]   interview CSOS, right? And other security. Let's talk about security.
[00:56:16.680 --> 00:56:22.200]   And I might as well give out your Twitter handle at dspark because you're not leaving, man.
[00:56:22.200 --> 00:56:29.400]   I'm holding it. I'm not one to do knee jerk reaction. I think that's smart.
[00:56:29.400 --> 00:56:34.440]   Let's see how this thing plays itself out. I'm also though not one to kind of hang around
[00:56:34.440 --> 00:56:39.160]   a toxic environment. I just don't need that in my... Oh, well, you don't have to necessarily
[00:56:39.160 --> 00:56:44.040]   shut down your account. It's just... Oh, no, I didn't kill my account. I got half a million followers.
[00:56:44.040 --> 00:56:48.760]   I didn't kill it, but I ain't visiting it. It's been years since I've been able to read
[00:56:48.760 --> 00:56:55.160]   out replies anyway. I've only been a minor user of Twitter for the last three or four years.
[00:56:55.160 --> 00:57:00.920]   But nobody hears leaving, which is interesting. Amy Webb, CEO of Future Today Institute.
[00:57:00.920 --> 00:57:08.360]   Did you predict this would... It's really not Twitter. It's a social media. Look at Facebook's
[00:57:09.240 --> 00:57:14.680]   valuation. Yeah, again, there are cycles that every industry goes through.
[00:57:14.680 --> 00:57:24.120]   From my point of view, we were at the zenith of the cycle. And so it makes sense what's happening
[00:57:24.120 --> 00:57:28.680]   right now. Maybe uncomfortable. Yeah.
[00:57:28.680 --> 00:57:32.520]   Amy, can I ask you a quick question about cycles right there?
[00:57:32.520 --> 00:57:37.640]   When things don't have much of a history or things are changing so rapidly,
[00:57:38.440 --> 00:57:43.240]   what do you use to predict? And I know I'm being super great. But the reason I bring this up is
[00:57:43.240 --> 00:57:47.480]   because one topic that comes up with us is cyber insurance. And I'm like, "Jesus,
[00:57:47.480 --> 00:57:52.520]   that changes so drastically. It's not like the industry has decades of information
[00:57:52.520 --> 00:57:57.720]   of actuarial tables, like auto insurance." Oh, no, we actually do a ton of work in insurance.
[00:57:57.720 --> 00:58:02.200]   And those actuarial tables have pretty much not been updated in the past century. So,
[00:58:02.200 --> 00:58:04.600]   no, they're still trying to sort things out. That's pretty true.
[00:58:04.600 --> 00:58:11.000]   So, what do you do when the history is either so drastically changing, you can't see a pattern?
[00:58:11.000 --> 00:58:17.160]   What is it that you do? We can always see patterns. The issue is making sure we're seeing
[00:58:17.160 --> 00:58:21.000]   evidence and data-backed patterns and not ones that we're seeing because of our own bias. But
[00:58:21.000 --> 00:58:27.480]   we actually don't make predictions. So, I didn't, we would not say that we would predict something
[00:58:27.480 --> 00:58:31.720]   would happen on such and such a date or by such and such time. Right, right. But you're showing
[00:58:31.720 --> 00:58:37.720]   trends more. Yeah. Yeah. So, we build models, evidence-based models, sometimes primary, second,
[00:58:37.720 --> 00:58:42.120]   yep. I don't want to get into the weeds because it would bore that out of everybody who's listening.
[00:58:42.120 --> 00:58:50.280]   But yeah, even when there are, so what social media is 15-ish, maybe a little bit longer,
[00:58:50.280 --> 00:58:57.320]   18 years old, we could say. It's every industry or micro industry moves at a different speed and
[00:58:57.320 --> 00:59:02.920]   trajectory. And we're just, one of the biggest signals we look for is consolidation. So,
[00:59:02.920 --> 00:59:07.560]   consolidation over time tends to attract regulatory scrutiny and, you know, all the other stuff that
[00:59:07.560 --> 00:59:14.360]   comes with it. And either that micro industry survives because there's no, there's nothing else,
[00:59:14.360 --> 00:59:19.240]   or there's an implosion and something else happens. So, a good analogy here would be drugstores in
[00:59:19.240 --> 00:59:27.160]   the United States. CVS may have incredible technology, but it is, it does not care about
[00:59:27.160 --> 00:59:33.800]   customers at pharmacies. And so, the tech might be great, but the user experience is horrible,
[00:59:33.800 --> 00:59:38.360]   but there's no where else to go, which is what left the opening for Amazon. Because there's so
[00:59:38.360 --> 00:59:43.240]   much market consolidation, it would be so challenging to set up a competitor that, you know, so we've
[00:59:43.240 --> 00:59:50.600]   got like two or three in the market. That one continues to hold. Social media didn't. So, yeah.
[00:59:50.600 --> 00:59:56.600]   Also with us, Father Robert Ballasier, the digital Jesuit, who is also not leaving Twitter,
[00:59:56.600 --> 01:00:00.120]   but does have four mastodon count and four mastodon accounts.
[01:00:00.120 --> 01:00:05.000]   I think it's more than that, actually. There's a bunch that I just have forgotten about.
[01:00:05.000 --> 01:00:10.680]   It was, I mean, it was a very interesting platform from the very beginning. It was very
[01:00:10.680 --> 01:00:15.560]   friendly to writing bots and I was experimenting. And it was that thing where I would
[01:00:15.560 --> 01:00:19.320]   create an account and forget about it and create another account. I did that for three or four years.
[01:00:19.320 --> 01:00:23.080]   That's fine. Yeah, you're going to find a lot. You're on Twitch social now. That's all it matters.
[01:00:23.880 --> 01:00:31.000]   Exactly. It's actually perfect. And I've hit the inflection point.
[01:00:31.000 --> 01:00:37.240]   So, you know, I may have what, 45,000 users on Twitter and only 2,000 on mastodon.
[01:00:37.240 --> 01:00:44.600]   But the engagement I'm starting to get from mastodon is getting very close to the engagement
[01:00:44.600 --> 01:00:49.320]   for the same topics on Twitter. So, that's good. That's good.
[01:00:51.160 --> 01:00:59.000]   To me, the lesson, and I also take this lesson from the collapse of FTX, is that centralization,
[01:00:59.000 --> 01:01:05.240]   Amy, you'd probably be good to talk about this. It's not always a good idea. Certainly,
[01:01:05.240 --> 01:01:14.280]   it's the kind of the way capitalism works because that's how you capture values by centralizing it,
[01:01:14.840 --> 01:01:22.360]   siloing it, and selling it. But for end users, I don't know if centralization is the right thing.
[01:01:22.360 --> 01:01:26.920]   Of course, that is the complaint. People who don't want to leave Twitter have as well. But I
[01:01:26.920 --> 01:01:35.320]   like all the people I can follow and talk to on Twitter. Yeah, I mean, listen, total decentralization.
[01:01:35.320 --> 01:01:42.600]   I don't know of any examples throughout history in any sector where total decentralization
[01:01:43.400 --> 01:01:49.320]   worked and worked in the long term. But maybe it's a pendulum that we've swung too far
[01:01:49.320 --> 01:01:55.800]   in the direction of economies of scale and centralization and mass. I mean, maybe if we're
[01:01:55.800 --> 01:02:03.880]   bridging to sort of the exchanges, the crypto exchanges, you know, listen, I don't think our central banks,
[01:02:03.880 --> 01:02:10.200]   I'm not sure what the Fed is. I don't know that I agree with what the Fed has been doing.
[01:02:12.120 --> 01:02:18.760]   But I also don't know that completely unfettered exchanges where you have a handful of people
[01:02:18.760 --> 01:02:24.360]   making the liquidity. I don't want to get super in the weeds on this, but we should not be in the
[01:02:24.360 --> 01:02:29.080]   place that we are right now. And this is no longer an experiment that's happening during COVID.
[01:02:29.080 --> 01:02:34.920]   There's real money being lost. That's a good point. Like significant real money for people.
[01:02:34.920 --> 01:02:40.040]   And again, if we play this forward, you put somebody in dire economic circumstances without
[01:02:40.920 --> 01:02:46.440]   in our current economy with the current and like with all of the other variables that are not
[01:02:46.440 --> 01:02:51.320]   playing to our favor. This this is where you have a vulnerable population that decides to,
[01:02:51.320 --> 01:02:59.160]   you know, that we've seen we've seen how this has happened throughout history. And it has
[01:02:59.160 --> 01:03:03.240]   typically not gone well. So that's what I'll say.
[01:03:04.840 --> 01:03:11.720]   It's interesting. I, you know, I maybe because I'm not a good capitalist. I'm not good at marketing.
[01:03:11.720 --> 01:03:17.720]   I'm not good at, you know, if I like the idea, for instance, on our mastodon, I don't really want
[01:03:17.720 --> 01:03:26.120]   to promote it. I just kind of like it better if it's it grows natively and it doesn't need to be
[01:03:26.120 --> 01:03:31.560]   big. I don't want to get big. Getting big sometimes is not a is I understand from a capitalist
[01:03:31.560 --> 01:03:36.200]   point of view from an industrial point of view. Getting big is a secret to economies of scale.
[01:03:36.200 --> 01:03:39.640]   It's where you really get the big money. I kind of like not getting big.
[01:03:39.640 --> 01:03:47.320]   Mastodon is more like having a cottage garden. Well, I'll tell you, I thought as efficient as
[01:03:47.320 --> 01:03:55.960]   having a giant, you know, 15 million acre beat farm, but it's kind of more personal and more
[01:03:55.960 --> 01:04:01.400]   satisfying. Sorry. It's all about niche and getting to your very specific audience, which
[01:04:01.400 --> 01:04:07.560]   is very much evidenced here. I mean, we we don't have enormous audiences, but
[01:04:07.560 --> 01:04:13.560]   I like the niche. Very desirable for our specific audience. I just like that.
[01:04:13.560 --> 01:04:17.640]   It's a big thing. I prefer to talk to people who get it than people who don't.
[01:04:17.640 --> 01:04:24.760]   That was just me. I don't know. Right. And so again, like, but that that is a better situation
[01:04:24.760 --> 01:04:30.200]   for potential advertiser, advertiser. The challenge with advertisers on platforms right now is you
[01:04:30.200 --> 01:04:36.920]   kind of don't know where your ads are going and you don't have clear metrics on anything like it.
[01:04:36.920 --> 01:04:41.800]   You know, if I had an opportunity to talk to 2,000 people about something that was aligned
[01:04:41.800 --> 01:04:45.720]   with whatever it is that I'm working on and I was going to do it in a way that they appreciated,
[01:04:45.720 --> 01:04:50.760]   that's the best possible advertising there is. Right. Doesn't even have to be that large. I mean,
[01:04:50.760 --> 01:04:55.480]   this is one of the things we talk about when people like start up doing podcasts and they're
[01:04:55.480 --> 01:04:59.960]   they get very depressed because they see download numbers of over 100 or 200 people.
[01:04:59.960 --> 01:05:06.280]   But if you just take a moment and say, what if I had 100 or 200 people just in a room listening
[01:05:06.280 --> 01:05:11.880]   to me, right? That's plenty. Yeah. That's really powerful. And if you just think about that.
[01:05:11.880 --> 01:05:17.000]   Yeah. And if it's the right 100 or 200 people, that's all you need. I mean, honestly, that is
[01:05:17.000 --> 01:05:22.840]   Twitter. Twitter's user base is pretty small relative to the global population. It just happened
[01:05:22.840 --> 01:05:28.600]   to be a concentration of media people and that's a good point. It's not Facebook, is it?
[01:05:28.600 --> 01:05:35.320]   But Twitter's user base had a advantage over some of the other social media platforms. And that is
[01:05:35.320 --> 01:05:42.760]   the user base tended to be a bit more educated and it tended to be a little wealthier. I think the
[01:05:42.760 --> 01:05:52.200]   last stat I saw was 87% make more than 30,000 a year, 40 some percent make more than 75,000 a year,
[01:05:52.200 --> 01:05:58.360]   and someone like 15% more make more than 100,000 a year. So if you're an advertiser and you're looking
[01:05:58.360 --> 01:06:05.240]   for your niche, that platform that has fewer users, but has more of the users that you want
[01:06:05.240 --> 01:06:11.320]   is a better buy. Do you think that's the move behind all of these sort of spaces, style,
[01:06:11.320 --> 01:06:17.560]   like small audio chat rooms that we're starting on? Oh, yeah. I mean, that's it's
[01:06:17.560 --> 01:06:24.600]   Patreon was the 1000 loyal listeners. That's all you really need. You get that and anything
[01:06:24.600 --> 01:06:29.080]   else is gravy. If you get the 1000 people who really want to hear what you have to say, who are
[01:06:29.080 --> 01:06:36.360]   willing to give you that dollar a week or dollar an episode. I will say this. If we were, we make,
[01:06:36.360 --> 01:06:44.280]   we do better being more niche to a smaller audience than if I was broader to a larger audience for
[01:06:44.280 --> 01:06:49.560]   sure. Of course. Do you want an advertiser who needs to sell a million items to make a million
[01:06:49.560 --> 01:06:54.120]   dollars or an advertiser who makes a million dollars off one item? Well, what the
[01:06:54.120 --> 01:06:57.320]   net's all have access to big budgets. That's why it sponsors exactly.
[01:06:57.320 --> 01:07:05.000]   Exactly. What the internet's all about is diversity, small groups, maybe it was a mistake to say,
[01:07:05.560 --> 01:07:16.040]   to make social get so big. Maybe that was a mistake or no. I think it was driven by the
[01:07:16.040 --> 01:07:20.760]   desire to reach a mass audience so you could sell it to advertisers as opposed to what people
[01:07:20.760 --> 01:07:25.960]   really needed or wanted. The general sort of this goes back to Jack Welsh. And if anybody's
[01:07:25.960 --> 01:07:31.640]   super interested in sort of business and how we got here, it's a critique of Welsh, but
[01:07:32.360 --> 01:07:38.520]   it's the man who broke capitalism. He was lion eyes at the time. And now David Gels.
[01:07:38.520 --> 01:07:46.360]   And still very much is in especially like in Latin, South America, but scale is everything.
[01:07:46.360 --> 01:07:52.520]   It has like our modern business in the US, especially has been built on scale and
[01:07:52.520 --> 01:07:57.240]   financialization, meaning you want to get to the biggest possible numbers that you can.
[01:07:57.240 --> 01:08:03.160]   And then the rest of the, you want to achieve non-organic growth is kind of how we got to now.
[01:08:03.160 --> 01:08:07.880]   And I don't think that's the best way to do things. It just became the way that we do things.
[01:08:07.880 --> 01:08:13.240]   Leo, I will tell you something that Jim Lauterback, some we both know, told me when he was running
[01:08:13.240 --> 01:08:21.560]   revision three, I asked him, what percentage of your producer's time is spent engaging directly
[01:08:21.560 --> 01:08:26.680]   with the audience? And he said 50%. Yeah. And that was critical to building his audience.
[01:08:27.240 --> 01:08:33.640]   And I would say we spend kind of close to that too. You've got to spend a lot of time
[01:08:33.640 --> 01:08:39.720]   engaging with your audience directly. Jack Lynch, a bill scale gives you one very important thing.
[01:08:39.720 --> 01:08:48.680]   The larger social media platform is, the easier it is to drive interactions without rage.
[01:08:50.040 --> 01:08:57.960]   At scale, the rage machine is amazing. You can get so many engagements, you can get so much spread
[01:08:57.960 --> 01:09:02.120]   by bringing up something that makes people upset. And then you can spend out your rage.
[01:09:02.120 --> 01:09:08.440]   Oh, this is parlour whole theory. The internet is a speed from bad to worse. It's like,
[01:09:08.440 --> 01:09:14.840]   how can I say the worst thing possible, the quickest, faster than anybody else?
[01:09:14.840 --> 01:09:20.840]   We've seen this with how people sort of do this pile on of like, I hate this person. I want to
[01:09:20.840 --> 01:09:25.080]   hurt this person. I want to kill this person. I'm like, boom, boom, boom, that fast.
[01:09:25.080 --> 01:09:29.480]   But this again, so like, but we didn't nobody invented this recently. This goes back to
[01:09:29.480 --> 01:09:34.840]   which is allowing for speed. It's just faster. This was a practice like 500 years ago. You go to
[01:09:34.840 --> 01:09:39.480]   see a Shakespeare show live and that happens in the audience, you know, but hold it. But the
[01:09:39.480 --> 01:09:46.440]   anonymity of Twitter is allows for the great number of connections. If I go into a massive
[01:09:46.440 --> 01:09:51.640]   on instance and I just start spewing out rage, I am going to get blocked so quickly, because people
[01:09:51.640 --> 01:09:55.960]   are going to say, I don't want to hear this. This is nothing to do with what we're discussing.
[01:09:55.960 --> 01:09:59.880]   I do that on Twitter. I'm still going to get people blocking me, but I will find a big
[01:09:59.880 --> 01:10:05.800]   enough critical mass of people who share the same rage and will amplify it. So that happens on
[01:10:05.800 --> 01:10:10.520]   large instances that doesn't happen on small ones. You know what Bill Maher had a great line,
[01:10:10.520 --> 01:10:17.160]   I could say to everybody on Twitter, you know, have a great day and people would still be pissed
[01:10:17.160 --> 01:10:22.440]   off about that. Because it's Bill Maher. Yeah. Nobody. Right. Right.
[01:10:22.440 --> 01:10:31.000]   So, but are we learning a lesson that maybe too big as you can get too big that it's, I mean,
[01:10:31.000 --> 01:10:36.200]   certainly Jack Welsh made, did he make GE too big? Is that what they're saying?
[01:10:36.200 --> 01:10:42.600]   It was fast, inorganic growth and a lot of, so organic growth would be developing new products
[01:10:42.600 --> 01:10:47.240]   and services and growing that market over time. Inorganic growth tends to happen through M&A.
[01:10:47.240 --> 01:10:53.080]   So you're buying up competitors or other companies and you're trying to sort of spin up your P&L
[01:10:53.880 --> 01:11:02.440]   to appease the market. And, you know, that's a risky, dangerous game, but typically that happens
[01:11:02.440 --> 01:11:08.200]   when you're also cutting way back on staff, on services, on people. There's a terrific,
[01:11:08.200 --> 01:11:16.440]   a terrific chapter of that book on GE, on Boeing and how Boeing got sort of gutted and the history
[01:11:16.440 --> 01:11:25.480]   of really like great R&D and Boeing, you know, was sort of blown apart. So, I think we have this
[01:11:25.480 --> 01:11:31.560]   tradition in the US of scale up, scale up, do less, you know, get rid of as many people as you can,
[01:11:31.560 --> 01:11:39.400]   divest in your talent pipeline. You know, there's a lot of that happening. And that's also the
[01:11:39.400 --> 01:11:47.320]   sort of playbook for a lot of companies that get consulting from a particularly large consulting
[01:11:47.320 --> 01:11:51.640]   company based in the United States. I won't say who it is, but that's kind of their playbook.
[01:11:51.640 --> 01:11:56.840]   And all of this is about driving up share prices as much as humanly possible. But, and then that's
[01:11:56.840 --> 01:12:02.200]   a great short-term game gain and it's good for the market in the very near term, but that is,
[01:12:02.200 --> 01:12:07.800]   it produces just disastrous results down the road. Now, I don't know if Elon himself has
[01:12:08.840 --> 01:12:15.000]   been a recipient of any of this advice or if any of the tech companies have been recently,
[01:12:15.000 --> 01:12:17.400]   but they seem to be following at least in parts of those models.
[01:12:17.400 --> 01:12:26.920]   Yeah. What about, Matt Meta, what about Facebook? 54% of the 11,000 laid off were in business positions,
[01:12:26.920 --> 01:12:35.800]   but the other 46% were technology roles. That's a lot of technologists to lay off.
[01:12:38.120 --> 01:12:42.520]   If he, let me ask you. They've decided to kill, they've decided to kill Portal.
[01:12:42.520 --> 01:12:48.920]   They're smart display division. They've decided they had smart watches in development. They decided
[01:12:48.920 --> 01:12:54.680]   to kill that. I'm not saying Facebook or Meta is a very well-run company because a lot of what's
[01:12:54.680 --> 01:13:02.520]   going on right now is Mark pivoting to a very speculative investment into a virtual reality.
[01:13:03.800 --> 01:13:10.440]   So I used to think he was a real smart guy. What happened? Well, he didn't all of a sudden become
[01:13:10.440 --> 01:13:18.200]   stupid. But I have a question. I have a question and I don't know if you've been tracking this
[01:13:18.200 --> 01:13:25.400]   specifically, but he seems to be betting Zuckerberg betting a lot on virtual reality.
[01:13:27.080 --> 01:13:32.440]   What is your understanding of how big that market's getting and growing?
[01:13:32.440 --> 01:13:41.720]   So we do a lot of people who listen to this show kind of know the line of work that I'm in and who
[01:13:41.720 --> 01:13:50.280]   we work with. The issue is that we're, I think, 10 years out, probably. We don't have enough
[01:13:50.280 --> 01:13:56.040]   compute. We don't have enough. We're a little far away from this really being able to work and
[01:13:56.040 --> 01:14:03.320]   have business cases. I wonder if-- Do you know what the demand is specifically for it?
[01:14:03.320 --> 01:14:09.720]   Oh, the demand? Zero. I don't see any, but there's none of the-- I mean, then I know of,
[01:14:09.720 --> 01:14:15.720]   there aren't any with the exception of maybe DoD, but there's no real investment. There's a lot
[01:14:15.720 --> 01:14:23.080]   like marketing departments are very excited about, meta-versal technologies and XR. There are a lot
[01:14:23.080 --> 01:14:31.640]   of anthropomorphic chatbot type of things that are being built. To me, this feels a lot like the
[01:14:31.640 --> 01:14:36.040]   days of four-square when everybody threw money at badges because they thought gamification was the
[01:14:36.040 --> 01:14:41.240]   next big thing and they followed the wrong trend. They followed the shiny, not the infrastructure.
[01:14:41.240 --> 01:14:43.480]   Always follow the infrastructure. That's what matters most.
[01:14:43.480 --> 01:14:52.440]   So do you think that what Zuckerberg is doing right now in terms of-- and I don't know if he is
[01:14:52.440 --> 01:14:57.000]   betting the farm, but it just seems that's all the press that comes out is that everything is
[01:14:57.000 --> 01:15:03.160]   more and more virtual reality driving everyone to the whole world have an Oculus on their head.
[01:15:03.160 --> 01:15:08.360]   Well, I was not in the room where it happened. So I don't know the conversations that are happening,
[01:15:08.360 --> 01:15:15.560]   but as an outsider, I see two plausible reasons. There's probably others. But one is, I think at
[01:15:15.560 --> 01:15:20.040]   some point, if you have consolidated enough wealth and power, I think you start to think,
[01:15:20.040 --> 01:15:26.120]   hey, I'm going to be the guy that does blah, like X, right? And through sheer force of my will,
[01:15:26.120 --> 01:15:31.560]   it shall be. So I don't know if it's that or if it's-- Facebook was facing some significant
[01:15:31.560 --> 01:15:36.520]   and I think warranted regulatory challenges and maybe this was a pivot away from that.
[01:15:36.520 --> 01:15:40.040]   I'm sure there's other plausible reasons, but those would be the two from my vantage point.
[01:15:40.040 --> 01:15:47.080]   I mean, I own an Oculus and I kind of like it. And actually, I love it for exercising, but
[01:15:48.120 --> 01:15:52.440]   I don't find the compelling need for it.
[01:15:52.440 --> 01:15:59.800]   If I have a VR headset on my head for more than three minutes, I will get violently ill.
[01:15:59.800 --> 01:16:03.800]   Does it matter how fast-- If you put Oculus on your head. Yes.
[01:16:03.800 --> 01:16:09.560]   And the prototype versions with these super high refresh screens, doesn't matter. It's just
[01:16:09.560 --> 01:16:16.280]   I'm one of these people who I will never be-- I can't even watch somebody play a first-person
[01:16:16.280 --> 01:16:21.000]   shooter game. That makes me sick. But I don't get sick from the Oculus. Actually,
[01:16:21.000 --> 01:16:24.680]   I'm quite impressed with how those are, I guess, balanced, if you will.
[01:16:24.680 --> 01:16:31.480]   I mean, the last Oculus Connect that I went to, I had a brand new headset with a really high refresh
[01:16:31.480 --> 01:16:37.000]   game. And they promised me to say, oh, no, you'll be fine wearing this. 30 seconds in, I had to pull
[01:16:37.000 --> 01:16:42.440]   it off my-- I was sweaty. I was dizzy. I had to go sit down for about an hour. So there's something
[01:16:42.440 --> 01:16:47.560]   about my brain that just will not let it go with VR. It just knows it's not right.
[01:16:47.560 --> 01:16:51.320]   Pardon, do you remember the last little contacts? Do you wear a corrective nothing?
[01:16:51.320 --> 01:16:58.920]   I'm 20/20. Okay. Well, man-- I mean, look, even if it's only, let's say, 5 or 10% of the population,
[01:16:58.920 --> 01:17:04.120]   if you have a technology that 5 or 10% of the population gets nauseated by, you're not going to
[01:17:04.120 --> 01:17:10.760]   be a mass product. It's just not. I got the new $1,600 or $1,500 Quest Pro.
[01:17:11.560 --> 01:17:16.600]   And-- [LAUGHTER]
[01:17:16.600 --> 01:17:21.400]   Have any of you played with the HoloLens 2? No. So I--
[01:17:21.400 --> 01:17:22.040]   Not yet.
[01:17:22.040 --> 01:17:26.680]   Everything that I've tried. And I was an early and still firm believer in Magic Leap. I think
[01:17:26.680 --> 01:17:33.400]   the technology was extraordinary. There was so much money. And they just-- investors demanded
[01:17:33.400 --> 01:17:41.080]   return ROI, which I thought was needed years to develop. But HoloLens 2 is a pretty amazing
[01:17:41.480 --> 01:17:43.720]   piece of equipment. And I can wear it with glasses on.
[01:17:43.720 --> 01:17:47.400]   Yeah. I can wear the-- I can wear the VR.
[01:17:47.400 --> 01:17:47.880]   I can wear the VR.
[01:17:47.880 --> 01:17:48.840]   VR than VR.
[01:17:48.840 --> 01:17:49.240]   Yeah.
[01:17:49.240 --> 01:17:49.800]   I have one--
[01:17:49.800 --> 01:17:54.760]   Well, that's more XR, right? And I really do think the next thing that's-- I don't see a world in
[01:17:54.760 --> 01:18:03.240]   which VR becomes a mass market technology outside of entertainment. I just-- AR-- like a heads-up
[01:18:03.240 --> 01:18:07.880]   display for life, right? For your everyday life, wearing a pair of glasses that look very similar
[01:18:07.880 --> 01:18:12.200]   to the sort of boring looking glasses that I've got on right now, I think that's our--
[01:18:12.200 --> 01:18:18.360]   I will say no. Here's the killer app for that. And we've talked about this a long time ago is
[01:18:18.360 --> 01:18:21.080]   dating. Imagine you go into a bar.
[01:18:21.080 --> 01:18:21.080]   Dating.
[01:18:21.080 --> 01:18:27.400]   And you can scan the room in that people can be identified and you can go into their social
[01:18:27.400 --> 01:18:34.120]   profiles and see things about them and essentially know a lot about them before you actually approach
[01:18:34.120 --> 01:18:34.280]   them.
[01:18:34.280 --> 01:18:37.320]   That sounds like a stalkers wet dream.
[01:18:37.320 --> 01:18:40.760]   I don't know. I know a lot of people who would just see that.
[01:18:40.760 --> 01:18:42.040]   Oh, it's a stalked open. It's just--
[01:18:42.040 --> 01:18:42.520]   Think about--
[01:18:42.520 --> 01:18:43.960]   You will have an opening line.
[01:18:43.960 --> 01:18:47.080]   That's terrible.
[01:18:47.080 --> 01:18:47.640]   Do you think--
[01:18:47.640 --> 01:18:50.600]   That would-- you know, but let me take that a step further because here's how--
[01:18:50.600 --> 01:18:57.000]   then I would sort of set myself up to be pixelated and so I just literally unblocked from your view.
[01:18:57.000 --> 01:18:58.520]   Not your personal view, David.
[01:18:58.520 --> 01:18:59.880]   [LAUGHTER]
[01:18:59.880 --> 01:19:00.840]   No, but I understand--
[01:19:00.840 --> 01:19:02.200]   But how do you pixelate--
[01:19:02.200 --> 01:19:02.520]   It's like--
[01:19:02.520 --> 01:19:03.240]   No, how do you do it?
[01:19:03.240 --> 01:19:04.360]   Oh, totally.
[01:19:04.360 --> 01:19:09.400]   There's a couple of guys who have knitted a sweater that I want to buy that completely
[01:19:09.400 --> 01:19:11.720]   blocks your face for most recognition algorithms.
[01:19:11.720 --> 01:19:12.280]   I think they're--
[01:19:12.280 --> 01:19:12.760]   Really?
[01:19:12.760 --> 01:19:14.040]   Georgia Tech, I think.
[01:19:14.040 --> 01:19:14.280]   Would--
[01:19:14.280 --> 01:19:19.160]   No, would that hold it-- but would that mess up all like, you know, street cameras as well?
[01:19:19.160 --> 01:19:19.640]   Totally.
[01:19:19.640 --> 01:19:19.960]   Yes.
[01:19:19.960 --> 01:19:21.160]   I'm literally going to ask--
[01:19:21.160 --> 01:19:23.160]   No, you're going to have to be wearing it over your head.
[01:19:23.160 --> 01:19:25.160]   Criminals could get away with a lot.
[01:19:25.160 --> 01:19:26.120]   Probably.
[01:19:26.120 --> 01:19:28.360]   So that's just near IR, right?
[01:19:28.360 --> 01:19:29.560]   Just a lot of LEDs.
[01:19:29.560 --> 01:19:31.000]   Oh, it's got lights.
[01:19:31.000 --> 01:19:31.560]   Yeah.
[01:19:31.560 --> 01:19:32.120]   Oh, no, no, no.
[01:19:32.120 --> 01:19:32.840]   It doesn't have lights.
[01:19:32.840 --> 01:19:35.640]   This is a-- there's something about the pattern.
[01:19:35.640 --> 01:19:38.040]   I'm sort of-- I don't remember.
[01:19:38.040 --> 01:19:38.520]   Is that--
[01:19:38.520 --> 01:19:42.120]   You know, it was-- I think it was World War I, where they--
[01:19:42.120 --> 01:19:42.520]   Yeah.
[01:19:42.520 --> 01:19:46.680]   What was the name of the way they painted the ships black and white, these bizarre patterns?
[01:19:46.680 --> 01:19:47.880]   Oh, Thomas, like the radar?
[01:19:47.880 --> 01:19:48.440]   Yeah.
[01:19:48.440 --> 01:19:48.440]   Yeah.
[01:19:48.440 --> 01:19:49.240]   Well, not just the radar.
[01:19:49.240 --> 01:19:49.720]   Not just the radar.
[01:19:49.720 --> 01:19:54.440]   Not just the scope to be able to know how close or far away the ships are.
[01:19:54.440 --> 01:19:56.840]   Oh, somebody in the chat room is going to tell me what this is.
[01:19:56.840 --> 01:19:59.000]   Yeah, I can't remember the name we've talked about it before.
[01:19:59.000 --> 01:19:59.640]   It's very cool.
[01:19:59.640 --> 01:20:01.000]   It looks like Zebra's.
[01:20:01.000 --> 01:20:01.400]   It's weird.
[01:20:01.400 --> 01:20:01.960]   Yeah, yeah, yeah.
[01:20:01.960 --> 01:20:02.920]   It's all broken up.
[01:20:02.920 --> 01:20:04.040]   Here's the sweater.
[01:20:04.040 --> 01:20:08.920]   Render yourself invisible to AI with this adversarial sweater of doom.
[01:20:08.920 --> 01:20:10.040]   Yeah.
[01:20:10.040 --> 01:20:11.880]   I actually think I heard about it from you guys.
[01:20:11.880 --> 01:20:13.000]   I--
[01:20:13.000 --> 01:20:18.840]   Yeah, I mean, I don't-- I'd like to believe it.
[01:20:18.840 --> 01:20:19.640]   Do you want to wear that sweater?
[01:20:19.640 --> 01:20:23.080]   I will tell you, I literally own only black clothing.
[01:20:23.080 --> 01:20:25.400]   I don't own any color that I'm aware of.
[01:20:25.400 --> 01:20:27.320]   And that, I would wear.
[01:20:27.320 --> 01:20:27.800]   I would wear that.
[01:20:27.800 --> 01:20:30.360]   I mean, I'm with you on that black clothing forever.
[01:20:31.320 --> 01:20:32.200]   Well done.
[01:20:32.200 --> 01:20:33.080]   Very nice.
[01:20:33.080 --> 01:20:34.520]   He's got weight there.
[01:20:34.520 --> 01:20:35.720]   You're kind of stuck with it, dude.
[01:20:35.720 --> 01:20:38.680]   That part of a Johnny Cash revival thing, right?
[01:20:38.680 --> 01:20:40.600]   So did Mark--
[01:20:40.600 --> 01:20:44.520]   Is Mark making a mistake pouring tens of billions of dollars a year
[01:20:44.520 --> 01:20:50.120]   into whatever it is VR mixed reality AR?
[01:20:50.120 --> 01:20:52.040]   Is that a mistake?
[01:20:52.040 --> 01:20:53.000]   I think the market would say yes.
[01:20:53.000 --> 01:20:54.360]   Yeah.
[01:20:54.360 --> 01:20:55.160]   The market seems to--
[01:20:55.160 --> 01:20:56.520]   Plus half their value, yeah.
[01:20:56.520 --> 01:21:00.200]   I mean, I know it's difficult.
[01:21:00.200 --> 01:21:01.880]   I know he's developing a new field
[01:21:01.880 --> 01:21:04.840]   and he's trying to drive interest where it doesn't exist.
[01:21:04.840 --> 01:21:08.600]   But the second left comparison is still dead on.
[01:21:08.600 --> 01:21:11.320]   You look at this and say, "It's a little bit better,
[01:21:11.320 --> 01:21:13.400]   but everything still looks like a cartoon."
[01:21:13.400 --> 01:21:18.840]   And I don't see myself working a day wearing VR goggles
[01:21:18.840 --> 01:21:21.080]   so that I can be on a space station when I'm working.
[01:21:21.080 --> 01:21:22.840]   What value does that give me?
[01:21:22.840 --> 01:21:23.160]   Yeah.
[01:21:23.160 --> 01:21:23.960]   So--
[01:21:23.960 --> 01:21:25.480]   I will throw this out.
[01:21:25.480 --> 01:21:28.840]   Somebody had posted that they actually did coding
[01:21:29.400 --> 01:21:32.520]   with VR goggles on and they showed their screen.
[01:21:32.520 --> 01:21:32.920]   Oh, no.
[01:21:32.920 --> 01:21:34.280]   Oh my god, it was endless.
[01:21:34.280 --> 01:21:34.760]   Awful.
[01:21:34.760 --> 01:21:35.720]   Awful.
[01:21:35.720 --> 01:21:36.680]   No.
[01:21:36.680 --> 01:21:38.680]   But then I like E-max.
[01:21:38.680 --> 01:21:41.160]   So I don't think about VRE Max yet.
[01:21:41.160 --> 01:21:45.240]   Here is on Etsy, the anti-facial recognition hoodie,
[01:21:45.240 --> 01:21:49.480]   $64 for tech data privacy protesters.
[01:21:49.480 --> 01:21:51.400]   Comes in yellow or purple.
[01:21:51.400 --> 01:21:53.320]   What about that?
[01:21:53.320 --> 01:21:54.280]   You'd wear that, yeah?
[01:21:54.280 --> 01:21:55.720]   I don't know if it works.
[01:21:55.720 --> 01:21:56.200]   Does it work?
[01:21:56.200 --> 01:21:56.600]   Well--
[01:21:56.600 --> 01:21:58.600]   I guess there's little faces so it would be confusing.
[01:21:58.600 --> 01:21:59.800]   That makes sense to me.
[01:21:59.800 --> 01:22:01.160]   I want to know how these are actually--
[01:22:01.160 --> 01:22:02.120]   I would wear them.
[01:22:02.120 --> 01:22:03.240]   --draming straightening these ones.
[01:22:03.240 --> 01:22:04.760]   This isn't the sweater of doom.
[01:22:04.760 --> 01:22:06.680]   This is the sweatpants of doom.
[01:22:06.680 --> 01:22:08.200]   I'm all about those pants.
[01:22:08.200 --> 01:22:09.720]   I'm wearing that the next time I go.
[01:22:09.720 --> 01:22:11.080]   This is the Holy Father.
[01:22:11.080 --> 01:22:13.720]   It's facial recognition.
[01:22:13.720 --> 01:22:15.400]   Why are you a--
[01:22:15.400 --> 01:22:18.200]   son, why are you wearing those pants?
[01:22:18.200 --> 01:22:22.120]   Actually, that's not that different from what the Swiss guard wear, right?
[01:22:22.120 --> 01:22:22.520]   Yeah.
[01:22:22.520 --> 01:22:23.000]   Yeah.
[01:22:23.000 --> 01:22:23.560]   That's--
[01:22:23.560 --> 01:22:25.400]   Do you think he's in the Swiss guard?
[01:22:25.400 --> 01:22:28.040]   Oh, look, you can also get a neck gator.
[01:22:29.000 --> 01:22:29.800]   Neck gator.
[01:22:29.800 --> 01:22:31.640]   That looks more like Minecraft.
[01:22:31.640 --> 01:22:33.720]   Here's a verified review from Victoria.
[01:22:33.720 --> 01:22:34.760]   Good quality.
[01:22:34.760 --> 01:22:37.160]   My robot Sextal did not recognize me.
[01:22:37.160 --> 01:22:41.000]   Let's take a little time out after that.
[01:22:41.000 --> 01:22:42.040]   I've got to adjust.
[01:22:42.040 --> 01:22:42.920]   I've got to absorb.
[01:22:42.920 --> 01:22:47.720]   I wish our sponsor were the sweater of doom.
[01:22:47.720 --> 01:22:52.040]   By the way, there is an entire academic paper
[01:22:52.040 --> 01:22:53.800]   making an invisibility cloak real-world
[01:22:53.800 --> 01:22:55.800]   out of a serial text on object detectors.
[01:22:56.840 --> 01:22:58.520]   PDF that you can read.
[01:22:58.520 --> 01:23:00.760]   And I will hope somebody will read this and tell me
[01:23:00.760 --> 01:23:02.280]   if it works.
[01:23:02.280 --> 01:23:04.520]   There's-- there's-- there's math.
[01:23:04.520 --> 01:23:06.680]   That's always good, right?
[01:23:06.680 --> 01:23:07.320]   When there's math.
[01:23:07.320 --> 01:23:12.120]   Well, it must work.
[01:23:12.120 --> 01:23:12.680]   Look at that.
[01:23:12.680 --> 01:23:16.440]   Our show today brought to you by these babies,
[01:23:16.440 --> 01:23:17.640]   these little computers,
[01:23:17.640 --> 01:23:19.000]   these things are so cool.
[01:23:19.000 --> 01:23:22.360]   This is on Logic,
[01:23:22.360 --> 01:23:23.480]   is the name of the company.
[01:23:23.480 --> 01:23:24.280]   And they make
[01:23:25.400 --> 01:23:26.600]   the greatest little
[01:23:26.600 --> 01:23:29.480]   hardened computers
[01:23:29.480 --> 01:23:31.640]   for industrial and other
[01:23:31.640 --> 01:23:33.880]   challenging environments.
[01:23:33.880 --> 01:23:35.720]   This is one of the--
[01:23:35.720 --> 01:23:38.120]   I think the trends that's changed the world is
[01:23:38.120 --> 01:23:40.760]   computing is moved from a thing on your desktop
[01:23:40.760 --> 01:23:43.080]   to computers everywhere.
[01:23:43.080 --> 01:23:44.920]   And that's where these on Logic computers
[01:23:44.920 --> 01:23:48.520]   are showing up everywhere.
[01:23:48.520 --> 01:23:51.560]   The world of hidden computing
[01:23:51.560 --> 01:23:54.760]   that's revolutionizing every area
[01:23:55.640 --> 01:23:58.120]   of sustainable agriculture,
[01:23:58.120 --> 01:23:59.080]   smart cities.
[01:23:59.080 --> 01:24:01.880]   You see them on the factory floor,
[01:24:01.880 --> 01:24:03.480]   increasing manufacturing,
[01:24:03.480 --> 01:24:05.960]   efficiency, operating robots,
[01:24:05.960 --> 01:24:08.200]   improving the quality of our lives.
[01:24:08.200 --> 01:24:11.400]   That's where you'll find these on Logic distinctive orange,
[01:24:11.400 --> 01:24:14.040]   industrial and embedded computers.
[01:24:14.040 --> 01:24:17.000]   On Logic is the first choice in industrial computing
[01:24:17.000 --> 01:24:18.920]   for innovators around the world.
[01:24:18.920 --> 01:24:20.680]   We're looking for computing power
[01:24:20.680 --> 01:24:23.880]   that can survive and thrive.
[01:24:23.880 --> 01:24:27.000]   We're traditional hardware is just going to choke and die.
[01:24:27.000 --> 01:24:29.320]   On Logic designs and creates computer solutions
[01:24:29.320 --> 01:24:31.080]   that can fit in the palm of your hand
[01:24:31.080 --> 01:24:34.840]   while powering everything from advanced robotics
[01:24:34.840 --> 01:24:36.920]   and AI to manufacturing automation,
[01:24:36.920 --> 01:24:38.520]   digital media solutions,
[01:24:38.520 --> 01:24:40.920]   smart agriculture technologies.
[01:24:40.920 --> 01:24:43.000]   These are engineered for reliability.
[01:24:43.000 --> 01:24:44.520]   For instance, this baby--
[01:24:44.520 --> 01:24:46.440]   and you could tell because of the fins--
[01:24:46.440 --> 01:24:48.520]   these guys are passively cooled.
[01:24:48.520 --> 01:24:50.920]   So they don't have any fans that eliminates
[01:24:50.920 --> 01:24:52.920]   any moving parts.
[01:24:52.920 --> 01:24:54.120]   There are no moving parts at all,
[01:24:54.120 --> 01:24:56.760]   which makes this a solid state device perfect
[01:24:56.760 --> 01:24:59.720]   for an environment where there's lots of vibration or shock.
[01:24:59.720 --> 01:25:02.040]   You can also get them sealed
[01:25:02.040 --> 01:25:05.160]   so they're completely protected from dust
[01:25:05.160 --> 01:25:07.640]   and other airborne contaminants,
[01:25:07.640 --> 01:25:09.080]   other design features.
[01:25:09.080 --> 01:25:11.640]   They're all different ones for different uses, obviously.
[01:25:11.640 --> 01:25:13.640]   Other design features and specialized components
[01:25:13.640 --> 01:25:17.720]   protect systems from extreme temperatures or interference.
[01:25:17.720 --> 01:25:19.960]   This is such a cool idea.
[01:25:19.960 --> 01:25:21.640]   If you're looking for a computer
[01:25:21.640 --> 01:25:24.600]   that can live in the most challenging environments,
[01:25:24.600 --> 01:25:27.080]   check out on Logic.
[01:25:27.080 --> 01:25:29.480]   On Logic systems are extensively tested
[01:25:29.480 --> 01:25:31.880]   to operate reliably wherever they're needed,
[01:25:31.880 --> 01:25:34.440]   whether monitoring a remote mining operation.
[01:25:34.440 --> 01:25:35.400]   You might even see one.
[01:25:35.400 --> 01:25:36.120]   I hope you don't.
[01:25:36.120 --> 01:25:39.080]   On a crash card at a hospital,
[01:25:39.080 --> 01:25:42.440]   the team at On Logic truly cares about
[01:25:42.440 --> 01:25:44.200]   creating right-fit technologies
[01:25:44.200 --> 01:25:47.160]   tailored specifically to solve your challenges.
[01:25:47.160 --> 01:25:49.480]   So get in touch, tell them what you need.
[01:25:49.480 --> 01:25:51.640]   They partner with great companies.
[01:25:51.640 --> 01:25:53.800]   For instance, they have an AWS IoT
[01:25:53.800 --> 01:25:55.720]   greengrass compatible computers.
[01:25:55.720 --> 01:25:57.320]   Vented by AWS, so you know,
[01:25:57.320 --> 01:25:58.840]   you get the piece of mine.
[01:25:58.840 --> 01:26:00.440]   It's going to work right out of the box.
[01:26:00.440 --> 01:26:03.320]   So if you're doing a greengrass solution, this is awesome.
[01:26:03.320 --> 01:26:06.200]   They work with these companies
[01:26:06.200 --> 01:26:07.880]   to enable rapid evaluation,
[01:26:07.880 --> 01:26:09.880]   deployment of edge computing solutions
[01:26:09.880 --> 01:26:13.400]   based on technology from companies like AWS.
[01:26:13.400 --> 01:26:14.920]   If you need a computing solution
[01:26:14.920 --> 01:26:17.560]   that can easily be configured
[01:26:17.560 --> 01:26:18.680]   to your particular needs,
[01:26:18.680 --> 01:26:20.600]   supported by industry experts,
[01:26:20.600 --> 01:26:22.200]   they're just a phone call, website,
[01:26:22.200 --> 01:26:23.400]   chat or email away.
[01:26:23.400 --> 01:26:25.720]   Tell them about your difficult environment.
[01:26:25.720 --> 01:26:27.480]   And by the way,
[01:26:27.480 --> 01:26:29.880]   they're delivering these quickly.
[01:26:29.880 --> 01:26:31.160]   You don't have to wait.
[01:26:31.160 --> 01:26:32.600]   The team at On Logic is here,
[01:26:32.600 --> 01:26:34.200]   ready to design, build,
[01:26:34.200 --> 01:26:35.960]   and get you the hardware you need to get
[01:26:35.960 --> 01:26:36.280]   started.
[01:26:36.280 --> 01:26:37.400]   Learn more about OnLogics,
[01:26:37.400 --> 01:26:39.960]   30-day risk-free hardware trial
[01:26:39.960 --> 01:26:41.720]   on logic.com/twit.
[01:26:41.720 --> 01:26:43.960]   Maybe you've never heard the name,
[01:26:43.960 --> 01:26:44.920]   but now you know.
[01:26:44.920 --> 01:26:46.120]   And I'll tell you,
[01:26:46.120 --> 01:26:47.240]   if you ask somebody who's working
[01:26:47.240 --> 01:26:48.280]   in these kinds of environments,
[01:26:48.280 --> 01:26:51.400]   they know OnLogic.com/twit.
[01:26:51.400 --> 01:26:53.320]   We thank them so much for supporting
[01:26:53.320 --> 01:26:53.640]   Twit.
[01:26:53.640 --> 01:26:54.600]   They're not getting these back.
[01:26:54.600 --> 01:26:58.440]   We could think of all sorts of things.
[01:26:58.440 --> 01:27:00.840]   This one's got, I don't know why,
[01:27:00.840 --> 01:27:04.120]   it's got one, two, three, four, five, six,
[01:27:04.120 --> 01:27:06.200]   Wi-Fi buttons on it.
[01:27:06.200 --> 01:27:08.360]   I don't know what that's for.
[01:27:08.360 --> 01:27:11.240]   And then all these LEDs,
[01:27:11.240 --> 01:27:15.800]   and it's got three display ports,
[01:27:16.120 --> 01:27:19.240]   it's got like two 10 gigabit ethernet ports,
[01:27:19.240 --> 01:27:23.000]   it's got breakouts for a bus,
[01:27:23.000 --> 01:27:25.000]   so you can access the data bus directly.
[01:27:25.000 --> 01:27:26.440]   Very cool.
[01:27:26.440 --> 01:27:28.760]   Very cool.
[01:27:28.760 --> 01:27:30.440]   OnLogic.com/twit.
[01:27:30.440 --> 01:27:33.240]   Thank you OnLogic for supporting the show.
[01:27:33.240 --> 01:27:36.040]   All right, we're going to talk about FTX
[01:27:36.040 --> 01:27:37.560]   because this is another drama.
[01:27:37.560 --> 01:27:39.720]   Oh, wait a minute, before I do though,
[01:27:39.720 --> 01:27:42.360]   this just in on Twitter,
[01:27:42.360 --> 01:27:45.720]   Sean Penn has just given his Oscar
[01:27:46.680 --> 01:27:47.480]   to the Ukraine.
[01:27:47.480 --> 01:27:50.280]   There's President Zelensky.
[01:27:50.280 --> 01:27:53.320]   I don't know if they're very happy about that.
[01:27:53.320 --> 01:27:53.960]   I don't know what...
[01:27:53.960 --> 01:27:58.280]   I don't know what they're going to do with his Oscar,
[01:27:58.280 --> 01:27:59.480]   but they've got it now.
[01:27:59.480 --> 01:28:03.880]   Don't you love it how Hollywood's really stepping up?
[01:28:03.880 --> 01:28:09.080]   Do you think President Zelensky would like Miami?
[01:28:09.080 --> 01:28:14.840]   Did you see the pictures of Banksies, though,
[01:28:14.840 --> 01:28:15.960]   putting all over Ukraine?
[01:28:15.960 --> 01:28:17.400]   That's cool, right?
[01:28:17.400 --> 01:28:21.080]   He's gone to Ukraine, he's putting up Banksy paintings all over.
[01:28:21.080 --> 01:28:24.760]   Anyway, I don't know, I had to bring this.
[01:28:24.760 --> 01:28:26.440]   See, there's good stuff on Twitter still.
[01:28:26.440 --> 01:28:29.320]   I would never have known about that.
[01:28:29.320 --> 01:28:30.440]   F.
[01:28:30.440 --> 01:28:32.360]   There is more than one person on Twitter.
[01:28:32.360 --> 01:28:35.640]   There is.
[01:28:35.640 --> 01:28:37.560]   We don't know how many there are.
[01:28:37.560 --> 01:28:39.720]   It's not only one must.
[01:28:39.720 --> 01:28:41.320]   We may all be talking about one must.
[01:28:41.320 --> 01:28:42.040]   In particular.
[01:28:42.040 --> 01:28:43.560]   Yeah, no, I know who you're talking about.
[01:28:43.560 --> 01:28:44.440]   There's a few others.
[01:28:44.440 --> 01:28:47.640]   Can we forget about crypto now?
[01:28:47.640 --> 01:28:48.120]   Is it over?
[01:28:48.120 --> 01:28:49.560]   Can we just move on?
[01:28:49.560 --> 01:28:51.960]   Sam...
[01:28:51.960 --> 01:28:54.120]   If you don't invest in it, you can forget about it.
[01:28:54.120 --> 01:28:57.080]   Yeah, if you've got a crypto wallet,
[01:28:57.080 --> 01:28:59.640]   especially if you've got a custodial wallet,
[01:28:59.640 --> 01:29:00.840]   you might want to think about it.
[01:29:00.840 --> 01:29:02.600]   How's your dogecoin doing, Father?
[01:29:02.600 --> 01:29:05.880]   I haven't checked on that for...
[01:29:05.880 --> 01:29:09.880]   I cashed out a huge chunk of the dogecoin
[01:29:09.880 --> 01:29:11.880]   so that I could invest in other cryptos.
[01:29:11.880 --> 01:29:13.720]   And then when it started dipping south,
[01:29:13.720 --> 01:29:15.720]   I cashed it out and I've just got it
[01:29:15.720 --> 01:29:17.400]   in a holding account right now.
[01:29:17.400 --> 01:29:19.480]   So I'm not affected by the dip at all.
[01:29:19.480 --> 01:29:21.320]   I'm just watching it just in horror
[01:29:21.320 --> 01:29:23.240]   for the people who are now stuck
[01:29:23.240 --> 01:29:26.600]   with tens or hundreds of thousands of dollars
[01:29:26.600 --> 01:29:30.280]   in cryptocurrency that will probably never hit.
[01:29:30.280 --> 01:29:31.000]   Yeah.
[01:29:31.000 --> 01:29:33.160]   Andre, are you just experimenting with crypto,
[01:29:33.160 --> 01:29:35.000]   or do you have a significant investment in it?
[01:29:35.000 --> 01:29:39.320]   I was experimenting with the dogecoin
[01:29:39.320 --> 01:29:40.360]   that I mined for fun.
[01:29:41.320 --> 01:29:42.600]   Back in the early, early days,
[01:29:42.600 --> 01:29:44.040]   I was one of the very first miners
[01:29:44.040 --> 01:29:45.320]   and I got a couple million,
[01:29:45.320 --> 01:29:48.680]   which became some real money just a couple of years ago.
[01:29:48.680 --> 01:29:51.800]   And then I sold that and started investing in some others
[01:29:51.800 --> 01:29:52.680]   and made some money there.
[01:29:52.680 --> 01:29:56.600]   But I said this on another twit.
[01:29:56.600 --> 01:30:00.360]   Cryptocurrency is great as a speculative investment,
[01:30:00.360 --> 01:30:03.000]   but you should not fool yourself into thinking
[01:30:03.000 --> 01:30:04.680]   that it's actually anything.
[01:30:04.680 --> 01:30:07.640]   It's not held up by anything.
[01:30:07.640 --> 01:30:09.320]   So you need to be ready to lose...
[01:30:09.320 --> 01:30:10.040]   It is.
[01:30:10.040 --> 01:30:11.560]   It is held up by marketing.
[01:30:11.560 --> 01:30:13.000]   Exactly.
[01:30:13.000 --> 01:30:13.400]   Exactly.
[01:30:13.400 --> 01:30:15.880]   By marketing and how much press it gets, period.
[01:30:15.880 --> 01:30:16.600]   Right.
[01:30:16.600 --> 01:30:19.720]   So people like me find it interesting,
[01:30:19.720 --> 01:30:22.520]   but there are some people who invest it
[01:30:22.520 --> 01:30:24.520]   because they thought it would just keep going up and up and up.
[01:30:24.520 --> 01:30:27.080]   And they are in so much pain right now.
[01:30:27.080 --> 01:30:30.280]   Well, the thing is for every person that goes up,
[01:30:30.280 --> 01:30:32.120]   somebody else loses money.
[01:30:32.120 --> 01:30:34.760]   So it can't go up for everyone.
[01:30:34.760 --> 01:30:36.040]   Look at...
[01:30:36.040 --> 01:30:36.760]   This is weird.
[01:30:36.760 --> 01:30:38.200]   I don't know what this has to do with anything,
[01:30:38.200 --> 01:30:40.280]   but you made me think of this with Dogecoin.
[01:30:40.280 --> 01:30:44.280]   Crypto.com, 30% of the assets are in Bitcoin.
[01:30:44.280 --> 01:30:47.640]   19% of the assets are in Shiba Inu.
[01:30:47.640 --> 01:30:50.520]   More than they have in Ethereum.
[01:30:50.520 --> 01:30:53.400]   That is a meme crypto from a meme coin.
[01:30:53.400 --> 01:30:55.640]   I mean, we're just doing multiple levels.
[01:30:55.640 --> 01:30:58.600]   This is basically the CDO of cryptos.
[01:30:58.600 --> 01:30:59.480]   It's crazy.
[01:30:59.480 --> 01:30:59.880]   All right.
[01:30:59.880 --> 01:31:03.160]   So let's cover the thing that's...
[01:31:04.760 --> 01:31:09.320]   So you may remember the Super Bowl last year,
[01:31:09.320 --> 01:31:12.680]   a guy named Larry David, Curb Your Enthusiasm did a very nice,
[01:31:12.680 --> 01:31:16.600]   very funny ad for a little company called FTX,
[01:31:16.600 --> 01:31:19.800]   in which Larry was through the ages.
[01:31:19.800 --> 01:31:22.440]   He was in the ancient times.
[01:31:22.440 --> 01:31:24.600]   He said, "The wheel, it's no good.
[01:31:24.600 --> 01:31:26.360]   You can eat a bagel.
[01:31:26.360 --> 01:31:26.680]   What?
[01:31:26.680 --> 01:31:28.120]   It's you can't eat a wheel."
[01:31:28.120 --> 01:31:32.600]   And then he downplays the fork, the toilet, every invention.
[01:31:32.600 --> 01:31:35.560]   And finally, the modern times he's sitting at his desk,
[01:31:35.560 --> 01:31:37.880]   and some guy says, "Well, let's talk about cryptocurrency."
[01:31:37.880 --> 01:31:40.680]   And Larry says, "Yeah, it's not going anywhere."
[01:31:40.680 --> 01:31:42.680]   And then the tagline for FTX.
[01:31:42.680 --> 01:31:43.480]   Remember that name?
[01:31:43.480 --> 01:31:45.480]   Don't be a Larry.
[01:31:45.480 --> 01:31:50.920]   Or maybe you remember Matt Damon doing an ad for crypto.com,
[01:31:50.920 --> 01:31:51.800]   as it turns out.
[01:31:51.800 --> 01:31:54.520]   Fortune favors the brave.
[01:31:54.520 --> 01:31:58.040]   You know, the Wright brothers, the moon landing.
[01:31:58.040 --> 01:31:59.640]   You should buy some crypto.
[01:31:59.640 --> 01:32:03.880]   Or maybe you remember Tom Brady and his model wife, Giselle Bunchen,
[01:32:03.880 --> 01:32:06.120]   with their phone saying, "Tom's saying,
[01:32:06.120 --> 01:32:09.160]   I'm going to make a trade on FTX."
[01:32:09.160 --> 01:32:12.440]   And Giselle saying, "Oh, how exciting.
[01:32:12.440 --> 01:32:17.080]   All these celebrities promoting cryptocurrencies."
[01:32:17.080 --> 01:32:20.040]   So what happened to FTX this week?
[01:32:20.040 --> 01:32:25.480]   It went from $19 billion in assets to zero.
[01:32:27.800 --> 01:32:31.000]   Sam Bankman-Freed, who was on the cover of Fortune Magazine,
[01:32:31.000 --> 01:32:37.800]   as the next great, I don't know, technology savior.
[01:32:37.800 --> 01:32:42.280]   Apparently, if he had been doing with FTX,
[01:32:42.280 --> 01:32:44.840]   been doing the same thing with real money,
[01:32:44.840 --> 01:32:46.920]   he'd be in jail by now.
[01:32:46.920 --> 01:32:48.680]   He had created FTX as a,
[01:32:48.680 --> 01:32:50.680]   and correct me if I'm wrong, Robert,
[01:32:50.680 --> 01:32:51.800]   because you're the expert on this,
[01:32:51.800 --> 01:32:54.280]   as a crypto exchange,
[01:32:54.280 --> 01:32:57.800]   a place you would store your crypto or trade your Shiba Inu
[01:32:57.800 --> 01:33:00.520]   for some doge or whatever.
[01:33:00.520 --> 01:33:05.080]   But at the same time, he founded a trading company called Alameda Research,
[01:33:05.080 --> 01:33:11.080]   which was funded by tokens, FT tokens from FTX.
[01:33:11.080 --> 01:33:12.600]   Correct.
[01:33:12.600 --> 01:33:17.720]   So he moved about $10 billion of customer funds into his trading company.
[01:33:17.720 --> 01:33:22.280]   At some point last week, customers got a little nervous,
[01:33:22.280 --> 01:33:27.880]   I think, honestly, it was a little feud between him and another big crypto exchange,
[01:33:27.880 --> 01:33:30.280]   Binance run by Chinese.
[01:33:30.280 --> 01:33:33.080]   Well, we don't know exactly, right?
[01:33:33.080 --> 01:33:36.120]   But the CEO is a founder, I think, is Chinese.
[01:33:36.120 --> 01:33:42.680]   Binance, somehow, I think it was on Twitter, even,
[01:33:42.680 --> 01:33:44.440]   somehow impugned FTX.
[01:33:44.440 --> 01:33:47.880]   And people said, "You know what? I'm going to get my money out of FTX.
[01:33:48.440 --> 01:33:54.360]   A run on the bank, $6 billion, later they had to freeze assets,
[01:33:54.360 --> 01:33:56.680]   because they didn't have reserves to cover the run."
[01:33:56.680 --> 01:34:02.840]   At that point, everybody went a little bit crazy.
[01:34:02.840 --> 01:34:07.960]   FTX had earlier this year, Sam, Sam is a smart guy.
[01:34:07.960 --> 01:34:09.880]   He bought burnished his image.
[01:34:09.880 --> 01:34:13.400]   He became famous for effective altruism,
[01:34:13.400 --> 01:34:15.640]   implied that he was giving away a lot of money.
[01:34:15.640 --> 01:34:20.040]   He was worth at $19 billion.
[01:34:20.040 --> 01:34:25.080]   He bailed out Robinhood when Robinhood kind of got underwater.
[01:34:25.080 --> 01:34:28.040]   He came in and said, "We'll hear. We're going to give you some FT."
[01:34:28.040 --> 01:34:29.800]   And we'll bail you out.
[01:34:29.800 --> 01:34:31.560]   He did the same thing for BlockFi.
[01:34:31.560 --> 01:34:35.000]   When they were starting to have some trouble,
[01:34:35.000 --> 01:34:36.360]   he was really the white knight.
[01:34:36.360 --> 01:34:38.520]   Everybody thought, "This is the guy who understands crypto.
[01:34:38.520 --> 01:34:40.520]   Who knows how to make it work? Who doesn't make it fly?"
[01:34:40.520 --> 01:34:43.320]   Now, apparently, he's in custody in the Bahamas,
[01:34:43.320 --> 01:34:45.400]   because he was fleeing to Dubai.
[01:34:45.400 --> 01:34:46.360]   The money is gone.
[01:34:46.360 --> 01:34:49.160]   His net worth has gone down below zero.
[01:34:49.160 --> 01:34:54.440]   And there are a lot of FTX wallet holders who are out of luck,
[01:34:54.440 --> 01:34:58.280]   just like BlockFi, which also froze withdrawals this week.
[01:34:58.280 --> 01:35:03.400]   Binance was going to swoop in like a white knight.
[01:35:03.400 --> 01:35:06.040]   And then, and I swear to God, I think they planned this.
[01:35:06.040 --> 01:35:07.720]   They said, "Oh, yeah, we'll save them."
[01:35:07.720 --> 01:35:08.920]   And then they said, "Oh, wait a minute.
[01:35:08.920 --> 01:35:10.120]   We just looked at the balance sheet."
[01:35:10.120 --> 01:35:11.720]   No, no, we don't have anything to do with it,
[01:35:11.720 --> 01:35:13.560]   which immediately created the whole thing,
[01:35:13.560 --> 01:35:16.920]   putting a competitor out of business.
[01:35:16.920 --> 01:35:20.200]   So, Robert, what's your take on all this?
[01:35:20.200 --> 01:35:27.640]   Was Sam Bankman freed a Bernie Madoff or a genius?
[01:35:27.640 --> 01:35:33.480]   No, he was more like, what's his name from Fire Festival?
[01:35:33.480 --> 01:35:35.720]   Starting up Magnesis.
[01:35:35.720 --> 01:35:37.240]   It is the Fire Festival.
[01:35:37.240 --> 01:35:38.600]   It is. It's exactly what it is.
[01:35:38.600 --> 01:35:40.040]   Yeah, that's what this is.
[01:35:40.040 --> 01:35:42.520]   Because, okay, he was trying to set himself up,
[01:35:42.520 --> 01:35:43.480]   as I think you nailed it.
[01:35:43.480 --> 01:35:44.760]   He wanted to be the white knight.
[01:35:44.760 --> 01:35:48.040]   He was going to be the one who would show that it was possible
[01:35:48.040 --> 01:35:52.600]   to create a professional exchange and just make money from the fees.
[01:35:52.600 --> 01:35:54.360]   In fact, it was making so much money
[01:35:54.360 --> 01:35:59.080]   that he was able to bail out fellow trading sites and services.
[01:35:59.080 --> 01:36:01.400]   But as it turned out,
[01:36:01.400 --> 01:36:05.800]   that the only profits he was making was off of the trading
[01:36:05.800 --> 01:36:07.560]   that he was doing with customer funds,
[01:36:08.200 --> 01:36:10.920]   which worked fine as long as the market continued to go up.
[01:36:10.920 --> 01:36:12.760]   All of these stories are the same.
[01:36:12.760 --> 01:36:14.680]   As long as the market continues to go up,
[01:36:14.680 --> 01:36:17.720]   he kept getting enough profit from that illegal trading
[01:36:17.720 --> 01:36:20.920]   to be able to make the business look reputable.
[01:36:20.920 --> 01:36:25.160]   The moment that it dipped and we started to lose market,
[01:36:25.160 --> 01:36:28.680]   there was not enough money to cover the customer funds by far.
[01:36:28.680 --> 01:36:32.120]   And when Binance came in and they looked at the books,
[01:36:32.120 --> 01:36:36.840]   the one thing that people heard was misallocation of customer funds.
[01:36:36.840 --> 01:36:39.320]   That kills an exchange instantly.
[01:36:39.320 --> 01:36:44.120]   It always has and it always will because then they realized
[01:36:44.120 --> 01:36:48.120]   the exchange is not playing on the up and up with the trading,
[01:36:48.120 --> 01:36:50.840]   with the tokens that it has been entrusted.
[01:36:50.840 --> 01:36:55.160]   And once it does not have the trust of the token holders,
[01:36:55.160 --> 01:36:58.520]   every token because of the nature of cryptocurrency
[01:36:58.520 --> 01:37:00.440]   can be withdrawn immediately.
[01:37:00.440 --> 01:37:04.280]   So the only option is so neatly halt all trading.
[01:37:04.280 --> 01:37:07.720]   Imagine this is a run on the bank on steroids,
[01:37:07.720 --> 01:37:11.400]   without even the waiting period to go down to the bank to close your account.
[01:37:11.400 --> 01:37:18.280]   And yeah, the entire crypto community looks at FTX
[01:37:18.280 --> 01:37:20.600]   and it's kind of resigned at this point.
[01:37:20.600 --> 01:37:22.120]   We know this is happening.
[01:37:22.120 --> 01:37:23.640]   We know it's going to happen a lot more.
[01:37:23.640 --> 01:37:26.120]   There are other exchanges that are going to go under
[01:37:26.120 --> 01:37:30.520]   and everyone is just hoping that theirs will hold out long enough
[01:37:30.520 --> 01:37:33.400]   for crypto to go back up so they can get some of their investment back out.
[01:37:33.400 --> 01:37:35.000]   It almost is a terrible idea.
[01:37:35.000 --> 01:37:36.040]   Yeah. That's a terrible idea.
[01:37:36.040 --> 01:37:36.520]   Get it out now.
[01:37:36.520 --> 01:37:42.440]   I mean, listen, almost 100 years to the day, it's what, 28, it's been almost...
[01:37:42.440 --> 01:37:44.200]   1929, almost 100 years.
[01:37:44.200 --> 01:37:44.760]   It's a 20-21.
[01:37:44.760 --> 01:37:45.560]   Yeah.
[01:37:45.560 --> 01:37:46.040]   Yeah.
[01:37:46.040 --> 01:37:49.800]   Crash, still the largest sell-off of shares in US history.
[01:37:49.800 --> 01:37:55.160]   You know, because of trust and confidence, any exchange,
[01:37:55.160 --> 01:37:59.400]   exchanges only work when they're not hyper-volatile
[01:37:59.400 --> 01:38:05.880]   and the volatility is tied directly to consumer and customer confidence and trust.
[01:38:05.880 --> 01:38:08.360]   So we're going to continue to have liquidity events.
[01:38:08.360 --> 01:38:12.760]   I don't care who's running it or how much they're promising to do
[01:38:12.760 --> 01:38:13.640]   altruistically.
[01:38:13.640 --> 01:38:17.640]   That's one variable that's outside of everybody's control.
[01:38:17.640 --> 01:38:23.480]   And by the way, once you introduce bots or once you introduce bad actors
[01:38:23.480 --> 01:38:29.080]   who start ceding mistrust, that run on the bank that Padre just talked about,
[01:38:29.080 --> 01:38:31.960]   happens faster and there's no way to stop it.
[01:38:31.960 --> 01:38:32.520]   Yeah.
[01:38:32.520 --> 01:38:32.680]   Right?
[01:38:32.680 --> 01:38:33.880]   So this is what I was talking about earlier.
[01:38:33.880 --> 01:38:41.480]   We have very long-tail challenges ahead of us and the technology is on a developmental
[01:38:41.480 --> 01:38:44.920]   track that's just going faster than our current regulatory track.
[01:38:44.920 --> 01:38:48.040]   By the time those decisions get made, it's too late.
[01:38:48.040 --> 01:38:54.040]   So I'm not usually a fan of regulating upfront and I'm not saying we should do that here either
[01:38:54.040 --> 01:38:55.320]   because it stifles innovation.
[01:38:55.320 --> 01:38:56.040]   That being said...
[01:38:56.040 --> 01:39:01.000]   But isn't that the point of crypto is to stay unregulated as best as possible?
[01:39:01.000 --> 01:39:06.680]   Well, listen, again, I think central banks have gotten us into some of the problem that
[01:39:06.680 --> 01:39:08.440]   we're in right now.
[01:39:08.440 --> 01:39:13.000]   But one of the things that we've learned is that the very institutions designed to
[01:39:13.000 --> 01:39:18.040]   keep us all afloat and to trust are the ones that are causing problems at the same time
[01:39:18.040 --> 01:39:21.880]   that these new exchanges that are totally decentralized or whatever are causing problems.
[01:39:21.880 --> 01:39:25.160]   We are in a bad situation, I think.
[01:39:25.720 --> 01:39:27.720]   I feel like...
[01:39:27.720 --> 01:39:31.480]   Well, maybe I'm wrong, but I feel like this was a scam.
[01:39:31.480 --> 01:39:39.560]   And that a lot of crypto and NFTs, I'll throw those into our essentially pyramid schemes that
[01:39:39.560 --> 01:39:44.440]   the people who got in early desperately need people to buy these products at a higher price
[01:39:44.440 --> 01:39:45.000]   to speculate on.
[01:39:45.000 --> 01:39:45.480]   Hey, is that...
[01:39:45.480 --> 01:39:48.680]   Oh my God, you couldn't be more on the mark, Leo.
[01:39:48.680 --> 01:39:52.760]   That is exactly why we saw what is four or five ads during the Super Bowl.
[01:39:52.760 --> 01:39:58.760]   You need people who are ignorant to the market, which I'm sorry, that was the point of those
[01:39:58.760 --> 01:39:59.560]   Super Bowl ads.
[01:39:59.560 --> 01:40:02.120]   Let's get people think they're missing out.
[01:40:02.120 --> 01:40:02.680]   Fortunately...
[01:40:02.680 --> 01:40:04.440]   Let's get more bodies.
[01:40:04.440 --> 01:40:06.760]   Just think about how paramutual betting works.
[01:40:06.760 --> 01:40:11.880]   If you go to a racetrack, it only works if more people put money into the pool.
[01:40:11.880 --> 01:40:17.960]   Well, the difference with paramutual betting is the betting pool then is distributed to the winners.
[01:40:17.960 --> 01:40:18.760]   I know, I know.
[01:40:18.760 --> 01:40:23.800]   I think it's important to separate the technology infrastructure from the business model supporting
[01:40:23.800 --> 01:40:28.040]   it because I will say there's some interesting applications here when it comes to privacy.
[01:40:28.040 --> 01:40:33.480]   So there's a way now to create an NFT for your genome, for example, and grant permission.
[01:40:33.480 --> 01:40:38.440]   So you can license out parts of your genome for, let's say, renumeration by a pharmaceutical company,
[01:40:38.440 --> 01:40:41.560]   but without giving them indefinite rights to it.
[01:40:41.560 --> 01:40:47.080]   So the technology is interesting, but so far, the business models are totally...
[01:40:48.200 --> 01:40:50.280]   There's nobody minding the store, right?
[01:40:50.280 --> 01:40:53.400]   I feel like that was in addition...
[01:40:53.400 --> 01:40:54.440]   There's no one minding... Hold on, hold on.
[01:40:54.440 --> 01:41:00.120]   In addition to these guys being scammers, they took advantage of the fact that the technology
[01:41:00.120 --> 01:41:03.560]   is somewhat confusing to do a lot of hand-waving.
[01:41:03.560 --> 01:41:05.720]   And I really... I hear it all the time.
[01:41:05.720 --> 01:41:08.200]   Other underlying technology is interesting.
[01:41:08.200 --> 01:41:10.440]   Blockchain is just a distributed database.
[01:41:10.440 --> 01:41:11.960]   That's all.
[01:41:11.960 --> 01:41:12.760]   It's not magic.
[01:41:13.560 --> 01:41:18.040]   Cryptocurrency is just... Their evolution.
[01:41:18.040 --> 01:41:25.240]   Fiat currency managed on a blockchain, and I'm not sure that that gives it any real benefit.
[01:41:25.240 --> 01:41:30.120]   Unfortunately, part of the way this was sold, and I think again,
[01:41:30.120 --> 01:41:32.520]   I feel like this is criminal.
[01:41:32.520 --> 01:41:33.320]   Maybe it wasn't.
[01:41:33.320 --> 01:41:34.760]   Maybe it was with the best intentions.
[01:41:34.760 --> 01:41:38.360]   But part of the way it sold is, well, there are a lot of people who the banking system
[01:41:38.360 --> 01:41:43.000]   disenfranchises in third world nations, in the poor people in ghettos.
[01:41:43.560 --> 01:41:50.280]   So JZ created a school to teach young black children, poor black children about cryptocurrency,
[01:41:50.280 --> 01:41:54.600]   because it's somehow magically going to re-enfranchise them.
[01:41:54.600 --> 01:41:57.080]   And it didn't, and it doesn't.
[01:41:57.080 --> 01:41:58.840]   And I'm not saying JZ knew better.
[01:41:58.840 --> 01:42:03.880]   Maybe I'm not saying he was a con man, but I think he was probably conned,
[01:42:03.880 --> 01:42:07.560]   because the technology, there's a lot of hand-waving on the technology,
[01:42:07.560 --> 01:42:11.800]   but really underneath all this, it was a Ponzi scheme.
[01:42:12.360 --> 01:42:19.240]   It was a very traditional old-fashioned scheme, a scam, and unfortunately,
[01:42:19.240 --> 01:42:23.000]   the promise of this technology fooled people.
[01:42:23.000 --> 01:42:29.560]   And that makes me mad because that is a really horrific misuse of technology,
[01:42:29.560 --> 01:42:32.600]   and it's going to make more people bitter about technology, too.
[01:42:32.600 --> 01:42:36.200]   I think they played off the fact that nobody really understood it.
[01:42:36.200 --> 01:42:38.200]   In fact, they intentionally made it obscure.
[01:42:38.200 --> 01:42:40.600]   Okay, I'm done.
[01:42:40.600 --> 01:42:41.880]   I'll get off my soapbox.
[01:42:42.760 --> 01:42:45.560]   I don't think you're wrong, but again, I'm looking at
[01:42:45.560 --> 01:42:51.560]   I see the convergence of various technologies that are on the horizon,
[01:42:51.560 --> 01:42:54.200]   and NFTs are kind of an intro.
[01:42:54.200 --> 01:43:01.560]   There's some protocols that would allow for better privacy and sharing and value creation that
[01:43:01.560 --> 01:43:02.040]   don't exist.
[01:43:02.040 --> 01:43:07.640]   Is it better than me having an encrypted vault of mail?
[01:43:07.640 --> 01:43:08.280]   Like solid?
[01:43:08.280 --> 01:43:09.560]   You mean like the new solid protocol?
[01:43:09.560 --> 01:43:11.160]   Yeah, that's Tim Berners-Lee's idea, right?
[01:43:11.160 --> 01:43:12.200]   So like that works, too.
[01:43:12.200 --> 01:43:15.800]   So the blockchain thing is not necessary.
[01:43:15.800 --> 01:43:17.480]   The distributed ledger is not necessary.
[01:43:17.480 --> 01:43:20.440]   Yeah, I mean, I think we're in this like we're getting the wiggles out.
[01:43:20.440 --> 01:43:25.480]   I think we're in we're just in the middle of this transition from our existing sophisticated
[01:43:25.480 --> 01:43:30.680]   technologies to the next set of sophisticated protocols and technologies,
[01:43:30.680 --> 01:43:33.480]   and it's just going to take us a while to settle on what that is.
[01:43:33.480 --> 01:43:36.760]   I'm not necessarily impugning Satoshi Nakamoto.
[01:43:36.760 --> 01:43:39.480]   He probably did well, who knows?
[01:43:39.480 --> 01:43:43.720]   I mean, whoever that was has billions of dollars in Bitcoin somewhere.
[01:43:43.720 --> 01:43:50.360]   But probably maybe let's say he intended this with the best intention, and it's an interesting
[01:43:50.360 --> 01:43:54.680]   technology, but there were a lot of people who jumped on this bandwagon as a way to make a
[01:43:54.680 --> 01:43:57.960]   quick buck, unfortunately, and it tarnishes it.
[01:43:57.960 --> 01:44:01.080]   Tim Berners-Lee said, "Web 3 is not the web."
[01:44:04.360 --> 01:44:10.600]   The people behind Web 3, the so-called distributed decentralized internet,
[01:44:10.600 --> 01:44:14.360]   are the very centralized and recent Horowitz.
[01:44:14.360 --> 01:44:16.360]   They want to make money on this.
[01:44:16.360 --> 01:44:22.280]   Jack would say like Web 5, everybody that I talk to is Web 5 is one matter.
[01:44:22.280 --> 01:44:25.640]   It just sounds to me like yet another way to take people's money.
[01:44:25.640 --> 01:44:31.160]   And I really feel badly for there were a lot of people who have lost a lot of money.
[01:44:33.240 --> 01:44:37.960]   What Sam Bank with Freeman was doing was similar to what a lot of people in crypto do,
[01:44:37.960 --> 01:44:40.440]   which is moving stuff around to make it look like.
[01:44:40.440 --> 01:44:44.920]   For instance, a lot of NFTs are bought by the same person who made the NFT to kind of
[01:44:44.920 --> 01:44:45.960]   pump up the value.
[01:44:45.960 --> 01:44:52.120]   I think a lot of Bitcoin bros were throwing their money into FTX.
[01:44:52.120 --> 01:44:54.680]   So probably a lot of the people lost money.
[01:44:54.680 --> 01:44:59.400]   We're playing with house money anyway.
[01:44:59.400 --> 01:45:04.760]   But let's make this clear. Everyone knew, and if you didn't know this,
[01:45:04.760 --> 01:45:08.600]   this was an extremely risky investment.
[01:45:08.600 --> 01:45:11.720]   Morning short-
[01:45:11.720 --> 01:45:13.640]   I think a lot of people buying cars on this.
[01:45:13.640 --> 01:45:15.320]   I think a lot of people buying crypto on
[01:45:15.320 --> 01:45:19.480]   Robinhood didn't know that.
[01:45:19.480 --> 01:45:20.680]   There's a hedge fund.
[01:45:20.680 --> 01:45:22.760]   That's their ignorance for not knowing that.
[01:45:22.760 --> 01:45:25.640]   But it is an extremely risky investment.
[01:45:25.640 --> 01:45:28.920]   There's so much that that throws risk on it.
[01:45:28.920 --> 01:45:31.400]   All investments have different levels of risk.
[01:45:31.400 --> 01:45:34.120]   This one has a super, super high one.
[01:45:34.120 --> 01:45:38.600]   And the reason they got excited, they were excited at the high part of the risk,
[01:45:38.600 --> 01:45:40.760]   the part that you can get a huge return.
[01:45:40.760 --> 01:45:41.480]   Guess what?
[01:45:41.480 --> 01:45:44.600]   When you were looking at risky things, there is not only a high part,
[01:45:44.600 --> 01:45:45.880]   there is also a low part.
[01:45:45.880 --> 01:45:47.800]   We're now seeing the low part.
[01:45:47.800 --> 01:45:52.280]   I don't think it's fair for a lot of people to sort of,
[01:45:53.240 --> 01:45:55.400]   not that David, you were shaming them or anything like that.
[01:45:55.400 --> 01:45:57.240]   I'm not. I'm not. I'm trying to get a dream.
[01:45:57.240 --> 01:45:58.600]   Risky, risky investment.
[01:45:58.600 --> 01:46:03.400]   But I think the point is, I don't think a lot of people saw it as risky.
[01:46:03.400 --> 01:46:04.360]   I told them it was okay.
[01:46:04.360 --> 01:46:06.200]   That statement said it's okay.
[01:46:06.200 --> 01:46:09.080]   I don't think a lot of people understand that it is a risk,
[01:46:09.080 --> 01:46:13.080]   especially when you have famous football players that you really admire,
[01:46:13.080 --> 01:46:16.600]   seemingly endorsing whatever it is.
[01:46:16.600 --> 01:46:17.640]   Let's not forget.
[01:46:17.640 --> 01:46:20.600]   Now, let me harp on American education for two seconds.
[01:46:20.600 --> 01:46:24.040]   It's not as though we teach kids coming up in our schools,
[01:46:24.040 --> 01:46:27.960]   regardless of the school, basic financial skills,
[01:46:27.960 --> 01:46:32.200]   basic statistics, like enough math to understand probabilities.
[01:46:32.200 --> 01:46:39.240]   I think we're dealing with a lot of people who were feeling excited
[01:46:39.240 --> 01:46:40.760]   and believed in the marketing.
[01:46:40.760 --> 01:46:44.840]   I genuinely do not think they understood risk,
[01:46:44.840 --> 01:46:46.920]   and especially when it comes to Robinhood,
[01:46:46.920 --> 01:46:49.640]   which literally sounds like a good, safe play.
[01:46:49.640 --> 01:46:55.800]   You know, I don't think we can entirely blame the consumer here.
[01:46:55.800 --> 01:46:59.320]   I don't know this. Can you actually short crypto?
[01:46:59.320 --> 01:47:02.520]   Well, I do you know the process to short anything
[01:47:02.520 --> 01:47:04.040]   is extraordinarily challenging.
[01:47:04.040 --> 01:47:06.120]   To me, it's a little bit like playing craps,
[01:47:06.120 --> 01:47:08.760]   which is the only game that I play when I'm in Vegas.
[01:47:08.760 --> 01:47:11.560]   In order to make any actual money,
[01:47:11.560 --> 01:47:16.920]   you have to have time and you have to have you have to have a lot of cash on the table
[01:47:16.920 --> 01:47:19.480]   to make that work and you have to be able to do math
[01:47:19.480 --> 01:47:21.720]   and you have to be able to sort of do that over a long period of time.
[01:47:21.720 --> 01:47:24.120]   It's like two ways to short a stock.
[01:47:24.120 --> 01:47:27.800]   One is you have to be Dr. Manhattan,
[01:47:27.800 --> 01:47:31.880]   like stop time, see what's coming in advance and pick the day
[01:47:31.880 --> 01:47:35.480]   that something bad will happen and then make sure that you get your money out first.
[01:47:35.480 --> 01:47:39.720]   But you still have to have $200,000 to make that work.
[01:47:39.720 --> 01:47:41.240]   And your heavily leveraged.
[01:47:41.240 --> 01:47:41.720]   It only works if you're a hedge fund, basically.
[01:47:41.720 --> 01:47:44.840]   And your heavily leveraged so you can get bad hurt as well.
[01:47:45.480 --> 01:47:46.480]   60,000.
[01:47:46.480 --> 01:47:47.480]   Listen to this.
[01:47:47.480 --> 01:47:49.960]   You can have someone create an instrument for you to short.
[01:47:49.960 --> 01:47:50.440]   Right.
[01:47:50.440 --> 01:47:54.600]   And everyone is going to create an instrument for crypto because it's so volatile.
[01:47:54.600 --> 01:47:55.080]   There are.
[01:47:55.080 --> 01:47:56.920]   There'd be no way to properly gauge the risk.
[01:47:56.920 --> 01:47:57.560]   Get ready.
[01:47:57.560 --> 01:47:59.160]   There are crypto derivatives.
[01:47:59.160 --> 01:48:05.160]   In fact, two-thirds of traditional hedge funds, not crypto hedge funds,
[01:48:05.160 --> 01:48:09.560]   two-thirds of traditional hedge funds are invested in Bitcoin and Ethereum.
[01:48:09.560 --> 01:48:15.320]   I have to think those hedge funds are hurting at this point.
[01:48:15.320 --> 01:48:15.640]   Right.
[01:48:15.640 --> 01:48:22.440]   And they're doing it with direct, but also futures options.
[01:48:22.440 --> 01:48:25.080]   They're doing derivatives as well.
[01:48:25.080 --> 01:48:29.720]   Derivatives is another great example of something that was so complicated.
[01:48:29.720 --> 01:48:34.200]   Nobody understood it except a handful of quants on Wall Street.
[01:48:34.200 --> 01:48:40.360]   And there aren't people who really lost their shirt when the derivatives market crashed because
[01:48:40.360 --> 01:48:41.800]   they didn't understand what they were investing in.
[01:48:42.680 --> 01:48:45.960]   So I don't want to blame people who lost money in crypto.
[01:48:45.960 --> 01:48:49.080]   Yes, it was risky, but nobody was telling them it was risky.
[01:48:49.080 --> 01:48:50.280]   I was telling them it was risky.
[01:48:50.280 --> 01:48:54.680]   I'm saying it's I would say and stay out for more than a year.
[01:48:54.680 --> 01:48:59.960]   But I mean, I don't even know, but like, you know, there are, you know,
[01:48:59.960 --> 01:49:07.640]   ratings, bureaus that rate stocks that rate bonds, you know, it seems like everything in
[01:49:07.640 --> 01:49:08.200]   crypto is.
[01:49:08.200 --> 01:49:09.240]   No, but they weren't ratings.
[01:49:09.240 --> 01:49:10.280]   No, but they weren't ratings.
[01:49:10.280 --> 01:49:11.720]   Are there any ratings on these things?
[01:49:11.720 --> 01:49:12.680]   No, no, no, no, no.
[01:49:12.680 --> 01:49:13.880]   No, no, no, no.
[01:49:13.880 --> 01:49:15.000]   It's an unregistered.
[01:49:15.000 --> 01:49:17.480]   You can't even rate it.
[01:49:17.480 --> 01:49:21.720]   It's like, no, it's all got the high risk, whatever, you know, the highest risk rating is,
[01:49:21.720 --> 01:49:22.920]   it's that times 10.
[01:49:22.920 --> 01:49:23.880]   I mean, yeah.
[01:49:23.880 --> 01:49:28.600]   Oh, but the average investor doesn't know that the average investor knows that Alec Baldwin got on TV
[01:49:28.600 --> 01:49:35.720]   and Toreau and they sell stock right next to be taking their investment advice from Alec Baldwin and Matt Damon.
[01:49:35.720 --> 01:49:39.000]   To be fair, no, but listen, F T I M. L. B.
[01:49:39.560 --> 01:49:40.760]   Let's talk about baseball, right?
[01:49:40.760 --> 01:49:44.440]   The umpires had FTX logos.
[01:49:44.440 --> 01:49:44.760]   Yeah.
[01:49:44.760 --> 01:49:49.160]   That the umpires were just, these are supposed to be the impartial judges on the field.
[01:49:49.160 --> 01:49:49.720]   This is brilliant.
[01:49:49.720 --> 01:49:51.880]   They have a sponsorship on their uniforms.
[01:49:51.880 --> 01:49:53.320]   S B F bought the umpires.
[01:49:53.320 --> 01:49:54.840]   Yeah, but they don't have a choice in that.
[01:49:54.840 --> 01:49:55.640]   You're wearing the shoes.
[01:49:55.640 --> 01:49:58.840]   No, no, no, but this is the point that I'm making.
[01:49:58.840 --> 01:49:59.800]   I trust that they're trusted.
[01:49:59.800 --> 01:50:03.880]   No, I would take investment, advertised investment advice from Alec Baldwin, but think about all the
[01:50:03.880 --> 01:50:05.400]   people selling reverse mortgages.
[01:50:05.400 --> 01:50:07.720]   All of the retirees selling reverse.
[01:50:07.720 --> 01:50:12.760]   I mean, listen, you may not be the audience for this, David, but there are plenty of people.
[01:50:12.760 --> 01:50:16.760]   Oh, no, I know there's an audience hence why there was so much money spent at the Super Bowl.
[01:50:16.760 --> 01:50:17.480]   I keep bringing that up.
[01:50:17.480 --> 01:50:22.440]   It's like they're trying to get people who are not in in Syria.
[01:50:22.440 --> 01:50:23.960]   So you're quite put on that.
[01:50:23.960 --> 01:50:27.880]   If I can offer FTX giving them a massive over here.
[01:50:27.880 --> 01:50:28.600]   Oh, sorry.
[01:50:28.600 --> 01:50:28.920]   Go ahead.
[01:50:28.920 --> 01:50:36.600]   Over here before the crypto crash, we would get at least one to two pitches a month
[01:50:36.600 --> 01:50:39.640]   for creating either a Jesuit crypto or a Vatican crypto.
[01:50:39.640 --> 01:50:40.440]   Oh, no.
[01:50:40.440 --> 01:50:46.840]   Because they knew that the number one currency in the crypto game is some sort of legitimacy.
[01:50:46.840 --> 01:50:53.720]   So for example, if a crypto market exchange has bought the naming rights for a sports arena,
[01:50:53.720 --> 01:50:57.480]   or if they are on the side of an F1 car, people will see that they'll say,
[01:50:57.480 --> 01:50:59.160]   there's obviously money there.
[01:50:59.160 --> 01:51:01.880]   So therefore it must be safe.
[01:51:01.880 --> 01:51:05.960]   So yes, you and I may look at that and say, no, this is a super high risk investment,
[01:51:05.960 --> 01:51:10.520]   but they're going to say, if I saw it on the news, if I saw it on Netflix,
[01:51:10.520 --> 01:51:15.080]   if I saw it on the the arena, the last time I went to go watch the Warriors play,
[01:51:15.080 --> 01:51:17.000]   then it can't possibly be a scam.
[01:51:17.000 --> 01:51:17.720]   Yeah.
[01:51:17.720 --> 01:51:21.400]   But this is back to the original premise that the value that it has,
[01:51:21.400 --> 01:51:25.240]   the only value it has because it doesn't isn't backed by anything.
[01:51:25.240 --> 01:51:30.040]   The value it has is how much marketing it has, what it associates its wealth,
[01:51:30.040 --> 01:51:31.560]   and the amount of news that comes out.
[01:51:31.560 --> 01:51:35.640]   That's why there's so many news outlets that are about cryptocurrency.
[01:51:35.640 --> 01:51:40.040]   And there's so much advertising that's going on because that is how they can
[01:51:40.040 --> 01:51:41.560]   keep their value high.
[01:51:41.560 --> 01:51:46.280]   If all of a sudden they stopped advertising, there is no more news, it would all crash.
[01:51:46.280 --> 01:51:47.480]   Yes, of course.
[01:51:47.480 --> 01:51:52.440]   I think there's got any currency at this point.
[01:51:52.440 --> 01:51:54.360]   But not that that is true.
[01:51:54.360 --> 01:52:03.160]   And that's partially why China, so a very long story short, when Russia did its thing
[01:52:03.160 --> 01:52:09.160]   and US levied sanctions, this gave the world an alternative to it.
[01:52:09.160 --> 01:52:13.400]   It showed the rest of the world just how strongly the dollar is tied to decisions
[01:52:13.400 --> 01:52:14.120]   that are being made.
[01:52:14.120 --> 01:52:16.440]   And it gave China an opening.
[01:52:16.440 --> 01:52:21.480]   China's digital yuan is backed by the government.
[01:52:21.480 --> 01:52:24.600]   So again, there's different types of crypto and different types of exchanges.
[01:52:24.600 --> 01:52:29.160]   So the freewheeling ones we have in the West may crash and burn,
[01:52:29.160 --> 01:52:35.080]   but the ones that are being developed elsewhere that are CBDCs or backed in some other way
[01:52:35.080 --> 01:52:40.840]   by the government, that actually poses a little bit of a geopolitical threat.
[01:52:40.840 --> 01:52:45.160]   Well, and if the US decided, as I'm sure they will at some point, to do a digital dollar,
[01:52:45.160 --> 01:52:49.240]   it's not really that much different than a regular dollar, right?
[01:52:49.240 --> 01:52:50.600]   It's all digital these days.
[01:52:50.600 --> 01:52:53.800]   It's not tied to anything.
[01:52:53.800 --> 01:52:54.920]   It's a fiat currency.
[01:52:55.480 --> 01:52:59.720]   But that doesn't necessarily, I think people would trust a digital dollar or a digital yuan,
[01:52:59.720 --> 01:53:02.280]   but that's not what we're talking about.
[01:53:02.280 --> 01:53:04.840]   We're talking about buying a big one.
[01:53:04.840 --> 01:53:05.960]   Just trust in exchanges.
[01:53:05.960 --> 01:53:08.200]   I don't know.
[01:53:08.200 --> 01:53:10.440]   I don't need to be super down.
[01:53:10.440 --> 01:53:15.880]   I'm a little down on energy because of the hike, but I'm really worried about these exchanges.
[01:53:15.880 --> 01:53:17.480]   Not for my own personal finances.
[01:53:17.480 --> 01:53:20.280]   I'm just really worried that a lot of people are
[01:53:20.280 --> 01:53:21.000]   that's what I think.
[01:53:21.000 --> 01:53:25.640]   Blowing through a lot of money, and that's going to just be these, it's going to cause us problems going forward.
[01:53:25.640 --> 01:53:29.640]   With the crash in crypto, I've got to wonder how these hedge funds are doing that have all this big,
[01:53:29.640 --> 01:53:32.040]   they're putting a lot of money in crypto.
[01:53:32.040 --> 01:53:37.000]   Fidelity is like a couple of weeks ago, you can invest your 401k.
[01:53:37.000 --> 01:53:38.280]   This is your saving.
[01:53:38.280 --> 01:53:39.480]   This is a bad idea.
[01:53:39.480 --> 01:53:40.440]   This is a bad idea.
[01:53:40.440 --> 01:53:40.920]   Bad idea.
[01:53:40.920 --> 01:53:48.440]   I'm terrified that people's retirement is going to evaporate, and we don't have a social safety net in place.
[01:53:48.440 --> 01:53:49.800]   You know why they do that?
[01:53:49.800 --> 01:53:51.320]   They do that because of demand from...
[01:53:51.320 --> 01:53:52.360]   They can't go away away their funds.
[01:53:52.360 --> 01:53:53.640]   Yeah.
[01:53:53.640 --> 01:53:53.720]   Yeah.
[01:53:53.720 --> 01:53:54.680]   Junk funds.
[01:53:54.680 --> 01:53:55.560]   You know why they do that?
[01:53:55.560 --> 01:53:57.160]   They do that because their customers demanded it.
[01:53:57.160 --> 01:53:59.880]   I don't think Fidelity thought, "Oh, we really got to come up with this."
[01:53:59.880 --> 01:54:00.280]   No, I don't.
[01:54:00.280 --> 01:54:02.200]   People were saying, "I want to invest in crypto."
[01:54:02.200 --> 01:54:05.880]   So they created an instrument to do that.
[01:54:05.880 --> 01:54:08.600]   There are a lot of exchanges too.
[01:54:08.600 --> 01:54:10.280]   They have to deliver a product.
[01:54:10.280 --> 01:54:11.800]   Their customers demanded it.
[01:54:11.800 --> 01:54:12.760]   That's the problem.
[01:54:12.760 --> 01:54:15.480]   I think you can't just say, "Well, it's an ignorant investment.
[01:54:15.480 --> 01:54:16.440]   You should never have done it.
[01:54:16.440 --> 01:54:17.240]   You should have known it was..."
[01:54:17.960 --> 01:54:21.720]   I mean, who's buying cartoon apes?
[01:54:21.720 --> 01:54:25.880]   I mean, that's obviously stupid.
[01:54:25.880 --> 01:54:27.720]   You know why people do it?
[01:54:27.720 --> 01:54:29.080]   Because they think they're going to make money on it.
[01:54:29.080 --> 01:54:33.240]   They think, "Well, you know, but it's hot right now, so I'm going to buy it low and sell high."
[01:54:33.240 --> 01:54:35.080]   Maybe it's pretty...
[01:54:35.080 --> 01:54:36.360]   Weaponized FOMO.
[01:54:36.360 --> 01:54:36.600]   Yeah.
[01:54:36.600 --> 01:54:37.640]   Yeah.
[01:54:37.640 --> 01:54:37.640]   Yeah.
[01:54:37.640 --> 01:54:39.000]   That's exactly what it is.
[01:54:39.000 --> 01:54:44.040]   If you want a guarantee, then it's going to be less risk, and you're going to get less upside, period.
[01:54:44.040 --> 01:54:44.360]   Yeah.
[01:54:45.320 --> 01:54:47.800]   Somebody's saying FTX guaranteed 8% returns.
[01:54:47.800 --> 01:54:49.080]   That's pretty good.
[01:54:49.080 --> 01:54:49.640]   I'd take it.
[01:54:49.640 --> 01:54:51.160]   Do they actually guarantee?
[01:54:51.160 --> 01:54:51.960]   Well, apparently not.
[01:54:51.960 --> 01:54:53.640]   I'm well, no, I'm at like...
[01:54:53.640 --> 01:54:54.280]   I wonder what...
[01:54:54.280 --> 01:54:55.800]   That should have been the first red flag.
[01:54:55.800 --> 01:54:56.280]   Yeah.
[01:54:56.280 --> 01:54:58.760]   It's like, "Oh, okay."
[01:54:58.760 --> 01:54:59.240]   No.
[01:54:59.240 --> 01:55:05.000]   There's a great article in the Atlantic about the match king of the last century.
[01:55:05.000 --> 01:55:05.720]   Did you read that?
[01:55:05.720 --> 01:55:13.240]   They're kind of comparing him Samuel Bankman-Freed to the match king,
[01:55:14.360 --> 01:55:22.680]   who guaranteed 20 to 30% returns, and was also basically a Ponzi scheme.
[01:55:22.680 --> 01:55:25.640]   But that's all what Ponzi schemes do.
[01:55:25.640 --> 01:55:26.680]   They offer...
[01:55:26.680 --> 01:55:27.400]   It's a matter of...
[01:55:27.400 --> 01:55:27.480]   It's a matter of...
[01:55:27.480 --> 01:55:28.680]   ...your universally high returns.
[01:55:28.680 --> 01:55:28.920]   Yeah.
[01:55:28.920 --> 01:55:32.280]   And shouldn't people go, "Oh, well, that's ridiculous?"
[01:55:32.280 --> 01:55:36.040]   Yes, they should, but they're not.
[01:55:36.040 --> 01:55:38.120]   Is it greed?
[01:55:38.120 --> 01:55:41.960]   Well, because every good Ponzi scheme does pay out at the start.
[01:55:41.960 --> 01:55:42.440]   Right.
[01:55:42.440 --> 01:55:46.440]   And so, if you have a friend who's getting paid by the Ponzi scheme,
[01:55:46.440 --> 01:55:50.280]   if you have a friend who has made a million dollars on crypto,
[01:55:50.280 --> 01:55:52.920]   it's no longer I'm being safe with my investment.
[01:55:52.920 --> 01:55:57.720]   It's I am being stupid because I am falling so far behind everybody else.
[01:55:57.720 --> 01:55:58.440]   Yeah.
[01:55:58.440 --> 01:56:00.440]   That's the weaponized formula that Amy was talking about.
[01:56:00.440 --> 01:56:01.560]   I think it's just people who are...
[01:56:01.560 --> 01:56:01.560]   People who are...
[01:56:01.560 --> 01:56:02.120]   ...believe.
[01:56:02.120 --> 01:56:03.320]   Oh, yeah, yeah.
[01:56:03.320 --> 01:56:04.200]   They want to believe.
[01:56:04.200 --> 01:56:04.680]   You know?
[01:56:04.680 --> 01:56:05.160]   And they want to believe...
[01:56:05.160 --> 01:56:05.880]   Sometimes like...
[01:56:05.880 --> 01:56:11.240]   Like, if anybody's out there interested in behavioral economics is a wonderful case study,
[01:56:11.240 --> 01:56:16.360]   because I think that desire to believe and to just then you start looking for information
[01:56:16.360 --> 01:56:18.360]   that reaffirms your existing beliefs, you know?
[01:56:18.360 --> 01:56:19.480]   Then you just get stuck.
[01:56:19.480 --> 01:56:20.840]   You get stuck.
[01:56:20.840 --> 01:56:22.120]   The fact that finance...
[01:56:22.120 --> 01:56:25.800]   Like, there's a whole influencer category that is just finance bros.
[01:56:25.800 --> 01:56:27.080]   Who would have...
[01:56:27.080 --> 01:56:28.200]   You know, that's crazy.
[01:56:28.200 --> 01:56:30.600]   You stop and think about it.
[01:56:30.600 --> 01:56:30.840]   Yeah.
[01:56:30.840 --> 01:56:36.120]   By the way, my lack of brush with greatness, my mom and birdie made off
[01:56:36.120 --> 01:56:38.920]   were high school class.
[01:56:38.920 --> 01:56:39.640]   Did they date?
[01:56:40.440 --> 01:56:41.400]   No, they did not.
[01:56:41.400 --> 01:56:46.360]   But I posted a photo, because when I found out that my mom was in the graduating class of
[01:56:46.360 --> 01:56:49.880]   birdie made off, I went to go find her your book and scan birdie made offs photo.
[01:56:49.880 --> 01:56:52.200]   And I posted to the chat here.
[01:56:52.200 --> 01:56:56.200]   He was a varsity swimming team locker guard.
[01:56:56.200 --> 01:56:57.400]   There's a job.
[01:56:57.400 --> 01:57:01.960]   He went to Alabama, I guess.
[01:57:01.960 --> 01:57:03.080]   Bernard Madoff.
[01:57:03.080 --> 01:57:03.480]   Wow.
[01:57:03.480 --> 01:57:06.040]   Good looking young man.
[01:57:06.040 --> 01:57:08.200]   You know, there've been great movies made about him.
[01:57:08.200 --> 01:57:14.840]   There've been great TV shows made about WeWork called We Crashed about Uber.
[01:57:14.840 --> 01:57:16.200]   Two about the fire festival.
[01:57:16.200 --> 01:57:17.560]   Two about the fire festival.
[01:57:17.560 --> 01:57:20.200]   Of course, Elizabeth Holmes, a dropout.
[01:57:20.200 --> 01:57:22.200]   I can't wait for this.
[01:57:22.200 --> 01:57:23.960]   And I'm sure it's being optioned already.
[01:57:23.960 --> 01:57:27.480]   The Samuel Bank and Freed story.
[01:57:27.480 --> 01:57:30.360]   Wow.
[01:57:30.360 --> 01:57:31.560]   So...
[01:57:31.560 --> 01:57:34.360]   Of course, Elizabeth would like to leave jail now,
[01:57:34.360 --> 01:57:35.880]   because she thinks she's suffered enough.
[01:57:35.880 --> 01:57:42.840]   Yeah, she the judge, by the way, rejected her appeal and she will be sentenced soon.
[01:57:42.840 --> 01:57:46.520]   The feds are asking for, and I almost feel like this is too much,
[01:57:46.520 --> 01:57:51.080]   15 years for Elizabeth Holmes for Theranos.
[01:57:51.080 --> 01:57:52.600]   Does that seem like kind of...
[01:57:52.600 --> 01:57:54.680]   How long did Sonny, whatever's last time...
[01:57:54.680 --> 01:57:57.000]   Sonny Balwani hasn't been sentenced yet.
[01:57:57.000 --> 01:57:58.520]   He will be sentenced after her.
[01:57:58.520 --> 01:58:02.840]   Leo, it's one thing to steal $100 million from the people.
[01:58:02.840 --> 01:58:07.160]   It's another thing to steal $100 million from one of the wealthiest people.
[01:58:07.160 --> 01:58:08.360]   Oh, yeah, there you go.
[01:58:08.360 --> 01:58:09.480]   That's what she did wrong.
[01:58:09.480 --> 01:58:12.200]   That's what she did wrong.
[01:58:12.200 --> 01:58:13.560]   Got the DeVos family for $100 million.
[01:58:13.560 --> 01:58:13.960]   Right.
[01:58:13.960 --> 01:58:17.880]   So, again, I come back to Elon, and is he...
[01:58:17.880 --> 01:58:23.400]   I don't know, is there some kind of defrauding that's happening?
[01:58:23.400 --> 01:58:25.640]   Or is just...
[01:58:25.640 --> 01:58:29.160]   You don't have any bear any real responsibility if you're just causing like a giant cluster?
[01:58:29.160 --> 01:58:30.440]   Well, I think that's why...
[01:58:30.440 --> 01:58:33.000]   Look, I don't think Elizabeth Holmes should get 15 years.
[01:58:33.000 --> 01:58:41.400]   It's 15 years and $800 million fine, but she should get some serious punishment
[01:58:41.400 --> 01:58:47.400]   as a warning to people that you don't get off Scott Free just because you're wealthy.
[01:58:47.400 --> 01:58:52.840]   Although, really, you're right, Father Robert, the warning is don't rip off rich people.
[01:58:52.840 --> 01:58:55.000]   Don't rip off the powerful.
[01:58:55.000 --> 01:58:56.120]   Rip off the little people.
[01:58:56.120 --> 01:58:59.240]   Bernie Madoff went to jail.
[01:59:00.040 --> 01:59:01.000]   Died in jail.
[01:59:01.000 --> 01:59:05.000]   All right, let's take...
[01:59:05.000 --> 01:59:06.840]   We keep lionizing and demonizing.
[01:59:06.840 --> 01:59:08.040]   That's the problem, right?
[01:59:08.040 --> 01:59:11.000]   We sort of how this to happen, and then we just get back, and a couple of people get punished,
[01:59:11.000 --> 01:59:12.760]   and then we just kind of go back to the way things were.
[01:59:12.760 --> 01:59:13.160]   We love him.
[01:59:13.160 --> 01:59:16.920]   Elon, for years we've known he was the pump and dump king.
[01:59:16.920 --> 01:59:19.320]   What? He pumped Dogecoin, right?
[01:59:19.320 --> 01:59:20.360]   Dogecoin, yeah.
[01:59:20.360 --> 01:59:25.080]   He's the reason why he was able to buy all those other cryptocurrencies,
[01:59:25.080 --> 01:59:32.040]   because he pumped Dogecoin so much that the previous batch of Dogecoin that I had that,
[01:59:32.040 --> 01:59:37.480]   I think, was worth a total of $5 was suddenly worth $15,000.
[01:59:37.480 --> 01:59:40.040]   That's ridiculous. That should never happen.
[01:59:40.040 --> 01:59:42.200]   And I benefited from it.
[01:59:42.200 --> 01:59:42.440]   Right.
[01:59:42.440 --> 01:59:46.120]   The church doesn't just take that when you make that money.
[01:59:46.120 --> 01:59:47.480]   They don't say thank you.
[01:59:47.480 --> 01:59:50.600]   It's never been transferred into dollars.
[01:59:50.600 --> 01:59:52.120]   It's always going from one point to one point.
[01:59:52.120 --> 01:59:53.800]   Yeah, don't tell, don't worry.
[01:59:54.440 --> 01:59:54.920]   Yeah.
[01:59:54.920 --> 01:59:57.720]   It's just bits.
[01:59:57.720 --> 01:59:59.400]   It's just bits.
[01:59:59.400 --> 02:00:00.360]   It's all it is.
[02:00:00.360 --> 02:00:03.960]   But for Elon, the pump and dump stuff,
[02:00:03.960 --> 02:00:09.720]   it would be really hard to prove that he has benefited from it.
[02:00:09.720 --> 02:00:11.000]   Others have benefited from it.
[02:00:11.000 --> 02:00:14.680]   I don't know if you could say that he has lost the threshold.
[02:00:14.680 --> 02:00:15.160]   Right.
[02:00:15.160 --> 02:00:15.560]   Correct.
[02:00:15.560 --> 02:00:16.040]   Right.
[02:00:16.040 --> 02:00:17.880]   Now he's done a lot of really other shady things.
[02:00:17.880 --> 02:00:19.720]   Like, for example, just was it yesterday?
[02:00:19.720 --> 02:00:23.000]   They found out that he had a 10-year,
[02:00:23.000 --> 02:00:25.320]   the director of the car division of Tesla,
[02:00:25.320 --> 02:00:29.240]   had he had been getting paid in stock options.
[02:00:29.240 --> 02:00:36.600]   And Elon Musk had asked him to forfeit $600 million worth of those stock options.
[02:00:36.600 --> 02:00:40.920]   That's one of the ways that that Elon continues to appear successful.
[02:00:40.920 --> 02:00:44.120]   He just takes away money from the employees that they were supposed to get.
[02:00:44.120 --> 02:00:45.800]   He did the same thing when he went to Twitter.
[02:00:45.800 --> 02:00:50.520]   He's going to go to this whole court case with the top executives because he says they were
[02:00:50.520 --> 02:00:51.640]   released for cause.
[02:00:51.640 --> 02:00:56.440]   Of course, that's ridiculous, but it will run through long enough for him to be able to
[02:00:56.440 --> 02:00:59.000]   report it on his balance sheet that that was a successful sale.
[02:00:59.000 --> 02:00:59.480]   Wow.
[02:00:59.480 --> 02:01:01.800]   It's so is it illegal?
[02:01:01.800 --> 02:01:03.800]   Probably not.
[02:01:03.800 --> 02:01:04.680]   Not criminally.
[02:01:04.680 --> 02:01:06.520]   Is it immoral?
[02:01:06.520 --> 02:01:07.400]   Absolutely.
[02:01:07.400 --> 02:01:09.800]   But his stands don't care about that.
[02:01:09.800 --> 02:01:11.240]   Wow.
[02:01:14.920 --> 02:01:18.760]   Do you think that we don't do popularity polls on
[02:01:18.760 --> 02:01:20.520]   moguls, but we should.
[02:01:20.520 --> 02:01:23.400]   I wonder what Twitter's approval rating is.
[02:01:23.400 --> 02:01:25.480]   I mean, Elon's approval rating is right now.
[02:01:25.480 --> 02:01:31.560]   I think the biggest benefit to the acquisition of Twitter by Elon Musk is Jeff Bezos,
[02:01:31.560 --> 02:01:34.680]   because suddenly he doesn't look like a bond villain anymore.
[02:01:34.680 --> 02:01:34.840]   Yeah.
[02:01:34.840 --> 02:01:37.800]   I think Elon has taken that position.
[02:01:37.800 --> 02:01:40.040]   Amy Webb is here.
[02:01:40.040 --> 02:01:43.320]   She is the author of a fabulous book.
[02:01:43.320 --> 02:01:47.160]   I was just reminded in the chat, The Genesis Machine.
[02:01:47.160 --> 02:01:50.680]   I love this book and you should absolutely read it.
[02:01:50.680 --> 02:01:56.040]   It's all about biotech and how it will change the world around us.
[02:01:56.040 --> 02:01:59.000]   It's a fascinating read.
[02:01:59.000 --> 02:02:07.000]   Our quest to rewrite life in the age of synthetic biology available on Amazon.
[02:02:07.000 --> 02:02:08.600]   There's an audible version of it.
[02:02:08.600 --> 02:02:11.000]   You can get it on your Kindle as well.
[02:02:11.960 --> 02:02:12.440]   Fantastic.
[02:02:12.440 --> 02:02:16.760]   He was just named to the New Yorkers Best Nonfiction Books of the Year.
[02:02:16.760 --> 02:02:18.360]   Congratulations.
[02:02:18.360 --> 02:02:19.720]   Thanks.
[02:02:19.720 --> 02:02:20.920]   That's really great.
[02:02:20.920 --> 02:02:22.120]   Thank you.
[02:02:22.120 --> 02:02:22.600]   They're right.
[02:02:22.600 --> 02:02:27.000]   It's one of those books I think it's important to read to kind of know about
[02:02:27.000 --> 02:02:31.480]   what's happening and what might happen in the near future.
[02:02:31.480 --> 02:02:32.840]   I mean, we're really headed into this.
[02:02:32.840 --> 02:02:35.000]   You know, we've been in the information age.
[02:02:35.000 --> 02:02:36.440]   I think we're indefinitely in the.
[02:02:37.640 --> 02:02:42.040]   The synthetic age of synthetic biology is coming rapidly.
[02:02:42.040 --> 02:02:42.600]   Yep.
[02:02:42.600 --> 02:02:45.480]   I really do think that's the next general purpose technology,
[02:02:45.480 --> 02:02:51.000]   which again is like, it's a technology that at some point fundamentally shifts
[02:02:51.000 --> 02:02:53.960]   societies, economies, geopolitics.
[02:02:53.960 --> 02:02:57.960]   So steam engine, internet, and at some point it gets so big,
[02:02:57.960 --> 02:03:00.920]   the only way to calculate the total value is to do it in reverse.
[02:03:00.920 --> 02:03:02.360]   You know, what if we took the internet away?
[02:03:02.360 --> 02:03:06.280]   I really do think that's that's engineering biology.
[02:03:06.280 --> 02:03:07.880]   Don't take the internet away, please.
[02:03:07.880 --> 02:03:10.120]   Please don't do that.
[02:03:10.120 --> 02:03:16.040]   Your timing was great, of course, because we all had a direct experience of synthetic biology
[02:03:16.040 --> 02:03:17.640]   with the mRNA vaccines.
[02:03:17.640 --> 02:03:22.040]   And what I've had five shots now.
[02:03:22.040 --> 02:03:24.280]   It's for you.
[02:03:24.280 --> 02:03:25.320]   It's kind of crazy.
[02:03:25.320 --> 02:03:26.600]   Yeah, it's a lot.
[02:03:26.600 --> 02:03:29.240]   Did you get all the same flavor?
[02:03:29.240 --> 02:03:29.640]   Did you get?
[02:03:29.640 --> 02:03:31.080]   Did you have a new flavor?
[02:03:31.080 --> 02:03:33.480]   I've been doing vanilla and I said to go chocolate.
[02:03:33.480 --> 02:03:37.880]   I had four modernas and you can't for some reason, I don't know why I couldn't get them
[02:03:37.880 --> 02:03:38.280]   to do it.
[02:03:38.280 --> 02:03:40.920]   They're like, I don't know why they just didn't have any.
[02:03:40.920 --> 02:03:42.520]   They've been out for months.
[02:03:42.520 --> 02:03:43.720]   So they said, we'll get the Pfizer.
[02:03:43.720 --> 02:03:44.680]   So I got the Pfizer.
[02:03:44.680 --> 02:03:47.160]   Apparently that was a good choice to mix it up.
[02:03:47.160 --> 02:03:48.120]   So throw me the book.
[02:03:48.120 --> 02:03:54.440]   John went out and got the book, which I have I have left here so I could prove I own it.
[02:03:54.440 --> 02:03:56.840]   Thank you.
[02:03:56.840 --> 02:03:58.680]   Really, really great stuff.
[02:03:58.680 --> 02:04:03.080]   You do talk about the mRNA vaccines, lab-grown hamburger.
[02:04:04.040 --> 02:04:08.840]   And you talk about risks, but you also have it's one of my favorite parts of the book.
[02:04:08.840 --> 02:04:15.400]   Scenarios, kind of future looking scenarios about what might happen.
[02:04:15.400 --> 02:04:17.480]   I just got a wonderful compliment.
[02:04:17.480 --> 02:04:19.720]   It's my version of a compliment.
[02:04:19.720 --> 02:04:27.400]   So I was at I was doing something with DoD last week and I had a two-star come up to me.
[02:04:27.400 --> 02:04:28.680]   I had somebody come up to me and say,
[02:04:30.360 --> 02:04:35.640]   normally their job is to tell others scenarios for the future.
[02:04:35.640 --> 02:04:39.080]   And usually the other people say, I don't want to go to sleep now.
[02:04:39.080 --> 02:04:39.960]   That's really terrifying.
[02:04:39.960 --> 02:04:43.160]   And the person who's in charge of doing that said that I just did that to him.
[02:04:43.160 --> 02:04:46.440]   And he feels like he's not going to be able to go to sleep.
[02:04:46.440 --> 02:04:51.800]   So it's nice to know that my waking nightmares put enough fear into the people.
[02:04:51.800 --> 02:04:52.920]   One, it should have accomplished.
[02:04:52.920 --> 02:04:54.440]   Yeah, it's a mission accomplished.
[02:04:54.440 --> 02:04:55.480]   Mission accomplished.
[02:04:55.480 --> 02:04:57.000]   It's not all bad, right?
[02:04:57.000 --> 02:05:00.280]   No, I listen, again, I well, first of all,
[02:05:00.280 --> 02:05:01.160]   I think it's inevitable.
[02:05:01.160 --> 02:05:02.680]   So it's good or bad.
[02:05:02.680 --> 02:05:03.240]   It's a matter, right?
[02:05:03.240 --> 02:05:03.640]   Right.
[02:05:03.640 --> 02:05:07.720]   I'm, I think the risks for me have more to do with
[02:05:07.720 --> 02:05:15.560]   bio-weapons, which I don't want to see, and not having equal distribution.
[02:05:15.560 --> 02:05:20.040]   I don't think it was good that big parts of the world didn't have access to any vaccines
[02:05:20.040 --> 02:05:21.720]   because of money.
[02:05:21.720 --> 02:05:22.920]   I thought that was ridiculous.
[02:05:22.920 --> 02:05:26.520]   So I think there's significant risk.
[02:05:26.520 --> 02:05:28.840]   I'm very worried about what China is doing.
[02:05:28.840 --> 02:05:30.600]   But I also think there's huge opportunity.
[02:05:30.600 --> 02:05:31.160]   I really do.
[02:05:31.160 --> 02:05:33.800]   I mean, I think it's probably our best at this point,
[02:05:33.800 --> 02:05:36.200]   best hope for climate change mediation.
[02:05:36.200 --> 02:05:38.120]   So yeah.
[02:05:38.120 --> 02:05:41.320]   Yeah.
[02:05:41.320 --> 02:05:42.760]   Anyways, it's a-
[02:05:42.760 --> 02:05:43.800]   I started reading the book.
[02:05:43.800 --> 02:05:44.200]   I'm sorry.
[02:05:44.200 --> 02:05:48.280]   It's a book about biology, but it's not like a super nerdy science book.
[02:05:48.280 --> 02:05:48.920]   No, it's great.
[02:05:48.920 --> 02:05:49.800]   It's one book about the future.
[02:05:49.800 --> 02:05:53.480]   I just started reading about collecting DNA off of President Trump's fork.
[02:05:53.480 --> 02:05:57.080]   And I now I've got to put that bad dreams back.
[02:05:57.080 --> 02:05:58.920]   Yeah, I was there when that happened actually.
[02:05:58.920 --> 02:06:00.440]   Yeah, at Davos, right?
[02:06:00.440 --> 02:06:00.920]   Oh, no.
[02:06:00.920 --> 02:06:01.400]   Yeah.
[02:06:01.400 --> 02:06:01.640]   Yeah.
[02:06:01.640 --> 02:06:08.840]   They collected DNA samples from the discarded utensils of world leaders,
[02:06:08.840 --> 02:06:11.480]   the earnest project.
[02:06:11.480 --> 02:06:12.040]   Crazy.
[02:06:12.040 --> 02:06:13.400]   You got to read the book.
[02:06:13.400 --> 02:06:18.600]   The Genesis machine, our quest to rewrite life in the age of synthetic biology.
[02:06:18.600 --> 02:06:19.480]   Let's take a little break.
[02:06:19.480 --> 02:06:23.480]   David Sparks also here, Father Robert Ballisaire.
[02:06:23.480 --> 02:06:26.360]   Always fun to have these three in the room.
[02:06:27.160 --> 02:06:28.520]   I'm just going to sit back and listen.
[02:06:28.520 --> 02:06:29.880]   It's fantastic.
[02:06:29.880 --> 02:06:31.720]   Our show today brought to you by Mint Mobile.
[02:06:31.720 --> 02:06:32.520]   This is now you.
[02:06:32.520 --> 02:06:36.280]   There are the big cell phone companies,
[02:06:36.280 --> 02:06:37.880]   which cost a lot of money.
[02:06:37.880 --> 02:06:41.800]   Just if you're an AT&T, T-Mobile, Verizon,
[02:06:41.800 --> 02:06:46.920]   you're probably paying, you know, 80 to $100 online.
[02:06:46.920 --> 02:06:48.040]   You're spending a lot of money.
[02:06:48.040 --> 02:06:50.120]   I look at sometimes I look at that cell bill and I go,
[02:06:50.120 --> 02:06:53.480]   wow, wow, I can't believe I'm paying that much.
[02:06:53.480 --> 02:06:55.640]   And it's going up and up and up, hasn't it?
[02:06:55.640 --> 02:06:56.920]   Well, there's a better way.
[02:06:56.920 --> 02:06:59.640]   There's something called Mint Mobile.
[02:06:59.640 --> 02:07:01.880]   And I talk to more and more people all the time who say,
[02:07:01.880 --> 02:07:03.960]   Leo, I did it and that's incredible.
[02:07:03.960 --> 02:07:08.520]   Premium wireless starting at $15 a month.
[02:07:08.520 --> 02:07:12.040]   And there's no catch.
[02:07:12.040 --> 02:07:13.240]   There's no plot twist.
[02:07:13.240 --> 02:07:15.880]   This is actually the price.
[02:07:15.880 --> 02:07:20.280]   Premium wireless from Mint Mobile, $15 a month,
[02:07:20.280 --> 02:07:25.160]   no two-year contract, no crazy fees when you open your bill,
[02:07:25.640 --> 02:07:30.280]   no, they don't lure you in with free subscriptions to streaming services
[02:07:30.280 --> 02:07:33.000]   that you'll forget to cancel and they get charged a full price for.
[02:07:33.000 --> 02:07:33.880]   They don't do any of that.
[02:07:33.880 --> 02:07:37.400]   In fact, the way they say money is by not doing that,
[02:07:37.400 --> 02:07:40.840]   by not even having stores, by doing it all online,
[02:07:40.840 --> 02:07:43.000]   and it is amazing.
[02:07:43.000 --> 02:07:45.880]   You're going to get unlimited talk and text,
[02:07:45.880 --> 02:07:49.800]   high-speed data on the nation's largest 5G network.
[02:07:50.840 --> 02:07:55.240]   $15 a month gets you 4GB a month of data plus unlimited talk and text.
[02:07:55.240 --> 02:07:57.800]   You can get more, 20 bucks a month, 10GB.
[02:07:57.800 --> 02:07:59.480]   That's more than, I think, almost anybody.
[02:07:59.480 --> 02:08:00.920]   I went crazy.
[02:08:00.920 --> 02:08:04.280]   I got the $25 a month.
[02:08:04.280 --> 02:08:05.560]   There's even an unlimited plan.
[02:08:05.560 --> 02:08:08.680]   It's still only $30 a month.
[02:08:08.680 --> 02:08:15.320]   Unlimited talk and text nationwide, high-speed data.
[02:08:15.320 --> 02:08:16.600]   It's on the T-Mobile network.
[02:08:16.600 --> 02:08:19.080]   I'll tell you the secret there, a T-Mobile Envino.
[02:08:19.080 --> 02:08:20.120]   It's amazing.
[02:08:20.120 --> 02:08:21.240]   You can bring your own phone.
[02:08:21.240 --> 02:08:23.640]   They will send you a SIM card for free.
[02:08:23.640 --> 02:08:25.320]   And by the way, they do e-SIM.
[02:08:25.320 --> 02:08:28.600]   So if you've got the new iPhone, they do e-SIM as well,
[02:08:28.600 --> 02:08:29.800]   which is great.
[02:08:29.800 --> 02:08:31.640]   Makes it very easy to switch over.
[02:08:31.640 --> 02:08:34.600]   You can port your number over, keep the same phone number.
[02:08:34.600 --> 02:08:35.640]   They sell phones too.
[02:08:35.640 --> 02:08:40.280]   So in fact, I bought an iPhone from them, iPhone SE for $15 a month.
[02:08:40.280 --> 02:08:42.360]   Such a great deal.
[02:08:42.360 --> 02:08:44.840]   Switch to Mint Mobile, get premium wireless service,
[02:08:44.840 --> 02:08:46.280]   starts at $15 a month.
[02:08:46.280 --> 02:08:50.040]   No hidden fees, no contract, no extras, just simple.
[02:08:50.360 --> 02:08:51.000]   Just simple.
[02:08:51.000 --> 02:08:53.000]   $15 a month.
[02:08:53.000 --> 02:08:54.760]   There are no unexpected plot twists.
[02:08:54.760 --> 02:08:56.600]   Mint Mobile.com/Twit.
[02:08:56.600 --> 02:08:58.760]   Take advantage of this.
[02:08:58.760 --> 02:09:04.520]   Everybody I know is switching because it just makes no sense to pay five times more.
[02:09:04.520 --> 02:09:06.920]   Mint Mobile.com/Twit.
[02:09:06.920 --> 02:09:08.680]   You'll make your wallet very happy.
[02:09:08.680 --> 02:09:10.200]   Mint Mobile.com/Twit.
[02:09:10.200 --> 02:09:13.960]   Thank you, Mint Mobile, for your support for the show.
[02:09:13.960 --> 02:09:15.640]   We are going long.
[02:09:15.640 --> 02:09:18.760]   I know, I always can tell, the shows goes long
[02:09:18.760 --> 02:09:20.360]   and we have a great panel here.
[02:09:20.360 --> 02:09:22.600]   Let's do a few other things.
[02:09:22.600 --> 02:09:24.440]   A bit of trouble for Apple.
[02:09:24.440 --> 02:09:28.440]   Gizmodo published a story
[02:09:28.440 --> 02:09:37.640]   that really, I think, stuck it to Apple a little bit,
[02:09:37.640 --> 02:09:41.720]   basically saying that it's lip service,
[02:09:41.720 --> 02:09:44.680]   that Apple's paying lip service to privacy.
[02:09:45.720 --> 02:09:51.640]   And they did a bunch of research.
[02:09:51.640 --> 02:09:56.520]   They found that the iPhone data is collected even when Apple says,
[02:09:56.520 --> 02:09:57.960]   "We're not going to collect data."
[02:09:57.960 --> 02:10:02.520]   They tested multiple iPhones.
[02:10:02.520 --> 02:10:08.360]   Apple analytics data, regardless of your settings, sent to Apple.
[02:10:08.360 --> 02:10:13.880]   As a result, there's a class action lawsuit filed immediately after the article came out
[02:10:13.880 --> 02:10:21.240]   in California federal court, claiming that the iPhone is violating privacy rules.
[02:10:21.240 --> 02:10:23.960]   The problem was spotted by two independent researchers,
[02:10:23.960 --> 02:10:27.480]   according to Gizmodo at a software company called MISC, M-Y-S-K.
[02:10:27.480 --> 02:10:31.800]   They found the Apple App Store sends the company exhaustive information
[02:10:31.800 --> 02:10:34.280]   about everything you do in the app.
[02:10:34.280 --> 02:10:36.280]   Now, you have to be in the App Store.
[02:10:36.280 --> 02:10:41.080]   Despite a privacy setting, iPhone analytics, which claims to, quote, "disabled"
[02:10:41.080 --> 02:10:43.320]   the sharing of device analytics altogether,
[02:10:43.320 --> 02:10:48.360]   when switched off, Gizmodo asked the researchers to run additional tests on other iPhone apps.
[02:10:48.360 --> 02:10:50.840]   Apple music does the same thing.
[02:10:50.840 --> 02:10:53.640]   Apple TV, Apple Books, Apple Stocks.
[02:10:53.640 --> 02:10:57.480]   Most of Apple's suite of built-in iPhone apps track you,
[02:10:57.480 --> 02:11:01.000]   violating the California invasion of Privacy Act.
[02:11:01.000 --> 02:11:09.480]   The plaintiff said in the suit, "Apple's privacy guarantees are completely illusory."
[02:11:10.840 --> 02:11:12.440]   Apple has not responded.
[02:11:12.440 --> 02:11:14.280]   Thoughts?
[02:11:14.280 --> 02:11:22.200]   Well, I was going to get an iPhone for the first time in many, many years, and the reason was privacy.
[02:11:22.200 --> 02:11:25.080]   That has been their differentiator.
[02:11:25.080 --> 02:11:25.800]   I know.
[02:11:25.800 --> 02:11:30.360]   They've in fact put billboards on this, and this has been like a hot topic we've discussed,
[02:11:30.360 --> 02:11:35.720]   is how security and privacy actually can be a product differentiator to push people to want
[02:11:35.720 --> 02:11:41.000]   to buy. This is flying in the face of what they are intending to sell.
[02:11:41.000 --> 02:11:43.880]   This is my first of hearing it right now.
[02:11:43.880 --> 02:11:51.560]   They could just come out and go, "Oh, we screwed up big time. This is how we're fixing it,
[02:11:51.560 --> 02:11:54.040]   but it'll be interesting to see how it's handled."
[02:11:54.040 --> 02:11:56.840]   Researchers looked at health and wallet.
[02:11:56.840 --> 02:12:02.200]   They did not collect analytics data, but the Stocks app sends your list of watch stocks,
[02:12:02.200 --> 02:12:06.760]   the names of stocks you viewed or searched for, time stamps for when you did it,
[02:12:06.760 --> 02:12:10.200]   as well as records of any news articles you saw in the app.
[02:12:10.200 --> 02:12:11.880]   What? Okay.
[02:12:11.880 --> 02:12:18.120]   Since the app is connected to a server that has those settings,
[02:12:18.120 --> 02:12:19.640]   those news articles come from...
[02:12:19.640 --> 02:12:24.440]   All of that stuff would be collected anyway in the nature of the business of the app,
[02:12:24.440 --> 02:12:25.000]   right?
[02:12:25.000 --> 02:12:29.880]   I'm not trying to let Apple off the hook, but sometimes you see these things,
[02:12:29.880 --> 02:12:32.120]   and it's like, "Oh my God, they know what stocks you picked."
[02:12:32.120 --> 02:12:38.680]   Well, yeah, because that's how it works so that when you choose stocks on the Mac,
[02:12:38.680 --> 02:12:42.520]   it propagates the phone and vice versa. Apple does have to keep track of that.
[02:12:42.520 --> 02:12:46.280]   That's part of the functionality of the app.
[02:12:46.280 --> 02:12:46.920]   That's the functionality of the app.
[02:12:46.920 --> 02:12:51.720]   You can't actually search for an app if the app doesn't know what apps are.
[02:12:51.720 --> 02:12:55.720]   You can't search for an app if the app store doesn't know what apps are compatible with your
[02:12:55.720 --> 02:12:56.120]   phone.
[02:12:56.120 --> 02:12:56.520]   Right.
[02:12:56.520 --> 02:13:01.400]   So it needs that data to be able to offer you the selection that will work with your device.
[02:13:01.400 --> 02:13:10.040]   And I get that, but the problem is, as both David and Amy have already stated,
[02:13:10.040 --> 02:13:12.680]   they sold this as a product feature.
[02:13:12.680 --> 02:13:19.400]   They told people in their commercials and in all of their press events that we will not track
[02:13:19.400 --> 02:13:21.400]   you at all. We give you that power.
[02:13:21.400 --> 02:13:26.040]   Now, if they had told you that, "Oh, except in the app store," then everything would be fine.
[02:13:26.040 --> 02:13:29.480]   Like I use an Android phone. I know I'm being tracked six ways to Tuesday.
[02:13:29.480 --> 02:13:29.800]   Yeah.
[02:13:29.800 --> 02:13:31.480]   I mean, I expect no privacy.
[02:13:31.480 --> 02:13:32.040]   It's a given.
[02:13:32.040 --> 02:13:37.640]   But if I went with an iPhone, that would be the reason why I did. And to now know that,
[02:13:37.640 --> 02:13:41.640]   "Oh, this was just another PR puff piece to try to get me to buy them."
[02:13:41.640 --> 02:13:44.280]   They'll then, "No, you've just broken my trust again."
[02:13:44.280 --> 02:13:50.200]   But Keith, honest to God, what amazes me most about all of this is how we care about it now.
[02:13:50.200 --> 02:13:55.800]   I'll just say, backtrack, 12 years ago, I was in an event of which I heard Eric Schmidt
[02:13:55.800 --> 02:14:02.680]   of Google saying, "We can track with very high probability where you will go next."
[02:14:02.680 --> 02:14:05.880]   And nobody blinked like, "Oh my God."
[02:14:05.880 --> 02:14:08.200]   Like nobody could. Everyone's like, "Oh, that's quite fascinating."
[02:14:08.200 --> 02:14:13.960]   Like no one cared about privacy 12 years ago. They simply did not.
[02:14:13.960 --> 02:14:18.600]   Today, they do. You could not say that without tomatoes being thrown at you.
[02:14:18.600 --> 02:14:18.840]   Right.
[02:14:21.320 --> 02:14:26.920]   I don't, yeah. And as my friend, Jeff Jarvis would remind me, I don't want to create techno panic
[02:14:26.920 --> 02:14:30.280]   because some of this is just the operation of the app itself.
[02:14:30.280 --> 02:14:35.480]   And also, an EFF is pointing this out as well. A lot of times,
[02:14:35.480 --> 02:14:41.240]   the thing is, "Well, they sell that information to third party." No, neither Google nor Facebook nor
[02:14:41.240 --> 02:14:46.360]   Apple tells third parties about you. They don't want to. They keep that to themselves so they can
[02:14:46.360 --> 02:14:51.080]   sell ads against it. It's not in their interest to give that information away.
[02:14:51.080 --> 02:14:56.040]   And so, in most cases, first party information is held by the first party, but
[02:14:56.040 --> 02:15:03.480]   Apple's perfectly happy to say, "No third party can let that information," and kind of pretend
[02:15:03.480 --> 02:15:07.880]   that they're not collecting the information when they certainly could if they wanted to.
[02:15:07.880 --> 02:15:12.840]   And now that they're putting ads in many places, including, by the way, Apple Maps,
[02:15:12.840 --> 02:15:18.840]   there is certainly an incentive for them to start collecting that data and use it to sell those ads.
[02:15:18.840 --> 02:15:24.040]   Right? I really want to see how they're going to respond.
[02:15:24.040 --> 02:15:29.320]   How they respond is going to. They can either say, "Look, this is not an issue,
[02:15:29.320 --> 02:15:34.440]   that they got it wrong," or they could say, "Ah, we overlooked this. We'll upgrade the privacy
[02:15:34.440 --> 02:15:39.560]   policy, but we do need to have this information." And that's an extremely good point, Patrick,
[02:15:39.560 --> 02:15:45.960]   because again, we bring this up all the time. Being breached, having a privacy issue or something,
[02:15:45.960 --> 02:15:50.040]   that is not the issue, because now it's literally happening to everybody.
[02:15:50.040 --> 02:15:56.760]   What the issue is, how you deal with it in a very public manner, how you address it and manage it.
[02:15:56.760 --> 02:16:06.200]   And also, these moments really can present staff and companies an opportunity to shine.
[02:16:06.200 --> 02:16:11.320]   This could actually turn out good for Apple if they respond extremely well.
[02:16:13.800 --> 02:16:18.680]   I do want to throw out one other comment here about people making comments of, "I don't care.
[02:16:18.680 --> 02:16:23.720]   Track me." I've heard this so many times. I just want to stress to people who say that,
[02:16:23.720 --> 02:16:33.640]   "Yes, you do need to care," because the way to hack, the way to get to you is to use your
[02:16:33.640 --> 02:16:39.320]   information and to use the information that others have on you, so your relatives, too,
[02:16:39.320 --> 02:16:46.200]   and your friends. And so you do actually need to care about this, even if you, quote, "have
[02:16:46.200 --> 02:16:53.320]   nothing to hide." Can I offer one additional dynamic to what David just said?
[02:16:53.320 --> 02:17:00.840]   We were in Ireland on a -- we rented a car -- this pre-COVID. We're driving around.
[02:17:00.840 --> 02:17:07.800]   We were at the southern tip. Brian decided to take a shortcut to get to the ferry, and
[02:17:07.800 --> 02:17:12.920]   was my daughter, and he and I. We were all very excited to be on a ferry. We'd never been on a
[02:17:12.920 --> 02:17:16.520]   car on a boat, so this was a very exciting moment, but we needed to make sure -- It's the
[02:17:16.520 --> 02:17:19.800]   little thing, isn't it? It's the little things that make such a difference. And so, anyhow,
[02:17:19.800 --> 02:17:28.280]   he says to me, "Drop a pin." And I say to him, "I can't." And he's yelling it. He's very
[02:17:28.280 --> 02:17:33.800]   frustrated. This is how he's trying to explain to me how to do it, right? You're on an android.
[02:17:33.800 --> 02:17:37.960]   You don't know. And I'm like, "Nope, I'm just telling you I can't do it." He starts losing his
[02:17:37.960 --> 02:17:43.160]   mind. And he never gets mad. He was so frustrated. He wanted to get the car on the boat so we can
[02:17:43.160 --> 02:17:47.000]   have the whole experience. And finally, I was like, "I don't have any location turned on."
[02:17:47.000 --> 02:17:53.640]   He was like, "What the hell is wrong?" If you were overseas, you're supposed to be navigating.
[02:17:53.640 --> 02:17:57.160]   I'm like, "Yeah, I'm following a static map, you know? Like, I know where we're going. I just
[02:17:57.160 --> 02:18:01.000]   can't drop a pin. I'll remember where we are." Well, he's -- Almost caused a divorce.
[02:18:02.440 --> 02:18:07.720]   But the other issue is people who post photos while they're on vacation or post-announced
[02:18:07.720 --> 02:18:11.400]   from the gun, there was -- I can't remember the name of the website again. I'm hoping the chat
[02:18:11.400 --> 02:18:16.840]   room comes through here. It was for people who posted on Twitter, like, "I'm on vacation here.
[02:18:16.840 --> 02:18:22.680]   I'm doing this." It was entitled, "Come rob me." You can't break it to myhouse.com.
[02:18:22.680 --> 02:18:28.280]   Advertising that fact. Because you are literally advertising, "I'm not home."
[02:18:28.280 --> 02:18:31.560]   Well, there's a business model that does work for some people.
[02:18:32.360 --> 02:18:37.160]   But one day, I'm going to show you my location data out of Google history.
[02:18:37.160 --> 02:18:42.360]   Because since I fuzz my data, Google constantly thinks that I'm in about 15 different places.
[02:18:42.360 --> 02:18:43.720]   How do you do that, Padre?
[02:18:43.720 --> 02:18:49.160]   I have dummy devices in many of the data closets that are doing this really hard.
[02:18:49.160 --> 02:18:49.320]   It works really hard.
[02:18:49.320 --> 02:18:49.960]   Around the world.
[02:18:49.960 --> 02:18:50.680]   Yeah.
[02:18:50.680 --> 02:18:57.320]   It does mean that my advertising is never accurate. I'm always getting pitched stuff from cities
[02:18:57.320 --> 02:19:00.040]   I'm not in or in languages I don't speak.
[02:19:00.040 --> 02:19:00.440]   That's fine.
[02:19:00.440 --> 02:19:01.000]   Here is the --
[02:19:01.000 --> 02:19:02.840]   By the way, it's pleaserobmy.com.
[02:19:02.840 --> 02:19:07.000]   Yeah, I was just going to say, Skitter X found the site. Please rob me.
[02:19:07.000 --> 02:19:12.840]   I don't care. I have alligators living in the house when I'm not home. I'm not worried too much.
[02:19:12.840 --> 02:19:14.760]   I share a lot of --
[02:19:14.760 --> 02:19:16.120]   As long as you have alligators.
[02:19:16.120 --> 02:19:21.160]   The only bad thing that ever happened to me was Robert Scoble once showed up at lunch.
[02:19:21.160 --> 02:19:24.680]   I said, "How'd you find me?" He said, "You posted on four square all the time."
[02:19:26.120 --> 02:19:28.520]   But that's the only bad thing that ever happened.
[02:19:28.520 --> 02:19:31.240]   Well, it specifically -- well, you just had to block Robert Scoble.
[02:19:31.240 --> 02:19:31.480]   Yeah.
[02:19:31.480 --> 02:19:35.560]   By the way, nobody else has ever done that.
[02:19:35.560 --> 02:19:36.200]   Just Scoble.
[02:19:36.200 --> 02:19:36.680]   Of course.
[02:19:36.680 --> 02:19:37.160]   Right?
[02:19:37.160 --> 02:19:37.880]   Just of course.
[02:19:37.880 --> 02:19:38.760]   Because of the alligators.
[02:19:38.760 --> 02:19:39.240]   Because the alligators.
[02:19:39.240 --> 02:19:39.800]   Because the alligators.
[02:19:39.800 --> 02:19:40.840]   Stay away from the house.
[02:19:40.840 --> 02:19:50.520]   Apple has lost its web search team to Google.
[02:19:50.520 --> 02:19:52.600]   They came to Apple from Google.
[02:19:52.600 --> 02:19:58.920]   And now they're going back. Apple in 2018 bought a company called Laser-Like.
[02:19:58.920 --> 02:20:00.840]   Now, this is not Search on the App Store.
[02:20:00.840 --> 02:20:09.720]   This is actually Apple creating technology like Google for spotlight and Siri suggestions.
[02:20:09.720 --> 02:20:16.440]   Siri, which is notoriously stupid, often says, "Let me show you what I found on the web about that.
[02:20:16.440 --> 02:20:17.720]   They wanted to make it smarter."
[02:20:17.720 --> 02:20:22.120]   So they hired or bought this company and hired the founders.
[02:20:22.760 --> 02:20:26.680]   Four years later, they probably vested their stock because they're gone back to Google.
[02:20:26.680 --> 02:20:33.560]   So don't expect Siri to get smarter any time soon.
[02:20:33.560 --> 02:20:36.440]   Let's see. What else?
[02:20:36.440 --> 02:20:41.960]   Let's take -- actually, let's take a little break.
[02:20:41.960 --> 02:20:45.000]   Because I want to -- you've got a couple of stories in here.
[02:20:45.000 --> 02:20:49.240]   I want to talk to you about David Spark, about security.
[02:20:49.240 --> 02:20:51.640]   I want to get to those in just a little bit.
[02:20:52.200 --> 02:20:55.000]   First, though, I want to talk a little bit about our sponsor,
[02:20:55.000 --> 02:21:00.360]   Nume. Usually when we do Nume, I like to parade my wife in here.
[02:21:00.360 --> 02:21:05.160]   She is the poster girl for Nume. We actually have a lot of poster people for Nume.
[02:21:05.160 --> 02:21:08.360]   Rec Con 5 was in our chatroom. He's the poster boy.
[02:21:08.360 --> 02:21:13.160]   We were on the Twit Cruise last July.
[02:21:13.160 --> 02:21:15.800]   And I knew that he was going to come on the cruise.
[02:21:15.800 --> 02:21:16.680]   And I couldn't find him.
[02:21:16.680 --> 02:21:21.480]   And I went into the Discord. I said, "Rec Con, I thought you were going on the trip with us."
[02:21:21.800 --> 02:21:24.600]   He said, "I'm right behind you. Turn around."
[02:21:24.600 --> 02:21:29.720]   He said, "I didn't even recognize him. He had lost -- this blows me away. 100 pounds.
[02:21:29.720 --> 02:21:33.400]   I saw him recently, kept the weight off, looks great. 100 pounds."
[02:21:33.400 --> 02:21:35.400]   I said, "How did you do it?" He said, "Nume."
[02:21:35.400 --> 02:21:40.280]   "I've done Nume lost 20 pounds. Lisa lost about the same. She did not need to lose it.
[02:21:40.280 --> 02:21:42.120]   She looks great, though. She's happy."
[02:21:42.120 --> 02:21:47.080]   And I'll tell you what, she's doing it religiously. She loves Nume.
[02:21:48.840 --> 02:21:53.800]   Nume weight has a psychology-first approach that really works. It works for me.
[02:21:53.800 --> 02:21:56.520]   It worked for Rec Con. It worked for my wife.
[02:21:56.520 --> 02:22:01.240]   It empowers you to build more sustainable habits and behaviors.
[02:22:01.240 --> 02:22:03.480]   And the results last.
[02:22:03.480 --> 02:22:07.400]   To date, Nume weight has helped more than 3.6 million people lose weight.
[02:22:07.400 --> 02:22:11.640]   And it does it with a very personal program. In fact,
[02:22:11.640 --> 02:22:16.280]   because they're completely customizing the program to you,
[02:22:16.280 --> 02:22:21.000]   to how you think about food, how do you relate to food, to your goals.
[02:22:21.000 --> 02:22:25.240]   Every journey is different. Your lessons are personalized to you, your goals,
[02:22:25.240 --> 02:22:27.160]   but also how much time you want to spend.
[02:22:27.160 --> 02:22:31.800]   You can choose your level of support from five-minute daily check-ins
[02:22:31.800 --> 02:22:34.440]   to personal coaching. They have a group you can join.
[02:22:34.440 --> 02:22:40.120]   So you get a coach, you get the lessons, you get the group.
[02:22:40.120 --> 02:22:44.120]   But one thing you don't get? A list of things you can't eat.
[02:22:44.840 --> 02:22:47.640]   Nume does not believe in restricting what you can or cannot eat.
[02:22:47.640 --> 02:22:51.720]   That was kind of a mind-blower for me. I've been on such restrictive diets in the past.
[02:22:51.720 --> 02:22:59.560]   Nume, it's not about perfection. It's not about restriction.
[02:22:59.560 --> 02:23:02.840]   It's about learning how you think about food. For instance,
[02:23:02.840 --> 02:23:06.840]   I learned I'm a fog eater. I come home after work and I stuff my face.
[02:23:06.840 --> 02:23:12.040]   And I'm not even aware of the food I've eaten. I don't even know it.
[02:23:12.040 --> 02:23:15.480]   It's almost unconscious. By becoming more conscious of where,
[02:23:15.480 --> 02:23:19.240]   Lisa and I now sit down. We turn off the TV. We put away our phones.
[02:23:19.240 --> 02:23:22.440]   We've got silverware, cloth napkins, and we eat.
[02:23:22.440 --> 02:23:26.280]   And sometimes, and it's a little weird, but it's just the two of us.
[02:23:26.280 --> 02:23:30.920]   We'll stop, we'll close our eyes, and we'll actually really taste what we're eating.
[02:23:30.920 --> 02:23:32.680]   And I can't tell you what a big difference that makes.
[02:23:32.680 --> 02:23:37.320]   Progress is in a straight line. Nume knows that. Off-days, totally okay.
[02:23:37.320 --> 02:23:40.440]   In fact, I even get bonus days when I've been doing well.
[02:23:40.440 --> 02:23:43.080]   They said, "I got to eat anything you want today." It's a bonus day.
[02:23:43.080 --> 02:23:45.480]   And then Nume Wake hits you back on track.
[02:23:45.480 --> 02:23:51.480]   It's grounded in science. Active Numer's lose an average of 15 pounds in 16 weeks.
[02:23:51.480 --> 02:23:56.040]   So I should say, Matt and Lisa and my experience is not typical.
[02:23:56.040 --> 02:24:01.960]   Typically, 15 pounds in 16 weeks. I have to say though, 95% of customers say
[02:24:01.960 --> 02:24:05.560]   Nume weight is a good long-term solution. I think that's a real test of anything.
[02:24:05.560 --> 02:24:10.040]   It's easy to lose weight, really. It's hard to keep that weight off, and Nume does it.
[02:24:10.360 --> 02:24:14.680]   They've published more than 30 peer-reviewed scientific articles that inform users,
[02:24:14.680 --> 02:24:18.600]   practitioners, scientists, and the public about their methods and effectiveness.
[02:24:18.600 --> 02:24:22.120]   The best thing about Nume, it's not work. It's not suffering. It's not deprivation.
[02:24:22.120 --> 02:24:26.920]   It's fun. It's like, "I'm learning. I'm doing..." And then it's natural.
[02:24:26.920 --> 02:24:30.120]   And you absorb it, and you do it for the rest of your life.
[02:24:30.120 --> 02:24:35.240]   Stay focused on what's important to you with Nume Wake psychology-based approach.
[02:24:35.240 --> 02:24:41.320]   It really works. Sign up for your trial today. Nume, N-O-O-M.com/twit.
[02:24:41.320 --> 02:24:45.000]   Go to the website, actually, and read up on it. There's some really good stuff there.
[02:24:45.000 --> 02:24:47.160]   If you have any questions, you'll find it all there.
[02:24:47.160 --> 02:24:53.160]   Nume.com/twit. Sign up for your trial at Nume.com/twit.
[02:24:53.160 --> 02:24:57.400]   They've got a new book too, which is great. You could check it out.
[02:24:57.400 --> 02:25:03.240]   The Nume Mindset, it's called "The Deep Dive into the Psychology of Behavior Change."
[02:25:03.240 --> 02:25:06.600]   Actually, it's not out yet, but you can pre-order it right now.
[02:25:06.600 --> 02:25:11.400]   The Nume Mindset, another great way to learn about what's going on.
[02:25:11.400 --> 02:25:15.160]   But honestly, I want you to get the app through the program.
[02:25:15.160 --> 02:25:20.120]   Put it on your iPhone, your Android phone, you log your meals, you do the lessons,
[02:25:20.120 --> 02:25:22.440]   you talk to your coach, you talk to your group.
[02:25:22.440 --> 02:25:26.440]   It is a system that really supports you and really works.
[02:25:26.440 --> 02:25:33.800]   Nume.O-O-O-M.com/twit. We are very happy Nummers in my family.
[02:25:33.800 --> 02:25:40.440]   Now, let's take a little break and watch a little movie that we made about this week on Twit.
[02:25:40.440 --> 02:25:48.440]   [MUSIC]
[02:25:48.440 --> 02:25:50.760]   Can you hear us now? Can you hear me?
[02:25:50.760 --> 02:25:51.800]   [MUSIC]
[02:25:51.800 --> 02:25:54.520]   Can anybody hear us now?
[02:25:54.520 --> 02:26:01.000]   [MUSIC]
[02:26:01.000 --> 02:26:02.760]   Previously on Twit.
[02:26:02.760 --> 02:26:07.960]   And next on Hands on Windows, we're going to take a look at how you can customize,
[02:26:07.960 --> 02:26:11.800]   the lock and sign-in screens in Windows 11.
[02:26:11.800 --> 02:26:13.000]   Floss Weekly.
[02:26:13.000 --> 02:26:16.680]   Our guest, Greg Crow Hartman, is a fellow of the Linux Foundation.
[02:26:16.680 --> 02:26:21.720]   He's currently responsible for the stable Linux kernel releases.
[02:26:21.720 --> 02:26:25.960]   You need to put processes in place to catch everybody, all the bugs.
[02:26:25.960 --> 02:26:30.200]   So I want our testing tools and processes and infrastructure
[02:26:30.200 --> 02:26:31.880]   to catch all the bugs that I write, because,
[02:26:31.880 --> 02:26:35.000]   infinously, I have written some pretty bad security bugs over the years.
[02:26:35.000 --> 02:26:36.920]   And fix them later.
[02:26:36.920 --> 02:26:39.960]   But it's just the way it goes. We're all human.
[02:26:39.960 --> 02:26:41.480]   Tech News Weekly.
[02:26:41.480 --> 02:26:46.760]   Amazon looking to cut costs, particularly in its devices area.
[02:26:46.760 --> 02:26:51.160]   I feel like that is because Amazon's realizing that they can't just throw a bunch of money
[02:26:51.160 --> 02:26:57.160]   at a house robot that nobody wants, and expect that they're not going to lose money there.
[02:26:57.160 --> 02:27:03.720]   So they just need to take their focus off of like twerking bears and other strange products
[02:27:03.720 --> 02:27:04.680]   that people don't want.
[02:27:04.680 --> 02:27:06.120]   That forgot about the bear.
[02:27:06.120 --> 02:27:07.880]   Twit.
[02:27:07.880 --> 02:27:10.120]   It was hilarious, but like pointless.
[02:27:10.120 --> 02:27:13.400]   We always have a lot of fun.
[02:27:13.400 --> 02:27:18.680]   We were playing with this new Zoom technology that allows us all to sing at the same time.
[02:27:18.680 --> 02:27:21.480]   Unfortunately, we were singing the same song.
[02:27:21.480 --> 02:27:26.440]   So that kind of didn't really work so well.
[02:27:26.440 --> 02:27:28.360]   That hands-on windows, by the way.
[02:27:28.360 --> 02:27:29.800]   You may say, "Where's that?
[02:27:29.800 --> 02:27:31.320]   That's a club-only special.
[02:27:31.320 --> 02:27:33.240]   If you're not a member of Club Twit, I want to remind you,
[02:27:33.240 --> 02:27:36.520]   we do some club-only shows because the club pays for them in effect.
[02:27:36.520 --> 02:27:40.760]   So I want you to join Club Twit at twit.tv/clubtwit.
[02:27:40.760 --> 02:27:41.960]   It's seven bucks a month.
[02:27:41.960 --> 02:27:44.760]   It really makes a difference to our bottom line.
[02:27:44.760 --> 02:27:48.280]   And frankly, going forward, I think it's going to be more and more important
[02:27:48.280 --> 02:27:49.880]   to keeping all of our shows on the air.
[02:27:49.880 --> 02:27:53.800]   You get ad-free versions of all the shows because we don't need to do ads.
[02:27:53.800 --> 02:27:54.760]   We don't need to track it.
[02:27:54.760 --> 02:27:56.600]   We just, you know, you're paying for it.
[02:27:56.600 --> 02:28:00.600]   You also get access to the wonderful Discord, which is always full of fun
[02:28:00.600 --> 02:28:02.520]   and great stuff.
[02:28:02.520 --> 02:28:06.360]   And I'm looking at an animated GIF right now.
[02:28:06.360 --> 02:28:07.080]   There it is.
[02:28:07.080 --> 02:28:12.280]   Looks like Elon has a Deacon.
[02:28:12.280 --> 02:28:14.120]   The Twit Army will not be defeated.
[02:28:14.120 --> 02:28:16.840]   There's a Shinny-eeboo in there.
[02:28:16.840 --> 02:28:17.880]   You can go hard.
[02:28:17.880 --> 02:28:18.760]   You can go hard.
[02:28:18.760 --> 02:28:20.360]   And Bitcoin, it's all in there together.
[02:28:20.360 --> 02:28:21.720]   It's all at once.
[02:28:21.720 --> 02:28:24.760]   Anyway, this is the kind of fun that happens in Discord.
[02:28:24.760 --> 02:28:30.280]   You also get the Twit Plus feed, which is where that whole singing thing showed up,
[02:28:30.280 --> 02:28:32.360]   stuff that happens before and after shows.
[02:28:32.360 --> 02:28:35.640]   Plus shows, like I said, that we don't normally put out in public,
[02:28:35.640 --> 02:28:38.760]   like Hands on Macintosh with Micah, Hands on Windows with Paul Therrot,
[02:28:38.760 --> 02:28:40.760]   the untitled Linux show with Jonathan Bennett,
[02:28:40.760 --> 02:28:43.640]   that gives Fizz with Dick T. Bartol Stacy's book club.
[02:28:43.640 --> 02:28:44.600]   There's a lot of stuff we do.
[02:28:44.600 --> 02:28:46.440]   I'm going to do more and more in the club as well.
[02:28:46.440 --> 02:28:48.840]   I've been planning a whole bunch of ideas.
[02:28:48.840 --> 02:28:50.600]   We have our own Minecraft servers.
[02:28:50.600 --> 02:28:51.880]   There's a lot of stuff going on.
[02:28:51.880 --> 02:28:57.880]   All of it is really about financing what we do here at Twit.
[02:28:57.880 --> 02:28:59.960]   So we're less dependent on advertisers.
[02:28:59.960 --> 02:29:05.480]   We love our advertisers, but frankly, with a recession, the downturn,
[02:29:05.480 --> 02:29:06.920]   it's getting a little harder.
[02:29:06.920 --> 02:29:08.120]   The club makes a big difference.
[02:29:08.120 --> 02:29:09.400]   We really need that.
[02:29:09.400 --> 02:29:11.240]   So please, if you're not a member, I don't understand.
[02:29:11.240 --> 02:29:11.880]   Money's tight.
[02:29:11.880 --> 02:29:15.000]   But if you're not a member, you can afford seven bucks a month or more.
[02:29:15.000 --> 02:29:16.760]   Twit.tv/clubtwit.
[02:29:16.760 --> 02:29:19.000]   It also finances the Twit forums,
[02:29:19.000 --> 02:29:22.040]   but you're open to all the Twit.community and the Mastinon instance,
[02:29:22.040 --> 02:29:27.400]   which 20 times more expensive than it was last month.
[02:29:27.400 --> 02:29:29.720]   Unfortunately.
[02:29:29.720 --> 02:29:31.000]   You had what?
[02:29:31.000 --> 02:29:32.520]   2,000% growth?
[02:29:32.520 --> 02:29:33.320]   No, no, no.
[02:29:33.320 --> 02:29:34.680]   That's you're underestimating it.
[02:29:34.680 --> 02:29:36.200]   I had 15,000.
[02:29:36.200 --> 02:29:37.160]   You want to see the growth?
[02:29:37.160 --> 02:29:39.320]   You want to see the growth, man?
[02:29:39.320 --> 02:29:40.680]   This is cray-cray.
[02:29:41.320 --> 02:29:47.240]   And what happened was now it's 18,642% growth in new users,
[02:29:47.240 --> 02:29:50.120]   active users, now almost 3,000.
[02:29:50.120 --> 02:29:52.920]   It was like 300 until two weeks ago.
[02:29:52.920 --> 02:29:58.440]   So that ends up costing money and the club supports that as well.
[02:29:58.440 --> 02:29:59.560]   Although it is open to all.
[02:29:59.560 --> 02:30:03.320]   So if people ask me, "How can we help you with the Mastinon server?"
[02:30:03.320 --> 02:30:04.120]   Join the club.
[02:30:04.120 --> 02:30:08.360]   Join the club and you get a lot more than just access to our Mastinon server.
[02:30:09.080 --> 02:30:11.640]   But everybody can go to the forums in the Mastinon server,
[02:30:11.640 --> 02:30:13.160]   in our IRC and all of that.
[02:30:13.160 --> 02:30:17.480]   We really want to continue to offer all the free services we do.
[02:30:17.480 --> 02:30:23.160]   It's just that it really is helpful if you help us out.
[02:30:23.160 --> 02:30:28.680]   Hey, I wanted to mention that this weekend is a tribute to the Internet's own boy,
[02:30:28.680 --> 02:30:30.200]   Aaron Schwartz.
[02:30:30.200 --> 02:30:37.240]   It was his birthday, I think, on the 12th.
[02:30:38.280 --> 02:30:41.560]   The EFF has been celebrating his life.
[02:30:41.560 --> 02:30:45.480]   Aaron was a very important person to the Internet,
[02:30:45.480 --> 02:30:55.640]   worked on some of the most important pieces of the Internet, including what RSS and a whole
[02:30:55.640 --> 02:30:56.360]   bunch of stuff.
[02:30:56.360 --> 02:31:07.320]   He was, I think, persecuted by the feds in 2013 under the Computer Front and Abuse Act.
[02:31:07.880 --> 02:31:09.000]   What did he do?
[02:31:09.000 --> 02:31:13.400]   He was trying to free academic journal articles from the JSTOR database.
[02:31:13.400 --> 02:31:17.880]   He downloaded a bunch of them because we paid for them in the first place.
[02:31:17.880 --> 02:31:20.280]   And JSTOR was charging people to get access to them,
[02:31:20.280 --> 02:31:24.680]   facing the prospect of a long and unjust sentence.
[02:31:24.680 --> 02:31:29.080]   He took his life at a very tender age, 26.
[02:31:29.080 --> 02:31:31.800]   But everyone in knew him.
[02:31:31.800 --> 02:31:34.520]   Just amazing.
[02:31:34.520 --> 02:31:36.600]   So we just wanted to mention that.
[02:31:36.600 --> 02:31:40.920]   And EFF had a streaming event at the Internet Archive this weekend.
[02:31:40.920 --> 02:31:46.440]   I'm sure they'll put that online for people who can't make it in person and so forth.
[02:31:46.440 --> 02:31:48.040]   The Internet's new boy.
[02:31:48.040 --> 02:31:49.000]   His story needs to be told more.
[02:31:49.000 --> 02:31:53.720]   We all know the story, but there are generations that need to know that, no,
[02:31:53.720 --> 02:31:57.320]   there was a man who was doing something absolutely right.
[02:31:57.320 --> 02:32:03.960]   And the full weight of the US government was brought down on him for no other reason than
[02:32:03.960 --> 02:32:07.880]   they didn't know how to deal with people who wanted the information they'd already paid for.
[02:32:07.880 --> 02:32:08.120]   Yep.
[02:32:08.120 --> 02:32:09.640]   That was it.
[02:32:09.640 --> 02:32:09.960]   Yep.
[02:32:09.960 --> 02:32:15.400]   He worked on RSS, Markdown, Creative Commons, the web framework web.py.
[02:32:15.400 --> 02:32:23.080]   He was given the title of co-founder of Reddit by Paul Graham after the formation of Not A Bug
[02:32:23.080 --> 02:32:29.560]   Incorporated, the company that he merged with Alexis O'Haney and Steve Huffman's
[02:32:29.560 --> 02:32:31.080]   Reddit.
[02:32:31.800 --> 02:32:33.800]   He worked on civic activism.
[02:32:33.800 --> 02:32:38.520]   He was a research fellow at Harvard's Safra Research Lab on Institutional Corruption
[02:32:38.520 --> 02:32:40.280]   under Larry Lessig.
[02:32:40.280 --> 02:32:42.920]   He founded the online group Demand Progress.
[02:32:42.920 --> 02:32:49.880]   That's the group that fought SOPA successfully, I might add.
[02:32:49.880 --> 02:32:56.680]   He was very important to the Internet and a great loss to us all.
[02:32:56.680 --> 02:33:00.360]   So I thought I would mention that.
[02:33:01.320 --> 02:33:09.160]   I wanted to talk a little bit about this piece you did in your CISO series, David,
[02:33:09.160 --> 02:33:11.560]   about cyber attacks.
[02:33:11.560 --> 02:33:16.440]   You'd think, oh, they're constantly creating new stuff.
[02:33:16.440 --> 02:33:25.880]   And no, in fact, some of the dumbest, easiest hacks are still in widespread use.
[02:33:25.880 --> 02:33:27.240]   You did this at Black Hat, yes?
[02:33:27.240 --> 02:33:28.920]   Yes, at Black Hat.
[02:33:28.920 --> 02:33:35.480]   So the two guys you see in that photo there for that video, that is Bart Stump and Neil
[02:33:35.480 --> 02:33:41.080]   Weiler, who goes by the name Grifter, and they run the NOC at Black Hat, the NOC standard
[02:33:41.080 --> 02:33:42.120]   of the network operation.
[02:33:42.120 --> 02:33:43.240]   Oh, that must be fun.
[02:33:43.240 --> 02:33:44.440]   Oh, yeah.
[02:33:44.440 --> 02:33:50.920]   And in fact, Grifter has referred to it as Christmas for him because what he sees in over
[02:33:50.920 --> 02:33:57.160]   just a period of three, four days on that specific network is what you might see
[02:33:57.800 --> 02:34:01.640]   if it's bad an entire year at another company.
[02:34:01.640 --> 02:34:07.480]   But it's just this highly abused network, mostly because people go to Black Hat the
[02:34:07.480 --> 02:34:13.720]   first couple of days for training, and they're purposely trying out techniques on the network.
[02:34:13.720 --> 02:34:17.320]   They most contested network in the history of mankind.
[02:34:17.320 --> 02:34:24.120]   Yeah, what Bart said, he goes, if this, you saw this in a real environment, it would be a bad year.
[02:34:24.680 --> 02:34:27.240]   So, yeah, which I thought was a good line.
[02:34:27.240 --> 02:34:27.640]   Yeah.
[02:34:27.640 --> 02:34:31.640]   The, but I always ask him, I go, so, you know, what's cool and new?
[02:34:31.640 --> 02:34:32.280]   Like, what did you see?
[02:34:32.280 --> 02:34:37.720]   Because honestly, that's why people come to Black Hat, you know, and to act to any
[02:34:37.720 --> 02:34:38.520]   conference.
[02:34:38.520 --> 02:34:40.200]   I want to know what's new and cool.
[02:34:40.200 --> 02:34:45.080]   And the sad reality, and I can't remember the CISO who said this, he said, the reality is,
[02:34:45.080 --> 02:34:50.680]   the thing you were worried about before you went to Black Hat is going to be the same thing
[02:34:50.680 --> 02:34:53.480]   you're worried about after you leave Black Hat.
[02:34:53.480 --> 02:34:55.640]   It's not going to be anything new.
[02:34:55.640 --> 02:35:03.640]   And honestly, it's because the simple things like credential stuffing and fishing work,
[02:35:03.640 --> 02:35:05.960]   and they're cheap and they're effective.
[02:35:05.960 --> 02:35:06.280]   Yeah.
[02:35:06.280 --> 02:35:10.600]   Sad to say, I bet you, you go to Black Hat.
[02:35:10.600 --> 02:35:12.600]   Did you go to Black Hat this year, Father Rubber?
[02:35:12.600 --> 02:35:18.520]   I was supposed to, but then there was a last-minute change of my schedule.
[02:35:18.520 --> 02:35:23.240]   I will be going this year, though, and I have promises from over here that they've cleared
[02:35:23.240 --> 02:35:29.640]   that there's nothing that can possibly change my schedule except for one person, one person.
[02:35:29.640 --> 02:35:32.680]   One thing, one little thing, one little thing.
[02:35:32.680 --> 02:35:37.800]   But other than that, so you usually go to Def Con in Black Hat, you would never attempt to hack
[02:35:37.800 --> 02:35:38.680]   the knock, would you?
[02:35:38.680 --> 02:35:40.280]   Apps always.
[02:35:40.280 --> 02:35:43.480]   That's the first thing I do.
[02:35:43.480 --> 02:35:47.480]   Well, there's the wall of sheep, isn't there?
[02:35:47.480 --> 02:35:49.560]   That's Def Con.
[02:35:49.560 --> 02:35:51.000]   That's Def Con where they put up.
[02:35:51.000 --> 02:35:53.000]   No, I think they also do it at Black Hat, actually.
[02:35:53.000 --> 02:35:53.800]   The list of...
[02:35:53.800 --> 02:35:54.360]   Oh, that's right.
[02:35:54.360 --> 02:35:54.920]   Yeah, they've added it.
[02:35:54.920 --> 02:35:55.880]   They do the Black Hat.
[02:35:55.880 --> 02:36:01.240]   But I will tell you, the wall of sheep is getting a lot of negative press.
[02:36:01.240 --> 02:36:01.640]   Oh.
[02:36:01.640 --> 02:36:02.520]   I will tell you, definitely...
[02:36:02.520 --> 02:36:02.920]   Why?
[02:36:02.920 --> 02:36:04.600]   Among the cybersecurity community.
[02:36:04.600 --> 02:36:05.560]   Because it is...
[02:36:05.560 --> 02:36:06.360]   It's embarrassing.
[02:36:06.360 --> 02:36:10.040]   It's embarrassing, and this is not what we want to do in cybersecurity.
[02:36:10.040 --> 02:36:18.200]   You don't bring people into cybersecurity, and let me define what the wall of sheep is.
[02:36:18.200 --> 02:36:22.120]   People who get hacked get literally exposed and published,
[02:36:22.120 --> 02:36:26.360]   and the idea being, look at how weak this person's security is.
[02:36:26.360 --> 02:36:28.280]   We were able to break into them.
[02:36:28.280 --> 02:36:35.640]   And that to mock someone is not a way to bring them in the fold of understanding the
[02:36:35.640 --> 02:36:39.640]   importance of cybersecurity, because one of the things that we talk about endlessly is,
[02:36:39.640 --> 02:36:45.320]   it is not just the security department's job for security, it is the entire company's
[02:36:45.320 --> 02:36:46.040]   responsibility.
[02:36:47.320 --> 02:36:48.760]   You want everyone to be on board.
[02:36:48.760 --> 02:36:52.520]   You do not want to expose people, mock them, make them look like a fool,
[02:36:52.520 --> 02:36:57.240]   because that is not a way to get people excited about joining your club.
[02:36:57.240 --> 02:36:58.600]   Here's why people are mad.
[02:36:58.600 --> 02:37:02.760]   I do have two plus sheep that I got from Wall of Sheep, but I have a very nice shirt.
[02:37:02.760 --> 02:37:07.480]   So if you ever do get hacked, you can go over there and they'll actually give you something to see.
[02:37:07.480 --> 02:37:10.680]   At least you get a reward on the Wall of Sheep website.
[02:37:10.680 --> 02:37:14.040]   What are a few of the most crazy things you've seen while sniffing traffic?
[02:37:14.840 --> 02:37:18.520]   Someone decided to be a good idea to file their taxes while at Defcon.
[02:37:18.520 --> 02:37:20.200]   We disagree.
[02:37:20.200 --> 02:37:25.800]   It's funny you say that I've gone out with an editor and the number one thing I say is,
[02:37:25.800 --> 02:37:28.840]   don't do anything financial while you're at Black Hat.
[02:37:28.840 --> 02:37:30.600]   Just stay off of it.
[02:37:30.600 --> 02:37:35.720]   Well respected author and authority in the security community decided to share their
[02:37:35.720 --> 02:37:39.720]   unpublished book and their bank statements with us by not using SSL.
[02:37:39.720 --> 02:37:40.760]   Great book, by the way.
[02:37:43.160 --> 02:37:45.400]   Anyway, it's funny.
[02:37:45.400 --> 02:37:51.080]   Well, I hope they keep doing it, but I understand why some people might be a little bit upset.
[02:37:51.080 --> 02:37:55.480]   The best years were when AOL Instant Messenger was still active,
[02:37:55.480 --> 02:37:57.800]   because that was completely unencrypted.
[02:37:57.800 --> 02:37:59.400]   That was all in the clear.
[02:37:59.400 --> 02:38:03.240]   So if you started sniffing the network, you would just get all of these conversations
[02:38:03.240 --> 02:38:07.400]   sometimes out of context, sometimes it's salacious information.
[02:38:09.000 --> 02:38:12.920]   Yeah, that's actually part of the show. Everyone expects that.
[02:38:12.920 --> 02:38:13.240]   Yeah.
[02:38:13.240 --> 02:38:18.360]   That's the reason why every device I have that goes to Defcon Black Hat gets reformatted
[02:38:18.360 --> 02:38:19.320]   immediately after the show.
[02:38:19.320 --> 02:38:19.960]   That's exactly.
[02:38:19.960 --> 02:38:22.600]   Do they still do Spot the Fed?
[02:38:22.600 --> 02:38:25.800]   Well, I don't even know about Spot the Fed.
[02:38:25.800 --> 02:38:26.680]   Yeah, yeah.
[02:38:26.680 --> 02:38:29.720]   But that's kind of...
[02:38:29.720 --> 02:38:30.920]   Is that old?
[02:38:30.920 --> 02:38:31.800]   It's easier now.
[02:38:31.800 --> 02:38:34.520]   Yeah, because the feds don't really hide in the fact that they're there.
[02:38:34.520 --> 02:38:39.640]   This is like, you know, big thing. Black Hat, they're there, and then they're there and they're
[02:38:39.640 --> 02:38:40.760]   publicly there.
[02:38:40.760 --> 02:38:44.360]   Defcon is where they might be surreptitiously there.
[02:38:44.360 --> 02:38:45.000]   Yeah.
[02:38:45.000 --> 02:38:47.960]   We've actually had feds do presentations at Defcon.
[02:38:47.960 --> 02:38:48.360]   Oh, okay.
[02:38:48.360 --> 02:38:49.080]   Well, they're so...
[02:38:49.080 --> 02:38:50.760]   That's easier to Spot the Fed.
[02:38:50.760 --> 02:38:58.680]   I have to say, I did not know this was a big deal, but the internet schooled me.
[02:38:59.400 --> 02:39:06.200]   Kevin Conroy passed away this week, and I confess I didn't know who he is.
[02:39:06.200 --> 02:39:11.240]   He was a young 66, passed away after a short battle with cancer.
[02:39:11.240 --> 02:39:14.120]   He was Batman.
[02:39:14.120 --> 02:39:15.480]   Now, you may say, wait a minute.
[02:39:15.480 --> 02:39:18.280]   Which Batman was he?
[02:39:18.280 --> 02:39:25.160]   Well, he was the voice of Batman and all the animated Batman serials that apparently most of the
[02:39:25.160 --> 02:39:26.120]   internet grew up with.
[02:39:26.760 --> 02:39:31.400]   So you knew who the animated series Superman, the animated series Justice League, Justice League
[02:39:31.400 --> 02:39:35.720]   Unlimited, the movies that came off of those Batman beyond.
[02:39:35.720 --> 02:39:41.320]   I mean, really, of all he had the most screen time of any actor playing Batman.
[02:39:41.320 --> 02:39:44.680]   Wow. Keith Levine died too.
[02:39:44.680 --> 02:39:45.640]   Who's Keith Levine?
[02:39:45.640 --> 02:39:47.960]   The Clash and...
[02:39:47.960 --> 02:39:49.000]   Oh, yeah, that's right.
[02:39:49.000 --> 02:39:49.400]   ...the public...
[02:39:49.400 --> 02:39:49.800]   ...APIL.
[02:39:49.800 --> 02:39:50.360]   Yep.
[02:39:50.360 --> 02:39:50.680]   Yep.
[02:39:50.680 --> 02:39:50.680]   Yep.
[02:39:50.680 --> 02:39:51.080]   You were...
[02:39:51.080 --> 02:39:51.320]   Yeah.
[02:39:51.320 --> 02:39:53.640]   Well, if we're talking also Gallagher passed away.
[02:39:53.640 --> 02:39:54.520]   Gallagher passed away.
[02:39:54.520 --> 02:39:55.320]   I saw him.
[02:39:55.320 --> 02:39:57.720]   My parents took me to see him a billion years ago.
[02:39:57.720 --> 02:40:00.280]   He was fairly young.
[02:40:00.280 --> 02:40:01.320]   He was 76.
[02:40:01.320 --> 02:40:05.800]   Great comedian, famous for smashing watermelon on stage.
[02:40:05.800 --> 02:40:08.360]   Apparently not the nicest guy ever, but very funny.
[02:40:08.360 --> 02:40:11.400]   Keith, Keith, he rubbed a few comedians the wrong way.
[02:40:11.400 --> 02:40:11.560]   Yeah.
[02:40:11.560 --> 02:40:16.760]   There's an infinite episode of Mark Marin's podcast, W...
[02:40:16.760 --> 02:40:17.960]   He walked out on it.
[02:40:17.960 --> 02:40:19.000]   Where he walked out on it.
[02:40:19.000 --> 02:40:19.560]   Why did he...
[02:40:19.560 --> 02:40:20.280]   You know, I haven't heard that.
[02:40:20.280 --> 02:40:21.080]   I saw that he did that.
[02:40:21.080 --> 02:40:22.440]   Hey, who walked on who?
[02:40:22.440 --> 02:40:24.680]   Gallagher walked out on Marin.
[02:40:24.680 --> 02:40:27.160]   I believe they actually were in a hotel room.
[02:40:27.160 --> 02:40:29.240]   I'm thinking of Marin's...
[02:40:29.240 --> 02:40:29.880]   And he walked out.
[02:40:29.880 --> 02:40:33.000]   And essentially, Marin just started getting into it with
[02:40:33.000 --> 02:40:36.840]   Gallagher, you know, just asking about his act.
[02:40:36.840 --> 02:40:40.680]   You know, honestly, I don't think Marin respected Gallagher's act.
[02:40:40.680 --> 02:40:41.000]   Right.
[02:40:41.000 --> 02:40:43.240]   There's a lot of comedians just flat out, do not respect...
[02:40:43.240 --> 02:40:45.960]   He's a little bit like Carrot top.
[02:40:45.960 --> 02:40:48.120]   Yeah, a little too much prop.
[02:40:48.120 --> 02:40:51.160]   So essentially Gallagher just didn't want to hear it.
[02:40:51.160 --> 02:40:51.800]   And he walked out.
[02:40:51.800 --> 02:40:52.920]   And he...
[02:40:52.920 --> 02:40:54.280]   Instead of sort of engaging Mark,
[02:40:54.280 --> 02:40:55.640]   he just said, "I'm not dealing with this."
[02:40:55.640 --> 02:40:56.680]   And he walked out on him.
[02:40:56.680 --> 02:40:57.880]   It's just a podcast.
[02:40:57.880 --> 02:40:58.920]   What do I...
[02:40:58.920 --> 02:41:00.200]   If nobody listens to this...
[02:41:00.200 --> 02:41:05.000]   Marin wasn't as huge as he is now when he interviewed,
[02:41:05.000 --> 02:41:06.520]   but he had a significant audience.
[02:41:06.520 --> 02:41:06.680]   Yeah, yeah.
[02:41:06.680 --> 02:41:07.480]   Well, he's...
[02:41:07.480 --> 02:41:10.520]   I gotta say, it's one of the things that everybody mentioned
[02:41:10.520 --> 02:41:11.800]   when they mentioned Gallagher.
[02:41:11.800 --> 02:41:14.840]   So, be careful who you walk out on.
[02:41:14.840 --> 02:41:17.160]   Thank you all for not walking out on me.
[02:41:17.160 --> 02:41:18.600]   I appreciate it.
[02:41:19.560 --> 02:41:23.240]   We love seeing you, Father Robert Ballisair, the digital Jesuit.
[02:41:23.240 --> 02:41:25.640]   Digitaljessuit.com.
[02:41:25.640 --> 02:41:27.400]   He's on the Mastodon.
[02:41:27.400 --> 02:41:28.200]   Dontnet.
[02:41:28.200 --> 02:41:28.760]   Dontnet.
[02:41:28.760 --> 02:41:29.400]   Dontnet.
[02:41:29.400 --> 02:41:29.960]   Let's fix that.
[02:41:29.960 --> 02:41:32.280]   We've had the wrong lower third the whole time.
[02:41:32.280 --> 02:41:33.160]   Dontnet.com.
[02:41:33.160 --> 02:41:34.760]   They all go to the same place.
[02:41:34.760 --> 02:41:35.320]   Oh, they all work, okay.
[02:41:35.320 --> 02:41:37.800]   I tore the website down because I needed to redo it.
[02:41:37.800 --> 02:41:39.400]   So it will verify Mastodon.
[02:41:39.400 --> 02:41:41.080]   Oh, wow.
[02:41:41.080 --> 02:41:43.880]   All you have to do is put a link that says, "rel equals me."
[02:41:43.880 --> 02:41:48.120]   I know, but I got obsessive compulsive and I didn't like it.
[02:41:48.120 --> 02:41:50.040]   So I just tore everything up.
[02:41:50.040 --> 02:41:52.200]   Ah, now I get it.
[02:41:52.200 --> 02:41:53.000]   You're one of those.
[02:41:53.000 --> 02:41:58.120]   Do you still do the factorial server and the various servers,
[02:41:58.120 --> 02:41:58.840]   the Vatican servers?
[02:41:58.840 --> 02:42:00.280]   We've got a couple of them running.
[02:42:00.280 --> 02:42:06.520]   The Minecraft server melted down, literally, melted down physically.
[02:42:06.520 --> 02:42:07.800]   Physically melted?
[02:42:07.800 --> 02:42:08.600]   Of graphene.
[02:42:08.600 --> 02:42:11.000]   But we're getting that back up.
[02:42:11.000 --> 02:42:11.800]   It's fun.
[02:42:11.800 --> 02:42:14.920]   I don't get as much time to game as I did during the pandemic.
[02:42:16.040 --> 02:42:22.920]   But yeah, I probably spent too much time playing a game called Oxygen Not Included.
[02:42:22.920 --> 02:42:24.920]   Oh, that sounds deadly.
[02:42:24.920 --> 02:42:29.640]   I think I've spent more time playing that than you played Simpsons tapped out.
[02:42:29.640 --> 02:42:31.880]   That's not possible.
[02:42:31.880 --> 02:42:34.360]   You didn't spend more money on it, though.
[02:42:34.360 --> 02:42:36.840]   Anyway, great to have you, Father Robert.
[02:42:36.840 --> 02:42:41.720]   I look forward to seeing you in the Vatican in the spring and perhaps here,
[02:42:41.720 --> 02:42:43.880]   sometimes sooner than that.
[02:42:44.680 --> 02:42:46.200]   I'll be back for CES.
[02:42:46.200 --> 02:42:46.440]   Yeah.
[02:42:46.440 --> 02:42:47.480]   Oh, that's right.
[02:42:47.480 --> 02:42:52.040]   You did put that up on the Twitter and the Mastodon that you're going to CES this year.
[02:42:52.040 --> 02:42:53.080]   Very brave of you.
[02:42:53.080 --> 02:42:55.480]   I'll all be messed up.
[02:42:55.480 --> 02:42:56.840]   I'm going to wear a bunny suit.
[02:42:56.840 --> 02:42:57.800]   Yeah, that's a good idea.
[02:42:57.800 --> 02:42:59.480]   Are you looking forward to that?
[02:42:59.480 --> 02:42:59.960]   CES?
[02:42:59.960 --> 02:43:03.480]   No?
[02:43:03.480 --> 02:43:03.880]   Sorry.
[02:43:03.880 --> 02:43:04.520]   You cut out.
[02:43:04.520 --> 02:43:05.240]   Oh, okay.
[02:43:05.240 --> 02:43:07.000]   He's not looking forward to that.
[02:43:07.000 --> 02:43:07.960]   Are you looking forward to it?
[02:43:07.960 --> 02:43:09.640]   Oh, absolutely.
[02:43:09.640 --> 02:43:09.960]   Okay.
[02:43:09.960 --> 02:43:13.240]   I mean, okay, for me, the tech is really secondary.
[02:43:13.240 --> 02:43:16.440]   It's seeing people that I only see once or twice a year.
[02:43:16.440 --> 02:43:18.680]   That's my network.
[02:43:18.680 --> 02:43:19.320]   That's the fuck I want.
[02:43:19.320 --> 02:43:19.800]   Those are my friends.
[02:43:19.800 --> 02:43:21.160]   Those are my colleagues.
[02:43:21.160 --> 02:43:25.400]   And they'll let me in on some interesting stories around techdom.
[02:43:25.400 --> 02:43:26.520]   Good.
[02:43:26.520 --> 02:43:30.360]   We hope you will give us some pieces or something.
[02:43:30.360 --> 02:43:32.120]   Report in.
[02:43:32.120 --> 02:43:32.840]   Something like that.
[02:43:32.840 --> 02:43:35.480]   Because I don't think I'm going to be going.
[02:43:35.480 --> 02:43:36.920]   I haven't gone since 2020.
[02:43:36.920 --> 02:43:39.240]   I was just at Skytex,
[02:43:39.240 --> 02:43:41.560]   which is like the Middle Eastern version of CES,
[02:43:41.560 --> 02:43:44.600]   where I did actually see some pretty interesting stuff.
[02:43:44.600 --> 02:43:46.440]   Because I was in Dubai.
[02:43:46.440 --> 02:43:48.280]   I was in Dubai working in Jitex.
[02:43:48.280 --> 02:43:51.480]   But Iranian technology,
[02:43:51.480 --> 02:43:53.240]   I saw Iranian drones.
[02:43:53.240 --> 02:43:54.440]   I saw stuff from Russia.
[02:43:54.440 --> 02:43:56.680]   You see stuff you won't see in the US.
[02:43:56.680 --> 02:43:56.920]   No.
[02:43:56.920 --> 02:43:58.920]   You might see in Ukraine, though.
[02:43:58.920 --> 02:44:00.840]   You definitely.
[02:44:00.840 --> 02:44:05.000]   Amy Webb, probably be at the international pavilion.
[02:44:05.000 --> 02:44:06.360]   There is a pavilion at CES.
[02:44:06.360 --> 02:44:08.440]   That's where it's specifically dedicated to smaller
[02:44:08.440 --> 02:44:09.960]   to national firms.
[02:44:09.960 --> 02:44:11.000]   I don't think you're wrong.
[02:44:11.000 --> 02:44:11.480]   I'll be there.
[02:44:11.480 --> 02:44:12.360]   I really, I don't.
[02:44:12.360 --> 02:44:15.480]   Thank you, Father Robert.
[02:44:15.480 --> 02:44:16.840]   Thank you, Amy Webb.
[02:44:16.840 --> 02:44:19.720]   Future Tech Today Institute is her,
[02:44:19.720 --> 02:44:22.920]   her futurist effort.
[02:44:22.920 --> 02:44:25.240]   Of course, her books, all of them are great.
[02:44:25.240 --> 02:44:27.400]   The latest, the Genesis machine,
[02:44:27.400 --> 02:44:31.080]   our quest to rewrite life in the age of synthetic biology.
[02:44:31.080 --> 02:44:34.760]   Her feet, her dogs might be barking,
[02:44:34.760 --> 02:44:37.560]   but she's happy to be here.
[02:44:37.560 --> 02:44:39.160]   That's, that's what I'm going to say.
[02:44:39.720 --> 02:44:40.520]   Thank you.
[02:44:40.520 --> 02:44:40.920]   Thank you so much.
[02:44:40.920 --> 02:44:41.800]   I really appreciate it.
[02:44:41.800 --> 02:44:43.240]   Always a pleasure to have you on, Amy.
[02:44:43.240 --> 02:44:45.400]   And thank you, David Spark.
[02:44:45.400 --> 02:44:51.400]   seesoseries.com if you're interested in security,
[02:44:51.400 --> 02:44:55.800]   and what the chief information security officer thinks about.
[02:44:55.800 --> 02:44:57.800]   See so series.
[02:44:57.800 --> 02:45:01.320]   In cybersecurity, want to break into cybersecurity,
[02:45:01.320 --> 02:45:03.320]   or a leader in cybersecurity.
[02:45:03.320 --> 02:45:05.000]   We like your community as well.
[02:45:05.000 --> 02:45:09.320]   You know, one of the unique things I found about the cyber security community.
[02:45:10.280 --> 02:45:11.720]   David, damn good sense of humor.
[02:45:11.720 --> 02:45:12.840]   I love a very funny group.
[02:45:12.840 --> 02:45:13.240]   Oh yeah.
[02:45:13.240 --> 02:45:15.160]   Oh yeah, I love them.
[02:45:15.160 --> 02:45:15.880]   Absolutely.
[02:45:15.880 --> 02:45:17.400]   Thank you, David.
[02:45:17.400 --> 02:45:18.040]   Thank you, Amy.
[02:45:18.040 --> 02:45:19.000]   Thank you, Father Robert.
[02:45:19.000 --> 02:45:19.800]   I want to give one plug.
[02:45:19.800 --> 02:45:21.720]   We don't usually do pics on this show,
[02:45:21.720 --> 02:45:23.560]   but this guy put so much effort into it.
[02:45:23.560 --> 02:45:24.520]   I don't know why.
[02:45:24.520 --> 02:45:27.000]   His name is Parker Reed.
[02:45:27.000 --> 02:45:31.800]   And he did a cut down of how many?
[02:45:31.800 --> 02:45:35.320]   Like two or 300 twit opens.
[02:45:36.280 --> 02:45:40.680]   This is from 2009 to a couple of weeks ago.
[02:45:40.680 --> 02:45:43.400]   You can watch my hairstyles change.
[02:45:43.400 --> 02:45:46.120]   You can watch my seat change.
[02:45:46.120 --> 02:45:47.320]   There we are at CES.
[02:45:47.320 --> 02:45:52.680]   You can watch and hear about what stories we were talking about at that time.
[02:45:52.680 --> 02:45:54.440]   You could you.
[02:45:54.440 --> 02:45:55.800]   I mean, it's just over and over.
[02:45:55.800 --> 02:45:56.600]   And there's a lot of them.
[02:45:56.600 --> 02:45:57.880]   It's time for Twit this weekend.
[02:45:57.880 --> 02:45:59.800]   The show where we cover the latest tech.
[02:45:59.800 --> 02:46:03.640]   So I don't know why Parker decided to do this.
[02:46:05.880 --> 02:46:07.880]   I guess I'm glad he did that.
[02:46:07.880 --> 02:46:09.000]   I watched the whole thing.
[02:46:09.000 --> 02:46:10.200]   You watched the whole thing.
[02:46:10.200 --> 02:46:12.040]   I watched the whole thing.
[02:46:12.040 --> 02:46:14.920]   I mean, it's fascinating to see the set change,
[02:46:14.920 --> 02:46:17.640]   the quality of the video change, the topics change.
[02:46:17.640 --> 02:46:22.200]   It's four hours and 41 minutes of nothing but me.
[02:46:22.200 --> 02:46:24.440]   I could watch it for one a minute.
[02:46:24.440 --> 02:46:27.000]   But I'm thank you for, I guess.
[02:46:27.000 --> 02:46:29.400]   And thank you very much, Parker, for doing this.
[02:46:29.400 --> 02:46:33.480]   It's on YouTube, youtube.com/parkerread.
[02:46:33.480 --> 02:46:36.280]   This weekend tech intro super cut.
[02:46:36.280 --> 02:46:40.680]   And you know, you might say, well, what happened to the first 200 episodes?
[02:46:40.680 --> 02:46:41.720]   Why aren't they there?
[02:46:41.720 --> 02:46:42.840]   Well, we didn't have video.
[02:46:42.840 --> 02:46:49.640]   So I guess he started with the very first video, which is me in the cottage.
[02:46:49.640 --> 02:46:51.400]   Kevin Rose goes make up free.
[02:46:51.400 --> 02:46:53.560]   Patrick Norton files his first lawsuit.
[02:46:53.560 --> 02:46:55.640]   And Devorak goes retro.
[02:46:55.640 --> 02:46:56.760]   Twit is next.
[02:46:56.760 --> 02:46:59.080]   That was like, that was like the beginnings of Twit.
[02:46:59.080 --> 02:47:00.760]   Devorak, Rose, Norton.
[02:47:00.760 --> 02:47:03.800]   I did one of those audio episodes.
[02:47:03.800 --> 02:47:07.560]   I remember with Kevin Rose and Jason Calicanis.
[02:47:07.560 --> 02:47:13.160]   And I was in the cottage once as well with Jim Louderback.
[02:47:13.160 --> 02:47:15.800]   And I'm trying to remember the other.
[02:47:15.800 --> 02:47:16.680]   Well, just go through this.
[02:47:16.680 --> 02:47:17.640]   You probably could find it.
[02:47:17.640 --> 02:47:19.960]   Oh, Baratunde as well.
[02:47:19.960 --> 02:47:20.280]   Yeah.
[02:47:20.280 --> 02:47:26.280]   Apparently Jason is in the room where it happens at Twitter, which is hysterical.
[02:47:26.280 --> 02:47:29.400]   In fact, we didn't talk about it,
[02:47:29.400 --> 02:47:36.440]   but the best article about the whole Twitter thing is a New York Times article that does,
[02:47:36.440 --> 02:47:39.080]   in fact, talk about Jason.
[02:47:39.080 --> 02:47:42.360]   It's called Two Weeks of Chaos.
[02:47:42.360 --> 02:47:45.400]   If you want a great read, this is the beginning of a book, I think.
[02:47:45.400 --> 02:47:48.280]   Two Weeks of Chaos inside Elon Musk's takeover of Twitter,
[02:47:48.280 --> 02:47:51.160]   they even have a picture of Jason.
[02:47:51.160 --> 02:47:59.000]   And they talk about one point where Jason was tweeting all these ideas about what to do
[02:47:59.000 --> 02:47:59.640]   with Twitter.
[02:47:59.640 --> 02:48:02.520]   Let me see if I can find this quote.
[02:48:02.520 --> 02:48:11.160]   And Elon sent down an assistant to say, "Can you not get offices embarrassing?"
[02:48:11.160 --> 02:48:17.320]   Which is now twice that Jason has been spanked.
[02:48:17.320 --> 02:48:22.040]   Last week, Mr. Musk dispatched Lieutenant to the war room to ask Mr. Calicanis who was
[02:48:22.040 --> 02:48:28.440]   there to cool it on Twitter and stop acting as if he were leading product development or policy.
[02:48:28.440 --> 02:48:34.040]   People familiar with the exchange said, "To be clear, Elon is the product manager and CEO
[02:48:34.040 --> 02:48:38.840]   Kenneth Kenneth tweeted, 'As a power user, and that's all I am.' I'm really excited."
[02:48:38.840 --> 02:48:43.240]   Knowing Jason, he is very excited to be in the room where it's happening.
[02:48:43.240 --> 02:48:47.640]   And probably does not want to take any of the blame for anything that did happen.
[02:48:47.640 --> 02:48:53.960]   Thank you, everybody. Have a great week. We do Twitter every Sunday, 2PM Pacific, 5PM Eastern,
[02:48:53.960 --> 02:49:01.480]   2200 UTC. You can watch us live at twit.tv/live. You can chat with us live at rc.twit.tv or in the
[02:49:01.480 --> 02:49:05.480]   club, Twit Discord after the fact, download what's up as this is the show from the website,
[02:49:05.480 --> 02:49:09.240]   from the YouTube channel, or use your favorite podcast player and subscribe.
[02:49:09.240 --> 02:49:13.160]   That's the best way to get it. We'll see you next time. Another Twit is in the can.
[02:49:13.160 --> 02:49:14.520]   This is amazing.
[02:49:14.520 --> 02:49:24.920]   [Music]
[02:49:24.920 --> 02:49:29.640]   This week on Twit, it's the cottage cavalcade of computer comedy featuring Baratunde,
[02:49:29.640 --> 02:49:35.320]   David Spark. And who's that guy? I don't even know him. Stay tuned. Twit is next.

whisper_print_timings:     load time =   336.32 ms
whisper_print_timings:     fallbacks =   2 p /   0 h
whisper_print_timings:      mel time = 70155.13 ms
whisper_print_timings:   sample time = 23191.26 ms / 43808 runs (    0.53 ms per run)
whisper_print_timings:   encode time = 7814908.50 ms /   370 runs (21121.38 ms per run)
whisper_print_timings:   decode time = 2702833.00 ms / 43806 runs (   61.70 ms per run)
whisper_print_timings:    total time = 10612213.00 ms

