;FFMETADATA1
title=My Husband's an iDoctor
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=613
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2017
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:02.640]   It's time for Twit this weekend tech, a great show for you.
[00:00:02.640 --> 00:00:08.280]   Branna Wu joins Nick Bilton and Amy Webb, three of the smartest people I know to talk
[00:00:08.280 --> 00:00:12.480]   about Mark Zuckerberg's ambitions for the presidency.
[00:00:12.480 --> 00:00:14.520]   Is he really a robot?
[00:00:14.520 --> 00:00:23.640]   What Apple should do with its $250 billion in cash and a great plan to save Twitter?
[00:00:23.640 --> 00:00:24.880]   I never thought of it.
[00:00:24.880 --> 00:00:26.560]   It's all coming up next on Twit.
[00:00:26.560 --> 00:00:32.280]   Net-cast you love.
[00:00:32.280 --> 00:00:35.920]   From people you trust.
[00:00:35.920 --> 00:00:39.760]   This is Twit.
[00:00:39.760 --> 00:00:50.720]   Bandwidth for this weekend tech is provided by CashFly at CACHEFLY.com.
[00:00:50.720 --> 00:00:57.840]   This is Twit, this weekend tech, episode 613, recorded Sunday, May 7, 2017.
[00:00:57.840 --> 00:01:01.080]   My husband's an eye doctor.
[00:01:01.080 --> 00:01:06.020]   This weekend tech is brought to you by CASPER, an online retailer or premium mattress's
[00:01:06.020 --> 00:01:10.160]   or fraction of the price because everyone deserves a great night's sleep.
[00:01:10.160 --> 00:01:15.960]   Get $50 off any mattress purchase by visiting CASPER.com/Twit and entering the promo code
[00:01:15.960 --> 00:01:16.960]   Twit.
[00:01:16.960 --> 00:01:20.160]   And by Stamps.com.
[00:01:20.160 --> 00:01:24.680]   After using your time more effectively with Stamps.com, use Stamps.com to buy and print
[00:01:24.680 --> 00:01:28.320]   real US postage the instant you need it right from your desk.
[00:01:28.320 --> 00:01:34.120]   To get my special offer, go to Stamps.com, click on the microphone and enter Twit.
[00:01:34.120 --> 00:01:36.200]   And by FreshBooks.
[00:01:36.200 --> 00:01:41.160]   The ridiculously easy to use cloud accounting software used by over 10 million small business
[00:01:41.160 --> 00:01:42.160]   owners.
[00:01:42.160 --> 00:01:46.440]   Try it free for 30 days at FreshBooks.com/Twit.
[00:01:46.440 --> 00:01:48.160]   And by Hover.
[00:01:48.160 --> 00:01:51.360]   Using the perfect domain name is incredibly easy with Hover.
[00:01:51.360 --> 00:02:00.600]   Go to Hover.com/Twit and save 10% off your first purchase.
[00:02:00.600 --> 00:02:03.960]   It's time for Twit this weekend tech, the show where we cover the latest tech news.
[00:02:03.960 --> 00:02:08.280]   This panel is so prestigious, so esteemed, so wonderful.
[00:02:08.280 --> 00:02:12.200]   I am just a bump on a log.
[00:02:12.200 --> 00:02:13.440]   First of all, they're all authors.
[00:02:13.440 --> 00:02:15.080]   One's running for Congress.
[00:02:15.080 --> 00:02:16.080]   I mean, this is amazing.
[00:02:16.080 --> 00:02:20.320]   I'll start on my right, your left, Amy Webb.
[00:02:20.320 --> 00:02:25.320]   I met Amy because she wrote a new book called The Signals Are Talking.
[00:02:25.320 --> 00:02:30.040]   How to Be a Futurist, which is crazy because that's her job.
[00:02:30.040 --> 00:02:33.320]   She's a CEO and founder of Future Today Institute.
[00:02:33.320 --> 00:02:37.520]   And she's giving away her skills, her secrets.
[00:02:37.520 --> 00:02:41.080]   And much like Steve Bannon, she's posing in front of her whiteboard.
[00:02:41.080 --> 00:02:43.880]   Did you see that picture?
[00:02:43.880 --> 00:02:45.880]   Oh, yeah.
[00:02:45.880 --> 00:02:48.120]   Yeah, but I do my thinking.
[00:02:48.120 --> 00:02:49.120]   Yeah.
[00:02:49.120 --> 00:02:53.400]   And it's changed, so we know it's not fake because it was different when I interviewed
[00:02:53.400 --> 00:02:54.400]   you.
[00:02:54.400 --> 00:02:55.400]   Nice to have you.
[00:02:55.400 --> 00:02:56.400]   Thank you for joining us.
[00:02:56.400 --> 00:02:57.400]   Yeah, good to be back.
[00:02:57.400 --> 00:02:59.040]   First time on this show.
[00:02:59.040 --> 00:03:02.400]   Not his first time he's been on this show many times, but I haven't seen him in a while
[00:03:02.400 --> 00:03:03.400]   and I'm thrilled to get him.
[00:03:03.400 --> 00:03:04.800]   Nick Bilton is here.
[00:03:04.800 --> 00:03:09.400]   He's also got a brand new book called American King, and the story of the Silk Road.
[00:03:09.400 --> 00:03:12.000]   Nick, great to see you.
[00:03:12.000 --> 00:03:13.000]   Thanks for having me.
[00:03:13.000 --> 00:03:14.000]   I'm excited to be back.
[00:03:14.000 --> 00:03:15.160]   Since, oh, yeah, you don't see.
[00:03:15.160 --> 00:03:16.160]   Seems so excited.
[00:03:16.160 --> 00:03:18.320]   I am excited.
[00:03:18.320 --> 00:03:23.040]   Right, and since we saw you last, we're now writing for Vanity Fair, a special correspondent.
[00:03:23.040 --> 00:03:24.240]   You've been doing great work there.
[00:03:24.240 --> 00:03:26.880]   You see him on CNBC, columnist.
[00:03:26.880 --> 00:03:29.560]   For the times for many years, that's when we used to have him on.
[00:03:29.560 --> 00:03:32.920]   And last book was Hatching Twitter, which did really well.
[00:03:32.920 --> 00:03:34.800]   It's great to have you back.
[00:03:34.800 --> 00:03:36.320]   And now we have two children.
[00:03:36.320 --> 00:03:38.000]   How's it been a papa?
[00:03:38.000 --> 00:03:40.720]   Exhausting, but amazing.
[00:03:40.720 --> 00:03:42.560]   It really is.
[00:03:42.560 --> 00:03:45.400]   I don't think I'm going to sleep for another two years and I haven't slept for two years
[00:03:45.400 --> 00:03:47.320]   so far, but it's amazing.
[00:03:47.320 --> 00:03:48.320]   Congratulations.
[00:03:48.320 --> 00:03:49.320]   That's awesome.
[00:03:49.320 --> 00:03:50.320]   Thank you.
[00:03:50.320 --> 00:03:51.320]   I haven't had a chance to congratulate you.
[00:03:51.320 --> 00:03:57.920]   Also, returning SpaceCatGal is here, Brianna Wu, running for Congress.
[00:03:57.920 --> 00:04:02.320]   I don't know if we have to do any equal time stuff, but if we do screw it.
[00:04:02.320 --> 00:04:03.320]   I will.
[00:04:03.320 --> 00:04:04.640]   I will keep it all nonpartisan today.
[00:04:04.640 --> 00:04:07.640]   We are just talking about tech, which is not a partisan issue.
[00:04:07.640 --> 00:04:08.640]   Oh, yeah.
[00:04:08.640 --> 00:04:09.640]   Right.
[00:04:09.640 --> 00:04:15.080]   I don't know what part is in party you're on, but it's getting more and more tech and
[00:04:15.080 --> 00:04:18.080]   politics lately.
[00:04:18.080 --> 00:04:19.280]   Brianna is running for Congress.
[00:04:19.280 --> 00:04:21.280]   Brianna Wu 2018.com.
[00:04:21.280 --> 00:04:23.280]   Was it 6th District of Massachusetts?
[00:04:23.280 --> 00:04:24.280]   It's the 8th, actually.
[00:04:24.280 --> 00:04:25.280]   Yeah.
[00:04:25.280 --> 00:04:27.000]   Going up against Stephen Lynch.
[00:04:27.000 --> 00:04:31.320]   My district did is we need some help in the tech industry out there.
[00:04:31.320 --> 00:04:34.640]   We don't have Wi-Fi vast areas of it.
[00:04:34.640 --> 00:04:37.960]   It's been really badly neglected from the tech point of view.
[00:04:37.960 --> 00:04:40.160]   I'm running to change all of that.
[00:04:40.160 --> 00:04:44.040]   Last time you were on, you really impressed people because I think everybody who watches
[00:04:44.040 --> 00:04:49.840]   this show understands the need for Congress to understand technology just a little bit
[00:04:49.840 --> 00:04:50.840]   better.
[00:04:50.840 --> 00:04:51.840]   Yeah.
[00:04:51.840 --> 00:04:57.480]   I was talking on Twitter this weekend about, I would like to open up a grant program similar
[00:04:57.480 --> 00:05:03.960]   to what we do with health research and basically opening up for information security as well.
[00:05:03.960 --> 00:05:10.400]   That people apply for information security grants and I think it's just a badly needed
[00:05:10.400 --> 00:05:11.400]   program.
[00:05:11.400 --> 00:05:17.920]   Well, come 2018, you may not be the only technologist in Washington, DC.
[00:05:17.920 --> 00:05:21.280]   Imagine yourself, a small town, Newton Falls, Ohio.
[00:05:21.280 --> 00:05:22.440]   Nothing ever happens exciting.
[00:05:22.440 --> 00:05:26.880]   You get a phone call from a company saying a billionaire philanthropist wants to have
[00:05:26.880 --> 00:05:28.080]   dinner with you next week.
[00:05:28.080 --> 00:05:29.440]   I can't tell you his name.
[00:05:29.440 --> 00:05:33.720]   In fact, I won't tell you his name until 15 minutes before he arrives, which you'll be
[00:05:33.720 --> 00:05:34.720]   willing to.
[00:05:34.720 --> 00:05:35.720]   This family said, yes.
[00:05:35.720 --> 00:05:41.680]   And here's the picture of Mark Zuckerberg meeting real people.
[00:05:41.680 --> 00:05:48.160]   The grandmother to his left looks like, Oh God.
[00:05:48.160 --> 00:05:51.480]   You know, anytime you see this picture, you assume she's looking at a smartphone, but
[00:05:51.480 --> 00:05:53.560]   there aren't any smartphones and evidence.
[00:05:53.560 --> 00:05:58.120]   They're eating on plastic plates because apparently a Mark won't eat real people's
[00:05:58.120 --> 00:05:59.120]   food.
[00:05:59.120 --> 00:06:00.120]   So they had it catered.
[00:06:00.120 --> 00:06:01.120]   Yeah.
[00:06:01.120 --> 00:06:03.120]   Did they really have a?
[00:06:03.120 --> 00:06:04.440]   Did they really have a catered?
[00:06:04.440 --> 00:06:05.440]   They had it catered.
[00:06:05.440 --> 00:06:06.840]   Oh my gosh.
[00:06:06.840 --> 00:06:14.440]   And the article, this is in Business Insider said that Mark's people were tapping away
[00:06:14.440 --> 00:06:18.000]   at their laptops in the other room like he brings an entourage.
[00:06:18.000 --> 00:06:19.600]   Of course, you bring it yet.
[00:06:19.600 --> 00:06:20.600]   So here's the thing.
[00:06:20.600 --> 00:06:21.600]   Okay.
[00:06:21.600 --> 00:06:23.760]   I really, I want to, I want to ask the panel here.
[00:06:23.760 --> 00:06:28.360]   So I wrote a piece, I don't know, a few months ago saying it sure as heck looks like Mark
[00:06:28.360 --> 00:06:30.440]   Zuckerberg's running for president.
[00:06:30.440 --> 00:06:33.720]   And there was some people that were like, absolutely, when you look at all these photos
[00:06:33.720 --> 00:06:36.640]   and there were other people like you, you're completely wrong.
[00:06:36.640 --> 00:06:39.440]   The old these photos, like what is going on?
[00:06:39.440 --> 00:06:40.440]   Like what is he doing?
[00:06:40.440 --> 00:06:43.680]   And don't tell me he's just going around the country meeting people because if he was
[00:06:43.680 --> 00:06:50.400]   doing that, why would he specifically said to this family, would you, if when the press
[00:06:50.400 --> 00:06:53.720]   calls you and they will, please tell them I'm not running for.
[00:06:53.720 --> 00:06:54.720]   President.
[00:06:54.720 --> 00:06:58.560]   So yeah, I don't think he's old enough to run for president.
[00:06:58.560 --> 00:06:59.560]   Is he?
[00:06:59.560 --> 00:07:00.560]   Yes.
[00:07:00.560 --> 00:07:01.560]   So he's.
[00:07:01.560 --> 00:07:06.600]   He turns 35 one month before the filing deadline for 2020.
[00:07:06.600 --> 00:07:08.760]   I mean, I, I think it's possible.
[00:07:08.760 --> 00:07:10.600]   I think it's highly improbable.
[00:07:10.600 --> 00:07:11.600]   And really?
[00:07:11.600 --> 00:07:12.600]   No, no.
[00:07:12.600 --> 00:07:15.200]   Why is he doing this then?
[00:07:15.200 --> 00:07:16.200]   Why is he?
[00:07:16.200 --> 00:07:19.120]   He's maybe eventually, eventually doing that.
[00:07:19.120 --> 00:07:24.760]   You've got a massive press problem on their hands right now with fake news and does this
[00:07:24.760 --> 00:07:29.360]   make people feel better about fake news to see Mark sitting at this table with catered
[00:07:29.360 --> 00:07:30.360]   food?
[00:07:30.360 --> 00:07:31.360]   I mean, really.
[00:07:31.360 --> 00:07:34.600]   So, so I think you're right, Amy.
[00:07:34.600 --> 00:07:36.760]   I think he's not running for in this cycle.
[00:07:36.760 --> 00:07:38.080]   He's not running in 2020.
[00:07:38.080 --> 00:07:40.040]   Unlike by theory, he's driving a drag.
[00:07:40.040 --> 00:07:41.040]   Oh, please.
[00:07:41.040 --> 00:07:46.240]   Oh, I have to tell you, I feel like I have a special point of view on this because, you
[00:07:46.240 --> 00:07:47.240]   know, I'm out there.
[00:07:47.240 --> 00:07:50.280]   In fact, right before I got here, I'm out there in whole Massachusetts.
[00:07:50.280 --> 00:07:51.280]   You're doing freaking.
[00:07:51.280 --> 00:07:52.920]   I, I'm doing this.
[00:07:52.920 --> 00:07:59.840]   And the thought of inviting myself over to someone's house is so deeply uncomfortable.
[00:07:59.840 --> 00:08:05.660]   And if you want to meet people, this is a completely wrong, really weird way to go
[00:08:05.660 --> 00:08:06.660]   about it.
[00:08:06.660 --> 00:08:11.720]   Like I have people that write me every week and I go, do you want to have coffee?
[00:08:11.720 --> 00:08:17.200]   And that sitting down in private and talking, it's not making it this weird media thing.
[00:08:17.200 --> 00:08:22.080]   And it's just, it's clear he brings photographers, lights, look at this picture talking to a
[00:08:22.080 --> 00:08:24.440]   fire department.
[00:08:24.440 --> 00:08:28.120]   This looks so canned, phony and staged.
[00:08:28.120 --> 00:08:32.160]   He doesn't, it looks like he's been green-screened into this.
[00:08:32.160 --> 00:08:33.160]   I okay.
[00:08:33.160 --> 00:08:39.240]   The better question is though, is it does, you know, so Brianna, you're asking people
[00:08:39.240 --> 00:08:42.640]   if they want to have, have coffee with you.
[00:08:42.640 --> 00:08:46.600]   Wouldn't, wouldn't Zuck just tell us we're going to have coffee with him through the
[00:08:46.600 --> 00:08:48.840]   miracles of AI and machine learning?
[00:08:48.840 --> 00:08:52.920]   And why bother going out into the real world when you have everybody's data, your finger
[00:08:52.920 --> 00:08:53.920]   to it?
[00:08:53.920 --> 00:08:54.920]   I think you're both right.
[00:08:54.920 --> 00:08:56.160]   I think, wait, wait, wait, hold on.
[00:08:56.160 --> 00:08:58.240]   Can we look at this, those other evidence here?
[00:08:58.240 --> 00:08:59.240]   Okay.
[00:08:59.240 --> 00:09:01.040]   To a politically, a political campaign.
[00:09:01.040 --> 00:09:08.840]   So they updated their, S1 filing with the SEC to note that, that if Mark Zuckerberg decides
[00:09:08.840 --> 00:09:13.000]   to run for office and he could still remain CEO, why would you, why would you put that
[00:09:13.000 --> 00:09:14.520]   in there if that didn't mean anything?
[00:09:14.520 --> 00:09:20.920]   They've been hiring people from the Obama campaign on the foundation.
[00:09:20.920 --> 00:09:22.680]   There's just all these things that he's been doing.
[00:09:22.680 --> 00:09:26.200]   And these photos, it doesn't add up.
[00:09:26.200 --> 00:09:30.800]   It's, you know, those people have been telling me, telling me, oh, well, it's just a PR push.
[00:09:30.800 --> 00:09:35.720]   But it's giving them more negative PR around this whole president thing.
[00:09:35.720 --> 00:09:36.720]   If the goal is-
[00:09:36.720 --> 00:09:38.600]   What the hell does he want to be president for though?
[00:09:38.600 --> 00:09:39.600]   I mean, honestly.
[00:09:39.600 --> 00:09:40.880]   So I have a theory.
[00:09:40.880 --> 00:09:44.680]   I don't know if it's president because I do think that it's too small of a job for him.
[00:09:44.680 --> 00:09:45.680]   I wonder if-
[00:09:45.680 --> 00:09:47.400]   He wants to be emperor of the world?
[00:09:47.400 --> 00:09:48.400]   He wants to be emperor.
[00:09:48.400 --> 00:09:50.200]   What's bigger than president of the United States?
[00:09:50.200 --> 00:09:54.640]   No, maybe he's going to try out for like governor or even mayor.
[00:09:54.640 --> 00:09:57.480]   I don't know something kind of, you know, as a testing ground.
[00:09:57.480 --> 00:09:58.480]   But there's something going on.
[00:09:58.480 --> 00:09:59.720]   There's clearly something going on.
[00:09:59.720 --> 00:10:01.400]   I think Amy, you're right.
[00:10:01.400 --> 00:10:02.400]   There is a question.
[00:10:02.400 --> 00:10:05.920]   He, in fact, many of our panelists have said this in the past.
[00:10:05.920 --> 00:10:08.560]   He's already got a very, probably the, he's a king.
[00:10:08.560 --> 00:10:09.560]   A king maker.
[00:10:09.560 --> 00:10:10.560]   He doesn't need to be the king.
[00:10:10.560 --> 00:10:12.640]   Always better to be the king maker.
[00:10:12.640 --> 00:10:18.320]   The ratio you rather than, you know, because then you have more power, frankly, the king
[00:10:18.320 --> 00:10:19.320]   maker.
[00:10:19.320 --> 00:10:22.160]   However, so I don't think he's running in 2020.
[00:10:22.160 --> 00:10:27.840]   I think though that this is a, he's playing the long game that at some point he would look
[00:10:27.840 --> 00:10:28.840]   at this.
[00:10:28.840 --> 00:10:31.240]   You don't do that because of fake news.
[00:10:31.240 --> 00:10:36.320]   You don't feed a cow.
[00:10:36.320 --> 00:10:37.800]   I don't know.
[00:10:37.800 --> 00:10:46.120]   I got to say, because I'm, you know, we advise many, many, many news organizations and also
[00:10:46.120 --> 00:10:48.560]   not this administration, but the previous administration.
[00:10:48.560 --> 00:10:51.600]   And he's got a pretty massive problem on his hands.
[00:10:51.600 --> 00:10:56.600]   And if every news organization in the country, in our country, decided to cut off the tap,
[00:10:56.600 --> 00:10:58.520]   that would be a tremendous problem.
[00:10:58.520 --> 00:10:59.920]   And they're doing it, right?
[00:10:59.920 --> 00:11:02.200]   Instant articles has lost the New York Times.
[00:11:02.200 --> 00:11:04.440]   They've lost the Washington Post.
[00:11:04.440 --> 00:11:08.840]   So I, there may be part of us, maybe politics, but I also think that there's a piece of this
[00:11:08.840 --> 00:11:10.640]   that's, that's a PR.
[00:11:10.640 --> 00:11:13.800]   Amy, how does this help?
[00:11:13.800 --> 00:11:14.920]   Fake news or instant articles.
[00:11:14.920 --> 00:11:17.320]   I don't understand how this helps anything.
[00:11:17.320 --> 00:11:19.800]   So am I right to connect the dots?
[00:11:19.800 --> 00:11:20.800]   Yeah.
[00:11:20.800 --> 00:11:22.840]   No, I don't, I don't think that it, uh, this is your job.
[00:11:22.840 --> 00:11:24.560]   I mean, this is what you do.
[00:11:24.560 --> 00:11:27.080]   You, you, you're predicting the future for us.
[00:11:27.080 --> 00:11:29.040]   So here's, here's the, here's the thing.
[00:11:29.040 --> 00:11:35.720]   I, you know, my, my hunch here is that we're also easily distracted no matter how smart we
[00:11:35.720 --> 00:11:39.360]   are and how, you know, prestigious our jobs might be.
[00:11:39.360 --> 00:11:41.520]   We get distracted over and over again.
[00:11:41.520 --> 00:11:43.400]   And I, my first career was as a journalist.
[00:11:43.400 --> 00:11:46.720]   So this is not about me, you know, disliking journalists or anything like that.
[00:11:46.720 --> 00:11:48.600]   I just think we get distracted.
[00:11:48.600 --> 00:11:54.560]   And I think that, um, that, that campaign, if, you know, whether it was for politics
[00:11:54.560 --> 00:12:01.080]   or for straight up, you know, graciating Zuck and Facebook, Facebook back into everybody's
[00:12:01.080 --> 00:12:06.800]   good graces, I think was more about, um, was a lot about likability.
[00:12:06.800 --> 00:12:10.000]   And I think that that has a reverberating effect.
[00:12:10.000 --> 00:12:11.480]   I really do.
[00:12:11.480 --> 00:12:13.880]   I think you have to look at the big picture here too.
[00:12:13.880 --> 00:12:19.520]   I mean, look at he recently gave away a huge portion of his wealth for, you know, philanthropy
[00:12:19.520 --> 00:12:24.520]   worldwide and, well, I'm a sort of.
[00:12:24.520 --> 00:12:25.520]   I know it's quirky.
[00:12:25.520 --> 00:12:26.880]   I know it's quirky.
[00:12:26.880 --> 00:12:31.920]   But my point there is he's trying to raise an international presence here.
[00:12:31.920 --> 00:12:38.080]   And when he has a positive view by the mass public, I think it enables him to, you know,
[00:12:38.080 --> 00:12:42.720]   do a lot of things like if Facebook is trying to, you know, work with local governments,
[00:12:42.720 --> 00:12:44.920]   it's a huge part of what they end up doing.
[00:12:44.920 --> 00:12:50.560]   So I see this as a, an overall political move meant to open many doors for him and maybe
[00:12:50.560 --> 00:12:54.680]   he runs for president, maybe he doesn't, but it certainly makes a lot of the things
[00:12:54.680 --> 00:12:57.000]   Facebook wants to do just easier.
[00:12:57.000 --> 00:13:01.560]   So I would mention to Mark's team that just remember Mike Dukakis in the tank with the
[00:13:01.560 --> 00:13:06.600]   helmet that picture with basketball players does not show it again.
[00:13:06.600 --> 00:13:08.960]   Does not raise Suck's stature.
[00:13:08.960 --> 00:13:11.600]   That's, that is probably the one you want to throw out.
[00:13:11.600 --> 00:13:13.720]   Just, um, I'm just saying.
[00:13:13.720 --> 00:13:15.480]   Also I think here's the thing.
[00:13:15.480 --> 00:13:21.160]   This is pretend that the four of us run Mark's public relations team.
[00:13:21.160 --> 00:13:22.160]   Can you imagine?
[00:13:22.160 --> 00:13:25.880]   So, you know, oh guys, it's been this huge problem with fake news.
[00:13:25.880 --> 00:13:26.880]   What do we do?
[00:13:26.880 --> 00:13:30.920]   And someone says, I know let's go take a picture of Mark feeding a cow.
[00:13:30.920 --> 00:13:32.920]   Well, remember that.
[00:13:32.920 --> 00:13:37.760]   And wait a minute, but wait, Nick, how many different places did we all see that photo?
[00:13:37.760 --> 00:13:42.160]   I completely agree with you, but it works in that respect.
[00:13:42.160 --> 00:13:43.160]   That's right.
[00:13:43.160 --> 00:13:44.160]   That's right.
[00:13:44.160 --> 00:13:50.920]   He did, you know, Mark does, as we know, these things like he was going to only cook and
[00:13:50.920 --> 00:13:51.920]   kill his own.
[00:13:51.920 --> 00:13:56.880]   He was only eating meaty, he was going to learn Chinese and actually did, which is kind of
[00:13:56.880 --> 00:14:00.200]   impressive having tried that for myself for four years in school.
[00:14:00.200 --> 00:14:02.800]   It's not easy.
[00:14:02.800 --> 00:14:06.760]   So one of his goals for this year was to go to all 50 states, talk to people in all
[00:14:06.760 --> 00:14:07.760]   50 states.
[00:14:07.760 --> 00:14:12.840]   So you could just say on the surface that's he's just doing his weird kind of, I'm a billionaire.
[00:14:12.840 --> 00:14:13.840]   I could do anything I want.
[00:14:13.840 --> 00:14:17.320]   So instead of building a rocket ship, I'm going to meet everybody and every.
[00:14:17.320 --> 00:14:18.320]   I don't know.
[00:14:18.320 --> 00:14:20.840]   I didn't see photos of him like studying conjure or whatever.
[00:14:20.840 --> 00:14:22.760]   I'm not getting verbs.
[00:14:22.760 --> 00:14:24.800]   Like sitting in the house.
[00:14:24.800 --> 00:14:25.800]   Knee how.
[00:14:25.800 --> 00:14:26.800]   How?
[00:14:26.800 --> 00:14:27.800]   Did you see photos of him killing his animals?
[00:14:27.800 --> 00:14:28.800]   No.
[00:14:28.800 --> 00:14:30.640]   So why are we seeing photos of this stuff?
[00:14:30.640 --> 00:14:31.640]   Let's pee.
[00:14:31.640 --> 00:14:32.640]   Okay.
[00:14:32.640 --> 00:14:33.640]   So we all agree it's PR.
[00:14:33.640 --> 00:14:34.640]   Oh, yes.
[00:14:34.640 --> 00:14:36.200]   Oh, look at medium generals.
[00:14:36.200 --> 00:14:37.760]   Now that's important.
[00:14:37.760 --> 00:14:39.560]   You got to you got to meet generals.
[00:14:39.560 --> 00:14:43.480]   If you're going to, is that an American general?
[00:14:43.480 --> 00:14:46.720]   What you want to form is that I've never seen that hat before.
[00:14:46.720 --> 00:14:47.720]   What is that?
[00:14:47.720 --> 00:14:48.720]   I like the thing.
[00:14:48.720 --> 00:14:50.280]   I do want to say one other thing.
[00:14:50.280 --> 00:14:57.200]   So there was when when I written that that family fair piece, BuzzFeed went along to,
[00:14:57.200 --> 00:15:00.240]   to Zuck, they got access to him and they said, are you running for president?
[00:15:00.240 --> 00:15:01.640]   He said, no.
[00:15:01.640 --> 00:15:05.120]   And then it was this big story like Zuck says he's not running for president.
[00:15:05.120 --> 00:15:07.720]   Of course he's not going to say he's running for president.
[00:15:07.720 --> 00:15:11.960]   If Facebook stock would literally drop 20, 30%, five seconds.
[00:15:11.960 --> 00:15:16.400]   So so it's, you know, I think that there's we should, we should not take it as base value
[00:15:16.400 --> 00:15:19.640]   when he says no or don't tell people I'm running for president.
[00:15:19.640 --> 00:15:23.240]   There is something going on and we should, we should be questioning it.
[00:15:23.240 --> 00:15:28.160]   Yeah, it was a Mar, I'm told a Mar Pat uniform from the US army.
[00:15:28.160 --> 00:15:29.160]   Thank you.
[00:15:29.160 --> 00:15:32.160]   Well, our, our, our chat room knows everything.
[00:15:32.160 --> 00:15:34.920]   There's nothing goes, nothing gets by them.
[00:15:34.920 --> 00:15:37.920]   So it's clearly PR.
[00:15:37.920 --> 00:15:42.120]   It's clearly designed to cultivate a certain image.
[00:15:42.120 --> 00:15:49.240]   Do you think that he, I mean, he's hiring people, as you said, with political operatives,
[00:15:49.240 --> 00:15:54.400]   he's, I think this is going to be the most carefully orchestrated campaign we've ever
[00:15:54.400 --> 00:16:00.720]   seen and that closer to 2020, there will be a draft Zuckerberg movement.
[00:16:00.720 --> 00:16:02.760]   I don't think he's ever going to announce.
[00:16:02.760 --> 00:16:08.960]   I think he, I think what he will do is arrange it so that he can't, the drum beat and he
[00:16:08.960 --> 00:16:11.600]   eventually says, look, I don't want to do this.
[00:16:11.600 --> 00:16:17.640]   I'm happy running Facebook, but I can see that America needs me.
[00:16:17.640 --> 00:16:25.480]   And then he's going to say, and I need this group of people here to be my campaign managers.
[00:16:25.480 --> 00:16:26.480]   Yeah.
[00:16:26.480 --> 00:16:30.520]   And then I'm going to call Mike judge and ask him to, to do a new series.
[00:16:30.520 --> 00:16:34.400]   That will be amazing.
[00:16:34.400 --> 00:16:38.280]   So I have to say though, that you can a little bit blame this in the last election, because
[00:16:38.280 --> 00:16:45.640]   I think when people like Zuckerberg, and I'll include Jason Calicannis, who wants to run
[00:16:45.640 --> 00:16:50.760]   for mayor of San Francisco, saw that somebody who has only business experience, marginal
[00:16:50.760 --> 00:16:55.400]   business experience and no political experience and knows, no record of, of political service
[00:16:55.400 --> 00:16:58.080]   or service of any kind can become a,
[00:16:58.080 --> 00:17:02.320]   elected color orange and is the can be elected president.
[00:17:02.320 --> 00:17:08.000]   It's, I think it might have, I can see in their heart of hearts, they might have said,
[00:17:08.000 --> 00:17:10.760]   Oh, well, yeah, I could, why not me?
[00:17:10.760 --> 00:17:12.680]   Oh, that's certainly why I'm bringing Leo.
[00:17:12.680 --> 00:17:14.800]   I mean, but it's, it's less of that.
[00:17:14.800 --> 00:17:20.600]   It's like, you know, I think there are so many people across this country that never imagined
[00:17:20.600 --> 00:17:26.280]   themselves as politicians and see that they don't have a choice other than to engage this
[00:17:26.280 --> 00:17:29.720]   political system because the status quo is so terrifying.
[00:17:29.720 --> 00:17:32.600]   I think that is why you're seeing an avalanche of women running.
[00:17:32.600 --> 00:17:37.200]   So the thing that scares me the most is that people are going to get turned off to politics.
[00:17:37.200 --> 00:17:38.200]   Yeah.
[00:17:38.200 --> 00:17:39.640]   That's why I'm glad to see you running.
[00:17:39.640 --> 00:17:40.640]   Thank you.
[00:17:40.640 --> 00:17:44.160]   Because the biggest fear that I have at this point is that all the cynics who have said
[00:17:44.160 --> 00:17:45.960]   all along see it's rigged.
[00:17:45.960 --> 00:17:47.680]   It doesn't matter.
[00:17:47.680 --> 00:17:50.160]   Are just getting confirmation now.
[00:17:50.160 --> 00:17:53.360]   And they look at the Democrats and they look at the Republicans and they, and they throw
[00:17:53.360 --> 00:17:56.880]   up their hands and say, screw it.
[00:17:56.880 --> 00:18:01.880]   So well, it isn't a lot of, I think, whatever I think, I think some of what we're seeing
[00:18:01.880 --> 00:18:09.040]   with, with Mark and I think some of what we're seeing right now is, is just a reaction to
[00:18:09.040 --> 00:18:12.080]   the two party system that's gone off the rails in the US.
[00:18:12.080 --> 00:18:13.080]   Right.
[00:18:13.080 --> 00:18:15.360]   And that's, and everything is polarized.
[00:18:15.360 --> 00:18:18.240]   Technology has become polarized and politicized.
[00:18:18.240 --> 00:18:23.720]   You know, health, all of these facets of our everyday lives are subject to morning talk
[00:18:23.720 --> 00:18:28.040]   shows and, you know, political wrangling.
[00:18:28.040 --> 00:18:32.120]   And I think that's just, you know, sort of stoke this great frustration that I certainly
[00:18:32.120 --> 00:18:34.080]   feel and I'm sure a whole bunch of people feel.
[00:18:34.080 --> 00:18:39.000]   And that's certainly, I'm sure, prodded some people to think, well, you know, I'm outside
[00:18:39.000 --> 00:18:40.000]   the system.
[00:18:40.000 --> 00:18:43.400]   I guess I'll have to pick a side in order to run or maybe Mark is starting a whole third
[00:18:43.400 --> 00:18:44.400]   party.
[00:18:44.400 --> 00:18:46.040]   I mean, maybe that's what's going on.
[00:18:46.040 --> 00:18:47.040]   I don't know.
[00:18:47.040 --> 00:18:49.040]   But actually that's plausible.
[00:18:49.040 --> 00:18:52.360]   That was something I think we're like the technocrat party.
[00:18:52.360 --> 00:18:53.360]   Oh, wow.
[00:18:53.360 --> 00:18:55.120]   All right, that idea.
[00:18:55.120 --> 00:18:56.960]   Did you want to be in that Brianna?
[00:18:56.960 --> 00:18:59.720]   Well, there are, I think we need technologists running.
[00:18:59.720 --> 00:19:00.720]   I really do.
[00:19:00.720 --> 00:19:02.080]   I would say this.
[00:19:02.080 --> 00:19:07.480]   I think the axis is not, I think like it's becoming very dated to think of as Democrat
[00:19:07.480 --> 00:19:09.600]   versus Republican.
[00:19:09.600 --> 00:19:15.400]   What I hear when I'm out there every single week is unmedicated fury at the system.
[00:19:15.400 --> 00:19:18.000]   It doesn't matter if the person is right or left.
[00:19:18.000 --> 00:19:20.480]   They're angry at the broken status quo.
[00:19:20.480 --> 00:19:24.360]   So what I'm always looking to do is just find the part.
[00:19:24.360 --> 00:19:25.600]   So I agree with the people.
[00:19:25.600 --> 00:19:29.520]   I don't even ask them what party they belong to because I think it's just about a system
[00:19:29.520 --> 00:19:31.040]   is not working for any of us.
[00:19:31.040 --> 00:19:32.040]   So I wholly, I don't.
[00:19:32.040 --> 00:19:34.600]   But it's totally, but it's totally entrenched.
[00:19:34.600 --> 00:19:35.600]   So here's the problem.
[00:19:35.600 --> 00:19:39.400]   The problem is that the system doesn't work.
[00:19:39.400 --> 00:19:43.400]   And I know this because of all the work we do in DC.
[00:19:43.400 --> 00:19:47.720]   It doesn't work without those two parties for a whole bunch of reasons.
[00:19:47.720 --> 00:19:48.720]   Sure.
[00:19:48.720 --> 00:19:49.720]   The machine.
[00:19:49.720 --> 00:19:51.120]   The machine is set up that way.
[00:19:51.120 --> 00:19:56.800]   And it would be very, very difficult to introduce, I think, a third party at all.
[00:19:56.800 --> 00:20:03.560]   Now that being said, AAAS and Rush Holt, who was a former Senator, sorry, representative,
[00:20:03.560 --> 00:20:09.960]   and is now the CEO of the AAAS, he's trying to encourage a whole bunch of people in the
[00:20:09.960 --> 00:20:12.000]   science and tech sector to run.
[00:20:12.000 --> 00:20:13.000]   Good.
[00:20:13.000 --> 00:20:16.760]   I think that's all I have to say, though, I worry when I hear that we should need more
[00:20:16.760 --> 00:20:20.000]   technologists in government because I don't know if that's really the right answer or
[00:20:20.000 --> 00:20:21.320]   we need more scientists to come up.
[00:20:21.320 --> 00:20:22.920]   We need more smart people in government.
[00:20:22.920 --> 00:20:23.920]   I'll grant you that.
[00:20:23.920 --> 00:20:24.920]   We do need.
[00:20:24.920 --> 00:20:25.920]   No, no.
[00:20:25.920 --> 00:20:30.840]   So here's the problem is we are entering an era where in the next X number of years, whether
[00:20:30.840 --> 00:20:34.560]   it's two years or five years or 10 years or 20, but we're going to have driverless cars
[00:20:34.560 --> 00:20:37.600]   are going to be taking millions of jobs and artificial intelligence and all these different
[00:20:37.600 --> 00:20:39.480]   things that are going to happen.
[00:20:39.480 --> 00:20:44.200]   Do you think Donald Trump is equipped to deal with that?
[00:20:44.200 --> 00:20:45.200]   Do you think Hillary Clinton was?
[00:20:45.200 --> 00:20:52.080]   No, you don't want to Steve Mnuchin, our Secretary of Treasury, who says, oh, it's
[00:20:52.080 --> 00:20:54.400]   50 to 100 years away.
[00:20:54.400 --> 00:20:59.240]   Yeah, automation is not even on our radar because nobody's going to lose their jobs
[00:20:59.240 --> 00:21:00.240]   for decades.
[00:21:00.240 --> 00:21:01.240]   Right.
[00:21:01.240 --> 00:21:02.960]   So here's my million truckers.
[00:21:02.960 --> 00:21:03.960]   So here's the thing.
[00:21:03.960 --> 00:21:10.280]   So I wrote an op-ed in the LA Times pretty recently about that statement and what it implies.
[00:21:10.280 --> 00:21:16.400]   I am concerned because there are so many jobs in state that still haven't been filled this
[00:21:16.400 --> 00:21:18.840]   far into the cycle.
[00:21:18.840 --> 00:21:24.840]   But what I would say is that within state, the State Department, there was a mass exodus,
[00:21:24.840 --> 00:21:31.160]   but there are still lots and lots of people there with PhDs who put thought and rational
[00:21:31.160 --> 00:21:34.000]   thought before politics.
[00:21:34.000 --> 00:21:39.320]   In two weeks, I'm going to be at the National Academies with some heads of automation and
[00:21:39.320 --> 00:21:45.240]   some car company people and government people hashing out the details of the future of self-driving
[00:21:45.240 --> 00:21:47.680]   cars and regulation.
[00:21:47.680 --> 00:21:51.320]   And quite frankly, that work is going to continue whether or not Trump is in office.
[00:21:51.320 --> 00:21:52.320]   Yes, of course.
[00:21:52.320 --> 00:21:58.120]   I think that he is significantly outside his debt.
[00:21:58.120 --> 00:22:01.280]   And that was an interesting slip.
[00:22:01.280 --> 00:22:05.400]   There's two messages about it, as it turns out.
[00:22:05.400 --> 00:22:14.360]   But I guess what I'm trying to say is the future comes whether or not we wanted to.
[00:22:14.360 --> 00:22:18.200]   And so there are meetings happening behind closed doors that don't get publicized because
[00:22:18.200 --> 00:22:22.160]   they can't get publicized, where people are talking about this and policy is getting
[00:22:22.160 --> 00:22:23.720]   made.
[00:22:23.720 --> 00:22:26.960]   And there are stumbling blocks and problems along the way.
[00:22:26.960 --> 00:22:33.920]   Ajit Patel, who's our head of the FCC, is making questionable decisions that trickle
[00:22:33.920 --> 00:22:34.920]   down.
[00:22:34.920 --> 00:22:39.520]   But ultimately, well, we all have to soldier on.
[00:22:39.520 --> 00:22:44.560]   If you're curious, and I know I am, what Mark Zuckerberg does first thing in the morning,
[00:22:44.560 --> 00:22:47.760]   we brought in comedian Jerry Seinfeld to ask him.
[00:22:47.760 --> 00:22:50.440]   I want to know the very first thing you do.
[00:22:50.440 --> 00:22:51.960]   You get out of the bed.
[00:22:51.960 --> 00:22:52.960]   You go to the bathroom.
[00:22:52.960 --> 00:22:53.960]   Oh, I don't know.
[00:22:53.960 --> 00:22:55.480]   The first thing I do is look at my phone.
[00:22:55.480 --> 00:22:58.280]   Before he goes to the bathroom, ladies and gentlemen.
[00:22:58.280 --> 00:23:00.280]   I look at Facebook.
[00:23:00.280 --> 00:23:01.280]   All right.
[00:23:01.280 --> 00:23:02.280]   Thank you very much.
[00:23:02.280 --> 00:23:08.480]   We're going to take a break and come back in just a second.
[00:23:08.480 --> 00:23:13.480]   What if the guy in the photos is actually an artificial intelligent robot?
[00:23:13.480 --> 00:23:16.920]   He doesn't like he has the complexion of data.
[00:23:16.920 --> 00:23:21.360]   The country and is going to run for president to win a touring test.
[00:23:21.360 --> 00:23:23.720]   I am the president now.
[00:23:23.720 --> 00:23:24.720]   He does not look human.
[00:23:24.720 --> 00:23:26.720]   I just want to point that out.
[00:23:26.720 --> 00:23:35.120]   He's not the Chinese lexical database though.
[00:23:35.120 --> 00:23:39.200]   So I can speak Chinese and control my home.
[00:23:39.200 --> 00:23:44.320]   I am going to milk this cow now.
[00:23:44.320 --> 00:23:45.320]   There he's smiling.
[00:23:45.320 --> 00:23:48.800]   It's the first one I've seen where he's actually looks like he's having fun.
[00:23:48.800 --> 00:23:50.920]   But that's because he's having fried chicken and waffles.
[00:23:50.920 --> 00:23:51.920]   And who isn't smiling?
[00:23:51.920 --> 00:23:53.720]   I'm not going to have a fried chicken and waffles.
[00:23:53.720 --> 00:23:57.440]   Nick, I'm never going to look at a picture of Mark again and not think about that.
[00:23:57.440 --> 00:23:58.440]   Like you've ruined it for me.
[00:23:58.440 --> 00:23:59.440]   Look at him.
[00:23:59.440 --> 00:24:00.440]   You're now on it's side work.
[00:24:00.440 --> 00:24:01.440]   That's all it is.
[00:24:01.440 --> 00:24:02.440]   That's all it is.
[00:24:02.440 --> 00:24:07.080]   Cybernetic organism created to mimic human thought.
[00:24:07.080 --> 00:24:11.360]   Oh, I'm talking to firemen.
[00:24:11.360 --> 00:24:15.000]   What was he talking to them about?
[00:24:15.000 --> 00:24:17.040]   That's what I wanted to know.
[00:24:17.040 --> 00:24:21.760]   Well, so you guys actually drive with a truck?
[00:24:21.760 --> 00:24:25.360]   Look at this.
[00:24:25.360 --> 00:24:30.800]   This is just.
[00:24:30.800 --> 00:24:31.800]   What is he saying right now?
[00:24:31.800 --> 00:24:32.800]   Get me out of here.
[00:24:32.800 --> 00:24:37.520]   I love our t-shirt to the right to the to his right love potion.
[00:24:37.520 --> 00:24:38.520]   Probably number nine.
[00:24:38.520 --> 00:24:43.400]   How much how much time in advance did they do you guys know how long like what between
[00:24:43.400 --> 00:24:46.520]   the call coming in 15, 15, 15, they knew.
[00:24:46.520 --> 00:24:51.200]   So they knew it was a billionaire philanthropist from Silicon Valley.
[00:24:51.200 --> 00:24:55.920]   They knew it was somebody that 90% of the people in the US use his product.
[00:24:55.920 --> 00:24:59.120]   I'm sure from that they could figure out who it was.
[00:24:59.120 --> 00:25:03.320]   So they're an interesting I remember years ago when I was at the times got a tip and did
[00:25:03.320 --> 00:25:09.800]   a story on there was a woman that I knew who lived in San Francisco and a very beautiful
[00:25:09.800 --> 00:25:11.800]   house and she got a knock on the door.
[00:25:11.800 --> 00:25:14.080]   I didn't actually put these two things together.
[00:25:14.080 --> 00:25:19.680]   She got a knock on the door from a lawyer and he said, I represent a very wealthy client
[00:25:19.680 --> 00:25:21.920]   who wants your house.
[00:25:21.920 --> 00:25:24.880]   You have and they offered in the house is worth like two and a half million.
[00:25:24.880 --> 00:25:25.880]   They offered.
[00:25:25.880 --> 00:25:31.080]   He said, my client is willing to pay five million dollars in cash and you have 24 hours to decide.
[00:25:31.080 --> 00:25:32.080]   Wow.
[00:25:32.080 --> 00:25:33.080]   And it turned out it was.
[00:25:33.080 --> 00:25:35.080]   He bought the neighborhood around him.
[00:25:35.080 --> 00:25:37.320]   And then they bought the neighborhood around him.
[00:25:37.320 --> 00:25:41.600]   But he's got this like it's this like way of saying we will be there in 15 minutes when
[00:25:41.600 --> 00:25:42.600]   I wrote.
[00:25:42.600 --> 00:25:43.600]   Yeah.
[00:25:43.600 --> 00:25:44.600]   That's so creepy.
[00:25:44.600 --> 00:25:50.840]   I used that blue light and slasher thing after they all leave.
[00:25:50.840 --> 00:25:52.560]   Resistance is futile.
[00:25:52.560 --> 00:25:53.760]   Resistance is futile.
[00:25:53.760 --> 00:25:55.920]   This is the thing to remember.
[00:25:55.920 --> 00:26:00.080]   If I ever do anything like that, just oh my God, just somebody come to my house and
[00:26:00.080 --> 00:26:02.560]   just slap me like I do not want to be that.
[00:26:02.560 --> 00:26:04.200]   Remember what F. Scott Fitzgerald said.
[00:26:04.200 --> 00:26:06.200]   He said, the rich are not like you and me.
[00:26:06.200 --> 00:26:10.480]   They really somebody this rich and by the way Fitzgerald was not talking about people
[00:26:10.480 --> 00:26:11.640]   as rich as this.
[00:26:11.640 --> 00:26:13.560]   No one's ever seen people.
[00:26:13.560 --> 00:26:15.760]   This rich, not JP Morgan, not Rockford.
[00:26:15.760 --> 00:26:17.680]   He's never existed in the history of conduct.
[00:26:17.680 --> 00:26:22.880]   These people are you know, people like, you know, Larry Page, Mark Zuckerberg.
[00:26:22.880 --> 00:26:24.880]   These are masters of the universe.
[00:26:24.880 --> 00:26:29.760]   They have virtually unlimited power, which begs the question, Amy, why would you want
[00:26:29.760 --> 00:26:31.400]   to be president?
[00:26:31.400 --> 00:26:36.000]   Even Trump is realizing that he presidents no, no better roses.
[00:26:36.000 --> 00:26:39.000]   I got to sleep in this musty old bedroom.
[00:26:39.000 --> 00:26:40.000]   Yeah.
[00:26:40.000 --> 00:26:41.000]   I can't.
[00:26:41.000 --> 00:26:42.440]   I mean, you don't get autonomy.
[00:26:42.440 --> 00:26:48.760]   I just I cannot imagine any smart person would be watching the circus in DC right now saying,
[00:26:48.760 --> 00:26:52.040]   A, I think I could do better and B, that sounds like something fun that I would want
[00:26:52.040 --> 00:26:53.040]   to do.
[00:26:53.040 --> 00:26:54.320]   I just I don't see it.
[00:26:54.320 --> 00:26:55.320]   Don't you think those.
[00:26:55.320 --> 00:26:58.800]   Zuckerberg thinks thinks of himself as an altruist.
[00:26:58.800 --> 00:27:01.840]   Like I, you know, I'm just trying to connect people.
[00:27:01.840 --> 00:27:05.040]   I know the world will be a better place if we're all connected.
[00:27:05.040 --> 00:27:11.640]   He really thinks and maybe it's true too that he is not in it for the money.
[00:27:11.640 --> 00:27:14.400]   He's not in it for personal enrichment or power.
[00:27:14.400 --> 00:27:16.000]   He's an altruist.
[00:27:16.000 --> 00:27:21.720]   And what more altruistic thing to do than to step off your magic golden throne into this,
[00:27:21.720 --> 00:27:26.520]   you know, beat up the old house in DC that was built by slaves 200 years ago.
[00:27:26.520 --> 00:27:30.160]   And maybe that's part of the getting, getting wealthy and having this.
[00:27:30.160 --> 00:27:31.520]   I think it warps you, right?
[00:27:31.520 --> 00:27:35.960]   Because if you look at a lot of the grand plans coming out of the valley, most of them
[00:27:35.960 --> 00:27:37.720]   aren't practical, right?
[00:27:37.720 --> 00:27:39.720]   Oh, they're like sea standing.
[00:27:39.720 --> 00:27:43.480]   And maybe there's another thing that's going to happen here.
[00:27:43.480 --> 00:27:46.200]   Maybe he's just going to knock on the White House door.
[00:27:46.200 --> 00:27:50.400]   He's going to have someone show up, knock on the door and say, Hey, Donald Trump, I
[00:27:50.400 --> 00:27:51.760]   don't know what this house is worth.
[00:27:51.760 --> 00:27:52.760]   I'm going to double.
[00:27:52.760 --> 00:27:53.760]   24 hours.
[00:27:53.760 --> 00:28:03.000]   I think there's a Russian oligarch who already did that is speaking to you.
[00:28:03.000 --> 00:28:07.880]   Let's take a break on that note and return with something else, not political.
[00:28:07.880 --> 00:28:10.280]   We weren't going to get political, were we?
[00:28:10.280 --> 00:28:11.280]   We got a great panel though.
[00:28:11.280 --> 00:28:15.600]   How can you not with Nick built from Vanity Fair, his new book, American Kingpin, the
[00:28:15.600 --> 00:28:21.040]   story of the epic hunt for the criminal mastermind of the Silk Road.
[00:28:21.040 --> 00:28:23.320]   And that is a story and a half.
[00:28:23.320 --> 00:28:28.840]   It is, it is a story and the crazy story I've ever worked on in my 15 years as a journalist.
[00:28:28.840 --> 00:28:31.440]   Chapter 39, kidney for sale.
[00:28:31.440 --> 00:28:33.240]   I'll just say that.
[00:28:33.240 --> 00:28:36.880]   Also with us, Amy Webb, she's the author of a brand new book called the signals are
[00:28:36.880 --> 00:28:40.080]   talking how to become a futurist.
[00:28:40.080 --> 00:28:43.280]   If you know where to look, the future can be yours.
[00:28:43.280 --> 00:28:46.760]   It's actually a fascinating book that reveals her secrets.
[00:28:46.760 --> 00:28:53.000]   She's CEO and founder of the future today Institute and the wonderful Brianna Wu, game
[00:28:53.000 --> 00:29:02.440]   developer and candidate for Congress in the Massachusetts eighth, Brianna Wu 2018.com.
[00:29:02.440 --> 00:29:04.600]   Our show today brought to you by my mattress.
[00:29:04.600 --> 00:29:06.200]   Let's bring this down to earth.
[00:29:06.200 --> 00:29:09.400]   We talk all the time about health and one of the keys we're learning more and more to
[00:29:09.400 --> 00:29:11.400]   good health is a good night's sleep.
[00:29:11.400 --> 00:29:14.600]   This is not like a truism.
[00:29:14.600 --> 00:29:16.200]   This is actually true.
[00:29:16.200 --> 00:29:20.320]   It's not I know every your mom told you this your grandma, but it's true.
[00:29:20.320 --> 00:29:25.240]   And one of the things that makes it good night's sleep is sleeping cool, sleeping comfortable,
[00:29:25.240 --> 00:29:29.680]   sleeping on a bed that doesn't bug you, but supports your better sleep.
[00:29:29.680 --> 00:29:30.680]   And that's Casper.
[00:29:30.680 --> 00:29:35.800]   Casper is an obsessively engineered mattress, supportive memory phones.
[00:29:35.800 --> 00:29:38.680]   It does both things you want with the mattress.
[00:29:38.680 --> 00:29:41.400]   It sinks where your pointy bits go.
[00:29:41.400 --> 00:29:47.000]   So like you lie on your side, your hip actually is comfortably supported, but then underneath
[00:29:47.000 --> 00:29:50.840]   that is a firm support.
[00:29:50.840 --> 00:29:52.720]   So your back feels great in the morning.
[00:29:52.720 --> 00:29:53.720]   It's hard to describe.
[00:29:53.720 --> 00:29:55.800]   You got to try it and it's breathable.
[00:29:55.800 --> 00:29:57.400]   So it stays cool.
[00:29:57.400 --> 00:29:59.360]   Your temperature is regulated through the night.
[00:29:59.360 --> 00:30:01.560]   They really they did many prototypes.
[00:30:01.560 --> 00:30:05.120]   They worked hard to make the best mattress possible and they make it in the US and they
[00:30:05.120 --> 00:30:09.760]   sell it to you direct, which means it's very affordable because there's no middleman.
[00:30:09.760 --> 00:30:11.400]   There's no department store.
[00:30:11.400 --> 00:30:14.400]   There's nothing in the way except there's that one little problem.
[00:30:14.400 --> 00:30:16.840]   You don't get to try it before you buy it.
[00:30:16.840 --> 00:30:20.400]   Now I understand that's a problem and Casper knows it too.
[00:30:20.400 --> 00:30:24.680]   And that's why they've solved it because instead of you know, lying on a mattress for
[00:30:24.680 --> 00:30:29.440]   five minutes in a showroom while the salesperson kind of gives you the stink eye, you can't
[00:30:29.440 --> 00:30:31.400]   take off your shoes in here, buddy.
[00:30:31.400 --> 00:30:33.960]   And no, make it out.
[00:30:33.960 --> 00:30:37.880]   You get to try these mattress in your home for a hundred nights.
[00:30:37.880 --> 00:30:42.000]   And if at any time in 100 nights, you say, yeah, it's not right for me.
[00:30:42.000 --> 00:30:43.800]   They come and they get it and they refund every penny.
[00:30:43.800 --> 00:30:45.400]   They literally come and take it away.
[00:30:45.400 --> 00:30:46.680]   It comes in a great box.
[00:30:46.680 --> 00:30:47.680]   It's easy to open.
[00:30:47.680 --> 00:30:50.960]   It smells great right out of the box.
[00:30:50.960 --> 00:30:52.360]   They have just figured it out.
[00:30:52.360 --> 00:30:57.400]   Free shipping and returns in the USA and Canada and you're going to save a lot.
[00:30:57.400 --> 00:31:01.160]   Go to Casper.com/twit and check it out.
[00:31:01.160 --> 00:31:07.280]   We'll give you an additional $50 off your mattress purchase if you use the offer code TWIT.
[00:31:07.280 --> 00:31:14.040]   Twin, queen, king, California, king, mattresses available now for a better night's sleep at
[00:31:14.040 --> 00:31:17.040]   Casper.com/twit.
[00:31:17.040 --> 00:31:22.920]   Don't forget the promo code, Twit, Twit, Twit, $50 off your first purchase.
[00:31:22.920 --> 00:31:25.840]   Sunday, Sunday, Sunday, Sunday, Sunday, Sunday.
[00:31:25.840 --> 00:31:27.840]   Terms and conditions apply.
[00:31:27.840 --> 00:31:32.360]   I always thought they must have gotten the idea for that from watching Mission Impossible,
[00:31:32.360 --> 00:31:37.200]   where the guy from Lost throws the mattress off the roof and then leaps off the roof and
[00:31:37.200 --> 00:31:39.000]   falls down into it.
[00:31:39.000 --> 00:31:43.400]   I think the guy that found at Casper saw that and said, that's it.
[00:31:43.400 --> 00:31:44.400]   That's what I'm doing.
[00:31:44.400 --> 00:31:45.400]   I'm going to launch that start out.
[00:31:45.400 --> 00:31:46.400]   That's right, Juicerillo.
[00:31:46.400 --> 00:31:49.160]   I have to stand corrected.
[00:31:49.160 --> 00:31:50.680]   That's a feeling.
[00:31:50.680 --> 00:31:56.960]   I thought that these oligarchs from the 21st century were somehow magically richer than everybody
[00:31:56.960 --> 00:32:02.640]   else, but Cobra in our chatroom says, adjusted for collation.
[00:32:02.640 --> 00:32:06.880]   Rockefeller's wealth in today's dollars would be $340 billion.
[00:32:06.880 --> 00:32:10.560]   Yes, but...
[00:32:10.560 --> 00:32:12.400]   What could he buy a submarine?
[00:32:12.400 --> 00:32:13.880]   Could he buy a rocket ship?
[00:32:13.880 --> 00:32:19.440]   No, the difference is that the top six richest people in the world today, which are mostly
[00:32:19.440 --> 00:32:24.680]   tech folks, have the same amount of wealth as the bottom 3.6 billion.
[00:32:24.680 --> 00:32:25.680]   Wow.
[00:32:25.680 --> 00:32:30.480]   There's a difference when you look at the relationship to the ratios of who is wealthy
[00:32:30.480 --> 00:32:31.720]   and who is not in Rockefeller time.
[00:32:31.720 --> 00:32:35.200]   There was a thing called the metal class that we no longer have.
[00:32:35.200 --> 00:32:36.720]   Yeah, or that.
[00:32:36.720 --> 00:32:38.200]   Yes.
[00:32:38.200 --> 00:32:39.600]   You know Alice is rich?
[00:32:39.600 --> 00:32:41.240]   Apple's rich, $250 billion.
[00:32:41.240 --> 00:32:45.680]   Quarter of a trillion in the bank.
[00:32:45.680 --> 00:32:50.480]   Let's put that in perspective, according to the Wall Street Journal.
[00:32:50.480 --> 00:32:52.680]   That's greater than the market value.
[00:32:52.680 --> 00:32:54.480]   This is not the value of Apple.
[00:32:54.480 --> 00:32:59.760]   This is just cash just sitting around in the cushions and the sofas.
[00:32:59.760 --> 00:33:03.440]   That's greater than the value of Walmart or Procter and Gamble.
[00:33:03.440 --> 00:33:09.280]   It exceeds the foreign currency reserves of the UK and Canada combined.
[00:33:09.280 --> 00:33:13.440]   And what's really kind of hysterical is they really can't do anything with it because most
[00:33:13.440 --> 00:33:21.400]   of it's stashed in Ireland and they can't repatriate it yet anyway.
[00:33:21.400 --> 00:33:25.000]   The question is why haven't they spent more of that money?
[00:33:25.000 --> 00:33:26.320]   Why haven't they given it back to you?
[00:33:26.320 --> 00:33:29.440]   No, everyone's like, oh, they should buy this or they should buy that.
[00:33:29.440 --> 00:33:31.240]   You know how much a charger costs?
[00:33:31.240 --> 00:33:34.240]   It's like more than the mortgage in my house.
[00:33:34.240 --> 00:33:39.160]   Like, why not say, you know what, you've all been such wonderful customers.
[00:33:39.160 --> 00:33:44.720]   You own 74 iPads and 12 phones and everything in your home has a little silly Apple logo
[00:33:44.720 --> 00:33:45.720]   on it.
[00:33:45.720 --> 00:33:49.520]   Take a free charger on us or take a $10 discount.
[00:33:49.520 --> 00:33:50.760]   What I would do.
[00:33:50.760 --> 00:33:56.960]   I almost wonder if they're period nearly going bankrupt in the 90s, permanently made them
[00:33:56.960 --> 00:33:58.960]   a little bit worried about the depreciate.
[00:33:58.960 --> 00:34:00.800]   It's like grandma's and the depression.
[00:34:00.800 --> 00:34:01.800]   Exactly.
[00:34:01.800 --> 00:34:02.800]   Look at the Nintendo Wii.
[00:34:02.800 --> 00:34:08.280]   You know, after the bottom fell out of Nintendo Wii sales and then the Wii U failed to catch
[00:34:08.280 --> 00:34:11.080]   on, Nintendo was in the red for gosh, what was it?
[00:34:11.080 --> 00:34:12.520]   I think it was like six years straight.
[00:34:12.520 --> 00:34:14.160]   It was a long time.
[00:34:14.160 --> 00:34:18.000]   You can look at the amount of cash the Apple has in the bank.
[00:34:18.000 --> 00:34:23.560]   It's enough to cover their annual expenses for a little over seven years.
[00:34:23.560 --> 00:34:28.080]   So you know, they've got some cushion there, but I almost think that near-death experience
[00:34:28.080 --> 00:34:30.280]   has them a little bit worried.
[00:34:30.280 --> 00:34:36.080]   And I also have to say, I think, you know, if you look at the way like financially money
[00:34:36.080 --> 00:34:40.160]   moves from country to country, you know, with these systems we put in place after World
[00:34:40.160 --> 00:34:45.040]   War II that kind of led to globalization, it is an interesting problem.
[00:34:45.040 --> 00:34:47.320]   Like how do you bring that money back to America?
[00:34:47.320 --> 00:34:49.880]   Do you just leave it in different countries?
[00:34:49.880 --> 00:34:53.560]   You know, it is a really complex problem to solve.
[00:34:53.560 --> 00:34:58.480]   I did not realize this, but I read an article that said that Bush had done a repatriation
[00:34:58.480 --> 00:35:02.520]   or a tax holiday in 2004.
[00:35:02.520 --> 00:35:06.320]   And it didn't make a very big difference.
[00:35:06.320 --> 00:35:13.680]   Mostly what companies did when they got their money back is buyback stock.
[00:35:13.680 --> 00:35:19.080]   You know, the 15 companies that repatriated the most after the 2004 tax break cut a net
[00:35:19.080 --> 00:35:27.480]   20,000 jobs between that time and 2007 decreased the pace of their spending and R&D.
[00:35:27.480 --> 00:35:34.000]   It did not in fact do any of the things one would hope repatriation would do.
[00:35:34.000 --> 00:35:37.760]   So that's just a little history lesson, not that anybody pays attention to history.
[00:35:37.760 --> 00:35:43.120]   In fact, I had forgotten that this has been done 13 years ago.
[00:35:43.120 --> 00:35:45.640]   Here's a question for you.
[00:35:45.640 --> 00:35:46.880]   What happens?
[00:35:46.880 --> 00:35:53.680]   So all these cash reserves and all this money and so little innovation and R&D that's come
[00:35:53.680 --> 00:35:59.720]   out, it makes you wonder what's really going on over there.
[00:35:59.720 --> 00:36:03.240]   Well, I don't know if I can agree with that.
[00:36:03.240 --> 00:36:04.840]   I think Fran is right though.
[00:36:04.840 --> 00:36:06.200]   It's a little pathological.
[00:36:06.200 --> 00:36:08.040]   It's like, you know what?
[00:36:08.040 --> 00:36:10.680]   We were broke, so we're going to just hoard it.
[00:36:10.680 --> 00:36:12.160]   They're hoarders.
[00:36:12.160 --> 00:36:14.080]   I mean, what is Apple going to do?
[00:36:14.080 --> 00:36:16.240]   Like go into markets where it doesn't make much sense.
[00:36:16.240 --> 00:36:20.280]   So I look at their automatic driverless car system of which they had a picture of it
[00:36:20.280 --> 00:36:25.760]   this week and it's, you know, I realize it's just a prototype and it's utterly terrible.
[00:36:25.760 --> 00:36:30.120]   I think like it would be much worse for Apple if they started going into different markets.
[00:36:30.120 --> 00:36:33.960]   It just didn't make any sense to their consumer base, you know?
[00:36:33.960 --> 00:36:38.480]   So I almost like them playing more conservatively here.
[00:36:38.480 --> 00:36:43.840]   And I think like it's always interesting to me the difference in Amazon stock price where
[00:36:43.840 --> 00:36:47.560]   the margins are raise, or raise, or raise are thin.
[00:36:47.560 --> 00:36:52.920]   And then the valuation of Apple stock where they have these giant cash reserves.
[00:36:52.920 --> 00:36:57.280]   I almost would like to see them looking at, you know, do they give stock dividends?
[00:36:57.280 --> 00:37:02.800]   So it like qualifies under different, you know, rules for people to buy long term.
[00:37:02.800 --> 00:37:04.040]   You know, do they give cash back?
[00:37:04.040 --> 00:37:07.480]   Well, Carl, Carl Icahn and many of the shareholders would love dividends.
[00:37:07.480 --> 00:37:08.480]   They would.
[00:37:08.480 --> 00:37:09.480]   Buy bags.
[00:37:09.480 --> 00:37:10.480]   But yeah.
[00:37:10.480 --> 00:37:12.760]   I don't know if that's the best thing for Apple.
[00:37:12.760 --> 00:37:14.160]   Dude, I like Nick's idea.
[00:37:14.160 --> 00:37:19.160]   Why don't you just cut the, because clearly, I mean, their margins are, I can't remember
[00:37:19.160 --> 00:37:22.120]   what this last quarter was something like 38% margin.
[00:37:22.120 --> 00:37:24.080]   Their margins are very, very good.
[00:37:24.080 --> 00:37:26.200]   They could take a little less off the top.
[00:37:26.200 --> 00:37:29.240]   From better than I think almost any company out there, right?
[00:37:29.240 --> 00:37:30.240]   Yeah.
[00:37:30.240 --> 00:37:31.240]   Yeah.
[00:37:31.240 --> 00:37:35.920]   On the other hand, if you're, if you're Tim Cook, you've got to always, you know, as
[00:37:35.920 --> 00:37:40.680]   Andy Grove wrote, you know, only the paranoid and Silicon Valley survive, you've got to
[00:37:40.680 --> 00:37:42.560]   be looking over your shoulder all the time.
[00:37:42.560 --> 00:37:45.880]   In fact, let's see what, let's mention what Microsoft did this week.
[00:37:45.880 --> 00:37:51.760]   Microsoft had their EDU event in New York City on, was it Tuesday morning?
[00:37:51.760 --> 00:37:52.760]   Yeah.
[00:37:52.760 --> 00:37:57.760]   And announced a new Surface laptop aimed clearly, well, announced a new version of Windows,
[00:37:57.760 --> 00:38:00.960]   Windows 10S aimed at schools.
[00:38:00.960 --> 00:38:04.520]   And they talked a lot about some of the features that they're going to put in the new Windows,
[00:38:04.520 --> 00:38:09.400]   including administrative software in tune for education.
[00:38:09.400 --> 00:38:14.960]   Their Slack competitor, Microsoft Teams, will be available for, there'll be Teams for education.
[00:38:14.960 --> 00:38:18.840]   And I think that's actually pretty smart because I'm sure kids would very much prefer
[00:38:18.840 --> 00:38:24.480]   to be, you know, talking with other kids over something more like Slack than, you know,
[00:38:24.480 --> 00:38:25.480]   whatever it is in some schools.
[00:38:25.480 --> 00:38:26.480]   I don't know about that.
[00:38:26.480 --> 00:38:30.720]   I've got a, so I've got a six-year-old and in her school, they're having a major prop,
[00:38:30.720 --> 00:38:32.600]   so her school's a STEM school.
[00:38:32.600 --> 00:38:33.600]   Yeah.
[00:38:33.600 --> 00:38:38.000]   And all the kids have to have, everybody has to be on iOS or--
[00:38:38.000 --> 00:38:39.000]   Well, that's the problem.
[00:38:39.000 --> 00:38:40.000]   You're on iPad.
[00:38:40.000 --> 00:38:41.000]   It's like the middle school.
[00:38:41.000 --> 00:38:43.200]   Yeah, but here's the real problem.
[00:38:43.200 --> 00:38:49.840]   The real problem is that chat software, so the real problem is iChat.
[00:38:49.840 --> 00:38:53.920]   And the teacher, the kids are distracted, they can't get them off of iChat.
[00:38:53.920 --> 00:38:56.120]   Any kind of chat software is a problem.
[00:38:56.120 --> 00:39:00.320]   And if you think about it, you know, when I was in school, we hand-wrote notes and passed
[00:39:00.320 --> 00:39:01.320]   them all on the table.
[00:39:01.320 --> 00:39:02.320]   Yeah, right.
[00:39:02.320 --> 00:39:03.320]   That was a problem.
[00:39:03.320 --> 00:39:07.080]   But what's the real issue here, and this is why I had a hard time getting excited about
[00:39:07.080 --> 00:39:12.320]   the new surface, is that, you know, what a six-year-old-- the way that a six-year-old
[00:39:12.320 --> 00:39:18.320]   sees the world and the exposure and experience they have to technology is fundamentally different
[00:39:18.320 --> 00:39:22.520]   from the 30 and 40 and 50-year-olds who are creating the new systems and who have this
[00:39:22.520 --> 00:39:26.120]   sort of top-down approach for what the kids-- what's going to work best for the kids.
[00:39:26.120 --> 00:39:30.480]   And they don't think through the future implications of what they're building.
[00:39:30.480 --> 00:39:32.160]   And so you wind up with problems down the road.
[00:39:32.160 --> 00:39:36.760]   Where now we've got these, like, it's constant kids who, you know, can't pay attention and
[00:39:36.760 --> 00:39:38.760]   they're constantly chatting with each other.
[00:39:38.760 --> 00:39:42.560]   Like, nobody in the classroom wants slack or slack.
[00:39:42.560 --> 00:39:43.560]   I agree.
[00:39:43.560 --> 00:39:44.560]   Or got you or anything like that.
[00:39:44.560 --> 00:39:45.560]   Six-year-olds are too young.
[00:39:45.560 --> 00:39:49.000]   But on the other hand, if you don't, don't you have to go to high school-- I mean, high
[00:39:49.000 --> 00:39:50.160]   schoolers are coming in.
[00:39:50.160 --> 00:39:54.840]   They've, you know, they've just spent the entire evening playing, you know, Minecraft
[00:39:54.840 --> 00:40:00.640]   or World of Warcraft or Overwatch with each other while on Skype.
[00:40:00.640 --> 00:40:03.440]   And then now they come in the classroom and it's like they're going back to the 19th
[00:40:03.440 --> 00:40:04.440]   century.
[00:40:04.440 --> 00:40:07.040]   You've got to kind of-- don't you have to reach out to that a little bit?
[00:40:07.040 --> 00:40:08.040]   I don't know.
[00:40:08.040 --> 00:40:13.880]   I have a pretty radically different viewpoint on just kids and technology.
[00:40:13.880 --> 00:40:18.200]   And even with the school that my kid goes to, I think the best possible preparation for
[00:40:18.200 --> 00:40:23.120]   the future is a super-hard core, super rigorous liberal arts education.
[00:40:23.120 --> 00:40:26.840]   Because the kids are going to pick up-- and if they want to be an engineer, that's awesome.
[00:40:26.840 --> 00:40:30.200]   And if my kid wants to be an engineer, that's great.
[00:40:30.200 --> 00:40:34.400]   But a lot of the critical reasoning skills are being sort of pushed out of the curriculum
[00:40:34.400 --> 00:40:35.400]   in a lot of places.
[00:40:35.400 --> 00:40:39.480]   Wait, are you saying that they should be technology or they should?
[00:40:39.480 --> 00:40:40.480]   No, no.
[00:40:40.480 --> 00:40:45.800]   I'm saying for some reason that I can't wrap my head around in the US, it's all or nothing.
[00:40:45.800 --> 00:40:51.480]   So all of this-- like a lot of the STEM programs are very, very intensive when it comes to
[00:40:51.480 --> 00:40:55.880]   the sort of technology and, you know, basic engineering stuff and basic math stuff.
[00:40:55.880 --> 00:41:00.720]   But there's no connection back to philosophy or humanities.
[00:41:00.720 --> 00:41:04.960]   It's sort of like a very, very intensive in sort of just one direction.
[00:41:04.960 --> 00:41:05.960]   I don't understand.
[00:41:05.960 --> 00:41:09.400]   I just started to say like Bill Bennett, do you want the Western canon to be taught in
[00:41:09.400 --> 00:41:10.400]   the--
[00:41:10.400 --> 00:41:11.400]   I don't know.
[00:41:11.400 --> 00:41:13.400]   You kids today, remember Socrates.
[00:41:13.400 --> 00:41:15.920]   It's not the kids.
[00:41:15.920 --> 00:41:16.920]   It's the adults.
[00:41:16.920 --> 00:41:17.920]   You look at a lot of the perpetrators.
[00:41:17.920 --> 00:41:18.920]   No, I completely agree with you.
[00:41:18.920 --> 00:41:19.920]   I'm just teasing you.
[00:41:19.920 --> 00:41:20.920]   Yes.
[00:41:20.920 --> 00:41:25.200]   Because the adults feel so left behind themselves that they're enforcing this-- no, because
[00:41:25.200 --> 00:41:26.680]   my job is to look ahead.
[00:41:26.680 --> 00:41:31.720]   And my real concern is 30 years into the future when my kid is-- you know, her age group
[00:41:31.720 --> 00:41:36.520]   is running the country, they will have missed out on things like comparative lit and religion.
[00:41:36.520 --> 00:41:39.720]   But they'll know how to use Twitter like nobody's business.
[00:41:39.720 --> 00:41:41.200]   That's correct.
[00:41:41.200 --> 00:41:46.560]   I do have to say, I feel like my background kind of has the opposite of this use case.
[00:41:46.560 --> 00:41:48.200]   You know, I grew up in Mississippi.
[00:41:48.200 --> 00:41:52.920]   I went to high school in Mississippi that was criminally underfunded in the '90s.
[00:41:52.920 --> 00:41:55.480]   And I work in technology today.
[00:41:55.480 --> 00:42:01.000]   And the only reason that I'm able to work in this field is because of privilege and having
[00:42:01.000 --> 00:42:06.320]   parents that bought me computers that were $3,000, $4,000 back then.
[00:42:06.320 --> 00:42:13.560]   And I can very clearly see from some of the play testing my studio has done about how
[00:42:13.560 --> 00:42:17.440]   technology is destroying kids' ability to focus.
[00:42:17.440 --> 00:42:20.240]   We absolutely see that.
[00:42:20.240 --> 00:42:27.080]   But at the same time, I just-- I think it's so unrealistic to have kids out there in the
[00:42:27.080 --> 00:42:32.960]   real world engaging in systems that meet your needs and your thoughts every few seconds.
[00:42:32.960 --> 00:42:38.760]   And then expecting them to go into a classroom where it's chalk and staring into books and
[00:42:38.760 --> 00:42:40.120]   taking notes that way.
[00:42:40.120 --> 00:42:42.760]   The human mind just doesn't work that way.
[00:42:42.760 --> 00:42:47.720]   And because our public schools are so criminally underfunded, I just-- I think there's got to
[00:42:47.720 --> 00:42:49.680]   be a better middle ground here.
[00:42:49.680 --> 00:42:50.680]   Good points.
[00:42:50.680 --> 00:42:58.600]   Apple could give it $250 trillion and a free charger.
[00:42:58.600 --> 00:43:01.200]   To every child in this country.
[00:43:01.200 --> 00:43:02.880]   In this country, the problem would be solved.
[00:43:02.880 --> 00:43:05.680]   Actually it's interesting because Apple is getting lapped now.
[00:43:05.680 --> 00:43:08.480]   They were getting lapped by Microsoft Windows.
[00:43:08.480 --> 00:43:13.480]   Apple put Apple twos in every classroom practically in the country 20 years ago.
[00:43:13.480 --> 00:43:16.960]   They're being lapped first by Windows and now by Chromebooks.
[00:43:16.960 --> 00:43:22.120]   And Apple I think is just at this point abandoned the education market.
[00:43:22.120 --> 00:43:25.840]   What do you think, Brianna, of this Minecraft education edition?
[00:43:25.840 --> 00:43:27.160]   To me, this is exciting.
[00:43:27.160 --> 00:43:28.160]   I love it.
[00:43:28.160 --> 00:43:30.520]   I absolutely love it, Leo.
[00:43:30.520 --> 00:43:35.080]   Something I think about a lot when we talk about Minecraft, Super Mario Brothers is something
[00:43:35.080 --> 00:43:41.840]   that we talk about like a classic of a classic and a literal exponential number more children
[00:43:41.840 --> 00:43:45.000]   play Minecraft and interact with Minecraft today.
[00:43:45.000 --> 00:43:50.560]   It has wonderful educational benefits for children with autism on the autism spectrum
[00:43:50.560 --> 00:43:56.360]   because they can interact with people socially and not look face to face.
[00:43:56.360 --> 00:43:58.800]   And it lets them develop social skills over type.
[00:43:58.800 --> 00:44:04.880]   I think this is absolutely wonderful and I'm very happy to see it.
[00:44:04.880 --> 00:44:09.360]   But Amy, when you see this-- I mean, this is obviously-- I would say this is aimed at
[00:44:09.360 --> 00:44:14.960]   maybe from fifth grade through tenth grade or ninth grade.
[00:44:14.960 --> 00:44:17.880]   Do you think this is an appropriate use of technology?
[00:44:17.880 --> 00:44:18.880]   It's not that.
[00:44:18.880 --> 00:44:20.480]   I mean, listen, we volunteer.
[00:44:20.480 --> 00:44:24.320]   It's a private school that our daughter goes to and we chose it.
[00:44:24.320 --> 00:44:26.400]   So we have her there for a reason.
[00:44:26.400 --> 00:44:34.040]   My concern is just sort of this global in our country, this sort of-- everybody's throwing
[00:44:34.040 --> 00:44:38.240]   money at maker spaces and which is fine.
[00:44:38.240 --> 00:44:40.880]   All of these skills are fine.
[00:44:40.880 --> 00:44:43.640]   And this sort of teach every child to code.
[00:44:43.640 --> 00:44:53.240]   The challenge is somebody needs to then bridge that to other critical thinking, flexing other
[00:44:53.240 --> 00:45:00.280]   parts of the muscle and other critical thinking areas and things like culture and gender.
[00:45:00.280 --> 00:45:04.000]   All of this other stuff sort of is not being addressed.
[00:45:04.000 --> 00:45:10.120]   So Minecraft is fine and collaborative tools for kids are fine.
[00:45:10.120 --> 00:45:14.200]   My concern is as I model this out because I'm a parent, so I've spent a lot of time thinking
[00:45:14.200 --> 00:45:15.200]   about it.
[00:45:15.200 --> 00:45:21.680]   But three decades into the future, we have a workforce full of people who have good ability
[00:45:21.680 --> 00:45:29.040]   to think in a linear structural way and whose technical skills will have compounded over
[00:45:29.040 --> 00:45:30.840]   a certain amount of time.
[00:45:30.840 --> 00:45:33.480]   But are they creative thinkers?
[00:45:33.480 --> 00:45:37.880]   How do they relate to their fellow humans?
[00:45:37.880 --> 00:45:42.240]   This actually ties back to the conversation about Mark Zuckerberg.
[00:45:42.240 --> 00:45:49.040]   I do think that much in the same way that in the first part of the 20th century, Hollywood
[00:45:49.040 --> 00:45:50.840]   culturally influenced the world.
[00:45:50.840 --> 00:45:56.520]   You know, really had a huge cultural impact on people's expectations, their understanding
[00:45:56.520 --> 00:46:01.800]   of how life was, that Silicon Valley is now doing the same here in the 21st century.
[00:46:01.800 --> 00:46:06.920]   And I do think there's reason to examine and even be concerned about the values that
[00:46:06.920 --> 00:46:13.720]   Silicon Valley unconsciously probably is spreading throughout the world.
[00:46:13.720 --> 00:46:15.560]   I don't disagree with you.
[00:46:15.560 --> 00:46:17.560]   Unconsciously, it's not unconscious.
[00:46:17.560 --> 00:46:21.120]   I mean, the values that Silicon Valley is spreading around the world are the same values
[00:46:21.120 --> 00:46:22.120]   of Wall Street.
[00:46:22.120 --> 00:46:25.080]   They just under this complete nonsensical guise that they're trying to make the world
[00:46:25.080 --> 00:46:26.080]   a better place.
[00:46:26.080 --> 00:46:31.200]   I mean, this has been the thing that I've despised about Silicon Valley for a decade.
[00:46:31.200 --> 00:46:37.520]   It's like every single thing that they do and everything they build has an impact and
[00:46:37.520 --> 00:46:40.120]   they fail to look mostly at the negatives.
[00:46:40.120 --> 00:46:42.640]   They just completely don't look at it.
[00:46:42.640 --> 00:46:47.920]   And then you have CEOs like the CEO of Uber and a million other little companies or big
[00:46:47.920 --> 00:46:51.720]   companies that do these incredibly unethical things.
[00:46:51.720 --> 00:46:53.240]   I think it's not unconscious.
[00:46:53.240 --> 00:46:58.720]   It's just that there's this complete nonsensical PR push that they push in front of it.
[00:46:58.720 --> 00:47:03.480]   I think it's partially that, but then there's also, but there's a whole entire layer.
[00:47:03.480 --> 00:47:07.880]   So my concern is just take AI and voice, right?
[00:47:07.880 --> 00:47:16.080]   So the training corpus that's being used by every AI developer for voice recognition
[00:47:16.080 --> 00:47:17.800]   comes from Philadelphia.
[00:47:17.800 --> 00:47:23.200]   It was a series of like 250 phone conversations that were recorded in the mid 1990s.
[00:47:23.200 --> 00:47:28.000]   And yes, that's the base corpus for every single, right.
[00:47:28.000 --> 00:47:33.320]   So forget the fact that if you have an accent, right?
[00:47:33.320 --> 00:47:35.800]   Nobody has thought through the implications of this.
[00:47:35.800 --> 00:47:38.240]   And the challenge is that technology compounds, right?
[00:47:38.240 --> 00:47:47.400]   So we have the obvious problems like CEO of Uber doing and saying ridiculous things.
[00:47:47.400 --> 00:47:53.440]   But we have all of these fissures and all of these areas of bias being woven into so
[00:47:53.440 --> 00:47:59.840]   many different facets of technology that we won't even start to realize until several
[00:47:59.840 --> 00:48:00.840]   years into the future.
[00:48:00.840 --> 00:48:05.640]   But the people who are working on them have either been told that there are problems in
[00:48:05.640 --> 00:48:11.600]   the foundation and they're ignoring it or they've chosen not to look.
[00:48:11.600 --> 00:48:13.400]   I think there's a pattern, right?
[00:48:13.400 --> 00:48:17.200]   So for example, I remember when they were building 3D printers, I was friends with
[00:48:17.200 --> 00:48:22.040]   Repress and that crew and they, I remember saying to them 10 years ago, what are you
[00:48:22.040 --> 00:48:23.040]   going to do with these?
[00:48:23.040 --> 00:48:24.920]   We're going to build wool hooks and iPhone cases.
[00:48:24.920 --> 00:48:25.920]   It's going to be amazing.
[00:48:25.920 --> 00:48:29.280]   If it was the person people did, they built 3D printed plastic guns.
[00:48:29.280 --> 00:48:33.960]   You know, when you look at the newsfeed, it's like people are going to use it to share
[00:48:33.960 --> 00:48:36.800]   pictures of their sunset or their dog or the set and the other.
[00:48:36.800 --> 00:48:42.880]   The first thing people do when they finally figure out how to use it for negatives is
[00:48:42.880 --> 00:48:45.720]   Russia takes advantage of it to spread fake news.
[00:48:45.720 --> 00:48:50.560]   Twitter was designed so that you could tell your friends you were at a club and to meet
[00:48:50.560 --> 00:48:51.560]   you there.
[00:48:51.560 --> 00:48:52.560]   What do we have?
[00:48:52.560 --> 00:48:57.040]   The biggest center for trolls that the world has ever seen.
[00:48:57.040 --> 00:48:59.680]   Right, but that's totally true, but Nick, that was totally avoidable.
[00:48:59.680 --> 00:49:02.360]   I mean, you're not completely avoidable.
[00:49:02.360 --> 00:49:03.880]   They chose not to avoid it.
[00:49:03.880 --> 00:49:04.880]   Oh, that's right.
[00:49:04.880 --> 00:49:05.880]   That's really cool.
[00:49:05.880 --> 00:49:08.600]   I want to know this because what could Twitter have done differently?
[00:49:08.600 --> 00:49:10.920]   Look at Instagram.
[00:49:10.920 --> 00:49:11.920]   Look at Instagram.
[00:49:11.920 --> 00:49:13.600]   They designed it from the ground up.
[00:49:13.600 --> 00:49:15.600]   There were a lot of women on the engineering team.
[00:49:15.600 --> 00:49:20.800]   So when they came up with their fundamental user paradigms, like it makes harassment fundamentally
[00:49:20.800 --> 00:49:22.000]   more different.
[00:49:22.000 --> 00:49:27.320]   You know, technology tends to mirror the issues in the real world unless we kind of think
[00:49:27.320 --> 00:49:28.320]   about it.
[00:49:28.320 --> 00:49:32.120]   And just to give you a little bit of pushback, I agree with you.
[00:49:32.120 --> 00:49:37.840]   There is a myopic haze over a lot of people in Silicon Valley.
[00:49:37.840 --> 00:49:42.960]   But I also think it's true that certain personality types are attracted to certain profession.
[00:49:42.960 --> 00:49:49.080]   In journalism, I sure know a lot of jaded, very cynical people that really enjoy tearing
[00:49:49.080 --> 00:49:50.080]   things apart.
[00:49:50.080 --> 00:49:51.080]   Yeah.
[00:49:51.080 --> 00:49:54.520]   It makes them very good journalists in that same way.
[00:49:54.520 --> 00:50:00.480]   I don't think you would be a really good technologist if you didn't like see the future
[00:50:00.480 --> 00:50:05.080]   and want to see this the best in it and try to bring it forward.
[00:50:05.080 --> 00:50:06.080]   I feel you.
[00:50:06.080 --> 00:50:08.080]   I feel you, but you are totally wrong.
[00:50:08.080 --> 00:50:09.080]   And I.
[00:50:09.080 --> 00:50:14.640]   So before I say what I'm about to say, let me verbally give you a cute, cute cat video,
[00:50:14.640 --> 00:50:16.120]   just verbally.
[00:50:16.120 --> 00:50:17.360]   You are totally wrong.
[00:50:17.360 --> 00:50:22.000]   So technologists don't see that they don't see the future.
[00:50:22.000 --> 00:50:24.280]   If technologists did see the future.
[00:50:24.280 --> 00:50:25.920]   So it's one of two scenarios.
[00:50:25.920 --> 00:50:33.120]   Either they do see the future and they are willfully putting money and personal fame in
[00:50:33.120 --> 00:50:40.960]   front of the practical realities of humanity, doing bad stuff with their technology, or
[00:50:40.960 --> 00:50:42.280]   they're not planning for it.
[00:50:42.280 --> 00:50:44.800]   There is no other way around it.
[00:50:44.800 --> 00:50:49.440]   Twitter had the ability multiple times over multiple years to deal with the troll problem
[00:50:49.440 --> 00:50:51.520]   before it began.
[00:50:51.520 --> 00:50:56.880]   But in order to do that, they would have had to adopt measures that would have been counter
[00:50:56.880 --> 00:51:02.720]   to the network being as big and as productive as it is.
[00:51:02.720 --> 00:51:04.560]   Same thing with Facebook.
[00:51:04.560 --> 00:51:06.320]   Still the case today.
[00:51:06.320 --> 00:51:11.880]   Look at yesterday what happened with the McCombs in France, the documents that were
[00:51:11.880 --> 00:51:14.560]   leaked obviously by the Russians.
[00:51:14.560 --> 00:51:22.720]   As my former colleague of the New York Times, Nicole Purlar tweeted, "The five percent of
[00:51:22.720 --> 00:51:28.920]   there was a certain tiny amount of fake accounts on Twitter that were sharing 80% of
[00:51:28.920 --> 00:51:31.320]   the content around it.
[00:51:31.320 --> 00:51:37.160]   And any algorithm that could exist on any platform would have shut that down in five
[00:51:37.160 --> 00:51:38.160]   seconds.
[00:51:38.160 --> 00:51:41.920]   Twitter didn't do that because it's antithetical to what they want.
[00:51:41.920 --> 00:51:44.320]   They want to show they have massive engagement.
[00:51:44.320 --> 00:51:47.080]   They want to show that they're growing so that Wall Street continues to invest them
[00:51:47.080 --> 00:51:48.400]   and so on and so forth.
[00:51:48.400 --> 00:51:52.760]   It is a very simple, these are very simple solutions sometimes even after the fact.
[00:51:52.760 --> 00:51:54.400]   And a lot of the times we don't see them implemented.
[00:51:54.400 --> 00:51:58.120]   You should point out Nick wrote the book, "The Book Hatching Twitter" is probably the
[00:51:58.120 --> 00:52:02.040]   foremost expert on how Twitter got started.
[00:52:02.040 --> 00:52:04.320]   And so I think you have some standing when you say that.
[00:52:04.320 --> 00:52:09.160]   But I wonder if Twitter hadn't been the free speech wing of the free speech party if they
[00:52:09.160 --> 00:52:11.400]   would have succeeded in the way they've succeeded today.
[00:52:11.400 --> 00:52:14.640]   I mean, wasn't that part and parcel of what made Twitter Twitter?
[00:52:14.640 --> 00:52:19.880]   I think yeah, but it's yes, but there were things that they could have done along the
[00:52:19.880 --> 00:52:25.160]   way that would have solved problems and wouldn't have solved all problems.
[00:52:25.160 --> 00:52:31.440]   But there has, if you look at Facebook, they made a decision very early on.
[00:52:31.440 --> 00:52:35.920]   There was a huge fight inside Facebook many years ago where there was a group of engineers
[00:52:35.920 --> 00:52:37.440]   that said, "We should be more like Twitter.
[00:52:37.440 --> 00:52:41.320]   People should be able to be anonymous so you don't know so people can say what's
[00:52:41.320 --> 00:52:42.320]   whatever they want.
[00:52:42.320 --> 00:52:44.880]   There should be no rules about what they talk about."
[00:52:44.880 --> 00:52:50.280]   And Zuckerberg actually was the one who finally came in and said, "No, we have to know you're
[00:52:50.280 --> 00:52:54.360]   a real person and we have to have guidelines."
[00:52:54.360 --> 00:52:58.920]   And there's a set of guidelines on Facebook's trust and safety board.
[00:52:58.920 --> 00:53:01.440]   Twitter decided not to implement any of these things.
[00:53:01.440 --> 00:53:05.600]   The only things that you can't do are technically post someone's name and address and social
[00:53:05.600 --> 00:53:06.880]   security number.
[00:53:06.880 --> 00:53:10.880]   But they could have built the system in a way, but they could have changed the system in
[00:53:10.880 --> 00:53:15.400]   a way that would have reduced the number of trolls, reduced the way people attack people
[00:53:15.400 --> 00:53:18.520]   on mosques, like all these things they could have done.
[00:53:18.520 --> 00:53:22.040]   And all it would have done would have made less people use the service, which would have
[00:53:22.040 --> 00:53:23.800]   made the company worth less money.
[00:53:23.800 --> 00:53:29.360]   And I think that that's another analogy to the valley being just like Wall Street.
[00:53:29.360 --> 00:53:30.360]   That's right.
[00:53:30.360 --> 00:53:35.840]   And so going, so to my point of yours, Brianna, that I was refuting has specifically to do
[00:53:35.840 --> 00:53:36.840]   with the fact that-
[00:53:36.840 --> 00:53:38.520]   Wait a minute, let me get the cat video.
[00:53:38.520 --> 00:53:39.840]   Hold on, hold on, hold on, keep going.
[00:53:39.840 --> 00:53:45.120]   Well, I want to make this clear because I think that people outside the valley think
[00:53:45.120 --> 00:53:51.280]   of people like Mark Zuckerberg and Eve Williams as futurists.
[00:53:51.280 --> 00:53:56.760]   And these are people who I would say are very much not thinking about the future.
[00:53:56.760 --> 00:54:01.720]   And the reason that I wrote Signals, the new book, was in a large part for that group
[00:54:01.720 --> 00:54:05.760]   of people, because I would love for all of the technologists who are building things
[00:54:05.760 --> 00:54:09.080]   that impact the future to start thinking like a futurist.
[00:54:09.080 --> 00:54:14.000]   Because what we would do is start modeling out different scenarios in advance to try
[00:54:14.000 --> 00:54:20.920]   to anticipate the various realities of this technology entering all of these different
[00:54:20.920 --> 00:54:25.040]   facets of human life, and then just sort of reverse engineer back to their present day
[00:54:25.040 --> 00:54:29.320]   what we should be thinking and doing about it.
[00:54:29.320 --> 00:54:34.320]   And to me, that's a huge concern.
[00:54:34.320 --> 00:54:36.880]   And I know you have to work on the product, and I know you've got to get, you've got
[00:54:36.880 --> 00:54:40.720]   to ship a product, and I get all of that, and the realities of shareholders and everything
[00:54:40.720 --> 00:54:41.720]   else.
[00:54:41.720 --> 00:54:45.800]   But these people may be building the future, but these are not futurists.
[00:54:45.800 --> 00:54:48.120]   These are not people who are thinking about it.
[00:54:48.120 --> 00:54:49.960]   Okay, but why should they be futurists?
[00:54:49.960 --> 00:54:50.960]   They're business people.
[00:54:50.960 --> 00:54:51.960]   And really-
[00:54:51.960 --> 00:54:52.960]   I'm a business person.
[00:54:52.960 --> 00:54:53.960]   I'm a capitalist.
[00:54:53.960 --> 00:54:56.920]   Analysts is optimized for one thing and one thing only.
[00:54:56.920 --> 00:55:00.840]   It's not worrying about the ultimate consequences.
[00:55:00.840 --> 00:55:02.760]   It's to optimize for profit.
[00:55:02.760 --> 00:55:08.800]   And so, and what I think has happened with Silicon Valley is that the technology, the
[00:55:08.800 --> 00:55:13.800]   tools that they have to optimize for profit have been weaponized.
[00:55:13.800 --> 00:55:20.600]   And so we really have nothing any different from Henry Ford or John D. Rockefeller or
[00:55:20.600 --> 00:55:22.800]   any form of capitalism.
[00:55:22.800 --> 00:55:28.280]   It's a predatory system, but we have tools that are so much more effective.
[00:55:28.280 --> 00:55:34.640]   But anyway, we're going to take a break because I think it's time for some cute kittens.
[00:55:34.640 --> 00:55:35.960]   And that's a hedgehog.
[00:55:35.960 --> 00:55:36.960]   That's a kitten.
[00:55:36.960 --> 00:55:41.120]   And you won't believe what happens next, but you're not going to see it because I'm going
[00:55:41.120 --> 00:55:43.320]   to do an ad for stamps.com and then we'll continue.
[00:55:43.320 --> 00:55:46.560]   What a great- we have the greatest panel ever.
[00:55:46.560 --> 00:55:53.600]   And I'm super smart and I'm thrilled to have you all, Brandon Wu, is a space cat gal on Twitter.
[00:55:53.600 --> 00:56:00.040]   She is, of course, a game developer, very well known as an outspoken feminist.
[00:56:00.040 --> 00:56:05.720]   And I'm proud to say a candidate for Congress in Massachusetts in the 8th district.
[00:56:05.720 --> 00:56:08.720]   You should vote for her whenever you get a chance.
[00:56:08.720 --> 00:56:10.560]   We should all move to Massachusetts to vote for.
[00:56:10.560 --> 00:56:11.840]   We should all move to Matt.
[00:56:11.840 --> 00:56:12.840]   That's Nick Bilton.
[00:56:12.840 --> 00:56:13.840]   He writes now for the baby fair.
[00:56:13.840 --> 00:56:16.720]   We have three power cord from Apple.
[00:56:16.720 --> 00:56:17.720]   I will do it.
[00:56:17.720 --> 00:56:20.440]   You know what Massachusetts does have?
[00:56:20.440 --> 00:56:21.440]   Single pair of house.
[00:56:21.440 --> 00:56:22.440]   We do.
[00:56:22.440 --> 00:56:23.440]   We do.
[00:56:23.440 --> 00:56:27.120]   For one, the state's is going to be least affected if the Republican health care bill
[00:56:27.120 --> 00:56:28.880]   goes through, which I'm very happy about.
[00:56:28.880 --> 00:56:30.360]   Thanks to Mitt Romney of all people.
[00:56:30.360 --> 00:56:31.360]   Yeah.
[00:56:31.360 --> 00:56:32.480]   He was not that bad a governor.
[00:56:32.480 --> 00:56:34.040]   I have worries.
[00:56:34.040 --> 00:56:39.840]   I'm telling you, he's looking really good right now.
[00:56:39.840 --> 00:56:42.120]   Anyway, good.
[00:56:42.120 --> 00:56:43.960]   You're thrilled that you would get you on.
[00:56:43.960 --> 00:56:47.400]   Great to have you as always and we'll have you back many, many times, whatever, until
[00:56:47.400 --> 00:56:49.160]   you become a member of Congress.
[00:56:49.160 --> 00:56:51.200]   And then I'm really going to put you on this one.
[00:56:51.200 --> 00:56:57.520]   Also, Nick's, by the way, new book is American Kingpin, the epic hunt for the criminal mastermind
[00:56:57.520 --> 00:56:59.040]   behind the Silk Road.
[00:56:59.040 --> 00:57:02.520]   And I will be getting Nick back in a couple of weeks to join us on triangulation, where
[00:57:02.520 --> 00:57:04.280]   we'll talk about that book in greater detail.
[00:57:04.280 --> 00:57:06.600]   Amy Webb is a veteran of triangulation.
[00:57:06.600 --> 00:57:10.360]   Her book, The Signals Are Talking, tells you how you can be a futurist.
[00:57:10.360 --> 00:57:15.560]   What she does for a living, and it's great to have you and somebody with more than an
[00:57:15.560 --> 00:57:18.640]   opinion, but opinion backed by data.
[00:57:18.640 --> 00:57:20.560]   That's unusual here.
[00:57:20.560 --> 00:57:22.120]   Mostly we just make stuff up.
[00:57:22.120 --> 00:57:27.160]   But all three of you are really very pleased to have all three of you.
[00:57:27.160 --> 00:57:30.960]   And of course, you at home, and I'm glad you're here and I bet you're glad you're here too,
[00:57:30.960 --> 00:57:33.200]   our show today brought to you by stamps.com.
[00:57:33.200 --> 00:57:39.280]   If you are in a business that involves the postal service, you mail bills, brochures,
[00:57:39.280 --> 00:57:43.560]   if you sell on Etsy or eBay or Amazon, you've got to know about stamps.com.
[00:57:43.560 --> 00:57:45.560]   I hope you know about stamps.com.
[00:57:45.560 --> 00:57:51.760]   It's going to turn your fulfillment process into a professional, well-oiled machine.
[00:57:51.760 --> 00:57:55.040]   And you know, the best part is you don't spend any time at the post office.
[00:57:55.040 --> 00:57:59.720]   Everything you could do at the post office, you do right from your desk with stamps.com.
[00:57:59.720 --> 00:58:03.400]   You can print, buy and print real US postage from your computer and printer.
[00:58:03.400 --> 00:58:04.760]   No, this is not a postage meter.
[00:58:04.760 --> 00:58:06.480]   You do not need one of those things.
[00:58:06.480 --> 00:58:09.240]   Talk about 19th century technology.
[00:58:09.240 --> 00:58:14.040]   You can also avoid mailing stuff because guess what?
[00:58:14.040 --> 00:58:15.440]   The mail carrier comes to you.
[00:58:15.440 --> 00:58:19.080]   There's even a button on stamps.com that says, "I need to pick up."
[00:58:19.080 --> 00:58:20.800]   You can create your stamps account in minutes.
[00:58:20.800 --> 00:58:22.000]   You don't have to lease equipment.
[00:58:22.000 --> 00:58:23.200]   There's no long-term commitment.
[00:58:23.200 --> 00:58:25.480]   They even will send you a digital scale.
[00:58:25.480 --> 00:58:28.520]   USB scale that automatically calculates the exact postage.
[00:58:28.520 --> 00:58:33.360]   Stamps.com will save you money by proposing different kinds of mail.
[00:58:33.360 --> 00:58:35.800]   You say, "You know, this could be media mail."
[00:58:35.800 --> 00:58:37.680]   They will print out the forms for you.
[00:58:37.680 --> 00:58:42.640]   If you have an envelope, it will actually print the postage, your address, the recipient's
[00:58:42.640 --> 00:58:46.680]   address, right on the envelope and get all that information from your address book.
[00:58:46.680 --> 00:58:51.480]   If you're printing mailing internationally, it will print out the customs forms and print
[00:58:51.480 --> 00:58:54.440]   in the info, fill in the information right from the website.
[00:58:54.440 --> 00:58:56.240]   Same thing with priority mail.
[00:58:56.240 --> 00:59:01.120]   It will automatically send out emails to your recipient letting them know the mail is on
[00:59:01.120 --> 00:59:02.280]   the way.
[00:59:02.280 --> 00:59:03.440]   I've got a great deal for you.
[00:59:03.440 --> 00:59:09.320]   A special offer that includes a four-week trial plus $55.
[00:59:09.320 --> 00:59:10.320]   Wait a minute.
[00:59:10.320 --> 00:59:11.720]   That says $5 in free postage.
[00:59:11.720 --> 00:59:12.720]   Here's the deal.
[00:59:12.720 --> 00:59:16.800]   Click that microphone in the upper right-hand corner and the offer code TWIT and that $5
[00:59:16.800 --> 00:59:19.720]   -- look at that, $55 in free postage.
[00:59:19.720 --> 00:59:20.720]   That's nice.
[00:59:20.720 --> 00:59:25.200]   You also get a digital scale and there's no long-term commitment.
[00:59:25.200 --> 00:59:26.560]   You pay the shipping and handling on the scale.
[00:59:26.560 --> 00:59:27.560]   It's about $5.
[00:59:27.560 --> 00:59:32.680]   They make that up with a $5 kit of activities, things you'll need, supplies.
[00:59:32.680 --> 00:59:33.680]   I love stamps.com.
[00:59:33.680 --> 00:59:34.680]   We use it here in the office.
[00:59:34.680 --> 00:59:35.920]   We've been using it for years.
[00:59:35.920 --> 00:59:40.040]   If you're doing mailing, make your mailing look as professional as you are.
[00:59:40.040 --> 00:59:43.880]   Stamps.com, go there, click the microphone in the upper right-hand corner, use the offer
[00:59:43.880 --> 00:59:46.320]   code TWIT for $110.
[00:59:46.320 --> 00:59:49.000]   Bonus offer from stamps.com.
[00:59:49.000 --> 00:59:50.000]   Who's buzzing?
[00:59:50.000 --> 00:59:52.160]   Somebody needs somebody to re-plug their headset.
[00:59:52.160 --> 00:59:53.840]   Did you figure that out, Karsten?
[00:59:53.840 --> 00:59:55.440]   Does it mean?
[00:59:55.440 --> 01:00:00.240]   No, Nick is actually the first person ever on the show to use AirPods.
[01:00:00.240 --> 01:00:03.960]   Apple's brought to you by.
[01:00:03.960 --> 01:00:06.280]   Did Apple give you a $50 discount?
[01:00:06.280 --> 01:00:08.240]   It was dental floss.
[01:00:08.240 --> 01:00:09.240]   Did you see this?
[01:00:09.240 --> 01:00:18.400]   Actually, Ben Behren did a customer satisfaction survey of, I think, 942 AirPod users, 98%
[01:00:18.400 --> 01:00:20.200]   customer satisfaction.
[01:00:20.200 --> 01:00:21.840]   None of them have curly hair.
[01:00:21.840 --> 01:00:22.840]   That's mine.
[01:00:22.840 --> 01:00:23.840]   [laughter]
[01:00:23.840 --> 01:00:25.240]   I'm telling you.
[01:00:25.240 --> 01:00:29.120]   Does your hair catch your AirPods?
[01:00:29.120 --> 01:00:30.120]   Does mine?
[01:00:30.120 --> 01:00:38.680]   Nobody designs head-mounted displays, head-related technology products, or glasses, or...
[01:00:38.680 --> 01:00:42.680]   I'm married to somebody who was one of the first people, he's an eye doctor.
[01:00:42.680 --> 01:00:46.160]   He was one of the first doctors in the country to write for Google Glass.
[01:00:46.160 --> 01:00:48.520]   He had a special thing.
[01:00:48.520 --> 01:00:51.280]   In two years, he had zero people come in wanting...
[01:00:51.280 --> 01:00:53.080]   It's so funny.
[01:00:53.080 --> 01:00:54.080]   It's so funny.
[01:00:54.080 --> 01:00:56.520]   When you said, we were talking about Apple and you said, "He's an eye doctor.
[01:00:56.520 --> 01:00:58.520]   I imagine my brain literally was like..."
[01:00:58.520 --> 01:01:03.080]   "That guy, you know, like what is that guy?"
[01:01:03.080 --> 01:01:05.600]   There's where you put your quarter of a trillion dollars.
[01:01:05.600 --> 01:01:06.600]   Let's get eye doctors.
[01:01:06.600 --> 01:01:07.920]   Apple, you don't need a new plug.
[01:01:07.920 --> 01:01:10.760]   Apple's clearly got something just jacked right into you.
[01:01:10.760 --> 01:01:16.120]   Can you think that like 2% of people would have the ears that they fall out of?
[01:01:16.120 --> 01:01:18.880]   Like, you know, the Apple headphones absolutely fall.
[01:01:18.880 --> 01:01:19.880]   I love the AirPods.
[01:01:19.880 --> 01:01:23.760]   They fit my ears, but they have one size, one size fits all, and it doesn't.
[01:01:23.760 --> 01:01:25.000]   My wife can't use them.
[01:01:25.000 --> 01:01:26.000]   Her ears are too small.
[01:01:26.000 --> 01:01:27.000]   What happens?
[01:01:27.000 --> 01:01:28.000]   They don't fit in?
[01:01:28.000 --> 01:01:29.000]   They're comfortable and they fall out.
[01:01:29.000 --> 01:01:33.160]   Yeah, like if you're out for a run, they just pop out of the ears.
[01:01:33.160 --> 01:01:35.120]   Like, I don't know what's up.
[01:01:35.120 --> 01:01:36.120]   But I...
[01:01:36.120 --> 01:01:38.400]   It's like, actually, the lookable sizes matter.
[01:01:38.400 --> 01:01:44.000]   It does matter, but for people like me and Nick with perfectly sized ear canals and no
[01:01:44.000 --> 01:01:46.000]   hair to be getting away, they work great.
[01:01:46.000 --> 01:01:47.000]   Beautiful short hair.
[01:01:47.000 --> 01:01:48.000]   Beautiful short hair.
[01:01:48.000 --> 01:01:49.920]   I have a serious question.
[01:01:49.920 --> 01:01:54.480]   I can't tell if I look awesome or stupid with these things.
[01:01:54.480 --> 01:01:57.840]   No, you're like sitting down a little...
[01:01:57.840 --> 01:01:59.840]   Right, you're the glass hole of...
[01:01:59.840 --> 01:02:00.840]   High glass hole.
[01:02:00.840 --> 01:02:03.520]   It's not as bad as a glass hole though, though.
[01:02:03.520 --> 01:02:06.320]   But it's just like an eye hole thing right here.
[01:02:06.320 --> 01:02:08.440]   No, you look really cool.
[01:02:08.440 --> 01:02:11.560]   You look like your ear is smoking a Mearsham pipe.
[01:02:11.560 --> 01:02:13.120]   That's what you look like.
[01:02:13.120 --> 01:02:16.560]   It's where it's got to be your ear hole.
[01:02:16.560 --> 01:02:17.560]   It looks like...
[01:02:17.560 --> 01:02:20.360]   I've always thought it looked like somebody just came by with a pair of scissors and
[01:02:20.360 --> 01:02:21.760]   snipped off the cord.
[01:02:21.760 --> 01:02:22.760]   To me, that's what they've always...
[01:02:22.760 --> 01:02:24.960]   Many people have wanted to do.
[01:02:24.960 --> 01:02:28.600]   And I was too lazy to take them out.
[01:02:28.600 --> 01:02:29.600]   Oh, God.
[01:02:29.600 --> 01:02:30.600]   Who is it that does that?
[01:02:30.600 --> 01:02:35.240]   Is that Richard Branson who cuts people's ties when he sees them?
[01:02:35.240 --> 01:02:36.240]   Really?
[01:02:36.240 --> 01:02:37.720]   I always thought that was kind of hostile.
[01:02:37.720 --> 01:02:40.880]   What if it's like a really expensive silk tie?
[01:02:40.880 --> 01:02:43.040]   What are you going to do?
[01:02:43.040 --> 01:02:44.520]   "Ah ha ha ha, Richard.
[01:02:44.520 --> 01:02:45.520]   You're so funny."
[01:02:45.520 --> 01:02:51.320]   I do have to blast this methodology a little bit because I've been very curious to get
[01:02:51.320 --> 01:02:52.320]   these.
[01:02:52.320 --> 01:02:58.760]   Every single time I go to the Apple Store or I look at the Apple app to try to get some,
[01:02:58.760 --> 01:03:01.240]   they're always back-quordered for six months.
[01:03:01.240 --> 01:03:06.880]   So I think it's worth saying if that number is 98 percent, that is 90...
[01:03:06.880 --> 01:03:12.280]   These are very hardcore Apple customers and people they're so psyched up about this product
[01:03:12.280 --> 01:03:14.640]   that they're willing to wait out for.
[01:03:14.640 --> 01:03:15.640]   That's a really good point.
[01:03:15.640 --> 01:03:19.120]   Either that stock an Apple store until they get it.
[01:03:19.120 --> 01:03:22.720]   It's somebody who wanted it so badly they did that they went back again and again and
[01:03:22.720 --> 01:03:23.720]   again again again.
[01:03:23.720 --> 01:03:24.720]   Absolutely.
[01:03:24.720 --> 01:03:26.720]   And I will say I didn't want them.
[01:03:26.720 --> 01:03:29.560]   Someone gave them to me and I really actually love them.
[01:03:29.560 --> 01:03:32.760]   I mean, they've got this little charger inside.
[01:03:32.760 --> 01:03:35.480]   Can you see the little light that comes on?
[01:03:35.480 --> 01:03:37.680]   I mean, they're really, they're pretty impressive.
[01:03:37.680 --> 01:03:38.680]   I've got to say.
[01:03:38.680 --> 01:03:41.600]   It's the best product Apple's done in years, frankly.
[01:03:41.600 --> 01:03:45.320]   And this is from a company that has fumbled the MacBook Air.
[01:03:45.320 --> 01:03:47.160]   The MacBook Pros are terrible.
[01:03:47.160 --> 01:03:49.160]   The Touch Bar is a joke.
[01:03:49.160 --> 01:03:53.480]   Why don't you like the Touch Bar?
[01:03:53.480 --> 01:03:57.160]   It doesn't do anything useful and you keep hitting it by accident.
[01:03:57.160 --> 01:04:02.080]   So every time I hit the Backspace key Siri goes, "Yeah, what?"
[01:04:02.080 --> 01:04:03.080]   That's not useful.
[01:04:03.080 --> 01:04:07.000]   And so you can move Siri and eventually you just end up disabling the Touch Bar because
[01:04:07.000 --> 01:04:12.480]   even when applications use it, you're not looking down to see what the application control
[01:04:12.480 --> 01:04:13.480]   is.
[01:04:13.480 --> 01:04:16.200]   You're looking at the screen like any normal typist.
[01:04:16.200 --> 01:04:18.000]   So it's just not, it's a bad idea.
[01:04:18.000 --> 01:04:21.560]   On the other hand, you notice I'm using, and by the way, I'm a Mac guy.
[01:04:21.560 --> 01:04:24.760]   I got a Mac and the first hundred days, I love Apple.
[01:04:24.760 --> 01:04:26.480]   I bought every Mac ever.
[01:04:26.480 --> 01:04:28.800]   I've got every, but what am I using?
[01:04:28.800 --> 01:04:30.880]   A Surface Studio.
[01:04:30.880 --> 01:04:31.880]   This thing is perfect.
[01:04:31.880 --> 01:04:33.160]   You see what it's doing here?
[01:04:33.160 --> 01:04:34.160]   It's gorgeous.
[01:04:34.160 --> 01:04:35.160]   And it's touching.
[01:04:35.160 --> 01:04:39.480]   I can make things big and I can, if I want to annotate, I've got a tellestrator.
[01:04:39.480 --> 01:04:40.480]   This is like--
[01:04:40.480 --> 01:04:41.480]   How much is that, Compu?
[01:04:41.480 --> 01:04:42.480]   Oh, don't ask.
[01:04:42.480 --> 01:04:46.080]   If you have to ask.
[01:04:46.080 --> 01:04:47.080]   It starts at $3,000.
[01:04:47.080 --> 01:04:48.640]   It starts at $3,000.
[01:04:48.640 --> 01:04:52.960]   It's not a practical product, but none of Microsoft's newest products are their aspirational products.
[01:04:52.960 --> 01:04:53.960]   Look at this.
[01:04:53.960 --> 01:04:58.320]   Look at this new, we started talking about this Surface laptop with a fabric, but it's
[01:04:58.320 --> 01:05:01.320]   got fabric on the keyboard.
[01:05:01.320 --> 01:05:02.320]   It's a--
[01:05:02.320 --> 01:05:03.320]   Oh, cantata.
[01:05:03.320 --> 01:05:05.320]   It's a cantata keyboard.
[01:05:05.320 --> 01:05:10.320]   Well, I thought that was a, that's a, it looks awfully like an Apple.
[01:05:10.320 --> 01:05:16.040]   Well, that's what's kind of ironic is that a Microsoft lately has been standing there.
[01:05:16.040 --> 01:05:17.640]   It's stealing Apple's design.
[01:05:17.640 --> 01:05:23.520]   I mean, it literally looks like that someone just stole the CSS from Apple.com.
[01:05:23.520 --> 01:05:25.120]   And I know.
[01:05:25.120 --> 01:05:26.960]   Even their ads, have you seen the ad?
[01:05:26.960 --> 01:05:29.080]   This is the ad for the Surface laptop.
[01:05:29.080 --> 01:05:32.320]   Their new laptop, if I can find it, I'll play it for you.
[01:05:32.320 --> 01:05:33.320]   It's a--
[01:05:33.320 --> 01:05:34.320]   It's got Johnny I being like--
[01:05:34.320 --> 01:05:38.760]   It might as well have a white room with Sir Johnny in it.
[01:05:38.760 --> 01:05:40.040]   But you know, it's working.
[01:05:40.040 --> 01:05:43.040]   It's kind of like Apple's left this vacuum.
[01:05:43.040 --> 01:05:44.680]   So Microsoft says, what the heck?
[01:05:44.680 --> 01:05:49.280]   Incidentally, there's been some for you Microsoft fans all five of you.
[01:05:49.280 --> 01:05:50.280]   Oh.
[01:05:50.280 --> 01:05:51.280]   No.
[01:05:51.280 --> 01:05:52.280]   Look what I'm using.
[01:05:52.280 --> 01:05:53.280]   $3,000.
[01:05:53.280 --> 01:05:54.280]   I love that.
[01:05:54.280 --> 01:05:55.280]   Yeah, that is such a steal.
[01:05:55.280 --> 01:05:56.280]   It's the best.
[01:05:56.280 --> 01:05:57.280]   $3,000.
[01:05:57.280 --> 01:05:58.280]   If you look at Awakums and T.
[01:05:58.280 --> 01:05:59.280]   Look up--
[01:05:59.280 --> 01:06:00.280]   Those are still $1,700.
[01:06:00.280 --> 01:06:02.480]   It's like they threw in a computer for free.
[01:06:02.480 --> 01:06:03.480]   Right.
[01:06:03.480 --> 01:06:05.000]   And that's about as good a computer as you're getting.
[01:06:05.000 --> 01:06:06.000]   It's not the greatest.
[01:06:06.000 --> 01:06:07.000]   Yeah.
[01:06:07.000 --> 01:06:12.360]   No, I-- Microsoft has announced that they're going to do a hardware event in Shanghai in
[01:06:12.360 --> 01:06:13.360]   a couple of weeks.
[01:06:13.360 --> 01:06:18.000]   There will be new-- we think there'll be new Surface Pros, maybe a new Surface Book.
[01:06:18.000 --> 01:06:21.400]   And some people are still holding out for a Surface phone.
[01:06:21.400 --> 01:06:27.200]   I think we're in a bit of a holding period that we're about to cross over into the next
[01:06:27.200 --> 01:06:33.080]   wave of significantly transformative technology, which is all voice-driven.
[01:06:33.080 --> 01:06:36.760]   So I think that for the next couple of years, we're not going to see a ton of innovation
[01:06:36.760 --> 01:06:40.160]   in terms of just the form factor and the machines.
[01:06:40.160 --> 01:06:41.160]   But all of our modeling--
[01:06:41.160 --> 01:06:42.160]   I agree.
[01:06:42.160 --> 01:06:47.640]   But within the next five years, 50% of your interaction with the machine is going to be
[01:06:47.640 --> 01:06:49.080]   with your voice.
[01:06:49.080 --> 01:06:53.800]   And then that will start to compound very, very quickly.
[01:06:53.800 --> 01:07:01.240]   And all the patents we've seen for-- and I'm an old Apple user, so my irritation with
[01:07:01.240 --> 01:07:04.480]   them has to do with a lack of innovation in what's coming next.
[01:07:04.480 --> 01:07:06.800]   Speaking of audio, I have a question.
[01:07:06.800 --> 01:07:09.800]   I'd like to pull the panel here.
[01:07:09.800 --> 01:07:16.400]   Who has an echo and uses it?
[01:07:16.400 --> 01:07:24.600]   I don't have one because I've seen the-- frankly, the lack of security that Amazon has on the
[01:07:24.600 --> 01:07:26.680]   APIs around it.
[01:07:26.680 --> 01:07:27.680]   And I just don't--
[01:07:27.680 --> 01:07:28.680]   But what could it do?
[01:07:28.680 --> 01:07:29.680]   And what can it do?
[01:07:29.680 --> 01:07:30.680]   I mean, what's it--
[01:07:30.680 --> 01:07:31.680]   No, it can listen to what you're saying.
[01:07:31.680 --> 01:07:33.680]   It's like, like, you're sitting there.
[01:07:33.680 --> 01:07:38.920]   Yeah, but imagine you're sitting there and you tell a bad joke or you say something about
[01:07:38.920 --> 01:07:41.240]   your best friend or something.
[01:07:41.240 --> 01:07:44.280]   And someone watches the show one day and decides they don't like something you said
[01:07:44.280 --> 01:07:46.760]   and they docks your echo.
[01:07:46.760 --> 01:07:49.640]   I mean, I don't have my plug-in because of that reason.
[01:07:49.640 --> 01:07:50.640]   Yeah, I have an echo.
[01:07:50.640 --> 01:07:54.360]   I have an echo or a dot in every single room of my house.
[01:07:54.360 --> 01:08:00.360]   And I am on the waiting list to get the look, the new one to take-- forget listening, it's
[01:08:00.360 --> 01:08:01.560]   going to see me.
[01:08:01.560 --> 01:08:03.560]   It's got a camera on it with a--
[01:08:03.560 --> 01:08:05.400]   You got it for fashion advice, right?
[01:08:05.400 --> 01:08:06.400]   That's why.
[01:08:06.400 --> 01:08:07.400]   Yeah, because I need--
[01:08:07.400 --> 01:08:08.400]   You guys are all wearing black.
[01:08:08.400 --> 01:08:10.400]   I actually believe in color.
[01:08:10.400 --> 01:08:11.400]   [LAUGHTER]
[01:08:11.400 --> 01:08:13.000]   No, I'm just kidding.
[01:08:13.000 --> 01:08:17.320]   No, I'm all in on this because I agree with you, Amy Webb, that this is the voice, is the
[01:08:17.320 --> 01:08:18.320]   interface of the future.
[01:08:18.320 --> 01:08:19.320]   Look at this.
[01:08:19.320 --> 01:08:22.160]   Evan Blass, Evan Leakes on Twitter, has leaked images.
[01:08:22.160 --> 01:08:24.360]   This is from Amazon's servers.
[01:08:24.360 --> 01:08:30.440]   Of the next echo we-- there seems to be-- I'm going to come out in the next few weeks.
[01:08:30.440 --> 01:08:32.280]   This has a big screen on it.
[01:08:32.280 --> 01:08:36.240]   In fact, somebody said, forget this is the next business phone.
[01:08:36.240 --> 01:08:38.360]   Forget your business-- your entire business phone.
[01:08:38.360 --> 01:08:39.360]   Throw it out the window.
[01:08:39.360 --> 01:08:46.240]   Now, what did-- tell me about this security with the API thing, Nick, because-- is it
[01:08:46.240 --> 01:08:47.240]   really that bad?
[01:08:47.240 --> 01:08:56.120]   I don't-- I mean, look, I think it's a little-- I mean, we trust these technologies a little
[01:08:56.120 --> 01:08:57.120]   too much.
[01:08:57.120 --> 01:09:03.080]   I mean, it's like you're putting a thing in your home where you have private conversations
[01:09:03.080 --> 01:09:07.320]   that is constantly recording and sharing it out over to the internet.
[01:09:07.320 --> 01:09:09.720]   It's like there's no-- I don't know.
[01:09:09.720 --> 01:09:11.440]   That gives me a little pause.
[01:09:11.440 --> 01:09:16.680]   Maybe I've been doing this so long and seeing some of the terrors of hackers and things
[01:09:16.680 --> 01:09:17.680]   like that.
[01:09:17.680 --> 01:09:20.320]   But there's no two factor authentication on it.
[01:09:20.320 --> 01:09:21.320]   It's just--
[01:09:21.320 --> 01:09:22.320]   It's OK, Lexi.
[01:09:22.320 --> 01:09:23.320]   He's not talking.
[01:09:23.320 --> 01:09:24.840]   I would have to say this.
[01:09:24.840 --> 01:09:30.280]   I was in an event with students the other day where we were playing with Amazon's frameworks
[01:09:30.280 --> 01:09:31.680]   for this.
[01:09:31.680 --> 01:09:37.280]   And it's not that the technology is bad because I agree 100% with Amy.
[01:09:37.280 --> 01:09:39.760]   It's going to be the future and I love this stuff.
[01:09:39.760 --> 01:09:45.800]   But speaking very generally, it kind of has the same problem that Google has where anyone
[01:09:45.800 --> 01:09:49.440]   can come into it and build with these APIs.
[01:09:49.440 --> 01:09:54.040]   We can't really trust the software that they're selling alongside it.
[01:09:54.040 --> 01:09:59.200]   We actually saw a good example of that Google Doc-- the Google Doc email hack, which was
[01:09:59.200 --> 01:10:02.280]   using Google's app engine.
[01:10:02.280 --> 01:10:04.200]   And Google's APIs.
[01:10:04.200 --> 01:10:05.200]   But go ahead.
[01:10:05.200 --> 01:10:07.520]   I just thought-- and I'll bring that up.
[01:10:07.520 --> 01:10:11.720]   No, I was just going to say, I think that-- you know, I think that it's obviously a great
[01:10:11.720 --> 01:10:12.720]   technology.
[01:10:12.720 --> 01:10:15.600]   I just want to see them get the security aspect of it.
[01:10:15.600 --> 01:10:22.480]   So are you all worried that-- I don't know-- that the microphone's going to be turned
[01:10:22.480 --> 01:10:24.400]   on and Amazon's going to be listening to you?
[01:10:24.400 --> 01:10:25.840]   Are you worried that a third party?
[01:10:25.840 --> 01:10:26.840]   No.
[01:10:26.840 --> 01:10:27.840]   That the NSA?
[01:10:27.840 --> 01:10:28.840]   But you have--
[01:10:28.840 --> 01:10:29.840]   I don't know.
[01:10:29.840 --> 01:10:33.120]   --are you not carry in your pocket a device that's always connected to the internet with
[01:10:33.120 --> 01:10:35.520]   a microphone and a camera and a GPS?
[01:10:35.520 --> 01:10:36.520]   Yes.
[01:10:36.520 --> 01:10:39.680]   But I keep a piece of tape over and--
[01:10:39.680 --> 01:10:41.680]   You peek a vase and--
[01:10:41.680 --> 01:10:42.680]   Wait a minute.
[01:10:42.680 --> 01:10:44.880]   On your smartphone, you have a tape on your camera?
[01:10:44.880 --> 01:10:45.880]   I have this thing.
[01:10:45.880 --> 01:10:46.880]   Here, let me show you.
[01:10:46.880 --> 01:10:47.880]   Yeah.
[01:10:47.880 --> 01:10:48.880]   I have a little--
[01:10:48.880 --> 01:10:49.880]   I have me too.
[01:10:49.880 --> 01:10:50.880]   You guys are so paranoid.
[01:10:50.880 --> 01:10:51.880]   There.
[01:10:51.880 --> 01:10:53.880]   I've got a little thing on top of mine.
[01:10:53.880 --> 01:10:54.880]   Oh, my goodness.
[01:10:54.880 --> 01:10:55.880]   Here's what I would say.
[01:10:55.880 --> 01:11:00.160]   Because we had an early early-- my lab is full of stuff.
[01:11:00.160 --> 01:11:05.240]   So I've got an echo in the lab, and we're constantly testing different tools and things.
[01:11:05.240 --> 01:11:09.520]   The reality is that literally every aspect of your life is hackable.
[01:11:09.520 --> 01:11:10.520]   Everything.
[01:11:10.520 --> 01:11:11.520]   Yes.
[01:11:11.520 --> 01:11:12.520]   You could be--
[01:11:12.520 --> 01:11:14.520]   Any part of your daily life to be--
[01:11:14.520 --> 01:11:15.520]   Yes.
[01:11:15.520 --> 01:11:16.520]   --is--
[01:11:16.520 --> 01:11:18.760]   And ninjas could jump you as you walk down the street and murder you in your--
[01:11:18.760 --> 01:11:19.760]   You could.
[01:11:19.760 --> 01:11:20.760]   --as you back.
[01:11:20.760 --> 01:11:24.080]   The chances are that ninjas aren't after you and one hopes it hackers with the exception
[01:11:24.080 --> 01:11:26.480]   of Brianna Wu that hackers aren't after you.
[01:11:26.480 --> 01:11:27.480]   That's true.
[01:11:27.480 --> 01:11:28.680]   But there's a deeper analogy here.
[01:11:28.680 --> 01:11:30.520]   I have a black belt.
[01:11:30.520 --> 01:11:31.760]   So I have-- I do.
[01:11:31.760 --> 01:11:37.280]   So if ninjas did jump me on the street, I have enough ability probably to fend them off
[01:11:37.280 --> 01:11:38.960]   at least enough so I could run away.
[01:11:38.960 --> 01:11:44.240]   But you would rent the investment in the black belt was significant enough that not
[01:11:44.240 --> 01:11:47.200]   everybody who walks down the street should have one.
[01:11:47.200 --> 01:11:48.200]   Oh.
[01:11:48.200 --> 01:11:49.200]   Here's my point.
[01:11:49.200 --> 01:11:54.520]   I do think that everybody should have some form of a digital black belt and street smarts.
[01:11:54.520 --> 01:11:55.520]   Wow.
[01:11:55.520 --> 01:11:59.360]   And the challenge here is it's the year 2017.
[01:11:59.360 --> 01:12:04.800]   The very fact that people opened up that Google attachment should tell us something.
[01:12:04.800 --> 01:12:05.800]   Right?
[01:12:05.800 --> 01:12:08.680]   So to me, the big story wasn't that Google had this breach.
[01:12:08.680 --> 01:12:14.040]   To me, the big story was it's the year 2017 and the majority-- vast majority of people
[01:12:14.040 --> 01:12:20.640]   using technology still don't have a sophisticated enough understanding of how they should be
[01:12:20.640 --> 01:12:23.120]   using that technology.
[01:12:23.120 --> 01:12:24.120]   Right?
[01:12:24.120 --> 01:12:27.720]   So everything is hackable and everything can listen.
[01:12:27.720 --> 01:12:31.040]   And yeah, I've got tape, whatever, over all my cameras.
[01:12:31.040 --> 01:12:36.800]   But are you saying-- is your point that everything is hackable and so on and so on?
[01:12:36.800 --> 01:12:37.800]   No big deal?
[01:12:37.800 --> 01:12:38.800]   Or are you saying--
[01:12:38.800 --> 01:12:42.200]   I just feel like it's a weird sort of narcissism that you all have things over your cameras
[01:12:42.200 --> 01:12:44.200]   like somebody's dying to look at you.
[01:12:44.200 --> 01:12:45.840]   Well, then that's my point.
[01:12:45.840 --> 01:12:50.920]   My feeling is everything is hackable and most of us aren't doing interesting enough stuff
[01:12:50.920 --> 01:12:56.320]   to warrant somebody looking for our specific--
[01:12:56.320 --> 01:12:58.040]   I have a story for you.
[01:12:58.040 --> 01:13:01.720]   Here's why I have a piece of tape over my-- this is a pencil, by the way.
[01:13:01.720 --> 01:13:02.720]   I found it.
[01:13:02.720 --> 01:13:03.720]   I don't know where it came from.
[01:13:03.720 --> 01:13:05.800]   Someone left it in my office.
[01:13:05.800 --> 01:13:06.800]   Anyway.
[01:13:06.800 --> 01:13:08.360]   So that's not your smartphone.
[01:13:08.360 --> 01:13:09.360]   That's just a pencil.
[01:13:09.360 --> 01:13:10.360]   That's my eye pencil.
[01:13:10.360 --> 01:13:11.360]   Oh, OK.
[01:13:11.360 --> 01:13:13.800]   Kevin Rusz worked for Fusion.
[01:13:13.800 --> 01:13:16.080]   That is such a story, yes.
[01:13:16.080 --> 01:13:23.760]   And he-- for the listeners out there that don't know the story, he did a show, "Prefusion"
[01:13:23.760 --> 01:13:24.760]   on hackers.
[01:13:24.760 --> 01:13:32.440]   And he went in New York and he said to this guy who is a professional security expert,
[01:13:32.440 --> 01:13:33.440]   "I want you to hack me.
[01:13:33.440 --> 01:13:37.480]   I want you to get everything you can for me."
[01:13:37.480 --> 01:13:42.040]   And he left and didn't think anything out of it and didn't think they had succeeded.
[01:13:42.040 --> 01:13:45.760]   And they ended up in Vegas two days later.
[01:13:45.760 --> 01:13:51.200]   And they-- with cameras on in the room and he says, "So what'd you get?"
[01:13:51.200 --> 01:13:52.200]   And they pulled everything up.
[01:13:52.200 --> 01:13:54.720]   And what they had done is he'd walked out of the room.
[01:13:54.720 --> 01:13:58.480]   They sent him-- they figured out that he had a photo bucket account from just doing
[01:13:58.480 --> 01:13:59.640]   a Google search.
[01:13:59.640 --> 01:14:05.240]   They sent him a fake photo bucket to log in to check something or whatever.
[01:14:05.240 --> 01:14:06.240]   He clicked it.
[01:14:06.240 --> 01:14:08.840]   This is a guy who's a reporter who's been covering this stuff.
[01:14:08.840 --> 01:14:10.440]   He clicked it, not thinking anything of it.
[01:14:10.440 --> 01:14:12.240]   As you wouldn't.
[01:14:12.240 --> 01:14:14.880]   And next thing you know, they installed a piece of software on his computer that is
[01:14:14.880 --> 01:14:18.880]   capturing a screenshot of his screen every two seconds and taking a picture of him every
[01:14:18.880 --> 01:14:21.520]   two seconds from his camera without the light going on.
[01:14:21.520 --> 01:14:23.880]   And the look on his face was terrifying.
[01:14:23.880 --> 01:14:30.000]   Yeah, but he did something that no one should do, which is I dared two expert hackers to
[01:14:30.000 --> 01:14:31.000]   destroy my life.
[01:14:31.000 --> 01:14:32.000]   I mean--
[01:14:32.000 --> 01:14:37.160]   So the point of it was that that is the future, right?
[01:14:37.160 --> 01:14:38.720]   That can happen to anyone.
[01:14:38.720 --> 01:14:40.640]   It could happen to you right now.
[01:14:40.640 --> 01:14:41.640]   The future--
[01:14:41.640 --> 01:14:42.640]   Yeah, I have it.
[01:14:42.640 --> 01:14:43.640]   It's right now.
[01:14:43.640 --> 01:14:44.640]   This happened right now.
[01:14:44.640 --> 01:14:48.760]   And the techniques that they use, the social engineering, this woman pretended to be his
[01:14:48.760 --> 01:14:55.560]   wife and played baby sounds from YouTube to the customer service representative at the
[01:14:55.560 --> 01:14:59.640]   phone company to get access to his cell phone account.
[01:14:59.640 --> 01:15:04.160]   She said, "My husband just left town and I got it and the baby's crying and I got it
[01:15:04.160 --> 01:15:05.160]   again from his phone."
[01:15:05.160 --> 01:15:09.040]   But that social engineering, this has been going on.
[01:15:09.040 --> 01:15:13.840]   So I don't understand what you guys are saying because I don't think putting a piece of tape
[01:15:13.840 --> 01:15:18.040]   over your camera is doing anything but making you feel better.
[01:15:18.040 --> 01:15:25.080]   So driving in a car with a safety belt, that doesn't mean you're automatically going to
[01:15:25.080 --> 01:15:28.720]   live if someone blindsides you or something like that.
[01:15:28.720 --> 01:15:30.560]   But it's a precaution.
[01:15:30.560 --> 01:15:33.000]   It's a probability play.
[01:15:33.000 --> 01:15:36.400]   I have a theory on where I think we're headed with all of this.
[01:15:36.400 --> 01:15:38.040]   So I used to live in Japan.
[01:15:38.040 --> 01:15:40.560]   And in Japanese, there is no word for privacy.
[01:15:40.560 --> 01:15:42.240]   They had to borrow it from English.
[01:15:42.240 --> 01:15:43.960]   And the word is put ayubashi.
[01:15:43.960 --> 01:15:46.240]   And the reason for that has to do with the internet.
[01:15:46.240 --> 01:15:49.240]   So it has to do with people making payments over the internet.
[01:15:49.240 --> 01:15:50.600]   And this was like a novel string.
[01:15:50.600 --> 01:15:54.080]   Even the Japan is very, very tech forward.
[01:15:54.080 --> 01:15:56.480]   This piece of it was really strange.
[01:15:56.480 --> 01:16:02.400]   And it's interesting to me how quickly everybody's feelings about their privacy eroded because
[01:16:02.400 --> 01:16:04.640]   of the sheer convenience of technology.
[01:16:04.640 --> 01:16:10.880]   And so my feeling is at some point, it's so difficult for everybody to have this massive
[01:16:10.880 --> 01:16:18.880]   coordinated effort to stay two steps ahead of hackers that at some point we all just
[01:16:18.880 --> 01:16:25.560]   say, the only way for me not to be compromised is to have zero privacy whatsoever.
[01:16:25.560 --> 01:16:27.560]   Right?
[01:16:27.560 --> 01:16:32.320]   And I wonder if at some point that's the direction that we start to get in.
[01:16:32.320 --> 01:16:34.920]   No, no, I have to say.
[01:16:34.920 --> 01:16:38.120]   I mean, during gamergate, I was hyper targeted.
[01:16:38.120 --> 01:16:41.720]   My companies, Nantles were targeted.
[01:16:41.720 --> 01:16:45.360]   I was doxing extreme ways and it continues to this day.
[01:16:45.360 --> 01:16:47.840]   Yeah, Leo, this is an utterly rational fear.
[01:16:47.840 --> 01:16:50.320]   John Podesta is a recently bright man.
[01:16:50.320 --> 01:16:52.640]   He was tricked by a phishing scam.
[01:16:52.640 --> 01:16:55.480]   If you've read "Shattered," it's an excellent book on that.
[01:16:55.480 --> 01:16:56.480]   We saw the same thing.
[01:16:56.480 --> 01:16:58.600]   It appears to have happened in the French election.
[01:16:58.600 --> 01:17:02.560]   So for me, it's not paranoia to worry about security.
[01:17:02.560 --> 01:17:05.840]   It is reality, like born out by the press.
[01:17:05.840 --> 01:17:12.640]   For me, I believe there's got to be a legislative component to solve this because the truth is,
[01:17:12.640 --> 01:17:18.640]   if you asked your audience and asked them how many were engineers and they were asked
[01:17:18.640 --> 01:17:24.320]   to compile or save tons of information about people to then go sell it later, they would
[01:17:24.320 --> 01:17:26.880]   all have horror stories to tell you.
[01:17:26.880 --> 01:17:30.240]   I believe that we've got to say, "Okay, you know what?
[01:17:30.240 --> 01:17:35.880]   If a company A wants to collate all this data, fine.
[01:17:35.880 --> 01:17:41.120]   If you get permission, then you have permission from the user to do that."
[01:17:41.120 --> 01:17:46.240]   If you don't keep it safe, if you don't hire infosec people to salt and hash it, to keep
[01:17:46.240 --> 01:17:52.680]   it secure way, if you're not doing due diligence to keep it safe, I do think we should pass
[01:17:52.680 --> 01:17:56.080]   laws that make you civilly liable.
[01:17:56.080 --> 01:18:02.880]   I also believe that we've got to pass an omnibus privacy bill because we can't compete with
[01:18:02.880 --> 01:18:06.680]   the corporate interests in selling our privacy out.
[01:18:06.680 --> 01:18:13.040]   I really believe if we work with Congress, I think that Congress can really put a legislative
[01:18:13.040 --> 01:18:15.560]   solution forward because the market's not going to solve it.
[01:18:15.560 --> 01:18:16.800]   You're being hacked right now, Brianna.
[01:18:16.800 --> 01:18:17.800]   Your memory comes from the US.
[01:18:17.800 --> 01:18:18.800]   I know I am.
[01:18:18.800 --> 01:18:19.800]   Yes, I am.
[01:18:19.800 --> 01:18:20.800]   I am.
[01:18:20.800 --> 01:18:21.800]   I am.
[01:18:21.800 --> 01:18:25.680]   There's no penalty for a company like Target, for instance.
[01:18:25.680 --> 01:18:26.680]   We lost Brianna.
[01:18:26.680 --> 01:18:28.760]   I'll wait until she gets back.
[01:18:28.760 --> 01:18:32.320]   That's a kind of interesting concept, though.
[01:18:32.320 --> 01:18:34.120]   There are seat belt laws.
[01:18:34.120 --> 01:18:39.960]   There are certain corporate laws and laws having to do with our safety.
[01:18:39.960 --> 01:18:42.280]   I think the biggest problem was something like that.
[01:18:42.280 --> 01:18:46.520]   We no longer have an Office of Technology Assessment, which would have been the part
[01:18:46.520 --> 01:18:50.760]   of the government charged with thinking this through.
[01:18:50.760 --> 01:18:52.360]   Maybe Brianna gets elected.
[01:18:52.360 --> 01:18:54.760]   We have some hope that there'll be some members of Congress.
[01:18:54.760 --> 01:18:58.360]   The other thing is, I think, I'm a reporter.
[01:18:58.360 --> 01:19:01.960]   I've written some stories that some people didn't like.
[01:19:01.960 --> 01:19:05.680]   I've had threats and God knows what else.
[01:19:05.680 --> 01:19:08.720]   Especially when I was at the New York Times.
[01:19:08.720 --> 01:19:16.600]   One of the things I think is what I've learned covering all of these stories, too, is that
[01:19:16.600 --> 01:19:20.720]   people with a lot of money and people that are very famous, they hire these people.
[01:19:20.720 --> 01:19:25.240]   These groups, these security firms, to come out and they make sure they have two parts
[01:19:25.240 --> 01:19:27.160]   for authentication turned on.
[01:19:27.160 --> 01:19:29.920]   All their passwords are hashed.
[01:19:29.920 --> 01:19:32.040]   They do all these things for them.
[01:19:32.040 --> 01:19:35.680]   They monitor the dark web to make sure their Social Security numbers aren't being sold
[01:19:35.680 --> 01:19:36.680]   most of the time.
[01:19:36.680 --> 01:19:40.240]   They are, actually, believe it or not, that their addresses aren't online and things
[01:19:40.240 --> 01:19:41.240]   like that.
[01:19:41.240 --> 01:19:42.240]   They do all these things.
[01:19:42.240 --> 01:19:44.440]   They monitor on social media and so on.
[01:19:44.440 --> 01:19:49.520]   I think that one of the things I do believe will happen is that will become part of something
[01:19:49.520 --> 01:19:51.480]   we pay for everyone does it.
[01:19:51.480 --> 01:19:53.360]   The price will come down eventually.
[01:19:53.360 --> 01:19:57.600]   In the same way I hire a locksmith to install a good lock on my front door, that we will
[01:19:57.600 --> 01:19:59.560]   do that with our technologies and devices.
[01:19:59.560 --> 01:20:03.840]   I don't believe that most people are just going to throw their hands in there and be
[01:20:03.840 --> 01:20:05.840]   like, "Oh, nothing happens."
[01:20:05.840 --> 01:20:08.160]   No, I wasn't saying that.
[01:20:08.160 --> 01:20:10.680]   I was saying it more as a collective defense.
[01:20:10.680 --> 01:20:11.680]   No, I was.
[01:20:11.680 --> 01:20:12.680]   No, I was.
[01:20:12.680 --> 01:20:17.720]   But Nick, what you just said, creates a new kind of digital divide, doesn't it?
[01:20:17.720 --> 01:20:18.720]   Yeah.
[01:20:18.720 --> 01:20:19.720]   It could be true.
[01:20:19.720 --> 01:20:22.040]   Those are going to afford privacy and those who can't.
[01:20:22.040 --> 01:20:26.320]   Well, not just afford, but understand the implications of that and then have the ability
[01:20:26.320 --> 01:20:28.640]   to do something about it.
[01:20:28.640 --> 01:20:30.640]   And I ought to be frank.
[01:20:30.640 --> 01:20:34.040]   We got banana back so I can say this now, but I don't have a lot of confidence that
[01:20:34.040 --> 01:20:37.440]   even laws will do much to changes.
[01:20:37.440 --> 01:20:42.400]   People who are doing this already are violating laws.
[01:20:42.400 --> 01:20:47.480]   I agree that companies that allow our databases to be leaked should be held responsible.
[01:20:47.480 --> 01:20:49.880]   In some cases, they have been.
[01:20:49.880 --> 01:20:55.480]   And I think Visa actually took target to the woodshed.
[01:20:55.480 --> 01:20:59.960]   So sometimes the punishment comes from sources non-governmental.
[01:20:59.960 --> 01:21:04.640]   But if you stop and think about it for a moment, Sony's been hacked how many times over the
[01:21:04.640 --> 01:21:06.800]   past 15 years, right?
[01:21:06.800 --> 01:21:08.400]   And Sony paid the price for it.
[01:21:08.400 --> 01:21:09.400]   Yeah, but they didn't care.
[01:21:09.400 --> 01:21:12.600]   But they've done this over and over again.
[01:21:12.600 --> 01:21:13.600]   Go ahead.
[01:21:13.600 --> 01:21:14.600]   Yahoo.
[01:21:14.600 --> 01:21:16.960]   They kept it secret.
[01:21:16.960 --> 01:21:20.040]   Is Merci Mer speaking in front of Congress and being asked what happens?
[01:21:20.040 --> 01:21:24.720]   Well, look at the complicity between Travis Kalanick and Uber and Apple.
[01:21:24.720 --> 01:21:31.200]   When Apple found out that Uber was violating its, severely violating its rules and the
[01:21:31.200 --> 01:21:34.600]   privacy of iPhone users, Apple said knock it off.
[01:21:34.600 --> 01:21:38.440]   They didn't say, oh, and you better tell everybody what you did.
[01:21:38.440 --> 01:21:40.520]   So there's complicity all around on this.
[01:21:40.520 --> 01:21:44.920]   There are no repercussions because of the repercussions.
[01:21:44.920 --> 01:21:46.280]   Everybody's vulnerable.
[01:21:46.280 --> 01:21:47.280]   And everybody.
[01:21:47.280 --> 01:21:52.000]   And that was an example with a Sony hack Amy is Sony knew that they were vulnerable and
[01:21:52.000 --> 01:21:56.880]   made a financial calculation that they'd rather be vulnerable than pay the money it took
[01:21:56.880 --> 01:21:57.880]   to protect themselves.
[01:21:57.880 --> 01:22:02.120]   But the thing that's fascinating to me about it is that they did that over and over and
[01:22:02.120 --> 01:22:03.120]   over again.
[01:22:03.120 --> 01:22:07.880]   And the people who were in charge saw each one of those incidences as a novel one-off.
[01:22:07.880 --> 01:22:14.000]   And you know, which thing is I remember with the AT&T hack, sorry, Amy.
[01:22:14.000 --> 01:22:15.000]   I remember with the AT&T.
[01:22:15.000 --> 01:22:22.840]   Remember when there was the AT&T hack by I forget the guy's name, Weave in New Jersey.
[01:22:22.840 --> 01:22:23.840]   Yeah.
[01:22:23.840 --> 01:22:24.840]   Yeah.
[01:22:24.840 --> 01:22:26.640]   So when that happened, it wasn't really a hack.
[01:22:26.640 --> 01:22:29.840]   It was they figured out how to, I guess it was a hack, but they figured out how to get
[01:22:29.840 --> 01:22:36.440]   people's email addresses and passcodes from hitting the AT&T server with the new iPad software,
[01:22:36.440 --> 01:22:37.440]   whatever.
[01:22:37.440 --> 01:22:44.080]   They ended up, first, you know, the news broke and it made a big splash in the tech industry.
[01:22:44.080 --> 01:22:48.640]   When it turned out that there were people who worked for the White House or for the Department
[01:22:48.640 --> 01:22:52.480]   of Justice and things like that or governors or senators who's email addresses became
[01:22:52.480 --> 01:22:58.520]   public, the DOJ came after it like a house on a fire and they ended up arresting people
[01:22:58.520 --> 01:23:00.600]   and putting them in jail and so on and so forth.
[01:23:00.600 --> 01:23:05.200]   And so I'm sure that the Sony hack didn't necessarily affect anyone that could have actually
[01:23:05.200 --> 01:23:09.400]   done something about it because if it had of, then there would have been repercussions.
[01:23:09.400 --> 01:23:14.240]   But because it's just everyday people, they don't care.
[01:23:14.240 --> 01:23:18.040]   Well, Sony paid the price, not an insignificant price, I might add.
[01:23:18.040 --> 01:23:22.120]   Let's take a little break and we will get back to the conversation with one of the best panels
[01:23:22.120 --> 01:23:25.840]   we've had in a long time and we've had some really good panels of like Nick Bilton is
[01:23:25.840 --> 01:23:31.560]   here, Vanity Fair, author of a brand new book called American Kingpin.
[01:23:31.560 --> 01:23:34.200]   I'm just guessing soon to be a major motion picture.
[01:23:34.200 --> 01:23:35.200]   Yeah.
[01:23:35.200 --> 01:23:39.960]   We have optioned it to box and the Cohen brothers are working on it.
[01:23:39.960 --> 01:23:40.960]   Oh my God.
[01:23:40.960 --> 01:23:44.880]   On a screenplay and I should do a giveaway.
[01:23:44.880 --> 01:23:47.840]   Can we like give some away to some of the people on the chat?
[01:23:47.840 --> 01:23:48.840]   How do we do that?
[01:23:48.840 --> 01:23:50.960]   Do they like tweet at me like the first?
[01:23:50.960 --> 01:23:51.960]   Don't don't open that door.
[01:23:51.960 --> 01:23:52.960]   I don't know what I do.
[01:23:52.960 --> 01:23:55.560]   I think it's think of something and let me know.
[01:23:55.560 --> 01:23:58.800]   You know what next time you come on and maybe do this when you're on triangulation.
[01:23:58.800 --> 01:24:02.160]   Just talk to your author, your publisher about getting a discount.
[01:24:02.160 --> 01:24:03.160]   Author.
[01:24:03.160 --> 01:24:04.160]   You're the author.
[01:24:04.160 --> 01:24:05.160]   Maybe get a discount code.
[01:24:05.160 --> 01:24:09.160]   Who's going to play Ross Albrecht in the movie?
[01:24:09.160 --> 01:24:14.160]   You know who was interested, believe it or not, Jared Leto.
[01:24:14.160 --> 01:24:16.160]   It looks exactly like him.
[01:24:16.160 --> 01:24:17.160]   That's good casting.
[01:24:17.160 --> 01:24:18.160]   Yeah.
[01:24:18.160 --> 01:24:20.160]   It looks like a skinny version of him, right?
[01:24:20.160 --> 01:24:23.160]   Well, you know, Jared could get any weight you want.
[01:24:23.160 --> 01:24:24.160]   Yeah.
[01:24:24.160 --> 01:24:25.160]   Yeah.
[01:24:25.160 --> 01:24:26.160]   What weight do you need?
[01:24:26.160 --> 01:24:27.160]   Anything you want?
[01:24:27.160 --> 01:24:32.560]   It's, no, it's, it's, it's, it's, it's essentially a real life breaking about the story.
[01:24:32.560 --> 01:24:33.560]   It's an amazing story.
[01:24:33.560 --> 01:24:34.560]   Yeah.
[01:24:34.560 --> 01:24:37.720]   There's cops that turn bad and start fake killing people.
[01:24:37.720 --> 01:24:39.000]   And it's, it's insane.
[01:24:39.000 --> 01:24:43.680]   It's a really, really fascinating tale.
[01:24:43.680 --> 01:24:45.720]   Soon to be a major motion picture.
[01:24:45.720 --> 01:24:46.880]   Nick will be on triangulation.
[01:24:46.880 --> 01:24:47.880]   Starring Leo.
[01:24:47.880 --> 01:24:48.880]   Starring.
[01:24:48.880 --> 01:24:49.880]   Jared Leo.
[01:24:49.880 --> 01:24:55.040]   When, when will Nick be on next week, the week after?
[01:24:55.040 --> 01:24:57.560]   Week from two weeks from Monday.
[01:24:57.560 --> 01:24:58.560]   Something like that.
[01:24:58.560 --> 01:24:59.560]   Something like that.
[01:24:59.560 --> 01:25:01.560]   Daniel Suarez actually is going to join us on Monday.
[01:25:01.560 --> 01:25:02.560]   And we'll do it.
[01:25:02.560 --> 01:25:03.560]   We'll do a book giveaway then.
[01:25:03.560 --> 01:25:04.560]   We'll figure it out.
[01:25:04.560 --> 01:25:05.560]   You got it.
[01:25:05.560 --> 01:25:06.560]   You and your publisher get together.
[01:25:06.560 --> 01:25:07.560]   Talk about it.
[01:25:07.560 --> 01:25:09.200]   Talk amongst yourselves.
[01:25:09.200 --> 01:25:11.000]   Sometimes people do offer codes.
[01:25:11.000 --> 01:25:13.400]   I also available on Audible, which is nice.
[01:25:13.400 --> 01:25:14.640]   It's, it's, there's an audiobook version.
[01:25:14.640 --> 01:25:17.080]   It's an amazing, it's really the, it's a really, it's a audible guide.
[01:25:17.080 --> 01:25:18.480]   You're going to do a good job?
[01:25:18.480 --> 01:25:19.880]   Audible, they're all fantastic.
[01:25:19.880 --> 01:25:22.000]   They're all, they're, I love Audible audiobooks.
[01:25:22.000 --> 01:25:23.000]   They're so great.
[01:25:23.000 --> 01:25:24.000]   I know.
[01:25:24.000 --> 01:25:25.160]   We were all before the show.
[01:25:25.160 --> 01:25:29.200]   We were saying that's the way to get through these is just get the audiobook and you can
[01:25:29.200 --> 01:25:35.480]   listen in your sleep or whatever you're not, you know, working.
[01:25:35.480 --> 01:25:38.400]   That would be, that would be the greatest thing ever if I could wire in while I was thinking.
[01:25:38.400 --> 01:25:39.880]   Could you invent that, Amy?
[01:25:39.880 --> 01:25:40.880]   Come on.
[01:25:40.880 --> 01:25:43.640]   I would be, that would be amazing.
[01:25:43.640 --> 01:25:44.640]   Sleep learning.
[01:25:44.640 --> 01:25:46.200]   Every science fiction book has it.
[01:25:46.200 --> 01:25:47.200]   Yeah.
[01:25:47.200 --> 01:25:48.960]   Why haven't we invented it yet?
[01:25:48.960 --> 01:25:50.920]   Amy is the author of The Signals Are Talking.
[01:25:50.920 --> 01:25:54.440]   Why today's French is tomorrow's mainstream.
[01:25:54.440 --> 01:25:55.880]   Become a futurist.
[01:25:55.880 --> 01:26:00.480]   You got everything you need to know right in here from Public Affairs Books.
[01:26:00.480 --> 01:26:01.480]   This is great.
[01:26:01.480 --> 01:26:04.560]   We had a great triangulation, which you can go back and watch.
[01:26:04.560 --> 01:26:06.840]   I could do this with my book.
[01:26:06.840 --> 01:26:10.920]   If you bought a copy of the book, if you have it or if you have it on Kindle, if you tweet
[01:26:10.920 --> 01:26:15.480]   me a photo of it, I'll send you a sign and I can send you a signed book plate.
[01:26:15.480 --> 01:26:16.480]   Oh, nice.
[01:26:16.480 --> 01:26:19.040]   So tweet, tweet at me with.
[01:26:19.040 --> 01:26:21.400]   Send you a picture.
[01:26:21.400 --> 01:26:25.440]   You have to have a, today's newspaper.
[01:26:25.440 --> 01:26:27.440]   Your head and the book.
[01:26:27.440 --> 01:26:28.720]   And a cipher like that.
[01:26:28.720 --> 01:26:31.680]   I'm going to, I'm going to one up that.
[01:26:31.680 --> 01:26:36.760]   If you bought a copy of my book and send me a picture of yourself naked with the book.
[01:26:36.760 --> 01:26:37.760]   No, Nick.
[01:26:37.760 --> 01:26:41.040]   I will send you a picture of Leo naked with the book.
[01:26:41.040 --> 01:26:42.560]   Those are widely available.
[01:26:42.560 --> 01:26:43.560]   No big deal.
[01:26:43.560 --> 01:26:47.760]   They're very, this is, they are both hilarious, but I was actually trying to be nice.
[01:26:47.760 --> 01:26:51.200]   So if anybody actually really wants my handwriting for whatever.
[01:26:51.200 --> 01:26:52.200]   Thank you, Amy.
[01:26:52.200 --> 01:26:53.200]   That's really nice.
[01:26:53.200 --> 01:26:54.200]   I'm going to do that.
[01:26:54.200 --> 01:26:55.200]   I guess you can hack Amy.
[01:26:55.200 --> 01:26:59.080]   Amy, I just bought, I just bought your book actually while we were on, on the show.
[01:26:59.080 --> 01:27:00.080]   So highly recommended.
[01:27:00.080 --> 01:27:05.880]   Well, I'm not sending you a signed anything until I see a photo or it didn't happen.
[01:27:05.880 --> 01:27:08.320]   Can I cover my privates with the book with that?
[01:27:08.320 --> 01:27:10.640]   As long as you're wearing that orange sweater.
[01:27:10.640 --> 01:27:11.960]   It's not quite big enough.
[01:27:11.960 --> 01:27:12.960]   All right.
[01:27:12.960 --> 01:27:13.960]   We're going to take a break.
[01:27:13.960 --> 01:27:14.960]   I'm sorry.
[01:27:14.960 --> 01:27:17.480]   Brianna Wu was just saying, I wish I weren't here and my camera is not working.
[01:27:17.480 --> 01:27:18.480]   And she's also here.
[01:27:18.480 --> 01:27:19.480]   We love Brianna.
[01:27:19.480 --> 01:27:21.680]   Brianna Wu's space cat gal.
[01:27:21.680 --> 01:27:22.680]   And we will have more in a moment.
[01:27:22.680 --> 01:27:24.080]   Our show brought to you by FreshBooks.
[01:27:24.080 --> 01:27:26.880]   If you're in business for yourself, I know you got business because you have something
[01:27:26.880 --> 01:27:29.880]   you love to do, a product you want to make, a service you want to offer.
[01:27:29.880 --> 01:27:33.440]   The last thing you want to do is be a bookkeeper, unless that's your business.
[01:27:33.440 --> 01:27:35.560]   In this case, this ad is not for you.
[01:27:35.560 --> 01:27:41.840]   But if you were a normal person and you don't want to do books, you got to know about FreshBooks.
[01:27:41.840 --> 01:27:47.440]   For me, FreshBooks was an eye opener 10 years ago when I used to have to send invoices
[01:27:47.440 --> 01:27:49.880]   right with expense and for expenses and time.
[01:27:49.880 --> 01:27:52.920]   And I would put it off and I wouldn't get paid.
[01:27:52.920 --> 01:27:54.800]   I learned something that day.
[01:27:54.800 --> 01:27:56.040]   You don't send an invoice.
[01:27:56.040 --> 01:27:57.560]   They don't pay you.
[01:27:57.560 --> 01:27:59.720]   In fact, I was complaining about it.
[01:27:59.720 --> 01:28:01.560]   Amber MacArthur was my co-host of my Canada.
[01:28:01.560 --> 01:28:06.120]   I said, Leo, there's a new company just started up in Toronto called FreshBooks.
[01:28:06.120 --> 01:28:07.320]   I'll tell you how long ago this was.
[01:28:07.320 --> 01:28:10.000]   It was a web 2.0 company.
[01:28:10.000 --> 01:28:11.160]   And you should use it.
[01:28:11.160 --> 01:28:12.680]   You can make your invoices easily.
[01:28:12.680 --> 01:28:13.680]   I did.
[01:28:13.680 --> 01:28:15.040]   It turned my life around.
[01:28:15.040 --> 01:28:17.560]   And since then, FreshBooks has grown and grown.
[01:28:17.560 --> 01:28:21.720]   Now 10 million small business owners use FreshBooks, not just for invoicing, but to
[01:28:21.720 --> 01:28:24.640]   keep track of profit, loss, expenses.
[01:28:24.640 --> 01:28:30.280]   You now know, and I think this is pretty true, most small business owners, if you said, "Are
[01:28:30.280 --> 01:28:31.960]   you making a profit this year?
[01:28:31.960 --> 01:28:32.960]   Did you make?"
[01:28:32.960 --> 01:28:35.200]   They don't know till tax time if they made money.
[01:28:35.200 --> 01:28:38.720]   FreshBooks, the dashboard will tell you at any given moment how much money are you
[01:28:38.720 --> 01:28:39.720]   making?
[01:28:39.720 --> 01:28:40.720]   Who owes you money?
[01:28:40.720 --> 01:28:42.320]   Who do you need to invoice?
[01:28:42.320 --> 01:28:43.840]   What invoices are unpaid?
[01:28:43.840 --> 01:28:45.560]   What are your expenses looking like?
[01:28:45.560 --> 01:28:46.800]   You can manage team time sheets.
[01:28:46.800 --> 01:28:51.680]   In fact, you could do all your hours tracking in the FreshBooks app on your phone.
[01:28:51.680 --> 01:28:56.760]   Or on the FreshBooks site, you can make your invoices look sweet, random, send them recurring
[01:28:56.760 --> 01:29:00.160]   invoices, auto payment reminders, automatic late fees.
[01:29:00.160 --> 01:29:02.720]   You can even make it so your client pays you automatically.
[01:29:02.720 --> 01:29:07.800]   If your client wants to do that, that takes the burden of paperwork off both of your shoulders.
[01:29:07.800 --> 01:29:08.800]   And that's awesome.
[01:29:08.800 --> 01:29:16.600]   In fact, FreshBooks customers get paid an average of 11 days faster.
[01:29:16.600 --> 01:29:18.760]   That is amazing.
[01:29:18.760 --> 01:29:19.760]   You got to try it.
[01:29:19.760 --> 01:29:21.200]   Free, 30 days.
[01:29:21.200 --> 01:29:23.200]   FreshBooks.com/twit.
[01:29:23.200 --> 01:29:26.040]   If you'd write this week in tech when they ask you how you hear about us, that would
[01:29:26.040 --> 01:29:28.560]   be a very nice thing for me.
[01:29:28.560 --> 01:29:32.400]   You FreshBooks integrates with many of the apps you use already, like Stripe and Shopify
[01:29:32.400 --> 01:29:34.000]   and Gusto Acuity Scheduling.
[01:29:34.000 --> 01:29:37.160]   So you can get those payments integrated in without any typing.
[01:29:37.160 --> 01:29:40.120]   I mean, this is really the easiest way to keep your books.
[01:29:40.120 --> 01:29:46.480]   FreshBooks.com/twit for 30 days free.
[01:29:46.480 --> 01:29:53.040]   The life saving service for freelancers and small businesses should save my behind.
[01:29:53.040 --> 01:29:59.040]   We are talking about the world of tech with three of the most interesting people.
[01:29:59.040 --> 01:30:03.960]   I know Brianna Wu running for Congress in Massachusetts, 8th District.
[01:30:03.960 --> 01:30:05.480]   You got your camera fixed?
[01:30:05.480 --> 01:30:06.880]   I did, I did.
[01:30:06.880 --> 01:30:07.880]   Good.
[01:30:07.880 --> 01:30:09.440]   You weren't being hacked, were you?
[01:30:09.440 --> 01:30:10.440]   No, no, no.
[01:30:10.440 --> 01:30:13.680]   We're safe at the Wu family headquarters here.
[01:30:13.680 --> 01:30:14.680]   Ultimate irony.
[01:30:14.680 --> 01:30:20.760]   You know, it's actually, was it you Amy or was it Nick who said this is an example of
[01:30:20.760 --> 01:30:24.160]   the haves, it was you Amy, the haves and have-nots.
[01:30:24.160 --> 01:30:27.720]   There will be people who have the tech savvy to protect themselves and then the people
[01:30:27.720 --> 01:30:33.680]   who don't, which is by the way 90%, but 90% of people don't even know there's an issue.
[01:30:33.680 --> 01:30:38.280]   And we haven't even talked about things like VR and AR and the people who can afford digital
[01:30:38.280 --> 01:30:42.760]   cloaking so that your faces can't be recognized and your eyes can't be seen and it's a whole
[01:30:42.760 --> 01:30:45.160]   other couple of years down the road.
[01:30:45.160 --> 01:30:50.320]   No, and the lack of understanding, one of the most Google things on Google is Google.
[01:30:50.320 --> 01:30:51.320]   I mean, that's-
[01:30:51.320 --> 01:30:52.320]   Yeah, people-
[01:30:52.320 --> 01:30:58.120]   I remember going to Google some years ago and they used to have in the lobby, the current
[01:30:58.120 --> 01:30:59.560]   real-time searches, remember?
[01:30:59.560 --> 01:31:01.280]   And the number one search was Yahoo.
[01:31:01.280 --> 01:31:02.280]   And it was-
[01:31:02.280 --> 01:31:08.480]   It's because people would just in the Google thing type Yahoo to get to Yahoo.
[01:31:08.480 --> 01:31:10.680]   That's how people are.
[01:31:10.680 --> 01:31:13.760]   And yet it was many of us smarter than the average bear types.
[01:31:13.760 --> 01:31:20.600]   A lot of journalists who got bit by this Google Doc email that spread like wildfire.
[01:31:20.600 --> 01:31:21.600]   When was this?
[01:31:21.600 --> 01:31:22.600]   Was this Tuesday or Wednesday of this week?
[01:31:22.600 --> 01:31:23.600]   It was a couple days ago.
[01:31:23.600 --> 01:31:24.600]   Yeah.
[01:31:24.600 --> 01:31:25.600]   It just happened.
[01:31:25.600 --> 01:31:27.880]   You'd get an email from someone you knew.
[01:31:27.880 --> 01:31:31.720]   This is why it was- and journalists particularly were vulnerable because journalists get a lot
[01:31:31.720 --> 01:31:34.200]   of shared documents, right?
[01:31:34.200 --> 01:31:40.200]   You'd get an email from somebody you knew that said, "This person has invited you to
[01:31:40.200 --> 01:31:41.680]   view the following document.
[01:31:41.680 --> 01:31:43.360]   You got the button you're used to, open in docs."
[01:31:43.360 --> 01:31:49.560]   And of course when it did that, it would launch Google's official OAuth login, including
[01:31:49.560 --> 01:31:51.120]   incidentally.
[01:31:51.120 --> 01:31:55.920]   It would- two factor would, you know, it all worked right because it wasn't illegal.
[01:31:55.920 --> 01:31:57.680]   It was a Google app.
[01:31:57.680 --> 01:32:01.760]   But if you paid attention, there were a couple- and I think most people aren't looking for
[01:32:01.760 --> 01:32:03.880]   this- a couple of things.
[01:32:03.880 --> 01:32:10.160]   First of all, this HHHH that it was shared with in addition to the person you know,
[01:32:10.160 --> 01:32:13.760]   but then there was one even more important, which I don't know if I have here, which is
[01:32:13.760 --> 01:32:17.120]   you would receive a permissions request.
[01:32:17.120 --> 01:32:23.840]   And in the permissions request, it would say Google Docs wants permission to open, read,
[01:32:23.840 --> 01:32:29.240]   send your email at Gmail and to look at all your contacts.
[01:32:29.240 --> 01:32:33.400]   And if you had thought about it, you might not have clicked the allow button at that
[01:32:33.400 --> 01:32:38.440]   point because it's exactly what happened is the app then got your contact list and
[01:32:38.440 --> 01:32:43.240]   emailed the same email to everybody you know, embarrassing you.
[01:32:43.240 --> 01:32:44.480]   That's the rub, right?
[01:32:44.480 --> 01:32:48.640]   If you had stopped and thought about it, and I- you know, this is what I keep coming back
[01:32:48.640 --> 01:32:49.640]   to.
[01:32:49.640 --> 01:32:53.520]   You know, I think that we humans feel very much in control of the machines in our lives
[01:32:53.520 --> 01:32:54.600]   and we're not.
[01:32:54.600 --> 01:32:58.200]   The machines are very much dictating our everyday actions.
[01:32:58.200 --> 01:33:03.240]   This is why by the way the United problem happened on the United Airlines flight because
[01:33:03.240 --> 01:33:09.160]   at every point along the way, an algorithm was making a decision and people were reading
[01:33:09.160 --> 01:33:12.520]   what was on their screens, the staff, they're just following the directions.
[01:33:12.520 --> 01:33:15.800]   Yeah, but that's called a fiasco.
[01:33:15.800 --> 01:33:17.400]   That's always happened, right?
[01:33:17.400 --> 01:33:24.040]   Every horrible thing that happens is a cascade of avoidable, small actions that add up to-
[01:33:24.040 --> 01:33:25.040]   And I get what I do.
[01:33:25.040 --> 01:33:26.040]   I agree with you.
[01:33:26.040 --> 01:33:30.480]   And I guess in this case, in this, you know, what I'm seeing more and more of is that sort
[01:33:30.480 --> 01:33:36.200]   of lack of we're just doing what the machines are telling us to do.
[01:33:36.200 --> 01:33:40.000]   So the email said to click and it never, you know, that the first thing I thought of
[01:33:40.000 --> 01:33:43.960]   was actually, because I got one, was this person wouldn't know how to send me a Google
[01:33:43.960 --> 01:33:45.960]   anything if you like to put it on it.
[01:33:45.960 --> 01:33:46.960]   That's not it.
[01:33:46.960 --> 01:33:49.280]   You are lucky that you got it from that person.
[01:33:49.280 --> 01:33:51.120]   I respect where you're coming from.
[01:33:51.120 --> 01:33:58.840]   But I think that if you have ever set in on like a user testing session, I think good
[01:33:58.840 --> 01:34:04.600]   technologists, I think we really put the user first.
[01:34:04.600 --> 01:34:09.800]   And I think all kinds of bad decisions get made in technology when we start blaming the
[01:34:09.800 --> 01:34:10.960]   user.
[01:34:10.960 --> 01:34:15.920]   And I think that, you know, for the hyper educated about these issues, I think it's really easy
[01:34:15.920 --> 01:34:20.800]   for us to lack empathy towards what normal users are feeling.
[01:34:20.800 --> 01:34:23.920]   And I think we've got to put them, you know, in the forefront here.
[01:34:23.920 --> 01:34:26.280]   So to me, I look at this issue.
[01:34:26.280 --> 01:34:28.600]   And you know, this was spread by email.
[01:34:28.600 --> 01:34:33.880]   I think it's almost time for us to, you know, update our email standards.
[01:34:33.880 --> 01:34:39.960]   And you know, like if Microsoft is sending me in the email, I'm sorry, a program update,
[01:34:39.960 --> 01:34:43.720]   you know, there are certificates inside of it to make sure that I'm running, you know,
[01:34:43.720 --> 01:34:45.120]   code from the right person.
[01:34:45.120 --> 01:34:50.880]   I almost want something to be built into these systems that can verify that these requests
[01:34:50.880 --> 01:34:54.800]   are coming from the real people behind it.
[01:34:54.800 --> 01:35:00.120]   Because, you know, it's just true, like we can blame users or call them dumb or belittle
[01:35:00.120 --> 01:35:01.120]   them.
[01:35:01.120 --> 01:35:06.560]   But like Facebook's chief security officer, he wrote a wonderful blog post a few months
[01:35:06.560 --> 01:35:11.840]   ago looking at how, you know, infosec people will look at edge cases and we love to talk
[01:35:11.840 --> 01:35:13.720]   about zero day exploits.
[01:35:13.720 --> 01:35:17.240]   The truth is the main thing that happens is people get fished.
[01:35:17.240 --> 01:35:21.840]   And that's just a fact that is most of these kinds of hacking vulnerabilities.
[01:35:21.840 --> 01:35:24.640]   So I totally agree with you.
[01:35:24.640 --> 01:35:28.920]   I think we've got to have compassion for them and figure out how to, you know, give them
[01:35:28.920 --> 01:35:31.760]   better indications of what they're being given.
[01:35:31.760 --> 01:35:32.920]   I completely agree with you.
[01:35:32.920 --> 01:35:35.800]   And I'm certainly not calling anybody stupid or anything at all like that.
[01:35:35.800 --> 01:35:41.920]   My point is my concern is that we are, you know, and I'm, we all do this, right?
[01:35:41.920 --> 01:35:50.360]   In the desire that we have for automation to do more without directly, you know, while
[01:35:50.360 --> 01:35:56.200]   directly doing less, as we outsource more and more of our daily tasks, what we have started
[01:35:56.200 --> 01:36:02.560]   to do is to, you know, a lot of that automation happens without a direct connection and we're
[01:36:02.560 --> 01:36:03.560]   in.
[01:36:03.560 --> 01:36:04.560]   And we're not thinking about it.
[01:36:04.560 --> 01:36:08.520]   And so what I would, what I want people to do is to insert themselves back into the
[01:36:08.520 --> 01:36:11.600]   process a little bit more and to, you know, just, you know, just, well, just here's, by
[01:36:11.600 --> 01:36:12.600]   the way, thanks to a Redditor.
[01:36:12.600 --> 01:36:13.600]   I want to go ahead, Nick.
[01:36:13.600 --> 01:36:19.880]   Leo, so one of my biggest frustrations is people that are like, damn you auto correct.
[01:36:19.880 --> 01:36:27.400]   If you pause before you hit send, you would see that it wrote the thing incorrectly.
[01:36:27.400 --> 01:36:32.880]   We're in such a rush all the time these days because of technology that even when we screw
[01:36:32.880 --> 01:36:37.440]   up, which is us screwing up because we haven't stopped to read it until after we've sent
[01:36:37.440 --> 01:36:39.520]   it, we tend to blame the technology.
[01:36:39.520 --> 01:36:44.120]   And I think it's, you know, it's a perfect example of the fact that everything is everyone
[01:36:44.120 --> 01:36:46.280]   else's fault except for us humans.
[01:36:46.280 --> 01:36:49.600]   But really at the end of the day, it's almost always us humans fault.
[01:36:49.600 --> 01:36:50.600]   We stopped and pause.
[01:36:50.600 --> 01:36:53.480]   And like you said, Amy, and thought, well, that person wouldn't send me this.
[01:36:53.480 --> 01:36:54.880]   Or it's about, I'm sorry.
[01:36:54.880 --> 01:36:55.880]   That's exactly.
[01:36:55.880 --> 01:36:58.160]   But look at this is the, so, so you got this.
[01:36:58.160 --> 01:37:00.240]   So this is from the Redditor who first exposed us.
[01:37:00.240 --> 01:37:02.200]   This is the permissions.
[01:37:02.200 --> 01:37:04.800]   And by the way, all of this is legitimate Google.
[01:37:04.800 --> 01:37:07.600]   You know, the authentication was real was OAuth two.
[01:37:07.600 --> 01:37:10.000]   The second factor, if you had it was real.
[01:37:10.000 --> 01:37:13.240]   This was the pop up that Google gives you Google Docs would like.
[01:37:13.240 --> 01:37:18.840]   And the only, by the way, the only spurious thing in all of this is that Google Docs isn't
[01:37:18.840 --> 01:37:19.840]   an application.
[01:37:19.840 --> 01:37:22.440]   Really, it doesn't need permission.
[01:37:22.440 --> 01:37:24.600]   It's a, you know, it's Google Docs.
[01:37:24.600 --> 01:37:28.720]   But if you saw Google Docs would like you to would like to read, send, delete and manage
[01:37:28.720 --> 01:37:32.400]   your email and manage your contacts.
[01:37:32.400 --> 01:37:37.200]   That I mean, if you're sophisticated, you might have looked at that and said, well,
[01:37:37.200 --> 01:37:38.400]   but it's Google.
[01:37:38.400 --> 01:37:42.280]   Why wouldn't you know, I give that so you'd hit the allow button.
[01:37:42.280 --> 01:37:44.360]   And that's by the way, you'd done at that point.
[01:37:44.360 --> 01:37:45.360]   This is when it's.
[01:37:45.360 --> 01:37:51.400]   And I would also say to Brianna's point, which was a good one, you know, a lot of times when
[01:37:51.400 --> 01:37:55.880]   you're when you're linking, when you're using Facebook's off to log into something, you're
[01:37:55.880 --> 01:37:58.240]   also being asked to give away the farm, right?
[01:37:58.240 --> 01:38:01.000]   You're sort of asking to click through to everything.
[01:38:01.000 --> 01:38:02.200]   And the more that.
[01:38:02.200 --> 01:38:03.840]   So again, it's become automatic.
[01:38:03.840 --> 01:38:06.800]   So we're not nobody reads the TOS, right?
[01:38:06.800 --> 01:38:08.920]   Nobody like none of us are doing it.
[01:38:08.920 --> 01:38:15.240]   As we now know from unroll me, nobody was paying attention.
[01:38:15.240 --> 01:38:16.240]   Yeah.
[01:38:16.240 --> 01:38:18.440]   Well, but so I have a question for Brianna.
[01:38:18.440 --> 01:38:19.440]   Can I ask?
[01:38:19.440 --> 01:38:20.440]   Yes.
[01:38:20.440 --> 01:38:22.560]   So how would you is there a how would you solve this?
[01:38:22.560 --> 01:38:23.560]   Right.
[01:38:23.560 --> 01:38:31.280]   So so how do you solve a problem where people are increasingly used to want automation?
[01:38:31.280 --> 01:38:34.640]   You can't I don't think you can solve this through really great UX, right?
[01:38:34.640 --> 01:38:36.600]   So what do you what do you what do you do?
[01:38:36.600 --> 01:38:37.600]   What do you think?
[01:38:37.600 --> 01:38:40.040]   Well, I think we should look at email standards.
[01:38:40.040 --> 01:38:41.040]   I really do.
[01:38:41.040 --> 01:38:48.000]   I had a really long discussion with people on my Twitter this week about SMTP servers,
[01:38:48.000 --> 01:38:52.760]   you know, and you can, you know, theoretically get two factor authentication, you know, through
[01:38:52.760 --> 01:38:57.680]   TLS and other technologies, but most, you know, organizations are not going to pay for
[01:38:57.680 --> 01:39:00.720]   that ends up being like single password.
[01:39:00.720 --> 01:39:08.100]   I would like us to look very critically at email and see if there's some way we can
[01:39:08.100 --> 01:39:13.920]   bake encryption into email, you know, like end to end user encryption, because typically
[01:39:13.920 --> 01:39:18.600]   it's server side encrypted, but it's not encrypted on like your hard drive.
[01:39:18.600 --> 01:39:24.560]   So I think looking at encryption with email, I think looking at ways to identify who, you
[01:39:24.560 --> 01:39:26.400]   know, email is coming from.
[01:39:26.400 --> 01:39:29.360]   I think Google has a lot of responsibility here.
[01:39:29.360 --> 01:39:34.520]   Like somebody found out a way to cheat that and make it sound like, you know, Google Docs
[01:39:34.520 --> 01:39:38.760]   right there, like that should be impossible if they're using, you know, Google's OAuth
[01:39:38.760 --> 01:39:39.760]   system.
[01:39:39.760 --> 01:39:43.400]   So I think that, you know, we have a term in engineering, multi factorial.
[01:39:43.400 --> 01:39:48.160]   They're multi factorial solutions here, but clearly it's not working as is.
[01:39:48.160 --> 01:39:49.160]   Yeah.
[01:39:49.160 --> 01:39:53.800]   It's it is a challenge because you don't want to blame users.
[01:39:53.800 --> 01:39:58.360]   They're not stupid, but at the same time, everything's designed.
[01:39:58.360 --> 01:39:59.360]   I don't know.
[01:39:59.360 --> 01:40:05.080]   I feel like again, like I feel like if you're going to insert yourself and start, you know,
[01:40:05.080 --> 01:40:10.400]   I don't, I don't, you don't need to know exactly how server size, you know, server side authentication
[01:40:10.400 --> 01:40:11.400]   works.
[01:40:11.400 --> 01:40:17.600]   But I think that we are setting ourselves up several years from now for serious problems.
[01:40:17.600 --> 01:40:18.600]   We are.
[01:40:18.600 --> 01:40:19.600]   If we don't, you don't.
[01:40:19.600 --> 01:40:24.200]   You don't require a amount of digital speech marks.
[01:40:24.200 --> 01:40:29.200]   It's, yeah, exactly.
[01:40:29.200 --> 01:40:32.200]   I think that it's okay.
[01:40:32.200 --> 01:40:34.800]   So what's a good analogy, a toilet, right?
[01:40:34.800 --> 01:40:36.720]   I don't, I don't know what happens.
[01:40:36.720 --> 01:40:40.600]   How the sewage system works and what happens to the things that go down there, but I know
[01:40:40.600 --> 01:40:43.600]   not to put a brick down the toilet and try to flush it.
[01:40:43.600 --> 01:40:46.600]   You know, I know it would be your problem, your fault.
[01:40:46.600 --> 01:40:47.600]   There's a better analogy.
[01:40:47.600 --> 01:40:48.600]   You don't.
[01:40:48.600 --> 01:40:49.600]   Right.
[01:40:49.600 --> 01:40:52.600]   So Nick, or whoever, like the panel, you don't understand exactly how your, maybe you do your
[01:40:52.600 --> 01:40:53.600]   car works, like the inside pieces of your car, but you know that you're going to get in a
[01:40:53.600 --> 01:40:54.600]   car.
[01:40:54.600 --> 01:40:55.600]   That's what I'm talking about.
[01:40:55.600 --> 01:41:00.440]   What if though, what if the real flaw here is that you're driving the car in the first
[01:41:00.440 --> 01:41:01.440]   place?
[01:41:01.440 --> 01:41:08.120]   What if the very, the very thing that Google Docs does is inherently highly risky and
[01:41:08.120 --> 01:41:15.720]   really it, there's no UI that would solve this problem because you are what you're doing
[01:41:15.720 --> 01:41:19.320]   is inherently dangerous as your documents are out there.
[01:41:19.320 --> 01:41:20.880]   You're giving permission.
[01:41:20.880 --> 01:41:23.400]   You know, Kerry Doctorow was on on Monday on triangulation.
[01:41:23.400 --> 01:41:27.400]   He says, I don't store my email in the cloud.
[01:41:27.400 --> 01:41:33.280]   I use pop mail and download it because I don't want, but I, all my mail is on Gmail at 11
[01:41:33.280 --> 01:41:34.720]   years worth the email.
[01:41:34.720 --> 01:41:38.920]   And if unroll me or slice or the NSA wanted to know everything about me, it would all
[01:41:38.920 --> 01:41:39.920]   be there.
[01:41:39.920 --> 01:41:40.920]   It's not encrypted.
[01:41:40.920 --> 01:41:41.680]   It's available.
[01:41:41.680 --> 01:41:47.080]   And so what if the very things we're doing are the danger and that there's no way to design
[01:41:47.080 --> 01:41:48.400]   yourself out of that.
[01:41:48.400 --> 01:41:53.480]   You are driving a car that has no body that you don't put on a seatbelt.
[01:41:53.480 --> 01:41:57.440]   Fine, but it's dangerous inherently.
[01:41:57.440 --> 01:41:59.120]   Isn't that the case?
[01:41:59.120 --> 01:42:00.120]   Yes.
[01:42:00.120 --> 01:42:01.120]   Yeah.
[01:42:01.120 --> 01:42:02.120]   It definitely is.
[01:42:02.120 --> 01:42:04.920]   We're wearing, we're putting tape over a camera.
[01:42:04.920 --> 01:42:09.800]   We are carrying a microphone, a camera, a GPS that's always on the internet in our pocket.
[01:42:09.800 --> 01:42:11.600]   And we're worried about the Amazon Echo.
[01:42:11.600 --> 01:42:16.680]   We are part, we are actively participating right now in many, many technologies that
[01:42:16.680 --> 01:42:18.800]   are eminently hackable and very dangerous.
[01:42:18.800 --> 01:42:22.080]   And it's silly for us to say, well, I'm not going to use an Amazon Echo because that's
[01:42:22.080 --> 01:42:25.480]   an attack vector when there's 3,000 other attack vectors.
[01:42:25.480 --> 01:42:26.480]   It's just the nature of modern technology.
[01:42:26.480 --> 01:42:28.000]   Now, tell you why we do it.
[01:42:28.000 --> 01:42:32.400]   I tell you why we do it because of the convenience and the value we get out of it.
[01:42:32.400 --> 01:42:36.800]   And we're, whether knowingly or not, and I would say mostly knowingly, believe it or
[01:42:36.800 --> 01:42:39.320]   not, you talk to teenagers, they kind of know what's going on.
[01:42:39.320 --> 01:42:41.200]   We are willing to give up this privacy.
[01:42:41.200 --> 01:42:44.880]   We are willing to expose ourselves because of the value for the same reason that people
[01:42:44.880 --> 01:42:49.280]   live in cities which are inherently more dangerous than the country because of the value of
[01:42:49.280 --> 01:42:50.760]   living in the city.
[01:42:50.760 --> 01:42:51.760]   That's right.
[01:42:51.760 --> 01:42:55.120]   And so if you spin this out, the earlier point that I was making about sort of, I mean,
[01:42:55.120 --> 01:42:56.840]   it sounds like a fatalistic scenario.
[01:42:56.840 --> 01:43:02.400]   But if you spin this out far enough, you know, do we no longer care?
[01:43:02.400 --> 01:43:06.040]   Is the way that we deal with our privacy to completely give up our privacy?
[01:43:06.040 --> 01:43:08.000]   It sounds like a black mirror episode.
[01:43:08.000 --> 01:43:13.400]   But if you model this out and forces continue the way that they're going, you know, is
[01:43:13.400 --> 01:43:16.320]   that the ultimate scenario?
[01:43:16.320 --> 01:43:18.320]   It's plausible.
[01:43:18.320 --> 01:43:23.320]   Um, Leo, everyone, guests, esteemed colleagues.
[01:43:23.320 --> 01:43:25.480]   Don't say you got to leave now.
[01:43:25.480 --> 01:43:27.320]   I have to leave in five minutes.
[01:43:27.320 --> 01:43:29.680]   I know I'm watching the clock.
[01:43:29.680 --> 01:43:30.760]   Sorry, we got one more thing.
[01:43:30.760 --> 01:43:31.760]   I want to ask you.
[01:43:31.760 --> 01:43:32.760]   President Zuckerberg.
[01:43:32.760 --> 01:43:33.760]   Sorry.
[01:43:33.760 --> 01:43:35.320]   We're going to do one more.
[01:43:35.320 --> 01:43:37.160]   And then we're going to let you go.
[01:43:37.160 --> 01:43:38.160]   Don't leave yet.
[01:43:38.160 --> 01:43:39.160]   Don't leave yet.
[01:43:39.160 --> 01:43:40.720]   Where are you going to the Guardians of the Galaxy?
[01:43:40.720 --> 01:43:41.720]   What you got?
[01:43:41.720 --> 01:43:42.720]   Is it five?
[01:43:42.720 --> 01:43:44.920]   No, it's my, honestly, it's my niece's birthday party.
[01:43:44.920 --> 01:43:45.920]   She's 10th.
[01:43:45.920 --> 01:43:46.920]   You're good on.
[01:43:46.920 --> 01:43:49.840]   So I have to, you know, you don't want, I don't want to disappoint your niece.
[01:43:49.840 --> 01:43:50.840]   Can we do it?
[01:43:50.840 --> 01:43:55.440]   Can I give a shout out to the, to the, to the book website before we, before, before I
[01:43:55.440 --> 01:43:58.280]   please American Kingpin.com?
[01:43:58.280 --> 01:43:59.760]   Well, that was hard.
[01:43:59.760 --> 01:44:00.760]   There you go.
[01:44:00.760 --> 01:44:05.200]   You left pictures, links to the audible, the Amazon, the iBooks, the you name it.
[01:44:05.200 --> 01:44:11.560]   And there's pictures of the drug deals and the murders and, you know, so.
[01:44:11.560 --> 01:44:16.120]   Stephen Levy writes in American Kingpin, Nick built and again proves he's one of technology's
[01:44:16.120 --> 01:44:21.560]   best storytellers with a stunningly researched and very scary portrait of the creator of
[01:44:21.560 --> 01:44:27.400]   a marketplace gone mad and the oddly uncoordinated officers who took him down.
[01:44:27.400 --> 01:44:28.400]   It's true.
[01:44:28.400 --> 01:44:32.840]   It tells the, it tells the bad side from the good side and the good side from the bad side.
[01:44:32.840 --> 01:44:36.000]   This is the one where they arrest him at the San Francisco Public Library.
[01:44:36.000 --> 01:44:37.480]   Oh my God.
[01:44:37.480 --> 01:44:38.880]   Leo, you just gave away the end.
[01:44:38.880 --> 01:44:39.880]   Oh crap.
[01:44:39.880 --> 01:44:43.040]   But that's not the, it's, it's a, it's a crazy.
[01:44:43.040 --> 01:44:44.040]   Yeah.
[01:44:44.040 --> 01:44:51.040]   That's just the beginning in a world where anything's available online.
[01:44:51.040 --> 01:44:57.000]   One man decided the Silk Road led to heaven.
[01:44:57.000 --> 01:44:59.400]   American Kingpin in stores now.
[01:44:59.400 --> 01:45:07.400]   How about that, Nick built in this press would pay money for movie phone.
[01:45:07.400 --> 01:45:09.400]   Press two for Amy.
[01:45:09.400 --> 01:45:12.400]   Web the signals are talking.
[01:45:12.400 --> 01:45:13.400]   That's Brianna.
[01:45:13.400 --> 01:45:14.400]   Whoa.
[01:45:14.400 --> 01:45:15.400]   She's running for Congress.
[01:45:15.400 --> 01:45:16.400]   All right.
[01:45:16.400 --> 01:45:17.600]   We're going to take a break.
[01:45:17.600 --> 01:45:18.760]   And then one more article.
[01:45:18.760 --> 01:45:24.080]   If you want to go Nick, just take on off, but I, I'm going to, I'm going to read the article.
[01:45:24.080 --> 01:45:26.360]   Don't let Facebook make you miserable.
[01:45:26.360 --> 01:45:27.760]   So we want to end on an up note.
[01:45:27.760 --> 01:45:29.680]   This is in the New York Times today.
[01:45:29.680 --> 01:45:31.320]   But I first start word from hover.com.
[01:45:31.320 --> 01:45:35.120]   If you've got a domain name, this is the funnest thing everybody I know does this.
[01:45:35.120 --> 01:45:44.800]   You're at a party, you're at dinner and somebody says fancy pants.com and you get an idea, right?
[01:45:44.800 --> 01:45:45.800]   Register the domain.
[01:45:45.800 --> 01:45:46.800]   You got to lock it in.
[01:45:46.800 --> 01:45:49.280]   Hey, are you pregnant and you haven't a baby?
[01:45:49.280 --> 01:45:52.120]   What's the first thing you do before you decide on the name of the baby?
[01:45:52.120 --> 01:45:54.120]   Nowadays you got to Google that name.
[01:45:54.120 --> 01:45:55.960]   You got to check and you get the domain name.
[01:45:55.960 --> 01:45:57.400]   I own fancy pants.
[01:45:57.400 --> 01:45:58.400]   Don't get it.
[01:45:58.400 --> 01:45:59.960]   It's mine buddy boy.
[01:45:59.960 --> 01:46:00.960]   I have it.
[01:46:00.960 --> 01:46:06.480]   I also have my kids names, Abby LaPorte and Henry LaPorte registered at hover.com.
[01:46:06.480 --> 01:46:10.440]   In fact, I registered for 20 years because I figured if they don't want it by the time
[01:46:10.440 --> 01:46:14.000]   they're 20, screw them.
[01:46:14.000 --> 01:46:18.240]   This is important domain name registration and there's one place to go.
[01:46:18.240 --> 01:46:22.160]   Don't even think about going any other place, hover.com.
[01:46:22.160 --> 01:46:30.920]   They have 400 domain name extensions including dot pizza dot ninja dot horse.
[01:46:30.920 --> 01:46:35.000]   This is the best place to go to register your domain names.
[01:46:35.000 --> 01:46:39.280]   Of course dot com dot net and even kind of fun stuff like dot tech.
[01:46:39.280 --> 01:46:42.040]   Brianna, you could get dot democrat.
[01:46:42.040 --> 01:46:43.960]   I could get dot club.
[01:46:43.960 --> 01:46:45.640]   They've got dot church.
[01:46:45.640 --> 01:46:47.280]   I mean, they've got everything.
[01:46:47.280 --> 01:46:51.640]   I think Brianna Wu dot democrat, Lao LaPorte the Republican.
[01:46:51.640 --> 01:46:52.640]   It'd be great.
[01:46:52.640 --> 01:46:53.640]   It'd be awesome.
[01:46:53.640 --> 01:46:54.640]   Love it.
[01:46:54.640 --> 01:46:55.640]   Yeah.
[01:46:55.640 --> 01:46:56.640]   Let's make it happen.
[01:46:56.640 --> 01:46:57.640]   You can't hover dot com.
[01:46:57.640 --> 01:46:59.520]   By the way, they know you want domain privacy.
[01:46:59.520 --> 01:47:00.520]   That's built in.
[01:47:00.520 --> 01:47:04.120]   This is not one of those places where you buy the domain name and then they go, "Oh, 50
[01:47:04.120 --> 01:47:05.560]   pages of what would you like this?
[01:47:05.560 --> 01:47:06.560]   Would you like to buy that?
[01:47:06.560 --> 01:47:07.560]   What do you want to buy this?"
[01:47:07.560 --> 01:47:08.560]   They just, it's simple.
[01:47:08.560 --> 01:47:09.560]   They're Canadians.
[01:47:09.560 --> 01:47:10.560]   It's nice.
[01:47:10.560 --> 01:47:12.320]   Best customer support in the business.
[01:47:12.320 --> 01:47:14.120]   Their support team will help you.
[01:47:14.120 --> 01:47:20.040]   In fact, if I moved all my domains because I was on every other, I had every other five
[01:47:20.040 --> 01:47:21.040]   or six different places.
[01:47:21.040 --> 01:47:22.040]   I got them all moved.
[01:47:22.040 --> 01:47:24.560]   Their concierge service just moved them all to hover.
[01:47:24.560 --> 01:47:28.000]   It's easy to manage their DNS management.
[01:47:28.000 --> 01:47:29.000]   So simple.
[01:47:29.000 --> 01:47:36.680]   So if you're on Google+ or Tumblr or WordPress or LinkedIn, they have a very easy way to move.
[01:47:36.680 --> 01:47:41.640]   To put that domain name on your service, I just registered leo.social for my microblog
[01:47:41.640 --> 01:47:42.640]   and it was very easy.
[01:47:42.640 --> 01:47:44.280]   WordPress, leo.report.blog.
[01:47:44.280 --> 01:47:45.880]   I set it up very easily.
[01:47:45.880 --> 01:47:48.600]   Hover dot com slash twit.
[01:47:48.600 --> 01:47:54.360]   And by the way, buy all the domain names because you'll get 10% off your first purchase.
[01:47:54.360 --> 01:47:58.400]   Hover dot com slash twit.
[01:47:58.400 --> 01:47:59.400]   I have hundreds.
[01:47:59.400 --> 01:48:01.200]   No, it's just says 59.
[01:48:01.200 --> 01:48:02.800]   That seems like hundreds.
[01:48:02.800 --> 01:48:04.320]   There's tunictime.com.
[01:48:04.320 --> 01:48:05.320]   Anybody wants to sell tunics?
[01:48:05.320 --> 01:48:06.880]   I got the name for you.
[01:48:06.880 --> 01:48:09.200]   Twit.expert, twit.watch, twit.com.
[01:48:09.200 --> 01:48:10.880]   So this was a good article I thought.
[01:48:10.880 --> 01:48:12.120]   What can wrap up with this?
[01:48:12.120 --> 01:48:14.800]   I know everybody wants to go to dinner.
[01:48:14.800 --> 01:48:20.000]   Seth Stevens-Dividovis is a economist.
[01:48:20.000 --> 01:48:23.600]   But he, I think, catalyzed something that's been in my head.
[01:48:23.600 --> 01:48:26.440]   Don't let Facebook make you miserable.
[01:48:26.440 --> 01:48:32.160]   Scholars have analyzed the data and confirmed what we knew already that social media is
[01:48:32.160 --> 01:48:41.200]   making us miserable because everybody puts their best stuff on social media.
[01:48:41.200 --> 01:48:45.440]   For instance, he says as an example, Americans spend about six times as much of their time
[01:48:45.440 --> 01:48:48.040]   cleaning dishes as they do golfing.
[01:48:48.040 --> 01:48:53.200]   But there are twice as many tweets reporting golfing as dish doing.
[01:48:53.200 --> 01:48:58.080]   The Las Vegas budget hotel circus circus and the Bellagio, same number of people checked
[01:48:58.080 --> 01:48:59.080]   in.
[01:48:59.080 --> 01:49:02.640]   Bellagio gets three times as many check-ins on Facebook because nobody wants to admit
[01:49:02.640 --> 01:49:06.640]   they're staying at circus, circus owners of BMWs and Mercedes.
[01:49:06.640 --> 01:49:13.360]   Two and a half times as likely to like their car company on Facebook as Honda owners and
[01:49:13.360 --> 01:49:14.680]   on and on and on.
[01:49:14.680 --> 01:49:20.160]   He quotes a good old saying from AA.
[01:49:20.160 --> 01:49:24.840]   But compare your insides to other people's outsides.
[01:49:24.840 --> 01:49:30.840]   He points out that one of the problems with social media is always putting their best foot
[01:49:30.840 --> 01:49:31.840]   forward.
[01:49:31.840 --> 01:49:33.680]   Everybody else's life looks great.
[01:49:33.680 --> 01:49:35.640]   Your life feels terrible.
[01:49:35.640 --> 01:49:37.000]   What's his fix?
[01:49:37.000 --> 01:49:39.080]   He says, "Use Google autocomplete."
[01:49:39.080 --> 01:49:50.120]   He says, "Any time you feel bad, just type in 'I always' and then you'll see the autocomplete
[01:49:50.120 --> 01:49:58.080]   have to pee, feel tired, just type in 'I always feel tired, bloated, hungry, sick.'
[01:49:58.080 --> 01:50:02.720]   This is what really is going on in the world because Google autocomplete is based on Google's
[01:50:02.720 --> 01:50:03.800]   searches.
[01:50:03.800 --> 01:50:05.960]   People are performing."
[01:50:05.960 --> 01:50:06.960]   So there.
[01:50:06.960 --> 01:50:09.880]   Don't get to it.
[01:50:09.880 --> 01:50:15.720]   We looked at this a while back at my game studio because we were asking, "We wanted to
[01:50:15.720 --> 01:50:19.440]   know what makes Twitter so wildly addictive."
[01:50:19.440 --> 01:50:23.440]   We started looking into the neuroscience behind it.
[01:50:23.440 --> 01:50:29.520]   You get into this feedback loop where every time, if I tweet something and I start getting
[01:50:29.520 --> 01:50:32.160]   likes or retweets, it's a dopamine hit.
[01:50:32.160 --> 01:50:37.320]   It's the exact same reward treadmill like in a game like Destiny where you might get
[01:50:37.320 --> 01:50:39.040]   more and more powerful weapons.
[01:50:39.040 --> 01:50:44.040]   You keep being compelled to keep playing so you get that next upgrade.
[01:50:44.040 --> 01:50:46.680]   But the inverse of this is also true.
[01:50:46.680 --> 01:50:51.320]   I think anyone that's a public figure that has said something and had it go a little
[01:50:51.320 --> 01:50:56.200]   bit of control on Twitter can attest to this.
[01:50:56.200 --> 01:51:02.240]   When you get that negative feedback, it has the exact opposite emotion on you.
[01:51:02.240 --> 01:51:04.440]   I was at a campaign event this weekend.
[01:51:04.440 --> 01:51:05.440]   I'm on the beach.
[01:51:05.440 --> 01:51:08.720]   I'm with my husband and it's gorgeous outside.
[01:51:08.720 --> 01:51:11.160]   I'm getting a bunch of static on Twitter.
[01:51:11.160 --> 01:51:14.560]   It's really hard to not let that affect your mood.
[01:51:14.560 --> 01:51:15.960]   The only solution.
[01:51:15.960 --> 01:51:19.760]   I have for that is to step away.
[01:51:19.760 --> 01:51:21.160]   Just step away.
[01:51:21.160 --> 01:51:24.480]   You can't win sometimes.
[01:51:24.480 --> 01:51:29.840]   Your life is with your spouse or your children or your friends.
[01:51:29.840 --> 01:51:31.760]   It's ultimately not real.
[01:51:31.760 --> 01:51:36.840]   We've got to all get better about putting those breaks in place.
[01:51:36.840 --> 01:51:44.040]   I think the thing to bear in mind is that in the digital realm, we present a far more
[01:51:44.040 --> 01:51:49.200]   aspirational version of ourselves than we are in the real world.
[01:51:49.200 --> 01:51:53.600]   We're far willing to be more vitriolic and horrible.
[01:51:53.600 --> 01:52:02.760]   The digital realm is this area of outliers, personality outliers and behavioral outliers.
[01:52:02.760 --> 01:52:09.160]   I think we forget about a lot of this.
[01:52:09.160 --> 01:52:12.840]   The easiest thing to do is also the hardest thing to do and that's unplugged.
[01:52:12.840 --> 01:52:17.480]   This is what I was referring to earlier when I said that Silicon Valley has weaponized
[01:52:17.480 --> 01:52:19.800]   capitalism.
[01:52:19.800 --> 01:52:26.560]   There are no more rapacious or greedy than capitalists in years gone by, but you look
[01:52:26.560 --> 01:52:30.560]   at world of warcrafts, a really good example.
[01:52:30.560 --> 01:52:34.480]   I actually would love to know what you think about this, Brianna, and I'm sure you dealt
[01:52:34.480 --> 01:52:36.040]   with this.
[01:52:36.040 --> 01:52:41.360]   They're getting feedback all the time, what keeps people playing longer.
[01:52:41.360 --> 01:52:46.360]   This data is so valuable because they can essentially design a more and more addictive
[01:52:46.360 --> 01:52:51.120]   game and they consistently get these signals.
[01:52:51.120 --> 01:52:58.320]   Every iPad game nowadays, these games that you buy donuts or coins or gems, they are,
[01:52:58.320 --> 01:53:02.920]   as everybody's addictive as the worst thing Vegas ever thought of, probably 10 times worse.
[01:53:02.920 --> 01:53:06.560]   Of course it is because they've got that much more data on you.
[01:53:06.560 --> 01:53:09.280]   I have a friend of mine that works for a mobile game company.
[01:53:09.280 --> 01:53:13.600]   I'm not going to say which one, but I was down at their studio and he started opening
[01:53:13.600 --> 01:53:19.680]   up all the information that they have about certain people and they have lists of users
[01:53:19.680 --> 01:53:26.640]   and what the probability is and they will tweak variables to get them to use that next
[01:53:26.640 --> 01:53:28.360]   cash purchase.
[01:53:28.360 --> 01:53:35.000]   It's like a Vegas statistician's dream to design something that keeps you going back.
[01:53:35.000 --> 01:53:38.600]   I do have to say that knowledge can be used for good.
[01:53:38.600 --> 01:53:41.200]   You've seen this with DOTA.
[01:53:41.200 --> 01:53:46.080]   One of the really interesting things, DOTA is a genre called a team.
[01:53:46.080 --> 01:53:47.080]   It's basically a team.
[01:53:47.080 --> 01:53:48.080]   It's a MOBA.
[01:53:48.080 --> 01:53:49.920]   Yeah, it's a MOBA.
[01:53:49.920 --> 01:53:52.640]   It's a bunch of people fighting together.
[01:53:52.640 --> 01:53:57.120]   What I love about this is they figured out if there's harassment on their platform, it
[01:53:57.120 --> 01:54:01.240]   drives players away, which is bad for their bottom line.
[01:54:01.240 --> 01:54:06.000]   They use that data to figure out, "What are the consequences?"
[01:54:06.000 --> 01:54:10.680]   We can introduce into the equation to make people think about their actions a little
[01:54:10.680 --> 01:54:11.680]   bit more.
[01:54:11.680 --> 01:54:14.800]   It could be used for good and evil, I think.
[01:54:14.800 --> 01:54:18.360]   Somebody's saying, though, that DOTA is a troll fest.
[01:54:18.360 --> 01:54:21.000]   Maybe they're not doing as well as they.
[01:54:21.000 --> 01:54:22.720]   It's easy enough to think about it.
[01:54:22.720 --> 01:54:24.920]   It's hard to make it work.
[01:54:24.920 --> 01:54:25.980]   If your voice isn't after throwing it out.
[01:54:25.980 --> 01:54:26.980]   Why does it not?
[01:54:26.980 --> 01:54:27.980]   Why does Twitter solve this?
[01:54:27.980 --> 01:54:30.040]   Everybody understands the problem and it must turn.
[01:54:30.040 --> 01:54:31.920]   Because they're incentivized.
[01:54:31.920 --> 01:54:35.160]   There's a financial disincentive to solve the problem.
[01:54:35.160 --> 01:54:39.000]   We too could solve that problem as users by just not using it.
[01:54:39.000 --> 01:54:44.800]   If you categorize all the digital things in your life that you rely on and you had to
[01:54:44.800 --> 01:54:49.520]   get rid of one by one in a priority order, I use Facebook.
[01:54:49.520 --> 01:54:52.280]   Would I not be able to run my business without Facebook?
[01:54:52.280 --> 01:54:54.440]   Would I not be able to feed my family without Facebook?
[01:54:54.440 --> 01:54:57.360]   I could get rid of Facebook like that and have no problem.
[01:54:57.360 --> 01:54:58.880]   I could do the same thing with Twitter.
[01:54:58.880 --> 01:55:03.320]   I was one of the original set of users of Twitter and I've enjoyed the service.
[01:55:03.320 --> 01:55:06.000]   I enjoyed the service more at the beginning than I do now.
[01:55:06.000 --> 01:55:09.920]   I could get rid of both of those easily without even blinking.
[01:55:09.920 --> 01:55:12.920]   Well early on the reward cycle was getting followers.
[01:55:12.920 --> 01:55:17.160]   I remember when Kevin Rose and I were in a battle to get to be number one on Twitter
[01:55:17.160 --> 01:55:22.320]   with 5,000 followers.
[01:55:22.320 --> 01:55:24.840]   Those days are long gone.
[01:55:24.840 --> 01:55:30.240]   I remember having because I was at one of the schools that was part of the original
[01:55:30.240 --> 01:55:32.240]   Facebook cohort.
[01:55:32.240 --> 01:55:35.040]   I went to Columbia.
[01:55:35.040 --> 01:55:40.080]   I just remember at some point using it and not really caring about it but then seeing
[01:55:40.080 --> 01:55:46.200]   a whole bunch of people who were decades older than me getting on to the platform and just
[01:55:46.200 --> 01:55:53.280]   like whatever, platforms they launched, they grow, they change.
[01:55:53.280 --> 01:55:56.520]   Ultimately we wind up at this point every time.
[01:55:56.520 --> 01:55:59.320]   We wind up at a point where people are...
[01:55:59.320 --> 01:56:03.520]   I do feel like people aren't any worse than they've always been.
[01:56:03.520 --> 01:56:04.520]   They've always been the same.
[01:56:04.520 --> 01:56:05.520]   I mean people are people.
[01:56:05.520 --> 01:56:10.960]   But what's happened is that technology has in this very strange way provided this megaphone,
[01:56:10.960 --> 01:56:16.240]   this amplifier, the tools, the big data tools, the analytics that we have now have given
[01:56:16.240 --> 01:56:18.880]   people literally...
[01:56:18.880 --> 01:56:25.160]   Well yeah, I could say literally weaponized people's worst impulses.
[01:56:25.160 --> 01:56:31.680]   I would say that we...so I cannot tell you what we did but we advised folks in the government
[01:56:31.680 --> 01:56:34.240]   have been advising for several years.
[01:56:34.240 --> 01:56:38.400]   Yeah, I would think that a lot of people would categorize them as weapons.
[01:56:38.400 --> 01:56:39.400]   Yeah, absolutely.
[01:56:39.400 --> 01:56:40.640]   Yeah, absolutely.
[01:56:40.640 --> 01:56:42.560]   And guess what?
[01:56:42.560 --> 01:56:47.760]   Facebook's next target TV.
[01:56:47.760 --> 01:56:51.160]   They have plans to push two dozen shows.
[01:56:51.160 --> 01:56:55.040]   They've greenlit multiple shows for production.
[01:56:55.040 --> 01:57:00.280]   They aim to premiere this mid-June next month.
[01:57:00.280 --> 01:57:01.280]   Facebook TV.
[01:57:01.280 --> 01:57:03.760]   And if I were the networks...
[01:57:03.760 --> 01:57:05.480]   I'd be a little bit worried.
[01:57:05.480 --> 01:57:06.480]   I'd be a little bit worried.
[01:57:06.480 --> 01:57:07.960]   Yep, VR dating.
[01:57:07.960 --> 01:57:10.800]   Now we've seen YouTube try this and fail.
[01:57:10.800 --> 01:57:14.600]   So it's always...and Facebook's failed with a great many things.
[01:57:14.600 --> 01:57:16.360]   But it's always possible.
[01:57:16.360 --> 01:57:21.320]   And I tell you, Facebook might fail nine times out of ten but all it takes is one out of
[01:57:21.320 --> 01:57:25.200]   ten with two billion users to become dominant.
[01:57:25.200 --> 01:57:30.680]   Yeah, I'd be really interested to ask you because I mean, you're the journalist here.
[01:57:30.680 --> 01:57:37.240]   It seems to me a lot of my friends that are journalists, you know, Vox or a major outlet,
[01:57:37.240 --> 01:57:40.520]   they do find Facebook live to be worth their time.
[01:57:40.520 --> 01:57:45.160]   So I mean, do you feel confident about this?
[01:57:45.160 --> 01:57:47.280]   Because I can see them expanding.
[01:57:47.280 --> 01:57:51.000]   I can see like Twit being a good fit for this.
[01:57:51.000 --> 01:57:52.000]   I can see like they tried to...
[01:57:52.000 --> 01:57:55.680]   Yeah, we've never tried to capitalize on it.
[01:57:55.680 --> 01:57:58.600]   Because I think the content is short-forming for the most part.
[01:57:58.600 --> 01:58:02.320]   Nobody wants to watch a two-hour show.
[01:58:02.320 --> 01:58:06.720]   For the same, you know, Twitter by the way is also launching a channel that will include
[01:58:06.720 --> 01:58:07.720]   stuff from Vox.
[01:58:07.720 --> 01:58:09.920]   Neil Ipatel is going to do a gadget show.
[01:58:09.920 --> 01:58:16.200]   That concerns me a little bit more only because Twit and Twitter sound a lot alike.
[01:58:16.200 --> 01:58:17.400]   We predate Twitter.
[01:58:17.400 --> 01:58:22.360]   But as Eve Williams once told me, he said, "Well, I knew about Twit, but I didn't think either
[01:58:22.360 --> 01:58:23.520]   of us were going anywhere.
[01:58:23.520 --> 01:58:26.480]   So it was okay to reuse the name."
[01:58:26.480 --> 01:58:27.480]   Yeah.
[01:58:27.480 --> 01:58:28.960]   I don't know.
[01:58:28.960 --> 01:58:32.680]   You know, Nike did this breaking two event this weekend and I'm a...
[01:58:32.680 --> 01:58:33.680]   I love to run.
[01:58:33.680 --> 01:58:34.680]   So I was watching it.
[01:58:34.680 --> 01:58:40.260]   I found myself up until 2 a.m. watching this live video on Twitter about these people
[01:58:40.260 --> 01:58:42.160]   trying to break a two-hour marathon.
[01:58:42.160 --> 01:58:44.000]   They failed by 24 seconds.
[01:58:44.000 --> 01:58:45.000]   Oh, oh.
[01:58:45.000 --> 01:58:46.000]   It was amazing.
[01:58:46.000 --> 01:58:48.200]   But it was really addictive.
[01:58:48.200 --> 01:58:56.200]   So I can see this being worthwhile for Facebook, but I also think that every single publisher
[01:58:56.200 --> 01:59:02.480]   has looked at the outcome for working with Facebook as a publisher with instant articles
[01:59:02.480 --> 01:59:04.480]   and the money just isn't there.
[01:59:04.480 --> 01:59:08.360]   So I would be very worried about getting in bed with Facebook for this.
[01:59:08.360 --> 01:59:09.360]   We've gone full circle.
[01:59:09.360 --> 01:59:12.200]   We're talking about that at the beginning of the show, Amy Webb.
[01:59:12.200 --> 01:59:15.000]   These publishers are pulling back now because they're not...
[01:59:15.000 --> 01:59:19.440]   Facebook's monetizing instant articles great, but none of that money's trickling down to
[01:59:19.440 --> 01:59:20.440]   the new space.
[01:59:20.440 --> 01:59:21.440]   Well, it was sort of a...
[01:59:21.440 --> 01:59:22.800]   Yeah, it was sort of a thought-ex.
[01:59:22.800 --> 01:59:23.800]   And I had to...
[01:59:23.800 --> 01:59:26.200]   I can't sit still for very long, so I had to stand up.
[01:59:26.200 --> 01:59:28.200]   Well, it was done.
[01:59:28.200 --> 01:59:30.000]   No, I sort of...
[01:59:30.000 --> 01:59:34.280]   Again, I was thinking through the future of news and money and Facebook.
[01:59:34.280 --> 01:59:37.720]   So I was modeling out these different scenarios given the data that I've got.
[01:59:37.720 --> 01:59:40.200]   And it occurred to me, what if every publisher...
[01:59:40.200 --> 01:59:41.960]   And this would only work if everybody did it.
[01:59:41.960 --> 01:59:45.120]   What if everybody yanked their content off of Facebook?
[01:59:45.120 --> 01:59:50.640]   If you look at the UI and the content that moves through it, a lot of that UI is dedicated
[01:59:50.640 --> 01:59:54.640]   to news stories and a lot of the content is news stories.
[01:59:54.640 --> 01:59:59.120]   And I think the only thing that would be left would be the obviously fake news stories.
[01:59:59.120 --> 02:00:06.120]   And at that point, without those other sources, if you just have cat photos and your photos
[02:00:06.120 --> 02:00:10.680]   that your aspirational photos that you're posting to make other people jealous and fake
[02:00:10.680 --> 02:00:13.880]   news, how interesting and compelling is the platform?
[02:00:13.880 --> 02:00:16.600]   And I think the answer is not super compelling.
[02:00:16.600 --> 02:00:17.600]   Oh, wait.
[02:00:17.600 --> 02:00:20.000]   There's two ways things appear on news appears on Facebook.
[02:00:20.000 --> 02:00:24.040]   One is, of course, the publications themselves putting instant articles there.
[02:00:24.040 --> 02:00:27.640]   The other is us sharing links to stories.
[02:00:27.640 --> 02:00:30.640]   And that doesn't matter what the New York Times can't stop that.
[02:00:30.640 --> 02:00:31.640]   And if they did, they did it.
[02:00:31.640 --> 02:00:32.640]   They did it at their peril.
[02:00:32.640 --> 02:00:35.680]   Look what financial times and other paywalls have done to those journals.
[02:00:35.680 --> 02:00:36.680]   No, no, no.
[02:00:36.680 --> 02:00:37.680]   I actually thought that through.
[02:00:37.680 --> 02:00:38.880]   I think that there's a way to...
[02:00:38.880 --> 02:00:42.040]   Yeah, I thought through and I was talking to a few friends who are developers, I think
[02:00:42.040 --> 02:00:47.360]   there's a way to append the URL to prevent it from being shared, which is not the thing
[02:00:47.360 --> 02:00:48.360]   is the case.
[02:00:48.360 --> 02:00:49.360]   Which would be crazy to do that.
[02:00:49.360 --> 02:00:50.360]   Would you?
[02:00:50.360 --> 02:00:51.360]   Yeah.
[02:00:51.360 --> 02:00:52.360]   Sure.
[02:00:52.360 --> 02:00:54.320]   Facebook drives a huge amount of traffic to these publications.
[02:00:54.320 --> 02:01:00.240]   Facebook is driving traffic, but Facebook is also cannibalizing the revenues of a lot
[02:01:00.240 --> 02:01:01.240]   of...
[02:01:01.240 --> 02:01:02.240]   And to be fair, it's because...
[02:01:02.240 --> 02:01:04.960]   I mean, this is a whole other show, but news organizations didn't think this through
[02:01:04.960 --> 02:01:05.960]   in advance, right?
[02:01:05.960 --> 02:01:08.560]   They didn't have a lot of choice either.
[02:01:08.560 --> 02:01:12.560]   Well, I think there was an alternate scenario that...
[02:01:12.560 --> 02:01:16.240]   They could have, you mean the beginning if they'd said, "No, you can't share ourselves."
[02:01:16.240 --> 02:01:17.240]   Right.
[02:01:17.240 --> 02:01:19.360]   And this is my point.
[02:01:19.360 --> 02:01:23.560]   My point is that the year 2017 is very similar to the year...
[02:01:23.560 --> 02:01:26.560]   It's very similar to the year 1987.
[02:01:26.560 --> 02:01:32.480]   1987 was that part of the bridge where the academic internet became the commercial internet
[02:01:32.480 --> 02:01:35.440]   and news organizations at that point.
[02:01:35.440 --> 02:01:40.880]   When I was a journalist in the very, very early days of this, and most of the news organizations
[02:01:40.880 --> 02:01:45.120]   were either ignoring it or they were demoting people over to the website.
[02:01:45.120 --> 02:01:46.440]   Nobody was taking it seriously.
[02:01:46.440 --> 02:01:50.040]   Certainly nobody had thought through business models or business plans or anything else like
[02:01:50.040 --> 02:01:52.440]   that or modeling out what this might look like.
[02:01:52.440 --> 02:01:58.480]   And so we wind up where we are today where digital media, social media have completely
[02:01:58.480 --> 02:02:04.160]   eroded the wallet share and the mind share of traditional media sources and they've never
[02:02:04.160 --> 02:02:05.160]   changed their model.
[02:02:05.160 --> 02:02:08.120]   Gear 2017 is the beginning of the next bridge.
[02:02:08.120 --> 02:02:14.280]   And that next bridge is from us typing on machines to us talking to machines and voices
[02:02:14.280 --> 02:02:18.600]   the thing that will reshape the information landscape going forward.
[02:02:18.600 --> 02:02:20.880]   And once again, none of them are at the table.
[02:02:20.880 --> 02:02:27.480]   So of the big seven AI companies that are working in an arms race for AI, journalists
[02:02:27.480 --> 02:02:28.480]   aren't there.
[02:02:28.480 --> 02:02:32.240]   Sure, there's all kinds of news organizations on Echo, but they haven't thought through
[02:02:32.240 --> 02:02:35.200]   that they've sort of put their stuff there because they feel like they need to put their
[02:02:35.200 --> 02:02:39.920]   stuff there and not a single publisher or television broadcaster, radio broadcaster
[02:02:39.920 --> 02:02:42.840]   has developed a model for the future.
[02:02:42.840 --> 02:02:44.600]   There's giving away their content.
[02:02:44.600 --> 02:02:48.400]   And so we're going to wind up in an even worse situation years from now than we are right
[02:02:48.400 --> 02:02:53.240]   now because nobody, you know, because history is repeating itself.
[02:02:53.240 --> 02:02:56.480]   I just, I don't see how you put the genie back in the bottle.
[02:02:56.480 --> 02:02:58.960]   I mean, for me, I pay for the New York Times.
[02:02:58.960 --> 02:03:02.720]   I pay for Washington Post and I pay for, you know, the Boston Globe because those are three
[02:03:02.720 --> 02:03:05.680]   papers that are very relevant for what I do.
[02:03:05.680 --> 02:03:11.760]   But I don't go to, I sometimes go to the New York Times, but generally speaking, I look
[02:03:11.760 --> 02:03:16.640]   at articles that people share and I think that's just a formula nowadays for how it
[02:03:16.640 --> 02:03:17.680]   doesn't have to be.
[02:03:17.680 --> 02:03:18.680]   You can turn off that tap.
[02:03:18.680 --> 02:03:20.440]   There's spent a lot of time model.
[02:03:20.440 --> 02:03:25.360]   Isn't there a counter example in Europe where the European publishers hated Google putting
[02:03:25.360 --> 02:03:28.800]   snippets of their content on Google search?
[02:03:28.800 --> 02:03:34.480]   So they said stop and they immediately regretted it because they weren't getting, they lost
[02:03:34.480 --> 02:03:38.920]   a huge amount of traffic by turning off Google.
[02:03:38.920 --> 02:03:40.320]   They didn't wait long enough.
[02:03:40.320 --> 02:03:41.320]   So here's the thing.
[02:03:41.320 --> 02:03:43.040]   This has to be a transition.
[02:03:43.040 --> 02:03:47.520]   So no, of course you're going to see an initial drop and that has to be explained to advertisers
[02:03:47.520 --> 02:03:50.480]   and that has to be explained to the, you know, everybody else in the organization that's
[02:03:50.480 --> 02:03:51.960]   thinking through ROI.
[02:03:51.960 --> 02:03:53.880]   So of course, there's a temporary drop.
[02:03:53.880 --> 02:03:57.520]   But again, if everybody yanks, there's, you know, if all the news organizations yank their
[02:03:57.520 --> 02:04:00.560]   content off of Facebook, right?
[02:04:00.560 --> 02:04:06.280]   And if the place that you have to get your news is on the website and let's go one more.
[02:04:06.280 --> 02:04:10.240]   Let's say that news organizations band together and buy Twitter, right?
[02:04:10.240 --> 02:04:14.240]   Twitter who share prices is going nowhere but, you know, right?
[02:04:14.240 --> 02:04:18.240]   So let's say that they band together and they buy Twitter and they turn Twitter into a 21st
[02:04:18.240 --> 02:04:20.000]   century wire service.
[02:04:20.000 --> 02:04:24.840]   And we all still use Twitter, but Twitter is a much better experience because there is
[02:04:24.840 --> 02:04:29.760]   better, you know, because they're dealing with trolls and we're using as a discovery
[02:04:29.760 --> 02:04:31.760]   tool, you know, I like it.
[02:04:31.760 --> 02:04:34.560]   And, you know, and it's not meant to turn a profit.
[02:04:34.560 --> 02:04:35.760]   It's not meant to turn a profit.
[02:04:35.760 --> 02:04:39.320]   It's similar to the AP or the original intent of the AP.
[02:04:39.320 --> 02:04:40.760]   It's meant as a distribution arm.
[02:04:40.760 --> 02:04:43.440]   Then all of a sudden because news organizations don't have to.
[02:04:43.440 --> 02:04:45.280]   Then they wouldn't need Facebook.
[02:04:45.280 --> 02:04:46.280]   That's right.
[02:04:46.280 --> 02:04:47.280]   Yeah.
[02:04:47.280 --> 02:04:48.280]   That's right.
[02:04:48.280 --> 02:04:49.280]   I don't think it's their solutions.
[02:04:49.280 --> 02:04:51.280]   There are solutions, but this is, this is my whole point.
[02:04:51.280 --> 02:04:52.280]   You have to figure out the box.
[02:04:52.280 --> 02:04:54.240]   Not enough people, that's right.
[02:04:54.240 --> 02:04:59.720]   And not enough people are thinking through far enough and they're not modeling this out
[02:04:59.720 --> 02:05:00.720]   far enough.
[02:05:00.720 --> 02:05:02.400]   They're so consumed with right now.
[02:05:02.400 --> 02:05:07.640]   And to be fair, everybody needs to be consumed right now because there are serious financial
[02:05:07.640 --> 02:05:11.080]   concerns, but tomorrow is far worse than today.
[02:05:11.080 --> 02:05:12.680]   Tomorrow's far worse than today.
[02:05:12.680 --> 02:05:17.800]   You know what protected us is I just had an aversion to living on somebody else's
[02:05:17.800 --> 02:05:18.800]   platform.
[02:05:18.800 --> 02:05:24.360]   What, Zinga and companies that created Facebook games suffered when Facebook pulled the plug
[02:05:24.360 --> 02:05:25.360]   on them.
[02:05:25.360 --> 02:05:29.440]   And it was very clear that if you live on somebody else's platform, you die on somebody
[02:05:29.440 --> 02:05:30.440]   else's platform.
[02:05:30.440 --> 02:05:32.120]   They control your destiny.
[02:05:32.120 --> 02:05:33.800]   And I just was never willing to do that.
[02:05:33.800 --> 02:05:35.680]   That's why we're not big on YouTube either.
[02:05:35.680 --> 02:05:36.680]   That's right.
[02:05:36.680 --> 02:05:42.720]   And you have a brilliant, I mean, you are brilliant and you have launched what I consider
[02:05:42.720 --> 02:05:48.760]   to be the only modern, well, the only really sort of modern media company, Vox.
[02:05:48.760 --> 02:05:52.480]   Has a tech stack and a media stack that's very interesting.
[02:05:52.480 --> 02:05:57.160]   But if I think about true success, like you're it, right?
[02:05:57.160 --> 02:05:59.200]   But it required you going off platform.
[02:05:59.200 --> 02:06:01.800]   And I think you had enough sense to do that.
[02:06:01.800 --> 02:06:03.760]   And it was a wise choice.
[02:06:03.760 --> 02:06:05.920]   It wasn't, it wasn't conscious.
[02:06:05.920 --> 02:06:12.200]   It was completely a gut and coming mostly from the experience of working for the man.
[02:06:12.200 --> 02:06:15.160]   I just didn't want to work for the man.
[02:06:15.160 --> 02:06:21.240]   I think, you know, I definitely see what you're saying, Leah, about like, you know, Zinga and
[02:06:21.240 --> 02:06:22.760]   those game companies dying.
[02:06:22.760 --> 02:06:24.240]   Like that happened here in Boston.
[02:06:24.240 --> 02:06:29.040]   We saw that happen repeatedly and Facebook wouldn't give them support and they just died.
[02:06:29.040 --> 02:06:31.960]   And these were people that put a lot of time into games.
[02:06:31.960 --> 02:06:38.280]   I think though, I don't think this is a distribution problem as much as I think it's a human nature
[02:06:38.280 --> 02:06:39.880]   problem.
[02:06:39.880 --> 02:06:44.320]   Because I think I definitely think you've got the problem of, you know, authors getting
[02:06:44.320 --> 02:06:48.480]   paid for their content and journalistic organizations getting paid for their content.
[02:06:48.480 --> 02:06:52.600]   But I think the current problem is really human nature problem.
[02:06:52.600 --> 02:06:57.600]   You know, the articles that I have to kind of, you know, even me forced myself to read
[02:06:57.600 --> 02:07:03.280]   like a long, you know, Wall Street Journal piece on like double default swaps and, you
[02:07:03.280 --> 02:07:09.160]   know, securitization of different, you know, financial products, that stuff I need to know.
[02:07:09.160 --> 02:07:11.880]   And it's boring, it's kind of terrible to go through.
[02:07:11.880 --> 02:07:16.640]   You know, it's much more fun and easy for me to click on, you know, story about guardians
[02:07:16.640 --> 02:07:19.040]   of the galaxy, which anyone can write.
[02:07:19.040 --> 02:07:21.040]   So if I human.
[02:07:21.040 --> 02:07:27.280]   I think the human nature problem is, I think that people are going to click on things that
[02:07:27.280 --> 02:07:28.280]   they already agree with.
[02:07:28.280 --> 02:07:32.720]   And I think that's always going to be cheaper and easier to manufacture.
[02:07:32.720 --> 02:07:37.120]   It's why you've seen these fake news organizations just explode.
[02:07:37.120 --> 02:07:41.360]   So the one of the most popular stories to date on the Washington Post is still nine things
[02:07:41.360 --> 02:07:44.320]   you need to know about Siri, but we're too afraid to ask.
[02:07:44.320 --> 02:07:45.920]   So and that was a sad.
[02:07:45.920 --> 02:07:48.600]   It's actually not.
[02:07:48.600 --> 02:07:51.440]   It was by Matt Fisher, Max Fisher.
[02:07:51.440 --> 02:07:55.520]   It was a listicle and it's what started all the journalists making fun of listicles, but
[02:07:55.520 --> 02:07:59.280]   I will tell you it was incredibly well done.
[02:07:59.280 --> 02:08:06.200]   It was incredibly informative and it, you know, but you would agree it didn't succeed because
[02:08:06.200 --> 02:08:08.160]   it was well done and informative.
[02:08:08.160 --> 02:08:09.960]   It succeeded because of the headline.
[02:08:09.960 --> 02:08:14.960]   Well, I think it initially did, but again, you know, listen, I used to write for the Wall
[02:08:14.960 --> 02:08:15.960]   Street Journal.
[02:08:15.960 --> 02:08:17.960]   I used to be on staff at the Journal and at Newsweek.
[02:08:17.960 --> 02:08:24.960]   So I'm, you know, I'm intimately familiar and aware of the power of great journalism
[02:08:24.960 --> 02:08:27.800]   and of good headlines and all the rest.
[02:08:27.800 --> 02:08:31.920]   But the challenge that we're facing is people have limited attention and news organizations
[02:08:31.920 --> 02:08:34.200]   aren't competing against other news organizations.
[02:08:34.200 --> 02:08:37.360]   They are competing against anything now that sucks away attention.
[02:08:37.360 --> 02:08:38.360]   Yeah.
[02:08:38.360 --> 02:08:40.080]   So that could be a game.
[02:08:40.080 --> 02:08:43.800]   You know, that could be the, the, my Fitbit, right?
[02:08:43.800 --> 02:08:44.800]   Right.
[02:08:44.800 --> 02:08:46.160]   It could be, it could be any number of things.
[02:08:46.160 --> 02:08:50.600]   And so this isn't a question of, this is a question of being smarter.
[02:08:50.600 --> 02:08:55.240]   And again, map mapping out the future.
[02:08:55.240 --> 02:09:01.760]   So, so there, and if, if something isn't done, we're going to be in a far worse place, a
[02:09:01.760 --> 02:09:03.760]   decade from now that then we are today.
[02:09:03.760 --> 02:09:08.760]   And if you think you've got a fake, we have a fake news problem today, you know, a decade
[02:09:08.760 --> 02:09:12.120]   from now, we're going to be even worse off because we're going to have fewer news organizations,
[02:09:12.120 --> 02:09:13.120]   not more of them.
[02:09:13.120 --> 02:09:18.520]   And it has a lot to do with this burgeoning AI voice ecosystem that everybody's ignoring.
[02:09:18.520 --> 02:09:20.960]   It's a great point.
[02:09:20.960 --> 02:09:22.680]   I'd love to see that by Twitter.
[02:09:22.680 --> 02:09:26.800]   I think that's a, you've come up with the first reasonable plan for Twitter.
[02:09:26.800 --> 02:09:28.320]   Sorry for the hard.
[02:09:28.320 --> 02:09:33.600]   It's, there's a whole, the whole thing is laid out both in mother Jones and in the
[02:09:33.600 --> 02:09:37.480]   there's a publication put up by Harvard called the Harvard Neiman Report.
[02:09:37.480 --> 02:09:38.480]   They're both there.
[02:09:38.480 --> 02:09:39.840]   Neiman Press, right.
[02:09:39.840 --> 02:09:41.320]   Thank you.
[02:09:41.320 --> 02:09:42.520]   I think we can wrap on this.
[02:09:42.520 --> 02:09:47.080]   We've gone a long time and I hate to because it's such a good conversation.
[02:09:47.080 --> 02:09:48.120]   You guys are great.
[02:09:48.120 --> 02:09:51.800]   We will do this again soon, I hope.
[02:09:51.800 --> 02:09:53.800]   Thank you so much, Brianna Wu for joining us.
[02:09:53.800 --> 02:09:54.800]   Brianna Wu for Congress.
[02:09:54.800 --> 02:09:55.800]   I'm voting for it.
[02:09:55.800 --> 02:09:56.800]   I don't even live in Massachusetts.
[02:09:56.800 --> 02:09:57.800]   Brianna?
[02:09:57.800 --> 02:09:58.800]   Don't do that, Leah.
[02:09:58.800 --> 02:09:59.800]   You're going to do that.
[02:09:59.800 --> 02:10:00.800]   Please don't do that.
[02:10:00.800 --> 02:10:01.800]   Please don't do that.
[02:10:01.800 --> 02:10:02.800]   I wouldn't do it.
[02:10:02.800 --> 02:10:04.800]   What are the primary there?
[02:10:04.800 --> 02:10:05.800]   It's next year.
[02:10:05.800 --> 02:10:11.160]   So you have got a little bit more than a year to get my name out there and it is, you know,
[02:10:11.160 --> 02:10:12.160]   it's hard work.
[02:10:12.160 --> 02:10:13.160]   You're going out there.
[02:10:13.160 --> 02:10:17.720]   You're meeting people and it's a real challenge for somebody that's more of a national figure
[02:10:17.720 --> 02:10:23.160]   to translate it to, you know, a small, a smaller local scale.
[02:10:23.160 --> 02:10:24.160]   Retail.
[02:10:24.160 --> 02:10:29.920]   Like a lot of it is and like a lot of, you know, engineers, I am most comfortable, you
[02:10:29.920 --> 02:10:31.440]   know, behind a computer screen.
[02:10:31.440 --> 02:10:34.040]   Like I love spending, you know, afternoons coding.
[02:10:34.040 --> 02:10:37.520]   This is something that's definitely using a different part of my brain.
[02:10:37.520 --> 02:10:42.320]   But I just, I think we've got to have a better quality of people serving their country.
[02:10:42.320 --> 02:10:44.920]   Well, I'm glad you haven't given up on the on the voters.
[02:10:44.920 --> 02:10:49.240]   I feel like I have, but I'm going to take my inspiration from you, Brianna Wu.
[02:10:49.240 --> 02:10:55.080]   And when you win, then I will say, wow, the voters are smart.
[02:10:55.080 --> 02:10:57.640]   Brianna Wu 2018.com.
[02:10:57.640 --> 02:11:00.400]   Call me up that there's some policy things like you want to talk about.
[02:11:00.400 --> 02:11:02.840]   I will, I will have a voice in Congress.
[02:11:02.840 --> 02:11:08.240]   Amy Webb, somebody in the chat room just said, why did it take Twitt so long to get Amy
[02:11:08.240 --> 02:11:10.240]   on?
[02:11:10.240 --> 02:11:11.240]   Because I'm slow.
[02:11:11.240 --> 02:11:14.120]   Amy Webb is the author of the signals are talking.
[02:11:14.120 --> 02:11:19.440]   We only met recently and well, you only met me recently, but I have been a very, very long
[02:11:19.440 --> 02:11:23.520]   time fan girl and listener and you were so well, you'll be back.
[02:11:23.520 --> 02:11:27.760]   Both of you, two of our newer panelists and two of our best.
[02:11:27.760 --> 02:11:29.200]   So I'm really glad to have you both on.
[02:11:29.200 --> 02:11:30.640]   Thank you so much.
[02:11:30.640 --> 02:11:31.640]   Really appreciate it.
[02:11:31.640 --> 02:11:36.920]   Reminder, we're going to be a maker fair on Feb, sorry, Mark May.
[02:11:36.920 --> 02:11:42.480]   What year is this 19th May 19th Friday with Father Robert Ballisare and many of the Twitt
[02:11:42.480 --> 02:11:44.920]   crew from one to five PM.
[02:11:44.920 --> 02:11:47.640]   They promised us a table or somewhere we can sit down.
[02:11:47.640 --> 02:11:48.840]   I'd love to see you.
[02:11:48.840 --> 02:11:52.880]   And if you have a project you've been working on or making and you could bring it by, that
[02:11:52.880 --> 02:11:53.880]   would be great.
[02:11:53.880 --> 02:11:58.240]   We were going to bring cameras as well and we can feature some of you on the new screensavers.
[02:11:58.240 --> 02:12:01.080]   makerfair.com with an E at the end.
[02:12:01.080 --> 02:12:06.480]   The Bay Area Maker Fair, the original is May 19th through the 21st and we'll be there on
[02:12:06.480 --> 02:12:07.480]   day one.
[02:12:07.480 --> 02:12:09.200]   I hope you will come by and say hi.
[02:12:09.200 --> 02:12:14.760]   We do this show every Sunday, 3 p.m. Pacific, 6 p.m. Eastern, 2200 UTC.
[02:12:14.760 --> 02:12:15.920]   Tune in and watch live.
[02:12:15.920 --> 02:12:20.000]   If you are watching live, I invite you to join us in the chatroom to irc.twitt.tv.
[02:12:20.000 --> 02:12:22.680]   You could be a part of the conversation.
[02:12:22.680 --> 02:12:26.200]   It's always great to have the active and engaged chat here.
[02:12:26.200 --> 02:12:30.400]   They're very, very helpful to the show.
[02:12:30.400 --> 02:12:33.400]   But if you can't be here live, of course, the real reason we're here is to make these
[02:12:33.400 --> 02:12:34.960]   shows available on demand.
[02:12:34.960 --> 02:12:37.240]   They used to be called podcasts.
[02:12:37.240 --> 02:12:38.340]   Remember those?
[02:12:38.340 --> 02:12:44.000]   If you go to twit.tv, you'll find all of our shows, all of our episodes, you can subscribe.
[02:12:44.000 --> 02:12:48.680]   Get them automatically delivered to your phone every week.
[02:12:48.680 --> 02:12:51.240]   That's the best way, of course, to consume our content.
[02:12:51.240 --> 02:12:53.640]   There's also Twit apps on every platform.
[02:12:53.640 --> 02:12:56.960]   You can use those as well or your favorite podcatcher.
[02:12:56.960 --> 02:13:01.520]   But don't miss an episode because a conversation is always very interesting on Twit.
[02:13:01.520 --> 02:13:02.840]   Thanks for being here.
[02:13:02.840 --> 02:13:03.840]   We'll see you next time.
[02:13:03.840 --> 02:13:05.680]   Another Twit is in the can.
[02:13:05.680 --> 02:13:06.680]   Bye-bye.
[02:13:06.680 --> 02:13:07.680]   Great show.
[02:13:07.680 --> 02:13:08.680]   Do the Twit.
[02:13:08.680 --> 02:13:09.680]   All right.
[02:13:09.680 --> 02:13:10.680]   Do the Twit, baby.
[02:13:10.680 --> 02:13:11.680]   Do the Twit.
[02:13:11.680 --> 02:13:12.680]   All right.
[02:13:12.680 --> 02:13:13.680]   Do the Twit.
[02:13:13.680 --> 02:13:14.680]   All right.

