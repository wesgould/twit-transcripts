;FFMETADATA1
title=Distracted by the Sex Robots
artist=TWiT
album_artist=TWiT
album=This Week in Tech
track=648
genre=Podcast
comment=http://twit.tv/twit
copyright=These netcasts are released under a Creative Commons License - Attribution-NonCommercial-NoDerivatives 4.0 International. TWiT and TWiT Logo are registered trademarks of Leo Laporte.
publisher=TWiT
date=2018
encoder=Lavf58.76.100



[00:00:00.000 --> 00:00:04.000]   It's time for Twit this week in Tech.
[00:00:04.000 --> 00:00:05.000]   What a great show.
[00:00:05.000 --> 00:00:08.000]   I mean, I mean, what a great show to kick off the new year with.
[00:00:08.000 --> 00:00:10.000]   Micah Sargent is here.
[00:00:10.000 --> 00:00:12.000]   He's senior editor at Mobile Nations.
[00:00:12.000 --> 00:00:15.000]   Amy Webb, our futurist from AmyWeb.io.
[00:00:15.000 --> 00:00:20.000]   And Mike Elgin, our gastro nomad, he's decided to light in our neck of the woods to talk
[00:00:20.000 --> 00:00:22.000]   about the week's Tech news.
[00:00:22.000 --> 00:00:26.000]   Of course, we're going to talk about meltdown and specter and come up with a really good, I think,
[00:00:26.000 --> 00:00:29.000]   long-term solution for this whole thing.
[00:00:29.000 --> 00:00:37.000]   We do have to recap the iPhone battery crisis and somehow we got distracted during that conversation.
[00:00:37.000 --> 00:00:42.000]   By Smart Sex Dolls and CES and a lot of stuff.
[00:00:42.000 --> 00:00:43.000]   It's a fun Twit.
[00:00:43.000 --> 00:00:44.000]   Stay tuned.
[00:00:44.000 --> 00:00:45.000]   It's next.
[00:00:45.000 --> 00:00:49.000]   Netcast you love.
[00:00:49.000 --> 00:00:51.000]   From people you trust.
[00:00:51.000 --> 00:00:56.000]   This is Twit.
[00:00:56.000 --> 00:01:03.000]   Bandwidth for this week in Tech is provided by CashFly at cachefly.com.
[00:01:03.000 --> 00:01:09.000]   This is Twit.
[00:01:09.000 --> 00:01:11.000]   This week in Tech.
[00:01:11.000 --> 00:01:16.000]   Episode 648 recorded Sunday, January 7, 2018.
[00:01:16.000 --> 00:01:20.000]   Distracted by the sex worldpots.
[00:01:20.000 --> 00:01:25.000]   This week in Tech is brought to you by Casper, a sleep brand that continues to revolutionize
[00:01:25.000 --> 00:01:30.000]   its line of products to create an exceptionally comfortable sleep experience one night at a time.
[00:01:30.000 --> 00:01:35.000]   Get $50 toward any mattress purchased by visiting casper.com/twit
[00:01:35.000 --> 00:01:38.000]   and using the promo code TWIT at check out.
[00:01:38.000 --> 00:01:42.000]   And by Rocket Mortgage by Quick and Loans, home plays a big role in your life.
[00:01:42.000 --> 00:01:45.000]   That's why Quick and Loans created Rocket Mortgage.
[00:01:45.000 --> 00:01:50.000]   It lets you apply simply and understand the entire mortgage process fully so you can be confident.
[00:01:50.000 --> 00:01:51.000]   You're getting the right mortgage for you.
[00:01:51.000 --> 00:01:55.000]   Get started at Rocket Mortgage.com/twit2
[00:01:55.000 --> 00:02:02.000]   and by ITProTV, the fun and entertaining way to sharpen your IT skills.
[00:02:02.000 --> 00:02:10.000]   Visit ITPro.tv/twit and use the code TWIT30 to get a free seven day trial and 30% off a monthly membership
[00:02:10.000 --> 00:02:12.000]   for the lifetime of your active subscription.
[00:02:12.000 --> 00:02:14.000]   And by ERO.
[00:02:14.000 --> 00:02:20.000]   Never think about Wi-Fi again with ERO's hyperfast, super simple Wi-Fi system.
[00:02:20.000 --> 00:02:24.000]   And out the second generation ERO is Tri-Band and twice as fast.
[00:02:24.000 --> 00:02:30.000]   For free overnight shipping to the US or Canada, visit ERO.com, select overnight shipping at checkout,
[00:02:30.000 --> 00:02:32.000]   and enter the code TWIT.
[00:02:32.000 --> 00:02:37.000]   It's time for TWIT this week at Tech!
[00:02:37.000 --> 00:02:41.000]   Happy New Year everybody, our first show of 2018.
[00:02:41.000 --> 00:02:43.000]   Let's kick things off with the bang.
[00:02:43.000 --> 00:02:48.000]   This is a show we get together and talk about the week's Tech News with very smart people.
[00:02:48.000 --> 00:02:57.000]   The goal besides just kind of schmoozing and having fun and drinking wine and coffee is to really understand better these stories.
[00:02:57.000 --> 00:03:05.000]   Mike Elgin is here as always a really deep thinker on these subjects, although I think you've retreated from Tech journalism.
[00:03:05.000 --> 00:03:08.000]   Have I? Not really. I still do Tech journalism.
[00:03:08.000 --> 00:03:15.000]   For Computer World's company still, yes, yes, and working on some other things and working on future.
[00:03:15.000 --> 00:03:23.000]   One might wonder why if you have a career traveling the world and eating great food, you would bother with technology.
[00:03:23.000 --> 00:03:26.000]   You know, technology is the thing that it started with.
[00:03:26.000 --> 00:03:30.000]   The Gaster Nomad is my new book that you're waving around there.
[00:03:30.000 --> 00:03:31.000]   So excited.
[00:03:31.000 --> 00:03:36.000]   Yeah, and this is a book basically I've been living on and off as a digital nomad for about 10 years.
[00:03:36.000 --> 00:03:42.000]   And lots of people, lots of people, I know a huge number of people in your audience, Leo, because I know that you inspire a lot of people to travel.
[00:03:42.000 --> 00:03:49.000]   They go read the digital nomad literature and just fall flat and say, "Well, I guess I can't really figure out how to do it."
[00:03:49.000 --> 00:04:02.000]   So I'm here to show you exactly how to do it, how you can actually travel anywhere from a little bit more to constantly for the rest of your life, and also why to travel that much.
[00:04:02.000 --> 00:04:03.000]   Yeah.
[00:04:03.000 --> 00:04:05.000]   It's about the food, Leo, basically.
[00:04:05.000 --> 00:04:10.000]   And it's a long story short is that life is really short and the world is really large.
[00:04:10.000 --> 00:04:14.000]   I'm going to be a Barcelona in September, anything I should eat.
[00:04:14.000 --> 00:04:15.000]   Oh, yeah.
[00:04:15.000 --> 00:04:16.000]   Everything.
[00:04:16.000 --> 00:04:17.000]   Yeah, everything.
[00:04:17.000 --> 00:04:18.000]   Payaya.
[00:04:18.000 --> 00:04:19.000]   Yes.
[00:04:19.000 --> 00:04:20.000]   HamÃ³n, hebatico.
[00:04:20.000 --> 00:04:25.000]   I'll send you a list because I probably know Barcelona better than any city.
[00:04:25.000 --> 00:04:26.000]   I know you do.
[00:04:26.000 --> 00:04:28.000]   You're thinking of staying at the Hotel Nary.
[00:04:28.000 --> 00:04:29.000]   It's in the Gothic Quarter.
[00:04:29.000 --> 00:04:31.000]   I got another recommendation for you.
[00:04:31.000 --> 00:04:35.000]   There's a hotel where the entire lobby is the best bakery in Spain.
[00:04:35.000 --> 00:04:36.000]   Okay, that's it.
[00:04:36.000 --> 00:04:37.000]   It's done.
[00:04:37.000 --> 00:04:39.000]   The lobby is a bakery.
[00:04:39.000 --> 00:04:41.000]   I'm going gluten free, baby.
[00:04:41.000 --> 00:04:43.000]   I'm going all gluten all the time.
[00:04:43.000 --> 00:04:45.000]   Go gluten free free free.
[00:04:45.000 --> 00:04:46.000]   Guten free free.
[00:04:46.000 --> 00:04:47.000]   Yeah.
[00:04:47.000 --> 00:04:48.000]   Also with us, Amy Webb.
[00:04:48.000 --> 00:04:50.000]   It's so great to have Amy back.
[00:04:50.000 --> 00:04:51.000]   We love Amy.
[00:04:51.000 --> 00:04:57.000]   Amy Webb.io is her website, but we first met Amy when we interviewed her on triangulation for her book.
[00:04:57.000 --> 00:04:58.000]   Great book.
[00:04:58.000 --> 00:04:59.000]   The signals are talking.
[00:04:59.000 --> 00:05:02.000]   Why today's, I love this subtitle.
[00:05:02.000 --> 00:05:06.000]   Why today's fringe is tomorrow's mainstream.
[00:05:06.000 --> 00:05:11.000]   It's on Amazon and you told me Audible is going to have a version of it soon.
[00:05:11.000 --> 00:05:12.000]   Yeah.
[00:05:12.000 --> 00:05:14.000]   I'm not sure when.
[00:05:14.000 --> 00:05:16.000]   Thankfully, I am not the person reading it.
[00:05:16.000 --> 00:05:20.000]   But somebody will read it and it will be out and you can listen to it.
[00:05:20.000 --> 00:05:21.000]   Hey, congratulations.
[00:05:21.000 --> 00:05:24.000]   I see you won the 2017 Axiom Business Book Award.
[00:05:24.000 --> 00:05:25.000]   I did.
[00:05:25.000 --> 00:05:31.000]   I also just won the Global Thinkors 50 Radar Award for the books.
[00:05:31.000 --> 00:05:32.000]   That was pretty cool.
[00:05:32.000 --> 00:05:33.000]   I just got back from London.
[00:05:33.000 --> 00:05:37.280]   It's all the secrets of, she's a futurist and it's all the secrets of futurists, what they
[00:05:37.280 --> 00:05:41.520]   do to figure out what's going to happen next.
[00:05:41.520 --> 00:05:42.520]   That's right.
[00:05:42.520 --> 00:05:43.520]   Love it.
[00:05:43.520 --> 00:05:44.520]   Also joining us.
[00:05:44.520 --> 00:05:45.760]   And by the way, you're not in front of your whiteboard.
[00:05:45.760 --> 00:05:48.440]   How are we supposed to know what's coming up?
[00:05:48.440 --> 00:05:50.240]   So I'm actually working on a new book.
[00:05:50.240 --> 00:05:51.240]   Oh, good.
[00:05:51.240 --> 00:05:56.360]   It's a manifesto about the real future of AI and my whiteboard right now is covered in
[00:05:56.360 --> 00:05:59.000]   book related stuff and it would get everybody a headache.
[00:05:59.000 --> 00:06:00.440]   So get us excited.
[00:06:00.440 --> 00:06:02.360]   Actually, this is a great topic.
[00:06:02.360 --> 00:06:03.840]   We had Phil Libbon on.
[00:06:03.840 --> 00:06:10.240]   He's got an AI incubator and we were talking and he said, yeah, there's a lot of misunderstanding
[00:06:10.240 --> 00:06:13.080]   about what really is about to happen.
[00:06:13.080 --> 00:06:14.520]   So I look forward to reading that.
[00:06:14.520 --> 00:06:15.520]   That's great.
[00:06:15.520 --> 00:06:16.880]   Also joining us from Mobile Nations.
[00:06:16.880 --> 00:06:17.880]   We've had him before.
[00:06:17.880 --> 00:06:18.880]   It's great to have Mike Asargent back.
[00:06:18.880 --> 00:06:20.720]   He's senior editor there.
[00:06:20.720 --> 00:06:21.920]   And Mike, I forgot.
[00:06:21.920 --> 00:06:23.520]   Are you in Brooklyn?
[00:06:23.520 --> 00:06:24.520]   No, no.
[00:06:24.520 --> 00:06:25.520]   I'm in the Midwest.
[00:06:25.520 --> 00:06:30.720]   I'm in Missouri or Missouri as people who don't live in Missouri think it's called.
[00:06:30.720 --> 00:06:32.280]   Are you normally there?
[00:06:32.280 --> 00:06:33.280]   Yeah, yeah.
[00:06:33.280 --> 00:06:34.840]   This is where I live.
[00:06:34.840 --> 00:06:38.200]   People already think I call them the flyover states.
[00:06:38.200 --> 00:06:43.920]   It's not good for me to say things like you live there by choice.
[00:06:43.920 --> 00:06:50.520]   You know, it's not as bad as as I think people who don't live here think it is.
[00:06:50.520 --> 00:06:54.040]   It's pretty calm, pretty quiet and pretty great.
[00:06:54.040 --> 00:06:55.680]   There are two other book writers on here.
[00:06:55.680 --> 00:06:57.720]   I feel like I should have written a book.
[00:06:57.720 --> 00:07:01.280]   It's called I Love Dogs and Coffee Is Good.
[00:07:01.280 --> 00:07:04.920]   Mike, you're way too young to have written a book.
[00:07:04.920 --> 00:07:08.520]   When you got plenty of time, nobody should write a book in their 20s.
[00:07:08.520 --> 00:07:10.360]   You would regret it deeply in your 30s.
[00:07:10.360 --> 00:07:11.360]   Let's put it.
[00:07:11.360 --> 00:07:13.160]   Well, that's good advice.
[00:07:13.160 --> 00:07:14.160]   I appreciate that.
[00:07:14.160 --> 00:07:15.800]   Wait, wait a year or two.
[00:07:15.800 --> 00:07:20.120]   Well, I guess we should talk about the big story, which is the worst security flaw in
[00:07:20.120 --> 00:07:22.880]   the history of all mankind.
[00:07:22.880 --> 00:07:25.400]   At least it would be if you watched mainstream media.
[00:07:25.400 --> 00:07:26.400]   Yes.
[00:07:26.400 --> 00:07:32.200]   And I've listened to Steve Gibson's explanation for this several times.
[00:07:32.200 --> 00:07:33.880]   It's over and over.
[00:07:33.880 --> 00:07:39.040]   And I think I understand how well I don't understand this, but it's a very complicated
[00:07:39.040 --> 00:07:40.040]   thing.
[00:07:40.040 --> 00:07:45.000]   But the way that I would characterize this is that chips for the last...
[00:07:45.000 --> 00:07:46.480]   What's 1995?
[00:07:46.480 --> 00:07:49.160]   13 years now.
[00:07:49.160 --> 00:07:53.000]   They guess what is going to happen to speed things up.
[00:07:53.000 --> 00:07:58.560]   And we've now discovered that that information that they're guessing can be peaked at.
[00:07:58.560 --> 00:07:59.560]   Leaked.
[00:07:59.560 --> 00:08:01.520]   Leaked, harvested, whatever, theoretically.
[00:08:01.520 --> 00:08:02.800]   It's all theoretical.
[00:08:02.800 --> 00:08:07.200]   So we'll start with the good news or the sunny perspective.
[00:08:07.200 --> 00:08:10.120]   That's why I'm here, Leo, is to provide the sunny, happy perspective.
[00:08:10.120 --> 00:08:12.120]   Mr. Optimus.
[00:08:12.120 --> 00:08:13.120]   Yes.
[00:08:13.120 --> 00:08:18.200]   Which is that, first of all, we don't know of any actual exploit that's taken place from
[00:08:18.200 --> 00:08:19.200]   this.
[00:08:19.200 --> 00:08:23.080]   And number two, it's going to be fixed and it's all going to be fine.
[00:08:23.080 --> 00:08:25.080]   That's the best possible...
[00:08:25.080 --> 00:08:31.080]   Yeah, it's the best puzzle scenario, but it may actually not be true, but we'll delve
[00:08:31.080 --> 00:08:32.080]   into that a little bit.
[00:08:32.080 --> 00:08:37.760]   These are the two flaws which are interestingly discovered simultaneously meltdown and specter.
[00:08:37.760 --> 00:08:43.960]   Specter effects, intel chips, meltdown effects, not just intel chips, but AMD chips and some
[00:08:43.960 --> 00:08:47.040]   arm chips, including Qualcomm chips.
[00:08:47.040 --> 00:08:49.600]   And that's what is really the issue here.
[00:08:49.600 --> 00:08:54.240]   And one of the reasons I say it is probably the most serious security flaw we've seen,
[00:08:54.240 --> 00:08:58.480]   is because it pretty much affects any modern computer, including an iPhone, including an
[00:08:58.480 --> 00:08:59.720]   iPad.
[00:08:59.720 --> 00:09:02.360]   And so that means it's widespread.
[00:09:02.360 --> 00:09:05.040]   What it isn't is easy.
[00:09:05.040 --> 00:09:06.040]   And there's a...
[00:09:06.040 --> 00:09:10.280]   Bruce Schneider talks a little bit about the synchronicity of two in completely independent
[00:09:10.280 --> 00:09:12.160]   teams discovering this.
[00:09:12.160 --> 00:09:14.280]   And apparently this is non-unusual.
[00:09:14.280 --> 00:09:19.160]   Well, Bruce is very literate, so he compares Leibniz and Newton discovering calculus at
[00:09:19.160 --> 00:09:20.160]   the same time.
[00:09:20.160 --> 00:09:21.360]   I don't think it's like that.
[00:09:21.360 --> 00:09:26.000]   But I think what it is, is researchers tend to look in the same alleyways.
[00:09:26.000 --> 00:09:32.160]   And this is a category of flaws called timing flaws that have been lately very fruitful.
[00:09:32.160 --> 00:09:38.080]   Rohammer is an example where people were able to kind of look into memory based on a kind
[00:09:38.080 --> 00:09:40.040]   of a weird race condition.
[00:09:40.040 --> 00:09:48.480]   So in other words, it's interesting that these are discovered by independent teams.
[00:09:48.480 --> 00:09:55.760]   Schneider says, "That's one reason we should worry about this, because if these two teams
[00:09:55.760 --> 00:10:00.600]   discovered it at roughly the same time independently, who else discovered it and who might have
[00:10:00.600 --> 00:10:02.800]   discovered it even a year ago or two years ago?"
[00:10:02.800 --> 00:10:03.800]   Or 10 years ago.
[00:10:03.800 --> 00:10:06.280]   Or 10 years ago, including the NSA.
[00:10:06.280 --> 00:10:14.160]   So, and I think was it, Micah suggested this XKCD, Comic Strip, Randall is great at explaining
[00:10:14.160 --> 00:10:15.840]   this stuff.
[00:10:15.840 --> 00:10:21.440]   This is XKCD number 1938.
[00:10:21.440 --> 00:10:25.760]   The meltdown inspector exploits you's speculative execution.
[00:10:25.760 --> 00:10:26.760]   What's that?
[00:10:26.760 --> 00:10:29.560]   Well, you know the trolley problem?
[00:10:29.560 --> 00:10:33.080]   Well for a while now, geeks know the trolley problem because it's talked about a lot in
[00:10:33.080 --> 00:10:34.080]   self-driving cars.
[00:10:34.080 --> 00:10:35.080]   Yes.
[00:10:35.080 --> 00:10:41.520]   CPUs have basically been sending trolleys down both paths, quantum style, while awaiting
[00:10:41.520 --> 00:10:42.520]   your choice.
[00:10:42.520 --> 00:10:43.520]   That's speculative execution.
[00:10:43.520 --> 00:10:47.120]   Intel started doing it in 1995 because it really did speed up processors.
[00:10:47.120 --> 00:10:52.120]   They would guess what you will do next, start that execution, and when, if they guess correctly,
[00:10:52.120 --> 00:10:53.560]   it would be massive improvements.
[00:10:53.560 --> 00:10:57.160]   If they guess wrong, yeah, you'd have to back up a little bit.
[00:10:57.160 --> 00:11:00.240]   But it turned out the massive improvement outweighed the slowdown.
[00:11:00.240 --> 00:11:04.760]   So every processor, modern processor uses speculative execution, except for maybe some
[00:11:04.760 --> 00:11:06.360]   of the dumbest ARM processors.
[00:11:06.360 --> 00:11:10.280]   Raspberry Pi, for instance, says we don't do that, so we don't have to worry.
[00:11:10.280 --> 00:11:12.560]   So you know the trolley problem?
[00:11:12.560 --> 00:11:17.280]   Well for a while now, both CPUs have basically been sending trolleys down both paths, quantum
[00:11:17.280 --> 00:11:20.040]   style, while awaiting your choice.
[00:11:20.040 --> 00:11:22.520]   Then the unneeded phantom trolley disappears.
[00:11:22.520 --> 00:11:24.720]   They say, well, you didn't do that.
[00:11:24.720 --> 00:11:28.960]   But the phantom trolley isn't supposed to touch anyone, but it turns out you can still
[00:11:28.960 --> 00:11:34.480]   use it to do stuff and it can drive through walls.
[00:11:34.480 --> 00:11:38.800]   What she's saying here, and what's happening with Meltdown Inspector is because another
[00:11:38.800 --> 00:11:42.320]   technique used by processors is to speed up execution.
[00:11:42.320 --> 00:11:46.520]   Very fast, level one cache right next to the processor.
[00:11:46.520 --> 00:11:53.720]   In the speculative branch, it will load that cache with data for the next process.
[00:11:53.720 --> 00:11:57.320]   And it turns out the current process can peak at that data.
[00:11:57.320 --> 00:12:00.680]   This is why it isn't the most useful flaw ever, because that data, you don't really
[00:12:00.680 --> 00:12:02.720]   get to choose what's in that cache.
[00:12:02.720 --> 00:12:07.520]   But it might contain passwords, logins, credit card numbers.
[00:12:07.520 --> 00:12:12.320]   If you could do it a lot, like for over a long period of time, it might be really valuable.
[00:12:12.320 --> 00:12:17.760]   That's where, by the way, the biggest risk is to people running on processors, shared
[00:12:17.760 --> 00:12:21.120]   processors like in virtual machines or in servers.
[00:12:21.120 --> 00:12:25.520]   So often when you have a web server, you're running on the same machine as a hundred other
[00:12:25.520 --> 00:12:27.120]   websites.
[00:12:27.120 --> 00:12:31.560]   If one of those websites were a bad actor, he could run software that would then peak
[00:12:31.560 --> 00:12:37.160]   into all the other websites, cache activities, and perhaps over a period of time, get stuff.
[00:12:37.160 --> 00:12:38.760]   So that's the biggest risk.
[00:12:38.760 --> 00:12:44.120]   And by the way, that's why Amazon Web Services, Google Services, Microsoft Azure, they've
[00:12:44.120 --> 00:12:45.800]   all been down lately.
[00:12:45.800 --> 00:12:47.640]   Even Epic Gaming went down.
[00:12:47.640 --> 00:12:51.920]   In fact, they said, if you're playing Fortnite on our servers, you may notice some slowness
[00:12:51.920 --> 00:12:55.720]   we're trying to patch them as fast as we can.
[00:12:55.720 --> 00:13:00.880]   So this is a big problem on shared processors, because then you wouldn't have to ever have
[00:13:00.880 --> 00:13:01.880]   malware on your system.
[00:13:01.880 --> 00:13:04.200]   It could just be somebody else acting badly.
[00:13:04.200 --> 00:13:10.520]   But if you had malware or even an Apple discovered that this is a scary one, I think it was
[00:13:10.520 --> 00:13:15.840]   meltdown can be achieved by a browser running a JavaScript program.
[00:13:15.840 --> 00:13:20.760]   So you could theoretically go to a bad website and the JavaScript hidden on that website could
[00:13:20.760 --> 00:13:23.120]   actually try to read your memory content.
[00:13:23.120 --> 00:13:25.080]   So we'd only get what was in the cache.
[00:13:25.080 --> 00:13:26.560]   It would be really hit or miss.
[00:13:26.560 --> 00:13:28.920]   It's a much better tool for targeted.
[00:13:28.920 --> 00:13:30.440]   That's why the NSA would love this.
[00:13:30.440 --> 00:13:36.000]   Because if you know, if I'm going after Mike Elgin, pardon this long explanation, but it's
[00:13:36.000 --> 00:13:37.000]   complicated.
[00:13:37.000 --> 00:13:38.240]   I think it's important.
[00:13:38.240 --> 00:13:41.000]   Even the cartoon has to be explained.
[00:13:41.000 --> 00:13:43.720]   So that sounds bad.
[00:13:43.720 --> 00:13:48.280]   Honestly, I've been assuming we were doomed ever since I learned about row hammer.
[00:13:48.280 --> 00:13:52.640]   That was the other timing flaw that did.
[00:13:52.640 --> 00:13:53.760]   What's row hammer?
[00:13:53.760 --> 00:13:58.160]   If you toggle a row of memory cells on and off really fast, you can use electrical interference
[00:13:58.160 --> 00:14:02.760]   to flip nearby bits and do we just suck at computers?
[00:14:02.760 --> 00:14:05.240]   Yep, especially shared ones.
[00:14:05.240 --> 00:14:09.480]   So you're saying the cloud is full of phantom trolleys armed with hammers.
[00:14:09.480 --> 00:14:11.920]   Yes, that's exactly right.
[00:14:11.920 --> 00:14:12.920]   Okay.
[00:14:12.920 --> 00:14:16.080]   I'll just install updates.
[00:14:16.080 --> 00:14:17.080]   Good idea.
[00:14:17.080 --> 00:14:18.080]   That's the bottom line.
[00:14:18.080 --> 00:14:19.080]   Yes.
[00:14:19.080 --> 00:14:20.080]   Update.
[00:14:20.080 --> 00:14:21.080]   Right.
[00:14:21.080 --> 00:14:27.120]   Because everybody, Microsoft, Apple, Qualcomm, even Intel and AMD say we're going to put out
[00:14:27.120 --> 00:14:28.120]   patches.
[00:14:28.120 --> 00:14:31.680]   Those would be the better patches, the patch that microcode level.
[00:14:31.680 --> 00:14:33.280]   But if you're running Windows, it's been patched.
[00:14:33.280 --> 00:14:36.880]   If you're running a recent version of Macintosh has been patched and those patches will continue
[00:14:36.880 --> 00:14:37.880]   to come out.
[00:14:37.880 --> 00:14:41.160]   For instance, the Safari patch, I don't think is out yet, but we'll come in.
[00:14:41.160 --> 00:14:42.760]   And there'll probably be future patches.
[00:14:42.760 --> 00:14:44.360]   There'll be more elegance right now.
[00:14:44.360 --> 00:14:48.240]   One hopes putting band-aids on it and making sure that it doesn't, you know, it can't
[00:14:48.240 --> 00:14:49.240]   just be in it.
[00:14:49.240 --> 00:14:50.840]   That's one issue here, of course.
[00:14:50.840 --> 00:14:54.280]   And the register when it first reported this on Tuesday, it was said everybody's here
[00:14:54.280 --> 00:14:58.520]   on fire because they said you're going to see 5 to 30% performance degradation.
[00:14:58.520 --> 00:15:02.240]   That's because the at least initial patches turn off speculatively.
[00:15:02.240 --> 00:15:03.240]   Exactly.
[00:15:03.240 --> 00:15:04.240]   You just turn that off.
[00:15:04.240 --> 00:15:05.240]   That's what you get.
[00:15:05.240 --> 00:15:08.240]   Specular of execution, by the way, is the official name of Saudi Arabia's death penalty
[00:15:08.240 --> 00:15:09.240]   loss.
[00:15:09.240 --> 00:15:10.240]   That's an entirely different issue.
[00:15:10.240 --> 00:15:11.240]   Yeah.
[00:15:11.240 --> 00:15:12.480]   That's if you just turn that off.
[00:15:12.480 --> 00:15:16.280]   And what they're going to do is over time, and Steve Gibson explained this beautifully,
[00:15:16.280 --> 00:15:21.880]   but they're going to get it back to the performance where there's no difference in performance.
[00:15:21.880 --> 00:15:24.480]   Eventually, I don't know, six months, whatever it is.
[00:15:24.480 --> 00:15:28.440]   Well, Apple, for instance, in its benchmark, said the patch on Safari, it's going to give
[00:15:28.440 --> 00:15:30.880]   you a 1 to 2% performance degradation.
[00:15:30.880 --> 00:15:31.880]   That's not noticeable.
[00:15:31.880 --> 00:15:33.880]   Anything below 5%, you wouldn't.
[00:15:33.880 --> 00:15:36.240]   Nobody will notice it.
[00:15:36.240 --> 00:15:43.160]   So bottom line is you should patch as soon as a patch is available.
[00:15:43.160 --> 00:15:45.880]   There are some complications, however.
[00:15:45.880 --> 00:15:50.640]   For instance, this is very annoying.
[00:15:50.640 --> 00:15:57.080]   If you are running a third party antivirus on Windows, it may be blocking the Microsoft
[00:15:57.080 --> 00:16:03.640]   patch because this is one of the reasons I tell people don't use third party antiviruses.
[00:16:03.640 --> 00:16:10.400]   Antiviruses use unsupported calls to make kernel memory calls.
[00:16:10.400 --> 00:16:13.720]   This would be blocked by the patch.
[00:16:13.720 --> 00:16:15.680]   And it would result in a blue screen of death.
[00:16:15.680 --> 00:16:19.480]   So a lot of people, I've heard from a number of people, said, well, I installed Microsoft's
[00:16:19.480 --> 00:16:21.600]   out of bandwidth patch.
[00:16:21.600 --> 00:16:23.480]   It was late last week.
[00:16:23.480 --> 00:16:25.680]   And I got a blue screen of death immediately.
[00:16:25.680 --> 00:16:28.000]   That's because of your third party antivirus.
[00:16:28.000 --> 00:16:30.600]   And there's a list of third party antiviruses.
[00:16:30.600 --> 00:16:32.720]   This is from ZDNet.
[00:16:32.720 --> 00:16:37.520]   Although the list is being maintained by Kevin Beaumont, a security researcher.
[00:16:37.520 --> 00:16:38.760]   He's got a public spreadsheet.
[00:16:38.760 --> 00:16:39.760]   That would be more up to date.
[00:16:39.760 --> 00:16:41.480]   This goes back to January 5th.
[00:16:41.480 --> 00:16:47.200]   A lot of antiviruses are either have not been fixed or haven't done what Microsoft asked,
[00:16:47.200 --> 00:16:51.040]   which was to change a registry key to signal to Windows update.
[00:16:51.040 --> 00:16:53.080]   OK, it's OK to install now.
[00:16:53.080 --> 00:16:56.400]   So you stay may still not be getting it.
[00:16:56.400 --> 00:16:58.240]   It's kind of a mess.
[00:16:58.240 --> 00:17:03.240]   But I think, and correct me if I'm wrong, I don't think it's anything, anybody should
[00:17:03.240 --> 00:17:04.240]   freak out about it.
[00:17:04.240 --> 00:17:05.400]   It's very hard to use this.
[00:17:05.400 --> 00:17:10.280]   If you do want to freak out about it, here's one reason why you might want to do that.
[00:17:10.280 --> 00:17:16.240]   So up until this point, whether anybody has exploited it is anyone's guess.
[00:17:16.240 --> 00:17:20.120]   And like you've said, it could be a state actor or whatever.
[00:17:20.120 --> 00:17:21.120]   Who knows what's going on?
[00:17:21.120 --> 00:17:22.200]   Could be the NSA.
[00:17:22.200 --> 00:17:28.080]   But now that it's been announced, there's a scramble, no doubt, among hackers to figure
[00:17:28.080 --> 00:17:31.360]   out if and how and when they can exploit this.
[00:17:31.360 --> 00:17:33.200]   So it's a challenge.
[00:17:33.200 --> 00:17:37.560]   And so the industry is quickly patching it and trying to fix it and fix the leaks.
[00:17:37.560 --> 00:17:40.320]   But as you pointed out, some Windows users aren't getting it.
[00:17:40.320 --> 00:17:41.960]   There's going to be different rollouts.
[00:17:41.960 --> 00:17:51.840]   And so there's a new urgency in the hacker malicious actor industrial complex to exploit
[00:17:51.840 --> 00:17:52.840]   this.
[00:17:52.840 --> 00:17:53.840]   Right.
[00:17:53.840 --> 00:17:54.840]   They know now it's there.
[00:17:54.840 --> 00:17:55.840]   Exactly.
[00:17:55.840 --> 00:17:57.320]   Timing Fluzr notoriously difficult, Amy.
[00:17:57.320 --> 00:18:01.000]   I would say to go ahead and freak out about it.
[00:18:01.000 --> 00:18:04.000]   Thanks for calming everything down.
[00:18:04.000 --> 00:18:06.440]   You're going to freak out faction.
[00:18:06.440 --> 00:18:08.240]   But here's why.
[00:18:08.240 --> 00:18:16.160]   If it was the case that, and this is particularly complicated because it's a hardware issue.
[00:18:16.160 --> 00:18:21.760]   And on top of it, it's not a bug that appeared overnight because somebody was trying to sort
[00:18:21.760 --> 00:18:24.760]   of make an update and the process of making the update, they broke something.
[00:18:24.760 --> 00:18:30.880]   This was a hardware flaw that has been inherent in the system that people knew about.
[00:18:30.880 --> 00:18:40.000]   I could say the same of Sony, which is no stranger to problems with hackers and has
[00:18:40.000 --> 00:18:44.640]   a very, very long history of ignoring all of the weak signals.
[00:18:44.640 --> 00:18:48.000]   And Sony would be learning something is wrong.
[00:18:48.000 --> 00:18:51.840]   Their attack was a targeted attack that would be particularly vulnerable to this kind of
[00:18:51.840 --> 00:18:52.840]   vulnerability.
[00:18:52.840 --> 00:18:57.640]   Well, the point that I'm making is that the part that people should be freaking out about
[00:18:57.640 --> 00:19:03.960]   is that as technology becomes more complicated, it's going to start breaking in sort of strange
[00:19:03.960 --> 00:19:09.320]   ways, which means that companies need to be ever more vigilant about forecasting out
[00:19:09.320 --> 00:19:15.560]   in advance and pressure testing systems for things like timing flaws, which somebody somewhere
[00:19:15.560 --> 00:19:20.440]   in the organization I have to have thought, they had to have raised that issue at some
[00:19:20.440 --> 00:19:21.440]   point.
[00:19:21.440 --> 00:19:26.600]   It's not like this could not have come as a giant surprise.
[00:19:26.600 --> 00:19:33.040]   But I might even argue that it was intentional and the consequences wound up being worse than
[00:19:33.040 --> 00:19:35.960]   maybe the risk management people might have thought in advance.
[00:19:35.960 --> 00:19:40.960]   So the reason to sort of freak out is because we think that once we purchase technology,
[00:19:40.960 --> 00:19:43.400]   it's ours and we have agency and domain over it.
[00:19:43.400 --> 00:19:47.400]   This is yet another example that that's not the case.
[00:19:47.400 --> 00:19:51.200]   And I know that these glitches and these hacks are getting more and more complicated for the
[00:19:51.200 --> 00:19:53.200]   average person to understand.
[00:19:53.200 --> 00:20:00.520]   However, it's incumbent upon us to keep state focused and to ask for some serious answers.
[00:20:00.520 --> 00:20:04.120]   Well, Intel, there are several, of course, class action lawsuits.
[00:20:04.120 --> 00:20:05.600]   That's not really probative.
[00:20:05.600 --> 00:20:11.280]   But there is some speculation that maybe this is a janky way to do it and Intel should
[00:20:11.280 --> 00:20:15.440]   be faulted for choosing this way to speed up their processors.
[00:20:15.440 --> 00:20:21.600]   This was like, I don't know, I'm not educated enough to say.
[00:20:21.600 --> 00:20:25.840]   And I'm not a, you know, I'm not a hardware person.
[00:20:25.840 --> 00:20:31.880]   You'd have to have chip designer to really understand if it's just not like this was
[00:20:31.880 --> 00:20:36.840]   a brand new design and they were trying to do something totally like radically different,
[00:20:36.840 --> 00:20:38.520]   you know, over the last design cycle.
[00:20:38.520 --> 00:20:40.800]   I mean, this is going back to me.
[00:20:40.800 --> 00:20:41.800]   Actually, you're right.
[00:20:41.800 --> 00:20:42.800]   Ninety five, this was a problem.
[00:20:42.800 --> 00:20:48.680]   But remember a little bit later, Intel's lunch is starting to get eaten by AMD.
[00:20:48.680 --> 00:20:50.960]   Intel went down a bad road with the I titanium.
[00:20:50.960 --> 00:20:54.200]   They suddenly realized you can't get faster than 4 gigahertz.
[00:20:54.200 --> 00:20:56.480]   They're just becomes unreliable.
[00:20:56.480 --> 00:21:02.640]   They were up against maybe not by 1995, but quickly thereafter up against a wall of performance.
[00:21:02.640 --> 00:21:05.680]   And there was huge demand for more performance.
[00:21:05.680 --> 00:21:10.600]   And I, as I remember it, remember Intel happened to have this skunk works project in Israel
[00:21:10.600 --> 00:21:15.880]   that they, when I tanium failed, they went to that we're doing things like branch prediction
[00:21:15.880 --> 00:21:21.320]   and speculative execution to get more performance out of the x86 platform.
[00:21:21.320 --> 00:21:26.760]   They were, they wanted to move away from it and they chose to go with x86.
[00:21:26.760 --> 00:21:33.000]   Perhaps could fault them for making a poor design decision, but on the other hand, it's
[00:21:33.000 --> 00:21:36.080]   been 22 years before we found the problem.
[00:21:36.080 --> 00:21:37.080]   Right.
[00:21:37.080 --> 00:21:42.240]   And so again, I come back to, I hear from a lot of people, this has been around for 22
[00:21:42.240 --> 00:21:46.280]   years, it's impossible that it's just now somebody figured this out.
[00:21:46.280 --> 00:21:48.040]   I wouldn't worry about it.
[00:21:48.040 --> 00:21:49.040]   Right.
[00:21:49.040 --> 00:21:50.680]   It's something horrible hasn't happened yet.
[00:21:50.680 --> 00:21:52.520]   You know, what's the, what's the probability?
[00:21:52.520 --> 00:22:00.920]   And again, to me, this is yet another example of organizations making sacrifices for whatever
[00:22:00.920 --> 00:22:06.400]   gain that put consumers in harm's way.
[00:22:06.400 --> 00:22:14.960]   Yeah, but how, I mean, this is a really good example of something so technical that even
[00:22:14.960 --> 00:22:21.200]   we as people who cover tech have a hard time wrapping our head around it.
[00:22:21.200 --> 00:22:24.960]   Steve Gibson, we talked about this on Tuesday and even on Tuesday, Steve Gibson was, you
[00:22:24.960 --> 00:22:28.320]   know, and nobody hears this other problem might cause a slowdown.
[00:22:28.320 --> 00:22:33.440]   We, nobody really understood the impact of this for a while and it took a lot of minds
[00:22:33.440 --> 00:22:40.160]   thinking about it. how do you then explain this to regular people and CEOs and other people?
[00:22:40.160 --> 00:22:44.240]   I mean, how do you, this is a, this is the world we're living in.
[00:22:44.240 --> 00:22:48.840]   In fact, I remember Jerry, the late Jerry Purnell writing, I think it was in Lucifer's
[00:22:48.840 --> 00:22:50.160]   hammer.
[00:22:50.160 --> 00:22:53.840]   And this was many years ago, he said, we were already in a world where most of the technology
[00:22:53.840 --> 00:22:56.840]   people use, they don't understand, they don't know how it's made.
[00:22:56.840 --> 00:22:59.360]   You could know how an internal combustion engine worked.
[00:22:59.360 --> 00:23:02.400]   Good luck explaining how a computer works.
[00:23:02.400 --> 00:23:07.320]   And if a catastrophic event were to happen like an asteroid hitting the earth as it does
[00:23:07.320 --> 00:23:10.040]   in the book, we wouldn't be able to recreate it.
[00:23:10.040 --> 00:23:13.520]   Nobody would know how it was made in the first place.
[00:23:13.520 --> 00:23:18.080]   We're using stuff that is effectively magical.
[00:23:18.080 --> 00:23:20.800]   And now we're asking the wizards who created the magic.
[00:23:20.800 --> 00:23:27.560]   Hey guys, don't screw up, but we have no way to test that and just wait until the AI is
[00:23:27.560 --> 00:23:28.560]   creating the AI.
[00:23:28.560 --> 00:23:30.040]   Well, that's just a matter of time.
[00:23:30.040 --> 00:23:32.720]   How do we then it's way out of our control?
[00:23:32.720 --> 00:23:35.440]   But that's already, so, but that's kind of my point, right?
[00:23:35.440 --> 00:23:40.480]   Because we're already, so I will preface this by saying there's a lot of misplaced optimism
[00:23:40.480 --> 00:23:43.320]   and fear when it comes to your writing the book.
[00:23:43.320 --> 00:23:44.320]   You're writing the book?
[00:23:44.320 --> 00:23:45.320]   I'm writing the book about that.
[00:23:45.320 --> 00:23:46.320]   Yes.
[00:23:46.320 --> 00:23:53.840]   So, but we can't submit ourselves every time we butt up against a rough problem and sort
[00:23:53.840 --> 00:23:56.400]   of just say, well, this is like a black box.
[00:23:56.400 --> 00:23:59.800]   If I had a nickel for every time I've heard somebody describe a complicated technology
[00:23:59.800 --> 00:24:04.200]   as a black box that we just can't understand, we shouldn't be in a position where we're
[00:24:04.200 --> 00:24:11.920]   creating technology that we can't understand let alone can't explain, right?
[00:24:11.920 --> 00:24:16.040]   And there's a lot of that.
[00:24:16.040 --> 00:24:21.040]   So that either tells me that humans have just gotten lazy and we don't want to take the
[00:24:21.040 --> 00:24:22.040]   time to understand.
[00:24:22.040 --> 00:24:28.280]   I mean, obviously technology has gotten a lot more complicated, but we should not be in
[00:24:28.280 --> 00:24:35.080]   a situation where we are entirely reliant on tools and services and code and devices
[00:24:35.080 --> 00:24:43.400]   that we can't in any meaningful way understand how it works, right?
[00:24:43.400 --> 00:24:45.640]   >> From a journalistic standpoint.
[00:24:45.640 --> 00:24:47.640]   >> That is hard.
[00:24:47.640 --> 00:24:49.040]   >> Yeah, exactly.
[00:24:49.040 --> 00:24:53.480]   >> That's my brain.
[00:24:53.480 --> 00:24:58.480]   >> From a journalistic standpoint, I think that this is another one of those challenges
[00:24:58.480 --> 00:24:59.760]   on multiple levels.
[00:24:59.760 --> 00:25:07.480]   One, because yes, we're talking about trying to explain this to I go back to normal people
[00:25:07.480 --> 00:25:12.040]   who, people who aren't steeped in technology like we are.
[00:25:12.040 --> 00:25:18.200]   And when we try to explain this, a lot of times it's like people maybe necessarily who
[00:25:18.200 --> 00:25:22.680]   aren't interested in hearing about it and are just like, you know, they'll figure it
[00:25:22.680 --> 00:25:23.680]   out.
[00:25:23.680 --> 00:25:29.120]   So it does sort of go back to that apathy, I think, about these magical boxes that we
[00:25:29.120 --> 00:25:30.320]   don't understand how they work.
[00:25:30.320 --> 00:25:35.880]   And so we're apathetic to the problems because maybe we've never experienced them before ourselves.
[00:25:35.880 --> 00:25:38.760]   And so there are two things that become an issue.
[00:25:38.760 --> 00:25:41.000]   One is just explaining it in general.
[00:25:41.000 --> 00:25:47.860]   But two is like keeping control of the message or making sure that the right message gets
[00:25:47.860 --> 00:25:54.580]   out there because inevitably how these things play out is it gets to the people that know
[00:25:54.580 --> 00:25:56.340]   more about this stuff.
[00:25:56.340 --> 00:26:00.300]   And they write their articles, they write their explanations, they put up their blog
[00:26:00.300 --> 00:26:04.140]   posts, they make comics, what have you.
[00:26:04.140 --> 00:26:09.740]   And then it trickles down to more mainstream, it's like secondary, and then the main mainstream,
[00:26:09.740 --> 00:26:12.680]   it gets on local news outlets and things like that.
[00:26:12.680 --> 00:26:21.400]   And by the time it gets there, it's been bastardized so much and sort of translated so much that
[00:26:21.400 --> 00:26:26.560]   oftentimes there's so much inaccuracy because of the vagueness of it.
[00:26:26.560 --> 00:26:31.760]   And so by the time it gets to a family member of mine who is not as steeped in technology
[00:26:31.760 --> 00:26:36.040]   as I am, they're hearing a whole different story and it's the worst possible thing.
[00:26:36.040 --> 00:26:41.560]   And you add on top of that the fact that all of these news outlets are scrambling for views,
[00:26:41.560 --> 00:26:46.120]   they're scrambling for clicks, they're scrambling for page views and things like that.
[00:26:46.120 --> 00:26:51.800]   And you have headlines that tell incorrect stories or don't tell the whole story.
[00:26:51.800 --> 00:26:56.480]   And a lot of times we know, like we can tell from all of these different things that track
[00:26:56.480 --> 00:27:02.840]   our usage and our activity on pages, that people oftentimes don't go through and read
[00:27:02.840 --> 00:27:08.880]   the article, they read headlines, they read Twitter sized bits of information.
[00:27:08.880 --> 00:27:14.080]   And so what we're doing is spreading misinformation and it's so hard to get caught up with that.
[00:27:14.080 --> 00:27:19.880]   And even if we do sort of come back and say, okay, you've heard this, but it's not exactly
[00:27:19.880 --> 00:27:24.280]   this, you try to explain what it is, those articles get far less views than any of the
[00:27:24.280 --> 00:27:28.640]   articles that originally went out there and said, this is the end of the world.
[00:27:28.640 --> 00:27:29.640]   Everything's melting down.
[00:27:29.640 --> 00:27:31.760]   So it's a problem.
[00:27:31.760 --> 00:27:32.760]   Like there's a problem.
[00:27:32.760 --> 00:27:36.240]   >> Your computer is a time bomb, film of 11.
[00:27:36.240 --> 00:27:39.880]   >> But it is the world we live in where you want to be, we were talking about this before
[00:27:39.880 --> 00:27:44.320]   the show, you want to be first, you want to, the only way to get the links is to be sensational
[00:27:44.320 --> 00:27:45.640]   and fast.
[00:27:45.640 --> 00:27:47.520]   And that's the worst situation.
[00:27:47.520 --> 00:27:52.240]   But I think both Amy and Michael are saying what I would completely agree with, what is
[00:27:52.240 --> 00:28:01.560]   ultimately, this is the job of journalism is to do the work to understand the tough stuff
[00:28:01.560 --> 00:28:06.840]   whether it's tech or government or whatever and explain it so that we can have an informed
[00:28:06.840 --> 00:28:07.840]   populace, right?
[00:28:07.840 --> 00:28:08.840]   That's the job.
[00:28:08.840 --> 00:28:12.640]   >> Well, they're journalists, they're reporters and they're editorialists like myself.
[00:28:12.640 --> 00:28:16.920]   And so one of the things that I would say is it's probably less important, I mean, the
[00:28:16.920 --> 00:28:18.800]   public is never going to understand this stuff.
[00:28:18.800 --> 00:28:21.920]   The public can't locate Australia on a map, right?
[00:28:21.920 --> 00:28:24.880]   So the public is never going to understand this kind of security.
[00:28:24.880 --> 00:28:30.200]   And it's far more important that they have a sort of a behavioral operating system for
[00:28:30.200 --> 00:28:31.760]   coping with this stuff.
[00:28:31.760 --> 00:28:33.400]   And here's why.
[00:28:33.400 --> 00:28:37.160]   Every time we learn about these things, it's already been out there forever.
[00:28:37.160 --> 00:28:40.400]   I mean, this one's been out there for two decades.
[00:28:40.400 --> 00:28:43.200]   So it's like, it doesn't matter what you know now.
[00:28:43.200 --> 00:28:45.680]   It matters what you've been doing for two decades.
[00:28:45.680 --> 00:28:53.840]   So we need to figure out how to instruct the public so that they can protect their data
[00:28:53.840 --> 00:28:58.000]   because we're always shocked and surprised by all the different creative ways that our
[00:28:58.000 --> 00:29:01.280]   information can be stolen from us.
[00:29:01.280 --> 00:29:05.400]   Every year, there's two or three major stories that it's like, I never even thought that
[00:29:05.400 --> 00:29:10.480]   was a way that people could harvest all my passwords.
[00:29:10.480 --> 00:29:17.000]   Third party cloud services are routinely hacked and all this stuff downloaded and so on.
[00:29:17.000 --> 00:29:23.480]   And so there needs to be a much greater emphasis on providing the public with an all-purpose
[00:29:23.480 --> 00:29:28.640]   set of behaviors and tools that will allow them to keep their data safe.
[00:29:28.640 --> 00:29:30.200]   No matter what is that possible?
[00:29:30.200 --> 00:29:31.200]   It's possible.
[00:29:31.200 --> 00:29:32.200]   Okay.
[00:29:32.200 --> 00:29:35.200]   It's possible to do the best we can with that.
[00:29:35.200 --> 00:29:37.200]   So right now there's certainly update.
[00:29:37.200 --> 00:29:38.200]   Yeah.
[00:29:38.200 --> 00:29:39.200]   Update.
[00:29:39.200 --> 00:29:40.200]   Back up in the cloud.
[00:29:40.200 --> 00:29:41.200]   Pass word.
[00:29:41.200 --> 00:29:42.200]   All that stuff.
[00:29:42.200 --> 00:29:43.200]   Amy doesn't seem convinced.
[00:29:43.200 --> 00:29:48.480]   Amy, you read the article in the Atlantic in September, the coming software apocalypse.
[00:29:48.480 --> 00:29:49.480]   Yeah.
[00:29:49.480 --> 00:29:50.480]   Yeah.
[00:29:50.480 --> 00:29:52.960]   I feel like sometimes I feel like we're at the top of a house of cards and it's starting
[00:29:52.960 --> 00:29:53.960]   the teeter.
[00:29:53.960 --> 00:29:54.960]   Yeah.
[00:29:54.960 --> 00:29:57.520]   So a couple of things.
[00:29:57.520 --> 00:30:05.840]   We already have the tools to protect ourselves in a reasonable way against threats and hacks.
[00:30:05.840 --> 00:30:11.320]   You know, I mean, I would walk out in the street right now and just ask random people
[00:30:11.320 --> 00:30:16.640]   when the last time was that they updated their, you know, OS, right?
[00:30:16.640 --> 00:30:17.640]   Updated the router settings.
[00:30:17.640 --> 00:30:19.520]   I mean, like updated everything.
[00:30:19.520 --> 00:30:24.920]   People just don't do it for any number of different reasons.
[00:30:24.920 --> 00:30:26.560]   You know, I don't know.
[00:30:26.560 --> 00:30:33.000]   And I would also just, I mean, it's plausible to give people the tool set, but it's improbable
[00:30:33.000 --> 00:30:39.720]   that, you know, the everyday person is going to see the urgency and keeping themselves
[00:30:39.720 --> 00:30:40.720]   constantly safe.
[00:30:40.720 --> 00:30:42.360]   I mean, it's replicate.
[00:30:42.360 --> 00:30:47.200]   This is how this is why companies don't, a lot of companies don't participate in strategic
[00:30:47.200 --> 00:30:52.560]   foresight and they don't do the work of a futurist because until something happens,
[00:30:52.560 --> 00:30:53.960]   they don't see the value in it.
[00:30:53.960 --> 00:30:56.600]   They want to be reactive every, like Sony.
[00:30:56.600 --> 00:30:58.400]   Well, and that's right.
[00:30:58.400 --> 00:31:03.200]   So, you know, for Sony to have pre, for Sony to have completely changed what it was doing,
[00:31:03.200 --> 00:31:08.760]   the net, the many, many, many times that it got hacked would have been an investment for
[00:31:08.760 --> 00:31:13.560]   everyday people for everyday people to do the type of safeguarding that we're talking
[00:31:13.560 --> 00:31:18.080]   about, which isn't just running patches and updates, but also paying attention to the
[00:31:18.080 --> 00:31:19.080]   news.
[00:31:19.080 --> 00:31:23.520]   The Secret Service told me this years ago, banks would rather pay the losses than fix
[00:31:23.520 --> 00:31:24.520]   the system.
[00:31:24.520 --> 00:31:25.520]   That's right.
[00:31:25.520 --> 00:31:30.080]   So we're talking about an investment in time and money that companies which have a financial
[00:31:30.080 --> 00:31:32.320]   stake in all of this aren't willing to make.
[00:31:32.320 --> 00:31:36.840]   You know, I just don't see everyday people being willing to do that either.
[00:31:36.840 --> 00:31:41.640]   Now, I will say that this, you know, I'm a quantitative futurist and this has been my
[00:31:41.640 --> 00:31:44.080]   job for 15 years, but I had a first career.
[00:31:44.080 --> 00:31:46.280]   I was a journalist.
[00:31:46.280 --> 00:31:50.200]   I was a correspondent and I worked at the Wall Street Journal and at Newsweek.
[00:31:50.200 --> 00:31:52.280]   That's why I lived in Japan and China.
[00:31:52.280 --> 00:31:55.520]   And I went to the Columbia University Graduate School of Journalism.
[00:31:55.520 --> 00:32:05.160]   So, you know, the comments about journalists exploiting tech news for traffic gain, I
[00:32:05.160 --> 00:32:08.120]   think is some of that may be true.
[00:32:08.120 --> 00:32:14.680]   And we certainly live in a digital economy where attention is currency, unfortunately.
[00:32:14.680 --> 00:32:19.560]   But just because you produce, like, just because, you know, Brandele Monroe creates a really
[00:32:19.560 --> 00:32:25.560]   great XKCD that explains this very complicated problem doesn't mean that my dad, you know,
[00:32:25.560 --> 00:32:28.880]   is reading it and it doesn't mean that my dad will read it even if I send it to him,
[00:32:28.880 --> 00:32:29.880]   right?
[00:32:29.880 --> 00:32:34.720]   So part of the problem is, again, we're quick to blame institutions and we're, you know,
[00:32:34.720 --> 00:32:37.160]   we're quick to blame journalists lately.
[00:32:37.160 --> 00:32:41.480]   We are reluctant to blame ourselves in being complicit in some of this.
[00:32:41.480 --> 00:32:45.520]   But I wouldn't expect your dad to learn what meltdown inspector do.
[00:32:45.520 --> 00:32:49.320]   I would expect him to run updates and he doesn't.
[00:32:49.320 --> 00:32:50.320]   Right.
[00:32:50.320 --> 00:32:53.760]   You know, I mean, and, and, you know, I have it scheduled.
[00:32:53.760 --> 00:32:57.880]   I don't run automatic updates because sometimes, you know, the update isn't the right one to
[00:32:57.880 --> 00:32:59.320]   run at that time.
[00:32:59.320 --> 00:33:03.800]   But I have, you know, I have a very methodical way of constantly updating all my stuff, you
[00:33:03.800 --> 00:33:07.680]   know, and, and I have secondary and tertiary checks running on different things.
[00:33:07.680 --> 00:33:09.280]   The average person's not going to do that.
[00:33:09.280 --> 00:33:16.520]   But like my dad can't wait for me to come home to visit him, you know, at that point,
[00:33:16.520 --> 00:33:17.960]   you all the tech support.
[00:33:17.960 --> 00:33:23.280]   I mean, you know, then, and then six hours a week on the radio talking to normal people
[00:33:23.280 --> 00:33:25.960]   trying to convince them to do this, but you're absolutely right.
[00:33:25.960 --> 00:33:30.440]   But until something happens to them, they don't see the, you know, they won't, but that's
[00:33:30.440 --> 00:33:31.440]   the problem.
[00:33:31.440 --> 00:33:35.680]   So, I love the idea of having some kind of digital.
[00:33:35.680 --> 00:33:38.280]   Uh oh, we lost your audio.
[00:33:38.280 --> 00:33:40.360]   Is that us or is that Amy?
[00:33:40.360 --> 00:33:41.840]   We'll call you right back, Amy.
[00:33:41.840 --> 00:33:42.840]   Oh, wait, wait.
[00:33:42.840 --> 00:33:44.640]   You said that was not the best factor.
[00:33:44.640 --> 00:33:45.640]   That was elbow.
[00:33:45.640 --> 00:33:48.800]   Elbow and I know it happens all the time.
[00:33:48.800 --> 00:33:49.800]   Yep.
[00:33:49.800 --> 00:33:50.800]   No, I was.
[00:33:50.800 --> 00:33:54.400]   You were saying about how we can't control our technology.
[00:33:54.400 --> 00:33:56.440]   No, going forward.
[00:33:56.440 --> 00:33:57.440]   Well, yeah.
[00:33:57.440 --> 00:34:03.760]   The irony is not lost, but I was fired up.
[00:34:03.760 --> 00:34:09.120]   Going forward, we all have to develop a different kind of digital street smarts that is, is,
[00:34:09.120 --> 00:34:14.160]   you know, unlike, I think in the past, we're just surrounded by more and more complicated
[00:34:14.160 --> 00:34:15.160]   technologies.
[00:34:15.160 --> 00:34:18.440]   And so this is the kind of thing we're not going to pick up on our own.
[00:34:18.440 --> 00:34:20.840]   So who do you say is responsible?
[00:34:20.840 --> 00:34:26.280]   I mean, our, our, our, our is like the every man, is your dad, is it up to him?
[00:34:26.280 --> 00:34:27.640]   Because I need to do that.
[00:34:27.640 --> 00:34:35.200]   Well, so I mean, yeah, I would say part because, you know, we're now all connected, right?
[00:34:35.200 --> 00:34:37.520]   So this isn't an individual's problem.
[00:34:37.520 --> 00:34:38.520]   It's not just him.
[00:34:38.520 --> 00:34:39.520]   Yeah.
[00:34:39.520 --> 00:34:40.520]   No, it's all of us.
[00:34:40.520 --> 00:34:45.760]   And to some extent, you know, I love my father, but my dad, like, jacking around and not updating
[00:34:45.760 --> 00:34:52.520]   his iPad for whatever reason, ultimately, in some way impacts me because when we have
[00:34:52.520 --> 00:34:58.120]   a network of devices that are vulnerable in some way, it makes it easier for people to
[00:34:58.120 --> 00:35:01.800]   find, you know, the big payoffs, the big zero days, right?
[00:35:01.800 --> 00:35:07.800]   Like the big zero days that everybody that everybody's looking for, you know, anyhow,
[00:35:07.800 --> 00:35:08.800]   just, just makes it.
[00:35:08.800 --> 00:35:12.720]   And what we're doing now, what we, what last year has been about and certainly this year
[00:35:12.720 --> 00:35:13.720]   will be bad.
[00:35:13.720 --> 00:35:18.040]   And we'll find out about it at CES is installing a bunch of internet connected dumb devices
[00:35:18.040 --> 00:35:24.680]   in our houses that are, in many cases, not updateable and are not properly configured.
[00:35:24.680 --> 00:35:29.360]   And just, you know, trusting to luck that they don't, again, you make the excellent
[00:35:29.360 --> 00:35:34.320]   point, Amy, it's not about protecting us as an individual is protecting the ecosystem.
[00:35:34.320 --> 00:35:39.200]   And we haven't, we're talking about a chip, like a chipset.
[00:35:39.200 --> 00:35:43.040]   We haven't even like, we have devices now that listen to us.
[00:35:43.040 --> 00:35:46.760]   So all of these digital assistants, I'm just waiting for the first exploit to come down
[00:35:46.760 --> 00:35:48.520]   the pipeline, because you know it is, right?
[00:35:48.520 --> 00:35:54.320]   At some point, it's sort of herd immunity and we're definitely failing as that said
[00:35:54.320 --> 00:35:55.320]   herd immunity.
[00:35:55.320 --> 00:35:56.720]   I don't see an easy answer.
[00:35:56.720 --> 00:36:03.520]   I mean, again, I would go back to the job of journalism is so important these days.
[00:36:03.520 --> 00:36:08.200]   It's a shame that it's being dragged through the mud.
[00:36:08.200 --> 00:36:14.440]   But this is an example of, I would, I would say, Amy, that if your dad doesn't need to
[00:36:14.440 --> 00:36:18.400]   understand it, but if your dad were writing for the Wall Street Journal, it would behoove
[00:36:18.400 --> 00:36:23.360]   him to talk to experts, find out as much as he can and do the best job he can explaining
[00:36:23.360 --> 00:36:27.000]   it to people who need to notice information.
[00:36:27.000 --> 00:36:28.920]   I mean, the easiest way to do it.
[00:36:28.920 --> 00:36:29.920]   Here's this.
[00:36:29.920 --> 00:36:33.200]   I think we just, I think the four of us actually just came up with the solution.
[00:36:33.200 --> 00:36:41.040]   And the solution is we need some kind of, no, no, no, no, no, here's the solution because
[00:36:41.040 --> 00:36:43.600]   I think we need like a CDC for.
[00:36:43.600 --> 00:36:44.600]   Oh, I love it.
[00:36:44.600 --> 00:36:45.600]   There we go.
[00:36:45.600 --> 00:36:46.600]   You're right.
[00:36:46.600 --> 00:36:49.920]   We need, we need a, you know, the Centers of Disease Control, if you stop and think
[00:36:49.920 --> 00:36:53.760]   about sort of their mandate and what they do and where they are and how they're funded,
[00:36:53.760 --> 00:36:59.640]   a lot of what they do is try to predict and manage epidemics to manage the overall health
[00:36:59.640 --> 00:37:02.600]   of people living in our country.
[00:37:02.600 --> 00:37:09.440]   I think that we're probably due for some kind of CDC that does exactly what we were talking
[00:37:09.440 --> 00:37:15.960]   about moments ago, you know, which is like, they're kind of is.
[00:37:15.960 --> 00:37:16.960]   They're kind of is.
[00:37:16.960 --> 00:37:19.360]   I'm not saying it does a good job.
[00:37:19.360 --> 00:37:20.920]   No, I know.
[00:37:20.920 --> 00:37:25.640]   But there's the US computer emergency readiness team, which is part of the Department of Homeland
[00:37:25.640 --> 00:37:26.640]   Security.
[00:37:26.640 --> 00:37:28.920]   That's kind of what it's supposed to be.
[00:37:28.920 --> 00:37:36.920]   Yeah, but I, because I've looked at their work before and I just, it's not, it's, it's
[00:37:36.920 --> 00:37:41.080]   not the same. so Mike made a very, very good point, which was that we need to position
[00:37:41.080 --> 00:37:47.000]   ourselves so that everyday people have tools and on that website, if you show it again,
[00:37:47.000 --> 00:37:50.480]   you can follow it and on the site they do every now and then list, right.
[00:37:50.480 --> 00:37:54.600]   So the current activity and they've got that alerts and tips.
[00:37:54.600 --> 00:38:00.160]   They do have like PDFs and things that you can download, but you know, it doesn't, we
[00:38:00.160 --> 00:38:08.000]   need a CDC level kind of sort of institution that has the press savvy, you know, that can,
[00:38:08.000 --> 00:38:10.480]   that can help educate the public.
[00:38:10.480 --> 00:38:12.000]   That's such a good point.
[00:38:12.000 --> 00:38:14.880]   CDC has been a huge boon, I think, to public health.
[00:38:14.880 --> 00:38:15.880]   Sure.
[00:38:15.880 --> 00:38:16.880]   That's right.
[00:38:16.880 --> 00:38:24.240]   And I would include digital, our digital ecosystem and how we behave inside of it and
[00:38:24.240 --> 00:38:29.480]   how we are affected by it as part of our overall system of public health, right?
[00:38:29.480 --> 00:38:30.480]   It is.
[00:38:30.480 --> 00:38:31.480]   Yeah.
[00:38:31.480 --> 00:38:33.880]   I mean, you know, I was just, this is a little off topic, but I was just thinking about
[00:38:33.880 --> 00:38:35.480]   this the other day.
[00:38:35.480 --> 00:38:39.840]   There's so many apps these days where one of the first things that asks you is, hey,
[00:38:39.840 --> 00:38:43.240]   if you want to get in touch with all of your friends, go ahead and click this button and
[00:38:43.240 --> 00:38:46.800]   we'll make sure that if your friends are on this app, then you can use it too.
[00:38:46.800 --> 00:38:52.320]   And then you upload your entire database of contact information to their database, your
[00:38:52.320 --> 00:38:56.280]   entire library of contact information.
[00:38:56.280 --> 00:39:01.440]   And it's just, it's a small thing, but it's essentially sort of breaking the trust of
[00:39:01.440 --> 00:39:05.720]   everybody who trusted you with their information, who trusted you with their phone number, their
[00:39:05.720 --> 00:39:10.040]   email, their address, whatever ends up getting updated to those clouds.
[00:39:10.040 --> 00:39:11.040]   And we all, yeah, we all do.
[00:39:11.040 --> 00:39:13.280]   It's like the office that have an STD alert.
[00:39:13.280 --> 00:39:14.280]   Yeah.
[00:39:14.280 --> 00:39:15.280]   Yeah.
[00:39:15.280 --> 00:39:16.280]   Yeah.
[00:39:16.280 --> 00:39:17.280]   Yeah.
[00:39:17.280 --> 00:39:23.840]   I mean, and to me, the most outrageous and underappreciated example of that is Google Photos, which I
[00:39:23.840 --> 00:39:28.520]   love, absolutely love Google Photos, but Google Photos, I can take pictures of all kinds of
[00:39:28.520 --> 00:39:31.640]   people, strangers, anybody and uploads of Google Photos.
[00:39:31.640 --> 00:39:32.640]   It identifies them.
[00:39:32.640 --> 00:39:34.160]   And then it asks me to say who they are.
[00:39:34.160 --> 00:39:36.200]   So I'm basically saying, oh, that's the base.
[00:39:36.200 --> 00:39:37.200]   Yeah.
[00:39:37.200 --> 00:39:38.200]   I didn't ask you permission.
[00:39:38.200 --> 00:39:40.200]   They didn't ask you permission, but Facebook's even worse than.
[00:39:40.200 --> 00:39:41.200]   Oh, yeah.
[00:39:41.200 --> 00:39:44.760]   And part of the problem is Facebook, for example, leaks information in ways that the user doesn't
[00:39:44.760 --> 00:39:45.760]   know.
[00:39:45.760 --> 00:39:46.760]   Yeah.
[00:39:46.760 --> 00:39:51.000]   So there's a friend-to-friend problem on Facebook, which is if you take a Facebook quiz, which
[00:39:51.000 --> 00:39:53.800]   isn't really a quiz, I know you really want to know which Game of Thrones.
[00:39:53.800 --> 00:39:57.320]   Game of Thrones character you are, but really, really what's happening is you're giving
[00:39:57.320 --> 00:40:02.360]   a lot of information to a third party, like let's say Cambridge Analytica.
[00:40:02.360 --> 00:40:07.200]   But worse than that, as soon as you agree to that, they're getting access not to your
[00:40:07.200 --> 00:40:10.200]   information, but your friend's information.
[00:40:10.200 --> 00:40:13.800]   So you're leaking, and I don't think people know that, and you're leaking your friend's
[00:40:13.800 --> 00:40:14.800]   information.
[00:40:14.800 --> 00:40:17.640]   You probably, if you knew that, might not even do it, but nobody knows.
[00:40:17.640 --> 00:40:18.640]   Right.
[00:40:18.640 --> 00:40:22.160]   So again, so now let's just take that entire story, Leo, that you just told.
[00:40:22.160 --> 00:40:28.080]   Instead of photos and instead of face of Game of Thrones and a game, let's say that,
[00:40:28.080 --> 00:40:30.800]   because you're describing a communicable disease.
[00:40:30.800 --> 00:40:31.800]   Yes.
[00:40:31.800 --> 00:40:32.800]   Right.
[00:40:32.800 --> 00:40:33.800]   So, so.
[00:40:33.800 --> 00:40:36.400]   Which everybody seems to want to get.
[00:40:36.400 --> 00:40:37.400]   It's worse.
[00:40:37.400 --> 00:40:40.400]   It's like, give me smallpox.
[00:40:40.400 --> 00:40:46.840]   If this was the CDC, and if we use the same vocabulary, because part of the problem is
[00:40:46.840 --> 00:40:53.160]   currently, like everyday people lack the analogies to understand the impact of these
[00:40:53.160 --> 00:40:54.200]   systems and tools.
[00:40:54.200 --> 00:40:59.360]   So you could describe what exactly what you just said and replace Facebook with smallpox,
[00:40:59.360 --> 00:41:04.600]   and people would immediately understand that, you know, this is something that they should
[00:41:04.600 --> 00:41:05.600]   be paying attention to.
[00:41:05.600 --> 00:41:07.600]   We have the shared vocabulary.
[00:41:07.600 --> 00:41:10.680]   And I'm not comparing, I'm not saying that Facebook is smallpox.
[00:41:10.680 --> 00:41:11.680]   I am.
[00:41:11.680 --> 00:41:12.680]   I'm not.
[00:41:12.680 --> 00:41:13.680]   I am.
[00:41:13.680 --> 00:41:14.680]   There's a headline.
[00:41:14.680 --> 00:41:15.680]   They should be smallpox.
[00:41:15.680 --> 00:41:16.680]   I am.
[00:41:16.680 --> 00:41:19.120]   Actually, if you look at what sort does.
[00:41:19.120 --> 00:41:20.120]   That's what we're looking for.
[00:41:20.120 --> 00:41:21.120]   That's what we're looking for.
[00:41:21.120 --> 00:41:26.280]   If you look at what CERT does, I'm looking at the CERT alert for Meltdown Inspector.
[00:41:26.280 --> 00:41:27.440]   And it's actually quite good.
[00:41:27.440 --> 00:41:29.120]   And they put it out on January 3rd.
[00:41:29.120 --> 00:41:30.920]   They updated it January 4th.
[00:41:30.920 --> 00:41:32.080]   They updated it again on Friday.
[00:41:32.080 --> 00:41:33.920]   Apparently, they went home for the weekend.
[00:41:33.920 --> 00:41:34.920]   That's too bad.
[00:41:34.920 --> 00:41:36.600]   But so they're underfunded.
[00:41:36.600 --> 00:41:41.000]   But it's pretty good with links to vendors and vendor mitigations.
[00:41:41.000 --> 00:41:45.840]   Lots of additional information, including the original research.
[00:41:45.840 --> 00:41:51.080]   They refer to the United Kingdom, which has a nice national cybersecurity center.
[00:41:51.080 --> 00:41:55.720]   So they're working with other governmental similar governmental agencies.
[00:41:55.720 --> 00:41:56.720]   This is right.
[00:41:56.720 --> 00:41:59.920]   But what they what they lack is the one thing I think you're right that they need to do,
[00:41:59.920 --> 00:42:02.600]   which is a better public facing presence.
[00:42:02.600 --> 00:42:05.080]   And yes, and it's not written.
[00:42:05.080 --> 00:42:08.840]   You assert is, you know, if you have some of the language and you're a little bit more
[00:42:08.840 --> 00:42:12.440]   technically inclined, it's a great site to be on.
[00:42:12.440 --> 00:42:14.640]   But I think it means an IT pros.
[00:42:14.640 --> 00:42:15.640]   Yeah.
[00:42:15.640 --> 00:42:16.640]   That's right.
[00:42:16.640 --> 00:42:22.640]   And to be fair, the CDC has plenty of very top level public health and medical vocabulary
[00:42:22.640 --> 00:42:23.720]   and language and everything else.
[00:42:23.720 --> 00:42:28.720]   But the everyday person and journalists can also go on to that website and still make
[00:42:28.720 --> 00:42:30.000]   sense of what's there.
[00:42:30.000 --> 00:42:35.960]   So that's, you know, we need and we have people from the CDC who go out and speak.
[00:42:35.960 --> 00:42:37.720]   We've had movies made about the CDC.
[00:42:37.720 --> 00:42:39.600]   We see the CDC as part of NCI.
[00:42:39.600 --> 00:42:40.600]   Make it sexy.
[00:42:40.600 --> 00:42:41.600]   Yeah.
[00:42:41.600 --> 00:42:42.600]   That's right.
[00:42:42.600 --> 00:42:43.600]   Yeah.
[00:42:43.600 --> 00:42:44.600]   I like that.
[00:42:44.600 --> 00:42:45.880]   So I think we just solved the problem.
[00:42:45.880 --> 00:42:47.040]   We have the backbone of it.
[00:42:47.040 --> 00:42:50.840]   We just need to really step it up and thank goodness President Trump is right up on the
[00:42:50.840 --> 00:42:51.840]   cyber.
[00:42:51.840 --> 00:42:52.840]   Yep.
[00:42:52.840 --> 00:42:54.200]   And it's going to be huge.
[00:42:54.200 --> 00:42:56.960]   Clearly going to get get right to work on this.
[00:42:56.960 --> 00:42:57.960]   I know Don watches.
[00:42:57.960 --> 00:42:59.200]   We have the biggest cyber.
[00:42:59.200 --> 00:43:02.560]   So if we can get Joe Scarborough to talk about it.
[00:43:02.560 --> 00:43:03.560]   Let's take a break.
[00:43:03.560 --> 00:43:04.880]   This is a great conversation.
[00:43:04.880 --> 00:43:05.880]   And we've just begun.
[00:43:05.880 --> 00:43:08.480]   I think we've done with Meltdown Inspector, but I do encourage everybody.
[00:43:08.480 --> 00:43:10.960]   I mean, the bottom line, you're watching this.
[00:43:10.960 --> 00:43:12.240]   You understand it.
[00:43:12.240 --> 00:43:15.600]   But tell your friends and family, make sure you update those systems.
[00:43:15.600 --> 00:43:17.480]   Make sure you do those patches.
[00:43:17.480 --> 00:43:22.400]   It isn't, you know, perhaps as easy as just doing the operating system patches.
[00:43:22.400 --> 00:43:26.120]   There will be micro code patches for the processors.
[00:43:26.120 --> 00:43:32.360]   And I have to say I've read reports, good reports from security experts who say in some
[00:43:32.360 --> 00:43:36.560]   cases new hardware will be required.
[00:43:36.560 --> 00:43:39.320]   That's a big problem because nobody, I just bought an iMac Pro.
[00:43:39.320 --> 00:43:41.480]   Do I have to buy another one in a year?
[00:43:41.480 --> 00:43:42.480]   Go it away.
[00:43:42.480 --> 00:43:44.000]   I'm going to throw it away.
[00:43:44.000 --> 00:43:46.680]   It's got Meltdown Spectre.
[00:43:46.680 --> 00:43:48.920]   So it's got the STG.
[00:43:48.920 --> 00:43:51.920]   It's got the, yeah, it's got the Meltdown.
[00:43:51.920 --> 00:43:52.920]   But good news.
[00:43:52.920 --> 00:43:54.080]   I'm Cersei Lannister.
[00:43:54.080 --> 00:43:55.080]   That's the yeah.
[00:43:55.080 --> 00:43:56.080]   So that's the good news.
[00:43:56.080 --> 00:43:57.080]   Let's take a little break.
[00:43:57.080 --> 00:43:58.080]   We'll have a lot more.
[00:43:58.080 --> 00:44:00.520]   It's great to have Michael Sargent here from Mobile Nations.
[00:44:00.520 --> 00:44:05.000]   His senior editor there and represents people always say, you should get more young people
[00:44:05.000 --> 00:44:06.000]   on Leo.
[00:44:06.000 --> 00:44:08.280]   You're so damn old.
[00:44:08.280 --> 00:44:15.200]   He represents the most useful age cohort on the show under 25.
[00:44:15.200 --> 00:44:16.200]   That's nice.
[00:44:16.200 --> 00:44:17.200]   That's nice.
[00:44:17.200 --> 00:44:18.200]   Thank you for being here, Micah.
[00:44:18.200 --> 00:44:19.600]   Amy is only a little bit older.
[00:44:19.600 --> 00:44:22.400]   Amy Webb from Amyweb.io.
[00:44:22.400 --> 00:44:23.400]   She of course is the author.
[00:44:23.400 --> 00:44:24.560]   The signals are talking.
[00:44:24.560 --> 00:44:25.680]   Have you got a name for the new book?
[00:44:25.680 --> 00:44:27.520]   I can't wait to read it.
[00:44:27.520 --> 00:44:28.520]   I do.
[00:44:28.520 --> 00:44:29.520]   I can't.
[00:44:29.520 --> 00:44:30.760]   Yeah, no, no, no, no.
[00:44:30.760 --> 00:44:32.000]   I do have a name.
[00:44:32.000 --> 00:44:33.000]   It's like title.
[00:44:33.000 --> 00:44:34.000]   Nice.
[00:44:34.000 --> 00:44:35.000]   A manifesto.
[00:44:35.000 --> 00:44:37.640]   Oh, she's writing a manifesto.
[00:44:37.640 --> 00:44:38.640]   Yes.
[00:44:38.640 --> 00:44:39.840]   The Karl Marx of AI.
[00:44:39.840 --> 00:44:40.840]   That's right.
[00:44:40.840 --> 00:44:41.840]   I will.
[00:44:41.840 --> 00:44:42.840]   Yes.
[00:44:42.840 --> 00:44:43.840]   The Thomas Paine of AI.
[00:44:43.840 --> 00:44:44.840]   Karl Marx's birthday is coming up.
[00:44:44.840 --> 00:44:45.840]   His 200th birthday.
[00:44:45.840 --> 00:44:46.840]   Happy birthday, Karl.
[00:44:46.840 --> 00:44:47.840]   Yeah.
[00:44:47.840 --> 00:44:48.840]   Yeah, that's nice.
[00:44:48.840 --> 00:44:49.840]   That's true.
[00:44:49.840 --> 00:44:51.320]   How do you know that?
[00:44:51.320 --> 00:44:53.400]   I know so many useless things.
[00:44:53.400 --> 00:44:58.480]   Hey, you know, it's almost Karl Marx's birthday.
[00:44:58.480 --> 00:45:01.800]   Heigels' birthday is just around a corner.
[00:45:01.800 --> 00:45:05.360]   We could do Trotsky day.
[00:45:05.360 --> 00:45:06.360]   Also Mike Galgin.
[00:45:06.360 --> 00:45:07.360]   Great to have Mike.
[00:45:07.360 --> 00:45:09.960]   He is the author of a brand new book.
[00:45:09.960 --> 00:45:10.960]   Everybody's an author here.
[00:45:10.960 --> 00:45:11.960]   Gastronomad.
[00:45:11.960 --> 00:45:14.160]   Self-published, I'm sure, right?
[00:45:14.160 --> 00:45:15.160]   Because you wouldn't...
[00:45:15.160 --> 00:45:16.560]   I mean, that would be the best way to go.
[00:45:16.560 --> 00:45:18.600]   Ain't nobody got time for traditional publishing?
[00:45:18.600 --> 00:45:20.600]   Nobody got time for that.
[00:45:20.600 --> 00:45:23.200]   Gastronomad.net, if you want to get your copy, it looks great.
[00:45:23.200 --> 00:45:24.200]   Pictures.
[00:45:24.200 --> 00:45:25.200]   Thank you.
[00:45:25.200 --> 00:45:26.200]   Yeah, really.
[00:45:26.200 --> 00:45:27.200]   I'm reading it tonight.
[00:45:27.200 --> 00:45:28.200]   I can't wait.
[00:45:28.200 --> 00:45:29.200]   A copy.
[00:45:29.200 --> 00:45:30.200]   Our show today brought to you by...
[00:45:30.200 --> 00:45:32.120]   I'll tell you where I'm reading this.
[00:45:32.120 --> 00:45:36.160]   Lying safe and sound and great comfort on my Casper mattress.
[00:45:36.160 --> 00:45:37.160]   Wait.
[00:45:37.160 --> 00:45:39.920]   I love my Casper.
[00:45:39.920 --> 00:45:41.520]   Casper, you know the name.
[00:45:41.520 --> 00:45:42.520]   Not the friendly ghost.
[00:45:42.520 --> 00:45:48.480]   We're talking the mattress company that continues to revolutionize the mattress industry, providing
[00:45:48.480 --> 00:45:55.280]   you with affordable, comfortable, long-lasting mattresses that are 10 times better than the
[00:45:55.280 --> 00:45:57.560]   mattress you get at the mattress store.
[00:45:57.560 --> 00:46:04.920]   What they noticed, what they observed is that this is what I love about modern entrepreneurship.
[00:46:04.920 --> 00:46:09.360]   They're looking for businesses where there's a real disconnect, where the customer isn't
[00:46:09.360 --> 00:46:11.800]   well-served and mattress stores man.
[00:46:11.800 --> 00:46:17.920]   Those got not only 100% markup, but it is a terrible way to choose a mattress.
[00:46:17.920 --> 00:46:19.800]   It's a hard sell experience.
[00:46:19.800 --> 00:46:22.480]   It's a hard mattress experience.
[00:46:22.480 --> 00:46:26.480]   They said we could do it better by selling direct, but they had to solve a problem.
[00:46:26.480 --> 00:46:27.480]   They had to solve a problem.
[00:46:27.480 --> 00:46:31.560]   They made a great mattress, but who's going to buy a mattress without lying on it?
[00:46:31.560 --> 00:46:33.960]   Well, here's the solution they came up with and I love it.
[00:46:33.960 --> 00:46:37.320]   A 100-day trial.
[00:46:37.320 --> 00:46:40.920]   Casper offers free delivery and painless returns within 100 days.
[00:46:40.920 --> 00:46:42.440]   You don't have to lie down in the showroom.
[00:46:42.440 --> 00:46:46.880]   Just get that Casper mattress, put it on your bed, sleep on it.
[00:46:46.880 --> 00:46:51.160]   Not for one night or one month, but for 100 days, up more than three months.
[00:46:51.160 --> 00:46:53.400]   Any time you say, "It's not right for me."
[00:46:53.400 --> 00:46:54.640]   They'll come and get it.
[00:46:54.640 --> 00:46:56.440]   They'll refund you every penny.
[00:46:56.440 --> 00:47:00.120]   Free shipping and returns in the US and Canada and now the UK too.
[00:47:00.120 --> 00:47:05.600]   I could tell you one of the reasons they can do this because very few people say no.
[00:47:05.600 --> 00:47:06.600]   That's the economics of it.
[00:47:06.600 --> 00:47:09.920]   Once you lie and I cast, you'll say, "What am I missing my whole life?"
[00:47:09.920 --> 00:47:15.280]   It combines multiple supportive memory phones for a sleep surface that does the impossible.
[00:47:15.280 --> 00:47:18.200]   It gives you just the right sink.
[00:47:18.200 --> 00:47:20.920]   Your hard bits sink in.
[00:47:20.920 --> 00:47:24.320]   It's soft, but you don't want a soft, squishy mattress.
[00:47:24.320 --> 00:47:26.640]   You want a firm mattress to give you back support.
[00:47:26.640 --> 00:47:29.560]   It gives you just the right bounce and the right sink.
[00:47:29.560 --> 00:47:33.520]   By the way, very important, even in the winter, it's breathable design that lets you sleep
[00:47:33.520 --> 00:47:34.520]   cool.
[00:47:34.520 --> 00:47:36.000]   You can regulate your temperature to the night.
[00:47:36.000 --> 00:47:39.720]   We know a good night's sleep can change your life.
[00:47:39.720 --> 00:47:41.000]   I am not kidding.
[00:47:41.000 --> 00:47:44.440]   A Casper mattress provides long lasting comfort, long lasting support.
[00:47:44.440 --> 00:47:45.840]   You can buy it easily online.
[00:47:45.840 --> 00:47:47.920]   It's completely risk-free.
[00:47:47.920 --> 00:47:49.520]   By the way, they now have two.
[00:47:49.520 --> 00:47:52.640]   They just introduced the new Casper Wave mattress.
[00:47:52.640 --> 00:47:53.640]   I've been lying on that.
[00:47:53.640 --> 00:47:59.160]   This is a natural geometry support system and a new top layer.
[00:47:59.160 --> 00:48:02.840]   It is even nicer than the original Casper.
[00:48:02.840 --> 00:48:08.520]   You choose, you can say $50 towards either by going to Casper.com/twit.
[00:48:08.520 --> 00:48:14.360]   Do use the promo code "twit@checkout" to say $50.casper.com/twit with the promo code
[00:48:14.360 --> 00:48:16.440]   to it, some terms and conditions.
[00:48:16.440 --> 00:48:17.440]   Apply.
[00:48:17.440 --> 00:48:19.400]   Can I put in a plug for their pillows?
[00:48:19.400 --> 00:48:22.280]   Yes, I love my Casper pillow.
[00:48:22.280 --> 00:48:28.800]   We at Southby last year, I just wanted to put my feet up and it was cold.
[00:48:28.800 --> 00:48:30.600]   I laid down and went with everything.
[00:48:30.600 --> 00:48:32.160]   It's in the little trailer.
[00:48:32.160 --> 00:48:34.280]   You woke up three days later.
[00:48:34.280 --> 00:48:35.800]   You woke up three days later.
[00:48:35.800 --> 00:48:37.960]   The pillows are actually really comfortable.
[00:48:37.960 --> 00:48:38.960]   We have their pillows.
[00:48:38.960 --> 00:48:42.240]   I don't know what you got, but I got the biggest ones they make.
[00:48:42.240 --> 00:48:43.240]   This is a big giant.
[00:48:43.240 --> 00:48:44.320]   It's a great body pillow.
[00:48:44.320 --> 00:48:45.760]   You can wrap yourself around it.
[00:48:45.760 --> 00:48:46.760]   It's just so comfy.
[00:48:46.760 --> 00:48:47.760]   They are.
[00:48:47.760 --> 00:48:49.480]   They're really great and they don't get hot.
[00:48:49.480 --> 00:48:52.640]   I don't know what that is, but that's really important.
[00:48:52.640 --> 00:48:57.000]   It's hard to describe, but you know when you wake up kind of sweaty, you don't want that.
[00:48:57.000 --> 00:48:58.000]   The Casper breathes.
[00:48:58.000 --> 00:49:01.240]   I don't know how they do that.
[00:49:01.240 --> 00:49:03.440]   Thank you for that unsolicited plug for the Casper pillow.
[00:49:03.440 --> 00:49:05.920]   Yes, that was totally unsolicited.
[00:49:05.920 --> 00:49:08.920]   Whatever you call it, I don't have a financial connection with them.
[00:49:08.920 --> 00:49:09.920]   I do.
[00:49:09.920 --> 00:49:11.920]   They pay your ads.
[00:49:11.920 --> 00:49:14.440]   I'll tell you that right now.
[00:49:14.440 --> 00:49:18.120]   Yes, I may have received a couple of Casper mattresses in my time.
[00:49:18.120 --> 00:49:20.760]   Although we bought Casper mattresses for my kids.
[00:49:20.760 --> 00:49:23.280]   So I put my money on.
[00:49:23.280 --> 00:49:25.160]   I've got all of the Casper stuff.
[00:49:25.160 --> 00:49:26.960]   It's about the dog beds.
[00:49:26.960 --> 00:49:27.960]   Yeah.
[00:49:27.960 --> 00:49:28.960]   And I bought all of it.
[00:49:28.960 --> 00:49:33.160]   They have sponsored some shows that I've done in the past, but I know that we're just
[00:49:33.160 --> 00:49:37.440]   doing the Casper Hour, but seriously, we've got the bed frame, the box frame.
[00:49:37.440 --> 00:49:39.960]   You even have a Casper bed frame?
[00:49:39.960 --> 00:49:42.160]   I know they made them.
[00:49:42.160 --> 00:49:43.160]   I did.
[00:49:43.160 --> 00:49:44.160]   I did a bed frame.
[00:49:44.160 --> 00:49:47.720]   I went online and I was like, "Oh, guess I got to have that too."
[00:49:47.720 --> 00:49:52.280]   How we were gone for the holiday break.
[00:49:52.280 --> 00:49:57.160]   The day I left, and I feel like it's old news now, but we haven't talked about it because
[00:49:57.160 --> 00:50:03.240]   we've been gone, the Apple iPhone battery kerfuffle.
[00:50:03.240 --> 00:50:05.800]   And I still don't know who's right in this one.
[00:50:05.800 --> 00:50:12.920]   So maybe, Micah, you've done some, some Redditors pointed out that as soon as they took their
[00:50:12.920 --> 00:50:19.200]   older iPhone, their iPhone 6, their 6S, their 5S or the iPhone SE, as soon as they put iOS
[00:50:19.200 --> 00:50:21.200]   11 on it, the thing got slew.
[00:50:21.200 --> 00:50:23.000]   And it sounds like an urban legend, right?
[00:50:23.000 --> 00:50:25.560]   It sounds like, "Oh, it's getting slower because they want me to buy a new one."
[00:50:25.560 --> 00:50:28.280]   And I've heard this so many times that I've always pooed.
[00:50:28.280 --> 00:50:29.680]   And even Steve Gibson said this.
[00:50:29.680 --> 00:50:31.480]   I said, "No, Steve, it's not slower."
[00:50:31.480 --> 00:50:32.480]   Yes, it is.
[00:50:32.480 --> 00:50:35.480]   It turns out it is.
[00:50:35.480 --> 00:50:40.960]   Apple finally copped to it, said, "Well, here's why we do this."
[00:50:40.960 --> 00:50:44.200]   And actually, I'm inclined to believe him.
[00:50:44.200 --> 00:50:47.880]   The problem is that as the battery, it's not the phone that's old, it's the battery that's
[00:50:47.880 --> 00:50:48.880]   old.
[00:50:48.880 --> 00:50:53.800]   And as the battery gets old, it can't reach the peaks of performance it needs to.
[00:50:53.800 --> 00:50:58.160]   In fact, the phone might crash if the battery is taxed.
[00:50:58.160 --> 00:51:01.960]   So what Apple did in the new operating system is it checks the state of the battery, and
[00:51:01.960 --> 00:51:05.760]   if it's an older battery, and by the way, we don't know what that means, checks the
[00:51:05.760 --> 00:51:06.760]   state.
[00:51:06.760 --> 00:51:09.600]   It could just check the date or it could see what capacity we don't know.
[00:51:09.600 --> 00:51:11.040]   Apple's not saying.
[00:51:11.040 --> 00:51:15.840]   But for whatever reason it decides that's an older battery, it limits performance peaks.
[00:51:15.840 --> 00:51:21.960]   It doesn't let the processor demand as much power because Apple's position, if you buy
[00:51:21.960 --> 00:51:26.960]   all this, is, "Well, we'd rather you have a phone that works than a phone that crashes."
[00:51:26.960 --> 00:51:33.360]   Now, it gets complicated because Samsung, Motorola, HTC, and LG, who make very nice
[00:51:33.360 --> 00:51:37.160]   Android phones, all said, "Well, we don't do that."
[00:51:37.160 --> 00:51:40.600]   >> Monica, do you think that those phones crash more because of it?
[00:51:40.600 --> 00:51:46.320]   >> Well, there's been some interesting stories lately, too, with other companies where they
[00:51:46.320 --> 00:51:52.400]   are apparently, or at least it was said, that they are limiting different apps' performance,
[00:51:52.400 --> 00:51:54.440]   except for their own key apps.
[00:51:54.440 --> 00:51:56.720]   And so in a way, they're also sort of-
[00:51:56.720 --> 00:52:01.480]   >> Yeah, Samsung got bit because they would check if a benchmark was running, right?
[00:52:01.480 --> 00:52:02.760]   And then they run faster.
[00:52:02.760 --> 00:52:05.520]   But normally- >> You wouldn't run that fast because you'd
[00:52:05.520 --> 00:52:07.320]   overheat and you'd crash.
[00:52:07.320 --> 00:52:08.320]   >> Yeah.
[00:52:08.320 --> 00:52:14.080]   So this is such a complicated thing on many levels, and I have to go back to a little
[00:52:14.080 --> 00:52:19.480]   bit of what we were talking about before with the job of us as sort of tech know-how
[00:52:19.480 --> 00:52:21.960]   people in our families and among our friendships.
[00:52:21.960 --> 00:52:25.640]   >> Oh, yeah, this was the topic of the week for Christmas week, right?
[00:52:25.640 --> 00:52:26.640]   >> What's wrong with that?
[00:52:26.640 --> 00:52:29.080]   >> What's the deal with the iPhone?
[00:52:29.080 --> 00:52:34.280]   >> And it was one of those things where I, like, am the stand-in for Apple among my
[00:52:34.280 --> 00:52:37.480]   family, so sometimes something goes wrong, they always, you know, are mad at me about
[00:52:37.480 --> 00:52:38.480]   it.
[00:52:38.480 --> 00:52:41.560]   But like, this was so frustrating because this was a situation where so many people got
[00:52:41.560 --> 00:52:43.440]   to say, I told you so.
[00:52:43.440 --> 00:52:45.760]   >> Yeah, you didn't believe me.
[00:52:45.760 --> 00:52:46.960]   >> Right, exactly.
[00:52:46.960 --> 00:52:51.080]   And I'd like, you want to be bigger than that, but frankly, I'm not.
[00:52:51.080 --> 00:52:53.560]   And so hearing, I told you so, kind of stinks.
[00:52:53.560 --> 00:52:56.120]   >> I will say, I told you so.
[00:52:56.120 --> 00:52:57.120]   >> Also.
[00:52:57.120 --> 00:53:00.240]   >> But here's the thing.
[00:53:00.240 --> 00:53:05.280]   We have been, so you have probably all of have a washing machine, right?
[00:53:05.280 --> 00:53:08.480]   And if the washing machine breaks after five years, you get a tax-
[00:53:08.480 --> 00:53:11.800]   >> No, actually Mike goes down to the river and pounds his clothes on.
[00:53:11.800 --> 00:53:12.800]   >> All right.
[00:53:12.800 --> 00:53:17.400]   >> So for the rest of us who live in civilization, right, so, you know, after a certain number
[00:53:17.400 --> 00:53:21.800]   of years, any mechanical device that any of us owns will break.
[00:53:21.800 --> 00:53:25.280]   And we will be told, oh, well, you know, that's how it is, seven years.
[00:53:25.280 --> 00:53:26.280]   That's about how long they last.
[00:53:26.280 --> 00:53:32.400]   Now to be fair, there are real practical reasons for that, bands were out, you know, the different
[00:53:32.400 --> 00:53:33.680]   uses of the mechanics were out.
[00:53:33.680 --> 00:53:36.000]   >> And we know one thing that does wear out is lithium ion batteries.
[00:53:36.000 --> 00:53:38.160]   They have a fixed number of charge cycles.
[00:53:38.160 --> 00:53:43.800]   >> Sure, however, the difference between buying a washing machine and buying it in the year
[00:53:43.800 --> 00:53:51.200]   2018 and buying a mobile device is that, you know, we've got crazy malware.
[00:53:51.200 --> 00:53:53.960]   We have specter out there, right?
[00:53:53.960 --> 00:53:54.960]   We've melt down.
[00:53:54.960 --> 00:53:59.360]   We are now being told that we must update, otherwise we face problems.
[00:53:59.360 --> 00:54:04.560]   However, you know, once we update, we may see that there are challenges on the other
[00:54:04.560 --> 00:54:05.560]   end.
[00:54:05.560 --> 00:54:13.320]   When we purchase a phone, we are consumers or under the misnomer that we own the device,
[00:54:13.320 --> 00:54:14.320]   right?
[00:54:14.320 --> 00:54:18.960]   And all we've really done is purchased the ability to use the device, but we don't have
[00:54:18.960 --> 00:54:23.040]   full agency over that device as we do the other things in our lives.
[00:54:23.040 --> 00:54:24.040]   >> Good point.
[00:54:24.040 --> 00:54:25.960]   You may not have legal ownership.
[00:54:25.960 --> 00:54:30.840]   Many times you see youlas that say you're only renting this.
[00:54:30.840 --> 00:54:31.840]   You know, we own it.
[00:54:31.840 --> 00:54:32.840]   You're using it.
[00:54:32.840 --> 00:54:36.440]   >> But the world was not always that way.
[00:54:36.440 --> 00:54:40.360]   It has been that way traditionally with the beginning of telephones, right?
[00:54:40.360 --> 00:54:45.480]   Because in the early days of phones, you didn't own the phone from Dell.
[00:54:45.480 --> 00:54:49.680]   You know, you didn't buy it the way that you do now, but we should probably be asking
[00:54:49.680 --> 00:54:54.560]   ourselves, you know, consumers are monetized in all these different ways.
[00:54:54.560 --> 00:54:59.960]   And you know, we're basically paying for access with our mobile devices and then being forced
[00:54:59.960 --> 00:55:04.720]   to turn them in and buy new ones, even though they work pretty well.
[00:55:04.720 --> 00:55:07.800]   And we're still being monetized as part of the process because our information is being
[00:55:07.800 --> 00:55:11.160]   used and parsed and, you know, used in different ways.
[00:55:11.160 --> 00:55:12.720]   So I don't know.
[00:55:12.720 --> 00:55:13.720]   I am.
[00:55:13.720 --> 00:55:23.280]   >> So, I mean, one, so A, the question is, is Apple's explanation credible or are they
[00:55:23.280 --> 00:55:27.880]   doing it as everybody has always suspected just to force you to buy in the phone?
[00:55:27.880 --> 00:55:29.280]   That would be question one.
[00:55:29.280 --> 00:55:30.280]   Where do you come down on that?
[00:55:30.280 --> 00:55:33.080]   >> I think it's actually not that black and white.
[00:55:33.080 --> 00:55:37.240]   It's a fine line between we want you to.
[00:55:37.240 --> 00:55:38.960]   I mean, clearly Apple wants you to upgrade.
[00:55:38.960 --> 00:55:45.280]   I mean, Apple is the kind of company where they want you to buy Apple accessories.
[00:55:45.280 --> 00:55:47.120]   They want you to buy laptop every two years.
[00:55:47.120 --> 00:55:48.840]   They want you to buy an iPhone every year.
[00:55:48.840 --> 00:55:50.640]   They want you to-- >> Frankly, they make it very attractive
[00:55:50.640 --> 00:55:54.840]   to just automatically get the new iPhone every year by paying them, you know, 30 bucks
[00:55:54.840 --> 00:55:55.840]   a month or whatever.
[00:55:55.840 --> 00:55:56.840]   >> Absolutely.
[00:55:56.840 --> 00:56:03.240]   So, if you're guzzling the Kool-Aid inside Apple, which they all are, the idea that
[00:56:03.240 --> 00:56:06.480]   you would go two years or three years without upgrading your phone, you must be nuts.
[00:56:06.480 --> 00:56:07.480]   >> That's the salient point.
[00:56:07.480 --> 00:56:08.480]   They don't even understand the issues.
[00:56:08.480 --> 00:56:10.920]   >> No, because they're not using an iPhone 6.
[00:56:10.920 --> 00:56:12.720]   >> Who would use an iPhone 6?
[00:56:12.720 --> 00:56:13.920]   >> You'd have to be nuts.
[00:56:13.920 --> 00:56:15.840]   >> You'd have to be nuts.
[00:56:15.840 --> 00:56:17.320]   So it's a fine line.
[00:56:17.320 --> 00:56:21.360]   But at the same time, I think-- >> Can I add some texture to that?
[00:56:21.360 --> 00:56:22.360]   >> Yes.
[00:56:22.360 --> 00:56:26.320]   >> Mike just said, because this is-- we are starting to finally see the sales of smart
[00:56:26.320 --> 00:56:29.360]   phones, smartphones, plateau, right?
[00:56:29.360 --> 00:56:31.200]   For the first time, they're not that bad.
[00:56:31.200 --> 00:56:32.200]   >> You're in saturation.
[00:56:32.200 --> 00:56:33.200]   >> That's right.
[00:56:33.200 --> 00:56:37.320]   And by my modeling, so my job again is to use data to figure out what's coming, I would
[00:56:37.320 --> 00:56:40.400]   say the 2018 is the beginning of the end of smartphones.
[00:56:40.400 --> 00:56:41.400]   >> Whoa.
[00:56:41.400 --> 00:56:42.400]   >> So--
[00:56:42.400 --> 00:56:43.400]   >> I'm going to hear with that.
[00:56:43.400 --> 00:56:44.400]   >> You heard it here first.
[00:56:44.400 --> 00:56:45.400]   You agreed?
[00:56:45.400 --> 00:56:46.400]   >> I totally.
[00:56:46.400 --> 00:56:47.400]   >> Whoa.
[00:56:47.400 --> 00:56:52.520]   >> I mean, not tomorrow, but again, there's just so many different signals that had that
[00:56:52.520 --> 00:56:54.880]   point towards the beginning of the end is now.
[00:56:54.880 --> 00:56:56.400]   >> I think you said this before.
[00:56:56.400 --> 00:56:58.160]   What is going to replace this though, Amy?
[00:56:58.160 --> 00:56:59.160]   >> Wearables.
[00:56:59.160 --> 00:57:00.160]   >> What?
[00:57:00.160 --> 00:57:01.160]   >> So the next--
[00:57:01.160 --> 00:57:02.160]   >> Wearables.
[00:57:02.160 --> 00:57:03.160]   >> You know, if you think of where we started--
[00:57:03.160 --> 00:57:04.560]   >> You said this for a while, Mike.
[00:57:04.560 --> 00:57:13.080]   >> So where we started was a landline that became a series of devices, a PDA, a pager,
[00:57:13.080 --> 00:57:14.720]   right?
[00:57:14.720 --> 00:57:19.040]   And then finally, over many, many, many years, many of these devices converged and we're
[00:57:19.040 --> 00:57:24.680]   all now carrying a smartphone rather than a cell phone and a digital camera and MP3
[00:57:24.680 --> 00:57:25.600]   player and all those other things.
[00:57:25.600 --> 00:57:32.000]   So the funnel is now going in the other direction as we adopt all of these new more advanced
[00:57:32.000 --> 00:57:37.880]   technologies and eventually many, many years from now, decade from now, we'll see a second
[00:57:37.880 --> 00:57:43.040]   area of convergence, which will probably be some kind of thing that we wear over our
[00:57:43.040 --> 00:57:50.400]   eyes combined with a hearable device and probably a ring or a wristlet for the time being.
[00:57:50.400 --> 00:57:51.400]   >> Oh my God.
[00:57:51.400 --> 00:57:54.440]   >> Well, this sounds like a medieval suit of armor.
[00:57:54.440 --> 00:57:59.040]   >> Sure, but if you were to go back 20 years and ask Leo 20 years ago--
[00:57:59.040 --> 00:58:00.040]   >> Yeah, you're right.
[00:58:00.040 --> 00:58:03.720]   >> Would I be staring at my screen as I walk down the street in front of traffic?
[00:58:03.720 --> 00:58:06.680]   >> And in fact, Leo, you are right now wearing hearables.
[00:58:06.680 --> 00:58:08.720]   You often wear glasses and you have a wristwatch.
[00:58:08.720 --> 00:58:10.200]   You're actually already wearing all that stuff.
[00:58:10.200 --> 00:58:11.920]   >> And I'm watching-- I'm looking at Instagram.
[00:58:11.920 --> 00:58:12.920]   >> Yes, but eventually you're wearing--
[00:58:12.920 --> 00:58:17.760]   >> But here's the thing, that provides the incentive, right?
[00:58:17.760 --> 00:58:21.960]   Because Apple at the moment doesn't have any other products, right?
[00:58:21.960 --> 00:58:26.760]   So they've got to extract as much value out of this current thing that they had.
[00:58:26.760 --> 00:58:27.760]   >> Brilliant.
[00:58:27.760 --> 00:58:30.720]   >> Because they know they see the future coming.
[00:58:30.720 --> 00:58:32.920]   They're working hard on the next thing.
[00:58:32.920 --> 00:58:33.920]   Meanwhile--
[00:58:33.920 --> 00:58:34.920]   >> Well, here's one more time.
[00:58:34.920 --> 00:58:39.080]   >> --we got to squeeze every penny of profit out of the current thing.
[00:58:39.080 --> 00:58:45.080]   >> And I would add one final point to this and that is that Magic Leap this year, they've
[00:58:45.080 --> 00:58:46.080]   got science for their--
[00:58:46.080 --> 00:58:49.160]   >> They've got to develop our edition coming out, right?
[00:58:49.160 --> 00:58:50.160]   >> That's right.
[00:58:50.160 --> 00:58:57.480]   And so whether or not this is commercially successful, all law, the original iPhone, we
[00:58:57.480 --> 00:58:58.480]   still don't know.
[00:58:58.480 --> 00:59:05.400]   But what we do know is that I would argue OEMs and the company-- I think that our smartphone
[00:59:05.400 --> 00:59:07.200]   makers are probably concerned.
[00:59:07.200 --> 00:59:11.200]   And if they're not yet concerned, they should be.
[00:59:11.200 --> 00:59:17.680]   Because they're going to have to transform-- 10 years from now, these stupid iPhones that
[00:59:17.680 --> 00:59:22.520]   we're all talking about and Samsung and everything else are going to look a lot like the flip
[00:59:22.520 --> 00:59:24.960]   phones of today.
[00:59:24.960 --> 00:59:28.280]   A few people will still have them, but everybody else will have moved on to something else.
[00:59:28.280 --> 00:59:29.280]   >> Wow.
[00:59:29.280 --> 00:59:34.800]   >> Apple's filed a ton of patents for various different wearable devices.
[00:59:34.800 --> 00:59:39.040]   So it's possible that what's happening, I think, with the updates and the batteries,
[00:59:39.040 --> 00:59:43.760]   part of it may be a hardware flaw, but part of it may just be monetizing as quickly as
[00:59:43.760 --> 00:59:47.120]   possible before we enter this next phase.
[00:59:47.120 --> 00:59:52.080]   >> This, by the way, this-- I probably should have made this a higher up story.
[00:59:52.080 --> 00:59:55.520]   This magically won Creator Edition.
[00:59:55.520 --> 00:59:56.840]   >> Yeah, the SBA is open.
[00:59:56.840 --> 01:00:01.120]   >> It's a huge deal because, well, frankly, many of us, including myself, speculated,
[01:00:01.120 --> 01:00:04.720]   they had nothing or that they weren't able to do what they had thought they were going
[01:00:04.720 --> 01:00:06.160]   to be able to do.
[01:00:06.160 --> 01:00:10.400]   And who knows what compromises this Creator's Edition will have.
[01:00:10.400 --> 01:00:15.040]   And if it's anything like HoloLens or the early Oculus versions, they'll-- it will not
[01:00:15.040 --> 01:00:16.720]   be ready for prime time.
[01:00:16.720 --> 01:00:18.720]   That's probably why it's a Creator's Edition.
[01:00:18.720 --> 01:00:20.000]   >> And of course, that's a great look.
[01:00:20.000 --> 01:00:21.760]   >> And you look like an idiot.
[01:00:21.760 --> 01:00:29.360]   But I think Amy, you will say, well, you probably-- you look like an idiot now.
[01:00:29.360 --> 01:00:30.600]   You just used to it.
[01:00:30.600 --> 01:00:33.160]   >> I'm used to looking like an idiot personally.
[01:00:33.160 --> 01:00:36.360]   You are a Google-- you are a Glasshole in the early days.
[01:00:36.360 --> 01:00:40.720]   >> I'm a proud card-carrying member of the Glasshole community.
[01:00:40.720 --> 01:00:42.000]   But there are essentially two things.
[01:00:42.000 --> 01:00:48.040]   If you think about cars and trucks, Steve Jobs is a metaphor for big expensive desktops
[01:00:48.040 --> 01:00:50.280]   and lighter everyday use device.
[01:00:50.280 --> 01:00:51.200]   >> I just bought a truck.
[01:00:51.200 --> 01:00:51.960]   >> Tesla?
[01:00:51.960 --> 01:00:55.160]   >> No, and I'm Mac Pro.
[01:00:55.160 --> 01:00:56.560]   >> Oh, yeah.
[01:00:56.560 --> 01:00:57.560]   Okay.
[01:00:57.560 --> 01:00:59.160]   >> Because more than many Chevy trucks.
[01:00:59.160 --> 01:01:00.160]   >> Yes, yes.
[01:01:00.160 --> 01:01:01.160]   >> Oh, 5,000.
[01:01:01.160 --> 01:01:02.160]   I remember pay--
[01:01:02.160 --> 01:01:03.160]   >> 8,000, the one I bought.
[01:01:03.160 --> 01:01:04.160]   >> 8,000.
[01:01:04.160 --> 01:01:07.520]   >> Well, I knew Meltdown and Spectre was going to really take away the performance.
[01:01:07.520 --> 01:01:08.680]   So I decided to get the 10.
[01:01:08.680 --> 01:01:09.680]   >> That's right.
[01:01:09.680 --> 01:01:10.680]   >> That's right.
[01:01:10.680 --> 01:01:11.680]   >> But he's not going to spend himself.
[01:01:11.680 --> 01:01:14.680]   >> It's only going to be good for two years though before they force you to buy another
[01:01:14.680 --> 01:01:15.680]   one.
[01:01:15.680 --> 01:01:24.520]   >> So the Magic Leap and the Hollow Lens, these are the trucks of developers.
[01:01:24.520 --> 01:01:25.520]   They are for developers now.
[01:01:25.520 --> 01:01:27.240]   >> They're not just the trucks.
[01:01:27.240 --> 01:01:28.240]   They're the Atari 2600s.
[01:01:28.240 --> 01:01:31.520]   >> The strangest thing we're going to see in five years is people are going to be sitting
[01:01:31.520 --> 01:01:36.360]   at their desks with stuff like that, like a intrusive, goofy looking things.
[01:01:36.360 --> 01:01:38.360]   And they have amazing things happening on their desk.
[01:01:38.360 --> 01:01:40.560]   It's going to be a beautiful experience.
[01:01:40.560 --> 01:01:43.320]   They're going to look like idiots, but it's going to be an amazing experience.
[01:01:43.320 --> 01:01:44.320]   The other things happening--
[01:01:44.320 --> 01:01:46.480]   >> Are we going to call them Leepholes?
[01:01:46.480 --> 01:01:48.200]   >> I hope so.
[01:01:48.200 --> 01:01:53.600]   >> I am uncomfortable just hearing that.
[01:01:53.600 --> 01:01:57.360]   >> But the other thing that's happening is that there are all kinds of technology being
[01:01:57.360 --> 01:02:01.600]   developed for the lenses that will enable pretty ordinary looking glasses, like Amy's
[01:02:01.600 --> 01:02:02.600]   glasses.
[01:02:02.600 --> 01:02:08.480]   Those web glasses look like Amy's glasses in three, four, five, six years that provide
[01:02:08.480 --> 01:02:11.840]   a light version of that sort of mixed reality.
[01:02:11.840 --> 01:02:13.280]   >> So this isn't what you'll look like.
[01:02:13.280 --> 01:02:14.280]   >> And this is the thing.
[01:02:14.280 --> 01:02:15.280]   It's like hard--
[01:02:15.280 --> 01:02:16.280]   >> It's good.
[01:02:16.280 --> 01:02:21.560]   >> I know that it's hard for people to grasp what Amy was saying, which is that the iPhone
[01:02:21.560 --> 01:02:25.360]   10 will seem so dated and old fashioned and dull.
[01:02:25.360 --> 01:02:29.560]   And the thing that will make it seem so dull is once you have things floating in front of
[01:02:29.560 --> 01:02:34.400]   you, information flying in and out, staring at this tiny little screen is going to be
[01:02:34.400 --> 01:02:35.400]   antiquated.
[01:02:35.400 --> 01:02:36.400]   >> Awful.
[01:02:36.400 --> 01:02:37.400]   >> It's going to be awful.
[01:02:37.400 --> 01:02:39.400]   >> Now let me take this one step further.
[01:02:39.400 --> 01:02:40.400]   >> Okay.
[01:02:40.400 --> 01:02:44.440]   So again, so my job is to figure out what all this means.
[01:02:44.440 --> 01:02:48.920]   So it's likely that we will be-- some of it will be glasses.
[01:02:48.920 --> 01:02:51.840]   There's retinal projection, so it looks like--
[01:02:51.840 --> 01:02:54.400]   >> I like that better.
[01:02:54.400 --> 01:02:57.040]   >> It has problems, but now here's the rub.
[01:02:57.040 --> 01:03:02.480]   The rub is that humans have not evolved biologically as quickly as all of the technology in our
[01:03:02.480 --> 01:03:03.480]   lives.
[01:03:03.480 --> 01:03:08.600]   And we've come a long way, but our eyes were never designed to see close biologically and
[01:03:08.600 --> 01:03:12.480]   neurologically, our eyes were designed to look far away.
[01:03:12.480 --> 01:03:18.240]   So the challenge is the technology will work and will be able to see magical things out
[01:03:18.240 --> 01:03:23.800]   of the technology, but our eyes are not going to be willing to cooperate, which tells us
[01:03:23.800 --> 01:03:28.720]   that we're probably going to have more and more ocular problems going forward.
[01:03:28.720 --> 01:03:32.880]   So kind of like the interesting weird piece of this is that--
[01:03:32.880 --> 01:03:36.240]   >> Isn't nearsightedness a relatively recent--
[01:03:36.240 --> 01:03:38.840]   >> It is-- yes, I'm making a question.
[01:03:38.840 --> 01:03:41.480]   >> A phenomenon because of reading so much?
[01:03:41.480 --> 01:03:42.480]   >> That's right.
[01:03:42.480 --> 01:03:46.280]   And so this actually presents a potential challenge for Warby Parker.
[01:03:46.280 --> 01:03:51.520]   So in a weird way, the advent, like the depth of the smartphone and the advent of smart
[01:03:51.520 --> 01:03:57.040]   glasses poses a problem for Warby Parker because Warby Parker's business model is predicated
[01:03:57.040 --> 01:04:00.520]   on the idea that you go to an optometrist or an ophthalmologist, you get your prescription
[01:04:00.520 --> 01:04:02.000]   and then send it to them.
[01:04:02.000 --> 01:04:07.000]   >> So you want to hear something really interesting?
[01:04:07.000 --> 01:04:15.000]   Warby Parker has announced a new iOS app that will do your prescription at home.
[01:04:15.000 --> 01:04:16.000]   >> Yeah, that's terrible.
[01:04:16.000 --> 01:04:17.000]   Let me just put a plug in.
[01:04:17.000 --> 01:04:18.000]   My husband and my doctor.
[01:04:18.000 --> 01:04:21.000]   >> He probably hates this idea.
[01:04:21.000 --> 01:04:24.000]   >> Well, yes, but here's the bigger issue.
[01:04:24.000 --> 01:04:28.000]   There's a whole host of people who are going to be able to do this.
[01:04:28.000 --> 01:04:31.240]   >> Yes, but here's the bigger issue.
[01:04:31.240 --> 01:04:36.400]   There's a whole host of eye-related problems that we now-- that younger people especially
[01:04:36.400 --> 01:04:39.480]   have because of the amount of time we spend in front of our screens.
[01:04:39.480 --> 01:04:44.760]   So the refraction that Warby Parker, you know, that an app does only tells you what
[01:04:44.760 --> 01:04:46.560]   the lens strength should be.
[01:04:46.560 --> 01:04:49.760]   What it doesn't tell you is do you have early macular degeneration?
[01:04:49.760 --> 01:04:51.840]   Do you have a swollen optic nerve?
[01:04:51.840 --> 01:04:56.360]   Like there are all these other issues that you can only find out about if you have somebody
[01:04:56.360 --> 01:04:59.360]   looking into your eye, anyhow, tension.
[01:04:59.360 --> 01:05:01.240]   >> No, of course.
[01:05:01.240 --> 01:05:03.240]   And Warby says this is only good for distance.
[01:05:03.240 --> 01:05:05.160]   It won't work for reading glasses.
[01:05:05.160 --> 01:05:07.520]   It won't work for progressive lenses.
[01:05:07.520 --> 01:05:08.520]   This is not it.
[01:05:08.520 --> 01:05:13.520]   This is an app that came out within the fall that measures your face so the glasses will
[01:05:13.520 --> 01:05:15.800]   fit using the iPhone X face camera.
[01:05:15.800 --> 01:05:19.720]   But there's a new app and I learned about it actually on the app store on Apple.
[01:05:19.720 --> 01:05:22.720]   I don't have a website so I can't show you.
[01:05:22.720 --> 01:05:29.720]   But basically the new app, you get the app on your phone, you need your computer, you
[01:05:29.720 --> 01:05:37.800]   put the QR code on the computer, the phone, then you walk back 12 feet and the computer
[01:05:37.800 --> 01:05:39.480]   will tell you when to stop.
[01:05:39.480 --> 01:05:44.800]   And then it does an eye test and then it prescribes for $40 glasses that you then buy
[01:05:44.800 --> 01:05:47.240]   from Warby Parker and they send them to you.
[01:05:47.240 --> 01:05:49.560]   And I'm sure it's driving your husband nuts because you're right.
[01:05:49.560 --> 01:05:51.360]   It's not doing a glaucoma test.
[01:05:51.360 --> 01:05:52.360]   It's not working for astigmatism.
[01:05:52.360 --> 01:05:53.360]   It's all sort of...
[01:05:53.360 --> 01:05:54.360]   Yeah, okay.
[01:05:54.360 --> 01:05:57.200]   But again, this is another...
[01:05:57.200 --> 01:06:05.680]   My point is my concern is that as more of these interesting whiz bang things come online,
[01:06:05.680 --> 01:06:10.040]   what they are essentially doing is training us that we need to be less plugged in.
[01:06:10.040 --> 01:06:14.880]   And if you stop and think for a minute, you can't not go to your eye.
[01:06:14.880 --> 01:06:16.480]   You can't not go to a dentist.
[01:06:16.480 --> 01:06:18.000]   You can't not go to an eye doctor.
[01:06:18.000 --> 01:06:19.000]   You can't...
[01:06:19.000 --> 01:06:20.000]   There's an app for that.
[01:06:20.000 --> 01:06:25.800]   I can't wait for the home dentistry kit.
[01:06:25.800 --> 01:06:29.400]   The thing that broke me at CS last year was an AI powered toothbrush.
[01:06:29.400 --> 01:06:30.400]   Oh, Lord.
[01:06:30.400 --> 01:06:31.400]   Right?
[01:06:31.400 --> 01:06:33.520]   As some kind of prevent...
[01:06:33.520 --> 01:06:36.600]   Like way for you to skip visits to the dentist.
[01:06:36.600 --> 01:06:38.440]   The dentist is doing more than checking your teeth.
[01:06:38.440 --> 01:06:40.400]   The dentist is looking to see if you have early mouth cancer.
[01:06:40.400 --> 01:06:42.400]   There's a lot of stuff going on.
[01:06:42.400 --> 01:06:46.840]   So it's nice that machines can make some decisions for us, but we have to continue to
[01:06:46.840 --> 01:06:48.280]   stay plugged in.
[01:06:48.280 --> 01:06:54.960]   So humans can't take ourselves out of the loop because there's a cool new app with a QR code.
[01:06:54.960 --> 01:06:55.960]   You know, I...
[01:06:55.960 --> 01:07:00.960]   Well, and then to get back to your earlier point, which is...
[01:07:00.960 --> 01:07:02.680]   And we saw this, by the way, with virtual reality.
[01:07:02.680 --> 01:07:06.560]   It's one of the things I think it's holding virtual reality back is the human biology
[01:07:06.560 --> 01:07:11.040]   is not adapted to even virtual reality, let alone augmented reality.
[01:07:11.040 --> 01:07:12.040]   And there's a different...
[01:07:12.040 --> 01:07:16.720]   Like a VR, you've got a problem because you've got a convergence distance and a focal distance.
[01:07:16.720 --> 01:07:21.080]   And they don't match and your body thinks you ate bad mushrooms and you throw up.
[01:07:21.080 --> 01:07:22.080]   And that's...
[01:07:22.080 --> 01:07:24.840]   I think it's kind of a lasting problem for VR.
[01:07:24.840 --> 01:07:29.000]   It's one of the things is I don't think VR is going to take off partly because of this.
[01:07:29.000 --> 01:07:30.000]   Just physiologically.
[01:07:30.000 --> 01:07:34.520]   There's a lot of people who are still affected by it regardless of like even as it gets
[01:07:34.520 --> 01:07:37.280]   more and more precise and more accurate and higher quality.
[01:07:37.280 --> 01:07:38.280]   It will never get still.
[01:07:38.280 --> 01:07:39.280]   Exactly.
[01:07:39.280 --> 01:07:40.280]   And that's the point.
[01:07:40.280 --> 01:07:43.040]   And even though Oculus was always saying, "Oh no, we just got to get the frame rate up."
[01:07:43.040 --> 01:07:45.160]   Or, "No, we just got to get better screens."
[01:07:45.160 --> 01:07:46.160]   Or whatever.
[01:07:46.160 --> 01:07:47.160]   It's a...
[01:07:47.160 --> 01:07:48.160]   It's a...
[01:07:48.160 --> 01:07:51.400]   I don't know Amy what your husband thinks about this, but it's a physiological problem
[01:07:51.400 --> 01:07:54.240]   because you have two methods of focusing.
[01:07:54.240 --> 01:07:59.000]   And if they don't match because your eyes are converging on something that's inches from
[01:07:59.000 --> 01:08:03.640]   your face, but your head is telling you you're focusing on something 10 feet away, if those
[01:08:03.640 --> 01:08:06.880]   don't match, your body says, "No, no, no, I'm hallucinating.
[01:08:06.880 --> 01:08:07.880]   This is not good."
[01:08:07.880 --> 01:08:08.880]   And...
[01:08:08.880 --> 01:08:09.880]   Right.
[01:08:09.880 --> 01:08:10.880]   So...
[01:08:10.880 --> 01:08:13.880]   According to the Air Force, 11% of the population, because they've been doing...
[01:08:13.880 --> 01:08:15.840]   Will suffer from simulation sickness.
[01:08:15.840 --> 01:08:19.760]   The Air Force's recommendation is after you use a simulator for an hour, you shouldn't
[01:08:19.760 --> 01:08:24.240]   drive for 24 because you'll be completely disoriented if you're one of these people.
[01:08:24.240 --> 01:08:25.400]   That seems to me a game's...
[01:08:25.400 --> 01:08:26.560]   A show stopper.
[01:08:26.560 --> 01:08:27.560]   Yeah.
[01:08:27.560 --> 01:08:32.840]   And so, again, just because the technology has advanced doesn't mean that our bodies
[01:08:32.840 --> 01:08:34.560]   have advanced along with it.
[01:08:34.560 --> 01:08:38.320]   And so, one of the big problems, and this is not true of Magic Leap, but it's true of
[01:08:38.320 --> 01:08:43.200]   a lot of other companies that are working on curable and wearable devices, they may have
[01:08:43.200 --> 01:08:48.000]   a doctor who's part of the team sort of in name, but they're not really thinking through
[01:08:48.000 --> 01:08:50.480]   the implications of what they're developing now.
[01:08:50.480 --> 01:08:51.480]   Or they are, and they don't...
[01:08:51.480 --> 01:08:55.840]   They say, "Well, just pretend it doesn't be a problem because otherwise we've got no
[01:08:55.840 --> 01:08:56.840]   business."
[01:08:56.840 --> 01:08:58.720]   That's right, because this is another case where every day...
[01:08:58.720 --> 01:09:01.760]   So many people are averse to wearing glasses to begin with.
[01:09:01.760 --> 01:09:02.760]   Yeah.
[01:09:02.760 --> 01:09:05.920]   But you probably were averse to carrying a computer in your pocket for a long time,
[01:09:05.920 --> 01:09:08.000]   too, and now you wouldn't be careful with that.
[01:09:08.000 --> 01:09:09.000]   Sure, of course.
[01:09:09.000 --> 01:09:10.000]   That's right.
[01:09:10.000 --> 01:09:15.760]   But once we are all wearing... I've had glasses since I was four years old, so once you're
[01:09:15.760 --> 01:09:22.160]   wearing progressives, which I do, for me, when I had Google Glass, I had a hard time making
[01:09:22.160 --> 01:09:24.480]   that adjustment.
[01:09:24.480 --> 01:09:28.000]   And my husband was one of the first people in the country to write prescriptions for
[01:09:28.000 --> 01:09:29.560]   Google Glass to ask me how many...
[01:09:29.560 --> 01:09:32.640]   No, ask me how many people came in asking for them.
[01:09:32.640 --> 01:09:33.640]   How many?
[01:09:33.640 --> 01:09:34.640]   Zero.
[01:09:34.640 --> 01:09:37.120]   Not a single person.
[01:09:37.120 --> 01:09:41.360]   Perhaps we have AR overlays and retinal projections and some of these other things.
[01:09:41.360 --> 01:09:47.680]   Again, it's going to be cool, but it's going to wind up... We're going to wind up with
[01:09:47.680 --> 01:09:48.840]   more eye problems.
[01:09:48.840 --> 01:09:53.720]   So how does this affect your prediction that the smartphone is dead, that we are going
[01:09:53.720 --> 01:09:54.720]   to go to these new platforms?
[01:09:54.720 --> 01:09:55.720]   We're going to do it anyway.
[01:09:55.720 --> 01:09:56.720]   It doesn't affect it.
[01:09:56.720 --> 01:09:58.200]   Well, it doesn't affect it at all.
[01:09:58.200 --> 01:10:00.160]   We're just going to go blind?
[01:10:00.160 --> 01:10:01.160]   Well...
[01:10:01.160 --> 01:10:02.160]   Yes.
[01:10:02.160 --> 01:10:03.160]   Well, yes.
[01:10:03.160 --> 01:10:04.160]   Yes, we're going to go blind.
[01:10:04.160 --> 01:10:09.640]   The answer is blind, no, but we are already dealing with eye strain.
[01:10:09.640 --> 01:10:12.560]   Most people already deal with eye strain and we've just sort of learned to live with it.
[01:10:12.560 --> 01:10:15.400]   I have terrible eyes and that's from reading too much.
[01:10:15.400 --> 01:10:16.400]   That's right.
[01:10:16.400 --> 01:10:21.080]   And think of everybody who has their mobile devices next to their beds and they read before
[01:10:21.080 --> 01:10:22.080]   they go to bed.
[01:10:22.080 --> 01:10:23.080]   Me too.
[01:10:23.080 --> 01:10:24.080]   That lights damaging.
[01:10:24.080 --> 01:10:25.880]   Blue love and all that stuff.
[01:10:25.880 --> 01:10:32.360]   The good news for eye damage is the coming age of wearables that deliver Alexa and Google
[01:10:32.360 --> 01:10:33.600]   Home and so on.
[01:10:33.600 --> 01:10:35.600]   That's going to be a big deal at CVS.
[01:10:35.600 --> 01:10:38.240]   We're going to see a lot of that.
[01:10:38.240 --> 01:10:40.200]   We're going to see people announcing things this week.
[01:10:40.200 --> 01:10:42.280]   So you could talk and listen instead of looking.
[01:10:42.280 --> 01:10:43.280]   Exactly.
[01:10:43.280 --> 01:10:44.280]   I love that.
[01:10:44.280 --> 01:10:45.280]   I love that.
[01:10:45.280 --> 01:10:46.280]   Absolutely.
[01:10:46.280 --> 01:10:47.280]   I have echoes everywhere in my house.
[01:10:47.280 --> 01:10:50.040]   Well, see, but if you had them in your classes, you wouldn't need them anywhere in your house.
[01:10:50.040 --> 01:10:51.040]   You'd have them everywhere.
[01:10:51.040 --> 01:10:52.040]   I just want to...
[01:10:52.040 --> 01:10:53.040]   Yeah, I just...
[01:10:53.040 --> 01:10:54.040]   Of course, there are problems.
[01:10:54.040 --> 01:10:56.040]   Tileo, aren't you concerned about...
[01:10:56.040 --> 01:10:57.040]   Aren't you concerned about...
[01:10:57.040 --> 01:10:59.200]   Steve, you've got them all over your house.
[01:10:59.200 --> 01:11:03.400]   I'm not... I mean, have you thought very much about somebody hacking your house?
[01:11:03.400 --> 01:11:05.400]   Yeah, so what?
[01:11:05.400 --> 01:11:06.400]   Okay.
[01:11:06.400 --> 01:11:07.400]   What are they going to get?
[01:11:07.400 --> 01:11:11.200]   They're going to hear me making dirty jokes.
[01:11:11.200 --> 01:11:12.200]   I don't know.
[01:11:12.200 --> 01:11:13.200]   I mean, what's the...
[01:11:13.200 --> 01:11:14.840]   I mean, I understand the theoretical problem.
[01:11:14.840 --> 01:11:19.040]   I'm just trying to understand what the actual harm is going to be.
[01:11:19.040 --> 01:11:22.040]   If somebody wanted to turn on a mic in my house, I guess they could...
[01:11:22.040 --> 01:11:24.240]   Well, what's the harm of Spectre and Meltdown?
[01:11:24.240 --> 01:11:26.680]   No, that's a good point.
[01:11:26.680 --> 01:11:27.680]   But I think the larger point is...
[01:11:27.680 --> 01:11:30.280]   I don't say my password and I'll pass it out loud.
[01:11:30.280 --> 01:11:31.280]   Right.
[01:11:31.280 --> 01:11:35.120]   The reason it doesn't matter is that the idea that we're going to be talking to computer,
[01:11:35.120 --> 01:11:40.480]   talking to the room, the room will recognize our room being that there are going to be
[01:11:40.480 --> 01:11:43.800]   devices either on our body or in the room.
[01:11:43.800 --> 01:11:45.040]   It will recognize who we are.
[01:11:45.040 --> 01:11:49.560]   It will be a artificial intelligence virtual assistant and that this will be the main way
[01:11:49.560 --> 01:11:51.840]   that most people interact with computers.
[01:11:51.840 --> 01:11:54.280]   It's perfectly inevitable as far as I'm...
[01:11:54.280 --> 01:11:55.280]   Whether...
[01:11:55.280 --> 01:11:57.480]   Just like going blind with Appogmented Reality is.
[01:11:57.480 --> 01:12:00.080]   Whether we like it or not, I think these are happening.
[01:12:00.080 --> 01:12:01.080]   They're going to...
[01:12:01.080 --> 01:12:02.640]   Even just as you basically said that.
[01:12:02.640 --> 01:12:03.640]   You said...
[01:12:03.640 --> 01:12:05.880]   Well, it's bad for your eyes, but so what?
[01:12:05.880 --> 01:12:08.680]   Well, but again, the point of a future...
[01:12:08.680 --> 01:12:10.760]   My job is not to make predictions.
[01:12:10.760 --> 01:12:15.480]   My job is to say, based on the data that we've got, these are the scenarios.
[01:12:15.480 --> 01:12:19.200]   And then we make different decisions in the present.
[01:12:19.200 --> 01:12:20.200]   So...
[01:12:20.200 --> 01:12:22.600]   Well, invest in Warby Parker.
[01:12:22.600 --> 01:12:27.440]   Well, and to go back to the eye point, I know that they're going to be...
[01:12:27.440 --> 01:12:35.640]   I've got these glasses here that they add a little bit of magnification and they also
[01:12:35.640 --> 01:12:36.640]   are blue light filtering.
[01:12:36.640 --> 01:12:39.120]   Yeah, I think it's a company called Felix Gray.
[01:12:39.120 --> 01:12:41.640]   Yes, there were a sponsor as was Warby Parker.
[01:12:41.640 --> 01:12:42.640]   Yeah.
[01:12:42.640 --> 01:12:43.640]   There you go.
[01:12:43.640 --> 01:12:45.840]   And I could see sort of...
[01:12:45.840 --> 01:12:51.280]   As we're solving the problems in the now, these companies working together to make this
[01:12:51.280 --> 01:12:52.280]   a possibility.
[01:12:52.280 --> 01:12:53.600]   So you sort of get the whole package.
[01:12:53.600 --> 01:12:58.320]   If you've got glasses that are built in with all the augmented reality stuff, they're also
[01:12:58.320 --> 01:12:59.560]   blue light filtering.
[01:12:59.560 --> 01:13:02.240]   They're also providing a little bit of magnification.
[01:13:02.240 --> 01:13:03.240]   Yeah.
[01:13:03.240 --> 01:13:06.040]   Hey, Amy, does your husband write prescriptions for computer glasses?
[01:13:06.040 --> 01:13:07.040]   He does.
[01:13:07.040 --> 01:13:09.040]   Actually, computer vision syndrome is his thing.
[01:13:09.040 --> 01:13:10.040]   He does.
[01:13:10.040 --> 01:13:11.040]   So he writes a lot of them.
[01:13:11.040 --> 01:13:15.880]   Because I realize I need to go back to my optometrist and say I have regular glasses,
[01:13:15.880 --> 01:13:16.880]   progressives.
[01:13:16.880 --> 01:13:20.560]   But what I notice is when I'm looking at my computer, my desktop, I'm tilting my head
[01:13:20.560 --> 01:13:24.480]   up because I want to use the distance, the reading glasses.
[01:13:24.480 --> 01:13:25.480]   That's right.
[01:13:25.480 --> 01:13:28.800]   So I need to get a new prescription just for arm's length reading.
[01:13:28.800 --> 01:13:30.760]   I wanted to look nice and fancy for you.
[01:13:30.760 --> 01:13:33.320]   I love your glasses.
[01:13:33.320 --> 01:13:36.200]   But these are my everyday glasses.
[01:13:36.200 --> 01:13:37.800]   These are my computer glasses.
[01:13:37.800 --> 01:13:39.560]   Yeah, don't wear those in public.
[01:13:39.560 --> 01:13:40.560]   Yeah.
[01:13:40.560 --> 01:13:41.560]   They're just not as fashionable.
[01:13:41.560 --> 01:13:42.560]   No, no.
[01:13:42.560 --> 01:13:43.560]   But they're totally different prescription.
[01:13:43.560 --> 01:13:44.560]   Yes.
[01:13:44.560 --> 01:13:45.560]   That's what I need.
[01:13:45.560 --> 01:13:46.560]   Yeah.
[01:13:46.560 --> 01:13:48.560]   Does he have an app that I can use so I don't have to...
[01:13:48.560 --> 01:13:49.560]   No, he doesn't have an app.
[01:13:49.560 --> 01:13:51.560]   No, he doesn't have an app.
[01:13:51.560 --> 01:13:55.000]   I need an app for that.
[01:13:55.000 --> 01:13:56.240]   He's rolling my husband.
[01:13:56.240 --> 01:13:57.560]   He's not even here.
[01:13:57.560 --> 01:13:58.560]   Yeah.
[01:13:58.560 --> 01:14:00.800]   But I'm not coming to Washington DC to get computer glasses.
[01:14:00.800 --> 01:14:01.800]   That's right.
[01:14:01.800 --> 01:14:02.800]   But I do need that.
[01:14:02.800 --> 01:14:04.800]   Now, Felix Gray makes them for people who don't wear prescriptions.
[01:14:04.800 --> 01:14:05.800]   Yeah.
[01:14:05.800 --> 01:14:09.440]   But what I need is I need my prescription, but I need a special one.
[01:14:09.440 --> 01:14:14.960]   So the convergence that we're heading towards and the death of smartphones, birth of whatever
[01:14:14.960 --> 01:14:18.600]   is next, right, is devices that we wear.
[01:14:18.600 --> 01:14:23.560]   And again, we always put the technology ahead of the intersection of that technology and
[01:14:23.560 --> 01:14:24.640]   our biology.
[01:14:24.640 --> 01:14:30.640]   So the fact that the AR glasses or the retinal projection works, we cannot disaggregate from
[01:14:30.640 --> 01:14:33.320]   how does it work with our biology?
[01:14:33.320 --> 01:14:36.760]   And I would say the same question applies to herables, right?
[01:14:36.760 --> 01:14:41.440]   So there are all these different Bluetooth earphones coming to market that do more than
[01:14:41.440 --> 01:14:46.040]   play music and that also double as virtual assistants.
[01:14:46.040 --> 01:14:50.080]   So the question is, how does that augment and change our hearing?
[01:14:50.080 --> 01:14:55.200]   In the case of hearing, that may actually mean that there are no more hearing aids.
[01:14:55.200 --> 01:14:59.400]   Like the primary companies that create hearing aids, which is a very lucrative market and
[01:14:59.400 --> 01:15:03.040]   there's an entire medical field, that may go away.
[01:15:03.040 --> 01:15:09.160]   Well, Starkey and Resound, the American companies who make $6,000 hearing aids are absolutely
[01:15:09.160 --> 01:15:10.960]   looking at ways to enter the...
[01:15:10.960 --> 01:15:12.520]   Well, no, I don't think they are toast.
[01:15:12.520 --> 01:15:13.520]   I think they're not...
[01:15:13.520 --> 01:15:16.360]   Those prices, though, it's going to be $100.
[01:15:16.360 --> 01:15:17.360]   Yeah.
[01:15:17.360 --> 01:15:23.880]   I mean, I have a pair of $120 Bluetooth earphones that double...
[01:15:23.880 --> 01:15:27.800]   They have digital assistants built in and they don't currently take my biometrics.
[01:15:27.800 --> 01:15:31.960]   I was really disappointed, frankly, when I bought hearing aids because I thought it
[01:15:31.960 --> 01:15:35.720]   would make music sound better and they're optimized for voice.
[01:15:35.720 --> 01:15:37.320]   Music sounds crappy.
[01:15:37.320 --> 01:15:41.600]   And so if I got a pair of AirPods that, by the way, amplified voice but also gave me
[01:15:41.600 --> 01:15:45.120]   full frequency hearing across the spectrum, that would be far better.
[01:15:45.120 --> 01:15:47.600]   I mean, that's the promise of hearing aids.
[01:15:47.600 --> 01:15:48.600]   That's the promise of hearing aids.
[01:15:48.600 --> 01:15:53.000]   You can be hearing aid mode, you can be in crowded room mode, you can be in making a
[01:15:53.000 --> 01:15:54.800]   phone call mode, you can turn off the baby.
[01:15:54.800 --> 01:15:56.320]   Well, my Starkeys will do that.
[01:15:56.320 --> 01:15:59.880]   They tie to the iPhone and they use geolocation and you set them...
[01:15:59.880 --> 01:16:04.840]   Well, whenever I'm at a restaurant, have it sound like this and that does help.
[01:16:04.840 --> 01:16:08.080]   But that's just a tiny step forward in what it will ultimately be.
[01:16:08.080 --> 01:16:11.520]   Huge magic leap forward.
[01:16:11.520 --> 01:16:12.520]   Oh!
[01:16:12.520 --> 01:16:13.520]   You did that.
[01:16:13.520 --> 01:16:16.720]   You gotta turn that into a second.
[01:16:16.720 --> 01:16:19.720]   So if it's not leap holes, would it be leapers or lepers?
[01:16:19.720 --> 01:16:21.520]   It's got to be leapers.
[01:16:21.520 --> 01:16:23.520]   Oh, good.
[01:16:23.520 --> 01:16:27.720]   Chantams go crazy with the privacy thing because they say, "Well, Leo, you're a public
[01:16:27.720 --> 01:16:28.720]   figure.
[01:16:28.720 --> 01:16:30.960]   You don't worry about privacy, but what about the rest of us?"
[01:16:30.960 --> 01:16:33.280]   Well, I think the choice is pretty binary.
[01:16:33.280 --> 01:16:38.880]   Either you have an assistant that listens to you and responds to you and privacy be damned
[01:16:38.880 --> 01:16:39.880]   or you don't.
[01:16:39.880 --> 01:16:41.320]   I mean, what's the interim?
[01:16:41.320 --> 01:16:45.600]   I also think that to a very large extent, these are just solvable problems in terms of
[01:16:45.600 --> 01:16:48.720]   these privacy implications of things that are listening.
[01:16:48.720 --> 01:16:52.600]   So you make laws, GDPR, will that solve it?
[01:16:52.600 --> 01:16:54.000]   I don't know that that'll be necessary.
[01:16:54.000 --> 01:16:55.720]   Hopefully the market will solve that sort of thing.
[01:16:55.720 --> 01:17:00.760]   I mean, Apple has claimed that so much of their processing of Siri and all that stuff
[01:17:00.760 --> 01:17:01.760]   is going to be a little bit...
[01:17:01.760 --> 01:17:06.960]   Because Apple's still storing and sending to the server and storing Siri, unlike Google,
[01:17:06.960 --> 01:17:13.800]   they don't let you see the stored recordings and should a government...
[01:17:13.800 --> 01:17:16.640]   Three-letter agency come to them and say, "We'd like them."
[01:17:16.640 --> 01:17:18.000]   Is Apple going to say no?
[01:17:18.000 --> 01:17:19.000]   They can't.
[01:17:19.000 --> 01:17:22.440]   Either way, it's a solvable problem.
[01:17:22.440 --> 01:17:24.440]   There's nothing inherent about technology.
[01:17:24.440 --> 01:17:26.760]   I shouldn't say the A word.
[01:17:26.760 --> 01:17:27.760]   I'm sorry.
[01:17:27.760 --> 01:17:36.000]   Apple-like devices or the wearable counterparts that is particularly invashing of privacy.
[01:17:36.000 --> 01:17:39.120]   You can solve that problem if there's will in the public.
[01:17:39.120 --> 01:17:45.560]   If the public insists that their privacy, which they may not, then the industry can make
[01:17:45.560 --> 01:17:46.560]   that happen.
[01:17:46.560 --> 01:17:51.280]   But I wanted to raise a really fun issue that Amy brought up this idea that the general
[01:17:51.280 --> 01:17:58.080]   idea that we keep advancing technology and our biology or IDNA doesn't advance along
[01:17:58.080 --> 01:17:59.080]   with it.
[01:17:59.080 --> 01:18:03.120]   It's always being confronted by these biologically incompatible technologies.
[01:18:03.120 --> 01:18:06.320]   There's also the psychological incompatibility.
[01:18:06.320 --> 01:18:13.920]   I wrote a column some time ago just exploring the theme that the psychological impact of
[01:18:13.920 --> 01:18:18.240]   mixed reality is going to be kind of crazy.
[01:18:18.240 --> 01:18:23.000]   Since the beginning of time, human beings have always thought that the believed in volcano
[01:18:23.000 --> 01:18:27.480]   gods and their ghosts inhabiting everything and spirits and angels and all this kind of
[01:18:27.480 --> 01:18:28.480]   stuff.
[01:18:28.480 --> 01:18:31.600]   We always believe this stuff and lots and lots of people still believe in those kinds of
[01:18:31.600 --> 01:18:32.600]   things.
[01:18:32.600 --> 01:18:34.720]   Many of us don't.
[01:18:34.720 --> 01:18:39.000]   But when we're wearing augmented reality glasses all the time, we'll be able to essentially
[01:18:39.000 --> 01:18:43.600]   subscribe to different services or get access to all kinds of stuff.
[01:18:43.600 --> 01:18:50.560]   There will be geo-located virtual objects in our environment everywhere.
[01:18:50.560 --> 01:18:54.000]   So for example, there'll be children like 10, 15 years, there'll be, I predict there'll
[01:18:54.000 --> 01:18:58.200]   be children's versions of augmented reality glasses where kids go walk around and Mickey
[01:18:58.200 --> 01:19:00.720]   Mouse is walking down the street and they've been this fairyland.
[01:19:00.720 --> 01:19:01.720]   How long?
[01:19:01.720 --> 01:19:02.720]   I don't be next year.
[01:19:02.720 --> 01:19:03.720]   I could.
[01:19:03.720 --> 01:19:05.880]   I mean, Tony's got my first AR girl.
[01:19:05.880 --> 01:19:07.480]   Yeah, here's my favorite.
[01:19:07.480 --> 01:19:09.240]   And this is already happening actually.
[01:19:09.240 --> 01:19:12.400]   And I'll tell how it's happening exactly in just a second.
[01:19:12.400 --> 01:19:17.480]   But basically right now if somebody dies in a car accident, for example, the family or
[01:19:17.480 --> 01:19:22.520]   loved ones will build a little shrine, flowers and stuff in the roadside to commemorate that
[01:19:22.520 --> 01:19:23.520]   person.
[01:19:23.520 --> 01:19:28.960]   I believe that in the future, the companies and virtual shrines, virtual shrines.
[01:19:28.960 --> 01:19:31.600]   And so the hologram of the person will be standing there.
[01:19:31.600 --> 01:19:33.800]   So you'll come to the studio and I'll be floating there.
[01:19:33.800 --> 01:19:34.800]   You will.
[01:19:34.800 --> 01:19:35.800]   And I'll say, who's the force?
[01:19:35.800 --> 01:19:38.680]   I wish you'd never been to great health for many decades to come, Leo.
[01:19:38.680 --> 01:19:41.200]   But there's actually a Japanese company that already does this.
[01:19:41.200 --> 01:19:42.200]   What?
[01:19:42.200 --> 01:19:44.400]   They put it at the grave site of hologram of the deceased.
[01:19:44.400 --> 01:19:46.960]   And in what way is that not a ghost?
[01:19:46.960 --> 01:19:47.960]   It's a ghost.
[01:19:47.960 --> 01:19:48.960]   In what way is that not?
[01:19:48.960 --> 01:19:49.960]   That's a ghost.
[01:19:49.960 --> 01:19:51.240]   Psychologically it's a ghost.
[01:19:51.240 --> 01:19:55.200]   And so I don't think we're psychologically ready for augmented reality either.
[01:19:55.200 --> 01:19:57.200]   And I don't think we've even thought about it.
[01:19:57.200 --> 01:20:04.920]   My, so I teach a, I teach an MBA level, um, futures forecasting class at NYU.
[01:20:04.920 --> 01:20:09.920]   And one of the final, so the final project is like the students, um, get to pick, they
[01:20:09.920 --> 01:20:12.280]   can forecast the future of whatever they want.
[01:20:12.280 --> 01:20:17.320]   I had a group that forecasts the future of dating using VR and mixed reality.
[01:20:17.320 --> 01:20:20.960]   And part of the job was to figure out the technology over the next 10 years, which was
[01:20:20.960 --> 01:20:22.040]   totally plausible.
[01:20:22.040 --> 01:20:28.160]   But Mike, you're on to something because we started cycling through all of these scenarios
[01:20:28.160 --> 01:20:32.320]   in which it totally makes sense to go out on your first date in a virtual, you know, virtual
[01:20:32.320 --> 01:20:33.320]   environment.
[01:20:33.320 --> 01:20:35.320]   It saves time.
[01:20:35.320 --> 01:20:41.480]   It broadens your dating pool from your local community to the world.
[01:20:41.480 --> 01:20:42.480]   Right?
[01:20:42.480 --> 01:20:47.280]   Um, but then they described all of these, I think plausible scenarios in which it becomes
[01:20:47.280 --> 01:20:52.960]   easy, more convenient, um, better to, to date in a virtual environment.
[01:20:52.960 --> 01:20:55.760]   And then when you, you know, what happens when you see the actual person?
[01:20:55.760 --> 01:20:56.760]   Yeah, you don't want to.
[01:20:56.760 --> 01:20:58.600]   So just stay virtual.
[01:20:58.600 --> 01:21:01.920]   The good news is that during the date, if it gets awkward, you can just swipe them
[01:21:01.920 --> 01:21:02.920]   away.
[01:21:02.920 --> 01:21:10.720]   Oh my Lord, I think you're all, I think already, uh, I spent the holiday week, uh, traveling
[01:21:10.720 --> 01:21:16.320]   to the British Virgin Islands with my 23 year old son and his 23 year old best friend guy.
[01:21:16.320 --> 01:21:22.800]   And I, I, it's very clear to me that apps like, uh, uh, Tinder have completely changed
[01:21:22.800 --> 01:21:26.920]   their attitude toward dating and sex and relationships.
[01:21:26.920 --> 01:21:31.560]   Neither of them have serious relationships and they have no interest in serious relationships.
[01:21:31.560 --> 01:21:35.400]   They, uh, don't need to have serious relationships.
[01:21:35.400 --> 01:21:40.680]   The CDC has a new study out that I think I saw on end gadget also, but, um, yeah, I
[01:21:40.680 --> 01:21:47.760]   mean, like people who are sub 25 or not hooking up, uh, like they used to, which runs contrary
[01:21:47.760 --> 01:21:51.600]   to the notion that if Tinder exists, people will hook up more, right?
[01:21:51.600 --> 01:21:54.000]   So that wound up being the opposite of what he thought.
[01:21:54.000 --> 01:22:01.440]   Well, they, I mean, we talked fairly bluntly about their experience versus my experience.
[01:22:01.440 --> 01:22:04.240]   And they have much more experience than I do.
[01:22:04.240 --> 01:22:07.200]   I mean, they, it's a, it's a complete.
[01:22:07.200 --> 01:22:08.560]   And I think that they're very common.
[01:22:08.560 --> 01:22:12.280]   I asked them, well, do you, do your friends have girlfriends?
[01:22:12.280 --> 01:22:13.680]   Mom, mostly not.
[01:22:13.680 --> 01:22:16.600]   Uh, and I, and I think that that's really an intro.
[01:22:16.600 --> 01:22:20.360]   And I've observed this with other, uh, young people, they were really is a different world.
[01:22:20.360 --> 01:22:25.440]   And I think you can absolutely trace that to social networks, uh, dating apps.
[01:22:25.440 --> 01:22:26.680]   It's just a different world for them.
[01:22:26.680 --> 01:22:30.560]   And I, I, I, it's interesting like hearing about this.
[01:22:30.560 --> 01:22:32.560]   Because most of it I've had to hear from friends.
[01:22:32.560 --> 01:22:33.560]   I think I'm an outlier.
[01:22:33.560 --> 01:22:34.920]   Do you have a girlfriend, Micah?
[01:22:34.920 --> 01:22:41.200]   Uh, no, I'm 25 and I have been in a relationship with a guy for five years now.
[01:22:41.200 --> 01:22:44.080]   Uh, we've been together for five years just as of January 1st.
[01:22:44.080 --> 01:22:47.000]   Now, see, isn't this funny because he hasn't heard about Tinder, I guess.
[01:22:47.000 --> 01:22:51.760]   So the notion, the notional aspect, a lot of people would have is, oh, well, if you're
[01:22:51.760 --> 01:22:58.000]   gay, that you, you were the, you invented this whole idea of hooking up in the relationships.
[01:22:58.000 --> 01:23:02.600]   And yet most of the gay people I know are actually in long term relationships.
[01:23:02.600 --> 01:23:05.800]   Most of the straight people I know are not young people.
[01:23:05.800 --> 01:23:06.800]   So there you go.
[01:23:06.800 --> 01:23:07.800]   Five years.
[01:23:07.800 --> 01:23:08.800]   Yeah, but actually ties.
[01:23:08.800 --> 01:23:12.720]   But I mean, in terms of, of what you're saying, certainly with, with friends that I
[01:23:12.720 --> 01:23:17.640]   have, I really do, you know, think that I am an outlier because yeah, I, I see that across
[01:23:17.640 --> 01:23:22.160]   the rest of the spectrum, like using these apps for casual things and, and it doesn't
[01:23:22.160 --> 01:23:23.520]   even have to be, you know, casual.
[01:23:23.520 --> 01:23:24.520]   It's too easy.
[01:23:24.520 --> 01:23:25.520]   It's too easy.
[01:23:25.520 --> 01:23:26.520]   It is.
[01:23:26.520 --> 01:23:27.520]   It's very easy.
[01:23:27.520 --> 01:23:28.520]   It's a very hard day.
[01:23:28.520 --> 01:23:30.520]   Man, you know, why go to the movies?
[01:23:30.520 --> 01:23:35.960]   Well, but it also, this is the year that, that, that a whole bunch of sex, we're going
[01:23:35.960 --> 01:23:37.040]   in a totally different direction.
[01:23:37.040 --> 01:23:38.040]   But this, this is the year.
[01:23:38.040 --> 01:23:42.760]   This show, I, I have, it's funny because I have themes I want to cover and we're not
[01:23:42.760 --> 01:23:43.760]   at all.
[01:23:43.760 --> 01:23:44.760]   Yeah.
[01:23:44.760 --> 01:23:45.760]   But no, because it's fascinating.
[01:23:45.760 --> 01:23:47.160]   I'd rather talk about this.
[01:23:47.160 --> 01:23:48.160]   This week in sex.
[01:23:48.160 --> 01:23:49.800]   Let's talk about robot sex.
[01:23:49.800 --> 01:23:50.800]   It's sex.
[01:23:50.800 --> 01:23:52.800]   So this is the year 2018.
[01:23:52.800 --> 01:23:54.520]   So I've been following this for a while.
[01:23:54.520 --> 01:23:59.000]   There's a, there's a couple of companies that are trying to use machine learning and deep
[01:23:59.000 --> 01:24:05.400]   learning for, with, and combine those with the traditional sex dolls.
[01:24:05.400 --> 01:24:12.240]   And so there are this year coming to market some sex dolls that are, that are smart.
[01:24:12.240 --> 01:24:14.160]   Let me guess Japan.
[01:24:14.160 --> 01:24:15.160]   Japan.
[01:24:15.160 --> 01:24:16.160]   Japan.
[01:24:16.160 --> 01:24:18.160]   So you live there for a while.
[01:24:18.160 --> 01:24:25.720]   What is going, so is Japan kind of a precursor, like a, on the, what's, a harbinger, a harbinger
[01:24:25.720 --> 01:24:26.720]   that sex dolls.
[01:24:26.720 --> 01:24:27.720]   Yeah.
[01:24:27.720 --> 01:24:28.720]   Well, I mean, here's the part that I find interesting.
[01:24:28.720 --> 01:24:34.240]   So they come with a base personality and, and you have rights.
[01:24:34.240 --> 01:24:38.480]   You can choose your base personality as you would the size of her breasts, I guess, and
[01:24:38.480 --> 01:24:40.960]   the style of hair.
[01:24:40.960 --> 01:24:46.120]   And then over time, the system is built to learn from you.
[01:24:46.120 --> 01:24:51.440]   No differently really than your Gmail learns from you the more that you use it.
[01:24:51.440 --> 01:24:52.440]   Right.
[01:24:52.440 --> 01:24:58.360]   But, but that also changes our expectations of, of how we interact with other people.
[01:24:58.360 --> 01:25:02.840]   If essentially we are, you know, not every, not the general public isn't going to buy
[01:25:02.840 --> 01:25:06.720]   and use one of these things, but, you know, for the people who do.
[01:25:06.720 --> 01:25:08.280]   What is, what is it?
[01:25:08.280 --> 01:25:13.680]   Is it, you know, we hear about the pillow brides and all this stuff.
[01:25:13.680 --> 01:25:19.160]   Is there something culturally going on in Japan that maybe because it's a small island
[01:25:19.160 --> 01:25:24.560]   and a lot of people and, I, I, what's going on?
[01:25:24.560 --> 01:25:26.400]   And it is just, that's plain Japan.
[01:25:26.400 --> 01:25:28.000]   Does it explain Japan to me?
[01:25:28.000 --> 01:25:29.720]   And does it, what's with Japan?
[01:25:29.720 --> 01:25:31.000]   Is it applicable to the West?
[01:25:31.000 --> 01:25:32.160]   I guess is the question.
[01:25:32.160 --> 01:25:37.400]   So Japan is a very different, um, there's a, there's a lot about Japan that's fascinating
[01:25:37.400 --> 01:25:39.000]   for many, I find it fascinating.
[01:25:39.000 --> 01:25:40.000]   Absolutely.
[01:25:40.000 --> 01:25:41.000]   Right.
[01:25:41.000 --> 01:25:43.000]   So, I think it's a very different time when I lived there.
[01:25:43.000 --> 01:25:45.120]   I felt like I was transported to the future.
[01:25:45.120 --> 01:25:48.920]   I had seen a smartphone for the first time that was kind of, I have found one in your
[01:25:48.920 --> 01:25:49.920]   book.
[01:25:49.920 --> 01:25:50.920]   Yes.
[01:25:50.920 --> 01:25:51.920]   Right.
[01:25:51.920 --> 01:25:52.920]   Yes.
[01:25:52.920 --> 01:25:55.360]   So there was, there was a lot that they were the world leaders and, and to be fair, they're
[01:25:55.360 --> 01:26:00.040]   probably, I wouldn't say that they are the world leaders when it comes to mobile technology,
[01:26:00.040 --> 01:26:01.880]   but they are rolling out 5G.
[01:26:01.880 --> 01:26:06.240]   They are undisputably the world leaders in robotics and collaborative robotics.
[01:26:06.240 --> 01:26:07.240]   They're AI.
[01:26:07.240 --> 01:26:08.240]   And cheap robotics.
[01:26:08.240 --> 01:26:11.280]   But isn't the birth rate tumbling in Japan?
[01:26:11.280 --> 01:26:12.280]   Right.
[01:26:12.280 --> 01:26:13.520]   So there are some other things going on.
[01:26:13.520 --> 01:26:19.400]   So for a long time, like even when I lived there, a lot of women didn't work.
[01:26:19.400 --> 01:26:22.360]   It's a very homogenous, uh, countries.
[01:26:22.360 --> 01:26:23.840]   There's not a lot of foreigners who live there.
[01:26:23.840 --> 01:26:30.880]   I was an anomaly, um, at that time, you know, so, and there are some, um, you know, the work
[01:26:30.880 --> 01:26:33.720]   ethic is very, very, very strong.
[01:26:33.720 --> 01:26:41.480]   Um, so it's not unusual for people, you know, now to be not married, but they don't really
[01:26:41.480 --> 01:26:47.520]   have the social structure set up for, you know, Tinder and, and J-Date.
[01:26:47.520 --> 01:26:52.240]   And I'll also note that the word privacy in Japan didn't exist until the internet age
[01:26:52.240 --> 01:26:53.560]   and they had to import it.
[01:26:53.560 --> 01:26:54.560]   And it's a loan word.
[01:26:54.560 --> 01:26:55.560]   Pudayibashi.
[01:26:55.560 --> 01:26:58.040]   Like that's the word in Japanese privacy.
[01:26:58.040 --> 01:27:04.520]   So it's a very, very interesting place, but the sex robots, um, you know, part of that
[01:27:04.520 --> 01:27:06.720]   is, is definitely coming out of Japan.
[01:27:06.720 --> 01:27:08.280]   What a world we live in.
[01:27:08.280 --> 01:27:11.080]   We just, I find it so fascinating.
[01:27:11.080 --> 01:27:15.640]   And, uh, you know, I think one of the things that's great about what you do, Mike, and
[01:27:15.640 --> 01:27:21.240]   I wish, you know, I told my kids travel all you can before you have family and responsibilities.
[01:27:21.240 --> 01:27:24.640]   It's, it's amazing to see what's going on.
[01:27:24.640 --> 01:27:29.200]   My advice to the youth of the world is to travel the world, meet human beings and have
[01:27:29.200 --> 01:27:30.200]   sex with them.
[01:27:30.200 --> 01:27:31.200]   Yes.
[01:27:31.200 --> 01:27:32.560]   Forget the apps, forget the robots.
[01:27:32.560 --> 01:27:33.720]   Spread the jeans, kids.
[01:27:33.720 --> 01:27:34.720]   Yes.
[01:27:34.720 --> 01:27:36.320]   Let's take a break on that note.
[01:27:36.320 --> 01:27:40.400]   I want to talk about CES where the germs will be spreading.
[01:27:40.400 --> 01:27:41.640]   Oh, that's going to be lovely.
[01:27:41.640 --> 01:27:42.640]   Later this week.
[01:27:42.640 --> 01:27:44.400]   I don't know about jeans.
[01:27:44.400 --> 01:27:48.400]   I don't think truthfully, uh, CES, this was the same in Comdex.
[01:27:48.400 --> 01:27:50.160]   The casino just shut down the tables.
[01:27:50.160 --> 01:27:51.960]   It's like these guys aren't going to gamble.
[01:27:51.960 --> 01:27:54.760]   But I'm guessing the sex workers probably go home too.
[01:27:54.760 --> 01:28:00.280]   It's just not, it's a bunch of, oh, that's, I saw, I saw some horrible, yeah.
[01:28:00.280 --> 01:28:01.280]   Okay.
[01:28:01.280 --> 01:28:03.520]   It could be, it could be quite the opposite.
[01:28:03.520 --> 01:28:04.520]   Travis Kalanik, let's face it.
[01:28:04.520 --> 01:28:05.520]   I think that's where you are.
[01:28:05.520 --> 01:28:09.520]   He's going and he's got some money because he sold a big chunk is going to get diseases.
[01:28:09.520 --> 01:28:10.520]   Yeah.
[01:28:10.520 --> 01:28:13.120]   Uh, geez, the shows are terrible.
[01:28:13.120 --> 01:28:15.240]   Terrible, terrible Amy.
[01:28:15.240 --> 01:28:16.240]   I apologize.
[01:28:16.240 --> 01:28:18.680]   Oh no, it's, I don't have delicate ears.
[01:28:18.680 --> 01:28:19.680]   You brought it up.
[01:28:19.680 --> 01:28:20.680]   Are you kidding?
[01:28:20.680 --> 01:28:25.840]   Uh, meanwhile I should do a bunch of ads because we're way behind.
[01:28:25.840 --> 01:28:28.200]   Let me talk about rocket mortgage very quickly.
[01:28:28.200 --> 01:28:31.560]   Uh, this episode brought to you by those good folks at quick and loans.
[01:28:31.560 --> 01:28:35.360]   We're actually, we're talking, what is the relationship of Intuit's quickened to quickened
[01:28:35.360 --> 01:28:36.360]   loans?
[01:28:36.360 --> 01:28:42.600]   And what I found out was in 1999 Intuit makers of quickened bought a company, I think was
[01:28:42.600 --> 01:28:46.440]   called Rock that they renamed quickened loans.
[01:28:46.440 --> 01:28:50.360]   And then three years later, Dan Gilbert, who was still the CEO of quick and loans, bought
[01:28:50.360 --> 01:28:51.360]   it back.
[01:28:51.360 --> 01:28:52.960]   So they were only in 2002.
[01:28:52.960 --> 01:28:56.560]   So they were only related to quickened, but they have the name and the name is a good
[01:28:56.560 --> 01:28:57.560]   name.
[01:28:57.560 --> 01:29:01.160]   So just so you understand, but really it's the genius of Dan Gilbert who was an amazing
[01:29:01.160 --> 01:29:02.160]   fellow.
[01:29:02.160 --> 01:29:03.160]   They're re, they're in Detroit.
[01:29:03.160 --> 01:29:04.920]   They're revitalizing downtown Detroit.
[01:29:04.920 --> 01:29:06.320]   This guy is a technologist.
[01:29:06.320 --> 01:29:10.960]   Uh, they have become very rapidly the, the, one of the top lenders.
[01:29:10.960 --> 01:29:14.520]   I think they're the number two lender right after that big bank with the stage coach in
[01:29:14.520 --> 01:29:17.000]   America, 92 billion dollars in home loans.
[01:29:17.000 --> 01:29:19.400]   They are number one in customer satisfaction.
[01:29:19.400 --> 01:29:23.360]   Look at this eight consecutive years, number one from JD power.
[01:29:23.360 --> 01:29:28.000]   And that's in a mortgage origination, number one for four consecutive years.
[01:29:28.000 --> 01:29:32.560]   They just got it in 2017 for both categories, number one for four consecutive years in mortgage
[01:29:32.560 --> 01:29:33.560]   servicing.
[01:29:33.560 --> 01:29:35.080]   So they're great lender.
[01:29:35.080 --> 01:29:37.080]   And Dan is so technically sophisticated.
[01:29:37.080 --> 01:29:40.880]   I love it because he said, you know what, that we need a technological revolution in
[01:29:40.880 --> 01:29:42.560]   the loan business.
[01:29:42.560 --> 01:29:43.560]   And they created rocket mortgage.
[01:29:43.560 --> 01:29:48.200]   And the reason they buy ads on this show is because they know you people would love to
[01:29:48.200 --> 01:29:52.400]   not, I don't know what Bigfoot's doing in there on their website, but you people who
[01:29:52.400 --> 01:29:58.160]   listen to this show do not want to go to a bank, put on a suit and tie, go to a bank,
[01:29:58.160 --> 01:30:04.960]   fill out forms, watch a guy with a calculator and a chief of loan information, prick a loan
[01:30:04.960 --> 01:30:09.240]   for them and then go home and go through the attic and find bank statements and pay.
[01:30:09.240 --> 01:30:10.240]   We don't want to do that.
[01:30:10.240 --> 01:30:11.240]   We just, we got a computer.
[01:30:11.240 --> 01:30:12.240]   What do I need that for?
[01:30:12.240 --> 01:30:13.240]   So they created rocket mortgage.
[01:30:13.240 --> 01:30:15.320]   You could do it on your phone.
[01:30:15.320 --> 01:30:16.920]   You could do it on your computer or on your tablet.
[01:30:16.920 --> 01:30:18.480]   You can do it.
[01:30:18.480 --> 01:30:23.620]   You could literally be at an open house and say, out of nowhere, I want to buy this house.
[01:30:23.620 --> 01:30:26.600]   Go to rocketmortgage.com/twit2.
[01:30:26.600 --> 01:30:27.760]   That's the URL.
[01:30:27.760 --> 01:30:29.120]   So they know you'd heard it here.
[01:30:29.120 --> 01:30:31.280]   Rocketmortgage.com/twit2.
[01:30:31.280 --> 01:30:34.520]   And in a minute or two, you can tell them some basic information that's already in your
[01:30:34.520 --> 01:30:35.520]   head.
[01:30:35.520 --> 01:30:36.840]   You don't have to go find anything.
[01:30:36.840 --> 01:30:39.480]   They have trusted relationships with all the financial institutions.
[01:30:39.480 --> 01:30:41.480]   So they get the other information they need.
[01:30:41.480 --> 01:30:46.360]   They crunch the numbers while you're there holding the phone right in front of you.
[01:30:46.360 --> 01:30:49.720]   And based on income assets and credits, then they say, okay, here's the loans you qualify
[01:30:49.720 --> 01:30:50.720]   for.
[01:30:50.720 --> 01:30:51.720]   You choose the rate.
[01:30:51.720 --> 01:30:52.720]   You choose the term.
[01:30:52.720 --> 01:30:53.720]   You choose the down payment.
[01:30:53.720 --> 01:30:54.720]   You get the loan.
[01:30:54.720 --> 01:30:58.640]   I mean, three minutes later, you're done and you get a big button, which I love, which you
[01:30:58.640 --> 01:30:59.640]   could show the realtor.
[01:30:59.640 --> 01:31:00.640]   I'm approved.
[01:31:00.640 --> 01:31:05.160]   By the way, we want this house and by that is a huge advantage.
[01:31:05.160 --> 01:31:07.800]   If you're buying a home, take it from me.
[01:31:07.800 --> 01:31:12.040]   If you go there, pre-approved for the loan, you're front of the line.
[01:31:12.040 --> 01:31:13.040]   The only person is going to beat you.
[01:31:13.040 --> 01:31:16.440]   Somebody's paying cash and you can't beat that.
[01:31:16.440 --> 01:31:20.080]   Sorry, but most of the time, that's not a problem.
[01:31:20.080 --> 01:31:21.880]   Rocketmortgage.com/twit2.
[01:31:21.880 --> 01:31:26.160]   And by the way, good time to refinance and they'll do that as well.
[01:31:26.160 --> 01:31:28.160]   Interest rates are only going to go up.
[01:31:28.160 --> 01:31:30.200]   Go to rocketmortgage.com/twit2.
[01:31:30.200 --> 01:31:31.720]   Apply simply.
[01:31:31.720 --> 01:31:35.240]   Understand fully and then mortgage confidently at Rocketmortgage.
[01:31:35.240 --> 01:31:45.720]   Equal housing lender licensed in all 50 states and MLSconsumeraccess.org 3030rocketmortgage.com/twit2.
[01:31:45.720 --> 01:31:47.320]   We thank them so much for their support.
[01:31:47.320 --> 01:31:52.520]   And the tip of the hat to Dan, who is one of the smartest guys, a great entrepreneur
[01:31:52.520 --> 01:31:53.800]   out there.
[01:31:53.800 --> 01:31:54.800]   We had a fun week.
[01:31:54.800 --> 01:31:56.040]   Lots of good stuff.
[01:31:56.040 --> 01:32:01.560]   Our first week back for 2018 and we've made a little synopsis just in case you missed anything
[01:32:01.560 --> 01:32:02.560]   watch.
[01:32:02.560 --> 01:32:04.560]   Previously on Twit.
[01:32:04.560 --> 01:32:05.560]   Oh, look!
[01:32:05.560 --> 01:32:07.560]   Oh, look at that.
[01:32:07.560 --> 01:32:13.720]   It's 3,035.5 megabytes per second.
[01:32:13.720 --> 01:32:14.720]   Right.
[01:32:14.720 --> 01:32:16.920]   2,491.2 read.
[01:32:16.920 --> 01:32:17.920]   Wow.
[01:32:17.920 --> 01:32:21.880]   I need one.
[01:32:21.880 --> 01:32:28.400]   This week in computer hardware, giant vulnerability security style is revealed that involves every
[01:32:28.400 --> 01:32:31.040]   single processor practically ever.
[01:32:31.040 --> 01:32:32.520]   I'm exaggerating slightly.
[01:32:32.520 --> 01:32:34.400]   I like how core doctor put it.
[01:32:34.400 --> 01:32:38.560]   Virtually every modern computer is vulnerable to a pair of devastating attacks.
[01:32:38.560 --> 01:32:40.360]   And there's only a fix for one of them.
[01:32:40.360 --> 01:32:41.840]   And it sucks.
[01:32:41.840 --> 01:32:43.360]   iOS Today.
[01:32:43.360 --> 01:32:45.760]   The Star Wars App.
[01:32:45.760 --> 01:32:46.760]   This was one of my favorite lines.
[01:32:46.760 --> 01:32:47.760]   The time is time.
[01:32:47.760 --> 01:32:48.760]   Kill it.
[01:32:48.760 --> 01:32:50.280]   Does it have my favorite?
[01:32:50.280 --> 01:32:54.000]   I want to put my fists for this beautiful city.
[01:32:54.000 --> 01:32:55.880]   What?
[01:32:55.880 --> 01:32:59.680]   You know what didn't Luke look like the dude in this movie?
[01:32:59.680 --> 01:33:01.320]   Just breathe, man.
[01:33:01.320 --> 01:33:02.320]   The new screen savers.
[01:33:02.320 --> 01:33:03.960]   Here's Desai Azane here.
[01:33:03.960 --> 01:33:04.960]   Biohacker.
[01:33:04.960 --> 01:33:05.960]   So you ate poop?
[01:33:05.960 --> 01:33:08.360]   I'm sorry, I keep getting back to that.
[01:33:08.360 --> 01:33:11.760]   But you can't bring it up and then not talk about it.
[01:33:11.760 --> 01:33:16.160]   You introduce new bacteria into your biome and change the weight or body processes.
[01:33:16.160 --> 01:33:17.160]   How much better weight it looks.
[01:33:17.160 --> 01:33:18.160]   Exactly.
[01:33:18.160 --> 01:33:19.160]   Yeah, it's really interesting.
[01:33:19.160 --> 01:33:20.160]   Twit.
[01:33:20.160 --> 01:33:23.960]   Learn something new every day and what it means.
[01:33:23.960 --> 01:33:26.120]   But don't eat poop because that's nuts.
[01:33:26.120 --> 01:33:30.000]   So you ate poop?
[01:33:30.000 --> 01:33:35.160]   You might know it better as a fecal transplant, which doesn't frankly make it sound any better.
[01:33:35.160 --> 01:33:37.040]   Still bad.
[01:33:37.040 --> 01:33:38.840]   Still not.
[01:33:38.840 --> 01:33:39.840]   Good.
[01:33:39.840 --> 01:33:40.840]   Mm.
[01:33:40.840 --> 01:33:41.840]   All right.
[01:33:41.840 --> 01:33:42.840]   CES, who's going?
[01:33:42.840 --> 01:33:44.760]   Show of hands.
[01:33:44.760 --> 01:33:45.920]   Nobody.
[01:33:45.920 --> 01:33:52.960]   You know, I feel bad saying this because those of us who go and you started as I did with
[01:33:52.960 --> 01:34:00.400]   Comdex, by the way, we can thank Comdex for Trump.
[01:34:00.400 --> 01:34:01.400]   What?
[01:34:01.400 --> 01:34:02.400]   Okay.
[01:34:02.400 --> 01:34:03.400]   Follow this.
[01:34:03.400 --> 01:34:04.760]   I want to hear this.
[01:34:04.760 --> 01:34:07.640]   So Comdex was owned by a guy named Sheldon Adelson.
[01:34:07.640 --> 01:34:08.640]   Right.
[01:34:08.640 --> 01:34:09.640]   Shelly.
[01:34:09.640 --> 01:34:12.600]   In fact, the Comdex awards were called Shelly's.
[01:34:12.600 --> 01:34:13.600]   Remember that?
[01:34:13.600 --> 01:34:14.600]   Yep.
[01:34:14.600 --> 01:34:17.080]   Shelly was smart.
[01:34:17.080 --> 01:34:22.040]   He had 19, I think it was 99 right before the computer bust said, "Hey, I'm going to
[01:34:22.040 --> 01:34:24.560]   sell Comdex."
[01:34:24.560 --> 01:34:25.560]   And sold it.
[01:34:25.560 --> 01:34:28.680]   And of course, it disappeared a couple of years later because it really was the end
[01:34:28.680 --> 01:34:34.720]   of the line for that and took the money and invested in casinos in Macau, which made
[01:34:34.720 --> 01:34:36.440]   him one of the richest men in the world.
[01:34:36.440 --> 01:34:39.920]   And he is a big donor, was a big donor to Trump.
[01:34:39.920 --> 01:34:45.480]   And he's a big, one of the reasons that the effect I should have said, thanks to Comdex,
[01:34:45.480 --> 01:34:50.120]   the US embassy is now in Jerusalem because that was his, that would really be the direct
[01:34:50.120 --> 01:34:51.120]   connection.
[01:34:51.120 --> 01:34:51.960]   Wow.
[01:34:51.960 --> 01:34:52.960]   Anyway, Comdex ended.
[01:34:52.960 --> 01:34:53.960]   Why did it end?
[01:34:53.960 --> 01:34:55.960]   I have no idea.
[01:34:55.960 --> 01:34:56.960]   It was great.
[01:34:56.960 --> 01:34:57.960]   It was boring.
[01:34:57.960 --> 01:35:01.560]   And there was this thing that went on twice a year called the Consumer Electronics Show
[01:35:01.560 --> 01:35:02.840]   that was kind of sexier.
[01:35:02.840 --> 01:35:03.840]   It was fun.
[01:35:03.840 --> 01:35:05.800]   It was fun because it's consumer electronics.
[01:35:05.800 --> 01:35:06.800]   It's not.
[01:35:06.800 --> 01:35:09.000]   What did Dell do this year?
[01:35:09.000 --> 01:35:13.880]   And Bill Gates used to give these great keynotes at Comdex the most boring thing.
[01:35:13.880 --> 01:35:17.480]   But he knew he was boring, so Bill would always make great videos.
[01:35:17.480 --> 01:35:18.480]   Right.
[01:35:18.480 --> 01:35:22.200]   So much fun if we could get those videos, you can't because they didn't secure rights
[01:35:22.200 --> 01:35:23.720]   for public.
[01:35:23.720 --> 01:35:26.440]   But remember the one with him and Steve Ballmer driving a Volkswagen?
[01:35:26.440 --> 01:35:27.440]   Yes.
[01:35:27.440 --> 01:35:33.360]   That, that, that, they're driving around and then they see a son microsystem computer on
[01:35:33.360 --> 01:35:38.600]   the curve because it's a parody of an ad where they, where some college kids see a so
[01:35:38.600 --> 01:35:42.640]   fun, and they put it in the backseat and they're driving around and they smell, smells
[01:35:42.640 --> 01:35:49.680]   and they throw a back on the curve that that that that was a weird but it was a crazy time.
[01:35:49.680 --> 01:35:50.680]   It was a crazy time, man.
[01:35:50.680 --> 01:35:53.200]   It was wild, wild.
[01:35:53.200 --> 01:35:56.760]   Then CES took over CES, even bigger, I think, than Comdex.
[01:35:56.760 --> 01:36:01.280]   180,000, I think, last year, maybe quarter of a million soon.
[01:36:01.280 --> 01:36:04.320]   Miles of miles, thousands of booths.
[01:36:04.320 --> 01:36:06.880]   The big computer guys, Apple's not there.
[01:36:06.880 --> 01:36:10.000]   Microsoft stopped going until I think still is there.
[01:36:10.000 --> 01:36:11.640]   Microsoft is there, I think.
[01:36:11.640 --> 01:36:13.520]   They go, but they don't have a booth.
[01:36:13.520 --> 01:36:14.520]   So like they have people.
[01:36:14.520 --> 01:36:16.040]   They linger and linger.
[01:36:16.040 --> 01:36:17.880]   If you look around, you can see people.
[01:36:17.880 --> 01:36:23.760]   The other thing that was interesting, Comdex was in multiple halls and the main hall got
[01:36:23.760 --> 01:36:27.560]   less and less interesting and then the fun stuff was in the sands.
[01:36:27.560 --> 01:36:31.360]   It like, you know, like all the way across all the gadgets and things like that.
[01:36:31.360 --> 01:36:35.320]   It's weird companies you never heard of and it would, you know, it would take you all,
[01:36:35.320 --> 01:36:37.960]   you know, days to go through the sands alone.
[01:36:37.960 --> 01:36:41.160]   And really, CES is just the stuff of the sands times five.
[01:36:41.160 --> 01:36:42.160]   Yeah.
[01:36:42.160 --> 01:36:47.280]   I would say everybody loves to hate on CES and everybody loves to hate on South by.
[01:36:47.280 --> 01:36:53.520]   And they've gotten big and unwieldy, but, you know, I'm not going this year.
[01:36:53.520 --> 01:36:54.520]   Last year I was there.
[01:36:54.520 --> 01:36:55.520]   You spoke.
[01:36:55.520 --> 01:36:56.520]   And I do?
[01:36:56.520 --> 01:36:57.520]   I spoke.
[01:36:57.520 --> 01:36:58.520]   Here's what I would say.
[01:36:58.520 --> 01:37:02.320]   I didn't see anything individually like the AI toothbrush.
[01:37:02.320 --> 01:37:06.600]   I didn't like see anything individually that really wowed me.
[01:37:06.600 --> 01:37:12.240]   However, if you go and you're able to sort of as you're walking through, force yourself
[01:37:12.240 --> 01:37:13.240]   to make connections.
[01:37:13.240 --> 01:37:14.240]   Well, I've seen this.
[01:37:14.240 --> 01:37:15.240]   I've seen this.
[01:37:15.240 --> 01:37:16.240]   I've seen this.
[01:37:16.240 --> 01:37:17.240]   That's where the signals are.
[01:37:17.240 --> 01:37:18.240]   Yes.
[01:37:18.240 --> 01:37:19.240]   That's right.
[01:37:19.240 --> 01:37:23.720]   And I think if you, if you on the ground force yourself to take a bird's eye view, you can
[01:37:23.720 --> 01:37:26.720]   actually glean some pretty decent information out.
[01:37:26.720 --> 01:37:31.520]   And there's always a prototype pavilion that's in a different spot.
[01:37:31.520 --> 01:37:36.760]   But again, if you sort of, you'll start to see patterns just forcing yourself to walk
[01:37:36.760 --> 01:37:37.760]   around.
[01:37:37.760 --> 01:37:38.760]   But here's the love.
[01:37:38.760 --> 01:37:40.080]   That was always the agenda, at least for me.
[01:37:40.080 --> 01:37:42.840]   And I'm sure for you when you go there is to say, what's the trend?
[01:37:42.840 --> 01:37:43.840]   What's the story?
[01:37:43.840 --> 01:37:45.280]   Is there an overarching story?
[01:37:45.280 --> 01:37:51.160]   Well, the story is going to be like, Oh, bigger TVs or OLED or whatever.
[01:37:51.160 --> 01:37:54.280]   You know, that's actually not the, yes, that's fine.
[01:37:54.280 --> 01:37:59.520]   But what you're, what you should try to force yourself to do is what does this imply, right?
[01:37:59.520 --> 01:38:00.520]   Right.
[01:38:00.520 --> 01:38:04.840]   The problem is that the problem is that it's doing what you're saying is far easier if
[01:38:04.840 --> 01:38:06.080]   you're not there.
[01:38:06.080 --> 01:38:10.200]   So literally every single product is far as for the trees problem.
[01:38:10.200 --> 01:38:11.200]   You're too close.
[01:38:11.200 --> 01:38:15.080]   You're too close and your time is consumed by just slogging from one place to another,
[01:38:15.080 --> 01:38:18.240]   getting from one place totally and trying to find bathroom, whatever.
[01:38:18.240 --> 01:38:23.480]   Whereas when you don't go, you can use, there's a, there's a great site called Google.com
[01:38:23.480 --> 01:38:27.320]   and you just search all the news and make those connections.
[01:38:27.320 --> 01:38:28.320]   It's one or two.
[01:38:28.320 --> 01:38:31.240]   Look at this picture.
[01:38:31.240 --> 01:38:37.080]   And gadget has a giant pink trailer and look what's on the Las Vegas monorail going above.
[01:38:37.080 --> 01:38:41.000]   It's the monorails plastered with the words, Hey, Google.
[01:38:41.000 --> 01:38:44.120]   That's an example of what CES is like.
[01:38:44.120 --> 01:38:49.400]   Part of the problem, just sort of, instead of lobby conning, like, you know, Google,
[01:38:49.400 --> 01:38:55.400]   Google.com conning, if that's such a thing, is you're now relying on everybody else to
[01:38:55.400 --> 01:38:58.720]   tell you what the story was and everybody else is kind of like telling the same story.
[01:38:58.720 --> 01:38:59.720]   Yeah.
[01:38:59.720 --> 01:39:03.480]   This is, but this is where you excel because this is, this is what you do is you kind of
[01:39:03.480 --> 01:39:04.480]   piece.
[01:39:04.480 --> 01:39:05.480]   Yeah.
[01:39:05.480 --> 01:39:09.080]   I'm sure people who are listening to the show, like some of them are going and maybe some
[01:39:09.080 --> 01:39:12.600]   of them are now feeling kind of bad because they were excited about going before.
[01:39:12.600 --> 01:39:14.960]   I guess what I'm saying is like you should be excited.
[01:39:14.960 --> 01:39:15.960]   You're at CES.
[01:39:15.960 --> 01:39:16.960]   It's exciting.
[01:39:16.960 --> 01:39:19.520]   You'll see, you know, all kinds of cool technology.
[01:39:19.520 --> 01:39:21.560]   You will see a bunch of stuff you've already seen.
[01:39:21.560 --> 01:39:25.040]   You know, Ford will probably have some ridiculously huge more mobility booth.
[01:39:25.040 --> 01:39:28.480]   You know, you'll see a whole bunch of products that will never see the light of day.
[01:39:28.480 --> 01:39:29.480]   Yeah.
[01:39:29.480 --> 01:39:30.480]   My favorite moment.
[01:39:30.480 --> 01:39:33.920]   But, you know, my favorite was the happy fork.
[01:39:33.920 --> 01:39:35.240]   Ah, the happy fork.
[01:39:35.240 --> 01:39:37.240]   This was a fork with no point.
[01:39:37.240 --> 01:39:38.240]   Haptic feedback.
[01:39:38.240 --> 01:39:41.160]   Oh, well, there was a hairbrush last year.
[01:39:41.160 --> 01:39:45.920]   There was a smart hairbrush last year that I was just one trend you see is a bunch of
[01:39:45.920 --> 01:39:52.560]   companies, very mostly sweaty guys in bad, ill-fitting suits trying to take two technologies
[01:39:52.560 --> 01:39:55.000]   that don't go together and shove them together.
[01:39:55.000 --> 01:39:58.360]   Two brushes in AI.
[01:39:58.360 --> 01:39:59.360]   That doesn't make so much sense.
[01:39:59.360 --> 01:40:04.520]   But in the Japanese Pivlion, there were a bunch of haptic flatware devices for people
[01:40:04.520 --> 01:40:05.520]   who have Parkinson's.
[01:40:05.520 --> 01:40:06.520]   Oh, that's cool.
[01:40:06.520 --> 01:40:08.440]   And that's not haptic.
[01:40:08.440 --> 01:40:10.440]   That's more like stabilization.
[01:40:10.440 --> 01:40:11.440]   Right.
[01:40:11.440 --> 01:40:12.440]   Right.
[01:40:12.440 --> 01:40:14.280]   So your hand is shaking, but the forks stay still.
[01:40:14.280 --> 01:40:15.280]   That's remarkable.
[01:40:15.280 --> 01:40:16.280]   Yeah.
[01:40:16.280 --> 01:40:17.280]   Yeah.
[01:40:17.280 --> 01:40:20.440]   So, I mean, you know, the thing has got huge and unwieldy and like, it's still, I think,
[01:40:20.440 --> 01:40:24.040]   the case that some of the more interesting, what's left of the interesting stuff is still
[01:40:24.040 --> 01:40:27.760]   like somewhere in a room on the Bellagio, like on the 30th floor.
[01:40:27.760 --> 01:40:29.160]   It's never on the show floor.
[01:40:29.160 --> 01:40:33.640]   Although LG says we're going to show an 88 inch 8K T.S.
[01:40:33.640 --> 01:40:34.720]   Get ready for it.
[01:40:34.720 --> 01:40:36.120]   You just got your 4K TV.
[01:40:36.120 --> 01:40:37.760]   Oh, that's sold hat 8K.
[01:40:37.760 --> 01:40:38.760]   Yeah.
[01:40:38.760 --> 01:40:39.760]   That's a lot of K.
[01:40:39.760 --> 01:40:40.760]   That's a lot of K.
[01:40:40.760 --> 01:40:42.560]   I was scared to miss so many cases.
[01:40:42.560 --> 01:40:47.600]   Blue ray, sorry, the, yeah, blue ray and HD DVD were both launching.
[01:40:47.600 --> 01:40:48.600]   Yeah.
[01:40:48.600 --> 01:40:52.080]   And I was sitting in the wrong, I oftentimes choose the wrong press conference.
[01:40:52.080 --> 01:40:53.080]   Yeah.
[01:40:53.080 --> 01:40:56.840]   And I was sitting in the Warner Brothers HD DVD press conference.
[01:40:56.840 --> 01:40:59.800]   Some guy walked on stage and said, we're dead.
[01:40:59.800 --> 01:41:02.880]   That there is no more like the whole product is dead.
[01:41:02.880 --> 01:41:03.880]   There's left.
[01:41:03.880 --> 01:41:07.440]   People forget that HD DVD and blue ray were at each other's throats.
[01:41:07.440 --> 01:41:10.440]   And if for a long time, consumers are going, well, which one should I buy there?
[01:41:10.440 --> 01:41:14.440]   Incompatible different movie, movie companies signed up with different ones, but eventually
[01:41:14.440 --> 01:41:16.040]   blue ray, Sony's technology.
[01:41:16.040 --> 01:41:17.040]   Yeah.
[01:41:17.040 --> 01:41:18.560]   So sometimes you do see that was big.
[01:41:18.560 --> 01:41:19.560]   I remember when that happened.
[01:41:19.560 --> 01:41:20.560]   That was huge.
[01:41:20.560 --> 01:41:25.080]   My favorite event from last year, it was sort of a bizarre and funny one.
[01:41:25.080 --> 01:41:32.360]   We were in the sands area and there was this little machine that you could put you because
[01:41:32.360 --> 01:41:37.600]   right now, like on Instagram, you can type in the word nail art and you're going to see
[01:41:37.600 --> 01:41:39.560]   a bunch of stuff, a bunch of ideas, a bunch of fun.
[01:41:39.560 --> 01:41:40.560]   Oh, I know.
[01:41:40.560 --> 01:41:41.560]   I know.
[01:41:41.560 --> 01:41:44.800]   And so I thought it would be funny because my, the calling I was with she heard the nail
[01:41:44.800 --> 01:41:45.800]   printer.
[01:41:45.800 --> 01:41:47.360]   Yeah, she bit her nails.
[01:41:47.360 --> 01:41:50.960]   And so she didn't have like longer nails and I don't bite my nails.
[01:41:50.960 --> 01:41:53.880]   So I walked up and I was like, Hey, we want to shoot a video of this.
[01:41:53.880 --> 01:41:57.080]   Would it be okay if I, you know, try it out?
[01:41:57.080 --> 01:42:02.120]   And there was like a five minute pause while I tried to explain to the woman who was, you
[01:42:02.120 --> 01:42:06.200]   know, running the machine that yes, I am a guy, but I still want to do this because I
[01:42:06.200 --> 01:42:07.720]   want to get video of it.
[01:42:07.720 --> 01:42:10.400]   As she kept saying, but you're a guy.
[01:42:10.400 --> 01:42:11.400]   Yeah.
[01:42:11.400 --> 01:42:15.440]   It's like, I realized that the machine actually worked though or was it?
[01:42:15.440 --> 01:42:19.800]   I got a stuff last year that like didn't actually work, right?
[01:42:19.800 --> 01:42:20.800]   I got a dog.
[01:42:20.800 --> 01:42:22.160]   I got a dog printed on my fingernail.
[01:42:22.160 --> 01:42:23.800]   I've got a video somewhere and I wish I had one.
[01:42:23.800 --> 01:42:24.800]   I'm looking for him right now.
[01:42:24.800 --> 01:42:25.800]   I know.
[01:42:25.800 --> 01:42:27.680]   I think I think I'm on the I'm or page.
[01:42:27.680 --> 01:42:30.200]   Yeah, it should be on the on I'm or Facebook page.
[01:42:30.200 --> 01:42:34.560]   There should be a video there somewhere and probably on the Instagram as well.
[01:42:34.560 --> 01:42:37.800]   But literally it took my finger it put it in.
[01:42:37.800 --> 01:42:42.280]   She, she did like a few base coats or something and like pressed down my cuticle and all that
[01:42:42.280 --> 01:42:43.280]   jazz.
[01:42:43.280 --> 01:42:49.760]   I stuck my finger inside of this like laser jet printer except for nail art and it actually
[01:42:49.760 --> 01:42:53.320]   printed a little Chihuahua onto my fingernail.
[01:42:53.320 --> 01:42:54.320]   It was oh, there we go.
[01:42:54.320 --> 01:42:56.320]   Who wouldn't want that?
[01:42:56.320 --> 01:42:59.280]   Michael Sergeant, this is last year.
[01:42:59.280 --> 01:43:03.440]   Now see, you'll be glad you're not there this year because Neutrogena is announcing a new
[01:43:03.440 --> 01:43:06.240]   iPhone app that will tell you what skin type you have.
[01:43:06.240 --> 01:43:07.240]   Yeah.
[01:43:07.240 --> 01:43:08.240]   Yeah.
[01:43:08.240 --> 01:43:10.400]   Not just to be fair.
[01:43:10.400 --> 01:43:12.200]   She said I had that 10 years ago.
[01:43:12.200 --> 01:43:14.040]   Oh, such old hat.
[01:43:14.040 --> 01:43:16.360]   Sado had they gave you a little lens.
[01:43:16.360 --> 01:43:20.240]   You stuck it on your iPhone and you could put it on your skin.
[01:43:20.240 --> 01:43:21.240]   Yeah.
[01:43:21.240 --> 01:43:25.160]   Oh, nice nail art like a nice nail art.
[01:43:25.160 --> 01:43:26.160]   It's a little Chihuahua.
[01:43:26.160 --> 01:43:32.520]   That's a little bit too much like that box in a dune stick your hand in it.
[01:43:32.520 --> 01:43:33.520]   Yeah.
[01:43:33.520 --> 01:43:34.880]   Are you the quiz that Chaturac?
[01:43:34.880 --> 01:43:36.760]   I think you are.
[01:43:36.760 --> 01:43:37.760]   I think you are.
[01:43:37.760 --> 01:43:38.760]   All right.
[01:43:38.760 --> 01:43:39.760]   What else is going to be.
[01:43:39.760 --> 01:43:44.120]   It does show how far CES is, how much it's changed because if you think in the earlier
[01:43:44.120 --> 01:43:50.120]   days it was components and CPUs and now you can vibrating hairbrush.
[01:43:50.120 --> 01:43:51.120]   Yeah.
[01:43:51.120 --> 01:43:56.280]   Honnas will show a bunch of bots, mobility bots.
[01:43:56.280 --> 01:43:57.920]   These are concepts, unfortunately.
[01:43:57.920 --> 01:44:00.480]   This is in gadgets paid here for $1,000.
[01:44:00.480 --> 01:44:04.320]   Our VUSIX is AR glasses with.
[01:44:04.320 --> 01:44:05.320]   With Echo.
[01:44:05.320 --> 01:44:06.320]   Echo.
[01:44:06.320 --> 01:44:07.320]   Echo.
[01:44:07.320 --> 01:44:09.720]   Echo glasses, those are the existing glasses.
[01:44:09.720 --> 01:44:10.880]   We haven't seen the real ones.
[01:44:10.880 --> 01:44:11.880]   Right.
[01:44:11.880 --> 01:44:12.880]   Those are horrific.
[01:44:12.880 --> 01:44:13.880]   Yeah.
[01:44:13.880 --> 01:44:14.880]   $1,000 is too much.
[01:44:14.880 --> 01:44:16.680]   Here's the 3D pen printer, which has been around for years.
[01:44:16.680 --> 01:44:17.680]   Still nobody wants it.
[01:44:17.680 --> 01:44:18.680]   Yeah.
[01:44:18.680 --> 01:44:19.680]   See that for a little.
[01:44:19.680 --> 01:44:20.680]   No.
[01:44:20.680 --> 01:44:27.320]   LG service robots, a serving robot, a Porter robot and a shopping cart robot.
[01:44:27.320 --> 01:44:31.240]   Here's that 88 inch display.
[01:44:31.240 --> 01:44:35.640]   It does 8K but of course as always nobody has 8K content.
[01:44:35.640 --> 01:44:36.640]   No.
[01:44:36.640 --> 01:44:37.640]   Some new roll up TV.
[01:44:37.640 --> 01:44:38.640]   Yes.
[01:44:38.640 --> 01:44:40.960]   There was a Samsung as a roll up TV.
[01:44:40.960 --> 01:44:47.200]   Here's the Neutrogena skin scanner concept iPhone accessory.
[01:44:47.200 --> 01:44:50.400]   You know, why do they show somebody with flawless skin doing it?
[01:44:50.400 --> 01:44:52.320]   Because all they're going to say is you.
[01:44:52.320 --> 01:44:53.760]   The bomb brand.
[01:44:53.760 --> 01:44:55.800]   You are perfect.
[01:44:55.800 --> 01:44:57.360]   You are perfect.
[01:44:57.360 --> 01:44:58.360]   No, it's a.
[01:44:58.360 --> 01:44:59.360]   It's a.
[01:44:59.360 --> 01:45:00.360]   Or at least that part of your forehead.
[01:45:00.360 --> 01:45:02.760]   If you look at it close enough, it's not.
[01:45:02.760 --> 01:45:04.760]   No skin is perfect.
[01:45:04.760 --> 01:45:12.440]   My, our friend Scott Wilkinson, who's our Aviguru says and he was ND8 so he couldn't
[01:45:12.440 --> 01:45:13.760]   tell us.
[01:45:13.760 --> 01:45:15.640]   There is a new TV technology.
[01:45:15.640 --> 01:45:21.120]   You know, it's kind of come down now to LCDs whether they're LED backlit or not LCDs and
[01:45:21.120 --> 01:45:28.840]   there's the quantum dot LCDs or OLED, which I like a lot OLED.
[01:45:28.840 --> 01:45:30.480]   But apparently there's something new.
[01:45:30.480 --> 01:45:31.640]   It's neither of the.
[01:45:31.640 --> 01:45:32.640]   Another one.
[01:45:32.640 --> 01:45:38.440]   And we've if you go to these every year, it's been a couple of years since we've had a new
[01:45:38.440 --> 01:45:41.480]   one, but there's there's for a long time there were these technologies they'd announce
[01:45:41.480 --> 01:45:43.000]   and they couldn't make.
[01:45:43.000 --> 01:45:45.520]   No, so we don't know.
[01:45:45.520 --> 01:45:46.520]   That's just fun.
[01:45:46.520 --> 01:45:47.520]   We'll cover it.
[01:45:47.520 --> 01:45:48.520]   Father Robert's down there.
[01:45:48.520 --> 01:45:49.520]   Scott Wilkinson's down there.
[01:45:49.520 --> 01:45:52.280]   Ticte Bartolo, the giz with his down there.
[01:45:52.280 --> 01:45:57.840]   He always goes to the back alleys and looks for the stuff, the junky stuff.
[01:45:57.840 --> 01:45:59.240]   We'll have some coverage effect.
[01:45:59.240 --> 01:46:03.320]   I think Father Robert is co-hosting the new screensavers with me next Saturday.
[01:46:03.320 --> 01:46:08.240]   So we'll have his his report from CES.
[01:46:08.240 --> 01:46:12.120]   Anybody want to make, you know, anybody got predictions or anything, you know, like your
[01:46:12.120 --> 01:46:15.120]   ex none of us are going.
[01:46:15.120 --> 01:46:20.680]   What will we roll up TV is really cool, but I like that that's something that, you know,
[01:46:20.680 --> 01:46:24.800]   we keep seeing people showing, Oh, this folds now and I've got a I've got a phone that's
[01:46:24.800 --> 01:46:26.800]   got three screens that fold out.
[01:46:26.800 --> 01:46:30.440]   I think all of that technology is really cool, but I mean, no predictions.
[01:46:30.440 --> 01:46:31.440]   This is hell Jesus.
[01:46:31.440 --> 01:46:32.680]   But what are you going to do?
[01:46:32.680 --> 01:46:37.200]   Roll up your 70 inch TV, put it under your arm and I mean, what's the point?
[01:46:37.200 --> 01:46:38.200]   Right.
[01:46:38.200 --> 01:46:39.200]   I mean, I.
[01:46:39.200 --> 01:46:42.840]   Well, that's not not that device, but I'm working on it.
[01:46:42.840 --> 01:46:47.320]   I can't disclose which one, but I'm working on a television show that's going to launch
[01:46:47.320 --> 01:46:50.880]   in a couple of months and it's set in the year 2030.
[01:46:50.880 --> 01:46:53.080]   I could probably see.
[01:46:53.080 --> 01:46:55.280]   Will we be able to roll the show up?
[01:46:55.280 --> 01:46:58.600]   Well, so what are the so I'm working on all of the like everyday life stuff.
[01:46:58.600 --> 01:47:00.720]   It's it's Bo Willemons new show.
[01:47:00.720 --> 01:47:01.720]   Oh my God.
[01:47:01.720 --> 01:47:03.880]   He's the hasse cards guy.
[01:47:03.880 --> 01:47:04.880]   That's right.
[01:47:04.880 --> 01:47:08.400]   So he's an amazing like I cannot show it's so amazing.
[01:47:08.400 --> 01:47:10.320]   It's called the first and it's going to launch.
[01:47:10.320 --> 01:47:11.320]   It's on Hulu.
[01:47:11.320 --> 01:47:16.960]   It is so effing good, but it takes place in the year 30 2030.
[01:47:16.960 --> 01:47:20.480]   And so we were trying to design all the different technologies that people would be using.
[01:47:20.480 --> 01:47:22.080]   Sean can is going to be in it.
[01:47:22.080 --> 01:47:23.080]   Yeah.
[01:47:23.080 --> 01:47:24.080]   Yeah.
[01:47:24.080 --> 01:47:25.840]   Oh, so, so, so good.
[01:47:25.840 --> 01:47:28.280]   The script is the shows are.
[01:47:28.280 --> 01:47:30.880]   So you like the technology consultant for it.
[01:47:30.880 --> 01:47:31.880]   I am the futurist.
[01:47:31.880 --> 01:47:34.480]   So it's the Jet Propulsion, NASA JPL.
[01:47:34.480 --> 01:47:35.480]   Great gig.
[01:47:35.480 --> 01:47:36.480]   So cool.
[01:47:36.480 --> 01:47:38.560]   That's really cool.
[01:47:38.560 --> 01:47:41.440]   But the that roll up technology.
[01:47:41.440 --> 01:47:46.360]   I think one of the directions that were likely headed in is that you know the sort of big
[01:47:46.360 --> 01:47:52.040]   the tablets that we have now will still have a desire for because we're not going to be
[01:47:52.040 --> 01:47:57.240]   able to read newspapers or the equivalent of a newspaper long form stuff on these glasses.
[01:47:57.240 --> 01:48:04.920]   So probably instead have a smart pen, not this, but you know pen that rolls has the ability
[01:48:04.920 --> 01:48:08.160]   to roll in and out and becomes more less flexible, right?
[01:48:08.160 --> 01:48:11.680]   To give us the ability to do that kind of reading and then make it portable.
[01:48:11.680 --> 01:48:16.000]   So anyhow, I can't I'm thrilled that you got that gig.
[01:48:16.000 --> 01:48:17.000]   What a great gig.
[01:48:17.000 --> 01:48:18.000]   Great work.
[01:48:18.000 --> 01:48:19.000]   We're doing it for fun.
[01:48:19.000 --> 01:48:20.960]   It's not like my obviously it's not my full time job.
[01:48:20.960 --> 01:48:25.000]   But that it's it's a show about you should have him on to talk about it.
[01:48:25.000 --> 01:48:31.680]   It's a show about the first people to colonize Mars, but it's it doesn't take place on Mars.
[01:48:31.680 --> 01:48:33.960]   It's the process of getting there and what that was like.
[01:48:33.960 --> 01:48:37.600]   So it's very much like House of Cards in that way.
[01:48:37.600 --> 01:48:41.000]   But it's all about the people in the relationships and it's so good.
[01:48:41.000 --> 01:48:42.000]   It's so good.
[01:48:42.000 --> 01:48:43.000]   All right.
[01:48:43.000 --> 01:48:44.000]   Yeah.
[01:48:44.000 --> 01:48:46.240]   Although I have to say if we're going to send people to Mars, I don't think Sean Penneby,
[01:48:46.240 --> 01:48:48.240]   my first choice like he came from Mars.
[01:48:48.240 --> 01:48:51.480]   Maybe he's not going to Mars.
[01:48:51.480 --> 01:48:55.200]   So he's a good person for the for the role he's got.
[01:48:55.200 --> 01:48:59.600]   Oh, he's like the NASA administrator says, you'll never get to Mars.
[01:48:59.600 --> 01:49:05.240]   I'm telling you guys this this is going to be it's well, Bo, I'm on left House of Cards.
[01:49:05.240 --> 01:49:06.240]   Yeah.
[01:49:06.240 --> 01:49:10.400]   But although they are doing a fifth season without Kevin Spacey.
[01:49:10.400 --> 01:49:11.920]   I think yeah.
[01:49:11.920 --> 01:49:15.040]   And Bo is he's lovely and super super smart.
[01:49:15.040 --> 01:49:17.720]   You get to like go in the writers room and stuff like that.
[01:49:17.720 --> 01:49:18.720]   Oh God, no.
[01:49:18.720 --> 01:49:22.200]   They don't like to do that.
[01:49:22.200 --> 01:49:25.160]   No, they it's very tense.
[01:49:25.160 --> 01:49:26.160]   So how fun.
[01:49:26.160 --> 01:49:27.800]   But it's been neat.
[01:49:27.800 --> 01:49:32.840]   I read through all the scripts and we help design the design the technology with them.
[01:49:32.840 --> 01:49:33.840]   Oh, that's neat.
[01:49:33.840 --> 01:49:34.840]   Good gig.
[01:49:34.840 --> 01:49:37.760]   And you read through the scripts and then you're like, wait a minute, wait a minute,
[01:49:37.760 --> 01:49:38.760]   Skittles will not exist.
[01:49:38.760 --> 01:49:41.800]   You're 24 and you're just like, but I love this.
[01:49:41.800 --> 01:49:44.680]   I love movies in the future where everybody smokes.
[01:49:44.680 --> 01:49:46.360]   Because I feel like that's what's going to happen.
[01:49:46.360 --> 01:49:48.960]   And then a cure cancer and then everybody starts smoking again.
[01:49:48.960 --> 01:49:49.960]   Right.
[01:49:49.960 --> 01:49:50.960]   Well, but so here's the thing.
[01:49:50.960 --> 01:49:53.960]   The year 2030 sounds like the far future.
[01:49:53.960 --> 01:49:55.200]   It's a decade away.
[01:49:55.200 --> 01:49:56.720]   It's not far.
[01:49:56.720 --> 01:49:59.960]   We won't really be going to Mars in 2030.
[01:49:59.960 --> 01:50:01.600]   We may have no choice.
[01:50:01.600 --> 01:50:05.200]   No, no, no, no.
[01:50:05.200 --> 01:50:11.600]   Mars is not going to be the escape pod for the planet Earth.
[01:50:11.600 --> 01:50:15.240]   That's a big no, it's hard to live on Mars.
[01:50:15.240 --> 01:50:21.800]   It was colder in Vermont and New Hampshire two days ago than it was on the surface of Mars.
[01:50:21.800 --> 01:50:27.800]   Well, during the during the daytime, but for my little night, I'm sure have a summer and
[01:50:27.800 --> 01:50:31.000]   also they have plants.
[01:50:31.000 --> 01:50:32.000]   Mars has potatoes.
[01:50:32.000 --> 01:50:33.200]   We know that from the market.
[01:50:33.200 --> 01:50:34.200]   That's true.
[01:50:34.200 --> 01:50:35.200]   You could grow potatoes.
[01:50:35.200 --> 01:50:36.200]   That's right.
[01:50:36.200 --> 01:50:37.200]   Don't eat cheese.
[01:50:37.200 --> 01:50:38.200]   We need fertilizer.
[01:50:38.200 --> 01:50:39.200]   All right.
[01:50:39.200 --> 01:50:40.200]   All right.
[01:50:40.200 --> 01:50:41.200]   You know what?
[01:50:41.200 --> 01:50:42.200]   You guys can joke all you want.
[01:50:42.200 --> 01:50:43.200]   We've got an answer for scene happening.
[01:50:43.200 --> 01:50:50.160]   And we've always said that I've said that I like the idea that right now there's somebody
[01:50:50.160 --> 01:50:52.800]   in high school who will be living on Mars.
[01:50:52.800 --> 01:50:54.840]   And I still think that's probably true.
[01:50:54.840 --> 01:50:57.800]   I mean, Elon Musk wants to do it, right?
[01:50:57.800 --> 01:50:59.000]   I don't know if that makes me fall.
[01:50:59.000 --> 01:51:06.440]   I mean, I think it sort of started out as a vanity project and as a fulfilling our sci-fi
[01:51:06.440 --> 01:51:07.440]   dreams.
[01:51:07.440 --> 01:51:09.280]   There are all kinds of challenges, right?
[01:51:09.280 --> 01:51:13.360]   I don't know that humans should be spreading their genetic material over the universe.
[01:51:13.360 --> 01:51:15.360]   I don't know that that's necessarily a thing.
[01:51:15.360 --> 01:51:16.360]   Well, I think that's our job.
[01:51:16.360 --> 01:51:21.840]   There's also a good argument for the fact that we ourselves are merchants that we originated.
[01:51:21.840 --> 01:51:22.840]   Really?
[01:51:22.840 --> 01:51:25.880]   Or there's a whole bunch of work being done now.
[01:51:25.880 --> 01:51:27.480]   You know what's depressing though?
[01:51:27.480 --> 01:51:29.920]   Really, maybe we will go to Mars, but it won't be the US.
[01:51:29.920 --> 01:51:31.240]   It'll almost certainly be China.
[01:51:31.240 --> 01:51:32.880]   It will not be us.
[01:51:32.880 --> 01:51:36.760]   Well, there's a lot going up into space.
[01:51:36.760 --> 01:51:40.280]   Over the next couple of years, there's an unprecedented number of CubeSats and other
[01:51:40.280 --> 01:51:43.160]   types of commercial space flights.
[01:51:43.160 --> 01:51:44.720]   There's a lot going up.
[01:51:44.720 --> 01:51:45.720]   So...
[01:51:45.720 --> 01:51:47.880]   This is the one with the dark side of the moon.
[01:51:47.880 --> 01:51:51.280]   We don't even want to go to Europe.
[01:51:51.280 --> 01:51:54.880]   The US is really kind of closing in.
[01:51:54.880 --> 01:51:55.880]   It's not reaching out.
[01:51:55.880 --> 01:51:56.880]   You know what I'm saying?
[01:51:56.880 --> 01:51:58.680]   I don't feel like Mars is in our future.
[01:51:58.680 --> 01:51:59.680]   But who knows?
[01:51:59.680 --> 01:52:01.280]   Apparently we're going back to the moon soon.
[01:52:01.280 --> 01:52:04.040]   I agree, but Trump has got moon fascination and that's...
[01:52:04.040 --> 01:52:06.480]   Yeah, because somebody said, "Hey, let's go to the moon."
[01:52:06.480 --> 01:52:07.480]   Okay.
[01:52:07.480 --> 01:52:09.160]   And then the next guy came in.
[01:52:09.160 --> 01:52:11.680]   I mean, I'm not counting on that.
[01:52:11.680 --> 01:52:13.000]   I'm not counting on that.
[01:52:13.000 --> 01:52:14.760]   But I will say one thing.
[01:52:14.760 --> 01:52:19.400]   If there's one thing I know about humans is spreading our genes across the universe,
[01:52:19.400 --> 01:52:21.120]   that's something we can do.
[01:52:21.120 --> 01:52:22.120]   That's right.
[01:52:22.120 --> 01:52:26.920]   That is our primary directive.
[01:52:26.920 --> 01:52:27.920]   Let's take a little break.
[01:52:27.920 --> 01:52:29.840]   Funcast, fun group here.
[01:52:29.840 --> 01:52:32.120]   If I were going to go to Mars, I'd want to go with you guys.
[01:52:32.120 --> 01:52:33.120]   This would be a great...
[01:52:33.120 --> 01:52:34.120]   Oh, crew.
[01:52:34.120 --> 01:52:35.120]   Let's just all go to Mars.
[01:52:35.120 --> 01:52:36.120]   Yeah, but we'll see.
[01:52:36.120 --> 01:52:38.960]   So we'd have an expert telling us what to eat.
[01:52:38.960 --> 01:52:39.960]   Yeah.
[01:52:39.960 --> 01:52:42.920]   I mean, talking about nomadic.
[01:52:42.920 --> 01:52:43.920]   That's really...
[01:52:43.920 --> 01:52:44.920]   Next.
[01:52:44.920 --> 01:52:46.920]   How's the coffee with Mars?
[01:52:46.920 --> 01:52:47.920]   That's the question.
[01:52:47.920 --> 01:52:48.920]   Yeah, I don't know.
[01:52:48.920 --> 01:52:49.920]   It's cold.
[01:52:49.920 --> 01:52:50.920]   It's cold brew.
[01:52:50.920 --> 01:52:51.920]   Cold brew.
[01:52:51.920 --> 01:52:52.920]   It's cold brew.
[01:52:52.920 --> 01:52:53.920]   It's cold brew on Mars.
[01:52:53.920 --> 01:52:54.920]   Red blend.
[01:52:54.920 --> 01:52:55.920]   Micah Sargent's here.
[01:52:55.920 --> 01:52:57.560]   He's senior editor at Mobile Nations.
[01:52:57.560 --> 01:52:58.560]   Great to have you.
[01:52:58.560 --> 01:52:59.560]   Micah, appreciate it.
[01:52:59.560 --> 01:53:00.560]   Like, what are you...
[01:53:00.560 --> 01:53:01.560]   What chair is that?
[01:53:01.560 --> 01:53:02.560]   That's a good chair.
[01:53:02.560 --> 01:53:03.560]   It looks like it's like...
[01:53:03.560 --> 01:53:04.560]   Oh, yeah.
[01:53:04.560 --> 01:53:05.560]   Oh, goodness.
[01:53:05.560 --> 01:53:07.680]   Is it called AI?
[01:53:07.680 --> 01:53:09.440]   Is the company something like that?
[01:53:09.440 --> 01:53:11.200]   It's like a fancy chair.
[01:53:11.200 --> 01:53:15.640]   Yeah, and it's sort of trying to compete with some of those really expensive ones,
[01:53:15.640 --> 01:53:18.480]   like the steel case, but it didn't cost me as much.
[01:53:18.480 --> 01:53:19.480]   Nice.
[01:53:19.480 --> 01:53:20.480]   Which I was happy about.
[01:53:20.480 --> 01:53:21.480]   Mine's from IKEA.
[01:53:21.480 --> 01:53:27.640]   It's the same chair that the guy on Silicon Valley, the CEO guy on Silicon Valley had,
[01:53:27.640 --> 01:53:29.800]   the crazy chair, but it was cheap.
[01:53:29.800 --> 01:53:30.800]   Right.
[01:53:30.800 --> 01:53:31.800]   Yeah.
[01:53:31.800 --> 01:53:37.760]   Man, she's like the superstar now that we know she's doing the first Amy Webb, AmyWeb.io's
[01:53:37.760 --> 01:53:39.440]   her website.
[01:53:39.440 --> 01:53:46.160]   Her books, The Signals are Talking Soon to Be a Major Audio Book Production.
[01:53:46.160 --> 01:53:49.760]   And I feel like I should tell everybody that I'm also sitting on a chair, but mine is an
[01:53:49.760 --> 01:53:50.760]   exercise ball.
[01:53:50.760 --> 01:53:55.880]   Ah, I sat on that for a long time until I got sciatica.
[01:53:55.880 --> 01:53:57.400]   Now I sit in a regular chair.
[01:53:57.400 --> 01:53:58.400]   Really?
[01:53:58.400 --> 01:53:59.400]   This is supposed to help with back issues.
[01:53:59.400 --> 01:54:00.880]   I thought it was too.
[01:54:00.880 --> 01:54:02.560]   But I sat on it for like 10 years.
[01:54:02.560 --> 01:54:06.160]   So it could be, there's could be a upper limit.
[01:54:06.160 --> 01:54:11.240]   Washington Post bestseller winner of the 2017 Gold Axiom Award, Fast Company's Best
[01:54:11.240 --> 01:54:15.040]   Books of 2016, Amazon's Best Books of 2016.
[01:54:15.040 --> 01:54:21.200]   The signals are talking Amy Webb futurist.
[01:54:21.200 --> 01:54:27.160]   And soon to be Amazon's best selling book of 2018, the author of Gastro Nomad, The Art
[01:54:27.160 --> 01:54:29.960]   of Living Everywhere and Eating Everything.
[01:54:29.960 --> 01:54:30.960]   That's right.
[01:54:30.960 --> 01:54:36.240]   Omnivore, Mike Elgin, Gastro Nomad.net, our show today brought to you by two friends
[01:54:36.240 --> 01:54:39.440]   of mine, Tim and Don, who were IT trainers.
[01:54:39.440 --> 01:54:42.880]   You know, they had classroom and people come and they'd learn the skills to get their
[01:54:42.880 --> 01:54:46.560]   A+ or their, you know, their MCSE certification.
[01:54:46.560 --> 01:54:49.200]   So they get a job in IT and they saw what we did here.
[01:54:49.200 --> 01:54:54.440]   They saw, I did a little panel at NAB and they said, you know what we should do?
[01:54:54.440 --> 01:54:56.880]   We should do a Twit for IT professionals.
[01:54:56.880 --> 01:54:59.560]   And they started ITProTV.
[01:54:59.560 --> 01:55:04.160]   Now, I should have done this because now they have five studios in Gainesville.
[01:55:04.160 --> 01:55:05.680]   We went to their grand opening.
[01:55:05.680 --> 01:55:09.440]   They are doing 125 hours of content a week.
[01:55:09.440 --> 01:55:14.600]   They have thousands of hours and they cover every topic you could ever consider an IT,
[01:55:14.600 --> 01:55:19.440]   whether you want to become an IT professional, get those certs so you can get a job or polish
[01:55:19.440 --> 01:55:22.920]   your skills so you can get a better job or get more pay.
[01:55:22.920 --> 01:55:24.680]   This is the place.
[01:55:24.680 --> 01:55:25.680]   ITProTV.
[01:55:25.680 --> 01:55:31.760]   Courses on demand or live via Chromecast, via Roku.
[01:55:31.760 --> 01:55:33.480]   They've got Amazon Fire TV app.
[01:55:33.480 --> 01:55:34.480]   They've got an Apple TV app.
[01:55:34.480 --> 01:55:38.560]   Of course, you can watch on your computer, your iOS and Android apps.
[01:55:38.560 --> 01:55:43.680]   So you can watch everywhere all day long and by osmosis, you can absorb stuff like certified
[01:55:43.680 --> 01:55:47.680]   ethical hacker, which is still the cert that I want the most.
[01:55:47.680 --> 01:55:51.120]   Callie Linux, great pen testing version of Linux.
[01:55:51.120 --> 01:55:55.120]   Linux security techniques, ITIL, ISC squared security search.
[01:55:55.120 --> 01:56:02.920]   You know, in 2018, there will be more than a million unfilled technology security jobs.
[01:56:02.920 --> 01:56:05.000]   This is a huge and growing field.
[01:56:05.000 --> 01:56:06.000]   Are you surprised?
[01:56:06.000 --> 01:56:09.360]   Hey, look, he's got a Leo Bobblehead behind him.
[01:56:09.360 --> 01:56:10.360]   Nice job.
[01:56:10.360 --> 01:56:11.360]   Done.
[01:56:11.360 --> 01:56:12.360]   Good job.
[01:56:12.360 --> 01:56:13.360]   ITProTV.
[01:56:13.360 --> 01:56:15.360]   They've got a team solution.
[01:56:15.360 --> 01:56:21.600]   I'll give you an example of the folks at Harvard, the IT team at Harvard University.
[01:56:21.600 --> 01:56:26.640]   They have the team solution because they want to keep all their IT people skills up to date.
[01:56:26.640 --> 01:56:31.160]   They've got group pricing and you have a supervisor portal, which lets you create custom training,
[01:56:31.160 --> 01:56:37.040]   custom assignments, see how people are doing, view logins and viewing time, track course
[01:56:37.040 --> 01:56:38.040]   completion.
[01:56:38.040 --> 01:56:40.440]   If you're an individual, oh, it's so much, first of all, it's engaging.
[01:56:40.440 --> 01:56:42.440]   So it's fun.
[01:56:42.440 --> 01:56:43.720]   You get to really like these guys.
[01:56:43.720 --> 01:56:45.480]   They've got a chat room just like we do.
[01:56:45.480 --> 01:56:49.760]   They also have study groups, lots of materials that could take the test online before you
[01:56:49.760 --> 01:56:51.520]   take the test for real.
[01:56:51.520 --> 01:56:52.880]   And it's all part of your membership.
[01:56:52.880 --> 01:56:56.560]   Go to itpro.tv/twit.
[01:56:56.560 --> 01:57:02.240]   And if you do that right now and use the offer code TWIT30, you'll get 30% off your subscription
[01:57:02.240 --> 01:57:05.040]   for the life of your active subscription and a free seven day trial.
[01:57:05.040 --> 01:57:06.040]   This is a nice deal.
[01:57:06.040 --> 01:57:07.040]   Itpro.tv/twit.
[01:57:07.040 --> 01:57:12.880]   If you're going there to learn about the team solution, you can sign up for a free demo
[01:57:12.880 --> 01:57:15.080]   of the supervisor portal as well.
[01:57:15.080 --> 01:57:22.040]   Itpro.tv/twit, use the code TWIT30 to try it free for seven days and get 30% off.
[01:57:22.040 --> 01:57:31.400]   Friendly training, binge worthy content, life changing results ITpro.tv/twit.
[01:57:31.400 --> 01:57:35.480]   Let's see.
[01:57:35.480 --> 01:57:36.480]   We did CES.
[01:57:36.480 --> 01:57:37.480]   We did Apple battery.
[01:57:37.480 --> 01:57:41.480]   We really didn't cover the Apple battery fully because we got distracted by sex robots.
[01:57:41.480 --> 01:57:46.480]   Just as happens.
[01:57:46.480 --> 01:57:53.480]   But just to complete the circle, Apple, I gather that Steve gives it with skeptical too.
[01:57:53.480 --> 01:57:55.480]   Your skeptical, Amy seems skeptical.
[01:57:55.480 --> 01:57:59.320]   I don't know about you, Michael, that Apple's explanation covered it and that maybe they
[01:57:59.320 --> 01:58:01.240]   really did want people to upgrade.
[01:58:01.240 --> 01:58:04.320]   But to their credit, they backpedaled quickly.
[01:58:04.320 --> 01:58:08.040]   They've offered $29 battery upgrades, which is fantastic.
[01:58:08.040 --> 01:58:10.760]   This is expensive for Apple to do this.
[01:58:10.760 --> 01:58:14.200]   Although I tried to get an appointment in Silicon Valley and they're just a little crowded.
[01:58:14.200 --> 01:58:15.200]   No.
[01:58:15.200 --> 01:58:17.920]   If you have an iPhone 6, 6s or SE, you just should do it.
[01:58:17.920 --> 01:58:20.040]   It's a $29 box.
[01:58:20.040 --> 01:58:24.200]   And it doesn't matter what your test says they will do the $29 replacement.
[01:58:24.200 --> 01:58:25.200]   No questions asked.
[01:58:25.200 --> 01:58:26.200]   And they accelerated it.
[01:58:26.200 --> 01:58:29.240]   They said it was going to be, I guess, in January and then they moved it up a month.
[01:58:29.240 --> 01:58:30.840]   That's a lot of credit to Apple.
[01:58:30.840 --> 01:58:32.160]   It's actually credit.
[01:58:32.160 --> 01:58:36.840]   It's also maybe a problem for their future because the common knowledge was that you can't
[01:58:36.840 --> 01:58:39.760]   upgrade the components of your phone at rent.
[01:58:39.760 --> 01:58:43.560]   You always could, but nobody knew it.
[01:58:43.560 --> 01:58:44.560]   Apple didn't.
[01:58:44.560 --> 01:58:45.560]   It was $80.
[01:58:45.560 --> 01:58:46.560]   Right.
[01:58:46.560 --> 01:58:51.680]   So now it tells us, oh, maybe the phone is more modular than we all thought and we can
[01:58:51.680 --> 01:58:55.160]   upgrade as needed rather than buying an entirely new phone.
[01:58:55.160 --> 01:58:57.560]   It's not like Apple can't afford it.
[01:58:57.560 --> 01:59:04.760]   Apple's App Store revenue $11 billion in 2017.
[01:59:04.760 --> 01:59:06.640]   That's Apple's cut.
[01:59:06.640 --> 01:59:08.080]   That's Apple's cut.
[01:59:08.080 --> 01:59:13.240]   The total revenue is $26.5 billion that went to developers $11 billion to Apple.
[01:59:13.240 --> 01:59:14.240]   That's a lot of money.
[01:59:14.240 --> 01:59:15.240]   That's just from the App Store.
[01:59:15.240 --> 01:59:16.240]   It's incredible.
[01:59:16.240 --> 01:59:21.200]   They also, thanks to the new tax bill, they're thinking about repatriating their hunt.
[01:59:21.200 --> 01:59:24.320]   They have more than $200 billion overseas.
[01:59:24.320 --> 01:59:30.480]   The new tax rate will get them back into the country for 15% Apple's thinking about it,
[01:59:30.480 --> 01:59:32.360]   which would give them a huge cash when fall.
[01:59:32.360 --> 01:59:35.240]   There's been lots of speculation about what they would do with that money.
[01:59:35.240 --> 01:59:40.480]   Here's a little point of trivia about Apple's cash hoard.
[01:59:40.480 --> 01:59:45.760]   People imagine that the cash is actually overseas, but it isn't.
[01:59:45.760 --> 01:59:48.120]   Most of that cash is actually in the United States.
[01:59:48.120 --> 01:59:49.120]   It's in Bitcoin.
[01:59:49.120 --> 01:59:52.760]   No, it's in the United States controlled by foreign companies.
[01:59:52.760 --> 01:59:53.760]   Oh, interesting.
[01:59:53.760 --> 01:59:54.760]   They're owned by Apple.
[01:59:54.760 --> 01:59:58.840]   So the money itself is not overseas.
[01:59:58.840 --> 02:00:05.520]   The companies that control the money are legally overseas.
[02:00:05.520 --> 02:00:06.520]   Wasn't that funny?
[02:00:06.520 --> 02:00:07.680]   The cash isn't anywhere.
[02:00:07.680 --> 02:00:08.680]   Exactly.
[02:00:08.680 --> 02:00:09.680]   Exactly.
[02:00:09.680 --> 02:00:10.680]   Exactly.
[02:00:10.680 --> 02:00:14.480]   It's a common thing that every big stack that Tim Cook goes and bathes in every night.
[02:00:14.480 --> 02:00:15.480]   Well, maybe.
[02:00:15.480 --> 02:00:16.480]   Maybe.
[02:00:16.480 --> 02:00:21.200]   I would totally do the Scrooge McDuck like gold coins in the giant vault thing, like
[02:00:21.200 --> 02:00:23.000]   dying and swim around.
[02:00:23.000 --> 02:00:27.560]   But anyway, it's a small point, but it's something that everybody repeats that the money is
[02:00:27.560 --> 02:00:28.560]   overseas.
[02:00:28.560 --> 02:00:31.000]   Well, most of that is actually in the United States.
[02:00:31.000 --> 02:00:32.000]   Controlled by overseas companies.
[02:00:32.000 --> 02:00:33.000]   Yes.
[02:00:33.000 --> 02:00:35.840]   That are themselves controlled by Apple, which is a US company.
[02:00:35.840 --> 02:00:42.040]   They're actually on boats inside of lakes so that it's high seas law or wintertime law.
[02:00:42.040 --> 02:00:46.000]   That always cracked me up when you go to New Orleans and you do riverboat gambling.
[02:00:46.000 --> 02:00:48.760]   It's really just a casino with a little bit of water.
[02:00:48.760 --> 02:00:54.760]   It's not exactly international waters.
[02:00:54.760 --> 02:00:56.960]   You know who won't be at CES?
[02:00:56.960 --> 02:00:58.960]   I'm not a fan of Agit Pie.
[02:00:58.960 --> 02:01:00.360]   Oh, hatches.
[02:01:00.360 --> 02:01:03.880]   He was scheduled to give the keynote.
[02:01:03.880 --> 02:01:04.880]   Not that I condone this.
[02:01:04.880 --> 02:01:08.240]   I'm not a fan of Agit Pie, but he got death threats.
[02:01:08.240 --> 02:01:12.840]   This is taking net neutrality a little too seriously, folks.
[02:01:12.840 --> 02:01:14.960]   Knock off the death threats.
[02:01:14.960 --> 02:01:20.280]   So he may Skype in, which would be funny because if he uses, if he tried to use Skype
[02:01:20.280 --> 02:01:22.240]   on Comcast, I think they'll throttle it.
[02:01:22.240 --> 02:01:26.640]   Wouldn't it be funny if he's moving one frame a second?
[02:01:26.640 --> 02:01:30.760]   It has to be said though.
[02:01:30.760 --> 02:01:35.520]   It's commonly phrased that when stuff like this happens and it is being phrased in this
[02:01:35.520 --> 02:01:41.680]   case, that he didn't, he's not going because of death threats.
[02:01:41.680 --> 02:01:46.040]   Yeah, there are death threats and he's decided not to go and blame that as the reason.
[02:01:46.040 --> 02:01:47.840]   It might be convenient for him not to show up.
[02:01:47.840 --> 02:01:51.560]   I mean, I wouldn't want to show my face in public right now if I were him.
[02:01:51.560 --> 02:01:53.640]   And yeah, there are always going to be death threats.
[02:01:53.640 --> 02:01:59.640]   It's like Twitter trolls, somebody like that is going to get a death threat every five
[02:01:59.640 --> 02:02:01.360]   seconds on average.
[02:02:01.360 --> 02:02:07.640]   It's like, I don't think that you necessarily have to have security at these events, right?
[02:02:07.640 --> 02:02:10.640]   To prevent maniacs from carrying weapons around it or whatever.
[02:02:10.640 --> 02:02:11.640]   He's safe there.
[02:02:11.640 --> 02:02:13.640]   Yeah, I mean, it seems to me that he's most of these dead.
[02:02:13.640 --> 02:02:17.080]   And as anywhere, actually, Mike, that's a really, really good point because of anywhere
[02:02:17.080 --> 02:02:18.080]   that he could possibly go.
[02:02:18.080 --> 02:02:20.000]   I mean, there's cameras everywhere.
[02:02:20.000 --> 02:02:21.000]   It's Vegas.
[02:02:21.000 --> 02:02:22.000]   Yeah.
[02:02:22.000 --> 02:02:24.560]   Yeah, yeah, no, actually, Vegas was probably safe.
[02:02:24.560 --> 02:02:27.600]   But he should keep his eye out for this guy.
[02:02:27.600 --> 02:02:32.360]   If anybody, anybody, by the way, this is the hacker uniform, a hoodie and a hat and he's
[02:02:32.360 --> 02:02:33.660]   wearing earbuds.
[02:02:33.660 --> 02:02:41.040]   I don't know who this guy is, but according to Krebs, Brian Krebs on security, 25, he's
[02:02:41.040 --> 02:02:42.840]   25 year old Tyler Raj Barris.
[02:02:42.840 --> 02:02:49.520]   He was arrested on Friday in Los Angeles because he, according to the police, is being accused
[02:02:49.520 --> 02:02:56.960]   of making the SWAT call that got a man killed in Kansas.
[02:02:56.960 --> 02:02:58.920]   Oh, geez.
[02:02:58.920 --> 02:03:04.040]   The story is his handle on Twitter was swatistic.
[02:03:04.040 --> 02:03:06.440]   He admits, in fact, that he's a serial swatter.
[02:03:06.440 --> 02:03:11.680]   He was convicted in 2016 for calling in a bomb threat to an ABC affiliate in Los Angeles.
[02:03:11.680 --> 02:03:15.920]   The AP said he was sentenced to two years in prison for that released in January of last
[02:03:15.920 --> 02:03:16.920]   year.
[02:03:16.920 --> 02:03:19.080]   Apparently didn't learn anything.
[02:03:19.080 --> 02:03:22.920]   He's claimed credit for bomb threats against a Dallas convention center of high school
[02:03:22.920 --> 02:03:24.200]   in Florida.
[02:03:24.200 --> 02:03:26.640]   But this one's really beyond the pale.
[02:03:26.640 --> 02:03:32.100]   If he is swatistic, somebody with a Twitter handle, Swatistic, got in a online Twitter
[02:03:32.100 --> 02:03:45.760]   battle over, I guess, gaming and said, you know, the guy he was, he was battling with
[02:03:45.760 --> 02:03:47.880]   said, well, he said, I'm going to swat you.
[02:03:47.880 --> 02:03:49.240]   He said, okay, sure.
[02:03:49.240 --> 02:03:53.960]   Here's my address, except it wasn't his address.
[02:03:53.960 --> 02:03:59.780]   It was the random address in Kansas in Wichita, Kansas.
[02:03:59.780 --> 02:04:01.040]   The police were called.
[02:04:01.040 --> 02:04:05.060]   They were told that it was a domestic violence situation.
[02:04:05.060 --> 02:04:06.600]   One person had already been killed.
[02:04:06.600 --> 02:04:08.920]   The perpetrator was armed in the house.
[02:04:08.920 --> 02:04:10.760]   Police responded.
[02:04:10.760 --> 02:04:14.220]   Now of course the police have some culpability here too.
[02:04:14.220 --> 02:04:16.220]   We don't know exactly what happened.
[02:04:16.220 --> 02:04:20.900]   Apparently an officer walked up to the door and just shot the guy, killed him.
[02:04:20.900 --> 02:04:22.100]   He was not a gamer.
[02:04:22.100 --> 02:04:24.180]   His brother said he never played video games.
[02:04:24.180 --> 02:04:25.180]   He was not the guy.
[02:04:25.180 --> 02:04:27.820]   He was just a random victim.
[02:04:27.820 --> 02:04:28.980]   Swatistic says, I didn't do it.
[02:04:28.980 --> 02:04:30.140]   I didn't pull the trigger.
[02:04:30.140 --> 02:04:33.020]   Well, he's in custody now.
[02:04:33.020 --> 02:04:34.100]   And here's a question.
[02:04:34.100 --> 02:04:43.980]   So again, we, I'm still confused as to how Twitter is somehow not taking a role in any
[02:04:43.980 --> 02:04:44.980]   of this.
[02:04:44.980 --> 02:04:49.060]   Well, he's a world leader and Twitter says, if you're a world leader, that's a different
[02:04:49.060 --> 02:04:50.060]   story.
[02:04:50.060 --> 02:04:51.060]   That's a different one.
[02:04:51.060 --> 02:04:52.060]   Okay.
[02:04:52.060 --> 02:04:53.060]   Okay.
[02:04:53.060 --> 02:04:54.060]   Every story.
[02:04:54.060 --> 02:04:57.860]   But how many times do we ask this question?
[02:04:57.860 --> 02:04:58.860]   I know.
[02:04:58.860 --> 02:05:01.900]   Well, nobody looks good in any of this.
[02:05:01.900 --> 02:05:06.660]   But you know, but Amy, I will say in defense, just let me answer this.
[02:05:06.660 --> 02:05:09.220]   The internet gets blamed for everything too.
[02:05:09.220 --> 02:05:11.340]   And it's just that's the way it is.
[02:05:11.340 --> 02:05:12.420]   The internet is the world.
[02:05:12.420 --> 02:05:14.420]   The world is a messed up place.
[02:05:14.420 --> 02:05:18.180]   Twitter is to some degree really a reflection of the world we live in.
[02:05:18.180 --> 02:05:20.580]   I don't know what Twitter is supposed to do.
[02:05:20.580 --> 02:05:25.180]   Is it a reflection of the world that we live in or is it part of a world that is it the
[02:05:25.180 --> 02:05:27.900]   cause of the current world that we live in?
[02:05:27.900 --> 02:05:28.900]   Does that make sense?
[02:05:28.900 --> 02:05:32.060]   No, but yeah, well, it's both cause and effect.
[02:05:32.060 --> 02:05:33.060]   You're right.
[02:05:33.060 --> 02:05:34.060]   I mean, it's both.
[02:05:34.060 --> 02:05:40.620]   To me, the biggest thing about our world that this story exposes is the militarization
[02:05:40.620 --> 02:05:44.540]   of police, domestic violence cases, calls like that.
[02:05:44.540 --> 02:05:47.740]   These happen every day across the nation.
[02:05:47.740 --> 02:05:53.460]   And if somebody is completely unarmed, completely innocent, has nothing to do with the apparent
[02:05:53.460 --> 02:06:00.020]   call, they shouldn't be shot to death in their own house by some mistake or whatever.
[02:06:00.020 --> 02:06:02.060]   So the police, I'm sure they'll have a review.
[02:06:02.060 --> 02:06:06.140]   I'm sure police departments all over the country should be thinking about this.
[02:06:06.140 --> 02:06:07.140]   So that's one issue.
[02:06:07.140 --> 02:06:08.140]   The police will have to deal with that.
[02:06:08.140 --> 02:06:12.300]   The other one is this douchebag with the hat and the earbuds.
[02:06:12.300 --> 02:06:14.660]   They should throw him back in the slammer.
[02:06:14.660 --> 02:06:18.660]   The other, but the thing that I want to know is picture again in case you run into a lot
[02:06:18.660 --> 02:06:21.620]   of street, in case you think I'm being unfair.
[02:06:21.620 --> 02:06:27.940]   So the person who gave the wrong address, I think he's got some reckoning.
[02:06:27.940 --> 02:06:28.940]   He's got some culpability.
[02:06:28.940 --> 02:06:30.860]   He's in jail.
[02:06:30.860 --> 02:06:33.180]   And I also want to know he's not in jail.
[02:06:33.180 --> 02:06:38.180]   So they were playing Call of Duty and this guy who gave the wrong address was taunted,
[02:06:38.180 --> 02:06:40.900]   Swatistic and said, "Come and swat me, baby."
[02:06:40.900 --> 02:06:42.820]   Let's just do a game of wrong address.
[02:06:42.820 --> 02:06:44.300]   So he's got some culpability.
[02:06:44.300 --> 02:06:45.300]   Absolutely.
[02:06:45.300 --> 02:06:49.420]   So let's say somebody who I believe is an actual killer says they're going to come and kill
[02:06:49.420 --> 02:06:51.700]   me and I give my neighbor's address.
[02:06:51.700 --> 02:06:52.700]   Yeah.
[02:06:52.700 --> 02:06:54.340]   Should I be arrested for...
[02:06:54.340 --> 02:06:58.700]   The question is, should Swatistic still have his Twitter account?
[02:06:58.700 --> 02:07:00.700]   Is that even bad?
[02:07:00.700 --> 02:07:08.940]   Well, he was able to tweet the whole thing and his Twitter, he changed his Twitter address.
[02:07:08.940 --> 02:07:11.340]   He changed it to @swattingaccount.
[02:07:11.340 --> 02:07:14.540]   So he's not exactly hiding what in the world.
[02:07:14.540 --> 02:07:15.540]   It's in Twitter.
[02:07:15.540 --> 02:07:20.100]   Listen, it is in Twitter's and Facebook's best interest to address this now and to come
[02:07:20.100 --> 02:07:26.460]   up with some reasonable way forward because if they don't, the government is going to
[02:07:26.460 --> 02:07:28.140]   do that for them.
[02:07:28.140 --> 02:07:32.600]   And as much as I don't like what I'm seeing right now on the socials, I don't think it
[02:07:32.600 --> 02:07:38.220]   makes sense for the government to come in and regulate what can and can't be done.
[02:07:38.220 --> 02:07:42.620]   In the United States, we're already seeing that happen in Europe, in Germany, and all
[02:07:42.620 --> 02:07:43.740]   around the rest of the world.
[02:07:43.740 --> 02:07:51.020]   And we're heading very, very quickly into a splintered internet, a series of splinternets
[02:07:51.020 --> 02:07:55.660]   where your geographic boundaries and your ISP dictate what you can and can't see.
[02:07:55.660 --> 02:07:59.260]   And that, to me, is more terrifying than being swatted.
[02:07:59.260 --> 02:08:00.700]   Well, you've been swatted.
[02:08:00.700 --> 02:08:01.700]   I didn't know what to do.
[02:08:01.700 --> 02:08:02.940]   Somebody swatted our studio.
[02:08:02.940 --> 02:08:03.940]   That's where you call.
[02:08:03.940 --> 02:08:04.940]   A couple years ago, right?
[02:08:04.940 --> 02:08:05.940]   Yeah.
[02:08:05.940 --> 02:08:08.020]   We call the police and you say, "There's a murderer."
[02:08:08.020 --> 02:08:13.160]   In this case, he said, "I'm the guy who just shot my father in the head and I've got four
[02:08:13.160 --> 02:08:15.700]   family members hostage."
[02:08:15.700 --> 02:08:22.060]   But it's a little shady because according to Krebs reporting, he called City Hall and
[02:08:22.060 --> 02:08:29.380]   then gave his number and then City Hall called 911 and the 911 called the number to talk
[02:08:29.380 --> 02:08:30.580]   to the guy.
[02:08:30.580 --> 02:08:33.660]   And Krebs says, "That's a little suspicious because, of course, if he was really local
[02:08:33.660 --> 02:08:35.660]   night, he would have just called 911."
[02:08:35.660 --> 02:08:39.980]   But anyway, and that's the same thing happened with us when we were swatted.
[02:08:39.980 --> 02:08:40.980]   It wasn't a 911 call.
[02:08:40.980 --> 02:08:44.380]   It was a call to the police number and then they could spoof the number.
[02:08:44.380 --> 02:08:48.700]   You can't call 911 in your region and spoof the number.
[02:08:48.700 --> 02:08:53.340]   But it's Petaluma, so Andy and Gomer showed up.
[02:08:53.340 --> 02:08:59.340]   No, the great thing about Petaluma is they didn't come in a giant armored vehicle with
[02:08:59.340 --> 02:09:00.340]   automatic weapons.
[02:09:00.340 --> 02:09:01.940]   They just came and they said, "Is everything okay?"
[02:09:01.940 --> 02:09:02.940]   We said, "Yeah."
[02:09:02.940 --> 02:09:05.900]   They said, "Do you mind if we check for bombs?"
[02:09:05.900 --> 02:09:07.580]   And we said, "No, they brought a dog."
[02:09:07.580 --> 02:09:08.940]   Actually, they didn't have a dog.
[02:09:08.940 --> 02:09:14.180]   So they called like San Rafael and they brought a dog up.
[02:09:14.180 --> 02:09:17.660]   And they were there all day and they kind of said, "Yeah, there's no bombs."
[02:09:17.660 --> 02:09:18.660]   And they gave us the recording.
[02:09:18.660 --> 02:09:19.940]   So that's how it should work.
[02:09:19.940 --> 02:09:20.940]   It was very calm.
[02:09:20.940 --> 02:09:23.940]   In this case, in Wichita, the SWAT squad was sent.
[02:09:23.940 --> 02:09:28.300]   The guy who was killed, as very sad, has been identified as a 28-year-old father to
[02:09:28.300 --> 02:09:31.700]   Andrew Finch and our deep condolences to his family.
[02:09:31.700 --> 02:09:32.700]   Very sad.
[02:09:32.700 --> 02:09:34.540]   He was a random victim.
[02:09:34.540 --> 02:09:39.660]   The officer fired the fatal shot according to the Wichita Eagle as a seven-year veteran.
[02:09:39.660 --> 02:09:40.820]   He's unadministrative leave.
[02:09:40.820 --> 02:09:43.660]   There will be an internal investigation.
[02:09:43.660 --> 02:09:47.380]   Everything is dangerous and militarized.
[02:09:47.380 --> 02:09:48.820]   Police make it even more dangerous.
[02:09:48.820 --> 02:09:50.980]   Poorly trained police make it even more dangerous.
[02:09:50.980 --> 02:09:55.220]   Nevertheless, you've got to also feel for a cop who's got to go in a situation where,
[02:09:55.220 --> 02:09:57.980]   to the best of his knowledge, there's a guy where the gun has already killed somebody who
[02:09:57.980 --> 02:10:00.060]   could very well just shoot him.
[02:10:00.060 --> 02:10:04.260]   So I think that there probably are ways to resolve a situation like that.
[02:10:04.260 --> 02:10:05.260]   I don't know.
[02:10:05.260 --> 02:10:06.260]   So the...
[02:10:06.260 --> 02:10:07.260]   You got a Mars.
[02:10:07.260 --> 02:10:08.260]   That's what you do.
[02:10:08.260 --> 02:10:09.260]   That's all go to Mars.
[02:10:09.260 --> 02:10:10.260]   We don't have to worry about it.
[02:10:10.260 --> 02:10:11.260]   We get a new on-muts spaceship.
[02:10:11.260 --> 02:10:12.260]   Yeah, phone calls.
[02:10:12.260 --> 02:10:14.260]   So here's the thing to see.
[02:10:14.260 --> 02:10:18.060]   A SWAT call, FBI estimates 400 swatting incidents a year.
[02:10:18.060 --> 02:10:19.060]   That's more than one a day.
[02:10:19.060 --> 02:10:22.260]   A lot of them come through Twitch, by the way, Twitch gamers.
[02:10:22.260 --> 02:10:25.420]   Each incident costs first responders about $10,000.
[02:10:25.420 --> 02:10:27.780]   It is a felony.
[02:10:27.780 --> 02:10:31.860]   It is illegal, but it's hard to catch these guys because there's no more.
[02:10:31.860 --> 02:10:36.140]   I bet pretty soon we'll probably have...
[02:10:36.140 --> 02:10:38.580]   So there's AI that's been deployed for customer service.
[02:10:38.580 --> 02:10:42.500]   And I have to think that somebody who's actually animated and in a state where they've actually
[02:10:42.500 --> 02:10:43.500]   done that...
[02:10:43.500 --> 02:10:44.500]   They sound different.
[02:10:44.500 --> 02:10:48.220]   Probably sounds different than somebody who's screwing around.
[02:10:48.220 --> 02:10:50.100]   So my hope is that...
[02:10:50.100 --> 02:10:56.660]   The other dimension of this is the vilification of gaming by the mainstream media and all
[02:10:56.660 --> 02:10:57.660]   this.
[02:10:57.660 --> 02:10:58.660]   There's a...
[02:10:58.660 --> 02:11:03.260]   In some of the reports around the story, it's like, "Oh, this is coming out of the dark
[02:11:03.260 --> 02:11:05.260]   and ugly world of video gaming."
[02:11:05.260 --> 02:11:07.380]   Well, that's just ridiculous.
[02:11:07.380 --> 02:11:12.680]   By 99.999% of video gamers have nothing to do with this kind of stuff.
[02:11:12.680 --> 02:11:13.680]   I know.
[02:11:13.680 --> 02:11:14.680]   They're just playing games.
[02:11:14.680 --> 02:11:19.380]   So the idea that this is a typical example of the type of person who plays video games
[02:11:19.380 --> 02:11:23.660]   is just absurd and any media that reports this is just...
[02:11:23.660 --> 02:11:24.660]   No, but there are...
[02:11:24.660 --> 02:11:25.660]   But there are...
[02:11:25.660 --> 02:11:31.380]   And to Amy's point, Twitter has enabled some evil, sadistic mofos out there.
[02:11:31.380 --> 02:11:35.460]   Whether the game or not, and they are taking advantage of a system that has no defense
[02:11:35.460 --> 02:11:37.940]   against them.
[02:11:37.940 --> 02:11:39.260]   Or maybe it does have a defense.
[02:11:39.260 --> 02:11:41.340]   Maybe it is up to Twitter to do something.
[02:11:41.340 --> 02:11:44.220]   By the way, I forgot to mention, it wasn't just death threats.
[02:11:44.220 --> 02:11:49.420]   Adjut Pai received John Konstein, who writes for TechCrunch, suggested on Twitter, "You
[02:11:49.420 --> 02:11:51.500]   see Twitter's the problem.
[02:11:51.500 --> 02:11:57.620]   Some tech billionaire, please buy out the local ISP where FCC Chairman Adjut Pai lives
[02:11:57.620 --> 02:12:00.860]   and give him 14 for dial up speeds."
[02:12:00.860 --> 02:12:01.860]   That's funny.
[02:12:01.860 --> 02:12:10.020]   And Matthew Prince, the CEO of CloudFlare, said, "You know, John, Josh, I could do this
[02:12:10.020 --> 02:12:12.980]   in a different but equally effective way.
[02:12:12.980 --> 02:12:15.100]   Josh says, 'Please go on.'
[02:12:15.100 --> 02:12:19.580]   Matthew said, 'I just sent a note to our general counsel to see if we could do this without
[02:12:19.580 --> 02:12:21.300]   breaking any laws.'
[02:12:21.300 --> 02:12:27.860]   CloudFlare, former sponsor on Twitter and good friends of Twitter, of course, probably
[02:12:27.860 --> 02:12:31.020]   most of the websites Adjut Pai goes to run through CloudFlare.
[02:12:31.020 --> 02:12:37.260]   And actually, CloudFlare is going to get, they're going to run into problems too because
[02:12:37.260 --> 02:12:39.860]   of Adjut's, because of net equality.
[02:12:39.860 --> 02:12:41.860]   Absolutely.
[02:12:41.860 --> 02:12:42.860]   Absolutely.
[02:12:42.860 --> 02:12:45.020]   Nobody is thinking this through.
[02:12:45.020 --> 02:12:49.660]   I just want to say one last thing on this.
[02:12:49.660 --> 02:12:51.580]   Why?
[02:12:51.580 --> 02:12:59.700]   We are all now in this predicament because the ISPs didn't think all of this through.
[02:12:59.700 --> 02:13:06.580]   And we are now paying for the ISPs' lack of foresight and planning.
[02:13:06.580 --> 02:13:09.620]   Splainess to me, please.
[02:13:09.620 --> 02:13:10.620]   What do you mean?
[02:13:10.620 --> 02:13:11.620]   So, okay.
[02:13:11.620 --> 02:13:18.460]   So in the early days of broadband, right, we didn't yet have, people looking at this
[02:13:18.460 --> 02:13:22.540]   could have built the scenarios and told you this was coming, but we didn't yet have mobile
[02:13:22.540 --> 02:13:27.340]   devices that were going to be ubiquitously connected and we didn't have Twitch.
[02:13:27.340 --> 02:13:33.540]   We didn't have games that pulled a lot of bandwidth and we didn't have this unbelievable
[02:13:33.540 --> 02:13:35.340]   amount of video and we didn't have Netflix.
[02:13:35.340 --> 02:13:36.340]   We didn't have Twitch.
[02:13:36.340 --> 02:13:37.340]   You're right.
[02:13:37.340 --> 02:13:38.340]   It's a lot of it.
[02:13:38.340 --> 02:13:39.340]   It's a lot of it.
[02:13:39.340 --> 02:13:43.940]   So, and at the very beginning, if everybody remembers, a lot of people were pretty stoked
[02:13:43.940 --> 02:13:47.620]   to have ADSL, right?
[02:13:47.620 --> 02:13:51.940]   And so there wasn't this rush into broadband.
[02:13:51.940 --> 02:13:55.460]   Well, all of a sudden, we have all of these different sources.
[02:13:55.460 --> 02:13:57.340]   The components and the prices have gone down.
[02:13:57.340 --> 02:14:04.500]   So we've got more devices that are connecting and AT&T and all of the ISPs should have said,
[02:14:04.500 --> 02:14:05.700]   we see where this is going.
[02:14:05.700 --> 02:14:08.700]   This is actually an opportunity for us to make more revenue.
[02:14:08.700 --> 02:14:13.300]   We can model out scenarios for how the cable bundling, which has been subsidizing a lot
[02:14:13.300 --> 02:14:16.420]   of this, is probably not going to work 10 years from now.
[02:14:16.420 --> 02:14:19.900]   So let's build this out in a different way.
[02:14:19.900 --> 02:14:21.420]   And they didn't do that.
[02:14:21.420 --> 02:14:27.460]   And it's because we have this system set up in the United States where only certain ISPs
[02:14:27.460 --> 02:14:32.540]   and cable providers get to dictate the rules for different parts of the country.
[02:14:32.540 --> 02:14:33.540]   And that's fine.
[02:14:33.540 --> 02:14:36.300]   And I'm not regudging anybody the ability to make a profit.
[02:14:36.300 --> 02:14:43.860]   But we are in the situation, so the rollback of the net neutrality regulations does not
[02:14:43.860 --> 02:14:44.860]   help the future.
[02:14:44.860 --> 02:14:47.860]   It's only addressing a problem with the status quo.
[02:14:47.860 --> 02:14:49.420]   And the status quo is a moving target.
[02:14:49.420 --> 02:14:51.500]   So we're going to have a different problem.
[02:14:51.500 --> 02:14:54.060]   Doesn't that argue that government shouldn't get involved at all?
[02:14:54.060 --> 02:14:57.020]   Because all government can do is shoot it as sitting still target.
[02:14:57.020 --> 02:15:03.780]   No, this is an argument for the FCC not allowing this continual revolving doors of its chair
[02:15:03.780 --> 02:15:06.180]   people back and forth to industry.
[02:15:06.180 --> 02:15:10.020]   Because nobody is who was looking out for the best interests of the infrastructure.
[02:15:10.020 --> 02:15:11.100]   Although it's like that in every case.
[02:15:11.100 --> 02:15:14.860]   To be fair, Tom Wheeler, who we thought was going to be, you know, he was a cable lobbyist
[02:15:14.860 --> 02:15:19.380]   and we thought he was going to be, he actually did the right thing, right?
[02:15:19.380 --> 02:15:20.380]   So a former FCC.
[02:15:20.380 --> 02:15:21.380]   But did he?
[02:15:21.380 --> 02:15:22.380]   But I don't know.
[02:15:22.380 --> 02:15:25.180]   I mean, he did the only thing he could do.
[02:15:25.180 --> 02:15:31.860]   The FCC hadn't, by the way, this is something widely misreported by conservative media who
[02:15:31.860 --> 02:15:34.820]   say, oh, but before 2015 there were no net neutrality rules.
[02:15:34.820 --> 02:15:37.340]   No, absolutely there were net neutrality rules.
[02:15:37.340 --> 02:15:43.260]   But Verizon sued and said the FCC doesn't have standing to make these rules.
[02:15:43.260 --> 02:15:48.860]   The judge agreed and said, told the FCC in 2015, now what you really got to do, you would
[02:15:48.860 --> 02:15:55.980]   have standing if you use the Telecommunications Act and invoked Title II and said that ISPs
[02:15:55.980 --> 02:16:01.860]   are carriers, not information services, and that they can be regulated like carriers.
[02:16:01.860 --> 02:16:04.620]   So Tom Wheeler, remember he took a lot of comments?
[02:16:04.620 --> 02:16:05.620]   We got very involved.
[02:16:05.620 --> 02:16:07.460]   There were millions of comments.
[02:16:07.460 --> 02:16:11.660]   He was going back and forth and he said, no, you're right, Title II regulation.
[02:16:11.660 --> 02:16:16.020]   And that's when that regulation was put into place in 2015 and took a while.
[02:16:16.020 --> 02:16:19.020]   That's great.
[02:16:19.020 --> 02:16:23.140]   But who literally just spent two, because we've just spent two hours talking about AR and
[02:16:23.140 --> 02:16:28.380]   sex robots and social media, all of this stuff needs bandwidth too.
[02:16:28.380 --> 02:16:29.780]   And so you're telling me, right?
[02:16:29.780 --> 02:16:32.140]   You know, it's so funny.
[02:16:32.140 --> 02:16:36.940]   The CEO, former AT&T CEO, the guy was CEO, Randall Stevenson, when the iPhone came out,
[02:16:36.940 --> 02:16:42.940]   said the biggest mistake I ever made was offering unlimited data for the iPhone exactly to your
[02:16:42.940 --> 02:16:44.100]   point.
[02:16:44.100 --> 02:16:45.740]   They had so they did not plan.
[02:16:45.740 --> 02:16:49.940]   They had no idea that people, what they're going to use the phone for the internet.
[02:16:49.940 --> 02:16:57.180]   So, I'm going to round right now with throttling our speeds simply because most people don't
[02:16:57.180 --> 02:17:02.220]   need or want 300 cable channels anymore, because we have other ways to get our content.
[02:17:02.220 --> 02:17:08.700]   And because the business model doesn't work is not addressing the coming apocalypse of
[02:17:08.700 --> 02:17:11.660]   always on herables and glasses.
[02:17:11.660 --> 02:17:12.660]   Well, isn't 5G.
[02:17:12.660 --> 02:17:14.020]   Isn't that the point of 5G?
[02:17:14.020 --> 02:17:16.220]   I mean, we're going to see a lot about 5G at CES.
[02:17:16.220 --> 02:17:18.220]   Isn't that what the industry is saying?
[02:17:18.220 --> 02:17:19.220]   The building.
[02:17:19.220 --> 02:17:22.580]   The industry is fighting 5G in the United States.
[02:17:22.580 --> 02:17:24.340]   So that's not true overseas.
[02:17:24.340 --> 02:17:27.740]   But in the United States, nobody's in a hurry to roll out 5G.
[02:17:27.740 --> 02:17:32.700]   Comcast, AT&T, plenty of our providers who spend all kinds of money on lobbyists and
[02:17:32.700 --> 02:17:38.620]   public information campaigns instructing us that 5G isn't really that necessary, that
[02:17:38.620 --> 02:17:40.420]   we don't actually need it.
[02:17:40.420 --> 02:17:49.980]   So, I am so sick and tired of the people making decisions for our future, looking only in
[02:17:49.980 --> 02:17:51.340]   the rearview mirror.
[02:17:51.340 --> 02:18:00.020]   Because for whatever short term game Ajit Pai has found himself for repealing the net neutrality
[02:18:00.020 --> 02:18:05.540]   regulations as they stood, we're all going to be screwed, including him within the decade.
[02:18:05.540 --> 02:18:07.140]   Well, there is good news.
[02:18:07.140 --> 02:18:12.340]   Mark Zuckerberg says his personal challenge for 2018 is going to be to fix Facebook.
[02:18:12.340 --> 02:18:14.620]   So there's some good news.
[02:18:14.620 --> 02:18:16.940]   Let's take a break.
[02:18:16.940 --> 02:18:21.380]   One last break, then we'll, because this is, this is the show that should be an eight-hour
[02:18:21.380 --> 02:18:22.380]   show.
[02:18:22.380 --> 02:18:23.380]   I don't want to stop.
[02:18:23.380 --> 02:18:24.780]   Let's do it.
[02:18:24.780 --> 02:18:27.140]   We could make the longest Twitter ever, just because you guys are so great.
[02:18:27.140 --> 02:18:32.220]   Mike Elgin, Amy Webb, Michael Sargent, fantastic panel.
[02:18:32.220 --> 02:18:36.500]   And as always, Amy, you bring the thunder.
[02:18:36.500 --> 02:18:42.580]   I really, seriously, I really appreciate that point of view is, well, we should not be planning
[02:18:42.580 --> 02:18:43.580]   for it.
[02:18:43.580 --> 02:18:44.580]   We should not be reactive.
[02:18:44.580 --> 02:18:45.580]   We need to be proactive.
[02:18:45.580 --> 02:18:47.420]   That's what you're saying, right?
[02:18:47.420 --> 02:18:49.940]   Let's think about the future.
[02:18:49.940 --> 02:18:50.940]   What's going to happen?
[02:18:50.940 --> 02:18:57.140]   All right, before we do that, there is a solution to your bad Wi-Fi.
[02:18:57.140 --> 02:18:58.140]   And that is Eero.
[02:18:58.140 --> 02:19:00.620]   And I have to say, it is a great solution.
[02:19:00.620 --> 02:19:05.180]   I know, because I have Eero at home, and I got the new, the new Eero system.
[02:19:05.180 --> 02:19:09.140]   I had to, originally, I bought the early 2016 Eero 1.
[02:19:09.140 --> 02:19:12.620]   And over time, they learned from hundreds of thousands of systems.
[02:19:12.620 --> 02:19:15.540]   And so the Eero got smarter and faster, and then they released the second generation
[02:19:15.540 --> 02:19:21.620]   Eero and the Eero beacon, tri-band 525G and a 2.4 gigahertz.
[02:19:21.620 --> 02:19:22.620]   The beacon's great.
[02:19:22.620 --> 02:19:27.460]   You plug it into the wall, and it just extends your Wi-Fi throughout the house.
[02:19:27.460 --> 02:19:31.540]   But I love it because I gave my old Eero to my mom.
[02:19:31.540 --> 02:19:35.060]   So she's got great Wi-Fi now.
[02:19:35.060 --> 02:19:38.060]   The new Eros are, don't listen to this, mom.
[02:19:38.060 --> 02:19:44.940]   The new Eros, elegant, even more elegant, better range, twice the speed, because they
[02:19:44.940 --> 02:19:48.140]   have the third radio.
[02:19:48.140 --> 02:19:52.260]   And now they have a thread radio in the new Eros, which is the low power, I think that's
[02:19:52.260 --> 02:19:53.500]   a Google technology, right?
[02:19:53.500 --> 02:19:56.580]   Low power radio for lock, store bells, other sensors.
[02:19:56.580 --> 02:19:59.500]   So it's got IoT compatibility.
[02:19:59.500 --> 02:20:00.980]   And it's easy.
[02:20:00.980 --> 02:20:05.340]   The Eero unit is about 1500 square feet, but as you grow or you find more problems like
[02:20:05.340 --> 02:20:10.980]   upstairs, downstairs, backyard, you can expand your cover, just plug in another Eero beacon
[02:20:10.980 --> 02:20:12.940]   and boom, another 1500 square feet.
[02:20:12.940 --> 02:20:14.820]   You can have as many as you want.
[02:20:14.820 --> 02:20:19.380]   And I should show you the Eero interface, because what's great is because mom's using
[02:20:19.380 --> 02:20:24.060]   Eero and I'm using Eero, I can look at my network and it says everything looks good.
[02:20:24.060 --> 02:20:26.540]   37 connected devices.
[02:20:26.540 --> 02:20:28.460]   I know.
[02:20:28.460 --> 02:20:34.100]   And I see my bandwidth 119 down 24 up, but, and by the way, one of the best things is
[02:20:34.100 --> 02:20:35.740]   the family profiles.
[02:20:35.740 --> 02:20:40.940]   So I've assigned each member of our household the devices, including our 15 year old.
[02:20:40.940 --> 02:20:43.380]   I can turn on safe filters for him.
[02:20:43.380 --> 02:20:47.100]   So safe search content filtering, that kind of thing.
[02:20:47.100 --> 02:20:52.260]   Safe search has turned on throughout the house to keep viruses and malware at bay.
[02:20:52.260 --> 02:20:54.460]   But also look at this pause button.
[02:20:54.460 --> 02:20:56.380]   I can pause Michael's internet anytime.
[02:20:56.380 --> 02:21:00.100]   I can even say to my echo, I say echo pause, Michael's internet and it goes off and I hear
[02:21:00.100 --> 02:21:01.620]   the scream.
[02:21:01.620 --> 02:21:03.060]   There's a lot of power.
[02:21:03.060 --> 02:21:04.900]   And notice I have a schedule.
[02:21:04.900 --> 02:21:07.740]   So every evening, a 10 p.m.
[02:21:07.740 --> 02:21:10.980]   internet goes down as it should.
[02:21:10.980 --> 02:21:15.100]   Michael goes to bed and in the morning it starts back up at 7 a.m.
[02:21:15.100 --> 02:21:17.300]   I let him do it go a little later on weekends.
[02:21:17.300 --> 02:21:19.140]   That's all the Eero software.
[02:21:19.140 --> 02:21:20.140]   It's all built in.
[02:21:20.140 --> 02:21:22.580]   That's Eero plus, but let's go to let's switch networks.
[02:21:22.580 --> 02:21:24.580]   Let's go to mom's network.
[02:21:24.580 --> 02:21:28.060]   I can go to mom's network now and I can see if she says, hey, I'm having trouble.
[02:21:28.060 --> 02:21:29.380]   I can see the devices.
[02:21:29.380 --> 02:21:32.380]   She's got five because we put her out in the backyard too.
[02:21:32.380 --> 02:21:34.100]   She's only got eight connected devices though.
[02:21:34.100 --> 02:21:35.500]   It's a little bit less.
[02:21:35.500 --> 02:21:36.500]   I could put her in the back.
[02:21:36.500 --> 02:21:37.500]   No, she's got a studio.
[02:21:37.500 --> 02:21:39.100]   That was the problem we solved with the Eero.
[02:21:39.100 --> 02:21:43.540]   I just love these free overnight shipping to the US or Canada with our offer code TWIT
[02:21:43.540 --> 02:21:46.540]   at Eero.com.
[02:21:46.540 --> 02:21:50.780]   Select overnight shipping and enter TWIT to make it free as the offer code.
[02:21:50.780 --> 02:21:52.780]   It's Eero.com offer code TWIT.
[02:21:52.780 --> 02:21:54.060]   I had a call on the radio show.
[02:21:54.060 --> 02:22:00.940]   I had a kind of old traditional, very good router, but he said, but I can't get my Weimos
[02:22:00.940 --> 02:22:02.780]   working because there's congestion.
[02:22:02.780 --> 02:22:04.460]   I said, well, you have two choices.
[02:22:04.460 --> 02:22:05.700]   You can download insider.
[02:22:05.700 --> 02:22:08.100]   You can analyze what's using what channels.
[02:22:08.100 --> 02:22:09.900]   You can manually assign channels.
[02:22:09.900 --> 02:22:11.300]   The Weimo won't let you do that.
[02:22:11.300 --> 02:22:14.220]   You can say, well, Weimo, you own it and we're going to move you.
[02:22:14.220 --> 02:22:18.460]   You can do all this and maybe it'll work because I've done all this many, many times, or you
[02:22:18.460 --> 02:22:20.460]   could just go get an Eero.
[02:22:20.460 --> 02:22:22.460]   That involves all those problems.
[02:22:22.460 --> 02:22:24.780]   Eero.com, don't forget the offer code TWIT.
[02:22:24.780 --> 02:22:28.300]   We thank not only thank you for their support, but thank you for making my Wi-Fi work in
[02:22:28.300 --> 02:22:31.140]   my house.
[02:22:31.140 --> 02:22:32.980]   Yeah.
[02:22:32.980 --> 02:22:36.420]   Title let mom in from the backyard.
[02:22:36.420 --> 02:22:37.420]   All right.
[02:22:37.420 --> 02:22:38.420]   Did we miss any big stories?
[02:22:38.420 --> 02:22:40.700]   We look forward to CES.
[02:22:40.700 --> 02:22:42.780]   We'll have lots of CES news next week.
[02:22:42.780 --> 02:22:47.220]   The week following or no, I guess two weeks later, there's mobile world congress in Barcelona,
[02:22:47.220 --> 02:22:49.940]   your favorite city in the whole wide world.
[02:22:49.940 --> 02:22:53.700]   Go for the Hamoni Barrico stay for the Galaxy S9.
[02:22:53.700 --> 02:22:54.700]   That's where...
[02:22:54.700 --> 02:22:55.700]   Okay.
[02:22:55.700 --> 02:22:56.900]   That's the tagline of the whole show.
[02:22:56.900 --> 02:22:57.900]   That's the whole show.
[02:22:57.900 --> 02:22:58.900]   Yeah.
[02:22:58.900 --> 02:23:03.780]   That's probably where Samsung will announce interesting leaks.
[02:23:03.780 --> 02:23:07.060]   Their tagline camera, the camera reimagined.
[02:23:07.060 --> 02:23:12.460]   So really the battle lines on these smart phones at least for now is all about the cameras.
[02:23:12.460 --> 02:23:18.940]   I have to say using the Pixel 2 XL, best hands down the best camera, even with one lens.
[02:23:18.940 --> 02:23:22.260]   But I'll be very interested because Samsung does amazing stuff.
[02:23:22.260 --> 02:23:24.420]   Oh, here's...
[02:23:24.420 --> 02:23:28.860]   I kind of mentioned this because I kind of got it wrong yesterday on the radio show.
[02:23:28.860 --> 02:23:30.660]   I said, "Oh, don't worry.
[02:23:30.660 --> 02:23:32.660]   Borders, searches of devices are going down."
[02:23:32.660 --> 02:23:35.820]   No, they're not going way up.
[02:23:35.820 --> 02:23:37.620]   They're going way up.
[02:23:37.620 --> 02:23:40.820]   Let me sign into the Wall Street Journal so that I can read this because it's behind a
[02:23:40.820 --> 02:23:41.820]   paywall.
[02:23:41.820 --> 02:23:42.820]   Thank you.
[02:23:42.820 --> 02:23:43.820]   Oh, I can't because my...
[02:23:43.820 --> 02:23:44.820]   Never mind.
[02:23:44.820 --> 02:23:46.060]   I can...
[02:23:46.060 --> 02:23:48.700]   But Wall Street, I can remember I think the story because I read it.
[02:23:48.700 --> 02:23:55.420]   They say that the government in the year 2013 fiscal year, which ended September 30th,
[02:23:55.420 --> 02:23:59.980]   that's why we have the numbers so soon, searched 30,000 devices, mostly as they were going out
[02:23:59.980 --> 02:24:04.060]   of the country, not going in, which is weird.
[02:24:04.060 --> 02:24:07.500]   The vast majority leaving the country, "Why does customers and border patrol search devices
[02:24:07.500 --> 02:24:08.500]   going out?"
[02:24:08.500 --> 02:24:10.020]   In case you're smuggling bits.
[02:24:10.020 --> 02:24:12.060]   I've never been stopped by them going out.
[02:24:12.060 --> 02:24:15.300]   The TSA stops you.
[02:24:15.300 --> 02:24:18.940]   This was 30,000 is up from the year before 19,000.
[02:24:18.940 --> 02:24:21.420]   So big jump.
[02:24:21.420 --> 02:24:28.300]   But they say that only 20% belong to US citizens.
[02:24:28.300 --> 02:24:34.380]   That by my calculation means 6,000 devices were searched.
[02:24:34.380 --> 02:24:36.580]   Is that something to be concerned if you're a US citizen?
[02:24:36.580 --> 02:24:39.220]   80% were foreigners, foreign nationals.
[02:24:39.220 --> 02:24:40.220]   It is.
[02:24:40.220 --> 02:24:46.900]   Well, it's a gray area because the Constitution explicitly protects against unreasonable searches
[02:24:46.900 --> 02:24:47.900]   and seizures.
[02:24:47.900 --> 02:24:51.300]   However, at borders, they go through your luggage and they search...
[02:24:51.300 --> 02:24:52.300]   They're allowed to do that.
[02:24:52.300 --> 02:25:01.700]   The ACLU says they're allowed to do that, but they're not allowed to access your device
[02:25:01.700 --> 02:25:03.300]   to say, "What's your password?"
[02:25:03.300 --> 02:25:10.140]   And Customs and Border Patrol did announce a new policy that won't look at your cloud
[02:25:10.140 --> 02:25:11.140]   information.
[02:25:11.140 --> 02:25:15.180]   They can look at what's on your device, which the ACLU disputes.
[02:25:15.180 --> 02:25:17.460]   They say that violates the Fourth Amendment.
[02:25:17.460 --> 02:25:21.460]   Well, I think this is where some technical legal challenges get in the way.
[02:25:21.460 --> 02:25:27.820]   So if you've got an iPhone X and your face unlocks your phone, we don't have any case
[02:25:27.820 --> 02:25:28.820]   law.
[02:25:28.820 --> 02:25:29.820]   Is this your phone?
[02:25:29.820 --> 02:25:30.820]   That's right.
[02:25:30.820 --> 02:25:32.620]   We don't have any case law.
[02:25:32.620 --> 02:25:38.340]   We don't have any case law saying that forcing somebody to look at their phone is a violation
[02:25:38.340 --> 02:25:41.780]   of any laws right now.
[02:25:41.780 --> 02:25:46.940]   So CPB gets to say, "Well, our policy is they won't look at your cloud stuff unless
[02:25:46.940 --> 02:25:51.740]   they got a warrant or reasonable cause, like a reason to think that you're up to no good."
[02:25:51.740 --> 02:25:57.980]   So somebody called the radio show yesterday and said, "Well, what should I do?"
[02:25:57.980 --> 02:26:01.500]   And I said, "Well, the odds are you won't be inspected.
[02:26:01.500 --> 02:26:02.500]   I've gone into another country.
[02:26:02.500 --> 02:26:03.500]   You have too many times.
[02:26:03.500 --> 02:26:05.460]   It's never been stopped, never anything."
[02:26:05.460 --> 02:26:07.180]   In fact, mostly it's just like, "Hi, come on.
[02:26:07.180 --> 02:26:08.180]   Welcome in."
[02:26:08.180 --> 02:26:09.180]   Right.
[02:26:09.180 --> 02:26:10.180]   Because we're not brown.
[02:26:10.180 --> 02:26:15.260]   We don't have turbines, but for whatever reason, not that they're racial profiling,
[02:26:15.260 --> 02:26:16.580]   having them for fed.
[02:26:16.580 --> 02:26:17.700]   We don't know they're not doing that.
[02:26:17.700 --> 02:26:18.700]   They don't get it.
[02:26:18.700 --> 02:26:19.700]   I know.
[02:26:19.700 --> 02:26:20.940]   We're middle aged white guys.
[02:26:20.940 --> 02:26:22.900]   We're safe.
[02:26:22.900 --> 02:26:29.060]   Nevertheless, if they were to say, "Well, I've decided that I want to search your phone.
[02:26:29.060 --> 02:26:31.260]   I have the right to."
[02:26:31.260 --> 02:26:32.700]   ACLU says, "No."
[02:26:32.700 --> 02:26:33.700]   Say, "No."
[02:26:33.700 --> 02:26:36.300]   Then call us and we'll send a lawyer down.
[02:26:36.300 --> 02:26:39.980]   What we know they'll do is just say, "Until you let us, you're not leaving.
[02:26:39.980 --> 02:26:41.380]   You're an airport jail."
[02:26:41.380 --> 02:26:42.380]   Right.
[02:26:42.380 --> 02:26:43.380]   Good luck.
[02:26:43.380 --> 02:26:44.380]   Have fun.
[02:26:44.380 --> 02:26:50.380]   So you might, if you want to go home or get on your flight, you might say, "Well, okay."
[02:26:50.380 --> 02:26:53.180]   But then this is what the guy asked, "Okay, so they take my phone and they take it into
[02:26:53.180 --> 02:26:55.580]   another room.
[02:26:55.580 --> 02:26:57.140]   What do I do when I get it back?"
[02:26:57.140 --> 02:27:01.500]   And I said, "Well, really, to be prudent, you throw the phone away."
[02:27:01.500 --> 02:27:04.260]   That's a thousand dollar iPhone at the door.
[02:27:04.260 --> 02:27:05.260]   Yeah.
[02:27:05.260 --> 02:27:08.860]   So do you never take your expensive phones and computers with you?
[02:27:08.860 --> 02:27:09.860]   You take it with you, right?
[02:27:09.860 --> 02:27:10.860]   I do.
[02:27:10.860 --> 02:27:11.860]   I take it with you.
[02:27:11.860 --> 02:27:12.860]   I have to.
[02:27:12.860 --> 02:27:14.860]   I take all my expensive stuff with me.
[02:27:14.860 --> 02:27:19.860]   You just have to decide in advance if you're really concerned about this.
[02:27:19.860 --> 02:27:22.220]   You have to think, "Okay, what's on my device?
[02:27:22.220 --> 02:27:24.620]   What can I take off the device before I travel?"
[02:27:24.620 --> 02:27:25.620]   Here's the problem.
[02:27:25.620 --> 02:27:26.620]   Another device I could carry instead.
[02:27:26.620 --> 02:27:27.620]   Here's the problem.
[02:27:27.620 --> 02:27:32.540]   One password has a flight setting where it erases anything that you don't think is
[02:27:32.540 --> 02:27:33.980]   absolutely necessary.
[02:27:33.980 --> 02:27:35.340]   And then when you get home, you re-sync.
[02:27:35.340 --> 02:27:37.220]   So your passwords aren't on your device.
[02:27:37.220 --> 02:27:38.220]   But that's not the problem.
[02:27:38.220 --> 02:27:41.020]   The problem is if they take the phone, yes, they can get everything that's on the phone.
[02:27:41.020 --> 02:27:43.460]   They can also put a Trojan horse on your phone.
[02:27:43.460 --> 02:27:45.860]   They could put something on your phone or your computer.
[02:27:45.860 --> 02:27:48.540]   And there's no way to really know.
[02:27:48.540 --> 02:27:55.460]   I mean, that's a potential risk, but I think the larger issue is the constitutional question,
[02:27:55.460 --> 02:27:58.020]   which really hasn't been fully resolved.
[02:27:58.020 --> 02:28:02.780]   Two other things about this before we get into that though, is that they say that one
[02:28:02.780 --> 02:28:06.660]   of the reasons is to combat, they give three reasons why they're searching it.
[02:28:06.660 --> 02:28:09.060]   Combat terrorism, okay, fine.
[02:28:09.060 --> 02:28:11.860]   Child pornography and visa fraud.
[02:28:11.860 --> 02:28:15.220]   The child pornography thing always comes up.
[02:28:15.220 --> 02:28:16.220]   Yeah, right.
[02:28:16.220 --> 02:28:19.420]   Because if you ever want to do something that sounds unpopular and invasior privacy
[02:28:19.420 --> 02:28:21.220]   defend pornot, childly.
[02:28:21.220 --> 02:28:22.220]   Exactly.
[02:28:22.220 --> 02:28:26.140]   And I think that child pornography is being smuggled on phones and laptops.
[02:28:26.140 --> 02:28:30.220]   I mean, it's going over the dark web or whatever.
[02:28:30.220 --> 02:28:31.220]   Who knows?
[02:28:31.220 --> 02:28:36.820]   But the other thing is that they said that the main reason for the wild increase, and
[02:28:36.820 --> 02:28:41.940]   I thought this was surprising, is that people are carrying more devices.
[02:28:41.940 --> 02:28:47.660]   So individual search orders on average have a larger number of devices, and that's why
[02:28:47.660 --> 02:28:48.660]   they're searching more devices.
[02:28:48.660 --> 02:28:49.900]   They didn't say more devices.
[02:28:49.900 --> 02:28:50.900]   Did they?
[02:28:50.900 --> 02:28:51.900]   Did they said more people?
[02:28:51.900 --> 02:28:54.500]   They're saying that travelers are carrying more devices.
[02:28:54.500 --> 02:28:55.500]   Okay.
[02:28:55.500 --> 02:28:59.300]   So multiple phones, a phone tablet and laptop, this sort of thing.
[02:28:59.300 --> 02:29:01.100]   And this is the claim anyway, I don't know.
[02:29:01.100 --> 02:29:08.740]   But the number from the government is the government searched the devices of 30,200 people.
[02:29:08.740 --> 02:29:09.980]   They didn't say how many devices.
[02:29:09.980 --> 02:29:14.420]   They said 30,000 people, which is up from 19,000 people.
[02:29:14.420 --> 02:29:16.300]   So they can say more devices.
[02:29:16.300 --> 02:29:18.140]   That's just, pay no attention.
[02:29:18.140 --> 02:29:21.300]   So I guess they're not telling us how many actual devices.
[02:29:21.300 --> 02:29:29.020]   The EFF, just so you know, EFF and the ACLU and last fall sued on behalf of 11 travelers,
[02:29:29.020 --> 02:29:34.700]   challenging the Department of Homeland Security on these, what they claim are unlawful searches.
[02:29:34.700 --> 02:29:39.380]   The plaintiffs include 10 U.S. citizens and one lawful permanent resident, including a
[02:29:39.380 --> 02:29:46.300]   military veteran, journalists, students and artists, a NASA engineer and a business owner,
[02:29:46.300 --> 02:29:50.100]   several are Muslims or people of color, all were reentering the country from business
[02:29:50.100 --> 02:29:53.860]   or personal travel when border officers searched their devices.
[02:29:53.860 --> 02:29:57.020]   They were not accused of any wrongdoing afterwards.
[02:29:57.020 --> 02:30:03.180]   Officers also confiscated and kept the devices of several plaintiffs for weeks or months.
[02:30:03.180 --> 02:30:05.740]   And this was filed in September.
[02:30:05.740 --> 02:30:07.740]   The DHS has held one plaintiff's device.
[02:30:07.740 --> 02:30:10.180]   Again, not accused of wrongdoing afterwards.
[02:30:10.180 --> 02:30:11.900]   They've held it since January.
[02:30:11.900 --> 02:30:14.020]   They had it for nine months.
[02:30:14.020 --> 02:30:21.420]   Listen, there is always going to be a tension between law enforcement, which wants, and
[02:30:21.420 --> 02:30:23.380]   to their, it makes perfect sense, right?
[02:30:23.380 --> 02:30:31.660]   They want as much access as they can possibly get for all, you know, plausible and reasonably
[02:30:31.660 --> 02:30:33.180]   good reasons.
[02:30:33.180 --> 02:30:37.260]   And as individuals, we don't want that access to happen.
[02:30:37.260 --> 02:30:38.900]   But here's the problem.
[02:30:38.900 --> 02:30:41.980]   The problem is, how are you going to scale this, right?
[02:30:41.980 --> 02:30:46.300]   So unless we just don't allow people to come into the United States anymore, we're going
[02:30:46.300 --> 02:30:49.820]   to have more gadgets over the next 10 years, not less.
[02:30:49.820 --> 02:30:54.900]   So just pulling everybody aside and forcing everybody, you know, as we enter the world
[02:30:54.900 --> 02:31:00.340]   of hearables and wearables and all these different devices, this search and seizures
[02:31:00.340 --> 02:31:02.020]   is totally impractical.
[02:31:02.020 --> 02:31:06.900]   And you know, there are lots of countries on the planet that have worse problems than
[02:31:06.900 --> 02:31:07.900]   we do.
[02:31:07.900 --> 02:31:11.140]   >> Israel apparently does a very good job of securing their borders.
[02:31:11.140 --> 02:31:12.140]   >> I have this.
[02:31:12.140 --> 02:31:13.220]   >> That's absolutely right.
[02:31:13.220 --> 02:31:19.180]   I have been detained in, so I've been in and out of Israel and I didn't get detained, but
[02:31:19.180 --> 02:31:24.300]   I did get pulled aside because somebody asked me a question in Hebrew and I understood what
[02:31:24.300 --> 02:31:28.260]   they were saying, although I don't, like I understand little bits and pieces, but I
[02:31:28.260 --> 02:31:29.740]   don't actually speak.
[02:31:29.740 --> 02:31:32.860]   And then I had to explain myself.
[02:31:32.860 --> 02:31:37.060]   >> But I've told, and you can, tell me if this is the case, but one of the things Israel
[02:31:37.060 --> 02:31:39.460]   does is they have highly trained officers.
[02:31:39.460 --> 02:31:41.900]   >> That's a good problem.
[02:31:41.900 --> 02:31:47.140]   >> They primarily do their job by looking in your eyes, looking at you and talking to
[02:31:47.140 --> 02:31:48.140]   you.
[02:31:48.140 --> 02:31:49.140]   >> That's absolutely right.
[02:31:49.140 --> 02:31:52.900]   I travel on average every three days.
[02:31:52.900 --> 02:32:00.940]   So I am at an airport or train or the train stations every three days and the way that
[02:32:00.940 --> 02:32:10.940]   our TSA uses the various rules and regulations is completely inconsistent.
[02:32:10.940 --> 02:32:18.340]   And if we want to get serious about, if we actually want to implement what a lot of these
[02:32:18.340 --> 02:32:25.140]   rules were set up to do, then just stopping people of color and demanding to see their
[02:32:25.140 --> 02:32:29.180]   devices is actually not the best way for the people.
[02:32:29.180 --> 02:32:30.820]   >> Security theater, we know that.
[02:32:30.820 --> 02:32:31.820]   >> It's all security.
[02:32:31.820 --> 02:32:35.900]   >> I also think to a certain extent, they may be trying to establish a norm that everyone
[02:32:35.900 --> 02:32:40.900]   accepts for a future when they decide that there's some real reason that they're really
[02:32:40.900 --> 02:32:41.900]   looking for something specific.
[02:32:41.900 --> 02:32:42.900]   >> That's plausible.
[02:32:42.900 --> 02:32:44.740]   >> And they just want to get us all used to it.
[02:32:44.740 --> 02:32:47.220]   That'd be my guess.
[02:32:47.220 --> 02:32:53.260]   One thing you can do though, if you don't want to unlock your phone, just turn it off.
[02:32:53.260 --> 02:32:57.100]   And then the first time you boot your phone, at least with an iPhone, you have to enter
[02:32:57.100 --> 02:32:59.860]   a passcode, which is a constitutionally protected speech.
[02:32:59.860 --> 02:33:00.860]   You don't have to give it up.
[02:33:00.860 --> 02:33:03.340]   Whereas they can't just point your face.
[02:33:03.340 --> 02:33:09.340]   >> Yeah, but they can still say, okay, you unlock this or you go to airport jail until
[02:33:09.340 --> 02:33:11.460]   we decide to let you go.
[02:33:11.460 --> 02:33:16.740]   They have ways of coercing you that may not be constitutionally protected or legal, but
[02:33:16.740 --> 02:33:18.140]   that doesn't stop them from doing it.
[02:33:18.140 --> 02:33:20.260]   >> It all depends on how important it is to you.
[02:33:20.260 --> 02:33:23.980]   I think the best course of action is to just get rid of anything.
[02:33:23.980 --> 02:33:29.700]   You care about delete to Google Photos, delete anything that has any sort of data that you
[02:33:29.700 --> 02:33:30.700]   might care about.
[02:33:30.700 --> 02:33:35.260]   And then just once you're through airport security, you can reinstall that stuff.
[02:33:35.260 --> 02:33:37.900]   >> Does anybody know what our tourism numbers are like?
[02:33:37.900 --> 02:33:39.700]   >> It's true down on the US.
[02:33:39.700 --> 02:33:40.700]   >> Inciderably.
[02:33:40.700 --> 02:33:41.700]   >> Yeah, so listen.
[02:33:41.700 --> 02:33:46.060]   >> I went through this when I had to go back and forth to China.
[02:33:46.060 --> 02:33:47.060]   And it was no fun.
[02:33:47.060 --> 02:33:48.460]   I was a journalist then.
[02:33:48.460 --> 02:33:55.980]   And it was no fun having my camera gone through in my lap, my Toshiba satellite pro.
[02:33:55.980 --> 02:33:59.140]   I don't think we want to be in a situation where it's that difficult to come in and
[02:33:59.140 --> 02:34:00.860]   out of a country.
[02:34:00.860 --> 02:34:05.820]   Our big part of our economy depends on the relationships that we have with other countries.
[02:34:05.820 --> 02:34:10.660]   And the desirability of the United States is for people to come and visit, to take their
[02:34:10.660 --> 02:34:11.660]   vacations.
[02:34:11.660 --> 02:34:16.060]   >> The US Department of Commerce, the only numbers I have here are from the first quarter
[02:34:16.060 --> 02:34:18.060]   of the year.
[02:34:18.060 --> 02:34:22.100]   Number of visitors was down by 700,000 in the first quarter.
[02:34:22.100 --> 02:34:26.540]   10% drop from European countries, 7% drop from Mexico, the largest drops from the Middle
[02:34:26.540 --> 02:34:29.220]   East in Africa.
[02:34:29.220 --> 02:34:34.980]   So it's a significant drop, 4.2% drop in tourism.
[02:34:34.980 --> 02:34:36.260]   And that was in the first quarter.
[02:34:36.260 --> 02:34:37.260]   I don't know what it is.
[02:34:37.260 --> 02:34:38.260]   >> Yeah.
[02:34:38.260 --> 02:34:39.260]   >> The rest of the year.
[02:34:39.260 --> 02:34:42.820]   >> Well, 4.2% doesn't sound like a lot, but multiplied by the amount of revenue brought
[02:34:42.820 --> 02:34:43.820]   in.
[02:34:43.820 --> 02:34:44.820]   >> 2.7 billion dollars.
[02:34:44.820 --> 02:34:45.820]   >> Sure.
[02:34:45.820 --> 02:34:49.500]   I mean, that could have a serious effect on local communities.
[02:34:49.500 --> 02:34:50.500]   >> Yeah.
[02:34:50.500 --> 02:34:51.500]   >> Sure.
[02:34:51.500 --> 02:34:53.220]   >> One last story.
[02:34:53.220 --> 02:34:54.620]   Doug Evans, you remember him?
[02:34:54.620 --> 02:34:58.940]   He's a guy who brought us the Juicero, the $400 juicer that turned out it didn't do anything.
[02:34:58.940 --> 02:34:59.940]   >> I love his story.
[02:34:59.940 --> 02:35:01.620]   >> He's got a new business.
[02:35:01.620 --> 02:35:03.980]   I think you're going to like it.
[02:35:03.980 --> 02:35:06.260]   He's selling raw water.
[02:35:06.260 --> 02:35:07.260]   He says, get off.
[02:35:07.260 --> 02:35:08.260]   >> Oh, no.
[02:35:08.260 --> 02:35:09.260]   Is that the same guy?
[02:35:09.260 --> 02:35:10.260]   >> Yeah.
[02:35:10.260 --> 02:35:12.620]   He says, get off the water grid.
[02:35:12.620 --> 02:35:20.940]   You're drinking water that all the nutrients, aka E. coli bacteria, has been removed from.
[02:35:20.940 --> 02:35:24.740]   He's got a new company called Live Water.
[02:35:24.740 --> 02:35:29.780]   Get off the water grid, unfiltered, untreated, unsterilized spring water.
[02:35:29.780 --> 02:35:32.500]   And by the way, it's not cheap.
[02:35:32.500 --> 02:35:35.900]   $37 for two and a half gallons.
[02:35:35.900 --> 02:35:39.340]   And then $15 thereafter for refills.
[02:35:39.340 --> 02:35:44.740]   It has a Kevin Freeman, who is a manager at Ringbo Grocery in the San Francisco's Mission
[02:35:44.740 --> 02:35:45.740]   District.
[02:35:45.740 --> 02:35:46.740]   But of course they sell this.
[02:35:46.740 --> 02:35:54.100]   It says it has a vaguely mild sweetness, a nice, smooth mouth feel, nothing that overwhelms
[02:35:54.100 --> 02:35:56.140]   the favored profile.
[02:35:56.140 --> 02:35:57.140]   And it's crunchy.
[02:35:57.140 --> 02:35:59.780]   It's the sea monkeys.
[02:35:59.780 --> 02:36:02.940]   Don't drink unprocessed water.
[02:36:02.940 --> 02:36:07.260]   And certainly don't pay $37 for it.
[02:36:07.260 --> 02:36:08.740]   And Doug, really, come on.
[02:36:08.740 --> 02:36:11.100]   Get a real job, Doug.
[02:36:11.100 --> 02:36:12.100]   >> That is really funny.
[02:36:12.100 --> 02:36:13.260]   I didn't realize it was the same.
[02:36:13.260 --> 02:36:14.260]   >> It was the same guy.
[02:36:14.260 --> 02:36:15.260]   >> Oh my God.
[02:36:15.260 --> 02:36:16.700]   >> Is this, did you predict this?
[02:36:16.700 --> 02:36:19.900]   >> In your book, I don't think you predicted this.
[02:36:19.900 --> 02:36:24.060]   >> His domain expertise is bad beverage ideas.
[02:36:24.060 --> 02:36:28.480]   >> Live water provides convenient delivery of pristine mountain spring water plus tools
[02:36:28.480 --> 02:36:31.940]   to gather it independently, like mushrooms.
[02:36:31.940 --> 02:36:37.540]   >> So people are sneaking on a private property and stealing brackish water from little creeks.
[02:36:37.540 --> 02:36:42.420]   >> Well, there are no synthetic toxins in raw water, at least as far as toxins are all
[02:36:42.420 --> 02:36:43.420]   natural.
[02:36:43.420 --> 02:36:44.420]   >> Here's Doug.
[02:36:44.420 --> 02:36:46.420]   Let's just see what he has to say.
[02:36:46.420 --> 02:36:51.860]   >> The thing that bothers me about this is the idea that, in theory, the idea of drinking
[02:36:51.860 --> 02:36:55.860]   out of streams and stuff like that seems like, "Oh, look, he's wearing those toes shoes."
[02:36:55.860 --> 02:36:59.060]   Of course, of course he has.
[02:36:59.060 --> 02:37:01.580]   It seems like this natural thing and all that stuff.
[02:37:01.580 --> 02:37:05.980]   But when you bottle something, the ecosystem continues to evolve.
[02:37:05.980 --> 02:37:07.860]   And you don't know what it's going to turn into.
[02:37:07.860 --> 02:37:09.340]   This is the worst idea.
[02:37:09.340 --> 02:37:13.700]   >> Today we hear so much about local food, but what about local water?
[02:37:13.700 --> 02:37:15.660]   >> Let's get some local water.
[02:37:15.660 --> 02:37:17.940]   >> No, don't drink the local water.
[02:37:17.940 --> 02:37:19.500]   I've drunk the water out of streams.
[02:37:19.500 --> 02:37:20.940]   It's a big mistake.
[02:37:20.940 --> 02:37:22.940]   >> If I may just say something.
[02:37:22.940 --> 02:37:23.940]   >> Yes.
[02:37:23.940 --> 02:37:26.180]   >> It's going to sound like I'm defending.
[02:37:26.180 --> 02:37:27.180]   I'm not.
[02:37:27.180 --> 02:37:31.700]   Oftentimes, fringe thinkers, when they first come up with ideas, they are chastised for
[02:37:31.700 --> 02:37:33.740]   coming up with crazy, hair-brained ideas.
[02:37:33.740 --> 02:37:37.660]   And after a period of time, it was in the fringe end of the man.
[02:37:37.660 --> 02:37:40.780]   I don't think that's the case now with the water.
[02:37:40.780 --> 02:37:44.700]   >> But this is why it shouldn't be banned outright.
[02:37:44.700 --> 02:37:46.700]   This sort of experimentation should be allowed to be.
[02:37:46.700 --> 02:37:47.700]   >> It should be allowed.
[02:37:47.700 --> 02:37:51.980]   It's fine if the market wants to buy unfiltered, untreated spring water.
[02:37:51.980 --> 02:37:52.980]   >> That's right.
[02:37:52.980 --> 02:37:54.620]   >> Darwin will sort it all out.
[02:37:54.620 --> 02:37:56.260]   That's what I'm saying.
[02:37:56.260 --> 02:37:57.260]   >> That's right.
[02:37:57.260 --> 02:37:59.060]   >> What Darwin did sort this out.
[02:37:59.060 --> 02:38:00.060]   >> Yeah.
[02:38:00.060 --> 02:38:01.060]   Unfortunately.
[02:38:01.060 --> 02:38:02.500]   >> That's why we drink it in bottles.
[02:38:02.500 --> 02:38:04.820]   >> The way nature intended.
[02:38:04.820 --> 02:38:07.820]   You should call it Darwin water.
[02:38:07.820 --> 02:38:12.980]   We lost Mike as Sergeant, but it was so great to have him senior editor at Mobile Nations.
[02:38:12.980 --> 02:38:18.260]   A great guest always is a pleasure to have somebody a little younger than me on the show.
[02:38:18.260 --> 02:38:19.580]   Thank you, Michael, for being here.
[02:38:19.580 --> 02:38:22.780]   Amy Webb, you rock hard.
[02:38:22.780 --> 02:38:24.540]   I'm really glad to have you on Always.
[02:38:24.540 --> 02:38:28.540]   Amy Webb with 2B's.io, Amy Webb on the Twitter.
[02:38:28.540 --> 02:38:30.180]   Do you actually still use the Twitter?
[02:38:30.180 --> 02:38:31.980]   >> I use the Twitter.
[02:38:31.980 --> 02:38:32.980]   >> You're a warm leader.
[02:38:32.980 --> 02:38:33.980]   You get to use the Twitter.
[02:38:33.980 --> 02:38:34.980]   >> I know.
[02:38:34.980 --> 02:38:35.980]   I'm a blue check mark.
[02:38:35.980 --> 02:38:36.980]   >> Say it.
[02:38:36.980 --> 02:38:37.980]   >> You're a blue check mark.
[02:38:37.980 --> 02:38:38.980]   You see?
[02:38:38.980 --> 02:38:39.980]   >> A blue check mark.
[02:38:39.980 --> 02:38:40.980]   >> She's verified, folks.
[02:38:40.980 --> 02:38:41.980]   >> That means so much.
[02:38:41.980 --> 02:38:43.780]   >> Apparently not.
[02:38:43.780 --> 02:38:47.100]   Mike Elgin is also here at Gastronomad.net.
[02:38:47.100 --> 02:38:49.900]   The new book is available there.
[02:38:49.900 --> 02:38:53.300]   It is called The Art of Living Everywhere.
[02:38:53.300 --> 02:38:57.300]   Gastronomad, The Art of Living Everywhere and Eating Everything.
[02:38:57.300 --> 02:39:00.140]   A man after my own heart.
[02:39:00.140 --> 02:39:01.140]   >> Thank you, Leo.
[02:39:01.140 --> 02:39:02.140]   Thank you so much.
[02:39:02.140 --> 02:39:03.140]   >> Thank you all for being here.
[02:39:03.140 --> 02:39:05.460]   With this was such a good show.
[02:39:05.460 --> 02:39:07.020]   I think you probably agree, right?
[02:39:07.020 --> 02:39:08.020]   I'm glad you were here.
[02:39:08.020 --> 02:39:11.460]   If you're still listening here, you must agree.
[02:39:11.460 --> 02:39:13.140]   Or you just don't have a bladder.
[02:39:13.140 --> 02:39:15.700]   And I'm sorry about that.
[02:39:15.700 --> 02:39:18.500]   But if you want to watch live, you can.
[02:39:18.500 --> 02:39:21.260]   You can even pause it if you're watching live.
[02:39:21.260 --> 02:39:25.300]   All you have to do is go to it.tv/live every Sunday afternoon around about 3 p.m.
[02:39:25.300 --> 02:39:26.300]   Pacific, 6 p.m.
[02:39:26.300 --> 02:39:27.780]   Easter 2300 UTC.
[02:39:27.780 --> 02:39:29.680]   If you do that, please go on the chatroom.
[02:39:29.680 --> 02:39:30.680]   IRC.twit.tv.
[02:39:30.680 --> 02:39:33.020]   You can't pause the chatroom.
[02:39:33.020 --> 02:39:38.660]   That is a living vital force that flows through all of our brains.
[02:39:38.660 --> 02:39:39.660]   It's like raw water.
[02:39:39.660 --> 02:39:40.660]   It's raw water.
[02:39:40.660 --> 02:39:44.180]   It's like raw water of technology.
[02:39:44.180 --> 02:39:47.540]   If you're not here in person, and by the way, we had some great people in the studio, all
[02:39:47.540 --> 02:39:50.980]   of whom have fled, you can email tickets@twit.tv.
[02:39:50.980 --> 02:39:54.980]   We will put a seat out for you and encourage you to stay as long as you want, but no longer.
[02:39:54.980 --> 02:39:57.900]   And if you can't be here in person, you can't watch a stream.
[02:39:57.900 --> 02:39:59.900]   There's always on demand versions of everything we do.
[02:39:59.900 --> 02:40:07.660]   Audio and video at Twit.tv for all of our shows or subscribing your favorite podcast application.
[02:40:07.660 --> 02:40:09.420]   Thank you very much for being here.
[02:40:09.420 --> 02:40:10.420]   We'll see you next time.
[02:40:10.420 --> 02:40:11.420]   The Twit is in the game.
[02:40:11.420 --> 02:40:11.420]   Bye bye.
[02:40:11.420 --> 02:40:22.320]   See you next time.

